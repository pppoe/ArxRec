<!DOCTYPE html>
<html lang="en">
  <head>
    <script async src="https://www.googletagmanager.com/gtag/js?id=G-V34CNNDP8V"></script>
    <script>
      window.dataLayer = window.dataLayer || [];
      function gtag(){dataLayer.push(arguments);}
      gtag('js', new Date());
      gtag('config', 'G-V34CNNDP8V');
    </script>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Arxiv Paper Selection</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.3/css/all.min.css" crossorigin="anonymous" referrerpolicy="no-referrer" />
    <style>
    body {
      font-family: Arial, sans-serif;
      margin: 0;
      padding: 0;
      background-color: #f4f4f4;
    }
    header {
      position: fixed;
      top: 0;
      left: 0;
      width: 100%;
      background-color: #ffffff;
      color: black;
      padding: 10px;
      text-align: center;
      z-index: 1000;
      border-bottom: 1px solid #ddd;
    }
    header div {
      display: block;
      margin: 10px auto;
    }

    #home-icon {
      display: block;
      float: left;
      margin: 5px;
      text-decoration: none;
      color: black;
    }

    main {
      margin-top: 60px; /* Adjusted margin to account for fixed header */
      padding: 20px;
    }

    .post {
      background-color: white;
      border: 1px solid #ddd;
      border-radius: 5px;
      margin-bottom: 10px;
      padding: 10px 20px;
      max-height: 2000px;
      overflow: scroll;
    }
    .post img {
      display: block;
      margin-top: 5px;
      max-width: auto;
      max-height: 100px;
    }
    .post .clear {
      clear: both;
      display: block;
    }
    .post a {
      text-decoration: none;
    }
    .post a:hover {
      color: #0056b3;
    }
    .post a:visited {
      color: #0056b3;
    }
    .post div.comment {
      text-align: right;
    }
    .post div.comment a {
      margin: 1em;
    }
    .post .text {
      margin: 1em 0em;
      padding: 0;
    }
    .post .text .title {
    }
    .post .text .author {
    }
    .post .text .abstract {
    }
    .post .topK {
      display: block;
      margin: 0.5em;
    }
    .post .date {
      margin: 0;
      padding: 0;
      text-size: small; 
      color: gray;
    }
    .post .link {
      margin: 0;
      padding: 0;
    }
    @media screen and (max-width: 600px) {
      body {
        max-width: 100%; 
      }
      #home-icon {
        float: none;
        display: block;
        text-align: center;
        margin-bottom: 10px;
      }
    }
    footer {
      width: 100%;
      background-color: #ddd;
      text-align: center;
      z-index: 1000;
      padding: 20px 0px;
      margin-bottom: 20px;
      left: 0;
    }

    #next-btn,
    #prev-btn {
      background-color: #4CAF50;
      color: white;
      padding: 8px 16px;
      margin: 0 50px;
      border: none;
      border-radius: 4px;
      cursor: pointer;
    }

    .links {
      padding: 20px;
    }
    .links a {
      text-decoration: none;
    }
    .links a:hover {
      color: #0056b3;
    }
    .links a:visited {
      color: #0056b3;
    }

    #page-index {
      font-size: small;
    }
    .ads {
      width: 100%;
    }
    #prev-btn {
      background-color: #4CAF50;
      color: white;
      padding: 8px 16px;
      margin: 0 50px;
      border: none;
      border-radius: 4px;
      cursor: pointer;
    }
    </style>
  </head>
  <body>

    <header>
      <a id="prev-btn" href="daily20241007.html"><i class="fas fa-chevron-left"></i></a>
      <a href="https://haoxiang.org/">About</a>
    </header>

    <main id="content">
      <!-- Posts will be dynamically added here using JavaScript -->
    </main>

    <script>
      // Dummy data for posts
      const posts = [
{"title": "LiDAR-GS:Real-time LiDAR Re-Simulation using Gaussian Splatting", "author": "Qifeng Chen and Sheng Yang and Sicong Du and Tao Tang and Peng Chen and Yuchi Huo", "abstract": "  LiDAR simulation plays a crucial role in closed-loop simulation for\nautonomous driving. Although recent advancements, such as the use of\nreconstructed mesh and Neural Radiance Fields (NeRF), have made progress in\nsimulating the physical properties of LiDAR, these methods have struggled to\nachieve satisfactory frame rates and rendering quality. To address these\nlimitations, we present LiDAR-GS, the first LiDAR Gaussian Splatting method,\nfor real-time high-fidelity re-simulation of LiDAR sensor scans in public urban\nroad scenes. The vanilla Gaussian Splatting, designed for camera models, cannot\nbe directly applied to LiDAR re-simulation. To bridge the gap between passive\ncamera and active LiDAR, our LiDAR-GS designs a differentiable laser beam\nsplatting, grounded in the LiDAR range view model. This innovation allows for\nprecise surface splatting by projecting lasers onto micro cross-sections,\neffectively eliminating artifacts associated with local affine approximations.\nAdditionally, LiDAR-GS leverages Neural Gaussian Fields, which further\nintegrate view-dependent clues, to represent key LiDAR properties that are\ninfluenced by the incident angle and external factors. Combining these\npractices with some essential adaptations, e.g., dynamic instances\ndecomposition, our approach succeeds in simultaneously re-simulating depth,\nintensity, and ray-drop channels, achieving state-of-the-art results in both\nrendering frame rate and quality on publically available large scene datasets.\nOur source code will be made publicly available.\n", "link": "http://arxiv.org/abs/2410.05111v1", "date": "2024-10-07", "relevancy": 3.3562, "topK": [{"title": "RTG-SLAM: Real-time 3D Reconstruction at Scale using Gaussian Splatting", "link": "http://arxiv.org/abs/2404.19706v1", "similarity": 0.7588}, {"title": "MiraGe: Editable 2D Images using Gaussian Splatting", "link": "http://arxiv.org/abs/2410.01521v1", "similarity": 0.6277}, {"title": "Dynamic Gaussians Mesh: Consistent Mesh Reconstruction from Monocular\n  Videos", "link": "http://arxiv.org/abs/2404.12379v2", "similarity": 0.6273}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20LiDAR-GS%3AReal-time%20LiDAR%20Re-Simulation%20using%20Gaussian%20Splatting&body=Title%3A%20LiDAR-GS%3AReal-time%20LiDAR%20Re-Simulation%20using%20Gaussian%20Splatting%0AAuthor%3A%20Qifeng%20Chen%20and%20Sheng%20Yang%20and%20Sicong%20Du%20and%20Tao%20Tang%20and%20Peng%20Chen%20and%20Yuchi%20Huo%0AAbstract%3A%20%20%20LiDAR%20simulation%20plays%20a%20crucial%20role%20in%20closed-loop%20simulation%20for%0Aautonomous%20driving.%20Although%20recent%20advancements%2C%20such%20as%20the%20use%20of%0Areconstructed%20mesh%20and%20Neural%20Radiance%20Fields%20%28NeRF%29%2C%20have%20made%20progress%20in%0Asimulating%20the%20physical%20properties%20of%20LiDAR%2C%20these%20methods%20have%20struggled%20to%0Aachieve%20satisfactory%20frame%20rates%20and%20rendering%20quality.%20To%20address%20these%0Alimitations%2C%20we%20present%20LiDAR-GS%2C%20the%20first%20LiDAR%20Gaussian%20Splatting%20method%2C%0Afor%20real-time%20high-fidelity%20re-simulation%20of%20LiDAR%20sensor%20scans%20in%20public%20urban%0Aroad%20scenes.%20The%20vanilla%20Gaussian%20Splatting%2C%20designed%20for%20camera%20models%2C%20cannot%0Abe%20directly%20applied%20to%20LiDAR%20re-simulation.%20To%20bridge%20the%20gap%20between%20passive%0Acamera%20and%20active%20LiDAR%2C%20our%20LiDAR-GS%20designs%20a%20differentiable%20laser%20beam%0Asplatting%2C%20grounded%20in%20the%20LiDAR%20range%20view%20model.%20This%20innovation%20allows%20for%0Aprecise%20surface%20splatting%20by%20projecting%20lasers%20onto%20micro%20cross-sections%2C%0Aeffectively%20eliminating%20artifacts%20associated%20with%20local%20affine%20approximations.%0AAdditionally%2C%20LiDAR-GS%20leverages%20Neural%20Gaussian%20Fields%2C%20which%20further%0Aintegrate%20view-dependent%20clues%2C%20to%20represent%20key%20LiDAR%20properties%20that%20are%0Ainfluenced%20by%20the%20incident%20angle%20and%20external%20factors.%20Combining%20these%0Apractices%20with%20some%20essential%20adaptations%2C%20e.g.%2C%20dynamic%20instances%0Adecomposition%2C%20our%20approach%20succeeds%20in%20simultaneously%20re-simulating%20depth%2C%0Aintensity%2C%20and%20ray-drop%20channels%2C%20achieving%20state-of-the-art%20results%20in%20both%0Arendering%20frame%20rate%20and%20quality%20on%20publically%20available%20large%20scene%20datasets.%0AOur%20source%20code%20will%20be%20made%20publicly%20available.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2410.05111v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DLiDAR-GS%253AReal-time%2520LiDAR%2520Re-Simulation%2520using%2520Gaussian%2520Splatting%26entry.906535625%3DQifeng%2520Chen%2520and%2520Sheng%2520Yang%2520and%2520Sicong%2520Du%2520and%2520Tao%2520Tang%2520and%2520Peng%2520Chen%2520and%2520Yuchi%2520Huo%26entry.1292438233%3D%2520%2520LiDAR%2520simulation%2520plays%2520a%2520crucial%2520role%2520in%2520closed-loop%2520simulation%2520for%250Aautonomous%2520driving.%2520Although%2520recent%2520advancements%252C%2520such%2520as%2520the%2520use%2520of%250Areconstructed%2520mesh%2520and%2520Neural%2520Radiance%2520Fields%2520%2528NeRF%2529%252C%2520have%2520made%2520progress%2520in%250Asimulating%2520the%2520physical%2520properties%2520of%2520LiDAR%252C%2520these%2520methods%2520have%2520struggled%2520to%250Aachieve%2520satisfactory%2520frame%2520rates%2520and%2520rendering%2520quality.%2520To%2520address%2520these%250Alimitations%252C%2520we%2520present%2520LiDAR-GS%252C%2520the%2520first%2520LiDAR%2520Gaussian%2520Splatting%2520method%252C%250Afor%2520real-time%2520high-fidelity%2520re-simulation%2520of%2520LiDAR%2520sensor%2520scans%2520in%2520public%2520urban%250Aroad%2520scenes.%2520The%2520vanilla%2520Gaussian%2520Splatting%252C%2520designed%2520for%2520camera%2520models%252C%2520cannot%250Abe%2520directly%2520applied%2520to%2520LiDAR%2520re-simulation.%2520To%2520bridge%2520the%2520gap%2520between%2520passive%250Acamera%2520and%2520active%2520LiDAR%252C%2520our%2520LiDAR-GS%2520designs%2520a%2520differentiable%2520laser%2520beam%250Asplatting%252C%2520grounded%2520in%2520the%2520LiDAR%2520range%2520view%2520model.%2520This%2520innovation%2520allows%2520for%250Aprecise%2520surface%2520splatting%2520by%2520projecting%2520lasers%2520onto%2520micro%2520cross-sections%252C%250Aeffectively%2520eliminating%2520artifacts%2520associated%2520with%2520local%2520affine%2520approximations.%250AAdditionally%252C%2520LiDAR-GS%2520leverages%2520Neural%2520Gaussian%2520Fields%252C%2520which%2520further%250Aintegrate%2520view-dependent%2520clues%252C%2520to%2520represent%2520key%2520LiDAR%2520properties%2520that%2520are%250Ainfluenced%2520by%2520the%2520incident%2520angle%2520and%2520external%2520factors.%2520Combining%2520these%250Apractices%2520with%2520some%2520essential%2520adaptations%252C%2520e.g.%252C%2520dynamic%2520instances%250Adecomposition%252C%2520our%2520approach%2520succeeds%2520in%2520simultaneously%2520re-simulating%2520depth%252C%250Aintensity%252C%2520and%2520ray-drop%2520channels%252C%2520achieving%2520state-of-the-art%2520results%2520in%2520both%250Arendering%2520frame%2520rate%2520and%2520quality%2520on%2520publically%2520available%2520large%2520scene%2520datasets.%250AOur%2520source%2520code%2520will%2520be%2520made%2520publicly%2520available.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2410.05111v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=LiDAR-GS%3AReal-time%20LiDAR%20Re-Simulation%20using%20Gaussian%20Splatting&entry.906535625=Qifeng%20Chen%20and%20Sheng%20Yang%20and%20Sicong%20Du%20and%20Tao%20Tang%20and%20Peng%20Chen%20and%20Yuchi%20Huo&entry.1292438233=%20%20LiDAR%20simulation%20plays%20a%20crucial%20role%20in%20closed-loop%20simulation%20for%0Aautonomous%20driving.%20Although%20recent%20advancements%2C%20such%20as%20the%20use%20of%0Areconstructed%20mesh%20and%20Neural%20Radiance%20Fields%20%28NeRF%29%2C%20have%20made%20progress%20in%0Asimulating%20the%20physical%20properties%20of%20LiDAR%2C%20these%20methods%20have%20struggled%20to%0Aachieve%20satisfactory%20frame%20rates%20and%20rendering%20quality.%20To%20address%20these%0Alimitations%2C%20we%20present%20LiDAR-GS%2C%20the%20first%20LiDAR%20Gaussian%20Splatting%20method%2C%0Afor%20real-time%20high-fidelity%20re-simulation%20of%20LiDAR%20sensor%20scans%20in%20public%20urban%0Aroad%20scenes.%20The%20vanilla%20Gaussian%20Splatting%2C%20designed%20for%20camera%20models%2C%20cannot%0Abe%20directly%20applied%20to%20LiDAR%20re-simulation.%20To%20bridge%20the%20gap%20between%20passive%0Acamera%20and%20active%20LiDAR%2C%20our%20LiDAR-GS%20designs%20a%20differentiable%20laser%20beam%0Asplatting%2C%20grounded%20in%20the%20LiDAR%20range%20view%20model.%20This%20innovation%20allows%20for%0Aprecise%20surface%20splatting%20by%20projecting%20lasers%20onto%20micro%20cross-sections%2C%0Aeffectively%20eliminating%20artifacts%20associated%20with%20local%20affine%20approximations.%0AAdditionally%2C%20LiDAR-GS%20leverages%20Neural%20Gaussian%20Fields%2C%20which%20further%0Aintegrate%20view-dependent%20clues%2C%20to%20represent%20key%20LiDAR%20properties%20that%20are%0Ainfluenced%20by%20the%20incident%20angle%20and%20external%20factors.%20Combining%20these%0Apractices%20with%20some%20essential%20adaptations%2C%20e.g.%2C%20dynamic%20instances%0Adecomposition%2C%20our%20approach%20succeeds%20in%20simultaneously%20re-simulating%20depth%2C%0Aintensity%2C%20and%20ray-drop%20channels%2C%20achieving%20state-of-the-art%20results%20in%20both%0Arendering%20frame%20rate%20and%20quality%20on%20publically%20available%20large%20scene%20datasets.%0AOur%20source%20code%20will%20be%20made%20publicly%20available.%0A&entry.1838667208=http%3A//arxiv.org/abs/2410.05111v1&entry.124074799=Read"},
{"title": "6DGS: Enhanced Direction-Aware Gaussian Splatting for Volumetric\n  Rendering", "author": "Zhongpai Gao and Benjamin Planche and Meng Zheng and Anwesa Choudhuri and Terrence Chen and Ziyan Wu", "abstract": "  Novel view synthesis has advanced significantly with the development of\nneural radiance fields (NeRF) and 3D Gaussian splatting (3DGS). However,\nachieving high quality without compromising real-time rendering remains\nchallenging, particularly for physically-based ray tracing with view-dependent\neffects. Recently, N-dimensional Gaussians (N-DG) introduced a 6D\nspatial-angular representation to better incorporate view-dependent effects,\nbut the Gaussian representation and control scheme are sub-optimal. In this\npaper, we revisit 6D Gaussians and introduce 6D Gaussian Splatting (6DGS),\nwhich enhances color and opacity representations and leverages the additional\ndirectional information in the 6D space for optimized Gaussian control. Our\napproach is fully compatible with the 3DGS framework and significantly improves\nreal-time radiance field rendering by better modeling view-dependent effects\nand fine details. Experiments demonstrate that 6DGS significantly outperforms\n3DGS and N-DG, achieving up to a 15.73 dB improvement in PSNR with a reduction\nof 66.5% Gaussian points compared to 3DGS.\n", "link": "http://arxiv.org/abs/2410.04974v1", "date": "2024-10-07", "relevancy": 3.3409, "topK": [{"title": "RTG-SLAM: Real-time 3D Reconstruction at Scale using Gaussian Splatting", "link": "http://arxiv.org/abs/2404.19706v1", "similarity": 0.6993}, {"title": "MiraGe: Editable 2D Images using Gaussian Splatting", "link": "http://arxiv.org/abs/2410.01521v1", "similarity": 0.6647}, {"title": "Dynamic Gaussians Mesh: Consistent Mesh Reconstruction from Monocular\n  Videos", "link": "http://arxiv.org/abs/2404.12379v2", "similarity": 0.6405}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%206DGS%3A%20Enhanced%20Direction-Aware%20Gaussian%20Splatting%20for%20Volumetric%0A%20%20Rendering&body=Title%3A%206DGS%3A%20Enhanced%20Direction-Aware%20Gaussian%20Splatting%20for%20Volumetric%0A%20%20Rendering%0AAuthor%3A%20Zhongpai%20Gao%20and%20Benjamin%20Planche%20and%20Meng%20Zheng%20and%20Anwesa%20Choudhuri%20and%20Terrence%20Chen%20and%20Ziyan%20Wu%0AAbstract%3A%20%20%20Novel%20view%20synthesis%20has%20advanced%20significantly%20with%20the%20development%20of%0Aneural%20radiance%20fields%20%28NeRF%29%20and%203D%20Gaussian%20splatting%20%283DGS%29.%20However%2C%0Aachieving%20high%20quality%20without%20compromising%20real-time%20rendering%20remains%0Achallenging%2C%20particularly%20for%20physically-based%20ray%20tracing%20with%20view-dependent%0Aeffects.%20Recently%2C%20N-dimensional%20Gaussians%20%28N-DG%29%20introduced%20a%206D%0Aspatial-angular%20representation%20to%20better%20incorporate%20view-dependent%20effects%2C%0Abut%20the%20Gaussian%20representation%20and%20control%20scheme%20are%20sub-optimal.%20In%20this%0Apaper%2C%20we%20revisit%206D%20Gaussians%20and%20introduce%206D%20Gaussian%20Splatting%20%286DGS%29%2C%0Awhich%20enhances%20color%20and%20opacity%20representations%20and%20leverages%20the%20additional%0Adirectional%20information%20in%20the%206D%20space%20for%20optimized%20Gaussian%20control.%20Our%0Aapproach%20is%20fully%20compatible%20with%20the%203DGS%20framework%20and%20significantly%20improves%0Areal-time%20radiance%20field%20rendering%20by%20better%20modeling%20view-dependent%20effects%0Aand%20fine%20details.%20Experiments%20demonstrate%20that%206DGS%20significantly%20outperforms%0A3DGS%20and%20N-DG%2C%20achieving%20up%20to%20a%2015.73%20dB%20improvement%20in%20PSNR%20with%20a%20reduction%0Aof%2066.5%25%20Gaussian%20points%20compared%20to%203DGS.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2410.04974v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3D6DGS%253A%2520Enhanced%2520Direction-Aware%2520Gaussian%2520Splatting%2520for%2520Volumetric%250A%2520%2520Rendering%26entry.906535625%3DZhongpai%2520Gao%2520and%2520Benjamin%2520Planche%2520and%2520Meng%2520Zheng%2520and%2520Anwesa%2520Choudhuri%2520and%2520Terrence%2520Chen%2520and%2520Ziyan%2520Wu%26entry.1292438233%3D%2520%2520Novel%2520view%2520synthesis%2520has%2520advanced%2520significantly%2520with%2520the%2520development%2520of%250Aneural%2520radiance%2520fields%2520%2528NeRF%2529%2520and%25203D%2520Gaussian%2520splatting%2520%25283DGS%2529.%2520However%252C%250Aachieving%2520high%2520quality%2520without%2520compromising%2520real-time%2520rendering%2520remains%250Achallenging%252C%2520particularly%2520for%2520physically-based%2520ray%2520tracing%2520with%2520view-dependent%250Aeffects.%2520Recently%252C%2520N-dimensional%2520Gaussians%2520%2528N-DG%2529%2520introduced%2520a%25206D%250Aspatial-angular%2520representation%2520to%2520better%2520incorporate%2520view-dependent%2520effects%252C%250Abut%2520the%2520Gaussian%2520representation%2520and%2520control%2520scheme%2520are%2520sub-optimal.%2520In%2520this%250Apaper%252C%2520we%2520revisit%25206D%2520Gaussians%2520and%2520introduce%25206D%2520Gaussian%2520Splatting%2520%25286DGS%2529%252C%250Awhich%2520enhances%2520color%2520and%2520opacity%2520representations%2520and%2520leverages%2520the%2520additional%250Adirectional%2520information%2520in%2520the%25206D%2520space%2520for%2520optimized%2520Gaussian%2520control.%2520Our%250Aapproach%2520is%2520fully%2520compatible%2520with%2520the%25203DGS%2520framework%2520and%2520significantly%2520improves%250Areal-time%2520radiance%2520field%2520rendering%2520by%2520better%2520modeling%2520view-dependent%2520effects%250Aand%2520fine%2520details.%2520Experiments%2520demonstrate%2520that%25206DGS%2520significantly%2520outperforms%250A3DGS%2520and%2520N-DG%252C%2520achieving%2520up%2520to%2520a%252015.73%2520dB%2520improvement%2520in%2520PSNR%2520with%2520a%2520reduction%250Aof%252066.5%2525%2520Gaussian%2520points%2520compared%2520to%25203DGS.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2410.04974v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=6DGS%3A%20Enhanced%20Direction-Aware%20Gaussian%20Splatting%20for%20Volumetric%0A%20%20Rendering&entry.906535625=Zhongpai%20Gao%20and%20Benjamin%20Planche%20and%20Meng%20Zheng%20and%20Anwesa%20Choudhuri%20and%20Terrence%20Chen%20and%20Ziyan%20Wu&entry.1292438233=%20%20Novel%20view%20synthesis%20has%20advanced%20significantly%20with%20the%20development%20of%0Aneural%20radiance%20fields%20%28NeRF%29%20and%203D%20Gaussian%20splatting%20%283DGS%29.%20However%2C%0Aachieving%20high%20quality%20without%20compromising%20real-time%20rendering%20remains%0Achallenging%2C%20particularly%20for%20physically-based%20ray%20tracing%20with%20view-dependent%0Aeffects.%20Recently%2C%20N-dimensional%20Gaussians%20%28N-DG%29%20introduced%20a%206D%0Aspatial-angular%20representation%20to%20better%20incorporate%20view-dependent%20effects%2C%0Abut%20the%20Gaussian%20representation%20and%20control%20scheme%20are%20sub-optimal.%20In%20this%0Apaper%2C%20we%20revisit%206D%20Gaussians%20and%20introduce%206D%20Gaussian%20Splatting%20%286DGS%29%2C%0Awhich%20enhances%20color%20and%20opacity%20representations%20and%20leverages%20the%20additional%0Adirectional%20information%20in%20the%206D%20space%20for%20optimized%20Gaussian%20control.%20Our%0Aapproach%20is%20fully%20compatible%20with%20the%203DGS%20framework%20and%20significantly%20improves%0Areal-time%20radiance%20field%20rendering%20by%20better%20modeling%20view-dependent%20effects%0Aand%20fine%20details.%20Experiments%20demonstrate%20that%206DGS%20significantly%20outperforms%0A3DGS%20and%20N-DG%2C%20achieving%20up%20to%20a%2015.73%20dB%20improvement%20in%20PSNR%20with%20a%20reduction%0Aof%2066.5%25%20Gaussian%20points%20compared%20to%203DGS.%0A&entry.1838667208=http%3A//arxiv.org/abs/2410.04974v1&entry.124074799=Read"},
{"title": "DreamSat: Towards a General 3D Model for Novel View Synthesis of Space\n  Objects", "author": "Nidhi Mathihalli and Audrey Wei and Giovanni Lavezzi and Peng Mun Siew and Victor Rodriguez-Fernandez and Hodei Urrutxua and Richard Linares", "abstract": "  Novel view synthesis (NVS) enables to generate new images of a scene or\nconvert a set of 2D images into a comprehensive 3D model. In the context of\nSpace Domain Awareness, since space is becoming increasingly congested, NVS can\naccurately map space objects and debris, improving the safety and efficiency of\nspace operations. Similarly, in Rendezvous and Proximity Operations missions,\n3D models can provide details about a target object's shape, size, and\norientation, allowing for better planning and prediction of the target's\nbehavior. In this work, we explore the generalization abilities of these\nreconstruction techniques, aiming to avoid the necessity of retraining for each\nnew scene, by presenting a novel approach to 3D spacecraft reconstruction from\nsingle-view images, DreamSat, by fine-tuning the Zero123 XL, a state-of-the-art\nsingle-view reconstruction model, on a high-quality dataset of 190 high-quality\nspacecraft models and integrating it into the DreamGaussian framework. We\ndemonstrate consistent improvements in reconstruction quality across multiple\nmetrics, including Contrastive Language-Image Pretraining (CLIP) score\n(+0.33%), Peak Signal-to-Noise Ratio (PSNR) (+2.53%), Structural Similarity\nIndex (SSIM) (+2.38%), and Learned Perceptual Image Patch Similarity (LPIPS)\n(+0.16%) on a test set of 30 previously unseen spacecraft images. Our method\naddresses the lack of domain-specific 3D reconstruction tools in the space\nindustry by leveraging state-of-the-art diffusion models and 3D Gaussian\nsplatting techniques. This approach maintains the efficiency of the\nDreamGaussian framework while enhancing the accuracy and detail of spacecraft\nreconstructions. The code for this work can be accessed on GitHub\n(https://github.com/ARCLab-MIT/space-nvs).\n", "link": "http://arxiv.org/abs/2410.05097v1", "date": "2024-10-07", "relevancy": 3.298, "topK": [{"title": "RTG-SLAM: Real-time 3D Reconstruction at Scale using Gaussian Splatting", "link": "http://arxiv.org/abs/2404.19706v1", "similarity": 0.6599}, {"title": "CAT3D: Create Anything in 3D with Multi-View Diffusion Models", "link": "http://arxiv.org/abs/2405.10314v1", "similarity": 0.6595}, {"title": "CAT3D: Create Anything in 3D with Multi-View Diffusion Models", "link": "http://arxiv.org/abs/2405.10314v1", "similarity": 0.6595}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20DreamSat%3A%20Towards%20a%20General%203D%20Model%20for%20Novel%20View%20Synthesis%20of%20Space%0A%20%20Objects&body=Title%3A%20DreamSat%3A%20Towards%20a%20General%203D%20Model%20for%20Novel%20View%20Synthesis%20of%20Space%0A%20%20Objects%0AAuthor%3A%20Nidhi%20Mathihalli%20and%20Audrey%20Wei%20and%20Giovanni%20Lavezzi%20and%20Peng%20Mun%20Siew%20and%20Victor%20Rodriguez-Fernandez%20and%20Hodei%20Urrutxua%20and%20Richard%20Linares%0AAbstract%3A%20%20%20Novel%20view%20synthesis%20%28NVS%29%20enables%20to%20generate%20new%20images%20of%20a%20scene%20or%0Aconvert%20a%20set%20of%202D%20images%20into%20a%20comprehensive%203D%20model.%20In%20the%20context%20of%0ASpace%20Domain%20Awareness%2C%20since%20space%20is%20becoming%20increasingly%20congested%2C%20NVS%20can%0Aaccurately%20map%20space%20objects%20and%20debris%2C%20improving%20the%20safety%20and%20efficiency%20of%0Aspace%20operations.%20Similarly%2C%20in%20Rendezvous%20and%20Proximity%20Operations%20missions%2C%0A3D%20models%20can%20provide%20details%20about%20a%20target%20object%27s%20shape%2C%20size%2C%20and%0Aorientation%2C%20allowing%20for%20better%20planning%20and%20prediction%20of%20the%20target%27s%0Abehavior.%20In%20this%20work%2C%20we%20explore%20the%20generalization%20abilities%20of%20these%0Areconstruction%20techniques%2C%20aiming%20to%20avoid%20the%20necessity%20of%20retraining%20for%20each%0Anew%20scene%2C%20by%20presenting%20a%20novel%20approach%20to%203D%20spacecraft%20reconstruction%20from%0Asingle-view%20images%2C%20DreamSat%2C%20by%20fine-tuning%20the%20Zero123%20XL%2C%20a%20state-of-the-art%0Asingle-view%20reconstruction%20model%2C%20on%20a%20high-quality%20dataset%20of%20190%20high-quality%0Aspacecraft%20models%20and%20integrating%20it%20into%20the%20DreamGaussian%20framework.%20We%0Ademonstrate%20consistent%20improvements%20in%20reconstruction%20quality%20across%20multiple%0Ametrics%2C%20including%20Contrastive%20Language-Image%20Pretraining%20%28CLIP%29%20score%0A%28%2B0.33%25%29%2C%20Peak%20Signal-to-Noise%20Ratio%20%28PSNR%29%20%28%2B2.53%25%29%2C%20Structural%20Similarity%0AIndex%20%28SSIM%29%20%28%2B2.38%25%29%2C%20and%20Learned%20Perceptual%20Image%20Patch%20Similarity%20%28LPIPS%29%0A%28%2B0.16%25%29%20on%20a%20test%20set%20of%2030%20previously%20unseen%20spacecraft%20images.%20Our%20method%0Aaddresses%20the%20lack%20of%20domain-specific%203D%20reconstruction%20tools%20in%20the%20space%0Aindustry%20by%20leveraging%20state-of-the-art%20diffusion%20models%20and%203D%20Gaussian%0Asplatting%20techniques.%20This%20approach%20maintains%20the%20efficiency%20of%20the%0ADreamGaussian%20framework%20while%20enhancing%20the%20accuracy%20and%20detail%20of%20spacecraft%0Areconstructions.%20The%20code%20for%20this%20work%20can%20be%20accessed%20on%20GitHub%0A%28https%3A//github.com/ARCLab-MIT/space-nvs%29.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2410.05097v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DDreamSat%253A%2520Towards%2520a%2520General%25203D%2520Model%2520for%2520Novel%2520View%2520Synthesis%2520of%2520Space%250A%2520%2520Objects%26entry.906535625%3DNidhi%2520Mathihalli%2520and%2520Audrey%2520Wei%2520and%2520Giovanni%2520Lavezzi%2520and%2520Peng%2520Mun%2520Siew%2520and%2520Victor%2520Rodriguez-Fernandez%2520and%2520Hodei%2520Urrutxua%2520and%2520Richard%2520Linares%26entry.1292438233%3D%2520%2520Novel%2520view%2520synthesis%2520%2528NVS%2529%2520enables%2520to%2520generate%2520new%2520images%2520of%2520a%2520scene%2520or%250Aconvert%2520a%2520set%2520of%25202D%2520images%2520into%2520a%2520comprehensive%25203D%2520model.%2520In%2520the%2520context%2520of%250ASpace%2520Domain%2520Awareness%252C%2520since%2520space%2520is%2520becoming%2520increasingly%2520congested%252C%2520NVS%2520can%250Aaccurately%2520map%2520space%2520objects%2520and%2520debris%252C%2520improving%2520the%2520safety%2520and%2520efficiency%2520of%250Aspace%2520operations.%2520Similarly%252C%2520in%2520Rendezvous%2520and%2520Proximity%2520Operations%2520missions%252C%250A3D%2520models%2520can%2520provide%2520details%2520about%2520a%2520target%2520object%2527s%2520shape%252C%2520size%252C%2520and%250Aorientation%252C%2520allowing%2520for%2520better%2520planning%2520and%2520prediction%2520of%2520the%2520target%2527s%250Abehavior.%2520In%2520this%2520work%252C%2520we%2520explore%2520the%2520generalization%2520abilities%2520of%2520these%250Areconstruction%2520techniques%252C%2520aiming%2520to%2520avoid%2520the%2520necessity%2520of%2520retraining%2520for%2520each%250Anew%2520scene%252C%2520by%2520presenting%2520a%2520novel%2520approach%2520to%25203D%2520spacecraft%2520reconstruction%2520from%250Asingle-view%2520images%252C%2520DreamSat%252C%2520by%2520fine-tuning%2520the%2520Zero123%2520XL%252C%2520a%2520state-of-the-art%250Asingle-view%2520reconstruction%2520model%252C%2520on%2520a%2520high-quality%2520dataset%2520of%2520190%2520high-quality%250Aspacecraft%2520models%2520and%2520integrating%2520it%2520into%2520the%2520DreamGaussian%2520framework.%2520We%250Ademonstrate%2520consistent%2520improvements%2520in%2520reconstruction%2520quality%2520across%2520multiple%250Ametrics%252C%2520including%2520Contrastive%2520Language-Image%2520Pretraining%2520%2528CLIP%2529%2520score%250A%2528%252B0.33%2525%2529%252C%2520Peak%2520Signal-to-Noise%2520Ratio%2520%2528PSNR%2529%2520%2528%252B2.53%2525%2529%252C%2520Structural%2520Similarity%250AIndex%2520%2528SSIM%2529%2520%2528%252B2.38%2525%2529%252C%2520and%2520Learned%2520Perceptual%2520Image%2520Patch%2520Similarity%2520%2528LPIPS%2529%250A%2528%252B0.16%2525%2529%2520on%2520a%2520test%2520set%2520of%252030%2520previously%2520unseen%2520spacecraft%2520images.%2520Our%2520method%250Aaddresses%2520the%2520lack%2520of%2520domain-specific%25203D%2520reconstruction%2520tools%2520in%2520the%2520space%250Aindustry%2520by%2520leveraging%2520state-of-the-art%2520diffusion%2520models%2520and%25203D%2520Gaussian%250Asplatting%2520techniques.%2520This%2520approach%2520maintains%2520the%2520efficiency%2520of%2520the%250ADreamGaussian%2520framework%2520while%2520enhancing%2520the%2520accuracy%2520and%2520detail%2520of%2520spacecraft%250Areconstructions.%2520The%2520code%2520for%2520this%2520work%2520can%2520be%2520accessed%2520on%2520GitHub%250A%2528https%253A//github.com/ARCLab-MIT/space-nvs%2529.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2410.05097v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=DreamSat%3A%20Towards%20a%20General%203D%20Model%20for%20Novel%20View%20Synthesis%20of%20Space%0A%20%20Objects&entry.906535625=Nidhi%20Mathihalli%20and%20Audrey%20Wei%20and%20Giovanni%20Lavezzi%20and%20Peng%20Mun%20Siew%20and%20Victor%20Rodriguez-Fernandez%20and%20Hodei%20Urrutxua%20and%20Richard%20Linares&entry.1292438233=%20%20Novel%20view%20synthesis%20%28NVS%29%20enables%20to%20generate%20new%20images%20of%20a%20scene%20or%0Aconvert%20a%20set%20of%202D%20images%20into%20a%20comprehensive%203D%20model.%20In%20the%20context%20of%0ASpace%20Domain%20Awareness%2C%20since%20space%20is%20becoming%20increasingly%20congested%2C%20NVS%20can%0Aaccurately%20map%20space%20objects%20and%20debris%2C%20improving%20the%20safety%20and%20efficiency%20of%0Aspace%20operations.%20Similarly%2C%20in%20Rendezvous%20and%20Proximity%20Operations%20missions%2C%0A3D%20models%20can%20provide%20details%20about%20a%20target%20object%27s%20shape%2C%20size%2C%20and%0Aorientation%2C%20allowing%20for%20better%20planning%20and%20prediction%20of%20the%20target%27s%0Abehavior.%20In%20this%20work%2C%20we%20explore%20the%20generalization%20abilities%20of%20these%0Areconstruction%20techniques%2C%20aiming%20to%20avoid%20the%20necessity%20of%20retraining%20for%20each%0Anew%20scene%2C%20by%20presenting%20a%20novel%20approach%20to%203D%20spacecraft%20reconstruction%20from%0Asingle-view%20images%2C%20DreamSat%2C%20by%20fine-tuning%20the%20Zero123%20XL%2C%20a%20state-of-the-art%0Asingle-view%20reconstruction%20model%2C%20on%20a%20high-quality%20dataset%20of%20190%20high-quality%0Aspacecraft%20models%20and%20integrating%20it%20into%20the%20DreamGaussian%20framework.%20We%0Ademonstrate%20consistent%20improvements%20in%20reconstruction%20quality%20across%20multiple%0Ametrics%2C%20including%20Contrastive%20Language-Image%20Pretraining%20%28CLIP%29%20score%0A%28%2B0.33%25%29%2C%20Peak%20Signal-to-Noise%20Ratio%20%28PSNR%29%20%28%2B2.53%25%29%2C%20Structural%20Similarity%0AIndex%20%28SSIM%29%20%28%2B2.38%25%29%2C%20and%20Learned%20Perceptual%20Image%20Patch%20Similarity%20%28LPIPS%29%0A%28%2B0.16%25%29%20on%20a%20test%20set%20of%2030%20previously%20unseen%20spacecraft%20images.%20Our%20method%0Aaddresses%20the%20lack%20of%20domain-specific%203D%20reconstruction%20tools%20in%20the%20space%0Aindustry%20by%20leveraging%20state-of-the-art%20diffusion%20models%20and%203D%20Gaussian%0Asplatting%20techniques.%20This%20approach%20maintains%20the%20efficiency%20of%20the%0ADreamGaussian%20framework%20while%20enhancing%20the%20accuracy%20and%20detail%20of%20spacecraft%0Areconstructions.%20The%20code%20for%20this%20work%20can%20be%20accessed%20on%20GitHub%0A%28https%3A//github.com/ARCLab-MIT/space-nvs%29.%0A&entry.1838667208=http%3A//arxiv.org/abs/2410.05097v1&entry.124074799=Read"},
{"title": "PhotoReg: Photometrically Registering 3D Gaussian Splatting Models", "author": "Ziwen Yuan and Tianyi Zhang and Matthew Johnson-Roberson and Weiming Zhi", "abstract": "  Building accurate representations of the environment is critical for\nintelligent robots to make decisions during deployment. Advances in\nphotorealistic environment models have enabled robots to develop\nhyper-realistic reconstructions, which can be used to generate images that are\nintuitive for human inspection. In particular, the recently introduced\n\\ac{3DGS}, which describes the scene with up to millions of primitive\nellipsoids, can be rendered in real time. \\ac{3DGS} has rapidly gained\nprominence. However, a critical unsolved problem persists: how can we fuse\nmultiple \\ac{3DGS} into a single coherent model? Solving this problem will\nenable robot teams to jointly build \\ac{3DGS} models of their surroundings. A\nkey insight of this work is to leverage the {duality} between photorealistic\nreconstructions, which render realistic 2D images from 3D structure, and\n\\emph{3D foundation models}, which predict 3D structure from image pairs. To\nthis end, we develop PhotoReg, a framework to register multiple photorealistic\n\\ac{3DGS} models with 3D foundation models. As \\ac{3DGS} models are generally\nbuilt from monocular camera images, they have \\emph{arbitrary scale}. To\nresolve this, PhotoReg actively enforces scale consistency among the different\n\\ac{3DGS} models by considering depth estimates within these models. Then, the\nalignment is iteratively refined with fine-grained photometric losses to\nproduce high-quality fused \\ac{3DGS} models. We rigorously evaluate PhotoReg on\nboth standard benchmark datasets and our custom-collected datasets, including\nwith two quadruped robots. The code is released at\n\\url{ziweny11.github.io/photoreg}.\n", "link": "http://arxiv.org/abs/2410.05044v1", "date": "2024-10-07", "relevancy": 3.2933, "topK": [{"title": "RTG-SLAM: Real-time 3D Reconstruction at Scale using Gaussian Splatting", "link": "http://arxiv.org/abs/2404.19706v1", "similarity": 0.6894}, {"title": "GST: Precise 3D Human Body from a Single Image with Gaussian Splatting\n  Transformers", "link": "http://arxiv.org/abs/2409.04196v1", "similarity": 0.6744}, {"title": "Dynamic Gaussians Mesh: Consistent Mesh Reconstruction from Monocular\n  Videos", "link": "http://arxiv.org/abs/2404.12379v2", "similarity": 0.6122}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20PhotoReg%3A%20Photometrically%20Registering%203D%20Gaussian%20Splatting%20Models&body=Title%3A%20PhotoReg%3A%20Photometrically%20Registering%203D%20Gaussian%20Splatting%20Models%0AAuthor%3A%20Ziwen%20Yuan%20and%20Tianyi%20Zhang%20and%20Matthew%20Johnson-Roberson%20and%20Weiming%20Zhi%0AAbstract%3A%20%20%20Building%20accurate%20representations%20of%20the%20environment%20is%20critical%20for%0Aintelligent%20robots%20to%20make%20decisions%20during%20deployment.%20Advances%20in%0Aphotorealistic%20environment%20models%20have%20enabled%20robots%20to%20develop%0Ahyper-realistic%20reconstructions%2C%20which%20can%20be%20used%20to%20generate%20images%20that%20are%0Aintuitive%20for%20human%20inspection.%20In%20particular%2C%20the%20recently%20introduced%0A%5Cac%7B3DGS%7D%2C%20which%20describes%20the%20scene%20with%20up%20to%20millions%20of%20primitive%0Aellipsoids%2C%20can%20be%20rendered%20in%20real%20time.%20%5Cac%7B3DGS%7D%20has%20rapidly%20gained%0Aprominence.%20However%2C%20a%20critical%20unsolved%20problem%20persists%3A%20how%20can%20we%20fuse%0Amultiple%20%5Cac%7B3DGS%7D%20into%20a%20single%20coherent%20model%3F%20Solving%20this%20problem%20will%0Aenable%20robot%20teams%20to%20jointly%20build%20%5Cac%7B3DGS%7D%20models%20of%20their%20surroundings.%20A%0Akey%20insight%20of%20this%20work%20is%20to%20leverage%20the%20%7Bduality%7D%20between%20photorealistic%0Areconstructions%2C%20which%20render%20realistic%202D%20images%20from%203D%20structure%2C%20and%0A%5Cemph%7B3D%20foundation%20models%7D%2C%20which%20predict%203D%20structure%20from%20image%20pairs.%20To%0Athis%20end%2C%20we%20develop%20PhotoReg%2C%20a%20framework%20to%20register%20multiple%20photorealistic%0A%5Cac%7B3DGS%7D%20models%20with%203D%20foundation%20models.%20As%20%5Cac%7B3DGS%7D%20models%20are%20generally%0Abuilt%20from%20monocular%20camera%20images%2C%20they%20have%20%5Cemph%7Barbitrary%20scale%7D.%20To%0Aresolve%20this%2C%20PhotoReg%20actively%20enforces%20scale%20consistency%20among%20the%20different%0A%5Cac%7B3DGS%7D%20models%20by%20considering%20depth%20estimates%20within%20these%20models.%20Then%2C%20the%0Aalignment%20is%20iteratively%20refined%20with%20fine-grained%20photometric%20losses%20to%0Aproduce%20high-quality%20fused%20%5Cac%7B3DGS%7D%20models.%20We%20rigorously%20evaluate%20PhotoReg%20on%0Aboth%20standard%20benchmark%20datasets%20and%20our%20custom-collected%20datasets%2C%20including%0Awith%20two%20quadruped%20robots.%20The%20code%20is%20released%20at%0A%5Curl%7Bziweny11.github.io/photoreg%7D.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2410.05044v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DPhotoReg%253A%2520Photometrically%2520Registering%25203D%2520Gaussian%2520Splatting%2520Models%26entry.906535625%3DZiwen%2520Yuan%2520and%2520Tianyi%2520Zhang%2520and%2520Matthew%2520Johnson-Roberson%2520and%2520Weiming%2520Zhi%26entry.1292438233%3D%2520%2520Building%2520accurate%2520representations%2520of%2520the%2520environment%2520is%2520critical%2520for%250Aintelligent%2520robots%2520to%2520make%2520decisions%2520during%2520deployment.%2520Advances%2520in%250Aphotorealistic%2520environment%2520models%2520have%2520enabled%2520robots%2520to%2520develop%250Ahyper-realistic%2520reconstructions%252C%2520which%2520can%2520be%2520used%2520to%2520generate%2520images%2520that%2520are%250Aintuitive%2520for%2520human%2520inspection.%2520In%2520particular%252C%2520the%2520recently%2520introduced%250A%255Cac%257B3DGS%257D%252C%2520which%2520describes%2520the%2520scene%2520with%2520up%2520to%2520millions%2520of%2520primitive%250Aellipsoids%252C%2520can%2520be%2520rendered%2520in%2520real%2520time.%2520%255Cac%257B3DGS%257D%2520has%2520rapidly%2520gained%250Aprominence.%2520However%252C%2520a%2520critical%2520unsolved%2520problem%2520persists%253A%2520how%2520can%2520we%2520fuse%250Amultiple%2520%255Cac%257B3DGS%257D%2520into%2520a%2520single%2520coherent%2520model%253F%2520Solving%2520this%2520problem%2520will%250Aenable%2520robot%2520teams%2520to%2520jointly%2520build%2520%255Cac%257B3DGS%257D%2520models%2520of%2520their%2520surroundings.%2520A%250Akey%2520insight%2520of%2520this%2520work%2520is%2520to%2520leverage%2520the%2520%257Bduality%257D%2520between%2520photorealistic%250Areconstructions%252C%2520which%2520render%2520realistic%25202D%2520images%2520from%25203D%2520structure%252C%2520and%250A%255Cemph%257B3D%2520foundation%2520models%257D%252C%2520which%2520predict%25203D%2520structure%2520from%2520image%2520pairs.%2520To%250Athis%2520end%252C%2520we%2520develop%2520PhotoReg%252C%2520a%2520framework%2520to%2520register%2520multiple%2520photorealistic%250A%255Cac%257B3DGS%257D%2520models%2520with%25203D%2520foundation%2520models.%2520As%2520%255Cac%257B3DGS%257D%2520models%2520are%2520generally%250Abuilt%2520from%2520monocular%2520camera%2520images%252C%2520they%2520have%2520%255Cemph%257Barbitrary%2520scale%257D.%2520To%250Aresolve%2520this%252C%2520PhotoReg%2520actively%2520enforces%2520scale%2520consistency%2520among%2520the%2520different%250A%255Cac%257B3DGS%257D%2520models%2520by%2520considering%2520depth%2520estimates%2520within%2520these%2520models.%2520Then%252C%2520the%250Aalignment%2520is%2520iteratively%2520refined%2520with%2520fine-grained%2520photometric%2520losses%2520to%250Aproduce%2520high-quality%2520fused%2520%255Cac%257B3DGS%257D%2520models.%2520We%2520rigorously%2520evaluate%2520PhotoReg%2520on%250Aboth%2520standard%2520benchmark%2520datasets%2520and%2520our%2520custom-collected%2520datasets%252C%2520including%250Awith%2520two%2520quadruped%2520robots.%2520The%2520code%2520is%2520released%2520at%250A%255Curl%257Bziweny11.github.io/photoreg%257D.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2410.05044v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=PhotoReg%3A%20Photometrically%20Registering%203D%20Gaussian%20Splatting%20Models&entry.906535625=Ziwen%20Yuan%20and%20Tianyi%20Zhang%20and%20Matthew%20Johnson-Roberson%20and%20Weiming%20Zhi&entry.1292438233=%20%20Building%20accurate%20representations%20of%20the%20environment%20is%20critical%20for%0Aintelligent%20robots%20to%20make%20decisions%20during%20deployment.%20Advances%20in%0Aphotorealistic%20environment%20models%20have%20enabled%20robots%20to%20develop%0Ahyper-realistic%20reconstructions%2C%20which%20can%20be%20used%20to%20generate%20images%20that%20are%0Aintuitive%20for%20human%20inspection.%20In%20particular%2C%20the%20recently%20introduced%0A%5Cac%7B3DGS%7D%2C%20which%20describes%20the%20scene%20with%20up%20to%20millions%20of%20primitive%0Aellipsoids%2C%20can%20be%20rendered%20in%20real%20time.%20%5Cac%7B3DGS%7D%20has%20rapidly%20gained%0Aprominence.%20However%2C%20a%20critical%20unsolved%20problem%20persists%3A%20how%20can%20we%20fuse%0Amultiple%20%5Cac%7B3DGS%7D%20into%20a%20single%20coherent%20model%3F%20Solving%20this%20problem%20will%0Aenable%20robot%20teams%20to%20jointly%20build%20%5Cac%7B3DGS%7D%20models%20of%20their%20surroundings.%20A%0Akey%20insight%20of%20this%20work%20is%20to%20leverage%20the%20%7Bduality%7D%20between%20photorealistic%0Areconstructions%2C%20which%20render%20realistic%202D%20images%20from%203D%20structure%2C%20and%0A%5Cemph%7B3D%20foundation%20models%7D%2C%20which%20predict%203D%20structure%20from%20image%20pairs.%20To%0Athis%20end%2C%20we%20develop%20PhotoReg%2C%20a%20framework%20to%20register%20multiple%20photorealistic%0A%5Cac%7B3DGS%7D%20models%20with%203D%20foundation%20models.%20As%20%5Cac%7B3DGS%7D%20models%20are%20generally%0Abuilt%20from%20monocular%20camera%20images%2C%20they%20have%20%5Cemph%7Barbitrary%20scale%7D.%20To%0Aresolve%20this%2C%20PhotoReg%20actively%20enforces%20scale%20consistency%20among%20the%20different%0A%5Cac%7B3DGS%7D%20models%20by%20considering%20depth%20estimates%20within%20these%20models.%20Then%2C%20the%0Aalignment%20is%20iteratively%20refined%20with%20fine-grained%20photometric%20losses%20to%0Aproduce%20high-quality%20fused%20%5Cac%7B3DGS%7D%20models.%20We%20rigorously%20evaluate%20PhotoReg%20on%0Aboth%20standard%20benchmark%20datasets%20and%20our%20custom-collected%20datasets%2C%20including%0Awith%20two%20quadruped%20robots.%20The%20code%20is%20released%20at%0A%5Curl%7Bziweny11.github.io/photoreg%7D.%0A&entry.1838667208=http%3A//arxiv.org/abs/2410.05044v1&entry.124074799=Read"},
{"title": "GS-VTON: Controllable 3D Virtual Try-on with Gaussian Splatting", "author": "Yukang Cao and Masoud Hadi and Liang Pan and Ziwei Liu", "abstract": "  Diffusion-based 2D virtual try-on (VTON) techniques have recently\ndemonstrated strong performance, while the development of 3D VTON has largely\nlagged behind. Despite recent advances in text-guided 3D scene editing,\nintegrating 2D VTON into these pipelines to achieve vivid 3D VTON remains\nchallenging. The reasons are twofold. First, text prompts cannot provide\nsufficient details in describing clothing. Second, 2D VTON results generated\nfrom different viewpoints of the same 3D scene lack coherence and spatial\nrelationships, hence frequently leading to appearance inconsistencies and\ngeometric distortions. To resolve these problems, we introduce an\nimage-prompted 3D VTON method (dubbed GS-VTON) which, by leveraging 3D Gaussian\nSplatting (3DGS) as the 3D representation, enables the transfer of pre-trained\nknowledge from 2D VTON models to 3D while improving cross-view consistency. (1)\nSpecifically, we propose a personalized diffusion model that utilizes low-rank\nadaptation (LoRA) fine-tuning to incorporate personalized information into\npre-trained 2D VTON models. To achieve effective LoRA training, we introduce a\nreference-driven image editing approach that enables the simultaneous editing\nof multi-view images while ensuring consistency. (2) Furthermore, we propose a\npersona-aware 3DGS editing framework to facilitate effective editing while\nmaintaining consistent cross-view appearance and high-quality 3D geometry. (3)\nAdditionally, we have established a new 3D VTON benchmark, 3D-VTONBench, which\nfacilitates comprehensive qualitative and quantitative 3D VTON evaluations.\nThrough extensive experiments and comparative analyses with existing methods,\nthe proposed \\OM has demonstrated superior fidelity and advanced editing\ncapabilities, affirming its effectiveness for 3D VTON.\n", "link": "http://arxiv.org/abs/2410.05259v1", "date": "2024-10-07", "relevancy": 3.1948, "topK": [{"title": "GST: Precise 3D Human Body from a Single Image with Gaussian Splatting\n  Transformers", "link": "http://arxiv.org/abs/2409.04196v1", "similarity": 0.6673}, {"title": "RTG-SLAM: Real-time 3D Reconstruction at Scale using Gaussian Splatting", "link": "http://arxiv.org/abs/2404.19706v1", "similarity": 0.6298}, {"title": "CAT3D: Create Anything in 3D with Multi-View Diffusion Models", "link": "http://arxiv.org/abs/2405.10314v1", "similarity": 0.6198}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20GS-VTON%3A%20Controllable%203D%20Virtual%20Try-on%20with%20Gaussian%20Splatting&body=Title%3A%20GS-VTON%3A%20Controllable%203D%20Virtual%20Try-on%20with%20Gaussian%20Splatting%0AAuthor%3A%20Yukang%20Cao%20and%20Masoud%20Hadi%20and%20Liang%20Pan%20and%20Ziwei%20Liu%0AAbstract%3A%20%20%20Diffusion-based%202D%20virtual%20try-on%20%28VTON%29%20techniques%20have%20recently%0Ademonstrated%20strong%20performance%2C%20while%20the%20development%20of%203D%20VTON%20has%20largely%0Alagged%20behind.%20Despite%20recent%20advances%20in%20text-guided%203D%20scene%20editing%2C%0Aintegrating%202D%20VTON%20into%20these%20pipelines%20to%20achieve%20vivid%203D%20VTON%20remains%0Achallenging.%20The%20reasons%20are%20twofold.%20First%2C%20text%20prompts%20cannot%20provide%0Asufficient%20details%20in%20describing%20clothing.%20Second%2C%202D%20VTON%20results%20generated%0Afrom%20different%20viewpoints%20of%20the%20same%203D%20scene%20lack%20coherence%20and%20spatial%0Arelationships%2C%20hence%20frequently%20leading%20to%20appearance%20inconsistencies%20and%0Ageometric%20distortions.%20To%20resolve%20these%20problems%2C%20we%20introduce%20an%0Aimage-prompted%203D%20VTON%20method%20%28dubbed%20GS-VTON%29%20which%2C%20by%20leveraging%203D%20Gaussian%0ASplatting%20%283DGS%29%20as%20the%203D%20representation%2C%20enables%20the%20transfer%20of%20pre-trained%0Aknowledge%20from%202D%20VTON%20models%20to%203D%20while%20improving%20cross-view%20consistency.%20%281%29%0ASpecifically%2C%20we%20propose%20a%20personalized%20diffusion%20model%20that%20utilizes%20low-rank%0Aadaptation%20%28LoRA%29%20fine-tuning%20to%20incorporate%20personalized%20information%20into%0Apre-trained%202D%20VTON%20models.%20To%20achieve%20effective%20LoRA%20training%2C%20we%20introduce%20a%0Areference-driven%20image%20editing%20approach%20that%20enables%20the%20simultaneous%20editing%0Aof%20multi-view%20images%20while%20ensuring%20consistency.%20%282%29%20Furthermore%2C%20we%20propose%20a%0Apersona-aware%203DGS%20editing%20framework%20to%20facilitate%20effective%20editing%20while%0Amaintaining%20consistent%20cross-view%20appearance%20and%20high-quality%203D%20geometry.%20%283%29%0AAdditionally%2C%20we%20have%20established%20a%20new%203D%20VTON%20benchmark%2C%203D-VTONBench%2C%20which%0Afacilitates%20comprehensive%20qualitative%20and%20quantitative%203D%20VTON%20evaluations.%0AThrough%20extensive%20experiments%20and%20comparative%20analyses%20with%20existing%20methods%2C%0Athe%20proposed%20%5COM%20has%20demonstrated%20superior%20fidelity%20and%20advanced%20editing%0Acapabilities%2C%20affirming%20its%20effectiveness%20for%203D%20VTON.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2410.05259v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DGS-VTON%253A%2520Controllable%25203D%2520Virtual%2520Try-on%2520with%2520Gaussian%2520Splatting%26entry.906535625%3DYukang%2520Cao%2520and%2520Masoud%2520Hadi%2520and%2520Liang%2520Pan%2520and%2520Ziwei%2520Liu%26entry.1292438233%3D%2520%2520Diffusion-based%25202D%2520virtual%2520try-on%2520%2528VTON%2529%2520techniques%2520have%2520recently%250Ademonstrated%2520strong%2520performance%252C%2520while%2520the%2520development%2520of%25203D%2520VTON%2520has%2520largely%250Alagged%2520behind.%2520Despite%2520recent%2520advances%2520in%2520text-guided%25203D%2520scene%2520editing%252C%250Aintegrating%25202D%2520VTON%2520into%2520these%2520pipelines%2520to%2520achieve%2520vivid%25203D%2520VTON%2520remains%250Achallenging.%2520The%2520reasons%2520are%2520twofold.%2520First%252C%2520text%2520prompts%2520cannot%2520provide%250Asufficient%2520details%2520in%2520describing%2520clothing.%2520Second%252C%25202D%2520VTON%2520results%2520generated%250Afrom%2520different%2520viewpoints%2520of%2520the%2520same%25203D%2520scene%2520lack%2520coherence%2520and%2520spatial%250Arelationships%252C%2520hence%2520frequently%2520leading%2520to%2520appearance%2520inconsistencies%2520and%250Ageometric%2520distortions.%2520To%2520resolve%2520these%2520problems%252C%2520we%2520introduce%2520an%250Aimage-prompted%25203D%2520VTON%2520method%2520%2528dubbed%2520GS-VTON%2529%2520which%252C%2520by%2520leveraging%25203D%2520Gaussian%250ASplatting%2520%25283DGS%2529%2520as%2520the%25203D%2520representation%252C%2520enables%2520the%2520transfer%2520of%2520pre-trained%250Aknowledge%2520from%25202D%2520VTON%2520models%2520to%25203D%2520while%2520improving%2520cross-view%2520consistency.%2520%25281%2529%250ASpecifically%252C%2520we%2520propose%2520a%2520personalized%2520diffusion%2520model%2520that%2520utilizes%2520low-rank%250Aadaptation%2520%2528LoRA%2529%2520fine-tuning%2520to%2520incorporate%2520personalized%2520information%2520into%250Apre-trained%25202D%2520VTON%2520models.%2520To%2520achieve%2520effective%2520LoRA%2520training%252C%2520we%2520introduce%2520a%250Areference-driven%2520image%2520editing%2520approach%2520that%2520enables%2520the%2520simultaneous%2520editing%250Aof%2520multi-view%2520images%2520while%2520ensuring%2520consistency.%2520%25282%2529%2520Furthermore%252C%2520we%2520propose%2520a%250Apersona-aware%25203DGS%2520editing%2520framework%2520to%2520facilitate%2520effective%2520editing%2520while%250Amaintaining%2520consistent%2520cross-view%2520appearance%2520and%2520high-quality%25203D%2520geometry.%2520%25283%2529%250AAdditionally%252C%2520we%2520have%2520established%2520a%2520new%25203D%2520VTON%2520benchmark%252C%25203D-VTONBench%252C%2520which%250Afacilitates%2520comprehensive%2520qualitative%2520and%2520quantitative%25203D%2520VTON%2520evaluations.%250AThrough%2520extensive%2520experiments%2520and%2520comparative%2520analyses%2520with%2520existing%2520methods%252C%250Athe%2520proposed%2520%255COM%2520has%2520demonstrated%2520superior%2520fidelity%2520and%2520advanced%2520editing%250Acapabilities%252C%2520affirming%2520its%2520effectiveness%2520for%25203D%2520VTON.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2410.05259v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=GS-VTON%3A%20Controllable%203D%20Virtual%20Try-on%20with%20Gaussian%20Splatting&entry.906535625=Yukang%20Cao%20and%20Masoud%20Hadi%20and%20Liang%20Pan%20and%20Ziwei%20Liu&entry.1292438233=%20%20Diffusion-based%202D%20virtual%20try-on%20%28VTON%29%20techniques%20have%20recently%0Ademonstrated%20strong%20performance%2C%20while%20the%20development%20of%203D%20VTON%20has%20largely%0Alagged%20behind.%20Despite%20recent%20advances%20in%20text-guided%203D%20scene%20editing%2C%0Aintegrating%202D%20VTON%20into%20these%20pipelines%20to%20achieve%20vivid%203D%20VTON%20remains%0Achallenging.%20The%20reasons%20are%20twofold.%20First%2C%20text%20prompts%20cannot%20provide%0Asufficient%20details%20in%20describing%20clothing.%20Second%2C%202D%20VTON%20results%20generated%0Afrom%20different%20viewpoints%20of%20the%20same%203D%20scene%20lack%20coherence%20and%20spatial%0Arelationships%2C%20hence%20frequently%20leading%20to%20appearance%20inconsistencies%20and%0Ageometric%20distortions.%20To%20resolve%20these%20problems%2C%20we%20introduce%20an%0Aimage-prompted%203D%20VTON%20method%20%28dubbed%20GS-VTON%29%20which%2C%20by%20leveraging%203D%20Gaussian%0ASplatting%20%283DGS%29%20as%20the%203D%20representation%2C%20enables%20the%20transfer%20of%20pre-trained%0Aknowledge%20from%202D%20VTON%20models%20to%203D%20while%20improving%20cross-view%20consistency.%20%281%29%0ASpecifically%2C%20we%20propose%20a%20personalized%20diffusion%20model%20that%20utilizes%20low-rank%0Aadaptation%20%28LoRA%29%20fine-tuning%20to%20incorporate%20personalized%20information%20into%0Apre-trained%202D%20VTON%20models.%20To%20achieve%20effective%20LoRA%20training%2C%20we%20introduce%20a%0Areference-driven%20image%20editing%20approach%20that%20enables%20the%20simultaneous%20editing%0Aof%20multi-view%20images%20while%20ensuring%20consistency.%20%282%29%20Furthermore%2C%20we%20propose%20a%0Apersona-aware%203DGS%20editing%20framework%20to%20facilitate%20effective%20editing%20while%0Amaintaining%20consistent%20cross-view%20appearance%20and%20high-quality%203D%20geometry.%20%283%29%0AAdditionally%2C%20we%20have%20established%20a%20new%203D%20VTON%20benchmark%2C%203D-VTONBench%2C%20which%0Afacilitates%20comprehensive%20qualitative%20and%20quantitative%203D%20VTON%20evaluations.%0AThrough%20extensive%20experiments%20and%20comparative%20analyses%20with%20existing%20methods%2C%0Athe%20proposed%20%5COM%20has%20demonstrated%20superior%20fidelity%20and%20advanced%20editing%0Acapabilities%2C%20affirming%20its%20effectiveness%20for%203D%20VTON.%0A&entry.1838667208=http%3A//arxiv.org/abs/2410.05259v1&entry.124074799=Read"},
{"title": "Preserving Multi-Modal Capabilities of Pre-trained VLMs for Improving\n  Vision-Linguistic Compositionality", "author": "Youngtaek Oh and Jae Won Cho and Dong-Jin Kim and In So Kweon and Junmo Kim", "abstract": "  In this paper, we propose a new method to enhance compositional understanding\nin pre-trained vision and language models (VLMs) without sacrificing\nperformance in zero-shot multi-modal tasks. Traditional fine-tuning approaches\noften improve compositional reasoning at the cost of degrading multi-modal\ncapabilities, primarily due to the use of global hard negative (HN) loss, which\ncontrasts global representations of images and texts. This global HN loss\npushes HN texts that are highly similar to the original ones, damaging the\nmodel's multi-modal representations. To overcome this limitation, we propose\nFine-grained Selective Calibrated CLIP (FSC-CLIP), which integrates local hard\nnegative loss and selective calibrated regularization. These innovations\nprovide fine-grained negative supervision while preserving the model's\nrepresentational integrity. Our extensive evaluations across diverse benchmarks\nfor both compositionality and multi-modal tasks show that FSC-CLIP not only\nachieves compositionality on par with state-of-the-art models but also retains\nstrong multi-modal capabilities. Code is available at:\nhttps://github.com/ytaek-oh/fsc-clip.\n", "link": "http://arxiv.org/abs/2410.05210v1", "date": "2024-10-07", "relevancy": 3.0188, "topK": [{"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.6291}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5911}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5911}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Preserving%20Multi-Modal%20Capabilities%20of%20Pre-trained%20VLMs%20for%20Improving%0A%20%20Vision-Linguistic%20Compositionality&body=Title%3A%20Preserving%20Multi-Modal%20Capabilities%20of%20Pre-trained%20VLMs%20for%20Improving%0A%20%20Vision-Linguistic%20Compositionality%0AAuthor%3A%20Youngtaek%20Oh%20and%20Jae%20Won%20Cho%20and%20Dong-Jin%20Kim%20and%20In%20So%20Kweon%20and%20Junmo%20Kim%0AAbstract%3A%20%20%20In%20this%20paper%2C%20we%20propose%20a%20new%20method%20to%20enhance%20compositional%20understanding%0Ain%20pre-trained%20vision%20and%20language%20models%20%28VLMs%29%20without%20sacrificing%0Aperformance%20in%20zero-shot%20multi-modal%20tasks.%20Traditional%20fine-tuning%20approaches%0Aoften%20improve%20compositional%20reasoning%20at%20the%20cost%20of%20degrading%20multi-modal%0Acapabilities%2C%20primarily%20due%20to%20the%20use%20of%20global%20hard%20negative%20%28HN%29%20loss%2C%20which%0Acontrasts%20global%20representations%20of%20images%20and%20texts.%20This%20global%20HN%20loss%0Apushes%20HN%20texts%20that%20are%20highly%20similar%20to%20the%20original%20ones%2C%20damaging%20the%0Amodel%27s%20multi-modal%20representations.%20To%20overcome%20this%20limitation%2C%20we%20propose%0AFine-grained%20Selective%20Calibrated%20CLIP%20%28FSC-CLIP%29%2C%20which%20integrates%20local%20hard%0Anegative%20loss%20and%20selective%20calibrated%20regularization.%20These%20innovations%0Aprovide%20fine-grained%20negative%20supervision%20while%20preserving%20the%20model%27s%0Arepresentational%20integrity.%20Our%20extensive%20evaluations%20across%20diverse%20benchmarks%0Afor%20both%20compositionality%20and%20multi-modal%20tasks%20show%20that%20FSC-CLIP%20not%20only%0Aachieves%20compositionality%20on%20par%20with%20state-of-the-art%20models%20but%20also%20retains%0Astrong%20multi-modal%20capabilities.%20Code%20is%20available%20at%3A%0Ahttps%3A//github.com/ytaek-oh/fsc-clip.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2410.05210v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DPreserving%2520Multi-Modal%2520Capabilities%2520of%2520Pre-trained%2520VLMs%2520for%2520Improving%250A%2520%2520Vision-Linguistic%2520Compositionality%26entry.906535625%3DYoungtaek%2520Oh%2520and%2520Jae%2520Won%2520Cho%2520and%2520Dong-Jin%2520Kim%2520and%2520In%2520So%2520Kweon%2520and%2520Junmo%2520Kim%26entry.1292438233%3D%2520%2520In%2520this%2520paper%252C%2520we%2520propose%2520a%2520new%2520method%2520to%2520enhance%2520compositional%2520understanding%250Ain%2520pre-trained%2520vision%2520and%2520language%2520models%2520%2528VLMs%2529%2520without%2520sacrificing%250Aperformance%2520in%2520zero-shot%2520multi-modal%2520tasks.%2520Traditional%2520fine-tuning%2520approaches%250Aoften%2520improve%2520compositional%2520reasoning%2520at%2520the%2520cost%2520of%2520degrading%2520multi-modal%250Acapabilities%252C%2520primarily%2520due%2520to%2520the%2520use%2520of%2520global%2520hard%2520negative%2520%2528HN%2529%2520loss%252C%2520which%250Acontrasts%2520global%2520representations%2520of%2520images%2520and%2520texts.%2520This%2520global%2520HN%2520loss%250Apushes%2520HN%2520texts%2520that%2520are%2520highly%2520similar%2520to%2520the%2520original%2520ones%252C%2520damaging%2520the%250Amodel%2527s%2520multi-modal%2520representations.%2520To%2520overcome%2520this%2520limitation%252C%2520we%2520propose%250AFine-grained%2520Selective%2520Calibrated%2520CLIP%2520%2528FSC-CLIP%2529%252C%2520which%2520integrates%2520local%2520hard%250Anegative%2520loss%2520and%2520selective%2520calibrated%2520regularization.%2520These%2520innovations%250Aprovide%2520fine-grained%2520negative%2520supervision%2520while%2520preserving%2520the%2520model%2527s%250Arepresentational%2520integrity.%2520Our%2520extensive%2520evaluations%2520across%2520diverse%2520benchmarks%250Afor%2520both%2520compositionality%2520and%2520multi-modal%2520tasks%2520show%2520that%2520FSC-CLIP%2520not%2520only%250Aachieves%2520compositionality%2520on%2520par%2520with%2520state-of-the-art%2520models%2520but%2520also%2520retains%250Astrong%2520multi-modal%2520capabilities.%2520Code%2520is%2520available%2520at%253A%250Ahttps%253A//github.com/ytaek-oh/fsc-clip.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2410.05210v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Preserving%20Multi-Modal%20Capabilities%20of%20Pre-trained%20VLMs%20for%20Improving%0A%20%20Vision-Linguistic%20Compositionality&entry.906535625=Youngtaek%20Oh%20and%20Jae%20Won%20Cho%20and%20Dong-Jin%20Kim%20and%20In%20So%20Kweon%20and%20Junmo%20Kim&entry.1292438233=%20%20In%20this%20paper%2C%20we%20propose%20a%20new%20method%20to%20enhance%20compositional%20understanding%0Ain%20pre-trained%20vision%20and%20language%20models%20%28VLMs%29%20without%20sacrificing%0Aperformance%20in%20zero-shot%20multi-modal%20tasks.%20Traditional%20fine-tuning%20approaches%0Aoften%20improve%20compositional%20reasoning%20at%20the%20cost%20of%20degrading%20multi-modal%0Acapabilities%2C%20primarily%20due%20to%20the%20use%20of%20global%20hard%20negative%20%28HN%29%20loss%2C%20which%0Acontrasts%20global%20representations%20of%20images%20and%20texts.%20This%20global%20HN%20loss%0Apushes%20HN%20texts%20that%20are%20highly%20similar%20to%20the%20original%20ones%2C%20damaging%20the%0Amodel%27s%20multi-modal%20representations.%20To%20overcome%20this%20limitation%2C%20we%20propose%0AFine-grained%20Selective%20Calibrated%20CLIP%20%28FSC-CLIP%29%2C%20which%20integrates%20local%20hard%0Anegative%20loss%20and%20selective%20calibrated%20regularization.%20These%20innovations%0Aprovide%20fine-grained%20negative%20supervision%20while%20preserving%20the%20model%27s%0Arepresentational%20integrity.%20Our%20extensive%20evaluations%20across%20diverse%20benchmarks%0Afor%20both%20compositionality%20and%20multi-modal%20tasks%20show%20that%20FSC-CLIP%20not%20only%0Aachieves%20compositionality%20on%20par%20with%20state-of-the-art%20models%20but%20also%20retains%0Astrong%20multi-modal%20capabilities.%20Code%20is%20available%20at%3A%0Ahttps%3A//github.com/ytaek-oh/fsc-clip.%0A&entry.1838667208=http%3A//arxiv.org/abs/2410.05210v1&entry.124074799=Read"},
{"title": "D-PoSE: Depth as an Intermediate Representation for 3D Human Pose and\n  Shape Estimation", "author": "Nikolaos Vasilikopoulos and Drosakis Drosakis and Antonis Argyros", "abstract": "  We present D-PoSE (Depth as an Intermediate Representation for 3D Human Pose\nand Shape Estimation), a one-stage method that estimates human pose and SMPL-X\nshape parameters from a single RGB image. Recent works use larger models with\ntransformer backbones and decoders to improve the accuracy in human pose and\nshape (HPS) benchmarks. D-PoSE proposes a vision based approach that uses the\nestimated human depth-maps as an intermediate representation for HPS and\nleverages training with synthetic data and the ground-truth depth-maps provided\nwith them for depth supervision during training. Although trained on synthetic\ndatasets, D-PoSE achieves state-of-the-art performance on the real-world\nbenchmark datasets, EMDB and 3DPW. Despite its simple lightweight design and\nthe CNN backbone, it outperforms ViT-based models that have a number of\nparameters that is larger by almost an order of magnitude. D-PoSE code is\navailable at: https://github.com/nvasilik/D-PoSE\n", "link": "http://arxiv.org/abs/2410.04889v1", "date": "2024-10-07", "relevancy": 3.0002, "topK": [{"title": "GST: Precise 3D Human Body from a Single Image with Gaussian Splatting\n  Transformers", "link": "http://arxiv.org/abs/2409.04196v1", "similarity": 0.6189}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5906}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5906}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20D-PoSE%3A%20Depth%20as%20an%20Intermediate%20Representation%20for%203D%20Human%20Pose%20and%0A%20%20Shape%20Estimation&body=Title%3A%20D-PoSE%3A%20Depth%20as%20an%20Intermediate%20Representation%20for%203D%20Human%20Pose%20and%0A%20%20Shape%20Estimation%0AAuthor%3A%20Nikolaos%20Vasilikopoulos%20and%20Drosakis%20Drosakis%20and%20Antonis%20Argyros%0AAbstract%3A%20%20%20We%20present%20D-PoSE%20%28Depth%20as%20an%20Intermediate%20Representation%20for%203D%20Human%20Pose%0Aand%20Shape%20Estimation%29%2C%20a%20one-stage%20method%20that%20estimates%20human%20pose%20and%20SMPL-X%0Ashape%20parameters%20from%20a%20single%20RGB%20image.%20Recent%20works%20use%20larger%20models%20with%0Atransformer%20backbones%20and%20decoders%20to%20improve%20the%20accuracy%20in%20human%20pose%20and%0Ashape%20%28HPS%29%20benchmarks.%20D-PoSE%20proposes%20a%20vision%20based%20approach%20that%20uses%20the%0Aestimated%20human%20depth-maps%20as%20an%20intermediate%20representation%20for%20HPS%20and%0Aleverages%20training%20with%20synthetic%20data%20and%20the%20ground-truth%20depth-maps%20provided%0Awith%20them%20for%20depth%20supervision%20during%20training.%20Although%20trained%20on%20synthetic%0Adatasets%2C%20D-PoSE%20achieves%20state-of-the-art%20performance%20on%20the%20real-world%0Abenchmark%20datasets%2C%20EMDB%20and%203DPW.%20Despite%20its%20simple%20lightweight%20design%20and%0Athe%20CNN%20backbone%2C%20it%20outperforms%20ViT-based%20models%20that%20have%20a%20number%20of%0Aparameters%20that%20is%20larger%20by%20almost%20an%20order%20of%20magnitude.%20D-PoSE%20code%20is%0Aavailable%20at%3A%20https%3A//github.com/nvasilik/D-PoSE%0A%0ALink%3A%20http%3A//arxiv.org/abs/2410.04889v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DD-PoSE%253A%2520Depth%2520as%2520an%2520Intermediate%2520Representation%2520for%25203D%2520Human%2520Pose%2520and%250A%2520%2520Shape%2520Estimation%26entry.906535625%3DNikolaos%2520Vasilikopoulos%2520and%2520Drosakis%2520Drosakis%2520and%2520Antonis%2520Argyros%26entry.1292438233%3D%2520%2520We%2520present%2520D-PoSE%2520%2528Depth%2520as%2520an%2520Intermediate%2520Representation%2520for%25203D%2520Human%2520Pose%250Aand%2520Shape%2520Estimation%2529%252C%2520a%2520one-stage%2520method%2520that%2520estimates%2520human%2520pose%2520and%2520SMPL-X%250Ashape%2520parameters%2520from%2520a%2520single%2520RGB%2520image.%2520Recent%2520works%2520use%2520larger%2520models%2520with%250Atransformer%2520backbones%2520and%2520decoders%2520to%2520improve%2520the%2520accuracy%2520in%2520human%2520pose%2520and%250Ashape%2520%2528HPS%2529%2520benchmarks.%2520D-PoSE%2520proposes%2520a%2520vision%2520based%2520approach%2520that%2520uses%2520the%250Aestimated%2520human%2520depth-maps%2520as%2520an%2520intermediate%2520representation%2520for%2520HPS%2520and%250Aleverages%2520training%2520with%2520synthetic%2520data%2520and%2520the%2520ground-truth%2520depth-maps%2520provided%250Awith%2520them%2520for%2520depth%2520supervision%2520during%2520training.%2520Although%2520trained%2520on%2520synthetic%250Adatasets%252C%2520D-PoSE%2520achieves%2520state-of-the-art%2520performance%2520on%2520the%2520real-world%250Abenchmark%2520datasets%252C%2520EMDB%2520and%25203DPW.%2520Despite%2520its%2520simple%2520lightweight%2520design%2520and%250Athe%2520CNN%2520backbone%252C%2520it%2520outperforms%2520ViT-based%2520models%2520that%2520have%2520a%2520number%2520of%250Aparameters%2520that%2520is%2520larger%2520by%2520almost%2520an%2520order%2520of%2520magnitude.%2520D-PoSE%2520code%2520is%250Aavailable%2520at%253A%2520https%253A//github.com/nvasilik/D-PoSE%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2410.04889v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=D-PoSE%3A%20Depth%20as%20an%20Intermediate%20Representation%20for%203D%20Human%20Pose%20and%0A%20%20Shape%20Estimation&entry.906535625=Nikolaos%20Vasilikopoulos%20and%20Drosakis%20Drosakis%20and%20Antonis%20Argyros&entry.1292438233=%20%20We%20present%20D-PoSE%20%28Depth%20as%20an%20Intermediate%20Representation%20for%203D%20Human%20Pose%0Aand%20Shape%20Estimation%29%2C%20a%20one-stage%20method%20that%20estimates%20human%20pose%20and%20SMPL-X%0Ashape%20parameters%20from%20a%20single%20RGB%20image.%20Recent%20works%20use%20larger%20models%20with%0Atransformer%20backbones%20and%20decoders%20to%20improve%20the%20accuracy%20in%20human%20pose%20and%0Ashape%20%28HPS%29%20benchmarks.%20D-PoSE%20proposes%20a%20vision%20based%20approach%20that%20uses%20the%0Aestimated%20human%20depth-maps%20as%20an%20intermediate%20representation%20for%20HPS%20and%0Aleverages%20training%20with%20synthetic%20data%20and%20the%20ground-truth%20depth-maps%20provided%0Awith%20them%20for%20depth%20supervision%20during%20training.%20Although%20trained%20on%20synthetic%0Adatasets%2C%20D-PoSE%20achieves%20state-of-the-art%20performance%20on%20the%20real-world%0Abenchmark%20datasets%2C%20EMDB%20and%203DPW.%20Despite%20its%20simple%20lightweight%20design%20and%0Athe%20CNN%20backbone%2C%20it%20outperforms%20ViT-based%20models%20that%20have%20a%20number%20of%0Aparameters%20that%20is%20larger%20by%20almost%20an%20order%20of%20magnitude.%20D-PoSE%20code%20is%0Aavailable%20at%3A%20https%3A//github.com/nvasilik/D-PoSE%0A&entry.1838667208=http%3A//arxiv.org/abs/2410.04889v1&entry.124074799=Read"},
{"title": "VLM2Vec: Training Vision-Language Models for Massive Multimodal\n  Embedding Tasks", "author": "Ziyan Jiang and Rui Meng and Xinyi Yang and Semih Yavuz and Yingbo Zhou and Wenhu Chen", "abstract": "  Embedding models have been crucial in enabling various downstream tasks such\nas semantic similarity, information retrieval, and clustering. Recently, there\nhas been a surge of interest in developing universal text embedding models that\ncan generalize across tasks (e.g., MTEB). However, progress in learning\nuniversal multimodal embedding models has been relatively slow despite their\nimportance. In this work, we aim to explore the potential for building\nuniversal embeddings capable of handling a wide range of downstream tasks. Our\ncontributions are twofold: (1) MMEB (Massive Multimodal Embedding Benchmark),\nwhich covers 4 meta-tasks (i.e. classification, visual question answering,\nmultimodal retrieval, and visual grounding) and 36 datasets, including 20\ntraining and 16 evaluation datasets, and (2) VLM2Vec (Vision-Language Model ->\nVector), a contrastive training framework that converts any state-of-the-art\nvision-language model into an embedding model via training on MMEB. Unlike\nprevious models such as CLIP and BLIP, VLM2Vec can process any combination of\nimages and text to generate a fixed-dimensional vector based on task\ninstructions. We build a series of VLM2Vec models on Phi-3.5-V and evaluate\nthem on MMEB's evaluation split. Our results show that \\model achieves an\nabsolute average improvement of 10% to 20% over existing multimodal embedding\nmodels on both in-distribution and out-of-distribution datasets in MMEB.\n", "link": "http://arxiv.org/abs/2410.05160v1", "date": "2024-10-07", "relevancy": 2.9176, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5852}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5852}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5802}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20VLM2Vec%3A%20Training%20Vision-Language%20Models%20for%20Massive%20Multimodal%0A%20%20Embedding%20Tasks&body=Title%3A%20VLM2Vec%3A%20Training%20Vision-Language%20Models%20for%20Massive%20Multimodal%0A%20%20Embedding%20Tasks%0AAuthor%3A%20Ziyan%20Jiang%20and%20Rui%20Meng%20and%20Xinyi%20Yang%20and%20Semih%20Yavuz%20and%20Yingbo%20Zhou%20and%20Wenhu%20Chen%0AAbstract%3A%20%20%20Embedding%20models%20have%20been%20crucial%20in%20enabling%20various%20downstream%20tasks%20such%0Aas%20semantic%20similarity%2C%20information%20retrieval%2C%20and%20clustering.%20Recently%2C%20there%0Ahas%20been%20a%20surge%20of%20interest%20in%20developing%20universal%20text%20embedding%20models%20that%0Acan%20generalize%20across%20tasks%20%28e.g.%2C%20MTEB%29.%20However%2C%20progress%20in%20learning%0Auniversal%20multimodal%20embedding%20models%20has%20been%20relatively%20slow%20despite%20their%0Aimportance.%20In%20this%20work%2C%20we%20aim%20to%20explore%20the%20potential%20for%20building%0Auniversal%20embeddings%20capable%20of%20handling%20a%20wide%20range%20of%20downstream%20tasks.%20Our%0Acontributions%20are%20twofold%3A%20%281%29%20MMEB%20%28Massive%20Multimodal%20Embedding%20Benchmark%29%2C%0Awhich%20covers%204%20meta-tasks%20%28i.e.%20classification%2C%20visual%20question%20answering%2C%0Amultimodal%20retrieval%2C%20and%20visual%20grounding%29%20and%2036%20datasets%2C%20including%2020%0Atraining%20and%2016%20evaluation%20datasets%2C%20and%20%282%29%20VLM2Vec%20%28Vision-Language%20Model%20-%3E%0AVector%29%2C%20a%20contrastive%20training%20framework%20that%20converts%20any%20state-of-the-art%0Avision-language%20model%20into%20an%20embedding%20model%20via%20training%20on%20MMEB.%20Unlike%0Aprevious%20models%20such%20as%20CLIP%20and%20BLIP%2C%20VLM2Vec%20can%20process%20any%20combination%20of%0Aimages%20and%20text%20to%20generate%20a%20fixed-dimensional%20vector%20based%20on%20task%0Ainstructions.%20We%20build%20a%20series%20of%20VLM2Vec%20models%20on%20Phi-3.5-V%20and%20evaluate%0Athem%20on%20MMEB%27s%20evaluation%20split.%20Our%20results%20show%20that%20%5Cmodel%20achieves%20an%0Aabsolute%20average%20improvement%20of%2010%25%20to%2020%25%20over%20existing%20multimodal%20embedding%0Amodels%20on%20both%20in-distribution%20and%20out-of-distribution%20datasets%20in%20MMEB.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2410.05160v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DVLM2Vec%253A%2520Training%2520Vision-Language%2520Models%2520for%2520Massive%2520Multimodal%250A%2520%2520Embedding%2520Tasks%26entry.906535625%3DZiyan%2520Jiang%2520and%2520Rui%2520Meng%2520and%2520Xinyi%2520Yang%2520and%2520Semih%2520Yavuz%2520and%2520Yingbo%2520Zhou%2520and%2520Wenhu%2520Chen%26entry.1292438233%3D%2520%2520Embedding%2520models%2520have%2520been%2520crucial%2520in%2520enabling%2520various%2520downstream%2520tasks%2520such%250Aas%2520semantic%2520similarity%252C%2520information%2520retrieval%252C%2520and%2520clustering.%2520Recently%252C%2520there%250Ahas%2520been%2520a%2520surge%2520of%2520interest%2520in%2520developing%2520universal%2520text%2520embedding%2520models%2520that%250Acan%2520generalize%2520across%2520tasks%2520%2528e.g.%252C%2520MTEB%2529.%2520However%252C%2520progress%2520in%2520learning%250Auniversal%2520multimodal%2520embedding%2520models%2520has%2520been%2520relatively%2520slow%2520despite%2520their%250Aimportance.%2520In%2520this%2520work%252C%2520we%2520aim%2520to%2520explore%2520the%2520potential%2520for%2520building%250Auniversal%2520embeddings%2520capable%2520of%2520handling%2520a%2520wide%2520range%2520of%2520downstream%2520tasks.%2520Our%250Acontributions%2520are%2520twofold%253A%2520%25281%2529%2520MMEB%2520%2528Massive%2520Multimodal%2520Embedding%2520Benchmark%2529%252C%250Awhich%2520covers%25204%2520meta-tasks%2520%2528i.e.%2520classification%252C%2520visual%2520question%2520answering%252C%250Amultimodal%2520retrieval%252C%2520and%2520visual%2520grounding%2529%2520and%252036%2520datasets%252C%2520including%252020%250Atraining%2520and%252016%2520evaluation%2520datasets%252C%2520and%2520%25282%2529%2520VLM2Vec%2520%2528Vision-Language%2520Model%2520-%253E%250AVector%2529%252C%2520a%2520contrastive%2520training%2520framework%2520that%2520converts%2520any%2520state-of-the-art%250Avision-language%2520model%2520into%2520an%2520embedding%2520model%2520via%2520training%2520on%2520MMEB.%2520Unlike%250Aprevious%2520models%2520such%2520as%2520CLIP%2520and%2520BLIP%252C%2520VLM2Vec%2520can%2520process%2520any%2520combination%2520of%250Aimages%2520and%2520text%2520to%2520generate%2520a%2520fixed-dimensional%2520vector%2520based%2520on%2520task%250Ainstructions.%2520We%2520build%2520a%2520series%2520of%2520VLM2Vec%2520models%2520on%2520Phi-3.5-V%2520and%2520evaluate%250Athem%2520on%2520MMEB%2527s%2520evaluation%2520split.%2520Our%2520results%2520show%2520that%2520%255Cmodel%2520achieves%2520an%250Aabsolute%2520average%2520improvement%2520of%252010%2525%2520to%252020%2525%2520over%2520existing%2520multimodal%2520embedding%250Amodels%2520on%2520both%2520in-distribution%2520and%2520out-of-distribution%2520datasets%2520in%2520MMEB.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2410.05160v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=VLM2Vec%3A%20Training%20Vision-Language%20Models%20for%20Massive%20Multimodal%0A%20%20Embedding%20Tasks&entry.906535625=Ziyan%20Jiang%20and%20Rui%20Meng%20and%20Xinyi%20Yang%20and%20Semih%20Yavuz%20and%20Yingbo%20Zhou%20and%20Wenhu%20Chen&entry.1292438233=%20%20Embedding%20models%20have%20been%20crucial%20in%20enabling%20various%20downstream%20tasks%20such%0Aas%20semantic%20similarity%2C%20information%20retrieval%2C%20and%20clustering.%20Recently%2C%20there%0Ahas%20been%20a%20surge%20of%20interest%20in%20developing%20universal%20text%20embedding%20models%20that%0Acan%20generalize%20across%20tasks%20%28e.g.%2C%20MTEB%29.%20However%2C%20progress%20in%20learning%0Auniversal%20multimodal%20embedding%20models%20has%20been%20relatively%20slow%20despite%20their%0Aimportance.%20In%20this%20work%2C%20we%20aim%20to%20explore%20the%20potential%20for%20building%0Auniversal%20embeddings%20capable%20of%20handling%20a%20wide%20range%20of%20downstream%20tasks.%20Our%0Acontributions%20are%20twofold%3A%20%281%29%20MMEB%20%28Massive%20Multimodal%20Embedding%20Benchmark%29%2C%0Awhich%20covers%204%20meta-tasks%20%28i.e.%20classification%2C%20visual%20question%20answering%2C%0Amultimodal%20retrieval%2C%20and%20visual%20grounding%29%20and%2036%20datasets%2C%20including%2020%0Atraining%20and%2016%20evaluation%20datasets%2C%20and%20%282%29%20VLM2Vec%20%28Vision-Language%20Model%20-%3E%0AVector%29%2C%20a%20contrastive%20training%20framework%20that%20converts%20any%20state-of-the-art%0Avision-language%20model%20into%20an%20embedding%20model%20via%20training%20on%20MMEB.%20Unlike%0Aprevious%20models%20such%20as%20CLIP%20and%20BLIP%2C%20VLM2Vec%20can%20process%20any%20combination%20of%0Aimages%20and%20text%20to%20generate%20a%20fixed-dimensional%20vector%20based%20on%20task%0Ainstructions.%20We%20build%20a%20series%20of%20VLM2Vec%20models%20on%20Phi-3.5-V%20and%20evaluate%0Athem%20on%20MMEB%27s%20evaluation%20split.%20Our%20results%20show%20that%20%5Cmodel%20achieves%20an%0Aabsolute%20average%20improvement%20of%2010%25%20to%2020%25%20over%20existing%20multimodal%20embedding%0Amodels%20on%20both%20in-distribution%20and%20out-of-distribution%20datasets%20in%20MMEB.%0A&entry.1838667208=http%3A//arxiv.org/abs/2410.05160v1&entry.124074799=Read"},
{"title": "GS-Hider: Hiding Messages into 3D Gaussian Splatting", "author": "Xuanyu Zhang and Jiarui Meng and Runyi Li and Zhipei Xu and Yongbing Zhang and Jian Zhang", "abstract": "  3D Gaussian Splatting (3DGS) has already become the emerging research focus\nin the fields of 3D scene reconstruction and novel view synthesis. Given that\ntraining a 3DGS requires a significant amount of time and computational cost,\nit is crucial to protect the copyright, integrity, and privacy of such 3D\nassets. Steganography, as a crucial technique for encrypted transmission and\ncopyright protection, has been extensively studied. However, it still lacks\nprofound exploration targeted at 3DGS. Unlike its predecessor NeRF, 3DGS\npossesses two distinct features: 1) explicit 3D representation; and 2)\nreal-time rendering speeds. These characteristics result in the 3DGS point\ncloud files being public and transparent, with each Gaussian point having a\nclear physical significance. Therefore, ensuring the security and fidelity of\nthe original 3D scene while embedding information into the 3DGS point cloud\nfiles is an extremely challenging task. To solve the above-mentioned issue, we\nfirst propose a steganography framework for 3DGS, dubbed GS-Hider, which can\nembed 3D scenes and images into original GS point clouds in an invisible manner\nand accurately extract the hidden messages. Specifically, we design a coupled\nsecured feature attribute to replace the original 3DGS's spherical harmonics\ncoefficients and then use a scene decoder and a message decoder to disentangle\nthe original RGB scene and the hidden message. Extensive experiments\ndemonstrated that the proposed GS-Hider can effectively conceal multimodal\nmessages without compromising rendering quality and possesses exceptional\nsecurity, robustness, capacity, and flexibility. Our project is available at:\nhttps://xuanyuzhang21.github.io/project/gshider.\n", "link": "http://arxiv.org/abs/2405.15118v2", "date": "2024-10-07", "relevancy": 2.8992, "topK": [{"title": "RTG-SLAM: Real-time 3D Reconstruction at Scale using Gaussian Splatting", "link": "http://arxiv.org/abs/2404.19706v1", "similarity": 0.5846}, {"title": "MiraGe: Editable 2D Images using Gaussian Splatting", "link": "http://arxiv.org/abs/2410.01521v1", "similarity": 0.5821}, {"title": "GST: Precise 3D Human Body from a Single Image with Gaussian Splatting\n  Transformers", "link": "http://arxiv.org/abs/2409.04196v1", "similarity": 0.5728}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20GS-Hider%3A%20Hiding%20Messages%20into%203D%20Gaussian%20Splatting&body=Title%3A%20GS-Hider%3A%20Hiding%20Messages%20into%203D%20Gaussian%20Splatting%0AAuthor%3A%20Xuanyu%20Zhang%20and%20Jiarui%20Meng%20and%20Runyi%20Li%20and%20Zhipei%20Xu%20and%20Yongbing%20Zhang%20and%20Jian%20Zhang%0AAbstract%3A%20%20%203D%20Gaussian%20Splatting%20%283DGS%29%20has%20already%20become%20the%20emerging%20research%20focus%0Ain%20the%20fields%20of%203D%20scene%20reconstruction%20and%20novel%20view%20synthesis.%20Given%20that%0Atraining%20a%203DGS%20requires%20a%20significant%20amount%20of%20time%20and%20computational%20cost%2C%0Ait%20is%20crucial%20to%20protect%20the%20copyright%2C%20integrity%2C%20and%20privacy%20of%20such%203D%0Aassets.%20Steganography%2C%20as%20a%20crucial%20technique%20for%20encrypted%20transmission%20and%0Acopyright%20protection%2C%20has%20been%20extensively%20studied.%20However%2C%20it%20still%20lacks%0Aprofound%20exploration%20targeted%20at%203DGS.%20Unlike%20its%20predecessor%20NeRF%2C%203DGS%0Apossesses%20two%20distinct%20features%3A%201%29%20explicit%203D%20representation%3B%20and%202%29%0Areal-time%20rendering%20speeds.%20These%20characteristics%20result%20in%20the%203DGS%20point%0Acloud%20files%20being%20public%20and%20transparent%2C%20with%20each%20Gaussian%20point%20having%20a%0Aclear%20physical%20significance.%20Therefore%2C%20ensuring%20the%20security%20and%20fidelity%20of%0Athe%20original%203D%20scene%20while%20embedding%20information%20into%20the%203DGS%20point%20cloud%0Afiles%20is%20an%20extremely%20challenging%20task.%20To%20solve%20the%20above-mentioned%20issue%2C%20we%0Afirst%20propose%20a%20steganography%20framework%20for%203DGS%2C%20dubbed%20GS-Hider%2C%20which%20can%0Aembed%203D%20scenes%20and%20images%20into%20original%20GS%20point%20clouds%20in%20an%20invisible%20manner%0Aand%20accurately%20extract%20the%20hidden%20messages.%20Specifically%2C%20we%20design%20a%20coupled%0Asecured%20feature%20attribute%20to%20replace%20the%20original%203DGS%27s%20spherical%20harmonics%0Acoefficients%20and%20then%20use%20a%20scene%20decoder%20and%20a%20message%20decoder%20to%20disentangle%0Athe%20original%20RGB%20scene%20and%20the%20hidden%20message.%20Extensive%20experiments%0Ademonstrated%20that%20the%20proposed%20GS-Hider%20can%20effectively%20conceal%20multimodal%0Amessages%20without%20compromising%20rendering%20quality%20and%20possesses%20exceptional%0Asecurity%2C%20robustness%2C%20capacity%2C%20and%20flexibility.%20Our%20project%20is%20available%20at%3A%0Ahttps%3A//xuanyuzhang21.github.io/project/gshider.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2405.15118v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DGS-Hider%253A%2520Hiding%2520Messages%2520into%25203D%2520Gaussian%2520Splatting%26entry.906535625%3DXuanyu%2520Zhang%2520and%2520Jiarui%2520Meng%2520and%2520Runyi%2520Li%2520and%2520Zhipei%2520Xu%2520and%2520Yongbing%2520Zhang%2520and%2520Jian%2520Zhang%26entry.1292438233%3D%2520%25203D%2520Gaussian%2520Splatting%2520%25283DGS%2529%2520has%2520already%2520become%2520the%2520emerging%2520research%2520focus%250Ain%2520the%2520fields%2520of%25203D%2520scene%2520reconstruction%2520and%2520novel%2520view%2520synthesis.%2520Given%2520that%250Atraining%2520a%25203DGS%2520requires%2520a%2520significant%2520amount%2520of%2520time%2520and%2520computational%2520cost%252C%250Ait%2520is%2520crucial%2520to%2520protect%2520the%2520copyright%252C%2520integrity%252C%2520and%2520privacy%2520of%2520such%25203D%250Aassets.%2520Steganography%252C%2520as%2520a%2520crucial%2520technique%2520for%2520encrypted%2520transmission%2520and%250Acopyright%2520protection%252C%2520has%2520been%2520extensively%2520studied.%2520However%252C%2520it%2520still%2520lacks%250Aprofound%2520exploration%2520targeted%2520at%25203DGS.%2520Unlike%2520its%2520predecessor%2520NeRF%252C%25203DGS%250Apossesses%2520two%2520distinct%2520features%253A%25201%2529%2520explicit%25203D%2520representation%253B%2520and%25202%2529%250Areal-time%2520rendering%2520speeds.%2520These%2520characteristics%2520result%2520in%2520the%25203DGS%2520point%250Acloud%2520files%2520being%2520public%2520and%2520transparent%252C%2520with%2520each%2520Gaussian%2520point%2520having%2520a%250Aclear%2520physical%2520significance.%2520Therefore%252C%2520ensuring%2520the%2520security%2520and%2520fidelity%2520of%250Athe%2520original%25203D%2520scene%2520while%2520embedding%2520information%2520into%2520the%25203DGS%2520point%2520cloud%250Afiles%2520is%2520an%2520extremely%2520challenging%2520task.%2520To%2520solve%2520the%2520above-mentioned%2520issue%252C%2520we%250Afirst%2520propose%2520a%2520steganography%2520framework%2520for%25203DGS%252C%2520dubbed%2520GS-Hider%252C%2520which%2520can%250Aembed%25203D%2520scenes%2520and%2520images%2520into%2520original%2520GS%2520point%2520clouds%2520in%2520an%2520invisible%2520manner%250Aand%2520accurately%2520extract%2520the%2520hidden%2520messages.%2520Specifically%252C%2520we%2520design%2520a%2520coupled%250Asecured%2520feature%2520attribute%2520to%2520replace%2520the%2520original%25203DGS%2527s%2520spherical%2520harmonics%250Acoefficients%2520and%2520then%2520use%2520a%2520scene%2520decoder%2520and%2520a%2520message%2520decoder%2520to%2520disentangle%250Athe%2520original%2520RGB%2520scene%2520and%2520the%2520hidden%2520message.%2520Extensive%2520experiments%250Ademonstrated%2520that%2520the%2520proposed%2520GS-Hider%2520can%2520effectively%2520conceal%2520multimodal%250Amessages%2520without%2520compromising%2520rendering%2520quality%2520and%2520possesses%2520exceptional%250Asecurity%252C%2520robustness%252C%2520capacity%252C%2520and%2520flexibility.%2520Our%2520project%2520is%2520available%2520at%253A%250Ahttps%253A//xuanyuzhang21.github.io/project/gshider.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2405.15118v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=GS-Hider%3A%20Hiding%20Messages%20into%203D%20Gaussian%20Splatting&entry.906535625=Xuanyu%20Zhang%20and%20Jiarui%20Meng%20and%20Runyi%20Li%20and%20Zhipei%20Xu%20and%20Yongbing%20Zhang%20and%20Jian%20Zhang&entry.1292438233=%20%203D%20Gaussian%20Splatting%20%283DGS%29%20has%20already%20become%20the%20emerging%20research%20focus%0Ain%20the%20fields%20of%203D%20scene%20reconstruction%20and%20novel%20view%20synthesis.%20Given%20that%0Atraining%20a%203DGS%20requires%20a%20significant%20amount%20of%20time%20and%20computational%20cost%2C%0Ait%20is%20crucial%20to%20protect%20the%20copyright%2C%20integrity%2C%20and%20privacy%20of%20such%203D%0Aassets.%20Steganography%2C%20as%20a%20crucial%20technique%20for%20encrypted%20transmission%20and%0Acopyright%20protection%2C%20has%20been%20extensively%20studied.%20However%2C%20it%20still%20lacks%0Aprofound%20exploration%20targeted%20at%203DGS.%20Unlike%20its%20predecessor%20NeRF%2C%203DGS%0Apossesses%20two%20distinct%20features%3A%201%29%20explicit%203D%20representation%3B%20and%202%29%0Areal-time%20rendering%20speeds.%20These%20characteristics%20result%20in%20the%203DGS%20point%0Acloud%20files%20being%20public%20and%20transparent%2C%20with%20each%20Gaussian%20point%20having%20a%0Aclear%20physical%20significance.%20Therefore%2C%20ensuring%20the%20security%20and%20fidelity%20of%0Athe%20original%203D%20scene%20while%20embedding%20information%20into%20the%203DGS%20point%20cloud%0Afiles%20is%20an%20extremely%20challenging%20task.%20To%20solve%20the%20above-mentioned%20issue%2C%20we%0Afirst%20propose%20a%20steganography%20framework%20for%203DGS%2C%20dubbed%20GS-Hider%2C%20which%20can%0Aembed%203D%20scenes%20and%20images%20into%20original%20GS%20point%20clouds%20in%20an%20invisible%20manner%0Aand%20accurately%20extract%20the%20hidden%20messages.%20Specifically%2C%20we%20design%20a%20coupled%0Asecured%20feature%20attribute%20to%20replace%20the%20original%203DGS%27s%20spherical%20harmonics%0Acoefficients%20and%20then%20use%20a%20scene%20decoder%20and%20a%20message%20decoder%20to%20disentangle%0Athe%20original%20RGB%20scene%20and%20the%20hidden%20message.%20Extensive%20experiments%0Ademonstrated%20that%20the%20proposed%20GS-Hider%20can%20effectively%20conceal%20multimodal%0Amessages%20without%20compromising%20rendering%20quality%20and%20possesses%20exceptional%0Asecurity%2C%20robustness%2C%20capacity%2C%20and%20flexibility.%20Our%20project%20is%20available%20at%3A%0Ahttps%3A//xuanyuzhang21.github.io/project/gshider.%0A&entry.1838667208=http%3A//arxiv.org/abs/2405.15118v2&entry.124074799=Read"},
{"title": "Control-oriented Clustering of Visual Latent Representation", "author": "Han Qi and Haocheng Yin and Heng Yang", "abstract": "  We initiate a study of the geometry of the visual representation space -- the\ninformation channel from the vision encoder to the action decoder -- in an\nimage-based control pipeline learned from behavior cloning. Inspired by the\nphenomenon of neural collapse (NC) in image classification, we investigate\nwhether a similar law of clustering emerges in the visual representation space.\nSince image-based control is a regression task without explicitly defined\nclasses, the central piece of the puzzle lies in determining according to what\nimplicit classes the visual features cluster, if such a law exists. Focusing on\nimage-based planar pushing, we posit the most important role of the visual\nrepresentation in a control task is to convey a goal to the action decoder. We\nthen classify training samples of expert demonstrations into eight\n\"control-oriented\" classes based on (a) the relative pose between the object\nand the target in the input or (b) the relative pose of the object induced by\nexpert actions in the output, where one class corresponds to one relative pose\northant (REPO). Across four different instantiations of architecture, we report\nthe prevalent emergence of control-oriented clustering in the visual\nrepresentation space according to the eight REPOs. Beyond empirical\nobservation, we show such a law of clustering can be leveraged as an\nalgorithmic tool to improve test-time performance when training a policy with\nlimited expert demonstrations. Particularly, we pretrain the vision encoder\nusing NC as a regularization to encourage control-oriented clustering of the\nvisual features. Surprisingly, such an NC-pretrained vision encoder, when\nfinetuned end-to-end with the action decoder, boosts the test-time performance\nby 10% to 35% in the low-data regime. Real-world vision-based planar pushing\nexperiments confirmed the surprising advantage of control-oriented visual\nrepresentation pretraining.\n", "link": "http://arxiv.org/abs/2410.05063v1", "date": "2024-10-07", "relevancy": 2.874, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5785}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5785}, {"title": "Self-supervised Photographic Image Layout Representation Learning", "link": "http://arxiv.org/abs/2403.03740v1", "similarity": 0.5674}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Control-oriented%20Clustering%20of%20Visual%20Latent%20Representation&body=Title%3A%20Control-oriented%20Clustering%20of%20Visual%20Latent%20Representation%0AAuthor%3A%20Han%20Qi%20and%20Haocheng%20Yin%20and%20Heng%20Yang%0AAbstract%3A%20%20%20We%20initiate%20a%20study%20of%20the%20geometry%20of%20the%20visual%20representation%20space%20--%20the%0Ainformation%20channel%20from%20the%20vision%20encoder%20to%20the%20action%20decoder%20--%20in%20an%0Aimage-based%20control%20pipeline%20learned%20from%20behavior%20cloning.%20Inspired%20by%20the%0Aphenomenon%20of%20neural%20collapse%20%28NC%29%20in%20image%20classification%2C%20we%20investigate%0Awhether%20a%20similar%20law%20of%20clustering%20emerges%20in%20the%20visual%20representation%20space.%0ASince%20image-based%20control%20is%20a%20regression%20task%20without%20explicitly%20defined%0Aclasses%2C%20the%20central%20piece%20of%20the%20puzzle%20lies%20in%20determining%20according%20to%20what%0Aimplicit%20classes%20the%20visual%20features%20cluster%2C%20if%20such%20a%20law%20exists.%20Focusing%20on%0Aimage-based%20planar%20pushing%2C%20we%20posit%20the%20most%20important%20role%20of%20the%20visual%0Arepresentation%20in%20a%20control%20task%20is%20to%20convey%20a%20goal%20to%20the%20action%20decoder.%20We%0Athen%20classify%20training%20samples%20of%20expert%20demonstrations%20into%20eight%0A%22control-oriented%22%20classes%20based%20on%20%28a%29%20the%20relative%20pose%20between%20the%20object%0Aand%20the%20target%20in%20the%20input%20or%20%28b%29%20the%20relative%20pose%20of%20the%20object%20induced%20by%0Aexpert%20actions%20in%20the%20output%2C%20where%20one%20class%20corresponds%20to%20one%20relative%20pose%0Aorthant%20%28REPO%29.%20Across%20four%20different%20instantiations%20of%20architecture%2C%20we%20report%0Athe%20prevalent%20emergence%20of%20control-oriented%20clustering%20in%20the%20visual%0Arepresentation%20space%20according%20to%20the%20eight%20REPOs.%20Beyond%20empirical%0Aobservation%2C%20we%20show%20such%20a%20law%20of%20clustering%20can%20be%20leveraged%20as%20an%0Aalgorithmic%20tool%20to%20improve%20test-time%20performance%20when%20training%20a%20policy%20with%0Alimited%20expert%20demonstrations.%20Particularly%2C%20we%20pretrain%20the%20vision%20encoder%0Ausing%20NC%20as%20a%20regularization%20to%20encourage%20control-oriented%20clustering%20of%20the%0Avisual%20features.%20Surprisingly%2C%20such%20an%20NC-pretrained%20vision%20encoder%2C%20when%0Afinetuned%20end-to-end%20with%20the%20action%20decoder%2C%20boosts%20the%20test-time%20performance%0Aby%2010%25%20to%2035%25%20in%20the%20low-data%20regime.%20Real-world%20vision-based%20planar%20pushing%0Aexperiments%20confirmed%20the%20surprising%20advantage%20of%20control-oriented%20visual%0Arepresentation%20pretraining.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2410.05063v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DControl-oriented%2520Clustering%2520of%2520Visual%2520Latent%2520Representation%26entry.906535625%3DHan%2520Qi%2520and%2520Haocheng%2520Yin%2520and%2520Heng%2520Yang%26entry.1292438233%3D%2520%2520We%2520initiate%2520a%2520study%2520of%2520the%2520geometry%2520of%2520the%2520visual%2520representation%2520space%2520--%2520the%250Ainformation%2520channel%2520from%2520the%2520vision%2520encoder%2520to%2520the%2520action%2520decoder%2520--%2520in%2520an%250Aimage-based%2520control%2520pipeline%2520learned%2520from%2520behavior%2520cloning.%2520Inspired%2520by%2520the%250Aphenomenon%2520of%2520neural%2520collapse%2520%2528NC%2529%2520in%2520image%2520classification%252C%2520we%2520investigate%250Awhether%2520a%2520similar%2520law%2520of%2520clustering%2520emerges%2520in%2520the%2520visual%2520representation%2520space.%250ASince%2520image-based%2520control%2520is%2520a%2520regression%2520task%2520without%2520explicitly%2520defined%250Aclasses%252C%2520the%2520central%2520piece%2520of%2520the%2520puzzle%2520lies%2520in%2520determining%2520according%2520to%2520what%250Aimplicit%2520classes%2520the%2520visual%2520features%2520cluster%252C%2520if%2520such%2520a%2520law%2520exists.%2520Focusing%2520on%250Aimage-based%2520planar%2520pushing%252C%2520we%2520posit%2520the%2520most%2520important%2520role%2520of%2520the%2520visual%250Arepresentation%2520in%2520a%2520control%2520task%2520is%2520to%2520convey%2520a%2520goal%2520to%2520the%2520action%2520decoder.%2520We%250Athen%2520classify%2520training%2520samples%2520of%2520expert%2520demonstrations%2520into%2520eight%250A%2522control-oriented%2522%2520classes%2520based%2520on%2520%2528a%2529%2520the%2520relative%2520pose%2520between%2520the%2520object%250Aand%2520the%2520target%2520in%2520the%2520input%2520or%2520%2528b%2529%2520the%2520relative%2520pose%2520of%2520the%2520object%2520induced%2520by%250Aexpert%2520actions%2520in%2520the%2520output%252C%2520where%2520one%2520class%2520corresponds%2520to%2520one%2520relative%2520pose%250Aorthant%2520%2528REPO%2529.%2520Across%2520four%2520different%2520instantiations%2520of%2520architecture%252C%2520we%2520report%250Athe%2520prevalent%2520emergence%2520of%2520control-oriented%2520clustering%2520in%2520the%2520visual%250Arepresentation%2520space%2520according%2520to%2520the%2520eight%2520REPOs.%2520Beyond%2520empirical%250Aobservation%252C%2520we%2520show%2520such%2520a%2520law%2520of%2520clustering%2520can%2520be%2520leveraged%2520as%2520an%250Aalgorithmic%2520tool%2520to%2520improve%2520test-time%2520performance%2520when%2520training%2520a%2520policy%2520with%250Alimited%2520expert%2520demonstrations.%2520Particularly%252C%2520we%2520pretrain%2520the%2520vision%2520encoder%250Ausing%2520NC%2520as%2520a%2520regularization%2520to%2520encourage%2520control-oriented%2520clustering%2520of%2520the%250Avisual%2520features.%2520Surprisingly%252C%2520such%2520an%2520NC-pretrained%2520vision%2520encoder%252C%2520when%250Afinetuned%2520end-to-end%2520with%2520the%2520action%2520decoder%252C%2520boosts%2520the%2520test-time%2520performance%250Aby%252010%2525%2520to%252035%2525%2520in%2520the%2520low-data%2520regime.%2520Real-world%2520vision-based%2520planar%2520pushing%250Aexperiments%2520confirmed%2520the%2520surprising%2520advantage%2520of%2520control-oriented%2520visual%250Arepresentation%2520pretraining.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2410.05063v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Control-oriented%20Clustering%20of%20Visual%20Latent%20Representation&entry.906535625=Han%20Qi%20and%20Haocheng%20Yin%20and%20Heng%20Yang&entry.1292438233=%20%20We%20initiate%20a%20study%20of%20the%20geometry%20of%20the%20visual%20representation%20space%20--%20the%0Ainformation%20channel%20from%20the%20vision%20encoder%20to%20the%20action%20decoder%20--%20in%20an%0Aimage-based%20control%20pipeline%20learned%20from%20behavior%20cloning.%20Inspired%20by%20the%0Aphenomenon%20of%20neural%20collapse%20%28NC%29%20in%20image%20classification%2C%20we%20investigate%0Awhether%20a%20similar%20law%20of%20clustering%20emerges%20in%20the%20visual%20representation%20space.%0ASince%20image-based%20control%20is%20a%20regression%20task%20without%20explicitly%20defined%0Aclasses%2C%20the%20central%20piece%20of%20the%20puzzle%20lies%20in%20determining%20according%20to%20what%0Aimplicit%20classes%20the%20visual%20features%20cluster%2C%20if%20such%20a%20law%20exists.%20Focusing%20on%0Aimage-based%20planar%20pushing%2C%20we%20posit%20the%20most%20important%20role%20of%20the%20visual%0Arepresentation%20in%20a%20control%20task%20is%20to%20convey%20a%20goal%20to%20the%20action%20decoder.%20We%0Athen%20classify%20training%20samples%20of%20expert%20demonstrations%20into%20eight%0A%22control-oriented%22%20classes%20based%20on%20%28a%29%20the%20relative%20pose%20between%20the%20object%0Aand%20the%20target%20in%20the%20input%20or%20%28b%29%20the%20relative%20pose%20of%20the%20object%20induced%20by%0Aexpert%20actions%20in%20the%20output%2C%20where%20one%20class%20corresponds%20to%20one%20relative%20pose%0Aorthant%20%28REPO%29.%20Across%20four%20different%20instantiations%20of%20architecture%2C%20we%20report%0Athe%20prevalent%20emergence%20of%20control-oriented%20clustering%20in%20the%20visual%0Arepresentation%20space%20according%20to%20the%20eight%20REPOs.%20Beyond%20empirical%0Aobservation%2C%20we%20show%20such%20a%20law%20of%20clustering%20can%20be%20leveraged%20as%20an%0Aalgorithmic%20tool%20to%20improve%20test-time%20performance%20when%20training%20a%20policy%20with%0Alimited%20expert%20demonstrations.%20Particularly%2C%20we%20pretrain%20the%20vision%20encoder%0Ausing%20NC%20as%20a%20regularization%20to%20encourage%20control-oriented%20clustering%20of%20the%0Avisual%20features.%20Surprisingly%2C%20such%20an%20NC-pretrained%20vision%20encoder%2C%20when%0Afinetuned%20end-to-end%20with%20the%20action%20decoder%2C%20boosts%20the%20test-time%20performance%0Aby%2010%25%20to%2035%25%20in%20the%20low-data%20regime.%20Real-world%20vision-based%20planar%20pushing%0Aexperiments%20confirmed%20the%20surprising%20advantage%20of%20control-oriented%20visual%0Arepresentation%20pretraining.%0A&entry.1838667208=http%3A//arxiv.org/abs/2410.05063v1&entry.124074799=Read"},
{"title": "TextHawk2: A Large Vision-Language Model Excels in Bilingual OCR and\n  Grounding with 16x Fewer Tokens", "author": "Ya-Qi Yu and Minghui Liao and Jiwen Zhang and Jihao Wu", "abstract": "  Reading dense text and locating objects within images are fundamental\nabilities for Large Vision-Language Models (LVLMs) tasked with advanced jobs.\nPrevious LVLMs, including superior proprietary models like GPT-4o, have\nstruggled to excel in both tasks simultaneously. Moreover, previous LVLMs with\nfine-grained perception cost thousands of tokens per image, making them\nresource-intensive. We present TextHawk2, a bilingual LVLM featuring efficient\nfine-grained perception and demonstrating cutting-edge performance across\ngeneral-purpose, OCR, and grounding tasks with 16 times fewer image tokens.\nCritical improvements include: (1) Token Compression: Building on the efficient\narchitecture of its predecessor, TextHawk2 significantly reduces the number of\ntokens per image by 16 times, facilitating training and deployment of the\nTextHawk series with minimal resources. (2) Visual Encoder Reinforcement: We\nenhance the visual encoder through LVLM co-training, unlocking its potential\nfor previously unseen tasks like Chinese OCR and grounding. (3) Data Diversity:\nWe maintain a comparable scale of 100 million samples while diversifying the\nsources of pre-training data. We assess TextHawk2 across multiple benchmarks,\nwhere it consistently delivers superior performance and outperforms\nclosed-source models of similar scale, such as achieving 78.4% accuracy on\nOCRBench, 81.4% accuracy on ChartQA, 89.6% ANLS on DocVQA, and 88.1%\naccuracy@0.5 on RefCOCOg-test.\n", "link": "http://arxiv.org/abs/2410.05261v1", "date": "2024-10-07", "relevancy": 2.8399, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5741}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5741}, {"title": "ToDo: Token Downsampling for Efficient Generation of High-Resolution\n  Images", "link": "http://arxiv.org/abs/2402.13573v2", "similarity": 0.5557}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20TextHawk2%3A%20A%20Large%20Vision-Language%20Model%20Excels%20in%20Bilingual%20OCR%20and%0A%20%20Grounding%20with%2016x%20Fewer%20Tokens&body=Title%3A%20TextHawk2%3A%20A%20Large%20Vision-Language%20Model%20Excels%20in%20Bilingual%20OCR%20and%0A%20%20Grounding%20with%2016x%20Fewer%20Tokens%0AAuthor%3A%20Ya-Qi%20Yu%20and%20Minghui%20Liao%20and%20Jiwen%20Zhang%20and%20Jihao%20Wu%0AAbstract%3A%20%20%20Reading%20dense%20text%20and%20locating%20objects%20within%20images%20are%20fundamental%0Aabilities%20for%20Large%20Vision-Language%20Models%20%28LVLMs%29%20tasked%20with%20advanced%20jobs.%0APrevious%20LVLMs%2C%20including%20superior%20proprietary%20models%20like%20GPT-4o%2C%20have%0Astruggled%20to%20excel%20in%20both%20tasks%20simultaneously.%20Moreover%2C%20previous%20LVLMs%20with%0Afine-grained%20perception%20cost%20thousands%20of%20tokens%20per%20image%2C%20making%20them%0Aresource-intensive.%20We%20present%20TextHawk2%2C%20a%20bilingual%20LVLM%20featuring%20efficient%0Afine-grained%20perception%20and%20demonstrating%20cutting-edge%20performance%20across%0Ageneral-purpose%2C%20OCR%2C%20and%20grounding%20tasks%20with%2016%20times%20fewer%20image%20tokens.%0ACritical%20improvements%20include%3A%20%281%29%20Token%20Compression%3A%20Building%20on%20the%20efficient%0Aarchitecture%20of%20its%20predecessor%2C%20TextHawk2%20significantly%20reduces%20the%20number%20of%0Atokens%20per%20image%20by%2016%20times%2C%20facilitating%20training%20and%20deployment%20of%20the%0ATextHawk%20series%20with%20minimal%20resources.%20%282%29%20Visual%20Encoder%20Reinforcement%3A%20We%0Aenhance%20the%20visual%20encoder%20through%20LVLM%20co-training%2C%20unlocking%20its%20potential%0Afor%20previously%20unseen%20tasks%20like%20Chinese%20OCR%20and%20grounding.%20%283%29%20Data%20Diversity%3A%0AWe%20maintain%20a%20comparable%20scale%20of%20100%20million%20samples%20while%20diversifying%20the%0Asources%20of%20pre-training%20data.%20We%20assess%20TextHawk2%20across%20multiple%20benchmarks%2C%0Awhere%20it%20consistently%20delivers%20superior%20performance%20and%20outperforms%0Aclosed-source%20models%20of%20similar%20scale%2C%20such%20as%20achieving%2078.4%25%20accuracy%20on%0AOCRBench%2C%2081.4%25%20accuracy%20on%20ChartQA%2C%2089.6%25%20ANLS%20on%20DocVQA%2C%20and%2088.1%25%0Aaccuracy%400.5%20on%20RefCOCOg-test.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2410.05261v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DTextHawk2%253A%2520A%2520Large%2520Vision-Language%2520Model%2520Excels%2520in%2520Bilingual%2520OCR%2520and%250A%2520%2520Grounding%2520with%252016x%2520Fewer%2520Tokens%26entry.906535625%3DYa-Qi%2520Yu%2520and%2520Minghui%2520Liao%2520and%2520Jiwen%2520Zhang%2520and%2520Jihao%2520Wu%26entry.1292438233%3D%2520%2520Reading%2520dense%2520text%2520and%2520locating%2520objects%2520within%2520images%2520are%2520fundamental%250Aabilities%2520for%2520Large%2520Vision-Language%2520Models%2520%2528LVLMs%2529%2520tasked%2520with%2520advanced%2520jobs.%250APrevious%2520LVLMs%252C%2520including%2520superior%2520proprietary%2520models%2520like%2520GPT-4o%252C%2520have%250Astruggled%2520to%2520excel%2520in%2520both%2520tasks%2520simultaneously.%2520Moreover%252C%2520previous%2520LVLMs%2520with%250Afine-grained%2520perception%2520cost%2520thousands%2520of%2520tokens%2520per%2520image%252C%2520making%2520them%250Aresource-intensive.%2520We%2520present%2520TextHawk2%252C%2520a%2520bilingual%2520LVLM%2520featuring%2520efficient%250Afine-grained%2520perception%2520and%2520demonstrating%2520cutting-edge%2520performance%2520across%250Ageneral-purpose%252C%2520OCR%252C%2520and%2520grounding%2520tasks%2520with%252016%2520times%2520fewer%2520image%2520tokens.%250ACritical%2520improvements%2520include%253A%2520%25281%2529%2520Token%2520Compression%253A%2520Building%2520on%2520the%2520efficient%250Aarchitecture%2520of%2520its%2520predecessor%252C%2520TextHawk2%2520significantly%2520reduces%2520the%2520number%2520of%250Atokens%2520per%2520image%2520by%252016%2520times%252C%2520facilitating%2520training%2520and%2520deployment%2520of%2520the%250ATextHawk%2520series%2520with%2520minimal%2520resources.%2520%25282%2529%2520Visual%2520Encoder%2520Reinforcement%253A%2520We%250Aenhance%2520the%2520visual%2520encoder%2520through%2520LVLM%2520co-training%252C%2520unlocking%2520its%2520potential%250Afor%2520previously%2520unseen%2520tasks%2520like%2520Chinese%2520OCR%2520and%2520grounding.%2520%25283%2529%2520Data%2520Diversity%253A%250AWe%2520maintain%2520a%2520comparable%2520scale%2520of%2520100%2520million%2520samples%2520while%2520diversifying%2520the%250Asources%2520of%2520pre-training%2520data.%2520We%2520assess%2520TextHawk2%2520across%2520multiple%2520benchmarks%252C%250Awhere%2520it%2520consistently%2520delivers%2520superior%2520performance%2520and%2520outperforms%250Aclosed-source%2520models%2520of%2520similar%2520scale%252C%2520such%2520as%2520achieving%252078.4%2525%2520accuracy%2520on%250AOCRBench%252C%252081.4%2525%2520accuracy%2520on%2520ChartQA%252C%252089.6%2525%2520ANLS%2520on%2520DocVQA%252C%2520and%252088.1%2525%250Aaccuracy%25400.5%2520on%2520RefCOCOg-test.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2410.05261v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=TextHawk2%3A%20A%20Large%20Vision-Language%20Model%20Excels%20in%20Bilingual%20OCR%20and%0A%20%20Grounding%20with%2016x%20Fewer%20Tokens&entry.906535625=Ya-Qi%20Yu%20and%20Minghui%20Liao%20and%20Jiwen%20Zhang%20and%20Jihao%20Wu&entry.1292438233=%20%20Reading%20dense%20text%20and%20locating%20objects%20within%20images%20are%20fundamental%0Aabilities%20for%20Large%20Vision-Language%20Models%20%28LVLMs%29%20tasked%20with%20advanced%20jobs.%0APrevious%20LVLMs%2C%20including%20superior%20proprietary%20models%20like%20GPT-4o%2C%20have%0Astruggled%20to%20excel%20in%20both%20tasks%20simultaneously.%20Moreover%2C%20previous%20LVLMs%20with%0Afine-grained%20perception%20cost%20thousands%20of%20tokens%20per%20image%2C%20making%20them%0Aresource-intensive.%20We%20present%20TextHawk2%2C%20a%20bilingual%20LVLM%20featuring%20efficient%0Afine-grained%20perception%20and%20demonstrating%20cutting-edge%20performance%20across%0Ageneral-purpose%2C%20OCR%2C%20and%20grounding%20tasks%20with%2016%20times%20fewer%20image%20tokens.%0ACritical%20improvements%20include%3A%20%281%29%20Token%20Compression%3A%20Building%20on%20the%20efficient%0Aarchitecture%20of%20its%20predecessor%2C%20TextHawk2%20significantly%20reduces%20the%20number%20of%0Atokens%20per%20image%20by%2016%20times%2C%20facilitating%20training%20and%20deployment%20of%20the%0ATextHawk%20series%20with%20minimal%20resources.%20%282%29%20Visual%20Encoder%20Reinforcement%3A%20We%0Aenhance%20the%20visual%20encoder%20through%20LVLM%20co-training%2C%20unlocking%20its%20potential%0Afor%20previously%20unseen%20tasks%20like%20Chinese%20OCR%20and%20grounding.%20%283%29%20Data%20Diversity%3A%0AWe%20maintain%20a%20comparable%20scale%20of%20100%20million%20samples%20while%20diversifying%20the%0Asources%20of%20pre-training%20data.%20We%20assess%20TextHawk2%20across%20multiple%20benchmarks%2C%0Awhere%20it%20consistently%20delivers%20superior%20performance%20and%20outperforms%0Aclosed-source%20models%20of%20similar%20scale%2C%20such%20as%20achieving%2078.4%25%20accuracy%20on%0AOCRBench%2C%2081.4%25%20accuracy%20on%20ChartQA%2C%2089.6%25%20ANLS%20on%20DocVQA%2C%20and%2088.1%25%0Aaccuracy%400.5%20on%20RefCOCOg-test.%0A&entry.1838667208=http%3A//arxiv.org/abs/2410.05261v1&entry.124074799=Read"},
{"title": "Visual Question Decomposition on Multimodal Large Language Models", "author": "Haowei Zhang and Jianzhe Liu and Zhen Han and Shuo Chen and Bailan He and Volker Tresp and Zhiqiang Xu and Jindong Gu", "abstract": "  Question decomposition has emerged as an effective strategy for prompting\nLarge Language Models (LLMs) to answer complex questions. However, while\nexisting methods primarily focus on unimodal language models, the question\ndecomposition capability of Multimodal Large Language Models (MLLMs) has yet to\nbe explored. To this end, this paper explores visual question decomposition on\nMLLMs. Specifically, we introduce a systematic evaluation framework including a\ndataset and several evaluation criteria to assess the quality of the decomposed\nsub-questions, revealing that existing MLLMs struggle to produce high-quality\nsub-questions. To address this limitation, we propose a specific finetuning\ndataset, DecoVQA+, for enhancing the model's question decomposition capability.\nAiming at enabling models to perform appropriate selective decomposition, we\npropose an efficient finetuning pipeline. The finetuning pipeline consists of\nour proposed dataset and a training objective for selective decomposition.\nFinetuned MLLMs demonstrate significant improvements in the quality of\nsub-questions and the policy of selective question decomposition. Additionally,\nthe models also achieve higher accuracy with selective decomposition on VQA\nbenchmark datasets.\n", "link": "http://arxiv.org/abs/2409.19339v2", "date": "2024-10-07", "relevancy": 2.8191, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5928}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5928}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5059}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Visual%20Question%20Decomposition%20on%20Multimodal%20Large%20Language%20Models&body=Title%3A%20Visual%20Question%20Decomposition%20on%20Multimodal%20Large%20Language%20Models%0AAuthor%3A%20Haowei%20Zhang%20and%20Jianzhe%20Liu%20and%20Zhen%20Han%20and%20Shuo%20Chen%20and%20Bailan%20He%20and%20Volker%20Tresp%20and%20Zhiqiang%20Xu%20and%20Jindong%20Gu%0AAbstract%3A%20%20%20Question%20decomposition%20has%20emerged%20as%20an%20effective%20strategy%20for%20prompting%0ALarge%20Language%20Models%20%28LLMs%29%20to%20answer%20complex%20questions.%20However%2C%20while%0Aexisting%20methods%20primarily%20focus%20on%20unimodal%20language%20models%2C%20the%20question%0Adecomposition%20capability%20of%20Multimodal%20Large%20Language%20Models%20%28MLLMs%29%20has%20yet%20to%0Abe%20explored.%20To%20this%20end%2C%20this%20paper%20explores%20visual%20question%20decomposition%20on%0AMLLMs.%20Specifically%2C%20we%20introduce%20a%20systematic%20evaluation%20framework%20including%20a%0Adataset%20and%20several%20evaluation%20criteria%20to%20assess%20the%20quality%20of%20the%20decomposed%0Asub-questions%2C%20revealing%20that%20existing%20MLLMs%20struggle%20to%20produce%20high-quality%0Asub-questions.%20To%20address%20this%20limitation%2C%20we%20propose%20a%20specific%20finetuning%0Adataset%2C%20DecoVQA%2B%2C%20for%20enhancing%20the%20model%27s%20question%20decomposition%20capability.%0AAiming%20at%20enabling%20models%20to%20perform%20appropriate%20selective%20decomposition%2C%20we%0Apropose%20an%20efficient%20finetuning%20pipeline.%20The%20finetuning%20pipeline%20consists%20of%0Aour%20proposed%20dataset%20and%20a%20training%20objective%20for%20selective%20decomposition.%0AFinetuned%20MLLMs%20demonstrate%20significant%20improvements%20in%20the%20quality%20of%0Asub-questions%20and%20the%20policy%20of%20selective%20question%20decomposition.%20Additionally%2C%0Athe%20models%20also%20achieve%20higher%20accuracy%20with%20selective%20decomposition%20on%20VQA%0Abenchmark%20datasets.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2409.19339v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DVisual%2520Question%2520Decomposition%2520on%2520Multimodal%2520Large%2520Language%2520Models%26entry.906535625%3DHaowei%2520Zhang%2520and%2520Jianzhe%2520Liu%2520and%2520Zhen%2520Han%2520and%2520Shuo%2520Chen%2520and%2520Bailan%2520He%2520and%2520Volker%2520Tresp%2520and%2520Zhiqiang%2520Xu%2520and%2520Jindong%2520Gu%26entry.1292438233%3D%2520%2520Question%2520decomposition%2520has%2520emerged%2520as%2520an%2520effective%2520strategy%2520for%2520prompting%250ALarge%2520Language%2520Models%2520%2528LLMs%2529%2520to%2520answer%2520complex%2520questions.%2520However%252C%2520while%250Aexisting%2520methods%2520primarily%2520focus%2520on%2520unimodal%2520language%2520models%252C%2520the%2520question%250Adecomposition%2520capability%2520of%2520Multimodal%2520Large%2520Language%2520Models%2520%2528MLLMs%2529%2520has%2520yet%2520to%250Abe%2520explored.%2520To%2520this%2520end%252C%2520this%2520paper%2520explores%2520visual%2520question%2520decomposition%2520on%250AMLLMs.%2520Specifically%252C%2520we%2520introduce%2520a%2520systematic%2520evaluation%2520framework%2520including%2520a%250Adataset%2520and%2520several%2520evaluation%2520criteria%2520to%2520assess%2520the%2520quality%2520of%2520the%2520decomposed%250Asub-questions%252C%2520revealing%2520that%2520existing%2520MLLMs%2520struggle%2520to%2520produce%2520high-quality%250Asub-questions.%2520To%2520address%2520this%2520limitation%252C%2520we%2520propose%2520a%2520specific%2520finetuning%250Adataset%252C%2520DecoVQA%252B%252C%2520for%2520enhancing%2520the%2520model%2527s%2520question%2520decomposition%2520capability.%250AAiming%2520at%2520enabling%2520models%2520to%2520perform%2520appropriate%2520selective%2520decomposition%252C%2520we%250Apropose%2520an%2520efficient%2520finetuning%2520pipeline.%2520The%2520finetuning%2520pipeline%2520consists%2520of%250Aour%2520proposed%2520dataset%2520and%2520a%2520training%2520objective%2520for%2520selective%2520decomposition.%250AFinetuned%2520MLLMs%2520demonstrate%2520significant%2520improvements%2520in%2520the%2520quality%2520of%250Asub-questions%2520and%2520the%2520policy%2520of%2520selective%2520question%2520decomposition.%2520Additionally%252C%250Athe%2520models%2520also%2520achieve%2520higher%2520accuracy%2520with%2520selective%2520decomposition%2520on%2520VQA%250Abenchmark%2520datasets.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2409.19339v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Visual%20Question%20Decomposition%20on%20Multimodal%20Large%20Language%20Models&entry.906535625=Haowei%20Zhang%20and%20Jianzhe%20Liu%20and%20Zhen%20Han%20and%20Shuo%20Chen%20and%20Bailan%20He%20and%20Volker%20Tresp%20and%20Zhiqiang%20Xu%20and%20Jindong%20Gu&entry.1292438233=%20%20Question%20decomposition%20has%20emerged%20as%20an%20effective%20strategy%20for%20prompting%0ALarge%20Language%20Models%20%28LLMs%29%20to%20answer%20complex%20questions.%20However%2C%20while%0Aexisting%20methods%20primarily%20focus%20on%20unimodal%20language%20models%2C%20the%20question%0Adecomposition%20capability%20of%20Multimodal%20Large%20Language%20Models%20%28MLLMs%29%20has%20yet%20to%0Abe%20explored.%20To%20this%20end%2C%20this%20paper%20explores%20visual%20question%20decomposition%20on%0AMLLMs.%20Specifically%2C%20we%20introduce%20a%20systematic%20evaluation%20framework%20including%20a%0Adataset%20and%20several%20evaluation%20criteria%20to%20assess%20the%20quality%20of%20the%20decomposed%0Asub-questions%2C%20revealing%20that%20existing%20MLLMs%20struggle%20to%20produce%20high-quality%0Asub-questions.%20To%20address%20this%20limitation%2C%20we%20propose%20a%20specific%20finetuning%0Adataset%2C%20DecoVQA%2B%2C%20for%20enhancing%20the%20model%27s%20question%20decomposition%20capability.%0AAiming%20at%20enabling%20models%20to%20perform%20appropriate%20selective%20decomposition%2C%20we%0Apropose%20an%20efficient%20finetuning%20pipeline.%20The%20finetuning%20pipeline%20consists%20of%0Aour%20proposed%20dataset%20and%20a%20training%20objective%20for%20selective%20decomposition.%0AFinetuned%20MLLMs%20demonstrate%20significant%20improvements%20in%20the%20quality%20of%0Asub-questions%20and%20the%20policy%20of%20selective%20question%20decomposition.%20Additionally%2C%0Athe%20models%20also%20achieve%20higher%20accuracy%20with%20selective%20decomposition%20on%20VQA%0Abenchmark%20datasets.%0A&entry.1838667208=http%3A//arxiv.org/abs/2409.19339v2&entry.124074799=Read"},
{"title": "Organizing Unstructured Image Collections using Natural Language", "author": "Mingxuan Liu and Zhun Zhong and Jun Li and Gianni Franchi and Subhankar Roy and Elisa Ricci", "abstract": "  Organizing unstructured visual data into semantic clusters is a key challenge\nin computer vision. Traditional deep clustering (DC) approaches focus on a\nsingle partition of data, while multiple clustering (MC) methods address this\nlimitation by uncovering distinct clustering solutions. The rise of large\nlanguage models (LLMs) and multimodal LLMs (MLLMs) has enhanced MC by allowing\nusers to define clustering criteria in natural language. However, manually\nspecifying criteria for large datasets is impractical. In this work, we\nintroduce the task Semantic Multiple Clustering (SMC) that aims to\nautomatically discover clustering criteria from large image collections,\nuncovering interpretable substructures without requiring human input. Our\nframework, Text Driven Semantic Multiple Clustering (TeDeSC), uses text as a\nproxy to concurrently reason over large image collections, discover\npartitioning criteria, expressed in natural language, and reveal semantic\nsubstructures. To evaluate TeDeSC, we introduce the COCO-4c and Food-4c\nbenchmarks, each containing four grouping criteria and ground-truth\nannotations. We apply TeDeSC to various applications, such as discovering\nbiases and analyzing social media image popularity, demonstrating its utility\nas a tool for automatically organizing image collections and revealing novel\ninsights.\n", "link": "http://arxiv.org/abs/2410.05217v1", "date": "2024-10-07", "relevancy": 2.7719, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5553}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5553}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5526}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Organizing%20Unstructured%20Image%20Collections%20using%20Natural%20Language&body=Title%3A%20Organizing%20Unstructured%20Image%20Collections%20using%20Natural%20Language%0AAuthor%3A%20Mingxuan%20Liu%20and%20Zhun%20Zhong%20and%20Jun%20Li%20and%20Gianni%20Franchi%20and%20Subhankar%20Roy%20and%20Elisa%20Ricci%0AAbstract%3A%20%20%20Organizing%20unstructured%20visual%20data%20into%20semantic%20clusters%20is%20a%20key%20challenge%0Ain%20computer%20vision.%20Traditional%20deep%20clustering%20%28DC%29%20approaches%20focus%20on%20a%0Asingle%20partition%20of%20data%2C%20while%20multiple%20clustering%20%28MC%29%20methods%20address%20this%0Alimitation%20by%20uncovering%20distinct%20clustering%20solutions.%20The%20rise%20of%20large%0Alanguage%20models%20%28LLMs%29%20and%20multimodal%20LLMs%20%28MLLMs%29%20has%20enhanced%20MC%20by%20allowing%0Ausers%20to%20define%20clustering%20criteria%20in%20natural%20language.%20However%2C%20manually%0Aspecifying%20criteria%20for%20large%20datasets%20is%20impractical.%20In%20this%20work%2C%20we%0Aintroduce%20the%20task%20Semantic%20Multiple%20Clustering%20%28SMC%29%20that%20aims%20to%0Aautomatically%20discover%20clustering%20criteria%20from%20large%20image%20collections%2C%0Auncovering%20interpretable%20substructures%20without%20requiring%20human%20input.%20Our%0Aframework%2C%20Text%20Driven%20Semantic%20Multiple%20Clustering%20%28TeDeSC%29%2C%20uses%20text%20as%20a%0Aproxy%20to%20concurrently%20reason%20over%20large%20image%20collections%2C%20discover%0Apartitioning%20criteria%2C%20expressed%20in%20natural%20language%2C%20and%20reveal%20semantic%0Asubstructures.%20To%20evaluate%20TeDeSC%2C%20we%20introduce%20the%20COCO-4c%20and%20Food-4c%0Abenchmarks%2C%20each%20containing%20four%20grouping%20criteria%20and%20ground-truth%0Aannotations.%20We%20apply%20TeDeSC%20to%20various%20applications%2C%20such%20as%20discovering%0Abiases%20and%20analyzing%20social%20media%20image%20popularity%2C%20demonstrating%20its%20utility%0Aas%20a%20tool%20for%20automatically%20organizing%20image%20collections%20and%20revealing%20novel%0Ainsights.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2410.05217v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DOrganizing%2520Unstructured%2520Image%2520Collections%2520using%2520Natural%2520Language%26entry.906535625%3DMingxuan%2520Liu%2520and%2520Zhun%2520Zhong%2520and%2520Jun%2520Li%2520and%2520Gianni%2520Franchi%2520and%2520Subhankar%2520Roy%2520and%2520Elisa%2520Ricci%26entry.1292438233%3D%2520%2520Organizing%2520unstructured%2520visual%2520data%2520into%2520semantic%2520clusters%2520is%2520a%2520key%2520challenge%250Ain%2520computer%2520vision.%2520Traditional%2520deep%2520clustering%2520%2528DC%2529%2520approaches%2520focus%2520on%2520a%250Asingle%2520partition%2520of%2520data%252C%2520while%2520multiple%2520clustering%2520%2528MC%2529%2520methods%2520address%2520this%250Alimitation%2520by%2520uncovering%2520distinct%2520clustering%2520solutions.%2520The%2520rise%2520of%2520large%250Alanguage%2520models%2520%2528LLMs%2529%2520and%2520multimodal%2520LLMs%2520%2528MLLMs%2529%2520has%2520enhanced%2520MC%2520by%2520allowing%250Ausers%2520to%2520define%2520clustering%2520criteria%2520in%2520natural%2520language.%2520However%252C%2520manually%250Aspecifying%2520criteria%2520for%2520large%2520datasets%2520is%2520impractical.%2520In%2520this%2520work%252C%2520we%250Aintroduce%2520the%2520task%2520Semantic%2520Multiple%2520Clustering%2520%2528SMC%2529%2520that%2520aims%2520to%250Aautomatically%2520discover%2520clustering%2520criteria%2520from%2520large%2520image%2520collections%252C%250Auncovering%2520interpretable%2520substructures%2520without%2520requiring%2520human%2520input.%2520Our%250Aframework%252C%2520Text%2520Driven%2520Semantic%2520Multiple%2520Clustering%2520%2528TeDeSC%2529%252C%2520uses%2520text%2520as%2520a%250Aproxy%2520to%2520concurrently%2520reason%2520over%2520large%2520image%2520collections%252C%2520discover%250Apartitioning%2520criteria%252C%2520expressed%2520in%2520natural%2520language%252C%2520and%2520reveal%2520semantic%250Asubstructures.%2520To%2520evaluate%2520TeDeSC%252C%2520we%2520introduce%2520the%2520COCO-4c%2520and%2520Food-4c%250Abenchmarks%252C%2520each%2520containing%2520four%2520grouping%2520criteria%2520and%2520ground-truth%250Aannotations.%2520We%2520apply%2520TeDeSC%2520to%2520various%2520applications%252C%2520such%2520as%2520discovering%250Abiases%2520and%2520analyzing%2520social%2520media%2520image%2520popularity%252C%2520demonstrating%2520its%2520utility%250Aas%2520a%2520tool%2520for%2520automatically%2520organizing%2520image%2520collections%2520and%2520revealing%2520novel%250Ainsights.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2410.05217v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Organizing%20Unstructured%20Image%20Collections%20using%20Natural%20Language&entry.906535625=Mingxuan%20Liu%20and%20Zhun%20Zhong%20and%20Jun%20Li%20and%20Gianni%20Franchi%20and%20Subhankar%20Roy%20and%20Elisa%20Ricci&entry.1292438233=%20%20Organizing%20unstructured%20visual%20data%20into%20semantic%20clusters%20is%20a%20key%20challenge%0Ain%20computer%20vision.%20Traditional%20deep%20clustering%20%28DC%29%20approaches%20focus%20on%20a%0Asingle%20partition%20of%20data%2C%20while%20multiple%20clustering%20%28MC%29%20methods%20address%20this%0Alimitation%20by%20uncovering%20distinct%20clustering%20solutions.%20The%20rise%20of%20large%0Alanguage%20models%20%28LLMs%29%20and%20multimodal%20LLMs%20%28MLLMs%29%20has%20enhanced%20MC%20by%20allowing%0Ausers%20to%20define%20clustering%20criteria%20in%20natural%20language.%20However%2C%20manually%0Aspecifying%20criteria%20for%20large%20datasets%20is%20impractical.%20In%20this%20work%2C%20we%0Aintroduce%20the%20task%20Semantic%20Multiple%20Clustering%20%28SMC%29%20that%20aims%20to%0Aautomatically%20discover%20clustering%20criteria%20from%20large%20image%20collections%2C%0Auncovering%20interpretable%20substructures%20without%20requiring%20human%20input.%20Our%0Aframework%2C%20Text%20Driven%20Semantic%20Multiple%20Clustering%20%28TeDeSC%29%2C%20uses%20text%20as%20a%0Aproxy%20to%20concurrently%20reason%20over%20large%20image%20collections%2C%20discover%0Apartitioning%20criteria%2C%20expressed%20in%20natural%20language%2C%20and%20reveal%20semantic%0Asubstructures.%20To%20evaluate%20TeDeSC%2C%20we%20introduce%20the%20COCO-4c%20and%20Food-4c%0Abenchmarks%2C%20each%20containing%20four%20grouping%20criteria%20and%20ground-truth%0Aannotations.%20We%20apply%20TeDeSC%20to%20various%20applications%2C%20such%20as%20discovering%0Abiases%20and%20analyzing%20social%20media%20image%20popularity%2C%20demonstrating%20its%20utility%0Aas%20a%20tool%20for%20automatically%20organizing%20image%20collections%20and%20revealing%20novel%0Ainsights.%0A&entry.1838667208=http%3A//arxiv.org/abs/2410.05217v1&entry.124074799=Read"},
{"title": "Real-time Ship Recognition and Georeferencing for the Improvement of\n  Maritime Situational Awareness", "author": "Borja Carrillo Perez", "abstract": "  In an era where maritime infrastructures are crucial, advanced situational\nawareness solutions are increasingly important. The use of optical camera\nsystems can allow real-time usage of maritime footage. This thesis presents an\ninvestigation into leveraging deep learning and computer vision to advance\nreal-time ship recognition and georeferencing for the improvement of maritime\nsituational awareness. A novel dataset, ShipSG, is introduced, containing 3,505\nimages and 11,625 ship masks with corresponding class and geographic position.\nAfter an exploration of state-of-the-art, a custom real-time segmentation\narchitecture, ScatYOLOv8+CBAM, is designed for the NVIDIA Jetson AGX Xavier\nembedded system. This architecture adds the 2D scattering transform and\nattention mechanisms to YOLOv8, achieving an mAP of 75.46% and an 25.3 ms per\nframe, outperforming state-of-the-art methods by over 5%. To improve small and\ndistant ship recognition in high-resolution images on embedded systems, an\nenhanced slicing mechanism is introduced, improving mAP by 8% to 11%.\nAdditionally, a georeferencing method is proposed, achieving positioning errors\nof 18 m for ships up to 400 m away and 44 m for ships between 400 m and 1200 m.\nThe findings are also applied in real-world scenarios, such as the detection of\nabnormal ship behaviour, camera integrity assessment and 3D reconstruction. The\napproach of this thesis outperforms existing methods and provides a framework\nfor integrating recognized and georeferenced ships into real-time systems,\nenhancing operational effectiveness and decision-making for maritime\nstakeholders. This thesis contributes to the maritime computer vision field by\nestablishing a benchmark for ship segmentation and georeferencing research,\ndemonstrating the viability of deep-learning-based recognition and\ngeoreferencing methods for real-time maritime monitoring.\n", "link": "http://arxiv.org/abs/2410.04946v1", "date": "2024-10-07", "relevancy": 2.7561, "topK": [{"title": "RTG-SLAM: Real-time 3D Reconstruction at Scale using Gaussian Splatting", "link": "http://arxiv.org/abs/2404.19706v1", "similarity": 0.5686}, {"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.5478}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5373}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Real-time%20Ship%20Recognition%20and%20Georeferencing%20for%20the%20Improvement%20of%0A%20%20Maritime%20Situational%20Awareness&body=Title%3A%20Real-time%20Ship%20Recognition%20and%20Georeferencing%20for%20the%20Improvement%20of%0A%20%20Maritime%20Situational%20Awareness%0AAuthor%3A%20Borja%20Carrillo%20Perez%0AAbstract%3A%20%20%20In%20an%20era%20where%20maritime%20infrastructures%20are%20crucial%2C%20advanced%20situational%0Aawareness%20solutions%20are%20increasingly%20important.%20The%20use%20of%20optical%20camera%0Asystems%20can%20allow%20real-time%20usage%20of%20maritime%20footage.%20This%20thesis%20presents%20an%0Ainvestigation%20into%20leveraging%20deep%20learning%20and%20computer%20vision%20to%20advance%0Areal-time%20ship%20recognition%20and%20georeferencing%20for%20the%20improvement%20of%20maritime%0Asituational%20awareness.%20A%20novel%20dataset%2C%20ShipSG%2C%20is%20introduced%2C%20containing%203%2C505%0Aimages%20and%2011%2C625%20ship%20masks%20with%20corresponding%20class%20and%20geographic%20position.%0AAfter%20an%20exploration%20of%20state-of-the-art%2C%20a%20custom%20real-time%20segmentation%0Aarchitecture%2C%20ScatYOLOv8%2BCBAM%2C%20is%20designed%20for%20the%20NVIDIA%20Jetson%20AGX%20Xavier%0Aembedded%20system.%20This%20architecture%20adds%20the%202D%20scattering%20transform%20and%0Aattention%20mechanisms%20to%20YOLOv8%2C%20achieving%20an%20mAP%20of%2075.46%25%20and%20an%2025.3%20ms%20per%0Aframe%2C%20outperforming%20state-of-the-art%20methods%20by%20over%205%25.%20To%20improve%20small%20and%0Adistant%20ship%20recognition%20in%20high-resolution%20images%20on%20embedded%20systems%2C%20an%0Aenhanced%20slicing%20mechanism%20is%20introduced%2C%20improving%20mAP%20by%208%25%20to%2011%25.%0AAdditionally%2C%20a%20georeferencing%20method%20is%20proposed%2C%20achieving%20positioning%20errors%0Aof%2018%20m%20for%20ships%20up%20to%20400%20m%20away%20and%2044%20m%20for%20ships%20between%20400%20m%20and%201200%20m.%0AThe%20findings%20are%20also%20applied%20in%20real-world%20scenarios%2C%20such%20as%20the%20detection%20of%0Aabnormal%20ship%20behaviour%2C%20camera%20integrity%20assessment%20and%203D%20reconstruction.%20The%0Aapproach%20of%20this%20thesis%20outperforms%20existing%20methods%20and%20provides%20a%20framework%0Afor%20integrating%20recognized%20and%20georeferenced%20ships%20into%20real-time%20systems%2C%0Aenhancing%20operational%20effectiveness%20and%20decision-making%20for%20maritime%0Astakeholders.%20This%20thesis%20contributes%20to%20the%20maritime%20computer%20vision%20field%20by%0Aestablishing%20a%20benchmark%20for%20ship%20segmentation%20and%20georeferencing%20research%2C%0Ademonstrating%20the%20viability%20of%20deep-learning-based%20recognition%20and%0Ageoreferencing%20methods%20for%20real-time%20maritime%20monitoring.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2410.04946v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DReal-time%2520Ship%2520Recognition%2520and%2520Georeferencing%2520for%2520the%2520Improvement%2520of%250A%2520%2520Maritime%2520Situational%2520Awareness%26entry.906535625%3DBorja%2520Carrillo%2520Perez%26entry.1292438233%3D%2520%2520In%2520an%2520era%2520where%2520maritime%2520infrastructures%2520are%2520crucial%252C%2520advanced%2520situational%250Aawareness%2520solutions%2520are%2520increasingly%2520important.%2520The%2520use%2520of%2520optical%2520camera%250Asystems%2520can%2520allow%2520real-time%2520usage%2520of%2520maritime%2520footage.%2520This%2520thesis%2520presents%2520an%250Ainvestigation%2520into%2520leveraging%2520deep%2520learning%2520and%2520computer%2520vision%2520to%2520advance%250Areal-time%2520ship%2520recognition%2520and%2520georeferencing%2520for%2520the%2520improvement%2520of%2520maritime%250Asituational%2520awareness.%2520A%2520novel%2520dataset%252C%2520ShipSG%252C%2520is%2520introduced%252C%2520containing%25203%252C505%250Aimages%2520and%252011%252C625%2520ship%2520masks%2520with%2520corresponding%2520class%2520and%2520geographic%2520position.%250AAfter%2520an%2520exploration%2520of%2520state-of-the-art%252C%2520a%2520custom%2520real-time%2520segmentation%250Aarchitecture%252C%2520ScatYOLOv8%252BCBAM%252C%2520is%2520designed%2520for%2520the%2520NVIDIA%2520Jetson%2520AGX%2520Xavier%250Aembedded%2520system.%2520This%2520architecture%2520adds%2520the%25202D%2520scattering%2520transform%2520and%250Aattention%2520mechanisms%2520to%2520YOLOv8%252C%2520achieving%2520an%2520mAP%2520of%252075.46%2525%2520and%2520an%252025.3%2520ms%2520per%250Aframe%252C%2520outperforming%2520state-of-the-art%2520methods%2520by%2520over%25205%2525.%2520To%2520improve%2520small%2520and%250Adistant%2520ship%2520recognition%2520in%2520high-resolution%2520images%2520on%2520embedded%2520systems%252C%2520an%250Aenhanced%2520slicing%2520mechanism%2520is%2520introduced%252C%2520improving%2520mAP%2520by%25208%2525%2520to%252011%2525.%250AAdditionally%252C%2520a%2520georeferencing%2520method%2520is%2520proposed%252C%2520achieving%2520positioning%2520errors%250Aof%252018%2520m%2520for%2520ships%2520up%2520to%2520400%2520m%2520away%2520and%252044%2520m%2520for%2520ships%2520between%2520400%2520m%2520and%25201200%2520m.%250AThe%2520findings%2520are%2520also%2520applied%2520in%2520real-world%2520scenarios%252C%2520such%2520as%2520the%2520detection%2520of%250Aabnormal%2520ship%2520behaviour%252C%2520camera%2520integrity%2520assessment%2520and%25203D%2520reconstruction.%2520The%250Aapproach%2520of%2520this%2520thesis%2520outperforms%2520existing%2520methods%2520and%2520provides%2520a%2520framework%250Afor%2520integrating%2520recognized%2520and%2520georeferenced%2520ships%2520into%2520real-time%2520systems%252C%250Aenhancing%2520operational%2520effectiveness%2520and%2520decision-making%2520for%2520maritime%250Astakeholders.%2520This%2520thesis%2520contributes%2520to%2520the%2520maritime%2520computer%2520vision%2520field%2520by%250Aestablishing%2520a%2520benchmark%2520for%2520ship%2520segmentation%2520and%2520georeferencing%2520research%252C%250Ademonstrating%2520the%2520viability%2520of%2520deep-learning-based%2520recognition%2520and%250Ageoreferencing%2520methods%2520for%2520real-time%2520maritime%2520monitoring.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2410.04946v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Real-time%20Ship%20Recognition%20and%20Georeferencing%20for%20the%20Improvement%20of%0A%20%20Maritime%20Situational%20Awareness&entry.906535625=Borja%20Carrillo%20Perez&entry.1292438233=%20%20In%20an%20era%20where%20maritime%20infrastructures%20are%20crucial%2C%20advanced%20situational%0Aawareness%20solutions%20are%20increasingly%20important.%20The%20use%20of%20optical%20camera%0Asystems%20can%20allow%20real-time%20usage%20of%20maritime%20footage.%20This%20thesis%20presents%20an%0Ainvestigation%20into%20leveraging%20deep%20learning%20and%20computer%20vision%20to%20advance%0Areal-time%20ship%20recognition%20and%20georeferencing%20for%20the%20improvement%20of%20maritime%0Asituational%20awareness.%20A%20novel%20dataset%2C%20ShipSG%2C%20is%20introduced%2C%20containing%203%2C505%0Aimages%20and%2011%2C625%20ship%20masks%20with%20corresponding%20class%20and%20geographic%20position.%0AAfter%20an%20exploration%20of%20state-of-the-art%2C%20a%20custom%20real-time%20segmentation%0Aarchitecture%2C%20ScatYOLOv8%2BCBAM%2C%20is%20designed%20for%20the%20NVIDIA%20Jetson%20AGX%20Xavier%0Aembedded%20system.%20This%20architecture%20adds%20the%202D%20scattering%20transform%20and%0Aattention%20mechanisms%20to%20YOLOv8%2C%20achieving%20an%20mAP%20of%2075.46%25%20and%20an%2025.3%20ms%20per%0Aframe%2C%20outperforming%20state-of-the-art%20methods%20by%20over%205%25.%20To%20improve%20small%20and%0Adistant%20ship%20recognition%20in%20high-resolution%20images%20on%20embedded%20systems%2C%20an%0Aenhanced%20slicing%20mechanism%20is%20introduced%2C%20improving%20mAP%20by%208%25%20to%2011%25.%0AAdditionally%2C%20a%20georeferencing%20method%20is%20proposed%2C%20achieving%20positioning%20errors%0Aof%2018%20m%20for%20ships%20up%20to%20400%20m%20away%20and%2044%20m%20for%20ships%20between%20400%20m%20and%201200%20m.%0AThe%20findings%20are%20also%20applied%20in%20real-world%20scenarios%2C%20such%20as%20the%20detection%20of%0Aabnormal%20ship%20behaviour%2C%20camera%20integrity%20assessment%20and%203D%20reconstruction.%20The%0Aapproach%20of%20this%20thesis%20outperforms%20existing%20methods%20and%20provides%20a%20framework%0Afor%20integrating%20recognized%20and%20georeferenced%20ships%20into%20real-time%20systems%2C%0Aenhancing%20operational%20effectiveness%20and%20decision-making%20for%20maritime%0Astakeholders.%20This%20thesis%20contributes%20to%20the%20maritime%20computer%20vision%20field%20by%0Aestablishing%20a%20benchmark%20for%20ship%20segmentation%20and%20georeferencing%20research%2C%0Ademonstrating%20the%20viability%20of%20deep-learning-based%20recognition%20and%0Ageoreferencing%20methods%20for%20real-time%20maritime%20monitoring.%0A&entry.1838667208=http%3A//arxiv.org/abs/2410.04946v1&entry.124074799=Read"},
{"title": "LoTLIP: Improving Language-Image Pre-training for Long Text\n  Understanding", "author": "Wei Wu and Kecheng Zheng and Shuailei Ma and Fan Lu and Yuxin Guo and Yifei Zhang and Wei Chen and Qingpei Guo and Yujun Shen and Zheng-Jun Zha", "abstract": "  Understanding long text is of great demands in practice but beyond the reach\nof most language-image pre-training (LIP) models. In this work, we empirically\nconfirm that the key reason causing such an issue is that the training images\nare usually paired with short captions, leaving certain tokens easily\novershadowed by salient tokens. Towards this problem, our initial attempt is to\nrelabel the data with long captions, however, directly learning with which may\nlead to performance degradation in understanding short text (e.g., in the image\nclassification task). Then, after incorporating corner tokens to aggregate\ndiverse textual information, we manage to help the model catch up to its\noriginal level of short text understanding yet greatly enhance its capability\nof long text understanding. We further look into whether the model can\ncontinuously benefit from longer captions and notice a clear trade-off between\nthe performance and the efficiency. Finally, we validate the effectiveness of\nour approach using a self-constructed large-scale dataset, which consists of\n100M long caption oriented text-image pairs. It is noteworthy that, on the task\nof long-text image retrieval, we beat the competitor using long captions with\n11.1% improvement (i.e., from 72.62% to 83.72%). We will release the code, the\nmodel, and the new dataset to facilitate the reproducibility and further\nresearch. The project page is available at https://wuw2019.github.io/lotlip.\n", "link": "http://arxiv.org/abs/2410.05249v1", "date": "2024-10-07", "relevancy": 2.7266, "topK": [{"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5784}, {"title": "Self-supervised Photographic Image Layout Representation Learning", "link": "http://arxiv.org/abs/2403.03740v1", "similarity": 0.5346}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5229}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20LoTLIP%3A%20Improving%20Language-Image%20Pre-training%20for%20Long%20Text%0A%20%20Understanding&body=Title%3A%20LoTLIP%3A%20Improving%20Language-Image%20Pre-training%20for%20Long%20Text%0A%20%20Understanding%0AAuthor%3A%20Wei%20Wu%20and%20Kecheng%20Zheng%20and%20Shuailei%20Ma%20and%20Fan%20Lu%20and%20Yuxin%20Guo%20and%20Yifei%20Zhang%20and%20Wei%20Chen%20and%20Qingpei%20Guo%20and%20Yujun%20Shen%20and%20Zheng-Jun%20Zha%0AAbstract%3A%20%20%20Understanding%20long%20text%20is%20of%20great%20demands%20in%20practice%20but%20beyond%20the%20reach%0Aof%20most%20language-image%20pre-training%20%28LIP%29%20models.%20In%20this%20work%2C%20we%20empirically%0Aconfirm%20that%20the%20key%20reason%20causing%20such%20an%20issue%20is%20that%20the%20training%20images%0Aare%20usually%20paired%20with%20short%20captions%2C%20leaving%20certain%20tokens%20easily%0Aovershadowed%20by%20salient%20tokens.%20Towards%20this%20problem%2C%20our%20initial%20attempt%20is%20to%0Arelabel%20the%20data%20with%20long%20captions%2C%20however%2C%20directly%20learning%20with%20which%20may%0Alead%20to%20performance%20degradation%20in%20understanding%20short%20text%20%28e.g.%2C%20in%20the%20image%0Aclassification%20task%29.%20Then%2C%20after%20incorporating%20corner%20tokens%20to%20aggregate%0Adiverse%20textual%20information%2C%20we%20manage%20to%20help%20the%20model%20catch%20up%20to%20its%0Aoriginal%20level%20of%20short%20text%20understanding%20yet%20greatly%20enhance%20its%20capability%0Aof%20long%20text%20understanding.%20We%20further%20look%20into%20whether%20the%20model%20can%0Acontinuously%20benefit%20from%20longer%20captions%20and%20notice%20a%20clear%20trade-off%20between%0Athe%20performance%20and%20the%20efficiency.%20Finally%2C%20we%20validate%20the%20effectiveness%20of%0Aour%20approach%20using%20a%20self-constructed%20large-scale%20dataset%2C%20which%20consists%20of%0A100M%20long%20caption%20oriented%20text-image%20pairs.%20It%20is%20noteworthy%20that%2C%20on%20the%20task%0Aof%20long-text%20image%20retrieval%2C%20we%20beat%20the%20competitor%20using%20long%20captions%20with%0A11.1%25%20improvement%20%28i.e.%2C%20from%2072.62%25%20to%2083.72%25%29.%20We%20will%20release%20the%20code%2C%20the%0Amodel%2C%20and%20the%20new%20dataset%20to%20facilitate%20the%20reproducibility%20and%20further%0Aresearch.%20The%20project%20page%20is%20available%20at%20https%3A//wuw2019.github.io/lotlip.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2410.05249v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DLoTLIP%253A%2520Improving%2520Language-Image%2520Pre-training%2520for%2520Long%2520Text%250A%2520%2520Understanding%26entry.906535625%3DWei%2520Wu%2520and%2520Kecheng%2520Zheng%2520and%2520Shuailei%2520Ma%2520and%2520Fan%2520Lu%2520and%2520Yuxin%2520Guo%2520and%2520Yifei%2520Zhang%2520and%2520Wei%2520Chen%2520and%2520Qingpei%2520Guo%2520and%2520Yujun%2520Shen%2520and%2520Zheng-Jun%2520Zha%26entry.1292438233%3D%2520%2520Understanding%2520long%2520text%2520is%2520of%2520great%2520demands%2520in%2520practice%2520but%2520beyond%2520the%2520reach%250Aof%2520most%2520language-image%2520pre-training%2520%2528LIP%2529%2520models.%2520In%2520this%2520work%252C%2520we%2520empirically%250Aconfirm%2520that%2520the%2520key%2520reason%2520causing%2520such%2520an%2520issue%2520is%2520that%2520the%2520training%2520images%250Aare%2520usually%2520paired%2520with%2520short%2520captions%252C%2520leaving%2520certain%2520tokens%2520easily%250Aovershadowed%2520by%2520salient%2520tokens.%2520Towards%2520this%2520problem%252C%2520our%2520initial%2520attempt%2520is%2520to%250Arelabel%2520the%2520data%2520with%2520long%2520captions%252C%2520however%252C%2520directly%2520learning%2520with%2520which%2520may%250Alead%2520to%2520performance%2520degradation%2520in%2520understanding%2520short%2520text%2520%2528e.g.%252C%2520in%2520the%2520image%250Aclassification%2520task%2529.%2520Then%252C%2520after%2520incorporating%2520corner%2520tokens%2520to%2520aggregate%250Adiverse%2520textual%2520information%252C%2520we%2520manage%2520to%2520help%2520the%2520model%2520catch%2520up%2520to%2520its%250Aoriginal%2520level%2520of%2520short%2520text%2520understanding%2520yet%2520greatly%2520enhance%2520its%2520capability%250Aof%2520long%2520text%2520understanding.%2520We%2520further%2520look%2520into%2520whether%2520the%2520model%2520can%250Acontinuously%2520benefit%2520from%2520longer%2520captions%2520and%2520notice%2520a%2520clear%2520trade-off%2520between%250Athe%2520performance%2520and%2520the%2520efficiency.%2520Finally%252C%2520we%2520validate%2520the%2520effectiveness%2520of%250Aour%2520approach%2520using%2520a%2520self-constructed%2520large-scale%2520dataset%252C%2520which%2520consists%2520of%250A100M%2520long%2520caption%2520oriented%2520text-image%2520pairs.%2520It%2520is%2520noteworthy%2520that%252C%2520on%2520the%2520task%250Aof%2520long-text%2520image%2520retrieval%252C%2520we%2520beat%2520the%2520competitor%2520using%2520long%2520captions%2520with%250A11.1%2525%2520improvement%2520%2528i.e.%252C%2520from%252072.62%2525%2520to%252083.72%2525%2529.%2520We%2520will%2520release%2520the%2520code%252C%2520the%250Amodel%252C%2520and%2520the%2520new%2520dataset%2520to%2520facilitate%2520the%2520reproducibility%2520and%2520further%250Aresearch.%2520The%2520project%2520page%2520is%2520available%2520at%2520https%253A//wuw2019.github.io/lotlip.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2410.05249v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=LoTLIP%3A%20Improving%20Language-Image%20Pre-training%20for%20Long%20Text%0A%20%20Understanding&entry.906535625=Wei%20Wu%20and%20Kecheng%20Zheng%20and%20Shuailei%20Ma%20and%20Fan%20Lu%20and%20Yuxin%20Guo%20and%20Yifei%20Zhang%20and%20Wei%20Chen%20and%20Qingpei%20Guo%20and%20Yujun%20Shen%20and%20Zheng-Jun%20Zha&entry.1292438233=%20%20Understanding%20long%20text%20is%20of%20great%20demands%20in%20practice%20but%20beyond%20the%20reach%0Aof%20most%20language-image%20pre-training%20%28LIP%29%20models.%20In%20this%20work%2C%20we%20empirically%0Aconfirm%20that%20the%20key%20reason%20causing%20such%20an%20issue%20is%20that%20the%20training%20images%0Aare%20usually%20paired%20with%20short%20captions%2C%20leaving%20certain%20tokens%20easily%0Aovershadowed%20by%20salient%20tokens.%20Towards%20this%20problem%2C%20our%20initial%20attempt%20is%20to%0Arelabel%20the%20data%20with%20long%20captions%2C%20however%2C%20directly%20learning%20with%20which%20may%0Alead%20to%20performance%20degradation%20in%20understanding%20short%20text%20%28e.g.%2C%20in%20the%20image%0Aclassification%20task%29.%20Then%2C%20after%20incorporating%20corner%20tokens%20to%20aggregate%0Adiverse%20textual%20information%2C%20we%20manage%20to%20help%20the%20model%20catch%20up%20to%20its%0Aoriginal%20level%20of%20short%20text%20understanding%20yet%20greatly%20enhance%20its%20capability%0Aof%20long%20text%20understanding.%20We%20further%20look%20into%20whether%20the%20model%20can%0Acontinuously%20benefit%20from%20longer%20captions%20and%20notice%20a%20clear%20trade-off%20between%0Athe%20performance%20and%20the%20efficiency.%20Finally%2C%20we%20validate%20the%20effectiveness%20of%0Aour%20approach%20using%20a%20self-constructed%20large-scale%20dataset%2C%20which%20consists%20of%0A100M%20long%20caption%20oriented%20text-image%20pairs.%20It%20is%20noteworthy%20that%2C%20on%20the%20task%0Aof%20long-text%20image%20retrieval%2C%20we%20beat%20the%20competitor%20using%20long%20captions%20with%0A11.1%25%20improvement%20%28i.e.%2C%20from%2072.62%25%20to%2083.72%25%29.%20We%20will%20release%20the%20code%2C%20the%0Amodel%2C%20and%20the%20new%20dataset%20to%20facilitate%20the%20reproducibility%20and%20further%0Aresearch.%20The%20project%20page%20is%20available%20at%20https%3A//wuw2019.github.io/lotlip.%0A&entry.1838667208=http%3A//arxiv.org/abs/2410.05249v1&entry.124074799=Read"},
{"title": "3D-free meets 3D priors: Novel View Synthesis from a Single Image with\n  Pretrained Diffusion Guidance", "author": "Taewon Kang and Divya Kothandaraman and Dinesh Manocha and Ming C. Lin", "abstract": "  Recent 3D novel view synthesis (NVS) methods are limited to\nsingle-object-centric scenes and struggle with complex environments. They often\nrequire extensive 3D data for training, lacking generalization beyond the\ntraining distribution. Conversely, 3D-free methods can generate text-controlled\nviews of complex, in-the-wild scenes using a pretrained stable diffusion model\nwithout the need for a large amount of 3D-based training data, but lack camera\ncontrol. In this paper, we introduce a method capable of generating\ncamera-controlled viewpoints from a single input image, by combining the\nbenefits of 3D-free and 3D-based approaches. Our method excels in handling\ncomplex and diverse scenes without extensive training or additional 3D and\nmultiview data. It leverages widely available pretrained NVS models for weak\nguidance, integrating this knowledge into a 3D-free view synthesis approach to\nachieve the desired results. Experimental results demonstrate that our method\noutperforms existing models in both qualitative and quantitative evaluations,\nproviding high-fidelity and consistent novel view synthesis at desired camera\nangles across a wide variety of scenes.\n", "link": "http://arxiv.org/abs/2408.06157v2", "date": "2024-10-07", "relevancy": 2.7247, "topK": [{"title": "CAT3D: Create Anything in 3D with Multi-View Diffusion Models", "link": "http://arxiv.org/abs/2405.10314v1", "similarity": 0.6959}, {"title": "CAT3D: Create Anything in 3D with Multi-View Diffusion Models", "link": "http://arxiv.org/abs/2405.10314v1", "similarity": 0.6959}, {"title": "Thinking Outside the BBox: Unconstrained Generative Object Compositing", "link": "http://arxiv.org/abs/2409.04559v2", "similarity": 0.6074}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%203D-free%20meets%203D%20priors%3A%20Novel%20View%20Synthesis%20from%20a%20Single%20Image%20with%0A%20%20Pretrained%20Diffusion%20Guidance&body=Title%3A%203D-free%20meets%203D%20priors%3A%20Novel%20View%20Synthesis%20from%20a%20Single%20Image%20with%0A%20%20Pretrained%20Diffusion%20Guidance%0AAuthor%3A%20Taewon%20Kang%20and%20Divya%20Kothandaraman%20and%20Dinesh%20Manocha%20and%20Ming%20C.%20Lin%0AAbstract%3A%20%20%20Recent%203D%20novel%20view%20synthesis%20%28NVS%29%20methods%20are%20limited%20to%0Asingle-object-centric%20scenes%20and%20struggle%20with%20complex%20environments.%20They%20often%0Arequire%20extensive%203D%20data%20for%20training%2C%20lacking%20generalization%20beyond%20the%0Atraining%20distribution.%20Conversely%2C%203D-free%20methods%20can%20generate%20text-controlled%0Aviews%20of%20complex%2C%20in-the-wild%20scenes%20using%20a%20pretrained%20stable%20diffusion%20model%0Awithout%20the%20need%20for%20a%20large%20amount%20of%203D-based%20training%20data%2C%20but%20lack%20camera%0Acontrol.%20In%20this%20paper%2C%20we%20introduce%20a%20method%20capable%20of%20generating%0Acamera-controlled%20viewpoints%20from%20a%20single%20input%20image%2C%20by%20combining%20the%0Abenefits%20of%203D-free%20and%203D-based%20approaches.%20Our%20method%20excels%20in%20handling%0Acomplex%20and%20diverse%20scenes%20without%20extensive%20training%20or%20additional%203D%20and%0Amultiview%20data.%20It%20leverages%20widely%20available%20pretrained%20NVS%20models%20for%20weak%0Aguidance%2C%20integrating%20this%20knowledge%20into%20a%203D-free%20view%20synthesis%20approach%20to%0Aachieve%20the%20desired%20results.%20Experimental%20results%20demonstrate%20that%20our%20method%0Aoutperforms%20existing%20models%20in%20both%20qualitative%20and%20quantitative%20evaluations%2C%0Aproviding%20high-fidelity%20and%20consistent%20novel%20view%20synthesis%20at%20desired%20camera%0Aangles%20across%20a%20wide%20variety%20of%20scenes.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2408.06157v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3D3D-free%2520meets%25203D%2520priors%253A%2520Novel%2520View%2520Synthesis%2520from%2520a%2520Single%2520Image%2520with%250A%2520%2520Pretrained%2520Diffusion%2520Guidance%26entry.906535625%3DTaewon%2520Kang%2520and%2520Divya%2520Kothandaraman%2520and%2520Dinesh%2520Manocha%2520and%2520Ming%2520C.%2520Lin%26entry.1292438233%3D%2520%2520Recent%25203D%2520novel%2520view%2520synthesis%2520%2528NVS%2529%2520methods%2520are%2520limited%2520to%250Asingle-object-centric%2520scenes%2520and%2520struggle%2520with%2520complex%2520environments.%2520They%2520often%250Arequire%2520extensive%25203D%2520data%2520for%2520training%252C%2520lacking%2520generalization%2520beyond%2520the%250Atraining%2520distribution.%2520Conversely%252C%25203D-free%2520methods%2520can%2520generate%2520text-controlled%250Aviews%2520of%2520complex%252C%2520in-the-wild%2520scenes%2520using%2520a%2520pretrained%2520stable%2520diffusion%2520model%250Awithout%2520the%2520need%2520for%2520a%2520large%2520amount%2520of%25203D-based%2520training%2520data%252C%2520but%2520lack%2520camera%250Acontrol.%2520In%2520this%2520paper%252C%2520we%2520introduce%2520a%2520method%2520capable%2520of%2520generating%250Acamera-controlled%2520viewpoints%2520from%2520a%2520single%2520input%2520image%252C%2520by%2520combining%2520the%250Abenefits%2520of%25203D-free%2520and%25203D-based%2520approaches.%2520Our%2520method%2520excels%2520in%2520handling%250Acomplex%2520and%2520diverse%2520scenes%2520without%2520extensive%2520training%2520or%2520additional%25203D%2520and%250Amultiview%2520data.%2520It%2520leverages%2520widely%2520available%2520pretrained%2520NVS%2520models%2520for%2520weak%250Aguidance%252C%2520integrating%2520this%2520knowledge%2520into%2520a%25203D-free%2520view%2520synthesis%2520approach%2520to%250Aachieve%2520the%2520desired%2520results.%2520Experimental%2520results%2520demonstrate%2520that%2520our%2520method%250Aoutperforms%2520existing%2520models%2520in%2520both%2520qualitative%2520and%2520quantitative%2520evaluations%252C%250Aproviding%2520high-fidelity%2520and%2520consistent%2520novel%2520view%2520synthesis%2520at%2520desired%2520camera%250Aangles%2520across%2520a%2520wide%2520variety%2520of%2520scenes.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2408.06157v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=3D-free%20meets%203D%20priors%3A%20Novel%20View%20Synthesis%20from%20a%20Single%20Image%20with%0A%20%20Pretrained%20Diffusion%20Guidance&entry.906535625=Taewon%20Kang%20and%20Divya%20Kothandaraman%20and%20Dinesh%20Manocha%20and%20Ming%20C.%20Lin&entry.1292438233=%20%20Recent%203D%20novel%20view%20synthesis%20%28NVS%29%20methods%20are%20limited%20to%0Asingle-object-centric%20scenes%20and%20struggle%20with%20complex%20environments.%20They%20often%0Arequire%20extensive%203D%20data%20for%20training%2C%20lacking%20generalization%20beyond%20the%0Atraining%20distribution.%20Conversely%2C%203D-free%20methods%20can%20generate%20text-controlled%0Aviews%20of%20complex%2C%20in-the-wild%20scenes%20using%20a%20pretrained%20stable%20diffusion%20model%0Awithout%20the%20need%20for%20a%20large%20amount%20of%203D-based%20training%20data%2C%20but%20lack%20camera%0Acontrol.%20In%20this%20paper%2C%20we%20introduce%20a%20method%20capable%20of%20generating%0Acamera-controlled%20viewpoints%20from%20a%20single%20input%20image%2C%20by%20combining%20the%0Abenefits%20of%203D-free%20and%203D-based%20approaches.%20Our%20method%20excels%20in%20handling%0Acomplex%20and%20diverse%20scenes%20without%20extensive%20training%20or%20additional%203D%20and%0Amultiview%20data.%20It%20leverages%20widely%20available%20pretrained%20NVS%20models%20for%20weak%0Aguidance%2C%20integrating%20this%20knowledge%20into%20a%203D-free%20view%20synthesis%20approach%20to%0Aachieve%20the%20desired%20results.%20Experimental%20results%20demonstrate%20that%20our%20method%0Aoutperforms%20existing%20models%20in%20both%20qualitative%20and%20quantitative%20evaluations%2C%0Aproviding%20high-fidelity%20and%20consistent%20novel%20view%20synthesis%20at%20desired%20camera%0Aangles%20across%20a%20wide%20variety%20of%20scenes.%0A&entry.1838667208=http%3A//arxiv.org/abs/2408.06157v2&entry.124074799=Read"},
{"title": "Comparison of marker-less 2D image-based methods for infant pose\n  estimation", "author": "Lennart Jahn and Sarah Fl\u00fcgge and Dajie Zhang and Luise Poustka and Sven B\u00f6lte and Florentin W\u00f6rg\u00f6tter and Peter B Marschik and Tomas Kulvicius", "abstract": "  There are increasing efforts to automate clinical methods for early diagnosis\nof developmental disorders, among them the General Movement Assessment (GMA), a\nvideo-based tool to classify infant motor functioning. Optimal pose estimation\nis a crucial part of the automated GMA. In this study we compare the\nperformance of available generic- and infant-pose estimators, and the choice of\nviewing angle for optimal recordings, i.e., conventional diagonal view used in\nGMA vs. top-down view. For this study, we used 4500 annotated video-frames from\n75 recordings of infant spontaneous motor functions from 4 to 26 weeks. To\ndetermine which available pose estimation method and camera angle yield the\nbest pose estimation accuracy on infants in a GMA related setting, the distance\nto human annotations as well as the percentage of correct key-points (PCK) were\ncomputed and compared. The results show that the best performing generic model\ntrained on adults, ViTPose, also performs best on infants. We see no\nimprovement from using specialized infant-pose estimators over the generic pose\nestimators on our own infant dataset. However, when retraining a generic model\non our data, there is a significant improvement in pose estimation accuracy.\nThe pose estimation accuracy obtained from the top-down view is significantly\nbetter than that obtained from the diagonal view, especially for the detection\nof the hip key-points. The results also indicate only limited generalization\ncapabilities of infant-pose estimators to other infant datasets, which hints\nthat one should be careful when choosing infant pose estimators and using them\non infant datasets which they were not trained on. While the standard GMA\nmethod uses a diagonal view for assessment, pose estimation accuracy\nsignificantly improves using a top-down view. This suggests that a top-down\nview should be included in recording setups for automated GMA research.\n", "link": "http://arxiv.org/abs/2410.04980v1", "date": "2024-10-07", "relevancy": 2.6997, "topK": [{"title": "GST: Precise 3D Human Body from a Single Image with Gaussian Splatting\n  Transformers", "link": "http://arxiv.org/abs/2409.04196v1", "similarity": 0.5589}, {"title": "PoseAnimate: Zero-shot high fidelity pose controllable character\n  animation", "link": "http://arxiv.org/abs/2404.13680v2", "similarity": 0.5492}, {"title": "VirtualModel: Generating Object-ID-retentive Human-object Interaction\n  Image by Diffusion Model for E-commerce Marketing", "link": "http://arxiv.org/abs/2405.09985v1", "similarity": 0.5117}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Comparison%20of%20marker-less%202D%20image-based%20methods%20for%20infant%20pose%0A%20%20estimation&body=Title%3A%20Comparison%20of%20marker-less%202D%20image-based%20methods%20for%20infant%20pose%0A%20%20estimation%0AAuthor%3A%20Lennart%20Jahn%20and%20Sarah%20Fl%C3%BCgge%20and%20Dajie%20Zhang%20and%20Luise%20Poustka%20and%20Sven%20B%C3%B6lte%20and%20Florentin%20W%C3%B6rg%C3%B6tter%20and%20Peter%20B%20Marschik%20and%20Tomas%20Kulvicius%0AAbstract%3A%20%20%20There%20are%20increasing%20efforts%20to%20automate%20clinical%20methods%20for%20early%20diagnosis%0Aof%20developmental%20disorders%2C%20among%20them%20the%20General%20Movement%20Assessment%20%28GMA%29%2C%20a%0Avideo-based%20tool%20to%20classify%20infant%20motor%20functioning.%20Optimal%20pose%20estimation%0Ais%20a%20crucial%20part%20of%20the%20automated%20GMA.%20In%20this%20study%20we%20compare%20the%0Aperformance%20of%20available%20generic-%20and%20infant-pose%20estimators%2C%20and%20the%20choice%20of%0Aviewing%20angle%20for%20optimal%20recordings%2C%20i.e.%2C%20conventional%20diagonal%20view%20used%20in%0AGMA%20vs.%20top-down%20view.%20For%20this%20study%2C%20we%20used%204500%20annotated%20video-frames%20from%0A75%20recordings%20of%20infant%20spontaneous%20motor%20functions%20from%204%20to%2026%20weeks.%20To%0Adetermine%20which%20available%20pose%20estimation%20method%20and%20camera%20angle%20yield%20the%0Abest%20pose%20estimation%20accuracy%20on%20infants%20in%20a%20GMA%20related%20setting%2C%20the%20distance%0Ato%20human%20annotations%20as%20well%20as%20the%20percentage%20of%20correct%20key-points%20%28PCK%29%20were%0Acomputed%20and%20compared.%20The%20results%20show%20that%20the%20best%20performing%20generic%20model%0Atrained%20on%20adults%2C%20ViTPose%2C%20also%20performs%20best%20on%20infants.%20We%20see%20no%0Aimprovement%20from%20using%20specialized%20infant-pose%20estimators%20over%20the%20generic%20pose%0Aestimators%20on%20our%20own%20infant%20dataset.%20However%2C%20when%20retraining%20a%20generic%20model%0Aon%20our%20data%2C%20there%20is%20a%20significant%20improvement%20in%20pose%20estimation%20accuracy.%0AThe%20pose%20estimation%20accuracy%20obtained%20from%20the%20top-down%20view%20is%20significantly%0Abetter%20than%20that%20obtained%20from%20the%20diagonal%20view%2C%20especially%20for%20the%20detection%0Aof%20the%20hip%20key-points.%20The%20results%20also%20indicate%20only%20limited%20generalization%0Acapabilities%20of%20infant-pose%20estimators%20to%20other%20infant%20datasets%2C%20which%20hints%0Athat%20one%20should%20be%20careful%20when%20choosing%20infant%20pose%20estimators%20and%20using%20them%0Aon%20infant%20datasets%20which%20they%20were%20not%20trained%20on.%20While%20the%20standard%20GMA%0Amethod%20uses%20a%20diagonal%20view%20for%20assessment%2C%20pose%20estimation%20accuracy%0Asignificantly%20improves%20using%20a%20top-down%20view.%20This%20suggests%20that%20a%20top-down%0Aview%20should%20be%20included%20in%20recording%20setups%20for%20automated%20GMA%20research.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2410.04980v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DComparison%2520of%2520marker-less%25202D%2520image-based%2520methods%2520for%2520infant%2520pose%250A%2520%2520estimation%26entry.906535625%3DLennart%2520Jahn%2520and%2520Sarah%2520Fl%25C3%25BCgge%2520and%2520Dajie%2520Zhang%2520and%2520Luise%2520Poustka%2520and%2520Sven%2520B%25C3%25B6lte%2520and%2520Florentin%2520W%25C3%25B6rg%25C3%25B6tter%2520and%2520Peter%2520B%2520Marschik%2520and%2520Tomas%2520Kulvicius%26entry.1292438233%3D%2520%2520There%2520are%2520increasing%2520efforts%2520to%2520automate%2520clinical%2520methods%2520for%2520early%2520diagnosis%250Aof%2520developmental%2520disorders%252C%2520among%2520them%2520the%2520General%2520Movement%2520Assessment%2520%2528GMA%2529%252C%2520a%250Avideo-based%2520tool%2520to%2520classify%2520infant%2520motor%2520functioning.%2520Optimal%2520pose%2520estimation%250Ais%2520a%2520crucial%2520part%2520of%2520the%2520automated%2520GMA.%2520In%2520this%2520study%2520we%2520compare%2520the%250Aperformance%2520of%2520available%2520generic-%2520and%2520infant-pose%2520estimators%252C%2520and%2520the%2520choice%2520of%250Aviewing%2520angle%2520for%2520optimal%2520recordings%252C%2520i.e.%252C%2520conventional%2520diagonal%2520view%2520used%2520in%250AGMA%2520vs.%2520top-down%2520view.%2520For%2520this%2520study%252C%2520we%2520used%25204500%2520annotated%2520video-frames%2520from%250A75%2520recordings%2520of%2520infant%2520spontaneous%2520motor%2520functions%2520from%25204%2520to%252026%2520weeks.%2520To%250Adetermine%2520which%2520available%2520pose%2520estimation%2520method%2520and%2520camera%2520angle%2520yield%2520the%250Abest%2520pose%2520estimation%2520accuracy%2520on%2520infants%2520in%2520a%2520GMA%2520related%2520setting%252C%2520the%2520distance%250Ato%2520human%2520annotations%2520as%2520well%2520as%2520the%2520percentage%2520of%2520correct%2520key-points%2520%2528PCK%2529%2520were%250Acomputed%2520and%2520compared.%2520The%2520results%2520show%2520that%2520the%2520best%2520performing%2520generic%2520model%250Atrained%2520on%2520adults%252C%2520ViTPose%252C%2520also%2520performs%2520best%2520on%2520infants.%2520We%2520see%2520no%250Aimprovement%2520from%2520using%2520specialized%2520infant-pose%2520estimators%2520over%2520the%2520generic%2520pose%250Aestimators%2520on%2520our%2520own%2520infant%2520dataset.%2520However%252C%2520when%2520retraining%2520a%2520generic%2520model%250Aon%2520our%2520data%252C%2520there%2520is%2520a%2520significant%2520improvement%2520in%2520pose%2520estimation%2520accuracy.%250AThe%2520pose%2520estimation%2520accuracy%2520obtained%2520from%2520the%2520top-down%2520view%2520is%2520significantly%250Abetter%2520than%2520that%2520obtained%2520from%2520the%2520diagonal%2520view%252C%2520especially%2520for%2520the%2520detection%250Aof%2520the%2520hip%2520key-points.%2520The%2520results%2520also%2520indicate%2520only%2520limited%2520generalization%250Acapabilities%2520of%2520infant-pose%2520estimators%2520to%2520other%2520infant%2520datasets%252C%2520which%2520hints%250Athat%2520one%2520should%2520be%2520careful%2520when%2520choosing%2520infant%2520pose%2520estimators%2520and%2520using%2520them%250Aon%2520infant%2520datasets%2520which%2520they%2520were%2520not%2520trained%2520on.%2520While%2520the%2520standard%2520GMA%250Amethod%2520uses%2520a%2520diagonal%2520view%2520for%2520assessment%252C%2520pose%2520estimation%2520accuracy%250Asignificantly%2520improves%2520using%2520a%2520top-down%2520view.%2520This%2520suggests%2520that%2520a%2520top-down%250Aview%2520should%2520be%2520included%2520in%2520recording%2520setups%2520for%2520automated%2520GMA%2520research.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2410.04980v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Comparison%20of%20marker-less%202D%20image-based%20methods%20for%20infant%20pose%0A%20%20estimation&entry.906535625=Lennart%20Jahn%20and%20Sarah%20Fl%C3%BCgge%20and%20Dajie%20Zhang%20and%20Luise%20Poustka%20and%20Sven%20B%C3%B6lte%20and%20Florentin%20W%C3%B6rg%C3%B6tter%20and%20Peter%20B%20Marschik%20and%20Tomas%20Kulvicius&entry.1292438233=%20%20There%20are%20increasing%20efforts%20to%20automate%20clinical%20methods%20for%20early%20diagnosis%0Aof%20developmental%20disorders%2C%20among%20them%20the%20General%20Movement%20Assessment%20%28GMA%29%2C%20a%0Avideo-based%20tool%20to%20classify%20infant%20motor%20functioning.%20Optimal%20pose%20estimation%0Ais%20a%20crucial%20part%20of%20the%20automated%20GMA.%20In%20this%20study%20we%20compare%20the%0Aperformance%20of%20available%20generic-%20and%20infant-pose%20estimators%2C%20and%20the%20choice%20of%0Aviewing%20angle%20for%20optimal%20recordings%2C%20i.e.%2C%20conventional%20diagonal%20view%20used%20in%0AGMA%20vs.%20top-down%20view.%20For%20this%20study%2C%20we%20used%204500%20annotated%20video-frames%20from%0A75%20recordings%20of%20infant%20spontaneous%20motor%20functions%20from%204%20to%2026%20weeks.%20To%0Adetermine%20which%20available%20pose%20estimation%20method%20and%20camera%20angle%20yield%20the%0Abest%20pose%20estimation%20accuracy%20on%20infants%20in%20a%20GMA%20related%20setting%2C%20the%20distance%0Ato%20human%20annotations%20as%20well%20as%20the%20percentage%20of%20correct%20key-points%20%28PCK%29%20were%0Acomputed%20and%20compared.%20The%20results%20show%20that%20the%20best%20performing%20generic%20model%0Atrained%20on%20adults%2C%20ViTPose%2C%20also%20performs%20best%20on%20infants.%20We%20see%20no%0Aimprovement%20from%20using%20specialized%20infant-pose%20estimators%20over%20the%20generic%20pose%0Aestimators%20on%20our%20own%20infant%20dataset.%20However%2C%20when%20retraining%20a%20generic%20model%0Aon%20our%20data%2C%20there%20is%20a%20significant%20improvement%20in%20pose%20estimation%20accuracy.%0AThe%20pose%20estimation%20accuracy%20obtained%20from%20the%20top-down%20view%20is%20significantly%0Abetter%20than%20that%20obtained%20from%20the%20diagonal%20view%2C%20especially%20for%20the%20detection%0Aof%20the%20hip%20key-points.%20The%20results%20also%20indicate%20only%20limited%20generalization%0Acapabilities%20of%20infant-pose%20estimators%20to%20other%20infant%20datasets%2C%20which%20hints%0Athat%20one%20should%20be%20careful%20when%20choosing%20infant%20pose%20estimators%20and%20using%20them%0Aon%20infant%20datasets%20which%20they%20were%20not%20trained%20on.%20While%20the%20standard%20GMA%0Amethod%20uses%20a%20diagonal%20view%20for%20assessment%2C%20pose%20estimation%20accuracy%0Asignificantly%20improves%20using%20a%20top-down%20view.%20This%20suggests%20that%20a%20top-down%0Aview%20should%20be%20included%20in%20recording%20setups%20for%20automated%20GMA%20research.%0A&entry.1838667208=http%3A//arxiv.org/abs/2410.04980v1&entry.124074799=Read"},
{"title": "Learning Contrastive Feature Representations for Facial Action Unit\n  Detection", "author": "Ziqiao Shang and Bin Liu and Fengmao Lv and Fei Teng and Tianrui Li", "abstract": "  Facial action unit (AU) detection has long encountered the challenge of\ndetecting subtle feature differences when AUs activate. Existing methods often\nrely on encoding pixel-level information of AUs, which not only encodes\nadditional redundant information but also leads to increased model complexity\nand limited generalizability. Additionally, the accuracy of AU detection is\nnegatively impacted by the class imbalance issue of each AU type, and the\npresence of noisy and false AU labels. In this paper, we introduce a novel\ncontrastive learning framework aimed for AU detection that incorporates both\nself-supervised and supervised signals, thereby enhancing the learning of\ndiscriminative features for accurate AU detection. To tackle the class\nimbalance issue, we employ a negative sample re-weighting strategy that adjusts\nthe step size of updating parameters for minority and majority class samples.\nMoreover, to address the challenges posed by noisy and false AU labels, we\nemploy a sampling technique that encompasses three distinct types of positive\nsample pairs. This enables us to inject self-supervised signals into the\nsupervised signal, effectively mitigating the adverse effects of noisy labels.\nOur experimental assessments, conducted on four widely-utilized benchmark\ndatasets (BP4D, DISFA, GFT and Aff-Wild2), underscore the superior performance\nof our approach compared to state-of-the-art methods of AU detection. Our code\nis available at \\url{https://github.com/Ziqiao-Shang/AUNCE}.\n", "link": "http://arxiv.org/abs/2402.06165v4", "date": "2024-10-07", "relevancy": 2.692, "topK": [{"title": "Weakly-guided self-supervised pretraining for temporal activity detection", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/25189", "similarity": 0.5551}, {"title": "Breadcrumbs: Adversarial class-balanced sampling for long-tailed recognition", "link": "https://link.springer.com/chapter/10.1007/978-3-031-20053-3_37", "similarity": 0.5355}, {"title": "DDM-NET: End-to-end learning of keypoint feature Detection, Description and Matching for 3D localization", "link": "https://arxiv.org/abs/2212.04575", "similarity": 0.5246}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Learning%20Contrastive%20Feature%20Representations%20for%20Facial%20Action%20Unit%0A%20%20Detection&body=Title%3A%20Learning%20Contrastive%20Feature%20Representations%20for%20Facial%20Action%20Unit%0A%20%20Detection%0AAuthor%3A%20Ziqiao%20Shang%20and%20Bin%20Liu%20and%20Fengmao%20Lv%20and%20Fei%20Teng%20and%20Tianrui%20Li%0AAbstract%3A%20%20%20Facial%20action%20unit%20%28AU%29%20detection%20has%20long%20encountered%20the%20challenge%20of%0Adetecting%20subtle%20feature%20differences%20when%20AUs%20activate.%20Existing%20methods%20often%0Arely%20on%20encoding%20pixel-level%20information%20of%20AUs%2C%20which%20not%20only%20encodes%0Aadditional%20redundant%20information%20but%20also%20leads%20to%20increased%20model%20complexity%0Aand%20limited%20generalizability.%20Additionally%2C%20the%20accuracy%20of%20AU%20detection%20is%0Anegatively%20impacted%20by%20the%20class%20imbalance%20issue%20of%20each%20AU%20type%2C%20and%20the%0Apresence%20of%20noisy%20and%20false%20AU%20labels.%20In%20this%20paper%2C%20we%20introduce%20a%20novel%0Acontrastive%20learning%20framework%20aimed%20for%20AU%20detection%20that%20incorporates%20both%0Aself-supervised%20and%20supervised%20signals%2C%20thereby%20enhancing%20the%20learning%20of%0Adiscriminative%20features%20for%20accurate%20AU%20detection.%20To%20tackle%20the%20class%0Aimbalance%20issue%2C%20we%20employ%20a%20negative%20sample%20re-weighting%20strategy%20that%20adjusts%0Athe%20step%20size%20of%20updating%20parameters%20for%20minority%20and%20majority%20class%20samples.%0AMoreover%2C%20to%20address%20the%20challenges%20posed%20by%20noisy%20and%20false%20AU%20labels%2C%20we%0Aemploy%20a%20sampling%20technique%20that%20encompasses%20three%20distinct%20types%20of%20positive%0Asample%20pairs.%20This%20enables%20us%20to%20inject%20self-supervised%20signals%20into%20the%0Asupervised%20signal%2C%20effectively%20mitigating%20the%20adverse%20effects%20of%20noisy%20labels.%0AOur%20experimental%20assessments%2C%20conducted%20on%20four%20widely-utilized%20benchmark%0Adatasets%20%28BP4D%2C%20DISFA%2C%20GFT%20and%20Aff-Wild2%29%2C%20underscore%20the%20superior%20performance%0Aof%20our%20approach%20compared%20to%20state-of-the-art%20methods%20of%20AU%20detection.%20Our%20code%0Ais%20available%20at%20%5Curl%7Bhttps%3A//github.com/Ziqiao-Shang/AUNCE%7D.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2402.06165v4%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DLearning%2520Contrastive%2520Feature%2520Representations%2520for%2520Facial%2520Action%2520Unit%250A%2520%2520Detection%26entry.906535625%3DZiqiao%2520Shang%2520and%2520Bin%2520Liu%2520and%2520Fengmao%2520Lv%2520and%2520Fei%2520Teng%2520and%2520Tianrui%2520Li%26entry.1292438233%3D%2520%2520Facial%2520action%2520unit%2520%2528AU%2529%2520detection%2520has%2520long%2520encountered%2520the%2520challenge%2520of%250Adetecting%2520subtle%2520feature%2520differences%2520when%2520AUs%2520activate.%2520Existing%2520methods%2520often%250Arely%2520on%2520encoding%2520pixel-level%2520information%2520of%2520AUs%252C%2520which%2520not%2520only%2520encodes%250Aadditional%2520redundant%2520information%2520but%2520also%2520leads%2520to%2520increased%2520model%2520complexity%250Aand%2520limited%2520generalizability.%2520Additionally%252C%2520the%2520accuracy%2520of%2520AU%2520detection%2520is%250Anegatively%2520impacted%2520by%2520the%2520class%2520imbalance%2520issue%2520of%2520each%2520AU%2520type%252C%2520and%2520the%250Apresence%2520of%2520noisy%2520and%2520false%2520AU%2520labels.%2520In%2520this%2520paper%252C%2520we%2520introduce%2520a%2520novel%250Acontrastive%2520learning%2520framework%2520aimed%2520for%2520AU%2520detection%2520that%2520incorporates%2520both%250Aself-supervised%2520and%2520supervised%2520signals%252C%2520thereby%2520enhancing%2520the%2520learning%2520of%250Adiscriminative%2520features%2520for%2520accurate%2520AU%2520detection.%2520To%2520tackle%2520the%2520class%250Aimbalance%2520issue%252C%2520we%2520employ%2520a%2520negative%2520sample%2520re-weighting%2520strategy%2520that%2520adjusts%250Athe%2520step%2520size%2520of%2520updating%2520parameters%2520for%2520minority%2520and%2520majority%2520class%2520samples.%250AMoreover%252C%2520to%2520address%2520the%2520challenges%2520posed%2520by%2520noisy%2520and%2520false%2520AU%2520labels%252C%2520we%250Aemploy%2520a%2520sampling%2520technique%2520that%2520encompasses%2520three%2520distinct%2520types%2520of%2520positive%250Asample%2520pairs.%2520This%2520enables%2520us%2520to%2520inject%2520self-supervised%2520signals%2520into%2520the%250Asupervised%2520signal%252C%2520effectively%2520mitigating%2520the%2520adverse%2520effects%2520of%2520noisy%2520labels.%250AOur%2520experimental%2520assessments%252C%2520conducted%2520on%2520four%2520widely-utilized%2520benchmark%250Adatasets%2520%2528BP4D%252C%2520DISFA%252C%2520GFT%2520and%2520Aff-Wild2%2529%252C%2520underscore%2520the%2520superior%2520performance%250Aof%2520our%2520approach%2520compared%2520to%2520state-of-the-art%2520methods%2520of%2520AU%2520detection.%2520Our%2520code%250Ais%2520available%2520at%2520%255Curl%257Bhttps%253A//github.com/Ziqiao-Shang/AUNCE%257D.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2402.06165v4%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Learning%20Contrastive%20Feature%20Representations%20for%20Facial%20Action%20Unit%0A%20%20Detection&entry.906535625=Ziqiao%20Shang%20and%20Bin%20Liu%20and%20Fengmao%20Lv%20and%20Fei%20Teng%20and%20Tianrui%20Li&entry.1292438233=%20%20Facial%20action%20unit%20%28AU%29%20detection%20has%20long%20encountered%20the%20challenge%20of%0Adetecting%20subtle%20feature%20differences%20when%20AUs%20activate.%20Existing%20methods%20often%0Arely%20on%20encoding%20pixel-level%20information%20of%20AUs%2C%20which%20not%20only%20encodes%0Aadditional%20redundant%20information%20but%20also%20leads%20to%20increased%20model%20complexity%0Aand%20limited%20generalizability.%20Additionally%2C%20the%20accuracy%20of%20AU%20detection%20is%0Anegatively%20impacted%20by%20the%20class%20imbalance%20issue%20of%20each%20AU%20type%2C%20and%20the%0Apresence%20of%20noisy%20and%20false%20AU%20labels.%20In%20this%20paper%2C%20we%20introduce%20a%20novel%0Acontrastive%20learning%20framework%20aimed%20for%20AU%20detection%20that%20incorporates%20both%0Aself-supervised%20and%20supervised%20signals%2C%20thereby%20enhancing%20the%20learning%20of%0Adiscriminative%20features%20for%20accurate%20AU%20detection.%20To%20tackle%20the%20class%0Aimbalance%20issue%2C%20we%20employ%20a%20negative%20sample%20re-weighting%20strategy%20that%20adjusts%0Athe%20step%20size%20of%20updating%20parameters%20for%20minority%20and%20majority%20class%20samples.%0AMoreover%2C%20to%20address%20the%20challenges%20posed%20by%20noisy%20and%20false%20AU%20labels%2C%20we%0Aemploy%20a%20sampling%20technique%20that%20encompasses%20three%20distinct%20types%20of%20positive%0Asample%20pairs.%20This%20enables%20us%20to%20inject%20self-supervised%20signals%20into%20the%0Asupervised%20signal%2C%20effectively%20mitigating%20the%20adverse%20effects%20of%20noisy%20labels.%0AOur%20experimental%20assessments%2C%20conducted%20on%20four%20widely-utilized%20benchmark%0Adatasets%20%28BP4D%2C%20DISFA%2C%20GFT%20and%20Aff-Wild2%29%2C%20underscore%20the%20superior%20performance%0Aof%20our%20approach%20compared%20to%20state-of-the-art%20methods%20of%20AU%20detection.%20Our%20code%0Ais%20available%20at%20%5Curl%7Bhttps%3A//github.com/Ziqiao-Shang/AUNCE%7D.%0A&entry.1838667208=http%3A//arxiv.org/abs/2402.06165v4&entry.124074799=Read"},
{"title": "TuneVLSeg: Prompt Tuning Benchmark for Vision-Language Segmentation\n  Models", "author": "Rabin Adhikari and Safal Thapaliya and Manish Dhakal and Bishesh Khanal", "abstract": "  Vision-Language Models (VLMs) have shown impressive performance in vision\ntasks, but adapting them to new domains often requires expensive fine-tuning.\nPrompt tuning techniques, including textual, visual, and multimodal prompting,\noffer efficient alternatives by leveraging learnable prompts. However, their\napplication to Vision-Language Segmentation Models (VLSMs) and evaluation under\nsignificant domain shifts remain unexplored. This work presents an open-source\nbenchmarking framework, TuneVLSeg, to integrate various unimodal and multimodal\nprompt tuning techniques into VLSMs, making prompt tuning usable for downstream\nsegmentation datasets with any number of classes. TuneVLSeg includes $6$ prompt\ntuning strategies on various prompt depths used in $2$ VLSMs totaling of $8$\ndifferent combinations. We test various prompt tuning on $8$ diverse medical\ndatasets, including $3$ radiology datasets (breast tumor, echocardiograph,\nchest X-ray pathologies) and $5$ non-radiology datasets (polyp, ulcer, skin\ncancer), and two natural domain segmentation datasets. Our study found that\ntextual prompt tuning struggles under significant domain shifts, from\nnatural-domain images to medical data. Furthermore, visual prompt tuning, with\nfewer hyperparameters than multimodal prompt tuning, often achieves performance\ncompetitive to multimodal approaches, making it a valuable first attempt. Our\nwork advances the understanding and applicability of different prompt-tuning\ntechniques for robust domain-specific segmentation. The source code is\navailable at https://github.com/naamiinepal/tunevlseg.\n", "link": "http://arxiv.org/abs/2410.05239v1", "date": "2024-10-07", "relevancy": 2.6614, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5477}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5477}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5014}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20TuneVLSeg%3A%20Prompt%20Tuning%20Benchmark%20for%20Vision-Language%20Segmentation%0A%20%20Models&body=Title%3A%20TuneVLSeg%3A%20Prompt%20Tuning%20Benchmark%20for%20Vision-Language%20Segmentation%0A%20%20Models%0AAuthor%3A%20Rabin%20Adhikari%20and%20Safal%20Thapaliya%20and%20Manish%20Dhakal%20and%20Bishesh%20Khanal%0AAbstract%3A%20%20%20Vision-Language%20Models%20%28VLMs%29%20have%20shown%20impressive%20performance%20in%20vision%0Atasks%2C%20but%20adapting%20them%20to%20new%20domains%20often%20requires%20expensive%20fine-tuning.%0APrompt%20tuning%20techniques%2C%20including%20textual%2C%20visual%2C%20and%20multimodal%20prompting%2C%0Aoffer%20efficient%20alternatives%20by%20leveraging%20learnable%20prompts.%20However%2C%20their%0Aapplication%20to%20Vision-Language%20Segmentation%20Models%20%28VLSMs%29%20and%20evaluation%20under%0Asignificant%20domain%20shifts%20remain%20unexplored.%20This%20work%20presents%20an%20open-source%0Abenchmarking%20framework%2C%20TuneVLSeg%2C%20to%20integrate%20various%20unimodal%20and%20multimodal%0Aprompt%20tuning%20techniques%20into%20VLSMs%2C%20making%20prompt%20tuning%20usable%20for%20downstream%0Asegmentation%20datasets%20with%20any%20number%20of%20classes.%20TuneVLSeg%20includes%20%246%24%20prompt%0Atuning%20strategies%20on%20various%20prompt%20depths%20used%20in%20%242%24%20VLSMs%20totaling%20of%20%248%24%0Adifferent%20combinations.%20We%20test%20various%20prompt%20tuning%20on%20%248%24%20diverse%20medical%0Adatasets%2C%20including%20%243%24%20radiology%20datasets%20%28breast%20tumor%2C%20echocardiograph%2C%0Achest%20X-ray%20pathologies%29%20and%20%245%24%20non-radiology%20datasets%20%28polyp%2C%20ulcer%2C%20skin%0Acancer%29%2C%20and%20two%20natural%20domain%20segmentation%20datasets.%20Our%20study%20found%20that%0Atextual%20prompt%20tuning%20struggles%20under%20significant%20domain%20shifts%2C%20from%0Anatural-domain%20images%20to%20medical%20data.%20Furthermore%2C%20visual%20prompt%20tuning%2C%20with%0Afewer%20hyperparameters%20than%20multimodal%20prompt%20tuning%2C%20often%20achieves%20performance%0Acompetitive%20to%20multimodal%20approaches%2C%20making%20it%20a%20valuable%20first%20attempt.%20Our%0Awork%20advances%20the%20understanding%20and%20applicability%20of%20different%20prompt-tuning%0Atechniques%20for%20robust%20domain-specific%20segmentation.%20The%20source%20code%20is%0Aavailable%20at%20https%3A//github.com/naamiinepal/tunevlseg.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2410.05239v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DTuneVLSeg%253A%2520Prompt%2520Tuning%2520Benchmark%2520for%2520Vision-Language%2520Segmentation%250A%2520%2520Models%26entry.906535625%3DRabin%2520Adhikari%2520and%2520Safal%2520Thapaliya%2520and%2520Manish%2520Dhakal%2520and%2520Bishesh%2520Khanal%26entry.1292438233%3D%2520%2520Vision-Language%2520Models%2520%2528VLMs%2529%2520have%2520shown%2520impressive%2520performance%2520in%2520vision%250Atasks%252C%2520but%2520adapting%2520them%2520to%2520new%2520domains%2520often%2520requires%2520expensive%2520fine-tuning.%250APrompt%2520tuning%2520techniques%252C%2520including%2520textual%252C%2520visual%252C%2520and%2520multimodal%2520prompting%252C%250Aoffer%2520efficient%2520alternatives%2520by%2520leveraging%2520learnable%2520prompts.%2520However%252C%2520their%250Aapplication%2520to%2520Vision-Language%2520Segmentation%2520Models%2520%2528VLSMs%2529%2520and%2520evaluation%2520under%250Asignificant%2520domain%2520shifts%2520remain%2520unexplored.%2520This%2520work%2520presents%2520an%2520open-source%250Abenchmarking%2520framework%252C%2520TuneVLSeg%252C%2520to%2520integrate%2520various%2520unimodal%2520and%2520multimodal%250Aprompt%2520tuning%2520techniques%2520into%2520VLSMs%252C%2520making%2520prompt%2520tuning%2520usable%2520for%2520downstream%250Asegmentation%2520datasets%2520with%2520any%2520number%2520of%2520classes.%2520TuneVLSeg%2520includes%2520%25246%2524%2520prompt%250Atuning%2520strategies%2520on%2520various%2520prompt%2520depths%2520used%2520in%2520%25242%2524%2520VLSMs%2520totaling%2520of%2520%25248%2524%250Adifferent%2520combinations.%2520We%2520test%2520various%2520prompt%2520tuning%2520on%2520%25248%2524%2520diverse%2520medical%250Adatasets%252C%2520including%2520%25243%2524%2520radiology%2520datasets%2520%2528breast%2520tumor%252C%2520echocardiograph%252C%250Achest%2520X-ray%2520pathologies%2529%2520and%2520%25245%2524%2520non-radiology%2520datasets%2520%2528polyp%252C%2520ulcer%252C%2520skin%250Acancer%2529%252C%2520and%2520two%2520natural%2520domain%2520segmentation%2520datasets.%2520Our%2520study%2520found%2520that%250Atextual%2520prompt%2520tuning%2520struggles%2520under%2520significant%2520domain%2520shifts%252C%2520from%250Anatural-domain%2520images%2520to%2520medical%2520data.%2520Furthermore%252C%2520visual%2520prompt%2520tuning%252C%2520with%250Afewer%2520hyperparameters%2520than%2520multimodal%2520prompt%2520tuning%252C%2520often%2520achieves%2520performance%250Acompetitive%2520to%2520multimodal%2520approaches%252C%2520making%2520it%2520a%2520valuable%2520first%2520attempt.%2520Our%250Awork%2520advances%2520the%2520understanding%2520and%2520applicability%2520of%2520different%2520prompt-tuning%250Atechniques%2520for%2520robust%2520domain-specific%2520segmentation.%2520The%2520source%2520code%2520is%250Aavailable%2520at%2520https%253A//github.com/naamiinepal/tunevlseg.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2410.05239v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=TuneVLSeg%3A%20Prompt%20Tuning%20Benchmark%20for%20Vision-Language%20Segmentation%0A%20%20Models&entry.906535625=Rabin%20Adhikari%20and%20Safal%20Thapaliya%20and%20Manish%20Dhakal%20and%20Bishesh%20Khanal&entry.1292438233=%20%20Vision-Language%20Models%20%28VLMs%29%20have%20shown%20impressive%20performance%20in%20vision%0Atasks%2C%20but%20adapting%20them%20to%20new%20domains%20often%20requires%20expensive%20fine-tuning.%0APrompt%20tuning%20techniques%2C%20including%20textual%2C%20visual%2C%20and%20multimodal%20prompting%2C%0Aoffer%20efficient%20alternatives%20by%20leveraging%20learnable%20prompts.%20However%2C%20their%0Aapplication%20to%20Vision-Language%20Segmentation%20Models%20%28VLSMs%29%20and%20evaluation%20under%0Asignificant%20domain%20shifts%20remain%20unexplored.%20This%20work%20presents%20an%20open-source%0Abenchmarking%20framework%2C%20TuneVLSeg%2C%20to%20integrate%20various%20unimodal%20and%20multimodal%0Aprompt%20tuning%20techniques%20into%20VLSMs%2C%20making%20prompt%20tuning%20usable%20for%20downstream%0Asegmentation%20datasets%20with%20any%20number%20of%20classes.%20TuneVLSeg%20includes%20%246%24%20prompt%0Atuning%20strategies%20on%20various%20prompt%20depths%20used%20in%20%242%24%20VLSMs%20totaling%20of%20%248%24%0Adifferent%20combinations.%20We%20test%20various%20prompt%20tuning%20on%20%248%24%20diverse%20medical%0Adatasets%2C%20including%20%243%24%20radiology%20datasets%20%28breast%20tumor%2C%20echocardiograph%2C%0Achest%20X-ray%20pathologies%29%20and%20%245%24%20non-radiology%20datasets%20%28polyp%2C%20ulcer%2C%20skin%0Acancer%29%2C%20and%20two%20natural%20domain%20segmentation%20datasets.%20Our%20study%20found%20that%0Atextual%20prompt%20tuning%20struggles%20under%20significant%20domain%20shifts%2C%20from%0Anatural-domain%20images%20to%20medical%20data.%20Furthermore%2C%20visual%20prompt%20tuning%2C%20with%0Afewer%20hyperparameters%20than%20multimodal%20prompt%20tuning%2C%20often%20achieves%20performance%0Acompetitive%20to%20multimodal%20approaches%2C%20making%20it%20a%20valuable%20first%20attempt.%20Our%0Awork%20advances%20the%20understanding%20and%20applicability%20of%20different%20prompt-tuning%0Atechniques%20for%20robust%20domain-specific%20segmentation.%20The%20source%20code%20is%0Aavailable%20at%20https%3A//github.com/naamiinepal/tunevlseg.%0A&entry.1838667208=http%3A//arxiv.org/abs/2410.05239v1&entry.124074799=Read"},
{"title": "GARField: Addressing the visual Sim-to-Real gap in garment manipulation\n  with mesh-attached radiance fields", "author": "Donatien Delehelle and Darwin G. Caldwell and Fei Chen", "abstract": "  While humans intuitively manipulate garments and other textiles items swiftly\nand accurately, it is a significant challenge for robots. A factor crucial to\nthe human performance is the ability to imagine, a priori, the intended result\nof the manipulation intents and hence develop predictions on the garment pose.\nThis allows us to plan from highly obstructed states, adapt our plans as we\ncollect more information and react swiftly to unforeseen circumstances. Robots,\non the other hand, struggle to establish such intuitions and form tight links\nbetween plans and observations. This can be attributed in part to the high cost\nof obtaining densely labelled data for textile manipulation, both in quality\nand quantity. The problem of data collection is a long standing issue in\ndata-based approaches to garment manipulation. Currently, the generation of\nhigh quality and labelled garment manipulation data is mainly attempted through\nadvanced data capture procedures that create simplified state estimations from\nreal-world observations. In this work, however, we propose to generate\nreal-world observations from given object states. To achieve this, we present\nGARField (Garment Attached Radiance Field) a differentiable rendering\narchitecture allowing data generation from simulated states stored as triangle\nmeshes. Code will be available on https://ddonatien.github.io/garfield-website/\n", "link": "http://arxiv.org/abs/2410.05038v1", "date": "2024-10-07", "relevancy": 2.6433, "topK": [{"title": "DressCode: Autoregressively Sewing and Generating Garments from Text\n  Guidance", "link": "http://arxiv.org/abs/2401.16465v3", "similarity": 0.6766}, {"title": "FabricDiffusion: High-Fidelity Texture Transfer for 3D Garments\n  Generation from In-The-Wild Clothing Images", "link": "http://arxiv.org/abs/2410.01801v1", "similarity": 0.6765}, {"title": "Multimodal-Conditioned Latent Diffusion Models for Fashion Image Editing", "link": "http://arxiv.org/abs/2403.14828v2", "similarity": 0.582}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20GARField%3A%20Addressing%20the%20visual%20Sim-to-Real%20gap%20in%20garment%20manipulation%0A%20%20with%20mesh-attached%20radiance%20fields&body=Title%3A%20GARField%3A%20Addressing%20the%20visual%20Sim-to-Real%20gap%20in%20garment%20manipulation%0A%20%20with%20mesh-attached%20radiance%20fields%0AAuthor%3A%20Donatien%20Delehelle%20and%20Darwin%20G.%20Caldwell%20and%20Fei%20Chen%0AAbstract%3A%20%20%20While%20humans%20intuitively%20manipulate%20garments%20and%20other%20textiles%20items%20swiftly%0Aand%20accurately%2C%20it%20is%20a%20significant%20challenge%20for%20robots.%20A%20factor%20crucial%20to%0Athe%20human%20performance%20is%20the%20ability%20to%20imagine%2C%20a%20priori%2C%20the%20intended%20result%0Aof%20the%20manipulation%20intents%20and%20hence%20develop%20predictions%20on%20the%20garment%20pose.%0AThis%20allows%20us%20to%20plan%20from%20highly%20obstructed%20states%2C%20adapt%20our%20plans%20as%20we%0Acollect%20more%20information%20and%20react%20swiftly%20to%20unforeseen%20circumstances.%20Robots%2C%0Aon%20the%20other%20hand%2C%20struggle%20to%20establish%20such%20intuitions%20and%20form%20tight%20links%0Abetween%20plans%20and%20observations.%20This%20can%20be%20attributed%20in%20part%20to%20the%20high%20cost%0Aof%20obtaining%20densely%20labelled%20data%20for%20textile%20manipulation%2C%20both%20in%20quality%0Aand%20quantity.%20The%20problem%20of%20data%20collection%20is%20a%20long%20standing%20issue%20in%0Adata-based%20approaches%20to%20garment%20manipulation.%20Currently%2C%20the%20generation%20of%0Ahigh%20quality%20and%20labelled%20garment%20manipulation%20data%20is%20mainly%20attempted%20through%0Aadvanced%20data%20capture%20procedures%20that%20create%20simplified%20state%20estimations%20from%0Areal-world%20observations.%20In%20this%20work%2C%20however%2C%20we%20propose%20to%20generate%0Areal-world%20observations%20from%20given%20object%20states.%20To%20achieve%20this%2C%20we%20present%0AGARField%20%28Garment%20Attached%20Radiance%20Field%29%20a%20differentiable%20rendering%0Aarchitecture%20allowing%20data%20generation%20from%20simulated%20states%20stored%20as%20triangle%0Ameshes.%20Code%20will%20be%20available%20on%20https%3A//ddonatien.github.io/garfield-website/%0A%0ALink%3A%20http%3A//arxiv.org/abs/2410.05038v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DGARField%253A%2520Addressing%2520the%2520visual%2520Sim-to-Real%2520gap%2520in%2520garment%2520manipulation%250A%2520%2520with%2520mesh-attached%2520radiance%2520fields%26entry.906535625%3DDonatien%2520Delehelle%2520and%2520Darwin%2520G.%2520Caldwell%2520and%2520Fei%2520Chen%26entry.1292438233%3D%2520%2520While%2520humans%2520intuitively%2520manipulate%2520garments%2520and%2520other%2520textiles%2520items%2520swiftly%250Aand%2520accurately%252C%2520it%2520is%2520a%2520significant%2520challenge%2520for%2520robots.%2520A%2520factor%2520crucial%2520to%250Athe%2520human%2520performance%2520is%2520the%2520ability%2520to%2520imagine%252C%2520a%2520priori%252C%2520the%2520intended%2520result%250Aof%2520the%2520manipulation%2520intents%2520and%2520hence%2520develop%2520predictions%2520on%2520the%2520garment%2520pose.%250AThis%2520allows%2520us%2520to%2520plan%2520from%2520highly%2520obstructed%2520states%252C%2520adapt%2520our%2520plans%2520as%2520we%250Acollect%2520more%2520information%2520and%2520react%2520swiftly%2520to%2520unforeseen%2520circumstances.%2520Robots%252C%250Aon%2520the%2520other%2520hand%252C%2520struggle%2520to%2520establish%2520such%2520intuitions%2520and%2520form%2520tight%2520links%250Abetween%2520plans%2520and%2520observations.%2520This%2520can%2520be%2520attributed%2520in%2520part%2520to%2520the%2520high%2520cost%250Aof%2520obtaining%2520densely%2520labelled%2520data%2520for%2520textile%2520manipulation%252C%2520both%2520in%2520quality%250Aand%2520quantity.%2520The%2520problem%2520of%2520data%2520collection%2520is%2520a%2520long%2520standing%2520issue%2520in%250Adata-based%2520approaches%2520to%2520garment%2520manipulation.%2520Currently%252C%2520the%2520generation%2520of%250Ahigh%2520quality%2520and%2520labelled%2520garment%2520manipulation%2520data%2520is%2520mainly%2520attempted%2520through%250Aadvanced%2520data%2520capture%2520procedures%2520that%2520create%2520simplified%2520state%2520estimations%2520from%250Areal-world%2520observations.%2520In%2520this%2520work%252C%2520however%252C%2520we%2520propose%2520to%2520generate%250Areal-world%2520observations%2520from%2520given%2520object%2520states.%2520To%2520achieve%2520this%252C%2520we%2520present%250AGARField%2520%2528Garment%2520Attached%2520Radiance%2520Field%2529%2520a%2520differentiable%2520rendering%250Aarchitecture%2520allowing%2520data%2520generation%2520from%2520simulated%2520states%2520stored%2520as%2520triangle%250Ameshes.%2520Code%2520will%2520be%2520available%2520on%2520https%253A//ddonatien.github.io/garfield-website/%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2410.05038v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=GARField%3A%20Addressing%20the%20visual%20Sim-to-Real%20gap%20in%20garment%20manipulation%0A%20%20with%20mesh-attached%20radiance%20fields&entry.906535625=Donatien%20Delehelle%20and%20Darwin%20G.%20Caldwell%20and%20Fei%20Chen&entry.1292438233=%20%20While%20humans%20intuitively%20manipulate%20garments%20and%20other%20textiles%20items%20swiftly%0Aand%20accurately%2C%20it%20is%20a%20significant%20challenge%20for%20robots.%20A%20factor%20crucial%20to%0Athe%20human%20performance%20is%20the%20ability%20to%20imagine%2C%20a%20priori%2C%20the%20intended%20result%0Aof%20the%20manipulation%20intents%20and%20hence%20develop%20predictions%20on%20the%20garment%20pose.%0AThis%20allows%20us%20to%20plan%20from%20highly%20obstructed%20states%2C%20adapt%20our%20plans%20as%20we%0Acollect%20more%20information%20and%20react%20swiftly%20to%20unforeseen%20circumstances.%20Robots%2C%0Aon%20the%20other%20hand%2C%20struggle%20to%20establish%20such%20intuitions%20and%20form%20tight%20links%0Abetween%20plans%20and%20observations.%20This%20can%20be%20attributed%20in%20part%20to%20the%20high%20cost%0Aof%20obtaining%20densely%20labelled%20data%20for%20textile%20manipulation%2C%20both%20in%20quality%0Aand%20quantity.%20The%20problem%20of%20data%20collection%20is%20a%20long%20standing%20issue%20in%0Adata-based%20approaches%20to%20garment%20manipulation.%20Currently%2C%20the%20generation%20of%0Ahigh%20quality%20and%20labelled%20garment%20manipulation%20data%20is%20mainly%20attempted%20through%0Aadvanced%20data%20capture%20procedures%20that%20create%20simplified%20state%20estimations%20from%0Areal-world%20observations.%20In%20this%20work%2C%20however%2C%20we%20propose%20to%20generate%0Areal-world%20observations%20from%20given%20object%20states.%20To%20achieve%20this%2C%20we%20present%0AGARField%20%28Garment%20Attached%20Radiance%20Field%29%20a%20differentiable%20rendering%0Aarchitecture%20allowing%20data%20generation%20from%20simulated%20states%20stored%20as%20triangle%0Ameshes.%20Code%20will%20be%20available%20on%20https%3A//ddonatien.github.io/garfield-website/%0A&entry.1838667208=http%3A//arxiv.org/abs/2410.05038v1&entry.124074799=Read"},
{"title": "Breaking the Frame: Visual Place Recognition by Overlap Prediction", "author": "Tong Wei and Philipp Lindenberger and Jiri Matas and Daniel Barath", "abstract": "  Visual place recognition methods struggle with occlusions and partial visual\noverlaps. We propose a novel visual place recognition approach based on overlap\nprediction, called VOP, shifting from traditional reliance on global image\nsimilarities and local features to image overlap prediction. VOP proceeds\nco-visible image sections by obtaining patch-level embeddings using a Vision\nTransformer backbone and establishing patch-to-patch correspondences without\nrequiring expensive feature detection and matching. Our approach uses a voting\nmechanism to assess overlap scores for potential database images. It provides a\nnuanced image retrieval metric in challenging scenarios. Experimental results\nshow that VOP leads to more accurate relative pose estimation and localization\nresults on the retrieved image pairs than state-of-the-art baselines on a\nnumber of large-scale, real-world indoor and outdoor benchmarks. The code is\navailable at https://github.com/weitong8591/vop.git.\n", "link": "http://arxiv.org/abs/2406.16204v2", "date": "2024-10-07", "relevancy": 2.6287, "topK": [{"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.5326}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5223}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5223}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Breaking%20the%20Frame%3A%20Visual%20Place%20Recognition%20by%20Overlap%20Prediction&body=Title%3A%20Breaking%20the%20Frame%3A%20Visual%20Place%20Recognition%20by%20Overlap%20Prediction%0AAuthor%3A%20Tong%20Wei%20and%20Philipp%20Lindenberger%20and%20Jiri%20Matas%20and%20Daniel%20Barath%0AAbstract%3A%20%20%20Visual%20place%20recognition%20methods%20struggle%20with%20occlusions%20and%20partial%20visual%0Aoverlaps.%20We%20propose%20a%20novel%20visual%20place%20recognition%20approach%20based%20on%20overlap%0Aprediction%2C%20called%20VOP%2C%20shifting%20from%20traditional%20reliance%20on%20global%20image%0Asimilarities%20and%20local%20features%20to%20image%20overlap%20prediction.%20VOP%20proceeds%0Aco-visible%20image%20sections%20by%20obtaining%20patch-level%20embeddings%20using%20a%20Vision%0ATransformer%20backbone%20and%20establishing%20patch-to-patch%20correspondences%20without%0Arequiring%20expensive%20feature%20detection%20and%20matching.%20Our%20approach%20uses%20a%20voting%0Amechanism%20to%20assess%20overlap%20scores%20for%20potential%20database%20images.%20It%20provides%20a%0Anuanced%20image%20retrieval%20metric%20in%20challenging%20scenarios.%20Experimental%20results%0Ashow%20that%20VOP%20leads%20to%20more%20accurate%20relative%20pose%20estimation%20and%20localization%0Aresults%20on%20the%20retrieved%20image%20pairs%20than%20state-of-the-art%20baselines%20on%20a%0Anumber%20of%20large-scale%2C%20real-world%20indoor%20and%20outdoor%20benchmarks.%20The%20code%20is%0Aavailable%20at%20https%3A//github.com/weitong8591/vop.git.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2406.16204v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DBreaking%2520the%2520Frame%253A%2520Visual%2520Place%2520Recognition%2520by%2520Overlap%2520Prediction%26entry.906535625%3DTong%2520Wei%2520and%2520Philipp%2520Lindenberger%2520and%2520Jiri%2520Matas%2520and%2520Daniel%2520Barath%26entry.1292438233%3D%2520%2520Visual%2520place%2520recognition%2520methods%2520struggle%2520with%2520occlusions%2520and%2520partial%2520visual%250Aoverlaps.%2520We%2520propose%2520a%2520novel%2520visual%2520place%2520recognition%2520approach%2520based%2520on%2520overlap%250Aprediction%252C%2520called%2520VOP%252C%2520shifting%2520from%2520traditional%2520reliance%2520on%2520global%2520image%250Asimilarities%2520and%2520local%2520features%2520to%2520image%2520overlap%2520prediction.%2520VOP%2520proceeds%250Aco-visible%2520image%2520sections%2520by%2520obtaining%2520patch-level%2520embeddings%2520using%2520a%2520Vision%250ATransformer%2520backbone%2520and%2520establishing%2520patch-to-patch%2520correspondences%2520without%250Arequiring%2520expensive%2520feature%2520detection%2520and%2520matching.%2520Our%2520approach%2520uses%2520a%2520voting%250Amechanism%2520to%2520assess%2520overlap%2520scores%2520for%2520potential%2520database%2520images.%2520It%2520provides%2520a%250Anuanced%2520image%2520retrieval%2520metric%2520in%2520challenging%2520scenarios.%2520Experimental%2520results%250Ashow%2520that%2520VOP%2520leads%2520to%2520more%2520accurate%2520relative%2520pose%2520estimation%2520and%2520localization%250Aresults%2520on%2520the%2520retrieved%2520image%2520pairs%2520than%2520state-of-the-art%2520baselines%2520on%2520a%250Anumber%2520of%2520large-scale%252C%2520real-world%2520indoor%2520and%2520outdoor%2520benchmarks.%2520The%2520code%2520is%250Aavailable%2520at%2520https%253A//github.com/weitong8591/vop.git.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2406.16204v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Breaking%20the%20Frame%3A%20Visual%20Place%20Recognition%20by%20Overlap%20Prediction&entry.906535625=Tong%20Wei%20and%20Philipp%20Lindenberger%20and%20Jiri%20Matas%20and%20Daniel%20Barath&entry.1292438233=%20%20Visual%20place%20recognition%20methods%20struggle%20with%20occlusions%20and%20partial%20visual%0Aoverlaps.%20We%20propose%20a%20novel%20visual%20place%20recognition%20approach%20based%20on%20overlap%0Aprediction%2C%20called%20VOP%2C%20shifting%20from%20traditional%20reliance%20on%20global%20image%0Asimilarities%20and%20local%20features%20to%20image%20overlap%20prediction.%20VOP%20proceeds%0Aco-visible%20image%20sections%20by%20obtaining%20patch-level%20embeddings%20using%20a%20Vision%0ATransformer%20backbone%20and%20establishing%20patch-to-patch%20correspondences%20without%0Arequiring%20expensive%20feature%20detection%20and%20matching.%20Our%20approach%20uses%20a%20voting%0Amechanism%20to%20assess%20overlap%20scores%20for%20potential%20database%20images.%20It%20provides%20a%0Anuanced%20image%20retrieval%20metric%20in%20challenging%20scenarios.%20Experimental%20results%0Ashow%20that%20VOP%20leads%20to%20more%20accurate%20relative%20pose%20estimation%20and%20localization%0Aresults%20on%20the%20retrieved%20image%20pairs%20than%20state-of-the-art%20baselines%20on%20a%0Anumber%20of%20large-scale%2C%20real-world%20indoor%20and%20outdoor%20benchmarks.%20The%20code%20is%0Aavailable%20at%20https%3A//github.com/weitong8591/vop.git.%0A&entry.1838667208=http%3A//arxiv.org/abs/2406.16204v2&entry.124074799=Read"},
{"title": "DEPT: Decoupled Embeddings for Pre-training Language Models", "author": "Alex Iacob and Lorenzo Sani and Meghdad Kurmanji and William F. Shen and Xinchi Qiu and Dongqi Cai and Yan Gao and Nicholas D. Lane", "abstract": "  Language Model pre-training benefits from a broader data mixture to enhance\nperformance across domains and languages. However, training on such\nheterogeneous text corpora is complex, requiring extensive and cost-intensive\nefforts. Since these data sources vary in lexical, syntactic, and semantic\naspects, they cause negative interference or the \"curse of multilinguality\". We\npropose a novel pre-training framework to alleviate this curse. Our method,\nDEPT, decouples the embedding layers from the transformer body while\nsimultaneously training the latter in multiple contexts. DEPT enables the model\nto train without being bound to a shared global vocabulary. DEPT: (1) can train\nrobustly and effectively under significant data heterogeneity, (2) reduces the\nparameter count of the token embeddings by up to 80% and the communication\ncosts by 675x for billion-scale models (3) enhances model generalization and\nplasticity in adapting to new languages and domains, and (4) allows training\nwith custom optimized vocabulary per data source. We prove DEPT's potential by\nperforming the first vocabulary-agnostic federated multilingual pre-training of\na 1.3 billion-parameter model across high and low-resource languages, reducing\nits parameter count by 409 million.\n", "link": "http://arxiv.org/abs/2410.05021v1", "date": "2024-10-07", "relevancy": 2.6272, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5266}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5266}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5231}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20DEPT%3A%20Decoupled%20Embeddings%20for%20Pre-training%20Language%20Models&body=Title%3A%20DEPT%3A%20Decoupled%20Embeddings%20for%20Pre-training%20Language%20Models%0AAuthor%3A%20Alex%20Iacob%20and%20Lorenzo%20Sani%20and%20Meghdad%20Kurmanji%20and%20William%20F.%20Shen%20and%20Xinchi%20Qiu%20and%20Dongqi%20Cai%20and%20Yan%20Gao%20and%20Nicholas%20D.%20Lane%0AAbstract%3A%20%20%20Language%20Model%20pre-training%20benefits%20from%20a%20broader%20data%20mixture%20to%20enhance%0Aperformance%20across%20domains%20and%20languages.%20However%2C%20training%20on%20such%0Aheterogeneous%20text%20corpora%20is%20complex%2C%20requiring%20extensive%20and%20cost-intensive%0Aefforts.%20Since%20these%20data%20sources%20vary%20in%20lexical%2C%20syntactic%2C%20and%20semantic%0Aaspects%2C%20they%20cause%20negative%20interference%20or%20the%20%22curse%20of%20multilinguality%22.%20We%0Apropose%20a%20novel%20pre-training%20framework%20to%20alleviate%20this%20curse.%20Our%20method%2C%0ADEPT%2C%20decouples%20the%20embedding%20layers%20from%20the%20transformer%20body%20while%0Asimultaneously%20training%20the%20latter%20in%20multiple%20contexts.%20DEPT%20enables%20the%20model%0Ato%20train%20without%20being%20bound%20to%20a%20shared%20global%20vocabulary.%20DEPT%3A%20%281%29%20can%20train%0Arobustly%20and%20effectively%20under%20significant%20data%20heterogeneity%2C%20%282%29%20reduces%20the%0Aparameter%20count%20of%20the%20token%20embeddings%20by%20up%20to%2080%25%20and%20the%20communication%0Acosts%20by%20675x%20for%20billion-scale%20models%20%283%29%20enhances%20model%20generalization%20and%0Aplasticity%20in%20adapting%20to%20new%20languages%20and%20domains%2C%20and%20%284%29%20allows%20training%0Awith%20custom%20optimized%20vocabulary%20per%20data%20source.%20We%20prove%20DEPT%27s%20potential%20by%0Aperforming%20the%20first%20vocabulary-agnostic%20federated%20multilingual%20pre-training%20of%0Aa%201.3%20billion-parameter%20model%20across%20high%20and%20low-resource%20languages%2C%20reducing%0Aits%20parameter%20count%20by%20409%20million.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2410.05021v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DDEPT%253A%2520Decoupled%2520Embeddings%2520for%2520Pre-training%2520Language%2520Models%26entry.906535625%3DAlex%2520Iacob%2520and%2520Lorenzo%2520Sani%2520and%2520Meghdad%2520Kurmanji%2520and%2520William%2520F.%2520Shen%2520and%2520Xinchi%2520Qiu%2520and%2520Dongqi%2520Cai%2520and%2520Yan%2520Gao%2520and%2520Nicholas%2520D.%2520Lane%26entry.1292438233%3D%2520%2520Language%2520Model%2520pre-training%2520benefits%2520from%2520a%2520broader%2520data%2520mixture%2520to%2520enhance%250Aperformance%2520across%2520domains%2520and%2520languages.%2520However%252C%2520training%2520on%2520such%250Aheterogeneous%2520text%2520corpora%2520is%2520complex%252C%2520requiring%2520extensive%2520and%2520cost-intensive%250Aefforts.%2520Since%2520these%2520data%2520sources%2520vary%2520in%2520lexical%252C%2520syntactic%252C%2520and%2520semantic%250Aaspects%252C%2520they%2520cause%2520negative%2520interference%2520or%2520the%2520%2522curse%2520of%2520multilinguality%2522.%2520We%250Apropose%2520a%2520novel%2520pre-training%2520framework%2520to%2520alleviate%2520this%2520curse.%2520Our%2520method%252C%250ADEPT%252C%2520decouples%2520the%2520embedding%2520layers%2520from%2520the%2520transformer%2520body%2520while%250Asimultaneously%2520training%2520the%2520latter%2520in%2520multiple%2520contexts.%2520DEPT%2520enables%2520the%2520model%250Ato%2520train%2520without%2520being%2520bound%2520to%2520a%2520shared%2520global%2520vocabulary.%2520DEPT%253A%2520%25281%2529%2520can%2520train%250Arobustly%2520and%2520effectively%2520under%2520significant%2520data%2520heterogeneity%252C%2520%25282%2529%2520reduces%2520the%250Aparameter%2520count%2520of%2520the%2520token%2520embeddings%2520by%2520up%2520to%252080%2525%2520and%2520the%2520communication%250Acosts%2520by%2520675x%2520for%2520billion-scale%2520models%2520%25283%2529%2520enhances%2520model%2520generalization%2520and%250Aplasticity%2520in%2520adapting%2520to%2520new%2520languages%2520and%2520domains%252C%2520and%2520%25284%2529%2520allows%2520training%250Awith%2520custom%2520optimized%2520vocabulary%2520per%2520data%2520source.%2520We%2520prove%2520DEPT%2527s%2520potential%2520by%250Aperforming%2520the%2520first%2520vocabulary-agnostic%2520federated%2520multilingual%2520pre-training%2520of%250Aa%25201.3%2520billion-parameter%2520model%2520across%2520high%2520and%2520low-resource%2520languages%252C%2520reducing%250Aits%2520parameter%2520count%2520by%2520409%2520million.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2410.05021v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=DEPT%3A%20Decoupled%20Embeddings%20for%20Pre-training%20Language%20Models&entry.906535625=Alex%20Iacob%20and%20Lorenzo%20Sani%20and%20Meghdad%20Kurmanji%20and%20William%20F.%20Shen%20and%20Xinchi%20Qiu%20and%20Dongqi%20Cai%20and%20Yan%20Gao%20and%20Nicholas%20D.%20Lane&entry.1292438233=%20%20Language%20Model%20pre-training%20benefits%20from%20a%20broader%20data%20mixture%20to%20enhance%0Aperformance%20across%20domains%20and%20languages.%20However%2C%20training%20on%20such%0Aheterogeneous%20text%20corpora%20is%20complex%2C%20requiring%20extensive%20and%20cost-intensive%0Aefforts.%20Since%20these%20data%20sources%20vary%20in%20lexical%2C%20syntactic%2C%20and%20semantic%0Aaspects%2C%20they%20cause%20negative%20interference%20or%20the%20%22curse%20of%20multilinguality%22.%20We%0Apropose%20a%20novel%20pre-training%20framework%20to%20alleviate%20this%20curse.%20Our%20method%2C%0ADEPT%2C%20decouples%20the%20embedding%20layers%20from%20the%20transformer%20body%20while%0Asimultaneously%20training%20the%20latter%20in%20multiple%20contexts.%20DEPT%20enables%20the%20model%0Ato%20train%20without%20being%20bound%20to%20a%20shared%20global%20vocabulary.%20DEPT%3A%20%281%29%20can%20train%0Arobustly%20and%20effectively%20under%20significant%20data%20heterogeneity%2C%20%282%29%20reduces%20the%0Aparameter%20count%20of%20the%20token%20embeddings%20by%20up%20to%2080%25%20and%20the%20communication%0Acosts%20by%20675x%20for%20billion-scale%20models%20%283%29%20enhances%20model%20generalization%20and%0Aplasticity%20in%20adapting%20to%20new%20languages%20and%20domains%2C%20and%20%284%29%20allows%20training%0Awith%20custom%20optimized%20vocabulary%20per%20data%20source.%20We%20prove%20DEPT%27s%20potential%20by%0Aperforming%20the%20first%20vocabulary-agnostic%20federated%20multilingual%20pre-training%20of%0Aa%201.3%20billion-parameter%20model%20across%20high%20and%20low-resource%20languages%2C%20reducing%0Aits%20parameter%20count%20by%20409%20million.%0A&entry.1838667208=http%3A//arxiv.org/abs/2410.05021v1&entry.124074799=Read"},
{"title": "Failure-Proof Non-Contrastive Self-Supervised Learning", "author": "Emanuele Sansone and Tim Lebailly and Tinne Tuytelaars", "abstract": "  We identify sufficient conditions to avoid known failure modes, including\nrepresentation, dimensional, cluster and intracluster collapses, occurring in\nnon-contrastive self-supervised learning. Based on these findings, we propose a\nprincipled design for the projector and loss function. We theoretically\ndemonstrate that this design introduces an inductive bias that promotes\nlearning representations that are both decorrelated and clustered without\nexplicit enforcing these properties and leading to improved generalization. To\nthe best of our knowledge, this is the first solution that achieves robust\ntraining with respect to these failure modes while guaranteeing enhanced\ngeneralization performance in downstream tasks. We validate our theoretical\nfindings on image datasets including SVHN, CIFAR10, CIFAR100 and ImageNet-100,\nand show that our solution, dubbed FALCON, outperforms existing feature\ndecorrelation and cluster-based self-supervised learning methods in terms of\ngeneralization to clustering and linear classification tasks.\n", "link": "http://arxiv.org/abs/2410.04959v1", "date": "2024-10-07", "relevancy": 2.6104, "topK": [{"title": "Self-supervised Photographic Image Layout Representation Learning", "link": "http://arxiv.org/abs/2403.03740v1", "similarity": 0.5929}, {"title": "Flexible Visual Recognition by Evidential Modeling of Confusion and Ignorance", "link": "http://openaccess.thecvf.com/content/ICCV2023/html/Fan_Flexible_Visual_Recognition_by_Evidential_Modeling_of_Confusion_and_Ignorance_ICCV_2023_paper.html", "similarity": 0.4882}, {"title": "Weakly-guided self-supervised pretraining for temporal activity detection", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/25189", "similarity": 0.4851}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Failure-Proof%20Non-Contrastive%20Self-Supervised%20Learning&body=Title%3A%20Failure-Proof%20Non-Contrastive%20Self-Supervised%20Learning%0AAuthor%3A%20Emanuele%20Sansone%20and%20Tim%20Lebailly%20and%20Tinne%20Tuytelaars%0AAbstract%3A%20%20%20We%20identify%20sufficient%20conditions%20to%20avoid%20known%20failure%20modes%2C%20including%0Arepresentation%2C%20dimensional%2C%20cluster%20and%20intracluster%20collapses%2C%20occurring%20in%0Anon-contrastive%20self-supervised%20learning.%20Based%20on%20these%20findings%2C%20we%20propose%20a%0Aprincipled%20design%20for%20the%20projector%20and%20loss%20function.%20We%20theoretically%0Ademonstrate%20that%20this%20design%20introduces%20an%20inductive%20bias%20that%20promotes%0Alearning%20representations%20that%20are%20both%20decorrelated%20and%20clustered%20without%0Aexplicit%20enforcing%20these%20properties%20and%20leading%20to%20improved%20generalization.%20To%0Athe%20best%20of%20our%20knowledge%2C%20this%20is%20the%20first%20solution%20that%20achieves%20robust%0Atraining%20with%20respect%20to%20these%20failure%20modes%20while%20guaranteeing%20enhanced%0Ageneralization%20performance%20in%20downstream%20tasks.%20We%20validate%20our%20theoretical%0Afindings%20on%20image%20datasets%20including%20SVHN%2C%20CIFAR10%2C%20CIFAR100%20and%20ImageNet-100%2C%0Aand%20show%20that%20our%20solution%2C%20dubbed%20FALCON%2C%20outperforms%20existing%20feature%0Adecorrelation%20and%20cluster-based%20self-supervised%20learning%20methods%20in%20terms%20of%0Ageneralization%20to%20clustering%20and%20linear%20classification%20tasks.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2410.04959v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DFailure-Proof%2520Non-Contrastive%2520Self-Supervised%2520Learning%26entry.906535625%3DEmanuele%2520Sansone%2520and%2520Tim%2520Lebailly%2520and%2520Tinne%2520Tuytelaars%26entry.1292438233%3D%2520%2520We%2520identify%2520sufficient%2520conditions%2520to%2520avoid%2520known%2520failure%2520modes%252C%2520including%250Arepresentation%252C%2520dimensional%252C%2520cluster%2520and%2520intracluster%2520collapses%252C%2520occurring%2520in%250Anon-contrastive%2520self-supervised%2520learning.%2520Based%2520on%2520these%2520findings%252C%2520we%2520propose%2520a%250Aprincipled%2520design%2520for%2520the%2520projector%2520and%2520loss%2520function.%2520We%2520theoretically%250Ademonstrate%2520that%2520this%2520design%2520introduces%2520an%2520inductive%2520bias%2520that%2520promotes%250Alearning%2520representations%2520that%2520are%2520both%2520decorrelated%2520and%2520clustered%2520without%250Aexplicit%2520enforcing%2520these%2520properties%2520and%2520leading%2520to%2520improved%2520generalization.%2520To%250Athe%2520best%2520of%2520our%2520knowledge%252C%2520this%2520is%2520the%2520first%2520solution%2520that%2520achieves%2520robust%250Atraining%2520with%2520respect%2520to%2520these%2520failure%2520modes%2520while%2520guaranteeing%2520enhanced%250Ageneralization%2520performance%2520in%2520downstream%2520tasks.%2520We%2520validate%2520our%2520theoretical%250Afindings%2520on%2520image%2520datasets%2520including%2520SVHN%252C%2520CIFAR10%252C%2520CIFAR100%2520and%2520ImageNet-100%252C%250Aand%2520show%2520that%2520our%2520solution%252C%2520dubbed%2520FALCON%252C%2520outperforms%2520existing%2520feature%250Adecorrelation%2520and%2520cluster-based%2520self-supervised%2520learning%2520methods%2520in%2520terms%2520of%250Ageneralization%2520to%2520clustering%2520and%2520linear%2520classification%2520tasks.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2410.04959v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Failure-Proof%20Non-Contrastive%20Self-Supervised%20Learning&entry.906535625=Emanuele%20Sansone%20and%20Tim%20Lebailly%20and%20Tinne%20Tuytelaars&entry.1292438233=%20%20We%20identify%20sufficient%20conditions%20to%20avoid%20known%20failure%20modes%2C%20including%0Arepresentation%2C%20dimensional%2C%20cluster%20and%20intracluster%20collapses%2C%20occurring%20in%0Anon-contrastive%20self-supervised%20learning.%20Based%20on%20these%20findings%2C%20we%20propose%20a%0Aprincipled%20design%20for%20the%20projector%20and%20loss%20function.%20We%20theoretically%0Ademonstrate%20that%20this%20design%20introduces%20an%20inductive%20bias%20that%20promotes%0Alearning%20representations%20that%20are%20both%20decorrelated%20and%20clustered%20without%0Aexplicit%20enforcing%20these%20properties%20and%20leading%20to%20improved%20generalization.%20To%0Athe%20best%20of%20our%20knowledge%2C%20this%20is%20the%20first%20solution%20that%20achieves%20robust%0Atraining%20with%20respect%20to%20these%20failure%20modes%20while%20guaranteeing%20enhanced%0Ageneralization%20performance%20in%20downstream%20tasks.%20We%20validate%20our%20theoretical%0Afindings%20on%20image%20datasets%20including%20SVHN%2C%20CIFAR10%2C%20CIFAR100%20and%20ImageNet-100%2C%0Aand%20show%20that%20our%20solution%2C%20dubbed%20FALCON%2C%20outperforms%20existing%20feature%0Adecorrelation%20and%20cluster-based%20self-supervised%20learning%20methods%20in%20terms%20of%0Ageneralization%20to%20clustering%20and%20linear%20classification%20tasks.%0A&entry.1838667208=http%3A//arxiv.org/abs/2410.04959v1&entry.124074799=Read"},
{"title": "NeRAF: 3D Scene Infused Neural Radiance and Acoustic Fields", "author": "Amandine Brunetto and Sascha Hornauer and Fabien Moutarde", "abstract": "  Sound plays a major role in human perception. Along with vision, it provides\nessential information for understanding our surroundings. Despite advances in\nneural implicit representations, learning acoustics that align with visual\nscenes remains a challenge. We propose NeRAF, a method that jointly learns\nacoustic and radiance fields. NeRAF synthesizes both novel views and\nspatialized room impulse responses (RIR) at new positions by conditioning the\nacoustic field on 3D scene geometric and appearance priors from the radiance\nfield. The generated RIR can be applied to auralize any audio signal. Each\nmodality can be rendered independently and at spatially distinct positions,\noffering greater versatility. We demonstrate that NeRAF generates high-quality\naudio on SoundSpaces and RAF datasets, achieving significant performance\nimprovements over prior methods while being more data-efficient. Additionally,\nNeRAF enhances novel view synthesis of complex scenes trained with sparse data\nthrough cross-modal learning. NeRAF is designed as a Nerfstudio module,\nproviding convenient access to realistic audio-visual generation.\n", "link": "http://arxiv.org/abs/2405.18213v2", "date": "2024-10-07", "relevancy": 2.6056, "topK": [{"title": "RTG-SLAM: Real-time 3D Reconstruction at Scale using Gaussian Splatting", "link": "http://arxiv.org/abs/2404.19706v1", "similarity": 0.5381}, {"title": "MiraGe: Editable 2D Images using Gaussian Splatting", "link": "http://arxiv.org/abs/2410.01521v1", "similarity": 0.5158}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5094}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20NeRAF%3A%203D%20Scene%20Infused%20Neural%20Radiance%20and%20Acoustic%20Fields&body=Title%3A%20NeRAF%3A%203D%20Scene%20Infused%20Neural%20Radiance%20and%20Acoustic%20Fields%0AAuthor%3A%20Amandine%20Brunetto%20and%20Sascha%20Hornauer%20and%20Fabien%20Moutarde%0AAbstract%3A%20%20%20Sound%20plays%20a%20major%20role%20in%20human%20perception.%20Along%20with%20vision%2C%20it%20provides%0Aessential%20information%20for%20understanding%20our%20surroundings.%20Despite%20advances%20in%0Aneural%20implicit%20representations%2C%20learning%20acoustics%20that%20align%20with%20visual%0Ascenes%20remains%20a%20challenge.%20We%20propose%20NeRAF%2C%20a%20method%20that%20jointly%20learns%0Aacoustic%20and%20radiance%20fields.%20NeRAF%20synthesizes%20both%20novel%20views%20and%0Aspatialized%20room%20impulse%20responses%20%28RIR%29%20at%20new%20positions%20by%20conditioning%20the%0Aacoustic%20field%20on%203D%20scene%20geometric%20and%20appearance%20priors%20from%20the%20radiance%0Afield.%20The%20generated%20RIR%20can%20be%20applied%20to%20auralize%20any%20audio%20signal.%20Each%0Amodality%20can%20be%20rendered%20independently%20and%20at%20spatially%20distinct%20positions%2C%0Aoffering%20greater%20versatility.%20We%20demonstrate%20that%20NeRAF%20generates%20high-quality%0Aaudio%20on%20SoundSpaces%20and%20RAF%20datasets%2C%20achieving%20significant%20performance%0Aimprovements%20over%20prior%20methods%20while%20being%20more%20data-efficient.%20Additionally%2C%0ANeRAF%20enhances%20novel%20view%20synthesis%20of%20complex%20scenes%20trained%20with%20sparse%20data%0Athrough%20cross-modal%20learning.%20NeRAF%20is%20designed%20as%20a%20Nerfstudio%20module%2C%0Aproviding%20convenient%20access%20to%20realistic%20audio-visual%20generation.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2405.18213v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DNeRAF%253A%25203D%2520Scene%2520Infused%2520Neural%2520Radiance%2520and%2520Acoustic%2520Fields%26entry.906535625%3DAmandine%2520Brunetto%2520and%2520Sascha%2520Hornauer%2520and%2520Fabien%2520Moutarde%26entry.1292438233%3D%2520%2520Sound%2520plays%2520a%2520major%2520role%2520in%2520human%2520perception.%2520Along%2520with%2520vision%252C%2520it%2520provides%250Aessential%2520information%2520for%2520understanding%2520our%2520surroundings.%2520Despite%2520advances%2520in%250Aneural%2520implicit%2520representations%252C%2520learning%2520acoustics%2520that%2520align%2520with%2520visual%250Ascenes%2520remains%2520a%2520challenge.%2520We%2520propose%2520NeRAF%252C%2520a%2520method%2520that%2520jointly%2520learns%250Aacoustic%2520and%2520radiance%2520fields.%2520NeRAF%2520synthesizes%2520both%2520novel%2520views%2520and%250Aspatialized%2520room%2520impulse%2520responses%2520%2528RIR%2529%2520at%2520new%2520positions%2520by%2520conditioning%2520the%250Aacoustic%2520field%2520on%25203D%2520scene%2520geometric%2520and%2520appearance%2520priors%2520from%2520the%2520radiance%250Afield.%2520The%2520generated%2520RIR%2520can%2520be%2520applied%2520to%2520auralize%2520any%2520audio%2520signal.%2520Each%250Amodality%2520can%2520be%2520rendered%2520independently%2520and%2520at%2520spatially%2520distinct%2520positions%252C%250Aoffering%2520greater%2520versatility.%2520We%2520demonstrate%2520that%2520NeRAF%2520generates%2520high-quality%250Aaudio%2520on%2520SoundSpaces%2520and%2520RAF%2520datasets%252C%2520achieving%2520significant%2520performance%250Aimprovements%2520over%2520prior%2520methods%2520while%2520being%2520more%2520data-efficient.%2520Additionally%252C%250ANeRAF%2520enhances%2520novel%2520view%2520synthesis%2520of%2520complex%2520scenes%2520trained%2520with%2520sparse%2520data%250Athrough%2520cross-modal%2520learning.%2520NeRAF%2520is%2520designed%2520as%2520a%2520Nerfstudio%2520module%252C%250Aproviding%2520convenient%2520access%2520to%2520realistic%2520audio-visual%2520generation.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2405.18213v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=NeRAF%3A%203D%20Scene%20Infused%20Neural%20Radiance%20and%20Acoustic%20Fields&entry.906535625=Amandine%20Brunetto%20and%20Sascha%20Hornauer%20and%20Fabien%20Moutarde&entry.1292438233=%20%20Sound%20plays%20a%20major%20role%20in%20human%20perception.%20Along%20with%20vision%2C%20it%20provides%0Aessential%20information%20for%20understanding%20our%20surroundings.%20Despite%20advances%20in%0Aneural%20implicit%20representations%2C%20learning%20acoustics%20that%20align%20with%20visual%0Ascenes%20remains%20a%20challenge.%20We%20propose%20NeRAF%2C%20a%20method%20that%20jointly%20learns%0Aacoustic%20and%20radiance%20fields.%20NeRAF%20synthesizes%20both%20novel%20views%20and%0Aspatialized%20room%20impulse%20responses%20%28RIR%29%20at%20new%20positions%20by%20conditioning%20the%0Aacoustic%20field%20on%203D%20scene%20geometric%20and%20appearance%20priors%20from%20the%20radiance%0Afield.%20The%20generated%20RIR%20can%20be%20applied%20to%20auralize%20any%20audio%20signal.%20Each%0Amodality%20can%20be%20rendered%20independently%20and%20at%20spatially%20distinct%20positions%2C%0Aoffering%20greater%20versatility.%20We%20demonstrate%20that%20NeRAF%20generates%20high-quality%0Aaudio%20on%20SoundSpaces%20and%20RAF%20datasets%2C%20achieving%20significant%20performance%0Aimprovements%20over%20prior%20methods%20while%20being%20more%20data-efficient.%20Additionally%2C%0ANeRAF%20enhances%20novel%20view%20synthesis%20of%20complex%20scenes%20trained%20with%20sparse%20data%0Athrough%20cross-modal%20learning.%20NeRAF%20is%20designed%20as%20a%20Nerfstudio%20module%2C%0Aproviding%20convenient%20access%20to%20realistic%20audio-visual%20generation.%0A&entry.1838667208=http%3A//arxiv.org/abs/2405.18213v2&entry.124074799=Read"},
{"title": "Seeking Flat Minima with Mean Teacher on Semi- and Weakly-Supervised\n  Domain Generalization for Object Detection", "author": "Ryosuke Furuta and Yoichi Sato", "abstract": "  Object detectors do not work well when domains largely differ between\ntraining and testing data. To overcome this domain gap in object detection\nwithout requiring expensive annotations, we consider two problem settings:\nsemi-supervised domain generalizable object detection (SS-DGOD) and\nweakly-supervised DGOD (WS-DGOD). In contrast to the conventional domain\ngeneralization for object detection that requires labeled data from multiple\ndomains, SS-DGOD and WS-DGOD require labeled data only from one domain and\nunlabeled or weakly-labeled data from multiple domains for training. In this\npaper, we show that object detectors can be effectively trained on the two\nsettings with the same Mean Teacher learning framework, where a student network\nis trained with pseudo-labels output from a teacher on the unlabeled or\nweakly-labeled data. We provide novel interpretations of why the Mean Teacher\nlearning framework works well on the two settings in terms of the relationships\nbetween the generalization gap and flat minima in parameter space. On the basis\nof the interpretations, we also show that incorporating a simple regularization\nmethod into the Mean Teacher learning framework leads to flatter minima. The\nexperimental results demonstrate that the regularization leads to flatter\nminima and boosts the performance of the detectors trained with the Mean\nTeacher learning framework on the two settings.\n", "link": "http://arxiv.org/abs/2310.19351v3", "date": "2024-10-07", "relevancy": 2.6014, "topK": [{"title": "DDM-NET: End-to-end learning of keypoint feature Detection, Description and Matching for 3D localization", "link": "https://arxiv.org/abs/2212.04575", "similarity": 0.5288}, {"title": "Self-supervised Photographic Image Layout Representation Learning", "link": "http://arxiv.org/abs/2403.03740v1", "similarity": 0.5178}, {"title": "Weakly-guided self-supervised pretraining for temporal activity detection", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/25189", "similarity": 0.5142}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Seeking%20Flat%20Minima%20with%20Mean%20Teacher%20on%20Semi-%20and%20Weakly-Supervised%0A%20%20Domain%20Generalization%20for%20Object%20Detection&body=Title%3A%20Seeking%20Flat%20Minima%20with%20Mean%20Teacher%20on%20Semi-%20and%20Weakly-Supervised%0A%20%20Domain%20Generalization%20for%20Object%20Detection%0AAuthor%3A%20Ryosuke%20Furuta%20and%20Yoichi%20Sato%0AAbstract%3A%20%20%20Object%20detectors%20do%20not%20work%20well%20when%20domains%20largely%20differ%20between%0Atraining%20and%20testing%20data.%20To%20overcome%20this%20domain%20gap%20in%20object%20detection%0Awithout%20requiring%20expensive%20annotations%2C%20we%20consider%20two%20problem%20settings%3A%0Asemi-supervised%20domain%20generalizable%20object%20detection%20%28SS-DGOD%29%20and%0Aweakly-supervised%20DGOD%20%28WS-DGOD%29.%20In%20contrast%20to%20the%20conventional%20domain%0Ageneralization%20for%20object%20detection%20that%20requires%20labeled%20data%20from%20multiple%0Adomains%2C%20SS-DGOD%20and%20WS-DGOD%20require%20labeled%20data%20only%20from%20one%20domain%20and%0Aunlabeled%20or%20weakly-labeled%20data%20from%20multiple%20domains%20for%20training.%20In%20this%0Apaper%2C%20we%20show%20that%20object%20detectors%20can%20be%20effectively%20trained%20on%20the%20two%0Asettings%20with%20the%20same%20Mean%20Teacher%20learning%20framework%2C%20where%20a%20student%20network%0Ais%20trained%20with%20pseudo-labels%20output%20from%20a%20teacher%20on%20the%20unlabeled%20or%0Aweakly-labeled%20data.%20We%20provide%20novel%20interpretations%20of%20why%20the%20Mean%20Teacher%0Alearning%20framework%20works%20well%20on%20the%20two%20settings%20in%20terms%20of%20the%20relationships%0Abetween%20the%20generalization%20gap%20and%20flat%20minima%20in%20parameter%20space.%20On%20the%20basis%0Aof%20the%20interpretations%2C%20we%20also%20show%20that%20incorporating%20a%20simple%20regularization%0Amethod%20into%20the%20Mean%20Teacher%20learning%20framework%20leads%20to%20flatter%20minima.%20The%0Aexperimental%20results%20demonstrate%20that%20the%20regularization%20leads%20to%20flatter%0Aminima%20and%20boosts%20the%20performance%20of%20the%20detectors%20trained%20with%20the%20Mean%0ATeacher%20learning%20framework%20on%20the%20two%20settings.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2310.19351v3%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DSeeking%2520Flat%2520Minima%2520with%2520Mean%2520Teacher%2520on%2520Semi-%2520and%2520Weakly-Supervised%250A%2520%2520Domain%2520Generalization%2520for%2520Object%2520Detection%26entry.906535625%3DRyosuke%2520Furuta%2520and%2520Yoichi%2520Sato%26entry.1292438233%3D%2520%2520Object%2520detectors%2520do%2520not%2520work%2520well%2520when%2520domains%2520largely%2520differ%2520between%250Atraining%2520and%2520testing%2520data.%2520To%2520overcome%2520this%2520domain%2520gap%2520in%2520object%2520detection%250Awithout%2520requiring%2520expensive%2520annotations%252C%2520we%2520consider%2520two%2520problem%2520settings%253A%250Asemi-supervised%2520domain%2520generalizable%2520object%2520detection%2520%2528SS-DGOD%2529%2520and%250Aweakly-supervised%2520DGOD%2520%2528WS-DGOD%2529.%2520In%2520contrast%2520to%2520the%2520conventional%2520domain%250Ageneralization%2520for%2520object%2520detection%2520that%2520requires%2520labeled%2520data%2520from%2520multiple%250Adomains%252C%2520SS-DGOD%2520and%2520WS-DGOD%2520require%2520labeled%2520data%2520only%2520from%2520one%2520domain%2520and%250Aunlabeled%2520or%2520weakly-labeled%2520data%2520from%2520multiple%2520domains%2520for%2520training.%2520In%2520this%250Apaper%252C%2520we%2520show%2520that%2520object%2520detectors%2520can%2520be%2520effectively%2520trained%2520on%2520the%2520two%250Asettings%2520with%2520the%2520same%2520Mean%2520Teacher%2520learning%2520framework%252C%2520where%2520a%2520student%2520network%250Ais%2520trained%2520with%2520pseudo-labels%2520output%2520from%2520a%2520teacher%2520on%2520the%2520unlabeled%2520or%250Aweakly-labeled%2520data.%2520We%2520provide%2520novel%2520interpretations%2520of%2520why%2520the%2520Mean%2520Teacher%250Alearning%2520framework%2520works%2520well%2520on%2520the%2520two%2520settings%2520in%2520terms%2520of%2520the%2520relationships%250Abetween%2520the%2520generalization%2520gap%2520and%2520flat%2520minima%2520in%2520parameter%2520space.%2520On%2520the%2520basis%250Aof%2520the%2520interpretations%252C%2520we%2520also%2520show%2520that%2520incorporating%2520a%2520simple%2520regularization%250Amethod%2520into%2520the%2520Mean%2520Teacher%2520learning%2520framework%2520leads%2520to%2520flatter%2520minima.%2520The%250Aexperimental%2520results%2520demonstrate%2520that%2520the%2520regularization%2520leads%2520to%2520flatter%250Aminima%2520and%2520boosts%2520the%2520performance%2520of%2520the%2520detectors%2520trained%2520with%2520the%2520Mean%250ATeacher%2520learning%2520framework%2520on%2520the%2520two%2520settings.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2310.19351v3%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Seeking%20Flat%20Minima%20with%20Mean%20Teacher%20on%20Semi-%20and%20Weakly-Supervised%0A%20%20Domain%20Generalization%20for%20Object%20Detection&entry.906535625=Ryosuke%20Furuta%20and%20Yoichi%20Sato&entry.1292438233=%20%20Object%20detectors%20do%20not%20work%20well%20when%20domains%20largely%20differ%20between%0Atraining%20and%20testing%20data.%20To%20overcome%20this%20domain%20gap%20in%20object%20detection%0Awithout%20requiring%20expensive%20annotations%2C%20we%20consider%20two%20problem%20settings%3A%0Asemi-supervised%20domain%20generalizable%20object%20detection%20%28SS-DGOD%29%20and%0Aweakly-supervised%20DGOD%20%28WS-DGOD%29.%20In%20contrast%20to%20the%20conventional%20domain%0Ageneralization%20for%20object%20detection%20that%20requires%20labeled%20data%20from%20multiple%0Adomains%2C%20SS-DGOD%20and%20WS-DGOD%20require%20labeled%20data%20only%20from%20one%20domain%20and%0Aunlabeled%20or%20weakly-labeled%20data%20from%20multiple%20domains%20for%20training.%20In%20this%0Apaper%2C%20we%20show%20that%20object%20detectors%20can%20be%20effectively%20trained%20on%20the%20two%0Asettings%20with%20the%20same%20Mean%20Teacher%20learning%20framework%2C%20where%20a%20student%20network%0Ais%20trained%20with%20pseudo-labels%20output%20from%20a%20teacher%20on%20the%20unlabeled%20or%0Aweakly-labeled%20data.%20We%20provide%20novel%20interpretations%20of%20why%20the%20Mean%20Teacher%0Alearning%20framework%20works%20well%20on%20the%20two%20settings%20in%20terms%20of%20the%20relationships%0Abetween%20the%20generalization%20gap%20and%20flat%20minima%20in%20parameter%20space.%20On%20the%20basis%0Aof%20the%20interpretations%2C%20we%20also%20show%20that%20incorporating%20a%20simple%20regularization%0Amethod%20into%20the%20Mean%20Teacher%20learning%20framework%20leads%20to%20flatter%20minima.%20The%0Aexperimental%20results%20demonstrate%20that%20the%20regularization%20leads%20to%20flatter%0Aminima%20and%20boosts%20the%20performance%20of%20the%20detectors%20trained%20with%20the%20Mean%0ATeacher%20learning%20framework%20on%20the%20two%20settings.%0A&entry.1838667208=http%3A//arxiv.org/abs/2310.19351v3&entry.124074799=Read"},
{"title": "LLMs Know More Than They Show: On the Intrinsic Representation of LLM\n  Hallucinations", "author": "Hadas Orgad and Michael Toker and Zorik Gekhman and Roi Reichart and Idan Szpektor and Hadas Kotek and Yonatan Belinkov", "abstract": "  Large language models (LLMs) often produce errors, including factual\ninaccuracies, biases, and reasoning failures, collectively referred to as\n\"hallucinations\". Recent studies have demonstrated that LLMs' internal states\nencode information regarding the truthfulness of their outputs, and that this\ninformation can be utilized to detect errors. In this work, we show that the\ninternal representations of LLMs encode much more information about\ntruthfulness than previously recognized. We first discover that the\ntruthfulness information is concentrated in specific tokens, and leveraging\nthis property significantly enhances error detection performance. Yet, we show\nthat such error detectors fail to generalize across datasets, implying that --\ncontrary to prior claims -- truthfulness encoding is not universal but rather\nmultifaceted. Next, we show that internal representations can also be used for\npredicting the types of errors the model is likely to make, facilitating the\ndevelopment of tailored mitigation strategies. Lastly, we reveal a discrepancy\nbetween LLMs' internal encoding and external behavior: they may encode the\ncorrect answer, yet consistently generate an incorrect one. Taken together,\nthese insights deepen our understanding of LLM errors from the model's internal\nperspective, which can guide future research on enhancing error analysis and\nmitigation.\n", "link": "http://arxiv.org/abs/2410.02707v2", "date": "2024-10-07", "relevancy": 2.59, "topK": [{"title": "Flexible Visual Recognition by Evidential Modeling of Confusion and Ignorance", "link": "http://openaccess.thecvf.com/content/ICCV2023/html/Fan_Flexible_Visual_Recognition_by_Evidential_Modeling_of_Confusion_and_Ignorance_ICCV_2023_paper.html", "similarity": 0.5261}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.514}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.514}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20LLMs%20Know%20More%20Than%20They%20Show%3A%20On%20the%20Intrinsic%20Representation%20of%20LLM%0A%20%20Hallucinations&body=Title%3A%20LLMs%20Know%20More%20Than%20They%20Show%3A%20On%20the%20Intrinsic%20Representation%20of%20LLM%0A%20%20Hallucinations%0AAuthor%3A%20Hadas%20Orgad%20and%20Michael%20Toker%20and%20Zorik%20Gekhman%20and%20Roi%20Reichart%20and%20Idan%20Szpektor%20and%20Hadas%20Kotek%20and%20Yonatan%20Belinkov%0AAbstract%3A%20%20%20Large%20language%20models%20%28LLMs%29%20often%20produce%20errors%2C%20including%20factual%0Ainaccuracies%2C%20biases%2C%20and%20reasoning%20failures%2C%20collectively%20referred%20to%20as%0A%22hallucinations%22.%20Recent%20studies%20have%20demonstrated%20that%20LLMs%27%20internal%20states%0Aencode%20information%20regarding%20the%20truthfulness%20of%20their%20outputs%2C%20and%20that%20this%0Ainformation%20can%20be%20utilized%20to%20detect%20errors.%20In%20this%20work%2C%20we%20show%20that%20the%0Ainternal%20representations%20of%20LLMs%20encode%20much%20more%20information%20about%0Atruthfulness%20than%20previously%20recognized.%20We%20first%20discover%20that%20the%0Atruthfulness%20information%20is%20concentrated%20in%20specific%20tokens%2C%20and%20leveraging%0Athis%20property%20significantly%20enhances%20error%20detection%20performance.%20Yet%2C%20we%20show%0Athat%20such%20error%20detectors%20fail%20to%20generalize%20across%20datasets%2C%20implying%20that%20--%0Acontrary%20to%20prior%20claims%20--%20truthfulness%20encoding%20is%20not%20universal%20but%20rather%0Amultifaceted.%20Next%2C%20we%20show%20that%20internal%20representations%20can%20also%20be%20used%20for%0Apredicting%20the%20types%20of%20errors%20the%20model%20is%20likely%20to%20make%2C%20facilitating%20the%0Adevelopment%20of%20tailored%20mitigation%20strategies.%20Lastly%2C%20we%20reveal%20a%20discrepancy%0Abetween%20LLMs%27%20internal%20encoding%20and%20external%20behavior%3A%20they%20may%20encode%20the%0Acorrect%20answer%2C%20yet%20consistently%20generate%20an%20incorrect%20one.%20Taken%20together%2C%0Athese%20insights%20deepen%20our%20understanding%20of%20LLM%20errors%20from%20the%20model%27s%20internal%0Aperspective%2C%20which%20can%20guide%20future%20research%20on%20enhancing%20error%20analysis%20and%0Amitigation.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2410.02707v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DLLMs%2520Know%2520More%2520Than%2520They%2520Show%253A%2520On%2520the%2520Intrinsic%2520Representation%2520of%2520LLM%250A%2520%2520Hallucinations%26entry.906535625%3DHadas%2520Orgad%2520and%2520Michael%2520Toker%2520and%2520Zorik%2520Gekhman%2520and%2520Roi%2520Reichart%2520and%2520Idan%2520Szpektor%2520and%2520Hadas%2520Kotek%2520and%2520Yonatan%2520Belinkov%26entry.1292438233%3D%2520%2520Large%2520language%2520models%2520%2528LLMs%2529%2520often%2520produce%2520errors%252C%2520including%2520factual%250Ainaccuracies%252C%2520biases%252C%2520and%2520reasoning%2520failures%252C%2520collectively%2520referred%2520to%2520as%250A%2522hallucinations%2522.%2520Recent%2520studies%2520have%2520demonstrated%2520that%2520LLMs%2527%2520internal%2520states%250Aencode%2520information%2520regarding%2520the%2520truthfulness%2520of%2520their%2520outputs%252C%2520and%2520that%2520this%250Ainformation%2520can%2520be%2520utilized%2520to%2520detect%2520errors.%2520In%2520this%2520work%252C%2520we%2520show%2520that%2520the%250Ainternal%2520representations%2520of%2520LLMs%2520encode%2520much%2520more%2520information%2520about%250Atruthfulness%2520than%2520previously%2520recognized.%2520We%2520first%2520discover%2520that%2520the%250Atruthfulness%2520information%2520is%2520concentrated%2520in%2520specific%2520tokens%252C%2520and%2520leveraging%250Athis%2520property%2520significantly%2520enhances%2520error%2520detection%2520performance.%2520Yet%252C%2520we%2520show%250Athat%2520such%2520error%2520detectors%2520fail%2520to%2520generalize%2520across%2520datasets%252C%2520implying%2520that%2520--%250Acontrary%2520to%2520prior%2520claims%2520--%2520truthfulness%2520encoding%2520is%2520not%2520universal%2520but%2520rather%250Amultifaceted.%2520Next%252C%2520we%2520show%2520that%2520internal%2520representations%2520can%2520also%2520be%2520used%2520for%250Apredicting%2520the%2520types%2520of%2520errors%2520the%2520model%2520is%2520likely%2520to%2520make%252C%2520facilitating%2520the%250Adevelopment%2520of%2520tailored%2520mitigation%2520strategies.%2520Lastly%252C%2520we%2520reveal%2520a%2520discrepancy%250Abetween%2520LLMs%2527%2520internal%2520encoding%2520and%2520external%2520behavior%253A%2520they%2520may%2520encode%2520the%250Acorrect%2520answer%252C%2520yet%2520consistently%2520generate%2520an%2520incorrect%2520one.%2520Taken%2520together%252C%250Athese%2520insights%2520deepen%2520our%2520understanding%2520of%2520LLM%2520errors%2520from%2520the%2520model%2527s%2520internal%250Aperspective%252C%2520which%2520can%2520guide%2520future%2520research%2520on%2520enhancing%2520error%2520analysis%2520and%250Amitigation.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2410.02707v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=LLMs%20Know%20More%20Than%20They%20Show%3A%20On%20the%20Intrinsic%20Representation%20of%20LLM%0A%20%20Hallucinations&entry.906535625=Hadas%20Orgad%20and%20Michael%20Toker%20and%20Zorik%20Gekhman%20and%20Roi%20Reichart%20and%20Idan%20Szpektor%20and%20Hadas%20Kotek%20and%20Yonatan%20Belinkov&entry.1292438233=%20%20Large%20language%20models%20%28LLMs%29%20often%20produce%20errors%2C%20including%20factual%0Ainaccuracies%2C%20biases%2C%20and%20reasoning%20failures%2C%20collectively%20referred%20to%20as%0A%22hallucinations%22.%20Recent%20studies%20have%20demonstrated%20that%20LLMs%27%20internal%20states%0Aencode%20information%20regarding%20the%20truthfulness%20of%20their%20outputs%2C%20and%20that%20this%0Ainformation%20can%20be%20utilized%20to%20detect%20errors.%20In%20this%20work%2C%20we%20show%20that%20the%0Ainternal%20representations%20of%20LLMs%20encode%20much%20more%20information%20about%0Atruthfulness%20than%20previously%20recognized.%20We%20first%20discover%20that%20the%0Atruthfulness%20information%20is%20concentrated%20in%20specific%20tokens%2C%20and%20leveraging%0Athis%20property%20significantly%20enhances%20error%20detection%20performance.%20Yet%2C%20we%20show%0Athat%20such%20error%20detectors%20fail%20to%20generalize%20across%20datasets%2C%20implying%20that%20--%0Acontrary%20to%20prior%20claims%20--%20truthfulness%20encoding%20is%20not%20universal%20but%20rather%0Amultifaceted.%20Next%2C%20we%20show%20that%20internal%20representations%20can%20also%20be%20used%20for%0Apredicting%20the%20types%20of%20errors%20the%20model%20is%20likely%20to%20make%2C%20facilitating%20the%0Adevelopment%20of%20tailored%20mitigation%20strategies.%20Lastly%2C%20we%20reveal%20a%20discrepancy%0Abetween%20LLMs%27%20internal%20encoding%20and%20external%20behavior%3A%20they%20may%20encode%20the%0Acorrect%20answer%2C%20yet%20consistently%20generate%20an%20incorrect%20one.%20Taken%20together%2C%0Athese%20insights%20deepen%20our%20understanding%20of%20LLM%20errors%20from%20the%20model%27s%20internal%0Aperspective%2C%20which%20can%20guide%20future%20research%20on%20enhancing%20error%20analysis%20and%0Amitigation.%0A&entry.1838667208=http%3A//arxiv.org/abs/2410.02707v2&entry.124074799=Read"},
{"title": "T-JEPA: Augmentation-Free Self-Supervised Learning for Tabular Data", "author": "Hugo Thimonier and Jos\u00e9 Lucas De Melo Costa and Fabrice Popineau and Arpad Rimmel and Bich-Li\u00ean Doan", "abstract": "  Self-supervision is often used for pre-training to foster performance on a\ndownstream task by constructing meaningful representations of samples.\nSelf-supervised learning (SSL) generally involves generating different views of\nthe same sample and thus requires data augmentations that are challenging to\nconstruct for tabular data. This constitutes one of the main challenges of\nself-supervision for structured data. In the present work, we propose a novel\naugmentation-free SSL method for tabular data. Our approach, T-JEPA, relies on\na Joint Embedding Predictive Architecture (JEPA) and is akin to mask\nreconstruction in the latent space. It involves predicting the latent\nrepresentation of one subset of features from the latent representation of a\ndifferent subset within the same sample, thereby learning rich representations\nwithout augmentations. We use our method as a pre-training technique and train\nseveral deep classifiers on the obtained representation. Our experimental\nresults demonstrate a substantial improvement in both classification and\nregression tasks, outperforming models trained directly on samples in their\noriginal data space. Moreover, T-JEPA enables some methods to consistently\noutperform or match the performance of traditional methods likes Gradient\nBoosted Decision Trees. To understand why, we extensively characterize the\nobtained representations and show that T-JEPA effectively identifies relevant\nfeatures for downstream tasks without access to the labels. Additionally, we\nintroduce regularization tokens, a novel regularization method critical for\ntraining of JEPA-based models on structured data.\n", "link": "http://arxiv.org/abs/2410.05016v1", "date": "2024-10-07", "relevancy": 2.5426, "topK": [{"title": "Self-supervised Photographic Image Layout Representation Learning", "link": "http://arxiv.org/abs/2403.03740v1", "similarity": 0.5183}, {"title": "Breadcrumbs: Adversarial class-balanced sampling for long-tailed recognition", "link": "https://link.springer.com/chapter/10.1007/978-3-031-20053-3_37", "similarity": 0.5115}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.4958}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20T-JEPA%3A%20Augmentation-Free%20Self-Supervised%20Learning%20for%20Tabular%20Data&body=Title%3A%20T-JEPA%3A%20Augmentation-Free%20Self-Supervised%20Learning%20for%20Tabular%20Data%0AAuthor%3A%20Hugo%20Thimonier%20and%20Jos%C3%A9%20Lucas%20De%20Melo%20Costa%20and%20Fabrice%20Popineau%20and%20Arpad%20Rimmel%20and%20Bich-Li%C3%AAn%20Doan%0AAbstract%3A%20%20%20Self-supervision%20is%20often%20used%20for%20pre-training%20to%20foster%20performance%20on%20a%0Adownstream%20task%20by%20constructing%20meaningful%20representations%20of%20samples.%0ASelf-supervised%20learning%20%28SSL%29%20generally%20involves%20generating%20different%20views%20of%0Athe%20same%20sample%20and%20thus%20requires%20data%20augmentations%20that%20are%20challenging%20to%0Aconstruct%20for%20tabular%20data.%20This%20constitutes%20one%20of%20the%20main%20challenges%20of%0Aself-supervision%20for%20structured%20data.%20In%20the%20present%20work%2C%20we%20propose%20a%20novel%0Aaugmentation-free%20SSL%20method%20for%20tabular%20data.%20Our%20approach%2C%20T-JEPA%2C%20relies%20on%0Aa%20Joint%20Embedding%20Predictive%20Architecture%20%28JEPA%29%20and%20is%20akin%20to%20mask%0Areconstruction%20in%20the%20latent%20space.%20It%20involves%20predicting%20the%20latent%0Arepresentation%20of%20one%20subset%20of%20features%20from%20the%20latent%20representation%20of%20a%0Adifferent%20subset%20within%20the%20same%20sample%2C%20thereby%20learning%20rich%20representations%0Awithout%20augmentations.%20We%20use%20our%20method%20as%20a%20pre-training%20technique%20and%20train%0Aseveral%20deep%20classifiers%20on%20the%20obtained%20representation.%20Our%20experimental%0Aresults%20demonstrate%20a%20substantial%20improvement%20in%20both%20classification%20and%0Aregression%20tasks%2C%20outperforming%20models%20trained%20directly%20on%20samples%20in%20their%0Aoriginal%20data%20space.%20Moreover%2C%20T-JEPA%20enables%20some%20methods%20to%20consistently%0Aoutperform%20or%20match%20the%20performance%20of%20traditional%20methods%20likes%20Gradient%0ABoosted%20Decision%20Trees.%20To%20understand%20why%2C%20we%20extensively%20characterize%20the%0Aobtained%20representations%20and%20show%20that%20T-JEPA%20effectively%20identifies%20relevant%0Afeatures%20for%20downstream%20tasks%20without%20access%20to%20the%20labels.%20Additionally%2C%20we%0Aintroduce%20regularization%20tokens%2C%20a%20novel%20regularization%20method%20critical%20for%0Atraining%20of%20JEPA-based%20models%20on%20structured%20data.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2410.05016v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DT-JEPA%253A%2520Augmentation-Free%2520Self-Supervised%2520Learning%2520for%2520Tabular%2520Data%26entry.906535625%3DHugo%2520Thimonier%2520and%2520Jos%25C3%25A9%2520Lucas%2520De%2520Melo%2520Costa%2520and%2520Fabrice%2520Popineau%2520and%2520Arpad%2520Rimmel%2520and%2520Bich-Li%25C3%25AAn%2520Doan%26entry.1292438233%3D%2520%2520Self-supervision%2520is%2520often%2520used%2520for%2520pre-training%2520to%2520foster%2520performance%2520on%2520a%250Adownstream%2520task%2520by%2520constructing%2520meaningful%2520representations%2520of%2520samples.%250ASelf-supervised%2520learning%2520%2528SSL%2529%2520generally%2520involves%2520generating%2520different%2520views%2520of%250Athe%2520same%2520sample%2520and%2520thus%2520requires%2520data%2520augmentations%2520that%2520are%2520challenging%2520to%250Aconstruct%2520for%2520tabular%2520data.%2520This%2520constitutes%2520one%2520of%2520the%2520main%2520challenges%2520of%250Aself-supervision%2520for%2520structured%2520data.%2520In%2520the%2520present%2520work%252C%2520we%2520propose%2520a%2520novel%250Aaugmentation-free%2520SSL%2520method%2520for%2520tabular%2520data.%2520Our%2520approach%252C%2520T-JEPA%252C%2520relies%2520on%250Aa%2520Joint%2520Embedding%2520Predictive%2520Architecture%2520%2528JEPA%2529%2520and%2520is%2520akin%2520to%2520mask%250Areconstruction%2520in%2520the%2520latent%2520space.%2520It%2520involves%2520predicting%2520the%2520latent%250Arepresentation%2520of%2520one%2520subset%2520of%2520features%2520from%2520the%2520latent%2520representation%2520of%2520a%250Adifferent%2520subset%2520within%2520the%2520same%2520sample%252C%2520thereby%2520learning%2520rich%2520representations%250Awithout%2520augmentations.%2520We%2520use%2520our%2520method%2520as%2520a%2520pre-training%2520technique%2520and%2520train%250Aseveral%2520deep%2520classifiers%2520on%2520the%2520obtained%2520representation.%2520Our%2520experimental%250Aresults%2520demonstrate%2520a%2520substantial%2520improvement%2520in%2520both%2520classification%2520and%250Aregression%2520tasks%252C%2520outperforming%2520models%2520trained%2520directly%2520on%2520samples%2520in%2520their%250Aoriginal%2520data%2520space.%2520Moreover%252C%2520T-JEPA%2520enables%2520some%2520methods%2520to%2520consistently%250Aoutperform%2520or%2520match%2520the%2520performance%2520of%2520traditional%2520methods%2520likes%2520Gradient%250ABoosted%2520Decision%2520Trees.%2520To%2520understand%2520why%252C%2520we%2520extensively%2520characterize%2520the%250Aobtained%2520representations%2520and%2520show%2520that%2520T-JEPA%2520effectively%2520identifies%2520relevant%250Afeatures%2520for%2520downstream%2520tasks%2520without%2520access%2520to%2520the%2520labels.%2520Additionally%252C%2520we%250Aintroduce%2520regularization%2520tokens%252C%2520a%2520novel%2520regularization%2520method%2520critical%2520for%250Atraining%2520of%2520JEPA-based%2520models%2520on%2520structured%2520data.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2410.05016v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=T-JEPA%3A%20Augmentation-Free%20Self-Supervised%20Learning%20for%20Tabular%20Data&entry.906535625=Hugo%20Thimonier%20and%20Jos%C3%A9%20Lucas%20De%20Melo%20Costa%20and%20Fabrice%20Popineau%20and%20Arpad%20Rimmel%20and%20Bich-Li%C3%AAn%20Doan&entry.1292438233=%20%20Self-supervision%20is%20often%20used%20for%20pre-training%20to%20foster%20performance%20on%20a%0Adownstream%20task%20by%20constructing%20meaningful%20representations%20of%20samples.%0ASelf-supervised%20learning%20%28SSL%29%20generally%20involves%20generating%20different%20views%20of%0Athe%20same%20sample%20and%20thus%20requires%20data%20augmentations%20that%20are%20challenging%20to%0Aconstruct%20for%20tabular%20data.%20This%20constitutes%20one%20of%20the%20main%20challenges%20of%0Aself-supervision%20for%20structured%20data.%20In%20the%20present%20work%2C%20we%20propose%20a%20novel%0Aaugmentation-free%20SSL%20method%20for%20tabular%20data.%20Our%20approach%2C%20T-JEPA%2C%20relies%20on%0Aa%20Joint%20Embedding%20Predictive%20Architecture%20%28JEPA%29%20and%20is%20akin%20to%20mask%0Areconstruction%20in%20the%20latent%20space.%20It%20involves%20predicting%20the%20latent%0Arepresentation%20of%20one%20subset%20of%20features%20from%20the%20latent%20representation%20of%20a%0Adifferent%20subset%20within%20the%20same%20sample%2C%20thereby%20learning%20rich%20representations%0Awithout%20augmentations.%20We%20use%20our%20method%20as%20a%20pre-training%20technique%20and%20train%0Aseveral%20deep%20classifiers%20on%20the%20obtained%20representation.%20Our%20experimental%0Aresults%20demonstrate%20a%20substantial%20improvement%20in%20both%20classification%20and%0Aregression%20tasks%2C%20outperforming%20models%20trained%20directly%20on%20samples%20in%20their%0Aoriginal%20data%20space.%20Moreover%2C%20T-JEPA%20enables%20some%20methods%20to%20consistently%0Aoutperform%20or%20match%20the%20performance%20of%20traditional%20methods%20likes%20Gradient%0ABoosted%20Decision%20Trees.%20To%20understand%20why%2C%20we%20extensively%20characterize%20the%0Aobtained%20representations%20and%20show%20that%20T-JEPA%20effectively%20identifies%20relevant%0Afeatures%20for%20downstream%20tasks%20without%20access%20to%20the%20labels.%20Additionally%2C%20we%0Aintroduce%20regularization%20tokens%2C%20a%20novel%20regularization%20method%20critical%20for%0Atraining%20of%20JEPA-based%20models%20on%20structured%20data.%0A&entry.1838667208=http%3A//arxiv.org/abs/2410.05016v1&entry.124074799=Read"},
{"title": "IRASNet: Improved Feature-Level Clutter Reduction for Domain Generalized\n  SAR-ATR", "author": "Oh-Tae Jang and Hae-Kang Song and Min-Jun Kim and Kyung-Hwan Lee and Geon Lee and Sung-Ho Kim and Hee-Sub Shin and Jae-Woo Ok and Min-Young Back and Jae-Hyuk Yoon and Kyung-Tae Kim", "abstract": "  Recently, computer-aided design models and electromagnetic simulations have\nbeen used to augment synthetic aperture radar (SAR) data for deep learning.\nHowever, an automatic target recognition (ATR) model struggles with domain\nshift when using synthetic data because the model learns specific clutter\npatterns present in such data, which disturbs performance when applied to\nmeasured data with different clutter distributions. This study proposes a\nframework particularly designed for domain-generalized SAR-ATR called IRASNet,\nenabling effective feature-level clutter reduction and domain-invariant feature\nlearning. First, we propose a clutter reduction module (CRM) that maximizes the\nsignal-to-clutter ratio on feature maps. The module reduces the impact of\nclutter at the feature level while preserving target and shadow information,\nthereby improving ATR performance. Second, we integrate adversarial learning\nwith CRM to extract clutter-reduced domain-invariant features. The integration\nbridges the gap between synthetic and measured datasets without requiring\nmeasured data during training. Third, we improve feature extraction from target\nand shadow regions by implementing a positional supervision task using mask\nground truth encoding. The improvement enhances the ability of the model to\ndiscriminate between classes. Our proposed IRASNet presents new\nstate-of-the-art public SAR datasets utilizing target and shadow information to\nachieve superior performance across various test conditions. IRASNet not only\nenhances generalization performance but also significantly improves\nfeature-level clutter reduction, making it a valuable advancement in the field\nof radar image pattern recognition.\n", "link": "http://arxiv.org/abs/2409.16845v2", "date": "2024-10-07", "relevancy": 2.5339, "topK": [{"title": "RTG-SLAM: Real-time 3D Reconstruction at Scale using Gaussian Splatting", "link": "http://arxiv.org/abs/2404.19706v1", "similarity": 0.5314}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.4949}, {"title": "Breadcrumbs: Adversarial class-balanced sampling for long-tailed recognition", "link": "https://link.springer.com/chapter/10.1007/978-3-031-20053-3_37", "similarity": 0.4941}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20IRASNet%3A%20Improved%20Feature-Level%20Clutter%20Reduction%20for%20Domain%20Generalized%0A%20%20SAR-ATR&body=Title%3A%20IRASNet%3A%20Improved%20Feature-Level%20Clutter%20Reduction%20for%20Domain%20Generalized%0A%20%20SAR-ATR%0AAuthor%3A%20Oh-Tae%20Jang%20and%20Hae-Kang%20Song%20and%20Min-Jun%20Kim%20and%20Kyung-Hwan%20Lee%20and%20Geon%20Lee%20and%20Sung-Ho%20Kim%20and%20Hee-Sub%20Shin%20and%20Jae-Woo%20Ok%20and%20Min-Young%20Back%20and%20Jae-Hyuk%20Yoon%20and%20Kyung-Tae%20Kim%0AAbstract%3A%20%20%20Recently%2C%20computer-aided%20design%20models%20and%20electromagnetic%20simulations%20have%0Abeen%20used%20to%20augment%20synthetic%20aperture%20radar%20%28SAR%29%20data%20for%20deep%20learning.%0AHowever%2C%20an%20automatic%20target%20recognition%20%28ATR%29%20model%20struggles%20with%20domain%0Ashift%20when%20using%20synthetic%20data%20because%20the%20model%20learns%20specific%20clutter%0Apatterns%20present%20in%20such%20data%2C%20which%20disturbs%20performance%20when%20applied%20to%0Ameasured%20data%20with%20different%20clutter%20distributions.%20This%20study%20proposes%20a%0Aframework%20particularly%20designed%20for%20domain-generalized%20SAR-ATR%20called%20IRASNet%2C%0Aenabling%20effective%20feature-level%20clutter%20reduction%20and%20domain-invariant%20feature%0Alearning.%20First%2C%20we%20propose%20a%20clutter%20reduction%20module%20%28CRM%29%20that%20maximizes%20the%0Asignal-to-clutter%20ratio%20on%20feature%20maps.%20The%20module%20reduces%20the%20impact%20of%0Aclutter%20at%20the%20feature%20level%20while%20preserving%20target%20and%20shadow%20information%2C%0Athereby%20improving%20ATR%20performance.%20Second%2C%20we%20integrate%20adversarial%20learning%0Awith%20CRM%20to%20extract%20clutter-reduced%20domain-invariant%20features.%20The%20integration%0Abridges%20the%20gap%20between%20synthetic%20and%20measured%20datasets%20without%20requiring%0Ameasured%20data%20during%20training.%20Third%2C%20we%20improve%20feature%20extraction%20from%20target%0Aand%20shadow%20regions%20by%20implementing%20a%20positional%20supervision%20task%20using%20mask%0Aground%20truth%20encoding.%20The%20improvement%20enhances%20the%20ability%20of%20the%20model%20to%0Adiscriminate%20between%20classes.%20Our%20proposed%20IRASNet%20presents%20new%0Astate-of-the-art%20public%20SAR%20datasets%20utilizing%20target%20and%20shadow%20information%20to%0Aachieve%20superior%20performance%20across%20various%20test%20conditions.%20IRASNet%20not%20only%0Aenhances%20generalization%20performance%20but%20also%20significantly%20improves%0Afeature-level%20clutter%20reduction%2C%20making%20it%20a%20valuable%20advancement%20in%20the%20field%0Aof%20radar%20image%20pattern%20recognition.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2409.16845v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DIRASNet%253A%2520Improved%2520Feature-Level%2520Clutter%2520Reduction%2520for%2520Domain%2520Generalized%250A%2520%2520SAR-ATR%26entry.906535625%3DOh-Tae%2520Jang%2520and%2520Hae-Kang%2520Song%2520and%2520Min-Jun%2520Kim%2520and%2520Kyung-Hwan%2520Lee%2520and%2520Geon%2520Lee%2520and%2520Sung-Ho%2520Kim%2520and%2520Hee-Sub%2520Shin%2520and%2520Jae-Woo%2520Ok%2520and%2520Min-Young%2520Back%2520and%2520Jae-Hyuk%2520Yoon%2520and%2520Kyung-Tae%2520Kim%26entry.1292438233%3D%2520%2520Recently%252C%2520computer-aided%2520design%2520models%2520and%2520electromagnetic%2520simulations%2520have%250Abeen%2520used%2520to%2520augment%2520synthetic%2520aperture%2520radar%2520%2528SAR%2529%2520data%2520for%2520deep%2520learning.%250AHowever%252C%2520an%2520automatic%2520target%2520recognition%2520%2528ATR%2529%2520model%2520struggles%2520with%2520domain%250Ashift%2520when%2520using%2520synthetic%2520data%2520because%2520the%2520model%2520learns%2520specific%2520clutter%250Apatterns%2520present%2520in%2520such%2520data%252C%2520which%2520disturbs%2520performance%2520when%2520applied%2520to%250Ameasured%2520data%2520with%2520different%2520clutter%2520distributions.%2520This%2520study%2520proposes%2520a%250Aframework%2520particularly%2520designed%2520for%2520domain-generalized%2520SAR-ATR%2520called%2520IRASNet%252C%250Aenabling%2520effective%2520feature-level%2520clutter%2520reduction%2520and%2520domain-invariant%2520feature%250Alearning.%2520First%252C%2520we%2520propose%2520a%2520clutter%2520reduction%2520module%2520%2528CRM%2529%2520that%2520maximizes%2520the%250Asignal-to-clutter%2520ratio%2520on%2520feature%2520maps.%2520The%2520module%2520reduces%2520the%2520impact%2520of%250Aclutter%2520at%2520the%2520feature%2520level%2520while%2520preserving%2520target%2520and%2520shadow%2520information%252C%250Athereby%2520improving%2520ATR%2520performance.%2520Second%252C%2520we%2520integrate%2520adversarial%2520learning%250Awith%2520CRM%2520to%2520extract%2520clutter-reduced%2520domain-invariant%2520features.%2520The%2520integration%250Abridges%2520the%2520gap%2520between%2520synthetic%2520and%2520measured%2520datasets%2520without%2520requiring%250Ameasured%2520data%2520during%2520training.%2520Third%252C%2520we%2520improve%2520feature%2520extraction%2520from%2520target%250Aand%2520shadow%2520regions%2520by%2520implementing%2520a%2520positional%2520supervision%2520task%2520using%2520mask%250Aground%2520truth%2520encoding.%2520The%2520improvement%2520enhances%2520the%2520ability%2520of%2520the%2520model%2520to%250Adiscriminate%2520between%2520classes.%2520Our%2520proposed%2520IRASNet%2520presents%2520new%250Astate-of-the-art%2520public%2520SAR%2520datasets%2520utilizing%2520target%2520and%2520shadow%2520information%2520to%250Aachieve%2520superior%2520performance%2520across%2520various%2520test%2520conditions.%2520IRASNet%2520not%2520only%250Aenhances%2520generalization%2520performance%2520but%2520also%2520significantly%2520improves%250Afeature-level%2520clutter%2520reduction%252C%2520making%2520it%2520a%2520valuable%2520advancement%2520in%2520the%2520field%250Aof%2520radar%2520image%2520pattern%2520recognition.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2409.16845v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=IRASNet%3A%20Improved%20Feature-Level%20Clutter%20Reduction%20for%20Domain%20Generalized%0A%20%20SAR-ATR&entry.906535625=Oh-Tae%20Jang%20and%20Hae-Kang%20Song%20and%20Min-Jun%20Kim%20and%20Kyung-Hwan%20Lee%20and%20Geon%20Lee%20and%20Sung-Ho%20Kim%20and%20Hee-Sub%20Shin%20and%20Jae-Woo%20Ok%20and%20Min-Young%20Back%20and%20Jae-Hyuk%20Yoon%20and%20Kyung-Tae%20Kim&entry.1292438233=%20%20Recently%2C%20computer-aided%20design%20models%20and%20electromagnetic%20simulations%20have%0Abeen%20used%20to%20augment%20synthetic%20aperture%20radar%20%28SAR%29%20data%20for%20deep%20learning.%0AHowever%2C%20an%20automatic%20target%20recognition%20%28ATR%29%20model%20struggles%20with%20domain%0Ashift%20when%20using%20synthetic%20data%20because%20the%20model%20learns%20specific%20clutter%0Apatterns%20present%20in%20such%20data%2C%20which%20disturbs%20performance%20when%20applied%20to%0Ameasured%20data%20with%20different%20clutter%20distributions.%20This%20study%20proposes%20a%0Aframework%20particularly%20designed%20for%20domain-generalized%20SAR-ATR%20called%20IRASNet%2C%0Aenabling%20effective%20feature-level%20clutter%20reduction%20and%20domain-invariant%20feature%0Alearning.%20First%2C%20we%20propose%20a%20clutter%20reduction%20module%20%28CRM%29%20that%20maximizes%20the%0Asignal-to-clutter%20ratio%20on%20feature%20maps.%20The%20module%20reduces%20the%20impact%20of%0Aclutter%20at%20the%20feature%20level%20while%20preserving%20target%20and%20shadow%20information%2C%0Athereby%20improving%20ATR%20performance.%20Second%2C%20we%20integrate%20adversarial%20learning%0Awith%20CRM%20to%20extract%20clutter-reduced%20domain-invariant%20features.%20The%20integration%0Abridges%20the%20gap%20between%20synthetic%20and%20measured%20datasets%20without%20requiring%0Ameasured%20data%20during%20training.%20Third%2C%20we%20improve%20feature%20extraction%20from%20target%0Aand%20shadow%20regions%20by%20implementing%20a%20positional%20supervision%20task%20using%20mask%0Aground%20truth%20encoding.%20The%20improvement%20enhances%20the%20ability%20of%20the%20model%20to%0Adiscriminate%20between%20classes.%20Our%20proposed%20IRASNet%20presents%20new%0Astate-of-the-art%20public%20SAR%20datasets%20utilizing%20target%20and%20shadow%20information%20to%0Aachieve%20superior%20performance%20across%20various%20test%20conditions.%20IRASNet%20not%20only%0Aenhances%20generalization%20performance%20but%20also%20significantly%20improves%0Afeature-level%20clutter%20reduction%2C%20making%20it%20a%20valuable%20advancement%20in%20the%20field%0Aof%20radar%20image%20pattern%20recognition.%0A&entry.1838667208=http%3A//arxiv.org/abs/2409.16845v2&entry.124074799=Read"},
{"title": "Self-MoE: Towards Compositional Large Language Models with\n  Self-Specialized Experts", "author": "Junmo Kang and Leonid Karlinsky and Hongyin Luo and Zhen Wang and Jacob Hansen and James Glass and David Cox and Rameswar Panda and Rogerio Feris and Alan Ritter", "abstract": "  We present Self-MoE, an approach that transforms a monolithic LLM into a\ncompositional, modular system of self-specialized experts, named MiXSE (MiXture\nof Self-specialized Experts). Our approach leverages self-specialization, which\nconstructs expert modules using self-generated synthetic data, each equipping a\nshared base LLM with distinct domain-specific capabilities, activated via\nself-optimized routing. This allows for dynamic and capability-specific\nhandling of various target tasks, enhancing overall capabilities, without\nextensive human-labeled data and added parameters. Our empirical results reveal\nthat specializing LLMs may exhibit potential trade-offs in performances on\nnon-specialized tasks. On the other hand, our Self-MoE demonstrates substantial\nimprovements (6.5%p on average) over the base LLM across diverse benchmarks\nsuch as knowledge, reasoning, math, and coding. It also consistently\noutperforms other methods, including instance merging and weight merging, while\noffering better flexibility and interpretability by design with semantic\nexperts and routing. Our findings highlight the critical role of modularity,\nthe applicability of Self-MoE to multiple base LLMs, and the potential of\nself-improvement in achieving efficient, scalable, and adaptable systems.\n", "link": "http://arxiv.org/abs/2406.12034v2", "date": "2024-10-07", "relevancy": 2.5302, "topK": [{"title": "Self-supervised Photographic Image Layout Representation Learning", "link": "http://arxiv.org/abs/2403.03740v1", "similarity": 0.5131}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5033}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5016}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Self-MoE%3A%20Towards%20Compositional%20Large%20Language%20Models%20with%0A%20%20Self-Specialized%20Experts&body=Title%3A%20Self-MoE%3A%20Towards%20Compositional%20Large%20Language%20Models%20with%0A%20%20Self-Specialized%20Experts%0AAuthor%3A%20Junmo%20Kang%20and%20Leonid%20Karlinsky%20and%20Hongyin%20Luo%20and%20Zhen%20Wang%20and%20Jacob%20Hansen%20and%20James%20Glass%20and%20David%20Cox%20and%20Rameswar%20Panda%20and%20Rogerio%20Feris%20and%20Alan%20Ritter%0AAbstract%3A%20%20%20We%20present%20Self-MoE%2C%20an%20approach%20that%20transforms%20a%20monolithic%20LLM%20into%20a%0Acompositional%2C%20modular%20system%20of%20self-specialized%20experts%2C%20named%20MiXSE%20%28MiXture%0Aof%20Self-specialized%20Experts%29.%20Our%20approach%20leverages%20self-specialization%2C%20which%0Aconstructs%20expert%20modules%20using%20self-generated%20synthetic%20data%2C%20each%20equipping%20a%0Ashared%20base%20LLM%20with%20distinct%20domain-specific%20capabilities%2C%20activated%20via%0Aself-optimized%20routing.%20This%20allows%20for%20dynamic%20and%20capability-specific%0Ahandling%20of%20various%20target%20tasks%2C%20enhancing%20overall%20capabilities%2C%20without%0Aextensive%20human-labeled%20data%20and%20added%20parameters.%20Our%20empirical%20results%20reveal%0Athat%20specializing%20LLMs%20may%20exhibit%20potential%20trade-offs%20in%20performances%20on%0Anon-specialized%20tasks.%20On%20the%20other%20hand%2C%20our%20Self-MoE%20demonstrates%20substantial%0Aimprovements%20%286.5%25p%20on%20average%29%20over%20the%20base%20LLM%20across%20diverse%20benchmarks%0Asuch%20as%20knowledge%2C%20reasoning%2C%20math%2C%20and%20coding.%20It%20also%20consistently%0Aoutperforms%20other%20methods%2C%20including%20instance%20merging%20and%20weight%20merging%2C%20while%0Aoffering%20better%20flexibility%20and%20interpretability%20by%20design%20with%20semantic%0Aexperts%20and%20routing.%20Our%20findings%20highlight%20the%20critical%20role%20of%20modularity%2C%0Athe%20applicability%20of%20Self-MoE%20to%20multiple%20base%20LLMs%2C%20and%20the%20potential%20of%0Aself-improvement%20in%20achieving%20efficient%2C%20scalable%2C%20and%20adaptable%20systems.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2406.12034v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DSelf-MoE%253A%2520Towards%2520Compositional%2520Large%2520Language%2520Models%2520with%250A%2520%2520Self-Specialized%2520Experts%26entry.906535625%3DJunmo%2520Kang%2520and%2520Leonid%2520Karlinsky%2520and%2520Hongyin%2520Luo%2520and%2520Zhen%2520Wang%2520and%2520Jacob%2520Hansen%2520and%2520James%2520Glass%2520and%2520David%2520Cox%2520and%2520Rameswar%2520Panda%2520and%2520Rogerio%2520Feris%2520and%2520Alan%2520Ritter%26entry.1292438233%3D%2520%2520We%2520present%2520Self-MoE%252C%2520an%2520approach%2520that%2520transforms%2520a%2520monolithic%2520LLM%2520into%2520a%250Acompositional%252C%2520modular%2520system%2520of%2520self-specialized%2520experts%252C%2520named%2520MiXSE%2520%2528MiXture%250Aof%2520Self-specialized%2520Experts%2529.%2520Our%2520approach%2520leverages%2520self-specialization%252C%2520which%250Aconstructs%2520expert%2520modules%2520using%2520self-generated%2520synthetic%2520data%252C%2520each%2520equipping%2520a%250Ashared%2520base%2520LLM%2520with%2520distinct%2520domain-specific%2520capabilities%252C%2520activated%2520via%250Aself-optimized%2520routing.%2520This%2520allows%2520for%2520dynamic%2520and%2520capability-specific%250Ahandling%2520of%2520various%2520target%2520tasks%252C%2520enhancing%2520overall%2520capabilities%252C%2520without%250Aextensive%2520human-labeled%2520data%2520and%2520added%2520parameters.%2520Our%2520empirical%2520results%2520reveal%250Athat%2520specializing%2520LLMs%2520may%2520exhibit%2520potential%2520trade-offs%2520in%2520performances%2520on%250Anon-specialized%2520tasks.%2520On%2520the%2520other%2520hand%252C%2520our%2520Self-MoE%2520demonstrates%2520substantial%250Aimprovements%2520%25286.5%2525p%2520on%2520average%2529%2520over%2520the%2520base%2520LLM%2520across%2520diverse%2520benchmarks%250Asuch%2520as%2520knowledge%252C%2520reasoning%252C%2520math%252C%2520and%2520coding.%2520It%2520also%2520consistently%250Aoutperforms%2520other%2520methods%252C%2520including%2520instance%2520merging%2520and%2520weight%2520merging%252C%2520while%250Aoffering%2520better%2520flexibility%2520and%2520interpretability%2520by%2520design%2520with%2520semantic%250Aexperts%2520and%2520routing.%2520Our%2520findings%2520highlight%2520the%2520critical%2520role%2520of%2520modularity%252C%250Athe%2520applicability%2520of%2520Self-MoE%2520to%2520multiple%2520base%2520LLMs%252C%2520and%2520the%2520potential%2520of%250Aself-improvement%2520in%2520achieving%2520efficient%252C%2520scalable%252C%2520and%2520adaptable%2520systems.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2406.12034v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Self-MoE%3A%20Towards%20Compositional%20Large%20Language%20Models%20with%0A%20%20Self-Specialized%20Experts&entry.906535625=Junmo%20Kang%20and%20Leonid%20Karlinsky%20and%20Hongyin%20Luo%20and%20Zhen%20Wang%20and%20Jacob%20Hansen%20and%20James%20Glass%20and%20David%20Cox%20and%20Rameswar%20Panda%20and%20Rogerio%20Feris%20and%20Alan%20Ritter&entry.1292438233=%20%20We%20present%20Self-MoE%2C%20an%20approach%20that%20transforms%20a%20monolithic%20LLM%20into%20a%0Acompositional%2C%20modular%20system%20of%20self-specialized%20experts%2C%20named%20MiXSE%20%28MiXture%0Aof%20Self-specialized%20Experts%29.%20Our%20approach%20leverages%20self-specialization%2C%20which%0Aconstructs%20expert%20modules%20using%20self-generated%20synthetic%20data%2C%20each%20equipping%20a%0Ashared%20base%20LLM%20with%20distinct%20domain-specific%20capabilities%2C%20activated%20via%0Aself-optimized%20routing.%20This%20allows%20for%20dynamic%20and%20capability-specific%0Ahandling%20of%20various%20target%20tasks%2C%20enhancing%20overall%20capabilities%2C%20without%0Aextensive%20human-labeled%20data%20and%20added%20parameters.%20Our%20empirical%20results%20reveal%0Athat%20specializing%20LLMs%20may%20exhibit%20potential%20trade-offs%20in%20performances%20on%0Anon-specialized%20tasks.%20On%20the%20other%20hand%2C%20our%20Self-MoE%20demonstrates%20substantial%0Aimprovements%20%286.5%25p%20on%20average%29%20over%20the%20base%20LLM%20across%20diverse%20benchmarks%0Asuch%20as%20knowledge%2C%20reasoning%2C%20math%2C%20and%20coding.%20It%20also%20consistently%0Aoutperforms%20other%20methods%2C%20including%20instance%20merging%20and%20weight%20merging%2C%20while%0Aoffering%20better%20flexibility%20and%20interpretability%20by%20design%20with%20semantic%0Aexperts%20and%20routing.%20Our%20findings%20highlight%20the%20critical%20role%20of%20modularity%2C%0Athe%20applicability%20of%20Self-MoE%20to%20multiple%20base%20LLMs%2C%20and%20the%20potential%20of%0Aself-improvement%20in%20achieving%20efficient%2C%20scalable%2C%20and%20adaptable%20systems.%0A&entry.1838667208=http%3A//arxiv.org/abs/2406.12034v2&entry.124074799=Read"},
{"title": "Contextual Document Embeddings", "author": "John X. Morris and Alexander M. Rush", "abstract": "  Dense document embeddings are central to neural retrieval. The dominant\nparadigm is to train and construct embeddings by running encoders directly on\nindividual documents. In this work, we argue that these embeddings, while\neffective, are implicitly out-of-context for targeted use cases of retrieval,\nand that a contextualized document embedding should take into account both the\ndocument and neighboring documents in context - analogous to contextualized\nword embeddings. We propose two complementary methods for contextualized\ndocument embeddings: first, an alternative contrastive learning objective that\nexplicitly incorporates the document neighbors into the intra-batch contextual\nloss; second, a new contextual architecture that explicitly encodes neighbor\ndocument information into the encoded representation. Results show that both\nmethods achieve better performance than biencoders in several settings, with\ndifferences especially pronounced out-of-domain. We achieve state-of-the-art\nresults on the MTEB benchmark with no hard negative mining, score distillation,\ndataset-specific instructions, intra-GPU example-sharing, or extremely large\nbatch sizes. Our method can be applied to improve performance on any\ncontrastive learning dataset and any biencoder.\n", "link": "http://arxiv.org/abs/2410.02525v2", "date": "2024-10-07", "relevancy": 2.5282, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5211}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5211}, {"title": "Boosted dynamic neural networks", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/26302", "similarity": 0.4746}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Contextual%20Document%20Embeddings&body=Title%3A%20Contextual%20Document%20Embeddings%0AAuthor%3A%20John%20X.%20Morris%20and%20Alexander%20M.%20Rush%0AAbstract%3A%20%20%20Dense%20document%20embeddings%20are%20central%20to%20neural%20retrieval.%20The%20dominant%0Aparadigm%20is%20to%20train%20and%20construct%20embeddings%20by%20running%20encoders%20directly%20on%0Aindividual%20documents.%20In%20this%20work%2C%20we%20argue%20that%20these%20embeddings%2C%20while%0Aeffective%2C%20are%20implicitly%20out-of-context%20for%20targeted%20use%20cases%20of%20retrieval%2C%0Aand%20that%20a%20contextualized%20document%20embedding%20should%20take%20into%20account%20both%20the%0Adocument%20and%20neighboring%20documents%20in%20context%20-%20analogous%20to%20contextualized%0Aword%20embeddings.%20We%20propose%20two%20complementary%20methods%20for%20contextualized%0Adocument%20embeddings%3A%20first%2C%20an%20alternative%20contrastive%20learning%20objective%20that%0Aexplicitly%20incorporates%20the%20document%20neighbors%20into%20the%20intra-batch%20contextual%0Aloss%3B%20second%2C%20a%20new%20contextual%20architecture%20that%20explicitly%20encodes%20neighbor%0Adocument%20information%20into%20the%20encoded%20representation.%20Results%20show%20that%20both%0Amethods%20achieve%20better%20performance%20than%20biencoders%20in%20several%20settings%2C%20with%0Adifferences%20especially%20pronounced%20out-of-domain.%20We%20achieve%20state-of-the-art%0Aresults%20on%20the%20MTEB%20benchmark%20with%20no%20hard%20negative%20mining%2C%20score%20distillation%2C%0Adataset-specific%20instructions%2C%20intra-GPU%20example-sharing%2C%20or%20extremely%20large%0Abatch%20sizes.%20Our%20method%20can%20be%20applied%20to%20improve%20performance%20on%20any%0Acontrastive%20learning%20dataset%20and%20any%20biencoder.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2410.02525v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DContextual%2520Document%2520Embeddings%26entry.906535625%3DJohn%2520X.%2520Morris%2520and%2520Alexander%2520M.%2520Rush%26entry.1292438233%3D%2520%2520Dense%2520document%2520embeddings%2520are%2520central%2520to%2520neural%2520retrieval.%2520The%2520dominant%250Aparadigm%2520is%2520to%2520train%2520and%2520construct%2520embeddings%2520by%2520running%2520encoders%2520directly%2520on%250Aindividual%2520documents.%2520In%2520this%2520work%252C%2520we%2520argue%2520that%2520these%2520embeddings%252C%2520while%250Aeffective%252C%2520are%2520implicitly%2520out-of-context%2520for%2520targeted%2520use%2520cases%2520of%2520retrieval%252C%250Aand%2520that%2520a%2520contextualized%2520document%2520embedding%2520should%2520take%2520into%2520account%2520both%2520the%250Adocument%2520and%2520neighboring%2520documents%2520in%2520context%2520-%2520analogous%2520to%2520contextualized%250Aword%2520embeddings.%2520We%2520propose%2520two%2520complementary%2520methods%2520for%2520contextualized%250Adocument%2520embeddings%253A%2520first%252C%2520an%2520alternative%2520contrastive%2520learning%2520objective%2520that%250Aexplicitly%2520incorporates%2520the%2520document%2520neighbors%2520into%2520the%2520intra-batch%2520contextual%250Aloss%253B%2520second%252C%2520a%2520new%2520contextual%2520architecture%2520that%2520explicitly%2520encodes%2520neighbor%250Adocument%2520information%2520into%2520the%2520encoded%2520representation.%2520Results%2520show%2520that%2520both%250Amethods%2520achieve%2520better%2520performance%2520than%2520biencoders%2520in%2520several%2520settings%252C%2520with%250Adifferences%2520especially%2520pronounced%2520out-of-domain.%2520We%2520achieve%2520state-of-the-art%250Aresults%2520on%2520the%2520MTEB%2520benchmark%2520with%2520no%2520hard%2520negative%2520mining%252C%2520score%2520distillation%252C%250Adataset-specific%2520instructions%252C%2520intra-GPU%2520example-sharing%252C%2520or%2520extremely%2520large%250Abatch%2520sizes.%2520Our%2520method%2520can%2520be%2520applied%2520to%2520improve%2520performance%2520on%2520any%250Acontrastive%2520learning%2520dataset%2520and%2520any%2520biencoder.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2410.02525v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Contextual%20Document%20Embeddings&entry.906535625=John%20X.%20Morris%20and%20Alexander%20M.%20Rush&entry.1292438233=%20%20Dense%20document%20embeddings%20are%20central%20to%20neural%20retrieval.%20The%20dominant%0Aparadigm%20is%20to%20train%20and%20construct%20embeddings%20by%20running%20encoders%20directly%20on%0Aindividual%20documents.%20In%20this%20work%2C%20we%20argue%20that%20these%20embeddings%2C%20while%0Aeffective%2C%20are%20implicitly%20out-of-context%20for%20targeted%20use%20cases%20of%20retrieval%2C%0Aand%20that%20a%20contextualized%20document%20embedding%20should%20take%20into%20account%20both%20the%0Adocument%20and%20neighboring%20documents%20in%20context%20-%20analogous%20to%20contextualized%0Aword%20embeddings.%20We%20propose%20two%20complementary%20methods%20for%20contextualized%0Adocument%20embeddings%3A%20first%2C%20an%20alternative%20contrastive%20learning%20objective%20that%0Aexplicitly%20incorporates%20the%20document%20neighbors%20into%20the%20intra-batch%20contextual%0Aloss%3B%20second%2C%20a%20new%20contextual%20architecture%20that%20explicitly%20encodes%20neighbor%0Adocument%20information%20into%20the%20encoded%20representation.%20Results%20show%20that%20both%0Amethods%20achieve%20better%20performance%20than%20biencoders%20in%20several%20settings%2C%20with%0Adifferences%20especially%20pronounced%20out-of-domain.%20We%20achieve%20state-of-the-art%0Aresults%20on%20the%20MTEB%20benchmark%20with%20no%20hard%20negative%20mining%2C%20score%20distillation%2C%0Adataset-specific%20instructions%2C%20intra-GPU%20example-sharing%2C%20or%20extremely%20large%0Abatch%20sizes.%20Our%20method%20can%20be%20applied%20to%20improve%20performance%20on%20any%0Acontrastive%20learning%20dataset%20and%20any%20biencoder.%0A&entry.1838667208=http%3A//arxiv.org/abs/2410.02525v2&entry.124074799=Read"},
{"title": "The Dawn of Video Generation: Preliminary Explorations with SORA-like\n  Models", "author": "Ailing Zeng and Yuhang Yang and Weidong Chen and Wei Liu", "abstract": "  High-quality video generation, encompassing text-to-video (T2V),\nimage-to-video (I2V), and video-to-video (V2V) generation, holds considerable\nsignificance in content creation to benefit anyone express their inherent\ncreativity in new ways and world simulation to modeling and understanding the\nworld. Models like SORA have advanced generating videos with higher resolution,\nmore natural motion, better vision-language alignment, and increased\ncontrollability, particularly for long video sequences. These improvements have\nbeen driven by the evolution of model architectures, shifting from UNet to more\nscalable and parameter-rich DiT models, along with large-scale data expansion\nand refined training strategies. However, despite the emergence of DiT-based\nclosed-source and open-source models, a comprehensive investigation into their\ncapabilities and limitations remains lacking. Furthermore, the rapid\ndevelopment has made it challenging for recent benchmarks to fully cover\nSORA-like models and recognize their significant advancements. Additionally,\nevaluation metrics often fail to align with human preferences.\n", "link": "http://arxiv.org/abs/2410.05227v1", "date": "2024-10-07", "relevancy": 2.5215, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.6333}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.6333}, {"title": "Customize-A-Video: One-Shot Motion Customization of Text-to-Video\n  Diffusion Models", "link": "http://arxiv.org/abs/2402.14780v1", "similarity": 0.616}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20The%20Dawn%20of%20Video%20Generation%3A%20Preliminary%20Explorations%20with%20SORA-like%0A%20%20Models&body=Title%3A%20The%20Dawn%20of%20Video%20Generation%3A%20Preliminary%20Explorations%20with%20SORA-like%0A%20%20Models%0AAuthor%3A%20Ailing%20Zeng%20and%20Yuhang%20Yang%20and%20Weidong%20Chen%20and%20Wei%20Liu%0AAbstract%3A%20%20%20High-quality%20video%20generation%2C%20encompassing%20text-to-video%20%28T2V%29%2C%0Aimage-to-video%20%28I2V%29%2C%20and%20video-to-video%20%28V2V%29%20generation%2C%20holds%20considerable%0Asignificance%20in%20content%20creation%20to%20benefit%20anyone%20express%20their%20inherent%0Acreativity%20in%20new%20ways%20and%20world%20simulation%20to%20modeling%20and%20understanding%20the%0Aworld.%20Models%20like%20SORA%20have%20advanced%20generating%20videos%20with%20higher%20resolution%2C%0Amore%20natural%20motion%2C%20better%20vision-language%20alignment%2C%20and%20increased%0Acontrollability%2C%20particularly%20for%20long%20video%20sequences.%20These%20improvements%20have%0Abeen%20driven%20by%20the%20evolution%20of%20model%20architectures%2C%20shifting%20from%20UNet%20to%20more%0Ascalable%20and%20parameter-rich%20DiT%20models%2C%20along%20with%20large-scale%20data%20expansion%0Aand%20refined%20training%20strategies.%20However%2C%20despite%20the%20emergence%20of%20DiT-based%0Aclosed-source%20and%20open-source%20models%2C%20a%20comprehensive%20investigation%20into%20their%0Acapabilities%20and%20limitations%20remains%20lacking.%20Furthermore%2C%20the%20rapid%0Adevelopment%20has%20made%20it%20challenging%20for%20recent%20benchmarks%20to%20fully%20cover%0ASORA-like%20models%20and%20recognize%20their%20significant%20advancements.%20Additionally%2C%0Aevaluation%20metrics%20often%20fail%20to%20align%20with%20human%20preferences.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2410.05227v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DThe%2520Dawn%2520of%2520Video%2520Generation%253A%2520Preliminary%2520Explorations%2520with%2520SORA-like%250A%2520%2520Models%26entry.906535625%3DAiling%2520Zeng%2520and%2520Yuhang%2520Yang%2520and%2520Weidong%2520Chen%2520and%2520Wei%2520Liu%26entry.1292438233%3D%2520%2520High-quality%2520video%2520generation%252C%2520encompassing%2520text-to-video%2520%2528T2V%2529%252C%250Aimage-to-video%2520%2528I2V%2529%252C%2520and%2520video-to-video%2520%2528V2V%2529%2520generation%252C%2520holds%2520considerable%250Asignificance%2520in%2520content%2520creation%2520to%2520benefit%2520anyone%2520express%2520their%2520inherent%250Acreativity%2520in%2520new%2520ways%2520and%2520world%2520simulation%2520to%2520modeling%2520and%2520understanding%2520the%250Aworld.%2520Models%2520like%2520SORA%2520have%2520advanced%2520generating%2520videos%2520with%2520higher%2520resolution%252C%250Amore%2520natural%2520motion%252C%2520better%2520vision-language%2520alignment%252C%2520and%2520increased%250Acontrollability%252C%2520particularly%2520for%2520long%2520video%2520sequences.%2520These%2520improvements%2520have%250Abeen%2520driven%2520by%2520the%2520evolution%2520of%2520model%2520architectures%252C%2520shifting%2520from%2520UNet%2520to%2520more%250Ascalable%2520and%2520parameter-rich%2520DiT%2520models%252C%2520along%2520with%2520large-scale%2520data%2520expansion%250Aand%2520refined%2520training%2520strategies.%2520However%252C%2520despite%2520the%2520emergence%2520of%2520DiT-based%250Aclosed-source%2520and%2520open-source%2520models%252C%2520a%2520comprehensive%2520investigation%2520into%2520their%250Acapabilities%2520and%2520limitations%2520remains%2520lacking.%2520Furthermore%252C%2520the%2520rapid%250Adevelopment%2520has%2520made%2520it%2520challenging%2520for%2520recent%2520benchmarks%2520to%2520fully%2520cover%250ASORA-like%2520models%2520and%2520recognize%2520their%2520significant%2520advancements.%2520Additionally%252C%250Aevaluation%2520metrics%2520often%2520fail%2520to%2520align%2520with%2520human%2520preferences.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2410.05227v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=The%20Dawn%20of%20Video%20Generation%3A%20Preliminary%20Explorations%20with%20SORA-like%0A%20%20Models&entry.906535625=Ailing%20Zeng%20and%20Yuhang%20Yang%20and%20Weidong%20Chen%20and%20Wei%20Liu&entry.1292438233=%20%20High-quality%20video%20generation%2C%20encompassing%20text-to-video%20%28T2V%29%2C%0Aimage-to-video%20%28I2V%29%2C%20and%20video-to-video%20%28V2V%29%20generation%2C%20holds%20considerable%0Asignificance%20in%20content%20creation%20to%20benefit%20anyone%20express%20their%20inherent%0Acreativity%20in%20new%20ways%20and%20world%20simulation%20to%20modeling%20and%20understanding%20the%0Aworld.%20Models%20like%20SORA%20have%20advanced%20generating%20videos%20with%20higher%20resolution%2C%0Amore%20natural%20motion%2C%20better%20vision-language%20alignment%2C%20and%20increased%0Acontrollability%2C%20particularly%20for%20long%20video%20sequences.%20These%20improvements%20have%0Abeen%20driven%20by%20the%20evolution%20of%20model%20architectures%2C%20shifting%20from%20UNet%20to%20more%0Ascalable%20and%20parameter-rich%20DiT%20models%2C%20along%20with%20large-scale%20data%20expansion%0Aand%20refined%20training%20strategies.%20However%2C%20despite%20the%20emergence%20of%20DiT-based%0Aclosed-source%20and%20open-source%20models%2C%20a%20comprehensive%20investigation%20into%20their%0Acapabilities%20and%20limitations%20remains%20lacking.%20Furthermore%2C%20the%20rapid%0Adevelopment%20has%20made%20it%20challenging%20for%20recent%20benchmarks%20to%20fully%20cover%0ASORA-like%20models%20and%20recognize%20their%20significant%20advancements.%20Additionally%2C%0Aevaluation%20metrics%20often%20fail%20to%20align%20with%20human%20preferences.%0A&entry.1838667208=http%3A//arxiv.org/abs/2410.05227v1&entry.124074799=Read"},
{"title": "xLSTM-FER: Enhancing Student Expression Recognition with Extended Vision\n  Long Short-Term Memory Network", "author": "Qionghao Huang and Jili Chen", "abstract": "  Student expression recognition has become an essential tool for assessing\nlearning experiences and emotional states. This paper introduces xLSTM-FER, a\nnovel architecture derived from the Extended Long Short-Term Memory (xLSTM),\ndesigned to enhance the accuracy and efficiency of expression recognition\nthrough advanced sequence processing capabilities for student facial expression\nrecognition. xLSTM-FER processes input images by segmenting them into a series\nof patches and leveraging a stack of xLSTM blocks to handle these patches.\nxLSTM-FER can capture subtle changes in real-world students' facial expressions\nand improve recognition accuracy by learning spatial-temporal relationships\nwithin the sequence. Experiments on CK+, RAF-DF, and FERplus demonstrate the\npotential of xLSTM-FER in expression recognition tasks, showing better\nperformance compared to state-of-the-art methods on standard datasets. The\nlinear computational and memory complexity of xLSTM-FER make it particularly\nsuitable for handling high-resolution images. Moreover, the design of xLSTM-FER\nallows for efficient processing of non-sequential inputs such as images without\nadditional computation.\n", "link": "http://arxiv.org/abs/2410.05074v1", "date": "2024-10-07", "relevancy": 2.5199, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5048}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5048}, {"title": "Flexible Visual Recognition by Evidential Modeling of Confusion and Ignorance", "link": "http://openaccess.thecvf.com/content/ICCV2023/html/Fan_Flexible_Visual_Recognition_by_Evidential_Modeling_of_Confusion_and_Ignorance_ICCV_2023_paper.html", "similarity": 0.5024}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20xLSTM-FER%3A%20Enhancing%20Student%20Expression%20Recognition%20with%20Extended%20Vision%0A%20%20Long%20Short-Term%20Memory%20Network&body=Title%3A%20xLSTM-FER%3A%20Enhancing%20Student%20Expression%20Recognition%20with%20Extended%20Vision%0A%20%20Long%20Short-Term%20Memory%20Network%0AAuthor%3A%20Qionghao%20Huang%20and%20Jili%20Chen%0AAbstract%3A%20%20%20Student%20expression%20recognition%20has%20become%20an%20essential%20tool%20for%20assessing%0Alearning%20experiences%20and%20emotional%20states.%20This%20paper%20introduces%20xLSTM-FER%2C%20a%0Anovel%20architecture%20derived%20from%20the%20Extended%20Long%20Short-Term%20Memory%20%28xLSTM%29%2C%0Adesigned%20to%20enhance%20the%20accuracy%20and%20efficiency%20of%20expression%20recognition%0Athrough%20advanced%20sequence%20processing%20capabilities%20for%20student%20facial%20expression%0Arecognition.%20xLSTM-FER%20processes%20input%20images%20by%20segmenting%20them%20into%20a%20series%0Aof%20patches%20and%20leveraging%20a%20stack%20of%20xLSTM%20blocks%20to%20handle%20these%20patches.%0AxLSTM-FER%20can%20capture%20subtle%20changes%20in%20real-world%20students%27%20facial%20expressions%0Aand%20improve%20recognition%20accuracy%20by%20learning%20spatial-temporal%20relationships%0Awithin%20the%20sequence.%20Experiments%20on%20CK%2B%2C%20RAF-DF%2C%20and%20FERplus%20demonstrate%20the%0Apotential%20of%20xLSTM-FER%20in%20expression%20recognition%20tasks%2C%20showing%20better%0Aperformance%20compared%20to%20state-of-the-art%20methods%20on%20standard%20datasets.%20The%0Alinear%20computational%20and%20memory%20complexity%20of%20xLSTM-FER%20make%20it%20particularly%0Asuitable%20for%20handling%20high-resolution%20images.%20Moreover%2C%20the%20design%20of%20xLSTM-FER%0Aallows%20for%20efficient%20processing%20of%20non-sequential%20inputs%20such%20as%20images%20without%0Aadditional%20computation.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2410.05074v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DxLSTM-FER%253A%2520Enhancing%2520Student%2520Expression%2520Recognition%2520with%2520Extended%2520Vision%250A%2520%2520Long%2520Short-Term%2520Memory%2520Network%26entry.906535625%3DQionghao%2520Huang%2520and%2520Jili%2520Chen%26entry.1292438233%3D%2520%2520Student%2520expression%2520recognition%2520has%2520become%2520an%2520essential%2520tool%2520for%2520assessing%250Alearning%2520experiences%2520and%2520emotional%2520states.%2520This%2520paper%2520introduces%2520xLSTM-FER%252C%2520a%250Anovel%2520architecture%2520derived%2520from%2520the%2520Extended%2520Long%2520Short-Term%2520Memory%2520%2528xLSTM%2529%252C%250Adesigned%2520to%2520enhance%2520the%2520accuracy%2520and%2520efficiency%2520of%2520expression%2520recognition%250Athrough%2520advanced%2520sequence%2520processing%2520capabilities%2520for%2520student%2520facial%2520expression%250Arecognition.%2520xLSTM-FER%2520processes%2520input%2520images%2520by%2520segmenting%2520them%2520into%2520a%2520series%250Aof%2520patches%2520and%2520leveraging%2520a%2520stack%2520of%2520xLSTM%2520blocks%2520to%2520handle%2520these%2520patches.%250AxLSTM-FER%2520can%2520capture%2520subtle%2520changes%2520in%2520real-world%2520students%2527%2520facial%2520expressions%250Aand%2520improve%2520recognition%2520accuracy%2520by%2520learning%2520spatial-temporal%2520relationships%250Awithin%2520the%2520sequence.%2520Experiments%2520on%2520CK%252B%252C%2520RAF-DF%252C%2520and%2520FERplus%2520demonstrate%2520the%250Apotential%2520of%2520xLSTM-FER%2520in%2520expression%2520recognition%2520tasks%252C%2520showing%2520better%250Aperformance%2520compared%2520to%2520state-of-the-art%2520methods%2520on%2520standard%2520datasets.%2520The%250Alinear%2520computational%2520and%2520memory%2520complexity%2520of%2520xLSTM-FER%2520make%2520it%2520particularly%250Asuitable%2520for%2520handling%2520high-resolution%2520images.%2520Moreover%252C%2520the%2520design%2520of%2520xLSTM-FER%250Aallows%2520for%2520efficient%2520processing%2520of%2520non-sequential%2520inputs%2520such%2520as%2520images%2520without%250Aadditional%2520computation.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2410.05074v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=xLSTM-FER%3A%20Enhancing%20Student%20Expression%20Recognition%20with%20Extended%20Vision%0A%20%20Long%20Short-Term%20Memory%20Network&entry.906535625=Qionghao%20Huang%20and%20Jili%20Chen&entry.1292438233=%20%20Student%20expression%20recognition%20has%20become%20an%20essential%20tool%20for%20assessing%0Alearning%20experiences%20and%20emotional%20states.%20This%20paper%20introduces%20xLSTM-FER%2C%20a%0Anovel%20architecture%20derived%20from%20the%20Extended%20Long%20Short-Term%20Memory%20%28xLSTM%29%2C%0Adesigned%20to%20enhance%20the%20accuracy%20and%20efficiency%20of%20expression%20recognition%0Athrough%20advanced%20sequence%20processing%20capabilities%20for%20student%20facial%20expression%0Arecognition.%20xLSTM-FER%20processes%20input%20images%20by%20segmenting%20them%20into%20a%20series%0Aof%20patches%20and%20leveraging%20a%20stack%20of%20xLSTM%20blocks%20to%20handle%20these%20patches.%0AxLSTM-FER%20can%20capture%20subtle%20changes%20in%20real-world%20students%27%20facial%20expressions%0Aand%20improve%20recognition%20accuracy%20by%20learning%20spatial-temporal%20relationships%0Awithin%20the%20sequence.%20Experiments%20on%20CK%2B%2C%20RAF-DF%2C%20and%20FERplus%20demonstrate%20the%0Apotential%20of%20xLSTM-FER%20in%20expression%20recognition%20tasks%2C%20showing%20better%0Aperformance%20compared%20to%20state-of-the-art%20methods%20on%20standard%20datasets.%20The%0Alinear%20computational%20and%20memory%20complexity%20of%20xLSTM-FER%20make%20it%20particularly%0Asuitable%20for%20handling%20high-resolution%20images.%20Moreover%2C%20the%20design%20of%20xLSTM-FER%0Aallows%20for%20efficient%20processing%20of%20non-sequential%20inputs%20such%20as%20images%20without%0Aadditional%20computation.%0A&entry.1838667208=http%3A//arxiv.org/abs/2410.05074v1&entry.124074799=Read"},
{"title": "Tokenization Is More Than Compression", "author": "Craig W. Schmidt and Varshini Reddy and Haoran Zhang and Alec Alameddine and Omri Uzan and Yuval Pinter and Chris Tanner", "abstract": "  Tokenization is a foundational step in natural language processing (NLP)\ntasks, bridging raw text and language models. Existing tokenization approaches\nlike Byte-Pair Encoding (BPE) originate from the field of data compression, and\nit has been suggested that the effectiveness of BPE stems from its ability to\ncondense text into a relatively small number of tokens. We test the hypothesis\nthat fewer tokens lead to better downstream performance by introducing\nPathPiece, a new tokenizer that segments a document's text into the minimum\nnumber of tokens for a given vocabulary. Through extensive experimentation we\nfind this hypothesis not to be the case, casting doubt on the understanding of\nthe reasons for effective tokenization. To examine which other factors play a\nrole, we evaluate design decisions across all three phases of tokenization:\npre-tokenization, vocabulary construction, and segmentation, offering new\ninsights into the design of effective tokenizers. Specifically, we illustrate\nthe importance of pre-tokenization and the benefits of using BPE to initialize\nvocabulary construction. We train 64 language models with varying tokenization,\nranging in size from 350M to 2.4B parameters, all of which are made publicly\navailable.\n", "link": "http://arxiv.org/abs/2402.18376v2", "date": "2024-10-07", "relevancy": 2.5163, "topK": [{"title": "ToDo: Token Downsampling for Efficient Generation of High-Resolution\n  Images", "link": "http://arxiv.org/abs/2402.13573v2", "similarity": 0.506}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5019}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5019}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Tokenization%20Is%20More%20Than%20Compression&body=Title%3A%20Tokenization%20Is%20More%20Than%20Compression%0AAuthor%3A%20Craig%20W.%20Schmidt%20and%20Varshini%20Reddy%20and%20Haoran%20Zhang%20and%20Alec%20Alameddine%20and%20Omri%20Uzan%20and%20Yuval%20Pinter%20and%20Chris%20Tanner%0AAbstract%3A%20%20%20Tokenization%20is%20a%20foundational%20step%20in%20natural%20language%20processing%20%28NLP%29%0Atasks%2C%20bridging%20raw%20text%20and%20language%20models.%20Existing%20tokenization%20approaches%0Alike%20Byte-Pair%20Encoding%20%28BPE%29%20originate%20from%20the%20field%20of%20data%20compression%2C%20and%0Ait%20has%20been%20suggested%20that%20the%20effectiveness%20of%20BPE%20stems%20from%20its%20ability%20to%0Acondense%20text%20into%20a%20relatively%20small%20number%20of%20tokens.%20We%20test%20the%20hypothesis%0Athat%20fewer%20tokens%20lead%20to%20better%20downstream%20performance%20by%20introducing%0APathPiece%2C%20a%20new%20tokenizer%20that%20segments%20a%20document%27s%20text%20into%20the%20minimum%0Anumber%20of%20tokens%20for%20a%20given%20vocabulary.%20Through%20extensive%20experimentation%20we%0Afind%20this%20hypothesis%20not%20to%20be%20the%20case%2C%20casting%20doubt%20on%20the%20understanding%20of%0Athe%20reasons%20for%20effective%20tokenization.%20To%20examine%20which%20other%20factors%20play%20a%0Arole%2C%20we%20evaluate%20design%20decisions%20across%20all%20three%20phases%20of%20tokenization%3A%0Apre-tokenization%2C%20vocabulary%20construction%2C%20and%20segmentation%2C%20offering%20new%0Ainsights%20into%20the%20design%20of%20effective%20tokenizers.%20Specifically%2C%20we%20illustrate%0Athe%20importance%20of%20pre-tokenization%20and%20the%20benefits%20of%20using%20BPE%20to%20initialize%0Avocabulary%20construction.%20We%20train%2064%20language%20models%20with%20varying%20tokenization%2C%0Aranging%20in%20size%20from%20350M%20to%202.4B%20parameters%2C%20all%20of%20which%20are%20made%20publicly%0Aavailable.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2402.18376v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DTokenization%2520Is%2520More%2520Than%2520Compression%26entry.906535625%3DCraig%2520W.%2520Schmidt%2520and%2520Varshini%2520Reddy%2520and%2520Haoran%2520Zhang%2520and%2520Alec%2520Alameddine%2520and%2520Omri%2520Uzan%2520and%2520Yuval%2520Pinter%2520and%2520Chris%2520Tanner%26entry.1292438233%3D%2520%2520Tokenization%2520is%2520a%2520foundational%2520step%2520in%2520natural%2520language%2520processing%2520%2528NLP%2529%250Atasks%252C%2520bridging%2520raw%2520text%2520and%2520language%2520models.%2520Existing%2520tokenization%2520approaches%250Alike%2520Byte-Pair%2520Encoding%2520%2528BPE%2529%2520originate%2520from%2520the%2520field%2520of%2520data%2520compression%252C%2520and%250Ait%2520has%2520been%2520suggested%2520that%2520the%2520effectiveness%2520of%2520BPE%2520stems%2520from%2520its%2520ability%2520to%250Acondense%2520text%2520into%2520a%2520relatively%2520small%2520number%2520of%2520tokens.%2520We%2520test%2520the%2520hypothesis%250Athat%2520fewer%2520tokens%2520lead%2520to%2520better%2520downstream%2520performance%2520by%2520introducing%250APathPiece%252C%2520a%2520new%2520tokenizer%2520that%2520segments%2520a%2520document%2527s%2520text%2520into%2520the%2520minimum%250Anumber%2520of%2520tokens%2520for%2520a%2520given%2520vocabulary.%2520Through%2520extensive%2520experimentation%2520we%250Afind%2520this%2520hypothesis%2520not%2520to%2520be%2520the%2520case%252C%2520casting%2520doubt%2520on%2520the%2520understanding%2520of%250Athe%2520reasons%2520for%2520effective%2520tokenization.%2520To%2520examine%2520which%2520other%2520factors%2520play%2520a%250Arole%252C%2520we%2520evaluate%2520design%2520decisions%2520across%2520all%2520three%2520phases%2520of%2520tokenization%253A%250Apre-tokenization%252C%2520vocabulary%2520construction%252C%2520and%2520segmentation%252C%2520offering%2520new%250Ainsights%2520into%2520the%2520design%2520of%2520effective%2520tokenizers.%2520Specifically%252C%2520we%2520illustrate%250Athe%2520importance%2520of%2520pre-tokenization%2520and%2520the%2520benefits%2520of%2520using%2520BPE%2520to%2520initialize%250Avocabulary%2520construction.%2520We%2520train%252064%2520language%2520models%2520with%2520varying%2520tokenization%252C%250Aranging%2520in%2520size%2520from%2520350M%2520to%25202.4B%2520parameters%252C%2520all%2520of%2520which%2520are%2520made%2520publicly%250Aavailable.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2402.18376v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Tokenization%20Is%20More%20Than%20Compression&entry.906535625=Craig%20W.%20Schmidt%20and%20Varshini%20Reddy%20and%20Haoran%20Zhang%20and%20Alec%20Alameddine%20and%20Omri%20Uzan%20and%20Yuval%20Pinter%20and%20Chris%20Tanner&entry.1292438233=%20%20Tokenization%20is%20a%20foundational%20step%20in%20natural%20language%20processing%20%28NLP%29%0Atasks%2C%20bridging%20raw%20text%20and%20language%20models.%20Existing%20tokenization%20approaches%0Alike%20Byte-Pair%20Encoding%20%28BPE%29%20originate%20from%20the%20field%20of%20data%20compression%2C%20and%0Ait%20has%20been%20suggested%20that%20the%20effectiveness%20of%20BPE%20stems%20from%20its%20ability%20to%0Acondense%20text%20into%20a%20relatively%20small%20number%20of%20tokens.%20We%20test%20the%20hypothesis%0Athat%20fewer%20tokens%20lead%20to%20better%20downstream%20performance%20by%20introducing%0APathPiece%2C%20a%20new%20tokenizer%20that%20segments%20a%20document%27s%20text%20into%20the%20minimum%0Anumber%20of%20tokens%20for%20a%20given%20vocabulary.%20Through%20extensive%20experimentation%20we%0Afind%20this%20hypothesis%20not%20to%20be%20the%20case%2C%20casting%20doubt%20on%20the%20understanding%20of%0Athe%20reasons%20for%20effective%20tokenization.%20To%20examine%20which%20other%20factors%20play%20a%0Arole%2C%20we%20evaluate%20design%20decisions%20across%20all%20three%20phases%20of%20tokenization%3A%0Apre-tokenization%2C%20vocabulary%20construction%2C%20and%20segmentation%2C%20offering%20new%0Ainsights%20into%20the%20design%20of%20effective%20tokenizers.%20Specifically%2C%20we%20illustrate%0Athe%20importance%20of%20pre-tokenization%20and%20the%20benefits%20of%20using%20BPE%20to%20initialize%0Avocabulary%20construction.%20We%20train%2064%20language%20models%20with%20varying%20tokenization%2C%0Aranging%20in%20size%20from%20350M%20to%202.4B%20parameters%2C%20all%20of%20which%20are%20made%20publicly%0Aavailable.%0A&entry.1838667208=http%3A//arxiv.org/abs/2402.18376v2&entry.124074799=Read"},
{"title": "SimO Loss: Anchor-Free Contrastive Loss for Fine-Grained Supervised\n  Contrastive Learning", "author": "Taha Bouhsine and Imad El Aaroussi and Atik Faysal and Wang Huaxia", "abstract": "  We introduce a novel anchor-free contrastive learning (AFCL) method\nleveraging our proposed Similarity-Orthogonality (SimO) loss. Our approach\nminimizes a semi-metric discriminative loss function that simultaneously\noptimizes two key objectives: reducing the distance and orthogonality between\nembeddings of similar inputs while maximizing these metrics for dissimilar\ninputs, facilitating more fine-grained contrastive learning. The AFCL method,\npowered by SimO loss, creates a fiber bundle topological structure in the\nembedding space, forming class-specific, internally cohesive yet orthogonal\nneighborhoods. We validate the efficacy of our method on the CIFAR-10 dataset,\nproviding visualizations that demonstrate the impact of SimO loss on the\nembedding space. Our results illustrate the formation of distinct, orthogonal\nclass neighborhoods, showcasing the method's ability to create well-structured\nembeddings that balance class separation with intra-class variability. This\nwork opens new avenues for understanding and leveraging the geometric\nproperties of learned representations in various machine learning tasks.\n", "link": "http://arxiv.org/abs/2410.05233v1", "date": "2024-10-07", "relevancy": 2.5088, "topK": [{"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.523}, {"title": "Self-supervised Photographic Image Layout Representation Learning", "link": "http://arxiv.org/abs/2403.03740v1", "similarity": 0.5078}, {"title": "DDM-NET: End-to-end learning of keypoint feature Detection, Description and Matching for 3D localization", "link": "https://arxiv.org/abs/2212.04575", "similarity": 0.4746}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20SimO%20Loss%3A%20Anchor-Free%20Contrastive%20Loss%20for%20Fine-Grained%20Supervised%0A%20%20Contrastive%20Learning&body=Title%3A%20SimO%20Loss%3A%20Anchor-Free%20Contrastive%20Loss%20for%20Fine-Grained%20Supervised%0A%20%20Contrastive%20Learning%0AAuthor%3A%20Taha%20Bouhsine%20and%20Imad%20El%20Aaroussi%20and%20Atik%20Faysal%20and%20Wang%20Huaxia%0AAbstract%3A%20%20%20We%20introduce%20a%20novel%20anchor-free%20contrastive%20learning%20%28AFCL%29%20method%0Aleveraging%20our%20proposed%20Similarity-Orthogonality%20%28SimO%29%20loss.%20Our%20approach%0Aminimizes%20a%20semi-metric%20discriminative%20loss%20function%20that%20simultaneously%0Aoptimizes%20two%20key%20objectives%3A%20reducing%20the%20distance%20and%20orthogonality%20between%0Aembeddings%20of%20similar%20inputs%20while%20maximizing%20these%20metrics%20for%20dissimilar%0Ainputs%2C%20facilitating%20more%20fine-grained%20contrastive%20learning.%20The%20AFCL%20method%2C%0Apowered%20by%20SimO%20loss%2C%20creates%20a%20fiber%20bundle%20topological%20structure%20in%20the%0Aembedding%20space%2C%20forming%20class-specific%2C%20internally%20cohesive%20yet%20orthogonal%0Aneighborhoods.%20We%20validate%20the%20efficacy%20of%20our%20method%20on%20the%20CIFAR-10%20dataset%2C%0Aproviding%20visualizations%20that%20demonstrate%20the%20impact%20of%20SimO%20loss%20on%20the%0Aembedding%20space.%20Our%20results%20illustrate%20the%20formation%20of%20distinct%2C%20orthogonal%0Aclass%20neighborhoods%2C%20showcasing%20the%20method%27s%20ability%20to%20create%20well-structured%0Aembeddings%20that%20balance%20class%20separation%20with%20intra-class%20variability.%20This%0Awork%20opens%20new%20avenues%20for%20understanding%20and%20leveraging%20the%20geometric%0Aproperties%20of%20learned%20representations%20in%20various%20machine%20learning%20tasks.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2410.05233v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DSimO%2520Loss%253A%2520Anchor-Free%2520Contrastive%2520Loss%2520for%2520Fine-Grained%2520Supervised%250A%2520%2520Contrastive%2520Learning%26entry.906535625%3DTaha%2520Bouhsine%2520and%2520Imad%2520El%2520Aaroussi%2520and%2520Atik%2520Faysal%2520and%2520Wang%2520Huaxia%26entry.1292438233%3D%2520%2520We%2520introduce%2520a%2520novel%2520anchor-free%2520contrastive%2520learning%2520%2528AFCL%2529%2520method%250Aleveraging%2520our%2520proposed%2520Similarity-Orthogonality%2520%2528SimO%2529%2520loss.%2520Our%2520approach%250Aminimizes%2520a%2520semi-metric%2520discriminative%2520loss%2520function%2520that%2520simultaneously%250Aoptimizes%2520two%2520key%2520objectives%253A%2520reducing%2520the%2520distance%2520and%2520orthogonality%2520between%250Aembeddings%2520of%2520similar%2520inputs%2520while%2520maximizing%2520these%2520metrics%2520for%2520dissimilar%250Ainputs%252C%2520facilitating%2520more%2520fine-grained%2520contrastive%2520learning.%2520The%2520AFCL%2520method%252C%250Apowered%2520by%2520SimO%2520loss%252C%2520creates%2520a%2520fiber%2520bundle%2520topological%2520structure%2520in%2520the%250Aembedding%2520space%252C%2520forming%2520class-specific%252C%2520internally%2520cohesive%2520yet%2520orthogonal%250Aneighborhoods.%2520We%2520validate%2520the%2520efficacy%2520of%2520our%2520method%2520on%2520the%2520CIFAR-10%2520dataset%252C%250Aproviding%2520visualizations%2520that%2520demonstrate%2520the%2520impact%2520of%2520SimO%2520loss%2520on%2520the%250Aembedding%2520space.%2520Our%2520results%2520illustrate%2520the%2520formation%2520of%2520distinct%252C%2520orthogonal%250Aclass%2520neighborhoods%252C%2520showcasing%2520the%2520method%2527s%2520ability%2520to%2520create%2520well-structured%250Aembeddings%2520that%2520balance%2520class%2520separation%2520with%2520intra-class%2520variability.%2520This%250Awork%2520opens%2520new%2520avenues%2520for%2520understanding%2520and%2520leveraging%2520the%2520geometric%250Aproperties%2520of%2520learned%2520representations%2520in%2520various%2520machine%2520learning%2520tasks.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2410.05233v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=SimO%20Loss%3A%20Anchor-Free%20Contrastive%20Loss%20for%20Fine-Grained%20Supervised%0A%20%20Contrastive%20Learning&entry.906535625=Taha%20Bouhsine%20and%20Imad%20El%20Aaroussi%20and%20Atik%20Faysal%20and%20Wang%20Huaxia&entry.1292438233=%20%20We%20introduce%20a%20novel%20anchor-free%20contrastive%20learning%20%28AFCL%29%20method%0Aleveraging%20our%20proposed%20Similarity-Orthogonality%20%28SimO%29%20loss.%20Our%20approach%0Aminimizes%20a%20semi-metric%20discriminative%20loss%20function%20that%20simultaneously%0Aoptimizes%20two%20key%20objectives%3A%20reducing%20the%20distance%20and%20orthogonality%20between%0Aembeddings%20of%20similar%20inputs%20while%20maximizing%20these%20metrics%20for%20dissimilar%0Ainputs%2C%20facilitating%20more%20fine-grained%20contrastive%20learning.%20The%20AFCL%20method%2C%0Apowered%20by%20SimO%20loss%2C%20creates%20a%20fiber%20bundle%20topological%20structure%20in%20the%0Aembedding%20space%2C%20forming%20class-specific%2C%20internally%20cohesive%20yet%20orthogonal%0Aneighborhoods.%20We%20validate%20the%20efficacy%20of%20our%20method%20on%20the%20CIFAR-10%20dataset%2C%0Aproviding%20visualizations%20that%20demonstrate%20the%20impact%20of%20SimO%20loss%20on%20the%0Aembedding%20space.%20Our%20results%20illustrate%20the%20formation%20of%20distinct%2C%20orthogonal%0Aclass%20neighborhoods%2C%20showcasing%20the%20method%27s%20ability%20to%20create%20well-structured%0Aembeddings%20that%20balance%20class%20separation%20with%20intra-class%20variability.%20This%0Awork%20opens%20new%20avenues%20for%20understanding%20and%20leveraging%20the%20geometric%0Aproperties%20of%20learned%20representations%20in%20various%20machine%20learning%20tasks.%0A&entry.1838667208=http%3A//arxiv.org/abs/2410.05233v1&entry.124074799=Read"},
{"title": "NoSENSE: Learned unrolled cardiac MRI reconstruction without explicit\n  sensitivity maps", "author": "Felix Frederik Zimmermann and Andreas Kofler", "abstract": "  We present a novel learned image reconstruction method for accelerated\ncardiac MRI with multiple receiver coils based on deep convolutional neural\nnetworks (CNNs) and algorithm unrolling. In contrast to many existing learned\nMR image reconstruction techniques that necessitate coil-sensitivity map (CSM)\nestimation as a distinct network component, our proposed approach avoids\nexplicit CSM estimation. Instead, it implicitly captures and learns to exploit\nthe inter-coil relationships of the images. Our method consists of a series of\nnovel learned image and k-space blocks with shared latent information and\nadaptation to the acquisition parameters by feature-wise modulation (FiLM), as\nwell as coil-wise data-consistency (DC) blocks.\n  Our method achieved PSNR values of 34.89 and 35.56 and SSIM values of 0.920\nand 0.942 in the cine track and mapping track validation leaderboard of the\nMICCAI STACOM CMRxRecon Challenge, respectively, ranking 4th among different\nteams at the time of writing.\n  Code will be made available at https://github.com/fzimmermann89/CMRxRecon\n", "link": "http://arxiv.org/abs/2309.15608v2", "date": "2024-10-07", "relevancy": 2.5006, "topK": [{"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.5075}, {"title": "RTG-SLAM: Real-time 3D Reconstruction at Scale using Gaussian Splatting", "link": "http://arxiv.org/abs/2404.19706v1", "similarity": 0.5032}, {"title": "DDM-NET: End-to-end learning of keypoint feature Detection, Description and Matching for 3D localization", "link": "https://arxiv.org/abs/2212.04575", "similarity": 0.4896}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20NoSENSE%3A%20Learned%20unrolled%20cardiac%20MRI%20reconstruction%20without%20explicit%0A%20%20sensitivity%20maps&body=Title%3A%20NoSENSE%3A%20Learned%20unrolled%20cardiac%20MRI%20reconstruction%20without%20explicit%0A%20%20sensitivity%20maps%0AAuthor%3A%20Felix%20Frederik%20Zimmermann%20and%20Andreas%20Kofler%0AAbstract%3A%20%20%20We%20present%20a%20novel%20learned%20image%20reconstruction%20method%20for%20accelerated%0Acardiac%20MRI%20with%20multiple%20receiver%20coils%20based%20on%20deep%20convolutional%20neural%0Anetworks%20%28CNNs%29%20and%20algorithm%20unrolling.%20In%20contrast%20to%20many%20existing%20learned%0AMR%20image%20reconstruction%20techniques%20that%20necessitate%20coil-sensitivity%20map%20%28CSM%29%0Aestimation%20as%20a%20distinct%20network%20component%2C%20our%20proposed%20approach%20avoids%0Aexplicit%20CSM%20estimation.%20Instead%2C%20it%20implicitly%20captures%20and%20learns%20to%20exploit%0Athe%20inter-coil%20relationships%20of%20the%20images.%20Our%20method%20consists%20of%20a%20series%20of%0Anovel%20learned%20image%20and%20k-space%20blocks%20with%20shared%20latent%20information%20and%0Aadaptation%20to%20the%20acquisition%20parameters%20by%20feature-wise%20modulation%20%28FiLM%29%2C%20as%0Awell%20as%20coil-wise%20data-consistency%20%28DC%29%20blocks.%0A%20%20Our%20method%20achieved%20PSNR%20values%20of%2034.89%20and%2035.56%20and%20SSIM%20values%20of%200.920%0Aand%200.942%20in%20the%20cine%20track%20and%20mapping%20track%20validation%20leaderboard%20of%20the%0AMICCAI%20STACOM%20CMRxRecon%20Challenge%2C%20respectively%2C%20ranking%204th%20among%20different%0Ateams%20at%20the%20time%20of%20writing.%0A%20%20Code%20will%20be%20made%20available%20at%20https%3A//github.com/fzimmermann89/CMRxRecon%0A%0ALink%3A%20http%3A//arxiv.org/abs/2309.15608v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DNoSENSE%253A%2520Learned%2520unrolled%2520cardiac%2520MRI%2520reconstruction%2520without%2520explicit%250A%2520%2520sensitivity%2520maps%26entry.906535625%3DFelix%2520Frederik%2520Zimmermann%2520and%2520Andreas%2520Kofler%26entry.1292438233%3D%2520%2520We%2520present%2520a%2520novel%2520learned%2520image%2520reconstruction%2520method%2520for%2520accelerated%250Acardiac%2520MRI%2520with%2520multiple%2520receiver%2520coils%2520based%2520on%2520deep%2520convolutional%2520neural%250Anetworks%2520%2528CNNs%2529%2520and%2520algorithm%2520unrolling.%2520In%2520contrast%2520to%2520many%2520existing%2520learned%250AMR%2520image%2520reconstruction%2520techniques%2520that%2520necessitate%2520coil-sensitivity%2520map%2520%2528CSM%2529%250Aestimation%2520as%2520a%2520distinct%2520network%2520component%252C%2520our%2520proposed%2520approach%2520avoids%250Aexplicit%2520CSM%2520estimation.%2520Instead%252C%2520it%2520implicitly%2520captures%2520and%2520learns%2520to%2520exploit%250Athe%2520inter-coil%2520relationships%2520of%2520the%2520images.%2520Our%2520method%2520consists%2520of%2520a%2520series%2520of%250Anovel%2520learned%2520image%2520and%2520k-space%2520blocks%2520with%2520shared%2520latent%2520information%2520and%250Aadaptation%2520to%2520the%2520acquisition%2520parameters%2520by%2520feature-wise%2520modulation%2520%2528FiLM%2529%252C%2520as%250Awell%2520as%2520coil-wise%2520data-consistency%2520%2528DC%2529%2520blocks.%250A%2520%2520Our%2520method%2520achieved%2520PSNR%2520values%2520of%252034.89%2520and%252035.56%2520and%2520SSIM%2520values%2520of%25200.920%250Aand%25200.942%2520in%2520the%2520cine%2520track%2520and%2520mapping%2520track%2520validation%2520leaderboard%2520of%2520the%250AMICCAI%2520STACOM%2520CMRxRecon%2520Challenge%252C%2520respectively%252C%2520ranking%25204th%2520among%2520different%250Ateams%2520at%2520the%2520time%2520of%2520writing.%250A%2520%2520Code%2520will%2520be%2520made%2520available%2520at%2520https%253A//github.com/fzimmermann89/CMRxRecon%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2309.15608v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=NoSENSE%3A%20Learned%20unrolled%20cardiac%20MRI%20reconstruction%20without%20explicit%0A%20%20sensitivity%20maps&entry.906535625=Felix%20Frederik%20Zimmermann%20and%20Andreas%20Kofler&entry.1292438233=%20%20We%20present%20a%20novel%20learned%20image%20reconstruction%20method%20for%20accelerated%0Acardiac%20MRI%20with%20multiple%20receiver%20coils%20based%20on%20deep%20convolutional%20neural%0Anetworks%20%28CNNs%29%20and%20algorithm%20unrolling.%20In%20contrast%20to%20many%20existing%20learned%0AMR%20image%20reconstruction%20techniques%20that%20necessitate%20coil-sensitivity%20map%20%28CSM%29%0Aestimation%20as%20a%20distinct%20network%20component%2C%20our%20proposed%20approach%20avoids%0Aexplicit%20CSM%20estimation.%20Instead%2C%20it%20implicitly%20captures%20and%20learns%20to%20exploit%0Athe%20inter-coil%20relationships%20of%20the%20images.%20Our%20method%20consists%20of%20a%20series%20of%0Anovel%20learned%20image%20and%20k-space%20blocks%20with%20shared%20latent%20information%20and%0Aadaptation%20to%20the%20acquisition%20parameters%20by%20feature-wise%20modulation%20%28FiLM%29%2C%20as%0Awell%20as%20coil-wise%20data-consistency%20%28DC%29%20blocks.%0A%20%20Our%20method%20achieved%20PSNR%20values%20of%2034.89%20and%2035.56%20and%20SSIM%20values%20of%200.920%0Aand%200.942%20in%20the%20cine%20track%20and%20mapping%20track%20validation%20leaderboard%20of%20the%0AMICCAI%20STACOM%20CMRxRecon%20Challenge%2C%20respectively%2C%20ranking%204th%20among%20different%0Ateams%20at%20the%20time%20of%20writing.%0A%20%20Code%20will%20be%20made%20available%20at%20https%3A//github.com/fzimmermann89/CMRxRecon%0A&entry.1838667208=http%3A//arxiv.org/abs/2309.15608v2&entry.124074799=Read"},
{"title": "Detecting and Approximating Redundant Computational Blocks in Neural\n  Networks", "author": "Irene Cannistraci and Emanuele Rodol\u00e0 and Bastian Rieck", "abstract": "  Deep neural networks often learn similar internal representations, both\nacross different models and within their own layers. While inter-network\nsimilarities have enabled techniques such as model stitching and merging,\nintra-network similarities present new opportunities for designing more\nefficient architectures. In this paper, we investigate the emergence of these\ninternal similarities across different layers in diverse neural architectures,\nshowing that similarity patterns emerge independently of the datataset used. We\nintroduce a simple metric, Block Redundancy, to detect redundant blocks,\nproviding a foundation for future architectural optimization methods. Building\non this, we propose Redundant Blocks Approximation (RBA), a general framework\nthat identifies and approximates one or more redundant computational blocks\nusing simpler transformations. We show that the transformation $\\mathcal{T}$\nbetween two representations can be efficiently computed in closed-form, and it\nis enough to replace the redundant blocks from the network. RBA reduces model\nparameters and time complexity while maintaining good performance. We validate\nour method on classification tasks in the vision domain using a variety of\npretrained foundational models and datasets.\n", "link": "http://arxiv.org/abs/2410.04941v1", "date": "2024-10-07", "relevancy": 2.4763, "topK": [{"title": "Self-supervised Photographic Image Layout Representation Learning", "link": "http://arxiv.org/abs/2403.03740v1", "similarity": 0.4968}, {"title": "ToDo: Token Downsampling for Efficient Generation of High-Resolution\n  Images", "link": "http://arxiv.org/abs/2402.13573v2", "similarity": 0.4964}, {"title": "Boosted dynamic neural networks", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/26302", "similarity": 0.4926}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Detecting%20and%20Approximating%20Redundant%20Computational%20Blocks%20in%20Neural%0A%20%20Networks&body=Title%3A%20Detecting%20and%20Approximating%20Redundant%20Computational%20Blocks%20in%20Neural%0A%20%20Networks%0AAuthor%3A%20Irene%20Cannistraci%20and%20Emanuele%20Rodol%C3%A0%20and%20Bastian%20Rieck%0AAbstract%3A%20%20%20Deep%20neural%20networks%20often%20learn%20similar%20internal%20representations%2C%20both%0Aacross%20different%20models%20and%20within%20their%20own%20layers.%20While%20inter-network%0Asimilarities%20have%20enabled%20techniques%20such%20as%20model%20stitching%20and%20merging%2C%0Aintra-network%20similarities%20present%20new%20opportunities%20for%20designing%20more%0Aefficient%20architectures.%20In%20this%20paper%2C%20we%20investigate%20the%20emergence%20of%20these%0Ainternal%20similarities%20across%20different%20layers%20in%20diverse%20neural%20architectures%2C%0Ashowing%20that%20similarity%20patterns%20emerge%20independently%20of%20the%20datataset%20used.%20We%0Aintroduce%20a%20simple%20metric%2C%20Block%20Redundancy%2C%20to%20detect%20redundant%20blocks%2C%0Aproviding%20a%20foundation%20for%20future%20architectural%20optimization%20methods.%20Building%0Aon%20this%2C%20we%20propose%20Redundant%20Blocks%20Approximation%20%28RBA%29%2C%20a%20general%20framework%0Athat%20identifies%20and%20approximates%20one%20or%20more%20redundant%20computational%20blocks%0Ausing%20simpler%20transformations.%20We%20show%20that%20the%20transformation%20%24%5Cmathcal%7BT%7D%24%0Abetween%20two%20representations%20can%20be%20efficiently%20computed%20in%20closed-form%2C%20and%20it%0Ais%20enough%20to%20replace%20the%20redundant%20blocks%20from%20the%20network.%20RBA%20reduces%20model%0Aparameters%20and%20time%20complexity%20while%20maintaining%20good%20performance.%20We%20validate%0Aour%20method%20on%20classification%20tasks%20in%20the%20vision%20domain%20using%20a%20variety%20of%0Apretrained%20foundational%20models%20and%20datasets.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2410.04941v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DDetecting%2520and%2520Approximating%2520Redundant%2520Computational%2520Blocks%2520in%2520Neural%250A%2520%2520Networks%26entry.906535625%3DIrene%2520Cannistraci%2520and%2520Emanuele%2520Rodol%25C3%25A0%2520and%2520Bastian%2520Rieck%26entry.1292438233%3D%2520%2520Deep%2520neural%2520networks%2520often%2520learn%2520similar%2520internal%2520representations%252C%2520both%250Aacross%2520different%2520models%2520and%2520within%2520their%2520own%2520layers.%2520While%2520inter-network%250Asimilarities%2520have%2520enabled%2520techniques%2520such%2520as%2520model%2520stitching%2520and%2520merging%252C%250Aintra-network%2520similarities%2520present%2520new%2520opportunities%2520for%2520designing%2520more%250Aefficient%2520architectures.%2520In%2520this%2520paper%252C%2520we%2520investigate%2520the%2520emergence%2520of%2520these%250Ainternal%2520similarities%2520across%2520different%2520layers%2520in%2520diverse%2520neural%2520architectures%252C%250Ashowing%2520that%2520similarity%2520patterns%2520emerge%2520independently%2520of%2520the%2520datataset%2520used.%2520We%250Aintroduce%2520a%2520simple%2520metric%252C%2520Block%2520Redundancy%252C%2520to%2520detect%2520redundant%2520blocks%252C%250Aproviding%2520a%2520foundation%2520for%2520future%2520architectural%2520optimization%2520methods.%2520Building%250Aon%2520this%252C%2520we%2520propose%2520Redundant%2520Blocks%2520Approximation%2520%2528RBA%2529%252C%2520a%2520general%2520framework%250Athat%2520identifies%2520and%2520approximates%2520one%2520or%2520more%2520redundant%2520computational%2520blocks%250Ausing%2520simpler%2520transformations.%2520We%2520show%2520that%2520the%2520transformation%2520%2524%255Cmathcal%257BT%257D%2524%250Abetween%2520two%2520representations%2520can%2520be%2520efficiently%2520computed%2520in%2520closed-form%252C%2520and%2520it%250Ais%2520enough%2520to%2520replace%2520the%2520redundant%2520blocks%2520from%2520the%2520network.%2520RBA%2520reduces%2520model%250Aparameters%2520and%2520time%2520complexity%2520while%2520maintaining%2520good%2520performance.%2520We%2520validate%250Aour%2520method%2520on%2520classification%2520tasks%2520in%2520the%2520vision%2520domain%2520using%2520a%2520variety%2520of%250Apretrained%2520foundational%2520models%2520and%2520datasets.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2410.04941v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Detecting%20and%20Approximating%20Redundant%20Computational%20Blocks%20in%20Neural%0A%20%20Networks&entry.906535625=Irene%20Cannistraci%20and%20Emanuele%20Rodol%C3%A0%20and%20Bastian%20Rieck&entry.1292438233=%20%20Deep%20neural%20networks%20often%20learn%20similar%20internal%20representations%2C%20both%0Aacross%20different%20models%20and%20within%20their%20own%20layers.%20While%20inter-network%0Asimilarities%20have%20enabled%20techniques%20such%20as%20model%20stitching%20and%20merging%2C%0Aintra-network%20similarities%20present%20new%20opportunities%20for%20designing%20more%0Aefficient%20architectures.%20In%20this%20paper%2C%20we%20investigate%20the%20emergence%20of%20these%0Ainternal%20similarities%20across%20different%20layers%20in%20diverse%20neural%20architectures%2C%0Ashowing%20that%20similarity%20patterns%20emerge%20independently%20of%20the%20datataset%20used.%20We%0Aintroduce%20a%20simple%20metric%2C%20Block%20Redundancy%2C%20to%20detect%20redundant%20blocks%2C%0Aproviding%20a%20foundation%20for%20future%20architectural%20optimization%20methods.%20Building%0Aon%20this%2C%20we%20propose%20Redundant%20Blocks%20Approximation%20%28RBA%29%2C%20a%20general%20framework%0Athat%20identifies%20and%20approximates%20one%20or%20more%20redundant%20computational%20blocks%0Ausing%20simpler%20transformations.%20We%20show%20that%20the%20transformation%20%24%5Cmathcal%7BT%7D%24%0Abetween%20two%20representations%20can%20be%20efficiently%20computed%20in%20closed-form%2C%20and%20it%0Ais%20enough%20to%20replace%20the%20redundant%20blocks%20from%20the%20network.%20RBA%20reduces%20model%0Aparameters%20and%20time%20complexity%20while%20maintaining%20good%20performance.%20We%20validate%0Aour%20method%20on%20classification%20tasks%20in%20the%20vision%20domain%20using%20a%20variety%20of%0Apretrained%20foundational%20models%20and%20datasets.%0A&entry.1838667208=http%3A//arxiv.org/abs/2410.04941v1&entry.124074799=Read"},
{"title": "ETGL-DDPG: A Deep Deterministic Policy Gradient Algorithm for Sparse\n  Reward Continuous Control", "author": "Ehsan Futuhi and Shayan Karimi and Chao Gao and Martin M\u00fcller", "abstract": "  We consider deep deterministic policy gradient (DDPG) in the context of\nreinforcement learning with sparse rewards. To enhance exploration, we\nintroduce a search procedure, \\emph{${\\epsilon}{t}$-greedy}, which generates\nexploratory options for exploring less-visited states. We prove that search\nusing $\\epsilon t$-greedy has polynomial sample complexity under mild MDP\nassumptions. To more efficiently use the information provided by rewarded\ntransitions, we develop a new dual experience replay buffer framework,\n\\emph{GDRB}, and implement \\emph{longest n-step returns}. The resulting\nalgorithm, \\emph{ETGL-DDPG}, integrates all three techniques: \\bm{$\\epsilon\nt$}-greedy, \\textbf{G}DRB, and \\textbf{L}ongest $n$-step, into DDPG. We\nevaluate ETGL-DDPG on standard benchmarks and demonstrate that it outperforms\nDDPG, as well as other state-of-the-art methods, across all tested\nsparse-reward continuous environments. Ablation studies further highlight how\neach strategy individually enhances the performance of DDPG in this setting.\n", "link": "http://arxiv.org/abs/2410.05225v1", "date": "2024-10-07", "relevancy": 2.4669, "topK": [{"title": "UGG: Unified Generative Grasping", "link": "https://arxiv.org/abs/2311.16917", "similarity": 0.508}, {"title": "Boosted dynamic neural networks", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/26302", "similarity": 0.49}, {"title": "ToDo: Token Downsampling for Efficient Generation of High-Resolution\n  Images", "link": "http://arxiv.org/abs/2402.13573v2", "similarity": 0.4821}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20ETGL-DDPG%3A%20A%20Deep%20Deterministic%20Policy%20Gradient%20Algorithm%20for%20Sparse%0A%20%20Reward%20Continuous%20Control&body=Title%3A%20ETGL-DDPG%3A%20A%20Deep%20Deterministic%20Policy%20Gradient%20Algorithm%20for%20Sparse%0A%20%20Reward%20Continuous%20Control%0AAuthor%3A%20Ehsan%20Futuhi%20and%20Shayan%20Karimi%20and%20Chao%20Gao%20and%20Martin%20M%C3%BCller%0AAbstract%3A%20%20%20We%20consider%20deep%20deterministic%20policy%20gradient%20%28DDPG%29%20in%20the%20context%20of%0Areinforcement%20learning%20with%20sparse%20rewards.%20To%20enhance%20exploration%2C%20we%0Aintroduce%20a%20search%20procedure%2C%20%5Cemph%7B%24%7B%5Cepsilon%7D%7Bt%7D%24-greedy%7D%2C%20which%20generates%0Aexploratory%20options%20for%20exploring%20less-visited%20states.%20We%20prove%20that%20search%0Ausing%20%24%5Cepsilon%20t%24-greedy%20has%20polynomial%20sample%20complexity%20under%20mild%20MDP%0Aassumptions.%20To%20more%20efficiently%20use%20the%20information%20provided%20by%20rewarded%0Atransitions%2C%20we%20develop%20a%20new%20dual%20experience%20replay%20buffer%20framework%2C%0A%5Cemph%7BGDRB%7D%2C%20and%20implement%20%5Cemph%7Blongest%20n-step%20returns%7D.%20The%20resulting%0Aalgorithm%2C%20%5Cemph%7BETGL-DDPG%7D%2C%20integrates%20all%20three%20techniques%3A%20%5Cbm%7B%24%5Cepsilon%0At%24%7D-greedy%2C%20%5Ctextbf%7BG%7DDRB%2C%20and%20%5Ctextbf%7BL%7Dongest%20%24n%24-step%2C%20into%20DDPG.%20We%0Aevaluate%20ETGL-DDPG%20on%20standard%20benchmarks%20and%20demonstrate%20that%20it%20outperforms%0ADDPG%2C%20as%20well%20as%20other%20state-of-the-art%20methods%2C%20across%20all%20tested%0Asparse-reward%20continuous%20environments.%20Ablation%20studies%20further%20highlight%20how%0Aeach%20strategy%20individually%20enhances%20the%20performance%20of%20DDPG%20in%20this%20setting.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2410.05225v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DETGL-DDPG%253A%2520A%2520Deep%2520Deterministic%2520Policy%2520Gradient%2520Algorithm%2520for%2520Sparse%250A%2520%2520Reward%2520Continuous%2520Control%26entry.906535625%3DEhsan%2520Futuhi%2520and%2520Shayan%2520Karimi%2520and%2520Chao%2520Gao%2520and%2520Martin%2520M%25C3%25BCller%26entry.1292438233%3D%2520%2520We%2520consider%2520deep%2520deterministic%2520policy%2520gradient%2520%2528DDPG%2529%2520in%2520the%2520context%2520of%250Areinforcement%2520learning%2520with%2520sparse%2520rewards.%2520To%2520enhance%2520exploration%252C%2520we%250Aintroduce%2520a%2520search%2520procedure%252C%2520%255Cemph%257B%2524%257B%255Cepsilon%257D%257Bt%257D%2524-greedy%257D%252C%2520which%2520generates%250Aexploratory%2520options%2520for%2520exploring%2520less-visited%2520states.%2520We%2520prove%2520that%2520search%250Ausing%2520%2524%255Cepsilon%2520t%2524-greedy%2520has%2520polynomial%2520sample%2520complexity%2520under%2520mild%2520MDP%250Aassumptions.%2520To%2520more%2520efficiently%2520use%2520the%2520information%2520provided%2520by%2520rewarded%250Atransitions%252C%2520we%2520develop%2520a%2520new%2520dual%2520experience%2520replay%2520buffer%2520framework%252C%250A%255Cemph%257BGDRB%257D%252C%2520and%2520implement%2520%255Cemph%257Blongest%2520n-step%2520returns%257D.%2520The%2520resulting%250Aalgorithm%252C%2520%255Cemph%257BETGL-DDPG%257D%252C%2520integrates%2520all%2520three%2520techniques%253A%2520%255Cbm%257B%2524%255Cepsilon%250At%2524%257D-greedy%252C%2520%255Ctextbf%257BG%257DDRB%252C%2520and%2520%255Ctextbf%257BL%257Dongest%2520%2524n%2524-step%252C%2520into%2520DDPG.%2520We%250Aevaluate%2520ETGL-DDPG%2520on%2520standard%2520benchmarks%2520and%2520demonstrate%2520that%2520it%2520outperforms%250ADDPG%252C%2520as%2520well%2520as%2520other%2520state-of-the-art%2520methods%252C%2520across%2520all%2520tested%250Asparse-reward%2520continuous%2520environments.%2520Ablation%2520studies%2520further%2520highlight%2520how%250Aeach%2520strategy%2520individually%2520enhances%2520the%2520performance%2520of%2520DDPG%2520in%2520this%2520setting.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2410.05225v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=ETGL-DDPG%3A%20A%20Deep%20Deterministic%20Policy%20Gradient%20Algorithm%20for%20Sparse%0A%20%20Reward%20Continuous%20Control&entry.906535625=Ehsan%20Futuhi%20and%20Shayan%20Karimi%20and%20Chao%20Gao%20and%20Martin%20M%C3%BCller&entry.1292438233=%20%20We%20consider%20deep%20deterministic%20policy%20gradient%20%28DDPG%29%20in%20the%20context%20of%0Areinforcement%20learning%20with%20sparse%20rewards.%20To%20enhance%20exploration%2C%20we%0Aintroduce%20a%20search%20procedure%2C%20%5Cemph%7B%24%7B%5Cepsilon%7D%7Bt%7D%24-greedy%7D%2C%20which%20generates%0Aexploratory%20options%20for%20exploring%20less-visited%20states.%20We%20prove%20that%20search%0Ausing%20%24%5Cepsilon%20t%24-greedy%20has%20polynomial%20sample%20complexity%20under%20mild%20MDP%0Aassumptions.%20To%20more%20efficiently%20use%20the%20information%20provided%20by%20rewarded%0Atransitions%2C%20we%20develop%20a%20new%20dual%20experience%20replay%20buffer%20framework%2C%0A%5Cemph%7BGDRB%7D%2C%20and%20implement%20%5Cemph%7Blongest%20n-step%20returns%7D.%20The%20resulting%0Aalgorithm%2C%20%5Cemph%7BETGL-DDPG%7D%2C%20integrates%20all%20three%20techniques%3A%20%5Cbm%7B%24%5Cepsilon%0At%24%7D-greedy%2C%20%5Ctextbf%7BG%7DDRB%2C%20and%20%5Ctextbf%7BL%7Dongest%20%24n%24-step%2C%20into%20DDPG.%20We%0Aevaluate%20ETGL-DDPG%20on%20standard%20benchmarks%20and%20demonstrate%20that%20it%20outperforms%0ADDPG%2C%20as%20well%20as%20other%20state-of-the-art%20methods%2C%20across%20all%20tested%0Asparse-reward%20continuous%20environments.%20Ablation%20studies%20further%20highlight%20how%0Aeach%20strategy%20individually%20enhances%20the%20performance%20of%20DDPG%20in%20this%20setting.%0A&entry.1838667208=http%3A//arxiv.org/abs/2410.05225v1&entry.124074799=Read"},
{"title": "Systematic Literature Review of Vision-Based Approaches to Outdoor\n  Livestock Monitoring with Lessons from Wildlife Studies", "author": "Stacey D. Scott and Zayn J. Abbas and Feerass Ellid and Eli-Henry Dykhne and Muhammad Muhaiminul Islam and Weam Ayad and Kristina Kacmorova and Dan Tulpan and Minglun Gong", "abstract": "  Precision livestock farming (PLF) aims to improve the health and welfare of\nlivestock animals and farming outcomes through the use of advanced\ntechnologies. Computer vision, combined with recent advances in machine\nlearning and deep learning artificial intelligence approaches, offers a\npossible solution to the PLF ideal of 24/7 livestock monitoring that helps\nfacilitate early detection of animal health and welfare issues. However, a\nsignificant number of livestock species are raised in large outdoor habitats\nthat pose technological challenges for computer vision approaches. This review\nprovides a comprehensive overview of computer vision methods and open\nchallenges in outdoor animal monitoring. We include research from both the\nlivestock and wildlife fields in the review because of the similarities in\nappearance, behaviour, and habitat for many livestock and wildlife. We focus on\nlarge terrestrial mammals, such as cattle, horses, deer, goats, sheep, koalas,\ngiraffes, and elephants. We use an image processing pipeline to frame our\ndiscussion and highlight the current capabilities and open technical challenges\nat each stage of the pipeline. The review found a clear trend towards the use\nof deep learning approaches for animal detection, counting, and multi-species\nclassification. We discuss in detail the applicability of current vision-based\nmethods to PLF contexts and promising directions for future research.\n", "link": "http://arxiv.org/abs/2410.05041v1", "date": "2024-10-07", "relevancy": 2.4631, "topK": [{"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.509}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.4844}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.4844}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Systematic%20Literature%20Review%20of%20Vision-Based%20Approaches%20to%20Outdoor%0A%20%20Livestock%20Monitoring%20with%20Lessons%20from%20Wildlife%20Studies&body=Title%3A%20Systematic%20Literature%20Review%20of%20Vision-Based%20Approaches%20to%20Outdoor%0A%20%20Livestock%20Monitoring%20with%20Lessons%20from%20Wildlife%20Studies%0AAuthor%3A%20Stacey%20D.%20Scott%20and%20Zayn%20J.%20Abbas%20and%20Feerass%20Ellid%20and%20Eli-Henry%20Dykhne%20and%20Muhammad%20Muhaiminul%20Islam%20and%20Weam%20Ayad%20and%20Kristina%20Kacmorova%20and%20Dan%20Tulpan%20and%20Minglun%20Gong%0AAbstract%3A%20%20%20Precision%20livestock%20farming%20%28PLF%29%20aims%20to%20improve%20the%20health%20and%20welfare%20of%0Alivestock%20animals%20and%20farming%20outcomes%20through%20the%20use%20of%20advanced%0Atechnologies.%20Computer%20vision%2C%20combined%20with%20recent%20advances%20in%20machine%0Alearning%20and%20deep%20learning%20artificial%20intelligence%20approaches%2C%20offers%20a%0Apossible%20solution%20to%20the%20PLF%20ideal%20of%2024/7%20livestock%20monitoring%20that%20helps%0Afacilitate%20early%20detection%20of%20animal%20health%20and%20welfare%20issues.%20However%2C%20a%0Asignificant%20number%20of%20livestock%20species%20are%20raised%20in%20large%20outdoor%20habitats%0Athat%20pose%20technological%20challenges%20for%20computer%20vision%20approaches.%20This%20review%0Aprovides%20a%20comprehensive%20overview%20of%20computer%20vision%20methods%20and%20open%0Achallenges%20in%20outdoor%20animal%20monitoring.%20We%20include%20research%20from%20both%20the%0Alivestock%20and%20wildlife%20fields%20in%20the%20review%20because%20of%20the%20similarities%20in%0Aappearance%2C%20behaviour%2C%20and%20habitat%20for%20many%20livestock%20and%20wildlife.%20We%20focus%20on%0Alarge%20terrestrial%20mammals%2C%20such%20as%20cattle%2C%20horses%2C%20deer%2C%20goats%2C%20sheep%2C%20koalas%2C%0Agiraffes%2C%20and%20elephants.%20We%20use%20an%20image%20processing%20pipeline%20to%20frame%20our%0Adiscussion%20and%20highlight%20the%20current%20capabilities%20and%20open%20technical%20challenges%0Aat%20each%20stage%20of%20the%20pipeline.%20The%20review%20found%20a%20clear%20trend%20towards%20the%20use%0Aof%20deep%20learning%20approaches%20for%20animal%20detection%2C%20counting%2C%20and%20multi-species%0Aclassification.%20We%20discuss%20in%20detail%20the%20applicability%20of%20current%20vision-based%0Amethods%20to%20PLF%20contexts%20and%20promising%20directions%20for%20future%20research.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2410.05041v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DSystematic%2520Literature%2520Review%2520of%2520Vision-Based%2520Approaches%2520to%2520Outdoor%250A%2520%2520Livestock%2520Monitoring%2520with%2520Lessons%2520from%2520Wildlife%2520Studies%26entry.906535625%3DStacey%2520D.%2520Scott%2520and%2520Zayn%2520J.%2520Abbas%2520and%2520Feerass%2520Ellid%2520and%2520Eli-Henry%2520Dykhne%2520and%2520Muhammad%2520Muhaiminul%2520Islam%2520and%2520Weam%2520Ayad%2520and%2520Kristina%2520Kacmorova%2520and%2520Dan%2520Tulpan%2520and%2520Minglun%2520Gong%26entry.1292438233%3D%2520%2520Precision%2520livestock%2520farming%2520%2528PLF%2529%2520aims%2520to%2520improve%2520the%2520health%2520and%2520welfare%2520of%250Alivestock%2520animals%2520and%2520farming%2520outcomes%2520through%2520the%2520use%2520of%2520advanced%250Atechnologies.%2520Computer%2520vision%252C%2520combined%2520with%2520recent%2520advances%2520in%2520machine%250Alearning%2520and%2520deep%2520learning%2520artificial%2520intelligence%2520approaches%252C%2520offers%2520a%250Apossible%2520solution%2520to%2520the%2520PLF%2520ideal%2520of%252024/7%2520livestock%2520monitoring%2520that%2520helps%250Afacilitate%2520early%2520detection%2520of%2520animal%2520health%2520and%2520welfare%2520issues.%2520However%252C%2520a%250Asignificant%2520number%2520of%2520livestock%2520species%2520are%2520raised%2520in%2520large%2520outdoor%2520habitats%250Athat%2520pose%2520technological%2520challenges%2520for%2520computer%2520vision%2520approaches.%2520This%2520review%250Aprovides%2520a%2520comprehensive%2520overview%2520of%2520computer%2520vision%2520methods%2520and%2520open%250Achallenges%2520in%2520outdoor%2520animal%2520monitoring.%2520We%2520include%2520research%2520from%2520both%2520the%250Alivestock%2520and%2520wildlife%2520fields%2520in%2520the%2520review%2520because%2520of%2520the%2520similarities%2520in%250Aappearance%252C%2520behaviour%252C%2520and%2520habitat%2520for%2520many%2520livestock%2520and%2520wildlife.%2520We%2520focus%2520on%250Alarge%2520terrestrial%2520mammals%252C%2520such%2520as%2520cattle%252C%2520horses%252C%2520deer%252C%2520goats%252C%2520sheep%252C%2520koalas%252C%250Agiraffes%252C%2520and%2520elephants.%2520We%2520use%2520an%2520image%2520processing%2520pipeline%2520to%2520frame%2520our%250Adiscussion%2520and%2520highlight%2520the%2520current%2520capabilities%2520and%2520open%2520technical%2520challenges%250Aat%2520each%2520stage%2520of%2520the%2520pipeline.%2520The%2520review%2520found%2520a%2520clear%2520trend%2520towards%2520the%2520use%250Aof%2520deep%2520learning%2520approaches%2520for%2520animal%2520detection%252C%2520counting%252C%2520and%2520multi-species%250Aclassification.%2520We%2520discuss%2520in%2520detail%2520the%2520applicability%2520of%2520current%2520vision-based%250Amethods%2520to%2520PLF%2520contexts%2520and%2520promising%2520directions%2520for%2520future%2520research.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2410.05041v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Systematic%20Literature%20Review%20of%20Vision-Based%20Approaches%20to%20Outdoor%0A%20%20Livestock%20Monitoring%20with%20Lessons%20from%20Wildlife%20Studies&entry.906535625=Stacey%20D.%20Scott%20and%20Zayn%20J.%20Abbas%20and%20Feerass%20Ellid%20and%20Eli-Henry%20Dykhne%20and%20Muhammad%20Muhaiminul%20Islam%20and%20Weam%20Ayad%20and%20Kristina%20Kacmorova%20and%20Dan%20Tulpan%20and%20Minglun%20Gong&entry.1292438233=%20%20Precision%20livestock%20farming%20%28PLF%29%20aims%20to%20improve%20the%20health%20and%20welfare%20of%0Alivestock%20animals%20and%20farming%20outcomes%20through%20the%20use%20of%20advanced%0Atechnologies.%20Computer%20vision%2C%20combined%20with%20recent%20advances%20in%20machine%0Alearning%20and%20deep%20learning%20artificial%20intelligence%20approaches%2C%20offers%20a%0Apossible%20solution%20to%20the%20PLF%20ideal%20of%2024/7%20livestock%20monitoring%20that%20helps%0Afacilitate%20early%20detection%20of%20animal%20health%20and%20welfare%20issues.%20However%2C%20a%0Asignificant%20number%20of%20livestock%20species%20are%20raised%20in%20large%20outdoor%20habitats%0Athat%20pose%20technological%20challenges%20for%20computer%20vision%20approaches.%20This%20review%0Aprovides%20a%20comprehensive%20overview%20of%20computer%20vision%20methods%20and%20open%0Achallenges%20in%20outdoor%20animal%20monitoring.%20We%20include%20research%20from%20both%20the%0Alivestock%20and%20wildlife%20fields%20in%20the%20review%20because%20of%20the%20similarities%20in%0Aappearance%2C%20behaviour%2C%20and%20habitat%20for%20many%20livestock%20and%20wildlife.%20We%20focus%20on%0Alarge%20terrestrial%20mammals%2C%20such%20as%20cattle%2C%20horses%2C%20deer%2C%20goats%2C%20sheep%2C%20koalas%2C%0Agiraffes%2C%20and%20elephants.%20We%20use%20an%20image%20processing%20pipeline%20to%20frame%20our%0Adiscussion%20and%20highlight%20the%20current%20capabilities%20and%20open%20technical%20challenges%0Aat%20each%20stage%20of%20the%20pipeline.%20The%20review%20found%20a%20clear%20trend%20towards%20the%20use%0Aof%20deep%20learning%20approaches%20for%20animal%20detection%2C%20counting%2C%20and%20multi-species%0Aclassification.%20We%20discuss%20in%20detail%20the%20applicability%20of%20current%20vision-based%0Amethods%20to%20PLF%20contexts%20and%20promising%20directions%20for%20future%20research.%0A&entry.1838667208=http%3A//arxiv.org/abs/2410.05041v1&entry.124074799=Read"},
{"title": "Finding Visual Task Vectors", "author": "Alberto Hojel and Yutong Bai and Trevor Darrell and Amir Globerson and Amir Bar", "abstract": "  Visual Prompting is a technique for teaching models to perform a visual task\nvia in-context examples, without any additional training. In this work, we\nanalyze the activations of MAE-VQGAN, a recent Visual Prompting model, and find\ntask vectors, activations that encode task-specific information. Equipped with\nthis insight, we demonstrate that it is possible to identify the task vectors\nand use them to guide the network towards performing different tasks without\nproviding any input-output examples. To find task vectors, we compute the\naverage intermediate activations per task and use the REINFORCE algorithm to\nsearch for the subset of task vectors. The resulting task vectors guide the\nmodel towards performing a task better than the original model without the need\nfor input-output examples.\n", "link": "http://arxiv.org/abs/2404.05729v2", "date": "2024-10-07", "relevancy": 2.4585, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5029}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5029}, {"title": "Genie: Generative Interactive Environments", "link": "http://arxiv.org/abs/2402.15391v1", "similarity": 0.4693}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Finding%20Visual%20Task%20Vectors&body=Title%3A%20Finding%20Visual%20Task%20Vectors%0AAuthor%3A%20Alberto%20Hojel%20and%20Yutong%20Bai%20and%20Trevor%20Darrell%20and%20Amir%20Globerson%20and%20Amir%20Bar%0AAbstract%3A%20%20%20Visual%20Prompting%20is%20a%20technique%20for%20teaching%20models%20to%20perform%20a%20visual%20task%0Avia%20in-context%20examples%2C%20without%20any%20additional%20training.%20In%20this%20work%2C%20we%0Aanalyze%20the%20activations%20of%20MAE-VQGAN%2C%20a%20recent%20Visual%20Prompting%20model%2C%20and%20find%0Atask%20vectors%2C%20activations%20that%20encode%20task-specific%20information.%20Equipped%20with%0Athis%20insight%2C%20we%20demonstrate%20that%20it%20is%20possible%20to%20identify%20the%20task%20vectors%0Aand%20use%20them%20to%20guide%20the%20network%20towards%20performing%20different%20tasks%20without%0Aproviding%20any%20input-output%20examples.%20To%20find%20task%20vectors%2C%20we%20compute%20the%0Aaverage%20intermediate%20activations%20per%20task%20and%20use%20the%20REINFORCE%20algorithm%20to%0Asearch%20for%20the%20subset%20of%20task%20vectors.%20The%20resulting%20task%20vectors%20guide%20the%0Amodel%20towards%20performing%20a%20task%20better%20than%20the%20original%20model%20without%20the%20need%0Afor%20input-output%20examples.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2404.05729v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DFinding%2520Visual%2520Task%2520Vectors%26entry.906535625%3DAlberto%2520Hojel%2520and%2520Yutong%2520Bai%2520and%2520Trevor%2520Darrell%2520and%2520Amir%2520Globerson%2520and%2520Amir%2520Bar%26entry.1292438233%3D%2520%2520Visual%2520Prompting%2520is%2520a%2520technique%2520for%2520teaching%2520models%2520to%2520perform%2520a%2520visual%2520task%250Avia%2520in-context%2520examples%252C%2520without%2520any%2520additional%2520training.%2520In%2520this%2520work%252C%2520we%250Aanalyze%2520the%2520activations%2520of%2520MAE-VQGAN%252C%2520a%2520recent%2520Visual%2520Prompting%2520model%252C%2520and%2520find%250Atask%2520vectors%252C%2520activations%2520that%2520encode%2520task-specific%2520information.%2520Equipped%2520with%250Athis%2520insight%252C%2520we%2520demonstrate%2520that%2520it%2520is%2520possible%2520to%2520identify%2520the%2520task%2520vectors%250Aand%2520use%2520them%2520to%2520guide%2520the%2520network%2520towards%2520performing%2520different%2520tasks%2520without%250Aproviding%2520any%2520input-output%2520examples.%2520To%2520find%2520task%2520vectors%252C%2520we%2520compute%2520the%250Aaverage%2520intermediate%2520activations%2520per%2520task%2520and%2520use%2520the%2520REINFORCE%2520algorithm%2520to%250Asearch%2520for%2520the%2520subset%2520of%2520task%2520vectors.%2520The%2520resulting%2520task%2520vectors%2520guide%2520the%250Amodel%2520towards%2520performing%2520a%2520task%2520better%2520than%2520the%2520original%2520model%2520without%2520the%2520need%250Afor%2520input-output%2520examples.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2404.05729v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Finding%20Visual%20Task%20Vectors&entry.906535625=Alberto%20Hojel%20and%20Yutong%20Bai%20and%20Trevor%20Darrell%20and%20Amir%20Globerson%20and%20Amir%20Bar&entry.1292438233=%20%20Visual%20Prompting%20is%20a%20technique%20for%20teaching%20models%20to%20perform%20a%20visual%20task%0Avia%20in-context%20examples%2C%20without%20any%20additional%20training.%20In%20this%20work%2C%20we%0Aanalyze%20the%20activations%20of%20MAE-VQGAN%2C%20a%20recent%20Visual%20Prompting%20model%2C%20and%20find%0Atask%20vectors%2C%20activations%20that%20encode%20task-specific%20information.%20Equipped%20with%0Athis%20insight%2C%20we%20demonstrate%20that%20it%20is%20possible%20to%20identify%20the%20task%20vectors%0Aand%20use%20them%20to%20guide%20the%20network%20towards%20performing%20different%20tasks%20without%0Aproviding%20any%20input-output%20examples.%20To%20find%20task%20vectors%2C%20we%20compute%20the%0Aaverage%20intermediate%20activations%20per%20task%20and%20use%20the%20REINFORCE%20algorithm%20to%0Asearch%20for%20the%20subset%20of%20task%20vectors.%20The%20resulting%20task%20vectors%20guide%20the%0Amodel%20towards%20performing%20a%20task%20better%20than%20the%20original%20model%20without%20the%20need%0Afor%20input-output%20examples.%0A&entry.1838667208=http%3A//arxiv.org/abs/2404.05729v2&entry.124074799=Read"},
{"title": "Fine-Tuning and Prompt Optimization: Two Great Steps that Work Better\n  Together", "author": "Dilara Soylu and Christopher Potts and Omar Khattab", "abstract": "  Natural Language Processing (NLP) systems are increasingly taking the form of\nsophisticated modular pipelines, e.g., Retrieval Augmented Generation (RAG),\nwhere each module may involve a distinct Language Model (LM) and an associated\nprompt template. These compound systems often lack intermediate labels or\ngradient flow to optimize each module, making their end-to-end optimization\nchallenging. Here we seek strategies to optimize both the module-level LM\nweights and the associated prompt templates of such systems to maximize a\ndownstream task metric. We propose for the first time combining the weight and\nprompt optimization strategies to optimize a modular LM pipeline by alternating\nbetween the two to get the same LM to teach itself. In experiments with\nmulti-hop QA, mathematical reasoning, and feature-based classification using\nmistral-7b, llama-2-7b, and llama-3-8b, these BetterTogether strategies\noptimizing the weights and prompts of a pipeline together outperform directly\noptimizing weights alone and prompts alone by up to 60% and 6%, respectively,\non average across LMs and tasks. BetterTogether optimizer is released in DSPy\nat http://dspy.ai\n", "link": "http://arxiv.org/abs/2407.10930v2", "date": "2024-10-07", "relevancy": 2.4509, "topK": [{"title": "ToDo: Token Downsampling for Efficient Generation of High-Resolution\n  Images", "link": "http://arxiv.org/abs/2402.13573v2", "similarity": 0.5002}, {"title": "Boosted dynamic neural networks", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/26302", "similarity": 0.492}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.4783}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Fine-Tuning%20and%20Prompt%20Optimization%3A%20Two%20Great%20Steps%20that%20Work%20Better%0A%20%20Together&body=Title%3A%20Fine-Tuning%20and%20Prompt%20Optimization%3A%20Two%20Great%20Steps%20that%20Work%20Better%0A%20%20Together%0AAuthor%3A%20Dilara%20Soylu%20and%20Christopher%20Potts%20and%20Omar%20Khattab%0AAbstract%3A%20%20%20Natural%20Language%20Processing%20%28NLP%29%20systems%20are%20increasingly%20taking%20the%20form%20of%0Asophisticated%20modular%20pipelines%2C%20e.g.%2C%20Retrieval%20Augmented%20Generation%20%28RAG%29%2C%0Awhere%20each%20module%20may%20involve%20a%20distinct%20Language%20Model%20%28LM%29%20and%20an%20associated%0Aprompt%20template.%20These%20compound%20systems%20often%20lack%20intermediate%20labels%20or%0Agradient%20flow%20to%20optimize%20each%20module%2C%20making%20their%20end-to-end%20optimization%0Achallenging.%20Here%20we%20seek%20strategies%20to%20optimize%20both%20the%20module-level%20LM%0Aweights%20and%20the%20associated%20prompt%20templates%20of%20such%20systems%20to%20maximize%20a%0Adownstream%20task%20metric.%20We%20propose%20for%20the%20first%20time%20combining%20the%20weight%20and%0Aprompt%20optimization%20strategies%20to%20optimize%20a%20modular%20LM%20pipeline%20by%20alternating%0Abetween%20the%20two%20to%20get%20the%20same%20LM%20to%20teach%20itself.%20In%20experiments%20with%0Amulti-hop%20QA%2C%20mathematical%20reasoning%2C%20and%20feature-based%20classification%20using%0Amistral-7b%2C%20llama-2-7b%2C%20and%20llama-3-8b%2C%20these%20BetterTogether%20strategies%0Aoptimizing%20the%20weights%20and%20prompts%20of%20a%20pipeline%20together%20outperform%20directly%0Aoptimizing%20weights%20alone%20and%20prompts%20alone%20by%20up%20to%2060%25%20and%206%25%2C%20respectively%2C%0Aon%20average%20across%20LMs%20and%20tasks.%20BetterTogether%20optimizer%20is%20released%20in%20DSPy%0Aat%20http%3A//dspy.ai%0A%0ALink%3A%20http%3A//arxiv.org/abs/2407.10930v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DFine-Tuning%2520and%2520Prompt%2520Optimization%253A%2520Two%2520Great%2520Steps%2520that%2520Work%2520Better%250A%2520%2520Together%26entry.906535625%3DDilara%2520Soylu%2520and%2520Christopher%2520Potts%2520and%2520Omar%2520Khattab%26entry.1292438233%3D%2520%2520Natural%2520Language%2520Processing%2520%2528NLP%2529%2520systems%2520are%2520increasingly%2520taking%2520the%2520form%2520of%250Asophisticated%2520modular%2520pipelines%252C%2520e.g.%252C%2520Retrieval%2520Augmented%2520Generation%2520%2528RAG%2529%252C%250Awhere%2520each%2520module%2520may%2520involve%2520a%2520distinct%2520Language%2520Model%2520%2528LM%2529%2520and%2520an%2520associated%250Aprompt%2520template.%2520These%2520compound%2520systems%2520often%2520lack%2520intermediate%2520labels%2520or%250Agradient%2520flow%2520to%2520optimize%2520each%2520module%252C%2520making%2520their%2520end-to-end%2520optimization%250Achallenging.%2520Here%2520we%2520seek%2520strategies%2520to%2520optimize%2520both%2520the%2520module-level%2520LM%250Aweights%2520and%2520the%2520associated%2520prompt%2520templates%2520of%2520such%2520systems%2520to%2520maximize%2520a%250Adownstream%2520task%2520metric.%2520We%2520propose%2520for%2520the%2520first%2520time%2520combining%2520the%2520weight%2520and%250Aprompt%2520optimization%2520strategies%2520to%2520optimize%2520a%2520modular%2520LM%2520pipeline%2520by%2520alternating%250Abetween%2520the%2520two%2520to%2520get%2520the%2520same%2520LM%2520to%2520teach%2520itself.%2520In%2520experiments%2520with%250Amulti-hop%2520QA%252C%2520mathematical%2520reasoning%252C%2520and%2520feature-based%2520classification%2520using%250Amistral-7b%252C%2520llama-2-7b%252C%2520and%2520llama-3-8b%252C%2520these%2520BetterTogether%2520strategies%250Aoptimizing%2520the%2520weights%2520and%2520prompts%2520of%2520a%2520pipeline%2520together%2520outperform%2520directly%250Aoptimizing%2520weights%2520alone%2520and%2520prompts%2520alone%2520by%2520up%2520to%252060%2525%2520and%25206%2525%252C%2520respectively%252C%250Aon%2520average%2520across%2520LMs%2520and%2520tasks.%2520BetterTogether%2520optimizer%2520is%2520released%2520in%2520DSPy%250Aat%2520http%253A//dspy.ai%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2407.10930v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Fine-Tuning%20and%20Prompt%20Optimization%3A%20Two%20Great%20Steps%20that%20Work%20Better%0A%20%20Together&entry.906535625=Dilara%20Soylu%20and%20Christopher%20Potts%20and%20Omar%20Khattab&entry.1292438233=%20%20Natural%20Language%20Processing%20%28NLP%29%20systems%20are%20increasingly%20taking%20the%20form%20of%0Asophisticated%20modular%20pipelines%2C%20e.g.%2C%20Retrieval%20Augmented%20Generation%20%28RAG%29%2C%0Awhere%20each%20module%20may%20involve%20a%20distinct%20Language%20Model%20%28LM%29%20and%20an%20associated%0Aprompt%20template.%20These%20compound%20systems%20often%20lack%20intermediate%20labels%20or%0Agradient%20flow%20to%20optimize%20each%20module%2C%20making%20their%20end-to-end%20optimization%0Achallenging.%20Here%20we%20seek%20strategies%20to%20optimize%20both%20the%20module-level%20LM%0Aweights%20and%20the%20associated%20prompt%20templates%20of%20such%20systems%20to%20maximize%20a%0Adownstream%20task%20metric.%20We%20propose%20for%20the%20first%20time%20combining%20the%20weight%20and%0Aprompt%20optimization%20strategies%20to%20optimize%20a%20modular%20LM%20pipeline%20by%20alternating%0Abetween%20the%20two%20to%20get%20the%20same%20LM%20to%20teach%20itself.%20In%20experiments%20with%0Amulti-hop%20QA%2C%20mathematical%20reasoning%2C%20and%20feature-based%20classification%20using%0Amistral-7b%2C%20llama-2-7b%2C%20and%20llama-3-8b%2C%20these%20BetterTogether%20strategies%0Aoptimizing%20the%20weights%20and%20prompts%20of%20a%20pipeline%20together%20outperform%20directly%0Aoptimizing%20weights%20alone%20and%20prompts%20alone%20by%20up%20to%2060%25%20and%206%25%2C%20respectively%2C%0Aon%20average%20across%20LMs%20and%20tasks.%20BetterTogether%20optimizer%20is%20released%20in%20DSPy%0Aat%20http%3A//dspy.ai%0A&entry.1838667208=http%3A//arxiv.org/abs/2407.10930v2&entry.124074799=Read"},
{"title": "PrefixQuant: Static Quantization Beats Dynamic through Prefixed Outliers\n  in LLMs", "author": "Mengzhao Chen and Yi Liu and Jiahao Wang and Yi Bin and Wenqi Shao and Ping Luo", "abstract": "  Quantization is essential for deploying Large Language Models (LLMs) by\nenhancing memory efficiency and inference speed. Existing methods for\nactivation quantization mainly address channel-wise outliers, often neglecting\ntoken-wise outliers, leading to reliance on costly per-token dynamic\nquantization. To address this, we introduce PrefixQuant, a novel technique that\nisolates outlier tokens offline without re-training. Specifically, PrefixQuant\nidentifies high-frequency outlier tokens and prefixes them in the KV cache,\npreventing the generation of outlier tokens during inference and simplifying\nquantization. To our knowledge, PrefixQuant is the first to enable efficient\nper-tensor static quantization to outperform expensive per-token dynamic\nquantization. For instance, in W4A4KV4 (4- bit weight, 4-bit activation, and\n4-bit KV cache) Llama-3-8B, PrefixQuant with per-tensor static quantization\nachieves a 7.43 WikiText2 perplexity and 71.08% average accuracy on 5\ncommon-sense reasoning tasks, outperforming previous per-token dynamic\nquantization methods like QuaRot with 0.98 perplexity improvement and +5.98\npoints accuracy. Additionally, the inference speed of W4A4 quantized models\nusing PrefixQuant is 1.60x to 2.81x faster than FP16 models and exceeds QuaRot\nmodels by 1.2x to 1.3x. Our code is available at\n\\url{https://github.com/ChenMnZ/PrefixQuant}.\n", "link": "http://arxiv.org/abs/2410.05265v1", "date": "2024-10-07", "relevancy": 2.4373, "topK": [{"title": "ToDo: Token Downsampling for Efficient Generation of High-Resolution\n  Images", "link": "http://arxiv.org/abs/2402.13573v2", "similarity": 0.5072}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.4776}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.4776}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20PrefixQuant%3A%20Static%20Quantization%20Beats%20Dynamic%20through%20Prefixed%20Outliers%0A%20%20in%20LLMs&body=Title%3A%20PrefixQuant%3A%20Static%20Quantization%20Beats%20Dynamic%20through%20Prefixed%20Outliers%0A%20%20in%20LLMs%0AAuthor%3A%20Mengzhao%20Chen%20and%20Yi%20Liu%20and%20Jiahao%20Wang%20and%20Yi%20Bin%20and%20Wenqi%20Shao%20and%20Ping%20Luo%0AAbstract%3A%20%20%20Quantization%20is%20essential%20for%20deploying%20Large%20Language%20Models%20%28LLMs%29%20by%0Aenhancing%20memory%20efficiency%20and%20inference%20speed.%20Existing%20methods%20for%0Aactivation%20quantization%20mainly%20address%20channel-wise%20outliers%2C%20often%20neglecting%0Atoken-wise%20outliers%2C%20leading%20to%20reliance%20on%20costly%20per-token%20dynamic%0Aquantization.%20To%20address%20this%2C%20we%20introduce%20PrefixQuant%2C%20a%20novel%20technique%20that%0Aisolates%20outlier%20tokens%20offline%20without%20re-training.%20Specifically%2C%20PrefixQuant%0Aidentifies%20high-frequency%20outlier%20tokens%20and%20prefixes%20them%20in%20the%20KV%20cache%2C%0Apreventing%20the%20generation%20of%20outlier%20tokens%20during%20inference%20and%20simplifying%0Aquantization.%20To%20our%20knowledge%2C%20PrefixQuant%20is%20the%20first%20to%20enable%20efficient%0Aper-tensor%20static%20quantization%20to%20outperform%20expensive%20per-token%20dynamic%0Aquantization.%20For%20instance%2C%20in%20W4A4KV4%20%284-%20bit%20weight%2C%204-bit%20activation%2C%20and%0A4-bit%20KV%20cache%29%20Llama-3-8B%2C%20PrefixQuant%20with%20per-tensor%20static%20quantization%0Aachieves%20a%207.43%20WikiText2%20perplexity%20and%2071.08%25%20average%20accuracy%20on%205%0Acommon-sense%20reasoning%20tasks%2C%20outperforming%20previous%20per-token%20dynamic%0Aquantization%20methods%20like%20QuaRot%20with%200.98%20perplexity%20improvement%20and%20%2B5.98%0Apoints%20accuracy.%20Additionally%2C%20the%20inference%20speed%20of%20W4A4%20quantized%20models%0Ausing%20PrefixQuant%20is%201.60x%20to%202.81x%20faster%20than%20FP16%20models%20and%20exceeds%20QuaRot%0Amodels%20by%201.2x%20to%201.3x.%20Our%20code%20is%20available%20at%0A%5Curl%7Bhttps%3A//github.com/ChenMnZ/PrefixQuant%7D.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2410.05265v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DPrefixQuant%253A%2520Static%2520Quantization%2520Beats%2520Dynamic%2520through%2520Prefixed%2520Outliers%250A%2520%2520in%2520LLMs%26entry.906535625%3DMengzhao%2520Chen%2520and%2520Yi%2520Liu%2520and%2520Jiahao%2520Wang%2520and%2520Yi%2520Bin%2520and%2520Wenqi%2520Shao%2520and%2520Ping%2520Luo%26entry.1292438233%3D%2520%2520Quantization%2520is%2520essential%2520for%2520deploying%2520Large%2520Language%2520Models%2520%2528LLMs%2529%2520by%250Aenhancing%2520memory%2520efficiency%2520and%2520inference%2520speed.%2520Existing%2520methods%2520for%250Aactivation%2520quantization%2520mainly%2520address%2520channel-wise%2520outliers%252C%2520often%2520neglecting%250Atoken-wise%2520outliers%252C%2520leading%2520to%2520reliance%2520on%2520costly%2520per-token%2520dynamic%250Aquantization.%2520To%2520address%2520this%252C%2520we%2520introduce%2520PrefixQuant%252C%2520a%2520novel%2520technique%2520that%250Aisolates%2520outlier%2520tokens%2520offline%2520without%2520re-training.%2520Specifically%252C%2520PrefixQuant%250Aidentifies%2520high-frequency%2520outlier%2520tokens%2520and%2520prefixes%2520them%2520in%2520the%2520KV%2520cache%252C%250Apreventing%2520the%2520generation%2520of%2520outlier%2520tokens%2520during%2520inference%2520and%2520simplifying%250Aquantization.%2520To%2520our%2520knowledge%252C%2520PrefixQuant%2520is%2520the%2520first%2520to%2520enable%2520efficient%250Aper-tensor%2520static%2520quantization%2520to%2520outperform%2520expensive%2520per-token%2520dynamic%250Aquantization.%2520For%2520instance%252C%2520in%2520W4A4KV4%2520%25284-%2520bit%2520weight%252C%25204-bit%2520activation%252C%2520and%250A4-bit%2520KV%2520cache%2529%2520Llama-3-8B%252C%2520PrefixQuant%2520with%2520per-tensor%2520static%2520quantization%250Aachieves%2520a%25207.43%2520WikiText2%2520perplexity%2520and%252071.08%2525%2520average%2520accuracy%2520on%25205%250Acommon-sense%2520reasoning%2520tasks%252C%2520outperforming%2520previous%2520per-token%2520dynamic%250Aquantization%2520methods%2520like%2520QuaRot%2520with%25200.98%2520perplexity%2520improvement%2520and%2520%252B5.98%250Apoints%2520accuracy.%2520Additionally%252C%2520the%2520inference%2520speed%2520of%2520W4A4%2520quantized%2520models%250Ausing%2520PrefixQuant%2520is%25201.60x%2520to%25202.81x%2520faster%2520than%2520FP16%2520models%2520and%2520exceeds%2520QuaRot%250Amodels%2520by%25201.2x%2520to%25201.3x.%2520Our%2520code%2520is%2520available%2520at%250A%255Curl%257Bhttps%253A//github.com/ChenMnZ/PrefixQuant%257D.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2410.05265v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=PrefixQuant%3A%20Static%20Quantization%20Beats%20Dynamic%20through%20Prefixed%20Outliers%0A%20%20in%20LLMs&entry.906535625=Mengzhao%20Chen%20and%20Yi%20Liu%20and%20Jiahao%20Wang%20and%20Yi%20Bin%20and%20Wenqi%20Shao%20and%20Ping%20Luo&entry.1292438233=%20%20Quantization%20is%20essential%20for%20deploying%20Large%20Language%20Models%20%28LLMs%29%20by%0Aenhancing%20memory%20efficiency%20and%20inference%20speed.%20Existing%20methods%20for%0Aactivation%20quantization%20mainly%20address%20channel-wise%20outliers%2C%20often%20neglecting%0Atoken-wise%20outliers%2C%20leading%20to%20reliance%20on%20costly%20per-token%20dynamic%0Aquantization.%20To%20address%20this%2C%20we%20introduce%20PrefixQuant%2C%20a%20novel%20technique%20that%0Aisolates%20outlier%20tokens%20offline%20without%20re-training.%20Specifically%2C%20PrefixQuant%0Aidentifies%20high-frequency%20outlier%20tokens%20and%20prefixes%20them%20in%20the%20KV%20cache%2C%0Apreventing%20the%20generation%20of%20outlier%20tokens%20during%20inference%20and%20simplifying%0Aquantization.%20To%20our%20knowledge%2C%20PrefixQuant%20is%20the%20first%20to%20enable%20efficient%0Aper-tensor%20static%20quantization%20to%20outperform%20expensive%20per-token%20dynamic%0Aquantization.%20For%20instance%2C%20in%20W4A4KV4%20%284-%20bit%20weight%2C%204-bit%20activation%2C%20and%0A4-bit%20KV%20cache%29%20Llama-3-8B%2C%20PrefixQuant%20with%20per-tensor%20static%20quantization%0Aachieves%20a%207.43%20WikiText2%20perplexity%20and%2071.08%25%20average%20accuracy%20on%205%0Acommon-sense%20reasoning%20tasks%2C%20outperforming%20previous%20per-token%20dynamic%0Aquantization%20methods%20like%20QuaRot%20with%200.98%20perplexity%20improvement%20and%20%2B5.98%0Apoints%20accuracy.%20Additionally%2C%20the%20inference%20speed%20of%20W4A4%20quantized%20models%0Ausing%20PrefixQuant%20is%201.60x%20to%202.81x%20faster%20than%20FP16%20models%20and%20exceeds%20QuaRot%0Amodels%20by%201.2x%20to%201.3x.%20Our%20code%20is%20available%20at%0A%5Curl%7Bhttps%3A//github.com/ChenMnZ/PrefixQuant%7D.%0A&entry.1838667208=http%3A//arxiv.org/abs/2410.05265v1&entry.124074799=Read"},
{"title": "Enhanced Multi-Robot SLAM System with Cross-Validation Matching and\n  Exponential Threshold Keyframe Selection", "author": "Ang He and Xi-mei Wu and Xiao-bin Guo and Li-bin Liu", "abstract": "  The evolving field of mobile robotics has indeed increased the demand for\nsimultaneous localization and mapping (SLAM) systems. To augment the\nlocalization accuracy and mapping efficacy of SLAM, we refined the core module\nof the SLAM system. Within the feature matching phase, we introduced\ncross-validation matching to filter out mismatches. In the keyframe selection\nstrategy, an exponential threshold function is constructed to quantify the\nkeyframe selection process. Compared with a single robot, the multi-robot\ncollaborative SLAM (CSLAM) system substantially improves task execution\nefficiency and robustness. By employing a centralized structure, we formulate a\nmulti-robot SLAM system and design a coarse-to-fine matching approach for\nmulti-map point cloud registration. Our system, built upon ORB-SLAM3, underwent\nextensive evaluation utilizing the TUM RGB-D, EuRoC MAV, and TUM_VI datasets.\nThe experimental results demonstrate a significant improvement in the\npositioning accuracy and mapping quality of our enhanced algorithm compared to\nthose of ORB-SLAM3, with a 12.90% reduction in the absolute trajectory error.\n", "link": "http://arxiv.org/abs/2410.05017v1", "date": "2024-10-07", "relevancy": 2.4235, "topK": [{"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.6203}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.5987}, {"title": "RTG-SLAM: Real-time 3D Reconstruction at Scale using Gaussian Splatting", "link": "http://arxiv.org/abs/2404.19706v1", "similarity": 0.5944}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Enhanced%20Multi-Robot%20SLAM%20System%20with%20Cross-Validation%20Matching%20and%0A%20%20Exponential%20Threshold%20Keyframe%20Selection&body=Title%3A%20Enhanced%20Multi-Robot%20SLAM%20System%20with%20Cross-Validation%20Matching%20and%0A%20%20Exponential%20Threshold%20Keyframe%20Selection%0AAuthor%3A%20Ang%20He%20and%20Xi-mei%20Wu%20and%20Xiao-bin%20Guo%20and%20Li-bin%20Liu%0AAbstract%3A%20%20%20The%20evolving%20field%20of%20mobile%20robotics%20has%20indeed%20increased%20the%20demand%20for%0Asimultaneous%20localization%20and%20mapping%20%28SLAM%29%20systems.%20To%20augment%20the%0Alocalization%20accuracy%20and%20mapping%20efficacy%20of%20SLAM%2C%20we%20refined%20the%20core%20module%0Aof%20the%20SLAM%20system.%20Within%20the%20feature%20matching%20phase%2C%20we%20introduced%0Across-validation%20matching%20to%20filter%20out%20mismatches.%20In%20the%20keyframe%20selection%0Astrategy%2C%20an%20exponential%20threshold%20function%20is%20constructed%20to%20quantify%20the%0Akeyframe%20selection%20process.%20Compared%20with%20a%20single%20robot%2C%20the%20multi-robot%0Acollaborative%20SLAM%20%28CSLAM%29%20system%20substantially%20improves%20task%20execution%0Aefficiency%20and%20robustness.%20By%20employing%20a%20centralized%20structure%2C%20we%20formulate%20a%0Amulti-robot%20SLAM%20system%20and%20design%20a%20coarse-to-fine%20matching%20approach%20for%0Amulti-map%20point%20cloud%20registration.%20Our%20system%2C%20built%20upon%20ORB-SLAM3%2C%20underwent%0Aextensive%20evaluation%20utilizing%20the%20TUM%20RGB-D%2C%20EuRoC%20MAV%2C%20and%20TUM_VI%20datasets.%0AThe%20experimental%20results%20demonstrate%20a%20significant%20improvement%20in%20the%0Apositioning%20accuracy%20and%20mapping%20quality%20of%20our%20enhanced%20algorithm%20compared%20to%0Athose%20of%20ORB-SLAM3%2C%20with%20a%2012.90%25%20reduction%20in%20the%20absolute%20trajectory%20error.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2410.05017v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DEnhanced%2520Multi-Robot%2520SLAM%2520System%2520with%2520Cross-Validation%2520Matching%2520and%250A%2520%2520Exponential%2520Threshold%2520Keyframe%2520Selection%26entry.906535625%3DAng%2520He%2520and%2520Xi-mei%2520Wu%2520and%2520Xiao-bin%2520Guo%2520and%2520Li-bin%2520Liu%26entry.1292438233%3D%2520%2520The%2520evolving%2520field%2520of%2520mobile%2520robotics%2520has%2520indeed%2520increased%2520the%2520demand%2520for%250Asimultaneous%2520localization%2520and%2520mapping%2520%2528SLAM%2529%2520systems.%2520To%2520augment%2520the%250Alocalization%2520accuracy%2520and%2520mapping%2520efficacy%2520of%2520SLAM%252C%2520we%2520refined%2520the%2520core%2520module%250Aof%2520the%2520SLAM%2520system.%2520Within%2520the%2520feature%2520matching%2520phase%252C%2520we%2520introduced%250Across-validation%2520matching%2520to%2520filter%2520out%2520mismatches.%2520In%2520the%2520keyframe%2520selection%250Astrategy%252C%2520an%2520exponential%2520threshold%2520function%2520is%2520constructed%2520to%2520quantify%2520the%250Akeyframe%2520selection%2520process.%2520Compared%2520with%2520a%2520single%2520robot%252C%2520the%2520multi-robot%250Acollaborative%2520SLAM%2520%2528CSLAM%2529%2520system%2520substantially%2520improves%2520task%2520execution%250Aefficiency%2520and%2520robustness.%2520By%2520employing%2520a%2520centralized%2520structure%252C%2520we%2520formulate%2520a%250Amulti-robot%2520SLAM%2520system%2520and%2520design%2520a%2520coarse-to-fine%2520matching%2520approach%2520for%250Amulti-map%2520point%2520cloud%2520registration.%2520Our%2520system%252C%2520built%2520upon%2520ORB-SLAM3%252C%2520underwent%250Aextensive%2520evaluation%2520utilizing%2520the%2520TUM%2520RGB-D%252C%2520EuRoC%2520MAV%252C%2520and%2520TUM_VI%2520datasets.%250AThe%2520experimental%2520results%2520demonstrate%2520a%2520significant%2520improvement%2520in%2520the%250Apositioning%2520accuracy%2520and%2520mapping%2520quality%2520of%2520our%2520enhanced%2520algorithm%2520compared%2520to%250Athose%2520of%2520ORB-SLAM3%252C%2520with%2520a%252012.90%2525%2520reduction%2520in%2520the%2520absolute%2520trajectory%2520error.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2410.05017v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Enhanced%20Multi-Robot%20SLAM%20System%20with%20Cross-Validation%20Matching%20and%0A%20%20Exponential%20Threshold%20Keyframe%20Selection&entry.906535625=Ang%20He%20and%20Xi-mei%20Wu%20and%20Xiao-bin%20Guo%20and%20Li-bin%20Liu&entry.1292438233=%20%20The%20evolving%20field%20of%20mobile%20robotics%20has%20indeed%20increased%20the%20demand%20for%0Asimultaneous%20localization%20and%20mapping%20%28SLAM%29%20systems.%20To%20augment%20the%0Alocalization%20accuracy%20and%20mapping%20efficacy%20of%20SLAM%2C%20we%20refined%20the%20core%20module%0Aof%20the%20SLAM%20system.%20Within%20the%20feature%20matching%20phase%2C%20we%20introduced%0Across-validation%20matching%20to%20filter%20out%20mismatches.%20In%20the%20keyframe%20selection%0Astrategy%2C%20an%20exponential%20threshold%20function%20is%20constructed%20to%20quantify%20the%0Akeyframe%20selection%20process.%20Compared%20with%20a%20single%20robot%2C%20the%20multi-robot%0Acollaborative%20SLAM%20%28CSLAM%29%20system%20substantially%20improves%20task%20execution%0Aefficiency%20and%20robustness.%20By%20employing%20a%20centralized%20structure%2C%20we%20formulate%20a%0Amulti-robot%20SLAM%20system%20and%20design%20a%20coarse-to-fine%20matching%20approach%20for%0Amulti-map%20point%20cloud%20registration.%20Our%20system%2C%20built%20upon%20ORB-SLAM3%2C%20underwent%0Aextensive%20evaluation%20utilizing%20the%20TUM%20RGB-D%2C%20EuRoC%20MAV%2C%20and%20TUM_VI%20datasets.%0AThe%20experimental%20results%20demonstrate%20a%20significant%20improvement%20in%20the%0Apositioning%20accuracy%20and%20mapping%20quality%20of%20our%20enhanced%20algorithm%20compared%20to%0Athose%20of%20ORB-SLAM3%2C%20with%20a%2012.90%25%20reduction%20in%20the%20absolute%20trajectory%20error.%0A&entry.1838667208=http%3A//arxiv.org/abs/2410.05017v1&entry.124074799=Read"},
{"title": "Studying and Mitigating Biases in Sign Language Understanding Models", "author": "Katherine Atwell and Danielle Bragg and Malihe Alikhani", "abstract": "  Ensuring that the benefits of sign language technologies are distributed\nequitably among all community members is crucial. Thus, it is important to\naddress potential biases and inequities that may arise from the design or use\nof these resources. Crowd-sourced sign language datasets, such as the ASL\nCitizen dataset, are great resources for improving accessibility and preserving\nlinguistic diversity, but they must be used thoughtfully to avoid reinforcing\nexisting biases.\n  In this work, we utilize the rich information about participant demographics\nand lexical features present in the ASL Citizen dataset to study and document\nthe biases that may result from models trained on crowd-sourced sign datasets.\nFurther, we apply several bias mitigation techniques during model training, and\nfind that these techniques reduce performance disparities without decreasing\naccuracy. With the publication of this work, we release the demographic\ninformation about the participants in the ASL Citizen dataset to encourage\nfuture bias mitigation work in this space.\n", "link": "http://arxiv.org/abs/2410.05206v1", "date": "2024-10-07", "relevancy": 2.4064, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.4836}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.4836}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.4767}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Studying%20and%20Mitigating%20Biases%20in%20Sign%20Language%20Understanding%20Models&body=Title%3A%20Studying%20and%20Mitigating%20Biases%20in%20Sign%20Language%20Understanding%20Models%0AAuthor%3A%20Katherine%20Atwell%20and%20Danielle%20Bragg%20and%20Malihe%20Alikhani%0AAbstract%3A%20%20%20Ensuring%20that%20the%20benefits%20of%20sign%20language%20technologies%20are%20distributed%0Aequitably%20among%20all%20community%20members%20is%20crucial.%20Thus%2C%20it%20is%20important%20to%0Aaddress%20potential%20biases%20and%20inequities%20that%20may%20arise%20from%20the%20design%20or%20use%0Aof%20these%20resources.%20Crowd-sourced%20sign%20language%20datasets%2C%20such%20as%20the%20ASL%0ACitizen%20dataset%2C%20are%20great%20resources%20for%20improving%20accessibility%20and%20preserving%0Alinguistic%20diversity%2C%20but%20they%20must%20be%20used%20thoughtfully%20to%20avoid%20reinforcing%0Aexisting%20biases.%0A%20%20In%20this%20work%2C%20we%20utilize%20the%20rich%20information%20about%20participant%20demographics%0Aand%20lexical%20features%20present%20in%20the%20ASL%20Citizen%20dataset%20to%20study%20and%20document%0Athe%20biases%20that%20may%20result%20from%20models%20trained%20on%20crowd-sourced%20sign%20datasets.%0AFurther%2C%20we%20apply%20several%20bias%20mitigation%20techniques%20during%20model%20training%2C%20and%0Afind%20that%20these%20techniques%20reduce%20performance%20disparities%20without%20decreasing%0Aaccuracy.%20With%20the%20publication%20of%20this%20work%2C%20we%20release%20the%20demographic%0Ainformation%20about%20the%20participants%20in%20the%20ASL%20Citizen%20dataset%20to%20encourage%0Afuture%20bias%20mitigation%20work%20in%20this%20space.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2410.05206v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DStudying%2520and%2520Mitigating%2520Biases%2520in%2520Sign%2520Language%2520Understanding%2520Models%26entry.906535625%3DKatherine%2520Atwell%2520and%2520Danielle%2520Bragg%2520and%2520Malihe%2520Alikhani%26entry.1292438233%3D%2520%2520Ensuring%2520that%2520the%2520benefits%2520of%2520sign%2520language%2520technologies%2520are%2520distributed%250Aequitably%2520among%2520all%2520community%2520members%2520is%2520crucial.%2520Thus%252C%2520it%2520is%2520important%2520to%250Aaddress%2520potential%2520biases%2520and%2520inequities%2520that%2520may%2520arise%2520from%2520the%2520design%2520or%2520use%250Aof%2520these%2520resources.%2520Crowd-sourced%2520sign%2520language%2520datasets%252C%2520such%2520as%2520the%2520ASL%250ACitizen%2520dataset%252C%2520are%2520great%2520resources%2520for%2520improving%2520accessibility%2520and%2520preserving%250Alinguistic%2520diversity%252C%2520but%2520they%2520must%2520be%2520used%2520thoughtfully%2520to%2520avoid%2520reinforcing%250Aexisting%2520biases.%250A%2520%2520In%2520this%2520work%252C%2520we%2520utilize%2520the%2520rich%2520information%2520about%2520participant%2520demographics%250Aand%2520lexical%2520features%2520present%2520in%2520the%2520ASL%2520Citizen%2520dataset%2520to%2520study%2520and%2520document%250Athe%2520biases%2520that%2520may%2520result%2520from%2520models%2520trained%2520on%2520crowd-sourced%2520sign%2520datasets.%250AFurther%252C%2520we%2520apply%2520several%2520bias%2520mitigation%2520techniques%2520during%2520model%2520training%252C%2520and%250Afind%2520that%2520these%2520techniques%2520reduce%2520performance%2520disparities%2520without%2520decreasing%250Aaccuracy.%2520With%2520the%2520publication%2520of%2520this%2520work%252C%2520we%2520release%2520the%2520demographic%250Ainformation%2520about%2520the%2520participants%2520in%2520the%2520ASL%2520Citizen%2520dataset%2520to%2520encourage%250Afuture%2520bias%2520mitigation%2520work%2520in%2520this%2520space.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2410.05206v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Studying%20and%20Mitigating%20Biases%20in%20Sign%20Language%20Understanding%20Models&entry.906535625=Katherine%20Atwell%20and%20Danielle%20Bragg%20and%20Malihe%20Alikhani&entry.1292438233=%20%20Ensuring%20that%20the%20benefits%20of%20sign%20language%20technologies%20are%20distributed%0Aequitably%20among%20all%20community%20members%20is%20crucial.%20Thus%2C%20it%20is%20important%20to%0Aaddress%20potential%20biases%20and%20inequities%20that%20may%20arise%20from%20the%20design%20or%20use%0Aof%20these%20resources.%20Crowd-sourced%20sign%20language%20datasets%2C%20such%20as%20the%20ASL%0ACitizen%20dataset%2C%20are%20great%20resources%20for%20improving%20accessibility%20and%20preserving%0Alinguistic%20diversity%2C%20but%20they%20must%20be%20used%20thoughtfully%20to%20avoid%20reinforcing%0Aexisting%20biases.%0A%20%20In%20this%20work%2C%20we%20utilize%20the%20rich%20information%20about%20participant%20demographics%0Aand%20lexical%20features%20present%20in%20the%20ASL%20Citizen%20dataset%20to%20study%20and%20document%0Athe%20biases%20that%20may%20result%20from%20models%20trained%20on%20crowd-sourced%20sign%20datasets.%0AFurther%2C%20we%20apply%20several%20bias%20mitigation%20techniques%20during%20model%20training%2C%20and%0Afind%20that%20these%20techniques%20reduce%20performance%20disparities%20without%20decreasing%0Aaccuracy.%20With%20the%20publication%20of%20this%20work%2C%20we%20release%20the%20demographic%0Ainformation%20about%20the%20participants%20in%20the%20ASL%20Citizen%20dataset%20to%20encourage%0Afuture%20bias%20mitigation%20work%20in%20this%20space.%0A&entry.1838667208=http%3A//arxiv.org/abs/2410.05206v1&entry.124074799=Read"},
{"title": "Improving Object Detection via Local-global Contrastive Learning", "author": "Danai Triantafyllidou and Sarah Parisot and Ales Leonardis and Steven McDonagh", "abstract": "  Visual domain gaps often impact object detection performance. Image-to-image\ntranslation can mitigate this effect, where contrastive approaches enable\nlearning of the image-to-image mapping under unsupervised regimes. However,\nexisting methods often fail to handle content-rich scenes with multiple object\ninstances, which manifests in unsatisfactory detection performance. Sensitivity\nto such instance-level content is typically only gained through object\nannotations, which can be expensive to obtain. Towards addressing this issue,\nwe present a novel image-to-image translation method that specifically targets\ncross-domain object detection. We formulate our approach as a contrastive\nlearning framework with an inductive prior that optimises the appearance of\nobject instances through spatial attention masks, implicitly delineating the\nscene into foreground regions associated with the target object instances and\nbackground non-object regions. Instead of relying on object annotations to\nexplicitly account for object instances during translation, our approach learns\nto represent objects by contrasting local-global information. This affords\ninvestigation of an under-explored challenge: obtaining performant detection,\nunder domain shifts, without relying on object annotations nor detector model\nfine-tuning. We experiment with multiple cross-domain object detection settings\nacross three challenging benchmarks and report state-of-the-art performance.\nProject page: https://local-global-detection.github.io\n", "link": "http://arxiv.org/abs/2410.05058v1", "date": "2024-10-07", "relevancy": 2.4023, "topK": [{"title": "DDM-NET: End-to-end learning of keypoint feature Detection, Description and Matching for 3D localization", "link": "https://arxiv.org/abs/2212.04575", "similarity": 0.6307}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5792}, {"title": "Thinking Outside the BBox: Unconstrained Generative Object Compositing", "link": "http://arxiv.org/abs/2409.04559v2", "similarity": 0.5787}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Improving%20Object%20Detection%20via%20Local-global%20Contrastive%20Learning&body=Title%3A%20Improving%20Object%20Detection%20via%20Local-global%20Contrastive%20Learning%0AAuthor%3A%20Danai%20Triantafyllidou%20and%20Sarah%20Parisot%20and%20Ales%20Leonardis%20and%20Steven%20McDonagh%0AAbstract%3A%20%20%20Visual%20domain%20gaps%20often%20impact%20object%20detection%20performance.%20Image-to-image%0Atranslation%20can%20mitigate%20this%20effect%2C%20where%20contrastive%20approaches%20enable%0Alearning%20of%20the%20image-to-image%20mapping%20under%20unsupervised%20regimes.%20However%2C%0Aexisting%20methods%20often%20fail%20to%20handle%20content-rich%20scenes%20with%20multiple%20object%0Ainstances%2C%20which%20manifests%20in%20unsatisfactory%20detection%20performance.%20Sensitivity%0Ato%20such%20instance-level%20content%20is%20typically%20only%20gained%20through%20object%0Aannotations%2C%20which%20can%20be%20expensive%20to%20obtain.%20Towards%20addressing%20this%20issue%2C%0Awe%20present%20a%20novel%20image-to-image%20translation%20method%20that%20specifically%20targets%0Across-domain%20object%20detection.%20We%20formulate%20our%20approach%20as%20a%20contrastive%0Alearning%20framework%20with%20an%20inductive%20prior%20that%20optimises%20the%20appearance%20of%0Aobject%20instances%20through%20spatial%20attention%20masks%2C%20implicitly%20delineating%20the%0Ascene%20into%20foreground%20regions%20associated%20with%20the%20target%20object%20instances%20and%0Abackground%20non-object%20regions.%20Instead%20of%20relying%20on%20object%20annotations%20to%0Aexplicitly%20account%20for%20object%20instances%20during%20translation%2C%20our%20approach%20learns%0Ato%20represent%20objects%20by%20contrasting%20local-global%20information.%20This%20affords%0Ainvestigation%20of%20an%20under-explored%20challenge%3A%20obtaining%20performant%20detection%2C%0Aunder%20domain%20shifts%2C%20without%20relying%20on%20object%20annotations%20nor%20detector%20model%0Afine-tuning.%20We%20experiment%20with%20multiple%20cross-domain%20object%20detection%20settings%0Aacross%20three%20challenging%20benchmarks%20and%20report%20state-of-the-art%20performance.%0AProject%20page%3A%20https%3A//local-global-detection.github.io%0A%0ALink%3A%20http%3A//arxiv.org/abs/2410.05058v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DImproving%2520Object%2520Detection%2520via%2520Local-global%2520Contrastive%2520Learning%26entry.906535625%3DDanai%2520Triantafyllidou%2520and%2520Sarah%2520Parisot%2520and%2520Ales%2520Leonardis%2520and%2520Steven%2520McDonagh%26entry.1292438233%3D%2520%2520Visual%2520domain%2520gaps%2520often%2520impact%2520object%2520detection%2520performance.%2520Image-to-image%250Atranslation%2520can%2520mitigate%2520this%2520effect%252C%2520where%2520contrastive%2520approaches%2520enable%250Alearning%2520of%2520the%2520image-to-image%2520mapping%2520under%2520unsupervised%2520regimes.%2520However%252C%250Aexisting%2520methods%2520often%2520fail%2520to%2520handle%2520content-rich%2520scenes%2520with%2520multiple%2520object%250Ainstances%252C%2520which%2520manifests%2520in%2520unsatisfactory%2520detection%2520performance.%2520Sensitivity%250Ato%2520such%2520instance-level%2520content%2520is%2520typically%2520only%2520gained%2520through%2520object%250Aannotations%252C%2520which%2520can%2520be%2520expensive%2520to%2520obtain.%2520Towards%2520addressing%2520this%2520issue%252C%250Awe%2520present%2520a%2520novel%2520image-to-image%2520translation%2520method%2520that%2520specifically%2520targets%250Across-domain%2520object%2520detection.%2520We%2520formulate%2520our%2520approach%2520as%2520a%2520contrastive%250Alearning%2520framework%2520with%2520an%2520inductive%2520prior%2520that%2520optimises%2520the%2520appearance%2520of%250Aobject%2520instances%2520through%2520spatial%2520attention%2520masks%252C%2520implicitly%2520delineating%2520the%250Ascene%2520into%2520foreground%2520regions%2520associated%2520with%2520the%2520target%2520object%2520instances%2520and%250Abackground%2520non-object%2520regions.%2520Instead%2520of%2520relying%2520on%2520object%2520annotations%2520to%250Aexplicitly%2520account%2520for%2520object%2520instances%2520during%2520translation%252C%2520our%2520approach%2520learns%250Ato%2520represent%2520objects%2520by%2520contrasting%2520local-global%2520information.%2520This%2520affords%250Ainvestigation%2520of%2520an%2520under-explored%2520challenge%253A%2520obtaining%2520performant%2520detection%252C%250Aunder%2520domain%2520shifts%252C%2520without%2520relying%2520on%2520object%2520annotations%2520nor%2520detector%2520model%250Afine-tuning.%2520We%2520experiment%2520with%2520multiple%2520cross-domain%2520object%2520detection%2520settings%250Aacross%2520three%2520challenging%2520benchmarks%2520and%2520report%2520state-of-the-art%2520performance.%250AProject%2520page%253A%2520https%253A//local-global-detection.github.io%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2410.05058v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Improving%20Object%20Detection%20via%20Local-global%20Contrastive%20Learning&entry.906535625=Danai%20Triantafyllidou%20and%20Sarah%20Parisot%20and%20Ales%20Leonardis%20and%20Steven%20McDonagh&entry.1292438233=%20%20Visual%20domain%20gaps%20often%20impact%20object%20detection%20performance.%20Image-to-image%0Atranslation%20can%20mitigate%20this%20effect%2C%20where%20contrastive%20approaches%20enable%0Alearning%20of%20the%20image-to-image%20mapping%20under%20unsupervised%20regimes.%20However%2C%0Aexisting%20methods%20often%20fail%20to%20handle%20content-rich%20scenes%20with%20multiple%20object%0Ainstances%2C%20which%20manifests%20in%20unsatisfactory%20detection%20performance.%20Sensitivity%0Ato%20such%20instance-level%20content%20is%20typically%20only%20gained%20through%20object%0Aannotations%2C%20which%20can%20be%20expensive%20to%20obtain.%20Towards%20addressing%20this%20issue%2C%0Awe%20present%20a%20novel%20image-to-image%20translation%20method%20that%20specifically%20targets%0Across-domain%20object%20detection.%20We%20formulate%20our%20approach%20as%20a%20contrastive%0Alearning%20framework%20with%20an%20inductive%20prior%20that%20optimises%20the%20appearance%20of%0Aobject%20instances%20through%20spatial%20attention%20masks%2C%20implicitly%20delineating%20the%0Ascene%20into%20foreground%20regions%20associated%20with%20the%20target%20object%20instances%20and%0Abackground%20non-object%20regions.%20Instead%20of%20relying%20on%20object%20annotations%20to%0Aexplicitly%20account%20for%20object%20instances%20during%20translation%2C%20our%20approach%20learns%0Ato%20represent%20objects%20by%20contrasting%20local-global%20information.%20This%20affords%0Ainvestigation%20of%20an%20under-explored%20challenge%3A%20obtaining%20performant%20detection%2C%0Aunder%20domain%20shifts%2C%20without%20relying%20on%20object%20annotations%20nor%20detector%20model%0Afine-tuning.%20We%20experiment%20with%20multiple%20cross-domain%20object%20detection%20settings%0Aacross%20three%20challenging%20benchmarks%20and%20report%20state-of-the-art%20performance.%0AProject%20page%3A%20https%3A//local-global-detection.github.io%0A&entry.1838667208=http%3A//arxiv.org/abs/2410.05058v1&entry.124074799=Read"},
{"title": "RelUNet: Relative Channel Fusion U-Net for Multichannel Speech\n  Enhancement", "author": "Ibrahim Aldarmaki and Thamar Solorio and Bhiksha Raj and Hanan Aldarmaki", "abstract": "  Neural multi-channel speech enhancement models, in particular those based on\nthe U-Net architecture, demonstrate promising performance and generalization\npotential. These models typically encode input channels independently, and\nintegrate the channels during later stages of the network. In this paper, we\npropose a novel modification of these models by incorporating relative\ninformation from the outset, where each channel is processed in conjunction\nwith a reference channel through stacking. This input strategy exploits\ncomparative differences to adaptively fuse information between channels,\nthereby capturing crucial spatial information and enhancing the overall\nperformance. The experiments conducted on the CHiME-3 dataset demonstrate\nimprovements in speech enhancement metrics across various architectures.\n", "link": "http://arxiv.org/abs/2410.05019v1", "date": "2024-10-07", "relevancy": 2.3994, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.4821}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.4821}, {"title": "Boosted dynamic neural networks", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/26302", "similarity": 0.4754}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20RelUNet%3A%20Relative%20Channel%20Fusion%20U-Net%20for%20Multichannel%20Speech%0A%20%20Enhancement&body=Title%3A%20RelUNet%3A%20Relative%20Channel%20Fusion%20U-Net%20for%20Multichannel%20Speech%0A%20%20Enhancement%0AAuthor%3A%20Ibrahim%20Aldarmaki%20and%20Thamar%20Solorio%20and%20Bhiksha%20Raj%20and%20Hanan%20Aldarmaki%0AAbstract%3A%20%20%20Neural%20multi-channel%20speech%20enhancement%20models%2C%20in%20particular%20those%20based%20on%0Athe%20U-Net%20architecture%2C%20demonstrate%20promising%20performance%20and%20generalization%0Apotential.%20These%20models%20typically%20encode%20input%20channels%20independently%2C%20and%0Aintegrate%20the%20channels%20during%20later%20stages%20of%20the%20network.%20In%20this%20paper%2C%20we%0Apropose%20a%20novel%20modification%20of%20these%20models%20by%20incorporating%20relative%0Ainformation%20from%20the%20outset%2C%20where%20each%20channel%20is%20processed%20in%20conjunction%0Awith%20a%20reference%20channel%20through%20stacking.%20This%20input%20strategy%20exploits%0Acomparative%20differences%20to%20adaptively%20fuse%20information%20between%20channels%2C%0Athereby%20capturing%20crucial%20spatial%20information%20and%20enhancing%20the%20overall%0Aperformance.%20The%20experiments%20conducted%20on%20the%20CHiME-3%20dataset%20demonstrate%0Aimprovements%20in%20speech%20enhancement%20metrics%20across%20various%20architectures.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2410.05019v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DRelUNet%253A%2520Relative%2520Channel%2520Fusion%2520U-Net%2520for%2520Multichannel%2520Speech%250A%2520%2520Enhancement%26entry.906535625%3DIbrahim%2520Aldarmaki%2520and%2520Thamar%2520Solorio%2520and%2520Bhiksha%2520Raj%2520and%2520Hanan%2520Aldarmaki%26entry.1292438233%3D%2520%2520Neural%2520multi-channel%2520speech%2520enhancement%2520models%252C%2520in%2520particular%2520those%2520based%2520on%250Athe%2520U-Net%2520architecture%252C%2520demonstrate%2520promising%2520performance%2520and%2520generalization%250Apotential.%2520These%2520models%2520typically%2520encode%2520input%2520channels%2520independently%252C%2520and%250Aintegrate%2520the%2520channels%2520during%2520later%2520stages%2520of%2520the%2520network.%2520In%2520this%2520paper%252C%2520we%250Apropose%2520a%2520novel%2520modification%2520of%2520these%2520models%2520by%2520incorporating%2520relative%250Ainformation%2520from%2520the%2520outset%252C%2520where%2520each%2520channel%2520is%2520processed%2520in%2520conjunction%250Awith%2520a%2520reference%2520channel%2520through%2520stacking.%2520This%2520input%2520strategy%2520exploits%250Acomparative%2520differences%2520to%2520adaptively%2520fuse%2520information%2520between%2520channels%252C%250Athereby%2520capturing%2520crucial%2520spatial%2520information%2520and%2520enhancing%2520the%2520overall%250Aperformance.%2520The%2520experiments%2520conducted%2520on%2520the%2520CHiME-3%2520dataset%2520demonstrate%250Aimprovements%2520in%2520speech%2520enhancement%2520metrics%2520across%2520various%2520architectures.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2410.05019v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=RelUNet%3A%20Relative%20Channel%20Fusion%20U-Net%20for%20Multichannel%20Speech%0A%20%20Enhancement&entry.906535625=Ibrahim%20Aldarmaki%20and%20Thamar%20Solorio%20and%20Bhiksha%20Raj%20and%20Hanan%20Aldarmaki&entry.1292438233=%20%20Neural%20multi-channel%20speech%20enhancement%20models%2C%20in%20particular%20those%20based%20on%0Athe%20U-Net%20architecture%2C%20demonstrate%20promising%20performance%20and%20generalization%0Apotential.%20These%20models%20typically%20encode%20input%20channels%20independently%2C%20and%0Aintegrate%20the%20channels%20during%20later%20stages%20of%20the%20network.%20In%20this%20paper%2C%20we%0Apropose%20a%20novel%20modification%20of%20these%20models%20by%20incorporating%20relative%0Ainformation%20from%20the%20outset%2C%20where%20each%20channel%20is%20processed%20in%20conjunction%0Awith%20a%20reference%20channel%20through%20stacking.%20This%20input%20strategy%20exploits%0Acomparative%20differences%20to%20adaptively%20fuse%20information%20between%20channels%2C%0Athereby%20capturing%20crucial%20spatial%20information%20and%20enhancing%20the%20overall%0Aperformance.%20The%20experiments%20conducted%20on%20the%20CHiME-3%20dataset%20demonstrate%0Aimprovements%20in%20speech%20enhancement%20metrics%20across%20various%20architectures.%0A&entry.1838667208=http%3A//arxiv.org/abs/2410.05019v1&entry.124074799=Read"},
{"title": "SELECT: A Large-Scale Benchmark of Data Curation Strategies for Image\n  Classification", "author": "Benjamin Feuer and Jiawei Xu and Niv Cohen and Patrick Yubeaton and Govind Mittal and Chinmay Hegde", "abstract": "  Data curation is the problem of how to collect and organize samples into a\ndataset that supports efficient learning. Despite the centrality of the task,\nlittle work has been devoted towards a large-scale, systematic comparison of\nvarious curation methods. In this work, we take steps towards a formal\nevaluation of data curation strategies and introduce SELECT, the first\nlarge-scale benchmark of curation strategies for image classification.\n  In order to generate baseline methods for the SELECT benchmark, we create a\nnew dataset, ImageNet++, which constitutes the largest superset of ImageNet-1K\nto date. Our dataset extends ImageNet with 5 new training-data shifts, each\napproximately the size of ImageNet-1K itself, and each assembled using a\ndistinct curation strategy. We evaluate our data curation baselines in two\nways: (i) using each training-data shift to train identical image\nclassification models from scratch (ii) using the data itself to fit a\npretrained self-supervised representation.\n  Our findings show interesting trends, particularly pertaining to recent\nmethods for data curation such as synthetic data generation and lookup based on\nCLIP embeddings. We show that although these strategies are highly competitive\nfor certain tasks, the curation strategy used to assemble the original\nImageNet-1K dataset remains the gold standard. We anticipate that our benchmark\ncan illuminate the path for new methods to further reduce the gap. We release\nour checkpoints, code, documentation, and a link to our dataset at\nhttps://github.com/jimmyxu123/SELECT.\n", "link": "http://arxiv.org/abs/2410.05057v1", "date": "2024-10-07", "relevancy": 2.3977, "topK": [{"title": "Self-supervised Photographic Image Layout Representation Learning", "link": "http://arxiv.org/abs/2403.03740v1", "similarity": 0.5022}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.4682}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.4682}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20SELECT%3A%20A%20Large-Scale%20Benchmark%20of%20Data%20Curation%20Strategies%20for%20Image%0A%20%20Classification&body=Title%3A%20SELECT%3A%20A%20Large-Scale%20Benchmark%20of%20Data%20Curation%20Strategies%20for%20Image%0A%20%20Classification%0AAuthor%3A%20Benjamin%20Feuer%20and%20Jiawei%20Xu%20and%20Niv%20Cohen%20and%20Patrick%20Yubeaton%20and%20Govind%20Mittal%20and%20Chinmay%20Hegde%0AAbstract%3A%20%20%20Data%20curation%20is%20the%20problem%20of%20how%20to%20collect%20and%20organize%20samples%20into%20a%0Adataset%20that%20supports%20efficient%20learning.%20Despite%20the%20centrality%20of%20the%20task%2C%0Alittle%20work%20has%20been%20devoted%20towards%20a%20large-scale%2C%20systematic%20comparison%20of%0Avarious%20curation%20methods.%20In%20this%20work%2C%20we%20take%20steps%20towards%20a%20formal%0Aevaluation%20of%20data%20curation%20strategies%20and%20introduce%20SELECT%2C%20the%20first%0Alarge-scale%20benchmark%20of%20curation%20strategies%20for%20image%20classification.%0A%20%20In%20order%20to%20generate%20baseline%20methods%20for%20the%20SELECT%20benchmark%2C%20we%20create%20a%0Anew%20dataset%2C%20ImageNet%2B%2B%2C%20which%20constitutes%20the%20largest%20superset%20of%20ImageNet-1K%0Ato%20date.%20Our%20dataset%20extends%20ImageNet%20with%205%20new%20training-data%20shifts%2C%20each%0Aapproximately%20the%20size%20of%20ImageNet-1K%20itself%2C%20and%20each%20assembled%20using%20a%0Adistinct%20curation%20strategy.%20We%20evaluate%20our%20data%20curation%20baselines%20in%20two%0Aways%3A%20%28i%29%20using%20each%20training-data%20shift%20to%20train%20identical%20image%0Aclassification%20models%20from%20scratch%20%28ii%29%20using%20the%20data%20itself%20to%20fit%20a%0Apretrained%20self-supervised%20representation.%0A%20%20Our%20findings%20show%20interesting%20trends%2C%20particularly%20pertaining%20to%20recent%0Amethods%20for%20data%20curation%20such%20as%20synthetic%20data%20generation%20and%20lookup%20based%20on%0ACLIP%20embeddings.%20We%20show%20that%20although%20these%20strategies%20are%20highly%20competitive%0Afor%20certain%20tasks%2C%20the%20curation%20strategy%20used%20to%20assemble%20the%20original%0AImageNet-1K%20dataset%20remains%20the%20gold%20standard.%20We%20anticipate%20that%20our%20benchmark%0Acan%20illuminate%20the%20path%20for%20new%20methods%20to%20further%20reduce%20the%20gap.%20We%20release%0Aour%20checkpoints%2C%20code%2C%20documentation%2C%20and%20a%20link%20to%20our%20dataset%20at%0Ahttps%3A//github.com/jimmyxu123/SELECT.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2410.05057v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DSELECT%253A%2520A%2520Large-Scale%2520Benchmark%2520of%2520Data%2520Curation%2520Strategies%2520for%2520Image%250A%2520%2520Classification%26entry.906535625%3DBenjamin%2520Feuer%2520and%2520Jiawei%2520Xu%2520and%2520Niv%2520Cohen%2520and%2520Patrick%2520Yubeaton%2520and%2520Govind%2520Mittal%2520and%2520Chinmay%2520Hegde%26entry.1292438233%3D%2520%2520Data%2520curation%2520is%2520the%2520problem%2520of%2520how%2520to%2520collect%2520and%2520organize%2520samples%2520into%2520a%250Adataset%2520that%2520supports%2520efficient%2520learning.%2520Despite%2520the%2520centrality%2520of%2520the%2520task%252C%250Alittle%2520work%2520has%2520been%2520devoted%2520towards%2520a%2520large-scale%252C%2520systematic%2520comparison%2520of%250Avarious%2520curation%2520methods.%2520In%2520this%2520work%252C%2520we%2520take%2520steps%2520towards%2520a%2520formal%250Aevaluation%2520of%2520data%2520curation%2520strategies%2520and%2520introduce%2520SELECT%252C%2520the%2520first%250Alarge-scale%2520benchmark%2520of%2520curation%2520strategies%2520for%2520image%2520classification.%250A%2520%2520In%2520order%2520to%2520generate%2520baseline%2520methods%2520for%2520the%2520SELECT%2520benchmark%252C%2520we%2520create%2520a%250Anew%2520dataset%252C%2520ImageNet%252B%252B%252C%2520which%2520constitutes%2520the%2520largest%2520superset%2520of%2520ImageNet-1K%250Ato%2520date.%2520Our%2520dataset%2520extends%2520ImageNet%2520with%25205%2520new%2520training-data%2520shifts%252C%2520each%250Aapproximately%2520the%2520size%2520of%2520ImageNet-1K%2520itself%252C%2520and%2520each%2520assembled%2520using%2520a%250Adistinct%2520curation%2520strategy.%2520We%2520evaluate%2520our%2520data%2520curation%2520baselines%2520in%2520two%250Aways%253A%2520%2528i%2529%2520using%2520each%2520training-data%2520shift%2520to%2520train%2520identical%2520image%250Aclassification%2520models%2520from%2520scratch%2520%2528ii%2529%2520using%2520the%2520data%2520itself%2520to%2520fit%2520a%250Apretrained%2520self-supervised%2520representation.%250A%2520%2520Our%2520findings%2520show%2520interesting%2520trends%252C%2520particularly%2520pertaining%2520to%2520recent%250Amethods%2520for%2520data%2520curation%2520such%2520as%2520synthetic%2520data%2520generation%2520and%2520lookup%2520based%2520on%250ACLIP%2520embeddings.%2520We%2520show%2520that%2520although%2520these%2520strategies%2520are%2520highly%2520competitive%250Afor%2520certain%2520tasks%252C%2520the%2520curation%2520strategy%2520used%2520to%2520assemble%2520the%2520original%250AImageNet-1K%2520dataset%2520remains%2520the%2520gold%2520standard.%2520We%2520anticipate%2520that%2520our%2520benchmark%250Acan%2520illuminate%2520the%2520path%2520for%2520new%2520methods%2520to%2520further%2520reduce%2520the%2520gap.%2520We%2520release%250Aour%2520checkpoints%252C%2520code%252C%2520documentation%252C%2520and%2520a%2520link%2520to%2520our%2520dataset%2520at%250Ahttps%253A//github.com/jimmyxu123/SELECT.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2410.05057v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=SELECT%3A%20A%20Large-Scale%20Benchmark%20of%20Data%20Curation%20Strategies%20for%20Image%0A%20%20Classification&entry.906535625=Benjamin%20Feuer%20and%20Jiawei%20Xu%20and%20Niv%20Cohen%20and%20Patrick%20Yubeaton%20and%20Govind%20Mittal%20and%20Chinmay%20Hegde&entry.1292438233=%20%20Data%20curation%20is%20the%20problem%20of%20how%20to%20collect%20and%20organize%20samples%20into%20a%0Adataset%20that%20supports%20efficient%20learning.%20Despite%20the%20centrality%20of%20the%20task%2C%0Alittle%20work%20has%20been%20devoted%20towards%20a%20large-scale%2C%20systematic%20comparison%20of%0Avarious%20curation%20methods.%20In%20this%20work%2C%20we%20take%20steps%20towards%20a%20formal%0Aevaluation%20of%20data%20curation%20strategies%20and%20introduce%20SELECT%2C%20the%20first%0Alarge-scale%20benchmark%20of%20curation%20strategies%20for%20image%20classification.%0A%20%20In%20order%20to%20generate%20baseline%20methods%20for%20the%20SELECT%20benchmark%2C%20we%20create%20a%0Anew%20dataset%2C%20ImageNet%2B%2B%2C%20which%20constitutes%20the%20largest%20superset%20of%20ImageNet-1K%0Ato%20date.%20Our%20dataset%20extends%20ImageNet%20with%205%20new%20training-data%20shifts%2C%20each%0Aapproximately%20the%20size%20of%20ImageNet-1K%20itself%2C%20and%20each%20assembled%20using%20a%0Adistinct%20curation%20strategy.%20We%20evaluate%20our%20data%20curation%20baselines%20in%20two%0Aways%3A%20%28i%29%20using%20each%20training-data%20shift%20to%20train%20identical%20image%0Aclassification%20models%20from%20scratch%20%28ii%29%20using%20the%20data%20itself%20to%20fit%20a%0Apretrained%20self-supervised%20representation.%0A%20%20Our%20findings%20show%20interesting%20trends%2C%20particularly%20pertaining%20to%20recent%0Amethods%20for%20data%20curation%20such%20as%20synthetic%20data%20generation%20and%20lookup%20based%20on%0ACLIP%20embeddings.%20We%20show%20that%20although%20these%20strategies%20are%20highly%20competitive%0Afor%20certain%20tasks%2C%20the%20curation%20strategy%20used%20to%20assemble%20the%20original%0AImageNet-1K%20dataset%20remains%20the%20gold%20standard.%20We%20anticipate%20that%20our%20benchmark%0Acan%20illuminate%20the%20path%20for%20new%20methods%20to%20further%20reduce%20the%20gap.%20We%20release%0Aour%20checkpoints%2C%20code%2C%20documentation%2C%20and%20a%20link%20to%20our%20dataset%20at%0Ahttps%3A//github.com/jimmyxu123/SELECT.%0A&entry.1838667208=http%3A//arxiv.org/abs/2410.05057v1&entry.124074799=Read"},
{"title": "L-C4: Language-Based Video Colorization for Creative and Consistent\n  Color", "author": "Zheng Chang and Shuchen Weng and Huan Ouyang and Yu Li and Si Li and Boxin Shi", "abstract": "  Automatic video colorization is inherently an ill-posed problem because each\nmonochrome frame has multiple optional color candidates. Previous\nexemplar-based video colorization methods restrict the user's imagination due\nto the elaborate retrieval process. Alternatively, conditional image\ncolorization methods combined with post-processing algorithms still struggle to\nmaintain temporal consistency. To address these issues, we present\nLanguage-based video Colorization for Creative and Consistent Colors (L-C4) to\nguide the colorization process using user-provided language descriptions. Our\nmodel is built upon a pre-trained cross-modality generative model, leveraging\nits comprehensive language understanding and robust color representation\nabilities. We introduce the cross-modality pre-fusion module to generate\ninstance-aware text embeddings, enabling the application of creative colors.\nAdditionally, we propose temporally deformable attention to prevent flickering\nor color shifts, and cross-clip fusion to maintain long-term color consistency.\nExtensive experimental results demonstrate that L-C4 outperforms relevant\nmethods, achieving semantically accurate colors, unrestricted creative\ncorrespondence, and temporally robust consistency.\n", "link": "http://arxiv.org/abs/2410.04972v1", "date": "2024-10-07", "relevancy": 2.3647, "topK": [{"title": "Magic-Me: Identity-Specific Video Customized Diffusion", "link": "http://arxiv.org/abs/2402.09368v1", "similarity": 0.6143}, {"title": "Customize-A-Video: One-Shot Motion Customization of Text-to-Video\n  Diffusion Models", "link": "http://arxiv.org/abs/2402.14780v1", "similarity": 0.6084}, {"title": "Tuning-Free Noise Rectification for High Fidelity Image-to-Video\n  Generation", "link": "http://arxiv.org/abs/2403.02827v1", "similarity": 0.5611}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20L-C4%3A%20Language-Based%20Video%20Colorization%20for%20Creative%20and%20Consistent%0A%20%20Color&body=Title%3A%20L-C4%3A%20Language-Based%20Video%20Colorization%20for%20Creative%20and%20Consistent%0A%20%20Color%0AAuthor%3A%20Zheng%20Chang%20and%20Shuchen%20Weng%20and%20Huan%20Ouyang%20and%20Yu%20Li%20and%20Si%20Li%20and%20Boxin%20Shi%0AAbstract%3A%20%20%20Automatic%20video%20colorization%20is%20inherently%20an%20ill-posed%20problem%20because%20each%0Amonochrome%20frame%20has%20multiple%20optional%20color%20candidates.%20Previous%0Aexemplar-based%20video%20colorization%20methods%20restrict%20the%20user%27s%20imagination%20due%0Ato%20the%20elaborate%20retrieval%20process.%20Alternatively%2C%20conditional%20image%0Acolorization%20methods%20combined%20with%20post-processing%20algorithms%20still%20struggle%20to%0Amaintain%20temporal%20consistency.%20To%20address%20these%20issues%2C%20we%20present%0ALanguage-based%20video%20Colorization%20for%20Creative%20and%20Consistent%20Colors%20%28L-C4%29%20to%0Aguide%20the%20colorization%20process%20using%20user-provided%20language%20descriptions.%20Our%0Amodel%20is%20built%20upon%20a%20pre-trained%20cross-modality%20generative%20model%2C%20leveraging%0Aits%20comprehensive%20language%20understanding%20and%20robust%20color%20representation%0Aabilities.%20We%20introduce%20the%20cross-modality%20pre-fusion%20module%20to%20generate%0Ainstance-aware%20text%20embeddings%2C%20enabling%20the%20application%20of%20creative%20colors.%0AAdditionally%2C%20we%20propose%20temporally%20deformable%20attention%20to%20prevent%20flickering%0Aor%20color%20shifts%2C%20and%20cross-clip%20fusion%20to%20maintain%20long-term%20color%20consistency.%0AExtensive%20experimental%20results%20demonstrate%20that%20L-C4%20outperforms%20relevant%0Amethods%2C%20achieving%20semantically%20accurate%20colors%2C%20unrestricted%20creative%0Acorrespondence%2C%20and%20temporally%20robust%20consistency.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2410.04972v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DL-C4%253A%2520Language-Based%2520Video%2520Colorization%2520for%2520Creative%2520and%2520Consistent%250A%2520%2520Color%26entry.906535625%3DZheng%2520Chang%2520and%2520Shuchen%2520Weng%2520and%2520Huan%2520Ouyang%2520and%2520Yu%2520Li%2520and%2520Si%2520Li%2520and%2520Boxin%2520Shi%26entry.1292438233%3D%2520%2520Automatic%2520video%2520colorization%2520is%2520inherently%2520an%2520ill-posed%2520problem%2520because%2520each%250Amonochrome%2520frame%2520has%2520multiple%2520optional%2520color%2520candidates.%2520Previous%250Aexemplar-based%2520video%2520colorization%2520methods%2520restrict%2520the%2520user%2527s%2520imagination%2520due%250Ato%2520the%2520elaborate%2520retrieval%2520process.%2520Alternatively%252C%2520conditional%2520image%250Acolorization%2520methods%2520combined%2520with%2520post-processing%2520algorithms%2520still%2520struggle%2520to%250Amaintain%2520temporal%2520consistency.%2520To%2520address%2520these%2520issues%252C%2520we%2520present%250ALanguage-based%2520video%2520Colorization%2520for%2520Creative%2520and%2520Consistent%2520Colors%2520%2528L-C4%2529%2520to%250Aguide%2520the%2520colorization%2520process%2520using%2520user-provided%2520language%2520descriptions.%2520Our%250Amodel%2520is%2520built%2520upon%2520a%2520pre-trained%2520cross-modality%2520generative%2520model%252C%2520leveraging%250Aits%2520comprehensive%2520language%2520understanding%2520and%2520robust%2520color%2520representation%250Aabilities.%2520We%2520introduce%2520the%2520cross-modality%2520pre-fusion%2520module%2520to%2520generate%250Ainstance-aware%2520text%2520embeddings%252C%2520enabling%2520the%2520application%2520of%2520creative%2520colors.%250AAdditionally%252C%2520we%2520propose%2520temporally%2520deformable%2520attention%2520to%2520prevent%2520flickering%250Aor%2520color%2520shifts%252C%2520and%2520cross-clip%2520fusion%2520to%2520maintain%2520long-term%2520color%2520consistency.%250AExtensive%2520experimental%2520results%2520demonstrate%2520that%2520L-C4%2520outperforms%2520relevant%250Amethods%252C%2520achieving%2520semantically%2520accurate%2520colors%252C%2520unrestricted%2520creative%250Acorrespondence%252C%2520and%2520temporally%2520robust%2520consistency.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2410.04972v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=L-C4%3A%20Language-Based%20Video%20Colorization%20for%20Creative%20and%20Consistent%0A%20%20Color&entry.906535625=Zheng%20Chang%20and%20Shuchen%20Weng%20and%20Huan%20Ouyang%20and%20Yu%20Li%20and%20Si%20Li%20and%20Boxin%20Shi&entry.1292438233=%20%20Automatic%20video%20colorization%20is%20inherently%20an%20ill-posed%20problem%20because%20each%0Amonochrome%20frame%20has%20multiple%20optional%20color%20candidates.%20Previous%0Aexemplar-based%20video%20colorization%20methods%20restrict%20the%20user%27s%20imagination%20due%0Ato%20the%20elaborate%20retrieval%20process.%20Alternatively%2C%20conditional%20image%0Acolorization%20methods%20combined%20with%20post-processing%20algorithms%20still%20struggle%20to%0Amaintain%20temporal%20consistency.%20To%20address%20these%20issues%2C%20we%20present%0ALanguage-based%20video%20Colorization%20for%20Creative%20and%20Consistent%20Colors%20%28L-C4%29%20to%0Aguide%20the%20colorization%20process%20using%20user-provided%20language%20descriptions.%20Our%0Amodel%20is%20built%20upon%20a%20pre-trained%20cross-modality%20generative%20model%2C%20leveraging%0Aits%20comprehensive%20language%20understanding%20and%20robust%20color%20representation%0Aabilities.%20We%20introduce%20the%20cross-modality%20pre-fusion%20module%20to%20generate%0Ainstance-aware%20text%20embeddings%2C%20enabling%20the%20application%20of%20creative%20colors.%0AAdditionally%2C%20we%20propose%20temporally%20deformable%20attention%20to%20prevent%20flickering%0Aor%20color%20shifts%2C%20and%20cross-clip%20fusion%20to%20maintain%20long-term%20color%20consistency.%0AExtensive%20experimental%20results%20demonstrate%20that%20L-C4%20outperforms%20relevant%0Amethods%2C%20achieving%20semantically%20accurate%20colors%2C%20unrestricted%20creative%0Acorrespondence%2C%20and%20temporally%20robust%20consistency.%0A&entry.1838667208=http%3A//arxiv.org/abs/2410.04972v1&entry.124074799=Read"},
{"title": "HE-Drive: Human-Like End-to-End Driving with Vision Language Models", "author": "Junming Wang and Xingyu Zhang and Zebin Xing and Songen Gu and Xiaoyang Guo and Yang Hu and Ziying Song and Qian Zhang and Xiaoxiao Long and Wei Yin", "abstract": "  In this paper, we propose HE-Drive: the first human-like-centric end-to-end\nautonomous driving system to generate trajectories that are both temporally\nconsistent and comfortable. Recent studies have shown that imitation\nlearning-based planners and learning-based trajectory scorers can effectively\ngenerate and select accuracy trajectories that closely mimic expert\ndemonstrations. However, such trajectory planners and scorers face the dilemma\nof generating temporally inconsistent and uncomfortable trajectories. To solve\nthe above problems, Our HE-Drive first extracts key 3D spatial representations\nthrough sparse perception, which then serves as conditional inputs for a\nConditional Denoising Diffusion Probabilistic Models (DDPMs)-based motion\nplanner to generate temporal consistency multi-modal trajectories. A\nVision-Language Models (VLMs)-guided trajectory scorer subsequently selects the\nmost comfortable trajectory from these candidates to control the vehicle,\nensuring human-like end-to-end driving. Experiments show that HE-Drive not only\nachieves state-of-the-art performance (i.e., reduces the average collision rate\nby 71% than VAD) and efficiency (i.e., 1.9X faster than SparseDrive) on the\nchallenging nuScenes and OpenScene datasets but also provides the most\ncomfortable driving experience on real-world data.For more information, visit\nthe project website: https://jmwang0117.github.io/HE-Drive/.\n", "link": "http://arxiv.org/abs/2410.05051v1", "date": "2024-10-07", "relevancy": 2.3549, "topK": [{"title": "Egocentric Computer Vision for Hands-Free Robotic Wheelchair Navigation", "link": "https://link.springer.com/article/10.1007/s10846-023-01807-4", "similarity": 0.6282}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.5824}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5517}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20HE-Drive%3A%20Human-Like%20End-to-End%20Driving%20with%20Vision%20Language%20Models&body=Title%3A%20HE-Drive%3A%20Human-Like%20End-to-End%20Driving%20with%20Vision%20Language%20Models%0AAuthor%3A%20Junming%20Wang%20and%20Xingyu%20Zhang%20and%20Zebin%20Xing%20and%20Songen%20Gu%20and%20Xiaoyang%20Guo%20and%20Yang%20Hu%20and%20Ziying%20Song%20and%20Qian%20Zhang%20and%20Xiaoxiao%20Long%20and%20Wei%20Yin%0AAbstract%3A%20%20%20In%20this%20paper%2C%20we%20propose%20HE-Drive%3A%20the%20first%20human-like-centric%20end-to-end%0Aautonomous%20driving%20system%20to%20generate%20trajectories%20that%20are%20both%20temporally%0Aconsistent%20and%20comfortable.%20Recent%20studies%20have%20shown%20that%20imitation%0Alearning-based%20planners%20and%20learning-based%20trajectory%20scorers%20can%20effectively%0Agenerate%20and%20select%20accuracy%20trajectories%20that%20closely%20mimic%20expert%0Ademonstrations.%20However%2C%20such%20trajectory%20planners%20and%20scorers%20face%20the%20dilemma%0Aof%20generating%20temporally%20inconsistent%20and%20uncomfortable%20trajectories.%20To%20solve%0Athe%20above%20problems%2C%20Our%20HE-Drive%20first%20extracts%20key%203D%20spatial%20representations%0Athrough%20sparse%20perception%2C%20which%20then%20serves%20as%20conditional%20inputs%20for%20a%0AConditional%20Denoising%20Diffusion%20Probabilistic%20Models%20%28DDPMs%29-based%20motion%0Aplanner%20to%20generate%20temporal%20consistency%20multi-modal%20trajectories.%20A%0AVision-Language%20Models%20%28VLMs%29-guided%20trajectory%20scorer%20subsequently%20selects%20the%0Amost%20comfortable%20trajectory%20from%20these%20candidates%20to%20control%20the%20vehicle%2C%0Aensuring%20human-like%20end-to-end%20driving.%20Experiments%20show%20that%20HE-Drive%20not%20only%0Aachieves%20state-of-the-art%20performance%20%28i.e.%2C%20reduces%20the%20average%20collision%20rate%0Aby%2071%25%20than%20VAD%29%20and%20efficiency%20%28i.e.%2C%201.9X%20faster%20than%20SparseDrive%29%20on%20the%0Achallenging%20nuScenes%20and%20OpenScene%20datasets%20but%20also%20provides%20the%20most%0Acomfortable%20driving%20experience%20on%20real-world%20data.For%20more%20information%2C%20visit%0Athe%20project%20website%3A%20https%3A//jmwang0117.github.io/HE-Drive/.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2410.05051v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DHE-Drive%253A%2520Human-Like%2520End-to-End%2520Driving%2520with%2520Vision%2520Language%2520Models%26entry.906535625%3DJunming%2520Wang%2520and%2520Xingyu%2520Zhang%2520and%2520Zebin%2520Xing%2520and%2520Songen%2520Gu%2520and%2520Xiaoyang%2520Guo%2520and%2520Yang%2520Hu%2520and%2520Ziying%2520Song%2520and%2520Qian%2520Zhang%2520and%2520Xiaoxiao%2520Long%2520and%2520Wei%2520Yin%26entry.1292438233%3D%2520%2520In%2520this%2520paper%252C%2520we%2520propose%2520HE-Drive%253A%2520the%2520first%2520human-like-centric%2520end-to-end%250Aautonomous%2520driving%2520system%2520to%2520generate%2520trajectories%2520that%2520are%2520both%2520temporally%250Aconsistent%2520and%2520comfortable.%2520Recent%2520studies%2520have%2520shown%2520that%2520imitation%250Alearning-based%2520planners%2520and%2520learning-based%2520trajectory%2520scorers%2520can%2520effectively%250Agenerate%2520and%2520select%2520accuracy%2520trajectories%2520that%2520closely%2520mimic%2520expert%250Ademonstrations.%2520However%252C%2520such%2520trajectory%2520planners%2520and%2520scorers%2520face%2520the%2520dilemma%250Aof%2520generating%2520temporally%2520inconsistent%2520and%2520uncomfortable%2520trajectories.%2520To%2520solve%250Athe%2520above%2520problems%252C%2520Our%2520HE-Drive%2520first%2520extracts%2520key%25203D%2520spatial%2520representations%250Athrough%2520sparse%2520perception%252C%2520which%2520then%2520serves%2520as%2520conditional%2520inputs%2520for%2520a%250AConditional%2520Denoising%2520Diffusion%2520Probabilistic%2520Models%2520%2528DDPMs%2529-based%2520motion%250Aplanner%2520to%2520generate%2520temporal%2520consistency%2520multi-modal%2520trajectories.%2520A%250AVision-Language%2520Models%2520%2528VLMs%2529-guided%2520trajectory%2520scorer%2520subsequently%2520selects%2520the%250Amost%2520comfortable%2520trajectory%2520from%2520these%2520candidates%2520to%2520control%2520the%2520vehicle%252C%250Aensuring%2520human-like%2520end-to-end%2520driving.%2520Experiments%2520show%2520that%2520HE-Drive%2520not%2520only%250Aachieves%2520state-of-the-art%2520performance%2520%2528i.e.%252C%2520reduces%2520the%2520average%2520collision%2520rate%250Aby%252071%2525%2520than%2520VAD%2529%2520and%2520efficiency%2520%2528i.e.%252C%25201.9X%2520faster%2520than%2520SparseDrive%2529%2520on%2520the%250Achallenging%2520nuScenes%2520and%2520OpenScene%2520datasets%2520but%2520also%2520provides%2520the%2520most%250Acomfortable%2520driving%2520experience%2520on%2520real-world%2520data.For%2520more%2520information%252C%2520visit%250Athe%2520project%2520website%253A%2520https%253A//jmwang0117.github.io/HE-Drive/.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2410.05051v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=HE-Drive%3A%20Human-Like%20End-to-End%20Driving%20with%20Vision%20Language%20Models&entry.906535625=Junming%20Wang%20and%20Xingyu%20Zhang%20and%20Zebin%20Xing%20and%20Songen%20Gu%20and%20Xiaoyang%20Guo%20and%20Yang%20Hu%20and%20Ziying%20Song%20and%20Qian%20Zhang%20and%20Xiaoxiao%20Long%20and%20Wei%20Yin&entry.1292438233=%20%20In%20this%20paper%2C%20we%20propose%20HE-Drive%3A%20the%20first%20human-like-centric%20end-to-end%0Aautonomous%20driving%20system%20to%20generate%20trajectories%20that%20are%20both%20temporally%0Aconsistent%20and%20comfortable.%20Recent%20studies%20have%20shown%20that%20imitation%0Alearning-based%20planners%20and%20learning-based%20trajectory%20scorers%20can%20effectively%0Agenerate%20and%20select%20accuracy%20trajectories%20that%20closely%20mimic%20expert%0Ademonstrations.%20However%2C%20such%20trajectory%20planners%20and%20scorers%20face%20the%20dilemma%0Aof%20generating%20temporally%20inconsistent%20and%20uncomfortable%20trajectories.%20To%20solve%0Athe%20above%20problems%2C%20Our%20HE-Drive%20first%20extracts%20key%203D%20spatial%20representations%0Athrough%20sparse%20perception%2C%20which%20then%20serves%20as%20conditional%20inputs%20for%20a%0AConditional%20Denoising%20Diffusion%20Probabilistic%20Models%20%28DDPMs%29-based%20motion%0Aplanner%20to%20generate%20temporal%20consistency%20multi-modal%20trajectories.%20A%0AVision-Language%20Models%20%28VLMs%29-guided%20trajectory%20scorer%20subsequently%20selects%20the%0Amost%20comfortable%20trajectory%20from%20these%20candidates%20to%20control%20the%20vehicle%2C%0Aensuring%20human-like%20end-to-end%20driving.%20Experiments%20show%20that%20HE-Drive%20not%20only%0Aachieves%20state-of-the-art%20performance%20%28i.e.%2C%20reduces%20the%20average%20collision%20rate%0Aby%2071%25%20than%20VAD%29%20and%20efficiency%20%28i.e.%2C%201.9X%20faster%20than%20SparseDrive%29%20on%20the%0Achallenging%20nuScenes%20and%20OpenScene%20datasets%20but%20also%20provides%20the%20most%0Acomfortable%20driving%20experience%20on%20real-world%20data.For%20more%20information%2C%20visit%0Athe%20project%20website%3A%20https%3A//jmwang0117.github.io/HE-Drive/.%0A&entry.1838667208=http%3A//arxiv.org/abs/2410.05051v1&entry.124074799=Read"},
{"title": "Autoregressive Image Diffusion: Generation of Image Sequence and\n  Application in MRI", "author": "Guanxiong Luo and Shoujin Huang and Martin Uecker", "abstract": "  Magnetic resonance imaging (MRI) is a widely used non-invasive imaging\nmodality. However, a persistent challenge lies in balancing image quality with\nimaging speed. This trade-off is primarily constrained by k-space measurements,\nwhich traverse specific trajectories in the spatial Fourier domain (k-space).\nThese measurements are often undersampled to shorten acquisition times,\nresulting in image artifacts and compromised quality. Generative models learn\nimage distributions and can be used to reconstruct high-quality images from\nundersampled k-space data. In this work, we present the autoregressive image\ndiffusion (AID) model for image sequences and use it to sample the posterior\nfor accelerated MRI reconstruction. The algorithm incorporates both\nundersampled k-space and pre-existing information. Models trained with fastMRI\ndataset are evaluated comprehensively. The results show that the AID model can\nrobustly generate sequentially coherent image sequences. In MRI applications,\nthe AID can outperform the standard diffusion model and reduce hallucinations,\ndue to the learned inter-image dependencies. The project code is available at\nhttps://github.com/mrirecon/aid.\n", "link": "http://arxiv.org/abs/2405.14327v4", "date": "2024-10-07", "relevancy": 2.3433, "topK": [{"title": "SDXS: Real-Time One-Step Latent Diffusion Models with Image Conditions", "link": "http://arxiv.org/abs/2403.16627v1", "similarity": 0.5961}, {"title": "CAT3D: Create Anything in 3D with Multi-View Diffusion Models", "link": "http://arxiv.org/abs/2405.10314v1", "similarity": 0.5838}, {"title": "CAT3D: Create Anything in 3D with Multi-View Diffusion Models", "link": "http://arxiv.org/abs/2405.10314v1", "similarity": 0.5838}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Autoregressive%20Image%20Diffusion%3A%20Generation%20of%20Image%20Sequence%20and%0A%20%20Application%20in%20MRI&body=Title%3A%20Autoregressive%20Image%20Diffusion%3A%20Generation%20of%20Image%20Sequence%20and%0A%20%20Application%20in%20MRI%0AAuthor%3A%20Guanxiong%20Luo%20and%20Shoujin%20Huang%20and%20Martin%20Uecker%0AAbstract%3A%20%20%20Magnetic%20resonance%20imaging%20%28MRI%29%20is%20a%20widely%20used%20non-invasive%20imaging%0Amodality.%20However%2C%20a%20persistent%20challenge%20lies%20in%20balancing%20image%20quality%20with%0Aimaging%20speed.%20This%20trade-off%20is%20primarily%20constrained%20by%20k-space%20measurements%2C%0Awhich%20traverse%20specific%20trajectories%20in%20the%20spatial%20Fourier%20domain%20%28k-space%29.%0AThese%20measurements%20are%20often%20undersampled%20to%20shorten%20acquisition%20times%2C%0Aresulting%20in%20image%20artifacts%20and%20compromised%20quality.%20Generative%20models%20learn%0Aimage%20distributions%20and%20can%20be%20used%20to%20reconstruct%20high-quality%20images%20from%0Aundersampled%20k-space%20data.%20In%20this%20work%2C%20we%20present%20the%20autoregressive%20image%0Adiffusion%20%28AID%29%20model%20for%20image%20sequences%20and%20use%20it%20to%20sample%20the%20posterior%0Afor%20accelerated%20MRI%20reconstruction.%20The%20algorithm%20incorporates%20both%0Aundersampled%20k-space%20and%20pre-existing%20information.%20Models%20trained%20with%20fastMRI%0Adataset%20are%20evaluated%20comprehensively.%20The%20results%20show%20that%20the%20AID%20model%20can%0Arobustly%20generate%20sequentially%20coherent%20image%20sequences.%20In%20MRI%20applications%2C%0Athe%20AID%20can%20outperform%20the%20standard%20diffusion%20model%20and%20reduce%20hallucinations%2C%0Adue%20to%20the%20learned%20inter-image%20dependencies.%20The%20project%20code%20is%20available%20at%0Ahttps%3A//github.com/mrirecon/aid.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2405.14327v4%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DAutoregressive%2520Image%2520Diffusion%253A%2520Generation%2520of%2520Image%2520Sequence%2520and%250A%2520%2520Application%2520in%2520MRI%26entry.906535625%3DGuanxiong%2520Luo%2520and%2520Shoujin%2520Huang%2520and%2520Martin%2520Uecker%26entry.1292438233%3D%2520%2520Magnetic%2520resonance%2520imaging%2520%2528MRI%2529%2520is%2520a%2520widely%2520used%2520non-invasive%2520imaging%250Amodality.%2520However%252C%2520a%2520persistent%2520challenge%2520lies%2520in%2520balancing%2520image%2520quality%2520with%250Aimaging%2520speed.%2520This%2520trade-off%2520is%2520primarily%2520constrained%2520by%2520k-space%2520measurements%252C%250Awhich%2520traverse%2520specific%2520trajectories%2520in%2520the%2520spatial%2520Fourier%2520domain%2520%2528k-space%2529.%250AThese%2520measurements%2520are%2520often%2520undersampled%2520to%2520shorten%2520acquisition%2520times%252C%250Aresulting%2520in%2520image%2520artifacts%2520and%2520compromised%2520quality.%2520Generative%2520models%2520learn%250Aimage%2520distributions%2520and%2520can%2520be%2520used%2520to%2520reconstruct%2520high-quality%2520images%2520from%250Aundersampled%2520k-space%2520data.%2520In%2520this%2520work%252C%2520we%2520present%2520the%2520autoregressive%2520image%250Adiffusion%2520%2528AID%2529%2520model%2520for%2520image%2520sequences%2520and%2520use%2520it%2520to%2520sample%2520the%2520posterior%250Afor%2520accelerated%2520MRI%2520reconstruction.%2520The%2520algorithm%2520incorporates%2520both%250Aundersampled%2520k-space%2520and%2520pre-existing%2520information.%2520Models%2520trained%2520with%2520fastMRI%250Adataset%2520are%2520evaluated%2520comprehensively.%2520The%2520results%2520show%2520that%2520the%2520AID%2520model%2520can%250Arobustly%2520generate%2520sequentially%2520coherent%2520image%2520sequences.%2520In%2520MRI%2520applications%252C%250Athe%2520AID%2520can%2520outperform%2520the%2520standard%2520diffusion%2520model%2520and%2520reduce%2520hallucinations%252C%250Adue%2520to%2520the%2520learned%2520inter-image%2520dependencies.%2520The%2520project%2520code%2520is%2520available%2520at%250Ahttps%253A//github.com/mrirecon/aid.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2405.14327v4%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Autoregressive%20Image%20Diffusion%3A%20Generation%20of%20Image%20Sequence%20and%0A%20%20Application%20in%20MRI&entry.906535625=Guanxiong%20Luo%20and%20Shoujin%20Huang%20and%20Martin%20Uecker&entry.1292438233=%20%20Magnetic%20resonance%20imaging%20%28MRI%29%20is%20a%20widely%20used%20non-invasive%20imaging%0Amodality.%20However%2C%20a%20persistent%20challenge%20lies%20in%20balancing%20image%20quality%20with%0Aimaging%20speed.%20This%20trade-off%20is%20primarily%20constrained%20by%20k-space%20measurements%2C%0Awhich%20traverse%20specific%20trajectories%20in%20the%20spatial%20Fourier%20domain%20%28k-space%29.%0AThese%20measurements%20are%20often%20undersampled%20to%20shorten%20acquisition%20times%2C%0Aresulting%20in%20image%20artifacts%20and%20compromised%20quality.%20Generative%20models%20learn%0Aimage%20distributions%20and%20can%20be%20used%20to%20reconstruct%20high-quality%20images%20from%0Aundersampled%20k-space%20data.%20In%20this%20work%2C%20we%20present%20the%20autoregressive%20image%0Adiffusion%20%28AID%29%20model%20for%20image%20sequences%20and%20use%20it%20to%20sample%20the%20posterior%0Afor%20accelerated%20MRI%20reconstruction.%20The%20algorithm%20incorporates%20both%0Aundersampled%20k-space%20and%20pre-existing%20information.%20Models%20trained%20with%20fastMRI%0Adataset%20are%20evaluated%20comprehensively.%20The%20results%20show%20that%20the%20AID%20model%20can%0Arobustly%20generate%20sequentially%20coherent%20image%20sequences.%20In%20MRI%20applications%2C%0Athe%20AID%20can%20outperform%20the%20standard%20diffusion%20model%20and%20reduce%20hallucinations%2C%0Adue%20to%20the%20learned%20inter-image%20dependencies.%20The%20project%20code%20is%20available%20at%0Ahttps%3A//github.com/mrirecon/aid.%0A&entry.1838667208=http%3A//arxiv.org/abs/2405.14327v4&entry.124074799=Read"},
{"title": "VILENS: Visual, Inertial, Lidar, and Leg Odometry for All-Terrain Legged\n  Robots", "author": "David Wisth and Marco Camurri and Maurice Fallon", "abstract": "  We present visual inertial lidar legged navigation system (VILENS), an\nodometry system for legged robots based on factor graphs. The key novelty is\nthe tight fusion of four different sensor modalities to achieve reliable\noperation when the individual sensors would otherwise produce degenerate\nestimation. To minimize leg odometry drift, we extend the robot's state with a\nlinear velocity bias term, which is estimated online. This bias is observable\nbecause of the tight fusion of this preintegrated velocity factor with vision,\nlidar, and inertial measurement unit (IMU) factors. Extensive experimental\nvalidation on different ANYmal quadruped robots is presented, for a total\nduration of 2 h and 1.8 km traveled. The experiments involved dynamic\nlocomotion over loose rocks, slopes, and mud, which caused challenges such as\nslippage and terrain deformation. Perceptual challenges included dark and dusty\nunderground caverns, and open and feature-deprived areas. We show an average\nimprovement of 62% translational and 51% rotational errors compared to a\nstate-of-the-art loosely coupled approach. To demonstrate its robustness,\nVILENS was also integrated with a perceptive controller and a local path\nplanner.\n", "link": "http://arxiv.org/abs/2107.07243v3", "date": "2024-10-07", "relevancy": 2.3405, "topK": [{"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.6065}, {"title": "Egocentric Computer Vision for Hands-Free Robotic Wheelchair Navigation", "link": "https://link.springer.com/article/10.1007/s10846-023-01807-4", "similarity": 0.5996}, {"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.5621}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20VILENS%3A%20Visual%2C%20Inertial%2C%20Lidar%2C%20and%20Leg%20Odometry%20for%20All-Terrain%20Legged%0A%20%20Robots&body=Title%3A%20VILENS%3A%20Visual%2C%20Inertial%2C%20Lidar%2C%20and%20Leg%20Odometry%20for%20All-Terrain%20Legged%0A%20%20Robots%0AAuthor%3A%20David%20Wisth%20and%20Marco%20Camurri%20and%20Maurice%20Fallon%0AAbstract%3A%20%20%20We%20present%20visual%20inertial%20lidar%20legged%20navigation%20system%20%28VILENS%29%2C%20an%0Aodometry%20system%20for%20legged%20robots%20based%20on%20factor%20graphs.%20The%20key%20novelty%20is%0Athe%20tight%20fusion%20of%20four%20different%20sensor%20modalities%20to%20achieve%20reliable%0Aoperation%20when%20the%20individual%20sensors%20would%20otherwise%20produce%20degenerate%0Aestimation.%20To%20minimize%20leg%20odometry%20drift%2C%20we%20extend%20the%20robot%27s%20state%20with%20a%0Alinear%20velocity%20bias%20term%2C%20which%20is%20estimated%20online.%20This%20bias%20is%20observable%0Abecause%20of%20the%20tight%20fusion%20of%20this%20preintegrated%20velocity%20factor%20with%20vision%2C%0Alidar%2C%20and%20inertial%20measurement%20unit%20%28IMU%29%20factors.%20Extensive%20experimental%0Avalidation%20on%20different%20ANYmal%20quadruped%20robots%20is%20presented%2C%20for%20a%20total%0Aduration%20of%202%20h%20and%201.8%20km%20traveled.%20The%20experiments%20involved%20dynamic%0Alocomotion%20over%20loose%20rocks%2C%20slopes%2C%20and%20mud%2C%20which%20caused%20challenges%20such%20as%0Aslippage%20and%20terrain%20deformation.%20Perceptual%20challenges%20included%20dark%20and%20dusty%0Aunderground%20caverns%2C%20and%20open%20and%20feature-deprived%20areas.%20We%20show%20an%20average%0Aimprovement%20of%2062%25%20translational%20and%2051%25%20rotational%20errors%20compared%20to%20a%0Astate-of-the-art%20loosely%20coupled%20approach.%20To%20demonstrate%20its%20robustness%2C%0AVILENS%20was%20also%20integrated%20with%20a%20perceptive%20controller%20and%20a%20local%20path%0Aplanner.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2107.07243v3%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DVILENS%253A%2520Visual%252C%2520Inertial%252C%2520Lidar%252C%2520and%2520Leg%2520Odometry%2520for%2520All-Terrain%2520Legged%250A%2520%2520Robots%26entry.906535625%3DDavid%2520Wisth%2520and%2520Marco%2520Camurri%2520and%2520Maurice%2520Fallon%26entry.1292438233%3D%2520%2520We%2520present%2520visual%2520inertial%2520lidar%2520legged%2520navigation%2520system%2520%2528VILENS%2529%252C%2520an%250Aodometry%2520system%2520for%2520legged%2520robots%2520based%2520on%2520factor%2520graphs.%2520The%2520key%2520novelty%2520is%250Athe%2520tight%2520fusion%2520of%2520four%2520different%2520sensor%2520modalities%2520to%2520achieve%2520reliable%250Aoperation%2520when%2520the%2520individual%2520sensors%2520would%2520otherwise%2520produce%2520degenerate%250Aestimation.%2520To%2520minimize%2520leg%2520odometry%2520drift%252C%2520we%2520extend%2520the%2520robot%2527s%2520state%2520with%2520a%250Alinear%2520velocity%2520bias%2520term%252C%2520which%2520is%2520estimated%2520online.%2520This%2520bias%2520is%2520observable%250Abecause%2520of%2520the%2520tight%2520fusion%2520of%2520this%2520preintegrated%2520velocity%2520factor%2520with%2520vision%252C%250Alidar%252C%2520and%2520inertial%2520measurement%2520unit%2520%2528IMU%2529%2520factors.%2520Extensive%2520experimental%250Avalidation%2520on%2520different%2520ANYmal%2520quadruped%2520robots%2520is%2520presented%252C%2520for%2520a%2520total%250Aduration%2520of%25202%2520h%2520and%25201.8%2520km%2520traveled.%2520The%2520experiments%2520involved%2520dynamic%250Alocomotion%2520over%2520loose%2520rocks%252C%2520slopes%252C%2520and%2520mud%252C%2520which%2520caused%2520challenges%2520such%2520as%250Aslippage%2520and%2520terrain%2520deformation.%2520Perceptual%2520challenges%2520included%2520dark%2520and%2520dusty%250Aunderground%2520caverns%252C%2520and%2520open%2520and%2520feature-deprived%2520areas.%2520We%2520show%2520an%2520average%250Aimprovement%2520of%252062%2525%2520translational%2520and%252051%2525%2520rotational%2520errors%2520compared%2520to%2520a%250Astate-of-the-art%2520loosely%2520coupled%2520approach.%2520To%2520demonstrate%2520its%2520robustness%252C%250AVILENS%2520was%2520also%2520integrated%2520with%2520a%2520perceptive%2520controller%2520and%2520a%2520local%2520path%250Aplanner.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2107.07243v3%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=VILENS%3A%20Visual%2C%20Inertial%2C%20Lidar%2C%20and%20Leg%20Odometry%20for%20All-Terrain%20Legged%0A%20%20Robots&entry.906535625=David%20Wisth%20and%20Marco%20Camurri%20and%20Maurice%20Fallon&entry.1292438233=%20%20We%20present%20visual%20inertial%20lidar%20legged%20navigation%20system%20%28VILENS%29%2C%20an%0Aodometry%20system%20for%20legged%20robots%20based%20on%20factor%20graphs.%20The%20key%20novelty%20is%0Athe%20tight%20fusion%20of%20four%20different%20sensor%20modalities%20to%20achieve%20reliable%0Aoperation%20when%20the%20individual%20sensors%20would%20otherwise%20produce%20degenerate%0Aestimation.%20To%20minimize%20leg%20odometry%20drift%2C%20we%20extend%20the%20robot%27s%20state%20with%20a%0Alinear%20velocity%20bias%20term%2C%20which%20is%20estimated%20online.%20This%20bias%20is%20observable%0Abecause%20of%20the%20tight%20fusion%20of%20this%20preintegrated%20velocity%20factor%20with%20vision%2C%0Alidar%2C%20and%20inertial%20measurement%20unit%20%28IMU%29%20factors.%20Extensive%20experimental%0Avalidation%20on%20different%20ANYmal%20quadruped%20robots%20is%20presented%2C%20for%20a%20total%0Aduration%20of%202%20h%20and%201.8%20km%20traveled.%20The%20experiments%20involved%20dynamic%0Alocomotion%20over%20loose%20rocks%2C%20slopes%2C%20and%20mud%2C%20which%20caused%20challenges%20such%20as%0Aslippage%20and%20terrain%20deformation.%20Perceptual%20challenges%20included%20dark%20and%20dusty%0Aunderground%20caverns%2C%20and%20open%20and%20feature-deprived%20areas.%20We%20show%20an%20average%0Aimprovement%20of%2062%25%20translational%20and%2051%25%20rotational%20errors%20compared%20to%20a%0Astate-of-the-art%20loosely%20coupled%20approach.%20To%20demonstrate%20its%20robustness%2C%0AVILENS%20was%20also%20integrated%20with%20a%20perceptive%20controller%20and%20a%20local%20path%0Aplanner.%0A&entry.1838667208=http%3A//arxiv.org/abs/2107.07243v3&entry.124074799=Read"},
{"title": "HE-Nav: A High-Performance and Efficient Navigation System for\n  Aerial-Ground Robots in Cluttered Environments", "author": "Junming Wang and Zekai Sun and Xiuxian Guan and Tianxiang Shen and Dong Huang and Zongyuan Zhang and Tianyang Duan and Fangming Liu and Heming Cui", "abstract": "  Existing AGR navigation systems have advanced in lightly occluded scenarios\n(e.g., buildings) by employing 3D semantic scene completion networks for voxel\noccupancy prediction and constructing Euclidean Signed Distance Field (ESDF)\nmaps for collision-free path planning. However, these systems exhibit\nsuboptimal performance and efficiency in cluttered environments with severe\nocclusions (e.g., dense forests or tall walls), due to limitations arising from\nperception networks' low prediction accuracy and path planners' high\ncomputational overhead. In this paper, we present HE-Nav, the first\nhigh-performance and efficient navigation system tailored for AGRs operating in\ncluttered environments. The perception module utilizes a lightweight semantic\nscene completion network (LBSCNet), guided by a bird's eye view (BEV) feature\nfusion and enhanced by an exquisitely designed SCB-Fusion module and attention\nmechanism. This enables real-time and efficient obstacle prediction in\ncluttered areas, generating a complete local map. Building upon this completed\nmap, our novel AG-Planner employs the energy-efficient kinodynamic A* search\nalgorithm to guarantee planning is energy-saving. Subsequent trajectory\noptimization processes yield safe, smooth, dynamically feasible and ESDF-free\naerial-ground hybrid paths. Extensive experiments demonstrate that HE-Nav\nachieved 7x energy savings in real-world situations while maintaining planning\nsuccess rates of 98% in simulation scenarios. Code and video are available on\nour project page: https://jmwang0117.github.io/HE-Nav/.\n", "link": "http://arxiv.org/abs/2410.05079v1", "date": "2024-10-07", "relevancy": 2.3384, "topK": [{"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.5905}, {"title": "RoboEXP: Action-Conditioned Scene Graph via Interactive Exploration for\n  Robotic Manipulation", "link": "http://arxiv.org/abs/2402.15487v1", "similarity": 0.5852}, {"title": "Egocentric Computer Vision for Hands-Free Robotic Wheelchair Navigation", "link": "https://link.springer.com/article/10.1007/s10846-023-01807-4", "similarity": 0.5784}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20HE-Nav%3A%20A%20High-Performance%20and%20Efficient%20Navigation%20System%20for%0A%20%20Aerial-Ground%20Robots%20in%20Cluttered%20Environments&body=Title%3A%20HE-Nav%3A%20A%20High-Performance%20and%20Efficient%20Navigation%20System%20for%0A%20%20Aerial-Ground%20Robots%20in%20Cluttered%20Environments%0AAuthor%3A%20Junming%20Wang%20and%20Zekai%20Sun%20and%20Xiuxian%20Guan%20and%20Tianxiang%20Shen%20and%20Dong%20Huang%20and%20Zongyuan%20Zhang%20and%20Tianyang%20Duan%20and%20Fangming%20Liu%20and%20Heming%20Cui%0AAbstract%3A%20%20%20Existing%20AGR%20navigation%20systems%20have%20advanced%20in%20lightly%20occluded%20scenarios%0A%28e.g.%2C%20buildings%29%20by%20employing%203D%20semantic%20scene%20completion%20networks%20for%20voxel%0Aoccupancy%20prediction%20and%20constructing%20Euclidean%20Signed%20Distance%20Field%20%28ESDF%29%0Amaps%20for%20collision-free%20path%20planning.%20However%2C%20these%20systems%20exhibit%0Asuboptimal%20performance%20and%20efficiency%20in%20cluttered%20environments%20with%20severe%0Aocclusions%20%28e.g.%2C%20dense%20forests%20or%20tall%20walls%29%2C%20due%20to%20limitations%20arising%20from%0Aperception%20networks%27%20low%20prediction%20accuracy%20and%20path%20planners%27%20high%0Acomputational%20overhead.%20In%20this%20paper%2C%20we%20present%20HE-Nav%2C%20the%20first%0Ahigh-performance%20and%20efficient%20navigation%20system%20tailored%20for%20AGRs%20operating%20in%0Acluttered%20environments.%20The%20perception%20module%20utilizes%20a%20lightweight%20semantic%0Ascene%20completion%20network%20%28LBSCNet%29%2C%20guided%20by%20a%20bird%27s%20eye%20view%20%28BEV%29%20feature%0Afusion%20and%20enhanced%20by%20an%20exquisitely%20designed%20SCB-Fusion%20module%20and%20attention%0Amechanism.%20This%20enables%20real-time%20and%20efficient%20obstacle%20prediction%20in%0Acluttered%20areas%2C%20generating%20a%20complete%20local%20map.%20Building%20upon%20this%20completed%0Amap%2C%20our%20novel%20AG-Planner%20employs%20the%20energy-efficient%20kinodynamic%20A%2A%20search%0Aalgorithm%20to%20guarantee%20planning%20is%20energy-saving.%20Subsequent%20trajectory%0Aoptimization%20processes%20yield%20safe%2C%20smooth%2C%20dynamically%20feasible%20and%20ESDF-free%0Aaerial-ground%20hybrid%20paths.%20Extensive%20experiments%20demonstrate%20that%20HE-Nav%0Aachieved%207x%20energy%20savings%20in%20real-world%20situations%20while%20maintaining%20planning%0Asuccess%20rates%20of%2098%25%20in%20simulation%20scenarios.%20Code%20and%20video%20are%20available%20on%0Aour%20project%20page%3A%20https%3A//jmwang0117.github.io/HE-Nav/.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2410.05079v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DHE-Nav%253A%2520A%2520High-Performance%2520and%2520Efficient%2520Navigation%2520System%2520for%250A%2520%2520Aerial-Ground%2520Robots%2520in%2520Cluttered%2520Environments%26entry.906535625%3DJunming%2520Wang%2520and%2520Zekai%2520Sun%2520and%2520Xiuxian%2520Guan%2520and%2520Tianxiang%2520Shen%2520and%2520Dong%2520Huang%2520and%2520Zongyuan%2520Zhang%2520and%2520Tianyang%2520Duan%2520and%2520Fangming%2520Liu%2520and%2520Heming%2520Cui%26entry.1292438233%3D%2520%2520Existing%2520AGR%2520navigation%2520systems%2520have%2520advanced%2520in%2520lightly%2520occluded%2520scenarios%250A%2528e.g.%252C%2520buildings%2529%2520by%2520employing%25203D%2520semantic%2520scene%2520completion%2520networks%2520for%2520voxel%250Aoccupancy%2520prediction%2520and%2520constructing%2520Euclidean%2520Signed%2520Distance%2520Field%2520%2528ESDF%2529%250Amaps%2520for%2520collision-free%2520path%2520planning.%2520However%252C%2520these%2520systems%2520exhibit%250Asuboptimal%2520performance%2520and%2520efficiency%2520in%2520cluttered%2520environments%2520with%2520severe%250Aocclusions%2520%2528e.g.%252C%2520dense%2520forests%2520or%2520tall%2520walls%2529%252C%2520due%2520to%2520limitations%2520arising%2520from%250Aperception%2520networks%2527%2520low%2520prediction%2520accuracy%2520and%2520path%2520planners%2527%2520high%250Acomputational%2520overhead.%2520In%2520this%2520paper%252C%2520we%2520present%2520HE-Nav%252C%2520the%2520first%250Ahigh-performance%2520and%2520efficient%2520navigation%2520system%2520tailored%2520for%2520AGRs%2520operating%2520in%250Acluttered%2520environments.%2520The%2520perception%2520module%2520utilizes%2520a%2520lightweight%2520semantic%250Ascene%2520completion%2520network%2520%2528LBSCNet%2529%252C%2520guided%2520by%2520a%2520bird%2527s%2520eye%2520view%2520%2528BEV%2529%2520feature%250Afusion%2520and%2520enhanced%2520by%2520an%2520exquisitely%2520designed%2520SCB-Fusion%2520module%2520and%2520attention%250Amechanism.%2520This%2520enables%2520real-time%2520and%2520efficient%2520obstacle%2520prediction%2520in%250Acluttered%2520areas%252C%2520generating%2520a%2520complete%2520local%2520map.%2520Building%2520upon%2520this%2520completed%250Amap%252C%2520our%2520novel%2520AG-Planner%2520employs%2520the%2520energy-efficient%2520kinodynamic%2520A%252A%2520search%250Aalgorithm%2520to%2520guarantee%2520planning%2520is%2520energy-saving.%2520Subsequent%2520trajectory%250Aoptimization%2520processes%2520yield%2520safe%252C%2520smooth%252C%2520dynamically%2520feasible%2520and%2520ESDF-free%250Aaerial-ground%2520hybrid%2520paths.%2520Extensive%2520experiments%2520demonstrate%2520that%2520HE-Nav%250Aachieved%25207x%2520energy%2520savings%2520in%2520real-world%2520situations%2520while%2520maintaining%2520planning%250Asuccess%2520rates%2520of%252098%2525%2520in%2520simulation%2520scenarios.%2520Code%2520and%2520video%2520are%2520available%2520on%250Aour%2520project%2520page%253A%2520https%253A//jmwang0117.github.io/HE-Nav/.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2410.05079v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=HE-Nav%3A%20A%20High-Performance%20and%20Efficient%20Navigation%20System%20for%0A%20%20Aerial-Ground%20Robots%20in%20Cluttered%20Environments&entry.906535625=Junming%20Wang%20and%20Zekai%20Sun%20and%20Xiuxian%20Guan%20and%20Tianxiang%20Shen%20and%20Dong%20Huang%20and%20Zongyuan%20Zhang%20and%20Tianyang%20Duan%20and%20Fangming%20Liu%20and%20Heming%20Cui&entry.1292438233=%20%20Existing%20AGR%20navigation%20systems%20have%20advanced%20in%20lightly%20occluded%20scenarios%0A%28e.g.%2C%20buildings%29%20by%20employing%203D%20semantic%20scene%20completion%20networks%20for%20voxel%0Aoccupancy%20prediction%20and%20constructing%20Euclidean%20Signed%20Distance%20Field%20%28ESDF%29%0Amaps%20for%20collision-free%20path%20planning.%20However%2C%20these%20systems%20exhibit%0Asuboptimal%20performance%20and%20efficiency%20in%20cluttered%20environments%20with%20severe%0Aocclusions%20%28e.g.%2C%20dense%20forests%20or%20tall%20walls%29%2C%20due%20to%20limitations%20arising%20from%0Aperception%20networks%27%20low%20prediction%20accuracy%20and%20path%20planners%27%20high%0Acomputational%20overhead.%20In%20this%20paper%2C%20we%20present%20HE-Nav%2C%20the%20first%0Ahigh-performance%20and%20efficient%20navigation%20system%20tailored%20for%20AGRs%20operating%20in%0Acluttered%20environments.%20The%20perception%20module%20utilizes%20a%20lightweight%20semantic%0Ascene%20completion%20network%20%28LBSCNet%29%2C%20guided%20by%20a%20bird%27s%20eye%20view%20%28BEV%29%20feature%0Afusion%20and%20enhanced%20by%20an%20exquisitely%20designed%20SCB-Fusion%20module%20and%20attention%0Amechanism.%20This%20enables%20real-time%20and%20efficient%20obstacle%20prediction%20in%0Acluttered%20areas%2C%20generating%20a%20complete%20local%20map.%20Building%20upon%20this%20completed%0Amap%2C%20our%20novel%20AG-Planner%20employs%20the%20energy-efficient%20kinodynamic%20A%2A%20search%0Aalgorithm%20to%20guarantee%20planning%20is%20energy-saving.%20Subsequent%20trajectory%0Aoptimization%20processes%20yield%20safe%2C%20smooth%2C%20dynamically%20feasible%20and%20ESDF-free%0Aaerial-ground%20hybrid%20paths.%20Extensive%20experiments%20demonstrate%20that%20HE-Nav%0Aachieved%207x%20energy%20savings%20in%20real-world%20situations%20while%20maintaining%20planning%0Asuccess%20rates%20of%2098%25%20in%20simulation%20scenarios.%20Code%20and%20video%20are%20available%20on%0Aour%20project%20page%3A%20https%3A//jmwang0117.github.io/HE-Nav/.%0A&entry.1838667208=http%3A//arxiv.org/abs/2410.05079v1&entry.124074799=Read"},
{"title": "MARs: Multi-view Attention Regularizations for Patch-based Feature\n  Recognition of Space Terrain", "author": "Timothy Chase Jr and Karthik Dantu", "abstract": "  The visual detection and tracking of surface terrain is required for\nspacecraft to safely land on or navigate within close proximity to celestial\nobjects. Current approaches rely on template matching with pre-gathered\npatch-based features, which are expensive to obtain and a limiting factor in\nperceptual capability. While recent literature has focused on in-situ detection\nmethods to enhance navigation and operational autonomy, robust description is\nstill needed. In this work, we explore metric learning as the lightweight\nfeature description mechanism and find that current solutions fail to address\ninter-class similarity and multi-view observational geometry. We attribute this\nto the view-unaware attention mechanism and introduce Multi-view Attention\nRegularizations (MARs) to constrain the channel and spatial attention across\nmultiple feature views, regularizing the what and where of attention focus. We\nthoroughly analyze many modern metric learning losses with and without MARs and\ndemonstrate improved terrain-feature recognition performance by upwards of 85%.\nWe additionally introduce the Luna-1 dataset, consisting of Moon crater\nlandmarks and reference navigation frames from NASA mission data to support\nfuture research in this difficult task. Luna-1 and source code are publicly\navailable at https://droneslab.github.io/mars/.\n", "link": "http://arxiv.org/abs/2410.05182v1", "date": "2024-10-07", "relevancy": 2.321, "topK": [{"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5875}, {"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.5765}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.5716}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20MARs%3A%20Multi-view%20Attention%20Regularizations%20for%20Patch-based%20Feature%0A%20%20Recognition%20of%20Space%20Terrain&body=Title%3A%20MARs%3A%20Multi-view%20Attention%20Regularizations%20for%20Patch-based%20Feature%0A%20%20Recognition%20of%20Space%20Terrain%0AAuthor%3A%20Timothy%20Chase%20Jr%20and%20Karthik%20Dantu%0AAbstract%3A%20%20%20The%20visual%20detection%20and%20tracking%20of%20surface%20terrain%20is%20required%20for%0Aspacecraft%20to%20safely%20land%20on%20or%20navigate%20within%20close%20proximity%20to%20celestial%0Aobjects.%20Current%20approaches%20rely%20on%20template%20matching%20with%20pre-gathered%0Apatch-based%20features%2C%20which%20are%20expensive%20to%20obtain%20and%20a%20limiting%20factor%20in%0Aperceptual%20capability.%20While%20recent%20literature%20has%20focused%20on%20in-situ%20detection%0Amethods%20to%20enhance%20navigation%20and%20operational%20autonomy%2C%20robust%20description%20is%0Astill%20needed.%20In%20this%20work%2C%20we%20explore%20metric%20learning%20as%20the%20lightweight%0Afeature%20description%20mechanism%20and%20find%20that%20current%20solutions%20fail%20to%20address%0Ainter-class%20similarity%20and%20multi-view%20observational%20geometry.%20We%20attribute%20this%0Ato%20the%20view-unaware%20attention%20mechanism%20and%20introduce%20Multi-view%20Attention%0ARegularizations%20%28MARs%29%20to%20constrain%20the%20channel%20and%20spatial%20attention%20across%0Amultiple%20feature%20views%2C%20regularizing%20the%20what%20and%20where%20of%20attention%20focus.%20We%0Athoroughly%20analyze%20many%20modern%20metric%20learning%20losses%20with%20and%20without%20MARs%20and%0Ademonstrate%20improved%20terrain-feature%20recognition%20performance%20by%20upwards%20of%2085%25.%0AWe%20additionally%20introduce%20the%20Luna-1%20dataset%2C%20consisting%20of%20Moon%20crater%0Alandmarks%20and%20reference%20navigation%20frames%20from%20NASA%20mission%20data%20to%20support%0Afuture%20research%20in%20this%20difficult%20task.%20Luna-1%20and%20source%20code%20are%20publicly%0Aavailable%20at%20https%3A//droneslab.github.io/mars/.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2410.05182v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DMARs%253A%2520Multi-view%2520Attention%2520Regularizations%2520for%2520Patch-based%2520Feature%250A%2520%2520Recognition%2520of%2520Space%2520Terrain%26entry.906535625%3DTimothy%2520Chase%2520Jr%2520and%2520Karthik%2520Dantu%26entry.1292438233%3D%2520%2520The%2520visual%2520detection%2520and%2520tracking%2520of%2520surface%2520terrain%2520is%2520required%2520for%250Aspacecraft%2520to%2520safely%2520land%2520on%2520or%2520navigate%2520within%2520close%2520proximity%2520to%2520celestial%250Aobjects.%2520Current%2520approaches%2520rely%2520on%2520template%2520matching%2520with%2520pre-gathered%250Apatch-based%2520features%252C%2520which%2520are%2520expensive%2520to%2520obtain%2520and%2520a%2520limiting%2520factor%2520in%250Aperceptual%2520capability.%2520While%2520recent%2520literature%2520has%2520focused%2520on%2520in-situ%2520detection%250Amethods%2520to%2520enhance%2520navigation%2520and%2520operational%2520autonomy%252C%2520robust%2520description%2520is%250Astill%2520needed.%2520In%2520this%2520work%252C%2520we%2520explore%2520metric%2520learning%2520as%2520the%2520lightweight%250Afeature%2520description%2520mechanism%2520and%2520find%2520that%2520current%2520solutions%2520fail%2520to%2520address%250Ainter-class%2520similarity%2520and%2520multi-view%2520observational%2520geometry.%2520We%2520attribute%2520this%250Ato%2520the%2520view-unaware%2520attention%2520mechanism%2520and%2520introduce%2520Multi-view%2520Attention%250ARegularizations%2520%2528MARs%2529%2520to%2520constrain%2520the%2520channel%2520and%2520spatial%2520attention%2520across%250Amultiple%2520feature%2520views%252C%2520regularizing%2520the%2520what%2520and%2520where%2520of%2520attention%2520focus.%2520We%250Athoroughly%2520analyze%2520many%2520modern%2520metric%2520learning%2520losses%2520with%2520and%2520without%2520MARs%2520and%250Ademonstrate%2520improved%2520terrain-feature%2520recognition%2520performance%2520by%2520upwards%2520of%252085%2525.%250AWe%2520additionally%2520introduce%2520the%2520Luna-1%2520dataset%252C%2520consisting%2520of%2520Moon%2520crater%250Alandmarks%2520and%2520reference%2520navigation%2520frames%2520from%2520NASA%2520mission%2520data%2520to%2520support%250Afuture%2520research%2520in%2520this%2520difficult%2520task.%2520Luna-1%2520and%2520source%2520code%2520are%2520publicly%250Aavailable%2520at%2520https%253A//droneslab.github.io/mars/.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2410.05182v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=MARs%3A%20Multi-view%20Attention%20Regularizations%20for%20Patch-based%20Feature%0A%20%20Recognition%20of%20Space%20Terrain&entry.906535625=Timothy%20Chase%20Jr%20and%20Karthik%20Dantu&entry.1292438233=%20%20The%20visual%20detection%20and%20tracking%20of%20surface%20terrain%20is%20required%20for%0Aspacecraft%20to%20safely%20land%20on%20or%20navigate%20within%20close%20proximity%20to%20celestial%0Aobjects.%20Current%20approaches%20rely%20on%20template%20matching%20with%20pre-gathered%0Apatch-based%20features%2C%20which%20are%20expensive%20to%20obtain%20and%20a%20limiting%20factor%20in%0Aperceptual%20capability.%20While%20recent%20literature%20has%20focused%20on%20in-situ%20detection%0Amethods%20to%20enhance%20navigation%20and%20operational%20autonomy%2C%20robust%20description%20is%0Astill%20needed.%20In%20this%20work%2C%20we%20explore%20metric%20learning%20as%20the%20lightweight%0Afeature%20description%20mechanism%20and%20find%20that%20current%20solutions%20fail%20to%20address%0Ainter-class%20similarity%20and%20multi-view%20observational%20geometry.%20We%20attribute%20this%0Ato%20the%20view-unaware%20attention%20mechanism%20and%20introduce%20Multi-view%20Attention%0ARegularizations%20%28MARs%29%20to%20constrain%20the%20channel%20and%20spatial%20attention%20across%0Amultiple%20feature%20views%2C%20regularizing%20the%20what%20and%20where%20of%20attention%20focus.%20We%0Athoroughly%20analyze%20many%20modern%20metric%20learning%20losses%20with%20and%20without%20MARs%20and%0Ademonstrate%20improved%20terrain-feature%20recognition%20performance%20by%20upwards%20of%2085%25.%0AWe%20additionally%20introduce%20the%20Luna-1%20dataset%2C%20consisting%20of%20Moon%20crater%0Alandmarks%20and%20reference%20navigation%20frames%20from%20NASA%20mission%20data%20to%20support%0Afuture%20research%20in%20this%20difficult%20task.%20Luna-1%20and%20source%20code%20are%20publicly%0Aavailable%20at%20https%3A//droneslab.github.io/mars/.%0A&entry.1838667208=http%3A//arxiv.org/abs/2410.05182v1&entry.124074799=Read"},
{"title": "Grounding Partially-Defined Events in Multimodal Data", "author": "Kate Sanders and Reno Kriz and David Etter and Hannah Recknor and Alexander Martin and Cameron Carpenter and Jingyang Lin and Benjamin Van Durme", "abstract": "  How are we able to learn about complex current events just from short\nsnippets of video? While natural language enables straightforward ways to\nrepresent under-specified, partially observable events, visual data does not\nfacilitate analogous methods and, consequently, introduces unique challenges in\nevent understanding. With the growing prevalence of vision-capable AI agents,\nthese systems must be able to model events from collections of unstructured\nvideo data. To tackle robust event modeling in multimodal settings, we\nintroduce a multimodal formulation for partially-defined events and cast the\nextraction of these events as a three-stage span retrieval task. We propose a\ncorresponding benchmark for this task, MultiVENT-G, that consists of 14.5 hours\nof densely annotated current event videos and 1,168 text documents, containing\n22.8K labeled event-centric entities. We propose a collection of LLM-driven\napproaches to the task of multimodal event analysis, and evaluate them on\nMultiVENT-G. Results illustrate the challenges that abstract event\nunderstanding poses and demonstrates promise in event-centric video-language\nsystems.\n", "link": "http://arxiv.org/abs/2410.05267v1", "date": "2024-10-07", "relevancy": 2.3161, "topK": [{"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.5946}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5759}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5759}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Grounding%20Partially-Defined%20Events%20in%20Multimodal%20Data&body=Title%3A%20Grounding%20Partially-Defined%20Events%20in%20Multimodal%20Data%0AAuthor%3A%20Kate%20Sanders%20and%20Reno%20Kriz%20and%20David%20Etter%20and%20Hannah%20Recknor%20and%20Alexander%20Martin%20and%20Cameron%20Carpenter%20and%20Jingyang%20Lin%20and%20Benjamin%20Van%20Durme%0AAbstract%3A%20%20%20How%20are%20we%20able%20to%20learn%20about%20complex%20current%20events%20just%20from%20short%0Asnippets%20of%20video%3F%20While%20natural%20language%20enables%20straightforward%20ways%20to%0Arepresent%20under-specified%2C%20partially%20observable%20events%2C%20visual%20data%20does%20not%0Afacilitate%20analogous%20methods%20and%2C%20consequently%2C%20introduces%20unique%20challenges%20in%0Aevent%20understanding.%20With%20the%20growing%20prevalence%20of%20vision-capable%20AI%20agents%2C%0Athese%20systems%20must%20be%20able%20to%20model%20events%20from%20collections%20of%20unstructured%0Avideo%20data.%20To%20tackle%20robust%20event%20modeling%20in%20multimodal%20settings%2C%20we%0Aintroduce%20a%20multimodal%20formulation%20for%20partially-defined%20events%20and%20cast%20the%0Aextraction%20of%20these%20events%20as%20a%20three-stage%20span%20retrieval%20task.%20We%20propose%20a%0Acorresponding%20benchmark%20for%20this%20task%2C%20MultiVENT-G%2C%20that%20consists%20of%2014.5%20hours%0Aof%20densely%20annotated%20current%20event%20videos%20and%201%2C168%20text%20documents%2C%20containing%0A22.8K%20labeled%20event-centric%20entities.%20We%20propose%20a%20collection%20of%20LLM-driven%0Aapproaches%20to%20the%20task%20of%20multimodal%20event%20analysis%2C%20and%20evaluate%20them%20on%0AMultiVENT-G.%20Results%20illustrate%20the%20challenges%20that%20abstract%20event%0Aunderstanding%20poses%20and%20demonstrates%20promise%20in%20event-centric%20video-language%0Asystems.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2410.05267v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DGrounding%2520Partially-Defined%2520Events%2520in%2520Multimodal%2520Data%26entry.906535625%3DKate%2520Sanders%2520and%2520Reno%2520Kriz%2520and%2520David%2520Etter%2520and%2520Hannah%2520Recknor%2520and%2520Alexander%2520Martin%2520and%2520Cameron%2520Carpenter%2520and%2520Jingyang%2520Lin%2520and%2520Benjamin%2520Van%2520Durme%26entry.1292438233%3D%2520%2520How%2520are%2520we%2520able%2520to%2520learn%2520about%2520complex%2520current%2520events%2520just%2520from%2520short%250Asnippets%2520of%2520video%253F%2520While%2520natural%2520language%2520enables%2520straightforward%2520ways%2520to%250Arepresent%2520under-specified%252C%2520partially%2520observable%2520events%252C%2520visual%2520data%2520does%2520not%250Afacilitate%2520analogous%2520methods%2520and%252C%2520consequently%252C%2520introduces%2520unique%2520challenges%2520in%250Aevent%2520understanding.%2520With%2520the%2520growing%2520prevalence%2520of%2520vision-capable%2520AI%2520agents%252C%250Athese%2520systems%2520must%2520be%2520able%2520to%2520model%2520events%2520from%2520collections%2520of%2520unstructured%250Avideo%2520data.%2520To%2520tackle%2520robust%2520event%2520modeling%2520in%2520multimodal%2520settings%252C%2520we%250Aintroduce%2520a%2520multimodal%2520formulation%2520for%2520partially-defined%2520events%2520and%2520cast%2520the%250Aextraction%2520of%2520these%2520events%2520as%2520a%2520three-stage%2520span%2520retrieval%2520task.%2520We%2520propose%2520a%250Acorresponding%2520benchmark%2520for%2520this%2520task%252C%2520MultiVENT-G%252C%2520that%2520consists%2520of%252014.5%2520hours%250Aof%2520densely%2520annotated%2520current%2520event%2520videos%2520and%25201%252C168%2520text%2520documents%252C%2520containing%250A22.8K%2520labeled%2520event-centric%2520entities.%2520We%2520propose%2520a%2520collection%2520of%2520LLM-driven%250Aapproaches%2520to%2520the%2520task%2520of%2520multimodal%2520event%2520analysis%252C%2520and%2520evaluate%2520them%2520on%250AMultiVENT-G.%2520Results%2520illustrate%2520the%2520challenges%2520that%2520abstract%2520event%250Aunderstanding%2520poses%2520and%2520demonstrates%2520promise%2520in%2520event-centric%2520video-language%250Asystems.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2410.05267v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Grounding%20Partially-Defined%20Events%20in%20Multimodal%20Data&entry.906535625=Kate%20Sanders%20and%20Reno%20Kriz%20and%20David%20Etter%20and%20Hannah%20Recknor%20and%20Alexander%20Martin%20and%20Cameron%20Carpenter%20and%20Jingyang%20Lin%20and%20Benjamin%20Van%20Durme&entry.1292438233=%20%20How%20are%20we%20able%20to%20learn%20about%20complex%20current%20events%20just%20from%20short%0Asnippets%20of%20video%3F%20While%20natural%20language%20enables%20straightforward%20ways%20to%0Arepresent%20under-specified%2C%20partially%20observable%20events%2C%20visual%20data%20does%20not%0Afacilitate%20analogous%20methods%20and%2C%20consequently%2C%20introduces%20unique%20challenges%20in%0Aevent%20understanding.%20With%20the%20growing%20prevalence%20of%20vision-capable%20AI%20agents%2C%0Athese%20systems%20must%20be%20able%20to%20model%20events%20from%20collections%20of%20unstructured%0Avideo%20data.%20To%20tackle%20robust%20event%20modeling%20in%20multimodal%20settings%2C%20we%0Aintroduce%20a%20multimodal%20formulation%20for%20partially-defined%20events%20and%20cast%20the%0Aextraction%20of%20these%20events%20as%20a%20three-stage%20span%20retrieval%20task.%20We%20propose%20a%0Acorresponding%20benchmark%20for%20this%20task%2C%20MultiVENT-G%2C%20that%20consists%20of%2014.5%20hours%0Aof%20densely%20annotated%20current%20event%20videos%20and%201%2C168%20text%20documents%2C%20containing%0A22.8K%20labeled%20event-centric%20entities.%20We%20propose%20a%20collection%20of%20LLM-driven%0Aapproaches%20to%20the%20task%20of%20multimodal%20event%20analysis%2C%20and%20evaluate%20them%20on%0AMultiVENT-G.%20Results%20illustrate%20the%20challenges%20that%20abstract%20event%0Aunderstanding%20poses%20and%20demonstrates%20promise%20in%20event-centric%20video-language%0Asystems.%0A&entry.1838667208=http%3A//arxiv.org/abs/2410.05267v1&entry.124074799=Read"},
{"title": "Human-Feedback Efficient Reinforcement Learning for Online Diffusion\n  Model Finetuning", "author": "Ayano Hiranaka and Shang-Fu Chen and Chieh-Hsin Lai and Dongjun Kim and Naoki Murata and Takashi Shibuya and Wei-Hsiang Liao and Shao-Hua Sun and Yuki Mitsufuji", "abstract": "  Controllable generation through Stable Diffusion (SD) fine-tuning aims to\nimprove fidelity, safety, and alignment with human guidance. Existing\nreinforcement learning from human feedback methods usually rely on predefined\nheuristic reward functions or pretrained reward models built on large-scale\ndatasets, limiting their applicability to scenarios where collecting such data\nis costly or difficult. To effectively and efficiently utilize human feedback,\nwe develop a framework, HERO, which leverages online human feedback collected\non the fly during model learning. Specifically, HERO features two key\nmechanisms: (1) Feedback-Aligned Representation Learning, an online training\nmethod that captures human feedback and provides informative learning signals\nfor fine-tuning, and (2) Feedback-Guided Image Generation, which involves\ngenerating images from SD's refined initialization samples, enabling faster\nconvergence towards the evaluator's intent. We demonstrate that HERO is 4x more\nefficient in online feedback for body part anomaly correction compared to the\nbest existing method. Additionally, experiments show that HERO can effectively\nhandle tasks like reasoning, counting, personalization, and reducing NSFW\ncontent with only 0.5K online feedback.\n", "link": "http://arxiv.org/abs/2410.05116v1", "date": "2024-10-07", "relevancy": 2.2949, "topK": [{"title": "SDXS: Real-Time One-Step Latent Diffusion Models with Image Conditions", "link": "http://arxiv.org/abs/2403.16627v1", "similarity": 0.5748}, {"title": "ToDo: Token Downsampling for Efficient Generation of High-Resolution\n  Images", "link": "http://arxiv.org/abs/2402.13573v2", "similarity": 0.5746}, {"title": "Tuning-Free Noise Rectification for High Fidelity Image-to-Video\n  Generation", "link": "http://arxiv.org/abs/2403.02827v1", "similarity": 0.5724}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Human-Feedback%20Efficient%20Reinforcement%20Learning%20for%20Online%20Diffusion%0A%20%20Model%20Finetuning&body=Title%3A%20Human-Feedback%20Efficient%20Reinforcement%20Learning%20for%20Online%20Diffusion%0A%20%20Model%20Finetuning%0AAuthor%3A%20Ayano%20Hiranaka%20and%20Shang-Fu%20Chen%20and%20Chieh-Hsin%20Lai%20and%20Dongjun%20Kim%20and%20Naoki%20Murata%20and%20Takashi%20Shibuya%20and%20Wei-Hsiang%20Liao%20and%20Shao-Hua%20Sun%20and%20Yuki%20Mitsufuji%0AAbstract%3A%20%20%20Controllable%20generation%20through%20Stable%20Diffusion%20%28SD%29%20fine-tuning%20aims%20to%0Aimprove%20fidelity%2C%20safety%2C%20and%20alignment%20with%20human%20guidance.%20Existing%0Areinforcement%20learning%20from%20human%20feedback%20methods%20usually%20rely%20on%20predefined%0Aheuristic%20reward%20functions%20or%20pretrained%20reward%20models%20built%20on%20large-scale%0Adatasets%2C%20limiting%20their%20applicability%20to%20scenarios%20where%20collecting%20such%20data%0Ais%20costly%20or%20difficult.%20To%20effectively%20and%20efficiently%20utilize%20human%20feedback%2C%0Awe%20develop%20a%20framework%2C%20HERO%2C%20which%20leverages%20online%20human%20feedback%20collected%0Aon%20the%20fly%20during%20model%20learning.%20Specifically%2C%20HERO%20features%20two%20key%0Amechanisms%3A%20%281%29%20Feedback-Aligned%20Representation%20Learning%2C%20an%20online%20training%0Amethod%20that%20captures%20human%20feedback%20and%20provides%20informative%20learning%20signals%0Afor%20fine-tuning%2C%20and%20%282%29%20Feedback-Guided%20Image%20Generation%2C%20which%20involves%0Agenerating%20images%20from%20SD%27s%20refined%20initialization%20samples%2C%20enabling%20faster%0Aconvergence%20towards%20the%20evaluator%27s%20intent.%20We%20demonstrate%20that%20HERO%20is%204x%20more%0Aefficient%20in%20online%20feedback%20for%20body%20part%20anomaly%20correction%20compared%20to%20the%0Abest%20existing%20method.%20Additionally%2C%20experiments%20show%20that%20HERO%20can%20effectively%0Ahandle%20tasks%20like%20reasoning%2C%20counting%2C%20personalization%2C%20and%20reducing%20NSFW%0Acontent%20with%20only%200.5K%20online%20feedback.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2410.05116v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DHuman-Feedback%2520Efficient%2520Reinforcement%2520Learning%2520for%2520Online%2520Diffusion%250A%2520%2520Model%2520Finetuning%26entry.906535625%3DAyano%2520Hiranaka%2520and%2520Shang-Fu%2520Chen%2520and%2520Chieh-Hsin%2520Lai%2520and%2520Dongjun%2520Kim%2520and%2520Naoki%2520Murata%2520and%2520Takashi%2520Shibuya%2520and%2520Wei-Hsiang%2520Liao%2520and%2520Shao-Hua%2520Sun%2520and%2520Yuki%2520Mitsufuji%26entry.1292438233%3D%2520%2520Controllable%2520generation%2520through%2520Stable%2520Diffusion%2520%2528SD%2529%2520fine-tuning%2520aims%2520to%250Aimprove%2520fidelity%252C%2520safety%252C%2520and%2520alignment%2520with%2520human%2520guidance.%2520Existing%250Areinforcement%2520learning%2520from%2520human%2520feedback%2520methods%2520usually%2520rely%2520on%2520predefined%250Aheuristic%2520reward%2520functions%2520or%2520pretrained%2520reward%2520models%2520built%2520on%2520large-scale%250Adatasets%252C%2520limiting%2520their%2520applicability%2520to%2520scenarios%2520where%2520collecting%2520such%2520data%250Ais%2520costly%2520or%2520difficult.%2520To%2520effectively%2520and%2520efficiently%2520utilize%2520human%2520feedback%252C%250Awe%2520develop%2520a%2520framework%252C%2520HERO%252C%2520which%2520leverages%2520online%2520human%2520feedback%2520collected%250Aon%2520the%2520fly%2520during%2520model%2520learning.%2520Specifically%252C%2520HERO%2520features%2520two%2520key%250Amechanisms%253A%2520%25281%2529%2520Feedback-Aligned%2520Representation%2520Learning%252C%2520an%2520online%2520training%250Amethod%2520that%2520captures%2520human%2520feedback%2520and%2520provides%2520informative%2520learning%2520signals%250Afor%2520fine-tuning%252C%2520and%2520%25282%2529%2520Feedback-Guided%2520Image%2520Generation%252C%2520which%2520involves%250Agenerating%2520images%2520from%2520SD%2527s%2520refined%2520initialization%2520samples%252C%2520enabling%2520faster%250Aconvergence%2520towards%2520the%2520evaluator%2527s%2520intent.%2520We%2520demonstrate%2520that%2520HERO%2520is%25204x%2520more%250Aefficient%2520in%2520online%2520feedback%2520for%2520body%2520part%2520anomaly%2520correction%2520compared%2520to%2520the%250Abest%2520existing%2520method.%2520Additionally%252C%2520experiments%2520show%2520that%2520HERO%2520can%2520effectively%250Ahandle%2520tasks%2520like%2520reasoning%252C%2520counting%252C%2520personalization%252C%2520and%2520reducing%2520NSFW%250Acontent%2520with%2520only%25200.5K%2520online%2520feedback.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2410.05116v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Human-Feedback%20Efficient%20Reinforcement%20Learning%20for%20Online%20Diffusion%0A%20%20Model%20Finetuning&entry.906535625=Ayano%20Hiranaka%20and%20Shang-Fu%20Chen%20and%20Chieh-Hsin%20Lai%20and%20Dongjun%20Kim%20and%20Naoki%20Murata%20and%20Takashi%20Shibuya%20and%20Wei-Hsiang%20Liao%20and%20Shao-Hua%20Sun%20and%20Yuki%20Mitsufuji&entry.1292438233=%20%20Controllable%20generation%20through%20Stable%20Diffusion%20%28SD%29%20fine-tuning%20aims%20to%0Aimprove%20fidelity%2C%20safety%2C%20and%20alignment%20with%20human%20guidance.%20Existing%0Areinforcement%20learning%20from%20human%20feedback%20methods%20usually%20rely%20on%20predefined%0Aheuristic%20reward%20functions%20or%20pretrained%20reward%20models%20built%20on%20large-scale%0Adatasets%2C%20limiting%20their%20applicability%20to%20scenarios%20where%20collecting%20such%20data%0Ais%20costly%20or%20difficult.%20To%20effectively%20and%20efficiently%20utilize%20human%20feedback%2C%0Awe%20develop%20a%20framework%2C%20HERO%2C%20which%20leverages%20online%20human%20feedback%20collected%0Aon%20the%20fly%20during%20model%20learning.%20Specifically%2C%20HERO%20features%20two%20key%0Amechanisms%3A%20%281%29%20Feedback-Aligned%20Representation%20Learning%2C%20an%20online%20training%0Amethod%20that%20captures%20human%20feedback%20and%20provides%20informative%20learning%20signals%0Afor%20fine-tuning%2C%20and%20%282%29%20Feedback-Guided%20Image%20Generation%2C%20which%20involves%0Agenerating%20images%20from%20SD%27s%20refined%20initialization%20samples%2C%20enabling%20faster%0Aconvergence%20towards%20the%20evaluator%27s%20intent.%20We%20demonstrate%20that%20HERO%20is%204x%20more%0Aefficient%20in%20online%20feedback%20for%20body%20part%20anomaly%20correction%20compared%20to%20the%0Abest%20existing%20method.%20Additionally%2C%20experiments%20show%20that%20HERO%20can%20effectively%0Ahandle%20tasks%20like%20reasoning%2C%20counting%2C%20personalization%2C%20and%20reducing%20NSFW%0Acontent%20with%20only%200.5K%20online%20feedback.%0A&entry.1838667208=http%3A//arxiv.org/abs/2410.05116v1&entry.124074799=Read"},
{"title": "Fine-Tuning CLIP's Last Visual Projector: A Few-Shot Cornucopia", "author": "Mohammad Fahes and Tuan-Hung Vu and Andrei Bursuc and Patrick P\u00e9rez and Raoul de Charette", "abstract": "  We consider the problem of adapting a contrastively pretrained\nvision-language model like CLIP (Radford et al., 2021) for few-shot\nclassification. The existing literature addresses this problem by learning a\nlinear classifier of the frozen visual features, optimizing word embeddings, or\nlearning external feature adapters. This paper introduces an alternative way\nfor CLIP adaptation without adding 'external' parameters to optimize. We find\nthat simply fine-tuning the last projection matrix of the vision encoder leads\nto strong performance compared to the existing baselines. Furthermore, we show\nthat regularizing training with the distance between the fine-tuned and\npretrained matrices adds reliability for adapting CLIP through this layer.\nPerhaps surprisingly, this approach, coined ProLIP, yields performances on par\nor better than state of the art on 11 few-shot classification benchmarks,\nfew-shot domain generalization, cross-dataset transfer and test-time\nadaptation. Code will be made available at\nhttps://github.com/astra-vision/ProLIP .\n", "link": "http://arxiv.org/abs/2410.05270v1", "date": "2024-10-07", "relevancy": 2.2909, "topK": [{"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.6057}, {"title": "Customize-A-Video: One-Shot Motion Customization of Text-to-Video\n  Diffusion Models", "link": "http://arxiv.org/abs/2402.14780v1", "similarity": 0.5515}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5482}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Fine-Tuning%20CLIP%27s%20Last%20Visual%20Projector%3A%20A%20Few-Shot%20Cornucopia&body=Title%3A%20Fine-Tuning%20CLIP%27s%20Last%20Visual%20Projector%3A%20A%20Few-Shot%20Cornucopia%0AAuthor%3A%20Mohammad%20Fahes%20and%20Tuan-Hung%20Vu%20and%20Andrei%20Bursuc%20and%20Patrick%20P%C3%A9rez%20and%20Raoul%20de%20Charette%0AAbstract%3A%20%20%20We%20consider%20the%20problem%20of%20adapting%20a%20contrastively%20pretrained%0Avision-language%20model%20like%20CLIP%20%28Radford%20et%20al.%2C%202021%29%20for%20few-shot%0Aclassification.%20The%20existing%20literature%20addresses%20this%20problem%20by%20learning%20a%0Alinear%20classifier%20of%20the%20frozen%20visual%20features%2C%20optimizing%20word%20embeddings%2C%20or%0Alearning%20external%20feature%20adapters.%20This%20paper%20introduces%20an%20alternative%20way%0Afor%20CLIP%20adaptation%20without%20adding%20%27external%27%20parameters%20to%20optimize.%20We%20find%0Athat%20simply%20fine-tuning%20the%20last%20projection%20matrix%20of%20the%20vision%20encoder%20leads%0Ato%20strong%20performance%20compared%20to%20the%20existing%20baselines.%20Furthermore%2C%20we%20show%0Athat%20regularizing%20training%20with%20the%20distance%20between%20the%20fine-tuned%20and%0Apretrained%20matrices%20adds%20reliability%20for%20adapting%20CLIP%20through%20this%20layer.%0APerhaps%20surprisingly%2C%20this%20approach%2C%20coined%20ProLIP%2C%20yields%20performances%20on%20par%0Aor%20better%20than%20state%20of%20the%20art%20on%2011%20few-shot%20classification%20benchmarks%2C%0Afew-shot%20domain%20generalization%2C%20cross-dataset%20transfer%20and%20test-time%0Aadaptation.%20Code%20will%20be%20made%20available%20at%0Ahttps%3A//github.com/astra-vision/ProLIP%20.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2410.05270v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DFine-Tuning%2520CLIP%2527s%2520Last%2520Visual%2520Projector%253A%2520A%2520Few-Shot%2520Cornucopia%26entry.906535625%3DMohammad%2520Fahes%2520and%2520Tuan-Hung%2520Vu%2520and%2520Andrei%2520Bursuc%2520and%2520Patrick%2520P%25C3%25A9rez%2520and%2520Raoul%2520de%2520Charette%26entry.1292438233%3D%2520%2520We%2520consider%2520the%2520problem%2520of%2520adapting%2520a%2520contrastively%2520pretrained%250Avision-language%2520model%2520like%2520CLIP%2520%2528Radford%2520et%2520al.%252C%25202021%2529%2520for%2520few-shot%250Aclassification.%2520The%2520existing%2520literature%2520addresses%2520this%2520problem%2520by%2520learning%2520a%250Alinear%2520classifier%2520of%2520the%2520frozen%2520visual%2520features%252C%2520optimizing%2520word%2520embeddings%252C%2520or%250Alearning%2520external%2520feature%2520adapters.%2520This%2520paper%2520introduces%2520an%2520alternative%2520way%250Afor%2520CLIP%2520adaptation%2520without%2520adding%2520%2527external%2527%2520parameters%2520to%2520optimize.%2520We%2520find%250Athat%2520simply%2520fine-tuning%2520the%2520last%2520projection%2520matrix%2520of%2520the%2520vision%2520encoder%2520leads%250Ato%2520strong%2520performance%2520compared%2520to%2520the%2520existing%2520baselines.%2520Furthermore%252C%2520we%2520show%250Athat%2520regularizing%2520training%2520with%2520the%2520distance%2520between%2520the%2520fine-tuned%2520and%250Apretrained%2520matrices%2520adds%2520reliability%2520for%2520adapting%2520CLIP%2520through%2520this%2520layer.%250APerhaps%2520surprisingly%252C%2520this%2520approach%252C%2520coined%2520ProLIP%252C%2520yields%2520performances%2520on%2520par%250Aor%2520better%2520than%2520state%2520of%2520the%2520art%2520on%252011%2520few-shot%2520classification%2520benchmarks%252C%250Afew-shot%2520domain%2520generalization%252C%2520cross-dataset%2520transfer%2520and%2520test-time%250Aadaptation.%2520Code%2520will%2520be%2520made%2520available%2520at%250Ahttps%253A//github.com/astra-vision/ProLIP%2520.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2410.05270v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Fine-Tuning%20CLIP%27s%20Last%20Visual%20Projector%3A%20A%20Few-Shot%20Cornucopia&entry.906535625=Mohammad%20Fahes%20and%20Tuan-Hung%20Vu%20and%20Andrei%20Bursuc%20and%20Patrick%20P%C3%A9rez%20and%20Raoul%20de%20Charette&entry.1292438233=%20%20We%20consider%20the%20problem%20of%20adapting%20a%20contrastively%20pretrained%0Avision-language%20model%20like%20CLIP%20%28Radford%20et%20al.%2C%202021%29%20for%20few-shot%0Aclassification.%20The%20existing%20literature%20addresses%20this%20problem%20by%20learning%20a%0Alinear%20classifier%20of%20the%20frozen%20visual%20features%2C%20optimizing%20word%20embeddings%2C%20or%0Alearning%20external%20feature%20adapters.%20This%20paper%20introduces%20an%20alternative%20way%0Afor%20CLIP%20adaptation%20without%20adding%20%27external%27%20parameters%20to%20optimize.%20We%20find%0Athat%20simply%20fine-tuning%20the%20last%20projection%20matrix%20of%20the%20vision%20encoder%20leads%0Ato%20strong%20performance%20compared%20to%20the%20existing%20baselines.%20Furthermore%2C%20we%20show%0Athat%20regularizing%20training%20with%20the%20distance%20between%20the%20fine-tuned%20and%0Apretrained%20matrices%20adds%20reliability%20for%20adapting%20CLIP%20through%20this%20layer.%0APerhaps%20surprisingly%2C%20this%20approach%2C%20coined%20ProLIP%2C%20yields%20performances%20on%20par%0Aor%20better%20than%20state%20of%20the%20art%20on%2011%20few-shot%20classification%20benchmarks%2C%0Afew-shot%20domain%20generalization%2C%20cross-dataset%20transfer%20and%20test-time%0Aadaptation.%20Code%20will%20be%20made%20available%20at%0Ahttps%3A//github.com/astra-vision/ProLIP%20.%0A&entry.1838667208=http%3A//arxiv.org/abs/2410.05270v1&entry.124074799=Read"},
{"title": "Stateful Large Language Model Serving with Pensieve", "author": "Lingfan Yu and Jinkun Lin and Jinyang Li", "abstract": "  Large Language Models (LLMs) are wildly popular today and it is important to\nserve them efficiently. Existing LLM serving systems are stateless across\nrequests. Consequently, when LLMs are used in the common setting of multi-turn\nconversations, a growing log of the conversation history must be processed\nalongside any request by the serving system at each turn, resulting in repeated\nprocessing.\n  In this paper, we design $Pensieve$, a system optimized for multi-turn\nconversation LLM serving. $Pensieve$ maintains the conversation state across\nrequests by caching previously processed history to avoid duplicate processing.\n$Pensieve$'s multi-tier caching strategy can utilize both GPU and CPU memory to\nefficiently store and retrieve cached data. $Pensieve$ also generalizes the\nrecent PagedAttention kernel to support attention between multiple input tokens\nwith a GPU cache spread over non-contiguous memory. Our evaluation shows that\n$Pensieve$ can achieve $1.14$-$3.0\\times$ the throughput of vLLM and\nTensorRT-LLM and significantly reduce latency.\n", "link": "http://arxiv.org/abs/2312.05516v3", "date": "2024-10-07", "relevancy": 2.2897, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.4663}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.4663}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.4412}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Stateful%20Large%20Language%20Model%20Serving%20with%20Pensieve&body=Title%3A%20Stateful%20Large%20Language%20Model%20Serving%20with%20Pensieve%0AAuthor%3A%20Lingfan%20Yu%20and%20Jinkun%20Lin%20and%20Jinyang%20Li%0AAbstract%3A%20%20%20Large%20Language%20Models%20%28LLMs%29%20are%20wildly%20popular%20today%20and%20it%20is%20important%20to%0Aserve%20them%20efficiently.%20Existing%20LLM%20serving%20systems%20are%20stateless%20across%0Arequests.%20Consequently%2C%20when%20LLMs%20are%20used%20in%20the%20common%20setting%20of%20multi-turn%0Aconversations%2C%20a%20growing%20log%20of%20the%20conversation%20history%20must%20be%20processed%0Aalongside%20any%20request%20by%20the%20serving%20system%20at%20each%20turn%2C%20resulting%20in%20repeated%0Aprocessing.%0A%20%20In%20this%20paper%2C%20we%20design%20%24Pensieve%24%2C%20a%20system%20optimized%20for%20multi-turn%0Aconversation%20LLM%20serving.%20%24Pensieve%24%20maintains%20the%20conversation%20state%20across%0Arequests%20by%20caching%20previously%20processed%20history%20to%20avoid%20duplicate%20processing.%0A%24Pensieve%24%27s%20multi-tier%20caching%20strategy%20can%20utilize%20both%20GPU%20and%20CPU%20memory%20to%0Aefficiently%20store%20and%20retrieve%20cached%20data.%20%24Pensieve%24%20also%20generalizes%20the%0Arecent%20PagedAttention%20kernel%20to%20support%20attention%20between%20multiple%20input%20tokens%0Awith%20a%20GPU%20cache%20spread%20over%20non-contiguous%20memory.%20Our%20evaluation%20shows%20that%0A%24Pensieve%24%20can%20achieve%20%241.14%24-%243.0%5Ctimes%24%20the%20throughput%20of%20vLLM%20and%0ATensorRT-LLM%20and%20significantly%20reduce%20latency.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2312.05516v3%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DStateful%2520Large%2520Language%2520Model%2520Serving%2520with%2520Pensieve%26entry.906535625%3DLingfan%2520Yu%2520and%2520Jinkun%2520Lin%2520and%2520Jinyang%2520Li%26entry.1292438233%3D%2520%2520Large%2520Language%2520Models%2520%2528LLMs%2529%2520are%2520wildly%2520popular%2520today%2520and%2520it%2520is%2520important%2520to%250Aserve%2520them%2520efficiently.%2520Existing%2520LLM%2520serving%2520systems%2520are%2520stateless%2520across%250Arequests.%2520Consequently%252C%2520when%2520LLMs%2520are%2520used%2520in%2520the%2520common%2520setting%2520of%2520multi-turn%250Aconversations%252C%2520a%2520growing%2520log%2520of%2520the%2520conversation%2520history%2520must%2520be%2520processed%250Aalongside%2520any%2520request%2520by%2520the%2520serving%2520system%2520at%2520each%2520turn%252C%2520resulting%2520in%2520repeated%250Aprocessing.%250A%2520%2520In%2520this%2520paper%252C%2520we%2520design%2520%2524Pensieve%2524%252C%2520a%2520system%2520optimized%2520for%2520multi-turn%250Aconversation%2520LLM%2520serving.%2520%2524Pensieve%2524%2520maintains%2520the%2520conversation%2520state%2520across%250Arequests%2520by%2520caching%2520previously%2520processed%2520history%2520to%2520avoid%2520duplicate%2520processing.%250A%2524Pensieve%2524%2527s%2520multi-tier%2520caching%2520strategy%2520can%2520utilize%2520both%2520GPU%2520and%2520CPU%2520memory%2520to%250Aefficiently%2520store%2520and%2520retrieve%2520cached%2520data.%2520%2524Pensieve%2524%2520also%2520generalizes%2520the%250Arecent%2520PagedAttention%2520kernel%2520to%2520support%2520attention%2520between%2520multiple%2520input%2520tokens%250Awith%2520a%2520GPU%2520cache%2520spread%2520over%2520non-contiguous%2520memory.%2520Our%2520evaluation%2520shows%2520that%250A%2524Pensieve%2524%2520can%2520achieve%2520%25241.14%2524-%25243.0%255Ctimes%2524%2520the%2520throughput%2520of%2520vLLM%2520and%250ATensorRT-LLM%2520and%2520significantly%2520reduce%2520latency.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2312.05516v3%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Stateful%20Large%20Language%20Model%20Serving%20with%20Pensieve&entry.906535625=Lingfan%20Yu%20and%20Jinkun%20Lin%20and%20Jinyang%20Li&entry.1292438233=%20%20Large%20Language%20Models%20%28LLMs%29%20are%20wildly%20popular%20today%20and%20it%20is%20important%20to%0Aserve%20them%20efficiently.%20Existing%20LLM%20serving%20systems%20are%20stateless%20across%0Arequests.%20Consequently%2C%20when%20LLMs%20are%20used%20in%20the%20common%20setting%20of%20multi-turn%0Aconversations%2C%20a%20growing%20log%20of%20the%20conversation%20history%20must%20be%20processed%0Aalongside%20any%20request%20by%20the%20serving%20system%20at%20each%20turn%2C%20resulting%20in%20repeated%0Aprocessing.%0A%20%20In%20this%20paper%2C%20we%20design%20%24Pensieve%24%2C%20a%20system%20optimized%20for%20multi-turn%0Aconversation%20LLM%20serving.%20%24Pensieve%24%20maintains%20the%20conversation%20state%20across%0Arequests%20by%20caching%20previously%20processed%20history%20to%20avoid%20duplicate%20processing.%0A%24Pensieve%24%27s%20multi-tier%20caching%20strategy%20can%20utilize%20both%20GPU%20and%20CPU%20memory%20to%0Aefficiently%20store%20and%20retrieve%20cached%20data.%20%24Pensieve%24%20also%20generalizes%20the%0Arecent%20PagedAttention%20kernel%20to%20support%20attention%20between%20multiple%20input%20tokens%0Awith%20a%20GPU%20cache%20spread%20over%20non-contiguous%20memory.%20Our%20evaluation%20shows%20that%0A%24Pensieve%24%20can%20achieve%20%241.14%24-%243.0%5Ctimes%24%20the%20throughput%20of%20vLLM%20and%0ATensorRT-LLM%20and%20significantly%20reduce%20latency.%0A&entry.1838667208=http%3A//arxiv.org/abs/2312.05516v3&entry.124074799=Read"},
{"title": "Real-Time Truly-Coupled Lidar-Inertial Motion Correction and\n  Spatiotemporal Dynamic Object Detection", "author": "Cedric Le Gentil and Raphael Falque and Teresa Vidal-Calleja", "abstract": "  Over the past decade, lidars have become a cornerstone of robotics state\nestimation and perception thanks to their ability to provide accurate geometric\ninformation about their surroundings in the form of 3D scans. Unfortunately,\nmost of nowadays lidars do not take snapshots of the environment but sweep the\nenvironment over a period of time (typically around 100 ms). Such a\nrolling-shutter-like mechanism introduces motion distortion into the collected\nlidar scan, thus hindering downstream perception applications. In this paper,\nwe present a novel method for motion distortion correction of lidar data by\ntightly coupling lidar with Inertial Measurement Unit (IMU) data. The\nmotivation of this work is a map-free dynamic object detection based on lidar.\nThe proposed lidar data undistortion method relies on continuous preintegrated\nof IMU measurements that allow parameterising the sensors' continuous 6-DoF\ntrajectory using solely eleven discrete state variables (biases, initial\nvelocity, and gravity direction). The undistortion consists of feature-based\ndistance minimisation of point-to-line and point-to-plane residuals in a\nnon-linear least-square formulation. Given undistorted geometric data over a\nshort temporal window, the proposed pipeline computes the spatiotemporal normal\nvector of each of the lidar points. The temporal component of the normals is a\nproxy for the corresponding point's velocity, therefore allowing for\nlearning-free dynamic object classification without the need for registration\nin a global reference frame. We demonstrate the soundness of the proposed\nmethod and its different components using public datasets and compare them with\nstate-of-the-art lidar-inertial state estimation and dynamic object detection\nalgorithms.\n", "link": "http://arxiv.org/abs/2410.05152v1", "date": "2024-10-07", "relevancy": 2.2863, "topK": [{"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.5785}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.5745}, {"title": "DDM-NET: End-to-end learning of keypoint feature Detection, Description and Matching for 3D localization", "link": "https://arxiv.org/abs/2212.04575", "similarity": 0.5634}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Real-Time%20Truly-Coupled%20Lidar-Inertial%20Motion%20Correction%20and%0A%20%20Spatiotemporal%20Dynamic%20Object%20Detection&body=Title%3A%20Real-Time%20Truly-Coupled%20Lidar-Inertial%20Motion%20Correction%20and%0A%20%20Spatiotemporal%20Dynamic%20Object%20Detection%0AAuthor%3A%20Cedric%20Le%20Gentil%20and%20Raphael%20Falque%20and%20Teresa%20Vidal-Calleja%0AAbstract%3A%20%20%20Over%20the%20past%20decade%2C%20lidars%20have%20become%20a%20cornerstone%20of%20robotics%20state%0Aestimation%20and%20perception%20thanks%20to%20their%20ability%20to%20provide%20accurate%20geometric%0Ainformation%20about%20their%20surroundings%20in%20the%20form%20of%203D%20scans.%20Unfortunately%2C%0Amost%20of%20nowadays%20lidars%20do%20not%20take%20snapshots%20of%20the%20environment%20but%20sweep%20the%0Aenvironment%20over%20a%20period%20of%20time%20%28typically%20around%20100%20ms%29.%20Such%20a%0Arolling-shutter-like%20mechanism%20introduces%20motion%20distortion%20into%20the%20collected%0Alidar%20scan%2C%20thus%20hindering%20downstream%20perception%20applications.%20In%20this%20paper%2C%0Awe%20present%20a%20novel%20method%20for%20motion%20distortion%20correction%20of%20lidar%20data%20by%0Atightly%20coupling%20lidar%20with%20Inertial%20Measurement%20Unit%20%28IMU%29%20data.%20The%0Amotivation%20of%20this%20work%20is%20a%20map-free%20dynamic%20object%20detection%20based%20on%20lidar.%0AThe%20proposed%20lidar%20data%20undistortion%20method%20relies%20on%20continuous%20preintegrated%0Aof%20IMU%20measurements%20that%20allow%20parameterising%20the%20sensors%27%20continuous%206-DoF%0Atrajectory%20using%20solely%20eleven%20discrete%20state%20variables%20%28biases%2C%20initial%0Avelocity%2C%20and%20gravity%20direction%29.%20The%20undistortion%20consists%20of%20feature-based%0Adistance%20minimisation%20of%20point-to-line%20and%20point-to-plane%20residuals%20in%20a%0Anon-linear%20least-square%20formulation.%20Given%20undistorted%20geometric%20data%20over%20a%0Ashort%20temporal%20window%2C%20the%20proposed%20pipeline%20computes%20the%20spatiotemporal%20normal%0Avector%20of%20each%20of%20the%20lidar%20points.%20The%20temporal%20component%20of%20the%20normals%20is%20a%0Aproxy%20for%20the%20corresponding%20point%27s%20velocity%2C%20therefore%20allowing%20for%0Alearning-free%20dynamic%20object%20classification%20without%20the%20need%20for%20registration%0Ain%20a%20global%20reference%20frame.%20We%20demonstrate%20the%20soundness%20of%20the%20proposed%0Amethod%20and%20its%20different%20components%20using%20public%20datasets%20and%20compare%20them%20with%0Astate-of-the-art%20lidar-inertial%20state%20estimation%20and%20dynamic%20object%20detection%0Aalgorithms.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2410.05152v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DReal-Time%2520Truly-Coupled%2520Lidar-Inertial%2520Motion%2520Correction%2520and%250A%2520%2520Spatiotemporal%2520Dynamic%2520Object%2520Detection%26entry.906535625%3DCedric%2520Le%2520Gentil%2520and%2520Raphael%2520Falque%2520and%2520Teresa%2520Vidal-Calleja%26entry.1292438233%3D%2520%2520Over%2520the%2520past%2520decade%252C%2520lidars%2520have%2520become%2520a%2520cornerstone%2520of%2520robotics%2520state%250Aestimation%2520and%2520perception%2520thanks%2520to%2520their%2520ability%2520to%2520provide%2520accurate%2520geometric%250Ainformation%2520about%2520their%2520surroundings%2520in%2520the%2520form%2520of%25203D%2520scans.%2520Unfortunately%252C%250Amost%2520of%2520nowadays%2520lidars%2520do%2520not%2520take%2520snapshots%2520of%2520the%2520environment%2520but%2520sweep%2520the%250Aenvironment%2520over%2520a%2520period%2520of%2520time%2520%2528typically%2520around%2520100%2520ms%2529.%2520Such%2520a%250Arolling-shutter-like%2520mechanism%2520introduces%2520motion%2520distortion%2520into%2520the%2520collected%250Alidar%2520scan%252C%2520thus%2520hindering%2520downstream%2520perception%2520applications.%2520In%2520this%2520paper%252C%250Awe%2520present%2520a%2520novel%2520method%2520for%2520motion%2520distortion%2520correction%2520of%2520lidar%2520data%2520by%250Atightly%2520coupling%2520lidar%2520with%2520Inertial%2520Measurement%2520Unit%2520%2528IMU%2529%2520data.%2520The%250Amotivation%2520of%2520this%2520work%2520is%2520a%2520map-free%2520dynamic%2520object%2520detection%2520based%2520on%2520lidar.%250AThe%2520proposed%2520lidar%2520data%2520undistortion%2520method%2520relies%2520on%2520continuous%2520preintegrated%250Aof%2520IMU%2520measurements%2520that%2520allow%2520parameterising%2520the%2520sensors%2527%2520continuous%25206-DoF%250Atrajectory%2520using%2520solely%2520eleven%2520discrete%2520state%2520variables%2520%2528biases%252C%2520initial%250Avelocity%252C%2520and%2520gravity%2520direction%2529.%2520The%2520undistortion%2520consists%2520of%2520feature-based%250Adistance%2520minimisation%2520of%2520point-to-line%2520and%2520point-to-plane%2520residuals%2520in%2520a%250Anon-linear%2520least-square%2520formulation.%2520Given%2520undistorted%2520geometric%2520data%2520over%2520a%250Ashort%2520temporal%2520window%252C%2520the%2520proposed%2520pipeline%2520computes%2520the%2520spatiotemporal%2520normal%250Avector%2520of%2520each%2520of%2520the%2520lidar%2520points.%2520The%2520temporal%2520component%2520of%2520the%2520normals%2520is%2520a%250Aproxy%2520for%2520the%2520corresponding%2520point%2527s%2520velocity%252C%2520therefore%2520allowing%2520for%250Alearning-free%2520dynamic%2520object%2520classification%2520without%2520the%2520need%2520for%2520registration%250Ain%2520a%2520global%2520reference%2520frame.%2520We%2520demonstrate%2520the%2520soundness%2520of%2520the%2520proposed%250Amethod%2520and%2520its%2520different%2520components%2520using%2520public%2520datasets%2520and%2520compare%2520them%2520with%250Astate-of-the-art%2520lidar-inertial%2520state%2520estimation%2520and%2520dynamic%2520object%2520detection%250Aalgorithms.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2410.05152v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Real-Time%20Truly-Coupled%20Lidar-Inertial%20Motion%20Correction%20and%0A%20%20Spatiotemporal%20Dynamic%20Object%20Detection&entry.906535625=Cedric%20Le%20Gentil%20and%20Raphael%20Falque%20and%20Teresa%20Vidal-Calleja&entry.1292438233=%20%20Over%20the%20past%20decade%2C%20lidars%20have%20become%20a%20cornerstone%20of%20robotics%20state%0Aestimation%20and%20perception%20thanks%20to%20their%20ability%20to%20provide%20accurate%20geometric%0Ainformation%20about%20their%20surroundings%20in%20the%20form%20of%203D%20scans.%20Unfortunately%2C%0Amost%20of%20nowadays%20lidars%20do%20not%20take%20snapshots%20of%20the%20environment%20but%20sweep%20the%0Aenvironment%20over%20a%20period%20of%20time%20%28typically%20around%20100%20ms%29.%20Such%20a%0Arolling-shutter-like%20mechanism%20introduces%20motion%20distortion%20into%20the%20collected%0Alidar%20scan%2C%20thus%20hindering%20downstream%20perception%20applications.%20In%20this%20paper%2C%0Awe%20present%20a%20novel%20method%20for%20motion%20distortion%20correction%20of%20lidar%20data%20by%0Atightly%20coupling%20lidar%20with%20Inertial%20Measurement%20Unit%20%28IMU%29%20data.%20The%0Amotivation%20of%20this%20work%20is%20a%20map-free%20dynamic%20object%20detection%20based%20on%20lidar.%0AThe%20proposed%20lidar%20data%20undistortion%20method%20relies%20on%20continuous%20preintegrated%0Aof%20IMU%20measurements%20that%20allow%20parameterising%20the%20sensors%27%20continuous%206-DoF%0Atrajectory%20using%20solely%20eleven%20discrete%20state%20variables%20%28biases%2C%20initial%0Avelocity%2C%20and%20gravity%20direction%29.%20The%20undistortion%20consists%20of%20feature-based%0Adistance%20minimisation%20of%20point-to-line%20and%20point-to-plane%20residuals%20in%20a%0Anon-linear%20least-square%20formulation.%20Given%20undistorted%20geometric%20data%20over%20a%0Ashort%20temporal%20window%2C%20the%20proposed%20pipeline%20computes%20the%20spatiotemporal%20normal%0Avector%20of%20each%20of%20the%20lidar%20points.%20The%20temporal%20component%20of%20the%20normals%20is%20a%0Aproxy%20for%20the%20corresponding%20point%27s%20velocity%2C%20therefore%20allowing%20for%0Alearning-free%20dynamic%20object%20classification%20without%20the%20need%20for%20registration%0Ain%20a%20global%20reference%20frame.%20We%20demonstrate%20the%20soundness%20of%20the%20proposed%0Amethod%20and%20its%20different%20components%20using%20public%20datasets%20and%20compare%20them%20with%0Astate-of-the-art%20lidar-inertial%20state%20estimation%20and%20dynamic%20object%20detection%0Aalgorithms.%0A&entry.1838667208=http%3A//arxiv.org/abs/2410.05152v1&entry.124074799=Read"},
{"title": "Tuning-Free Bilevel Optimization: New Algorithms and Convergence\n  Analysis", "author": "Yifan Yang and Hao Ban and Minhui Huang and Shiqian Ma and Kaiyi Ji", "abstract": "  Bilevel optimization has recently attracted considerable attention due to its\nabundant applications in machine learning problems. However, existing methods\nrely on prior knowledge of problem parameters to determine stepsizes, resulting\nin significant effort in tuning stepsizes when these parameters are unknown. In\nthis paper, we propose two novel tuning-free algorithms, D-TFBO and S-TFBO.\nD-TFBO employs a double-loop structure with stepsizes adaptively adjusted by\nthe \"inverse of cumulative gradient norms\" strategy. S-TFBO features a simpler\nfully single-loop structure that updates three variables simultaneously with a\ntheory-motivated joint design of adaptive stepsizes for all variables. We\nprovide a comprehensive convergence analysis for both algorithms and show that\nD-TFBO and S-TFBO respectively require $O(\\frac{1}{\\epsilon})$ and\n$O(\\frac{1}{\\epsilon}\\log^4(\\frac{1}{\\epsilon}))$ iterations to find an\n$\\epsilon$-accurate stationary point, (nearly) matching their well-tuned\ncounterparts using the information of problem parameters. Experiments on\nvarious problems show that our methods achieve performance comparable to\nexisting well-tuned approaches, while being more robust to the selection of\ninitial stepsizes. To the best of our knowledge, our methods are the first to\ncompletely eliminate the need for stepsize tuning, while achieving theoretical\nguarantees.\n", "link": "http://arxiv.org/abs/2410.05140v1", "date": "2024-10-07", "relevancy": 2.2857, "topK": [{"title": "Tuning-Free Noise Rectification for High Fidelity Image-to-Video\n  Generation", "link": "http://arxiv.org/abs/2403.02827v1", "similarity": 0.4993}, {"title": "Boosted dynamic neural networks", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/26302", "similarity": 0.4373}, {"title": "ToDo: Token Downsampling for Efficient Generation of High-Resolution\n  Images", "link": "http://arxiv.org/abs/2402.13573v2", "similarity": 0.4348}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Tuning-Free%20Bilevel%20Optimization%3A%20New%20Algorithms%20and%20Convergence%0A%20%20Analysis&body=Title%3A%20Tuning-Free%20Bilevel%20Optimization%3A%20New%20Algorithms%20and%20Convergence%0A%20%20Analysis%0AAuthor%3A%20Yifan%20Yang%20and%20Hao%20Ban%20and%20Minhui%20Huang%20and%20Shiqian%20Ma%20and%20Kaiyi%20Ji%0AAbstract%3A%20%20%20Bilevel%20optimization%20has%20recently%20attracted%20considerable%20attention%20due%20to%20its%0Aabundant%20applications%20in%20machine%20learning%20problems.%20However%2C%20existing%20methods%0Arely%20on%20prior%20knowledge%20of%20problem%20parameters%20to%20determine%20stepsizes%2C%20resulting%0Ain%20significant%20effort%20in%20tuning%20stepsizes%20when%20these%20parameters%20are%20unknown.%20In%0Athis%20paper%2C%20we%20propose%20two%20novel%20tuning-free%20algorithms%2C%20D-TFBO%20and%20S-TFBO.%0AD-TFBO%20employs%20a%20double-loop%20structure%20with%20stepsizes%20adaptively%20adjusted%20by%0Athe%20%22inverse%20of%20cumulative%20gradient%20norms%22%20strategy.%20S-TFBO%20features%20a%20simpler%0Afully%20single-loop%20structure%20that%20updates%20three%20variables%20simultaneously%20with%20a%0Atheory-motivated%20joint%20design%20of%20adaptive%20stepsizes%20for%20all%20variables.%20We%0Aprovide%20a%20comprehensive%20convergence%20analysis%20for%20both%20algorithms%20and%20show%20that%0AD-TFBO%20and%20S-TFBO%20respectively%20require%20%24O%28%5Cfrac%7B1%7D%7B%5Cepsilon%7D%29%24%20and%0A%24O%28%5Cfrac%7B1%7D%7B%5Cepsilon%7D%5Clog%5E4%28%5Cfrac%7B1%7D%7B%5Cepsilon%7D%29%29%24%20iterations%20to%20find%20an%0A%24%5Cepsilon%24-accurate%20stationary%20point%2C%20%28nearly%29%20matching%20their%20well-tuned%0Acounterparts%20using%20the%20information%20of%20problem%20parameters.%20Experiments%20on%0Avarious%20problems%20show%20that%20our%20methods%20achieve%20performance%20comparable%20to%0Aexisting%20well-tuned%20approaches%2C%20while%20being%20more%20robust%20to%20the%20selection%20of%0Ainitial%20stepsizes.%20To%20the%20best%20of%20our%20knowledge%2C%20our%20methods%20are%20the%20first%20to%0Acompletely%20eliminate%20the%20need%20for%20stepsize%20tuning%2C%20while%20achieving%20theoretical%0Aguarantees.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2410.05140v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DTuning-Free%2520Bilevel%2520Optimization%253A%2520New%2520Algorithms%2520and%2520Convergence%250A%2520%2520Analysis%26entry.906535625%3DYifan%2520Yang%2520and%2520Hao%2520Ban%2520and%2520Minhui%2520Huang%2520and%2520Shiqian%2520Ma%2520and%2520Kaiyi%2520Ji%26entry.1292438233%3D%2520%2520Bilevel%2520optimization%2520has%2520recently%2520attracted%2520considerable%2520attention%2520due%2520to%2520its%250Aabundant%2520applications%2520in%2520machine%2520learning%2520problems.%2520However%252C%2520existing%2520methods%250Arely%2520on%2520prior%2520knowledge%2520of%2520problem%2520parameters%2520to%2520determine%2520stepsizes%252C%2520resulting%250Ain%2520significant%2520effort%2520in%2520tuning%2520stepsizes%2520when%2520these%2520parameters%2520are%2520unknown.%2520In%250Athis%2520paper%252C%2520we%2520propose%2520two%2520novel%2520tuning-free%2520algorithms%252C%2520D-TFBO%2520and%2520S-TFBO.%250AD-TFBO%2520employs%2520a%2520double-loop%2520structure%2520with%2520stepsizes%2520adaptively%2520adjusted%2520by%250Athe%2520%2522inverse%2520of%2520cumulative%2520gradient%2520norms%2522%2520strategy.%2520S-TFBO%2520features%2520a%2520simpler%250Afully%2520single-loop%2520structure%2520that%2520updates%2520three%2520variables%2520simultaneously%2520with%2520a%250Atheory-motivated%2520joint%2520design%2520of%2520adaptive%2520stepsizes%2520for%2520all%2520variables.%2520We%250Aprovide%2520a%2520comprehensive%2520convergence%2520analysis%2520for%2520both%2520algorithms%2520and%2520show%2520that%250AD-TFBO%2520and%2520S-TFBO%2520respectively%2520require%2520%2524O%2528%255Cfrac%257B1%257D%257B%255Cepsilon%257D%2529%2524%2520and%250A%2524O%2528%255Cfrac%257B1%257D%257B%255Cepsilon%257D%255Clog%255E4%2528%255Cfrac%257B1%257D%257B%255Cepsilon%257D%2529%2529%2524%2520iterations%2520to%2520find%2520an%250A%2524%255Cepsilon%2524-accurate%2520stationary%2520point%252C%2520%2528nearly%2529%2520matching%2520their%2520well-tuned%250Acounterparts%2520using%2520the%2520information%2520of%2520problem%2520parameters.%2520Experiments%2520on%250Avarious%2520problems%2520show%2520that%2520our%2520methods%2520achieve%2520performance%2520comparable%2520to%250Aexisting%2520well-tuned%2520approaches%252C%2520while%2520being%2520more%2520robust%2520to%2520the%2520selection%2520of%250Ainitial%2520stepsizes.%2520To%2520the%2520best%2520of%2520our%2520knowledge%252C%2520our%2520methods%2520are%2520the%2520first%2520to%250Acompletely%2520eliminate%2520the%2520need%2520for%2520stepsize%2520tuning%252C%2520while%2520achieving%2520theoretical%250Aguarantees.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2410.05140v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Tuning-Free%20Bilevel%20Optimization%3A%20New%20Algorithms%20and%20Convergence%0A%20%20Analysis&entry.906535625=Yifan%20Yang%20and%20Hao%20Ban%20and%20Minhui%20Huang%20and%20Shiqian%20Ma%20and%20Kaiyi%20Ji&entry.1292438233=%20%20Bilevel%20optimization%20has%20recently%20attracted%20considerable%20attention%20due%20to%20its%0Aabundant%20applications%20in%20machine%20learning%20problems.%20However%2C%20existing%20methods%0Arely%20on%20prior%20knowledge%20of%20problem%20parameters%20to%20determine%20stepsizes%2C%20resulting%0Ain%20significant%20effort%20in%20tuning%20stepsizes%20when%20these%20parameters%20are%20unknown.%20In%0Athis%20paper%2C%20we%20propose%20two%20novel%20tuning-free%20algorithms%2C%20D-TFBO%20and%20S-TFBO.%0AD-TFBO%20employs%20a%20double-loop%20structure%20with%20stepsizes%20adaptively%20adjusted%20by%0Athe%20%22inverse%20of%20cumulative%20gradient%20norms%22%20strategy.%20S-TFBO%20features%20a%20simpler%0Afully%20single-loop%20structure%20that%20updates%20three%20variables%20simultaneously%20with%20a%0Atheory-motivated%20joint%20design%20of%20adaptive%20stepsizes%20for%20all%20variables.%20We%0Aprovide%20a%20comprehensive%20convergence%20analysis%20for%20both%20algorithms%20and%20show%20that%0AD-TFBO%20and%20S-TFBO%20respectively%20require%20%24O%28%5Cfrac%7B1%7D%7B%5Cepsilon%7D%29%24%20and%0A%24O%28%5Cfrac%7B1%7D%7B%5Cepsilon%7D%5Clog%5E4%28%5Cfrac%7B1%7D%7B%5Cepsilon%7D%29%29%24%20iterations%20to%20find%20an%0A%24%5Cepsilon%24-accurate%20stationary%20point%2C%20%28nearly%29%20matching%20their%20well-tuned%0Acounterparts%20using%20the%20information%20of%20problem%20parameters.%20Experiments%20on%0Avarious%20problems%20show%20that%20our%20methods%20achieve%20performance%20comparable%20to%0Aexisting%20well-tuned%20approaches%2C%20while%20being%20more%20robust%20to%20the%20selection%20of%0Ainitial%20stepsizes.%20To%20the%20best%20of%20our%20knowledge%2C%20our%20methods%20are%20the%20first%20to%0Acompletely%20eliminate%20the%20need%20for%20stepsize%20tuning%2C%20while%20achieving%20theoretical%0Aguarantees.%0A&entry.1838667208=http%3A//arxiv.org/abs/2410.05140v1&entry.124074799=Read"},
{"title": "Beyond FVD: Enhanced Evaluation Metrics for Video Generation Quality", "author": "Ge Ya and  Luo and Gian Favero and Zhi Hao Luo and Alexia Jolicoeur-Martineau and Christopher Pal", "abstract": "  The Fr\\'echet Video Distance (FVD) is a widely adopted metric for evaluating\nvideo generation distribution quality. However, its effectiveness relies on\ncritical assumptions. Our analysis reveals three significant limitations: (1)\nthe non-Gaussianity of the Inflated 3D Convnet (I3D) feature space; (2) the\ninsensitivity of I3D features to temporal distortions; (3) the impractical\nsample sizes required for reliable estimation. These findings undermine FVD's\nreliability and show that FVD falls short as a standalone metric for video\ngeneration evaluation. After extensive analysis of a wide range of metrics and\nbackbone architectures, we propose JEDi, the JEPA Embedding Distance, based on\nfeatures derived from a Joint Embedding Predictive Architecture, measured using\nMaximum Mean Discrepancy with polynomial kernel. Our experiments on multiple\nopen-source datasets show clear evidence that it is a superior alternative to\nthe widely used FVD metric, requiring only 16% of the samples to reach its\nsteady value, while increasing alignment with human evaluation by 34%, on\naverage.\n", "link": "http://arxiv.org/abs/2410.05203v1", "date": "2024-10-07", "relevancy": 2.2845, "topK": [{"title": "Tuning-Free Noise Rectification for High Fidelity Image-to-Video\n  Generation", "link": "http://arxiv.org/abs/2403.02827v1", "similarity": 0.5887}, {"title": "Magic-Me: Identity-Specific Video Customized Diffusion", "link": "http://arxiv.org/abs/2402.09368v1", "similarity": 0.5669}, {"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.5377}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Beyond%20FVD%3A%20Enhanced%20Evaluation%20Metrics%20for%20Video%20Generation%20Quality&body=Title%3A%20Beyond%20FVD%3A%20Enhanced%20Evaluation%20Metrics%20for%20Video%20Generation%20Quality%0AAuthor%3A%20Ge%20Ya%20and%20%20Luo%20and%20Gian%20Favero%20and%20Zhi%20Hao%20Luo%20and%20Alexia%20Jolicoeur-Martineau%20and%20Christopher%20Pal%0AAbstract%3A%20%20%20The%20Fr%5C%27echet%20Video%20Distance%20%28FVD%29%20is%20a%20widely%20adopted%20metric%20for%20evaluating%0Avideo%20generation%20distribution%20quality.%20However%2C%20its%20effectiveness%20relies%20on%0Acritical%20assumptions.%20Our%20analysis%20reveals%20three%20significant%20limitations%3A%20%281%29%0Athe%20non-Gaussianity%20of%20the%20Inflated%203D%20Convnet%20%28I3D%29%20feature%20space%3B%20%282%29%20the%0Ainsensitivity%20of%20I3D%20features%20to%20temporal%20distortions%3B%20%283%29%20the%20impractical%0Asample%20sizes%20required%20for%20reliable%20estimation.%20These%20findings%20undermine%20FVD%27s%0Areliability%20and%20show%20that%20FVD%20falls%20short%20as%20a%20standalone%20metric%20for%20video%0Ageneration%20evaluation.%20After%20extensive%20analysis%20of%20a%20wide%20range%20of%20metrics%20and%0Abackbone%20architectures%2C%20we%20propose%20JEDi%2C%20the%20JEPA%20Embedding%20Distance%2C%20based%20on%0Afeatures%20derived%20from%20a%20Joint%20Embedding%20Predictive%20Architecture%2C%20measured%20using%0AMaximum%20Mean%20Discrepancy%20with%20polynomial%20kernel.%20Our%20experiments%20on%20multiple%0Aopen-source%20datasets%20show%20clear%20evidence%20that%20it%20is%20a%20superior%20alternative%20to%0Athe%20widely%20used%20FVD%20metric%2C%20requiring%20only%2016%25%20of%20the%20samples%20to%20reach%20its%0Asteady%20value%2C%20while%20increasing%20alignment%20with%20human%20evaluation%20by%2034%25%2C%20on%0Aaverage.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2410.05203v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DBeyond%2520FVD%253A%2520Enhanced%2520Evaluation%2520Metrics%2520for%2520Video%2520Generation%2520Quality%26entry.906535625%3DGe%2520Ya%2520and%2520%2520Luo%2520and%2520Gian%2520Favero%2520and%2520Zhi%2520Hao%2520Luo%2520and%2520Alexia%2520Jolicoeur-Martineau%2520and%2520Christopher%2520Pal%26entry.1292438233%3D%2520%2520The%2520Fr%255C%2527echet%2520Video%2520Distance%2520%2528FVD%2529%2520is%2520a%2520widely%2520adopted%2520metric%2520for%2520evaluating%250Avideo%2520generation%2520distribution%2520quality.%2520However%252C%2520its%2520effectiveness%2520relies%2520on%250Acritical%2520assumptions.%2520Our%2520analysis%2520reveals%2520three%2520significant%2520limitations%253A%2520%25281%2529%250Athe%2520non-Gaussianity%2520of%2520the%2520Inflated%25203D%2520Convnet%2520%2528I3D%2529%2520feature%2520space%253B%2520%25282%2529%2520the%250Ainsensitivity%2520of%2520I3D%2520features%2520to%2520temporal%2520distortions%253B%2520%25283%2529%2520the%2520impractical%250Asample%2520sizes%2520required%2520for%2520reliable%2520estimation.%2520These%2520findings%2520undermine%2520FVD%2527s%250Areliability%2520and%2520show%2520that%2520FVD%2520falls%2520short%2520as%2520a%2520standalone%2520metric%2520for%2520video%250Ageneration%2520evaluation.%2520After%2520extensive%2520analysis%2520of%2520a%2520wide%2520range%2520of%2520metrics%2520and%250Abackbone%2520architectures%252C%2520we%2520propose%2520JEDi%252C%2520the%2520JEPA%2520Embedding%2520Distance%252C%2520based%2520on%250Afeatures%2520derived%2520from%2520a%2520Joint%2520Embedding%2520Predictive%2520Architecture%252C%2520measured%2520using%250AMaximum%2520Mean%2520Discrepancy%2520with%2520polynomial%2520kernel.%2520Our%2520experiments%2520on%2520multiple%250Aopen-source%2520datasets%2520show%2520clear%2520evidence%2520that%2520it%2520is%2520a%2520superior%2520alternative%2520to%250Athe%2520widely%2520used%2520FVD%2520metric%252C%2520requiring%2520only%252016%2525%2520of%2520the%2520samples%2520to%2520reach%2520its%250Asteady%2520value%252C%2520while%2520increasing%2520alignment%2520with%2520human%2520evaluation%2520by%252034%2525%252C%2520on%250Aaverage.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2410.05203v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Beyond%20FVD%3A%20Enhanced%20Evaluation%20Metrics%20for%20Video%20Generation%20Quality&entry.906535625=Ge%20Ya%20and%20%20Luo%20and%20Gian%20Favero%20and%20Zhi%20Hao%20Luo%20and%20Alexia%20Jolicoeur-Martineau%20and%20Christopher%20Pal&entry.1292438233=%20%20The%20Fr%5C%27echet%20Video%20Distance%20%28FVD%29%20is%20a%20widely%20adopted%20metric%20for%20evaluating%0Avideo%20generation%20distribution%20quality.%20However%2C%20its%20effectiveness%20relies%20on%0Acritical%20assumptions.%20Our%20analysis%20reveals%20three%20significant%20limitations%3A%20%281%29%0Athe%20non-Gaussianity%20of%20the%20Inflated%203D%20Convnet%20%28I3D%29%20feature%20space%3B%20%282%29%20the%0Ainsensitivity%20of%20I3D%20features%20to%20temporal%20distortions%3B%20%283%29%20the%20impractical%0Asample%20sizes%20required%20for%20reliable%20estimation.%20These%20findings%20undermine%20FVD%27s%0Areliability%20and%20show%20that%20FVD%20falls%20short%20as%20a%20standalone%20metric%20for%20video%0Ageneration%20evaluation.%20After%20extensive%20analysis%20of%20a%20wide%20range%20of%20metrics%20and%0Abackbone%20architectures%2C%20we%20propose%20JEDi%2C%20the%20JEPA%20Embedding%20Distance%2C%20based%20on%0Afeatures%20derived%20from%20a%20Joint%20Embedding%20Predictive%20Architecture%2C%20measured%20using%0AMaximum%20Mean%20Discrepancy%20with%20polynomial%20kernel.%20Our%20experiments%20on%20multiple%0Aopen-source%20datasets%20show%20clear%20evidence%20that%20it%20is%20a%20superior%20alternative%20to%0Athe%20widely%20used%20FVD%20metric%2C%20requiring%20only%2016%25%20of%20the%20samples%20to%20reach%20its%0Asteady%20value%2C%20while%20increasing%20alignment%20with%20human%20evaluation%20by%2034%25%2C%20on%0Aaverage.%0A&entry.1838667208=http%3A//arxiv.org/abs/2410.05203v1&entry.124074799=Read"},
{"title": "Conditional Variational Autoencoders for Probabilistic Pose Regression", "author": "Fereidoon Zangeneh and Leonard Bruns and Amit Dekel and Alessandro Pieropan and Patric Jensfelt", "abstract": "  Robots rely on visual relocalization to estimate their pose from camera\nimages when they lose track. One of the challenges in visual relocalization is\nrepetitive structures in the operation environment of the robot. This calls for\nprobabilistic methods that support multiple hypotheses for robot's pose. We\npropose such a probabilistic method to predict the posterior distribution of\ncamera poses given an observed image. Our proposed training strategy results in\na generative model of camera poses given an image, which can be used to draw\nsamples from the pose posterior distribution. Our method is streamlined and\nwell-founded in theory and outperforms existing methods on localization in\npresence of ambiguities.\n", "link": "http://arxiv.org/abs/2410.04989v1", "date": "2024-10-07", "relevancy": 2.2735, "topK": [{"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.632}, {"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.5759}, {"title": "Flexible Visual Recognition by Evidential Modeling of Confusion and Ignorance", "link": "http://openaccess.thecvf.com/content/ICCV2023/html/Fan_Flexible_Visual_Recognition_by_Evidential_Modeling_of_Confusion_and_Ignorance_ICCV_2023_paper.html", "similarity": 0.5354}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Conditional%20Variational%20Autoencoders%20for%20Probabilistic%20Pose%20Regression&body=Title%3A%20Conditional%20Variational%20Autoencoders%20for%20Probabilistic%20Pose%20Regression%0AAuthor%3A%20Fereidoon%20Zangeneh%20and%20Leonard%20Bruns%20and%20Amit%20Dekel%20and%20Alessandro%20Pieropan%20and%20Patric%20Jensfelt%0AAbstract%3A%20%20%20Robots%20rely%20on%20visual%20relocalization%20to%20estimate%20their%20pose%20from%20camera%0Aimages%20when%20they%20lose%20track.%20One%20of%20the%20challenges%20in%20visual%20relocalization%20is%0Arepetitive%20structures%20in%20the%20operation%20environment%20of%20the%20robot.%20This%20calls%20for%0Aprobabilistic%20methods%20that%20support%20multiple%20hypotheses%20for%20robot%27s%20pose.%20We%0Apropose%20such%20a%20probabilistic%20method%20to%20predict%20the%20posterior%20distribution%20of%0Acamera%20poses%20given%20an%20observed%20image.%20Our%20proposed%20training%20strategy%20results%20in%0Aa%20generative%20model%20of%20camera%20poses%20given%20an%20image%2C%20which%20can%20be%20used%20to%20draw%0Asamples%20from%20the%20pose%20posterior%20distribution.%20Our%20method%20is%20streamlined%20and%0Awell-founded%20in%20theory%20and%20outperforms%20existing%20methods%20on%20localization%20in%0Apresence%20of%20ambiguities.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2410.04989v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DConditional%2520Variational%2520Autoencoders%2520for%2520Probabilistic%2520Pose%2520Regression%26entry.906535625%3DFereidoon%2520Zangeneh%2520and%2520Leonard%2520Bruns%2520and%2520Amit%2520Dekel%2520and%2520Alessandro%2520Pieropan%2520and%2520Patric%2520Jensfelt%26entry.1292438233%3D%2520%2520Robots%2520rely%2520on%2520visual%2520relocalization%2520to%2520estimate%2520their%2520pose%2520from%2520camera%250Aimages%2520when%2520they%2520lose%2520track.%2520One%2520of%2520the%2520challenges%2520in%2520visual%2520relocalization%2520is%250Arepetitive%2520structures%2520in%2520the%2520operation%2520environment%2520of%2520the%2520robot.%2520This%2520calls%2520for%250Aprobabilistic%2520methods%2520that%2520support%2520multiple%2520hypotheses%2520for%2520robot%2527s%2520pose.%2520We%250Apropose%2520such%2520a%2520probabilistic%2520method%2520to%2520predict%2520the%2520posterior%2520distribution%2520of%250Acamera%2520poses%2520given%2520an%2520observed%2520image.%2520Our%2520proposed%2520training%2520strategy%2520results%2520in%250Aa%2520generative%2520model%2520of%2520camera%2520poses%2520given%2520an%2520image%252C%2520which%2520can%2520be%2520used%2520to%2520draw%250Asamples%2520from%2520the%2520pose%2520posterior%2520distribution.%2520Our%2520method%2520is%2520streamlined%2520and%250Awell-founded%2520in%2520theory%2520and%2520outperforms%2520existing%2520methods%2520on%2520localization%2520in%250Apresence%2520of%2520ambiguities.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2410.04989v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Conditional%20Variational%20Autoencoders%20for%20Probabilistic%20Pose%20Regression&entry.906535625=Fereidoon%20Zangeneh%20and%20Leonard%20Bruns%20and%20Amit%20Dekel%20and%20Alessandro%20Pieropan%20and%20Patric%20Jensfelt&entry.1292438233=%20%20Robots%20rely%20on%20visual%20relocalization%20to%20estimate%20their%20pose%20from%20camera%0Aimages%20when%20they%20lose%20track.%20One%20of%20the%20challenges%20in%20visual%20relocalization%20is%0Arepetitive%20structures%20in%20the%20operation%20environment%20of%20the%20robot.%20This%20calls%20for%0Aprobabilistic%20methods%20that%20support%20multiple%20hypotheses%20for%20robot%27s%20pose.%20We%0Apropose%20such%20a%20probabilistic%20method%20to%20predict%20the%20posterior%20distribution%20of%0Acamera%20poses%20given%20an%20observed%20image.%20Our%20proposed%20training%20strategy%20results%20in%0Aa%20generative%20model%20of%20camera%20poses%20given%20an%20image%2C%20which%20can%20be%20used%20to%20draw%0Asamples%20from%20the%20pose%20posterior%20distribution.%20Our%20method%20is%20streamlined%20and%0Awell-founded%20in%20theory%20and%20outperforms%20existing%20methods%20on%20localization%20in%0Apresence%20of%20ambiguities.%0A&entry.1838667208=http%3A//arxiv.org/abs/2410.04989v1&entry.124074799=Read"},
{"title": "PRFusion: Toward Effective and Robust Multi-Modal Place Recognition with\n  Image and Point Cloud Fusion", "author": "Sijie Wang and Qiyu Kang and Rui She and Kai Zhao and Yang Song and Wee Peng Tay", "abstract": "  Place recognition plays a crucial role in the fields of robotics and computer\nvision, finding applications in areas such as autonomous driving, mapping, and\nlocalization. Place recognition identifies a place using query sensor data and\na known database. One of the main challenges is to develop a model that can\ndeliver accurate results while being robust to environmental variations. We\npropose two multi-modal place recognition models, namely PRFusion and\nPRFusion++. PRFusion utilizes global fusion with manifold metric attention,\nenabling effective interaction between features without requiring camera-LiDAR\nextrinsic calibrations. In contrast, PRFusion++ assumes the availability of\nextrinsic calibrations and leverages pixel-point correspondences to enhance\nfeature learning on local windows. Additionally, both models incorporate neural\ndiffusion layers, which enable reliable operation even in challenging\nenvironments. We verify the state-of-the-art performance of both models on\nthree large-scale benchmarks. Notably, they outperform existing models by a\nsubstantial margin of +3.0 AR@1 on the demanding Boreas dataset. Furthermore,\nwe conduct ablation studies to validate the effectiveness of our proposed\nmethods. The codes are available at: https://github.com/sijieaaa/PRFusion\n", "link": "http://arxiv.org/abs/2410.04939v1", "date": "2024-10-07", "relevancy": 2.2663, "topK": [{"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.595}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.5681}, {"title": "DDM-NET: End-to-end learning of keypoint feature Detection, Description and Matching for 3D localization", "link": "https://arxiv.org/abs/2212.04575", "similarity": 0.5375}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20PRFusion%3A%20Toward%20Effective%20and%20Robust%20Multi-Modal%20Place%20Recognition%20with%0A%20%20Image%20and%20Point%20Cloud%20Fusion&body=Title%3A%20PRFusion%3A%20Toward%20Effective%20and%20Robust%20Multi-Modal%20Place%20Recognition%20with%0A%20%20Image%20and%20Point%20Cloud%20Fusion%0AAuthor%3A%20Sijie%20Wang%20and%20Qiyu%20Kang%20and%20Rui%20She%20and%20Kai%20Zhao%20and%20Yang%20Song%20and%20Wee%20Peng%20Tay%0AAbstract%3A%20%20%20Place%20recognition%20plays%20a%20crucial%20role%20in%20the%20fields%20of%20robotics%20and%20computer%0Avision%2C%20finding%20applications%20in%20areas%20such%20as%20autonomous%20driving%2C%20mapping%2C%20and%0Alocalization.%20Place%20recognition%20identifies%20a%20place%20using%20query%20sensor%20data%20and%0Aa%20known%20database.%20One%20of%20the%20main%20challenges%20is%20to%20develop%20a%20model%20that%20can%0Adeliver%20accurate%20results%20while%20being%20robust%20to%20environmental%20variations.%20We%0Apropose%20two%20multi-modal%20place%20recognition%20models%2C%20namely%20PRFusion%20and%0APRFusion%2B%2B.%20PRFusion%20utilizes%20global%20fusion%20with%20manifold%20metric%20attention%2C%0Aenabling%20effective%20interaction%20between%20features%20without%20requiring%20camera-LiDAR%0Aextrinsic%20calibrations.%20In%20contrast%2C%20PRFusion%2B%2B%20assumes%20the%20availability%20of%0Aextrinsic%20calibrations%20and%20leverages%20pixel-point%20correspondences%20to%20enhance%0Afeature%20learning%20on%20local%20windows.%20Additionally%2C%20both%20models%20incorporate%20neural%0Adiffusion%20layers%2C%20which%20enable%20reliable%20operation%20even%20in%20challenging%0Aenvironments.%20We%20verify%20the%20state-of-the-art%20performance%20of%20both%20models%20on%0Athree%20large-scale%20benchmarks.%20Notably%2C%20they%20outperform%20existing%20models%20by%20a%0Asubstantial%20margin%20of%20%2B3.0%20AR%401%20on%20the%20demanding%20Boreas%20dataset.%20Furthermore%2C%0Awe%20conduct%20ablation%20studies%20to%20validate%20the%20effectiveness%20of%20our%20proposed%0Amethods.%20The%20codes%20are%20available%20at%3A%20https%3A//github.com/sijieaaa/PRFusion%0A%0ALink%3A%20http%3A//arxiv.org/abs/2410.04939v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DPRFusion%253A%2520Toward%2520Effective%2520and%2520Robust%2520Multi-Modal%2520Place%2520Recognition%2520with%250A%2520%2520Image%2520and%2520Point%2520Cloud%2520Fusion%26entry.906535625%3DSijie%2520Wang%2520and%2520Qiyu%2520Kang%2520and%2520Rui%2520She%2520and%2520Kai%2520Zhao%2520and%2520Yang%2520Song%2520and%2520Wee%2520Peng%2520Tay%26entry.1292438233%3D%2520%2520Place%2520recognition%2520plays%2520a%2520crucial%2520role%2520in%2520the%2520fields%2520of%2520robotics%2520and%2520computer%250Avision%252C%2520finding%2520applications%2520in%2520areas%2520such%2520as%2520autonomous%2520driving%252C%2520mapping%252C%2520and%250Alocalization.%2520Place%2520recognition%2520identifies%2520a%2520place%2520using%2520query%2520sensor%2520data%2520and%250Aa%2520known%2520database.%2520One%2520of%2520the%2520main%2520challenges%2520is%2520to%2520develop%2520a%2520model%2520that%2520can%250Adeliver%2520accurate%2520results%2520while%2520being%2520robust%2520to%2520environmental%2520variations.%2520We%250Apropose%2520two%2520multi-modal%2520place%2520recognition%2520models%252C%2520namely%2520PRFusion%2520and%250APRFusion%252B%252B.%2520PRFusion%2520utilizes%2520global%2520fusion%2520with%2520manifold%2520metric%2520attention%252C%250Aenabling%2520effective%2520interaction%2520between%2520features%2520without%2520requiring%2520camera-LiDAR%250Aextrinsic%2520calibrations.%2520In%2520contrast%252C%2520PRFusion%252B%252B%2520assumes%2520the%2520availability%2520of%250Aextrinsic%2520calibrations%2520and%2520leverages%2520pixel-point%2520correspondences%2520to%2520enhance%250Afeature%2520learning%2520on%2520local%2520windows.%2520Additionally%252C%2520both%2520models%2520incorporate%2520neural%250Adiffusion%2520layers%252C%2520which%2520enable%2520reliable%2520operation%2520even%2520in%2520challenging%250Aenvironments.%2520We%2520verify%2520the%2520state-of-the-art%2520performance%2520of%2520both%2520models%2520on%250Athree%2520large-scale%2520benchmarks.%2520Notably%252C%2520they%2520outperform%2520existing%2520models%2520by%2520a%250Asubstantial%2520margin%2520of%2520%252B3.0%2520AR%25401%2520on%2520the%2520demanding%2520Boreas%2520dataset.%2520Furthermore%252C%250Awe%2520conduct%2520ablation%2520studies%2520to%2520validate%2520the%2520effectiveness%2520of%2520our%2520proposed%250Amethods.%2520The%2520codes%2520are%2520available%2520at%253A%2520https%253A//github.com/sijieaaa/PRFusion%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2410.04939v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=PRFusion%3A%20Toward%20Effective%20and%20Robust%20Multi-Modal%20Place%20Recognition%20with%0A%20%20Image%20and%20Point%20Cloud%20Fusion&entry.906535625=Sijie%20Wang%20and%20Qiyu%20Kang%20and%20Rui%20She%20and%20Kai%20Zhao%20and%20Yang%20Song%20and%20Wee%20Peng%20Tay&entry.1292438233=%20%20Place%20recognition%20plays%20a%20crucial%20role%20in%20the%20fields%20of%20robotics%20and%20computer%0Avision%2C%20finding%20applications%20in%20areas%20such%20as%20autonomous%20driving%2C%20mapping%2C%20and%0Alocalization.%20Place%20recognition%20identifies%20a%20place%20using%20query%20sensor%20data%20and%0Aa%20known%20database.%20One%20of%20the%20main%20challenges%20is%20to%20develop%20a%20model%20that%20can%0Adeliver%20accurate%20results%20while%20being%20robust%20to%20environmental%20variations.%20We%0Apropose%20two%20multi-modal%20place%20recognition%20models%2C%20namely%20PRFusion%20and%0APRFusion%2B%2B.%20PRFusion%20utilizes%20global%20fusion%20with%20manifold%20metric%20attention%2C%0Aenabling%20effective%20interaction%20between%20features%20without%20requiring%20camera-LiDAR%0Aextrinsic%20calibrations.%20In%20contrast%2C%20PRFusion%2B%2B%20assumes%20the%20availability%20of%0Aextrinsic%20calibrations%20and%20leverages%20pixel-point%20correspondences%20to%20enhance%0Afeature%20learning%20on%20local%20windows.%20Additionally%2C%20both%20models%20incorporate%20neural%0Adiffusion%20layers%2C%20which%20enable%20reliable%20operation%20even%20in%20challenging%0Aenvironments.%20We%20verify%20the%20state-of-the-art%20performance%20of%20both%20models%20on%0Athree%20large-scale%20benchmarks.%20Notably%2C%20they%20outperform%20existing%20models%20by%20a%0Asubstantial%20margin%20of%20%2B3.0%20AR%401%20on%20the%20demanding%20Boreas%20dataset.%20Furthermore%2C%0Awe%20conduct%20ablation%20studies%20to%20validate%20the%20effectiveness%20of%20our%20proposed%0Amethods.%20The%20codes%20are%20available%20at%3A%20https%3A//github.com/sijieaaa/PRFusion%0A&entry.1838667208=http%3A//arxiv.org/abs/2410.04939v1&entry.124074799=Read"},
{"title": "TidalDecode: Fast and Accurate LLM Decoding with Position Persistent\n  Sparse Attention", "author": "Lijie Yang and Zhihao Zhang and Zhuofu Chen and Zikun Li and Zhihao Jia", "abstract": "  Large language models (LLMs) have driven significant advancements across\ndiverse NLP tasks, with long-context models gaining prominence for handling\nextended inputs. However, the expanding key-value (KV) cache size required by\nTransformer architectures intensifies the memory constraints, particularly\nduring the decoding phase, creating a significant bottleneck. Existing sparse\nattention mechanisms designed to address this bottleneck have two limitations:\n(1) they often fail to reliably identify the most relevant tokens for\nattention, and (2) they overlook the spatial coherence of token selection\nacross consecutive Transformer layers, which can lead to performance\ndegradation and substantial overhead in token selection. This paper introduces\nTidalDecode, a simple yet effective algorithm and system for fast and accurate\nLLM decoding through position persistent sparse attention. TidalDecode\nleverages the spatial coherence of tokens selected by existing sparse attention\nmethods and introduces a few token selection layers that perform full attention\nto identify the tokens with the highest attention scores, while all other\nlayers perform sparse attention with the pre-selected tokens. This design\nenables TidalDecode to substantially reduce the overhead of token selection for\nsparse attention without sacrificing the quality of the generated results.\nEvaluation on a diverse set of LLMs and tasks shows that TidalDecode closely\nmatches the generative performance of full attention methods while reducing the\nLLM decoding latency by up to 2.1x.\n", "link": "http://arxiv.org/abs/2410.05076v1", "date": "2024-10-07", "relevancy": 2.2602, "topK": [{"title": "ToDo: Token Downsampling for Efficient Generation of High-Resolution\n  Images", "link": "http://arxiv.org/abs/2402.13573v2", "similarity": 0.5948}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5491}, {"title": "SDXS: Real-Time One-Step Latent Diffusion Models with Image Conditions", "link": "http://arxiv.org/abs/2403.16627v1", "similarity": 0.5308}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20TidalDecode%3A%20Fast%20and%20Accurate%20LLM%20Decoding%20with%20Position%20Persistent%0A%20%20Sparse%20Attention&body=Title%3A%20TidalDecode%3A%20Fast%20and%20Accurate%20LLM%20Decoding%20with%20Position%20Persistent%0A%20%20Sparse%20Attention%0AAuthor%3A%20Lijie%20Yang%20and%20Zhihao%20Zhang%20and%20Zhuofu%20Chen%20and%20Zikun%20Li%20and%20Zhihao%20Jia%0AAbstract%3A%20%20%20Large%20language%20models%20%28LLMs%29%20have%20driven%20significant%20advancements%20across%0Adiverse%20NLP%20tasks%2C%20with%20long-context%20models%20gaining%20prominence%20for%20handling%0Aextended%20inputs.%20However%2C%20the%20expanding%20key-value%20%28KV%29%20cache%20size%20required%20by%0ATransformer%20architectures%20intensifies%20the%20memory%20constraints%2C%20particularly%0Aduring%20the%20decoding%20phase%2C%20creating%20a%20significant%20bottleneck.%20Existing%20sparse%0Aattention%20mechanisms%20designed%20to%20address%20this%20bottleneck%20have%20two%20limitations%3A%0A%281%29%20they%20often%20fail%20to%20reliably%20identify%20the%20most%20relevant%20tokens%20for%0Aattention%2C%20and%20%282%29%20they%20overlook%20the%20spatial%20coherence%20of%20token%20selection%0Aacross%20consecutive%20Transformer%20layers%2C%20which%20can%20lead%20to%20performance%0Adegradation%20and%20substantial%20overhead%20in%20token%20selection.%20This%20paper%20introduces%0ATidalDecode%2C%20a%20simple%20yet%20effective%20algorithm%20and%20system%20for%20fast%20and%20accurate%0ALLM%20decoding%20through%20position%20persistent%20sparse%20attention.%20TidalDecode%0Aleverages%20the%20spatial%20coherence%20of%20tokens%20selected%20by%20existing%20sparse%20attention%0Amethods%20and%20introduces%20a%20few%20token%20selection%20layers%20that%20perform%20full%20attention%0Ato%20identify%20the%20tokens%20with%20the%20highest%20attention%20scores%2C%20while%20all%20other%0Alayers%20perform%20sparse%20attention%20with%20the%20pre-selected%20tokens.%20This%20design%0Aenables%20TidalDecode%20to%20substantially%20reduce%20the%20overhead%20of%20token%20selection%20for%0Asparse%20attention%20without%20sacrificing%20the%20quality%20of%20the%20generated%20results.%0AEvaluation%20on%20a%20diverse%20set%20of%20LLMs%20and%20tasks%20shows%20that%20TidalDecode%20closely%0Amatches%20the%20generative%20performance%20of%20full%20attention%20methods%20while%20reducing%20the%0ALLM%20decoding%20latency%20by%20up%20to%202.1x.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2410.05076v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DTidalDecode%253A%2520Fast%2520and%2520Accurate%2520LLM%2520Decoding%2520with%2520Position%2520Persistent%250A%2520%2520Sparse%2520Attention%26entry.906535625%3DLijie%2520Yang%2520and%2520Zhihao%2520Zhang%2520and%2520Zhuofu%2520Chen%2520and%2520Zikun%2520Li%2520and%2520Zhihao%2520Jia%26entry.1292438233%3D%2520%2520Large%2520language%2520models%2520%2528LLMs%2529%2520have%2520driven%2520significant%2520advancements%2520across%250Adiverse%2520NLP%2520tasks%252C%2520with%2520long-context%2520models%2520gaining%2520prominence%2520for%2520handling%250Aextended%2520inputs.%2520However%252C%2520the%2520expanding%2520key-value%2520%2528KV%2529%2520cache%2520size%2520required%2520by%250ATransformer%2520architectures%2520intensifies%2520the%2520memory%2520constraints%252C%2520particularly%250Aduring%2520the%2520decoding%2520phase%252C%2520creating%2520a%2520significant%2520bottleneck.%2520Existing%2520sparse%250Aattention%2520mechanisms%2520designed%2520to%2520address%2520this%2520bottleneck%2520have%2520two%2520limitations%253A%250A%25281%2529%2520they%2520often%2520fail%2520to%2520reliably%2520identify%2520the%2520most%2520relevant%2520tokens%2520for%250Aattention%252C%2520and%2520%25282%2529%2520they%2520overlook%2520the%2520spatial%2520coherence%2520of%2520token%2520selection%250Aacross%2520consecutive%2520Transformer%2520layers%252C%2520which%2520can%2520lead%2520to%2520performance%250Adegradation%2520and%2520substantial%2520overhead%2520in%2520token%2520selection.%2520This%2520paper%2520introduces%250ATidalDecode%252C%2520a%2520simple%2520yet%2520effective%2520algorithm%2520and%2520system%2520for%2520fast%2520and%2520accurate%250ALLM%2520decoding%2520through%2520position%2520persistent%2520sparse%2520attention.%2520TidalDecode%250Aleverages%2520the%2520spatial%2520coherence%2520of%2520tokens%2520selected%2520by%2520existing%2520sparse%2520attention%250Amethods%2520and%2520introduces%2520a%2520few%2520token%2520selection%2520layers%2520that%2520perform%2520full%2520attention%250Ato%2520identify%2520the%2520tokens%2520with%2520the%2520highest%2520attention%2520scores%252C%2520while%2520all%2520other%250Alayers%2520perform%2520sparse%2520attention%2520with%2520the%2520pre-selected%2520tokens.%2520This%2520design%250Aenables%2520TidalDecode%2520to%2520substantially%2520reduce%2520the%2520overhead%2520of%2520token%2520selection%2520for%250Asparse%2520attention%2520without%2520sacrificing%2520the%2520quality%2520of%2520the%2520generated%2520results.%250AEvaluation%2520on%2520a%2520diverse%2520set%2520of%2520LLMs%2520and%2520tasks%2520shows%2520that%2520TidalDecode%2520closely%250Amatches%2520the%2520generative%2520performance%2520of%2520full%2520attention%2520methods%2520while%2520reducing%2520the%250ALLM%2520decoding%2520latency%2520by%2520up%2520to%25202.1x.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2410.05076v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=TidalDecode%3A%20Fast%20and%20Accurate%20LLM%20Decoding%20with%20Position%20Persistent%0A%20%20Sparse%20Attention&entry.906535625=Lijie%20Yang%20and%20Zhihao%20Zhang%20and%20Zhuofu%20Chen%20and%20Zikun%20Li%20and%20Zhihao%20Jia&entry.1292438233=%20%20Large%20language%20models%20%28LLMs%29%20have%20driven%20significant%20advancements%20across%0Adiverse%20NLP%20tasks%2C%20with%20long-context%20models%20gaining%20prominence%20for%20handling%0Aextended%20inputs.%20However%2C%20the%20expanding%20key-value%20%28KV%29%20cache%20size%20required%20by%0ATransformer%20architectures%20intensifies%20the%20memory%20constraints%2C%20particularly%0Aduring%20the%20decoding%20phase%2C%20creating%20a%20significant%20bottleneck.%20Existing%20sparse%0Aattention%20mechanisms%20designed%20to%20address%20this%20bottleneck%20have%20two%20limitations%3A%0A%281%29%20they%20often%20fail%20to%20reliably%20identify%20the%20most%20relevant%20tokens%20for%0Aattention%2C%20and%20%282%29%20they%20overlook%20the%20spatial%20coherence%20of%20token%20selection%0Aacross%20consecutive%20Transformer%20layers%2C%20which%20can%20lead%20to%20performance%0Adegradation%20and%20substantial%20overhead%20in%20token%20selection.%20This%20paper%20introduces%0ATidalDecode%2C%20a%20simple%20yet%20effective%20algorithm%20and%20system%20for%20fast%20and%20accurate%0ALLM%20decoding%20through%20position%20persistent%20sparse%20attention.%20TidalDecode%0Aleverages%20the%20spatial%20coherence%20of%20tokens%20selected%20by%20existing%20sparse%20attention%0Amethods%20and%20introduces%20a%20few%20token%20selection%20layers%20that%20perform%20full%20attention%0Ato%20identify%20the%20tokens%20with%20the%20highest%20attention%20scores%2C%20while%20all%20other%0Alayers%20perform%20sparse%20attention%20with%20the%20pre-selected%20tokens.%20This%20design%0Aenables%20TidalDecode%20to%20substantially%20reduce%20the%20overhead%20of%20token%20selection%20for%0Asparse%20attention%20without%20sacrificing%20the%20quality%20of%20the%20generated%20results.%0AEvaluation%20on%20a%20diverse%20set%20of%20LLMs%20and%20tasks%20shows%20that%20TidalDecode%20closely%0Amatches%20the%20generative%20performance%20of%20full%20attention%20methods%20while%20reducing%20the%0ALLM%20decoding%20latency%20by%20up%20to%202.1x.%0A&entry.1838667208=http%3A//arxiv.org/abs/2410.05076v1&entry.124074799=Read"},
{"title": "Compression via Pre-trained Transformers: A Study on Byte-Level\n  Multimodal Data", "author": "David Heurtel-Depeiges and Anian Ruoss and Joel Veness and Tim Genewein", "abstract": "  Foundation models have recently been shown to be strong data compressors.\nHowever, when accounting for their excessive parameter count, their compression\nratios are actually inferior to standard compression algorithms. Moreover,\nnaively reducing the number of parameters may not necessarily help as it leads\nto worse predictions and thus weaker compression. In this paper, we conduct a\nlarge-scale empirical study to investigate whether there is a sweet spot where\ncompetitive compression ratios with pre-trained vanilla transformers are\npossible. To this end, we train families of models on 165GB of raw byte\nsequences of either text, image, or audio data (and all possible combinations\nof the three) and then compress 1GB of out-of-distribution (OOD) data from each\nmodality. We find that relatively small models (i.e., millions of parameters)\ncan outperform standard general-purpose compression algorithms (gzip, LZMA2)\nand even domain-specific compressors (PNG, JPEG 2000, FLAC) - even when\nfactoring in parameter count. We achieve, e.g., the lowest compression ratio of\n0.49 on OOD audio data (vs. 0.54 for FLAC). To study the impact of model- and\ndataset scale, we conduct extensive ablations and hyperparameter sweeps, and we\ninvestigate the effect of unimodal versus multimodal training. We find that\neven small models can be trained to perform well on multiple modalities, but,\nin contrast to previously reported results with large-scale foundation models,\ntransfer to unseen modalities is generally weak.\n", "link": "http://arxiv.org/abs/2410.05078v1", "date": "2024-10-07", "relevancy": 2.2594, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5654}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5654}, {"title": "Scaling Rectified Flow Transformers for High-Resolution Image Synthesis", "link": "http://arxiv.org/abs/2403.03206v1", "similarity": 0.5621}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Compression%20via%20Pre-trained%20Transformers%3A%20A%20Study%20on%20Byte-Level%0A%20%20Multimodal%20Data&body=Title%3A%20Compression%20via%20Pre-trained%20Transformers%3A%20A%20Study%20on%20Byte-Level%0A%20%20Multimodal%20Data%0AAuthor%3A%20David%20Heurtel-Depeiges%20and%20Anian%20Ruoss%20and%20Joel%20Veness%20and%20Tim%20Genewein%0AAbstract%3A%20%20%20Foundation%20models%20have%20recently%20been%20shown%20to%20be%20strong%20data%20compressors.%0AHowever%2C%20when%20accounting%20for%20their%20excessive%20parameter%20count%2C%20their%20compression%0Aratios%20are%20actually%20inferior%20to%20standard%20compression%20algorithms.%20Moreover%2C%0Anaively%20reducing%20the%20number%20of%20parameters%20may%20not%20necessarily%20help%20as%20it%20leads%0Ato%20worse%20predictions%20and%20thus%20weaker%20compression.%20In%20this%20paper%2C%20we%20conduct%20a%0Alarge-scale%20empirical%20study%20to%20investigate%20whether%20there%20is%20a%20sweet%20spot%20where%0Acompetitive%20compression%20ratios%20with%20pre-trained%20vanilla%20transformers%20are%0Apossible.%20To%20this%20end%2C%20we%20train%20families%20of%20models%20on%20165GB%20of%20raw%20byte%0Asequences%20of%20either%20text%2C%20image%2C%20or%20audio%20data%20%28and%20all%20possible%20combinations%0Aof%20the%20three%29%20and%20then%20compress%201GB%20of%20out-of-distribution%20%28OOD%29%20data%20from%20each%0Amodality.%20We%20find%20that%20relatively%20small%20models%20%28i.e.%2C%20millions%20of%20parameters%29%0Acan%20outperform%20standard%20general-purpose%20compression%20algorithms%20%28gzip%2C%20LZMA2%29%0Aand%20even%20domain-specific%20compressors%20%28PNG%2C%20JPEG%202000%2C%20FLAC%29%20-%20even%20when%0Afactoring%20in%20parameter%20count.%20We%20achieve%2C%20e.g.%2C%20the%20lowest%20compression%20ratio%20of%0A0.49%20on%20OOD%20audio%20data%20%28vs.%200.54%20for%20FLAC%29.%20To%20study%20the%20impact%20of%20model-%20and%0Adataset%20scale%2C%20we%20conduct%20extensive%20ablations%20and%20hyperparameter%20sweeps%2C%20and%20we%0Ainvestigate%20the%20effect%20of%20unimodal%20versus%20multimodal%20training.%20We%20find%20that%0Aeven%20small%20models%20can%20be%20trained%20to%20perform%20well%20on%20multiple%20modalities%2C%20but%2C%0Ain%20contrast%20to%20previously%20reported%20results%20with%20large-scale%20foundation%20models%2C%0Atransfer%20to%20unseen%20modalities%20is%20generally%20weak.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2410.05078v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DCompression%2520via%2520Pre-trained%2520Transformers%253A%2520A%2520Study%2520on%2520Byte-Level%250A%2520%2520Multimodal%2520Data%26entry.906535625%3DDavid%2520Heurtel-Depeiges%2520and%2520Anian%2520Ruoss%2520and%2520Joel%2520Veness%2520and%2520Tim%2520Genewein%26entry.1292438233%3D%2520%2520Foundation%2520models%2520have%2520recently%2520been%2520shown%2520to%2520be%2520strong%2520data%2520compressors.%250AHowever%252C%2520when%2520accounting%2520for%2520their%2520excessive%2520parameter%2520count%252C%2520their%2520compression%250Aratios%2520are%2520actually%2520inferior%2520to%2520standard%2520compression%2520algorithms.%2520Moreover%252C%250Anaively%2520reducing%2520the%2520number%2520of%2520parameters%2520may%2520not%2520necessarily%2520help%2520as%2520it%2520leads%250Ato%2520worse%2520predictions%2520and%2520thus%2520weaker%2520compression.%2520In%2520this%2520paper%252C%2520we%2520conduct%2520a%250Alarge-scale%2520empirical%2520study%2520to%2520investigate%2520whether%2520there%2520is%2520a%2520sweet%2520spot%2520where%250Acompetitive%2520compression%2520ratios%2520with%2520pre-trained%2520vanilla%2520transformers%2520are%250Apossible.%2520To%2520this%2520end%252C%2520we%2520train%2520families%2520of%2520models%2520on%2520165GB%2520of%2520raw%2520byte%250Asequences%2520of%2520either%2520text%252C%2520image%252C%2520or%2520audio%2520data%2520%2528and%2520all%2520possible%2520combinations%250Aof%2520the%2520three%2529%2520and%2520then%2520compress%25201GB%2520of%2520out-of-distribution%2520%2528OOD%2529%2520data%2520from%2520each%250Amodality.%2520We%2520find%2520that%2520relatively%2520small%2520models%2520%2528i.e.%252C%2520millions%2520of%2520parameters%2529%250Acan%2520outperform%2520standard%2520general-purpose%2520compression%2520algorithms%2520%2528gzip%252C%2520LZMA2%2529%250Aand%2520even%2520domain-specific%2520compressors%2520%2528PNG%252C%2520JPEG%25202000%252C%2520FLAC%2529%2520-%2520even%2520when%250Afactoring%2520in%2520parameter%2520count.%2520We%2520achieve%252C%2520e.g.%252C%2520the%2520lowest%2520compression%2520ratio%2520of%250A0.49%2520on%2520OOD%2520audio%2520data%2520%2528vs.%25200.54%2520for%2520FLAC%2529.%2520To%2520study%2520the%2520impact%2520of%2520model-%2520and%250Adataset%2520scale%252C%2520we%2520conduct%2520extensive%2520ablations%2520and%2520hyperparameter%2520sweeps%252C%2520and%2520we%250Ainvestigate%2520the%2520effect%2520of%2520unimodal%2520versus%2520multimodal%2520training.%2520We%2520find%2520that%250Aeven%2520small%2520models%2520can%2520be%2520trained%2520to%2520perform%2520well%2520on%2520multiple%2520modalities%252C%2520but%252C%250Ain%2520contrast%2520to%2520previously%2520reported%2520results%2520with%2520large-scale%2520foundation%2520models%252C%250Atransfer%2520to%2520unseen%2520modalities%2520is%2520generally%2520weak.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2410.05078v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Compression%20via%20Pre-trained%20Transformers%3A%20A%20Study%20on%20Byte-Level%0A%20%20Multimodal%20Data&entry.906535625=David%20Heurtel-Depeiges%20and%20Anian%20Ruoss%20and%20Joel%20Veness%20and%20Tim%20Genewein&entry.1292438233=%20%20Foundation%20models%20have%20recently%20been%20shown%20to%20be%20strong%20data%20compressors.%0AHowever%2C%20when%20accounting%20for%20their%20excessive%20parameter%20count%2C%20their%20compression%0Aratios%20are%20actually%20inferior%20to%20standard%20compression%20algorithms.%20Moreover%2C%0Anaively%20reducing%20the%20number%20of%20parameters%20may%20not%20necessarily%20help%20as%20it%20leads%0Ato%20worse%20predictions%20and%20thus%20weaker%20compression.%20In%20this%20paper%2C%20we%20conduct%20a%0Alarge-scale%20empirical%20study%20to%20investigate%20whether%20there%20is%20a%20sweet%20spot%20where%0Acompetitive%20compression%20ratios%20with%20pre-trained%20vanilla%20transformers%20are%0Apossible.%20To%20this%20end%2C%20we%20train%20families%20of%20models%20on%20165GB%20of%20raw%20byte%0Asequences%20of%20either%20text%2C%20image%2C%20or%20audio%20data%20%28and%20all%20possible%20combinations%0Aof%20the%20three%29%20and%20then%20compress%201GB%20of%20out-of-distribution%20%28OOD%29%20data%20from%20each%0Amodality.%20We%20find%20that%20relatively%20small%20models%20%28i.e.%2C%20millions%20of%20parameters%29%0Acan%20outperform%20standard%20general-purpose%20compression%20algorithms%20%28gzip%2C%20LZMA2%29%0Aand%20even%20domain-specific%20compressors%20%28PNG%2C%20JPEG%202000%2C%20FLAC%29%20-%20even%20when%0Afactoring%20in%20parameter%20count.%20We%20achieve%2C%20e.g.%2C%20the%20lowest%20compression%20ratio%20of%0A0.49%20on%20OOD%20audio%20data%20%28vs.%200.54%20for%20FLAC%29.%20To%20study%20the%20impact%20of%20model-%20and%0Adataset%20scale%2C%20we%20conduct%20extensive%20ablations%20and%20hyperparameter%20sweeps%2C%20and%20we%0Ainvestigate%20the%20effect%20of%20unimodal%20versus%20multimodal%20training.%20We%20find%20that%0Aeven%20small%20models%20can%20be%20trained%20to%20perform%20well%20on%20multiple%20modalities%2C%20but%2C%0Ain%20contrast%20to%20previously%20reported%20results%20with%20large-scale%20foundation%20models%2C%0Atransfer%20to%20unseen%20modalities%20is%20generally%20weak.%0A&entry.1838667208=http%3A//arxiv.org/abs/2410.05078v1&entry.124074799=Read"},
{"title": "A Narrative Review of Image Processing Techniques Related to Prostate\n  Ultrasound", "author": "Haiqiao Wang and Hong Wu and Zhuoyuan Wang and Peiyan Yue and Dong Ni and Pheng-Ann Heng and Yi Wang", "abstract": "  Prostate cancer (PCa) poses a significant threat to men's health, with early\ndiagnosis being crucial for improving prognosis and reducing mortality rates.\nTransrectal ultrasound (TRUS) plays a vital role in the diagnosis and\nimage-guided intervention of PCa.To facilitate physicians with more accurate\nand efficient computer-assisted diagnosis and interventions, many image\nprocessing algorithms in TRUS have been proposed and achieved state-of-the-art\nperformance in several tasks, including prostate gland segmentation, prostate\nimage registration, PCa classification and detection, and interventional needle\ndetection. The rapid development of these algorithms over the past two decades\nnecessitates a comprehensive summary. In consequence, this survey provides a\n\\textcolor{blue}{narrative } analysis of this field, outlining the evolution of\nimage processing methods in the context of TRUS image analysis and meanwhile\nhighlighting their relevant contributions. Furthermore, this survey discusses\ncurrent challenges and suggests future research directions to possibly advance\nthis field further.\n", "link": "http://arxiv.org/abs/2407.00678v2", "date": "2024-10-07", "relevancy": 2.2384, "topK": [{"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.4797}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.4317}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.4317}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20A%20Narrative%20Review%20of%20Image%20Processing%20Techniques%20Related%20to%20Prostate%0A%20%20Ultrasound&body=Title%3A%20A%20Narrative%20Review%20of%20Image%20Processing%20Techniques%20Related%20to%20Prostate%0A%20%20Ultrasound%0AAuthor%3A%20Haiqiao%20Wang%20and%20Hong%20Wu%20and%20Zhuoyuan%20Wang%20and%20Peiyan%20Yue%20and%20Dong%20Ni%20and%20Pheng-Ann%20Heng%20and%20Yi%20Wang%0AAbstract%3A%20%20%20Prostate%20cancer%20%28PCa%29%20poses%20a%20significant%20threat%20to%20men%27s%20health%2C%20with%20early%0Adiagnosis%20being%20crucial%20for%20improving%20prognosis%20and%20reducing%20mortality%20rates.%0ATransrectal%20ultrasound%20%28TRUS%29%20plays%20a%20vital%20role%20in%20the%20diagnosis%20and%0Aimage-guided%20intervention%20of%20PCa.To%20facilitate%20physicians%20with%20more%20accurate%0Aand%20efficient%20computer-assisted%20diagnosis%20and%20interventions%2C%20many%20image%0Aprocessing%20algorithms%20in%20TRUS%20have%20been%20proposed%20and%20achieved%20state-of-the-art%0Aperformance%20in%20several%20tasks%2C%20including%20prostate%20gland%20segmentation%2C%20prostate%0Aimage%20registration%2C%20PCa%20classification%20and%20detection%2C%20and%20interventional%20needle%0Adetection.%20The%20rapid%20development%20of%20these%20algorithms%20over%20the%20past%20two%20decades%0Anecessitates%20a%20comprehensive%20summary.%20In%20consequence%2C%20this%20survey%20provides%20a%0A%5Ctextcolor%7Bblue%7D%7Bnarrative%20%7D%20analysis%20of%20this%20field%2C%20outlining%20the%20evolution%20of%0Aimage%20processing%20methods%20in%20the%20context%20of%20TRUS%20image%20analysis%20and%20meanwhile%0Ahighlighting%20their%20relevant%20contributions.%20Furthermore%2C%20this%20survey%20discusses%0Acurrent%20challenges%20and%20suggests%20future%20research%20directions%20to%20possibly%20advance%0Athis%20field%20further.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2407.00678v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DA%2520Narrative%2520Review%2520of%2520Image%2520Processing%2520Techniques%2520Related%2520to%2520Prostate%250A%2520%2520Ultrasound%26entry.906535625%3DHaiqiao%2520Wang%2520and%2520Hong%2520Wu%2520and%2520Zhuoyuan%2520Wang%2520and%2520Peiyan%2520Yue%2520and%2520Dong%2520Ni%2520and%2520Pheng-Ann%2520Heng%2520and%2520Yi%2520Wang%26entry.1292438233%3D%2520%2520Prostate%2520cancer%2520%2528PCa%2529%2520poses%2520a%2520significant%2520threat%2520to%2520men%2527s%2520health%252C%2520with%2520early%250Adiagnosis%2520being%2520crucial%2520for%2520improving%2520prognosis%2520and%2520reducing%2520mortality%2520rates.%250ATransrectal%2520ultrasound%2520%2528TRUS%2529%2520plays%2520a%2520vital%2520role%2520in%2520the%2520diagnosis%2520and%250Aimage-guided%2520intervention%2520of%2520PCa.To%2520facilitate%2520physicians%2520with%2520more%2520accurate%250Aand%2520efficient%2520computer-assisted%2520diagnosis%2520and%2520interventions%252C%2520many%2520image%250Aprocessing%2520algorithms%2520in%2520TRUS%2520have%2520been%2520proposed%2520and%2520achieved%2520state-of-the-art%250Aperformance%2520in%2520several%2520tasks%252C%2520including%2520prostate%2520gland%2520segmentation%252C%2520prostate%250Aimage%2520registration%252C%2520PCa%2520classification%2520and%2520detection%252C%2520and%2520interventional%2520needle%250Adetection.%2520The%2520rapid%2520development%2520of%2520these%2520algorithms%2520over%2520the%2520past%2520two%2520decades%250Anecessitates%2520a%2520comprehensive%2520summary.%2520In%2520consequence%252C%2520this%2520survey%2520provides%2520a%250A%255Ctextcolor%257Bblue%257D%257Bnarrative%2520%257D%2520analysis%2520of%2520this%2520field%252C%2520outlining%2520the%2520evolution%2520of%250Aimage%2520processing%2520methods%2520in%2520the%2520context%2520of%2520TRUS%2520image%2520analysis%2520and%2520meanwhile%250Ahighlighting%2520their%2520relevant%2520contributions.%2520Furthermore%252C%2520this%2520survey%2520discusses%250Acurrent%2520challenges%2520and%2520suggests%2520future%2520research%2520directions%2520to%2520possibly%2520advance%250Athis%2520field%2520further.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2407.00678v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=A%20Narrative%20Review%20of%20Image%20Processing%20Techniques%20Related%20to%20Prostate%0A%20%20Ultrasound&entry.906535625=Haiqiao%20Wang%20and%20Hong%20Wu%20and%20Zhuoyuan%20Wang%20and%20Peiyan%20Yue%20and%20Dong%20Ni%20and%20Pheng-Ann%20Heng%20and%20Yi%20Wang&entry.1292438233=%20%20Prostate%20cancer%20%28PCa%29%20poses%20a%20significant%20threat%20to%20men%27s%20health%2C%20with%20early%0Adiagnosis%20being%20crucial%20for%20improving%20prognosis%20and%20reducing%20mortality%20rates.%0ATransrectal%20ultrasound%20%28TRUS%29%20plays%20a%20vital%20role%20in%20the%20diagnosis%20and%0Aimage-guided%20intervention%20of%20PCa.To%20facilitate%20physicians%20with%20more%20accurate%0Aand%20efficient%20computer-assisted%20diagnosis%20and%20interventions%2C%20many%20image%0Aprocessing%20algorithms%20in%20TRUS%20have%20been%20proposed%20and%20achieved%20state-of-the-art%0Aperformance%20in%20several%20tasks%2C%20including%20prostate%20gland%20segmentation%2C%20prostate%0Aimage%20registration%2C%20PCa%20classification%20and%20detection%2C%20and%20interventional%20needle%0Adetection.%20The%20rapid%20development%20of%20these%20algorithms%20over%20the%20past%20two%20decades%0Anecessitates%20a%20comprehensive%20summary.%20In%20consequence%2C%20this%20survey%20provides%20a%0A%5Ctextcolor%7Bblue%7D%7Bnarrative%20%7D%20analysis%20of%20this%20field%2C%20outlining%20the%20evolution%20of%0Aimage%20processing%20methods%20in%20the%20context%20of%20TRUS%20image%20analysis%20and%20meanwhile%0Ahighlighting%20their%20relevant%20contributions.%20Furthermore%2C%20this%20survey%20discusses%0Acurrent%20challenges%20and%20suggests%20future%20research%20directions%20to%20possibly%20advance%0Athis%20field%20further.%0A&entry.1838667208=http%3A//arxiv.org/abs/2407.00678v2&entry.124074799=Read"},
{"title": "Deep Fusion: Capturing Dependencies in Contrastive Learning via\n  Transformer Projection Heads", "author": "Huanran Li and Daniel Pimentel-Alarc\u00f3n", "abstract": "  Contrastive Learning (CL) has emerged as a powerful method for training\nfeature extraction models using unlabeled data. Recent studies suggest that\nincorporating a linear projection head post-backbone significantly enhances\nmodel performance. In this work, we investigate the use of a transformer model\nas a projection head within the CL framework, aiming to exploit the\ntransformer's capacity for capturing long-range dependencies across embeddings\nto further improve performance. Our key contributions are fourfold: First, we\nintroduce a novel application of transformers in the projection head role for\ncontrastive learning, marking the first endeavor of its kind. Second, our\nexperiments reveal a compelling \"Deep Fusion\" phenomenon where the attention\nmechanism progressively captures the correct relational dependencies among\nsamples from the same class in deeper layers. Third, we provide a theoretical\nframework that explains and supports this \"Deep Fusion\" behavior. Finally, we\ndemonstrate through experimental results that our model achieves superior\nperformance compared to the existing approach of using a feed-forward layer.\n", "link": "http://arxiv.org/abs/2403.18681v2", "date": "2024-10-07", "relevancy": 2.2368, "topK": [{"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5753}, {"title": "Scaling Rectified Flow Transformers for High-Resolution Image Synthesis", "link": "http://arxiv.org/abs/2403.03206v1", "similarity": 0.5628}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5417}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Deep%20Fusion%3A%20Capturing%20Dependencies%20in%20Contrastive%20Learning%20via%0A%20%20Transformer%20Projection%20Heads&body=Title%3A%20Deep%20Fusion%3A%20Capturing%20Dependencies%20in%20Contrastive%20Learning%20via%0A%20%20Transformer%20Projection%20Heads%0AAuthor%3A%20Huanran%20Li%20and%20Daniel%20Pimentel-Alarc%C3%B3n%0AAbstract%3A%20%20%20Contrastive%20Learning%20%28CL%29%20has%20emerged%20as%20a%20powerful%20method%20for%20training%0Afeature%20extraction%20models%20using%20unlabeled%20data.%20Recent%20studies%20suggest%20that%0Aincorporating%20a%20linear%20projection%20head%20post-backbone%20significantly%20enhances%0Amodel%20performance.%20In%20this%20work%2C%20we%20investigate%20the%20use%20of%20a%20transformer%20model%0Aas%20a%20projection%20head%20within%20the%20CL%20framework%2C%20aiming%20to%20exploit%20the%0Atransformer%27s%20capacity%20for%20capturing%20long-range%20dependencies%20across%20embeddings%0Ato%20further%20improve%20performance.%20Our%20key%20contributions%20are%20fourfold%3A%20First%2C%20we%0Aintroduce%20a%20novel%20application%20of%20transformers%20in%20the%20projection%20head%20role%20for%0Acontrastive%20learning%2C%20marking%20the%20first%20endeavor%20of%20its%20kind.%20Second%2C%20our%0Aexperiments%20reveal%20a%20compelling%20%22Deep%20Fusion%22%20phenomenon%20where%20the%20attention%0Amechanism%20progressively%20captures%20the%20correct%20relational%20dependencies%20among%0Asamples%20from%20the%20same%20class%20in%20deeper%20layers.%20Third%2C%20we%20provide%20a%20theoretical%0Aframework%20that%20explains%20and%20supports%20this%20%22Deep%20Fusion%22%20behavior.%20Finally%2C%20we%0Ademonstrate%20through%20experimental%20results%20that%20our%20model%20achieves%20superior%0Aperformance%20compared%20to%20the%20existing%20approach%20of%20using%20a%20feed-forward%20layer.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2403.18681v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DDeep%2520Fusion%253A%2520Capturing%2520Dependencies%2520in%2520Contrastive%2520Learning%2520via%250A%2520%2520Transformer%2520Projection%2520Heads%26entry.906535625%3DHuanran%2520Li%2520and%2520Daniel%2520Pimentel-Alarc%25C3%25B3n%26entry.1292438233%3D%2520%2520Contrastive%2520Learning%2520%2528CL%2529%2520has%2520emerged%2520as%2520a%2520powerful%2520method%2520for%2520training%250Afeature%2520extraction%2520models%2520using%2520unlabeled%2520data.%2520Recent%2520studies%2520suggest%2520that%250Aincorporating%2520a%2520linear%2520projection%2520head%2520post-backbone%2520significantly%2520enhances%250Amodel%2520performance.%2520In%2520this%2520work%252C%2520we%2520investigate%2520the%2520use%2520of%2520a%2520transformer%2520model%250Aas%2520a%2520projection%2520head%2520within%2520the%2520CL%2520framework%252C%2520aiming%2520to%2520exploit%2520the%250Atransformer%2527s%2520capacity%2520for%2520capturing%2520long-range%2520dependencies%2520across%2520embeddings%250Ato%2520further%2520improve%2520performance.%2520Our%2520key%2520contributions%2520are%2520fourfold%253A%2520First%252C%2520we%250Aintroduce%2520a%2520novel%2520application%2520of%2520transformers%2520in%2520the%2520projection%2520head%2520role%2520for%250Acontrastive%2520learning%252C%2520marking%2520the%2520first%2520endeavor%2520of%2520its%2520kind.%2520Second%252C%2520our%250Aexperiments%2520reveal%2520a%2520compelling%2520%2522Deep%2520Fusion%2522%2520phenomenon%2520where%2520the%2520attention%250Amechanism%2520progressively%2520captures%2520the%2520correct%2520relational%2520dependencies%2520among%250Asamples%2520from%2520the%2520same%2520class%2520in%2520deeper%2520layers.%2520Third%252C%2520we%2520provide%2520a%2520theoretical%250Aframework%2520that%2520explains%2520and%2520supports%2520this%2520%2522Deep%2520Fusion%2522%2520behavior.%2520Finally%252C%2520we%250Ademonstrate%2520through%2520experimental%2520results%2520that%2520our%2520model%2520achieves%2520superior%250Aperformance%2520compared%2520to%2520the%2520existing%2520approach%2520of%2520using%2520a%2520feed-forward%2520layer.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2403.18681v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Deep%20Fusion%3A%20Capturing%20Dependencies%20in%20Contrastive%20Learning%20via%0A%20%20Transformer%20Projection%20Heads&entry.906535625=Huanran%20Li%20and%20Daniel%20Pimentel-Alarc%C3%B3n&entry.1292438233=%20%20Contrastive%20Learning%20%28CL%29%20has%20emerged%20as%20a%20powerful%20method%20for%20training%0Afeature%20extraction%20models%20using%20unlabeled%20data.%20Recent%20studies%20suggest%20that%0Aincorporating%20a%20linear%20projection%20head%20post-backbone%20significantly%20enhances%0Amodel%20performance.%20In%20this%20work%2C%20we%20investigate%20the%20use%20of%20a%20transformer%20model%0Aas%20a%20projection%20head%20within%20the%20CL%20framework%2C%20aiming%20to%20exploit%20the%0Atransformer%27s%20capacity%20for%20capturing%20long-range%20dependencies%20across%20embeddings%0Ato%20further%20improve%20performance.%20Our%20key%20contributions%20are%20fourfold%3A%20First%2C%20we%0Aintroduce%20a%20novel%20application%20of%20transformers%20in%20the%20projection%20head%20role%20for%0Acontrastive%20learning%2C%20marking%20the%20first%20endeavor%20of%20its%20kind.%20Second%2C%20our%0Aexperiments%20reveal%20a%20compelling%20%22Deep%20Fusion%22%20phenomenon%20where%20the%20attention%0Amechanism%20progressively%20captures%20the%20correct%20relational%20dependencies%20among%0Asamples%20from%20the%20same%20class%20in%20deeper%20layers.%20Third%2C%20we%20provide%20a%20theoretical%0Aframework%20that%20explains%20and%20supports%20this%20%22Deep%20Fusion%22%20behavior.%20Finally%2C%20we%0Ademonstrate%20through%20experimental%20results%20that%20our%20model%20achieves%20superior%0Aperformance%20compared%20to%20the%20existing%20approach%20of%20using%20a%20feed-forward%20layer.%0A&entry.1838667208=http%3A//arxiv.org/abs/2403.18681v2&entry.124074799=Read"},
{"title": "Brain Mapping with Dense Features: Grounding Cortical Semantic\n  Selectivity in Natural Images With Vision Transformers", "author": "Andrew F. Luo and Jacob Yeung and Rushikesh Zawar and Shaurya Dewan and Margaret M. Henderson and Leila Wehbe and Michael J. Tarr", "abstract": "  Advances in large-scale artificial neural networks have facilitated novel\ninsights into the functional topology of the brain. Here, we leverage this\napproach to study how semantic categories are organized in the human visual\ncortex. To overcome the challenge presented by the co-occurrence of multiple\ncategories in natural images, we introduce BrainSAIL (Semantic Attribution and\nImage Localization), a method for isolating specific neurally-activating visual\nconcepts in images. BrainSAIL exploits semantically consistent, dense spatial\nfeatures from pre-trained vision models, building upon their demonstrated\nability to robustly predict neural activity. This method derives clean,\nspatially dense embeddings without requiring any additional training, and\nemploys a novel denoising process that leverages the semantic consistency of\nimages under random augmentations. By unifying the space of whole-image\nembeddings and dense visual features and then applying voxel-wise encoding\nmodels to these features, we enable the identification of specific subregions\nof each image which drive selectivity patterns in different areas of the higher\nvisual cortex. We validate BrainSAIL on cortical regions with known category\nselectivity, demonstrating its ability to accurately localize and disentangle\nselectivity to diverse visual concepts. Next, we demonstrate BrainSAIL's\nability to characterize high-level visual selectivity to scene properties and\nlow-level visual features such as depth, luminance, and saturation, providing\ninsights into the encoding of complex visual information. Finally, we use\nBrainSAIL to directly compare the feature selectivity of different brain\nencoding models across different regions of interest in visual cortex. Our\ninnovative method paves the way for significant advances in mapping and\ndecomposing high-level visual representations in the human brain.\n", "link": "http://arxiv.org/abs/2410.05266v1", "date": "2024-10-07", "relevancy": 2.2311, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5622}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5622}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.5356}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Brain%20Mapping%20with%20Dense%20Features%3A%20Grounding%20Cortical%20Semantic%0A%20%20Selectivity%20in%20Natural%20Images%20With%20Vision%20Transformers&body=Title%3A%20Brain%20Mapping%20with%20Dense%20Features%3A%20Grounding%20Cortical%20Semantic%0A%20%20Selectivity%20in%20Natural%20Images%20With%20Vision%20Transformers%0AAuthor%3A%20Andrew%20F.%20Luo%20and%20Jacob%20Yeung%20and%20Rushikesh%20Zawar%20and%20Shaurya%20Dewan%20and%20Margaret%20M.%20Henderson%20and%20Leila%20Wehbe%20and%20Michael%20J.%20Tarr%0AAbstract%3A%20%20%20Advances%20in%20large-scale%20artificial%20neural%20networks%20have%20facilitated%20novel%0Ainsights%20into%20the%20functional%20topology%20of%20the%20brain.%20Here%2C%20we%20leverage%20this%0Aapproach%20to%20study%20how%20semantic%20categories%20are%20organized%20in%20the%20human%20visual%0Acortex.%20To%20overcome%20the%20challenge%20presented%20by%20the%20co-occurrence%20of%20multiple%0Acategories%20in%20natural%20images%2C%20we%20introduce%20BrainSAIL%20%28Semantic%20Attribution%20and%0AImage%20Localization%29%2C%20a%20method%20for%20isolating%20specific%20neurally-activating%20visual%0Aconcepts%20in%20images.%20BrainSAIL%20exploits%20semantically%20consistent%2C%20dense%20spatial%0Afeatures%20from%20pre-trained%20vision%20models%2C%20building%20upon%20their%20demonstrated%0Aability%20to%20robustly%20predict%20neural%20activity.%20This%20method%20derives%20clean%2C%0Aspatially%20dense%20embeddings%20without%20requiring%20any%20additional%20training%2C%20and%0Aemploys%20a%20novel%20denoising%20process%20that%20leverages%20the%20semantic%20consistency%20of%0Aimages%20under%20random%20augmentations.%20By%20unifying%20the%20space%20of%20whole-image%0Aembeddings%20and%20dense%20visual%20features%20and%20then%20applying%20voxel-wise%20encoding%0Amodels%20to%20these%20features%2C%20we%20enable%20the%20identification%20of%20specific%20subregions%0Aof%20each%20image%20which%20drive%20selectivity%20patterns%20in%20different%20areas%20of%20the%20higher%0Avisual%20cortex.%20We%20validate%20BrainSAIL%20on%20cortical%20regions%20with%20known%20category%0Aselectivity%2C%20demonstrating%20its%20ability%20to%20accurately%20localize%20and%20disentangle%0Aselectivity%20to%20diverse%20visual%20concepts.%20Next%2C%20we%20demonstrate%20BrainSAIL%27s%0Aability%20to%20characterize%20high-level%20visual%20selectivity%20to%20scene%20properties%20and%0Alow-level%20visual%20features%20such%20as%20depth%2C%20luminance%2C%20and%20saturation%2C%20providing%0Ainsights%20into%20the%20encoding%20of%20complex%20visual%20information.%20Finally%2C%20we%20use%0ABrainSAIL%20to%20directly%20compare%20the%20feature%20selectivity%20of%20different%20brain%0Aencoding%20models%20across%20different%20regions%20of%20interest%20in%20visual%20cortex.%20Our%0Ainnovative%20method%20paves%20the%20way%20for%20significant%20advances%20in%20mapping%20and%0Adecomposing%20high-level%20visual%20representations%20in%20the%20human%20brain.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2410.05266v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DBrain%2520Mapping%2520with%2520Dense%2520Features%253A%2520Grounding%2520Cortical%2520Semantic%250A%2520%2520Selectivity%2520in%2520Natural%2520Images%2520With%2520Vision%2520Transformers%26entry.906535625%3DAndrew%2520F.%2520Luo%2520and%2520Jacob%2520Yeung%2520and%2520Rushikesh%2520Zawar%2520and%2520Shaurya%2520Dewan%2520and%2520Margaret%2520M.%2520Henderson%2520and%2520Leila%2520Wehbe%2520and%2520Michael%2520J.%2520Tarr%26entry.1292438233%3D%2520%2520Advances%2520in%2520large-scale%2520artificial%2520neural%2520networks%2520have%2520facilitated%2520novel%250Ainsights%2520into%2520the%2520functional%2520topology%2520of%2520the%2520brain.%2520Here%252C%2520we%2520leverage%2520this%250Aapproach%2520to%2520study%2520how%2520semantic%2520categories%2520are%2520organized%2520in%2520the%2520human%2520visual%250Acortex.%2520To%2520overcome%2520the%2520challenge%2520presented%2520by%2520the%2520co-occurrence%2520of%2520multiple%250Acategories%2520in%2520natural%2520images%252C%2520we%2520introduce%2520BrainSAIL%2520%2528Semantic%2520Attribution%2520and%250AImage%2520Localization%2529%252C%2520a%2520method%2520for%2520isolating%2520specific%2520neurally-activating%2520visual%250Aconcepts%2520in%2520images.%2520BrainSAIL%2520exploits%2520semantically%2520consistent%252C%2520dense%2520spatial%250Afeatures%2520from%2520pre-trained%2520vision%2520models%252C%2520building%2520upon%2520their%2520demonstrated%250Aability%2520to%2520robustly%2520predict%2520neural%2520activity.%2520This%2520method%2520derives%2520clean%252C%250Aspatially%2520dense%2520embeddings%2520without%2520requiring%2520any%2520additional%2520training%252C%2520and%250Aemploys%2520a%2520novel%2520denoising%2520process%2520that%2520leverages%2520the%2520semantic%2520consistency%2520of%250Aimages%2520under%2520random%2520augmentations.%2520By%2520unifying%2520the%2520space%2520of%2520whole-image%250Aembeddings%2520and%2520dense%2520visual%2520features%2520and%2520then%2520applying%2520voxel-wise%2520encoding%250Amodels%2520to%2520these%2520features%252C%2520we%2520enable%2520the%2520identification%2520of%2520specific%2520subregions%250Aof%2520each%2520image%2520which%2520drive%2520selectivity%2520patterns%2520in%2520different%2520areas%2520of%2520the%2520higher%250Avisual%2520cortex.%2520We%2520validate%2520BrainSAIL%2520on%2520cortical%2520regions%2520with%2520known%2520category%250Aselectivity%252C%2520demonstrating%2520its%2520ability%2520to%2520accurately%2520localize%2520and%2520disentangle%250Aselectivity%2520to%2520diverse%2520visual%2520concepts.%2520Next%252C%2520we%2520demonstrate%2520BrainSAIL%2527s%250Aability%2520to%2520characterize%2520high-level%2520visual%2520selectivity%2520to%2520scene%2520properties%2520and%250Alow-level%2520visual%2520features%2520such%2520as%2520depth%252C%2520luminance%252C%2520and%2520saturation%252C%2520providing%250Ainsights%2520into%2520the%2520encoding%2520of%2520complex%2520visual%2520information.%2520Finally%252C%2520we%2520use%250ABrainSAIL%2520to%2520directly%2520compare%2520the%2520feature%2520selectivity%2520of%2520different%2520brain%250Aencoding%2520models%2520across%2520different%2520regions%2520of%2520interest%2520in%2520visual%2520cortex.%2520Our%250Ainnovative%2520method%2520paves%2520the%2520way%2520for%2520significant%2520advances%2520in%2520mapping%2520and%250Adecomposing%2520high-level%2520visual%2520representations%2520in%2520the%2520human%2520brain.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2410.05266v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Brain%20Mapping%20with%20Dense%20Features%3A%20Grounding%20Cortical%20Semantic%0A%20%20Selectivity%20in%20Natural%20Images%20With%20Vision%20Transformers&entry.906535625=Andrew%20F.%20Luo%20and%20Jacob%20Yeung%20and%20Rushikesh%20Zawar%20and%20Shaurya%20Dewan%20and%20Margaret%20M.%20Henderson%20and%20Leila%20Wehbe%20and%20Michael%20J.%20Tarr&entry.1292438233=%20%20Advances%20in%20large-scale%20artificial%20neural%20networks%20have%20facilitated%20novel%0Ainsights%20into%20the%20functional%20topology%20of%20the%20brain.%20Here%2C%20we%20leverage%20this%0Aapproach%20to%20study%20how%20semantic%20categories%20are%20organized%20in%20the%20human%20visual%0Acortex.%20To%20overcome%20the%20challenge%20presented%20by%20the%20co-occurrence%20of%20multiple%0Acategories%20in%20natural%20images%2C%20we%20introduce%20BrainSAIL%20%28Semantic%20Attribution%20and%0AImage%20Localization%29%2C%20a%20method%20for%20isolating%20specific%20neurally-activating%20visual%0Aconcepts%20in%20images.%20BrainSAIL%20exploits%20semantically%20consistent%2C%20dense%20spatial%0Afeatures%20from%20pre-trained%20vision%20models%2C%20building%20upon%20their%20demonstrated%0Aability%20to%20robustly%20predict%20neural%20activity.%20This%20method%20derives%20clean%2C%0Aspatially%20dense%20embeddings%20without%20requiring%20any%20additional%20training%2C%20and%0Aemploys%20a%20novel%20denoising%20process%20that%20leverages%20the%20semantic%20consistency%20of%0Aimages%20under%20random%20augmentations.%20By%20unifying%20the%20space%20of%20whole-image%0Aembeddings%20and%20dense%20visual%20features%20and%20then%20applying%20voxel-wise%20encoding%0Amodels%20to%20these%20features%2C%20we%20enable%20the%20identification%20of%20specific%20subregions%0Aof%20each%20image%20which%20drive%20selectivity%20patterns%20in%20different%20areas%20of%20the%20higher%0Avisual%20cortex.%20We%20validate%20BrainSAIL%20on%20cortical%20regions%20with%20known%20category%0Aselectivity%2C%20demonstrating%20its%20ability%20to%20accurately%20localize%20and%20disentangle%0Aselectivity%20to%20diverse%20visual%20concepts.%20Next%2C%20we%20demonstrate%20BrainSAIL%27s%0Aability%20to%20characterize%20high-level%20visual%20selectivity%20to%20scene%20properties%20and%0Alow-level%20visual%20features%20such%20as%20depth%2C%20luminance%2C%20and%20saturation%2C%20providing%0Ainsights%20into%20the%20encoding%20of%20complex%20visual%20information.%20Finally%2C%20we%20use%0ABrainSAIL%20to%20directly%20compare%20the%20feature%20selectivity%20of%20different%20brain%0Aencoding%20models%20across%20different%20regions%20of%20interest%20in%20visual%20cortex.%20Our%0Ainnovative%20method%20paves%20the%20way%20for%20significant%20advances%20in%20mapping%20and%0Adecomposing%20high-level%20visual%20representations%20in%20the%20human%20brain.%0A&entry.1838667208=http%3A//arxiv.org/abs/2410.05266v1&entry.124074799=Read"},
{"title": "SePPO: Semi-Policy Preference Optimization for Diffusion Alignment", "author": "Daoan Zhang and Guangchen Lan and Dong-Jun Han and Wenlin Yao and Xiaoman Pan and Hongming Zhang and Mingxiao Li and Pengcheng Chen and Yu Dong and Christopher Brinton and Jiebo Luo", "abstract": "  Reinforcement learning from human feedback (RLHF) methods are emerging as a\nway to fine-tune diffusion models (DMs) for visual generation. However,\ncommonly used on-policy strategies are limited by the generalization capability\nof the reward model, while off-policy approaches require large amounts of\ndifficult-to-obtain paired human-annotated data, particularly in visual\ngeneration tasks. To address the limitations of both on- and off-policy RLHF,\nwe propose a preference optimization method that aligns DMs with preferences\nwithout relying on reward models or paired human-annotated data. Specifically,\nwe introduce a Semi-Policy Preference Optimization (SePPO) method. SePPO\nleverages previous checkpoints as reference models while using them to generate\non-policy reference samples, which replace \"losing images\" in preference pairs.\nThis approach allows us to optimize using only off-policy \"winning images.\"\nFurthermore, we design a strategy for reference model selection that expands\nthe exploration in the policy space. Notably, we do not simply treat reference\nsamples as negative examples for learning. Instead, we design an anchor-based\ncriterion to assess whether the reference samples are likely to be winning or\nlosing images, allowing the model to selectively learn from the generated\nreference samples. This approach mitigates performance degradation caused by\nthe uncertainty in reference sample quality. We validate SePPO across both\ntext-to-image and text-to-video benchmarks. SePPO surpasses all previous\napproaches on the text-to-image benchmarks and also demonstrates outstanding\nperformance on the text-to-video benchmarks. Code will be released in\nhttps://github.com/DwanZhang-AI/SePPO.\n", "link": "http://arxiv.org/abs/2410.05255v1", "date": "2024-10-07", "relevancy": 2.2246, "topK": [{"title": "ToDo: Token Downsampling for Efficient Generation of High-Resolution\n  Images", "link": "http://arxiv.org/abs/2402.13573v2", "similarity": 0.5622}, {"title": "Scaling Rectified Flow Transformers for High-Resolution Image Synthesis", "link": "http://arxiv.org/abs/2403.03206v1", "similarity": 0.5534}, {"title": "Magic-Me: Identity-Specific Video Customized Diffusion", "link": "http://arxiv.org/abs/2402.09368v1", "similarity": 0.5512}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20SePPO%3A%20Semi-Policy%20Preference%20Optimization%20for%20Diffusion%20Alignment&body=Title%3A%20SePPO%3A%20Semi-Policy%20Preference%20Optimization%20for%20Diffusion%20Alignment%0AAuthor%3A%20Daoan%20Zhang%20and%20Guangchen%20Lan%20and%20Dong-Jun%20Han%20and%20Wenlin%20Yao%20and%20Xiaoman%20Pan%20and%20Hongming%20Zhang%20and%20Mingxiao%20Li%20and%20Pengcheng%20Chen%20and%20Yu%20Dong%20and%20Christopher%20Brinton%20and%20Jiebo%20Luo%0AAbstract%3A%20%20%20Reinforcement%20learning%20from%20human%20feedback%20%28RLHF%29%20methods%20are%20emerging%20as%20a%0Away%20to%20fine-tune%20diffusion%20models%20%28DMs%29%20for%20visual%20generation.%20However%2C%0Acommonly%20used%20on-policy%20strategies%20are%20limited%20by%20the%20generalization%20capability%0Aof%20the%20reward%20model%2C%20while%20off-policy%20approaches%20require%20large%20amounts%20of%0Adifficult-to-obtain%20paired%20human-annotated%20data%2C%20particularly%20in%20visual%0Ageneration%20tasks.%20To%20address%20the%20limitations%20of%20both%20on-%20and%20off-policy%20RLHF%2C%0Awe%20propose%20a%20preference%20optimization%20method%20that%20aligns%20DMs%20with%20preferences%0Awithout%20relying%20on%20reward%20models%20or%20paired%20human-annotated%20data.%20Specifically%2C%0Awe%20introduce%20a%20Semi-Policy%20Preference%20Optimization%20%28SePPO%29%20method.%20SePPO%0Aleverages%20previous%20checkpoints%20as%20reference%20models%20while%20using%20them%20to%20generate%0Aon-policy%20reference%20samples%2C%20which%20replace%20%22losing%20images%22%20in%20preference%20pairs.%0AThis%20approach%20allows%20us%20to%20optimize%20using%20only%20off-policy%20%22winning%20images.%22%0AFurthermore%2C%20we%20design%20a%20strategy%20for%20reference%20model%20selection%20that%20expands%0Athe%20exploration%20in%20the%20policy%20space.%20Notably%2C%20we%20do%20not%20simply%20treat%20reference%0Asamples%20as%20negative%20examples%20for%20learning.%20Instead%2C%20we%20design%20an%20anchor-based%0Acriterion%20to%20assess%20whether%20the%20reference%20samples%20are%20likely%20to%20be%20winning%20or%0Alosing%20images%2C%20allowing%20the%20model%20to%20selectively%20learn%20from%20the%20generated%0Areference%20samples.%20This%20approach%20mitigates%20performance%20degradation%20caused%20by%0Athe%20uncertainty%20in%20reference%20sample%20quality.%20We%20validate%20SePPO%20across%20both%0Atext-to-image%20and%20text-to-video%20benchmarks.%20SePPO%20surpasses%20all%20previous%0Aapproaches%20on%20the%20text-to-image%20benchmarks%20and%20also%20demonstrates%20outstanding%0Aperformance%20on%20the%20text-to-video%20benchmarks.%20Code%20will%20be%20released%20in%0Ahttps%3A//github.com/DwanZhang-AI/SePPO.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2410.05255v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DSePPO%253A%2520Semi-Policy%2520Preference%2520Optimization%2520for%2520Diffusion%2520Alignment%26entry.906535625%3DDaoan%2520Zhang%2520and%2520Guangchen%2520Lan%2520and%2520Dong-Jun%2520Han%2520and%2520Wenlin%2520Yao%2520and%2520Xiaoman%2520Pan%2520and%2520Hongming%2520Zhang%2520and%2520Mingxiao%2520Li%2520and%2520Pengcheng%2520Chen%2520and%2520Yu%2520Dong%2520and%2520Christopher%2520Brinton%2520and%2520Jiebo%2520Luo%26entry.1292438233%3D%2520%2520Reinforcement%2520learning%2520from%2520human%2520feedback%2520%2528RLHF%2529%2520methods%2520are%2520emerging%2520as%2520a%250Away%2520to%2520fine-tune%2520diffusion%2520models%2520%2528DMs%2529%2520for%2520visual%2520generation.%2520However%252C%250Acommonly%2520used%2520on-policy%2520strategies%2520are%2520limited%2520by%2520the%2520generalization%2520capability%250Aof%2520the%2520reward%2520model%252C%2520while%2520off-policy%2520approaches%2520require%2520large%2520amounts%2520of%250Adifficult-to-obtain%2520paired%2520human-annotated%2520data%252C%2520particularly%2520in%2520visual%250Ageneration%2520tasks.%2520To%2520address%2520the%2520limitations%2520of%2520both%2520on-%2520and%2520off-policy%2520RLHF%252C%250Awe%2520propose%2520a%2520preference%2520optimization%2520method%2520that%2520aligns%2520DMs%2520with%2520preferences%250Awithout%2520relying%2520on%2520reward%2520models%2520or%2520paired%2520human-annotated%2520data.%2520Specifically%252C%250Awe%2520introduce%2520a%2520Semi-Policy%2520Preference%2520Optimization%2520%2528SePPO%2529%2520method.%2520SePPO%250Aleverages%2520previous%2520checkpoints%2520as%2520reference%2520models%2520while%2520using%2520them%2520to%2520generate%250Aon-policy%2520reference%2520samples%252C%2520which%2520replace%2520%2522losing%2520images%2522%2520in%2520preference%2520pairs.%250AThis%2520approach%2520allows%2520us%2520to%2520optimize%2520using%2520only%2520off-policy%2520%2522winning%2520images.%2522%250AFurthermore%252C%2520we%2520design%2520a%2520strategy%2520for%2520reference%2520model%2520selection%2520that%2520expands%250Athe%2520exploration%2520in%2520the%2520policy%2520space.%2520Notably%252C%2520we%2520do%2520not%2520simply%2520treat%2520reference%250Asamples%2520as%2520negative%2520examples%2520for%2520learning.%2520Instead%252C%2520we%2520design%2520an%2520anchor-based%250Acriterion%2520to%2520assess%2520whether%2520the%2520reference%2520samples%2520are%2520likely%2520to%2520be%2520winning%2520or%250Alosing%2520images%252C%2520allowing%2520the%2520model%2520to%2520selectively%2520learn%2520from%2520the%2520generated%250Areference%2520samples.%2520This%2520approach%2520mitigates%2520performance%2520degradation%2520caused%2520by%250Athe%2520uncertainty%2520in%2520reference%2520sample%2520quality.%2520We%2520validate%2520SePPO%2520across%2520both%250Atext-to-image%2520and%2520text-to-video%2520benchmarks.%2520SePPO%2520surpasses%2520all%2520previous%250Aapproaches%2520on%2520the%2520text-to-image%2520benchmarks%2520and%2520also%2520demonstrates%2520outstanding%250Aperformance%2520on%2520the%2520text-to-video%2520benchmarks.%2520Code%2520will%2520be%2520released%2520in%250Ahttps%253A//github.com/DwanZhang-AI/SePPO.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2410.05255v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=SePPO%3A%20Semi-Policy%20Preference%20Optimization%20for%20Diffusion%20Alignment&entry.906535625=Daoan%20Zhang%20and%20Guangchen%20Lan%20and%20Dong-Jun%20Han%20and%20Wenlin%20Yao%20and%20Xiaoman%20Pan%20and%20Hongming%20Zhang%20and%20Mingxiao%20Li%20and%20Pengcheng%20Chen%20and%20Yu%20Dong%20and%20Christopher%20Brinton%20and%20Jiebo%20Luo&entry.1292438233=%20%20Reinforcement%20learning%20from%20human%20feedback%20%28RLHF%29%20methods%20are%20emerging%20as%20a%0Away%20to%20fine-tune%20diffusion%20models%20%28DMs%29%20for%20visual%20generation.%20However%2C%0Acommonly%20used%20on-policy%20strategies%20are%20limited%20by%20the%20generalization%20capability%0Aof%20the%20reward%20model%2C%20while%20off-policy%20approaches%20require%20large%20amounts%20of%0Adifficult-to-obtain%20paired%20human-annotated%20data%2C%20particularly%20in%20visual%0Ageneration%20tasks.%20To%20address%20the%20limitations%20of%20both%20on-%20and%20off-policy%20RLHF%2C%0Awe%20propose%20a%20preference%20optimization%20method%20that%20aligns%20DMs%20with%20preferences%0Awithout%20relying%20on%20reward%20models%20or%20paired%20human-annotated%20data.%20Specifically%2C%0Awe%20introduce%20a%20Semi-Policy%20Preference%20Optimization%20%28SePPO%29%20method.%20SePPO%0Aleverages%20previous%20checkpoints%20as%20reference%20models%20while%20using%20them%20to%20generate%0Aon-policy%20reference%20samples%2C%20which%20replace%20%22losing%20images%22%20in%20preference%20pairs.%0AThis%20approach%20allows%20us%20to%20optimize%20using%20only%20off-policy%20%22winning%20images.%22%0AFurthermore%2C%20we%20design%20a%20strategy%20for%20reference%20model%20selection%20that%20expands%0Athe%20exploration%20in%20the%20policy%20space.%20Notably%2C%20we%20do%20not%20simply%20treat%20reference%0Asamples%20as%20negative%20examples%20for%20learning.%20Instead%2C%20we%20design%20an%20anchor-based%0Acriterion%20to%20assess%20whether%20the%20reference%20samples%20are%20likely%20to%20be%20winning%20or%0Alosing%20images%2C%20allowing%20the%20model%20to%20selectively%20learn%20from%20the%20generated%0Areference%20samples.%20This%20approach%20mitigates%20performance%20degradation%20caused%20by%0Athe%20uncertainty%20in%20reference%20sample%20quality.%20We%20validate%20SePPO%20across%20both%0Atext-to-image%20and%20text-to-video%20benchmarks.%20SePPO%20surpasses%20all%20previous%0Aapproaches%20on%20the%20text-to-image%20benchmarks%20and%20also%20demonstrates%20outstanding%0Aperformance%20on%20the%20text-to-video%20benchmarks.%20Code%20will%20be%20released%20in%0Ahttps%3A//github.com/DwanZhang-AI/SePPO.%0A&entry.1838667208=http%3A//arxiv.org/abs/2410.05255v1&entry.124074799=Read"},
{"title": "Leverage Knowledge Graph and Large Language Model for Law Article\n  Recommendation: A Case Study of Chinese Criminal Law", "author": "Yongming Chen and Miner Chen and Ye Zhu and Juan Pei and Siyu Chen and Yu Zhou and Yi Wang and Yifan Zhou and Hao Li and Songan Zhang", "abstract": "  Court efficiency is vital for social stability. However, in most countries\naround the world, the grassroots courts face case backlogs, with decisions\nrelying heavily on judicial personnel's cognitive labor, lacking intelligent\ntools to improve efficiency. To address this issue, we propose an efficient law\narticle recommendation approach utilizing a Knowledge Graph (KG) and a Large\nLanguage Model (LLM). Firstly, we propose a Case-Enhanced Law Article Knowledge\nGraph (CLAKG) as a database to store current law statutes, historical case\ninformation, and correspondence between law articles and historical cases.\nAdditionally, we introduce an automated CLAKG construction method based on LLM.\nOn this basis, we propose a closed-loop law article recommendation method.\nFinally, through a series of experiments using judgment documents from the\nwebsite \"China Judgements Online\", we have improved the accuracy of law article\nrecommendation in cases from 0.549 to 0.694, demonstrating that our proposed\nmethod significantly outperforms baseline approaches.\n", "link": "http://arxiv.org/abs/2410.04949v1", "date": "2024-10-07", "relevancy": 2.2191, "topK": [{"title": "Self-supervised Photographic Image Layout Representation Learning", "link": "http://arxiv.org/abs/2403.03740v1", "similarity": 0.4553}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.4381}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.4381}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Leverage%20Knowledge%20Graph%20and%20Large%20Language%20Model%20for%20Law%20Article%0A%20%20Recommendation%3A%20A%20Case%20Study%20of%20Chinese%20Criminal%20Law&body=Title%3A%20Leverage%20Knowledge%20Graph%20and%20Large%20Language%20Model%20for%20Law%20Article%0A%20%20Recommendation%3A%20A%20Case%20Study%20of%20Chinese%20Criminal%20Law%0AAuthor%3A%20Yongming%20Chen%20and%20Miner%20Chen%20and%20Ye%20Zhu%20and%20Juan%20Pei%20and%20Siyu%20Chen%20and%20Yu%20Zhou%20and%20Yi%20Wang%20and%20Yifan%20Zhou%20and%20Hao%20Li%20and%20Songan%20Zhang%0AAbstract%3A%20%20%20Court%20efficiency%20is%20vital%20for%20social%20stability.%20However%2C%20in%20most%20countries%0Aaround%20the%20world%2C%20the%20grassroots%20courts%20face%20case%20backlogs%2C%20with%20decisions%0Arelying%20heavily%20on%20judicial%20personnel%27s%20cognitive%20labor%2C%20lacking%20intelligent%0Atools%20to%20improve%20efficiency.%20To%20address%20this%20issue%2C%20we%20propose%20an%20efficient%20law%0Aarticle%20recommendation%20approach%20utilizing%20a%20Knowledge%20Graph%20%28KG%29%20and%20a%20Large%0ALanguage%20Model%20%28LLM%29.%20Firstly%2C%20we%20propose%20a%20Case-Enhanced%20Law%20Article%20Knowledge%0AGraph%20%28CLAKG%29%20as%20a%20database%20to%20store%20current%20law%20statutes%2C%20historical%20case%0Ainformation%2C%20and%20correspondence%20between%20law%20articles%20and%20historical%20cases.%0AAdditionally%2C%20we%20introduce%20an%20automated%20CLAKG%20construction%20method%20based%20on%20LLM.%0AOn%20this%20basis%2C%20we%20propose%20a%20closed-loop%20law%20article%20recommendation%20method.%0AFinally%2C%20through%20a%20series%20of%20experiments%20using%20judgment%20documents%20from%20the%0Awebsite%20%22China%20Judgements%20Online%22%2C%20we%20have%20improved%20the%20accuracy%20of%20law%20article%0Arecommendation%20in%20cases%20from%200.549%20to%200.694%2C%20demonstrating%20that%20our%20proposed%0Amethod%20significantly%20outperforms%20baseline%20approaches.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2410.04949v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DLeverage%2520Knowledge%2520Graph%2520and%2520Large%2520Language%2520Model%2520for%2520Law%2520Article%250A%2520%2520Recommendation%253A%2520A%2520Case%2520Study%2520of%2520Chinese%2520Criminal%2520Law%26entry.906535625%3DYongming%2520Chen%2520and%2520Miner%2520Chen%2520and%2520Ye%2520Zhu%2520and%2520Juan%2520Pei%2520and%2520Siyu%2520Chen%2520and%2520Yu%2520Zhou%2520and%2520Yi%2520Wang%2520and%2520Yifan%2520Zhou%2520and%2520Hao%2520Li%2520and%2520Songan%2520Zhang%26entry.1292438233%3D%2520%2520Court%2520efficiency%2520is%2520vital%2520for%2520social%2520stability.%2520However%252C%2520in%2520most%2520countries%250Aaround%2520the%2520world%252C%2520the%2520grassroots%2520courts%2520face%2520case%2520backlogs%252C%2520with%2520decisions%250Arelying%2520heavily%2520on%2520judicial%2520personnel%2527s%2520cognitive%2520labor%252C%2520lacking%2520intelligent%250Atools%2520to%2520improve%2520efficiency.%2520To%2520address%2520this%2520issue%252C%2520we%2520propose%2520an%2520efficient%2520law%250Aarticle%2520recommendation%2520approach%2520utilizing%2520a%2520Knowledge%2520Graph%2520%2528KG%2529%2520and%2520a%2520Large%250ALanguage%2520Model%2520%2528LLM%2529.%2520Firstly%252C%2520we%2520propose%2520a%2520Case-Enhanced%2520Law%2520Article%2520Knowledge%250AGraph%2520%2528CLAKG%2529%2520as%2520a%2520database%2520to%2520store%2520current%2520law%2520statutes%252C%2520historical%2520case%250Ainformation%252C%2520and%2520correspondence%2520between%2520law%2520articles%2520and%2520historical%2520cases.%250AAdditionally%252C%2520we%2520introduce%2520an%2520automated%2520CLAKG%2520construction%2520method%2520based%2520on%2520LLM.%250AOn%2520this%2520basis%252C%2520we%2520propose%2520a%2520closed-loop%2520law%2520article%2520recommendation%2520method.%250AFinally%252C%2520through%2520a%2520series%2520of%2520experiments%2520using%2520judgment%2520documents%2520from%2520the%250Awebsite%2520%2522China%2520Judgements%2520Online%2522%252C%2520we%2520have%2520improved%2520the%2520accuracy%2520of%2520law%2520article%250Arecommendation%2520in%2520cases%2520from%25200.549%2520to%25200.694%252C%2520demonstrating%2520that%2520our%2520proposed%250Amethod%2520significantly%2520outperforms%2520baseline%2520approaches.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2410.04949v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Leverage%20Knowledge%20Graph%20and%20Large%20Language%20Model%20for%20Law%20Article%0A%20%20Recommendation%3A%20A%20Case%20Study%20of%20Chinese%20Criminal%20Law&entry.906535625=Yongming%20Chen%20and%20Miner%20Chen%20and%20Ye%20Zhu%20and%20Juan%20Pei%20and%20Siyu%20Chen%20and%20Yu%20Zhou%20and%20Yi%20Wang%20and%20Yifan%20Zhou%20and%20Hao%20Li%20and%20Songan%20Zhang&entry.1292438233=%20%20Court%20efficiency%20is%20vital%20for%20social%20stability.%20However%2C%20in%20most%20countries%0Aaround%20the%20world%2C%20the%20grassroots%20courts%20face%20case%20backlogs%2C%20with%20decisions%0Arelying%20heavily%20on%20judicial%20personnel%27s%20cognitive%20labor%2C%20lacking%20intelligent%0Atools%20to%20improve%20efficiency.%20To%20address%20this%20issue%2C%20we%20propose%20an%20efficient%20law%0Aarticle%20recommendation%20approach%20utilizing%20a%20Knowledge%20Graph%20%28KG%29%20and%20a%20Large%0ALanguage%20Model%20%28LLM%29.%20Firstly%2C%20we%20propose%20a%20Case-Enhanced%20Law%20Article%20Knowledge%0AGraph%20%28CLAKG%29%20as%20a%20database%20to%20store%20current%20law%20statutes%2C%20historical%20case%0Ainformation%2C%20and%20correspondence%20between%20law%20articles%20and%20historical%20cases.%0AAdditionally%2C%20we%20introduce%20an%20automated%20CLAKG%20construction%20method%20based%20on%20LLM.%0AOn%20this%20basis%2C%20we%20propose%20a%20closed-loop%20law%20article%20recommendation%20method.%0AFinally%2C%20through%20a%20series%20of%20experiments%20using%20judgment%20documents%20from%20the%0Awebsite%20%22China%20Judgements%20Online%22%2C%20we%20have%20improved%20the%20accuracy%20of%20law%20article%0Arecommendation%20in%20cases%20from%200.549%20to%200.694%2C%20demonstrating%20that%20our%20proposed%0Amethod%20significantly%20outperforms%20baseline%20approaches.%0A&entry.1838667208=http%3A//arxiv.org/abs/2410.04949v1&entry.124074799=Read"},
{"title": "CYCLO: Cyclic Graph Transformer Approach to Multi-Object Relationship\n  Modeling in Aerial Videos", "author": "Trong-Thuan Nguyen and Pha Nguyen and Xin Li and Jackson Cothren and Alper Yilmaz and Khoa Luu", "abstract": "  Video scene graph generation (VidSGG) has emerged as a transformative\napproach to capturing and interpreting the intricate relationships among\nobjects and their temporal dynamics in video sequences. In this paper, we\nintroduce the new AeroEye dataset that focuses on multi-object relationship\nmodeling in aerial videos. Our AeroEye dataset features various drone scenes\nand includes a visually comprehensive and precise collection of predicates that\ncapture the intricate relationships and spatial arrangements among objects. To\nthis end, we propose the novel Cyclic Graph Transformer (CYCLO) approach that\nallows the model to capture both direct and long-range temporal dependencies by\ncontinuously updating the history of interactions in a circular manner. The\nproposed approach also allows one to handle sequences with inherent cyclical\npatterns and process object relationships in the correct sequential order.\nTherefore, it can effectively capture periodic and overlapping relationships\nwhile minimizing information loss. The extensive experiments on the AeroEye\ndataset demonstrate the effectiveness of the proposed CYCLO model,\ndemonstrating its potential to perform scene understanding on drone videos.\nFinally, the CYCLO method consistently achieves State-of-the-Art (SOTA) results\non two in-the-wild scene graph generation benchmarks, i.e., PVSG and ASPIRe.\n", "link": "http://arxiv.org/abs/2406.01029v2", "date": "2024-10-07", "relevancy": 2.2175, "topK": [{"title": "RoboEXP: Action-Conditioned Scene Graph via Interactive Exploration for\n  Robotic Manipulation", "link": "http://arxiv.org/abs/2402.15487v1", "similarity": 0.5884}, {"title": "CAT3D: Create Anything in 3D with Multi-View Diffusion Models", "link": "http://arxiv.org/abs/2405.10314v1", "similarity": 0.5476}, {"title": "CAT3D: Create Anything in 3D with Multi-View Diffusion Models", "link": "http://arxiv.org/abs/2405.10314v1", "similarity": 0.5476}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20CYCLO%3A%20Cyclic%20Graph%20Transformer%20Approach%20to%20Multi-Object%20Relationship%0A%20%20Modeling%20in%20Aerial%20Videos&body=Title%3A%20CYCLO%3A%20Cyclic%20Graph%20Transformer%20Approach%20to%20Multi-Object%20Relationship%0A%20%20Modeling%20in%20Aerial%20Videos%0AAuthor%3A%20Trong-Thuan%20Nguyen%20and%20Pha%20Nguyen%20and%20Xin%20Li%20and%20Jackson%20Cothren%20and%20Alper%20Yilmaz%20and%20Khoa%20Luu%0AAbstract%3A%20%20%20Video%20scene%20graph%20generation%20%28VidSGG%29%20has%20emerged%20as%20a%20transformative%0Aapproach%20to%20capturing%20and%20interpreting%20the%20intricate%20relationships%20among%0Aobjects%20and%20their%20temporal%20dynamics%20in%20video%20sequences.%20In%20this%20paper%2C%20we%0Aintroduce%20the%20new%20AeroEye%20dataset%20that%20focuses%20on%20multi-object%20relationship%0Amodeling%20in%20aerial%20videos.%20Our%20AeroEye%20dataset%20features%20various%20drone%20scenes%0Aand%20includes%20a%20visually%20comprehensive%20and%20precise%20collection%20of%20predicates%20that%0Acapture%20the%20intricate%20relationships%20and%20spatial%20arrangements%20among%20objects.%20To%0Athis%20end%2C%20we%20propose%20the%20novel%20Cyclic%20Graph%20Transformer%20%28CYCLO%29%20approach%20that%0Aallows%20the%20model%20to%20capture%20both%20direct%20and%20long-range%20temporal%20dependencies%20by%0Acontinuously%20updating%20the%20history%20of%20interactions%20in%20a%20circular%20manner.%20The%0Aproposed%20approach%20also%20allows%20one%20to%20handle%20sequences%20with%20inherent%20cyclical%0Apatterns%20and%20process%20object%20relationships%20in%20the%20correct%20sequential%20order.%0ATherefore%2C%20it%20can%20effectively%20capture%20periodic%20and%20overlapping%20relationships%0Awhile%20minimizing%20information%20loss.%20The%20extensive%20experiments%20on%20the%20AeroEye%0Adataset%20demonstrate%20the%20effectiveness%20of%20the%20proposed%20CYCLO%20model%2C%0Ademonstrating%20its%20potential%20to%20perform%20scene%20understanding%20on%20drone%20videos.%0AFinally%2C%20the%20CYCLO%20method%20consistently%20achieves%20State-of-the-Art%20%28SOTA%29%20results%0Aon%20two%20in-the-wild%20scene%20graph%20generation%20benchmarks%2C%20i.e.%2C%20PVSG%20and%20ASPIRe.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2406.01029v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DCYCLO%253A%2520Cyclic%2520Graph%2520Transformer%2520Approach%2520to%2520Multi-Object%2520Relationship%250A%2520%2520Modeling%2520in%2520Aerial%2520Videos%26entry.906535625%3DTrong-Thuan%2520Nguyen%2520and%2520Pha%2520Nguyen%2520and%2520Xin%2520Li%2520and%2520Jackson%2520Cothren%2520and%2520Alper%2520Yilmaz%2520and%2520Khoa%2520Luu%26entry.1292438233%3D%2520%2520Video%2520scene%2520graph%2520generation%2520%2528VidSGG%2529%2520has%2520emerged%2520as%2520a%2520transformative%250Aapproach%2520to%2520capturing%2520and%2520interpreting%2520the%2520intricate%2520relationships%2520among%250Aobjects%2520and%2520their%2520temporal%2520dynamics%2520in%2520video%2520sequences.%2520In%2520this%2520paper%252C%2520we%250Aintroduce%2520the%2520new%2520AeroEye%2520dataset%2520that%2520focuses%2520on%2520multi-object%2520relationship%250Amodeling%2520in%2520aerial%2520videos.%2520Our%2520AeroEye%2520dataset%2520features%2520various%2520drone%2520scenes%250Aand%2520includes%2520a%2520visually%2520comprehensive%2520and%2520precise%2520collection%2520of%2520predicates%2520that%250Acapture%2520the%2520intricate%2520relationships%2520and%2520spatial%2520arrangements%2520among%2520objects.%2520To%250Athis%2520end%252C%2520we%2520propose%2520the%2520novel%2520Cyclic%2520Graph%2520Transformer%2520%2528CYCLO%2529%2520approach%2520that%250Aallows%2520the%2520model%2520to%2520capture%2520both%2520direct%2520and%2520long-range%2520temporal%2520dependencies%2520by%250Acontinuously%2520updating%2520the%2520history%2520of%2520interactions%2520in%2520a%2520circular%2520manner.%2520The%250Aproposed%2520approach%2520also%2520allows%2520one%2520to%2520handle%2520sequences%2520with%2520inherent%2520cyclical%250Apatterns%2520and%2520process%2520object%2520relationships%2520in%2520the%2520correct%2520sequential%2520order.%250ATherefore%252C%2520it%2520can%2520effectively%2520capture%2520periodic%2520and%2520overlapping%2520relationships%250Awhile%2520minimizing%2520information%2520loss.%2520The%2520extensive%2520experiments%2520on%2520the%2520AeroEye%250Adataset%2520demonstrate%2520the%2520effectiveness%2520of%2520the%2520proposed%2520CYCLO%2520model%252C%250Ademonstrating%2520its%2520potential%2520to%2520perform%2520scene%2520understanding%2520on%2520drone%2520videos.%250AFinally%252C%2520the%2520CYCLO%2520method%2520consistently%2520achieves%2520State-of-the-Art%2520%2528SOTA%2529%2520results%250Aon%2520two%2520in-the-wild%2520scene%2520graph%2520generation%2520benchmarks%252C%2520i.e.%252C%2520PVSG%2520and%2520ASPIRe.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2406.01029v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=CYCLO%3A%20Cyclic%20Graph%20Transformer%20Approach%20to%20Multi-Object%20Relationship%0A%20%20Modeling%20in%20Aerial%20Videos&entry.906535625=Trong-Thuan%20Nguyen%20and%20Pha%20Nguyen%20and%20Xin%20Li%20and%20Jackson%20Cothren%20and%20Alper%20Yilmaz%20and%20Khoa%20Luu&entry.1292438233=%20%20Video%20scene%20graph%20generation%20%28VidSGG%29%20has%20emerged%20as%20a%20transformative%0Aapproach%20to%20capturing%20and%20interpreting%20the%20intricate%20relationships%20among%0Aobjects%20and%20their%20temporal%20dynamics%20in%20video%20sequences.%20In%20this%20paper%2C%20we%0Aintroduce%20the%20new%20AeroEye%20dataset%20that%20focuses%20on%20multi-object%20relationship%0Amodeling%20in%20aerial%20videos.%20Our%20AeroEye%20dataset%20features%20various%20drone%20scenes%0Aand%20includes%20a%20visually%20comprehensive%20and%20precise%20collection%20of%20predicates%20that%0Acapture%20the%20intricate%20relationships%20and%20spatial%20arrangements%20among%20objects.%20To%0Athis%20end%2C%20we%20propose%20the%20novel%20Cyclic%20Graph%20Transformer%20%28CYCLO%29%20approach%20that%0Aallows%20the%20model%20to%20capture%20both%20direct%20and%20long-range%20temporal%20dependencies%20by%0Acontinuously%20updating%20the%20history%20of%20interactions%20in%20a%20circular%20manner.%20The%0Aproposed%20approach%20also%20allows%20one%20to%20handle%20sequences%20with%20inherent%20cyclical%0Apatterns%20and%20process%20object%20relationships%20in%20the%20correct%20sequential%20order.%0ATherefore%2C%20it%20can%20effectively%20capture%20periodic%20and%20overlapping%20relationships%0Awhile%20minimizing%20information%20loss.%20The%20extensive%20experiments%20on%20the%20AeroEye%0Adataset%20demonstrate%20the%20effectiveness%20of%20the%20proposed%20CYCLO%20model%2C%0Ademonstrating%20its%20potential%20to%20perform%20scene%20understanding%20on%20drone%20videos.%0AFinally%2C%20the%20CYCLO%20method%20consistently%20achieves%20State-of-the-Art%20%28SOTA%29%20results%0Aon%20two%20in-the-wild%20scene%20graph%20generation%20benchmarks%2C%20i.e.%2C%20PVSG%20and%20ASPIRe.%0A&entry.1838667208=http%3A//arxiv.org/abs/2406.01029v2&entry.124074799=Read"},
{"title": "LADEV: A Language-Driven Testing and Evaluation Platform for\n  Vision-Language-Action Models in Robotic Manipulation", "author": "Zhijie Wang and Zhehua Zhou and Jiayang Song and Yuheng Huang and Zhan Shu and Lei Ma", "abstract": "  Building on the advancements of Large Language Models (LLMs) and Vision\nLanguage Models (VLMs), recent research has introduced Vision-Language-Action\n(VLA) models as an integrated solution for robotic manipulation tasks. These\nmodels take camera images and natural language task instructions as input and\ndirectly generate control actions for robots to perform specified tasks,\ngreatly improving both decision-making capabilities and interaction with human\nusers. However, the data-driven nature of VLA models, combined with their lack\nof interpretability, makes the assurance of their effectiveness and robustness\na challenging task. This highlights the need for a reliable testing and\nevaluation platform. For this purpose, in this work, we propose LADEV, a\ncomprehensive and efficient platform specifically designed for evaluating VLA\nmodels. We first present a language-driven approach that automatically\ngenerates simulation environments from natural language inputs, mitigating the\nneed for manual adjustments and significantly improving testing efficiency.\nThen, to further assess the influence of language input on the VLA models, we\nimplement a paraphrase mechanism that produces diverse natural language task\ninstructions for testing. Finally, to expedite the evaluation process, we\nintroduce a batch-style method for conducting large-scale testing of VLA\nmodels. Using LADEV, we conducted experiments on several state-of-the-art VLA\nmodels, demonstrating its effectiveness as a tool for evaluating these models.\nOur results showed that LADEV not only enhances testing efficiency but also\nestablishes a solid baseline for evaluating VLA models, paving the way for the\ndevelopment of more intelligent and advanced robotic systems.\n", "link": "http://arxiv.org/abs/2410.05191v1", "date": "2024-10-07", "relevancy": 2.2172, "topK": [{"title": "RoboEXP: Action-Conditioned Scene Graph via Interactive Exploration for\n  Robotic Manipulation", "link": "http://arxiv.org/abs/2402.15487v1", "similarity": 0.559}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5534}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5534}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20LADEV%3A%20A%20Language-Driven%20Testing%20and%20Evaluation%20Platform%20for%0A%20%20Vision-Language-Action%20Models%20in%20Robotic%20Manipulation&body=Title%3A%20LADEV%3A%20A%20Language-Driven%20Testing%20and%20Evaluation%20Platform%20for%0A%20%20Vision-Language-Action%20Models%20in%20Robotic%20Manipulation%0AAuthor%3A%20Zhijie%20Wang%20and%20Zhehua%20Zhou%20and%20Jiayang%20Song%20and%20Yuheng%20Huang%20and%20Zhan%20Shu%20and%20Lei%20Ma%0AAbstract%3A%20%20%20Building%20on%20the%20advancements%20of%20Large%20Language%20Models%20%28LLMs%29%20and%20Vision%0ALanguage%20Models%20%28VLMs%29%2C%20recent%20research%20has%20introduced%20Vision-Language-Action%0A%28VLA%29%20models%20as%20an%20integrated%20solution%20for%20robotic%20manipulation%20tasks.%20These%0Amodels%20take%20camera%20images%20and%20natural%20language%20task%20instructions%20as%20input%20and%0Adirectly%20generate%20control%20actions%20for%20robots%20to%20perform%20specified%20tasks%2C%0Agreatly%20improving%20both%20decision-making%20capabilities%20and%20interaction%20with%20human%0Ausers.%20However%2C%20the%20data-driven%20nature%20of%20VLA%20models%2C%20combined%20with%20their%20lack%0Aof%20interpretability%2C%20makes%20the%20assurance%20of%20their%20effectiveness%20and%20robustness%0Aa%20challenging%20task.%20This%20highlights%20the%20need%20for%20a%20reliable%20testing%20and%0Aevaluation%20platform.%20For%20this%20purpose%2C%20in%20this%20work%2C%20we%20propose%20LADEV%2C%20a%0Acomprehensive%20and%20efficient%20platform%20specifically%20designed%20for%20evaluating%20VLA%0Amodels.%20We%20first%20present%20a%20language-driven%20approach%20that%20automatically%0Agenerates%20simulation%20environments%20from%20natural%20language%20inputs%2C%20mitigating%20the%0Aneed%20for%20manual%20adjustments%20and%20significantly%20improving%20testing%20efficiency.%0AThen%2C%20to%20further%20assess%20the%20influence%20of%20language%20input%20on%20the%20VLA%20models%2C%20we%0Aimplement%20a%20paraphrase%20mechanism%20that%20produces%20diverse%20natural%20language%20task%0Ainstructions%20for%20testing.%20Finally%2C%20to%20expedite%20the%20evaluation%20process%2C%20we%0Aintroduce%20a%20batch-style%20method%20for%20conducting%20large-scale%20testing%20of%20VLA%0Amodels.%20Using%20LADEV%2C%20we%20conducted%20experiments%20on%20several%20state-of-the-art%20VLA%0Amodels%2C%20demonstrating%20its%20effectiveness%20as%20a%20tool%20for%20evaluating%20these%20models.%0AOur%20results%20showed%20that%20LADEV%20not%20only%20enhances%20testing%20efficiency%20but%20also%0Aestablishes%20a%20solid%20baseline%20for%20evaluating%20VLA%20models%2C%20paving%20the%20way%20for%20the%0Adevelopment%20of%20more%20intelligent%20and%20advanced%20robotic%20systems.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2410.05191v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DLADEV%253A%2520A%2520Language-Driven%2520Testing%2520and%2520Evaluation%2520Platform%2520for%250A%2520%2520Vision-Language-Action%2520Models%2520in%2520Robotic%2520Manipulation%26entry.906535625%3DZhijie%2520Wang%2520and%2520Zhehua%2520Zhou%2520and%2520Jiayang%2520Song%2520and%2520Yuheng%2520Huang%2520and%2520Zhan%2520Shu%2520and%2520Lei%2520Ma%26entry.1292438233%3D%2520%2520Building%2520on%2520the%2520advancements%2520of%2520Large%2520Language%2520Models%2520%2528LLMs%2529%2520and%2520Vision%250ALanguage%2520Models%2520%2528VLMs%2529%252C%2520recent%2520research%2520has%2520introduced%2520Vision-Language-Action%250A%2528VLA%2529%2520models%2520as%2520an%2520integrated%2520solution%2520for%2520robotic%2520manipulation%2520tasks.%2520These%250Amodels%2520take%2520camera%2520images%2520and%2520natural%2520language%2520task%2520instructions%2520as%2520input%2520and%250Adirectly%2520generate%2520control%2520actions%2520for%2520robots%2520to%2520perform%2520specified%2520tasks%252C%250Agreatly%2520improving%2520both%2520decision-making%2520capabilities%2520and%2520interaction%2520with%2520human%250Ausers.%2520However%252C%2520the%2520data-driven%2520nature%2520of%2520VLA%2520models%252C%2520combined%2520with%2520their%2520lack%250Aof%2520interpretability%252C%2520makes%2520the%2520assurance%2520of%2520their%2520effectiveness%2520and%2520robustness%250Aa%2520challenging%2520task.%2520This%2520highlights%2520the%2520need%2520for%2520a%2520reliable%2520testing%2520and%250Aevaluation%2520platform.%2520For%2520this%2520purpose%252C%2520in%2520this%2520work%252C%2520we%2520propose%2520LADEV%252C%2520a%250Acomprehensive%2520and%2520efficient%2520platform%2520specifically%2520designed%2520for%2520evaluating%2520VLA%250Amodels.%2520We%2520first%2520present%2520a%2520language-driven%2520approach%2520that%2520automatically%250Agenerates%2520simulation%2520environments%2520from%2520natural%2520language%2520inputs%252C%2520mitigating%2520the%250Aneed%2520for%2520manual%2520adjustments%2520and%2520significantly%2520improving%2520testing%2520efficiency.%250AThen%252C%2520to%2520further%2520assess%2520the%2520influence%2520of%2520language%2520input%2520on%2520the%2520VLA%2520models%252C%2520we%250Aimplement%2520a%2520paraphrase%2520mechanism%2520that%2520produces%2520diverse%2520natural%2520language%2520task%250Ainstructions%2520for%2520testing.%2520Finally%252C%2520to%2520expedite%2520the%2520evaluation%2520process%252C%2520we%250Aintroduce%2520a%2520batch-style%2520method%2520for%2520conducting%2520large-scale%2520testing%2520of%2520VLA%250Amodels.%2520Using%2520LADEV%252C%2520we%2520conducted%2520experiments%2520on%2520several%2520state-of-the-art%2520VLA%250Amodels%252C%2520demonstrating%2520its%2520effectiveness%2520as%2520a%2520tool%2520for%2520evaluating%2520these%2520models.%250AOur%2520results%2520showed%2520that%2520LADEV%2520not%2520only%2520enhances%2520testing%2520efficiency%2520but%2520also%250Aestablishes%2520a%2520solid%2520baseline%2520for%2520evaluating%2520VLA%2520models%252C%2520paving%2520the%2520way%2520for%2520the%250Adevelopment%2520of%2520more%2520intelligent%2520and%2520advanced%2520robotic%2520systems.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2410.05191v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=LADEV%3A%20A%20Language-Driven%20Testing%20and%20Evaluation%20Platform%20for%0A%20%20Vision-Language-Action%20Models%20in%20Robotic%20Manipulation&entry.906535625=Zhijie%20Wang%20and%20Zhehua%20Zhou%20and%20Jiayang%20Song%20and%20Yuheng%20Huang%20and%20Zhan%20Shu%20and%20Lei%20Ma&entry.1292438233=%20%20Building%20on%20the%20advancements%20of%20Large%20Language%20Models%20%28LLMs%29%20and%20Vision%0ALanguage%20Models%20%28VLMs%29%2C%20recent%20research%20has%20introduced%20Vision-Language-Action%0A%28VLA%29%20models%20as%20an%20integrated%20solution%20for%20robotic%20manipulation%20tasks.%20These%0Amodels%20take%20camera%20images%20and%20natural%20language%20task%20instructions%20as%20input%20and%0Adirectly%20generate%20control%20actions%20for%20robots%20to%20perform%20specified%20tasks%2C%0Agreatly%20improving%20both%20decision-making%20capabilities%20and%20interaction%20with%20human%0Ausers.%20However%2C%20the%20data-driven%20nature%20of%20VLA%20models%2C%20combined%20with%20their%20lack%0Aof%20interpretability%2C%20makes%20the%20assurance%20of%20their%20effectiveness%20and%20robustness%0Aa%20challenging%20task.%20This%20highlights%20the%20need%20for%20a%20reliable%20testing%20and%0Aevaluation%20platform.%20For%20this%20purpose%2C%20in%20this%20work%2C%20we%20propose%20LADEV%2C%20a%0Acomprehensive%20and%20efficient%20platform%20specifically%20designed%20for%20evaluating%20VLA%0Amodels.%20We%20first%20present%20a%20language-driven%20approach%20that%20automatically%0Agenerates%20simulation%20environments%20from%20natural%20language%20inputs%2C%20mitigating%20the%0Aneed%20for%20manual%20adjustments%20and%20significantly%20improving%20testing%20efficiency.%0AThen%2C%20to%20further%20assess%20the%20influence%20of%20language%20input%20on%20the%20VLA%20models%2C%20we%0Aimplement%20a%20paraphrase%20mechanism%20that%20produces%20diverse%20natural%20language%20task%0Ainstructions%20for%20testing.%20Finally%2C%20to%20expedite%20the%20evaluation%20process%2C%20we%0Aintroduce%20a%20batch-style%20method%20for%20conducting%20large-scale%20testing%20of%20VLA%0Amodels.%20Using%20LADEV%2C%20we%20conducted%20experiments%20on%20several%20state-of-the-art%20VLA%0Amodels%2C%20demonstrating%20its%20effectiveness%20as%20a%20tool%20for%20evaluating%20these%20models.%0AOur%20results%20showed%20that%20LADEV%20not%20only%20enhances%20testing%20efficiency%20but%20also%0Aestablishes%20a%20solid%20baseline%20for%20evaluating%20VLA%20models%2C%20paving%20the%20way%20for%20the%0Adevelopment%20of%20more%20intelligent%20and%20advanced%20robotic%20systems.%0A&entry.1838667208=http%3A//arxiv.org/abs/2410.05191v1&entry.124074799=Read"},
{"title": "Navigating the Digital World as Humans Do: Universal Visual Grounding\n  for GUI Agents", "author": "Boyu Gou and Ruohan Wang and Boyuan Zheng and Yanan Xie and Cheng Chang and Yiheng Shu and Huan Sun and Yu Su", "abstract": "  Multimodal large language models (MLLMs) are transforming the capabilities of\ngraphical user interface (GUI) agents, facilitating their transition from\ncontrolled simulations to complex, real-world applications across various\nplatforms. However, the effectiveness of these agents hinges on the robustness\nof their grounding capability. Current GUI agents predominantly utilize\ntext-based representations such as HTML or accessibility trees, which, despite\ntheir utility, often introduce noise, incompleteness, and increased\ncomputational overhead. In this paper, we advocate a human-like embodiment for\nGUI agents that perceive the environment entirely visually and directly take\npixel-level operations on the GUI. The key is visual grounding models that can\naccurately map diverse referring expressions of GUI elements to their\ncoordinates on the GUI across different platforms. We show that a simple\nrecipe, which includes web-based synthetic data and slight adaptation of the\nLLaVA architecture, is surprisingly effective for training such visual\ngrounding models. We collect the largest dataset for GUI visual grounding so\nfar, containing 10M GUI elements and their referring expressions over 1.3M\nscreenshots, and use it to train UGround, a strong universal visual grounding\nmodel for GUI agents. Empirical results on six benchmarks spanning three\ncategories (grounding, offline agent, and online agent) show that 1) UGround\nsubstantially outperforms existing visual grounding models for GUI agents, by\nup to 20% absolute, and 2) agents with UGround outperform state-of-the-art\nagents, despite the fact that existing agents use additional text-based input\nwhile ours only uses visual perception. These results provide strong support\nfor the feasibility and promises of GUI agents that navigate the digital world\nas humans do.\n", "link": "http://arxiv.org/abs/2410.05243v1", "date": "2024-10-07", "relevancy": 2.2104, "topK": [{"title": "UGG: Unified Generative Grasping", "link": "https://arxiv.org/abs/2311.16917", "similarity": 0.5619}, {"title": "Genie: Generative Interactive Environments", "link": "http://arxiv.org/abs/2402.15391v1", "similarity": 0.5477}, {"title": "RoboEXP: Action-Conditioned Scene Graph via Interactive Exploration for\n  Robotic Manipulation", "link": "http://arxiv.org/abs/2402.15487v1", "similarity": 0.5416}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Navigating%20the%20Digital%20World%20as%20Humans%20Do%3A%20Universal%20Visual%20Grounding%0A%20%20for%20GUI%20Agents&body=Title%3A%20Navigating%20the%20Digital%20World%20as%20Humans%20Do%3A%20Universal%20Visual%20Grounding%0A%20%20for%20GUI%20Agents%0AAuthor%3A%20Boyu%20Gou%20and%20Ruohan%20Wang%20and%20Boyuan%20Zheng%20and%20Yanan%20Xie%20and%20Cheng%20Chang%20and%20Yiheng%20Shu%20and%20Huan%20Sun%20and%20Yu%20Su%0AAbstract%3A%20%20%20Multimodal%20large%20language%20models%20%28MLLMs%29%20are%20transforming%20the%20capabilities%20of%0Agraphical%20user%20interface%20%28GUI%29%20agents%2C%20facilitating%20their%20transition%20from%0Acontrolled%20simulations%20to%20complex%2C%20real-world%20applications%20across%20various%0Aplatforms.%20However%2C%20the%20effectiveness%20of%20these%20agents%20hinges%20on%20the%20robustness%0Aof%20their%20grounding%20capability.%20Current%20GUI%20agents%20predominantly%20utilize%0Atext-based%20representations%20such%20as%20HTML%20or%20accessibility%20trees%2C%20which%2C%20despite%0Atheir%20utility%2C%20often%20introduce%20noise%2C%20incompleteness%2C%20and%20increased%0Acomputational%20overhead.%20In%20this%20paper%2C%20we%20advocate%20a%20human-like%20embodiment%20for%0AGUI%20agents%20that%20perceive%20the%20environment%20entirely%20visually%20and%20directly%20take%0Apixel-level%20operations%20on%20the%20GUI.%20The%20key%20is%20visual%20grounding%20models%20that%20can%0Aaccurately%20map%20diverse%20referring%20expressions%20of%20GUI%20elements%20to%20their%0Acoordinates%20on%20the%20GUI%20across%20different%20platforms.%20We%20show%20that%20a%20simple%0Arecipe%2C%20which%20includes%20web-based%20synthetic%20data%20and%20slight%20adaptation%20of%20the%0ALLaVA%20architecture%2C%20is%20surprisingly%20effective%20for%20training%20such%20visual%0Agrounding%20models.%20We%20collect%20the%20largest%20dataset%20for%20GUI%20visual%20grounding%20so%0Afar%2C%20containing%2010M%20GUI%20elements%20and%20their%20referring%20expressions%20over%201.3M%0Ascreenshots%2C%20and%20use%20it%20to%20train%20UGround%2C%20a%20strong%20universal%20visual%20grounding%0Amodel%20for%20GUI%20agents.%20Empirical%20results%20on%20six%20benchmarks%20spanning%20three%0Acategories%20%28grounding%2C%20offline%20agent%2C%20and%20online%20agent%29%20show%20that%201%29%20UGround%0Asubstantially%20outperforms%20existing%20visual%20grounding%20models%20for%20GUI%20agents%2C%20by%0Aup%20to%2020%25%20absolute%2C%20and%202%29%20agents%20with%20UGround%20outperform%20state-of-the-art%0Aagents%2C%20despite%20the%20fact%20that%20existing%20agents%20use%20additional%20text-based%20input%0Awhile%20ours%20only%20uses%20visual%20perception.%20These%20results%20provide%20strong%20support%0Afor%20the%20feasibility%20and%20promises%20of%20GUI%20agents%20that%20navigate%20the%20digital%20world%0Aas%20humans%20do.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2410.05243v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DNavigating%2520the%2520Digital%2520World%2520as%2520Humans%2520Do%253A%2520Universal%2520Visual%2520Grounding%250A%2520%2520for%2520GUI%2520Agents%26entry.906535625%3DBoyu%2520Gou%2520and%2520Ruohan%2520Wang%2520and%2520Boyuan%2520Zheng%2520and%2520Yanan%2520Xie%2520and%2520Cheng%2520Chang%2520and%2520Yiheng%2520Shu%2520and%2520Huan%2520Sun%2520and%2520Yu%2520Su%26entry.1292438233%3D%2520%2520Multimodal%2520large%2520language%2520models%2520%2528MLLMs%2529%2520are%2520transforming%2520the%2520capabilities%2520of%250Agraphical%2520user%2520interface%2520%2528GUI%2529%2520agents%252C%2520facilitating%2520their%2520transition%2520from%250Acontrolled%2520simulations%2520to%2520complex%252C%2520real-world%2520applications%2520across%2520various%250Aplatforms.%2520However%252C%2520the%2520effectiveness%2520of%2520these%2520agents%2520hinges%2520on%2520the%2520robustness%250Aof%2520their%2520grounding%2520capability.%2520Current%2520GUI%2520agents%2520predominantly%2520utilize%250Atext-based%2520representations%2520such%2520as%2520HTML%2520or%2520accessibility%2520trees%252C%2520which%252C%2520despite%250Atheir%2520utility%252C%2520often%2520introduce%2520noise%252C%2520incompleteness%252C%2520and%2520increased%250Acomputational%2520overhead.%2520In%2520this%2520paper%252C%2520we%2520advocate%2520a%2520human-like%2520embodiment%2520for%250AGUI%2520agents%2520that%2520perceive%2520the%2520environment%2520entirely%2520visually%2520and%2520directly%2520take%250Apixel-level%2520operations%2520on%2520the%2520GUI.%2520The%2520key%2520is%2520visual%2520grounding%2520models%2520that%2520can%250Aaccurately%2520map%2520diverse%2520referring%2520expressions%2520of%2520GUI%2520elements%2520to%2520their%250Acoordinates%2520on%2520the%2520GUI%2520across%2520different%2520platforms.%2520We%2520show%2520that%2520a%2520simple%250Arecipe%252C%2520which%2520includes%2520web-based%2520synthetic%2520data%2520and%2520slight%2520adaptation%2520of%2520the%250ALLaVA%2520architecture%252C%2520is%2520surprisingly%2520effective%2520for%2520training%2520such%2520visual%250Agrounding%2520models.%2520We%2520collect%2520the%2520largest%2520dataset%2520for%2520GUI%2520visual%2520grounding%2520so%250Afar%252C%2520containing%252010M%2520GUI%2520elements%2520and%2520their%2520referring%2520expressions%2520over%25201.3M%250Ascreenshots%252C%2520and%2520use%2520it%2520to%2520train%2520UGround%252C%2520a%2520strong%2520universal%2520visual%2520grounding%250Amodel%2520for%2520GUI%2520agents.%2520Empirical%2520results%2520on%2520six%2520benchmarks%2520spanning%2520three%250Acategories%2520%2528grounding%252C%2520offline%2520agent%252C%2520and%2520online%2520agent%2529%2520show%2520that%25201%2529%2520UGround%250Asubstantially%2520outperforms%2520existing%2520visual%2520grounding%2520models%2520for%2520GUI%2520agents%252C%2520by%250Aup%2520to%252020%2525%2520absolute%252C%2520and%25202%2529%2520agents%2520with%2520UGround%2520outperform%2520state-of-the-art%250Aagents%252C%2520despite%2520the%2520fact%2520that%2520existing%2520agents%2520use%2520additional%2520text-based%2520input%250Awhile%2520ours%2520only%2520uses%2520visual%2520perception.%2520These%2520results%2520provide%2520strong%2520support%250Afor%2520the%2520feasibility%2520and%2520promises%2520of%2520GUI%2520agents%2520that%2520navigate%2520the%2520digital%2520world%250Aas%2520humans%2520do.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2410.05243v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Navigating%20the%20Digital%20World%20as%20Humans%20Do%3A%20Universal%20Visual%20Grounding%0A%20%20for%20GUI%20Agents&entry.906535625=Boyu%20Gou%20and%20Ruohan%20Wang%20and%20Boyuan%20Zheng%20and%20Yanan%20Xie%20and%20Cheng%20Chang%20and%20Yiheng%20Shu%20and%20Huan%20Sun%20and%20Yu%20Su&entry.1292438233=%20%20Multimodal%20large%20language%20models%20%28MLLMs%29%20are%20transforming%20the%20capabilities%20of%0Agraphical%20user%20interface%20%28GUI%29%20agents%2C%20facilitating%20their%20transition%20from%0Acontrolled%20simulations%20to%20complex%2C%20real-world%20applications%20across%20various%0Aplatforms.%20However%2C%20the%20effectiveness%20of%20these%20agents%20hinges%20on%20the%20robustness%0Aof%20their%20grounding%20capability.%20Current%20GUI%20agents%20predominantly%20utilize%0Atext-based%20representations%20such%20as%20HTML%20or%20accessibility%20trees%2C%20which%2C%20despite%0Atheir%20utility%2C%20often%20introduce%20noise%2C%20incompleteness%2C%20and%20increased%0Acomputational%20overhead.%20In%20this%20paper%2C%20we%20advocate%20a%20human-like%20embodiment%20for%0AGUI%20agents%20that%20perceive%20the%20environment%20entirely%20visually%20and%20directly%20take%0Apixel-level%20operations%20on%20the%20GUI.%20The%20key%20is%20visual%20grounding%20models%20that%20can%0Aaccurately%20map%20diverse%20referring%20expressions%20of%20GUI%20elements%20to%20their%0Acoordinates%20on%20the%20GUI%20across%20different%20platforms.%20We%20show%20that%20a%20simple%0Arecipe%2C%20which%20includes%20web-based%20synthetic%20data%20and%20slight%20adaptation%20of%20the%0ALLaVA%20architecture%2C%20is%20surprisingly%20effective%20for%20training%20such%20visual%0Agrounding%20models.%20We%20collect%20the%20largest%20dataset%20for%20GUI%20visual%20grounding%20so%0Afar%2C%20containing%2010M%20GUI%20elements%20and%20their%20referring%20expressions%20over%201.3M%0Ascreenshots%2C%20and%20use%20it%20to%20train%20UGround%2C%20a%20strong%20universal%20visual%20grounding%0Amodel%20for%20GUI%20agents.%20Empirical%20results%20on%20six%20benchmarks%20spanning%20three%0Acategories%20%28grounding%2C%20offline%20agent%2C%20and%20online%20agent%29%20show%20that%201%29%20UGround%0Asubstantially%20outperforms%20existing%20visual%20grounding%20models%20for%20GUI%20agents%2C%20by%0Aup%20to%2020%25%20absolute%2C%20and%202%29%20agents%20with%20UGround%20outperform%20state-of-the-art%0Aagents%2C%20despite%20the%20fact%20that%20existing%20agents%20use%20additional%20text-based%20input%0Awhile%20ours%20only%20uses%20visual%20perception.%20These%20results%20provide%20strong%20support%0Afor%20the%20feasibility%20and%20promises%20of%20GUI%20agents%20that%20navigate%20the%20digital%20world%0Aas%20humans%20do.%0A&entry.1838667208=http%3A//arxiv.org/abs/2410.05243v1&entry.124074799=Read"},
{"title": "When \"A Helpful Assistant\" Is Not Really Helpful: Personas in System\n  Prompts Do Not Improve Performances of Large Language Models", "author": "Mingqian Zheng and Jiaxin Pei and Lajanugen Logeswaran and Moontae Lee and David Jurgens", "abstract": "  Prompting serves as the major way humans interact with Large Language Models\n(LLM). Commercial AI systems commonly define the role of the LLM in system\nprompts. For example, ChatGPT uses \"You are a helpful assistant\" as part of its\ndefault system prompt. Despite current practices of adding personas to system\nprompts, it remains unclear how different personas affect a model's performance\non objective tasks. In this study, we present a systematic evaluation of\npersonas in system prompts. We curate a list of 162 roles covering 6 types of\ninterpersonal relationships and 8 domains of expertise. Through extensive\nanalysis of 4 popular families of LLMs and 2,410 factual questions, we\ndemonstrate that adding personas in system prompts does not improve model\nperformance across a range of questions compared to the control setting where\nno persona is added. Nevertheless, further analysis suggests that the gender,\ntype, and domain of the persona can all influence the resulting prediction\naccuracies. We further experimented with a list of persona search strategies\nand found that, while aggregating results from the best persona for each\nquestion significantly improves prediction accuracy, automatically identifying\nthe best persona is challenging, with predictions often performing no better\nthan random selection. Overall, our findings suggest that while adding a\npersona may lead to performance gains in certain settings, the effect of each\npersona can be largely random. Code and data are available at\nhttps://github.com/Jiaxin-Pei/Prompting-with-Social-Roles.\n", "link": "http://arxiv.org/abs/2311.10054v2", "date": "2024-10-07", "relevancy": 2.1875, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.4419}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.4419}, {"title": "DressCode: Autoregressively Sewing and Generating Garments from Text\n  Guidance", "link": "http://arxiv.org/abs/2401.16465v3", "similarity": 0.4286}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20When%20%22A%20Helpful%20Assistant%22%20Is%20Not%20Really%20Helpful%3A%20Personas%20in%20System%0A%20%20Prompts%20Do%20Not%20Improve%20Performances%20of%20Large%20Language%20Models&body=Title%3A%20When%20%22A%20Helpful%20Assistant%22%20Is%20Not%20Really%20Helpful%3A%20Personas%20in%20System%0A%20%20Prompts%20Do%20Not%20Improve%20Performances%20of%20Large%20Language%20Models%0AAuthor%3A%20Mingqian%20Zheng%20and%20Jiaxin%20Pei%20and%20Lajanugen%20Logeswaran%20and%20Moontae%20Lee%20and%20David%20Jurgens%0AAbstract%3A%20%20%20Prompting%20serves%20as%20the%20major%20way%20humans%20interact%20with%20Large%20Language%20Models%0A%28LLM%29.%20Commercial%20AI%20systems%20commonly%20define%20the%20role%20of%20the%20LLM%20in%20system%0Aprompts.%20For%20example%2C%20ChatGPT%20uses%20%22You%20are%20a%20helpful%20assistant%22%20as%20part%20of%20its%0Adefault%20system%20prompt.%20Despite%20current%20practices%20of%20adding%20personas%20to%20system%0Aprompts%2C%20it%20remains%20unclear%20how%20different%20personas%20affect%20a%20model%27s%20performance%0Aon%20objective%20tasks.%20In%20this%20study%2C%20we%20present%20a%20systematic%20evaluation%20of%0Apersonas%20in%20system%20prompts.%20We%20curate%20a%20list%20of%20162%20roles%20covering%206%20types%20of%0Ainterpersonal%20relationships%20and%208%20domains%20of%20expertise.%20Through%20extensive%0Aanalysis%20of%204%20popular%20families%20of%20LLMs%20and%202%2C410%20factual%20questions%2C%20we%0Ademonstrate%20that%20adding%20personas%20in%20system%20prompts%20does%20not%20improve%20model%0Aperformance%20across%20a%20range%20of%20questions%20compared%20to%20the%20control%20setting%20where%0Ano%20persona%20is%20added.%20Nevertheless%2C%20further%20analysis%20suggests%20that%20the%20gender%2C%0Atype%2C%20and%20domain%20of%20the%20persona%20can%20all%20influence%20the%20resulting%20prediction%0Aaccuracies.%20We%20further%20experimented%20with%20a%20list%20of%20persona%20search%20strategies%0Aand%20found%20that%2C%20while%20aggregating%20results%20from%20the%20best%20persona%20for%20each%0Aquestion%20significantly%20improves%20prediction%20accuracy%2C%20automatically%20identifying%0Athe%20best%20persona%20is%20challenging%2C%20with%20predictions%20often%20performing%20no%20better%0Athan%20random%20selection.%20Overall%2C%20our%20findings%20suggest%20that%20while%20adding%20a%0Apersona%20may%20lead%20to%20performance%20gains%20in%20certain%20settings%2C%20the%20effect%20of%20each%0Apersona%20can%20be%20largely%20random.%20Code%20and%20data%20are%20available%20at%0Ahttps%3A//github.com/Jiaxin-Pei/Prompting-with-Social-Roles.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2311.10054v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DWhen%2520%2522A%2520Helpful%2520Assistant%2522%2520Is%2520Not%2520Really%2520Helpful%253A%2520Personas%2520in%2520System%250A%2520%2520Prompts%2520Do%2520Not%2520Improve%2520Performances%2520of%2520Large%2520Language%2520Models%26entry.906535625%3DMingqian%2520Zheng%2520and%2520Jiaxin%2520Pei%2520and%2520Lajanugen%2520Logeswaran%2520and%2520Moontae%2520Lee%2520and%2520David%2520Jurgens%26entry.1292438233%3D%2520%2520Prompting%2520serves%2520as%2520the%2520major%2520way%2520humans%2520interact%2520with%2520Large%2520Language%2520Models%250A%2528LLM%2529.%2520Commercial%2520AI%2520systems%2520commonly%2520define%2520the%2520role%2520of%2520the%2520LLM%2520in%2520system%250Aprompts.%2520For%2520example%252C%2520ChatGPT%2520uses%2520%2522You%2520are%2520a%2520helpful%2520assistant%2522%2520as%2520part%2520of%2520its%250Adefault%2520system%2520prompt.%2520Despite%2520current%2520practices%2520of%2520adding%2520personas%2520to%2520system%250Aprompts%252C%2520it%2520remains%2520unclear%2520how%2520different%2520personas%2520affect%2520a%2520model%2527s%2520performance%250Aon%2520objective%2520tasks.%2520In%2520this%2520study%252C%2520we%2520present%2520a%2520systematic%2520evaluation%2520of%250Apersonas%2520in%2520system%2520prompts.%2520We%2520curate%2520a%2520list%2520of%2520162%2520roles%2520covering%25206%2520types%2520of%250Ainterpersonal%2520relationships%2520and%25208%2520domains%2520of%2520expertise.%2520Through%2520extensive%250Aanalysis%2520of%25204%2520popular%2520families%2520of%2520LLMs%2520and%25202%252C410%2520factual%2520questions%252C%2520we%250Ademonstrate%2520that%2520adding%2520personas%2520in%2520system%2520prompts%2520does%2520not%2520improve%2520model%250Aperformance%2520across%2520a%2520range%2520of%2520questions%2520compared%2520to%2520the%2520control%2520setting%2520where%250Ano%2520persona%2520is%2520added.%2520Nevertheless%252C%2520further%2520analysis%2520suggests%2520that%2520the%2520gender%252C%250Atype%252C%2520and%2520domain%2520of%2520the%2520persona%2520can%2520all%2520influence%2520the%2520resulting%2520prediction%250Aaccuracies.%2520We%2520further%2520experimented%2520with%2520a%2520list%2520of%2520persona%2520search%2520strategies%250Aand%2520found%2520that%252C%2520while%2520aggregating%2520results%2520from%2520the%2520best%2520persona%2520for%2520each%250Aquestion%2520significantly%2520improves%2520prediction%2520accuracy%252C%2520automatically%2520identifying%250Athe%2520best%2520persona%2520is%2520challenging%252C%2520with%2520predictions%2520often%2520performing%2520no%2520better%250Athan%2520random%2520selection.%2520Overall%252C%2520our%2520findings%2520suggest%2520that%2520while%2520adding%2520a%250Apersona%2520may%2520lead%2520to%2520performance%2520gains%2520in%2520certain%2520settings%252C%2520the%2520effect%2520of%2520each%250Apersona%2520can%2520be%2520largely%2520random.%2520Code%2520and%2520data%2520are%2520available%2520at%250Ahttps%253A//github.com/Jiaxin-Pei/Prompting-with-Social-Roles.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2311.10054v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=When%20%22A%20Helpful%20Assistant%22%20Is%20Not%20Really%20Helpful%3A%20Personas%20in%20System%0A%20%20Prompts%20Do%20Not%20Improve%20Performances%20of%20Large%20Language%20Models&entry.906535625=Mingqian%20Zheng%20and%20Jiaxin%20Pei%20and%20Lajanugen%20Logeswaran%20and%20Moontae%20Lee%20and%20David%20Jurgens&entry.1292438233=%20%20Prompting%20serves%20as%20the%20major%20way%20humans%20interact%20with%20Large%20Language%20Models%0A%28LLM%29.%20Commercial%20AI%20systems%20commonly%20define%20the%20role%20of%20the%20LLM%20in%20system%0Aprompts.%20For%20example%2C%20ChatGPT%20uses%20%22You%20are%20a%20helpful%20assistant%22%20as%20part%20of%20its%0Adefault%20system%20prompt.%20Despite%20current%20practices%20of%20adding%20personas%20to%20system%0Aprompts%2C%20it%20remains%20unclear%20how%20different%20personas%20affect%20a%20model%27s%20performance%0Aon%20objective%20tasks.%20In%20this%20study%2C%20we%20present%20a%20systematic%20evaluation%20of%0Apersonas%20in%20system%20prompts.%20We%20curate%20a%20list%20of%20162%20roles%20covering%206%20types%20of%0Ainterpersonal%20relationships%20and%208%20domains%20of%20expertise.%20Through%20extensive%0Aanalysis%20of%204%20popular%20families%20of%20LLMs%20and%202%2C410%20factual%20questions%2C%20we%0Ademonstrate%20that%20adding%20personas%20in%20system%20prompts%20does%20not%20improve%20model%0Aperformance%20across%20a%20range%20of%20questions%20compared%20to%20the%20control%20setting%20where%0Ano%20persona%20is%20added.%20Nevertheless%2C%20further%20analysis%20suggests%20that%20the%20gender%2C%0Atype%2C%20and%20domain%20of%20the%20persona%20can%20all%20influence%20the%20resulting%20prediction%0Aaccuracies.%20We%20further%20experimented%20with%20a%20list%20of%20persona%20search%20strategies%0Aand%20found%20that%2C%20while%20aggregating%20results%20from%20the%20best%20persona%20for%20each%0Aquestion%20significantly%20improves%20prediction%20accuracy%2C%20automatically%20identifying%0Athe%20best%20persona%20is%20challenging%2C%20with%20predictions%20often%20performing%20no%20better%0Athan%20random%20selection.%20Overall%2C%20our%20findings%20suggest%20that%20while%20adding%20a%0Apersona%20may%20lead%20to%20performance%20gains%20in%20certain%20settings%2C%20the%20effect%20of%20each%0Apersona%20can%20be%20largely%20random.%20Code%20and%20data%20are%20available%20at%0Ahttps%3A//github.com/Jiaxin-Pei/Prompting-with-Social-Roles.%0A&entry.1838667208=http%3A//arxiv.org/abs/2311.10054v2&entry.124074799=Read"},
{"title": "Decoding Intelligence: A Framework for Certifying Knowledge\n  Comprehension in LLMs", "author": "Isha Chaudhary and Vedaant V. Jain and Gagandeep Singh", "abstract": "  Knowledge comprehension capability is an important aspect of human\nintelligence. As Large Language Models (LLMs) are being envisioned as\nsuperhuman agents, it is crucial for them to be proficient at knowledge\ncomprehension. However, existing benchmarking studies do not provide\nconsistent, generalizable, and formal guarantees on the knowledge comprehension\ncapabilities of LLMs. In this work, we propose the first framework to certify\nknowledge comprehension in LLMs with formal probabilistic guarantees. Our\ncertificates are quantitative -- they consist of high-confidence, tight bounds\non the probability that a target LLM gives the correct answer on any knowledge\ncomprehension prompt sampled from a distribution. We design and certify novel\nspecifications that precisely represent distributions of knowledge\ncomprehension prompts leveraging knowledge graphs. We certify SOTA LLMs for\nspecifications over the Wikidata5m knowledge graph. We find that the knowledge\ncomprehension capability improves significantly with scaling the size of the\nmodels.\n", "link": "http://arxiv.org/abs/2402.15929v2", "date": "2024-10-07", "relevancy": 2.1839, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5519}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5519}, {"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.5162}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Decoding%20Intelligence%3A%20A%20Framework%20for%20Certifying%20Knowledge%0A%20%20Comprehension%20in%20LLMs&body=Title%3A%20Decoding%20Intelligence%3A%20A%20Framework%20for%20Certifying%20Knowledge%0A%20%20Comprehension%20in%20LLMs%0AAuthor%3A%20Isha%20Chaudhary%20and%20Vedaant%20V.%20Jain%20and%20Gagandeep%20Singh%0AAbstract%3A%20%20%20Knowledge%20comprehension%20capability%20is%20an%20important%20aspect%20of%20human%0Aintelligence.%20As%20Large%20Language%20Models%20%28LLMs%29%20are%20being%20envisioned%20as%0Asuperhuman%20agents%2C%20it%20is%20crucial%20for%20them%20to%20be%20proficient%20at%20knowledge%0Acomprehension.%20However%2C%20existing%20benchmarking%20studies%20do%20not%20provide%0Aconsistent%2C%20generalizable%2C%20and%20formal%20guarantees%20on%20the%20knowledge%20comprehension%0Acapabilities%20of%20LLMs.%20In%20this%20work%2C%20we%20propose%20the%20first%20framework%20to%20certify%0Aknowledge%20comprehension%20in%20LLMs%20with%20formal%20probabilistic%20guarantees.%20Our%0Acertificates%20are%20quantitative%20--%20they%20consist%20of%20high-confidence%2C%20tight%20bounds%0Aon%20the%20probability%20that%20a%20target%20LLM%20gives%20the%20correct%20answer%20on%20any%20knowledge%0Acomprehension%20prompt%20sampled%20from%20a%20distribution.%20We%20design%20and%20certify%20novel%0Aspecifications%20that%20precisely%20represent%20distributions%20of%20knowledge%0Acomprehension%20prompts%20leveraging%20knowledge%20graphs.%20We%20certify%20SOTA%20LLMs%20for%0Aspecifications%20over%20the%20Wikidata5m%20knowledge%20graph.%20We%20find%20that%20the%20knowledge%0Acomprehension%20capability%20improves%20significantly%20with%20scaling%20the%20size%20of%20the%0Amodels.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2402.15929v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DDecoding%2520Intelligence%253A%2520A%2520Framework%2520for%2520Certifying%2520Knowledge%250A%2520%2520Comprehension%2520in%2520LLMs%26entry.906535625%3DIsha%2520Chaudhary%2520and%2520Vedaant%2520V.%2520Jain%2520and%2520Gagandeep%2520Singh%26entry.1292438233%3D%2520%2520Knowledge%2520comprehension%2520capability%2520is%2520an%2520important%2520aspect%2520of%2520human%250Aintelligence.%2520As%2520Large%2520Language%2520Models%2520%2528LLMs%2529%2520are%2520being%2520envisioned%2520as%250Asuperhuman%2520agents%252C%2520it%2520is%2520crucial%2520for%2520them%2520to%2520be%2520proficient%2520at%2520knowledge%250Acomprehension.%2520However%252C%2520existing%2520benchmarking%2520studies%2520do%2520not%2520provide%250Aconsistent%252C%2520generalizable%252C%2520and%2520formal%2520guarantees%2520on%2520the%2520knowledge%2520comprehension%250Acapabilities%2520of%2520LLMs.%2520In%2520this%2520work%252C%2520we%2520propose%2520the%2520first%2520framework%2520to%2520certify%250Aknowledge%2520comprehension%2520in%2520LLMs%2520with%2520formal%2520probabilistic%2520guarantees.%2520Our%250Acertificates%2520are%2520quantitative%2520--%2520they%2520consist%2520of%2520high-confidence%252C%2520tight%2520bounds%250Aon%2520the%2520probability%2520that%2520a%2520target%2520LLM%2520gives%2520the%2520correct%2520answer%2520on%2520any%2520knowledge%250Acomprehension%2520prompt%2520sampled%2520from%2520a%2520distribution.%2520We%2520design%2520and%2520certify%2520novel%250Aspecifications%2520that%2520precisely%2520represent%2520distributions%2520of%2520knowledge%250Acomprehension%2520prompts%2520leveraging%2520knowledge%2520graphs.%2520We%2520certify%2520SOTA%2520LLMs%2520for%250Aspecifications%2520over%2520the%2520Wikidata5m%2520knowledge%2520graph.%2520We%2520find%2520that%2520the%2520knowledge%250Acomprehension%2520capability%2520improves%2520significantly%2520with%2520scaling%2520the%2520size%2520of%2520the%250Amodels.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2402.15929v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Decoding%20Intelligence%3A%20A%20Framework%20for%20Certifying%20Knowledge%0A%20%20Comprehension%20in%20LLMs&entry.906535625=Isha%20Chaudhary%20and%20Vedaant%20V.%20Jain%20and%20Gagandeep%20Singh&entry.1292438233=%20%20Knowledge%20comprehension%20capability%20is%20an%20important%20aspect%20of%20human%0Aintelligence.%20As%20Large%20Language%20Models%20%28LLMs%29%20are%20being%20envisioned%20as%0Asuperhuman%20agents%2C%20it%20is%20crucial%20for%20them%20to%20be%20proficient%20at%20knowledge%0Acomprehension.%20However%2C%20existing%20benchmarking%20studies%20do%20not%20provide%0Aconsistent%2C%20generalizable%2C%20and%20formal%20guarantees%20on%20the%20knowledge%20comprehension%0Acapabilities%20of%20LLMs.%20In%20this%20work%2C%20we%20propose%20the%20first%20framework%20to%20certify%0Aknowledge%20comprehension%20in%20LLMs%20with%20formal%20probabilistic%20guarantees.%20Our%0Acertificates%20are%20quantitative%20--%20they%20consist%20of%20high-confidence%2C%20tight%20bounds%0Aon%20the%20probability%20that%20a%20target%20LLM%20gives%20the%20correct%20answer%20on%20any%20knowledge%0Acomprehension%20prompt%20sampled%20from%20a%20distribution.%20We%20design%20and%20certify%20novel%0Aspecifications%20that%20precisely%20represent%20distributions%20of%20knowledge%0Acomprehension%20prompts%20leveraging%20knowledge%20graphs.%20We%20certify%20SOTA%20LLMs%20for%0Aspecifications%20over%20the%20Wikidata5m%20knowledge%20graph.%20We%20find%20that%20the%20knowledge%0Acomprehension%20capability%20improves%20significantly%20with%20scaling%20the%20size%20of%20the%0Amodels.%0A&entry.1838667208=http%3A//arxiv.org/abs/2402.15929v2&entry.124074799=Read"},
{"title": "GMAI-MMBench: A Comprehensive Multimodal Evaluation Benchmark Towards\n  General Medical AI", "author": "Pengcheng Chen and Jin Ye and Guoan Wang and Yanjun Li and Zhongying Deng and Wei Li and Tianbin Li and Haodong Duan and Ziyan Huang and Yanzhou Su and Benyou Wang and Shaoting Zhang and Bin Fu and Jianfei Cai and Bohan Zhuang and Eric J Seibel and Junjun He and Yu Qiao", "abstract": "  Large Vision-Language Models (LVLMs) are capable of handling diverse data\ntypes such as imaging, text, and physiological signals, and can be applied in\nvarious fields. In the medical field, LVLMs have a high potential to offer\nsubstantial assistance for diagnosis and treatment. Before that, it is crucial\nto develop benchmarks to evaluate LVLMs' effectiveness in various medical\napplications. Current benchmarks are often built upon specific academic\nliterature, mainly focusing on a single domain, and lacking varying perceptual\ngranularities. Thus, they face specific challenges, including limited clinical\nrelevance, incomplete evaluations, and insufficient guidance for interactive\nLVLMs. To address these limitations, we developed the GMAI-MMBench, the most\ncomprehensive general medical AI benchmark with well-categorized data structure\nand multi-perceptual granularity to date. It is constructed from 284 datasets\nacross 38 medical image modalities, 18 clinical-related tasks, 18 departments,\nand 4 perceptual granularities in a Visual Question Answering (VQA) format.\nAdditionally, we implemented a lexical tree structure that allows users to\ncustomize evaluation tasks, accommodating various assessment needs and\nsubstantially supporting medical AI research and applications. We evaluated 50\nLVLMs, and the results show that even the advanced GPT-4o only achieves an\naccuracy of 53.96%, indicating significant room for improvement. Moreover, we\nidentified five key insufficiencies in current cutting-edge LVLMs that need to\nbe addressed to advance the development of better medical applications. We\nbelieve that GMAI-MMBench will stimulate the community to build the next\ngeneration of LVLMs toward GMAI.\n", "link": "http://arxiv.org/abs/2408.03361v6", "date": "2024-10-07", "relevancy": 2.1746, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5456}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5456}, {"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.5341}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20GMAI-MMBench%3A%20A%20Comprehensive%20Multimodal%20Evaluation%20Benchmark%20Towards%0A%20%20General%20Medical%20AI&body=Title%3A%20GMAI-MMBench%3A%20A%20Comprehensive%20Multimodal%20Evaluation%20Benchmark%20Towards%0A%20%20General%20Medical%20AI%0AAuthor%3A%20Pengcheng%20Chen%20and%20Jin%20Ye%20and%20Guoan%20Wang%20and%20Yanjun%20Li%20and%20Zhongying%20Deng%20and%20Wei%20Li%20and%20Tianbin%20Li%20and%20Haodong%20Duan%20and%20Ziyan%20Huang%20and%20Yanzhou%20Su%20and%20Benyou%20Wang%20and%20Shaoting%20Zhang%20and%20Bin%20Fu%20and%20Jianfei%20Cai%20and%20Bohan%20Zhuang%20and%20Eric%20J%20Seibel%20and%20Junjun%20He%20and%20Yu%20Qiao%0AAbstract%3A%20%20%20Large%20Vision-Language%20Models%20%28LVLMs%29%20are%20capable%20of%20handling%20diverse%20data%0Atypes%20such%20as%20imaging%2C%20text%2C%20and%20physiological%20signals%2C%20and%20can%20be%20applied%20in%0Avarious%20fields.%20In%20the%20medical%20field%2C%20LVLMs%20have%20a%20high%20potential%20to%20offer%0Asubstantial%20assistance%20for%20diagnosis%20and%20treatment.%20Before%20that%2C%20it%20is%20crucial%0Ato%20develop%20benchmarks%20to%20evaluate%20LVLMs%27%20effectiveness%20in%20various%20medical%0Aapplications.%20Current%20benchmarks%20are%20often%20built%20upon%20specific%20academic%0Aliterature%2C%20mainly%20focusing%20on%20a%20single%20domain%2C%20and%20lacking%20varying%20perceptual%0Agranularities.%20Thus%2C%20they%20face%20specific%20challenges%2C%20including%20limited%20clinical%0Arelevance%2C%20incomplete%20evaluations%2C%20and%20insufficient%20guidance%20for%20interactive%0ALVLMs.%20To%20address%20these%20limitations%2C%20we%20developed%20the%20GMAI-MMBench%2C%20the%20most%0Acomprehensive%20general%20medical%20AI%20benchmark%20with%20well-categorized%20data%20structure%0Aand%20multi-perceptual%20granularity%20to%20date.%20It%20is%20constructed%20from%20284%20datasets%0Aacross%2038%20medical%20image%20modalities%2C%2018%20clinical-related%20tasks%2C%2018%20departments%2C%0Aand%204%20perceptual%20granularities%20in%20a%20Visual%20Question%20Answering%20%28VQA%29%20format.%0AAdditionally%2C%20we%20implemented%20a%20lexical%20tree%20structure%20that%20allows%20users%20to%0Acustomize%20evaluation%20tasks%2C%20accommodating%20various%20assessment%20needs%20and%0Asubstantially%20supporting%20medical%20AI%20research%20and%20applications.%20We%20evaluated%2050%0ALVLMs%2C%20and%20the%20results%20show%20that%20even%20the%20advanced%20GPT-4o%20only%20achieves%20an%0Aaccuracy%20of%2053.96%25%2C%20indicating%20significant%20room%20for%20improvement.%20Moreover%2C%20we%0Aidentified%20five%20key%20insufficiencies%20in%20current%20cutting-edge%20LVLMs%20that%20need%20to%0Abe%20addressed%20to%20advance%20the%20development%20of%20better%20medical%20applications.%20We%0Abelieve%20that%20GMAI-MMBench%20will%20stimulate%20the%20community%20to%20build%20the%20next%0Ageneration%20of%20LVLMs%20toward%20GMAI.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2408.03361v6%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DGMAI-MMBench%253A%2520A%2520Comprehensive%2520Multimodal%2520Evaluation%2520Benchmark%2520Towards%250A%2520%2520General%2520Medical%2520AI%26entry.906535625%3DPengcheng%2520Chen%2520and%2520Jin%2520Ye%2520and%2520Guoan%2520Wang%2520and%2520Yanjun%2520Li%2520and%2520Zhongying%2520Deng%2520and%2520Wei%2520Li%2520and%2520Tianbin%2520Li%2520and%2520Haodong%2520Duan%2520and%2520Ziyan%2520Huang%2520and%2520Yanzhou%2520Su%2520and%2520Benyou%2520Wang%2520and%2520Shaoting%2520Zhang%2520and%2520Bin%2520Fu%2520and%2520Jianfei%2520Cai%2520and%2520Bohan%2520Zhuang%2520and%2520Eric%2520J%2520Seibel%2520and%2520Junjun%2520He%2520and%2520Yu%2520Qiao%26entry.1292438233%3D%2520%2520Large%2520Vision-Language%2520Models%2520%2528LVLMs%2529%2520are%2520capable%2520of%2520handling%2520diverse%2520data%250Atypes%2520such%2520as%2520imaging%252C%2520text%252C%2520and%2520physiological%2520signals%252C%2520and%2520can%2520be%2520applied%2520in%250Avarious%2520fields.%2520In%2520the%2520medical%2520field%252C%2520LVLMs%2520have%2520a%2520high%2520potential%2520to%2520offer%250Asubstantial%2520assistance%2520for%2520diagnosis%2520and%2520treatment.%2520Before%2520that%252C%2520it%2520is%2520crucial%250Ato%2520develop%2520benchmarks%2520to%2520evaluate%2520LVLMs%2527%2520effectiveness%2520in%2520various%2520medical%250Aapplications.%2520Current%2520benchmarks%2520are%2520often%2520built%2520upon%2520specific%2520academic%250Aliterature%252C%2520mainly%2520focusing%2520on%2520a%2520single%2520domain%252C%2520and%2520lacking%2520varying%2520perceptual%250Agranularities.%2520Thus%252C%2520they%2520face%2520specific%2520challenges%252C%2520including%2520limited%2520clinical%250Arelevance%252C%2520incomplete%2520evaluations%252C%2520and%2520insufficient%2520guidance%2520for%2520interactive%250ALVLMs.%2520To%2520address%2520these%2520limitations%252C%2520we%2520developed%2520the%2520GMAI-MMBench%252C%2520the%2520most%250Acomprehensive%2520general%2520medical%2520AI%2520benchmark%2520with%2520well-categorized%2520data%2520structure%250Aand%2520multi-perceptual%2520granularity%2520to%2520date.%2520It%2520is%2520constructed%2520from%2520284%2520datasets%250Aacross%252038%2520medical%2520image%2520modalities%252C%252018%2520clinical-related%2520tasks%252C%252018%2520departments%252C%250Aand%25204%2520perceptual%2520granularities%2520in%2520a%2520Visual%2520Question%2520Answering%2520%2528VQA%2529%2520format.%250AAdditionally%252C%2520we%2520implemented%2520a%2520lexical%2520tree%2520structure%2520that%2520allows%2520users%2520to%250Acustomize%2520evaluation%2520tasks%252C%2520accommodating%2520various%2520assessment%2520needs%2520and%250Asubstantially%2520supporting%2520medical%2520AI%2520research%2520and%2520applications.%2520We%2520evaluated%252050%250ALVLMs%252C%2520and%2520the%2520results%2520show%2520that%2520even%2520the%2520advanced%2520GPT-4o%2520only%2520achieves%2520an%250Aaccuracy%2520of%252053.96%2525%252C%2520indicating%2520significant%2520room%2520for%2520improvement.%2520Moreover%252C%2520we%250Aidentified%2520five%2520key%2520insufficiencies%2520in%2520current%2520cutting-edge%2520LVLMs%2520that%2520need%2520to%250Abe%2520addressed%2520to%2520advance%2520the%2520development%2520of%2520better%2520medical%2520applications.%2520We%250Abelieve%2520that%2520GMAI-MMBench%2520will%2520stimulate%2520the%2520community%2520to%2520build%2520the%2520next%250Ageneration%2520of%2520LVLMs%2520toward%2520GMAI.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2408.03361v6%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=GMAI-MMBench%3A%20A%20Comprehensive%20Multimodal%20Evaluation%20Benchmark%20Towards%0A%20%20General%20Medical%20AI&entry.906535625=Pengcheng%20Chen%20and%20Jin%20Ye%20and%20Guoan%20Wang%20and%20Yanjun%20Li%20and%20Zhongying%20Deng%20and%20Wei%20Li%20and%20Tianbin%20Li%20and%20Haodong%20Duan%20and%20Ziyan%20Huang%20and%20Yanzhou%20Su%20and%20Benyou%20Wang%20and%20Shaoting%20Zhang%20and%20Bin%20Fu%20and%20Jianfei%20Cai%20and%20Bohan%20Zhuang%20and%20Eric%20J%20Seibel%20and%20Junjun%20He%20and%20Yu%20Qiao&entry.1292438233=%20%20Large%20Vision-Language%20Models%20%28LVLMs%29%20are%20capable%20of%20handling%20diverse%20data%0Atypes%20such%20as%20imaging%2C%20text%2C%20and%20physiological%20signals%2C%20and%20can%20be%20applied%20in%0Avarious%20fields.%20In%20the%20medical%20field%2C%20LVLMs%20have%20a%20high%20potential%20to%20offer%0Asubstantial%20assistance%20for%20diagnosis%20and%20treatment.%20Before%20that%2C%20it%20is%20crucial%0Ato%20develop%20benchmarks%20to%20evaluate%20LVLMs%27%20effectiveness%20in%20various%20medical%0Aapplications.%20Current%20benchmarks%20are%20often%20built%20upon%20specific%20academic%0Aliterature%2C%20mainly%20focusing%20on%20a%20single%20domain%2C%20and%20lacking%20varying%20perceptual%0Agranularities.%20Thus%2C%20they%20face%20specific%20challenges%2C%20including%20limited%20clinical%0Arelevance%2C%20incomplete%20evaluations%2C%20and%20insufficient%20guidance%20for%20interactive%0ALVLMs.%20To%20address%20these%20limitations%2C%20we%20developed%20the%20GMAI-MMBench%2C%20the%20most%0Acomprehensive%20general%20medical%20AI%20benchmark%20with%20well-categorized%20data%20structure%0Aand%20multi-perceptual%20granularity%20to%20date.%20It%20is%20constructed%20from%20284%20datasets%0Aacross%2038%20medical%20image%20modalities%2C%2018%20clinical-related%20tasks%2C%2018%20departments%2C%0Aand%204%20perceptual%20granularities%20in%20a%20Visual%20Question%20Answering%20%28VQA%29%20format.%0AAdditionally%2C%20we%20implemented%20a%20lexical%20tree%20structure%20that%20allows%20users%20to%0Acustomize%20evaluation%20tasks%2C%20accommodating%20various%20assessment%20needs%20and%0Asubstantially%20supporting%20medical%20AI%20research%20and%20applications.%20We%20evaluated%2050%0ALVLMs%2C%20and%20the%20results%20show%20that%20even%20the%20advanced%20GPT-4o%20only%20achieves%20an%0Aaccuracy%20of%2053.96%25%2C%20indicating%20significant%20room%20for%20improvement.%20Moreover%2C%20we%0Aidentified%20five%20key%20insufficiencies%20in%20current%20cutting-edge%20LVLMs%20that%20need%20to%0Abe%20addressed%20to%20advance%20the%20development%20of%20better%20medical%20applications.%20We%0Abelieve%20that%20GMAI-MMBench%20will%20stimulate%20the%20community%20to%20build%20the%20next%0Ageneration%20of%20LVLMs%20toward%20GMAI.%0A&entry.1838667208=http%3A//arxiv.org/abs/2408.03361v6&entry.124074799=Read"},
{"title": "Reinforcement Learning Control for Autonomous Hydraulic Material\n  Handling Machines with Underactuated Tools", "author": "Filippo A. Spinelli and Pascal Egli and Julian Nubert and Fang Nan and Thilo Bleumer and Patrick Goegler and Stephan Brockes and Ferdinand Hofmann and Marco Hutter", "abstract": "  The precise and safe control of heavy material handling machines presents\nnumerous challenges due to the hard-to-model hydraulically actuated joints and\nthe need for collision-free trajectory planning with a free-swinging\nend-effector tool. In this work, we propose an RL-based controller that\ncommands the cabin joint and the arm simultaneously. It is trained in a\nsimulation combining data-driven modeling techniques with first-principles\nmodeling. On the one hand, we employ a neural network model to capture the\nhighly nonlinear dynamics of the upper carriage turn hydraulic motor,\nincorporating explicit pressure prediction to handle delays better. On the\nother hand, we model the arm as velocity-controllable and the free-swinging\nend-effector tool as a damped pendulum using first principles. This combined\nmodel enhances our simulation environment, enabling the training of RL\ncontrollers that can be directly transferred to the real machine. Designed to\nreach steady-state Cartesian targets, the RL controller learns to leverage the\nhydraulic dynamics to improve accuracy, maintain high speeds, and minimize\nend-effector tool oscillations. Our controller, tested on a mid-size prototype\nmaterial handler, is more accurate than an inexperienced operator and causes\nfewer tool oscillations. It demonstrates competitive performance even compared\nto an experienced professional driver.\n", "link": "http://arxiv.org/abs/2410.05093v1", "date": "2024-10-07", "relevancy": 2.1703, "topK": [{"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.56}, {"title": "Safe and efficient training of a control agent", "link": "https://patents.google.com/patent/US11709462B2/en", "similarity": 0.5586}, {"title": "Egocentric Computer Vision for Hands-Free Robotic Wheelchair Navigation", "link": "https://link.springer.com/article/10.1007/s10846-023-01807-4", "similarity": 0.5195}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Reinforcement%20Learning%20Control%20for%20Autonomous%20Hydraulic%20Material%0A%20%20Handling%20Machines%20with%20Underactuated%20Tools&body=Title%3A%20Reinforcement%20Learning%20Control%20for%20Autonomous%20Hydraulic%20Material%0A%20%20Handling%20Machines%20with%20Underactuated%20Tools%0AAuthor%3A%20Filippo%20A.%20Spinelli%20and%20Pascal%20Egli%20and%20Julian%20Nubert%20and%20Fang%20Nan%20and%20Thilo%20Bleumer%20and%20Patrick%20Goegler%20and%20Stephan%20Brockes%20and%20Ferdinand%20Hofmann%20and%20Marco%20Hutter%0AAbstract%3A%20%20%20The%20precise%20and%20safe%20control%20of%20heavy%20material%20handling%20machines%20presents%0Anumerous%20challenges%20due%20to%20the%20hard-to-model%20hydraulically%20actuated%20joints%20and%0Athe%20need%20for%20collision-free%20trajectory%20planning%20with%20a%20free-swinging%0Aend-effector%20tool.%20In%20this%20work%2C%20we%20propose%20an%20RL-based%20controller%20that%0Acommands%20the%20cabin%20joint%20and%20the%20arm%20simultaneously.%20It%20is%20trained%20in%20a%0Asimulation%20combining%20data-driven%20modeling%20techniques%20with%20first-principles%0Amodeling.%20On%20the%20one%20hand%2C%20we%20employ%20a%20neural%20network%20model%20to%20capture%20the%0Ahighly%20nonlinear%20dynamics%20of%20the%20upper%20carriage%20turn%20hydraulic%20motor%2C%0Aincorporating%20explicit%20pressure%20prediction%20to%20handle%20delays%20better.%20On%20the%0Aother%20hand%2C%20we%20model%20the%20arm%20as%20velocity-controllable%20and%20the%20free-swinging%0Aend-effector%20tool%20as%20a%20damped%20pendulum%20using%20first%20principles.%20This%20combined%0Amodel%20enhances%20our%20simulation%20environment%2C%20enabling%20the%20training%20of%20RL%0Acontrollers%20that%20can%20be%20directly%20transferred%20to%20the%20real%20machine.%20Designed%20to%0Areach%20steady-state%20Cartesian%20targets%2C%20the%20RL%20controller%20learns%20to%20leverage%20the%0Ahydraulic%20dynamics%20to%20improve%20accuracy%2C%20maintain%20high%20speeds%2C%20and%20minimize%0Aend-effector%20tool%20oscillations.%20Our%20controller%2C%20tested%20on%20a%20mid-size%20prototype%0Amaterial%20handler%2C%20is%20more%20accurate%20than%20an%20inexperienced%20operator%20and%20causes%0Afewer%20tool%20oscillations.%20It%20demonstrates%20competitive%20performance%20even%20compared%0Ato%20an%20experienced%20professional%20driver.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2410.05093v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DReinforcement%2520Learning%2520Control%2520for%2520Autonomous%2520Hydraulic%2520Material%250A%2520%2520Handling%2520Machines%2520with%2520Underactuated%2520Tools%26entry.906535625%3DFilippo%2520A.%2520Spinelli%2520and%2520Pascal%2520Egli%2520and%2520Julian%2520Nubert%2520and%2520Fang%2520Nan%2520and%2520Thilo%2520Bleumer%2520and%2520Patrick%2520Goegler%2520and%2520Stephan%2520Brockes%2520and%2520Ferdinand%2520Hofmann%2520and%2520Marco%2520Hutter%26entry.1292438233%3D%2520%2520The%2520precise%2520and%2520safe%2520control%2520of%2520heavy%2520material%2520handling%2520machines%2520presents%250Anumerous%2520challenges%2520due%2520to%2520the%2520hard-to-model%2520hydraulically%2520actuated%2520joints%2520and%250Athe%2520need%2520for%2520collision-free%2520trajectory%2520planning%2520with%2520a%2520free-swinging%250Aend-effector%2520tool.%2520In%2520this%2520work%252C%2520we%2520propose%2520an%2520RL-based%2520controller%2520that%250Acommands%2520the%2520cabin%2520joint%2520and%2520the%2520arm%2520simultaneously.%2520It%2520is%2520trained%2520in%2520a%250Asimulation%2520combining%2520data-driven%2520modeling%2520techniques%2520with%2520first-principles%250Amodeling.%2520On%2520the%2520one%2520hand%252C%2520we%2520employ%2520a%2520neural%2520network%2520model%2520to%2520capture%2520the%250Ahighly%2520nonlinear%2520dynamics%2520of%2520the%2520upper%2520carriage%2520turn%2520hydraulic%2520motor%252C%250Aincorporating%2520explicit%2520pressure%2520prediction%2520to%2520handle%2520delays%2520better.%2520On%2520the%250Aother%2520hand%252C%2520we%2520model%2520the%2520arm%2520as%2520velocity-controllable%2520and%2520the%2520free-swinging%250Aend-effector%2520tool%2520as%2520a%2520damped%2520pendulum%2520using%2520first%2520principles.%2520This%2520combined%250Amodel%2520enhances%2520our%2520simulation%2520environment%252C%2520enabling%2520the%2520training%2520of%2520RL%250Acontrollers%2520that%2520can%2520be%2520directly%2520transferred%2520to%2520the%2520real%2520machine.%2520Designed%2520to%250Areach%2520steady-state%2520Cartesian%2520targets%252C%2520the%2520RL%2520controller%2520learns%2520to%2520leverage%2520the%250Ahydraulic%2520dynamics%2520to%2520improve%2520accuracy%252C%2520maintain%2520high%2520speeds%252C%2520and%2520minimize%250Aend-effector%2520tool%2520oscillations.%2520Our%2520controller%252C%2520tested%2520on%2520a%2520mid-size%2520prototype%250Amaterial%2520handler%252C%2520is%2520more%2520accurate%2520than%2520an%2520inexperienced%2520operator%2520and%2520causes%250Afewer%2520tool%2520oscillations.%2520It%2520demonstrates%2520competitive%2520performance%2520even%2520compared%250Ato%2520an%2520experienced%2520professional%2520driver.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2410.05093v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Reinforcement%20Learning%20Control%20for%20Autonomous%20Hydraulic%20Material%0A%20%20Handling%20Machines%20with%20Underactuated%20Tools&entry.906535625=Filippo%20A.%20Spinelli%20and%20Pascal%20Egli%20and%20Julian%20Nubert%20and%20Fang%20Nan%20and%20Thilo%20Bleumer%20and%20Patrick%20Goegler%20and%20Stephan%20Brockes%20and%20Ferdinand%20Hofmann%20and%20Marco%20Hutter&entry.1292438233=%20%20The%20precise%20and%20safe%20control%20of%20heavy%20material%20handling%20machines%20presents%0Anumerous%20challenges%20due%20to%20the%20hard-to-model%20hydraulically%20actuated%20joints%20and%0Athe%20need%20for%20collision-free%20trajectory%20planning%20with%20a%20free-swinging%0Aend-effector%20tool.%20In%20this%20work%2C%20we%20propose%20an%20RL-based%20controller%20that%0Acommands%20the%20cabin%20joint%20and%20the%20arm%20simultaneously.%20It%20is%20trained%20in%20a%0Asimulation%20combining%20data-driven%20modeling%20techniques%20with%20first-principles%0Amodeling.%20On%20the%20one%20hand%2C%20we%20employ%20a%20neural%20network%20model%20to%20capture%20the%0Ahighly%20nonlinear%20dynamics%20of%20the%20upper%20carriage%20turn%20hydraulic%20motor%2C%0Aincorporating%20explicit%20pressure%20prediction%20to%20handle%20delays%20better.%20On%20the%0Aother%20hand%2C%20we%20model%20the%20arm%20as%20velocity-controllable%20and%20the%20free-swinging%0Aend-effector%20tool%20as%20a%20damped%20pendulum%20using%20first%20principles.%20This%20combined%0Amodel%20enhances%20our%20simulation%20environment%2C%20enabling%20the%20training%20of%20RL%0Acontrollers%20that%20can%20be%20directly%20transferred%20to%20the%20real%20machine.%20Designed%20to%0Areach%20steady-state%20Cartesian%20targets%2C%20the%20RL%20controller%20learns%20to%20leverage%20the%0Ahydraulic%20dynamics%20to%20improve%20accuracy%2C%20maintain%20high%20speeds%2C%20and%20minimize%0Aend-effector%20tool%20oscillations.%20Our%20controller%2C%20tested%20on%20a%20mid-size%20prototype%0Amaterial%20handler%2C%20is%20more%20accurate%20than%20an%20inexperienced%20operator%20and%20causes%0Afewer%20tool%20oscillations.%20It%20demonstrates%20competitive%20performance%20even%20compared%0Ato%20an%20experienced%20professional%20driver.%0A&entry.1838667208=http%3A//arxiv.org/abs/2410.05093v1&entry.124074799=Read"},
{"title": "Improved detection of discarded fish species through BoxAL active\n  learning", "author": "Maria Sokolova and Pieter M. Blok and Angelo Mencarelli and Arjan Vroegop and Aloysius van Helmond and Gert Kootstra", "abstract": "  In recent years, powerful data-driven deep-learning techniques have been\ndeveloped and applied for automated catch registration. However, these methods\nare dependent on the labelled data, which is time-consuming, labour-intensive,\nexpensive to collect and need expert knowledge. In this study, we present an\nactive learning technique, named BoxAL, which includes estimation of epistemic\ncertainty of the Faster R-CNN object-detection model. The method allows\nselecting the most uncertain training images from an unlabeled pool, which are\nthen used to train the object-detection model. To evaluate the method, we used\nan open-source image dataset obtained with a dedicated image-acquisition system\ndeveloped for commercial trawlers targeting demersal species. We demonstrated,\nthat our approach allows reaching the same object-detection performance as with\nthe random sampling using 400 fewer labelled images. Besides, mean AP score was\nsignificantly higher at the last training iteration with 1100 training images,\nspecifically, 39.0&plusmn;1.6 and 34.8&plusmn;1.8 for certainty-based sampling\nand random sampling, respectively. Additionally, we showed that epistemic\ncertainty is a suitable method to sample images that the current iteration of\nthe model cannot deal with yet. Our study additionally showed that the sampled\nnew data is more valuable for training than the remaining unlabeled data. Our\nsoftware is available on https://github.com/pieterblok/boxal.\n", "link": "http://arxiv.org/abs/2410.04880v1", "date": "2024-10-07", "relevancy": 2.1703, "topK": [{"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.5563}, {"title": "Breadcrumbs: Adversarial class-balanced sampling for long-tailed recognition", "link": "https://link.springer.com/chapter/10.1007/978-3-031-20053-3_37", "similarity": 0.5428}, {"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.5368}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Improved%20detection%20of%20discarded%20fish%20species%20through%20BoxAL%20active%0A%20%20learning&body=Title%3A%20Improved%20detection%20of%20discarded%20fish%20species%20through%20BoxAL%20active%0A%20%20learning%0AAuthor%3A%20Maria%20Sokolova%20and%20Pieter%20M.%20Blok%20and%20Angelo%20Mencarelli%20and%20Arjan%20Vroegop%20and%20Aloysius%20van%20Helmond%20and%20Gert%20Kootstra%0AAbstract%3A%20%20%20In%20recent%20years%2C%20powerful%20data-driven%20deep-learning%20techniques%20have%20been%0Adeveloped%20and%20applied%20for%20automated%20catch%20registration.%20However%2C%20these%20methods%0Aare%20dependent%20on%20the%20labelled%20data%2C%20which%20is%20time-consuming%2C%20labour-intensive%2C%0Aexpensive%20to%20collect%20and%20need%20expert%20knowledge.%20In%20this%20study%2C%20we%20present%20an%0Aactive%20learning%20technique%2C%20named%20BoxAL%2C%20which%20includes%20estimation%20of%20epistemic%0Acertainty%20of%20the%20Faster%20R-CNN%20object-detection%20model.%20The%20method%20allows%0Aselecting%20the%20most%20uncertain%20training%20images%20from%20an%20unlabeled%20pool%2C%20which%20are%0Athen%20used%20to%20train%20the%20object-detection%20model.%20To%20evaluate%20the%20method%2C%20we%20used%0Aan%20open-source%20image%20dataset%20obtained%20with%20a%20dedicated%20image-acquisition%20system%0Adeveloped%20for%20commercial%20trawlers%20targeting%20demersal%20species.%20We%20demonstrated%2C%0Athat%20our%20approach%20allows%20reaching%20the%20same%20object-detection%20performance%20as%20with%0Athe%20random%20sampling%20using%20400%20fewer%20labelled%20images.%20Besides%2C%20mean%20AP%20score%20was%0Asignificantly%20higher%20at%20the%20last%20training%20iteration%20with%201100%20training%20images%2C%0Aspecifically%2C%2039.0%26plusmn%3B1.6%20and%2034.8%26plusmn%3B1.8%20for%20certainty-based%20sampling%0Aand%20random%20sampling%2C%20respectively.%20Additionally%2C%20we%20showed%20that%20epistemic%0Acertainty%20is%20a%20suitable%20method%20to%20sample%20images%20that%20the%20current%20iteration%20of%0Athe%20model%20cannot%20deal%20with%20yet.%20Our%20study%20additionally%20showed%20that%20the%20sampled%0Anew%20data%20is%20more%20valuable%20for%20training%20than%20the%20remaining%20unlabeled%20data.%20Our%0Asoftware%20is%20available%20on%20https%3A//github.com/pieterblok/boxal.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2410.04880v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DImproved%2520detection%2520of%2520discarded%2520fish%2520species%2520through%2520BoxAL%2520active%250A%2520%2520learning%26entry.906535625%3DMaria%2520Sokolova%2520and%2520Pieter%2520M.%2520Blok%2520and%2520Angelo%2520Mencarelli%2520and%2520Arjan%2520Vroegop%2520and%2520Aloysius%2520van%2520Helmond%2520and%2520Gert%2520Kootstra%26entry.1292438233%3D%2520%2520In%2520recent%2520years%252C%2520powerful%2520data-driven%2520deep-learning%2520techniques%2520have%2520been%250Adeveloped%2520and%2520applied%2520for%2520automated%2520catch%2520registration.%2520However%252C%2520these%2520methods%250Aare%2520dependent%2520on%2520the%2520labelled%2520data%252C%2520which%2520is%2520time-consuming%252C%2520labour-intensive%252C%250Aexpensive%2520to%2520collect%2520and%2520need%2520expert%2520knowledge.%2520In%2520this%2520study%252C%2520we%2520present%2520an%250Aactive%2520learning%2520technique%252C%2520named%2520BoxAL%252C%2520which%2520includes%2520estimation%2520of%2520epistemic%250Acertainty%2520of%2520the%2520Faster%2520R-CNN%2520object-detection%2520model.%2520The%2520method%2520allows%250Aselecting%2520the%2520most%2520uncertain%2520training%2520images%2520from%2520an%2520unlabeled%2520pool%252C%2520which%2520are%250Athen%2520used%2520to%2520train%2520the%2520object-detection%2520model.%2520To%2520evaluate%2520the%2520method%252C%2520we%2520used%250Aan%2520open-source%2520image%2520dataset%2520obtained%2520with%2520a%2520dedicated%2520image-acquisition%2520system%250Adeveloped%2520for%2520commercial%2520trawlers%2520targeting%2520demersal%2520species.%2520We%2520demonstrated%252C%250Athat%2520our%2520approach%2520allows%2520reaching%2520the%2520same%2520object-detection%2520performance%2520as%2520with%250Athe%2520random%2520sampling%2520using%2520400%2520fewer%2520labelled%2520images.%2520Besides%252C%2520mean%2520AP%2520score%2520was%250Asignificantly%2520higher%2520at%2520the%2520last%2520training%2520iteration%2520with%25201100%2520training%2520images%252C%250Aspecifically%252C%252039.0%2526plusmn%253B1.6%2520and%252034.8%2526plusmn%253B1.8%2520for%2520certainty-based%2520sampling%250Aand%2520random%2520sampling%252C%2520respectively.%2520Additionally%252C%2520we%2520showed%2520that%2520epistemic%250Acertainty%2520is%2520a%2520suitable%2520method%2520to%2520sample%2520images%2520that%2520the%2520current%2520iteration%2520of%250Athe%2520model%2520cannot%2520deal%2520with%2520yet.%2520Our%2520study%2520additionally%2520showed%2520that%2520the%2520sampled%250Anew%2520data%2520is%2520more%2520valuable%2520for%2520training%2520than%2520the%2520remaining%2520unlabeled%2520data.%2520Our%250Asoftware%2520is%2520available%2520on%2520https%253A//github.com/pieterblok/boxal.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2410.04880v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Improved%20detection%20of%20discarded%20fish%20species%20through%20BoxAL%20active%0A%20%20learning&entry.906535625=Maria%20Sokolova%20and%20Pieter%20M.%20Blok%20and%20Angelo%20Mencarelli%20and%20Arjan%20Vroegop%20and%20Aloysius%20van%20Helmond%20and%20Gert%20Kootstra&entry.1292438233=%20%20In%20recent%20years%2C%20powerful%20data-driven%20deep-learning%20techniques%20have%20been%0Adeveloped%20and%20applied%20for%20automated%20catch%20registration.%20However%2C%20these%20methods%0Aare%20dependent%20on%20the%20labelled%20data%2C%20which%20is%20time-consuming%2C%20labour-intensive%2C%0Aexpensive%20to%20collect%20and%20need%20expert%20knowledge.%20In%20this%20study%2C%20we%20present%20an%0Aactive%20learning%20technique%2C%20named%20BoxAL%2C%20which%20includes%20estimation%20of%20epistemic%0Acertainty%20of%20the%20Faster%20R-CNN%20object-detection%20model.%20The%20method%20allows%0Aselecting%20the%20most%20uncertain%20training%20images%20from%20an%20unlabeled%20pool%2C%20which%20are%0Athen%20used%20to%20train%20the%20object-detection%20model.%20To%20evaluate%20the%20method%2C%20we%20used%0Aan%20open-source%20image%20dataset%20obtained%20with%20a%20dedicated%20image-acquisition%20system%0Adeveloped%20for%20commercial%20trawlers%20targeting%20demersal%20species.%20We%20demonstrated%2C%0Athat%20our%20approach%20allows%20reaching%20the%20same%20object-detection%20performance%20as%20with%0Athe%20random%20sampling%20using%20400%20fewer%20labelled%20images.%20Besides%2C%20mean%20AP%20score%20was%0Asignificantly%20higher%20at%20the%20last%20training%20iteration%20with%201100%20training%20images%2C%0Aspecifically%2C%2039.0%26plusmn%3B1.6%20and%2034.8%26plusmn%3B1.8%20for%20certainty-based%20sampling%0Aand%20random%20sampling%2C%20respectively.%20Additionally%2C%20we%20showed%20that%20epistemic%0Acertainty%20is%20a%20suitable%20method%20to%20sample%20images%20that%20the%20current%20iteration%20of%0Athe%20model%20cannot%20deal%20with%20yet.%20Our%20study%20additionally%20showed%20that%20the%20sampled%0Anew%20data%20is%20more%20valuable%20for%20training%20than%20the%20remaining%20unlabeled%20data.%20Our%0Asoftware%20is%20available%20on%20https%3A//github.com/pieterblok/boxal.%0A&entry.1838667208=http%3A//arxiv.org/abs/2410.04880v1&entry.124074799=Read"},
{"title": "MetaDD: Boosting Dataset Distillation with Neural Network\n  Architecture-Invariant Generalization", "author": "Yunlong Zhao and Xiaoheng Deng and Xiu Su and Hongyan Xu and Xiuxing Li and Yijing Liu and Shan You", "abstract": "  Dataset distillation (DD) entails creating a refined, compact distilled\ndataset from a large-scale dataset to facilitate efficient training. A\nsignificant challenge in DD is the dependency between the distilled dataset and\nthe neural network (NN) architecture used. Training a different NN architecture\nwith a distilled dataset distilled using a specific architecture often results\nin diminished trainning performance for other architectures. This paper\nintroduces MetaDD, designed to enhance the generalizability of DD across\nvarious NN architectures. Specifically, MetaDD partitions distilled data into\nmeta features (i.e., the data's common characteristics that remain consistent\nacross different NN architectures) and heterogeneous features (i.e., the data's\nunique feature to each NN architecture). Then, MetaDD employs an\narchitecture-invariant loss function for multi-architecture feature alignment,\nwhich increases meta features and reduces heterogeneous features in distilled\ndata. As a low-memory consumption component, MetaDD can be seamlessly\nintegrated into any DD methodology. Experimental results demonstrate that\nMetaDD significantly improves performance across various DD methods. On the\nDistilled Tiny-Imagenet with Sre2L (50 IPC), MetaDD achieves cross-architecture\nNN accuracy of up to 30.1\\%, surpassing the second-best method (GLaD) by 1.7\\%.\n", "link": "http://arxiv.org/abs/2410.05103v1", "date": "2024-10-07", "relevancy": 2.1692, "topK": [{"title": "Boosted dynamic neural networks", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/26302", "similarity": 0.5673}, {"title": "ToDo: Token Downsampling for Efficient Generation of High-Resolution\n  Images", "link": "http://arxiv.org/abs/2402.13573v2", "similarity": 0.5255}, {"title": "SDXS: Real-Time One-Step Latent Diffusion Models with Image Conditions", "link": "http://arxiv.org/abs/2403.16627v1", "similarity": 0.5219}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20MetaDD%3A%20Boosting%20Dataset%20Distillation%20with%20Neural%20Network%0A%20%20Architecture-Invariant%20Generalization&body=Title%3A%20MetaDD%3A%20Boosting%20Dataset%20Distillation%20with%20Neural%20Network%0A%20%20Architecture-Invariant%20Generalization%0AAuthor%3A%20Yunlong%20Zhao%20and%20Xiaoheng%20Deng%20and%20Xiu%20Su%20and%20Hongyan%20Xu%20and%20Xiuxing%20Li%20and%20Yijing%20Liu%20and%20Shan%20You%0AAbstract%3A%20%20%20Dataset%20distillation%20%28DD%29%20entails%20creating%20a%20refined%2C%20compact%20distilled%0Adataset%20from%20a%20large-scale%20dataset%20to%20facilitate%20efficient%20training.%20A%0Asignificant%20challenge%20in%20DD%20is%20the%20dependency%20between%20the%20distilled%20dataset%20and%0Athe%20neural%20network%20%28NN%29%20architecture%20used.%20Training%20a%20different%20NN%20architecture%0Awith%20a%20distilled%20dataset%20distilled%20using%20a%20specific%20architecture%20often%20results%0Ain%20diminished%20trainning%20performance%20for%20other%20architectures.%20This%20paper%0Aintroduces%20MetaDD%2C%20designed%20to%20enhance%20the%20generalizability%20of%20DD%20across%0Avarious%20NN%20architectures.%20Specifically%2C%20MetaDD%20partitions%20distilled%20data%20into%0Ameta%20features%20%28i.e.%2C%20the%20data%27s%20common%20characteristics%20that%20remain%20consistent%0Aacross%20different%20NN%20architectures%29%20and%20heterogeneous%20features%20%28i.e.%2C%20the%20data%27s%0Aunique%20feature%20to%20each%20NN%20architecture%29.%20Then%2C%20MetaDD%20employs%20an%0Aarchitecture-invariant%20loss%20function%20for%20multi-architecture%20feature%20alignment%2C%0Awhich%20increases%20meta%20features%20and%20reduces%20heterogeneous%20features%20in%20distilled%0Adata.%20As%20a%20low-memory%20consumption%20component%2C%20MetaDD%20can%20be%20seamlessly%0Aintegrated%20into%20any%20DD%20methodology.%20Experimental%20results%20demonstrate%20that%0AMetaDD%20significantly%20improves%20performance%20across%20various%20DD%20methods.%20On%20the%0ADistilled%20Tiny-Imagenet%20with%20Sre2L%20%2850%20IPC%29%2C%20MetaDD%20achieves%20cross-architecture%0ANN%20accuracy%20of%20up%20to%2030.1%5C%25%2C%20surpassing%20the%20second-best%20method%20%28GLaD%29%20by%201.7%5C%25.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2410.05103v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DMetaDD%253A%2520Boosting%2520Dataset%2520Distillation%2520with%2520Neural%2520Network%250A%2520%2520Architecture-Invariant%2520Generalization%26entry.906535625%3DYunlong%2520Zhao%2520and%2520Xiaoheng%2520Deng%2520and%2520Xiu%2520Su%2520and%2520Hongyan%2520Xu%2520and%2520Xiuxing%2520Li%2520and%2520Yijing%2520Liu%2520and%2520Shan%2520You%26entry.1292438233%3D%2520%2520Dataset%2520distillation%2520%2528DD%2529%2520entails%2520creating%2520a%2520refined%252C%2520compact%2520distilled%250Adataset%2520from%2520a%2520large-scale%2520dataset%2520to%2520facilitate%2520efficient%2520training.%2520A%250Asignificant%2520challenge%2520in%2520DD%2520is%2520the%2520dependency%2520between%2520the%2520distilled%2520dataset%2520and%250Athe%2520neural%2520network%2520%2528NN%2529%2520architecture%2520used.%2520Training%2520a%2520different%2520NN%2520architecture%250Awith%2520a%2520distilled%2520dataset%2520distilled%2520using%2520a%2520specific%2520architecture%2520often%2520results%250Ain%2520diminished%2520trainning%2520performance%2520for%2520other%2520architectures.%2520This%2520paper%250Aintroduces%2520MetaDD%252C%2520designed%2520to%2520enhance%2520the%2520generalizability%2520of%2520DD%2520across%250Avarious%2520NN%2520architectures.%2520Specifically%252C%2520MetaDD%2520partitions%2520distilled%2520data%2520into%250Ameta%2520features%2520%2528i.e.%252C%2520the%2520data%2527s%2520common%2520characteristics%2520that%2520remain%2520consistent%250Aacross%2520different%2520NN%2520architectures%2529%2520and%2520heterogeneous%2520features%2520%2528i.e.%252C%2520the%2520data%2527s%250Aunique%2520feature%2520to%2520each%2520NN%2520architecture%2529.%2520Then%252C%2520MetaDD%2520employs%2520an%250Aarchitecture-invariant%2520loss%2520function%2520for%2520multi-architecture%2520feature%2520alignment%252C%250Awhich%2520increases%2520meta%2520features%2520and%2520reduces%2520heterogeneous%2520features%2520in%2520distilled%250Adata.%2520As%2520a%2520low-memory%2520consumption%2520component%252C%2520MetaDD%2520can%2520be%2520seamlessly%250Aintegrated%2520into%2520any%2520DD%2520methodology.%2520Experimental%2520results%2520demonstrate%2520that%250AMetaDD%2520significantly%2520improves%2520performance%2520across%2520various%2520DD%2520methods.%2520On%2520the%250ADistilled%2520Tiny-Imagenet%2520with%2520Sre2L%2520%252850%2520IPC%2529%252C%2520MetaDD%2520achieves%2520cross-architecture%250ANN%2520accuracy%2520of%2520up%2520to%252030.1%255C%2525%252C%2520surpassing%2520the%2520second-best%2520method%2520%2528GLaD%2529%2520by%25201.7%255C%2525.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2410.05103v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=MetaDD%3A%20Boosting%20Dataset%20Distillation%20with%20Neural%20Network%0A%20%20Architecture-Invariant%20Generalization&entry.906535625=Yunlong%20Zhao%20and%20Xiaoheng%20Deng%20and%20Xiu%20Su%20and%20Hongyan%20Xu%20and%20Xiuxing%20Li%20and%20Yijing%20Liu%20and%20Shan%20You&entry.1292438233=%20%20Dataset%20distillation%20%28DD%29%20entails%20creating%20a%20refined%2C%20compact%20distilled%0Adataset%20from%20a%20large-scale%20dataset%20to%20facilitate%20efficient%20training.%20A%0Asignificant%20challenge%20in%20DD%20is%20the%20dependency%20between%20the%20distilled%20dataset%20and%0Athe%20neural%20network%20%28NN%29%20architecture%20used.%20Training%20a%20different%20NN%20architecture%0Awith%20a%20distilled%20dataset%20distilled%20using%20a%20specific%20architecture%20often%20results%0Ain%20diminished%20trainning%20performance%20for%20other%20architectures.%20This%20paper%0Aintroduces%20MetaDD%2C%20designed%20to%20enhance%20the%20generalizability%20of%20DD%20across%0Avarious%20NN%20architectures.%20Specifically%2C%20MetaDD%20partitions%20distilled%20data%20into%0Ameta%20features%20%28i.e.%2C%20the%20data%27s%20common%20characteristics%20that%20remain%20consistent%0Aacross%20different%20NN%20architectures%29%20and%20heterogeneous%20features%20%28i.e.%2C%20the%20data%27s%0Aunique%20feature%20to%20each%20NN%20architecture%29.%20Then%2C%20MetaDD%20employs%20an%0Aarchitecture-invariant%20loss%20function%20for%20multi-architecture%20feature%20alignment%2C%0Awhich%20increases%20meta%20features%20and%20reduces%20heterogeneous%20features%20in%20distilled%0Adata.%20As%20a%20low-memory%20consumption%20component%2C%20MetaDD%20can%20be%20seamlessly%0Aintegrated%20into%20any%20DD%20methodology.%20Experimental%20results%20demonstrate%20that%0AMetaDD%20significantly%20improves%20performance%20across%20various%20DD%20methods.%20On%20the%0ADistilled%20Tiny-Imagenet%20with%20Sre2L%20%2850%20IPC%29%2C%20MetaDD%20achieves%20cross-architecture%0ANN%20accuracy%20of%20up%20to%2030.1%5C%25%2C%20surpassing%20the%20second-best%20method%20%28GLaD%29%20by%201.7%5C%25.%0A&entry.1838667208=http%3A//arxiv.org/abs/2410.05103v1&entry.124074799=Read"},
{"title": "Next state prediction gives rise to entangled, yet compositional\n  representations of objects", "author": "Tankred Saanum and Luca M. Schulze Buschoff and Peter Dayan and Eric Schulz", "abstract": "  Compositional representations are thought to enable humans to generalize\nacross combinatorially vast state spaces. Models with learnable object slots,\nwhich encode information about objects in separate latent codes, have shown\npromise for this type of generalization but rely on strong architectural\npriors. Models with distributed representations, on the other hand, use\noverlapping, potentially entangled neural codes, and their ability to support\ncompositional generalization remains underexplored. In this paper we examine\nwhether distributed models can develop linearly separable representations of\nobjects, like slotted models, through unsupervised training on videos of object\ninteractions. We show that, surprisingly, models with distributed\nrepresentations often match or outperform models with object slots in\ndownstream prediction tasks. Furthermore, we find that linearly separable\nobject representations can emerge without object-centric priors, with auxiliary\nobjectives like next-state prediction playing a key role. Finally, we observe\nthat distributed models' object representations are never fully disentangled,\neven if they are linearly separable: Multiple objects can be encoded through\npartially overlapping neural populations while still being highly separable\nwith a linear classifier. We hypothesize that maintaining partially shared\ncodes enables distributed models to better compress object dynamics,\npotentially enhancing generalization.\n", "link": "http://arxiv.org/abs/2410.04940v1", "date": "2024-10-07", "relevancy": 2.1583, "topK": [{"title": "Thinking Outside the BBox: Unconstrained Generative Object Compositing", "link": "http://arxiv.org/abs/2409.04559v2", "similarity": 0.573}, {"title": "Self-supervised Photographic Image Layout Representation Learning", "link": "http://arxiv.org/abs/2403.03740v1", "similarity": 0.5342}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5316}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Next%20state%20prediction%20gives%20rise%20to%20entangled%2C%20yet%20compositional%0A%20%20representations%20of%20objects&body=Title%3A%20Next%20state%20prediction%20gives%20rise%20to%20entangled%2C%20yet%20compositional%0A%20%20representations%20of%20objects%0AAuthor%3A%20Tankred%20Saanum%20and%20Luca%20M.%20Schulze%20Buschoff%20and%20Peter%20Dayan%20and%20Eric%20Schulz%0AAbstract%3A%20%20%20Compositional%20representations%20are%20thought%20to%20enable%20humans%20to%20generalize%0Aacross%20combinatorially%20vast%20state%20spaces.%20Models%20with%20learnable%20object%20slots%2C%0Awhich%20encode%20information%20about%20objects%20in%20separate%20latent%20codes%2C%20have%20shown%0Apromise%20for%20this%20type%20of%20generalization%20but%20rely%20on%20strong%20architectural%0Apriors.%20Models%20with%20distributed%20representations%2C%20on%20the%20other%20hand%2C%20use%0Aoverlapping%2C%20potentially%20entangled%20neural%20codes%2C%20and%20their%20ability%20to%20support%0Acompositional%20generalization%20remains%20underexplored.%20In%20this%20paper%20we%20examine%0Awhether%20distributed%20models%20can%20develop%20linearly%20separable%20representations%20of%0Aobjects%2C%20like%20slotted%20models%2C%20through%20unsupervised%20training%20on%20videos%20of%20object%0Ainteractions.%20We%20show%20that%2C%20surprisingly%2C%20models%20with%20distributed%0Arepresentations%20often%20match%20or%20outperform%20models%20with%20object%20slots%20in%0Adownstream%20prediction%20tasks.%20Furthermore%2C%20we%20find%20that%20linearly%20separable%0Aobject%20representations%20can%20emerge%20without%20object-centric%20priors%2C%20with%20auxiliary%0Aobjectives%20like%20next-state%20prediction%20playing%20a%20key%20role.%20Finally%2C%20we%20observe%0Athat%20distributed%20models%27%20object%20representations%20are%20never%20fully%20disentangled%2C%0Aeven%20if%20they%20are%20linearly%20separable%3A%20Multiple%20objects%20can%20be%20encoded%20through%0Apartially%20overlapping%20neural%20populations%20while%20still%20being%20highly%20separable%0Awith%20a%20linear%20classifier.%20We%20hypothesize%20that%20maintaining%20partially%20shared%0Acodes%20enables%20distributed%20models%20to%20better%20compress%20object%20dynamics%2C%0Apotentially%20enhancing%20generalization.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2410.04940v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DNext%2520state%2520prediction%2520gives%2520rise%2520to%2520entangled%252C%2520yet%2520compositional%250A%2520%2520representations%2520of%2520objects%26entry.906535625%3DTankred%2520Saanum%2520and%2520Luca%2520M.%2520Schulze%2520Buschoff%2520and%2520Peter%2520Dayan%2520and%2520Eric%2520Schulz%26entry.1292438233%3D%2520%2520Compositional%2520representations%2520are%2520thought%2520to%2520enable%2520humans%2520to%2520generalize%250Aacross%2520combinatorially%2520vast%2520state%2520spaces.%2520Models%2520with%2520learnable%2520object%2520slots%252C%250Awhich%2520encode%2520information%2520about%2520objects%2520in%2520separate%2520latent%2520codes%252C%2520have%2520shown%250Apromise%2520for%2520this%2520type%2520of%2520generalization%2520but%2520rely%2520on%2520strong%2520architectural%250Apriors.%2520Models%2520with%2520distributed%2520representations%252C%2520on%2520the%2520other%2520hand%252C%2520use%250Aoverlapping%252C%2520potentially%2520entangled%2520neural%2520codes%252C%2520and%2520their%2520ability%2520to%2520support%250Acompositional%2520generalization%2520remains%2520underexplored.%2520In%2520this%2520paper%2520we%2520examine%250Awhether%2520distributed%2520models%2520can%2520develop%2520linearly%2520separable%2520representations%2520of%250Aobjects%252C%2520like%2520slotted%2520models%252C%2520through%2520unsupervised%2520training%2520on%2520videos%2520of%2520object%250Ainteractions.%2520We%2520show%2520that%252C%2520surprisingly%252C%2520models%2520with%2520distributed%250Arepresentations%2520often%2520match%2520or%2520outperform%2520models%2520with%2520object%2520slots%2520in%250Adownstream%2520prediction%2520tasks.%2520Furthermore%252C%2520we%2520find%2520that%2520linearly%2520separable%250Aobject%2520representations%2520can%2520emerge%2520without%2520object-centric%2520priors%252C%2520with%2520auxiliary%250Aobjectives%2520like%2520next-state%2520prediction%2520playing%2520a%2520key%2520role.%2520Finally%252C%2520we%2520observe%250Athat%2520distributed%2520models%2527%2520object%2520representations%2520are%2520never%2520fully%2520disentangled%252C%250Aeven%2520if%2520they%2520are%2520linearly%2520separable%253A%2520Multiple%2520objects%2520can%2520be%2520encoded%2520through%250Apartially%2520overlapping%2520neural%2520populations%2520while%2520still%2520being%2520highly%2520separable%250Awith%2520a%2520linear%2520classifier.%2520We%2520hypothesize%2520that%2520maintaining%2520partially%2520shared%250Acodes%2520enables%2520distributed%2520models%2520to%2520better%2520compress%2520object%2520dynamics%252C%250Apotentially%2520enhancing%2520generalization.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2410.04940v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Next%20state%20prediction%20gives%20rise%20to%20entangled%2C%20yet%20compositional%0A%20%20representations%20of%20objects&entry.906535625=Tankred%20Saanum%20and%20Luca%20M.%20Schulze%20Buschoff%20and%20Peter%20Dayan%20and%20Eric%20Schulz&entry.1292438233=%20%20Compositional%20representations%20are%20thought%20to%20enable%20humans%20to%20generalize%0Aacross%20combinatorially%20vast%20state%20spaces.%20Models%20with%20learnable%20object%20slots%2C%0Awhich%20encode%20information%20about%20objects%20in%20separate%20latent%20codes%2C%20have%20shown%0Apromise%20for%20this%20type%20of%20generalization%20but%20rely%20on%20strong%20architectural%0Apriors.%20Models%20with%20distributed%20representations%2C%20on%20the%20other%20hand%2C%20use%0Aoverlapping%2C%20potentially%20entangled%20neural%20codes%2C%20and%20their%20ability%20to%20support%0Acompositional%20generalization%20remains%20underexplored.%20In%20this%20paper%20we%20examine%0Awhether%20distributed%20models%20can%20develop%20linearly%20separable%20representations%20of%0Aobjects%2C%20like%20slotted%20models%2C%20through%20unsupervised%20training%20on%20videos%20of%20object%0Ainteractions.%20We%20show%20that%2C%20surprisingly%2C%20models%20with%20distributed%0Arepresentations%20often%20match%20or%20outperform%20models%20with%20object%20slots%20in%0Adownstream%20prediction%20tasks.%20Furthermore%2C%20we%20find%20that%20linearly%20separable%0Aobject%20representations%20can%20emerge%20without%20object-centric%20priors%2C%20with%20auxiliary%0Aobjectives%20like%20next-state%20prediction%20playing%20a%20key%20role.%20Finally%2C%20we%20observe%0Athat%20distributed%20models%27%20object%20representations%20are%20never%20fully%20disentangled%2C%0Aeven%20if%20they%20are%20linearly%20separable%3A%20Multiple%20objects%20can%20be%20encoded%20through%0Apartially%20overlapping%20neural%20populations%20while%20still%20being%20highly%20separable%0Awith%20a%20linear%20classifier.%20We%20hypothesize%20that%20maintaining%20partially%20shared%0Acodes%20enables%20distributed%20models%20to%20better%20compress%20object%20dynamics%2C%0Apotentially%20enhancing%20generalization.%0A&entry.1838667208=http%3A//arxiv.org/abs/2410.04940v1&entry.124074799=Read"},
{"title": "mDPO: Conditional Preference Optimization for Multimodal Large Language\n  Models", "author": "Fei Wang and Wenxuan Zhou and James Y. Huang and Nan Xu and Sheng Zhang and Hoifung Poon and Muhao Chen", "abstract": "  Direct preference optimization (DPO) has shown to be an effective method for\nlarge language model (LLM) alignment. Recent works have attempted to apply DPO\nto multimodal scenarios but have found it challenging to achieve consistent\nimprovement. Through a comparative experiment, we identify the unconditional\npreference problem in multimodal preference optimization, where the model\noverlooks the image condition. To address this problem, we propose mDPO, a\nmultimodal DPO objective that prevents the over-prioritization of language-only\npreferences by also optimizing image preference. Moreover, we introduce a\nreward anchor that forces the reward to be positive for chosen responses,\nthereby avoiding the decrease in their likelihood -- an intrinsic problem of\nrelative preference optimization. Experiments on two multimodal LLMs of\ndifferent sizes and three widely used benchmarks demonstrate that mDPO\neffectively addresses the unconditional preference problem in multimodal\npreference optimization and significantly improves model performance,\nparticularly in reducing hallucination.\n", "link": "http://arxiv.org/abs/2406.11839v2", "date": "2024-10-07", "relevancy": 2.1498, "topK": [{"title": "Multimodal-Conditioned Latent Diffusion Models for Fashion Image Editing", "link": "http://arxiv.org/abs/2403.14828v2", "similarity": 0.564}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5321}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5321}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20mDPO%3A%20Conditional%20Preference%20Optimization%20for%20Multimodal%20Large%20Language%0A%20%20Models&body=Title%3A%20mDPO%3A%20Conditional%20Preference%20Optimization%20for%20Multimodal%20Large%20Language%0A%20%20Models%0AAuthor%3A%20Fei%20Wang%20and%20Wenxuan%20Zhou%20and%20James%20Y.%20Huang%20and%20Nan%20Xu%20and%20Sheng%20Zhang%20and%20Hoifung%20Poon%20and%20Muhao%20Chen%0AAbstract%3A%20%20%20Direct%20preference%20optimization%20%28DPO%29%20has%20shown%20to%20be%20an%20effective%20method%20for%0Alarge%20language%20model%20%28LLM%29%20alignment.%20Recent%20works%20have%20attempted%20to%20apply%20DPO%0Ato%20multimodal%20scenarios%20but%20have%20found%20it%20challenging%20to%20achieve%20consistent%0Aimprovement.%20Through%20a%20comparative%20experiment%2C%20we%20identify%20the%20unconditional%0Apreference%20problem%20in%20multimodal%20preference%20optimization%2C%20where%20the%20model%0Aoverlooks%20the%20image%20condition.%20To%20address%20this%20problem%2C%20we%20propose%20mDPO%2C%20a%0Amultimodal%20DPO%20objective%20that%20prevents%20the%20over-prioritization%20of%20language-only%0Apreferences%20by%20also%20optimizing%20image%20preference.%20Moreover%2C%20we%20introduce%20a%0Areward%20anchor%20that%20forces%20the%20reward%20to%20be%20positive%20for%20chosen%20responses%2C%0Athereby%20avoiding%20the%20decrease%20in%20their%20likelihood%20--%20an%20intrinsic%20problem%20of%0Arelative%20preference%20optimization.%20Experiments%20on%20two%20multimodal%20LLMs%20of%0Adifferent%20sizes%20and%20three%20widely%20used%20benchmarks%20demonstrate%20that%20mDPO%0Aeffectively%20addresses%20the%20unconditional%20preference%20problem%20in%20multimodal%0Apreference%20optimization%20and%20significantly%20improves%20model%20performance%2C%0Aparticularly%20in%20reducing%20hallucination.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2406.11839v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DmDPO%253A%2520Conditional%2520Preference%2520Optimization%2520for%2520Multimodal%2520Large%2520Language%250A%2520%2520Models%26entry.906535625%3DFei%2520Wang%2520and%2520Wenxuan%2520Zhou%2520and%2520James%2520Y.%2520Huang%2520and%2520Nan%2520Xu%2520and%2520Sheng%2520Zhang%2520and%2520Hoifung%2520Poon%2520and%2520Muhao%2520Chen%26entry.1292438233%3D%2520%2520Direct%2520preference%2520optimization%2520%2528DPO%2529%2520has%2520shown%2520to%2520be%2520an%2520effective%2520method%2520for%250Alarge%2520language%2520model%2520%2528LLM%2529%2520alignment.%2520Recent%2520works%2520have%2520attempted%2520to%2520apply%2520DPO%250Ato%2520multimodal%2520scenarios%2520but%2520have%2520found%2520it%2520challenging%2520to%2520achieve%2520consistent%250Aimprovement.%2520Through%2520a%2520comparative%2520experiment%252C%2520we%2520identify%2520the%2520unconditional%250Apreference%2520problem%2520in%2520multimodal%2520preference%2520optimization%252C%2520where%2520the%2520model%250Aoverlooks%2520the%2520image%2520condition.%2520To%2520address%2520this%2520problem%252C%2520we%2520propose%2520mDPO%252C%2520a%250Amultimodal%2520DPO%2520objective%2520that%2520prevents%2520the%2520over-prioritization%2520of%2520language-only%250Apreferences%2520by%2520also%2520optimizing%2520image%2520preference.%2520Moreover%252C%2520we%2520introduce%2520a%250Areward%2520anchor%2520that%2520forces%2520the%2520reward%2520to%2520be%2520positive%2520for%2520chosen%2520responses%252C%250Athereby%2520avoiding%2520the%2520decrease%2520in%2520their%2520likelihood%2520--%2520an%2520intrinsic%2520problem%2520of%250Arelative%2520preference%2520optimization.%2520Experiments%2520on%2520two%2520multimodal%2520LLMs%2520of%250Adifferent%2520sizes%2520and%2520three%2520widely%2520used%2520benchmarks%2520demonstrate%2520that%2520mDPO%250Aeffectively%2520addresses%2520the%2520unconditional%2520preference%2520problem%2520in%2520multimodal%250Apreference%2520optimization%2520and%2520significantly%2520improves%2520model%2520performance%252C%250Aparticularly%2520in%2520reducing%2520hallucination.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2406.11839v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=mDPO%3A%20Conditional%20Preference%20Optimization%20for%20Multimodal%20Large%20Language%0A%20%20Models&entry.906535625=Fei%20Wang%20and%20Wenxuan%20Zhou%20and%20James%20Y.%20Huang%20and%20Nan%20Xu%20and%20Sheng%20Zhang%20and%20Hoifung%20Poon%20and%20Muhao%20Chen&entry.1292438233=%20%20Direct%20preference%20optimization%20%28DPO%29%20has%20shown%20to%20be%20an%20effective%20method%20for%0Alarge%20language%20model%20%28LLM%29%20alignment.%20Recent%20works%20have%20attempted%20to%20apply%20DPO%0Ato%20multimodal%20scenarios%20but%20have%20found%20it%20challenging%20to%20achieve%20consistent%0Aimprovement.%20Through%20a%20comparative%20experiment%2C%20we%20identify%20the%20unconditional%0Apreference%20problem%20in%20multimodal%20preference%20optimization%2C%20where%20the%20model%0Aoverlooks%20the%20image%20condition.%20To%20address%20this%20problem%2C%20we%20propose%20mDPO%2C%20a%0Amultimodal%20DPO%20objective%20that%20prevents%20the%20over-prioritization%20of%20language-only%0Apreferences%20by%20also%20optimizing%20image%20preference.%20Moreover%2C%20we%20introduce%20a%0Areward%20anchor%20that%20forces%20the%20reward%20to%20be%20positive%20for%20chosen%20responses%2C%0Athereby%20avoiding%20the%20decrease%20in%20their%20likelihood%20--%20an%20intrinsic%20problem%20of%0Arelative%20preference%20optimization.%20Experiments%20on%20two%20multimodal%20LLMs%20of%0Adifferent%20sizes%20and%20three%20widely%20used%20benchmarks%20demonstrate%20that%20mDPO%0Aeffectively%20addresses%20the%20unconditional%20preference%20problem%20in%20multimodal%0Apreference%20optimization%20and%20significantly%20improves%20model%20performance%2C%0Aparticularly%20in%20reducing%20hallucination.%0A&entry.1838667208=http%3A//arxiv.org/abs/2406.11839v2&entry.124074799=Read"},
{"title": "Towards Embedding Dynamic Personas in Interactive Robots: Masquerading\n  Animated Social Kinematics (MASK)", "author": "Jeongeun Park and Taemoon Jeong and Hyeonseong Kim and Taehyun Byun and Seungyoon Shin and Keunjun Choi and Jaewoon Kwon and Taeyoon Lee and Matthew Pan and Sungjoon Choi", "abstract": "  This paper presents the design and development of an innovative interactive\nrobotic system to enhance audience engagement using character-like personas.\nBuilt upon the foundations of persona-driven dialog agents, this work extends\nthe agent's application to the physical realm, employing robots to provide a\nmore captivating and interactive experience. The proposed system, named the\nMasquerading Animated Social Kinematic (MASK), leverages an anthropomorphic\nrobot which interacts with guests using non-verbal interactions, including\nfacial expressions and gestures. A behavior generation system based upon a\nfinite-state machine structure effectively conditions robotic behavior to\nconvey distinct personas. The MASK framework integrates a perception engine, a\nbehavior selection engine, and a comprehensive action library to enable\nreal-time, dynamic interactions with minimal human intervention in behavior\ndesign. Throughout the user subject studies, we examined whether the users\ncould recognize the intended character in both personality- and\nfilm-character-based persona conditions. We conclude by discussing the role of\npersonas in interactive agents and the factors to consider for creating an\nengaging user experience.\n", "link": "http://arxiv.org/abs/2403.10041v2", "date": "2024-10-07", "relevancy": 2.1429, "topK": [{"title": "RoboEXP: Action-Conditioned Scene Graph via Interactive Exploration for\n  Robotic Manipulation", "link": "http://arxiv.org/abs/2402.15487v1", "similarity": 0.5584}, {"title": "PoseAnimate: Zero-shot high fidelity pose controllable character\n  animation", "link": "http://arxiv.org/abs/2404.13680v2", "similarity": 0.5352}, {"title": "VirtualModel: Generating Object-ID-retentive Human-object Interaction\n  Image by Diffusion Model for E-commerce Marketing", "link": "http://arxiv.org/abs/2405.09985v1", "similarity": 0.5272}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Towards%20Embedding%20Dynamic%20Personas%20in%20Interactive%20Robots%3A%20Masquerading%0A%20%20Animated%20Social%20Kinematics%20%28MASK%29&body=Title%3A%20Towards%20Embedding%20Dynamic%20Personas%20in%20Interactive%20Robots%3A%20Masquerading%0A%20%20Animated%20Social%20Kinematics%20%28MASK%29%0AAuthor%3A%20Jeongeun%20Park%20and%20Taemoon%20Jeong%20and%20Hyeonseong%20Kim%20and%20Taehyun%20Byun%20and%20Seungyoon%20Shin%20and%20Keunjun%20Choi%20and%20Jaewoon%20Kwon%20and%20Taeyoon%20Lee%20and%20Matthew%20Pan%20and%20Sungjoon%20Choi%0AAbstract%3A%20%20%20This%20paper%20presents%20the%20design%20and%20development%20of%20an%20innovative%20interactive%0Arobotic%20system%20to%20enhance%20audience%20engagement%20using%20character-like%20personas.%0ABuilt%20upon%20the%20foundations%20of%20persona-driven%20dialog%20agents%2C%20this%20work%20extends%0Athe%20agent%27s%20application%20to%20the%20physical%20realm%2C%20employing%20robots%20to%20provide%20a%0Amore%20captivating%20and%20interactive%20experience.%20The%20proposed%20system%2C%20named%20the%0AMasquerading%20Animated%20Social%20Kinematic%20%28MASK%29%2C%20leverages%20an%20anthropomorphic%0Arobot%20which%20interacts%20with%20guests%20using%20non-verbal%20interactions%2C%20including%0Afacial%20expressions%20and%20gestures.%20A%20behavior%20generation%20system%20based%20upon%20a%0Afinite-state%20machine%20structure%20effectively%20conditions%20robotic%20behavior%20to%0Aconvey%20distinct%20personas.%20The%20MASK%20framework%20integrates%20a%20perception%20engine%2C%20a%0Abehavior%20selection%20engine%2C%20and%20a%20comprehensive%20action%20library%20to%20enable%0Areal-time%2C%20dynamic%20interactions%20with%20minimal%20human%20intervention%20in%20behavior%0Adesign.%20Throughout%20the%20user%20subject%20studies%2C%20we%20examined%20whether%20the%20users%0Acould%20recognize%20the%20intended%20character%20in%20both%20personality-%20and%0Afilm-character-based%20persona%20conditions.%20We%20conclude%20by%20discussing%20the%20role%20of%0Apersonas%20in%20interactive%20agents%20and%20the%20factors%20to%20consider%20for%20creating%20an%0Aengaging%20user%20experience.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2403.10041v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DTowards%2520Embedding%2520Dynamic%2520Personas%2520in%2520Interactive%2520Robots%253A%2520Masquerading%250A%2520%2520Animated%2520Social%2520Kinematics%2520%2528MASK%2529%26entry.906535625%3DJeongeun%2520Park%2520and%2520Taemoon%2520Jeong%2520and%2520Hyeonseong%2520Kim%2520and%2520Taehyun%2520Byun%2520and%2520Seungyoon%2520Shin%2520and%2520Keunjun%2520Choi%2520and%2520Jaewoon%2520Kwon%2520and%2520Taeyoon%2520Lee%2520and%2520Matthew%2520Pan%2520and%2520Sungjoon%2520Choi%26entry.1292438233%3D%2520%2520This%2520paper%2520presents%2520the%2520design%2520and%2520development%2520of%2520an%2520innovative%2520interactive%250Arobotic%2520system%2520to%2520enhance%2520audience%2520engagement%2520using%2520character-like%2520personas.%250ABuilt%2520upon%2520the%2520foundations%2520of%2520persona-driven%2520dialog%2520agents%252C%2520this%2520work%2520extends%250Athe%2520agent%2527s%2520application%2520to%2520the%2520physical%2520realm%252C%2520employing%2520robots%2520to%2520provide%2520a%250Amore%2520captivating%2520and%2520interactive%2520experience.%2520The%2520proposed%2520system%252C%2520named%2520the%250AMasquerading%2520Animated%2520Social%2520Kinematic%2520%2528MASK%2529%252C%2520leverages%2520an%2520anthropomorphic%250Arobot%2520which%2520interacts%2520with%2520guests%2520using%2520non-verbal%2520interactions%252C%2520including%250Afacial%2520expressions%2520and%2520gestures.%2520A%2520behavior%2520generation%2520system%2520based%2520upon%2520a%250Afinite-state%2520machine%2520structure%2520effectively%2520conditions%2520robotic%2520behavior%2520to%250Aconvey%2520distinct%2520personas.%2520The%2520MASK%2520framework%2520integrates%2520a%2520perception%2520engine%252C%2520a%250Abehavior%2520selection%2520engine%252C%2520and%2520a%2520comprehensive%2520action%2520library%2520to%2520enable%250Areal-time%252C%2520dynamic%2520interactions%2520with%2520minimal%2520human%2520intervention%2520in%2520behavior%250Adesign.%2520Throughout%2520the%2520user%2520subject%2520studies%252C%2520we%2520examined%2520whether%2520the%2520users%250Acould%2520recognize%2520the%2520intended%2520character%2520in%2520both%2520personality-%2520and%250Afilm-character-based%2520persona%2520conditions.%2520We%2520conclude%2520by%2520discussing%2520the%2520role%2520of%250Apersonas%2520in%2520interactive%2520agents%2520and%2520the%2520factors%2520to%2520consider%2520for%2520creating%2520an%250Aengaging%2520user%2520experience.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2403.10041v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Towards%20Embedding%20Dynamic%20Personas%20in%20Interactive%20Robots%3A%20Masquerading%0A%20%20Animated%20Social%20Kinematics%20%28MASK%29&entry.906535625=Jeongeun%20Park%20and%20Taemoon%20Jeong%20and%20Hyeonseong%20Kim%20and%20Taehyun%20Byun%20and%20Seungyoon%20Shin%20and%20Keunjun%20Choi%20and%20Jaewoon%20Kwon%20and%20Taeyoon%20Lee%20and%20Matthew%20Pan%20and%20Sungjoon%20Choi&entry.1292438233=%20%20This%20paper%20presents%20the%20design%20and%20development%20of%20an%20innovative%20interactive%0Arobotic%20system%20to%20enhance%20audience%20engagement%20using%20character-like%20personas.%0ABuilt%20upon%20the%20foundations%20of%20persona-driven%20dialog%20agents%2C%20this%20work%20extends%0Athe%20agent%27s%20application%20to%20the%20physical%20realm%2C%20employing%20robots%20to%20provide%20a%0Amore%20captivating%20and%20interactive%20experience.%20The%20proposed%20system%2C%20named%20the%0AMasquerading%20Animated%20Social%20Kinematic%20%28MASK%29%2C%20leverages%20an%20anthropomorphic%0Arobot%20which%20interacts%20with%20guests%20using%20non-verbal%20interactions%2C%20including%0Afacial%20expressions%20and%20gestures.%20A%20behavior%20generation%20system%20based%20upon%20a%0Afinite-state%20machine%20structure%20effectively%20conditions%20robotic%20behavior%20to%0Aconvey%20distinct%20personas.%20The%20MASK%20framework%20integrates%20a%20perception%20engine%2C%20a%0Abehavior%20selection%20engine%2C%20and%20a%20comprehensive%20action%20library%20to%20enable%0Areal-time%2C%20dynamic%20interactions%20with%20minimal%20human%20intervention%20in%20behavior%0Adesign.%20Throughout%20the%20user%20subject%20studies%2C%20we%20examined%20whether%20the%20users%0Acould%20recognize%20the%20intended%20character%20in%20both%20personality-%20and%0Afilm-character-based%20persona%20conditions.%20We%20conclude%20by%20discussing%20the%20role%20of%0Apersonas%20in%20interactive%20agents%20and%20the%20factors%20to%20consider%20for%20creating%20an%0Aengaging%20user%20experience.%0A&entry.1838667208=http%3A//arxiv.org/abs/2403.10041v2&entry.124074799=Read"},
{"title": "Towards a Modern and Lightweight Rendering Engine for Dynamic Robotic\n  Simulations", "author": "Christopher John Allison and Haoying Zhou and Adnan Munawar and Peter Kazanzides and Juan Antonio Barragan", "abstract": "  Interactive dynamic simulators are an accelerator for developing novel\nrobotic control algorithms and complex systems involving humans and robots. In\nuser training and synthetic data generation applications, a high-fidelity\nvisualization of the simulation is essential. Visual fidelity is dependent on\nthe quality of the computer graphics algorithms used to render the simulated\nscene. Furthermore, the rendering algorithms must be implemented on the\ngraphics processing unit (GPU) to achieve real-time performance, requiring the\nuse of a graphics application programming interface (API). This paper presents\na performance-focused and lightweight rendering engine supporting the Vulkan\ngraphics API. The engine is designed to modernize the legacy rendering pipeline\nof Asynchronous Multi-Body Framework (AMBF), a dynamic simulation framework\nused extensively for interactive robotics simulation development. This new\nrendering engine implements graphical features such as physically based\nrendering (PBR), anti-aliasing, and ray-traced shadows, significantly improving\nthe image quality of AMBF. Computational experiments show that the engine can\nrender a simulated scene with over seven million triangles while maintaining\nGPU computation times within two milliseconds.\n", "link": "http://arxiv.org/abs/2410.05095v1", "date": "2024-10-07", "relevancy": 2.1341, "topK": [{"title": "PhysGen: Rigid-Body Physics-Grounded Image-to-Video Generation", "link": "http://arxiv.org/abs/2409.18964v1", "similarity": 0.553}, {"title": "Dynamic Gaussians Mesh: Consistent Mesh Reconstruction from Monocular\n  Videos", "link": "http://arxiv.org/abs/2404.12379v2", "similarity": 0.523}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.5112}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Towards%20a%20Modern%20and%20Lightweight%20Rendering%20Engine%20for%20Dynamic%20Robotic%0A%20%20Simulations&body=Title%3A%20Towards%20a%20Modern%20and%20Lightweight%20Rendering%20Engine%20for%20Dynamic%20Robotic%0A%20%20Simulations%0AAuthor%3A%20Christopher%20John%20Allison%20and%20Haoying%20Zhou%20and%20Adnan%20Munawar%20and%20Peter%20Kazanzides%20and%20Juan%20Antonio%20Barragan%0AAbstract%3A%20%20%20Interactive%20dynamic%20simulators%20are%20an%20accelerator%20for%20developing%20novel%0Arobotic%20control%20algorithms%20and%20complex%20systems%20involving%20humans%20and%20robots.%20In%0Auser%20training%20and%20synthetic%20data%20generation%20applications%2C%20a%20high-fidelity%0Avisualization%20of%20the%20simulation%20is%20essential.%20Visual%20fidelity%20is%20dependent%20on%0Athe%20quality%20of%20the%20computer%20graphics%20algorithms%20used%20to%20render%20the%20simulated%0Ascene.%20Furthermore%2C%20the%20rendering%20algorithms%20must%20be%20implemented%20on%20the%0Agraphics%20processing%20unit%20%28GPU%29%20to%20achieve%20real-time%20performance%2C%20requiring%20the%0Ause%20of%20a%20graphics%20application%20programming%20interface%20%28API%29.%20This%20paper%20presents%0Aa%20performance-focused%20and%20lightweight%20rendering%20engine%20supporting%20the%20Vulkan%0Agraphics%20API.%20The%20engine%20is%20designed%20to%20modernize%20the%20legacy%20rendering%20pipeline%0Aof%20Asynchronous%20Multi-Body%20Framework%20%28AMBF%29%2C%20a%20dynamic%20simulation%20framework%0Aused%20extensively%20for%20interactive%20robotics%20simulation%20development.%20This%20new%0Arendering%20engine%20implements%20graphical%20features%20such%20as%20physically%20based%0Arendering%20%28PBR%29%2C%20anti-aliasing%2C%20and%20ray-traced%20shadows%2C%20significantly%20improving%0Athe%20image%20quality%20of%20AMBF.%20Computational%20experiments%20show%20that%20the%20engine%20can%0Arender%20a%20simulated%20scene%20with%20over%20seven%20million%20triangles%20while%20maintaining%0AGPU%20computation%20times%20within%20two%20milliseconds.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2410.05095v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DTowards%2520a%2520Modern%2520and%2520Lightweight%2520Rendering%2520Engine%2520for%2520Dynamic%2520Robotic%250A%2520%2520Simulations%26entry.906535625%3DChristopher%2520John%2520Allison%2520and%2520Haoying%2520Zhou%2520and%2520Adnan%2520Munawar%2520and%2520Peter%2520Kazanzides%2520and%2520Juan%2520Antonio%2520Barragan%26entry.1292438233%3D%2520%2520Interactive%2520dynamic%2520simulators%2520are%2520an%2520accelerator%2520for%2520developing%2520novel%250Arobotic%2520control%2520algorithms%2520and%2520complex%2520systems%2520involving%2520humans%2520and%2520robots.%2520In%250Auser%2520training%2520and%2520synthetic%2520data%2520generation%2520applications%252C%2520a%2520high-fidelity%250Avisualization%2520of%2520the%2520simulation%2520is%2520essential.%2520Visual%2520fidelity%2520is%2520dependent%2520on%250Athe%2520quality%2520of%2520the%2520computer%2520graphics%2520algorithms%2520used%2520to%2520render%2520the%2520simulated%250Ascene.%2520Furthermore%252C%2520the%2520rendering%2520algorithms%2520must%2520be%2520implemented%2520on%2520the%250Agraphics%2520processing%2520unit%2520%2528GPU%2529%2520to%2520achieve%2520real-time%2520performance%252C%2520requiring%2520the%250Ause%2520of%2520a%2520graphics%2520application%2520programming%2520interface%2520%2528API%2529.%2520This%2520paper%2520presents%250Aa%2520performance-focused%2520and%2520lightweight%2520rendering%2520engine%2520supporting%2520the%2520Vulkan%250Agraphics%2520API.%2520The%2520engine%2520is%2520designed%2520to%2520modernize%2520the%2520legacy%2520rendering%2520pipeline%250Aof%2520Asynchronous%2520Multi-Body%2520Framework%2520%2528AMBF%2529%252C%2520a%2520dynamic%2520simulation%2520framework%250Aused%2520extensively%2520for%2520interactive%2520robotics%2520simulation%2520development.%2520This%2520new%250Arendering%2520engine%2520implements%2520graphical%2520features%2520such%2520as%2520physically%2520based%250Arendering%2520%2528PBR%2529%252C%2520anti-aliasing%252C%2520and%2520ray-traced%2520shadows%252C%2520significantly%2520improving%250Athe%2520image%2520quality%2520of%2520AMBF.%2520Computational%2520experiments%2520show%2520that%2520the%2520engine%2520can%250Arender%2520a%2520simulated%2520scene%2520with%2520over%2520seven%2520million%2520triangles%2520while%2520maintaining%250AGPU%2520computation%2520times%2520within%2520two%2520milliseconds.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2410.05095v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Towards%20a%20Modern%20and%20Lightweight%20Rendering%20Engine%20for%20Dynamic%20Robotic%0A%20%20Simulations&entry.906535625=Christopher%20John%20Allison%20and%20Haoying%20Zhou%20and%20Adnan%20Munawar%20and%20Peter%20Kazanzides%20and%20Juan%20Antonio%20Barragan&entry.1292438233=%20%20Interactive%20dynamic%20simulators%20are%20an%20accelerator%20for%20developing%20novel%0Arobotic%20control%20algorithms%20and%20complex%20systems%20involving%20humans%20and%20robots.%20In%0Auser%20training%20and%20synthetic%20data%20generation%20applications%2C%20a%20high-fidelity%0Avisualization%20of%20the%20simulation%20is%20essential.%20Visual%20fidelity%20is%20dependent%20on%0Athe%20quality%20of%20the%20computer%20graphics%20algorithms%20used%20to%20render%20the%20simulated%0Ascene.%20Furthermore%2C%20the%20rendering%20algorithms%20must%20be%20implemented%20on%20the%0Agraphics%20processing%20unit%20%28GPU%29%20to%20achieve%20real-time%20performance%2C%20requiring%20the%0Ause%20of%20a%20graphics%20application%20programming%20interface%20%28API%29.%20This%20paper%20presents%0Aa%20performance-focused%20and%20lightweight%20rendering%20engine%20supporting%20the%20Vulkan%0Agraphics%20API.%20The%20engine%20is%20designed%20to%20modernize%20the%20legacy%20rendering%20pipeline%0Aof%20Asynchronous%20Multi-Body%20Framework%20%28AMBF%29%2C%20a%20dynamic%20simulation%20framework%0Aused%20extensively%20for%20interactive%20robotics%20simulation%20development.%20This%20new%0Arendering%20engine%20implements%20graphical%20features%20such%20as%20physically%20based%0Arendering%20%28PBR%29%2C%20anti-aliasing%2C%20and%20ray-traced%20shadows%2C%20significantly%20improving%0Athe%20image%20quality%20of%20AMBF.%20Computational%20experiments%20show%20that%20the%20engine%20can%0Arender%20a%20simulated%20scene%20with%20over%20seven%20million%20triangles%20while%20maintaining%0AGPU%20computation%20times%20within%20two%20milliseconds.%0A&entry.1838667208=http%3A//arxiv.org/abs/2410.05095v1&entry.124074799=Read"},
{"title": "Preventing Collapse in Contrastive Learning with Orthonormal Prototypes\n  (CLOP)", "author": "Huanran Li and Manh Nguyen and Daniel Pimentel-Alarc\u00f3n", "abstract": "  Contrastive learning has emerged as a powerful method in deep learning,\nexcelling at learning effective representations through contrasting samples\nfrom different distributions. However, neural collapse, where embeddings\nconverge into a lower-dimensional space, poses a significant challenge,\nespecially in semi-supervised and self-supervised setups. In this paper, we\nfirst theoretically analyze the effect of large learning rates on contrastive\nlosses that solely rely on the cosine similarity metric, and derive a\ntheoretical bound to mitigate this collapse. {Building on these insights, we\npropose CLOP, a novel semi-supervised loss function designed to prevent neural\ncollapse by promoting the formation of orthogonal linear subspaces among class\nembeddings.} Unlike prior approaches that enforce a simplex ETF structure, CLOP\nfocuses on subspace separation, leading to more distinguishable embeddings.\nThrough extensive experiments on real and synthetic datasets, we demonstrate\nthat CLOP enhances performance, providing greater stability across different\nlearning rates and batch sizes.\n", "link": "http://arxiv.org/abs/2403.18699v2", "date": "2024-10-07", "relevancy": 2.1331, "topK": [{"title": "Self-supervised Photographic Image Layout Representation Learning", "link": "http://arxiv.org/abs/2403.03740v1", "similarity": 0.5563}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5294}, {"title": "Thinking Outside the BBox: Unconstrained Generative Object Compositing", "link": "http://arxiv.org/abs/2409.04559v2", "similarity": 0.4854}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Preventing%20Collapse%20in%20Contrastive%20Learning%20with%20Orthonormal%20Prototypes%0A%20%20%28CLOP%29&body=Title%3A%20Preventing%20Collapse%20in%20Contrastive%20Learning%20with%20Orthonormal%20Prototypes%0A%20%20%28CLOP%29%0AAuthor%3A%20Huanran%20Li%20and%20Manh%20Nguyen%20and%20Daniel%20Pimentel-Alarc%C3%B3n%0AAbstract%3A%20%20%20Contrastive%20learning%20has%20emerged%20as%20a%20powerful%20method%20in%20deep%20learning%2C%0Aexcelling%20at%20learning%20effective%20representations%20through%20contrasting%20samples%0Afrom%20different%20distributions.%20However%2C%20neural%20collapse%2C%20where%20embeddings%0Aconverge%20into%20a%20lower-dimensional%20space%2C%20poses%20a%20significant%20challenge%2C%0Aespecially%20in%20semi-supervised%20and%20self-supervised%20setups.%20In%20this%20paper%2C%20we%0Afirst%20theoretically%20analyze%20the%20effect%20of%20large%20learning%20rates%20on%20contrastive%0Alosses%20that%20solely%20rely%20on%20the%20cosine%20similarity%20metric%2C%20and%20derive%20a%0Atheoretical%20bound%20to%20mitigate%20this%20collapse.%20%7BBuilding%20on%20these%20insights%2C%20we%0Apropose%20CLOP%2C%20a%20novel%20semi-supervised%20loss%20function%20designed%20to%20prevent%20neural%0Acollapse%20by%20promoting%20the%20formation%20of%20orthogonal%20linear%20subspaces%20among%20class%0Aembeddings.%7D%20Unlike%20prior%20approaches%20that%20enforce%20a%20simplex%20ETF%20structure%2C%20CLOP%0Afocuses%20on%20subspace%20separation%2C%20leading%20to%20more%20distinguishable%20embeddings.%0AThrough%20extensive%20experiments%20on%20real%20and%20synthetic%20datasets%2C%20we%20demonstrate%0Athat%20CLOP%20enhances%20performance%2C%20providing%20greater%20stability%20across%20different%0Alearning%20rates%20and%20batch%20sizes.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2403.18699v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DPreventing%2520Collapse%2520in%2520Contrastive%2520Learning%2520with%2520Orthonormal%2520Prototypes%250A%2520%2520%2528CLOP%2529%26entry.906535625%3DHuanran%2520Li%2520and%2520Manh%2520Nguyen%2520and%2520Daniel%2520Pimentel-Alarc%25C3%25B3n%26entry.1292438233%3D%2520%2520Contrastive%2520learning%2520has%2520emerged%2520as%2520a%2520powerful%2520method%2520in%2520deep%2520learning%252C%250Aexcelling%2520at%2520learning%2520effective%2520representations%2520through%2520contrasting%2520samples%250Afrom%2520different%2520distributions.%2520However%252C%2520neural%2520collapse%252C%2520where%2520embeddings%250Aconverge%2520into%2520a%2520lower-dimensional%2520space%252C%2520poses%2520a%2520significant%2520challenge%252C%250Aespecially%2520in%2520semi-supervised%2520and%2520self-supervised%2520setups.%2520In%2520this%2520paper%252C%2520we%250Afirst%2520theoretically%2520analyze%2520the%2520effect%2520of%2520large%2520learning%2520rates%2520on%2520contrastive%250Alosses%2520that%2520solely%2520rely%2520on%2520the%2520cosine%2520similarity%2520metric%252C%2520and%2520derive%2520a%250Atheoretical%2520bound%2520to%2520mitigate%2520this%2520collapse.%2520%257BBuilding%2520on%2520these%2520insights%252C%2520we%250Apropose%2520CLOP%252C%2520a%2520novel%2520semi-supervised%2520loss%2520function%2520designed%2520to%2520prevent%2520neural%250Acollapse%2520by%2520promoting%2520the%2520formation%2520of%2520orthogonal%2520linear%2520subspaces%2520among%2520class%250Aembeddings.%257D%2520Unlike%2520prior%2520approaches%2520that%2520enforce%2520a%2520simplex%2520ETF%2520structure%252C%2520CLOP%250Afocuses%2520on%2520subspace%2520separation%252C%2520leading%2520to%2520more%2520distinguishable%2520embeddings.%250AThrough%2520extensive%2520experiments%2520on%2520real%2520and%2520synthetic%2520datasets%252C%2520we%2520demonstrate%250Athat%2520CLOP%2520enhances%2520performance%252C%2520providing%2520greater%2520stability%2520across%2520different%250Alearning%2520rates%2520and%2520batch%2520sizes.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2403.18699v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Preventing%20Collapse%20in%20Contrastive%20Learning%20with%20Orthonormal%20Prototypes%0A%20%20%28CLOP%29&entry.906535625=Huanran%20Li%20and%20Manh%20Nguyen%20and%20Daniel%20Pimentel-Alarc%C3%B3n&entry.1292438233=%20%20Contrastive%20learning%20has%20emerged%20as%20a%20powerful%20method%20in%20deep%20learning%2C%0Aexcelling%20at%20learning%20effective%20representations%20through%20contrasting%20samples%0Afrom%20different%20distributions.%20However%2C%20neural%20collapse%2C%20where%20embeddings%0Aconverge%20into%20a%20lower-dimensional%20space%2C%20poses%20a%20significant%20challenge%2C%0Aespecially%20in%20semi-supervised%20and%20self-supervised%20setups.%20In%20this%20paper%2C%20we%0Afirst%20theoretically%20analyze%20the%20effect%20of%20large%20learning%20rates%20on%20contrastive%0Alosses%20that%20solely%20rely%20on%20the%20cosine%20similarity%20metric%2C%20and%20derive%20a%0Atheoretical%20bound%20to%20mitigate%20this%20collapse.%20%7BBuilding%20on%20these%20insights%2C%20we%0Apropose%20CLOP%2C%20a%20novel%20semi-supervised%20loss%20function%20designed%20to%20prevent%20neural%0Acollapse%20by%20promoting%20the%20formation%20of%20orthogonal%20linear%20subspaces%20among%20class%0Aembeddings.%7D%20Unlike%20prior%20approaches%20that%20enforce%20a%20simplex%20ETF%20structure%2C%20CLOP%0Afocuses%20on%20subspace%20separation%2C%20leading%20to%20more%20distinguishable%20embeddings.%0AThrough%20extensive%20experiments%20on%20real%20and%20synthetic%20datasets%2C%20we%20demonstrate%0Athat%20CLOP%20enhances%20performance%2C%20providing%20greater%20stability%20across%20different%0Alearning%20rates%20and%20batch%20sizes.%0A&entry.1838667208=http%3A//arxiv.org/abs/2403.18699v2&entry.124074799=Read"},
{"title": "MFE-ETP: A Comprehensive Evaluation Benchmark for Multi-modal Foundation\n  Models on Embodied Task Planning", "author": "Min Zhang and Xian Fu and Jianye Hao and Peilong Han and Hao Zhang and Lei Shi and Hongyao Tang and Yan Zheng", "abstract": "  In recent years, Multi-modal Foundation Models (MFMs) and Embodied Artificial\nIntelligence (EAI) have been advancing side by side at an unprecedented pace.\nThe integration of the two has garnered significant attention from the AI\nresearch community. In this work, we attempt to provide an in-depth and\ncomprehensive evaluation of the performance of MFM s on embodied task planning,\naiming to shed light on their capabilities and limitations in this domain. To\nthis end, based on the characteristics of embodied task planning, we first\ndevelop a systematic evaluation framework, which encapsulates four crucial\ncapabilities of MFMs: object understanding, spatio-temporal perception, task\nunderstanding, and embodied reasoning. Following this, we propose a new\nbenchmark, named MFE-ETP, characterized its complex and variable task\nscenarios, typical yet diverse task types, task instances of varying\ndifficulties, and rich test case types ranging from multiple embodied question\nanswering to embodied task reasoning. Finally, we offer a simple and\neasy-to-use automatic evaluation platform that enables the automated testing of\nmultiple MFMs on the proposed benchmark. Using the benchmark and evaluation\nplatform, we evaluated several state-of-the-art MFMs and found that they\nsignificantly lag behind human-level performance. The MFE-ETP is a\nhigh-quality, large-scale, and challenging benchmark relevant to real-world\ntasks.\n", "link": "http://arxiv.org/abs/2407.05047v3", "date": "2024-10-07", "relevancy": 2.1183, "topK": [{"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.539}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5277}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5277}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20MFE-ETP%3A%20A%20Comprehensive%20Evaluation%20Benchmark%20for%20Multi-modal%20Foundation%0A%20%20Models%20on%20Embodied%20Task%20Planning&body=Title%3A%20MFE-ETP%3A%20A%20Comprehensive%20Evaluation%20Benchmark%20for%20Multi-modal%20Foundation%0A%20%20Models%20on%20Embodied%20Task%20Planning%0AAuthor%3A%20Min%20Zhang%20and%20Xian%20Fu%20and%20Jianye%20Hao%20and%20Peilong%20Han%20and%20Hao%20Zhang%20and%20Lei%20Shi%20and%20Hongyao%20Tang%20and%20Yan%20Zheng%0AAbstract%3A%20%20%20In%20recent%20years%2C%20Multi-modal%20Foundation%20Models%20%28MFMs%29%20and%20Embodied%20Artificial%0AIntelligence%20%28EAI%29%20have%20been%20advancing%20side%20by%20side%20at%20an%20unprecedented%20pace.%0AThe%20integration%20of%20the%20two%20has%20garnered%20significant%20attention%20from%20the%20AI%0Aresearch%20community.%20In%20this%20work%2C%20we%20attempt%20to%20provide%20an%20in-depth%20and%0Acomprehensive%20evaluation%20of%20the%20performance%20of%20MFM%20s%20on%20embodied%20task%20planning%2C%0Aaiming%20to%20shed%20light%20on%20their%20capabilities%20and%20limitations%20in%20this%20domain.%20To%0Athis%20end%2C%20based%20on%20the%20characteristics%20of%20embodied%20task%20planning%2C%20we%20first%0Adevelop%20a%20systematic%20evaluation%20framework%2C%20which%20encapsulates%20four%20crucial%0Acapabilities%20of%20MFMs%3A%20object%20understanding%2C%20spatio-temporal%20perception%2C%20task%0Aunderstanding%2C%20and%20embodied%20reasoning.%20Following%20this%2C%20we%20propose%20a%20new%0Abenchmark%2C%20named%20MFE-ETP%2C%20characterized%20its%20complex%20and%20variable%20task%0Ascenarios%2C%20typical%20yet%20diverse%20task%20types%2C%20task%20instances%20of%20varying%0Adifficulties%2C%20and%20rich%20test%20case%20types%20ranging%20from%20multiple%20embodied%20question%0Aanswering%20to%20embodied%20task%20reasoning.%20Finally%2C%20we%20offer%20a%20simple%20and%0Aeasy-to-use%20automatic%20evaluation%20platform%20that%20enables%20the%20automated%20testing%20of%0Amultiple%20MFMs%20on%20the%20proposed%20benchmark.%20Using%20the%20benchmark%20and%20evaluation%0Aplatform%2C%20we%20evaluated%20several%20state-of-the-art%20MFMs%20and%20found%20that%20they%0Asignificantly%20lag%20behind%20human-level%20performance.%20The%20MFE-ETP%20is%20a%0Ahigh-quality%2C%20large-scale%2C%20and%20challenging%20benchmark%20relevant%20to%20real-world%0Atasks.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2407.05047v3%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DMFE-ETP%253A%2520A%2520Comprehensive%2520Evaluation%2520Benchmark%2520for%2520Multi-modal%2520Foundation%250A%2520%2520Models%2520on%2520Embodied%2520Task%2520Planning%26entry.906535625%3DMin%2520Zhang%2520and%2520Xian%2520Fu%2520and%2520Jianye%2520Hao%2520and%2520Peilong%2520Han%2520and%2520Hao%2520Zhang%2520and%2520Lei%2520Shi%2520and%2520Hongyao%2520Tang%2520and%2520Yan%2520Zheng%26entry.1292438233%3D%2520%2520In%2520recent%2520years%252C%2520Multi-modal%2520Foundation%2520Models%2520%2528MFMs%2529%2520and%2520Embodied%2520Artificial%250AIntelligence%2520%2528EAI%2529%2520have%2520been%2520advancing%2520side%2520by%2520side%2520at%2520an%2520unprecedented%2520pace.%250AThe%2520integration%2520of%2520the%2520two%2520has%2520garnered%2520significant%2520attention%2520from%2520the%2520AI%250Aresearch%2520community.%2520In%2520this%2520work%252C%2520we%2520attempt%2520to%2520provide%2520an%2520in-depth%2520and%250Acomprehensive%2520evaluation%2520of%2520the%2520performance%2520of%2520MFM%2520s%2520on%2520embodied%2520task%2520planning%252C%250Aaiming%2520to%2520shed%2520light%2520on%2520their%2520capabilities%2520and%2520limitations%2520in%2520this%2520domain.%2520To%250Athis%2520end%252C%2520based%2520on%2520the%2520characteristics%2520of%2520embodied%2520task%2520planning%252C%2520we%2520first%250Adevelop%2520a%2520systematic%2520evaluation%2520framework%252C%2520which%2520encapsulates%2520four%2520crucial%250Acapabilities%2520of%2520MFMs%253A%2520object%2520understanding%252C%2520spatio-temporal%2520perception%252C%2520task%250Aunderstanding%252C%2520and%2520embodied%2520reasoning.%2520Following%2520this%252C%2520we%2520propose%2520a%2520new%250Abenchmark%252C%2520named%2520MFE-ETP%252C%2520characterized%2520its%2520complex%2520and%2520variable%2520task%250Ascenarios%252C%2520typical%2520yet%2520diverse%2520task%2520types%252C%2520task%2520instances%2520of%2520varying%250Adifficulties%252C%2520and%2520rich%2520test%2520case%2520types%2520ranging%2520from%2520multiple%2520embodied%2520question%250Aanswering%2520to%2520embodied%2520task%2520reasoning.%2520Finally%252C%2520we%2520offer%2520a%2520simple%2520and%250Aeasy-to-use%2520automatic%2520evaluation%2520platform%2520that%2520enables%2520the%2520automated%2520testing%2520of%250Amultiple%2520MFMs%2520on%2520the%2520proposed%2520benchmark.%2520Using%2520the%2520benchmark%2520and%2520evaluation%250Aplatform%252C%2520we%2520evaluated%2520several%2520state-of-the-art%2520MFMs%2520and%2520found%2520that%2520they%250Asignificantly%2520lag%2520behind%2520human-level%2520performance.%2520The%2520MFE-ETP%2520is%2520a%250Ahigh-quality%252C%2520large-scale%252C%2520and%2520challenging%2520benchmark%2520relevant%2520to%2520real-world%250Atasks.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2407.05047v3%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=MFE-ETP%3A%20A%20Comprehensive%20Evaluation%20Benchmark%20for%20Multi-modal%20Foundation%0A%20%20Models%20on%20Embodied%20Task%20Planning&entry.906535625=Min%20Zhang%20and%20Xian%20Fu%20and%20Jianye%20Hao%20and%20Peilong%20Han%20and%20Hao%20Zhang%20and%20Lei%20Shi%20and%20Hongyao%20Tang%20and%20Yan%20Zheng&entry.1292438233=%20%20In%20recent%20years%2C%20Multi-modal%20Foundation%20Models%20%28MFMs%29%20and%20Embodied%20Artificial%0AIntelligence%20%28EAI%29%20have%20been%20advancing%20side%20by%20side%20at%20an%20unprecedented%20pace.%0AThe%20integration%20of%20the%20two%20has%20garnered%20significant%20attention%20from%20the%20AI%0Aresearch%20community.%20In%20this%20work%2C%20we%20attempt%20to%20provide%20an%20in-depth%20and%0Acomprehensive%20evaluation%20of%20the%20performance%20of%20MFM%20s%20on%20embodied%20task%20planning%2C%0Aaiming%20to%20shed%20light%20on%20their%20capabilities%20and%20limitations%20in%20this%20domain.%20To%0Athis%20end%2C%20based%20on%20the%20characteristics%20of%20embodied%20task%20planning%2C%20we%20first%0Adevelop%20a%20systematic%20evaluation%20framework%2C%20which%20encapsulates%20four%20crucial%0Acapabilities%20of%20MFMs%3A%20object%20understanding%2C%20spatio-temporal%20perception%2C%20task%0Aunderstanding%2C%20and%20embodied%20reasoning.%20Following%20this%2C%20we%20propose%20a%20new%0Abenchmark%2C%20named%20MFE-ETP%2C%20characterized%20its%20complex%20and%20variable%20task%0Ascenarios%2C%20typical%20yet%20diverse%20task%20types%2C%20task%20instances%20of%20varying%0Adifficulties%2C%20and%20rich%20test%20case%20types%20ranging%20from%20multiple%20embodied%20question%0Aanswering%20to%20embodied%20task%20reasoning.%20Finally%2C%20we%20offer%20a%20simple%20and%0Aeasy-to-use%20automatic%20evaluation%20platform%20that%20enables%20the%20automated%20testing%20of%0Amultiple%20MFMs%20on%20the%20proposed%20benchmark.%20Using%20the%20benchmark%20and%20evaluation%0Aplatform%2C%20we%20evaluated%20several%20state-of-the-art%20MFMs%20and%20found%20that%20they%0Asignificantly%20lag%20behind%20human-level%20performance.%20The%20MFE-ETP%20is%20a%0Ahigh-quality%2C%20large-scale%2C%20and%20challenging%20benchmark%20relevant%20to%20real-world%0Atasks.%0A&entry.1838667208=http%3A//arxiv.org/abs/2407.05047v3&entry.124074799=Read"},
{"title": "StructuReiser: A Structure-preserving Video Stylization Method", "author": "Radim Spetlik and David Futschik and Daniel Sykora", "abstract": "  We introduce StructuReiser, a novel video-to-video translation method that\ntransforms input videos into stylized sequences using a set of user-provided\nkeyframes. Unlike existing approaches, StructuReiser maintains strict adherence\nto the structural elements of the target video, preserving the original\nidentity while seamlessly applying the desired stylistic transformations. This\nenables a level of control and consistency that was previously unattainable\nwith traditional text-driven or keyframe-based methods. Furthermore,\nStructuReiser supports real-time inference and custom keyframe editing, making\nit ideal for interactive applications and expanding the possibilities for\ncreative expression and video manipulation.\n", "link": "http://arxiv.org/abs/2409.15341v2", "date": "2024-10-07", "relevancy": 2.1102, "topK": [{"title": "Customize-A-Video: One-Shot Motion Customization of Text-to-Video\n  Diffusion Models", "link": "http://arxiv.org/abs/2402.14780v1", "similarity": 0.5676}, {"title": "PoseAnimate: Zero-shot high fidelity pose controllable character\n  animation", "link": "http://arxiv.org/abs/2404.13680v2", "similarity": 0.5346}, {"title": "Tuning-Free Noise Rectification for High Fidelity Image-to-Video\n  Generation", "link": "http://arxiv.org/abs/2403.02827v1", "similarity": 0.5045}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20StructuReiser%3A%20A%20Structure-preserving%20Video%20Stylization%20Method&body=Title%3A%20StructuReiser%3A%20A%20Structure-preserving%20Video%20Stylization%20Method%0AAuthor%3A%20Radim%20Spetlik%20and%20David%20Futschik%20and%20Daniel%20Sykora%0AAbstract%3A%20%20%20We%20introduce%20StructuReiser%2C%20a%20novel%20video-to-video%20translation%20method%20that%0Atransforms%20input%20videos%20into%20stylized%20sequences%20using%20a%20set%20of%20user-provided%0Akeyframes.%20Unlike%20existing%20approaches%2C%20StructuReiser%20maintains%20strict%20adherence%0Ato%20the%20structural%20elements%20of%20the%20target%20video%2C%20preserving%20the%20original%0Aidentity%20while%20seamlessly%20applying%20the%20desired%20stylistic%20transformations.%20This%0Aenables%20a%20level%20of%20control%20and%20consistency%20that%20was%20previously%20unattainable%0Awith%20traditional%20text-driven%20or%20keyframe-based%20methods.%20Furthermore%2C%0AStructuReiser%20supports%20real-time%20inference%20and%20custom%20keyframe%20editing%2C%20making%0Ait%20ideal%20for%20interactive%20applications%20and%20expanding%20the%20possibilities%20for%0Acreative%20expression%20and%20video%20manipulation.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2409.15341v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DStructuReiser%253A%2520A%2520Structure-preserving%2520Video%2520Stylization%2520Method%26entry.906535625%3DRadim%2520Spetlik%2520and%2520David%2520Futschik%2520and%2520Daniel%2520Sykora%26entry.1292438233%3D%2520%2520We%2520introduce%2520StructuReiser%252C%2520a%2520novel%2520video-to-video%2520translation%2520method%2520that%250Atransforms%2520input%2520videos%2520into%2520stylized%2520sequences%2520using%2520a%2520set%2520of%2520user-provided%250Akeyframes.%2520Unlike%2520existing%2520approaches%252C%2520StructuReiser%2520maintains%2520strict%2520adherence%250Ato%2520the%2520structural%2520elements%2520of%2520the%2520target%2520video%252C%2520preserving%2520the%2520original%250Aidentity%2520while%2520seamlessly%2520applying%2520the%2520desired%2520stylistic%2520transformations.%2520This%250Aenables%2520a%2520level%2520of%2520control%2520and%2520consistency%2520that%2520was%2520previously%2520unattainable%250Awith%2520traditional%2520text-driven%2520or%2520keyframe-based%2520methods.%2520Furthermore%252C%250AStructuReiser%2520supports%2520real-time%2520inference%2520and%2520custom%2520keyframe%2520editing%252C%2520making%250Ait%2520ideal%2520for%2520interactive%2520applications%2520and%2520expanding%2520the%2520possibilities%2520for%250Acreative%2520expression%2520and%2520video%2520manipulation.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2409.15341v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=StructuReiser%3A%20A%20Structure-preserving%20Video%20Stylization%20Method&entry.906535625=Radim%20Spetlik%20and%20David%20Futschik%20and%20Daniel%20Sykora&entry.1292438233=%20%20We%20introduce%20StructuReiser%2C%20a%20novel%20video-to-video%20translation%20method%20that%0Atransforms%20input%20videos%20into%20stylized%20sequences%20using%20a%20set%20of%20user-provided%0Akeyframes.%20Unlike%20existing%20approaches%2C%20StructuReiser%20maintains%20strict%20adherence%0Ato%20the%20structural%20elements%20of%20the%20target%20video%2C%20preserving%20the%20original%0Aidentity%20while%20seamlessly%20applying%20the%20desired%20stylistic%20transformations.%20This%0Aenables%20a%20level%20of%20control%20and%20consistency%20that%20was%20previously%20unattainable%0Awith%20traditional%20text-driven%20or%20keyframe-based%20methods.%20Furthermore%2C%0AStructuReiser%20supports%20real-time%20inference%20and%20custom%20keyframe%20editing%2C%20making%0Ait%20ideal%20for%20interactive%20applications%20and%20expanding%20the%20possibilities%20for%0Acreative%20expression%20and%20video%20manipulation.%0A&entry.1838667208=http%3A//arxiv.org/abs/2409.15341v2&entry.124074799=Read"},
{"title": "Proprioceptive State Estimation for Quadruped Robots using Invariant\n  Kalman Filtering and Scale-Variant Robust Cost Functions", "author": "Hilton Marques Souza Santana and Jo\u00e3o Carlos Virgolino Soares and Ylenia Nistic\u00f2 and Marco Antonio Meggiolaro and Claudio Semini", "abstract": "  Accurate state estimation is crucial for legged robot locomotion, as it\nprovides the necessary information to allow control and navigation. However, it\nis also challenging, especially in scenarios with uneven and slippery terrain.\nThis paper presents a new Invariant Extended Kalman filter for legged robot\nstate estimation using only proprioceptive sensors. We formulate the\nmethodology by combining recent advances in state estimation theory with the\nuse of robust cost functions in the measurement update. We tested our\nmethodology on quadruped robots through experiments and public datasets,\nshowing that we can obtain a pose drift up to 40% lower in trajectories\ncovering a distance of over 450m, in comparison with a state-of-the-art\nInvariant Extended Kalman filter.\n", "link": "http://arxiv.org/abs/2410.05256v1", "date": "2024-10-07", "relevancy": 2.1025, "topK": [{"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.6451}, {"title": "Egocentric Computer Vision for Hands-Free Robotic Wheelchair Navigation", "link": "https://link.springer.com/article/10.1007/s10846-023-01807-4", "similarity": 0.5082}, {"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.4953}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Proprioceptive%20State%20Estimation%20for%20Quadruped%20Robots%20using%20Invariant%0A%20%20Kalman%20Filtering%20and%20Scale-Variant%20Robust%20Cost%20Functions&body=Title%3A%20Proprioceptive%20State%20Estimation%20for%20Quadruped%20Robots%20using%20Invariant%0A%20%20Kalman%20Filtering%20and%20Scale-Variant%20Robust%20Cost%20Functions%0AAuthor%3A%20Hilton%20Marques%20Souza%20Santana%20and%20Jo%C3%A3o%20Carlos%20Virgolino%20Soares%20and%20Ylenia%20Nistic%C3%B2%20and%20Marco%20Antonio%20Meggiolaro%20and%20Claudio%20Semini%0AAbstract%3A%20%20%20Accurate%20state%20estimation%20is%20crucial%20for%20legged%20robot%20locomotion%2C%20as%20it%0Aprovides%20the%20necessary%20information%20to%20allow%20control%20and%20navigation.%20However%2C%20it%0Ais%20also%20challenging%2C%20especially%20in%20scenarios%20with%20uneven%20and%20slippery%20terrain.%0AThis%20paper%20presents%20a%20new%20Invariant%20Extended%20Kalman%20filter%20for%20legged%20robot%0Astate%20estimation%20using%20only%20proprioceptive%20sensors.%20We%20formulate%20the%0Amethodology%20by%20combining%20recent%20advances%20in%20state%20estimation%20theory%20with%20the%0Ause%20of%20robust%20cost%20functions%20in%20the%20measurement%20update.%20We%20tested%20our%0Amethodology%20on%20quadruped%20robots%20through%20experiments%20and%20public%20datasets%2C%0Ashowing%20that%20we%20can%20obtain%20a%20pose%20drift%20up%20to%2040%25%20lower%20in%20trajectories%0Acovering%20a%20distance%20of%20over%20450m%2C%20in%20comparison%20with%20a%20state-of-the-art%0AInvariant%20Extended%20Kalman%20filter.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2410.05256v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DProprioceptive%2520State%2520Estimation%2520for%2520Quadruped%2520Robots%2520using%2520Invariant%250A%2520%2520Kalman%2520Filtering%2520and%2520Scale-Variant%2520Robust%2520Cost%2520Functions%26entry.906535625%3DHilton%2520Marques%2520Souza%2520Santana%2520and%2520Jo%25C3%25A3o%2520Carlos%2520Virgolino%2520Soares%2520and%2520Ylenia%2520Nistic%25C3%25B2%2520and%2520Marco%2520Antonio%2520Meggiolaro%2520and%2520Claudio%2520Semini%26entry.1292438233%3D%2520%2520Accurate%2520state%2520estimation%2520is%2520crucial%2520for%2520legged%2520robot%2520locomotion%252C%2520as%2520it%250Aprovides%2520the%2520necessary%2520information%2520to%2520allow%2520control%2520and%2520navigation.%2520However%252C%2520it%250Ais%2520also%2520challenging%252C%2520especially%2520in%2520scenarios%2520with%2520uneven%2520and%2520slippery%2520terrain.%250AThis%2520paper%2520presents%2520a%2520new%2520Invariant%2520Extended%2520Kalman%2520filter%2520for%2520legged%2520robot%250Astate%2520estimation%2520using%2520only%2520proprioceptive%2520sensors.%2520We%2520formulate%2520the%250Amethodology%2520by%2520combining%2520recent%2520advances%2520in%2520state%2520estimation%2520theory%2520with%2520the%250Ause%2520of%2520robust%2520cost%2520functions%2520in%2520the%2520measurement%2520update.%2520We%2520tested%2520our%250Amethodology%2520on%2520quadruped%2520robots%2520through%2520experiments%2520and%2520public%2520datasets%252C%250Ashowing%2520that%2520we%2520can%2520obtain%2520a%2520pose%2520drift%2520up%2520to%252040%2525%2520lower%2520in%2520trajectories%250Acovering%2520a%2520distance%2520of%2520over%2520450m%252C%2520in%2520comparison%2520with%2520a%2520state-of-the-art%250AInvariant%2520Extended%2520Kalman%2520filter.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2410.05256v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Proprioceptive%20State%20Estimation%20for%20Quadruped%20Robots%20using%20Invariant%0A%20%20Kalman%20Filtering%20and%20Scale-Variant%20Robust%20Cost%20Functions&entry.906535625=Hilton%20Marques%20Souza%20Santana%20and%20Jo%C3%A3o%20Carlos%20Virgolino%20Soares%20and%20Ylenia%20Nistic%C3%B2%20and%20Marco%20Antonio%20Meggiolaro%20and%20Claudio%20Semini&entry.1292438233=%20%20Accurate%20state%20estimation%20is%20crucial%20for%20legged%20robot%20locomotion%2C%20as%20it%0Aprovides%20the%20necessary%20information%20to%20allow%20control%20and%20navigation.%20However%2C%20it%0Ais%20also%20challenging%2C%20especially%20in%20scenarios%20with%20uneven%20and%20slippery%20terrain.%0AThis%20paper%20presents%20a%20new%20Invariant%20Extended%20Kalman%20filter%20for%20legged%20robot%0Astate%20estimation%20using%20only%20proprioceptive%20sensors.%20We%20formulate%20the%0Amethodology%20by%20combining%20recent%20advances%20in%20state%20estimation%20theory%20with%20the%0Ause%20of%20robust%20cost%20functions%20in%20the%20measurement%20update.%20We%20tested%20our%0Amethodology%20on%20quadruped%20robots%20through%20experiments%20and%20public%20datasets%2C%0Ashowing%20that%20we%20can%20obtain%20a%20pose%20drift%20up%20to%2040%25%20lower%20in%20trajectories%0Acovering%20a%20distance%20of%20over%20450m%2C%20in%20comparison%20with%20a%20state-of-the-art%0AInvariant%20Extended%20Kalman%20filter.%0A&entry.1838667208=http%3A//arxiv.org/abs/2410.05256v1&entry.124074799=Read"},
{"title": "DYNAMICQA: Tracing Internal Knowledge Conflicts in Language Models", "author": "Sara Vera Marjanovi\u0107 and Haeun Yu and Pepa Atanasova and Maria Maistro and Christina Lioma and Isabelle Augenstein", "abstract": "  Knowledge-intensive language understanding tasks require Language Models\n(LMs) to integrate relevant context, mitigating their inherent weaknesses, such\nas incomplete or outdated knowledge. However, conflicting knowledge can be\npresent in the LM's parameters, termed intra-memory conflict, which can affect\na model's propensity to accept contextual knowledge. To study the effect of\nintra-memory conflict on an LM's ability to accept relevant context, we utilize\ntwo knowledge conflict measures and a novel dataset containing inherently\nconflicting data, DynamicQA. This dataset includes facts with a temporal\ndynamic nature where facts can change over time and disputable dynamic facts,\nwhich can change depending on the viewpoint. DynamicQA is the first to include\nreal-world knowledge conflicts and provide context to study the link between\nthe different types of knowledge conflicts. We also evaluate several measures\non their ability to reflect the presence of intra-memory conflict: semantic\nentropy and a novel coherent persuasion score. With our extensive experiments,\nwe verify that LMs exhibit a greater degree of intra-memory conflict with\ndynamic facts compared to facts that have a single truth value. Furthermore, we\nreveal that facts with intra-memory conflict are harder to update with context,\nsuggesting that retrieval-augmented generation will struggle with the most\ncommonly adapted facts.\n", "link": "http://arxiv.org/abs/2407.17023v2", "date": "2024-10-07", "relevancy": 2.0862, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5251}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5251}, {"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.5038}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20DYNAMICQA%3A%20Tracing%20Internal%20Knowledge%20Conflicts%20in%20Language%20Models&body=Title%3A%20DYNAMICQA%3A%20Tracing%20Internal%20Knowledge%20Conflicts%20in%20Language%20Models%0AAuthor%3A%20Sara%20Vera%20Marjanovi%C4%87%20and%20Haeun%20Yu%20and%20Pepa%20Atanasova%20and%20Maria%20Maistro%20and%20Christina%20Lioma%20and%20Isabelle%20Augenstein%0AAbstract%3A%20%20%20Knowledge-intensive%20language%20understanding%20tasks%20require%20Language%20Models%0A%28LMs%29%20to%20integrate%20relevant%20context%2C%20mitigating%20their%20inherent%20weaknesses%2C%20such%0Aas%20incomplete%20or%20outdated%20knowledge.%20However%2C%20conflicting%20knowledge%20can%20be%0Apresent%20in%20the%20LM%27s%20parameters%2C%20termed%20intra-memory%20conflict%2C%20which%20can%20affect%0Aa%20model%27s%20propensity%20to%20accept%20contextual%20knowledge.%20To%20study%20the%20effect%20of%0Aintra-memory%20conflict%20on%20an%20LM%27s%20ability%20to%20accept%20relevant%20context%2C%20we%20utilize%0Atwo%20knowledge%20conflict%20measures%20and%20a%20novel%20dataset%20containing%20inherently%0Aconflicting%20data%2C%20DynamicQA.%20This%20dataset%20includes%20facts%20with%20a%20temporal%0Adynamic%20nature%20where%20facts%20can%20change%20over%20time%20and%20disputable%20dynamic%20facts%2C%0Awhich%20can%20change%20depending%20on%20the%20viewpoint.%20DynamicQA%20is%20the%20first%20to%20include%0Areal-world%20knowledge%20conflicts%20and%20provide%20context%20to%20study%20the%20link%20between%0Athe%20different%20types%20of%20knowledge%20conflicts.%20We%20also%20evaluate%20several%20measures%0Aon%20their%20ability%20to%20reflect%20the%20presence%20of%20intra-memory%20conflict%3A%20semantic%0Aentropy%20and%20a%20novel%20coherent%20persuasion%20score.%20With%20our%20extensive%20experiments%2C%0Awe%20verify%20that%20LMs%20exhibit%20a%20greater%20degree%20of%20intra-memory%20conflict%20with%0Adynamic%20facts%20compared%20to%20facts%20that%20have%20a%20single%20truth%20value.%20Furthermore%2C%20we%0Areveal%20that%20facts%20with%20intra-memory%20conflict%20are%20harder%20to%20update%20with%20context%2C%0Asuggesting%20that%20retrieval-augmented%20generation%20will%20struggle%20with%20the%20most%0Acommonly%20adapted%20facts.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2407.17023v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DDYNAMICQA%253A%2520Tracing%2520Internal%2520Knowledge%2520Conflicts%2520in%2520Language%2520Models%26entry.906535625%3DSara%2520Vera%2520Marjanovi%25C4%2587%2520and%2520Haeun%2520Yu%2520and%2520Pepa%2520Atanasova%2520and%2520Maria%2520Maistro%2520and%2520Christina%2520Lioma%2520and%2520Isabelle%2520Augenstein%26entry.1292438233%3D%2520%2520Knowledge-intensive%2520language%2520understanding%2520tasks%2520require%2520Language%2520Models%250A%2528LMs%2529%2520to%2520integrate%2520relevant%2520context%252C%2520mitigating%2520their%2520inherent%2520weaknesses%252C%2520such%250Aas%2520incomplete%2520or%2520outdated%2520knowledge.%2520However%252C%2520conflicting%2520knowledge%2520can%2520be%250Apresent%2520in%2520the%2520LM%2527s%2520parameters%252C%2520termed%2520intra-memory%2520conflict%252C%2520which%2520can%2520affect%250Aa%2520model%2527s%2520propensity%2520to%2520accept%2520contextual%2520knowledge.%2520To%2520study%2520the%2520effect%2520of%250Aintra-memory%2520conflict%2520on%2520an%2520LM%2527s%2520ability%2520to%2520accept%2520relevant%2520context%252C%2520we%2520utilize%250Atwo%2520knowledge%2520conflict%2520measures%2520and%2520a%2520novel%2520dataset%2520containing%2520inherently%250Aconflicting%2520data%252C%2520DynamicQA.%2520This%2520dataset%2520includes%2520facts%2520with%2520a%2520temporal%250Adynamic%2520nature%2520where%2520facts%2520can%2520change%2520over%2520time%2520and%2520disputable%2520dynamic%2520facts%252C%250Awhich%2520can%2520change%2520depending%2520on%2520the%2520viewpoint.%2520DynamicQA%2520is%2520the%2520first%2520to%2520include%250Areal-world%2520knowledge%2520conflicts%2520and%2520provide%2520context%2520to%2520study%2520the%2520link%2520between%250Athe%2520different%2520types%2520of%2520knowledge%2520conflicts.%2520We%2520also%2520evaluate%2520several%2520measures%250Aon%2520their%2520ability%2520to%2520reflect%2520the%2520presence%2520of%2520intra-memory%2520conflict%253A%2520semantic%250Aentropy%2520and%2520a%2520novel%2520coherent%2520persuasion%2520score.%2520With%2520our%2520extensive%2520experiments%252C%250Awe%2520verify%2520that%2520LMs%2520exhibit%2520a%2520greater%2520degree%2520of%2520intra-memory%2520conflict%2520with%250Adynamic%2520facts%2520compared%2520to%2520facts%2520that%2520have%2520a%2520single%2520truth%2520value.%2520Furthermore%252C%2520we%250Areveal%2520that%2520facts%2520with%2520intra-memory%2520conflict%2520are%2520harder%2520to%2520update%2520with%2520context%252C%250Asuggesting%2520that%2520retrieval-augmented%2520generation%2520will%2520struggle%2520with%2520the%2520most%250Acommonly%2520adapted%2520facts.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2407.17023v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=DYNAMICQA%3A%20Tracing%20Internal%20Knowledge%20Conflicts%20in%20Language%20Models&entry.906535625=Sara%20Vera%20Marjanovi%C4%87%20and%20Haeun%20Yu%20and%20Pepa%20Atanasova%20and%20Maria%20Maistro%20and%20Christina%20Lioma%20and%20Isabelle%20Augenstein&entry.1292438233=%20%20Knowledge-intensive%20language%20understanding%20tasks%20require%20Language%20Models%0A%28LMs%29%20to%20integrate%20relevant%20context%2C%20mitigating%20their%20inherent%20weaknesses%2C%20such%0Aas%20incomplete%20or%20outdated%20knowledge.%20However%2C%20conflicting%20knowledge%20can%20be%0Apresent%20in%20the%20LM%27s%20parameters%2C%20termed%20intra-memory%20conflict%2C%20which%20can%20affect%0Aa%20model%27s%20propensity%20to%20accept%20contextual%20knowledge.%20To%20study%20the%20effect%20of%0Aintra-memory%20conflict%20on%20an%20LM%27s%20ability%20to%20accept%20relevant%20context%2C%20we%20utilize%0Atwo%20knowledge%20conflict%20measures%20and%20a%20novel%20dataset%20containing%20inherently%0Aconflicting%20data%2C%20DynamicQA.%20This%20dataset%20includes%20facts%20with%20a%20temporal%0Adynamic%20nature%20where%20facts%20can%20change%20over%20time%20and%20disputable%20dynamic%20facts%2C%0Awhich%20can%20change%20depending%20on%20the%20viewpoint.%20DynamicQA%20is%20the%20first%20to%20include%0Areal-world%20knowledge%20conflicts%20and%20provide%20context%20to%20study%20the%20link%20between%0Athe%20different%20types%20of%20knowledge%20conflicts.%20We%20also%20evaluate%20several%20measures%0Aon%20their%20ability%20to%20reflect%20the%20presence%20of%20intra-memory%20conflict%3A%20semantic%0Aentropy%20and%20a%20novel%20coherent%20persuasion%20score.%20With%20our%20extensive%20experiments%2C%0Awe%20verify%20that%20LMs%20exhibit%20a%20greater%20degree%20of%20intra-memory%20conflict%20with%0Adynamic%20facts%20compared%20to%20facts%20that%20have%20a%20single%20truth%20value.%20Furthermore%2C%20we%0Areveal%20that%20facts%20with%20intra-memory%20conflict%20are%20harder%20to%20update%20with%20context%2C%0Asuggesting%20that%20retrieval-augmented%20generation%20will%20struggle%20with%20the%20most%0Acommonly%20adapted%20facts.%0A&entry.1838667208=http%3A//arxiv.org/abs/2407.17023v2&entry.124074799=Read"},
{"title": "Art2Mus: Bridging Visual Arts and Music through Cross-Modal Generation", "author": "Ivan Rinaldi and Nicola Fanelli and Giovanna Castellano and Gennaro Vessio", "abstract": "  Artificial Intelligence and generative models have revolutionized music\ncreation, with many models leveraging textual or visual prompts for guidance.\nHowever, existing image-to-music models are limited to simple images, lacking\nthe capability to generate music from complex digitized artworks. To address\nthis gap, we introduce $\\mathcal{A}\\textit{rt2}\\mathcal{M}\\textit{us}$, a novel\nmodel designed to create music from digitized artworks or text inputs.\n$\\mathcal{A}\\textit{rt2}\\mathcal{M}\\textit{us}$ extends the AudioLDM~2\narchitecture, a text-to-audio model, and employs our newly curated datasets,\ncreated via ImageBind, which pair digitized artworks with music. Experimental\nresults demonstrate that $\\mathcal{A}\\textit{rt2}\\mathcal{M}\\textit{us}$ can\ngenerate music that resonates with the input stimuli. These findings suggest\npromising applications in multimedia art, interactive installations, and\nAI-driven creative tools.\n", "link": "http://arxiv.org/abs/2410.04906v1", "date": "2024-10-07", "relevancy": 2.0699, "topK": [{"title": "Thinking Outside the BBox: Unconstrained Generative Object Compositing", "link": "http://arxiv.org/abs/2409.04559v2", "similarity": 0.5311}, {"title": "Magic-Me: Identity-Specific Video Customized Diffusion", "link": "http://arxiv.org/abs/2402.09368v1", "similarity": 0.5189}, {"title": "Genie: Generative Interactive Environments", "link": "http://arxiv.org/abs/2402.15391v1", "similarity": 0.5106}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Art2Mus%3A%20Bridging%20Visual%20Arts%20and%20Music%20through%20Cross-Modal%20Generation&body=Title%3A%20Art2Mus%3A%20Bridging%20Visual%20Arts%20and%20Music%20through%20Cross-Modal%20Generation%0AAuthor%3A%20Ivan%20Rinaldi%20and%20Nicola%20Fanelli%20and%20Giovanna%20Castellano%20and%20Gennaro%20Vessio%0AAbstract%3A%20%20%20Artificial%20Intelligence%20and%20generative%20models%20have%20revolutionized%20music%0Acreation%2C%20with%20many%20models%20leveraging%20textual%20or%20visual%20prompts%20for%20guidance.%0AHowever%2C%20existing%20image-to-music%20models%20are%20limited%20to%20simple%20images%2C%20lacking%0Athe%20capability%20to%20generate%20music%20from%20complex%20digitized%20artworks.%20To%20address%0Athis%20gap%2C%20we%20introduce%20%24%5Cmathcal%7BA%7D%5Ctextit%7Brt2%7D%5Cmathcal%7BM%7D%5Ctextit%7Bus%7D%24%2C%20a%20novel%0Amodel%20designed%20to%20create%20music%20from%20digitized%20artworks%20or%20text%20inputs.%0A%24%5Cmathcal%7BA%7D%5Ctextit%7Brt2%7D%5Cmathcal%7BM%7D%5Ctextit%7Bus%7D%24%20extends%20the%20AudioLDM~2%0Aarchitecture%2C%20a%20text-to-audio%20model%2C%20and%20employs%20our%20newly%20curated%20datasets%2C%0Acreated%20via%20ImageBind%2C%20which%20pair%20digitized%20artworks%20with%20music.%20Experimental%0Aresults%20demonstrate%20that%20%24%5Cmathcal%7BA%7D%5Ctextit%7Brt2%7D%5Cmathcal%7BM%7D%5Ctextit%7Bus%7D%24%20can%0Agenerate%20music%20that%20resonates%20with%20the%20input%20stimuli.%20These%20findings%20suggest%0Apromising%20applications%20in%20multimedia%20art%2C%20interactive%20installations%2C%20and%0AAI-driven%20creative%20tools.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2410.04906v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DArt2Mus%253A%2520Bridging%2520Visual%2520Arts%2520and%2520Music%2520through%2520Cross-Modal%2520Generation%26entry.906535625%3DIvan%2520Rinaldi%2520and%2520Nicola%2520Fanelli%2520and%2520Giovanna%2520Castellano%2520and%2520Gennaro%2520Vessio%26entry.1292438233%3D%2520%2520Artificial%2520Intelligence%2520and%2520generative%2520models%2520have%2520revolutionized%2520music%250Acreation%252C%2520with%2520many%2520models%2520leveraging%2520textual%2520or%2520visual%2520prompts%2520for%2520guidance.%250AHowever%252C%2520existing%2520image-to-music%2520models%2520are%2520limited%2520to%2520simple%2520images%252C%2520lacking%250Athe%2520capability%2520to%2520generate%2520music%2520from%2520complex%2520digitized%2520artworks.%2520To%2520address%250Athis%2520gap%252C%2520we%2520introduce%2520%2524%255Cmathcal%257BA%257D%255Ctextit%257Brt2%257D%255Cmathcal%257BM%257D%255Ctextit%257Bus%257D%2524%252C%2520a%2520novel%250Amodel%2520designed%2520to%2520create%2520music%2520from%2520digitized%2520artworks%2520or%2520text%2520inputs.%250A%2524%255Cmathcal%257BA%257D%255Ctextit%257Brt2%257D%255Cmathcal%257BM%257D%255Ctextit%257Bus%257D%2524%2520extends%2520the%2520AudioLDM~2%250Aarchitecture%252C%2520a%2520text-to-audio%2520model%252C%2520and%2520employs%2520our%2520newly%2520curated%2520datasets%252C%250Acreated%2520via%2520ImageBind%252C%2520which%2520pair%2520digitized%2520artworks%2520with%2520music.%2520Experimental%250Aresults%2520demonstrate%2520that%2520%2524%255Cmathcal%257BA%257D%255Ctextit%257Brt2%257D%255Cmathcal%257BM%257D%255Ctextit%257Bus%257D%2524%2520can%250Agenerate%2520music%2520that%2520resonates%2520with%2520the%2520input%2520stimuli.%2520These%2520findings%2520suggest%250Apromising%2520applications%2520in%2520multimedia%2520art%252C%2520interactive%2520installations%252C%2520and%250AAI-driven%2520creative%2520tools.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2410.04906v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Art2Mus%3A%20Bridging%20Visual%20Arts%20and%20Music%20through%20Cross-Modal%20Generation&entry.906535625=Ivan%20Rinaldi%20and%20Nicola%20Fanelli%20and%20Giovanna%20Castellano%20and%20Gennaro%20Vessio&entry.1292438233=%20%20Artificial%20Intelligence%20and%20generative%20models%20have%20revolutionized%20music%0Acreation%2C%20with%20many%20models%20leveraging%20textual%20or%20visual%20prompts%20for%20guidance.%0AHowever%2C%20existing%20image-to-music%20models%20are%20limited%20to%20simple%20images%2C%20lacking%0Athe%20capability%20to%20generate%20music%20from%20complex%20digitized%20artworks.%20To%20address%0Athis%20gap%2C%20we%20introduce%20%24%5Cmathcal%7BA%7D%5Ctextit%7Brt2%7D%5Cmathcal%7BM%7D%5Ctextit%7Bus%7D%24%2C%20a%20novel%0Amodel%20designed%20to%20create%20music%20from%20digitized%20artworks%20or%20text%20inputs.%0A%24%5Cmathcal%7BA%7D%5Ctextit%7Brt2%7D%5Cmathcal%7BM%7D%5Ctextit%7Bus%7D%24%20extends%20the%20AudioLDM~2%0Aarchitecture%2C%20a%20text-to-audio%20model%2C%20and%20employs%20our%20newly%20curated%20datasets%2C%0Acreated%20via%20ImageBind%2C%20which%20pair%20digitized%20artworks%20with%20music.%20Experimental%0Aresults%20demonstrate%20that%20%24%5Cmathcal%7BA%7D%5Ctextit%7Brt2%7D%5Cmathcal%7BM%7D%5Ctextit%7Bus%7D%24%20can%0Agenerate%20music%20that%20resonates%20with%20the%20input%20stimuli.%20These%20findings%20suggest%0Apromising%20applications%20in%20multimedia%20art%2C%20interactive%20installations%2C%20and%0AAI-driven%20creative%20tools.%0A&entry.1838667208=http%3A//arxiv.org/abs/2410.04906v1&entry.124074799=Read"},
{"title": "Autonomous Evaluation and Refinement of Digital Agents", "author": "Jiayi Pan and Yichi Zhang and Nicholas Tomlin and Yifei Zhou and Sergey Levine and Alane Suhr", "abstract": "  We show that domain-general automatic evaluators can significantly improve\nthe performance of agents for web navigation and device control. We experiment\nwith multiple evaluation models that trade off between inference cost,\nmodularity of design, and accuracy. We validate the performance of these models\nin several popular benchmarks for digital agents, finding between 74.4 and\n92.9% agreement with oracle evaluation metrics. Finally, we use these\nevaluators to improve the performance of existing agents via fine-tuning and\ninference-time guidance. Without any additional supervision, we improve\nstate-of-the-art performance by 29% on the popular benchmark WebArena, and\nachieve around 75% relative improvement in device control settings.\n", "link": "http://arxiv.org/abs/2404.06474v3", "date": "2024-10-07", "relevancy": 2.0689, "topK": [{"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.5328}, {"title": "Safe and efficient training of a control agent", "link": "https://patents.google.com/patent/US11709462B2/en", "similarity": 0.5196}, {"title": "Egocentric Computer Vision for Hands-Free Robotic Wheelchair Navigation", "link": "https://link.springer.com/article/10.1007/s10846-023-01807-4", "similarity": 0.5087}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Autonomous%20Evaluation%20and%20Refinement%20of%20Digital%20Agents&body=Title%3A%20Autonomous%20Evaluation%20and%20Refinement%20of%20Digital%20Agents%0AAuthor%3A%20Jiayi%20Pan%20and%20Yichi%20Zhang%20and%20Nicholas%20Tomlin%20and%20Yifei%20Zhou%20and%20Sergey%20Levine%20and%20Alane%20Suhr%0AAbstract%3A%20%20%20We%20show%20that%20domain-general%20automatic%20evaluators%20can%20significantly%20improve%0Athe%20performance%20of%20agents%20for%20web%20navigation%20and%20device%20control.%20We%20experiment%0Awith%20multiple%20evaluation%20models%20that%20trade%20off%20between%20inference%20cost%2C%0Amodularity%20of%20design%2C%20and%20accuracy.%20We%20validate%20the%20performance%20of%20these%20models%0Ain%20several%20popular%20benchmarks%20for%20digital%20agents%2C%20finding%20between%2074.4%20and%0A92.9%25%20agreement%20with%20oracle%20evaluation%20metrics.%20Finally%2C%20we%20use%20these%0Aevaluators%20to%20improve%20the%20performance%20of%20existing%20agents%20via%20fine-tuning%20and%0Ainference-time%20guidance.%20Without%20any%20additional%20supervision%2C%20we%20improve%0Astate-of-the-art%20performance%20by%2029%25%20on%20the%20popular%20benchmark%20WebArena%2C%20and%0Aachieve%20around%2075%25%20relative%20improvement%20in%20device%20control%20settings.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2404.06474v3%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DAutonomous%2520Evaluation%2520and%2520Refinement%2520of%2520Digital%2520Agents%26entry.906535625%3DJiayi%2520Pan%2520and%2520Yichi%2520Zhang%2520and%2520Nicholas%2520Tomlin%2520and%2520Yifei%2520Zhou%2520and%2520Sergey%2520Levine%2520and%2520Alane%2520Suhr%26entry.1292438233%3D%2520%2520We%2520show%2520that%2520domain-general%2520automatic%2520evaluators%2520can%2520significantly%2520improve%250Athe%2520performance%2520of%2520agents%2520for%2520web%2520navigation%2520and%2520device%2520control.%2520We%2520experiment%250Awith%2520multiple%2520evaluation%2520models%2520that%2520trade%2520off%2520between%2520inference%2520cost%252C%250Amodularity%2520of%2520design%252C%2520and%2520accuracy.%2520We%2520validate%2520the%2520performance%2520of%2520these%2520models%250Ain%2520several%2520popular%2520benchmarks%2520for%2520digital%2520agents%252C%2520finding%2520between%252074.4%2520and%250A92.9%2525%2520agreement%2520with%2520oracle%2520evaluation%2520metrics.%2520Finally%252C%2520we%2520use%2520these%250Aevaluators%2520to%2520improve%2520the%2520performance%2520of%2520existing%2520agents%2520via%2520fine-tuning%2520and%250Ainference-time%2520guidance.%2520Without%2520any%2520additional%2520supervision%252C%2520we%2520improve%250Astate-of-the-art%2520performance%2520by%252029%2525%2520on%2520the%2520popular%2520benchmark%2520WebArena%252C%2520and%250Aachieve%2520around%252075%2525%2520relative%2520improvement%2520in%2520device%2520control%2520settings.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2404.06474v3%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Autonomous%20Evaluation%20and%20Refinement%20of%20Digital%20Agents&entry.906535625=Jiayi%20Pan%20and%20Yichi%20Zhang%20and%20Nicholas%20Tomlin%20and%20Yifei%20Zhou%20and%20Sergey%20Levine%20and%20Alane%20Suhr&entry.1292438233=%20%20We%20show%20that%20domain-general%20automatic%20evaluators%20can%20significantly%20improve%0Athe%20performance%20of%20agents%20for%20web%20navigation%20and%20device%20control.%20We%20experiment%0Awith%20multiple%20evaluation%20models%20that%20trade%20off%20between%20inference%20cost%2C%0Amodularity%20of%20design%2C%20and%20accuracy.%20We%20validate%20the%20performance%20of%20these%20models%0Ain%20several%20popular%20benchmarks%20for%20digital%20agents%2C%20finding%20between%2074.4%20and%0A92.9%25%20agreement%20with%20oracle%20evaluation%20metrics.%20Finally%2C%20we%20use%20these%0Aevaluators%20to%20improve%20the%20performance%20of%20existing%20agents%20via%20fine-tuning%20and%0Ainference-time%20guidance.%20Without%20any%20additional%20supervision%2C%20we%20improve%0Astate-of-the-art%20performance%20by%2029%25%20on%20the%20popular%20benchmark%20WebArena%2C%20and%0Aachieve%20around%2075%25%20relative%20improvement%20in%20device%20control%20settings.%0A&entry.1838667208=http%3A//arxiv.org/abs/2404.06474v3&entry.124074799=Read"},
{"title": "Human-in-the-loop Reasoning For Traffic Sign Detection: Collaborative\n  Approach Yolo With Video-llava", "author": "Mehdi Azarafza and Fatima Idrees and Ali Ehteshami Bejnordi and Charles Steinmetz and Stefan Henkler and Achim Rettberg", "abstract": "  Traffic Sign Recognition (TSR) detection is a crucial component of autonomous\nvehicles. While You Only Look Once (YOLO) is a popular real-time object\ndetection algorithm, factors like training data quality and adverse weather\nconditions (e.g., heavy rain) can lead to detection failures. These failures\ncan be particularly dangerous when visual similarities between objects exist,\nsuch as mistaking a 30 km/h sign for a higher speed limit sign. This paper\nproposes a method that combines video analysis and reasoning, prompting with a\nhuman-in-the-loop guide large vision model to improve YOLOs accuracy in\ndetecting road speed limit signs, especially in semi-real-world conditions. It\nis hypothesized that the guided prompting and reasoning abilities of\nVideo-LLava can enhance YOLOs traffic sign detection capabilities. This\nhypothesis is supported by an evaluation based on human-annotated accuracy\nmetrics within a dataset of recorded videos from the CARLA car simulator. The\nresults demonstrate that a collaborative approach combining YOLO with\nVideo-LLava and reasoning can effectively address challenging situations such\nas heavy rain and overcast conditions that hinder YOLOs detection capabilities.\n", "link": "http://arxiv.org/abs/2410.05096v1", "date": "2024-10-07", "relevancy": 2.0685, "topK": [{"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.525}, {"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.5173}, {"title": "Flexible Visual Recognition by Evidential Modeling of Confusion and Ignorance", "link": "http://openaccess.thecvf.com/content/ICCV2023/html/Fan_Flexible_Visual_Recognition_by_Evidential_Modeling_of_Confusion_and_Ignorance_ICCV_2023_paper.html", "similarity": 0.5138}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Human-in-the-loop%20Reasoning%20For%20Traffic%20Sign%20Detection%3A%20Collaborative%0A%20%20Approach%20Yolo%20With%20Video-llava&body=Title%3A%20Human-in-the-loop%20Reasoning%20For%20Traffic%20Sign%20Detection%3A%20Collaborative%0A%20%20Approach%20Yolo%20With%20Video-llava%0AAuthor%3A%20Mehdi%20Azarafza%20and%20Fatima%20Idrees%20and%20Ali%20Ehteshami%20Bejnordi%20and%20Charles%20Steinmetz%20and%20Stefan%20Henkler%20and%20Achim%20Rettberg%0AAbstract%3A%20%20%20Traffic%20Sign%20Recognition%20%28TSR%29%20detection%20is%20a%20crucial%20component%20of%20autonomous%0Avehicles.%20While%20You%20Only%20Look%20Once%20%28YOLO%29%20is%20a%20popular%20real-time%20object%0Adetection%20algorithm%2C%20factors%20like%20training%20data%20quality%20and%20adverse%20weather%0Aconditions%20%28e.g.%2C%20heavy%20rain%29%20can%20lead%20to%20detection%20failures.%20These%20failures%0Acan%20be%20particularly%20dangerous%20when%20visual%20similarities%20between%20objects%20exist%2C%0Asuch%20as%20mistaking%20a%2030%20km/h%20sign%20for%20a%20higher%20speed%20limit%20sign.%20This%20paper%0Aproposes%20a%20method%20that%20combines%20video%20analysis%20and%20reasoning%2C%20prompting%20with%20a%0Ahuman-in-the-loop%20guide%20large%20vision%20model%20to%20improve%20YOLOs%20accuracy%20in%0Adetecting%20road%20speed%20limit%20signs%2C%20especially%20in%20semi-real-world%20conditions.%20It%0Ais%20hypothesized%20that%20the%20guided%20prompting%20and%20reasoning%20abilities%20of%0AVideo-LLava%20can%20enhance%20YOLOs%20traffic%20sign%20detection%20capabilities.%20This%0Ahypothesis%20is%20supported%20by%20an%20evaluation%20based%20on%20human-annotated%20accuracy%0Ametrics%20within%20a%20dataset%20of%20recorded%20videos%20from%20the%20CARLA%20car%20simulator.%20The%0Aresults%20demonstrate%20that%20a%20collaborative%20approach%20combining%20YOLO%20with%0AVideo-LLava%20and%20reasoning%20can%20effectively%20address%20challenging%20situations%20such%0Aas%20heavy%20rain%20and%20overcast%20conditions%20that%20hinder%20YOLOs%20detection%20capabilities.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2410.05096v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DHuman-in-the-loop%2520Reasoning%2520For%2520Traffic%2520Sign%2520Detection%253A%2520Collaborative%250A%2520%2520Approach%2520Yolo%2520With%2520Video-llava%26entry.906535625%3DMehdi%2520Azarafza%2520and%2520Fatima%2520Idrees%2520and%2520Ali%2520Ehteshami%2520Bejnordi%2520and%2520Charles%2520Steinmetz%2520and%2520Stefan%2520Henkler%2520and%2520Achim%2520Rettberg%26entry.1292438233%3D%2520%2520Traffic%2520Sign%2520Recognition%2520%2528TSR%2529%2520detection%2520is%2520a%2520crucial%2520component%2520of%2520autonomous%250Avehicles.%2520While%2520You%2520Only%2520Look%2520Once%2520%2528YOLO%2529%2520is%2520a%2520popular%2520real-time%2520object%250Adetection%2520algorithm%252C%2520factors%2520like%2520training%2520data%2520quality%2520and%2520adverse%2520weather%250Aconditions%2520%2528e.g.%252C%2520heavy%2520rain%2529%2520can%2520lead%2520to%2520detection%2520failures.%2520These%2520failures%250Acan%2520be%2520particularly%2520dangerous%2520when%2520visual%2520similarities%2520between%2520objects%2520exist%252C%250Asuch%2520as%2520mistaking%2520a%252030%2520km/h%2520sign%2520for%2520a%2520higher%2520speed%2520limit%2520sign.%2520This%2520paper%250Aproposes%2520a%2520method%2520that%2520combines%2520video%2520analysis%2520and%2520reasoning%252C%2520prompting%2520with%2520a%250Ahuman-in-the-loop%2520guide%2520large%2520vision%2520model%2520to%2520improve%2520YOLOs%2520accuracy%2520in%250Adetecting%2520road%2520speed%2520limit%2520signs%252C%2520especially%2520in%2520semi-real-world%2520conditions.%2520It%250Ais%2520hypothesized%2520that%2520the%2520guided%2520prompting%2520and%2520reasoning%2520abilities%2520of%250AVideo-LLava%2520can%2520enhance%2520YOLOs%2520traffic%2520sign%2520detection%2520capabilities.%2520This%250Ahypothesis%2520is%2520supported%2520by%2520an%2520evaluation%2520based%2520on%2520human-annotated%2520accuracy%250Ametrics%2520within%2520a%2520dataset%2520of%2520recorded%2520videos%2520from%2520the%2520CARLA%2520car%2520simulator.%2520The%250Aresults%2520demonstrate%2520that%2520a%2520collaborative%2520approach%2520combining%2520YOLO%2520with%250AVideo-LLava%2520and%2520reasoning%2520can%2520effectively%2520address%2520challenging%2520situations%2520such%250Aas%2520heavy%2520rain%2520and%2520overcast%2520conditions%2520that%2520hinder%2520YOLOs%2520detection%2520capabilities.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2410.05096v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Human-in-the-loop%20Reasoning%20For%20Traffic%20Sign%20Detection%3A%20Collaborative%0A%20%20Approach%20Yolo%20With%20Video-llava&entry.906535625=Mehdi%20Azarafza%20and%20Fatima%20Idrees%20and%20Ali%20Ehteshami%20Bejnordi%20and%20Charles%20Steinmetz%20and%20Stefan%20Henkler%20and%20Achim%20Rettberg&entry.1292438233=%20%20Traffic%20Sign%20Recognition%20%28TSR%29%20detection%20is%20a%20crucial%20component%20of%20autonomous%0Avehicles.%20While%20You%20Only%20Look%20Once%20%28YOLO%29%20is%20a%20popular%20real-time%20object%0Adetection%20algorithm%2C%20factors%20like%20training%20data%20quality%20and%20adverse%20weather%0Aconditions%20%28e.g.%2C%20heavy%20rain%29%20can%20lead%20to%20detection%20failures.%20These%20failures%0Acan%20be%20particularly%20dangerous%20when%20visual%20similarities%20between%20objects%20exist%2C%0Asuch%20as%20mistaking%20a%2030%20km/h%20sign%20for%20a%20higher%20speed%20limit%20sign.%20This%20paper%0Aproposes%20a%20method%20that%20combines%20video%20analysis%20and%20reasoning%2C%20prompting%20with%20a%0Ahuman-in-the-loop%20guide%20large%20vision%20model%20to%20improve%20YOLOs%20accuracy%20in%0Adetecting%20road%20speed%20limit%20signs%2C%20especially%20in%20semi-real-world%20conditions.%20It%0Ais%20hypothesized%20that%20the%20guided%20prompting%20and%20reasoning%20abilities%20of%0AVideo-LLava%20can%20enhance%20YOLOs%20traffic%20sign%20detection%20capabilities.%20This%0Ahypothesis%20is%20supported%20by%20an%20evaluation%20based%20on%20human-annotated%20accuracy%0Ametrics%20within%20a%20dataset%20of%20recorded%20videos%20from%20the%20CARLA%20car%20simulator.%20The%0Aresults%20demonstrate%20that%20a%20collaborative%20approach%20combining%20YOLO%20with%0AVideo-LLava%20and%20reasoning%20can%20effectively%20address%20challenging%20situations%20such%0Aas%20heavy%20rain%20and%20overcast%20conditions%20that%20hinder%20YOLOs%20detection%20capabilities.%0A&entry.1838667208=http%3A//arxiv.org/abs/2410.05096v1&entry.124074799=Read"},
{"title": "Think-on-Graph 2.0: Deep and Faithful Large Language Model Reasoning\n  with Knowledge-guided Retrieval Augmented Generation", "author": "Shengjie Ma and Chengjin Xu and Xuhui Jiang and Muzhi Li and Huaren Qu and Cehao Yang and Jiaxin Mao and Jian Guo", "abstract": "  Retrieval-augmented generation (RAG) has enhanced large language models\n(LLMs) by using knowledge retrieval to address knowledge gaps. However,\nexisting RAG approaches often fail to ensure the depth and completeness of the\ninformation retrieved, which is essential for complex reasoning tasks. In this\nwork, we present Think-on-Graph 2.0 (ToG-2), a hybrid RAG framework that\niteratively retrieves information from both unstructured and structured\nknowledge sources in a tightly integrated manner. Specifically, ToG-2 leverages\nknowledge graphs (KGs) to connect documents via entities, facilitating deep and\nknowledge-guided context retrieval. Simultaneously, it uses documents as entity\ncontexts to enable precise and efficient graph retrieval.\n  ToG-2 alternates between graph retrieval and context retrieval to search for\nin-depth clues relevant to the question, enabling LLMs to generate accurate\nanswers. We conduct a series of experiments to demonstrate the following\nadvantages of ToG-2: (1) ToG-2 tightly integrates context retrieval and graph\nretrieval, enhancing context retrieval through the KG while enabling reliable\ngraph retrieval based on contexts; (2) it achieves deep and faithful reasoning\nin LLMs through an iterative knowledge retrieval process that integrates\ncontexts and the KG; and (3) ToG-2 is training-free and compatible with various\nLLMs as a plug-and-play solution. Extensive experiments show that ToG-2\nachieves state-of-the-art (SOTA) performance on 6 out of 7 knowledge-intensive\ndatasets with GPT-3.5, and can elevate the performance of smaller models (e.g.,\nLLAMA-2-13B) to the level of GPT-3.5's direct reasoning.\n", "link": "http://arxiv.org/abs/2407.10805v4", "date": "2024-10-07", "relevancy": 2.0674, "topK": [{"title": "UGG: Unified Generative Grasping", "link": "https://arxiv.org/abs/2311.16917", "similarity": 0.5313}, {"title": "RoboEXP: Action-Conditioned Scene Graph via Interactive Exploration for\n  Robotic Manipulation", "link": "http://arxiv.org/abs/2402.15487v1", "similarity": 0.5202}, {"title": "Genie: Generative Interactive Environments", "link": "http://arxiv.org/abs/2402.15391v1", "similarity": 0.501}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Think-on-Graph%202.0%3A%20Deep%20and%20Faithful%20Large%20Language%20Model%20Reasoning%0A%20%20with%20Knowledge-guided%20Retrieval%20Augmented%20Generation&body=Title%3A%20Think-on-Graph%202.0%3A%20Deep%20and%20Faithful%20Large%20Language%20Model%20Reasoning%0A%20%20with%20Knowledge-guided%20Retrieval%20Augmented%20Generation%0AAuthor%3A%20Shengjie%20Ma%20and%20Chengjin%20Xu%20and%20Xuhui%20Jiang%20and%20Muzhi%20Li%20and%20Huaren%20Qu%20and%20Cehao%20Yang%20and%20Jiaxin%20Mao%20and%20Jian%20Guo%0AAbstract%3A%20%20%20Retrieval-augmented%20generation%20%28RAG%29%20has%20enhanced%20large%20language%20models%0A%28LLMs%29%20by%20using%20knowledge%20retrieval%20to%20address%20knowledge%20gaps.%20However%2C%0Aexisting%20RAG%20approaches%20often%20fail%20to%20ensure%20the%20depth%20and%20completeness%20of%20the%0Ainformation%20retrieved%2C%20which%20is%20essential%20for%20complex%20reasoning%20tasks.%20In%20this%0Awork%2C%20we%20present%20Think-on-Graph%202.0%20%28ToG-2%29%2C%20a%20hybrid%20RAG%20framework%20that%0Aiteratively%20retrieves%20information%20from%20both%20unstructured%20and%20structured%0Aknowledge%20sources%20in%20a%20tightly%20integrated%20manner.%20Specifically%2C%20ToG-2%20leverages%0Aknowledge%20graphs%20%28KGs%29%20to%20connect%20documents%20via%20entities%2C%20facilitating%20deep%20and%0Aknowledge-guided%20context%20retrieval.%20Simultaneously%2C%20it%20uses%20documents%20as%20entity%0Acontexts%20to%20enable%20precise%20and%20efficient%20graph%20retrieval.%0A%20%20ToG-2%20alternates%20between%20graph%20retrieval%20and%20context%20retrieval%20to%20search%20for%0Ain-depth%20clues%20relevant%20to%20the%20question%2C%20enabling%20LLMs%20to%20generate%20accurate%0Aanswers.%20We%20conduct%20a%20series%20of%20experiments%20to%20demonstrate%20the%20following%0Aadvantages%20of%20ToG-2%3A%20%281%29%20ToG-2%20tightly%20integrates%20context%20retrieval%20and%20graph%0Aretrieval%2C%20enhancing%20context%20retrieval%20through%20the%20KG%20while%20enabling%20reliable%0Agraph%20retrieval%20based%20on%20contexts%3B%20%282%29%20it%20achieves%20deep%20and%20faithful%20reasoning%0Ain%20LLMs%20through%20an%20iterative%20knowledge%20retrieval%20process%20that%20integrates%0Acontexts%20and%20the%20KG%3B%20and%20%283%29%20ToG-2%20is%20training-free%20and%20compatible%20with%20various%0ALLMs%20as%20a%20plug-and-play%20solution.%20Extensive%20experiments%20show%20that%20ToG-2%0Aachieves%20state-of-the-art%20%28SOTA%29%20performance%20on%206%20out%20of%207%20knowledge-intensive%0Adatasets%20with%20GPT-3.5%2C%20and%20can%20elevate%20the%20performance%20of%20smaller%20models%20%28e.g.%2C%0ALLAMA-2-13B%29%20to%20the%20level%20of%20GPT-3.5%27s%20direct%20reasoning.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2407.10805v4%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DThink-on-Graph%25202.0%253A%2520Deep%2520and%2520Faithful%2520Large%2520Language%2520Model%2520Reasoning%250A%2520%2520with%2520Knowledge-guided%2520Retrieval%2520Augmented%2520Generation%26entry.906535625%3DShengjie%2520Ma%2520and%2520Chengjin%2520Xu%2520and%2520Xuhui%2520Jiang%2520and%2520Muzhi%2520Li%2520and%2520Huaren%2520Qu%2520and%2520Cehao%2520Yang%2520and%2520Jiaxin%2520Mao%2520and%2520Jian%2520Guo%26entry.1292438233%3D%2520%2520Retrieval-augmented%2520generation%2520%2528RAG%2529%2520has%2520enhanced%2520large%2520language%2520models%250A%2528LLMs%2529%2520by%2520using%2520knowledge%2520retrieval%2520to%2520address%2520knowledge%2520gaps.%2520However%252C%250Aexisting%2520RAG%2520approaches%2520often%2520fail%2520to%2520ensure%2520the%2520depth%2520and%2520completeness%2520of%2520the%250Ainformation%2520retrieved%252C%2520which%2520is%2520essential%2520for%2520complex%2520reasoning%2520tasks.%2520In%2520this%250Awork%252C%2520we%2520present%2520Think-on-Graph%25202.0%2520%2528ToG-2%2529%252C%2520a%2520hybrid%2520RAG%2520framework%2520that%250Aiteratively%2520retrieves%2520information%2520from%2520both%2520unstructured%2520and%2520structured%250Aknowledge%2520sources%2520in%2520a%2520tightly%2520integrated%2520manner.%2520Specifically%252C%2520ToG-2%2520leverages%250Aknowledge%2520graphs%2520%2528KGs%2529%2520to%2520connect%2520documents%2520via%2520entities%252C%2520facilitating%2520deep%2520and%250Aknowledge-guided%2520context%2520retrieval.%2520Simultaneously%252C%2520it%2520uses%2520documents%2520as%2520entity%250Acontexts%2520to%2520enable%2520precise%2520and%2520efficient%2520graph%2520retrieval.%250A%2520%2520ToG-2%2520alternates%2520between%2520graph%2520retrieval%2520and%2520context%2520retrieval%2520to%2520search%2520for%250Ain-depth%2520clues%2520relevant%2520to%2520the%2520question%252C%2520enabling%2520LLMs%2520to%2520generate%2520accurate%250Aanswers.%2520We%2520conduct%2520a%2520series%2520of%2520experiments%2520to%2520demonstrate%2520the%2520following%250Aadvantages%2520of%2520ToG-2%253A%2520%25281%2529%2520ToG-2%2520tightly%2520integrates%2520context%2520retrieval%2520and%2520graph%250Aretrieval%252C%2520enhancing%2520context%2520retrieval%2520through%2520the%2520KG%2520while%2520enabling%2520reliable%250Agraph%2520retrieval%2520based%2520on%2520contexts%253B%2520%25282%2529%2520it%2520achieves%2520deep%2520and%2520faithful%2520reasoning%250Ain%2520LLMs%2520through%2520an%2520iterative%2520knowledge%2520retrieval%2520process%2520that%2520integrates%250Acontexts%2520and%2520the%2520KG%253B%2520and%2520%25283%2529%2520ToG-2%2520is%2520training-free%2520and%2520compatible%2520with%2520various%250ALLMs%2520as%2520a%2520plug-and-play%2520solution.%2520Extensive%2520experiments%2520show%2520that%2520ToG-2%250Aachieves%2520state-of-the-art%2520%2528SOTA%2529%2520performance%2520on%25206%2520out%2520of%25207%2520knowledge-intensive%250Adatasets%2520with%2520GPT-3.5%252C%2520and%2520can%2520elevate%2520the%2520performance%2520of%2520smaller%2520models%2520%2528e.g.%252C%250ALLAMA-2-13B%2529%2520to%2520the%2520level%2520of%2520GPT-3.5%2527s%2520direct%2520reasoning.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2407.10805v4%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Think-on-Graph%202.0%3A%20Deep%20and%20Faithful%20Large%20Language%20Model%20Reasoning%0A%20%20with%20Knowledge-guided%20Retrieval%20Augmented%20Generation&entry.906535625=Shengjie%20Ma%20and%20Chengjin%20Xu%20and%20Xuhui%20Jiang%20and%20Muzhi%20Li%20and%20Huaren%20Qu%20and%20Cehao%20Yang%20and%20Jiaxin%20Mao%20and%20Jian%20Guo&entry.1292438233=%20%20Retrieval-augmented%20generation%20%28RAG%29%20has%20enhanced%20large%20language%20models%0A%28LLMs%29%20by%20using%20knowledge%20retrieval%20to%20address%20knowledge%20gaps.%20However%2C%0Aexisting%20RAG%20approaches%20often%20fail%20to%20ensure%20the%20depth%20and%20completeness%20of%20the%0Ainformation%20retrieved%2C%20which%20is%20essential%20for%20complex%20reasoning%20tasks.%20In%20this%0Awork%2C%20we%20present%20Think-on-Graph%202.0%20%28ToG-2%29%2C%20a%20hybrid%20RAG%20framework%20that%0Aiteratively%20retrieves%20information%20from%20both%20unstructured%20and%20structured%0Aknowledge%20sources%20in%20a%20tightly%20integrated%20manner.%20Specifically%2C%20ToG-2%20leverages%0Aknowledge%20graphs%20%28KGs%29%20to%20connect%20documents%20via%20entities%2C%20facilitating%20deep%20and%0Aknowledge-guided%20context%20retrieval.%20Simultaneously%2C%20it%20uses%20documents%20as%20entity%0Acontexts%20to%20enable%20precise%20and%20efficient%20graph%20retrieval.%0A%20%20ToG-2%20alternates%20between%20graph%20retrieval%20and%20context%20retrieval%20to%20search%20for%0Ain-depth%20clues%20relevant%20to%20the%20question%2C%20enabling%20LLMs%20to%20generate%20accurate%0Aanswers.%20We%20conduct%20a%20series%20of%20experiments%20to%20demonstrate%20the%20following%0Aadvantages%20of%20ToG-2%3A%20%281%29%20ToG-2%20tightly%20integrates%20context%20retrieval%20and%20graph%0Aretrieval%2C%20enhancing%20context%20retrieval%20through%20the%20KG%20while%20enabling%20reliable%0Agraph%20retrieval%20based%20on%20contexts%3B%20%282%29%20it%20achieves%20deep%20and%20faithful%20reasoning%0Ain%20LLMs%20through%20an%20iterative%20knowledge%20retrieval%20process%20that%20integrates%0Acontexts%20and%20the%20KG%3B%20and%20%283%29%20ToG-2%20is%20training-free%20and%20compatible%20with%20various%0ALLMs%20as%20a%20plug-and-play%20solution.%20Extensive%20experiments%20show%20that%20ToG-2%0Aachieves%20state-of-the-art%20%28SOTA%29%20performance%20on%206%20out%20of%207%20knowledge-intensive%0Adatasets%20with%20GPT-3.5%2C%20and%20can%20elevate%20the%20performance%20of%20smaller%20models%20%28e.g.%2C%0ALLAMA-2-13B%29%20to%20the%20level%20of%20GPT-3.5%27s%20direct%20reasoning.%0A&entry.1838667208=http%3A//arxiv.org/abs/2407.10805v4&entry.124074799=Read"},
{"title": "Data-Centric Foundation Models in Computational Healthcare: A Survey", "author": "Yunkun Zhang and Jin Gao and Zheling Tan and Lingfeng Zhou and Kexin Ding and Mu Zhou and Shaoting Zhang and Dequan Wang", "abstract": "  The advent of foundation models (FMs) as an emerging suite of AI techniques\nhas struck a wave of opportunities in computational healthcare. The interactive\nnature of these models, guided by pre-training data and human instructions, has\nignited a data-centric AI paradigm that emphasizes better data\ncharacterization, quality, and scale. In healthcare AI, obtaining and\nprocessing high-quality clinical data records has been a longstanding\nchallenge, ranging from data quantity, annotation, patient privacy, and ethics.\nIn this survey, we investigate a wide range of data-centric approaches in the\nFM era (from model pre-training to inference) towards improving the healthcare\nworkflow. We discuss key perspectives in AI security, assessment, and alignment\nwith human values. Finally, we offer a promising outlook of FM-based analytics\nto enhance the performance of patient outcome and clinical workflow in the\nevolving landscape of healthcare and medicine. We provide an up-to-date list of\nhealthcare-related foundation models and datasets at\nhttps://github.com/Yunkun-Zhang/Data-Centric-FM-Healthcare .\n", "link": "http://arxiv.org/abs/2401.02458v2", "date": "2024-10-07", "relevancy": 1.9261, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.4906}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.4906}, {"title": "Multimodal-Conditioned Latent Diffusion Models for Fashion Image Editing", "link": "http://arxiv.org/abs/2403.14828v2", "similarity": 0.4361}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Data-Centric%20Foundation%20Models%20in%20Computational%20Healthcare%3A%20A%20Survey&body=Title%3A%20Data-Centric%20Foundation%20Models%20in%20Computational%20Healthcare%3A%20A%20Survey%0AAuthor%3A%20Yunkun%20Zhang%20and%20Jin%20Gao%20and%20Zheling%20Tan%20and%20Lingfeng%20Zhou%20and%20Kexin%20Ding%20and%20Mu%20Zhou%20and%20Shaoting%20Zhang%20and%20Dequan%20Wang%0AAbstract%3A%20%20%20The%20advent%20of%20foundation%20models%20%28FMs%29%20as%20an%20emerging%20suite%20of%20AI%20techniques%0Ahas%20struck%20a%20wave%20of%20opportunities%20in%20computational%20healthcare.%20The%20interactive%0Anature%20of%20these%20models%2C%20guided%20by%20pre-training%20data%20and%20human%20instructions%2C%20has%0Aignited%20a%20data-centric%20AI%20paradigm%20that%20emphasizes%20better%20data%0Acharacterization%2C%20quality%2C%20and%20scale.%20In%20healthcare%20AI%2C%20obtaining%20and%0Aprocessing%20high-quality%20clinical%20data%20records%20has%20been%20a%20longstanding%0Achallenge%2C%20ranging%20from%20data%20quantity%2C%20annotation%2C%20patient%20privacy%2C%20and%20ethics.%0AIn%20this%20survey%2C%20we%20investigate%20a%20wide%20range%20of%20data-centric%20approaches%20in%20the%0AFM%20era%20%28from%20model%20pre-training%20to%20inference%29%20towards%20improving%20the%20healthcare%0Aworkflow.%20We%20discuss%20key%20perspectives%20in%20AI%20security%2C%20assessment%2C%20and%20alignment%0Awith%20human%20values.%20Finally%2C%20we%20offer%20a%20promising%20outlook%20of%20FM-based%20analytics%0Ato%20enhance%20the%20performance%20of%20patient%20outcome%20and%20clinical%20workflow%20in%20the%0Aevolving%20landscape%20of%20healthcare%20and%20medicine.%20We%20provide%20an%20up-to-date%20list%20of%0Ahealthcare-related%20foundation%20models%20and%20datasets%20at%0Ahttps%3A//github.com/Yunkun-Zhang/Data-Centric-FM-Healthcare%20.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2401.02458v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DData-Centric%2520Foundation%2520Models%2520in%2520Computational%2520Healthcare%253A%2520A%2520Survey%26entry.906535625%3DYunkun%2520Zhang%2520and%2520Jin%2520Gao%2520and%2520Zheling%2520Tan%2520and%2520Lingfeng%2520Zhou%2520and%2520Kexin%2520Ding%2520and%2520Mu%2520Zhou%2520and%2520Shaoting%2520Zhang%2520and%2520Dequan%2520Wang%26entry.1292438233%3D%2520%2520The%2520advent%2520of%2520foundation%2520models%2520%2528FMs%2529%2520as%2520an%2520emerging%2520suite%2520of%2520AI%2520techniques%250Ahas%2520struck%2520a%2520wave%2520of%2520opportunities%2520in%2520computational%2520healthcare.%2520The%2520interactive%250Anature%2520of%2520these%2520models%252C%2520guided%2520by%2520pre-training%2520data%2520and%2520human%2520instructions%252C%2520has%250Aignited%2520a%2520data-centric%2520AI%2520paradigm%2520that%2520emphasizes%2520better%2520data%250Acharacterization%252C%2520quality%252C%2520and%2520scale.%2520In%2520healthcare%2520AI%252C%2520obtaining%2520and%250Aprocessing%2520high-quality%2520clinical%2520data%2520records%2520has%2520been%2520a%2520longstanding%250Achallenge%252C%2520ranging%2520from%2520data%2520quantity%252C%2520annotation%252C%2520patient%2520privacy%252C%2520and%2520ethics.%250AIn%2520this%2520survey%252C%2520we%2520investigate%2520a%2520wide%2520range%2520of%2520data-centric%2520approaches%2520in%2520the%250AFM%2520era%2520%2528from%2520model%2520pre-training%2520to%2520inference%2529%2520towards%2520improving%2520the%2520healthcare%250Aworkflow.%2520We%2520discuss%2520key%2520perspectives%2520in%2520AI%2520security%252C%2520assessment%252C%2520and%2520alignment%250Awith%2520human%2520values.%2520Finally%252C%2520we%2520offer%2520a%2520promising%2520outlook%2520of%2520FM-based%2520analytics%250Ato%2520enhance%2520the%2520performance%2520of%2520patient%2520outcome%2520and%2520clinical%2520workflow%2520in%2520the%250Aevolving%2520landscape%2520of%2520healthcare%2520and%2520medicine.%2520We%2520provide%2520an%2520up-to-date%2520list%2520of%250Ahealthcare-related%2520foundation%2520models%2520and%2520datasets%2520at%250Ahttps%253A//github.com/Yunkun-Zhang/Data-Centric-FM-Healthcare%2520.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2401.02458v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Data-Centric%20Foundation%20Models%20in%20Computational%20Healthcare%3A%20A%20Survey&entry.906535625=Yunkun%20Zhang%20and%20Jin%20Gao%20and%20Zheling%20Tan%20and%20Lingfeng%20Zhou%20and%20Kexin%20Ding%20and%20Mu%20Zhou%20and%20Shaoting%20Zhang%20and%20Dequan%20Wang&entry.1292438233=%20%20The%20advent%20of%20foundation%20models%20%28FMs%29%20as%20an%20emerging%20suite%20of%20AI%20techniques%0Ahas%20struck%20a%20wave%20of%20opportunities%20in%20computational%20healthcare.%20The%20interactive%0Anature%20of%20these%20models%2C%20guided%20by%20pre-training%20data%20and%20human%20instructions%2C%20has%0Aignited%20a%20data-centric%20AI%20paradigm%20that%20emphasizes%20better%20data%0Acharacterization%2C%20quality%2C%20and%20scale.%20In%20healthcare%20AI%2C%20obtaining%20and%0Aprocessing%20high-quality%20clinical%20data%20records%20has%20been%20a%20longstanding%0Achallenge%2C%20ranging%20from%20data%20quantity%2C%20annotation%2C%20patient%20privacy%2C%20and%20ethics.%0AIn%20this%20survey%2C%20we%20investigate%20a%20wide%20range%20of%20data-centric%20approaches%20in%20the%0AFM%20era%20%28from%20model%20pre-training%20to%20inference%29%20towards%20improving%20the%20healthcare%0Aworkflow.%20We%20discuss%20key%20perspectives%20in%20AI%20security%2C%20assessment%2C%20and%20alignment%0Awith%20human%20values.%20Finally%2C%20we%20offer%20a%20promising%20outlook%20of%20FM-based%20analytics%0Ato%20enhance%20the%20performance%20of%20patient%20outcome%20and%20clinical%20workflow%20in%20the%0Aevolving%20landscape%20of%20healthcare%20and%20medicine.%20We%20provide%20an%20up-to-date%20list%20of%0Ahealthcare-related%20foundation%20models%20and%20datasets%20at%0Ahttps%3A//github.com/Yunkun-Zhang/Data-Centric-FM-Healthcare%20.%0A&entry.1838667208=http%3A//arxiv.org/abs/2401.02458v2&entry.124074799=Read"},
{"title": "Stage-Wise and Prior-Aware Neural Speech Phase Prediction", "author": "Fei Liu and Yang Ai and Hui-Peng Du and Ye-Xin Lu and Rui-Chen Zheng and Zhen-Hua Ling", "abstract": "  This paper proposes a novel Stage-wise and Prior-aware Neural Speech Phase\nPrediction (SP-NSPP) model, which predicts the phase spectrum from input\namplitude spectrum by two-stage neural networks. In the initial\nprior-construction stage, we preliminarily predict a rough prior phase spectrum\nfrom the amplitude spectrum. The subsequent refinement stage transforms the\namplitude spectrum into a refined high-quality phase spectrum conditioned on\nthe prior phase. Networks in both stages use ConvNeXt v2 blocks as the backbone\nand adopt adversarial training by innovatively introducing a phase spectrum\ndiscriminator (PSD). To further improve the continuity of the refined phase, we\nalso incorporate a time-frequency integrated difference (TFID) loss in the\nrefinement stage. Experimental results confirm that, compared to neural\nnetwork-based no-prior phase prediction methods, the proposed SP-NSPP achieves\nhigher phase prediction accuracy, thanks to introducing the coarse phase priors\nand diverse training criteria. Compared to iterative phase estimation\nalgorithms, our proposed SP-NSPP does not require multiple rounds of staged\niterations, resulting in higher generation efficiency.\n", "link": "http://arxiv.org/abs/2410.04990v1", "date": "2024-10-07", "relevancy": 1.8771, "topK": [{"title": "Weakly-guided self-supervised pretraining for temporal activity detection", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/25189", "similarity": 0.4744}, {"title": "Boosted dynamic neural networks", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/26302", "similarity": 0.4669}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.4623}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Stage-Wise%20and%20Prior-Aware%20Neural%20Speech%20Phase%20Prediction&body=Title%3A%20Stage-Wise%20and%20Prior-Aware%20Neural%20Speech%20Phase%20Prediction%0AAuthor%3A%20Fei%20Liu%20and%20Yang%20Ai%20and%20Hui-Peng%20Du%20and%20Ye-Xin%20Lu%20and%20Rui-Chen%20Zheng%20and%20Zhen-Hua%20Ling%0AAbstract%3A%20%20%20This%20paper%20proposes%20a%20novel%20Stage-wise%20and%20Prior-aware%20Neural%20Speech%20Phase%0APrediction%20%28SP-NSPP%29%20model%2C%20which%20predicts%20the%20phase%20spectrum%20from%20input%0Aamplitude%20spectrum%20by%20two-stage%20neural%20networks.%20In%20the%20initial%0Aprior-construction%20stage%2C%20we%20preliminarily%20predict%20a%20rough%20prior%20phase%20spectrum%0Afrom%20the%20amplitude%20spectrum.%20The%20subsequent%20refinement%20stage%20transforms%20the%0Aamplitude%20spectrum%20into%20a%20refined%20high-quality%20phase%20spectrum%20conditioned%20on%0Athe%20prior%20phase.%20Networks%20in%20both%20stages%20use%20ConvNeXt%20v2%20blocks%20as%20the%20backbone%0Aand%20adopt%20adversarial%20training%20by%20innovatively%20introducing%20a%20phase%20spectrum%0Adiscriminator%20%28PSD%29.%20To%20further%20improve%20the%20continuity%20of%20the%20refined%20phase%2C%20we%0Aalso%20incorporate%20a%20time-frequency%20integrated%20difference%20%28TFID%29%20loss%20in%20the%0Arefinement%20stage.%20Experimental%20results%20confirm%20that%2C%20compared%20to%20neural%0Anetwork-based%20no-prior%20phase%20prediction%20methods%2C%20the%20proposed%20SP-NSPP%20achieves%0Ahigher%20phase%20prediction%20accuracy%2C%20thanks%20to%20introducing%20the%20coarse%20phase%20priors%0Aand%20diverse%20training%20criteria.%20Compared%20to%20iterative%20phase%20estimation%0Aalgorithms%2C%20our%20proposed%20SP-NSPP%20does%20not%20require%20multiple%20rounds%20of%20staged%0Aiterations%2C%20resulting%20in%20higher%20generation%20efficiency.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2410.04990v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DStage-Wise%2520and%2520Prior-Aware%2520Neural%2520Speech%2520Phase%2520Prediction%26entry.906535625%3DFei%2520Liu%2520and%2520Yang%2520Ai%2520and%2520Hui-Peng%2520Du%2520and%2520Ye-Xin%2520Lu%2520and%2520Rui-Chen%2520Zheng%2520and%2520Zhen-Hua%2520Ling%26entry.1292438233%3D%2520%2520This%2520paper%2520proposes%2520a%2520novel%2520Stage-wise%2520and%2520Prior-aware%2520Neural%2520Speech%2520Phase%250APrediction%2520%2528SP-NSPP%2529%2520model%252C%2520which%2520predicts%2520the%2520phase%2520spectrum%2520from%2520input%250Aamplitude%2520spectrum%2520by%2520two-stage%2520neural%2520networks.%2520In%2520the%2520initial%250Aprior-construction%2520stage%252C%2520we%2520preliminarily%2520predict%2520a%2520rough%2520prior%2520phase%2520spectrum%250Afrom%2520the%2520amplitude%2520spectrum.%2520The%2520subsequent%2520refinement%2520stage%2520transforms%2520the%250Aamplitude%2520spectrum%2520into%2520a%2520refined%2520high-quality%2520phase%2520spectrum%2520conditioned%2520on%250Athe%2520prior%2520phase.%2520Networks%2520in%2520both%2520stages%2520use%2520ConvNeXt%2520v2%2520blocks%2520as%2520the%2520backbone%250Aand%2520adopt%2520adversarial%2520training%2520by%2520innovatively%2520introducing%2520a%2520phase%2520spectrum%250Adiscriminator%2520%2528PSD%2529.%2520To%2520further%2520improve%2520the%2520continuity%2520of%2520the%2520refined%2520phase%252C%2520we%250Aalso%2520incorporate%2520a%2520time-frequency%2520integrated%2520difference%2520%2528TFID%2529%2520loss%2520in%2520the%250Arefinement%2520stage.%2520Experimental%2520results%2520confirm%2520that%252C%2520compared%2520to%2520neural%250Anetwork-based%2520no-prior%2520phase%2520prediction%2520methods%252C%2520the%2520proposed%2520SP-NSPP%2520achieves%250Ahigher%2520phase%2520prediction%2520accuracy%252C%2520thanks%2520to%2520introducing%2520the%2520coarse%2520phase%2520priors%250Aand%2520diverse%2520training%2520criteria.%2520Compared%2520to%2520iterative%2520phase%2520estimation%250Aalgorithms%252C%2520our%2520proposed%2520SP-NSPP%2520does%2520not%2520require%2520multiple%2520rounds%2520of%2520staged%250Aiterations%252C%2520resulting%2520in%2520higher%2520generation%2520efficiency.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2410.04990v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Stage-Wise%20and%20Prior-Aware%20Neural%20Speech%20Phase%20Prediction&entry.906535625=Fei%20Liu%20and%20Yang%20Ai%20and%20Hui-Peng%20Du%20and%20Ye-Xin%20Lu%20and%20Rui-Chen%20Zheng%20and%20Zhen-Hua%20Ling&entry.1292438233=%20%20This%20paper%20proposes%20a%20novel%20Stage-wise%20and%20Prior-aware%20Neural%20Speech%20Phase%0APrediction%20%28SP-NSPP%29%20model%2C%20which%20predicts%20the%20phase%20spectrum%20from%20input%0Aamplitude%20spectrum%20by%20two-stage%20neural%20networks.%20In%20the%20initial%0Aprior-construction%20stage%2C%20we%20preliminarily%20predict%20a%20rough%20prior%20phase%20spectrum%0Afrom%20the%20amplitude%20spectrum.%20The%20subsequent%20refinement%20stage%20transforms%20the%0Aamplitude%20spectrum%20into%20a%20refined%20high-quality%20phase%20spectrum%20conditioned%20on%0Athe%20prior%20phase.%20Networks%20in%20both%20stages%20use%20ConvNeXt%20v2%20blocks%20as%20the%20backbone%0Aand%20adopt%20adversarial%20training%20by%20innovatively%20introducing%20a%20phase%20spectrum%0Adiscriminator%20%28PSD%29.%20To%20further%20improve%20the%20continuity%20of%20the%20refined%20phase%2C%20we%0Aalso%20incorporate%20a%20time-frequency%20integrated%20difference%20%28TFID%29%20loss%20in%20the%0Arefinement%20stage.%20Experimental%20results%20confirm%20that%2C%20compared%20to%20neural%0Anetwork-based%20no-prior%20phase%20prediction%20methods%2C%20the%20proposed%20SP-NSPP%20achieves%0Ahigher%20phase%20prediction%20accuracy%2C%20thanks%20to%20introducing%20the%20coarse%20phase%20priors%0Aand%20diverse%20training%20criteria.%20Compared%20to%20iterative%20phase%20estimation%0Aalgorithms%2C%20our%20proposed%20SP-NSPP%20does%20not%20require%20multiple%20rounds%20of%20staged%0Aiterations%2C%20resulting%20in%20higher%20generation%20efficiency.%0A&entry.1838667208=http%3A//arxiv.org/abs/2410.04990v1&entry.124074799=Read"},
{"title": "CR-CTC: Consistency regularization on CTC for improved speech\n  recognition", "author": "Zengwei Yao and Wei Kang and Xiaoyu Yang and Fangjun Kuang and Liyong Guo and Han Zhu and Zengrui Jin and Zhaoqing Li and Long Lin and Daniel Povey", "abstract": "  Connectionist Temporal Classification (CTC) is a widely used method for\nautomatic speech recognition (ASR), renowned for its simplicity and\ncomputational efficiency. However, it often falls short in recognition\nperformance compared to transducer or systems combining CTC and attention-based\nencoder-decoder (CTC/AED). In this work, we propose the Consistency-Regularized\nCTC (CR-CTC), which enforces consistency between two CTC distributions obtained\nfrom different augmented views of the input speech mel-spectrogram. We provide\nin-depth insights into its essential behaviors from three perspectives: 1) it\nconducts self-distillation between random pairs of sub-models that process\ndifferent augmented views; 2) it learns contextual representation through\nmasked prediction for positions within time-masked regions, especially when we\nincrease the amount of time masking; 3) it suppresses the extremely peaky CTC\ndistributions, thereby reducing overfitting and improving the generalization\nability. Extensive experiments on LibriSpeech, Aishell-1, and GigaSpeech\ndatasets demonstrate the effectiveness of our CR-CTC, which achieves\nperformance comparable to, or even slightly better than, that of transducer and\nCTC/AED.\n", "link": "http://arxiv.org/abs/2410.05101v1", "date": "2024-10-07", "relevancy": 1.9086, "topK": [{"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.4964}, {"title": "Scaling Rectified Flow Transformers for High-Resolution Image Synthesis", "link": "http://arxiv.org/abs/2403.03206v1", "similarity": 0.4648}, {"title": "Breadcrumbs: Adversarial class-balanced sampling for long-tailed recognition", "link": "https://link.springer.com/chapter/10.1007/978-3-031-20053-3_37", "similarity": 0.4628}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20CR-CTC%3A%20Consistency%20regularization%20on%20CTC%20for%20improved%20speech%0A%20%20recognition&body=Title%3A%20CR-CTC%3A%20Consistency%20regularization%20on%20CTC%20for%20improved%20speech%0A%20%20recognition%0AAuthor%3A%20Zengwei%20Yao%20and%20Wei%20Kang%20and%20Xiaoyu%20Yang%20and%20Fangjun%20Kuang%20and%20Liyong%20Guo%20and%20Han%20Zhu%20and%20Zengrui%20Jin%20and%20Zhaoqing%20Li%20and%20Long%20Lin%20and%20Daniel%20Povey%0AAbstract%3A%20%20%20Connectionist%20Temporal%20Classification%20%28CTC%29%20is%20a%20widely%20used%20method%20for%0Aautomatic%20speech%20recognition%20%28ASR%29%2C%20renowned%20for%20its%20simplicity%20and%0Acomputational%20efficiency.%20However%2C%20it%20often%20falls%20short%20in%20recognition%0Aperformance%20compared%20to%20transducer%20or%20systems%20combining%20CTC%20and%20attention-based%0Aencoder-decoder%20%28CTC/AED%29.%20In%20this%20work%2C%20we%20propose%20the%20Consistency-Regularized%0ACTC%20%28CR-CTC%29%2C%20which%20enforces%20consistency%20between%20two%20CTC%20distributions%20obtained%0Afrom%20different%20augmented%20views%20of%20the%20input%20speech%20mel-spectrogram.%20We%20provide%0Ain-depth%20insights%20into%20its%20essential%20behaviors%20from%20three%20perspectives%3A%201%29%20it%0Aconducts%20self-distillation%20between%20random%20pairs%20of%20sub-models%20that%20process%0Adifferent%20augmented%20views%3B%202%29%20it%20learns%20contextual%20representation%20through%0Amasked%20prediction%20for%20positions%20within%20time-masked%20regions%2C%20especially%20when%20we%0Aincrease%20the%20amount%20of%20time%20masking%3B%203%29%20it%20suppresses%20the%20extremely%20peaky%20CTC%0Adistributions%2C%20thereby%20reducing%20overfitting%20and%20improving%20the%20generalization%0Aability.%20Extensive%20experiments%20on%20LibriSpeech%2C%20Aishell-1%2C%20and%20GigaSpeech%0Adatasets%20demonstrate%20the%20effectiveness%20of%20our%20CR-CTC%2C%20which%20achieves%0Aperformance%20comparable%20to%2C%20or%20even%20slightly%20better%20than%2C%20that%20of%20transducer%20and%0ACTC/AED.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2410.05101v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DCR-CTC%253A%2520Consistency%2520regularization%2520on%2520CTC%2520for%2520improved%2520speech%250A%2520%2520recognition%26entry.906535625%3DZengwei%2520Yao%2520and%2520Wei%2520Kang%2520and%2520Xiaoyu%2520Yang%2520and%2520Fangjun%2520Kuang%2520and%2520Liyong%2520Guo%2520and%2520Han%2520Zhu%2520and%2520Zengrui%2520Jin%2520and%2520Zhaoqing%2520Li%2520and%2520Long%2520Lin%2520and%2520Daniel%2520Povey%26entry.1292438233%3D%2520%2520Connectionist%2520Temporal%2520Classification%2520%2528CTC%2529%2520is%2520a%2520widely%2520used%2520method%2520for%250Aautomatic%2520speech%2520recognition%2520%2528ASR%2529%252C%2520renowned%2520for%2520its%2520simplicity%2520and%250Acomputational%2520efficiency.%2520However%252C%2520it%2520often%2520falls%2520short%2520in%2520recognition%250Aperformance%2520compared%2520to%2520transducer%2520or%2520systems%2520combining%2520CTC%2520and%2520attention-based%250Aencoder-decoder%2520%2528CTC/AED%2529.%2520In%2520this%2520work%252C%2520we%2520propose%2520the%2520Consistency-Regularized%250ACTC%2520%2528CR-CTC%2529%252C%2520which%2520enforces%2520consistency%2520between%2520two%2520CTC%2520distributions%2520obtained%250Afrom%2520different%2520augmented%2520views%2520of%2520the%2520input%2520speech%2520mel-spectrogram.%2520We%2520provide%250Ain-depth%2520insights%2520into%2520its%2520essential%2520behaviors%2520from%2520three%2520perspectives%253A%25201%2529%2520it%250Aconducts%2520self-distillation%2520between%2520random%2520pairs%2520of%2520sub-models%2520that%2520process%250Adifferent%2520augmented%2520views%253B%25202%2529%2520it%2520learns%2520contextual%2520representation%2520through%250Amasked%2520prediction%2520for%2520positions%2520within%2520time-masked%2520regions%252C%2520especially%2520when%2520we%250Aincrease%2520the%2520amount%2520of%2520time%2520masking%253B%25203%2529%2520it%2520suppresses%2520the%2520extremely%2520peaky%2520CTC%250Adistributions%252C%2520thereby%2520reducing%2520overfitting%2520and%2520improving%2520the%2520generalization%250Aability.%2520Extensive%2520experiments%2520on%2520LibriSpeech%252C%2520Aishell-1%252C%2520and%2520GigaSpeech%250Adatasets%2520demonstrate%2520the%2520effectiveness%2520of%2520our%2520CR-CTC%252C%2520which%2520achieves%250Aperformance%2520comparable%2520to%252C%2520or%2520even%2520slightly%2520better%2520than%252C%2520that%2520of%2520transducer%2520and%250ACTC/AED.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2410.05101v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=CR-CTC%3A%20Consistency%20regularization%20on%20CTC%20for%20improved%20speech%0A%20%20recognition&entry.906535625=Zengwei%20Yao%20and%20Wei%20Kang%20and%20Xiaoyu%20Yang%20and%20Fangjun%20Kuang%20and%20Liyong%20Guo%20and%20Han%20Zhu%20and%20Zengrui%20Jin%20and%20Zhaoqing%20Li%20and%20Long%20Lin%20and%20Daniel%20Povey&entry.1292438233=%20%20Connectionist%20Temporal%20Classification%20%28CTC%29%20is%20a%20widely%20used%20method%20for%0Aautomatic%20speech%20recognition%20%28ASR%29%2C%20renowned%20for%20its%20simplicity%20and%0Acomputational%20efficiency.%20However%2C%20it%20often%20falls%20short%20in%20recognition%0Aperformance%20compared%20to%20transducer%20or%20systems%20combining%20CTC%20and%20attention-based%0Aencoder-decoder%20%28CTC/AED%29.%20In%20this%20work%2C%20we%20propose%20the%20Consistency-Regularized%0ACTC%20%28CR-CTC%29%2C%20which%20enforces%20consistency%20between%20two%20CTC%20distributions%20obtained%0Afrom%20different%20augmented%20views%20of%20the%20input%20speech%20mel-spectrogram.%20We%20provide%0Ain-depth%20insights%20into%20its%20essential%20behaviors%20from%20three%20perspectives%3A%201%29%20it%0Aconducts%20self-distillation%20between%20random%20pairs%20of%20sub-models%20that%20process%0Adifferent%20augmented%20views%3B%202%29%20it%20learns%20contextual%20representation%20through%0Amasked%20prediction%20for%20positions%20within%20time-masked%20regions%2C%20especially%20when%20we%0Aincrease%20the%20amount%20of%20time%20masking%3B%203%29%20it%20suppresses%20the%20extremely%20peaky%20CTC%0Adistributions%2C%20thereby%20reducing%20overfitting%20and%20improving%20the%20generalization%0Aability.%20Extensive%20experiments%20on%20LibriSpeech%2C%20Aishell-1%2C%20and%20GigaSpeech%0Adatasets%20demonstrate%20the%20effectiveness%20of%20our%20CR-CTC%2C%20which%20achieves%0Aperformance%20comparable%20to%2C%20or%20even%20slightly%20better%20than%2C%20that%20of%20transducer%20and%0ACTC/AED.%0A&entry.1838667208=http%3A//arxiv.org/abs/2410.05101v1&entry.124074799=Read"},
{"title": "SKT: Integrating State-Aware Keypoint Trajectories with Vision-Language\n  Models for Robotic Garment Manipulation", "author": "Xin Li and Siyuan Huang and Qiaojun Yu and Zhengkai Jiang and Ce Hao and Yimeng Zhu and Hongsheng Li and Peng Gao and Cewu Lu", "abstract": "  Automating garment manipulation poses a significant challenge for assistive\nrobotics due to the diverse and deformable nature of garments. Traditional\napproaches typically require separate models for each garment type, which\nlimits scalability and adaptability. In contrast, this paper presents a unified\napproach using vision-language models (VLMs) to improve keypoint prediction\nacross various garment categories. By interpreting both visual and semantic\ninformation, our model enables robots to manage different garment states with a\nsingle model. We created a large-scale synthetic dataset using advanced\nsimulation techniques, allowing scalable training without extensive real-world\ndata. Experimental results indicate that the VLM-based method significantly\nenhances keypoint detection accuracy and task success rates, providing a more\nflexible and general solution for robotic garment manipulation. In addition,\nthis research also underscores the potential of VLMs to unify various garment\nmanipulation tasks within a single framework, paving the way for broader\napplications in home automation and assistive robotics for future.\n", "link": "http://arxiv.org/abs/2409.18082v2", "date": "2024-10-07", "relevancy": 1.7828, "topK": [{"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.6225}, {"title": "DressCode: Autoregressively Sewing and Generating Garments from Text\n  Guidance", "link": "http://arxiv.org/abs/2401.16465v3", "similarity": 0.5909}, {"title": "RoboEXP: Action-Conditioned Scene Graph via Interactive Exploration for\n  Robotic Manipulation", "link": "http://arxiv.org/abs/2402.15487v1", "similarity": 0.5745}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20SKT%3A%20Integrating%20State-Aware%20Keypoint%20Trajectories%20with%20Vision-Language%0A%20%20Models%20for%20Robotic%20Garment%20Manipulation&body=Title%3A%20SKT%3A%20Integrating%20State-Aware%20Keypoint%20Trajectories%20with%20Vision-Language%0A%20%20Models%20for%20Robotic%20Garment%20Manipulation%0AAuthor%3A%20Xin%20Li%20and%20Siyuan%20Huang%20and%20Qiaojun%20Yu%20and%20Zhengkai%20Jiang%20and%20Ce%20Hao%20and%20Yimeng%20Zhu%20and%20Hongsheng%20Li%20and%20Peng%20Gao%20and%20Cewu%20Lu%0AAbstract%3A%20%20%20Automating%20garment%20manipulation%20poses%20a%20significant%20challenge%20for%20assistive%0Arobotics%20due%20to%20the%20diverse%20and%20deformable%20nature%20of%20garments.%20Traditional%0Aapproaches%20typically%20require%20separate%20models%20for%20each%20garment%20type%2C%20which%0Alimits%20scalability%20and%20adaptability.%20In%20contrast%2C%20this%20paper%20presents%20a%20unified%0Aapproach%20using%20vision-language%20models%20%28VLMs%29%20to%20improve%20keypoint%20prediction%0Aacross%20various%20garment%20categories.%20By%20interpreting%20both%20visual%20and%20semantic%0Ainformation%2C%20our%20model%20enables%20robots%20to%20manage%20different%20garment%20states%20with%20a%0Asingle%20model.%20We%20created%20a%20large-scale%20synthetic%20dataset%20using%20advanced%0Asimulation%20techniques%2C%20allowing%20scalable%20training%20without%20extensive%20real-world%0Adata.%20Experimental%20results%20indicate%20that%20the%20VLM-based%20method%20significantly%0Aenhances%20keypoint%20detection%20accuracy%20and%20task%20success%20rates%2C%20providing%20a%20more%0Aflexible%20and%20general%20solution%20for%20robotic%20garment%20manipulation.%20In%20addition%2C%0Athis%20research%20also%20underscores%20the%20potential%20of%20VLMs%20to%20unify%20various%20garment%0Amanipulation%20tasks%20within%20a%20single%20framework%2C%20paving%20the%20way%20for%20broader%0Aapplications%20in%20home%20automation%20and%20assistive%20robotics%20for%20future.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2409.18082v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DSKT%253A%2520Integrating%2520State-Aware%2520Keypoint%2520Trajectories%2520with%2520Vision-Language%250A%2520%2520Models%2520for%2520Robotic%2520Garment%2520Manipulation%26entry.906535625%3DXin%2520Li%2520and%2520Siyuan%2520Huang%2520and%2520Qiaojun%2520Yu%2520and%2520Zhengkai%2520Jiang%2520and%2520Ce%2520Hao%2520and%2520Yimeng%2520Zhu%2520and%2520Hongsheng%2520Li%2520and%2520Peng%2520Gao%2520and%2520Cewu%2520Lu%26entry.1292438233%3D%2520%2520Automating%2520garment%2520manipulation%2520poses%2520a%2520significant%2520challenge%2520for%2520assistive%250Arobotics%2520due%2520to%2520the%2520diverse%2520and%2520deformable%2520nature%2520of%2520garments.%2520Traditional%250Aapproaches%2520typically%2520require%2520separate%2520models%2520for%2520each%2520garment%2520type%252C%2520which%250Alimits%2520scalability%2520and%2520adaptability.%2520In%2520contrast%252C%2520this%2520paper%2520presents%2520a%2520unified%250Aapproach%2520using%2520vision-language%2520models%2520%2528VLMs%2529%2520to%2520improve%2520keypoint%2520prediction%250Aacross%2520various%2520garment%2520categories.%2520By%2520interpreting%2520both%2520visual%2520and%2520semantic%250Ainformation%252C%2520our%2520model%2520enables%2520robots%2520to%2520manage%2520different%2520garment%2520states%2520with%2520a%250Asingle%2520model.%2520We%2520created%2520a%2520large-scale%2520synthetic%2520dataset%2520using%2520advanced%250Asimulation%2520techniques%252C%2520allowing%2520scalable%2520training%2520without%2520extensive%2520real-world%250Adata.%2520Experimental%2520results%2520indicate%2520that%2520the%2520VLM-based%2520method%2520significantly%250Aenhances%2520keypoint%2520detection%2520accuracy%2520and%2520task%2520success%2520rates%252C%2520providing%2520a%2520more%250Aflexible%2520and%2520general%2520solution%2520for%2520robotic%2520garment%2520manipulation.%2520In%2520addition%252C%250Athis%2520research%2520also%2520underscores%2520the%2520potential%2520of%2520VLMs%2520to%2520unify%2520various%2520garment%250Amanipulation%2520tasks%2520within%2520a%2520single%2520framework%252C%2520paving%2520the%2520way%2520for%2520broader%250Aapplications%2520in%2520home%2520automation%2520and%2520assistive%2520robotics%2520for%2520future.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2409.18082v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=SKT%3A%20Integrating%20State-Aware%20Keypoint%20Trajectories%20with%20Vision-Language%0A%20%20Models%20for%20Robotic%20Garment%20Manipulation&entry.906535625=Xin%20Li%20and%20Siyuan%20Huang%20and%20Qiaojun%20Yu%20and%20Zhengkai%20Jiang%20and%20Ce%20Hao%20and%20Yimeng%20Zhu%20and%20Hongsheng%20Li%20and%20Peng%20Gao%20and%20Cewu%20Lu&entry.1292438233=%20%20Automating%20garment%20manipulation%20poses%20a%20significant%20challenge%20for%20assistive%0Arobotics%20due%20to%20the%20diverse%20and%20deformable%20nature%20of%20garments.%20Traditional%0Aapproaches%20typically%20require%20separate%20models%20for%20each%20garment%20type%2C%20which%0Alimits%20scalability%20and%20adaptability.%20In%20contrast%2C%20this%20paper%20presents%20a%20unified%0Aapproach%20using%20vision-language%20models%20%28VLMs%29%20to%20improve%20keypoint%20prediction%0Aacross%20various%20garment%20categories.%20By%20interpreting%20both%20visual%20and%20semantic%0Ainformation%2C%20our%20model%20enables%20robots%20to%20manage%20different%20garment%20states%20with%20a%0Asingle%20model.%20We%20created%20a%20large-scale%20synthetic%20dataset%20using%20advanced%0Asimulation%20techniques%2C%20allowing%20scalable%20training%20without%20extensive%20real-world%0Adata.%20Experimental%20results%20indicate%20that%20the%20VLM-based%20method%20significantly%0Aenhances%20keypoint%20detection%20accuracy%20and%20task%20success%20rates%2C%20providing%20a%20more%0Aflexible%20and%20general%20solution%20for%20robotic%20garment%20manipulation.%20In%20addition%2C%0Athis%20research%20also%20underscores%20the%20potential%20of%20VLMs%20to%20unify%20various%20garment%0Amanipulation%20tasks%20within%20a%20single%20framework%2C%20paving%20the%20way%20for%20broader%0Aapplications%20in%20home%20automation%20and%20assistive%20robotics%20for%20future.%0A&entry.1838667208=http%3A//arxiv.org/abs/2409.18082v2&entry.124074799=Read"},
{"title": "Neural Collaborative Filtering to Detect Anomalies in Human Semantic\n  Trajectories", "author": "Yueyang Liu and Lance Kennedy and Hossein Amiri and Andreas Z\u00fcfle", "abstract": "  Human trajectory anomaly detection has become increasingly important across a\nwide range of applications, including security surveillance and public health.\nHowever, existing trajectory anomaly detection methods are primarily focused on\nvehicle-level traffic, while human-level trajectory anomaly detection remains\nunder-explored. Since human trajectory data is often very sparse, machine\nlearning methods have become the preferred approach for identifying complex\npatterns. However, concerns regarding potential biases and the robustness of\nthese models have intensified the demand for more transparent and explainable\nalternatives. In response to these challenges, our research focuses on\ndeveloping a lightweight anomaly detection model specifically designed to\ndetect anomalies in human trajectories. We propose a Neural Collaborative\nFiltering approach to model and predict normal mobility. Our method is designed\nto model users' daily patterns of life without requiring prior knowledge,\nthereby enhancing performance in scenarios where data is sparse or incomplete,\nsuch as in cold start situations. Our algorithm consists of two main modules.\nThe first is the collaborative filtering module, which applies collaborative\nfiltering to model normal mobility of individual humans to places of interest.\nThe second is the neural module, responsible for interpreting the complex\nspatio-temporal relationships inherent in human trajectory data. To validate\nour approach, we conducted extensive experiments using simulated and real-world\ndatasets comparing to numerous state-of-the-art trajectory anomaly detection\napproaches.\n", "link": "http://arxiv.org/abs/2409.18427v2", "date": "2024-10-07", "relevancy": 1.5911, "topK": [{"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.5828}, {"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.5179}, {"title": "Multimodal-Conditioned Latent Diffusion Models for Fashion Image Editing", "link": "http://arxiv.org/abs/2403.14828v2", "similarity": 0.5092}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Neural%20Collaborative%20Filtering%20to%20Detect%20Anomalies%20in%20Human%20Semantic%0A%20%20Trajectories&body=Title%3A%20Neural%20Collaborative%20Filtering%20to%20Detect%20Anomalies%20in%20Human%20Semantic%0A%20%20Trajectories%0AAuthor%3A%20Yueyang%20Liu%20and%20Lance%20Kennedy%20and%20Hossein%20Amiri%20and%20Andreas%20Z%C3%BCfle%0AAbstract%3A%20%20%20Human%20trajectory%20anomaly%20detection%20has%20become%20increasingly%20important%20across%20a%0Awide%20range%20of%20applications%2C%20including%20security%20surveillance%20and%20public%20health.%0AHowever%2C%20existing%20trajectory%20anomaly%20detection%20methods%20are%20primarily%20focused%20on%0Avehicle-level%20traffic%2C%20while%20human-level%20trajectory%20anomaly%20detection%20remains%0Aunder-explored.%20Since%20human%20trajectory%20data%20is%20often%20very%20sparse%2C%20machine%0Alearning%20methods%20have%20become%20the%20preferred%20approach%20for%20identifying%20complex%0Apatterns.%20However%2C%20concerns%20regarding%20potential%20biases%20and%20the%20robustness%20of%0Athese%20models%20have%20intensified%20the%20demand%20for%20more%20transparent%20and%20explainable%0Aalternatives.%20In%20response%20to%20these%20challenges%2C%20our%20research%20focuses%20on%0Adeveloping%20a%20lightweight%20anomaly%20detection%20model%20specifically%20designed%20to%0Adetect%20anomalies%20in%20human%20trajectories.%20We%20propose%20a%20Neural%20Collaborative%0AFiltering%20approach%20to%20model%20and%20predict%20normal%20mobility.%20Our%20method%20is%20designed%0Ato%20model%20users%27%20daily%20patterns%20of%20life%20without%20requiring%20prior%20knowledge%2C%0Athereby%20enhancing%20performance%20in%20scenarios%20where%20data%20is%20sparse%20or%20incomplete%2C%0Asuch%20as%20in%20cold%20start%20situations.%20Our%20algorithm%20consists%20of%20two%20main%20modules.%0AThe%20first%20is%20the%20collaborative%20filtering%20module%2C%20which%20applies%20collaborative%0Afiltering%20to%20model%20normal%20mobility%20of%20individual%20humans%20to%20places%20of%20interest.%0AThe%20second%20is%20the%20neural%20module%2C%20responsible%20for%20interpreting%20the%20complex%0Aspatio-temporal%20relationships%20inherent%20in%20human%20trajectory%20data.%20To%20validate%0Aour%20approach%2C%20we%20conducted%20extensive%20experiments%20using%20simulated%20and%20real-world%0Adatasets%20comparing%20to%20numerous%20state-of-the-art%20trajectory%20anomaly%20detection%0Aapproaches.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2409.18427v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DNeural%2520Collaborative%2520Filtering%2520to%2520Detect%2520Anomalies%2520in%2520Human%2520Semantic%250A%2520%2520Trajectories%26entry.906535625%3DYueyang%2520Liu%2520and%2520Lance%2520Kennedy%2520and%2520Hossein%2520Amiri%2520and%2520Andreas%2520Z%25C3%25BCfle%26entry.1292438233%3D%2520%2520Human%2520trajectory%2520anomaly%2520detection%2520has%2520become%2520increasingly%2520important%2520across%2520a%250Awide%2520range%2520of%2520applications%252C%2520including%2520security%2520surveillance%2520and%2520public%2520health.%250AHowever%252C%2520existing%2520trajectory%2520anomaly%2520detection%2520methods%2520are%2520primarily%2520focused%2520on%250Avehicle-level%2520traffic%252C%2520while%2520human-level%2520trajectory%2520anomaly%2520detection%2520remains%250Aunder-explored.%2520Since%2520human%2520trajectory%2520data%2520is%2520often%2520very%2520sparse%252C%2520machine%250Alearning%2520methods%2520have%2520become%2520the%2520preferred%2520approach%2520for%2520identifying%2520complex%250Apatterns.%2520However%252C%2520concerns%2520regarding%2520potential%2520biases%2520and%2520the%2520robustness%2520of%250Athese%2520models%2520have%2520intensified%2520the%2520demand%2520for%2520more%2520transparent%2520and%2520explainable%250Aalternatives.%2520In%2520response%2520to%2520these%2520challenges%252C%2520our%2520research%2520focuses%2520on%250Adeveloping%2520a%2520lightweight%2520anomaly%2520detection%2520model%2520specifically%2520designed%2520to%250Adetect%2520anomalies%2520in%2520human%2520trajectories.%2520We%2520propose%2520a%2520Neural%2520Collaborative%250AFiltering%2520approach%2520to%2520model%2520and%2520predict%2520normal%2520mobility.%2520Our%2520method%2520is%2520designed%250Ato%2520model%2520users%2527%2520daily%2520patterns%2520of%2520life%2520without%2520requiring%2520prior%2520knowledge%252C%250Athereby%2520enhancing%2520performance%2520in%2520scenarios%2520where%2520data%2520is%2520sparse%2520or%2520incomplete%252C%250Asuch%2520as%2520in%2520cold%2520start%2520situations.%2520Our%2520algorithm%2520consists%2520of%2520two%2520main%2520modules.%250AThe%2520first%2520is%2520the%2520collaborative%2520filtering%2520module%252C%2520which%2520applies%2520collaborative%250Afiltering%2520to%2520model%2520normal%2520mobility%2520of%2520individual%2520humans%2520to%2520places%2520of%2520interest.%250AThe%2520second%2520is%2520the%2520neural%2520module%252C%2520responsible%2520for%2520interpreting%2520the%2520complex%250Aspatio-temporal%2520relationships%2520inherent%2520in%2520human%2520trajectory%2520data.%2520To%2520validate%250Aour%2520approach%252C%2520we%2520conducted%2520extensive%2520experiments%2520using%2520simulated%2520and%2520real-world%250Adatasets%2520comparing%2520to%2520numerous%2520state-of-the-art%2520trajectory%2520anomaly%2520detection%250Aapproaches.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2409.18427v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Neural%20Collaborative%20Filtering%20to%20Detect%20Anomalies%20in%20Human%20Semantic%0A%20%20Trajectories&entry.906535625=Yueyang%20Liu%20and%20Lance%20Kennedy%20and%20Hossein%20Amiri%20and%20Andreas%20Z%C3%BCfle&entry.1292438233=%20%20Human%20trajectory%20anomaly%20detection%20has%20become%20increasingly%20important%20across%20a%0Awide%20range%20of%20applications%2C%20including%20security%20surveillance%20and%20public%20health.%0AHowever%2C%20existing%20trajectory%20anomaly%20detection%20methods%20are%20primarily%20focused%20on%0Avehicle-level%20traffic%2C%20while%20human-level%20trajectory%20anomaly%20detection%20remains%0Aunder-explored.%20Since%20human%20trajectory%20data%20is%20often%20very%20sparse%2C%20machine%0Alearning%20methods%20have%20become%20the%20preferred%20approach%20for%20identifying%20complex%0Apatterns.%20However%2C%20concerns%20regarding%20potential%20biases%20and%20the%20robustness%20of%0Athese%20models%20have%20intensified%20the%20demand%20for%20more%20transparent%20and%20explainable%0Aalternatives.%20In%20response%20to%20these%20challenges%2C%20our%20research%20focuses%20on%0Adeveloping%20a%20lightweight%20anomaly%20detection%20model%20specifically%20designed%20to%0Adetect%20anomalies%20in%20human%20trajectories.%20We%20propose%20a%20Neural%20Collaborative%0AFiltering%20approach%20to%20model%20and%20predict%20normal%20mobility.%20Our%20method%20is%20designed%0Ato%20model%20users%27%20daily%20patterns%20of%20life%20without%20requiring%20prior%20knowledge%2C%0Athereby%20enhancing%20performance%20in%20scenarios%20where%20data%20is%20sparse%20or%20incomplete%2C%0Asuch%20as%20in%20cold%20start%20situations.%20Our%20algorithm%20consists%20of%20two%20main%20modules.%0AThe%20first%20is%20the%20collaborative%20filtering%20module%2C%20which%20applies%20collaborative%0Afiltering%20to%20model%20normal%20mobility%20of%20individual%20humans%20to%20places%20of%20interest.%0AThe%20second%20is%20the%20neural%20module%2C%20responsible%20for%20interpreting%20the%20complex%0Aspatio-temporal%20relationships%20inherent%20in%20human%20trajectory%20data.%20To%20validate%0Aour%20approach%2C%20we%20conducted%20extensive%20experiments%20using%20simulated%20and%20real-world%0Adatasets%20comparing%20to%20numerous%20state-of-the-art%20trajectory%20anomaly%20detection%0Aapproaches.%0A&entry.1838667208=http%3A//arxiv.org/abs/2409.18427v2&entry.124074799=Read"},
{"title": "QMP: Q-switch Mixture of Policies for Multi-Task Behavior Sharing", "author": "Grace Zhang and Ayush Jain and Injune Hwang and Shao-Hua Sun and Joseph J. Lim", "abstract": "  Multi-task reinforcement learning (MTRL) aims to learn several tasks\nsimultaneously for better sample efficiency than learning them separately.\nTraditional methods achieve this by sharing parameters or relabeled data\nbetween tasks. In this work, we introduce a new framework for sharing\nbehavioral policies across tasks, which can be used in addition to existing\nMTRL methods. The key idea is to improve each task's off-policy data collection\nby employing behaviors from other task policies. Selectively sharing helpful\nbehaviors acquired in one task to collect training data for another task can\nlead to higher-quality trajectories, leading to more sample-efficient MTRL.\nThus, we introduce a simple and principled framework called Q-switch mixture of\npolicies (QMP) that selectively shares behavior between different task policies\nby using the task's Q-function to evaluate and select useful shareable\nbehaviors. We theoretically analyze how QMP improves the sample efficiency of\nthe underlying RL algorithm. Our experiments show that QMP's behavioral policy\nsharing provides complementary gains over many popular MTRL algorithms and\noutperforms alternative ways to share behaviors in various manipulation,\nlocomotion, and navigation environments. Videos are available at\nhttps://qmp-mtrl.github.io.\n", "link": "http://arxiv.org/abs/2302.00671v2", "date": "2024-10-07", "relevancy": 1.9128, "topK": [{"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.5143}, {"title": "Safe and efficient training of a control agent", "link": "https://patents.google.com/patent/US11709462B2/en", "similarity": 0.4743}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.4676}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20QMP%3A%20Q-switch%20Mixture%20of%20Policies%20for%20Multi-Task%20Behavior%20Sharing&body=Title%3A%20QMP%3A%20Q-switch%20Mixture%20of%20Policies%20for%20Multi-Task%20Behavior%20Sharing%0AAuthor%3A%20Grace%20Zhang%20and%20Ayush%20Jain%20and%20Injune%20Hwang%20and%20Shao-Hua%20Sun%20and%20Joseph%20J.%20Lim%0AAbstract%3A%20%20%20Multi-task%20reinforcement%20learning%20%28MTRL%29%20aims%20to%20learn%20several%20tasks%0Asimultaneously%20for%20better%20sample%20efficiency%20than%20learning%20them%20separately.%0ATraditional%20methods%20achieve%20this%20by%20sharing%20parameters%20or%20relabeled%20data%0Abetween%20tasks.%20In%20this%20work%2C%20we%20introduce%20a%20new%20framework%20for%20sharing%0Abehavioral%20policies%20across%20tasks%2C%20which%20can%20be%20used%20in%20addition%20to%20existing%0AMTRL%20methods.%20The%20key%20idea%20is%20to%20improve%20each%20task%27s%20off-policy%20data%20collection%0Aby%20employing%20behaviors%20from%20other%20task%20policies.%20Selectively%20sharing%20helpful%0Abehaviors%20acquired%20in%20one%20task%20to%20collect%20training%20data%20for%20another%20task%20can%0Alead%20to%20higher-quality%20trajectories%2C%20leading%20to%20more%20sample-efficient%20MTRL.%0AThus%2C%20we%20introduce%20a%20simple%20and%20principled%20framework%20called%20Q-switch%20mixture%20of%0Apolicies%20%28QMP%29%20that%20selectively%20shares%20behavior%20between%20different%20task%20policies%0Aby%20using%20the%20task%27s%20Q-function%20to%20evaluate%20and%20select%20useful%20shareable%0Abehaviors.%20We%20theoretically%20analyze%20how%20QMP%20improves%20the%20sample%20efficiency%20of%0Athe%20underlying%20RL%20algorithm.%20Our%20experiments%20show%20that%20QMP%27s%20behavioral%20policy%0Asharing%20provides%20complementary%20gains%20over%20many%20popular%20MTRL%20algorithms%20and%0Aoutperforms%20alternative%20ways%20to%20share%20behaviors%20in%20various%20manipulation%2C%0Alocomotion%2C%20and%20navigation%20environments.%20Videos%20are%20available%20at%0Ahttps%3A//qmp-mtrl.github.io.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2302.00671v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DQMP%253A%2520Q-switch%2520Mixture%2520of%2520Policies%2520for%2520Multi-Task%2520Behavior%2520Sharing%26entry.906535625%3DGrace%2520Zhang%2520and%2520Ayush%2520Jain%2520and%2520Injune%2520Hwang%2520and%2520Shao-Hua%2520Sun%2520and%2520Joseph%2520J.%2520Lim%26entry.1292438233%3D%2520%2520Multi-task%2520reinforcement%2520learning%2520%2528MTRL%2529%2520aims%2520to%2520learn%2520several%2520tasks%250Asimultaneously%2520for%2520better%2520sample%2520efficiency%2520than%2520learning%2520them%2520separately.%250ATraditional%2520methods%2520achieve%2520this%2520by%2520sharing%2520parameters%2520or%2520relabeled%2520data%250Abetween%2520tasks.%2520In%2520this%2520work%252C%2520we%2520introduce%2520a%2520new%2520framework%2520for%2520sharing%250Abehavioral%2520policies%2520across%2520tasks%252C%2520which%2520can%2520be%2520used%2520in%2520addition%2520to%2520existing%250AMTRL%2520methods.%2520The%2520key%2520idea%2520is%2520to%2520improve%2520each%2520task%2527s%2520off-policy%2520data%2520collection%250Aby%2520employing%2520behaviors%2520from%2520other%2520task%2520policies.%2520Selectively%2520sharing%2520helpful%250Abehaviors%2520acquired%2520in%2520one%2520task%2520to%2520collect%2520training%2520data%2520for%2520another%2520task%2520can%250Alead%2520to%2520higher-quality%2520trajectories%252C%2520leading%2520to%2520more%2520sample-efficient%2520MTRL.%250AThus%252C%2520we%2520introduce%2520a%2520simple%2520and%2520principled%2520framework%2520called%2520Q-switch%2520mixture%2520of%250Apolicies%2520%2528QMP%2529%2520that%2520selectively%2520shares%2520behavior%2520between%2520different%2520task%2520policies%250Aby%2520using%2520the%2520task%2527s%2520Q-function%2520to%2520evaluate%2520and%2520select%2520useful%2520shareable%250Abehaviors.%2520We%2520theoretically%2520analyze%2520how%2520QMP%2520improves%2520the%2520sample%2520efficiency%2520of%250Athe%2520underlying%2520RL%2520algorithm.%2520Our%2520experiments%2520show%2520that%2520QMP%2527s%2520behavioral%2520policy%250Asharing%2520provides%2520complementary%2520gains%2520over%2520many%2520popular%2520MTRL%2520algorithms%2520and%250Aoutperforms%2520alternative%2520ways%2520to%2520share%2520behaviors%2520in%2520various%2520manipulation%252C%250Alocomotion%252C%2520and%2520navigation%2520environments.%2520Videos%2520are%2520available%2520at%250Ahttps%253A//qmp-mtrl.github.io.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2302.00671v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=QMP%3A%20Q-switch%20Mixture%20of%20Policies%20for%20Multi-Task%20Behavior%20Sharing&entry.906535625=Grace%20Zhang%20and%20Ayush%20Jain%20and%20Injune%20Hwang%20and%20Shao-Hua%20Sun%20and%20Joseph%20J.%20Lim&entry.1292438233=%20%20Multi-task%20reinforcement%20learning%20%28MTRL%29%20aims%20to%20learn%20several%20tasks%0Asimultaneously%20for%20better%20sample%20efficiency%20than%20learning%20them%20separately.%0ATraditional%20methods%20achieve%20this%20by%20sharing%20parameters%20or%20relabeled%20data%0Abetween%20tasks.%20In%20this%20work%2C%20we%20introduce%20a%20new%20framework%20for%20sharing%0Abehavioral%20policies%20across%20tasks%2C%20which%20can%20be%20used%20in%20addition%20to%20existing%0AMTRL%20methods.%20The%20key%20idea%20is%20to%20improve%20each%20task%27s%20off-policy%20data%20collection%0Aby%20employing%20behaviors%20from%20other%20task%20policies.%20Selectively%20sharing%20helpful%0Abehaviors%20acquired%20in%20one%20task%20to%20collect%20training%20data%20for%20another%20task%20can%0Alead%20to%20higher-quality%20trajectories%2C%20leading%20to%20more%20sample-efficient%20MTRL.%0AThus%2C%20we%20introduce%20a%20simple%20and%20principled%20framework%20called%20Q-switch%20mixture%20of%0Apolicies%20%28QMP%29%20that%20selectively%20shares%20behavior%20between%20different%20task%20policies%0Aby%20using%20the%20task%27s%20Q-function%20to%20evaluate%20and%20select%20useful%20shareable%0Abehaviors.%20We%20theoretically%20analyze%20how%20QMP%20improves%20the%20sample%20efficiency%20of%0Athe%20underlying%20RL%20algorithm.%20Our%20experiments%20show%20that%20QMP%27s%20behavioral%20policy%0Asharing%20provides%20complementary%20gains%20over%20many%20popular%20MTRL%20algorithms%20and%0Aoutperforms%20alternative%20ways%20to%20share%20behaviors%20in%20various%20manipulation%2C%0Alocomotion%2C%20and%20navigation%20environments.%20Videos%20are%20available%20at%0Ahttps%3A//qmp-mtrl.github.io.%0A&entry.1838667208=http%3A//arxiv.org/abs/2302.00671v2&entry.124074799=Read"},
{"title": "Safe Learning-Based Optimization of Model Predictive Control:\n  Application to Battery Fast-Charging", "author": "Sebastian Hirt and Andreas H\u00f6hl and Johannes Pohlodek and Joachim Schaeffer and Maik Pfefferkorn and Richard D. Braatz and Rolf Findeisen", "abstract": "  Model predictive control (MPC) is a powerful tool for controlling complex\nnonlinear systems under constraints, but often struggles with model\nuncertainties and the design of suitable cost functions. To address these\nchallenges, we discuss an approach that integrates MPC with safe Bayesian\noptimization to optimize long-term closed-loop performance despite significant\nmodel-plant mismatches. By parameterizing the MPC stage cost function using a\nradial basis function network, we employ Bayesian optimization as a\nmulti-episode learning strategy to tune the controller without relying on\nprecise system models. This method mitigates conservativeness introduced by\noverly cautious soft constraints in the MPC cost function and provides\nprobabilistic safety guarantees during learning, ensuring that safety-critical\nconstraints are met with high probability. As a practical application, we apply\nour approach to fast charging of lithium-ion batteries, a challenging task due\nto the complicated battery dynamics and strict safety requirements, subject to\nthe requirement to be implementable in real time. Simulation results\ndemonstrate that, in the context of model-plant mismatch, our method reduces\ncharging times compared to traditional MPC methods while maintaining safety.\nThis work extends previous research by emphasizing closed-loop constraint\nsatisfaction and offers a promising solution for enhancing performance in\nsystems where model uncertainties and safety are critical concerns.\n", "link": "http://arxiv.org/abs/2410.04982v1", "date": "2024-10-07", "relevancy": 2.0043, "topK": [{"title": "Safe and efficient training of a control agent", "link": "https://patents.google.com/patent/US11709462B2/en", "similarity": 0.5425}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.5256}, {"title": "Boosted dynamic neural networks", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/26302", "similarity": 0.4499}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Safe%20Learning-Based%20Optimization%20of%20Model%20Predictive%20Control%3A%0A%20%20Application%20to%20Battery%20Fast-Charging&body=Title%3A%20Safe%20Learning-Based%20Optimization%20of%20Model%20Predictive%20Control%3A%0A%20%20Application%20to%20Battery%20Fast-Charging%0AAuthor%3A%20Sebastian%20Hirt%20and%20Andreas%20H%C3%B6hl%20and%20Johannes%20Pohlodek%20and%20Joachim%20Schaeffer%20and%20Maik%20Pfefferkorn%20and%20Richard%20D.%20Braatz%20and%20Rolf%20Findeisen%0AAbstract%3A%20%20%20Model%20predictive%20control%20%28MPC%29%20is%20a%20powerful%20tool%20for%20controlling%20complex%0Anonlinear%20systems%20under%20constraints%2C%20but%20often%20struggles%20with%20model%0Auncertainties%20and%20the%20design%20of%20suitable%20cost%20functions.%20To%20address%20these%0Achallenges%2C%20we%20discuss%20an%20approach%20that%20integrates%20MPC%20with%20safe%20Bayesian%0Aoptimization%20to%20optimize%20long-term%20closed-loop%20performance%20despite%20significant%0Amodel-plant%20mismatches.%20By%20parameterizing%20the%20MPC%20stage%20cost%20function%20using%20a%0Aradial%20basis%20function%20network%2C%20we%20employ%20Bayesian%20optimization%20as%20a%0Amulti-episode%20learning%20strategy%20to%20tune%20the%20controller%20without%20relying%20on%0Aprecise%20system%20models.%20This%20method%20mitigates%20conservativeness%20introduced%20by%0Aoverly%20cautious%20soft%20constraints%20in%20the%20MPC%20cost%20function%20and%20provides%0Aprobabilistic%20safety%20guarantees%20during%20learning%2C%20ensuring%20that%20safety-critical%0Aconstraints%20are%20met%20with%20high%20probability.%20As%20a%20practical%20application%2C%20we%20apply%0Aour%20approach%20to%20fast%20charging%20of%20lithium-ion%20batteries%2C%20a%20challenging%20task%20due%0Ato%20the%20complicated%20battery%20dynamics%20and%20strict%20safety%20requirements%2C%20subject%20to%0Athe%20requirement%20to%20be%20implementable%20in%20real%20time.%20Simulation%20results%0Ademonstrate%20that%2C%20in%20the%20context%20of%20model-plant%20mismatch%2C%20our%20method%20reduces%0Acharging%20times%20compared%20to%20traditional%20MPC%20methods%20while%20maintaining%20safety.%0AThis%20work%20extends%20previous%20research%20by%20emphasizing%20closed-loop%20constraint%0Asatisfaction%20and%20offers%20a%20promising%20solution%20for%20enhancing%20performance%20in%0Asystems%20where%20model%20uncertainties%20and%20safety%20are%20critical%20concerns.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2410.04982v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DSafe%2520Learning-Based%2520Optimization%2520of%2520Model%2520Predictive%2520Control%253A%250A%2520%2520Application%2520to%2520Battery%2520Fast-Charging%26entry.906535625%3DSebastian%2520Hirt%2520and%2520Andreas%2520H%25C3%25B6hl%2520and%2520Johannes%2520Pohlodek%2520and%2520Joachim%2520Schaeffer%2520and%2520Maik%2520Pfefferkorn%2520and%2520Richard%2520D.%2520Braatz%2520and%2520Rolf%2520Findeisen%26entry.1292438233%3D%2520%2520Model%2520predictive%2520control%2520%2528MPC%2529%2520is%2520a%2520powerful%2520tool%2520for%2520controlling%2520complex%250Anonlinear%2520systems%2520under%2520constraints%252C%2520but%2520often%2520struggles%2520with%2520model%250Auncertainties%2520and%2520the%2520design%2520of%2520suitable%2520cost%2520functions.%2520To%2520address%2520these%250Achallenges%252C%2520we%2520discuss%2520an%2520approach%2520that%2520integrates%2520MPC%2520with%2520safe%2520Bayesian%250Aoptimization%2520to%2520optimize%2520long-term%2520closed-loop%2520performance%2520despite%2520significant%250Amodel-plant%2520mismatches.%2520By%2520parameterizing%2520the%2520MPC%2520stage%2520cost%2520function%2520using%2520a%250Aradial%2520basis%2520function%2520network%252C%2520we%2520employ%2520Bayesian%2520optimization%2520as%2520a%250Amulti-episode%2520learning%2520strategy%2520to%2520tune%2520the%2520controller%2520without%2520relying%2520on%250Aprecise%2520system%2520models.%2520This%2520method%2520mitigates%2520conservativeness%2520introduced%2520by%250Aoverly%2520cautious%2520soft%2520constraints%2520in%2520the%2520MPC%2520cost%2520function%2520and%2520provides%250Aprobabilistic%2520safety%2520guarantees%2520during%2520learning%252C%2520ensuring%2520that%2520safety-critical%250Aconstraints%2520are%2520met%2520with%2520high%2520probability.%2520As%2520a%2520practical%2520application%252C%2520we%2520apply%250Aour%2520approach%2520to%2520fast%2520charging%2520of%2520lithium-ion%2520batteries%252C%2520a%2520challenging%2520task%2520due%250Ato%2520the%2520complicated%2520battery%2520dynamics%2520and%2520strict%2520safety%2520requirements%252C%2520subject%2520to%250Athe%2520requirement%2520to%2520be%2520implementable%2520in%2520real%2520time.%2520Simulation%2520results%250Ademonstrate%2520that%252C%2520in%2520the%2520context%2520of%2520model-plant%2520mismatch%252C%2520our%2520method%2520reduces%250Acharging%2520times%2520compared%2520to%2520traditional%2520MPC%2520methods%2520while%2520maintaining%2520safety.%250AThis%2520work%2520extends%2520previous%2520research%2520by%2520emphasizing%2520closed-loop%2520constraint%250Asatisfaction%2520and%2520offers%2520a%2520promising%2520solution%2520for%2520enhancing%2520performance%2520in%250Asystems%2520where%2520model%2520uncertainties%2520and%2520safety%2520are%2520critical%2520concerns.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2410.04982v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Safe%20Learning-Based%20Optimization%20of%20Model%20Predictive%20Control%3A%0A%20%20Application%20to%20Battery%20Fast-Charging&entry.906535625=Sebastian%20Hirt%20and%20Andreas%20H%C3%B6hl%20and%20Johannes%20Pohlodek%20and%20Joachim%20Schaeffer%20and%20Maik%20Pfefferkorn%20and%20Richard%20D.%20Braatz%20and%20Rolf%20Findeisen&entry.1292438233=%20%20Model%20predictive%20control%20%28MPC%29%20is%20a%20powerful%20tool%20for%20controlling%20complex%0Anonlinear%20systems%20under%20constraints%2C%20but%20often%20struggles%20with%20model%0Auncertainties%20and%20the%20design%20of%20suitable%20cost%20functions.%20To%20address%20these%0Achallenges%2C%20we%20discuss%20an%20approach%20that%20integrates%20MPC%20with%20safe%20Bayesian%0Aoptimization%20to%20optimize%20long-term%20closed-loop%20performance%20despite%20significant%0Amodel-plant%20mismatches.%20By%20parameterizing%20the%20MPC%20stage%20cost%20function%20using%20a%0Aradial%20basis%20function%20network%2C%20we%20employ%20Bayesian%20optimization%20as%20a%0Amulti-episode%20learning%20strategy%20to%20tune%20the%20controller%20without%20relying%20on%0Aprecise%20system%20models.%20This%20method%20mitigates%20conservativeness%20introduced%20by%0Aoverly%20cautious%20soft%20constraints%20in%20the%20MPC%20cost%20function%20and%20provides%0Aprobabilistic%20safety%20guarantees%20during%20learning%2C%20ensuring%20that%20safety-critical%0Aconstraints%20are%20met%20with%20high%20probability.%20As%20a%20practical%20application%2C%20we%20apply%0Aour%20approach%20to%20fast%20charging%20of%20lithium-ion%20batteries%2C%20a%20challenging%20task%20due%0Ato%20the%20complicated%20battery%20dynamics%20and%20strict%20safety%20requirements%2C%20subject%20to%0Athe%20requirement%20to%20be%20implementable%20in%20real%20time.%20Simulation%20results%0Ademonstrate%20that%2C%20in%20the%20context%20of%20model-plant%20mismatch%2C%20our%20method%20reduces%0Acharging%20times%20compared%20to%20traditional%20MPC%20methods%20while%20maintaining%20safety.%0AThis%20work%20extends%20previous%20research%20by%20emphasizing%20closed-loop%20constraint%0Asatisfaction%20and%20offers%20a%20promising%20solution%20for%20enhancing%20performance%20in%0Asystems%20where%20model%20uncertainties%20and%20safety%20are%20critical%20concerns.%0A&entry.1838667208=http%3A//arxiv.org/abs/2410.04982v1&entry.124074799=Read"},
{"title": "Collaboration! Towards Robust Neural Methods for Routing Problems", "author": "Jianan Zhou and Yaoxin Wu and Zhiguang Cao and Wen Song and Jie Zhang and Zhiqi Shen", "abstract": "  Despite enjoying desirable efficiency and reduced reliance on domain\nexpertise, existing neural methods for vehicle routing problems (VRPs) suffer\nfrom severe robustness issues -- their performance significantly deteriorates\non clean instances with crafted perturbations. To enhance robustness, we\npropose an ensemble-based Collaborative Neural Framework (CNF) w.r.t. the\ndefense of neural VRP methods, which is crucial yet underexplored in the\nliterature. Given a neural VRP method, we adversarially train multiple models\nin a collaborative manner to synergistically promote robustness against\nattacks, while boosting standard generalization on clean instances. A neural\nrouter is designed to adeptly distribute training instances among models,\nenhancing overall load balancing and collaborative efficacy. Extensive\nexperiments verify the effectiveness and versatility of CNF in defending\nagainst various attacks across different neural VRP methods. Notably, our\napproach also achieves impressive out-of-distribution generalization on\nbenchmark instances.\n", "link": "http://arxiv.org/abs/2410.04968v1", "date": "2024-10-07", "relevancy": 2.0042, "topK": [{"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.5139}, {"title": "Boosted dynamic neural networks", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/26302", "similarity": 0.4936}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.4875}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Collaboration%21%20Towards%20Robust%20Neural%20Methods%20for%20Routing%20Problems&body=Title%3A%20Collaboration%21%20Towards%20Robust%20Neural%20Methods%20for%20Routing%20Problems%0AAuthor%3A%20Jianan%20Zhou%20and%20Yaoxin%20Wu%20and%20Zhiguang%20Cao%20and%20Wen%20Song%20and%20Jie%20Zhang%20and%20Zhiqi%20Shen%0AAbstract%3A%20%20%20Despite%20enjoying%20desirable%20efficiency%20and%20reduced%20reliance%20on%20domain%0Aexpertise%2C%20existing%20neural%20methods%20for%20vehicle%20routing%20problems%20%28VRPs%29%20suffer%0Afrom%20severe%20robustness%20issues%20--%20their%20performance%20significantly%20deteriorates%0Aon%20clean%20instances%20with%20crafted%20perturbations.%20To%20enhance%20robustness%2C%20we%0Apropose%20an%20ensemble-based%20Collaborative%20Neural%20Framework%20%28CNF%29%20w.r.t.%20the%0Adefense%20of%20neural%20VRP%20methods%2C%20which%20is%20crucial%20yet%20underexplored%20in%20the%0Aliterature.%20Given%20a%20neural%20VRP%20method%2C%20we%20adversarially%20train%20multiple%20models%0Ain%20a%20collaborative%20manner%20to%20synergistically%20promote%20robustness%20against%0Aattacks%2C%20while%20boosting%20standard%20generalization%20on%20clean%20instances.%20A%20neural%0Arouter%20is%20designed%20to%20adeptly%20distribute%20training%20instances%20among%20models%2C%0Aenhancing%20overall%20load%20balancing%20and%20collaborative%20efficacy.%20Extensive%0Aexperiments%20verify%20the%20effectiveness%20and%20versatility%20of%20CNF%20in%20defending%0Aagainst%20various%20attacks%20across%20different%20neural%20VRP%20methods.%20Notably%2C%20our%0Aapproach%20also%20achieves%20impressive%20out-of-distribution%20generalization%20on%0Abenchmark%20instances.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2410.04968v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DCollaboration%2521%2520Towards%2520Robust%2520Neural%2520Methods%2520for%2520Routing%2520Problems%26entry.906535625%3DJianan%2520Zhou%2520and%2520Yaoxin%2520Wu%2520and%2520Zhiguang%2520Cao%2520and%2520Wen%2520Song%2520and%2520Jie%2520Zhang%2520and%2520Zhiqi%2520Shen%26entry.1292438233%3D%2520%2520Despite%2520enjoying%2520desirable%2520efficiency%2520and%2520reduced%2520reliance%2520on%2520domain%250Aexpertise%252C%2520existing%2520neural%2520methods%2520for%2520vehicle%2520routing%2520problems%2520%2528VRPs%2529%2520suffer%250Afrom%2520severe%2520robustness%2520issues%2520--%2520their%2520performance%2520significantly%2520deteriorates%250Aon%2520clean%2520instances%2520with%2520crafted%2520perturbations.%2520To%2520enhance%2520robustness%252C%2520we%250Apropose%2520an%2520ensemble-based%2520Collaborative%2520Neural%2520Framework%2520%2528CNF%2529%2520w.r.t.%2520the%250Adefense%2520of%2520neural%2520VRP%2520methods%252C%2520which%2520is%2520crucial%2520yet%2520underexplored%2520in%2520the%250Aliterature.%2520Given%2520a%2520neural%2520VRP%2520method%252C%2520we%2520adversarially%2520train%2520multiple%2520models%250Ain%2520a%2520collaborative%2520manner%2520to%2520synergistically%2520promote%2520robustness%2520against%250Aattacks%252C%2520while%2520boosting%2520standard%2520generalization%2520on%2520clean%2520instances.%2520A%2520neural%250Arouter%2520is%2520designed%2520to%2520adeptly%2520distribute%2520training%2520instances%2520among%2520models%252C%250Aenhancing%2520overall%2520load%2520balancing%2520and%2520collaborative%2520efficacy.%2520Extensive%250Aexperiments%2520verify%2520the%2520effectiveness%2520and%2520versatility%2520of%2520CNF%2520in%2520defending%250Aagainst%2520various%2520attacks%2520across%2520different%2520neural%2520VRP%2520methods.%2520Notably%252C%2520our%250Aapproach%2520also%2520achieves%2520impressive%2520out-of-distribution%2520generalization%2520on%250Abenchmark%2520instances.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2410.04968v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Collaboration%21%20Towards%20Robust%20Neural%20Methods%20for%20Routing%20Problems&entry.906535625=Jianan%20Zhou%20and%20Yaoxin%20Wu%20and%20Zhiguang%20Cao%20and%20Wen%20Song%20and%20Jie%20Zhang%20and%20Zhiqi%20Shen&entry.1292438233=%20%20Despite%20enjoying%20desirable%20efficiency%20and%20reduced%20reliance%20on%20domain%0Aexpertise%2C%20existing%20neural%20methods%20for%20vehicle%20routing%20problems%20%28VRPs%29%20suffer%0Afrom%20severe%20robustness%20issues%20--%20their%20performance%20significantly%20deteriorates%0Aon%20clean%20instances%20with%20crafted%20perturbations.%20To%20enhance%20robustness%2C%20we%0Apropose%20an%20ensemble-based%20Collaborative%20Neural%20Framework%20%28CNF%29%20w.r.t.%20the%0Adefense%20of%20neural%20VRP%20methods%2C%20which%20is%20crucial%20yet%20underexplored%20in%20the%0Aliterature.%20Given%20a%20neural%20VRP%20method%2C%20we%20adversarially%20train%20multiple%20models%0Ain%20a%20collaborative%20manner%20to%20synergistically%20promote%20robustness%20against%0Aattacks%2C%20while%20boosting%20standard%20generalization%20on%20clean%20instances.%20A%20neural%0Arouter%20is%20designed%20to%20adeptly%20distribute%20training%20instances%20among%20models%2C%0Aenhancing%20overall%20load%20balancing%20and%20collaborative%20efficacy.%20Extensive%0Aexperiments%20verify%20the%20effectiveness%20and%20versatility%20of%20CNF%20in%20defending%0Aagainst%20various%20attacks%20across%20different%20neural%20VRP%20methods.%20Notably%2C%20our%0Aapproach%20also%20achieves%20impressive%20out-of-distribution%20generalization%20on%0Abenchmark%20instances.%0A&entry.1838667208=http%3A//arxiv.org/abs/2410.04968v1&entry.124074799=Read"},
{"title": "PAMLR: A Passive-Active Multi-Armed Bandit-Based Solution for LoRa\n  Channel Allocation", "author": "Jihoon Yun and Chengzhang Li and Anish Arora", "abstract": "  Achieving low duty cycle operation in low-power wireless networks in urban\nenvironments is complicated by the complex and variable dynamics of external\ninterference and fading. We explore the use of reinforcement learning for\nachieving low power consumption for the task of optimal selection of channels.\nThe learning relies on a hybrid of passive channel sampling for dealing with\nexternal interference and active channel sampling for dealing with fading. Our\nsolution, Passive-Active Multi-armed bandit for LoRa (PAMLR, pronounced\n\"Pamela\"), balances the two types of samples to achieve energy-efficient\nchannel selection: active channel measurements are tuned to an appropriately\nlow level to update noise thresholds, and to compensate passive channel\nmeasurements are tuned to an appropriately high level for selecting the\ntop-most channels from channel exploration using the noise thresholds. The\nrates of both types of samples are adapted in response to channel dynamics.\nBased on extensive testing in multiple environments in different cities, we\nvalidate that PAMLR can maintain excellent communication quality, as\ndemonstrated by a low SNR regret compared to the optimal channel allocation\npolicy, while substantially minimizing the energy cost associated with channel\nmeasurements.\n", "link": "http://arxiv.org/abs/2410.05147v1", "date": "2024-10-07", "relevancy": 1.8559, "topK": [{"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.4796}, {"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.4726}, {"title": "Breadcrumbs: Adversarial class-balanced sampling for long-tailed recognition", "link": "https://link.springer.com/chapter/10.1007/978-3-031-20053-3_37", "similarity": 0.4491}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20PAMLR%3A%20A%20Passive-Active%20Multi-Armed%20Bandit-Based%20Solution%20for%20LoRa%0A%20%20Channel%20Allocation&body=Title%3A%20PAMLR%3A%20A%20Passive-Active%20Multi-Armed%20Bandit-Based%20Solution%20for%20LoRa%0A%20%20Channel%20Allocation%0AAuthor%3A%20Jihoon%20Yun%20and%20Chengzhang%20Li%20and%20Anish%20Arora%0AAbstract%3A%20%20%20Achieving%20low%20duty%20cycle%20operation%20in%20low-power%20wireless%20networks%20in%20urban%0Aenvironments%20is%20complicated%20by%20the%20complex%20and%20variable%20dynamics%20of%20external%0Ainterference%20and%20fading.%20We%20explore%20the%20use%20of%20reinforcement%20learning%20for%0Aachieving%20low%20power%20consumption%20for%20the%20task%20of%20optimal%20selection%20of%20channels.%0AThe%20learning%20relies%20on%20a%20hybrid%20of%20passive%20channel%20sampling%20for%20dealing%20with%0Aexternal%20interference%20and%20active%20channel%20sampling%20for%20dealing%20with%20fading.%20Our%0Asolution%2C%20Passive-Active%20Multi-armed%20bandit%20for%20LoRa%20%28PAMLR%2C%20pronounced%0A%22Pamela%22%29%2C%20balances%20the%20two%20types%20of%20samples%20to%20achieve%20energy-efficient%0Achannel%20selection%3A%20active%20channel%20measurements%20are%20tuned%20to%20an%20appropriately%0Alow%20level%20to%20update%20noise%20thresholds%2C%20and%20to%20compensate%20passive%20channel%0Ameasurements%20are%20tuned%20to%20an%20appropriately%20high%20level%20for%20selecting%20the%0Atop-most%20channels%20from%20channel%20exploration%20using%20the%20noise%20thresholds.%20The%0Arates%20of%20both%20types%20of%20samples%20are%20adapted%20in%20response%20to%20channel%20dynamics.%0ABased%20on%20extensive%20testing%20in%20multiple%20environments%20in%20different%20cities%2C%20we%0Avalidate%20that%20PAMLR%20can%20maintain%20excellent%20communication%20quality%2C%20as%0Ademonstrated%20by%20a%20low%20SNR%20regret%20compared%20to%20the%20optimal%20channel%20allocation%0Apolicy%2C%20while%20substantially%20minimizing%20the%20energy%20cost%20associated%20with%20channel%0Ameasurements.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2410.05147v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DPAMLR%253A%2520A%2520Passive-Active%2520Multi-Armed%2520Bandit-Based%2520Solution%2520for%2520LoRa%250A%2520%2520Channel%2520Allocation%26entry.906535625%3DJihoon%2520Yun%2520and%2520Chengzhang%2520Li%2520and%2520Anish%2520Arora%26entry.1292438233%3D%2520%2520Achieving%2520low%2520duty%2520cycle%2520operation%2520in%2520low-power%2520wireless%2520networks%2520in%2520urban%250Aenvironments%2520is%2520complicated%2520by%2520the%2520complex%2520and%2520variable%2520dynamics%2520of%2520external%250Ainterference%2520and%2520fading.%2520We%2520explore%2520the%2520use%2520of%2520reinforcement%2520learning%2520for%250Aachieving%2520low%2520power%2520consumption%2520for%2520the%2520task%2520of%2520optimal%2520selection%2520of%2520channels.%250AThe%2520learning%2520relies%2520on%2520a%2520hybrid%2520of%2520passive%2520channel%2520sampling%2520for%2520dealing%2520with%250Aexternal%2520interference%2520and%2520active%2520channel%2520sampling%2520for%2520dealing%2520with%2520fading.%2520Our%250Asolution%252C%2520Passive-Active%2520Multi-armed%2520bandit%2520for%2520LoRa%2520%2528PAMLR%252C%2520pronounced%250A%2522Pamela%2522%2529%252C%2520balances%2520the%2520two%2520types%2520of%2520samples%2520to%2520achieve%2520energy-efficient%250Achannel%2520selection%253A%2520active%2520channel%2520measurements%2520are%2520tuned%2520to%2520an%2520appropriately%250Alow%2520level%2520to%2520update%2520noise%2520thresholds%252C%2520and%2520to%2520compensate%2520passive%2520channel%250Ameasurements%2520are%2520tuned%2520to%2520an%2520appropriately%2520high%2520level%2520for%2520selecting%2520the%250Atop-most%2520channels%2520from%2520channel%2520exploration%2520using%2520the%2520noise%2520thresholds.%2520The%250Arates%2520of%2520both%2520types%2520of%2520samples%2520are%2520adapted%2520in%2520response%2520to%2520channel%2520dynamics.%250ABased%2520on%2520extensive%2520testing%2520in%2520multiple%2520environments%2520in%2520different%2520cities%252C%2520we%250Avalidate%2520that%2520PAMLR%2520can%2520maintain%2520excellent%2520communication%2520quality%252C%2520as%250Ademonstrated%2520by%2520a%2520low%2520SNR%2520regret%2520compared%2520to%2520the%2520optimal%2520channel%2520allocation%250Apolicy%252C%2520while%2520substantially%2520minimizing%2520the%2520energy%2520cost%2520associated%2520with%2520channel%250Ameasurements.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2410.05147v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=PAMLR%3A%20A%20Passive-Active%20Multi-Armed%20Bandit-Based%20Solution%20for%20LoRa%0A%20%20Channel%20Allocation&entry.906535625=Jihoon%20Yun%20and%20Chengzhang%20Li%20and%20Anish%20Arora&entry.1292438233=%20%20Achieving%20low%20duty%20cycle%20operation%20in%20low-power%20wireless%20networks%20in%20urban%0Aenvironments%20is%20complicated%20by%20the%20complex%20and%20variable%20dynamics%20of%20external%0Ainterference%20and%20fading.%20We%20explore%20the%20use%20of%20reinforcement%20learning%20for%0Aachieving%20low%20power%20consumption%20for%20the%20task%20of%20optimal%20selection%20of%20channels.%0AThe%20learning%20relies%20on%20a%20hybrid%20of%20passive%20channel%20sampling%20for%20dealing%20with%0Aexternal%20interference%20and%20active%20channel%20sampling%20for%20dealing%20with%20fading.%20Our%0Asolution%2C%20Passive-Active%20Multi-armed%20bandit%20for%20LoRa%20%28PAMLR%2C%20pronounced%0A%22Pamela%22%29%2C%20balances%20the%20two%20types%20of%20samples%20to%20achieve%20energy-efficient%0Achannel%20selection%3A%20active%20channel%20measurements%20are%20tuned%20to%20an%20appropriately%0Alow%20level%20to%20update%20noise%20thresholds%2C%20and%20to%20compensate%20passive%20channel%0Ameasurements%20are%20tuned%20to%20an%20appropriately%20high%20level%20for%20selecting%20the%0Atop-most%20channels%20from%20channel%20exploration%20using%20the%20noise%20thresholds.%20The%0Arates%20of%20both%20types%20of%20samples%20are%20adapted%20in%20response%20to%20channel%20dynamics.%0ABased%20on%20extensive%20testing%20in%20multiple%20environments%20in%20different%20cities%2C%20we%0Avalidate%20that%20PAMLR%20can%20maintain%20excellent%20communication%20quality%2C%20as%0Ademonstrated%20by%20a%20low%20SNR%20regret%20compared%20to%20the%20optimal%20channel%20allocation%0Apolicy%2C%20while%20substantially%20minimizing%20the%20energy%20cost%20associated%20with%20channel%0Ameasurements.%0A&entry.1838667208=http%3A//arxiv.org/abs/2410.05147v1&entry.124074799=Read"},
{"title": "Better Instruction-Following Through Minimum Bayes Risk", "author": "Ian Wu and Patrick Fernandes and Amanda Bertsch and Seungone Kim and Sina Pakazad and Graham Neubig", "abstract": "  General-purpose LLM judges capable of human-level evaluation provide not only\na scalable and accurate way of evaluating instruction-following LLMs but also\nnew avenues for supervising and improving their performance. One promising way\nof leveraging LLM judges for supervision is through Minimum Bayes Risk (MBR)\ndecoding, which uses a reference-based evaluator to select a high-quality\noutput from amongst a set of candidate outputs. In the first part of this work,\nwe explore using MBR decoding as a method for improving the test-time\nperformance of instruction-following LLMs. We find that MBR decoding with\nreference-based LLM judges substantially improves over greedy decoding,\nbest-of-N decoding with reference-free judges and MBR decoding with lexical and\nembedding-based metrics on AlpacaEval and MT-Bench. These gains are consistent\nacross LLMs with up to 70B parameters, demonstrating that smaller LLM judges\ncan be used to supervise much larger LLMs. Then, seeking to retain the\nimprovements from MBR decoding while mitigating additional test-time costs, we\nexplore iterative self-training on MBR-decoded outputs. We find that\nself-training using Direct Preference Optimisation leads to significant\nperformance gains, such that the self-trained models with greedy decoding\ngenerally match and sometimes exceed the performance of their base models with\nMBR decoding.\n", "link": "http://arxiv.org/abs/2410.02902v2", "date": "2024-10-07", "relevancy": 1.5255, "topK": [{"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.516}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.5158}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5026}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Better%20Instruction-Following%20Through%20Minimum%20Bayes%20Risk&body=Title%3A%20Better%20Instruction-Following%20Through%20Minimum%20Bayes%20Risk%0AAuthor%3A%20Ian%20Wu%20and%20Patrick%20Fernandes%20and%20Amanda%20Bertsch%20and%20Seungone%20Kim%20and%20Sina%20Pakazad%20and%20Graham%20Neubig%0AAbstract%3A%20%20%20General-purpose%20LLM%20judges%20capable%20of%20human-level%20evaluation%20provide%20not%20only%0Aa%20scalable%20and%20accurate%20way%20of%20evaluating%20instruction-following%20LLMs%20but%20also%0Anew%20avenues%20for%20supervising%20and%20improving%20their%20performance.%20One%20promising%20way%0Aof%20leveraging%20LLM%20judges%20for%20supervision%20is%20through%20Minimum%20Bayes%20Risk%20%28MBR%29%0Adecoding%2C%20which%20uses%20a%20reference-based%20evaluator%20to%20select%20a%20high-quality%0Aoutput%20from%20amongst%20a%20set%20of%20candidate%20outputs.%20In%20the%20first%20part%20of%20this%20work%2C%0Awe%20explore%20using%20MBR%20decoding%20as%20a%20method%20for%20improving%20the%20test-time%0Aperformance%20of%20instruction-following%20LLMs.%20We%20find%20that%20MBR%20decoding%20with%0Areference-based%20LLM%20judges%20substantially%20improves%20over%20greedy%20decoding%2C%0Abest-of-N%20decoding%20with%20reference-free%20judges%20and%20MBR%20decoding%20with%20lexical%20and%0Aembedding-based%20metrics%20on%20AlpacaEval%20and%20MT-Bench.%20These%20gains%20are%20consistent%0Aacross%20LLMs%20with%20up%20to%2070B%20parameters%2C%20demonstrating%20that%20smaller%20LLM%20judges%0Acan%20be%20used%20to%20supervise%20much%20larger%20LLMs.%20Then%2C%20seeking%20to%20retain%20the%0Aimprovements%20from%20MBR%20decoding%20while%20mitigating%20additional%20test-time%20costs%2C%20we%0Aexplore%20iterative%20self-training%20on%20MBR-decoded%20outputs.%20We%20find%20that%0Aself-training%20using%20Direct%20Preference%20Optimisation%20leads%20to%20significant%0Aperformance%20gains%2C%20such%20that%20the%20self-trained%20models%20with%20greedy%20decoding%0Agenerally%20match%20and%20sometimes%20exceed%20the%20performance%20of%20their%20base%20models%20with%0AMBR%20decoding.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2410.02902v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DBetter%2520Instruction-Following%2520Through%2520Minimum%2520Bayes%2520Risk%26entry.906535625%3DIan%2520Wu%2520and%2520Patrick%2520Fernandes%2520and%2520Amanda%2520Bertsch%2520and%2520Seungone%2520Kim%2520and%2520Sina%2520Pakazad%2520and%2520Graham%2520Neubig%26entry.1292438233%3D%2520%2520General-purpose%2520LLM%2520judges%2520capable%2520of%2520human-level%2520evaluation%2520provide%2520not%2520only%250Aa%2520scalable%2520and%2520accurate%2520way%2520of%2520evaluating%2520instruction-following%2520LLMs%2520but%2520also%250Anew%2520avenues%2520for%2520supervising%2520and%2520improving%2520their%2520performance.%2520One%2520promising%2520way%250Aof%2520leveraging%2520LLM%2520judges%2520for%2520supervision%2520is%2520through%2520Minimum%2520Bayes%2520Risk%2520%2528MBR%2529%250Adecoding%252C%2520which%2520uses%2520a%2520reference-based%2520evaluator%2520to%2520select%2520a%2520high-quality%250Aoutput%2520from%2520amongst%2520a%2520set%2520of%2520candidate%2520outputs.%2520In%2520the%2520first%2520part%2520of%2520this%2520work%252C%250Awe%2520explore%2520using%2520MBR%2520decoding%2520as%2520a%2520method%2520for%2520improving%2520the%2520test-time%250Aperformance%2520of%2520instruction-following%2520LLMs.%2520We%2520find%2520that%2520MBR%2520decoding%2520with%250Areference-based%2520LLM%2520judges%2520substantially%2520improves%2520over%2520greedy%2520decoding%252C%250Abest-of-N%2520decoding%2520with%2520reference-free%2520judges%2520and%2520MBR%2520decoding%2520with%2520lexical%2520and%250Aembedding-based%2520metrics%2520on%2520AlpacaEval%2520and%2520MT-Bench.%2520These%2520gains%2520are%2520consistent%250Aacross%2520LLMs%2520with%2520up%2520to%252070B%2520parameters%252C%2520demonstrating%2520that%2520smaller%2520LLM%2520judges%250Acan%2520be%2520used%2520to%2520supervise%2520much%2520larger%2520LLMs.%2520Then%252C%2520seeking%2520to%2520retain%2520the%250Aimprovements%2520from%2520MBR%2520decoding%2520while%2520mitigating%2520additional%2520test-time%2520costs%252C%2520we%250Aexplore%2520iterative%2520self-training%2520on%2520MBR-decoded%2520outputs.%2520We%2520find%2520that%250Aself-training%2520using%2520Direct%2520Preference%2520Optimisation%2520leads%2520to%2520significant%250Aperformance%2520gains%252C%2520such%2520that%2520the%2520self-trained%2520models%2520with%2520greedy%2520decoding%250Agenerally%2520match%2520and%2520sometimes%2520exceed%2520the%2520performance%2520of%2520their%2520base%2520models%2520with%250AMBR%2520decoding.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2410.02902v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Better%20Instruction-Following%20Through%20Minimum%20Bayes%20Risk&entry.906535625=Ian%20Wu%20and%20Patrick%20Fernandes%20and%20Amanda%20Bertsch%20and%20Seungone%20Kim%20and%20Sina%20Pakazad%20and%20Graham%20Neubig&entry.1292438233=%20%20General-purpose%20LLM%20judges%20capable%20of%20human-level%20evaluation%20provide%20not%20only%0Aa%20scalable%20and%20accurate%20way%20of%20evaluating%20instruction-following%20LLMs%20but%20also%0Anew%20avenues%20for%20supervising%20and%20improving%20their%20performance.%20One%20promising%20way%0Aof%20leveraging%20LLM%20judges%20for%20supervision%20is%20through%20Minimum%20Bayes%20Risk%20%28MBR%29%0Adecoding%2C%20which%20uses%20a%20reference-based%20evaluator%20to%20select%20a%20high-quality%0Aoutput%20from%20amongst%20a%20set%20of%20candidate%20outputs.%20In%20the%20first%20part%20of%20this%20work%2C%0Awe%20explore%20using%20MBR%20decoding%20as%20a%20method%20for%20improving%20the%20test-time%0Aperformance%20of%20instruction-following%20LLMs.%20We%20find%20that%20MBR%20decoding%20with%0Areference-based%20LLM%20judges%20substantially%20improves%20over%20greedy%20decoding%2C%0Abest-of-N%20decoding%20with%20reference-free%20judges%20and%20MBR%20decoding%20with%20lexical%20and%0Aembedding-based%20metrics%20on%20AlpacaEval%20and%20MT-Bench.%20These%20gains%20are%20consistent%0Aacross%20LLMs%20with%20up%20to%2070B%20parameters%2C%20demonstrating%20that%20smaller%20LLM%20judges%0Acan%20be%20used%20to%20supervise%20much%20larger%20LLMs.%20Then%2C%20seeking%20to%20retain%20the%0Aimprovements%20from%20MBR%20decoding%20while%20mitigating%20additional%20test-time%20costs%2C%20we%0Aexplore%20iterative%20self-training%20on%20MBR-decoded%20outputs.%20We%20find%20that%0Aself-training%20using%20Direct%20Preference%20Optimisation%20leads%20to%20significant%0Aperformance%20gains%2C%20such%20that%20the%20self-trained%20models%20with%20greedy%20decoding%0Agenerally%20match%20and%20sometimes%20exceed%20the%20performance%20of%20their%20base%20models%20with%0AMBR%20decoding.%0A&entry.1838667208=http%3A//arxiv.org/abs/2410.02902v2&entry.124074799=Read"},
      ];
      const content = document.getElementById('content');
      function createPostElement(post) {
        const postElement = document.createElement('div');
        postElement.className = 'post';
        const dateElem = document.createElement('p');
        dateElem.setAttribute("class", "date");
        dateElem.textContent = post.date;
        postElement.appendChild(dateElem);

        const textElem = document.createElement('p');
        textElem.setAttribute("class", "text");
        const titleElem = document.createElement('p');
        titleElem.setAttribute("class", "title");
        titleElem.textContent = post.title;
        textElem.appendChild(titleElem);
        const authorElem = document.createElement('p');
        authorElem.setAttribute("class", "author");
        authorElem.textContent = post.author;
        textElem.appendChild(authorElem);
        const abstractElem = document.createElement('p');
        abstractElem.setAttribute("class", "abstract");
        abstractElem.textContent = post.abstract;
        textElem.appendChild(abstractElem);

        const linkElement = document.createElement('a');
        linkElement.setAttribute("class", "link");
        linkElement.href = post.link;
        linkElement.target = "_blank";
        linkElement.textContent = post.link.length > 50 ? post.link.substring(0, 50) + '...' : post.link;
        textElem.appendChild(linkElement);
        postElement.appendChild(textElem);

        const linkElementContainer = document.createElement('div');
        linkElementContainer.setAttribute("class", "comment");
        const actionElement = document.createElement('a');
        actionElement.setAttribute("class", "comment");
        actionElement.href = post.form;
        actionElement.textContent = "Action";
        actionElement.target = "_blank";
        linkElementContainer.appendChild(actionElement);
        const emailElement = document.createElement('a');
        emailElement.setAttribute("class", "comment");
        emailElement.href = post.mailto;
        emailElement.textContent = "Email";
        emailElement.target = "_blank";
        linkElementContainer.appendChild(emailElement);
        postElement.appendChild(linkElementContainer);
        const e = document.createElement('div');
        e.setAttribute("class", "clear");
        postElement.appendChild(e);

        const relevancyContainer = document.createElement('div');
        const relevancyValElem = document.createElement('p');
        relevancyValElem.textContent = "Relevancy " + post.relevancy;
        relevancyContainer.appendChild(relevancyValElem);
        post.topK.forEach((sub) => {
          const topKElem = document.createElement('a');
          topKElem.setAttribute("class", "topK");
          topKElem.href = sub.link;
          topKElem.textContent = sub.title + " (" + sub.similarity + ")";
          topKElem.target = "_blank";
          relevancyContainer.appendChild(topKElem);
        });
        postElement.appendChild(relevancyContainer);
        return postElement;
      }
      function loadPosts() {
        // Simulate loading more posts
        posts.forEach((post) => {
          const postElement = createPostElement(post);
          content.appendChild(postElement);
        });
      }
      // Load initial posts
      loadPosts();
    </script>

  </body>
</html>


