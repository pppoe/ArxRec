<!DOCTYPE html>
<html lang="en">
  <head>
    <script async src="https://www.googletagmanager.com/gtag/js?id=G-V34CNNDP8V"></script>
    <script>
      window.dataLayer = window.dataLayer || [];
      function gtag(){dataLayer.push(arguments);}
      gtag('js', new Date());
      gtag('config', 'G-V34CNNDP8V');
    </script>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Arxiv Paper Selection</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.3/css/all.min.css" crossorigin="anonymous" referrerpolicy="no-referrer" />
    <style>
    body {
      font-family: Arial, sans-serif;
      margin: 0;
      padding: 0;
      background-color: #f4f4f4;
    }
    header {
      position: fixed;
      top: 0;
      left: 0;
      width: 100%;
      background-color: #ffffff;
      color: black;
      padding: 10px;
      text-align: center;
      z-index: 1000;
      border-bottom: 1px solid #ddd;
    }
    header div {
      display: block;
      margin: 10px auto;
    }

    #home-icon {
      display: block;
      float: left;
      margin: 5px;
      text-decoration: none;
      color: black;
    }

    main {
      margin-top: 60px; /* Adjusted margin to account for fixed header */
      padding: 20px;
    }

    .post {
      background-color: white;
      border: 1px solid #ddd;
      border-radius: 5px;
      margin-bottom: 10px;
      padding: 10px 20px;
      max-height: 2000px;
      overflow: scroll;
    }
    .post img {
      display: block;
      margin-top: 5px;
      max-width: auto;
      max-height: 100px;
    }
    .post .clear {
      clear: both;
      display: block;
    }
    .post a {
      text-decoration: none;
    }
    .post a:hover {
      color: #0056b3;
    }
    .post a:visited {
      color: #0056b3;
    }
    .post div.comment {
      text-align: right;
    }
    .post div.comment a {
      margin: 1em;
    }
    .post .text {
      margin: 1em 0em;
      padding: 0;
    }
    .post .text .title {
    }
    .post .text .author {
    }
    .post .text .abstract {
    }
    .post .topK {
      display: block;
      margin: 0.5em;
    }
    .post .date {
      margin: 0;
      padding: 0;
      text-size: small; 
      color: gray;
    }
    .post .link {
      margin: 0;
      padding: 0;
    }
    @media screen and (max-width: 600px) {
      body {
        max-width: 100%; 
      }
      #home-icon {
        float: none;
        display: block;
        text-align: center;
        margin-bottom: 10px;
      }
    }
    footer {
      width: 100%;
      background-color: #ddd;
      text-align: center;
      z-index: 1000;
      padding: 20px 0px;
      margin-bottom: 20px;
      left: 0;
    }

    #next-btn,
    #prev-btn {
      background-color: #4CAF50;
      color: white;
      padding: 8px 16px;
      margin: 0 50px;
      border: none;
      border-radius: 4px;
      cursor: pointer;
    }

    .links {
      padding: 20px;
    }
    .links a {
      text-decoration: none;
    }
    .links a:hover {
      color: #0056b3;
    }
    .links a:visited {
      color: #0056b3;
    }

    #page-index {
      font-size: small;
    }
    .ads {
      width: 100%;
    }
    #prev-btn {
      background-color: #4CAF50;
      color: white;
      padding: 8px 16px;
      margin: 0 50px;
      border: none;
      border-radius: 4px;
      cursor: pointer;
    }
    </style>
  </head>
  <body>

    <header>
      <a id="prev-btn" href="daily20240519.html"><i class="fas fa-chevron-left"></i></a>
      <a href="https://haoxiang.org/">About</a>
    </header>

    <main id="content">
      <!-- Posts will be dynamically added here using JavaScript -->
    </main>

    <script>
      // Dummy data for posts
      const posts = [
{"title": "CG-HOI: Contact-Guided 3D Human-Object Interaction Generation", "author": "Christian Diller and Angela Dai", "abstract": "  We propose CG-HOI, the first method to address the task of generating dynamic\n3D human-object interactions (HOIs) from text. We model the motion of both\nhuman and object in an interdependent fashion, as semantically rich human\nmotion rarely happens in isolation without any interactions. Our key insight is\nthat explicitly modeling contact between the human body surface and object\ngeometry can be used as strong proxy guidance, both during training and\ninference. Using this guidance to bridge human and object motion enables\ngenerating more realistic and physically plausible interaction sequences, where\nthe human body and corresponding object move in a coherent manner. Our method\nfirst learns to model human motion, object motion, and contact in a joint\ndiffusion process, inter-correlated through cross-attention. We then leverage\nthis learned contact for guidance during inference to synthesize realistic and\ncoherent HOIs. Extensive evaluation shows that our joint contact-based\nhuman-object interaction approach generates realistic and physically plausible\nsequences, and we show two applications highlighting the capabilities of our\nmethod. Conditioned on a given object trajectory, we can generate the\ncorresponding human motion without re-training, demonstrating strong\nhuman-object interdependency learning. Our approach is also flexible, and can\nbe applied to static real-world 3D scene scans.\n", "link": "http://arxiv.org/abs/2311.16097v2", "date": "2024-05-17", "relevancy": 2.9092, "topK": [{"title": "VirtualModel: Generating Object-ID-retentive Human-object Interaction\n  Image by Diffusion Model for E-commerce Marketing", "link": "http://arxiv.org/abs/2405.09985v1", "similarity": 0.6349}, {"title": "UGG: Unified Generative Grasping", "link": "https://arxiv.org/abs/2311.16917", "similarity": 0.563}, {"title": "PoseAnimate: Zero-shot high fidelity pose controllable character\n  animation", "link": "http://arxiv.org/abs/2404.13680v2", "similarity": 0.5476}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20CG-HOI%3A%20Contact-Guided%203D%20Human-Object%20Interaction%20Generation&body=Title%3A%20CG-HOI%3A%20Contact-Guided%203D%20Human-Object%20Interaction%20Generation%0AAuthor%3A%20Christian%20Diller%20and%20Angela%20Dai%0AAbstract%3A%20%20%20We%20propose%20CG-HOI%2C%20the%20first%20method%20to%20address%20the%20task%20of%20generating%20dynamic%0A3D%20human-object%20interactions%20%28HOIs%29%20from%20text.%20We%20model%20the%20motion%20of%20both%0Ahuman%20and%20object%20in%20an%20interdependent%20fashion%2C%20as%20semantically%20rich%20human%0Amotion%20rarely%20happens%20in%20isolation%20without%20any%20interactions.%20Our%20key%20insight%20is%0Athat%20explicitly%20modeling%20contact%20between%20the%20human%20body%20surface%20and%20object%0Ageometry%20can%20be%20used%20as%20strong%20proxy%20guidance%2C%20both%20during%20training%20and%0Ainference.%20Using%20this%20guidance%20to%20bridge%20human%20and%20object%20motion%20enables%0Agenerating%20more%20realistic%20and%20physically%20plausible%20interaction%20sequences%2C%20where%0Athe%20human%20body%20and%20corresponding%20object%20move%20in%20a%20coherent%20manner.%20Our%20method%0Afirst%20learns%20to%20model%20human%20motion%2C%20object%20motion%2C%20and%20contact%20in%20a%20joint%0Adiffusion%20process%2C%20inter-correlated%20through%20cross-attention.%20We%20then%20leverage%0Athis%20learned%20contact%20for%20guidance%20during%20inference%20to%20synthesize%20realistic%20and%0Acoherent%20HOIs.%20Extensive%20evaluation%20shows%20that%20our%20joint%20contact-based%0Ahuman-object%20interaction%20approach%20generates%20realistic%20and%20physically%20plausible%0Asequences%2C%20and%20we%20show%20two%20applications%20highlighting%20the%20capabilities%20of%20our%0Amethod.%20Conditioned%20on%20a%20given%20object%20trajectory%2C%20we%20can%20generate%20the%0Acorresponding%20human%20motion%20without%20re-training%2C%20demonstrating%20strong%0Ahuman-object%20interdependency%20learning.%20Our%20approach%20is%20also%20flexible%2C%20and%20can%0Abe%20applied%20to%20static%20real-world%203D%20scene%20scans.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2311.16097v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DCG-HOI%253A%2520Contact-Guided%25203D%2520Human-Object%2520Interaction%2520Generation%26entry.906535625%3DChristian%2520Diller%2520and%2520Angela%2520Dai%26entry.1292438233%3D%2520%2520We%2520propose%2520CG-HOI%252C%2520the%2520first%2520method%2520to%2520address%2520the%2520task%2520of%2520generating%2520dynamic%250A3D%2520human-object%2520interactions%2520%2528HOIs%2529%2520from%2520text.%2520We%2520model%2520the%2520motion%2520of%2520both%250Ahuman%2520and%2520object%2520in%2520an%2520interdependent%2520fashion%252C%2520as%2520semantically%2520rich%2520human%250Amotion%2520rarely%2520happens%2520in%2520isolation%2520without%2520any%2520interactions.%2520Our%2520key%2520insight%2520is%250Athat%2520explicitly%2520modeling%2520contact%2520between%2520the%2520human%2520body%2520surface%2520and%2520object%250Ageometry%2520can%2520be%2520used%2520as%2520strong%2520proxy%2520guidance%252C%2520both%2520during%2520training%2520and%250Ainference.%2520Using%2520this%2520guidance%2520to%2520bridge%2520human%2520and%2520object%2520motion%2520enables%250Agenerating%2520more%2520realistic%2520and%2520physically%2520plausible%2520interaction%2520sequences%252C%2520where%250Athe%2520human%2520body%2520and%2520corresponding%2520object%2520move%2520in%2520a%2520coherent%2520manner.%2520Our%2520method%250Afirst%2520learns%2520to%2520model%2520human%2520motion%252C%2520object%2520motion%252C%2520and%2520contact%2520in%2520a%2520joint%250Adiffusion%2520process%252C%2520inter-correlated%2520through%2520cross-attention.%2520We%2520then%2520leverage%250Athis%2520learned%2520contact%2520for%2520guidance%2520during%2520inference%2520to%2520synthesize%2520realistic%2520and%250Acoherent%2520HOIs.%2520Extensive%2520evaluation%2520shows%2520that%2520our%2520joint%2520contact-based%250Ahuman-object%2520interaction%2520approach%2520generates%2520realistic%2520and%2520physically%2520plausible%250Asequences%252C%2520and%2520we%2520show%2520two%2520applications%2520highlighting%2520the%2520capabilities%2520of%2520our%250Amethod.%2520Conditioned%2520on%2520a%2520given%2520object%2520trajectory%252C%2520we%2520can%2520generate%2520the%250Acorresponding%2520human%2520motion%2520without%2520re-training%252C%2520demonstrating%2520strong%250Ahuman-object%2520interdependency%2520learning.%2520Our%2520approach%2520is%2520also%2520flexible%252C%2520and%2520can%250Abe%2520applied%2520to%2520static%2520real-world%25203D%2520scene%2520scans.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2311.16097v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=CG-HOI%3A%20Contact-Guided%203D%20Human-Object%20Interaction%20Generation&entry.906535625=Christian%20Diller%20and%20Angela%20Dai&entry.1292438233=%20%20We%20propose%20CG-HOI%2C%20the%20first%20method%20to%20address%20the%20task%20of%20generating%20dynamic%0A3D%20human-object%20interactions%20%28HOIs%29%20from%20text.%20We%20model%20the%20motion%20of%20both%0Ahuman%20and%20object%20in%20an%20interdependent%20fashion%2C%20as%20semantically%20rich%20human%0Amotion%20rarely%20happens%20in%20isolation%20without%20any%20interactions.%20Our%20key%20insight%20is%0Athat%20explicitly%20modeling%20contact%20between%20the%20human%20body%20surface%20and%20object%0Ageometry%20can%20be%20used%20as%20strong%20proxy%20guidance%2C%20both%20during%20training%20and%0Ainference.%20Using%20this%20guidance%20to%20bridge%20human%20and%20object%20motion%20enables%0Agenerating%20more%20realistic%20and%20physically%20plausible%20interaction%20sequences%2C%20where%0Athe%20human%20body%20and%20corresponding%20object%20move%20in%20a%20coherent%20manner.%20Our%20method%0Afirst%20learns%20to%20model%20human%20motion%2C%20object%20motion%2C%20and%20contact%20in%20a%20joint%0Adiffusion%20process%2C%20inter-correlated%20through%20cross-attention.%20We%20then%20leverage%0Athis%20learned%20contact%20for%20guidance%20during%20inference%20to%20synthesize%20realistic%20and%0Acoherent%20HOIs.%20Extensive%20evaluation%20shows%20that%20our%20joint%20contact-based%0Ahuman-object%20interaction%20approach%20generates%20realistic%20and%20physically%20plausible%0Asequences%2C%20and%20we%20show%20two%20applications%20highlighting%20the%20capabilities%20of%20our%0Amethod.%20Conditioned%20on%20a%20given%20object%20trajectory%2C%20we%20can%20generate%20the%0Acorresponding%20human%20motion%20without%20re-training%2C%20demonstrating%20strong%0Ahuman-object%20interdependency%20learning.%20Our%20approach%20is%20also%20flexible%2C%20and%20can%0Abe%20applied%20to%20static%20real-world%203D%20scene%20scans.%0A&entry.1838667208=http%3A//arxiv.org/abs/2311.16097v2&entry.124074799=Read"},
{"title": "CCTNet: A Circular Convolutional Transformer Network for LiDAR-based\n  Place Recognition Handling Movable Objects Occlusion", "author": "Gang Wang and Chaoran Zhu and Qian Xu and Tongzhou Zhang and Hai Zhang and XiaoPeng Fan and Jue Hu", "abstract": "  Place recognition is a fundamental task for robotic application, allowing\nrobots to perform loop closure detection within simultaneous localization and\nmapping (SLAM), and achieve relocalization on prior maps. Current range\nimage-based networks use single-column convolution to maintain feature\ninvariance to shifts in image columns caused by LiDAR viewpoint change.However,\nthis raises the issues such as \"restricted receptive fields\" and \"excessive\nfocus on local regions\", degrading the performance of networks. To address the\naforementioned issues, we propose a lightweight circular convolutional\nTransformer network denoted as CCTNet, which boosts performance by capturing\nstructural information in point clouds and facilitating crossdimensional\ninteraction of spatial and channel information. Initially, a Circular\nConvolution Module (CCM) is introduced, expanding the network's perceptual\nfield while maintaining feature consistency across varying LiDAR perspectives.\nThen, a Range Transformer Module (RTM) is proposed, which enhances place\nrecognition accuracy in scenarios with movable objects by employing a\ncombination of channel and spatial attention mechanisms. Furthermore, we\npropose an Overlap-based loss function, transforming the place recognition task\nfrom a binary loop closure classification into a regression problem linked to\nthe overlap between LiDAR frames. Through extensive experiments on the KITTI\nand Ford Campus datasets, CCTNet surpasses comparable methods, achieving\nRecall@1 of 0.924 and 0.965, and Recall@1% of 0.990 and 0.993 on the test set,\nshowcasing a superior performance. Results on the selfcollected dataset further\ndemonstrate the proposed method's potential for practical implementation in\ncomplex scenarios to handle movable objects, showing improved generalization in\nvarious datasets.\n", "link": "http://arxiv.org/abs/2405.10793v1", "date": "2024-05-17", "relevancy": 2.8844, "topK": [{"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.5918}, {"title": "DDM-NET: End-to-end learning of keypoint feature Detection, Description and Matching for 3D localization", "link": "https://arxiv.org/abs/2212.04575", "similarity": 0.5697}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5691}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20CCTNet%3A%20A%20Circular%20Convolutional%20Transformer%20Network%20for%20LiDAR-based%0A%20%20Place%20Recognition%20Handling%20Movable%20Objects%20Occlusion&body=Title%3A%20CCTNet%3A%20A%20Circular%20Convolutional%20Transformer%20Network%20for%20LiDAR-based%0A%20%20Place%20Recognition%20Handling%20Movable%20Objects%20Occlusion%0AAuthor%3A%20Gang%20Wang%20and%20Chaoran%20Zhu%20and%20Qian%20Xu%20and%20Tongzhou%20Zhang%20and%20Hai%20Zhang%20and%20XiaoPeng%20Fan%20and%20Jue%20Hu%0AAbstract%3A%20%20%20Place%20recognition%20is%20a%20fundamental%20task%20for%20robotic%20application%2C%20allowing%0Arobots%20to%20perform%20loop%20closure%20detection%20within%20simultaneous%20localization%20and%0Amapping%20%28SLAM%29%2C%20and%20achieve%20relocalization%20on%20prior%20maps.%20Current%20range%0Aimage-based%20networks%20use%20single-column%20convolution%20to%20maintain%20feature%0Ainvariance%20to%20shifts%20in%20image%20columns%20caused%20by%20LiDAR%20viewpoint%20change.However%2C%0Athis%20raises%20the%20issues%20such%20as%20%22restricted%20receptive%20fields%22%20and%20%22excessive%0Afocus%20on%20local%20regions%22%2C%20degrading%20the%20performance%20of%20networks.%20To%20address%20the%0Aaforementioned%20issues%2C%20we%20propose%20a%20lightweight%20circular%20convolutional%0ATransformer%20network%20denoted%20as%20CCTNet%2C%20which%20boosts%20performance%20by%20capturing%0Astructural%20information%20in%20point%20clouds%20and%20facilitating%20crossdimensional%0Ainteraction%20of%20spatial%20and%20channel%20information.%20Initially%2C%20a%20Circular%0AConvolution%20Module%20%28CCM%29%20is%20introduced%2C%20expanding%20the%20network%27s%20perceptual%0Afield%20while%20maintaining%20feature%20consistency%20across%20varying%20LiDAR%20perspectives.%0AThen%2C%20a%20Range%20Transformer%20Module%20%28RTM%29%20is%20proposed%2C%20which%20enhances%20place%0Arecognition%20accuracy%20in%20scenarios%20with%20movable%20objects%20by%20employing%20a%0Acombination%20of%20channel%20and%20spatial%20attention%20mechanisms.%20Furthermore%2C%20we%0Apropose%20an%20Overlap-based%20loss%20function%2C%20transforming%20the%20place%20recognition%20task%0Afrom%20a%20binary%20loop%20closure%20classification%20into%20a%20regression%20problem%20linked%20to%0Athe%20overlap%20between%20LiDAR%20frames.%20Through%20extensive%20experiments%20on%20the%20KITTI%0Aand%20Ford%20Campus%20datasets%2C%20CCTNet%20surpasses%20comparable%20methods%2C%20achieving%0ARecall%401%20of%200.924%20and%200.965%2C%20and%20Recall%401%25%20of%200.990%20and%200.993%20on%20the%20test%20set%2C%0Ashowcasing%20a%20superior%20performance.%20Results%20on%20the%20selfcollected%20dataset%20further%0Ademonstrate%20the%20proposed%20method%27s%20potential%20for%20practical%20implementation%20in%0Acomplex%20scenarios%20to%20handle%20movable%20objects%2C%20showing%20improved%20generalization%20in%0Avarious%20datasets.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2405.10793v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DCCTNet%253A%2520A%2520Circular%2520Convolutional%2520Transformer%2520Network%2520for%2520LiDAR-based%250A%2520%2520Place%2520Recognition%2520Handling%2520Movable%2520Objects%2520Occlusion%26entry.906535625%3DGang%2520Wang%2520and%2520Chaoran%2520Zhu%2520and%2520Qian%2520Xu%2520and%2520Tongzhou%2520Zhang%2520and%2520Hai%2520Zhang%2520and%2520XiaoPeng%2520Fan%2520and%2520Jue%2520Hu%26entry.1292438233%3D%2520%2520Place%2520recognition%2520is%2520a%2520fundamental%2520task%2520for%2520robotic%2520application%252C%2520allowing%250Arobots%2520to%2520perform%2520loop%2520closure%2520detection%2520within%2520simultaneous%2520localization%2520and%250Amapping%2520%2528SLAM%2529%252C%2520and%2520achieve%2520relocalization%2520on%2520prior%2520maps.%2520Current%2520range%250Aimage-based%2520networks%2520use%2520single-column%2520convolution%2520to%2520maintain%2520feature%250Ainvariance%2520to%2520shifts%2520in%2520image%2520columns%2520caused%2520by%2520LiDAR%2520viewpoint%2520change.However%252C%250Athis%2520raises%2520the%2520issues%2520such%2520as%2520%2522restricted%2520receptive%2520fields%2522%2520and%2520%2522excessive%250Afocus%2520on%2520local%2520regions%2522%252C%2520degrading%2520the%2520performance%2520of%2520networks.%2520To%2520address%2520the%250Aaforementioned%2520issues%252C%2520we%2520propose%2520a%2520lightweight%2520circular%2520convolutional%250ATransformer%2520network%2520denoted%2520as%2520CCTNet%252C%2520which%2520boosts%2520performance%2520by%2520capturing%250Astructural%2520information%2520in%2520point%2520clouds%2520and%2520facilitating%2520crossdimensional%250Ainteraction%2520of%2520spatial%2520and%2520channel%2520information.%2520Initially%252C%2520a%2520Circular%250AConvolution%2520Module%2520%2528CCM%2529%2520is%2520introduced%252C%2520expanding%2520the%2520network%2527s%2520perceptual%250Afield%2520while%2520maintaining%2520feature%2520consistency%2520across%2520varying%2520LiDAR%2520perspectives.%250AThen%252C%2520a%2520Range%2520Transformer%2520Module%2520%2528RTM%2529%2520is%2520proposed%252C%2520which%2520enhances%2520place%250Arecognition%2520accuracy%2520in%2520scenarios%2520with%2520movable%2520objects%2520by%2520employing%2520a%250Acombination%2520of%2520channel%2520and%2520spatial%2520attention%2520mechanisms.%2520Furthermore%252C%2520we%250Apropose%2520an%2520Overlap-based%2520loss%2520function%252C%2520transforming%2520the%2520place%2520recognition%2520task%250Afrom%2520a%2520binary%2520loop%2520closure%2520classification%2520into%2520a%2520regression%2520problem%2520linked%2520to%250Athe%2520overlap%2520between%2520LiDAR%2520frames.%2520Through%2520extensive%2520experiments%2520on%2520the%2520KITTI%250Aand%2520Ford%2520Campus%2520datasets%252C%2520CCTNet%2520surpasses%2520comparable%2520methods%252C%2520achieving%250ARecall%25401%2520of%25200.924%2520and%25200.965%252C%2520and%2520Recall%25401%2525%2520of%25200.990%2520and%25200.993%2520on%2520the%2520test%2520set%252C%250Ashowcasing%2520a%2520superior%2520performance.%2520Results%2520on%2520the%2520selfcollected%2520dataset%2520further%250Ademonstrate%2520the%2520proposed%2520method%2527s%2520potential%2520for%2520practical%2520implementation%2520in%250Acomplex%2520scenarios%2520to%2520handle%2520movable%2520objects%252C%2520showing%2520improved%2520generalization%2520in%250Avarious%2520datasets.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2405.10793v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=CCTNet%3A%20A%20Circular%20Convolutional%20Transformer%20Network%20for%20LiDAR-based%0A%20%20Place%20Recognition%20Handling%20Movable%20Objects%20Occlusion&entry.906535625=Gang%20Wang%20and%20Chaoran%20Zhu%20and%20Qian%20Xu%20and%20Tongzhou%20Zhang%20and%20Hai%20Zhang%20and%20XiaoPeng%20Fan%20and%20Jue%20Hu&entry.1292438233=%20%20Place%20recognition%20is%20a%20fundamental%20task%20for%20robotic%20application%2C%20allowing%0Arobots%20to%20perform%20loop%20closure%20detection%20within%20simultaneous%20localization%20and%0Amapping%20%28SLAM%29%2C%20and%20achieve%20relocalization%20on%20prior%20maps.%20Current%20range%0Aimage-based%20networks%20use%20single-column%20convolution%20to%20maintain%20feature%0Ainvariance%20to%20shifts%20in%20image%20columns%20caused%20by%20LiDAR%20viewpoint%20change.However%2C%0Athis%20raises%20the%20issues%20such%20as%20%22restricted%20receptive%20fields%22%20and%20%22excessive%0Afocus%20on%20local%20regions%22%2C%20degrading%20the%20performance%20of%20networks.%20To%20address%20the%0Aaforementioned%20issues%2C%20we%20propose%20a%20lightweight%20circular%20convolutional%0ATransformer%20network%20denoted%20as%20CCTNet%2C%20which%20boosts%20performance%20by%20capturing%0Astructural%20information%20in%20point%20clouds%20and%20facilitating%20crossdimensional%0Ainteraction%20of%20spatial%20and%20channel%20information.%20Initially%2C%20a%20Circular%0AConvolution%20Module%20%28CCM%29%20is%20introduced%2C%20expanding%20the%20network%27s%20perceptual%0Afield%20while%20maintaining%20feature%20consistency%20across%20varying%20LiDAR%20perspectives.%0AThen%2C%20a%20Range%20Transformer%20Module%20%28RTM%29%20is%20proposed%2C%20which%20enhances%20place%0Arecognition%20accuracy%20in%20scenarios%20with%20movable%20objects%20by%20employing%20a%0Acombination%20of%20channel%20and%20spatial%20attention%20mechanisms.%20Furthermore%2C%20we%0Apropose%20an%20Overlap-based%20loss%20function%2C%20transforming%20the%20place%20recognition%20task%0Afrom%20a%20binary%20loop%20closure%20classification%20into%20a%20regression%20problem%20linked%20to%0Athe%20overlap%20between%20LiDAR%20frames.%20Through%20extensive%20experiments%20on%20the%20KITTI%0Aand%20Ford%20Campus%20datasets%2C%20CCTNet%20surpasses%20comparable%20methods%2C%20achieving%0ARecall%401%20of%200.924%20and%200.965%2C%20and%20Recall%401%25%20of%200.990%20and%200.993%20on%20the%20test%20set%2C%0Ashowcasing%20a%20superior%20performance.%20Results%20on%20the%20selfcollected%20dataset%20further%0Ademonstrate%20the%20proposed%20method%27s%20potential%20for%20practical%20implementation%20in%0Acomplex%20scenarios%20to%20handle%20movable%20objects%2C%20showing%20improved%20generalization%20in%0Avarious%20datasets.%0A&entry.1838667208=http%3A//arxiv.org/abs/2405.10793v1&entry.124074799=Read"},
{"title": "From Sora What We Can See: A Survey of Text-to-Video Generation", "author": "Rui Sun and Yumin Zhang and Tejal Shah and Jiahao Sun and Shuoying Zhang and Wenqi Li and Haoran Duan and Bo Wei and Rajiv Ranjan", "abstract": "  With impressive achievements made, artificial intelligence is on the path\nforward to artificial general intelligence. Sora, developed by OpenAI, which is\ncapable of minute-level world-simulative abilities can be considered as a\nmilestone on this developmental path. However, despite its notable successes,\nSora still encounters various obstacles that need to be resolved. In this\nsurvey, we embark from the perspective of disassembling Sora in text-to-video\ngeneration, and conducting a comprehensive review of literature, trying to\nanswer the question, \\textit{From Sora What We Can See}. Specifically, after\nbasic preliminaries regarding the general algorithms are introduced, the\nliterature is categorized from three mutually perpendicular dimensions:\nevolutionary generators, excellent pursuit, and realistic panorama.\nSubsequently, the widely used datasets and metrics are organized in detail.\nLast but more importantly, we identify several challenges and open problems in\nthis domain and propose potential future directions for research and\ndevelopment.\n", "link": "http://arxiv.org/abs/2405.10674v1", "date": "2024-05-17", "relevancy": 2.8211, "topK": [{"title": "Genie: Generative Interactive Environments", "link": "http://arxiv.org/abs/2402.15391v1", "similarity": 0.6039}, {"title": "DressCode: Autoregressively Sewing and Generating Garments from Text\n  Guidance", "link": "http://arxiv.org/abs/2401.16465v3", "similarity": 0.5462}, {"title": "Magic-Me: Identity-Specific Video Customized Diffusion", "link": "http://arxiv.org/abs/2402.09368v1", "similarity": 0.5426}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20From%20Sora%20What%20We%20Can%20See%3A%20A%20Survey%20of%20Text-to-Video%20Generation&body=Title%3A%20From%20Sora%20What%20We%20Can%20See%3A%20A%20Survey%20of%20Text-to-Video%20Generation%0AAuthor%3A%20Rui%20Sun%20and%20Yumin%20Zhang%20and%20Tejal%20Shah%20and%20Jiahao%20Sun%20and%20Shuoying%20Zhang%20and%20Wenqi%20Li%20and%20Haoran%20Duan%20and%20Bo%20Wei%20and%20Rajiv%20Ranjan%0AAbstract%3A%20%20%20With%20impressive%20achievements%20made%2C%20artificial%20intelligence%20is%20on%20the%20path%0Aforward%20to%20artificial%20general%20intelligence.%20Sora%2C%20developed%20by%20OpenAI%2C%20which%20is%0Acapable%20of%20minute-level%20world-simulative%20abilities%20can%20be%20considered%20as%20a%0Amilestone%20on%20this%20developmental%20path.%20However%2C%20despite%20its%20notable%20successes%2C%0ASora%20still%20encounters%20various%20obstacles%20that%20need%20to%20be%20resolved.%20In%20this%0Asurvey%2C%20we%20embark%20from%20the%20perspective%20of%20disassembling%20Sora%20in%20text-to-video%0Ageneration%2C%20and%20conducting%20a%20comprehensive%20review%20of%20literature%2C%20trying%20to%0Aanswer%20the%20question%2C%20%5Ctextit%7BFrom%20Sora%20What%20We%20Can%20See%7D.%20Specifically%2C%20after%0Abasic%20preliminaries%20regarding%20the%20general%20algorithms%20are%20introduced%2C%20the%0Aliterature%20is%20categorized%20from%20three%20mutually%20perpendicular%20dimensions%3A%0Aevolutionary%20generators%2C%20excellent%20pursuit%2C%20and%20realistic%20panorama.%0ASubsequently%2C%20the%20widely%20used%20datasets%20and%20metrics%20are%20organized%20in%20detail.%0ALast%20but%20more%20importantly%2C%20we%20identify%20several%20challenges%20and%20open%20problems%20in%0Athis%20domain%20and%20propose%20potential%20future%20directions%20for%20research%20and%0Adevelopment.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2405.10674v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DFrom%2520Sora%2520What%2520We%2520Can%2520See%253A%2520A%2520Survey%2520of%2520Text-to-Video%2520Generation%26entry.906535625%3DRui%2520Sun%2520and%2520Yumin%2520Zhang%2520and%2520Tejal%2520Shah%2520and%2520Jiahao%2520Sun%2520and%2520Shuoying%2520Zhang%2520and%2520Wenqi%2520Li%2520and%2520Haoran%2520Duan%2520and%2520Bo%2520Wei%2520and%2520Rajiv%2520Ranjan%26entry.1292438233%3D%2520%2520With%2520impressive%2520achievements%2520made%252C%2520artificial%2520intelligence%2520is%2520on%2520the%2520path%250Aforward%2520to%2520artificial%2520general%2520intelligence.%2520Sora%252C%2520developed%2520by%2520OpenAI%252C%2520which%2520is%250Acapable%2520of%2520minute-level%2520world-simulative%2520abilities%2520can%2520be%2520considered%2520as%2520a%250Amilestone%2520on%2520this%2520developmental%2520path.%2520However%252C%2520despite%2520its%2520notable%2520successes%252C%250ASora%2520still%2520encounters%2520various%2520obstacles%2520that%2520need%2520to%2520be%2520resolved.%2520In%2520this%250Asurvey%252C%2520we%2520embark%2520from%2520the%2520perspective%2520of%2520disassembling%2520Sora%2520in%2520text-to-video%250Ageneration%252C%2520and%2520conducting%2520a%2520comprehensive%2520review%2520of%2520literature%252C%2520trying%2520to%250Aanswer%2520the%2520question%252C%2520%255Ctextit%257BFrom%2520Sora%2520What%2520We%2520Can%2520See%257D.%2520Specifically%252C%2520after%250Abasic%2520preliminaries%2520regarding%2520the%2520general%2520algorithms%2520are%2520introduced%252C%2520the%250Aliterature%2520is%2520categorized%2520from%2520three%2520mutually%2520perpendicular%2520dimensions%253A%250Aevolutionary%2520generators%252C%2520excellent%2520pursuit%252C%2520and%2520realistic%2520panorama.%250ASubsequently%252C%2520the%2520widely%2520used%2520datasets%2520and%2520metrics%2520are%2520organized%2520in%2520detail.%250ALast%2520but%2520more%2520importantly%252C%2520we%2520identify%2520several%2520challenges%2520and%2520open%2520problems%2520in%250Athis%2520domain%2520and%2520propose%2520potential%2520future%2520directions%2520for%2520research%2520and%250Adevelopment.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2405.10674v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=From%20Sora%20What%20We%20Can%20See%3A%20A%20Survey%20of%20Text-to-Video%20Generation&entry.906535625=Rui%20Sun%20and%20Yumin%20Zhang%20and%20Tejal%20Shah%20and%20Jiahao%20Sun%20and%20Shuoying%20Zhang%20and%20Wenqi%20Li%20and%20Haoran%20Duan%20and%20Bo%20Wei%20and%20Rajiv%20Ranjan&entry.1292438233=%20%20With%20impressive%20achievements%20made%2C%20artificial%20intelligence%20is%20on%20the%20path%0Aforward%20to%20artificial%20general%20intelligence.%20Sora%2C%20developed%20by%20OpenAI%2C%20which%20is%0Acapable%20of%20minute-level%20world-simulative%20abilities%20can%20be%20considered%20as%20a%0Amilestone%20on%20this%20developmental%20path.%20However%2C%20despite%20its%20notable%20successes%2C%0ASora%20still%20encounters%20various%20obstacles%20that%20need%20to%20be%20resolved.%20In%20this%0Asurvey%2C%20we%20embark%20from%20the%20perspective%20of%20disassembling%20Sora%20in%20text-to-video%0Ageneration%2C%20and%20conducting%20a%20comprehensive%20review%20of%20literature%2C%20trying%20to%0Aanswer%20the%20question%2C%20%5Ctextit%7BFrom%20Sora%20What%20We%20Can%20See%7D.%20Specifically%2C%20after%0Abasic%20preliminaries%20regarding%20the%20general%20algorithms%20are%20introduced%2C%20the%0Aliterature%20is%20categorized%20from%20three%20mutually%20perpendicular%20dimensions%3A%0Aevolutionary%20generators%2C%20excellent%20pursuit%2C%20and%20realistic%20panorama.%0ASubsequently%2C%20the%20widely%20used%20datasets%20and%20metrics%20are%20organized%20in%20detail.%0ALast%20but%20more%20importantly%2C%20we%20identify%20several%20challenges%20and%20open%20problems%20in%0Athis%20domain%20and%20propose%20potential%20future%20directions%20for%20research%20and%0Adevelopment.%0A&entry.1838667208=http%3A//arxiv.org/abs/2405.10674v1&entry.124074799=Read"},
{"title": "UFORecon: Generalizable Sparse-View Surface Reconstruction from\n  Arbitrary and UnFavOrable Sets", "author": "Youngju Na and Woo Jae Kim and Kyu Beom Han and Suhyeon Ha and Sung-eui Yoon", "abstract": "  Generalizable neural implicit surface reconstruction aims to obtain an\naccurate underlying geometry given a limited number of multi-view images from\nunseen scenes. However, existing methods select only informative and relevant\nviews using predefined scores for training and testing phases. This constraint\nrenders the model impractical in real-world scenarios, where the availability\nof favorable combinations cannot always be ensured. We introduce and validate a\nview-combination score to indicate the effectiveness of the input view\ncombination. We observe that previous methods output degenerate solutions under\narbitrary and unfavorable sets. Building upon this finding, we propose\nUFORecon, a robust view-combination generalizable surface reconstruction\nframework. To achieve this, we apply cross-view matching transformers to model\ninteractions between source images and build correlation frustums to capture\nglobal correlations. Additionally, we explicitly encode pairwise feature\nsimilarities as view-consistent priors. Our proposed framework significantly\noutperforms previous methods in terms of view-combination generalizability and\nalso in the conventional generalizable protocol trained with favorable\nview-combinations. The code is available at\nhttps://github.com/Youngju-Na/UFORecon.\n", "link": "http://arxiv.org/abs/2403.05086v3", "date": "2024-05-17", "relevancy": 2.729, "topK": [{"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.5509}, {"title": "RTG-SLAM: Real-time 3D Reconstruction at Scale using Gaussian Splatting", "link": "http://arxiv.org/abs/2404.19706v1", "similarity": 0.5495}, {"title": "CAT3D: Create Anything in 3D with Multi-View Diffusion Models", "link": "http://arxiv.org/abs/2405.10314v1", "similarity": 0.537}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20UFORecon%3A%20Generalizable%20Sparse-View%20Surface%20Reconstruction%20from%0A%20%20Arbitrary%20and%20UnFavOrable%20Sets&body=Title%3A%20UFORecon%3A%20Generalizable%20Sparse-View%20Surface%20Reconstruction%20from%0A%20%20Arbitrary%20and%20UnFavOrable%20Sets%0AAuthor%3A%20Youngju%20Na%20and%20Woo%20Jae%20Kim%20and%20Kyu%20Beom%20Han%20and%20Suhyeon%20Ha%20and%20Sung-eui%20Yoon%0AAbstract%3A%20%20%20Generalizable%20neural%20implicit%20surface%20reconstruction%20aims%20to%20obtain%20an%0Aaccurate%20underlying%20geometry%20given%20a%20limited%20number%20of%20multi-view%20images%20from%0Aunseen%20scenes.%20However%2C%20existing%20methods%20select%20only%20informative%20and%20relevant%0Aviews%20using%20predefined%20scores%20for%20training%20and%20testing%20phases.%20This%20constraint%0Arenders%20the%20model%20impractical%20in%20real-world%20scenarios%2C%20where%20the%20availability%0Aof%20favorable%20combinations%20cannot%20always%20be%20ensured.%20We%20introduce%20and%20validate%20a%0Aview-combination%20score%20to%20indicate%20the%20effectiveness%20of%20the%20input%20view%0Acombination.%20We%20observe%20that%20previous%20methods%20output%20degenerate%20solutions%20under%0Aarbitrary%20and%20unfavorable%20sets.%20Building%20upon%20this%20finding%2C%20we%20propose%0AUFORecon%2C%20a%20robust%20view-combination%20generalizable%20surface%20reconstruction%0Aframework.%20To%20achieve%20this%2C%20we%20apply%20cross-view%20matching%20transformers%20to%20model%0Ainteractions%20between%20source%20images%20and%20build%20correlation%20frustums%20to%20capture%0Aglobal%20correlations.%20Additionally%2C%20we%20explicitly%20encode%20pairwise%20feature%0Asimilarities%20as%20view-consistent%20priors.%20Our%20proposed%20framework%20significantly%0Aoutperforms%20previous%20methods%20in%20terms%20of%20view-combination%20generalizability%20and%0Aalso%20in%20the%20conventional%20generalizable%20protocol%20trained%20with%20favorable%0Aview-combinations.%20The%20code%20is%20available%20at%0Ahttps%3A//github.com/Youngju-Na/UFORecon.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2403.05086v3%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DUFORecon%253A%2520Generalizable%2520Sparse-View%2520Surface%2520Reconstruction%2520from%250A%2520%2520Arbitrary%2520and%2520UnFavOrable%2520Sets%26entry.906535625%3DYoungju%2520Na%2520and%2520Woo%2520Jae%2520Kim%2520and%2520Kyu%2520Beom%2520Han%2520and%2520Suhyeon%2520Ha%2520and%2520Sung-eui%2520Yoon%26entry.1292438233%3D%2520%2520Generalizable%2520neural%2520implicit%2520surface%2520reconstruction%2520aims%2520to%2520obtain%2520an%250Aaccurate%2520underlying%2520geometry%2520given%2520a%2520limited%2520number%2520of%2520multi-view%2520images%2520from%250Aunseen%2520scenes.%2520However%252C%2520existing%2520methods%2520select%2520only%2520informative%2520and%2520relevant%250Aviews%2520using%2520predefined%2520scores%2520for%2520training%2520and%2520testing%2520phases.%2520This%2520constraint%250Arenders%2520the%2520model%2520impractical%2520in%2520real-world%2520scenarios%252C%2520where%2520the%2520availability%250Aof%2520favorable%2520combinations%2520cannot%2520always%2520be%2520ensured.%2520We%2520introduce%2520and%2520validate%2520a%250Aview-combination%2520score%2520to%2520indicate%2520the%2520effectiveness%2520of%2520the%2520input%2520view%250Acombination.%2520We%2520observe%2520that%2520previous%2520methods%2520output%2520degenerate%2520solutions%2520under%250Aarbitrary%2520and%2520unfavorable%2520sets.%2520Building%2520upon%2520this%2520finding%252C%2520we%2520propose%250AUFORecon%252C%2520a%2520robust%2520view-combination%2520generalizable%2520surface%2520reconstruction%250Aframework.%2520To%2520achieve%2520this%252C%2520we%2520apply%2520cross-view%2520matching%2520transformers%2520to%2520model%250Ainteractions%2520between%2520source%2520images%2520and%2520build%2520correlation%2520frustums%2520to%2520capture%250Aglobal%2520correlations.%2520Additionally%252C%2520we%2520explicitly%2520encode%2520pairwise%2520feature%250Asimilarities%2520as%2520view-consistent%2520priors.%2520Our%2520proposed%2520framework%2520significantly%250Aoutperforms%2520previous%2520methods%2520in%2520terms%2520of%2520view-combination%2520generalizability%2520and%250Aalso%2520in%2520the%2520conventional%2520generalizable%2520protocol%2520trained%2520with%2520favorable%250Aview-combinations.%2520The%2520code%2520is%2520available%2520at%250Ahttps%253A//github.com/Youngju-Na/UFORecon.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2403.05086v3%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=UFORecon%3A%20Generalizable%20Sparse-View%20Surface%20Reconstruction%20from%0A%20%20Arbitrary%20and%20UnFavOrable%20Sets&entry.906535625=Youngju%20Na%20and%20Woo%20Jae%20Kim%20and%20Kyu%20Beom%20Han%20and%20Suhyeon%20Ha%20and%20Sung-eui%20Yoon&entry.1292438233=%20%20Generalizable%20neural%20implicit%20surface%20reconstruction%20aims%20to%20obtain%20an%0Aaccurate%20underlying%20geometry%20given%20a%20limited%20number%20of%20multi-view%20images%20from%0Aunseen%20scenes.%20However%2C%20existing%20methods%20select%20only%20informative%20and%20relevant%0Aviews%20using%20predefined%20scores%20for%20training%20and%20testing%20phases.%20This%20constraint%0Arenders%20the%20model%20impractical%20in%20real-world%20scenarios%2C%20where%20the%20availability%0Aof%20favorable%20combinations%20cannot%20always%20be%20ensured.%20We%20introduce%20and%20validate%20a%0Aview-combination%20score%20to%20indicate%20the%20effectiveness%20of%20the%20input%20view%0Acombination.%20We%20observe%20that%20previous%20methods%20output%20degenerate%20solutions%20under%0Aarbitrary%20and%20unfavorable%20sets.%20Building%20upon%20this%20finding%2C%20we%20propose%0AUFORecon%2C%20a%20robust%20view-combination%20generalizable%20surface%20reconstruction%0Aframework.%20To%20achieve%20this%2C%20we%20apply%20cross-view%20matching%20transformers%20to%20model%0Ainteractions%20between%20source%20images%20and%20build%20correlation%20frustums%20to%20capture%0Aglobal%20correlations.%20Additionally%2C%20we%20explicitly%20encode%20pairwise%20feature%0Asimilarities%20as%20view-consistent%20priors.%20Our%20proposed%20framework%20significantly%0Aoutperforms%20previous%20methods%20in%20terms%20of%20view-combination%20generalizability%20and%0Aalso%20in%20the%20conventional%20generalizable%20protocol%20trained%20with%20favorable%0Aview-combinations.%20The%20code%20is%20available%20at%0Ahttps%3A//github.com/Youngju-Na/UFORecon.%0A&entry.1838667208=http%3A//arxiv.org/abs/2403.05086v3&entry.124074799=Read"},
{"title": "Cross-Silo Federated Learning Across Divergent Domains with Iterative\n  Parameter Alignment", "author": "Matt Gorbett and Hossein Shirazi and Indrakshi Ray", "abstract": "  Learning from the collective knowledge of data dispersed across private\nsources can provide neural networks with enhanced generalization capabilities.\nFederated learning, a method for collaboratively training a machine learning\nmodel across remote clients, achieves this by combining client models via the\norchestration of a central server. However, current approaches face two\ncritical limitations: i) they struggle to converge when client domains are\nsufficiently different, and ii) current aggregation techniques produce an\nidentical global model for each client. In this work, we address these issues\nby reformulating the typical federated learning setup: rather than learning a\nsingle global model, we learn N models each optimized for a common objective.\nTo achieve this, we apply a weighted distance minimization to model parameters\nshared in a peer-to-peer topology. The resulting framework, Iterative Parameter\nAlignment, applies naturally to the cross-silo setting, and has the following\nproperties: (i) a unique solution for each participant, with the option to\nglobally converge each model in the federation, and (ii) an optional\nearly-stopping mechanism to elicit fairness among peers in collaborative\nlearning settings. These characteristics jointly provide a flexible new\nframework for iteratively learning from peer models trained on disparate\ndatasets. We find that the technique achieves competitive results on a variety\nof data partitions compared to state-of-the-art approaches. Further, we show\nthat the method is robust to divergent domains (i.e. disjoint classes across\npeers) where existing approaches struggle.\n", "link": "http://arxiv.org/abs/2311.04818v5", "date": "2024-05-17", "relevancy": 2.6934, "topK": [{"title": "DDM-NET: End-to-end learning of keypoint feature Detection, Description and Matching for 3D localization", "link": "https://arxiv.org/abs/2212.04575", "similarity": 0.5496}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5437}, {"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.5228}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Cross-Silo%20Federated%20Learning%20Across%20Divergent%20Domains%20with%20Iterative%0A%20%20Parameter%20Alignment&body=Title%3A%20Cross-Silo%20Federated%20Learning%20Across%20Divergent%20Domains%20with%20Iterative%0A%20%20Parameter%20Alignment%0AAuthor%3A%20Matt%20Gorbett%20and%20Hossein%20Shirazi%20and%20Indrakshi%20Ray%0AAbstract%3A%20%20%20Learning%20from%20the%20collective%20knowledge%20of%20data%20dispersed%20across%20private%0Asources%20can%20provide%20neural%20networks%20with%20enhanced%20generalization%20capabilities.%0AFederated%20learning%2C%20a%20method%20for%20collaboratively%20training%20a%20machine%20learning%0Amodel%20across%20remote%20clients%2C%20achieves%20this%20by%20combining%20client%20models%20via%20the%0Aorchestration%20of%20a%20central%20server.%20However%2C%20current%20approaches%20face%20two%0Acritical%20limitations%3A%20i%29%20they%20struggle%20to%20converge%20when%20client%20domains%20are%0Asufficiently%20different%2C%20and%20ii%29%20current%20aggregation%20techniques%20produce%20an%0Aidentical%20global%20model%20for%20each%20client.%20In%20this%20work%2C%20we%20address%20these%20issues%0Aby%20reformulating%20the%20typical%20federated%20learning%20setup%3A%20rather%20than%20learning%20a%0Asingle%20global%20model%2C%20we%20learn%20N%20models%20each%20optimized%20for%20a%20common%20objective.%0ATo%20achieve%20this%2C%20we%20apply%20a%20weighted%20distance%20minimization%20to%20model%20parameters%0Ashared%20in%20a%20peer-to-peer%20topology.%20The%20resulting%20framework%2C%20Iterative%20Parameter%0AAlignment%2C%20applies%20naturally%20to%20the%20cross-silo%20setting%2C%20and%20has%20the%20following%0Aproperties%3A%20%28i%29%20a%20unique%20solution%20for%20each%20participant%2C%20with%20the%20option%20to%0Aglobally%20converge%20each%20model%20in%20the%20federation%2C%20and%20%28ii%29%20an%20optional%0Aearly-stopping%20mechanism%20to%20elicit%20fairness%20among%20peers%20in%20collaborative%0Alearning%20settings.%20These%20characteristics%20jointly%20provide%20a%20flexible%20new%0Aframework%20for%20iteratively%20learning%20from%20peer%20models%20trained%20on%20disparate%0Adatasets.%20We%20find%20that%20the%20technique%20achieves%20competitive%20results%20on%20a%20variety%0Aof%20data%20partitions%20compared%20to%20state-of-the-art%20approaches.%20Further%2C%20we%20show%0Athat%20the%20method%20is%20robust%20to%20divergent%20domains%20%28i.e.%20disjoint%20classes%20across%0Apeers%29%20where%20existing%20approaches%20struggle.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2311.04818v5%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DCross-Silo%2520Federated%2520Learning%2520Across%2520Divergent%2520Domains%2520with%2520Iterative%250A%2520%2520Parameter%2520Alignment%26entry.906535625%3DMatt%2520Gorbett%2520and%2520Hossein%2520Shirazi%2520and%2520Indrakshi%2520Ray%26entry.1292438233%3D%2520%2520Learning%2520from%2520the%2520collective%2520knowledge%2520of%2520data%2520dispersed%2520across%2520private%250Asources%2520can%2520provide%2520neural%2520networks%2520with%2520enhanced%2520generalization%2520capabilities.%250AFederated%2520learning%252C%2520a%2520method%2520for%2520collaboratively%2520training%2520a%2520machine%2520learning%250Amodel%2520across%2520remote%2520clients%252C%2520achieves%2520this%2520by%2520combining%2520client%2520models%2520via%2520the%250Aorchestration%2520of%2520a%2520central%2520server.%2520However%252C%2520current%2520approaches%2520face%2520two%250Acritical%2520limitations%253A%2520i%2529%2520they%2520struggle%2520to%2520converge%2520when%2520client%2520domains%2520are%250Asufficiently%2520different%252C%2520and%2520ii%2529%2520current%2520aggregation%2520techniques%2520produce%2520an%250Aidentical%2520global%2520model%2520for%2520each%2520client.%2520In%2520this%2520work%252C%2520we%2520address%2520these%2520issues%250Aby%2520reformulating%2520the%2520typical%2520federated%2520learning%2520setup%253A%2520rather%2520than%2520learning%2520a%250Asingle%2520global%2520model%252C%2520we%2520learn%2520N%2520models%2520each%2520optimized%2520for%2520a%2520common%2520objective.%250ATo%2520achieve%2520this%252C%2520we%2520apply%2520a%2520weighted%2520distance%2520minimization%2520to%2520model%2520parameters%250Ashared%2520in%2520a%2520peer-to-peer%2520topology.%2520The%2520resulting%2520framework%252C%2520Iterative%2520Parameter%250AAlignment%252C%2520applies%2520naturally%2520to%2520the%2520cross-silo%2520setting%252C%2520and%2520has%2520the%2520following%250Aproperties%253A%2520%2528i%2529%2520a%2520unique%2520solution%2520for%2520each%2520participant%252C%2520with%2520the%2520option%2520to%250Aglobally%2520converge%2520each%2520model%2520in%2520the%2520federation%252C%2520and%2520%2528ii%2529%2520an%2520optional%250Aearly-stopping%2520mechanism%2520to%2520elicit%2520fairness%2520among%2520peers%2520in%2520collaborative%250Alearning%2520settings.%2520These%2520characteristics%2520jointly%2520provide%2520a%2520flexible%2520new%250Aframework%2520for%2520iteratively%2520learning%2520from%2520peer%2520models%2520trained%2520on%2520disparate%250Adatasets.%2520We%2520find%2520that%2520the%2520technique%2520achieves%2520competitive%2520results%2520on%2520a%2520variety%250Aof%2520data%2520partitions%2520compared%2520to%2520state-of-the-art%2520approaches.%2520Further%252C%2520we%2520show%250Athat%2520the%2520method%2520is%2520robust%2520to%2520divergent%2520domains%2520%2528i.e.%2520disjoint%2520classes%2520across%250Apeers%2529%2520where%2520existing%2520approaches%2520struggle.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2311.04818v5%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Cross-Silo%20Federated%20Learning%20Across%20Divergent%20Domains%20with%20Iterative%0A%20%20Parameter%20Alignment&entry.906535625=Matt%20Gorbett%20and%20Hossein%20Shirazi%20and%20Indrakshi%20Ray&entry.1292438233=%20%20Learning%20from%20the%20collective%20knowledge%20of%20data%20dispersed%20across%20private%0Asources%20can%20provide%20neural%20networks%20with%20enhanced%20generalization%20capabilities.%0AFederated%20learning%2C%20a%20method%20for%20collaboratively%20training%20a%20machine%20learning%0Amodel%20across%20remote%20clients%2C%20achieves%20this%20by%20combining%20client%20models%20via%20the%0Aorchestration%20of%20a%20central%20server.%20However%2C%20current%20approaches%20face%20two%0Acritical%20limitations%3A%20i%29%20they%20struggle%20to%20converge%20when%20client%20domains%20are%0Asufficiently%20different%2C%20and%20ii%29%20current%20aggregation%20techniques%20produce%20an%0Aidentical%20global%20model%20for%20each%20client.%20In%20this%20work%2C%20we%20address%20these%20issues%0Aby%20reformulating%20the%20typical%20federated%20learning%20setup%3A%20rather%20than%20learning%20a%0Asingle%20global%20model%2C%20we%20learn%20N%20models%20each%20optimized%20for%20a%20common%20objective.%0ATo%20achieve%20this%2C%20we%20apply%20a%20weighted%20distance%20minimization%20to%20model%20parameters%0Ashared%20in%20a%20peer-to-peer%20topology.%20The%20resulting%20framework%2C%20Iterative%20Parameter%0AAlignment%2C%20applies%20naturally%20to%20the%20cross-silo%20setting%2C%20and%20has%20the%20following%0Aproperties%3A%20%28i%29%20a%20unique%20solution%20for%20each%20participant%2C%20with%20the%20option%20to%0Aglobally%20converge%20each%20model%20in%20the%20federation%2C%20and%20%28ii%29%20an%20optional%0Aearly-stopping%20mechanism%20to%20elicit%20fairness%20among%20peers%20in%20collaborative%0Alearning%20settings.%20These%20characteristics%20jointly%20provide%20a%20flexible%20new%0Aframework%20for%20iteratively%20learning%20from%20peer%20models%20trained%20on%20disparate%0Adatasets.%20We%20find%20that%20the%20technique%20achieves%20competitive%20results%20on%20a%20variety%0Aof%20data%20partitions%20compared%20to%20state-of-the-art%20approaches.%20Further%2C%20we%20show%0Athat%20the%20method%20is%20robust%20to%20divergent%20domains%20%28i.e.%20disjoint%20classes%20across%0Apeers%29%20where%20existing%20approaches%20struggle.%0A&entry.1838667208=http%3A//arxiv.org/abs/2311.04818v5&entry.124074799=Read"},
{"title": "Adapt Before Comparison: A New Perspective on Cross-Domain Few-Shot\n  Segmentation", "author": "Jonas Herzog", "abstract": "  Few-shot segmentation performance declines substantially when facing images\nfrom a domain different than the training domain, effectively limiting\nreal-world use cases. To alleviate this, recently cross-domain few-shot\nsegmentation (CD-FSS) has emerged. Works that address this task mainly\nattempted to learn segmentation on a source domain in a manner that generalizes\nacross domains. Surprisingly, we can outperform these approaches while\neliminating the training stage and removing their main segmentation network. We\nshow test-time task-adaption is the key for successful CD-FSS instead.\nTask-adaption is achieved by appending small networks to the feature pyramid of\na conventionally classification-pretrained backbone. To avoid overfitting to\nthe few labeled samples in supervised fine-tuning, consistency across augmented\nviews of input images serves as guidance while learning the parameters of the\nattached layers. Despite our self-restriction not to use any images other than\nthe few labeled samples at test time, we achieve new state-of-the-art\nperformance in CD-FSS, evidencing the need to rethink approaches for the task.\n", "link": "http://arxiv.org/abs/2402.17614v2", "date": "2024-05-17", "relevancy": 2.5745, "topK": [{"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5441}, {"title": "ToDo: Token Downsampling for Efficient Generation of High-Resolution\n  Images", "link": "http://arxiv.org/abs/2402.13573v2", "similarity": 0.5014}, {"title": "Breadcrumbs: Adversarial class-balanced sampling for long-tailed recognition", "link": "https://link.springer.com/chapter/10.1007/978-3-031-20053-3_37", "similarity": 0.4992}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Adapt%20Before%20Comparison%3A%20A%20New%20Perspective%20on%20Cross-Domain%20Few-Shot%0A%20%20Segmentation&body=Title%3A%20Adapt%20Before%20Comparison%3A%20A%20New%20Perspective%20on%20Cross-Domain%20Few-Shot%0A%20%20Segmentation%0AAuthor%3A%20Jonas%20Herzog%0AAbstract%3A%20%20%20Few-shot%20segmentation%20performance%20declines%20substantially%20when%20facing%20images%0Afrom%20a%20domain%20different%20than%20the%20training%20domain%2C%20effectively%20limiting%0Areal-world%20use%20cases.%20To%20alleviate%20this%2C%20recently%20cross-domain%20few-shot%0Asegmentation%20%28CD-FSS%29%20has%20emerged.%20Works%20that%20address%20this%20task%20mainly%0Aattempted%20to%20learn%20segmentation%20on%20a%20source%20domain%20in%20a%20manner%20that%20generalizes%0Aacross%20domains.%20Surprisingly%2C%20we%20can%20outperform%20these%20approaches%20while%0Aeliminating%20the%20training%20stage%20and%20removing%20their%20main%20segmentation%20network.%20We%0Ashow%20test-time%20task-adaption%20is%20the%20key%20for%20successful%20CD-FSS%20instead.%0ATask-adaption%20is%20achieved%20by%20appending%20small%20networks%20to%20the%20feature%20pyramid%20of%0Aa%20conventionally%20classification-pretrained%20backbone.%20To%20avoid%20overfitting%20to%0Athe%20few%20labeled%20samples%20in%20supervised%20fine-tuning%2C%20consistency%20across%20augmented%0Aviews%20of%20input%20images%20serves%20as%20guidance%20while%20learning%20the%20parameters%20of%20the%0Aattached%20layers.%20Despite%20our%20self-restriction%20not%20to%20use%20any%20images%20other%20than%0Athe%20few%20labeled%20samples%20at%20test%20time%2C%20we%20achieve%20new%20state-of-the-art%0Aperformance%20in%20CD-FSS%2C%20evidencing%20the%20need%20to%20rethink%20approaches%20for%20the%20task.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2402.17614v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DAdapt%2520Before%2520Comparison%253A%2520A%2520New%2520Perspective%2520on%2520Cross-Domain%2520Few-Shot%250A%2520%2520Segmentation%26entry.906535625%3DJonas%2520Herzog%26entry.1292438233%3D%2520%2520Few-shot%2520segmentation%2520performance%2520declines%2520substantially%2520when%2520facing%2520images%250Afrom%2520a%2520domain%2520different%2520than%2520the%2520training%2520domain%252C%2520effectively%2520limiting%250Areal-world%2520use%2520cases.%2520To%2520alleviate%2520this%252C%2520recently%2520cross-domain%2520few-shot%250Asegmentation%2520%2528CD-FSS%2529%2520has%2520emerged.%2520Works%2520that%2520address%2520this%2520task%2520mainly%250Aattempted%2520to%2520learn%2520segmentation%2520on%2520a%2520source%2520domain%2520in%2520a%2520manner%2520that%2520generalizes%250Aacross%2520domains.%2520Surprisingly%252C%2520we%2520can%2520outperform%2520these%2520approaches%2520while%250Aeliminating%2520the%2520training%2520stage%2520and%2520removing%2520their%2520main%2520segmentation%2520network.%2520We%250Ashow%2520test-time%2520task-adaption%2520is%2520the%2520key%2520for%2520successful%2520CD-FSS%2520instead.%250ATask-adaption%2520is%2520achieved%2520by%2520appending%2520small%2520networks%2520to%2520the%2520feature%2520pyramid%2520of%250Aa%2520conventionally%2520classification-pretrained%2520backbone.%2520To%2520avoid%2520overfitting%2520to%250Athe%2520few%2520labeled%2520samples%2520in%2520supervised%2520fine-tuning%252C%2520consistency%2520across%2520augmented%250Aviews%2520of%2520input%2520images%2520serves%2520as%2520guidance%2520while%2520learning%2520the%2520parameters%2520of%2520the%250Aattached%2520layers.%2520Despite%2520our%2520self-restriction%2520not%2520to%2520use%2520any%2520images%2520other%2520than%250Athe%2520few%2520labeled%2520samples%2520at%2520test%2520time%252C%2520we%2520achieve%2520new%2520state-of-the-art%250Aperformance%2520in%2520CD-FSS%252C%2520evidencing%2520the%2520need%2520to%2520rethink%2520approaches%2520for%2520the%2520task.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2402.17614v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Adapt%20Before%20Comparison%3A%20A%20New%20Perspective%20on%20Cross-Domain%20Few-Shot%0A%20%20Segmentation&entry.906535625=Jonas%20Herzog&entry.1292438233=%20%20Few-shot%20segmentation%20performance%20declines%20substantially%20when%20facing%20images%0Afrom%20a%20domain%20different%20than%20the%20training%20domain%2C%20effectively%20limiting%0Areal-world%20use%20cases.%20To%20alleviate%20this%2C%20recently%20cross-domain%20few-shot%0Asegmentation%20%28CD-FSS%29%20has%20emerged.%20Works%20that%20address%20this%20task%20mainly%0Aattempted%20to%20learn%20segmentation%20on%20a%20source%20domain%20in%20a%20manner%20that%20generalizes%0Aacross%20domains.%20Surprisingly%2C%20we%20can%20outperform%20these%20approaches%20while%0Aeliminating%20the%20training%20stage%20and%20removing%20their%20main%20segmentation%20network.%20We%0Ashow%20test-time%20task-adaption%20is%20the%20key%20for%20successful%20CD-FSS%20instead.%0ATask-adaption%20is%20achieved%20by%20appending%20small%20networks%20to%20the%20feature%20pyramid%20of%0Aa%20conventionally%20classification-pretrained%20backbone.%20To%20avoid%20overfitting%20to%0Athe%20few%20labeled%20samples%20in%20supervised%20fine-tuning%2C%20consistency%20across%20augmented%0Aviews%20of%20input%20images%20serves%20as%20guidance%20while%20learning%20the%20parameters%20of%20the%0Aattached%20layers.%20Despite%20our%20self-restriction%20not%20to%20use%20any%20images%20other%20than%0Athe%20few%20labeled%20samples%20at%20test%20time%2C%20we%20achieve%20new%20state-of-the-art%0Aperformance%20in%20CD-FSS%2C%20evidencing%20the%20need%20to%20rethink%20approaches%20for%20the%20task.%0A&entry.1838667208=http%3A//arxiv.org/abs/2402.17614v2&entry.124074799=Read"},
{"title": "One registration is worth two segmentations", "author": "Shiqi Huang and Tingfa Xu and Ziyi Shen and Shaheer Ullah Saeed and Wen Yan and Dean Barratt and Yipeng Hu", "abstract": "  The goal of image registration is to establish spatial correspondence between\ntwo or more images, traditionally through dense displacement fields (DDFs) or\nparametric transformations (e.g., rigid, affine, and splines). Rethinking the\nexisting paradigms of achieving alignment via spatial transformations, we\nuncover an alternative but more intuitive correspondence representation: a set\nof corresponding regions-of-interest (ROI) pairs, which we demonstrate to have\nsufficient representational capability as other correspondence representation\nmethods.Further, it is neither necessary nor sufficient for these ROIs to hold\nspecific anatomical or semantic significance. In turn, we formulate image\nregistration as searching for the same set of corresponding ROIs from both\nmoving and fixed images - in other words, two multi-class segmentation tasks on\na pair of images. For a general-purpose and practical implementation, we\nintegrate the segment anything model (SAM) into our proposed algorithms,\nresulting in a SAM-enabled registration (SAMReg) that does not require any\ntraining data, gradient-based fine-tuning or engineered prompts. We\nexperimentally show that the proposed SAMReg is capable of segmenting and\nmatching multiple ROI pairs, which establish sufficiently accurate\ncorrespondences, in three clinical applications of registering prostate MR,\ncardiac MR and abdominal CT images. Based on metrics including Dice and target\nregistration errors on anatomical structures, the proposed registration\noutperforms both intensity-based iterative algorithms and DDF-predicting\nlearning-based networks, even yielding competitive performance with\nweakly-supervised registration which requires fully-segmented training data.\n", "link": "http://arxiv.org/abs/2405.10879v1", "date": "2024-05-17", "relevancy": 2.5506, "topK": [{"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5267}, {"title": "DDM-NET: End-to-end learning of keypoint feature Detection, Description and Matching for 3D localization", "link": "https://arxiv.org/abs/2212.04575", "similarity": 0.513}, {"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.4906}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20One%20registration%20is%20worth%20two%20segmentations&body=Title%3A%20One%20registration%20is%20worth%20two%20segmentations%0AAuthor%3A%20Shiqi%20Huang%20and%20Tingfa%20Xu%20and%20Ziyi%20Shen%20and%20Shaheer%20Ullah%20Saeed%20and%20Wen%20Yan%20and%20Dean%20Barratt%20and%20Yipeng%20Hu%0AAbstract%3A%20%20%20The%20goal%20of%20image%20registration%20is%20to%20establish%20spatial%20correspondence%20between%0Atwo%20or%20more%20images%2C%20traditionally%20through%20dense%20displacement%20fields%20%28DDFs%29%20or%0Aparametric%20transformations%20%28e.g.%2C%20rigid%2C%20affine%2C%20and%20splines%29.%20Rethinking%20the%0Aexisting%20paradigms%20of%20achieving%20alignment%20via%20spatial%20transformations%2C%20we%0Auncover%20an%20alternative%20but%20more%20intuitive%20correspondence%20representation%3A%20a%20set%0Aof%20corresponding%20regions-of-interest%20%28ROI%29%20pairs%2C%20which%20we%20demonstrate%20to%20have%0Asufficient%20representational%20capability%20as%20other%20correspondence%20representation%0Amethods.Further%2C%20it%20is%20neither%20necessary%20nor%20sufficient%20for%20these%20ROIs%20to%20hold%0Aspecific%20anatomical%20or%20semantic%20significance.%20In%20turn%2C%20we%20formulate%20image%0Aregistration%20as%20searching%20for%20the%20same%20set%20of%20corresponding%20ROIs%20from%20both%0Amoving%20and%20fixed%20images%20-%20in%20other%20words%2C%20two%20multi-class%20segmentation%20tasks%20on%0Aa%20pair%20of%20images.%20For%20a%20general-purpose%20and%20practical%20implementation%2C%20we%0Aintegrate%20the%20segment%20anything%20model%20%28SAM%29%20into%20our%20proposed%20algorithms%2C%0Aresulting%20in%20a%20SAM-enabled%20registration%20%28SAMReg%29%20that%20does%20not%20require%20any%0Atraining%20data%2C%20gradient-based%20fine-tuning%20or%20engineered%20prompts.%20We%0Aexperimentally%20show%20that%20the%20proposed%20SAMReg%20is%20capable%20of%20segmenting%20and%0Amatching%20multiple%20ROI%20pairs%2C%20which%20establish%20sufficiently%20accurate%0Acorrespondences%2C%20in%20three%20clinical%20applications%20of%20registering%20prostate%20MR%2C%0Acardiac%20MR%20and%20abdominal%20CT%20images.%20Based%20on%20metrics%20including%20Dice%20and%20target%0Aregistration%20errors%20on%20anatomical%20structures%2C%20the%20proposed%20registration%0Aoutperforms%20both%20intensity-based%20iterative%20algorithms%20and%20DDF-predicting%0Alearning-based%20networks%2C%20even%20yielding%20competitive%20performance%20with%0Aweakly-supervised%20registration%20which%20requires%20fully-segmented%20training%20data.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2405.10879v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DOne%2520registration%2520is%2520worth%2520two%2520segmentations%26entry.906535625%3DShiqi%2520Huang%2520and%2520Tingfa%2520Xu%2520and%2520Ziyi%2520Shen%2520and%2520Shaheer%2520Ullah%2520Saeed%2520and%2520Wen%2520Yan%2520and%2520Dean%2520Barratt%2520and%2520Yipeng%2520Hu%26entry.1292438233%3D%2520%2520The%2520goal%2520of%2520image%2520registration%2520is%2520to%2520establish%2520spatial%2520correspondence%2520between%250Atwo%2520or%2520more%2520images%252C%2520traditionally%2520through%2520dense%2520displacement%2520fields%2520%2528DDFs%2529%2520or%250Aparametric%2520transformations%2520%2528e.g.%252C%2520rigid%252C%2520affine%252C%2520and%2520splines%2529.%2520Rethinking%2520the%250Aexisting%2520paradigms%2520of%2520achieving%2520alignment%2520via%2520spatial%2520transformations%252C%2520we%250Auncover%2520an%2520alternative%2520but%2520more%2520intuitive%2520correspondence%2520representation%253A%2520a%2520set%250Aof%2520corresponding%2520regions-of-interest%2520%2528ROI%2529%2520pairs%252C%2520which%2520we%2520demonstrate%2520to%2520have%250Asufficient%2520representational%2520capability%2520as%2520other%2520correspondence%2520representation%250Amethods.Further%252C%2520it%2520is%2520neither%2520necessary%2520nor%2520sufficient%2520for%2520these%2520ROIs%2520to%2520hold%250Aspecific%2520anatomical%2520or%2520semantic%2520significance.%2520In%2520turn%252C%2520we%2520formulate%2520image%250Aregistration%2520as%2520searching%2520for%2520the%2520same%2520set%2520of%2520corresponding%2520ROIs%2520from%2520both%250Amoving%2520and%2520fixed%2520images%2520-%2520in%2520other%2520words%252C%2520two%2520multi-class%2520segmentation%2520tasks%2520on%250Aa%2520pair%2520of%2520images.%2520For%2520a%2520general-purpose%2520and%2520practical%2520implementation%252C%2520we%250Aintegrate%2520the%2520segment%2520anything%2520model%2520%2528SAM%2529%2520into%2520our%2520proposed%2520algorithms%252C%250Aresulting%2520in%2520a%2520SAM-enabled%2520registration%2520%2528SAMReg%2529%2520that%2520does%2520not%2520require%2520any%250Atraining%2520data%252C%2520gradient-based%2520fine-tuning%2520or%2520engineered%2520prompts.%2520We%250Aexperimentally%2520show%2520that%2520the%2520proposed%2520SAMReg%2520is%2520capable%2520of%2520segmenting%2520and%250Amatching%2520multiple%2520ROI%2520pairs%252C%2520which%2520establish%2520sufficiently%2520accurate%250Acorrespondences%252C%2520in%2520three%2520clinical%2520applications%2520of%2520registering%2520prostate%2520MR%252C%250Acardiac%2520MR%2520and%2520abdominal%2520CT%2520images.%2520Based%2520on%2520metrics%2520including%2520Dice%2520and%2520target%250Aregistration%2520errors%2520on%2520anatomical%2520structures%252C%2520the%2520proposed%2520registration%250Aoutperforms%2520both%2520intensity-based%2520iterative%2520algorithms%2520and%2520DDF-predicting%250Alearning-based%2520networks%252C%2520even%2520yielding%2520competitive%2520performance%2520with%250Aweakly-supervised%2520registration%2520which%2520requires%2520fully-segmented%2520training%2520data.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2405.10879v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=One%20registration%20is%20worth%20two%20segmentations&entry.906535625=Shiqi%20Huang%20and%20Tingfa%20Xu%20and%20Ziyi%20Shen%20and%20Shaheer%20Ullah%20Saeed%20and%20Wen%20Yan%20and%20Dean%20Barratt%20and%20Yipeng%20Hu&entry.1292438233=%20%20The%20goal%20of%20image%20registration%20is%20to%20establish%20spatial%20correspondence%20between%0Atwo%20or%20more%20images%2C%20traditionally%20through%20dense%20displacement%20fields%20%28DDFs%29%20or%0Aparametric%20transformations%20%28e.g.%2C%20rigid%2C%20affine%2C%20and%20splines%29.%20Rethinking%20the%0Aexisting%20paradigms%20of%20achieving%20alignment%20via%20spatial%20transformations%2C%20we%0Auncover%20an%20alternative%20but%20more%20intuitive%20correspondence%20representation%3A%20a%20set%0Aof%20corresponding%20regions-of-interest%20%28ROI%29%20pairs%2C%20which%20we%20demonstrate%20to%20have%0Asufficient%20representational%20capability%20as%20other%20correspondence%20representation%0Amethods.Further%2C%20it%20is%20neither%20necessary%20nor%20sufficient%20for%20these%20ROIs%20to%20hold%0Aspecific%20anatomical%20or%20semantic%20significance.%20In%20turn%2C%20we%20formulate%20image%0Aregistration%20as%20searching%20for%20the%20same%20set%20of%20corresponding%20ROIs%20from%20both%0Amoving%20and%20fixed%20images%20-%20in%20other%20words%2C%20two%20multi-class%20segmentation%20tasks%20on%0Aa%20pair%20of%20images.%20For%20a%20general-purpose%20and%20practical%20implementation%2C%20we%0Aintegrate%20the%20segment%20anything%20model%20%28SAM%29%20into%20our%20proposed%20algorithms%2C%0Aresulting%20in%20a%20SAM-enabled%20registration%20%28SAMReg%29%20that%20does%20not%20require%20any%0Atraining%20data%2C%20gradient-based%20fine-tuning%20or%20engineered%20prompts.%20We%0Aexperimentally%20show%20that%20the%20proposed%20SAMReg%20is%20capable%20of%20segmenting%20and%0Amatching%20multiple%20ROI%20pairs%2C%20which%20establish%20sufficiently%20accurate%0Acorrespondences%2C%20in%20three%20clinical%20applications%20of%20registering%20prostate%20MR%2C%0Acardiac%20MR%20and%20abdominal%20CT%20images.%20Based%20on%20metrics%20including%20Dice%20and%20target%0Aregistration%20errors%20on%20anatomical%20structures%2C%20the%20proposed%20registration%0Aoutperforms%20both%20intensity-based%20iterative%20algorithms%20and%20DDF-predicting%0Alearning-based%20networks%2C%20even%20yielding%20competitive%20performance%20with%0Aweakly-supervised%20registration%20which%20requires%20fully-segmented%20training%20data.%0A&entry.1838667208=http%3A//arxiv.org/abs/2405.10879v1&entry.124074799=Read"},
{"title": "CapHuman: Capture Your Moments in Parallel Universes", "author": "Chao Liang and Fan Ma and Linchao Zhu and Yingying Deng and Yi Yang", "abstract": "  We concentrate on a novel human-centric image synthesis task, that is, given\nonly one reference facial photograph, it is expected to generate specific\nindividual images with diverse head positions, poses, facial expressions, and\nilluminations in different contexts. To accomplish this goal, we argue that our\ngenerative model should be capable of the following favorable characteristics:\n(1) a strong visual and semantic understanding of our world and human society\nfor basic object and human image generation. (2) generalizable identity\npreservation ability. (3) flexible and fine-grained head control. Recently,\nlarge pre-trained text-to-image diffusion models have shown remarkable results,\nserving as a powerful generative foundation. As a basis, we aim to unleash the\nabove two capabilities of the pre-trained model. In this work, we present a new\nframework named CapHuman. We embrace the \"encode then learn to align\" paradigm,\nwhich enables generalizable identity preservation for new individuals without\ncumbersome tuning at inference. CapHuman encodes identity features and then\nlearns to align them into the latent space. Moreover, we introduce the 3D\nfacial prior to equip our model with control over the human head in a flexible\nand 3D-consistent manner. Extensive qualitative and quantitative analyses\ndemonstrate our CapHuman can produce well-identity-preserved, photo-realistic,\nand high-fidelity portraits with content-rich representations and various head\nrenditions, superior to established baselines. Code and checkpoint will be\nreleased at https://github.com/VamosC/CapHuman.\n", "link": "http://arxiv.org/abs/2402.00627v3", "date": "2024-05-17", "relevancy": 2.5473, "topK": [{"title": "Magic-Me: Identity-Specific Video Customized Diffusion", "link": "http://arxiv.org/abs/2402.09368v1", "similarity": 0.6424}, {"title": "VirtualModel: Generating Object-ID-retentive Human-object Interaction\n  Image by Diffusion Model for E-commerce Marketing", "link": "http://arxiv.org/abs/2405.09985v1", "similarity": 0.6351}, {"title": "Total Selfie: Generating Full-Body Selfies", "link": "http://arxiv.org/abs/2308.14740v2", "similarity": 0.627}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20CapHuman%3A%20Capture%20Your%20Moments%20in%20Parallel%20Universes&body=Title%3A%20CapHuman%3A%20Capture%20Your%20Moments%20in%20Parallel%20Universes%0AAuthor%3A%20Chao%20Liang%20and%20Fan%20Ma%20and%20Linchao%20Zhu%20and%20Yingying%20Deng%20and%20Yi%20Yang%0AAbstract%3A%20%20%20We%20concentrate%20on%20a%20novel%20human-centric%20image%20synthesis%20task%2C%20that%20is%2C%20given%0Aonly%20one%20reference%20facial%20photograph%2C%20it%20is%20expected%20to%20generate%20specific%0Aindividual%20images%20with%20diverse%20head%20positions%2C%20poses%2C%20facial%20expressions%2C%20and%0Ailluminations%20in%20different%20contexts.%20To%20accomplish%20this%20goal%2C%20we%20argue%20that%20our%0Agenerative%20model%20should%20be%20capable%20of%20the%20following%20favorable%20characteristics%3A%0A%281%29%20a%20strong%20visual%20and%20semantic%20understanding%20of%20our%20world%20and%20human%20society%0Afor%20basic%20object%20and%20human%20image%20generation.%20%282%29%20generalizable%20identity%0Apreservation%20ability.%20%283%29%20flexible%20and%20fine-grained%20head%20control.%20Recently%2C%0Alarge%20pre-trained%20text-to-image%20diffusion%20models%20have%20shown%20remarkable%20results%2C%0Aserving%20as%20a%20powerful%20generative%20foundation.%20As%20a%20basis%2C%20we%20aim%20to%20unleash%20the%0Aabove%20two%20capabilities%20of%20the%20pre-trained%20model.%20In%20this%20work%2C%20we%20present%20a%20new%0Aframework%20named%20CapHuman.%20We%20embrace%20the%20%22encode%20then%20learn%20to%20align%22%20paradigm%2C%0Awhich%20enables%20generalizable%20identity%20preservation%20for%20new%20individuals%20without%0Acumbersome%20tuning%20at%20inference.%20CapHuman%20encodes%20identity%20features%20and%20then%0Alearns%20to%20align%20them%20into%20the%20latent%20space.%20Moreover%2C%20we%20introduce%20the%203D%0Afacial%20prior%20to%20equip%20our%20model%20with%20control%20over%20the%20human%20head%20in%20a%20flexible%0Aand%203D-consistent%20manner.%20Extensive%20qualitative%20and%20quantitative%20analyses%0Ademonstrate%20our%20CapHuman%20can%20produce%20well-identity-preserved%2C%20photo-realistic%2C%0Aand%20high-fidelity%20portraits%20with%20content-rich%20representations%20and%20various%20head%0Arenditions%2C%20superior%20to%20established%20baselines.%20Code%20and%20checkpoint%20will%20be%0Areleased%20at%20https%3A//github.com/VamosC/CapHuman.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2402.00627v3%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DCapHuman%253A%2520Capture%2520Your%2520Moments%2520in%2520Parallel%2520Universes%26entry.906535625%3DChao%2520Liang%2520and%2520Fan%2520Ma%2520and%2520Linchao%2520Zhu%2520and%2520Yingying%2520Deng%2520and%2520Yi%2520Yang%26entry.1292438233%3D%2520%2520We%2520concentrate%2520on%2520a%2520novel%2520human-centric%2520image%2520synthesis%2520task%252C%2520that%2520is%252C%2520given%250Aonly%2520one%2520reference%2520facial%2520photograph%252C%2520it%2520is%2520expected%2520to%2520generate%2520specific%250Aindividual%2520images%2520with%2520diverse%2520head%2520positions%252C%2520poses%252C%2520facial%2520expressions%252C%2520and%250Ailluminations%2520in%2520different%2520contexts.%2520To%2520accomplish%2520this%2520goal%252C%2520we%2520argue%2520that%2520our%250Agenerative%2520model%2520should%2520be%2520capable%2520of%2520the%2520following%2520favorable%2520characteristics%253A%250A%25281%2529%2520a%2520strong%2520visual%2520and%2520semantic%2520understanding%2520of%2520our%2520world%2520and%2520human%2520society%250Afor%2520basic%2520object%2520and%2520human%2520image%2520generation.%2520%25282%2529%2520generalizable%2520identity%250Apreservation%2520ability.%2520%25283%2529%2520flexible%2520and%2520fine-grained%2520head%2520control.%2520Recently%252C%250Alarge%2520pre-trained%2520text-to-image%2520diffusion%2520models%2520have%2520shown%2520remarkable%2520results%252C%250Aserving%2520as%2520a%2520powerful%2520generative%2520foundation.%2520As%2520a%2520basis%252C%2520we%2520aim%2520to%2520unleash%2520the%250Aabove%2520two%2520capabilities%2520of%2520the%2520pre-trained%2520model.%2520In%2520this%2520work%252C%2520we%2520present%2520a%2520new%250Aframework%2520named%2520CapHuman.%2520We%2520embrace%2520the%2520%2522encode%2520then%2520learn%2520to%2520align%2522%2520paradigm%252C%250Awhich%2520enables%2520generalizable%2520identity%2520preservation%2520for%2520new%2520individuals%2520without%250Acumbersome%2520tuning%2520at%2520inference.%2520CapHuman%2520encodes%2520identity%2520features%2520and%2520then%250Alearns%2520to%2520align%2520them%2520into%2520the%2520latent%2520space.%2520Moreover%252C%2520we%2520introduce%2520the%25203D%250Afacial%2520prior%2520to%2520equip%2520our%2520model%2520with%2520control%2520over%2520the%2520human%2520head%2520in%2520a%2520flexible%250Aand%25203D-consistent%2520manner.%2520Extensive%2520qualitative%2520and%2520quantitative%2520analyses%250Ademonstrate%2520our%2520CapHuman%2520can%2520produce%2520well-identity-preserved%252C%2520photo-realistic%252C%250Aand%2520high-fidelity%2520portraits%2520with%2520content-rich%2520representations%2520and%2520various%2520head%250Arenditions%252C%2520superior%2520to%2520established%2520baselines.%2520Code%2520and%2520checkpoint%2520will%2520be%250Areleased%2520at%2520https%253A//github.com/VamosC/CapHuman.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2402.00627v3%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=CapHuman%3A%20Capture%20Your%20Moments%20in%20Parallel%20Universes&entry.906535625=Chao%20Liang%20and%20Fan%20Ma%20and%20Linchao%20Zhu%20and%20Yingying%20Deng%20and%20Yi%20Yang&entry.1292438233=%20%20We%20concentrate%20on%20a%20novel%20human-centric%20image%20synthesis%20task%2C%20that%20is%2C%20given%0Aonly%20one%20reference%20facial%20photograph%2C%20it%20is%20expected%20to%20generate%20specific%0Aindividual%20images%20with%20diverse%20head%20positions%2C%20poses%2C%20facial%20expressions%2C%20and%0Ailluminations%20in%20different%20contexts.%20To%20accomplish%20this%20goal%2C%20we%20argue%20that%20our%0Agenerative%20model%20should%20be%20capable%20of%20the%20following%20favorable%20characteristics%3A%0A%281%29%20a%20strong%20visual%20and%20semantic%20understanding%20of%20our%20world%20and%20human%20society%0Afor%20basic%20object%20and%20human%20image%20generation.%20%282%29%20generalizable%20identity%0Apreservation%20ability.%20%283%29%20flexible%20and%20fine-grained%20head%20control.%20Recently%2C%0Alarge%20pre-trained%20text-to-image%20diffusion%20models%20have%20shown%20remarkable%20results%2C%0Aserving%20as%20a%20powerful%20generative%20foundation.%20As%20a%20basis%2C%20we%20aim%20to%20unleash%20the%0Aabove%20two%20capabilities%20of%20the%20pre-trained%20model.%20In%20this%20work%2C%20we%20present%20a%20new%0Aframework%20named%20CapHuman.%20We%20embrace%20the%20%22encode%20then%20learn%20to%20align%22%20paradigm%2C%0Awhich%20enables%20generalizable%20identity%20preservation%20for%20new%20individuals%20without%0Acumbersome%20tuning%20at%20inference.%20CapHuman%20encodes%20identity%20features%20and%20then%0Alearns%20to%20align%20them%20into%20the%20latent%20space.%20Moreover%2C%20we%20introduce%20the%203D%0Afacial%20prior%20to%20equip%20our%20model%20with%20control%20over%20the%20human%20head%20in%20a%20flexible%0Aand%203D-consistent%20manner.%20Extensive%20qualitative%20and%20quantitative%20analyses%0Ademonstrate%20our%20CapHuman%20can%20produce%20well-identity-preserved%2C%20photo-realistic%2C%0Aand%20high-fidelity%20portraits%20with%20content-rich%20representations%20and%20various%20head%0Arenditions%2C%20superior%20to%20established%20baselines.%20Code%20and%20checkpoint%20will%20be%0Areleased%20at%20https%3A//github.com/VamosC/CapHuman.%0A&entry.1838667208=http%3A//arxiv.org/abs/2402.00627v3&entry.124074799=Read"},
{"title": "Square-Root Inverse Filter-based GNSS-Visual-Inertial Navigation", "author": "Jun Hu and Xiaoming Lang and Feng Zhang and Yinian Mao and Guoquan Huang", "abstract": "  While Global Navigation Satellite System (GNSS) is often used to provide\nglobal positioning if available, its intermittency and/or inaccuracy calls for\nfusion with other sensors. In this paper, we develop a novel\nGNSS-Visual-Inertial Navigation System (GVINS) that fuses visual, inertial, and\nraw GNSS measurements within the square-root inverse sliding window filtering\n(SRI-SWF) framework in a tightly coupled fashion, which thus is termed\nSRI-GVINS. In particular, for the first time, we deeply fuse the GNSS\npseudorange, Doppler shift, single-differenced pseudorange, and\ndouble-differenced carrier phase measurements, along with the visual-inertial\nmeasurements. Inherited from the SRI-SWF, the proposed SRI-GVINS gains\nsignificant numerical stability and computational efficiency over the\nstart-of-the-art methods. Additionally, we propose to use a filter to\nsequentially initialize the reference frame transformation till converges,\nrather than collecting measurements for batch optimization. We also perform\nonline calibration of GNSS-IMU extrinsic parameters to mitigate the possible\nextrinsic parameter degradation. The proposed SRI-GVINS is extensively\nevaluated on our own collected UAV datasets and the results demonstrate that\nthe proposed method is able to suppress VIO drift in real-time and also show\nthe effectiveness of online GNSS-IMU extrinsic calibration. The experimental\nvalidation on the public datasets further reveals that the proposed SRI-GVINS\noutperforms the state-of-the-art methods in terms of both accuracy and\nefficiency.\n", "link": "http://arxiv.org/abs/2405.10874v1", "date": "2024-05-17", "relevancy": 2.546, "topK": [{"title": "RTG-SLAM: Real-time 3D Reconstruction at Scale using Gaussian Splatting", "link": "http://arxiv.org/abs/2404.19706v1", "similarity": 0.5453}, {"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.4983}, {"title": "Egocentric Computer Vision for Hands-Free Robotic Wheelchair Navigation", "link": "https://link.springer.com/article/10.1007/s10846-023-01807-4", "similarity": 0.4841}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Square-Root%20Inverse%20Filter-based%20GNSS-Visual-Inertial%20Navigation&body=Title%3A%20Square-Root%20Inverse%20Filter-based%20GNSS-Visual-Inertial%20Navigation%0AAuthor%3A%20Jun%20Hu%20and%20Xiaoming%20Lang%20and%20Feng%20Zhang%20and%20Yinian%20Mao%20and%20Guoquan%20Huang%0AAbstract%3A%20%20%20While%20Global%20Navigation%20Satellite%20System%20%28GNSS%29%20is%20often%20used%20to%20provide%0Aglobal%20positioning%20if%20available%2C%20its%20intermittency%20and/or%20inaccuracy%20calls%20for%0Afusion%20with%20other%20sensors.%20In%20this%20paper%2C%20we%20develop%20a%20novel%0AGNSS-Visual-Inertial%20Navigation%20System%20%28GVINS%29%20that%20fuses%20visual%2C%20inertial%2C%20and%0Araw%20GNSS%20measurements%20within%20the%20square-root%20inverse%20sliding%20window%20filtering%0A%28SRI-SWF%29%20framework%20in%20a%20tightly%20coupled%20fashion%2C%20which%20thus%20is%20termed%0ASRI-GVINS.%20In%20particular%2C%20for%20the%20first%20time%2C%20we%20deeply%20fuse%20the%20GNSS%0Apseudorange%2C%20Doppler%20shift%2C%20single-differenced%20pseudorange%2C%20and%0Adouble-differenced%20carrier%20phase%20measurements%2C%20along%20with%20the%20visual-inertial%0Ameasurements.%20Inherited%20from%20the%20SRI-SWF%2C%20the%20proposed%20SRI-GVINS%20gains%0Asignificant%20numerical%20stability%20and%20computational%20efficiency%20over%20the%0Astart-of-the-art%20methods.%20Additionally%2C%20we%20propose%20to%20use%20a%20filter%20to%0Asequentially%20initialize%20the%20reference%20frame%20transformation%20till%20converges%2C%0Arather%20than%20collecting%20measurements%20for%20batch%20optimization.%20We%20also%20perform%0Aonline%20calibration%20of%20GNSS-IMU%20extrinsic%20parameters%20to%20mitigate%20the%20possible%0Aextrinsic%20parameter%20degradation.%20The%20proposed%20SRI-GVINS%20is%20extensively%0Aevaluated%20on%20our%20own%20collected%20UAV%20datasets%20and%20the%20results%20demonstrate%20that%0Athe%20proposed%20method%20is%20able%20to%20suppress%20VIO%20drift%20in%20real-time%20and%20also%20show%0Athe%20effectiveness%20of%20online%20GNSS-IMU%20extrinsic%20calibration.%20The%20experimental%0Avalidation%20on%20the%20public%20datasets%20further%20reveals%20that%20the%20proposed%20SRI-GVINS%0Aoutperforms%20the%20state-of-the-art%20methods%20in%20terms%20of%20both%20accuracy%20and%0Aefficiency.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2405.10874v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DSquare-Root%2520Inverse%2520Filter-based%2520GNSS-Visual-Inertial%2520Navigation%26entry.906535625%3DJun%2520Hu%2520and%2520Xiaoming%2520Lang%2520and%2520Feng%2520Zhang%2520and%2520Yinian%2520Mao%2520and%2520Guoquan%2520Huang%26entry.1292438233%3D%2520%2520While%2520Global%2520Navigation%2520Satellite%2520System%2520%2528GNSS%2529%2520is%2520often%2520used%2520to%2520provide%250Aglobal%2520positioning%2520if%2520available%252C%2520its%2520intermittency%2520and/or%2520inaccuracy%2520calls%2520for%250Afusion%2520with%2520other%2520sensors.%2520In%2520this%2520paper%252C%2520we%2520develop%2520a%2520novel%250AGNSS-Visual-Inertial%2520Navigation%2520System%2520%2528GVINS%2529%2520that%2520fuses%2520visual%252C%2520inertial%252C%2520and%250Araw%2520GNSS%2520measurements%2520within%2520the%2520square-root%2520inverse%2520sliding%2520window%2520filtering%250A%2528SRI-SWF%2529%2520framework%2520in%2520a%2520tightly%2520coupled%2520fashion%252C%2520which%2520thus%2520is%2520termed%250ASRI-GVINS.%2520In%2520particular%252C%2520for%2520the%2520first%2520time%252C%2520we%2520deeply%2520fuse%2520the%2520GNSS%250Apseudorange%252C%2520Doppler%2520shift%252C%2520single-differenced%2520pseudorange%252C%2520and%250Adouble-differenced%2520carrier%2520phase%2520measurements%252C%2520along%2520with%2520the%2520visual-inertial%250Ameasurements.%2520Inherited%2520from%2520the%2520SRI-SWF%252C%2520the%2520proposed%2520SRI-GVINS%2520gains%250Asignificant%2520numerical%2520stability%2520and%2520computational%2520efficiency%2520over%2520the%250Astart-of-the-art%2520methods.%2520Additionally%252C%2520we%2520propose%2520to%2520use%2520a%2520filter%2520to%250Asequentially%2520initialize%2520the%2520reference%2520frame%2520transformation%2520till%2520converges%252C%250Arather%2520than%2520collecting%2520measurements%2520for%2520batch%2520optimization.%2520We%2520also%2520perform%250Aonline%2520calibration%2520of%2520GNSS-IMU%2520extrinsic%2520parameters%2520to%2520mitigate%2520the%2520possible%250Aextrinsic%2520parameter%2520degradation.%2520The%2520proposed%2520SRI-GVINS%2520is%2520extensively%250Aevaluated%2520on%2520our%2520own%2520collected%2520UAV%2520datasets%2520and%2520the%2520results%2520demonstrate%2520that%250Athe%2520proposed%2520method%2520is%2520able%2520to%2520suppress%2520VIO%2520drift%2520in%2520real-time%2520and%2520also%2520show%250Athe%2520effectiveness%2520of%2520online%2520GNSS-IMU%2520extrinsic%2520calibration.%2520The%2520experimental%250Avalidation%2520on%2520the%2520public%2520datasets%2520further%2520reveals%2520that%2520the%2520proposed%2520SRI-GVINS%250Aoutperforms%2520the%2520state-of-the-art%2520methods%2520in%2520terms%2520of%2520both%2520accuracy%2520and%250Aefficiency.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2405.10874v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Square-Root%20Inverse%20Filter-based%20GNSS-Visual-Inertial%20Navigation&entry.906535625=Jun%20Hu%20and%20Xiaoming%20Lang%20and%20Feng%20Zhang%20and%20Yinian%20Mao%20and%20Guoquan%20Huang&entry.1292438233=%20%20While%20Global%20Navigation%20Satellite%20System%20%28GNSS%29%20is%20often%20used%20to%20provide%0Aglobal%20positioning%20if%20available%2C%20its%20intermittency%20and/or%20inaccuracy%20calls%20for%0Afusion%20with%20other%20sensors.%20In%20this%20paper%2C%20we%20develop%20a%20novel%0AGNSS-Visual-Inertial%20Navigation%20System%20%28GVINS%29%20that%20fuses%20visual%2C%20inertial%2C%20and%0Araw%20GNSS%20measurements%20within%20the%20square-root%20inverse%20sliding%20window%20filtering%0A%28SRI-SWF%29%20framework%20in%20a%20tightly%20coupled%20fashion%2C%20which%20thus%20is%20termed%0ASRI-GVINS.%20In%20particular%2C%20for%20the%20first%20time%2C%20we%20deeply%20fuse%20the%20GNSS%0Apseudorange%2C%20Doppler%20shift%2C%20single-differenced%20pseudorange%2C%20and%0Adouble-differenced%20carrier%20phase%20measurements%2C%20along%20with%20the%20visual-inertial%0Ameasurements.%20Inherited%20from%20the%20SRI-SWF%2C%20the%20proposed%20SRI-GVINS%20gains%0Asignificant%20numerical%20stability%20and%20computational%20efficiency%20over%20the%0Astart-of-the-art%20methods.%20Additionally%2C%20we%20propose%20to%20use%20a%20filter%20to%0Asequentially%20initialize%20the%20reference%20frame%20transformation%20till%20converges%2C%0Arather%20than%20collecting%20measurements%20for%20batch%20optimization.%20We%20also%20perform%0Aonline%20calibration%20of%20GNSS-IMU%20extrinsic%20parameters%20to%20mitigate%20the%20possible%0Aextrinsic%20parameter%20degradation.%20The%20proposed%20SRI-GVINS%20is%20extensively%0Aevaluated%20on%20our%20own%20collected%20UAV%20datasets%20and%20the%20results%20demonstrate%20that%0Athe%20proposed%20method%20is%20able%20to%20suppress%20VIO%20drift%20in%20real-time%20and%20also%20show%0Athe%20effectiveness%20of%20online%20GNSS-IMU%20extrinsic%20calibration.%20The%20experimental%0Avalidation%20on%20the%20public%20datasets%20further%20reveals%20that%20the%20proposed%20SRI-GVINS%0Aoutperforms%20the%20state-of-the-art%20methods%20in%20terms%20of%20both%20accuracy%20and%0Aefficiency.%0A&entry.1838667208=http%3A//arxiv.org/abs/2405.10874v1&entry.124074799=Read"},
{"title": "Occupancy-SLAM: Simultaneously Optimizing Robot Poses and Continuous\n  Occupancy Map", "author": "Liang Zhao and Yingyu Wang and Shoudong Huang", "abstract": "  In this paper, we propose an optimization based SLAM approach to\nsimultaneously optimize the robot trajectory and the occupancy map using 2D\nlaser scans (and odometry) information. The key novelty is that the robot poses\nand the occupancy map are optimized together, which is significantly different\nfrom existing occupancy mapping strategies where the robot poses need to be\nobtained first before the map can be estimated. In our formulation, the map is\nrepresented as a continuous occupancy map where each 2D point in the\nenvironment has a corresponding evidence value. The Occupancy-SLAM problem is\nformulated as an optimization problem where the variables include all the robot\nposes and the occupancy values at the selected discrete grid cell nodes. We\npropose a variation of Gauss-Newton method to solve this new formulated\nproblem, obtaining the optimized occupancy map and robot trajectory together\nwith their uncertainties. Our algorithm is an offline approach since it is\nbased on batch optimization and the number of variables involved is large.\nEvaluations using simulations and publicly available practical 2D laser\ndatasets demonstrate that the proposed approach can estimate the maps and robot\ntrajectories more accurately than the state-of-the-art techniques, when a\nrelatively accurate initial guess is provided to our algorithm. The video shows\nthe convergence process of the proposed Occupancy-SLAM and comparison of\nresults to Cartographer can be found at \\url{https://youtu.be/4oLyVEUC4iY}.\n", "link": "http://arxiv.org/abs/2405.10743v1", "date": "2024-05-17", "relevancy": 2.46, "topK": [{"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.6214}, {"title": "RTG-SLAM: Real-time 3D Reconstruction at Scale using Gaussian Splatting", "link": "http://arxiv.org/abs/2404.19706v1", "similarity": 0.6161}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.5963}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Occupancy-SLAM%3A%20Simultaneously%20Optimizing%20Robot%20Poses%20and%20Continuous%0A%20%20Occupancy%20Map&body=Title%3A%20Occupancy-SLAM%3A%20Simultaneously%20Optimizing%20Robot%20Poses%20and%20Continuous%0A%20%20Occupancy%20Map%0AAuthor%3A%20Liang%20Zhao%20and%20Yingyu%20Wang%20and%20Shoudong%20Huang%0AAbstract%3A%20%20%20In%20this%20paper%2C%20we%20propose%20an%20optimization%20based%20SLAM%20approach%20to%0Asimultaneously%20optimize%20the%20robot%20trajectory%20and%20the%20occupancy%20map%20using%202D%0Alaser%20scans%20%28and%20odometry%29%20information.%20The%20key%20novelty%20is%20that%20the%20robot%20poses%0Aand%20the%20occupancy%20map%20are%20optimized%20together%2C%20which%20is%20significantly%20different%0Afrom%20existing%20occupancy%20mapping%20strategies%20where%20the%20robot%20poses%20need%20to%20be%0Aobtained%20first%20before%20the%20map%20can%20be%20estimated.%20In%20our%20formulation%2C%20the%20map%20is%0Arepresented%20as%20a%20continuous%20occupancy%20map%20where%20each%202D%20point%20in%20the%0Aenvironment%20has%20a%20corresponding%20evidence%20value.%20The%20Occupancy-SLAM%20problem%20is%0Aformulated%20as%20an%20optimization%20problem%20where%20the%20variables%20include%20all%20the%20robot%0Aposes%20and%20the%20occupancy%20values%20at%20the%20selected%20discrete%20grid%20cell%20nodes.%20We%0Apropose%20a%20variation%20of%20Gauss-Newton%20method%20to%20solve%20this%20new%20formulated%0Aproblem%2C%20obtaining%20the%20optimized%20occupancy%20map%20and%20robot%20trajectory%20together%0Awith%20their%20uncertainties.%20Our%20algorithm%20is%20an%20offline%20approach%20since%20it%20is%0Abased%20on%20batch%20optimization%20and%20the%20number%20of%20variables%20involved%20is%20large.%0AEvaluations%20using%20simulations%20and%20publicly%20available%20practical%202D%20laser%0Adatasets%20demonstrate%20that%20the%20proposed%20approach%20can%20estimate%20the%20maps%20and%20robot%0Atrajectories%20more%20accurately%20than%20the%20state-of-the-art%20techniques%2C%20when%20a%0Arelatively%20accurate%20initial%20guess%20is%20provided%20to%20our%20algorithm.%20The%20video%20shows%0Athe%20convergence%20process%20of%20the%20proposed%20Occupancy-SLAM%20and%20comparison%20of%0Aresults%20to%20Cartographer%20can%20be%20found%20at%20%5Curl%7Bhttps%3A//youtu.be/4oLyVEUC4iY%7D.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2405.10743v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DOccupancy-SLAM%253A%2520Simultaneously%2520Optimizing%2520Robot%2520Poses%2520and%2520Continuous%250A%2520%2520Occupancy%2520Map%26entry.906535625%3DLiang%2520Zhao%2520and%2520Yingyu%2520Wang%2520and%2520Shoudong%2520Huang%26entry.1292438233%3D%2520%2520In%2520this%2520paper%252C%2520we%2520propose%2520an%2520optimization%2520based%2520SLAM%2520approach%2520to%250Asimultaneously%2520optimize%2520the%2520robot%2520trajectory%2520and%2520the%2520occupancy%2520map%2520using%25202D%250Alaser%2520scans%2520%2528and%2520odometry%2529%2520information.%2520The%2520key%2520novelty%2520is%2520that%2520the%2520robot%2520poses%250Aand%2520the%2520occupancy%2520map%2520are%2520optimized%2520together%252C%2520which%2520is%2520significantly%2520different%250Afrom%2520existing%2520occupancy%2520mapping%2520strategies%2520where%2520the%2520robot%2520poses%2520need%2520to%2520be%250Aobtained%2520first%2520before%2520the%2520map%2520can%2520be%2520estimated.%2520In%2520our%2520formulation%252C%2520the%2520map%2520is%250Arepresented%2520as%2520a%2520continuous%2520occupancy%2520map%2520where%2520each%25202D%2520point%2520in%2520the%250Aenvironment%2520has%2520a%2520corresponding%2520evidence%2520value.%2520The%2520Occupancy-SLAM%2520problem%2520is%250Aformulated%2520as%2520an%2520optimization%2520problem%2520where%2520the%2520variables%2520include%2520all%2520the%2520robot%250Aposes%2520and%2520the%2520occupancy%2520values%2520at%2520the%2520selected%2520discrete%2520grid%2520cell%2520nodes.%2520We%250Apropose%2520a%2520variation%2520of%2520Gauss-Newton%2520method%2520to%2520solve%2520this%2520new%2520formulated%250Aproblem%252C%2520obtaining%2520the%2520optimized%2520occupancy%2520map%2520and%2520robot%2520trajectory%2520together%250Awith%2520their%2520uncertainties.%2520Our%2520algorithm%2520is%2520an%2520offline%2520approach%2520since%2520it%2520is%250Abased%2520on%2520batch%2520optimization%2520and%2520the%2520number%2520of%2520variables%2520involved%2520is%2520large.%250AEvaluations%2520using%2520simulations%2520and%2520publicly%2520available%2520practical%25202D%2520laser%250Adatasets%2520demonstrate%2520that%2520the%2520proposed%2520approach%2520can%2520estimate%2520the%2520maps%2520and%2520robot%250Atrajectories%2520more%2520accurately%2520than%2520the%2520state-of-the-art%2520techniques%252C%2520when%2520a%250Arelatively%2520accurate%2520initial%2520guess%2520is%2520provided%2520to%2520our%2520algorithm.%2520The%2520video%2520shows%250Athe%2520convergence%2520process%2520of%2520the%2520proposed%2520Occupancy-SLAM%2520and%2520comparison%2520of%250Aresults%2520to%2520Cartographer%2520can%2520be%2520found%2520at%2520%255Curl%257Bhttps%253A//youtu.be/4oLyVEUC4iY%257D.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2405.10743v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Occupancy-SLAM%3A%20Simultaneously%20Optimizing%20Robot%20Poses%20and%20Continuous%0A%20%20Occupancy%20Map&entry.906535625=Liang%20Zhao%20and%20Yingyu%20Wang%20and%20Shoudong%20Huang&entry.1292438233=%20%20In%20this%20paper%2C%20we%20propose%20an%20optimization%20based%20SLAM%20approach%20to%0Asimultaneously%20optimize%20the%20robot%20trajectory%20and%20the%20occupancy%20map%20using%202D%0Alaser%20scans%20%28and%20odometry%29%20information.%20The%20key%20novelty%20is%20that%20the%20robot%20poses%0Aand%20the%20occupancy%20map%20are%20optimized%20together%2C%20which%20is%20significantly%20different%0Afrom%20existing%20occupancy%20mapping%20strategies%20where%20the%20robot%20poses%20need%20to%20be%0Aobtained%20first%20before%20the%20map%20can%20be%20estimated.%20In%20our%20formulation%2C%20the%20map%20is%0Arepresented%20as%20a%20continuous%20occupancy%20map%20where%20each%202D%20point%20in%20the%0Aenvironment%20has%20a%20corresponding%20evidence%20value.%20The%20Occupancy-SLAM%20problem%20is%0Aformulated%20as%20an%20optimization%20problem%20where%20the%20variables%20include%20all%20the%20robot%0Aposes%20and%20the%20occupancy%20values%20at%20the%20selected%20discrete%20grid%20cell%20nodes.%20We%0Apropose%20a%20variation%20of%20Gauss-Newton%20method%20to%20solve%20this%20new%20formulated%0Aproblem%2C%20obtaining%20the%20optimized%20occupancy%20map%20and%20robot%20trajectory%20together%0Awith%20their%20uncertainties.%20Our%20algorithm%20is%20an%20offline%20approach%20since%20it%20is%0Abased%20on%20batch%20optimization%20and%20the%20number%20of%20variables%20involved%20is%20large.%0AEvaluations%20using%20simulations%20and%20publicly%20available%20practical%202D%20laser%0Adatasets%20demonstrate%20that%20the%20proposed%20approach%20can%20estimate%20the%20maps%20and%20robot%0Atrajectories%20more%20accurately%20than%20the%20state-of-the-art%20techniques%2C%20when%20a%0Arelatively%20accurate%20initial%20guess%20is%20provided%20to%20our%20algorithm.%20The%20video%20shows%0Athe%20convergence%20process%20of%20the%20proposed%20Occupancy-SLAM%20and%20comparison%20of%0Aresults%20to%20Cartographer%20can%20be%20found%20at%20%5Curl%7Bhttps%3A//youtu.be/4oLyVEUC4iY%7D.%0A&entry.1838667208=http%3A//arxiv.org/abs/2405.10743v1&entry.124074799=Read"},
{"title": "GenToC: Leveraging Partially-Labeled Data for Product Attribute-Value\n  Identification", "author": "D. Subhalingam and Keshav Kolluru and  Mausam and Saurabh Singal", "abstract": "  In the e-commerce domain, the accurate extraction of attribute-value pairs\nfrom product listings (e.g., Brand: Apple) is crucial for enhancing search and\nrecommendation systems. The automation of this extraction process is\nchallenging due to the vast diversity of product categories and their\nrespective attributes, compounded by the lack of extensive, accurately\nannotated training datasets and the demand for low latency to meet the\nreal-time needs of e-commerce platforms. To address these challenges, we\nintroduce GenToC, a novel two-stage model for extracting attribute-value pairs\nfrom product titles. GenToC is designed to train with partially-labeled data,\nleveraging incomplete attribute-value pairs and obviating the need for a fully\nannotated dataset. Moreover, we introduce a bootstrapping method that enables\nGenToC to progressively refine and expand its training dataset. This\nenhancement substantially improves the quality of data available for training\nother neural network models that are typically faster but are inherently less\ncapable than GenToC in terms of their capacity to handle partially-labeled\ndata. By supplying an enriched dataset for training, GenToC significantly\nadvances the performance of these alternative models, making them more suitable\nfor real-time deployment. Our results highlight the unique capability of GenToC\nto learn from a limited set of labeled data and to contribute to the training\nof more efficient models, marking a significant leap forward in the automated\nextraction of attribute-value pairs from product titles. GenToC has been\nsuccessfully integrated into India's largest B2B e-commerce platform,\nIndiaMART.com, achieving a significant increase of 21.1% in recall over the\nexisting deployed system while maintaining a high precision of 89.5% in this\nchallenging task.\n", "link": "http://arxiv.org/abs/2405.10918v1", "date": "2024-05-17", "relevancy": 2.4181, "topK": [{"title": "VirtualModel: Generating Object-ID-retentive Human-object Interaction\n  Image by Diffusion Model for E-commerce Marketing", "link": "http://arxiv.org/abs/2405.09985v1", "similarity": 0.5166}, {"title": "DressCode: Autoregressively Sewing and Generating Garments from Text\n  Guidance", "link": "http://arxiv.org/abs/2401.16465v3", "similarity": 0.4685}, {"title": "Genie: Generative Interactive Environments", "link": "http://arxiv.org/abs/2402.15391v1", "similarity": 0.4657}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20GenToC%3A%20Leveraging%20Partially-Labeled%20Data%20for%20Product%20Attribute-Value%0A%20%20Identification&body=Title%3A%20GenToC%3A%20Leveraging%20Partially-Labeled%20Data%20for%20Product%20Attribute-Value%0A%20%20Identification%0AAuthor%3A%20D.%20Subhalingam%20and%20Keshav%20Kolluru%20and%20%20Mausam%20and%20Saurabh%20Singal%0AAbstract%3A%20%20%20In%20the%20e-commerce%20domain%2C%20the%20accurate%20extraction%20of%20attribute-value%20pairs%0Afrom%20product%20listings%20%28e.g.%2C%20Brand%3A%20Apple%29%20is%20crucial%20for%20enhancing%20search%20and%0Arecommendation%20systems.%20The%20automation%20of%20this%20extraction%20process%20is%0Achallenging%20due%20to%20the%20vast%20diversity%20of%20product%20categories%20and%20their%0Arespective%20attributes%2C%20compounded%20by%20the%20lack%20of%20extensive%2C%20accurately%0Aannotated%20training%20datasets%20and%20the%20demand%20for%20low%20latency%20to%20meet%20the%0Areal-time%20needs%20of%20e-commerce%20platforms.%20To%20address%20these%20challenges%2C%20we%0Aintroduce%20GenToC%2C%20a%20novel%20two-stage%20model%20for%20extracting%20attribute-value%20pairs%0Afrom%20product%20titles.%20GenToC%20is%20designed%20to%20train%20with%20partially-labeled%20data%2C%0Aleveraging%20incomplete%20attribute-value%20pairs%20and%20obviating%20the%20need%20for%20a%20fully%0Aannotated%20dataset.%20Moreover%2C%20we%20introduce%20a%20bootstrapping%20method%20that%20enables%0AGenToC%20to%20progressively%20refine%20and%20expand%20its%20training%20dataset.%20This%0Aenhancement%20substantially%20improves%20the%20quality%20of%20data%20available%20for%20training%0Aother%20neural%20network%20models%20that%20are%20typically%20faster%20but%20are%20inherently%20less%0Acapable%20than%20GenToC%20in%20terms%20of%20their%20capacity%20to%20handle%20partially-labeled%0Adata.%20By%20supplying%20an%20enriched%20dataset%20for%20training%2C%20GenToC%20significantly%0Aadvances%20the%20performance%20of%20these%20alternative%20models%2C%20making%20them%20more%20suitable%0Afor%20real-time%20deployment.%20Our%20results%20highlight%20the%20unique%20capability%20of%20GenToC%0Ato%20learn%20from%20a%20limited%20set%20of%20labeled%20data%20and%20to%20contribute%20to%20the%20training%0Aof%20more%20efficient%20models%2C%20marking%20a%20significant%20leap%20forward%20in%20the%20automated%0Aextraction%20of%20attribute-value%20pairs%20from%20product%20titles.%20GenToC%20has%20been%0Asuccessfully%20integrated%20into%20India%27s%20largest%20B2B%20e-commerce%20platform%2C%0AIndiaMART.com%2C%20achieving%20a%20significant%20increase%20of%2021.1%25%20in%20recall%20over%20the%0Aexisting%20deployed%20system%20while%20maintaining%20a%20high%20precision%20of%2089.5%25%20in%20this%0Achallenging%20task.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2405.10918v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DGenToC%253A%2520Leveraging%2520Partially-Labeled%2520Data%2520for%2520Product%2520Attribute-Value%250A%2520%2520Identification%26entry.906535625%3DD.%2520Subhalingam%2520and%2520Keshav%2520Kolluru%2520and%2520%2520Mausam%2520and%2520Saurabh%2520Singal%26entry.1292438233%3D%2520%2520In%2520the%2520e-commerce%2520domain%252C%2520the%2520accurate%2520extraction%2520of%2520attribute-value%2520pairs%250Afrom%2520product%2520listings%2520%2528e.g.%252C%2520Brand%253A%2520Apple%2529%2520is%2520crucial%2520for%2520enhancing%2520search%2520and%250Arecommendation%2520systems.%2520The%2520automation%2520of%2520this%2520extraction%2520process%2520is%250Achallenging%2520due%2520to%2520the%2520vast%2520diversity%2520of%2520product%2520categories%2520and%2520their%250Arespective%2520attributes%252C%2520compounded%2520by%2520the%2520lack%2520of%2520extensive%252C%2520accurately%250Aannotated%2520training%2520datasets%2520and%2520the%2520demand%2520for%2520low%2520latency%2520to%2520meet%2520the%250Areal-time%2520needs%2520of%2520e-commerce%2520platforms.%2520To%2520address%2520these%2520challenges%252C%2520we%250Aintroduce%2520GenToC%252C%2520a%2520novel%2520two-stage%2520model%2520for%2520extracting%2520attribute-value%2520pairs%250Afrom%2520product%2520titles.%2520GenToC%2520is%2520designed%2520to%2520train%2520with%2520partially-labeled%2520data%252C%250Aleveraging%2520incomplete%2520attribute-value%2520pairs%2520and%2520obviating%2520the%2520need%2520for%2520a%2520fully%250Aannotated%2520dataset.%2520Moreover%252C%2520we%2520introduce%2520a%2520bootstrapping%2520method%2520that%2520enables%250AGenToC%2520to%2520progressively%2520refine%2520and%2520expand%2520its%2520training%2520dataset.%2520This%250Aenhancement%2520substantially%2520improves%2520the%2520quality%2520of%2520data%2520available%2520for%2520training%250Aother%2520neural%2520network%2520models%2520that%2520are%2520typically%2520faster%2520but%2520are%2520inherently%2520less%250Acapable%2520than%2520GenToC%2520in%2520terms%2520of%2520their%2520capacity%2520to%2520handle%2520partially-labeled%250Adata.%2520By%2520supplying%2520an%2520enriched%2520dataset%2520for%2520training%252C%2520GenToC%2520significantly%250Aadvances%2520the%2520performance%2520of%2520these%2520alternative%2520models%252C%2520making%2520them%2520more%2520suitable%250Afor%2520real-time%2520deployment.%2520Our%2520results%2520highlight%2520the%2520unique%2520capability%2520of%2520GenToC%250Ato%2520learn%2520from%2520a%2520limited%2520set%2520of%2520labeled%2520data%2520and%2520to%2520contribute%2520to%2520the%2520training%250Aof%2520more%2520efficient%2520models%252C%2520marking%2520a%2520significant%2520leap%2520forward%2520in%2520the%2520automated%250Aextraction%2520of%2520attribute-value%2520pairs%2520from%2520product%2520titles.%2520GenToC%2520has%2520been%250Asuccessfully%2520integrated%2520into%2520India%2527s%2520largest%2520B2B%2520e-commerce%2520platform%252C%250AIndiaMART.com%252C%2520achieving%2520a%2520significant%2520increase%2520of%252021.1%2525%2520in%2520recall%2520over%2520the%250Aexisting%2520deployed%2520system%2520while%2520maintaining%2520a%2520high%2520precision%2520of%252089.5%2525%2520in%2520this%250Achallenging%2520task.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2405.10918v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=GenToC%3A%20Leveraging%20Partially-Labeled%20Data%20for%20Product%20Attribute-Value%0A%20%20Identification&entry.906535625=D.%20Subhalingam%20and%20Keshav%20Kolluru%20and%20%20Mausam%20and%20Saurabh%20Singal&entry.1292438233=%20%20In%20the%20e-commerce%20domain%2C%20the%20accurate%20extraction%20of%20attribute-value%20pairs%0Afrom%20product%20listings%20%28e.g.%2C%20Brand%3A%20Apple%29%20is%20crucial%20for%20enhancing%20search%20and%0Arecommendation%20systems.%20The%20automation%20of%20this%20extraction%20process%20is%0Achallenging%20due%20to%20the%20vast%20diversity%20of%20product%20categories%20and%20their%0Arespective%20attributes%2C%20compounded%20by%20the%20lack%20of%20extensive%2C%20accurately%0Aannotated%20training%20datasets%20and%20the%20demand%20for%20low%20latency%20to%20meet%20the%0Areal-time%20needs%20of%20e-commerce%20platforms.%20To%20address%20these%20challenges%2C%20we%0Aintroduce%20GenToC%2C%20a%20novel%20two-stage%20model%20for%20extracting%20attribute-value%20pairs%0Afrom%20product%20titles.%20GenToC%20is%20designed%20to%20train%20with%20partially-labeled%20data%2C%0Aleveraging%20incomplete%20attribute-value%20pairs%20and%20obviating%20the%20need%20for%20a%20fully%0Aannotated%20dataset.%20Moreover%2C%20we%20introduce%20a%20bootstrapping%20method%20that%20enables%0AGenToC%20to%20progressively%20refine%20and%20expand%20its%20training%20dataset.%20This%0Aenhancement%20substantially%20improves%20the%20quality%20of%20data%20available%20for%20training%0Aother%20neural%20network%20models%20that%20are%20typically%20faster%20but%20are%20inherently%20less%0Acapable%20than%20GenToC%20in%20terms%20of%20their%20capacity%20to%20handle%20partially-labeled%0Adata.%20By%20supplying%20an%20enriched%20dataset%20for%20training%2C%20GenToC%20significantly%0Aadvances%20the%20performance%20of%20these%20alternative%20models%2C%20making%20them%20more%20suitable%0Afor%20real-time%20deployment.%20Our%20results%20highlight%20the%20unique%20capability%20of%20GenToC%0Ato%20learn%20from%20a%20limited%20set%20of%20labeled%20data%20and%20to%20contribute%20to%20the%20training%0Aof%20more%20efficient%20models%2C%20marking%20a%20significant%20leap%20forward%20in%20the%20automated%0Aextraction%20of%20attribute-value%20pairs%20from%20product%20titles.%20GenToC%20has%20been%0Asuccessfully%20integrated%20into%20India%27s%20largest%20B2B%20e-commerce%20platform%2C%0AIndiaMART.com%2C%20achieving%20a%20significant%20increase%20of%2021.1%25%20in%20recall%20over%20the%0Aexisting%20deployed%20system%20while%20maintaining%20a%20high%20precision%20of%2089.5%25%20in%20this%0Achallenging%20task.%0A&entry.1838667208=http%3A//arxiv.org/abs/2405.10918v1&entry.124074799=Read"},
{"title": "How Spurious Features Are Memorized: Precise Analysis for Random and NTK\n  Features", "author": "Simone Bombari and Marco Mondelli", "abstract": "  Deep learning models are known to overfit and memorize spurious features in\nthe training dataset. While numerous empirical studies have aimed at\nunderstanding this phenomenon, a rigorous theoretical framework to quantify it\nis still missing. In this paper, we consider spurious features that are\nuncorrelated with the learning task, and we provide a precise characterization\nof how they are memorized via two separate terms: (i) the stability of the\nmodel with respect to individual training samples, and (ii) the feature\nalignment between the spurious feature and the full sample. While the first\nterm is well established in learning theory and it is connected to the\ngeneralization error in classical work, the second one is, to the best of our\nknowledge, novel. Our key technical result gives a precise characterization of\nthe feature alignment for the two prototypical settings of random features (RF)\nand neural tangent kernel (NTK) regression. We prove that the memorization of\nspurious features weakens as the generalization capability increases and,\nthrough the analysis of the feature alignment, we unveil the role of the model\nand of its activation function. Numerical experiments show the predictive power\nof our theory on standard datasets (MNIST, CIFAR-10).\n", "link": "http://arxiv.org/abs/2305.12100v3", "date": "2024-05-17", "relevancy": 2.3407, "topK": [{"title": "Breadcrumbs: Adversarial class-balanced sampling for long-tailed recognition", "link": "https://link.springer.com/chapter/10.1007/978-3-031-20053-3_37", "similarity": 0.4845}, {"title": "ToDo: Token Downsampling for Efficient Generation of High-Resolution\n  Images", "link": "http://arxiv.org/abs/2402.13573v2", "similarity": 0.4726}, {"title": "Flexible Visual Recognition by Evidential Modeling of Confusion and Ignorance", "link": "http://openaccess.thecvf.com/content/ICCV2023/html/Fan_Flexible_Visual_Recognition_by_Evidential_Modeling_of_Confusion_and_Ignorance_ICCV_2023_paper.html", "similarity": 0.4473}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20How%20Spurious%20Features%20Are%20Memorized%3A%20Precise%20Analysis%20for%20Random%20and%20NTK%0A%20%20Features&body=Title%3A%20How%20Spurious%20Features%20Are%20Memorized%3A%20Precise%20Analysis%20for%20Random%20and%20NTK%0A%20%20Features%0AAuthor%3A%20Simone%20Bombari%20and%20Marco%20Mondelli%0AAbstract%3A%20%20%20Deep%20learning%20models%20are%20known%20to%20overfit%20and%20memorize%20spurious%20features%20in%0Athe%20training%20dataset.%20While%20numerous%20empirical%20studies%20have%20aimed%20at%0Aunderstanding%20this%20phenomenon%2C%20a%20rigorous%20theoretical%20framework%20to%20quantify%20it%0Ais%20still%20missing.%20In%20this%20paper%2C%20we%20consider%20spurious%20features%20that%20are%0Auncorrelated%20with%20the%20learning%20task%2C%20and%20we%20provide%20a%20precise%20characterization%0Aof%20how%20they%20are%20memorized%20via%20two%20separate%20terms%3A%20%28i%29%20the%20stability%20of%20the%0Amodel%20with%20respect%20to%20individual%20training%20samples%2C%20and%20%28ii%29%20the%20feature%0Aalignment%20between%20the%20spurious%20feature%20and%20the%20full%20sample.%20While%20the%20first%0Aterm%20is%20well%20established%20in%20learning%20theory%20and%20it%20is%20connected%20to%20the%0Ageneralization%20error%20in%20classical%20work%2C%20the%20second%20one%20is%2C%20to%20the%20best%20of%20our%0Aknowledge%2C%20novel.%20Our%20key%20technical%20result%20gives%20a%20precise%20characterization%20of%0Athe%20feature%20alignment%20for%20the%20two%20prototypical%20settings%20of%20random%20features%20%28RF%29%0Aand%20neural%20tangent%20kernel%20%28NTK%29%20regression.%20We%20prove%20that%20the%20memorization%20of%0Aspurious%20features%20weakens%20as%20the%20generalization%20capability%20increases%20and%2C%0Athrough%20the%20analysis%20of%20the%20feature%20alignment%2C%20we%20unveil%20the%20role%20of%20the%20model%0Aand%20of%20its%20activation%20function.%20Numerical%20experiments%20show%20the%20predictive%20power%0Aof%20our%20theory%20on%20standard%20datasets%20%28MNIST%2C%20CIFAR-10%29.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2305.12100v3%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DHow%2520Spurious%2520Features%2520Are%2520Memorized%253A%2520Precise%2520Analysis%2520for%2520Random%2520and%2520NTK%250A%2520%2520Features%26entry.906535625%3DSimone%2520Bombari%2520and%2520Marco%2520Mondelli%26entry.1292438233%3D%2520%2520Deep%2520learning%2520models%2520are%2520known%2520to%2520overfit%2520and%2520memorize%2520spurious%2520features%2520in%250Athe%2520training%2520dataset.%2520While%2520numerous%2520empirical%2520studies%2520have%2520aimed%2520at%250Aunderstanding%2520this%2520phenomenon%252C%2520a%2520rigorous%2520theoretical%2520framework%2520to%2520quantify%2520it%250Ais%2520still%2520missing.%2520In%2520this%2520paper%252C%2520we%2520consider%2520spurious%2520features%2520that%2520are%250Auncorrelated%2520with%2520the%2520learning%2520task%252C%2520and%2520we%2520provide%2520a%2520precise%2520characterization%250Aof%2520how%2520they%2520are%2520memorized%2520via%2520two%2520separate%2520terms%253A%2520%2528i%2529%2520the%2520stability%2520of%2520the%250Amodel%2520with%2520respect%2520to%2520individual%2520training%2520samples%252C%2520and%2520%2528ii%2529%2520the%2520feature%250Aalignment%2520between%2520the%2520spurious%2520feature%2520and%2520the%2520full%2520sample.%2520While%2520the%2520first%250Aterm%2520is%2520well%2520established%2520in%2520learning%2520theory%2520and%2520it%2520is%2520connected%2520to%2520the%250Ageneralization%2520error%2520in%2520classical%2520work%252C%2520the%2520second%2520one%2520is%252C%2520to%2520the%2520best%2520of%2520our%250Aknowledge%252C%2520novel.%2520Our%2520key%2520technical%2520result%2520gives%2520a%2520precise%2520characterization%2520of%250Athe%2520feature%2520alignment%2520for%2520the%2520two%2520prototypical%2520settings%2520of%2520random%2520features%2520%2528RF%2529%250Aand%2520neural%2520tangent%2520kernel%2520%2528NTK%2529%2520regression.%2520We%2520prove%2520that%2520the%2520memorization%2520of%250Aspurious%2520features%2520weakens%2520as%2520the%2520generalization%2520capability%2520increases%2520and%252C%250Athrough%2520the%2520analysis%2520of%2520the%2520feature%2520alignment%252C%2520we%2520unveil%2520the%2520role%2520of%2520the%2520model%250Aand%2520of%2520its%2520activation%2520function.%2520Numerical%2520experiments%2520show%2520the%2520predictive%2520power%250Aof%2520our%2520theory%2520on%2520standard%2520datasets%2520%2528MNIST%252C%2520CIFAR-10%2529.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2305.12100v3%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=How%20Spurious%20Features%20Are%20Memorized%3A%20Precise%20Analysis%20for%20Random%20and%20NTK%0A%20%20Features&entry.906535625=Simone%20Bombari%20and%20Marco%20Mondelli&entry.1292438233=%20%20Deep%20learning%20models%20are%20known%20to%20overfit%20and%20memorize%20spurious%20features%20in%0Athe%20training%20dataset.%20While%20numerous%20empirical%20studies%20have%20aimed%20at%0Aunderstanding%20this%20phenomenon%2C%20a%20rigorous%20theoretical%20framework%20to%20quantify%20it%0Ais%20still%20missing.%20In%20this%20paper%2C%20we%20consider%20spurious%20features%20that%20are%0Auncorrelated%20with%20the%20learning%20task%2C%20and%20we%20provide%20a%20precise%20characterization%0Aof%20how%20they%20are%20memorized%20via%20two%20separate%20terms%3A%20%28i%29%20the%20stability%20of%20the%0Amodel%20with%20respect%20to%20individual%20training%20samples%2C%20and%20%28ii%29%20the%20feature%0Aalignment%20between%20the%20spurious%20feature%20and%20the%20full%20sample.%20While%20the%20first%0Aterm%20is%20well%20established%20in%20learning%20theory%20and%20it%20is%20connected%20to%20the%0Ageneralization%20error%20in%20classical%20work%2C%20the%20second%20one%20is%2C%20to%20the%20best%20of%20our%0Aknowledge%2C%20novel.%20Our%20key%20technical%20result%20gives%20a%20precise%20characterization%20of%0Athe%20feature%20alignment%20for%20the%20two%20prototypical%20settings%20of%20random%20features%20%28RF%29%0Aand%20neural%20tangent%20kernel%20%28NTK%29%20regression.%20We%20prove%20that%20the%20memorization%20of%0Aspurious%20features%20weakens%20as%20the%20generalization%20capability%20increases%20and%2C%0Athrough%20the%20analysis%20of%20the%20feature%20alignment%2C%20we%20unveil%20the%20role%20of%20the%20model%0Aand%20of%20its%20activation%20function.%20Numerical%20experiments%20show%20the%20predictive%20power%0Aof%20our%20theory%20on%20standard%20datasets%20%28MNIST%2C%20CIFAR-10%29.%0A&entry.1838667208=http%3A//arxiv.org/abs/2305.12100v3&entry.124074799=Read"},
{"title": "LoCI-DiffCom: Longitudinal Consistency-Informed Diffusion Model for 3D\n  Infant Brain Image Completion", "author": "Zihao Zhu and Tianli Tao and Yitian Tao and Haowen Deng and Xinyi Cai and Gaofeng Wu and Kaidong Wang and Haifeng Tang and Lixuan Zhu and Zhuoyang Gu and Jiawei Huang and Dinggang Shen and Han Zhang", "abstract": "  The infant brain undergoes rapid development in the first few years after\nbirth.Compared to cross-sectional studies, longitudinal studies can depict the\ntrajectories of infants brain development with higher accuracy, statistical\npower and flexibility.However, the collection of infant longitudinal magnetic\nresonance (MR) data suffers a notorious dropout problem, resulting in\nincomplete datasets with missing time points. This limitation significantly\nimpedes subsequent neuroscience and clinical modeling. Yet, existing deep\ngenerative models are facing difficulties in missing brain image completion,\ndue to sparse data and the nonlinear, dramatic contrast/geometric variations in\nthe developing brain. We propose LoCI-DiffCom, a novel Longitudinal\nConsistency-Informed Diffusion model for infant brain image Completion,which\nintegrates the images from preceding and subsequent time points to guide a\ndiffusion model for generating high-fidelity missing data. Our designed LoCI\nmodule can work on highly sparse sequences, relying solely on data from two\ntemporal points. Despite wide separation and diversity between age time points,\nour approach can extract individualized developmental features while ensuring\ncontext-aware consistency. Our experiments on a large infant brain MR dataset\ndemonstrate its effectiveness with consistent performance on missing infant\nbrain MR completion even in big gap scenarios, aiding in better delineation of\nearly developmental trajectories.\n", "link": "http://arxiv.org/abs/2405.10691v1", "date": "2024-05-17", "relevancy": 2.3397, "topK": [{"title": "SDXS: Real-Time One-Step Latent Diffusion Models with Image Conditions", "link": "http://arxiv.org/abs/2403.16627v1", "similarity": 0.601}, {"title": "CAT3D: Create Anything in 3D with Multi-View Diffusion Models", "link": "http://arxiv.org/abs/2405.10314v1", "similarity": 0.5817}, {"title": "CAT3D: Create Anything in 3D with Multi-View Diffusion Models", "link": "http://arxiv.org/abs/2405.10314v1", "similarity": 0.5817}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20LoCI-DiffCom%3A%20Longitudinal%20Consistency-Informed%20Diffusion%20Model%20for%203D%0A%20%20Infant%20Brain%20Image%20Completion&body=Title%3A%20LoCI-DiffCom%3A%20Longitudinal%20Consistency-Informed%20Diffusion%20Model%20for%203D%0A%20%20Infant%20Brain%20Image%20Completion%0AAuthor%3A%20Zihao%20Zhu%20and%20Tianli%20Tao%20and%20Yitian%20Tao%20and%20Haowen%20Deng%20and%20Xinyi%20Cai%20and%20Gaofeng%20Wu%20and%20Kaidong%20Wang%20and%20Haifeng%20Tang%20and%20Lixuan%20Zhu%20and%20Zhuoyang%20Gu%20and%20Jiawei%20Huang%20and%20Dinggang%20Shen%20and%20Han%20Zhang%0AAbstract%3A%20%20%20The%20infant%20brain%20undergoes%20rapid%20development%20in%20the%20first%20few%20years%20after%0Abirth.Compared%20to%20cross-sectional%20studies%2C%20longitudinal%20studies%20can%20depict%20the%0Atrajectories%20of%20infants%20brain%20development%20with%20higher%20accuracy%2C%20statistical%0Apower%20and%20flexibility.However%2C%20the%20collection%20of%20infant%20longitudinal%20magnetic%0Aresonance%20%28MR%29%20data%20suffers%20a%20notorious%20dropout%20problem%2C%20resulting%20in%0Aincomplete%20datasets%20with%20missing%20time%20points.%20This%20limitation%20significantly%0Aimpedes%20subsequent%20neuroscience%20and%20clinical%20modeling.%20Yet%2C%20existing%20deep%0Agenerative%20models%20are%20facing%20difficulties%20in%20missing%20brain%20image%20completion%2C%0Adue%20to%20sparse%20data%20and%20the%20nonlinear%2C%20dramatic%20contrast/geometric%20variations%20in%0Athe%20developing%20brain.%20We%20propose%20LoCI-DiffCom%2C%20a%20novel%20Longitudinal%0AConsistency-Informed%20Diffusion%20model%20for%20infant%20brain%20image%20Completion%2Cwhich%0Aintegrates%20the%20images%20from%20preceding%20and%20subsequent%20time%20points%20to%20guide%20a%0Adiffusion%20model%20for%20generating%20high-fidelity%20missing%20data.%20Our%20designed%20LoCI%0Amodule%20can%20work%20on%20highly%20sparse%20sequences%2C%20relying%20solely%20on%20data%20from%20two%0Atemporal%20points.%20Despite%20wide%20separation%20and%20diversity%20between%20age%20time%20points%2C%0Aour%20approach%20can%20extract%20individualized%20developmental%20features%20while%20ensuring%0Acontext-aware%20consistency.%20Our%20experiments%20on%20a%20large%20infant%20brain%20MR%20dataset%0Ademonstrate%20its%20effectiveness%20with%20consistent%20performance%20on%20missing%20infant%0Abrain%20MR%20completion%20even%20in%20big%20gap%20scenarios%2C%20aiding%20in%20better%20delineation%20of%0Aearly%20developmental%20trajectories.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2405.10691v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DLoCI-DiffCom%253A%2520Longitudinal%2520Consistency-Informed%2520Diffusion%2520Model%2520for%25203D%250A%2520%2520Infant%2520Brain%2520Image%2520Completion%26entry.906535625%3DZihao%2520Zhu%2520and%2520Tianli%2520Tao%2520and%2520Yitian%2520Tao%2520and%2520Haowen%2520Deng%2520and%2520Xinyi%2520Cai%2520and%2520Gaofeng%2520Wu%2520and%2520Kaidong%2520Wang%2520and%2520Haifeng%2520Tang%2520and%2520Lixuan%2520Zhu%2520and%2520Zhuoyang%2520Gu%2520and%2520Jiawei%2520Huang%2520and%2520Dinggang%2520Shen%2520and%2520Han%2520Zhang%26entry.1292438233%3D%2520%2520The%2520infant%2520brain%2520undergoes%2520rapid%2520development%2520in%2520the%2520first%2520few%2520years%2520after%250Abirth.Compared%2520to%2520cross-sectional%2520studies%252C%2520longitudinal%2520studies%2520can%2520depict%2520the%250Atrajectories%2520of%2520infants%2520brain%2520development%2520with%2520higher%2520accuracy%252C%2520statistical%250Apower%2520and%2520flexibility.However%252C%2520the%2520collection%2520of%2520infant%2520longitudinal%2520magnetic%250Aresonance%2520%2528MR%2529%2520data%2520suffers%2520a%2520notorious%2520dropout%2520problem%252C%2520resulting%2520in%250Aincomplete%2520datasets%2520with%2520missing%2520time%2520points.%2520This%2520limitation%2520significantly%250Aimpedes%2520subsequent%2520neuroscience%2520and%2520clinical%2520modeling.%2520Yet%252C%2520existing%2520deep%250Agenerative%2520models%2520are%2520facing%2520difficulties%2520in%2520missing%2520brain%2520image%2520completion%252C%250Adue%2520to%2520sparse%2520data%2520and%2520the%2520nonlinear%252C%2520dramatic%2520contrast/geometric%2520variations%2520in%250Athe%2520developing%2520brain.%2520We%2520propose%2520LoCI-DiffCom%252C%2520a%2520novel%2520Longitudinal%250AConsistency-Informed%2520Diffusion%2520model%2520for%2520infant%2520brain%2520image%2520Completion%252Cwhich%250Aintegrates%2520the%2520images%2520from%2520preceding%2520and%2520subsequent%2520time%2520points%2520to%2520guide%2520a%250Adiffusion%2520model%2520for%2520generating%2520high-fidelity%2520missing%2520data.%2520Our%2520designed%2520LoCI%250Amodule%2520can%2520work%2520on%2520highly%2520sparse%2520sequences%252C%2520relying%2520solely%2520on%2520data%2520from%2520two%250Atemporal%2520points.%2520Despite%2520wide%2520separation%2520and%2520diversity%2520between%2520age%2520time%2520points%252C%250Aour%2520approach%2520can%2520extract%2520individualized%2520developmental%2520features%2520while%2520ensuring%250Acontext-aware%2520consistency.%2520Our%2520experiments%2520on%2520a%2520large%2520infant%2520brain%2520MR%2520dataset%250Ademonstrate%2520its%2520effectiveness%2520with%2520consistent%2520performance%2520on%2520missing%2520infant%250Abrain%2520MR%2520completion%2520even%2520in%2520big%2520gap%2520scenarios%252C%2520aiding%2520in%2520better%2520delineation%2520of%250Aearly%2520developmental%2520trajectories.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2405.10691v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=LoCI-DiffCom%3A%20Longitudinal%20Consistency-Informed%20Diffusion%20Model%20for%203D%0A%20%20Infant%20Brain%20Image%20Completion&entry.906535625=Zihao%20Zhu%20and%20Tianli%20Tao%20and%20Yitian%20Tao%20and%20Haowen%20Deng%20and%20Xinyi%20Cai%20and%20Gaofeng%20Wu%20and%20Kaidong%20Wang%20and%20Haifeng%20Tang%20and%20Lixuan%20Zhu%20and%20Zhuoyang%20Gu%20and%20Jiawei%20Huang%20and%20Dinggang%20Shen%20and%20Han%20Zhang&entry.1292438233=%20%20The%20infant%20brain%20undergoes%20rapid%20development%20in%20the%20first%20few%20years%20after%0Abirth.Compared%20to%20cross-sectional%20studies%2C%20longitudinal%20studies%20can%20depict%20the%0Atrajectories%20of%20infants%20brain%20development%20with%20higher%20accuracy%2C%20statistical%0Apower%20and%20flexibility.However%2C%20the%20collection%20of%20infant%20longitudinal%20magnetic%0Aresonance%20%28MR%29%20data%20suffers%20a%20notorious%20dropout%20problem%2C%20resulting%20in%0Aincomplete%20datasets%20with%20missing%20time%20points.%20This%20limitation%20significantly%0Aimpedes%20subsequent%20neuroscience%20and%20clinical%20modeling.%20Yet%2C%20existing%20deep%0Agenerative%20models%20are%20facing%20difficulties%20in%20missing%20brain%20image%20completion%2C%0Adue%20to%20sparse%20data%20and%20the%20nonlinear%2C%20dramatic%20contrast/geometric%20variations%20in%0Athe%20developing%20brain.%20We%20propose%20LoCI-DiffCom%2C%20a%20novel%20Longitudinal%0AConsistency-Informed%20Diffusion%20model%20for%20infant%20brain%20image%20Completion%2Cwhich%0Aintegrates%20the%20images%20from%20preceding%20and%20subsequent%20time%20points%20to%20guide%20a%0Adiffusion%20model%20for%20generating%20high-fidelity%20missing%20data.%20Our%20designed%20LoCI%0Amodule%20can%20work%20on%20highly%20sparse%20sequences%2C%20relying%20solely%20on%20data%20from%20two%0Atemporal%20points.%20Despite%20wide%20separation%20and%20diversity%20between%20age%20time%20points%2C%0Aour%20approach%20can%20extract%20individualized%20developmental%20features%20while%20ensuring%0Acontext-aware%20consistency.%20Our%20experiments%20on%20a%20large%20infant%20brain%20MR%20dataset%0Ademonstrate%20its%20effectiveness%20with%20consistent%20performance%20on%20missing%20infant%0Abrain%20MR%20completion%20even%20in%20big%20gap%20scenarios%2C%20aiding%20in%20better%20delineation%20of%0Aearly%20developmental%20trajectories.%0A&entry.1838667208=http%3A//arxiv.org/abs/2405.10691v1&entry.124074799=Read"},
{"title": "Reconstruction of Manipulated Garment with Guided Deformation Prior", "author": "Ren Li and Corentin Dumery and Zhantao Deng and Pascal Fua", "abstract": "  Modeling the shape of garments has received much attention, but most existing\napproaches assume the garments to be worn by someone, which constrains the\nrange of shapes they can assume. In this work, we address shape recovery when\ngarments are being manipulated instead of worn, which gives rise to an even\nlarger range of possible shapes. To this end, we leverage the implicit sewing\npatterns (ISP) model for garment modeling and extend it by adding a\ndiffusion-based deformation prior to represent these shapes. To recover 3D\ngarment shapes from incomplete 3D point clouds acquired when the garment is\nfolded, we map the points to UV space, in which our priors are learned, to\nproduce partial UV maps, and then fit the priors to recover complete UV maps\nand 2D to 3D mappings. Experimental results demonstrate the superior\nreconstruction accuracy of our method compared to previous ones, especially\nwhen dealing with large non-rigid deformations arising from the manipulations.\n", "link": "http://arxiv.org/abs/2405.10934v1", "date": "2024-05-17", "relevancy": 2.3371, "topK": [{"title": "DressCode: Autoregressively Sewing and Generating Garments from Text\n  Guidance", "link": "http://arxiv.org/abs/2401.16465v3", "similarity": 0.6299}, {"title": "Multimodal-Conditioned Latent Diffusion Models for Fashion Image Editing", "link": "http://arxiv.org/abs/2403.14828v2", "similarity": 0.5896}, {"title": "UGG: Unified Generative Grasping", "link": "https://arxiv.org/abs/2311.16917", "similarity": 0.5365}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Reconstruction%20of%20Manipulated%20Garment%20with%20Guided%20Deformation%20Prior&body=Title%3A%20Reconstruction%20of%20Manipulated%20Garment%20with%20Guided%20Deformation%20Prior%0AAuthor%3A%20Ren%20Li%20and%20Corentin%20Dumery%20and%20Zhantao%20Deng%20and%20Pascal%20Fua%0AAbstract%3A%20%20%20Modeling%20the%20shape%20of%20garments%20has%20received%20much%20attention%2C%20but%20most%20existing%0Aapproaches%20assume%20the%20garments%20to%20be%20worn%20by%20someone%2C%20which%20constrains%20the%0Arange%20of%20shapes%20they%20can%20assume.%20In%20this%20work%2C%20we%20address%20shape%20recovery%20when%0Agarments%20are%20being%20manipulated%20instead%20of%20worn%2C%20which%20gives%20rise%20to%20an%20even%0Alarger%20range%20of%20possible%20shapes.%20To%20this%20end%2C%20we%20leverage%20the%20implicit%20sewing%0Apatterns%20%28ISP%29%20model%20for%20garment%20modeling%20and%20extend%20it%20by%20adding%20a%0Adiffusion-based%20deformation%20prior%20to%20represent%20these%20shapes.%20To%20recover%203D%0Agarment%20shapes%20from%20incomplete%203D%20point%20clouds%20acquired%20when%20the%20garment%20is%0Afolded%2C%20we%20map%20the%20points%20to%20UV%20space%2C%20in%20which%20our%20priors%20are%20learned%2C%20to%0Aproduce%20partial%20UV%20maps%2C%20and%20then%20fit%20the%20priors%20to%20recover%20complete%20UV%20maps%0Aand%202D%20to%203D%20mappings.%20Experimental%20results%20demonstrate%20the%20superior%0Areconstruction%20accuracy%20of%20our%20method%20compared%20to%20previous%20ones%2C%20especially%0Awhen%20dealing%20with%20large%20non-rigid%20deformations%20arising%20from%20the%20manipulations.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2405.10934v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DReconstruction%2520of%2520Manipulated%2520Garment%2520with%2520Guided%2520Deformation%2520Prior%26entry.906535625%3DRen%2520Li%2520and%2520Corentin%2520Dumery%2520and%2520Zhantao%2520Deng%2520and%2520Pascal%2520Fua%26entry.1292438233%3D%2520%2520Modeling%2520the%2520shape%2520of%2520garments%2520has%2520received%2520much%2520attention%252C%2520but%2520most%2520existing%250Aapproaches%2520assume%2520the%2520garments%2520to%2520be%2520worn%2520by%2520someone%252C%2520which%2520constrains%2520the%250Arange%2520of%2520shapes%2520they%2520can%2520assume.%2520In%2520this%2520work%252C%2520we%2520address%2520shape%2520recovery%2520when%250Agarments%2520are%2520being%2520manipulated%2520instead%2520of%2520worn%252C%2520which%2520gives%2520rise%2520to%2520an%2520even%250Alarger%2520range%2520of%2520possible%2520shapes.%2520To%2520this%2520end%252C%2520we%2520leverage%2520the%2520implicit%2520sewing%250Apatterns%2520%2528ISP%2529%2520model%2520for%2520garment%2520modeling%2520and%2520extend%2520it%2520by%2520adding%2520a%250Adiffusion-based%2520deformation%2520prior%2520to%2520represent%2520these%2520shapes.%2520To%2520recover%25203D%250Agarment%2520shapes%2520from%2520incomplete%25203D%2520point%2520clouds%2520acquired%2520when%2520the%2520garment%2520is%250Afolded%252C%2520we%2520map%2520the%2520points%2520to%2520UV%2520space%252C%2520in%2520which%2520our%2520priors%2520are%2520learned%252C%2520to%250Aproduce%2520partial%2520UV%2520maps%252C%2520and%2520then%2520fit%2520the%2520priors%2520to%2520recover%2520complete%2520UV%2520maps%250Aand%25202D%2520to%25203D%2520mappings.%2520Experimental%2520results%2520demonstrate%2520the%2520superior%250Areconstruction%2520accuracy%2520of%2520our%2520method%2520compared%2520to%2520previous%2520ones%252C%2520especially%250Awhen%2520dealing%2520with%2520large%2520non-rigid%2520deformations%2520arising%2520from%2520the%2520manipulations.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2405.10934v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Reconstruction%20of%20Manipulated%20Garment%20with%20Guided%20Deformation%20Prior&entry.906535625=Ren%20Li%20and%20Corentin%20Dumery%20and%20Zhantao%20Deng%20and%20Pascal%20Fua&entry.1292438233=%20%20Modeling%20the%20shape%20of%20garments%20has%20received%20much%20attention%2C%20but%20most%20existing%0Aapproaches%20assume%20the%20garments%20to%20be%20worn%20by%20someone%2C%20which%20constrains%20the%0Arange%20of%20shapes%20they%20can%20assume.%20In%20this%20work%2C%20we%20address%20shape%20recovery%20when%0Agarments%20are%20being%20manipulated%20instead%20of%20worn%2C%20which%20gives%20rise%20to%20an%20even%0Alarger%20range%20of%20possible%20shapes.%20To%20this%20end%2C%20we%20leverage%20the%20implicit%20sewing%0Apatterns%20%28ISP%29%20model%20for%20garment%20modeling%20and%20extend%20it%20by%20adding%20a%0Adiffusion-based%20deformation%20prior%20to%20represent%20these%20shapes.%20To%20recover%203D%0Agarment%20shapes%20from%20incomplete%203D%20point%20clouds%20acquired%20when%20the%20garment%20is%0Afolded%2C%20we%20map%20the%20points%20to%20UV%20space%2C%20in%20which%20our%20priors%20are%20learned%2C%20to%0Aproduce%20partial%20UV%20maps%2C%20and%20then%20fit%20the%20priors%20to%20recover%20complete%20UV%20maps%0Aand%202D%20to%203D%20mappings.%20Experimental%20results%20demonstrate%20the%20superior%0Areconstruction%20accuracy%20of%20our%20method%20compared%20to%20previous%20ones%2C%20especially%0Awhen%20dealing%20with%20large%20non-rigid%20deformations%20arising%20from%20the%20manipulations.%0A&entry.1838667208=http%3A//arxiv.org/abs/2405.10934v1&entry.124074799=Read"},
{"title": "Air Signing and Privacy-Preserving Signature Verification for Digital\n  Documents", "author": "P. Sarveswarasarma and T. Sathulakjan and V. J. V. Godfrey and Thanuja D. Ambegoda", "abstract": "  This paper presents a novel approach to the digital signing of electronic\ndocuments through the use of a camera-based interaction system, single-finger\ntracking for sign recognition, and multi commands executing hand gestures. The\nproposed solution, referred to as \"Air Signature,\" involves writing the\nsignature in front of the camera, rather than relying on traditional methods\nsuch as mouse drawing or physically signing on paper and showing it to a web\ncamera. The goal is to develop a state-of-the-art method for detecting and\ntracking gestures and objects in real-time. The proposed methods include\napplying existing gesture recognition and object tracking systems, improving\naccuracy through smoothing and line drawing, and maintaining continuity during\nfast finger movements. An evaluation of the fingertip detection, sketching, and\noverall signing process is performed to assess the effectiveness of the\nproposed solution. The secondary objective of this research is to develop a\nmodel that can effectively recognize the unique signature of a user. This type\nof signature can be verified by neural cores that analyze the movement, speed,\nand stroke pixels of the signing in real time. The neural cores use machine\nlearning algorithms to match air signatures to the individual's stored\nsignatures, providing a secure and efficient method of verification. Our\nproposed System does not require sensors or any hardware other than the camera.\n", "link": "http://arxiv.org/abs/2405.10868v1", "date": "2024-05-17", "relevancy": 2.3277, "topK": [{"title": "Egocentric Computer Vision for Hands-Free Robotic Wheelchair Navigation", "link": "https://link.springer.com/article/10.1007/s10846-023-01807-4", "similarity": 0.473}, {"title": "VirtualModel: Generating Object-ID-retentive Human-object Interaction\n  Image by Diffusion Model for E-commerce Marketing", "link": "http://arxiv.org/abs/2405.09985v1", "similarity": 0.4671}, {"title": "Magic-Me: Identity-Specific Video Customized Diffusion", "link": "http://arxiv.org/abs/2402.09368v1", "similarity": 0.4565}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Air%20Signing%20and%20Privacy-Preserving%20Signature%20Verification%20for%20Digital%0A%20%20Documents&body=Title%3A%20Air%20Signing%20and%20Privacy-Preserving%20Signature%20Verification%20for%20Digital%0A%20%20Documents%0AAuthor%3A%20P.%20Sarveswarasarma%20and%20T.%20Sathulakjan%20and%20V.%20J.%20V.%20Godfrey%20and%20Thanuja%20D.%20Ambegoda%0AAbstract%3A%20%20%20This%20paper%20presents%20a%20novel%20approach%20to%20the%20digital%20signing%20of%20electronic%0Adocuments%20through%20the%20use%20of%20a%20camera-based%20interaction%20system%2C%20single-finger%0Atracking%20for%20sign%20recognition%2C%20and%20multi%20commands%20executing%20hand%20gestures.%20The%0Aproposed%20solution%2C%20referred%20to%20as%20%22Air%20Signature%2C%22%20involves%20writing%20the%0Asignature%20in%20front%20of%20the%20camera%2C%20rather%20than%20relying%20on%20traditional%20methods%0Asuch%20as%20mouse%20drawing%20or%20physically%20signing%20on%20paper%20and%20showing%20it%20to%20a%20web%0Acamera.%20The%20goal%20is%20to%20develop%20a%20state-of-the-art%20method%20for%20detecting%20and%0Atracking%20gestures%20and%20objects%20in%20real-time.%20The%20proposed%20methods%20include%0Aapplying%20existing%20gesture%20recognition%20and%20object%20tracking%20systems%2C%20improving%0Aaccuracy%20through%20smoothing%20and%20line%20drawing%2C%20and%20maintaining%20continuity%20during%0Afast%20finger%20movements.%20An%20evaluation%20of%20the%20fingertip%20detection%2C%20sketching%2C%20and%0Aoverall%20signing%20process%20is%20performed%20to%20assess%20the%20effectiveness%20of%20the%0Aproposed%20solution.%20The%20secondary%20objective%20of%20this%20research%20is%20to%20develop%20a%0Amodel%20that%20can%20effectively%20recognize%20the%20unique%20signature%20of%20a%20user.%20This%20type%0Aof%20signature%20can%20be%20verified%20by%20neural%20cores%20that%20analyze%20the%20movement%2C%20speed%2C%0Aand%20stroke%20pixels%20of%20the%20signing%20in%20real%20time.%20The%20neural%20cores%20use%20machine%0Alearning%20algorithms%20to%20match%20air%20signatures%20to%20the%20individual%27s%20stored%0Asignatures%2C%20providing%20a%20secure%20and%20efficient%20method%20of%20verification.%20Our%0Aproposed%20System%20does%20not%20require%20sensors%20or%20any%20hardware%20other%20than%20the%20camera.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2405.10868v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DAir%2520Signing%2520and%2520Privacy-Preserving%2520Signature%2520Verification%2520for%2520Digital%250A%2520%2520Documents%26entry.906535625%3DP.%2520Sarveswarasarma%2520and%2520T.%2520Sathulakjan%2520and%2520V.%2520J.%2520V.%2520Godfrey%2520and%2520Thanuja%2520D.%2520Ambegoda%26entry.1292438233%3D%2520%2520This%2520paper%2520presents%2520a%2520novel%2520approach%2520to%2520the%2520digital%2520signing%2520of%2520electronic%250Adocuments%2520through%2520the%2520use%2520of%2520a%2520camera-based%2520interaction%2520system%252C%2520single-finger%250Atracking%2520for%2520sign%2520recognition%252C%2520and%2520multi%2520commands%2520executing%2520hand%2520gestures.%2520The%250Aproposed%2520solution%252C%2520referred%2520to%2520as%2520%2522Air%2520Signature%252C%2522%2520involves%2520writing%2520the%250Asignature%2520in%2520front%2520of%2520the%2520camera%252C%2520rather%2520than%2520relying%2520on%2520traditional%2520methods%250Asuch%2520as%2520mouse%2520drawing%2520or%2520physically%2520signing%2520on%2520paper%2520and%2520showing%2520it%2520to%2520a%2520web%250Acamera.%2520The%2520goal%2520is%2520to%2520develop%2520a%2520state-of-the-art%2520method%2520for%2520detecting%2520and%250Atracking%2520gestures%2520and%2520objects%2520in%2520real-time.%2520The%2520proposed%2520methods%2520include%250Aapplying%2520existing%2520gesture%2520recognition%2520and%2520object%2520tracking%2520systems%252C%2520improving%250Aaccuracy%2520through%2520smoothing%2520and%2520line%2520drawing%252C%2520and%2520maintaining%2520continuity%2520during%250Afast%2520finger%2520movements.%2520An%2520evaluation%2520of%2520the%2520fingertip%2520detection%252C%2520sketching%252C%2520and%250Aoverall%2520signing%2520process%2520is%2520performed%2520to%2520assess%2520the%2520effectiveness%2520of%2520the%250Aproposed%2520solution.%2520The%2520secondary%2520objective%2520of%2520this%2520research%2520is%2520to%2520develop%2520a%250Amodel%2520that%2520can%2520effectively%2520recognize%2520the%2520unique%2520signature%2520of%2520a%2520user.%2520This%2520type%250Aof%2520signature%2520can%2520be%2520verified%2520by%2520neural%2520cores%2520that%2520analyze%2520the%2520movement%252C%2520speed%252C%250Aand%2520stroke%2520pixels%2520of%2520the%2520signing%2520in%2520real%2520time.%2520The%2520neural%2520cores%2520use%2520machine%250Alearning%2520algorithms%2520to%2520match%2520air%2520signatures%2520to%2520the%2520individual%2527s%2520stored%250Asignatures%252C%2520providing%2520a%2520secure%2520and%2520efficient%2520method%2520of%2520verification.%2520Our%250Aproposed%2520System%2520does%2520not%2520require%2520sensors%2520or%2520any%2520hardware%2520other%2520than%2520the%2520camera.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2405.10868v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Air%20Signing%20and%20Privacy-Preserving%20Signature%20Verification%20for%20Digital%0A%20%20Documents&entry.906535625=P.%20Sarveswarasarma%20and%20T.%20Sathulakjan%20and%20V.%20J.%20V.%20Godfrey%20and%20Thanuja%20D.%20Ambegoda&entry.1292438233=%20%20This%20paper%20presents%20a%20novel%20approach%20to%20the%20digital%20signing%20of%20electronic%0Adocuments%20through%20the%20use%20of%20a%20camera-based%20interaction%20system%2C%20single-finger%0Atracking%20for%20sign%20recognition%2C%20and%20multi%20commands%20executing%20hand%20gestures.%20The%0Aproposed%20solution%2C%20referred%20to%20as%20%22Air%20Signature%2C%22%20involves%20writing%20the%0Asignature%20in%20front%20of%20the%20camera%2C%20rather%20than%20relying%20on%20traditional%20methods%0Asuch%20as%20mouse%20drawing%20or%20physically%20signing%20on%20paper%20and%20showing%20it%20to%20a%20web%0Acamera.%20The%20goal%20is%20to%20develop%20a%20state-of-the-art%20method%20for%20detecting%20and%0Atracking%20gestures%20and%20objects%20in%20real-time.%20The%20proposed%20methods%20include%0Aapplying%20existing%20gesture%20recognition%20and%20object%20tracking%20systems%2C%20improving%0Aaccuracy%20through%20smoothing%20and%20line%20drawing%2C%20and%20maintaining%20continuity%20during%0Afast%20finger%20movements.%20An%20evaluation%20of%20the%20fingertip%20detection%2C%20sketching%2C%20and%0Aoverall%20signing%20process%20is%20performed%20to%20assess%20the%20effectiveness%20of%20the%0Aproposed%20solution.%20The%20secondary%20objective%20of%20this%20research%20is%20to%20develop%20a%0Amodel%20that%20can%20effectively%20recognize%20the%20unique%20signature%20of%20a%20user.%20This%20type%0Aof%20signature%20can%20be%20verified%20by%20neural%20cores%20that%20analyze%20the%20movement%2C%20speed%2C%0Aand%20stroke%20pixels%20of%20the%20signing%20in%20real%20time.%20The%20neural%20cores%20use%20machine%0Alearning%20algorithms%20to%20match%20air%20signatures%20to%20the%20individual%27s%20stored%0Asignatures%2C%20providing%20a%20secure%20and%20efficient%20method%20of%20verification.%20Our%0Aproposed%20System%20does%20not%20require%20sensors%20or%20any%20hardware%20other%20than%20the%20camera.%0A&entry.1838667208=http%3A//arxiv.org/abs/2405.10868v1&entry.124074799=Read"},
{"title": "FutureHuman3D: Forecasting Complex Long-Term 3D Human Behavior from\n  Video Observations", "author": "Christian Diller and Thomas Funkhouser and Angela Dai", "abstract": "  We present a generative approach to forecast long-term future human behavior\nin 3D, requiring only weak supervision from readily available 2D human action\ndata. This is a fundamental task enabling many downstream applications. The\nrequired ground-truth data is hard to capture in 3D (mocap suits, expensive\nsetups) but easy to acquire in 2D (simple RGB cameras). Thus, we design our\nmethod to only require 2D RGB data at inference time while being able to\ngenerate 3D human motion sequences. We use a differentiable 2D projection\nscheme in an autoregressive manner for weak supervision, and an adversarial\nloss for 3D regularization. Our method predicts long and complex human behavior\nsequences (e.g., cooking, assembly) consisting of multiple sub-actions. We\ntackle this in a semantically hierarchical manner, jointly predicting\nhigh-level coarse action labels together with their low-level fine-grained\nrealizations as characteristic 3D human poses. We observe that these two action\nrepresentations are coupled in nature, and joint prediction benefits both\naction and pose forecasting. Our experiments demonstrate the complementary\nnature of joint action and 3D pose prediction: our joint approach outperforms\neach task treated individually, enables robust longer-term sequence prediction,\nand improves over alternative approaches to forecast actions and characteristic\n3D poses.\n", "link": "http://arxiv.org/abs/2211.14309v3", "date": "2024-05-17", "relevancy": 2.3219, "topK": [{"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.6016}, {"title": "PoseAnimate: Zero-shot high fidelity pose controllable character\n  animation", "link": "http://arxiv.org/abs/2404.13680v2", "similarity": 0.5808}, {"title": "Genie: Generative Interactive Environments", "link": "http://arxiv.org/abs/2402.15391v1", "similarity": 0.5717}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20FutureHuman3D%3A%20Forecasting%20Complex%20Long-Term%203D%20Human%20Behavior%20from%0A%20%20Video%20Observations&body=Title%3A%20FutureHuman3D%3A%20Forecasting%20Complex%20Long-Term%203D%20Human%20Behavior%20from%0A%20%20Video%20Observations%0AAuthor%3A%20Christian%20Diller%20and%20Thomas%20Funkhouser%20and%20Angela%20Dai%0AAbstract%3A%20%20%20We%20present%20a%20generative%20approach%20to%20forecast%20long-term%20future%20human%20behavior%0Ain%203D%2C%20requiring%20only%20weak%20supervision%20from%20readily%20available%202D%20human%20action%0Adata.%20This%20is%20a%20fundamental%20task%20enabling%20many%20downstream%20applications.%20The%0Arequired%20ground-truth%20data%20is%20hard%20to%20capture%20in%203D%20%28mocap%20suits%2C%20expensive%0Asetups%29%20but%20easy%20to%20acquire%20in%202D%20%28simple%20RGB%20cameras%29.%20Thus%2C%20we%20design%20our%0Amethod%20to%20only%20require%202D%20RGB%20data%20at%20inference%20time%20while%20being%20able%20to%0Agenerate%203D%20human%20motion%20sequences.%20We%20use%20a%20differentiable%202D%20projection%0Ascheme%20in%20an%20autoregressive%20manner%20for%20weak%20supervision%2C%20and%20an%20adversarial%0Aloss%20for%203D%20regularization.%20Our%20method%20predicts%20long%20and%20complex%20human%20behavior%0Asequences%20%28e.g.%2C%20cooking%2C%20assembly%29%20consisting%20of%20multiple%20sub-actions.%20We%0Atackle%20this%20in%20a%20semantically%20hierarchical%20manner%2C%20jointly%20predicting%0Ahigh-level%20coarse%20action%20labels%20together%20with%20their%20low-level%20fine-grained%0Arealizations%20as%20characteristic%203D%20human%20poses.%20We%20observe%20that%20these%20two%20action%0Arepresentations%20are%20coupled%20in%20nature%2C%20and%20joint%20prediction%20benefits%20both%0Aaction%20and%20pose%20forecasting.%20Our%20experiments%20demonstrate%20the%20complementary%0Anature%20of%20joint%20action%20and%203D%20pose%20prediction%3A%20our%20joint%20approach%20outperforms%0Aeach%20task%20treated%20individually%2C%20enables%20robust%20longer-term%20sequence%20prediction%2C%0Aand%20improves%20over%20alternative%20approaches%20to%20forecast%20actions%20and%20characteristic%0A3D%20poses.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2211.14309v3%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DFutureHuman3D%253A%2520Forecasting%2520Complex%2520Long-Term%25203D%2520Human%2520Behavior%2520from%250A%2520%2520Video%2520Observations%26entry.906535625%3DChristian%2520Diller%2520and%2520Thomas%2520Funkhouser%2520and%2520Angela%2520Dai%26entry.1292438233%3D%2520%2520We%2520present%2520a%2520generative%2520approach%2520to%2520forecast%2520long-term%2520future%2520human%2520behavior%250Ain%25203D%252C%2520requiring%2520only%2520weak%2520supervision%2520from%2520readily%2520available%25202D%2520human%2520action%250Adata.%2520This%2520is%2520a%2520fundamental%2520task%2520enabling%2520many%2520downstream%2520applications.%2520The%250Arequired%2520ground-truth%2520data%2520is%2520hard%2520to%2520capture%2520in%25203D%2520%2528mocap%2520suits%252C%2520expensive%250Asetups%2529%2520but%2520easy%2520to%2520acquire%2520in%25202D%2520%2528simple%2520RGB%2520cameras%2529.%2520Thus%252C%2520we%2520design%2520our%250Amethod%2520to%2520only%2520require%25202D%2520RGB%2520data%2520at%2520inference%2520time%2520while%2520being%2520able%2520to%250Agenerate%25203D%2520human%2520motion%2520sequences.%2520We%2520use%2520a%2520differentiable%25202D%2520projection%250Ascheme%2520in%2520an%2520autoregressive%2520manner%2520for%2520weak%2520supervision%252C%2520and%2520an%2520adversarial%250Aloss%2520for%25203D%2520regularization.%2520Our%2520method%2520predicts%2520long%2520and%2520complex%2520human%2520behavior%250Asequences%2520%2528e.g.%252C%2520cooking%252C%2520assembly%2529%2520consisting%2520of%2520multiple%2520sub-actions.%2520We%250Atackle%2520this%2520in%2520a%2520semantically%2520hierarchical%2520manner%252C%2520jointly%2520predicting%250Ahigh-level%2520coarse%2520action%2520labels%2520together%2520with%2520their%2520low-level%2520fine-grained%250Arealizations%2520as%2520characteristic%25203D%2520human%2520poses.%2520We%2520observe%2520that%2520these%2520two%2520action%250Arepresentations%2520are%2520coupled%2520in%2520nature%252C%2520and%2520joint%2520prediction%2520benefits%2520both%250Aaction%2520and%2520pose%2520forecasting.%2520Our%2520experiments%2520demonstrate%2520the%2520complementary%250Anature%2520of%2520joint%2520action%2520and%25203D%2520pose%2520prediction%253A%2520our%2520joint%2520approach%2520outperforms%250Aeach%2520task%2520treated%2520individually%252C%2520enables%2520robust%2520longer-term%2520sequence%2520prediction%252C%250Aand%2520improves%2520over%2520alternative%2520approaches%2520to%2520forecast%2520actions%2520and%2520characteristic%250A3D%2520poses.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2211.14309v3%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=FutureHuman3D%3A%20Forecasting%20Complex%20Long-Term%203D%20Human%20Behavior%20from%0A%20%20Video%20Observations&entry.906535625=Christian%20Diller%20and%20Thomas%20Funkhouser%20and%20Angela%20Dai&entry.1292438233=%20%20We%20present%20a%20generative%20approach%20to%20forecast%20long-term%20future%20human%20behavior%0Ain%203D%2C%20requiring%20only%20weak%20supervision%20from%20readily%20available%202D%20human%20action%0Adata.%20This%20is%20a%20fundamental%20task%20enabling%20many%20downstream%20applications.%20The%0Arequired%20ground-truth%20data%20is%20hard%20to%20capture%20in%203D%20%28mocap%20suits%2C%20expensive%0Asetups%29%20but%20easy%20to%20acquire%20in%202D%20%28simple%20RGB%20cameras%29.%20Thus%2C%20we%20design%20our%0Amethod%20to%20only%20require%202D%20RGB%20data%20at%20inference%20time%20while%20being%20able%20to%0Agenerate%203D%20human%20motion%20sequences.%20We%20use%20a%20differentiable%202D%20projection%0Ascheme%20in%20an%20autoregressive%20manner%20for%20weak%20supervision%2C%20and%20an%20adversarial%0Aloss%20for%203D%20regularization.%20Our%20method%20predicts%20long%20and%20complex%20human%20behavior%0Asequences%20%28e.g.%2C%20cooking%2C%20assembly%29%20consisting%20of%20multiple%20sub-actions.%20We%0Atackle%20this%20in%20a%20semantically%20hierarchical%20manner%2C%20jointly%20predicting%0Ahigh-level%20coarse%20action%20labels%20together%20with%20their%20low-level%20fine-grained%0Arealizations%20as%20characteristic%203D%20human%20poses.%20We%20observe%20that%20these%20two%20action%0Arepresentations%20are%20coupled%20in%20nature%2C%20and%20joint%20prediction%20benefits%20both%0Aaction%20and%20pose%20forecasting.%20Our%20experiments%20demonstrate%20the%20complementary%0Anature%20of%20joint%20action%20and%203D%20pose%20prediction%3A%20our%20joint%20approach%20outperforms%0Aeach%20task%20treated%20individually%2C%20enables%20robust%20longer-term%20sequence%20prediction%2C%0Aand%20improves%20over%20alternative%20approaches%20to%20forecast%20actions%20and%20characteristic%0A3D%20poses.%0A&entry.1838667208=http%3A//arxiv.org/abs/2211.14309v3&entry.124074799=Read"},
{"title": "Node Feature Augmentation Vitaminizes Network Alignment", "author": "Jin-Duk Park and Cong Tran and Won-Yong Shin and Xin Cao", "abstract": "  Network alignment (NA) is the task of discovering node correspondences across\nmultiple networks. Although NA methods have achieved remarkable success in a\nmyriad of scenarios, their effectiveness is not without additional information\nsuch as prior anchor links and/or node features, which may not always be\navailable due to privacy concerns or access restrictions. To tackle this\nchallenge, we propose Grad-Align+, a novel NA method built upon a recent\nstate-of-the-art NA method, the so-called Grad-Align, that gradually discovers\na part of node pairs until all node pairs are found. In designing Grad-Align+,\nwe account for how to augment node features in the sense of performing the NA\ntask and how to design our NA method by maximally exploiting the augmented node\nfeatures. To achieve this goal, Grad-Align+ consists of three key components:\n1) centrality-based node feature augmentation (CNFA), 2) graph neural network\n(GNN)-aided embedding similarity calculation alongside the augmented node\nfeatures, and 3) gradual NA with similarity calculation using aligned\ncross-network neighbor-pairs (ACNs). Through comprehensive experiments, we\ndemonstrate that Grad-Align+ exhibits (a) the superiority over benchmark NA\nmethods, (b) empirical validations as well as our theoretical findings to see\nthe effectiveness of CNFA, (c) the influence of each component, (d) the\nrobustness to network noises, and (e) the computational efficiency.\n", "link": "http://arxiv.org/abs/2304.12751v4", "date": "2024-05-17", "relevancy": 2.313, "topK": [{"title": "Boosted dynamic neural networks", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/26302", "similarity": 0.4761}, {"title": "DDM-NET: End-to-end learning of keypoint feature Detection, Description and Matching for 3D localization", "link": "https://arxiv.org/abs/2212.04575", "similarity": 0.466}, {"title": "Breadcrumbs: Adversarial class-balanced sampling for long-tailed recognition", "link": "https://link.springer.com/chapter/10.1007/978-3-031-20053-3_37", "similarity": 0.4457}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Node%20Feature%20Augmentation%20Vitaminizes%20Network%20Alignment&body=Title%3A%20Node%20Feature%20Augmentation%20Vitaminizes%20Network%20Alignment%0AAuthor%3A%20Jin-Duk%20Park%20and%20Cong%20Tran%20and%20Won-Yong%20Shin%20and%20Xin%20Cao%0AAbstract%3A%20%20%20Network%20alignment%20%28NA%29%20is%20the%20task%20of%20discovering%20node%20correspondences%20across%0Amultiple%20networks.%20Although%20NA%20methods%20have%20achieved%20remarkable%20success%20in%20a%0Amyriad%20of%20scenarios%2C%20their%20effectiveness%20is%20not%20without%20additional%20information%0Asuch%20as%20prior%20anchor%20links%20and/or%20node%20features%2C%20which%20may%20not%20always%20be%0Aavailable%20due%20to%20privacy%20concerns%20or%20access%20restrictions.%20To%20tackle%20this%0Achallenge%2C%20we%20propose%20Grad-Align%2B%2C%20a%20novel%20NA%20method%20built%20upon%20a%20recent%0Astate-of-the-art%20NA%20method%2C%20the%20so-called%20Grad-Align%2C%20that%20gradually%20discovers%0Aa%20part%20of%20node%20pairs%20until%20all%20node%20pairs%20are%20found.%20In%20designing%20Grad-Align%2B%2C%0Awe%20account%20for%20how%20to%20augment%20node%20features%20in%20the%20sense%20of%20performing%20the%20NA%0Atask%20and%20how%20to%20design%20our%20NA%20method%20by%20maximally%20exploiting%20the%20augmented%20node%0Afeatures.%20To%20achieve%20this%20goal%2C%20Grad-Align%2B%20consists%20of%20three%20key%20components%3A%0A1%29%20centrality-based%20node%20feature%20augmentation%20%28CNFA%29%2C%202%29%20graph%20neural%20network%0A%28GNN%29-aided%20embedding%20similarity%20calculation%20alongside%20the%20augmented%20node%0Afeatures%2C%20and%203%29%20gradual%20NA%20with%20similarity%20calculation%20using%20aligned%0Across-network%20neighbor-pairs%20%28ACNs%29.%20Through%20comprehensive%20experiments%2C%20we%0Ademonstrate%20that%20Grad-Align%2B%20exhibits%20%28a%29%20the%20superiority%20over%20benchmark%20NA%0Amethods%2C%20%28b%29%20empirical%20validations%20as%20well%20as%20our%20theoretical%20findings%20to%20see%0Athe%20effectiveness%20of%20CNFA%2C%20%28c%29%20the%20influence%20of%20each%20component%2C%20%28d%29%20the%0Arobustness%20to%20network%20noises%2C%20and%20%28e%29%20the%20computational%20efficiency.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2304.12751v4%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DNode%2520Feature%2520Augmentation%2520Vitaminizes%2520Network%2520Alignment%26entry.906535625%3DJin-Duk%2520Park%2520and%2520Cong%2520Tran%2520and%2520Won-Yong%2520Shin%2520and%2520Xin%2520Cao%26entry.1292438233%3D%2520%2520Network%2520alignment%2520%2528NA%2529%2520is%2520the%2520task%2520of%2520discovering%2520node%2520correspondences%2520across%250Amultiple%2520networks.%2520Although%2520NA%2520methods%2520have%2520achieved%2520remarkable%2520success%2520in%2520a%250Amyriad%2520of%2520scenarios%252C%2520their%2520effectiveness%2520is%2520not%2520without%2520additional%2520information%250Asuch%2520as%2520prior%2520anchor%2520links%2520and/or%2520node%2520features%252C%2520which%2520may%2520not%2520always%2520be%250Aavailable%2520due%2520to%2520privacy%2520concerns%2520or%2520access%2520restrictions.%2520To%2520tackle%2520this%250Achallenge%252C%2520we%2520propose%2520Grad-Align%252B%252C%2520a%2520novel%2520NA%2520method%2520built%2520upon%2520a%2520recent%250Astate-of-the-art%2520NA%2520method%252C%2520the%2520so-called%2520Grad-Align%252C%2520that%2520gradually%2520discovers%250Aa%2520part%2520of%2520node%2520pairs%2520until%2520all%2520node%2520pairs%2520are%2520found.%2520In%2520designing%2520Grad-Align%252B%252C%250Awe%2520account%2520for%2520how%2520to%2520augment%2520node%2520features%2520in%2520the%2520sense%2520of%2520performing%2520the%2520NA%250Atask%2520and%2520how%2520to%2520design%2520our%2520NA%2520method%2520by%2520maximally%2520exploiting%2520the%2520augmented%2520node%250Afeatures.%2520To%2520achieve%2520this%2520goal%252C%2520Grad-Align%252B%2520consists%2520of%2520three%2520key%2520components%253A%250A1%2529%2520centrality-based%2520node%2520feature%2520augmentation%2520%2528CNFA%2529%252C%25202%2529%2520graph%2520neural%2520network%250A%2528GNN%2529-aided%2520embedding%2520similarity%2520calculation%2520alongside%2520the%2520augmented%2520node%250Afeatures%252C%2520and%25203%2529%2520gradual%2520NA%2520with%2520similarity%2520calculation%2520using%2520aligned%250Across-network%2520neighbor-pairs%2520%2528ACNs%2529.%2520Through%2520comprehensive%2520experiments%252C%2520we%250Ademonstrate%2520that%2520Grad-Align%252B%2520exhibits%2520%2528a%2529%2520the%2520superiority%2520over%2520benchmark%2520NA%250Amethods%252C%2520%2528b%2529%2520empirical%2520validations%2520as%2520well%2520as%2520our%2520theoretical%2520findings%2520to%2520see%250Athe%2520effectiveness%2520of%2520CNFA%252C%2520%2528c%2529%2520the%2520influence%2520of%2520each%2520component%252C%2520%2528d%2529%2520the%250Arobustness%2520to%2520network%2520noises%252C%2520and%2520%2528e%2529%2520the%2520computational%2520efficiency.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2304.12751v4%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Node%20Feature%20Augmentation%20Vitaminizes%20Network%20Alignment&entry.906535625=Jin-Duk%20Park%20and%20Cong%20Tran%20and%20Won-Yong%20Shin%20and%20Xin%20Cao&entry.1292438233=%20%20Network%20alignment%20%28NA%29%20is%20the%20task%20of%20discovering%20node%20correspondences%20across%0Amultiple%20networks.%20Although%20NA%20methods%20have%20achieved%20remarkable%20success%20in%20a%0Amyriad%20of%20scenarios%2C%20their%20effectiveness%20is%20not%20without%20additional%20information%0Asuch%20as%20prior%20anchor%20links%20and/or%20node%20features%2C%20which%20may%20not%20always%20be%0Aavailable%20due%20to%20privacy%20concerns%20or%20access%20restrictions.%20To%20tackle%20this%0Achallenge%2C%20we%20propose%20Grad-Align%2B%2C%20a%20novel%20NA%20method%20built%20upon%20a%20recent%0Astate-of-the-art%20NA%20method%2C%20the%20so-called%20Grad-Align%2C%20that%20gradually%20discovers%0Aa%20part%20of%20node%20pairs%20until%20all%20node%20pairs%20are%20found.%20In%20designing%20Grad-Align%2B%2C%0Awe%20account%20for%20how%20to%20augment%20node%20features%20in%20the%20sense%20of%20performing%20the%20NA%0Atask%20and%20how%20to%20design%20our%20NA%20method%20by%20maximally%20exploiting%20the%20augmented%20node%0Afeatures.%20To%20achieve%20this%20goal%2C%20Grad-Align%2B%20consists%20of%20three%20key%20components%3A%0A1%29%20centrality-based%20node%20feature%20augmentation%20%28CNFA%29%2C%202%29%20graph%20neural%20network%0A%28GNN%29-aided%20embedding%20similarity%20calculation%20alongside%20the%20augmented%20node%0Afeatures%2C%20and%203%29%20gradual%20NA%20with%20similarity%20calculation%20using%20aligned%0Across-network%20neighbor-pairs%20%28ACNs%29.%20Through%20comprehensive%20experiments%2C%20we%0Ademonstrate%20that%20Grad-Align%2B%20exhibits%20%28a%29%20the%20superiority%20over%20benchmark%20NA%0Amethods%2C%20%28b%29%20empirical%20validations%20as%20well%20as%20our%20theoretical%20findings%20to%20see%0Athe%20effectiveness%20of%20CNFA%2C%20%28c%29%20the%20influence%20of%20each%20component%2C%20%28d%29%20the%0Arobustness%20to%20network%20noises%2C%20and%20%28e%29%20the%20computational%20efficiency.%0A&entry.1838667208=http%3A//arxiv.org/abs/2304.12751v4&entry.124074799=Read"},
{"title": "Open-Vocabulary Spatio-Temporal Action Detection", "author": "Tao Wu and Shuqiu Ge and Jie Qin and Gangshan Wu and Limin Wang", "abstract": "  Spatio-temporal action detection (STAD) is an important fine-grained video\nunderstanding task. Current methods require box and label supervision for all\naction classes in advance. However, in real-world applications, it is very\nlikely to come across new action classes not seen in training because the\naction category space is large and hard to enumerate. Also, the cost of data\nannotation and model training for new classes is extremely high for traditional\nmethods, as we need to perform detailed box annotations and re-train the whole\nnetwork from scratch. In this paper, we propose a new challenging setting by\nperforming open-vocabulary STAD to better mimic the situation of action\ndetection in an open world. Open-vocabulary spatio-temporal action detection\n(OV-STAD) requires training a model on a limited set of base classes with box\nand label supervision, which is expected to yield good generalization\nperformance on novel action classes. For OV-STAD, we build two benchmarks based\non the existing STAD datasets and propose a simple but effective method based\non pretrained video-language models (VLM). To better adapt the holistic VLM for\nthe fine-grained action detection task, we carefully fine-tune it on the\nlocalized video region-text pairs. This customized fine-tuning endows the VLM\nwith better motion understanding, thus contributing to a more accurate\nalignment between video regions and texts. Local region feature and global\nvideo feature fusion before alignment is adopted to further improve the action\ndetection performance by providing global context. Our method achieves a\npromising performance on novel classes.\n", "link": "http://arxiv.org/abs/2405.10832v1", "date": "2024-05-17", "relevancy": 2.2957, "topK": [{"title": "Weakly-guided self-supervised pretraining for temporal activity detection", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/25189", "similarity": 0.6404}, {"title": "Customize-A-Video: One-Shot Motion Customization of Text-to-Video\n  Diffusion Models", "link": "http://arxiv.org/abs/2402.14780v1", "similarity": 0.5376}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.522}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Open-Vocabulary%20Spatio-Temporal%20Action%20Detection&body=Title%3A%20Open-Vocabulary%20Spatio-Temporal%20Action%20Detection%0AAuthor%3A%20Tao%20Wu%20and%20Shuqiu%20Ge%20and%20Jie%20Qin%20and%20Gangshan%20Wu%20and%20Limin%20Wang%0AAbstract%3A%20%20%20Spatio-temporal%20action%20detection%20%28STAD%29%20is%20an%20important%20fine-grained%20video%0Aunderstanding%20task.%20Current%20methods%20require%20box%20and%20label%20supervision%20for%20all%0Aaction%20classes%20in%20advance.%20However%2C%20in%20real-world%20applications%2C%20it%20is%20very%0Alikely%20to%20come%20across%20new%20action%20classes%20not%20seen%20in%20training%20because%20the%0Aaction%20category%20space%20is%20large%20and%20hard%20to%20enumerate.%20Also%2C%20the%20cost%20of%20data%0Aannotation%20and%20model%20training%20for%20new%20classes%20is%20extremely%20high%20for%20traditional%0Amethods%2C%20as%20we%20need%20to%20perform%20detailed%20box%20annotations%20and%20re-train%20the%20whole%0Anetwork%20from%20scratch.%20In%20this%20paper%2C%20we%20propose%20a%20new%20challenging%20setting%20by%0Aperforming%20open-vocabulary%20STAD%20to%20better%20mimic%20the%20situation%20of%20action%0Adetection%20in%20an%20open%20world.%20Open-vocabulary%20spatio-temporal%20action%20detection%0A%28OV-STAD%29%20requires%20training%20a%20model%20on%20a%20limited%20set%20of%20base%20classes%20with%20box%0Aand%20label%20supervision%2C%20which%20is%20expected%20to%20yield%20good%20generalization%0Aperformance%20on%20novel%20action%20classes.%20For%20OV-STAD%2C%20we%20build%20two%20benchmarks%20based%0Aon%20the%20existing%20STAD%20datasets%20and%20propose%20a%20simple%20but%20effective%20method%20based%0Aon%20pretrained%20video-language%20models%20%28VLM%29.%20To%20better%20adapt%20the%20holistic%20VLM%20for%0Athe%20fine-grained%20action%20detection%20task%2C%20we%20carefully%20fine-tune%20it%20on%20the%0Alocalized%20video%20region-text%20pairs.%20This%20customized%20fine-tuning%20endows%20the%20VLM%0Awith%20better%20motion%20understanding%2C%20thus%20contributing%20to%20a%20more%20accurate%0Aalignment%20between%20video%20regions%20and%20texts.%20Local%20region%20feature%20and%20global%0Avideo%20feature%20fusion%20before%20alignment%20is%20adopted%20to%20further%20improve%20the%20action%0Adetection%20performance%20by%20providing%20global%20context.%20Our%20method%20achieves%20a%0Apromising%20performance%20on%20novel%20classes.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2405.10832v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DOpen-Vocabulary%2520Spatio-Temporal%2520Action%2520Detection%26entry.906535625%3DTao%2520Wu%2520and%2520Shuqiu%2520Ge%2520and%2520Jie%2520Qin%2520and%2520Gangshan%2520Wu%2520and%2520Limin%2520Wang%26entry.1292438233%3D%2520%2520Spatio-temporal%2520action%2520detection%2520%2528STAD%2529%2520is%2520an%2520important%2520fine-grained%2520video%250Aunderstanding%2520task.%2520Current%2520methods%2520require%2520box%2520and%2520label%2520supervision%2520for%2520all%250Aaction%2520classes%2520in%2520advance.%2520However%252C%2520in%2520real-world%2520applications%252C%2520it%2520is%2520very%250Alikely%2520to%2520come%2520across%2520new%2520action%2520classes%2520not%2520seen%2520in%2520training%2520because%2520the%250Aaction%2520category%2520space%2520is%2520large%2520and%2520hard%2520to%2520enumerate.%2520Also%252C%2520the%2520cost%2520of%2520data%250Aannotation%2520and%2520model%2520training%2520for%2520new%2520classes%2520is%2520extremely%2520high%2520for%2520traditional%250Amethods%252C%2520as%2520we%2520need%2520to%2520perform%2520detailed%2520box%2520annotations%2520and%2520re-train%2520the%2520whole%250Anetwork%2520from%2520scratch.%2520In%2520this%2520paper%252C%2520we%2520propose%2520a%2520new%2520challenging%2520setting%2520by%250Aperforming%2520open-vocabulary%2520STAD%2520to%2520better%2520mimic%2520the%2520situation%2520of%2520action%250Adetection%2520in%2520an%2520open%2520world.%2520Open-vocabulary%2520spatio-temporal%2520action%2520detection%250A%2528OV-STAD%2529%2520requires%2520training%2520a%2520model%2520on%2520a%2520limited%2520set%2520of%2520base%2520classes%2520with%2520box%250Aand%2520label%2520supervision%252C%2520which%2520is%2520expected%2520to%2520yield%2520good%2520generalization%250Aperformance%2520on%2520novel%2520action%2520classes.%2520For%2520OV-STAD%252C%2520we%2520build%2520two%2520benchmarks%2520based%250Aon%2520the%2520existing%2520STAD%2520datasets%2520and%2520propose%2520a%2520simple%2520but%2520effective%2520method%2520based%250Aon%2520pretrained%2520video-language%2520models%2520%2528VLM%2529.%2520To%2520better%2520adapt%2520the%2520holistic%2520VLM%2520for%250Athe%2520fine-grained%2520action%2520detection%2520task%252C%2520we%2520carefully%2520fine-tune%2520it%2520on%2520the%250Alocalized%2520video%2520region-text%2520pairs.%2520This%2520customized%2520fine-tuning%2520endows%2520the%2520VLM%250Awith%2520better%2520motion%2520understanding%252C%2520thus%2520contributing%2520to%2520a%2520more%2520accurate%250Aalignment%2520between%2520video%2520regions%2520and%2520texts.%2520Local%2520region%2520feature%2520and%2520global%250Avideo%2520feature%2520fusion%2520before%2520alignment%2520is%2520adopted%2520to%2520further%2520improve%2520the%2520action%250Adetection%2520performance%2520by%2520providing%2520global%2520context.%2520Our%2520method%2520achieves%2520a%250Apromising%2520performance%2520on%2520novel%2520classes.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2405.10832v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Open-Vocabulary%20Spatio-Temporal%20Action%20Detection&entry.906535625=Tao%20Wu%20and%20Shuqiu%20Ge%20and%20Jie%20Qin%20and%20Gangshan%20Wu%20and%20Limin%20Wang&entry.1292438233=%20%20Spatio-temporal%20action%20detection%20%28STAD%29%20is%20an%20important%20fine-grained%20video%0Aunderstanding%20task.%20Current%20methods%20require%20box%20and%20label%20supervision%20for%20all%0Aaction%20classes%20in%20advance.%20However%2C%20in%20real-world%20applications%2C%20it%20is%20very%0Alikely%20to%20come%20across%20new%20action%20classes%20not%20seen%20in%20training%20because%20the%0Aaction%20category%20space%20is%20large%20and%20hard%20to%20enumerate.%20Also%2C%20the%20cost%20of%20data%0Aannotation%20and%20model%20training%20for%20new%20classes%20is%20extremely%20high%20for%20traditional%0Amethods%2C%20as%20we%20need%20to%20perform%20detailed%20box%20annotations%20and%20re-train%20the%20whole%0Anetwork%20from%20scratch.%20In%20this%20paper%2C%20we%20propose%20a%20new%20challenging%20setting%20by%0Aperforming%20open-vocabulary%20STAD%20to%20better%20mimic%20the%20situation%20of%20action%0Adetection%20in%20an%20open%20world.%20Open-vocabulary%20spatio-temporal%20action%20detection%0A%28OV-STAD%29%20requires%20training%20a%20model%20on%20a%20limited%20set%20of%20base%20classes%20with%20box%0Aand%20label%20supervision%2C%20which%20is%20expected%20to%20yield%20good%20generalization%0Aperformance%20on%20novel%20action%20classes.%20For%20OV-STAD%2C%20we%20build%20two%20benchmarks%20based%0Aon%20the%20existing%20STAD%20datasets%20and%20propose%20a%20simple%20but%20effective%20method%20based%0Aon%20pretrained%20video-language%20models%20%28VLM%29.%20To%20better%20adapt%20the%20holistic%20VLM%20for%0Athe%20fine-grained%20action%20detection%20task%2C%20we%20carefully%20fine-tune%20it%20on%20the%0Alocalized%20video%20region-text%20pairs.%20This%20customized%20fine-tuning%20endows%20the%20VLM%0Awith%20better%20motion%20understanding%2C%20thus%20contributing%20to%20a%20more%20accurate%0Aalignment%20between%20video%20regions%20and%20texts.%20Local%20region%20feature%20and%20global%0Avideo%20feature%20fusion%20before%20alignment%20is%20adopted%20to%20further%20improve%20the%20action%0Adetection%20performance%20by%20providing%20global%20context.%20Our%20method%20achieves%20a%0Apromising%20performance%20on%20novel%20classes.%0A&entry.1838667208=http%3A//arxiv.org/abs/2405.10832v1&entry.124074799=Read"},
{"title": "Rethinking Graph Backdoor Attacks: A Distribution-Preserving Perspective", "author": "Zhiwei Zhang and Minhua Lin and Enyan Dai and Suhang Wang", "abstract": "  Graph Neural Networks (GNNs) have shown remarkable performance in various\ntasks. However, recent works reveal that GNNs are vulnerable to backdoor\nattacks. Generally, backdoor attack poisons the graph by attaching backdoor\ntriggers and the target class label to a set of nodes in the training graph. A\nGNN trained on the poisoned graph will then be misled to predict test nodes\nattached with trigger to the target class. Despite their effectiveness, our\nempirical analysis shows that triggers generated by existing methods tend to be\nout-of-distribution (OOD), which significantly differ from the clean data.\nHence, these injected triggers can be easily detected and pruned with widely\nused outlier detection methods in real-world applications. Therefore, in this\npaper, we study a novel problem of unnoticeable graph backdoor attacks with\nin-distribution (ID) triggers. To generate ID triggers, we introduce an OOD\ndetector in conjunction with an adversarial learning strategy to generate the\nattributes of the triggers within distribution. To ensure a high attack success\nrate with ID triggers, we introduce novel modules designed to enhance trigger\nmemorization by the victim model trained on poisoned graph. Extensive\nexperiments on real-world datasets demonstrate the effectiveness of the\nproposed method in generating in distribution triggers that can by-pass various\ndefense strategies while maintaining a high attack success rate.\n", "link": "http://arxiv.org/abs/2405.10757v1", "date": "2024-05-17", "relevancy": 2.2655, "topK": [{"title": "Boosted dynamic neural networks", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/26302", "similarity": 0.485}, {"title": "ToDo: Token Downsampling for Efficient Generation of High-Resolution\n  Images", "link": "http://arxiv.org/abs/2402.13573v2", "similarity": 0.4423}, {"title": "Self-supervised Photographic Image Layout Representation Learning", "link": "http://arxiv.org/abs/2403.03740v1", "similarity": 0.4321}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Rethinking%20Graph%20Backdoor%20Attacks%3A%20A%20Distribution-Preserving%20Perspective&body=Title%3A%20Rethinking%20Graph%20Backdoor%20Attacks%3A%20A%20Distribution-Preserving%20Perspective%0AAuthor%3A%20Zhiwei%20Zhang%20and%20Minhua%20Lin%20and%20Enyan%20Dai%20and%20Suhang%20Wang%0AAbstract%3A%20%20%20Graph%20Neural%20Networks%20%28GNNs%29%20have%20shown%20remarkable%20performance%20in%20various%0Atasks.%20However%2C%20recent%20works%20reveal%20that%20GNNs%20are%20vulnerable%20to%20backdoor%0Aattacks.%20Generally%2C%20backdoor%20attack%20poisons%20the%20graph%20by%20attaching%20backdoor%0Atriggers%20and%20the%20target%20class%20label%20to%20a%20set%20of%20nodes%20in%20the%20training%20graph.%20A%0AGNN%20trained%20on%20the%20poisoned%20graph%20will%20then%20be%20misled%20to%20predict%20test%20nodes%0Aattached%20with%20trigger%20to%20the%20target%20class.%20Despite%20their%20effectiveness%2C%20our%0Aempirical%20analysis%20shows%20that%20triggers%20generated%20by%20existing%20methods%20tend%20to%20be%0Aout-of-distribution%20%28OOD%29%2C%20which%20significantly%20differ%20from%20the%20clean%20data.%0AHence%2C%20these%20injected%20triggers%20can%20be%20easily%20detected%20and%20pruned%20with%20widely%0Aused%20outlier%20detection%20methods%20in%20real-world%20applications.%20Therefore%2C%20in%20this%0Apaper%2C%20we%20study%20a%20novel%20problem%20of%20unnoticeable%20graph%20backdoor%20attacks%20with%0Ain-distribution%20%28ID%29%20triggers.%20To%20generate%20ID%20triggers%2C%20we%20introduce%20an%20OOD%0Adetector%20in%20conjunction%20with%20an%20adversarial%20learning%20strategy%20to%20generate%20the%0Aattributes%20of%20the%20triggers%20within%20distribution.%20To%20ensure%20a%20high%20attack%20success%0Arate%20with%20ID%20triggers%2C%20we%20introduce%20novel%20modules%20designed%20to%20enhance%20trigger%0Amemorization%20by%20the%20victim%20model%20trained%20on%20poisoned%20graph.%20Extensive%0Aexperiments%20on%20real-world%20datasets%20demonstrate%20the%20effectiveness%20of%20the%0Aproposed%20method%20in%20generating%20in%20distribution%20triggers%20that%20can%20by-pass%20various%0Adefense%20strategies%20while%20maintaining%20a%20high%20attack%20success%20rate.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2405.10757v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DRethinking%2520Graph%2520Backdoor%2520Attacks%253A%2520A%2520Distribution-Preserving%2520Perspective%26entry.906535625%3DZhiwei%2520Zhang%2520and%2520Minhua%2520Lin%2520and%2520Enyan%2520Dai%2520and%2520Suhang%2520Wang%26entry.1292438233%3D%2520%2520Graph%2520Neural%2520Networks%2520%2528GNNs%2529%2520have%2520shown%2520remarkable%2520performance%2520in%2520various%250Atasks.%2520However%252C%2520recent%2520works%2520reveal%2520that%2520GNNs%2520are%2520vulnerable%2520to%2520backdoor%250Aattacks.%2520Generally%252C%2520backdoor%2520attack%2520poisons%2520the%2520graph%2520by%2520attaching%2520backdoor%250Atriggers%2520and%2520the%2520target%2520class%2520label%2520to%2520a%2520set%2520of%2520nodes%2520in%2520the%2520training%2520graph.%2520A%250AGNN%2520trained%2520on%2520the%2520poisoned%2520graph%2520will%2520then%2520be%2520misled%2520to%2520predict%2520test%2520nodes%250Aattached%2520with%2520trigger%2520to%2520the%2520target%2520class.%2520Despite%2520their%2520effectiveness%252C%2520our%250Aempirical%2520analysis%2520shows%2520that%2520triggers%2520generated%2520by%2520existing%2520methods%2520tend%2520to%2520be%250Aout-of-distribution%2520%2528OOD%2529%252C%2520which%2520significantly%2520differ%2520from%2520the%2520clean%2520data.%250AHence%252C%2520these%2520injected%2520triggers%2520can%2520be%2520easily%2520detected%2520and%2520pruned%2520with%2520widely%250Aused%2520outlier%2520detection%2520methods%2520in%2520real-world%2520applications.%2520Therefore%252C%2520in%2520this%250Apaper%252C%2520we%2520study%2520a%2520novel%2520problem%2520of%2520unnoticeable%2520graph%2520backdoor%2520attacks%2520with%250Ain-distribution%2520%2528ID%2529%2520triggers.%2520To%2520generate%2520ID%2520triggers%252C%2520we%2520introduce%2520an%2520OOD%250Adetector%2520in%2520conjunction%2520with%2520an%2520adversarial%2520learning%2520strategy%2520to%2520generate%2520the%250Aattributes%2520of%2520the%2520triggers%2520within%2520distribution.%2520To%2520ensure%2520a%2520high%2520attack%2520success%250Arate%2520with%2520ID%2520triggers%252C%2520we%2520introduce%2520novel%2520modules%2520designed%2520to%2520enhance%2520trigger%250Amemorization%2520by%2520the%2520victim%2520model%2520trained%2520on%2520poisoned%2520graph.%2520Extensive%250Aexperiments%2520on%2520real-world%2520datasets%2520demonstrate%2520the%2520effectiveness%2520of%2520the%250Aproposed%2520method%2520in%2520generating%2520in%2520distribution%2520triggers%2520that%2520can%2520by-pass%2520various%250Adefense%2520strategies%2520while%2520maintaining%2520a%2520high%2520attack%2520success%2520rate.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2405.10757v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Rethinking%20Graph%20Backdoor%20Attacks%3A%20A%20Distribution-Preserving%20Perspective&entry.906535625=Zhiwei%20Zhang%20and%20Minhua%20Lin%20and%20Enyan%20Dai%20and%20Suhang%20Wang&entry.1292438233=%20%20Graph%20Neural%20Networks%20%28GNNs%29%20have%20shown%20remarkable%20performance%20in%20various%0Atasks.%20However%2C%20recent%20works%20reveal%20that%20GNNs%20are%20vulnerable%20to%20backdoor%0Aattacks.%20Generally%2C%20backdoor%20attack%20poisons%20the%20graph%20by%20attaching%20backdoor%0Atriggers%20and%20the%20target%20class%20label%20to%20a%20set%20of%20nodes%20in%20the%20training%20graph.%20A%0AGNN%20trained%20on%20the%20poisoned%20graph%20will%20then%20be%20misled%20to%20predict%20test%20nodes%0Aattached%20with%20trigger%20to%20the%20target%20class.%20Despite%20their%20effectiveness%2C%20our%0Aempirical%20analysis%20shows%20that%20triggers%20generated%20by%20existing%20methods%20tend%20to%20be%0Aout-of-distribution%20%28OOD%29%2C%20which%20significantly%20differ%20from%20the%20clean%20data.%0AHence%2C%20these%20injected%20triggers%20can%20be%20easily%20detected%20and%20pruned%20with%20widely%0Aused%20outlier%20detection%20methods%20in%20real-world%20applications.%20Therefore%2C%20in%20this%0Apaper%2C%20we%20study%20a%20novel%20problem%20of%20unnoticeable%20graph%20backdoor%20attacks%20with%0Ain-distribution%20%28ID%29%20triggers.%20To%20generate%20ID%20triggers%2C%20we%20introduce%20an%20OOD%0Adetector%20in%20conjunction%20with%20an%20adversarial%20learning%20strategy%20to%20generate%20the%0Aattributes%20of%20the%20triggers%20within%20distribution.%20To%20ensure%20a%20high%20attack%20success%0Arate%20with%20ID%20triggers%2C%20we%20introduce%20novel%20modules%20designed%20to%20enhance%20trigger%0Amemorization%20by%20the%20victim%20model%20trained%20on%20poisoned%20graph.%20Extensive%0Aexperiments%20on%20real-world%20datasets%20demonstrate%20the%20effectiveness%20of%20the%0Aproposed%20method%20in%20generating%20in%20distribution%20triggers%20that%20can%20by-pass%20various%0Adefense%20strategies%20while%20maintaining%20a%20high%20attack%20success%20rate.%0A&entry.1838667208=http%3A//arxiv.org/abs/2405.10757v1&entry.124074799=Read"},
{"title": "Vision Beyond Boundaries: An Initial Design Space of Domain-specific\n  Large Vision Models in Human-robot Interaction", "author": "Yuchong Zhang and Yong Ma and Danica Kragic", "abstract": "  The emergence of Large Vision Models (LVMs) is following in the footsteps of\nthe recent prosperity of Large Language Models (LLMs) in following years.\nHowever, there's a noticeable gap in structured research applying LVMs to\nHuman-Robot Interaction (HRI), despite extensive evidence supporting the\nefficacy of vision models in enhancing interactions between humans and robots.\nRecognizing the vast and anticipated potential, we introduce an initial design\nspace that incorporates domain-specific LVMs, chosen for their superior\nperformance over normal models. We delve into three primary dimensions: HRI\ncontexts, vision-based tasks, and specific domains. The empirical validation\nwas implemented among 15 experts across six evaluated metrics, showcasing the\nprimary efficacy in relevant decision-making scenarios. We explore the process\nof ideation and potential application scenarios, envisioning this design space\nas a foundational guideline for future HRI system design, emphasizing accurate\ndomain alignment and model selection.\n", "link": "http://arxiv.org/abs/2404.14965v2", "date": "2024-05-17", "relevancy": 2.2433, "topK": [{"title": "VirtualModel: Generating Object-ID-retentive Human-object Interaction\n  Image by Diffusion Model for E-commerce Marketing", "link": "http://arxiv.org/abs/2405.09985v1", "similarity": 0.5678}, {"title": "RoboEXP: Action-Conditioned Scene Graph via Interactive Exploration for\n  Robotic Manipulation", "link": "http://arxiv.org/abs/2402.15487v1", "similarity": 0.5608}, {"title": "Egocentric Computer Vision for Hands-Free Robotic Wheelchair Navigation", "link": "https://link.springer.com/article/10.1007/s10846-023-01807-4", "similarity": 0.5538}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Vision%20Beyond%20Boundaries%3A%20An%20Initial%20Design%20Space%20of%20Domain-specific%0A%20%20Large%20Vision%20Models%20in%20Human-robot%20Interaction&body=Title%3A%20Vision%20Beyond%20Boundaries%3A%20An%20Initial%20Design%20Space%20of%20Domain-specific%0A%20%20Large%20Vision%20Models%20in%20Human-robot%20Interaction%0AAuthor%3A%20Yuchong%20Zhang%20and%20Yong%20Ma%20and%20Danica%20Kragic%0AAbstract%3A%20%20%20The%20emergence%20of%20Large%20Vision%20Models%20%28LVMs%29%20is%20following%20in%20the%20footsteps%20of%0Athe%20recent%20prosperity%20of%20Large%20Language%20Models%20%28LLMs%29%20in%20following%20years.%0AHowever%2C%20there%27s%20a%20noticeable%20gap%20in%20structured%20research%20applying%20LVMs%20to%0AHuman-Robot%20Interaction%20%28HRI%29%2C%20despite%20extensive%20evidence%20supporting%20the%0Aefficacy%20of%20vision%20models%20in%20enhancing%20interactions%20between%20humans%20and%20robots.%0ARecognizing%20the%20vast%20and%20anticipated%20potential%2C%20we%20introduce%20an%20initial%20design%0Aspace%20that%20incorporates%20domain-specific%20LVMs%2C%20chosen%20for%20their%20superior%0Aperformance%20over%20normal%20models.%20We%20delve%20into%20three%20primary%20dimensions%3A%20HRI%0Acontexts%2C%20vision-based%20tasks%2C%20and%20specific%20domains.%20The%20empirical%20validation%0Awas%20implemented%20among%2015%20experts%20across%20six%20evaluated%20metrics%2C%20showcasing%20the%0Aprimary%20efficacy%20in%20relevant%20decision-making%20scenarios.%20We%20explore%20the%20process%0Aof%20ideation%20and%20potential%20application%20scenarios%2C%20envisioning%20this%20design%20space%0Aas%20a%20foundational%20guideline%20for%20future%20HRI%20system%20design%2C%20emphasizing%20accurate%0Adomain%20alignment%20and%20model%20selection.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2404.14965v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DVision%2520Beyond%2520Boundaries%253A%2520An%2520Initial%2520Design%2520Space%2520of%2520Domain-specific%250A%2520%2520Large%2520Vision%2520Models%2520in%2520Human-robot%2520Interaction%26entry.906535625%3DYuchong%2520Zhang%2520and%2520Yong%2520Ma%2520and%2520Danica%2520Kragic%26entry.1292438233%3D%2520%2520The%2520emergence%2520of%2520Large%2520Vision%2520Models%2520%2528LVMs%2529%2520is%2520following%2520in%2520the%2520footsteps%2520of%250Athe%2520recent%2520prosperity%2520of%2520Large%2520Language%2520Models%2520%2528LLMs%2529%2520in%2520following%2520years.%250AHowever%252C%2520there%2527s%2520a%2520noticeable%2520gap%2520in%2520structured%2520research%2520applying%2520LVMs%2520to%250AHuman-Robot%2520Interaction%2520%2528HRI%2529%252C%2520despite%2520extensive%2520evidence%2520supporting%2520the%250Aefficacy%2520of%2520vision%2520models%2520in%2520enhancing%2520interactions%2520between%2520humans%2520and%2520robots.%250ARecognizing%2520the%2520vast%2520and%2520anticipated%2520potential%252C%2520we%2520introduce%2520an%2520initial%2520design%250Aspace%2520that%2520incorporates%2520domain-specific%2520LVMs%252C%2520chosen%2520for%2520their%2520superior%250Aperformance%2520over%2520normal%2520models.%2520We%2520delve%2520into%2520three%2520primary%2520dimensions%253A%2520HRI%250Acontexts%252C%2520vision-based%2520tasks%252C%2520and%2520specific%2520domains.%2520The%2520empirical%2520validation%250Awas%2520implemented%2520among%252015%2520experts%2520across%2520six%2520evaluated%2520metrics%252C%2520showcasing%2520the%250Aprimary%2520efficacy%2520in%2520relevant%2520decision-making%2520scenarios.%2520We%2520explore%2520the%2520process%250Aof%2520ideation%2520and%2520potential%2520application%2520scenarios%252C%2520envisioning%2520this%2520design%2520space%250Aas%2520a%2520foundational%2520guideline%2520for%2520future%2520HRI%2520system%2520design%252C%2520emphasizing%2520accurate%250Adomain%2520alignment%2520and%2520model%2520selection.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2404.14965v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Vision%20Beyond%20Boundaries%3A%20An%20Initial%20Design%20Space%20of%20Domain-specific%0A%20%20Large%20Vision%20Models%20in%20Human-robot%20Interaction&entry.906535625=Yuchong%20Zhang%20and%20Yong%20Ma%20and%20Danica%20Kragic&entry.1292438233=%20%20The%20emergence%20of%20Large%20Vision%20Models%20%28LVMs%29%20is%20following%20in%20the%20footsteps%20of%0Athe%20recent%20prosperity%20of%20Large%20Language%20Models%20%28LLMs%29%20in%20following%20years.%0AHowever%2C%20there%27s%20a%20noticeable%20gap%20in%20structured%20research%20applying%20LVMs%20to%0AHuman-Robot%20Interaction%20%28HRI%29%2C%20despite%20extensive%20evidence%20supporting%20the%0Aefficacy%20of%20vision%20models%20in%20enhancing%20interactions%20between%20humans%20and%20robots.%0ARecognizing%20the%20vast%20and%20anticipated%20potential%2C%20we%20introduce%20an%20initial%20design%0Aspace%20that%20incorporates%20domain-specific%20LVMs%2C%20chosen%20for%20their%20superior%0Aperformance%20over%20normal%20models.%20We%20delve%20into%20three%20primary%20dimensions%3A%20HRI%0Acontexts%2C%20vision-based%20tasks%2C%20and%20specific%20domains.%20The%20empirical%20validation%0Awas%20implemented%20among%2015%20experts%20across%20six%20evaluated%20metrics%2C%20showcasing%20the%0Aprimary%20efficacy%20in%20relevant%20decision-making%20scenarios.%20We%20explore%20the%20process%0Aof%20ideation%20and%20potential%20application%20scenarios%2C%20envisioning%20this%20design%20space%0Aas%20a%20foundational%20guideline%20for%20future%20HRI%20system%20design%2C%20emphasizing%20accurate%0Adomain%20alignment%20and%20model%20selection.%0A&entry.1838667208=http%3A//arxiv.org/abs/2404.14965v2&entry.124074799=Read"},
{"title": "3D Vessel Reconstruction from Sparse-View Dynamic DSA Images via Vessel\n  Probability Guided Attenuation Learning", "author": "Zhentao Liu and Huangxuan Zhao and Wenhui Qin and Zhenghong Zhou and Xinggang Wang and Wenping Wang and Xiaochun Lai and Chuansheng Zheng and Dinggang Shen and Zhiming Cui", "abstract": "  Digital Subtraction Angiography (DSA) is one of the gold standards in\nvascular disease diagnosing. With the help of contrast agent, time-resolved 2D\nDSA images deliver comprehensive insights into blood flow information and can\nbe utilized to reconstruct 3D vessel structures. Current commercial DSA systems\ntypically demand hundreds of scanning views to perform reconstruction,\nresulting in substantial radiation exposure. However, sparse-view DSA\nreconstruction, aimed at reducing radiation dosage, is still underexplored in\nthe research community. The dynamic blood flow and insufficient input of\nsparse-view DSA images present significant challenges to the 3D vessel\nreconstruction task. In this study, we propose to use a time-agnostic vessel\nprobability field to solve this problem effectively. Our approach, termed as\nvessel probability guided attenuation learning, represents the DSA imaging as a\ncomplementary weighted combination of static and dynamic attenuation fields,\nwith the weights derived from the vessel probability field. Functioning as a\ndynamic mask, vessel probability provides proper gradients for both static and\ndynamic fields adaptive to different scene types. This mechanism facilitates a\nself-supervised decomposition between static backgrounds and dynamic contrast\nagent flow, and significantly improves the reconstruction quality. Our model is\ntrained by minimizing the disparity between synthesized projections and real\ncaptured DSA images. We further employ two training strategies to improve our\nreconstruction quality: (1) coarse-to-fine progressive training to achieve\nbetter geometry and (2) temporal perturbed rendering loss to enforce temporal\nconsistency. Experimental results have demonstrated superior quality on both 3D\nvessel reconstruction and 2D view synthesis.\n", "link": "http://arxiv.org/abs/2405.10705v1", "date": "2024-05-17", "relevancy": 2.2365, "topK": [{"title": "SDXS: Real-Time One-Step Latent Diffusion Models with Image Conditions", "link": "http://arxiv.org/abs/2403.16627v1", "similarity": 0.5677}, {"title": "Dynamic Gaussians Mesh: Consistent Mesh Reconstruction from Monocular\n  Videos", "link": "http://arxiv.org/abs/2404.12379v2", "similarity": 0.5583}, {"title": "CAT3D: Create Anything in 3D with Multi-View Diffusion Models", "link": "http://arxiv.org/abs/2405.10314v1", "similarity": 0.5565}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%203D%20Vessel%20Reconstruction%20from%20Sparse-View%20Dynamic%20DSA%20Images%20via%20Vessel%0A%20%20Probability%20Guided%20Attenuation%20Learning&body=Title%3A%203D%20Vessel%20Reconstruction%20from%20Sparse-View%20Dynamic%20DSA%20Images%20via%20Vessel%0A%20%20Probability%20Guided%20Attenuation%20Learning%0AAuthor%3A%20Zhentao%20Liu%20and%20Huangxuan%20Zhao%20and%20Wenhui%20Qin%20and%20Zhenghong%20Zhou%20and%20Xinggang%20Wang%20and%20Wenping%20Wang%20and%20Xiaochun%20Lai%20and%20Chuansheng%20Zheng%20and%20Dinggang%20Shen%20and%20Zhiming%20Cui%0AAbstract%3A%20%20%20Digital%20Subtraction%20Angiography%20%28DSA%29%20is%20one%20of%20the%20gold%20standards%20in%0Avascular%20disease%20diagnosing.%20With%20the%20help%20of%20contrast%20agent%2C%20time-resolved%202D%0ADSA%20images%20deliver%20comprehensive%20insights%20into%20blood%20flow%20information%20and%20can%0Abe%20utilized%20to%20reconstruct%203D%20vessel%20structures.%20Current%20commercial%20DSA%20systems%0Atypically%20demand%20hundreds%20of%20scanning%20views%20to%20perform%20reconstruction%2C%0Aresulting%20in%20substantial%20radiation%20exposure.%20However%2C%20sparse-view%20DSA%0Areconstruction%2C%20aimed%20at%20reducing%20radiation%20dosage%2C%20is%20still%20underexplored%20in%0Athe%20research%20community.%20The%20dynamic%20blood%20flow%20and%20insufficient%20input%20of%0Asparse-view%20DSA%20images%20present%20significant%20challenges%20to%20the%203D%20vessel%0Areconstruction%20task.%20In%20this%20study%2C%20we%20propose%20to%20use%20a%20time-agnostic%20vessel%0Aprobability%20field%20to%20solve%20this%20problem%20effectively.%20Our%20approach%2C%20termed%20as%0Avessel%20probability%20guided%20attenuation%20learning%2C%20represents%20the%20DSA%20imaging%20as%20a%0Acomplementary%20weighted%20combination%20of%20static%20and%20dynamic%20attenuation%20fields%2C%0Awith%20the%20weights%20derived%20from%20the%20vessel%20probability%20field.%20Functioning%20as%20a%0Adynamic%20mask%2C%20vessel%20probability%20provides%20proper%20gradients%20for%20both%20static%20and%0Adynamic%20fields%20adaptive%20to%20different%20scene%20types.%20This%20mechanism%20facilitates%20a%0Aself-supervised%20decomposition%20between%20static%20backgrounds%20and%20dynamic%20contrast%0Aagent%20flow%2C%20and%20significantly%20improves%20the%20reconstruction%20quality.%20Our%20model%20is%0Atrained%20by%20minimizing%20the%20disparity%20between%20synthesized%20projections%20and%20real%0Acaptured%20DSA%20images.%20We%20further%20employ%20two%20training%20strategies%20to%20improve%20our%0Areconstruction%20quality%3A%20%281%29%20coarse-to-fine%20progressive%20training%20to%20achieve%0Abetter%20geometry%20and%20%282%29%20temporal%20perturbed%20rendering%20loss%20to%20enforce%20temporal%0Aconsistency.%20Experimental%20results%20have%20demonstrated%20superior%20quality%20on%20both%203D%0Avessel%20reconstruction%20and%202D%20view%20synthesis.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2405.10705v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3D3D%2520Vessel%2520Reconstruction%2520from%2520Sparse-View%2520Dynamic%2520DSA%2520Images%2520via%2520Vessel%250A%2520%2520Probability%2520Guided%2520Attenuation%2520Learning%26entry.906535625%3DZhentao%2520Liu%2520and%2520Huangxuan%2520Zhao%2520and%2520Wenhui%2520Qin%2520and%2520Zhenghong%2520Zhou%2520and%2520Xinggang%2520Wang%2520and%2520Wenping%2520Wang%2520and%2520Xiaochun%2520Lai%2520and%2520Chuansheng%2520Zheng%2520and%2520Dinggang%2520Shen%2520and%2520Zhiming%2520Cui%26entry.1292438233%3D%2520%2520Digital%2520Subtraction%2520Angiography%2520%2528DSA%2529%2520is%2520one%2520of%2520the%2520gold%2520standards%2520in%250Avascular%2520disease%2520diagnosing.%2520With%2520the%2520help%2520of%2520contrast%2520agent%252C%2520time-resolved%25202D%250ADSA%2520images%2520deliver%2520comprehensive%2520insights%2520into%2520blood%2520flow%2520information%2520and%2520can%250Abe%2520utilized%2520to%2520reconstruct%25203D%2520vessel%2520structures.%2520Current%2520commercial%2520DSA%2520systems%250Atypically%2520demand%2520hundreds%2520of%2520scanning%2520views%2520to%2520perform%2520reconstruction%252C%250Aresulting%2520in%2520substantial%2520radiation%2520exposure.%2520However%252C%2520sparse-view%2520DSA%250Areconstruction%252C%2520aimed%2520at%2520reducing%2520radiation%2520dosage%252C%2520is%2520still%2520underexplored%2520in%250Athe%2520research%2520community.%2520The%2520dynamic%2520blood%2520flow%2520and%2520insufficient%2520input%2520of%250Asparse-view%2520DSA%2520images%2520present%2520significant%2520challenges%2520to%2520the%25203D%2520vessel%250Areconstruction%2520task.%2520In%2520this%2520study%252C%2520we%2520propose%2520to%2520use%2520a%2520time-agnostic%2520vessel%250Aprobability%2520field%2520to%2520solve%2520this%2520problem%2520effectively.%2520Our%2520approach%252C%2520termed%2520as%250Avessel%2520probability%2520guided%2520attenuation%2520learning%252C%2520represents%2520the%2520DSA%2520imaging%2520as%2520a%250Acomplementary%2520weighted%2520combination%2520of%2520static%2520and%2520dynamic%2520attenuation%2520fields%252C%250Awith%2520the%2520weights%2520derived%2520from%2520the%2520vessel%2520probability%2520field.%2520Functioning%2520as%2520a%250Adynamic%2520mask%252C%2520vessel%2520probability%2520provides%2520proper%2520gradients%2520for%2520both%2520static%2520and%250Adynamic%2520fields%2520adaptive%2520to%2520different%2520scene%2520types.%2520This%2520mechanism%2520facilitates%2520a%250Aself-supervised%2520decomposition%2520between%2520static%2520backgrounds%2520and%2520dynamic%2520contrast%250Aagent%2520flow%252C%2520and%2520significantly%2520improves%2520the%2520reconstruction%2520quality.%2520Our%2520model%2520is%250Atrained%2520by%2520minimizing%2520the%2520disparity%2520between%2520synthesized%2520projections%2520and%2520real%250Acaptured%2520DSA%2520images.%2520We%2520further%2520employ%2520two%2520training%2520strategies%2520to%2520improve%2520our%250Areconstruction%2520quality%253A%2520%25281%2529%2520coarse-to-fine%2520progressive%2520training%2520to%2520achieve%250Abetter%2520geometry%2520and%2520%25282%2529%2520temporal%2520perturbed%2520rendering%2520loss%2520to%2520enforce%2520temporal%250Aconsistency.%2520Experimental%2520results%2520have%2520demonstrated%2520superior%2520quality%2520on%2520both%25203D%250Avessel%2520reconstruction%2520and%25202D%2520view%2520synthesis.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2405.10705v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=3D%20Vessel%20Reconstruction%20from%20Sparse-View%20Dynamic%20DSA%20Images%20via%20Vessel%0A%20%20Probability%20Guided%20Attenuation%20Learning&entry.906535625=Zhentao%20Liu%20and%20Huangxuan%20Zhao%20and%20Wenhui%20Qin%20and%20Zhenghong%20Zhou%20and%20Xinggang%20Wang%20and%20Wenping%20Wang%20and%20Xiaochun%20Lai%20and%20Chuansheng%20Zheng%20and%20Dinggang%20Shen%20and%20Zhiming%20Cui&entry.1292438233=%20%20Digital%20Subtraction%20Angiography%20%28DSA%29%20is%20one%20of%20the%20gold%20standards%20in%0Avascular%20disease%20diagnosing.%20With%20the%20help%20of%20contrast%20agent%2C%20time-resolved%202D%0ADSA%20images%20deliver%20comprehensive%20insights%20into%20blood%20flow%20information%20and%20can%0Abe%20utilized%20to%20reconstruct%203D%20vessel%20structures.%20Current%20commercial%20DSA%20systems%0Atypically%20demand%20hundreds%20of%20scanning%20views%20to%20perform%20reconstruction%2C%0Aresulting%20in%20substantial%20radiation%20exposure.%20However%2C%20sparse-view%20DSA%0Areconstruction%2C%20aimed%20at%20reducing%20radiation%20dosage%2C%20is%20still%20underexplored%20in%0Athe%20research%20community.%20The%20dynamic%20blood%20flow%20and%20insufficient%20input%20of%0Asparse-view%20DSA%20images%20present%20significant%20challenges%20to%20the%203D%20vessel%0Areconstruction%20task.%20In%20this%20study%2C%20we%20propose%20to%20use%20a%20time-agnostic%20vessel%0Aprobability%20field%20to%20solve%20this%20problem%20effectively.%20Our%20approach%2C%20termed%20as%0Avessel%20probability%20guided%20attenuation%20learning%2C%20represents%20the%20DSA%20imaging%20as%20a%0Acomplementary%20weighted%20combination%20of%20static%20and%20dynamic%20attenuation%20fields%2C%0Awith%20the%20weights%20derived%20from%20the%20vessel%20probability%20field.%20Functioning%20as%20a%0Adynamic%20mask%2C%20vessel%20probability%20provides%20proper%20gradients%20for%20both%20static%20and%0Adynamic%20fields%20adaptive%20to%20different%20scene%20types.%20This%20mechanism%20facilitates%20a%0Aself-supervised%20decomposition%20between%20static%20backgrounds%20and%20dynamic%20contrast%0Aagent%20flow%2C%20and%20significantly%20improves%20the%20reconstruction%20quality.%20Our%20model%20is%0Atrained%20by%20minimizing%20the%20disparity%20between%20synthesized%20projections%20and%20real%0Acaptured%20DSA%20images.%20We%20further%20employ%20two%20training%20strategies%20to%20improve%20our%0Areconstruction%20quality%3A%20%281%29%20coarse-to-fine%20progressive%20training%20to%20achieve%0Abetter%20geometry%20and%20%282%29%20temporal%20perturbed%20rendering%20loss%20to%20enforce%20temporal%0Aconsistency.%20Experimental%20results%20have%20demonstrated%20superior%20quality%20on%20both%203D%0Avessel%20reconstruction%20and%202D%20view%20synthesis.%0A&entry.1838667208=http%3A//arxiv.org/abs/2405.10705v1&entry.124074799=Read"},
{"title": "Leveraging SO(3)-steerable convolutions for pose-robust semantic\n  segmentation in 3D medical data", "author": "Ivan Diaz and Mario Geiger and Richard Iain McKinley", "abstract": "  Convolutional neural networks (CNNs) allow for parameter sharing and\ntranslational equivariance by using convolutional kernels in their linear\nlayers. By restricting these kernels to be SO(3)-steerable, CNNs can further\nimprove parameter sharing. These rotationally-equivariant convolutional layers\nhave several advantages over standard convolutional layers, including increased\nrobustness to unseen poses, smaller network size, and improved sample\nefficiency. Despite this, most segmentation networks used in medical image\nanalysis continue to rely on standard convolutional kernels. In this paper, we\npresent a new family of segmentation networks that use equivariant voxel\nconvolutions based on spherical harmonics. These networks are robust to data\nposes not seen during training, and do not require rotation-based data\naugmentation during training. In addition, we demonstrate improved segmentation\nperformance in MRI brain tumor and healthy brain structure segmentation tasks,\nwith enhanced robustness to reduced amounts of training data and improved\nparameter efficiency. Code to reproduce our results, and to implement the\nequivariant segmentation networks for other tasks is available at\nhttp://github.com/SCAN-NRAD/e3nn_Unet\n", "link": "http://arxiv.org/abs/2303.00351v3", "date": "2024-05-17", "relevancy": 2.2329, "topK": [{"title": "DDM-NET: End-to-end learning of keypoint feature Detection, Description and Matching for 3D localization", "link": "https://arxiv.org/abs/2212.04575", "similarity": 0.5743}, {"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.5664}, {"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.5389}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Leveraging%20SO%283%29-steerable%20convolutions%20for%20pose-robust%20semantic%0A%20%20segmentation%20in%203D%20medical%20data&body=Title%3A%20Leveraging%20SO%283%29-steerable%20convolutions%20for%20pose-robust%20semantic%0A%20%20segmentation%20in%203D%20medical%20data%0AAuthor%3A%20Ivan%20Diaz%20and%20Mario%20Geiger%20and%20Richard%20Iain%20McKinley%0AAbstract%3A%20%20%20Convolutional%20neural%20networks%20%28CNNs%29%20allow%20for%20parameter%20sharing%20and%0Atranslational%20equivariance%20by%20using%20convolutional%20kernels%20in%20their%20linear%0Alayers.%20By%20restricting%20these%20kernels%20to%20be%20SO%283%29-steerable%2C%20CNNs%20can%20further%0Aimprove%20parameter%20sharing.%20These%20rotationally-equivariant%20convolutional%20layers%0Ahave%20several%20advantages%20over%20standard%20convolutional%20layers%2C%20including%20increased%0Arobustness%20to%20unseen%20poses%2C%20smaller%20network%20size%2C%20and%20improved%20sample%0Aefficiency.%20Despite%20this%2C%20most%20segmentation%20networks%20used%20in%20medical%20image%0Aanalysis%20continue%20to%20rely%20on%20standard%20convolutional%20kernels.%20In%20this%20paper%2C%20we%0Apresent%20a%20new%20family%20of%20segmentation%20networks%20that%20use%20equivariant%20voxel%0Aconvolutions%20based%20on%20spherical%20harmonics.%20These%20networks%20are%20robust%20to%20data%0Aposes%20not%20seen%20during%20training%2C%20and%20do%20not%20require%20rotation-based%20data%0Aaugmentation%20during%20training.%20In%20addition%2C%20we%20demonstrate%20improved%20segmentation%0Aperformance%20in%20MRI%20brain%20tumor%20and%20healthy%20brain%20structure%20segmentation%20tasks%2C%0Awith%20enhanced%20robustness%20to%20reduced%20amounts%20of%20training%20data%20and%20improved%0Aparameter%20efficiency.%20Code%20to%20reproduce%20our%20results%2C%20and%20to%20implement%20the%0Aequivariant%20segmentation%20networks%20for%20other%20tasks%20is%20available%20at%0Ahttp%3A//github.com/SCAN-NRAD/e3nn_Unet%0A%0ALink%3A%20http%3A//arxiv.org/abs/2303.00351v3%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DLeveraging%2520SO%25283%2529-steerable%2520convolutions%2520for%2520pose-robust%2520semantic%250A%2520%2520segmentation%2520in%25203D%2520medical%2520data%26entry.906535625%3DIvan%2520Diaz%2520and%2520Mario%2520Geiger%2520and%2520Richard%2520Iain%2520McKinley%26entry.1292438233%3D%2520%2520Convolutional%2520neural%2520networks%2520%2528CNNs%2529%2520allow%2520for%2520parameter%2520sharing%2520and%250Atranslational%2520equivariance%2520by%2520using%2520convolutional%2520kernels%2520in%2520their%2520linear%250Alayers.%2520By%2520restricting%2520these%2520kernels%2520to%2520be%2520SO%25283%2529-steerable%252C%2520CNNs%2520can%2520further%250Aimprove%2520parameter%2520sharing.%2520These%2520rotationally-equivariant%2520convolutional%2520layers%250Ahave%2520several%2520advantages%2520over%2520standard%2520convolutional%2520layers%252C%2520including%2520increased%250Arobustness%2520to%2520unseen%2520poses%252C%2520smaller%2520network%2520size%252C%2520and%2520improved%2520sample%250Aefficiency.%2520Despite%2520this%252C%2520most%2520segmentation%2520networks%2520used%2520in%2520medical%2520image%250Aanalysis%2520continue%2520to%2520rely%2520on%2520standard%2520convolutional%2520kernels.%2520In%2520this%2520paper%252C%2520we%250Apresent%2520a%2520new%2520family%2520of%2520segmentation%2520networks%2520that%2520use%2520equivariant%2520voxel%250Aconvolutions%2520based%2520on%2520spherical%2520harmonics.%2520These%2520networks%2520are%2520robust%2520to%2520data%250Aposes%2520not%2520seen%2520during%2520training%252C%2520and%2520do%2520not%2520require%2520rotation-based%2520data%250Aaugmentation%2520during%2520training.%2520In%2520addition%252C%2520we%2520demonstrate%2520improved%2520segmentation%250Aperformance%2520in%2520MRI%2520brain%2520tumor%2520and%2520healthy%2520brain%2520structure%2520segmentation%2520tasks%252C%250Awith%2520enhanced%2520robustness%2520to%2520reduced%2520amounts%2520of%2520training%2520data%2520and%2520improved%250Aparameter%2520efficiency.%2520Code%2520to%2520reproduce%2520our%2520results%252C%2520and%2520to%2520implement%2520the%250Aequivariant%2520segmentation%2520networks%2520for%2520other%2520tasks%2520is%2520available%2520at%250Ahttp%253A//github.com/SCAN-NRAD/e3nn_Unet%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2303.00351v3%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Leveraging%20SO%283%29-steerable%20convolutions%20for%20pose-robust%20semantic%0A%20%20segmentation%20in%203D%20medical%20data&entry.906535625=Ivan%20Diaz%20and%20Mario%20Geiger%20and%20Richard%20Iain%20McKinley&entry.1292438233=%20%20Convolutional%20neural%20networks%20%28CNNs%29%20allow%20for%20parameter%20sharing%20and%0Atranslational%20equivariance%20by%20using%20convolutional%20kernels%20in%20their%20linear%0Alayers.%20By%20restricting%20these%20kernels%20to%20be%20SO%283%29-steerable%2C%20CNNs%20can%20further%0Aimprove%20parameter%20sharing.%20These%20rotationally-equivariant%20convolutional%20layers%0Ahave%20several%20advantages%20over%20standard%20convolutional%20layers%2C%20including%20increased%0Arobustness%20to%20unseen%20poses%2C%20smaller%20network%20size%2C%20and%20improved%20sample%0Aefficiency.%20Despite%20this%2C%20most%20segmentation%20networks%20used%20in%20medical%20image%0Aanalysis%20continue%20to%20rely%20on%20standard%20convolutional%20kernels.%20In%20this%20paper%2C%20we%0Apresent%20a%20new%20family%20of%20segmentation%20networks%20that%20use%20equivariant%20voxel%0Aconvolutions%20based%20on%20spherical%20harmonics.%20These%20networks%20are%20robust%20to%20data%0Aposes%20not%20seen%20during%20training%2C%20and%20do%20not%20require%20rotation-based%20data%0Aaugmentation%20during%20training.%20In%20addition%2C%20we%20demonstrate%20improved%20segmentation%0Aperformance%20in%20MRI%20brain%20tumor%20and%20healthy%20brain%20structure%20segmentation%20tasks%2C%0Awith%20enhanced%20robustness%20to%20reduced%20amounts%20of%20training%20data%20and%20improved%0Aparameter%20efficiency.%20Code%20to%20reproduce%20our%20results%2C%20and%20to%20implement%20the%0Aequivariant%20segmentation%20networks%20for%20other%20tasks%20is%20available%20at%0Ahttp%3A//github.com/SCAN-NRAD/e3nn_Unet%0A&entry.1838667208=http%3A//arxiv.org/abs/2303.00351v3&entry.124074799=Read"},
{"title": "Stable Phase Retrieval with Mirror Descent", "author": "Jean-Jacques Godeme and Jalal Fadili and Claude Amra and Myriam Zerrad", "abstract": "  In this paper, we aim to reconstruct an n-dimensional real vector from m\nphaseless measurements corrupted by an additive noise. We extend the noiseless\nframework developed in [15], based on mirror descent (or Bregman gradient\ndescent), to deal with noisy measurements and prove that the procedure is\nstable to (small enough) additive noise. In the deterministic case, we show\nthat mirror descent converges to a critical point of the phase retrieval\nproblem, and if the algorithm is well initialized and the noise is small\nenough, the critical point is near the true vector up to a global sign change.\nWhen the measurements are i.i.d Gaussian and the signal-to-noise ratio is large\nenough, we provide global convergence guarantees that ensure that with high\nprobability, mirror descent converges to a global minimizer near the true\nvector (up to a global sign change), as soon as the number of measurements m is\nlarge enough. The sample complexity bound can be improved if a spectral method\nis used to provide a good initial guess. We complement our theoretical study\nwith several numerical results showing that mirror descent is both a\ncomputationally and statistically efficient scheme to solve the phase retrieval\nproblem.\n", "link": "http://arxiv.org/abs/2405.10754v1", "date": "2024-05-17", "relevancy": 2.2267, "topK": [{"title": "Tuning-Free Noise Rectification for High Fidelity Image-to-Video\n  Generation", "link": "http://arxiv.org/abs/2403.02827v1", "similarity": 0.4496}, {"title": "Dynamic Gaussians Mesh: Consistent Mesh Reconstruction from Monocular\n  Videos", "link": "http://arxiv.org/abs/2404.12379v2", "similarity": 0.4486}, {"title": "DDM-NET: End-to-end learning of keypoint feature Detection, Description and Matching for 3D localization", "link": "https://arxiv.org/abs/2212.04575", "similarity": 0.4378}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Stable%20Phase%20Retrieval%20with%20Mirror%20Descent&body=Title%3A%20Stable%20Phase%20Retrieval%20with%20Mirror%20Descent%0AAuthor%3A%20Jean-Jacques%20Godeme%20and%20Jalal%20Fadili%20and%20Claude%20Amra%20and%20Myriam%20Zerrad%0AAbstract%3A%20%20%20In%20this%20paper%2C%20we%20aim%20to%20reconstruct%20an%20n-dimensional%20real%20vector%20from%20m%0Aphaseless%20measurements%20corrupted%20by%20an%20additive%20noise.%20We%20extend%20the%20noiseless%0Aframework%20developed%20in%20%5B15%5D%2C%20based%20on%20mirror%20descent%20%28or%20Bregman%20gradient%0Adescent%29%2C%20to%20deal%20with%20noisy%20measurements%20and%20prove%20that%20the%20procedure%20is%0Astable%20to%20%28small%20enough%29%20additive%20noise.%20In%20the%20deterministic%20case%2C%20we%20show%0Athat%20mirror%20descent%20converges%20to%20a%20critical%20point%20of%20the%20phase%20retrieval%0Aproblem%2C%20and%20if%20the%20algorithm%20is%20well%20initialized%20and%20the%20noise%20is%20small%0Aenough%2C%20the%20critical%20point%20is%20near%20the%20true%20vector%20up%20to%20a%20global%20sign%20change.%0AWhen%20the%20measurements%20are%20i.i.d%20Gaussian%20and%20the%20signal-to-noise%20ratio%20is%20large%0Aenough%2C%20we%20provide%20global%20convergence%20guarantees%20that%20ensure%20that%20with%20high%0Aprobability%2C%20mirror%20descent%20converges%20to%20a%20global%20minimizer%20near%20the%20true%0Avector%20%28up%20to%20a%20global%20sign%20change%29%2C%20as%20soon%20as%20the%20number%20of%20measurements%20m%20is%0Alarge%20enough.%20The%20sample%20complexity%20bound%20can%20be%20improved%20if%20a%20spectral%20method%0Ais%20used%20to%20provide%20a%20good%20initial%20guess.%20We%20complement%20our%20theoretical%20study%0Awith%20several%20numerical%20results%20showing%20that%20mirror%20descent%20is%20both%20a%0Acomputationally%20and%20statistically%20efficient%20scheme%20to%20solve%20the%20phase%20retrieval%0Aproblem.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2405.10754v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DStable%2520Phase%2520Retrieval%2520with%2520Mirror%2520Descent%26entry.906535625%3DJean-Jacques%2520Godeme%2520and%2520Jalal%2520Fadili%2520and%2520Claude%2520Amra%2520and%2520Myriam%2520Zerrad%26entry.1292438233%3D%2520%2520In%2520this%2520paper%252C%2520we%2520aim%2520to%2520reconstruct%2520an%2520n-dimensional%2520real%2520vector%2520from%2520m%250Aphaseless%2520measurements%2520corrupted%2520by%2520an%2520additive%2520noise.%2520We%2520extend%2520the%2520noiseless%250Aframework%2520developed%2520in%2520%255B15%255D%252C%2520based%2520on%2520mirror%2520descent%2520%2528or%2520Bregman%2520gradient%250Adescent%2529%252C%2520to%2520deal%2520with%2520noisy%2520measurements%2520and%2520prove%2520that%2520the%2520procedure%2520is%250Astable%2520to%2520%2528small%2520enough%2529%2520additive%2520noise.%2520In%2520the%2520deterministic%2520case%252C%2520we%2520show%250Athat%2520mirror%2520descent%2520converges%2520to%2520a%2520critical%2520point%2520of%2520the%2520phase%2520retrieval%250Aproblem%252C%2520and%2520if%2520the%2520algorithm%2520is%2520well%2520initialized%2520and%2520the%2520noise%2520is%2520small%250Aenough%252C%2520the%2520critical%2520point%2520is%2520near%2520the%2520true%2520vector%2520up%2520to%2520a%2520global%2520sign%2520change.%250AWhen%2520the%2520measurements%2520are%2520i.i.d%2520Gaussian%2520and%2520the%2520signal-to-noise%2520ratio%2520is%2520large%250Aenough%252C%2520we%2520provide%2520global%2520convergence%2520guarantees%2520that%2520ensure%2520that%2520with%2520high%250Aprobability%252C%2520mirror%2520descent%2520converges%2520to%2520a%2520global%2520minimizer%2520near%2520the%2520true%250Avector%2520%2528up%2520to%2520a%2520global%2520sign%2520change%2529%252C%2520as%2520soon%2520as%2520the%2520number%2520of%2520measurements%2520m%2520is%250Alarge%2520enough.%2520The%2520sample%2520complexity%2520bound%2520can%2520be%2520improved%2520if%2520a%2520spectral%2520method%250Ais%2520used%2520to%2520provide%2520a%2520good%2520initial%2520guess.%2520We%2520complement%2520our%2520theoretical%2520study%250Awith%2520several%2520numerical%2520results%2520showing%2520that%2520mirror%2520descent%2520is%2520both%2520a%250Acomputationally%2520and%2520statistically%2520efficient%2520scheme%2520to%2520solve%2520the%2520phase%2520retrieval%250Aproblem.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2405.10754v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Stable%20Phase%20Retrieval%20with%20Mirror%20Descent&entry.906535625=Jean-Jacques%20Godeme%20and%20Jalal%20Fadili%20and%20Claude%20Amra%20and%20Myriam%20Zerrad&entry.1292438233=%20%20In%20this%20paper%2C%20we%20aim%20to%20reconstruct%20an%20n-dimensional%20real%20vector%20from%20m%0Aphaseless%20measurements%20corrupted%20by%20an%20additive%20noise.%20We%20extend%20the%20noiseless%0Aframework%20developed%20in%20%5B15%5D%2C%20based%20on%20mirror%20descent%20%28or%20Bregman%20gradient%0Adescent%29%2C%20to%20deal%20with%20noisy%20measurements%20and%20prove%20that%20the%20procedure%20is%0Astable%20to%20%28small%20enough%29%20additive%20noise.%20In%20the%20deterministic%20case%2C%20we%20show%0Athat%20mirror%20descent%20converges%20to%20a%20critical%20point%20of%20the%20phase%20retrieval%0Aproblem%2C%20and%20if%20the%20algorithm%20is%20well%20initialized%20and%20the%20noise%20is%20small%0Aenough%2C%20the%20critical%20point%20is%20near%20the%20true%20vector%20up%20to%20a%20global%20sign%20change.%0AWhen%20the%20measurements%20are%20i.i.d%20Gaussian%20and%20the%20signal-to-noise%20ratio%20is%20large%0Aenough%2C%20we%20provide%20global%20convergence%20guarantees%20that%20ensure%20that%20with%20high%0Aprobability%2C%20mirror%20descent%20converges%20to%20a%20global%20minimizer%20near%20the%20true%0Avector%20%28up%20to%20a%20global%20sign%20change%29%2C%20as%20soon%20as%20the%20number%20of%20measurements%20m%20is%0Alarge%20enough.%20The%20sample%20complexity%20bound%20can%20be%20improved%20if%20a%20spectral%20method%0Ais%20used%20to%20provide%20a%20good%20initial%20guess.%20We%20complement%20our%20theoretical%20study%0Awith%20several%20numerical%20results%20showing%20that%20mirror%20descent%20is%20both%20a%0Acomputationally%20and%20statistically%20efficient%20scheme%20to%20solve%20the%20phase%20retrieval%0Aproblem.%0A&entry.1838667208=http%3A//arxiv.org/abs/2405.10754v1&entry.124074799=Read"},
{"title": "Two-Stage Stance Labeling: User-Hashtag Heuristics with Graph Neural\n  Networks", "author": "Joshua Melton and Shannon Reid and Gabriel Terejanu and Siddharth Krishnan", "abstract": "  The high volume and rapid evolution of content on social media present major\nchallenges for studying the stance of social media users. In this work, we\ndevelop a two stage stance labeling method that utilizes the user-hashtag\nbipartite graph and the user-user interaction graph. In the first stage, a\nsimple and efficient heuristic for stance labeling uses the user-hashtag\nbipartite graph to iteratively update the stance association of user and\nhashtag nodes via a label propagation mechanism. This set of soft labels is\nthen integrated with the user-user interaction graph to train a graph neural\nnetwork (GNN) model using semi-supervised learning. We evaluate this method on\ntwo large-scale datasets containing tweets related to climate change from June\n2021 to June 2022 and gun control from January 2022 to January 2023. Our\nexperiments demonstrate that enriching text-based embeddings of users with\nnetwork information from the user interaction graph using our semi-supervised\nGNN method outperforms both classifiers trained on user textual embeddings and\nzero-shot classification using LLMs such as GPT4. We discuss the need for\nintegrating nuanced understanding from social science with the scalability of\ncomputational methods to better understand how polarization on social media\noccurs for divisive issues such as climate change and gun control.\n", "link": "http://arxiv.org/abs/2404.10228v2", "date": "2024-05-17", "relevancy": 2.2133, "topK": [{"title": "Self-supervised Photographic Image Layout Representation Learning", "link": "http://arxiv.org/abs/2403.03740v1", "similarity": 0.4439}, {"title": "Boosted dynamic neural networks", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/26302", "similarity": 0.4435}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.4406}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Two-Stage%20Stance%20Labeling%3A%20User-Hashtag%20Heuristics%20with%20Graph%20Neural%0A%20%20Networks&body=Title%3A%20Two-Stage%20Stance%20Labeling%3A%20User-Hashtag%20Heuristics%20with%20Graph%20Neural%0A%20%20Networks%0AAuthor%3A%20Joshua%20Melton%20and%20Shannon%20Reid%20and%20Gabriel%20Terejanu%20and%20Siddharth%20Krishnan%0AAbstract%3A%20%20%20The%20high%20volume%20and%20rapid%20evolution%20of%20content%20on%20social%20media%20present%20major%0Achallenges%20for%20studying%20the%20stance%20of%20social%20media%20users.%20In%20this%20work%2C%20we%0Adevelop%20a%20two%20stage%20stance%20labeling%20method%20that%20utilizes%20the%20user-hashtag%0Abipartite%20graph%20and%20the%20user-user%20interaction%20graph.%20In%20the%20first%20stage%2C%20a%0Asimple%20and%20efficient%20heuristic%20for%20stance%20labeling%20uses%20the%20user-hashtag%0Abipartite%20graph%20to%20iteratively%20update%20the%20stance%20association%20of%20user%20and%0Ahashtag%20nodes%20via%20a%20label%20propagation%20mechanism.%20This%20set%20of%20soft%20labels%20is%0Athen%20integrated%20with%20the%20user-user%20interaction%20graph%20to%20train%20a%20graph%20neural%0Anetwork%20%28GNN%29%20model%20using%20semi-supervised%20learning.%20We%20evaluate%20this%20method%20on%0Atwo%20large-scale%20datasets%20containing%20tweets%20related%20to%20climate%20change%20from%20June%0A2021%20to%20June%202022%20and%20gun%20control%20from%20January%202022%20to%20January%202023.%20Our%0Aexperiments%20demonstrate%20that%20enriching%20text-based%20embeddings%20of%20users%20with%0Anetwork%20information%20from%20the%20user%20interaction%20graph%20using%20our%20semi-supervised%0AGNN%20method%20outperforms%20both%20classifiers%20trained%20on%20user%20textual%20embeddings%20and%0Azero-shot%20classification%20using%20LLMs%20such%20as%20GPT4.%20We%20discuss%20the%20need%20for%0Aintegrating%20nuanced%20understanding%20from%20social%20science%20with%20the%20scalability%20of%0Acomputational%20methods%20to%20better%20understand%20how%20polarization%20on%20social%20media%0Aoccurs%20for%20divisive%20issues%20such%20as%20climate%20change%20and%20gun%20control.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2404.10228v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DTwo-Stage%2520Stance%2520Labeling%253A%2520User-Hashtag%2520Heuristics%2520with%2520Graph%2520Neural%250A%2520%2520Networks%26entry.906535625%3DJoshua%2520Melton%2520and%2520Shannon%2520Reid%2520and%2520Gabriel%2520Terejanu%2520and%2520Siddharth%2520Krishnan%26entry.1292438233%3D%2520%2520The%2520high%2520volume%2520and%2520rapid%2520evolution%2520of%2520content%2520on%2520social%2520media%2520present%2520major%250Achallenges%2520for%2520studying%2520the%2520stance%2520of%2520social%2520media%2520users.%2520In%2520this%2520work%252C%2520we%250Adevelop%2520a%2520two%2520stage%2520stance%2520labeling%2520method%2520that%2520utilizes%2520the%2520user-hashtag%250Abipartite%2520graph%2520and%2520the%2520user-user%2520interaction%2520graph.%2520In%2520the%2520first%2520stage%252C%2520a%250Asimple%2520and%2520efficient%2520heuristic%2520for%2520stance%2520labeling%2520uses%2520the%2520user-hashtag%250Abipartite%2520graph%2520to%2520iteratively%2520update%2520the%2520stance%2520association%2520of%2520user%2520and%250Ahashtag%2520nodes%2520via%2520a%2520label%2520propagation%2520mechanism.%2520This%2520set%2520of%2520soft%2520labels%2520is%250Athen%2520integrated%2520with%2520the%2520user-user%2520interaction%2520graph%2520to%2520train%2520a%2520graph%2520neural%250Anetwork%2520%2528GNN%2529%2520model%2520using%2520semi-supervised%2520learning.%2520We%2520evaluate%2520this%2520method%2520on%250Atwo%2520large-scale%2520datasets%2520containing%2520tweets%2520related%2520to%2520climate%2520change%2520from%2520June%250A2021%2520to%2520June%25202022%2520and%2520gun%2520control%2520from%2520January%25202022%2520to%2520January%25202023.%2520Our%250Aexperiments%2520demonstrate%2520that%2520enriching%2520text-based%2520embeddings%2520of%2520users%2520with%250Anetwork%2520information%2520from%2520the%2520user%2520interaction%2520graph%2520using%2520our%2520semi-supervised%250AGNN%2520method%2520outperforms%2520both%2520classifiers%2520trained%2520on%2520user%2520textual%2520embeddings%2520and%250Azero-shot%2520classification%2520using%2520LLMs%2520such%2520as%2520GPT4.%2520We%2520discuss%2520the%2520need%2520for%250Aintegrating%2520nuanced%2520understanding%2520from%2520social%2520science%2520with%2520the%2520scalability%2520of%250Acomputational%2520methods%2520to%2520better%2520understand%2520how%2520polarization%2520on%2520social%2520media%250Aoccurs%2520for%2520divisive%2520issues%2520such%2520as%2520climate%2520change%2520and%2520gun%2520control.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2404.10228v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Two-Stage%20Stance%20Labeling%3A%20User-Hashtag%20Heuristics%20with%20Graph%20Neural%0A%20%20Networks&entry.906535625=Joshua%20Melton%20and%20Shannon%20Reid%20and%20Gabriel%20Terejanu%20and%20Siddharth%20Krishnan&entry.1292438233=%20%20The%20high%20volume%20and%20rapid%20evolution%20of%20content%20on%20social%20media%20present%20major%0Achallenges%20for%20studying%20the%20stance%20of%20social%20media%20users.%20In%20this%20work%2C%20we%0Adevelop%20a%20two%20stage%20stance%20labeling%20method%20that%20utilizes%20the%20user-hashtag%0Abipartite%20graph%20and%20the%20user-user%20interaction%20graph.%20In%20the%20first%20stage%2C%20a%0Asimple%20and%20efficient%20heuristic%20for%20stance%20labeling%20uses%20the%20user-hashtag%0Abipartite%20graph%20to%20iteratively%20update%20the%20stance%20association%20of%20user%20and%0Ahashtag%20nodes%20via%20a%20label%20propagation%20mechanism.%20This%20set%20of%20soft%20labels%20is%0Athen%20integrated%20with%20the%20user-user%20interaction%20graph%20to%20train%20a%20graph%20neural%0Anetwork%20%28GNN%29%20model%20using%20semi-supervised%20learning.%20We%20evaluate%20this%20method%20on%0Atwo%20large-scale%20datasets%20containing%20tweets%20related%20to%20climate%20change%20from%20June%0A2021%20to%20June%202022%20and%20gun%20control%20from%20January%202022%20to%20January%202023.%20Our%0Aexperiments%20demonstrate%20that%20enriching%20text-based%20embeddings%20of%20users%20with%0Anetwork%20information%20from%20the%20user%20interaction%20graph%20using%20our%20semi-supervised%0AGNN%20method%20outperforms%20both%20classifiers%20trained%20on%20user%20textual%20embeddings%20and%0Azero-shot%20classification%20using%20LLMs%20such%20as%20GPT4.%20We%20discuss%20the%20need%20for%0Aintegrating%20nuanced%20understanding%20from%20social%20science%20with%20the%20scalability%20of%0Acomputational%20methods%20to%20better%20understand%20how%20polarization%20on%20social%20media%0Aoccurs%20for%20divisive%20issues%20such%20as%20climate%20change%20and%20gun%20control.%0A&entry.1838667208=http%3A//arxiv.org/abs/2404.10228v2&entry.124074799=Read"},
{"title": "A Versatile Framework for Analyzing Galaxy Image Data by Implanting\n  Human-in-the-loop on a Large Vision Model", "author": "Mingxiang Fu and Yu Song and Jiameng Lv and Liang Cao and Peng Jia and Nan Li and Xiangru Li and Jifeng Liu and A-Li Luo and Bo Qiu and Shiyin Shen and Liangping Tu and Lili Wang and Shoulin Wei and Haifeng Yang and Zhenping Yi and Zhiqiang Zou", "abstract": "  The exponential growth of astronomical datasets provides an unprecedented\nopportunity for humans to gain insight into the Universe. However, effectively\nanalyzing this vast amount of data poses a significant challenge. Astronomers\nare turning to deep learning techniques to address this, but the methods are\nlimited by their specific training sets, leading to considerable duplicate\nworkloads too. Hence, as an example to present how to overcome the issue, we\nbuilt a framework for general analysis of galaxy images, based on a large\nvision model (LVM) plus downstream tasks (DST), including galaxy morphological\nclassification, image restoration, object detection, parameter extraction, and\nmore. Considering the low signal-to-noise ratio of galaxy images and the\nimbalanced distribution of galaxy categories, we have incorporated a\nHuman-in-the-loop (HITL) module into our large vision model, which leverages\nhuman knowledge to enhance the reliability and interpretability of processing\ngalaxy images interactively. The proposed framework exhibits notable few-shot\nlearning capabilities and versatile adaptability to all the abovementioned\ntasks on galaxy images in the DESI legacy imaging surveys. Expressly, for\nobject detection, trained by 1000 data points, our DST upon the LVM achieves an\naccuracy of 96.7%, while ResNet50 plus Mask R-CNN gives an accuracy of 93.1%;\nfor morphology classification, to obtain AUC ~0.9, LVM plus DST and HITL only\nrequests 1/50 training sets compared to ResNet18. Expectedly, multimodal data\ncan be integrated similarly, which opens up possibilities for conducting joint\nanalyses with datasets spanning diverse domains in the era of multi-message\nastronomy.\n", "link": "http://arxiv.org/abs/2405.10890v1", "date": "2024-05-17", "relevancy": 2.2085, "topK": [{"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5695}, {"title": "SDXS: Real-Time One-Step Latent Diffusion Models with Image Conditions", "link": "http://arxiv.org/abs/2403.16627v1", "similarity": 0.54}, {"title": "Self-supervised Photographic Image Layout Representation Learning", "link": "http://arxiv.org/abs/2403.03740v1", "similarity": 0.5396}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20A%20Versatile%20Framework%20for%20Analyzing%20Galaxy%20Image%20Data%20by%20Implanting%0A%20%20Human-in-the-loop%20on%20a%20Large%20Vision%20Model&body=Title%3A%20A%20Versatile%20Framework%20for%20Analyzing%20Galaxy%20Image%20Data%20by%20Implanting%0A%20%20Human-in-the-loop%20on%20a%20Large%20Vision%20Model%0AAuthor%3A%20Mingxiang%20Fu%20and%20Yu%20Song%20and%20Jiameng%20Lv%20and%20Liang%20Cao%20and%20Peng%20Jia%20and%20Nan%20Li%20and%20Xiangru%20Li%20and%20Jifeng%20Liu%20and%20A-Li%20Luo%20and%20Bo%20Qiu%20and%20Shiyin%20Shen%20and%20Liangping%20Tu%20and%20Lili%20Wang%20and%20Shoulin%20Wei%20and%20Haifeng%20Yang%20and%20Zhenping%20Yi%20and%20Zhiqiang%20Zou%0AAbstract%3A%20%20%20The%20exponential%20growth%20of%20astronomical%20datasets%20provides%20an%20unprecedented%0Aopportunity%20for%20humans%20to%20gain%20insight%20into%20the%20Universe.%20However%2C%20effectively%0Aanalyzing%20this%20vast%20amount%20of%20data%20poses%20a%20significant%20challenge.%20Astronomers%0Aare%20turning%20to%20deep%20learning%20techniques%20to%20address%20this%2C%20but%20the%20methods%20are%0Alimited%20by%20their%20specific%20training%20sets%2C%20leading%20to%20considerable%20duplicate%0Aworkloads%20too.%20Hence%2C%20as%20an%20example%20to%20present%20how%20to%20overcome%20the%20issue%2C%20we%0Abuilt%20a%20framework%20for%20general%20analysis%20of%20galaxy%20images%2C%20based%20on%20a%20large%0Avision%20model%20%28LVM%29%20plus%20downstream%20tasks%20%28DST%29%2C%20including%20galaxy%20morphological%0Aclassification%2C%20image%20restoration%2C%20object%20detection%2C%20parameter%20extraction%2C%20and%0Amore.%20Considering%20the%20low%20signal-to-noise%20ratio%20of%20galaxy%20images%20and%20the%0Aimbalanced%20distribution%20of%20galaxy%20categories%2C%20we%20have%20incorporated%20a%0AHuman-in-the-loop%20%28HITL%29%20module%20into%20our%20large%20vision%20model%2C%20which%20leverages%0Ahuman%20knowledge%20to%20enhance%20the%20reliability%20and%20interpretability%20of%20processing%0Agalaxy%20images%20interactively.%20The%20proposed%20framework%20exhibits%20notable%20few-shot%0Alearning%20capabilities%20and%20versatile%20adaptability%20to%20all%20the%20abovementioned%0Atasks%20on%20galaxy%20images%20in%20the%20DESI%20legacy%20imaging%20surveys.%20Expressly%2C%20for%0Aobject%20detection%2C%20trained%20by%201000%20data%20points%2C%20our%20DST%20upon%20the%20LVM%20achieves%20an%0Aaccuracy%20of%2096.7%25%2C%20while%20ResNet50%20plus%20Mask%20R-CNN%20gives%20an%20accuracy%20of%2093.1%25%3B%0Afor%20morphology%20classification%2C%20to%20obtain%20AUC%20~0.9%2C%20LVM%20plus%20DST%20and%20HITL%20only%0Arequests%201/50%20training%20sets%20compared%20to%20ResNet18.%20Expectedly%2C%20multimodal%20data%0Acan%20be%20integrated%20similarly%2C%20which%20opens%20up%20possibilities%20for%20conducting%20joint%0Aanalyses%20with%20datasets%20spanning%20diverse%20domains%20in%20the%20era%20of%20multi-message%0Aastronomy.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2405.10890v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DA%2520Versatile%2520Framework%2520for%2520Analyzing%2520Galaxy%2520Image%2520Data%2520by%2520Implanting%250A%2520%2520Human-in-the-loop%2520on%2520a%2520Large%2520Vision%2520Model%26entry.906535625%3DMingxiang%2520Fu%2520and%2520Yu%2520Song%2520and%2520Jiameng%2520Lv%2520and%2520Liang%2520Cao%2520and%2520Peng%2520Jia%2520and%2520Nan%2520Li%2520and%2520Xiangru%2520Li%2520and%2520Jifeng%2520Liu%2520and%2520A-Li%2520Luo%2520and%2520Bo%2520Qiu%2520and%2520Shiyin%2520Shen%2520and%2520Liangping%2520Tu%2520and%2520Lili%2520Wang%2520and%2520Shoulin%2520Wei%2520and%2520Haifeng%2520Yang%2520and%2520Zhenping%2520Yi%2520and%2520Zhiqiang%2520Zou%26entry.1292438233%3D%2520%2520The%2520exponential%2520growth%2520of%2520astronomical%2520datasets%2520provides%2520an%2520unprecedented%250Aopportunity%2520for%2520humans%2520to%2520gain%2520insight%2520into%2520the%2520Universe.%2520However%252C%2520effectively%250Aanalyzing%2520this%2520vast%2520amount%2520of%2520data%2520poses%2520a%2520significant%2520challenge.%2520Astronomers%250Aare%2520turning%2520to%2520deep%2520learning%2520techniques%2520to%2520address%2520this%252C%2520but%2520the%2520methods%2520are%250Alimited%2520by%2520their%2520specific%2520training%2520sets%252C%2520leading%2520to%2520considerable%2520duplicate%250Aworkloads%2520too.%2520Hence%252C%2520as%2520an%2520example%2520to%2520present%2520how%2520to%2520overcome%2520the%2520issue%252C%2520we%250Abuilt%2520a%2520framework%2520for%2520general%2520analysis%2520of%2520galaxy%2520images%252C%2520based%2520on%2520a%2520large%250Avision%2520model%2520%2528LVM%2529%2520plus%2520downstream%2520tasks%2520%2528DST%2529%252C%2520including%2520galaxy%2520morphological%250Aclassification%252C%2520image%2520restoration%252C%2520object%2520detection%252C%2520parameter%2520extraction%252C%2520and%250Amore.%2520Considering%2520the%2520low%2520signal-to-noise%2520ratio%2520of%2520galaxy%2520images%2520and%2520the%250Aimbalanced%2520distribution%2520of%2520galaxy%2520categories%252C%2520we%2520have%2520incorporated%2520a%250AHuman-in-the-loop%2520%2528HITL%2529%2520module%2520into%2520our%2520large%2520vision%2520model%252C%2520which%2520leverages%250Ahuman%2520knowledge%2520to%2520enhance%2520the%2520reliability%2520and%2520interpretability%2520of%2520processing%250Agalaxy%2520images%2520interactively.%2520The%2520proposed%2520framework%2520exhibits%2520notable%2520few-shot%250Alearning%2520capabilities%2520and%2520versatile%2520adaptability%2520to%2520all%2520the%2520abovementioned%250Atasks%2520on%2520galaxy%2520images%2520in%2520the%2520DESI%2520legacy%2520imaging%2520surveys.%2520Expressly%252C%2520for%250Aobject%2520detection%252C%2520trained%2520by%25201000%2520data%2520points%252C%2520our%2520DST%2520upon%2520the%2520LVM%2520achieves%2520an%250Aaccuracy%2520of%252096.7%2525%252C%2520while%2520ResNet50%2520plus%2520Mask%2520R-CNN%2520gives%2520an%2520accuracy%2520of%252093.1%2525%253B%250Afor%2520morphology%2520classification%252C%2520to%2520obtain%2520AUC%2520~0.9%252C%2520LVM%2520plus%2520DST%2520and%2520HITL%2520only%250Arequests%25201/50%2520training%2520sets%2520compared%2520to%2520ResNet18.%2520Expectedly%252C%2520multimodal%2520data%250Acan%2520be%2520integrated%2520similarly%252C%2520which%2520opens%2520up%2520possibilities%2520for%2520conducting%2520joint%250Aanalyses%2520with%2520datasets%2520spanning%2520diverse%2520domains%2520in%2520the%2520era%2520of%2520multi-message%250Aastronomy.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2405.10890v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=A%20Versatile%20Framework%20for%20Analyzing%20Galaxy%20Image%20Data%20by%20Implanting%0A%20%20Human-in-the-loop%20on%20a%20Large%20Vision%20Model&entry.906535625=Mingxiang%20Fu%20and%20Yu%20Song%20and%20Jiameng%20Lv%20and%20Liang%20Cao%20and%20Peng%20Jia%20and%20Nan%20Li%20and%20Xiangru%20Li%20and%20Jifeng%20Liu%20and%20A-Li%20Luo%20and%20Bo%20Qiu%20and%20Shiyin%20Shen%20and%20Liangping%20Tu%20and%20Lili%20Wang%20and%20Shoulin%20Wei%20and%20Haifeng%20Yang%20and%20Zhenping%20Yi%20and%20Zhiqiang%20Zou&entry.1292438233=%20%20The%20exponential%20growth%20of%20astronomical%20datasets%20provides%20an%20unprecedented%0Aopportunity%20for%20humans%20to%20gain%20insight%20into%20the%20Universe.%20However%2C%20effectively%0Aanalyzing%20this%20vast%20amount%20of%20data%20poses%20a%20significant%20challenge.%20Astronomers%0Aare%20turning%20to%20deep%20learning%20techniques%20to%20address%20this%2C%20but%20the%20methods%20are%0Alimited%20by%20their%20specific%20training%20sets%2C%20leading%20to%20considerable%20duplicate%0Aworkloads%20too.%20Hence%2C%20as%20an%20example%20to%20present%20how%20to%20overcome%20the%20issue%2C%20we%0Abuilt%20a%20framework%20for%20general%20analysis%20of%20galaxy%20images%2C%20based%20on%20a%20large%0Avision%20model%20%28LVM%29%20plus%20downstream%20tasks%20%28DST%29%2C%20including%20galaxy%20morphological%0Aclassification%2C%20image%20restoration%2C%20object%20detection%2C%20parameter%20extraction%2C%20and%0Amore.%20Considering%20the%20low%20signal-to-noise%20ratio%20of%20galaxy%20images%20and%20the%0Aimbalanced%20distribution%20of%20galaxy%20categories%2C%20we%20have%20incorporated%20a%0AHuman-in-the-loop%20%28HITL%29%20module%20into%20our%20large%20vision%20model%2C%20which%20leverages%0Ahuman%20knowledge%20to%20enhance%20the%20reliability%20and%20interpretability%20of%20processing%0Agalaxy%20images%20interactively.%20The%20proposed%20framework%20exhibits%20notable%20few-shot%0Alearning%20capabilities%20and%20versatile%20adaptability%20to%20all%20the%20abovementioned%0Atasks%20on%20galaxy%20images%20in%20the%20DESI%20legacy%20imaging%20surveys.%20Expressly%2C%20for%0Aobject%20detection%2C%20trained%20by%201000%20data%20points%2C%20our%20DST%20upon%20the%20LVM%20achieves%20an%0Aaccuracy%20of%2096.7%25%2C%20while%20ResNet50%20plus%20Mask%20R-CNN%20gives%20an%20accuracy%20of%2093.1%25%3B%0Afor%20morphology%20classification%2C%20to%20obtain%20AUC%20~0.9%2C%20LVM%20plus%20DST%20and%20HITL%20only%0Arequests%201/50%20training%20sets%20compared%20to%20ResNet18.%20Expectedly%2C%20multimodal%20data%0Acan%20be%20integrated%20similarly%2C%20which%20opens%20up%20possibilities%20for%20conducting%20joint%0Aanalyses%20with%20datasets%20spanning%20diverse%20domains%20in%20the%20era%20of%20multi-message%0Aastronomy.%0A&entry.1838667208=http%3A//arxiv.org/abs/2405.10890v1&entry.124074799=Read"},
{"title": "Sharpness-Aware Minimization in Genetic Programming", "author": "Illya Bakurov and Nathan Haut and Wolfgang Banzhaf", "abstract": "  Sharpness-Aware Minimization (SAM) was recently introduced as a\nregularization procedure for training deep neural networks. It simultaneously\nminimizes the fitness (or loss) function and the so-called fitness sharpness.\nThe latter serves as a measure of the nonlinear behavior of a solution and does\nso by finding solutions that lie in neighborhoods having uniformly similar loss\nvalues across all fitness cases. In this contribution, we adapt SAM for tree\nGenetic Programming (TGP) by exploring the semantic neighborhoods of solutions\nusing two simple approaches. By capitalizing upon perturbing input and output\nof program trees, sharpness can be estimated and used as a second optimization\ncriterion during the evolution. To better understand the impact of this variant\nof SAM on TGP, we collect numerous indicators of the evolutionary process,\nincluding generalization ability, complexity, diversity, and a recently\nproposed genotype-phenotype mapping to study the amount of redundancy in trees.\nThe experimental results demonstrate that using any of the two proposed SAM\nadaptations in TGP allows (i) a significant reduction of tree sizes in the\npopulation and (ii) a decrease in redundancy of the trees. When assessed on\nreal-world benchmarks, the generalization ability of the elite solutions does\nnot deteriorate.\n", "link": "http://arxiv.org/abs/2405.10267v2", "date": "2024-05-17", "relevancy": 2.1721, "topK": [{"title": "ToDo: Token Downsampling for Efficient Generation of High-Resolution\n  Images", "link": "http://arxiv.org/abs/2402.13573v2", "similarity": 0.4652}, {"title": "Tuning-Free Noise Rectification for High Fidelity Image-to-Video\n  Generation", "link": "http://arxiv.org/abs/2403.02827v1", "similarity": 0.4205}, {"title": "UGG: Unified Generative Grasping", "link": "https://arxiv.org/abs/2311.16917", "similarity": 0.4176}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Sharpness-Aware%20Minimization%20in%20Genetic%20Programming&body=Title%3A%20Sharpness-Aware%20Minimization%20in%20Genetic%20Programming%0AAuthor%3A%20Illya%20Bakurov%20and%20Nathan%20Haut%20and%20Wolfgang%20Banzhaf%0AAbstract%3A%20%20%20Sharpness-Aware%20Minimization%20%28SAM%29%20was%20recently%20introduced%20as%20a%0Aregularization%20procedure%20for%20training%20deep%20neural%20networks.%20It%20simultaneously%0Aminimizes%20the%20fitness%20%28or%20loss%29%20function%20and%20the%20so-called%20fitness%20sharpness.%0AThe%20latter%20serves%20as%20a%20measure%20of%20the%20nonlinear%20behavior%20of%20a%20solution%20and%20does%0Aso%20by%20finding%20solutions%20that%20lie%20in%20neighborhoods%20having%20uniformly%20similar%20loss%0Avalues%20across%20all%20fitness%20cases.%20In%20this%20contribution%2C%20we%20adapt%20SAM%20for%20tree%0AGenetic%20Programming%20%28TGP%29%20by%20exploring%20the%20semantic%20neighborhoods%20of%20solutions%0Ausing%20two%20simple%20approaches.%20By%20capitalizing%20upon%20perturbing%20input%20and%20output%0Aof%20program%20trees%2C%20sharpness%20can%20be%20estimated%20and%20used%20as%20a%20second%20optimization%0Acriterion%20during%20the%20evolution.%20To%20better%20understand%20the%20impact%20of%20this%20variant%0Aof%20SAM%20on%20TGP%2C%20we%20collect%20numerous%20indicators%20of%20the%20evolutionary%20process%2C%0Aincluding%20generalization%20ability%2C%20complexity%2C%20diversity%2C%20and%20a%20recently%0Aproposed%20genotype-phenotype%20mapping%20to%20study%20the%20amount%20of%20redundancy%20in%20trees.%0AThe%20experimental%20results%20demonstrate%20that%20using%20any%20of%20the%20two%20proposed%20SAM%0Aadaptations%20in%20TGP%20allows%20%28i%29%20a%20significant%20reduction%20of%20tree%20sizes%20in%20the%0Apopulation%20and%20%28ii%29%20a%20decrease%20in%20redundancy%20of%20the%20trees.%20When%20assessed%20on%0Areal-world%20benchmarks%2C%20the%20generalization%20ability%20of%20the%20elite%20solutions%20does%0Anot%20deteriorate.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2405.10267v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DSharpness-Aware%2520Minimization%2520in%2520Genetic%2520Programming%26entry.906535625%3DIllya%2520Bakurov%2520and%2520Nathan%2520Haut%2520and%2520Wolfgang%2520Banzhaf%26entry.1292438233%3D%2520%2520Sharpness-Aware%2520Minimization%2520%2528SAM%2529%2520was%2520recently%2520introduced%2520as%2520a%250Aregularization%2520procedure%2520for%2520training%2520deep%2520neural%2520networks.%2520It%2520simultaneously%250Aminimizes%2520the%2520fitness%2520%2528or%2520loss%2529%2520function%2520and%2520the%2520so-called%2520fitness%2520sharpness.%250AThe%2520latter%2520serves%2520as%2520a%2520measure%2520of%2520the%2520nonlinear%2520behavior%2520of%2520a%2520solution%2520and%2520does%250Aso%2520by%2520finding%2520solutions%2520that%2520lie%2520in%2520neighborhoods%2520having%2520uniformly%2520similar%2520loss%250Avalues%2520across%2520all%2520fitness%2520cases.%2520In%2520this%2520contribution%252C%2520we%2520adapt%2520SAM%2520for%2520tree%250AGenetic%2520Programming%2520%2528TGP%2529%2520by%2520exploring%2520the%2520semantic%2520neighborhoods%2520of%2520solutions%250Ausing%2520two%2520simple%2520approaches.%2520By%2520capitalizing%2520upon%2520perturbing%2520input%2520and%2520output%250Aof%2520program%2520trees%252C%2520sharpness%2520can%2520be%2520estimated%2520and%2520used%2520as%2520a%2520second%2520optimization%250Acriterion%2520during%2520the%2520evolution.%2520To%2520better%2520understand%2520the%2520impact%2520of%2520this%2520variant%250Aof%2520SAM%2520on%2520TGP%252C%2520we%2520collect%2520numerous%2520indicators%2520of%2520the%2520evolutionary%2520process%252C%250Aincluding%2520generalization%2520ability%252C%2520complexity%252C%2520diversity%252C%2520and%2520a%2520recently%250Aproposed%2520genotype-phenotype%2520mapping%2520to%2520study%2520the%2520amount%2520of%2520redundancy%2520in%2520trees.%250AThe%2520experimental%2520results%2520demonstrate%2520that%2520using%2520any%2520of%2520the%2520two%2520proposed%2520SAM%250Aadaptations%2520in%2520TGP%2520allows%2520%2528i%2529%2520a%2520significant%2520reduction%2520of%2520tree%2520sizes%2520in%2520the%250Apopulation%2520and%2520%2528ii%2529%2520a%2520decrease%2520in%2520redundancy%2520of%2520the%2520trees.%2520When%2520assessed%2520on%250Areal-world%2520benchmarks%252C%2520the%2520generalization%2520ability%2520of%2520the%2520elite%2520solutions%2520does%250Anot%2520deteriorate.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2405.10267v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Sharpness-Aware%20Minimization%20in%20Genetic%20Programming&entry.906535625=Illya%20Bakurov%20and%20Nathan%20Haut%20and%20Wolfgang%20Banzhaf&entry.1292438233=%20%20Sharpness-Aware%20Minimization%20%28SAM%29%20was%20recently%20introduced%20as%20a%0Aregularization%20procedure%20for%20training%20deep%20neural%20networks.%20It%20simultaneously%0Aminimizes%20the%20fitness%20%28or%20loss%29%20function%20and%20the%20so-called%20fitness%20sharpness.%0AThe%20latter%20serves%20as%20a%20measure%20of%20the%20nonlinear%20behavior%20of%20a%20solution%20and%20does%0Aso%20by%20finding%20solutions%20that%20lie%20in%20neighborhoods%20having%20uniformly%20similar%20loss%0Avalues%20across%20all%20fitness%20cases.%20In%20this%20contribution%2C%20we%20adapt%20SAM%20for%20tree%0AGenetic%20Programming%20%28TGP%29%20by%20exploring%20the%20semantic%20neighborhoods%20of%20solutions%0Ausing%20two%20simple%20approaches.%20By%20capitalizing%20upon%20perturbing%20input%20and%20output%0Aof%20program%20trees%2C%20sharpness%20can%20be%20estimated%20and%20used%20as%20a%20second%20optimization%0Acriterion%20during%20the%20evolution.%20To%20better%20understand%20the%20impact%20of%20this%20variant%0Aof%20SAM%20on%20TGP%2C%20we%20collect%20numerous%20indicators%20of%20the%20evolutionary%20process%2C%0Aincluding%20generalization%20ability%2C%20complexity%2C%20diversity%2C%20and%20a%20recently%0Aproposed%20genotype-phenotype%20mapping%20to%20study%20the%20amount%20of%20redundancy%20in%20trees.%0AThe%20experimental%20results%20demonstrate%20that%20using%20any%20of%20the%20two%20proposed%20SAM%0Aadaptations%20in%20TGP%20allows%20%28i%29%20a%20significant%20reduction%20of%20tree%20sizes%20in%20the%0Apopulation%20and%20%28ii%29%20a%20decrease%20in%20redundancy%20of%20the%20trees.%20When%20assessed%20on%0Areal-world%20benchmarks%2C%20the%20generalization%20ability%20of%20the%20elite%20solutions%20does%0Anot%20deteriorate.%0A&entry.1838667208=http%3A//arxiv.org/abs/2405.10267v2&entry.124074799=Read"},
{"title": "Efficient Deep Learning with Decorrelated Backpropagation", "author": "Sander Dalm and Joshua Offergeld and Nasir Ahmad and Marcel van Gerven", "abstract": "  The backpropagation algorithm remains the dominant and most successful method\nfor training deep neural networks (DNNs). At the same time, training DNNs at\nscale comes at a significant computational cost and therefore a high carbon\nfootprint. Converging evidence suggests that input decorrelation may speed up\ndeep learning. However, to date, this has not yet translated into substantial\nimprovements in training efficiency in large-scale DNNs. This is mainly caused\nby the challenge of enforcing fast and stable network-wide decorrelation. Here,\nwe show for the first time that much more efficient training of very deep\nneural networks using decorrelated backpropagation is feasible. To achieve this\ngoal we made use of a novel algorithm which induces network-wide input\ndecorrelation using minimal computational overhead. By combining this algorithm\nwith careful optimizations, we obtain a more than two-fold speed-up and higher\ntest accuracy compared to backpropagation when training a 18-layer deep\nresidual network. This demonstrates that decorrelation provides exciting\nprospects for efficient deep learning at scale.\n", "link": "http://arxiv.org/abs/2405.02385v2", "date": "2024-05-17", "relevancy": 2.1538, "topK": [{"title": "Boosted dynamic neural networks", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/26302", "similarity": 0.5603}, {"title": "ToDo: Token Downsampling for Efficient Generation of High-Resolution\n  Images", "link": "http://arxiv.org/abs/2402.13573v2", "similarity": 0.525}, {"title": "SDXS: Real-Time One-Step Latent Diffusion Models with Image Conditions", "link": "http://arxiv.org/abs/2403.16627v1", "similarity": 0.5173}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Efficient%20Deep%20Learning%20with%20Decorrelated%20Backpropagation&body=Title%3A%20Efficient%20Deep%20Learning%20with%20Decorrelated%20Backpropagation%0AAuthor%3A%20Sander%20Dalm%20and%20Joshua%20Offergeld%20and%20Nasir%20Ahmad%20and%20Marcel%20van%20Gerven%0AAbstract%3A%20%20%20The%20backpropagation%20algorithm%20remains%20the%20dominant%20and%20most%20successful%20method%0Afor%20training%20deep%20neural%20networks%20%28DNNs%29.%20At%20the%20same%20time%2C%20training%20DNNs%20at%0Ascale%20comes%20at%20a%20significant%20computational%20cost%20and%20therefore%20a%20high%20carbon%0Afootprint.%20Converging%20evidence%20suggests%20that%20input%20decorrelation%20may%20speed%20up%0Adeep%20learning.%20However%2C%20to%20date%2C%20this%20has%20not%20yet%20translated%20into%20substantial%0Aimprovements%20in%20training%20efficiency%20in%20large-scale%20DNNs.%20This%20is%20mainly%20caused%0Aby%20the%20challenge%20of%20enforcing%20fast%20and%20stable%20network-wide%20decorrelation.%20Here%2C%0Awe%20show%20for%20the%20first%20time%20that%20much%20more%20efficient%20training%20of%20very%20deep%0Aneural%20networks%20using%20decorrelated%20backpropagation%20is%20feasible.%20To%20achieve%20this%0Agoal%20we%20made%20use%20of%20a%20novel%20algorithm%20which%20induces%20network-wide%20input%0Adecorrelation%20using%20minimal%20computational%20overhead.%20By%20combining%20this%20algorithm%0Awith%20careful%20optimizations%2C%20we%20obtain%20a%20more%20than%20two-fold%20speed-up%20and%20higher%0Atest%20accuracy%20compared%20to%20backpropagation%20when%20training%20a%2018-layer%20deep%0Aresidual%20network.%20This%20demonstrates%20that%20decorrelation%20provides%20exciting%0Aprospects%20for%20efficient%20deep%20learning%20at%20scale.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2405.02385v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DEfficient%2520Deep%2520Learning%2520with%2520Decorrelated%2520Backpropagation%26entry.906535625%3DSander%2520Dalm%2520and%2520Joshua%2520Offergeld%2520and%2520Nasir%2520Ahmad%2520and%2520Marcel%2520van%2520Gerven%26entry.1292438233%3D%2520%2520The%2520backpropagation%2520algorithm%2520remains%2520the%2520dominant%2520and%2520most%2520successful%2520method%250Afor%2520training%2520deep%2520neural%2520networks%2520%2528DNNs%2529.%2520At%2520the%2520same%2520time%252C%2520training%2520DNNs%2520at%250Ascale%2520comes%2520at%2520a%2520significant%2520computational%2520cost%2520and%2520therefore%2520a%2520high%2520carbon%250Afootprint.%2520Converging%2520evidence%2520suggests%2520that%2520input%2520decorrelation%2520may%2520speed%2520up%250Adeep%2520learning.%2520However%252C%2520to%2520date%252C%2520this%2520has%2520not%2520yet%2520translated%2520into%2520substantial%250Aimprovements%2520in%2520training%2520efficiency%2520in%2520large-scale%2520DNNs.%2520This%2520is%2520mainly%2520caused%250Aby%2520the%2520challenge%2520of%2520enforcing%2520fast%2520and%2520stable%2520network-wide%2520decorrelation.%2520Here%252C%250Awe%2520show%2520for%2520the%2520first%2520time%2520that%2520much%2520more%2520efficient%2520training%2520of%2520very%2520deep%250Aneural%2520networks%2520using%2520decorrelated%2520backpropagation%2520is%2520feasible.%2520To%2520achieve%2520this%250Agoal%2520we%2520made%2520use%2520of%2520a%2520novel%2520algorithm%2520which%2520induces%2520network-wide%2520input%250Adecorrelation%2520using%2520minimal%2520computational%2520overhead.%2520By%2520combining%2520this%2520algorithm%250Awith%2520careful%2520optimizations%252C%2520we%2520obtain%2520a%2520more%2520than%2520two-fold%2520speed-up%2520and%2520higher%250Atest%2520accuracy%2520compared%2520to%2520backpropagation%2520when%2520training%2520a%252018-layer%2520deep%250Aresidual%2520network.%2520This%2520demonstrates%2520that%2520decorrelation%2520provides%2520exciting%250Aprospects%2520for%2520efficient%2520deep%2520learning%2520at%2520scale.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2405.02385v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Efficient%20Deep%20Learning%20with%20Decorrelated%20Backpropagation&entry.906535625=Sander%20Dalm%20and%20Joshua%20Offergeld%20and%20Nasir%20Ahmad%20and%20Marcel%20van%20Gerven&entry.1292438233=%20%20The%20backpropagation%20algorithm%20remains%20the%20dominant%20and%20most%20successful%20method%0Afor%20training%20deep%20neural%20networks%20%28DNNs%29.%20At%20the%20same%20time%2C%20training%20DNNs%20at%0Ascale%20comes%20at%20a%20significant%20computational%20cost%20and%20therefore%20a%20high%20carbon%0Afootprint.%20Converging%20evidence%20suggests%20that%20input%20decorrelation%20may%20speed%20up%0Adeep%20learning.%20However%2C%20to%20date%2C%20this%20has%20not%20yet%20translated%20into%20substantial%0Aimprovements%20in%20training%20efficiency%20in%20large-scale%20DNNs.%20This%20is%20mainly%20caused%0Aby%20the%20challenge%20of%20enforcing%20fast%20and%20stable%20network-wide%20decorrelation.%20Here%2C%0Awe%20show%20for%20the%20first%20time%20that%20much%20more%20efficient%20training%20of%20very%20deep%0Aneural%20networks%20using%20decorrelated%20backpropagation%20is%20feasible.%20To%20achieve%20this%0Agoal%20we%20made%20use%20of%20a%20novel%20algorithm%20which%20induces%20network-wide%20input%0Adecorrelation%20using%20minimal%20computational%20overhead.%20By%20combining%20this%20algorithm%0Awith%20careful%20optimizations%2C%20we%20obtain%20a%20more%20than%20two-fold%20speed-up%20and%20higher%0Atest%20accuracy%20compared%20to%20backpropagation%20when%20training%20a%2018-layer%20deep%0Aresidual%20network.%20This%20demonstrates%20that%20decorrelation%20provides%20exciting%0Aprospects%20for%20efficient%20deep%20learning%20at%20scale.%0A&entry.1838667208=http%3A//arxiv.org/abs/2405.02385v2&entry.124074799=Read"},
{"title": "ViCor: Bridging Visual Understanding and Commonsense Reasoning with\n  Large Language Models", "author": "Kaiwen Zhou and Kwonjoon Lee and Teruhisa Misu and Xin Eric Wang", "abstract": "  In our work, we explore the synergistic capabilities of pre-trained\nvision-and-language models (VLMs) and large language models (LLMs) on visual\ncommonsense reasoning (VCR) problems. We find that VLMs and LLMs-based decision\npipelines are good at different kinds of VCR problems. Pre-trained VLMs exhibit\nstrong performance for problems involving understanding the literal visual\ncontent, which we noted as visual commonsense understanding (VCU). For problems\nwhere the goal is to infer conclusions beyond image content, which we noted as\nvisual commonsense inference (VCI), VLMs face difficulties, while LLMs, given\nsufficient visual evidence, can use commonsense to infer the answer well. We\nempirically validate this by letting LLMs classify VCR problems into these two\ncategories and show the significant difference between VLM and LLM with image\ncaption decision pipelines on two subproblems. Moreover, we identify a\nchallenge with VLMs' passive perception, which may miss crucial context\ninformation, leading to incorrect reasoning by LLMs. Based on these, we suggest\na collaborative approach, named ViCor, where pre-trained LLMs serve as problem\nclassifiers to analyze the problem category, then either use VLMs to answer the\nquestion directly or actively instruct VLMs to concentrate on and gather\nrelevant visual elements to support potential commonsense inferences. We\nevaluate our framework on two VCR benchmark datasets and outperform all other\nmethods that do not require in-domain fine-tuning.\n", "link": "http://arxiv.org/abs/2310.05872v2", "date": "2024-05-17", "relevancy": 2.1494, "topK": [{"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5543}, {"title": "Flexible Visual Recognition by Evidential Modeling of Confusion and Ignorance", "link": "http://openaccess.thecvf.com/content/ICCV2023/html/Fan_Flexible_Visual_Recognition_by_Evidential_Modeling_of_Confusion_and_Ignorance_ICCV_2023_paper.html", "similarity": 0.5314}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.5098}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20ViCor%3A%20Bridging%20Visual%20Understanding%20and%20Commonsense%20Reasoning%20with%0A%20%20Large%20Language%20Models&body=Title%3A%20ViCor%3A%20Bridging%20Visual%20Understanding%20and%20Commonsense%20Reasoning%20with%0A%20%20Large%20Language%20Models%0AAuthor%3A%20Kaiwen%20Zhou%20and%20Kwonjoon%20Lee%20and%20Teruhisa%20Misu%20and%20Xin%20Eric%20Wang%0AAbstract%3A%20%20%20In%20our%20work%2C%20we%20explore%20the%20synergistic%20capabilities%20of%20pre-trained%0Avision-and-language%20models%20%28VLMs%29%20and%20large%20language%20models%20%28LLMs%29%20on%20visual%0Acommonsense%20reasoning%20%28VCR%29%20problems.%20We%20find%20that%20VLMs%20and%20LLMs-based%20decision%0Apipelines%20are%20good%20at%20different%20kinds%20of%20VCR%20problems.%20Pre-trained%20VLMs%20exhibit%0Astrong%20performance%20for%20problems%20involving%20understanding%20the%20literal%20visual%0Acontent%2C%20which%20we%20noted%20as%20visual%20commonsense%20understanding%20%28VCU%29.%20For%20problems%0Awhere%20the%20goal%20is%20to%20infer%20conclusions%20beyond%20image%20content%2C%20which%20we%20noted%20as%0Avisual%20commonsense%20inference%20%28VCI%29%2C%20VLMs%20face%20difficulties%2C%20while%20LLMs%2C%20given%0Asufficient%20visual%20evidence%2C%20can%20use%20commonsense%20to%20infer%20the%20answer%20well.%20We%0Aempirically%20validate%20this%20by%20letting%20LLMs%20classify%20VCR%20problems%20into%20these%20two%0Acategories%20and%20show%20the%20significant%20difference%20between%20VLM%20and%20LLM%20with%20image%0Acaption%20decision%20pipelines%20on%20two%20subproblems.%20Moreover%2C%20we%20identify%20a%0Achallenge%20with%20VLMs%27%20passive%20perception%2C%20which%20may%20miss%20crucial%20context%0Ainformation%2C%20leading%20to%20incorrect%20reasoning%20by%20LLMs.%20Based%20on%20these%2C%20we%20suggest%0Aa%20collaborative%20approach%2C%20named%20ViCor%2C%20where%20pre-trained%20LLMs%20serve%20as%20problem%0Aclassifiers%20to%20analyze%20the%20problem%20category%2C%20then%20either%20use%20VLMs%20to%20answer%20the%0Aquestion%20directly%20or%20actively%20instruct%20VLMs%20to%20concentrate%20on%20and%20gather%0Arelevant%20visual%20elements%20to%20support%20potential%20commonsense%20inferences.%20We%0Aevaluate%20our%20framework%20on%20two%20VCR%20benchmark%20datasets%20and%20outperform%20all%20other%0Amethods%20that%20do%20not%20require%20in-domain%20fine-tuning.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2310.05872v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DViCor%253A%2520Bridging%2520Visual%2520Understanding%2520and%2520Commonsense%2520Reasoning%2520with%250A%2520%2520Large%2520Language%2520Models%26entry.906535625%3DKaiwen%2520Zhou%2520and%2520Kwonjoon%2520Lee%2520and%2520Teruhisa%2520Misu%2520and%2520Xin%2520Eric%2520Wang%26entry.1292438233%3D%2520%2520In%2520our%2520work%252C%2520we%2520explore%2520the%2520synergistic%2520capabilities%2520of%2520pre-trained%250Avision-and-language%2520models%2520%2528VLMs%2529%2520and%2520large%2520language%2520models%2520%2528LLMs%2529%2520on%2520visual%250Acommonsense%2520reasoning%2520%2528VCR%2529%2520problems.%2520We%2520find%2520that%2520VLMs%2520and%2520LLMs-based%2520decision%250Apipelines%2520are%2520good%2520at%2520different%2520kinds%2520of%2520VCR%2520problems.%2520Pre-trained%2520VLMs%2520exhibit%250Astrong%2520performance%2520for%2520problems%2520involving%2520understanding%2520the%2520literal%2520visual%250Acontent%252C%2520which%2520we%2520noted%2520as%2520visual%2520commonsense%2520understanding%2520%2528VCU%2529.%2520For%2520problems%250Awhere%2520the%2520goal%2520is%2520to%2520infer%2520conclusions%2520beyond%2520image%2520content%252C%2520which%2520we%2520noted%2520as%250Avisual%2520commonsense%2520inference%2520%2528VCI%2529%252C%2520VLMs%2520face%2520difficulties%252C%2520while%2520LLMs%252C%2520given%250Asufficient%2520visual%2520evidence%252C%2520can%2520use%2520commonsense%2520to%2520infer%2520the%2520answer%2520well.%2520We%250Aempirically%2520validate%2520this%2520by%2520letting%2520LLMs%2520classify%2520VCR%2520problems%2520into%2520these%2520two%250Acategories%2520and%2520show%2520the%2520significant%2520difference%2520between%2520VLM%2520and%2520LLM%2520with%2520image%250Acaption%2520decision%2520pipelines%2520on%2520two%2520subproblems.%2520Moreover%252C%2520we%2520identify%2520a%250Achallenge%2520with%2520VLMs%2527%2520passive%2520perception%252C%2520which%2520may%2520miss%2520crucial%2520context%250Ainformation%252C%2520leading%2520to%2520incorrect%2520reasoning%2520by%2520LLMs.%2520Based%2520on%2520these%252C%2520we%2520suggest%250Aa%2520collaborative%2520approach%252C%2520named%2520ViCor%252C%2520where%2520pre-trained%2520LLMs%2520serve%2520as%2520problem%250Aclassifiers%2520to%2520analyze%2520the%2520problem%2520category%252C%2520then%2520either%2520use%2520VLMs%2520to%2520answer%2520the%250Aquestion%2520directly%2520or%2520actively%2520instruct%2520VLMs%2520to%2520concentrate%2520on%2520and%2520gather%250Arelevant%2520visual%2520elements%2520to%2520support%2520potential%2520commonsense%2520inferences.%2520We%250Aevaluate%2520our%2520framework%2520on%2520two%2520VCR%2520benchmark%2520datasets%2520and%2520outperform%2520all%2520other%250Amethods%2520that%2520do%2520not%2520require%2520in-domain%2520fine-tuning.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2310.05872v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=ViCor%3A%20Bridging%20Visual%20Understanding%20and%20Commonsense%20Reasoning%20with%0A%20%20Large%20Language%20Models&entry.906535625=Kaiwen%20Zhou%20and%20Kwonjoon%20Lee%20and%20Teruhisa%20Misu%20and%20Xin%20Eric%20Wang&entry.1292438233=%20%20In%20our%20work%2C%20we%20explore%20the%20synergistic%20capabilities%20of%20pre-trained%0Avision-and-language%20models%20%28VLMs%29%20and%20large%20language%20models%20%28LLMs%29%20on%20visual%0Acommonsense%20reasoning%20%28VCR%29%20problems.%20We%20find%20that%20VLMs%20and%20LLMs-based%20decision%0Apipelines%20are%20good%20at%20different%20kinds%20of%20VCR%20problems.%20Pre-trained%20VLMs%20exhibit%0Astrong%20performance%20for%20problems%20involving%20understanding%20the%20literal%20visual%0Acontent%2C%20which%20we%20noted%20as%20visual%20commonsense%20understanding%20%28VCU%29.%20For%20problems%0Awhere%20the%20goal%20is%20to%20infer%20conclusions%20beyond%20image%20content%2C%20which%20we%20noted%20as%0Avisual%20commonsense%20inference%20%28VCI%29%2C%20VLMs%20face%20difficulties%2C%20while%20LLMs%2C%20given%0Asufficient%20visual%20evidence%2C%20can%20use%20commonsense%20to%20infer%20the%20answer%20well.%20We%0Aempirically%20validate%20this%20by%20letting%20LLMs%20classify%20VCR%20problems%20into%20these%20two%0Acategories%20and%20show%20the%20significant%20difference%20between%20VLM%20and%20LLM%20with%20image%0Acaption%20decision%20pipelines%20on%20two%20subproblems.%20Moreover%2C%20we%20identify%20a%0Achallenge%20with%20VLMs%27%20passive%20perception%2C%20which%20may%20miss%20crucial%20context%0Ainformation%2C%20leading%20to%20incorrect%20reasoning%20by%20LLMs.%20Based%20on%20these%2C%20we%20suggest%0Aa%20collaborative%20approach%2C%20named%20ViCor%2C%20where%20pre-trained%20LLMs%20serve%20as%20problem%0Aclassifiers%20to%20analyze%20the%20problem%20category%2C%20then%20either%20use%20VLMs%20to%20answer%20the%0Aquestion%20directly%20or%20actively%20instruct%20VLMs%20to%20concentrate%20on%20and%20gather%0Arelevant%20visual%20elements%20to%20support%20potential%20commonsense%20inferences.%20We%0Aevaluate%20our%20framework%20on%20two%20VCR%20benchmark%20datasets%20and%20outperform%20all%20other%0Amethods%20that%20do%20not%20require%20in-domain%20fine-tuning.%0A&entry.1838667208=http%3A//arxiv.org/abs/2310.05872v2&entry.124074799=Read"},
{"title": "StackOverflowVQA: Stack Overflow Visual Question Answering Dataset", "author": "Motahhare Mirzaei and Mohammad Javad Pirhadi and Sauleh Eetemadi", "abstract": "  In recent years, people have increasingly used AI to help them with their\nproblems by asking questions on different topics. One of these topics can be\nsoftware-related and programming questions. In this work, we focus on the\nquestions which need the understanding of images in addition to the question\nitself. We introduce the StackOverflowVQA dataset, which includes questions\nfrom StackOverflow that have one or more accompanying images. This is the first\nVQA dataset that focuses on software-related questions and contains multiple\nhuman-generated full-sentence answers. Additionally, we provide a baseline for\nanswering the questions with respect to images in the introduced dataset using\nthe GIT model. All versions of the dataset are available at\nhttps://huggingface.co/mirzaei2114.\n", "link": "http://arxiv.org/abs/2405.10736v1", "date": "2024-05-17", "relevancy": 2.1445, "topK": [{"title": "VirtualModel: Generating Object-ID-retentive Human-object Interaction\n  Image by Diffusion Model for E-commerce Marketing", "link": "http://arxiv.org/abs/2405.09985v1", "similarity": 0.4395}, {"title": "Self-supervised Photographic Image Layout Representation Learning", "link": "http://arxiv.org/abs/2403.03740v1", "similarity": 0.4236}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.4235}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20StackOverflowVQA%3A%20Stack%20Overflow%20Visual%20Question%20Answering%20Dataset&body=Title%3A%20StackOverflowVQA%3A%20Stack%20Overflow%20Visual%20Question%20Answering%20Dataset%0AAuthor%3A%20Motahhare%20Mirzaei%20and%20Mohammad%20Javad%20Pirhadi%20and%20Sauleh%20Eetemadi%0AAbstract%3A%20%20%20In%20recent%20years%2C%20people%20have%20increasingly%20used%20AI%20to%20help%20them%20with%20their%0Aproblems%20by%20asking%20questions%20on%20different%20topics.%20One%20of%20these%20topics%20can%20be%0Asoftware-related%20and%20programming%20questions.%20In%20this%20work%2C%20we%20focus%20on%20the%0Aquestions%20which%20need%20the%20understanding%20of%20images%20in%20addition%20to%20the%20question%0Aitself.%20We%20introduce%20the%20StackOverflowVQA%20dataset%2C%20which%20includes%20questions%0Afrom%20StackOverflow%20that%20have%20one%20or%20more%20accompanying%20images.%20This%20is%20the%20first%0AVQA%20dataset%20that%20focuses%20on%20software-related%20questions%20and%20contains%20multiple%0Ahuman-generated%20full-sentence%20answers.%20Additionally%2C%20we%20provide%20a%20baseline%20for%0Aanswering%20the%20questions%20with%20respect%20to%20images%20in%20the%20introduced%20dataset%20using%0Athe%20GIT%20model.%20All%20versions%20of%20the%20dataset%20are%20available%20at%0Ahttps%3A//huggingface.co/mirzaei2114.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2405.10736v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DStackOverflowVQA%253A%2520Stack%2520Overflow%2520Visual%2520Question%2520Answering%2520Dataset%26entry.906535625%3DMotahhare%2520Mirzaei%2520and%2520Mohammad%2520Javad%2520Pirhadi%2520and%2520Sauleh%2520Eetemadi%26entry.1292438233%3D%2520%2520In%2520recent%2520years%252C%2520people%2520have%2520increasingly%2520used%2520AI%2520to%2520help%2520them%2520with%2520their%250Aproblems%2520by%2520asking%2520questions%2520on%2520different%2520topics.%2520One%2520of%2520these%2520topics%2520can%2520be%250Asoftware-related%2520and%2520programming%2520questions.%2520In%2520this%2520work%252C%2520we%2520focus%2520on%2520the%250Aquestions%2520which%2520need%2520the%2520understanding%2520of%2520images%2520in%2520addition%2520to%2520the%2520question%250Aitself.%2520We%2520introduce%2520the%2520StackOverflowVQA%2520dataset%252C%2520which%2520includes%2520questions%250Afrom%2520StackOverflow%2520that%2520have%2520one%2520or%2520more%2520accompanying%2520images.%2520This%2520is%2520the%2520first%250AVQA%2520dataset%2520that%2520focuses%2520on%2520software-related%2520questions%2520and%2520contains%2520multiple%250Ahuman-generated%2520full-sentence%2520answers.%2520Additionally%252C%2520we%2520provide%2520a%2520baseline%2520for%250Aanswering%2520the%2520questions%2520with%2520respect%2520to%2520images%2520in%2520the%2520introduced%2520dataset%2520using%250Athe%2520GIT%2520model.%2520All%2520versions%2520of%2520the%2520dataset%2520are%2520available%2520at%250Ahttps%253A//huggingface.co/mirzaei2114.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2405.10736v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=StackOverflowVQA%3A%20Stack%20Overflow%20Visual%20Question%20Answering%20Dataset&entry.906535625=Motahhare%20Mirzaei%20and%20Mohammad%20Javad%20Pirhadi%20and%20Sauleh%20Eetemadi&entry.1292438233=%20%20In%20recent%20years%2C%20people%20have%20increasingly%20used%20AI%20to%20help%20them%20with%20their%0Aproblems%20by%20asking%20questions%20on%20different%20topics.%20One%20of%20these%20topics%20can%20be%0Asoftware-related%20and%20programming%20questions.%20In%20this%20work%2C%20we%20focus%20on%20the%0Aquestions%20which%20need%20the%20understanding%20of%20images%20in%20addition%20to%20the%20question%0Aitself.%20We%20introduce%20the%20StackOverflowVQA%20dataset%2C%20which%20includes%20questions%0Afrom%20StackOverflow%20that%20have%20one%20or%20more%20accompanying%20images.%20This%20is%20the%20first%0AVQA%20dataset%20that%20focuses%20on%20software-related%20questions%20and%20contains%20multiple%0Ahuman-generated%20full-sentence%20answers.%20Additionally%2C%20we%20provide%20a%20baseline%20for%0Aanswering%20the%20questions%20with%20respect%20to%20images%20in%20the%20introduced%20dataset%20using%0Athe%20GIT%20model.%20All%20versions%20of%20the%20dataset%20are%20available%20at%0Ahttps%3A//huggingface.co/mirzaei2114.%0A&entry.1838667208=http%3A//arxiv.org/abs/2405.10736v1&entry.124074799=Read"},
{"title": "HelixFold-Multimer: Elevating Protein Complex Structure Prediction to\n  New Heights", "author": "Xiaomin Fang and Jie Gao and Jing Hu and Lihang Liu and Yang Xue and Xiaonan Zhang and Kunrui Zhu", "abstract": "  While monomer protein structure prediction tools boast impressive accuracy,\nthe prediction of protein complex structures remains a daunting challenge in\nthe field. This challenge is particularly pronounced in scenarios involving\ncomplexes with protein chains from different species, such as antigen-antibody\ninteractions, where accuracy often falls short. Limited by the accuracy of\ncomplex prediction, tasks based on precise protein-protein interaction analysis\nalso face obstacles. In this report, we highlight the ongoing advancements of\nour protein complex structure prediction model, HelixFold-Multimer,\nunderscoring its enhanced performance. HelixFold-Multimer provides precise\npredictions for diverse protein complex structures, especially in therapeutic\nprotein interactions. Notably, HelixFold-Multimer achieves remarkable success\nin antigen-antibody and peptide-protein structure prediction, greatly\nsurpassing AlphaFold 3. HelixFold-Multimer is now available for public use on\nthe PaddleHelix platform, offering both a general version and an\nantigen-antibody version. Researchers can conveniently access and utilize this\nservice for their development needs.\n", "link": "http://arxiv.org/abs/2404.10260v2", "date": "2024-05-17", "relevancy": 2.1391, "topK": [{"title": "CAT3D: Create Anything in 3D with Multi-View Diffusion Models", "link": "http://arxiv.org/abs/2405.10314v1", "similarity": 0.4298}, {"title": "CAT3D: Create Anything in 3D with Multi-View Diffusion Models", "link": "http://arxiv.org/abs/2405.10314v1", "similarity": 0.4298}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.4239}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20HelixFold-Multimer%3A%20Elevating%20Protein%20Complex%20Structure%20Prediction%20to%0A%20%20New%20Heights&body=Title%3A%20HelixFold-Multimer%3A%20Elevating%20Protein%20Complex%20Structure%20Prediction%20to%0A%20%20New%20Heights%0AAuthor%3A%20Xiaomin%20Fang%20and%20Jie%20Gao%20and%20Jing%20Hu%20and%20Lihang%20Liu%20and%20Yang%20Xue%20and%20Xiaonan%20Zhang%20and%20Kunrui%20Zhu%0AAbstract%3A%20%20%20While%20monomer%20protein%20structure%20prediction%20tools%20boast%20impressive%20accuracy%2C%0Athe%20prediction%20of%20protein%20complex%20structures%20remains%20a%20daunting%20challenge%20in%0Athe%20field.%20This%20challenge%20is%20particularly%20pronounced%20in%20scenarios%20involving%0Acomplexes%20with%20protein%20chains%20from%20different%20species%2C%20such%20as%20antigen-antibody%0Ainteractions%2C%20where%20accuracy%20often%20falls%20short.%20Limited%20by%20the%20accuracy%20of%0Acomplex%20prediction%2C%20tasks%20based%20on%20precise%20protein-protein%20interaction%20analysis%0Aalso%20face%20obstacles.%20In%20this%20report%2C%20we%20highlight%20the%20ongoing%20advancements%20of%0Aour%20protein%20complex%20structure%20prediction%20model%2C%20HelixFold-Multimer%2C%0Aunderscoring%20its%20enhanced%20performance.%20HelixFold-Multimer%20provides%20precise%0Apredictions%20for%20diverse%20protein%20complex%20structures%2C%20especially%20in%20therapeutic%0Aprotein%20interactions.%20Notably%2C%20HelixFold-Multimer%20achieves%20remarkable%20success%0Ain%20antigen-antibody%20and%20peptide-protein%20structure%20prediction%2C%20greatly%0Asurpassing%20AlphaFold%203.%20HelixFold-Multimer%20is%20now%20available%20for%20public%20use%20on%0Athe%20PaddleHelix%20platform%2C%20offering%20both%20a%20general%20version%20and%20an%0Aantigen-antibody%20version.%20Researchers%20can%20conveniently%20access%20and%20utilize%20this%0Aservice%20for%20their%20development%20needs.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2404.10260v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DHelixFold-Multimer%253A%2520Elevating%2520Protein%2520Complex%2520Structure%2520Prediction%2520to%250A%2520%2520New%2520Heights%26entry.906535625%3DXiaomin%2520Fang%2520and%2520Jie%2520Gao%2520and%2520Jing%2520Hu%2520and%2520Lihang%2520Liu%2520and%2520Yang%2520Xue%2520and%2520Xiaonan%2520Zhang%2520and%2520Kunrui%2520Zhu%26entry.1292438233%3D%2520%2520While%2520monomer%2520protein%2520structure%2520prediction%2520tools%2520boast%2520impressive%2520accuracy%252C%250Athe%2520prediction%2520of%2520protein%2520complex%2520structures%2520remains%2520a%2520daunting%2520challenge%2520in%250Athe%2520field.%2520This%2520challenge%2520is%2520particularly%2520pronounced%2520in%2520scenarios%2520involving%250Acomplexes%2520with%2520protein%2520chains%2520from%2520different%2520species%252C%2520such%2520as%2520antigen-antibody%250Ainteractions%252C%2520where%2520accuracy%2520often%2520falls%2520short.%2520Limited%2520by%2520the%2520accuracy%2520of%250Acomplex%2520prediction%252C%2520tasks%2520based%2520on%2520precise%2520protein-protein%2520interaction%2520analysis%250Aalso%2520face%2520obstacles.%2520In%2520this%2520report%252C%2520we%2520highlight%2520the%2520ongoing%2520advancements%2520of%250Aour%2520protein%2520complex%2520structure%2520prediction%2520model%252C%2520HelixFold-Multimer%252C%250Aunderscoring%2520its%2520enhanced%2520performance.%2520HelixFold-Multimer%2520provides%2520precise%250Apredictions%2520for%2520diverse%2520protein%2520complex%2520structures%252C%2520especially%2520in%2520therapeutic%250Aprotein%2520interactions.%2520Notably%252C%2520HelixFold-Multimer%2520achieves%2520remarkable%2520success%250Ain%2520antigen-antibody%2520and%2520peptide-protein%2520structure%2520prediction%252C%2520greatly%250Asurpassing%2520AlphaFold%25203.%2520HelixFold-Multimer%2520is%2520now%2520available%2520for%2520public%2520use%2520on%250Athe%2520PaddleHelix%2520platform%252C%2520offering%2520both%2520a%2520general%2520version%2520and%2520an%250Aantigen-antibody%2520version.%2520Researchers%2520can%2520conveniently%2520access%2520and%2520utilize%2520this%250Aservice%2520for%2520their%2520development%2520needs.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2404.10260v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=HelixFold-Multimer%3A%20Elevating%20Protein%20Complex%20Structure%20Prediction%20to%0A%20%20New%20Heights&entry.906535625=Xiaomin%20Fang%20and%20Jie%20Gao%20and%20Jing%20Hu%20and%20Lihang%20Liu%20and%20Yang%20Xue%20and%20Xiaonan%20Zhang%20and%20Kunrui%20Zhu&entry.1292438233=%20%20While%20monomer%20protein%20structure%20prediction%20tools%20boast%20impressive%20accuracy%2C%0Athe%20prediction%20of%20protein%20complex%20structures%20remains%20a%20daunting%20challenge%20in%0Athe%20field.%20This%20challenge%20is%20particularly%20pronounced%20in%20scenarios%20involving%0Acomplexes%20with%20protein%20chains%20from%20different%20species%2C%20such%20as%20antigen-antibody%0Ainteractions%2C%20where%20accuracy%20often%20falls%20short.%20Limited%20by%20the%20accuracy%20of%0Acomplex%20prediction%2C%20tasks%20based%20on%20precise%20protein-protein%20interaction%20analysis%0Aalso%20face%20obstacles.%20In%20this%20report%2C%20we%20highlight%20the%20ongoing%20advancements%20of%0Aour%20protein%20complex%20structure%20prediction%20model%2C%20HelixFold-Multimer%2C%0Aunderscoring%20its%20enhanced%20performance.%20HelixFold-Multimer%20provides%20precise%0Apredictions%20for%20diverse%20protein%20complex%20structures%2C%20especially%20in%20therapeutic%0Aprotein%20interactions.%20Notably%2C%20HelixFold-Multimer%20achieves%20remarkable%20success%0Ain%20antigen-antibody%20and%20peptide-protein%20structure%20prediction%2C%20greatly%0Asurpassing%20AlphaFold%203.%20HelixFold-Multimer%20is%20now%20available%20for%20public%20use%20on%0Athe%20PaddleHelix%20platform%2C%20offering%20both%20a%20general%20version%20and%20an%0Aantigen-antibody%20version.%20Researchers%20can%20conveniently%20access%20and%20utilize%20this%0Aservice%20for%20their%20development%20needs.%0A&entry.1838667208=http%3A//arxiv.org/abs/2404.10260v2&entry.124074799=Read"},
{"title": "Generative modeling through internal high-dimensional chaotic activity", "author": "Samantha J. Fournier and Pierfrancesco Urbani", "abstract": "  Generative modeling aims at producing new datapoints whose statistical\nproperties resemble the ones in a training dataset. In recent years, there has\nbeen a burst of machine learning techniques and settings that can achieve this\ngoal with remarkable performances. In most of these settings, one uses the\ntraining dataset in conjunction with noise, which is added as a source of\nstatistical variability and is essential for the generative task. Here, we\nexplore the idea of using internal chaotic dynamics in high-dimensional chaotic\nsystems as a way to generate new datapoints from a training dataset. We show\nthat simple learning rules can achieve this goal within a set of vanilla\narchitectures and characterize the quality of the generated datapoints through\nstandard accuracy measures.\n", "link": "http://arxiv.org/abs/2405.10822v1", "date": "2024-05-17", "relevancy": 2.1326, "topK": [{"title": "Genie: Generative Interactive Environments", "link": "http://arxiv.org/abs/2402.15391v1", "similarity": 0.58}, {"title": "Scaling Rectified Flow Transformers for High-Resolution Image Synthesis", "link": "http://arxiv.org/abs/2403.03206v1", "similarity": 0.514}, {"title": "UGG: Unified Generative Grasping", "link": "https://arxiv.org/abs/2311.16917", "similarity": 0.494}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Generative%20modeling%20through%20internal%20high-dimensional%20chaotic%20activity&body=Title%3A%20Generative%20modeling%20through%20internal%20high-dimensional%20chaotic%20activity%0AAuthor%3A%20Samantha%20J.%20Fournier%20and%20Pierfrancesco%20Urbani%0AAbstract%3A%20%20%20Generative%20modeling%20aims%20at%20producing%20new%20datapoints%20whose%20statistical%0Aproperties%20resemble%20the%20ones%20in%20a%20training%20dataset.%20In%20recent%20years%2C%20there%20has%0Abeen%20a%20burst%20of%20machine%20learning%20techniques%20and%20settings%20that%20can%20achieve%20this%0Agoal%20with%20remarkable%20performances.%20In%20most%20of%20these%20settings%2C%20one%20uses%20the%0Atraining%20dataset%20in%20conjunction%20with%20noise%2C%20which%20is%20added%20as%20a%20source%20of%0Astatistical%20variability%20and%20is%20essential%20for%20the%20generative%20task.%20Here%2C%20we%0Aexplore%20the%20idea%20of%20using%20internal%20chaotic%20dynamics%20in%20high-dimensional%20chaotic%0Asystems%20as%20a%20way%20to%20generate%20new%20datapoints%20from%20a%20training%20dataset.%20We%20show%0Athat%20simple%20learning%20rules%20can%20achieve%20this%20goal%20within%20a%20set%20of%20vanilla%0Aarchitectures%20and%20characterize%20the%20quality%20of%20the%20generated%20datapoints%20through%0Astandard%20accuracy%20measures.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2405.10822v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DGenerative%2520modeling%2520through%2520internal%2520high-dimensional%2520chaotic%2520activity%26entry.906535625%3DSamantha%2520J.%2520Fournier%2520and%2520Pierfrancesco%2520Urbani%26entry.1292438233%3D%2520%2520Generative%2520modeling%2520aims%2520at%2520producing%2520new%2520datapoints%2520whose%2520statistical%250Aproperties%2520resemble%2520the%2520ones%2520in%2520a%2520training%2520dataset.%2520In%2520recent%2520years%252C%2520there%2520has%250Abeen%2520a%2520burst%2520of%2520machine%2520learning%2520techniques%2520and%2520settings%2520that%2520can%2520achieve%2520this%250Agoal%2520with%2520remarkable%2520performances.%2520In%2520most%2520of%2520these%2520settings%252C%2520one%2520uses%2520the%250Atraining%2520dataset%2520in%2520conjunction%2520with%2520noise%252C%2520which%2520is%2520added%2520as%2520a%2520source%2520of%250Astatistical%2520variability%2520and%2520is%2520essential%2520for%2520the%2520generative%2520task.%2520Here%252C%2520we%250Aexplore%2520the%2520idea%2520of%2520using%2520internal%2520chaotic%2520dynamics%2520in%2520high-dimensional%2520chaotic%250Asystems%2520as%2520a%2520way%2520to%2520generate%2520new%2520datapoints%2520from%2520a%2520training%2520dataset.%2520We%2520show%250Athat%2520simple%2520learning%2520rules%2520can%2520achieve%2520this%2520goal%2520within%2520a%2520set%2520of%2520vanilla%250Aarchitectures%2520and%2520characterize%2520the%2520quality%2520of%2520the%2520generated%2520datapoints%2520through%250Astandard%2520accuracy%2520measures.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2405.10822v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Generative%20modeling%20through%20internal%20high-dimensional%20chaotic%20activity&entry.906535625=Samantha%20J.%20Fournier%20and%20Pierfrancesco%20Urbani&entry.1292438233=%20%20Generative%20modeling%20aims%20at%20producing%20new%20datapoints%20whose%20statistical%0Aproperties%20resemble%20the%20ones%20in%20a%20training%20dataset.%20In%20recent%20years%2C%20there%20has%0Abeen%20a%20burst%20of%20machine%20learning%20techniques%20and%20settings%20that%20can%20achieve%20this%0Agoal%20with%20remarkable%20performances.%20In%20most%20of%20these%20settings%2C%20one%20uses%20the%0Atraining%20dataset%20in%20conjunction%20with%20noise%2C%20which%20is%20added%20as%20a%20source%20of%0Astatistical%20variability%20and%20is%20essential%20for%20the%20generative%20task.%20Here%2C%20we%0Aexplore%20the%20idea%20of%20using%20internal%20chaotic%20dynamics%20in%20high-dimensional%20chaotic%0Asystems%20as%20a%20way%20to%20generate%20new%20datapoints%20from%20a%20training%20dataset.%20We%20show%0Athat%20simple%20learning%20rules%20can%20achieve%20this%20goal%20within%20a%20set%20of%20vanilla%0Aarchitectures%20and%20characterize%20the%20quality%20of%20the%20generated%20datapoints%20through%0Astandard%20accuracy%20measures.%0A&entry.1838667208=http%3A//arxiv.org/abs/2405.10822v1&entry.124074799=Read"},
{"title": "Prospective Role of Foundation Models in Advancing Autonomous Vehicles", "author": "Jianhua Wu and Bingzhao Gao and Jincheng Gao and Jianhao Yu and Hongqing Chu and Qiankun Yu and Xun Gong and Yi Chang and H. Eric Tseng and Hong Chen and Jie Chen", "abstract": "  With the development of artificial intelligence and breakthroughs in deep\nlearning, large-scale Foundation Models (FMs), such as GPT, Sora, etc., have\nachieved remarkable results in many fields including natural language\nprocessing and computer vision. The application of FMs in autonomous driving\nholds considerable promise. For example, they can contribute to enhancing scene\nunderstanding and reasoning. By pre-training on rich linguistic and visual\ndata, FMs can understand and interpret various elements in a driving scene, and\nprovide cognitive reasoning to give linguistic and action instructions for\ndriving decisions and planning. Furthermore, FMs can augment data based on the\nunderstanding of driving scenarios to provide feasible scenes of those rare\noccurrences in the long tail distribution that are unlikely to be encountered\nduring routine driving and data collection. The enhancement can subsequently\nlead to improvement in the accuracy and reliability of autonomous driving\nsystems. Another testament to the potential of FMs' applications lies in World\nModels, exemplified by the DREAMER series, which showcases the ability to\ncomprehend physical laws and dynamics. Learning from massive data under the\nparadigm of self-supervised learning, World Model can generate unseen yet\nplausible driving environments, facilitating the enhancement in the prediction\nof road users' behaviors and the off-line training of driving strategies. In\nthis paper, we synthesize the applications and future trends of FMs in\nautonomous driving. By utilizing the powerful capabilities of FMs, we strive to\ntackle the potential issues stemming from the long-tail distribution in\nautonomous driving, consequently advancing overall safety in this domain.\n", "link": "http://arxiv.org/abs/2405.02288v2", "date": "2024-05-17", "relevancy": 2.1255, "topK": [{"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.5654}, {"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.5337}, {"title": "Genie: Generative Interactive Environments", "link": "http://arxiv.org/abs/2402.15391v1", "similarity": 0.5155}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Prospective%20Role%20of%20Foundation%20Models%20in%20Advancing%20Autonomous%20Vehicles&body=Title%3A%20Prospective%20Role%20of%20Foundation%20Models%20in%20Advancing%20Autonomous%20Vehicles%0AAuthor%3A%20Jianhua%20Wu%20and%20Bingzhao%20Gao%20and%20Jincheng%20Gao%20and%20Jianhao%20Yu%20and%20Hongqing%20Chu%20and%20Qiankun%20Yu%20and%20Xun%20Gong%20and%20Yi%20Chang%20and%20H.%20Eric%20Tseng%20and%20Hong%20Chen%20and%20Jie%20Chen%0AAbstract%3A%20%20%20With%20the%20development%20of%20artificial%20intelligence%20and%20breakthroughs%20in%20deep%0Alearning%2C%20large-scale%20Foundation%20Models%20%28FMs%29%2C%20such%20as%20GPT%2C%20Sora%2C%20etc.%2C%20have%0Aachieved%20remarkable%20results%20in%20many%20fields%20including%20natural%20language%0Aprocessing%20and%20computer%20vision.%20The%20application%20of%20FMs%20in%20autonomous%20driving%0Aholds%20considerable%20promise.%20For%20example%2C%20they%20can%20contribute%20to%20enhancing%20scene%0Aunderstanding%20and%20reasoning.%20By%20pre-training%20on%20rich%20linguistic%20and%20visual%0Adata%2C%20FMs%20can%20understand%20and%20interpret%20various%20elements%20in%20a%20driving%20scene%2C%20and%0Aprovide%20cognitive%20reasoning%20to%20give%20linguistic%20and%20action%20instructions%20for%0Adriving%20decisions%20and%20planning.%20Furthermore%2C%20FMs%20can%20augment%20data%20based%20on%20the%0Aunderstanding%20of%20driving%20scenarios%20to%20provide%20feasible%20scenes%20of%20those%20rare%0Aoccurrences%20in%20the%20long%20tail%20distribution%20that%20are%20unlikely%20to%20be%20encountered%0Aduring%20routine%20driving%20and%20data%20collection.%20The%20enhancement%20can%20subsequently%0Alead%20to%20improvement%20in%20the%20accuracy%20and%20reliability%20of%20autonomous%20driving%0Asystems.%20Another%20testament%20to%20the%20potential%20of%20FMs%27%20applications%20lies%20in%20World%0AModels%2C%20exemplified%20by%20the%20DREAMER%20series%2C%20which%20showcases%20the%20ability%20to%0Acomprehend%20physical%20laws%20and%20dynamics.%20Learning%20from%20massive%20data%20under%20the%0Aparadigm%20of%20self-supervised%20learning%2C%20World%20Model%20can%20generate%20unseen%20yet%0Aplausible%20driving%20environments%2C%20facilitating%20the%20enhancement%20in%20the%20prediction%0Aof%20road%20users%27%20behaviors%20and%20the%20off-line%20training%20of%20driving%20strategies.%20In%0Athis%20paper%2C%20we%20synthesize%20the%20applications%20and%20future%20trends%20of%20FMs%20in%0Aautonomous%20driving.%20By%20utilizing%20the%20powerful%20capabilities%20of%20FMs%2C%20we%20strive%20to%0Atackle%20the%20potential%20issues%20stemming%20from%20the%20long-tail%20distribution%20in%0Aautonomous%20driving%2C%20consequently%20advancing%20overall%20safety%20in%20this%20domain.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2405.02288v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DProspective%2520Role%2520of%2520Foundation%2520Models%2520in%2520Advancing%2520Autonomous%2520Vehicles%26entry.906535625%3DJianhua%2520Wu%2520and%2520Bingzhao%2520Gao%2520and%2520Jincheng%2520Gao%2520and%2520Jianhao%2520Yu%2520and%2520Hongqing%2520Chu%2520and%2520Qiankun%2520Yu%2520and%2520Xun%2520Gong%2520and%2520Yi%2520Chang%2520and%2520H.%2520Eric%2520Tseng%2520and%2520Hong%2520Chen%2520and%2520Jie%2520Chen%26entry.1292438233%3D%2520%2520With%2520the%2520development%2520of%2520artificial%2520intelligence%2520and%2520breakthroughs%2520in%2520deep%250Alearning%252C%2520large-scale%2520Foundation%2520Models%2520%2528FMs%2529%252C%2520such%2520as%2520GPT%252C%2520Sora%252C%2520etc.%252C%2520have%250Aachieved%2520remarkable%2520results%2520in%2520many%2520fields%2520including%2520natural%2520language%250Aprocessing%2520and%2520computer%2520vision.%2520The%2520application%2520of%2520FMs%2520in%2520autonomous%2520driving%250Aholds%2520considerable%2520promise.%2520For%2520example%252C%2520they%2520can%2520contribute%2520to%2520enhancing%2520scene%250Aunderstanding%2520and%2520reasoning.%2520By%2520pre-training%2520on%2520rich%2520linguistic%2520and%2520visual%250Adata%252C%2520FMs%2520can%2520understand%2520and%2520interpret%2520various%2520elements%2520in%2520a%2520driving%2520scene%252C%2520and%250Aprovide%2520cognitive%2520reasoning%2520to%2520give%2520linguistic%2520and%2520action%2520instructions%2520for%250Adriving%2520decisions%2520and%2520planning.%2520Furthermore%252C%2520FMs%2520can%2520augment%2520data%2520based%2520on%2520the%250Aunderstanding%2520of%2520driving%2520scenarios%2520to%2520provide%2520feasible%2520scenes%2520of%2520those%2520rare%250Aoccurrences%2520in%2520the%2520long%2520tail%2520distribution%2520that%2520are%2520unlikely%2520to%2520be%2520encountered%250Aduring%2520routine%2520driving%2520and%2520data%2520collection.%2520The%2520enhancement%2520can%2520subsequently%250Alead%2520to%2520improvement%2520in%2520the%2520accuracy%2520and%2520reliability%2520of%2520autonomous%2520driving%250Asystems.%2520Another%2520testament%2520to%2520the%2520potential%2520of%2520FMs%2527%2520applications%2520lies%2520in%2520World%250AModels%252C%2520exemplified%2520by%2520the%2520DREAMER%2520series%252C%2520which%2520showcases%2520the%2520ability%2520to%250Acomprehend%2520physical%2520laws%2520and%2520dynamics.%2520Learning%2520from%2520massive%2520data%2520under%2520the%250Aparadigm%2520of%2520self-supervised%2520learning%252C%2520World%2520Model%2520can%2520generate%2520unseen%2520yet%250Aplausible%2520driving%2520environments%252C%2520facilitating%2520the%2520enhancement%2520in%2520the%2520prediction%250Aof%2520road%2520users%2527%2520behaviors%2520and%2520the%2520off-line%2520training%2520of%2520driving%2520strategies.%2520In%250Athis%2520paper%252C%2520we%2520synthesize%2520the%2520applications%2520and%2520future%2520trends%2520of%2520FMs%2520in%250Aautonomous%2520driving.%2520By%2520utilizing%2520the%2520powerful%2520capabilities%2520of%2520FMs%252C%2520we%2520strive%2520to%250Atackle%2520the%2520potential%2520issues%2520stemming%2520from%2520the%2520long-tail%2520distribution%2520in%250Aautonomous%2520driving%252C%2520consequently%2520advancing%2520overall%2520safety%2520in%2520this%2520domain.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2405.02288v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Prospective%20Role%20of%20Foundation%20Models%20in%20Advancing%20Autonomous%20Vehicles&entry.906535625=Jianhua%20Wu%20and%20Bingzhao%20Gao%20and%20Jincheng%20Gao%20and%20Jianhao%20Yu%20and%20Hongqing%20Chu%20and%20Qiankun%20Yu%20and%20Xun%20Gong%20and%20Yi%20Chang%20and%20H.%20Eric%20Tseng%20and%20Hong%20Chen%20and%20Jie%20Chen&entry.1292438233=%20%20With%20the%20development%20of%20artificial%20intelligence%20and%20breakthroughs%20in%20deep%0Alearning%2C%20large-scale%20Foundation%20Models%20%28FMs%29%2C%20such%20as%20GPT%2C%20Sora%2C%20etc.%2C%20have%0Aachieved%20remarkable%20results%20in%20many%20fields%20including%20natural%20language%0Aprocessing%20and%20computer%20vision.%20The%20application%20of%20FMs%20in%20autonomous%20driving%0Aholds%20considerable%20promise.%20For%20example%2C%20they%20can%20contribute%20to%20enhancing%20scene%0Aunderstanding%20and%20reasoning.%20By%20pre-training%20on%20rich%20linguistic%20and%20visual%0Adata%2C%20FMs%20can%20understand%20and%20interpret%20various%20elements%20in%20a%20driving%20scene%2C%20and%0Aprovide%20cognitive%20reasoning%20to%20give%20linguistic%20and%20action%20instructions%20for%0Adriving%20decisions%20and%20planning.%20Furthermore%2C%20FMs%20can%20augment%20data%20based%20on%20the%0Aunderstanding%20of%20driving%20scenarios%20to%20provide%20feasible%20scenes%20of%20those%20rare%0Aoccurrences%20in%20the%20long%20tail%20distribution%20that%20are%20unlikely%20to%20be%20encountered%0Aduring%20routine%20driving%20and%20data%20collection.%20The%20enhancement%20can%20subsequently%0Alead%20to%20improvement%20in%20the%20accuracy%20and%20reliability%20of%20autonomous%20driving%0Asystems.%20Another%20testament%20to%20the%20potential%20of%20FMs%27%20applications%20lies%20in%20World%0AModels%2C%20exemplified%20by%20the%20DREAMER%20series%2C%20which%20showcases%20the%20ability%20to%0Acomprehend%20physical%20laws%20and%20dynamics.%20Learning%20from%20massive%20data%20under%20the%0Aparadigm%20of%20self-supervised%20learning%2C%20World%20Model%20can%20generate%20unseen%20yet%0Aplausible%20driving%20environments%2C%20facilitating%20the%20enhancement%20in%20the%20prediction%0Aof%20road%20users%27%20behaviors%20and%20the%20off-line%20training%20of%20driving%20strategies.%20In%0Athis%20paper%2C%20we%20synthesize%20the%20applications%20and%20future%20trends%20of%20FMs%20in%0Aautonomous%20driving.%20By%20utilizing%20the%20powerful%20capabilities%20of%20FMs%2C%20we%20strive%20to%0Atackle%20the%20potential%20issues%20stemming%20from%20the%20long-tail%20distribution%20in%0Aautonomous%20driving%2C%20consequently%20advancing%20overall%20safety%20in%20this%20domain.%0A&entry.1838667208=http%3A//arxiv.org/abs/2405.02288v2&entry.124074799=Read"},
{"title": "Pose2Gest: A Few-Shot Model-Free Approach Applied In South Indian\n  Classical Dance Gesture Recognition", "author": "Kavitha Raju and Nandini J. Warrier and Manu Madhavan and Selvi C. and Arun B. Warrier and Thulasi Kumar", "abstract": "  The classical dances from India utilize a set of hand gestures known as\nMudras, serving as the foundational elements of its posture vocabulary.\nIdentifying these mudras represents a primary task in digitizing the dance\nperformances. With Kathakali, a dance-drama, as the focus, this work addresses\nmudra recognition by framing it as a 24-class classification problem and\nproposes a novel vector-similarity-based approach leveraging pose estimation\ntechniques. This method obviates the need for extensive training or\nfine-tuning, thus mitigating the issue of limited data availability common in\nsimilar AI applications. Achieving an accuracy rate of 92%, our approach\ndemonstrates comparable or superior performance to existing\nmodel-training-based methodologies in this domain. Notably, it remains\neffective even with small datasets comprising just 1 or 5 samples, albeit with\na slightly diminished performance. Furthermore, our system supports processing\nimages, videos, and real-time streams, accommodating both hand-cropped and\nfull-body images. As part of this research, we have curated and released a\npublicly accessible Hasta Mudra dataset, which applies to multiple South Indian\nart forms including Kathakali. The implementation of the proposed method is\nalso made available as a web application.\n", "link": "http://arxiv.org/abs/2404.11205v2", "date": "2024-05-17", "relevancy": 2.1193, "topK": [{"title": "PoseAnimate: Zero-shot high fidelity pose controllable character\n  animation", "link": "http://arxiv.org/abs/2404.13680v2", "similarity": 0.573}, {"title": "UGG: Unified Generative Grasping", "link": "https://arxiv.org/abs/2311.16917", "similarity": 0.5044}, {"title": "Multimodal-Conditioned Latent Diffusion Models for Fashion Image Editing", "link": "http://arxiv.org/abs/2403.14828v2", "similarity": 0.4853}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Pose2Gest%3A%20A%20Few-Shot%20Model-Free%20Approach%20Applied%20In%20South%20Indian%0A%20%20Classical%20Dance%20Gesture%20Recognition&body=Title%3A%20Pose2Gest%3A%20A%20Few-Shot%20Model-Free%20Approach%20Applied%20In%20South%20Indian%0A%20%20Classical%20Dance%20Gesture%20Recognition%0AAuthor%3A%20Kavitha%20Raju%20and%20Nandini%20J.%20Warrier%20and%20Manu%20Madhavan%20and%20Selvi%20C.%20and%20Arun%20B.%20Warrier%20and%20Thulasi%20Kumar%0AAbstract%3A%20%20%20The%20classical%20dances%20from%20India%20utilize%20a%20set%20of%20hand%20gestures%20known%20as%0AMudras%2C%20serving%20as%20the%20foundational%20elements%20of%20its%20posture%20vocabulary.%0AIdentifying%20these%20mudras%20represents%20a%20primary%20task%20in%20digitizing%20the%20dance%0Aperformances.%20With%20Kathakali%2C%20a%20dance-drama%2C%20as%20the%20focus%2C%20this%20work%20addresses%0Amudra%20recognition%20by%20framing%20it%20as%20a%2024-class%20classification%20problem%20and%0Aproposes%20a%20novel%20vector-similarity-based%20approach%20leveraging%20pose%20estimation%0Atechniques.%20This%20method%20obviates%20the%20need%20for%20extensive%20training%20or%0Afine-tuning%2C%20thus%20mitigating%20the%20issue%20of%20limited%20data%20availability%20common%20in%0Asimilar%20AI%20applications.%20Achieving%20an%20accuracy%20rate%20of%2092%25%2C%20our%20approach%0Ademonstrates%20comparable%20or%20superior%20performance%20to%20existing%0Amodel-training-based%20methodologies%20in%20this%20domain.%20Notably%2C%20it%20remains%0Aeffective%20even%20with%20small%20datasets%20comprising%20just%201%20or%205%20samples%2C%20albeit%20with%0Aa%20slightly%20diminished%20performance.%20Furthermore%2C%20our%20system%20supports%20processing%0Aimages%2C%20videos%2C%20and%20real-time%20streams%2C%20accommodating%20both%20hand-cropped%20and%0Afull-body%20images.%20As%20part%20of%20this%20research%2C%20we%20have%20curated%20and%20released%20a%0Apublicly%20accessible%20Hasta%20Mudra%20dataset%2C%20which%20applies%20to%20multiple%20South%20Indian%0Aart%20forms%20including%20Kathakali.%20The%20implementation%20of%20the%20proposed%20method%20is%0Aalso%20made%20available%20as%20a%20web%20application.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2404.11205v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DPose2Gest%253A%2520A%2520Few-Shot%2520Model-Free%2520Approach%2520Applied%2520In%2520South%2520Indian%250A%2520%2520Classical%2520Dance%2520Gesture%2520Recognition%26entry.906535625%3DKavitha%2520Raju%2520and%2520Nandini%2520J.%2520Warrier%2520and%2520Manu%2520Madhavan%2520and%2520Selvi%2520C.%2520and%2520Arun%2520B.%2520Warrier%2520and%2520Thulasi%2520Kumar%26entry.1292438233%3D%2520%2520The%2520classical%2520dances%2520from%2520India%2520utilize%2520a%2520set%2520of%2520hand%2520gestures%2520known%2520as%250AMudras%252C%2520serving%2520as%2520the%2520foundational%2520elements%2520of%2520its%2520posture%2520vocabulary.%250AIdentifying%2520these%2520mudras%2520represents%2520a%2520primary%2520task%2520in%2520digitizing%2520the%2520dance%250Aperformances.%2520With%2520Kathakali%252C%2520a%2520dance-drama%252C%2520as%2520the%2520focus%252C%2520this%2520work%2520addresses%250Amudra%2520recognition%2520by%2520framing%2520it%2520as%2520a%252024-class%2520classification%2520problem%2520and%250Aproposes%2520a%2520novel%2520vector-similarity-based%2520approach%2520leveraging%2520pose%2520estimation%250Atechniques.%2520This%2520method%2520obviates%2520the%2520need%2520for%2520extensive%2520training%2520or%250Afine-tuning%252C%2520thus%2520mitigating%2520the%2520issue%2520of%2520limited%2520data%2520availability%2520common%2520in%250Asimilar%2520AI%2520applications.%2520Achieving%2520an%2520accuracy%2520rate%2520of%252092%2525%252C%2520our%2520approach%250Ademonstrates%2520comparable%2520or%2520superior%2520performance%2520to%2520existing%250Amodel-training-based%2520methodologies%2520in%2520this%2520domain.%2520Notably%252C%2520it%2520remains%250Aeffective%2520even%2520with%2520small%2520datasets%2520comprising%2520just%25201%2520or%25205%2520samples%252C%2520albeit%2520with%250Aa%2520slightly%2520diminished%2520performance.%2520Furthermore%252C%2520our%2520system%2520supports%2520processing%250Aimages%252C%2520videos%252C%2520and%2520real-time%2520streams%252C%2520accommodating%2520both%2520hand-cropped%2520and%250Afull-body%2520images.%2520As%2520part%2520of%2520this%2520research%252C%2520we%2520have%2520curated%2520and%2520released%2520a%250Apublicly%2520accessible%2520Hasta%2520Mudra%2520dataset%252C%2520which%2520applies%2520to%2520multiple%2520South%2520Indian%250Aart%2520forms%2520including%2520Kathakali.%2520The%2520implementation%2520of%2520the%2520proposed%2520method%2520is%250Aalso%2520made%2520available%2520as%2520a%2520web%2520application.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2404.11205v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Pose2Gest%3A%20A%20Few-Shot%20Model-Free%20Approach%20Applied%20In%20South%20Indian%0A%20%20Classical%20Dance%20Gesture%20Recognition&entry.906535625=Kavitha%20Raju%20and%20Nandini%20J.%20Warrier%20and%20Manu%20Madhavan%20and%20Selvi%20C.%20and%20Arun%20B.%20Warrier%20and%20Thulasi%20Kumar&entry.1292438233=%20%20The%20classical%20dances%20from%20India%20utilize%20a%20set%20of%20hand%20gestures%20known%20as%0AMudras%2C%20serving%20as%20the%20foundational%20elements%20of%20its%20posture%20vocabulary.%0AIdentifying%20these%20mudras%20represents%20a%20primary%20task%20in%20digitizing%20the%20dance%0Aperformances.%20With%20Kathakali%2C%20a%20dance-drama%2C%20as%20the%20focus%2C%20this%20work%20addresses%0Amudra%20recognition%20by%20framing%20it%20as%20a%2024-class%20classification%20problem%20and%0Aproposes%20a%20novel%20vector-similarity-based%20approach%20leveraging%20pose%20estimation%0Atechniques.%20This%20method%20obviates%20the%20need%20for%20extensive%20training%20or%0Afine-tuning%2C%20thus%20mitigating%20the%20issue%20of%20limited%20data%20availability%20common%20in%0Asimilar%20AI%20applications.%20Achieving%20an%20accuracy%20rate%20of%2092%25%2C%20our%20approach%0Ademonstrates%20comparable%20or%20superior%20performance%20to%20existing%0Amodel-training-based%20methodologies%20in%20this%20domain.%20Notably%2C%20it%20remains%0Aeffective%20even%20with%20small%20datasets%20comprising%20just%201%20or%205%20samples%2C%20albeit%20with%0Aa%20slightly%20diminished%20performance.%20Furthermore%2C%20our%20system%20supports%20processing%0Aimages%2C%20videos%2C%20and%20real-time%20streams%2C%20accommodating%20both%20hand-cropped%20and%0Afull-body%20images.%20As%20part%20of%20this%20research%2C%20we%20have%20curated%20and%20released%20a%0Apublicly%20accessible%20Hasta%20Mudra%20dataset%2C%20which%20applies%20to%20multiple%20South%20Indian%0Aart%20forms%20including%20Kathakali.%20The%20implementation%20of%20the%20proposed%20method%20is%0Aalso%20made%20available%20as%20a%20web%20application.%0A&entry.1838667208=http%3A//arxiv.org/abs/2404.11205v2&entry.124074799=Read"},
{"title": "Multicenter Privacy-Preserving Model Training for Deep Learning Brain\n  Metastases Autosegmentation", "author": "Yixing Huang and Zahra Khodabakhshi and Ahmed Gomaa and Manuel Schmidt and Rainer Fietkau and Matthias Guckenberger and Nicolaus Andratschke and Christoph Bert and Stephanie Tanadini-Lang and Florian Putz", "abstract": "  Objectives: This work aims to explore the impact of multicenter data\nheterogeneity on deep learning brain metastases (BM) autosegmentation\nperformance, and assess the efficacy of an incremental transfer learning\ntechnique, namely learning without forgetting (LWF), to improve model\ngeneralizability without sharing raw data.\n  Materials and methods: A total of six BM datasets from University Hospital\nErlangen (UKER), University Hospital Zurich (USZ), Stanford, UCSF, NYU and\nBraTS Challenge 2023 on BM segmentation were used for this evaluation. First,\nthe multicenter performance of a convolutional neural network (DeepMedic) for\nBM autosegmentation was established for exclusive single-center training and\nfor training on pooled data, respectively. Subsequently bilateral collaboration\nwas evaluated, where a UKER pretrained model is shared to another center for\nfurther training using transfer learning (TL) either with or without LWF.\n  Results: For single-center training, average F1 scores of BM detection range\nfrom 0.625 (NYU) to 0.876 (UKER) on respective single-center test data. Mixed\nmulticenter training notably improves F1 scores at Stanford and NYU, with\nnegligible improvement at other centers. When the UKER pretrained model is\napplied to USZ, LWF achieves a higher average F1 score (0.839) than naive TL\n(0.570) and single-center training (0.688) on combined UKER and USZ test data.\nNaive TL improves sensitivity and contouring accuracy, but compromises\nprecision. Conversely, LWF demonstrates commendable sensitivity, precision and\ncontouring accuracy. When applied to Stanford, similar performance was\nobserved.\n  Conclusion: Data heterogeneity results in varying performance in BM\nautosegmentation, posing challenges to model generalizability. LWF is a\npromising approach to peer-to-peer privacy-preserving model training.\n", "link": "http://arxiv.org/abs/2405.10870v1", "date": "2024-05-17", "relevancy": 2.1078, "topK": [{"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.5516}, {"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.5253}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5187}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Multicenter%20Privacy-Preserving%20Model%20Training%20for%20Deep%20Learning%20Brain%0A%20%20Metastases%20Autosegmentation&body=Title%3A%20Multicenter%20Privacy-Preserving%20Model%20Training%20for%20Deep%20Learning%20Brain%0A%20%20Metastases%20Autosegmentation%0AAuthor%3A%20Yixing%20Huang%20and%20Zahra%20Khodabakhshi%20and%20Ahmed%20Gomaa%20and%20Manuel%20Schmidt%20and%20Rainer%20Fietkau%20and%20Matthias%20Guckenberger%20and%20Nicolaus%20Andratschke%20and%20Christoph%20Bert%20and%20Stephanie%20Tanadini-Lang%20and%20Florian%20Putz%0AAbstract%3A%20%20%20Objectives%3A%20This%20work%20aims%20to%20explore%20the%20impact%20of%20multicenter%20data%0Aheterogeneity%20on%20deep%20learning%20brain%20metastases%20%28BM%29%20autosegmentation%0Aperformance%2C%20and%20assess%20the%20efficacy%20of%20an%20incremental%20transfer%20learning%0Atechnique%2C%20namely%20learning%20without%20forgetting%20%28LWF%29%2C%20to%20improve%20model%0Ageneralizability%20without%20sharing%20raw%20data.%0A%20%20Materials%20and%20methods%3A%20A%20total%20of%20six%20BM%20datasets%20from%20University%20Hospital%0AErlangen%20%28UKER%29%2C%20University%20Hospital%20Zurich%20%28USZ%29%2C%20Stanford%2C%20UCSF%2C%20NYU%20and%0ABraTS%20Challenge%202023%20on%20BM%20segmentation%20were%20used%20for%20this%20evaluation.%20First%2C%0Athe%20multicenter%20performance%20of%20a%20convolutional%20neural%20network%20%28DeepMedic%29%20for%0ABM%20autosegmentation%20was%20established%20for%20exclusive%20single-center%20training%20and%0Afor%20training%20on%20pooled%20data%2C%20respectively.%20Subsequently%20bilateral%20collaboration%0Awas%20evaluated%2C%20where%20a%20UKER%20pretrained%20model%20is%20shared%20to%20another%20center%20for%0Afurther%20training%20using%20transfer%20learning%20%28TL%29%20either%20with%20or%20without%20LWF.%0A%20%20Results%3A%20For%20single-center%20training%2C%20average%20F1%20scores%20of%20BM%20detection%20range%0Afrom%200.625%20%28NYU%29%20to%200.876%20%28UKER%29%20on%20respective%20single-center%20test%20data.%20Mixed%0Amulticenter%20training%20notably%20improves%20F1%20scores%20at%20Stanford%20and%20NYU%2C%20with%0Anegligible%20improvement%20at%20other%20centers.%20When%20the%20UKER%20pretrained%20model%20is%0Aapplied%20to%20USZ%2C%20LWF%20achieves%20a%20higher%20average%20F1%20score%20%280.839%29%20than%20naive%20TL%0A%280.570%29%20and%20single-center%20training%20%280.688%29%20on%20combined%20UKER%20and%20USZ%20test%20data.%0ANaive%20TL%20improves%20sensitivity%20and%20contouring%20accuracy%2C%20but%20compromises%0Aprecision.%20Conversely%2C%20LWF%20demonstrates%20commendable%20sensitivity%2C%20precision%20and%0Acontouring%20accuracy.%20When%20applied%20to%20Stanford%2C%20similar%20performance%20was%0Aobserved.%0A%20%20Conclusion%3A%20Data%20heterogeneity%20results%20in%20varying%20performance%20in%20BM%0Aautosegmentation%2C%20posing%20challenges%20to%20model%20generalizability.%20LWF%20is%20a%0Apromising%20approach%20to%20peer-to-peer%20privacy-preserving%20model%20training.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2405.10870v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DMulticenter%2520Privacy-Preserving%2520Model%2520Training%2520for%2520Deep%2520Learning%2520Brain%250A%2520%2520Metastases%2520Autosegmentation%26entry.906535625%3DYixing%2520Huang%2520and%2520Zahra%2520Khodabakhshi%2520and%2520Ahmed%2520Gomaa%2520and%2520Manuel%2520Schmidt%2520and%2520Rainer%2520Fietkau%2520and%2520Matthias%2520Guckenberger%2520and%2520Nicolaus%2520Andratschke%2520and%2520Christoph%2520Bert%2520and%2520Stephanie%2520Tanadini-Lang%2520and%2520Florian%2520Putz%26entry.1292438233%3D%2520%2520Objectives%253A%2520This%2520work%2520aims%2520to%2520explore%2520the%2520impact%2520of%2520multicenter%2520data%250Aheterogeneity%2520on%2520deep%2520learning%2520brain%2520metastases%2520%2528BM%2529%2520autosegmentation%250Aperformance%252C%2520and%2520assess%2520the%2520efficacy%2520of%2520an%2520incremental%2520transfer%2520learning%250Atechnique%252C%2520namely%2520learning%2520without%2520forgetting%2520%2528LWF%2529%252C%2520to%2520improve%2520model%250Ageneralizability%2520without%2520sharing%2520raw%2520data.%250A%2520%2520Materials%2520and%2520methods%253A%2520A%2520total%2520of%2520six%2520BM%2520datasets%2520from%2520University%2520Hospital%250AErlangen%2520%2528UKER%2529%252C%2520University%2520Hospital%2520Zurich%2520%2528USZ%2529%252C%2520Stanford%252C%2520UCSF%252C%2520NYU%2520and%250ABraTS%2520Challenge%25202023%2520on%2520BM%2520segmentation%2520were%2520used%2520for%2520this%2520evaluation.%2520First%252C%250Athe%2520multicenter%2520performance%2520of%2520a%2520convolutional%2520neural%2520network%2520%2528DeepMedic%2529%2520for%250ABM%2520autosegmentation%2520was%2520established%2520for%2520exclusive%2520single-center%2520training%2520and%250Afor%2520training%2520on%2520pooled%2520data%252C%2520respectively.%2520Subsequently%2520bilateral%2520collaboration%250Awas%2520evaluated%252C%2520where%2520a%2520UKER%2520pretrained%2520model%2520is%2520shared%2520to%2520another%2520center%2520for%250Afurther%2520training%2520using%2520transfer%2520learning%2520%2528TL%2529%2520either%2520with%2520or%2520without%2520LWF.%250A%2520%2520Results%253A%2520For%2520single-center%2520training%252C%2520average%2520F1%2520scores%2520of%2520BM%2520detection%2520range%250Afrom%25200.625%2520%2528NYU%2529%2520to%25200.876%2520%2528UKER%2529%2520on%2520respective%2520single-center%2520test%2520data.%2520Mixed%250Amulticenter%2520training%2520notably%2520improves%2520F1%2520scores%2520at%2520Stanford%2520and%2520NYU%252C%2520with%250Anegligible%2520improvement%2520at%2520other%2520centers.%2520When%2520the%2520UKER%2520pretrained%2520model%2520is%250Aapplied%2520to%2520USZ%252C%2520LWF%2520achieves%2520a%2520higher%2520average%2520F1%2520score%2520%25280.839%2529%2520than%2520naive%2520TL%250A%25280.570%2529%2520and%2520single-center%2520training%2520%25280.688%2529%2520on%2520combined%2520UKER%2520and%2520USZ%2520test%2520data.%250ANaive%2520TL%2520improves%2520sensitivity%2520and%2520contouring%2520accuracy%252C%2520but%2520compromises%250Aprecision.%2520Conversely%252C%2520LWF%2520demonstrates%2520commendable%2520sensitivity%252C%2520precision%2520and%250Acontouring%2520accuracy.%2520When%2520applied%2520to%2520Stanford%252C%2520similar%2520performance%2520was%250Aobserved.%250A%2520%2520Conclusion%253A%2520Data%2520heterogeneity%2520results%2520in%2520varying%2520performance%2520in%2520BM%250Aautosegmentation%252C%2520posing%2520challenges%2520to%2520model%2520generalizability.%2520LWF%2520is%2520a%250Apromising%2520approach%2520to%2520peer-to-peer%2520privacy-preserving%2520model%2520training.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2405.10870v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Multicenter%20Privacy-Preserving%20Model%20Training%20for%20Deep%20Learning%20Brain%0A%20%20Metastases%20Autosegmentation&entry.906535625=Yixing%20Huang%20and%20Zahra%20Khodabakhshi%20and%20Ahmed%20Gomaa%20and%20Manuel%20Schmidt%20and%20Rainer%20Fietkau%20and%20Matthias%20Guckenberger%20and%20Nicolaus%20Andratschke%20and%20Christoph%20Bert%20and%20Stephanie%20Tanadini-Lang%20and%20Florian%20Putz&entry.1292438233=%20%20Objectives%3A%20This%20work%20aims%20to%20explore%20the%20impact%20of%20multicenter%20data%0Aheterogeneity%20on%20deep%20learning%20brain%20metastases%20%28BM%29%20autosegmentation%0Aperformance%2C%20and%20assess%20the%20efficacy%20of%20an%20incremental%20transfer%20learning%0Atechnique%2C%20namely%20learning%20without%20forgetting%20%28LWF%29%2C%20to%20improve%20model%0Ageneralizability%20without%20sharing%20raw%20data.%0A%20%20Materials%20and%20methods%3A%20A%20total%20of%20six%20BM%20datasets%20from%20University%20Hospital%0AErlangen%20%28UKER%29%2C%20University%20Hospital%20Zurich%20%28USZ%29%2C%20Stanford%2C%20UCSF%2C%20NYU%20and%0ABraTS%20Challenge%202023%20on%20BM%20segmentation%20were%20used%20for%20this%20evaluation.%20First%2C%0Athe%20multicenter%20performance%20of%20a%20convolutional%20neural%20network%20%28DeepMedic%29%20for%0ABM%20autosegmentation%20was%20established%20for%20exclusive%20single-center%20training%20and%0Afor%20training%20on%20pooled%20data%2C%20respectively.%20Subsequently%20bilateral%20collaboration%0Awas%20evaluated%2C%20where%20a%20UKER%20pretrained%20model%20is%20shared%20to%20another%20center%20for%0Afurther%20training%20using%20transfer%20learning%20%28TL%29%20either%20with%20or%20without%20LWF.%0A%20%20Results%3A%20For%20single-center%20training%2C%20average%20F1%20scores%20of%20BM%20detection%20range%0Afrom%200.625%20%28NYU%29%20to%200.876%20%28UKER%29%20on%20respective%20single-center%20test%20data.%20Mixed%0Amulticenter%20training%20notably%20improves%20F1%20scores%20at%20Stanford%20and%20NYU%2C%20with%0Anegligible%20improvement%20at%20other%20centers.%20When%20the%20UKER%20pretrained%20model%20is%0Aapplied%20to%20USZ%2C%20LWF%20achieves%20a%20higher%20average%20F1%20score%20%280.839%29%20than%20naive%20TL%0A%280.570%29%20and%20single-center%20training%20%280.688%29%20on%20combined%20UKER%20and%20USZ%20test%20data.%0ANaive%20TL%20improves%20sensitivity%20and%20contouring%20accuracy%2C%20but%20compromises%0Aprecision.%20Conversely%2C%20LWF%20demonstrates%20commendable%20sensitivity%2C%20precision%20and%0Acontouring%20accuracy.%20When%20applied%20to%20Stanford%2C%20similar%20performance%20was%0Aobserved.%0A%20%20Conclusion%3A%20Data%20heterogeneity%20results%20in%20varying%20performance%20in%20BM%0Aautosegmentation%2C%20posing%20challenges%20to%20model%20generalizability.%20LWF%20is%20a%0Apromising%20approach%20to%20peer-to-peer%20privacy-preserving%20model%20training.%0A&entry.1838667208=http%3A//arxiv.org/abs/2405.10870v1&entry.124074799=Read"},
{"title": "Shifting to Machine Supervision: Annotation-Efficient Semi and\n  Self-Supervised Learning for Automatic Medical Image Segmentation and\n  Classification", "author": "Pranav Singh and Raviteja Chukkapalli and Shravan Chaudhari and Luoyao Chen and Mei Chen and Jinqian Pan and Craig Smuda and Jacopo Cirrone", "abstract": "  Advancements in clinical treatment are increasingly constrained by the\nlimitations of supervised learning techniques, which depend heavily on large\nvolumes of annotated data. The annotation process is not only costly but also\ndemands substantial time from clinical specialists. Addressing this issue, we\nintroduce the S4MI (Self-Supervision and Semi-Supervision for Medical Imaging)\npipeline, a novel approach that leverages advancements in self-supervised and\nsemi-supervised learning. These techniques engage in auxiliary tasks that do\nnot require labeling, thus simplifying the scaling of machine supervision\ncompared to fully-supervised methods. Our study benchmarks these techniques on\nthree distinct medical imaging datasets to evaluate their effectiveness in\nclassification and segmentation tasks. Notably, we observed that self\nsupervised learning significantly surpassed the performance of supervised\nmethods in the classification of all evaluated datasets. Remarkably, the\nsemi-supervised approach demonstrated superior outcomes in segmentation,\noutperforming fully-supervised methods while using 50% fewer labels across all\ndatasets. In line with our commitment to contributing to the scientific\ncommunity, we have made the S4MI code openly accessible, allowing for broader\napplication and further development of these methods.\n", "link": "http://arxiv.org/abs/2311.10319v6", "date": "2024-05-17", "relevancy": 2.1072, "topK": [{"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.5409}, {"title": "Self-supervised Photographic Image Layout Representation Learning", "link": "http://arxiv.org/abs/2403.03740v1", "similarity": 0.5318}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5161}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Shifting%20to%20Machine%20Supervision%3A%20Annotation-Efficient%20Semi%20and%0A%20%20Self-Supervised%20Learning%20for%20Automatic%20Medical%20Image%20Segmentation%20and%0A%20%20Classification&body=Title%3A%20Shifting%20to%20Machine%20Supervision%3A%20Annotation-Efficient%20Semi%20and%0A%20%20Self-Supervised%20Learning%20for%20Automatic%20Medical%20Image%20Segmentation%20and%0A%20%20Classification%0AAuthor%3A%20Pranav%20Singh%20and%20Raviteja%20Chukkapalli%20and%20Shravan%20Chaudhari%20and%20Luoyao%20Chen%20and%20Mei%20Chen%20and%20Jinqian%20Pan%20and%20Craig%20Smuda%20and%20Jacopo%20Cirrone%0AAbstract%3A%20%20%20Advancements%20in%20clinical%20treatment%20are%20increasingly%20constrained%20by%20the%0Alimitations%20of%20supervised%20learning%20techniques%2C%20which%20depend%20heavily%20on%20large%0Avolumes%20of%20annotated%20data.%20The%20annotation%20process%20is%20not%20only%20costly%20but%20also%0Ademands%20substantial%20time%20from%20clinical%20specialists.%20Addressing%20this%20issue%2C%20we%0Aintroduce%20the%20S4MI%20%28Self-Supervision%20and%20Semi-Supervision%20for%20Medical%20Imaging%29%0Apipeline%2C%20a%20novel%20approach%20that%20leverages%20advancements%20in%20self-supervised%20and%0Asemi-supervised%20learning.%20These%20techniques%20engage%20in%20auxiliary%20tasks%20that%20do%0Anot%20require%20labeling%2C%20thus%20simplifying%20the%20scaling%20of%20machine%20supervision%0Acompared%20to%20fully-supervised%20methods.%20Our%20study%20benchmarks%20these%20techniques%20on%0Athree%20distinct%20medical%20imaging%20datasets%20to%20evaluate%20their%20effectiveness%20in%0Aclassification%20and%20segmentation%20tasks.%20Notably%2C%20we%20observed%20that%20self%0Asupervised%20learning%20significantly%20surpassed%20the%20performance%20of%20supervised%0Amethods%20in%20the%20classification%20of%20all%20evaluated%20datasets.%20Remarkably%2C%20the%0Asemi-supervised%20approach%20demonstrated%20superior%20outcomes%20in%20segmentation%2C%0Aoutperforming%20fully-supervised%20methods%20while%20using%2050%25%20fewer%20labels%20across%20all%0Adatasets.%20In%20line%20with%20our%20commitment%20to%20contributing%20to%20the%20scientific%0Acommunity%2C%20we%20have%20made%20the%20S4MI%20code%20openly%20accessible%2C%20allowing%20for%20broader%0Aapplication%20and%20further%20development%20of%20these%20methods.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2311.10319v6%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DShifting%2520to%2520Machine%2520Supervision%253A%2520Annotation-Efficient%2520Semi%2520and%250A%2520%2520Self-Supervised%2520Learning%2520for%2520Automatic%2520Medical%2520Image%2520Segmentation%2520and%250A%2520%2520Classification%26entry.906535625%3DPranav%2520Singh%2520and%2520Raviteja%2520Chukkapalli%2520and%2520Shravan%2520Chaudhari%2520and%2520Luoyao%2520Chen%2520and%2520Mei%2520Chen%2520and%2520Jinqian%2520Pan%2520and%2520Craig%2520Smuda%2520and%2520Jacopo%2520Cirrone%26entry.1292438233%3D%2520%2520Advancements%2520in%2520clinical%2520treatment%2520are%2520increasingly%2520constrained%2520by%2520the%250Alimitations%2520of%2520supervised%2520learning%2520techniques%252C%2520which%2520depend%2520heavily%2520on%2520large%250Avolumes%2520of%2520annotated%2520data.%2520The%2520annotation%2520process%2520is%2520not%2520only%2520costly%2520but%2520also%250Ademands%2520substantial%2520time%2520from%2520clinical%2520specialists.%2520Addressing%2520this%2520issue%252C%2520we%250Aintroduce%2520the%2520S4MI%2520%2528Self-Supervision%2520and%2520Semi-Supervision%2520for%2520Medical%2520Imaging%2529%250Apipeline%252C%2520a%2520novel%2520approach%2520that%2520leverages%2520advancements%2520in%2520self-supervised%2520and%250Asemi-supervised%2520learning.%2520These%2520techniques%2520engage%2520in%2520auxiliary%2520tasks%2520that%2520do%250Anot%2520require%2520labeling%252C%2520thus%2520simplifying%2520the%2520scaling%2520of%2520machine%2520supervision%250Acompared%2520to%2520fully-supervised%2520methods.%2520Our%2520study%2520benchmarks%2520these%2520techniques%2520on%250Athree%2520distinct%2520medical%2520imaging%2520datasets%2520to%2520evaluate%2520their%2520effectiveness%2520in%250Aclassification%2520and%2520segmentation%2520tasks.%2520Notably%252C%2520we%2520observed%2520that%2520self%250Asupervised%2520learning%2520significantly%2520surpassed%2520the%2520performance%2520of%2520supervised%250Amethods%2520in%2520the%2520classification%2520of%2520all%2520evaluated%2520datasets.%2520Remarkably%252C%2520the%250Asemi-supervised%2520approach%2520demonstrated%2520superior%2520outcomes%2520in%2520segmentation%252C%250Aoutperforming%2520fully-supervised%2520methods%2520while%2520using%252050%2525%2520fewer%2520labels%2520across%2520all%250Adatasets.%2520In%2520line%2520with%2520our%2520commitment%2520to%2520contributing%2520to%2520the%2520scientific%250Acommunity%252C%2520we%2520have%2520made%2520the%2520S4MI%2520code%2520openly%2520accessible%252C%2520allowing%2520for%2520broader%250Aapplication%2520and%2520further%2520development%2520of%2520these%2520methods.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2311.10319v6%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Shifting%20to%20Machine%20Supervision%3A%20Annotation-Efficient%20Semi%20and%0A%20%20Self-Supervised%20Learning%20for%20Automatic%20Medical%20Image%20Segmentation%20and%0A%20%20Classification&entry.906535625=Pranav%20Singh%20and%20Raviteja%20Chukkapalli%20and%20Shravan%20Chaudhari%20and%20Luoyao%20Chen%20and%20Mei%20Chen%20and%20Jinqian%20Pan%20and%20Craig%20Smuda%20and%20Jacopo%20Cirrone&entry.1292438233=%20%20Advancements%20in%20clinical%20treatment%20are%20increasingly%20constrained%20by%20the%0Alimitations%20of%20supervised%20learning%20techniques%2C%20which%20depend%20heavily%20on%20large%0Avolumes%20of%20annotated%20data.%20The%20annotation%20process%20is%20not%20only%20costly%20but%20also%0Ademands%20substantial%20time%20from%20clinical%20specialists.%20Addressing%20this%20issue%2C%20we%0Aintroduce%20the%20S4MI%20%28Self-Supervision%20and%20Semi-Supervision%20for%20Medical%20Imaging%29%0Apipeline%2C%20a%20novel%20approach%20that%20leverages%20advancements%20in%20self-supervised%20and%0Asemi-supervised%20learning.%20These%20techniques%20engage%20in%20auxiliary%20tasks%20that%20do%0Anot%20require%20labeling%2C%20thus%20simplifying%20the%20scaling%20of%20machine%20supervision%0Acompared%20to%20fully-supervised%20methods.%20Our%20study%20benchmarks%20these%20techniques%20on%0Athree%20distinct%20medical%20imaging%20datasets%20to%20evaluate%20their%20effectiveness%20in%0Aclassification%20and%20segmentation%20tasks.%20Notably%2C%20we%20observed%20that%20self%0Asupervised%20learning%20significantly%20surpassed%20the%20performance%20of%20supervised%0Amethods%20in%20the%20classification%20of%20all%20evaluated%20datasets.%20Remarkably%2C%20the%0Asemi-supervised%20approach%20demonstrated%20superior%20outcomes%20in%20segmentation%2C%0Aoutperforming%20fully-supervised%20methods%20while%20using%2050%25%20fewer%20labels%20across%20all%0Adatasets.%20In%20line%20with%20our%20commitment%20to%20contributing%20to%20the%20scientific%0Acommunity%2C%20we%20have%20made%20the%20S4MI%20code%20openly%20accessible%2C%20allowing%20for%20broader%0Aapplication%20and%20further%20development%20of%20these%20methods.%0A&entry.1838667208=http%3A//arxiv.org/abs/2311.10319v6&entry.124074799=Read"},
{"title": "Topological Data Analysis in smart manufacturing", "author": "Martin Uray and Barbara Giunti and Michael Kerber and Stefan Huber", "abstract": "  Topological Data Analysis (TDA) is a discipline that applies algebraic\ntopology techniques to analyze complex, multi-dimensional data. Although it is\na relatively new field, TDA has been widely and successfully applied across\nvarious domains, such as medicine, materials science, and biology. This survey\nprovides an overview of the state of the art of TDA within a dynamic and\npromising application area: industrial manufacturing and production,\nparticularly within the Industry 4.0 context. We have conducted a rigorous and\nreproducible literature search focusing on TDA applications in industrial\nproduction and manufacturing settings. The identified works are categorized\nbased on their application areas within the manufacturing process and the types\nof input data. We highlight the principal advantages of TDA tools in this\ncontext, address the challenges encountered and the future potential of the\nfield. Furthermore, we identify TDA methods that are currently underexploited\nin specific industrial areas and discuss how their application could be\nbeneficial, with the aim of stimulating further research in this field. This\nwork seeks to bridge the theoretical advancements in TDA with the practical\nneeds of industrial production. Our goal is to serve as a guide for\npractitioners and researchers applying TDA in industrial production and\nmanufacturing systems. We advocate for the untapped potential of TDA in this\ndomain and encourage continued exploration and research.\n", "link": "http://arxiv.org/abs/2310.09319v2", "date": "2024-05-17", "relevancy": 2.082, "topK": [{"title": "CAT3D: Create Anything in 3D with Multi-View Diffusion Models", "link": "http://arxiv.org/abs/2405.10314v1", "similarity": 0.4179}, {"title": "CAT3D: Create Anything in 3D with Multi-View Diffusion Models", "link": "http://arxiv.org/abs/2405.10314v1", "similarity": 0.4179}, {"title": "DressCode: Autoregressively Sewing and Generating Garments from Text\n  Guidance", "link": "http://arxiv.org/abs/2401.16465v3", "similarity": 0.4135}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Topological%20Data%20Analysis%20in%20smart%20manufacturing&body=Title%3A%20Topological%20Data%20Analysis%20in%20smart%20manufacturing%0AAuthor%3A%20Martin%20Uray%20and%20Barbara%20Giunti%20and%20Michael%20Kerber%20and%20Stefan%20Huber%0AAbstract%3A%20%20%20Topological%20Data%20Analysis%20%28TDA%29%20is%20a%20discipline%20that%20applies%20algebraic%0Atopology%20techniques%20to%20analyze%20complex%2C%20multi-dimensional%20data.%20Although%20it%20is%0Aa%20relatively%20new%20field%2C%20TDA%20has%20been%20widely%20and%20successfully%20applied%20across%0Avarious%20domains%2C%20such%20as%20medicine%2C%20materials%20science%2C%20and%20biology.%20This%20survey%0Aprovides%20an%20overview%20of%20the%20state%20of%20the%20art%20of%20TDA%20within%20a%20dynamic%20and%0Apromising%20application%20area%3A%20industrial%20manufacturing%20and%20production%2C%0Aparticularly%20within%20the%20Industry%204.0%20context.%20We%20have%20conducted%20a%20rigorous%20and%0Areproducible%20literature%20search%20focusing%20on%20TDA%20applications%20in%20industrial%0Aproduction%20and%20manufacturing%20settings.%20The%20identified%20works%20are%20categorized%0Abased%20on%20their%20application%20areas%20within%20the%20manufacturing%20process%20and%20the%20types%0Aof%20input%20data.%20We%20highlight%20the%20principal%20advantages%20of%20TDA%20tools%20in%20this%0Acontext%2C%20address%20the%20challenges%20encountered%20and%20the%20future%20potential%20of%20the%0Afield.%20Furthermore%2C%20we%20identify%20TDA%20methods%20that%20are%20currently%20underexploited%0Ain%20specific%20industrial%20areas%20and%20discuss%20how%20their%20application%20could%20be%0Abeneficial%2C%20with%20the%20aim%20of%20stimulating%20further%20research%20in%20this%20field.%20This%0Awork%20seeks%20to%20bridge%20the%20theoretical%20advancements%20in%20TDA%20with%20the%20practical%0Aneeds%20of%20industrial%20production.%20Our%20goal%20is%20to%20serve%20as%20a%20guide%20for%0Apractitioners%20and%20researchers%20applying%20TDA%20in%20industrial%20production%20and%0Amanufacturing%20systems.%20We%20advocate%20for%20the%20untapped%20potential%20of%20TDA%20in%20this%0Adomain%20and%20encourage%20continued%20exploration%20and%20research.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2310.09319v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DTopological%2520Data%2520Analysis%2520in%2520smart%2520manufacturing%26entry.906535625%3DMartin%2520Uray%2520and%2520Barbara%2520Giunti%2520and%2520Michael%2520Kerber%2520and%2520Stefan%2520Huber%26entry.1292438233%3D%2520%2520Topological%2520Data%2520Analysis%2520%2528TDA%2529%2520is%2520a%2520discipline%2520that%2520applies%2520algebraic%250Atopology%2520techniques%2520to%2520analyze%2520complex%252C%2520multi-dimensional%2520data.%2520Although%2520it%2520is%250Aa%2520relatively%2520new%2520field%252C%2520TDA%2520has%2520been%2520widely%2520and%2520successfully%2520applied%2520across%250Avarious%2520domains%252C%2520such%2520as%2520medicine%252C%2520materials%2520science%252C%2520and%2520biology.%2520This%2520survey%250Aprovides%2520an%2520overview%2520of%2520the%2520state%2520of%2520the%2520art%2520of%2520TDA%2520within%2520a%2520dynamic%2520and%250Apromising%2520application%2520area%253A%2520industrial%2520manufacturing%2520and%2520production%252C%250Aparticularly%2520within%2520the%2520Industry%25204.0%2520context.%2520We%2520have%2520conducted%2520a%2520rigorous%2520and%250Areproducible%2520literature%2520search%2520focusing%2520on%2520TDA%2520applications%2520in%2520industrial%250Aproduction%2520and%2520manufacturing%2520settings.%2520The%2520identified%2520works%2520are%2520categorized%250Abased%2520on%2520their%2520application%2520areas%2520within%2520the%2520manufacturing%2520process%2520and%2520the%2520types%250Aof%2520input%2520data.%2520We%2520highlight%2520the%2520principal%2520advantages%2520of%2520TDA%2520tools%2520in%2520this%250Acontext%252C%2520address%2520the%2520challenges%2520encountered%2520and%2520the%2520future%2520potential%2520of%2520the%250Afield.%2520Furthermore%252C%2520we%2520identify%2520TDA%2520methods%2520that%2520are%2520currently%2520underexploited%250Ain%2520specific%2520industrial%2520areas%2520and%2520discuss%2520how%2520their%2520application%2520could%2520be%250Abeneficial%252C%2520with%2520the%2520aim%2520of%2520stimulating%2520further%2520research%2520in%2520this%2520field.%2520This%250Awork%2520seeks%2520to%2520bridge%2520the%2520theoretical%2520advancements%2520in%2520TDA%2520with%2520the%2520practical%250Aneeds%2520of%2520industrial%2520production.%2520Our%2520goal%2520is%2520to%2520serve%2520as%2520a%2520guide%2520for%250Apractitioners%2520and%2520researchers%2520applying%2520TDA%2520in%2520industrial%2520production%2520and%250Amanufacturing%2520systems.%2520We%2520advocate%2520for%2520the%2520untapped%2520potential%2520of%2520TDA%2520in%2520this%250Adomain%2520and%2520encourage%2520continued%2520exploration%2520and%2520research.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2310.09319v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Topological%20Data%20Analysis%20in%20smart%20manufacturing&entry.906535625=Martin%20Uray%20and%20Barbara%20Giunti%20and%20Michael%20Kerber%20and%20Stefan%20Huber&entry.1292438233=%20%20Topological%20Data%20Analysis%20%28TDA%29%20is%20a%20discipline%20that%20applies%20algebraic%0Atopology%20techniques%20to%20analyze%20complex%2C%20multi-dimensional%20data.%20Although%20it%20is%0Aa%20relatively%20new%20field%2C%20TDA%20has%20been%20widely%20and%20successfully%20applied%20across%0Avarious%20domains%2C%20such%20as%20medicine%2C%20materials%20science%2C%20and%20biology.%20This%20survey%0Aprovides%20an%20overview%20of%20the%20state%20of%20the%20art%20of%20TDA%20within%20a%20dynamic%20and%0Apromising%20application%20area%3A%20industrial%20manufacturing%20and%20production%2C%0Aparticularly%20within%20the%20Industry%204.0%20context.%20We%20have%20conducted%20a%20rigorous%20and%0Areproducible%20literature%20search%20focusing%20on%20TDA%20applications%20in%20industrial%0Aproduction%20and%20manufacturing%20settings.%20The%20identified%20works%20are%20categorized%0Abased%20on%20their%20application%20areas%20within%20the%20manufacturing%20process%20and%20the%20types%0Aof%20input%20data.%20We%20highlight%20the%20principal%20advantages%20of%20TDA%20tools%20in%20this%0Acontext%2C%20address%20the%20challenges%20encountered%20and%20the%20future%20potential%20of%20the%0Afield.%20Furthermore%2C%20we%20identify%20TDA%20methods%20that%20are%20currently%20underexploited%0Ain%20specific%20industrial%20areas%20and%20discuss%20how%20their%20application%20could%20be%0Abeneficial%2C%20with%20the%20aim%20of%20stimulating%20further%20research%20in%20this%20field.%20This%0Awork%20seeks%20to%20bridge%20the%20theoretical%20advancements%20in%20TDA%20with%20the%20practical%0Aneeds%20of%20industrial%20production.%20Our%20goal%20is%20to%20serve%20as%20a%20guide%20for%0Apractitioners%20and%20researchers%20applying%20TDA%20in%20industrial%20production%20and%0Amanufacturing%20systems.%20We%20advocate%20for%20the%20untapped%20potential%20of%20TDA%20in%20this%0Adomain%20and%20encourage%20continued%20exploration%20and%20research.%0A&entry.1838667208=http%3A//arxiv.org/abs/2310.09319v2&entry.124074799=Read"},
{"title": "Fast Collision Probability Estimation for Automated Driving using\n  Multi-circular Shape Approximations", "author": "Leon Tolksdorf and Christian Birkner and Arturo Tejada and Nathan van de Wouw", "abstract": "  Many state-of-the-art methods for safety assessment and motion planning for\nautomated driving require estimation of the probability of collision (POC). To\nestimate the POC, a shape approximation of the colliding actors and probability\ndensity functions of the associated uncertain kinematic variables are required.\nEven with such information available, the derivation of the POC is in general,\ni.e., for any shape and density, only possible with Monte Carlo sampling (MCS).\nRandom sampling of the POC, however, is challenging as computational resources\nare limited in real-world applications. We present expressions for the POC in\nthe presence of Gaussian uncertainties, based on multi-circular shape\napproximations. In addition, we show that the proposed approach is\ncomputationally more efficient than MCS. Lastly, we provide a method for upper\nand lower bounding the estimation error for the POC induced by the used shape\napproximations.\n", "link": "http://arxiv.org/abs/2405.10765v1", "date": "2024-05-17", "relevancy": 2.0791, "topK": [{"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.5725}, {"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.5364}, {"title": "Dynamic Gaussians Mesh: Consistent Mesh Reconstruction from Monocular\n  Videos", "link": "http://arxiv.org/abs/2404.12379v2", "similarity": 0.4821}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Fast%20Collision%20Probability%20Estimation%20for%20Automated%20Driving%20using%0A%20%20Multi-circular%20Shape%20Approximations&body=Title%3A%20Fast%20Collision%20Probability%20Estimation%20for%20Automated%20Driving%20using%0A%20%20Multi-circular%20Shape%20Approximations%0AAuthor%3A%20Leon%20Tolksdorf%20and%20Christian%20Birkner%20and%20Arturo%20Tejada%20and%20Nathan%20van%20de%20Wouw%0AAbstract%3A%20%20%20Many%20state-of-the-art%20methods%20for%20safety%20assessment%20and%20motion%20planning%20for%0Aautomated%20driving%20require%20estimation%20of%20the%20probability%20of%20collision%20%28POC%29.%20To%0Aestimate%20the%20POC%2C%20a%20shape%20approximation%20of%20the%20colliding%20actors%20and%20probability%0Adensity%20functions%20of%20the%20associated%20uncertain%20kinematic%20variables%20are%20required.%0AEven%20with%20such%20information%20available%2C%20the%20derivation%20of%20the%20POC%20is%20in%20general%2C%0Ai.e.%2C%20for%20any%20shape%20and%20density%2C%20only%20possible%20with%20Monte%20Carlo%20sampling%20%28MCS%29.%0ARandom%20sampling%20of%20the%20POC%2C%20however%2C%20is%20challenging%20as%20computational%20resources%0Aare%20limited%20in%20real-world%20applications.%20We%20present%20expressions%20for%20the%20POC%20in%0Athe%20presence%20of%20Gaussian%20uncertainties%2C%20based%20on%20multi-circular%20shape%0Aapproximations.%20In%20addition%2C%20we%20show%20that%20the%20proposed%20approach%20is%0Acomputationally%20more%20efficient%20than%20MCS.%20Lastly%2C%20we%20provide%20a%20method%20for%20upper%0Aand%20lower%20bounding%20the%20estimation%20error%20for%20the%20POC%20induced%20by%20the%20used%20shape%0Aapproximations.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2405.10765v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DFast%2520Collision%2520Probability%2520Estimation%2520for%2520Automated%2520Driving%2520using%250A%2520%2520Multi-circular%2520Shape%2520Approximations%26entry.906535625%3DLeon%2520Tolksdorf%2520and%2520Christian%2520Birkner%2520and%2520Arturo%2520Tejada%2520and%2520Nathan%2520van%2520de%2520Wouw%26entry.1292438233%3D%2520%2520Many%2520state-of-the-art%2520methods%2520for%2520safety%2520assessment%2520and%2520motion%2520planning%2520for%250Aautomated%2520driving%2520require%2520estimation%2520of%2520the%2520probability%2520of%2520collision%2520%2528POC%2529.%2520To%250Aestimate%2520the%2520POC%252C%2520a%2520shape%2520approximation%2520of%2520the%2520colliding%2520actors%2520and%2520probability%250Adensity%2520functions%2520of%2520the%2520associated%2520uncertain%2520kinematic%2520variables%2520are%2520required.%250AEven%2520with%2520such%2520information%2520available%252C%2520the%2520derivation%2520of%2520the%2520POC%2520is%2520in%2520general%252C%250Ai.e.%252C%2520for%2520any%2520shape%2520and%2520density%252C%2520only%2520possible%2520with%2520Monte%2520Carlo%2520sampling%2520%2528MCS%2529.%250ARandom%2520sampling%2520of%2520the%2520POC%252C%2520however%252C%2520is%2520challenging%2520as%2520computational%2520resources%250Aare%2520limited%2520in%2520real-world%2520applications.%2520We%2520present%2520expressions%2520for%2520the%2520POC%2520in%250Athe%2520presence%2520of%2520Gaussian%2520uncertainties%252C%2520based%2520on%2520multi-circular%2520shape%250Aapproximations.%2520In%2520addition%252C%2520we%2520show%2520that%2520the%2520proposed%2520approach%2520is%250Acomputationally%2520more%2520efficient%2520than%2520MCS.%2520Lastly%252C%2520we%2520provide%2520a%2520method%2520for%2520upper%250Aand%2520lower%2520bounding%2520the%2520estimation%2520error%2520for%2520the%2520POC%2520induced%2520by%2520the%2520used%2520shape%250Aapproximations.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2405.10765v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Fast%20Collision%20Probability%20Estimation%20for%20Automated%20Driving%20using%0A%20%20Multi-circular%20Shape%20Approximations&entry.906535625=Leon%20Tolksdorf%20and%20Christian%20Birkner%20and%20Arturo%20Tejada%20and%20Nathan%20van%20de%20Wouw&entry.1292438233=%20%20Many%20state-of-the-art%20methods%20for%20safety%20assessment%20and%20motion%20planning%20for%0Aautomated%20driving%20require%20estimation%20of%20the%20probability%20of%20collision%20%28POC%29.%20To%0Aestimate%20the%20POC%2C%20a%20shape%20approximation%20of%20the%20colliding%20actors%20and%20probability%0Adensity%20functions%20of%20the%20associated%20uncertain%20kinematic%20variables%20are%20required.%0AEven%20with%20such%20information%20available%2C%20the%20derivation%20of%20the%20POC%20is%20in%20general%2C%0Ai.e.%2C%20for%20any%20shape%20and%20density%2C%20only%20possible%20with%20Monte%20Carlo%20sampling%20%28MCS%29.%0ARandom%20sampling%20of%20the%20POC%2C%20however%2C%20is%20challenging%20as%20computational%20resources%0Aare%20limited%20in%20real-world%20applications.%20We%20present%20expressions%20for%20the%20POC%20in%0Athe%20presence%20of%20Gaussian%20uncertainties%2C%20based%20on%20multi-circular%20shape%0Aapproximations.%20In%20addition%2C%20we%20show%20that%20the%20proposed%20approach%20is%0Acomputationally%20more%20efficient%20than%20MCS.%20Lastly%2C%20we%20provide%20a%20method%20for%20upper%0Aand%20lower%20bounding%20the%20estimation%20error%20for%20the%20POC%20induced%20by%20the%20used%20shape%0Aapproximations.%0A&entry.1838667208=http%3A//arxiv.org/abs/2405.10765v1&entry.124074799=Read"},
{"title": "Training Compute Thresholds: Features and Functions in AI Governance", "author": "Lennart Heim", "abstract": "  This paper examines the use of training compute thresholds as a tool for\ngoverning artificial intelligence (AI) systems. We argue that compute\nthresholds serve as a valuable trigger for further evaluation of AI models,\nrather than being the sole determinant of the regulation. Key advantages of\ncompute thresholds include their correlation with model capabilities and risks,\nquantifiability, ease of measurement, robustness to circumvention, knowability\nbefore model development and deployment, potential for external verification,\nand targeted scope. Compute thresholds provide a practical starting point for\nidentifying potentially high-risk models and can be used as an initial filter\nin AI governance frameworks alongside other sector-specific regulations and\nbroader governance measures.\n", "link": "http://arxiv.org/abs/2405.10799v1", "date": "2024-05-17", "relevancy": 2.0721, "topK": [{"title": "Safe and efficient training of a control agent", "link": "https://patents.google.com/patent/US11709462B2/en", "similarity": 0.4237}, {"title": "ToDo: Token Downsampling for Efficient Generation of High-Resolution\n  Images", "link": "http://arxiv.org/abs/2402.13573v2", "similarity": 0.4103}, {"title": "Genie: Generative Interactive Environments", "link": "http://arxiv.org/abs/2402.15391v1", "similarity": 0.4093}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Training%20Compute%20Thresholds%3A%20Features%20and%20Functions%20in%20AI%20Governance&body=Title%3A%20Training%20Compute%20Thresholds%3A%20Features%20and%20Functions%20in%20AI%20Governance%0AAuthor%3A%20Lennart%20Heim%0AAbstract%3A%20%20%20This%20paper%20examines%20the%20use%20of%20training%20compute%20thresholds%20as%20a%20tool%20for%0Agoverning%20artificial%20intelligence%20%28AI%29%20systems.%20We%20argue%20that%20compute%0Athresholds%20serve%20as%20a%20valuable%20trigger%20for%20further%20evaluation%20of%20AI%20models%2C%0Arather%20than%20being%20the%20sole%20determinant%20of%20the%20regulation.%20Key%20advantages%20of%0Acompute%20thresholds%20include%20their%20correlation%20with%20model%20capabilities%20and%20risks%2C%0Aquantifiability%2C%20ease%20of%20measurement%2C%20robustness%20to%20circumvention%2C%20knowability%0Abefore%20model%20development%20and%20deployment%2C%20potential%20for%20external%20verification%2C%0Aand%20targeted%20scope.%20Compute%20thresholds%20provide%20a%20practical%20starting%20point%20for%0Aidentifying%20potentially%20high-risk%20models%20and%20can%20be%20used%20as%20an%20initial%20filter%0Ain%20AI%20governance%20frameworks%20alongside%20other%20sector-specific%20regulations%20and%0Abroader%20governance%20measures.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2405.10799v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DTraining%2520Compute%2520Thresholds%253A%2520Features%2520and%2520Functions%2520in%2520AI%2520Governance%26entry.906535625%3DLennart%2520Heim%26entry.1292438233%3D%2520%2520This%2520paper%2520examines%2520the%2520use%2520of%2520training%2520compute%2520thresholds%2520as%2520a%2520tool%2520for%250Agoverning%2520artificial%2520intelligence%2520%2528AI%2529%2520systems.%2520We%2520argue%2520that%2520compute%250Athresholds%2520serve%2520as%2520a%2520valuable%2520trigger%2520for%2520further%2520evaluation%2520of%2520AI%2520models%252C%250Arather%2520than%2520being%2520the%2520sole%2520determinant%2520of%2520the%2520regulation.%2520Key%2520advantages%2520of%250Acompute%2520thresholds%2520include%2520their%2520correlation%2520with%2520model%2520capabilities%2520and%2520risks%252C%250Aquantifiability%252C%2520ease%2520of%2520measurement%252C%2520robustness%2520to%2520circumvention%252C%2520knowability%250Abefore%2520model%2520development%2520and%2520deployment%252C%2520potential%2520for%2520external%2520verification%252C%250Aand%2520targeted%2520scope.%2520Compute%2520thresholds%2520provide%2520a%2520practical%2520starting%2520point%2520for%250Aidentifying%2520potentially%2520high-risk%2520models%2520and%2520can%2520be%2520used%2520as%2520an%2520initial%2520filter%250Ain%2520AI%2520governance%2520frameworks%2520alongside%2520other%2520sector-specific%2520regulations%2520and%250Abroader%2520governance%2520measures.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2405.10799v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Training%20Compute%20Thresholds%3A%20Features%20and%20Functions%20in%20AI%20Governance&entry.906535625=Lennart%20Heim&entry.1292438233=%20%20This%20paper%20examines%20the%20use%20of%20training%20compute%20thresholds%20as%20a%20tool%20for%0Agoverning%20artificial%20intelligence%20%28AI%29%20systems.%20We%20argue%20that%20compute%0Athresholds%20serve%20as%20a%20valuable%20trigger%20for%20further%20evaluation%20of%20AI%20models%2C%0Arather%20than%20being%20the%20sole%20determinant%20of%20the%20regulation.%20Key%20advantages%20of%0Acompute%20thresholds%20include%20their%20correlation%20with%20model%20capabilities%20and%20risks%2C%0Aquantifiability%2C%20ease%20of%20measurement%2C%20robustness%20to%20circumvention%2C%20knowability%0Abefore%20model%20development%20and%20deployment%2C%20potential%20for%20external%20verification%2C%0Aand%20targeted%20scope.%20Compute%20thresholds%20provide%20a%20practical%20starting%20point%20for%0Aidentifying%20potentially%20high-risk%20models%20and%20can%20be%20used%20as%20an%20initial%20filter%0Ain%20AI%20governance%20frameworks%20alongside%20other%20sector-specific%20regulations%20and%0Abroader%20governance%20measures.%0A&entry.1838667208=http%3A//arxiv.org/abs/2405.10799v1&entry.124074799=Read"},
{"title": "Robust Online Learning over Networks", "author": "Nicola Bastianello and Diego Deplano and Mauro Franceschelli and Karl H. Johansson", "abstract": "  The recent deployment of multi-agent networks has enabled the distributed\nsolution of learning problems, where agents cooperate to train a global model\nwithout sharing their local, private data. This work specifically targets some\nprevalent challenges inherent to distributed learning: (i) online training,\ni.e., the local data change over time; (ii) asynchronous agent computations;\n(iii) unreliable and limited communications; and (iv) inexact local\ncomputations. To tackle these challenges, we apply the Distributed Operator\nTheoretical (DOT) version of the Alternating Direction Method of Multipliers\n(ADMM), which we call \"DOT-ADMM\". We prove that if the DOT-ADMM operator is\nmetric subregular, then it converges with a linear rate for a large class of\n(not necessarily strongly) convex learning problems toward a bounded\nneighborhood of the optimal time-varying solution, and characterize how such\nneighborhood depends on (i)-(iv). We first derive an easy-to-verify condition\nfor ensuring the metric subregularity of an operator, followed by tutorial\nexamples on linear and logistic regression problems. We corroborate the\ntheoretical analysis with numerical simulations comparing DOT-ADMM with other\nstate-of-the-art algorithms, showing that only the proposed algorithm exhibits\nrobustness to (i)-(iv).\n", "link": "http://arxiv.org/abs/2309.00520v2", "date": "2024-05-17", "relevancy": 2.071, "topK": [{"title": "Boosted dynamic neural networks", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/26302", "similarity": 0.5322}, {"title": "DDM-NET: End-to-end learning of keypoint feature Detection, Description and Matching for 3D localization", "link": "https://arxiv.org/abs/2212.04575", "similarity": 0.5089}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.5038}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Robust%20Online%20Learning%20over%20Networks&body=Title%3A%20Robust%20Online%20Learning%20over%20Networks%0AAuthor%3A%20Nicola%20Bastianello%20and%20Diego%20Deplano%20and%20Mauro%20Franceschelli%20and%20Karl%20H.%20Johansson%0AAbstract%3A%20%20%20The%20recent%20deployment%20of%20multi-agent%20networks%20has%20enabled%20the%20distributed%0Asolution%20of%20learning%20problems%2C%20where%20agents%20cooperate%20to%20train%20a%20global%20model%0Awithout%20sharing%20their%20local%2C%20private%20data.%20This%20work%20specifically%20targets%20some%0Aprevalent%20challenges%20inherent%20to%20distributed%20learning%3A%20%28i%29%20online%20training%2C%0Ai.e.%2C%20the%20local%20data%20change%20over%20time%3B%20%28ii%29%20asynchronous%20agent%20computations%3B%0A%28iii%29%20unreliable%20and%20limited%20communications%3B%20and%20%28iv%29%20inexact%20local%0Acomputations.%20To%20tackle%20these%20challenges%2C%20we%20apply%20the%20Distributed%20Operator%0ATheoretical%20%28DOT%29%20version%20of%20the%20Alternating%20Direction%20Method%20of%20Multipliers%0A%28ADMM%29%2C%20which%20we%20call%20%22DOT-ADMM%22.%20We%20prove%20that%20if%20the%20DOT-ADMM%20operator%20is%0Ametric%20subregular%2C%20then%20it%20converges%20with%20a%20linear%20rate%20for%20a%20large%20class%20of%0A%28not%20necessarily%20strongly%29%20convex%20learning%20problems%20toward%20a%20bounded%0Aneighborhood%20of%20the%20optimal%20time-varying%20solution%2C%20and%20characterize%20how%20such%0Aneighborhood%20depends%20on%20%28i%29-%28iv%29.%20We%20first%20derive%20an%20easy-to-verify%20condition%0Afor%20ensuring%20the%20metric%20subregularity%20of%20an%20operator%2C%20followed%20by%20tutorial%0Aexamples%20on%20linear%20and%20logistic%20regression%20problems.%20We%20corroborate%20the%0Atheoretical%20analysis%20with%20numerical%20simulations%20comparing%20DOT-ADMM%20with%20other%0Astate-of-the-art%20algorithms%2C%20showing%20that%20only%20the%20proposed%20algorithm%20exhibits%0Arobustness%20to%20%28i%29-%28iv%29.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2309.00520v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DRobust%2520Online%2520Learning%2520over%2520Networks%26entry.906535625%3DNicola%2520Bastianello%2520and%2520Diego%2520Deplano%2520and%2520Mauro%2520Franceschelli%2520and%2520Karl%2520H.%2520Johansson%26entry.1292438233%3D%2520%2520The%2520recent%2520deployment%2520of%2520multi-agent%2520networks%2520has%2520enabled%2520the%2520distributed%250Asolution%2520of%2520learning%2520problems%252C%2520where%2520agents%2520cooperate%2520to%2520train%2520a%2520global%2520model%250Awithout%2520sharing%2520their%2520local%252C%2520private%2520data.%2520This%2520work%2520specifically%2520targets%2520some%250Aprevalent%2520challenges%2520inherent%2520to%2520distributed%2520learning%253A%2520%2528i%2529%2520online%2520training%252C%250Ai.e.%252C%2520the%2520local%2520data%2520change%2520over%2520time%253B%2520%2528ii%2529%2520asynchronous%2520agent%2520computations%253B%250A%2528iii%2529%2520unreliable%2520and%2520limited%2520communications%253B%2520and%2520%2528iv%2529%2520inexact%2520local%250Acomputations.%2520To%2520tackle%2520these%2520challenges%252C%2520we%2520apply%2520the%2520Distributed%2520Operator%250ATheoretical%2520%2528DOT%2529%2520version%2520of%2520the%2520Alternating%2520Direction%2520Method%2520of%2520Multipliers%250A%2528ADMM%2529%252C%2520which%2520we%2520call%2520%2522DOT-ADMM%2522.%2520We%2520prove%2520that%2520if%2520the%2520DOT-ADMM%2520operator%2520is%250Ametric%2520subregular%252C%2520then%2520it%2520converges%2520with%2520a%2520linear%2520rate%2520for%2520a%2520large%2520class%2520of%250A%2528not%2520necessarily%2520strongly%2529%2520convex%2520learning%2520problems%2520toward%2520a%2520bounded%250Aneighborhood%2520of%2520the%2520optimal%2520time-varying%2520solution%252C%2520and%2520characterize%2520how%2520such%250Aneighborhood%2520depends%2520on%2520%2528i%2529-%2528iv%2529.%2520We%2520first%2520derive%2520an%2520easy-to-verify%2520condition%250Afor%2520ensuring%2520the%2520metric%2520subregularity%2520of%2520an%2520operator%252C%2520followed%2520by%2520tutorial%250Aexamples%2520on%2520linear%2520and%2520logistic%2520regression%2520problems.%2520We%2520corroborate%2520the%250Atheoretical%2520analysis%2520with%2520numerical%2520simulations%2520comparing%2520DOT-ADMM%2520with%2520other%250Astate-of-the-art%2520algorithms%252C%2520showing%2520that%2520only%2520the%2520proposed%2520algorithm%2520exhibits%250Arobustness%2520to%2520%2528i%2529-%2528iv%2529.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2309.00520v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Robust%20Online%20Learning%20over%20Networks&entry.906535625=Nicola%20Bastianello%20and%20Diego%20Deplano%20and%20Mauro%20Franceschelli%20and%20Karl%20H.%20Johansson&entry.1292438233=%20%20The%20recent%20deployment%20of%20multi-agent%20networks%20has%20enabled%20the%20distributed%0Asolution%20of%20learning%20problems%2C%20where%20agents%20cooperate%20to%20train%20a%20global%20model%0Awithout%20sharing%20their%20local%2C%20private%20data.%20This%20work%20specifically%20targets%20some%0Aprevalent%20challenges%20inherent%20to%20distributed%20learning%3A%20%28i%29%20online%20training%2C%0Ai.e.%2C%20the%20local%20data%20change%20over%20time%3B%20%28ii%29%20asynchronous%20agent%20computations%3B%0A%28iii%29%20unreliable%20and%20limited%20communications%3B%20and%20%28iv%29%20inexact%20local%0Acomputations.%20To%20tackle%20these%20challenges%2C%20we%20apply%20the%20Distributed%20Operator%0ATheoretical%20%28DOT%29%20version%20of%20the%20Alternating%20Direction%20Method%20of%20Multipliers%0A%28ADMM%29%2C%20which%20we%20call%20%22DOT-ADMM%22.%20We%20prove%20that%20if%20the%20DOT-ADMM%20operator%20is%0Ametric%20subregular%2C%20then%20it%20converges%20with%20a%20linear%20rate%20for%20a%20large%20class%20of%0A%28not%20necessarily%20strongly%29%20convex%20learning%20problems%20toward%20a%20bounded%0Aneighborhood%20of%20the%20optimal%20time-varying%20solution%2C%20and%20characterize%20how%20such%0Aneighborhood%20depends%20on%20%28i%29-%28iv%29.%20We%20first%20derive%20an%20easy-to-verify%20condition%0Afor%20ensuring%20the%20metric%20subregularity%20of%20an%20operator%2C%20followed%20by%20tutorial%0Aexamples%20on%20linear%20and%20logistic%20regression%20problems.%20We%20corroborate%20the%0Atheoretical%20analysis%20with%20numerical%20simulations%20comparing%20DOT-ADMM%20with%20other%0Astate-of-the-art%20algorithms%2C%20showing%20that%20only%20the%20proposed%20algorithm%20exhibits%0Arobustness%20to%20%28i%29-%28iv%29.%0A&entry.1838667208=http%3A//arxiv.org/abs/2309.00520v2&entry.124074799=Read"},
{"title": "Anatomically aware dual-hop learning for pulmonary embolism detection in\n  CT pulmonary angiograms", "author": "Florin Condrea and Saikiran Rapaka and Lucian Itu and Puneet Sharma and Jonathan Sperl and A Mohamed Ali and Marius Leordeanu", "abstract": "  Pulmonary Embolisms (PE) represent a leading cause of cardiovascular death.\nWhile medical imaging, through computed tomographic pulmonary angiography\n(CTPA), represents the gold standard for PE diagnosis, it is still susceptible\nto misdiagnosis or significant diagnosis delays, which may be fatal for\ncritical cases. Despite the recently demonstrated power of deep learning to\nbring a significant boost in performance in a wide range of medical imaging\ntasks, there are still very few published researches on automatic pulmonary\nembolism detection. Herein we introduce a deep learning based approach, which\nefficiently combines computer vision and deep neural networks for pulmonary\nembolism detection in CTPA. Our method features novel improvements along three\northogonal axes: 1) automatic detection of anatomical structures; 2) anatomical\naware pretraining, and 3) a dual-hop deep neural net for PE detection. We\nobtain state-of-the-art results on the publicly available multicenter\nlarge-scale RSNA dataset.\n", "link": "http://arxiv.org/abs/2303.17593v2", "date": "2024-05-17", "relevancy": 2.0455, "topK": [{"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.5353}, {"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.5105}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5027}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Anatomically%20aware%20dual-hop%20learning%20for%20pulmonary%20embolism%20detection%20in%0A%20%20CT%20pulmonary%20angiograms&body=Title%3A%20Anatomically%20aware%20dual-hop%20learning%20for%20pulmonary%20embolism%20detection%20in%0A%20%20CT%20pulmonary%20angiograms%0AAuthor%3A%20Florin%20Condrea%20and%20Saikiran%20Rapaka%20and%20Lucian%20Itu%20and%20Puneet%20Sharma%20and%20Jonathan%20Sperl%20and%20A%20Mohamed%20Ali%20and%20Marius%20Leordeanu%0AAbstract%3A%20%20%20Pulmonary%20Embolisms%20%28PE%29%20represent%20a%20leading%20cause%20of%20cardiovascular%20death.%0AWhile%20medical%20imaging%2C%20through%20computed%20tomographic%20pulmonary%20angiography%0A%28CTPA%29%2C%20represents%20the%20gold%20standard%20for%20PE%20diagnosis%2C%20it%20is%20still%20susceptible%0Ato%20misdiagnosis%20or%20significant%20diagnosis%20delays%2C%20which%20may%20be%20fatal%20for%0Acritical%20cases.%20Despite%20the%20recently%20demonstrated%20power%20of%20deep%20learning%20to%0Abring%20a%20significant%20boost%20in%20performance%20in%20a%20wide%20range%20of%20medical%20imaging%0Atasks%2C%20there%20are%20still%20very%20few%20published%20researches%20on%20automatic%20pulmonary%0Aembolism%20detection.%20Herein%20we%20introduce%20a%20deep%20learning%20based%20approach%2C%20which%0Aefficiently%20combines%20computer%20vision%20and%20deep%20neural%20networks%20for%20pulmonary%0Aembolism%20detection%20in%20CTPA.%20Our%20method%20features%20novel%20improvements%20along%20three%0Aorthogonal%20axes%3A%201%29%20automatic%20detection%20of%20anatomical%20structures%3B%202%29%20anatomical%0Aaware%20pretraining%2C%20and%203%29%20a%20dual-hop%20deep%20neural%20net%20for%20PE%20detection.%20We%0Aobtain%20state-of-the-art%20results%20on%20the%20publicly%20available%20multicenter%0Alarge-scale%20RSNA%20dataset.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2303.17593v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DAnatomically%2520aware%2520dual-hop%2520learning%2520for%2520pulmonary%2520embolism%2520detection%2520in%250A%2520%2520CT%2520pulmonary%2520angiograms%26entry.906535625%3DFlorin%2520Condrea%2520and%2520Saikiran%2520Rapaka%2520and%2520Lucian%2520Itu%2520and%2520Puneet%2520Sharma%2520and%2520Jonathan%2520Sperl%2520and%2520A%2520Mohamed%2520Ali%2520and%2520Marius%2520Leordeanu%26entry.1292438233%3D%2520%2520Pulmonary%2520Embolisms%2520%2528PE%2529%2520represent%2520a%2520leading%2520cause%2520of%2520cardiovascular%2520death.%250AWhile%2520medical%2520imaging%252C%2520through%2520computed%2520tomographic%2520pulmonary%2520angiography%250A%2528CTPA%2529%252C%2520represents%2520the%2520gold%2520standard%2520for%2520PE%2520diagnosis%252C%2520it%2520is%2520still%2520susceptible%250Ato%2520misdiagnosis%2520or%2520significant%2520diagnosis%2520delays%252C%2520which%2520may%2520be%2520fatal%2520for%250Acritical%2520cases.%2520Despite%2520the%2520recently%2520demonstrated%2520power%2520of%2520deep%2520learning%2520to%250Abring%2520a%2520significant%2520boost%2520in%2520performance%2520in%2520a%2520wide%2520range%2520of%2520medical%2520imaging%250Atasks%252C%2520there%2520are%2520still%2520very%2520few%2520published%2520researches%2520on%2520automatic%2520pulmonary%250Aembolism%2520detection.%2520Herein%2520we%2520introduce%2520a%2520deep%2520learning%2520based%2520approach%252C%2520which%250Aefficiently%2520combines%2520computer%2520vision%2520and%2520deep%2520neural%2520networks%2520for%2520pulmonary%250Aembolism%2520detection%2520in%2520CTPA.%2520Our%2520method%2520features%2520novel%2520improvements%2520along%2520three%250Aorthogonal%2520axes%253A%25201%2529%2520automatic%2520detection%2520of%2520anatomical%2520structures%253B%25202%2529%2520anatomical%250Aaware%2520pretraining%252C%2520and%25203%2529%2520a%2520dual-hop%2520deep%2520neural%2520net%2520for%2520PE%2520detection.%2520We%250Aobtain%2520state-of-the-art%2520results%2520on%2520the%2520publicly%2520available%2520multicenter%250Alarge-scale%2520RSNA%2520dataset.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2303.17593v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Anatomically%20aware%20dual-hop%20learning%20for%20pulmonary%20embolism%20detection%20in%0A%20%20CT%20pulmonary%20angiograms&entry.906535625=Florin%20Condrea%20and%20Saikiran%20Rapaka%20and%20Lucian%20Itu%20and%20Puneet%20Sharma%20and%20Jonathan%20Sperl%20and%20A%20Mohamed%20Ali%20and%20Marius%20Leordeanu&entry.1292438233=%20%20Pulmonary%20Embolisms%20%28PE%29%20represent%20a%20leading%20cause%20of%20cardiovascular%20death.%0AWhile%20medical%20imaging%2C%20through%20computed%20tomographic%20pulmonary%20angiography%0A%28CTPA%29%2C%20represents%20the%20gold%20standard%20for%20PE%20diagnosis%2C%20it%20is%20still%20susceptible%0Ato%20misdiagnosis%20or%20significant%20diagnosis%20delays%2C%20which%20may%20be%20fatal%20for%0Acritical%20cases.%20Despite%20the%20recently%20demonstrated%20power%20of%20deep%20learning%20to%0Abring%20a%20significant%20boost%20in%20performance%20in%20a%20wide%20range%20of%20medical%20imaging%0Atasks%2C%20there%20are%20still%20very%20few%20published%20researches%20on%20automatic%20pulmonary%0Aembolism%20detection.%20Herein%20we%20introduce%20a%20deep%20learning%20based%20approach%2C%20which%0Aefficiently%20combines%20computer%20vision%20and%20deep%20neural%20networks%20for%20pulmonary%0Aembolism%20detection%20in%20CTPA.%20Our%20method%20features%20novel%20improvements%20along%20three%0Aorthogonal%20axes%3A%201%29%20automatic%20detection%20of%20anatomical%20structures%3B%202%29%20anatomical%0Aaware%20pretraining%2C%20and%203%29%20a%20dual-hop%20deep%20neural%20net%20for%20PE%20detection.%20We%0Aobtain%20state-of-the-art%20results%20on%20the%20publicly%20available%20multicenter%0Alarge-scale%20RSNA%20dataset.%0A&entry.1838667208=http%3A//arxiv.org/abs/2303.17593v2&entry.124074799=Read"},
{"title": "Exploiting Autoencoder's Weakness to Generate Pseudo Anomalies", "author": "Marcella Astrid and Muhammad Zaigham Zaheer and Djamila Aouada and Seung-Ik Lee", "abstract": "  Due to the rare occurrence of anomalous events, a typical approach to anomaly\ndetection is to train an autoencoder (AE) with normal data only so that it\nlearns the patterns or representations of the normal training data. At test\ntime, the trained AE is expected to well reconstruct normal but to poorly\nreconstruct anomalous data. However, contrary to the expectation, anomalous\ndata is often well reconstructed as well. In order to further separate the\nreconstruction quality between normal and anomalous data, we propose creating\npseudo anomalies from learned adaptive noise by exploiting the aforementioned\nweakness of AE, i.e., reconstructing anomalies too well. The generated noise is\nadded to the normal data to create pseudo anomalies. Extensive experiments on\nPed2, Avenue, ShanghaiTech, CIFAR-10, and KDDCUP datasets demonstrate the\neffectiveness and generic applicability of our approach in improving the\ndiscriminative capability of AEs for anomaly detection.\n", "link": "http://arxiv.org/abs/2405.05886v2", "date": "2024-05-17", "relevancy": 2.0422, "topK": [{"title": "IAE: Implicit Autoencoder for Point Cloud Self-supervised Representation Learning", "link": "https://arxiv.org/abs/2201.00785", "similarity": 0.527}, {"title": "Breadcrumbs: Adversarial class-balanced sampling for long-tailed recognition", "link": "https://link.springer.com/chapter/10.1007/978-3-031-20053-3_37", "similarity": 0.5029}, {"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.4884}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Exploiting%20Autoencoder%27s%20Weakness%20to%20Generate%20Pseudo%20Anomalies&body=Title%3A%20Exploiting%20Autoencoder%27s%20Weakness%20to%20Generate%20Pseudo%20Anomalies%0AAuthor%3A%20Marcella%20Astrid%20and%20Muhammad%20Zaigham%20Zaheer%20and%20Djamila%20Aouada%20and%20Seung-Ik%20Lee%0AAbstract%3A%20%20%20Due%20to%20the%20rare%20occurrence%20of%20anomalous%20events%2C%20a%20typical%20approach%20to%20anomaly%0Adetection%20is%20to%20train%20an%20autoencoder%20%28AE%29%20with%20normal%20data%20only%20so%20that%20it%0Alearns%20the%20patterns%20or%20representations%20of%20the%20normal%20training%20data.%20At%20test%0Atime%2C%20the%20trained%20AE%20is%20expected%20to%20well%20reconstruct%20normal%20but%20to%20poorly%0Areconstruct%20anomalous%20data.%20However%2C%20contrary%20to%20the%20expectation%2C%20anomalous%0Adata%20is%20often%20well%20reconstructed%20as%20well.%20In%20order%20to%20further%20separate%20the%0Areconstruction%20quality%20between%20normal%20and%20anomalous%20data%2C%20we%20propose%20creating%0Apseudo%20anomalies%20from%20learned%20adaptive%20noise%20by%20exploiting%20the%20aforementioned%0Aweakness%20of%20AE%2C%20i.e.%2C%20reconstructing%20anomalies%20too%20well.%20The%20generated%20noise%20is%0Aadded%20to%20the%20normal%20data%20to%20create%20pseudo%20anomalies.%20Extensive%20experiments%20on%0APed2%2C%20Avenue%2C%20ShanghaiTech%2C%20CIFAR-10%2C%20and%20KDDCUP%20datasets%20demonstrate%20the%0Aeffectiveness%20and%20generic%20applicability%20of%20our%20approach%20in%20improving%20the%0Adiscriminative%20capability%20of%20AEs%20for%20anomaly%20detection.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2405.05886v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DExploiting%2520Autoencoder%2527s%2520Weakness%2520to%2520Generate%2520Pseudo%2520Anomalies%26entry.906535625%3DMarcella%2520Astrid%2520and%2520Muhammad%2520Zaigham%2520Zaheer%2520and%2520Djamila%2520Aouada%2520and%2520Seung-Ik%2520Lee%26entry.1292438233%3D%2520%2520Due%2520to%2520the%2520rare%2520occurrence%2520of%2520anomalous%2520events%252C%2520a%2520typical%2520approach%2520to%2520anomaly%250Adetection%2520is%2520to%2520train%2520an%2520autoencoder%2520%2528AE%2529%2520with%2520normal%2520data%2520only%2520so%2520that%2520it%250Alearns%2520the%2520patterns%2520or%2520representations%2520of%2520the%2520normal%2520training%2520data.%2520At%2520test%250Atime%252C%2520the%2520trained%2520AE%2520is%2520expected%2520to%2520well%2520reconstruct%2520normal%2520but%2520to%2520poorly%250Areconstruct%2520anomalous%2520data.%2520However%252C%2520contrary%2520to%2520the%2520expectation%252C%2520anomalous%250Adata%2520is%2520often%2520well%2520reconstructed%2520as%2520well.%2520In%2520order%2520to%2520further%2520separate%2520the%250Areconstruction%2520quality%2520between%2520normal%2520and%2520anomalous%2520data%252C%2520we%2520propose%2520creating%250Apseudo%2520anomalies%2520from%2520learned%2520adaptive%2520noise%2520by%2520exploiting%2520the%2520aforementioned%250Aweakness%2520of%2520AE%252C%2520i.e.%252C%2520reconstructing%2520anomalies%2520too%2520well.%2520The%2520generated%2520noise%2520is%250Aadded%2520to%2520the%2520normal%2520data%2520to%2520create%2520pseudo%2520anomalies.%2520Extensive%2520experiments%2520on%250APed2%252C%2520Avenue%252C%2520ShanghaiTech%252C%2520CIFAR-10%252C%2520and%2520KDDCUP%2520datasets%2520demonstrate%2520the%250Aeffectiveness%2520and%2520generic%2520applicability%2520of%2520our%2520approach%2520in%2520improving%2520the%250Adiscriminative%2520capability%2520of%2520AEs%2520for%2520anomaly%2520detection.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2405.05886v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Exploiting%20Autoencoder%27s%20Weakness%20to%20Generate%20Pseudo%20Anomalies&entry.906535625=Marcella%20Astrid%20and%20Muhammad%20Zaigham%20Zaheer%20and%20Djamila%20Aouada%20and%20Seung-Ik%20Lee&entry.1292438233=%20%20Due%20to%20the%20rare%20occurrence%20of%20anomalous%20events%2C%20a%20typical%20approach%20to%20anomaly%0Adetection%20is%20to%20train%20an%20autoencoder%20%28AE%29%20with%20normal%20data%20only%20so%20that%20it%0Alearns%20the%20patterns%20or%20representations%20of%20the%20normal%20training%20data.%20At%20test%0Atime%2C%20the%20trained%20AE%20is%20expected%20to%20well%20reconstruct%20normal%20but%20to%20poorly%0Areconstruct%20anomalous%20data.%20However%2C%20contrary%20to%20the%20expectation%2C%20anomalous%0Adata%20is%20often%20well%20reconstructed%20as%20well.%20In%20order%20to%20further%20separate%20the%0Areconstruction%20quality%20between%20normal%20and%20anomalous%20data%2C%20we%20propose%20creating%0Apseudo%20anomalies%20from%20learned%20adaptive%20noise%20by%20exploiting%20the%20aforementioned%0Aweakness%20of%20AE%2C%20i.e.%2C%20reconstructing%20anomalies%20too%20well.%20The%20generated%20noise%20is%0Aadded%20to%20the%20normal%20data%20to%20create%20pseudo%20anomalies.%20Extensive%20experiments%20on%0APed2%2C%20Avenue%2C%20ShanghaiTech%2C%20CIFAR-10%2C%20and%20KDDCUP%20datasets%20demonstrate%20the%0Aeffectiveness%20and%20generic%20applicability%20of%20our%20approach%20in%20improving%20the%0Adiscriminative%20capability%20of%20AEs%20for%20anomaly%20detection.%0A&entry.1838667208=http%3A//arxiv.org/abs/2405.05886v2&entry.124074799=Read"},
{"title": "A Nonlinear Model Predictive Control for Automated Drifting with a\n  Standard Passenger Vehicle", "author": "Stan Meijer and Alberto Bertipaglia and Barys Shyrokau", "abstract": "  This paper presents a novel approach to automated drifting with a standard\npassenger vehicle, which involves a Nonlinear Model Predictive Control to\nstabilise and maintain the vehicle at high sideslip angle conditions. The\nproposed controller architecture is split into three components. The first part\nconsists of the offline computed equilibrium maps, which provide the\nequilibrium points for each vehicle state given the desired sideslip angle and\nradius of the path. The second is the predictive controller minimising the\nerrors between the equilibrium and actual vehicle states. The third is a\npath-following controller, which reduces the path error, altering the\nequilibrium curvature path. In a high-fidelity simulation environment, we\nvalidate the controller architecture capacity to stabilise the vehicle in\nautomated drifting along a desired path, with a maximal lateral path deviation\nof 1 m. In the experiments with a standard passenger vehicle, we demonstrate\nthat the proposed approach is capable of bringing and maintaining the vehicle\nat the desired 30 deg sideslip angle in both high and low friction conditions.\n", "link": "http://arxiv.org/abs/2405.10859v1", "date": "2024-05-17", "relevancy": 2.0304, "topK": [{"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.542}, {"title": "Egocentric Computer Vision for Hands-Free Robotic Wheelchair Navigation", "link": "https://link.springer.com/article/10.1007/s10846-023-01807-4", "similarity": 0.5028}, {"title": "Safe and efficient training of a control agent", "link": "https://patents.google.com/patent/US11709462B2/en", "similarity": 0.4986}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20A%20Nonlinear%20Model%20Predictive%20Control%20for%20Automated%20Drifting%20with%20a%0A%20%20Standard%20Passenger%20Vehicle&body=Title%3A%20A%20Nonlinear%20Model%20Predictive%20Control%20for%20Automated%20Drifting%20with%20a%0A%20%20Standard%20Passenger%20Vehicle%0AAuthor%3A%20Stan%20Meijer%20and%20Alberto%20Bertipaglia%20and%20Barys%20Shyrokau%0AAbstract%3A%20%20%20This%20paper%20presents%20a%20novel%20approach%20to%20automated%20drifting%20with%20a%20standard%0Apassenger%20vehicle%2C%20which%20involves%20a%20Nonlinear%20Model%20Predictive%20Control%20to%0Astabilise%20and%20maintain%20the%20vehicle%20at%20high%20sideslip%20angle%20conditions.%20The%0Aproposed%20controller%20architecture%20is%20split%20into%20three%20components.%20The%20first%20part%0Aconsists%20of%20the%20offline%20computed%20equilibrium%20maps%2C%20which%20provide%20the%0Aequilibrium%20points%20for%20each%20vehicle%20state%20given%20the%20desired%20sideslip%20angle%20and%0Aradius%20of%20the%20path.%20The%20second%20is%20the%20predictive%20controller%20minimising%20the%0Aerrors%20between%20the%20equilibrium%20and%20actual%20vehicle%20states.%20The%20third%20is%20a%0Apath-following%20controller%2C%20which%20reduces%20the%20path%20error%2C%20altering%20the%0Aequilibrium%20curvature%20path.%20In%20a%20high-fidelity%20simulation%20environment%2C%20we%0Avalidate%20the%20controller%20architecture%20capacity%20to%20stabilise%20the%20vehicle%20in%0Aautomated%20drifting%20along%20a%20desired%20path%2C%20with%20a%20maximal%20lateral%20path%20deviation%0Aof%201%20m.%20In%20the%20experiments%20with%20a%20standard%20passenger%20vehicle%2C%20we%20demonstrate%0Athat%20the%20proposed%20approach%20is%20capable%20of%20bringing%20and%20maintaining%20the%20vehicle%0Aat%20the%20desired%2030%20deg%20sideslip%20angle%20in%20both%20high%20and%20low%20friction%20conditions.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2405.10859v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DA%2520Nonlinear%2520Model%2520Predictive%2520Control%2520for%2520Automated%2520Drifting%2520with%2520a%250A%2520%2520Standard%2520Passenger%2520Vehicle%26entry.906535625%3DStan%2520Meijer%2520and%2520Alberto%2520Bertipaglia%2520and%2520Barys%2520Shyrokau%26entry.1292438233%3D%2520%2520This%2520paper%2520presents%2520a%2520novel%2520approach%2520to%2520automated%2520drifting%2520with%2520a%2520standard%250Apassenger%2520vehicle%252C%2520which%2520involves%2520a%2520Nonlinear%2520Model%2520Predictive%2520Control%2520to%250Astabilise%2520and%2520maintain%2520the%2520vehicle%2520at%2520high%2520sideslip%2520angle%2520conditions.%2520The%250Aproposed%2520controller%2520architecture%2520is%2520split%2520into%2520three%2520components.%2520The%2520first%2520part%250Aconsists%2520of%2520the%2520offline%2520computed%2520equilibrium%2520maps%252C%2520which%2520provide%2520the%250Aequilibrium%2520points%2520for%2520each%2520vehicle%2520state%2520given%2520the%2520desired%2520sideslip%2520angle%2520and%250Aradius%2520of%2520the%2520path.%2520The%2520second%2520is%2520the%2520predictive%2520controller%2520minimising%2520the%250Aerrors%2520between%2520the%2520equilibrium%2520and%2520actual%2520vehicle%2520states.%2520The%2520third%2520is%2520a%250Apath-following%2520controller%252C%2520which%2520reduces%2520the%2520path%2520error%252C%2520altering%2520the%250Aequilibrium%2520curvature%2520path.%2520In%2520a%2520high-fidelity%2520simulation%2520environment%252C%2520we%250Avalidate%2520the%2520controller%2520architecture%2520capacity%2520to%2520stabilise%2520the%2520vehicle%2520in%250Aautomated%2520drifting%2520along%2520a%2520desired%2520path%252C%2520with%2520a%2520maximal%2520lateral%2520path%2520deviation%250Aof%25201%2520m.%2520In%2520the%2520experiments%2520with%2520a%2520standard%2520passenger%2520vehicle%252C%2520we%2520demonstrate%250Athat%2520the%2520proposed%2520approach%2520is%2520capable%2520of%2520bringing%2520and%2520maintaining%2520the%2520vehicle%250Aat%2520the%2520desired%252030%2520deg%2520sideslip%2520angle%2520in%2520both%2520high%2520and%2520low%2520friction%2520conditions.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2405.10859v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=A%20Nonlinear%20Model%20Predictive%20Control%20for%20Automated%20Drifting%20with%20a%0A%20%20Standard%20Passenger%20Vehicle&entry.906535625=Stan%20Meijer%20and%20Alberto%20Bertipaglia%20and%20Barys%20Shyrokau&entry.1292438233=%20%20This%20paper%20presents%20a%20novel%20approach%20to%20automated%20drifting%20with%20a%20standard%0Apassenger%20vehicle%2C%20which%20involves%20a%20Nonlinear%20Model%20Predictive%20Control%20to%0Astabilise%20and%20maintain%20the%20vehicle%20at%20high%20sideslip%20angle%20conditions.%20The%0Aproposed%20controller%20architecture%20is%20split%20into%20three%20components.%20The%20first%20part%0Aconsists%20of%20the%20offline%20computed%20equilibrium%20maps%2C%20which%20provide%20the%0Aequilibrium%20points%20for%20each%20vehicle%20state%20given%20the%20desired%20sideslip%20angle%20and%0Aradius%20of%20the%20path.%20The%20second%20is%20the%20predictive%20controller%20minimising%20the%0Aerrors%20between%20the%20equilibrium%20and%20actual%20vehicle%20states.%20The%20third%20is%20a%0Apath-following%20controller%2C%20which%20reduces%20the%20path%20error%2C%20altering%20the%0Aequilibrium%20curvature%20path.%20In%20a%20high-fidelity%20simulation%20environment%2C%20we%0Avalidate%20the%20controller%20architecture%20capacity%20to%20stabilise%20the%20vehicle%20in%0Aautomated%20drifting%20along%20a%20desired%20path%2C%20with%20a%20maximal%20lateral%20path%20deviation%0Aof%201%20m.%20In%20the%20experiments%20with%20a%20standard%20passenger%20vehicle%2C%20we%20demonstrate%0Athat%20the%20proposed%20approach%20is%20capable%20of%20bringing%20and%20maintaining%20the%20vehicle%0Aat%20the%20desired%2030%20deg%20sideslip%20angle%20in%20both%20high%20and%20low%20friction%20conditions.%0A&entry.1838667208=http%3A//arxiv.org/abs/2405.10859v1&entry.124074799=Read"},
{"title": "C(NN)FD -- a deep learning framework for turbomachinery CFD analysis", "author": "Giuseppe Bruni and Sepehr Maleki and Senthil K. Krishnababu", "abstract": "  Deep Learning methods have seen a wide range of successful applications\nacross different industries. Up until now, applications to physical simulations\nsuch as CFD (Computational Fluid Dynamics), have been limited to simple\ntest-cases of minor industrial relevance. This paper demonstrates the\ndevelopment of a novel deep learning framework for real-time predictions of the\nimpact of manufacturing and build variations on the overall performance of\naxial compressors in gas turbines, with a focus on tip clearance variations.\nThe associated scatter in efficiency can significantly increase the CO2\nemissions, thus being of great industrial and environmental relevance. The\nproposed C(NN)FD architecture achieves in real-time accuracy comparable to the\nCFD benchmark. Predicting the flow field and using it to calculate the\ncorresponding overall performance renders the methodology generalisable, while\nfiltering only relevant parts of the CFD solution makes the methodology\nscalable to industrial applications.\n", "link": "http://arxiv.org/abs/2306.05889v2", "date": "2024-05-17", "relevancy": 2.0257, "topK": [{"title": "Scaling Rectified Flow Transformers for High-Resolution Image Synthesis", "link": "http://arxiv.org/abs/2403.03206v1", "similarity": 0.5411}, {"title": "Boosted dynamic neural networks", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/26302", "similarity": 0.5104}, {"title": "Tuning-Free Noise Rectification for High Fidelity Image-to-Video\n  Generation", "link": "http://arxiv.org/abs/2403.02827v1", "similarity": 0.4886}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20C%28NN%29FD%20--%20a%20deep%20learning%20framework%20for%20turbomachinery%20CFD%20analysis&body=Title%3A%20C%28NN%29FD%20--%20a%20deep%20learning%20framework%20for%20turbomachinery%20CFD%20analysis%0AAuthor%3A%20Giuseppe%20Bruni%20and%20Sepehr%20Maleki%20and%20Senthil%20K.%20Krishnababu%0AAbstract%3A%20%20%20Deep%20Learning%20methods%20have%20seen%20a%20wide%20range%20of%20successful%20applications%0Aacross%20different%20industries.%20Up%20until%20now%2C%20applications%20to%20physical%20simulations%0Asuch%20as%20CFD%20%28Computational%20Fluid%20Dynamics%29%2C%20have%20been%20limited%20to%20simple%0Atest-cases%20of%20minor%20industrial%20relevance.%20This%20paper%20demonstrates%20the%0Adevelopment%20of%20a%20novel%20deep%20learning%20framework%20for%20real-time%20predictions%20of%20the%0Aimpact%20of%20manufacturing%20and%20build%20variations%20on%20the%20overall%20performance%20of%0Aaxial%20compressors%20in%20gas%20turbines%2C%20with%20a%20focus%20on%20tip%20clearance%20variations.%0AThe%20associated%20scatter%20in%20efficiency%20can%20significantly%20increase%20the%20CO2%0Aemissions%2C%20thus%20being%20of%20great%20industrial%20and%20environmental%20relevance.%20The%0Aproposed%20C%28NN%29FD%20architecture%20achieves%20in%20real-time%20accuracy%20comparable%20to%20the%0ACFD%20benchmark.%20Predicting%20the%20flow%20field%20and%20using%20it%20to%20calculate%20the%0Acorresponding%20overall%20performance%20renders%20the%20methodology%20generalisable%2C%20while%0Afiltering%20only%20relevant%20parts%20of%20the%20CFD%20solution%20makes%20the%20methodology%0Ascalable%20to%20industrial%20applications.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2306.05889v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DC%2528NN%2529FD%2520--%2520a%2520deep%2520learning%2520framework%2520for%2520turbomachinery%2520CFD%2520analysis%26entry.906535625%3DGiuseppe%2520Bruni%2520and%2520Sepehr%2520Maleki%2520and%2520Senthil%2520K.%2520Krishnababu%26entry.1292438233%3D%2520%2520Deep%2520Learning%2520methods%2520have%2520seen%2520a%2520wide%2520range%2520of%2520successful%2520applications%250Aacross%2520different%2520industries.%2520Up%2520until%2520now%252C%2520applications%2520to%2520physical%2520simulations%250Asuch%2520as%2520CFD%2520%2528Computational%2520Fluid%2520Dynamics%2529%252C%2520have%2520been%2520limited%2520to%2520simple%250Atest-cases%2520of%2520minor%2520industrial%2520relevance.%2520This%2520paper%2520demonstrates%2520the%250Adevelopment%2520of%2520a%2520novel%2520deep%2520learning%2520framework%2520for%2520real-time%2520predictions%2520of%2520the%250Aimpact%2520of%2520manufacturing%2520and%2520build%2520variations%2520on%2520the%2520overall%2520performance%2520of%250Aaxial%2520compressors%2520in%2520gas%2520turbines%252C%2520with%2520a%2520focus%2520on%2520tip%2520clearance%2520variations.%250AThe%2520associated%2520scatter%2520in%2520efficiency%2520can%2520significantly%2520increase%2520the%2520CO2%250Aemissions%252C%2520thus%2520being%2520of%2520great%2520industrial%2520and%2520environmental%2520relevance.%2520The%250Aproposed%2520C%2528NN%2529FD%2520architecture%2520achieves%2520in%2520real-time%2520accuracy%2520comparable%2520to%2520the%250ACFD%2520benchmark.%2520Predicting%2520the%2520flow%2520field%2520and%2520using%2520it%2520to%2520calculate%2520the%250Acorresponding%2520overall%2520performance%2520renders%2520the%2520methodology%2520generalisable%252C%2520while%250Afiltering%2520only%2520relevant%2520parts%2520of%2520the%2520CFD%2520solution%2520makes%2520the%2520methodology%250Ascalable%2520to%2520industrial%2520applications.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2306.05889v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=C%28NN%29FD%20--%20a%20deep%20learning%20framework%20for%20turbomachinery%20CFD%20analysis&entry.906535625=Giuseppe%20Bruni%20and%20Sepehr%20Maleki%20and%20Senthil%20K.%20Krishnababu&entry.1292438233=%20%20Deep%20Learning%20methods%20have%20seen%20a%20wide%20range%20of%20successful%20applications%0Aacross%20different%20industries.%20Up%20until%20now%2C%20applications%20to%20physical%20simulations%0Asuch%20as%20CFD%20%28Computational%20Fluid%20Dynamics%29%2C%20have%20been%20limited%20to%20simple%0Atest-cases%20of%20minor%20industrial%20relevance.%20This%20paper%20demonstrates%20the%0Adevelopment%20of%20a%20novel%20deep%20learning%20framework%20for%20real-time%20predictions%20of%20the%0Aimpact%20of%20manufacturing%20and%20build%20variations%20on%20the%20overall%20performance%20of%0Aaxial%20compressors%20in%20gas%20turbines%2C%20with%20a%20focus%20on%20tip%20clearance%20variations.%0AThe%20associated%20scatter%20in%20efficiency%20can%20significantly%20increase%20the%20CO2%0Aemissions%2C%20thus%20being%20of%20great%20industrial%20and%20environmental%20relevance.%20The%0Aproposed%20C%28NN%29FD%20architecture%20achieves%20in%20real-time%20accuracy%20comparable%20to%20the%0ACFD%20benchmark.%20Predicting%20the%20flow%20field%20and%20using%20it%20to%20calculate%20the%0Acorresponding%20overall%20performance%20renders%20the%20methodology%20generalisable%2C%20while%0Afiltering%20only%20relevant%20parts%20of%20the%20CFD%20solution%20makes%20the%20methodology%0Ascalable%20to%20industrial%20applications.%0A&entry.1838667208=http%3A//arxiv.org/abs/2306.05889v2&entry.124074799=Read"},
{"title": "GeoEval: Benchmark for Evaluating LLMs and Multi-Modal Models on\n  Geometry Problem-Solving", "author": "Jiaxin Zhang and Zhongzhi Li and Mingliang Zhang and Fei Yin and Chenglin Liu and Yashar Moshfeghi", "abstract": "  Recent advancements in large language models (LLMs) and multi-modal models\n(MMs) have demonstrated their remarkable capabilities in problem-solving. Yet,\ntheir proficiency in tackling geometry math problems, which necessitates an\nintegrated understanding of both textual and visual information, has not been\nthoroughly evaluated. To address this gap, we introduce the GeoEval benchmark,\na comprehensive collection that includes a main subset of 2,000 problems, a 750\nproblems subset focusing on backward reasoning, an augmented subset of 2,000\nproblems, and a hard subset of 300 problems. This benchmark facilitates a\ndeeper investigation into the performance of LLMs and MMs in solving geometry\nmath problems. Our evaluation of ten LLMs and MMs across these varied subsets\nreveals that the WizardMath model excels, achieving a 55.67\\% accuracy rate on\nthe main subset but only a 6.00\\% accuracy on the hard subset. This highlights\nthe critical need for testing models against datasets on which they have not\nbeen pre-trained. Additionally, our findings indicate that GPT-series models\nperform more effectively on problems they have rephrased, suggesting a\npromising method for enhancing model capabilities.\n", "link": "http://arxiv.org/abs/2402.10104v2", "date": "2024-05-17", "relevancy": 2.0256, "topK": [{"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.518}, {"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.5035}, {"title": "Dynamic Gaussians Mesh: Consistent Mesh Reconstruction from Monocular\n  Videos", "link": "http://arxiv.org/abs/2404.12379v2", "similarity": 0.496}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20GeoEval%3A%20Benchmark%20for%20Evaluating%20LLMs%20and%20Multi-Modal%20Models%20on%0A%20%20Geometry%20Problem-Solving&body=Title%3A%20GeoEval%3A%20Benchmark%20for%20Evaluating%20LLMs%20and%20Multi-Modal%20Models%20on%0A%20%20Geometry%20Problem-Solving%0AAuthor%3A%20Jiaxin%20Zhang%20and%20Zhongzhi%20Li%20and%20Mingliang%20Zhang%20and%20Fei%20Yin%20and%20Chenglin%20Liu%20and%20Yashar%20Moshfeghi%0AAbstract%3A%20%20%20Recent%20advancements%20in%20large%20language%20models%20%28LLMs%29%20and%20multi-modal%20models%0A%28MMs%29%20have%20demonstrated%20their%20remarkable%20capabilities%20in%20problem-solving.%20Yet%2C%0Atheir%20proficiency%20in%20tackling%20geometry%20math%20problems%2C%20which%20necessitates%20an%0Aintegrated%20understanding%20of%20both%20textual%20and%20visual%20information%2C%20has%20not%20been%0Athoroughly%20evaluated.%20To%20address%20this%20gap%2C%20we%20introduce%20the%20GeoEval%20benchmark%2C%0Aa%20comprehensive%20collection%20that%20includes%20a%20main%20subset%20of%202%2C000%20problems%2C%20a%20750%0Aproblems%20subset%20focusing%20on%20backward%20reasoning%2C%20an%20augmented%20subset%20of%202%2C000%0Aproblems%2C%20and%20a%20hard%20subset%20of%20300%20problems.%20This%20benchmark%20facilitates%20a%0Adeeper%20investigation%20into%20the%20performance%20of%20LLMs%20and%20MMs%20in%20solving%20geometry%0Amath%20problems.%20Our%20evaluation%20of%20ten%20LLMs%20and%20MMs%20across%20these%20varied%20subsets%0Areveals%20that%20the%20WizardMath%20model%20excels%2C%20achieving%20a%2055.67%5C%25%20accuracy%20rate%20on%0Athe%20main%20subset%20but%20only%20a%206.00%5C%25%20accuracy%20on%20the%20hard%20subset.%20This%20highlights%0Athe%20critical%20need%20for%20testing%20models%20against%20datasets%20on%20which%20they%20have%20not%0Abeen%20pre-trained.%20Additionally%2C%20our%20findings%20indicate%20that%20GPT-series%20models%0Aperform%20more%20effectively%20on%20problems%20they%20have%20rephrased%2C%20suggesting%20a%0Apromising%20method%20for%20enhancing%20model%20capabilities.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2402.10104v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DGeoEval%253A%2520Benchmark%2520for%2520Evaluating%2520LLMs%2520and%2520Multi-Modal%2520Models%2520on%250A%2520%2520Geometry%2520Problem-Solving%26entry.906535625%3DJiaxin%2520Zhang%2520and%2520Zhongzhi%2520Li%2520and%2520Mingliang%2520Zhang%2520and%2520Fei%2520Yin%2520and%2520Chenglin%2520Liu%2520and%2520Yashar%2520Moshfeghi%26entry.1292438233%3D%2520%2520Recent%2520advancements%2520in%2520large%2520language%2520models%2520%2528LLMs%2529%2520and%2520multi-modal%2520models%250A%2528MMs%2529%2520have%2520demonstrated%2520their%2520remarkable%2520capabilities%2520in%2520problem-solving.%2520Yet%252C%250Atheir%2520proficiency%2520in%2520tackling%2520geometry%2520math%2520problems%252C%2520which%2520necessitates%2520an%250Aintegrated%2520understanding%2520of%2520both%2520textual%2520and%2520visual%2520information%252C%2520has%2520not%2520been%250Athoroughly%2520evaluated.%2520To%2520address%2520this%2520gap%252C%2520we%2520introduce%2520the%2520GeoEval%2520benchmark%252C%250Aa%2520comprehensive%2520collection%2520that%2520includes%2520a%2520main%2520subset%2520of%25202%252C000%2520problems%252C%2520a%2520750%250Aproblems%2520subset%2520focusing%2520on%2520backward%2520reasoning%252C%2520an%2520augmented%2520subset%2520of%25202%252C000%250Aproblems%252C%2520and%2520a%2520hard%2520subset%2520of%2520300%2520problems.%2520This%2520benchmark%2520facilitates%2520a%250Adeeper%2520investigation%2520into%2520the%2520performance%2520of%2520LLMs%2520and%2520MMs%2520in%2520solving%2520geometry%250Amath%2520problems.%2520Our%2520evaluation%2520of%2520ten%2520LLMs%2520and%2520MMs%2520across%2520these%2520varied%2520subsets%250Areveals%2520that%2520the%2520WizardMath%2520model%2520excels%252C%2520achieving%2520a%252055.67%255C%2525%2520accuracy%2520rate%2520on%250Athe%2520main%2520subset%2520but%2520only%2520a%25206.00%255C%2525%2520accuracy%2520on%2520the%2520hard%2520subset.%2520This%2520highlights%250Athe%2520critical%2520need%2520for%2520testing%2520models%2520against%2520datasets%2520on%2520which%2520they%2520have%2520not%250Abeen%2520pre-trained.%2520Additionally%252C%2520our%2520findings%2520indicate%2520that%2520GPT-series%2520models%250Aperform%2520more%2520effectively%2520on%2520problems%2520they%2520have%2520rephrased%252C%2520suggesting%2520a%250Apromising%2520method%2520for%2520enhancing%2520model%2520capabilities.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2402.10104v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=GeoEval%3A%20Benchmark%20for%20Evaluating%20LLMs%20and%20Multi-Modal%20Models%20on%0A%20%20Geometry%20Problem-Solving&entry.906535625=Jiaxin%20Zhang%20and%20Zhongzhi%20Li%20and%20Mingliang%20Zhang%20and%20Fei%20Yin%20and%20Chenglin%20Liu%20and%20Yashar%20Moshfeghi&entry.1292438233=%20%20Recent%20advancements%20in%20large%20language%20models%20%28LLMs%29%20and%20multi-modal%20models%0A%28MMs%29%20have%20demonstrated%20their%20remarkable%20capabilities%20in%20problem-solving.%20Yet%2C%0Atheir%20proficiency%20in%20tackling%20geometry%20math%20problems%2C%20which%20necessitates%20an%0Aintegrated%20understanding%20of%20both%20textual%20and%20visual%20information%2C%20has%20not%20been%0Athoroughly%20evaluated.%20To%20address%20this%20gap%2C%20we%20introduce%20the%20GeoEval%20benchmark%2C%0Aa%20comprehensive%20collection%20that%20includes%20a%20main%20subset%20of%202%2C000%20problems%2C%20a%20750%0Aproblems%20subset%20focusing%20on%20backward%20reasoning%2C%20an%20augmented%20subset%20of%202%2C000%0Aproblems%2C%20and%20a%20hard%20subset%20of%20300%20problems.%20This%20benchmark%20facilitates%20a%0Adeeper%20investigation%20into%20the%20performance%20of%20LLMs%20and%20MMs%20in%20solving%20geometry%0Amath%20problems.%20Our%20evaluation%20of%20ten%20LLMs%20and%20MMs%20across%20these%20varied%20subsets%0Areveals%20that%20the%20WizardMath%20model%20excels%2C%20achieving%20a%2055.67%5C%25%20accuracy%20rate%20on%0Athe%20main%20subset%20but%20only%20a%206.00%5C%25%20accuracy%20on%20the%20hard%20subset.%20This%20highlights%0Athe%20critical%20need%20for%20testing%20models%20against%20datasets%20on%20which%20they%20have%20not%0Abeen%20pre-trained.%20Additionally%2C%20our%20findings%20indicate%20that%20GPT-series%20models%0Aperform%20more%20effectively%20on%20problems%20they%20have%20rephrased%2C%20suggesting%20a%0Apromising%20method%20for%20enhancing%20model%20capabilities.%0A&entry.1838667208=http%3A//arxiv.org/abs/2402.10104v2&entry.124074799=Read"},
{"title": "Automatic segmentation of Organs at Risk in Head and Neck cancer\n  patients from CT and MRI scans", "author": "S\u00e9bastien Quetin and Andrew Heschl and Mauricio Murillo and Murali Rohit and Shirin A. Enger and Farhad Maleki", "abstract": "  Background and purpose: Deep Learning (DL) has been widely explored for\nOrgans at Risk (OARs) segmentation; however, most studies have focused on a\nsingle modality, either CT or MRI, not both simultaneously. This study presents\na high-performing DL pipeline for segmentation of 30 OARs from MRI and CT scans\nof Head and Neck (H&N) cancer patients.\n  Materials and methods: Paired CT and MRI-T1 images from 42 H&N cancer\npatients alongside annotation for 30 OARs from the H&N OAR CT & MR segmentation\nchallenge dataset were used to develop a segmentation pipeline. After cropping\nirrelevant regions, rigid followed by non-rigid registration of CT and MRI\nvolumes was performed. Two versions of the CT volume, representing soft tissues\nand bone anatomy, were stacked with the MRI volume and used as input to an\nnnU-Net pipeline. Modality Dropout was used during the training to force the\nmodel to learn from the different modalities. Segmentation masks were predicted\nwith the trained model for an independent set of 14 new patients. The mean Dice\nScore (DS) and Hausdorff Distance (HD) were calculated for each OAR across\nthese patients to evaluate the pipeline.\n  Results: This resulted in an overall mean DS and HD of 0.777 +- 0.118 and\n3.455 +- 1.679, respectively, establishing the state-of-the-art (SOTA) for this\nchallenge at the time of submission.\n  Conclusion: The proposed pipeline achieved the best DS and HD among all\nparticipants of the H&N OAR CT and MR segmentation challenge and sets a new\nSOTA for automated segmentation of H&N OARs.\n", "link": "http://arxiv.org/abs/2405.10833v1", "date": "2024-05-17", "relevancy": 2.0199, "topK": [{"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.5249}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5177}, {"title": "DDM-NET: End-to-end learning of keypoint feature Detection, Description and Matching for 3D localization", "link": "https://arxiv.org/abs/2212.04575", "similarity": 0.4843}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Automatic%20segmentation%20of%20Organs%20at%20Risk%20in%20Head%20and%20Neck%20cancer%0A%20%20patients%20from%20CT%20and%20MRI%20scans&body=Title%3A%20Automatic%20segmentation%20of%20Organs%20at%20Risk%20in%20Head%20and%20Neck%20cancer%0A%20%20patients%20from%20CT%20and%20MRI%20scans%0AAuthor%3A%20S%C3%A9bastien%20Quetin%20and%20Andrew%20Heschl%20and%20Mauricio%20Murillo%20and%20Murali%20Rohit%20and%20Shirin%20A.%20Enger%20and%20Farhad%20Maleki%0AAbstract%3A%20%20%20Background%20and%20purpose%3A%20Deep%20Learning%20%28DL%29%20has%20been%20widely%20explored%20for%0AOrgans%20at%20Risk%20%28OARs%29%20segmentation%3B%20however%2C%20most%20studies%20have%20focused%20on%20a%0Asingle%20modality%2C%20either%20CT%20or%20MRI%2C%20not%20both%20simultaneously.%20This%20study%20presents%0Aa%20high-performing%20DL%20pipeline%20for%20segmentation%20of%2030%20OARs%20from%20MRI%20and%20CT%20scans%0Aof%20Head%20and%20Neck%20%28H%26N%29%20cancer%20patients.%0A%20%20Materials%20and%20methods%3A%20Paired%20CT%20and%20MRI-T1%20images%20from%2042%20H%26N%20cancer%0Apatients%20alongside%20annotation%20for%2030%20OARs%20from%20the%20H%26N%20OAR%20CT%20%26%20MR%20segmentation%0Achallenge%20dataset%20were%20used%20to%20develop%20a%20segmentation%20pipeline.%20After%20cropping%0Airrelevant%20regions%2C%20rigid%20followed%20by%20non-rigid%20registration%20of%20CT%20and%20MRI%0Avolumes%20was%20performed.%20Two%20versions%20of%20the%20CT%20volume%2C%20representing%20soft%20tissues%0Aand%20bone%20anatomy%2C%20were%20stacked%20with%20the%20MRI%20volume%20and%20used%20as%20input%20to%20an%0AnnU-Net%20pipeline.%20Modality%20Dropout%20was%20used%20during%20the%20training%20to%20force%20the%0Amodel%20to%20learn%20from%20the%20different%20modalities.%20Segmentation%20masks%20were%20predicted%0Awith%20the%20trained%20model%20for%20an%20independent%20set%20of%2014%20new%20patients.%20The%20mean%20Dice%0AScore%20%28DS%29%20and%20Hausdorff%20Distance%20%28HD%29%20were%20calculated%20for%20each%20OAR%20across%0Athese%20patients%20to%20evaluate%20the%20pipeline.%0A%20%20Results%3A%20This%20resulted%20in%20an%20overall%20mean%20DS%20and%20HD%20of%200.777%20%2B-%200.118%20and%0A3.455%20%2B-%201.679%2C%20respectively%2C%20establishing%20the%20state-of-the-art%20%28SOTA%29%20for%20this%0Achallenge%20at%20the%20time%20of%20submission.%0A%20%20Conclusion%3A%20The%20proposed%20pipeline%20achieved%20the%20best%20DS%20and%20HD%20among%20all%0Aparticipants%20of%20the%20H%26N%20OAR%20CT%20and%20MR%20segmentation%20challenge%20and%20sets%20a%20new%0ASOTA%20for%20automated%20segmentation%20of%20H%26N%20OARs.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2405.10833v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DAutomatic%2520segmentation%2520of%2520Organs%2520at%2520Risk%2520in%2520Head%2520and%2520Neck%2520cancer%250A%2520%2520patients%2520from%2520CT%2520and%2520MRI%2520scans%26entry.906535625%3DS%25C3%25A9bastien%2520Quetin%2520and%2520Andrew%2520Heschl%2520and%2520Mauricio%2520Murillo%2520and%2520Murali%2520Rohit%2520and%2520Shirin%2520A.%2520Enger%2520and%2520Farhad%2520Maleki%26entry.1292438233%3D%2520%2520Background%2520and%2520purpose%253A%2520Deep%2520Learning%2520%2528DL%2529%2520has%2520been%2520widely%2520explored%2520for%250AOrgans%2520at%2520Risk%2520%2528OARs%2529%2520segmentation%253B%2520however%252C%2520most%2520studies%2520have%2520focused%2520on%2520a%250Asingle%2520modality%252C%2520either%2520CT%2520or%2520MRI%252C%2520not%2520both%2520simultaneously.%2520This%2520study%2520presents%250Aa%2520high-performing%2520DL%2520pipeline%2520for%2520segmentation%2520of%252030%2520OARs%2520from%2520MRI%2520and%2520CT%2520scans%250Aof%2520Head%2520and%2520Neck%2520%2528H%2526N%2529%2520cancer%2520patients.%250A%2520%2520Materials%2520and%2520methods%253A%2520Paired%2520CT%2520and%2520MRI-T1%2520images%2520from%252042%2520H%2526N%2520cancer%250Apatients%2520alongside%2520annotation%2520for%252030%2520OARs%2520from%2520the%2520H%2526N%2520OAR%2520CT%2520%2526%2520MR%2520segmentation%250Achallenge%2520dataset%2520were%2520used%2520to%2520develop%2520a%2520segmentation%2520pipeline.%2520After%2520cropping%250Airrelevant%2520regions%252C%2520rigid%2520followed%2520by%2520non-rigid%2520registration%2520of%2520CT%2520and%2520MRI%250Avolumes%2520was%2520performed.%2520Two%2520versions%2520of%2520the%2520CT%2520volume%252C%2520representing%2520soft%2520tissues%250Aand%2520bone%2520anatomy%252C%2520were%2520stacked%2520with%2520the%2520MRI%2520volume%2520and%2520used%2520as%2520input%2520to%2520an%250AnnU-Net%2520pipeline.%2520Modality%2520Dropout%2520was%2520used%2520during%2520the%2520training%2520to%2520force%2520the%250Amodel%2520to%2520learn%2520from%2520the%2520different%2520modalities.%2520Segmentation%2520masks%2520were%2520predicted%250Awith%2520the%2520trained%2520model%2520for%2520an%2520independent%2520set%2520of%252014%2520new%2520patients.%2520The%2520mean%2520Dice%250AScore%2520%2528DS%2529%2520and%2520Hausdorff%2520Distance%2520%2528HD%2529%2520were%2520calculated%2520for%2520each%2520OAR%2520across%250Athese%2520patients%2520to%2520evaluate%2520the%2520pipeline.%250A%2520%2520Results%253A%2520This%2520resulted%2520in%2520an%2520overall%2520mean%2520DS%2520and%2520HD%2520of%25200.777%2520%252B-%25200.118%2520and%250A3.455%2520%252B-%25201.679%252C%2520respectively%252C%2520establishing%2520the%2520state-of-the-art%2520%2528SOTA%2529%2520for%2520this%250Achallenge%2520at%2520the%2520time%2520of%2520submission.%250A%2520%2520Conclusion%253A%2520The%2520proposed%2520pipeline%2520achieved%2520the%2520best%2520DS%2520and%2520HD%2520among%2520all%250Aparticipants%2520of%2520the%2520H%2526N%2520OAR%2520CT%2520and%2520MR%2520segmentation%2520challenge%2520and%2520sets%2520a%2520new%250ASOTA%2520for%2520automated%2520segmentation%2520of%2520H%2526N%2520OARs.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2405.10833v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Automatic%20segmentation%20of%20Organs%20at%20Risk%20in%20Head%20and%20Neck%20cancer%0A%20%20patients%20from%20CT%20and%20MRI%20scans&entry.906535625=S%C3%A9bastien%20Quetin%20and%20Andrew%20Heschl%20and%20Mauricio%20Murillo%20and%20Murali%20Rohit%20and%20Shirin%20A.%20Enger%20and%20Farhad%20Maleki&entry.1292438233=%20%20Background%20and%20purpose%3A%20Deep%20Learning%20%28DL%29%20has%20been%20widely%20explored%20for%0AOrgans%20at%20Risk%20%28OARs%29%20segmentation%3B%20however%2C%20most%20studies%20have%20focused%20on%20a%0Asingle%20modality%2C%20either%20CT%20or%20MRI%2C%20not%20both%20simultaneously.%20This%20study%20presents%0Aa%20high-performing%20DL%20pipeline%20for%20segmentation%20of%2030%20OARs%20from%20MRI%20and%20CT%20scans%0Aof%20Head%20and%20Neck%20%28H%26N%29%20cancer%20patients.%0A%20%20Materials%20and%20methods%3A%20Paired%20CT%20and%20MRI-T1%20images%20from%2042%20H%26N%20cancer%0Apatients%20alongside%20annotation%20for%2030%20OARs%20from%20the%20H%26N%20OAR%20CT%20%26%20MR%20segmentation%0Achallenge%20dataset%20were%20used%20to%20develop%20a%20segmentation%20pipeline.%20After%20cropping%0Airrelevant%20regions%2C%20rigid%20followed%20by%20non-rigid%20registration%20of%20CT%20and%20MRI%0Avolumes%20was%20performed.%20Two%20versions%20of%20the%20CT%20volume%2C%20representing%20soft%20tissues%0Aand%20bone%20anatomy%2C%20were%20stacked%20with%20the%20MRI%20volume%20and%20used%20as%20input%20to%20an%0AnnU-Net%20pipeline.%20Modality%20Dropout%20was%20used%20during%20the%20training%20to%20force%20the%0Amodel%20to%20learn%20from%20the%20different%20modalities.%20Segmentation%20masks%20were%20predicted%0Awith%20the%20trained%20model%20for%20an%20independent%20set%20of%2014%20new%20patients.%20The%20mean%20Dice%0AScore%20%28DS%29%20and%20Hausdorff%20Distance%20%28HD%29%20were%20calculated%20for%20each%20OAR%20across%0Athese%20patients%20to%20evaluate%20the%20pipeline.%0A%20%20Results%3A%20This%20resulted%20in%20an%20overall%20mean%20DS%20and%20HD%20of%200.777%20%2B-%200.118%20and%0A3.455%20%2B-%201.679%2C%20respectively%2C%20establishing%20the%20state-of-the-art%20%28SOTA%29%20for%20this%0Achallenge%20at%20the%20time%20of%20submission.%0A%20%20Conclusion%3A%20The%20proposed%20pipeline%20achieved%20the%20best%20DS%20and%20HD%20among%20all%0Aparticipants%20of%20the%20H%26N%20OAR%20CT%20and%20MR%20segmentation%20challenge%20and%20sets%20a%20new%0ASOTA%20for%20automated%20segmentation%20of%20H%26N%20OARs.%0A&entry.1838667208=http%3A//arxiv.org/abs/2405.10833v1&entry.124074799=Read"},
{"title": "CoLeaF: A Contrastive-Collaborative Learning Framework for Weakly\n  Supervised Audio-Visual Video Parsing", "author": "Faegheh Sardari and Armin Mustafa and Philip J. B. Jackson and Adrian Hilton", "abstract": "  Weakly supervised audio-visual video parsing (AVVP) methods aim to detect\naudible-only, visible-only, and audible-visible events using only video-level\nlabels. Existing approaches tackle this by leveraging unimodal and cross-modal\ncontexts. However, we argue that while cross-modal learning is beneficial for\ndetecting audible-visible events, in the weakly supervised scenario, it\nnegatively impacts unaligned audible or visible events by introducing\nirrelevant modality information. In this paper, we propose CoLeaF, a novel\nlearning framework that optimizes the integration of cross-modal context in the\nembedding space such that the network explicitly learns to combine cross-modal\ninformation for audible-visible events while filtering them out for unaligned\nevents. Additionally, as videos often involve complex class relationships,\nmodelling them improves performance. However, this introduces extra\ncomputational costs into the network. Our framework is designed to leverage\ncross-class relationships during training without incurring additional\ncomputations at inference. Furthermore, we propose new metrics to better\nevaluate a method's capabilities in performing AVVP. Our extensive experiments\ndemonstrate that CoLeaF significantly improves the state-of-the-art results by\nan average of 1.9% and 2.4% F-score on the LLP and UnAV-100 datasets,\nrespectively.\n", "link": "http://arxiv.org/abs/2405.10690v1", "date": "2024-05-17", "relevancy": 2.0153, "topK": [{"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5207}, {"title": "Self-supervised Photographic Image Layout Representation Learning", "link": "http://arxiv.org/abs/2403.03740v1", "similarity": 0.4934}, {"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.4879}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20CoLeaF%3A%20A%20Contrastive-Collaborative%20Learning%20Framework%20for%20Weakly%0A%20%20Supervised%20Audio-Visual%20Video%20Parsing&body=Title%3A%20CoLeaF%3A%20A%20Contrastive-Collaborative%20Learning%20Framework%20for%20Weakly%0A%20%20Supervised%20Audio-Visual%20Video%20Parsing%0AAuthor%3A%20Faegheh%20Sardari%20and%20Armin%20Mustafa%20and%20Philip%20J.%20B.%20Jackson%20and%20Adrian%20Hilton%0AAbstract%3A%20%20%20Weakly%20supervised%20audio-visual%20video%20parsing%20%28AVVP%29%20methods%20aim%20to%20detect%0Aaudible-only%2C%20visible-only%2C%20and%20audible-visible%20events%20using%20only%20video-level%0Alabels.%20Existing%20approaches%20tackle%20this%20by%20leveraging%20unimodal%20and%20cross-modal%0Acontexts.%20However%2C%20we%20argue%20that%20while%20cross-modal%20learning%20is%20beneficial%20for%0Adetecting%20audible-visible%20events%2C%20in%20the%20weakly%20supervised%20scenario%2C%20it%0Anegatively%20impacts%20unaligned%20audible%20or%20visible%20events%20by%20introducing%0Airrelevant%20modality%20information.%20In%20this%20paper%2C%20we%20propose%20CoLeaF%2C%20a%20novel%0Alearning%20framework%20that%20optimizes%20the%20integration%20of%20cross-modal%20context%20in%20the%0Aembedding%20space%20such%20that%20the%20network%20explicitly%20learns%20to%20combine%20cross-modal%0Ainformation%20for%20audible-visible%20events%20while%20filtering%20them%20out%20for%20unaligned%0Aevents.%20Additionally%2C%20as%20videos%20often%20involve%20complex%20class%20relationships%2C%0Amodelling%20them%20improves%20performance.%20However%2C%20this%20introduces%20extra%0Acomputational%20costs%20into%20the%20network.%20Our%20framework%20is%20designed%20to%20leverage%0Across-class%20relationships%20during%20training%20without%20incurring%20additional%0Acomputations%20at%20inference.%20Furthermore%2C%20we%20propose%20new%20metrics%20to%20better%0Aevaluate%20a%20method%27s%20capabilities%20in%20performing%20AVVP.%20Our%20extensive%20experiments%0Ademonstrate%20that%20CoLeaF%20significantly%20improves%20the%20state-of-the-art%20results%20by%0Aan%20average%20of%201.9%25%20and%202.4%25%20F-score%20on%20the%20LLP%20and%20UnAV-100%20datasets%2C%0Arespectively.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2405.10690v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DCoLeaF%253A%2520A%2520Contrastive-Collaborative%2520Learning%2520Framework%2520for%2520Weakly%250A%2520%2520Supervised%2520Audio-Visual%2520Video%2520Parsing%26entry.906535625%3DFaegheh%2520Sardari%2520and%2520Armin%2520Mustafa%2520and%2520Philip%2520J.%2520B.%2520Jackson%2520and%2520Adrian%2520Hilton%26entry.1292438233%3D%2520%2520Weakly%2520supervised%2520audio-visual%2520video%2520parsing%2520%2528AVVP%2529%2520methods%2520aim%2520to%2520detect%250Aaudible-only%252C%2520visible-only%252C%2520and%2520audible-visible%2520events%2520using%2520only%2520video-level%250Alabels.%2520Existing%2520approaches%2520tackle%2520this%2520by%2520leveraging%2520unimodal%2520and%2520cross-modal%250Acontexts.%2520However%252C%2520we%2520argue%2520that%2520while%2520cross-modal%2520learning%2520is%2520beneficial%2520for%250Adetecting%2520audible-visible%2520events%252C%2520in%2520the%2520weakly%2520supervised%2520scenario%252C%2520it%250Anegatively%2520impacts%2520unaligned%2520audible%2520or%2520visible%2520events%2520by%2520introducing%250Airrelevant%2520modality%2520information.%2520In%2520this%2520paper%252C%2520we%2520propose%2520CoLeaF%252C%2520a%2520novel%250Alearning%2520framework%2520that%2520optimizes%2520the%2520integration%2520of%2520cross-modal%2520context%2520in%2520the%250Aembedding%2520space%2520such%2520that%2520the%2520network%2520explicitly%2520learns%2520to%2520combine%2520cross-modal%250Ainformation%2520for%2520audible-visible%2520events%2520while%2520filtering%2520them%2520out%2520for%2520unaligned%250Aevents.%2520Additionally%252C%2520as%2520videos%2520often%2520involve%2520complex%2520class%2520relationships%252C%250Amodelling%2520them%2520improves%2520performance.%2520However%252C%2520this%2520introduces%2520extra%250Acomputational%2520costs%2520into%2520the%2520network.%2520Our%2520framework%2520is%2520designed%2520to%2520leverage%250Across-class%2520relationships%2520during%2520training%2520without%2520incurring%2520additional%250Acomputations%2520at%2520inference.%2520Furthermore%252C%2520we%2520propose%2520new%2520metrics%2520to%2520better%250Aevaluate%2520a%2520method%2527s%2520capabilities%2520in%2520performing%2520AVVP.%2520Our%2520extensive%2520experiments%250Ademonstrate%2520that%2520CoLeaF%2520significantly%2520improves%2520the%2520state-of-the-art%2520results%2520by%250Aan%2520average%2520of%25201.9%2525%2520and%25202.4%2525%2520F-score%2520on%2520the%2520LLP%2520and%2520UnAV-100%2520datasets%252C%250Arespectively.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2405.10690v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=CoLeaF%3A%20A%20Contrastive-Collaborative%20Learning%20Framework%20for%20Weakly%0A%20%20Supervised%20Audio-Visual%20Video%20Parsing&entry.906535625=Faegheh%20Sardari%20and%20Armin%20Mustafa%20and%20Philip%20J.%20B.%20Jackson%20and%20Adrian%20Hilton&entry.1292438233=%20%20Weakly%20supervised%20audio-visual%20video%20parsing%20%28AVVP%29%20methods%20aim%20to%20detect%0Aaudible-only%2C%20visible-only%2C%20and%20audible-visible%20events%20using%20only%20video-level%0Alabels.%20Existing%20approaches%20tackle%20this%20by%20leveraging%20unimodal%20and%20cross-modal%0Acontexts.%20However%2C%20we%20argue%20that%20while%20cross-modal%20learning%20is%20beneficial%20for%0Adetecting%20audible-visible%20events%2C%20in%20the%20weakly%20supervised%20scenario%2C%20it%0Anegatively%20impacts%20unaligned%20audible%20or%20visible%20events%20by%20introducing%0Airrelevant%20modality%20information.%20In%20this%20paper%2C%20we%20propose%20CoLeaF%2C%20a%20novel%0Alearning%20framework%20that%20optimizes%20the%20integration%20of%20cross-modal%20context%20in%20the%0Aembedding%20space%20such%20that%20the%20network%20explicitly%20learns%20to%20combine%20cross-modal%0Ainformation%20for%20audible-visible%20events%20while%20filtering%20them%20out%20for%20unaligned%0Aevents.%20Additionally%2C%20as%20videos%20often%20involve%20complex%20class%20relationships%2C%0Amodelling%20them%20improves%20performance.%20However%2C%20this%20introduces%20extra%0Acomputational%20costs%20into%20the%20network.%20Our%20framework%20is%20designed%20to%20leverage%0Across-class%20relationships%20during%20training%20without%20incurring%20additional%0Acomputations%20at%20inference.%20Furthermore%2C%20we%20propose%20new%20metrics%20to%20better%0Aevaluate%20a%20method%27s%20capabilities%20in%20performing%20AVVP.%20Our%20extensive%20experiments%0Ademonstrate%20that%20CoLeaF%20significantly%20improves%20the%20state-of-the-art%20results%20by%0Aan%20average%20of%201.9%25%20and%202.4%25%20F-score%20on%20the%20LLP%20and%20UnAV-100%20datasets%2C%0Arespectively.%0A&entry.1838667208=http%3A//arxiv.org/abs/2405.10690v1&entry.124074799=Read"},
{"title": "Model Predictive Contouring Control for Vehicle Obstacle Avoidance at\n  the Limit of Handling Using Torque Vectoring", "author": "Alberto Bertipaglia and Davide Tavernini and Umberto Montanaro and Mohsen Alirezaei and Riender Happee and Aldo Sorniotti and Barys Shyrokau", "abstract": "  This paper presents an original approach to vehicle obstacle avoidance. It\ninvolves the development of a nonlinear Model Predictive Contouring Control,\nwhich uses torque vectoring to stabilise and drive the vehicle in evasive\nmanoeuvres at the limit of handling. The proposed algorithm combines motion\nplanning, path tracking and vehicle stability objectives, prioritising\ncollision avoidance in emergencies. The controller's prediction model is a\nnonlinear double-track vehicle model based on an extended Fiala tyre to capture\nthe nonlinear coupled longitudinal and lateral dynamics. The controller\ncomputes the optimal steering angle and the longitudinal forces per each of the\nfour wheels to minimise tracking error in safe situations and maximise the\nvehicle-to-obstacle distance in emergencies. Thanks to the optimisation of the\nlongitudinal tyre forces, the proposed controller can produce an extra yaw\nmoment, increasing the vehicle's lateral agility to avoid obstacles while\nkeeping the vehicle stable. The optimal forces are constrained in the tyre\nfriction circle not to exceed the tyres and vehicle capabilities. In a\nhigh-fidelity simulation environment, we demonstrate the benefits of torque\nvectoring, showing that our proposed approach is capable of successfully\navoiding obstacles and keeping the vehicle stable while driving a double-lane\nchange manoeuvre, in comparison to baselines lacking torque vectoring or\ncollision avoidance prioritisation.\n", "link": "http://arxiv.org/abs/2405.10847v1", "date": "2024-05-17", "relevancy": 2.0025, "topK": [{"title": "Egocentric Computer Vision for Hands-Free Robotic Wheelchair Navigation", "link": "https://link.springer.com/article/10.1007/s10846-023-01807-4", "similarity": 0.5306}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.5047}, {"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.469}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Model%20Predictive%20Contouring%20Control%20for%20Vehicle%20Obstacle%20Avoidance%20at%0A%20%20the%20Limit%20of%20Handling%20Using%20Torque%20Vectoring&body=Title%3A%20Model%20Predictive%20Contouring%20Control%20for%20Vehicle%20Obstacle%20Avoidance%20at%0A%20%20the%20Limit%20of%20Handling%20Using%20Torque%20Vectoring%0AAuthor%3A%20Alberto%20Bertipaglia%20and%20Davide%20Tavernini%20and%20Umberto%20Montanaro%20and%20Mohsen%20Alirezaei%20and%20Riender%20Happee%20and%20Aldo%20Sorniotti%20and%20Barys%20Shyrokau%0AAbstract%3A%20%20%20This%20paper%20presents%20an%20original%20approach%20to%20vehicle%20obstacle%20avoidance.%20It%0Ainvolves%20the%20development%20of%20a%20nonlinear%20Model%20Predictive%20Contouring%20Control%2C%0Awhich%20uses%20torque%20vectoring%20to%20stabilise%20and%20drive%20the%20vehicle%20in%20evasive%0Amanoeuvres%20at%20the%20limit%20of%20handling.%20The%20proposed%20algorithm%20combines%20motion%0Aplanning%2C%20path%20tracking%20and%20vehicle%20stability%20objectives%2C%20prioritising%0Acollision%20avoidance%20in%20emergencies.%20The%20controller%27s%20prediction%20model%20is%20a%0Anonlinear%20double-track%20vehicle%20model%20based%20on%20an%20extended%20Fiala%20tyre%20to%20capture%0Athe%20nonlinear%20coupled%20longitudinal%20and%20lateral%20dynamics.%20The%20controller%0Acomputes%20the%20optimal%20steering%20angle%20and%20the%20longitudinal%20forces%20per%20each%20of%20the%0Afour%20wheels%20to%20minimise%20tracking%20error%20in%20safe%20situations%20and%20maximise%20the%0Avehicle-to-obstacle%20distance%20in%20emergencies.%20Thanks%20to%20the%20optimisation%20of%20the%0Alongitudinal%20tyre%20forces%2C%20the%20proposed%20controller%20can%20produce%20an%20extra%20yaw%0Amoment%2C%20increasing%20the%20vehicle%27s%20lateral%20agility%20to%20avoid%20obstacles%20while%0Akeeping%20the%20vehicle%20stable.%20The%20optimal%20forces%20are%20constrained%20in%20the%20tyre%0Afriction%20circle%20not%20to%20exceed%20the%20tyres%20and%20vehicle%20capabilities.%20In%20a%0Ahigh-fidelity%20simulation%20environment%2C%20we%20demonstrate%20the%20benefits%20of%20torque%0Avectoring%2C%20showing%20that%20our%20proposed%20approach%20is%20capable%20of%20successfully%0Aavoiding%20obstacles%20and%20keeping%20the%20vehicle%20stable%20while%20driving%20a%20double-lane%0Achange%20manoeuvre%2C%20in%20comparison%20to%20baselines%20lacking%20torque%20vectoring%20or%0Acollision%20avoidance%20prioritisation.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2405.10847v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DModel%2520Predictive%2520Contouring%2520Control%2520for%2520Vehicle%2520Obstacle%2520Avoidance%2520at%250A%2520%2520the%2520Limit%2520of%2520Handling%2520Using%2520Torque%2520Vectoring%26entry.906535625%3DAlberto%2520Bertipaglia%2520and%2520Davide%2520Tavernini%2520and%2520Umberto%2520Montanaro%2520and%2520Mohsen%2520Alirezaei%2520and%2520Riender%2520Happee%2520and%2520Aldo%2520Sorniotti%2520and%2520Barys%2520Shyrokau%26entry.1292438233%3D%2520%2520This%2520paper%2520presents%2520an%2520original%2520approach%2520to%2520vehicle%2520obstacle%2520avoidance.%2520It%250Ainvolves%2520the%2520development%2520of%2520a%2520nonlinear%2520Model%2520Predictive%2520Contouring%2520Control%252C%250Awhich%2520uses%2520torque%2520vectoring%2520to%2520stabilise%2520and%2520drive%2520the%2520vehicle%2520in%2520evasive%250Amanoeuvres%2520at%2520the%2520limit%2520of%2520handling.%2520The%2520proposed%2520algorithm%2520combines%2520motion%250Aplanning%252C%2520path%2520tracking%2520and%2520vehicle%2520stability%2520objectives%252C%2520prioritising%250Acollision%2520avoidance%2520in%2520emergencies.%2520The%2520controller%2527s%2520prediction%2520model%2520is%2520a%250Anonlinear%2520double-track%2520vehicle%2520model%2520based%2520on%2520an%2520extended%2520Fiala%2520tyre%2520to%2520capture%250Athe%2520nonlinear%2520coupled%2520longitudinal%2520and%2520lateral%2520dynamics.%2520The%2520controller%250Acomputes%2520the%2520optimal%2520steering%2520angle%2520and%2520the%2520longitudinal%2520forces%2520per%2520each%2520of%2520the%250Afour%2520wheels%2520to%2520minimise%2520tracking%2520error%2520in%2520safe%2520situations%2520and%2520maximise%2520the%250Avehicle-to-obstacle%2520distance%2520in%2520emergencies.%2520Thanks%2520to%2520the%2520optimisation%2520of%2520the%250Alongitudinal%2520tyre%2520forces%252C%2520the%2520proposed%2520controller%2520can%2520produce%2520an%2520extra%2520yaw%250Amoment%252C%2520increasing%2520the%2520vehicle%2527s%2520lateral%2520agility%2520to%2520avoid%2520obstacles%2520while%250Akeeping%2520the%2520vehicle%2520stable.%2520The%2520optimal%2520forces%2520are%2520constrained%2520in%2520the%2520tyre%250Afriction%2520circle%2520not%2520to%2520exceed%2520the%2520tyres%2520and%2520vehicle%2520capabilities.%2520In%2520a%250Ahigh-fidelity%2520simulation%2520environment%252C%2520we%2520demonstrate%2520the%2520benefits%2520of%2520torque%250Avectoring%252C%2520showing%2520that%2520our%2520proposed%2520approach%2520is%2520capable%2520of%2520successfully%250Aavoiding%2520obstacles%2520and%2520keeping%2520the%2520vehicle%2520stable%2520while%2520driving%2520a%2520double-lane%250Achange%2520manoeuvre%252C%2520in%2520comparison%2520to%2520baselines%2520lacking%2520torque%2520vectoring%2520or%250Acollision%2520avoidance%2520prioritisation.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2405.10847v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Model%20Predictive%20Contouring%20Control%20for%20Vehicle%20Obstacle%20Avoidance%20at%0A%20%20the%20Limit%20of%20Handling%20Using%20Torque%20Vectoring&entry.906535625=Alberto%20Bertipaglia%20and%20Davide%20Tavernini%20and%20Umberto%20Montanaro%20and%20Mohsen%20Alirezaei%20and%20Riender%20Happee%20and%20Aldo%20Sorniotti%20and%20Barys%20Shyrokau&entry.1292438233=%20%20This%20paper%20presents%20an%20original%20approach%20to%20vehicle%20obstacle%20avoidance.%20It%0Ainvolves%20the%20development%20of%20a%20nonlinear%20Model%20Predictive%20Contouring%20Control%2C%0Awhich%20uses%20torque%20vectoring%20to%20stabilise%20and%20drive%20the%20vehicle%20in%20evasive%0Amanoeuvres%20at%20the%20limit%20of%20handling.%20The%20proposed%20algorithm%20combines%20motion%0Aplanning%2C%20path%20tracking%20and%20vehicle%20stability%20objectives%2C%20prioritising%0Acollision%20avoidance%20in%20emergencies.%20The%20controller%27s%20prediction%20model%20is%20a%0Anonlinear%20double-track%20vehicle%20model%20based%20on%20an%20extended%20Fiala%20tyre%20to%20capture%0Athe%20nonlinear%20coupled%20longitudinal%20and%20lateral%20dynamics.%20The%20controller%0Acomputes%20the%20optimal%20steering%20angle%20and%20the%20longitudinal%20forces%20per%20each%20of%20the%0Afour%20wheels%20to%20minimise%20tracking%20error%20in%20safe%20situations%20and%20maximise%20the%0Avehicle-to-obstacle%20distance%20in%20emergencies.%20Thanks%20to%20the%20optimisation%20of%20the%0Alongitudinal%20tyre%20forces%2C%20the%20proposed%20controller%20can%20produce%20an%20extra%20yaw%0Amoment%2C%20increasing%20the%20vehicle%27s%20lateral%20agility%20to%20avoid%20obstacles%20while%0Akeeping%20the%20vehicle%20stable.%20The%20optimal%20forces%20are%20constrained%20in%20the%20tyre%0Afriction%20circle%20not%20to%20exceed%20the%20tyres%20and%20vehicle%20capabilities.%20In%20a%0Ahigh-fidelity%20simulation%20environment%2C%20we%20demonstrate%20the%20benefits%20of%20torque%0Avectoring%2C%20showing%20that%20our%20proposed%20approach%20is%20capable%20of%20successfully%0Aavoiding%20obstacles%20and%20keeping%20the%20vehicle%20stable%20while%20driving%20a%20double-lane%0Achange%20manoeuvre%2C%20in%20comparison%20to%20baselines%20lacking%20torque%20vectoring%20or%0Acollision%20avoidance%20prioritisation.%0A&entry.1838667208=http%3A//arxiv.org/abs/2405.10847v1&entry.124074799=Read"},
{"title": "Tackling the Curse of Dimensionality with Physics-Informed Neural\n  Networks", "author": "Zheyuan Hu and Khemraj Shukla and George Em Karniadakis and Kenji Kawaguchi", "abstract": "  The curse-of-dimensionality taxes computational resources heavily with\nexponentially increasing computational cost as the dimension increases. This\nposes great challenges in solving high-dimensional PDEs, as Richard E. Bellman\nfirst pointed out over 60 years ago. While there has been some recent success\nin solving numerically partial differential equations (PDEs) in high\ndimensions, such computations are prohibitively expensive, and true scaling of\ngeneral nonlinear PDEs to high dimensions has never been achieved. We develop a\nnew method of scaling up physics-informed neural networks (PINNs) to solve\narbitrary high-dimensional PDEs. The new method, called Stochastic Dimension\nGradient Descent (SDGD), decomposes a gradient of PDEs into pieces\ncorresponding to different dimensions and randomly samples a subset of these\ndimensional pieces in each iteration of training PINNs. We prove theoretically\nthe convergence and other desired properties of the proposed method. We\ndemonstrate in various diverse tests that the proposed method can solve many\nnotoriously hard high-dimensional PDEs, including the Hamilton-Jacobi-Bellman\n(HJB) and the Schr\\\"{o}dinger equations in tens of thousands of dimensions very\nfast on a single GPU using the PINNs mesh-free approach. Notably, we solve\nnonlinear PDEs with nontrivial, anisotropic, and inseparable solutions in\n100,000 effective dimensions in 12 hours on a single GPU using SDGD with PINNs.\nSince SDGD is a general training methodology of PINNs, it can be applied to any\ncurrent and future variants of PINNs to scale them up for arbitrary\nhigh-dimensional PDEs.\n", "link": "http://arxiv.org/abs/2307.12306v6", "date": "2024-05-17", "relevancy": 1.9851, "topK": [{"title": "Dynamic Gaussians Mesh: Consistent Mesh Reconstruction from Monocular\n  Videos", "link": "http://arxiv.org/abs/2404.12379v2", "similarity": 0.4966}, {"title": "Scaling Rectified Flow Transformers for High-Resolution Image Synthesis", "link": "http://arxiv.org/abs/2403.03206v1", "similarity": 0.4961}, {"title": "Boosted dynamic neural networks", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/26302", "similarity": 0.496}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Tackling%20the%20Curse%20of%20Dimensionality%20with%20Physics-Informed%20Neural%0A%20%20Networks&body=Title%3A%20Tackling%20the%20Curse%20of%20Dimensionality%20with%20Physics-Informed%20Neural%0A%20%20Networks%0AAuthor%3A%20Zheyuan%20Hu%20and%20Khemraj%20Shukla%20and%20George%20Em%20Karniadakis%20and%20Kenji%20Kawaguchi%0AAbstract%3A%20%20%20The%20curse-of-dimensionality%20taxes%20computational%20resources%20heavily%20with%0Aexponentially%20increasing%20computational%20cost%20as%20the%20dimension%20increases.%20This%0Aposes%20great%20challenges%20in%20solving%20high-dimensional%20PDEs%2C%20as%20Richard%20E.%20Bellman%0Afirst%20pointed%20out%20over%2060%20years%20ago.%20While%20there%20has%20been%20some%20recent%20success%0Ain%20solving%20numerically%20partial%20differential%20equations%20%28PDEs%29%20in%20high%0Adimensions%2C%20such%20computations%20are%20prohibitively%20expensive%2C%20and%20true%20scaling%20of%0Ageneral%20nonlinear%20PDEs%20to%20high%20dimensions%20has%20never%20been%20achieved.%20We%20develop%20a%0Anew%20method%20of%20scaling%20up%20physics-informed%20neural%20networks%20%28PINNs%29%20to%20solve%0Aarbitrary%20high-dimensional%20PDEs.%20The%20new%20method%2C%20called%20Stochastic%20Dimension%0AGradient%20Descent%20%28SDGD%29%2C%20decomposes%20a%20gradient%20of%20PDEs%20into%20pieces%0Acorresponding%20to%20different%20dimensions%20and%20randomly%20samples%20a%20subset%20of%20these%0Adimensional%20pieces%20in%20each%20iteration%20of%20training%20PINNs.%20We%20prove%20theoretically%0Athe%20convergence%20and%20other%20desired%20properties%20of%20the%20proposed%20method.%20We%0Ademonstrate%20in%20various%20diverse%20tests%20that%20the%20proposed%20method%20can%20solve%20many%0Anotoriously%20hard%20high-dimensional%20PDEs%2C%20including%20the%20Hamilton-Jacobi-Bellman%0A%28HJB%29%20and%20the%20Schr%5C%22%7Bo%7Ddinger%20equations%20in%20tens%20of%20thousands%20of%20dimensions%20very%0Afast%20on%20a%20single%20GPU%20using%20the%20PINNs%20mesh-free%20approach.%20Notably%2C%20we%20solve%0Anonlinear%20PDEs%20with%20nontrivial%2C%20anisotropic%2C%20and%20inseparable%20solutions%20in%0A100%2C000%20effective%20dimensions%20in%2012%20hours%20on%20a%20single%20GPU%20using%20SDGD%20with%20PINNs.%0ASince%20SDGD%20is%20a%20general%20training%20methodology%20of%20PINNs%2C%20it%20can%20be%20applied%20to%20any%0Acurrent%20and%20future%20variants%20of%20PINNs%20to%20scale%20them%20up%20for%20arbitrary%0Ahigh-dimensional%20PDEs.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2307.12306v6%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DTackling%2520the%2520Curse%2520of%2520Dimensionality%2520with%2520Physics-Informed%2520Neural%250A%2520%2520Networks%26entry.906535625%3DZheyuan%2520Hu%2520and%2520Khemraj%2520Shukla%2520and%2520George%2520Em%2520Karniadakis%2520and%2520Kenji%2520Kawaguchi%26entry.1292438233%3D%2520%2520The%2520curse-of-dimensionality%2520taxes%2520computational%2520resources%2520heavily%2520with%250Aexponentially%2520increasing%2520computational%2520cost%2520as%2520the%2520dimension%2520increases.%2520This%250Aposes%2520great%2520challenges%2520in%2520solving%2520high-dimensional%2520PDEs%252C%2520as%2520Richard%2520E.%2520Bellman%250Afirst%2520pointed%2520out%2520over%252060%2520years%2520ago.%2520While%2520there%2520has%2520been%2520some%2520recent%2520success%250Ain%2520solving%2520numerically%2520partial%2520differential%2520equations%2520%2528PDEs%2529%2520in%2520high%250Adimensions%252C%2520such%2520computations%2520are%2520prohibitively%2520expensive%252C%2520and%2520true%2520scaling%2520of%250Ageneral%2520nonlinear%2520PDEs%2520to%2520high%2520dimensions%2520has%2520never%2520been%2520achieved.%2520We%2520develop%2520a%250Anew%2520method%2520of%2520scaling%2520up%2520physics-informed%2520neural%2520networks%2520%2528PINNs%2529%2520to%2520solve%250Aarbitrary%2520high-dimensional%2520PDEs.%2520The%2520new%2520method%252C%2520called%2520Stochastic%2520Dimension%250AGradient%2520Descent%2520%2528SDGD%2529%252C%2520decomposes%2520a%2520gradient%2520of%2520PDEs%2520into%2520pieces%250Acorresponding%2520to%2520different%2520dimensions%2520and%2520randomly%2520samples%2520a%2520subset%2520of%2520these%250Adimensional%2520pieces%2520in%2520each%2520iteration%2520of%2520training%2520PINNs.%2520We%2520prove%2520theoretically%250Athe%2520convergence%2520and%2520other%2520desired%2520properties%2520of%2520the%2520proposed%2520method.%2520We%250Ademonstrate%2520in%2520various%2520diverse%2520tests%2520that%2520the%2520proposed%2520method%2520can%2520solve%2520many%250Anotoriously%2520hard%2520high-dimensional%2520PDEs%252C%2520including%2520the%2520Hamilton-Jacobi-Bellman%250A%2528HJB%2529%2520and%2520the%2520Schr%255C%2522%257Bo%257Ddinger%2520equations%2520in%2520tens%2520of%2520thousands%2520of%2520dimensions%2520very%250Afast%2520on%2520a%2520single%2520GPU%2520using%2520the%2520PINNs%2520mesh-free%2520approach.%2520Notably%252C%2520we%2520solve%250Anonlinear%2520PDEs%2520with%2520nontrivial%252C%2520anisotropic%252C%2520and%2520inseparable%2520solutions%2520in%250A100%252C000%2520effective%2520dimensions%2520in%252012%2520hours%2520on%2520a%2520single%2520GPU%2520using%2520SDGD%2520with%2520PINNs.%250ASince%2520SDGD%2520is%2520a%2520general%2520training%2520methodology%2520of%2520PINNs%252C%2520it%2520can%2520be%2520applied%2520to%2520any%250Acurrent%2520and%2520future%2520variants%2520of%2520PINNs%2520to%2520scale%2520them%2520up%2520for%2520arbitrary%250Ahigh-dimensional%2520PDEs.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2307.12306v6%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Tackling%20the%20Curse%20of%20Dimensionality%20with%20Physics-Informed%20Neural%0A%20%20Networks&entry.906535625=Zheyuan%20Hu%20and%20Khemraj%20Shukla%20and%20George%20Em%20Karniadakis%20and%20Kenji%20Kawaguchi&entry.1292438233=%20%20The%20curse-of-dimensionality%20taxes%20computational%20resources%20heavily%20with%0Aexponentially%20increasing%20computational%20cost%20as%20the%20dimension%20increases.%20This%0Aposes%20great%20challenges%20in%20solving%20high-dimensional%20PDEs%2C%20as%20Richard%20E.%20Bellman%0Afirst%20pointed%20out%20over%2060%20years%20ago.%20While%20there%20has%20been%20some%20recent%20success%0Ain%20solving%20numerically%20partial%20differential%20equations%20%28PDEs%29%20in%20high%0Adimensions%2C%20such%20computations%20are%20prohibitively%20expensive%2C%20and%20true%20scaling%20of%0Ageneral%20nonlinear%20PDEs%20to%20high%20dimensions%20has%20never%20been%20achieved.%20We%20develop%20a%0Anew%20method%20of%20scaling%20up%20physics-informed%20neural%20networks%20%28PINNs%29%20to%20solve%0Aarbitrary%20high-dimensional%20PDEs.%20The%20new%20method%2C%20called%20Stochastic%20Dimension%0AGradient%20Descent%20%28SDGD%29%2C%20decomposes%20a%20gradient%20of%20PDEs%20into%20pieces%0Acorresponding%20to%20different%20dimensions%20and%20randomly%20samples%20a%20subset%20of%20these%0Adimensional%20pieces%20in%20each%20iteration%20of%20training%20PINNs.%20We%20prove%20theoretically%0Athe%20convergence%20and%20other%20desired%20properties%20of%20the%20proposed%20method.%20We%0Ademonstrate%20in%20various%20diverse%20tests%20that%20the%20proposed%20method%20can%20solve%20many%0Anotoriously%20hard%20high-dimensional%20PDEs%2C%20including%20the%20Hamilton-Jacobi-Bellman%0A%28HJB%29%20and%20the%20Schr%5C%22%7Bo%7Ddinger%20equations%20in%20tens%20of%20thousands%20of%20dimensions%20very%0Afast%20on%20a%20single%20GPU%20using%20the%20PINNs%20mesh-free%20approach.%20Notably%2C%20we%20solve%0Anonlinear%20PDEs%20with%20nontrivial%2C%20anisotropic%2C%20and%20inseparable%20solutions%20in%0A100%2C000%20effective%20dimensions%20in%2012%20hours%20on%20a%20single%20GPU%20using%20SDGD%20with%20PINNs.%0ASince%20SDGD%20is%20a%20general%20training%20methodology%20of%20PINNs%2C%20it%20can%20be%20applied%20to%20any%0Acurrent%20and%20future%20variants%20of%20PINNs%20to%20scale%20them%20up%20for%20arbitrary%0Ahigh-dimensional%20PDEs.%0A&entry.1838667208=http%3A//arxiv.org/abs/2307.12306v6&entry.124074799=Read"},
{"title": "HARIS: Human-Like Attention for Reference Image Segmentation", "author": "Mengxi Zhang and Heqing Lian and Yiming Liu and Kang Rong and Jie Chen", "abstract": "  Referring image segmentation (RIS) aims to locate the particular region\ncorresponding to the language expression. Existing methods incorporate features\nfrom different modalities in a \\emph{bottom-up} manner. This design may get\nsome unnecessary image-text pairs, which leads to an inaccurate segmentation\nmask. In this paper, we propose a referring image segmentation method called\nHARIS, which introduces the Human-Like Attention mechanism and uses the\nparameter-efficient fine-tuning (PEFT) framework. To be specific, the\nHuman-Like Attention gets a \\emph{feedback} signal from multi-modal features,\nwhich makes the network center on the specific objects and discard the\nirrelevant image-text pairs. Besides, we introduce the PEFT framework to\npreserve the zero-shot ability of pre-trained encoders. Extensive experiments\non three widely used RIS benchmarks and the PhraseCut dataset demonstrate that\nour method achieves state-of-the-art performance and great zero-shot ability.\n", "link": "http://arxiv.org/abs/2405.10707v1", "date": "2024-05-17", "relevancy": 1.9772, "topK": [{"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.503}, {"title": "RTG-SLAM: Real-time 3D Reconstruction at Scale using Gaussian Splatting", "link": "http://arxiv.org/abs/2404.19706v1", "similarity": 0.4921}, {"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.4781}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20HARIS%3A%20Human-Like%20Attention%20for%20Reference%20Image%20Segmentation&body=Title%3A%20HARIS%3A%20Human-Like%20Attention%20for%20Reference%20Image%20Segmentation%0AAuthor%3A%20Mengxi%20Zhang%20and%20Heqing%20Lian%20and%20Yiming%20Liu%20and%20Kang%20Rong%20and%20Jie%20Chen%0AAbstract%3A%20%20%20Referring%20image%20segmentation%20%28RIS%29%20aims%20to%20locate%20the%20particular%20region%0Acorresponding%20to%20the%20language%20expression.%20Existing%20methods%20incorporate%20features%0Afrom%20different%20modalities%20in%20a%20%5Cemph%7Bbottom-up%7D%20manner.%20This%20design%20may%20get%0Asome%20unnecessary%20image-text%20pairs%2C%20which%20leads%20to%20an%20inaccurate%20segmentation%0Amask.%20In%20this%20paper%2C%20we%20propose%20a%20referring%20image%20segmentation%20method%20called%0AHARIS%2C%20which%20introduces%20the%20Human-Like%20Attention%20mechanism%20and%20uses%20the%0Aparameter-efficient%20fine-tuning%20%28PEFT%29%20framework.%20To%20be%20specific%2C%20the%0AHuman-Like%20Attention%20gets%20a%20%5Cemph%7Bfeedback%7D%20signal%20from%20multi-modal%20features%2C%0Awhich%20makes%20the%20network%20center%20on%20the%20specific%20objects%20and%20discard%20the%0Airrelevant%20image-text%20pairs.%20Besides%2C%20we%20introduce%20the%20PEFT%20framework%20to%0Apreserve%20the%20zero-shot%20ability%20of%20pre-trained%20encoders.%20Extensive%20experiments%0Aon%20three%20widely%20used%20RIS%20benchmarks%20and%20the%20PhraseCut%20dataset%20demonstrate%20that%0Aour%20method%20achieves%20state-of-the-art%20performance%20and%20great%20zero-shot%20ability.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2405.10707v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DHARIS%253A%2520Human-Like%2520Attention%2520for%2520Reference%2520Image%2520Segmentation%26entry.906535625%3DMengxi%2520Zhang%2520and%2520Heqing%2520Lian%2520and%2520Yiming%2520Liu%2520and%2520Kang%2520Rong%2520and%2520Jie%2520Chen%26entry.1292438233%3D%2520%2520Referring%2520image%2520segmentation%2520%2528RIS%2529%2520aims%2520to%2520locate%2520the%2520particular%2520region%250Acorresponding%2520to%2520the%2520language%2520expression.%2520Existing%2520methods%2520incorporate%2520features%250Afrom%2520different%2520modalities%2520in%2520a%2520%255Cemph%257Bbottom-up%257D%2520manner.%2520This%2520design%2520may%2520get%250Asome%2520unnecessary%2520image-text%2520pairs%252C%2520which%2520leads%2520to%2520an%2520inaccurate%2520segmentation%250Amask.%2520In%2520this%2520paper%252C%2520we%2520propose%2520a%2520referring%2520image%2520segmentation%2520method%2520called%250AHARIS%252C%2520which%2520introduces%2520the%2520Human-Like%2520Attention%2520mechanism%2520and%2520uses%2520the%250Aparameter-efficient%2520fine-tuning%2520%2528PEFT%2529%2520framework.%2520To%2520be%2520specific%252C%2520the%250AHuman-Like%2520Attention%2520gets%2520a%2520%255Cemph%257Bfeedback%257D%2520signal%2520from%2520multi-modal%2520features%252C%250Awhich%2520makes%2520the%2520network%2520center%2520on%2520the%2520specific%2520objects%2520and%2520discard%2520the%250Airrelevant%2520image-text%2520pairs.%2520Besides%252C%2520we%2520introduce%2520the%2520PEFT%2520framework%2520to%250Apreserve%2520the%2520zero-shot%2520ability%2520of%2520pre-trained%2520encoders.%2520Extensive%2520experiments%250Aon%2520three%2520widely%2520used%2520RIS%2520benchmarks%2520and%2520the%2520PhraseCut%2520dataset%2520demonstrate%2520that%250Aour%2520method%2520achieves%2520state-of-the-art%2520performance%2520and%2520great%2520zero-shot%2520ability.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2405.10707v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=HARIS%3A%20Human-Like%20Attention%20for%20Reference%20Image%20Segmentation&entry.906535625=Mengxi%20Zhang%20and%20Heqing%20Lian%20and%20Yiming%20Liu%20and%20Kang%20Rong%20and%20Jie%20Chen&entry.1292438233=%20%20Referring%20image%20segmentation%20%28RIS%29%20aims%20to%20locate%20the%20particular%20region%0Acorresponding%20to%20the%20language%20expression.%20Existing%20methods%20incorporate%20features%0Afrom%20different%20modalities%20in%20a%20%5Cemph%7Bbottom-up%7D%20manner.%20This%20design%20may%20get%0Asome%20unnecessary%20image-text%20pairs%2C%20which%20leads%20to%20an%20inaccurate%20segmentation%0Amask.%20In%20this%20paper%2C%20we%20propose%20a%20referring%20image%20segmentation%20method%20called%0AHARIS%2C%20which%20introduces%20the%20Human-Like%20Attention%20mechanism%20and%20uses%20the%0Aparameter-efficient%20fine-tuning%20%28PEFT%29%20framework.%20To%20be%20specific%2C%20the%0AHuman-Like%20Attention%20gets%20a%20%5Cemph%7Bfeedback%7D%20signal%20from%20multi-modal%20features%2C%0Awhich%20makes%20the%20network%20center%20on%20the%20specific%20objects%20and%20discard%20the%0Airrelevant%20image-text%20pairs.%20Besides%2C%20we%20introduce%20the%20PEFT%20framework%20to%0Apreserve%20the%20zero-shot%20ability%20of%20pre-trained%20encoders.%20Extensive%20experiments%0Aon%20three%20widely%20used%20RIS%20benchmarks%20and%20the%20PhraseCut%20dataset%20demonstrate%20that%0Aour%20method%20achieves%20state-of-the-art%20performance%20and%20great%20zero-shot%20ability.%0A&entry.1838667208=http%3A//arxiv.org/abs/2405.10707v1&entry.124074799=Read"},
{"title": "Transpose Attack: Stealing Datasets with Bidirectional Training", "author": "Guy Amit and Mosh Levy and Yisroel Mirsky", "abstract": "  Deep neural networks are normally executed in the forward direction. However,\nin this work, we identify a vulnerability that enables models to be trained in\nboth directions and on different tasks. Adversaries can exploit this capability\nto hide rogue models within seemingly legitimate models. In addition, in this\nwork we show that neural networks can be taught to systematically memorize and\nretrieve specific samples from datasets. Together, these findings expose a\nnovel method in which adversaries can exfiltrate datasets from protected\nlearning environments under the guise of legitimate models. We focus on the\ndata exfiltration attack and show that modern architectures can be used to\nsecretly exfiltrate tens of thousands of samples with high fidelity, high\nenough to compromise data privacy and even train new models. Moreover, to\nmitigate this threat we propose a novel approach for detecting infected models.\n", "link": "http://arxiv.org/abs/2311.07389v2", "date": "2024-05-17", "relevancy": 1.9708, "topK": [{"title": "Boosted dynamic neural networks", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/26302", "similarity": 0.4985}, {"title": "Breadcrumbs: Adversarial class-balanced sampling for long-tailed recognition", "link": "https://link.springer.com/chapter/10.1007/978-3-031-20053-3_37", "similarity": 0.4928}, {"title": "SDXS: Real-Time One-Step Latent Diffusion Models with Image Conditions", "link": "http://arxiv.org/abs/2403.16627v1", "similarity": 0.478}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Transpose%20Attack%3A%20Stealing%20Datasets%20with%20Bidirectional%20Training&body=Title%3A%20Transpose%20Attack%3A%20Stealing%20Datasets%20with%20Bidirectional%20Training%0AAuthor%3A%20Guy%20Amit%20and%20Mosh%20Levy%20and%20Yisroel%20Mirsky%0AAbstract%3A%20%20%20Deep%20neural%20networks%20are%20normally%20executed%20in%20the%20forward%20direction.%20However%2C%0Ain%20this%20work%2C%20we%20identify%20a%20vulnerability%20that%20enables%20models%20to%20be%20trained%20in%0Aboth%20directions%20and%20on%20different%20tasks.%20Adversaries%20can%20exploit%20this%20capability%0Ato%20hide%20rogue%20models%20within%20seemingly%20legitimate%20models.%20In%20addition%2C%20in%20this%0Awork%20we%20show%20that%20neural%20networks%20can%20be%20taught%20to%20systematically%20memorize%20and%0Aretrieve%20specific%20samples%20from%20datasets.%20Together%2C%20these%20findings%20expose%20a%0Anovel%20method%20in%20which%20adversaries%20can%20exfiltrate%20datasets%20from%20protected%0Alearning%20environments%20under%20the%20guise%20of%20legitimate%20models.%20We%20focus%20on%20the%0Adata%20exfiltration%20attack%20and%20show%20that%20modern%20architectures%20can%20be%20used%20to%0Asecretly%20exfiltrate%20tens%20of%20thousands%20of%20samples%20with%20high%20fidelity%2C%20high%0Aenough%20to%20compromise%20data%20privacy%20and%20even%20train%20new%20models.%20Moreover%2C%20to%0Amitigate%20this%20threat%20we%20propose%20a%20novel%20approach%20for%20detecting%20infected%20models.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2311.07389v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DTranspose%2520Attack%253A%2520Stealing%2520Datasets%2520with%2520Bidirectional%2520Training%26entry.906535625%3DGuy%2520Amit%2520and%2520Mosh%2520Levy%2520and%2520Yisroel%2520Mirsky%26entry.1292438233%3D%2520%2520Deep%2520neural%2520networks%2520are%2520normally%2520executed%2520in%2520the%2520forward%2520direction.%2520However%252C%250Ain%2520this%2520work%252C%2520we%2520identify%2520a%2520vulnerability%2520that%2520enables%2520models%2520to%2520be%2520trained%2520in%250Aboth%2520directions%2520and%2520on%2520different%2520tasks.%2520Adversaries%2520can%2520exploit%2520this%2520capability%250Ato%2520hide%2520rogue%2520models%2520within%2520seemingly%2520legitimate%2520models.%2520In%2520addition%252C%2520in%2520this%250Awork%2520we%2520show%2520that%2520neural%2520networks%2520can%2520be%2520taught%2520to%2520systematically%2520memorize%2520and%250Aretrieve%2520specific%2520samples%2520from%2520datasets.%2520Together%252C%2520these%2520findings%2520expose%2520a%250Anovel%2520method%2520in%2520which%2520adversaries%2520can%2520exfiltrate%2520datasets%2520from%2520protected%250Alearning%2520environments%2520under%2520the%2520guise%2520of%2520legitimate%2520models.%2520We%2520focus%2520on%2520the%250Adata%2520exfiltration%2520attack%2520and%2520show%2520that%2520modern%2520architectures%2520can%2520be%2520used%2520to%250Asecretly%2520exfiltrate%2520tens%2520of%2520thousands%2520of%2520samples%2520with%2520high%2520fidelity%252C%2520high%250Aenough%2520to%2520compromise%2520data%2520privacy%2520and%2520even%2520train%2520new%2520models.%2520Moreover%252C%2520to%250Amitigate%2520this%2520threat%2520we%2520propose%2520a%2520novel%2520approach%2520for%2520detecting%2520infected%2520models.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2311.07389v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Transpose%20Attack%3A%20Stealing%20Datasets%20with%20Bidirectional%20Training&entry.906535625=Guy%20Amit%20and%20Mosh%20Levy%20and%20Yisroel%20Mirsky&entry.1292438233=%20%20Deep%20neural%20networks%20are%20normally%20executed%20in%20the%20forward%20direction.%20However%2C%0Ain%20this%20work%2C%20we%20identify%20a%20vulnerability%20that%20enables%20models%20to%20be%20trained%20in%0Aboth%20directions%20and%20on%20different%20tasks.%20Adversaries%20can%20exploit%20this%20capability%0Ato%20hide%20rogue%20models%20within%20seemingly%20legitimate%20models.%20In%20addition%2C%20in%20this%0Awork%20we%20show%20that%20neural%20networks%20can%20be%20taught%20to%20systematically%20memorize%20and%0Aretrieve%20specific%20samples%20from%20datasets.%20Together%2C%20these%20findings%20expose%20a%0Anovel%20method%20in%20which%20adversaries%20can%20exfiltrate%20datasets%20from%20protected%0Alearning%20environments%20under%20the%20guise%20of%20legitimate%20models.%20We%20focus%20on%20the%0Adata%20exfiltration%20attack%20and%20show%20that%20modern%20architectures%20can%20be%20used%20to%0Asecretly%20exfiltrate%20tens%20of%20thousands%20of%20samples%20with%20high%20fidelity%2C%20high%0Aenough%20to%20compromise%20data%20privacy%20and%20even%20train%20new%20models.%20Moreover%2C%20to%0Amitigate%20this%20threat%20we%20propose%20a%20novel%20approach%20for%20detecting%20infected%20models.%0A&entry.1838667208=http%3A//arxiv.org/abs/2311.07389v2&entry.124074799=Read"},
{"title": "Tailoring Vaccine Messaging with Common-Ground Opinions", "author": "Rickard Stureborg and Sanxing Chen and Ruoyu Xie and Aayushi Patel and Christopher Li and Chloe Qinyu Zhu and Tingnan Hu and Jun Yang and Bhuwan Dhingra", "abstract": "  One way to personalize chatbot interactions is by establishing common ground\nwith the intended reader. A domain where establishing mutual understanding\ncould be particularly impactful is vaccine concerns and misinformation. Vaccine\ninterventions are forms of messaging which aim to answer concerns expressed\nabout vaccination. Tailoring responses in this domain is difficult, since\nopinions often have seemingly little ideological overlap. We define the task of\ntailoring vaccine interventions to a Common-Ground Opinion (CGO). Tailoring\nresponses to a CGO involves meaningfully improving the answer by relating it to\nan opinion or belief the reader holds. In this paper we introduce TAILOR-CGO, a\ndataset for evaluating how well responses are tailored to provided CGOs. We\nbenchmark several major LLMs on this task; finding GPT-4-Turbo performs\nsignificantly better than others. We also build automatic evaluation metrics,\nincluding an efficient and accurate BERT model that outperforms finetuned LLMs,\ninvestigate how to successfully tailor vaccine messaging to CGOs, and provide\nactionable recommendations from this investigation.\n  Code and model weights: https://github.com/rickardstureborg/tailor-cgo\nDataset: https://huggingface.co/datasets/DukeNLP/tailor-cgo\n", "link": "http://arxiv.org/abs/2405.10861v1", "date": "2024-05-17", "relevancy": 1.9571, "topK": [{"title": "Customize-A-Video: One-Shot Motion Customization of Text-to-Video\n  Diffusion Models", "link": "http://arxiv.org/abs/2402.14780v1", "similarity": 0.5041}, {"title": "Magic-Me: Identity-Specific Video Customized Diffusion", "link": "http://arxiv.org/abs/2402.09368v1", "similarity": 0.4873}, {"title": "DressCode: Autoregressively Sewing and Generating Garments from Text\n  Guidance", "link": "http://arxiv.org/abs/2401.16465v3", "similarity": 0.4853}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Tailoring%20Vaccine%20Messaging%20with%20Common-Ground%20Opinions&body=Title%3A%20Tailoring%20Vaccine%20Messaging%20with%20Common-Ground%20Opinions%0AAuthor%3A%20Rickard%20Stureborg%20and%20Sanxing%20Chen%20and%20Ruoyu%20Xie%20and%20Aayushi%20Patel%20and%20Christopher%20Li%20and%20Chloe%20Qinyu%20Zhu%20and%20Tingnan%20Hu%20and%20Jun%20Yang%20and%20Bhuwan%20Dhingra%0AAbstract%3A%20%20%20One%20way%20to%20personalize%20chatbot%20interactions%20is%20by%20establishing%20common%20ground%0Awith%20the%20intended%20reader.%20A%20domain%20where%20establishing%20mutual%20understanding%0Acould%20be%20particularly%20impactful%20is%20vaccine%20concerns%20and%20misinformation.%20Vaccine%0Ainterventions%20are%20forms%20of%20messaging%20which%20aim%20to%20answer%20concerns%20expressed%0Aabout%20vaccination.%20Tailoring%20responses%20in%20this%20domain%20is%20difficult%2C%20since%0Aopinions%20often%20have%20seemingly%20little%20ideological%20overlap.%20We%20define%20the%20task%20of%0Atailoring%20vaccine%20interventions%20to%20a%20Common-Ground%20Opinion%20%28CGO%29.%20Tailoring%0Aresponses%20to%20a%20CGO%20involves%20meaningfully%20improving%20the%20answer%20by%20relating%20it%20to%0Aan%20opinion%20or%20belief%20the%20reader%20holds.%20In%20this%20paper%20we%20introduce%20TAILOR-CGO%2C%20a%0Adataset%20for%20evaluating%20how%20well%20responses%20are%20tailored%20to%20provided%20CGOs.%20We%0Abenchmark%20several%20major%20LLMs%20on%20this%20task%3B%20finding%20GPT-4-Turbo%20performs%0Asignificantly%20better%20than%20others.%20We%20also%20build%20automatic%20evaluation%20metrics%2C%0Aincluding%20an%20efficient%20and%20accurate%20BERT%20model%20that%20outperforms%20finetuned%20LLMs%2C%0Ainvestigate%20how%20to%20successfully%20tailor%20vaccine%20messaging%20to%20CGOs%2C%20and%20provide%0Aactionable%20recommendations%20from%20this%20investigation.%0A%20%20Code%20and%20model%20weights%3A%20https%3A//github.com/rickardstureborg/tailor-cgo%0ADataset%3A%20https%3A//huggingface.co/datasets/DukeNLP/tailor-cgo%0A%0ALink%3A%20http%3A//arxiv.org/abs/2405.10861v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DTailoring%2520Vaccine%2520Messaging%2520with%2520Common-Ground%2520Opinions%26entry.906535625%3DRickard%2520Stureborg%2520and%2520Sanxing%2520Chen%2520and%2520Ruoyu%2520Xie%2520and%2520Aayushi%2520Patel%2520and%2520Christopher%2520Li%2520and%2520Chloe%2520Qinyu%2520Zhu%2520and%2520Tingnan%2520Hu%2520and%2520Jun%2520Yang%2520and%2520Bhuwan%2520Dhingra%26entry.1292438233%3D%2520%2520One%2520way%2520to%2520personalize%2520chatbot%2520interactions%2520is%2520by%2520establishing%2520common%2520ground%250Awith%2520the%2520intended%2520reader.%2520A%2520domain%2520where%2520establishing%2520mutual%2520understanding%250Acould%2520be%2520particularly%2520impactful%2520is%2520vaccine%2520concerns%2520and%2520misinformation.%2520Vaccine%250Ainterventions%2520are%2520forms%2520of%2520messaging%2520which%2520aim%2520to%2520answer%2520concerns%2520expressed%250Aabout%2520vaccination.%2520Tailoring%2520responses%2520in%2520this%2520domain%2520is%2520difficult%252C%2520since%250Aopinions%2520often%2520have%2520seemingly%2520little%2520ideological%2520overlap.%2520We%2520define%2520the%2520task%2520of%250Atailoring%2520vaccine%2520interventions%2520to%2520a%2520Common-Ground%2520Opinion%2520%2528CGO%2529.%2520Tailoring%250Aresponses%2520to%2520a%2520CGO%2520involves%2520meaningfully%2520improving%2520the%2520answer%2520by%2520relating%2520it%2520to%250Aan%2520opinion%2520or%2520belief%2520the%2520reader%2520holds.%2520In%2520this%2520paper%2520we%2520introduce%2520TAILOR-CGO%252C%2520a%250Adataset%2520for%2520evaluating%2520how%2520well%2520responses%2520are%2520tailored%2520to%2520provided%2520CGOs.%2520We%250Abenchmark%2520several%2520major%2520LLMs%2520on%2520this%2520task%253B%2520finding%2520GPT-4-Turbo%2520performs%250Asignificantly%2520better%2520than%2520others.%2520We%2520also%2520build%2520automatic%2520evaluation%2520metrics%252C%250Aincluding%2520an%2520efficient%2520and%2520accurate%2520BERT%2520model%2520that%2520outperforms%2520finetuned%2520LLMs%252C%250Ainvestigate%2520how%2520to%2520successfully%2520tailor%2520vaccine%2520messaging%2520to%2520CGOs%252C%2520and%2520provide%250Aactionable%2520recommendations%2520from%2520this%2520investigation.%250A%2520%2520Code%2520and%2520model%2520weights%253A%2520https%253A//github.com/rickardstureborg/tailor-cgo%250ADataset%253A%2520https%253A//huggingface.co/datasets/DukeNLP/tailor-cgo%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2405.10861v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Tailoring%20Vaccine%20Messaging%20with%20Common-Ground%20Opinions&entry.906535625=Rickard%20Stureborg%20and%20Sanxing%20Chen%20and%20Ruoyu%20Xie%20and%20Aayushi%20Patel%20and%20Christopher%20Li%20and%20Chloe%20Qinyu%20Zhu%20and%20Tingnan%20Hu%20and%20Jun%20Yang%20and%20Bhuwan%20Dhingra&entry.1292438233=%20%20One%20way%20to%20personalize%20chatbot%20interactions%20is%20by%20establishing%20common%20ground%0Awith%20the%20intended%20reader.%20A%20domain%20where%20establishing%20mutual%20understanding%0Acould%20be%20particularly%20impactful%20is%20vaccine%20concerns%20and%20misinformation.%20Vaccine%0Ainterventions%20are%20forms%20of%20messaging%20which%20aim%20to%20answer%20concerns%20expressed%0Aabout%20vaccination.%20Tailoring%20responses%20in%20this%20domain%20is%20difficult%2C%20since%0Aopinions%20often%20have%20seemingly%20little%20ideological%20overlap.%20We%20define%20the%20task%20of%0Atailoring%20vaccine%20interventions%20to%20a%20Common-Ground%20Opinion%20%28CGO%29.%20Tailoring%0Aresponses%20to%20a%20CGO%20involves%20meaningfully%20improving%20the%20answer%20by%20relating%20it%20to%0Aan%20opinion%20or%20belief%20the%20reader%20holds.%20In%20this%20paper%20we%20introduce%20TAILOR-CGO%2C%20a%0Adataset%20for%20evaluating%20how%20well%20responses%20are%20tailored%20to%20provided%20CGOs.%20We%0Abenchmark%20several%20major%20LLMs%20on%20this%20task%3B%20finding%20GPT-4-Turbo%20performs%0Asignificantly%20better%20than%20others.%20We%20also%20build%20automatic%20evaluation%20metrics%2C%0Aincluding%20an%20efficient%20and%20accurate%20BERT%20model%20that%20outperforms%20finetuned%20LLMs%2C%0Ainvestigate%20how%20to%20successfully%20tailor%20vaccine%20messaging%20to%20CGOs%2C%20and%20provide%0Aactionable%20recommendations%20from%20this%20investigation.%0A%20%20Code%20and%20model%20weights%3A%20https%3A//github.com/rickardstureborg/tailor-cgo%0ADataset%3A%20https%3A//huggingface.co/datasets/DukeNLP/tailor-cgo%0A&entry.1838667208=http%3A//arxiv.org/abs/2405.10861v1&entry.124074799=Read"},
{"title": "Exploration and Anti-Exploration with Distributional Random Network\n  Distillation", "author": "Kai Yang and Jian Tao and Jiafei Lyu and Xiu Li", "abstract": "  Exploration remains a critical issue in deep reinforcement learning for an\nagent to attain high returns in unknown environments. Although the prevailing\nexploration Random Network Distillation (RND) algorithm has been demonstrated\nto be effective in numerous environments, it often needs more discriminative\npower in bonus allocation. This paper highlights the \"bonus inconsistency\"\nissue within RND, pinpointing its primary limitation. To address this issue, we\nintroduce the Distributional RND (DRND), a derivative of the RND. DRND enhances\nthe exploration process by distilling a distribution of random networks and\nimplicitly incorporating pseudo counts to improve the precision of bonus\nallocation. This refinement encourages agents to engage in more extensive\nexploration. Our method effectively mitigates the inconsistency issue without\nintroducing significant computational overhead. Both theoretical analysis and\nexperimental results demonstrate the superiority of our approach over the\noriginal RND algorithm. Our method excels in challenging online exploration\nscenarios and effectively serves as an anti-exploration mechanism in D4RL\noffline tasks. Our code is publicly available at\nhttps://github.com/yk7333/DRND.\n", "link": "http://arxiv.org/abs/2401.09750v3", "date": "2024-05-17", "relevancy": 1.9372, "topK": [{"title": "Boosted dynamic neural networks", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/26302", "similarity": 0.4969}, {"title": "RoboEXP: Action-Conditioned Scene Graph via Interactive Exploration for\n  Robotic Manipulation", "link": "http://arxiv.org/abs/2402.15487v1", "similarity": 0.4936}, {"title": "ToDo: Token Downsampling for Efficient Generation of High-Resolution\n  Images", "link": "http://arxiv.org/abs/2402.13573v2", "similarity": 0.468}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Exploration%20and%20Anti-Exploration%20with%20Distributional%20Random%20Network%0A%20%20Distillation&body=Title%3A%20Exploration%20and%20Anti-Exploration%20with%20Distributional%20Random%20Network%0A%20%20Distillation%0AAuthor%3A%20Kai%20Yang%20and%20Jian%20Tao%20and%20Jiafei%20Lyu%20and%20Xiu%20Li%0AAbstract%3A%20%20%20Exploration%20remains%20a%20critical%20issue%20in%20deep%20reinforcement%20learning%20for%20an%0Aagent%20to%20attain%20high%20returns%20in%20unknown%20environments.%20Although%20the%20prevailing%0Aexploration%20Random%20Network%20Distillation%20%28RND%29%20algorithm%20has%20been%20demonstrated%0Ato%20be%20effective%20in%20numerous%20environments%2C%20it%20often%20needs%20more%20discriminative%0Apower%20in%20bonus%20allocation.%20This%20paper%20highlights%20the%20%22bonus%20inconsistency%22%0Aissue%20within%20RND%2C%20pinpointing%20its%20primary%20limitation.%20To%20address%20this%20issue%2C%20we%0Aintroduce%20the%20Distributional%20RND%20%28DRND%29%2C%20a%20derivative%20of%20the%20RND.%20DRND%20enhances%0Athe%20exploration%20process%20by%20distilling%20a%20distribution%20of%20random%20networks%20and%0Aimplicitly%20incorporating%20pseudo%20counts%20to%20improve%20the%20precision%20of%20bonus%0Aallocation.%20This%20refinement%20encourages%20agents%20to%20engage%20in%20more%20extensive%0Aexploration.%20Our%20method%20effectively%20mitigates%20the%20inconsistency%20issue%20without%0Aintroducing%20significant%20computational%20overhead.%20Both%20theoretical%20analysis%20and%0Aexperimental%20results%20demonstrate%20the%20superiority%20of%20our%20approach%20over%20the%0Aoriginal%20RND%20algorithm.%20Our%20method%20excels%20in%20challenging%20online%20exploration%0Ascenarios%20and%20effectively%20serves%20as%20an%20anti-exploration%20mechanism%20in%20D4RL%0Aoffline%20tasks.%20Our%20code%20is%20publicly%20available%20at%0Ahttps%3A//github.com/yk7333/DRND.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2401.09750v3%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DExploration%2520and%2520Anti-Exploration%2520with%2520Distributional%2520Random%2520Network%250A%2520%2520Distillation%26entry.906535625%3DKai%2520Yang%2520and%2520Jian%2520Tao%2520and%2520Jiafei%2520Lyu%2520and%2520Xiu%2520Li%26entry.1292438233%3D%2520%2520Exploration%2520remains%2520a%2520critical%2520issue%2520in%2520deep%2520reinforcement%2520learning%2520for%2520an%250Aagent%2520to%2520attain%2520high%2520returns%2520in%2520unknown%2520environments.%2520Although%2520the%2520prevailing%250Aexploration%2520Random%2520Network%2520Distillation%2520%2528RND%2529%2520algorithm%2520has%2520been%2520demonstrated%250Ato%2520be%2520effective%2520in%2520numerous%2520environments%252C%2520it%2520often%2520needs%2520more%2520discriminative%250Apower%2520in%2520bonus%2520allocation.%2520This%2520paper%2520highlights%2520the%2520%2522bonus%2520inconsistency%2522%250Aissue%2520within%2520RND%252C%2520pinpointing%2520its%2520primary%2520limitation.%2520To%2520address%2520this%2520issue%252C%2520we%250Aintroduce%2520the%2520Distributional%2520RND%2520%2528DRND%2529%252C%2520a%2520derivative%2520of%2520the%2520RND.%2520DRND%2520enhances%250Athe%2520exploration%2520process%2520by%2520distilling%2520a%2520distribution%2520of%2520random%2520networks%2520and%250Aimplicitly%2520incorporating%2520pseudo%2520counts%2520to%2520improve%2520the%2520precision%2520of%2520bonus%250Aallocation.%2520This%2520refinement%2520encourages%2520agents%2520to%2520engage%2520in%2520more%2520extensive%250Aexploration.%2520Our%2520method%2520effectively%2520mitigates%2520the%2520inconsistency%2520issue%2520without%250Aintroducing%2520significant%2520computational%2520overhead.%2520Both%2520theoretical%2520analysis%2520and%250Aexperimental%2520results%2520demonstrate%2520the%2520superiority%2520of%2520our%2520approach%2520over%2520the%250Aoriginal%2520RND%2520algorithm.%2520Our%2520method%2520excels%2520in%2520challenging%2520online%2520exploration%250Ascenarios%2520and%2520effectively%2520serves%2520as%2520an%2520anti-exploration%2520mechanism%2520in%2520D4RL%250Aoffline%2520tasks.%2520Our%2520code%2520is%2520publicly%2520available%2520at%250Ahttps%253A//github.com/yk7333/DRND.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2401.09750v3%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Exploration%20and%20Anti-Exploration%20with%20Distributional%20Random%20Network%0A%20%20Distillation&entry.906535625=Kai%20Yang%20and%20Jian%20Tao%20and%20Jiafei%20Lyu%20and%20Xiu%20Li&entry.1292438233=%20%20Exploration%20remains%20a%20critical%20issue%20in%20deep%20reinforcement%20learning%20for%20an%0Aagent%20to%20attain%20high%20returns%20in%20unknown%20environments.%20Although%20the%20prevailing%0Aexploration%20Random%20Network%20Distillation%20%28RND%29%20algorithm%20has%20been%20demonstrated%0Ato%20be%20effective%20in%20numerous%20environments%2C%20it%20often%20needs%20more%20discriminative%0Apower%20in%20bonus%20allocation.%20This%20paper%20highlights%20the%20%22bonus%20inconsistency%22%0Aissue%20within%20RND%2C%20pinpointing%20its%20primary%20limitation.%20To%20address%20this%20issue%2C%20we%0Aintroduce%20the%20Distributional%20RND%20%28DRND%29%2C%20a%20derivative%20of%20the%20RND.%20DRND%20enhances%0Athe%20exploration%20process%20by%20distilling%20a%20distribution%20of%20random%20networks%20and%0Aimplicitly%20incorporating%20pseudo%20counts%20to%20improve%20the%20precision%20of%20bonus%0Aallocation.%20This%20refinement%20encourages%20agents%20to%20engage%20in%20more%20extensive%0Aexploration.%20Our%20method%20effectively%20mitigates%20the%20inconsistency%20issue%20without%0Aintroducing%20significant%20computational%20overhead.%20Both%20theoretical%20analysis%20and%0Aexperimental%20results%20demonstrate%20the%20superiority%20of%20our%20approach%20over%20the%0Aoriginal%20RND%20algorithm.%20Our%20method%20excels%20in%20challenging%20online%20exploration%0Ascenarios%20and%20effectively%20serves%20as%20an%20anti-exploration%20mechanism%20in%20D4RL%0Aoffline%20tasks.%20Our%20code%20is%20publicly%20available%20at%0Ahttps%3A//github.com/yk7333/DRND.%0A&entry.1838667208=http%3A//arxiv.org/abs/2401.09750v3&entry.124074799=Read"},
{"title": "Research on Splicing Image Detection Algorithms Based on Natural Image\n  Statistical Characteristics", "author": "Ao Xiang and Jingyu Zhang and Qin Yang and Liyang Wang and Yu Cheng", "abstract": "  With the development and widespread application of digital image processing\ntechnology, image splicing has become a common method of image manipulation,\nraising numerous security and legal issues. This paper introduces a new\nsplicing image detection algorithm based on the statistical characteristics of\nnatural images, aimed at improving the accuracy and efficiency of splicing\nimage detection. By analyzing the limitations of traditional methods, we have\ndeveloped a detection framework that integrates advanced statistical analysis\ntechniques and machine learning methods. The algorithm has been validated using\nmultiple public datasets, showing high accuracy in detecting spliced edges and\nlocating tampered areas, as well as good robustness. Additionally, we explore\nthe potential applications and challenges faced by the algorithm in real-world\nscenarios. This research not only provides an effective technological means for\nthe field of image tampering detection but also offers new ideas and methods\nfor future related research.\n", "link": "http://arxiv.org/abs/2404.16296v3", "date": "2024-05-17", "relevancy": 1.8935, "topK": [{"title": "Tuning-Free Noise Rectification for High Fidelity Image-to-Video\n  Generation", "link": "http://arxiv.org/abs/2403.02827v1", "similarity": 0.4883}, {"title": "Multimodal-Conditioned Latent Diffusion Models for Fashion Image Editing", "link": "http://arxiv.org/abs/2403.14828v2", "similarity": 0.4689}, {"title": "RTG-SLAM: Real-time 3D Reconstruction at Scale using Gaussian Splatting", "link": "http://arxiv.org/abs/2404.19706v1", "similarity": 0.4603}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Research%20on%20Splicing%20Image%20Detection%20Algorithms%20Based%20on%20Natural%20Image%0A%20%20Statistical%20Characteristics&body=Title%3A%20Research%20on%20Splicing%20Image%20Detection%20Algorithms%20Based%20on%20Natural%20Image%0A%20%20Statistical%20Characteristics%0AAuthor%3A%20Ao%20Xiang%20and%20Jingyu%20Zhang%20and%20Qin%20Yang%20and%20Liyang%20Wang%20and%20Yu%20Cheng%0AAbstract%3A%20%20%20With%20the%20development%20and%20widespread%20application%20of%20digital%20image%20processing%0Atechnology%2C%20image%20splicing%20has%20become%20a%20common%20method%20of%20image%20manipulation%2C%0Araising%20numerous%20security%20and%20legal%20issues.%20This%20paper%20introduces%20a%20new%0Asplicing%20image%20detection%20algorithm%20based%20on%20the%20statistical%20characteristics%20of%0Anatural%20images%2C%20aimed%20at%20improving%20the%20accuracy%20and%20efficiency%20of%20splicing%0Aimage%20detection.%20By%20analyzing%20the%20limitations%20of%20traditional%20methods%2C%20we%20have%0Adeveloped%20a%20detection%20framework%20that%20integrates%20advanced%20statistical%20analysis%0Atechniques%20and%20machine%20learning%20methods.%20The%20algorithm%20has%20been%20validated%20using%0Amultiple%20public%20datasets%2C%20showing%20high%20accuracy%20in%20detecting%20spliced%20edges%20and%0Alocating%20tampered%20areas%2C%20as%20well%20as%20good%20robustness.%20Additionally%2C%20we%20explore%0Athe%20potential%20applications%20and%20challenges%20faced%20by%20the%20algorithm%20in%20real-world%0Ascenarios.%20This%20research%20not%20only%20provides%20an%20effective%20technological%20means%20for%0Athe%20field%20of%20image%20tampering%20detection%20but%20also%20offers%20new%20ideas%20and%20methods%0Afor%20future%20related%20research.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2404.16296v3%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DResearch%2520on%2520Splicing%2520Image%2520Detection%2520Algorithms%2520Based%2520on%2520Natural%2520Image%250A%2520%2520Statistical%2520Characteristics%26entry.906535625%3DAo%2520Xiang%2520and%2520Jingyu%2520Zhang%2520and%2520Qin%2520Yang%2520and%2520Liyang%2520Wang%2520and%2520Yu%2520Cheng%26entry.1292438233%3D%2520%2520With%2520the%2520development%2520and%2520widespread%2520application%2520of%2520digital%2520image%2520processing%250Atechnology%252C%2520image%2520splicing%2520has%2520become%2520a%2520common%2520method%2520of%2520image%2520manipulation%252C%250Araising%2520numerous%2520security%2520and%2520legal%2520issues.%2520This%2520paper%2520introduces%2520a%2520new%250Asplicing%2520image%2520detection%2520algorithm%2520based%2520on%2520the%2520statistical%2520characteristics%2520of%250Anatural%2520images%252C%2520aimed%2520at%2520improving%2520the%2520accuracy%2520and%2520efficiency%2520of%2520splicing%250Aimage%2520detection.%2520By%2520analyzing%2520the%2520limitations%2520of%2520traditional%2520methods%252C%2520we%2520have%250Adeveloped%2520a%2520detection%2520framework%2520that%2520integrates%2520advanced%2520statistical%2520analysis%250Atechniques%2520and%2520machine%2520learning%2520methods.%2520The%2520algorithm%2520has%2520been%2520validated%2520using%250Amultiple%2520public%2520datasets%252C%2520showing%2520high%2520accuracy%2520in%2520detecting%2520spliced%2520edges%2520and%250Alocating%2520tampered%2520areas%252C%2520as%2520well%2520as%2520good%2520robustness.%2520Additionally%252C%2520we%2520explore%250Athe%2520potential%2520applications%2520and%2520challenges%2520faced%2520by%2520the%2520algorithm%2520in%2520real-world%250Ascenarios.%2520This%2520research%2520not%2520only%2520provides%2520an%2520effective%2520technological%2520means%2520for%250Athe%2520field%2520of%2520image%2520tampering%2520detection%2520but%2520also%2520offers%2520new%2520ideas%2520and%2520methods%250Afor%2520future%2520related%2520research.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2404.16296v3%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Research%20on%20Splicing%20Image%20Detection%20Algorithms%20Based%20on%20Natural%20Image%0A%20%20Statistical%20Characteristics&entry.906535625=Ao%20Xiang%20and%20Jingyu%20Zhang%20and%20Qin%20Yang%20and%20Liyang%20Wang%20and%20Yu%20Cheng&entry.1292438233=%20%20With%20the%20development%20and%20widespread%20application%20of%20digital%20image%20processing%0Atechnology%2C%20image%20splicing%20has%20become%20a%20common%20method%20of%20image%20manipulation%2C%0Araising%20numerous%20security%20and%20legal%20issues.%20This%20paper%20introduces%20a%20new%0Asplicing%20image%20detection%20algorithm%20based%20on%20the%20statistical%20characteristics%20of%0Anatural%20images%2C%20aimed%20at%20improving%20the%20accuracy%20and%20efficiency%20of%20splicing%0Aimage%20detection.%20By%20analyzing%20the%20limitations%20of%20traditional%20methods%2C%20we%20have%0Adeveloped%20a%20detection%20framework%20that%20integrates%20advanced%20statistical%20analysis%0Atechniques%20and%20machine%20learning%20methods.%20The%20algorithm%20has%20been%20validated%20using%0Amultiple%20public%20datasets%2C%20showing%20high%20accuracy%20in%20detecting%20spliced%20edges%20and%0Alocating%20tampered%20areas%2C%20as%20well%20as%20good%20robustness.%20Additionally%2C%20we%20explore%0Athe%20potential%20applications%20and%20challenges%20faced%20by%20the%20algorithm%20in%20real-world%0Ascenarios.%20This%20research%20not%20only%20provides%20an%20effective%20technological%20means%20for%0Athe%20field%20of%20image%20tampering%20detection%20but%20also%20offers%20new%20ideas%20and%20methods%0Afor%20future%20related%20research.%0A&entry.1838667208=http%3A//arxiv.org/abs/2404.16296v3&entry.124074799=Read"},
{"title": "Forecasting with Hyper-Trees", "author": "Alexander M\u00e4rz and Kashif Rasul", "abstract": "  This paper introduces the concept of Hyper-Trees and offers a new direction\nin applying tree-based models to time series data. Unlike conventional\napplications of decision trees that forecast time series directly, Hyper-Trees\nare designed to learn the parameters of a target time series model. Our\nframework leverages the gradient-based nature of boosted trees, which allows us\nto extend the concept of Hyper-Networks to Hyper-Trees and to induce a\ntime-series inductive bias to tree models. By relating the parameters of a\ntarget time series model to features, Hyper-Trees address the issue of\nparameter non-stationarity and enable tree-based forecasts to extend beyond\ntheir training range. With our research, we aim to explore the effectiveness of\nHyper-Trees across various forecasting scenarios and to extend the application\nof gradient boosted decision trees outside their conventional use in time\nseries modeling.\n", "link": "http://arxiv.org/abs/2405.07836v2", "date": "2024-05-17", "relevancy": 1.8915, "topK": [{"title": "Boosted dynamic neural networks", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/26302", "similarity": 0.5353}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.4442}, {"title": "Weakly-guided self-supervised pretraining for temporal activity detection", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/25189", "similarity": 0.4219}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Forecasting%20with%20Hyper-Trees&body=Title%3A%20Forecasting%20with%20Hyper-Trees%0AAuthor%3A%20Alexander%20M%C3%A4rz%20and%20Kashif%20Rasul%0AAbstract%3A%20%20%20This%20paper%20introduces%20the%20concept%20of%20Hyper-Trees%20and%20offers%20a%20new%20direction%0Ain%20applying%20tree-based%20models%20to%20time%20series%20data.%20Unlike%20conventional%0Aapplications%20of%20decision%20trees%20that%20forecast%20time%20series%20directly%2C%20Hyper-Trees%0Aare%20designed%20to%20learn%20the%20parameters%20of%20a%20target%20time%20series%20model.%20Our%0Aframework%20leverages%20the%20gradient-based%20nature%20of%20boosted%20trees%2C%20which%20allows%20us%0Ato%20extend%20the%20concept%20of%20Hyper-Networks%20to%20Hyper-Trees%20and%20to%20induce%20a%0Atime-series%20inductive%20bias%20to%20tree%20models.%20By%20relating%20the%20parameters%20of%20a%0Atarget%20time%20series%20model%20to%20features%2C%20Hyper-Trees%20address%20the%20issue%20of%0Aparameter%20non-stationarity%20and%20enable%20tree-based%20forecasts%20to%20extend%20beyond%0Atheir%20training%20range.%20With%20our%20research%2C%20we%20aim%20to%20explore%20the%20effectiveness%20of%0AHyper-Trees%20across%20various%20forecasting%20scenarios%20and%20to%20extend%20the%20application%0Aof%20gradient%20boosted%20decision%20trees%20outside%20their%20conventional%20use%20in%20time%0Aseries%20modeling.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2405.07836v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DForecasting%2520with%2520Hyper-Trees%26entry.906535625%3DAlexander%2520M%25C3%25A4rz%2520and%2520Kashif%2520Rasul%26entry.1292438233%3D%2520%2520This%2520paper%2520introduces%2520the%2520concept%2520of%2520Hyper-Trees%2520and%2520offers%2520a%2520new%2520direction%250Ain%2520applying%2520tree-based%2520models%2520to%2520time%2520series%2520data.%2520Unlike%2520conventional%250Aapplications%2520of%2520decision%2520trees%2520that%2520forecast%2520time%2520series%2520directly%252C%2520Hyper-Trees%250Aare%2520designed%2520to%2520learn%2520the%2520parameters%2520of%2520a%2520target%2520time%2520series%2520model.%2520Our%250Aframework%2520leverages%2520the%2520gradient-based%2520nature%2520of%2520boosted%2520trees%252C%2520which%2520allows%2520us%250Ato%2520extend%2520the%2520concept%2520of%2520Hyper-Networks%2520to%2520Hyper-Trees%2520and%2520to%2520induce%2520a%250Atime-series%2520inductive%2520bias%2520to%2520tree%2520models.%2520By%2520relating%2520the%2520parameters%2520of%2520a%250Atarget%2520time%2520series%2520model%2520to%2520features%252C%2520Hyper-Trees%2520address%2520the%2520issue%2520of%250Aparameter%2520non-stationarity%2520and%2520enable%2520tree-based%2520forecasts%2520to%2520extend%2520beyond%250Atheir%2520training%2520range.%2520With%2520our%2520research%252C%2520we%2520aim%2520to%2520explore%2520the%2520effectiveness%2520of%250AHyper-Trees%2520across%2520various%2520forecasting%2520scenarios%2520and%2520to%2520extend%2520the%2520application%250Aof%2520gradient%2520boosted%2520decision%2520trees%2520outside%2520their%2520conventional%2520use%2520in%2520time%250Aseries%2520modeling.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2405.07836v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Forecasting%20with%20Hyper-Trees&entry.906535625=Alexander%20M%C3%A4rz%20and%20Kashif%20Rasul&entry.1292438233=%20%20This%20paper%20introduces%20the%20concept%20of%20Hyper-Trees%20and%20offers%20a%20new%20direction%0Ain%20applying%20tree-based%20models%20to%20time%20series%20data.%20Unlike%20conventional%0Aapplications%20of%20decision%20trees%20that%20forecast%20time%20series%20directly%2C%20Hyper-Trees%0Aare%20designed%20to%20learn%20the%20parameters%20of%20a%20target%20time%20series%20model.%20Our%0Aframework%20leverages%20the%20gradient-based%20nature%20of%20boosted%20trees%2C%20which%20allows%20us%0Ato%20extend%20the%20concept%20of%20Hyper-Networks%20to%20Hyper-Trees%20and%20to%20induce%20a%0Atime-series%20inductive%20bias%20to%20tree%20models.%20By%20relating%20the%20parameters%20of%20a%0Atarget%20time%20series%20model%20to%20features%2C%20Hyper-Trees%20address%20the%20issue%20of%0Aparameter%20non-stationarity%20and%20enable%20tree-based%20forecasts%20to%20extend%20beyond%0Atheir%20training%20range.%20With%20our%20research%2C%20we%20aim%20to%20explore%20the%20effectiveness%20of%0AHyper-Trees%20across%20various%20forecasting%20scenarios%20and%20to%20extend%20the%20application%0Aof%20gradient%20boosted%20decision%20trees%20outside%20their%20conventional%20use%20in%20time%0Aseries%20modeling.%0A&entry.1838667208=http%3A//arxiv.org/abs/2405.07836v2&entry.124074799=Read"},
{"title": "Efficient Learning of Accurate Surrogates for Simulations of Complex\n  Systems", "author": "A. Diaw and M. McKerns and I. Sagert and L. G. Stanton and M. S. Murillo", "abstract": "  Machine learning methods are increasingly used to build computationally\ninexpensive surrogates for complex physical models. The predictive capability\nof these surrogates suffers when data are noisy, sparse, or time-dependent. As\nwe are interested in finding a surrogate that provides valid predictions of any\npotential future model evaluations, we introduce an online learning method\nempowered by optimizer-driven sampling. The method has two advantages over\ncurrent approaches. First, it ensures that all turning points on the model\nresponse surface are included in the training data. Second, after any new model\nevaluations, surrogates are tested and \"retrained\" (updated) if the \"score\"\ndrops below a validity threshold. Tests on benchmark functions reveal that\noptimizer-directed sampling generally outperforms traditional sampling methods\nin terms of accuracy around local extrema, even when the scoring metric favors\noverall accuracy. We apply our method to simulations of nuclear matter to\ndemonstrate that highly accurate surrogates for the nuclear equation of state\ncan be reliably auto-generated from expensive calculations using a few model\nevaluations.\n", "link": "http://arxiv.org/abs/2207.12855v3", "date": "2024-05-17", "relevancy": 1.8827, "topK": [{"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.5079}, {"title": "Safe and efficient training of a control agent", "link": "https://patents.google.com/patent/US11709462B2/en", "similarity": 0.4679}, {"title": "ToDo: Token Downsampling for Efficient Generation of High-Resolution\n  Images", "link": "http://arxiv.org/abs/2402.13573v2", "similarity": 0.4585}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Efficient%20Learning%20of%20Accurate%20Surrogates%20for%20Simulations%20of%20Complex%0A%20%20Systems&body=Title%3A%20Efficient%20Learning%20of%20Accurate%20Surrogates%20for%20Simulations%20of%20Complex%0A%20%20Systems%0AAuthor%3A%20A.%20Diaw%20and%20M.%20McKerns%20and%20I.%20Sagert%20and%20L.%20G.%20Stanton%20and%20M.%20S.%20Murillo%0AAbstract%3A%20%20%20Machine%20learning%20methods%20are%20increasingly%20used%20to%20build%20computationally%0Ainexpensive%20surrogates%20for%20complex%20physical%20models.%20The%20predictive%20capability%0Aof%20these%20surrogates%20suffers%20when%20data%20are%20noisy%2C%20sparse%2C%20or%20time-dependent.%20As%0Awe%20are%20interested%20in%20finding%20a%20surrogate%20that%20provides%20valid%20predictions%20of%20any%0Apotential%20future%20model%20evaluations%2C%20we%20introduce%20an%20online%20learning%20method%0Aempowered%20by%20optimizer-driven%20sampling.%20The%20method%20has%20two%20advantages%20over%0Acurrent%20approaches.%20First%2C%20it%20ensures%20that%20all%20turning%20points%20on%20the%20model%0Aresponse%20surface%20are%20included%20in%20the%20training%20data.%20Second%2C%20after%20any%20new%20model%0Aevaluations%2C%20surrogates%20are%20tested%20and%20%22retrained%22%20%28updated%29%20if%20the%20%22score%22%0Adrops%20below%20a%20validity%20threshold.%20Tests%20on%20benchmark%20functions%20reveal%20that%0Aoptimizer-directed%20sampling%20generally%20outperforms%20traditional%20sampling%20methods%0Ain%20terms%20of%20accuracy%20around%20local%20extrema%2C%20even%20when%20the%20scoring%20metric%20favors%0Aoverall%20accuracy.%20We%20apply%20our%20method%20to%20simulations%20of%20nuclear%20matter%20to%0Ademonstrate%20that%20highly%20accurate%20surrogates%20for%20the%20nuclear%20equation%20of%20state%0Acan%20be%20reliably%20auto-generated%20from%20expensive%20calculations%20using%20a%20few%20model%0Aevaluations.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2207.12855v3%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DEfficient%2520Learning%2520of%2520Accurate%2520Surrogates%2520for%2520Simulations%2520of%2520Complex%250A%2520%2520Systems%26entry.906535625%3DA.%2520Diaw%2520and%2520M.%2520McKerns%2520and%2520I.%2520Sagert%2520and%2520L.%2520G.%2520Stanton%2520and%2520M.%2520S.%2520Murillo%26entry.1292438233%3D%2520%2520Machine%2520learning%2520methods%2520are%2520increasingly%2520used%2520to%2520build%2520computationally%250Ainexpensive%2520surrogates%2520for%2520complex%2520physical%2520models.%2520The%2520predictive%2520capability%250Aof%2520these%2520surrogates%2520suffers%2520when%2520data%2520are%2520noisy%252C%2520sparse%252C%2520or%2520time-dependent.%2520As%250Awe%2520are%2520interested%2520in%2520finding%2520a%2520surrogate%2520that%2520provides%2520valid%2520predictions%2520of%2520any%250Apotential%2520future%2520model%2520evaluations%252C%2520we%2520introduce%2520an%2520online%2520learning%2520method%250Aempowered%2520by%2520optimizer-driven%2520sampling.%2520The%2520method%2520has%2520two%2520advantages%2520over%250Acurrent%2520approaches.%2520First%252C%2520it%2520ensures%2520that%2520all%2520turning%2520points%2520on%2520the%2520model%250Aresponse%2520surface%2520are%2520included%2520in%2520the%2520training%2520data.%2520Second%252C%2520after%2520any%2520new%2520model%250Aevaluations%252C%2520surrogates%2520are%2520tested%2520and%2520%2522retrained%2522%2520%2528updated%2529%2520if%2520the%2520%2522score%2522%250Adrops%2520below%2520a%2520validity%2520threshold.%2520Tests%2520on%2520benchmark%2520functions%2520reveal%2520that%250Aoptimizer-directed%2520sampling%2520generally%2520outperforms%2520traditional%2520sampling%2520methods%250Ain%2520terms%2520of%2520accuracy%2520around%2520local%2520extrema%252C%2520even%2520when%2520the%2520scoring%2520metric%2520favors%250Aoverall%2520accuracy.%2520We%2520apply%2520our%2520method%2520to%2520simulations%2520of%2520nuclear%2520matter%2520to%250Ademonstrate%2520that%2520highly%2520accurate%2520surrogates%2520for%2520the%2520nuclear%2520equation%2520of%2520state%250Acan%2520be%2520reliably%2520auto-generated%2520from%2520expensive%2520calculations%2520using%2520a%2520few%2520model%250Aevaluations.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2207.12855v3%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Efficient%20Learning%20of%20Accurate%20Surrogates%20for%20Simulations%20of%20Complex%0A%20%20Systems&entry.906535625=A.%20Diaw%20and%20M.%20McKerns%20and%20I.%20Sagert%20and%20L.%20G.%20Stanton%20and%20M.%20S.%20Murillo&entry.1292438233=%20%20Machine%20learning%20methods%20are%20increasingly%20used%20to%20build%20computationally%0Ainexpensive%20surrogates%20for%20complex%20physical%20models.%20The%20predictive%20capability%0Aof%20these%20surrogates%20suffers%20when%20data%20are%20noisy%2C%20sparse%2C%20or%20time-dependent.%20As%0Awe%20are%20interested%20in%20finding%20a%20surrogate%20that%20provides%20valid%20predictions%20of%20any%0Apotential%20future%20model%20evaluations%2C%20we%20introduce%20an%20online%20learning%20method%0Aempowered%20by%20optimizer-driven%20sampling.%20The%20method%20has%20two%20advantages%20over%0Acurrent%20approaches.%20First%2C%20it%20ensures%20that%20all%20turning%20points%20on%20the%20model%0Aresponse%20surface%20are%20included%20in%20the%20training%20data.%20Second%2C%20after%20any%20new%20model%0Aevaluations%2C%20surrogates%20are%20tested%20and%20%22retrained%22%20%28updated%29%20if%20the%20%22score%22%0Adrops%20below%20a%20validity%20threshold.%20Tests%20on%20benchmark%20functions%20reveal%20that%0Aoptimizer-directed%20sampling%20generally%20outperforms%20traditional%20sampling%20methods%0Ain%20terms%20of%20accuracy%20around%20local%20extrema%2C%20even%20when%20the%20scoring%20metric%20favors%0Aoverall%20accuracy.%20We%20apply%20our%20method%20to%20simulations%20of%20nuclear%20matter%20to%0Ademonstrate%20that%20highly%20accurate%20surrogates%20for%20the%20nuclear%20equation%20of%20state%0Acan%20be%20reliably%20auto-generated%20from%20expensive%20calculations%20using%20a%20few%20model%0Aevaluations.%0A&entry.1838667208=http%3A//arxiv.org/abs/2207.12855v3&entry.124074799=Read"},
{"title": "SynDy: Synthetic Dynamic Dataset Generation Framework for Misinformation\n  Tasks", "author": "Michael Shliselberg and Ashkan Kazemi and Scott A. Hale and Shiri Dori-Hacohen", "abstract": "  Diaspora communities are disproportionately impacted by off-the-radar\nmisinformation and often neglected by mainstream fact-checking efforts,\ncreating a critical need to scale-up efforts of nascent fact-checking\ninitiatives. In this paper we present SynDy, a framework for Synthetic Dynamic\nDataset Generation to leverage the capabilities of the largest frontier Large\nLanguage Models (LLMs) to train local, specialized language models. To the best\nof our knowledge, SynDy is the first paper utilizing LLMs to create\nfine-grained synthetic labels for tasks of direct relevance to misinformation\nmitigation, namely Claim Matching, Topical Clustering, and Claim Relationship\nClassification. SynDy utilizes LLMs and social media queries to automatically\ngenerate distantly-supervised, topically-focused datasets with synthetic labels\non these three tasks, providing essential tools to scale up human-led\nfact-checking at a fraction of the cost of human-annotated data. Training on\nSynDy's generated labels shows improvement over a standard baseline and is not\nsignificantly worse compared to training on human labels (which may be\ninfeasible to acquire). SynDy is being integrated into Meedan's chatbot\ntiplines that are used by over 50 organizations, serve over 230K users\nannually, and automatically distribute human-written fact-checks via messaging\napps such as WhatsApp. SynDy will also be integrated into our deployed\nCo-Insights toolkit, enabling low-resource organizations to launch tiplines for\ntheir communities. Finally, we envision SynDy enabling additional fact-checking\ntools such as matching new misinformation claims to high-quality explainers on\ncommon misinformation topics.\n", "link": "http://arxiv.org/abs/2405.10700v1", "date": "2024-05-17", "relevancy": 1.8739, "topK": [{"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.5326}, {"title": "Magic-Me: Identity-Specific Video Customized Diffusion", "link": "http://arxiv.org/abs/2402.09368v1", "similarity": 0.4567}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.4546}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20SynDy%3A%20Synthetic%20Dynamic%20Dataset%20Generation%20Framework%20for%20Misinformation%0A%20%20Tasks&body=Title%3A%20SynDy%3A%20Synthetic%20Dynamic%20Dataset%20Generation%20Framework%20for%20Misinformation%0A%20%20Tasks%0AAuthor%3A%20Michael%20Shliselberg%20and%20Ashkan%20Kazemi%20and%20Scott%20A.%20Hale%20and%20Shiri%20Dori-Hacohen%0AAbstract%3A%20%20%20Diaspora%20communities%20are%20disproportionately%20impacted%20by%20off-the-radar%0Amisinformation%20and%20often%20neglected%20by%20mainstream%20fact-checking%20efforts%2C%0Acreating%20a%20critical%20need%20to%20scale-up%20efforts%20of%20nascent%20fact-checking%0Ainitiatives.%20In%20this%20paper%20we%20present%20SynDy%2C%20a%20framework%20for%20Synthetic%20Dynamic%0ADataset%20Generation%20to%20leverage%20the%20capabilities%20of%20the%20largest%20frontier%20Large%0ALanguage%20Models%20%28LLMs%29%20to%20train%20local%2C%20specialized%20language%20models.%20To%20the%20best%0Aof%20our%20knowledge%2C%20SynDy%20is%20the%20first%20paper%20utilizing%20LLMs%20to%20create%0Afine-grained%20synthetic%20labels%20for%20tasks%20of%20direct%20relevance%20to%20misinformation%0Amitigation%2C%20namely%20Claim%20Matching%2C%20Topical%20Clustering%2C%20and%20Claim%20Relationship%0AClassification.%20SynDy%20utilizes%20LLMs%20and%20social%20media%20queries%20to%20automatically%0Agenerate%20distantly-supervised%2C%20topically-focused%20datasets%20with%20synthetic%20labels%0Aon%20these%20three%20tasks%2C%20providing%20essential%20tools%20to%20scale%20up%20human-led%0Afact-checking%20at%20a%20fraction%20of%20the%20cost%20of%20human-annotated%20data.%20Training%20on%0ASynDy%27s%20generated%20labels%20shows%20improvement%20over%20a%20standard%20baseline%20and%20is%20not%0Asignificantly%20worse%20compared%20to%20training%20on%20human%20labels%20%28which%20may%20be%0Ainfeasible%20to%20acquire%29.%20SynDy%20is%20being%20integrated%20into%20Meedan%27s%20chatbot%0Atiplines%20that%20are%20used%20by%20over%2050%20organizations%2C%20serve%20over%20230K%20users%0Aannually%2C%20and%20automatically%20distribute%20human-written%20fact-checks%20via%20messaging%0Aapps%20such%20as%20WhatsApp.%20SynDy%20will%20also%20be%20integrated%20into%20our%20deployed%0ACo-Insights%20toolkit%2C%20enabling%20low-resource%20organizations%20to%20launch%20tiplines%20for%0Atheir%20communities.%20Finally%2C%20we%20envision%20SynDy%20enabling%20additional%20fact-checking%0Atools%20such%20as%20matching%20new%20misinformation%20claims%20to%20high-quality%20explainers%20on%0Acommon%20misinformation%20topics.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2405.10700v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DSynDy%253A%2520Synthetic%2520Dynamic%2520Dataset%2520Generation%2520Framework%2520for%2520Misinformation%250A%2520%2520Tasks%26entry.906535625%3DMichael%2520Shliselberg%2520and%2520Ashkan%2520Kazemi%2520and%2520Scott%2520A.%2520Hale%2520and%2520Shiri%2520Dori-Hacohen%26entry.1292438233%3D%2520%2520Diaspora%2520communities%2520are%2520disproportionately%2520impacted%2520by%2520off-the-radar%250Amisinformation%2520and%2520often%2520neglected%2520by%2520mainstream%2520fact-checking%2520efforts%252C%250Acreating%2520a%2520critical%2520need%2520to%2520scale-up%2520efforts%2520of%2520nascent%2520fact-checking%250Ainitiatives.%2520In%2520this%2520paper%2520we%2520present%2520SynDy%252C%2520a%2520framework%2520for%2520Synthetic%2520Dynamic%250ADataset%2520Generation%2520to%2520leverage%2520the%2520capabilities%2520of%2520the%2520largest%2520frontier%2520Large%250ALanguage%2520Models%2520%2528LLMs%2529%2520to%2520train%2520local%252C%2520specialized%2520language%2520models.%2520To%2520the%2520best%250Aof%2520our%2520knowledge%252C%2520SynDy%2520is%2520the%2520first%2520paper%2520utilizing%2520LLMs%2520to%2520create%250Afine-grained%2520synthetic%2520labels%2520for%2520tasks%2520of%2520direct%2520relevance%2520to%2520misinformation%250Amitigation%252C%2520namely%2520Claim%2520Matching%252C%2520Topical%2520Clustering%252C%2520and%2520Claim%2520Relationship%250AClassification.%2520SynDy%2520utilizes%2520LLMs%2520and%2520social%2520media%2520queries%2520to%2520automatically%250Agenerate%2520distantly-supervised%252C%2520topically-focused%2520datasets%2520with%2520synthetic%2520labels%250Aon%2520these%2520three%2520tasks%252C%2520providing%2520essential%2520tools%2520to%2520scale%2520up%2520human-led%250Afact-checking%2520at%2520a%2520fraction%2520of%2520the%2520cost%2520of%2520human-annotated%2520data.%2520Training%2520on%250ASynDy%2527s%2520generated%2520labels%2520shows%2520improvement%2520over%2520a%2520standard%2520baseline%2520and%2520is%2520not%250Asignificantly%2520worse%2520compared%2520to%2520training%2520on%2520human%2520labels%2520%2528which%2520may%2520be%250Ainfeasible%2520to%2520acquire%2529.%2520SynDy%2520is%2520being%2520integrated%2520into%2520Meedan%2527s%2520chatbot%250Atiplines%2520that%2520are%2520used%2520by%2520over%252050%2520organizations%252C%2520serve%2520over%2520230K%2520users%250Aannually%252C%2520and%2520automatically%2520distribute%2520human-written%2520fact-checks%2520via%2520messaging%250Aapps%2520such%2520as%2520WhatsApp.%2520SynDy%2520will%2520also%2520be%2520integrated%2520into%2520our%2520deployed%250ACo-Insights%2520toolkit%252C%2520enabling%2520low-resource%2520organizations%2520to%2520launch%2520tiplines%2520for%250Atheir%2520communities.%2520Finally%252C%2520we%2520envision%2520SynDy%2520enabling%2520additional%2520fact-checking%250Atools%2520such%2520as%2520matching%2520new%2520misinformation%2520claims%2520to%2520high-quality%2520explainers%2520on%250Acommon%2520misinformation%2520topics.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2405.10700v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=SynDy%3A%20Synthetic%20Dynamic%20Dataset%20Generation%20Framework%20for%20Misinformation%0A%20%20Tasks&entry.906535625=Michael%20Shliselberg%20and%20Ashkan%20Kazemi%20and%20Scott%20A.%20Hale%20and%20Shiri%20Dori-Hacohen&entry.1292438233=%20%20Diaspora%20communities%20are%20disproportionately%20impacted%20by%20off-the-radar%0Amisinformation%20and%20often%20neglected%20by%20mainstream%20fact-checking%20efforts%2C%0Acreating%20a%20critical%20need%20to%20scale-up%20efforts%20of%20nascent%20fact-checking%0Ainitiatives.%20In%20this%20paper%20we%20present%20SynDy%2C%20a%20framework%20for%20Synthetic%20Dynamic%0ADataset%20Generation%20to%20leverage%20the%20capabilities%20of%20the%20largest%20frontier%20Large%0ALanguage%20Models%20%28LLMs%29%20to%20train%20local%2C%20specialized%20language%20models.%20To%20the%20best%0Aof%20our%20knowledge%2C%20SynDy%20is%20the%20first%20paper%20utilizing%20LLMs%20to%20create%0Afine-grained%20synthetic%20labels%20for%20tasks%20of%20direct%20relevance%20to%20misinformation%0Amitigation%2C%20namely%20Claim%20Matching%2C%20Topical%20Clustering%2C%20and%20Claim%20Relationship%0AClassification.%20SynDy%20utilizes%20LLMs%20and%20social%20media%20queries%20to%20automatically%0Agenerate%20distantly-supervised%2C%20topically-focused%20datasets%20with%20synthetic%20labels%0Aon%20these%20three%20tasks%2C%20providing%20essential%20tools%20to%20scale%20up%20human-led%0Afact-checking%20at%20a%20fraction%20of%20the%20cost%20of%20human-annotated%20data.%20Training%20on%0ASynDy%27s%20generated%20labels%20shows%20improvement%20over%20a%20standard%20baseline%20and%20is%20not%0Asignificantly%20worse%20compared%20to%20training%20on%20human%20labels%20%28which%20may%20be%0Ainfeasible%20to%20acquire%29.%20SynDy%20is%20being%20integrated%20into%20Meedan%27s%20chatbot%0Atiplines%20that%20are%20used%20by%20over%2050%20organizations%2C%20serve%20over%20230K%20users%0Aannually%2C%20and%20automatically%20distribute%20human-written%20fact-checks%20via%20messaging%0Aapps%20such%20as%20WhatsApp.%20SynDy%20will%20also%20be%20integrated%20into%20our%20deployed%0ACo-Insights%20toolkit%2C%20enabling%20low-resource%20organizations%20to%20launch%20tiplines%20for%0Atheir%20communities.%20Finally%2C%20we%20envision%20SynDy%20enabling%20additional%20fact-checking%0Atools%20such%20as%20matching%20new%20misinformation%20claims%20to%20high-quality%20explainers%20on%0Acommon%20misinformation%20topics.%0A&entry.1838667208=http%3A//arxiv.org/abs/2405.10700v1&entry.124074799=Read"},
{"title": "SignLLM: Sign Languages Production Large Language Models", "author": "Sen Fang and Lei Wang and Ce Zheng and Yapeng Tian and Chen Chen", "abstract": "  In this paper, we introduce the first comprehensive multilingual sign\nlanguage dataset named Prompt2Sign, which builds from public data including\nAmerican Sign Language (ASL) and seven others. Our dataset transforms a vast\narray of videos into a streamlined, model-friendly format, optimized for\ntraining with translation models like seq2seq and text2text. Building on this\nnew dataset, we propose SignLLM, the first multilingual Sign Language\nProduction (SLP) model, which includes two novel multilingual SLP modes that\nallow for the generation of sign language gestures from input text or prompt.\nBoth of the modes can use a new loss and a module based on reinforcement\nlearning, which accelerates the training by enhancing the model's capability to\nautonomously sample high-quality data. We present benchmark results of SignLLM,\nwhich demonstrate that our model achieves state-of-the-art performance on SLP\ntasks across eight sign languages.\n", "link": "http://arxiv.org/abs/2405.10718v1", "date": "2024-05-17", "relevancy": 1.872, "topK": [{"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.498}, {"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.4542}, {"title": "Self-supervised Photographic Image Layout Representation Learning", "link": "http://arxiv.org/abs/2403.03740v1", "similarity": 0.4435}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20SignLLM%3A%20Sign%20Languages%20Production%20Large%20Language%20Models&body=Title%3A%20SignLLM%3A%20Sign%20Languages%20Production%20Large%20Language%20Models%0AAuthor%3A%20Sen%20Fang%20and%20Lei%20Wang%20and%20Ce%20Zheng%20and%20Yapeng%20Tian%20and%20Chen%20Chen%0AAbstract%3A%20%20%20In%20this%20paper%2C%20we%20introduce%20the%20first%20comprehensive%20multilingual%20sign%0Alanguage%20dataset%20named%20Prompt2Sign%2C%20which%20builds%20from%20public%20data%20including%0AAmerican%20Sign%20Language%20%28ASL%29%20and%20seven%20others.%20Our%20dataset%20transforms%20a%20vast%0Aarray%20of%20videos%20into%20a%20streamlined%2C%20model-friendly%20format%2C%20optimized%20for%0Atraining%20with%20translation%20models%20like%20seq2seq%20and%20text2text.%20Building%20on%20this%0Anew%20dataset%2C%20we%20propose%20SignLLM%2C%20the%20first%20multilingual%20Sign%20Language%0AProduction%20%28SLP%29%20model%2C%20which%20includes%20two%20novel%20multilingual%20SLP%20modes%20that%0Aallow%20for%20the%20generation%20of%20sign%20language%20gestures%20from%20input%20text%20or%20prompt.%0ABoth%20of%20the%20modes%20can%20use%20a%20new%20loss%20and%20a%20module%20based%20on%20reinforcement%0Alearning%2C%20which%20accelerates%20the%20training%20by%20enhancing%20the%20model%27s%20capability%20to%0Aautonomously%20sample%20high-quality%20data.%20We%20present%20benchmark%20results%20of%20SignLLM%2C%0Awhich%20demonstrate%20that%20our%20model%20achieves%20state-of-the-art%20performance%20on%20SLP%0Atasks%20across%20eight%20sign%20languages.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2405.10718v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DSignLLM%253A%2520Sign%2520Languages%2520Production%2520Large%2520Language%2520Models%26entry.906535625%3DSen%2520Fang%2520and%2520Lei%2520Wang%2520and%2520Ce%2520Zheng%2520and%2520Yapeng%2520Tian%2520and%2520Chen%2520Chen%26entry.1292438233%3D%2520%2520In%2520this%2520paper%252C%2520we%2520introduce%2520the%2520first%2520comprehensive%2520multilingual%2520sign%250Alanguage%2520dataset%2520named%2520Prompt2Sign%252C%2520which%2520builds%2520from%2520public%2520data%2520including%250AAmerican%2520Sign%2520Language%2520%2528ASL%2529%2520and%2520seven%2520others.%2520Our%2520dataset%2520transforms%2520a%2520vast%250Aarray%2520of%2520videos%2520into%2520a%2520streamlined%252C%2520model-friendly%2520format%252C%2520optimized%2520for%250Atraining%2520with%2520translation%2520models%2520like%2520seq2seq%2520and%2520text2text.%2520Building%2520on%2520this%250Anew%2520dataset%252C%2520we%2520propose%2520SignLLM%252C%2520the%2520first%2520multilingual%2520Sign%2520Language%250AProduction%2520%2528SLP%2529%2520model%252C%2520which%2520includes%2520two%2520novel%2520multilingual%2520SLP%2520modes%2520that%250Aallow%2520for%2520the%2520generation%2520of%2520sign%2520language%2520gestures%2520from%2520input%2520text%2520or%2520prompt.%250ABoth%2520of%2520the%2520modes%2520can%2520use%2520a%2520new%2520loss%2520and%2520a%2520module%2520based%2520on%2520reinforcement%250Alearning%252C%2520which%2520accelerates%2520the%2520training%2520by%2520enhancing%2520the%2520model%2527s%2520capability%2520to%250Aautonomously%2520sample%2520high-quality%2520data.%2520We%2520present%2520benchmark%2520results%2520of%2520SignLLM%252C%250Awhich%2520demonstrate%2520that%2520our%2520model%2520achieves%2520state-of-the-art%2520performance%2520on%2520SLP%250Atasks%2520across%2520eight%2520sign%2520languages.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2405.10718v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=SignLLM%3A%20Sign%20Languages%20Production%20Large%20Language%20Models&entry.906535625=Sen%20Fang%20and%20Lei%20Wang%20and%20Ce%20Zheng%20and%20Yapeng%20Tian%20and%20Chen%20Chen&entry.1292438233=%20%20In%20this%20paper%2C%20we%20introduce%20the%20first%20comprehensive%20multilingual%20sign%0Alanguage%20dataset%20named%20Prompt2Sign%2C%20which%20builds%20from%20public%20data%20including%0AAmerican%20Sign%20Language%20%28ASL%29%20and%20seven%20others.%20Our%20dataset%20transforms%20a%20vast%0Aarray%20of%20videos%20into%20a%20streamlined%2C%20model-friendly%20format%2C%20optimized%20for%0Atraining%20with%20translation%20models%20like%20seq2seq%20and%20text2text.%20Building%20on%20this%0Anew%20dataset%2C%20we%20propose%20SignLLM%2C%20the%20first%20multilingual%20Sign%20Language%0AProduction%20%28SLP%29%20model%2C%20which%20includes%20two%20novel%20multilingual%20SLP%20modes%20that%0Aallow%20for%20the%20generation%20of%20sign%20language%20gestures%20from%20input%20text%20or%20prompt.%0ABoth%20of%20the%20modes%20can%20use%20a%20new%20loss%20and%20a%20module%20based%20on%20reinforcement%0Alearning%2C%20which%20accelerates%20the%20training%20by%20enhancing%20the%20model%27s%20capability%20to%0Aautonomously%20sample%20high-quality%20data.%20We%20present%20benchmark%20results%20of%20SignLLM%2C%0Awhich%20demonstrate%20that%20our%20model%20achieves%20state-of-the-art%20performance%20on%20SLP%0Atasks%20across%20eight%20sign%20languages.%0A&entry.1838667208=http%3A//arxiv.org/abs/2405.10718v1&entry.124074799=Read"},
{"title": "Heterogeneity-Informed Meta-Parameter Learning for Spatiotemporal Time\n  Series Forecasting", "author": "Zheng Dong and Renhe Jiang and Haotian Gao and Hangchen Liu and Jinliang Deng and Qingsong Wen and Xuan Song", "abstract": "  Spatiotemporal time series forecasting plays a key role in a wide range of\nreal-world applications. While significant progress has been made in this area,\nfully capturing and leveraging spatiotemporal heterogeneity remains a\nfundamental challenge. Therefore, we propose a novel Heterogeneity-Informed\nMeta-Parameter Learning scheme. Specifically, our approach implicitly captures\nspatiotemporal heterogeneity through learning spatial and temporal embeddings,\nwhich can be viewed as a clustering process. Then, a novel spatiotemporal\nmeta-parameter learning paradigm is proposed to learn spatiotemporal-specific\nparameters from meta-parameter pools, which is informed by the captured\nheterogeneity. Based on these ideas, we develop a Heterogeneity-Informed\nSpatiotemporal Meta-Network (HimNet) for spatiotemporal time series\nforecasting. Extensive experiments on five widely-used benchmarks demonstrate\nour method achieves state-of-the-art performance while exhibiting superior\ninterpretability. Our code is available at\nhttps://github.com/XDZhelheim/HimNet.\n", "link": "http://arxiv.org/abs/2405.10800v1", "date": "2024-05-17", "relevancy": 1.8698, "topK": [{"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.4997}, {"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.4751}, {"title": "DDM-NET: End-to-end learning of keypoint feature Detection, Description and Matching for 3D localization", "link": "https://arxiv.org/abs/2212.04575", "similarity": 0.4469}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Heterogeneity-Informed%20Meta-Parameter%20Learning%20for%20Spatiotemporal%20Time%0A%20%20Series%20Forecasting&body=Title%3A%20Heterogeneity-Informed%20Meta-Parameter%20Learning%20for%20Spatiotemporal%20Time%0A%20%20Series%20Forecasting%0AAuthor%3A%20Zheng%20Dong%20and%20Renhe%20Jiang%20and%20Haotian%20Gao%20and%20Hangchen%20Liu%20and%20Jinliang%20Deng%20and%20Qingsong%20Wen%20and%20Xuan%20Song%0AAbstract%3A%20%20%20Spatiotemporal%20time%20series%20forecasting%20plays%20a%20key%20role%20in%20a%20wide%20range%20of%0Areal-world%20applications.%20While%20significant%20progress%20has%20been%20made%20in%20this%20area%2C%0Afully%20capturing%20and%20leveraging%20spatiotemporal%20heterogeneity%20remains%20a%0Afundamental%20challenge.%20Therefore%2C%20we%20propose%20a%20novel%20Heterogeneity-Informed%0AMeta-Parameter%20Learning%20scheme.%20Specifically%2C%20our%20approach%20implicitly%20captures%0Aspatiotemporal%20heterogeneity%20through%20learning%20spatial%20and%20temporal%20embeddings%2C%0Awhich%20can%20be%20viewed%20as%20a%20clustering%20process.%20Then%2C%20a%20novel%20spatiotemporal%0Ameta-parameter%20learning%20paradigm%20is%20proposed%20to%20learn%20spatiotemporal-specific%0Aparameters%20from%20meta-parameter%20pools%2C%20which%20is%20informed%20by%20the%20captured%0Aheterogeneity.%20Based%20on%20these%20ideas%2C%20we%20develop%20a%20Heterogeneity-Informed%0ASpatiotemporal%20Meta-Network%20%28HimNet%29%20for%20spatiotemporal%20time%20series%0Aforecasting.%20Extensive%20experiments%20on%20five%20widely-used%20benchmarks%20demonstrate%0Aour%20method%20achieves%20state-of-the-art%20performance%20while%20exhibiting%20superior%0Ainterpretability.%20Our%20code%20is%20available%20at%0Ahttps%3A//github.com/XDZhelheim/HimNet.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2405.10800v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DHeterogeneity-Informed%2520Meta-Parameter%2520Learning%2520for%2520Spatiotemporal%2520Time%250A%2520%2520Series%2520Forecasting%26entry.906535625%3DZheng%2520Dong%2520and%2520Renhe%2520Jiang%2520and%2520Haotian%2520Gao%2520and%2520Hangchen%2520Liu%2520and%2520Jinliang%2520Deng%2520and%2520Qingsong%2520Wen%2520and%2520Xuan%2520Song%26entry.1292438233%3D%2520%2520Spatiotemporal%2520time%2520series%2520forecasting%2520plays%2520a%2520key%2520role%2520in%2520a%2520wide%2520range%2520of%250Areal-world%2520applications.%2520While%2520significant%2520progress%2520has%2520been%2520made%2520in%2520this%2520area%252C%250Afully%2520capturing%2520and%2520leveraging%2520spatiotemporal%2520heterogeneity%2520remains%2520a%250Afundamental%2520challenge.%2520Therefore%252C%2520we%2520propose%2520a%2520novel%2520Heterogeneity-Informed%250AMeta-Parameter%2520Learning%2520scheme.%2520Specifically%252C%2520our%2520approach%2520implicitly%2520captures%250Aspatiotemporal%2520heterogeneity%2520through%2520learning%2520spatial%2520and%2520temporal%2520embeddings%252C%250Awhich%2520can%2520be%2520viewed%2520as%2520a%2520clustering%2520process.%2520Then%252C%2520a%2520novel%2520spatiotemporal%250Ameta-parameter%2520learning%2520paradigm%2520is%2520proposed%2520to%2520learn%2520spatiotemporal-specific%250Aparameters%2520from%2520meta-parameter%2520pools%252C%2520which%2520is%2520informed%2520by%2520the%2520captured%250Aheterogeneity.%2520Based%2520on%2520these%2520ideas%252C%2520we%2520develop%2520a%2520Heterogeneity-Informed%250ASpatiotemporal%2520Meta-Network%2520%2528HimNet%2529%2520for%2520spatiotemporal%2520time%2520series%250Aforecasting.%2520Extensive%2520experiments%2520on%2520five%2520widely-used%2520benchmarks%2520demonstrate%250Aour%2520method%2520achieves%2520state-of-the-art%2520performance%2520while%2520exhibiting%2520superior%250Ainterpretability.%2520Our%2520code%2520is%2520available%2520at%250Ahttps%253A//github.com/XDZhelheim/HimNet.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2405.10800v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Heterogeneity-Informed%20Meta-Parameter%20Learning%20for%20Spatiotemporal%20Time%0A%20%20Series%20Forecasting&entry.906535625=Zheng%20Dong%20and%20Renhe%20Jiang%20and%20Haotian%20Gao%20and%20Hangchen%20Liu%20and%20Jinliang%20Deng%20and%20Qingsong%20Wen%20and%20Xuan%20Song&entry.1292438233=%20%20Spatiotemporal%20time%20series%20forecasting%20plays%20a%20key%20role%20in%20a%20wide%20range%20of%0Areal-world%20applications.%20While%20significant%20progress%20has%20been%20made%20in%20this%20area%2C%0Afully%20capturing%20and%20leveraging%20spatiotemporal%20heterogeneity%20remains%20a%0Afundamental%20challenge.%20Therefore%2C%20we%20propose%20a%20novel%20Heterogeneity-Informed%0AMeta-Parameter%20Learning%20scheme.%20Specifically%2C%20our%20approach%20implicitly%20captures%0Aspatiotemporal%20heterogeneity%20through%20learning%20spatial%20and%20temporal%20embeddings%2C%0Awhich%20can%20be%20viewed%20as%20a%20clustering%20process.%20Then%2C%20a%20novel%20spatiotemporal%0Ameta-parameter%20learning%20paradigm%20is%20proposed%20to%20learn%20spatiotemporal-specific%0Aparameters%20from%20meta-parameter%20pools%2C%20which%20is%20informed%20by%20the%20captured%0Aheterogeneity.%20Based%20on%20these%20ideas%2C%20we%20develop%20a%20Heterogeneity-Informed%0ASpatiotemporal%20Meta-Network%20%28HimNet%29%20for%20spatiotemporal%20time%20series%0Aforecasting.%20Extensive%20experiments%20on%20five%20widely-used%20benchmarks%20demonstrate%0Aour%20method%20achieves%20state-of-the-art%20performance%20while%20exhibiting%20superior%0Ainterpretability.%20Our%20code%20is%20available%20at%0Ahttps%3A//github.com/XDZhelheim/HimNet.%0A&entry.1838667208=http%3A//arxiv.org/abs/2405.10800v1&entry.124074799=Read"},
{"title": "HLSFactory: A Framework Empowering High-Level Synthesis Datasets for\n  Machine Learning and Beyond", "author": "Stefan Abi-Karam and Rishov Sarkar and Allison Seigler and Sean Lowe and Zhigang Wei and Hanqiu Chen and Nanditha Rao and Lizy John and Aman Arora and Cong Hao", "abstract": "  Machine learning (ML) techniques have been applied to high-level synthesis\n(HLS) flows for quality-of-result (QoR) prediction and design space exploration\n(DSE). Nevertheless, the scarcity of accessible high-quality HLS datasets and\nthe complexity of building such datasets present challenges. Existing datasets\nhave limitations in terms of benchmark coverage, design space enumeration,\nvendor extensibility, or lack of reproducible and extensible software for\ndataset construction. Many works also lack user-friendly ways to add more\ndesigns, limiting wider adoption of such datasets.\n  In response to these challenges, we introduce HLSFactory, a comprehensive\nframework designed to facilitate the curation and generation of high-quality\nHLS design datasets. HLSFactory has three main stages: 1) a design space\nexpansion stage to elaborate single HLS designs into large design spaces using\nvarious optimization directives across multiple vendor tools, 2) a design\nsynthesis stage to execute HLS and FPGA tool flows concurrently across designs,\nand 3) a data aggregation stage for extracting standardized data into packaged\ndatasets for ML usage. This tripartite architecture ensures broad design space\ncoverage via design space expansion and supports multiple vendor tools. Users\ncan contribute to each stage with their own HLS designs and synthesis results\nand extend the framework itself with custom frontends and tool flows. We also\ninclude an initial set of built-in designs from common HLS benchmarks curated\nopen-source HLS designs.\n  We showcase the versatility and multi-functionality of our framework through\nsix case studies: I) Design space sampling; II) Fine-grained parallelism\nbackend speedup; III) Targeting Intel's HLS flow; IV) Adding new auxiliary\ndesigns; V) Integrating published HLS data; VI) HLS tool version regression\nbenchmarking.\n  Code at https://github.com/sharc-lab/HLSFactory.\n", "link": "http://arxiv.org/abs/2405.00820v2", "date": "2024-05-17", "relevancy": 1.8651, "topK": [{"title": "Scaling Rectified Flow Transformers for High-Resolution Image Synthesis", "link": "http://arxiv.org/abs/2403.03206v1", "similarity": 0.4679}, {"title": "DressCode: Autoregressively Sewing and Generating Garments from Text\n  Guidance", "link": "http://arxiv.org/abs/2401.16465v3", "similarity": 0.4664}, {"title": "Dynamic Gaussians Mesh: Consistent Mesh Reconstruction from Monocular\n  Videos", "link": "http://arxiv.org/abs/2404.12379v2", "similarity": 0.4655}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20HLSFactory%3A%20A%20Framework%20Empowering%20High-Level%20Synthesis%20Datasets%20for%0A%20%20Machine%20Learning%20and%20Beyond&body=Title%3A%20HLSFactory%3A%20A%20Framework%20Empowering%20High-Level%20Synthesis%20Datasets%20for%0A%20%20Machine%20Learning%20and%20Beyond%0AAuthor%3A%20Stefan%20Abi-Karam%20and%20Rishov%20Sarkar%20and%20Allison%20Seigler%20and%20Sean%20Lowe%20and%20Zhigang%20Wei%20and%20Hanqiu%20Chen%20and%20Nanditha%20Rao%20and%20Lizy%20John%20and%20Aman%20Arora%20and%20Cong%20Hao%0AAbstract%3A%20%20%20Machine%20learning%20%28ML%29%20techniques%20have%20been%20applied%20to%20high-level%20synthesis%0A%28HLS%29%20flows%20for%20quality-of-result%20%28QoR%29%20prediction%20and%20design%20space%20exploration%0A%28DSE%29.%20Nevertheless%2C%20the%20scarcity%20of%20accessible%20high-quality%20HLS%20datasets%20and%0Athe%20complexity%20of%20building%20such%20datasets%20present%20challenges.%20Existing%20datasets%0Ahave%20limitations%20in%20terms%20of%20benchmark%20coverage%2C%20design%20space%20enumeration%2C%0Avendor%20extensibility%2C%20or%20lack%20of%20reproducible%20and%20extensible%20software%20for%0Adataset%20construction.%20Many%20works%20also%20lack%20user-friendly%20ways%20to%20add%20more%0Adesigns%2C%20limiting%20wider%20adoption%20of%20such%20datasets.%0A%20%20In%20response%20to%20these%20challenges%2C%20we%20introduce%20HLSFactory%2C%20a%20comprehensive%0Aframework%20designed%20to%20facilitate%20the%20curation%20and%20generation%20of%20high-quality%0AHLS%20design%20datasets.%20HLSFactory%20has%20three%20main%20stages%3A%201%29%20a%20design%20space%0Aexpansion%20stage%20to%20elaborate%20single%20HLS%20designs%20into%20large%20design%20spaces%20using%0Avarious%20optimization%20directives%20across%20multiple%20vendor%20tools%2C%202%29%20a%20design%0Asynthesis%20stage%20to%20execute%20HLS%20and%20FPGA%20tool%20flows%20concurrently%20across%20designs%2C%0Aand%203%29%20a%20data%20aggregation%20stage%20for%20extracting%20standardized%20data%20into%20packaged%0Adatasets%20for%20ML%20usage.%20This%20tripartite%20architecture%20ensures%20broad%20design%20space%0Acoverage%20via%20design%20space%20expansion%20and%20supports%20multiple%20vendor%20tools.%20Users%0Acan%20contribute%20to%20each%20stage%20with%20their%20own%20HLS%20designs%20and%20synthesis%20results%0Aand%20extend%20the%20framework%20itself%20with%20custom%20frontends%20and%20tool%20flows.%20We%20also%0Ainclude%20an%20initial%20set%20of%20built-in%20designs%20from%20common%20HLS%20benchmarks%20curated%0Aopen-source%20HLS%20designs.%0A%20%20We%20showcase%20the%20versatility%20and%20multi-functionality%20of%20our%20framework%20through%0Asix%20case%20studies%3A%20I%29%20Design%20space%20sampling%3B%20II%29%20Fine-grained%20parallelism%0Abackend%20speedup%3B%20III%29%20Targeting%20Intel%27s%20HLS%20flow%3B%20IV%29%20Adding%20new%20auxiliary%0Adesigns%3B%20V%29%20Integrating%20published%20HLS%20data%3B%20VI%29%20HLS%20tool%20version%20regression%0Abenchmarking.%0A%20%20Code%20at%20https%3A//github.com/sharc-lab/HLSFactory.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2405.00820v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DHLSFactory%253A%2520A%2520Framework%2520Empowering%2520High-Level%2520Synthesis%2520Datasets%2520for%250A%2520%2520Machine%2520Learning%2520and%2520Beyond%26entry.906535625%3DStefan%2520Abi-Karam%2520and%2520Rishov%2520Sarkar%2520and%2520Allison%2520Seigler%2520and%2520Sean%2520Lowe%2520and%2520Zhigang%2520Wei%2520and%2520Hanqiu%2520Chen%2520and%2520Nanditha%2520Rao%2520and%2520Lizy%2520John%2520and%2520Aman%2520Arora%2520and%2520Cong%2520Hao%26entry.1292438233%3D%2520%2520Machine%2520learning%2520%2528ML%2529%2520techniques%2520have%2520been%2520applied%2520to%2520high-level%2520synthesis%250A%2528HLS%2529%2520flows%2520for%2520quality-of-result%2520%2528QoR%2529%2520prediction%2520and%2520design%2520space%2520exploration%250A%2528DSE%2529.%2520Nevertheless%252C%2520the%2520scarcity%2520of%2520accessible%2520high-quality%2520HLS%2520datasets%2520and%250Athe%2520complexity%2520of%2520building%2520such%2520datasets%2520present%2520challenges.%2520Existing%2520datasets%250Ahave%2520limitations%2520in%2520terms%2520of%2520benchmark%2520coverage%252C%2520design%2520space%2520enumeration%252C%250Avendor%2520extensibility%252C%2520or%2520lack%2520of%2520reproducible%2520and%2520extensible%2520software%2520for%250Adataset%2520construction.%2520Many%2520works%2520also%2520lack%2520user-friendly%2520ways%2520to%2520add%2520more%250Adesigns%252C%2520limiting%2520wider%2520adoption%2520of%2520such%2520datasets.%250A%2520%2520In%2520response%2520to%2520these%2520challenges%252C%2520we%2520introduce%2520HLSFactory%252C%2520a%2520comprehensive%250Aframework%2520designed%2520to%2520facilitate%2520the%2520curation%2520and%2520generation%2520of%2520high-quality%250AHLS%2520design%2520datasets.%2520HLSFactory%2520has%2520three%2520main%2520stages%253A%25201%2529%2520a%2520design%2520space%250Aexpansion%2520stage%2520to%2520elaborate%2520single%2520HLS%2520designs%2520into%2520large%2520design%2520spaces%2520using%250Avarious%2520optimization%2520directives%2520across%2520multiple%2520vendor%2520tools%252C%25202%2529%2520a%2520design%250Asynthesis%2520stage%2520to%2520execute%2520HLS%2520and%2520FPGA%2520tool%2520flows%2520concurrently%2520across%2520designs%252C%250Aand%25203%2529%2520a%2520data%2520aggregation%2520stage%2520for%2520extracting%2520standardized%2520data%2520into%2520packaged%250Adatasets%2520for%2520ML%2520usage.%2520This%2520tripartite%2520architecture%2520ensures%2520broad%2520design%2520space%250Acoverage%2520via%2520design%2520space%2520expansion%2520and%2520supports%2520multiple%2520vendor%2520tools.%2520Users%250Acan%2520contribute%2520to%2520each%2520stage%2520with%2520their%2520own%2520HLS%2520designs%2520and%2520synthesis%2520results%250Aand%2520extend%2520the%2520framework%2520itself%2520with%2520custom%2520frontends%2520and%2520tool%2520flows.%2520We%2520also%250Ainclude%2520an%2520initial%2520set%2520of%2520built-in%2520designs%2520from%2520common%2520HLS%2520benchmarks%2520curated%250Aopen-source%2520HLS%2520designs.%250A%2520%2520We%2520showcase%2520the%2520versatility%2520and%2520multi-functionality%2520of%2520our%2520framework%2520through%250Asix%2520case%2520studies%253A%2520I%2529%2520Design%2520space%2520sampling%253B%2520II%2529%2520Fine-grained%2520parallelism%250Abackend%2520speedup%253B%2520III%2529%2520Targeting%2520Intel%2527s%2520HLS%2520flow%253B%2520IV%2529%2520Adding%2520new%2520auxiliary%250Adesigns%253B%2520V%2529%2520Integrating%2520published%2520HLS%2520data%253B%2520VI%2529%2520HLS%2520tool%2520version%2520regression%250Abenchmarking.%250A%2520%2520Code%2520at%2520https%253A//github.com/sharc-lab/HLSFactory.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2405.00820v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=HLSFactory%3A%20A%20Framework%20Empowering%20High-Level%20Synthesis%20Datasets%20for%0A%20%20Machine%20Learning%20and%20Beyond&entry.906535625=Stefan%20Abi-Karam%20and%20Rishov%20Sarkar%20and%20Allison%20Seigler%20and%20Sean%20Lowe%20and%20Zhigang%20Wei%20and%20Hanqiu%20Chen%20and%20Nanditha%20Rao%20and%20Lizy%20John%20and%20Aman%20Arora%20and%20Cong%20Hao&entry.1292438233=%20%20Machine%20learning%20%28ML%29%20techniques%20have%20been%20applied%20to%20high-level%20synthesis%0A%28HLS%29%20flows%20for%20quality-of-result%20%28QoR%29%20prediction%20and%20design%20space%20exploration%0A%28DSE%29.%20Nevertheless%2C%20the%20scarcity%20of%20accessible%20high-quality%20HLS%20datasets%20and%0Athe%20complexity%20of%20building%20such%20datasets%20present%20challenges.%20Existing%20datasets%0Ahave%20limitations%20in%20terms%20of%20benchmark%20coverage%2C%20design%20space%20enumeration%2C%0Avendor%20extensibility%2C%20or%20lack%20of%20reproducible%20and%20extensible%20software%20for%0Adataset%20construction.%20Many%20works%20also%20lack%20user-friendly%20ways%20to%20add%20more%0Adesigns%2C%20limiting%20wider%20adoption%20of%20such%20datasets.%0A%20%20In%20response%20to%20these%20challenges%2C%20we%20introduce%20HLSFactory%2C%20a%20comprehensive%0Aframework%20designed%20to%20facilitate%20the%20curation%20and%20generation%20of%20high-quality%0AHLS%20design%20datasets.%20HLSFactory%20has%20three%20main%20stages%3A%201%29%20a%20design%20space%0Aexpansion%20stage%20to%20elaborate%20single%20HLS%20designs%20into%20large%20design%20spaces%20using%0Avarious%20optimization%20directives%20across%20multiple%20vendor%20tools%2C%202%29%20a%20design%0Asynthesis%20stage%20to%20execute%20HLS%20and%20FPGA%20tool%20flows%20concurrently%20across%20designs%2C%0Aand%203%29%20a%20data%20aggregation%20stage%20for%20extracting%20standardized%20data%20into%20packaged%0Adatasets%20for%20ML%20usage.%20This%20tripartite%20architecture%20ensures%20broad%20design%20space%0Acoverage%20via%20design%20space%20expansion%20and%20supports%20multiple%20vendor%20tools.%20Users%0Acan%20contribute%20to%20each%20stage%20with%20their%20own%20HLS%20designs%20and%20synthesis%20results%0Aand%20extend%20the%20framework%20itself%20with%20custom%20frontends%20and%20tool%20flows.%20We%20also%0Ainclude%20an%20initial%20set%20of%20built-in%20designs%20from%20common%20HLS%20benchmarks%20curated%0Aopen-source%20HLS%20designs.%0A%20%20We%20showcase%20the%20versatility%20and%20multi-functionality%20of%20our%20framework%20through%0Asix%20case%20studies%3A%20I%29%20Design%20space%20sampling%3B%20II%29%20Fine-grained%20parallelism%0Abackend%20speedup%3B%20III%29%20Targeting%20Intel%27s%20HLS%20flow%3B%20IV%29%20Adding%20new%20auxiliary%0Adesigns%3B%20V%29%20Integrating%20published%20HLS%20data%3B%20VI%29%20HLS%20tool%20version%20regression%0Abenchmarking.%0A%20%20Code%20at%20https%3A//github.com/sharc-lab/HLSFactory.%0A&entry.1838667208=http%3A//arxiv.org/abs/2405.00820v2&entry.124074799=Read"},
{"title": "Restless Linear Bandits", "author": "Azadeh Khaleghi", "abstract": "  A more general formulation of the linear bandit problem is considered to\nallow for dependencies over time. Specifically, it is assumed that there exists\nan unknown $\\mathbb{R}^d$-valued stationary $\\varphi$-mixing sequence of\nparameters $(\\theta_t,~t \\in \\mathbb{N})$ which gives rise to pay-offs. This\ninstance of the problem can be viewed as a generalization of both the classical\nlinear bandits with iid noise, and the finite-armed restless bandits. In light\nof the well-known computational hardness of optimal policies for restless\nbandits, an approximation is proposed whose error is shown to be controlled by\nthe $\\varphi$-dependence between consecutive $\\theta_t$. An optimistic\nalgorithm, called LinMix-UCB, is proposed for the case where $\\theta_t$ has an\nexponential mixing rate. The proposed algorithm is shown to incur a sub-linear\nregret of $\\mathcal{O}\\left(\\sqrt{d n\\mathrm{polylog}(n) }\\right)$ with respect\nto an oracle that always plays a multiple of $\\mathbb{E}\\theta_t$. The main\nchallenge in this setting is to ensure that the exploration-exploitation\nstrategy is robust against long-range dependencies. The proposed method relies\non Berbee's coupling lemma to carefully select near-independent samples and\nconstruct confidence ellipsoids around empirical estimates of\n$\\mathbb{E}\\theta_t$.\n", "link": "http://arxiv.org/abs/2405.10817v1", "date": "2024-05-17", "relevancy": 1.8637, "topK": [{"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.4884}, {"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.473}, {"title": "Flexible Visual Recognition by Evidential Modeling of Confusion and Ignorance", "link": "http://openaccess.thecvf.com/content/ICCV2023/html/Fan_Flexible_Visual_Recognition_by_Evidential_Modeling_of_Confusion_and_Ignorance_ICCV_2023_paper.html", "similarity": 0.4498}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Restless%20Linear%20Bandits&body=Title%3A%20Restless%20Linear%20Bandits%0AAuthor%3A%20Azadeh%20Khaleghi%0AAbstract%3A%20%20%20A%20more%20general%20formulation%20of%20the%20linear%20bandit%20problem%20is%20considered%20to%0Aallow%20for%20dependencies%20over%20time.%20Specifically%2C%20it%20is%20assumed%20that%20there%20exists%0Aan%20unknown%20%24%5Cmathbb%7BR%7D%5Ed%24-valued%20stationary%20%24%5Cvarphi%24-mixing%20sequence%20of%0Aparameters%20%24%28%5Ctheta_t%2C~t%20%5Cin%20%5Cmathbb%7BN%7D%29%24%20which%20gives%20rise%20to%20pay-offs.%20This%0Ainstance%20of%20the%20problem%20can%20be%20viewed%20as%20a%20generalization%20of%20both%20the%20classical%0Alinear%20bandits%20with%20iid%20noise%2C%20and%20the%20finite-armed%20restless%20bandits.%20In%20light%0Aof%20the%20well-known%20computational%20hardness%20of%20optimal%20policies%20for%20restless%0Abandits%2C%20an%20approximation%20is%20proposed%20whose%20error%20is%20shown%20to%20be%20controlled%20by%0Athe%20%24%5Cvarphi%24-dependence%20between%20consecutive%20%24%5Ctheta_t%24.%20An%20optimistic%0Aalgorithm%2C%20called%20LinMix-UCB%2C%20is%20proposed%20for%20the%20case%20where%20%24%5Ctheta_t%24%20has%20an%0Aexponential%20mixing%20rate.%20The%20proposed%20algorithm%20is%20shown%20to%20incur%20a%20sub-linear%0Aregret%20of%20%24%5Cmathcal%7BO%7D%5Cleft%28%5Csqrt%7Bd%20n%5Cmathrm%7Bpolylog%7D%28n%29%20%7D%5Cright%29%24%20with%20respect%0Ato%20an%20oracle%20that%20always%20plays%20a%20multiple%20of%20%24%5Cmathbb%7BE%7D%5Ctheta_t%24.%20The%20main%0Achallenge%20in%20this%20setting%20is%20to%20ensure%20that%20the%20exploration-exploitation%0Astrategy%20is%20robust%20against%20long-range%20dependencies.%20The%20proposed%20method%20relies%0Aon%20Berbee%27s%20coupling%20lemma%20to%20carefully%20select%20near-independent%20samples%20and%0Aconstruct%20confidence%20ellipsoids%20around%20empirical%20estimates%20of%0A%24%5Cmathbb%7BE%7D%5Ctheta_t%24.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2405.10817v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DRestless%2520Linear%2520Bandits%26entry.906535625%3DAzadeh%2520Khaleghi%26entry.1292438233%3D%2520%2520A%2520more%2520general%2520formulation%2520of%2520the%2520linear%2520bandit%2520problem%2520is%2520considered%2520to%250Aallow%2520for%2520dependencies%2520over%2520time.%2520Specifically%252C%2520it%2520is%2520assumed%2520that%2520there%2520exists%250Aan%2520unknown%2520%2524%255Cmathbb%257BR%257D%255Ed%2524-valued%2520stationary%2520%2524%255Cvarphi%2524-mixing%2520sequence%2520of%250Aparameters%2520%2524%2528%255Ctheta_t%252C~t%2520%255Cin%2520%255Cmathbb%257BN%257D%2529%2524%2520which%2520gives%2520rise%2520to%2520pay-offs.%2520This%250Ainstance%2520of%2520the%2520problem%2520can%2520be%2520viewed%2520as%2520a%2520generalization%2520of%2520both%2520the%2520classical%250Alinear%2520bandits%2520with%2520iid%2520noise%252C%2520and%2520the%2520finite-armed%2520restless%2520bandits.%2520In%2520light%250Aof%2520the%2520well-known%2520computational%2520hardness%2520of%2520optimal%2520policies%2520for%2520restless%250Abandits%252C%2520an%2520approximation%2520is%2520proposed%2520whose%2520error%2520is%2520shown%2520to%2520be%2520controlled%2520by%250Athe%2520%2524%255Cvarphi%2524-dependence%2520between%2520consecutive%2520%2524%255Ctheta_t%2524.%2520An%2520optimistic%250Aalgorithm%252C%2520called%2520LinMix-UCB%252C%2520is%2520proposed%2520for%2520the%2520case%2520where%2520%2524%255Ctheta_t%2524%2520has%2520an%250Aexponential%2520mixing%2520rate.%2520The%2520proposed%2520algorithm%2520is%2520shown%2520to%2520incur%2520a%2520sub-linear%250Aregret%2520of%2520%2524%255Cmathcal%257BO%257D%255Cleft%2528%255Csqrt%257Bd%2520n%255Cmathrm%257Bpolylog%257D%2528n%2529%2520%257D%255Cright%2529%2524%2520with%2520respect%250Ato%2520an%2520oracle%2520that%2520always%2520plays%2520a%2520multiple%2520of%2520%2524%255Cmathbb%257BE%257D%255Ctheta_t%2524.%2520The%2520main%250Achallenge%2520in%2520this%2520setting%2520is%2520to%2520ensure%2520that%2520the%2520exploration-exploitation%250Astrategy%2520is%2520robust%2520against%2520long-range%2520dependencies.%2520The%2520proposed%2520method%2520relies%250Aon%2520Berbee%2527s%2520coupling%2520lemma%2520to%2520carefully%2520select%2520near-independent%2520samples%2520and%250Aconstruct%2520confidence%2520ellipsoids%2520around%2520empirical%2520estimates%2520of%250A%2524%255Cmathbb%257BE%257D%255Ctheta_t%2524.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2405.10817v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Restless%20Linear%20Bandits&entry.906535625=Azadeh%20Khaleghi&entry.1292438233=%20%20A%20more%20general%20formulation%20of%20the%20linear%20bandit%20problem%20is%20considered%20to%0Aallow%20for%20dependencies%20over%20time.%20Specifically%2C%20it%20is%20assumed%20that%20there%20exists%0Aan%20unknown%20%24%5Cmathbb%7BR%7D%5Ed%24-valued%20stationary%20%24%5Cvarphi%24-mixing%20sequence%20of%0Aparameters%20%24%28%5Ctheta_t%2C~t%20%5Cin%20%5Cmathbb%7BN%7D%29%24%20which%20gives%20rise%20to%20pay-offs.%20This%0Ainstance%20of%20the%20problem%20can%20be%20viewed%20as%20a%20generalization%20of%20both%20the%20classical%0Alinear%20bandits%20with%20iid%20noise%2C%20and%20the%20finite-armed%20restless%20bandits.%20In%20light%0Aof%20the%20well-known%20computational%20hardness%20of%20optimal%20policies%20for%20restless%0Abandits%2C%20an%20approximation%20is%20proposed%20whose%20error%20is%20shown%20to%20be%20controlled%20by%0Athe%20%24%5Cvarphi%24-dependence%20between%20consecutive%20%24%5Ctheta_t%24.%20An%20optimistic%0Aalgorithm%2C%20called%20LinMix-UCB%2C%20is%20proposed%20for%20the%20case%20where%20%24%5Ctheta_t%24%20has%20an%0Aexponential%20mixing%20rate.%20The%20proposed%20algorithm%20is%20shown%20to%20incur%20a%20sub-linear%0Aregret%20of%20%24%5Cmathcal%7BO%7D%5Cleft%28%5Csqrt%7Bd%20n%5Cmathrm%7Bpolylog%7D%28n%29%20%7D%5Cright%29%24%20with%20respect%0Ato%20an%20oracle%20that%20always%20plays%20a%20multiple%20of%20%24%5Cmathbb%7BE%7D%5Ctheta_t%24.%20The%20main%0Achallenge%20in%20this%20setting%20is%20to%20ensure%20that%20the%20exploration-exploitation%0Astrategy%20is%20robust%20against%20long-range%20dependencies.%20The%20proposed%20method%20relies%0Aon%20Berbee%27s%20coupling%20lemma%20to%20carefully%20select%20near-independent%20samples%20and%0Aconstruct%20confidence%20ellipsoids%20around%20empirical%20estimates%20of%0A%24%5Cmathbb%7BE%7D%5Ctheta_t%24.%0A&entry.1838667208=http%3A//arxiv.org/abs/2405.10817v1&entry.124074799=Read"},
{"title": "Data-Driven Symbol Detection for Intersymbol Interference Channels with\n  Bursty Impulsive Noise", "author": "Boris Karanov and Chin-Hung Chen and Yan Wu and Alex Young and Wim van Houtum", "abstract": "  We developed machine learning approaches for data-driven trellis-based soft\nsymbol detection in coded transmission over intersymbol interference (ISI)\nchannels in presence of bursty impulsive noise (IN), for example encountered in\nwireless digital broadcasting systems and vehicular communications. This\nenabled us to obtain optimized detectors based on the Bahl-Cocke-Jelinek-Raviv\n(BCJR) algorithm while circumventing the use of full channel state information\n(CSI) for computing likelihoods and trellis state transition probabilities.\nFirst, we extended the application of the neural network (NN)-aided BCJR,\nrecently proposed for ISI channels with additive white Gaussian noise (AWGN).\nAlthough suitable for estimating likelihoods via labeling of transmission\nsequences, the BCJR-NN method does not provide a framework for learning the\ntrellis state transitions. In addition to detection over the joint ISI and IN\nstates we also focused on another scenario where trellis transitions are not\ntrivial: detection for the ISI channel with AWGN with inaccurate knowledge of\nthe channel memory at the receiver. Without access to the accurate state\ntransition matrix, the BCJR- NN performance significantly degrades in both\nsettings. To this end, we devised an alternative approach for data-driven BCJR\ndetection based on the unsupervised learning of a hidden Markov model (HMM).\nThe BCJR-HMM allowed us to optimize both the likelihood function and the state\ntransition matrix without labeling. Moreover, we demonstrated the viability of\na hybrid NN and HMM BCJR detection where NN is used for learning the\nlikelihoods, while the state transitions are optimized via HMM. While reducing\nthe required prior channel knowledge, the examined data-driven detectors with\nlearned trellis state transitions achieve bit error rates close to the optimal\nfull CSI-based BCJR, significantly outperforming detection with inaccurate CSI.\n", "link": "http://arxiv.org/abs/2405.10814v1", "date": "2024-05-17", "relevancy": 1.8524, "topK": [{"title": "Boosted dynamic neural networks", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/26302", "similarity": 0.4773}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.4588}, {"title": "Tuning-Free Noise Rectification for High Fidelity Image-to-Video\n  Generation", "link": "http://arxiv.org/abs/2403.02827v1", "similarity": 0.4507}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Data-Driven%20Symbol%20Detection%20for%20Intersymbol%20Interference%20Channels%20with%0A%20%20Bursty%20Impulsive%20Noise&body=Title%3A%20Data-Driven%20Symbol%20Detection%20for%20Intersymbol%20Interference%20Channels%20with%0A%20%20Bursty%20Impulsive%20Noise%0AAuthor%3A%20Boris%20Karanov%20and%20Chin-Hung%20Chen%20and%20Yan%20Wu%20and%20Alex%20Young%20and%20Wim%20van%20Houtum%0AAbstract%3A%20%20%20We%20developed%20machine%20learning%20approaches%20for%20data-driven%20trellis-based%20soft%0Asymbol%20detection%20in%20coded%20transmission%20over%20intersymbol%20interference%20%28ISI%29%0Achannels%20in%20presence%20of%20bursty%20impulsive%20noise%20%28IN%29%2C%20for%20example%20encountered%20in%0Awireless%20digital%20broadcasting%20systems%20and%20vehicular%20communications.%20This%0Aenabled%20us%20to%20obtain%20optimized%20detectors%20based%20on%20the%20Bahl-Cocke-Jelinek-Raviv%0A%28BCJR%29%20algorithm%20while%20circumventing%20the%20use%20of%20full%20channel%20state%20information%0A%28CSI%29%20for%20computing%20likelihoods%20and%20trellis%20state%20transition%20probabilities.%0AFirst%2C%20we%20extended%20the%20application%20of%20the%20neural%20network%20%28NN%29-aided%20BCJR%2C%0Arecently%20proposed%20for%20ISI%20channels%20with%20additive%20white%20Gaussian%20noise%20%28AWGN%29.%0AAlthough%20suitable%20for%20estimating%20likelihoods%20via%20labeling%20of%20transmission%0Asequences%2C%20the%20BCJR-NN%20method%20does%20not%20provide%20a%20framework%20for%20learning%20the%0Atrellis%20state%20transitions.%20In%20addition%20to%20detection%20over%20the%20joint%20ISI%20and%20IN%0Astates%20we%20also%20focused%20on%20another%20scenario%20where%20trellis%20transitions%20are%20not%0Atrivial%3A%20detection%20for%20the%20ISI%20channel%20with%20AWGN%20with%20inaccurate%20knowledge%20of%0Athe%20channel%20memory%20at%20the%20receiver.%20Without%20access%20to%20the%20accurate%20state%0Atransition%20matrix%2C%20the%20BCJR-%20NN%20performance%20significantly%20degrades%20in%20both%0Asettings.%20To%20this%20end%2C%20we%20devised%20an%20alternative%20approach%20for%20data-driven%20BCJR%0Adetection%20based%20on%20the%20unsupervised%20learning%20of%20a%20hidden%20Markov%20model%20%28HMM%29.%0AThe%20BCJR-HMM%20allowed%20us%20to%20optimize%20both%20the%20likelihood%20function%20and%20the%20state%0Atransition%20matrix%20without%20labeling.%20Moreover%2C%20we%20demonstrated%20the%20viability%20of%0Aa%20hybrid%20NN%20and%20HMM%20BCJR%20detection%20where%20NN%20is%20used%20for%20learning%20the%0Alikelihoods%2C%20while%20the%20state%20transitions%20are%20optimized%20via%20HMM.%20While%20reducing%0Athe%20required%20prior%20channel%20knowledge%2C%20the%20examined%20data-driven%20detectors%20with%0Alearned%20trellis%20state%20transitions%20achieve%20bit%20error%20rates%20close%20to%20the%20optimal%0Afull%20CSI-based%20BCJR%2C%20significantly%20outperforming%20detection%20with%20inaccurate%20CSI.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2405.10814v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DData-Driven%2520Symbol%2520Detection%2520for%2520Intersymbol%2520Interference%2520Channels%2520with%250A%2520%2520Bursty%2520Impulsive%2520Noise%26entry.906535625%3DBoris%2520Karanov%2520and%2520Chin-Hung%2520Chen%2520and%2520Yan%2520Wu%2520and%2520Alex%2520Young%2520and%2520Wim%2520van%2520Houtum%26entry.1292438233%3D%2520%2520We%2520developed%2520machine%2520learning%2520approaches%2520for%2520data-driven%2520trellis-based%2520soft%250Asymbol%2520detection%2520in%2520coded%2520transmission%2520over%2520intersymbol%2520interference%2520%2528ISI%2529%250Achannels%2520in%2520presence%2520of%2520bursty%2520impulsive%2520noise%2520%2528IN%2529%252C%2520for%2520example%2520encountered%2520in%250Awireless%2520digital%2520broadcasting%2520systems%2520and%2520vehicular%2520communications.%2520This%250Aenabled%2520us%2520to%2520obtain%2520optimized%2520detectors%2520based%2520on%2520the%2520Bahl-Cocke-Jelinek-Raviv%250A%2528BCJR%2529%2520algorithm%2520while%2520circumventing%2520the%2520use%2520of%2520full%2520channel%2520state%2520information%250A%2528CSI%2529%2520for%2520computing%2520likelihoods%2520and%2520trellis%2520state%2520transition%2520probabilities.%250AFirst%252C%2520we%2520extended%2520the%2520application%2520of%2520the%2520neural%2520network%2520%2528NN%2529-aided%2520BCJR%252C%250Arecently%2520proposed%2520for%2520ISI%2520channels%2520with%2520additive%2520white%2520Gaussian%2520noise%2520%2528AWGN%2529.%250AAlthough%2520suitable%2520for%2520estimating%2520likelihoods%2520via%2520labeling%2520of%2520transmission%250Asequences%252C%2520the%2520BCJR-NN%2520method%2520does%2520not%2520provide%2520a%2520framework%2520for%2520learning%2520the%250Atrellis%2520state%2520transitions.%2520In%2520addition%2520to%2520detection%2520over%2520the%2520joint%2520ISI%2520and%2520IN%250Astates%2520we%2520also%2520focused%2520on%2520another%2520scenario%2520where%2520trellis%2520transitions%2520are%2520not%250Atrivial%253A%2520detection%2520for%2520the%2520ISI%2520channel%2520with%2520AWGN%2520with%2520inaccurate%2520knowledge%2520of%250Athe%2520channel%2520memory%2520at%2520the%2520receiver.%2520Without%2520access%2520to%2520the%2520accurate%2520state%250Atransition%2520matrix%252C%2520the%2520BCJR-%2520NN%2520performance%2520significantly%2520degrades%2520in%2520both%250Asettings.%2520To%2520this%2520end%252C%2520we%2520devised%2520an%2520alternative%2520approach%2520for%2520data-driven%2520BCJR%250Adetection%2520based%2520on%2520the%2520unsupervised%2520learning%2520of%2520a%2520hidden%2520Markov%2520model%2520%2528HMM%2529.%250AThe%2520BCJR-HMM%2520allowed%2520us%2520to%2520optimize%2520both%2520the%2520likelihood%2520function%2520and%2520the%2520state%250Atransition%2520matrix%2520without%2520labeling.%2520Moreover%252C%2520we%2520demonstrated%2520the%2520viability%2520of%250Aa%2520hybrid%2520NN%2520and%2520HMM%2520BCJR%2520detection%2520where%2520NN%2520is%2520used%2520for%2520learning%2520the%250Alikelihoods%252C%2520while%2520the%2520state%2520transitions%2520are%2520optimized%2520via%2520HMM.%2520While%2520reducing%250Athe%2520required%2520prior%2520channel%2520knowledge%252C%2520the%2520examined%2520data-driven%2520detectors%2520with%250Alearned%2520trellis%2520state%2520transitions%2520achieve%2520bit%2520error%2520rates%2520close%2520to%2520the%2520optimal%250Afull%2520CSI-based%2520BCJR%252C%2520significantly%2520outperforming%2520detection%2520with%2520inaccurate%2520CSI.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2405.10814v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Data-Driven%20Symbol%20Detection%20for%20Intersymbol%20Interference%20Channels%20with%0A%20%20Bursty%20Impulsive%20Noise&entry.906535625=Boris%20Karanov%20and%20Chin-Hung%20Chen%20and%20Yan%20Wu%20and%20Alex%20Young%20and%20Wim%20van%20Houtum&entry.1292438233=%20%20We%20developed%20machine%20learning%20approaches%20for%20data-driven%20trellis-based%20soft%0Asymbol%20detection%20in%20coded%20transmission%20over%20intersymbol%20interference%20%28ISI%29%0Achannels%20in%20presence%20of%20bursty%20impulsive%20noise%20%28IN%29%2C%20for%20example%20encountered%20in%0Awireless%20digital%20broadcasting%20systems%20and%20vehicular%20communications.%20This%0Aenabled%20us%20to%20obtain%20optimized%20detectors%20based%20on%20the%20Bahl-Cocke-Jelinek-Raviv%0A%28BCJR%29%20algorithm%20while%20circumventing%20the%20use%20of%20full%20channel%20state%20information%0A%28CSI%29%20for%20computing%20likelihoods%20and%20trellis%20state%20transition%20probabilities.%0AFirst%2C%20we%20extended%20the%20application%20of%20the%20neural%20network%20%28NN%29-aided%20BCJR%2C%0Arecently%20proposed%20for%20ISI%20channels%20with%20additive%20white%20Gaussian%20noise%20%28AWGN%29.%0AAlthough%20suitable%20for%20estimating%20likelihoods%20via%20labeling%20of%20transmission%0Asequences%2C%20the%20BCJR-NN%20method%20does%20not%20provide%20a%20framework%20for%20learning%20the%0Atrellis%20state%20transitions.%20In%20addition%20to%20detection%20over%20the%20joint%20ISI%20and%20IN%0Astates%20we%20also%20focused%20on%20another%20scenario%20where%20trellis%20transitions%20are%20not%0Atrivial%3A%20detection%20for%20the%20ISI%20channel%20with%20AWGN%20with%20inaccurate%20knowledge%20of%0Athe%20channel%20memory%20at%20the%20receiver.%20Without%20access%20to%20the%20accurate%20state%0Atransition%20matrix%2C%20the%20BCJR-%20NN%20performance%20significantly%20degrades%20in%20both%0Asettings.%20To%20this%20end%2C%20we%20devised%20an%20alternative%20approach%20for%20data-driven%20BCJR%0Adetection%20based%20on%20the%20unsupervised%20learning%20of%20a%20hidden%20Markov%20model%20%28HMM%29.%0AThe%20BCJR-HMM%20allowed%20us%20to%20optimize%20both%20the%20likelihood%20function%20and%20the%20state%0Atransition%20matrix%20without%20labeling.%20Moreover%2C%20we%20demonstrated%20the%20viability%20of%0Aa%20hybrid%20NN%20and%20HMM%20BCJR%20detection%20where%20NN%20is%20used%20for%20learning%20the%0Alikelihoods%2C%20while%20the%20state%20transitions%20are%20optimized%20via%20HMM.%20While%20reducing%0Athe%20required%20prior%20channel%20knowledge%2C%20the%20examined%20data-driven%20detectors%20with%0Alearned%20trellis%20state%20transitions%20achieve%20bit%20error%20rates%20close%20to%20the%20optimal%0Afull%20CSI-based%20BCJR%2C%20significantly%20outperforming%20detection%20with%20inaccurate%20CSI.%0A&entry.1838667208=http%3A//arxiv.org/abs/2405.10814v1&entry.124074799=Read"},
{"title": "A Large-scale Multi Domain Leukemia Dataset for the White Blood Cells\n  Detection with Morphological Attributes for Explainability", "author": "Abdul Rehman and Talha Meraj and Aiman Mahmood Minhas and Ayisha Imran and Mohsen Ali and Waqas Sultani", "abstract": "  Earlier diagnosis of Leukemia can save thousands of lives annually. The\nprognosis of leukemia is challenging without the morphological information of\nWhite Blood Cells (WBC) and relies on the accessibility of expensive\nmicroscopes and the availability of hematologists to analyze Peripheral Blood\nSamples (PBS). Deep Learning based methods can be employed to assist\nhematologists. However, these algorithms require a large amount of labeled\ndata, which is not readily available. To overcome this limitation, we have\nacquired a realistic, generalized, and large dataset. To collect this\ncomprehensive dataset for real-world applications, two microscopes from two\ndifferent cost spectrums (high-cost HCM and low-cost LCM) are used for dataset\ncapturing at three magnifications (100x, 40x, 10x) through different sensors\n(high-end camera for HCM, middle-level camera for LCM and mobile-phone camera\nfor both). The high-sensor camera is 47 times more expensive than the\nmiddle-level camera and HCM is 17 times more expensive than LCM. In this\ncollection, using HCM at high resolution (100x), experienced hematologists\nannotated 10.3k WBC types (14) and artifacts, having 55k morphological labels\n(Cell Size, Nuclear Chromatin, Nuclear Shape, etc.) from 2.4k images of several\nPBS leukemia patients. Later on, these annotations are transferred to other 2\nmagnifications of HCM, and 3 magnifications of LCM, and on each camera captured\nimages. Along with the LeukemiaAttri dataset, we provide baselines over\nmultiple object detectors and Unsupervised Domain Adaptation (UDA) strategies,\nalong with morphological information-based attribute prediction. The dataset\nwill be publicly available after publication to facilitate the research in this\ndirection.\n", "link": "http://arxiv.org/abs/2405.10803v1", "date": "2024-05-17", "relevancy": 1.8345, "topK": [{"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.4745}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.4575}, {"title": "Breadcrumbs: Adversarial class-balanced sampling for long-tailed recognition", "link": "https://link.springer.com/chapter/10.1007/978-3-031-20053-3_37", "similarity": 0.4534}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20A%20Large-scale%20Multi%20Domain%20Leukemia%20Dataset%20for%20the%20White%20Blood%20Cells%0A%20%20Detection%20with%20Morphological%20Attributes%20for%20Explainability&body=Title%3A%20A%20Large-scale%20Multi%20Domain%20Leukemia%20Dataset%20for%20the%20White%20Blood%20Cells%0A%20%20Detection%20with%20Morphological%20Attributes%20for%20Explainability%0AAuthor%3A%20Abdul%20Rehman%20and%20Talha%20Meraj%20and%20Aiman%20Mahmood%20Minhas%20and%20Ayisha%20Imran%20and%20Mohsen%20Ali%20and%20Waqas%20Sultani%0AAbstract%3A%20%20%20Earlier%20diagnosis%20of%20Leukemia%20can%20save%20thousands%20of%20lives%20annually.%20The%0Aprognosis%20of%20leukemia%20is%20challenging%20without%20the%20morphological%20information%20of%0AWhite%20Blood%20Cells%20%28WBC%29%20and%20relies%20on%20the%20accessibility%20of%20expensive%0Amicroscopes%20and%20the%20availability%20of%20hematologists%20to%20analyze%20Peripheral%20Blood%0ASamples%20%28PBS%29.%20Deep%20Learning%20based%20methods%20can%20be%20employed%20to%20assist%0Ahematologists.%20However%2C%20these%20algorithms%20require%20a%20large%20amount%20of%20labeled%0Adata%2C%20which%20is%20not%20readily%20available.%20To%20overcome%20this%20limitation%2C%20we%20have%0Aacquired%20a%20realistic%2C%20generalized%2C%20and%20large%20dataset.%20To%20collect%20this%0Acomprehensive%20dataset%20for%20real-world%20applications%2C%20two%20microscopes%20from%20two%0Adifferent%20cost%20spectrums%20%28high-cost%20HCM%20and%20low-cost%20LCM%29%20are%20used%20for%20dataset%0Acapturing%20at%20three%20magnifications%20%28100x%2C%2040x%2C%2010x%29%20through%20different%20sensors%0A%28high-end%20camera%20for%20HCM%2C%20middle-level%20camera%20for%20LCM%20and%20mobile-phone%20camera%0Afor%20both%29.%20The%20high-sensor%20camera%20is%2047%20times%20more%20expensive%20than%20the%0Amiddle-level%20camera%20and%20HCM%20is%2017%20times%20more%20expensive%20than%20LCM.%20In%20this%0Acollection%2C%20using%20HCM%20at%20high%20resolution%20%28100x%29%2C%20experienced%20hematologists%0Aannotated%2010.3k%20WBC%20types%20%2814%29%20and%20artifacts%2C%20having%2055k%20morphological%20labels%0A%28Cell%20Size%2C%20Nuclear%20Chromatin%2C%20Nuclear%20Shape%2C%20etc.%29%20from%202.4k%20images%20of%20several%0APBS%20leukemia%20patients.%20Later%20on%2C%20these%20annotations%20are%20transferred%20to%20other%202%0Amagnifications%20of%20HCM%2C%20and%203%20magnifications%20of%20LCM%2C%20and%20on%20each%20camera%20captured%0Aimages.%20Along%20with%20the%20LeukemiaAttri%20dataset%2C%20we%20provide%20baselines%20over%0Amultiple%20object%20detectors%20and%20Unsupervised%20Domain%20Adaptation%20%28UDA%29%20strategies%2C%0Aalong%20with%20morphological%20information-based%20attribute%20prediction.%20The%20dataset%0Awill%20be%20publicly%20available%20after%20publication%20to%20facilitate%20the%20research%20in%20this%0Adirection.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2405.10803v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DA%2520Large-scale%2520Multi%2520Domain%2520Leukemia%2520Dataset%2520for%2520the%2520White%2520Blood%2520Cells%250A%2520%2520Detection%2520with%2520Morphological%2520Attributes%2520for%2520Explainability%26entry.906535625%3DAbdul%2520Rehman%2520and%2520Talha%2520Meraj%2520and%2520Aiman%2520Mahmood%2520Minhas%2520and%2520Ayisha%2520Imran%2520and%2520Mohsen%2520Ali%2520and%2520Waqas%2520Sultani%26entry.1292438233%3D%2520%2520Earlier%2520diagnosis%2520of%2520Leukemia%2520can%2520save%2520thousands%2520of%2520lives%2520annually.%2520The%250Aprognosis%2520of%2520leukemia%2520is%2520challenging%2520without%2520the%2520morphological%2520information%2520of%250AWhite%2520Blood%2520Cells%2520%2528WBC%2529%2520and%2520relies%2520on%2520the%2520accessibility%2520of%2520expensive%250Amicroscopes%2520and%2520the%2520availability%2520of%2520hematologists%2520to%2520analyze%2520Peripheral%2520Blood%250ASamples%2520%2528PBS%2529.%2520Deep%2520Learning%2520based%2520methods%2520can%2520be%2520employed%2520to%2520assist%250Ahematologists.%2520However%252C%2520these%2520algorithms%2520require%2520a%2520large%2520amount%2520of%2520labeled%250Adata%252C%2520which%2520is%2520not%2520readily%2520available.%2520To%2520overcome%2520this%2520limitation%252C%2520we%2520have%250Aacquired%2520a%2520realistic%252C%2520generalized%252C%2520and%2520large%2520dataset.%2520To%2520collect%2520this%250Acomprehensive%2520dataset%2520for%2520real-world%2520applications%252C%2520two%2520microscopes%2520from%2520two%250Adifferent%2520cost%2520spectrums%2520%2528high-cost%2520HCM%2520and%2520low-cost%2520LCM%2529%2520are%2520used%2520for%2520dataset%250Acapturing%2520at%2520three%2520magnifications%2520%2528100x%252C%252040x%252C%252010x%2529%2520through%2520different%2520sensors%250A%2528high-end%2520camera%2520for%2520HCM%252C%2520middle-level%2520camera%2520for%2520LCM%2520and%2520mobile-phone%2520camera%250Afor%2520both%2529.%2520The%2520high-sensor%2520camera%2520is%252047%2520times%2520more%2520expensive%2520than%2520the%250Amiddle-level%2520camera%2520and%2520HCM%2520is%252017%2520times%2520more%2520expensive%2520than%2520LCM.%2520In%2520this%250Acollection%252C%2520using%2520HCM%2520at%2520high%2520resolution%2520%2528100x%2529%252C%2520experienced%2520hematologists%250Aannotated%252010.3k%2520WBC%2520types%2520%252814%2529%2520and%2520artifacts%252C%2520having%252055k%2520morphological%2520labels%250A%2528Cell%2520Size%252C%2520Nuclear%2520Chromatin%252C%2520Nuclear%2520Shape%252C%2520etc.%2529%2520from%25202.4k%2520images%2520of%2520several%250APBS%2520leukemia%2520patients.%2520Later%2520on%252C%2520these%2520annotations%2520are%2520transferred%2520to%2520other%25202%250Amagnifications%2520of%2520HCM%252C%2520and%25203%2520magnifications%2520of%2520LCM%252C%2520and%2520on%2520each%2520camera%2520captured%250Aimages.%2520Along%2520with%2520the%2520LeukemiaAttri%2520dataset%252C%2520we%2520provide%2520baselines%2520over%250Amultiple%2520object%2520detectors%2520and%2520Unsupervised%2520Domain%2520Adaptation%2520%2528UDA%2529%2520strategies%252C%250Aalong%2520with%2520morphological%2520information-based%2520attribute%2520prediction.%2520The%2520dataset%250Awill%2520be%2520publicly%2520available%2520after%2520publication%2520to%2520facilitate%2520the%2520research%2520in%2520this%250Adirection.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2405.10803v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=A%20Large-scale%20Multi%20Domain%20Leukemia%20Dataset%20for%20the%20White%20Blood%20Cells%0A%20%20Detection%20with%20Morphological%20Attributes%20for%20Explainability&entry.906535625=Abdul%20Rehman%20and%20Talha%20Meraj%20and%20Aiman%20Mahmood%20Minhas%20and%20Ayisha%20Imran%20and%20Mohsen%20Ali%20and%20Waqas%20Sultani&entry.1292438233=%20%20Earlier%20diagnosis%20of%20Leukemia%20can%20save%20thousands%20of%20lives%20annually.%20The%0Aprognosis%20of%20leukemia%20is%20challenging%20without%20the%20morphological%20information%20of%0AWhite%20Blood%20Cells%20%28WBC%29%20and%20relies%20on%20the%20accessibility%20of%20expensive%0Amicroscopes%20and%20the%20availability%20of%20hematologists%20to%20analyze%20Peripheral%20Blood%0ASamples%20%28PBS%29.%20Deep%20Learning%20based%20methods%20can%20be%20employed%20to%20assist%0Ahematologists.%20However%2C%20these%20algorithms%20require%20a%20large%20amount%20of%20labeled%0Adata%2C%20which%20is%20not%20readily%20available.%20To%20overcome%20this%20limitation%2C%20we%20have%0Aacquired%20a%20realistic%2C%20generalized%2C%20and%20large%20dataset.%20To%20collect%20this%0Acomprehensive%20dataset%20for%20real-world%20applications%2C%20two%20microscopes%20from%20two%0Adifferent%20cost%20spectrums%20%28high-cost%20HCM%20and%20low-cost%20LCM%29%20are%20used%20for%20dataset%0Acapturing%20at%20three%20magnifications%20%28100x%2C%2040x%2C%2010x%29%20through%20different%20sensors%0A%28high-end%20camera%20for%20HCM%2C%20middle-level%20camera%20for%20LCM%20and%20mobile-phone%20camera%0Afor%20both%29.%20The%20high-sensor%20camera%20is%2047%20times%20more%20expensive%20than%20the%0Amiddle-level%20camera%20and%20HCM%20is%2017%20times%20more%20expensive%20than%20LCM.%20In%20this%0Acollection%2C%20using%20HCM%20at%20high%20resolution%20%28100x%29%2C%20experienced%20hematologists%0Aannotated%2010.3k%20WBC%20types%20%2814%29%20and%20artifacts%2C%20having%2055k%20morphological%20labels%0A%28Cell%20Size%2C%20Nuclear%20Chromatin%2C%20Nuclear%20Shape%2C%20etc.%29%20from%202.4k%20images%20of%20several%0APBS%20leukemia%20patients.%20Later%20on%2C%20these%20annotations%20are%20transferred%20to%20other%202%0Amagnifications%20of%20HCM%2C%20and%203%20magnifications%20of%20LCM%2C%20and%20on%20each%20camera%20captured%0Aimages.%20Along%20with%20the%20LeukemiaAttri%20dataset%2C%20we%20provide%20baselines%20over%0Amultiple%20object%20detectors%20and%20Unsupervised%20Domain%20Adaptation%20%28UDA%29%20strategies%2C%0Aalong%20with%20morphological%20information-based%20attribute%20prediction.%20The%20dataset%0Awill%20be%20publicly%20available%20after%20publication%20to%20facilitate%20the%20research%20in%20this%0Adirection.%0A&entry.1838667208=http%3A//arxiv.org/abs/2405.10803v1&entry.124074799=Read"},
{"title": "COGNET-MD, an evaluation framework and dataset for Large Language Model\n  benchmarks in the medical domain", "author": "Dimitrios P. Panagoulias and Persephone Papatheodosiou and Anastasios P. Palamidas and Mattheos Sanoudos and Evridiki Tsoureli-Nikita and Maria Virvou and George A. Tsihrintzis", "abstract": "  Large Language Models (LLMs) constitute a breakthrough state-of-the-art\nArtificial Intelligence (AI) technology which is rapidly evolving and promises\nto aid in medical diagnosis either by assisting doctors or by simulating a\ndoctor's workflow in more advanced and complex implementations. In this\ntechnical paper, we outline Cognitive Network Evaluation Toolkit for Medical\nDomains (COGNET-MD), which constitutes a novel benchmark for LLM evaluation in\nthe medical domain. Specifically, we propose a scoring-framework with increased\ndifficulty to assess the ability of LLMs in interpreting medical text. The\nproposed framework is accompanied with a database of Multiple Choice Quizzes\n(MCQs). To ensure alignment with current medical trends and enhance safety,\nusefulness, and applicability, these MCQs have been constructed in\ncollaboration with several associated medical experts in various medical\ndomains and are characterized by varying degrees of difficulty. The current\n(first) version of the database includes the medical domains of Psychiatry,\nDentistry, Pulmonology, Dermatology and Endocrinology, but it will be\ncontinuously extended and expanded to include additional medical domains.\n", "link": "http://arxiv.org/abs/2405.10893v1", "date": "2024-05-17", "relevancy": 1.8338, "topK": [{"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.5081}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.4585}, {"title": "VirtualModel: Generating Object-ID-retentive Human-object Interaction\n  Image by Diffusion Model for E-commerce Marketing", "link": "http://arxiv.org/abs/2405.09985v1", "similarity": 0.4385}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20COGNET-MD%2C%20an%20evaluation%20framework%20and%20dataset%20for%20Large%20Language%20Model%0A%20%20benchmarks%20in%20the%20medical%20domain&body=Title%3A%20COGNET-MD%2C%20an%20evaluation%20framework%20and%20dataset%20for%20Large%20Language%20Model%0A%20%20benchmarks%20in%20the%20medical%20domain%0AAuthor%3A%20Dimitrios%20P.%20Panagoulias%20and%20Persephone%20Papatheodosiou%20and%20Anastasios%20P.%20Palamidas%20and%20Mattheos%20Sanoudos%20and%20Evridiki%20Tsoureli-Nikita%20and%20Maria%20Virvou%20and%20George%20A.%20Tsihrintzis%0AAbstract%3A%20%20%20Large%20Language%20Models%20%28LLMs%29%20constitute%20a%20breakthrough%20state-of-the-art%0AArtificial%20Intelligence%20%28AI%29%20technology%20which%20is%20rapidly%20evolving%20and%20promises%0Ato%20aid%20in%20medical%20diagnosis%20either%20by%20assisting%20doctors%20or%20by%20simulating%20a%0Adoctor%27s%20workflow%20in%20more%20advanced%20and%20complex%20implementations.%20In%20this%0Atechnical%20paper%2C%20we%20outline%20Cognitive%20Network%20Evaluation%20Toolkit%20for%20Medical%0ADomains%20%28COGNET-MD%29%2C%20which%20constitutes%20a%20novel%20benchmark%20for%20LLM%20evaluation%20in%0Athe%20medical%20domain.%20Specifically%2C%20we%20propose%20a%20scoring-framework%20with%20increased%0Adifficulty%20to%20assess%20the%20ability%20of%20LLMs%20in%20interpreting%20medical%20text.%20The%0Aproposed%20framework%20is%20accompanied%20with%20a%20database%20of%20Multiple%20Choice%20Quizzes%0A%28MCQs%29.%20To%20ensure%20alignment%20with%20current%20medical%20trends%20and%20enhance%20safety%2C%0Ausefulness%2C%20and%20applicability%2C%20these%20MCQs%20have%20been%20constructed%20in%0Acollaboration%20with%20several%20associated%20medical%20experts%20in%20various%20medical%0Adomains%20and%20are%20characterized%20by%20varying%20degrees%20of%20difficulty.%20The%20current%0A%28first%29%20version%20of%20the%20database%20includes%20the%20medical%20domains%20of%20Psychiatry%2C%0ADentistry%2C%20Pulmonology%2C%20Dermatology%20and%20Endocrinology%2C%20but%20it%20will%20be%0Acontinuously%20extended%20and%20expanded%20to%20include%20additional%20medical%20domains.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2405.10893v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DCOGNET-MD%252C%2520an%2520evaluation%2520framework%2520and%2520dataset%2520for%2520Large%2520Language%2520Model%250A%2520%2520benchmarks%2520in%2520the%2520medical%2520domain%26entry.906535625%3DDimitrios%2520P.%2520Panagoulias%2520and%2520Persephone%2520Papatheodosiou%2520and%2520Anastasios%2520P.%2520Palamidas%2520and%2520Mattheos%2520Sanoudos%2520and%2520Evridiki%2520Tsoureli-Nikita%2520and%2520Maria%2520Virvou%2520and%2520George%2520A.%2520Tsihrintzis%26entry.1292438233%3D%2520%2520Large%2520Language%2520Models%2520%2528LLMs%2529%2520constitute%2520a%2520breakthrough%2520state-of-the-art%250AArtificial%2520Intelligence%2520%2528AI%2529%2520technology%2520which%2520is%2520rapidly%2520evolving%2520and%2520promises%250Ato%2520aid%2520in%2520medical%2520diagnosis%2520either%2520by%2520assisting%2520doctors%2520or%2520by%2520simulating%2520a%250Adoctor%2527s%2520workflow%2520in%2520more%2520advanced%2520and%2520complex%2520implementations.%2520In%2520this%250Atechnical%2520paper%252C%2520we%2520outline%2520Cognitive%2520Network%2520Evaluation%2520Toolkit%2520for%2520Medical%250ADomains%2520%2528COGNET-MD%2529%252C%2520which%2520constitutes%2520a%2520novel%2520benchmark%2520for%2520LLM%2520evaluation%2520in%250Athe%2520medical%2520domain.%2520Specifically%252C%2520we%2520propose%2520a%2520scoring-framework%2520with%2520increased%250Adifficulty%2520to%2520assess%2520the%2520ability%2520of%2520LLMs%2520in%2520interpreting%2520medical%2520text.%2520The%250Aproposed%2520framework%2520is%2520accompanied%2520with%2520a%2520database%2520of%2520Multiple%2520Choice%2520Quizzes%250A%2528MCQs%2529.%2520To%2520ensure%2520alignment%2520with%2520current%2520medical%2520trends%2520and%2520enhance%2520safety%252C%250Ausefulness%252C%2520and%2520applicability%252C%2520these%2520MCQs%2520have%2520been%2520constructed%2520in%250Acollaboration%2520with%2520several%2520associated%2520medical%2520experts%2520in%2520various%2520medical%250Adomains%2520and%2520are%2520characterized%2520by%2520varying%2520degrees%2520of%2520difficulty.%2520The%2520current%250A%2528first%2529%2520version%2520of%2520the%2520database%2520includes%2520the%2520medical%2520domains%2520of%2520Psychiatry%252C%250ADentistry%252C%2520Pulmonology%252C%2520Dermatology%2520and%2520Endocrinology%252C%2520but%2520it%2520will%2520be%250Acontinuously%2520extended%2520and%2520expanded%2520to%2520include%2520additional%2520medical%2520domains.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2405.10893v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=COGNET-MD%2C%20an%20evaluation%20framework%20and%20dataset%20for%20Large%20Language%20Model%0A%20%20benchmarks%20in%20the%20medical%20domain&entry.906535625=Dimitrios%20P.%20Panagoulias%20and%20Persephone%20Papatheodosiou%20and%20Anastasios%20P.%20Palamidas%20and%20Mattheos%20Sanoudos%20and%20Evridiki%20Tsoureli-Nikita%20and%20Maria%20Virvou%20and%20George%20A.%20Tsihrintzis&entry.1292438233=%20%20Large%20Language%20Models%20%28LLMs%29%20constitute%20a%20breakthrough%20state-of-the-art%0AArtificial%20Intelligence%20%28AI%29%20technology%20which%20is%20rapidly%20evolving%20and%20promises%0Ato%20aid%20in%20medical%20diagnosis%20either%20by%20assisting%20doctors%20or%20by%20simulating%20a%0Adoctor%27s%20workflow%20in%20more%20advanced%20and%20complex%20implementations.%20In%20this%0Atechnical%20paper%2C%20we%20outline%20Cognitive%20Network%20Evaluation%20Toolkit%20for%20Medical%0ADomains%20%28COGNET-MD%29%2C%20which%20constitutes%20a%20novel%20benchmark%20for%20LLM%20evaluation%20in%0Athe%20medical%20domain.%20Specifically%2C%20we%20propose%20a%20scoring-framework%20with%20increased%0Adifficulty%20to%20assess%20the%20ability%20of%20LLMs%20in%20interpreting%20medical%20text.%20The%0Aproposed%20framework%20is%20accompanied%20with%20a%20database%20of%20Multiple%20Choice%20Quizzes%0A%28MCQs%29.%20To%20ensure%20alignment%20with%20current%20medical%20trends%20and%20enhance%20safety%2C%0Ausefulness%2C%20and%20applicability%2C%20these%20MCQs%20have%20been%20constructed%20in%0Acollaboration%20with%20several%20associated%20medical%20experts%20in%20various%20medical%0Adomains%20and%20are%20characterized%20by%20varying%20degrees%20of%20difficulty.%20The%20current%0A%28first%29%20version%20of%20the%20database%20includes%20the%20medical%20domains%20of%20Psychiatry%2C%0ADentistry%2C%20Pulmonology%2C%20Dermatology%20and%20Endocrinology%2C%20but%20it%20will%20be%0Acontinuously%20extended%20and%20expanded%20to%20include%20additional%20medical%20domains.%0A&entry.1838667208=http%3A//arxiv.org/abs/2405.10893v1&entry.124074799=Read"},
{"title": "Boosting Few-Pixel Robustness Verification via Covering Verification\n  Designs", "author": "Yuval Shapira and Naor Wiesel and Shahar Shabelman and Dana Drachsler-Cohen", "abstract": "  Proving local robustness is crucial to increase the reliability of neural\nnetworks. While many verifiers prove robustness in $L_\\infty$ $\\epsilon$-balls,\nvery little work deals with robustness verification in $L_0$ $\\epsilon$-balls,\ncapturing robustness to few pixel attacks. This verification introduces a\ncombinatorial challenge, because the space of pixels to perturb is discrete and\nof exponential size. A previous work relies on covering designs to identify\nsets for defining $L_\\infty$ neighborhoods, which if proven robust imply that\nthe $L_0$ $\\epsilon$-ball is robust. However, the number of neighborhoods to\nverify remains very high, leading to a high analysis time. We propose covering\nverification designs, a combinatorial design that tailors effective but\nanalysis-incompatible coverings to $L_0$ robustness verification. The challenge\nis that computing a covering verification design introduces a high time and\nmemory overhead, which is intensified in our setting, where multiple candidate\ncoverings are required to identify how to reduce the overall analysis time. We\nintroduce CoVerD, an $L_0$ robustness verifier that selects between different\ncandidate coverings without constructing them, but by predicting their block\nsize distribution. This prediction relies on a theorem providing closed-form\nexpressions for the mean and variance of this distribution. CoVerD constructs\nthe chosen covering verification design on-the-fly, while keeping the memory\nconsumption minimal and enabling to parallelize the analysis. The experimental\nresults show that CoVerD reduces the verification time on average by up to 5.1x\ncompared to prior work and that it scales to larger $L_0$ $\\epsilon$-balls.\n", "link": "http://arxiv.org/abs/2405.10924v1", "date": "2024-05-17", "relevancy": 1.8328, "topK": [{"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.4632}, {"title": "Tuning-Free Noise Rectification for High Fidelity Image-to-Video\n  Generation", "link": "http://arxiv.org/abs/2403.02827v1", "similarity": 0.455}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.4537}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Boosting%20Few-Pixel%20Robustness%20Verification%20via%20Covering%20Verification%0A%20%20Designs&body=Title%3A%20Boosting%20Few-Pixel%20Robustness%20Verification%20via%20Covering%20Verification%0A%20%20Designs%0AAuthor%3A%20Yuval%20Shapira%20and%20Naor%20Wiesel%20and%20Shahar%20Shabelman%20and%20Dana%20Drachsler-Cohen%0AAbstract%3A%20%20%20Proving%20local%20robustness%20is%20crucial%20to%20increase%20the%20reliability%20of%20neural%0Anetworks.%20While%20many%20verifiers%20prove%20robustness%20in%20%24L_%5Cinfty%24%20%24%5Cepsilon%24-balls%2C%0Avery%20little%20work%20deals%20with%20robustness%20verification%20in%20%24L_0%24%20%24%5Cepsilon%24-balls%2C%0Acapturing%20robustness%20to%20few%20pixel%20attacks.%20This%20verification%20introduces%20a%0Acombinatorial%20challenge%2C%20because%20the%20space%20of%20pixels%20to%20perturb%20is%20discrete%20and%0Aof%20exponential%20size.%20A%20previous%20work%20relies%20on%20covering%20designs%20to%20identify%0Asets%20for%20defining%20%24L_%5Cinfty%24%20neighborhoods%2C%20which%20if%20proven%20robust%20imply%20that%0Athe%20%24L_0%24%20%24%5Cepsilon%24-ball%20is%20robust.%20However%2C%20the%20number%20of%20neighborhoods%20to%0Averify%20remains%20very%20high%2C%20leading%20to%20a%20high%20analysis%20time.%20We%20propose%20covering%0Averification%20designs%2C%20a%20combinatorial%20design%20that%20tailors%20effective%20but%0Aanalysis-incompatible%20coverings%20to%20%24L_0%24%20robustness%20verification.%20The%20challenge%0Ais%20that%20computing%20a%20covering%20verification%20design%20introduces%20a%20high%20time%20and%0Amemory%20overhead%2C%20which%20is%20intensified%20in%20our%20setting%2C%20where%20multiple%20candidate%0Acoverings%20are%20required%20to%20identify%20how%20to%20reduce%20the%20overall%20analysis%20time.%20We%0Aintroduce%20CoVerD%2C%20an%20%24L_0%24%20robustness%20verifier%20that%20selects%20between%20different%0Acandidate%20coverings%20without%20constructing%20them%2C%20but%20by%20predicting%20their%20block%0Asize%20distribution.%20This%20prediction%20relies%20on%20a%20theorem%20providing%20closed-form%0Aexpressions%20for%20the%20mean%20and%20variance%20of%20this%20distribution.%20CoVerD%20constructs%0Athe%20chosen%20covering%20verification%20design%20on-the-fly%2C%20while%20keeping%20the%20memory%0Aconsumption%20minimal%20and%20enabling%20to%20parallelize%20the%20analysis.%20The%20experimental%0Aresults%20show%20that%20CoVerD%20reduces%20the%20verification%20time%20on%20average%20by%20up%20to%205.1x%0Acompared%20to%20prior%20work%20and%20that%20it%20scales%20to%20larger%20%24L_0%24%20%24%5Cepsilon%24-balls.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2405.10924v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DBoosting%2520Few-Pixel%2520Robustness%2520Verification%2520via%2520Covering%2520Verification%250A%2520%2520Designs%26entry.906535625%3DYuval%2520Shapira%2520and%2520Naor%2520Wiesel%2520and%2520Shahar%2520Shabelman%2520and%2520Dana%2520Drachsler-Cohen%26entry.1292438233%3D%2520%2520Proving%2520local%2520robustness%2520is%2520crucial%2520to%2520increase%2520the%2520reliability%2520of%2520neural%250Anetworks.%2520While%2520many%2520verifiers%2520prove%2520robustness%2520in%2520%2524L_%255Cinfty%2524%2520%2524%255Cepsilon%2524-balls%252C%250Avery%2520little%2520work%2520deals%2520with%2520robustness%2520verification%2520in%2520%2524L_0%2524%2520%2524%255Cepsilon%2524-balls%252C%250Acapturing%2520robustness%2520to%2520few%2520pixel%2520attacks.%2520This%2520verification%2520introduces%2520a%250Acombinatorial%2520challenge%252C%2520because%2520the%2520space%2520of%2520pixels%2520to%2520perturb%2520is%2520discrete%2520and%250Aof%2520exponential%2520size.%2520A%2520previous%2520work%2520relies%2520on%2520covering%2520designs%2520to%2520identify%250Asets%2520for%2520defining%2520%2524L_%255Cinfty%2524%2520neighborhoods%252C%2520which%2520if%2520proven%2520robust%2520imply%2520that%250Athe%2520%2524L_0%2524%2520%2524%255Cepsilon%2524-ball%2520is%2520robust.%2520However%252C%2520the%2520number%2520of%2520neighborhoods%2520to%250Averify%2520remains%2520very%2520high%252C%2520leading%2520to%2520a%2520high%2520analysis%2520time.%2520We%2520propose%2520covering%250Averification%2520designs%252C%2520a%2520combinatorial%2520design%2520that%2520tailors%2520effective%2520but%250Aanalysis-incompatible%2520coverings%2520to%2520%2524L_0%2524%2520robustness%2520verification.%2520The%2520challenge%250Ais%2520that%2520computing%2520a%2520covering%2520verification%2520design%2520introduces%2520a%2520high%2520time%2520and%250Amemory%2520overhead%252C%2520which%2520is%2520intensified%2520in%2520our%2520setting%252C%2520where%2520multiple%2520candidate%250Acoverings%2520are%2520required%2520to%2520identify%2520how%2520to%2520reduce%2520the%2520overall%2520analysis%2520time.%2520We%250Aintroduce%2520CoVerD%252C%2520an%2520%2524L_0%2524%2520robustness%2520verifier%2520that%2520selects%2520between%2520different%250Acandidate%2520coverings%2520without%2520constructing%2520them%252C%2520but%2520by%2520predicting%2520their%2520block%250Asize%2520distribution.%2520This%2520prediction%2520relies%2520on%2520a%2520theorem%2520providing%2520closed-form%250Aexpressions%2520for%2520the%2520mean%2520and%2520variance%2520of%2520this%2520distribution.%2520CoVerD%2520constructs%250Athe%2520chosen%2520covering%2520verification%2520design%2520on-the-fly%252C%2520while%2520keeping%2520the%2520memory%250Aconsumption%2520minimal%2520and%2520enabling%2520to%2520parallelize%2520the%2520analysis.%2520The%2520experimental%250Aresults%2520show%2520that%2520CoVerD%2520reduces%2520the%2520verification%2520time%2520on%2520average%2520by%2520up%2520to%25205.1x%250Acompared%2520to%2520prior%2520work%2520and%2520that%2520it%2520scales%2520to%2520larger%2520%2524L_0%2524%2520%2524%255Cepsilon%2524-balls.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2405.10924v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Boosting%20Few-Pixel%20Robustness%20Verification%20via%20Covering%20Verification%0A%20%20Designs&entry.906535625=Yuval%20Shapira%20and%20Naor%20Wiesel%20and%20Shahar%20Shabelman%20and%20Dana%20Drachsler-Cohen&entry.1292438233=%20%20Proving%20local%20robustness%20is%20crucial%20to%20increase%20the%20reliability%20of%20neural%0Anetworks.%20While%20many%20verifiers%20prove%20robustness%20in%20%24L_%5Cinfty%24%20%24%5Cepsilon%24-balls%2C%0Avery%20little%20work%20deals%20with%20robustness%20verification%20in%20%24L_0%24%20%24%5Cepsilon%24-balls%2C%0Acapturing%20robustness%20to%20few%20pixel%20attacks.%20This%20verification%20introduces%20a%0Acombinatorial%20challenge%2C%20because%20the%20space%20of%20pixels%20to%20perturb%20is%20discrete%20and%0Aof%20exponential%20size.%20A%20previous%20work%20relies%20on%20covering%20designs%20to%20identify%0Asets%20for%20defining%20%24L_%5Cinfty%24%20neighborhoods%2C%20which%20if%20proven%20robust%20imply%20that%0Athe%20%24L_0%24%20%24%5Cepsilon%24-ball%20is%20robust.%20However%2C%20the%20number%20of%20neighborhoods%20to%0Averify%20remains%20very%20high%2C%20leading%20to%20a%20high%20analysis%20time.%20We%20propose%20covering%0Averification%20designs%2C%20a%20combinatorial%20design%20that%20tailors%20effective%20but%0Aanalysis-incompatible%20coverings%20to%20%24L_0%24%20robustness%20verification.%20The%20challenge%0Ais%20that%20computing%20a%20covering%20verification%20design%20introduces%20a%20high%20time%20and%0Amemory%20overhead%2C%20which%20is%20intensified%20in%20our%20setting%2C%20where%20multiple%20candidate%0Acoverings%20are%20required%20to%20identify%20how%20to%20reduce%20the%20overall%20analysis%20time.%20We%0Aintroduce%20CoVerD%2C%20an%20%24L_0%24%20robustness%20verifier%20that%20selects%20between%20different%0Acandidate%20coverings%20without%20constructing%20them%2C%20but%20by%20predicting%20their%20block%0Asize%20distribution.%20This%20prediction%20relies%20on%20a%20theorem%20providing%20closed-form%0Aexpressions%20for%20the%20mean%20and%20variance%20of%20this%20distribution.%20CoVerD%20constructs%0Athe%20chosen%20covering%20verification%20design%20on-the-fly%2C%20while%20keeping%20the%20memory%0Aconsumption%20minimal%20and%20enabling%20to%20parallelize%20the%20analysis.%20The%20experimental%0Aresults%20show%20that%20CoVerD%20reduces%20the%20verification%20time%20on%20average%20by%20up%20to%205.1x%0Acompared%20to%20prior%20work%20and%20that%20it%20scales%20to%20larger%20%24L_0%24%20%24%5Cepsilon%24-balls.%0A&entry.1838667208=http%3A//arxiv.org/abs/2405.10924v1&entry.124074799=Read"},
{"title": "A Survey on Large Language Models with Multilingualism: Recent Advances\n  and New Frontiers", "author": "Kaiyu Huang and Fengran Mo and Hongliang Li and You Li and Yuanchi Zhang and Weijian Yi and Yulong Mao and Jinchen Liu and Yuzhuang Xu and Jinan Xu and Jian-Yun Nie and Yang Liu", "abstract": "  The rapid development of Large Language Models (LLMs) demonstrates remarkable\nmultilingual capabilities in natural language processing, attracting global\nattention in both academia and industry. To mitigate potential discrimination\nand enhance the overall usability and accessibility for diverse language user\ngroups, it is important for the development of language-fair technology.\nDespite the breakthroughs of LLMs, the investigation into the multilingual\nscenario remains insufficient, where a comprehensive survey to summarize recent\napproaches, developments, limitations, and potential solutions is desirable. To\nthis end, we provide a survey with multiple perspectives on the utilization of\nLLMs in the multilingual scenario. We first rethink the transitions between\nprevious and current research on pre-trained language models. Then we introduce\nseveral perspectives on the multilingualism of LLMs, including training and\ninference methods, model security, multi-domain with language culture, and\nusage of datasets. We also discuss the major challenges that arise in these\naspects, along with possible solutions. Besides, we highlight future research\ndirections that aim at further enhancing LLMs with multilingualism. The survey\naims to help the research community address multilingual problems and provide a\ncomprehensive understanding of the core concepts, key techniques, and latest\ndevelopments in multilingual natural language processing based on LLMs.\n", "link": "http://arxiv.org/abs/2405.10936v1", "date": "2024-05-17", "relevancy": 1.8297, "topK": [{"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.4753}, {"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.4594}, {"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.4387}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20A%20Survey%20on%20Large%20Language%20Models%20with%20Multilingualism%3A%20Recent%20Advances%0A%20%20and%20New%20Frontiers&body=Title%3A%20A%20Survey%20on%20Large%20Language%20Models%20with%20Multilingualism%3A%20Recent%20Advances%0A%20%20and%20New%20Frontiers%0AAuthor%3A%20Kaiyu%20Huang%20and%20Fengran%20Mo%20and%20Hongliang%20Li%20and%20You%20Li%20and%20Yuanchi%20Zhang%20and%20Weijian%20Yi%20and%20Yulong%20Mao%20and%20Jinchen%20Liu%20and%20Yuzhuang%20Xu%20and%20Jinan%20Xu%20and%20Jian-Yun%20Nie%20and%20Yang%20Liu%0AAbstract%3A%20%20%20The%20rapid%20development%20of%20Large%20Language%20Models%20%28LLMs%29%20demonstrates%20remarkable%0Amultilingual%20capabilities%20in%20natural%20language%20processing%2C%20attracting%20global%0Aattention%20in%20both%20academia%20and%20industry.%20To%20mitigate%20potential%20discrimination%0Aand%20enhance%20the%20overall%20usability%20and%20accessibility%20for%20diverse%20language%20user%0Agroups%2C%20it%20is%20important%20for%20the%20development%20of%20language-fair%20technology.%0ADespite%20the%20breakthroughs%20of%20LLMs%2C%20the%20investigation%20into%20the%20multilingual%0Ascenario%20remains%20insufficient%2C%20where%20a%20comprehensive%20survey%20to%20summarize%20recent%0Aapproaches%2C%20developments%2C%20limitations%2C%20and%20potential%20solutions%20is%20desirable.%20To%0Athis%20end%2C%20we%20provide%20a%20survey%20with%20multiple%20perspectives%20on%20the%20utilization%20of%0ALLMs%20in%20the%20multilingual%20scenario.%20We%20first%20rethink%20the%20transitions%20between%0Aprevious%20and%20current%20research%20on%20pre-trained%20language%20models.%20Then%20we%20introduce%0Aseveral%20perspectives%20on%20the%20multilingualism%20of%20LLMs%2C%20including%20training%20and%0Ainference%20methods%2C%20model%20security%2C%20multi-domain%20with%20language%20culture%2C%20and%0Ausage%20of%20datasets.%20We%20also%20discuss%20the%20major%20challenges%20that%20arise%20in%20these%0Aaspects%2C%20along%20with%20possible%20solutions.%20Besides%2C%20we%20highlight%20future%20research%0Adirections%20that%20aim%20at%20further%20enhancing%20LLMs%20with%20multilingualism.%20The%20survey%0Aaims%20to%20help%20the%20research%20community%20address%20multilingual%20problems%20and%20provide%20a%0Acomprehensive%20understanding%20of%20the%20core%20concepts%2C%20key%20techniques%2C%20and%20latest%0Adevelopments%20in%20multilingual%20natural%20language%20processing%20based%20on%20LLMs.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2405.10936v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DA%2520Survey%2520on%2520Large%2520Language%2520Models%2520with%2520Multilingualism%253A%2520Recent%2520Advances%250A%2520%2520and%2520New%2520Frontiers%26entry.906535625%3DKaiyu%2520Huang%2520and%2520Fengran%2520Mo%2520and%2520Hongliang%2520Li%2520and%2520You%2520Li%2520and%2520Yuanchi%2520Zhang%2520and%2520Weijian%2520Yi%2520and%2520Yulong%2520Mao%2520and%2520Jinchen%2520Liu%2520and%2520Yuzhuang%2520Xu%2520and%2520Jinan%2520Xu%2520and%2520Jian-Yun%2520Nie%2520and%2520Yang%2520Liu%26entry.1292438233%3D%2520%2520The%2520rapid%2520development%2520of%2520Large%2520Language%2520Models%2520%2528LLMs%2529%2520demonstrates%2520remarkable%250Amultilingual%2520capabilities%2520in%2520natural%2520language%2520processing%252C%2520attracting%2520global%250Aattention%2520in%2520both%2520academia%2520and%2520industry.%2520To%2520mitigate%2520potential%2520discrimination%250Aand%2520enhance%2520the%2520overall%2520usability%2520and%2520accessibility%2520for%2520diverse%2520language%2520user%250Agroups%252C%2520it%2520is%2520important%2520for%2520the%2520development%2520of%2520language-fair%2520technology.%250ADespite%2520the%2520breakthroughs%2520of%2520LLMs%252C%2520the%2520investigation%2520into%2520the%2520multilingual%250Ascenario%2520remains%2520insufficient%252C%2520where%2520a%2520comprehensive%2520survey%2520to%2520summarize%2520recent%250Aapproaches%252C%2520developments%252C%2520limitations%252C%2520and%2520potential%2520solutions%2520is%2520desirable.%2520To%250Athis%2520end%252C%2520we%2520provide%2520a%2520survey%2520with%2520multiple%2520perspectives%2520on%2520the%2520utilization%2520of%250ALLMs%2520in%2520the%2520multilingual%2520scenario.%2520We%2520first%2520rethink%2520the%2520transitions%2520between%250Aprevious%2520and%2520current%2520research%2520on%2520pre-trained%2520language%2520models.%2520Then%2520we%2520introduce%250Aseveral%2520perspectives%2520on%2520the%2520multilingualism%2520of%2520LLMs%252C%2520including%2520training%2520and%250Ainference%2520methods%252C%2520model%2520security%252C%2520multi-domain%2520with%2520language%2520culture%252C%2520and%250Ausage%2520of%2520datasets.%2520We%2520also%2520discuss%2520the%2520major%2520challenges%2520that%2520arise%2520in%2520these%250Aaspects%252C%2520along%2520with%2520possible%2520solutions.%2520Besides%252C%2520we%2520highlight%2520future%2520research%250Adirections%2520that%2520aim%2520at%2520further%2520enhancing%2520LLMs%2520with%2520multilingualism.%2520The%2520survey%250Aaims%2520to%2520help%2520the%2520research%2520community%2520address%2520multilingual%2520problems%2520and%2520provide%2520a%250Acomprehensive%2520understanding%2520of%2520the%2520core%2520concepts%252C%2520key%2520techniques%252C%2520and%2520latest%250Adevelopments%2520in%2520multilingual%2520natural%2520language%2520processing%2520based%2520on%2520LLMs.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2405.10936v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=A%20Survey%20on%20Large%20Language%20Models%20with%20Multilingualism%3A%20Recent%20Advances%0A%20%20and%20New%20Frontiers&entry.906535625=Kaiyu%20Huang%20and%20Fengran%20Mo%20and%20Hongliang%20Li%20and%20You%20Li%20and%20Yuanchi%20Zhang%20and%20Weijian%20Yi%20and%20Yulong%20Mao%20and%20Jinchen%20Liu%20and%20Yuzhuang%20Xu%20and%20Jinan%20Xu%20and%20Jian-Yun%20Nie%20and%20Yang%20Liu&entry.1292438233=%20%20The%20rapid%20development%20of%20Large%20Language%20Models%20%28LLMs%29%20demonstrates%20remarkable%0Amultilingual%20capabilities%20in%20natural%20language%20processing%2C%20attracting%20global%0Aattention%20in%20both%20academia%20and%20industry.%20To%20mitigate%20potential%20discrimination%0Aand%20enhance%20the%20overall%20usability%20and%20accessibility%20for%20diverse%20language%20user%0Agroups%2C%20it%20is%20important%20for%20the%20development%20of%20language-fair%20technology.%0ADespite%20the%20breakthroughs%20of%20LLMs%2C%20the%20investigation%20into%20the%20multilingual%0Ascenario%20remains%20insufficient%2C%20where%20a%20comprehensive%20survey%20to%20summarize%20recent%0Aapproaches%2C%20developments%2C%20limitations%2C%20and%20potential%20solutions%20is%20desirable.%20To%0Athis%20end%2C%20we%20provide%20a%20survey%20with%20multiple%20perspectives%20on%20the%20utilization%20of%0ALLMs%20in%20the%20multilingual%20scenario.%20We%20first%20rethink%20the%20transitions%20between%0Aprevious%20and%20current%20research%20on%20pre-trained%20language%20models.%20Then%20we%20introduce%0Aseveral%20perspectives%20on%20the%20multilingualism%20of%20LLMs%2C%20including%20training%20and%0Ainference%20methods%2C%20model%20security%2C%20multi-domain%20with%20language%20culture%2C%20and%0Ausage%20of%20datasets.%20We%20also%20discuss%20the%20major%20challenges%20that%20arise%20in%20these%0Aaspects%2C%20along%20with%20possible%20solutions.%20Besides%2C%20we%20highlight%20future%20research%0Adirections%20that%20aim%20at%20further%20enhancing%20LLMs%20with%20multilingualism.%20The%20survey%0Aaims%20to%20help%20the%20research%20community%20address%20multilingual%20problems%20and%20provide%20a%0Acomprehensive%20understanding%20of%20the%20core%20concepts%2C%20key%20techniques%2C%20and%20latest%0Adevelopments%20in%20multilingual%20natural%20language%20processing%20based%20on%20LLMs.%0A&entry.1838667208=http%3A//arxiv.org/abs/2405.10936v1&entry.124074799=Read"},
{"title": "Exploring new territory: Calibration-free decoding for c-VEP BCI", "author": "J. Thielen and J. Sosulski and M. Tangermann", "abstract": "  This study explores two zero-training methods aimed at enhancing the\nusability of brain-computer interfaces (BCIs) by eliminating the need for a\ncalibration session. We introduce a novel method rooted in the event-related\npotential (ERP) domain, unsupervised mean maximization (UMM), to the fast\ncode-modulated visual evoked potential (c-VEP) stimulus protocol. We compare\nUMM to the state-of-the-art c-VEP zero-training method that uses canonical\ncorrelation analysis (CCA). The comparison includes instantaneous\nclassification and classification with cumulative learning from previously\nclassified trials for both CCA and UMM. Our study shows the effectiveness of\nboth methods in navigating the complexities of a c-VEP dataset, highlighting\ntheir differences and distinct strengths. This research not only provides\ninsights into the practical implementation of calibration-free BCI methods but\nalso paves the way for further exploration and refinement. Ultimately, the\nfusion of CCA and UMM holds promise for enhancing the accessibility and\nusability of BCI systems across various application domains and a multitude of\nstimulus protocols.\n", "link": "http://arxiv.org/abs/2403.15521v2", "date": "2024-05-17", "relevancy": 1.8253, "topK": [{"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.4684}, {"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.4592}, {"title": "Egocentric Computer Vision for Hands-Free Robotic Wheelchair Navigation", "link": "https://link.springer.com/article/10.1007/s10846-023-01807-4", "similarity": 0.4487}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Exploring%20new%20territory%3A%20Calibration-free%20decoding%20for%20c-VEP%20BCI&body=Title%3A%20Exploring%20new%20territory%3A%20Calibration-free%20decoding%20for%20c-VEP%20BCI%0AAuthor%3A%20J.%20Thielen%20and%20J.%20Sosulski%20and%20M.%20Tangermann%0AAbstract%3A%20%20%20This%20study%20explores%20two%20zero-training%20methods%20aimed%20at%20enhancing%20the%0Ausability%20of%20brain-computer%20interfaces%20%28BCIs%29%20by%20eliminating%20the%20need%20for%20a%0Acalibration%20session.%20We%20introduce%20a%20novel%20method%20rooted%20in%20the%20event-related%0Apotential%20%28ERP%29%20domain%2C%20unsupervised%20mean%20maximization%20%28UMM%29%2C%20to%20the%20fast%0Acode-modulated%20visual%20evoked%20potential%20%28c-VEP%29%20stimulus%20protocol.%20We%20compare%0AUMM%20to%20the%20state-of-the-art%20c-VEP%20zero-training%20method%20that%20uses%20canonical%0Acorrelation%20analysis%20%28CCA%29.%20The%20comparison%20includes%20instantaneous%0Aclassification%20and%20classification%20with%20cumulative%20learning%20from%20previously%0Aclassified%20trials%20for%20both%20CCA%20and%20UMM.%20Our%20study%20shows%20the%20effectiveness%20of%0Aboth%20methods%20in%20navigating%20the%20complexities%20of%20a%20c-VEP%20dataset%2C%20highlighting%0Atheir%20differences%20and%20distinct%20strengths.%20This%20research%20not%20only%20provides%0Ainsights%20into%20the%20practical%20implementation%20of%20calibration-free%20BCI%20methods%20but%0Aalso%20paves%20the%20way%20for%20further%20exploration%20and%20refinement.%20Ultimately%2C%20the%0Afusion%20of%20CCA%20and%20UMM%20holds%20promise%20for%20enhancing%20the%20accessibility%20and%0Ausability%20of%20BCI%20systems%20across%20various%20application%20domains%20and%20a%20multitude%20of%0Astimulus%20protocols.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2403.15521v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DExploring%2520new%2520territory%253A%2520Calibration-free%2520decoding%2520for%2520c-VEP%2520BCI%26entry.906535625%3DJ.%2520Thielen%2520and%2520J.%2520Sosulski%2520and%2520M.%2520Tangermann%26entry.1292438233%3D%2520%2520This%2520study%2520explores%2520two%2520zero-training%2520methods%2520aimed%2520at%2520enhancing%2520the%250Ausability%2520of%2520brain-computer%2520interfaces%2520%2528BCIs%2529%2520by%2520eliminating%2520the%2520need%2520for%2520a%250Acalibration%2520session.%2520We%2520introduce%2520a%2520novel%2520method%2520rooted%2520in%2520the%2520event-related%250Apotential%2520%2528ERP%2529%2520domain%252C%2520unsupervised%2520mean%2520maximization%2520%2528UMM%2529%252C%2520to%2520the%2520fast%250Acode-modulated%2520visual%2520evoked%2520potential%2520%2528c-VEP%2529%2520stimulus%2520protocol.%2520We%2520compare%250AUMM%2520to%2520the%2520state-of-the-art%2520c-VEP%2520zero-training%2520method%2520that%2520uses%2520canonical%250Acorrelation%2520analysis%2520%2528CCA%2529.%2520The%2520comparison%2520includes%2520instantaneous%250Aclassification%2520and%2520classification%2520with%2520cumulative%2520learning%2520from%2520previously%250Aclassified%2520trials%2520for%2520both%2520CCA%2520and%2520UMM.%2520Our%2520study%2520shows%2520the%2520effectiveness%2520of%250Aboth%2520methods%2520in%2520navigating%2520the%2520complexities%2520of%2520a%2520c-VEP%2520dataset%252C%2520highlighting%250Atheir%2520differences%2520and%2520distinct%2520strengths.%2520This%2520research%2520not%2520only%2520provides%250Ainsights%2520into%2520the%2520practical%2520implementation%2520of%2520calibration-free%2520BCI%2520methods%2520but%250Aalso%2520paves%2520the%2520way%2520for%2520further%2520exploration%2520and%2520refinement.%2520Ultimately%252C%2520the%250Afusion%2520of%2520CCA%2520and%2520UMM%2520holds%2520promise%2520for%2520enhancing%2520the%2520accessibility%2520and%250Ausability%2520of%2520BCI%2520systems%2520across%2520various%2520application%2520domains%2520and%2520a%2520multitude%2520of%250Astimulus%2520protocols.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2403.15521v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Exploring%20new%20territory%3A%20Calibration-free%20decoding%20for%20c-VEP%20BCI&entry.906535625=J.%20Thielen%20and%20J.%20Sosulski%20and%20M.%20Tangermann&entry.1292438233=%20%20This%20study%20explores%20two%20zero-training%20methods%20aimed%20at%20enhancing%20the%0Ausability%20of%20brain-computer%20interfaces%20%28BCIs%29%20by%20eliminating%20the%20need%20for%20a%0Acalibration%20session.%20We%20introduce%20a%20novel%20method%20rooted%20in%20the%20event-related%0Apotential%20%28ERP%29%20domain%2C%20unsupervised%20mean%20maximization%20%28UMM%29%2C%20to%20the%20fast%0Acode-modulated%20visual%20evoked%20potential%20%28c-VEP%29%20stimulus%20protocol.%20We%20compare%0AUMM%20to%20the%20state-of-the-art%20c-VEP%20zero-training%20method%20that%20uses%20canonical%0Acorrelation%20analysis%20%28CCA%29.%20The%20comparison%20includes%20instantaneous%0Aclassification%20and%20classification%20with%20cumulative%20learning%20from%20previously%0Aclassified%20trials%20for%20both%20CCA%20and%20UMM.%20Our%20study%20shows%20the%20effectiveness%20of%0Aboth%20methods%20in%20navigating%20the%20complexities%20of%20a%20c-VEP%20dataset%2C%20highlighting%0Atheir%20differences%20and%20distinct%20strengths.%20This%20research%20not%20only%20provides%0Ainsights%20into%20the%20practical%20implementation%20of%20calibration-free%20BCI%20methods%20but%0Aalso%20paves%20the%20way%20for%20further%20exploration%20and%20refinement.%20Ultimately%2C%20the%0Afusion%20of%20CCA%20and%20UMM%20holds%20promise%20for%20enhancing%20the%20accessibility%20and%0Ausability%20of%20BCI%20systems%20across%20various%20application%20domains%20and%20a%20multitude%20of%0Astimulus%20protocols.%0A&entry.1838667208=http%3A//arxiv.org/abs/2403.15521v2&entry.124074799=Read"},
{"title": "Improving face generation quality and prompt following with synthetic\n  captions", "author": "Michail Tarasiou and Stylianos Moschoglou and Jiankang Deng and Stefanos Zafeiriou", "abstract": "  Recent advancements in text-to-image generation using diffusion models have\nsignificantly improved the quality of generated images and expanded the ability\nto depict a wide range of objects. However, ensuring that these models adhere\nclosely to the text prompts remains a considerable challenge. This issue is\nparticularly pronounced when trying to generate photorealistic images of\nhumans. Without significant prompt engineering efforts models often produce\nunrealistic images and typically fail to incorporate the full extent of the\nprompt information. This limitation can be largely attributed to the nature of\ncaptions accompanying the images used in training large scale diffusion models,\nwhich typically prioritize contextual information over details related to the\nperson's appearance. In this paper we address this issue by introducing a\ntraining-free pipeline designed to generate accurate appearance descriptions\nfrom images of people. We apply this method to create approximately 250,000\ncaptions for publicly available face datasets. We then use these synthetic\ncaptions to fine-tune a text-to-image diffusion model. Our results demonstrate\nthat this approach significantly improves the model's ability to generate\nhigh-quality, realistic human faces and enhances adherence to the given\nprompts, compared to the baseline model. We share our synthetic captions,\npretrained checkpoints and training code.\n", "link": "http://arxiv.org/abs/2405.10864v1", "date": "2024-05-17", "relevancy": 1.8126, "topK": [{"title": "Scaling Rectified Flow Transformers for High-Resolution Image Synthesis", "link": "http://arxiv.org/abs/2403.03206v1", "similarity": 0.6151}, {"title": "Total Selfie: Generating Full-Body Selfies", "link": "http://arxiv.org/abs/2308.14740v2", "similarity": 0.6023}, {"title": "Magic-Me: Identity-Specific Video Customized Diffusion", "link": "http://arxiv.org/abs/2402.09368v1", "similarity": 0.6006}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Improving%20face%20generation%20quality%20and%20prompt%20following%20with%20synthetic%0A%20%20captions&body=Title%3A%20Improving%20face%20generation%20quality%20and%20prompt%20following%20with%20synthetic%0A%20%20captions%0AAuthor%3A%20Michail%20Tarasiou%20and%20Stylianos%20Moschoglou%20and%20Jiankang%20Deng%20and%20Stefanos%20Zafeiriou%0AAbstract%3A%20%20%20Recent%20advancements%20in%20text-to-image%20generation%20using%20diffusion%20models%20have%0Asignificantly%20improved%20the%20quality%20of%20generated%20images%20and%20expanded%20the%20ability%0Ato%20depict%20a%20wide%20range%20of%20objects.%20However%2C%20ensuring%20that%20these%20models%20adhere%0Aclosely%20to%20the%20text%20prompts%20remains%20a%20considerable%20challenge.%20This%20issue%20is%0Aparticularly%20pronounced%20when%20trying%20to%20generate%20photorealistic%20images%20of%0Ahumans.%20Without%20significant%20prompt%20engineering%20efforts%20models%20often%20produce%0Aunrealistic%20images%20and%20typically%20fail%20to%20incorporate%20the%20full%20extent%20of%20the%0Aprompt%20information.%20This%20limitation%20can%20be%20largely%20attributed%20to%20the%20nature%20of%0Acaptions%20accompanying%20the%20images%20used%20in%20training%20large%20scale%20diffusion%20models%2C%0Awhich%20typically%20prioritize%20contextual%20information%20over%20details%20related%20to%20the%0Aperson%27s%20appearance.%20In%20this%20paper%20we%20address%20this%20issue%20by%20introducing%20a%0Atraining-free%20pipeline%20designed%20to%20generate%20accurate%20appearance%20descriptions%0Afrom%20images%20of%20people.%20We%20apply%20this%20method%20to%20create%20approximately%20250%2C000%0Acaptions%20for%20publicly%20available%20face%20datasets.%20We%20then%20use%20these%20synthetic%0Acaptions%20to%20fine-tune%20a%20text-to-image%20diffusion%20model.%20Our%20results%20demonstrate%0Athat%20this%20approach%20significantly%20improves%20the%20model%27s%20ability%20to%20generate%0Ahigh-quality%2C%20realistic%20human%20faces%20and%20enhances%20adherence%20to%20the%20given%0Aprompts%2C%20compared%20to%20the%20baseline%20model.%20We%20share%20our%20synthetic%20captions%2C%0Apretrained%20checkpoints%20and%20training%20code.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2405.10864v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DImproving%2520face%2520generation%2520quality%2520and%2520prompt%2520following%2520with%2520synthetic%250A%2520%2520captions%26entry.906535625%3DMichail%2520Tarasiou%2520and%2520Stylianos%2520Moschoglou%2520and%2520Jiankang%2520Deng%2520and%2520Stefanos%2520Zafeiriou%26entry.1292438233%3D%2520%2520Recent%2520advancements%2520in%2520text-to-image%2520generation%2520using%2520diffusion%2520models%2520have%250Asignificantly%2520improved%2520the%2520quality%2520of%2520generated%2520images%2520and%2520expanded%2520the%2520ability%250Ato%2520depict%2520a%2520wide%2520range%2520of%2520objects.%2520However%252C%2520ensuring%2520that%2520these%2520models%2520adhere%250Aclosely%2520to%2520the%2520text%2520prompts%2520remains%2520a%2520considerable%2520challenge.%2520This%2520issue%2520is%250Aparticularly%2520pronounced%2520when%2520trying%2520to%2520generate%2520photorealistic%2520images%2520of%250Ahumans.%2520Without%2520significant%2520prompt%2520engineering%2520efforts%2520models%2520often%2520produce%250Aunrealistic%2520images%2520and%2520typically%2520fail%2520to%2520incorporate%2520the%2520full%2520extent%2520of%2520the%250Aprompt%2520information.%2520This%2520limitation%2520can%2520be%2520largely%2520attributed%2520to%2520the%2520nature%2520of%250Acaptions%2520accompanying%2520the%2520images%2520used%2520in%2520training%2520large%2520scale%2520diffusion%2520models%252C%250Awhich%2520typically%2520prioritize%2520contextual%2520information%2520over%2520details%2520related%2520to%2520the%250Aperson%2527s%2520appearance.%2520In%2520this%2520paper%2520we%2520address%2520this%2520issue%2520by%2520introducing%2520a%250Atraining-free%2520pipeline%2520designed%2520to%2520generate%2520accurate%2520appearance%2520descriptions%250Afrom%2520images%2520of%2520people.%2520We%2520apply%2520this%2520method%2520to%2520create%2520approximately%2520250%252C000%250Acaptions%2520for%2520publicly%2520available%2520face%2520datasets.%2520We%2520then%2520use%2520these%2520synthetic%250Acaptions%2520to%2520fine-tune%2520a%2520text-to-image%2520diffusion%2520model.%2520Our%2520results%2520demonstrate%250Athat%2520this%2520approach%2520significantly%2520improves%2520the%2520model%2527s%2520ability%2520to%2520generate%250Ahigh-quality%252C%2520realistic%2520human%2520faces%2520and%2520enhances%2520adherence%2520to%2520the%2520given%250Aprompts%252C%2520compared%2520to%2520the%2520baseline%2520model.%2520We%2520share%2520our%2520synthetic%2520captions%252C%250Apretrained%2520checkpoints%2520and%2520training%2520code.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2405.10864v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Improving%20face%20generation%20quality%20and%20prompt%20following%20with%20synthetic%0A%20%20captions&entry.906535625=Michail%20Tarasiou%20and%20Stylianos%20Moschoglou%20and%20Jiankang%20Deng%20and%20Stefanos%20Zafeiriou&entry.1292438233=%20%20Recent%20advancements%20in%20text-to-image%20generation%20using%20diffusion%20models%20have%0Asignificantly%20improved%20the%20quality%20of%20generated%20images%20and%20expanded%20the%20ability%0Ato%20depict%20a%20wide%20range%20of%20objects.%20However%2C%20ensuring%20that%20these%20models%20adhere%0Aclosely%20to%20the%20text%20prompts%20remains%20a%20considerable%20challenge.%20This%20issue%20is%0Aparticularly%20pronounced%20when%20trying%20to%20generate%20photorealistic%20images%20of%0Ahumans.%20Without%20significant%20prompt%20engineering%20efforts%20models%20often%20produce%0Aunrealistic%20images%20and%20typically%20fail%20to%20incorporate%20the%20full%20extent%20of%20the%0Aprompt%20information.%20This%20limitation%20can%20be%20largely%20attributed%20to%20the%20nature%20of%0Acaptions%20accompanying%20the%20images%20used%20in%20training%20large%20scale%20diffusion%20models%2C%0Awhich%20typically%20prioritize%20contextual%20information%20over%20details%20related%20to%20the%0Aperson%27s%20appearance.%20In%20this%20paper%20we%20address%20this%20issue%20by%20introducing%20a%0Atraining-free%20pipeline%20designed%20to%20generate%20accurate%20appearance%20descriptions%0Afrom%20images%20of%20people.%20We%20apply%20this%20method%20to%20create%20approximately%20250%2C000%0Acaptions%20for%20publicly%20available%20face%20datasets.%20We%20then%20use%20these%20synthetic%0Acaptions%20to%20fine-tune%20a%20text-to-image%20diffusion%20model.%20Our%20results%20demonstrate%0Athat%20this%20approach%20significantly%20improves%20the%20model%27s%20ability%20to%20generate%0Ahigh-quality%2C%20realistic%20human%20faces%20and%20enhances%20adherence%20to%20the%20given%0Aprompts%2C%20compared%20to%20the%20baseline%20model.%20We%20share%20our%20synthetic%20captions%2C%0Apretrained%20checkpoints%20and%20training%20code.%0A&entry.1838667208=http%3A//arxiv.org/abs/2405.10864v1&entry.124074799=Read"},
{"title": "Visibility into AI Agents", "author": "Alan Chan and Carson Ezell and Max Kaufmann and Kevin Wei and Lewis Hammond and Herbie Bradley and Emma Bluemke and Nitarshan Rajkumar and David Krueger and Noam Kolt and Lennart Heim and Markus Anderljung", "abstract": "  Increased delegation of commercial, scientific, governmental, and personal\nactivities to AI agents -- systems capable of pursuing complex goals with\nlimited supervision -- may exacerbate existing societal risks and introduce new\nrisks. Understanding and mitigating these risks involves critically evaluating\nexisting governance structures, revising and adapting these structures where\nneeded, and ensuring accountability of key stakeholders. Information about\nwhere, why, how, and by whom certain AI agents are used, which we refer to as\nvisibility, is critical to these objectives. In this paper, we assess three\ncategories of measures to increase visibility into AI agents: agent\nidentifiers, real-time monitoring, and activity logging. For each, we outline\npotential implementations that vary in intrusiveness and informativeness. We\nanalyze how the measures apply across a spectrum of centralized through\ndecentralized deployment contexts, accounting for various actors in the supply\nchain including hardware and software service providers. Finally, we discuss\nthe implications of our measures for privacy and concentration of power.\nFurther work into understanding the measures and mitigating their negative\nimpacts can help to build a foundation for the governance of AI agents.\n", "link": "http://arxiv.org/abs/2401.13138v6", "date": "2024-05-17", "relevancy": 1.8041, "topK": [{"title": "Safe and efficient training of a control agent", "link": "https://patents.google.com/patent/US11709462B2/en", "similarity": 0.4651}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.4419}, {"title": "Genie: Generative Interactive Environments", "link": "http://arxiv.org/abs/2402.15391v1", "similarity": 0.4406}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Visibility%20into%20AI%20Agents&body=Title%3A%20Visibility%20into%20AI%20Agents%0AAuthor%3A%20Alan%20Chan%20and%20Carson%20Ezell%20and%20Max%20Kaufmann%20and%20Kevin%20Wei%20and%20Lewis%20Hammond%20and%20Herbie%20Bradley%20and%20Emma%20Bluemke%20and%20Nitarshan%20Rajkumar%20and%20David%20Krueger%20and%20Noam%20Kolt%20and%20Lennart%20Heim%20and%20Markus%20Anderljung%0AAbstract%3A%20%20%20Increased%20delegation%20of%20commercial%2C%20scientific%2C%20governmental%2C%20and%20personal%0Aactivities%20to%20AI%20agents%20--%20systems%20capable%20of%20pursuing%20complex%20goals%20with%0Alimited%20supervision%20--%20may%20exacerbate%20existing%20societal%20risks%20and%20introduce%20new%0Arisks.%20Understanding%20and%20mitigating%20these%20risks%20involves%20critically%20evaluating%0Aexisting%20governance%20structures%2C%20revising%20and%20adapting%20these%20structures%20where%0Aneeded%2C%20and%20ensuring%20accountability%20of%20key%20stakeholders.%20Information%20about%0Awhere%2C%20why%2C%20how%2C%20and%20by%20whom%20certain%20AI%20agents%20are%20used%2C%20which%20we%20refer%20to%20as%0Avisibility%2C%20is%20critical%20to%20these%20objectives.%20In%20this%20paper%2C%20we%20assess%20three%0Acategories%20of%20measures%20to%20increase%20visibility%20into%20AI%20agents%3A%20agent%0Aidentifiers%2C%20real-time%20monitoring%2C%20and%20activity%20logging.%20For%20each%2C%20we%20outline%0Apotential%20implementations%20that%20vary%20in%20intrusiveness%20and%20informativeness.%20We%0Aanalyze%20how%20the%20measures%20apply%20across%20a%20spectrum%20of%20centralized%20through%0Adecentralized%20deployment%20contexts%2C%20accounting%20for%20various%20actors%20in%20the%20supply%0Achain%20including%20hardware%20and%20software%20service%20providers.%20Finally%2C%20we%20discuss%0Athe%20implications%20of%20our%20measures%20for%20privacy%20and%20concentration%20of%20power.%0AFurther%20work%20into%20understanding%20the%20measures%20and%20mitigating%20their%20negative%0Aimpacts%20can%20help%20to%20build%20a%20foundation%20for%20the%20governance%20of%20AI%20agents.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2401.13138v6%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DVisibility%2520into%2520AI%2520Agents%26entry.906535625%3DAlan%2520Chan%2520and%2520Carson%2520Ezell%2520and%2520Max%2520Kaufmann%2520and%2520Kevin%2520Wei%2520and%2520Lewis%2520Hammond%2520and%2520Herbie%2520Bradley%2520and%2520Emma%2520Bluemke%2520and%2520Nitarshan%2520Rajkumar%2520and%2520David%2520Krueger%2520and%2520Noam%2520Kolt%2520and%2520Lennart%2520Heim%2520and%2520Markus%2520Anderljung%26entry.1292438233%3D%2520%2520Increased%2520delegation%2520of%2520commercial%252C%2520scientific%252C%2520governmental%252C%2520and%2520personal%250Aactivities%2520to%2520AI%2520agents%2520--%2520systems%2520capable%2520of%2520pursuing%2520complex%2520goals%2520with%250Alimited%2520supervision%2520--%2520may%2520exacerbate%2520existing%2520societal%2520risks%2520and%2520introduce%2520new%250Arisks.%2520Understanding%2520and%2520mitigating%2520these%2520risks%2520involves%2520critically%2520evaluating%250Aexisting%2520governance%2520structures%252C%2520revising%2520and%2520adapting%2520these%2520structures%2520where%250Aneeded%252C%2520and%2520ensuring%2520accountability%2520of%2520key%2520stakeholders.%2520Information%2520about%250Awhere%252C%2520why%252C%2520how%252C%2520and%2520by%2520whom%2520certain%2520AI%2520agents%2520are%2520used%252C%2520which%2520we%2520refer%2520to%2520as%250Avisibility%252C%2520is%2520critical%2520to%2520these%2520objectives.%2520In%2520this%2520paper%252C%2520we%2520assess%2520three%250Acategories%2520of%2520measures%2520to%2520increase%2520visibility%2520into%2520AI%2520agents%253A%2520agent%250Aidentifiers%252C%2520real-time%2520monitoring%252C%2520and%2520activity%2520logging.%2520For%2520each%252C%2520we%2520outline%250Apotential%2520implementations%2520that%2520vary%2520in%2520intrusiveness%2520and%2520informativeness.%2520We%250Aanalyze%2520how%2520the%2520measures%2520apply%2520across%2520a%2520spectrum%2520of%2520centralized%2520through%250Adecentralized%2520deployment%2520contexts%252C%2520accounting%2520for%2520various%2520actors%2520in%2520the%2520supply%250Achain%2520including%2520hardware%2520and%2520software%2520service%2520providers.%2520Finally%252C%2520we%2520discuss%250Athe%2520implications%2520of%2520our%2520measures%2520for%2520privacy%2520and%2520concentration%2520of%2520power.%250AFurther%2520work%2520into%2520understanding%2520the%2520measures%2520and%2520mitigating%2520their%2520negative%250Aimpacts%2520can%2520help%2520to%2520build%2520a%2520foundation%2520for%2520the%2520governance%2520of%2520AI%2520agents.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2401.13138v6%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Visibility%20into%20AI%20Agents&entry.906535625=Alan%20Chan%20and%20Carson%20Ezell%20and%20Max%20Kaufmann%20and%20Kevin%20Wei%20and%20Lewis%20Hammond%20and%20Herbie%20Bradley%20and%20Emma%20Bluemke%20and%20Nitarshan%20Rajkumar%20and%20David%20Krueger%20and%20Noam%20Kolt%20and%20Lennart%20Heim%20and%20Markus%20Anderljung&entry.1292438233=%20%20Increased%20delegation%20of%20commercial%2C%20scientific%2C%20governmental%2C%20and%20personal%0Aactivities%20to%20AI%20agents%20--%20systems%20capable%20of%20pursuing%20complex%20goals%20with%0Alimited%20supervision%20--%20may%20exacerbate%20existing%20societal%20risks%20and%20introduce%20new%0Arisks.%20Understanding%20and%20mitigating%20these%20risks%20involves%20critically%20evaluating%0Aexisting%20governance%20structures%2C%20revising%20and%20adapting%20these%20structures%20where%0Aneeded%2C%20and%20ensuring%20accountability%20of%20key%20stakeholders.%20Information%20about%0Awhere%2C%20why%2C%20how%2C%20and%20by%20whom%20certain%20AI%20agents%20are%20used%2C%20which%20we%20refer%20to%20as%0Avisibility%2C%20is%20critical%20to%20these%20objectives.%20In%20this%20paper%2C%20we%20assess%20three%0Acategories%20of%20measures%20to%20increase%20visibility%20into%20AI%20agents%3A%20agent%0Aidentifiers%2C%20real-time%20monitoring%2C%20and%20activity%20logging.%20For%20each%2C%20we%20outline%0Apotential%20implementations%20that%20vary%20in%20intrusiveness%20and%20informativeness.%20We%0Aanalyze%20how%20the%20measures%20apply%20across%20a%20spectrum%20of%20centralized%20through%0Adecentralized%20deployment%20contexts%2C%20accounting%20for%20various%20actors%20in%20the%20supply%0Achain%20including%20hardware%20and%20software%20service%20providers.%20Finally%2C%20we%20discuss%0Athe%20implications%20of%20our%20measures%20for%20privacy%20and%20concentration%20of%20power.%0AFurther%20work%20into%20understanding%20the%20measures%20and%20mitigating%20their%20negative%0Aimpacts%20can%20help%20to%20build%20a%20foundation%20for%20the%20governance%20of%20AI%20agents.%0A&entry.1838667208=http%3A//arxiv.org/abs/2401.13138v6&entry.124074799=Read"},
{"title": "Towards gaze-independent c-VEP BCI: A pilot study", "author": "S. Narayanan and S. Ahmadi and P. Desain and J. Thielen", "abstract": "  A limitation of brain-computer interface (BCI) spellers is that they require\nthe user to be able to move the eyes to fixate on targets. This poses an issue\nfor users who cannot voluntarily control their eye movements, for instance,\npeople living with late-stage amyotrophic lateral sclerosis (ALS). This pilot\nstudy makes the first step towards a gaze-independent speller based on the\ncode-modulated visual evoked potential (c-VEP). Participants were presented\nwith two bi-laterally located stimuli, one of which was flashing, and were\ntasked to attend to one of these stimuli either by directly looking at the\nstimuli (overt condition) or by using spatial attention, eliminating the need\nfor eye movement (covert condition). The attended stimuli were decoded from\nelectroencephalography (EEG) and classification accuracies of 88% and 100% were\nobtained for the covert and overt conditions, respectively. These fundamental\ninsights show the promising feasibility of utilizing the c-VEP protocol for\ngaze-independent BCIs that use covert spatial attention when both stimuli flash\nsimultaneously.\n", "link": "http://arxiv.org/abs/2404.00031v2", "date": "2024-05-17", "relevancy": 1.7547, "topK": [{"title": "Egocentric Computer Vision for Hands-Free Robotic Wheelchair Navigation", "link": "https://link.springer.com/article/10.1007/s10846-023-01807-4", "similarity": 0.467}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.4262}, {"title": "Magic-Me: Identity-Specific Video Customized Diffusion", "link": "http://arxiv.org/abs/2402.09368v1", "similarity": 0.4153}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Towards%20gaze-independent%20c-VEP%20BCI%3A%20A%20pilot%20study&body=Title%3A%20Towards%20gaze-independent%20c-VEP%20BCI%3A%20A%20pilot%20study%0AAuthor%3A%20S.%20Narayanan%20and%20S.%20Ahmadi%20and%20P.%20Desain%20and%20J.%20Thielen%0AAbstract%3A%20%20%20A%20limitation%20of%20brain-computer%20interface%20%28BCI%29%20spellers%20is%20that%20they%20require%0Athe%20user%20to%20be%20able%20to%20move%20the%20eyes%20to%20fixate%20on%20targets.%20This%20poses%20an%20issue%0Afor%20users%20who%20cannot%20voluntarily%20control%20their%20eye%20movements%2C%20for%20instance%2C%0Apeople%20living%20with%20late-stage%20amyotrophic%20lateral%20sclerosis%20%28ALS%29.%20This%20pilot%0Astudy%20makes%20the%20first%20step%20towards%20a%20gaze-independent%20speller%20based%20on%20the%0Acode-modulated%20visual%20evoked%20potential%20%28c-VEP%29.%20Participants%20were%20presented%0Awith%20two%20bi-laterally%20located%20stimuli%2C%20one%20of%20which%20was%20flashing%2C%20and%20were%0Atasked%20to%20attend%20to%20one%20of%20these%20stimuli%20either%20by%20directly%20looking%20at%20the%0Astimuli%20%28overt%20condition%29%20or%20by%20using%20spatial%20attention%2C%20eliminating%20the%20need%0Afor%20eye%20movement%20%28covert%20condition%29.%20The%20attended%20stimuli%20were%20decoded%20from%0Aelectroencephalography%20%28EEG%29%20and%20classification%20accuracies%20of%2088%25%20and%20100%25%20were%0Aobtained%20for%20the%20covert%20and%20overt%20conditions%2C%20respectively.%20These%20fundamental%0Ainsights%20show%20the%20promising%20feasibility%20of%20utilizing%20the%20c-VEP%20protocol%20for%0Agaze-independent%20BCIs%20that%20use%20covert%20spatial%20attention%20when%20both%20stimuli%20flash%0Asimultaneously.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2404.00031v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DTowards%2520gaze-independent%2520c-VEP%2520BCI%253A%2520A%2520pilot%2520study%26entry.906535625%3DS.%2520Narayanan%2520and%2520S.%2520Ahmadi%2520and%2520P.%2520Desain%2520and%2520J.%2520Thielen%26entry.1292438233%3D%2520%2520A%2520limitation%2520of%2520brain-computer%2520interface%2520%2528BCI%2529%2520spellers%2520is%2520that%2520they%2520require%250Athe%2520user%2520to%2520be%2520able%2520to%2520move%2520the%2520eyes%2520to%2520fixate%2520on%2520targets.%2520This%2520poses%2520an%2520issue%250Afor%2520users%2520who%2520cannot%2520voluntarily%2520control%2520their%2520eye%2520movements%252C%2520for%2520instance%252C%250Apeople%2520living%2520with%2520late-stage%2520amyotrophic%2520lateral%2520sclerosis%2520%2528ALS%2529.%2520This%2520pilot%250Astudy%2520makes%2520the%2520first%2520step%2520towards%2520a%2520gaze-independent%2520speller%2520based%2520on%2520the%250Acode-modulated%2520visual%2520evoked%2520potential%2520%2528c-VEP%2529.%2520Participants%2520were%2520presented%250Awith%2520two%2520bi-laterally%2520located%2520stimuli%252C%2520one%2520of%2520which%2520was%2520flashing%252C%2520and%2520were%250Atasked%2520to%2520attend%2520to%2520one%2520of%2520these%2520stimuli%2520either%2520by%2520directly%2520looking%2520at%2520the%250Astimuli%2520%2528overt%2520condition%2529%2520or%2520by%2520using%2520spatial%2520attention%252C%2520eliminating%2520the%2520need%250Afor%2520eye%2520movement%2520%2528covert%2520condition%2529.%2520The%2520attended%2520stimuli%2520were%2520decoded%2520from%250Aelectroencephalography%2520%2528EEG%2529%2520and%2520classification%2520accuracies%2520of%252088%2525%2520and%2520100%2525%2520were%250Aobtained%2520for%2520the%2520covert%2520and%2520overt%2520conditions%252C%2520respectively.%2520These%2520fundamental%250Ainsights%2520show%2520the%2520promising%2520feasibility%2520of%2520utilizing%2520the%2520c-VEP%2520protocol%2520for%250Agaze-independent%2520BCIs%2520that%2520use%2520covert%2520spatial%2520attention%2520when%2520both%2520stimuli%2520flash%250Asimultaneously.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2404.00031v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Towards%20gaze-independent%20c-VEP%20BCI%3A%20A%20pilot%20study&entry.906535625=S.%20Narayanan%20and%20S.%20Ahmadi%20and%20P.%20Desain%20and%20J.%20Thielen&entry.1292438233=%20%20A%20limitation%20of%20brain-computer%20interface%20%28BCI%29%20spellers%20is%20that%20they%20require%0Athe%20user%20to%20be%20able%20to%20move%20the%20eyes%20to%20fixate%20on%20targets.%20This%20poses%20an%20issue%0Afor%20users%20who%20cannot%20voluntarily%20control%20their%20eye%20movements%2C%20for%20instance%2C%0Apeople%20living%20with%20late-stage%20amyotrophic%20lateral%20sclerosis%20%28ALS%29.%20This%20pilot%0Astudy%20makes%20the%20first%20step%20towards%20a%20gaze-independent%20speller%20based%20on%20the%0Acode-modulated%20visual%20evoked%20potential%20%28c-VEP%29.%20Participants%20were%20presented%0Awith%20two%20bi-laterally%20located%20stimuli%2C%20one%20of%20which%20was%20flashing%2C%20and%20were%0Atasked%20to%20attend%20to%20one%20of%20these%20stimuli%20either%20by%20directly%20looking%20at%20the%0Astimuli%20%28overt%20condition%29%20or%20by%20using%20spatial%20attention%2C%20eliminating%20the%20need%0Afor%20eye%20movement%20%28covert%20condition%29.%20The%20attended%20stimuli%20were%20decoded%20from%0Aelectroencephalography%20%28EEG%29%20and%20classification%20accuracies%20of%2088%25%20and%20100%25%20were%0Aobtained%20for%20the%20covert%20and%20overt%20conditions%2C%20respectively.%20These%20fundamental%0Ainsights%20show%20the%20promising%20feasibility%20of%20utilizing%20the%20c-VEP%20protocol%20for%0Agaze-independent%20BCIs%20that%20use%20covert%20spatial%20attention%20when%20both%20stimuli%20flash%0Asimultaneously.%0A&entry.1838667208=http%3A//arxiv.org/abs/2404.00031v2&entry.124074799=Read"},
{"title": "Research on Credit Risk Early Warning Model of Commercial Banks Based on\n  Neural Network Algorithm", "author": "Yu Cheng and Qin Yang and Liyang Wang and Ao Xiang and Jingyu Zhang", "abstract": "  In the realm of globalized financial markets, commercial banks are confronted\nwith an escalating magnitude of credit risk, thereby imposing heightened\nrequisites upon the security of bank assets and financial stability. This study\nharnesses advanced neural network techniques, notably the Backpropagation (BP)\nneural network, to pioneer a novel model for preempting credit risk in\ncommercial banks. The discourse initially scrutinizes conventional financial\nrisk preemptive models, such as ARMA, ARCH, and Logistic regression models,\ncritically analyzing their real-world applications. Subsequently, the\nexposition elaborates on the construction process of the BP neural network\nmodel, encompassing network architecture design, activation function selection,\nparameter initialization, and objective function construction. Through\ncomparative analysis, the superiority of neural network models in preempting\ncredit risk in commercial banks is elucidated. The experimental segment selects\nspecific bank data, validating the model's predictive accuracy and\npracticality. Research findings evince that this model efficaciously enhances\nthe foresight and precision of credit risk management.\n", "link": "http://arxiv.org/abs/2405.10762v1", "date": "2024-05-17", "relevancy": 1.7516, "topK": [{"title": "Boosted dynamic neural networks", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/26302", "similarity": 0.4788}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.4197}, {"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.4043}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Research%20on%20Credit%20Risk%20Early%20Warning%20Model%20of%20Commercial%20Banks%20Based%20on%0A%20%20Neural%20Network%20Algorithm&body=Title%3A%20Research%20on%20Credit%20Risk%20Early%20Warning%20Model%20of%20Commercial%20Banks%20Based%20on%0A%20%20Neural%20Network%20Algorithm%0AAuthor%3A%20Yu%20Cheng%20and%20Qin%20Yang%20and%20Liyang%20Wang%20and%20Ao%20Xiang%20and%20Jingyu%20Zhang%0AAbstract%3A%20%20%20In%20the%20realm%20of%20globalized%20financial%20markets%2C%20commercial%20banks%20are%20confronted%0Awith%20an%20escalating%20magnitude%20of%20credit%20risk%2C%20thereby%20imposing%20heightened%0Arequisites%20upon%20the%20security%20of%20bank%20assets%20and%20financial%20stability.%20This%20study%0Aharnesses%20advanced%20neural%20network%20techniques%2C%20notably%20the%20Backpropagation%20%28BP%29%0Aneural%20network%2C%20to%20pioneer%20a%20novel%20model%20for%20preempting%20credit%20risk%20in%0Acommercial%20banks.%20The%20discourse%20initially%20scrutinizes%20conventional%20financial%0Arisk%20preemptive%20models%2C%20such%20as%20ARMA%2C%20ARCH%2C%20and%20Logistic%20regression%20models%2C%0Acritically%20analyzing%20their%20real-world%20applications.%20Subsequently%2C%20the%0Aexposition%20elaborates%20on%20the%20construction%20process%20of%20the%20BP%20neural%20network%0Amodel%2C%20encompassing%20network%20architecture%20design%2C%20activation%20function%20selection%2C%0Aparameter%20initialization%2C%20and%20objective%20function%20construction.%20Through%0Acomparative%20analysis%2C%20the%20superiority%20of%20neural%20network%20models%20in%20preempting%0Acredit%20risk%20in%20commercial%20banks%20is%20elucidated.%20The%20experimental%20segment%20selects%0Aspecific%20bank%20data%2C%20validating%20the%20model%27s%20predictive%20accuracy%20and%0Apracticality.%20Research%20findings%20evince%20that%20this%20model%20efficaciously%20enhances%0Athe%20foresight%20and%20precision%20of%20credit%20risk%20management.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2405.10762v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DResearch%2520on%2520Credit%2520Risk%2520Early%2520Warning%2520Model%2520of%2520Commercial%2520Banks%2520Based%2520on%250A%2520%2520Neural%2520Network%2520Algorithm%26entry.906535625%3DYu%2520Cheng%2520and%2520Qin%2520Yang%2520and%2520Liyang%2520Wang%2520and%2520Ao%2520Xiang%2520and%2520Jingyu%2520Zhang%26entry.1292438233%3D%2520%2520In%2520the%2520realm%2520of%2520globalized%2520financial%2520markets%252C%2520commercial%2520banks%2520are%2520confronted%250Awith%2520an%2520escalating%2520magnitude%2520of%2520credit%2520risk%252C%2520thereby%2520imposing%2520heightened%250Arequisites%2520upon%2520the%2520security%2520of%2520bank%2520assets%2520and%2520financial%2520stability.%2520This%2520study%250Aharnesses%2520advanced%2520neural%2520network%2520techniques%252C%2520notably%2520the%2520Backpropagation%2520%2528BP%2529%250Aneural%2520network%252C%2520to%2520pioneer%2520a%2520novel%2520model%2520for%2520preempting%2520credit%2520risk%2520in%250Acommercial%2520banks.%2520The%2520discourse%2520initially%2520scrutinizes%2520conventional%2520financial%250Arisk%2520preemptive%2520models%252C%2520such%2520as%2520ARMA%252C%2520ARCH%252C%2520and%2520Logistic%2520regression%2520models%252C%250Acritically%2520analyzing%2520their%2520real-world%2520applications.%2520Subsequently%252C%2520the%250Aexposition%2520elaborates%2520on%2520the%2520construction%2520process%2520of%2520the%2520BP%2520neural%2520network%250Amodel%252C%2520encompassing%2520network%2520architecture%2520design%252C%2520activation%2520function%2520selection%252C%250Aparameter%2520initialization%252C%2520and%2520objective%2520function%2520construction.%2520Through%250Acomparative%2520analysis%252C%2520the%2520superiority%2520of%2520neural%2520network%2520models%2520in%2520preempting%250Acredit%2520risk%2520in%2520commercial%2520banks%2520is%2520elucidated.%2520The%2520experimental%2520segment%2520selects%250Aspecific%2520bank%2520data%252C%2520validating%2520the%2520model%2527s%2520predictive%2520accuracy%2520and%250Apracticality.%2520Research%2520findings%2520evince%2520that%2520this%2520model%2520efficaciously%2520enhances%250Athe%2520foresight%2520and%2520precision%2520of%2520credit%2520risk%2520management.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2405.10762v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Research%20on%20Credit%20Risk%20Early%20Warning%20Model%20of%20Commercial%20Banks%20Based%20on%0A%20%20Neural%20Network%20Algorithm&entry.906535625=Yu%20Cheng%20and%20Qin%20Yang%20and%20Liyang%20Wang%20and%20Ao%20Xiang%20and%20Jingyu%20Zhang&entry.1292438233=%20%20In%20the%20realm%20of%20globalized%20financial%20markets%2C%20commercial%20banks%20are%20confronted%0Awith%20an%20escalating%20magnitude%20of%20credit%20risk%2C%20thereby%20imposing%20heightened%0Arequisites%20upon%20the%20security%20of%20bank%20assets%20and%20financial%20stability.%20This%20study%0Aharnesses%20advanced%20neural%20network%20techniques%2C%20notably%20the%20Backpropagation%20%28BP%29%0Aneural%20network%2C%20to%20pioneer%20a%20novel%20model%20for%20preempting%20credit%20risk%20in%0Acommercial%20banks.%20The%20discourse%20initially%20scrutinizes%20conventional%20financial%0Arisk%20preemptive%20models%2C%20such%20as%20ARMA%2C%20ARCH%2C%20and%20Logistic%20regression%20models%2C%0Acritically%20analyzing%20their%20real-world%20applications.%20Subsequently%2C%20the%0Aexposition%20elaborates%20on%20the%20construction%20process%20of%20the%20BP%20neural%20network%0Amodel%2C%20encompassing%20network%20architecture%20design%2C%20activation%20function%20selection%2C%0Aparameter%20initialization%2C%20and%20objective%20function%20construction.%20Through%0Acomparative%20analysis%2C%20the%20superiority%20of%20neural%20network%20models%20in%20preempting%0Acredit%20risk%20in%20commercial%20banks%20is%20elucidated.%20The%20experimental%20segment%20selects%0Aspecific%20bank%20data%2C%20validating%20the%20model%27s%20predictive%20accuracy%20and%0Apracticality.%20Research%20findings%20evince%20that%20this%20model%20efficaciously%20enhances%0Athe%20foresight%20and%20precision%20of%20credit%20risk%20management.%0A&entry.1838667208=http%3A//arxiv.org/abs/2405.10762v1&entry.124074799=Read"},
{"title": "ScionFL: Efficient and Robust Secure Quantized Aggregation", "author": "Yaniv Ben-Itzhak and Helen M\u00f6llering and Benny Pinkas and Thomas Schneider and Ajith Suresh and Oleksandr Tkachenko and Shay Vargaftik and Christian Weinert and Hossein Yalame and Avishay Yanai", "abstract": "  Secure aggregation is commonly used in federated learning (FL) to alleviate\nprivacy concerns related to the central aggregator seeing all parameter updates\nin the clear. Unfortunately, most existing secure aggregation schemes ignore\ntwo critical orthogonal research directions that aim to (i) significantly\nreduce client-server communication and (ii) mitigate the impact of malicious\nclients. However, both of these additional properties are essential to\nfacilitate cross-device FL with thousands or even millions of (mobile)\nparticipants.\n  In this paper, we unite both research directions by introducing ScionFL, the\nfirst secure aggregation framework for FL that operates efficiently on\nquantized inputs and simultaneously provides robustness against malicious\nclients. Our framework leverages (novel) multi-party computation (MPC)\ntechniques and supports multiple linear (1-bit) quantization schemes, including\nones that utilize the randomized Hadamard transform and Kashin's\nrepresentation.\n  Our theoretical results are supported by extensive evaluations. We show that\nwith no overhead for clients and moderate overhead for the server compared to\ntransferring and processing quantized updates in plaintext, we obtain\ncomparable accuracy for standard FL benchmarks. Moreover, we demonstrate the\nrobustness of our framework against state-of-the-art poisoning attacks.\n", "link": "http://arxiv.org/abs/2210.07376v3", "date": "2024-05-17", "relevancy": 1.7448, "topK": [{"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.4429}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.4345}, {"title": "RTG-SLAM: Real-time 3D Reconstruction at Scale using Gaussian Splatting", "link": "http://arxiv.org/abs/2404.19706v1", "similarity": 0.4302}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20ScionFL%3A%20Efficient%20and%20Robust%20Secure%20Quantized%20Aggregation&body=Title%3A%20ScionFL%3A%20Efficient%20and%20Robust%20Secure%20Quantized%20Aggregation%0AAuthor%3A%20Yaniv%20Ben-Itzhak%20and%20Helen%20M%C3%B6llering%20and%20Benny%20Pinkas%20and%20Thomas%20Schneider%20and%20Ajith%20Suresh%20and%20Oleksandr%20Tkachenko%20and%20Shay%20Vargaftik%20and%20Christian%20Weinert%20and%20Hossein%20Yalame%20and%20Avishay%20Yanai%0AAbstract%3A%20%20%20Secure%20aggregation%20is%20commonly%20used%20in%20federated%20learning%20%28FL%29%20to%20alleviate%0Aprivacy%20concerns%20related%20to%20the%20central%20aggregator%20seeing%20all%20parameter%20updates%0Ain%20the%20clear.%20Unfortunately%2C%20most%20existing%20secure%20aggregation%20schemes%20ignore%0Atwo%20critical%20orthogonal%20research%20directions%20that%20aim%20to%20%28i%29%20significantly%0Areduce%20client-server%20communication%20and%20%28ii%29%20mitigate%20the%20impact%20of%20malicious%0Aclients.%20However%2C%20both%20of%20these%20additional%20properties%20are%20essential%20to%0Afacilitate%20cross-device%20FL%20with%20thousands%20or%20even%20millions%20of%20%28mobile%29%0Aparticipants.%0A%20%20In%20this%20paper%2C%20we%20unite%20both%20research%20directions%20by%20introducing%20ScionFL%2C%20the%0Afirst%20secure%20aggregation%20framework%20for%20FL%20that%20operates%20efficiently%20on%0Aquantized%20inputs%20and%20simultaneously%20provides%20robustness%20against%20malicious%0Aclients.%20Our%20framework%20leverages%20%28novel%29%20multi-party%20computation%20%28MPC%29%0Atechniques%20and%20supports%20multiple%20linear%20%281-bit%29%20quantization%20schemes%2C%20including%0Aones%20that%20utilize%20the%20randomized%20Hadamard%20transform%20and%20Kashin%27s%0Arepresentation.%0A%20%20Our%20theoretical%20results%20are%20supported%20by%20extensive%20evaluations.%20We%20show%20that%0Awith%20no%20overhead%20for%20clients%20and%20moderate%20overhead%20for%20the%20server%20compared%20to%0Atransferring%20and%20processing%20quantized%20updates%20in%20plaintext%2C%20we%20obtain%0Acomparable%20accuracy%20for%20standard%20FL%20benchmarks.%20Moreover%2C%20we%20demonstrate%20the%0Arobustness%20of%20our%20framework%20against%20state-of-the-art%20poisoning%20attacks.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2210.07376v3%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DScionFL%253A%2520Efficient%2520and%2520Robust%2520Secure%2520Quantized%2520Aggregation%26entry.906535625%3DYaniv%2520Ben-Itzhak%2520and%2520Helen%2520M%25C3%25B6llering%2520and%2520Benny%2520Pinkas%2520and%2520Thomas%2520Schneider%2520and%2520Ajith%2520Suresh%2520and%2520Oleksandr%2520Tkachenko%2520and%2520Shay%2520Vargaftik%2520and%2520Christian%2520Weinert%2520and%2520Hossein%2520Yalame%2520and%2520Avishay%2520Yanai%26entry.1292438233%3D%2520%2520Secure%2520aggregation%2520is%2520commonly%2520used%2520in%2520federated%2520learning%2520%2528FL%2529%2520to%2520alleviate%250Aprivacy%2520concerns%2520related%2520to%2520the%2520central%2520aggregator%2520seeing%2520all%2520parameter%2520updates%250Ain%2520the%2520clear.%2520Unfortunately%252C%2520most%2520existing%2520secure%2520aggregation%2520schemes%2520ignore%250Atwo%2520critical%2520orthogonal%2520research%2520directions%2520that%2520aim%2520to%2520%2528i%2529%2520significantly%250Areduce%2520client-server%2520communication%2520and%2520%2528ii%2529%2520mitigate%2520the%2520impact%2520of%2520malicious%250Aclients.%2520However%252C%2520both%2520of%2520these%2520additional%2520properties%2520are%2520essential%2520to%250Afacilitate%2520cross-device%2520FL%2520with%2520thousands%2520or%2520even%2520millions%2520of%2520%2528mobile%2529%250Aparticipants.%250A%2520%2520In%2520this%2520paper%252C%2520we%2520unite%2520both%2520research%2520directions%2520by%2520introducing%2520ScionFL%252C%2520the%250Afirst%2520secure%2520aggregation%2520framework%2520for%2520FL%2520that%2520operates%2520efficiently%2520on%250Aquantized%2520inputs%2520and%2520simultaneously%2520provides%2520robustness%2520against%2520malicious%250Aclients.%2520Our%2520framework%2520leverages%2520%2528novel%2529%2520multi-party%2520computation%2520%2528MPC%2529%250Atechniques%2520and%2520supports%2520multiple%2520linear%2520%25281-bit%2529%2520quantization%2520schemes%252C%2520including%250Aones%2520that%2520utilize%2520the%2520randomized%2520Hadamard%2520transform%2520and%2520Kashin%2527s%250Arepresentation.%250A%2520%2520Our%2520theoretical%2520results%2520are%2520supported%2520by%2520extensive%2520evaluations.%2520We%2520show%2520that%250Awith%2520no%2520overhead%2520for%2520clients%2520and%2520moderate%2520overhead%2520for%2520the%2520server%2520compared%2520to%250Atransferring%2520and%2520processing%2520quantized%2520updates%2520in%2520plaintext%252C%2520we%2520obtain%250Acomparable%2520accuracy%2520for%2520standard%2520FL%2520benchmarks.%2520Moreover%252C%2520we%2520demonstrate%2520the%250Arobustness%2520of%2520our%2520framework%2520against%2520state-of-the-art%2520poisoning%2520attacks.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2210.07376v3%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=ScionFL%3A%20Efficient%20and%20Robust%20Secure%20Quantized%20Aggregation&entry.906535625=Yaniv%20Ben-Itzhak%20and%20Helen%20M%C3%B6llering%20and%20Benny%20Pinkas%20and%20Thomas%20Schneider%20and%20Ajith%20Suresh%20and%20Oleksandr%20Tkachenko%20and%20Shay%20Vargaftik%20and%20Christian%20Weinert%20and%20Hossein%20Yalame%20and%20Avishay%20Yanai&entry.1292438233=%20%20Secure%20aggregation%20is%20commonly%20used%20in%20federated%20learning%20%28FL%29%20to%20alleviate%0Aprivacy%20concerns%20related%20to%20the%20central%20aggregator%20seeing%20all%20parameter%20updates%0Ain%20the%20clear.%20Unfortunately%2C%20most%20existing%20secure%20aggregation%20schemes%20ignore%0Atwo%20critical%20orthogonal%20research%20directions%20that%20aim%20to%20%28i%29%20significantly%0Areduce%20client-server%20communication%20and%20%28ii%29%20mitigate%20the%20impact%20of%20malicious%0Aclients.%20However%2C%20both%20of%20these%20additional%20properties%20are%20essential%20to%0Afacilitate%20cross-device%20FL%20with%20thousands%20or%20even%20millions%20of%20%28mobile%29%0Aparticipants.%0A%20%20In%20this%20paper%2C%20we%20unite%20both%20research%20directions%20by%20introducing%20ScionFL%2C%20the%0Afirst%20secure%20aggregation%20framework%20for%20FL%20that%20operates%20efficiently%20on%0Aquantized%20inputs%20and%20simultaneously%20provides%20robustness%20against%20malicious%0Aclients.%20Our%20framework%20leverages%20%28novel%29%20multi-party%20computation%20%28MPC%29%0Atechniques%20and%20supports%20multiple%20linear%20%281-bit%29%20quantization%20schemes%2C%20including%0Aones%20that%20utilize%20the%20randomized%20Hadamard%20transform%20and%20Kashin%27s%0Arepresentation.%0A%20%20Our%20theoretical%20results%20are%20supported%20by%20extensive%20evaluations.%20We%20show%20that%0Awith%20no%20overhead%20for%20clients%20and%20moderate%20overhead%20for%20the%20server%20compared%20to%0Atransferring%20and%20processing%20quantized%20updates%20in%20plaintext%2C%20we%20obtain%0Acomparable%20accuracy%20for%20standard%20FL%20benchmarks.%20Moreover%2C%20we%20demonstrate%20the%0Arobustness%20of%20our%20framework%20against%20state-of-the-art%20poisoning%20attacks.%0A&entry.1838667208=http%3A//arxiv.org/abs/2210.07376v3&entry.124074799=Read"},
{"title": "Large Language Model (LLM) for Telecommunications: A Comprehensive\n  Survey on Principles, Key Techniques, and Opportunities", "author": "Hao Zhou and Chengming Hu and Ye Yuan and Yufei Cui and Yili Jin and Can Chen and Haolun Wu and Dun Yuan and Li Jiang and Di Wu and Xue Liu and Charlie Zhang and Xianbin Wang and Jiangchuan Liu", "abstract": "  Large language models (LLMs) have received considerable attention recently\ndue to their outstanding comprehension and reasoning capabilities, leading to\ngreat progress in many fields. The advancement of LLM techniques also offers\npromising opportunities to automate many tasks in the telecommunication\n(telecom) field. After pre-training and fine-tuning, LLMs can perform diverse\ndownstream tasks based on human instructions, paving the way to artificial\ngeneral intelligence (AGI)-enabled 6G. Given the great potential of LLM\ntechnologies, this work aims to provide a comprehensive overview of LLM-enabled\ntelecom networks. In particular, we first present LLM fundamentals, including\nmodel architecture, pre-training, fine-tuning, inference and utilization, model\nevaluation, and telecom deployment. Then, we introduce LLM-enabled key\ntechniques and telecom applications in terms of generation, classification,\noptimization, and prediction problems. Specifically, the LLM-enabled generation\napplications include telecom domain knowledge, code, and network configuration\ngeneration. After that, the LLM-based classification applications involve\nnetwork security, text, image, and traffic classification problems. Moreover,\nmultiple LLM-enabled optimization techniques are introduced, such as automated\nreward function design for reinforcement learning and verbal reinforcement\nlearning. Furthermore, for LLM-aided prediction problems, we discussed\ntime-series prediction models and multi-modality prediction problems for\ntelecom. Finally, we highlight the challenges and identify the future\ndirections of LLM-enabled telecom networks.\n", "link": "http://arxiv.org/abs/2405.10825v1", "date": "2024-05-17", "relevancy": 1.7444, "topK": [{"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.4423}, {"title": "Self-supervised Photographic Image Layout Representation Learning", "link": "http://arxiv.org/abs/2403.03740v1", "similarity": 0.4331}, {"title": "Scaling Rectified Flow Transformers for High-Resolution Image Synthesis", "link": "http://arxiv.org/abs/2403.03206v1", "similarity": 0.428}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Large%20Language%20Model%20%28LLM%29%20for%20Telecommunications%3A%20A%20Comprehensive%0A%20%20Survey%20on%20Principles%2C%20Key%20Techniques%2C%20and%20Opportunities&body=Title%3A%20Large%20Language%20Model%20%28LLM%29%20for%20Telecommunications%3A%20A%20Comprehensive%0A%20%20Survey%20on%20Principles%2C%20Key%20Techniques%2C%20and%20Opportunities%0AAuthor%3A%20Hao%20Zhou%20and%20Chengming%20Hu%20and%20Ye%20Yuan%20and%20Yufei%20Cui%20and%20Yili%20Jin%20and%20Can%20Chen%20and%20Haolun%20Wu%20and%20Dun%20Yuan%20and%20Li%20Jiang%20and%20Di%20Wu%20and%20Xue%20Liu%20and%20Charlie%20Zhang%20and%20Xianbin%20Wang%20and%20Jiangchuan%20Liu%0AAbstract%3A%20%20%20Large%20language%20models%20%28LLMs%29%20have%20received%20considerable%20attention%20recently%0Adue%20to%20their%20outstanding%20comprehension%20and%20reasoning%20capabilities%2C%20leading%20to%0Agreat%20progress%20in%20many%20fields.%20The%20advancement%20of%20LLM%20techniques%20also%20offers%0Apromising%20opportunities%20to%20automate%20many%20tasks%20in%20the%20telecommunication%0A%28telecom%29%20field.%20After%20pre-training%20and%20fine-tuning%2C%20LLMs%20can%20perform%20diverse%0Adownstream%20tasks%20based%20on%20human%20instructions%2C%20paving%20the%20way%20to%20artificial%0Ageneral%20intelligence%20%28AGI%29-enabled%206G.%20Given%20the%20great%20potential%20of%20LLM%0Atechnologies%2C%20this%20work%20aims%20to%20provide%20a%20comprehensive%20overview%20of%20LLM-enabled%0Atelecom%20networks.%20In%20particular%2C%20we%20first%20present%20LLM%20fundamentals%2C%20including%0Amodel%20architecture%2C%20pre-training%2C%20fine-tuning%2C%20inference%20and%20utilization%2C%20model%0Aevaluation%2C%20and%20telecom%20deployment.%20Then%2C%20we%20introduce%20LLM-enabled%20key%0Atechniques%20and%20telecom%20applications%20in%20terms%20of%20generation%2C%20classification%2C%0Aoptimization%2C%20and%20prediction%20problems.%20Specifically%2C%20the%20LLM-enabled%20generation%0Aapplications%20include%20telecom%20domain%20knowledge%2C%20code%2C%20and%20network%20configuration%0Ageneration.%20After%20that%2C%20the%20LLM-based%20classification%20applications%20involve%0Anetwork%20security%2C%20text%2C%20image%2C%20and%20traffic%20classification%20problems.%20Moreover%2C%0Amultiple%20LLM-enabled%20optimization%20techniques%20are%20introduced%2C%20such%20as%20automated%0Areward%20function%20design%20for%20reinforcement%20learning%20and%20verbal%20reinforcement%0Alearning.%20Furthermore%2C%20for%20LLM-aided%20prediction%20problems%2C%20we%20discussed%0Atime-series%20prediction%20models%20and%20multi-modality%20prediction%20problems%20for%0Atelecom.%20Finally%2C%20we%20highlight%20the%20challenges%20and%20identify%20the%20future%0Adirections%20of%20LLM-enabled%20telecom%20networks.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2405.10825v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DLarge%2520Language%2520Model%2520%2528LLM%2529%2520for%2520Telecommunications%253A%2520A%2520Comprehensive%250A%2520%2520Survey%2520on%2520Principles%252C%2520Key%2520Techniques%252C%2520and%2520Opportunities%26entry.906535625%3DHao%2520Zhou%2520and%2520Chengming%2520Hu%2520and%2520Ye%2520Yuan%2520and%2520Yufei%2520Cui%2520and%2520Yili%2520Jin%2520and%2520Can%2520Chen%2520and%2520Haolun%2520Wu%2520and%2520Dun%2520Yuan%2520and%2520Li%2520Jiang%2520and%2520Di%2520Wu%2520and%2520Xue%2520Liu%2520and%2520Charlie%2520Zhang%2520and%2520Xianbin%2520Wang%2520and%2520Jiangchuan%2520Liu%26entry.1292438233%3D%2520%2520Large%2520language%2520models%2520%2528LLMs%2529%2520have%2520received%2520considerable%2520attention%2520recently%250Adue%2520to%2520their%2520outstanding%2520comprehension%2520and%2520reasoning%2520capabilities%252C%2520leading%2520to%250Agreat%2520progress%2520in%2520many%2520fields.%2520The%2520advancement%2520of%2520LLM%2520techniques%2520also%2520offers%250Apromising%2520opportunities%2520to%2520automate%2520many%2520tasks%2520in%2520the%2520telecommunication%250A%2528telecom%2529%2520field.%2520After%2520pre-training%2520and%2520fine-tuning%252C%2520LLMs%2520can%2520perform%2520diverse%250Adownstream%2520tasks%2520based%2520on%2520human%2520instructions%252C%2520paving%2520the%2520way%2520to%2520artificial%250Ageneral%2520intelligence%2520%2528AGI%2529-enabled%25206G.%2520Given%2520the%2520great%2520potential%2520of%2520LLM%250Atechnologies%252C%2520this%2520work%2520aims%2520to%2520provide%2520a%2520comprehensive%2520overview%2520of%2520LLM-enabled%250Atelecom%2520networks.%2520In%2520particular%252C%2520we%2520first%2520present%2520LLM%2520fundamentals%252C%2520including%250Amodel%2520architecture%252C%2520pre-training%252C%2520fine-tuning%252C%2520inference%2520and%2520utilization%252C%2520model%250Aevaluation%252C%2520and%2520telecom%2520deployment.%2520Then%252C%2520we%2520introduce%2520LLM-enabled%2520key%250Atechniques%2520and%2520telecom%2520applications%2520in%2520terms%2520of%2520generation%252C%2520classification%252C%250Aoptimization%252C%2520and%2520prediction%2520problems.%2520Specifically%252C%2520the%2520LLM-enabled%2520generation%250Aapplications%2520include%2520telecom%2520domain%2520knowledge%252C%2520code%252C%2520and%2520network%2520configuration%250Ageneration.%2520After%2520that%252C%2520the%2520LLM-based%2520classification%2520applications%2520involve%250Anetwork%2520security%252C%2520text%252C%2520image%252C%2520and%2520traffic%2520classification%2520problems.%2520Moreover%252C%250Amultiple%2520LLM-enabled%2520optimization%2520techniques%2520are%2520introduced%252C%2520such%2520as%2520automated%250Areward%2520function%2520design%2520for%2520reinforcement%2520learning%2520and%2520verbal%2520reinforcement%250Alearning.%2520Furthermore%252C%2520for%2520LLM-aided%2520prediction%2520problems%252C%2520we%2520discussed%250Atime-series%2520prediction%2520models%2520and%2520multi-modality%2520prediction%2520problems%2520for%250Atelecom.%2520Finally%252C%2520we%2520highlight%2520the%2520challenges%2520and%2520identify%2520the%2520future%250Adirections%2520of%2520LLM-enabled%2520telecom%2520networks.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2405.10825v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Large%20Language%20Model%20%28LLM%29%20for%20Telecommunications%3A%20A%20Comprehensive%0A%20%20Survey%20on%20Principles%2C%20Key%20Techniques%2C%20and%20Opportunities&entry.906535625=Hao%20Zhou%20and%20Chengming%20Hu%20and%20Ye%20Yuan%20and%20Yufei%20Cui%20and%20Yili%20Jin%20and%20Can%20Chen%20and%20Haolun%20Wu%20and%20Dun%20Yuan%20and%20Li%20Jiang%20and%20Di%20Wu%20and%20Xue%20Liu%20and%20Charlie%20Zhang%20and%20Xianbin%20Wang%20and%20Jiangchuan%20Liu&entry.1292438233=%20%20Large%20language%20models%20%28LLMs%29%20have%20received%20considerable%20attention%20recently%0Adue%20to%20their%20outstanding%20comprehension%20and%20reasoning%20capabilities%2C%20leading%20to%0Agreat%20progress%20in%20many%20fields.%20The%20advancement%20of%20LLM%20techniques%20also%20offers%0Apromising%20opportunities%20to%20automate%20many%20tasks%20in%20the%20telecommunication%0A%28telecom%29%20field.%20After%20pre-training%20and%20fine-tuning%2C%20LLMs%20can%20perform%20diverse%0Adownstream%20tasks%20based%20on%20human%20instructions%2C%20paving%20the%20way%20to%20artificial%0Ageneral%20intelligence%20%28AGI%29-enabled%206G.%20Given%20the%20great%20potential%20of%20LLM%0Atechnologies%2C%20this%20work%20aims%20to%20provide%20a%20comprehensive%20overview%20of%20LLM-enabled%0Atelecom%20networks.%20In%20particular%2C%20we%20first%20present%20LLM%20fundamentals%2C%20including%0Amodel%20architecture%2C%20pre-training%2C%20fine-tuning%2C%20inference%20and%20utilization%2C%20model%0Aevaluation%2C%20and%20telecom%20deployment.%20Then%2C%20we%20introduce%20LLM-enabled%20key%0Atechniques%20and%20telecom%20applications%20in%20terms%20of%20generation%2C%20classification%2C%0Aoptimization%2C%20and%20prediction%20problems.%20Specifically%2C%20the%20LLM-enabled%20generation%0Aapplications%20include%20telecom%20domain%20knowledge%2C%20code%2C%20and%20network%20configuration%0Ageneration.%20After%20that%2C%20the%20LLM-based%20classification%20applications%20involve%0Anetwork%20security%2C%20text%2C%20image%2C%20and%20traffic%20classification%20problems.%20Moreover%2C%0Amultiple%20LLM-enabled%20optimization%20techniques%20are%20introduced%2C%20such%20as%20automated%0Areward%20function%20design%20for%20reinforcement%20learning%20and%20verbal%20reinforcement%0Alearning.%20Furthermore%2C%20for%20LLM-aided%20prediction%20problems%2C%20we%20discussed%0Atime-series%20prediction%20models%20and%20multi-modality%20prediction%20problems%20for%0Atelecom.%20Finally%2C%20we%20highlight%20the%20challenges%20and%20identify%20the%20future%0Adirections%20of%20LLM-enabled%20telecom%20networks.%0A&entry.1838667208=http%3A//arxiv.org/abs/2405.10825v1&entry.124074799=Read"},
{"title": "FA-Depth: Toward Fast and Accurate Self-supervised Monocular Depth\n  Estimation", "author": "Fei Wang and Jun Cheng", "abstract": "  Most existing methods often rely on complex models to predict scene depth\nwith high accuracy, resulting in slow inference that is not conducive to\ndeployment. To better balance precision and speed, we first designed SmallDepth\nbased on sparsity. Second, to enhance the feature representation ability of\nSmallDepth during training under the condition of equal complexity during\ninference, we propose an equivalent transformation module(ETM). Third, to\nimprove the ability of each layer in the case of a fixed SmallDepth to perceive\ndifferent context information and improve the robustness of SmallDepth to the\nleft-right direction and illumination changes, we propose pyramid loss. Fourth,\nto further improve the accuracy of SmallDepth, we utilized the proposed\nfunction approximation loss (APX) to transfer knowledge in the pretrained\nHQDecv2, obtained by optimizing the previous HQDec to address grid artifacts in\nsome regions, to SmallDepth. Extensive experiments demonstrate that each\nproposed component improves the precision of SmallDepth without changing the\ncomplexity of SmallDepth during inference, and the developed approach achieves\nstate-of-the-art results on KITTI at an inference speed of more than 500 frames\nper second and with approximately 2 M parameters. The code and models will be\npublicly available at https://github.com/fwucas/FA-Depth.\n", "link": "http://arxiv.org/abs/2405.10885v1", "date": "2024-05-17", "relevancy": 1.7375, "topK": [{"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.5851}, {"title": "SDXS: Real-Time One-Step Latent Diffusion Models with Image Conditions", "link": "http://arxiv.org/abs/2403.16627v1", "similarity": 0.5793}, {"title": "ToDo: Token Downsampling for Efficient Generation of High-Resolution\n  Images", "link": "http://arxiv.org/abs/2402.13573v2", "similarity": 0.5767}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20FA-Depth%3A%20Toward%20Fast%20and%20Accurate%20Self-supervised%20Monocular%20Depth%0A%20%20Estimation&body=Title%3A%20FA-Depth%3A%20Toward%20Fast%20and%20Accurate%20Self-supervised%20Monocular%20Depth%0A%20%20Estimation%0AAuthor%3A%20Fei%20Wang%20and%20Jun%20Cheng%0AAbstract%3A%20%20%20Most%20existing%20methods%20often%20rely%20on%20complex%20models%20to%20predict%20scene%20depth%0Awith%20high%20accuracy%2C%20resulting%20in%20slow%20inference%20that%20is%20not%20conducive%20to%0Adeployment.%20To%20better%20balance%20precision%20and%20speed%2C%20we%20first%20designed%20SmallDepth%0Abased%20on%20sparsity.%20Second%2C%20to%20enhance%20the%20feature%20representation%20ability%20of%0ASmallDepth%20during%20training%20under%20the%20condition%20of%20equal%20complexity%20during%0Ainference%2C%20we%20propose%20an%20equivalent%20transformation%20module%28ETM%29.%20Third%2C%20to%0Aimprove%20the%20ability%20of%20each%20layer%20in%20the%20case%20of%20a%20fixed%20SmallDepth%20to%20perceive%0Adifferent%20context%20information%20and%20improve%20the%20robustness%20of%20SmallDepth%20to%20the%0Aleft-right%20direction%20and%20illumination%20changes%2C%20we%20propose%20pyramid%20loss.%20Fourth%2C%0Ato%20further%20improve%20the%20accuracy%20of%20SmallDepth%2C%20we%20utilized%20the%20proposed%0Afunction%20approximation%20loss%20%28APX%29%20to%20transfer%20knowledge%20in%20the%20pretrained%0AHQDecv2%2C%20obtained%20by%20optimizing%20the%20previous%20HQDec%20to%20address%20grid%20artifacts%20in%0Asome%20regions%2C%20to%20SmallDepth.%20Extensive%20experiments%20demonstrate%20that%20each%0Aproposed%20component%20improves%20the%20precision%20of%20SmallDepth%20without%20changing%20the%0Acomplexity%20of%20SmallDepth%20during%20inference%2C%20and%20the%20developed%20approach%20achieves%0Astate-of-the-art%20results%20on%20KITTI%20at%20an%20inference%20speed%20of%20more%20than%20500%20frames%0Aper%20second%20and%20with%20approximately%202%20M%20parameters.%20The%20code%20and%20models%20will%20be%0Apublicly%20available%20at%20https%3A//github.com/fwucas/FA-Depth.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2405.10885v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DFA-Depth%253A%2520Toward%2520Fast%2520and%2520Accurate%2520Self-supervised%2520Monocular%2520Depth%250A%2520%2520Estimation%26entry.906535625%3DFei%2520Wang%2520and%2520Jun%2520Cheng%26entry.1292438233%3D%2520%2520Most%2520existing%2520methods%2520often%2520rely%2520on%2520complex%2520models%2520to%2520predict%2520scene%2520depth%250Awith%2520high%2520accuracy%252C%2520resulting%2520in%2520slow%2520inference%2520that%2520is%2520not%2520conducive%2520to%250Adeployment.%2520To%2520better%2520balance%2520precision%2520and%2520speed%252C%2520we%2520first%2520designed%2520SmallDepth%250Abased%2520on%2520sparsity.%2520Second%252C%2520to%2520enhance%2520the%2520feature%2520representation%2520ability%2520of%250ASmallDepth%2520during%2520training%2520under%2520the%2520condition%2520of%2520equal%2520complexity%2520during%250Ainference%252C%2520we%2520propose%2520an%2520equivalent%2520transformation%2520module%2528ETM%2529.%2520Third%252C%2520to%250Aimprove%2520the%2520ability%2520of%2520each%2520layer%2520in%2520the%2520case%2520of%2520a%2520fixed%2520SmallDepth%2520to%2520perceive%250Adifferent%2520context%2520information%2520and%2520improve%2520the%2520robustness%2520of%2520SmallDepth%2520to%2520the%250Aleft-right%2520direction%2520and%2520illumination%2520changes%252C%2520we%2520propose%2520pyramid%2520loss.%2520Fourth%252C%250Ato%2520further%2520improve%2520the%2520accuracy%2520of%2520SmallDepth%252C%2520we%2520utilized%2520the%2520proposed%250Afunction%2520approximation%2520loss%2520%2528APX%2529%2520to%2520transfer%2520knowledge%2520in%2520the%2520pretrained%250AHQDecv2%252C%2520obtained%2520by%2520optimizing%2520the%2520previous%2520HQDec%2520to%2520address%2520grid%2520artifacts%2520in%250Asome%2520regions%252C%2520to%2520SmallDepth.%2520Extensive%2520experiments%2520demonstrate%2520that%2520each%250Aproposed%2520component%2520improves%2520the%2520precision%2520of%2520SmallDepth%2520without%2520changing%2520the%250Acomplexity%2520of%2520SmallDepth%2520during%2520inference%252C%2520and%2520the%2520developed%2520approach%2520achieves%250Astate-of-the-art%2520results%2520on%2520KITTI%2520at%2520an%2520inference%2520speed%2520of%2520more%2520than%2520500%2520frames%250Aper%2520second%2520and%2520with%2520approximately%25202%2520M%2520parameters.%2520The%2520code%2520and%2520models%2520will%2520be%250Apublicly%2520available%2520at%2520https%253A//github.com/fwucas/FA-Depth.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2405.10885v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=FA-Depth%3A%20Toward%20Fast%20and%20Accurate%20Self-supervised%20Monocular%20Depth%0A%20%20Estimation&entry.906535625=Fei%20Wang%20and%20Jun%20Cheng&entry.1292438233=%20%20Most%20existing%20methods%20often%20rely%20on%20complex%20models%20to%20predict%20scene%20depth%0Awith%20high%20accuracy%2C%20resulting%20in%20slow%20inference%20that%20is%20not%20conducive%20to%0Adeployment.%20To%20better%20balance%20precision%20and%20speed%2C%20we%20first%20designed%20SmallDepth%0Abased%20on%20sparsity.%20Second%2C%20to%20enhance%20the%20feature%20representation%20ability%20of%0ASmallDepth%20during%20training%20under%20the%20condition%20of%20equal%20complexity%20during%0Ainference%2C%20we%20propose%20an%20equivalent%20transformation%20module%28ETM%29.%20Third%2C%20to%0Aimprove%20the%20ability%20of%20each%20layer%20in%20the%20case%20of%20a%20fixed%20SmallDepth%20to%20perceive%0Adifferent%20context%20information%20and%20improve%20the%20robustness%20of%20SmallDepth%20to%20the%0Aleft-right%20direction%20and%20illumination%20changes%2C%20we%20propose%20pyramid%20loss.%20Fourth%2C%0Ato%20further%20improve%20the%20accuracy%20of%20SmallDepth%2C%20we%20utilized%20the%20proposed%0Afunction%20approximation%20loss%20%28APX%29%20to%20transfer%20knowledge%20in%20the%20pretrained%0AHQDecv2%2C%20obtained%20by%20optimizing%20the%20previous%20HQDec%20to%20address%20grid%20artifacts%20in%0Asome%20regions%2C%20to%20SmallDepth.%20Extensive%20experiments%20demonstrate%20that%20each%0Aproposed%20component%20improves%20the%20precision%20of%20SmallDepth%20without%20changing%20the%0Acomplexity%20of%20SmallDepth%20during%20inference%2C%20and%20the%20developed%20approach%20achieves%0Astate-of-the-art%20results%20on%20KITTI%20at%20an%20inference%20speed%20of%20more%20than%20500%20frames%0Aper%20second%20and%20with%20approximately%202%20M%20parameters.%20The%20code%20and%20models%20will%20be%0Apublicly%20available%20at%20https%3A//github.com/fwucas/FA-Depth.%0A&entry.1838667208=http%3A//arxiv.org/abs/2405.10885v1&entry.124074799=Read"},
{"title": "Towards auditory attention decoding with noise-tagging: A pilot study", "author": "H. A. Scheppink and S. Ahmadi and P. Desain and M. Tangermann and J. Thielen", "abstract": "  Auditory attention decoding (AAD) aims to extract from brain activity the\nattended speaker amidst candidate speakers, offering promising applications for\nneuro-steered hearing devices and brain-computer interfacing. This pilot study\nmakes a first step towards AAD using the noise-tagging stimulus protocol, which\nevokes reliable code-modulated evoked potentials, but is minimally explored in\nthe auditory modality. Participants were sequentially presented with two Dutch\nspeech stimuli that were amplitude-modulated with a unique binary pseudo-random\nnoise-code, effectively tagging these with additional decodable information. We\ncompared the decoding of unmodulated audio against audio modulated with various\nmodulation depths, and a conventional AAD method against a standard method to\ndecode noise-codes. Our pilot study revealed higher performances for the\nconventional method with 70 to 100 percent modulation depths compared to\nunmodulated audio. The noise-code decoder did not further improve these\nresults. These fundamental insights highlight the potential of integrating\nnoise-codes in speech to enhance auditory speaker detection when multiple\nspeakers are presented simultaneously.\n", "link": "http://arxiv.org/abs/2403.15523v2", "date": "2024-05-17", "relevancy": 1.7366, "topK": [{"title": "ToDo: Token Downsampling for Efficient Generation of High-Resolution\n  Images", "link": "http://arxiv.org/abs/2402.13573v2", "similarity": 0.4396}, {"title": "Tuning-Free Noise Rectification for High Fidelity Image-to-Video\n  Generation", "link": "http://arxiv.org/abs/2403.02827v1", "similarity": 0.4338}, {"title": "SDXS: Real-Time One-Step Latent Diffusion Models with Image Conditions", "link": "http://arxiv.org/abs/2403.16627v1", "similarity": 0.4212}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Towards%20auditory%20attention%20decoding%20with%20noise-tagging%3A%20A%20pilot%20study&body=Title%3A%20Towards%20auditory%20attention%20decoding%20with%20noise-tagging%3A%20A%20pilot%20study%0AAuthor%3A%20H.%20A.%20Scheppink%20and%20S.%20Ahmadi%20and%20P.%20Desain%20and%20M.%20Tangermann%20and%20J.%20Thielen%0AAbstract%3A%20%20%20Auditory%20attention%20decoding%20%28AAD%29%20aims%20to%20extract%20from%20brain%20activity%20the%0Aattended%20speaker%20amidst%20candidate%20speakers%2C%20offering%20promising%20applications%20for%0Aneuro-steered%20hearing%20devices%20and%20brain-computer%20interfacing.%20This%20pilot%20study%0Amakes%20a%20first%20step%20towards%20AAD%20using%20the%20noise-tagging%20stimulus%20protocol%2C%20which%0Aevokes%20reliable%20code-modulated%20evoked%20potentials%2C%20but%20is%20minimally%20explored%20in%0Athe%20auditory%20modality.%20Participants%20were%20sequentially%20presented%20with%20two%20Dutch%0Aspeech%20stimuli%20that%20were%20amplitude-modulated%20with%20a%20unique%20binary%20pseudo-random%0Anoise-code%2C%20effectively%20tagging%20these%20with%20additional%20decodable%20information.%20We%0Acompared%20the%20decoding%20of%20unmodulated%20audio%20against%20audio%20modulated%20with%20various%0Amodulation%20depths%2C%20and%20a%20conventional%20AAD%20method%20against%20a%20standard%20method%20to%0Adecode%20noise-codes.%20Our%20pilot%20study%20revealed%20higher%20performances%20for%20the%0Aconventional%20method%20with%2070%20to%20100%20percent%20modulation%20depths%20compared%20to%0Aunmodulated%20audio.%20The%20noise-code%20decoder%20did%20not%20further%20improve%20these%0Aresults.%20These%20fundamental%20insights%20highlight%20the%20potential%20of%20integrating%0Anoise-codes%20in%20speech%20to%20enhance%20auditory%20speaker%20detection%20when%20multiple%0Aspeakers%20are%20presented%20simultaneously.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2403.15523v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DTowards%2520auditory%2520attention%2520decoding%2520with%2520noise-tagging%253A%2520A%2520pilot%2520study%26entry.906535625%3DH.%2520A.%2520Scheppink%2520and%2520S.%2520Ahmadi%2520and%2520P.%2520Desain%2520and%2520M.%2520Tangermann%2520and%2520J.%2520Thielen%26entry.1292438233%3D%2520%2520Auditory%2520attention%2520decoding%2520%2528AAD%2529%2520aims%2520to%2520extract%2520from%2520brain%2520activity%2520the%250Aattended%2520speaker%2520amidst%2520candidate%2520speakers%252C%2520offering%2520promising%2520applications%2520for%250Aneuro-steered%2520hearing%2520devices%2520and%2520brain-computer%2520interfacing.%2520This%2520pilot%2520study%250Amakes%2520a%2520first%2520step%2520towards%2520AAD%2520using%2520the%2520noise-tagging%2520stimulus%2520protocol%252C%2520which%250Aevokes%2520reliable%2520code-modulated%2520evoked%2520potentials%252C%2520but%2520is%2520minimally%2520explored%2520in%250Athe%2520auditory%2520modality.%2520Participants%2520were%2520sequentially%2520presented%2520with%2520two%2520Dutch%250Aspeech%2520stimuli%2520that%2520were%2520amplitude-modulated%2520with%2520a%2520unique%2520binary%2520pseudo-random%250Anoise-code%252C%2520effectively%2520tagging%2520these%2520with%2520additional%2520decodable%2520information.%2520We%250Acompared%2520the%2520decoding%2520of%2520unmodulated%2520audio%2520against%2520audio%2520modulated%2520with%2520various%250Amodulation%2520depths%252C%2520and%2520a%2520conventional%2520AAD%2520method%2520against%2520a%2520standard%2520method%2520to%250Adecode%2520noise-codes.%2520Our%2520pilot%2520study%2520revealed%2520higher%2520performances%2520for%2520the%250Aconventional%2520method%2520with%252070%2520to%2520100%2520percent%2520modulation%2520depths%2520compared%2520to%250Aunmodulated%2520audio.%2520The%2520noise-code%2520decoder%2520did%2520not%2520further%2520improve%2520these%250Aresults.%2520These%2520fundamental%2520insights%2520highlight%2520the%2520potential%2520of%2520integrating%250Anoise-codes%2520in%2520speech%2520to%2520enhance%2520auditory%2520speaker%2520detection%2520when%2520multiple%250Aspeakers%2520are%2520presented%2520simultaneously.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2403.15523v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Towards%20auditory%20attention%20decoding%20with%20noise-tagging%3A%20A%20pilot%20study&entry.906535625=H.%20A.%20Scheppink%20and%20S.%20Ahmadi%20and%20P.%20Desain%20and%20M.%20Tangermann%20and%20J.%20Thielen&entry.1292438233=%20%20Auditory%20attention%20decoding%20%28AAD%29%20aims%20to%20extract%20from%20brain%20activity%20the%0Aattended%20speaker%20amidst%20candidate%20speakers%2C%20offering%20promising%20applications%20for%0Aneuro-steered%20hearing%20devices%20and%20brain-computer%20interfacing.%20This%20pilot%20study%0Amakes%20a%20first%20step%20towards%20AAD%20using%20the%20noise-tagging%20stimulus%20protocol%2C%20which%0Aevokes%20reliable%20code-modulated%20evoked%20potentials%2C%20but%20is%20minimally%20explored%20in%0Athe%20auditory%20modality.%20Participants%20were%20sequentially%20presented%20with%20two%20Dutch%0Aspeech%20stimuli%20that%20were%20amplitude-modulated%20with%20a%20unique%20binary%20pseudo-random%0Anoise-code%2C%20effectively%20tagging%20these%20with%20additional%20decodable%20information.%20We%0Acompared%20the%20decoding%20of%20unmodulated%20audio%20against%20audio%20modulated%20with%20various%0Amodulation%20depths%2C%20and%20a%20conventional%20AAD%20method%20against%20a%20standard%20method%20to%0Adecode%20noise-codes.%20Our%20pilot%20study%20revealed%20higher%20performances%20for%20the%0Aconventional%20method%20with%2070%20to%20100%20percent%20modulation%20depths%20compared%20to%0Aunmodulated%20audio.%20The%20noise-code%20decoder%20did%20not%20further%20improve%20these%0Aresults.%20These%20fundamental%20insights%20highlight%20the%20potential%20of%20integrating%0Anoise-codes%20in%20speech%20to%20enhance%20auditory%20speaker%20detection%20when%20multiple%0Aspeakers%20are%20presented%20simultaneously.%0A&entry.1838667208=http%3A//arxiv.org/abs/2403.15523v2&entry.124074799=Read"},
{"title": "TeenyTinyLlama: open-source tiny language models trained in Brazilian\n  Portuguese", "author": "Nicholas Kluge Corr\u00eaa and Sophia Falk and Shiza Fatimah and Aniket Sen and Nythamar de Oliveira", "abstract": "  Large language models (LLMs) have significantly advanced natural language\nprocessing, but their progress has yet to be equal across languages. While most\nLLMs are trained in high-resource languages like English, multilingual models\ngenerally underperform monolingual ones. Additionally, aspects of their\nmultilingual foundation sometimes restrict the byproducts they produce, like\ncomputational demands and licensing regimes. In this study, we document the\ndevelopment of open-foundation models tailored for use in low-resource\nsettings, their limitations, and their benefits. This is the TeenyTinyLlama\npair: two compact models for Brazilian Portuguese text generation. We release\nthem under the permissive Apache 2.0 license on GitHub and Hugging Face for\ncommunity use and further development. See\nhttps://github.com/Nkluge-correa/TeenyTinyLlama\n", "link": "http://arxiv.org/abs/2401.16640v3", "date": "2024-05-17", "relevancy": 1.7332, "topK": [{"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.4609}, {"title": "Genie: Generative Interactive Environments", "link": "http://arxiv.org/abs/2402.15391v1", "similarity": 0.4149}, {"title": "SDXS: Real-Time One-Step Latent Diffusion Models with Image Conditions", "link": "http://arxiv.org/abs/2403.16627v1", "similarity": 0.4102}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20TeenyTinyLlama%3A%20open-source%20tiny%20language%20models%20trained%20in%20Brazilian%0A%20%20Portuguese&body=Title%3A%20TeenyTinyLlama%3A%20open-source%20tiny%20language%20models%20trained%20in%20Brazilian%0A%20%20Portuguese%0AAuthor%3A%20Nicholas%20Kluge%20Corr%C3%AAa%20and%20Sophia%20Falk%20and%20Shiza%20Fatimah%20and%20Aniket%20Sen%20and%20Nythamar%20de%20Oliveira%0AAbstract%3A%20%20%20Large%20language%20models%20%28LLMs%29%20have%20significantly%20advanced%20natural%20language%0Aprocessing%2C%20but%20their%20progress%20has%20yet%20to%20be%20equal%20across%20languages.%20While%20most%0ALLMs%20are%20trained%20in%20high-resource%20languages%20like%20English%2C%20multilingual%20models%0Agenerally%20underperform%20monolingual%20ones.%20Additionally%2C%20aspects%20of%20their%0Amultilingual%20foundation%20sometimes%20restrict%20the%20byproducts%20they%20produce%2C%20like%0Acomputational%20demands%20and%20licensing%20regimes.%20In%20this%20study%2C%20we%20document%20the%0Adevelopment%20of%20open-foundation%20models%20tailored%20for%20use%20in%20low-resource%0Asettings%2C%20their%20limitations%2C%20and%20their%20benefits.%20This%20is%20the%20TeenyTinyLlama%0Apair%3A%20two%20compact%20models%20for%20Brazilian%20Portuguese%20text%20generation.%20We%20release%0Athem%20under%20the%20permissive%20Apache%202.0%20license%20on%20GitHub%20and%20Hugging%20Face%20for%0Acommunity%20use%20and%20further%20development.%20See%0Ahttps%3A//github.com/Nkluge-correa/TeenyTinyLlama%0A%0ALink%3A%20http%3A//arxiv.org/abs/2401.16640v3%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DTeenyTinyLlama%253A%2520open-source%2520tiny%2520language%2520models%2520trained%2520in%2520Brazilian%250A%2520%2520Portuguese%26entry.906535625%3DNicholas%2520Kluge%2520Corr%25C3%25AAa%2520and%2520Sophia%2520Falk%2520and%2520Shiza%2520Fatimah%2520and%2520Aniket%2520Sen%2520and%2520Nythamar%2520de%2520Oliveira%26entry.1292438233%3D%2520%2520Large%2520language%2520models%2520%2528LLMs%2529%2520have%2520significantly%2520advanced%2520natural%2520language%250Aprocessing%252C%2520but%2520their%2520progress%2520has%2520yet%2520to%2520be%2520equal%2520across%2520languages.%2520While%2520most%250ALLMs%2520are%2520trained%2520in%2520high-resource%2520languages%2520like%2520English%252C%2520multilingual%2520models%250Agenerally%2520underperform%2520monolingual%2520ones.%2520Additionally%252C%2520aspects%2520of%2520their%250Amultilingual%2520foundation%2520sometimes%2520restrict%2520the%2520byproducts%2520they%2520produce%252C%2520like%250Acomputational%2520demands%2520and%2520licensing%2520regimes.%2520In%2520this%2520study%252C%2520we%2520document%2520the%250Adevelopment%2520of%2520open-foundation%2520models%2520tailored%2520for%2520use%2520in%2520low-resource%250Asettings%252C%2520their%2520limitations%252C%2520and%2520their%2520benefits.%2520This%2520is%2520the%2520TeenyTinyLlama%250Apair%253A%2520two%2520compact%2520models%2520for%2520Brazilian%2520Portuguese%2520text%2520generation.%2520We%2520release%250Athem%2520under%2520the%2520permissive%2520Apache%25202.0%2520license%2520on%2520GitHub%2520and%2520Hugging%2520Face%2520for%250Acommunity%2520use%2520and%2520further%2520development.%2520See%250Ahttps%253A//github.com/Nkluge-correa/TeenyTinyLlama%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2401.16640v3%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=TeenyTinyLlama%3A%20open-source%20tiny%20language%20models%20trained%20in%20Brazilian%0A%20%20Portuguese&entry.906535625=Nicholas%20Kluge%20Corr%C3%AAa%20and%20Sophia%20Falk%20and%20Shiza%20Fatimah%20and%20Aniket%20Sen%20and%20Nythamar%20de%20Oliveira&entry.1292438233=%20%20Large%20language%20models%20%28LLMs%29%20have%20significantly%20advanced%20natural%20language%0Aprocessing%2C%20but%20their%20progress%20has%20yet%20to%20be%20equal%20across%20languages.%20While%20most%0ALLMs%20are%20trained%20in%20high-resource%20languages%20like%20English%2C%20multilingual%20models%0Agenerally%20underperform%20monolingual%20ones.%20Additionally%2C%20aspects%20of%20their%0Amultilingual%20foundation%20sometimes%20restrict%20the%20byproducts%20they%20produce%2C%20like%0Acomputational%20demands%20and%20licensing%20regimes.%20In%20this%20study%2C%20we%20document%20the%0Adevelopment%20of%20open-foundation%20models%20tailored%20for%20use%20in%20low-resource%0Asettings%2C%20their%20limitations%2C%20and%20their%20benefits.%20This%20is%20the%20TeenyTinyLlama%0Apair%3A%20two%20compact%20models%20for%20Brazilian%20Portuguese%20text%20generation.%20We%20release%0Athem%20under%20the%20permissive%20Apache%202.0%20license%20on%20GitHub%20and%20Hugging%20Face%20for%0Acommunity%20use%20and%20further%20development.%20See%0Ahttps%3A//github.com/Nkluge-correa/TeenyTinyLlama%0A&entry.1838667208=http%3A//arxiv.org/abs/2401.16640v3&entry.124074799=Read"},
{"title": "BraTS-Path Challenge: Assessing Heterogeneous Histopathologic Brain\n  Tumor Sub-regions", "author": "Spyridon Bakas and Siddhesh P. Thakur and Shahriar Faghani and Mana Moassefi and Ujjwal Baid and Verena Chung and Sarthak Pati and Shubham Innani and Bhakti Baheti and Jake Albrecht and Alexandros Karargyris and Hasan Kassem and MacLean P. Nasrallah and Jared T. Ahrendsen and Valeria Barresi and Maria A. Gubbiotti and Giselle Y. L\u00f3pez and Calixto-Hope G. Lucas and Michael L. Miller and Lee A. D. Cooper and Jason T. Huse and William R. Bell", "abstract": "  Glioblastoma is the most common primary adult brain tumor, with a grim\nprognosis - median survival of 12-18 months following treatment, and 4 months\notherwise. Glioblastoma is widely infiltrative in the cerebral hemispheres and\nwell-defined by heterogeneous molecular and micro-environmental histopathologic\nprofiles, which pose a major obstacle in treatment. Correctly diagnosing these\ntumors and assessing their heterogeneity is crucial for choosing the precise\ntreatment and potentially enhancing patient survival rates. In the\ngold-standard histopathology-based approach to tumor diagnosis, detecting\nvarious morpho-pathological features of distinct histology throughout digitized\ntissue sections is crucial. Such \"features\" include the presence of cellular\ntumor, geographic necrosis, pseudopalisading necrosis, areas abundant in\nmicrovascular proliferation, infiltration into the cortex, wide extension in\nsubcortical white matter, leptomeningeal infiltration, regions dense with\nmacrophages, and the presence of perivascular or scattered lymphocytes. With\nthese features in mind and building upon the main aim of the BraTS Cluster of\nChallenges https://www.synapse.org/brats2024, the goal of the BraTS-Path\nchallenge is to provide a systematically prepared comprehensive dataset and a\nbenchmarking environment to develop and fairly compare deep-learning models\ncapable of identifying tumor sub-regions of distinct histologic profile. These\nmodels aim to further our understanding of the disease and assist in the\ndiagnosis and grading of conditions in a consistent manner.\n", "link": "http://arxiv.org/abs/2405.10871v1", "date": "2024-05-17", "relevancy": 1.7129, "topK": [{"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.4418}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.4397}, {"title": "RTG-SLAM: Real-time 3D Reconstruction at Scale using Gaussian Splatting", "link": "http://arxiv.org/abs/2404.19706v1", "similarity": 0.41}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20BraTS-Path%20Challenge%3A%20Assessing%20Heterogeneous%20Histopathologic%20Brain%0A%20%20Tumor%20Sub-regions&body=Title%3A%20BraTS-Path%20Challenge%3A%20Assessing%20Heterogeneous%20Histopathologic%20Brain%0A%20%20Tumor%20Sub-regions%0AAuthor%3A%20Spyridon%20Bakas%20and%20Siddhesh%20P.%20Thakur%20and%20Shahriar%20Faghani%20and%20Mana%20Moassefi%20and%20Ujjwal%20Baid%20and%20Verena%20Chung%20and%20Sarthak%20Pati%20and%20Shubham%20Innani%20and%20Bhakti%20Baheti%20and%20Jake%20Albrecht%20and%20Alexandros%20Karargyris%20and%20Hasan%20Kassem%20and%20MacLean%20P.%20Nasrallah%20and%20Jared%20T.%20Ahrendsen%20and%20Valeria%20Barresi%20and%20Maria%20A.%20Gubbiotti%20and%20Giselle%20Y.%20L%C3%B3pez%20and%20Calixto-Hope%20G.%20Lucas%20and%20Michael%20L.%20Miller%20and%20Lee%20A.%20D.%20Cooper%20and%20Jason%20T.%20Huse%20and%20William%20R.%20Bell%0AAbstract%3A%20%20%20Glioblastoma%20is%20the%20most%20common%20primary%20adult%20brain%20tumor%2C%20with%20a%20grim%0Aprognosis%20-%20median%20survival%20of%2012-18%20months%20following%20treatment%2C%20and%204%20months%0Aotherwise.%20Glioblastoma%20is%20widely%20infiltrative%20in%20the%20cerebral%20hemispheres%20and%0Awell-defined%20by%20heterogeneous%20molecular%20and%20micro-environmental%20histopathologic%0Aprofiles%2C%20which%20pose%20a%20major%20obstacle%20in%20treatment.%20Correctly%20diagnosing%20these%0Atumors%20and%20assessing%20their%20heterogeneity%20is%20crucial%20for%20choosing%20the%20precise%0Atreatment%20and%20potentially%20enhancing%20patient%20survival%20rates.%20In%20the%0Agold-standard%20histopathology-based%20approach%20to%20tumor%20diagnosis%2C%20detecting%0Avarious%20morpho-pathological%20features%20of%20distinct%20histology%20throughout%20digitized%0Atissue%20sections%20is%20crucial.%20Such%20%22features%22%20include%20the%20presence%20of%20cellular%0Atumor%2C%20geographic%20necrosis%2C%20pseudopalisading%20necrosis%2C%20areas%20abundant%20in%0Amicrovascular%20proliferation%2C%20infiltration%20into%20the%20cortex%2C%20wide%20extension%20in%0Asubcortical%20white%20matter%2C%20leptomeningeal%20infiltration%2C%20regions%20dense%20with%0Amacrophages%2C%20and%20the%20presence%20of%20perivascular%20or%20scattered%20lymphocytes.%20With%0Athese%20features%20in%20mind%20and%20building%20upon%20the%20main%20aim%20of%20the%20BraTS%20Cluster%20of%0AChallenges%20https%3A//www.synapse.org/brats2024%2C%20the%20goal%20of%20the%20BraTS-Path%0Achallenge%20is%20to%20provide%20a%20systematically%20prepared%20comprehensive%20dataset%20and%20a%0Abenchmarking%20environment%20to%20develop%20and%20fairly%20compare%20deep-learning%20models%0Acapable%20of%20identifying%20tumor%20sub-regions%20of%20distinct%20histologic%20profile.%20These%0Amodels%20aim%20to%20further%20our%20understanding%20of%20the%20disease%20and%20assist%20in%20the%0Adiagnosis%20and%20grading%20of%20conditions%20in%20a%20consistent%20manner.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2405.10871v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DBraTS-Path%2520Challenge%253A%2520Assessing%2520Heterogeneous%2520Histopathologic%2520Brain%250A%2520%2520Tumor%2520Sub-regions%26entry.906535625%3DSpyridon%2520Bakas%2520and%2520Siddhesh%2520P.%2520Thakur%2520and%2520Shahriar%2520Faghani%2520and%2520Mana%2520Moassefi%2520and%2520Ujjwal%2520Baid%2520and%2520Verena%2520Chung%2520and%2520Sarthak%2520Pati%2520and%2520Shubham%2520Innani%2520and%2520Bhakti%2520Baheti%2520and%2520Jake%2520Albrecht%2520and%2520Alexandros%2520Karargyris%2520and%2520Hasan%2520Kassem%2520and%2520MacLean%2520P.%2520Nasrallah%2520and%2520Jared%2520T.%2520Ahrendsen%2520and%2520Valeria%2520Barresi%2520and%2520Maria%2520A.%2520Gubbiotti%2520and%2520Giselle%2520Y.%2520L%25C3%25B3pez%2520and%2520Calixto-Hope%2520G.%2520Lucas%2520and%2520Michael%2520L.%2520Miller%2520and%2520Lee%2520A.%2520D.%2520Cooper%2520and%2520Jason%2520T.%2520Huse%2520and%2520William%2520R.%2520Bell%26entry.1292438233%3D%2520%2520Glioblastoma%2520is%2520the%2520most%2520common%2520primary%2520adult%2520brain%2520tumor%252C%2520with%2520a%2520grim%250Aprognosis%2520-%2520median%2520survival%2520of%252012-18%2520months%2520following%2520treatment%252C%2520and%25204%2520months%250Aotherwise.%2520Glioblastoma%2520is%2520widely%2520infiltrative%2520in%2520the%2520cerebral%2520hemispheres%2520and%250Awell-defined%2520by%2520heterogeneous%2520molecular%2520and%2520micro-environmental%2520histopathologic%250Aprofiles%252C%2520which%2520pose%2520a%2520major%2520obstacle%2520in%2520treatment.%2520Correctly%2520diagnosing%2520these%250Atumors%2520and%2520assessing%2520their%2520heterogeneity%2520is%2520crucial%2520for%2520choosing%2520the%2520precise%250Atreatment%2520and%2520potentially%2520enhancing%2520patient%2520survival%2520rates.%2520In%2520the%250Agold-standard%2520histopathology-based%2520approach%2520to%2520tumor%2520diagnosis%252C%2520detecting%250Avarious%2520morpho-pathological%2520features%2520of%2520distinct%2520histology%2520throughout%2520digitized%250Atissue%2520sections%2520is%2520crucial.%2520Such%2520%2522features%2522%2520include%2520the%2520presence%2520of%2520cellular%250Atumor%252C%2520geographic%2520necrosis%252C%2520pseudopalisading%2520necrosis%252C%2520areas%2520abundant%2520in%250Amicrovascular%2520proliferation%252C%2520infiltration%2520into%2520the%2520cortex%252C%2520wide%2520extension%2520in%250Asubcortical%2520white%2520matter%252C%2520leptomeningeal%2520infiltration%252C%2520regions%2520dense%2520with%250Amacrophages%252C%2520and%2520the%2520presence%2520of%2520perivascular%2520or%2520scattered%2520lymphocytes.%2520With%250Athese%2520features%2520in%2520mind%2520and%2520building%2520upon%2520the%2520main%2520aim%2520of%2520the%2520BraTS%2520Cluster%2520of%250AChallenges%2520https%253A//www.synapse.org/brats2024%252C%2520the%2520goal%2520of%2520the%2520BraTS-Path%250Achallenge%2520is%2520to%2520provide%2520a%2520systematically%2520prepared%2520comprehensive%2520dataset%2520and%2520a%250Abenchmarking%2520environment%2520to%2520develop%2520and%2520fairly%2520compare%2520deep-learning%2520models%250Acapable%2520of%2520identifying%2520tumor%2520sub-regions%2520of%2520distinct%2520histologic%2520profile.%2520These%250Amodels%2520aim%2520to%2520further%2520our%2520understanding%2520of%2520the%2520disease%2520and%2520assist%2520in%2520the%250Adiagnosis%2520and%2520grading%2520of%2520conditions%2520in%2520a%2520consistent%2520manner.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2405.10871v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=BraTS-Path%20Challenge%3A%20Assessing%20Heterogeneous%20Histopathologic%20Brain%0A%20%20Tumor%20Sub-regions&entry.906535625=Spyridon%20Bakas%20and%20Siddhesh%20P.%20Thakur%20and%20Shahriar%20Faghani%20and%20Mana%20Moassefi%20and%20Ujjwal%20Baid%20and%20Verena%20Chung%20and%20Sarthak%20Pati%20and%20Shubham%20Innani%20and%20Bhakti%20Baheti%20and%20Jake%20Albrecht%20and%20Alexandros%20Karargyris%20and%20Hasan%20Kassem%20and%20MacLean%20P.%20Nasrallah%20and%20Jared%20T.%20Ahrendsen%20and%20Valeria%20Barresi%20and%20Maria%20A.%20Gubbiotti%20and%20Giselle%20Y.%20L%C3%B3pez%20and%20Calixto-Hope%20G.%20Lucas%20and%20Michael%20L.%20Miller%20and%20Lee%20A.%20D.%20Cooper%20and%20Jason%20T.%20Huse%20and%20William%20R.%20Bell&entry.1292438233=%20%20Glioblastoma%20is%20the%20most%20common%20primary%20adult%20brain%20tumor%2C%20with%20a%20grim%0Aprognosis%20-%20median%20survival%20of%2012-18%20months%20following%20treatment%2C%20and%204%20months%0Aotherwise.%20Glioblastoma%20is%20widely%20infiltrative%20in%20the%20cerebral%20hemispheres%20and%0Awell-defined%20by%20heterogeneous%20molecular%20and%20micro-environmental%20histopathologic%0Aprofiles%2C%20which%20pose%20a%20major%20obstacle%20in%20treatment.%20Correctly%20diagnosing%20these%0Atumors%20and%20assessing%20their%20heterogeneity%20is%20crucial%20for%20choosing%20the%20precise%0Atreatment%20and%20potentially%20enhancing%20patient%20survival%20rates.%20In%20the%0Agold-standard%20histopathology-based%20approach%20to%20tumor%20diagnosis%2C%20detecting%0Avarious%20morpho-pathological%20features%20of%20distinct%20histology%20throughout%20digitized%0Atissue%20sections%20is%20crucial.%20Such%20%22features%22%20include%20the%20presence%20of%20cellular%0Atumor%2C%20geographic%20necrosis%2C%20pseudopalisading%20necrosis%2C%20areas%20abundant%20in%0Amicrovascular%20proliferation%2C%20infiltration%20into%20the%20cortex%2C%20wide%20extension%20in%0Asubcortical%20white%20matter%2C%20leptomeningeal%20infiltration%2C%20regions%20dense%20with%0Amacrophages%2C%20and%20the%20presence%20of%20perivascular%20or%20scattered%20lymphocytes.%20With%0Athese%20features%20in%20mind%20and%20building%20upon%20the%20main%20aim%20of%20the%20BraTS%20Cluster%20of%0AChallenges%20https%3A//www.synapse.org/brats2024%2C%20the%20goal%20of%20the%20BraTS-Path%0Achallenge%20is%20to%20provide%20a%20systematically%20prepared%20comprehensive%20dataset%20and%20a%0Abenchmarking%20environment%20to%20develop%20and%20fairly%20compare%20deep-learning%20models%0Acapable%20of%20identifying%20tumor%20sub-regions%20of%20distinct%20histologic%20profile.%20These%0Amodels%20aim%20to%20further%20our%20understanding%20of%20the%20disease%20and%20assist%20in%20the%0Adiagnosis%20and%20grading%20of%20conditions%20in%20a%20consistent%20manner.%0A&entry.1838667208=http%3A//arxiv.org/abs/2405.10871v1&entry.124074799=Read"},
{"title": "Off-the-Shelf Neural Network Architectures for Forex Time Series\n  Prediction come at a Cost", "author": "Theodoros Zafeiriou and Dimitris Kalles", "abstract": "  Our study focuses on comparing the performance and resource requirements\nbetween different Long Short-Term Memory (LSTM) neural network architectures\nand an ANN specialized architecture for forex market prediction. We analyze the\nexecution time of the models as well as the resources consumed, such as memory\nand computational power. Our aim is to demonstrate that the specialized\narchitecture not only achieves better results in forex market prediction but\nalso executes using fewer resources and in a shorter time frame compared to\nLSTM architectures. This comparative analysis will provide significant insights\ninto the suitability of these two types of architectures for time series\nprediction in the forex market environment.\n", "link": "http://arxiv.org/abs/2405.10679v1", "date": "2024-05-17", "relevancy": 1.7014, "topK": [{"title": "Boosted dynamic neural networks", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/26302", "similarity": 0.4467}, {"title": "SDXS: Real-Time One-Step Latent Diffusion Models with Image Conditions", "link": "http://arxiv.org/abs/2403.16627v1", "similarity": 0.4145}, {"title": "Self-supervised Photographic Image Layout Representation Learning", "link": "http://arxiv.org/abs/2403.03740v1", "similarity": 0.4083}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Off-the-Shelf%20Neural%20Network%20Architectures%20for%20Forex%20Time%20Series%0A%20%20Prediction%20come%20at%20a%20Cost&body=Title%3A%20Off-the-Shelf%20Neural%20Network%20Architectures%20for%20Forex%20Time%20Series%0A%20%20Prediction%20come%20at%20a%20Cost%0AAuthor%3A%20Theodoros%20Zafeiriou%20and%20Dimitris%20Kalles%0AAbstract%3A%20%20%20Our%20study%20focuses%20on%20comparing%20the%20performance%20and%20resource%20requirements%0Abetween%20different%20Long%20Short-Term%20Memory%20%28LSTM%29%20neural%20network%20architectures%0Aand%20an%20ANN%20specialized%20architecture%20for%20forex%20market%20prediction.%20We%20analyze%20the%0Aexecution%20time%20of%20the%20models%20as%20well%20as%20the%20resources%20consumed%2C%20such%20as%20memory%0Aand%20computational%20power.%20Our%20aim%20is%20to%20demonstrate%20that%20the%20specialized%0Aarchitecture%20not%20only%20achieves%20better%20results%20in%20forex%20market%20prediction%20but%0Aalso%20executes%20using%20fewer%20resources%20and%20in%20a%20shorter%20time%20frame%20compared%20to%0ALSTM%20architectures.%20This%20comparative%20analysis%20will%20provide%20significant%20insights%0Ainto%20the%20suitability%20of%20these%20two%20types%20of%20architectures%20for%20time%20series%0Aprediction%20in%20the%20forex%20market%20environment.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2405.10679v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DOff-the-Shelf%2520Neural%2520Network%2520Architectures%2520for%2520Forex%2520Time%2520Series%250A%2520%2520Prediction%2520come%2520at%2520a%2520Cost%26entry.906535625%3DTheodoros%2520Zafeiriou%2520and%2520Dimitris%2520Kalles%26entry.1292438233%3D%2520%2520Our%2520study%2520focuses%2520on%2520comparing%2520the%2520performance%2520and%2520resource%2520requirements%250Abetween%2520different%2520Long%2520Short-Term%2520Memory%2520%2528LSTM%2529%2520neural%2520network%2520architectures%250Aand%2520an%2520ANN%2520specialized%2520architecture%2520for%2520forex%2520market%2520prediction.%2520We%2520analyze%2520the%250Aexecution%2520time%2520of%2520the%2520models%2520as%2520well%2520as%2520the%2520resources%2520consumed%252C%2520such%2520as%2520memory%250Aand%2520computational%2520power.%2520Our%2520aim%2520is%2520to%2520demonstrate%2520that%2520the%2520specialized%250Aarchitecture%2520not%2520only%2520achieves%2520better%2520results%2520in%2520forex%2520market%2520prediction%2520but%250Aalso%2520executes%2520using%2520fewer%2520resources%2520and%2520in%2520a%2520shorter%2520time%2520frame%2520compared%2520to%250ALSTM%2520architectures.%2520This%2520comparative%2520analysis%2520will%2520provide%2520significant%2520insights%250Ainto%2520the%2520suitability%2520of%2520these%2520two%2520types%2520of%2520architectures%2520for%2520time%2520series%250Aprediction%2520in%2520the%2520forex%2520market%2520environment.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2405.10679v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Off-the-Shelf%20Neural%20Network%20Architectures%20for%20Forex%20Time%20Series%0A%20%20Prediction%20come%20at%20a%20Cost&entry.906535625=Theodoros%20Zafeiriou%20and%20Dimitris%20Kalles&entry.1292438233=%20%20Our%20study%20focuses%20on%20comparing%20the%20performance%20and%20resource%20requirements%0Abetween%20different%20Long%20Short-Term%20Memory%20%28LSTM%29%20neural%20network%20architectures%0Aand%20an%20ANN%20specialized%20architecture%20for%20forex%20market%20prediction.%20We%20analyze%20the%0Aexecution%20time%20of%20the%20models%20as%20well%20as%20the%20resources%20consumed%2C%20such%20as%20memory%0Aand%20computational%20power.%20Our%20aim%20is%20to%20demonstrate%20that%20the%20specialized%0Aarchitecture%20not%20only%20achieves%20better%20results%20in%20forex%20market%20prediction%20but%0Aalso%20executes%20using%20fewer%20resources%20and%20in%20a%20shorter%20time%20frame%20compared%20to%0ALSTM%20architectures.%20This%20comparative%20analysis%20will%20provide%20significant%20insights%0Ainto%20the%20suitability%20of%20these%20two%20types%20of%20architectures%20for%20time%20series%0Aprediction%20in%20the%20forex%20market%20environment.%0A&entry.1838667208=http%3A//arxiv.org/abs/2405.10679v1&entry.124074799=Read"},
{"title": "Variational Mode Decomposition-Based Nonstationary Coherent Structure\n  Analysis for Spatiotemporal Data", "author": "Yuya Ohmichi", "abstract": "  The conventional modal analysis techniques face difficulties in handling\nnonstationary phenomena, such as transient, nonperiodic, or intermittent\nphenomena. This paper presents a variational mode decomposition--based\nnonstationary coherent structure (VMD-NCS) analysis that enables the extraction\nand analysis of coherent structures in the case of nonstationary phenomena from\nhigh-dimensional spatiotemporal data. The VMD-NCS analysis decomposes the input\nspatiotemporal data into intrinsic coherent structures (ICSs) that represent\nnonstationary spatiotemporal patterns and exhibit coherence in both spatial and\ntemporal directions. Unlike many conventional modal analysis techniques, the\nproposed method accounts for the temporal changes in the spatial distribution\nwith time. Tthe VMD-NCS analysis was validated based on the transient growth\nphenomena in the flow around a cylinder. It was confirmed that the temporal\nchanges in the spatial distribution, depicting the transient growth of vortex\nshedding where fluctuations arising in the far-wake region gradually approach\nthe near-wake region, were represented as a single ICS. Furthermore, in the\nanalysis of the quasi-periodic flow field around a pitching airfoil, the\ntemporal changes in the spatial distribution and the amplitude of vortex\nshedding behind the airfoil, influenced by the pitching motion of the airfoil,\nwere captured as a single ICS. The impact of two parameters that control the\nnumber of ICSs ($K$) and the penalty factor related to the temporal coherence\n($\\alpha$), was investigated. The results revealed that $K$ has a significant\nimpact on the VMD-NCS analysis results. In the case of a relatively high $K$,\nthe VMD-NCS analysis tends to extract more periodic spatiotemporal patterns\nresembling the results of dynamic mode decomposition. In the case of a small\n$K$, it tends to extract more nonstationary spatiotemporal patterns.\n", "link": "http://arxiv.org/abs/2312.12113v2", "date": "2024-05-17", "relevancy": 1.7001, "topK": [{"title": "Dynamic Gaussians Mesh: Consistent Mesh Reconstruction from Monocular\n  Videos", "link": "http://arxiv.org/abs/2404.12379v2", "similarity": 0.4353}, {"title": "Multimodal-Conditioned Latent Diffusion Models for Fashion Image Editing", "link": "http://arxiv.org/abs/2403.14828v2", "similarity": 0.4274}, {"title": "Magic-Me: Identity-Specific Video Customized Diffusion", "link": "http://arxiv.org/abs/2402.09368v1", "similarity": 0.4138}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Variational%20Mode%20Decomposition-Based%20Nonstationary%20Coherent%20Structure%0A%20%20Analysis%20for%20Spatiotemporal%20Data&body=Title%3A%20Variational%20Mode%20Decomposition-Based%20Nonstationary%20Coherent%20Structure%0A%20%20Analysis%20for%20Spatiotemporal%20Data%0AAuthor%3A%20Yuya%20Ohmichi%0AAbstract%3A%20%20%20The%20conventional%20modal%20analysis%20techniques%20face%20difficulties%20in%20handling%0Anonstationary%20phenomena%2C%20such%20as%20transient%2C%20nonperiodic%2C%20or%20intermittent%0Aphenomena.%20This%20paper%20presents%20a%20variational%20mode%20decomposition--based%0Anonstationary%20coherent%20structure%20%28VMD-NCS%29%20analysis%20that%20enables%20the%20extraction%0Aand%20analysis%20of%20coherent%20structures%20in%20the%20case%20of%20nonstationary%20phenomena%20from%0Ahigh-dimensional%20spatiotemporal%20data.%20The%20VMD-NCS%20analysis%20decomposes%20the%20input%0Aspatiotemporal%20data%20into%20intrinsic%20coherent%20structures%20%28ICSs%29%20that%20represent%0Anonstationary%20spatiotemporal%20patterns%20and%20exhibit%20coherence%20in%20both%20spatial%20and%0Atemporal%20directions.%20Unlike%20many%20conventional%20modal%20analysis%20techniques%2C%20the%0Aproposed%20method%20accounts%20for%20the%20temporal%20changes%20in%20the%20spatial%20distribution%0Awith%20time.%20Tthe%20VMD-NCS%20analysis%20was%20validated%20based%20on%20the%20transient%20growth%0Aphenomena%20in%20the%20flow%20around%20a%20cylinder.%20It%20was%20confirmed%20that%20the%20temporal%0Achanges%20in%20the%20spatial%20distribution%2C%20depicting%20the%20transient%20growth%20of%20vortex%0Ashedding%20where%20fluctuations%20arising%20in%20the%20far-wake%20region%20gradually%20approach%0Athe%20near-wake%20region%2C%20were%20represented%20as%20a%20single%20ICS.%20Furthermore%2C%20in%20the%0Aanalysis%20of%20the%20quasi-periodic%20flow%20field%20around%20a%20pitching%20airfoil%2C%20the%0Atemporal%20changes%20in%20the%20spatial%20distribution%20and%20the%20amplitude%20of%20vortex%0Ashedding%20behind%20the%20airfoil%2C%20influenced%20by%20the%20pitching%20motion%20of%20the%20airfoil%2C%0Awere%20captured%20as%20a%20single%20ICS.%20The%20impact%20of%20two%20parameters%20that%20control%20the%0Anumber%20of%20ICSs%20%28%24K%24%29%20and%20the%20penalty%20factor%20related%20to%20the%20temporal%20coherence%0A%28%24%5Calpha%24%29%2C%20was%20investigated.%20The%20results%20revealed%20that%20%24K%24%20has%20a%20significant%0Aimpact%20on%20the%20VMD-NCS%20analysis%20results.%20In%20the%20case%20of%20a%20relatively%20high%20%24K%24%2C%0Athe%20VMD-NCS%20analysis%20tends%20to%20extract%20more%20periodic%20spatiotemporal%20patterns%0Aresembling%20the%20results%20of%20dynamic%20mode%20decomposition.%20In%20the%20case%20of%20a%20small%0A%24K%24%2C%20it%20tends%20to%20extract%20more%20nonstationary%20spatiotemporal%20patterns.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2312.12113v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DVariational%2520Mode%2520Decomposition-Based%2520Nonstationary%2520Coherent%2520Structure%250A%2520%2520Analysis%2520for%2520Spatiotemporal%2520Data%26entry.906535625%3DYuya%2520Ohmichi%26entry.1292438233%3D%2520%2520The%2520conventional%2520modal%2520analysis%2520techniques%2520face%2520difficulties%2520in%2520handling%250Anonstationary%2520phenomena%252C%2520such%2520as%2520transient%252C%2520nonperiodic%252C%2520or%2520intermittent%250Aphenomena.%2520This%2520paper%2520presents%2520a%2520variational%2520mode%2520decomposition--based%250Anonstationary%2520coherent%2520structure%2520%2528VMD-NCS%2529%2520analysis%2520that%2520enables%2520the%2520extraction%250Aand%2520analysis%2520of%2520coherent%2520structures%2520in%2520the%2520case%2520of%2520nonstationary%2520phenomena%2520from%250Ahigh-dimensional%2520spatiotemporal%2520data.%2520The%2520VMD-NCS%2520analysis%2520decomposes%2520the%2520input%250Aspatiotemporal%2520data%2520into%2520intrinsic%2520coherent%2520structures%2520%2528ICSs%2529%2520that%2520represent%250Anonstationary%2520spatiotemporal%2520patterns%2520and%2520exhibit%2520coherence%2520in%2520both%2520spatial%2520and%250Atemporal%2520directions.%2520Unlike%2520many%2520conventional%2520modal%2520analysis%2520techniques%252C%2520the%250Aproposed%2520method%2520accounts%2520for%2520the%2520temporal%2520changes%2520in%2520the%2520spatial%2520distribution%250Awith%2520time.%2520Tthe%2520VMD-NCS%2520analysis%2520was%2520validated%2520based%2520on%2520the%2520transient%2520growth%250Aphenomena%2520in%2520the%2520flow%2520around%2520a%2520cylinder.%2520It%2520was%2520confirmed%2520that%2520the%2520temporal%250Achanges%2520in%2520the%2520spatial%2520distribution%252C%2520depicting%2520the%2520transient%2520growth%2520of%2520vortex%250Ashedding%2520where%2520fluctuations%2520arising%2520in%2520the%2520far-wake%2520region%2520gradually%2520approach%250Athe%2520near-wake%2520region%252C%2520were%2520represented%2520as%2520a%2520single%2520ICS.%2520Furthermore%252C%2520in%2520the%250Aanalysis%2520of%2520the%2520quasi-periodic%2520flow%2520field%2520around%2520a%2520pitching%2520airfoil%252C%2520the%250Atemporal%2520changes%2520in%2520the%2520spatial%2520distribution%2520and%2520the%2520amplitude%2520of%2520vortex%250Ashedding%2520behind%2520the%2520airfoil%252C%2520influenced%2520by%2520the%2520pitching%2520motion%2520of%2520the%2520airfoil%252C%250Awere%2520captured%2520as%2520a%2520single%2520ICS.%2520The%2520impact%2520of%2520two%2520parameters%2520that%2520control%2520the%250Anumber%2520of%2520ICSs%2520%2528%2524K%2524%2529%2520and%2520the%2520penalty%2520factor%2520related%2520to%2520the%2520temporal%2520coherence%250A%2528%2524%255Calpha%2524%2529%252C%2520was%2520investigated.%2520The%2520results%2520revealed%2520that%2520%2524K%2524%2520has%2520a%2520significant%250Aimpact%2520on%2520the%2520VMD-NCS%2520analysis%2520results.%2520In%2520the%2520case%2520of%2520a%2520relatively%2520high%2520%2524K%2524%252C%250Athe%2520VMD-NCS%2520analysis%2520tends%2520to%2520extract%2520more%2520periodic%2520spatiotemporal%2520patterns%250Aresembling%2520the%2520results%2520of%2520dynamic%2520mode%2520decomposition.%2520In%2520the%2520case%2520of%2520a%2520small%250A%2524K%2524%252C%2520it%2520tends%2520to%2520extract%2520more%2520nonstationary%2520spatiotemporal%2520patterns.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2312.12113v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Variational%20Mode%20Decomposition-Based%20Nonstationary%20Coherent%20Structure%0A%20%20Analysis%20for%20Spatiotemporal%20Data&entry.906535625=Yuya%20Ohmichi&entry.1292438233=%20%20The%20conventional%20modal%20analysis%20techniques%20face%20difficulties%20in%20handling%0Anonstationary%20phenomena%2C%20such%20as%20transient%2C%20nonperiodic%2C%20or%20intermittent%0Aphenomena.%20This%20paper%20presents%20a%20variational%20mode%20decomposition--based%0Anonstationary%20coherent%20structure%20%28VMD-NCS%29%20analysis%20that%20enables%20the%20extraction%0Aand%20analysis%20of%20coherent%20structures%20in%20the%20case%20of%20nonstationary%20phenomena%20from%0Ahigh-dimensional%20spatiotemporal%20data.%20The%20VMD-NCS%20analysis%20decomposes%20the%20input%0Aspatiotemporal%20data%20into%20intrinsic%20coherent%20structures%20%28ICSs%29%20that%20represent%0Anonstationary%20spatiotemporal%20patterns%20and%20exhibit%20coherence%20in%20both%20spatial%20and%0Atemporal%20directions.%20Unlike%20many%20conventional%20modal%20analysis%20techniques%2C%20the%0Aproposed%20method%20accounts%20for%20the%20temporal%20changes%20in%20the%20spatial%20distribution%0Awith%20time.%20Tthe%20VMD-NCS%20analysis%20was%20validated%20based%20on%20the%20transient%20growth%0Aphenomena%20in%20the%20flow%20around%20a%20cylinder.%20It%20was%20confirmed%20that%20the%20temporal%0Achanges%20in%20the%20spatial%20distribution%2C%20depicting%20the%20transient%20growth%20of%20vortex%0Ashedding%20where%20fluctuations%20arising%20in%20the%20far-wake%20region%20gradually%20approach%0Athe%20near-wake%20region%2C%20were%20represented%20as%20a%20single%20ICS.%20Furthermore%2C%20in%20the%0Aanalysis%20of%20the%20quasi-periodic%20flow%20field%20around%20a%20pitching%20airfoil%2C%20the%0Atemporal%20changes%20in%20the%20spatial%20distribution%20and%20the%20amplitude%20of%20vortex%0Ashedding%20behind%20the%20airfoil%2C%20influenced%20by%20the%20pitching%20motion%20of%20the%20airfoil%2C%0Awere%20captured%20as%20a%20single%20ICS.%20The%20impact%20of%20two%20parameters%20that%20control%20the%0Anumber%20of%20ICSs%20%28%24K%24%29%20and%20the%20penalty%20factor%20related%20to%20the%20temporal%20coherence%0A%28%24%5Calpha%24%29%2C%20was%20investigated.%20The%20results%20revealed%20that%20%24K%24%20has%20a%20significant%0Aimpact%20on%20the%20VMD-NCS%20analysis%20results.%20In%20the%20case%20of%20a%20relatively%20high%20%24K%24%2C%0Athe%20VMD-NCS%20analysis%20tends%20to%20extract%20more%20periodic%20spatiotemporal%20patterns%0Aresembling%20the%20results%20of%20dynamic%20mode%20decomposition.%20In%20the%20case%20of%20a%20small%0A%24K%24%2C%20it%20tends%20to%20extract%20more%20nonstationary%20spatiotemporal%20patterns.%0A&entry.1838667208=http%3A//arxiv.org/abs/2312.12113v2&entry.124074799=Read"},
{"title": "Picking watermarks from noise (PWFN): an improved robust watermarking\n  model against intensive distortions", "author": "Sijing Xie and Chengxin Zhao and Nan Sun and Wei Li and Hefei Ling", "abstract": "  Digital watermarking is the process of embedding secret information by\naltering images in an undetectable way to the human eye. To increase the\nrobustness of the model, many deep learning-based watermarking methods use the\nencoder-noise-decoder architecture by adding different noises to the noise\nlayer. The decoder then extracts the watermarked information from the distorted\nimage. However, this method can only resist weak noise attacks. To improve the\nrobustness of the decoder against stronger noise, this paper proposes to\nintroduce a denoise module between the noise layer and the decoder. The module\naims to reduce noise and recover some of the information lost caused by\ndistortion. Additionally, the paper introduces the SE module to fuse the\nwatermarking information pixel-wise and channel dimensions-wise, improving the\nencoder's efficiency. Experimental results show that our proposed method is\ncomparable to existing models and outperforms state-of-the-art under different\nnoise intensities. In addition, ablation experiments show the superiority of\nour proposed module.\n", "link": "http://arxiv.org/abs/2405.05170v2", "date": "2024-05-17", "relevancy": 1.6873, "topK": [{"title": "Tuning-Free Noise Rectification for High Fidelity Image-to-Video\n  Generation", "link": "http://arxiv.org/abs/2403.02827v1", "similarity": 0.5957}, {"title": "Scaling Rectified Flow Transformers for High-Resolution Image Synthesis", "link": "http://arxiv.org/abs/2403.03206v1", "similarity": 0.5525}, {"title": "SDXS: Real-Time One-Step Latent Diffusion Models with Image Conditions", "link": "http://arxiv.org/abs/2403.16627v1", "similarity": 0.4892}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Picking%20watermarks%20from%20noise%20%28PWFN%29%3A%20an%20improved%20robust%20watermarking%0A%20%20model%20against%20intensive%20distortions&body=Title%3A%20Picking%20watermarks%20from%20noise%20%28PWFN%29%3A%20an%20improved%20robust%20watermarking%0A%20%20model%20against%20intensive%20distortions%0AAuthor%3A%20Sijing%20Xie%20and%20Chengxin%20Zhao%20and%20Nan%20Sun%20and%20Wei%20Li%20and%20Hefei%20Ling%0AAbstract%3A%20%20%20Digital%20watermarking%20is%20the%20process%20of%20embedding%20secret%20information%20by%0Aaltering%20images%20in%20an%20undetectable%20way%20to%20the%20human%20eye.%20To%20increase%20the%0Arobustness%20of%20the%20model%2C%20many%20deep%20learning-based%20watermarking%20methods%20use%20the%0Aencoder-noise-decoder%20architecture%20by%20adding%20different%20noises%20to%20the%20noise%0Alayer.%20The%20decoder%20then%20extracts%20the%20watermarked%20information%20from%20the%20distorted%0Aimage.%20However%2C%20this%20method%20can%20only%20resist%20weak%20noise%20attacks.%20To%20improve%20the%0Arobustness%20of%20the%20decoder%20against%20stronger%20noise%2C%20this%20paper%20proposes%20to%0Aintroduce%20a%20denoise%20module%20between%20the%20noise%20layer%20and%20the%20decoder.%20The%20module%0Aaims%20to%20reduce%20noise%20and%20recover%20some%20of%20the%20information%20lost%20caused%20by%0Adistortion.%20Additionally%2C%20the%20paper%20introduces%20the%20SE%20module%20to%20fuse%20the%0Awatermarking%20information%20pixel-wise%20and%20channel%20dimensions-wise%2C%20improving%20the%0Aencoder%27s%20efficiency.%20Experimental%20results%20show%20that%20our%20proposed%20method%20is%0Acomparable%20to%20existing%20models%20and%20outperforms%20state-of-the-art%20under%20different%0Anoise%20intensities.%20In%20addition%2C%20ablation%20experiments%20show%20the%20superiority%20of%0Aour%20proposed%20module.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2405.05170v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DPicking%2520watermarks%2520from%2520noise%2520%2528PWFN%2529%253A%2520an%2520improved%2520robust%2520watermarking%250A%2520%2520model%2520against%2520intensive%2520distortions%26entry.906535625%3DSijing%2520Xie%2520and%2520Chengxin%2520Zhao%2520and%2520Nan%2520Sun%2520and%2520Wei%2520Li%2520and%2520Hefei%2520Ling%26entry.1292438233%3D%2520%2520Digital%2520watermarking%2520is%2520the%2520process%2520of%2520embedding%2520secret%2520information%2520by%250Aaltering%2520images%2520in%2520an%2520undetectable%2520way%2520to%2520the%2520human%2520eye.%2520To%2520increase%2520the%250Arobustness%2520of%2520the%2520model%252C%2520many%2520deep%2520learning-based%2520watermarking%2520methods%2520use%2520the%250Aencoder-noise-decoder%2520architecture%2520by%2520adding%2520different%2520noises%2520to%2520the%2520noise%250Alayer.%2520The%2520decoder%2520then%2520extracts%2520the%2520watermarked%2520information%2520from%2520the%2520distorted%250Aimage.%2520However%252C%2520this%2520method%2520can%2520only%2520resist%2520weak%2520noise%2520attacks.%2520To%2520improve%2520the%250Arobustness%2520of%2520the%2520decoder%2520against%2520stronger%2520noise%252C%2520this%2520paper%2520proposes%2520to%250Aintroduce%2520a%2520denoise%2520module%2520between%2520the%2520noise%2520layer%2520and%2520the%2520decoder.%2520The%2520module%250Aaims%2520to%2520reduce%2520noise%2520and%2520recover%2520some%2520of%2520the%2520information%2520lost%2520caused%2520by%250Adistortion.%2520Additionally%252C%2520the%2520paper%2520introduces%2520the%2520SE%2520module%2520to%2520fuse%2520the%250Awatermarking%2520information%2520pixel-wise%2520and%2520channel%2520dimensions-wise%252C%2520improving%2520the%250Aencoder%2527s%2520efficiency.%2520Experimental%2520results%2520show%2520that%2520our%2520proposed%2520method%2520is%250Acomparable%2520to%2520existing%2520models%2520and%2520outperforms%2520state-of-the-art%2520under%2520different%250Anoise%2520intensities.%2520In%2520addition%252C%2520ablation%2520experiments%2520show%2520the%2520superiority%2520of%250Aour%2520proposed%2520module.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2405.05170v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Picking%20watermarks%20from%20noise%20%28PWFN%29%3A%20an%20improved%20robust%20watermarking%0A%20%20model%20against%20intensive%20distortions&entry.906535625=Sijing%20Xie%20and%20Chengxin%20Zhao%20and%20Nan%20Sun%20and%20Wei%20Li%20and%20Hefei%20Ling&entry.1292438233=%20%20Digital%20watermarking%20is%20the%20process%20of%20embedding%20secret%20information%20by%0Aaltering%20images%20in%20an%20undetectable%20way%20to%20the%20human%20eye.%20To%20increase%20the%0Arobustness%20of%20the%20model%2C%20many%20deep%20learning-based%20watermarking%20methods%20use%20the%0Aencoder-noise-decoder%20architecture%20by%20adding%20different%20noises%20to%20the%20noise%0Alayer.%20The%20decoder%20then%20extracts%20the%20watermarked%20information%20from%20the%20distorted%0Aimage.%20However%2C%20this%20method%20can%20only%20resist%20weak%20noise%20attacks.%20To%20improve%20the%0Arobustness%20of%20the%20decoder%20against%20stronger%20noise%2C%20this%20paper%20proposes%20to%0Aintroduce%20a%20denoise%20module%20between%20the%20noise%20layer%20and%20the%20decoder.%20The%20module%0Aaims%20to%20reduce%20noise%20and%20recover%20some%20of%20the%20information%20lost%20caused%20by%0Adistortion.%20Additionally%2C%20the%20paper%20introduces%20the%20SE%20module%20to%20fuse%20the%0Awatermarking%20information%20pixel-wise%20and%20channel%20dimensions-wise%2C%20improving%20the%0Aencoder%27s%20efficiency.%20Experimental%20results%20show%20that%20our%20proposed%20method%20is%0Acomparable%20to%20existing%20models%20and%20outperforms%20state-of-the-art%20under%20different%0Anoise%20intensities.%20In%20addition%2C%20ablation%20experiments%20show%20the%20superiority%20of%0Aour%20proposed%20module.%0A&entry.1838667208=http%3A//arxiv.org/abs/2405.05170v2&entry.124074799=Read"},
{"title": "AdaptiX -- A Transitional XR Framework for Development and Evaluation of\n  Shared Control Applications in Assistive Robotics", "author": "Max Pascher and Felix Ferdinand Goldau and Kirill Kronhardt and Udo Frese and Jens Gerken", "abstract": "  With the ongoing efforts to empower people with mobility impairments and the\nincrease in technological acceptance by the general public, assistive\ntechnologies, such as collaborative robotic arms, are gaining popularity. Yet,\ntheir widespread success is limited by usability issues, specifically the\ndisparity between user input and software control along the autonomy continuum.\nTo address this, shared control concepts provide opportunities to combine the\ntargeted increase of user autonomy with a certain level of computer assistance.\nThis paper presents the free and open-source AdaptiX XR framework for\ndeveloping and evaluating shared control applications in a high-resolution\nsimulation environment. The initial framework consists of a simulated robotic\narm with an example scenario in Virtual Reality (VR), multiple standard control\ninterfaces, and a specialized recording/replay system. AdaptiX can easily be\nextended for specific research needs, allowing Human-Robot Interaction (HRI)\nresearchers to rapidly design and test novel interaction methods, intervention\nstrategies, and multi-modal feedback techniques, without requiring an actual\nphysical robotic arm during the early phases of ideation, prototyping, and\nevaluation. Also, a Robot Operating System (ROS) integration enables the\ncontrolling of a real robotic arm in a PhysicalTwin approach without any\nsimulation-reality gap. Here, we review the capabilities and limitations of\nAdaptiX in detail and present three bodies of research based on the framework.\nAdaptiX can be accessed at https://adaptix.robot-research.de.\n", "link": "http://arxiv.org/abs/2310.15887v3", "date": "2024-05-17", "relevancy": 1.6863, "topK": [{"title": "RoboEXP: Action-Conditioned Scene Graph via Interactive Exploration for\n  Robotic Manipulation", "link": "http://arxiv.org/abs/2402.15487v1", "similarity": 0.5867}, {"title": "Egocentric Computer Vision for Hands-Free Robotic Wheelchair Navigation", "link": "https://link.springer.com/article/10.1007/s10846-023-01807-4", "similarity": 0.5741}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.5074}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20AdaptiX%20--%20A%20Transitional%20XR%20Framework%20for%20Development%20and%20Evaluation%20of%0A%20%20Shared%20Control%20Applications%20in%20Assistive%20Robotics&body=Title%3A%20AdaptiX%20--%20A%20Transitional%20XR%20Framework%20for%20Development%20and%20Evaluation%20of%0A%20%20Shared%20Control%20Applications%20in%20Assistive%20Robotics%0AAuthor%3A%20Max%20Pascher%20and%20Felix%20Ferdinand%20Goldau%20and%20Kirill%20Kronhardt%20and%20Udo%20Frese%20and%20Jens%20Gerken%0AAbstract%3A%20%20%20With%20the%20ongoing%20efforts%20to%20empower%20people%20with%20mobility%20impairments%20and%20the%0Aincrease%20in%20technological%20acceptance%20by%20the%20general%20public%2C%20assistive%0Atechnologies%2C%20such%20as%20collaborative%20robotic%20arms%2C%20are%20gaining%20popularity.%20Yet%2C%0Atheir%20widespread%20success%20is%20limited%20by%20usability%20issues%2C%20specifically%20the%0Adisparity%20between%20user%20input%20and%20software%20control%20along%20the%20autonomy%20continuum.%0ATo%20address%20this%2C%20shared%20control%20concepts%20provide%20opportunities%20to%20combine%20the%0Atargeted%20increase%20of%20user%20autonomy%20with%20a%20certain%20level%20of%20computer%20assistance.%0AThis%20paper%20presents%20the%20free%20and%20open-source%20AdaptiX%20XR%20framework%20for%0Adeveloping%20and%20evaluating%20shared%20control%20applications%20in%20a%20high-resolution%0Asimulation%20environment.%20The%20initial%20framework%20consists%20of%20a%20simulated%20robotic%0Aarm%20with%20an%20example%20scenario%20in%20Virtual%20Reality%20%28VR%29%2C%20multiple%20standard%20control%0Ainterfaces%2C%20and%20a%20specialized%20recording/replay%20system.%20AdaptiX%20can%20easily%20be%0Aextended%20for%20specific%20research%20needs%2C%20allowing%20Human-Robot%20Interaction%20%28HRI%29%0Aresearchers%20to%20rapidly%20design%20and%20test%20novel%20interaction%20methods%2C%20intervention%0Astrategies%2C%20and%20multi-modal%20feedback%20techniques%2C%20without%20requiring%20an%20actual%0Aphysical%20robotic%20arm%20during%20the%20early%20phases%20of%20ideation%2C%20prototyping%2C%20and%0Aevaluation.%20Also%2C%20a%20Robot%20Operating%20System%20%28ROS%29%20integration%20enables%20the%0Acontrolling%20of%20a%20real%20robotic%20arm%20in%20a%20PhysicalTwin%20approach%20without%20any%0Asimulation-reality%20gap.%20Here%2C%20we%20review%20the%20capabilities%20and%20limitations%20of%0AAdaptiX%20in%20detail%20and%20present%20three%20bodies%20of%20research%20based%20on%20the%20framework.%0AAdaptiX%20can%20be%20accessed%20at%20https%3A//adaptix.robot-research.de.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2310.15887v3%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DAdaptiX%2520--%2520A%2520Transitional%2520XR%2520Framework%2520for%2520Development%2520and%2520Evaluation%2520of%250A%2520%2520Shared%2520Control%2520Applications%2520in%2520Assistive%2520Robotics%26entry.906535625%3DMax%2520Pascher%2520and%2520Felix%2520Ferdinand%2520Goldau%2520and%2520Kirill%2520Kronhardt%2520and%2520Udo%2520Frese%2520and%2520Jens%2520Gerken%26entry.1292438233%3D%2520%2520With%2520the%2520ongoing%2520efforts%2520to%2520empower%2520people%2520with%2520mobility%2520impairments%2520and%2520the%250Aincrease%2520in%2520technological%2520acceptance%2520by%2520the%2520general%2520public%252C%2520assistive%250Atechnologies%252C%2520such%2520as%2520collaborative%2520robotic%2520arms%252C%2520are%2520gaining%2520popularity.%2520Yet%252C%250Atheir%2520widespread%2520success%2520is%2520limited%2520by%2520usability%2520issues%252C%2520specifically%2520the%250Adisparity%2520between%2520user%2520input%2520and%2520software%2520control%2520along%2520the%2520autonomy%2520continuum.%250ATo%2520address%2520this%252C%2520shared%2520control%2520concepts%2520provide%2520opportunities%2520to%2520combine%2520the%250Atargeted%2520increase%2520of%2520user%2520autonomy%2520with%2520a%2520certain%2520level%2520of%2520computer%2520assistance.%250AThis%2520paper%2520presents%2520the%2520free%2520and%2520open-source%2520AdaptiX%2520XR%2520framework%2520for%250Adeveloping%2520and%2520evaluating%2520shared%2520control%2520applications%2520in%2520a%2520high-resolution%250Asimulation%2520environment.%2520The%2520initial%2520framework%2520consists%2520of%2520a%2520simulated%2520robotic%250Aarm%2520with%2520an%2520example%2520scenario%2520in%2520Virtual%2520Reality%2520%2528VR%2529%252C%2520multiple%2520standard%2520control%250Ainterfaces%252C%2520and%2520a%2520specialized%2520recording/replay%2520system.%2520AdaptiX%2520can%2520easily%2520be%250Aextended%2520for%2520specific%2520research%2520needs%252C%2520allowing%2520Human-Robot%2520Interaction%2520%2528HRI%2529%250Aresearchers%2520to%2520rapidly%2520design%2520and%2520test%2520novel%2520interaction%2520methods%252C%2520intervention%250Astrategies%252C%2520and%2520multi-modal%2520feedback%2520techniques%252C%2520without%2520requiring%2520an%2520actual%250Aphysical%2520robotic%2520arm%2520during%2520the%2520early%2520phases%2520of%2520ideation%252C%2520prototyping%252C%2520and%250Aevaluation.%2520Also%252C%2520a%2520Robot%2520Operating%2520System%2520%2528ROS%2529%2520integration%2520enables%2520the%250Acontrolling%2520of%2520a%2520real%2520robotic%2520arm%2520in%2520a%2520PhysicalTwin%2520approach%2520without%2520any%250Asimulation-reality%2520gap.%2520Here%252C%2520we%2520review%2520the%2520capabilities%2520and%2520limitations%2520of%250AAdaptiX%2520in%2520detail%2520and%2520present%2520three%2520bodies%2520of%2520research%2520based%2520on%2520the%2520framework.%250AAdaptiX%2520can%2520be%2520accessed%2520at%2520https%253A//adaptix.robot-research.de.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2310.15887v3%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=AdaptiX%20--%20A%20Transitional%20XR%20Framework%20for%20Development%20and%20Evaluation%20of%0A%20%20Shared%20Control%20Applications%20in%20Assistive%20Robotics&entry.906535625=Max%20Pascher%20and%20Felix%20Ferdinand%20Goldau%20and%20Kirill%20Kronhardt%20and%20Udo%20Frese%20and%20Jens%20Gerken&entry.1292438233=%20%20With%20the%20ongoing%20efforts%20to%20empower%20people%20with%20mobility%20impairments%20and%20the%0Aincrease%20in%20technological%20acceptance%20by%20the%20general%20public%2C%20assistive%0Atechnologies%2C%20such%20as%20collaborative%20robotic%20arms%2C%20are%20gaining%20popularity.%20Yet%2C%0Atheir%20widespread%20success%20is%20limited%20by%20usability%20issues%2C%20specifically%20the%0Adisparity%20between%20user%20input%20and%20software%20control%20along%20the%20autonomy%20continuum.%0ATo%20address%20this%2C%20shared%20control%20concepts%20provide%20opportunities%20to%20combine%20the%0Atargeted%20increase%20of%20user%20autonomy%20with%20a%20certain%20level%20of%20computer%20assistance.%0AThis%20paper%20presents%20the%20free%20and%20open-source%20AdaptiX%20XR%20framework%20for%0Adeveloping%20and%20evaluating%20shared%20control%20applications%20in%20a%20high-resolution%0Asimulation%20environment.%20The%20initial%20framework%20consists%20of%20a%20simulated%20robotic%0Aarm%20with%20an%20example%20scenario%20in%20Virtual%20Reality%20%28VR%29%2C%20multiple%20standard%20control%0Ainterfaces%2C%20and%20a%20specialized%20recording/replay%20system.%20AdaptiX%20can%20easily%20be%0Aextended%20for%20specific%20research%20needs%2C%20allowing%20Human-Robot%20Interaction%20%28HRI%29%0Aresearchers%20to%20rapidly%20design%20and%20test%20novel%20interaction%20methods%2C%20intervention%0Astrategies%2C%20and%20multi-modal%20feedback%20techniques%2C%20without%20requiring%20an%20actual%0Aphysical%20robotic%20arm%20during%20the%20early%20phases%20of%20ideation%2C%20prototyping%2C%20and%0Aevaluation.%20Also%2C%20a%20Robot%20Operating%20System%20%28ROS%29%20integration%20enables%20the%0Acontrolling%20of%20a%20real%20robotic%20arm%20in%20a%20PhysicalTwin%20approach%20without%20any%0Asimulation-reality%20gap.%20Here%2C%20we%20review%20the%20capabilities%20and%20limitations%20of%0AAdaptiX%20in%20detail%20and%20present%20three%20bodies%20of%20research%20based%20on%20the%20framework.%0AAdaptiX%20can%20be%20accessed%20at%20https%3A//adaptix.robot-research.de.%0A&entry.1838667208=http%3A//arxiv.org/abs/2310.15887v3&entry.124074799=Read"},
{"title": "Safe Control using Occupancy Grid Map-based Control Barrier Function\n  (OGM-CBF)", "author": "Golnaz Raja and Teemu M\u00f6kk\u00f6nen and Reza Ghabcheloo", "abstract": "  Safe navigation in unknown environments stands as a significant challenge in\nthe field of robotics. Control Barrier Function (CBF) is a strong mathematical\ntool to guarantee safety requirements. However, a common assumption in many\nworks is that the CBF is already known and obstacles have predefined shapes. In\nthis letter, we present a novel method called Occupancy Grid Map-based Control\nBarrier Function (OGM-CBF), which defines Control Barrier Function based on\nOccupancy Grid Maps. This enables generalization to unknown environments while\ngenerating online local or global maps of the environment using onboard\nperception sensors such as LiDAR or camera. With this method, the system\nguarantees safety via a single, continuously differentiable CBF per time step,\nwhich can be represented as one constraint in the CBF-QP optimization\nformulation while having an arbitrary number of obstacles with unknown shapes\nin the environment. This enables practical real-time implementation of CBF in\nboth unknown and known environments. The efficacy of OGM-CBF is demonstrated in\nthe safe control of an autonomous car in the CARLA simulator and a real-world\nindustrial mobile robot.\n", "link": "http://arxiv.org/abs/2405.10703v1", "date": "2024-05-17", "relevancy": 1.672, "topK": [{"title": "Egocentric Computer Vision for Hands-Free Robotic Wheelchair Navigation", "link": "https://link.springer.com/article/10.1007/s10846-023-01807-4", "similarity": 0.561}, {"title": "RoboEXP: Action-Conditioned Scene Graph via Interactive Exploration for\n  Robotic Manipulation", "link": "http://arxiv.org/abs/2402.15487v1", "similarity": 0.5538}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.5517}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Safe%20Control%20using%20Occupancy%20Grid%20Map-based%20Control%20Barrier%20Function%0A%20%20%28OGM-CBF%29&body=Title%3A%20Safe%20Control%20using%20Occupancy%20Grid%20Map-based%20Control%20Barrier%20Function%0A%20%20%28OGM-CBF%29%0AAuthor%3A%20Golnaz%20Raja%20and%20Teemu%20M%C3%B6kk%C3%B6nen%20and%20Reza%20Ghabcheloo%0AAbstract%3A%20%20%20Safe%20navigation%20in%20unknown%20environments%20stands%20as%20a%20significant%20challenge%20in%0Athe%20field%20of%20robotics.%20Control%20Barrier%20Function%20%28CBF%29%20is%20a%20strong%20mathematical%0Atool%20to%20guarantee%20safety%20requirements.%20However%2C%20a%20common%20assumption%20in%20many%0Aworks%20is%20that%20the%20CBF%20is%20already%20known%20and%20obstacles%20have%20predefined%20shapes.%20In%0Athis%20letter%2C%20we%20present%20a%20novel%20method%20called%20Occupancy%20Grid%20Map-based%20Control%0ABarrier%20Function%20%28OGM-CBF%29%2C%20which%20defines%20Control%20Barrier%20Function%20based%20on%0AOccupancy%20Grid%20Maps.%20This%20enables%20generalization%20to%20unknown%20environments%20while%0Agenerating%20online%20local%20or%20global%20maps%20of%20the%20environment%20using%20onboard%0Aperception%20sensors%20such%20as%20LiDAR%20or%20camera.%20With%20this%20method%2C%20the%20system%0Aguarantees%20safety%20via%20a%20single%2C%20continuously%20differentiable%20CBF%20per%20time%20step%2C%0Awhich%20can%20be%20represented%20as%20one%20constraint%20in%20the%20CBF-QP%20optimization%0Aformulation%20while%20having%20an%20arbitrary%20number%20of%20obstacles%20with%20unknown%20shapes%0Ain%20the%20environment.%20This%20enables%20practical%20real-time%20implementation%20of%20CBF%20in%0Aboth%20unknown%20and%20known%20environments.%20The%20efficacy%20of%20OGM-CBF%20is%20demonstrated%20in%0Athe%20safe%20control%20of%20an%20autonomous%20car%20in%20the%20CARLA%20simulator%20and%20a%20real-world%0Aindustrial%20mobile%20robot.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2405.10703v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DSafe%2520Control%2520using%2520Occupancy%2520Grid%2520Map-based%2520Control%2520Barrier%2520Function%250A%2520%2520%2528OGM-CBF%2529%26entry.906535625%3DGolnaz%2520Raja%2520and%2520Teemu%2520M%25C3%25B6kk%25C3%25B6nen%2520and%2520Reza%2520Ghabcheloo%26entry.1292438233%3D%2520%2520Safe%2520navigation%2520in%2520unknown%2520environments%2520stands%2520as%2520a%2520significant%2520challenge%2520in%250Athe%2520field%2520of%2520robotics.%2520Control%2520Barrier%2520Function%2520%2528CBF%2529%2520is%2520a%2520strong%2520mathematical%250Atool%2520to%2520guarantee%2520safety%2520requirements.%2520However%252C%2520a%2520common%2520assumption%2520in%2520many%250Aworks%2520is%2520that%2520the%2520CBF%2520is%2520already%2520known%2520and%2520obstacles%2520have%2520predefined%2520shapes.%2520In%250Athis%2520letter%252C%2520we%2520present%2520a%2520novel%2520method%2520called%2520Occupancy%2520Grid%2520Map-based%2520Control%250ABarrier%2520Function%2520%2528OGM-CBF%2529%252C%2520which%2520defines%2520Control%2520Barrier%2520Function%2520based%2520on%250AOccupancy%2520Grid%2520Maps.%2520This%2520enables%2520generalization%2520to%2520unknown%2520environments%2520while%250Agenerating%2520online%2520local%2520or%2520global%2520maps%2520of%2520the%2520environment%2520using%2520onboard%250Aperception%2520sensors%2520such%2520as%2520LiDAR%2520or%2520camera.%2520With%2520this%2520method%252C%2520the%2520system%250Aguarantees%2520safety%2520via%2520a%2520single%252C%2520continuously%2520differentiable%2520CBF%2520per%2520time%2520step%252C%250Awhich%2520can%2520be%2520represented%2520as%2520one%2520constraint%2520in%2520the%2520CBF-QP%2520optimization%250Aformulation%2520while%2520having%2520an%2520arbitrary%2520number%2520of%2520obstacles%2520with%2520unknown%2520shapes%250Ain%2520the%2520environment.%2520This%2520enables%2520practical%2520real-time%2520implementation%2520of%2520CBF%2520in%250Aboth%2520unknown%2520and%2520known%2520environments.%2520The%2520efficacy%2520of%2520OGM-CBF%2520is%2520demonstrated%2520in%250Athe%2520safe%2520control%2520of%2520an%2520autonomous%2520car%2520in%2520the%2520CARLA%2520simulator%2520and%2520a%2520real-world%250Aindustrial%2520mobile%2520robot.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2405.10703v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Safe%20Control%20using%20Occupancy%20Grid%20Map-based%20Control%20Barrier%20Function%0A%20%20%28OGM-CBF%29&entry.906535625=Golnaz%20Raja%20and%20Teemu%20M%C3%B6kk%C3%B6nen%20and%20Reza%20Ghabcheloo&entry.1292438233=%20%20Safe%20navigation%20in%20unknown%20environments%20stands%20as%20a%20significant%20challenge%20in%0Athe%20field%20of%20robotics.%20Control%20Barrier%20Function%20%28CBF%29%20is%20a%20strong%20mathematical%0Atool%20to%20guarantee%20safety%20requirements.%20However%2C%20a%20common%20assumption%20in%20many%0Aworks%20is%20that%20the%20CBF%20is%20already%20known%20and%20obstacles%20have%20predefined%20shapes.%20In%0Athis%20letter%2C%20we%20present%20a%20novel%20method%20called%20Occupancy%20Grid%20Map-based%20Control%0ABarrier%20Function%20%28OGM-CBF%29%2C%20which%20defines%20Control%20Barrier%20Function%20based%20on%0AOccupancy%20Grid%20Maps.%20This%20enables%20generalization%20to%20unknown%20environments%20while%0Agenerating%20online%20local%20or%20global%20maps%20of%20the%20environment%20using%20onboard%0Aperception%20sensors%20such%20as%20LiDAR%20or%20camera.%20With%20this%20method%2C%20the%20system%0Aguarantees%20safety%20via%20a%20single%2C%20continuously%20differentiable%20CBF%20per%20time%20step%2C%0Awhich%20can%20be%20represented%20as%20one%20constraint%20in%20the%20CBF-QP%20optimization%0Aformulation%20while%20having%20an%20arbitrary%20number%20of%20obstacles%20with%20unknown%20shapes%0Ain%20the%20environment.%20This%20enables%20practical%20real-time%20implementation%20of%20CBF%20in%0Aboth%20unknown%20and%20known%20environments.%20The%20efficacy%20of%20OGM-CBF%20is%20demonstrated%20in%0Athe%20safe%20control%20of%20an%20autonomous%20car%20in%20the%20CARLA%20simulator%20and%20a%20real-world%0Aindustrial%20mobile%20robot.%0A&entry.1838667208=http%3A//arxiv.org/abs/2405.10703v1&entry.124074799=Read"},
{"title": "Combining Teacher-Student with Representation Learning: A Concurrent\n  Teacher-Student Reinforcement Learning Paradigm for Legged Locomotion", "author": "Hongxi Wang and Haoxiang Luo and Wei Zhang and Hua Chen", "abstract": "  Thanks to the explosive developments of data-driven learning methodologies\nrecently, reinforcement learning (RL) emerges as a promising solution to\naddress the legged locomotion problem in robotics. In this manuscript, we\npropose a novel concurrent teacher-student reinforcement learning architecture\nfor legged locomotion over challenging terrains, based only on proprioceptive\nmeasurements in real-world deployment. Different from convectional\nteacher-student architecture that trains the teacher policy via RL and\ntransfers the knowledge to the student policy through supervised learning, our\nproposed architecture trains teacher and student policy networks concurrently\nunder the reinforcement learning paradigm. To achieve this, we develop a new\ntraining scheme based on conventional proximal policy gradient (PPO) method to\naccommodate the interaction between teacher policy network and student policy\nnetwork. The effectiveness of the proposed architecture as well as the new\ntraining scheme is demonstrated through extensive indoor and outdoor\nexperiments on quadrupedal robots and point-foot bipedal robot, showcasing\nrobust locomotion over challenging terrains and improved performance compared\nto two-stage training methods.\n", "link": "http://arxiv.org/abs/2405.10830v1", "date": "2024-05-17", "relevancy": 1.6505, "topK": [{"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.5976}, {"title": "Safe and efficient training of a control agent", "link": "https://patents.google.com/patent/US11709462B2/en", "similarity": 0.5412}, {"title": "RoboEXP: Action-Conditioned Scene Graph via Interactive Exploration for\n  Robotic Manipulation", "link": "http://arxiv.org/abs/2402.15487v1", "similarity": 0.525}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Combining%20Teacher-Student%20with%20Representation%20Learning%3A%20A%20Concurrent%0A%20%20Teacher-Student%20Reinforcement%20Learning%20Paradigm%20for%20Legged%20Locomotion&body=Title%3A%20Combining%20Teacher-Student%20with%20Representation%20Learning%3A%20A%20Concurrent%0A%20%20Teacher-Student%20Reinforcement%20Learning%20Paradigm%20for%20Legged%20Locomotion%0AAuthor%3A%20Hongxi%20Wang%20and%20Haoxiang%20Luo%20and%20Wei%20Zhang%20and%20Hua%20Chen%0AAbstract%3A%20%20%20Thanks%20to%20the%20explosive%20developments%20of%20data-driven%20learning%20methodologies%0Arecently%2C%20reinforcement%20learning%20%28RL%29%20emerges%20as%20a%20promising%20solution%20to%0Aaddress%20the%20legged%20locomotion%20problem%20in%20robotics.%20In%20this%20manuscript%2C%20we%0Apropose%20a%20novel%20concurrent%20teacher-student%20reinforcement%20learning%20architecture%0Afor%20legged%20locomotion%20over%20challenging%20terrains%2C%20based%20only%20on%20proprioceptive%0Ameasurements%20in%20real-world%20deployment.%20Different%20from%20convectional%0Ateacher-student%20architecture%20that%20trains%20the%20teacher%20policy%20via%20RL%20and%0Atransfers%20the%20knowledge%20to%20the%20student%20policy%20through%20supervised%20learning%2C%20our%0Aproposed%20architecture%20trains%20teacher%20and%20student%20policy%20networks%20concurrently%0Aunder%20the%20reinforcement%20learning%20paradigm.%20To%20achieve%20this%2C%20we%20develop%20a%20new%0Atraining%20scheme%20based%20on%20conventional%20proximal%20policy%20gradient%20%28PPO%29%20method%20to%0Aaccommodate%20the%20interaction%20between%20teacher%20policy%20network%20and%20student%20policy%0Anetwork.%20The%20effectiveness%20of%20the%20proposed%20architecture%20as%20well%20as%20the%20new%0Atraining%20scheme%20is%20demonstrated%20through%20extensive%20indoor%20and%20outdoor%0Aexperiments%20on%20quadrupedal%20robots%20and%20point-foot%20bipedal%20robot%2C%20showcasing%0Arobust%20locomotion%20over%20challenging%20terrains%20and%20improved%20performance%20compared%0Ato%20two-stage%20training%20methods.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2405.10830v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DCombining%2520Teacher-Student%2520with%2520Representation%2520Learning%253A%2520A%2520Concurrent%250A%2520%2520Teacher-Student%2520Reinforcement%2520Learning%2520Paradigm%2520for%2520Legged%2520Locomotion%26entry.906535625%3DHongxi%2520Wang%2520and%2520Haoxiang%2520Luo%2520and%2520Wei%2520Zhang%2520and%2520Hua%2520Chen%26entry.1292438233%3D%2520%2520Thanks%2520to%2520the%2520explosive%2520developments%2520of%2520data-driven%2520learning%2520methodologies%250Arecently%252C%2520reinforcement%2520learning%2520%2528RL%2529%2520emerges%2520as%2520a%2520promising%2520solution%2520to%250Aaddress%2520the%2520legged%2520locomotion%2520problem%2520in%2520robotics.%2520In%2520this%2520manuscript%252C%2520we%250Apropose%2520a%2520novel%2520concurrent%2520teacher-student%2520reinforcement%2520learning%2520architecture%250Afor%2520legged%2520locomotion%2520over%2520challenging%2520terrains%252C%2520based%2520only%2520on%2520proprioceptive%250Ameasurements%2520in%2520real-world%2520deployment.%2520Different%2520from%2520convectional%250Ateacher-student%2520architecture%2520that%2520trains%2520the%2520teacher%2520policy%2520via%2520RL%2520and%250Atransfers%2520the%2520knowledge%2520to%2520the%2520student%2520policy%2520through%2520supervised%2520learning%252C%2520our%250Aproposed%2520architecture%2520trains%2520teacher%2520and%2520student%2520policy%2520networks%2520concurrently%250Aunder%2520the%2520reinforcement%2520learning%2520paradigm.%2520To%2520achieve%2520this%252C%2520we%2520develop%2520a%2520new%250Atraining%2520scheme%2520based%2520on%2520conventional%2520proximal%2520policy%2520gradient%2520%2528PPO%2529%2520method%2520to%250Aaccommodate%2520the%2520interaction%2520between%2520teacher%2520policy%2520network%2520and%2520student%2520policy%250Anetwork.%2520The%2520effectiveness%2520of%2520the%2520proposed%2520architecture%2520as%2520well%2520as%2520the%2520new%250Atraining%2520scheme%2520is%2520demonstrated%2520through%2520extensive%2520indoor%2520and%2520outdoor%250Aexperiments%2520on%2520quadrupedal%2520robots%2520and%2520point-foot%2520bipedal%2520robot%252C%2520showcasing%250Arobust%2520locomotion%2520over%2520challenging%2520terrains%2520and%2520improved%2520performance%2520compared%250Ato%2520two-stage%2520training%2520methods.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2405.10830v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Combining%20Teacher-Student%20with%20Representation%20Learning%3A%20A%20Concurrent%0A%20%20Teacher-Student%20Reinforcement%20Learning%20Paradigm%20for%20Legged%20Locomotion&entry.906535625=Hongxi%20Wang%20and%20Haoxiang%20Luo%20and%20Wei%20Zhang%20and%20Hua%20Chen&entry.1292438233=%20%20Thanks%20to%20the%20explosive%20developments%20of%20data-driven%20learning%20methodologies%0Arecently%2C%20reinforcement%20learning%20%28RL%29%20emerges%20as%20a%20promising%20solution%20to%0Aaddress%20the%20legged%20locomotion%20problem%20in%20robotics.%20In%20this%20manuscript%2C%20we%0Apropose%20a%20novel%20concurrent%20teacher-student%20reinforcement%20learning%20architecture%0Afor%20legged%20locomotion%20over%20challenging%20terrains%2C%20based%20only%20on%20proprioceptive%0Ameasurements%20in%20real-world%20deployment.%20Different%20from%20convectional%0Ateacher-student%20architecture%20that%20trains%20the%20teacher%20policy%20via%20RL%20and%0Atransfers%20the%20knowledge%20to%20the%20student%20policy%20through%20supervised%20learning%2C%20our%0Aproposed%20architecture%20trains%20teacher%20and%20student%20policy%20networks%20concurrently%0Aunder%20the%20reinforcement%20learning%20paradigm.%20To%20achieve%20this%2C%20we%20develop%20a%20new%0Atraining%20scheme%20based%20on%20conventional%20proximal%20policy%20gradient%20%28PPO%29%20method%20to%0Aaccommodate%20the%20interaction%20between%20teacher%20policy%20network%20and%20student%20policy%0Anetwork.%20The%20effectiveness%20of%20the%20proposed%20architecture%20as%20well%20as%20the%20new%0Atraining%20scheme%20is%20demonstrated%20through%20extensive%20indoor%20and%20outdoor%0Aexperiments%20on%20quadrupedal%20robots%20and%20point-foot%20bipedal%20robot%2C%20showcasing%0Arobust%20locomotion%20over%20challenging%20terrains%20and%20improved%20performance%20compared%0Ato%20two-stage%20training%20methods.%0A&entry.1838667208=http%3A//arxiv.org/abs/2405.10830v1&entry.124074799=Read"},
{"title": "Baseline Results for Selected Nonlinear System Identification Benchmarks", "author": "Max D. Champneys and Gerben I. Beintema and Roland T\u00f3th and Maarten Schoukens and Maarten Schoukens and Timothy J. Rogers", "abstract": "  Nonlinear system identification remains an important open challenge across\nresearch and academia. Large numbers of novel approaches are seen published\neach year, each presenting improvements or extensions to existing methods. It\nis natural, therefore, to consider how one might choose between these competing\nmodels. Benchmark datasets provide one clear way to approach this question.\nHowever, to make meaningful inference based on benchmark performance it is\nimportant to understand how well a new method performs comparatively to results\navailable with well-established methods. This paper presents a set of ten\nbaseline techniques and their relative performances on five popular benchmarks.\nThe aim of this contribution is to stimulate thought and discussion regarding\nobjective comparison of identification methodologies.\n", "link": "http://arxiv.org/abs/2405.10779v1", "date": "2024-05-17", "relevancy": 1.6463, "topK": [{"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.4218}, {"title": "Breadcrumbs: Adversarial class-balanced sampling for long-tailed recognition", "link": "https://link.springer.com/chapter/10.1007/978-3-031-20053-3_37", "similarity": 0.4119}, {"title": "Boosted dynamic neural networks", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/26302", "similarity": 0.4071}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Baseline%20Results%20for%20Selected%20Nonlinear%20System%20Identification%20Benchmarks&body=Title%3A%20Baseline%20Results%20for%20Selected%20Nonlinear%20System%20Identification%20Benchmarks%0AAuthor%3A%20Max%20D.%20Champneys%20and%20Gerben%20I.%20Beintema%20and%20Roland%20T%C3%B3th%20and%20Maarten%20Schoukens%20and%20Maarten%20Schoukens%20and%20Timothy%20J.%20Rogers%0AAbstract%3A%20%20%20Nonlinear%20system%20identification%20remains%20an%20important%20open%20challenge%20across%0Aresearch%20and%20academia.%20Large%20numbers%20of%20novel%20approaches%20are%20seen%20published%0Aeach%20year%2C%20each%20presenting%20improvements%20or%20extensions%20to%20existing%20methods.%20It%0Ais%20natural%2C%20therefore%2C%20to%20consider%20how%20one%20might%20choose%20between%20these%20competing%0Amodels.%20Benchmark%20datasets%20provide%20one%20clear%20way%20to%20approach%20this%20question.%0AHowever%2C%20to%20make%20meaningful%20inference%20based%20on%20benchmark%20performance%20it%20is%0Aimportant%20to%20understand%20how%20well%20a%20new%20method%20performs%20comparatively%20to%20results%0Aavailable%20with%20well-established%20methods.%20This%20paper%20presents%20a%20set%20of%20ten%0Abaseline%20techniques%20and%20their%20relative%20performances%20on%20five%20popular%20benchmarks.%0AThe%20aim%20of%20this%20contribution%20is%20to%20stimulate%20thought%20and%20discussion%20regarding%0Aobjective%20comparison%20of%20identification%20methodologies.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2405.10779v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DBaseline%2520Results%2520for%2520Selected%2520Nonlinear%2520System%2520Identification%2520Benchmarks%26entry.906535625%3DMax%2520D.%2520Champneys%2520and%2520Gerben%2520I.%2520Beintema%2520and%2520Roland%2520T%25C3%25B3th%2520and%2520Maarten%2520Schoukens%2520and%2520Maarten%2520Schoukens%2520and%2520Timothy%2520J.%2520Rogers%26entry.1292438233%3D%2520%2520Nonlinear%2520system%2520identification%2520remains%2520an%2520important%2520open%2520challenge%2520across%250Aresearch%2520and%2520academia.%2520Large%2520numbers%2520of%2520novel%2520approaches%2520are%2520seen%2520published%250Aeach%2520year%252C%2520each%2520presenting%2520improvements%2520or%2520extensions%2520to%2520existing%2520methods.%2520It%250Ais%2520natural%252C%2520therefore%252C%2520to%2520consider%2520how%2520one%2520might%2520choose%2520between%2520these%2520competing%250Amodels.%2520Benchmark%2520datasets%2520provide%2520one%2520clear%2520way%2520to%2520approach%2520this%2520question.%250AHowever%252C%2520to%2520make%2520meaningful%2520inference%2520based%2520on%2520benchmark%2520performance%2520it%2520is%250Aimportant%2520to%2520understand%2520how%2520well%2520a%2520new%2520method%2520performs%2520comparatively%2520to%2520results%250Aavailable%2520with%2520well-established%2520methods.%2520This%2520paper%2520presents%2520a%2520set%2520of%2520ten%250Abaseline%2520techniques%2520and%2520their%2520relative%2520performances%2520on%2520five%2520popular%2520benchmarks.%250AThe%2520aim%2520of%2520this%2520contribution%2520is%2520to%2520stimulate%2520thought%2520and%2520discussion%2520regarding%250Aobjective%2520comparison%2520of%2520identification%2520methodologies.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2405.10779v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Baseline%20Results%20for%20Selected%20Nonlinear%20System%20Identification%20Benchmarks&entry.906535625=Max%20D.%20Champneys%20and%20Gerben%20I.%20Beintema%20and%20Roland%20T%C3%B3th%20and%20Maarten%20Schoukens%20and%20Maarten%20Schoukens%20and%20Timothy%20J.%20Rogers&entry.1292438233=%20%20Nonlinear%20system%20identification%20remains%20an%20important%20open%20challenge%20across%0Aresearch%20and%20academia.%20Large%20numbers%20of%20novel%20approaches%20are%20seen%20published%0Aeach%20year%2C%20each%20presenting%20improvements%20or%20extensions%20to%20existing%20methods.%20It%0Ais%20natural%2C%20therefore%2C%20to%20consider%20how%20one%20might%20choose%20between%20these%20competing%0Amodels.%20Benchmark%20datasets%20provide%20one%20clear%20way%20to%20approach%20this%20question.%0AHowever%2C%20to%20make%20meaningful%20inference%20based%20on%20benchmark%20performance%20it%20is%0Aimportant%20to%20understand%20how%20well%20a%20new%20method%20performs%20comparatively%20to%20results%0Aavailable%20with%20well-established%20methods.%20This%20paper%20presents%20a%20set%20of%20ten%0Abaseline%20techniques%20and%20their%20relative%20performances%20on%20five%20popular%20benchmarks.%0AThe%20aim%20of%20this%20contribution%20is%20to%20stimulate%20thought%20and%20discussion%20regarding%0Aobjective%20comparison%20of%20identification%20methodologies.%0A&entry.1838667208=http%3A//arxiv.org/abs/2405.10779v1&entry.124074799=Read"},
{"title": "Exploiting Style Latent Flows for Generalizing Deepfake Video Detection", "author": "Jongwook Choi and Taehoon Kim and Yonghyun Jeong and Seungryul Baek and Jongwon Choi", "abstract": "  This paper presents a new approach for the detection of fake videos, based on\nthe analysis of style latent vectors and their abnormal behavior in temporal\nchanges in the generated videos. We discovered that the generated facial videos\nsuffer from the temporal distinctiveness in the temporal changes of style\nlatent vectors, which are inevitable during the generation of temporally stable\nvideos with various facial expressions and geometric transformations. Our\nframework utilizes the StyleGRU module, trained by contrastive learning, to\nrepresent the dynamic properties of style latent vectors. Additionally, we\nintroduce a style attention module that integrates StyleGRU-generated features\nwith content-based features, enabling the detection of visual and temporal\nartifacts. We demonstrate our approach across various benchmark scenarios in\ndeepfake detection, showing its superiority in cross-dataset and\ncross-manipulation scenarios. Through further analysis, we also validate the\nimportance of using temporal changes of style latent vectors to improve the\ngenerality of deepfake video detection.\n", "link": "http://arxiv.org/abs/2403.06592v2", "date": "2024-05-17", "relevancy": 1.644, "topK": [{"title": "Customize-A-Video: One-Shot Motion Customization of Text-to-Video\n  Diffusion Models", "link": "http://arxiv.org/abs/2402.14780v1", "similarity": 0.5556}, {"title": "Magic-Me: Identity-Specific Video Customized Diffusion", "link": "http://arxiv.org/abs/2402.09368v1", "similarity": 0.5507}, {"title": "Multimodal-Conditioned Latent Diffusion Models for Fashion Image Editing", "link": "http://arxiv.org/abs/2403.14828v2", "similarity": 0.5337}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Exploiting%20Style%20Latent%20Flows%20for%20Generalizing%20Deepfake%20Video%20Detection&body=Title%3A%20Exploiting%20Style%20Latent%20Flows%20for%20Generalizing%20Deepfake%20Video%20Detection%0AAuthor%3A%20Jongwook%20Choi%20and%20Taehoon%20Kim%20and%20Yonghyun%20Jeong%20and%20Seungryul%20Baek%20and%20Jongwon%20Choi%0AAbstract%3A%20%20%20This%20paper%20presents%20a%20new%20approach%20for%20the%20detection%20of%20fake%20videos%2C%20based%20on%0Athe%20analysis%20of%20style%20latent%20vectors%20and%20their%20abnormal%20behavior%20in%20temporal%0Achanges%20in%20the%20generated%20videos.%20We%20discovered%20that%20the%20generated%20facial%20videos%0Asuffer%20from%20the%20temporal%20distinctiveness%20in%20the%20temporal%20changes%20of%20style%0Alatent%20vectors%2C%20which%20are%20inevitable%20during%20the%20generation%20of%20temporally%20stable%0Avideos%20with%20various%20facial%20expressions%20and%20geometric%20transformations.%20Our%0Aframework%20utilizes%20the%20StyleGRU%20module%2C%20trained%20by%20contrastive%20learning%2C%20to%0Arepresent%20the%20dynamic%20properties%20of%20style%20latent%20vectors.%20Additionally%2C%20we%0Aintroduce%20a%20style%20attention%20module%20that%20integrates%20StyleGRU-generated%20features%0Awith%20content-based%20features%2C%20enabling%20the%20detection%20of%20visual%20and%20temporal%0Aartifacts.%20We%20demonstrate%20our%20approach%20across%20various%20benchmark%20scenarios%20in%0Adeepfake%20detection%2C%20showing%20its%20superiority%20in%20cross-dataset%20and%0Across-manipulation%20scenarios.%20Through%20further%20analysis%2C%20we%20also%20validate%20the%0Aimportance%20of%20using%20temporal%20changes%20of%20style%20latent%20vectors%20to%20improve%20the%0Agenerality%20of%20deepfake%20video%20detection.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2403.06592v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DExploiting%2520Style%2520Latent%2520Flows%2520for%2520Generalizing%2520Deepfake%2520Video%2520Detection%26entry.906535625%3DJongwook%2520Choi%2520and%2520Taehoon%2520Kim%2520and%2520Yonghyun%2520Jeong%2520and%2520Seungryul%2520Baek%2520and%2520Jongwon%2520Choi%26entry.1292438233%3D%2520%2520This%2520paper%2520presents%2520a%2520new%2520approach%2520for%2520the%2520detection%2520of%2520fake%2520videos%252C%2520based%2520on%250Athe%2520analysis%2520of%2520style%2520latent%2520vectors%2520and%2520their%2520abnormal%2520behavior%2520in%2520temporal%250Achanges%2520in%2520the%2520generated%2520videos.%2520We%2520discovered%2520that%2520the%2520generated%2520facial%2520videos%250Asuffer%2520from%2520the%2520temporal%2520distinctiveness%2520in%2520the%2520temporal%2520changes%2520of%2520style%250Alatent%2520vectors%252C%2520which%2520are%2520inevitable%2520during%2520the%2520generation%2520of%2520temporally%2520stable%250Avideos%2520with%2520various%2520facial%2520expressions%2520and%2520geometric%2520transformations.%2520Our%250Aframework%2520utilizes%2520the%2520StyleGRU%2520module%252C%2520trained%2520by%2520contrastive%2520learning%252C%2520to%250Arepresent%2520the%2520dynamic%2520properties%2520of%2520style%2520latent%2520vectors.%2520Additionally%252C%2520we%250Aintroduce%2520a%2520style%2520attention%2520module%2520that%2520integrates%2520StyleGRU-generated%2520features%250Awith%2520content-based%2520features%252C%2520enabling%2520the%2520detection%2520of%2520visual%2520and%2520temporal%250Aartifacts.%2520We%2520demonstrate%2520our%2520approach%2520across%2520various%2520benchmark%2520scenarios%2520in%250Adeepfake%2520detection%252C%2520showing%2520its%2520superiority%2520in%2520cross-dataset%2520and%250Across-manipulation%2520scenarios.%2520Through%2520further%2520analysis%252C%2520we%2520also%2520validate%2520the%250Aimportance%2520of%2520using%2520temporal%2520changes%2520of%2520style%2520latent%2520vectors%2520to%2520improve%2520the%250Agenerality%2520of%2520deepfake%2520video%2520detection.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2403.06592v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Exploiting%20Style%20Latent%20Flows%20for%20Generalizing%20Deepfake%20Video%20Detection&entry.906535625=Jongwook%20Choi%20and%20Taehoon%20Kim%20and%20Yonghyun%20Jeong%20and%20Seungryul%20Baek%20and%20Jongwon%20Choi&entry.1292438233=%20%20This%20paper%20presents%20a%20new%20approach%20for%20the%20detection%20of%20fake%20videos%2C%20based%20on%0Athe%20analysis%20of%20style%20latent%20vectors%20and%20their%20abnormal%20behavior%20in%20temporal%0Achanges%20in%20the%20generated%20videos.%20We%20discovered%20that%20the%20generated%20facial%20videos%0Asuffer%20from%20the%20temporal%20distinctiveness%20in%20the%20temporal%20changes%20of%20style%0Alatent%20vectors%2C%20which%20are%20inevitable%20during%20the%20generation%20of%20temporally%20stable%0Avideos%20with%20various%20facial%20expressions%20and%20geometric%20transformations.%20Our%0Aframework%20utilizes%20the%20StyleGRU%20module%2C%20trained%20by%20contrastive%20learning%2C%20to%0Arepresent%20the%20dynamic%20properties%20of%20style%20latent%20vectors.%20Additionally%2C%20we%0Aintroduce%20a%20style%20attention%20module%20that%20integrates%20StyleGRU-generated%20features%0Awith%20content-based%20features%2C%20enabling%20the%20detection%20of%20visual%20and%20temporal%0Aartifacts.%20We%20demonstrate%20our%20approach%20across%20various%20benchmark%20scenarios%20in%0Adeepfake%20detection%2C%20showing%20its%20superiority%20in%20cross-dataset%20and%0Across-manipulation%20scenarios.%20Through%20further%20analysis%2C%20we%20also%20validate%20the%0Aimportance%20of%20using%20temporal%20changes%20of%20style%20latent%20vectors%20to%20improve%20the%0Agenerality%20of%20deepfake%20video%20detection.%0A&entry.1838667208=http%3A//arxiv.org/abs/2403.06592v2&entry.124074799=Read"},
{"title": "Super-Exponential Regret for UCT, AlphaGo and Variants", "author": "Laurent Orseau and Remi Munos", "abstract": "  We improve the proofs of the lower bounds of Coquelin and Munos (2007) that\ndemonstrate that UCT can have $\\exp(\\dots\\exp(1)\\dots)$ regret (with\n$\\Omega(D)$ exp terms) on the $D$-chain environment, and that a `polynomial'\nUCT variant has $\\exp_2(\\exp_2(D - O(\\log D)))$ regret on the same environment\n-- the original proofs contain an oversight for rewards bounded in $[0, 1]$,\nwhich we fix in the present draft. We also adapt the proofs to AlphaGo's MCTS\nand its descendants (e.g., AlphaZero, Leela Zero) to also show $\\exp_2(\\exp_2(D\n- O(\\log D)))$ regret.\n", "link": "http://arxiv.org/abs/2405.04407v2", "date": "2024-05-17", "relevancy": 1.6295, "topK": [{"title": "UGG: Unified Generative Grasping", "link": "https://arxiv.org/abs/2311.16917", "similarity": 0.4384}, {"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.3986}, {"title": "ToDo: Token Downsampling for Efficient Generation of High-Resolution\n  Images", "link": "http://arxiv.org/abs/2402.13573v2", "similarity": 0.3799}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Super-Exponential%20Regret%20for%20UCT%2C%20AlphaGo%20and%20Variants&body=Title%3A%20Super-Exponential%20Regret%20for%20UCT%2C%20AlphaGo%20and%20Variants%0AAuthor%3A%20Laurent%20Orseau%20and%20Remi%20Munos%0AAbstract%3A%20%20%20We%20improve%20the%20proofs%20of%20the%20lower%20bounds%20of%20Coquelin%20and%20Munos%20%282007%29%20that%0Ademonstrate%20that%20UCT%20can%20have%20%24%5Cexp%28%5Cdots%5Cexp%281%29%5Cdots%29%24%20regret%20%28with%0A%24%5COmega%28D%29%24%20exp%20terms%29%20on%20the%20%24D%24-chain%20environment%2C%20and%20that%20a%20%60polynomial%27%0AUCT%20variant%20has%20%24%5Cexp_2%28%5Cexp_2%28D%20-%20O%28%5Clog%20D%29%29%29%24%20regret%20on%20the%20same%20environment%0A--%20the%20original%20proofs%20contain%20an%20oversight%20for%20rewards%20bounded%20in%20%24%5B0%2C%201%5D%24%2C%0Awhich%20we%20fix%20in%20the%20present%20draft.%20We%20also%20adapt%20the%20proofs%20to%20AlphaGo%27s%20MCTS%0Aand%20its%20descendants%20%28e.g.%2C%20AlphaZero%2C%20Leela%20Zero%29%20to%20also%20show%20%24%5Cexp_2%28%5Cexp_2%28D%0A-%20O%28%5Clog%20D%29%29%29%24%20regret.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2405.04407v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DSuper-Exponential%2520Regret%2520for%2520UCT%252C%2520AlphaGo%2520and%2520Variants%26entry.906535625%3DLaurent%2520Orseau%2520and%2520Remi%2520Munos%26entry.1292438233%3D%2520%2520We%2520improve%2520the%2520proofs%2520of%2520the%2520lower%2520bounds%2520of%2520Coquelin%2520and%2520Munos%2520%25282007%2529%2520that%250Ademonstrate%2520that%2520UCT%2520can%2520have%2520%2524%255Cexp%2528%255Cdots%255Cexp%25281%2529%255Cdots%2529%2524%2520regret%2520%2528with%250A%2524%255COmega%2528D%2529%2524%2520exp%2520terms%2529%2520on%2520the%2520%2524D%2524-chain%2520environment%252C%2520and%2520that%2520a%2520%2560polynomial%2527%250AUCT%2520variant%2520has%2520%2524%255Cexp_2%2528%255Cexp_2%2528D%2520-%2520O%2528%255Clog%2520D%2529%2529%2529%2524%2520regret%2520on%2520the%2520same%2520environment%250A--%2520the%2520original%2520proofs%2520contain%2520an%2520oversight%2520for%2520rewards%2520bounded%2520in%2520%2524%255B0%252C%25201%255D%2524%252C%250Awhich%2520we%2520fix%2520in%2520the%2520present%2520draft.%2520We%2520also%2520adapt%2520the%2520proofs%2520to%2520AlphaGo%2527s%2520MCTS%250Aand%2520its%2520descendants%2520%2528e.g.%252C%2520AlphaZero%252C%2520Leela%2520Zero%2529%2520to%2520also%2520show%2520%2524%255Cexp_2%2528%255Cexp_2%2528D%250A-%2520O%2528%255Clog%2520D%2529%2529%2529%2524%2520regret.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2405.04407v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Super-Exponential%20Regret%20for%20UCT%2C%20AlphaGo%20and%20Variants&entry.906535625=Laurent%20Orseau%20and%20Remi%20Munos&entry.1292438233=%20%20We%20improve%20the%20proofs%20of%20the%20lower%20bounds%20of%20Coquelin%20and%20Munos%20%282007%29%20that%0Ademonstrate%20that%20UCT%20can%20have%20%24%5Cexp%28%5Cdots%5Cexp%281%29%5Cdots%29%24%20regret%20%28with%0A%24%5COmega%28D%29%24%20exp%20terms%29%20on%20the%20%24D%24-chain%20environment%2C%20and%20that%20a%20%60polynomial%27%0AUCT%20variant%20has%20%24%5Cexp_2%28%5Cexp_2%28D%20-%20O%28%5Clog%20D%29%29%29%24%20regret%20on%20the%20same%20environment%0A--%20the%20original%20proofs%20contain%20an%20oversight%20for%20rewards%20bounded%20in%20%24%5B0%2C%201%5D%24%2C%0Awhich%20we%20fix%20in%20the%20present%20draft.%20We%20also%20adapt%20the%20proofs%20to%20AlphaGo%27s%20MCTS%0Aand%20its%20descendants%20%28e.g.%2C%20AlphaZero%2C%20Leela%20Zero%29%20to%20also%20show%20%24%5Cexp_2%28%5Cexp_2%28D%0A-%20O%28%5Clog%20D%29%29%29%24%20regret.%0A&entry.1838667208=http%3A//arxiv.org/abs/2405.04407v2&entry.124074799=Read"},
{"title": "SafEDMD: A certified learning architecture tailored to data-driven\n  control of nonlinear dynamical systems", "author": "Robin Str\u00e4sser and Manuel Schaller and Karl Worthmann and Julian Berberich and Frank Allg\u00f6wer", "abstract": "  The Koopman operator serves as the theoretical backbone for machine learning\nof dynamical control systems, where the operator is heuristically approximated\nby extended dynamic mode decomposition (EDMD). In this paper, we propose\nStability- and certificate-oriented EDMD (SafEDMD): a novel EDMD-based learning\narchitecture which comes along with rigorous certificates, resulting in a\nreliable surrogate model generated in a data-driven fashion. To ensure the\ntrustworthiness of SafEDMD, we derive proportional error bounds, which vanish\nat the origin and are tailored to control tasks, leading to certified\ncontroller design based on semi-definite programming. We illustrate the\ndeveloped method by means of several benchmark examples and highlight the\nadvantages over state-of-the-art methods.\n", "link": "http://arxiv.org/abs/2402.03145v2", "date": "2024-05-17", "relevancy": 1.614, "topK": [{"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.546}, {"title": "Safe and efficient training of a control agent", "link": "https://patents.google.com/patent/US11709462B2/en", "similarity": 0.5412}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.5221}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20SafEDMD%3A%20A%20certified%20learning%20architecture%20tailored%20to%20data-driven%0A%20%20control%20of%20nonlinear%20dynamical%20systems&body=Title%3A%20SafEDMD%3A%20A%20certified%20learning%20architecture%20tailored%20to%20data-driven%0A%20%20control%20of%20nonlinear%20dynamical%20systems%0AAuthor%3A%20Robin%20Str%C3%A4sser%20and%20Manuel%20Schaller%20and%20Karl%20Worthmann%20and%20Julian%20Berberich%20and%20Frank%20Allg%C3%B6wer%0AAbstract%3A%20%20%20The%20Koopman%20operator%20serves%20as%20the%20theoretical%20backbone%20for%20machine%20learning%0Aof%20dynamical%20control%20systems%2C%20where%20the%20operator%20is%20heuristically%20approximated%0Aby%20extended%20dynamic%20mode%20decomposition%20%28EDMD%29.%20In%20this%20paper%2C%20we%20propose%0AStability-%20and%20certificate-oriented%20EDMD%20%28SafEDMD%29%3A%20a%20novel%20EDMD-based%20learning%0Aarchitecture%20which%20comes%20along%20with%20rigorous%20certificates%2C%20resulting%20in%20a%0Areliable%20surrogate%20model%20generated%20in%20a%20data-driven%20fashion.%20To%20ensure%20the%0Atrustworthiness%20of%20SafEDMD%2C%20we%20derive%20proportional%20error%20bounds%2C%20which%20vanish%0Aat%20the%20origin%20and%20are%20tailored%20to%20control%20tasks%2C%20leading%20to%20certified%0Acontroller%20design%20based%20on%20semi-definite%20programming.%20We%20illustrate%20the%0Adeveloped%20method%20by%20means%20of%20several%20benchmark%20examples%20and%20highlight%20the%0Aadvantages%20over%20state-of-the-art%20methods.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2402.03145v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DSafEDMD%253A%2520A%2520certified%2520learning%2520architecture%2520tailored%2520to%2520data-driven%250A%2520%2520control%2520of%2520nonlinear%2520dynamical%2520systems%26entry.906535625%3DRobin%2520Str%25C3%25A4sser%2520and%2520Manuel%2520Schaller%2520and%2520Karl%2520Worthmann%2520and%2520Julian%2520Berberich%2520and%2520Frank%2520Allg%25C3%25B6wer%26entry.1292438233%3D%2520%2520The%2520Koopman%2520operator%2520serves%2520as%2520the%2520theoretical%2520backbone%2520for%2520machine%2520learning%250Aof%2520dynamical%2520control%2520systems%252C%2520where%2520the%2520operator%2520is%2520heuristically%2520approximated%250Aby%2520extended%2520dynamic%2520mode%2520decomposition%2520%2528EDMD%2529.%2520In%2520this%2520paper%252C%2520we%2520propose%250AStability-%2520and%2520certificate-oriented%2520EDMD%2520%2528SafEDMD%2529%253A%2520a%2520novel%2520EDMD-based%2520learning%250Aarchitecture%2520which%2520comes%2520along%2520with%2520rigorous%2520certificates%252C%2520resulting%2520in%2520a%250Areliable%2520surrogate%2520model%2520generated%2520in%2520a%2520data-driven%2520fashion.%2520To%2520ensure%2520the%250Atrustworthiness%2520of%2520SafEDMD%252C%2520we%2520derive%2520proportional%2520error%2520bounds%252C%2520which%2520vanish%250Aat%2520the%2520origin%2520and%2520are%2520tailored%2520to%2520control%2520tasks%252C%2520leading%2520to%2520certified%250Acontroller%2520design%2520based%2520on%2520semi-definite%2520programming.%2520We%2520illustrate%2520the%250Adeveloped%2520method%2520by%2520means%2520of%2520several%2520benchmark%2520examples%2520and%2520highlight%2520the%250Aadvantages%2520over%2520state-of-the-art%2520methods.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2402.03145v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=SafEDMD%3A%20A%20certified%20learning%20architecture%20tailored%20to%20data-driven%0A%20%20control%20of%20nonlinear%20dynamical%20systems&entry.906535625=Robin%20Str%C3%A4sser%20and%20Manuel%20Schaller%20and%20Karl%20Worthmann%20and%20Julian%20Berberich%20and%20Frank%20Allg%C3%B6wer&entry.1292438233=%20%20The%20Koopman%20operator%20serves%20as%20the%20theoretical%20backbone%20for%20machine%20learning%0Aof%20dynamical%20control%20systems%2C%20where%20the%20operator%20is%20heuristically%20approximated%0Aby%20extended%20dynamic%20mode%20decomposition%20%28EDMD%29.%20In%20this%20paper%2C%20we%20propose%0AStability-%20and%20certificate-oriented%20EDMD%20%28SafEDMD%29%3A%20a%20novel%20EDMD-based%20learning%0Aarchitecture%20which%20comes%20along%20with%20rigorous%20certificates%2C%20resulting%20in%20a%0Areliable%20surrogate%20model%20generated%20in%20a%20data-driven%20fashion.%20To%20ensure%20the%0Atrustworthiness%20of%20SafEDMD%2C%20we%20derive%20proportional%20error%20bounds%2C%20which%20vanish%0Aat%20the%20origin%20and%20are%20tailored%20to%20control%20tasks%2C%20leading%20to%20certified%0Acontroller%20design%20based%20on%20semi-definite%20programming.%20We%20illustrate%20the%0Adeveloped%20method%20by%20means%20of%20several%20benchmark%20examples%20and%20highlight%20the%0Aadvantages%20over%20state-of-the-art%20methods.%0A&entry.1838667208=http%3A//arxiv.org/abs/2402.03145v2&entry.124074799=Read"},
{"title": "Differentially private projection-depth-based medians", "author": "Kelly Ramsay and Dylan Spicker", "abstract": "  We develop $(\\epsilon,\\delta)$-differentially private projection-depth-based\nmedians using the propose-test-release (PTR) and exponential mechanisms. Under\ngeneral conditions on the input parameters and the population measure, (e.g. we\ndo not assume any moment bounds), we quantify the probability the test in PTR\nfails, as well as the cost of privacy via finite sample deviation bounds. We\nthen present a new definition of the finite sample breakdown point which\napplies to a mechanism, and present a lower bound on the finite sample\nbreakdown point of the projection-depth-based median. We demonstrate our main\nresults on the canonical projection-depth-based median, as well as on\nprojection-depth-based medians derived from trimmed estimators. In the Gaussian\nsetting, we show that the resulting deviation bound matches the known lower\nbound for private Gaussian mean estimation. In the Cauchy setting, we show that\nthe \"outlier error amplification\" effect resulting from the heavy tails\noutweighs the cost of privacy. This result is then verified via numerical\nsimulations. Additionally, we present results on general PTR mechanisms and a\nuniform concentration result on the projected spacings of order statistics,\nwhich may be of general interest.\n", "link": "http://arxiv.org/abs/2312.07792v2", "date": "2024-05-17", "relevancy": 1.6133, "topK": [{"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.4252}, {"title": "Dynamic Gaussians Mesh: Consistent Mesh Reconstruction from Monocular\n  Videos", "link": "http://arxiv.org/abs/2404.12379v2", "similarity": 0.4072}, {"title": "ToDo: Token Downsampling for Efficient Generation of High-Resolution\n  Images", "link": "http://arxiv.org/abs/2402.13573v2", "similarity": 0.3907}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Differentially%20private%20projection-depth-based%20medians&body=Title%3A%20Differentially%20private%20projection-depth-based%20medians%0AAuthor%3A%20Kelly%20Ramsay%20and%20Dylan%20Spicker%0AAbstract%3A%20%20%20We%20develop%20%24%28%5Cepsilon%2C%5Cdelta%29%24-differentially%20private%20projection-depth-based%0Amedians%20using%20the%20propose-test-release%20%28PTR%29%20and%20exponential%20mechanisms.%20Under%0Ageneral%20conditions%20on%20the%20input%20parameters%20and%20the%20population%20measure%2C%20%28e.g.%20we%0Ado%20not%20assume%20any%20moment%20bounds%29%2C%20we%20quantify%20the%20probability%20the%20test%20in%20PTR%0Afails%2C%20as%20well%20as%20the%20cost%20of%20privacy%20via%20finite%20sample%20deviation%20bounds.%20We%0Athen%20present%20a%20new%20definition%20of%20the%20finite%20sample%20breakdown%20point%20which%0Aapplies%20to%20a%20mechanism%2C%20and%20present%20a%20lower%20bound%20on%20the%20finite%20sample%0Abreakdown%20point%20of%20the%20projection-depth-based%20median.%20We%20demonstrate%20our%20main%0Aresults%20on%20the%20canonical%20projection-depth-based%20median%2C%20as%20well%20as%20on%0Aprojection-depth-based%20medians%20derived%20from%20trimmed%20estimators.%20In%20the%20Gaussian%0Asetting%2C%20we%20show%20that%20the%20resulting%20deviation%20bound%20matches%20the%20known%20lower%0Abound%20for%20private%20Gaussian%20mean%20estimation.%20In%20the%20Cauchy%20setting%2C%20we%20show%20that%0Athe%20%22outlier%20error%20amplification%22%20effect%20resulting%20from%20the%20heavy%20tails%0Aoutweighs%20the%20cost%20of%20privacy.%20This%20result%20is%20then%20verified%20via%20numerical%0Asimulations.%20Additionally%2C%20we%20present%20results%20on%20general%20PTR%20mechanisms%20and%20a%0Auniform%20concentration%20result%20on%20the%20projected%20spacings%20of%20order%20statistics%2C%0Awhich%20may%20be%20of%20general%20interest.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2312.07792v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DDifferentially%2520private%2520projection-depth-based%2520medians%26entry.906535625%3DKelly%2520Ramsay%2520and%2520Dylan%2520Spicker%26entry.1292438233%3D%2520%2520We%2520develop%2520%2524%2528%255Cepsilon%252C%255Cdelta%2529%2524-differentially%2520private%2520projection-depth-based%250Amedians%2520using%2520the%2520propose-test-release%2520%2528PTR%2529%2520and%2520exponential%2520mechanisms.%2520Under%250Ageneral%2520conditions%2520on%2520the%2520input%2520parameters%2520and%2520the%2520population%2520measure%252C%2520%2528e.g.%2520we%250Ado%2520not%2520assume%2520any%2520moment%2520bounds%2529%252C%2520we%2520quantify%2520the%2520probability%2520the%2520test%2520in%2520PTR%250Afails%252C%2520as%2520well%2520as%2520the%2520cost%2520of%2520privacy%2520via%2520finite%2520sample%2520deviation%2520bounds.%2520We%250Athen%2520present%2520a%2520new%2520definition%2520of%2520the%2520finite%2520sample%2520breakdown%2520point%2520which%250Aapplies%2520to%2520a%2520mechanism%252C%2520and%2520present%2520a%2520lower%2520bound%2520on%2520the%2520finite%2520sample%250Abreakdown%2520point%2520of%2520the%2520projection-depth-based%2520median.%2520We%2520demonstrate%2520our%2520main%250Aresults%2520on%2520the%2520canonical%2520projection-depth-based%2520median%252C%2520as%2520well%2520as%2520on%250Aprojection-depth-based%2520medians%2520derived%2520from%2520trimmed%2520estimators.%2520In%2520the%2520Gaussian%250Asetting%252C%2520we%2520show%2520that%2520the%2520resulting%2520deviation%2520bound%2520matches%2520the%2520known%2520lower%250Abound%2520for%2520private%2520Gaussian%2520mean%2520estimation.%2520In%2520the%2520Cauchy%2520setting%252C%2520we%2520show%2520that%250Athe%2520%2522outlier%2520error%2520amplification%2522%2520effect%2520resulting%2520from%2520the%2520heavy%2520tails%250Aoutweighs%2520the%2520cost%2520of%2520privacy.%2520This%2520result%2520is%2520then%2520verified%2520via%2520numerical%250Asimulations.%2520Additionally%252C%2520we%2520present%2520results%2520on%2520general%2520PTR%2520mechanisms%2520and%2520a%250Auniform%2520concentration%2520result%2520on%2520the%2520projected%2520spacings%2520of%2520order%2520statistics%252C%250Awhich%2520may%2520be%2520of%2520general%2520interest.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2312.07792v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Differentially%20private%20projection-depth-based%20medians&entry.906535625=Kelly%20Ramsay%20and%20Dylan%20Spicker&entry.1292438233=%20%20We%20develop%20%24%28%5Cepsilon%2C%5Cdelta%29%24-differentially%20private%20projection-depth-based%0Amedians%20using%20the%20propose-test-release%20%28PTR%29%20and%20exponential%20mechanisms.%20Under%0Ageneral%20conditions%20on%20the%20input%20parameters%20and%20the%20population%20measure%2C%20%28e.g.%20we%0Ado%20not%20assume%20any%20moment%20bounds%29%2C%20we%20quantify%20the%20probability%20the%20test%20in%20PTR%0Afails%2C%20as%20well%20as%20the%20cost%20of%20privacy%20via%20finite%20sample%20deviation%20bounds.%20We%0Athen%20present%20a%20new%20definition%20of%20the%20finite%20sample%20breakdown%20point%20which%0Aapplies%20to%20a%20mechanism%2C%20and%20present%20a%20lower%20bound%20on%20the%20finite%20sample%0Abreakdown%20point%20of%20the%20projection-depth-based%20median.%20We%20demonstrate%20our%20main%0Aresults%20on%20the%20canonical%20projection-depth-based%20median%2C%20as%20well%20as%20on%0Aprojection-depth-based%20medians%20derived%20from%20trimmed%20estimators.%20In%20the%20Gaussian%0Asetting%2C%20we%20show%20that%20the%20resulting%20deviation%20bound%20matches%20the%20known%20lower%0Abound%20for%20private%20Gaussian%20mean%20estimation.%20In%20the%20Cauchy%20setting%2C%20we%20show%20that%0Athe%20%22outlier%20error%20amplification%22%20effect%20resulting%20from%20the%20heavy%20tails%0Aoutweighs%20the%20cost%20of%20privacy.%20This%20result%20is%20then%20verified%20via%20numerical%0Asimulations.%20Additionally%2C%20we%20present%20results%20on%20general%20PTR%20mechanisms%20and%20a%0Auniform%20concentration%20result%20on%20the%20projected%20spacings%20of%20order%20statistics%2C%0Awhich%20may%20be%20of%20general%20interest.%0A&entry.1838667208=http%3A//arxiv.org/abs/2312.07792v2&entry.124074799=Read"},
{"title": "Autonomous AI-enabled Industrial Sorting Pipeline for Advanced Textile\n  Recycling", "author": "Yannis Spyridis and Vasileios Argyriou and Antonios Sarigiannidis and Panagiotis Radoglou and Panagiotis Sarigiannidis", "abstract": "  The escalating volumes of textile waste globally necessitate innovative waste\nmanagement solutions to mitigate the environmental impact and promote\nsustainability in the fashion industry. This paper addresses the inefficiencies\nof traditional textile sorting methods by introducing an autonomous textile\nanalysis pipeline. Utilising robotics, spectral imaging, and AI-driven\nclassification, our system enhances the accuracy, efficiency, and scalability\nof textile sorting processes, contributing to a more sustainable and circular\napproach to waste management. The integration of a Digital Twin system further\nallows critical evaluation of technical and economic feasibility, providing\nvaluable insights into the sorting system's accuracy and reliability. The\nproposed framework, inspired by Industry 4.0 principles, comprises five\ninterconnected layers facilitating seamless data exchange and coordination\nwithin the system. Preliminary results highlight the potential of our holistic\napproach to mitigate environmental impact and foster a positive shift towards\nrecycling in the textile industry.\n", "link": "http://arxiv.org/abs/2405.10696v1", "date": "2024-05-17", "relevancy": 1.6059, "topK": [{"title": "DressCode: Autoregressively Sewing and Generating Garments from Text\n  Guidance", "link": "http://arxiv.org/abs/2401.16465v3", "similarity": 0.6003}, {"title": "Multimodal-Conditioned Latent Diffusion Models for Fashion Image Editing", "link": "http://arxiv.org/abs/2403.14828v2", "similarity": 0.4728}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.4352}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Autonomous%20AI-enabled%20Industrial%20Sorting%20Pipeline%20for%20Advanced%20Textile%0A%20%20Recycling&body=Title%3A%20Autonomous%20AI-enabled%20Industrial%20Sorting%20Pipeline%20for%20Advanced%20Textile%0A%20%20Recycling%0AAuthor%3A%20Yannis%20Spyridis%20and%20Vasileios%20Argyriou%20and%20Antonios%20Sarigiannidis%20and%20Panagiotis%20Radoglou%20and%20Panagiotis%20Sarigiannidis%0AAbstract%3A%20%20%20The%20escalating%20volumes%20of%20textile%20waste%20globally%20necessitate%20innovative%20waste%0Amanagement%20solutions%20to%20mitigate%20the%20environmental%20impact%20and%20promote%0Asustainability%20in%20the%20fashion%20industry.%20This%20paper%20addresses%20the%20inefficiencies%0Aof%20traditional%20textile%20sorting%20methods%20by%20introducing%20an%20autonomous%20textile%0Aanalysis%20pipeline.%20Utilising%20robotics%2C%20spectral%20imaging%2C%20and%20AI-driven%0Aclassification%2C%20our%20system%20enhances%20the%20accuracy%2C%20efficiency%2C%20and%20scalability%0Aof%20textile%20sorting%20processes%2C%20contributing%20to%20a%20more%20sustainable%20and%20circular%0Aapproach%20to%20waste%20management.%20The%20integration%20of%20a%20Digital%20Twin%20system%20further%0Aallows%20critical%20evaluation%20of%20technical%20and%20economic%20feasibility%2C%20providing%0Avaluable%20insights%20into%20the%20sorting%20system%27s%20accuracy%20and%20reliability.%20The%0Aproposed%20framework%2C%20inspired%20by%20Industry%204.0%20principles%2C%20comprises%20five%0Ainterconnected%20layers%20facilitating%20seamless%20data%20exchange%20and%20coordination%0Awithin%20the%20system.%20Preliminary%20results%20highlight%20the%20potential%20of%20our%20holistic%0Aapproach%20to%20mitigate%20environmental%20impact%20and%20foster%20a%20positive%20shift%20towards%0Arecycling%20in%20the%20textile%20industry.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2405.10696v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DAutonomous%2520AI-enabled%2520Industrial%2520Sorting%2520Pipeline%2520for%2520Advanced%2520Textile%250A%2520%2520Recycling%26entry.906535625%3DYannis%2520Spyridis%2520and%2520Vasileios%2520Argyriou%2520and%2520Antonios%2520Sarigiannidis%2520and%2520Panagiotis%2520Radoglou%2520and%2520Panagiotis%2520Sarigiannidis%26entry.1292438233%3D%2520%2520The%2520escalating%2520volumes%2520of%2520textile%2520waste%2520globally%2520necessitate%2520innovative%2520waste%250Amanagement%2520solutions%2520to%2520mitigate%2520the%2520environmental%2520impact%2520and%2520promote%250Asustainability%2520in%2520the%2520fashion%2520industry.%2520This%2520paper%2520addresses%2520the%2520inefficiencies%250Aof%2520traditional%2520textile%2520sorting%2520methods%2520by%2520introducing%2520an%2520autonomous%2520textile%250Aanalysis%2520pipeline.%2520Utilising%2520robotics%252C%2520spectral%2520imaging%252C%2520and%2520AI-driven%250Aclassification%252C%2520our%2520system%2520enhances%2520the%2520accuracy%252C%2520efficiency%252C%2520and%2520scalability%250Aof%2520textile%2520sorting%2520processes%252C%2520contributing%2520to%2520a%2520more%2520sustainable%2520and%2520circular%250Aapproach%2520to%2520waste%2520management.%2520The%2520integration%2520of%2520a%2520Digital%2520Twin%2520system%2520further%250Aallows%2520critical%2520evaluation%2520of%2520technical%2520and%2520economic%2520feasibility%252C%2520providing%250Avaluable%2520insights%2520into%2520the%2520sorting%2520system%2527s%2520accuracy%2520and%2520reliability.%2520The%250Aproposed%2520framework%252C%2520inspired%2520by%2520Industry%25204.0%2520principles%252C%2520comprises%2520five%250Ainterconnected%2520layers%2520facilitating%2520seamless%2520data%2520exchange%2520and%2520coordination%250Awithin%2520the%2520system.%2520Preliminary%2520results%2520highlight%2520the%2520potential%2520of%2520our%2520holistic%250Aapproach%2520to%2520mitigate%2520environmental%2520impact%2520and%2520foster%2520a%2520positive%2520shift%2520towards%250Arecycling%2520in%2520the%2520textile%2520industry.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2405.10696v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Autonomous%20AI-enabled%20Industrial%20Sorting%20Pipeline%20for%20Advanced%20Textile%0A%20%20Recycling&entry.906535625=Yannis%20Spyridis%20and%20Vasileios%20Argyriou%20and%20Antonios%20Sarigiannidis%20and%20Panagiotis%20Radoglou%20and%20Panagiotis%20Sarigiannidis&entry.1292438233=%20%20The%20escalating%20volumes%20of%20textile%20waste%20globally%20necessitate%20innovative%20waste%0Amanagement%20solutions%20to%20mitigate%20the%20environmental%20impact%20and%20promote%0Asustainability%20in%20the%20fashion%20industry.%20This%20paper%20addresses%20the%20inefficiencies%0Aof%20traditional%20textile%20sorting%20methods%20by%20introducing%20an%20autonomous%20textile%0Aanalysis%20pipeline.%20Utilising%20robotics%2C%20spectral%20imaging%2C%20and%20AI-driven%0Aclassification%2C%20our%20system%20enhances%20the%20accuracy%2C%20efficiency%2C%20and%20scalability%0Aof%20textile%20sorting%20processes%2C%20contributing%20to%20a%20more%20sustainable%20and%20circular%0Aapproach%20to%20waste%20management.%20The%20integration%20of%20a%20Digital%20Twin%20system%20further%0Aallows%20critical%20evaluation%20of%20technical%20and%20economic%20feasibility%2C%20providing%0Avaluable%20insights%20into%20the%20sorting%20system%27s%20accuracy%20and%20reliability.%20The%0Aproposed%20framework%2C%20inspired%20by%20Industry%204.0%20principles%2C%20comprises%20five%0Ainterconnected%20layers%20facilitating%20seamless%20data%20exchange%20and%20coordination%0Awithin%20the%20system.%20Preliminary%20results%20highlight%20the%20potential%20of%20our%20holistic%0Aapproach%20to%20mitigate%20environmental%20impact%20and%20foster%20a%20positive%20shift%20towards%0Arecycling%20in%20the%20textile%20industry.%0A&entry.1838667208=http%3A//arxiv.org/abs/2405.10696v1&entry.124074799=Read"},
{"title": "Attention-Driven Multi-Agent Reinforcement Learning: Enhancing Decisions\n  with Expertise-Informed Tasks", "author": "Andre R Kuroswiski and Annie S Wu and Angelo Passaro", "abstract": "  In this paper, we introduce an alternative approach to enhancing Multi-Agent\nReinforcement Learning (MARL) through the integration of domain knowledge and\nattention-based policy mechanisms. Our methodology focuses on the incorporation\nof domain-specific expertise into the learning process, which simplifies the\ndevelopment of collaborative behaviors. This approach aims to reduce the\ncomplexity and learning overhead typically associated with MARL by enabling\nagents to concentrate on essential aspects of complex tasks, thus optimizing\nthe learning curve. The utilization of attention mechanisms plays a key role in\nour model. It allows for the effective processing of dynamic context data and\nnuanced agent interactions, leading to more refined decision-making. Applied in\nstandard MARL scenarios, such as the Stanford Intelligent Systems Laboratory\n(SISL) Pursuit and Multi-Particle Environments (MPE) Simple Spread, our method\nhas been shown to improve both learning efficiency and the effectiveness of\ncollaborative behaviors. The results indicate that our attention-based approach\ncan be a viable approach for improving the efficiency of MARL training process,\nintegrating domain-specific knowledge at the action level.\n", "link": "http://arxiv.org/abs/2404.05840v3", "date": "2024-05-17", "relevancy": 1.6047, "topK": [{"title": "Safe and efficient training of a control agent", "link": "https://patents.google.com/patent/US11709462B2/en", "similarity": 0.539}, {"title": "RoboEXP: Action-Conditioned Scene Graph via Interactive Exploration for\n  Robotic Manipulation", "link": "http://arxiv.org/abs/2402.15487v1", "similarity": 0.5321}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.5276}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Attention-Driven%20Multi-Agent%20Reinforcement%20Learning%3A%20Enhancing%20Decisions%0A%20%20with%20Expertise-Informed%20Tasks&body=Title%3A%20Attention-Driven%20Multi-Agent%20Reinforcement%20Learning%3A%20Enhancing%20Decisions%0A%20%20with%20Expertise-Informed%20Tasks%0AAuthor%3A%20Andre%20R%20Kuroswiski%20and%20Annie%20S%20Wu%20and%20Angelo%20Passaro%0AAbstract%3A%20%20%20In%20this%20paper%2C%20we%20introduce%20an%20alternative%20approach%20to%20enhancing%20Multi-Agent%0AReinforcement%20Learning%20%28MARL%29%20through%20the%20integration%20of%20domain%20knowledge%20and%0Aattention-based%20policy%20mechanisms.%20Our%20methodology%20focuses%20on%20the%20incorporation%0Aof%20domain-specific%20expertise%20into%20the%20learning%20process%2C%20which%20simplifies%20the%0Adevelopment%20of%20collaborative%20behaviors.%20This%20approach%20aims%20to%20reduce%20the%0Acomplexity%20and%20learning%20overhead%20typically%20associated%20with%20MARL%20by%20enabling%0Aagents%20to%20concentrate%20on%20essential%20aspects%20of%20complex%20tasks%2C%20thus%20optimizing%0Athe%20learning%20curve.%20The%20utilization%20of%20attention%20mechanisms%20plays%20a%20key%20role%20in%0Aour%20model.%20It%20allows%20for%20the%20effective%20processing%20of%20dynamic%20context%20data%20and%0Anuanced%20agent%20interactions%2C%20leading%20to%20more%20refined%20decision-making.%20Applied%20in%0Astandard%20MARL%20scenarios%2C%20such%20as%20the%20Stanford%20Intelligent%20Systems%20Laboratory%0A%28SISL%29%20Pursuit%20and%20Multi-Particle%20Environments%20%28MPE%29%20Simple%20Spread%2C%20our%20method%0Ahas%20been%20shown%20to%20improve%20both%20learning%20efficiency%20and%20the%20effectiveness%20of%0Acollaborative%20behaviors.%20The%20results%20indicate%20that%20our%20attention-based%20approach%0Acan%20be%20a%20viable%20approach%20for%20improving%20the%20efficiency%20of%20MARL%20training%20process%2C%0Aintegrating%20domain-specific%20knowledge%20at%20the%20action%20level.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2404.05840v3%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DAttention-Driven%2520Multi-Agent%2520Reinforcement%2520Learning%253A%2520Enhancing%2520Decisions%250A%2520%2520with%2520Expertise-Informed%2520Tasks%26entry.906535625%3DAndre%2520R%2520Kuroswiski%2520and%2520Annie%2520S%2520Wu%2520and%2520Angelo%2520Passaro%26entry.1292438233%3D%2520%2520In%2520this%2520paper%252C%2520we%2520introduce%2520an%2520alternative%2520approach%2520to%2520enhancing%2520Multi-Agent%250AReinforcement%2520Learning%2520%2528MARL%2529%2520through%2520the%2520integration%2520of%2520domain%2520knowledge%2520and%250Aattention-based%2520policy%2520mechanisms.%2520Our%2520methodology%2520focuses%2520on%2520the%2520incorporation%250Aof%2520domain-specific%2520expertise%2520into%2520the%2520learning%2520process%252C%2520which%2520simplifies%2520the%250Adevelopment%2520of%2520collaborative%2520behaviors.%2520This%2520approach%2520aims%2520to%2520reduce%2520the%250Acomplexity%2520and%2520learning%2520overhead%2520typically%2520associated%2520with%2520MARL%2520by%2520enabling%250Aagents%2520to%2520concentrate%2520on%2520essential%2520aspects%2520of%2520complex%2520tasks%252C%2520thus%2520optimizing%250Athe%2520learning%2520curve.%2520The%2520utilization%2520of%2520attention%2520mechanisms%2520plays%2520a%2520key%2520role%2520in%250Aour%2520model.%2520It%2520allows%2520for%2520the%2520effective%2520processing%2520of%2520dynamic%2520context%2520data%2520and%250Anuanced%2520agent%2520interactions%252C%2520leading%2520to%2520more%2520refined%2520decision-making.%2520Applied%2520in%250Astandard%2520MARL%2520scenarios%252C%2520such%2520as%2520the%2520Stanford%2520Intelligent%2520Systems%2520Laboratory%250A%2528SISL%2529%2520Pursuit%2520and%2520Multi-Particle%2520Environments%2520%2528MPE%2529%2520Simple%2520Spread%252C%2520our%2520method%250Ahas%2520been%2520shown%2520to%2520improve%2520both%2520learning%2520efficiency%2520and%2520the%2520effectiveness%2520of%250Acollaborative%2520behaviors.%2520The%2520results%2520indicate%2520that%2520our%2520attention-based%2520approach%250Acan%2520be%2520a%2520viable%2520approach%2520for%2520improving%2520the%2520efficiency%2520of%2520MARL%2520training%2520process%252C%250Aintegrating%2520domain-specific%2520knowledge%2520at%2520the%2520action%2520level.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2404.05840v3%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Attention-Driven%20Multi-Agent%20Reinforcement%20Learning%3A%20Enhancing%20Decisions%0A%20%20with%20Expertise-Informed%20Tasks&entry.906535625=Andre%20R%20Kuroswiski%20and%20Annie%20S%20Wu%20and%20Angelo%20Passaro&entry.1292438233=%20%20In%20this%20paper%2C%20we%20introduce%20an%20alternative%20approach%20to%20enhancing%20Multi-Agent%0AReinforcement%20Learning%20%28MARL%29%20through%20the%20integration%20of%20domain%20knowledge%20and%0Aattention-based%20policy%20mechanisms.%20Our%20methodology%20focuses%20on%20the%20incorporation%0Aof%20domain-specific%20expertise%20into%20the%20learning%20process%2C%20which%20simplifies%20the%0Adevelopment%20of%20collaborative%20behaviors.%20This%20approach%20aims%20to%20reduce%20the%0Acomplexity%20and%20learning%20overhead%20typically%20associated%20with%20MARL%20by%20enabling%0Aagents%20to%20concentrate%20on%20essential%20aspects%20of%20complex%20tasks%2C%20thus%20optimizing%0Athe%20learning%20curve.%20The%20utilization%20of%20attention%20mechanisms%20plays%20a%20key%20role%20in%0Aour%20model.%20It%20allows%20for%20the%20effective%20processing%20of%20dynamic%20context%20data%20and%0Anuanced%20agent%20interactions%2C%20leading%20to%20more%20refined%20decision-making.%20Applied%20in%0Astandard%20MARL%20scenarios%2C%20such%20as%20the%20Stanford%20Intelligent%20Systems%20Laboratory%0A%28SISL%29%20Pursuit%20and%20Multi-Particle%20Environments%20%28MPE%29%20Simple%20Spread%2C%20our%20method%0Ahas%20been%20shown%20to%20improve%20both%20learning%20efficiency%20and%20the%20effectiveness%20of%0Acollaborative%20behaviors.%20The%20results%20indicate%20that%20our%20attention-based%20approach%0Acan%20be%20a%20viable%20approach%20for%20improving%20the%20efficiency%20of%20MARL%20training%20process%2C%0Aintegrating%20domain-specific%20knowledge%20at%20the%20action%20level.%0A&entry.1838667208=http%3A//arxiv.org/abs/2404.05840v3&entry.124074799=Read"},
{"title": "Reduced storage direct tensor ring decomposition for convolutional\n  neural networks compression", "author": "Mateusz Gabor and Rafa\u0142 Zdunek", "abstract": "  Convolutional neural networks (CNNs) are among the most widely used machine\nlearning models for computer vision tasks, such as image classification. To\nimprove the efficiency of CNNs, many CNNs compressing approaches have been\ndeveloped. Low-rank methods approximate the original convolutional kernel with\na sequence of smaller convolutional kernels, which leads to reduced storage and\ntime complexities. In this study, we propose a novel low-rank CNNs compression\nmethod that is based on reduced storage direct tensor ring decomposition\n(RSDTR). The proposed method offers a higher circular mode permutation\nflexibility, and it is characterized by large parameter and FLOPS compression\nrates, while preserving a good classification accuracy of the compressed\nnetwork. The experiments, performed on the CIFAR-10 and ImageNet datasets,\nclearly demonstrate the efficiency of RSDTR in comparison to other\nstate-of-the-art CNNs compression approaches.\n", "link": "http://arxiv.org/abs/2405.10802v1", "date": "2024-05-17", "relevancy": 1.5942, "topK": [{"title": "SDXS: Real-Time One-Step Latent Diffusion Models with Image Conditions", "link": "http://arxiv.org/abs/2403.16627v1", "similarity": 0.5422}, {"title": "ToDo: Token Downsampling for Efficient Generation of High-Resolution\n  Images", "link": "http://arxiv.org/abs/2402.13573v2", "similarity": 0.5376}, {"title": "Scaling Rectified Flow Transformers for High-Resolution Image Synthesis", "link": "http://arxiv.org/abs/2403.03206v1", "similarity": 0.5052}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Reduced%20storage%20direct%20tensor%20ring%20decomposition%20for%20convolutional%0A%20%20neural%20networks%20compression&body=Title%3A%20Reduced%20storage%20direct%20tensor%20ring%20decomposition%20for%20convolutional%0A%20%20neural%20networks%20compression%0AAuthor%3A%20Mateusz%20Gabor%20and%20Rafa%C5%82%20Zdunek%0AAbstract%3A%20%20%20Convolutional%20neural%20networks%20%28CNNs%29%20are%20among%20the%20most%20widely%20used%20machine%0Alearning%20models%20for%20computer%20vision%20tasks%2C%20such%20as%20image%20classification.%20To%0Aimprove%20the%20efficiency%20of%20CNNs%2C%20many%20CNNs%20compressing%20approaches%20have%20been%0Adeveloped.%20Low-rank%20methods%20approximate%20the%20original%20convolutional%20kernel%20with%0Aa%20sequence%20of%20smaller%20convolutional%20kernels%2C%20which%20leads%20to%20reduced%20storage%20and%0Atime%20complexities.%20In%20this%20study%2C%20we%20propose%20a%20novel%20low-rank%20CNNs%20compression%0Amethod%20that%20is%20based%20on%20reduced%20storage%20direct%20tensor%20ring%20decomposition%0A%28RSDTR%29.%20The%20proposed%20method%20offers%20a%20higher%20circular%20mode%20permutation%0Aflexibility%2C%20and%20it%20is%20characterized%20by%20large%20parameter%20and%20FLOPS%20compression%0Arates%2C%20while%20preserving%20a%20good%20classification%20accuracy%20of%20the%20compressed%0Anetwork.%20The%20experiments%2C%20performed%20on%20the%20CIFAR-10%20and%20ImageNet%20datasets%2C%0Aclearly%20demonstrate%20the%20efficiency%20of%20RSDTR%20in%20comparison%20to%20other%0Astate-of-the-art%20CNNs%20compression%20approaches.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2405.10802v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DReduced%2520storage%2520direct%2520tensor%2520ring%2520decomposition%2520for%2520convolutional%250A%2520%2520neural%2520networks%2520compression%26entry.906535625%3DMateusz%2520Gabor%2520and%2520Rafa%25C5%2582%2520Zdunek%26entry.1292438233%3D%2520%2520Convolutional%2520neural%2520networks%2520%2528CNNs%2529%2520are%2520among%2520the%2520most%2520widely%2520used%2520machine%250Alearning%2520models%2520for%2520computer%2520vision%2520tasks%252C%2520such%2520as%2520image%2520classification.%2520To%250Aimprove%2520the%2520efficiency%2520of%2520CNNs%252C%2520many%2520CNNs%2520compressing%2520approaches%2520have%2520been%250Adeveloped.%2520Low-rank%2520methods%2520approximate%2520the%2520original%2520convolutional%2520kernel%2520with%250Aa%2520sequence%2520of%2520smaller%2520convolutional%2520kernels%252C%2520which%2520leads%2520to%2520reduced%2520storage%2520and%250Atime%2520complexities.%2520In%2520this%2520study%252C%2520we%2520propose%2520a%2520novel%2520low-rank%2520CNNs%2520compression%250Amethod%2520that%2520is%2520based%2520on%2520reduced%2520storage%2520direct%2520tensor%2520ring%2520decomposition%250A%2528RSDTR%2529.%2520The%2520proposed%2520method%2520offers%2520a%2520higher%2520circular%2520mode%2520permutation%250Aflexibility%252C%2520and%2520it%2520is%2520characterized%2520by%2520large%2520parameter%2520and%2520FLOPS%2520compression%250Arates%252C%2520while%2520preserving%2520a%2520good%2520classification%2520accuracy%2520of%2520the%2520compressed%250Anetwork.%2520The%2520experiments%252C%2520performed%2520on%2520the%2520CIFAR-10%2520and%2520ImageNet%2520datasets%252C%250Aclearly%2520demonstrate%2520the%2520efficiency%2520of%2520RSDTR%2520in%2520comparison%2520to%2520other%250Astate-of-the-art%2520CNNs%2520compression%2520approaches.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2405.10802v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Reduced%20storage%20direct%20tensor%20ring%20decomposition%20for%20convolutional%0A%20%20neural%20networks%20compression&entry.906535625=Mateusz%20Gabor%20and%20Rafa%C5%82%20Zdunek&entry.1292438233=%20%20Convolutional%20neural%20networks%20%28CNNs%29%20are%20among%20the%20most%20widely%20used%20machine%0Alearning%20models%20for%20computer%20vision%20tasks%2C%20such%20as%20image%20classification.%20To%0Aimprove%20the%20efficiency%20of%20CNNs%2C%20many%20CNNs%20compressing%20approaches%20have%20been%0Adeveloped.%20Low-rank%20methods%20approximate%20the%20original%20convolutional%20kernel%20with%0Aa%20sequence%20of%20smaller%20convolutional%20kernels%2C%20which%20leads%20to%20reduced%20storage%20and%0Atime%20complexities.%20In%20this%20study%2C%20we%20propose%20a%20novel%20low-rank%20CNNs%20compression%0Amethod%20that%20is%20based%20on%20reduced%20storage%20direct%20tensor%20ring%20decomposition%0A%28RSDTR%29.%20The%20proposed%20method%20offers%20a%20higher%20circular%20mode%20permutation%0Aflexibility%2C%20and%20it%20is%20characterized%20by%20large%20parameter%20and%20FLOPS%20compression%0Arates%2C%20while%20preserving%20a%20good%20classification%20accuracy%20of%20the%20compressed%0Anetwork.%20The%20experiments%2C%20performed%20on%20the%20CIFAR-10%20and%20ImageNet%20datasets%2C%0Aclearly%20demonstrate%20the%20efficiency%20of%20RSDTR%20in%20comparison%20to%20other%0Astate-of-the-art%20CNNs%20compression%20approaches.%0A&entry.1838667208=http%3A//arxiv.org/abs/2405.10802v1&entry.124074799=Read"},
{"title": "Empowering Prior to Court Legal Analysis: A Transparent and Accessible\n  Dataset for Defensive Statement Classification and Interpretation", "author": "Yannis Spyridis and  Jean-Paul and Haneen Deeb and Vasileios Argyriou", "abstract": "  The classification of statements provided by individuals during police\ninterviews is a complex and significant task within the domain of natural\nlanguage processing (NLP) and legal informatics. The lack of extensive\ndomain-specific datasets raises challenges to the advancement of NLP methods in\nthe field. This paper aims to address some of the present challenges by\nintroducing a novel dataset tailored for classification of statements made\nduring police interviews, prior to court proceedings. Utilising the curated\ndataset for training and evaluation, we introduce a fine-tuned DistilBERT model\nthat achieves state-of-the-art performance in distinguishing truthful from\ndeceptive statements. To enhance interpretability, we employ explainable\nartificial intelligence (XAI) methods to offer explainability through saliency\nmaps, that interpret the model's decision-making process. Lastly, we present an\nXAI interface that empowers both legal professionals and non-specialists to\ninteract with and benefit from our system. Our model achieves an accuracy of\n86%, and is shown to outperform a custom transformer architecture in a\ncomparative study. This holistic approach advances the accessibility,\ntransparency, and effectiveness of statement analysis, with promising\nimplications for both legal practice and research.\n", "link": "http://arxiv.org/abs/2405.10702v1", "date": "2024-05-17", "relevancy": 1.419, "topK": [{"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.4969}, {"title": "DressCode: Autoregressively Sewing and Generating Garments from Text\n  Guidance", "link": "http://arxiv.org/abs/2401.16465v3", "similarity": 0.4665}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.4653}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Empowering%20Prior%20to%20Court%20Legal%20Analysis%3A%20A%20Transparent%20and%20Accessible%0A%20%20Dataset%20for%20Defensive%20Statement%20Classification%20and%20Interpretation&body=Title%3A%20Empowering%20Prior%20to%20Court%20Legal%20Analysis%3A%20A%20Transparent%20and%20Accessible%0A%20%20Dataset%20for%20Defensive%20Statement%20Classification%20and%20Interpretation%0AAuthor%3A%20Yannis%20Spyridis%20and%20%20Jean-Paul%20and%20Haneen%20Deeb%20and%20Vasileios%20Argyriou%0AAbstract%3A%20%20%20The%20classification%20of%20statements%20provided%20by%20individuals%20during%20police%0Ainterviews%20is%20a%20complex%20and%20significant%20task%20within%20the%20domain%20of%20natural%0Alanguage%20processing%20%28NLP%29%20and%20legal%20informatics.%20The%20lack%20of%20extensive%0Adomain-specific%20datasets%20raises%20challenges%20to%20the%20advancement%20of%20NLP%20methods%20in%0Athe%20field.%20This%20paper%20aims%20to%20address%20some%20of%20the%20present%20challenges%20by%0Aintroducing%20a%20novel%20dataset%20tailored%20for%20classification%20of%20statements%20made%0Aduring%20police%20interviews%2C%20prior%20to%20court%20proceedings.%20Utilising%20the%20curated%0Adataset%20for%20training%20and%20evaluation%2C%20we%20introduce%20a%20fine-tuned%20DistilBERT%20model%0Athat%20achieves%20state-of-the-art%20performance%20in%20distinguishing%20truthful%20from%0Adeceptive%20statements.%20To%20enhance%20interpretability%2C%20we%20employ%20explainable%0Aartificial%20intelligence%20%28XAI%29%20methods%20to%20offer%20explainability%20through%20saliency%0Amaps%2C%20that%20interpret%20the%20model%27s%20decision-making%20process.%20Lastly%2C%20we%20present%20an%0AXAI%20interface%20that%20empowers%20both%20legal%20professionals%20and%20non-specialists%20to%0Ainteract%20with%20and%20benefit%20from%20our%20system.%20Our%20model%20achieves%20an%20accuracy%20of%0A86%25%2C%20and%20is%20shown%20to%20outperform%20a%20custom%20transformer%20architecture%20in%20a%0Acomparative%20study.%20This%20holistic%20approach%20advances%20the%20accessibility%2C%0Atransparency%2C%20and%20effectiveness%20of%20statement%20analysis%2C%20with%20promising%0Aimplications%20for%20both%20legal%20practice%20and%20research.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2405.10702v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DEmpowering%2520Prior%2520to%2520Court%2520Legal%2520Analysis%253A%2520A%2520Transparent%2520and%2520Accessible%250A%2520%2520Dataset%2520for%2520Defensive%2520Statement%2520Classification%2520and%2520Interpretation%26entry.906535625%3DYannis%2520Spyridis%2520and%2520%2520Jean-Paul%2520and%2520Haneen%2520Deeb%2520and%2520Vasileios%2520Argyriou%26entry.1292438233%3D%2520%2520The%2520classification%2520of%2520statements%2520provided%2520by%2520individuals%2520during%2520police%250Ainterviews%2520is%2520a%2520complex%2520and%2520significant%2520task%2520within%2520the%2520domain%2520of%2520natural%250Alanguage%2520processing%2520%2528NLP%2529%2520and%2520legal%2520informatics.%2520The%2520lack%2520of%2520extensive%250Adomain-specific%2520datasets%2520raises%2520challenges%2520to%2520the%2520advancement%2520of%2520NLP%2520methods%2520in%250Athe%2520field.%2520This%2520paper%2520aims%2520to%2520address%2520some%2520of%2520the%2520present%2520challenges%2520by%250Aintroducing%2520a%2520novel%2520dataset%2520tailored%2520for%2520classification%2520of%2520statements%2520made%250Aduring%2520police%2520interviews%252C%2520prior%2520to%2520court%2520proceedings.%2520Utilising%2520the%2520curated%250Adataset%2520for%2520training%2520and%2520evaluation%252C%2520we%2520introduce%2520a%2520fine-tuned%2520DistilBERT%2520model%250Athat%2520achieves%2520state-of-the-art%2520performance%2520in%2520distinguishing%2520truthful%2520from%250Adeceptive%2520statements.%2520To%2520enhance%2520interpretability%252C%2520we%2520employ%2520explainable%250Aartificial%2520intelligence%2520%2528XAI%2529%2520methods%2520to%2520offer%2520explainability%2520through%2520saliency%250Amaps%252C%2520that%2520interpret%2520the%2520model%2527s%2520decision-making%2520process.%2520Lastly%252C%2520we%2520present%2520an%250AXAI%2520interface%2520that%2520empowers%2520both%2520legal%2520professionals%2520and%2520non-specialists%2520to%250Ainteract%2520with%2520and%2520benefit%2520from%2520our%2520system.%2520Our%2520model%2520achieves%2520an%2520accuracy%2520of%250A86%2525%252C%2520and%2520is%2520shown%2520to%2520outperform%2520a%2520custom%2520transformer%2520architecture%2520in%2520a%250Acomparative%2520study.%2520This%2520holistic%2520approach%2520advances%2520the%2520accessibility%252C%250Atransparency%252C%2520and%2520effectiveness%2520of%2520statement%2520analysis%252C%2520with%2520promising%250Aimplications%2520for%2520both%2520legal%2520practice%2520and%2520research.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2405.10702v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Empowering%20Prior%20to%20Court%20Legal%20Analysis%3A%20A%20Transparent%20and%20Accessible%0A%20%20Dataset%20for%20Defensive%20Statement%20Classification%20and%20Interpretation&entry.906535625=Yannis%20Spyridis%20and%20%20Jean-Paul%20and%20Haneen%20Deeb%20and%20Vasileios%20Argyriou&entry.1292438233=%20%20The%20classification%20of%20statements%20provided%20by%20individuals%20during%20police%0Ainterviews%20is%20a%20complex%20and%20significant%20task%20within%20the%20domain%20of%20natural%0Alanguage%20processing%20%28NLP%29%20and%20legal%20informatics.%20The%20lack%20of%20extensive%0Adomain-specific%20datasets%20raises%20challenges%20to%20the%20advancement%20of%20NLP%20methods%20in%0Athe%20field.%20This%20paper%20aims%20to%20address%20some%20of%20the%20present%20challenges%20by%0Aintroducing%20a%20novel%20dataset%20tailored%20for%20classification%20of%20statements%20made%0Aduring%20police%20interviews%2C%20prior%20to%20court%20proceedings.%20Utilising%20the%20curated%0Adataset%20for%20training%20and%20evaluation%2C%20we%20introduce%20a%20fine-tuned%20DistilBERT%20model%0Athat%20achieves%20state-of-the-art%20performance%20in%20distinguishing%20truthful%20from%0Adeceptive%20statements.%20To%20enhance%20interpretability%2C%20we%20employ%20explainable%0Aartificial%20intelligence%20%28XAI%29%20methods%20to%20offer%20explainability%20through%20saliency%0Amaps%2C%20that%20interpret%20the%20model%27s%20decision-making%20process.%20Lastly%2C%20we%20present%20an%0AXAI%20interface%20that%20empowers%20both%20legal%20professionals%20and%20non-specialists%20to%0Ainteract%20with%20and%20benefit%20from%20our%20system.%20Our%20model%20achieves%20an%20accuracy%20of%0A86%25%2C%20and%20is%20shown%20to%20outperform%20a%20custom%20transformer%20architecture%20in%20a%0Acomparative%20study.%20This%20holistic%20approach%20advances%20the%20accessibility%2C%0Atransparency%2C%20and%20effectiveness%20of%20statement%20analysis%2C%20with%20promising%0Aimplications%20for%20both%20legal%20practice%20and%20research.%0A&entry.1838667208=http%3A//arxiv.org/abs/2405.10702v1&entry.124074799=Read"},
{"title": "Identifiability of total effects from abstractions of time series causal\n  graphs", "author": "Charles K. Assaad and Emilie Devijver and Eric Gaussier and Gregor G\u00f6ssler and Anouar Meynaoui", "abstract": "  We study the problem of identifiability of the total effect of an\nintervention from observational time series in the situation, common in\npractice, where one only has access to abstractions of the true causal graph.\nWe consider here two abstractions: the extended summary causal graph, which\nconflates all lagged causal relations but distinguishes between lagged and\ninstantaneous relations, and the summary causal graph which does not give any\nindication about the lag between causal relations. We show that the total\neffect is always identifiable in extended summary causal graphs and provide\nsufficient conditions for identifiability in summary causal graphs. We\nfurthermore provide adjustment sets allowing to estimate the total effect\nwhenever it is identifiable.\n", "link": "http://arxiv.org/abs/2310.14691v4", "date": "2024-05-17", "relevancy": 1.0242, "topK": [{"title": "RoboEXP: Action-Conditioned Scene Graph via Interactive Exploration for\n  Robotic Manipulation", "link": "http://arxiv.org/abs/2402.15487v1", "similarity": 0.3668}, {"title": "Flexible Visual Recognition by Evidential Modeling of Confusion and Ignorance", "link": "http://openaccess.thecvf.com/content/ICCV2023/html/Fan_Flexible_Visual_Recognition_by_Evidential_Modeling_of_Confusion_and_Ignorance_ICCV_2023_paper.html", "similarity": 0.3357}, {"title": "Multimodal-Conditioned Latent Diffusion Models for Fashion Image Editing", "link": "http://arxiv.org/abs/2403.14828v2", "similarity": 0.3304}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Identifiability%20of%20total%20effects%20from%20abstractions%20of%20time%20series%20causal%0A%20%20graphs&body=Title%3A%20Identifiability%20of%20total%20effects%20from%20abstractions%20of%20time%20series%20causal%0A%20%20graphs%0AAuthor%3A%20Charles%20K.%20Assaad%20and%20Emilie%20Devijver%20and%20Eric%20Gaussier%20and%20Gregor%20G%C3%B6ssler%20and%20Anouar%20Meynaoui%0AAbstract%3A%20%20%20We%20study%20the%20problem%20of%20identifiability%20of%20the%20total%20effect%20of%20an%0Aintervention%20from%20observational%20time%20series%20in%20the%20situation%2C%20common%20in%0Apractice%2C%20where%20one%20only%20has%20access%20to%20abstractions%20of%20the%20true%20causal%20graph.%0AWe%20consider%20here%20two%20abstractions%3A%20the%20extended%20summary%20causal%20graph%2C%20which%0Aconflates%20all%20lagged%20causal%20relations%20but%20distinguishes%20between%20lagged%20and%0Ainstantaneous%20relations%2C%20and%20the%20summary%20causal%20graph%20which%20does%20not%20give%20any%0Aindication%20about%20the%20lag%20between%20causal%20relations.%20We%20show%20that%20the%20total%0Aeffect%20is%20always%20identifiable%20in%20extended%20summary%20causal%20graphs%20and%20provide%0Asufficient%20conditions%20for%20identifiability%20in%20summary%20causal%20graphs.%20We%0Afurthermore%20provide%20adjustment%20sets%20allowing%20to%20estimate%20the%20total%20effect%0Awhenever%20it%20is%20identifiable.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2310.14691v4%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DIdentifiability%2520of%2520total%2520effects%2520from%2520abstractions%2520of%2520time%2520series%2520causal%250A%2520%2520graphs%26entry.906535625%3DCharles%2520K.%2520Assaad%2520and%2520Emilie%2520Devijver%2520and%2520Eric%2520Gaussier%2520and%2520Gregor%2520G%25C3%25B6ssler%2520and%2520Anouar%2520Meynaoui%26entry.1292438233%3D%2520%2520We%2520study%2520the%2520problem%2520of%2520identifiability%2520of%2520the%2520total%2520effect%2520of%2520an%250Aintervention%2520from%2520observational%2520time%2520series%2520in%2520the%2520situation%252C%2520common%2520in%250Apractice%252C%2520where%2520one%2520only%2520has%2520access%2520to%2520abstractions%2520of%2520the%2520true%2520causal%2520graph.%250AWe%2520consider%2520here%2520two%2520abstractions%253A%2520the%2520extended%2520summary%2520causal%2520graph%252C%2520which%250Aconflates%2520all%2520lagged%2520causal%2520relations%2520but%2520distinguishes%2520between%2520lagged%2520and%250Ainstantaneous%2520relations%252C%2520and%2520the%2520summary%2520causal%2520graph%2520which%2520does%2520not%2520give%2520any%250Aindication%2520about%2520the%2520lag%2520between%2520causal%2520relations.%2520We%2520show%2520that%2520the%2520total%250Aeffect%2520is%2520always%2520identifiable%2520in%2520extended%2520summary%2520causal%2520graphs%2520and%2520provide%250Asufficient%2520conditions%2520for%2520identifiability%2520in%2520summary%2520causal%2520graphs.%2520We%250Afurthermore%2520provide%2520adjustment%2520sets%2520allowing%2520to%2520estimate%2520the%2520total%2520effect%250Awhenever%2520it%2520is%2520identifiable.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2310.14691v4%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Identifiability%20of%20total%20effects%20from%20abstractions%20of%20time%20series%20causal%0A%20%20graphs&entry.906535625=Charles%20K.%20Assaad%20and%20Emilie%20Devijver%20and%20Eric%20Gaussier%20and%20Gregor%20G%C3%B6ssler%20and%20Anouar%20Meynaoui&entry.1292438233=%20%20We%20study%20the%20problem%20of%20identifiability%20of%20the%20total%20effect%20of%20an%0Aintervention%20from%20observational%20time%20series%20in%20the%20situation%2C%20common%20in%0Apractice%2C%20where%20one%20only%20has%20access%20to%20abstractions%20of%20the%20true%20causal%20graph.%0AWe%20consider%20here%20two%20abstractions%3A%20the%20extended%20summary%20causal%20graph%2C%20which%0Aconflates%20all%20lagged%20causal%20relations%20but%20distinguishes%20between%20lagged%20and%0Ainstantaneous%20relations%2C%20and%20the%20summary%20causal%20graph%20which%20does%20not%20give%20any%0Aindication%20about%20the%20lag%20between%20causal%20relations.%20We%20show%20that%20the%20total%0Aeffect%20is%20always%20identifiable%20in%20extended%20summary%20causal%20graphs%20and%20provide%0Asufficient%20conditions%20for%20identifiability%20in%20summary%20causal%20graphs.%20We%0Afurthermore%20provide%20adjustment%20sets%20allowing%20to%20estimate%20the%20total%20effect%0Awhenever%20it%20is%20identifiable.%0A&entry.1838667208=http%3A//arxiv.org/abs/2310.14691v4&entry.124074799=Read"},
{"title": "The Local Interaction Basis: Identifying Computationally-Relevant and\n  Sparsely Interacting Features in Neural Networks", "author": "Lucius Bushnaq and Stefan Heimersheim Nicholas Goldowsky-Dill and Dan Braun and Jake Mendel and Kaarel H\u00e4nni and Avery Griffin and J\u00f6rn St\u00f6hler and Magdalena Wache and Marius Hobbhahn", "abstract": "  Mechanistic interpretability aims to understand the behavior of neural\nnetworks by reverse-engineering their internal computations. However, current\nmethods struggle to find clear interpretations of neural network activations\nbecause a decomposition of activations into computational features is missing.\nIndividual neurons or model components do not cleanly correspond to distinct\nfeatures or functions. We present a novel interpretability method that aims to\novercome this limitation by transforming the activations of the network into a\nnew basis - the Local Interaction Basis (LIB). LIB aims to identify\ncomputational features by removing irrelevant activations and interactions. Our\nmethod drops irrelevant activation directions and aligns the basis with the\nsingular vectors of the Jacobian matrix between adjacent layers. It also scales\nfeatures based on their importance for downstream computation, producing an\ninteraction graph that shows all computationally-relevant features and\ninteractions in a model. We evaluate the effectiveness of LIB on modular\naddition and CIFAR-10 models, finding that it identifies more\ncomputationally-relevant features that interact more sparsely, compared to\nprincipal component analysis. However, LIB does not yield substantial\nimprovements in interpretability or interaction sparsity when applied to\nlanguage models. We conclude that LIB is a promising theory-driven approach for\nanalyzing neural networks, but in its current form is not applicable to large\nlanguage models.\n", "link": "http://arxiv.org/abs/2405.10928v1", "date": "2024-05-17", "relevancy": 1.4201, "topK": [{"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.4776}, {"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.4738}, {"title": "SDXS: Real-Time One-Step Latent Diffusion Models with Image Conditions", "link": "http://arxiv.org/abs/2403.16627v1", "similarity": 0.4681}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20The%20Local%20Interaction%20Basis%3A%20Identifying%20Computationally-Relevant%20and%0A%20%20Sparsely%20Interacting%20Features%20in%20Neural%20Networks&body=Title%3A%20The%20Local%20Interaction%20Basis%3A%20Identifying%20Computationally-Relevant%20and%0A%20%20Sparsely%20Interacting%20Features%20in%20Neural%20Networks%0AAuthor%3A%20Lucius%20Bushnaq%20and%20Stefan%20Heimersheim%20Nicholas%20Goldowsky-Dill%20and%20Dan%20Braun%20and%20Jake%20Mendel%20and%20Kaarel%20H%C3%A4nni%20and%20Avery%20Griffin%20and%20J%C3%B6rn%20St%C3%B6hler%20and%20Magdalena%20Wache%20and%20Marius%20Hobbhahn%0AAbstract%3A%20%20%20Mechanistic%20interpretability%20aims%20to%20understand%20the%20behavior%20of%20neural%0Anetworks%20by%20reverse-engineering%20their%20internal%20computations.%20However%2C%20current%0Amethods%20struggle%20to%20find%20clear%20interpretations%20of%20neural%20network%20activations%0Abecause%20a%20decomposition%20of%20activations%20into%20computational%20features%20is%20missing.%0AIndividual%20neurons%20or%20model%20components%20do%20not%20cleanly%20correspond%20to%20distinct%0Afeatures%20or%20functions.%20We%20present%20a%20novel%20interpretability%20method%20that%20aims%20to%0Aovercome%20this%20limitation%20by%20transforming%20the%20activations%20of%20the%20network%20into%20a%0Anew%20basis%20-%20the%20Local%20Interaction%20Basis%20%28LIB%29.%20LIB%20aims%20to%20identify%0Acomputational%20features%20by%20removing%20irrelevant%20activations%20and%20interactions.%20Our%0Amethod%20drops%20irrelevant%20activation%20directions%20and%20aligns%20the%20basis%20with%20the%0Asingular%20vectors%20of%20the%20Jacobian%20matrix%20between%20adjacent%20layers.%20It%20also%20scales%0Afeatures%20based%20on%20their%20importance%20for%20downstream%20computation%2C%20producing%20an%0Ainteraction%20graph%20that%20shows%20all%20computationally-relevant%20features%20and%0Ainteractions%20in%20a%20model.%20We%20evaluate%20the%20effectiveness%20of%20LIB%20on%20modular%0Aaddition%20and%20CIFAR-10%20models%2C%20finding%20that%20it%20identifies%20more%0Acomputationally-relevant%20features%20that%20interact%20more%20sparsely%2C%20compared%20to%0Aprincipal%20component%20analysis.%20However%2C%20LIB%20does%20not%20yield%20substantial%0Aimprovements%20in%20interpretability%20or%20interaction%20sparsity%20when%20applied%20to%0Alanguage%20models.%20We%20conclude%20that%20LIB%20is%20a%20promising%20theory-driven%20approach%20for%0Aanalyzing%20neural%20networks%2C%20but%20in%20its%20current%20form%20is%20not%20applicable%20to%20large%0Alanguage%20models.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2405.10928v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DThe%2520Local%2520Interaction%2520Basis%253A%2520Identifying%2520Computationally-Relevant%2520and%250A%2520%2520Sparsely%2520Interacting%2520Features%2520in%2520Neural%2520Networks%26entry.906535625%3DLucius%2520Bushnaq%2520and%2520Stefan%2520Heimersheim%2520Nicholas%2520Goldowsky-Dill%2520and%2520Dan%2520Braun%2520and%2520Jake%2520Mendel%2520and%2520Kaarel%2520H%25C3%25A4nni%2520and%2520Avery%2520Griffin%2520and%2520J%25C3%25B6rn%2520St%25C3%25B6hler%2520and%2520Magdalena%2520Wache%2520and%2520Marius%2520Hobbhahn%26entry.1292438233%3D%2520%2520Mechanistic%2520interpretability%2520aims%2520to%2520understand%2520the%2520behavior%2520of%2520neural%250Anetworks%2520by%2520reverse-engineering%2520their%2520internal%2520computations.%2520However%252C%2520current%250Amethods%2520struggle%2520to%2520find%2520clear%2520interpretations%2520of%2520neural%2520network%2520activations%250Abecause%2520a%2520decomposition%2520of%2520activations%2520into%2520computational%2520features%2520is%2520missing.%250AIndividual%2520neurons%2520or%2520model%2520components%2520do%2520not%2520cleanly%2520correspond%2520to%2520distinct%250Afeatures%2520or%2520functions.%2520We%2520present%2520a%2520novel%2520interpretability%2520method%2520that%2520aims%2520to%250Aovercome%2520this%2520limitation%2520by%2520transforming%2520the%2520activations%2520of%2520the%2520network%2520into%2520a%250Anew%2520basis%2520-%2520the%2520Local%2520Interaction%2520Basis%2520%2528LIB%2529.%2520LIB%2520aims%2520to%2520identify%250Acomputational%2520features%2520by%2520removing%2520irrelevant%2520activations%2520and%2520interactions.%2520Our%250Amethod%2520drops%2520irrelevant%2520activation%2520directions%2520and%2520aligns%2520the%2520basis%2520with%2520the%250Asingular%2520vectors%2520of%2520the%2520Jacobian%2520matrix%2520between%2520adjacent%2520layers.%2520It%2520also%2520scales%250Afeatures%2520based%2520on%2520their%2520importance%2520for%2520downstream%2520computation%252C%2520producing%2520an%250Ainteraction%2520graph%2520that%2520shows%2520all%2520computationally-relevant%2520features%2520and%250Ainteractions%2520in%2520a%2520model.%2520We%2520evaluate%2520the%2520effectiveness%2520of%2520LIB%2520on%2520modular%250Aaddition%2520and%2520CIFAR-10%2520models%252C%2520finding%2520that%2520it%2520identifies%2520more%250Acomputationally-relevant%2520features%2520that%2520interact%2520more%2520sparsely%252C%2520compared%2520to%250Aprincipal%2520component%2520analysis.%2520However%252C%2520LIB%2520does%2520not%2520yield%2520substantial%250Aimprovements%2520in%2520interpretability%2520or%2520interaction%2520sparsity%2520when%2520applied%2520to%250Alanguage%2520models.%2520We%2520conclude%2520that%2520LIB%2520is%2520a%2520promising%2520theory-driven%2520approach%2520for%250Aanalyzing%2520neural%2520networks%252C%2520but%2520in%2520its%2520current%2520form%2520is%2520not%2520applicable%2520to%2520large%250Alanguage%2520models.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2405.10928v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=The%20Local%20Interaction%20Basis%3A%20Identifying%20Computationally-Relevant%20and%0A%20%20Sparsely%20Interacting%20Features%20in%20Neural%20Networks&entry.906535625=Lucius%20Bushnaq%20and%20Stefan%20Heimersheim%20Nicholas%20Goldowsky-Dill%20and%20Dan%20Braun%20and%20Jake%20Mendel%20and%20Kaarel%20H%C3%A4nni%20and%20Avery%20Griffin%20and%20J%C3%B6rn%20St%C3%B6hler%20and%20Magdalena%20Wache%20and%20Marius%20Hobbhahn&entry.1292438233=%20%20Mechanistic%20interpretability%20aims%20to%20understand%20the%20behavior%20of%20neural%0Anetworks%20by%20reverse-engineering%20their%20internal%20computations.%20However%2C%20current%0Amethods%20struggle%20to%20find%20clear%20interpretations%20of%20neural%20network%20activations%0Abecause%20a%20decomposition%20of%20activations%20into%20computational%20features%20is%20missing.%0AIndividual%20neurons%20or%20model%20components%20do%20not%20cleanly%20correspond%20to%20distinct%0Afeatures%20or%20functions.%20We%20present%20a%20novel%20interpretability%20method%20that%20aims%20to%0Aovercome%20this%20limitation%20by%20transforming%20the%20activations%20of%20the%20network%20into%20a%0Anew%20basis%20-%20the%20Local%20Interaction%20Basis%20%28LIB%29.%20LIB%20aims%20to%20identify%0Acomputational%20features%20by%20removing%20irrelevant%20activations%20and%20interactions.%20Our%0Amethod%20drops%20irrelevant%20activation%20directions%20and%20aligns%20the%20basis%20with%20the%0Asingular%20vectors%20of%20the%20Jacobian%20matrix%20between%20adjacent%20layers.%20It%20also%20scales%0Afeatures%20based%20on%20their%20importance%20for%20downstream%20computation%2C%20producing%20an%0Ainteraction%20graph%20that%20shows%20all%20computationally-relevant%20features%20and%0Ainteractions%20in%20a%20model.%20We%20evaluate%20the%20effectiveness%20of%20LIB%20on%20modular%0Aaddition%20and%20CIFAR-10%20models%2C%20finding%20that%20it%20identifies%20more%0Acomputationally-relevant%20features%20that%20interact%20more%20sparsely%2C%20compared%20to%0Aprincipal%20component%20analysis.%20However%2C%20LIB%20does%20not%20yield%20substantial%0Aimprovements%20in%20interpretability%20or%20interaction%20sparsity%20when%20applied%20to%0Alanguage%20models.%20We%20conclude%20that%20LIB%20is%20a%20promising%20theory-driven%20approach%20for%0Aanalyzing%20neural%20networks%2C%20but%20in%20its%20current%20form%20is%20not%20applicable%20to%20large%0Alanguage%20models.%0A&entry.1838667208=http%3A//arxiv.org/abs/2405.10928v1&entry.124074799=Read"},
{"title": "Towards Understanding the Word Sensitivity of Attention Layers: A Study\n  via Random Features", "author": "Simone Bombari and Marco Mondelli", "abstract": "  Understanding the reasons behind the exceptional success of transformers\nrequires a better analysis of why attention layers are suitable for NLP tasks.\nIn particular, such tasks require predictive models to capture contextual\nmeaning which often depends on one or few words, even if the sentence is long.\nOur work studies this key property, dubbed word sensitivity (WS), in the\nprototypical setting of random features. We show that attention layers enjoy\nhigh WS, namely, there exists a vector in the space of embeddings that largely\nperturbs the random attention features map. The argument critically exploits\nthe role of the softmax in the attention layer, highlighting its benefit\ncompared to other activations (e.g., ReLU). In contrast, the WS of standard\nrandom features is of order $1/\\sqrt{n}$, $n$ being the number of words in the\ntextual sample, and thus it decays with the length of the context. We then\ntranslate these results on the word sensitivity into generalization bounds: due\nto their low WS, random features provably cannot learn to distinguish between\ntwo sentences that differ only in a single word; in contrast, due to their high\nWS, random attention features have higher generalization capabilities. We\nvalidate our theoretical results with experimental evidence over the BERT-Base\nword embeddings of the imdb review dataset.\n", "link": "http://arxiv.org/abs/2402.02969v2", "date": "2024-05-17", "relevancy": 1.4225, "topK": [{"title": "Scaling Rectified Flow Transformers for High-Resolution Image Synthesis", "link": "http://arxiv.org/abs/2403.03206v1", "similarity": 0.5077}, {"title": "ToDo: Token Downsampling for Efficient Generation of High-Resolution\n  Images", "link": "http://arxiv.org/abs/2402.13573v2", "similarity": 0.4693}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.4529}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Towards%20Understanding%20the%20Word%20Sensitivity%20of%20Attention%20Layers%3A%20A%20Study%0A%20%20via%20Random%20Features&body=Title%3A%20Towards%20Understanding%20the%20Word%20Sensitivity%20of%20Attention%20Layers%3A%20A%20Study%0A%20%20via%20Random%20Features%0AAuthor%3A%20Simone%20Bombari%20and%20Marco%20Mondelli%0AAbstract%3A%20%20%20Understanding%20the%20reasons%20behind%20the%20exceptional%20success%20of%20transformers%0Arequires%20a%20better%20analysis%20of%20why%20attention%20layers%20are%20suitable%20for%20NLP%20tasks.%0AIn%20particular%2C%20such%20tasks%20require%20predictive%20models%20to%20capture%20contextual%0Ameaning%20which%20often%20depends%20on%20one%20or%20few%20words%2C%20even%20if%20the%20sentence%20is%20long.%0AOur%20work%20studies%20this%20key%20property%2C%20dubbed%20word%20sensitivity%20%28WS%29%2C%20in%20the%0Aprototypical%20setting%20of%20random%20features.%20We%20show%20that%20attention%20layers%20enjoy%0Ahigh%20WS%2C%20namely%2C%20there%20exists%20a%20vector%20in%20the%20space%20of%20embeddings%20that%20largely%0Aperturbs%20the%20random%20attention%20features%20map.%20The%20argument%20critically%20exploits%0Athe%20role%20of%20the%20softmax%20in%20the%20attention%20layer%2C%20highlighting%20its%20benefit%0Acompared%20to%20other%20activations%20%28e.g.%2C%20ReLU%29.%20In%20contrast%2C%20the%20WS%20of%20standard%0Arandom%20features%20is%20of%20order%20%241/%5Csqrt%7Bn%7D%24%2C%20%24n%24%20being%20the%20number%20of%20words%20in%20the%0Atextual%20sample%2C%20and%20thus%20it%20decays%20with%20the%20length%20of%20the%20context.%20We%20then%0Atranslate%20these%20results%20on%20the%20word%20sensitivity%20into%20generalization%20bounds%3A%20due%0Ato%20their%20low%20WS%2C%20random%20features%20provably%20cannot%20learn%20to%20distinguish%20between%0Atwo%20sentences%20that%20differ%20only%20in%20a%20single%20word%3B%20in%20contrast%2C%20due%20to%20their%20high%0AWS%2C%20random%20attention%20features%20have%20higher%20generalization%20capabilities.%20We%0Avalidate%20our%20theoretical%20results%20with%20experimental%20evidence%20over%20the%20BERT-Base%0Aword%20embeddings%20of%20the%20imdb%20review%20dataset.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2402.02969v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DTowards%2520Understanding%2520the%2520Word%2520Sensitivity%2520of%2520Attention%2520Layers%253A%2520A%2520Study%250A%2520%2520via%2520Random%2520Features%26entry.906535625%3DSimone%2520Bombari%2520and%2520Marco%2520Mondelli%26entry.1292438233%3D%2520%2520Understanding%2520the%2520reasons%2520behind%2520the%2520exceptional%2520success%2520of%2520transformers%250Arequires%2520a%2520better%2520analysis%2520of%2520why%2520attention%2520layers%2520are%2520suitable%2520for%2520NLP%2520tasks.%250AIn%2520particular%252C%2520such%2520tasks%2520require%2520predictive%2520models%2520to%2520capture%2520contextual%250Ameaning%2520which%2520often%2520depends%2520on%2520one%2520or%2520few%2520words%252C%2520even%2520if%2520the%2520sentence%2520is%2520long.%250AOur%2520work%2520studies%2520this%2520key%2520property%252C%2520dubbed%2520word%2520sensitivity%2520%2528WS%2529%252C%2520in%2520the%250Aprototypical%2520setting%2520of%2520random%2520features.%2520We%2520show%2520that%2520attention%2520layers%2520enjoy%250Ahigh%2520WS%252C%2520namely%252C%2520there%2520exists%2520a%2520vector%2520in%2520the%2520space%2520of%2520embeddings%2520that%2520largely%250Aperturbs%2520the%2520random%2520attention%2520features%2520map.%2520The%2520argument%2520critically%2520exploits%250Athe%2520role%2520of%2520the%2520softmax%2520in%2520the%2520attention%2520layer%252C%2520highlighting%2520its%2520benefit%250Acompared%2520to%2520other%2520activations%2520%2528e.g.%252C%2520ReLU%2529.%2520In%2520contrast%252C%2520the%2520WS%2520of%2520standard%250Arandom%2520features%2520is%2520of%2520order%2520%25241/%255Csqrt%257Bn%257D%2524%252C%2520%2524n%2524%2520being%2520the%2520number%2520of%2520words%2520in%2520the%250Atextual%2520sample%252C%2520and%2520thus%2520it%2520decays%2520with%2520the%2520length%2520of%2520the%2520context.%2520We%2520then%250Atranslate%2520these%2520results%2520on%2520the%2520word%2520sensitivity%2520into%2520generalization%2520bounds%253A%2520due%250Ato%2520their%2520low%2520WS%252C%2520random%2520features%2520provably%2520cannot%2520learn%2520to%2520distinguish%2520between%250Atwo%2520sentences%2520that%2520differ%2520only%2520in%2520a%2520single%2520word%253B%2520in%2520contrast%252C%2520due%2520to%2520their%2520high%250AWS%252C%2520random%2520attention%2520features%2520have%2520higher%2520generalization%2520capabilities.%2520We%250Avalidate%2520our%2520theoretical%2520results%2520with%2520experimental%2520evidence%2520over%2520the%2520BERT-Base%250Aword%2520embeddings%2520of%2520the%2520imdb%2520review%2520dataset.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2402.02969v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Towards%20Understanding%20the%20Word%20Sensitivity%20of%20Attention%20Layers%3A%20A%20Study%0A%20%20via%20Random%20Features&entry.906535625=Simone%20Bombari%20and%20Marco%20Mondelli&entry.1292438233=%20%20Understanding%20the%20reasons%20behind%20the%20exceptional%20success%20of%20transformers%0Arequires%20a%20better%20analysis%20of%20why%20attention%20layers%20are%20suitable%20for%20NLP%20tasks.%0AIn%20particular%2C%20such%20tasks%20require%20predictive%20models%20to%20capture%20contextual%0Ameaning%20which%20often%20depends%20on%20one%20or%20few%20words%2C%20even%20if%20the%20sentence%20is%20long.%0AOur%20work%20studies%20this%20key%20property%2C%20dubbed%20word%20sensitivity%20%28WS%29%2C%20in%20the%0Aprototypical%20setting%20of%20random%20features.%20We%20show%20that%20attention%20layers%20enjoy%0Ahigh%20WS%2C%20namely%2C%20there%20exists%20a%20vector%20in%20the%20space%20of%20embeddings%20that%20largely%0Aperturbs%20the%20random%20attention%20features%20map.%20The%20argument%20critically%20exploits%0Athe%20role%20of%20the%20softmax%20in%20the%20attention%20layer%2C%20highlighting%20its%20benefit%0Acompared%20to%20other%20activations%20%28e.g.%2C%20ReLU%29.%20In%20contrast%2C%20the%20WS%20of%20standard%0Arandom%20features%20is%20of%20order%20%241/%5Csqrt%7Bn%7D%24%2C%20%24n%24%20being%20the%20number%20of%20words%20in%20the%0Atextual%20sample%2C%20and%20thus%20it%20decays%20with%20the%20length%20of%20the%20context.%20We%20then%0Atranslate%20these%20results%20on%20the%20word%20sensitivity%20into%20generalization%20bounds%3A%20due%0Ato%20their%20low%20WS%2C%20random%20features%20provably%20cannot%20learn%20to%20distinguish%20between%0Atwo%20sentences%20that%20differ%20only%20in%20a%20single%20word%3B%20in%20contrast%2C%20due%20to%20their%20high%0AWS%2C%20random%20attention%20features%20have%20higher%20generalization%20capabilities.%20We%0Avalidate%20our%20theoretical%20results%20with%20experimental%20evidence%20over%20the%20BERT-Base%0Aword%20embeddings%20of%20the%20imdb%20review%20dataset.%0A&entry.1838667208=http%3A//arxiv.org/abs/2402.02969v2&entry.124074799=Read"},
{"title": "Evaluating Saliency Explanations in NLP by Crowdsourcing", "author": "Xiaotian Lu and Jiyi Li and Zhen Wan and Xiaofeng Lin and Koh Takeuchi and Hisashi Kashima", "abstract": "  Deep learning models have performed well on many NLP tasks. However, their\ninternal mechanisms are typically difficult for humans to understand. The\ndevelopment of methods to explain models has become a key issue in the\nreliability of deep learning models in many important applications. Various\nsaliency explanation methods, which give each feature of input a score\nproportional to the contribution of output, have been proposed to determine the\npart of the input which a model values most. Despite a considerable body of\nwork on the evaluation of saliency methods, whether the results of various\nevaluation metrics agree with human cognition remains an open question. In this\nstudy, we propose a new human-based method to evaluate saliency methods in NLP\nby crowdsourcing. We recruited 800 crowd workers and empirically evaluated\nseven saliency methods on two datasets with the proposed method. We analyzed\nthe performance of saliency methods, compared our results with existing\nautomated evaluation methods, and identified notable differences between NLP\nand computer vision (CV) fields when using saliency methods. The instance-level\ndata of our crowdsourced experiments and the code to reproduce the explanations\nare available at https://github.com/xtlu/lreccoling_evaluation.\n", "link": "http://arxiv.org/abs/2405.10767v1", "date": "2024-05-17", "relevancy": 1.429, "topK": [{"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.5017}, {"title": "Flexible Visual Recognition by Evidential Modeling of Confusion and Ignorance", "link": "http://openaccess.thecvf.com/content/ICCV2023/html/Fan_Flexible_Visual_Recognition_by_Evidential_Modeling_of_Confusion_and_Ignorance_ICCV_2023_paper.html", "similarity": 0.4704}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.4658}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Evaluating%20Saliency%20Explanations%20in%20NLP%20by%20Crowdsourcing&body=Title%3A%20Evaluating%20Saliency%20Explanations%20in%20NLP%20by%20Crowdsourcing%0AAuthor%3A%20Xiaotian%20Lu%20and%20Jiyi%20Li%20and%20Zhen%20Wan%20and%20Xiaofeng%20Lin%20and%20Koh%20Takeuchi%20and%20Hisashi%20Kashima%0AAbstract%3A%20%20%20Deep%20learning%20models%20have%20performed%20well%20on%20many%20NLP%20tasks.%20However%2C%20their%0Ainternal%20mechanisms%20are%20typically%20difficult%20for%20humans%20to%20understand.%20The%0Adevelopment%20of%20methods%20to%20explain%20models%20has%20become%20a%20key%20issue%20in%20the%0Areliability%20of%20deep%20learning%20models%20in%20many%20important%20applications.%20Various%0Asaliency%20explanation%20methods%2C%20which%20give%20each%20feature%20of%20input%20a%20score%0Aproportional%20to%20the%20contribution%20of%20output%2C%20have%20been%20proposed%20to%20determine%20the%0Apart%20of%20the%20input%20which%20a%20model%20values%20most.%20Despite%20a%20considerable%20body%20of%0Awork%20on%20the%20evaluation%20of%20saliency%20methods%2C%20whether%20the%20results%20of%20various%0Aevaluation%20metrics%20agree%20with%20human%20cognition%20remains%20an%20open%20question.%20In%20this%0Astudy%2C%20we%20propose%20a%20new%20human-based%20method%20to%20evaluate%20saliency%20methods%20in%20NLP%0Aby%20crowdsourcing.%20We%20recruited%20800%20crowd%20workers%20and%20empirically%20evaluated%0Aseven%20saliency%20methods%20on%20two%20datasets%20with%20the%20proposed%20method.%20We%20analyzed%0Athe%20performance%20of%20saliency%20methods%2C%20compared%20our%20results%20with%20existing%0Aautomated%20evaluation%20methods%2C%20and%20identified%20notable%20differences%20between%20NLP%0Aand%20computer%20vision%20%28CV%29%20fields%20when%20using%20saliency%20methods.%20The%20instance-level%0Adata%20of%20our%20crowdsourced%20experiments%20and%20the%20code%20to%20reproduce%20the%20explanations%0Aare%20available%20at%20https%3A//github.com/xtlu/lreccoling_evaluation.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2405.10767v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DEvaluating%2520Saliency%2520Explanations%2520in%2520NLP%2520by%2520Crowdsourcing%26entry.906535625%3DXiaotian%2520Lu%2520and%2520Jiyi%2520Li%2520and%2520Zhen%2520Wan%2520and%2520Xiaofeng%2520Lin%2520and%2520Koh%2520Takeuchi%2520and%2520Hisashi%2520Kashima%26entry.1292438233%3D%2520%2520Deep%2520learning%2520models%2520have%2520performed%2520well%2520on%2520many%2520NLP%2520tasks.%2520However%252C%2520their%250Ainternal%2520mechanisms%2520are%2520typically%2520difficult%2520for%2520humans%2520to%2520understand.%2520The%250Adevelopment%2520of%2520methods%2520to%2520explain%2520models%2520has%2520become%2520a%2520key%2520issue%2520in%2520the%250Areliability%2520of%2520deep%2520learning%2520models%2520in%2520many%2520important%2520applications.%2520Various%250Asaliency%2520explanation%2520methods%252C%2520which%2520give%2520each%2520feature%2520of%2520input%2520a%2520score%250Aproportional%2520to%2520the%2520contribution%2520of%2520output%252C%2520have%2520been%2520proposed%2520to%2520determine%2520the%250Apart%2520of%2520the%2520input%2520which%2520a%2520model%2520values%2520most.%2520Despite%2520a%2520considerable%2520body%2520of%250Awork%2520on%2520the%2520evaluation%2520of%2520saliency%2520methods%252C%2520whether%2520the%2520results%2520of%2520various%250Aevaluation%2520metrics%2520agree%2520with%2520human%2520cognition%2520remains%2520an%2520open%2520question.%2520In%2520this%250Astudy%252C%2520we%2520propose%2520a%2520new%2520human-based%2520method%2520to%2520evaluate%2520saliency%2520methods%2520in%2520NLP%250Aby%2520crowdsourcing.%2520We%2520recruited%2520800%2520crowd%2520workers%2520and%2520empirically%2520evaluated%250Aseven%2520saliency%2520methods%2520on%2520two%2520datasets%2520with%2520the%2520proposed%2520method.%2520We%2520analyzed%250Athe%2520performance%2520of%2520saliency%2520methods%252C%2520compared%2520our%2520results%2520with%2520existing%250Aautomated%2520evaluation%2520methods%252C%2520and%2520identified%2520notable%2520differences%2520between%2520NLP%250Aand%2520computer%2520vision%2520%2528CV%2529%2520fields%2520when%2520using%2520saliency%2520methods.%2520The%2520instance-level%250Adata%2520of%2520our%2520crowdsourced%2520experiments%2520and%2520the%2520code%2520to%2520reproduce%2520the%2520explanations%250Aare%2520available%2520at%2520https%253A//github.com/xtlu/lreccoling_evaluation.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2405.10767v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Evaluating%20Saliency%20Explanations%20in%20NLP%20by%20Crowdsourcing&entry.906535625=Xiaotian%20Lu%20and%20Jiyi%20Li%20and%20Zhen%20Wan%20and%20Xiaofeng%20Lin%20and%20Koh%20Takeuchi%20and%20Hisashi%20Kashima&entry.1292438233=%20%20Deep%20learning%20models%20have%20performed%20well%20on%20many%20NLP%20tasks.%20However%2C%20their%0Ainternal%20mechanisms%20are%20typically%20difficult%20for%20humans%20to%20understand.%20The%0Adevelopment%20of%20methods%20to%20explain%20models%20has%20become%20a%20key%20issue%20in%20the%0Areliability%20of%20deep%20learning%20models%20in%20many%20important%20applications.%20Various%0Asaliency%20explanation%20methods%2C%20which%20give%20each%20feature%20of%20input%20a%20score%0Aproportional%20to%20the%20contribution%20of%20output%2C%20have%20been%20proposed%20to%20determine%20the%0Apart%20of%20the%20input%20which%20a%20model%20values%20most.%20Despite%20a%20considerable%20body%20of%0Awork%20on%20the%20evaluation%20of%20saliency%20methods%2C%20whether%20the%20results%20of%20various%0Aevaluation%20metrics%20agree%20with%20human%20cognition%20remains%20an%20open%20question.%20In%20this%0Astudy%2C%20we%20propose%20a%20new%20human-based%20method%20to%20evaluate%20saliency%20methods%20in%20NLP%0Aby%20crowdsourcing.%20We%20recruited%20800%20crowd%20workers%20and%20empirically%20evaluated%0Aseven%20saliency%20methods%20on%20two%20datasets%20with%20the%20proposed%20method.%20We%20analyzed%0Athe%20performance%20of%20saliency%20methods%2C%20compared%20our%20results%20with%20existing%0Aautomated%20evaluation%20methods%2C%20and%20identified%20notable%20differences%20between%20NLP%0Aand%20computer%20vision%20%28CV%29%20fields%20when%20using%20saliency%20methods.%20The%20instance-level%0Adata%20of%20our%20crowdsourced%20experiments%20and%20the%20code%20to%20reproduce%20the%20explanations%0Aare%20available%20at%20https%3A//github.com/xtlu/lreccoling_evaluation.%0A&entry.1838667208=http%3A//arxiv.org/abs/2405.10767v1&entry.124074799=Read"},
{"title": "KernelSHAP-IQ: Weighted Least-Square Optimization for Shapley\n  Interactions", "author": "Fabian Fumagalli and Maximilian Muschalik and Patrick Kolpaczki and Eyke H\u00fcllermeier and Barbara Hammer", "abstract": "  The Shapley value (SV) is a prevalent approach of allocating credit to\nmachine learning (ML) entities to understand black box ML models. Enriching\nsuch interpretations with higher-order interactions is inevitable for complex\nsystems, where the Shapley Interaction Index (SII) is a direct axiomatic\nextension of the SV. While it is well-known that the SV yields an optimal\napproximation of any game via a weighted least square (WLS) objective, an\nextension of this result to SII has been a long-standing open problem, which\neven led to the proposal of an alternative index. In this work, we characterize\nhigher-order SII as a solution to a WLS problem, which constructs an optimal\napproximation via SII and $k$-Shapley values ($k$-SII). We prove this\nrepresentation for the SV and pairwise SII and give empirically validated\nconjectures for higher orders. As a result, we propose KernelSHAP-IQ, a direct\nextension of KernelSHAP for SII, and demonstrate state-of-the-art performance\nfor feature interactions.\n", "link": "http://arxiv.org/abs/2405.10852v1", "date": "2024-05-17", "relevancy": 1.2146, "topK": [{"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.4239}, {"title": "VirtualModel: Generating Object-ID-retentive Human-object Interaction\n  Image by Diffusion Model for E-commerce Marketing", "link": "http://arxiv.org/abs/2405.09985v1", "similarity": 0.4007}, {"title": "SDXS: Real-Time One-Step Latent Diffusion Models with Image Conditions", "link": "http://arxiv.org/abs/2403.16627v1", "similarity": 0.3962}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20KernelSHAP-IQ%3A%20Weighted%20Least-Square%20Optimization%20for%20Shapley%0A%20%20Interactions&body=Title%3A%20KernelSHAP-IQ%3A%20Weighted%20Least-Square%20Optimization%20for%20Shapley%0A%20%20Interactions%0AAuthor%3A%20Fabian%20Fumagalli%20and%20Maximilian%20Muschalik%20and%20Patrick%20Kolpaczki%20and%20Eyke%20H%C3%BCllermeier%20and%20Barbara%20Hammer%0AAbstract%3A%20%20%20The%20Shapley%20value%20%28SV%29%20is%20a%20prevalent%20approach%20of%20allocating%20credit%20to%0Amachine%20learning%20%28ML%29%20entities%20to%20understand%20black%20box%20ML%20models.%20Enriching%0Asuch%20interpretations%20with%20higher-order%20interactions%20is%20inevitable%20for%20complex%0Asystems%2C%20where%20the%20Shapley%20Interaction%20Index%20%28SII%29%20is%20a%20direct%20axiomatic%0Aextension%20of%20the%20SV.%20While%20it%20is%20well-known%20that%20the%20SV%20yields%20an%20optimal%0Aapproximation%20of%20any%20game%20via%20a%20weighted%20least%20square%20%28WLS%29%20objective%2C%20an%0Aextension%20of%20this%20result%20to%20SII%20has%20been%20a%20long-standing%20open%20problem%2C%20which%0Aeven%20led%20to%20the%20proposal%20of%20an%20alternative%20index.%20In%20this%20work%2C%20we%20characterize%0Ahigher-order%20SII%20as%20a%20solution%20to%20a%20WLS%20problem%2C%20which%20constructs%20an%20optimal%0Aapproximation%20via%20SII%20and%20%24k%24-Shapley%20values%20%28%24k%24-SII%29.%20We%20prove%20this%0Arepresentation%20for%20the%20SV%20and%20pairwise%20SII%20and%20give%20empirically%20validated%0Aconjectures%20for%20higher%20orders.%20As%20a%20result%2C%20we%20propose%20KernelSHAP-IQ%2C%20a%20direct%0Aextension%20of%20KernelSHAP%20for%20SII%2C%20and%20demonstrate%20state-of-the-art%20performance%0Afor%20feature%20interactions.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2405.10852v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DKernelSHAP-IQ%253A%2520Weighted%2520Least-Square%2520Optimization%2520for%2520Shapley%250A%2520%2520Interactions%26entry.906535625%3DFabian%2520Fumagalli%2520and%2520Maximilian%2520Muschalik%2520and%2520Patrick%2520Kolpaczki%2520and%2520Eyke%2520H%25C3%25BCllermeier%2520and%2520Barbara%2520Hammer%26entry.1292438233%3D%2520%2520The%2520Shapley%2520value%2520%2528SV%2529%2520is%2520a%2520prevalent%2520approach%2520of%2520allocating%2520credit%2520to%250Amachine%2520learning%2520%2528ML%2529%2520entities%2520to%2520understand%2520black%2520box%2520ML%2520models.%2520Enriching%250Asuch%2520interpretations%2520with%2520higher-order%2520interactions%2520is%2520inevitable%2520for%2520complex%250Asystems%252C%2520where%2520the%2520Shapley%2520Interaction%2520Index%2520%2528SII%2529%2520is%2520a%2520direct%2520axiomatic%250Aextension%2520of%2520the%2520SV.%2520While%2520it%2520is%2520well-known%2520that%2520the%2520SV%2520yields%2520an%2520optimal%250Aapproximation%2520of%2520any%2520game%2520via%2520a%2520weighted%2520least%2520square%2520%2528WLS%2529%2520objective%252C%2520an%250Aextension%2520of%2520this%2520result%2520to%2520SII%2520has%2520been%2520a%2520long-standing%2520open%2520problem%252C%2520which%250Aeven%2520led%2520to%2520the%2520proposal%2520of%2520an%2520alternative%2520index.%2520In%2520this%2520work%252C%2520we%2520characterize%250Ahigher-order%2520SII%2520as%2520a%2520solution%2520to%2520a%2520WLS%2520problem%252C%2520which%2520constructs%2520an%2520optimal%250Aapproximation%2520via%2520SII%2520and%2520%2524k%2524-Shapley%2520values%2520%2528%2524k%2524-SII%2529.%2520We%2520prove%2520this%250Arepresentation%2520for%2520the%2520SV%2520and%2520pairwise%2520SII%2520and%2520give%2520empirically%2520validated%250Aconjectures%2520for%2520higher%2520orders.%2520As%2520a%2520result%252C%2520we%2520propose%2520KernelSHAP-IQ%252C%2520a%2520direct%250Aextension%2520of%2520KernelSHAP%2520for%2520SII%252C%2520and%2520demonstrate%2520state-of-the-art%2520performance%250Afor%2520feature%2520interactions.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2405.10852v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=KernelSHAP-IQ%3A%20Weighted%20Least-Square%20Optimization%20for%20Shapley%0A%20%20Interactions&entry.906535625=Fabian%20Fumagalli%20and%20Maximilian%20Muschalik%20and%20Patrick%20Kolpaczki%20and%20Eyke%20H%C3%BCllermeier%20and%20Barbara%20Hammer&entry.1292438233=%20%20The%20Shapley%20value%20%28SV%29%20is%20a%20prevalent%20approach%20of%20allocating%20credit%20to%0Amachine%20learning%20%28ML%29%20entities%20to%20understand%20black%20box%20ML%20models.%20Enriching%0Asuch%20interpretations%20with%20higher-order%20interactions%20is%20inevitable%20for%20complex%0Asystems%2C%20where%20the%20Shapley%20Interaction%20Index%20%28SII%29%20is%20a%20direct%20axiomatic%0Aextension%20of%20the%20SV.%20While%20it%20is%20well-known%20that%20the%20SV%20yields%20an%20optimal%0Aapproximation%20of%20any%20game%20via%20a%20weighted%20least%20square%20%28WLS%29%20objective%2C%20an%0Aextension%20of%20this%20result%20to%20SII%20has%20been%20a%20long-standing%20open%20problem%2C%20which%0Aeven%20led%20to%20the%20proposal%20of%20an%20alternative%20index.%20In%20this%20work%2C%20we%20characterize%0Ahigher-order%20SII%20as%20a%20solution%20to%20a%20WLS%20problem%2C%20which%20constructs%20an%20optimal%0Aapproximation%20via%20SII%20and%20%24k%24-Shapley%20values%20%28%24k%24-SII%29.%20We%20prove%20this%0Arepresentation%20for%20the%20SV%20and%20pairwise%20SII%20and%20give%20empirically%20validated%0Aconjectures%20for%20higher%20orders.%20As%20a%20result%2C%20we%20propose%20KernelSHAP-IQ%2C%20a%20direct%0Aextension%20of%20KernelSHAP%20for%20SII%2C%20and%20demonstrate%20state-of-the-art%20performance%0Afor%20feature%20interactions.%0A&entry.1838667208=http%3A//arxiv.org/abs/2405.10852v1&entry.124074799=Read"},
{"title": "Observational Scaling Laws and the Predictability of Language Model\n  Performance", "author": "Yangjun Ruan and Chris J. Maddison and Tatsunori Hashimoto", "abstract": "  Understanding how language model performance varies with scale is critical to\nbenchmark and algorithm development. Scaling laws are one approach to building\nthis understanding, but the requirement of training models across many\ndifferent scales has limited their use. We propose an alternative,\nobservational approach that bypasses model training and instead builds scaling\nlaws from ~80 publically available models. Building a single scaling law from\nmultiple model families is challenging due to large variations in their\ntraining compute efficiencies and capabilities. However, we show that these\nvariations are consistent with a simple, generalized scaling law where language\nmodel performance is a function of a low-dimensional capability space, and\nmodel families only vary in their efficiency in converting training compute to\ncapabilities. Using this approach, we show the surprising predictability of\ncomplex scaling phenomena: we show that several emergent phenomena follow a\nsmooth, sigmoidal behavior and are predictable from small models; we show that\nthe agent performance of models such as GPT-4 can be precisely predicted from\nsimpler non-agentic benchmarks; and we show how to predict the impact of\npost-training interventions like Chain-of-Thought and Self-Consistency as\nlanguage model capabilities continue to improve.\n", "link": "http://arxiv.org/abs/2405.10938v1", "date": "2024-05-17", "relevancy": 1.0098, "topK": [{"title": "Scaling Rectified Flow Transformers for High-Resolution Image Synthesis", "link": "http://arxiv.org/abs/2403.03206v1", "similarity": 0.5638}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.5032}, {"title": "SDXS: Real-Time One-Step Latent Diffusion Models with Image Conditions", "link": "http://arxiv.org/abs/2403.16627v1", "similarity": 0.4476}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Observational%20Scaling%20Laws%20and%20the%20Predictability%20of%20Language%20Model%0A%20%20Performance&body=Title%3A%20Observational%20Scaling%20Laws%20and%20the%20Predictability%20of%20Language%20Model%0A%20%20Performance%0AAuthor%3A%20Yangjun%20Ruan%20and%20Chris%20J.%20Maddison%20and%20Tatsunori%20Hashimoto%0AAbstract%3A%20%20%20Understanding%20how%20language%20model%20performance%20varies%20with%20scale%20is%20critical%20to%0Abenchmark%20and%20algorithm%20development.%20Scaling%20laws%20are%20one%20approach%20to%20building%0Athis%20understanding%2C%20but%20the%20requirement%20of%20training%20models%20across%20many%0Adifferent%20scales%20has%20limited%20their%20use.%20We%20propose%20an%20alternative%2C%0Aobservational%20approach%20that%20bypasses%20model%20training%20and%20instead%20builds%20scaling%0Alaws%20from%20~80%20publically%20available%20models.%20Building%20a%20single%20scaling%20law%20from%0Amultiple%20model%20families%20is%20challenging%20due%20to%20large%20variations%20in%20their%0Atraining%20compute%20efficiencies%20and%20capabilities.%20However%2C%20we%20show%20that%20these%0Avariations%20are%20consistent%20with%20a%20simple%2C%20generalized%20scaling%20law%20where%20language%0Amodel%20performance%20is%20a%20function%20of%20a%20low-dimensional%20capability%20space%2C%20and%0Amodel%20families%20only%20vary%20in%20their%20efficiency%20in%20converting%20training%20compute%20to%0Acapabilities.%20Using%20this%20approach%2C%20we%20show%20the%20surprising%20predictability%20of%0Acomplex%20scaling%20phenomena%3A%20we%20show%20that%20several%20emergent%20phenomena%20follow%20a%0Asmooth%2C%20sigmoidal%20behavior%20and%20are%20predictable%20from%20small%20models%3B%20we%20show%20that%0Athe%20agent%20performance%20of%20models%20such%20as%20GPT-4%20can%20be%20precisely%20predicted%20from%0Asimpler%20non-agentic%20benchmarks%3B%20and%20we%20show%20how%20to%20predict%20the%20impact%20of%0Apost-training%20interventions%20like%20Chain-of-Thought%20and%20Self-Consistency%20as%0Alanguage%20model%20capabilities%20continue%20to%20improve.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2405.10938v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DObservational%2520Scaling%2520Laws%2520and%2520the%2520Predictability%2520of%2520Language%2520Model%250A%2520%2520Performance%26entry.906535625%3DYangjun%2520Ruan%2520and%2520Chris%2520J.%2520Maddison%2520and%2520Tatsunori%2520Hashimoto%26entry.1292438233%3D%2520%2520Understanding%2520how%2520language%2520model%2520performance%2520varies%2520with%2520scale%2520is%2520critical%2520to%250Abenchmark%2520and%2520algorithm%2520development.%2520Scaling%2520laws%2520are%2520one%2520approach%2520to%2520building%250Athis%2520understanding%252C%2520but%2520the%2520requirement%2520of%2520training%2520models%2520across%2520many%250Adifferent%2520scales%2520has%2520limited%2520their%2520use.%2520We%2520propose%2520an%2520alternative%252C%250Aobservational%2520approach%2520that%2520bypasses%2520model%2520training%2520and%2520instead%2520builds%2520scaling%250Alaws%2520from%2520~80%2520publically%2520available%2520models.%2520Building%2520a%2520single%2520scaling%2520law%2520from%250Amultiple%2520model%2520families%2520is%2520challenging%2520due%2520to%2520large%2520variations%2520in%2520their%250Atraining%2520compute%2520efficiencies%2520and%2520capabilities.%2520However%252C%2520we%2520show%2520that%2520these%250Avariations%2520are%2520consistent%2520with%2520a%2520simple%252C%2520generalized%2520scaling%2520law%2520where%2520language%250Amodel%2520performance%2520is%2520a%2520function%2520of%2520a%2520low-dimensional%2520capability%2520space%252C%2520and%250Amodel%2520families%2520only%2520vary%2520in%2520their%2520efficiency%2520in%2520converting%2520training%2520compute%2520to%250Acapabilities.%2520Using%2520this%2520approach%252C%2520we%2520show%2520the%2520surprising%2520predictability%2520of%250Acomplex%2520scaling%2520phenomena%253A%2520we%2520show%2520that%2520several%2520emergent%2520phenomena%2520follow%2520a%250Asmooth%252C%2520sigmoidal%2520behavior%2520and%2520are%2520predictable%2520from%2520small%2520models%253B%2520we%2520show%2520that%250Athe%2520agent%2520performance%2520of%2520models%2520such%2520as%2520GPT-4%2520can%2520be%2520precisely%2520predicted%2520from%250Asimpler%2520non-agentic%2520benchmarks%253B%2520and%2520we%2520show%2520how%2520to%2520predict%2520the%2520impact%2520of%250Apost-training%2520interventions%2520like%2520Chain-of-Thought%2520and%2520Self-Consistency%2520as%250Alanguage%2520model%2520capabilities%2520continue%2520to%2520improve.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2405.10938v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Observational%20Scaling%20Laws%20and%20the%20Predictability%20of%20Language%20Model%0A%20%20Performance&entry.906535625=Yangjun%20Ruan%20and%20Chris%20J.%20Maddison%20and%20Tatsunori%20Hashimoto&entry.1292438233=%20%20Understanding%20how%20language%20model%20performance%20varies%20with%20scale%20is%20critical%20to%0Abenchmark%20and%20algorithm%20development.%20Scaling%20laws%20are%20one%20approach%20to%20building%0Athis%20understanding%2C%20but%20the%20requirement%20of%20training%20models%20across%20many%0Adifferent%20scales%20has%20limited%20their%20use.%20We%20propose%20an%20alternative%2C%0Aobservational%20approach%20that%20bypasses%20model%20training%20and%20instead%20builds%20scaling%0Alaws%20from%20~80%20publically%20available%20models.%20Building%20a%20single%20scaling%20law%20from%0Amultiple%20model%20families%20is%20challenging%20due%20to%20large%20variations%20in%20their%0Atraining%20compute%20efficiencies%20and%20capabilities.%20However%2C%20we%20show%20that%20these%0Avariations%20are%20consistent%20with%20a%20simple%2C%20generalized%20scaling%20law%20where%20language%0Amodel%20performance%20is%20a%20function%20of%20a%20low-dimensional%20capability%20space%2C%20and%0Amodel%20families%20only%20vary%20in%20their%20efficiency%20in%20converting%20training%20compute%20to%0Acapabilities.%20Using%20this%20approach%2C%20we%20show%20the%20surprising%20predictability%20of%0Acomplex%20scaling%20phenomena%3A%20we%20show%20that%20several%20emergent%20phenomena%20follow%20a%0Asmooth%2C%20sigmoidal%20behavior%20and%20are%20predictable%20from%20small%20models%3B%20we%20show%20that%0Athe%20agent%20performance%20of%20models%20such%20as%20GPT-4%20can%20be%20precisely%20predicted%20from%0Asimpler%20non-agentic%20benchmarks%3B%20and%20we%20show%20how%20to%20predict%20the%20impact%20of%0Apost-training%20interventions%20like%20Chain-of-Thought%20and%20Self-Consistency%20as%0Alanguage%20model%20capabilities%20continue%20to%20improve.%0A&entry.1838667208=http%3A//arxiv.org/abs/2405.10938v1&entry.124074799=Read"},
{"title": "Submodular Information Selection for Hypothesis Testing with\n  Misclassification Penalties", "author": "Jayanth Bhargav and Mahsa Ghasemi and Shreyas Sundaram", "abstract": "  We consider the problem of selecting an optimal subset of information sources\nfor a hypothesis testing/classification task where the goal is to identify the\ntrue state of the world from a finite set of hypotheses, based on finite\nobservation samples from the sources. In order to characterize the learning\nperformance, we propose a misclassification penalty framework, which enables\nnon-uniform treatment of different misclassification errors. In a centralized\nBayesian learning setting, we study two variants of the subset selection\nproblem: (i) selecting a minimum cost information set to ensure that the\nmaximum penalty of misclassifying the true hypothesis remains bounded and (ii)\nselecting an optimal information set under a limited budget to minimize the\nmaximum penalty of misclassifying the true hypothesis. Under mild assumptions,\nwe prove that the objective (or constraints) of these combinatorial\noptimization problems are weak (or approximate) submodular, and establish\nhigh-probability performance guarantees for greedy algorithms. Further, we\npropose an alternate metric for information set selection which is based on the\ntotal penalty of misclassification. We prove that this metric is submodular and\nestablish near-optimal guarantees for the greedy algorithms for both the\ninformation set selection problems. Finally, we present numerical simulations\nto validate our theoretical results over several randomly generated instances.\n", "link": "http://arxiv.org/abs/2405.10930v1", "date": "2024-05-17", "relevancy": 1.3469, "topK": [{"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.4817}, {"title": "Flexible Visual Recognition by Evidential Modeling of Confusion and Ignorance", "link": "http://openaccess.thecvf.com/content/ICCV2023/html/Fan_Flexible_Visual_Recognition_by_Evidential_Modeling_of_Confusion_and_Ignorance_ICCV_2023_paper.html", "similarity": 0.4574}, {"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.3952}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Submodular%20Information%20Selection%20for%20Hypothesis%20Testing%20with%0A%20%20Misclassification%20Penalties&body=Title%3A%20Submodular%20Information%20Selection%20for%20Hypothesis%20Testing%20with%0A%20%20Misclassification%20Penalties%0AAuthor%3A%20Jayanth%20Bhargav%20and%20Mahsa%20Ghasemi%20and%20Shreyas%20Sundaram%0AAbstract%3A%20%20%20We%20consider%20the%20problem%20of%20selecting%20an%20optimal%20subset%20of%20information%20sources%0Afor%20a%20hypothesis%20testing/classification%20task%20where%20the%20goal%20is%20to%20identify%20the%0Atrue%20state%20of%20the%20world%20from%20a%20finite%20set%20of%20hypotheses%2C%20based%20on%20finite%0Aobservation%20samples%20from%20the%20sources.%20In%20order%20to%20characterize%20the%20learning%0Aperformance%2C%20we%20propose%20a%20misclassification%20penalty%20framework%2C%20which%20enables%0Anon-uniform%20treatment%20of%20different%20misclassification%20errors.%20In%20a%20centralized%0ABayesian%20learning%20setting%2C%20we%20study%20two%20variants%20of%20the%20subset%20selection%0Aproblem%3A%20%28i%29%20selecting%20a%20minimum%20cost%20information%20set%20to%20ensure%20that%20the%0Amaximum%20penalty%20of%20misclassifying%20the%20true%20hypothesis%20remains%20bounded%20and%20%28ii%29%0Aselecting%20an%20optimal%20information%20set%20under%20a%20limited%20budget%20to%20minimize%20the%0Amaximum%20penalty%20of%20misclassifying%20the%20true%20hypothesis.%20Under%20mild%20assumptions%2C%0Awe%20prove%20that%20the%20objective%20%28or%20constraints%29%20of%20these%20combinatorial%0Aoptimization%20problems%20are%20weak%20%28or%20approximate%29%20submodular%2C%20and%20establish%0Ahigh-probability%20performance%20guarantees%20for%20greedy%20algorithms.%20Further%2C%20we%0Apropose%20an%20alternate%20metric%20for%20information%20set%20selection%20which%20is%20based%20on%20the%0Atotal%20penalty%20of%20misclassification.%20We%20prove%20that%20this%20metric%20is%20submodular%20and%0Aestablish%20near-optimal%20guarantees%20for%20the%20greedy%20algorithms%20for%20both%20the%0Ainformation%20set%20selection%20problems.%20Finally%2C%20we%20present%20numerical%20simulations%0Ato%20validate%20our%20theoretical%20results%20over%20several%20randomly%20generated%20instances.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2405.10930v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DSubmodular%2520Information%2520Selection%2520for%2520Hypothesis%2520Testing%2520with%250A%2520%2520Misclassification%2520Penalties%26entry.906535625%3DJayanth%2520Bhargav%2520and%2520Mahsa%2520Ghasemi%2520and%2520Shreyas%2520Sundaram%26entry.1292438233%3D%2520%2520We%2520consider%2520the%2520problem%2520of%2520selecting%2520an%2520optimal%2520subset%2520of%2520information%2520sources%250Afor%2520a%2520hypothesis%2520testing/classification%2520task%2520where%2520the%2520goal%2520is%2520to%2520identify%2520the%250Atrue%2520state%2520of%2520the%2520world%2520from%2520a%2520finite%2520set%2520of%2520hypotheses%252C%2520based%2520on%2520finite%250Aobservation%2520samples%2520from%2520the%2520sources.%2520In%2520order%2520to%2520characterize%2520the%2520learning%250Aperformance%252C%2520we%2520propose%2520a%2520misclassification%2520penalty%2520framework%252C%2520which%2520enables%250Anon-uniform%2520treatment%2520of%2520different%2520misclassification%2520errors.%2520In%2520a%2520centralized%250ABayesian%2520learning%2520setting%252C%2520we%2520study%2520two%2520variants%2520of%2520the%2520subset%2520selection%250Aproblem%253A%2520%2528i%2529%2520selecting%2520a%2520minimum%2520cost%2520information%2520set%2520to%2520ensure%2520that%2520the%250Amaximum%2520penalty%2520of%2520misclassifying%2520the%2520true%2520hypothesis%2520remains%2520bounded%2520and%2520%2528ii%2529%250Aselecting%2520an%2520optimal%2520information%2520set%2520under%2520a%2520limited%2520budget%2520to%2520minimize%2520the%250Amaximum%2520penalty%2520of%2520misclassifying%2520the%2520true%2520hypothesis.%2520Under%2520mild%2520assumptions%252C%250Awe%2520prove%2520that%2520the%2520objective%2520%2528or%2520constraints%2529%2520of%2520these%2520combinatorial%250Aoptimization%2520problems%2520are%2520weak%2520%2528or%2520approximate%2529%2520submodular%252C%2520and%2520establish%250Ahigh-probability%2520performance%2520guarantees%2520for%2520greedy%2520algorithms.%2520Further%252C%2520we%250Apropose%2520an%2520alternate%2520metric%2520for%2520information%2520set%2520selection%2520which%2520is%2520based%2520on%2520the%250Atotal%2520penalty%2520of%2520misclassification.%2520We%2520prove%2520that%2520this%2520metric%2520is%2520submodular%2520and%250Aestablish%2520near-optimal%2520guarantees%2520for%2520the%2520greedy%2520algorithms%2520for%2520both%2520the%250Ainformation%2520set%2520selection%2520problems.%2520Finally%252C%2520we%2520present%2520numerical%2520simulations%250Ato%2520validate%2520our%2520theoretical%2520results%2520over%2520several%2520randomly%2520generated%2520instances.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2405.10930v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Submodular%20Information%20Selection%20for%20Hypothesis%20Testing%20with%0A%20%20Misclassification%20Penalties&entry.906535625=Jayanth%20Bhargav%20and%20Mahsa%20Ghasemi%20and%20Shreyas%20Sundaram&entry.1292438233=%20%20We%20consider%20the%20problem%20of%20selecting%20an%20optimal%20subset%20of%20information%20sources%0Afor%20a%20hypothesis%20testing/classification%20task%20where%20the%20goal%20is%20to%20identify%20the%0Atrue%20state%20of%20the%20world%20from%20a%20finite%20set%20of%20hypotheses%2C%20based%20on%20finite%0Aobservation%20samples%20from%20the%20sources.%20In%20order%20to%20characterize%20the%20learning%0Aperformance%2C%20we%20propose%20a%20misclassification%20penalty%20framework%2C%20which%20enables%0Anon-uniform%20treatment%20of%20different%20misclassification%20errors.%20In%20a%20centralized%0ABayesian%20learning%20setting%2C%20we%20study%20two%20variants%20of%20the%20subset%20selection%0Aproblem%3A%20%28i%29%20selecting%20a%20minimum%20cost%20information%20set%20to%20ensure%20that%20the%0Amaximum%20penalty%20of%20misclassifying%20the%20true%20hypothesis%20remains%20bounded%20and%20%28ii%29%0Aselecting%20an%20optimal%20information%20set%20under%20a%20limited%20budget%20to%20minimize%20the%0Amaximum%20penalty%20of%20misclassifying%20the%20true%20hypothesis.%20Under%20mild%20assumptions%2C%0Awe%20prove%20that%20the%20objective%20%28or%20constraints%29%20of%20these%20combinatorial%0Aoptimization%20problems%20are%20weak%20%28or%20approximate%29%20submodular%2C%20and%20establish%0Ahigh-probability%20performance%20guarantees%20for%20greedy%20algorithms.%20Further%2C%20we%0Apropose%20an%20alternate%20metric%20for%20information%20set%20selection%20which%20is%20based%20on%20the%0Atotal%20penalty%20of%20misclassification.%20We%20prove%20that%20this%20metric%20is%20submodular%20and%0Aestablish%20near-optimal%20guarantees%20for%20the%20greedy%20algorithms%20for%20both%20the%0Ainformation%20set%20selection%20problems.%20Finally%2C%20we%20present%20numerical%20simulations%0Ato%20validate%20our%20theoretical%20results%20over%20several%20randomly%20generated%20instances.%0A&entry.1838667208=http%3A//arxiv.org/abs/2405.10930v1&entry.124074799=Read"},
{"title": "A Functional Model Method for Nonconvex Nonsmooth Conditional Stochastic\n  Optimization", "author": "Andrzej Ruszczy\u0144ski and Shangzhe Yang", "abstract": "  We consider stochastic optimization problems involving an expected value of a\nnonlinear function of a base random vector and a conditional expectation of\nanother function depending on the base random vector, a dependent random\nvector, and the decision variables. We call such problems conditional\nstochastic optimization problems. They arise in many applications, such as\nuplift modeling, reinforcement learning, and contextual optimization. We\npropose a specialized single time-scale stochastic method for nonconvex\nconstrained conditional stochastic optimization problems with a Lipschitz\nsmooth outer function and a generalized differentiable inner function. In the\nmethod, we approximate the inner conditional expectation with a rich parametric\nmodel whose mean squared error satisfies a stochastic version of a\n{\\L}ojasiewicz condition. The model is used by an inner learning algorithm. The\nmain feature of our approach is that unbiased stochastic estimates of the\ndirections used by the method can be generated with one observation from the\njoint distribution per iteration, which makes it applicable to real-time\nlearning. The directions, however, are not gradients or subgradients of any\noverall objective function. We prove the convergence of the method with\nprobability one, using the method of differential inclusions and a specially\ndesigned Lyapunov function, involving a stochastic generalization of the\nBregman distance. Finally, a numerical illustration demonstrates the viability\nof our approach.\n", "link": "http://arxiv.org/abs/2405.10815v1", "date": "2024-05-17", "relevancy": 0.9986, "topK": [{"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.5268}, {"title": "Multimodal-Conditioned Latent Diffusion Models for Fashion Image Editing", "link": "http://arxiv.org/abs/2403.14828v2", "similarity": 0.4905}, {"title": "SDXS: Real-Time One-Step Latent Diffusion Models with Image Conditions", "link": "http://arxiv.org/abs/2403.16627v1", "similarity": 0.4805}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20A%20Functional%20Model%20Method%20for%20Nonconvex%20Nonsmooth%20Conditional%20Stochastic%0A%20%20Optimization&body=Title%3A%20A%20Functional%20Model%20Method%20for%20Nonconvex%20Nonsmooth%20Conditional%20Stochastic%0A%20%20Optimization%0AAuthor%3A%20Andrzej%20Ruszczy%C5%84ski%20and%20Shangzhe%20Yang%0AAbstract%3A%20%20%20We%20consider%20stochastic%20optimization%20problems%20involving%20an%20expected%20value%20of%20a%0Anonlinear%20function%20of%20a%20base%20random%20vector%20and%20a%20conditional%20expectation%20of%0Aanother%20function%20depending%20on%20the%20base%20random%20vector%2C%20a%20dependent%20random%0Avector%2C%20and%20the%20decision%20variables.%20We%20call%20such%20problems%20conditional%0Astochastic%20optimization%20problems.%20They%20arise%20in%20many%20applications%2C%20such%20as%0Auplift%20modeling%2C%20reinforcement%20learning%2C%20and%20contextual%20optimization.%20We%0Apropose%20a%20specialized%20single%20time-scale%20stochastic%20method%20for%20nonconvex%0Aconstrained%20conditional%20stochastic%20optimization%20problems%20with%20a%20Lipschitz%0Asmooth%20outer%20function%20and%20a%20generalized%20differentiable%20inner%20function.%20In%20the%0Amethod%2C%20we%20approximate%20the%20inner%20conditional%20expectation%20with%20a%20rich%20parametric%0Amodel%20whose%20mean%20squared%20error%20satisfies%20a%20stochastic%20version%20of%20a%0A%7B%5CL%7Dojasiewicz%20condition.%20The%20model%20is%20used%20by%20an%20inner%20learning%20algorithm.%20The%0Amain%20feature%20of%20our%20approach%20is%20that%20unbiased%20stochastic%20estimates%20of%20the%0Adirections%20used%20by%20the%20method%20can%20be%20generated%20with%20one%20observation%20from%20the%0Ajoint%20distribution%20per%20iteration%2C%20which%20makes%20it%20applicable%20to%20real-time%0Alearning.%20The%20directions%2C%20however%2C%20are%20not%20gradients%20or%20subgradients%20of%20any%0Aoverall%20objective%20function.%20We%20prove%20the%20convergence%20of%20the%20method%20with%0Aprobability%20one%2C%20using%20the%20method%20of%20differential%20inclusions%20and%20a%20specially%0Adesigned%20Lyapunov%20function%2C%20involving%20a%20stochastic%20generalization%20of%20the%0ABregman%20distance.%20Finally%2C%20a%20numerical%20illustration%20demonstrates%20the%20viability%0Aof%20our%20approach.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2405.10815v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DA%2520Functional%2520Model%2520Method%2520for%2520Nonconvex%2520Nonsmooth%2520Conditional%2520Stochastic%250A%2520%2520Optimization%26entry.906535625%3DAndrzej%2520Ruszczy%25C5%2584ski%2520and%2520Shangzhe%2520Yang%26entry.1292438233%3D%2520%2520We%2520consider%2520stochastic%2520optimization%2520problems%2520involving%2520an%2520expected%2520value%2520of%2520a%250Anonlinear%2520function%2520of%2520a%2520base%2520random%2520vector%2520and%2520a%2520conditional%2520expectation%2520of%250Aanother%2520function%2520depending%2520on%2520the%2520base%2520random%2520vector%252C%2520a%2520dependent%2520random%250Avector%252C%2520and%2520the%2520decision%2520variables.%2520We%2520call%2520such%2520problems%2520conditional%250Astochastic%2520optimization%2520problems.%2520They%2520arise%2520in%2520many%2520applications%252C%2520such%2520as%250Auplift%2520modeling%252C%2520reinforcement%2520learning%252C%2520and%2520contextual%2520optimization.%2520We%250Apropose%2520a%2520specialized%2520single%2520time-scale%2520stochastic%2520method%2520for%2520nonconvex%250Aconstrained%2520conditional%2520stochastic%2520optimization%2520problems%2520with%2520a%2520Lipschitz%250Asmooth%2520outer%2520function%2520and%2520a%2520generalized%2520differentiable%2520inner%2520function.%2520In%2520the%250Amethod%252C%2520we%2520approximate%2520the%2520inner%2520conditional%2520expectation%2520with%2520a%2520rich%2520parametric%250Amodel%2520whose%2520mean%2520squared%2520error%2520satisfies%2520a%2520stochastic%2520version%2520of%2520a%250A%257B%255CL%257Dojasiewicz%2520condition.%2520The%2520model%2520is%2520used%2520by%2520an%2520inner%2520learning%2520algorithm.%2520The%250Amain%2520feature%2520of%2520our%2520approach%2520is%2520that%2520unbiased%2520stochastic%2520estimates%2520of%2520the%250Adirections%2520used%2520by%2520the%2520method%2520can%2520be%2520generated%2520with%2520one%2520observation%2520from%2520the%250Ajoint%2520distribution%2520per%2520iteration%252C%2520which%2520makes%2520it%2520applicable%2520to%2520real-time%250Alearning.%2520The%2520directions%252C%2520however%252C%2520are%2520not%2520gradients%2520or%2520subgradients%2520of%2520any%250Aoverall%2520objective%2520function.%2520We%2520prove%2520the%2520convergence%2520of%2520the%2520method%2520with%250Aprobability%2520one%252C%2520using%2520the%2520method%2520of%2520differential%2520inclusions%2520and%2520a%2520specially%250Adesigned%2520Lyapunov%2520function%252C%2520involving%2520a%2520stochastic%2520generalization%2520of%2520the%250ABregman%2520distance.%2520Finally%252C%2520a%2520numerical%2520illustration%2520demonstrates%2520the%2520viability%250Aof%2520our%2520approach.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2405.10815v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=A%20Functional%20Model%20Method%20for%20Nonconvex%20Nonsmooth%20Conditional%20Stochastic%0A%20%20Optimization&entry.906535625=Andrzej%20Ruszczy%C5%84ski%20and%20Shangzhe%20Yang&entry.1292438233=%20%20We%20consider%20stochastic%20optimization%20problems%20involving%20an%20expected%20value%20of%20a%0Anonlinear%20function%20of%20a%20base%20random%20vector%20and%20a%20conditional%20expectation%20of%0Aanother%20function%20depending%20on%20the%20base%20random%20vector%2C%20a%20dependent%20random%0Avector%2C%20and%20the%20decision%20variables.%20We%20call%20such%20problems%20conditional%0Astochastic%20optimization%20problems.%20They%20arise%20in%20many%20applications%2C%20such%20as%0Auplift%20modeling%2C%20reinforcement%20learning%2C%20and%20contextual%20optimization.%20We%0Apropose%20a%20specialized%20single%20time-scale%20stochastic%20method%20for%20nonconvex%0Aconstrained%20conditional%20stochastic%20optimization%20problems%20with%20a%20Lipschitz%0Asmooth%20outer%20function%20and%20a%20generalized%20differentiable%20inner%20function.%20In%20the%0Amethod%2C%20we%20approximate%20the%20inner%20conditional%20expectation%20with%20a%20rich%20parametric%0Amodel%20whose%20mean%20squared%20error%20satisfies%20a%20stochastic%20version%20of%20a%0A%7B%5CL%7Dojasiewicz%20condition.%20The%20model%20is%20used%20by%20an%20inner%20learning%20algorithm.%20The%0Amain%20feature%20of%20our%20approach%20is%20that%20unbiased%20stochastic%20estimates%20of%20the%0Adirections%20used%20by%20the%20method%20can%20be%20generated%20with%20one%20observation%20from%20the%0Ajoint%20distribution%20per%20iteration%2C%20which%20makes%20it%20applicable%20to%20real-time%0Alearning.%20The%20directions%2C%20however%2C%20are%20not%20gradients%20or%20subgradients%20of%20any%0Aoverall%20objective%20function.%20We%20prove%20the%20convergence%20of%20the%20method%20with%0Aprobability%20one%2C%20using%20the%20method%20of%20differential%20inclusions%20and%20a%20specially%0Adesigned%20Lyapunov%20function%2C%20involving%20a%20stochastic%20generalization%20of%20the%0ABregman%20distance.%20Finally%2C%20a%20numerical%20illustration%20demonstrates%20the%20viability%0Aof%20our%20approach.%0A&entry.1838667208=http%3A//arxiv.org/abs/2405.10815v1&entry.124074799=Read"},
{"title": "Development of Semantics-Based Distributed Middleware for Heterogeneous\n  Data Integration and its Application for Drought", "author": "A Akanbi", "abstract": "  Drought is a complex environmental phenomenon that affects millions of people\nand communities all over the globe and is too elusive to be accurately\npredicted. This is mostly due to the scalability and variability of the web of\nenvironmental parameters that directly/indirectly causes the onset of different\ncategories of drought. Since the dawn of man, efforts have been made to\nuniquely understand the natural indicators that provide signs of likely\nenvironmental events. These indicators/signs in the form of indigenous\nknowledge system have been used for generations. The intricate complexity of\ndrought has, however, always been a major stumbling block for accurate drought\nprediction and forecasting systems. Recently, scientists in the field of\nagriculture and environmental monitoring have been discussing the integration\nof indigenous knowledge and scientific knowledge for a more accurate\nenvironmental forecasting system in order to incorporate diverse environmental\ninformation for a reliable drought forecast. Hence, in this research, the core\nobjective is the development of a semantics-based data integration middleware\nthat encompasses and integrates heterogeneous data models of local indigenous\nknowledge and sensor data towards an accurate drought forecasting system for\nthe study areas. The local indigenous knowledge on drought gathered from the\ndomain experts is transformed into rules to be used for performing deductive\ninference in conjunction with sensors data for determining the onset of drought\nthrough an automated inference generation module of the middleware. The\nsemantic middleware incorporates, inter alia, a distributed architecture that\nconsists of a streaming data processing engine based on Apache Kafka for\nreal-time stream processing; a rule-based reasoning module; an ontology module\nfor semantic representation of the knowledge bases.\n", "link": "http://arxiv.org/abs/2405.10713v1", "date": "2024-05-17", "relevancy": 0.8127, "topK": [{"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.431}, {"title": "SDXS: Real-Time One-Step Latent Diffusion Models with Image Conditions", "link": "http://arxiv.org/abs/2403.16627v1", "similarity": 0.3969}, {"title": "Multimodal-Conditioned Latent Diffusion Models for Fashion Image Editing", "link": "http://arxiv.org/abs/2403.14828v2", "similarity": 0.3912}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Development%20of%20Semantics-Based%20Distributed%20Middleware%20for%20Heterogeneous%0A%20%20Data%20Integration%20and%20its%20Application%20for%20Drought&body=Title%3A%20Development%20of%20Semantics-Based%20Distributed%20Middleware%20for%20Heterogeneous%0A%20%20Data%20Integration%20and%20its%20Application%20for%20Drought%0AAuthor%3A%20A%20Akanbi%0AAbstract%3A%20%20%20Drought%20is%20a%20complex%20environmental%20phenomenon%20that%20affects%20millions%20of%20people%0Aand%20communities%20all%20over%20the%20globe%20and%20is%20too%20elusive%20to%20be%20accurately%0Apredicted.%20This%20is%20mostly%20due%20to%20the%20scalability%20and%20variability%20of%20the%20web%20of%0Aenvironmental%20parameters%20that%20directly/indirectly%20causes%20the%20onset%20of%20different%0Acategories%20of%20drought.%20Since%20the%20dawn%20of%20man%2C%20efforts%20have%20been%20made%20to%0Auniquely%20understand%20the%20natural%20indicators%20that%20provide%20signs%20of%20likely%0Aenvironmental%20events.%20These%20indicators/signs%20in%20the%20form%20of%20indigenous%0Aknowledge%20system%20have%20been%20used%20for%20generations.%20The%20intricate%20complexity%20of%0Adrought%20has%2C%20however%2C%20always%20been%20a%20major%20stumbling%20block%20for%20accurate%20drought%0Aprediction%20and%20forecasting%20systems.%20Recently%2C%20scientists%20in%20the%20field%20of%0Aagriculture%20and%20environmental%20monitoring%20have%20been%20discussing%20the%20integration%0Aof%20indigenous%20knowledge%20and%20scientific%20knowledge%20for%20a%20more%20accurate%0Aenvironmental%20forecasting%20system%20in%20order%20to%20incorporate%20diverse%20environmental%0Ainformation%20for%20a%20reliable%20drought%20forecast.%20Hence%2C%20in%20this%20research%2C%20the%20core%0Aobjective%20is%20the%20development%20of%20a%20semantics-based%20data%20integration%20middleware%0Athat%20encompasses%20and%20integrates%20heterogeneous%20data%20models%20of%20local%20indigenous%0Aknowledge%20and%20sensor%20data%20towards%20an%20accurate%20drought%20forecasting%20system%20for%0Athe%20study%20areas.%20The%20local%20indigenous%20knowledge%20on%20drought%20gathered%20from%20the%0Adomain%20experts%20is%20transformed%20into%20rules%20to%20be%20used%20for%20performing%20deductive%0Ainference%20in%20conjunction%20with%20sensors%20data%20for%20determining%20the%20onset%20of%20drought%0Athrough%20an%20automated%20inference%20generation%20module%20of%20the%20middleware.%20The%0Asemantic%20middleware%20incorporates%2C%20inter%20alia%2C%20a%20distributed%20architecture%20that%0Aconsists%20of%20a%20streaming%20data%20processing%20engine%20based%20on%20Apache%20Kafka%20for%0Areal-time%20stream%20processing%3B%20a%20rule-based%20reasoning%20module%3B%20an%20ontology%20module%0Afor%20semantic%20representation%20of%20the%20knowledge%20bases.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2405.10713v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DDevelopment%2520of%2520Semantics-Based%2520Distributed%2520Middleware%2520for%2520Heterogeneous%250A%2520%2520Data%2520Integration%2520and%2520its%2520Application%2520for%2520Drought%26entry.906535625%3DA%2520Akanbi%26entry.1292438233%3D%2520%2520Drought%2520is%2520a%2520complex%2520environmental%2520phenomenon%2520that%2520affects%2520millions%2520of%2520people%250Aand%2520communities%2520all%2520over%2520the%2520globe%2520and%2520is%2520too%2520elusive%2520to%2520be%2520accurately%250Apredicted.%2520This%2520is%2520mostly%2520due%2520to%2520the%2520scalability%2520and%2520variability%2520of%2520the%2520web%2520of%250Aenvironmental%2520parameters%2520that%2520directly/indirectly%2520causes%2520the%2520onset%2520of%2520different%250Acategories%2520of%2520drought.%2520Since%2520the%2520dawn%2520of%2520man%252C%2520efforts%2520have%2520been%2520made%2520to%250Auniquely%2520understand%2520the%2520natural%2520indicators%2520that%2520provide%2520signs%2520of%2520likely%250Aenvironmental%2520events.%2520These%2520indicators/signs%2520in%2520the%2520form%2520of%2520indigenous%250Aknowledge%2520system%2520have%2520been%2520used%2520for%2520generations.%2520The%2520intricate%2520complexity%2520of%250Adrought%2520has%252C%2520however%252C%2520always%2520been%2520a%2520major%2520stumbling%2520block%2520for%2520accurate%2520drought%250Aprediction%2520and%2520forecasting%2520systems.%2520Recently%252C%2520scientists%2520in%2520the%2520field%2520of%250Aagriculture%2520and%2520environmental%2520monitoring%2520have%2520been%2520discussing%2520the%2520integration%250Aof%2520indigenous%2520knowledge%2520and%2520scientific%2520knowledge%2520for%2520a%2520more%2520accurate%250Aenvironmental%2520forecasting%2520system%2520in%2520order%2520to%2520incorporate%2520diverse%2520environmental%250Ainformation%2520for%2520a%2520reliable%2520drought%2520forecast.%2520Hence%252C%2520in%2520this%2520research%252C%2520the%2520core%250Aobjective%2520is%2520the%2520development%2520of%2520a%2520semantics-based%2520data%2520integration%2520middleware%250Athat%2520encompasses%2520and%2520integrates%2520heterogeneous%2520data%2520models%2520of%2520local%2520indigenous%250Aknowledge%2520and%2520sensor%2520data%2520towards%2520an%2520accurate%2520drought%2520forecasting%2520system%2520for%250Athe%2520study%2520areas.%2520The%2520local%2520indigenous%2520knowledge%2520on%2520drought%2520gathered%2520from%2520the%250Adomain%2520experts%2520is%2520transformed%2520into%2520rules%2520to%2520be%2520used%2520for%2520performing%2520deductive%250Ainference%2520in%2520conjunction%2520with%2520sensors%2520data%2520for%2520determining%2520the%2520onset%2520of%2520drought%250Athrough%2520an%2520automated%2520inference%2520generation%2520module%2520of%2520the%2520middleware.%2520The%250Asemantic%2520middleware%2520incorporates%252C%2520inter%2520alia%252C%2520a%2520distributed%2520architecture%2520that%250Aconsists%2520of%2520a%2520streaming%2520data%2520processing%2520engine%2520based%2520on%2520Apache%2520Kafka%2520for%250Areal-time%2520stream%2520processing%253B%2520a%2520rule-based%2520reasoning%2520module%253B%2520an%2520ontology%2520module%250Afor%2520semantic%2520representation%2520of%2520the%2520knowledge%2520bases.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2405.10713v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Development%20of%20Semantics-Based%20Distributed%20Middleware%20for%20Heterogeneous%0A%20%20Data%20Integration%20and%20its%20Application%20for%20Drought&entry.906535625=A%20Akanbi&entry.1292438233=%20%20Drought%20is%20a%20complex%20environmental%20phenomenon%20that%20affects%20millions%20of%20people%0Aand%20communities%20all%20over%20the%20globe%20and%20is%20too%20elusive%20to%20be%20accurately%0Apredicted.%20This%20is%20mostly%20due%20to%20the%20scalability%20and%20variability%20of%20the%20web%20of%0Aenvironmental%20parameters%20that%20directly/indirectly%20causes%20the%20onset%20of%20different%0Acategories%20of%20drought.%20Since%20the%20dawn%20of%20man%2C%20efforts%20have%20been%20made%20to%0Auniquely%20understand%20the%20natural%20indicators%20that%20provide%20signs%20of%20likely%0Aenvironmental%20events.%20These%20indicators/signs%20in%20the%20form%20of%20indigenous%0Aknowledge%20system%20have%20been%20used%20for%20generations.%20The%20intricate%20complexity%20of%0Adrought%20has%2C%20however%2C%20always%20been%20a%20major%20stumbling%20block%20for%20accurate%20drought%0Aprediction%20and%20forecasting%20systems.%20Recently%2C%20scientists%20in%20the%20field%20of%0Aagriculture%20and%20environmental%20monitoring%20have%20been%20discussing%20the%20integration%0Aof%20indigenous%20knowledge%20and%20scientific%20knowledge%20for%20a%20more%20accurate%0Aenvironmental%20forecasting%20system%20in%20order%20to%20incorporate%20diverse%20environmental%0Ainformation%20for%20a%20reliable%20drought%20forecast.%20Hence%2C%20in%20this%20research%2C%20the%20core%0Aobjective%20is%20the%20development%20of%20a%20semantics-based%20data%20integration%20middleware%0Athat%20encompasses%20and%20integrates%20heterogeneous%20data%20models%20of%20local%20indigenous%0Aknowledge%20and%20sensor%20data%20towards%20an%20accurate%20drought%20forecasting%20system%20for%0Athe%20study%20areas.%20The%20local%20indigenous%20knowledge%20on%20drought%20gathered%20from%20the%0Adomain%20experts%20is%20transformed%20into%20rules%20to%20be%20used%20for%20performing%20deductive%0Ainference%20in%20conjunction%20with%20sensors%20data%20for%20determining%20the%20onset%20of%20drought%0Athrough%20an%20automated%20inference%20generation%20module%20of%20the%20middleware.%20The%0Asemantic%20middleware%20incorporates%2C%20inter%20alia%2C%20a%20distributed%20architecture%20that%0Aconsists%20of%20a%20streaming%20data%20processing%20engine%20based%20on%20Apache%20Kafka%20for%0Areal-time%20stream%20processing%3B%20a%20rule-based%20reasoning%20module%3B%20an%20ontology%20module%0Afor%20semantic%20representation%20of%20the%20knowledge%20bases.%0A&entry.1838667208=http%3A//arxiv.org/abs/2405.10713v1&entry.124074799=Read"},
      ];
      const content = document.getElementById('content');
      function createPostElement(post) {
        const postElement = document.createElement('div');
        postElement.className = 'post';
        const dateElem = document.createElement('p');
        dateElem.setAttribute("class", "date");
        dateElem.textContent = post.date;
        postElement.appendChild(dateElem);

        const textElem = document.createElement('p');
        textElem.setAttribute("class", "text");
        const titleElem = document.createElement('p');
        titleElem.setAttribute("class", "title");
        titleElem.textContent = post.title;
        textElem.appendChild(titleElem);
        const authorElem = document.createElement('p');
        authorElem.setAttribute("class", "author");
        authorElem.textContent = post.author;
        textElem.appendChild(authorElem);
        const abstractElem = document.createElement('p');
        abstractElem.setAttribute("class", "abstract");
        abstractElem.textContent = post.abstract;
        textElem.appendChild(abstractElem);

        const linkElement = document.createElement('a');
        linkElement.setAttribute("class", "link");
        linkElement.href = post.link;
        linkElement.target = "_blank";
        linkElement.textContent = post.link.length > 50 ? post.link.substring(0, 50) + '...' : post.link;
        textElem.appendChild(linkElement);
        postElement.appendChild(textElem);

        const linkElementContainer = document.createElement('div');
        linkElementContainer.setAttribute("class", "comment");
        const actionElement = document.createElement('a');
        actionElement.setAttribute("class", "comment");
        actionElement.href = post.form;
        actionElement.textContent = "Action";
        actionElement.target = "_blank";
        linkElementContainer.appendChild(actionElement);
        const emailElement = document.createElement('a');
        emailElement.setAttribute("class", "comment");
        emailElement.href = post.mailto;
        emailElement.textContent = "Email";
        emailElement.target = "_blank";
        linkElementContainer.appendChild(emailElement);
        postElement.appendChild(linkElementContainer);
        const e = document.createElement('div');
        e.setAttribute("class", "clear");
        postElement.appendChild(e);

        const relevancyContainer = document.createElement('div');
        const relevancyValElem = document.createElement('p');
        relevancyValElem.textContent = "Relevancy " + post.relevancy;
        relevancyContainer.appendChild(relevancyValElem);
        post.topK.forEach((sub) => {
          const topKElem = document.createElement('a');
          topKElem.setAttribute("class", "topK");
          topKElem.href = sub.link;
          topKElem.textContent = sub.title + " (" + sub.similarity + ")";
          topKElem.target = "_blank";
          relevancyContainer.appendChild(topKElem);
        });
        postElement.appendChild(relevancyContainer);
        return postElement;
      }
      function loadPosts() {
        // Simulate loading more posts
        posts.forEach((post) => {
          const postElement = createPostElement(post);
          content.appendChild(postElement);
        });
      }
      // Load initial posts
      loadPosts();
    </script>

  </body>
</html>


