<!DOCTYPE html>
<html lang="en">
  <head>
    <script async src="https://www.googletagmanager.com/gtag/js?id=G-V34CNNDP8V"></script>
    <script>
      window.dataLayer = window.dataLayer || [];
      function gtag(){dataLayer.push(arguments);}
      gtag('js', new Date());
      gtag('config', 'G-V34CNNDP8V');
    </script>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Arxiv Paper Selection</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.3/css/all.min.css" crossorigin="anonymous" referrerpolicy="no-referrer" />
    <style>
    body {
      font-family: Arial, sans-serif;
      margin: 0;
      padding: 0;
      background-color: #f4f4f4;
    }
    header {
      position: fixed;
      top: 0;
      left: 0;
      width: 100%;
      background-color: #ffffff;
      color: black;
      padding: 10px;
      text-align: center;
      z-index: 1000;
      border-bottom: 1px solid #ddd;
    }
    header div {
      display: block;
      margin: 10px auto;
    }

    #home-icon {
      display: block;
      float: left;
      margin: 5px;
      text-decoration: none;
      color: black;
    }

    main {
      margin-top: 60px; /* Adjusted margin to account for fixed header */
      padding: 20px;
    }

    .post {
      background-color: white;
      border: 1px solid #ddd;
      border-radius: 5px;
      margin-bottom: 10px;
      padding: 10px 20px;
      max-height: 2000px;
      overflow: scroll;
    }
    .post img {
      display: block;
      margin-top: 5px;
      max-width: auto;
      max-height: 100px;
    }
    .post .clear {
      clear: both;
      display: block;
    }
    .post a {
      text-decoration: none;
    }
    .post a:hover {
      color: #0056b3;
    }
    .post a:visited {
      color: #0056b3;
    }
    .post div.comment {
      text-align: right;
    }
    .post div.comment a {
      margin: 1em;
    }
    .post .text {
      margin: 1em 0em;
      padding: 0;
    }
    .post .text .title {
    }
    .post .text .author {
    }
    .post .text .abstract {
    }
    .post .topK {
      display: block;
      margin: 0.5em;
    }
    .post .date {
      margin: 0;
      padding: 0;
      text-size: small; 
      color: gray;
    }
    .post .link {
      margin: 0;
      padding: 0;
    }
    @media screen and (max-width: 600px) {
      body {
        max-width: 100%; 
      }
      #home-icon {
        float: none;
        display: block;
        text-align: center;
        margin-bottom: 10px;
      }
    }
    footer {
      width: 100%;
      background-color: #ddd;
      text-align: center;
      z-index: 1000;
      padding: 20px 0px;
      margin-bottom: 20px;
      left: 0;
    }

    #next-btn,
    #prev-btn {
      background-color: #4CAF50;
      color: white;
      padding: 8px 16px;
      margin: 0 50px;
      border: none;
      border-radius: 4px;
      cursor: pointer;
    }

    .links {
      padding: 20px;
    }
    .links a {
      text-decoration: none;
    }
    .links a:hover {
      color: #0056b3;
    }
    .links a:visited {
      color: #0056b3;
    }

    #page-index {
      font-size: small;
    }
    .ads {
      width: 100%;
    }
    #prev-btn {
      background-color: #4CAF50;
      color: white;
      padding: 8px 16px;
      margin: 0 50px;
      border: none;
      border-radius: 4px;
      cursor: pointer;
    }
    </style>
  </head>
  <body>

    <header>
      <a id="prev-btn" href="daily20251019.html"><i class="fas fa-chevron-left"></i></a>
      <a href="https://haoxiang.org/">About</a>
    </header>

    <main id="content">
      <!-- Posts will be dynamically added here using JavaScript -->
    </main>

    <script>
      // Dummy data for posts
      const posts = [
{"title": "Low-Frequency First: Eliminating Floating Artifacts in 3D Gaussian\n  Splatting", "author": "Jianchao Wang and Peng Zhou and Cen Li and Rong Quan and Jie Qin", "abstract": "  3D Gaussian Splatting (3DGS) is a powerful and computationally efficient\nrepresentation for 3D reconstruction. Despite its strengths, 3DGS often\nproduces floating artifacts, which are erroneous structures detached from the\nactual geometry and significantly degrade visual fidelity. The underlying\nmechanisms causing these artifacts, particularly in low-quality initialization\nscenarios, have not been fully explored. In this paper, we investigate the\norigins of floating artifacts from a frequency-domain perspective and identify\nunder-optimized Gaussians as the primary source. Based on our analysis, we\npropose \\textit{Eliminating-Floating-Artifacts} Gaussian Splatting (EFA-GS),\nwhich selectively expands under-optimized Gaussians to prioritize accurate\nlow-frequency learning. Additionally, we introduce complementary depth-based\nand scale-based strategies to dynamically refine Gaussian expansion,\neffectively mitigating detail erosion. Extensive experiments on both synthetic\nand real-world datasets demonstrate that EFA-GS substantially reduces floating\nartifacts while preserving high-frequency details, achieving an improvement of\n1.68 dB in PSNR over baseline method on our RWLQ dataset. Furthermore, we\nvalidate the effectiveness of our approach in downstream 3D editing tasks.\nProject Website: https://jcwang-gh.github.io/EFA-GS\n", "link": "http://arxiv.org/abs/2508.02493v3", "date": "2025-10-17", "relevancy": 3.2999, "topK": [{"title": "RTG-SLAM: Real-time 3D Reconstruction at Scale using Gaussian Splatting", "link": "http://arxiv.org/abs/2404.19706v1", "similarity": 0.6853}, {"title": "GST: Precise 3D Human Body from a Single Image with Gaussian Splatting\n  Transformers", "link": "http://arxiv.org/abs/2409.04196v1", "similarity": 0.6653}, {"title": "Dynamic Gaussians Mesh: Consistent Mesh Reconstruction from Monocular\n  Videos", "link": "http://arxiv.org/abs/2404.12379v2", "similarity": 0.6293}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Low-Frequency%20First%3A%20Eliminating%20Floating%20Artifacts%20in%203D%20Gaussian%0A%20%20Splatting&body=Title%3A%20Low-Frequency%20First%3A%20Eliminating%20Floating%20Artifacts%20in%203D%20Gaussian%0A%20%20Splatting%0AAuthor%3A%20Jianchao%20Wang%20and%20Peng%20Zhou%20and%20Cen%20Li%20and%20Rong%20Quan%20and%20Jie%20Qin%0AAbstract%3A%20%20%203D%20Gaussian%20Splatting%20%283DGS%29%20is%20a%20powerful%20and%20computationally%20efficient%0Arepresentation%20for%203D%20reconstruction.%20Despite%20its%20strengths%2C%203DGS%20often%0Aproduces%20floating%20artifacts%2C%20which%20are%20erroneous%20structures%20detached%20from%20the%0Aactual%20geometry%20and%20significantly%20degrade%20visual%20fidelity.%20The%20underlying%0Amechanisms%20causing%20these%20artifacts%2C%20particularly%20in%20low-quality%20initialization%0Ascenarios%2C%20have%20not%20been%20fully%20explored.%20In%20this%20paper%2C%20we%20investigate%20the%0Aorigins%20of%20floating%20artifacts%20from%20a%20frequency-domain%20perspective%20and%20identify%0Aunder-optimized%20Gaussians%20as%20the%20primary%20source.%20Based%20on%20our%20analysis%2C%20we%0Apropose%20%5Ctextit%7BEliminating-Floating-Artifacts%7D%20Gaussian%20Splatting%20%28EFA-GS%29%2C%0Awhich%20selectively%20expands%20under-optimized%20Gaussians%20to%20prioritize%20accurate%0Alow-frequency%20learning.%20Additionally%2C%20we%20introduce%20complementary%20depth-based%0Aand%20scale-based%20strategies%20to%20dynamically%20refine%20Gaussian%20expansion%2C%0Aeffectively%20mitigating%20detail%20erosion.%20Extensive%20experiments%20on%20both%20synthetic%0Aand%20real-world%20datasets%20demonstrate%20that%20EFA-GS%20substantially%20reduces%20floating%0Aartifacts%20while%20preserving%20high-frequency%20details%2C%20achieving%20an%20improvement%20of%0A1.68%20dB%20in%20PSNR%20over%20baseline%20method%20on%20our%20RWLQ%20dataset.%20Furthermore%2C%20we%0Avalidate%20the%20effectiveness%20of%20our%20approach%20in%20downstream%203D%20editing%20tasks.%0AProject%20Website%3A%20https%3A//jcwang-gh.github.io/EFA-GS%0A%0ALink%3A%20http%3A//arxiv.org/abs/2508.02493v3%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DLow-Frequency%2520First%253A%2520Eliminating%2520Floating%2520Artifacts%2520in%25203D%2520Gaussian%250A%2520%2520Splatting%26entry.906535625%3DJianchao%2520Wang%2520and%2520Peng%2520Zhou%2520and%2520Cen%2520Li%2520and%2520Rong%2520Quan%2520and%2520Jie%2520Qin%26entry.1292438233%3D%2520%25203D%2520Gaussian%2520Splatting%2520%25283DGS%2529%2520is%2520a%2520powerful%2520and%2520computationally%2520efficient%250Arepresentation%2520for%25203D%2520reconstruction.%2520Despite%2520its%2520strengths%252C%25203DGS%2520often%250Aproduces%2520floating%2520artifacts%252C%2520which%2520are%2520erroneous%2520structures%2520detached%2520from%2520the%250Aactual%2520geometry%2520and%2520significantly%2520degrade%2520visual%2520fidelity.%2520The%2520underlying%250Amechanisms%2520causing%2520these%2520artifacts%252C%2520particularly%2520in%2520low-quality%2520initialization%250Ascenarios%252C%2520have%2520not%2520been%2520fully%2520explored.%2520In%2520this%2520paper%252C%2520we%2520investigate%2520the%250Aorigins%2520of%2520floating%2520artifacts%2520from%2520a%2520frequency-domain%2520perspective%2520and%2520identify%250Aunder-optimized%2520Gaussians%2520as%2520the%2520primary%2520source.%2520Based%2520on%2520our%2520analysis%252C%2520we%250Apropose%2520%255Ctextit%257BEliminating-Floating-Artifacts%257D%2520Gaussian%2520Splatting%2520%2528EFA-GS%2529%252C%250Awhich%2520selectively%2520expands%2520under-optimized%2520Gaussians%2520to%2520prioritize%2520accurate%250Alow-frequency%2520learning.%2520Additionally%252C%2520we%2520introduce%2520complementary%2520depth-based%250Aand%2520scale-based%2520strategies%2520to%2520dynamically%2520refine%2520Gaussian%2520expansion%252C%250Aeffectively%2520mitigating%2520detail%2520erosion.%2520Extensive%2520experiments%2520on%2520both%2520synthetic%250Aand%2520real-world%2520datasets%2520demonstrate%2520that%2520EFA-GS%2520substantially%2520reduces%2520floating%250Aartifacts%2520while%2520preserving%2520high-frequency%2520details%252C%2520achieving%2520an%2520improvement%2520of%250A1.68%2520dB%2520in%2520PSNR%2520over%2520baseline%2520method%2520on%2520our%2520RWLQ%2520dataset.%2520Furthermore%252C%2520we%250Avalidate%2520the%2520effectiveness%2520of%2520our%2520approach%2520in%2520downstream%25203D%2520editing%2520tasks.%250AProject%2520Website%253A%2520https%253A//jcwang-gh.github.io/EFA-GS%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2508.02493v3%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Low-Frequency%20First%3A%20Eliminating%20Floating%20Artifacts%20in%203D%20Gaussian%0A%20%20Splatting&entry.906535625=Jianchao%20Wang%20and%20Peng%20Zhou%20and%20Cen%20Li%20and%20Rong%20Quan%20and%20Jie%20Qin&entry.1292438233=%20%203D%20Gaussian%20Splatting%20%283DGS%29%20is%20a%20powerful%20and%20computationally%20efficient%0Arepresentation%20for%203D%20reconstruction.%20Despite%20its%20strengths%2C%203DGS%20often%0Aproduces%20floating%20artifacts%2C%20which%20are%20erroneous%20structures%20detached%20from%20the%0Aactual%20geometry%20and%20significantly%20degrade%20visual%20fidelity.%20The%20underlying%0Amechanisms%20causing%20these%20artifacts%2C%20particularly%20in%20low-quality%20initialization%0Ascenarios%2C%20have%20not%20been%20fully%20explored.%20In%20this%20paper%2C%20we%20investigate%20the%0Aorigins%20of%20floating%20artifacts%20from%20a%20frequency-domain%20perspective%20and%20identify%0Aunder-optimized%20Gaussians%20as%20the%20primary%20source.%20Based%20on%20our%20analysis%2C%20we%0Apropose%20%5Ctextit%7BEliminating-Floating-Artifacts%7D%20Gaussian%20Splatting%20%28EFA-GS%29%2C%0Awhich%20selectively%20expands%20under-optimized%20Gaussians%20to%20prioritize%20accurate%0Alow-frequency%20learning.%20Additionally%2C%20we%20introduce%20complementary%20depth-based%0Aand%20scale-based%20strategies%20to%20dynamically%20refine%20Gaussian%20expansion%2C%0Aeffectively%20mitigating%20detail%20erosion.%20Extensive%20experiments%20on%20both%20synthetic%0Aand%20real-world%20datasets%20demonstrate%20that%20EFA-GS%20substantially%20reduces%20floating%0Aartifacts%20while%20preserving%20high-frequency%20details%2C%20achieving%20an%20improvement%20of%0A1.68%20dB%20in%20PSNR%20over%20baseline%20method%20on%20our%20RWLQ%20dataset.%20Furthermore%2C%20we%0Avalidate%20the%20effectiveness%20of%20our%20approach%20in%20downstream%203D%20editing%20tasks.%0AProject%20Website%3A%20https%3A//jcwang-gh.github.io/EFA-GS%0A&entry.1838667208=http%3A//arxiv.org/abs/2508.02493v3&entry.124074799=Read"},
{"title": "3DPR: Single Image 3D Portrait Relight using Generative Priors", "author": "Pramod Rao and Abhimitra Meka and Xilong Zhou and Gereon Fox and Mallikarjun B R and Fangneng Zhan and Tim Weyrich and Bernd Bickel and Hanspeter Pfister and Wojciech Matusik and Thabo Beeler and Mohamed Elgharib and Marc Habermann and Christian Theobalt", "abstract": "  Rendering novel, relit views of a human head, given a monocular portrait\nimage as input, is an inherently underconstrained problem. The traditional\ngraphics solution is to explicitly decompose the input image into geometry,\nmaterial and lighting via differentiable rendering; but this is constrained by\nthe multiple assumptions and approximations of the underlying models and\nparameterizations of these scene components. We propose 3DPR, an image-based\nrelighting model that leverages generative priors learnt from multi-view\nOne-Light-at-A-Time (OLAT) images captured in a light stage. We introduce a new\ndiverse and large-scale multi-view 4K OLAT dataset of 139 subjects to learn a\nhigh-quality prior over the distribution of high-frequency face reflectance. We\nleverage the latent space of a pre-trained generative head model that provides\na rich prior over face geometry learnt from in-the-wild image datasets. The\ninput portrait is first embedded in the latent manifold of such a model through\nan encoder-based inversion process. Then a novel triplane-based reflectance\nnetwork trained on our lightstage data is used to synthesize high-fidelity OLAT\nimages to enable image-based relighting. Our reflectance network operates in\nthe latent space of the generative head model, crucially enabling a relatively\nsmall number of lightstage images to train the reflectance model. Combining the\ngenerated OLATs according to a given HDRI environment maps yields physically\naccurate environmental relighting results. Through quantitative and qualitative\nevaluations, we demonstrate that 3DPR outperforms previous methods,\nparticularly in preserving identity and in capturing lighting effects such as\nspecularities, self-shadows, and subsurface scattering. Project Page:\nhttps://vcai.mpi-inf.mpg.de/projects/3dpr/\n", "link": "http://arxiv.org/abs/2510.15846v1", "date": "2025-10-17", "relevancy": 3.1605, "topK": [{"title": "CAT3D: Create Anything in 3D with Multi-View Diffusion Models", "link": "http://arxiv.org/abs/2405.10314v1", "similarity": 0.6321}, {"title": "CAT3D: Create Anything in 3D with Multi-View Diffusion Models", "link": "http://arxiv.org/abs/2405.10314v1", "similarity": 0.6321}, {"title": "Synthesizing Moving People with 3D Control", "link": "http://arxiv.org/abs/2401.10889v2", "similarity": 0.632}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%203DPR%3A%20Single%20Image%203D%20Portrait%20Relight%20using%20Generative%20Priors&body=Title%3A%203DPR%3A%20Single%20Image%203D%20Portrait%20Relight%20using%20Generative%20Priors%0AAuthor%3A%20Pramod%20Rao%20and%20Abhimitra%20Meka%20and%20Xilong%20Zhou%20and%20Gereon%20Fox%20and%20Mallikarjun%20B%20R%20and%20Fangneng%20Zhan%20and%20Tim%20Weyrich%20and%20Bernd%20Bickel%20and%20Hanspeter%20Pfister%20and%20Wojciech%20Matusik%20and%20Thabo%20Beeler%20and%20Mohamed%20Elgharib%20and%20Marc%20Habermann%20and%20Christian%20Theobalt%0AAbstract%3A%20%20%20Rendering%20novel%2C%20relit%20views%20of%20a%20human%20head%2C%20given%20a%20monocular%20portrait%0Aimage%20as%20input%2C%20is%20an%20inherently%20underconstrained%20problem.%20The%20traditional%0Agraphics%20solution%20is%20to%20explicitly%20decompose%20the%20input%20image%20into%20geometry%2C%0Amaterial%20and%20lighting%20via%20differentiable%20rendering%3B%20but%20this%20is%20constrained%20by%0Athe%20multiple%20assumptions%20and%20approximations%20of%20the%20underlying%20models%20and%0Aparameterizations%20of%20these%20scene%20components.%20We%20propose%203DPR%2C%20an%20image-based%0Arelighting%20model%20that%20leverages%20generative%20priors%20learnt%20from%20multi-view%0AOne-Light-at-A-Time%20%28OLAT%29%20images%20captured%20in%20a%20light%20stage.%20We%20introduce%20a%20new%0Adiverse%20and%20large-scale%20multi-view%204K%20OLAT%20dataset%20of%20139%20subjects%20to%20learn%20a%0Ahigh-quality%20prior%20over%20the%20distribution%20of%20high-frequency%20face%20reflectance.%20We%0Aleverage%20the%20latent%20space%20of%20a%20pre-trained%20generative%20head%20model%20that%20provides%0Aa%20rich%20prior%20over%20face%20geometry%20learnt%20from%20in-the-wild%20image%20datasets.%20The%0Ainput%20portrait%20is%20first%20embedded%20in%20the%20latent%20manifold%20of%20such%20a%20model%20through%0Aan%20encoder-based%20inversion%20process.%20Then%20a%20novel%20triplane-based%20reflectance%0Anetwork%20trained%20on%20our%20lightstage%20data%20is%20used%20to%20synthesize%20high-fidelity%20OLAT%0Aimages%20to%20enable%20image-based%20relighting.%20Our%20reflectance%20network%20operates%20in%0Athe%20latent%20space%20of%20the%20generative%20head%20model%2C%20crucially%20enabling%20a%20relatively%0Asmall%20number%20of%20lightstage%20images%20to%20train%20the%20reflectance%20model.%20Combining%20the%0Agenerated%20OLATs%20according%20to%20a%20given%20HDRI%20environment%20maps%20yields%20physically%0Aaccurate%20environmental%20relighting%20results.%20Through%20quantitative%20and%20qualitative%0Aevaluations%2C%20we%20demonstrate%20that%203DPR%20outperforms%20previous%20methods%2C%0Aparticularly%20in%20preserving%20identity%20and%20in%20capturing%20lighting%20effects%20such%20as%0Aspecularities%2C%20self-shadows%2C%20and%20subsurface%20scattering.%20Project%20Page%3A%0Ahttps%3A//vcai.mpi-inf.mpg.de/projects/3dpr/%0A%0ALink%3A%20http%3A//arxiv.org/abs/2510.15846v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3D3DPR%253A%2520Single%2520Image%25203D%2520Portrait%2520Relight%2520using%2520Generative%2520Priors%26entry.906535625%3DPramod%2520Rao%2520and%2520Abhimitra%2520Meka%2520and%2520Xilong%2520Zhou%2520and%2520Gereon%2520Fox%2520and%2520Mallikarjun%2520B%2520R%2520and%2520Fangneng%2520Zhan%2520and%2520Tim%2520Weyrich%2520and%2520Bernd%2520Bickel%2520and%2520Hanspeter%2520Pfister%2520and%2520Wojciech%2520Matusik%2520and%2520Thabo%2520Beeler%2520and%2520Mohamed%2520Elgharib%2520and%2520Marc%2520Habermann%2520and%2520Christian%2520Theobalt%26entry.1292438233%3D%2520%2520Rendering%2520novel%252C%2520relit%2520views%2520of%2520a%2520human%2520head%252C%2520given%2520a%2520monocular%2520portrait%250Aimage%2520as%2520input%252C%2520is%2520an%2520inherently%2520underconstrained%2520problem.%2520The%2520traditional%250Agraphics%2520solution%2520is%2520to%2520explicitly%2520decompose%2520the%2520input%2520image%2520into%2520geometry%252C%250Amaterial%2520and%2520lighting%2520via%2520differentiable%2520rendering%253B%2520but%2520this%2520is%2520constrained%2520by%250Athe%2520multiple%2520assumptions%2520and%2520approximations%2520of%2520the%2520underlying%2520models%2520and%250Aparameterizations%2520of%2520these%2520scene%2520components.%2520We%2520propose%25203DPR%252C%2520an%2520image-based%250Arelighting%2520model%2520that%2520leverages%2520generative%2520priors%2520learnt%2520from%2520multi-view%250AOne-Light-at-A-Time%2520%2528OLAT%2529%2520images%2520captured%2520in%2520a%2520light%2520stage.%2520We%2520introduce%2520a%2520new%250Adiverse%2520and%2520large-scale%2520multi-view%25204K%2520OLAT%2520dataset%2520of%2520139%2520subjects%2520to%2520learn%2520a%250Ahigh-quality%2520prior%2520over%2520the%2520distribution%2520of%2520high-frequency%2520face%2520reflectance.%2520We%250Aleverage%2520the%2520latent%2520space%2520of%2520a%2520pre-trained%2520generative%2520head%2520model%2520that%2520provides%250Aa%2520rich%2520prior%2520over%2520face%2520geometry%2520learnt%2520from%2520in-the-wild%2520image%2520datasets.%2520The%250Ainput%2520portrait%2520is%2520first%2520embedded%2520in%2520the%2520latent%2520manifold%2520of%2520such%2520a%2520model%2520through%250Aan%2520encoder-based%2520inversion%2520process.%2520Then%2520a%2520novel%2520triplane-based%2520reflectance%250Anetwork%2520trained%2520on%2520our%2520lightstage%2520data%2520is%2520used%2520to%2520synthesize%2520high-fidelity%2520OLAT%250Aimages%2520to%2520enable%2520image-based%2520relighting.%2520Our%2520reflectance%2520network%2520operates%2520in%250Athe%2520latent%2520space%2520of%2520the%2520generative%2520head%2520model%252C%2520crucially%2520enabling%2520a%2520relatively%250Asmall%2520number%2520of%2520lightstage%2520images%2520to%2520train%2520the%2520reflectance%2520model.%2520Combining%2520the%250Agenerated%2520OLATs%2520according%2520to%2520a%2520given%2520HDRI%2520environment%2520maps%2520yields%2520physically%250Aaccurate%2520environmental%2520relighting%2520results.%2520Through%2520quantitative%2520and%2520qualitative%250Aevaluations%252C%2520we%2520demonstrate%2520that%25203DPR%2520outperforms%2520previous%2520methods%252C%250Aparticularly%2520in%2520preserving%2520identity%2520and%2520in%2520capturing%2520lighting%2520effects%2520such%2520as%250Aspecularities%252C%2520self-shadows%252C%2520and%2520subsurface%2520scattering.%2520Project%2520Page%253A%250Ahttps%253A//vcai.mpi-inf.mpg.de/projects/3dpr/%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2510.15846v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=3DPR%3A%20Single%20Image%203D%20Portrait%20Relight%20using%20Generative%20Priors&entry.906535625=Pramod%20Rao%20and%20Abhimitra%20Meka%20and%20Xilong%20Zhou%20and%20Gereon%20Fox%20and%20Mallikarjun%20B%20R%20and%20Fangneng%20Zhan%20and%20Tim%20Weyrich%20and%20Bernd%20Bickel%20and%20Hanspeter%20Pfister%20and%20Wojciech%20Matusik%20and%20Thabo%20Beeler%20and%20Mohamed%20Elgharib%20and%20Marc%20Habermann%20and%20Christian%20Theobalt&entry.1292438233=%20%20Rendering%20novel%2C%20relit%20views%20of%20a%20human%20head%2C%20given%20a%20monocular%20portrait%0Aimage%20as%20input%2C%20is%20an%20inherently%20underconstrained%20problem.%20The%20traditional%0Agraphics%20solution%20is%20to%20explicitly%20decompose%20the%20input%20image%20into%20geometry%2C%0Amaterial%20and%20lighting%20via%20differentiable%20rendering%3B%20but%20this%20is%20constrained%20by%0Athe%20multiple%20assumptions%20and%20approximations%20of%20the%20underlying%20models%20and%0Aparameterizations%20of%20these%20scene%20components.%20We%20propose%203DPR%2C%20an%20image-based%0Arelighting%20model%20that%20leverages%20generative%20priors%20learnt%20from%20multi-view%0AOne-Light-at-A-Time%20%28OLAT%29%20images%20captured%20in%20a%20light%20stage.%20We%20introduce%20a%20new%0Adiverse%20and%20large-scale%20multi-view%204K%20OLAT%20dataset%20of%20139%20subjects%20to%20learn%20a%0Ahigh-quality%20prior%20over%20the%20distribution%20of%20high-frequency%20face%20reflectance.%20We%0Aleverage%20the%20latent%20space%20of%20a%20pre-trained%20generative%20head%20model%20that%20provides%0Aa%20rich%20prior%20over%20face%20geometry%20learnt%20from%20in-the-wild%20image%20datasets.%20The%0Ainput%20portrait%20is%20first%20embedded%20in%20the%20latent%20manifold%20of%20such%20a%20model%20through%0Aan%20encoder-based%20inversion%20process.%20Then%20a%20novel%20triplane-based%20reflectance%0Anetwork%20trained%20on%20our%20lightstage%20data%20is%20used%20to%20synthesize%20high-fidelity%20OLAT%0Aimages%20to%20enable%20image-based%20relighting.%20Our%20reflectance%20network%20operates%20in%0Athe%20latent%20space%20of%20the%20generative%20head%20model%2C%20crucially%20enabling%20a%20relatively%0Asmall%20number%20of%20lightstage%20images%20to%20train%20the%20reflectance%20model.%20Combining%20the%0Agenerated%20OLATs%20according%20to%20a%20given%20HDRI%20environment%20maps%20yields%20physically%0Aaccurate%20environmental%20relighting%20results.%20Through%20quantitative%20and%20qualitative%0Aevaluations%2C%20we%20demonstrate%20that%203DPR%20outperforms%20previous%20methods%2C%0Aparticularly%20in%20preserving%20identity%20and%20in%20capturing%20lighting%20effects%20such%20as%0Aspecularities%2C%20self-shadows%2C%20and%20subsurface%20scattering.%20Project%20Page%3A%0Ahttps%3A//vcai.mpi-inf.mpg.de/projects/3dpr/%0A&entry.1838667208=http%3A//arxiv.org/abs/2510.15846v1&entry.124074799=Read"},
{"title": "Fix False Transparency by Noise Guided Splatting", "author": "Aly El Hakie and Yiren Lu and Yu Yin and Michael Jenkins and Yehe Liu", "abstract": "  Opaque objects reconstructed by 3DGS often exhibit a falsely transparent\nsurface, leading to inconsistent background and internal patterns under camera\nmotion in interactive viewing. This issue stems from the ill-posed optimization\nin 3DGS. During training, background and foreground Gaussians are blended via\nalpha-compositing and optimized solely against the input RGB images using a\nphotometric loss. As this process lacks an explicit constraint on surface\nopacity, the optimization may incorrectly assign transparency to opaque\nregions, resulting in view-inconsistent and falsely transparent. This issue is\ndifficult to detect in standard evaluation settings but becomes particularly\nevident in object-centric reconstructions under interactive viewing. Although\nother causes of view-inconsistency have been explored recently, false\ntransparency has not been explicitly identified. To the best of our knowledge,\nwe are the first to identify, characterize, and develop solutions for this\nartifact, an underreported artifact in 3DGS. Our strategy, NGS, encourages\nsurface Gaussians to adopt higher opacity by injecting opaque noise Gaussians\nin the object volume during training, requiring only minimal modifications to\nthe existing splatting process. To quantitatively evaluate false transparency\nin static renderings, we propose a transmittance-based metric that measures the\nseverity of this artifact. In addition, we introduce a customized, high-quality\nobject-centric scan dataset exhibiting pronounced transparency issues, and we\naugment popular existing datasets with complementary infill noise specifically\ndesigned to assess the robustness of 3D reconstruction methods to false\ntransparency. Experiments across multiple datasets show that NGS substantially\nreduces false transparency while maintaining competitive performance on\nstandard rendering metrics, demonstrating its overall effectiveness.\n", "link": "http://arxiv.org/abs/2510.15736v1", "date": "2025-10-17", "relevancy": 3.1542, "topK": [{"title": "RTG-SLAM: Real-time 3D Reconstruction at Scale using Gaussian Splatting", "link": "http://arxiv.org/abs/2404.19706v1", "similarity": 0.6767}, {"title": "GST: Precise 3D Human Body from a Single Image with Gaussian Splatting\n  Transformers", "link": "http://arxiv.org/abs/2409.04196v1", "similarity": 0.6128}, {"title": "Dynamic Gaussians Mesh: Consistent Mesh Reconstruction from Monocular\n  Videos", "link": "http://arxiv.org/abs/2404.12379v2", "similarity": 0.603}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Fix%20False%20Transparency%20by%20Noise%20Guided%20Splatting&body=Title%3A%20Fix%20False%20Transparency%20by%20Noise%20Guided%20Splatting%0AAuthor%3A%20Aly%20El%20Hakie%20and%20Yiren%20Lu%20and%20Yu%20Yin%20and%20Michael%20Jenkins%20and%20Yehe%20Liu%0AAbstract%3A%20%20%20Opaque%20objects%20reconstructed%20by%203DGS%20often%20exhibit%20a%20falsely%20transparent%0Asurface%2C%20leading%20to%20inconsistent%20background%20and%20internal%20patterns%20under%20camera%0Amotion%20in%20interactive%20viewing.%20This%20issue%20stems%20from%20the%20ill-posed%20optimization%0Ain%203DGS.%20During%20training%2C%20background%20and%20foreground%20Gaussians%20are%20blended%20via%0Aalpha-compositing%20and%20optimized%20solely%20against%20the%20input%20RGB%20images%20using%20a%0Aphotometric%20loss.%20As%20this%20process%20lacks%20an%20explicit%20constraint%20on%20surface%0Aopacity%2C%20the%20optimization%20may%20incorrectly%20assign%20transparency%20to%20opaque%0Aregions%2C%20resulting%20in%20view-inconsistent%20and%20falsely%20transparent.%20This%20issue%20is%0Adifficult%20to%20detect%20in%20standard%20evaluation%20settings%20but%20becomes%20particularly%0Aevident%20in%20object-centric%20reconstructions%20under%20interactive%20viewing.%20Although%0Aother%20causes%20of%20view-inconsistency%20have%20been%20explored%20recently%2C%20false%0Atransparency%20has%20not%20been%20explicitly%20identified.%20To%20the%20best%20of%20our%20knowledge%2C%0Awe%20are%20the%20first%20to%20identify%2C%20characterize%2C%20and%20develop%20solutions%20for%20this%0Aartifact%2C%20an%20underreported%20artifact%20in%203DGS.%20Our%20strategy%2C%20NGS%2C%20encourages%0Asurface%20Gaussians%20to%20adopt%20higher%20opacity%20by%20injecting%20opaque%20noise%20Gaussians%0Ain%20the%20object%20volume%20during%20training%2C%20requiring%20only%20minimal%20modifications%20to%0Athe%20existing%20splatting%20process.%20To%20quantitatively%20evaluate%20false%20transparency%0Ain%20static%20renderings%2C%20we%20propose%20a%20transmittance-based%20metric%20that%20measures%20the%0Aseverity%20of%20this%20artifact.%20In%20addition%2C%20we%20introduce%20a%20customized%2C%20high-quality%0Aobject-centric%20scan%20dataset%20exhibiting%20pronounced%20transparency%20issues%2C%20and%20we%0Aaugment%20popular%20existing%20datasets%20with%20complementary%20infill%20noise%20specifically%0Adesigned%20to%20assess%20the%20robustness%20of%203D%20reconstruction%20methods%20to%20false%0Atransparency.%20Experiments%20across%20multiple%20datasets%20show%20that%20NGS%20substantially%0Areduces%20false%20transparency%20while%20maintaining%20competitive%20performance%20on%0Astandard%20rendering%20metrics%2C%20demonstrating%20its%20overall%20effectiveness.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2510.15736v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DFix%2520False%2520Transparency%2520by%2520Noise%2520Guided%2520Splatting%26entry.906535625%3DAly%2520El%2520Hakie%2520and%2520Yiren%2520Lu%2520and%2520Yu%2520Yin%2520and%2520Michael%2520Jenkins%2520and%2520Yehe%2520Liu%26entry.1292438233%3D%2520%2520Opaque%2520objects%2520reconstructed%2520by%25203DGS%2520often%2520exhibit%2520a%2520falsely%2520transparent%250Asurface%252C%2520leading%2520to%2520inconsistent%2520background%2520and%2520internal%2520patterns%2520under%2520camera%250Amotion%2520in%2520interactive%2520viewing.%2520This%2520issue%2520stems%2520from%2520the%2520ill-posed%2520optimization%250Ain%25203DGS.%2520During%2520training%252C%2520background%2520and%2520foreground%2520Gaussians%2520are%2520blended%2520via%250Aalpha-compositing%2520and%2520optimized%2520solely%2520against%2520the%2520input%2520RGB%2520images%2520using%2520a%250Aphotometric%2520loss.%2520As%2520this%2520process%2520lacks%2520an%2520explicit%2520constraint%2520on%2520surface%250Aopacity%252C%2520the%2520optimization%2520may%2520incorrectly%2520assign%2520transparency%2520to%2520opaque%250Aregions%252C%2520resulting%2520in%2520view-inconsistent%2520and%2520falsely%2520transparent.%2520This%2520issue%2520is%250Adifficult%2520to%2520detect%2520in%2520standard%2520evaluation%2520settings%2520but%2520becomes%2520particularly%250Aevident%2520in%2520object-centric%2520reconstructions%2520under%2520interactive%2520viewing.%2520Although%250Aother%2520causes%2520of%2520view-inconsistency%2520have%2520been%2520explored%2520recently%252C%2520false%250Atransparency%2520has%2520not%2520been%2520explicitly%2520identified.%2520To%2520the%2520best%2520of%2520our%2520knowledge%252C%250Awe%2520are%2520the%2520first%2520to%2520identify%252C%2520characterize%252C%2520and%2520develop%2520solutions%2520for%2520this%250Aartifact%252C%2520an%2520underreported%2520artifact%2520in%25203DGS.%2520Our%2520strategy%252C%2520NGS%252C%2520encourages%250Asurface%2520Gaussians%2520to%2520adopt%2520higher%2520opacity%2520by%2520injecting%2520opaque%2520noise%2520Gaussians%250Ain%2520the%2520object%2520volume%2520during%2520training%252C%2520requiring%2520only%2520minimal%2520modifications%2520to%250Athe%2520existing%2520splatting%2520process.%2520To%2520quantitatively%2520evaluate%2520false%2520transparency%250Ain%2520static%2520renderings%252C%2520we%2520propose%2520a%2520transmittance-based%2520metric%2520that%2520measures%2520the%250Aseverity%2520of%2520this%2520artifact.%2520In%2520addition%252C%2520we%2520introduce%2520a%2520customized%252C%2520high-quality%250Aobject-centric%2520scan%2520dataset%2520exhibiting%2520pronounced%2520transparency%2520issues%252C%2520and%2520we%250Aaugment%2520popular%2520existing%2520datasets%2520with%2520complementary%2520infill%2520noise%2520specifically%250Adesigned%2520to%2520assess%2520the%2520robustness%2520of%25203D%2520reconstruction%2520methods%2520to%2520false%250Atransparency.%2520Experiments%2520across%2520multiple%2520datasets%2520show%2520that%2520NGS%2520substantially%250Areduces%2520false%2520transparency%2520while%2520maintaining%2520competitive%2520performance%2520on%250Astandard%2520rendering%2520metrics%252C%2520demonstrating%2520its%2520overall%2520effectiveness.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2510.15736v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Fix%20False%20Transparency%20by%20Noise%20Guided%20Splatting&entry.906535625=Aly%20El%20Hakie%20and%20Yiren%20Lu%20and%20Yu%20Yin%20and%20Michael%20Jenkins%20and%20Yehe%20Liu&entry.1292438233=%20%20Opaque%20objects%20reconstructed%20by%203DGS%20often%20exhibit%20a%20falsely%20transparent%0Asurface%2C%20leading%20to%20inconsistent%20background%20and%20internal%20patterns%20under%20camera%0Amotion%20in%20interactive%20viewing.%20This%20issue%20stems%20from%20the%20ill-posed%20optimization%0Ain%203DGS.%20During%20training%2C%20background%20and%20foreground%20Gaussians%20are%20blended%20via%0Aalpha-compositing%20and%20optimized%20solely%20against%20the%20input%20RGB%20images%20using%20a%0Aphotometric%20loss.%20As%20this%20process%20lacks%20an%20explicit%20constraint%20on%20surface%0Aopacity%2C%20the%20optimization%20may%20incorrectly%20assign%20transparency%20to%20opaque%0Aregions%2C%20resulting%20in%20view-inconsistent%20and%20falsely%20transparent.%20This%20issue%20is%0Adifficult%20to%20detect%20in%20standard%20evaluation%20settings%20but%20becomes%20particularly%0Aevident%20in%20object-centric%20reconstructions%20under%20interactive%20viewing.%20Although%0Aother%20causes%20of%20view-inconsistency%20have%20been%20explored%20recently%2C%20false%0Atransparency%20has%20not%20been%20explicitly%20identified.%20To%20the%20best%20of%20our%20knowledge%2C%0Awe%20are%20the%20first%20to%20identify%2C%20characterize%2C%20and%20develop%20solutions%20for%20this%0Aartifact%2C%20an%20underreported%20artifact%20in%203DGS.%20Our%20strategy%2C%20NGS%2C%20encourages%0Asurface%20Gaussians%20to%20adopt%20higher%20opacity%20by%20injecting%20opaque%20noise%20Gaussians%0Ain%20the%20object%20volume%20during%20training%2C%20requiring%20only%20minimal%20modifications%20to%0Athe%20existing%20splatting%20process.%20To%20quantitatively%20evaluate%20false%20transparency%0Ain%20static%20renderings%2C%20we%20propose%20a%20transmittance-based%20metric%20that%20measures%20the%0Aseverity%20of%20this%20artifact.%20In%20addition%2C%20we%20introduce%20a%20customized%2C%20high-quality%0Aobject-centric%20scan%20dataset%20exhibiting%20pronounced%20transparency%20issues%2C%20and%20we%0Aaugment%20popular%20existing%20datasets%20with%20complementary%20infill%20noise%20specifically%0Adesigned%20to%20assess%20the%20robustness%20of%203D%20reconstruction%20methods%20to%20false%0Atransparency.%20Experiments%20across%20multiple%20datasets%20show%20that%20NGS%20substantially%0Areduces%20false%20transparency%20while%20maintaining%20competitive%20performance%20on%0Astandard%20rendering%20metrics%2C%20demonstrating%20its%20overall%20effectiveness.%0A&entry.1838667208=http%3A//arxiv.org/abs/2510.15736v1&entry.124074799=Read"},
{"title": "OmniVinci: Enhancing Architecture and Data for Omni-Modal Understanding\n  LLM", "author": "Hanrong Ye and Chao-Han Huck Yang and Arushi Goel and Wei Huang and Ligeng Zhu and Yuanhang Su and Sean Lin and An-Chieh Cheng and Zhen Wan and Jinchuan Tian and Yuming Lou and Dong Yang and Zhijian Liu and Yukang Chen and Ambrish Dantrey and Ehsan Jahangiri and Sreyan Ghosh and Daguang Xu and Ehsan Hosseini-Asl and Danial Mohseni Taheri and Vidya Murali and Sifei Liu and Jason Lu and Oluwatobi Olabiyi and Frank Wang and Rafael Valle and Bryan Catanzaro and Andrew Tao and Song Han and Jan Kautz and Hongxu Yin and Pavlo Molchanov", "abstract": "  Advancing machine intelligence requires developing the ability to perceive\nacross multiple modalities, much as humans sense the world. We introduce\nOmniVinci, an initiative to build a strong, open-source, omni-modal LLM. We\ncarefully study the design choices across model architecture and data curation.\nFor model architecture, we present three key innovations: (i) OmniAlignNet for\nstrengthening alignment between vision and audio embeddings in a shared\nomni-modal latent space; (ii) Temporal Embedding Grouping for capturing\nrelative temporal alignment between vision and audio signals; and (iii)\nConstrained Rotary Time Embedding for encoding absolute temporal information in\nomni-modal embeddings. We introduce a curation and synthesis pipeline that\ngenerates 24M single-modal and omni-modal conversations. We find that\nmodalities reinforce one another in both perception and reasoning. Our model,\nOmniVinci, outperforms Qwen2.5-Omni with +19.05 on DailyOmni (cross-modal\nunderstanding), +1.7 on MMAR (audio), and +3.9 on Video-MME (vision), while\nusing just 0.2T training tokens - a 6 times reduction compared to\nQwen2.5-Omni's 1.2T. We finally demonstrate omni-modal advantages in downstream\napplications spanning robotics, medical AI, and smart factory.\n", "link": "http://arxiv.org/abs/2510.15870v1", "date": "2025-10-17", "relevancy": 3.0606, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.618}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.618}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.6004}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20OmniVinci%3A%20Enhancing%20Architecture%20and%20Data%20for%20Omni-Modal%20Understanding%0A%20%20LLM&body=Title%3A%20OmniVinci%3A%20Enhancing%20Architecture%20and%20Data%20for%20Omni-Modal%20Understanding%0A%20%20LLM%0AAuthor%3A%20Hanrong%20Ye%20and%20Chao-Han%20Huck%20Yang%20and%20Arushi%20Goel%20and%20Wei%20Huang%20and%20Ligeng%20Zhu%20and%20Yuanhang%20Su%20and%20Sean%20Lin%20and%20An-Chieh%20Cheng%20and%20Zhen%20Wan%20and%20Jinchuan%20Tian%20and%20Yuming%20Lou%20and%20Dong%20Yang%20and%20Zhijian%20Liu%20and%20Yukang%20Chen%20and%20Ambrish%20Dantrey%20and%20Ehsan%20Jahangiri%20and%20Sreyan%20Ghosh%20and%20Daguang%20Xu%20and%20Ehsan%20Hosseini-Asl%20and%20Danial%20Mohseni%20Taheri%20and%20Vidya%20Murali%20and%20Sifei%20Liu%20and%20Jason%20Lu%20and%20Oluwatobi%20Olabiyi%20and%20Frank%20Wang%20and%20Rafael%20Valle%20and%20Bryan%20Catanzaro%20and%20Andrew%20Tao%20and%20Song%20Han%20and%20Jan%20Kautz%20and%20Hongxu%20Yin%20and%20Pavlo%20Molchanov%0AAbstract%3A%20%20%20Advancing%20machine%20intelligence%20requires%20developing%20the%20ability%20to%20perceive%0Aacross%20multiple%20modalities%2C%20much%20as%20humans%20sense%20the%20world.%20We%20introduce%0AOmniVinci%2C%20an%20initiative%20to%20build%20a%20strong%2C%20open-source%2C%20omni-modal%20LLM.%20We%0Acarefully%20study%20the%20design%20choices%20across%20model%20architecture%20and%20data%20curation.%0AFor%20model%20architecture%2C%20we%20present%20three%20key%20innovations%3A%20%28i%29%20OmniAlignNet%20for%0Astrengthening%20alignment%20between%20vision%20and%20audio%20embeddings%20in%20a%20shared%0Aomni-modal%20latent%20space%3B%20%28ii%29%20Temporal%20Embedding%20Grouping%20for%20capturing%0Arelative%20temporal%20alignment%20between%20vision%20and%20audio%20signals%3B%20and%20%28iii%29%0AConstrained%20Rotary%20Time%20Embedding%20for%20encoding%20absolute%20temporal%20information%20in%0Aomni-modal%20embeddings.%20We%20introduce%20a%20curation%20and%20synthesis%20pipeline%20that%0Agenerates%2024M%20single-modal%20and%20omni-modal%20conversations.%20We%20find%20that%0Amodalities%20reinforce%20one%20another%20in%20both%20perception%20and%20reasoning.%20Our%20model%2C%0AOmniVinci%2C%20outperforms%20Qwen2.5-Omni%20with%20%2B19.05%20on%20DailyOmni%20%28cross-modal%0Aunderstanding%29%2C%20%2B1.7%20on%20MMAR%20%28audio%29%2C%20and%20%2B3.9%20on%20Video-MME%20%28vision%29%2C%20while%0Ausing%20just%200.2T%20training%20tokens%20-%20a%206%20times%20reduction%20compared%20to%0AQwen2.5-Omni%27s%201.2T.%20We%20finally%20demonstrate%20omni-modal%20advantages%20in%20downstream%0Aapplications%20spanning%20robotics%2C%20medical%20AI%2C%20and%20smart%20factory.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2510.15870v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DOmniVinci%253A%2520Enhancing%2520Architecture%2520and%2520Data%2520for%2520Omni-Modal%2520Understanding%250A%2520%2520LLM%26entry.906535625%3DHanrong%2520Ye%2520and%2520Chao-Han%2520Huck%2520Yang%2520and%2520Arushi%2520Goel%2520and%2520Wei%2520Huang%2520and%2520Ligeng%2520Zhu%2520and%2520Yuanhang%2520Su%2520and%2520Sean%2520Lin%2520and%2520An-Chieh%2520Cheng%2520and%2520Zhen%2520Wan%2520and%2520Jinchuan%2520Tian%2520and%2520Yuming%2520Lou%2520and%2520Dong%2520Yang%2520and%2520Zhijian%2520Liu%2520and%2520Yukang%2520Chen%2520and%2520Ambrish%2520Dantrey%2520and%2520Ehsan%2520Jahangiri%2520and%2520Sreyan%2520Ghosh%2520and%2520Daguang%2520Xu%2520and%2520Ehsan%2520Hosseini-Asl%2520and%2520Danial%2520Mohseni%2520Taheri%2520and%2520Vidya%2520Murali%2520and%2520Sifei%2520Liu%2520and%2520Jason%2520Lu%2520and%2520Oluwatobi%2520Olabiyi%2520and%2520Frank%2520Wang%2520and%2520Rafael%2520Valle%2520and%2520Bryan%2520Catanzaro%2520and%2520Andrew%2520Tao%2520and%2520Song%2520Han%2520and%2520Jan%2520Kautz%2520and%2520Hongxu%2520Yin%2520and%2520Pavlo%2520Molchanov%26entry.1292438233%3D%2520%2520Advancing%2520machine%2520intelligence%2520requires%2520developing%2520the%2520ability%2520to%2520perceive%250Aacross%2520multiple%2520modalities%252C%2520much%2520as%2520humans%2520sense%2520the%2520world.%2520We%2520introduce%250AOmniVinci%252C%2520an%2520initiative%2520to%2520build%2520a%2520strong%252C%2520open-source%252C%2520omni-modal%2520LLM.%2520We%250Acarefully%2520study%2520the%2520design%2520choices%2520across%2520model%2520architecture%2520and%2520data%2520curation.%250AFor%2520model%2520architecture%252C%2520we%2520present%2520three%2520key%2520innovations%253A%2520%2528i%2529%2520OmniAlignNet%2520for%250Astrengthening%2520alignment%2520between%2520vision%2520and%2520audio%2520embeddings%2520in%2520a%2520shared%250Aomni-modal%2520latent%2520space%253B%2520%2528ii%2529%2520Temporal%2520Embedding%2520Grouping%2520for%2520capturing%250Arelative%2520temporal%2520alignment%2520between%2520vision%2520and%2520audio%2520signals%253B%2520and%2520%2528iii%2529%250AConstrained%2520Rotary%2520Time%2520Embedding%2520for%2520encoding%2520absolute%2520temporal%2520information%2520in%250Aomni-modal%2520embeddings.%2520We%2520introduce%2520a%2520curation%2520and%2520synthesis%2520pipeline%2520that%250Agenerates%252024M%2520single-modal%2520and%2520omni-modal%2520conversations.%2520We%2520find%2520that%250Amodalities%2520reinforce%2520one%2520another%2520in%2520both%2520perception%2520and%2520reasoning.%2520Our%2520model%252C%250AOmniVinci%252C%2520outperforms%2520Qwen2.5-Omni%2520with%2520%252B19.05%2520on%2520DailyOmni%2520%2528cross-modal%250Aunderstanding%2529%252C%2520%252B1.7%2520on%2520MMAR%2520%2528audio%2529%252C%2520and%2520%252B3.9%2520on%2520Video-MME%2520%2528vision%2529%252C%2520while%250Ausing%2520just%25200.2T%2520training%2520tokens%2520-%2520a%25206%2520times%2520reduction%2520compared%2520to%250AQwen2.5-Omni%2527s%25201.2T.%2520We%2520finally%2520demonstrate%2520omni-modal%2520advantages%2520in%2520downstream%250Aapplications%2520spanning%2520robotics%252C%2520medical%2520AI%252C%2520and%2520smart%2520factory.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2510.15870v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=OmniVinci%3A%20Enhancing%20Architecture%20and%20Data%20for%20Omni-Modal%20Understanding%0A%20%20LLM&entry.906535625=Hanrong%20Ye%20and%20Chao-Han%20Huck%20Yang%20and%20Arushi%20Goel%20and%20Wei%20Huang%20and%20Ligeng%20Zhu%20and%20Yuanhang%20Su%20and%20Sean%20Lin%20and%20An-Chieh%20Cheng%20and%20Zhen%20Wan%20and%20Jinchuan%20Tian%20and%20Yuming%20Lou%20and%20Dong%20Yang%20and%20Zhijian%20Liu%20and%20Yukang%20Chen%20and%20Ambrish%20Dantrey%20and%20Ehsan%20Jahangiri%20and%20Sreyan%20Ghosh%20and%20Daguang%20Xu%20and%20Ehsan%20Hosseini-Asl%20and%20Danial%20Mohseni%20Taheri%20and%20Vidya%20Murali%20and%20Sifei%20Liu%20and%20Jason%20Lu%20and%20Oluwatobi%20Olabiyi%20and%20Frank%20Wang%20and%20Rafael%20Valle%20and%20Bryan%20Catanzaro%20and%20Andrew%20Tao%20and%20Song%20Han%20and%20Jan%20Kautz%20and%20Hongxu%20Yin%20and%20Pavlo%20Molchanov&entry.1292438233=%20%20Advancing%20machine%20intelligence%20requires%20developing%20the%20ability%20to%20perceive%0Aacross%20multiple%20modalities%2C%20much%20as%20humans%20sense%20the%20world.%20We%20introduce%0AOmniVinci%2C%20an%20initiative%20to%20build%20a%20strong%2C%20open-source%2C%20omni-modal%20LLM.%20We%0Acarefully%20study%20the%20design%20choices%20across%20model%20architecture%20and%20data%20curation.%0AFor%20model%20architecture%2C%20we%20present%20three%20key%20innovations%3A%20%28i%29%20OmniAlignNet%20for%0Astrengthening%20alignment%20between%20vision%20and%20audio%20embeddings%20in%20a%20shared%0Aomni-modal%20latent%20space%3B%20%28ii%29%20Temporal%20Embedding%20Grouping%20for%20capturing%0Arelative%20temporal%20alignment%20between%20vision%20and%20audio%20signals%3B%20and%20%28iii%29%0AConstrained%20Rotary%20Time%20Embedding%20for%20encoding%20absolute%20temporal%20information%20in%0Aomni-modal%20embeddings.%20We%20introduce%20a%20curation%20and%20synthesis%20pipeline%20that%0Agenerates%2024M%20single-modal%20and%20omni-modal%20conversations.%20We%20find%20that%0Amodalities%20reinforce%20one%20another%20in%20both%20perception%20and%20reasoning.%20Our%20model%2C%0AOmniVinci%2C%20outperforms%20Qwen2.5-Omni%20with%20%2B19.05%20on%20DailyOmni%20%28cross-modal%0Aunderstanding%29%2C%20%2B1.7%20on%20MMAR%20%28audio%29%2C%20and%20%2B3.9%20on%20Video-MME%20%28vision%29%2C%20while%0Ausing%20just%200.2T%20training%20tokens%20-%20a%206%20times%20reduction%20compared%20to%0AQwen2.5-Omni%27s%201.2T.%20We%20finally%20demonstrate%20omni-modal%20advantages%20in%20downstream%0Aapplications%20spanning%20robotics%2C%20medical%20AI%2C%20and%20smart%20factory.%0A&entry.1838667208=http%3A//arxiv.org/abs/2510.15870v1&entry.124074799=Read"},
{"title": "CLOVER: Context-aware Long-term Object Viewpoint- and Environment-\n  Invariant Representation Learning", "author": "Dongmyeong Lee and Amanda Adkins and Joydeep Biswas", "abstract": "  Mobile service robots can benefit from object-level understanding of their\nenvironments, including the ability to distinguish object instances and\nre-identify previously seen instances. Object re-identification is challenging\nacross different viewpoints and in scenes with significant appearance variation\narising from weather or lighting changes. Existing works on object\nre-identification either focus on specific classes or require foreground\nsegmentation. Further, these methods, along with object re-identification\ndatasets, have limited consideration of challenges such as outdoor scenes and\nillumination changes. To address this problem, we introduce CODa Re-ID: an\nin-the-wild object re-identification dataset containing 1,037,814 observations\nof 557 objects across 8 classes under diverse lighting conditions and\nviewpoints. Further, we propose CLOVER, a representation learning method for\nobject observations that can distinguish between static object instances\nwithout requiring foreground segmentation. We also introduce MapCLOVER, a\nmethod for scalably summarizing CLOVER descriptors for use in object maps and\nmatching new observations to summarized descriptors. Our results show that\nCLOVER achieves superior performance in static object re-identification under\nvarying lighting conditions and viewpoint changes and can generalize to unseen\ninstances and classes.\n", "link": "http://arxiv.org/abs/2407.09718v3", "date": "2025-10-17", "relevancy": 2.8483, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5804}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5804}, {"title": "Self-supervised Photographic Image Layout Representation Learning", "link": "http://arxiv.org/abs/2403.03740v1", "similarity": 0.5482}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20CLOVER%3A%20Context-aware%20Long-term%20Object%20Viewpoint-%20and%20Environment-%0A%20%20Invariant%20Representation%20Learning&body=Title%3A%20CLOVER%3A%20Context-aware%20Long-term%20Object%20Viewpoint-%20and%20Environment-%0A%20%20Invariant%20Representation%20Learning%0AAuthor%3A%20Dongmyeong%20Lee%20and%20Amanda%20Adkins%20and%20Joydeep%20Biswas%0AAbstract%3A%20%20%20Mobile%20service%20robots%20can%20benefit%20from%20object-level%20understanding%20of%20their%0Aenvironments%2C%20including%20the%20ability%20to%20distinguish%20object%20instances%20and%0Are-identify%20previously%20seen%20instances.%20Object%20re-identification%20is%20challenging%0Aacross%20different%20viewpoints%20and%20in%20scenes%20with%20significant%20appearance%20variation%0Aarising%20from%20weather%20or%20lighting%20changes.%20Existing%20works%20on%20object%0Are-identification%20either%20focus%20on%20specific%20classes%20or%20require%20foreground%0Asegmentation.%20Further%2C%20these%20methods%2C%20along%20with%20object%20re-identification%0Adatasets%2C%20have%20limited%20consideration%20of%20challenges%20such%20as%20outdoor%20scenes%20and%0Aillumination%20changes.%20To%20address%20this%20problem%2C%20we%20introduce%20CODa%20Re-ID%3A%20an%0Ain-the-wild%20object%20re-identification%20dataset%20containing%201%2C037%2C814%20observations%0Aof%20557%20objects%20across%208%20classes%20under%20diverse%20lighting%20conditions%20and%0Aviewpoints.%20Further%2C%20we%20propose%20CLOVER%2C%20a%20representation%20learning%20method%20for%0Aobject%20observations%20that%20can%20distinguish%20between%20static%20object%20instances%0Awithout%20requiring%20foreground%20segmentation.%20We%20also%20introduce%20MapCLOVER%2C%20a%0Amethod%20for%20scalably%20summarizing%20CLOVER%20descriptors%20for%20use%20in%20object%20maps%20and%0Amatching%20new%20observations%20to%20summarized%20descriptors.%20Our%20results%20show%20that%0ACLOVER%20achieves%20superior%20performance%20in%20static%20object%20re-identification%20under%0Avarying%20lighting%20conditions%20and%20viewpoint%20changes%20and%20can%20generalize%20to%20unseen%0Ainstances%20and%20classes.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2407.09718v3%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DCLOVER%253A%2520Context-aware%2520Long-term%2520Object%2520Viewpoint-%2520and%2520Environment-%250A%2520%2520Invariant%2520Representation%2520Learning%26entry.906535625%3DDongmyeong%2520Lee%2520and%2520Amanda%2520Adkins%2520and%2520Joydeep%2520Biswas%26entry.1292438233%3D%2520%2520Mobile%2520service%2520robots%2520can%2520benefit%2520from%2520object-level%2520understanding%2520of%2520their%250Aenvironments%252C%2520including%2520the%2520ability%2520to%2520distinguish%2520object%2520instances%2520and%250Are-identify%2520previously%2520seen%2520instances.%2520Object%2520re-identification%2520is%2520challenging%250Aacross%2520different%2520viewpoints%2520and%2520in%2520scenes%2520with%2520significant%2520appearance%2520variation%250Aarising%2520from%2520weather%2520or%2520lighting%2520changes.%2520Existing%2520works%2520on%2520object%250Are-identification%2520either%2520focus%2520on%2520specific%2520classes%2520or%2520require%2520foreground%250Asegmentation.%2520Further%252C%2520these%2520methods%252C%2520along%2520with%2520object%2520re-identification%250Adatasets%252C%2520have%2520limited%2520consideration%2520of%2520challenges%2520such%2520as%2520outdoor%2520scenes%2520and%250Aillumination%2520changes.%2520To%2520address%2520this%2520problem%252C%2520we%2520introduce%2520CODa%2520Re-ID%253A%2520an%250Ain-the-wild%2520object%2520re-identification%2520dataset%2520containing%25201%252C037%252C814%2520observations%250Aof%2520557%2520objects%2520across%25208%2520classes%2520under%2520diverse%2520lighting%2520conditions%2520and%250Aviewpoints.%2520Further%252C%2520we%2520propose%2520CLOVER%252C%2520a%2520representation%2520learning%2520method%2520for%250Aobject%2520observations%2520that%2520can%2520distinguish%2520between%2520static%2520object%2520instances%250Awithout%2520requiring%2520foreground%2520segmentation.%2520We%2520also%2520introduce%2520MapCLOVER%252C%2520a%250Amethod%2520for%2520scalably%2520summarizing%2520CLOVER%2520descriptors%2520for%2520use%2520in%2520object%2520maps%2520and%250Amatching%2520new%2520observations%2520to%2520summarized%2520descriptors.%2520Our%2520results%2520show%2520that%250ACLOVER%2520achieves%2520superior%2520performance%2520in%2520static%2520object%2520re-identification%2520under%250Avarying%2520lighting%2520conditions%2520and%2520viewpoint%2520changes%2520and%2520can%2520generalize%2520to%2520unseen%250Ainstances%2520and%2520classes.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2407.09718v3%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=CLOVER%3A%20Context-aware%20Long-term%20Object%20Viewpoint-%20and%20Environment-%0A%20%20Invariant%20Representation%20Learning&entry.906535625=Dongmyeong%20Lee%20and%20Amanda%20Adkins%20and%20Joydeep%20Biswas&entry.1292438233=%20%20Mobile%20service%20robots%20can%20benefit%20from%20object-level%20understanding%20of%20their%0Aenvironments%2C%20including%20the%20ability%20to%20distinguish%20object%20instances%20and%0Are-identify%20previously%20seen%20instances.%20Object%20re-identification%20is%20challenging%0Aacross%20different%20viewpoints%20and%20in%20scenes%20with%20significant%20appearance%20variation%0Aarising%20from%20weather%20or%20lighting%20changes.%20Existing%20works%20on%20object%0Are-identification%20either%20focus%20on%20specific%20classes%20or%20require%20foreground%0Asegmentation.%20Further%2C%20these%20methods%2C%20along%20with%20object%20re-identification%0Adatasets%2C%20have%20limited%20consideration%20of%20challenges%20such%20as%20outdoor%20scenes%20and%0Aillumination%20changes.%20To%20address%20this%20problem%2C%20we%20introduce%20CODa%20Re-ID%3A%20an%0Ain-the-wild%20object%20re-identification%20dataset%20containing%201%2C037%2C814%20observations%0Aof%20557%20objects%20across%208%20classes%20under%20diverse%20lighting%20conditions%20and%0Aviewpoints.%20Further%2C%20we%20propose%20CLOVER%2C%20a%20representation%20learning%20method%20for%0Aobject%20observations%20that%20can%20distinguish%20between%20static%20object%20instances%0Awithout%20requiring%20foreground%20segmentation.%20We%20also%20introduce%20MapCLOVER%2C%20a%0Amethod%20for%20scalably%20summarizing%20CLOVER%20descriptors%20for%20use%20in%20object%20maps%20and%0Amatching%20new%20observations%20to%20summarized%20descriptors.%20Our%20results%20show%20that%0ACLOVER%20achieves%20superior%20performance%20in%20static%20object%20re-identification%20under%0Avarying%20lighting%20conditions%20and%20viewpoint%20changes%20and%20can%20generalize%20to%20unseen%0Ainstances%20and%20classes.%0A&entry.1838667208=http%3A//arxiv.org/abs/2407.09718v3&entry.124074799=Read"},
{"title": "Refer to Any Segmentation Mask Group With Vision-Language Prompts", "author": "Shengcao Cao and Zijun Wei and Jason Kuen and Kangning Liu and Lingzhi Zhang and Jiuxiang Gu and HyunJoon Jung and Liang-Yan Gui and Yu-Xiong Wang", "abstract": "  Recent image segmentation models have advanced to segment images into\nhigh-quality masks for visual entities, and yet they cannot provide\ncomprehensive semantic understanding for complex queries based on both language\nand vision. This limitation reduces their effectiveness in applications that\nrequire user-friendly interactions driven by vision-language prompts. To bridge\nthis gap, we introduce a novel task of omnimodal referring expression\nsegmentation (ORES). In this task, a model produces a group of masks based on\narbitrary prompts specified by text only or text plus reference visual\nentities. To address this new challenge, we propose a novel framework to \"Refer\nto Any Segmentation Mask Group\" (RAS), which augments segmentation models with\ncomplex multimodal interactions and comprehension via a mask-centric large\nmultimodal model. For training and benchmarking ORES models, we create datasets\nMaskGroups-2M and MaskGroups-HQ to include diverse mask groups specified by\ntext and reference entities. Through extensive evaluation, we demonstrate\nsuperior performance of RAS on our new ORES task, as well as classic referring\nexpression segmentation (RES) and generalized referring expression segmentation\n(GRES) tasks. Project page: https://Ref2Any.github.io.\n", "link": "http://arxiv.org/abs/2506.05342v2", "date": "2025-10-17", "relevancy": 2.7496, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5633}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5633}, {"title": "VirtualModel: Generating Object-ID-retentive Human-object Interaction\n  Image by Diffusion Model for E-commerce Marketing", "link": "http://arxiv.org/abs/2405.09985v1", "similarity": 0.5232}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Refer%20to%20Any%20Segmentation%20Mask%20Group%20With%20Vision-Language%20Prompts&body=Title%3A%20Refer%20to%20Any%20Segmentation%20Mask%20Group%20With%20Vision-Language%20Prompts%0AAuthor%3A%20Shengcao%20Cao%20and%20Zijun%20Wei%20and%20Jason%20Kuen%20and%20Kangning%20Liu%20and%20Lingzhi%20Zhang%20and%20Jiuxiang%20Gu%20and%20HyunJoon%20Jung%20and%20Liang-Yan%20Gui%20and%20Yu-Xiong%20Wang%0AAbstract%3A%20%20%20Recent%20image%20segmentation%20models%20have%20advanced%20to%20segment%20images%20into%0Ahigh-quality%20masks%20for%20visual%20entities%2C%20and%20yet%20they%20cannot%20provide%0Acomprehensive%20semantic%20understanding%20for%20complex%20queries%20based%20on%20both%20language%0Aand%20vision.%20This%20limitation%20reduces%20their%20effectiveness%20in%20applications%20that%0Arequire%20user-friendly%20interactions%20driven%20by%20vision-language%20prompts.%20To%20bridge%0Athis%20gap%2C%20we%20introduce%20a%20novel%20task%20of%20omnimodal%20referring%20expression%0Asegmentation%20%28ORES%29.%20In%20this%20task%2C%20a%20model%20produces%20a%20group%20of%20masks%20based%20on%0Aarbitrary%20prompts%20specified%20by%20text%20only%20or%20text%20plus%20reference%20visual%0Aentities.%20To%20address%20this%20new%20challenge%2C%20we%20propose%20a%20novel%20framework%20to%20%22Refer%0Ato%20Any%20Segmentation%20Mask%20Group%22%20%28RAS%29%2C%20which%20augments%20segmentation%20models%20with%0Acomplex%20multimodal%20interactions%20and%20comprehension%20via%20a%20mask-centric%20large%0Amultimodal%20model.%20For%20training%20and%20benchmarking%20ORES%20models%2C%20we%20create%20datasets%0AMaskGroups-2M%20and%20MaskGroups-HQ%20to%20include%20diverse%20mask%20groups%20specified%20by%0Atext%20and%20reference%20entities.%20Through%20extensive%20evaluation%2C%20we%20demonstrate%0Asuperior%20performance%20of%20RAS%20on%20our%20new%20ORES%20task%2C%20as%20well%20as%20classic%20referring%0Aexpression%20segmentation%20%28RES%29%20and%20generalized%20referring%20expression%20segmentation%0A%28GRES%29%20tasks.%20Project%20page%3A%20https%3A//Ref2Any.github.io.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2506.05342v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DRefer%2520to%2520Any%2520Segmentation%2520Mask%2520Group%2520With%2520Vision-Language%2520Prompts%26entry.906535625%3DShengcao%2520Cao%2520and%2520Zijun%2520Wei%2520and%2520Jason%2520Kuen%2520and%2520Kangning%2520Liu%2520and%2520Lingzhi%2520Zhang%2520and%2520Jiuxiang%2520Gu%2520and%2520HyunJoon%2520Jung%2520and%2520Liang-Yan%2520Gui%2520and%2520Yu-Xiong%2520Wang%26entry.1292438233%3D%2520%2520Recent%2520image%2520segmentation%2520models%2520have%2520advanced%2520to%2520segment%2520images%2520into%250Ahigh-quality%2520masks%2520for%2520visual%2520entities%252C%2520and%2520yet%2520they%2520cannot%2520provide%250Acomprehensive%2520semantic%2520understanding%2520for%2520complex%2520queries%2520based%2520on%2520both%2520language%250Aand%2520vision.%2520This%2520limitation%2520reduces%2520their%2520effectiveness%2520in%2520applications%2520that%250Arequire%2520user-friendly%2520interactions%2520driven%2520by%2520vision-language%2520prompts.%2520To%2520bridge%250Athis%2520gap%252C%2520we%2520introduce%2520a%2520novel%2520task%2520of%2520omnimodal%2520referring%2520expression%250Asegmentation%2520%2528ORES%2529.%2520In%2520this%2520task%252C%2520a%2520model%2520produces%2520a%2520group%2520of%2520masks%2520based%2520on%250Aarbitrary%2520prompts%2520specified%2520by%2520text%2520only%2520or%2520text%2520plus%2520reference%2520visual%250Aentities.%2520To%2520address%2520this%2520new%2520challenge%252C%2520we%2520propose%2520a%2520novel%2520framework%2520to%2520%2522Refer%250Ato%2520Any%2520Segmentation%2520Mask%2520Group%2522%2520%2528RAS%2529%252C%2520which%2520augments%2520segmentation%2520models%2520with%250Acomplex%2520multimodal%2520interactions%2520and%2520comprehension%2520via%2520a%2520mask-centric%2520large%250Amultimodal%2520model.%2520For%2520training%2520and%2520benchmarking%2520ORES%2520models%252C%2520we%2520create%2520datasets%250AMaskGroups-2M%2520and%2520MaskGroups-HQ%2520to%2520include%2520diverse%2520mask%2520groups%2520specified%2520by%250Atext%2520and%2520reference%2520entities.%2520Through%2520extensive%2520evaluation%252C%2520we%2520demonstrate%250Asuperior%2520performance%2520of%2520RAS%2520on%2520our%2520new%2520ORES%2520task%252C%2520as%2520well%2520as%2520classic%2520referring%250Aexpression%2520segmentation%2520%2528RES%2529%2520and%2520generalized%2520referring%2520expression%2520segmentation%250A%2528GRES%2529%2520tasks.%2520Project%2520page%253A%2520https%253A//Ref2Any.github.io.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2506.05342v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Refer%20to%20Any%20Segmentation%20Mask%20Group%20With%20Vision-Language%20Prompts&entry.906535625=Shengcao%20Cao%20and%20Zijun%20Wei%20and%20Jason%20Kuen%20and%20Kangning%20Liu%20and%20Lingzhi%20Zhang%20and%20Jiuxiang%20Gu%20and%20HyunJoon%20Jung%20and%20Liang-Yan%20Gui%20and%20Yu-Xiong%20Wang&entry.1292438233=%20%20Recent%20image%20segmentation%20models%20have%20advanced%20to%20segment%20images%20into%0Ahigh-quality%20masks%20for%20visual%20entities%2C%20and%20yet%20they%20cannot%20provide%0Acomprehensive%20semantic%20understanding%20for%20complex%20queries%20based%20on%20both%20language%0Aand%20vision.%20This%20limitation%20reduces%20their%20effectiveness%20in%20applications%20that%0Arequire%20user-friendly%20interactions%20driven%20by%20vision-language%20prompts.%20To%20bridge%0Athis%20gap%2C%20we%20introduce%20a%20novel%20task%20of%20omnimodal%20referring%20expression%0Asegmentation%20%28ORES%29.%20In%20this%20task%2C%20a%20model%20produces%20a%20group%20of%20masks%20based%20on%0Aarbitrary%20prompts%20specified%20by%20text%20only%20or%20text%20plus%20reference%20visual%0Aentities.%20To%20address%20this%20new%20challenge%2C%20we%20propose%20a%20novel%20framework%20to%20%22Refer%0Ato%20Any%20Segmentation%20Mask%20Group%22%20%28RAS%29%2C%20which%20augments%20segmentation%20models%20with%0Acomplex%20multimodal%20interactions%20and%20comprehension%20via%20a%20mask-centric%20large%0Amultimodal%20model.%20For%20training%20and%20benchmarking%20ORES%20models%2C%20we%20create%20datasets%0AMaskGroups-2M%20and%20MaskGroups-HQ%20to%20include%20diverse%20mask%20groups%20specified%20by%0Atext%20and%20reference%20entities.%20Through%20extensive%20evaluation%2C%20we%20demonstrate%0Asuperior%20performance%20of%20RAS%20on%20our%20new%20ORES%20task%2C%20as%20well%20as%20classic%20referring%0Aexpression%20segmentation%20%28RES%29%20and%20generalized%20referring%20expression%20segmentation%0A%28GRES%29%20tasks.%20Project%20page%3A%20https%3A//Ref2Any.github.io.%0A&entry.1838667208=http%3A//arxiv.org/abs/2506.05342v2&entry.124074799=Read"},
{"title": "DeepRV: Accelerating spatiotemporal inference with pre-trained neural\n  priors", "author": "Jhonathan Navott and Daniel Jenson and Seth Flaxman and Elizaveta Semenova", "abstract": "  Gaussian Processes (GPs) provide a flexible and statistically principled\nfoundation for modelling spatiotemporal phenomena, but their $O(N^3)$ scaling\nmakes them intractable for large datasets. Approximate methods such as\nvariational inference (VI), inducing points (sparse GPs), low-rank\nfactorizations (RFFs), local factorizations and approximations (INLA), improve\nscalability but trade off accuracy or flexibility. We introduce DeepRV, a\nneural-network surrogate that closely matches full GP accuracy including\nhyperparameter estimates, while reducing computational complexity to $O(N^2)$,\nincreasing scalability and inference speed. DeepRV serves as a drop-in\nreplacement for GP prior realisations in e.g. MCMC-based probabilistic\nprogramming pipelines, preserving full model flexibility. Across simulated\nbenchmarks, non-separable spatiotemporal GPs, and a real-world application to\neducation deprivation in London (n = 4,994 locations), DeepRV achieves the\nhighest fidelity to exact GPs while substantially accelerating inference. Code\nis provided in the accompanying ZIP archive, with all experiments run on a\nsingle consumer-grade GPU to ensure accessibility for practitioners.\n", "link": "http://arxiv.org/abs/2503.21473v2", "date": "2025-10-17", "relevancy": 2.7451, "topK": [{"title": "RTG-SLAM: Real-time 3D Reconstruction at Scale using Gaussian Splatting", "link": "http://arxiv.org/abs/2404.19706v1", "similarity": 0.5801}, {"title": "Dynamic Gaussians Mesh: Consistent Mesh Reconstruction from Monocular\n  Videos", "link": "http://arxiv.org/abs/2404.12379v2", "similarity": 0.5378}, {"title": "GST: Precise 3D Human Body from a Single Image with Gaussian Splatting\n  Transformers", "link": "http://arxiv.org/abs/2409.04196v1", "similarity": 0.5292}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20DeepRV%3A%20Accelerating%20spatiotemporal%20inference%20with%20pre-trained%20neural%0A%20%20priors&body=Title%3A%20DeepRV%3A%20Accelerating%20spatiotemporal%20inference%20with%20pre-trained%20neural%0A%20%20priors%0AAuthor%3A%20Jhonathan%20Navott%20and%20Daniel%20Jenson%20and%20Seth%20Flaxman%20and%20Elizaveta%20Semenova%0AAbstract%3A%20%20%20Gaussian%20Processes%20%28GPs%29%20provide%20a%20flexible%20and%20statistically%20principled%0Afoundation%20for%20modelling%20spatiotemporal%20phenomena%2C%20but%20their%20%24O%28N%5E3%29%24%20scaling%0Amakes%20them%20intractable%20for%20large%20datasets.%20Approximate%20methods%20such%20as%0Avariational%20inference%20%28VI%29%2C%20inducing%20points%20%28sparse%20GPs%29%2C%20low-rank%0Afactorizations%20%28RFFs%29%2C%20local%20factorizations%20and%20approximations%20%28INLA%29%2C%20improve%0Ascalability%20but%20trade%20off%20accuracy%20or%20flexibility.%20We%20introduce%20DeepRV%2C%20a%0Aneural-network%20surrogate%20that%20closely%20matches%20full%20GP%20accuracy%20including%0Ahyperparameter%20estimates%2C%20while%20reducing%20computational%20complexity%20to%20%24O%28N%5E2%29%24%2C%0Aincreasing%20scalability%20and%20inference%20speed.%20DeepRV%20serves%20as%20a%20drop-in%0Areplacement%20for%20GP%20prior%20realisations%20in%20e.g.%20MCMC-based%20probabilistic%0Aprogramming%20pipelines%2C%20preserving%20full%20model%20flexibility.%20Across%20simulated%0Abenchmarks%2C%20non-separable%20spatiotemporal%20GPs%2C%20and%20a%20real-world%20application%20to%0Aeducation%20deprivation%20in%20London%20%28n%20%3D%204%2C994%20locations%29%2C%20DeepRV%20achieves%20the%0Ahighest%20fidelity%20to%20exact%20GPs%20while%20substantially%20accelerating%20inference.%20Code%0Ais%20provided%20in%20the%20accompanying%20ZIP%20archive%2C%20with%20all%20experiments%20run%20on%20a%0Asingle%20consumer-grade%20GPU%20to%20ensure%20accessibility%20for%20practitioners.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2503.21473v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DDeepRV%253A%2520Accelerating%2520spatiotemporal%2520inference%2520with%2520pre-trained%2520neural%250A%2520%2520priors%26entry.906535625%3DJhonathan%2520Navott%2520and%2520Daniel%2520Jenson%2520and%2520Seth%2520Flaxman%2520and%2520Elizaveta%2520Semenova%26entry.1292438233%3D%2520%2520Gaussian%2520Processes%2520%2528GPs%2529%2520provide%2520a%2520flexible%2520and%2520statistically%2520principled%250Afoundation%2520for%2520modelling%2520spatiotemporal%2520phenomena%252C%2520but%2520their%2520%2524O%2528N%255E3%2529%2524%2520scaling%250Amakes%2520them%2520intractable%2520for%2520large%2520datasets.%2520Approximate%2520methods%2520such%2520as%250Avariational%2520inference%2520%2528VI%2529%252C%2520inducing%2520points%2520%2528sparse%2520GPs%2529%252C%2520low-rank%250Afactorizations%2520%2528RFFs%2529%252C%2520local%2520factorizations%2520and%2520approximations%2520%2528INLA%2529%252C%2520improve%250Ascalability%2520but%2520trade%2520off%2520accuracy%2520or%2520flexibility.%2520We%2520introduce%2520DeepRV%252C%2520a%250Aneural-network%2520surrogate%2520that%2520closely%2520matches%2520full%2520GP%2520accuracy%2520including%250Ahyperparameter%2520estimates%252C%2520while%2520reducing%2520computational%2520complexity%2520to%2520%2524O%2528N%255E2%2529%2524%252C%250Aincreasing%2520scalability%2520and%2520inference%2520speed.%2520DeepRV%2520serves%2520as%2520a%2520drop-in%250Areplacement%2520for%2520GP%2520prior%2520realisations%2520in%2520e.g.%2520MCMC-based%2520probabilistic%250Aprogramming%2520pipelines%252C%2520preserving%2520full%2520model%2520flexibility.%2520Across%2520simulated%250Abenchmarks%252C%2520non-separable%2520spatiotemporal%2520GPs%252C%2520and%2520a%2520real-world%2520application%2520to%250Aeducation%2520deprivation%2520in%2520London%2520%2528n%2520%253D%25204%252C994%2520locations%2529%252C%2520DeepRV%2520achieves%2520the%250Ahighest%2520fidelity%2520to%2520exact%2520GPs%2520while%2520substantially%2520accelerating%2520inference.%2520Code%250Ais%2520provided%2520in%2520the%2520accompanying%2520ZIP%2520archive%252C%2520with%2520all%2520experiments%2520run%2520on%2520a%250Asingle%2520consumer-grade%2520GPU%2520to%2520ensure%2520accessibility%2520for%2520practitioners.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2503.21473v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=DeepRV%3A%20Accelerating%20spatiotemporal%20inference%20with%20pre-trained%20neural%0A%20%20priors&entry.906535625=Jhonathan%20Navott%20and%20Daniel%20Jenson%20and%20Seth%20Flaxman%20and%20Elizaveta%20Semenova&entry.1292438233=%20%20Gaussian%20Processes%20%28GPs%29%20provide%20a%20flexible%20and%20statistically%20principled%0Afoundation%20for%20modelling%20spatiotemporal%20phenomena%2C%20but%20their%20%24O%28N%5E3%29%24%20scaling%0Amakes%20them%20intractable%20for%20large%20datasets.%20Approximate%20methods%20such%20as%0Avariational%20inference%20%28VI%29%2C%20inducing%20points%20%28sparse%20GPs%29%2C%20low-rank%0Afactorizations%20%28RFFs%29%2C%20local%20factorizations%20and%20approximations%20%28INLA%29%2C%20improve%0Ascalability%20but%20trade%20off%20accuracy%20or%20flexibility.%20We%20introduce%20DeepRV%2C%20a%0Aneural-network%20surrogate%20that%20closely%20matches%20full%20GP%20accuracy%20including%0Ahyperparameter%20estimates%2C%20while%20reducing%20computational%20complexity%20to%20%24O%28N%5E2%29%24%2C%0Aincreasing%20scalability%20and%20inference%20speed.%20DeepRV%20serves%20as%20a%20drop-in%0Areplacement%20for%20GP%20prior%20realisations%20in%20e.g.%20MCMC-based%20probabilistic%0Aprogramming%20pipelines%2C%20preserving%20full%20model%20flexibility.%20Across%20simulated%0Abenchmarks%2C%20non-separable%20spatiotemporal%20GPs%2C%20and%20a%20real-world%20application%20to%0Aeducation%20deprivation%20in%20London%20%28n%20%3D%204%2C994%20locations%29%2C%20DeepRV%20achieves%20the%0Ahighest%20fidelity%20to%20exact%20GPs%20while%20substantially%20accelerating%20inference.%20Code%0Ais%20provided%20in%20the%20accompanying%20ZIP%20archive%2C%20with%20all%20experiments%20run%20on%20a%0Asingle%20consumer-grade%20GPU%20to%20ensure%20accessibility%20for%20practitioners.%0A&entry.1838667208=http%3A//arxiv.org/abs/2503.21473v2&entry.124074799=Read"},
{"title": "Neuro-Symbolic Spatial Reasoning in Segmentation", "author": "Jiayi Lin and Jiabo Huang and Shaogang Gong", "abstract": "  Open-Vocabulary Semantic Segmentation (OVSS) assigns pixel-level labels from\nan open set of categories, requiring generalization to unseen and unlabelled\nobjects. Using vision-language models (VLMs) to correlate local image patches\nwith potential unseen object categories suffers from a lack of understanding of\nspatial relations of objects in a scene. To solve this problem, we introduce\nneuro-symbolic (NeSy) spatial reasoning in OVSS. In contrast to contemporary\nVLM correlation-based approaches, we propose Relational Segmentor (RelateSeg)\nto impose explicit spatial relational constraints by first order logic (FOL)\nformulated in a neural network architecture. This is the first attempt to\nexplore NeSy spatial reasoning in OVSS. Specifically, RelateSeg automatically\nextracts spatial relations, e.g., <cat, to-right-of, person>, and encodes them\nas first-order logic formulas using our proposed pseudo categories. Each pixel\nlearns to predict both a semantic category (e.g., \"cat\") and a spatial pseudo\ncategory (e.g., \"right of person\") simultaneously, enforcing relational\nconstraints (e.g., a \"cat\" pixel must lie to the right of a \"person\"). Finally,\nthese logic constraints are formulated in a deep network architecture by fuzzy\nlogic relaxation, enabling end-to-end learning of spatial-relationally\nconsistent segmentation. RelateSeg achieves state-of-the-art performance in\nterms of average mIoU across four benchmark datasets and particularly shows\nclear advantages on images containing multiple categories, with the cost of\nonly introducing a single auxiliary loss function and no additional parameters,\nvalidating the effectiveness of NeSy spatial reasoning in OVSS.\n", "link": "http://arxiv.org/abs/2510.15841v1", "date": "2025-10-17", "relevancy": 2.7273, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5578}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5578}, {"title": "Self-supervised Photographic Image Layout Representation Learning", "link": "http://arxiv.org/abs/2403.03740v1", "similarity": 0.5207}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Neuro-Symbolic%20Spatial%20Reasoning%20in%20Segmentation&body=Title%3A%20Neuro-Symbolic%20Spatial%20Reasoning%20in%20Segmentation%0AAuthor%3A%20Jiayi%20Lin%20and%20Jiabo%20Huang%20and%20Shaogang%20Gong%0AAbstract%3A%20%20%20Open-Vocabulary%20Semantic%20Segmentation%20%28OVSS%29%20assigns%20pixel-level%20labels%20from%0Aan%20open%20set%20of%20categories%2C%20requiring%20generalization%20to%20unseen%20and%20unlabelled%0Aobjects.%20Using%20vision-language%20models%20%28VLMs%29%20to%20correlate%20local%20image%20patches%0Awith%20potential%20unseen%20object%20categories%20suffers%20from%20a%20lack%20of%20understanding%20of%0Aspatial%20relations%20of%20objects%20in%20a%20scene.%20To%20solve%20this%20problem%2C%20we%20introduce%0Aneuro-symbolic%20%28NeSy%29%20spatial%20reasoning%20in%20OVSS.%20In%20contrast%20to%20contemporary%0AVLM%20correlation-based%20approaches%2C%20we%20propose%20Relational%20Segmentor%20%28RelateSeg%29%0Ato%20impose%20explicit%20spatial%20relational%20constraints%20by%20first%20order%20logic%20%28FOL%29%0Aformulated%20in%20a%20neural%20network%20architecture.%20This%20is%20the%20first%20attempt%20to%0Aexplore%20NeSy%20spatial%20reasoning%20in%20OVSS.%20Specifically%2C%20RelateSeg%20automatically%0Aextracts%20spatial%20relations%2C%20e.g.%2C%20%3Ccat%2C%20to-right-of%2C%20person%3E%2C%20and%20encodes%20them%0Aas%20first-order%20logic%20formulas%20using%20our%20proposed%20pseudo%20categories.%20Each%20pixel%0Alearns%20to%20predict%20both%20a%20semantic%20category%20%28e.g.%2C%20%22cat%22%29%20and%20a%20spatial%20pseudo%0Acategory%20%28e.g.%2C%20%22right%20of%20person%22%29%20simultaneously%2C%20enforcing%20relational%0Aconstraints%20%28e.g.%2C%20a%20%22cat%22%20pixel%20must%20lie%20to%20the%20right%20of%20a%20%22person%22%29.%20Finally%2C%0Athese%20logic%20constraints%20are%20formulated%20in%20a%20deep%20network%20architecture%20by%20fuzzy%0Alogic%20relaxation%2C%20enabling%20end-to-end%20learning%20of%20spatial-relationally%0Aconsistent%20segmentation.%20RelateSeg%20achieves%20state-of-the-art%20performance%20in%0Aterms%20of%20average%20mIoU%20across%20four%20benchmark%20datasets%20and%20particularly%20shows%0Aclear%20advantages%20on%20images%20containing%20multiple%20categories%2C%20with%20the%20cost%20of%0Aonly%20introducing%20a%20single%20auxiliary%20loss%20function%20and%20no%20additional%20parameters%2C%0Avalidating%20the%20effectiveness%20of%20NeSy%20spatial%20reasoning%20in%20OVSS.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2510.15841v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DNeuro-Symbolic%2520Spatial%2520Reasoning%2520in%2520Segmentation%26entry.906535625%3DJiayi%2520Lin%2520and%2520Jiabo%2520Huang%2520and%2520Shaogang%2520Gong%26entry.1292438233%3D%2520%2520Open-Vocabulary%2520Semantic%2520Segmentation%2520%2528OVSS%2529%2520assigns%2520pixel-level%2520labels%2520from%250Aan%2520open%2520set%2520of%2520categories%252C%2520requiring%2520generalization%2520to%2520unseen%2520and%2520unlabelled%250Aobjects.%2520Using%2520vision-language%2520models%2520%2528VLMs%2529%2520to%2520correlate%2520local%2520image%2520patches%250Awith%2520potential%2520unseen%2520object%2520categories%2520suffers%2520from%2520a%2520lack%2520of%2520understanding%2520of%250Aspatial%2520relations%2520of%2520objects%2520in%2520a%2520scene.%2520To%2520solve%2520this%2520problem%252C%2520we%2520introduce%250Aneuro-symbolic%2520%2528NeSy%2529%2520spatial%2520reasoning%2520in%2520OVSS.%2520In%2520contrast%2520to%2520contemporary%250AVLM%2520correlation-based%2520approaches%252C%2520we%2520propose%2520Relational%2520Segmentor%2520%2528RelateSeg%2529%250Ato%2520impose%2520explicit%2520spatial%2520relational%2520constraints%2520by%2520first%2520order%2520logic%2520%2528FOL%2529%250Aformulated%2520in%2520a%2520neural%2520network%2520architecture.%2520This%2520is%2520the%2520first%2520attempt%2520to%250Aexplore%2520NeSy%2520spatial%2520reasoning%2520in%2520OVSS.%2520Specifically%252C%2520RelateSeg%2520automatically%250Aextracts%2520spatial%2520relations%252C%2520e.g.%252C%2520%253Ccat%252C%2520to-right-of%252C%2520person%253E%252C%2520and%2520encodes%2520them%250Aas%2520first-order%2520logic%2520formulas%2520using%2520our%2520proposed%2520pseudo%2520categories.%2520Each%2520pixel%250Alearns%2520to%2520predict%2520both%2520a%2520semantic%2520category%2520%2528e.g.%252C%2520%2522cat%2522%2529%2520and%2520a%2520spatial%2520pseudo%250Acategory%2520%2528e.g.%252C%2520%2522right%2520of%2520person%2522%2529%2520simultaneously%252C%2520enforcing%2520relational%250Aconstraints%2520%2528e.g.%252C%2520a%2520%2522cat%2522%2520pixel%2520must%2520lie%2520to%2520the%2520right%2520of%2520a%2520%2522person%2522%2529.%2520Finally%252C%250Athese%2520logic%2520constraints%2520are%2520formulated%2520in%2520a%2520deep%2520network%2520architecture%2520by%2520fuzzy%250Alogic%2520relaxation%252C%2520enabling%2520end-to-end%2520learning%2520of%2520spatial-relationally%250Aconsistent%2520segmentation.%2520RelateSeg%2520achieves%2520state-of-the-art%2520performance%2520in%250Aterms%2520of%2520average%2520mIoU%2520across%2520four%2520benchmark%2520datasets%2520and%2520particularly%2520shows%250Aclear%2520advantages%2520on%2520images%2520containing%2520multiple%2520categories%252C%2520with%2520the%2520cost%2520of%250Aonly%2520introducing%2520a%2520single%2520auxiliary%2520loss%2520function%2520and%2520no%2520additional%2520parameters%252C%250Avalidating%2520the%2520effectiveness%2520of%2520NeSy%2520spatial%2520reasoning%2520in%2520OVSS.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2510.15841v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Neuro-Symbolic%20Spatial%20Reasoning%20in%20Segmentation&entry.906535625=Jiayi%20Lin%20and%20Jiabo%20Huang%20and%20Shaogang%20Gong&entry.1292438233=%20%20Open-Vocabulary%20Semantic%20Segmentation%20%28OVSS%29%20assigns%20pixel-level%20labels%20from%0Aan%20open%20set%20of%20categories%2C%20requiring%20generalization%20to%20unseen%20and%20unlabelled%0Aobjects.%20Using%20vision-language%20models%20%28VLMs%29%20to%20correlate%20local%20image%20patches%0Awith%20potential%20unseen%20object%20categories%20suffers%20from%20a%20lack%20of%20understanding%20of%0Aspatial%20relations%20of%20objects%20in%20a%20scene.%20To%20solve%20this%20problem%2C%20we%20introduce%0Aneuro-symbolic%20%28NeSy%29%20spatial%20reasoning%20in%20OVSS.%20In%20contrast%20to%20contemporary%0AVLM%20correlation-based%20approaches%2C%20we%20propose%20Relational%20Segmentor%20%28RelateSeg%29%0Ato%20impose%20explicit%20spatial%20relational%20constraints%20by%20first%20order%20logic%20%28FOL%29%0Aformulated%20in%20a%20neural%20network%20architecture.%20This%20is%20the%20first%20attempt%20to%0Aexplore%20NeSy%20spatial%20reasoning%20in%20OVSS.%20Specifically%2C%20RelateSeg%20automatically%0Aextracts%20spatial%20relations%2C%20e.g.%2C%20%3Ccat%2C%20to-right-of%2C%20person%3E%2C%20and%20encodes%20them%0Aas%20first-order%20logic%20formulas%20using%20our%20proposed%20pseudo%20categories.%20Each%20pixel%0Alearns%20to%20predict%20both%20a%20semantic%20category%20%28e.g.%2C%20%22cat%22%29%20and%20a%20spatial%20pseudo%0Acategory%20%28e.g.%2C%20%22right%20of%20person%22%29%20simultaneously%2C%20enforcing%20relational%0Aconstraints%20%28e.g.%2C%20a%20%22cat%22%20pixel%20must%20lie%20to%20the%20right%20of%20a%20%22person%22%29.%20Finally%2C%0Athese%20logic%20constraints%20are%20formulated%20in%20a%20deep%20network%20architecture%20by%20fuzzy%0Alogic%20relaxation%2C%20enabling%20end-to-end%20learning%20of%20spatial-relationally%0Aconsistent%20segmentation.%20RelateSeg%20achieves%20state-of-the-art%20performance%20in%0Aterms%20of%20average%20mIoU%20across%20four%20benchmark%20datasets%20and%20particularly%20shows%0Aclear%20advantages%20on%20images%20containing%20multiple%20categories%2C%20with%20the%20cost%20of%0Aonly%20introducing%20a%20single%20auxiliary%20loss%20function%20and%20no%20additional%20parameters%2C%0Avalidating%20the%20effectiveness%20of%20NeSy%20spatial%20reasoning%20in%20OVSS.%0A&entry.1838667208=http%3A//arxiv.org/abs/2510.15841v1&entry.124074799=Read"},
{"title": "GENESIS: A Generative Model of Episodic-Semantic Interaction", "author": "Marco D'Alessandro and Leo D'Amato and Mikel Elkano and Mikel Uriz and Giovanni Pezzulo", "abstract": "  A central challenge in cognitive neuroscience is to explain how semantic and\nepisodic memory, two major forms of declarative memory, typically associated\nwith cortical and hippocampal processing, interact to support learning, recall,\nand imagination. Despite significant advances, we still lack a unified\ncomputational framework that jointly accounts for core empirical phenomena\nacross both semantic and episodic processing domains. Here, we introduce the\nGenerative Episodic-Semantic Integration System (GENESIS), a computational\nmodel that formalizes memory as the interaction between two limited-capacity\ngenerative systems: a Cortical-VAE, supporting semantic learning and\ngeneralization, and a Hippocampal-VAE, supporting episodic encoding and\nretrieval within a retrieval-augmented generation (RAG) architecture. GENESIS\nreproduces hallmark behavioral findings, including generalization in semantic\nmemory, recognition, serial recall effects and gist-based distortions in\nepisodic memory, and constructive episodic simulation, while capturing their\ndynamic interactions. The model elucidates how capacity constraints shape the\nfidelity and memorability of experiences, how semantic processing introduces\nsystematic distortions in episodic recall, and how episodic replay can\nrecombine previous experiences. Together, these results provide a principled\naccount of memory as an active, constructive, and resource-bounded process.\nGENESIS thus advances a unified theoretical framework that bridges semantic and\nepisodic memory, offering new insights into the generative foundations of human\ncognition.\n", "link": "http://arxiv.org/abs/2510.15828v1", "date": "2025-10-17", "relevancy": 2.7194, "topK": [{"title": "Genie: Generative Interactive Environments", "link": "http://arxiv.org/abs/2402.15391v1", "similarity": 0.5778}, {"title": "GeneVA: A Dataset of Human Annotations for Generative Text to Video\n  Artifacts", "link": "http://arxiv.org/abs/2509.08818v1", "similarity": 0.5431}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5108}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20GENESIS%3A%20A%20Generative%20Model%20of%20Episodic-Semantic%20Interaction&body=Title%3A%20GENESIS%3A%20A%20Generative%20Model%20of%20Episodic-Semantic%20Interaction%0AAuthor%3A%20Marco%20D%27Alessandro%20and%20Leo%20D%27Amato%20and%20Mikel%20Elkano%20and%20Mikel%20Uriz%20and%20Giovanni%20Pezzulo%0AAbstract%3A%20%20%20A%20central%20challenge%20in%20cognitive%20neuroscience%20is%20to%20explain%20how%20semantic%20and%0Aepisodic%20memory%2C%20two%20major%20forms%20of%20declarative%20memory%2C%20typically%20associated%0Awith%20cortical%20and%20hippocampal%20processing%2C%20interact%20to%20support%20learning%2C%20recall%2C%0Aand%20imagination.%20Despite%20significant%20advances%2C%20we%20still%20lack%20a%20unified%0Acomputational%20framework%20that%20jointly%20accounts%20for%20core%20empirical%20phenomena%0Aacross%20both%20semantic%20and%20episodic%20processing%20domains.%20Here%2C%20we%20introduce%20the%0AGenerative%20Episodic-Semantic%20Integration%20System%20%28GENESIS%29%2C%20a%20computational%0Amodel%20that%20formalizes%20memory%20as%20the%20interaction%20between%20two%20limited-capacity%0Agenerative%20systems%3A%20a%20Cortical-VAE%2C%20supporting%20semantic%20learning%20and%0Ageneralization%2C%20and%20a%20Hippocampal-VAE%2C%20supporting%20episodic%20encoding%20and%0Aretrieval%20within%20a%20retrieval-augmented%20generation%20%28RAG%29%20architecture.%20GENESIS%0Areproduces%20hallmark%20behavioral%20findings%2C%20including%20generalization%20in%20semantic%0Amemory%2C%20recognition%2C%20serial%20recall%20effects%20and%20gist-based%20distortions%20in%0Aepisodic%20memory%2C%20and%20constructive%20episodic%20simulation%2C%20while%20capturing%20their%0Adynamic%20interactions.%20The%20model%20elucidates%20how%20capacity%20constraints%20shape%20the%0Afidelity%20and%20memorability%20of%20experiences%2C%20how%20semantic%20processing%20introduces%0Asystematic%20distortions%20in%20episodic%20recall%2C%20and%20how%20episodic%20replay%20can%0Arecombine%20previous%20experiences.%20Together%2C%20these%20results%20provide%20a%20principled%0Aaccount%20of%20memory%20as%20an%20active%2C%20constructive%2C%20and%20resource-bounded%20process.%0AGENESIS%20thus%20advances%20a%20unified%20theoretical%20framework%20that%20bridges%20semantic%20and%0Aepisodic%20memory%2C%20offering%20new%20insights%20into%20the%20generative%20foundations%20of%20human%0Acognition.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2510.15828v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DGENESIS%253A%2520A%2520Generative%2520Model%2520of%2520Episodic-Semantic%2520Interaction%26entry.906535625%3DMarco%2520D%2527Alessandro%2520and%2520Leo%2520D%2527Amato%2520and%2520Mikel%2520Elkano%2520and%2520Mikel%2520Uriz%2520and%2520Giovanni%2520Pezzulo%26entry.1292438233%3D%2520%2520A%2520central%2520challenge%2520in%2520cognitive%2520neuroscience%2520is%2520to%2520explain%2520how%2520semantic%2520and%250Aepisodic%2520memory%252C%2520two%2520major%2520forms%2520of%2520declarative%2520memory%252C%2520typically%2520associated%250Awith%2520cortical%2520and%2520hippocampal%2520processing%252C%2520interact%2520to%2520support%2520learning%252C%2520recall%252C%250Aand%2520imagination.%2520Despite%2520significant%2520advances%252C%2520we%2520still%2520lack%2520a%2520unified%250Acomputational%2520framework%2520that%2520jointly%2520accounts%2520for%2520core%2520empirical%2520phenomena%250Aacross%2520both%2520semantic%2520and%2520episodic%2520processing%2520domains.%2520Here%252C%2520we%2520introduce%2520the%250AGenerative%2520Episodic-Semantic%2520Integration%2520System%2520%2528GENESIS%2529%252C%2520a%2520computational%250Amodel%2520that%2520formalizes%2520memory%2520as%2520the%2520interaction%2520between%2520two%2520limited-capacity%250Agenerative%2520systems%253A%2520a%2520Cortical-VAE%252C%2520supporting%2520semantic%2520learning%2520and%250Ageneralization%252C%2520and%2520a%2520Hippocampal-VAE%252C%2520supporting%2520episodic%2520encoding%2520and%250Aretrieval%2520within%2520a%2520retrieval-augmented%2520generation%2520%2528RAG%2529%2520architecture.%2520GENESIS%250Areproduces%2520hallmark%2520behavioral%2520findings%252C%2520including%2520generalization%2520in%2520semantic%250Amemory%252C%2520recognition%252C%2520serial%2520recall%2520effects%2520and%2520gist-based%2520distortions%2520in%250Aepisodic%2520memory%252C%2520and%2520constructive%2520episodic%2520simulation%252C%2520while%2520capturing%2520their%250Adynamic%2520interactions.%2520The%2520model%2520elucidates%2520how%2520capacity%2520constraints%2520shape%2520the%250Afidelity%2520and%2520memorability%2520of%2520experiences%252C%2520how%2520semantic%2520processing%2520introduces%250Asystematic%2520distortions%2520in%2520episodic%2520recall%252C%2520and%2520how%2520episodic%2520replay%2520can%250Arecombine%2520previous%2520experiences.%2520Together%252C%2520these%2520results%2520provide%2520a%2520principled%250Aaccount%2520of%2520memory%2520as%2520an%2520active%252C%2520constructive%252C%2520and%2520resource-bounded%2520process.%250AGENESIS%2520thus%2520advances%2520a%2520unified%2520theoretical%2520framework%2520that%2520bridges%2520semantic%2520and%250Aepisodic%2520memory%252C%2520offering%2520new%2520insights%2520into%2520the%2520generative%2520foundations%2520of%2520human%250Acognition.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2510.15828v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=GENESIS%3A%20A%20Generative%20Model%20of%20Episodic-Semantic%20Interaction&entry.906535625=Marco%20D%27Alessandro%20and%20Leo%20D%27Amato%20and%20Mikel%20Elkano%20and%20Mikel%20Uriz%20and%20Giovanni%20Pezzulo&entry.1292438233=%20%20A%20central%20challenge%20in%20cognitive%20neuroscience%20is%20to%20explain%20how%20semantic%20and%0Aepisodic%20memory%2C%20two%20major%20forms%20of%20declarative%20memory%2C%20typically%20associated%0Awith%20cortical%20and%20hippocampal%20processing%2C%20interact%20to%20support%20learning%2C%20recall%2C%0Aand%20imagination.%20Despite%20significant%20advances%2C%20we%20still%20lack%20a%20unified%0Acomputational%20framework%20that%20jointly%20accounts%20for%20core%20empirical%20phenomena%0Aacross%20both%20semantic%20and%20episodic%20processing%20domains.%20Here%2C%20we%20introduce%20the%0AGenerative%20Episodic-Semantic%20Integration%20System%20%28GENESIS%29%2C%20a%20computational%0Amodel%20that%20formalizes%20memory%20as%20the%20interaction%20between%20two%20limited-capacity%0Agenerative%20systems%3A%20a%20Cortical-VAE%2C%20supporting%20semantic%20learning%20and%0Ageneralization%2C%20and%20a%20Hippocampal-VAE%2C%20supporting%20episodic%20encoding%20and%0Aretrieval%20within%20a%20retrieval-augmented%20generation%20%28RAG%29%20architecture.%20GENESIS%0Areproduces%20hallmark%20behavioral%20findings%2C%20including%20generalization%20in%20semantic%0Amemory%2C%20recognition%2C%20serial%20recall%20effects%20and%20gist-based%20distortions%20in%0Aepisodic%20memory%2C%20and%20constructive%20episodic%20simulation%2C%20while%20capturing%20their%0Adynamic%20interactions.%20The%20model%20elucidates%20how%20capacity%20constraints%20shape%20the%0Afidelity%20and%20memorability%20of%20experiences%2C%20how%20semantic%20processing%20introduces%0Asystematic%20distortions%20in%20episodic%20recall%2C%20and%20how%20episodic%20replay%20can%0Arecombine%20previous%20experiences.%20Together%2C%20these%20results%20provide%20a%20principled%0Aaccount%20of%20memory%20as%20an%20active%2C%20constructive%2C%20and%20resource-bounded%20process.%0AGENESIS%20thus%20advances%20a%20unified%20theoretical%20framework%20that%20bridges%20semantic%20and%0Aepisodic%20memory%2C%20offering%20new%20insights%20into%20the%20generative%20foundations%20of%20human%0Acognition.%0A&entry.1838667208=http%3A//arxiv.org/abs/2510.15828v1&entry.124074799=Read"},
{"title": "ERNet: Efficient Non-Rigid Registration Network for Point Sequences", "author": "Guangzhao He and Yuxi Xiao and Zhen Xu and Xiaowei Zhou and Sida Peng", "abstract": "  Registering an object shape to a sequence of point clouds undergoing\nnon-rigid deformation is a long-standing challenge. The key difficulties stem\nfrom two factors: (i) the presence of local minima due to the non-convexity of\nregistration objectives, especially under noisy or partial inputs, which\nhinders accurate and robust deformation estimation, and (ii) error accumulation\nover long sequences, leading to tracking failures. To address these challenges,\nwe introduce to adopt a scalable data-driven approach and propose ERNet, an\nefficient feed-forward model trained on large deformation datasets. It is\ndesigned to handle noisy and partial inputs while effectively leveraging\ntemporal information for accurate and consistent sequential registration. The\nkey to our design is predicting a sequence of deformation graphs through a\ntwo-stage pipeline, which first estimates frame-wise coarse graph nodes for\nrobust initialization, before refining their trajectories over time in a\nsliding-window fashion. Extensive experiments show that our proposed approach\n(i) outperforms previous state-of-the-art on both the DeformingThings4D and\nD-FAUST datasets, and (ii) achieves more than 4x speedup compared to the\nprevious best, offering significant efficiency improvement.\n", "link": "http://arxiv.org/abs/2510.15800v1", "date": "2025-10-17", "relevancy": 2.6997, "topK": [{"title": "DDM-NET: End-to-end learning of keypoint feature Detection, Description and Matching for 3D localization", "link": "https://arxiv.org/abs/2212.04575", "similarity": 0.5647}, {"title": "RTG-SLAM: Real-time 3D Reconstruction at Scale using Gaussian Splatting", "link": "http://arxiv.org/abs/2404.19706v1", "similarity": 0.5287}, {"title": "Dynamic Gaussians Mesh: Consistent Mesh Reconstruction from Monocular\n  Videos", "link": "http://arxiv.org/abs/2404.12379v2", "similarity": 0.5265}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20ERNet%3A%20Efficient%20Non-Rigid%20Registration%20Network%20for%20Point%20Sequences&body=Title%3A%20ERNet%3A%20Efficient%20Non-Rigid%20Registration%20Network%20for%20Point%20Sequences%0AAuthor%3A%20Guangzhao%20He%20and%20Yuxi%20Xiao%20and%20Zhen%20Xu%20and%20Xiaowei%20Zhou%20and%20Sida%20Peng%0AAbstract%3A%20%20%20Registering%20an%20object%20shape%20to%20a%20sequence%20of%20point%20clouds%20undergoing%0Anon-rigid%20deformation%20is%20a%20long-standing%20challenge.%20The%20key%20difficulties%20stem%0Afrom%20two%20factors%3A%20%28i%29%20the%20presence%20of%20local%20minima%20due%20to%20the%20non-convexity%20of%0Aregistration%20objectives%2C%20especially%20under%20noisy%20or%20partial%20inputs%2C%20which%0Ahinders%20accurate%20and%20robust%20deformation%20estimation%2C%20and%20%28ii%29%20error%20accumulation%0Aover%20long%20sequences%2C%20leading%20to%20tracking%20failures.%20To%20address%20these%20challenges%2C%0Awe%20introduce%20to%20adopt%20a%20scalable%20data-driven%20approach%20and%20propose%20ERNet%2C%20an%0Aefficient%20feed-forward%20model%20trained%20on%20large%20deformation%20datasets.%20It%20is%0Adesigned%20to%20handle%20noisy%20and%20partial%20inputs%20while%20effectively%20leveraging%0Atemporal%20information%20for%20accurate%20and%20consistent%20sequential%20registration.%20The%0Akey%20to%20our%20design%20is%20predicting%20a%20sequence%20of%20deformation%20graphs%20through%20a%0Atwo-stage%20pipeline%2C%20which%20first%20estimates%20frame-wise%20coarse%20graph%20nodes%20for%0Arobust%20initialization%2C%20before%20refining%20their%20trajectories%20over%20time%20in%20a%0Asliding-window%20fashion.%20Extensive%20experiments%20show%20that%20our%20proposed%20approach%0A%28i%29%20outperforms%20previous%20state-of-the-art%20on%20both%20the%20DeformingThings4D%20and%0AD-FAUST%20datasets%2C%20and%20%28ii%29%20achieves%20more%20than%204x%20speedup%20compared%20to%20the%0Aprevious%20best%2C%20offering%20significant%20efficiency%20improvement.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2510.15800v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DERNet%253A%2520Efficient%2520Non-Rigid%2520Registration%2520Network%2520for%2520Point%2520Sequences%26entry.906535625%3DGuangzhao%2520He%2520and%2520Yuxi%2520Xiao%2520and%2520Zhen%2520Xu%2520and%2520Xiaowei%2520Zhou%2520and%2520Sida%2520Peng%26entry.1292438233%3D%2520%2520Registering%2520an%2520object%2520shape%2520to%2520a%2520sequence%2520of%2520point%2520clouds%2520undergoing%250Anon-rigid%2520deformation%2520is%2520a%2520long-standing%2520challenge.%2520The%2520key%2520difficulties%2520stem%250Afrom%2520two%2520factors%253A%2520%2528i%2529%2520the%2520presence%2520of%2520local%2520minima%2520due%2520to%2520the%2520non-convexity%2520of%250Aregistration%2520objectives%252C%2520especially%2520under%2520noisy%2520or%2520partial%2520inputs%252C%2520which%250Ahinders%2520accurate%2520and%2520robust%2520deformation%2520estimation%252C%2520and%2520%2528ii%2529%2520error%2520accumulation%250Aover%2520long%2520sequences%252C%2520leading%2520to%2520tracking%2520failures.%2520To%2520address%2520these%2520challenges%252C%250Awe%2520introduce%2520to%2520adopt%2520a%2520scalable%2520data-driven%2520approach%2520and%2520propose%2520ERNet%252C%2520an%250Aefficient%2520feed-forward%2520model%2520trained%2520on%2520large%2520deformation%2520datasets.%2520It%2520is%250Adesigned%2520to%2520handle%2520noisy%2520and%2520partial%2520inputs%2520while%2520effectively%2520leveraging%250Atemporal%2520information%2520for%2520accurate%2520and%2520consistent%2520sequential%2520registration.%2520The%250Akey%2520to%2520our%2520design%2520is%2520predicting%2520a%2520sequence%2520of%2520deformation%2520graphs%2520through%2520a%250Atwo-stage%2520pipeline%252C%2520which%2520first%2520estimates%2520frame-wise%2520coarse%2520graph%2520nodes%2520for%250Arobust%2520initialization%252C%2520before%2520refining%2520their%2520trajectories%2520over%2520time%2520in%2520a%250Asliding-window%2520fashion.%2520Extensive%2520experiments%2520show%2520that%2520our%2520proposed%2520approach%250A%2528i%2529%2520outperforms%2520previous%2520state-of-the-art%2520on%2520both%2520the%2520DeformingThings4D%2520and%250AD-FAUST%2520datasets%252C%2520and%2520%2528ii%2529%2520achieves%2520more%2520than%25204x%2520speedup%2520compared%2520to%2520the%250Aprevious%2520best%252C%2520offering%2520significant%2520efficiency%2520improvement.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2510.15800v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=ERNet%3A%20Efficient%20Non-Rigid%20Registration%20Network%20for%20Point%20Sequences&entry.906535625=Guangzhao%20He%20and%20Yuxi%20Xiao%20and%20Zhen%20Xu%20and%20Xiaowei%20Zhou%20and%20Sida%20Peng&entry.1292438233=%20%20Registering%20an%20object%20shape%20to%20a%20sequence%20of%20point%20clouds%20undergoing%0Anon-rigid%20deformation%20is%20a%20long-standing%20challenge.%20The%20key%20difficulties%20stem%0Afrom%20two%20factors%3A%20%28i%29%20the%20presence%20of%20local%20minima%20due%20to%20the%20non-convexity%20of%0Aregistration%20objectives%2C%20especially%20under%20noisy%20or%20partial%20inputs%2C%20which%0Ahinders%20accurate%20and%20robust%20deformation%20estimation%2C%20and%20%28ii%29%20error%20accumulation%0Aover%20long%20sequences%2C%20leading%20to%20tracking%20failures.%20To%20address%20these%20challenges%2C%0Awe%20introduce%20to%20adopt%20a%20scalable%20data-driven%20approach%20and%20propose%20ERNet%2C%20an%0Aefficient%20feed-forward%20model%20trained%20on%20large%20deformation%20datasets.%20It%20is%0Adesigned%20to%20handle%20noisy%20and%20partial%20inputs%20while%20effectively%20leveraging%0Atemporal%20information%20for%20accurate%20and%20consistent%20sequential%20registration.%20The%0Akey%20to%20our%20design%20is%20predicting%20a%20sequence%20of%20deformation%20graphs%20through%20a%0Atwo-stage%20pipeline%2C%20which%20first%20estimates%20frame-wise%20coarse%20graph%20nodes%20for%0Arobust%20initialization%2C%20before%20refining%20their%20trajectories%20over%20time%20in%20a%0Asliding-window%20fashion.%20Extensive%20experiments%20show%20that%20our%20proposed%20approach%0A%28i%29%20outperforms%20previous%20state-of-the-art%20on%20both%20the%20DeformingThings4D%20and%0AD-FAUST%20datasets%2C%20and%20%28ii%29%20achieves%20more%20than%204x%20speedup%20compared%20to%20the%0Aprevious%20best%2C%20offering%20significant%20efficiency%20improvement.%0A&entry.1838667208=http%3A//arxiv.org/abs/2510.15800v1&entry.124074799=Read"},
{"title": "Euclidean Distance Matrix Completion via Asymmetric Projected Gradient\n  Descent", "author": "Yicheng Li and Xinghua Sun", "abstract": "  This paper proposes and analyzes a gradient-type algorithm based on\nBurer-Monteiro factorization, called the Asymmetric Projected Gradient Descent\n(APGD), for reconstructing the point set configuration from partial Euclidean\ndistance measurements, known as the Euclidean Distance Matrix Completion (EDMC)\nproblem. By paralleling the incoherence matrix completion framework, we show\nfor the first time that global convergence guarantee with exact recovery of\nthis routine can be established given $\\mathcal{O}(\\mu^2 r^3 \\kappa^2 n \\log\nn)$ Bernoulli random observations without any sample splitting. Unlike\nleveraging the tangent space Restricted Isometry Property (RIP) and local\ncurvature of the low-rank embedding manifold in some very recent works, our\nproof provides extra upper bounds that act as analogies of the random graph\nlemma under EDMC setting. The APGD works surprisingly well and numerical\nexperiments demonstrate exact linear convergence behavior in rich-sample\nregions yet deteriorates rapidly when compared with the performance obtained by\noptimizing the s-stress function, i.e., the standard but unexplained non-convex\napproach for EDMC, if the sample size is limited. While virtually matching our\ntheoretical prediction, this unusual phenomenon might indicate that: (i) the\npower of implicit regularization is weakened when specified in the APGD case;\n(ii) the stabilization of such new gradient direction requires substantially\nmore samples than the information-theoretic limit would suggest.\n", "link": "http://arxiv.org/abs/2504.19530v2", "date": "2025-10-17", "relevancy": 2.666, "topK": [{"title": "Dynamic Gaussians Mesh: Consistent Mesh Reconstruction from Monocular\n  Videos", "link": "http://arxiv.org/abs/2404.12379v2", "similarity": 0.5613}, {"title": "DDM-NET: End-to-end learning of keypoint feature Detection, Description and Matching for 3D localization", "link": "https://arxiv.org/abs/2212.04575", "similarity": 0.525}, {"title": "RTG-SLAM: Real-time 3D Reconstruction at Scale using Gaussian Splatting", "link": "http://arxiv.org/abs/2404.19706v1", "similarity": 0.5133}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Euclidean%20Distance%20Matrix%20Completion%20via%20Asymmetric%20Projected%20Gradient%0A%20%20Descent&body=Title%3A%20Euclidean%20Distance%20Matrix%20Completion%20via%20Asymmetric%20Projected%20Gradient%0A%20%20Descent%0AAuthor%3A%20Yicheng%20Li%20and%20Xinghua%20Sun%0AAbstract%3A%20%20%20This%20paper%20proposes%20and%20analyzes%20a%20gradient-type%20algorithm%20based%20on%0ABurer-Monteiro%20factorization%2C%20called%20the%20Asymmetric%20Projected%20Gradient%20Descent%0A%28APGD%29%2C%20for%20reconstructing%20the%20point%20set%20configuration%20from%20partial%20Euclidean%0Adistance%20measurements%2C%20known%20as%20the%20Euclidean%20Distance%20Matrix%20Completion%20%28EDMC%29%0Aproblem.%20By%20paralleling%20the%20incoherence%20matrix%20completion%20framework%2C%20we%20show%0Afor%20the%20first%20time%20that%20global%20convergence%20guarantee%20with%20exact%20recovery%20of%0Athis%20routine%20can%20be%20established%20given%20%24%5Cmathcal%7BO%7D%28%5Cmu%5E2%20r%5E3%20%5Ckappa%5E2%20n%20%5Clog%0An%29%24%20Bernoulli%20random%20observations%20without%20any%20sample%20splitting.%20Unlike%0Aleveraging%20the%20tangent%20space%20Restricted%20Isometry%20Property%20%28RIP%29%20and%20local%0Acurvature%20of%20the%20low-rank%20embedding%20manifold%20in%20some%20very%20recent%20works%2C%20our%0Aproof%20provides%20extra%20upper%20bounds%20that%20act%20as%20analogies%20of%20the%20random%20graph%0Alemma%20under%20EDMC%20setting.%20The%20APGD%20works%20surprisingly%20well%20and%20numerical%0Aexperiments%20demonstrate%20exact%20linear%20convergence%20behavior%20in%20rich-sample%0Aregions%20yet%20deteriorates%20rapidly%20when%20compared%20with%20the%20performance%20obtained%20by%0Aoptimizing%20the%20s-stress%20function%2C%20i.e.%2C%20the%20standard%20but%20unexplained%20non-convex%0Aapproach%20for%20EDMC%2C%20if%20the%20sample%20size%20is%20limited.%20While%20virtually%20matching%20our%0Atheoretical%20prediction%2C%20this%20unusual%20phenomenon%20might%20indicate%20that%3A%20%28i%29%20the%0Apower%20of%20implicit%20regularization%20is%20weakened%20when%20specified%20in%20the%20APGD%20case%3B%0A%28ii%29%20the%20stabilization%20of%20such%20new%20gradient%20direction%20requires%20substantially%0Amore%20samples%20than%20the%20information-theoretic%20limit%20would%20suggest.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2504.19530v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DEuclidean%2520Distance%2520Matrix%2520Completion%2520via%2520Asymmetric%2520Projected%2520Gradient%250A%2520%2520Descent%26entry.906535625%3DYicheng%2520Li%2520and%2520Xinghua%2520Sun%26entry.1292438233%3D%2520%2520This%2520paper%2520proposes%2520and%2520analyzes%2520a%2520gradient-type%2520algorithm%2520based%2520on%250ABurer-Monteiro%2520factorization%252C%2520called%2520the%2520Asymmetric%2520Projected%2520Gradient%2520Descent%250A%2528APGD%2529%252C%2520for%2520reconstructing%2520the%2520point%2520set%2520configuration%2520from%2520partial%2520Euclidean%250Adistance%2520measurements%252C%2520known%2520as%2520the%2520Euclidean%2520Distance%2520Matrix%2520Completion%2520%2528EDMC%2529%250Aproblem.%2520By%2520paralleling%2520the%2520incoherence%2520matrix%2520completion%2520framework%252C%2520we%2520show%250Afor%2520the%2520first%2520time%2520that%2520global%2520convergence%2520guarantee%2520with%2520exact%2520recovery%2520of%250Athis%2520routine%2520can%2520be%2520established%2520given%2520%2524%255Cmathcal%257BO%257D%2528%255Cmu%255E2%2520r%255E3%2520%255Ckappa%255E2%2520n%2520%255Clog%250An%2529%2524%2520Bernoulli%2520random%2520observations%2520without%2520any%2520sample%2520splitting.%2520Unlike%250Aleveraging%2520the%2520tangent%2520space%2520Restricted%2520Isometry%2520Property%2520%2528RIP%2529%2520and%2520local%250Acurvature%2520of%2520the%2520low-rank%2520embedding%2520manifold%2520in%2520some%2520very%2520recent%2520works%252C%2520our%250Aproof%2520provides%2520extra%2520upper%2520bounds%2520that%2520act%2520as%2520analogies%2520of%2520the%2520random%2520graph%250Alemma%2520under%2520EDMC%2520setting.%2520The%2520APGD%2520works%2520surprisingly%2520well%2520and%2520numerical%250Aexperiments%2520demonstrate%2520exact%2520linear%2520convergence%2520behavior%2520in%2520rich-sample%250Aregions%2520yet%2520deteriorates%2520rapidly%2520when%2520compared%2520with%2520the%2520performance%2520obtained%2520by%250Aoptimizing%2520the%2520s-stress%2520function%252C%2520i.e.%252C%2520the%2520standard%2520but%2520unexplained%2520non-convex%250Aapproach%2520for%2520EDMC%252C%2520if%2520the%2520sample%2520size%2520is%2520limited.%2520While%2520virtually%2520matching%2520our%250Atheoretical%2520prediction%252C%2520this%2520unusual%2520phenomenon%2520might%2520indicate%2520that%253A%2520%2528i%2529%2520the%250Apower%2520of%2520implicit%2520regularization%2520is%2520weakened%2520when%2520specified%2520in%2520the%2520APGD%2520case%253B%250A%2528ii%2529%2520the%2520stabilization%2520of%2520such%2520new%2520gradient%2520direction%2520requires%2520substantially%250Amore%2520samples%2520than%2520the%2520information-theoretic%2520limit%2520would%2520suggest.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2504.19530v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Euclidean%20Distance%20Matrix%20Completion%20via%20Asymmetric%20Projected%20Gradient%0A%20%20Descent&entry.906535625=Yicheng%20Li%20and%20Xinghua%20Sun&entry.1292438233=%20%20This%20paper%20proposes%20and%20analyzes%20a%20gradient-type%20algorithm%20based%20on%0ABurer-Monteiro%20factorization%2C%20called%20the%20Asymmetric%20Projected%20Gradient%20Descent%0A%28APGD%29%2C%20for%20reconstructing%20the%20point%20set%20configuration%20from%20partial%20Euclidean%0Adistance%20measurements%2C%20known%20as%20the%20Euclidean%20Distance%20Matrix%20Completion%20%28EDMC%29%0Aproblem.%20By%20paralleling%20the%20incoherence%20matrix%20completion%20framework%2C%20we%20show%0Afor%20the%20first%20time%20that%20global%20convergence%20guarantee%20with%20exact%20recovery%20of%0Athis%20routine%20can%20be%20established%20given%20%24%5Cmathcal%7BO%7D%28%5Cmu%5E2%20r%5E3%20%5Ckappa%5E2%20n%20%5Clog%0An%29%24%20Bernoulli%20random%20observations%20without%20any%20sample%20splitting.%20Unlike%0Aleveraging%20the%20tangent%20space%20Restricted%20Isometry%20Property%20%28RIP%29%20and%20local%0Acurvature%20of%20the%20low-rank%20embedding%20manifold%20in%20some%20very%20recent%20works%2C%20our%0Aproof%20provides%20extra%20upper%20bounds%20that%20act%20as%20analogies%20of%20the%20random%20graph%0Alemma%20under%20EDMC%20setting.%20The%20APGD%20works%20surprisingly%20well%20and%20numerical%0Aexperiments%20demonstrate%20exact%20linear%20convergence%20behavior%20in%20rich-sample%0Aregions%20yet%20deteriorates%20rapidly%20when%20compared%20with%20the%20performance%20obtained%20by%0Aoptimizing%20the%20s-stress%20function%2C%20i.e.%2C%20the%20standard%20but%20unexplained%20non-convex%0Aapproach%20for%20EDMC%2C%20if%20the%20sample%20size%20is%20limited.%20While%20virtually%20matching%20our%0Atheoretical%20prediction%2C%20this%20unusual%20phenomenon%20might%20indicate%20that%3A%20%28i%29%20the%0Apower%20of%20implicit%20regularization%20is%20weakened%20when%20specified%20in%20the%20APGD%20case%3B%0A%28ii%29%20the%20stabilization%20of%20such%20new%20gradient%20direction%20requires%20substantially%0Amore%20samples%20than%20the%20information-theoretic%20limit%20would%20suggest.%0A&entry.1838667208=http%3A//arxiv.org/abs/2504.19530v2&entry.124074799=Read"},
{"title": "PaddleOCR-VL: Boosting Multilingual Document Parsing via a 0.9B\n  Ultra-Compact Vision-Language Model", "author": "Cheng Cui and Ting Sun and Suyin Liang and Tingquan Gao and Zelun Zhang and Jiaxuan Liu and Xueqing Wang and Changda Zhou and Hongen Liu and Manhui Lin and Yue Zhang and Yubo Zhang and Handong Zheng and Jing Zhang and Jun Zhang and Yi Liu and Dianhai Yu and Yanjun Ma", "abstract": "  In this report, we propose PaddleOCR-VL, a SOTA and resource-efficient model\ntailored for document parsing. Its core component is PaddleOCR-VL-0.9B, a\ncompact yet powerful vision-language model (VLM) that integrates a NaViT-style\ndynamic resolution visual encoder with the ERNIE-4.5-0.3B language model to\nenable accurate element recognition. This innovative model efficiently supports\n109 languages and excels in recognizing complex elements (e.g., text, tables,\nformulas, and charts), while maintaining minimal resource consumption. Through\ncomprehensive evaluations on widely used public benchmarks and in-house\nbenchmarks, PaddleOCR-VL achieves SOTA performance in both page-level document\nparsing and element-level recognition. It significantly outperforms existing\nsolutions, exhibits strong competitiveness against top-tier VLMs, and delivers\nfast inference speeds. These strengths make it highly suitable for practical\ndeployment in real-world scenarios. Code is available at\nhttps://github.com/PaddlePaddle/PaddleOCR .\n", "link": "http://arxiv.org/abs/2510.14528v2", "date": "2025-10-17", "relevancy": 2.6505, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5523}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5523}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.4856}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20PaddleOCR-VL%3A%20Boosting%20Multilingual%20Document%20Parsing%20via%20a%200.9B%0A%20%20Ultra-Compact%20Vision-Language%20Model&body=Title%3A%20PaddleOCR-VL%3A%20Boosting%20Multilingual%20Document%20Parsing%20via%20a%200.9B%0A%20%20Ultra-Compact%20Vision-Language%20Model%0AAuthor%3A%20Cheng%20Cui%20and%20Ting%20Sun%20and%20Suyin%20Liang%20and%20Tingquan%20Gao%20and%20Zelun%20Zhang%20and%20Jiaxuan%20Liu%20and%20Xueqing%20Wang%20and%20Changda%20Zhou%20and%20Hongen%20Liu%20and%20Manhui%20Lin%20and%20Yue%20Zhang%20and%20Yubo%20Zhang%20and%20Handong%20Zheng%20and%20Jing%20Zhang%20and%20Jun%20Zhang%20and%20Yi%20Liu%20and%20Dianhai%20Yu%20and%20Yanjun%20Ma%0AAbstract%3A%20%20%20In%20this%20report%2C%20we%20propose%20PaddleOCR-VL%2C%20a%20SOTA%20and%20resource-efficient%20model%0Atailored%20for%20document%20parsing.%20Its%20core%20component%20is%20PaddleOCR-VL-0.9B%2C%20a%0Acompact%20yet%20powerful%20vision-language%20model%20%28VLM%29%20that%20integrates%20a%20NaViT-style%0Adynamic%20resolution%20visual%20encoder%20with%20the%20ERNIE-4.5-0.3B%20language%20model%20to%0Aenable%20accurate%20element%20recognition.%20This%20innovative%20model%20efficiently%20supports%0A109%20languages%20and%20excels%20in%20recognizing%20complex%20elements%20%28e.g.%2C%20text%2C%20tables%2C%0Aformulas%2C%20and%20charts%29%2C%20while%20maintaining%20minimal%20resource%20consumption.%20Through%0Acomprehensive%20evaluations%20on%20widely%20used%20public%20benchmarks%20and%20in-house%0Abenchmarks%2C%20PaddleOCR-VL%20achieves%20SOTA%20performance%20in%20both%20page-level%20document%0Aparsing%20and%20element-level%20recognition.%20It%20significantly%20outperforms%20existing%0Asolutions%2C%20exhibits%20strong%20competitiveness%20against%20top-tier%20VLMs%2C%20and%20delivers%0Afast%20inference%20speeds.%20These%20strengths%20make%20it%20highly%20suitable%20for%20practical%0Adeployment%20in%20real-world%20scenarios.%20Code%20is%20available%20at%0Ahttps%3A//github.com/PaddlePaddle/PaddleOCR%20.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2510.14528v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DPaddleOCR-VL%253A%2520Boosting%2520Multilingual%2520Document%2520Parsing%2520via%2520a%25200.9B%250A%2520%2520Ultra-Compact%2520Vision-Language%2520Model%26entry.906535625%3DCheng%2520Cui%2520and%2520Ting%2520Sun%2520and%2520Suyin%2520Liang%2520and%2520Tingquan%2520Gao%2520and%2520Zelun%2520Zhang%2520and%2520Jiaxuan%2520Liu%2520and%2520Xueqing%2520Wang%2520and%2520Changda%2520Zhou%2520and%2520Hongen%2520Liu%2520and%2520Manhui%2520Lin%2520and%2520Yue%2520Zhang%2520and%2520Yubo%2520Zhang%2520and%2520Handong%2520Zheng%2520and%2520Jing%2520Zhang%2520and%2520Jun%2520Zhang%2520and%2520Yi%2520Liu%2520and%2520Dianhai%2520Yu%2520and%2520Yanjun%2520Ma%26entry.1292438233%3D%2520%2520In%2520this%2520report%252C%2520we%2520propose%2520PaddleOCR-VL%252C%2520a%2520SOTA%2520and%2520resource-efficient%2520model%250Atailored%2520for%2520document%2520parsing.%2520Its%2520core%2520component%2520is%2520PaddleOCR-VL-0.9B%252C%2520a%250Acompact%2520yet%2520powerful%2520vision-language%2520model%2520%2528VLM%2529%2520that%2520integrates%2520a%2520NaViT-style%250Adynamic%2520resolution%2520visual%2520encoder%2520with%2520the%2520ERNIE-4.5-0.3B%2520language%2520model%2520to%250Aenable%2520accurate%2520element%2520recognition.%2520This%2520innovative%2520model%2520efficiently%2520supports%250A109%2520languages%2520and%2520excels%2520in%2520recognizing%2520complex%2520elements%2520%2528e.g.%252C%2520text%252C%2520tables%252C%250Aformulas%252C%2520and%2520charts%2529%252C%2520while%2520maintaining%2520minimal%2520resource%2520consumption.%2520Through%250Acomprehensive%2520evaluations%2520on%2520widely%2520used%2520public%2520benchmarks%2520and%2520in-house%250Abenchmarks%252C%2520PaddleOCR-VL%2520achieves%2520SOTA%2520performance%2520in%2520both%2520page-level%2520document%250Aparsing%2520and%2520element-level%2520recognition.%2520It%2520significantly%2520outperforms%2520existing%250Asolutions%252C%2520exhibits%2520strong%2520competitiveness%2520against%2520top-tier%2520VLMs%252C%2520and%2520delivers%250Afast%2520inference%2520speeds.%2520These%2520strengths%2520make%2520it%2520highly%2520suitable%2520for%2520practical%250Adeployment%2520in%2520real-world%2520scenarios.%2520Code%2520is%2520available%2520at%250Ahttps%253A//github.com/PaddlePaddle/PaddleOCR%2520.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2510.14528v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=PaddleOCR-VL%3A%20Boosting%20Multilingual%20Document%20Parsing%20via%20a%200.9B%0A%20%20Ultra-Compact%20Vision-Language%20Model&entry.906535625=Cheng%20Cui%20and%20Ting%20Sun%20and%20Suyin%20Liang%20and%20Tingquan%20Gao%20and%20Zelun%20Zhang%20and%20Jiaxuan%20Liu%20and%20Xueqing%20Wang%20and%20Changda%20Zhou%20and%20Hongen%20Liu%20and%20Manhui%20Lin%20and%20Yue%20Zhang%20and%20Yubo%20Zhang%20and%20Handong%20Zheng%20and%20Jing%20Zhang%20and%20Jun%20Zhang%20and%20Yi%20Liu%20and%20Dianhai%20Yu%20and%20Yanjun%20Ma&entry.1292438233=%20%20In%20this%20report%2C%20we%20propose%20PaddleOCR-VL%2C%20a%20SOTA%20and%20resource-efficient%20model%0Atailored%20for%20document%20parsing.%20Its%20core%20component%20is%20PaddleOCR-VL-0.9B%2C%20a%0Acompact%20yet%20powerful%20vision-language%20model%20%28VLM%29%20that%20integrates%20a%20NaViT-style%0Adynamic%20resolution%20visual%20encoder%20with%20the%20ERNIE-4.5-0.3B%20language%20model%20to%0Aenable%20accurate%20element%20recognition.%20This%20innovative%20model%20efficiently%20supports%0A109%20languages%20and%20excels%20in%20recognizing%20complex%20elements%20%28e.g.%2C%20text%2C%20tables%2C%0Aformulas%2C%20and%20charts%29%2C%20while%20maintaining%20minimal%20resource%20consumption.%20Through%0Acomprehensive%20evaluations%20on%20widely%20used%20public%20benchmarks%20and%20in-house%0Abenchmarks%2C%20PaddleOCR-VL%20achieves%20SOTA%20performance%20in%20both%20page-level%20document%0Aparsing%20and%20element-level%20recognition.%20It%20significantly%20outperforms%20existing%0Asolutions%2C%20exhibits%20strong%20competitiveness%20against%20top-tier%20VLMs%2C%20and%20delivers%0Afast%20inference%20speeds.%20These%20strengths%20make%20it%20highly%20suitable%20for%20practical%0Adeployment%20in%20real-world%20scenarios.%20Code%20is%20available%20at%0Ahttps%3A//github.com/PaddlePaddle/PaddleOCR%20.%0A&entry.1838667208=http%3A//arxiv.org/abs/2510.14528v2&entry.124074799=Read"},
{"title": "Skyfall-GS: Synthesizing Immersive 3D Urban Scenes from Satellite\n  Imagery", "author": "Jie-Ying Lee and Yi-Ruei Liu and Shr-Ruei Tsai and Wei-Cheng Chang and Chung-Ho Wu and Jiewen Chan and Zhenjun Zhao and Chieh Hubert Lin and Yu-Lun Liu", "abstract": "  Synthesizing large-scale, explorable, and geometrically accurate 3D urban\nscenes is a challenging yet valuable task in providing immersive and embodied\napplications. The challenges lie in the lack of large-scale and high-quality\nreal-world 3D scans for training generalizable generative models. In this\npaper, we take an alternative route to create large-scale 3D scenes by\nsynergizing the readily available satellite imagery that supplies realistic\ncoarse geometry and the open-domain diffusion model for creating high-quality\nclose-up appearances. We propose \\textbf{Skyfall-GS}, the first city-block\nscale 3D scene creation framework without costly 3D annotations, also featuring\nreal-time, immersive 3D exploration. We tailor a curriculum-driven iterative\nrefinement strategy to progressively enhance geometric completeness and\nphotorealistic textures. Extensive experiments demonstrate that Skyfall-GS\nprovides improved cross-view consistent geometry and more realistic textures\ncompared to state-of-the-art approaches. Project page:\nhttps://skyfall-gs.jayinnn.dev/\n", "link": "http://arxiv.org/abs/2510.15869v1", "date": "2025-10-17", "relevancy": 2.6461, "topK": [{"title": "WorldExplorer: Towards Generating Fully Navigable 3D Scenes", "link": "http://arxiv.org/abs/2506.01799v2", "similarity": 0.6674}, {"title": "CAT3D: Create Anything in 3D with Multi-View Diffusion Models", "link": "http://arxiv.org/abs/2405.10314v1", "similarity": 0.6604}, {"title": "CAT3D: Create Anything in 3D with Multi-View Diffusion Models", "link": "http://arxiv.org/abs/2405.10314v1", "similarity": 0.6604}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Skyfall-GS%3A%20Synthesizing%20Immersive%203D%20Urban%20Scenes%20from%20Satellite%0A%20%20Imagery&body=Title%3A%20Skyfall-GS%3A%20Synthesizing%20Immersive%203D%20Urban%20Scenes%20from%20Satellite%0A%20%20Imagery%0AAuthor%3A%20Jie-Ying%20Lee%20and%20Yi-Ruei%20Liu%20and%20Shr-Ruei%20Tsai%20and%20Wei-Cheng%20Chang%20and%20Chung-Ho%20Wu%20and%20Jiewen%20Chan%20and%20Zhenjun%20Zhao%20and%20Chieh%20Hubert%20Lin%20and%20Yu-Lun%20Liu%0AAbstract%3A%20%20%20Synthesizing%20large-scale%2C%20explorable%2C%20and%20geometrically%20accurate%203D%20urban%0Ascenes%20is%20a%20challenging%20yet%20valuable%20task%20in%20providing%20immersive%20and%20embodied%0Aapplications.%20The%20challenges%20lie%20in%20the%20lack%20of%20large-scale%20and%20high-quality%0Areal-world%203D%20scans%20for%20training%20generalizable%20generative%20models.%20In%20this%0Apaper%2C%20we%20take%20an%20alternative%20route%20to%20create%20large-scale%203D%20scenes%20by%0Asynergizing%20the%20readily%20available%20satellite%20imagery%20that%20supplies%20realistic%0Acoarse%20geometry%20and%20the%20open-domain%20diffusion%20model%20for%20creating%20high-quality%0Aclose-up%20appearances.%20We%20propose%20%5Ctextbf%7BSkyfall-GS%7D%2C%20the%20first%20city-block%0Ascale%203D%20scene%20creation%20framework%20without%20costly%203D%20annotations%2C%20also%20featuring%0Areal-time%2C%20immersive%203D%20exploration.%20We%20tailor%20a%20curriculum-driven%20iterative%0Arefinement%20strategy%20to%20progressively%20enhance%20geometric%20completeness%20and%0Aphotorealistic%20textures.%20Extensive%20experiments%20demonstrate%20that%20Skyfall-GS%0Aprovides%20improved%20cross-view%20consistent%20geometry%20and%20more%20realistic%20textures%0Acompared%20to%20state-of-the-art%20approaches.%20Project%20page%3A%0Ahttps%3A//skyfall-gs.jayinnn.dev/%0A%0ALink%3A%20http%3A//arxiv.org/abs/2510.15869v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DSkyfall-GS%253A%2520Synthesizing%2520Immersive%25203D%2520Urban%2520Scenes%2520from%2520Satellite%250A%2520%2520Imagery%26entry.906535625%3DJie-Ying%2520Lee%2520and%2520Yi-Ruei%2520Liu%2520and%2520Shr-Ruei%2520Tsai%2520and%2520Wei-Cheng%2520Chang%2520and%2520Chung-Ho%2520Wu%2520and%2520Jiewen%2520Chan%2520and%2520Zhenjun%2520Zhao%2520and%2520Chieh%2520Hubert%2520Lin%2520and%2520Yu-Lun%2520Liu%26entry.1292438233%3D%2520%2520Synthesizing%2520large-scale%252C%2520explorable%252C%2520and%2520geometrically%2520accurate%25203D%2520urban%250Ascenes%2520is%2520a%2520challenging%2520yet%2520valuable%2520task%2520in%2520providing%2520immersive%2520and%2520embodied%250Aapplications.%2520The%2520challenges%2520lie%2520in%2520the%2520lack%2520of%2520large-scale%2520and%2520high-quality%250Areal-world%25203D%2520scans%2520for%2520training%2520generalizable%2520generative%2520models.%2520In%2520this%250Apaper%252C%2520we%2520take%2520an%2520alternative%2520route%2520to%2520create%2520large-scale%25203D%2520scenes%2520by%250Asynergizing%2520the%2520readily%2520available%2520satellite%2520imagery%2520that%2520supplies%2520realistic%250Acoarse%2520geometry%2520and%2520the%2520open-domain%2520diffusion%2520model%2520for%2520creating%2520high-quality%250Aclose-up%2520appearances.%2520We%2520propose%2520%255Ctextbf%257BSkyfall-GS%257D%252C%2520the%2520first%2520city-block%250Ascale%25203D%2520scene%2520creation%2520framework%2520without%2520costly%25203D%2520annotations%252C%2520also%2520featuring%250Areal-time%252C%2520immersive%25203D%2520exploration.%2520We%2520tailor%2520a%2520curriculum-driven%2520iterative%250Arefinement%2520strategy%2520to%2520progressively%2520enhance%2520geometric%2520completeness%2520and%250Aphotorealistic%2520textures.%2520Extensive%2520experiments%2520demonstrate%2520that%2520Skyfall-GS%250Aprovides%2520improved%2520cross-view%2520consistent%2520geometry%2520and%2520more%2520realistic%2520textures%250Acompared%2520to%2520state-of-the-art%2520approaches.%2520Project%2520page%253A%250Ahttps%253A//skyfall-gs.jayinnn.dev/%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2510.15869v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Skyfall-GS%3A%20Synthesizing%20Immersive%203D%20Urban%20Scenes%20from%20Satellite%0A%20%20Imagery&entry.906535625=Jie-Ying%20Lee%20and%20Yi-Ruei%20Liu%20and%20Shr-Ruei%20Tsai%20and%20Wei-Cheng%20Chang%20and%20Chung-Ho%20Wu%20and%20Jiewen%20Chan%20and%20Zhenjun%20Zhao%20and%20Chieh%20Hubert%20Lin%20and%20Yu-Lun%20Liu&entry.1292438233=%20%20Synthesizing%20large-scale%2C%20explorable%2C%20and%20geometrically%20accurate%203D%20urban%0Ascenes%20is%20a%20challenging%20yet%20valuable%20task%20in%20providing%20immersive%20and%20embodied%0Aapplications.%20The%20challenges%20lie%20in%20the%20lack%20of%20large-scale%20and%20high-quality%0Areal-world%203D%20scans%20for%20training%20generalizable%20generative%20models.%20In%20this%0Apaper%2C%20we%20take%20an%20alternative%20route%20to%20create%20large-scale%203D%20scenes%20by%0Asynergizing%20the%20readily%20available%20satellite%20imagery%20that%20supplies%20realistic%0Acoarse%20geometry%20and%20the%20open-domain%20diffusion%20model%20for%20creating%20high-quality%0Aclose-up%20appearances.%20We%20propose%20%5Ctextbf%7BSkyfall-GS%7D%2C%20the%20first%20city-block%0Ascale%203D%20scene%20creation%20framework%20without%20costly%203D%20annotations%2C%20also%20featuring%0Areal-time%2C%20immersive%203D%20exploration.%20We%20tailor%20a%20curriculum-driven%20iterative%0Arefinement%20strategy%20to%20progressively%20enhance%20geometric%20completeness%20and%0Aphotorealistic%20textures.%20Extensive%20experiments%20demonstrate%20that%20Skyfall-GS%0Aprovides%20improved%20cross-view%20consistent%20geometry%20and%20more%20realistic%20textures%0Acompared%20to%20state-of-the-art%20approaches.%20Project%20page%3A%0Ahttps%3A//skyfall-gs.jayinnn.dev/%0A&entry.1838667208=http%3A//arxiv.org/abs/2510.15869v1&entry.124074799=Read"},
{"title": "SAMix: Calibrated and Accurate Continual Learning via Sphere-Adaptive\n  Mixup and Neural Collapse", "author": "Trung-Anh Dang and Vincent Nguyen and Ngoc-Son Vu and Christel Vrain", "abstract": "  While most continual learning methods focus on mitigating forgetting and\nimproving accuracy, they often overlook the critical aspect of network\ncalibration, despite its importance. Neural collapse, a phenomenon where\nlast-layer features collapse to their class means, has demonstrated advantages\nin continual learning by reducing feature-classifier misalignment. Few works\naim to improve the calibration of continual models for more reliable\npredictions. Our work goes a step further by proposing a novel method that not\nonly enhances calibration but also improves performance by reducing\noverconfidence, mitigating forgetting, and increasing accuracy. We introduce\nSphere-Adaptive Mixup (SAMix), an adaptive mixup strategy tailored for neural\ncollapse-based methods. SAMix adapts the mixing process to the geometric\nproperties of feature spaces under neural collapse, ensuring more robust\nregularization and alignment. Experiments show that SAMix significantly boosts\nperformance, surpassing SOTA methods in continual learning while also improving\nmodel calibration. SAMix enhances both across-task accuracy and the broader\nreliability of predictions, making it a promising advancement for robust\ncontinual learning systems.\n", "link": "http://arxiv.org/abs/2510.15751v1", "date": "2025-10-17", "relevancy": 2.6003, "topK": [{"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5331}, {"title": "Breadcrumbs: Adversarial class-balanced sampling for long-tailed recognition", "link": "https://link.springer.com/chapter/10.1007/978-3-031-20053-3_37", "similarity": 0.515}, {"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.5121}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20SAMix%3A%20Calibrated%20and%20Accurate%20Continual%20Learning%20via%20Sphere-Adaptive%0A%20%20Mixup%20and%20Neural%20Collapse&body=Title%3A%20SAMix%3A%20Calibrated%20and%20Accurate%20Continual%20Learning%20via%20Sphere-Adaptive%0A%20%20Mixup%20and%20Neural%20Collapse%0AAuthor%3A%20Trung-Anh%20Dang%20and%20Vincent%20Nguyen%20and%20Ngoc-Son%20Vu%20and%20Christel%20Vrain%0AAbstract%3A%20%20%20While%20most%20continual%20learning%20methods%20focus%20on%20mitigating%20forgetting%20and%0Aimproving%20accuracy%2C%20they%20often%20overlook%20the%20critical%20aspect%20of%20network%0Acalibration%2C%20despite%20its%20importance.%20Neural%20collapse%2C%20a%20phenomenon%20where%0Alast-layer%20features%20collapse%20to%20their%20class%20means%2C%20has%20demonstrated%20advantages%0Ain%20continual%20learning%20by%20reducing%20feature-classifier%20misalignment.%20Few%20works%0Aaim%20to%20improve%20the%20calibration%20of%20continual%20models%20for%20more%20reliable%0Apredictions.%20Our%20work%20goes%20a%20step%20further%20by%20proposing%20a%20novel%20method%20that%20not%0Aonly%20enhances%20calibration%20but%20also%20improves%20performance%20by%20reducing%0Aoverconfidence%2C%20mitigating%20forgetting%2C%20and%20increasing%20accuracy.%20We%20introduce%0ASphere-Adaptive%20Mixup%20%28SAMix%29%2C%20an%20adaptive%20mixup%20strategy%20tailored%20for%20neural%0Acollapse-based%20methods.%20SAMix%20adapts%20the%20mixing%20process%20to%20the%20geometric%0Aproperties%20of%20feature%20spaces%20under%20neural%20collapse%2C%20ensuring%20more%20robust%0Aregularization%20and%20alignment.%20Experiments%20show%20that%20SAMix%20significantly%20boosts%0Aperformance%2C%20surpassing%20SOTA%20methods%20in%20continual%20learning%20while%20also%20improving%0Amodel%20calibration.%20SAMix%20enhances%20both%20across-task%20accuracy%20and%20the%20broader%0Areliability%20of%20predictions%2C%20making%20it%20a%20promising%20advancement%20for%20robust%0Acontinual%20learning%20systems.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2510.15751v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DSAMix%253A%2520Calibrated%2520and%2520Accurate%2520Continual%2520Learning%2520via%2520Sphere-Adaptive%250A%2520%2520Mixup%2520and%2520Neural%2520Collapse%26entry.906535625%3DTrung-Anh%2520Dang%2520and%2520Vincent%2520Nguyen%2520and%2520Ngoc-Son%2520Vu%2520and%2520Christel%2520Vrain%26entry.1292438233%3D%2520%2520While%2520most%2520continual%2520learning%2520methods%2520focus%2520on%2520mitigating%2520forgetting%2520and%250Aimproving%2520accuracy%252C%2520they%2520often%2520overlook%2520the%2520critical%2520aspect%2520of%2520network%250Acalibration%252C%2520despite%2520its%2520importance.%2520Neural%2520collapse%252C%2520a%2520phenomenon%2520where%250Alast-layer%2520features%2520collapse%2520to%2520their%2520class%2520means%252C%2520has%2520demonstrated%2520advantages%250Ain%2520continual%2520learning%2520by%2520reducing%2520feature-classifier%2520misalignment.%2520Few%2520works%250Aaim%2520to%2520improve%2520the%2520calibration%2520of%2520continual%2520models%2520for%2520more%2520reliable%250Apredictions.%2520Our%2520work%2520goes%2520a%2520step%2520further%2520by%2520proposing%2520a%2520novel%2520method%2520that%2520not%250Aonly%2520enhances%2520calibration%2520but%2520also%2520improves%2520performance%2520by%2520reducing%250Aoverconfidence%252C%2520mitigating%2520forgetting%252C%2520and%2520increasing%2520accuracy.%2520We%2520introduce%250ASphere-Adaptive%2520Mixup%2520%2528SAMix%2529%252C%2520an%2520adaptive%2520mixup%2520strategy%2520tailored%2520for%2520neural%250Acollapse-based%2520methods.%2520SAMix%2520adapts%2520the%2520mixing%2520process%2520to%2520the%2520geometric%250Aproperties%2520of%2520feature%2520spaces%2520under%2520neural%2520collapse%252C%2520ensuring%2520more%2520robust%250Aregularization%2520and%2520alignment.%2520Experiments%2520show%2520that%2520SAMix%2520significantly%2520boosts%250Aperformance%252C%2520surpassing%2520SOTA%2520methods%2520in%2520continual%2520learning%2520while%2520also%2520improving%250Amodel%2520calibration.%2520SAMix%2520enhances%2520both%2520across-task%2520accuracy%2520and%2520the%2520broader%250Areliability%2520of%2520predictions%252C%2520making%2520it%2520a%2520promising%2520advancement%2520for%2520robust%250Acontinual%2520learning%2520systems.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2510.15751v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=SAMix%3A%20Calibrated%20and%20Accurate%20Continual%20Learning%20via%20Sphere-Adaptive%0A%20%20Mixup%20and%20Neural%20Collapse&entry.906535625=Trung-Anh%20Dang%20and%20Vincent%20Nguyen%20and%20Ngoc-Son%20Vu%20and%20Christel%20Vrain&entry.1292438233=%20%20While%20most%20continual%20learning%20methods%20focus%20on%20mitigating%20forgetting%20and%0Aimproving%20accuracy%2C%20they%20often%20overlook%20the%20critical%20aspect%20of%20network%0Acalibration%2C%20despite%20its%20importance.%20Neural%20collapse%2C%20a%20phenomenon%20where%0Alast-layer%20features%20collapse%20to%20their%20class%20means%2C%20has%20demonstrated%20advantages%0Ain%20continual%20learning%20by%20reducing%20feature-classifier%20misalignment.%20Few%20works%0Aaim%20to%20improve%20the%20calibration%20of%20continual%20models%20for%20more%20reliable%0Apredictions.%20Our%20work%20goes%20a%20step%20further%20by%20proposing%20a%20novel%20method%20that%20not%0Aonly%20enhances%20calibration%20but%20also%20improves%20performance%20by%20reducing%0Aoverconfidence%2C%20mitigating%20forgetting%2C%20and%20increasing%20accuracy.%20We%20introduce%0ASphere-Adaptive%20Mixup%20%28SAMix%29%2C%20an%20adaptive%20mixup%20strategy%20tailored%20for%20neural%0Acollapse-based%20methods.%20SAMix%20adapts%20the%20mixing%20process%20to%20the%20geometric%0Aproperties%20of%20feature%20spaces%20under%20neural%20collapse%2C%20ensuring%20more%20robust%0Aregularization%20and%20alignment.%20Experiments%20show%20that%20SAMix%20significantly%20boosts%0Aperformance%2C%20surpassing%20SOTA%20methods%20in%20continual%20learning%20while%20also%20improving%0Amodel%20calibration.%20SAMix%20enhances%20both%20across-task%20accuracy%20and%20the%20broader%0Areliability%20of%20predictions%2C%20making%20it%20a%20promising%20advancement%20for%20robust%0Acontinual%20learning%20systems.%0A&entry.1838667208=http%3A//arxiv.org/abs/2510.15751v1&entry.124074799=Read"},
{"title": "Foundation Model-Driven Classification of Atypical Mitotic Figures with\n  Domain-Aware Training Strategies", "author": "Piotr Giedziun and Jan So\u0142tysik and Mateusz G\u00f3rczany and Norbert Ropiak and Marcin Przymus and Piotr Krajewski and Jaros\u0142aw Kwiecie\u0144 and Artur Bartczak and Izabela Wasiak and Mateusz Maniewski", "abstract": "  We present a solution for the MIDOG 2025 Challenge Track~2, addressing binary\nclassification of normal mitotic figures (NMFs) versus atypical mitotic figures\n(AMFs). The approach leverages pathology-specific foundation model H-optimus-0,\nselected based on recent cross-domain generalization benchmarks and our\nempirical testing, with Low-Rank Adaptation (LoRA) fine-tuning and MixUp\naugmentation. Implementation includes soft labels based on multi-expert\nconsensus, hard negative mining, and adaptive focal loss, metric learning and\ndomain adaptation. The method demonstrates both the promise and challenges of\napplying foundation models to this complex classification task, achieving\nreasonable performance in the preliminary evaluation phase.\n", "link": "http://arxiv.org/abs/2509.02601v2", "date": "2025-10-17", "relevancy": 2.5999, "topK": [{"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5339}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.513}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.513}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Foundation%20Model-Driven%20Classification%20of%20Atypical%20Mitotic%20Figures%20with%0A%20%20Domain-Aware%20Training%20Strategies&body=Title%3A%20Foundation%20Model-Driven%20Classification%20of%20Atypical%20Mitotic%20Figures%20with%0A%20%20Domain-Aware%20Training%20Strategies%0AAuthor%3A%20Piotr%20Giedziun%20and%20Jan%20So%C5%82tysik%20and%20Mateusz%20G%C3%B3rczany%20and%20Norbert%20Ropiak%20and%20Marcin%20Przymus%20and%20Piotr%20Krajewski%20and%20Jaros%C5%82aw%20Kwiecie%C5%84%20and%20Artur%20Bartczak%20and%20Izabela%20Wasiak%20and%20Mateusz%20Maniewski%0AAbstract%3A%20%20%20We%20present%20a%20solution%20for%20the%20MIDOG%202025%20Challenge%20Track~2%2C%20addressing%20binary%0Aclassification%20of%20normal%20mitotic%20figures%20%28NMFs%29%20versus%20atypical%20mitotic%20figures%0A%28AMFs%29.%20The%20approach%20leverages%20pathology-specific%20foundation%20model%20H-optimus-0%2C%0Aselected%20based%20on%20recent%20cross-domain%20generalization%20benchmarks%20and%20our%0Aempirical%20testing%2C%20with%20Low-Rank%20Adaptation%20%28LoRA%29%20fine-tuning%20and%20MixUp%0Aaugmentation.%20Implementation%20includes%20soft%20labels%20based%20on%20multi-expert%0Aconsensus%2C%20hard%20negative%20mining%2C%20and%20adaptive%20focal%20loss%2C%20metric%20learning%20and%0Adomain%20adaptation.%20The%20method%20demonstrates%20both%20the%20promise%20and%20challenges%20of%0Aapplying%20foundation%20models%20to%20this%20complex%20classification%20task%2C%20achieving%0Areasonable%20performance%20in%20the%20preliminary%20evaluation%20phase.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2509.02601v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DFoundation%2520Model-Driven%2520Classification%2520of%2520Atypical%2520Mitotic%2520Figures%2520with%250A%2520%2520Domain-Aware%2520Training%2520Strategies%26entry.906535625%3DPiotr%2520Giedziun%2520and%2520Jan%2520So%25C5%2582tysik%2520and%2520Mateusz%2520G%25C3%25B3rczany%2520and%2520Norbert%2520Ropiak%2520and%2520Marcin%2520Przymus%2520and%2520Piotr%2520Krajewski%2520and%2520Jaros%25C5%2582aw%2520Kwiecie%25C5%2584%2520and%2520Artur%2520Bartczak%2520and%2520Izabela%2520Wasiak%2520and%2520Mateusz%2520Maniewski%26entry.1292438233%3D%2520%2520We%2520present%2520a%2520solution%2520for%2520the%2520MIDOG%25202025%2520Challenge%2520Track~2%252C%2520addressing%2520binary%250Aclassification%2520of%2520normal%2520mitotic%2520figures%2520%2528NMFs%2529%2520versus%2520atypical%2520mitotic%2520figures%250A%2528AMFs%2529.%2520The%2520approach%2520leverages%2520pathology-specific%2520foundation%2520model%2520H-optimus-0%252C%250Aselected%2520based%2520on%2520recent%2520cross-domain%2520generalization%2520benchmarks%2520and%2520our%250Aempirical%2520testing%252C%2520with%2520Low-Rank%2520Adaptation%2520%2528LoRA%2529%2520fine-tuning%2520and%2520MixUp%250Aaugmentation.%2520Implementation%2520includes%2520soft%2520labels%2520based%2520on%2520multi-expert%250Aconsensus%252C%2520hard%2520negative%2520mining%252C%2520and%2520adaptive%2520focal%2520loss%252C%2520metric%2520learning%2520and%250Adomain%2520adaptation.%2520The%2520method%2520demonstrates%2520both%2520the%2520promise%2520and%2520challenges%2520of%250Aapplying%2520foundation%2520models%2520to%2520this%2520complex%2520classification%2520task%252C%2520achieving%250Areasonable%2520performance%2520in%2520the%2520preliminary%2520evaluation%2520phase.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2509.02601v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Foundation%20Model-Driven%20Classification%20of%20Atypical%20Mitotic%20Figures%20with%0A%20%20Domain-Aware%20Training%20Strategies&entry.906535625=Piotr%20Giedziun%20and%20Jan%20So%C5%82tysik%20and%20Mateusz%20G%C3%B3rczany%20and%20Norbert%20Ropiak%20and%20Marcin%20Przymus%20and%20Piotr%20Krajewski%20and%20Jaros%C5%82aw%20Kwiecie%C5%84%20and%20Artur%20Bartczak%20and%20Izabela%20Wasiak%20and%20Mateusz%20Maniewski&entry.1292438233=%20%20We%20present%20a%20solution%20for%20the%20MIDOG%202025%20Challenge%20Track~2%2C%20addressing%20binary%0Aclassification%20of%20normal%20mitotic%20figures%20%28NMFs%29%20versus%20atypical%20mitotic%20figures%0A%28AMFs%29.%20The%20approach%20leverages%20pathology-specific%20foundation%20model%20H-optimus-0%2C%0Aselected%20based%20on%20recent%20cross-domain%20generalization%20benchmarks%20and%20our%0Aempirical%20testing%2C%20with%20Low-Rank%20Adaptation%20%28LoRA%29%20fine-tuning%20and%20MixUp%0Aaugmentation.%20Implementation%20includes%20soft%20labels%20based%20on%20multi-expert%0Aconsensus%2C%20hard%20negative%20mining%2C%20and%20adaptive%20focal%20loss%2C%20metric%20learning%20and%0Adomain%20adaptation.%20The%20method%20demonstrates%20both%20the%20promise%20and%20challenges%20of%0Aapplying%20foundation%20models%20to%20this%20complex%20classification%20task%2C%20achieving%0Areasonable%20performance%20in%20the%20preliminary%20evaluation%20phase.%0A&entry.1838667208=http%3A//arxiv.org/abs/2509.02601v2&entry.124074799=Read"},
{"title": "Deep Learning Based Domain Adaptation Methods in Remote Sensing: A\n  Comprehensive Survey", "author": "Shuchang Lyu and Qi Zhao and Zheng Zhou and Meng Li and You Zhou and Dingding Yao and Guangliang Cheng and Huiyu Zhou and Zhenwei Shi", "abstract": "  Domain adaptation is a crucial and increasingly important task in remote\nsensing, aiming to transfer knowledge from a source domain a differently\ndistributed target domain. It has broad applications across various real-world\napplications, including remote sensing element interpretation, ecological\nenvironment monitoring, and urban/rural planning. However, domain adaptation in\nremote sensing poses significant challenges due to differences in data, such as\nvariations in ground sampling distance, imaging modes from various sensors,\ngeographical landscapes, and environmental conditions. In recent years, deep\nlearning has emerged as a powerful tool for feature representation and\ncross-domain knowledge transfer, leading to widespread adoption in remote\nsensing tasks. In this paper, we present a comprehensive survey of significant\nadvancements in deep learning based domain adaptation for remote sensing. We\nfirst introduce the preliminary knowledge to clarify key concepts, mathematical\nnotations, and the taxonomy of methodologies. We then organize existing\nalgorithms from multiple perspectives, including task categorization, input\nmode, supervision paradigm, and algorithmic granularity, providing readers with\na structured understanding of the field. Next, we review widely used datasets\nand summarize the performance of state-of-the-art methods to provide an\noverview of current progress. We also identify open challenges and potential\ndirections to guide future research in domain adaptation for remote sensing.\nCompared to previous surveys, this work addresses a broader range of domain\nadaptation tasks in remote sensing, rather than concentrating on a few\nsubfields. It also presents a systematic taxonomy, providing a more\ncomprehensive and organized understanding of the field. As a whole, this survey\ncan inspire the research community, foster understanding, and guide future work\nin the field.\n", "link": "http://arxiv.org/abs/2510.15615v1", "date": "2025-10-17", "relevancy": 2.5857, "topK": [{"title": "Self-supervised Photographic Image Layout Representation Learning", "link": "http://arxiv.org/abs/2403.03740v1", "similarity": 0.5413}, {"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.5145}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.4957}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Deep%20Learning%20Based%20Domain%20Adaptation%20Methods%20in%20Remote%20Sensing%3A%20A%0A%20%20Comprehensive%20Survey&body=Title%3A%20Deep%20Learning%20Based%20Domain%20Adaptation%20Methods%20in%20Remote%20Sensing%3A%20A%0A%20%20Comprehensive%20Survey%0AAuthor%3A%20Shuchang%20Lyu%20and%20Qi%20Zhao%20and%20Zheng%20Zhou%20and%20Meng%20Li%20and%20You%20Zhou%20and%20Dingding%20Yao%20and%20Guangliang%20Cheng%20and%20Huiyu%20Zhou%20and%20Zhenwei%20Shi%0AAbstract%3A%20%20%20Domain%20adaptation%20is%20a%20crucial%20and%20increasingly%20important%20task%20in%20remote%0Asensing%2C%20aiming%20to%20transfer%20knowledge%20from%20a%20source%20domain%20a%20differently%0Adistributed%20target%20domain.%20It%20has%20broad%20applications%20across%20various%20real-world%0Aapplications%2C%20including%20remote%20sensing%20element%20interpretation%2C%20ecological%0Aenvironment%20monitoring%2C%20and%20urban/rural%20planning.%20However%2C%20domain%20adaptation%20in%0Aremote%20sensing%20poses%20significant%20challenges%20due%20to%20differences%20in%20data%2C%20such%20as%0Avariations%20in%20ground%20sampling%20distance%2C%20imaging%20modes%20from%20various%20sensors%2C%0Ageographical%20landscapes%2C%20and%20environmental%20conditions.%20In%20recent%20years%2C%20deep%0Alearning%20has%20emerged%20as%20a%20powerful%20tool%20for%20feature%20representation%20and%0Across-domain%20knowledge%20transfer%2C%20leading%20to%20widespread%20adoption%20in%20remote%0Asensing%20tasks.%20In%20this%20paper%2C%20we%20present%20a%20comprehensive%20survey%20of%20significant%0Aadvancements%20in%20deep%20learning%20based%20domain%20adaptation%20for%20remote%20sensing.%20We%0Afirst%20introduce%20the%20preliminary%20knowledge%20to%20clarify%20key%20concepts%2C%20mathematical%0Anotations%2C%20and%20the%20taxonomy%20of%20methodologies.%20We%20then%20organize%20existing%0Aalgorithms%20from%20multiple%20perspectives%2C%20including%20task%20categorization%2C%20input%0Amode%2C%20supervision%20paradigm%2C%20and%20algorithmic%20granularity%2C%20providing%20readers%20with%0Aa%20structured%20understanding%20of%20the%20field.%20Next%2C%20we%20review%20widely%20used%20datasets%0Aand%20summarize%20the%20performance%20of%20state-of-the-art%20methods%20to%20provide%20an%0Aoverview%20of%20current%20progress.%20We%20also%20identify%20open%20challenges%20and%20potential%0Adirections%20to%20guide%20future%20research%20in%20domain%20adaptation%20for%20remote%20sensing.%0ACompared%20to%20previous%20surveys%2C%20this%20work%20addresses%20a%20broader%20range%20of%20domain%0Aadaptation%20tasks%20in%20remote%20sensing%2C%20rather%20than%20concentrating%20on%20a%20few%0Asubfields.%20It%20also%20presents%20a%20systematic%20taxonomy%2C%20providing%20a%20more%0Acomprehensive%20and%20organized%20understanding%20of%20the%20field.%20As%20a%20whole%2C%20this%20survey%0Acan%20inspire%20the%20research%20community%2C%20foster%20understanding%2C%20and%20guide%20future%20work%0Ain%20the%20field.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2510.15615v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DDeep%2520Learning%2520Based%2520Domain%2520Adaptation%2520Methods%2520in%2520Remote%2520Sensing%253A%2520A%250A%2520%2520Comprehensive%2520Survey%26entry.906535625%3DShuchang%2520Lyu%2520and%2520Qi%2520Zhao%2520and%2520Zheng%2520Zhou%2520and%2520Meng%2520Li%2520and%2520You%2520Zhou%2520and%2520Dingding%2520Yao%2520and%2520Guangliang%2520Cheng%2520and%2520Huiyu%2520Zhou%2520and%2520Zhenwei%2520Shi%26entry.1292438233%3D%2520%2520Domain%2520adaptation%2520is%2520a%2520crucial%2520and%2520increasingly%2520important%2520task%2520in%2520remote%250Asensing%252C%2520aiming%2520to%2520transfer%2520knowledge%2520from%2520a%2520source%2520domain%2520a%2520differently%250Adistributed%2520target%2520domain.%2520It%2520has%2520broad%2520applications%2520across%2520various%2520real-world%250Aapplications%252C%2520including%2520remote%2520sensing%2520element%2520interpretation%252C%2520ecological%250Aenvironment%2520monitoring%252C%2520and%2520urban/rural%2520planning.%2520However%252C%2520domain%2520adaptation%2520in%250Aremote%2520sensing%2520poses%2520significant%2520challenges%2520due%2520to%2520differences%2520in%2520data%252C%2520such%2520as%250Avariations%2520in%2520ground%2520sampling%2520distance%252C%2520imaging%2520modes%2520from%2520various%2520sensors%252C%250Ageographical%2520landscapes%252C%2520and%2520environmental%2520conditions.%2520In%2520recent%2520years%252C%2520deep%250Alearning%2520has%2520emerged%2520as%2520a%2520powerful%2520tool%2520for%2520feature%2520representation%2520and%250Across-domain%2520knowledge%2520transfer%252C%2520leading%2520to%2520widespread%2520adoption%2520in%2520remote%250Asensing%2520tasks.%2520In%2520this%2520paper%252C%2520we%2520present%2520a%2520comprehensive%2520survey%2520of%2520significant%250Aadvancements%2520in%2520deep%2520learning%2520based%2520domain%2520adaptation%2520for%2520remote%2520sensing.%2520We%250Afirst%2520introduce%2520the%2520preliminary%2520knowledge%2520to%2520clarify%2520key%2520concepts%252C%2520mathematical%250Anotations%252C%2520and%2520the%2520taxonomy%2520of%2520methodologies.%2520We%2520then%2520organize%2520existing%250Aalgorithms%2520from%2520multiple%2520perspectives%252C%2520including%2520task%2520categorization%252C%2520input%250Amode%252C%2520supervision%2520paradigm%252C%2520and%2520algorithmic%2520granularity%252C%2520providing%2520readers%2520with%250Aa%2520structured%2520understanding%2520of%2520the%2520field.%2520Next%252C%2520we%2520review%2520widely%2520used%2520datasets%250Aand%2520summarize%2520the%2520performance%2520of%2520state-of-the-art%2520methods%2520to%2520provide%2520an%250Aoverview%2520of%2520current%2520progress.%2520We%2520also%2520identify%2520open%2520challenges%2520and%2520potential%250Adirections%2520to%2520guide%2520future%2520research%2520in%2520domain%2520adaptation%2520for%2520remote%2520sensing.%250ACompared%2520to%2520previous%2520surveys%252C%2520this%2520work%2520addresses%2520a%2520broader%2520range%2520of%2520domain%250Aadaptation%2520tasks%2520in%2520remote%2520sensing%252C%2520rather%2520than%2520concentrating%2520on%2520a%2520few%250Asubfields.%2520It%2520also%2520presents%2520a%2520systematic%2520taxonomy%252C%2520providing%2520a%2520more%250Acomprehensive%2520and%2520organized%2520understanding%2520of%2520the%2520field.%2520As%2520a%2520whole%252C%2520this%2520survey%250Acan%2520inspire%2520the%2520research%2520community%252C%2520foster%2520understanding%252C%2520and%2520guide%2520future%2520work%250Ain%2520the%2520field.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2510.15615v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Deep%20Learning%20Based%20Domain%20Adaptation%20Methods%20in%20Remote%20Sensing%3A%20A%0A%20%20Comprehensive%20Survey&entry.906535625=Shuchang%20Lyu%20and%20Qi%20Zhao%20and%20Zheng%20Zhou%20and%20Meng%20Li%20and%20You%20Zhou%20and%20Dingding%20Yao%20and%20Guangliang%20Cheng%20and%20Huiyu%20Zhou%20and%20Zhenwei%20Shi&entry.1292438233=%20%20Domain%20adaptation%20is%20a%20crucial%20and%20increasingly%20important%20task%20in%20remote%0Asensing%2C%20aiming%20to%20transfer%20knowledge%20from%20a%20source%20domain%20a%20differently%0Adistributed%20target%20domain.%20It%20has%20broad%20applications%20across%20various%20real-world%0Aapplications%2C%20including%20remote%20sensing%20element%20interpretation%2C%20ecological%0Aenvironment%20monitoring%2C%20and%20urban/rural%20planning.%20However%2C%20domain%20adaptation%20in%0Aremote%20sensing%20poses%20significant%20challenges%20due%20to%20differences%20in%20data%2C%20such%20as%0Avariations%20in%20ground%20sampling%20distance%2C%20imaging%20modes%20from%20various%20sensors%2C%0Ageographical%20landscapes%2C%20and%20environmental%20conditions.%20In%20recent%20years%2C%20deep%0Alearning%20has%20emerged%20as%20a%20powerful%20tool%20for%20feature%20representation%20and%0Across-domain%20knowledge%20transfer%2C%20leading%20to%20widespread%20adoption%20in%20remote%0Asensing%20tasks.%20In%20this%20paper%2C%20we%20present%20a%20comprehensive%20survey%20of%20significant%0Aadvancements%20in%20deep%20learning%20based%20domain%20adaptation%20for%20remote%20sensing.%20We%0Afirst%20introduce%20the%20preliminary%20knowledge%20to%20clarify%20key%20concepts%2C%20mathematical%0Anotations%2C%20and%20the%20taxonomy%20of%20methodologies.%20We%20then%20organize%20existing%0Aalgorithms%20from%20multiple%20perspectives%2C%20including%20task%20categorization%2C%20input%0Amode%2C%20supervision%20paradigm%2C%20and%20algorithmic%20granularity%2C%20providing%20readers%20with%0Aa%20structured%20understanding%20of%20the%20field.%20Next%2C%20we%20review%20widely%20used%20datasets%0Aand%20summarize%20the%20performance%20of%20state-of-the-art%20methods%20to%20provide%20an%0Aoverview%20of%20current%20progress.%20We%20also%20identify%20open%20challenges%20and%20potential%0Adirections%20to%20guide%20future%20research%20in%20domain%20adaptation%20for%20remote%20sensing.%0ACompared%20to%20previous%20surveys%2C%20this%20work%20addresses%20a%20broader%20range%20of%20domain%0Aadaptation%20tasks%20in%20remote%20sensing%2C%20rather%20than%20concentrating%20on%20a%20few%0Asubfields.%20It%20also%20presents%20a%20systematic%20taxonomy%2C%20providing%20a%20more%0Acomprehensive%20and%20organized%20understanding%20of%20the%20field.%20As%20a%20whole%2C%20this%20survey%0Acan%20inspire%20the%20research%20community%2C%20foster%20understanding%2C%20and%20guide%20future%20work%0Ain%20the%20field.%0A&entry.1838667208=http%3A//arxiv.org/abs/2510.15615v1&entry.124074799=Read"},
{"title": "Retro3D: A 3D-aware Template-free Method for Enhancing Retrosynthesis\n  via Molecular Conformer Information", "author": "Jiaxi Zhuang and Yu Zhang and Yan Zhang and Ying Qian and Aimin Zhou", "abstract": "  Retrosynthesis plays a crucial role in the fields of organic synthesis and\ndrug development, where the goal is to identify suitable reactants that can\nyield a target product molecule. Although existing methods have achieved\nnotable success, they typically overlook the 3D conformational details and\ninternal spatial organization of molecules. This oversight makes it challenging\nto predict reactants that conform to genuine chemical principles, particularly\nwhen dealing with complex molecular structures, such as polycyclic and\nheteroaromatic compounds. In response to this challenge, we introduce a novel\ntransformer-based, template-free approach that incorporates 3D conformer data\nand spatial information. Our approach includes an Atom-align Fusion module that\nintegrates 3D positional data at the input stage, ensuring correct alignment\nbetween atom tokens and their respective 3D coordinates. Additionally, we\npropose a Distance-weighted Attention mechanism that refines the self-attention\nprocess, constricting the model s focus to relevant atom pairs in 3D space.\nExtensive experiments on the USPTO-50K dataset demonstrate that our model\noutperforms previous template-free methods, setting a new benchmark for the\nfield. A case study further highlights our method s ability to predict\nreasonable and accurate reactants.\n", "link": "http://arxiv.org/abs/2501.12434v2", "date": "2025-10-17", "relevancy": 2.5395, "topK": [{"title": "Synthesizing Moving People with 3D Control", "link": "http://arxiv.org/abs/2401.10889v2", "similarity": 0.5258}, {"title": "CAT3D: Create Anything in 3D with Multi-View Diffusion Models", "link": "http://arxiv.org/abs/2405.10314v1", "similarity": 0.499}, {"title": "CAT3D: Create Anything in 3D with Multi-View Diffusion Models", "link": "http://arxiv.org/abs/2405.10314v1", "similarity": 0.499}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Retro3D%3A%20A%203D-aware%20Template-free%20Method%20for%20Enhancing%20Retrosynthesis%0A%20%20via%20Molecular%20Conformer%20Information&body=Title%3A%20Retro3D%3A%20A%203D-aware%20Template-free%20Method%20for%20Enhancing%20Retrosynthesis%0A%20%20via%20Molecular%20Conformer%20Information%0AAuthor%3A%20Jiaxi%20Zhuang%20and%20Yu%20Zhang%20and%20Yan%20Zhang%20and%20Ying%20Qian%20and%20Aimin%20Zhou%0AAbstract%3A%20%20%20Retrosynthesis%20plays%20a%20crucial%20role%20in%20the%20fields%20of%20organic%20synthesis%20and%0Adrug%20development%2C%20where%20the%20goal%20is%20to%20identify%20suitable%20reactants%20that%20can%0Ayield%20a%20target%20product%20molecule.%20Although%20existing%20methods%20have%20achieved%0Anotable%20success%2C%20they%20typically%20overlook%20the%203D%20conformational%20details%20and%0Ainternal%20spatial%20organization%20of%20molecules.%20This%20oversight%20makes%20it%20challenging%0Ato%20predict%20reactants%20that%20conform%20to%20genuine%20chemical%20principles%2C%20particularly%0Awhen%20dealing%20with%20complex%20molecular%20structures%2C%20such%20as%20polycyclic%20and%0Aheteroaromatic%20compounds.%20In%20response%20to%20this%20challenge%2C%20we%20introduce%20a%20novel%0Atransformer-based%2C%20template-free%20approach%20that%20incorporates%203D%20conformer%20data%0Aand%20spatial%20information.%20Our%20approach%20includes%20an%20Atom-align%20Fusion%20module%20that%0Aintegrates%203D%20positional%20data%20at%20the%20input%20stage%2C%20ensuring%20correct%20alignment%0Abetween%20atom%20tokens%20and%20their%20respective%203D%20coordinates.%20Additionally%2C%20we%0Apropose%20a%20Distance-weighted%20Attention%20mechanism%20that%20refines%20the%20self-attention%0Aprocess%2C%20constricting%20the%20model%20s%20focus%20to%20relevant%20atom%20pairs%20in%203D%20space.%0AExtensive%20experiments%20on%20the%20USPTO-50K%20dataset%20demonstrate%20that%20our%20model%0Aoutperforms%20previous%20template-free%20methods%2C%20setting%20a%20new%20benchmark%20for%20the%0Afield.%20A%20case%20study%20further%20highlights%20our%20method%20s%20ability%20to%20predict%0Areasonable%20and%20accurate%20reactants.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2501.12434v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DRetro3D%253A%2520A%25203D-aware%2520Template-free%2520Method%2520for%2520Enhancing%2520Retrosynthesis%250A%2520%2520via%2520Molecular%2520Conformer%2520Information%26entry.906535625%3DJiaxi%2520Zhuang%2520and%2520Yu%2520Zhang%2520and%2520Yan%2520Zhang%2520and%2520Ying%2520Qian%2520and%2520Aimin%2520Zhou%26entry.1292438233%3D%2520%2520Retrosynthesis%2520plays%2520a%2520crucial%2520role%2520in%2520the%2520fields%2520of%2520organic%2520synthesis%2520and%250Adrug%2520development%252C%2520where%2520the%2520goal%2520is%2520to%2520identify%2520suitable%2520reactants%2520that%2520can%250Ayield%2520a%2520target%2520product%2520molecule.%2520Although%2520existing%2520methods%2520have%2520achieved%250Anotable%2520success%252C%2520they%2520typically%2520overlook%2520the%25203D%2520conformational%2520details%2520and%250Ainternal%2520spatial%2520organization%2520of%2520molecules.%2520This%2520oversight%2520makes%2520it%2520challenging%250Ato%2520predict%2520reactants%2520that%2520conform%2520to%2520genuine%2520chemical%2520principles%252C%2520particularly%250Awhen%2520dealing%2520with%2520complex%2520molecular%2520structures%252C%2520such%2520as%2520polycyclic%2520and%250Aheteroaromatic%2520compounds.%2520In%2520response%2520to%2520this%2520challenge%252C%2520we%2520introduce%2520a%2520novel%250Atransformer-based%252C%2520template-free%2520approach%2520that%2520incorporates%25203D%2520conformer%2520data%250Aand%2520spatial%2520information.%2520Our%2520approach%2520includes%2520an%2520Atom-align%2520Fusion%2520module%2520that%250Aintegrates%25203D%2520positional%2520data%2520at%2520the%2520input%2520stage%252C%2520ensuring%2520correct%2520alignment%250Abetween%2520atom%2520tokens%2520and%2520their%2520respective%25203D%2520coordinates.%2520Additionally%252C%2520we%250Apropose%2520a%2520Distance-weighted%2520Attention%2520mechanism%2520that%2520refines%2520the%2520self-attention%250Aprocess%252C%2520constricting%2520the%2520model%2520s%2520focus%2520to%2520relevant%2520atom%2520pairs%2520in%25203D%2520space.%250AExtensive%2520experiments%2520on%2520the%2520USPTO-50K%2520dataset%2520demonstrate%2520that%2520our%2520model%250Aoutperforms%2520previous%2520template-free%2520methods%252C%2520setting%2520a%2520new%2520benchmark%2520for%2520the%250Afield.%2520A%2520case%2520study%2520further%2520highlights%2520our%2520method%2520s%2520ability%2520to%2520predict%250Areasonable%2520and%2520accurate%2520reactants.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2501.12434v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Retro3D%3A%20A%203D-aware%20Template-free%20Method%20for%20Enhancing%20Retrosynthesis%0A%20%20via%20Molecular%20Conformer%20Information&entry.906535625=Jiaxi%20Zhuang%20and%20Yu%20Zhang%20and%20Yan%20Zhang%20and%20Ying%20Qian%20and%20Aimin%20Zhou&entry.1292438233=%20%20Retrosynthesis%20plays%20a%20crucial%20role%20in%20the%20fields%20of%20organic%20synthesis%20and%0Adrug%20development%2C%20where%20the%20goal%20is%20to%20identify%20suitable%20reactants%20that%20can%0Ayield%20a%20target%20product%20molecule.%20Although%20existing%20methods%20have%20achieved%0Anotable%20success%2C%20they%20typically%20overlook%20the%203D%20conformational%20details%20and%0Ainternal%20spatial%20organization%20of%20molecules.%20This%20oversight%20makes%20it%20challenging%0Ato%20predict%20reactants%20that%20conform%20to%20genuine%20chemical%20principles%2C%20particularly%0Awhen%20dealing%20with%20complex%20molecular%20structures%2C%20such%20as%20polycyclic%20and%0Aheteroaromatic%20compounds.%20In%20response%20to%20this%20challenge%2C%20we%20introduce%20a%20novel%0Atransformer-based%2C%20template-free%20approach%20that%20incorporates%203D%20conformer%20data%0Aand%20spatial%20information.%20Our%20approach%20includes%20an%20Atom-align%20Fusion%20module%20that%0Aintegrates%203D%20positional%20data%20at%20the%20input%20stage%2C%20ensuring%20correct%20alignment%0Abetween%20atom%20tokens%20and%20their%20respective%203D%20coordinates.%20Additionally%2C%20we%0Apropose%20a%20Distance-weighted%20Attention%20mechanism%20that%20refines%20the%20self-attention%0Aprocess%2C%20constricting%20the%20model%20s%20focus%20to%20relevant%20atom%20pairs%20in%203D%20space.%0AExtensive%20experiments%20on%20the%20USPTO-50K%20dataset%20demonstrate%20that%20our%20model%0Aoutperforms%20previous%20template-free%20methods%2C%20setting%20a%20new%20benchmark%20for%20the%0Afield.%20A%20case%20study%20further%20highlights%20our%20method%20s%20ability%20to%20predict%0Areasonable%20and%20accurate%20reactants.%0A&entry.1838667208=http%3A//arxiv.org/abs/2501.12434v2&entry.124074799=Read"},
{"title": "Quantized FCA: Efficient Zero-Shot Texture Anomaly Detection", "author": "Andrei-Timotei Ardelean and Patrick R\u00fcckbeil and Tim Weyrich", "abstract": "  Zero-shot anomaly localization is a rising field in computer vision research,\nwith important progress in recent years. This work focuses on the problem of\ndetecting and localizing anomalies in textures, where anomalies can be defined\nas the regions that deviate from the overall statistics, violating the\nstationarity assumption. The main limitation of existing methods is their high\nrunning time, making them impractical for deployment in real-world scenarios,\nsuch as assembly line monitoring. We propose a real-time method, named QFCA,\nwhich implements a quantized version of the feature correspondence analysis\n(FCA) algorithm. By carefully adapting the patch statistics comparison to work\non histograms of quantized values, we obtain a 10x speedup with little to no\nloss in accuracy. Moreover, we introduce a feature preprocessing step based on\nprincipal component analysis, which enhances the contrast between normal and\nanomalous features, improving the detection precision on complex textures. Our\nmethod is thoroughly evaluated against prior art, comparing favorably with\nexisting methods. Project page:\nhttps://reality.tf.fau.de/pub/ardelean2025quantized.html\n", "link": "http://arxiv.org/abs/2510.15602v1", "date": "2025-10-17", "relevancy": 2.5238, "topK": [{"title": "PoseAnimate: Zero-shot high fidelity pose controllable character\n  animation", "link": "http://arxiv.org/abs/2404.13680v2", "similarity": 0.5198}, {"title": "Tuning-Free Noise Rectification for High Fidelity Image-to-Video\n  Generation", "link": "http://arxiv.org/abs/2403.02827v1", "similarity": 0.5024}, {"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.4921}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Quantized%20FCA%3A%20Efficient%20Zero-Shot%20Texture%20Anomaly%20Detection&body=Title%3A%20Quantized%20FCA%3A%20Efficient%20Zero-Shot%20Texture%20Anomaly%20Detection%0AAuthor%3A%20Andrei-Timotei%20Ardelean%20and%20Patrick%20R%C3%BCckbeil%20and%20Tim%20Weyrich%0AAbstract%3A%20%20%20Zero-shot%20anomaly%20localization%20is%20a%20rising%20field%20in%20computer%20vision%20research%2C%0Awith%20important%20progress%20in%20recent%20years.%20This%20work%20focuses%20on%20the%20problem%20of%0Adetecting%20and%20localizing%20anomalies%20in%20textures%2C%20where%20anomalies%20can%20be%20defined%0Aas%20the%20regions%20that%20deviate%20from%20the%20overall%20statistics%2C%20violating%20the%0Astationarity%20assumption.%20The%20main%20limitation%20of%20existing%20methods%20is%20their%20high%0Arunning%20time%2C%20making%20them%20impractical%20for%20deployment%20in%20real-world%20scenarios%2C%0Asuch%20as%20assembly%20line%20monitoring.%20We%20propose%20a%20real-time%20method%2C%20named%20QFCA%2C%0Awhich%20implements%20a%20quantized%20version%20of%20the%20feature%20correspondence%20analysis%0A%28FCA%29%20algorithm.%20By%20carefully%20adapting%20the%20patch%20statistics%20comparison%20to%20work%0Aon%20histograms%20of%20quantized%20values%2C%20we%20obtain%20a%2010x%20speedup%20with%20little%20to%20no%0Aloss%20in%20accuracy.%20Moreover%2C%20we%20introduce%20a%20feature%20preprocessing%20step%20based%20on%0Aprincipal%20component%20analysis%2C%20which%20enhances%20the%20contrast%20between%20normal%20and%0Aanomalous%20features%2C%20improving%20the%20detection%20precision%20on%20complex%20textures.%20Our%0Amethod%20is%20thoroughly%20evaluated%20against%20prior%20art%2C%20comparing%20favorably%20with%0Aexisting%20methods.%20Project%20page%3A%0Ahttps%3A//reality.tf.fau.de/pub/ardelean2025quantized.html%0A%0ALink%3A%20http%3A//arxiv.org/abs/2510.15602v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DQuantized%2520FCA%253A%2520Efficient%2520Zero-Shot%2520Texture%2520Anomaly%2520Detection%26entry.906535625%3DAndrei-Timotei%2520Ardelean%2520and%2520Patrick%2520R%25C3%25BCckbeil%2520and%2520Tim%2520Weyrich%26entry.1292438233%3D%2520%2520Zero-shot%2520anomaly%2520localization%2520is%2520a%2520rising%2520field%2520in%2520computer%2520vision%2520research%252C%250Awith%2520important%2520progress%2520in%2520recent%2520years.%2520This%2520work%2520focuses%2520on%2520the%2520problem%2520of%250Adetecting%2520and%2520localizing%2520anomalies%2520in%2520textures%252C%2520where%2520anomalies%2520can%2520be%2520defined%250Aas%2520the%2520regions%2520that%2520deviate%2520from%2520the%2520overall%2520statistics%252C%2520violating%2520the%250Astationarity%2520assumption.%2520The%2520main%2520limitation%2520of%2520existing%2520methods%2520is%2520their%2520high%250Arunning%2520time%252C%2520making%2520them%2520impractical%2520for%2520deployment%2520in%2520real-world%2520scenarios%252C%250Asuch%2520as%2520assembly%2520line%2520monitoring.%2520We%2520propose%2520a%2520real-time%2520method%252C%2520named%2520QFCA%252C%250Awhich%2520implements%2520a%2520quantized%2520version%2520of%2520the%2520feature%2520correspondence%2520analysis%250A%2528FCA%2529%2520algorithm.%2520By%2520carefully%2520adapting%2520the%2520patch%2520statistics%2520comparison%2520to%2520work%250Aon%2520histograms%2520of%2520quantized%2520values%252C%2520we%2520obtain%2520a%252010x%2520speedup%2520with%2520little%2520to%2520no%250Aloss%2520in%2520accuracy.%2520Moreover%252C%2520we%2520introduce%2520a%2520feature%2520preprocessing%2520step%2520based%2520on%250Aprincipal%2520component%2520analysis%252C%2520which%2520enhances%2520the%2520contrast%2520between%2520normal%2520and%250Aanomalous%2520features%252C%2520improving%2520the%2520detection%2520precision%2520on%2520complex%2520textures.%2520Our%250Amethod%2520is%2520thoroughly%2520evaluated%2520against%2520prior%2520art%252C%2520comparing%2520favorably%2520with%250Aexisting%2520methods.%2520Project%2520page%253A%250Ahttps%253A//reality.tf.fau.de/pub/ardelean2025quantized.html%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2510.15602v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Quantized%20FCA%3A%20Efficient%20Zero-Shot%20Texture%20Anomaly%20Detection&entry.906535625=Andrei-Timotei%20Ardelean%20and%20Patrick%20R%C3%BCckbeil%20and%20Tim%20Weyrich&entry.1292438233=%20%20Zero-shot%20anomaly%20localization%20is%20a%20rising%20field%20in%20computer%20vision%20research%2C%0Awith%20important%20progress%20in%20recent%20years.%20This%20work%20focuses%20on%20the%20problem%20of%0Adetecting%20and%20localizing%20anomalies%20in%20textures%2C%20where%20anomalies%20can%20be%20defined%0Aas%20the%20regions%20that%20deviate%20from%20the%20overall%20statistics%2C%20violating%20the%0Astationarity%20assumption.%20The%20main%20limitation%20of%20existing%20methods%20is%20their%20high%0Arunning%20time%2C%20making%20them%20impractical%20for%20deployment%20in%20real-world%20scenarios%2C%0Asuch%20as%20assembly%20line%20monitoring.%20We%20propose%20a%20real-time%20method%2C%20named%20QFCA%2C%0Awhich%20implements%20a%20quantized%20version%20of%20the%20feature%20correspondence%20analysis%0A%28FCA%29%20algorithm.%20By%20carefully%20adapting%20the%20patch%20statistics%20comparison%20to%20work%0Aon%20histograms%20of%20quantized%20values%2C%20we%20obtain%20a%2010x%20speedup%20with%20little%20to%20no%0Aloss%20in%20accuracy.%20Moreover%2C%20we%20introduce%20a%20feature%20preprocessing%20step%20based%20on%0Aprincipal%20component%20analysis%2C%20which%20enhances%20the%20contrast%20between%20normal%20and%0Aanomalous%20features%2C%20improving%20the%20detection%20precision%20on%20complex%20textures.%20Our%0Amethod%20is%20thoroughly%20evaluated%20against%20prior%20art%2C%20comparing%20favorably%20with%0Aexisting%20methods.%20Project%20page%3A%0Ahttps%3A//reality.tf.fau.de/pub/ardelean2025quantized.html%0A&entry.1838667208=http%3A//arxiv.org/abs/2510.15602v1&entry.124074799=Read"},
{"title": "SpeechLLMs for Large-scale Contextualized Zero-shot Slot Filling", "author": "Kadri Hacioglu and Manjunath K E and Andreas Stolcke", "abstract": "  Slot filling is a crucial subtask in spoken language understanding (SLU),\ntraditionally implemented as a cascade of speech recognition followed by one or\nmore natural language understanding (NLU) components. The recent advent of\nspeech-based large language models (speechLLMs), which integrate speech and\ntextual foundation models, has opened new avenues for achieving speech\nunderstanding tasks in a more unified, generative, and instruction-following\nmanner while promising data and compute efficiency with zero-shot abilities,\ngeneralizing to unseen slot labels. We address the slot-filling task by\ncreating an empirical upper bound for the task, identifying performance,\nrobustness, and generalization gaps, and proposing improvements to the training\ndata, architecture, and training strategies to narrow the gap with the upper\nbound result. We show that each of these measures improve performance\nsubstantially, while highlighting practical challenges and providing empirical\nguidance and insights for harnessing these emerging models.\n", "link": "http://arxiv.org/abs/2510.15851v1", "date": "2025-10-17", "relevancy": 2.4877, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5089}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5089}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.4748}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20SpeechLLMs%20for%20Large-scale%20Contextualized%20Zero-shot%20Slot%20Filling&body=Title%3A%20SpeechLLMs%20for%20Large-scale%20Contextualized%20Zero-shot%20Slot%20Filling%0AAuthor%3A%20Kadri%20Hacioglu%20and%20Manjunath%20K%20E%20and%20Andreas%20Stolcke%0AAbstract%3A%20%20%20Slot%20filling%20is%20a%20crucial%20subtask%20in%20spoken%20language%20understanding%20%28SLU%29%2C%0Atraditionally%20implemented%20as%20a%20cascade%20of%20speech%20recognition%20followed%20by%20one%20or%0Amore%20natural%20language%20understanding%20%28NLU%29%20components.%20The%20recent%20advent%20of%0Aspeech-based%20large%20language%20models%20%28speechLLMs%29%2C%20which%20integrate%20speech%20and%0Atextual%20foundation%20models%2C%20has%20opened%20new%20avenues%20for%20achieving%20speech%0Aunderstanding%20tasks%20in%20a%20more%20unified%2C%20generative%2C%20and%20instruction-following%0Amanner%20while%20promising%20data%20and%20compute%20efficiency%20with%20zero-shot%20abilities%2C%0Ageneralizing%20to%20unseen%20slot%20labels.%20We%20address%20the%20slot-filling%20task%20by%0Acreating%20an%20empirical%20upper%20bound%20for%20the%20task%2C%20identifying%20performance%2C%0Arobustness%2C%20and%20generalization%20gaps%2C%20and%20proposing%20improvements%20to%20the%20training%0Adata%2C%20architecture%2C%20and%20training%20strategies%20to%20narrow%20the%20gap%20with%20the%20upper%0Abound%20result.%20We%20show%20that%20each%20of%20these%20measures%20improve%20performance%0Asubstantially%2C%20while%20highlighting%20practical%20challenges%20and%20providing%20empirical%0Aguidance%20and%20insights%20for%20harnessing%20these%20emerging%20models.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2510.15851v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DSpeechLLMs%2520for%2520Large-scale%2520Contextualized%2520Zero-shot%2520Slot%2520Filling%26entry.906535625%3DKadri%2520Hacioglu%2520and%2520Manjunath%2520K%2520E%2520and%2520Andreas%2520Stolcke%26entry.1292438233%3D%2520%2520Slot%2520filling%2520is%2520a%2520crucial%2520subtask%2520in%2520spoken%2520language%2520understanding%2520%2528SLU%2529%252C%250Atraditionally%2520implemented%2520as%2520a%2520cascade%2520of%2520speech%2520recognition%2520followed%2520by%2520one%2520or%250Amore%2520natural%2520language%2520understanding%2520%2528NLU%2529%2520components.%2520The%2520recent%2520advent%2520of%250Aspeech-based%2520large%2520language%2520models%2520%2528speechLLMs%2529%252C%2520which%2520integrate%2520speech%2520and%250Atextual%2520foundation%2520models%252C%2520has%2520opened%2520new%2520avenues%2520for%2520achieving%2520speech%250Aunderstanding%2520tasks%2520in%2520a%2520more%2520unified%252C%2520generative%252C%2520and%2520instruction-following%250Amanner%2520while%2520promising%2520data%2520and%2520compute%2520efficiency%2520with%2520zero-shot%2520abilities%252C%250Ageneralizing%2520to%2520unseen%2520slot%2520labels.%2520We%2520address%2520the%2520slot-filling%2520task%2520by%250Acreating%2520an%2520empirical%2520upper%2520bound%2520for%2520the%2520task%252C%2520identifying%2520performance%252C%250Arobustness%252C%2520and%2520generalization%2520gaps%252C%2520and%2520proposing%2520improvements%2520to%2520the%2520training%250Adata%252C%2520architecture%252C%2520and%2520training%2520strategies%2520to%2520narrow%2520the%2520gap%2520with%2520the%2520upper%250Abound%2520result.%2520We%2520show%2520that%2520each%2520of%2520these%2520measures%2520improve%2520performance%250Asubstantially%252C%2520while%2520highlighting%2520practical%2520challenges%2520and%2520providing%2520empirical%250Aguidance%2520and%2520insights%2520for%2520harnessing%2520these%2520emerging%2520models.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2510.15851v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=SpeechLLMs%20for%20Large-scale%20Contextualized%20Zero-shot%20Slot%20Filling&entry.906535625=Kadri%20Hacioglu%20and%20Manjunath%20K%20E%20and%20Andreas%20Stolcke&entry.1292438233=%20%20Slot%20filling%20is%20a%20crucial%20subtask%20in%20spoken%20language%20understanding%20%28SLU%29%2C%0Atraditionally%20implemented%20as%20a%20cascade%20of%20speech%20recognition%20followed%20by%20one%20or%0Amore%20natural%20language%20understanding%20%28NLU%29%20components.%20The%20recent%20advent%20of%0Aspeech-based%20large%20language%20models%20%28speechLLMs%29%2C%20which%20integrate%20speech%20and%0Atextual%20foundation%20models%2C%20has%20opened%20new%20avenues%20for%20achieving%20speech%0Aunderstanding%20tasks%20in%20a%20more%20unified%2C%20generative%2C%20and%20instruction-following%0Amanner%20while%20promising%20data%20and%20compute%20efficiency%20with%20zero-shot%20abilities%2C%0Ageneralizing%20to%20unseen%20slot%20labels.%20We%20address%20the%20slot-filling%20task%20by%0Acreating%20an%20empirical%20upper%20bound%20for%20the%20task%2C%20identifying%20performance%2C%0Arobustness%2C%20and%20generalization%20gaps%2C%20and%20proposing%20improvements%20to%20the%20training%0Adata%2C%20architecture%2C%20and%20training%20strategies%20to%20narrow%20the%20gap%20with%20the%20upper%0Abound%20result.%20We%20show%20that%20each%20of%20these%20measures%20improve%20performance%0Asubstantially%2C%20while%20highlighting%20practical%20challenges%20and%20providing%20empirical%0Aguidance%20and%20insights%20for%20harnessing%20these%20emerging%20models.%0A&entry.1838667208=http%3A//arxiv.org/abs/2510.15851v1&entry.124074799=Read"},
{"title": "Standardization for improved Spatio-Temporal Image Fusion", "author": "Harkaitz Goyena and Peter M. Atkinson and Unai P\u00e9rez-Goya and M. Dolores Ugarte", "abstract": "  Spatio-Temporal Image Fusion (STIF) methods usually require sets of images\nwith matching spatial and spectral resolutions captured by different sensors.\nTo facilitate the application of STIF methods, we propose and compare two\ndifferent standardization approaches. The first method is based on traditional\nupscaling of the fine-resolution images. The second method is a sharpening\napproach called Anomaly Based Satellite Image Standardization (ABSIS) that\nblends the overall features found in the fine-resolution image series with the\ndistinctive attributes of a specific coarse-resolution image to produce images\nthat more closely resemble the outcome of aggregating the fine-resolution\nimages. Both methods produce a significant increase in accuracy of the Unpaired\nSpatio Temporal Fusion of Image Patches (USTFIP) STIF method, with the\nsharpening approach increasing the spectral and spatial accuracies of the fused\nimages by up to 49.46\\% and 78.40\\%, respectively.\n", "link": "http://arxiv.org/abs/2510.15589v1", "date": "2025-10-17", "relevancy": 2.4676, "topK": [{"title": "Tuning-Free Noise Rectification for High Fidelity Image-to-Video\n  Generation", "link": "http://arxiv.org/abs/2403.02827v1", "similarity": 0.4977}, {"title": "GST: Precise 3D Human Body from a Single Image with Gaussian Splatting\n  Transformers", "link": "http://arxiv.org/abs/2409.04196v1", "similarity": 0.4915}, {"title": "ToDo: Token Downsampling for Efficient Generation of High-Resolution\n  Images", "link": "http://arxiv.org/abs/2402.13573v2", "similarity": 0.4915}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Standardization%20for%20improved%20Spatio-Temporal%20Image%20Fusion&body=Title%3A%20Standardization%20for%20improved%20Spatio-Temporal%20Image%20Fusion%0AAuthor%3A%20Harkaitz%20Goyena%20and%20Peter%20M.%20Atkinson%20and%20Unai%20P%C3%A9rez-Goya%20and%20M.%20Dolores%20Ugarte%0AAbstract%3A%20%20%20Spatio-Temporal%20Image%20Fusion%20%28STIF%29%20methods%20usually%20require%20sets%20of%20images%0Awith%20matching%20spatial%20and%20spectral%20resolutions%20captured%20by%20different%20sensors.%0ATo%20facilitate%20the%20application%20of%20STIF%20methods%2C%20we%20propose%20and%20compare%20two%0Adifferent%20standardization%20approaches.%20The%20first%20method%20is%20based%20on%20traditional%0Aupscaling%20of%20the%20fine-resolution%20images.%20The%20second%20method%20is%20a%20sharpening%0Aapproach%20called%20Anomaly%20Based%20Satellite%20Image%20Standardization%20%28ABSIS%29%20that%0Ablends%20the%20overall%20features%20found%20in%20the%20fine-resolution%20image%20series%20with%20the%0Adistinctive%20attributes%20of%20a%20specific%20coarse-resolution%20image%20to%20produce%20images%0Athat%20more%20closely%20resemble%20the%20outcome%20of%20aggregating%20the%20fine-resolution%0Aimages.%20Both%20methods%20produce%20a%20significant%20increase%20in%20accuracy%20of%20the%20Unpaired%0ASpatio%20Temporal%20Fusion%20of%20Image%20Patches%20%28USTFIP%29%20STIF%20method%2C%20with%20the%0Asharpening%20approach%20increasing%20the%20spectral%20and%20spatial%20accuracies%20of%20the%20fused%0Aimages%20by%20up%20to%2049.46%5C%25%20and%2078.40%5C%25%2C%20respectively.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2510.15589v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DStandardization%2520for%2520improved%2520Spatio-Temporal%2520Image%2520Fusion%26entry.906535625%3DHarkaitz%2520Goyena%2520and%2520Peter%2520M.%2520Atkinson%2520and%2520Unai%2520P%25C3%25A9rez-Goya%2520and%2520M.%2520Dolores%2520Ugarte%26entry.1292438233%3D%2520%2520Spatio-Temporal%2520Image%2520Fusion%2520%2528STIF%2529%2520methods%2520usually%2520require%2520sets%2520of%2520images%250Awith%2520matching%2520spatial%2520and%2520spectral%2520resolutions%2520captured%2520by%2520different%2520sensors.%250ATo%2520facilitate%2520the%2520application%2520of%2520STIF%2520methods%252C%2520we%2520propose%2520and%2520compare%2520two%250Adifferent%2520standardization%2520approaches.%2520The%2520first%2520method%2520is%2520based%2520on%2520traditional%250Aupscaling%2520of%2520the%2520fine-resolution%2520images.%2520The%2520second%2520method%2520is%2520a%2520sharpening%250Aapproach%2520called%2520Anomaly%2520Based%2520Satellite%2520Image%2520Standardization%2520%2528ABSIS%2529%2520that%250Ablends%2520the%2520overall%2520features%2520found%2520in%2520the%2520fine-resolution%2520image%2520series%2520with%2520the%250Adistinctive%2520attributes%2520of%2520a%2520specific%2520coarse-resolution%2520image%2520to%2520produce%2520images%250Athat%2520more%2520closely%2520resemble%2520the%2520outcome%2520of%2520aggregating%2520the%2520fine-resolution%250Aimages.%2520Both%2520methods%2520produce%2520a%2520significant%2520increase%2520in%2520accuracy%2520of%2520the%2520Unpaired%250ASpatio%2520Temporal%2520Fusion%2520of%2520Image%2520Patches%2520%2528USTFIP%2529%2520STIF%2520method%252C%2520with%2520the%250Asharpening%2520approach%2520increasing%2520the%2520spectral%2520and%2520spatial%2520accuracies%2520of%2520the%2520fused%250Aimages%2520by%2520up%2520to%252049.46%255C%2525%2520and%252078.40%255C%2525%252C%2520respectively.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2510.15589v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Standardization%20for%20improved%20Spatio-Temporal%20Image%20Fusion&entry.906535625=Harkaitz%20Goyena%20and%20Peter%20M.%20Atkinson%20and%20Unai%20P%C3%A9rez-Goya%20and%20M.%20Dolores%20Ugarte&entry.1292438233=%20%20Spatio-Temporal%20Image%20Fusion%20%28STIF%29%20methods%20usually%20require%20sets%20of%20images%0Awith%20matching%20spatial%20and%20spectral%20resolutions%20captured%20by%20different%20sensors.%0ATo%20facilitate%20the%20application%20of%20STIF%20methods%2C%20we%20propose%20and%20compare%20two%0Adifferent%20standardization%20approaches.%20The%20first%20method%20is%20based%20on%20traditional%0Aupscaling%20of%20the%20fine-resolution%20images.%20The%20second%20method%20is%20a%20sharpening%0Aapproach%20called%20Anomaly%20Based%20Satellite%20Image%20Standardization%20%28ABSIS%29%20that%0Ablends%20the%20overall%20features%20found%20in%20the%20fine-resolution%20image%20series%20with%20the%0Adistinctive%20attributes%20of%20a%20specific%20coarse-resolution%20image%20to%20produce%20images%0Athat%20more%20closely%20resemble%20the%20outcome%20of%20aggregating%20the%20fine-resolution%0Aimages.%20Both%20methods%20produce%20a%20significant%20increase%20in%20accuracy%20of%20the%20Unpaired%0ASpatio%20Temporal%20Fusion%20of%20Image%20Patches%20%28USTFIP%29%20STIF%20method%2C%20with%20the%0Asharpening%20approach%20increasing%20the%20spectral%20and%20spatial%20accuracies%20of%20the%20fused%0Aimages%20by%20up%20to%2049.46%5C%25%20and%2078.40%5C%25%2C%20respectively.%0A&entry.1838667208=http%3A//arxiv.org/abs/2510.15589v1&entry.124074799=Read"},
{"title": "From Language to Locomotion: Retargeting-free Humanoid Control via\n  Motion Latent Guidance", "author": "Zhe Li and Cheng Chi and Yangyang Wei and Boan Zhu and Yibo Peng and Tao Huang and Pengwei Wang and Zhongyuan Wang and Shanghang Zhang and Chang Xu", "abstract": "  Natural language offers a natural interface for humanoid robots, but existing\nlanguage-guided humanoid locomotion pipelines remain cumbersome and\nuntrustworthy. They typically decode human motion, retarget it to robot\nmorphology, and then track it with a physics-based controller. However, this\nmulti-stage process is prone to cumulative errors, introduces high latency, and\nyields weak coupling between semantics and control. These limitations call for\na more direct pathway from language to action, one that eliminates fragile\nintermediate stages. Therefore, we present RoboGhost, a retargeting-free\nframework that directly conditions humanoid policies on language-grounded\nmotion latents. By bypassing explicit motion decoding and retargeting,\nRoboGhost enables a diffusion-based policy to denoise executable actions\ndirectly from noise, preserving semantic intent and supporting fast, reactive\ncontrol. A hybrid causal transformer-diffusion motion generator further ensures\nlong-horizon consistency while maintaining stability and diversity, yielding\nrich latent representations for precise humanoid behavior. Extensive\nexperiments demonstrate that RoboGhost substantially reduces deployment\nlatency, improves success rates and tracking precision, and produces smooth,\nsemantically aligned locomotion on real humanoids. Beyond text, the framework\nnaturally extends to other modalities such as images, audio, and music,\nproviding a universal foundation for vision-language-action humanoid systems.\n", "link": "http://arxiv.org/abs/2510.14952v2", "date": "2025-10-17", "relevancy": 2.4455, "topK": [{"title": "Synthesizing Moving People with 3D Control", "link": "http://arxiv.org/abs/2401.10889v2", "similarity": 0.6366}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.6069}, {"title": "Egocentric Computer Vision for Hands-Free Robotic Wheelchair Navigation", "link": "https://link.springer.com/article/10.1007/s10846-023-01807-4", "similarity": 0.588}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20From%20Language%20to%20Locomotion%3A%20Retargeting-free%20Humanoid%20Control%20via%0A%20%20Motion%20Latent%20Guidance&body=Title%3A%20From%20Language%20to%20Locomotion%3A%20Retargeting-free%20Humanoid%20Control%20via%0A%20%20Motion%20Latent%20Guidance%0AAuthor%3A%20Zhe%20Li%20and%20Cheng%20Chi%20and%20Yangyang%20Wei%20and%20Boan%20Zhu%20and%20Yibo%20Peng%20and%20Tao%20Huang%20and%20Pengwei%20Wang%20and%20Zhongyuan%20Wang%20and%20Shanghang%20Zhang%20and%20Chang%20Xu%0AAbstract%3A%20%20%20Natural%20language%20offers%20a%20natural%20interface%20for%20humanoid%20robots%2C%20but%20existing%0Alanguage-guided%20humanoid%20locomotion%20pipelines%20remain%20cumbersome%20and%0Auntrustworthy.%20They%20typically%20decode%20human%20motion%2C%20retarget%20it%20to%20robot%0Amorphology%2C%20and%20then%20track%20it%20with%20a%20physics-based%20controller.%20However%2C%20this%0Amulti-stage%20process%20is%20prone%20to%20cumulative%20errors%2C%20introduces%20high%20latency%2C%20and%0Ayields%20weak%20coupling%20between%20semantics%20and%20control.%20These%20limitations%20call%20for%0Aa%20more%20direct%20pathway%20from%20language%20to%20action%2C%20one%20that%20eliminates%20fragile%0Aintermediate%20stages.%20Therefore%2C%20we%20present%20RoboGhost%2C%20a%20retargeting-free%0Aframework%20that%20directly%20conditions%20humanoid%20policies%20on%20language-grounded%0Amotion%20latents.%20By%20bypassing%20explicit%20motion%20decoding%20and%20retargeting%2C%0ARoboGhost%20enables%20a%20diffusion-based%20policy%20to%20denoise%20executable%20actions%0Adirectly%20from%20noise%2C%20preserving%20semantic%20intent%20and%20supporting%20fast%2C%20reactive%0Acontrol.%20A%20hybrid%20causal%20transformer-diffusion%20motion%20generator%20further%20ensures%0Along-horizon%20consistency%20while%20maintaining%20stability%20and%20diversity%2C%20yielding%0Arich%20latent%20representations%20for%20precise%20humanoid%20behavior.%20Extensive%0Aexperiments%20demonstrate%20that%20RoboGhost%20substantially%20reduces%20deployment%0Alatency%2C%20improves%20success%20rates%20and%20tracking%20precision%2C%20and%20produces%20smooth%2C%0Asemantically%20aligned%20locomotion%20on%20real%20humanoids.%20Beyond%20text%2C%20the%20framework%0Anaturally%20extends%20to%20other%20modalities%20such%20as%20images%2C%20audio%2C%20and%20music%2C%0Aproviding%20a%20universal%20foundation%20for%20vision-language-action%20humanoid%20systems.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2510.14952v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DFrom%2520Language%2520to%2520Locomotion%253A%2520Retargeting-free%2520Humanoid%2520Control%2520via%250A%2520%2520Motion%2520Latent%2520Guidance%26entry.906535625%3DZhe%2520Li%2520and%2520Cheng%2520Chi%2520and%2520Yangyang%2520Wei%2520and%2520Boan%2520Zhu%2520and%2520Yibo%2520Peng%2520and%2520Tao%2520Huang%2520and%2520Pengwei%2520Wang%2520and%2520Zhongyuan%2520Wang%2520and%2520Shanghang%2520Zhang%2520and%2520Chang%2520Xu%26entry.1292438233%3D%2520%2520Natural%2520language%2520offers%2520a%2520natural%2520interface%2520for%2520humanoid%2520robots%252C%2520but%2520existing%250Alanguage-guided%2520humanoid%2520locomotion%2520pipelines%2520remain%2520cumbersome%2520and%250Auntrustworthy.%2520They%2520typically%2520decode%2520human%2520motion%252C%2520retarget%2520it%2520to%2520robot%250Amorphology%252C%2520and%2520then%2520track%2520it%2520with%2520a%2520physics-based%2520controller.%2520However%252C%2520this%250Amulti-stage%2520process%2520is%2520prone%2520to%2520cumulative%2520errors%252C%2520introduces%2520high%2520latency%252C%2520and%250Ayields%2520weak%2520coupling%2520between%2520semantics%2520and%2520control.%2520These%2520limitations%2520call%2520for%250Aa%2520more%2520direct%2520pathway%2520from%2520language%2520to%2520action%252C%2520one%2520that%2520eliminates%2520fragile%250Aintermediate%2520stages.%2520Therefore%252C%2520we%2520present%2520RoboGhost%252C%2520a%2520retargeting-free%250Aframework%2520that%2520directly%2520conditions%2520humanoid%2520policies%2520on%2520language-grounded%250Amotion%2520latents.%2520By%2520bypassing%2520explicit%2520motion%2520decoding%2520and%2520retargeting%252C%250ARoboGhost%2520enables%2520a%2520diffusion-based%2520policy%2520to%2520denoise%2520executable%2520actions%250Adirectly%2520from%2520noise%252C%2520preserving%2520semantic%2520intent%2520and%2520supporting%2520fast%252C%2520reactive%250Acontrol.%2520A%2520hybrid%2520causal%2520transformer-diffusion%2520motion%2520generator%2520further%2520ensures%250Along-horizon%2520consistency%2520while%2520maintaining%2520stability%2520and%2520diversity%252C%2520yielding%250Arich%2520latent%2520representations%2520for%2520precise%2520humanoid%2520behavior.%2520Extensive%250Aexperiments%2520demonstrate%2520that%2520RoboGhost%2520substantially%2520reduces%2520deployment%250Alatency%252C%2520improves%2520success%2520rates%2520and%2520tracking%2520precision%252C%2520and%2520produces%2520smooth%252C%250Asemantically%2520aligned%2520locomotion%2520on%2520real%2520humanoids.%2520Beyond%2520text%252C%2520the%2520framework%250Anaturally%2520extends%2520to%2520other%2520modalities%2520such%2520as%2520images%252C%2520audio%252C%2520and%2520music%252C%250Aproviding%2520a%2520universal%2520foundation%2520for%2520vision-language-action%2520humanoid%2520systems.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2510.14952v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=From%20Language%20to%20Locomotion%3A%20Retargeting-free%20Humanoid%20Control%20via%0A%20%20Motion%20Latent%20Guidance&entry.906535625=Zhe%20Li%20and%20Cheng%20Chi%20and%20Yangyang%20Wei%20and%20Boan%20Zhu%20and%20Yibo%20Peng%20and%20Tao%20Huang%20and%20Pengwei%20Wang%20and%20Zhongyuan%20Wang%20and%20Shanghang%20Zhang%20and%20Chang%20Xu&entry.1292438233=%20%20Natural%20language%20offers%20a%20natural%20interface%20for%20humanoid%20robots%2C%20but%20existing%0Alanguage-guided%20humanoid%20locomotion%20pipelines%20remain%20cumbersome%20and%0Auntrustworthy.%20They%20typically%20decode%20human%20motion%2C%20retarget%20it%20to%20robot%0Amorphology%2C%20and%20then%20track%20it%20with%20a%20physics-based%20controller.%20However%2C%20this%0Amulti-stage%20process%20is%20prone%20to%20cumulative%20errors%2C%20introduces%20high%20latency%2C%20and%0Ayields%20weak%20coupling%20between%20semantics%20and%20control.%20These%20limitations%20call%20for%0Aa%20more%20direct%20pathway%20from%20language%20to%20action%2C%20one%20that%20eliminates%20fragile%0Aintermediate%20stages.%20Therefore%2C%20we%20present%20RoboGhost%2C%20a%20retargeting-free%0Aframework%20that%20directly%20conditions%20humanoid%20policies%20on%20language-grounded%0Amotion%20latents.%20By%20bypassing%20explicit%20motion%20decoding%20and%20retargeting%2C%0ARoboGhost%20enables%20a%20diffusion-based%20policy%20to%20denoise%20executable%20actions%0Adirectly%20from%20noise%2C%20preserving%20semantic%20intent%20and%20supporting%20fast%2C%20reactive%0Acontrol.%20A%20hybrid%20causal%20transformer-diffusion%20motion%20generator%20further%20ensures%0Along-horizon%20consistency%20while%20maintaining%20stability%20and%20diversity%2C%20yielding%0Arich%20latent%20representations%20for%20precise%20humanoid%20behavior.%20Extensive%0Aexperiments%20demonstrate%20that%20RoboGhost%20substantially%20reduces%20deployment%0Alatency%2C%20improves%20success%20rates%20and%20tracking%20precision%2C%20and%20produces%20smooth%2C%0Asemantically%20aligned%20locomotion%20on%20real%20humanoids.%20Beyond%20text%2C%20the%20framework%0Anaturally%20extends%20to%20other%20modalities%20such%20as%20images%2C%20audio%2C%20and%20music%2C%0Aproviding%20a%20universal%20foundation%20for%20vision-language-action%20humanoid%20systems.%0A&entry.1838667208=http%3A//arxiv.org/abs/2510.14952v2&entry.124074799=Read"},
{"title": "Unimedvl: Unifying Medical Multimodal Understanding And Generation\n  Through Observation-Knowledge-Analysis", "author": "Junzhi Ning and Wei Li and Cheng Tang and Jiashi Lin and Chenglong Ma and Chaoyang Zhang and Jiyao Liu and Ying Chen and Shujian Gao and Lihao Liu and Yuandong Pu and Huihui Xu and Chenhui Gou and Ziyan Huang and Yi Xin and Qi Qin and Zhongying Deng and Diping Song and Bin Fu and Guang Yang and Yuanfeng Ji and Tianbin Li and Yanzhou Su and Jin Ye and Shixiang Tang and Ming Hu and Junjun He", "abstract": "  Medical diagnostic applications require models that can process multimodal\nmedical inputs (images, patient histories, lab results) and generate diverse\noutputs including both textual reports and visual content (annotations,\nsegmentation masks, and images). Despite this need, existing medical AI systems\ndisrupt this unified process: medical image understanding models interpret\nimages but cannot generate visual outputs, while medical image generation\nmodels synthesize images but cannot provide textual explanations. This leads to\ngaps in data representation, feature integration, and task-level multimodal\ncapabilities. To this end, we propose a multi-level framework that draws\ninspiration from diagnostic workflows through the\nObservation-Knowledge-Analysis (OKA) paradigm. Specifically, at the observation\nlevel, we construct UniMed-5M, a dataset comprising over 5.6M samples that\nreformat diverse unimodal data into multimodal pairs for foundational\nobservation. At the knowledge level, we propose Progressive Curriculum Learning\nthat systematically introduces medical multimodal knowledge. At the analysis\nlevel, we introduce UniMedVL, the first medical unified multimodal model for\nthe simultaneous analysis of image understanding and generation tasks within a\nsingle architecture. UniMedVL achieves superior performance on five medical\nimage understanding benchmarks, while matching specialized models in generation\nquality across eight medical imaging modalities. Crucially, our unified\narchitecture enables bidirectional knowledge sharing: generation tasks enhance\nvisual understanding features, demonstrating that integrating traditionally\nseparate capabilities within a single medical framework unlocks improvements\nacross diverse medical vision-language tasks. Code is available at\nhttps://github.com/uni-medical/UniMedVL.\n", "link": "http://arxiv.org/abs/2510.15710v1", "date": "2025-10-17", "relevancy": 2.3844, "topK": [{"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.6254}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5902}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5902}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Unimedvl%3A%20Unifying%20Medical%20Multimodal%20Understanding%20And%20Generation%0A%20%20Through%20Observation-Knowledge-Analysis&body=Title%3A%20Unimedvl%3A%20Unifying%20Medical%20Multimodal%20Understanding%20And%20Generation%0A%20%20Through%20Observation-Knowledge-Analysis%0AAuthor%3A%20Junzhi%20Ning%20and%20Wei%20Li%20and%20Cheng%20Tang%20and%20Jiashi%20Lin%20and%20Chenglong%20Ma%20and%20Chaoyang%20Zhang%20and%20Jiyao%20Liu%20and%20Ying%20Chen%20and%20Shujian%20Gao%20and%20Lihao%20Liu%20and%20Yuandong%20Pu%20and%20Huihui%20Xu%20and%20Chenhui%20Gou%20and%20Ziyan%20Huang%20and%20Yi%20Xin%20and%20Qi%20Qin%20and%20Zhongying%20Deng%20and%20Diping%20Song%20and%20Bin%20Fu%20and%20Guang%20Yang%20and%20Yuanfeng%20Ji%20and%20Tianbin%20Li%20and%20Yanzhou%20Su%20and%20Jin%20Ye%20and%20Shixiang%20Tang%20and%20Ming%20Hu%20and%20Junjun%20He%0AAbstract%3A%20%20%20Medical%20diagnostic%20applications%20require%20models%20that%20can%20process%20multimodal%0Amedical%20inputs%20%28images%2C%20patient%20histories%2C%20lab%20results%29%20and%20generate%20diverse%0Aoutputs%20including%20both%20textual%20reports%20and%20visual%20content%20%28annotations%2C%0Asegmentation%20masks%2C%20and%20images%29.%20Despite%20this%20need%2C%20existing%20medical%20AI%20systems%0Adisrupt%20this%20unified%20process%3A%20medical%20image%20understanding%20models%20interpret%0Aimages%20but%20cannot%20generate%20visual%20outputs%2C%20while%20medical%20image%20generation%0Amodels%20synthesize%20images%20but%20cannot%20provide%20textual%20explanations.%20This%20leads%20to%0Agaps%20in%20data%20representation%2C%20feature%20integration%2C%20and%20task-level%20multimodal%0Acapabilities.%20To%20this%20end%2C%20we%20propose%20a%20multi-level%20framework%20that%20draws%0Ainspiration%20from%20diagnostic%20workflows%20through%20the%0AObservation-Knowledge-Analysis%20%28OKA%29%20paradigm.%20Specifically%2C%20at%20the%20observation%0Alevel%2C%20we%20construct%20UniMed-5M%2C%20a%20dataset%20comprising%20over%205.6M%20samples%20that%0Areformat%20diverse%20unimodal%20data%20into%20multimodal%20pairs%20for%20foundational%0Aobservation.%20At%20the%20knowledge%20level%2C%20we%20propose%20Progressive%20Curriculum%20Learning%0Athat%20systematically%20introduces%20medical%20multimodal%20knowledge.%20At%20the%20analysis%0Alevel%2C%20we%20introduce%20UniMedVL%2C%20the%20first%20medical%20unified%20multimodal%20model%20for%0Athe%20simultaneous%20analysis%20of%20image%20understanding%20and%20generation%20tasks%20within%20a%0Asingle%20architecture.%20UniMedVL%20achieves%20superior%20performance%20on%20five%20medical%0Aimage%20understanding%20benchmarks%2C%20while%20matching%20specialized%20models%20in%20generation%0Aquality%20across%20eight%20medical%20imaging%20modalities.%20Crucially%2C%20our%20unified%0Aarchitecture%20enables%20bidirectional%20knowledge%20sharing%3A%20generation%20tasks%20enhance%0Avisual%20understanding%20features%2C%20demonstrating%20that%20integrating%20traditionally%0Aseparate%20capabilities%20within%20a%20single%20medical%20framework%20unlocks%20improvements%0Aacross%20diverse%20medical%20vision-language%20tasks.%20Code%20is%20available%20at%0Ahttps%3A//github.com/uni-medical/UniMedVL.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2510.15710v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DUnimedvl%253A%2520Unifying%2520Medical%2520Multimodal%2520Understanding%2520And%2520Generation%250A%2520%2520Through%2520Observation-Knowledge-Analysis%26entry.906535625%3DJunzhi%2520Ning%2520and%2520Wei%2520Li%2520and%2520Cheng%2520Tang%2520and%2520Jiashi%2520Lin%2520and%2520Chenglong%2520Ma%2520and%2520Chaoyang%2520Zhang%2520and%2520Jiyao%2520Liu%2520and%2520Ying%2520Chen%2520and%2520Shujian%2520Gao%2520and%2520Lihao%2520Liu%2520and%2520Yuandong%2520Pu%2520and%2520Huihui%2520Xu%2520and%2520Chenhui%2520Gou%2520and%2520Ziyan%2520Huang%2520and%2520Yi%2520Xin%2520and%2520Qi%2520Qin%2520and%2520Zhongying%2520Deng%2520and%2520Diping%2520Song%2520and%2520Bin%2520Fu%2520and%2520Guang%2520Yang%2520and%2520Yuanfeng%2520Ji%2520and%2520Tianbin%2520Li%2520and%2520Yanzhou%2520Su%2520and%2520Jin%2520Ye%2520and%2520Shixiang%2520Tang%2520and%2520Ming%2520Hu%2520and%2520Junjun%2520He%26entry.1292438233%3D%2520%2520Medical%2520diagnostic%2520applications%2520require%2520models%2520that%2520can%2520process%2520multimodal%250Amedical%2520inputs%2520%2528images%252C%2520patient%2520histories%252C%2520lab%2520results%2529%2520and%2520generate%2520diverse%250Aoutputs%2520including%2520both%2520textual%2520reports%2520and%2520visual%2520content%2520%2528annotations%252C%250Asegmentation%2520masks%252C%2520and%2520images%2529.%2520Despite%2520this%2520need%252C%2520existing%2520medical%2520AI%2520systems%250Adisrupt%2520this%2520unified%2520process%253A%2520medical%2520image%2520understanding%2520models%2520interpret%250Aimages%2520but%2520cannot%2520generate%2520visual%2520outputs%252C%2520while%2520medical%2520image%2520generation%250Amodels%2520synthesize%2520images%2520but%2520cannot%2520provide%2520textual%2520explanations.%2520This%2520leads%2520to%250Agaps%2520in%2520data%2520representation%252C%2520feature%2520integration%252C%2520and%2520task-level%2520multimodal%250Acapabilities.%2520To%2520this%2520end%252C%2520we%2520propose%2520a%2520multi-level%2520framework%2520that%2520draws%250Ainspiration%2520from%2520diagnostic%2520workflows%2520through%2520the%250AObservation-Knowledge-Analysis%2520%2528OKA%2529%2520paradigm.%2520Specifically%252C%2520at%2520the%2520observation%250Alevel%252C%2520we%2520construct%2520UniMed-5M%252C%2520a%2520dataset%2520comprising%2520over%25205.6M%2520samples%2520that%250Areformat%2520diverse%2520unimodal%2520data%2520into%2520multimodal%2520pairs%2520for%2520foundational%250Aobservation.%2520At%2520the%2520knowledge%2520level%252C%2520we%2520propose%2520Progressive%2520Curriculum%2520Learning%250Athat%2520systematically%2520introduces%2520medical%2520multimodal%2520knowledge.%2520At%2520the%2520analysis%250Alevel%252C%2520we%2520introduce%2520UniMedVL%252C%2520the%2520first%2520medical%2520unified%2520multimodal%2520model%2520for%250Athe%2520simultaneous%2520analysis%2520of%2520image%2520understanding%2520and%2520generation%2520tasks%2520within%2520a%250Asingle%2520architecture.%2520UniMedVL%2520achieves%2520superior%2520performance%2520on%2520five%2520medical%250Aimage%2520understanding%2520benchmarks%252C%2520while%2520matching%2520specialized%2520models%2520in%2520generation%250Aquality%2520across%2520eight%2520medical%2520imaging%2520modalities.%2520Crucially%252C%2520our%2520unified%250Aarchitecture%2520enables%2520bidirectional%2520knowledge%2520sharing%253A%2520generation%2520tasks%2520enhance%250Avisual%2520understanding%2520features%252C%2520demonstrating%2520that%2520integrating%2520traditionally%250Aseparate%2520capabilities%2520within%2520a%2520single%2520medical%2520framework%2520unlocks%2520improvements%250Aacross%2520diverse%2520medical%2520vision-language%2520tasks.%2520Code%2520is%2520available%2520at%250Ahttps%253A//github.com/uni-medical/UniMedVL.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2510.15710v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Unimedvl%3A%20Unifying%20Medical%20Multimodal%20Understanding%20And%20Generation%0A%20%20Through%20Observation-Knowledge-Analysis&entry.906535625=Junzhi%20Ning%20and%20Wei%20Li%20and%20Cheng%20Tang%20and%20Jiashi%20Lin%20and%20Chenglong%20Ma%20and%20Chaoyang%20Zhang%20and%20Jiyao%20Liu%20and%20Ying%20Chen%20and%20Shujian%20Gao%20and%20Lihao%20Liu%20and%20Yuandong%20Pu%20and%20Huihui%20Xu%20and%20Chenhui%20Gou%20and%20Ziyan%20Huang%20and%20Yi%20Xin%20and%20Qi%20Qin%20and%20Zhongying%20Deng%20and%20Diping%20Song%20and%20Bin%20Fu%20and%20Guang%20Yang%20and%20Yuanfeng%20Ji%20and%20Tianbin%20Li%20and%20Yanzhou%20Su%20and%20Jin%20Ye%20and%20Shixiang%20Tang%20and%20Ming%20Hu%20and%20Junjun%20He&entry.1292438233=%20%20Medical%20diagnostic%20applications%20require%20models%20that%20can%20process%20multimodal%0Amedical%20inputs%20%28images%2C%20patient%20histories%2C%20lab%20results%29%20and%20generate%20diverse%0Aoutputs%20including%20both%20textual%20reports%20and%20visual%20content%20%28annotations%2C%0Asegmentation%20masks%2C%20and%20images%29.%20Despite%20this%20need%2C%20existing%20medical%20AI%20systems%0Adisrupt%20this%20unified%20process%3A%20medical%20image%20understanding%20models%20interpret%0Aimages%20but%20cannot%20generate%20visual%20outputs%2C%20while%20medical%20image%20generation%0Amodels%20synthesize%20images%20but%20cannot%20provide%20textual%20explanations.%20This%20leads%20to%0Agaps%20in%20data%20representation%2C%20feature%20integration%2C%20and%20task-level%20multimodal%0Acapabilities.%20To%20this%20end%2C%20we%20propose%20a%20multi-level%20framework%20that%20draws%0Ainspiration%20from%20diagnostic%20workflows%20through%20the%0AObservation-Knowledge-Analysis%20%28OKA%29%20paradigm.%20Specifically%2C%20at%20the%20observation%0Alevel%2C%20we%20construct%20UniMed-5M%2C%20a%20dataset%20comprising%20over%205.6M%20samples%20that%0Areformat%20diverse%20unimodal%20data%20into%20multimodal%20pairs%20for%20foundational%0Aobservation.%20At%20the%20knowledge%20level%2C%20we%20propose%20Progressive%20Curriculum%20Learning%0Athat%20systematically%20introduces%20medical%20multimodal%20knowledge.%20At%20the%20analysis%0Alevel%2C%20we%20introduce%20UniMedVL%2C%20the%20first%20medical%20unified%20multimodal%20model%20for%0Athe%20simultaneous%20analysis%20of%20image%20understanding%20and%20generation%20tasks%20within%20a%0Asingle%20architecture.%20UniMedVL%20achieves%20superior%20performance%20on%20five%20medical%0Aimage%20understanding%20benchmarks%2C%20while%20matching%20specialized%20models%20in%20generation%0Aquality%20across%20eight%20medical%20imaging%20modalities.%20Crucially%2C%20our%20unified%0Aarchitecture%20enables%20bidirectional%20knowledge%20sharing%3A%20generation%20tasks%20enhance%0Avisual%20understanding%20features%2C%20demonstrating%20that%20integrating%20traditionally%0Aseparate%20capabilities%20within%20a%20single%20medical%20framework%20unlocks%20improvements%0Aacross%20diverse%20medical%20vision-language%20tasks.%20Code%20is%20available%20at%0Ahttps%3A//github.com/uni-medical/UniMedVL.%0A&entry.1838667208=http%3A//arxiv.org/abs/2510.15710v1&entry.124074799=Read"},
{"title": "VISTA: A Test-Time Self-Improving Video Generation Agent", "author": "Do Xuan Long and Xingchen Wan and Hootan Nakhost and Chen-Yu Lee and Tomas Pfister and Sercan \u00d6. Ar\u0131k", "abstract": "  Despite rapid advances in text-to-video synthesis, generated video quality\nremains critically dependent on precise user prompts. Existing test-time\noptimization methods, successful in other domains, struggle with the\nmulti-faceted nature of video. In this work, we introduce VISTA (Video\nIterative Self-improvemenT Agent), a novel multi-agent system that autonomously\nimproves video generation through refining prompts in an iterative loop. VISTA\nfirst decomposes a user idea into a structured temporal plan. After generation,\nthe best video is identified through a robust pairwise tournament. This winning\nvideo is then critiqued by a trio of specialized agents focusing on visual,\naudio, and contextual fidelity. Finally, a reasoning agent synthesizes this\nfeedback to introspectively rewrite and enhance the prompt for the next\ngeneration cycle. Experiments on single- and multi-scene video generation\nscenarios show that while prior methods yield inconsistent gains, VISTA\nconsistently improves video quality and alignment with user intent, achieving\nup to 60% pairwise win rate against state-of-the-art baselines. Human\nevaluators concur, preferring VISTA outputs in 66.4% of comparisons.\n", "link": "http://arxiv.org/abs/2510.15831v1", "date": "2025-10-17", "relevancy": 2.3758, "topK": [{"title": "GeneVA: A Dataset of Human Annotations for Generative Text to Video\n  Artifacts", "link": "http://arxiv.org/abs/2509.08818v1", "similarity": 0.604}, {"title": "Magic-Me: Identity-Specific Video Customized Diffusion", "link": "http://arxiv.org/abs/2402.09368v1", "similarity": 0.5938}, {"title": "Customize-A-Video: One-Shot Motion Customization of Text-to-Video\n  Diffusion Models", "link": "http://arxiv.org/abs/2402.14780v1", "similarity": 0.5691}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20VISTA%3A%20A%20Test-Time%20Self-Improving%20Video%20Generation%20Agent&body=Title%3A%20VISTA%3A%20A%20Test-Time%20Self-Improving%20Video%20Generation%20Agent%0AAuthor%3A%20Do%20Xuan%20Long%20and%20Xingchen%20Wan%20and%20Hootan%20Nakhost%20and%20Chen-Yu%20Lee%20and%20Tomas%20Pfister%20and%20Sercan%20%C3%96.%20Ar%C4%B1k%0AAbstract%3A%20%20%20Despite%20rapid%20advances%20in%20text-to-video%20synthesis%2C%20generated%20video%20quality%0Aremains%20critically%20dependent%20on%20precise%20user%20prompts.%20Existing%20test-time%0Aoptimization%20methods%2C%20successful%20in%20other%20domains%2C%20struggle%20with%20the%0Amulti-faceted%20nature%20of%20video.%20In%20this%20work%2C%20we%20introduce%20VISTA%20%28Video%0AIterative%20Self-improvemenT%20Agent%29%2C%20a%20novel%20multi-agent%20system%20that%20autonomously%0Aimproves%20video%20generation%20through%20refining%20prompts%20in%20an%20iterative%20loop.%20VISTA%0Afirst%20decomposes%20a%20user%20idea%20into%20a%20structured%20temporal%20plan.%20After%20generation%2C%0Athe%20best%20video%20is%20identified%20through%20a%20robust%20pairwise%20tournament.%20This%20winning%0Avideo%20is%20then%20critiqued%20by%20a%20trio%20of%20specialized%20agents%20focusing%20on%20visual%2C%0Aaudio%2C%20and%20contextual%20fidelity.%20Finally%2C%20a%20reasoning%20agent%20synthesizes%20this%0Afeedback%20to%20introspectively%20rewrite%20and%20enhance%20the%20prompt%20for%20the%20next%0Ageneration%20cycle.%20Experiments%20on%20single-%20and%20multi-scene%20video%20generation%0Ascenarios%20show%20that%20while%20prior%20methods%20yield%20inconsistent%20gains%2C%20VISTA%0Aconsistently%20improves%20video%20quality%20and%20alignment%20with%20user%20intent%2C%20achieving%0Aup%20to%2060%25%20pairwise%20win%20rate%20against%20state-of-the-art%20baselines.%20Human%0Aevaluators%20concur%2C%20preferring%20VISTA%20outputs%20in%2066.4%25%20of%20comparisons.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2510.15831v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DVISTA%253A%2520A%2520Test-Time%2520Self-Improving%2520Video%2520Generation%2520Agent%26entry.906535625%3DDo%2520Xuan%2520Long%2520and%2520Xingchen%2520Wan%2520and%2520Hootan%2520Nakhost%2520and%2520Chen-Yu%2520Lee%2520and%2520Tomas%2520Pfister%2520and%2520Sercan%2520%25C3%2596.%2520Ar%25C4%25B1k%26entry.1292438233%3D%2520%2520Despite%2520rapid%2520advances%2520in%2520text-to-video%2520synthesis%252C%2520generated%2520video%2520quality%250Aremains%2520critically%2520dependent%2520on%2520precise%2520user%2520prompts.%2520Existing%2520test-time%250Aoptimization%2520methods%252C%2520successful%2520in%2520other%2520domains%252C%2520struggle%2520with%2520the%250Amulti-faceted%2520nature%2520of%2520video.%2520In%2520this%2520work%252C%2520we%2520introduce%2520VISTA%2520%2528Video%250AIterative%2520Self-improvemenT%2520Agent%2529%252C%2520a%2520novel%2520multi-agent%2520system%2520that%2520autonomously%250Aimproves%2520video%2520generation%2520through%2520refining%2520prompts%2520in%2520an%2520iterative%2520loop.%2520VISTA%250Afirst%2520decomposes%2520a%2520user%2520idea%2520into%2520a%2520structured%2520temporal%2520plan.%2520After%2520generation%252C%250Athe%2520best%2520video%2520is%2520identified%2520through%2520a%2520robust%2520pairwise%2520tournament.%2520This%2520winning%250Avideo%2520is%2520then%2520critiqued%2520by%2520a%2520trio%2520of%2520specialized%2520agents%2520focusing%2520on%2520visual%252C%250Aaudio%252C%2520and%2520contextual%2520fidelity.%2520Finally%252C%2520a%2520reasoning%2520agent%2520synthesizes%2520this%250Afeedback%2520to%2520introspectively%2520rewrite%2520and%2520enhance%2520the%2520prompt%2520for%2520the%2520next%250Ageneration%2520cycle.%2520Experiments%2520on%2520single-%2520and%2520multi-scene%2520video%2520generation%250Ascenarios%2520show%2520that%2520while%2520prior%2520methods%2520yield%2520inconsistent%2520gains%252C%2520VISTA%250Aconsistently%2520improves%2520video%2520quality%2520and%2520alignment%2520with%2520user%2520intent%252C%2520achieving%250Aup%2520to%252060%2525%2520pairwise%2520win%2520rate%2520against%2520state-of-the-art%2520baselines.%2520Human%250Aevaluators%2520concur%252C%2520preferring%2520VISTA%2520outputs%2520in%252066.4%2525%2520of%2520comparisons.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2510.15831v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=VISTA%3A%20A%20Test-Time%20Self-Improving%20Video%20Generation%20Agent&entry.906535625=Do%20Xuan%20Long%20and%20Xingchen%20Wan%20and%20Hootan%20Nakhost%20and%20Chen-Yu%20Lee%20and%20Tomas%20Pfister%20and%20Sercan%20%C3%96.%20Ar%C4%B1k&entry.1292438233=%20%20Despite%20rapid%20advances%20in%20text-to-video%20synthesis%2C%20generated%20video%20quality%0Aremains%20critically%20dependent%20on%20precise%20user%20prompts.%20Existing%20test-time%0Aoptimization%20methods%2C%20successful%20in%20other%20domains%2C%20struggle%20with%20the%0Amulti-faceted%20nature%20of%20video.%20In%20this%20work%2C%20we%20introduce%20VISTA%20%28Video%0AIterative%20Self-improvemenT%20Agent%29%2C%20a%20novel%20multi-agent%20system%20that%20autonomously%0Aimproves%20video%20generation%20through%20refining%20prompts%20in%20an%20iterative%20loop.%20VISTA%0Afirst%20decomposes%20a%20user%20idea%20into%20a%20structured%20temporal%20plan.%20After%20generation%2C%0Athe%20best%20video%20is%20identified%20through%20a%20robust%20pairwise%20tournament.%20This%20winning%0Avideo%20is%20then%20critiqued%20by%20a%20trio%20of%20specialized%20agents%20focusing%20on%20visual%2C%0Aaudio%2C%20and%20contextual%20fidelity.%20Finally%2C%20a%20reasoning%20agent%20synthesizes%20this%0Afeedback%20to%20introspectively%20rewrite%20and%20enhance%20the%20prompt%20for%20the%20next%0Ageneration%20cycle.%20Experiments%20on%20single-%20and%20multi-scene%20video%20generation%0Ascenarios%20show%20that%20while%20prior%20methods%20yield%20inconsistent%20gains%2C%20VISTA%0Aconsistently%20improves%20video%20quality%20and%20alignment%20with%20user%20intent%2C%20achieving%0Aup%20to%2060%25%20pairwise%20win%20rate%20against%20state-of-the-art%20baselines.%20Human%0Aevaluators%20concur%2C%20preferring%20VISTA%20outputs%20in%2066.4%25%20of%20comparisons.%0A&entry.1838667208=http%3A//arxiv.org/abs/2510.15831v1&entry.124074799=Read"},
{"title": "Disentanglement of Sources in a Multi-Stream Variational Autoencoder", "author": "Veranika Boukun and J\u00f6rg L\u00fccke", "abstract": "  Variational autoencoders (VAEs) are a leading approach to address the problem\nof learning disentangled representations. Typically a single VAE is used and\ndisentangled representations are sought in its continuous latent space. Here we\nexplore a different approach by using discrete latents to combine\nVAE-representations of individual sources. The combination is done based on an\nexplicit model for source combination, and we here use a linear combination\nmodel which is well suited, e.g., for acoustic data. We formally define such a\nmulti-stream VAE (MS-VAE) approach, derive its inference and learning\nequations, and we numerically investigate its principled functionality. The\nMS-VAE is domain-agnostic, and we here explore its ability to separate sources\ninto different streams using superimposed hand-written digits, and mixed\nacoustic sources in a speaker diarization task. We observe a clear separation\nof digits, and on speaker diarization we observe an especially low rate of\nmissed speakers. Numerical experiments further highlight the flexibility of the\napproach across varying amounts of supervision and training data.\n", "link": "http://arxiv.org/abs/2510.15669v1", "date": "2025-10-17", "relevancy": 2.3726, "topK": [{"title": "IAE: Implicit Autoencoder for Point Cloud Self-supervised Representation Learning", "link": "https://arxiv.org/abs/2201.00785", "similarity": 0.4826}, {"title": "Magic-Me: Identity-Specific Video Customized Diffusion", "link": "http://arxiv.org/abs/2402.09368v1", "similarity": 0.4757}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.4653}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Disentanglement%20of%20Sources%20in%20a%20Multi-Stream%20Variational%20Autoencoder&body=Title%3A%20Disentanglement%20of%20Sources%20in%20a%20Multi-Stream%20Variational%20Autoencoder%0AAuthor%3A%20Veranika%20Boukun%20and%20J%C3%B6rg%20L%C3%BCcke%0AAbstract%3A%20%20%20Variational%20autoencoders%20%28VAEs%29%20are%20a%20leading%20approach%20to%20address%20the%20problem%0Aof%20learning%20disentangled%20representations.%20Typically%20a%20single%20VAE%20is%20used%20and%0Adisentangled%20representations%20are%20sought%20in%20its%20continuous%20latent%20space.%20Here%20we%0Aexplore%20a%20different%20approach%20by%20using%20discrete%20latents%20to%20combine%0AVAE-representations%20of%20individual%20sources.%20The%20combination%20is%20done%20based%20on%20an%0Aexplicit%20model%20for%20source%20combination%2C%20and%20we%20here%20use%20a%20linear%20combination%0Amodel%20which%20is%20well%20suited%2C%20e.g.%2C%20for%20acoustic%20data.%20We%20formally%20define%20such%20a%0Amulti-stream%20VAE%20%28MS-VAE%29%20approach%2C%20derive%20its%20inference%20and%20learning%0Aequations%2C%20and%20we%20numerically%20investigate%20its%20principled%20functionality.%20The%0AMS-VAE%20is%20domain-agnostic%2C%20and%20we%20here%20explore%20its%20ability%20to%20separate%20sources%0Ainto%20different%20streams%20using%20superimposed%20hand-written%20digits%2C%20and%20mixed%0Aacoustic%20sources%20in%20a%20speaker%20diarization%20task.%20We%20observe%20a%20clear%20separation%0Aof%20digits%2C%20and%20on%20speaker%20diarization%20we%20observe%20an%20especially%20low%20rate%20of%0Amissed%20speakers.%20Numerical%20experiments%20further%20highlight%20the%20flexibility%20of%20the%0Aapproach%20across%20varying%20amounts%20of%20supervision%20and%20training%20data.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2510.15669v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DDisentanglement%2520of%2520Sources%2520in%2520a%2520Multi-Stream%2520Variational%2520Autoencoder%26entry.906535625%3DVeranika%2520Boukun%2520and%2520J%25C3%25B6rg%2520L%25C3%25BCcke%26entry.1292438233%3D%2520%2520Variational%2520autoencoders%2520%2528VAEs%2529%2520are%2520a%2520leading%2520approach%2520to%2520address%2520the%2520problem%250Aof%2520learning%2520disentangled%2520representations.%2520Typically%2520a%2520single%2520VAE%2520is%2520used%2520and%250Adisentangled%2520representations%2520are%2520sought%2520in%2520its%2520continuous%2520latent%2520space.%2520Here%2520we%250Aexplore%2520a%2520different%2520approach%2520by%2520using%2520discrete%2520latents%2520to%2520combine%250AVAE-representations%2520of%2520individual%2520sources.%2520The%2520combination%2520is%2520done%2520based%2520on%2520an%250Aexplicit%2520model%2520for%2520source%2520combination%252C%2520and%2520we%2520here%2520use%2520a%2520linear%2520combination%250Amodel%2520which%2520is%2520well%2520suited%252C%2520e.g.%252C%2520for%2520acoustic%2520data.%2520We%2520formally%2520define%2520such%2520a%250Amulti-stream%2520VAE%2520%2528MS-VAE%2529%2520approach%252C%2520derive%2520its%2520inference%2520and%2520learning%250Aequations%252C%2520and%2520we%2520numerically%2520investigate%2520its%2520principled%2520functionality.%2520The%250AMS-VAE%2520is%2520domain-agnostic%252C%2520and%2520we%2520here%2520explore%2520its%2520ability%2520to%2520separate%2520sources%250Ainto%2520different%2520streams%2520using%2520superimposed%2520hand-written%2520digits%252C%2520and%2520mixed%250Aacoustic%2520sources%2520in%2520a%2520speaker%2520diarization%2520task.%2520We%2520observe%2520a%2520clear%2520separation%250Aof%2520digits%252C%2520and%2520on%2520speaker%2520diarization%2520we%2520observe%2520an%2520especially%2520low%2520rate%2520of%250Amissed%2520speakers.%2520Numerical%2520experiments%2520further%2520highlight%2520the%2520flexibility%2520of%2520the%250Aapproach%2520across%2520varying%2520amounts%2520of%2520supervision%2520and%2520training%2520data.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2510.15669v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Disentanglement%20of%20Sources%20in%20a%20Multi-Stream%20Variational%20Autoencoder&entry.906535625=Veranika%20Boukun%20and%20J%C3%B6rg%20L%C3%BCcke&entry.1292438233=%20%20Variational%20autoencoders%20%28VAEs%29%20are%20a%20leading%20approach%20to%20address%20the%20problem%0Aof%20learning%20disentangled%20representations.%20Typically%20a%20single%20VAE%20is%20used%20and%0Adisentangled%20representations%20are%20sought%20in%20its%20continuous%20latent%20space.%20Here%20we%0Aexplore%20a%20different%20approach%20by%20using%20discrete%20latents%20to%20combine%0AVAE-representations%20of%20individual%20sources.%20The%20combination%20is%20done%20based%20on%20an%0Aexplicit%20model%20for%20source%20combination%2C%20and%20we%20here%20use%20a%20linear%20combination%0Amodel%20which%20is%20well%20suited%2C%20e.g.%2C%20for%20acoustic%20data.%20We%20formally%20define%20such%20a%0Amulti-stream%20VAE%20%28MS-VAE%29%20approach%2C%20derive%20its%20inference%20and%20learning%0Aequations%2C%20and%20we%20numerically%20investigate%20its%20principled%20functionality.%20The%0AMS-VAE%20is%20domain-agnostic%2C%20and%20we%20here%20explore%20its%20ability%20to%20separate%20sources%0Ainto%20different%20streams%20using%20superimposed%20hand-written%20digits%2C%20and%20mixed%0Aacoustic%20sources%20in%20a%20speaker%20diarization%20task.%20We%20observe%20a%20clear%20separation%0Aof%20digits%2C%20and%20on%20speaker%20diarization%20we%20observe%20an%20especially%20low%20rate%20of%0Amissed%20speakers.%20Numerical%20experiments%20further%20highlight%20the%20flexibility%20of%20the%0Aapproach%20across%20varying%20amounts%20of%20supervision%20and%20training%20data.%0A&entry.1838667208=http%3A//arxiv.org/abs/2510.15669v1&entry.124074799=Read"},
{"title": "Real-time Recognition of Human Interactions from a Single RGB-D Camera\n  for Socially-Aware Robot Navigation", "author": "Thanh Long Nguyen and Duc Phu Nguyen and Thanh Thao Ton Nu and Quan Le and Thuan Hoang Tran and Manh Duong Phung", "abstract": "  {Recognizing human interactions is essential for social robots as it enables\nthem to navigate safely and naturally in shared environments. Conventional\nrobotic systems however often focus on obstacle avoidance, neglecting social\ncues necessary for seamless human-robot interaction. To address this gap, we\npropose a framework to recognize human group interactions for socially aware\nnavigation. Our method utilizes color and depth frames from a monocular RGB-D\ncamera to estimate 3D human keypoints and positions. Principal component\nanalysis (PCA) is then used to determine dominant interaction directions. The\nshoelace formula is finally applied to compute interest points and engagement\nareas. Extensive experiments have been conducted to evaluate the validity of\nthe proposed method. The results show that our method is capable of recognizing\ngroup interactions across different scenarios with varying numbers of\nindividuals. It also achieves high-speed performance, processing each frame in\napproximately 4 ms on a single-board computer used in robotic systems. The\nmethod is implemented as a ROS 2 package making it simple to integrate into\nexisting navigation systems. Source code is available at\nhttps://github.com/thanhlong103/social-interaction-detector\n", "link": "http://arxiv.org/abs/2509.24907v2", "date": "2025-10-17", "relevancy": 2.3617, "topK": [{"title": "Egocentric Computer Vision for Hands-Free Robotic Wheelchair Navigation", "link": "https://link.springer.com/article/10.1007/s10846-023-01807-4", "similarity": 0.6141}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.6027}, {"title": "Synthesizing Moving People with 3D Control", "link": "http://arxiv.org/abs/2401.10889v2", "similarity": 0.5619}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Real-time%20Recognition%20of%20Human%20Interactions%20from%20a%20Single%20RGB-D%20Camera%0A%20%20for%20Socially-Aware%20Robot%20Navigation&body=Title%3A%20Real-time%20Recognition%20of%20Human%20Interactions%20from%20a%20Single%20RGB-D%20Camera%0A%20%20for%20Socially-Aware%20Robot%20Navigation%0AAuthor%3A%20Thanh%20Long%20Nguyen%20and%20Duc%20Phu%20Nguyen%20and%20Thanh%20Thao%20Ton%20Nu%20and%20Quan%20Le%20and%20Thuan%20Hoang%20Tran%20and%20Manh%20Duong%20Phung%0AAbstract%3A%20%20%20%7BRecognizing%20human%20interactions%20is%20essential%20for%20social%20robots%20as%20it%20enables%0Athem%20to%20navigate%20safely%20and%20naturally%20in%20shared%20environments.%20Conventional%0Arobotic%20systems%20however%20often%20focus%20on%20obstacle%20avoidance%2C%20neglecting%20social%0Acues%20necessary%20for%20seamless%20human-robot%20interaction.%20To%20address%20this%20gap%2C%20we%0Apropose%20a%20framework%20to%20recognize%20human%20group%20interactions%20for%20socially%20aware%0Anavigation.%20Our%20method%20utilizes%20color%20and%20depth%20frames%20from%20a%20monocular%20RGB-D%0Acamera%20to%20estimate%203D%20human%20keypoints%20and%20positions.%20Principal%20component%0Aanalysis%20%28PCA%29%20is%20then%20used%20to%20determine%20dominant%20interaction%20directions.%20The%0Ashoelace%20formula%20is%20finally%20applied%20to%20compute%20interest%20points%20and%20engagement%0Aareas.%20Extensive%20experiments%20have%20been%20conducted%20to%20evaluate%20the%20validity%20of%0Athe%20proposed%20method.%20The%20results%20show%20that%20our%20method%20is%20capable%20of%20recognizing%0Agroup%20interactions%20across%20different%20scenarios%20with%20varying%20numbers%20of%0Aindividuals.%20It%20also%20achieves%20high-speed%20performance%2C%20processing%20each%20frame%20in%0Aapproximately%204%20ms%20on%20a%20single-board%20computer%20used%20in%20robotic%20systems.%20The%0Amethod%20is%20implemented%20as%20a%20ROS%202%20package%20making%20it%20simple%20to%20integrate%20into%0Aexisting%20navigation%20systems.%20Source%20code%20is%20available%20at%0Ahttps%3A//github.com/thanhlong103/social-interaction-detector%0A%0ALink%3A%20http%3A//arxiv.org/abs/2509.24907v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DReal-time%2520Recognition%2520of%2520Human%2520Interactions%2520from%2520a%2520Single%2520RGB-D%2520Camera%250A%2520%2520for%2520Socially-Aware%2520Robot%2520Navigation%26entry.906535625%3DThanh%2520Long%2520Nguyen%2520and%2520Duc%2520Phu%2520Nguyen%2520and%2520Thanh%2520Thao%2520Ton%2520Nu%2520and%2520Quan%2520Le%2520and%2520Thuan%2520Hoang%2520Tran%2520and%2520Manh%2520Duong%2520Phung%26entry.1292438233%3D%2520%2520%257BRecognizing%2520human%2520interactions%2520is%2520essential%2520for%2520social%2520robots%2520as%2520it%2520enables%250Athem%2520to%2520navigate%2520safely%2520and%2520naturally%2520in%2520shared%2520environments.%2520Conventional%250Arobotic%2520systems%2520however%2520often%2520focus%2520on%2520obstacle%2520avoidance%252C%2520neglecting%2520social%250Acues%2520necessary%2520for%2520seamless%2520human-robot%2520interaction.%2520To%2520address%2520this%2520gap%252C%2520we%250Apropose%2520a%2520framework%2520to%2520recognize%2520human%2520group%2520interactions%2520for%2520socially%2520aware%250Anavigation.%2520Our%2520method%2520utilizes%2520color%2520and%2520depth%2520frames%2520from%2520a%2520monocular%2520RGB-D%250Acamera%2520to%2520estimate%25203D%2520human%2520keypoints%2520and%2520positions.%2520Principal%2520component%250Aanalysis%2520%2528PCA%2529%2520is%2520then%2520used%2520to%2520determine%2520dominant%2520interaction%2520directions.%2520The%250Ashoelace%2520formula%2520is%2520finally%2520applied%2520to%2520compute%2520interest%2520points%2520and%2520engagement%250Aareas.%2520Extensive%2520experiments%2520have%2520been%2520conducted%2520to%2520evaluate%2520the%2520validity%2520of%250Athe%2520proposed%2520method.%2520The%2520results%2520show%2520that%2520our%2520method%2520is%2520capable%2520of%2520recognizing%250Agroup%2520interactions%2520across%2520different%2520scenarios%2520with%2520varying%2520numbers%2520of%250Aindividuals.%2520It%2520also%2520achieves%2520high-speed%2520performance%252C%2520processing%2520each%2520frame%2520in%250Aapproximately%25204%2520ms%2520on%2520a%2520single-board%2520computer%2520used%2520in%2520robotic%2520systems.%2520The%250Amethod%2520is%2520implemented%2520as%2520a%2520ROS%25202%2520package%2520making%2520it%2520simple%2520to%2520integrate%2520into%250Aexisting%2520navigation%2520systems.%2520Source%2520code%2520is%2520available%2520at%250Ahttps%253A//github.com/thanhlong103/social-interaction-detector%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2509.24907v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Real-time%20Recognition%20of%20Human%20Interactions%20from%20a%20Single%20RGB-D%20Camera%0A%20%20for%20Socially-Aware%20Robot%20Navigation&entry.906535625=Thanh%20Long%20Nguyen%20and%20Duc%20Phu%20Nguyen%20and%20Thanh%20Thao%20Ton%20Nu%20and%20Quan%20Le%20and%20Thuan%20Hoang%20Tran%20and%20Manh%20Duong%20Phung&entry.1292438233=%20%20%7BRecognizing%20human%20interactions%20is%20essential%20for%20social%20robots%20as%20it%20enables%0Athem%20to%20navigate%20safely%20and%20naturally%20in%20shared%20environments.%20Conventional%0Arobotic%20systems%20however%20often%20focus%20on%20obstacle%20avoidance%2C%20neglecting%20social%0Acues%20necessary%20for%20seamless%20human-robot%20interaction.%20To%20address%20this%20gap%2C%20we%0Apropose%20a%20framework%20to%20recognize%20human%20group%20interactions%20for%20socially%20aware%0Anavigation.%20Our%20method%20utilizes%20color%20and%20depth%20frames%20from%20a%20monocular%20RGB-D%0Acamera%20to%20estimate%203D%20human%20keypoints%20and%20positions.%20Principal%20component%0Aanalysis%20%28PCA%29%20is%20then%20used%20to%20determine%20dominant%20interaction%20directions.%20The%0Ashoelace%20formula%20is%20finally%20applied%20to%20compute%20interest%20points%20and%20engagement%0Aareas.%20Extensive%20experiments%20have%20been%20conducted%20to%20evaluate%20the%20validity%20of%0Athe%20proposed%20method.%20The%20results%20show%20that%20our%20method%20is%20capable%20of%20recognizing%0Agroup%20interactions%20across%20different%20scenarios%20with%20varying%20numbers%20of%0Aindividuals.%20It%20also%20achieves%20high-speed%20performance%2C%20processing%20each%20frame%20in%0Aapproximately%204%20ms%20on%20a%20single-board%20computer%20used%20in%20robotic%20systems.%20The%0Amethod%20is%20implemented%20as%20a%20ROS%202%20package%20making%20it%20simple%20to%20integrate%20into%0Aexisting%20navigation%20systems.%20Source%20code%20is%20available%20at%0Ahttps%3A//github.com/thanhlong103/social-interaction-detector%0A&entry.1838667208=http%3A//arxiv.org/abs/2509.24907v2&entry.124074799=Read"},
{"title": "Dynamic Recalibration in LiDAR SLAM: Integrating AI and Geometric\n  Methods with Real-Time Feedback Using INAF Fusion", "author": "Zahra Arjmandi and Gunho Sohn", "abstract": "  This paper presents a novel fusion technique for LiDAR Simultaneous\nLocalization and Mapping (SLAM), aimed at improving localization and 3D mapping\nusing LiDAR sensor. Our approach centers on the Inferred Attention Fusion\n(INAF) module, which integrates AI with geometric odometry. Utilizing the KITTI\ndataset's LiDAR data, INAF dynamically adjusts attention weights based on\nenvironmental feedback, enhancing the system's adaptability and measurement\naccuracy. This method advances the precision of both localization and 3D\nmapping, demonstrating the potential of our fusion technique to enhance\nautonomous navigation systems in complex scenarios.\n", "link": "http://arxiv.org/abs/2510.15803v1", "date": "2025-10-17", "relevancy": 2.3568, "topK": [{"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.6067}, {"title": "RTG-SLAM: Real-time 3D Reconstruction at Scale using Gaussian Splatting", "link": "http://arxiv.org/abs/2404.19706v1", "similarity": 0.584}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.5587}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Dynamic%20Recalibration%20in%20LiDAR%20SLAM%3A%20Integrating%20AI%20and%20Geometric%0A%20%20Methods%20with%20Real-Time%20Feedback%20Using%20INAF%20Fusion&body=Title%3A%20Dynamic%20Recalibration%20in%20LiDAR%20SLAM%3A%20Integrating%20AI%20and%20Geometric%0A%20%20Methods%20with%20Real-Time%20Feedback%20Using%20INAF%20Fusion%0AAuthor%3A%20Zahra%20Arjmandi%20and%20Gunho%20Sohn%0AAbstract%3A%20%20%20This%20paper%20presents%20a%20novel%20fusion%20technique%20for%20LiDAR%20Simultaneous%0ALocalization%20and%20Mapping%20%28SLAM%29%2C%20aimed%20at%20improving%20localization%20and%203D%20mapping%0Ausing%20LiDAR%20sensor.%20Our%20approach%20centers%20on%20the%20Inferred%20Attention%20Fusion%0A%28INAF%29%20module%2C%20which%20integrates%20AI%20with%20geometric%20odometry.%20Utilizing%20the%20KITTI%0Adataset%27s%20LiDAR%20data%2C%20INAF%20dynamically%20adjusts%20attention%20weights%20based%20on%0Aenvironmental%20feedback%2C%20enhancing%20the%20system%27s%20adaptability%20and%20measurement%0Aaccuracy.%20This%20method%20advances%20the%20precision%20of%20both%20localization%20and%203D%0Amapping%2C%20demonstrating%20the%20potential%20of%20our%20fusion%20technique%20to%20enhance%0Aautonomous%20navigation%20systems%20in%20complex%20scenarios.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2510.15803v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DDynamic%2520Recalibration%2520in%2520LiDAR%2520SLAM%253A%2520Integrating%2520AI%2520and%2520Geometric%250A%2520%2520Methods%2520with%2520Real-Time%2520Feedback%2520Using%2520INAF%2520Fusion%26entry.906535625%3DZahra%2520Arjmandi%2520and%2520Gunho%2520Sohn%26entry.1292438233%3D%2520%2520This%2520paper%2520presents%2520a%2520novel%2520fusion%2520technique%2520for%2520LiDAR%2520Simultaneous%250ALocalization%2520and%2520Mapping%2520%2528SLAM%2529%252C%2520aimed%2520at%2520improving%2520localization%2520and%25203D%2520mapping%250Ausing%2520LiDAR%2520sensor.%2520Our%2520approach%2520centers%2520on%2520the%2520Inferred%2520Attention%2520Fusion%250A%2528INAF%2529%2520module%252C%2520which%2520integrates%2520AI%2520with%2520geometric%2520odometry.%2520Utilizing%2520the%2520KITTI%250Adataset%2527s%2520LiDAR%2520data%252C%2520INAF%2520dynamically%2520adjusts%2520attention%2520weights%2520based%2520on%250Aenvironmental%2520feedback%252C%2520enhancing%2520the%2520system%2527s%2520adaptability%2520and%2520measurement%250Aaccuracy.%2520This%2520method%2520advances%2520the%2520precision%2520of%2520both%2520localization%2520and%25203D%250Amapping%252C%2520demonstrating%2520the%2520potential%2520of%2520our%2520fusion%2520technique%2520to%2520enhance%250Aautonomous%2520navigation%2520systems%2520in%2520complex%2520scenarios.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2510.15803v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Dynamic%20Recalibration%20in%20LiDAR%20SLAM%3A%20Integrating%20AI%20and%20Geometric%0A%20%20Methods%20with%20Real-Time%20Feedback%20Using%20INAF%20Fusion&entry.906535625=Zahra%20Arjmandi%20and%20Gunho%20Sohn&entry.1292438233=%20%20This%20paper%20presents%20a%20novel%20fusion%20technique%20for%20LiDAR%20Simultaneous%0ALocalization%20and%20Mapping%20%28SLAM%29%2C%20aimed%20at%20improving%20localization%20and%203D%20mapping%0Ausing%20LiDAR%20sensor.%20Our%20approach%20centers%20on%20the%20Inferred%20Attention%20Fusion%0A%28INAF%29%20module%2C%20which%20integrates%20AI%20with%20geometric%20odometry.%20Utilizing%20the%20KITTI%0Adataset%27s%20LiDAR%20data%2C%20INAF%20dynamically%20adjusts%20attention%20weights%20based%20on%0Aenvironmental%20feedback%2C%20enhancing%20the%20system%27s%20adaptability%20and%20measurement%0Aaccuracy.%20This%20method%20advances%20the%20precision%20of%20both%20localization%20and%203D%0Amapping%2C%20demonstrating%20the%20potential%20of%20our%20fusion%20technique%20to%20enhance%0Aautonomous%20navigation%20systems%20in%20complex%20scenarios.%0A&entry.1838667208=http%3A//arxiv.org/abs/2510.15803v1&entry.124074799=Read"},
{"title": "DexCanvas: Bridging Human Demonstrations and Robot Learning for\n  Dexterous Manipulation", "author": "Xinyue Xu and Jieqiang Sun and  Jing and  Dai and Siyuan Chen and Lanjie Ma and Ke Sun and Bin Zhao and Jianbo Yuan and Yiwen Lu", "abstract": "  We present DexCanvas, a large-scale hybrid real-synthetic human manipulation\ndataset containing 7,000 hours of dexterous hand-object interactions seeded\nfrom 70 hours of real human demonstrations, organized across 21 fundamental\nmanipulation types based on the Cutkosky taxonomy. Each entry combines\nsynchronized multi-view RGB-D, high-precision mocap with MANO hand parameters,\nand per-frame contact points with physically consistent force profiles. Our\nreal-to-sim pipeline uses reinforcement learning to train policies that control\nan actuated MANO hand in physics simulation, reproducing human demonstrations\nwhile discovering the underlying contact forces that generate the observed\nobject motion. DexCanvas is the first manipulation dataset to combine\nlarge-scale real demonstrations, systematic skill coverage based on established\ntaxonomies, and physics-validated contact annotations. The dataset can\nfacilitate research in robotic manipulation learning, contact-rich control, and\nskill transfer across different hand morphologies.\n", "link": "http://arxiv.org/abs/2510.15786v1", "date": "2025-10-17", "relevancy": 2.3507, "topK": [{"title": "UGG: Unified Generative Grasping", "link": "https://arxiv.org/abs/2311.16917", "similarity": 0.597}, {"title": "RoboEXP: Action-Conditioned Scene Graph via Interactive Exploration for\n  Robotic Manipulation", "link": "http://arxiv.org/abs/2402.15487v1", "similarity": 0.5822}, {"title": "Synthesizing Moving People with 3D Control", "link": "http://arxiv.org/abs/2401.10889v2", "similarity": 0.5806}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20DexCanvas%3A%20Bridging%20Human%20Demonstrations%20and%20Robot%20Learning%20for%0A%20%20Dexterous%20Manipulation&body=Title%3A%20DexCanvas%3A%20Bridging%20Human%20Demonstrations%20and%20Robot%20Learning%20for%0A%20%20Dexterous%20Manipulation%0AAuthor%3A%20Xinyue%20Xu%20and%20Jieqiang%20Sun%20and%20%20Jing%20and%20%20Dai%20and%20Siyuan%20Chen%20and%20Lanjie%20Ma%20and%20Ke%20Sun%20and%20Bin%20Zhao%20and%20Jianbo%20Yuan%20and%20Yiwen%20Lu%0AAbstract%3A%20%20%20We%20present%20DexCanvas%2C%20a%20large-scale%20hybrid%20real-synthetic%20human%20manipulation%0Adataset%20containing%207%2C000%20hours%20of%20dexterous%20hand-object%20interactions%20seeded%0Afrom%2070%20hours%20of%20real%20human%20demonstrations%2C%20organized%20across%2021%20fundamental%0Amanipulation%20types%20based%20on%20the%20Cutkosky%20taxonomy.%20Each%20entry%20combines%0Asynchronized%20multi-view%20RGB-D%2C%20high-precision%20mocap%20with%20MANO%20hand%20parameters%2C%0Aand%20per-frame%20contact%20points%20with%20physically%20consistent%20force%20profiles.%20Our%0Areal-to-sim%20pipeline%20uses%20reinforcement%20learning%20to%20train%20policies%20that%20control%0Aan%20actuated%20MANO%20hand%20in%20physics%20simulation%2C%20reproducing%20human%20demonstrations%0Awhile%20discovering%20the%20underlying%20contact%20forces%20that%20generate%20the%20observed%0Aobject%20motion.%20DexCanvas%20is%20the%20first%20manipulation%20dataset%20to%20combine%0Alarge-scale%20real%20demonstrations%2C%20systematic%20skill%20coverage%20based%20on%20established%0Ataxonomies%2C%20and%20physics-validated%20contact%20annotations.%20The%20dataset%20can%0Afacilitate%20research%20in%20robotic%20manipulation%20learning%2C%20contact-rich%20control%2C%20and%0Askill%20transfer%20across%20different%20hand%20morphologies.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2510.15786v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DDexCanvas%253A%2520Bridging%2520Human%2520Demonstrations%2520and%2520Robot%2520Learning%2520for%250A%2520%2520Dexterous%2520Manipulation%26entry.906535625%3DXinyue%2520Xu%2520and%2520Jieqiang%2520Sun%2520and%2520%2520Jing%2520and%2520%2520Dai%2520and%2520Siyuan%2520Chen%2520and%2520Lanjie%2520Ma%2520and%2520Ke%2520Sun%2520and%2520Bin%2520Zhao%2520and%2520Jianbo%2520Yuan%2520and%2520Yiwen%2520Lu%26entry.1292438233%3D%2520%2520We%2520present%2520DexCanvas%252C%2520a%2520large-scale%2520hybrid%2520real-synthetic%2520human%2520manipulation%250Adataset%2520containing%25207%252C000%2520hours%2520of%2520dexterous%2520hand-object%2520interactions%2520seeded%250Afrom%252070%2520hours%2520of%2520real%2520human%2520demonstrations%252C%2520organized%2520across%252021%2520fundamental%250Amanipulation%2520types%2520based%2520on%2520the%2520Cutkosky%2520taxonomy.%2520Each%2520entry%2520combines%250Asynchronized%2520multi-view%2520RGB-D%252C%2520high-precision%2520mocap%2520with%2520MANO%2520hand%2520parameters%252C%250Aand%2520per-frame%2520contact%2520points%2520with%2520physically%2520consistent%2520force%2520profiles.%2520Our%250Areal-to-sim%2520pipeline%2520uses%2520reinforcement%2520learning%2520to%2520train%2520policies%2520that%2520control%250Aan%2520actuated%2520MANO%2520hand%2520in%2520physics%2520simulation%252C%2520reproducing%2520human%2520demonstrations%250Awhile%2520discovering%2520the%2520underlying%2520contact%2520forces%2520that%2520generate%2520the%2520observed%250Aobject%2520motion.%2520DexCanvas%2520is%2520the%2520first%2520manipulation%2520dataset%2520to%2520combine%250Alarge-scale%2520real%2520demonstrations%252C%2520systematic%2520skill%2520coverage%2520based%2520on%2520established%250Ataxonomies%252C%2520and%2520physics-validated%2520contact%2520annotations.%2520The%2520dataset%2520can%250Afacilitate%2520research%2520in%2520robotic%2520manipulation%2520learning%252C%2520contact-rich%2520control%252C%2520and%250Askill%2520transfer%2520across%2520different%2520hand%2520morphologies.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2510.15786v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=DexCanvas%3A%20Bridging%20Human%20Demonstrations%20and%20Robot%20Learning%20for%0A%20%20Dexterous%20Manipulation&entry.906535625=Xinyue%20Xu%20and%20Jieqiang%20Sun%20and%20%20Jing%20and%20%20Dai%20and%20Siyuan%20Chen%20and%20Lanjie%20Ma%20and%20Ke%20Sun%20and%20Bin%20Zhao%20and%20Jianbo%20Yuan%20and%20Yiwen%20Lu&entry.1292438233=%20%20We%20present%20DexCanvas%2C%20a%20large-scale%20hybrid%20real-synthetic%20human%20manipulation%0Adataset%20containing%207%2C000%20hours%20of%20dexterous%20hand-object%20interactions%20seeded%0Afrom%2070%20hours%20of%20real%20human%20demonstrations%2C%20organized%20across%2021%20fundamental%0Amanipulation%20types%20based%20on%20the%20Cutkosky%20taxonomy.%20Each%20entry%20combines%0Asynchronized%20multi-view%20RGB-D%2C%20high-precision%20mocap%20with%20MANO%20hand%20parameters%2C%0Aand%20per-frame%20contact%20points%20with%20physically%20consistent%20force%20profiles.%20Our%0Areal-to-sim%20pipeline%20uses%20reinforcement%20learning%20to%20train%20policies%20that%20control%0Aan%20actuated%20MANO%20hand%20in%20physics%20simulation%2C%20reproducing%20human%20demonstrations%0Awhile%20discovering%20the%20underlying%20contact%20forces%20that%20generate%20the%20observed%0Aobject%20motion.%20DexCanvas%20is%20the%20first%20manipulation%20dataset%20to%20combine%0Alarge-scale%20real%20demonstrations%2C%20systematic%20skill%20coverage%20based%20on%20established%0Ataxonomies%2C%20and%20physics-validated%20contact%20annotations.%20The%20dataset%20can%0Afacilitate%20research%20in%20robotic%20manipulation%20learning%2C%20contact-rich%20control%2C%20and%0Askill%20transfer%20across%20different%20hand%20morphologies.%0A&entry.1838667208=http%3A//arxiv.org/abs/2510.15786v1&entry.124074799=Read"},
{"title": "Freehand 3D Ultrasound Imaging: Sim-in-the-Loop Probe Pose Optimization\n  via Visual Servoing", "author": "Yameng Zhang and Dianye Huang and Max Q. -H. Meng and Nassir Navab and Zhongliang Jiang", "abstract": "  Freehand 3D ultrasound (US) imaging using conventional 2D probes offers\nflexibility and accessibility for diverse clinical applications but faces\nchallenges in accurate probe pose estimation. Traditional methods depend on\ncostly tracking systems, while neural network-based methods struggle with image\nnoise and error accumulation, compromising reconstruction precision. We propose\na cost-effective and versatile solution that leverages lightweight cameras and\nvisual servoing in simulated environments for precise 3D US imaging. These\ncameras capture visual feedback from a textured planar workspace. To counter\nocclusions and lighting issues, we introduce an image restoration method that\nreconstructs occluded regions by matching surrounding texture patterns. For\npose estimation, we develop a simulation-in-the-loop approach, which replicates\nthe system setup in simulation and iteratively minimizes pose errors between\nsimulated and real-world observations. A visual servoing controller refines the\nalignment of camera views, improving translational estimation by optimizing\nimage alignment. Validations on a soft vascular phantom, a 3D-printed conical\nmodel, and a human arm demonstrate the robustness and accuracy of our approach,\nwith Hausdorff distances to the reference reconstructions of 0.359 mm, 1.171\nmm, and 0.858 mm, respectively. These results confirm the method's potential\nfor reliable freehand 3D US reconstruction.\n", "link": "http://arxiv.org/abs/2510.15668v1", "date": "2025-10-17", "relevancy": 2.3437, "topK": [{"title": "CamCtrl3D: Single-Image Scene Exploration with Precise 3D Camera Control", "link": "http://arxiv.org/abs/2501.06006v1", "similarity": 0.5939}, {"title": "Synthesizing Moving People with 3D Control", "link": "http://arxiv.org/abs/2401.10889v2", "similarity": 0.5928}, {"title": "GST: Precise 3D Human Body from a Single Image with Gaussian Splatting\n  Transformers", "link": "http://arxiv.org/abs/2409.04196v1", "similarity": 0.5759}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Freehand%203D%20Ultrasound%20Imaging%3A%20Sim-in-the-Loop%20Probe%20Pose%20Optimization%0A%20%20via%20Visual%20Servoing&body=Title%3A%20Freehand%203D%20Ultrasound%20Imaging%3A%20Sim-in-the-Loop%20Probe%20Pose%20Optimization%0A%20%20via%20Visual%20Servoing%0AAuthor%3A%20Yameng%20Zhang%20and%20Dianye%20Huang%20and%20Max%20Q.%20-H.%20Meng%20and%20Nassir%20Navab%20and%20Zhongliang%20Jiang%0AAbstract%3A%20%20%20Freehand%203D%20ultrasound%20%28US%29%20imaging%20using%20conventional%202D%20probes%20offers%0Aflexibility%20and%20accessibility%20for%20diverse%20clinical%20applications%20but%20faces%0Achallenges%20in%20accurate%20probe%20pose%20estimation.%20Traditional%20methods%20depend%20on%0Acostly%20tracking%20systems%2C%20while%20neural%20network-based%20methods%20struggle%20with%20image%0Anoise%20and%20error%20accumulation%2C%20compromising%20reconstruction%20precision.%20We%20propose%0Aa%20cost-effective%20and%20versatile%20solution%20that%20leverages%20lightweight%20cameras%20and%0Avisual%20servoing%20in%20simulated%20environments%20for%20precise%203D%20US%20imaging.%20These%0Acameras%20capture%20visual%20feedback%20from%20a%20textured%20planar%20workspace.%20To%20counter%0Aocclusions%20and%20lighting%20issues%2C%20we%20introduce%20an%20image%20restoration%20method%20that%0Areconstructs%20occluded%20regions%20by%20matching%20surrounding%20texture%20patterns.%20For%0Apose%20estimation%2C%20we%20develop%20a%20simulation-in-the-loop%20approach%2C%20which%20replicates%0Athe%20system%20setup%20in%20simulation%20and%20iteratively%20minimizes%20pose%20errors%20between%0Asimulated%20and%20real-world%20observations.%20A%20visual%20servoing%20controller%20refines%20the%0Aalignment%20of%20camera%20views%2C%20improving%20translational%20estimation%20by%20optimizing%0Aimage%20alignment.%20Validations%20on%20a%20soft%20vascular%20phantom%2C%20a%203D-printed%20conical%0Amodel%2C%20and%20a%20human%20arm%20demonstrate%20the%20robustness%20and%20accuracy%20of%20our%20approach%2C%0Awith%20Hausdorff%20distances%20to%20the%20reference%20reconstructions%20of%200.359%20mm%2C%201.171%0Amm%2C%20and%200.858%20mm%2C%20respectively.%20These%20results%20confirm%20the%20method%27s%20potential%0Afor%20reliable%20freehand%203D%20US%20reconstruction.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2510.15668v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DFreehand%25203D%2520Ultrasound%2520Imaging%253A%2520Sim-in-the-Loop%2520Probe%2520Pose%2520Optimization%250A%2520%2520via%2520Visual%2520Servoing%26entry.906535625%3DYameng%2520Zhang%2520and%2520Dianye%2520Huang%2520and%2520Max%2520Q.%2520-H.%2520Meng%2520and%2520Nassir%2520Navab%2520and%2520Zhongliang%2520Jiang%26entry.1292438233%3D%2520%2520Freehand%25203D%2520ultrasound%2520%2528US%2529%2520imaging%2520using%2520conventional%25202D%2520probes%2520offers%250Aflexibility%2520and%2520accessibility%2520for%2520diverse%2520clinical%2520applications%2520but%2520faces%250Achallenges%2520in%2520accurate%2520probe%2520pose%2520estimation.%2520Traditional%2520methods%2520depend%2520on%250Acostly%2520tracking%2520systems%252C%2520while%2520neural%2520network-based%2520methods%2520struggle%2520with%2520image%250Anoise%2520and%2520error%2520accumulation%252C%2520compromising%2520reconstruction%2520precision.%2520We%2520propose%250Aa%2520cost-effective%2520and%2520versatile%2520solution%2520that%2520leverages%2520lightweight%2520cameras%2520and%250Avisual%2520servoing%2520in%2520simulated%2520environments%2520for%2520precise%25203D%2520US%2520imaging.%2520These%250Acameras%2520capture%2520visual%2520feedback%2520from%2520a%2520textured%2520planar%2520workspace.%2520To%2520counter%250Aocclusions%2520and%2520lighting%2520issues%252C%2520we%2520introduce%2520an%2520image%2520restoration%2520method%2520that%250Areconstructs%2520occluded%2520regions%2520by%2520matching%2520surrounding%2520texture%2520patterns.%2520For%250Apose%2520estimation%252C%2520we%2520develop%2520a%2520simulation-in-the-loop%2520approach%252C%2520which%2520replicates%250Athe%2520system%2520setup%2520in%2520simulation%2520and%2520iteratively%2520minimizes%2520pose%2520errors%2520between%250Asimulated%2520and%2520real-world%2520observations.%2520A%2520visual%2520servoing%2520controller%2520refines%2520the%250Aalignment%2520of%2520camera%2520views%252C%2520improving%2520translational%2520estimation%2520by%2520optimizing%250Aimage%2520alignment.%2520Validations%2520on%2520a%2520soft%2520vascular%2520phantom%252C%2520a%25203D-printed%2520conical%250Amodel%252C%2520and%2520a%2520human%2520arm%2520demonstrate%2520the%2520robustness%2520and%2520accuracy%2520of%2520our%2520approach%252C%250Awith%2520Hausdorff%2520distances%2520to%2520the%2520reference%2520reconstructions%2520of%25200.359%2520mm%252C%25201.171%250Amm%252C%2520and%25200.858%2520mm%252C%2520respectively.%2520These%2520results%2520confirm%2520the%2520method%2527s%2520potential%250Afor%2520reliable%2520freehand%25203D%2520US%2520reconstruction.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2510.15668v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Freehand%203D%20Ultrasound%20Imaging%3A%20Sim-in-the-Loop%20Probe%20Pose%20Optimization%0A%20%20via%20Visual%20Servoing&entry.906535625=Yameng%20Zhang%20and%20Dianye%20Huang%20and%20Max%20Q.%20-H.%20Meng%20and%20Nassir%20Navab%20and%20Zhongliang%20Jiang&entry.1292438233=%20%20Freehand%203D%20ultrasound%20%28US%29%20imaging%20using%20conventional%202D%20probes%20offers%0Aflexibility%20and%20accessibility%20for%20diverse%20clinical%20applications%20but%20faces%0Achallenges%20in%20accurate%20probe%20pose%20estimation.%20Traditional%20methods%20depend%20on%0Acostly%20tracking%20systems%2C%20while%20neural%20network-based%20methods%20struggle%20with%20image%0Anoise%20and%20error%20accumulation%2C%20compromising%20reconstruction%20precision.%20We%20propose%0Aa%20cost-effective%20and%20versatile%20solution%20that%20leverages%20lightweight%20cameras%20and%0Avisual%20servoing%20in%20simulated%20environments%20for%20precise%203D%20US%20imaging.%20These%0Acameras%20capture%20visual%20feedback%20from%20a%20textured%20planar%20workspace.%20To%20counter%0Aocclusions%20and%20lighting%20issues%2C%20we%20introduce%20an%20image%20restoration%20method%20that%0Areconstructs%20occluded%20regions%20by%20matching%20surrounding%20texture%20patterns.%20For%0Apose%20estimation%2C%20we%20develop%20a%20simulation-in-the-loop%20approach%2C%20which%20replicates%0Athe%20system%20setup%20in%20simulation%20and%20iteratively%20minimizes%20pose%20errors%20between%0Asimulated%20and%20real-world%20observations.%20A%20visual%20servoing%20controller%20refines%20the%0Aalignment%20of%20camera%20views%2C%20improving%20translational%20estimation%20by%20optimizing%0Aimage%20alignment.%20Validations%20on%20a%20soft%20vascular%20phantom%2C%20a%203D-printed%20conical%0Amodel%2C%20and%20a%20human%20arm%20demonstrate%20the%20robustness%20and%20accuracy%20of%20our%20approach%2C%0Awith%20Hausdorff%20distances%20to%20the%20reference%20reconstructions%20of%200.359%20mm%2C%201.171%0Amm%2C%20and%200.858%20mm%2C%20respectively.%20These%20results%20confirm%20the%20method%27s%20potential%0Afor%20reliable%20freehand%203D%20US%20reconstruction.%0A&entry.1838667208=http%3A//arxiv.org/abs/2510.15668v1&entry.124074799=Read"},
{"title": "SEGA: A Stepwise Evolution Paradigm for Content-Aware Layout Generation\n  with Design Prior", "author": "Haoran Wang and Bo Zhao and Jinghui Wang and Hanzhang Wang and Huan Yang and Wei Ji and Hao Liu and Xinyan Xiao", "abstract": "  In this paper, we study the content-aware layout generation problem, which\naims to automatically generate layouts that are harmonious with a given\nbackground image. Existing methods usually deal with this task with a\nsingle-step reasoning framework. The lack of a feedback-based self-correction\nmechanism leads to their failure rates significantly increasing when faced with\ncomplex element layout planning. To address this challenge, we introduce SEGA,\na novel Stepwise Evolution Paradigm for Content-Aware Layout Generation.\nInspired by the systematic mode of human thinking, SEGA employs a hierarchical\nreasoning framework with a coarse-to-fine strategy: first, a coarse-level\nmodule roughly estimates the layout planning results; then, another refining\nmodule performs fine-level reasoning regarding the coarse planning results.\nFurthermore, we incorporate layout design principles as prior knowledge into\nthe model to enhance its layout planning ability. Besides, we present\nGenPoster-100K that is a new large-scale poster dataset with rich\nmeta-information annotation. The experiments demonstrate the effectiveness of\nour approach by achieving the state-of-the-art results on multiple benchmark\ndatasets. Our project page is at: https://brucew91.github.io/SEGA.github.io/\n", "link": "http://arxiv.org/abs/2510.15749v1", "date": "2025-10-17", "relevancy": 2.3415, "topK": [{"title": "Self-supervised Photographic Image Layout Representation Learning", "link": "http://arxiv.org/abs/2403.03740v1", "similarity": 0.6254}, {"title": "DressCode: Autoregressively Sewing and Generating Garments from Text\n  Guidance", "link": "http://arxiv.org/abs/2401.16465v3", "similarity": 0.5656}, {"title": "Thinking Outside the BBox: Unconstrained Generative Object Compositing", "link": "http://arxiv.org/abs/2409.04559v2", "similarity": 0.5346}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20SEGA%3A%20A%20Stepwise%20Evolution%20Paradigm%20for%20Content-Aware%20Layout%20Generation%0A%20%20with%20Design%20Prior&body=Title%3A%20SEGA%3A%20A%20Stepwise%20Evolution%20Paradigm%20for%20Content-Aware%20Layout%20Generation%0A%20%20with%20Design%20Prior%0AAuthor%3A%20Haoran%20Wang%20and%20Bo%20Zhao%20and%20Jinghui%20Wang%20and%20Hanzhang%20Wang%20and%20Huan%20Yang%20and%20Wei%20Ji%20and%20Hao%20Liu%20and%20Xinyan%20Xiao%0AAbstract%3A%20%20%20In%20this%20paper%2C%20we%20study%20the%20content-aware%20layout%20generation%20problem%2C%20which%0Aaims%20to%20automatically%20generate%20layouts%20that%20are%20harmonious%20with%20a%20given%0Abackground%20image.%20Existing%20methods%20usually%20deal%20with%20this%20task%20with%20a%0Asingle-step%20reasoning%20framework.%20The%20lack%20of%20a%20feedback-based%20self-correction%0Amechanism%20leads%20to%20their%20failure%20rates%20significantly%20increasing%20when%20faced%20with%0Acomplex%20element%20layout%20planning.%20To%20address%20this%20challenge%2C%20we%20introduce%20SEGA%2C%0Aa%20novel%20Stepwise%20Evolution%20Paradigm%20for%20Content-Aware%20Layout%20Generation.%0AInspired%20by%20the%20systematic%20mode%20of%20human%20thinking%2C%20SEGA%20employs%20a%20hierarchical%0Areasoning%20framework%20with%20a%20coarse-to-fine%20strategy%3A%20first%2C%20a%20coarse-level%0Amodule%20roughly%20estimates%20the%20layout%20planning%20results%3B%20then%2C%20another%20refining%0Amodule%20performs%20fine-level%20reasoning%20regarding%20the%20coarse%20planning%20results.%0AFurthermore%2C%20we%20incorporate%20layout%20design%20principles%20as%20prior%20knowledge%20into%0Athe%20model%20to%20enhance%20its%20layout%20planning%20ability.%20Besides%2C%20we%20present%0AGenPoster-100K%20that%20is%20a%20new%20large-scale%20poster%20dataset%20with%20rich%0Ameta-information%20annotation.%20The%20experiments%20demonstrate%20the%20effectiveness%20of%0Aour%20approach%20by%20achieving%20the%20state-of-the-art%20results%20on%20multiple%20benchmark%0Adatasets.%20Our%20project%20page%20is%20at%3A%20https%3A//brucew91.github.io/SEGA.github.io/%0A%0ALink%3A%20http%3A//arxiv.org/abs/2510.15749v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DSEGA%253A%2520A%2520Stepwise%2520Evolution%2520Paradigm%2520for%2520Content-Aware%2520Layout%2520Generation%250A%2520%2520with%2520Design%2520Prior%26entry.906535625%3DHaoran%2520Wang%2520and%2520Bo%2520Zhao%2520and%2520Jinghui%2520Wang%2520and%2520Hanzhang%2520Wang%2520and%2520Huan%2520Yang%2520and%2520Wei%2520Ji%2520and%2520Hao%2520Liu%2520and%2520Xinyan%2520Xiao%26entry.1292438233%3D%2520%2520In%2520this%2520paper%252C%2520we%2520study%2520the%2520content-aware%2520layout%2520generation%2520problem%252C%2520which%250Aaims%2520to%2520automatically%2520generate%2520layouts%2520that%2520are%2520harmonious%2520with%2520a%2520given%250Abackground%2520image.%2520Existing%2520methods%2520usually%2520deal%2520with%2520this%2520task%2520with%2520a%250Asingle-step%2520reasoning%2520framework.%2520The%2520lack%2520of%2520a%2520feedback-based%2520self-correction%250Amechanism%2520leads%2520to%2520their%2520failure%2520rates%2520significantly%2520increasing%2520when%2520faced%2520with%250Acomplex%2520element%2520layout%2520planning.%2520To%2520address%2520this%2520challenge%252C%2520we%2520introduce%2520SEGA%252C%250Aa%2520novel%2520Stepwise%2520Evolution%2520Paradigm%2520for%2520Content-Aware%2520Layout%2520Generation.%250AInspired%2520by%2520the%2520systematic%2520mode%2520of%2520human%2520thinking%252C%2520SEGA%2520employs%2520a%2520hierarchical%250Areasoning%2520framework%2520with%2520a%2520coarse-to-fine%2520strategy%253A%2520first%252C%2520a%2520coarse-level%250Amodule%2520roughly%2520estimates%2520the%2520layout%2520planning%2520results%253B%2520then%252C%2520another%2520refining%250Amodule%2520performs%2520fine-level%2520reasoning%2520regarding%2520the%2520coarse%2520planning%2520results.%250AFurthermore%252C%2520we%2520incorporate%2520layout%2520design%2520principles%2520as%2520prior%2520knowledge%2520into%250Athe%2520model%2520to%2520enhance%2520its%2520layout%2520planning%2520ability.%2520Besides%252C%2520we%2520present%250AGenPoster-100K%2520that%2520is%2520a%2520new%2520large-scale%2520poster%2520dataset%2520with%2520rich%250Ameta-information%2520annotation.%2520The%2520experiments%2520demonstrate%2520the%2520effectiveness%2520of%250Aour%2520approach%2520by%2520achieving%2520the%2520state-of-the-art%2520results%2520on%2520multiple%2520benchmark%250Adatasets.%2520Our%2520project%2520page%2520is%2520at%253A%2520https%253A//brucew91.github.io/SEGA.github.io/%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2510.15749v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=SEGA%3A%20A%20Stepwise%20Evolution%20Paradigm%20for%20Content-Aware%20Layout%20Generation%0A%20%20with%20Design%20Prior&entry.906535625=Haoran%20Wang%20and%20Bo%20Zhao%20and%20Jinghui%20Wang%20and%20Hanzhang%20Wang%20and%20Huan%20Yang%20and%20Wei%20Ji%20and%20Hao%20Liu%20and%20Xinyan%20Xiao&entry.1292438233=%20%20In%20this%20paper%2C%20we%20study%20the%20content-aware%20layout%20generation%20problem%2C%20which%0Aaims%20to%20automatically%20generate%20layouts%20that%20are%20harmonious%20with%20a%20given%0Abackground%20image.%20Existing%20methods%20usually%20deal%20with%20this%20task%20with%20a%0Asingle-step%20reasoning%20framework.%20The%20lack%20of%20a%20feedback-based%20self-correction%0Amechanism%20leads%20to%20their%20failure%20rates%20significantly%20increasing%20when%20faced%20with%0Acomplex%20element%20layout%20planning.%20To%20address%20this%20challenge%2C%20we%20introduce%20SEGA%2C%0Aa%20novel%20Stepwise%20Evolution%20Paradigm%20for%20Content-Aware%20Layout%20Generation.%0AInspired%20by%20the%20systematic%20mode%20of%20human%20thinking%2C%20SEGA%20employs%20a%20hierarchical%0Areasoning%20framework%20with%20a%20coarse-to-fine%20strategy%3A%20first%2C%20a%20coarse-level%0Amodule%20roughly%20estimates%20the%20layout%20planning%20results%3B%20then%2C%20another%20refining%0Amodule%20performs%20fine-level%20reasoning%20regarding%20the%20coarse%20planning%20results.%0AFurthermore%2C%20we%20incorporate%20layout%20design%20principles%20as%20prior%20knowledge%20into%0Athe%20model%20to%20enhance%20its%20layout%20planning%20ability.%20Besides%2C%20we%20present%0AGenPoster-100K%20that%20is%20a%20new%20large-scale%20poster%20dataset%20with%20rich%0Ameta-information%20annotation.%20The%20experiments%20demonstrate%20the%20effectiveness%20of%0Aour%20approach%20by%20achieving%20the%20state-of-the-art%20results%20on%20multiple%20benchmark%0Adatasets.%20Our%20project%20page%20is%20at%3A%20https%3A//brucew91.github.io/SEGA.github.io/%0A&entry.1838667208=http%3A//arxiv.org/abs/2510.15749v1&entry.124074799=Read"},
{"title": "Scaling Instruction-Based Video Editing with a High-Quality Synthetic\n  Dataset", "author": "Qingyan Bai and Qiuyu Wang and Hao Ouyang and Yue Yu and Hanlin Wang and Wen Wang and Ka Leong Cheng and Shuailei Ma and Yanhong Zeng and Zichen Liu and Yinghao Xu and Yujun Shen and Qifeng Chen", "abstract": "  Instruction-based video editing promises to democratize content creation, yet\nits progress is severely hampered by the scarcity of large-scale, high-quality\ntraining data. We introduce Ditto, a holistic framework designed to tackle this\nfundamental challenge. At its heart, Ditto features a novel data generation\npipeline that fuses the creative diversity of a leading image editor with an\nin-context video generator, overcoming the limited scope of existing models. To\nmake this process viable, our framework resolves the prohibitive cost-quality\ntrade-off by employing an efficient, distilled model architecture augmented by\na temporal enhancer, which simultaneously reduces computational overhead and\nimproves temporal coherence. Finally, to achieve full scalability, this entire\npipeline is driven by an intelligent agent that crafts diverse instructions and\nrigorously filters the output, ensuring quality control at scale. Using this\nframework, we invested over 12,000 GPU-days to build Ditto-1M, a new dataset of\none million high-fidelity video editing examples. We trained our model, Editto,\non Ditto-1M with a curriculum learning strategy. The results demonstrate\nsuperior instruction-following ability and establish a new state-of-the-art in\ninstruction-based video editing.\n", "link": "http://arxiv.org/abs/2510.15742v1", "date": "2025-10-17", "relevancy": 2.3352, "topK": [{"title": "Customize-A-Video: One-Shot Motion Customization of Text-to-Video\n  Diffusion Models", "link": "http://arxiv.org/abs/2402.14780v1", "similarity": 0.5896}, {"title": "Magic-Me: Identity-Specific Video Customized Diffusion", "link": "http://arxiv.org/abs/2402.09368v1", "similarity": 0.5861}, {"title": "GeneVA: A Dataset of Human Annotations for Generative Text to Video\n  Artifacts", "link": "http://arxiv.org/abs/2509.08818v1", "similarity": 0.5792}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Scaling%20Instruction-Based%20Video%20Editing%20with%20a%20High-Quality%20Synthetic%0A%20%20Dataset&body=Title%3A%20Scaling%20Instruction-Based%20Video%20Editing%20with%20a%20High-Quality%20Synthetic%0A%20%20Dataset%0AAuthor%3A%20Qingyan%20Bai%20and%20Qiuyu%20Wang%20and%20Hao%20Ouyang%20and%20Yue%20Yu%20and%20Hanlin%20Wang%20and%20Wen%20Wang%20and%20Ka%20Leong%20Cheng%20and%20Shuailei%20Ma%20and%20Yanhong%20Zeng%20and%20Zichen%20Liu%20and%20Yinghao%20Xu%20and%20Yujun%20Shen%20and%20Qifeng%20Chen%0AAbstract%3A%20%20%20Instruction-based%20video%20editing%20promises%20to%20democratize%20content%20creation%2C%20yet%0Aits%20progress%20is%20severely%20hampered%20by%20the%20scarcity%20of%20large-scale%2C%20high-quality%0Atraining%20data.%20We%20introduce%20Ditto%2C%20a%20holistic%20framework%20designed%20to%20tackle%20this%0Afundamental%20challenge.%20At%20its%20heart%2C%20Ditto%20features%20a%20novel%20data%20generation%0Apipeline%20that%20fuses%20the%20creative%20diversity%20of%20a%20leading%20image%20editor%20with%20an%0Ain-context%20video%20generator%2C%20overcoming%20the%20limited%20scope%20of%20existing%20models.%20To%0Amake%20this%20process%20viable%2C%20our%20framework%20resolves%20the%20prohibitive%20cost-quality%0Atrade-off%20by%20employing%20an%20efficient%2C%20distilled%20model%20architecture%20augmented%20by%0Aa%20temporal%20enhancer%2C%20which%20simultaneously%20reduces%20computational%20overhead%20and%0Aimproves%20temporal%20coherence.%20Finally%2C%20to%20achieve%20full%20scalability%2C%20this%20entire%0Apipeline%20is%20driven%20by%20an%20intelligent%20agent%20that%20crafts%20diverse%20instructions%20and%0Arigorously%20filters%20the%20output%2C%20ensuring%20quality%20control%20at%20scale.%20Using%20this%0Aframework%2C%20we%20invested%20over%2012%2C000%20GPU-days%20to%20build%20Ditto-1M%2C%20a%20new%20dataset%20of%0Aone%20million%20high-fidelity%20video%20editing%20examples.%20We%20trained%20our%20model%2C%20Editto%2C%0Aon%20Ditto-1M%20with%20a%20curriculum%20learning%20strategy.%20The%20results%20demonstrate%0Asuperior%20instruction-following%20ability%20and%20establish%20a%20new%20state-of-the-art%20in%0Ainstruction-based%20video%20editing.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2510.15742v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DScaling%2520Instruction-Based%2520Video%2520Editing%2520with%2520a%2520High-Quality%2520Synthetic%250A%2520%2520Dataset%26entry.906535625%3DQingyan%2520Bai%2520and%2520Qiuyu%2520Wang%2520and%2520Hao%2520Ouyang%2520and%2520Yue%2520Yu%2520and%2520Hanlin%2520Wang%2520and%2520Wen%2520Wang%2520and%2520Ka%2520Leong%2520Cheng%2520and%2520Shuailei%2520Ma%2520and%2520Yanhong%2520Zeng%2520and%2520Zichen%2520Liu%2520and%2520Yinghao%2520Xu%2520and%2520Yujun%2520Shen%2520and%2520Qifeng%2520Chen%26entry.1292438233%3D%2520%2520Instruction-based%2520video%2520editing%2520promises%2520to%2520democratize%2520content%2520creation%252C%2520yet%250Aits%2520progress%2520is%2520severely%2520hampered%2520by%2520the%2520scarcity%2520of%2520large-scale%252C%2520high-quality%250Atraining%2520data.%2520We%2520introduce%2520Ditto%252C%2520a%2520holistic%2520framework%2520designed%2520to%2520tackle%2520this%250Afundamental%2520challenge.%2520At%2520its%2520heart%252C%2520Ditto%2520features%2520a%2520novel%2520data%2520generation%250Apipeline%2520that%2520fuses%2520the%2520creative%2520diversity%2520of%2520a%2520leading%2520image%2520editor%2520with%2520an%250Ain-context%2520video%2520generator%252C%2520overcoming%2520the%2520limited%2520scope%2520of%2520existing%2520models.%2520To%250Amake%2520this%2520process%2520viable%252C%2520our%2520framework%2520resolves%2520the%2520prohibitive%2520cost-quality%250Atrade-off%2520by%2520employing%2520an%2520efficient%252C%2520distilled%2520model%2520architecture%2520augmented%2520by%250Aa%2520temporal%2520enhancer%252C%2520which%2520simultaneously%2520reduces%2520computational%2520overhead%2520and%250Aimproves%2520temporal%2520coherence.%2520Finally%252C%2520to%2520achieve%2520full%2520scalability%252C%2520this%2520entire%250Apipeline%2520is%2520driven%2520by%2520an%2520intelligent%2520agent%2520that%2520crafts%2520diverse%2520instructions%2520and%250Arigorously%2520filters%2520the%2520output%252C%2520ensuring%2520quality%2520control%2520at%2520scale.%2520Using%2520this%250Aframework%252C%2520we%2520invested%2520over%252012%252C000%2520GPU-days%2520to%2520build%2520Ditto-1M%252C%2520a%2520new%2520dataset%2520of%250Aone%2520million%2520high-fidelity%2520video%2520editing%2520examples.%2520We%2520trained%2520our%2520model%252C%2520Editto%252C%250Aon%2520Ditto-1M%2520with%2520a%2520curriculum%2520learning%2520strategy.%2520The%2520results%2520demonstrate%250Asuperior%2520instruction-following%2520ability%2520and%2520establish%2520a%2520new%2520state-of-the-art%2520in%250Ainstruction-based%2520video%2520editing.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2510.15742v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Scaling%20Instruction-Based%20Video%20Editing%20with%20a%20High-Quality%20Synthetic%0A%20%20Dataset&entry.906535625=Qingyan%20Bai%20and%20Qiuyu%20Wang%20and%20Hao%20Ouyang%20and%20Yue%20Yu%20and%20Hanlin%20Wang%20and%20Wen%20Wang%20and%20Ka%20Leong%20Cheng%20and%20Shuailei%20Ma%20and%20Yanhong%20Zeng%20and%20Zichen%20Liu%20and%20Yinghao%20Xu%20and%20Yujun%20Shen%20and%20Qifeng%20Chen&entry.1292438233=%20%20Instruction-based%20video%20editing%20promises%20to%20democratize%20content%20creation%2C%20yet%0Aits%20progress%20is%20severely%20hampered%20by%20the%20scarcity%20of%20large-scale%2C%20high-quality%0Atraining%20data.%20We%20introduce%20Ditto%2C%20a%20holistic%20framework%20designed%20to%20tackle%20this%0Afundamental%20challenge.%20At%20its%20heart%2C%20Ditto%20features%20a%20novel%20data%20generation%0Apipeline%20that%20fuses%20the%20creative%20diversity%20of%20a%20leading%20image%20editor%20with%20an%0Ain-context%20video%20generator%2C%20overcoming%20the%20limited%20scope%20of%20existing%20models.%20To%0Amake%20this%20process%20viable%2C%20our%20framework%20resolves%20the%20prohibitive%20cost-quality%0Atrade-off%20by%20employing%20an%20efficient%2C%20distilled%20model%20architecture%20augmented%20by%0Aa%20temporal%20enhancer%2C%20which%20simultaneously%20reduces%20computational%20overhead%20and%0Aimproves%20temporal%20coherence.%20Finally%2C%20to%20achieve%20full%20scalability%2C%20this%20entire%0Apipeline%20is%20driven%20by%20an%20intelligent%20agent%20that%20crafts%20diverse%20instructions%20and%0Arigorously%20filters%20the%20output%2C%20ensuring%20quality%20control%20at%20scale.%20Using%20this%0Aframework%2C%20we%20invested%20over%2012%2C000%20GPU-days%20to%20build%20Ditto-1M%2C%20a%20new%20dataset%20of%0Aone%20million%20high-fidelity%20video%20editing%20examples.%20We%20trained%20our%20model%2C%20Editto%2C%0Aon%20Ditto-1M%20with%20a%20curriculum%20learning%20strategy.%20The%20results%20demonstrate%0Asuperior%20instruction-following%20ability%20and%20establish%20a%20new%20state-of-the-art%20in%0Ainstruction-based%20video%20editing.%0A&entry.1838667208=http%3A//arxiv.org/abs/2510.15742v1&entry.124074799=Read"},
{"title": "CLASP: General-Purpose Clothes Manipulation with Semantic Keypoints", "author": "Yuhong Deng and Chao Tang and Cunjun Yu and Linfeng Li and David Hsu", "abstract": "  Clothes manipulation, such as folding or hanging, is a critical capability\nfor home service robots. Despite recent advances, most existing methods remain\nlimited to specific clothes types and tasks, due to the complex,\nhigh-dimensional geometry of clothes. This paper presents CLothes mAnipulation\nwith Semantic keyPoints (CLASP), which aims at general-purpose clothes\nmanipulation over diverse clothes types, T-shirts, shorts, skirts, long\ndresses, ..., as well as different tasks, folding, flattening, hanging, ....\nThe core idea of CLASP is semantic keypoints-e.g., ''left sleeve'' and ''right\nshoulder''-a sparse spatial-semantic representation, salient for both\nperception and action. Semantic keypoints of clothes can be reliably extracted\nfrom RGB-D images and provide an effective representation for a wide range of\nclothes manipulation policies. CLASP uses semantic keypoints as an intermediate\nrepresentation to connect high-level task planning and low-level action\nexecution. At the high level, it exploits vision language models (VLMs) to\npredict task plans over the semantic keypoints. At the low level, it executes\nthe plans with the help of a set of pre-built manipulation skills conditioned\non the keypoints. Extensive simulation experiments show that CLASP outperforms\nstate-of-the-art baseline methods on multiple tasks across diverse clothes\ntypes, demonstrating strong performance and generalization. Further experiments\nwith a Franka dual-arm system on four distinct tasks-folding, flattening,\nhanging, and placing-confirm CLASP's performance on real-life clothes\nmanipulation.\n", "link": "http://arxiv.org/abs/2507.19983v2", "date": "2025-10-17", "relevancy": 2.3237, "topK": [{"title": "DressCode: Autoregressively Sewing and Generating Garments from Text\n  Guidance", "link": "http://arxiv.org/abs/2401.16465v3", "similarity": 0.5993}, {"title": "RoboEXP: Action-Conditioned Scene Graph via Interactive Exploration for\n  Robotic Manipulation", "link": "http://arxiv.org/abs/2402.15487v1", "similarity": 0.5887}, {"title": "FabricDiffusion: High-Fidelity Texture Transfer for 3D Garments\n  Generation from In-The-Wild Clothing Images", "link": "http://arxiv.org/abs/2410.01801v1", "similarity": 0.5594}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20CLASP%3A%20General-Purpose%20Clothes%20Manipulation%20with%20Semantic%20Keypoints&body=Title%3A%20CLASP%3A%20General-Purpose%20Clothes%20Manipulation%20with%20Semantic%20Keypoints%0AAuthor%3A%20Yuhong%20Deng%20and%20Chao%20Tang%20and%20Cunjun%20Yu%20and%20Linfeng%20Li%20and%20David%20Hsu%0AAbstract%3A%20%20%20Clothes%20manipulation%2C%20such%20as%20folding%20or%20hanging%2C%20is%20a%20critical%20capability%0Afor%20home%20service%20robots.%20Despite%20recent%20advances%2C%20most%20existing%20methods%20remain%0Alimited%20to%20specific%20clothes%20types%20and%20tasks%2C%20due%20to%20the%20complex%2C%0Ahigh-dimensional%20geometry%20of%20clothes.%20This%20paper%20presents%20CLothes%20mAnipulation%0Awith%20Semantic%20keyPoints%20%28CLASP%29%2C%20which%20aims%20at%20general-purpose%20clothes%0Amanipulation%20over%20diverse%20clothes%20types%2C%20T-shirts%2C%20shorts%2C%20skirts%2C%20long%0Adresses%2C%20...%2C%20as%20well%20as%20different%20tasks%2C%20folding%2C%20flattening%2C%20hanging%2C%20....%0AThe%20core%20idea%20of%20CLASP%20is%20semantic%20keypoints-e.g.%2C%20%27%27left%20sleeve%27%27%20and%20%27%27right%0Ashoulder%27%27-a%20sparse%20spatial-semantic%20representation%2C%20salient%20for%20both%0Aperception%20and%20action.%20Semantic%20keypoints%20of%20clothes%20can%20be%20reliably%20extracted%0Afrom%20RGB-D%20images%20and%20provide%20an%20effective%20representation%20for%20a%20wide%20range%20of%0Aclothes%20manipulation%20policies.%20CLASP%20uses%20semantic%20keypoints%20as%20an%20intermediate%0Arepresentation%20to%20connect%20high-level%20task%20planning%20and%20low-level%20action%0Aexecution.%20At%20the%20high%20level%2C%20it%20exploits%20vision%20language%20models%20%28VLMs%29%20to%0Apredict%20task%20plans%20over%20the%20semantic%20keypoints.%20At%20the%20low%20level%2C%20it%20executes%0Athe%20plans%20with%20the%20help%20of%20a%20set%20of%20pre-built%20manipulation%20skills%20conditioned%0Aon%20the%20keypoints.%20Extensive%20simulation%20experiments%20show%20that%20CLASP%20outperforms%0Astate-of-the-art%20baseline%20methods%20on%20multiple%20tasks%20across%20diverse%20clothes%0Atypes%2C%20demonstrating%20strong%20performance%20and%20generalization.%20Further%20experiments%0Awith%20a%20Franka%20dual-arm%20system%20on%20four%20distinct%20tasks-folding%2C%20flattening%2C%0Ahanging%2C%20and%20placing-confirm%20CLASP%27s%20performance%20on%20real-life%20clothes%0Amanipulation.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2507.19983v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DCLASP%253A%2520General-Purpose%2520Clothes%2520Manipulation%2520with%2520Semantic%2520Keypoints%26entry.906535625%3DYuhong%2520Deng%2520and%2520Chao%2520Tang%2520and%2520Cunjun%2520Yu%2520and%2520Linfeng%2520Li%2520and%2520David%2520Hsu%26entry.1292438233%3D%2520%2520Clothes%2520manipulation%252C%2520such%2520as%2520folding%2520or%2520hanging%252C%2520is%2520a%2520critical%2520capability%250Afor%2520home%2520service%2520robots.%2520Despite%2520recent%2520advances%252C%2520most%2520existing%2520methods%2520remain%250Alimited%2520to%2520specific%2520clothes%2520types%2520and%2520tasks%252C%2520due%2520to%2520the%2520complex%252C%250Ahigh-dimensional%2520geometry%2520of%2520clothes.%2520This%2520paper%2520presents%2520CLothes%2520mAnipulation%250Awith%2520Semantic%2520keyPoints%2520%2528CLASP%2529%252C%2520which%2520aims%2520at%2520general-purpose%2520clothes%250Amanipulation%2520over%2520diverse%2520clothes%2520types%252C%2520T-shirts%252C%2520shorts%252C%2520skirts%252C%2520long%250Adresses%252C%2520...%252C%2520as%2520well%2520as%2520different%2520tasks%252C%2520folding%252C%2520flattening%252C%2520hanging%252C%2520....%250AThe%2520core%2520idea%2520of%2520CLASP%2520is%2520semantic%2520keypoints-e.g.%252C%2520%2527%2527left%2520sleeve%2527%2527%2520and%2520%2527%2527right%250Ashoulder%2527%2527-a%2520sparse%2520spatial-semantic%2520representation%252C%2520salient%2520for%2520both%250Aperception%2520and%2520action.%2520Semantic%2520keypoints%2520of%2520clothes%2520can%2520be%2520reliably%2520extracted%250Afrom%2520RGB-D%2520images%2520and%2520provide%2520an%2520effective%2520representation%2520for%2520a%2520wide%2520range%2520of%250Aclothes%2520manipulation%2520policies.%2520CLASP%2520uses%2520semantic%2520keypoints%2520as%2520an%2520intermediate%250Arepresentation%2520to%2520connect%2520high-level%2520task%2520planning%2520and%2520low-level%2520action%250Aexecution.%2520At%2520the%2520high%2520level%252C%2520it%2520exploits%2520vision%2520language%2520models%2520%2528VLMs%2529%2520to%250Apredict%2520task%2520plans%2520over%2520the%2520semantic%2520keypoints.%2520At%2520the%2520low%2520level%252C%2520it%2520executes%250Athe%2520plans%2520with%2520the%2520help%2520of%2520a%2520set%2520of%2520pre-built%2520manipulation%2520skills%2520conditioned%250Aon%2520the%2520keypoints.%2520Extensive%2520simulation%2520experiments%2520show%2520that%2520CLASP%2520outperforms%250Astate-of-the-art%2520baseline%2520methods%2520on%2520multiple%2520tasks%2520across%2520diverse%2520clothes%250Atypes%252C%2520demonstrating%2520strong%2520performance%2520and%2520generalization.%2520Further%2520experiments%250Awith%2520a%2520Franka%2520dual-arm%2520system%2520on%2520four%2520distinct%2520tasks-folding%252C%2520flattening%252C%250Ahanging%252C%2520and%2520placing-confirm%2520CLASP%2527s%2520performance%2520on%2520real-life%2520clothes%250Amanipulation.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2507.19983v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=CLASP%3A%20General-Purpose%20Clothes%20Manipulation%20with%20Semantic%20Keypoints&entry.906535625=Yuhong%20Deng%20and%20Chao%20Tang%20and%20Cunjun%20Yu%20and%20Linfeng%20Li%20and%20David%20Hsu&entry.1292438233=%20%20Clothes%20manipulation%2C%20such%20as%20folding%20or%20hanging%2C%20is%20a%20critical%20capability%0Afor%20home%20service%20robots.%20Despite%20recent%20advances%2C%20most%20existing%20methods%20remain%0Alimited%20to%20specific%20clothes%20types%20and%20tasks%2C%20due%20to%20the%20complex%2C%0Ahigh-dimensional%20geometry%20of%20clothes.%20This%20paper%20presents%20CLothes%20mAnipulation%0Awith%20Semantic%20keyPoints%20%28CLASP%29%2C%20which%20aims%20at%20general-purpose%20clothes%0Amanipulation%20over%20diverse%20clothes%20types%2C%20T-shirts%2C%20shorts%2C%20skirts%2C%20long%0Adresses%2C%20...%2C%20as%20well%20as%20different%20tasks%2C%20folding%2C%20flattening%2C%20hanging%2C%20....%0AThe%20core%20idea%20of%20CLASP%20is%20semantic%20keypoints-e.g.%2C%20%27%27left%20sleeve%27%27%20and%20%27%27right%0Ashoulder%27%27-a%20sparse%20spatial-semantic%20representation%2C%20salient%20for%20both%0Aperception%20and%20action.%20Semantic%20keypoints%20of%20clothes%20can%20be%20reliably%20extracted%0Afrom%20RGB-D%20images%20and%20provide%20an%20effective%20representation%20for%20a%20wide%20range%20of%0Aclothes%20manipulation%20policies.%20CLASP%20uses%20semantic%20keypoints%20as%20an%20intermediate%0Arepresentation%20to%20connect%20high-level%20task%20planning%20and%20low-level%20action%0Aexecution.%20At%20the%20high%20level%2C%20it%20exploits%20vision%20language%20models%20%28VLMs%29%20to%0Apredict%20task%20plans%20over%20the%20semantic%20keypoints.%20At%20the%20low%20level%2C%20it%20executes%0Athe%20plans%20with%20the%20help%20of%20a%20set%20of%20pre-built%20manipulation%20skills%20conditioned%0Aon%20the%20keypoints.%20Extensive%20simulation%20experiments%20show%20that%20CLASP%20outperforms%0Astate-of-the-art%20baseline%20methods%20on%20multiple%20tasks%20across%20diverse%20clothes%0Atypes%2C%20demonstrating%20strong%20performance%20and%20generalization.%20Further%20experiments%0Awith%20a%20Franka%20dual-arm%20system%20on%20four%20distinct%20tasks-folding%2C%20flattening%2C%0Ahanging%2C%20and%20placing-confirm%20CLASP%27s%20performance%20on%20real-life%20clothes%0Amanipulation.%0A&entry.1838667208=http%3A//arxiv.org/abs/2507.19983v2&entry.124074799=Read"},
{"title": "Adaptive Legged Locomotion via Online Learning for Model Predictive\n  Control", "author": "Hongyu Zhou and Xiaoyu Zhang and Vasileios Tzoumas", "abstract": "  We provide an algorithm for adaptive legged locomotion via online learning\nand model predictive control. The algorithm is composed of two interacting\nmodules: model predictive control (MPC) and online learning of residual\ndynamics. The residual dynamics can represent modeling errors and external\ndisturbances. We are motivated by the future of autonomy where quadrupeds will\nautonomously perform complex tasks despite real-world unknown uncertainty, such\nas unknown payload and uneven terrains. The algorithm uses random Fourier\nfeatures to approximate the residual dynamics in reproducing kernel Hilbert\nspaces. Then, it employs MPC based on the current learned model of the residual\ndynamics. The model is updated online in a self-supervised manner using least\nsquares based on the data collected while controlling the quadruped. The\nalgorithm enjoys sublinear \\textit{dynamic regret}, defined as the\nsuboptimality against an optimal clairvoyant controller that knows how the\nresidual dynamics. We validate our algorithm in Gazebo and MuJoCo simulations,\nwhere the quadruped aims to track reference trajectories. The Gazebo\nsimulations include constant unknown external forces up to $12\\boldsymbol{g}$,\nwhere $\\boldsymbol{g}$ is the gravity vector, in flat terrain, slope terrain\nwith $20\\degree$ inclination, and rough terrain with $0.25m$ height variation.\nThe MuJoCo simulations include time-varying unknown disturbances with payload\nup to $8~kg$ and time-varying ground friction coefficients in flat terrain.\n", "link": "http://arxiv.org/abs/2510.15626v1", "date": "2025-10-17", "relevancy": 2.3224, "topK": [{"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.6532}, {"title": "Egocentric Computer Vision for Hands-Free Robotic Wheelchair Navigation", "link": "https://link.springer.com/article/10.1007/s10846-023-01807-4", "similarity": 0.5779}, {"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.5543}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Adaptive%20Legged%20Locomotion%20via%20Online%20Learning%20for%20Model%20Predictive%0A%20%20Control&body=Title%3A%20Adaptive%20Legged%20Locomotion%20via%20Online%20Learning%20for%20Model%20Predictive%0A%20%20Control%0AAuthor%3A%20Hongyu%20Zhou%20and%20Xiaoyu%20Zhang%20and%20Vasileios%20Tzoumas%0AAbstract%3A%20%20%20We%20provide%20an%20algorithm%20for%20adaptive%20legged%20locomotion%20via%20online%20learning%0Aand%20model%20predictive%20control.%20The%20algorithm%20is%20composed%20of%20two%20interacting%0Amodules%3A%20model%20predictive%20control%20%28MPC%29%20and%20online%20learning%20of%20residual%0Adynamics.%20The%20residual%20dynamics%20can%20represent%20modeling%20errors%20and%20external%0Adisturbances.%20We%20are%20motivated%20by%20the%20future%20of%20autonomy%20where%20quadrupeds%20will%0Aautonomously%20perform%20complex%20tasks%20despite%20real-world%20unknown%20uncertainty%2C%20such%0Aas%20unknown%20payload%20and%20uneven%20terrains.%20The%20algorithm%20uses%20random%20Fourier%0Afeatures%20to%20approximate%20the%20residual%20dynamics%20in%20reproducing%20kernel%20Hilbert%0Aspaces.%20Then%2C%20it%20employs%20MPC%20based%20on%20the%20current%20learned%20model%20of%20the%20residual%0Adynamics.%20The%20model%20is%20updated%20online%20in%20a%20self-supervised%20manner%20using%20least%0Asquares%20based%20on%20the%20data%20collected%20while%20controlling%20the%20quadruped.%20The%0Aalgorithm%20enjoys%20sublinear%20%5Ctextit%7Bdynamic%20regret%7D%2C%20defined%20as%20the%0Asuboptimality%20against%20an%20optimal%20clairvoyant%20controller%20that%20knows%20how%20the%0Aresidual%20dynamics.%20We%20validate%20our%20algorithm%20in%20Gazebo%20and%20MuJoCo%20simulations%2C%0Awhere%20the%20quadruped%20aims%20to%20track%20reference%20trajectories.%20The%20Gazebo%0Asimulations%20include%20constant%20unknown%20external%20forces%20up%20to%20%2412%5Cboldsymbol%7Bg%7D%24%2C%0Awhere%20%24%5Cboldsymbol%7Bg%7D%24%20is%20the%20gravity%20vector%2C%20in%20flat%20terrain%2C%20slope%20terrain%0Awith%20%2420%5Cdegree%24%20inclination%2C%20and%20rough%20terrain%20with%20%240.25m%24%20height%20variation.%0AThe%20MuJoCo%20simulations%20include%20time-varying%20unknown%20disturbances%20with%20payload%0Aup%20to%20%248~kg%24%20and%20time-varying%20ground%20friction%20coefficients%20in%20flat%20terrain.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2510.15626v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DAdaptive%2520Legged%2520Locomotion%2520via%2520Online%2520Learning%2520for%2520Model%2520Predictive%250A%2520%2520Control%26entry.906535625%3DHongyu%2520Zhou%2520and%2520Xiaoyu%2520Zhang%2520and%2520Vasileios%2520Tzoumas%26entry.1292438233%3D%2520%2520We%2520provide%2520an%2520algorithm%2520for%2520adaptive%2520legged%2520locomotion%2520via%2520online%2520learning%250Aand%2520model%2520predictive%2520control.%2520The%2520algorithm%2520is%2520composed%2520of%2520two%2520interacting%250Amodules%253A%2520model%2520predictive%2520control%2520%2528MPC%2529%2520and%2520online%2520learning%2520of%2520residual%250Adynamics.%2520The%2520residual%2520dynamics%2520can%2520represent%2520modeling%2520errors%2520and%2520external%250Adisturbances.%2520We%2520are%2520motivated%2520by%2520the%2520future%2520of%2520autonomy%2520where%2520quadrupeds%2520will%250Aautonomously%2520perform%2520complex%2520tasks%2520despite%2520real-world%2520unknown%2520uncertainty%252C%2520such%250Aas%2520unknown%2520payload%2520and%2520uneven%2520terrains.%2520The%2520algorithm%2520uses%2520random%2520Fourier%250Afeatures%2520to%2520approximate%2520the%2520residual%2520dynamics%2520in%2520reproducing%2520kernel%2520Hilbert%250Aspaces.%2520Then%252C%2520it%2520employs%2520MPC%2520based%2520on%2520the%2520current%2520learned%2520model%2520of%2520the%2520residual%250Adynamics.%2520The%2520model%2520is%2520updated%2520online%2520in%2520a%2520self-supervised%2520manner%2520using%2520least%250Asquares%2520based%2520on%2520the%2520data%2520collected%2520while%2520controlling%2520the%2520quadruped.%2520The%250Aalgorithm%2520enjoys%2520sublinear%2520%255Ctextit%257Bdynamic%2520regret%257D%252C%2520defined%2520as%2520the%250Asuboptimality%2520against%2520an%2520optimal%2520clairvoyant%2520controller%2520that%2520knows%2520how%2520the%250Aresidual%2520dynamics.%2520We%2520validate%2520our%2520algorithm%2520in%2520Gazebo%2520and%2520MuJoCo%2520simulations%252C%250Awhere%2520the%2520quadruped%2520aims%2520to%2520track%2520reference%2520trajectories.%2520The%2520Gazebo%250Asimulations%2520include%2520constant%2520unknown%2520external%2520forces%2520up%2520to%2520%252412%255Cboldsymbol%257Bg%257D%2524%252C%250Awhere%2520%2524%255Cboldsymbol%257Bg%257D%2524%2520is%2520the%2520gravity%2520vector%252C%2520in%2520flat%2520terrain%252C%2520slope%2520terrain%250Awith%2520%252420%255Cdegree%2524%2520inclination%252C%2520and%2520rough%2520terrain%2520with%2520%25240.25m%2524%2520height%2520variation.%250AThe%2520MuJoCo%2520simulations%2520include%2520time-varying%2520unknown%2520disturbances%2520with%2520payload%250Aup%2520to%2520%25248~kg%2524%2520and%2520time-varying%2520ground%2520friction%2520coefficients%2520in%2520flat%2520terrain.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2510.15626v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Adaptive%20Legged%20Locomotion%20via%20Online%20Learning%20for%20Model%20Predictive%0A%20%20Control&entry.906535625=Hongyu%20Zhou%20and%20Xiaoyu%20Zhang%20and%20Vasileios%20Tzoumas&entry.1292438233=%20%20We%20provide%20an%20algorithm%20for%20adaptive%20legged%20locomotion%20via%20online%20learning%0Aand%20model%20predictive%20control.%20The%20algorithm%20is%20composed%20of%20two%20interacting%0Amodules%3A%20model%20predictive%20control%20%28MPC%29%20and%20online%20learning%20of%20residual%0Adynamics.%20The%20residual%20dynamics%20can%20represent%20modeling%20errors%20and%20external%0Adisturbances.%20We%20are%20motivated%20by%20the%20future%20of%20autonomy%20where%20quadrupeds%20will%0Aautonomously%20perform%20complex%20tasks%20despite%20real-world%20unknown%20uncertainty%2C%20such%0Aas%20unknown%20payload%20and%20uneven%20terrains.%20The%20algorithm%20uses%20random%20Fourier%0Afeatures%20to%20approximate%20the%20residual%20dynamics%20in%20reproducing%20kernel%20Hilbert%0Aspaces.%20Then%2C%20it%20employs%20MPC%20based%20on%20the%20current%20learned%20model%20of%20the%20residual%0Adynamics.%20The%20model%20is%20updated%20online%20in%20a%20self-supervised%20manner%20using%20least%0Asquares%20based%20on%20the%20data%20collected%20while%20controlling%20the%20quadruped.%20The%0Aalgorithm%20enjoys%20sublinear%20%5Ctextit%7Bdynamic%20regret%7D%2C%20defined%20as%20the%0Asuboptimality%20against%20an%20optimal%20clairvoyant%20controller%20that%20knows%20how%20the%0Aresidual%20dynamics.%20We%20validate%20our%20algorithm%20in%20Gazebo%20and%20MuJoCo%20simulations%2C%0Awhere%20the%20quadruped%20aims%20to%20track%20reference%20trajectories.%20The%20Gazebo%0Asimulations%20include%20constant%20unknown%20external%20forces%20up%20to%20%2412%5Cboldsymbol%7Bg%7D%24%2C%0Awhere%20%24%5Cboldsymbol%7Bg%7D%24%20is%20the%20gravity%20vector%2C%20in%20flat%20terrain%2C%20slope%20terrain%0Awith%20%2420%5Cdegree%24%20inclination%2C%20and%20rough%20terrain%20with%20%240.25m%24%20height%20variation.%0AThe%20MuJoCo%20simulations%20include%20time-varying%20unknown%20disturbances%20with%20payload%0Aup%20to%20%248~kg%24%20and%20time-varying%20ground%20friction%20coefficients%20in%20flat%20terrain.%0A&entry.1838667208=http%3A//arxiv.org/abs/2510.15626v1&entry.124074799=Read"},
{"title": "NDM: A Noise-driven Detection and Mitigation Framework against Implicit\n  Sexual Intentions in Text-to-Image Generation", "author": "Yitong Sun and Yao Huang and Ruochen Zhang and Huanran Chen and Shouwei Ruan and Ranjie Duan and Xingxing Wei", "abstract": "  Despite the impressive generative capabilities of text-to-image (T2I)\ndiffusion models, they remain vulnerable to generating inappropriate content,\nespecially when confronted with implicit sexual prompts. Unlike explicit\nharmful prompts, these subtle cues, often disguised as seemingly benign terms,\ncan unexpectedly trigger sexual content due to underlying model biases, raising\nsignificant ethical concerns. However, existing detection methods are primarily\ndesigned to identify explicit sexual content and therefore struggle to detect\nthese implicit cues. Fine-tuning approaches, while effective to some extent,\nrisk degrading the model's generative quality, creating an undesirable\ntrade-off. To address this, we propose NDM, the first noise-driven detection\nand mitigation framework, which could detect and mitigate implicit malicious\nintention in T2I generation while preserving the model's original generative\ncapabilities. Specifically, we introduce two key innovations: first, we\nleverage the separability of early-stage predicted noise to develop a\nnoise-based detection method that could identify malicious content with high\naccuracy and efficiency; second, we propose a noise-enhanced adaptive negative\nguidance mechanism that could optimize the initial noise by suppressing the\nprominent region's attention, thereby enhancing the effectiveness of adaptive\nnegative guidance for sexual mitigation. Experimentally, we validate NDM on\nboth natural and adversarial datasets, demonstrating its superior performance\nover existing SOTA methods, including SLD, UCE, and RECE, etc. Code and\nresources are available at https://github.com/lorraine021/NDM.\n", "link": "http://arxiv.org/abs/2510.15752v1", "date": "2025-10-17", "relevancy": 2.3134, "topK": [{"title": "Magic-Me: Identity-Specific Video Customized Diffusion", "link": "http://arxiv.org/abs/2402.09368v1", "similarity": 0.5859}, {"title": "Tuning-Free Noise Rectification for High Fidelity Image-to-Video\n  Generation", "link": "http://arxiv.org/abs/2403.02827v1", "similarity": 0.5813}, {"title": "Multimodal-Conditioned Latent Diffusion Models for Fashion Image Editing", "link": "http://arxiv.org/abs/2403.14828v2", "similarity": 0.5521}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20NDM%3A%20A%20Noise-driven%20Detection%20and%20Mitigation%20Framework%20against%20Implicit%0A%20%20Sexual%20Intentions%20in%20Text-to-Image%20Generation&body=Title%3A%20NDM%3A%20A%20Noise-driven%20Detection%20and%20Mitigation%20Framework%20against%20Implicit%0A%20%20Sexual%20Intentions%20in%20Text-to-Image%20Generation%0AAuthor%3A%20Yitong%20Sun%20and%20Yao%20Huang%20and%20Ruochen%20Zhang%20and%20Huanran%20Chen%20and%20Shouwei%20Ruan%20and%20Ranjie%20Duan%20and%20Xingxing%20Wei%0AAbstract%3A%20%20%20Despite%20the%20impressive%20generative%20capabilities%20of%20text-to-image%20%28T2I%29%0Adiffusion%20models%2C%20they%20remain%20vulnerable%20to%20generating%20inappropriate%20content%2C%0Aespecially%20when%20confronted%20with%20implicit%20sexual%20prompts.%20Unlike%20explicit%0Aharmful%20prompts%2C%20these%20subtle%20cues%2C%20often%20disguised%20as%20seemingly%20benign%20terms%2C%0Acan%20unexpectedly%20trigger%20sexual%20content%20due%20to%20underlying%20model%20biases%2C%20raising%0Asignificant%20ethical%20concerns.%20However%2C%20existing%20detection%20methods%20are%20primarily%0Adesigned%20to%20identify%20explicit%20sexual%20content%20and%20therefore%20struggle%20to%20detect%0Athese%20implicit%20cues.%20Fine-tuning%20approaches%2C%20while%20effective%20to%20some%20extent%2C%0Arisk%20degrading%20the%20model%27s%20generative%20quality%2C%20creating%20an%20undesirable%0Atrade-off.%20To%20address%20this%2C%20we%20propose%20NDM%2C%20the%20first%20noise-driven%20detection%0Aand%20mitigation%20framework%2C%20which%20could%20detect%20and%20mitigate%20implicit%20malicious%0Aintention%20in%20T2I%20generation%20while%20preserving%20the%20model%27s%20original%20generative%0Acapabilities.%20Specifically%2C%20we%20introduce%20two%20key%20innovations%3A%20first%2C%20we%0Aleverage%20the%20separability%20of%20early-stage%20predicted%20noise%20to%20develop%20a%0Anoise-based%20detection%20method%20that%20could%20identify%20malicious%20content%20with%20high%0Aaccuracy%20and%20efficiency%3B%20second%2C%20we%20propose%20a%20noise-enhanced%20adaptive%20negative%0Aguidance%20mechanism%20that%20could%20optimize%20the%20initial%20noise%20by%20suppressing%20the%0Aprominent%20region%27s%20attention%2C%20thereby%20enhancing%20the%20effectiveness%20of%20adaptive%0Anegative%20guidance%20for%20sexual%20mitigation.%20Experimentally%2C%20we%20validate%20NDM%20on%0Aboth%20natural%20and%20adversarial%20datasets%2C%20demonstrating%20its%20superior%20performance%0Aover%20existing%20SOTA%20methods%2C%20including%20SLD%2C%20UCE%2C%20and%20RECE%2C%20etc.%20Code%20and%0Aresources%20are%20available%20at%20https%3A//github.com/lorraine021/NDM.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2510.15752v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DNDM%253A%2520A%2520Noise-driven%2520Detection%2520and%2520Mitigation%2520Framework%2520against%2520Implicit%250A%2520%2520Sexual%2520Intentions%2520in%2520Text-to-Image%2520Generation%26entry.906535625%3DYitong%2520Sun%2520and%2520Yao%2520Huang%2520and%2520Ruochen%2520Zhang%2520and%2520Huanran%2520Chen%2520and%2520Shouwei%2520Ruan%2520and%2520Ranjie%2520Duan%2520and%2520Xingxing%2520Wei%26entry.1292438233%3D%2520%2520Despite%2520the%2520impressive%2520generative%2520capabilities%2520of%2520text-to-image%2520%2528T2I%2529%250Adiffusion%2520models%252C%2520they%2520remain%2520vulnerable%2520to%2520generating%2520inappropriate%2520content%252C%250Aespecially%2520when%2520confronted%2520with%2520implicit%2520sexual%2520prompts.%2520Unlike%2520explicit%250Aharmful%2520prompts%252C%2520these%2520subtle%2520cues%252C%2520often%2520disguised%2520as%2520seemingly%2520benign%2520terms%252C%250Acan%2520unexpectedly%2520trigger%2520sexual%2520content%2520due%2520to%2520underlying%2520model%2520biases%252C%2520raising%250Asignificant%2520ethical%2520concerns.%2520However%252C%2520existing%2520detection%2520methods%2520are%2520primarily%250Adesigned%2520to%2520identify%2520explicit%2520sexual%2520content%2520and%2520therefore%2520struggle%2520to%2520detect%250Athese%2520implicit%2520cues.%2520Fine-tuning%2520approaches%252C%2520while%2520effective%2520to%2520some%2520extent%252C%250Arisk%2520degrading%2520the%2520model%2527s%2520generative%2520quality%252C%2520creating%2520an%2520undesirable%250Atrade-off.%2520To%2520address%2520this%252C%2520we%2520propose%2520NDM%252C%2520the%2520first%2520noise-driven%2520detection%250Aand%2520mitigation%2520framework%252C%2520which%2520could%2520detect%2520and%2520mitigate%2520implicit%2520malicious%250Aintention%2520in%2520T2I%2520generation%2520while%2520preserving%2520the%2520model%2527s%2520original%2520generative%250Acapabilities.%2520Specifically%252C%2520we%2520introduce%2520two%2520key%2520innovations%253A%2520first%252C%2520we%250Aleverage%2520the%2520separability%2520of%2520early-stage%2520predicted%2520noise%2520to%2520develop%2520a%250Anoise-based%2520detection%2520method%2520that%2520could%2520identify%2520malicious%2520content%2520with%2520high%250Aaccuracy%2520and%2520efficiency%253B%2520second%252C%2520we%2520propose%2520a%2520noise-enhanced%2520adaptive%2520negative%250Aguidance%2520mechanism%2520that%2520could%2520optimize%2520the%2520initial%2520noise%2520by%2520suppressing%2520the%250Aprominent%2520region%2527s%2520attention%252C%2520thereby%2520enhancing%2520the%2520effectiveness%2520of%2520adaptive%250Anegative%2520guidance%2520for%2520sexual%2520mitigation.%2520Experimentally%252C%2520we%2520validate%2520NDM%2520on%250Aboth%2520natural%2520and%2520adversarial%2520datasets%252C%2520demonstrating%2520its%2520superior%2520performance%250Aover%2520existing%2520SOTA%2520methods%252C%2520including%2520SLD%252C%2520UCE%252C%2520and%2520RECE%252C%2520etc.%2520Code%2520and%250Aresources%2520are%2520available%2520at%2520https%253A//github.com/lorraine021/NDM.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2510.15752v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=NDM%3A%20A%20Noise-driven%20Detection%20and%20Mitigation%20Framework%20against%20Implicit%0A%20%20Sexual%20Intentions%20in%20Text-to-Image%20Generation&entry.906535625=Yitong%20Sun%20and%20Yao%20Huang%20and%20Ruochen%20Zhang%20and%20Huanran%20Chen%20and%20Shouwei%20Ruan%20and%20Ranjie%20Duan%20and%20Xingxing%20Wei&entry.1292438233=%20%20Despite%20the%20impressive%20generative%20capabilities%20of%20text-to-image%20%28T2I%29%0Adiffusion%20models%2C%20they%20remain%20vulnerable%20to%20generating%20inappropriate%20content%2C%0Aespecially%20when%20confronted%20with%20implicit%20sexual%20prompts.%20Unlike%20explicit%0Aharmful%20prompts%2C%20these%20subtle%20cues%2C%20often%20disguised%20as%20seemingly%20benign%20terms%2C%0Acan%20unexpectedly%20trigger%20sexual%20content%20due%20to%20underlying%20model%20biases%2C%20raising%0Asignificant%20ethical%20concerns.%20However%2C%20existing%20detection%20methods%20are%20primarily%0Adesigned%20to%20identify%20explicit%20sexual%20content%20and%20therefore%20struggle%20to%20detect%0Athese%20implicit%20cues.%20Fine-tuning%20approaches%2C%20while%20effective%20to%20some%20extent%2C%0Arisk%20degrading%20the%20model%27s%20generative%20quality%2C%20creating%20an%20undesirable%0Atrade-off.%20To%20address%20this%2C%20we%20propose%20NDM%2C%20the%20first%20noise-driven%20detection%0Aand%20mitigation%20framework%2C%20which%20could%20detect%20and%20mitigate%20implicit%20malicious%0Aintention%20in%20T2I%20generation%20while%20preserving%20the%20model%27s%20original%20generative%0Acapabilities.%20Specifically%2C%20we%20introduce%20two%20key%20innovations%3A%20first%2C%20we%0Aleverage%20the%20separability%20of%20early-stage%20predicted%20noise%20to%20develop%20a%0Anoise-based%20detection%20method%20that%20could%20identify%20malicious%20content%20with%20high%0Aaccuracy%20and%20efficiency%3B%20second%2C%20we%20propose%20a%20noise-enhanced%20adaptive%20negative%0Aguidance%20mechanism%20that%20could%20optimize%20the%20initial%20noise%20by%20suppressing%20the%0Aprominent%20region%27s%20attention%2C%20thereby%20enhancing%20the%20effectiveness%20of%20adaptive%0Anegative%20guidance%20for%20sexual%20mitigation.%20Experimentally%2C%20we%20validate%20NDM%20on%0Aboth%20natural%20and%20adversarial%20datasets%2C%20demonstrating%20its%20superior%20performance%0Aover%20existing%20SOTA%20methods%2C%20including%20SLD%2C%20UCE%2C%20and%20RECE%2C%20etc.%20Code%20and%0Aresources%20are%20available%20at%20https%3A//github.com/lorraine021/NDM.%0A&entry.1838667208=http%3A//arxiv.org/abs/2510.15752v1&entry.124074799=Read"},
{"title": "Uncertainty-Aware Extreme Point Tracing for Weakly Supervised Ultrasound\n  Image Segmentation", "author": "Lei Shi and Gang Li and Junxing Zhang", "abstract": "  Automatic medical image segmentation is a fundamental step in computer-aided\ndiagnosis, yet fully supervised approaches demand extensive pixel-level\nannotations that are costly and time-consuming. To alleviate this burden, we\npropose a weakly supervised segmentation framework that leverages only four\nextreme points as annotation. Specifically, bounding boxes derived from the\nextreme points are used as prompts for the Segment Anything Model 2 (SAM2) to\ngenerate reliable initial pseudo labels. These pseudo labels are progressively\nrefined by an enhanced Feature-Guided Extreme Point Masking (FGEPM) algorithm,\nwhich incorporates Monte Carlo dropout-based uncertainty estimation to\nconstruct a unified gradient uncertainty cost map for boundary tracing.\nFurthermore, a dual-branch Uncertainty-aware Scale Consistency (USC) loss and a\nbox alignment loss are introduced to ensure spatial consistency and precise\nboundary alignment during training. Extensive experiments on two public\nultrasound datasets, BUSI and UNS, demonstrate that our method achieves\nperformance comparable to, and even surpassing fully supervised counterparts\nwhile significantly reducing annotation cost. These results validate the\neffectiveness and practicality of the proposed weakly supervised framework for\nultrasound image segmentation.\n", "link": "http://arxiv.org/abs/2510.15666v1", "date": "2025-10-17", "relevancy": 2.3087, "topK": [{"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.6286}, {"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.6048}, {"title": "GST: Precise 3D Human Body from a Single Image with Gaussian Splatting\n  Transformers", "link": "http://arxiv.org/abs/2409.04196v1", "similarity": 0.529}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Uncertainty-Aware%20Extreme%20Point%20Tracing%20for%20Weakly%20Supervised%20Ultrasound%0A%20%20Image%20Segmentation&body=Title%3A%20Uncertainty-Aware%20Extreme%20Point%20Tracing%20for%20Weakly%20Supervised%20Ultrasound%0A%20%20Image%20Segmentation%0AAuthor%3A%20Lei%20Shi%20and%20Gang%20Li%20and%20Junxing%20Zhang%0AAbstract%3A%20%20%20Automatic%20medical%20image%20segmentation%20is%20a%20fundamental%20step%20in%20computer-aided%0Adiagnosis%2C%20yet%20fully%20supervised%20approaches%20demand%20extensive%20pixel-level%0Aannotations%20that%20are%20costly%20and%20time-consuming.%20To%20alleviate%20this%20burden%2C%20we%0Apropose%20a%20weakly%20supervised%20segmentation%20framework%20that%20leverages%20only%20four%0Aextreme%20points%20as%20annotation.%20Specifically%2C%20bounding%20boxes%20derived%20from%20the%0Aextreme%20points%20are%20used%20as%20prompts%20for%20the%20Segment%20Anything%20Model%202%20%28SAM2%29%20to%0Agenerate%20reliable%20initial%20pseudo%20labels.%20These%20pseudo%20labels%20are%20progressively%0Arefined%20by%20an%20enhanced%20Feature-Guided%20Extreme%20Point%20Masking%20%28FGEPM%29%20algorithm%2C%0Awhich%20incorporates%20Monte%20Carlo%20dropout-based%20uncertainty%20estimation%20to%0Aconstruct%20a%20unified%20gradient%20uncertainty%20cost%20map%20for%20boundary%20tracing.%0AFurthermore%2C%20a%20dual-branch%20Uncertainty-aware%20Scale%20Consistency%20%28USC%29%20loss%20and%20a%0Abox%20alignment%20loss%20are%20introduced%20to%20ensure%20spatial%20consistency%20and%20precise%0Aboundary%20alignment%20during%20training.%20Extensive%20experiments%20on%20two%20public%0Aultrasound%20datasets%2C%20BUSI%20and%20UNS%2C%20demonstrate%20that%20our%20method%20achieves%0Aperformance%20comparable%20to%2C%20and%20even%20surpassing%20fully%20supervised%20counterparts%0Awhile%20significantly%20reducing%20annotation%20cost.%20These%20results%20validate%20the%0Aeffectiveness%20and%20practicality%20of%20the%20proposed%20weakly%20supervised%20framework%20for%0Aultrasound%20image%20segmentation.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2510.15666v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DUncertainty-Aware%2520Extreme%2520Point%2520Tracing%2520for%2520Weakly%2520Supervised%2520Ultrasound%250A%2520%2520Image%2520Segmentation%26entry.906535625%3DLei%2520Shi%2520and%2520Gang%2520Li%2520and%2520Junxing%2520Zhang%26entry.1292438233%3D%2520%2520Automatic%2520medical%2520image%2520segmentation%2520is%2520a%2520fundamental%2520step%2520in%2520computer-aided%250Adiagnosis%252C%2520yet%2520fully%2520supervised%2520approaches%2520demand%2520extensive%2520pixel-level%250Aannotations%2520that%2520are%2520costly%2520and%2520time-consuming.%2520To%2520alleviate%2520this%2520burden%252C%2520we%250Apropose%2520a%2520weakly%2520supervised%2520segmentation%2520framework%2520that%2520leverages%2520only%2520four%250Aextreme%2520points%2520as%2520annotation.%2520Specifically%252C%2520bounding%2520boxes%2520derived%2520from%2520the%250Aextreme%2520points%2520are%2520used%2520as%2520prompts%2520for%2520the%2520Segment%2520Anything%2520Model%25202%2520%2528SAM2%2529%2520to%250Agenerate%2520reliable%2520initial%2520pseudo%2520labels.%2520These%2520pseudo%2520labels%2520are%2520progressively%250Arefined%2520by%2520an%2520enhanced%2520Feature-Guided%2520Extreme%2520Point%2520Masking%2520%2528FGEPM%2529%2520algorithm%252C%250Awhich%2520incorporates%2520Monte%2520Carlo%2520dropout-based%2520uncertainty%2520estimation%2520to%250Aconstruct%2520a%2520unified%2520gradient%2520uncertainty%2520cost%2520map%2520for%2520boundary%2520tracing.%250AFurthermore%252C%2520a%2520dual-branch%2520Uncertainty-aware%2520Scale%2520Consistency%2520%2528USC%2529%2520loss%2520and%2520a%250Abox%2520alignment%2520loss%2520are%2520introduced%2520to%2520ensure%2520spatial%2520consistency%2520and%2520precise%250Aboundary%2520alignment%2520during%2520training.%2520Extensive%2520experiments%2520on%2520two%2520public%250Aultrasound%2520datasets%252C%2520BUSI%2520and%2520UNS%252C%2520demonstrate%2520that%2520our%2520method%2520achieves%250Aperformance%2520comparable%2520to%252C%2520and%2520even%2520surpassing%2520fully%2520supervised%2520counterparts%250Awhile%2520significantly%2520reducing%2520annotation%2520cost.%2520These%2520results%2520validate%2520the%250Aeffectiveness%2520and%2520practicality%2520of%2520the%2520proposed%2520weakly%2520supervised%2520framework%2520for%250Aultrasound%2520image%2520segmentation.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2510.15666v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Uncertainty-Aware%20Extreme%20Point%20Tracing%20for%20Weakly%20Supervised%20Ultrasound%0A%20%20Image%20Segmentation&entry.906535625=Lei%20Shi%20and%20Gang%20Li%20and%20Junxing%20Zhang&entry.1292438233=%20%20Automatic%20medical%20image%20segmentation%20is%20a%20fundamental%20step%20in%20computer-aided%0Adiagnosis%2C%20yet%20fully%20supervised%20approaches%20demand%20extensive%20pixel-level%0Aannotations%20that%20are%20costly%20and%20time-consuming.%20To%20alleviate%20this%20burden%2C%20we%0Apropose%20a%20weakly%20supervised%20segmentation%20framework%20that%20leverages%20only%20four%0Aextreme%20points%20as%20annotation.%20Specifically%2C%20bounding%20boxes%20derived%20from%20the%0Aextreme%20points%20are%20used%20as%20prompts%20for%20the%20Segment%20Anything%20Model%202%20%28SAM2%29%20to%0Agenerate%20reliable%20initial%20pseudo%20labels.%20These%20pseudo%20labels%20are%20progressively%0Arefined%20by%20an%20enhanced%20Feature-Guided%20Extreme%20Point%20Masking%20%28FGEPM%29%20algorithm%2C%0Awhich%20incorporates%20Monte%20Carlo%20dropout-based%20uncertainty%20estimation%20to%0Aconstruct%20a%20unified%20gradient%20uncertainty%20cost%20map%20for%20boundary%20tracing.%0AFurthermore%2C%20a%20dual-branch%20Uncertainty-aware%20Scale%20Consistency%20%28USC%29%20loss%20and%20a%0Abox%20alignment%20loss%20are%20introduced%20to%20ensure%20spatial%20consistency%20and%20precise%0Aboundary%20alignment%20during%20training.%20Extensive%20experiments%20on%20two%20public%0Aultrasound%20datasets%2C%20BUSI%20and%20UNS%2C%20demonstrate%20that%20our%20method%20achieves%0Aperformance%20comparable%20to%2C%20and%20even%20surpassing%20fully%20supervised%20counterparts%0Awhile%20significantly%20reducing%20annotation%20cost.%20These%20results%20validate%20the%0Aeffectiveness%20and%20practicality%20of%20the%20proposed%20weakly%20supervised%20framework%20for%0Aultrasound%20image%20segmentation.%0A&entry.1838667208=http%3A//arxiv.org/abs/2510.15666v1&entry.124074799=Read"},
{"title": "Lightweight Data-Free Denoising for Detail-Preserving Biomedical Image\n  Restoration", "author": "Tom\u00e1\u0161 Chobola and Julia A. Schnabel and Tingying Peng", "abstract": "  Current self-supervised denoising techniques achieve impressive results, yet\ntheir real-world application is frequently constrained by substantial\ncomputational and memory demands, necessitating a compromise between inference\nspeed and reconstruction quality. In this paper, we present an\nultra-lightweight model that addresses this challenge, achieving both fast\ndenoising and high quality image restoration. Built upon the Noise2Noise\ntraining framework-which removes the reliance on clean reference images or\nexplicit noise modeling-we introduce an innovative multistage denoising\npipeline named Noise2Detail (N2D). During inference, this approach disrupts the\nspatial correlations of noise patterns to produce intermediate smooth\nstructures, which are subsequently refined to recapture fine details directly\nfrom the noisy input. Extensive testing reveals that Noise2Detail surpasses\nexisting dataset-free techniques in performance, while requiring only a\nfraction of the computational resources. This combination of efficiency, low\ncomputational cost, and data-free approach make it a valuable tool for\nbiomedical imaging, overcoming the challenges of scarce clean training data-due\nto rare and complex imaging modalities-while enabling fast inference for\npractical use.\n", "link": "http://arxiv.org/abs/2510.15611v1", "date": "2025-10-17", "relevancy": 2.3037, "topK": [{"title": "Tuning-Free Noise Rectification for High Fidelity Image-to-Video\n  Generation", "link": "http://arxiv.org/abs/2403.02827v1", "similarity": 0.6279}, {"title": "SDXS: Real-Time One-Step Latent Diffusion Models with Image Conditions", "link": "http://arxiv.org/abs/2403.16627v1", "similarity": 0.5416}, {"title": "ToDo: Token Downsampling for Efficient Generation of High-Resolution\n  Images", "link": "http://arxiv.org/abs/2402.13573v2", "similarity": 0.5377}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Lightweight%20Data-Free%20Denoising%20for%20Detail-Preserving%20Biomedical%20Image%0A%20%20Restoration&body=Title%3A%20Lightweight%20Data-Free%20Denoising%20for%20Detail-Preserving%20Biomedical%20Image%0A%20%20Restoration%0AAuthor%3A%20Tom%C3%A1%C5%A1%20Chobola%20and%20Julia%20A.%20Schnabel%20and%20Tingying%20Peng%0AAbstract%3A%20%20%20Current%20self-supervised%20denoising%20techniques%20achieve%20impressive%20results%2C%20yet%0Atheir%20real-world%20application%20is%20frequently%20constrained%20by%20substantial%0Acomputational%20and%20memory%20demands%2C%20necessitating%20a%20compromise%20between%20inference%0Aspeed%20and%20reconstruction%20quality.%20In%20this%20paper%2C%20we%20present%20an%0Aultra-lightweight%20model%20that%20addresses%20this%20challenge%2C%20achieving%20both%20fast%0Adenoising%20and%20high%20quality%20image%20restoration.%20Built%20upon%20the%20Noise2Noise%0Atraining%20framework-which%20removes%20the%20reliance%20on%20clean%20reference%20images%20or%0Aexplicit%20noise%20modeling-we%20introduce%20an%20innovative%20multistage%20denoising%0Apipeline%20named%20Noise2Detail%20%28N2D%29.%20During%20inference%2C%20this%20approach%20disrupts%20the%0Aspatial%20correlations%20of%20noise%20patterns%20to%20produce%20intermediate%20smooth%0Astructures%2C%20which%20are%20subsequently%20refined%20to%20recapture%20fine%20details%20directly%0Afrom%20the%20noisy%20input.%20Extensive%20testing%20reveals%20that%20Noise2Detail%20surpasses%0Aexisting%20dataset-free%20techniques%20in%20performance%2C%20while%20requiring%20only%20a%0Afraction%20of%20the%20computational%20resources.%20This%20combination%20of%20efficiency%2C%20low%0Acomputational%20cost%2C%20and%20data-free%20approach%20make%20it%20a%20valuable%20tool%20for%0Abiomedical%20imaging%2C%20overcoming%20the%20challenges%20of%20scarce%20clean%20training%20data-due%0Ato%20rare%20and%20complex%20imaging%20modalities-while%20enabling%20fast%20inference%20for%0Apractical%20use.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2510.15611v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DLightweight%2520Data-Free%2520Denoising%2520for%2520Detail-Preserving%2520Biomedical%2520Image%250A%2520%2520Restoration%26entry.906535625%3DTom%25C3%25A1%25C5%25A1%2520Chobola%2520and%2520Julia%2520A.%2520Schnabel%2520and%2520Tingying%2520Peng%26entry.1292438233%3D%2520%2520Current%2520self-supervised%2520denoising%2520techniques%2520achieve%2520impressive%2520results%252C%2520yet%250Atheir%2520real-world%2520application%2520is%2520frequently%2520constrained%2520by%2520substantial%250Acomputational%2520and%2520memory%2520demands%252C%2520necessitating%2520a%2520compromise%2520between%2520inference%250Aspeed%2520and%2520reconstruction%2520quality.%2520In%2520this%2520paper%252C%2520we%2520present%2520an%250Aultra-lightweight%2520model%2520that%2520addresses%2520this%2520challenge%252C%2520achieving%2520both%2520fast%250Adenoising%2520and%2520high%2520quality%2520image%2520restoration.%2520Built%2520upon%2520the%2520Noise2Noise%250Atraining%2520framework-which%2520removes%2520the%2520reliance%2520on%2520clean%2520reference%2520images%2520or%250Aexplicit%2520noise%2520modeling-we%2520introduce%2520an%2520innovative%2520multistage%2520denoising%250Apipeline%2520named%2520Noise2Detail%2520%2528N2D%2529.%2520During%2520inference%252C%2520this%2520approach%2520disrupts%2520the%250Aspatial%2520correlations%2520of%2520noise%2520patterns%2520to%2520produce%2520intermediate%2520smooth%250Astructures%252C%2520which%2520are%2520subsequently%2520refined%2520to%2520recapture%2520fine%2520details%2520directly%250Afrom%2520the%2520noisy%2520input.%2520Extensive%2520testing%2520reveals%2520that%2520Noise2Detail%2520surpasses%250Aexisting%2520dataset-free%2520techniques%2520in%2520performance%252C%2520while%2520requiring%2520only%2520a%250Afraction%2520of%2520the%2520computational%2520resources.%2520This%2520combination%2520of%2520efficiency%252C%2520low%250Acomputational%2520cost%252C%2520and%2520data-free%2520approach%2520make%2520it%2520a%2520valuable%2520tool%2520for%250Abiomedical%2520imaging%252C%2520overcoming%2520the%2520challenges%2520of%2520scarce%2520clean%2520training%2520data-due%250Ato%2520rare%2520and%2520complex%2520imaging%2520modalities-while%2520enabling%2520fast%2520inference%2520for%250Apractical%2520use.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2510.15611v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Lightweight%20Data-Free%20Denoising%20for%20Detail-Preserving%20Biomedical%20Image%0A%20%20Restoration&entry.906535625=Tom%C3%A1%C5%A1%20Chobola%20and%20Julia%20A.%20Schnabel%20and%20Tingying%20Peng&entry.1292438233=%20%20Current%20self-supervised%20denoising%20techniques%20achieve%20impressive%20results%2C%20yet%0Atheir%20real-world%20application%20is%20frequently%20constrained%20by%20substantial%0Acomputational%20and%20memory%20demands%2C%20necessitating%20a%20compromise%20between%20inference%0Aspeed%20and%20reconstruction%20quality.%20In%20this%20paper%2C%20we%20present%20an%0Aultra-lightweight%20model%20that%20addresses%20this%20challenge%2C%20achieving%20both%20fast%0Adenoising%20and%20high%20quality%20image%20restoration.%20Built%20upon%20the%20Noise2Noise%0Atraining%20framework-which%20removes%20the%20reliance%20on%20clean%20reference%20images%20or%0Aexplicit%20noise%20modeling-we%20introduce%20an%20innovative%20multistage%20denoising%0Apipeline%20named%20Noise2Detail%20%28N2D%29.%20During%20inference%2C%20this%20approach%20disrupts%20the%0Aspatial%20correlations%20of%20noise%20patterns%20to%20produce%20intermediate%20smooth%0Astructures%2C%20which%20are%20subsequently%20refined%20to%20recapture%20fine%20details%20directly%0Afrom%20the%20noisy%20input.%20Extensive%20testing%20reveals%20that%20Noise2Detail%20surpasses%0Aexisting%20dataset-free%20techniques%20in%20performance%2C%20while%20requiring%20only%20a%0Afraction%20of%20the%20computational%20resources.%20This%20combination%20of%20efficiency%2C%20low%0Acomputational%20cost%2C%20and%20data-free%20approach%20make%20it%20a%20valuable%20tool%20for%0Abiomedical%20imaging%2C%20overcoming%20the%20challenges%20of%20scarce%20clean%20training%20data-due%0Ato%20rare%20and%20complex%20imaging%20modalities-while%20enabling%20fast%20inference%20for%0Apractical%20use.%0A&entry.1838667208=http%3A//arxiv.org/abs/2510.15611v1&entry.124074799=Read"},
{"title": "Cavity Duplexer Tuning with 1d Resnet-like Neural Networks", "author": "Anton Raskovalov", "abstract": "  This paper presents machine learning method for tuning of cavity duplexer\nwith a large amount of adjustment screws. After testing we declined\nconventional reinforcement learning approach and reformulated our task in the\nsupervised learning setup. The suggested neural network architecture includes\n1d ResNet-like backbone and processing of some additional information about\nS-parameters, like the shape of curve and peaks positions and amplitudes. This\nneural network with external control algorithm is capable to reach almost the\ntuned state of the duplexer within 4-5 rotations per screw.\n", "link": "http://arxiv.org/abs/2510.15796v1", "date": "2025-10-17", "relevancy": 2.2997, "topK": [{"title": "Boosted dynamic neural networks", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/26302", "similarity": 0.4829}, {"title": "DDM-NET: End-to-end learning of keypoint feature Detection, Description and Matching for 3D localization", "link": "https://arxiv.org/abs/2212.04575", "similarity": 0.4558}, {"title": "Self-supervised Photographic Image Layout Representation Learning", "link": "http://arxiv.org/abs/2403.03740v1", "similarity": 0.4411}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Cavity%20Duplexer%20Tuning%20with%201d%20Resnet-like%20Neural%20Networks&body=Title%3A%20Cavity%20Duplexer%20Tuning%20with%201d%20Resnet-like%20Neural%20Networks%0AAuthor%3A%20Anton%20Raskovalov%0AAbstract%3A%20%20%20This%20paper%20presents%20machine%20learning%20method%20for%20tuning%20of%20cavity%20duplexer%0Awith%20a%20large%20amount%20of%20adjustment%20screws.%20After%20testing%20we%20declined%0Aconventional%20reinforcement%20learning%20approach%20and%20reformulated%20our%20task%20in%20the%0Asupervised%20learning%20setup.%20The%20suggested%20neural%20network%20architecture%20includes%0A1d%20ResNet-like%20backbone%20and%20processing%20of%20some%20additional%20information%20about%0AS-parameters%2C%20like%20the%20shape%20of%20curve%20and%20peaks%20positions%20and%20amplitudes.%20This%0Aneural%20network%20with%20external%20control%20algorithm%20is%20capable%20to%20reach%20almost%20the%0Atuned%20state%20of%20the%20duplexer%20within%204-5%20rotations%20per%20screw.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2510.15796v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DCavity%2520Duplexer%2520Tuning%2520with%25201d%2520Resnet-like%2520Neural%2520Networks%26entry.906535625%3DAnton%2520Raskovalov%26entry.1292438233%3D%2520%2520This%2520paper%2520presents%2520machine%2520learning%2520method%2520for%2520tuning%2520of%2520cavity%2520duplexer%250Awith%2520a%2520large%2520amount%2520of%2520adjustment%2520screws.%2520After%2520testing%2520we%2520declined%250Aconventional%2520reinforcement%2520learning%2520approach%2520and%2520reformulated%2520our%2520task%2520in%2520the%250Asupervised%2520learning%2520setup.%2520The%2520suggested%2520neural%2520network%2520architecture%2520includes%250A1d%2520ResNet-like%2520backbone%2520and%2520processing%2520of%2520some%2520additional%2520information%2520about%250AS-parameters%252C%2520like%2520the%2520shape%2520of%2520curve%2520and%2520peaks%2520positions%2520and%2520amplitudes.%2520This%250Aneural%2520network%2520with%2520external%2520control%2520algorithm%2520is%2520capable%2520to%2520reach%2520almost%2520the%250Atuned%2520state%2520of%2520the%2520duplexer%2520within%25204-5%2520rotations%2520per%2520screw.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2510.15796v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Cavity%20Duplexer%20Tuning%20with%201d%20Resnet-like%20Neural%20Networks&entry.906535625=Anton%20Raskovalov&entry.1292438233=%20%20This%20paper%20presents%20machine%20learning%20method%20for%20tuning%20of%20cavity%20duplexer%0Awith%20a%20large%20amount%20of%20adjustment%20screws.%20After%20testing%20we%20declined%0Aconventional%20reinforcement%20learning%20approach%20and%20reformulated%20our%20task%20in%20the%0Asupervised%20learning%20setup.%20The%20suggested%20neural%20network%20architecture%20includes%0A1d%20ResNet-like%20backbone%20and%20processing%20of%20some%20additional%20information%20about%0AS-parameters%2C%20like%20the%20shape%20of%20curve%20and%20peaks%20positions%20and%20amplitudes.%20This%0Aneural%20network%20with%20external%20control%20algorithm%20is%20capable%20to%20reach%20almost%20the%0Atuned%20state%20of%20the%20duplexer%20within%204-5%20rotations%20per%20screw.%0A&entry.1838667208=http%3A//arxiv.org/abs/2510.15796v1&entry.124074799=Read"},
{"title": "NarraBench: A Comprehensive Framework for Narrative Benchmarking", "author": "Sil Hamilton and Matthew Wilkens and Andrew Piper", "abstract": "  We present NarraBench, a theory-informed taxonomy of narrative-understanding\ntasks, as well as an associated survey of 78 existing benchmarks in the area.\nWe find significant need for new evaluations covering aspects of narrative\nunderstanding that are either overlooked in current work or are poorly aligned\nwith existing metrics. Specifically, we estimate that only 27% of narrative\ntasks are well captured by existing benchmarks, and we note that some areas --\nincluding narrative events, style, perspective, and revelation -- are nearly\nabsent from current evaluations. We also note the need for increased\ndevelopment of benchmarks capable of assessing constitutively subjective and\nperspectival aspects of narrative, that is, aspects for which there is\ngenerally no single correct answer. Our taxonomy, survey, and methodology are\nof value to NLP researchers seeking to test LLM narrative understanding.\n", "link": "http://arxiv.org/abs/2510.09869v2", "date": "2025-10-17", "relevancy": 2.295, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.4799}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.4799}, {"title": "Self-supervised Photographic Image Layout Representation Learning", "link": "http://arxiv.org/abs/2403.03740v1", "similarity": 0.4173}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20NarraBench%3A%20A%20Comprehensive%20Framework%20for%20Narrative%20Benchmarking&body=Title%3A%20NarraBench%3A%20A%20Comprehensive%20Framework%20for%20Narrative%20Benchmarking%0AAuthor%3A%20Sil%20Hamilton%20and%20Matthew%20Wilkens%20and%20Andrew%20Piper%0AAbstract%3A%20%20%20We%20present%20NarraBench%2C%20a%20theory-informed%20taxonomy%20of%20narrative-understanding%0Atasks%2C%20as%20well%20as%20an%20associated%20survey%20of%2078%20existing%20benchmarks%20in%20the%20area.%0AWe%20find%20significant%20need%20for%20new%20evaluations%20covering%20aspects%20of%20narrative%0Aunderstanding%20that%20are%20either%20overlooked%20in%20current%20work%20or%20are%20poorly%20aligned%0Awith%20existing%20metrics.%20Specifically%2C%20we%20estimate%20that%20only%2027%25%20of%20narrative%0Atasks%20are%20well%20captured%20by%20existing%20benchmarks%2C%20and%20we%20note%20that%20some%20areas%20--%0Aincluding%20narrative%20events%2C%20style%2C%20perspective%2C%20and%20revelation%20--%20are%20nearly%0Aabsent%20from%20current%20evaluations.%20We%20also%20note%20the%20need%20for%20increased%0Adevelopment%20of%20benchmarks%20capable%20of%20assessing%20constitutively%20subjective%20and%0Aperspectival%20aspects%20of%20narrative%2C%20that%20is%2C%20aspects%20for%20which%20there%20is%0Agenerally%20no%20single%20correct%20answer.%20Our%20taxonomy%2C%20survey%2C%20and%20methodology%20are%0Aof%20value%20to%20NLP%20researchers%20seeking%20to%20test%20LLM%20narrative%20understanding.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2510.09869v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DNarraBench%253A%2520A%2520Comprehensive%2520Framework%2520for%2520Narrative%2520Benchmarking%26entry.906535625%3DSil%2520Hamilton%2520and%2520Matthew%2520Wilkens%2520and%2520Andrew%2520Piper%26entry.1292438233%3D%2520%2520We%2520present%2520NarraBench%252C%2520a%2520theory-informed%2520taxonomy%2520of%2520narrative-understanding%250Atasks%252C%2520as%2520well%2520as%2520an%2520associated%2520survey%2520of%252078%2520existing%2520benchmarks%2520in%2520the%2520area.%250AWe%2520find%2520significant%2520need%2520for%2520new%2520evaluations%2520covering%2520aspects%2520of%2520narrative%250Aunderstanding%2520that%2520are%2520either%2520overlooked%2520in%2520current%2520work%2520or%2520are%2520poorly%2520aligned%250Awith%2520existing%2520metrics.%2520Specifically%252C%2520we%2520estimate%2520that%2520only%252027%2525%2520of%2520narrative%250Atasks%2520are%2520well%2520captured%2520by%2520existing%2520benchmarks%252C%2520and%2520we%2520note%2520that%2520some%2520areas%2520--%250Aincluding%2520narrative%2520events%252C%2520style%252C%2520perspective%252C%2520and%2520revelation%2520--%2520are%2520nearly%250Aabsent%2520from%2520current%2520evaluations.%2520We%2520also%2520note%2520the%2520need%2520for%2520increased%250Adevelopment%2520of%2520benchmarks%2520capable%2520of%2520assessing%2520constitutively%2520subjective%2520and%250Aperspectival%2520aspects%2520of%2520narrative%252C%2520that%2520is%252C%2520aspects%2520for%2520which%2520there%2520is%250Agenerally%2520no%2520single%2520correct%2520answer.%2520Our%2520taxonomy%252C%2520survey%252C%2520and%2520methodology%2520are%250Aof%2520value%2520to%2520NLP%2520researchers%2520seeking%2520to%2520test%2520LLM%2520narrative%2520understanding.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2510.09869v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=NarraBench%3A%20A%20Comprehensive%20Framework%20for%20Narrative%20Benchmarking&entry.906535625=Sil%20Hamilton%20and%20Matthew%20Wilkens%20and%20Andrew%20Piper&entry.1292438233=%20%20We%20present%20NarraBench%2C%20a%20theory-informed%20taxonomy%20of%20narrative-understanding%0Atasks%2C%20as%20well%20as%20an%20associated%20survey%20of%2078%20existing%20benchmarks%20in%20the%20area.%0AWe%20find%20significant%20need%20for%20new%20evaluations%20covering%20aspects%20of%20narrative%0Aunderstanding%20that%20are%20either%20overlooked%20in%20current%20work%20or%20are%20poorly%20aligned%0Awith%20existing%20metrics.%20Specifically%2C%20we%20estimate%20that%20only%2027%25%20of%20narrative%0Atasks%20are%20well%20captured%20by%20existing%20benchmarks%2C%20and%20we%20note%20that%20some%20areas%20--%0Aincluding%20narrative%20events%2C%20style%2C%20perspective%2C%20and%20revelation%20--%20are%20nearly%0Aabsent%20from%20current%20evaluations.%20We%20also%20note%20the%20need%20for%20increased%0Adevelopment%20of%20benchmarks%20capable%20of%20assessing%20constitutively%20subjective%20and%0Aperspectival%20aspects%20of%20narrative%2C%20that%20is%2C%20aspects%20for%20which%20there%20is%0Agenerally%20no%20single%20correct%20answer.%20Our%20taxonomy%2C%20survey%2C%20and%20methodology%20are%0Aof%20value%20to%20NLP%20researchers%20seeking%20to%20test%20LLM%20narrative%20understanding.%0A&entry.1838667208=http%3A//arxiv.org/abs/2510.09869v2&entry.124074799=Read"},
{"title": "DGME-T: Directional Grid Motion Encoding for Transformer-Based\n  Historical Camera Movement Classification", "author": "Tingyu Lin and Armin Dadras and Florian Kleber and Robert Sablatnig", "abstract": "  Camera movement classification (CMC) models trained on contemporary,\nhigh-quality footage often degrade when applied to archival film, where noise,\nmissing frames, and low contrast obscure motion cues. We bridge this gap by\nassembling a unified benchmark that consolidates two modern corpora into four\ncanonical classes and restructures the HISTORIAN collection into five balanced\ncategories. Building on this benchmark, we introduce DGME-T, a lightweight\nextension to the Video Swin Transformer that injects directional grid motion\nencoding, derived from optical flow, via a learnable and normalised late-fusion\nlayer. DGME-T raises the backbone's top-1 accuracy from 81.78% to 86.14% and\nits macro F1 from 82.08% to 87.81% on modern clips, while still improving the\ndemanding World-War-II footage from 83.43% to 84.62% accuracy and from 81.72%\nto 82.63% macro F1. A cross-domain study further shows that an intermediate\nfine-tuning stage on modern data increases historical performance by more than\nfive percentage points. These results demonstrate that structured motion priors\nand transformer representations are complementary and that even a small,\ncarefully calibrated motion head can substantially enhance robustness in\ndegraded film analysis. Related resources are available at\nhttps://github.com/linty5/DGME-T.\n", "link": "http://arxiv.org/abs/2510.15725v1", "date": "2025-10-17", "relevancy": 2.2812, "topK": [{"title": "CamCtrl3D: Single-Image Scene Exploration with Precise 3D Camera Control", "link": "http://arxiv.org/abs/2501.06006v1", "similarity": 0.6116}, {"title": "Dynamic Gaussians Mesh: Consistent Mesh Reconstruction from Monocular\n  Videos", "link": "http://arxiv.org/abs/2404.12379v2", "similarity": 0.5745}, {"title": "Magic-Me: Identity-Specific Video Customized Diffusion", "link": "http://arxiv.org/abs/2402.09368v1", "similarity": 0.5496}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20DGME-T%3A%20Directional%20Grid%20Motion%20Encoding%20for%20Transformer-Based%0A%20%20Historical%20Camera%20Movement%20Classification&body=Title%3A%20DGME-T%3A%20Directional%20Grid%20Motion%20Encoding%20for%20Transformer-Based%0A%20%20Historical%20Camera%20Movement%20Classification%0AAuthor%3A%20Tingyu%20Lin%20and%20Armin%20Dadras%20and%20Florian%20Kleber%20and%20Robert%20Sablatnig%0AAbstract%3A%20%20%20Camera%20movement%20classification%20%28CMC%29%20models%20trained%20on%20contemporary%2C%0Ahigh-quality%20footage%20often%20degrade%20when%20applied%20to%20archival%20film%2C%20where%20noise%2C%0Amissing%20frames%2C%20and%20low%20contrast%20obscure%20motion%20cues.%20We%20bridge%20this%20gap%20by%0Aassembling%20a%20unified%20benchmark%20that%20consolidates%20two%20modern%20corpora%20into%20four%0Acanonical%20classes%20and%20restructures%20the%20HISTORIAN%20collection%20into%20five%20balanced%0Acategories.%20Building%20on%20this%20benchmark%2C%20we%20introduce%20DGME-T%2C%20a%20lightweight%0Aextension%20to%20the%20Video%20Swin%20Transformer%20that%20injects%20directional%20grid%20motion%0Aencoding%2C%20derived%20from%20optical%20flow%2C%20via%20a%20learnable%20and%20normalised%20late-fusion%0Alayer.%20DGME-T%20raises%20the%20backbone%27s%20top-1%20accuracy%20from%2081.78%25%20to%2086.14%25%20and%0Aits%20macro%20F1%20from%2082.08%25%20to%2087.81%25%20on%20modern%20clips%2C%20while%20still%20improving%20the%0Ademanding%20World-War-II%20footage%20from%2083.43%25%20to%2084.62%25%20accuracy%20and%20from%2081.72%25%0Ato%2082.63%25%20macro%20F1.%20A%20cross-domain%20study%20further%20shows%20that%20an%20intermediate%0Afine-tuning%20stage%20on%20modern%20data%20increases%20historical%20performance%20by%20more%20than%0Afive%20percentage%20points.%20These%20results%20demonstrate%20that%20structured%20motion%20priors%0Aand%20transformer%20representations%20are%20complementary%20and%20that%20even%20a%20small%2C%0Acarefully%20calibrated%20motion%20head%20can%20substantially%20enhance%20robustness%20in%0Adegraded%20film%20analysis.%20Related%20resources%20are%20available%20at%0Ahttps%3A//github.com/linty5/DGME-T.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2510.15725v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DDGME-T%253A%2520Directional%2520Grid%2520Motion%2520Encoding%2520for%2520Transformer-Based%250A%2520%2520Historical%2520Camera%2520Movement%2520Classification%26entry.906535625%3DTingyu%2520Lin%2520and%2520Armin%2520Dadras%2520and%2520Florian%2520Kleber%2520and%2520Robert%2520Sablatnig%26entry.1292438233%3D%2520%2520Camera%2520movement%2520classification%2520%2528CMC%2529%2520models%2520trained%2520on%2520contemporary%252C%250Ahigh-quality%2520footage%2520often%2520degrade%2520when%2520applied%2520to%2520archival%2520film%252C%2520where%2520noise%252C%250Amissing%2520frames%252C%2520and%2520low%2520contrast%2520obscure%2520motion%2520cues.%2520We%2520bridge%2520this%2520gap%2520by%250Aassembling%2520a%2520unified%2520benchmark%2520that%2520consolidates%2520two%2520modern%2520corpora%2520into%2520four%250Acanonical%2520classes%2520and%2520restructures%2520the%2520HISTORIAN%2520collection%2520into%2520five%2520balanced%250Acategories.%2520Building%2520on%2520this%2520benchmark%252C%2520we%2520introduce%2520DGME-T%252C%2520a%2520lightweight%250Aextension%2520to%2520the%2520Video%2520Swin%2520Transformer%2520that%2520injects%2520directional%2520grid%2520motion%250Aencoding%252C%2520derived%2520from%2520optical%2520flow%252C%2520via%2520a%2520learnable%2520and%2520normalised%2520late-fusion%250Alayer.%2520DGME-T%2520raises%2520the%2520backbone%2527s%2520top-1%2520accuracy%2520from%252081.78%2525%2520to%252086.14%2525%2520and%250Aits%2520macro%2520F1%2520from%252082.08%2525%2520to%252087.81%2525%2520on%2520modern%2520clips%252C%2520while%2520still%2520improving%2520the%250Ademanding%2520World-War-II%2520footage%2520from%252083.43%2525%2520to%252084.62%2525%2520accuracy%2520and%2520from%252081.72%2525%250Ato%252082.63%2525%2520macro%2520F1.%2520A%2520cross-domain%2520study%2520further%2520shows%2520that%2520an%2520intermediate%250Afine-tuning%2520stage%2520on%2520modern%2520data%2520increases%2520historical%2520performance%2520by%2520more%2520than%250Afive%2520percentage%2520points.%2520These%2520results%2520demonstrate%2520that%2520structured%2520motion%2520priors%250Aand%2520transformer%2520representations%2520are%2520complementary%2520and%2520that%2520even%2520a%2520small%252C%250Acarefully%2520calibrated%2520motion%2520head%2520can%2520substantially%2520enhance%2520robustness%2520in%250Adegraded%2520film%2520analysis.%2520Related%2520resources%2520are%2520available%2520at%250Ahttps%253A//github.com/linty5/DGME-T.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2510.15725v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=DGME-T%3A%20Directional%20Grid%20Motion%20Encoding%20for%20Transformer-Based%0A%20%20Historical%20Camera%20Movement%20Classification&entry.906535625=Tingyu%20Lin%20and%20Armin%20Dadras%20and%20Florian%20Kleber%20and%20Robert%20Sablatnig&entry.1292438233=%20%20Camera%20movement%20classification%20%28CMC%29%20models%20trained%20on%20contemporary%2C%0Ahigh-quality%20footage%20often%20degrade%20when%20applied%20to%20archival%20film%2C%20where%20noise%2C%0Amissing%20frames%2C%20and%20low%20contrast%20obscure%20motion%20cues.%20We%20bridge%20this%20gap%20by%0Aassembling%20a%20unified%20benchmark%20that%20consolidates%20two%20modern%20corpora%20into%20four%0Acanonical%20classes%20and%20restructures%20the%20HISTORIAN%20collection%20into%20five%20balanced%0Acategories.%20Building%20on%20this%20benchmark%2C%20we%20introduce%20DGME-T%2C%20a%20lightweight%0Aextension%20to%20the%20Video%20Swin%20Transformer%20that%20injects%20directional%20grid%20motion%0Aencoding%2C%20derived%20from%20optical%20flow%2C%20via%20a%20learnable%20and%20normalised%20late-fusion%0Alayer.%20DGME-T%20raises%20the%20backbone%27s%20top-1%20accuracy%20from%2081.78%25%20to%2086.14%25%20and%0Aits%20macro%20F1%20from%2082.08%25%20to%2087.81%25%20on%20modern%20clips%2C%20while%20still%20improving%20the%0Ademanding%20World-War-II%20footage%20from%2083.43%25%20to%2084.62%25%20accuracy%20and%20from%2081.72%25%0Ato%2082.63%25%20macro%20F1.%20A%20cross-domain%20study%20further%20shows%20that%20an%20intermediate%0Afine-tuning%20stage%20on%20modern%20data%20increases%20historical%20performance%20by%20more%20than%0Afive%20percentage%20points.%20These%20results%20demonstrate%20that%20structured%20motion%20priors%0Aand%20transformer%20representations%20are%20complementary%20and%20that%20even%20a%20small%2C%0Acarefully%20calibrated%20motion%20head%20can%20substantially%20enhance%20robustness%20in%0Adegraded%20film%20analysis.%20Related%20resources%20are%20available%20at%0Ahttps%3A//github.com/linty5/DGME-T.%0A&entry.1838667208=http%3A//arxiv.org/abs/2510.15725v1&entry.124074799=Read"},
{"title": "Pseudo-Kinematic Trajectory Control and Planning of Tracked Vehicles", "author": "Michele Focchi and Daniele Fontanelli and Davide Stocco and Riccardo Bussola and Luigi Palopoli", "abstract": "  Tracked vehicles distribute their weight continuously over a large surface\narea (the tracks). This distinctive feature makes them the preferred choice for\nvehicles required to traverse soft and uneven terrain. From a robotics\nperspective, however, this flexibility comes at a cost: the complexity of\nmodelling the system and the resulting difficulty in designing theoretically\nsound navigation solutions. In this paper, we aim to bridge this gap by\nproposing a framework for the navigation of tracked vehicles, built upon three\nkey pillars. The first pillar comprises two models: a simulation model and a\ncontrol-oriented model. The simulation model captures the intricate\nterramechanics dynamics arising from soil-track interaction and is employed to\ndevelop faithful digital twins of the system across a wide range of operating\nconditions. The control-oriented model is pseudo-kinematic and mathematically\ntractable, enabling the design of efficient and theoretically robust control\nschemes. The second pillar is a Lyapunov-based feedback trajectory controller\nthat provides certifiable tracking guarantees. The third pillar is a portfolio\nof motion planning solutions, each offering different complexity-accuracy\ntrade-offs. The various components of the proposed approach are validated\nthrough an extensive set of simulation and experimental data.\n", "link": "http://arxiv.org/abs/2409.18641v3", "date": "2025-10-17", "relevancy": 2.2594, "topK": [{"title": "Egocentric Computer Vision for Hands-Free Robotic Wheelchair Navigation", "link": "https://link.springer.com/article/10.1007/s10846-023-01807-4", "similarity": 0.5899}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.5829}, {"title": "Synthesizing Moving People with 3D Control", "link": "http://arxiv.org/abs/2401.10889v2", "similarity": 0.5326}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Pseudo-Kinematic%20Trajectory%20Control%20and%20Planning%20of%20Tracked%20Vehicles&body=Title%3A%20Pseudo-Kinematic%20Trajectory%20Control%20and%20Planning%20of%20Tracked%20Vehicles%0AAuthor%3A%20Michele%20Focchi%20and%20Daniele%20Fontanelli%20and%20Davide%20Stocco%20and%20Riccardo%20Bussola%20and%20Luigi%20Palopoli%0AAbstract%3A%20%20%20Tracked%20vehicles%20distribute%20their%20weight%20continuously%20over%20a%20large%20surface%0Aarea%20%28the%20tracks%29.%20This%20distinctive%20feature%20makes%20them%20the%20preferred%20choice%20for%0Avehicles%20required%20to%20traverse%20soft%20and%20uneven%20terrain.%20From%20a%20robotics%0Aperspective%2C%20however%2C%20this%20flexibility%20comes%20at%20a%20cost%3A%20the%20complexity%20of%0Amodelling%20the%20system%20and%20the%20resulting%20difficulty%20in%20designing%20theoretically%0Asound%20navigation%20solutions.%20In%20this%20paper%2C%20we%20aim%20to%20bridge%20this%20gap%20by%0Aproposing%20a%20framework%20for%20the%20navigation%20of%20tracked%20vehicles%2C%20built%20upon%20three%0Akey%20pillars.%20The%20first%20pillar%20comprises%20two%20models%3A%20a%20simulation%20model%20and%20a%0Acontrol-oriented%20model.%20The%20simulation%20model%20captures%20the%20intricate%0Aterramechanics%20dynamics%20arising%20from%20soil-track%20interaction%20and%20is%20employed%20to%0Adevelop%20faithful%20digital%20twins%20of%20the%20system%20across%20a%20wide%20range%20of%20operating%0Aconditions.%20The%20control-oriented%20model%20is%20pseudo-kinematic%20and%20mathematically%0Atractable%2C%20enabling%20the%20design%20of%20efficient%20and%20theoretically%20robust%20control%0Aschemes.%20The%20second%20pillar%20is%20a%20Lyapunov-based%20feedback%20trajectory%20controller%0Athat%20provides%20certifiable%20tracking%20guarantees.%20The%20third%20pillar%20is%20a%20portfolio%0Aof%20motion%20planning%20solutions%2C%20each%20offering%20different%20complexity-accuracy%0Atrade-offs.%20The%20various%20components%20of%20the%20proposed%20approach%20are%20validated%0Athrough%20an%20extensive%20set%20of%20simulation%20and%20experimental%20data.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2409.18641v3%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DPseudo-Kinematic%2520Trajectory%2520Control%2520and%2520Planning%2520of%2520Tracked%2520Vehicles%26entry.906535625%3DMichele%2520Focchi%2520and%2520Daniele%2520Fontanelli%2520and%2520Davide%2520Stocco%2520and%2520Riccardo%2520Bussola%2520and%2520Luigi%2520Palopoli%26entry.1292438233%3D%2520%2520Tracked%2520vehicles%2520distribute%2520their%2520weight%2520continuously%2520over%2520a%2520large%2520surface%250Aarea%2520%2528the%2520tracks%2529.%2520This%2520distinctive%2520feature%2520makes%2520them%2520the%2520preferred%2520choice%2520for%250Avehicles%2520required%2520to%2520traverse%2520soft%2520and%2520uneven%2520terrain.%2520From%2520a%2520robotics%250Aperspective%252C%2520however%252C%2520this%2520flexibility%2520comes%2520at%2520a%2520cost%253A%2520the%2520complexity%2520of%250Amodelling%2520the%2520system%2520and%2520the%2520resulting%2520difficulty%2520in%2520designing%2520theoretically%250Asound%2520navigation%2520solutions.%2520In%2520this%2520paper%252C%2520we%2520aim%2520to%2520bridge%2520this%2520gap%2520by%250Aproposing%2520a%2520framework%2520for%2520the%2520navigation%2520of%2520tracked%2520vehicles%252C%2520built%2520upon%2520three%250Akey%2520pillars.%2520The%2520first%2520pillar%2520comprises%2520two%2520models%253A%2520a%2520simulation%2520model%2520and%2520a%250Acontrol-oriented%2520model.%2520The%2520simulation%2520model%2520captures%2520the%2520intricate%250Aterramechanics%2520dynamics%2520arising%2520from%2520soil-track%2520interaction%2520and%2520is%2520employed%2520to%250Adevelop%2520faithful%2520digital%2520twins%2520of%2520the%2520system%2520across%2520a%2520wide%2520range%2520of%2520operating%250Aconditions.%2520The%2520control-oriented%2520model%2520is%2520pseudo-kinematic%2520and%2520mathematically%250Atractable%252C%2520enabling%2520the%2520design%2520of%2520efficient%2520and%2520theoretically%2520robust%2520control%250Aschemes.%2520The%2520second%2520pillar%2520is%2520a%2520Lyapunov-based%2520feedback%2520trajectory%2520controller%250Athat%2520provides%2520certifiable%2520tracking%2520guarantees.%2520The%2520third%2520pillar%2520is%2520a%2520portfolio%250Aof%2520motion%2520planning%2520solutions%252C%2520each%2520offering%2520different%2520complexity-accuracy%250Atrade-offs.%2520The%2520various%2520components%2520of%2520the%2520proposed%2520approach%2520are%2520validated%250Athrough%2520an%2520extensive%2520set%2520of%2520simulation%2520and%2520experimental%2520data.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2409.18641v3%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Pseudo-Kinematic%20Trajectory%20Control%20and%20Planning%20of%20Tracked%20Vehicles&entry.906535625=Michele%20Focchi%20and%20Daniele%20Fontanelli%20and%20Davide%20Stocco%20and%20Riccardo%20Bussola%20and%20Luigi%20Palopoli&entry.1292438233=%20%20Tracked%20vehicles%20distribute%20their%20weight%20continuously%20over%20a%20large%20surface%0Aarea%20%28the%20tracks%29.%20This%20distinctive%20feature%20makes%20them%20the%20preferred%20choice%20for%0Avehicles%20required%20to%20traverse%20soft%20and%20uneven%20terrain.%20From%20a%20robotics%0Aperspective%2C%20however%2C%20this%20flexibility%20comes%20at%20a%20cost%3A%20the%20complexity%20of%0Amodelling%20the%20system%20and%20the%20resulting%20difficulty%20in%20designing%20theoretically%0Asound%20navigation%20solutions.%20In%20this%20paper%2C%20we%20aim%20to%20bridge%20this%20gap%20by%0Aproposing%20a%20framework%20for%20the%20navigation%20of%20tracked%20vehicles%2C%20built%20upon%20three%0Akey%20pillars.%20The%20first%20pillar%20comprises%20two%20models%3A%20a%20simulation%20model%20and%20a%0Acontrol-oriented%20model.%20The%20simulation%20model%20captures%20the%20intricate%0Aterramechanics%20dynamics%20arising%20from%20soil-track%20interaction%20and%20is%20employed%20to%0Adevelop%20faithful%20digital%20twins%20of%20the%20system%20across%20a%20wide%20range%20of%20operating%0Aconditions.%20The%20control-oriented%20model%20is%20pseudo-kinematic%20and%20mathematically%0Atractable%2C%20enabling%20the%20design%20of%20efficient%20and%20theoretically%20robust%20control%0Aschemes.%20The%20second%20pillar%20is%20a%20Lyapunov-based%20feedback%20trajectory%20controller%0Athat%20provides%20certifiable%20tracking%20guarantees.%20The%20third%20pillar%20is%20a%20portfolio%0Aof%20motion%20planning%20solutions%2C%20each%20offering%20different%20complexity-accuracy%0Atrade-offs.%20The%20various%20components%20of%20the%20proposed%20approach%20are%20validated%0Athrough%20an%20extensive%20set%20of%20simulation%20and%20experimental%20data.%0A&entry.1838667208=http%3A//arxiv.org/abs/2409.18641v3&entry.124074799=Read"},
{"title": "Methods and Trends in Detecting AI-Generated Images: A Comprehensive\n  Review", "author": "Arpan Mahara and Naphtali Rishe", "abstract": "  The proliferation of generative models, such as Generative Adversarial\nNetworks (GANs), Diffusion Models, and Variational Autoencoders (VAEs), has\nenabled the synthesis of high-quality multimedia data. However, these\nadvancements have also raised significant concerns regarding adversarial\nattacks, unethical usage, and societal harm. Recognizing these challenges,\nresearchers have increasingly focused on developing methodologies to detect\nsynthesized data effectively, aiming to mitigate potential risks. Prior reviews\nhave predominantly focused on deepfake detection and often overlook recent\nadvancements in synthetic image forensics, particularly approaches that\nincorporate multimodal frameworks, reasoning-based detection, and training-free\nmethodologies. To bridge this gap, this survey provides a comprehensive and\nup-to-date review of state-of-the-art techniques for detecting and classifying\nsynthetic images generated by advanced generative AI models. The review\nsystematically examines core detection paradigms, categorizes them into\nspatial-domain, frequency-domain, fingerprint-based, patch-based,\ntraining-free, and multimodal reasoning-based frameworks, and offers concise\ndescriptions of their underlying principles. We further provide detailed\ncomparative analyses of these methods on publicly available datasets to assess\ntheir generalizability, robustness, and interpretability. Finally, the survey\nhighlights open challenges and future directions, emphasizing the potential of\nhybrid frameworks that combine the efficiency of training-free approaches with\nthe semantic reasoning of multimodal models to advance trustworthy and\nexplainable synthetic image forensics.\n", "link": "http://arxiv.org/abs/2502.15176v2", "date": "2025-10-17", "relevancy": 2.25, "topK": [{"title": "Thinking Outside the BBox: Unconstrained Generative Object Compositing", "link": "http://arxiv.org/abs/2409.04559v2", "similarity": 0.5803}, {"title": "GeneVA: A Dataset of Human Annotations for Generative Text to Video\n  Artifacts", "link": "http://arxiv.org/abs/2509.08818v1", "similarity": 0.5796}, {"title": "Magic-Me: Identity-Specific Video Customized Diffusion", "link": "http://arxiv.org/abs/2402.09368v1", "similarity": 0.5383}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Methods%20and%20Trends%20in%20Detecting%20AI-Generated%20Images%3A%20A%20Comprehensive%0A%20%20Review&body=Title%3A%20Methods%20and%20Trends%20in%20Detecting%20AI-Generated%20Images%3A%20A%20Comprehensive%0A%20%20Review%0AAuthor%3A%20Arpan%20Mahara%20and%20Naphtali%20Rishe%0AAbstract%3A%20%20%20The%20proliferation%20of%20generative%20models%2C%20such%20as%20Generative%20Adversarial%0ANetworks%20%28GANs%29%2C%20Diffusion%20Models%2C%20and%20Variational%20Autoencoders%20%28VAEs%29%2C%20has%0Aenabled%20the%20synthesis%20of%20high-quality%20multimedia%20data.%20However%2C%20these%0Aadvancements%20have%20also%20raised%20significant%20concerns%20regarding%20adversarial%0Aattacks%2C%20unethical%20usage%2C%20and%20societal%20harm.%20Recognizing%20these%20challenges%2C%0Aresearchers%20have%20increasingly%20focused%20on%20developing%20methodologies%20to%20detect%0Asynthesized%20data%20effectively%2C%20aiming%20to%20mitigate%20potential%20risks.%20Prior%20reviews%0Ahave%20predominantly%20focused%20on%20deepfake%20detection%20and%20often%20overlook%20recent%0Aadvancements%20in%20synthetic%20image%20forensics%2C%20particularly%20approaches%20that%0Aincorporate%20multimodal%20frameworks%2C%20reasoning-based%20detection%2C%20and%20training-free%0Amethodologies.%20To%20bridge%20this%20gap%2C%20this%20survey%20provides%20a%20comprehensive%20and%0Aup-to-date%20review%20of%20state-of-the-art%20techniques%20for%20detecting%20and%20classifying%0Asynthetic%20images%20generated%20by%20advanced%20generative%20AI%20models.%20The%20review%0Asystematically%20examines%20core%20detection%20paradigms%2C%20categorizes%20them%20into%0Aspatial-domain%2C%20frequency-domain%2C%20fingerprint-based%2C%20patch-based%2C%0Atraining-free%2C%20and%20multimodal%20reasoning-based%20frameworks%2C%20and%20offers%20concise%0Adescriptions%20of%20their%20underlying%20principles.%20We%20further%20provide%20detailed%0Acomparative%20analyses%20of%20these%20methods%20on%20publicly%20available%20datasets%20to%20assess%0Atheir%20generalizability%2C%20robustness%2C%20and%20interpretability.%20Finally%2C%20the%20survey%0Ahighlights%20open%20challenges%20and%20future%20directions%2C%20emphasizing%20the%20potential%20of%0Ahybrid%20frameworks%20that%20combine%20the%20efficiency%20of%20training-free%20approaches%20with%0Athe%20semantic%20reasoning%20of%20multimodal%20models%20to%20advance%20trustworthy%20and%0Aexplainable%20synthetic%20image%20forensics.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2502.15176v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DMethods%2520and%2520Trends%2520in%2520Detecting%2520AI-Generated%2520Images%253A%2520A%2520Comprehensive%250A%2520%2520Review%26entry.906535625%3DArpan%2520Mahara%2520and%2520Naphtali%2520Rishe%26entry.1292438233%3D%2520%2520The%2520proliferation%2520of%2520generative%2520models%252C%2520such%2520as%2520Generative%2520Adversarial%250ANetworks%2520%2528GANs%2529%252C%2520Diffusion%2520Models%252C%2520and%2520Variational%2520Autoencoders%2520%2528VAEs%2529%252C%2520has%250Aenabled%2520the%2520synthesis%2520of%2520high-quality%2520multimedia%2520data.%2520However%252C%2520these%250Aadvancements%2520have%2520also%2520raised%2520significant%2520concerns%2520regarding%2520adversarial%250Aattacks%252C%2520unethical%2520usage%252C%2520and%2520societal%2520harm.%2520Recognizing%2520these%2520challenges%252C%250Aresearchers%2520have%2520increasingly%2520focused%2520on%2520developing%2520methodologies%2520to%2520detect%250Asynthesized%2520data%2520effectively%252C%2520aiming%2520to%2520mitigate%2520potential%2520risks.%2520Prior%2520reviews%250Ahave%2520predominantly%2520focused%2520on%2520deepfake%2520detection%2520and%2520often%2520overlook%2520recent%250Aadvancements%2520in%2520synthetic%2520image%2520forensics%252C%2520particularly%2520approaches%2520that%250Aincorporate%2520multimodal%2520frameworks%252C%2520reasoning-based%2520detection%252C%2520and%2520training-free%250Amethodologies.%2520To%2520bridge%2520this%2520gap%252C%2520this%2520survey%2520provides%2520a%2520comprehensive%2520and%250Aup-to-date%2520review%2520of%2520state-of-the-art%2520techniques%2520for%2520detecting%2520and%2520classifying%250Asynthetic%2520images%2520generated%2520by%2520advanced%2520generative%2520AI%2520models.%2520The%2520review%250Asystematically%2520examines%2520core%2520detection%2520paradigms%252C%2520categorizes%2520them%2520into%250Aspatial-domain%252C%2520frequency-domain%252C%2520fingerprint-based%252C%2520patch-based%252C%250Atraining-free%252C%2520and%2520multimodal%2520reasoning-based%2520frameworks%252C%2520and%2520offers%2520concise%250Adescriptions%2520of%2520their%2520underlying%2520principles.%2520We%2520further%2520provide%2520detailed%250Acomparative%2520analyses%2520of%2520these%2520methods%2520on%2520publicly%2520available%2520datasets%2520to%2520assess%250Atheir%2520generalizability%252C%2520robustness%252C%2520and%2520interpretability.%2520Finally%252C%2520the%2520survey%250Ahighlights%2520open%2520challenges%2520and%2520future%2520directions%252C%2520emphasizing%2520the%2520potential%2520of%250Ahybrid%2520frameworks%2520that%2520combine%2520the%2520efficiency%2520of%2520training-free%2520approaches%2520with%250Athe%2520semantic%2520reasoning%2520of%2520multimodal%2520models%2520to%2520advance%2520trustworthy%2520and%250Aexplainable%2520synthetic%2520image%2520forensics.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2502.15176v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Methods%20and%20Trends%20in%20Detecting%20AI-Generated%20Images%3A%20A%20Comprehensive%0A%20%20Review&entry.906535625=Arpan%20Mahara%20and%20Naphtali%20Rishe&entry.1292438233=%20%20The%20proliferation%20of%20generative%20models%2C%20such%20as%20Generative%20Adversarial%0ANetworks%20%28GANs%29%2C%20Diffusion%20Models%2C%20and%20Variational%20Autoencoders%20%28VAEs%29%2C%20has%0Aenabled%20the%20synthesis%20of%20high-quality%20multimedia%20data.%20However%2C%20these%0Aadvancements%20have%20also%20raised%20significant%20concerns%20regarding%20adversarial%0Aattacks%2C%20unethical%20usage%2C%20and%20societal%20harm.%20Recognizing%20these%20challenges%2C%0Aresearchers%20have%20increasingly%20focused%20on%20developing%20methodologies%20to%20detect%0Asynthesized%20data%20effectively%2C%20aiming%20to%20mitigate%20potential%20risks.%20Prior%20reviews%0Ahave%20predominantly%20focused%20on%20deepfake%20detection%20and%20often%20overlook%20recent%0Aadvancements%20in%20synthetic%20image%20forensics%2C%20particularly%20approaches%20that%0Aincorporate%20multimodal%20frameworks%2C%20reasoning-based%20detection%2C%20and%20training-free%0Amethodologies.%20To%20bridge%20this%20gap%2C%20this%20survey%20provides%20a%20comprehensive%20and%0Aup-to-date%20review%20of%20state-of-the-art%20techniques%20for%20detecting%20and%20classifying%0Asynthetic%20images%20generated%20by%20advanced%20generative%20AI%20models.%20The%20review%0Asystematically%20examines%20core%20detection%20paradigms%2C%20categorizes%20them%20into%0Aspatial-domain%2C%20frequency-domain%2C%20fingerprint-based%2C%20patch-based%2C%0Atraining-free%2C%20and%20multimodal%20reasoning-based%20frameworks%2C%20and%20offers%20concise%0Adescriptions%20of%20their%20underlying%20principles.%20We%20further%20provide%20detailed%0Acomparative%20analyses%20of%20these%20methods%20on%20publicly%20available%20datasets%20to%20assess%0Atheir%20generalizability%2C%20robustness%2C%20and%20interpretability.%20Finally%2C%20the%20survey%0Ahighlights%20open%20challenges%20and%20future%20directions%2C%20emphasizing%20the%20potential%20of%0Ahybrid%20frameworks%20that%20combine%20the%20efficiency%20of%20training-free%20approaches%20with%0Athe%20semantic%20reasoning%20of%20multimodal%20models%20to%20advance%20trustworthy%20and%0Aexplainable%20synthetic%20image%20forensics.%0A&entry.1838667208=http%3A//arxiv.org/abs/2502.15176v2&entry.124074799=Read"},
{"title": "BiomedXPro: Prompt Optimization for Explainable Diagnosis with\n  Biomedical Vision Language Models", "author": "Kaushitha Silva and Mansitha Eashwara and Sanduni Ubayasiri and Ruwan Tennakoon and Damayanthi Herath", "abstract": "  The clinical adoption of biomedical vision-language models is hindered by\nprompt optimization techniques that produce either uninterpretable latent\nvectors or single textual prompts. This lack of transparency and failure to\ncapture the multi-faceted nature of clinical diagnosis, which relies on\nintegrating diverse observations, limits their trustworthiness in high-stakes\nsettings. To address this, we introduce BiomedXPro, an evolutionary framework\nthat leverages a large language model as both a biomedical knowledge extractor\nand an adaptive optimizer to automatically generate a diverse ensemble of\ninterpretable, natural-language prompt pairs for disease diagnosis. Experiments\non multiple biomedical benchmarks show that BiomedXPro consistently outperforms\nstate-of-the-art prompt-tuning methods, particularly in data-scarce few-shot\nsettings. Furthermore, our analysis demonstrates a strong semantic alignment\nbetween the discovered prompts and statistically significant clinical features,\ngrounding the model's performance in verifiable concepts. By producing a\ndiverse ensemble of interpretable prompts, BiomedXPro provides a verifiable\nbasis for model predictions, representing a critical step toward the\ndevelopment of more trustworthy and clinically-aligned AI systems.\n", "link": "http://arxiv.org/abs/2510.15866v1", "date": "2025-10-17", "relevancy": 2.2488, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5759}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5759}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.4937}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20BiomedXPro%3A%20Prompt%20Optimization%20for%20Explainable%20Diagnosis%20with%0A%20%20Biomedical%20Vision%20Language%20Models&body=Title%3A%20BiomedXPro%3A%20Prompt%20Optimization%20for%20Explainable%20Diagnosis%20with%0A%20%20Biomedical%20Vision%20Language%20Models%0AAuthor%3A%20Kaushitha%20Silva%20and%20Mansitha%20Eashwara%20and%20Sanduni%20Ubayasiri%20and%20Ruwan%20Tennakoon%20and%20Damayanthi%20Herath%0AAbstract%3A%20%20%20The%20clinical%20adoption%20of%20biomedical%20vision-language%20models%20is%20hindered%20by%0Aprompt%20optimization%20techniques%20that%20produce%20either%20uninterpretable%20latent%0Avectors%20or%20single%20textual%20prompts.%20This%20lack%20of%20transparency%20and%20failure%20to%0Acapture%20the%20multi-faceted%20nature%20of%20clinical%20diagnosis%2C%20which%20relies%20on%0Aintegrating%20diverse%20observations%2C%20limits%20their%20trustworthiness%20in%20high-stakes%0Asettings.%20To%20address%20this%2C%20we%20introduce%20BiomedXPro%2C%20an%20evolutionary%20framework%0Athat%20leverages%20a%20large%20language%20model%20as%20both%20a%20biomedical%20knowledge%20extractor%0Aand%20an%20adaptive%20optimizer%20to%20automatically%20generate%20a%20diverse%20ensemble%20of%0Ainterpretable%2C%20natural-language%20prompt%20pairs%20for%20disease%20diagnosis.%20Experiments%0Aon%20multiple%20biomedical%20benchmarks%20show%20that%20BiomedXPro%20consistently%20outperforms%0Astate-of-the-art%20prompt-tuning%20methods%2C%20particularly%20in%20data-scarce%20few-shot%0Asettings.%20Furthermore%2C%20our%20analysis%20demonstrates%20a%20strong%20semantic%20alignment%0Abetween%20the%20discovered%20prompts%20and%20statistically%20significant%20clinical%20features%2C%0Agrounding%20the%20model%27s%20performance%20in%20verifiable%20concepts.%20By%20producing%20a%0Adiverse%20ensemble%20of%20interpretable%20prompts%2C%20BiomedXPro%20provides%20a%20verifiable%0Abasis%20for%20model%20predictions%2C%20representing%20a%20critical%20step%20toward%20the%0Adevelopment%20of%20more%20trustworthy%20and%20clinically-aligned%20AI%20systems.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2510.15866v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DBiomedXPro%253A%2520Prompt%2520Optimization%2520for%2520Explainable%2520Diagnosis%2520with%250A%2520%2520Biomedical%2520Vision%2520Language%2520Models%26entry.906535625%3DKaushitha%2520Silva%2520and%2520Mansitha%2520Eashwara%2520and%2520Sanduni%2520Ubayasiri%2520and%2520Ruwan%2520Tennakoon%2520and%2520Damayanthi%2520Herath%26entry.1292438233%3D%2520%2520The%2520clinical%2520adoption%2520of%2520biomedical%2520vision-language%2520models%2520is%2520hindered%2520by%250Aprompt%2520optimization%2520techniques%2520that%2520produce%2520either%2520uninterpretable%2520latent%250Avectors%2520or%2520single%2520textual%2520prompts.%2520This%2520lack%2520of%2520transparency%2520and%2520failure%2520to%250Acapture%2520the%2520multi-faceted%2520nature%2520of%2520clinical%2520diagnosis%252C%2520which%2520relies%2520on%250Aintegrating%2520diverse%2520observations%252C%2520limits%2520their%2520trustworthiness%2520in%2520high-stakes%250Asettings.%2520To%2520address%2520this%252C%2520we%2520introduce%2520BiomedXPro%252C%2520an%2520evolutionary%2520framework%250Athat%2520leverages%2520a%2520large%2520language%2520model%2520as%2520both%2520a%2520biomedical%2520knowledge%2520extractor%250Aand%2520an%2520adaptive%2520optimizer%2520to%2520automatically%2520generate%2520a%2520diverse%2520ensemble%2520of%250Ainterpretable%252C%2520natural-language%2520prompt%2520pairs%2520for%2520disease%2520diagnosis.%2520Experiments%250Aon%2520multiple%2520biomedical%2520benchmarks%2520show%2520that%2520BiomedXPro%2520consistently%2520outperforms%250Astate-of-the-art%2520prompt-tuning%2520methods%252C%2520particularly%2520in%2520data-scarce%2520few-shot%250Asettings.%2520Furthermore%252C%2520our%2520analysis%2520demonstrates%2520a%2520strong%2520semantic%2520alignment%250Abetween%2520the%2520discovered%2520prompts%2520and%2520statistically%2520significant%2520clinical%2520features%252C%250Agrounding%2520the%2520model%2527s%2520performance%2520in%2520verifiable%2520concepts.%2520By%2520producing%2520a%250Adiverse%2520ensemble%2520of%2520interpretable%2520prompts%252C%2520BiomedXPro%2520provides%2520a%2520verifiable%250Abasis%2520for%2520model%2520predictions%252C%2520representing%2520a%2520critical%2520step%2520toward%2520the%250Adevelopment%2520of%2520more%2520trustworthy%2520and%2520clinically-aligned%2520AI%2520systems.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2510.15866v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=BiomedXPro%3A%20Prompt%20Optimization%20for%20Explainable%20Diagnosis%20with%0A%20%20Biomedical%20Vision%20Language%20Models&entry.906535625=Kaushitha%20Silva%20and%20Mansitha%20Eashwara%20and%20Sanduni%20Ubayasiri%20and%20Ruwan%20Tennakoon%20and%20Damayanthi%20Herath&entry.1292438233=%20%20The%20clinical%20adoption%20of%20biomedical%20vision-language%20models%20is%20hindered%20by%0Aprompt%20optimization%20techniques%20that%20produce%20either%20uninterpretable%20latent%0Avectors%20or%20single%20textual%20prompts.%20This%20lack%20of%20transparency%20and%20failure%20to%0Acapture%20the%20multi-faceted%20nature%20of%20clinical%20diagnosis%2C%20which%20relies%20on%0Aintegrating%20diverse%20observations%2C%20limits%20their%20trustworthiness%20in%20high-stakes%0Asettings.%20To%20address%20this%2C%20we%20introduce%20BiomedXPro%2C%20an%20evolutionary%20framework%0Athat%20leverages%20a%20large%20language%20model%20as%20both%20a%20biomedical%20knowledge%20extractor%0Aand%20an%20adaptive%20optimizer%20to%20automatically%20generate%20a%20diverse%20ensemble%20of%0Ainterpretable%2C%20natural-language%20prompt%20pairs%20for%20disease%20diagnosis.%20Experiments%0Aon%20multiple%20biomedical%20benchmarks%20show%20that%20BiomedXPro%20consistently%20outperforms%0Astate-of-the-art%20prompt-tuning%20methods%2C%20particularly%20in%20data-scarce%20few-shot%0Asettings.%20Furthermore%2C%20our%20analysis%20demonstrates%20a%20strong%20semantic%20alignment%0Abetween%20the%20discovered%20prompts%20and%20statistically%20significant%20clinical%20features%2C%0Agrounding%20the%20model%27s%20performance%20in%20verifiable%20concepts.%20By%20producing%20a%0Adiverse%20ensemble%20of%20interpretable%20prompts%2C%20BiomedXPro%20provides%20a%20verifiable%0Abasis%20for%20model%20predictions%2C%20representing%20a%20critical%20step%20toward%20the%0Adevelopment%20of%20more%20trustworthy%20and%20clinically-aligned%20AI%20systems.%0A&entry.1838667208=http%3A//arxiv.org/abs/2510.15866v1&entry.124074799=Read"},
{"title": "How Sparse Can We Prune A Deep Network: A Fundamental Limit Perspective", "author": "Qiaozhe Zhang and Ruijie Zhang and Jun Sun and Yingzhuang Liu", "abstract": "  Network pruning is a commonly used measure to alleviate the storage and\ncomputational burden of deep neural networks. However, the fundamental limit of\nnetwork pruning is still lacking. To close the gap, in this work we'll take a\nfirst-principles approach, i.e. we'll directly impose the sparsity constraint\non the loss function and leverage the framework of statistical dimension in\nconvex geometry, thus enabling us to characterize the sharp phase transition\npoint, which can be regarded as the fundamental limit of the pruning ratio.\nThrough this limit, we're able to identify two key factors that determine the\npruning ratio limit, namely, weight magnitude and network sharpness. Generally\nspeaking, the flatter the loss landscape or the smaller the weight magnitude,\nthe smaller pruning ratio. Moreover, we provide efficient countermeasures to\naddress the challenges in the computation of the pruning limit, which mainly\ninvolves the accurate spectrum estimation of a large-scale and non-positive\nHessian matrix. Moreover, through the lens of the pruning ratio threshold, we\ncan also provide rigorous interpretations on several heuristics in existing\npruning algorithms. Extensive experiments are performed which demonstrate that\nour theoretical pruning ratio threshold coincides very well with the\nexperiments. All codes are available at:\nhttps://github.com/QiaozheZhang/Global-One-shot-Pruning\n", "link": "http://arxiv.org/abs/2306.05857v5", "date": "2025-10-17", "relevancy": 2.244, "topK": [{"title": "ToDo: Token Downsampling for Efficient Generation of High-Resolution\n  Images", "link": "http://arxiv.org/abs/2402.13573v2", "similarity": 0.4669}, {"title": "Self-supervised Photographic Image Layout Representation Learning", "link": "http://arxiv.org/abs/2403.03740v1", "similarity": 0.4427}, {"title": "Boosted dynamic neural networks", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/26302", "similarity": 0.4369}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20How%20Sparse%20Can%20We%20Prune%20A%20Deep%20Network%3A%20A%20Fundamental%20Limit%20Perspective&body=Title%3A%20How%20Sparse%20Can%20We%20Prune%20A%20Deep%20Network%3A%20A%20Fundamental%20Limit%20Perspective%0AAuthor%3A%20Qiaozhe%20Zhang%20and%20Ruijie%20Zhang%20and%20Jun%20Sun%20and%20Yingzhuang%20Liu%0AAbstract%3A%20%20%20Network%20pruning%20is%20a%20commonly%20used%20measure%20to%20alleviate%20the%20storage%20and%0Acomputational%20burden%20of%20deep%20neural%20networks.%20However%2C%20the%20fundamental%20limit%20of%0Anetwork%20pruning%20is%20still%20lacking.%20To%20close%20the%20gap%2C%20in%20this%20work%20we%27ll%20take%20a%0Afirst-principles%20approach%2C%20i.e.%20we%27ll%20directly%20impose%20the%20sparsity%20constraint%0Aon%20the%20loss%20function%20and%20leverage%20the%20framework%20of%20statistical%20dimension%20in%0Aconvex%20geometry%2C%20thus%20enabling%20us%20to%20characterize%20the%20sharp%20phase%20transition%0Apoint%2C%20which%20can%20be%20regarded%20as%20the%20fundamental%20limit%20of%20the%20pruning%20ratio.%0AThrough%20this%20limit%2C%20we%27re%20able%20to%20identify%20two%20key%20factors%20that%20determine%20the%0Apruning%20ratio%20limit%2C%20namely%2C%20weight%20magnitude%20and%20network%20sharpness.%20Generally%0Aspeaking%2C%20the%20flatter%20the%20loss%20landscape%20or%20the%20smaller%20the%20weight%20magnitude%2C%0Athe%20smaller%20pruning%20ratio.%20Moreover%2C%20we%20provide%20efficient%20countermeasures%20to%0Aaddress%20the%20challenges%20in%20the%20computation%20of%20the%20pruning%20limit%2C%20which%20mainly%0Ainvolves%20the%20accurate%20spectrum%20estimation%20of%20a%20large-scale%20and%20non-positive%0AHessian%20matrix.%20Moreover%2C%20through%20the%20lens%20of%20the%20pruning%20ratio%20threshold%2C%20we%0Acan%20also%20provide%20rigorous%20interpretations%20on%20several%20heuristics%20in%20existing%0Apruning%20algorithms.%20Extensive%20experiments%20are%20performed%20which%20demonstrate%20that%0Aour%20theoretical%20pruning%20ratio%20threshold%20coincides%20very%20well%20with%20the%0Aexperiments.%20All%20codes%20are%20available%20at%3A%0Ahttps%3A//github.com/QiaozheZhang/Global-One-shot-Pruning%0A%0ALink%3A%20http%3A//arxiv.org/abs/2306.05857v5%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DHow%2520Sparse%2520Can%2520We%2520Prune%2520A%2520Deep%2520Network%253A%2520A%2520Fundamental%2520Limit%2520Perspective%26entry.906535625%3DQiaozhe%2520Zhang%2520and%2520Ruijie%2520Zhang%2520and%2520Jun%2520Sun%2520and%2520Yingzhuang%2520Liu%26entry.1292438233%3D%2520%2520Network%2520pruning%2520is%2520a%2520commonly%2520used%2520measure%2520to%2520alleviate%2520the%2520storage%2520and%250Acomputational%2520burden%2520of%2520deep%2520neural%2520networks.%2520However%252C%2520the%2520fundamental%2520limit%2520of%250Anetwork%2520pruning%2520is%2520still%2520lacking.%2520To%2520close%2520the%2520gap%252C%2520in%2520this%2520work%2520we%2527ll%2520take%2520a%250Afirst-principles%2520approach%252C%2520i.e.%2520we%2527ll%2520directly%2520impose%2520the%2520sparsity%2520constraint%250Aon%2520the%2520loss%2520function%2520and%2520leverage%2520the%2520framework%2520of%2520statistical%2520dimension%2520in%250Aconvex%2520geometry%252C%2520thus%2520enabling%2520us%2520to%2520characterize%2520the%2520sharp%2520phase%2520transition%250Apoint%252C%2520which%2520can%2520be%2520regarded%2520as%2520the%2520fundamental%2520limit%2520of%2520the%2520pruning%2520ratio.%250AThrough%2520this%2520limit%252C%2520we%2527re%2520able%2520to%2520identify%2520two%2520key%2520factors%2520that%2520determine%2520the%250Apruning%2520ratio%2520limit%252C%2520namely%252C%2520weight%2520magnitude%2520and%2520network%2520sharpness.%2520Generally%250Aspeaking%252C%2520the%2520flatter%2520the%2520loss%2520landscape%2520or%2520the%2520smaller%2520the%2520weight%2520magnitude%252C%250Athe%2520smaller%2520pruning%2520ratio.%2520Moreover%252C%2520we%2520provide%2520efficient%2520countermeasures%2520to%250Aaddress%2520the%2520challenges%2520in%2520the%2520computation%2520of%2520the%2520pruning%2520limit%252C%2520which%2520mainly%250Ainvolves%2520the%2520accurate%2520spectrum%2520estimation%2520of%2520a%2520large-scale%2520and%2520non-positive%250AHessian%2520matrix.%2520Moreover%252C%2520through%2520the%2520lens%2520of%2520the%2520pruning%2520ratio%2520threshold%252C%2520we%250Acan%2520also%2520provide%2520rigorous%2520interpretations%2520on%2520several%2520heuristics%2520in%2520existing%250Apruning%2520algorithms.%2520Extensive%2520experiments%2520are%2520performed%2520which%2520demonstrate%2520that%250Aour%2520theoretical%2520pruning%2520ratio%2520threshold%2520coincides%2520very%2520well%2520with%2520the%250Aexperiments.%2520All%2520codes%2520are%2520available%2520at%253A%250Ahttps%253A//github.com/QiaozheZhang/Global-One-shot-Pruning%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2306.05857v5%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=How%20Sparse%20Can%20We%20Prune%20A%20Deep%20Network%3A%20A%20Fundamental%20Limit%20Perspective&entry.906535625=Qiaozhe%20Zhang%20and%20Ruijie%20Zhang%20and%20Jun%20Sun%20and%20Yingzhuang%20Liu&entry.1292438233=%20%20Network%20pruning%20is%20a%20commonly%20used%20measure%20to%20alleviate%20the%20storage%20and%0Acomputational%20burden%20of%20deep%20neural%20networks.%20However%2C%20the%20fundamental%20limit%20of%0Anetwork%20pruning%20is%20still%20lacking.%20To%20close%20the%20gap%2C%20in%20this%20work%20we%27ll%20take%20a%0Afirst-principles%20approach%2C%20i.e.%20we%27ll%20directly%20impose%20the%20sparsity%20constraint%0Aon%20the%20loss%20function%20and%20leverage%20the%20framework%20of%20statistical%20dimension%20in%0Aconvex%20geometry%2C%20thus%20enabling%20us%20to%20characterize%20the%20sharp%20phase%20transition%0Apoint%2C%20which%20can%20be%20regarded%20as%20the%20fundamental%20limit%20of%20the%20pruning%20ratio.%0AThrough%20this%20limit%2C%20we%27re%20able%20to%20identify%20two%20key%20factors%20that%20determine%20the%0Apruning%20ratio%20limit%2C%20namely%2C%20weight%20magnitude%20and%20network%20sharpness.%20Generally%0Aspeaking%2C%20the%20flatter%20the%20loss%20landscape%20or%20the%20smaller%20the%20weight%20magnitude%2C%0Athe%20smaller%20pruning%20ratio.%20Moreover%2C%20we%20provide%20efficient%20countermeasures%20to%0Aaddress%20the%20challenges%20in%20the%20computation%20of%20the%20pruning%20limit%2C%20which%20mainly%0Ainvolves%20the%20accurate%20spectrum%20estimation%20of%20a%20large-scale%20and%20non-positive%0AHessian%20matrix.%20Moreover%2C%20through%20the%20lens%20of%20the%20pruning%20ratio%20threshold%2C%20we%0Acan%20also%20provide%20rigorous%20interpretations%20on%20several%20heuristics%20in%20existing%0Apruning%20algorithms.%20Extensive%20experiments%20are%20performed%20which%20demonstrate%20that%0Aour%20theoretical%20pruning%20ratio%20threshold%20coincides%20very%20well%20with%20the%0Aexperiments.%20All%20codes%20are%20available%20at%3A%0Ahttps%3A//github.com/QiaozheZhang/Global-One-shot-Pruning%0A&entry.1838667208=http%3A//arxiv.org/abs/2306.05857v5&entry.124074799=Read"},
{"title": "Towards more holistic interpretability: A lightweight disentangled\n  Concept Bottleneck Model", "author": "Gaoxiang Huang and Songning Lai and Yutao Yue", "abstract": "  Concept Bottleneck Models (CBMs) enhance interpretability by predicting\nhuman-understandable concepts as intermediate representations. However,\nexisting CBMs often suffer from input-to-concept mapping bias and limited\ncontrollability, which restricts their practical value, directly damage the\nresponsibility of strategy from concept-based methods. We propose a lightweight\nDisentangled Concept Bottleneck Model (LDCBM) that automatically groups visual\nfeatures into semantically meaningful components without region annotation. By\nintroducing a filter grouping loss and joint concept supervision, our method\nimproves the alignment between visual patterns and concepts, enabling more\ntransparent and robust decision-making. Notably, Experiments on three diverse\ndatasets demonstrate that LDCBM achieves higher concept and class accuracy,\noutperforming previous CBMs in both interpretability and classification\nperformance. By grounding concepts in visual evidence, our method overcomes a\nfundamental limitation of prior models and enhances the reliability of\ninterpretable AI.\n", "link": "http://arxiv.org/abs/2510.15770v1", "date": "2025-10-17", "relevancy": 2.2319, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5657}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5657}, {"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.5195}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Towards%20more%20holistic%20interpretability%3A%20A%20lightweight%20disentangled%0A%20%20Concept%20Bottleneck%20Model&body=Title%3A%20Towards%20more%20holistic%20interpretability%3A%20A%20lightweight%20disentangled%0A%20%20Concept%20Bottleneck%20Model%0AAuthor%3A%20Gaoxiang%20Huang%20and%20Songning%20Lai%20and%20Yutao%20Yue%0AAbstract%3A%20%20%20Concept%20Bottleneck%20Models%20%28CBMs%29%20enhance%20interpretability%20by%20predicting%0Ahuman-understandable%20concepts%20as%20intermediate%20representations.%20However%2C%0Aexisting%20CBMs%20often%20suffer%20from%20input-to-concept%20mapping%20bias%20and%20limited%0Acontrollability%2C%20which%20restricts%20their%20practical%20value%2C%20directly%20damage%20the%0Aresponsibility%20of%20strategy%20from%20concept-based%20methods.%20We%20propose%20a%20lightweight%0ADisentangled%20Concept%20Bottleneck%20Model%20%28LDCBM%29%20that%20automatically%20groups%20visual%0Afeatures%20into%20semantically%20meaningful%20components%20without%20region%20annotation.%20By%0Aintroducing%20a%20filter%20grouping%20loss%20and%20joint%20concept%20supervision%2C%20our%20method%0Aimproves%20the%20alignment%20between%20visual%20patterns%20and%20concepts%2C%20enabling%20more%0Atransparent%20and%20robust%20decision-making.%20Notably%2C%20Experiments%20on%20three%20diverse%0Adatasets%20demonstrate%20that%20LDCBM%20achieves%20higher%20concept%20and%20class%20accuracy%2C%0Aoutperforming%20previous%20CBMs%20in%20both%20interpretability%20and%20classification%0Aperformance.%20By%20grounding%20concepts%20in%20visual%20evidence%2C%20our%20method%20overcomes%20a%0Afundamental%20limitation%20of%20prior%20models%20and%20enhances%20the%20reliability%20of%0Ainterpretable%20AI.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2510.15770v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DTowards%2520more%2520holistic%2520interpretability%253A%2520A%2520lightweight%2520disentangled%250A%2520%2520Concept%2520Bottleneck%2520Model%26entry.906535625%3DGaoxiang%2520Huang%2520and%2520Songning%2520Lai%2520and%2520Yutao%2520Yue%26entry.1292438233%3D%2520%2520Concept%2520Bottleneck%2520Models%2520%2528CBMs%2529%2520enhance%2520interpretability%2520by%2520predicting%250Ahuman-understandable%2520concepts%2520as%2520intermediate%2520representations.%2520However%252C%250Aexisting%2520CBMs%2520often%2520suffer%2520from%2520input-to-concept%2520mapping%2520bias%2520and%2520limited%250Acontrollability%252C%2520which%2520restricts%2520their%2520practical%2520value%252C%2520directly%2520damage%2520the%250Aresponsibility%2520of%2520strategy%2520from%2520concept-based%2520methods.%2520We%2520propose%2520a%2520lightweight%250ADisentangled%2520Concept%2520Bottleneck%2520Model%2520%2528LDCBM%2529%2520that%2520automatically%2520groups%2520visual%250Afeatures%2520into%2520semantically%2520meaningful%2520components%2520without%2520region%2520annotation.%2520By%250Aintroducing%2520a%2520filter%2520grouping%2520loss%2520and%2520joint%2520concept%2520supervision%252C%2520our%2520method%250Aimproves%2520the%2520alignment%2520between%2520visual%2520patterns%2520and%2520concepts%252C%2520enabling%2520more%250Atransparent%2520and%2520robust%2520decision-making.%2520Notably%252C%2520Experiments%2520on%2520three%2520diverse%250Adatasets%2520demonstrate%2520that%2520LDCBM%2520achieves%2520higher%2520concept%2520and%2520class%2520accuracy%252C%250Aoutperforming%2520previous%2520CBMs%2520in%2520both%2520interpretability%2520and%2520classification%250Aperformance.%2520By%2520grounding%2520concepts%2520in%2520visual%2520evidence%252C%2520our%2520method%2520overcomes%2520a%250Afundamental%2520limitation%2520of%2520prior%2520models%2520and%2520enhances%2520the%2520reliability%2520of%250Ainterpretable%2520AI.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2510.15770v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Towards%20more%20holistic%20interpretability%3A%20A%20lightweight%20disentangled%0A%20%20Concept%20Bottleneck%20Model&entry.906535625=Gaoxiang%20Huang%20and%20Songning%20Lai%20and%20Yutao%20Yue&entry.1292438233=%20%20Concept%20Bottleneck%20Models%20%28CBMs%29%20enhance%20interpretability%20by%20predicting%0Ahuman-understandable%20concepts%20as%20intermediate%20representations.%20However%2C%0Aexisting%20CBMs%20often%20suffer%20from%20input-to-concept%20mapping%20bias%20and%20limited%0Acontrollability%2C%20which%20restricts%20their%20practical%20value%2C%20directly%20damage%20the%0Aresponsibility%20of%20strategy%20from%20concept-based%20methods.%20We%20propose%20a%20lightweight%0ADisentangled%20Concept%20Bottleneck%20Model%20%28LDCBM%29%20that%20automatically%20groups%20visual%0Afeatures%20into%20semantically%20meaningful%20components%20without%20region%20annotation.%20By%0Aintroducing%20a%20filter%20grouping%20loss%20and%20joint%20concept%20supervision%2C%20our%20method%0Aimproves%20the%20alignment%20between%20visual%20patterns%20and%20concepts%2C%20enabling%20more%0Atransparent%20and%20robust%20decision-making.%20Notably%2C%20Experiments%20on%20three%20diverse%0Adatasets%20demonstrate%20that%20LDCBM%20achieves%20higher%20concept%20and%20class%20accuracy%2C%0Aoutperforming%20previous%20CBMs%20in%20both%20interpretability%20and%20classification%0Aperformance.%20By%20grounding%20concepts%20in%20visual%20evidence%2C%20our%20method%20overcomes%20a%0Afundamental%20limitation%20of%20prior%20models%20and%20enhances%20the%20reliability%20of%0Ainterpretable%20AI.%0A&entry.1838667208=http%3A//arxiv.org/abs/2510.15770v1&entry.124074799=Read"},
{"title": "Towards Label-Free Brain Tumor Segmentation: Unsupervised Learning with\n  Multimodal MRI", "author": "Gerard Comas-Quiles and Carles Garcia-Cabrera and Julia Dietlmeier and Noel E. O'Connor and Ferran Marques", "abstract": "  Unsupervised anomaly detection (UAD) presents a complementary alternative to\nsupervised learning for brain tumor segmentation in magnetic resonance imaging\n(MRI), particularly when annotated datasets are limited, costly, or\ninconsistent. In this work, we propose a novel Multimodal Vision Transformer\nAutoencoder (MViT-AE) trained exclusively on healthy brain MRIs to detect and\nlocalize tumors via reconstruction-based error maps. This unsupervised paradigm\nenables segmentation without reliance on manual labels, addressing a key\nscalability bottleneck in neuroimaging workflows. Our method is evaluated in\nthe BraTS-GoAT 2025 Lighthouse dataset, which includes various types of tumors\nsuch as gliomas, meningiomas, and pediatric brain tumors. To enhance\nperformance, we introduce a multimodal early-late fusion strategy that\nleverages complementary information across multiple MRI sequences, and a\npost-processing pipeline that integrates the Segment Anything Model (SAM) to\nrefine predicted tumor contours. Despite the known challenges of UAD,\nparticularly in detecting small or non-enhancing lesions, our method achieves\nclinically meaningful tumor localization, with lesion-wise Dice Similarity\nCoefficient of 0.437 (Whole Tumor), 0.316 (Tumor Core), and 0.350 (Enhancing\nTumor) on the test set, and an anomaly Detection Rate of 89.4% on the\nvalidation set. These findings highlight the potential of transformer-based\nunsupervised models to serve as scalable, label-efficient tools for\nneuro-oncological imaging.\n", "link": "http://arxiv.org/abs/2510.15684v1", "date": "2025-10-17", "relevancy": 2.2043, "topK": [{"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.5824}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5692}, {"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.5204}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Towards%20Label-Free%20Brain%20Tumor%20Segmentation%3A%20Unsupervised%20Learning%20with%0A%20%20Multimodal%20MRI&body=Title%3A%20Towards%20Label-Free%20Brain%20Tumor%20Segmentation%3A%20Unsupervised%20Learning%20with%0A%20%20Multimodal%20MRI%0AAuthor%3A%20Gerard%20Comas-Quiles%20and%20Carles%20Garcia-Cabrera%20and%20Julia%20Dietlmeier%20and%20Noel%20E.%20O%27Connor%20and%20Ferran%20Marques%0AAbstract%3A%20%20%20Unsupervised%20anomaly%20detection%20%28UAD%29%20presents%20a%20complementary%20alternative%20to%0Asupervised%20learning%20for%20brain%20tumor%20segmentation%20in%20magnetic%20resonance%20imaging%0A%28MRI%29%2C%20particularly%20when%20annotated%20datasets%20are%20limited%2C%20costly%2C%20or%0Ainconsistent.%20In%20this%20work%2C%20we%20propose%20a%20novel%20Multimodal%20Vision%20Transformer%0AAutoencoder%20%28MViT-AE%29%20trained%20exclusively%20on%20healthy%20brain%20MRIs%20to%20detect%20and%0Alocalize%20tumors%20via%20reconstruction-based%20error%20maps.%20This%20unsupervised%20paradigm%0Aenables%20segmentation%20without%20reliance%20on%20manual%20labels%2C%20addressing%20a%20key%0Ascalability%20bottleneck%20in%20neuroimaging%20workflows.%20Our%20method%20is%20evaluated%20in%0Athe%20BraTS-GoAT%202025%20Lighthouse%20dataset%2C%20which%20includes%20various%20types%20of%20tumors%0Asuch%20as%20gliomas%2C%20meningiomas%2C%20and%20pediatric%20brain%20tumors.%20To%20enhance%0Aperformance%2C%20we%20introduce%20a%20multimodal%20early-late%20fusion%20strategy%20that%0Aleverages%20complementary%20information%20across%20multiple%20MRI%20sequences%2C%20and%20a%0Apost-processing%20pipeline%20that%20integrates%20the%20Segment%20Anything%20Model%20%28SAM%29%20to%0Arefine%20predicted%20tumor%20contours.%20Despite%20the%20known%20challenges%20of%20UAD%2C%0Aparticularly%20in%20detecting%20small%20or%20non-enhancing%20lesions%2C%20our%20method%20achieves%0Aclinically%20meaningful%20tumor%20localization%2C%20with%20lesion-wise%20Dice%20Similarity%0ACoefficient%20of%200.437%20%28Whole%20Tumor%29%2C%200.316%20%28Tumor%20Core%29%2C%20and%200.350%20%28Enhancing%0ATumor%29%20on%20the%20test%20set%2C%20and%20an%20anomaly%20Detection%20Rate%20of%2089.4%25%20on%20the%0Avalidation%20set.%20These%20findings%20highlight%20the%20potential%20of%20transformer-based%0Aunsupervised%20models%20to%20serve%20as%20scalable%2C%20label-efficient%20tools%20for%0Aneuro-oncological%20imaging.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2510.15684v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DTowards%2520Label-Free%2520Brain%2520Tumor%2520Segmentation%253A%2520Unsupervised%2520Learning%2520with%250A%2520%2520Multimodal%2520MRI%26entry.906535625%3DGerard%2520Comas-Quiles%2520and%2520Carles%2520Garcia-Cabrera%2520and%2520Julia%2520Dietlmeier%2520and%2520Noel%2520E.%2520O%2527Connor%2520and%2520Ferran%2520Marques%26entry.1292438233%3D%2520%2520Unsupervised%2520anomaly%2520detection%2520%2528UAD%2529%2520presents%2520a%2520complementary%2520alternative%2520to%250Asupervised%2520learning%2520for%2520brain%2520tumor%2520segmentation%2520in%2520magnetic%2520resonance%2520imaging%250A%2528MRI%2529%252C%2520particularly%2520when%2520annotated%2520datasets%2520are%2520limited%252C%2520costly%252C%2520or%250Ainconsistent.%2520In%2520this%2520work%252C%2520we%2520propose%2520a%2520novel%2520Multimodal%2520Vision%2520Transformer%250AAutoencoder%2520%2528MViT-AE%2529%2520trained%2520exclusively%2520on%2520healthy%2520brain%2520MRIs%2520to%2520detect%2520and%250Alocalize%2520tumors%2520via%2520reconstruction-based%2520error%2520maps.%2520This%2520unsupervised%2520paradigm%250Aenables%2520segmentation%2520without%2520reliance%2520on%2520manual%2520labels%252C%2520addressing%2520a%2520key%250Ascalability%2520bottleneck%2520in%2520neuroimaging%2520workflows.%2520Our%2520method%2520is%2520evaluated%2520in%250Athe%2520BraTS-GoAT%25202025%2520Lighthouse%2520dataset%252C%2520which%2520includes%2520various%2520types%2520of%2520tumors%250Asuch%2520as%2520gliomas%252C%2520meningiomas%252C%2520and%2520pediatric%2520brain%2520tumors.%2520To%2520enhance%250Aperformance%252C%2520we%2520introduce%2520a%2520multimodal%2520early-late%2520fusion%2520strategy%2520that%250Aleverages%2520complementary%2520information%2520across%2520multiple%2520MRI%2520sequences%252C%2520and%2520a%250Apost-processing%2520pipeline%2520that%2520integrates%2520the%2520Segment%2520Anything%2520Model%2520%2528SAM%2529%2520to%250Arefine%2520predicted%2520tumor%2520contours.%2520Despite%2520the%2520known%2520challenges%2520of%2520UAD%252C%250Aparticularly%2520in%2520detecting%2520small%2520or%2520non-enhancing%2520lesions%252C%2520our%2520method%2520achieves%250Aclinically%2520meaningful%2520tumor%2520localization%252C%2520with%2520lesion-wise%2520Dice%2520Similarity%250ACoefficient%2520of%25200.437%2520%2528Whole%2520Tumor%2529%252C%25200.316%2520%2528Tumor%2520Core%2529%252C%2520and%25200.350%2520%2528Enhancing%250ATumor%2529%2520on%2520the%2520test%2520set%252C%2520and%2520an%2520anomaly%2520Detection%2520Rate%2520of%252089.4%2525%2520on%2520the%250Avalidation%2520set.%2520These%2520findings%2520highlight%2520the%2520potential%2520of%2520transformer-based%250Aunsupervised%2520models%2520to%2520serve%2520as%2520scalable%252C%2520label-efficient%2520tools%2520for%250Aneuro-oncological%2520imaging.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2510.15684v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Towards%20Label-Free%20Brain%20Tumor%20Segmentation%3A%20Unsupervised%20Learning%20with%0A%20%20Multimodal%20MRI&entry.906535625=Gerard%20Comas-Quiles%20and%20Carles%20Garcia-Cabrera%20and%20Julia%20Dietlmeier%20and%20Noel%20E.%20O%27Connor%20and%20Ferran%20Marques&entry.1292438233=%20%20Unsupervised%20anomaly%20detection%20%28UAD%29%20presents%20a%20complementary%20alternative%20to%0Asupervised%20learning%20for%20brain%20tumor%20segmentation%20in%20magnetic%20resonance%20imaging%0A%28MRI%29%2C%20particularly%20when%20annotated%20datasets%20are%20limited%2C%20costly%2C%20or%0Ainconsistent.%20In%20this%20work%2C%20we%20propose%20a%20novel%20Multimodal%20Vision%20Transformer%0AAutoencoder%20%28MViT-AE%29%20trained%20exclusively%20on%20healthy%20brain%20MRIs%20to%20detect%20and%0Alocalize%20tumors%20via%20reconstruction-based%20error%20maps.%20This%20unsupervised%20paradigm%0Aenables%20segmentation%20without%20reliance%20on%20manual%20labels%2C%20addressing%20a%20key%0Ascalability%20bottleneck%20in%20neuroimaging%20workflows.%20Our%20method%20is%20evaluated%20in%0Athe%20BraTS-GoAT%202025%20Lighthouse%20dataset%2C%20which%20includes%20various%20types%20of%20tumors%0Asuch%20as%20gliomas%2C%20meningiomas%2C%20and%20pediatric%20brain%20tumors.%20To%20enhance%0Aperformance%2C%20we%20introduce%20a%20multimodal%20early-late%20fusion%20strategy%20that%0Aleverages%20complementary%20information%20across%20multiple%20MRI%20sequences%2C%20and%20a%0Apost-processing%20pipeline%20that%20integrates%20the%20Segment%20Anything%20Model%20%28SAM%29%20to%0Arefine%20predicted%20tumor%20contours.%20Despite%20the%20known%20challenges%20of%20UAD%2C%0Aparticularly%20in%20detecting%20small%20or%20non-enhancing%20lesions%2C%20our%20method%20achieves%0Aclinically%20meaningful%20tumor%20localization%2C%20with%20lesion-wise%20Dice%20Similarity%0ACoefficient%20of%200.437%20%28Whole%20Tumor%29%2C%200.316%20%28Tumor%20Core%29%2C%20and%200.350%20%28Enhancing%0ATumor%29%20on%20the%20test%20set%2C%20and%20an%20anomaly%20Detection%20Rate%20of%2089.4%25%20on%20the%0Avalidation%20set.%20These%20findings%20highlight%20the%20potential%20of%20transformer-based%0Aunsupervised%20models%20to%20serve%20as%20scalable%2C%20label-efficient%20tools%20for%0Aneuro-oncological%20imaging.%0A&entry.1838667208=http%3A//arxiv.org/abs/2510.15684v1&entry.124074799=Read"},
{"title": "Enhanced Sentiment Interpretation via a Lexicon-Fuzzy-Transformer\n  Framework", "author": "Shayan Rokhva and Mousa Alizadeh and Maryam Abdollahi Shamami", "abstract": "  Accurately detecting sentiment polarity and intensity in product reviews and\nsocial media posts remains challenging due to informal and domain-specific\nlanguage. To address this, we propose a novel hybrid lexicon-fuzzy-transformer\nframework that combines rule-based heuristics, contextual deep learning, and\nfuzzy logic to generate continuous sentiment scores reflecting both polarity\nand strength. The pipeline begins with VADER-based initial sentiment\nestimations, which are refined through a two-stage adjustment process. This\ninvolves leveraging confidence scores from DistilBERT, a lightweight\ntransformer and applying fuzzy logic principles to mitigate excessive\nneutrality bias and enhance granularity. A custom fuzzy inference system then\nmaps the refined scores onto a 0 to 1 continuum, producing expert)like\njudgments. The framework is rigorously evaluated on four domain-specific\ndatasets. food delivery, e-commerce, tourism, and fashion. Results show\nimproved alignment with user ratings, better identification of sentiment\nextremes, and reduced misclassifications. Both quantitative metrics\n(distributional alignment, confusion matrices) and qualitative insights (case\nstudies, runtime analysis) affirm the models robustness and efficiency. This\nwork demonstrates the value of integrating symbolic reasoning with neural\nmodels for interpretable, finegrained sentiment analysis in linguistically\ndynamic domains.\n", "link": "http://arxiv.org/abs/2510.15843v1", "date": "2025-10-17", "relevancy": 2.2034, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5569}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5569}, {"title": "Scaling Rectified Flow Transformers for High-Resolution Image Synthesis", "link": "http://arxiv.org/abs/2403.03206v1", "similarity": 0.5209}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Enhanced%20Sentiment%20Interpretation%20via%20a%20Lexicon-Fuzzy-Transformer%0A%20%20Framework&body=Title%3A%20Enhanced%20Sentiment%20Interpretation%20via%20a%20Lexicon-Fuzzy-Transformer%0A%20%20Framework%0AAuthor%3A%20Shayan%20Rokhva%20and%20Mousa%20Alizadeh%20and%20Maryam%20Abdollahi%20Shamami%0AAbstract%3A%20%20%20Accurately%20detecting%20sentiment%20polarity%20and%20intensity%20in%20product%20reviews%20and%0Asocial%20media%20posts%20remains%20challenging%20due%20to%20informal%20and%20domain-specific%0Alanguage.%20To%20address%20this%2C%20we%20propose%20a%20novel%20hybrid%20lexicon-fuzzy-transformer%0Aframework%20that%20combines%20rule-based%20heuristics%2C%20contextual%20deep%20learning%2C%20and%0Afuzzy%20logic%20to%20generate%20continuous%20sentiment%20scores%20reflecting%20both%20polarity%0Aand%20strength.%20The%20pipeline%20begins%20with%20VADER-based%20initial%20sentiment%0Aestimations%2C%20which%20are%20refined%20through%20a%20two-stage%20adjustment%20process.%20This%0Ainvolves%20leveraging%20confidence%20scores%20from%20DistilBERT%2C%20a%20lightweight%0Atransformer%20and%20applying%20fuzzy%20logic%20principles%20to%20mitigate%20excessive%0Aneutrality%20bias%20and%20enhance%20granularity.%20A%20custom%20fuzzy%20inference%20system%20then%0Amaps%20the%20refined%20scores%20onto%20a%200%20to%201%20continuum%2C%20producing%20expert%29like%0Ajudgments.%20The%20framework%20is%20rigorously%20evaluated%20on%20four%20domain-specific%0Adatasets.%20food%20delivery%2C%20e-commerce%2C%20tourism%2C%20and%20fashion.%20Results%20show%0Aimproved%20alignment%20with%20user%20ratings%2C%20better%20identification%20of%20sentiment%0Aextremes%2C%20and%20reduced%20misclassifications.%20Both%20quantitative%20metrics%0A%28distributional%20alignment%2C%20confusion%20matrices%29%20and%20qualitative%20insights%20%28case%0Astudies%2C%20runtime%20analysis%29%20affirm%20the%20models%20robustness%20and%20efficiency.%20This%0Awork%20demonstrates%20the%20value%20of%20integrating%20symbolic%20reasoning%20with%20neural%0Amodels%20for%20interpretable%2C%20finegrained%20sentiment%20analysis%20in%20linguistically%0Adynamic%20domains.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2510.15843v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DEnhanced%2520Sentiment%2520Interpretation%2520via%2520a%2520Lexicon-Fuzzy-Transformer%250A%2520%2520Framework%26entry.906535625%3DShayan%2520Rokhva%2520and%2520Mousa%2520Alizadeh%2520and%2520Maryam%2520Abdollahi%2520Shamami%26entry.1292438233%3D%2520%2520Accurately%2520detecting%2520sentiment%2520polarity%2520and%2520intensity%2520in%2520product%2520reviews%2520and%250Asocial%2520media%2520posts%2520remains%2520challenging%2520due%2520to%2520informal%2520and%2520domain-specific%250Alanguage.%2520To%2520address%2520this%252C%2520we%2520propose%2520a%2520novel%2520hybrid%2520lexicon-fuzzy-transformer%250Aframework%2520that%2520combines%2520rule-based%2520heuristics%252C%2520contextual%2520deep%2520learning%252C%2520and%250Afuzzy%2520logic%2520to%2520generate%2520continuous%2520sentiment%2520scores%2520reflecting%2520both%2520polarity%250Aand%2520strength.%2520The%2520pipeline%2520begins%2520with%2520VADER-based%2520initial%2520sentiment%250Aestimations%252C%2520which%2520are%2520refined%2520through%2520a%2520two-stage%2520adjustment%2520process.%2520This%250Ainvolves%2520leveraging%2520confidence%2520scores%2520from%2520DistilBERT%252C%2520a%2520lightweight%250Atransformer%2520and%2520applying%2520fuzzy%2520logic%2520principles%2520to%2520mitigate%2520excessive%250Aneutrality%2520bias%2520and%2520enhance%2520granularity.%2520A%2520custom%2520fuzzy%2520inference%2520system%2520then%250Amaps%2520the%2520refined%2520scores%2520onto%2520a%25200%2520to%25201%2520continuum%252C%2520producing%2520expert%2529like%250Ajudgments.%2520The%2520framework%2520is%2520rigorously%2520evaluated%2520on%2520four%2520domain-specific%250Adatasets.%2520food%2520delivery%252C%2520e-commerce%252C%2520tourism%252C%2520and%2520fashion.%2520Results%2520show%250Aimproved%2520alignment%2520with%2520user%2520ratings%252C%2520better%2520identification%2520of%2520sentiment%250Aextremes%252C%2520and%2520reduced%2520misclassifications.%2520Both%2520quantitative%2520metrics%250A%2528distributional%2520alignment%252C%2520confusion%2520matrices%2529%2520and%2520qualitative%2520insights%2520%2528case%250Astudies%252C%2520runtime%2520analysis%2529%2520affirm%2520the%2520models%2520robustness%2520and%2520efficiency.%2520This%250Awork%2520demonstrates%2520the%2520value%2520of%2520integrating%2520symbolic%2520reasoning%2520with%2520neural%250Amodels%2520for%2520interpretable%252C%2520finegrained%2520sentiment%2520analysis%2520in%2520linguistically%250Adynamic%2520domains.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2510.15843v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Enhanced%20Sentiment%20Interpretation%20via%20a%20Lexicon-Fuzzy-Transformer%0A%20%20Framework&entry.906535625=Shayan%20Rokhva%20and%20Mousa%20Alizadeh%20and%20Maryam%20Abdollahi%20Shamami&entry.1292438233=%20%20Accurately%20detecting%20sentiment%20polarity%20and%20intensity%20in%20product%20reviews%20and%0Asocial%20media%20posts%20remains%20challenging%20due%20to%20informal%20and%20domain-specific%0Alanguage.%20To%20address%20this%2C%20we%20propose%20a%20novel%20hybrid%20lexicon-fuzzy-transformer%0Aframework%20that%20combines%20rule-based%20heuristics%2C%20contextual%20deep%20learning%2C%20and%0Afuzzy%20logic%20to%20generate%20continuous%20sentiment%20scores%20reflecting%20both%20polarity%0Aand%20strength.%20The%20pipeline%20begins%20with%20VADER-based%20initial%20sentiment%0Aestimations%2C%20which%20are%20refined%20through%20a%20two-stage%20adjustment%20process.%20This%0Ainvolves%20leveraging%20confidence%20scores%20from%20DistilBERT%2C%20a%20lightweight%0Atransformer%20and%20applying%20fuzzy%20logic%20principles%20to%20mitigate%20excessive%0Aneutrality%20bias%20and%20enhance%20granularity.%20A%20custom%20fuzzy%20inference%20system%20then%0Amaps%20the%20refined%20scores%20onto%20a%200%20to%201%20continuum%2C%20producing%20expert%29like%0Ajudgments.%20The%20framework%20is%20rigorously%20evaluated%20on%20four%20domain-specific%0Adatasets.%20food%20delivery%2C%20e-commerce%2C%20tourism%2C%20and%20fashion.%20Results%20show%0Aimproved%20alignment%20with%20user%20ratings%2C%20better%20identification%20of%20sentiment%0Aextremes%2C%20and%20reduced%20misclassifications.%20Both%20quantitative%20metrics%0A%28distributional%20alignment%2C%20confusion%20matrices%29%20and%20qualitative%20insights%20%28case%0Astudies%2C%20runtime%20analysis%29%20affirm%20the%20models%20robustness%20and%20efficiency.%20This%0Awork%20demonstrates%20the%20value%20of%20integrating%20symbolic%20reasoning%20with%20neural%0Amodels%20for%20interpretable%2C%20finegrained%20sentiment%20analysis%20in%20linguistically%0Adynamic%20domains.%0A&entry.1838667208=http%3A//arxiv.org/abs/2510.15843v1&entry.124074799=Read"},
{"title": "FlexiReID: Adaptive Mixture of Expert for Multi-Modal Person\n  Re-Identification", "author": "Zhen Sun and Lei Tan and Yunhang Shen and Chengmao Cai and Xing Sun and Pingyang Dai and Liujuan Cao and Rongrong Ji", "abstract": "  Multimodal person re-identification (Re-ID) aims to match pedestrian images\nacross different modalities. However, most existing methods focus on limited\ncross-modal settings and fail to support arbitrary query-retrieval\ncombinations, hindering practical deployment. We propose FlexiReID, a flexible\nframework that supports seven retrieval modes across four modalities: rgb,\ninfrared, sketches, and text. FlexiReID introduces an adaptive\nmixture-of-experts (MoE) mechanism to dynamically integrate diverse modality\nfeatures and a cross-modal query fusion module to enhance multimodal feature\nextraction. To facilitate comprehensive evaluation, we construct CIRS-PEDES, a\nunified dataset extending four popular Re-ID datasets to include all four\nmodalities. Extensive experiments demonstrate that FlexiReID achieves\nstate-of-the-art performance and offers strong generalization in complex\nscenarios.\n", "link": "http://arxiv.org/abs/2510.15595v1", "date": "2025-10-17", "relevancy": 2.1925, "topK": [{"title": "Flexible Visual Recognition by Evidential Modeling of Confusion and Ignorance", "link": "http://openaccess.thecvf.com/content/ICCV2023/html/Fan_Flexible_Visual_Recognition_by_Evidential_Modeling_of_Confusion_and_Ignorance_ICCV_2023_paper.html", "similarity": 0.5794}, {"title": "Magic-Me: Identity-Specific Video Customized Diffusion", "link": "http://arxiv.org/abs/2402.09368v1", "similarity": 0.5311}, {"title": "Multimodal-Conditioned Latent Diffusion Models for Fashion Image Editing", "link": "http://arxiv.org/abs/2403.14828v2", "similarity": 0.5126}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20FlexiReID%3A%20Adaptive%20Mixture%20of%20Expert%20for%20Multi-Modal%20Person%0A%20%20Re-Identification&body=Title%3A%20FlexiReID%3A%20Adaptive%20Mixture%20of%20Expert%20for%20Multi-Modal%20Person%0A%20%20Re-Identification%0AAuthor%3A%20Zhen%20Sun%20and%20Lei%20Tan%20and%20Yunhang%20Shen%20and%20Chengmao%20Cai%20and%20Xing%20Sun%20and%20Pingyang%20Dai%20and%20Liujuan%20Cao%20and%20Rongrong%20Ji%0AAbstract%3A%20%20%20Multimodal%20person%20re-identification%20%28Re-ID%29%20aims%20to%20match%20pedestrian%20images%0Aacross%20different%20modalities.%20However%2C%20most%20existing%20methods%20focus%20on%20limited%0Across-modal%20settings%20and%20fail%20to%20support%20arbitrary%20query-retrieval%0Acombinations%2C%20hindering%20practical%20deployment.%20We%20propose%20FlexiReID%2C%20a%20flexible%0Aframework%20that%20supports%20seven%20retrieval%20modes%20across%20four%20modalities%3A%20rgb%2C%0Ainfrared%2C%20sketches%2C%20and%20text.%20FlexiReID%20introduces%20an%20adaptive%0Amixture-of-experts%20%28MoE%29%20mechanism%20to%20dynamically%20integrate%20diverse%20modality%0Afeatures%20and%20a%20cross-modal%20query%20fusion%20module%20to%20enhance%20multimodal%20feature%0Aextraction.%20To%20facilitate%20comprehensive%20evaluation%2C%20we%20construct%20CIRS-PEDES%2C%20a%0Aunified%20dataset%20extending%20four%20popular%20Re-ID%20datasets%20to%20include%20all%20four%0Amodalities.%20Extensive%20experiments%20demonstrate%20that%20FlexiReID%20achieves%0Astate-of-the-art%20performance%20and%20offers%20strong%20generalization%20in%20complex%0Ascenarios.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2510.15595v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DFlexiReID%253A%2520Adaptive%2520Mixture%2520of%2520Expert%2520for%2520Multi-Modal%2520Person%250A%2520%2520Re-Identification%26entry.906535625%3DZhen%2520Sun%2520and%2520Lei%2520Tan%2520and%2520Yunhang%2520Shen%2520and%2520Chengmao%2520Cai%2520and%2520Xing%2520Sun%2520and%2520Pingyang%2520Dai%2520and%2520Liujuan%2520Cao%2520and%2520Rongrong%2520Ji%26entry.1292438233%3D%2520%2520Multimodal%2520person%2520re-identification%2520%2528Re-ID%2529%2520aims%2520to%2520match%2520pedestrian%2520images%250Aacross%2520different%2520modalities.%2520However%252C%2520most%2520existing%2520methods%2520focus%2520on%2520limited%250Across-modal%2520settings%2520and%2520fail%2520to%2520support%2520arbitrary%2520query-retrieval%250Acombinations%252C%2520hindering%2520practical%2520deployment.%2520We%2520propose%2520FlexiReID%252C%2520a%2520flexible%250Aframework%2520that%2520supports%2520seven%2520retrieval%2520modes%2520across%2520four%2520modalities%253A%2520rgb%252C%250Ainfrared%252C%2520sketches%252C%2520and%2520text.%2520FlexiReID%2520introduces%2520an%2520adaptive%250Amixture-of-experts%2520%2528MoE%2529%2520mechanism%2520to%2520dynamically%2520integrate%2520diverse%2520modality%250Afeatures%2520and%2520a%2520cross-modal%2520query%2520fusion%2520module%2520to%2520enhance%2520multimodal%2520feature%250Aextraction.%2520To%2520facilitate%2520comprehensive%2520evaluation%252C%2520we%2520construct%2520CIRS-PEDES%252C%2520a%250Aunified%2520dataset%2520extending%2520four%2520popular%2520Re-ID%2520datasets%2520to%2520include%2520all%2520four%250Amodalities.%2520Extensive%2520experiments%2520demonstrate%2520that%2520FlexiReID%2520achieves%250Astate-of-the-art%2520performance%2520and%2520offers%2520strong%2520generalization%2520in%2520complex%250Ascenarios.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2510.15595v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=FlexiReID%3A%20Adaptive%20Mixture%20of%20Expert%20for%20Multi-Modal%20Person%0A%20%20Re-Identification&entry.906535625=Zhen%20Sun%20and%20Lei%20Tan%20and%20Yunhang%20Shen%20and%20Chengmao%20Cai%20and%20Xing%20Sun%20and%20Pingyang%20Dai%20and%20Liujuan%20Cao%20and%20Rongrong%20Ji&entry.1292438233=%20%20Multimodal%20person%20re-identification%20%28Re-ID%29%20aims%20to%20match%20pedestrian%20images%0Aacross%20different%20modalities.%20However%2C%20most%20existing%20methods%20focus%20on%20limited%0Across-modal%20settings%20and%20fail%20to%20support%20arbitrary%20query-retrieval%0Acombinations%2C%20hindering%20practical%20deployment.%20We%20propose%20FlexiReID%2C%20a%20flexible%0Aframework%20that%20supports%20seven%20retrieval%20modes%20across%20four%20modalities%3A%20rgb%2C%0Ainfrared%2C%20sketches%2C%20and%20text.%20FlexiReID%20introduces%20an%20adaptive%0Amixture-of-experts%20%28MoE%29%20mechanism%20to%20dynamically%20integrate%20diverse%20modality%0Afeatures%20and%20a%20cross-modal%20query%20fusion%20module%20to%20enhance%20multimodal%20feature%0Aextraction.%20To%20facilitate%20comprehensive%20evaluation%2C%20we%20construct%20CIRS-PEDES%2C%20a%0Aunified%20dataset%20extending%20four%20popular%20Re-ID%20datasets%20to%20include%20all%20four%0Amodalities.%20Extensive%20experiments%20demonstrate%20that%20FlexiReID%20achieves%0Astate-of-the-art%20performance%20and%20offers%20strong%20generalization%20in%20complex%0Ascenarios.%0A&entry.1838667208=http%3A//arxiv.org/abs/2510.15595v1&entry.124074799=Read"},
{"title": "A Multimodal Deep Learning Approach for White Matter Shape Prediction in\n  Diffusion MRI Tractography", "author": "Yui Lo and Yuqian Chen and Dongnan Liu and Leo Zekelman and Jarrett Rushmore and Yogesh Rathi and Nikos Makris and Alexandra J. Golby and Fan Zhang and Weidong Cai and Lauren J. O'Donnell", "abstract": "  Shape measures have emerged as promising descriptors of white matter\ntractography, offering complementary insights into anatomical variability and\nassociations with cognitive and clinical phenotypes. However, conventional\nmethods for computing shape measures are computationally expensive and\ntime-consuming for large-scale datasets due to reliance on voxel-based\nrepresentations. We propose Tract2Shape, a novel multimodal deep learning\nframework that leverages geometric (point cloud) and scalar (tabular) features\nto predict ten white matter tractography shape measures. To enhance model\nefficiency, we utilize a dimensionality reduction algorithm for the model to\npredict five primary shape components. The model is trained and evaluated on\ntwo independently acquired datasets, the HCP-YA dataset, and the PPMI dataset.\nWe evaluate the performance of Tract2Shape by training and testing it on the\nHCP-YA dataset and comparing the results with state-of-the-art models. To\nfurther assess its robustness and generalization ability, we also test\nTract2Shape on the unseen PPMI dataset. Tract2Shape outperforms SOTA deep\nlearning models across all ten shape measures, achieving the highest average\nPearson's r and the lowest nMSE on the HCP-YA dataset. The ablation study shows\nthat both multimodal input and PCA contribute to performance gains. On the\nunseen testing PPMI dataset, Tract2Shape maintains a high Pearson's r and low\nnMSE, demonstrating strong generalizability in cross-dataset evaluation.\nTract2Shape enables fast, accurate, and generalizable prediction of white\nmatter shape measures from tractography data, supporting scalable analysis\nacross datasets. This framework lays a promising foundation for future\nlarge-scale white matter shape analysis.\n", "link": "http://arxiv.org/abs/2504.18400v3", "date": "2025-10-17", "relevancy": 2.1915, "topK": [{"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5937}, {"title": "Synthesizing Moving People with 3D Control", "link": "http://arxiv.org/abs/2401.10889v2", "similarity": 0.5174}, {"title": "SDXS: Real-Time One-Step Latent Diffusion Models with Image Conditions", "link": "http://arxiv.org/abs/2403.16627v1", "similarity": 0.5094}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20A%20Multimodal%20Deep%20Learning%20Approach%20for%20White%20Matter%20Shape%20Prediction%20in%0A%20%20Diffusion%20MRI%20Tractography&body=Title%3A%20A%20Multimodal%20Deep%20Learning%20Approach%20for%20White%20Matter%20Shape%20Prediction%20in%0A%20%20Diffusion%20MRI%20Tractography%0AAuthor%3A%20Yui%20Lo%20and%20Yuqian%20Chen%20and%20Dongnan%20Liu%20and%20Leo%20Zekelman%20and%20Jarrett%20Rushmore%20and%20Yogesh%20Rathi%20and%20Nikos%20Makris%20and%20Alexandra%20J.%20Golby%20and%20Fan%20Zhang%20and%20Weidong%20Cai%20and%20Lauren%20J.%20O%27Donnell%0AAbstract%3A%20%20%20Shape%20measures%20have%20emerged%20as%20promising%20descriptors%20of%20white%20matter%0Atractography%2C%20offering%20complementary%20insights%20into%20anatomical%20variability%20and%0Aassociations%20with%20cognitive%20and%20clinical%20phenotypes.%20However%2C%20conventional%0Amethods%20for%20computing%20shape%20measures%20are%20computationally%20expensive%20and%0Atime-consuming%20for%20large-scale%20datasets%20due%20to%20reliance%20on%20voxel-based%0Arepresentations.%20We%20propose%20Tract2Shape%2C%20a%20novel%20multimodal%20deep%20learning%0Aframework%20that%20leverages%20geometric%20%28point%20cloud%29%20and%20scalar%20%28tabular%29%20features%0Ato%20predict%20ten%20white%20matter%20tractography%20shape%20measures.%20To%20enhance%20model%0Aefficiency%2C%20we%20utilize%20a%20dimensionality%20reduction%20algorithm%20for%20the%20model%20to%0Apredict%20five%20primary%20shape%20components.%20The%20model%20is%20trained%20and%20evaluated%20on%0Atwo%20independently%20acquired%20datasets%2C%20the%20HCP-YA%20dataset%2C%20and%20the%20PPMI%20dataset.%0AWe%20evaluate%20the%20performance%20of%20Tract2Shape%20by%20training%20and%20testing%20it%20on%20the%0AHCP-YA%20dataset%20and%20comparing%20the%20results%20with%20state-of-the-art%20models.%20To%0Afurther%20assess%20its%20robustness%20and%20generalization%20ability%2C%20we%20also%20test%0ATract2Shape%20on%20the%20unseen%20PPMI%20dataset.%20Tract2Shape%20outperforms%20SOTA%20deep%0Alearning%20models%20across%20all%20ten%20shape%20measures%2C%20achieving%20the%20highest%20average%0APearson%27s%20r%20and%20the%20lowest%20nMSE%20on%20the%20HCP-YA%20dataset.%20The%20ablation%20study%20shows%0Athat%20both%20multimodal%20input%20and%20PCA%20contribute%20to%20performance%20gains.%20On%20the%0Aunseen%20testing%20PPMI%20dataset%2C%20Tract2Shape%20maintains%20a%20high%20Pearson%27s%20r%20and%20low%0AnMSE%2C%20demonstrating%20strong%20generalizability%20in%20cross-dataset%20evaluation.%0ATract2Shape%20enables%20fast%2C%20accurate%2C%20and%20generalizable%20prediction%20of%20white%0Amatter%20shape%20measures%20from%20tractography%20data%2C%20supporting%20scalable%20analysis%0Aacross%20datasets.%20This%20framework%20lays%20a%20promising%20foundation%20for%20future%0Alarge-scale%20white%20matter%20shape%20analysis.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2504.18400v3%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DA%2520Multimodal%2520Deep%2520Learning%2520Approach%2520for%2520White%2520Matter%2520Shape%2520Prediction%2520in%250A%2520%2520Diffusion%2520MRI%2520Tractography%26entry.906535625%3DYui%2520Lo%2520and%2520Yuqian%2520Chen%2520and%2520Dongnan%2520Liu%2520and%2520Leo%2520Zekelman%2520and%2520Jarrett%2520Rushmore%2520and%2520Yogesh%2520Rathi%2520and%2520Nikos%2520Makris%2520and%2520Alexandra%2520J.%2520Golby%2520and%2520Fan%2520Zhang%2520and%2520Weidong%2520Cai%2520and%2520Lauren%2520J.%2520O%2527Donnell%26entry.1292438233%3D%2520%2520Shape%2520measures%2520have%2520emerged%2520as%2520promising%2520descriptors%2520of%2520white%2520matter%250Atractography%252C%2520offering%2520complementary%2520insights%2520into%2520anatomical%2520variability%2520and%250Aassociations%2520with%2520cognitive%2520and%2520clinical%2520phenotypes.%2520However%252C%2520conventional%250Amethods%2520for%2520computing%2520shape%2520measures%2520are%2520computationally%2520expensive%2520and%250Atime-consuming%2520for%2520large-scale%2520datasets%2520due%2520to%2520reliance%2520on%2520voxel-based%250Arepresentations.%2520We%2520propose%2520Tract2Shape%252C%2520a%2520novel%2520multimodal%2520deep%2520learning%250Aframework%2520that%2520leverages%2520geometric%2520%2528point%2520cloud%2529%2520and%2520scalar%2520%2528tabular%2529%2520features%250Ato%2520predict%2520ten%2520white%2520matter%2520tractography%2520shape%2520measures.%2520To%2520enhance%2520model%250Aefficiency%252C%2520we%2520utilize%2520a%2520dimensionality%2520reduction%2520algorithm%2520for%2520the%2520model%2520to%250Apredict%2520five%2520primary%2520shape%2520components.%2520The%2520model%2520is%2520trained%2520and%2520evaluated%2520on%250Atwo%2520independently%2520acquired%2520datasets%252C%2520the%2520HCP-YA%2520dataset%252C%2520and%2520the%2520PPMI%2520dataset.%250AWe%2520evaluate%2520the%2520performance%2520of%2520Tract2Shape%2520by%2520training%2520and%2520testing%2520it%2520on%2520the%250AHCP-YA%2520dataset%2520and%2520comparing%2520the%2520results%2520with%2520state-of-the-art%2520models.%2520To%250Afurther%2520assess%2520its%2520robustness%2520and%2520generalization%2520ability%252C%2520we%2520also%2520test%250ATract2Shape%2520on%2520the%2520unseen%2520PPMI%2520dataset.%2520Tract2Shape%2520outperforms%2520SOTA%2520deep%250Alearning%2520models%2520across%2520all%2520ten%2520shape%2520measures%252C%2520achieving%2520the%2520highest%2520average%250APearson%2527s%2520r%2520and%2520the%2520lowest%2520nMSE%2520on%2520the%2520HCP-YA%2520dataset.%2520The%2520ablation%2520study%2520shows%250Athat%2520both%2520multimodal%2520input%2520and%2520PCA%2520contribute%2520to%2520performance%2520gains.%2520On%2520the%250Aunseen%2520testing%2520PPMI%2520dataset%252C%2520Tract2Shape%2520maintains%2520a%2520high%2520Pearson%2527s%2520r%2520and%2520low%250AnMSE%252C%2520demonstrating%2520strong%2520generalizability%2520in%2520cross-dataset%2520evaluation.%250ATract2Shape%2520enables%2520fast%252C%2520accurate%252C%2520and%2520generalizable%2520prediction%2520of%2520white%250Amatter%2520shape%2520measures%2520from%2520tractography%2520data%252C%2520supporting%2520scalable%2520analysis%250Aacross%2520datasets.%2520This%2520framework%2520lays%2520a%2520promising%2520foundation%2520for%2520future%250Alarge-scale%2520white%2520matter%2520shape%2520analysis.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2504.18400v3%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=A%20Multimodal%20Deep%20Learning%20Approach%20for%20White%20Matter%20Shape%20Prediction%20in%0A%20%20Diffusion%20MRI%20Tractography&entry.906535625=Yui%20Lo%20and%20Yuqian%20Chen%20and%20Dongnan%20Liu%20and%20Leo%20Zekelman%20and%20Jarrett%20Rushmore%20and%20Yogesh%20Rathi%20and%20Nikos%20Makris%20and%20Alexandra%20J.%20Golby%20and%20Fan%20Zhang%20and%20Weidong%20Cai%20and%20Lauren%20J.%20O%27Donnell&entry.1292438233=%20%20Shape%20measures%20have%20emerged%20as%20promising%20descriptors%20of%20white%20matter%0Atractography%2C%20offering%20complementary%20insights%20into%20anatomical%20variability%20and%0Aassociations%20with%20cognitive%20and%20clinical%20phenotypes.%20However%2C%20conventional%0Amethods%20for%20computing%20shape%20measures%20are%20computationally%20expensive%20and%0Atime-consuming%20for%20large-scale%20datasets%20due%20to%20reliance%20on%20voxel-based%0Arepresentations.%20We%20propose%20Tract2Shape%2C%20a%20novel%20multimodal%20deep%20learning%0Aframework%20that%20leverages%20geometric%20%28point%20cloud%29%20and%20scalar%20%28tabular%29%20features%0Ato%20predict%20ten%20white%20matter%20tractography%20shape%20measures.%20To%20enhance%20model%0Aefficiency%2C%20we%20utilize%20a%20dimensionality%20reduction%20algorithm%20for%20the%20model%20to%0Apredict%20five%20primary%20shape%20components.%20The%20model%20is%20trained%20and%20evaluated%20on%0Atwo%20independently%20acquired%20datasets%2C%20the%20HCP-YA%20dataset%2C%20and%20the%20PPMI%20dataset.%0AWe%20evaluate%20the%20performance%20of%20Tract2Shape%20by%20training%20and%20testing%20it%20on%20the%0AHCP-YA%20dataset%20and%20comparing%20the%20results%20with%20state-of-the-art%20models.%20To%0Afurther%20assess%20its%20robustness%20and%20generalization%20ability%2C%20we%20also%20test%0ATract2Shape%20on%20the%20unseen%20PPMI%20dataset.%20Tract2Shape%20outperforms%20SOTA%20deep%0Alearning%20models%20across%20all%20ten%20shape%20measures%2C%20achieving%20the%20highest%20average%0APearson%27s%20r%20and%20the%20lowest%20nMSE%20on%20the%20HCP-YA%20dataset.%20The%20ablation%20study%20shows%0Athat%20both%20multimodal%20input%20and%20PCA%20contribute%20to%20performance%20gains.%20On%20the%0Aunseen%20testing%20PPMI%20dataset%2C%20Tract2Shape%20maintains%20a%20high%20Pearson%27s%20r%20and%20low%0AnMSE%2C%20demonstrating%20strong%20generalizability%20in%20cross-dataset%20evaluation.%0ATract2Shape%20enables%20fast%2C%20accurate%2C%20and%20generalizable%20prediction%20of%20white%0Amatter%20shape%20measures%20from%20tractography%20data%2C%20supporting%20scalable%20analysis%0Aacross%20datasets.%20This%20framework%20lays%20a%20promising%20foundation%20for%20future%0Alarge-scale%20white%20matter%20shape%20analysis.%0A&entry.1838667208=http%3A//arxiv.org/abs/2504.18400v3&entry.124074799=Read"},
{"title": "SANR: Scene-Aware Neural Representation for Light Field Image\n  Compression with Rate-Distortion Optimization", "author": "Gai Zhang and Xinfeng Zhang and Lv Tang and Hongyu An and Li Zhang and Qingming Huang", "abstract": "  Light field images capture multi-view scene information and play a crucial\nrole in 3D scene reconstruction. However, their high-dimensional nature results\nin enormous data volumes, posing a significant challenge for efficient\ncompression in practical storage and transmission scenarios. Although neural\nrepresentation-based methods have shown promise in light field image\ncompression, most approaches rely on direct coordinate-to-pixel mapping through\nimplicit neural representation (INR), often neglecting the explicit modeling of\nscene structure. Moreover, they typically lack end-to-end rate-distortion\noptimization, limiting their compression efficiency. To address these\nlimitations, we propose SANR, a Scene-Aware Neural Representation framework for\nlight field image compression with end-to-end rate-distortion optimization. For\nscene awareness, SANR introduces a hierarchical scene modeling block that\nleverages multi-scale latent codes to capture intrinsic scene structures,\nthereby reducing the information gap between INR input coordinates and the\ntarget light field image. From a compression perspective, SANR is the first to\nincorporate entropy-constrained quantization-aware training (QAT) into neural\nrepresentation-based light field image compression, enabling end-to-end\nrate-distortion optimization. Extensive experiment results demonstrate that\nSANR significantly outperforms state-of-the-art techniques regarding\nrate-distortion performance with a 65.62\\% BD-rate saving against HEVC.\n", "link": "http://arxiv.org/abs/2510.15775v1", "date": "2025-10-17", "relevancy": 2.1879, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5483}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5483}, {"title": "SDXS: Real-Time One-Step Latent Diffusion Models with Image Conditions", "link": "http://arxiv.org/abs/2403.16627v1", "similarity": 0.5402}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20SANR%3A%20Scene-Aware%20Neural%20Representation%20for%20Light%20Field%20Image%0A%20%20Compression%20with%20Rate-Distortion%20Optimization&body=Title%3A%20SANR%3A%20Scene-Aware%20Neural%20Representation%20for%20Light%20Field%20Image%0A%20%20Compression%20with%20Rate-Distortion%20Optimization%0AAuthor%3A%20Gai%20Zhang%20and%20Xinfeng%20Zhang%20and%20Lv%20Tang%20and%20Hongyu%20An%20and%20Li%20Zhang%20and%20Qingming%20Huang%0AAbstract%3A%20%20%20Light%20field%20images%20capture%20multi-view%20scene%20information%20and%20play%20a%20crucial%0Arole%20in%203D%20scene%20reconstruction.%20However%2C%20their%20high-dimensional%20nature%20results%0Ain%20enormous%20data%20volumes%2C%20posing%20a%20significant%20challenge%20for%20efficient%0Acompression%20in%20practical%20storage%20and%20transmission%20scenarios.%20Although%20neural%0Arepresentation-based%20methods%20have%20shown%20promise%20in%20light%20field%20image%0Acompression%2C%20most%20approaches%20rely%20on%20direct%20coordinate-to-pixel%20mapping%20through%0Aimplicit%20neural%20representation%20%28INR%29%2C%20often%20neglecting%20the%20explicit%20modeling%20of%0Ascene%20structure.%20Moreover%2C%20they%20typically%20lack%20end-to-end%20rate-distortion%0Aoptimization%2C%20limiting%20their%20compression%20efficiency.%20To%20address%20these%0Alimitations%2C%20we%20propose%20SANR%2C%20a%20Scene-Aware%20Neural%20Representation%20framework%20for%0Alight%20field%20image%20compression%20with%20end-to-end%20rate-distortion%20optimization.%20For%0Ascene%20awareness%2C%20SANR%20introduces%20a%20hierarchical%20scene%20modeling%20block%20that%0Aleverages%20multi-scale%20latent%20codes%20to%20capture%20intrinsic%20scene%20structures%2C%0Athereby%20reducing%20the%20information%20gap%20between%20INR%20input%20coordinates%20and%20the%0Atarget%20light%20field%20image.%20From%20a%20compression%20perspective%2C%20SANR%20is%20the%20first%20to%0Aincorporate%20entropy-constrained%20quantization-aware%20training%20%28QAT%29%20into%20neural%0Arepresentation-based%20light%20field%20image%20compression%2C%20enabling%20end-to-end%0Arate-distortion%20optimization.%20Extensive%20experiment%20results%20demonstrate%20that%0ASANR%20significantly%20outperforms%20state-of-the-art%20techniques%20regarding%0Arate-distortion%20performance%20with%20a%2065.62%5C%25%20BD-rate%20saving%20against%20HEVC.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2510.15775v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DSANR%253A%2520Scene-Aware%2520Neural%2520Representation%2520for%2520Light%2520Field%2520Image%250A%2520%2520Compression%2520with%2520Rate-Distortion%2520Optimization%26entry.906535625%3DGai%2520Zhang%2520and%2520Xinfeng%2520Zhang%2520and%2520Lv%2520Tang%2520and%2520Hongyu%2520An%2520and%2520Li%2520Zhang%2520and%2520Qingming%2520Huang%26entry.1292438233%3D%2520%2520Light%2520field%2520images%2520capture%2520multi-view%2520scene%2520information%2520and%2520play%2520a%2520crucial%250Arole%2520in%25203D%2520scene%2520reconstruction.%2520However%252C%2520their%2520high-dimensional%2520nature%2520results%250Ain%2520enormous%2520data%2520volumes%252C%2520posing%2520a%2520significant%2520challenge%2520for%2520efficient%250Acompression%2520in%2520practical%2520storage%2520and%2520transmission%2520scenarios.%2520Although%2520neural%250Arepresentation-based%2520methods%2520have%2520shown%2520promise%2520in%2520light%2520field%2520image%250Acompression%252C%2520most%2520approaches%2520rely%2520on%2520direct%2520coordinate-to-pixel%2520mapping%2520through%250Aimplicit%2520neural%2520representation%2520%2528INR%2529%252C%2520often%2520neglecting%2520the%2520explicit%2520modeling%2520of%250Ascene%2520structure.%2520Moreover%252C%2520they%2520typically%2520lack%2520end-to-end%2520rate-distortion%250Aoptimization%252C%2520limiting%2520their%2520compression%2520efficiency.%2520To%2520address%2520these%250Alimitations%252C%2520we%2520propose%2520SANR%252C%2520a%2520Scene-Aware%2520Neural%2520Representation%2520framework%2520for%250Alight%2520field%2520image%2520compression%2520with%2520end-to-end%2520rate-distortion%2520optimization.%2520For%250Ascene%2520awareness%252C%2520SANR%2520introduces%2520a%2520hierarchical%2520scene%2520modeling%2520block%2520that%250Aleverages%2520multi-scale%2520latent%2520codes%2520to%2520capture%2520intrinsic%2520scene%2520structures%252C%250Athereby%2520reducing%2520the%2520information%2520gap%2520between%2520INR%2520input%2520coordinates%2520and%2520the%250Atarget%2520light%2520field%2520image.%2520From%2520a%2520compression%2520perspective%252C%2520SANR%2520is%2520the%2520first%2520to%250Aincorporate%2520entropy-constrained%2520quantization-aware%2520training%2520%2528QAT%2529%2520into%2520neural%250Arepresentation-based%2520light%2520field%2520image%2520compression%252C%2520enabling%2520end-to-end%250Arate-distortion%2520optimization.%2520Extensive%2520experiment%2520results%2520demonstrate%2520that%250ASANR%2520significantly%2520outperforms%2520state-of-the-art%2520techniques%2520regarding%250Arate-distortion%2520performance%2520with%2520a%252065.62%255C%2525%2520BD-rate%2520saving%2520against%2520HEVC.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2510.15775v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=SANR%3A%20Scene-Aware%20Neural%20Representation%20for%20Light%20Field%20Image%0A%20%20Compression%20with%20Rate-Distortion%20Optimization&entry.906535625=Gai%20Zhang%20and%20Xinfeng%20Zhang%20and%20Lv%20Tang%20and%20Hongyu%20An%20and%20Li%20Zhang%20and%20Qingming%20Huang&entry.1292438233=%20%20Light%20field%20images%20capture%20multi-view%20scene%20information%20and%20play%20a%20crucial%0Arole%20in%203D%20scene%20reconstruction.%20However%2C%20their%20high-dimensional%20nature%20results%0Ain%20enormous%20data%20volumes%2C%20posing%20a%20significant%20challenge%20for%20efficient%0Acompression%20in%20practical%20storage%20and%20transmission%20scenarios.%20Although%20neural%0Arepresentation-based%20methods%20have%20shown%20promise%20in%20light%20field%20image%0Acompression%2C%20most%20approaches%20rely%20on%20direct%20coordinate-to-pixel%20mapping%20through%0Aimplicit%20neural%20representation%20%28INR%29%2C%20often%20neglecting%20the%20explicit%20modeling%20of%0Ascene%20structure.%20Moreover%2C%20they%20typically%20lack%20end-to-end%20rate-distortion%0Aoptimization%2C%20limiting%20their%20compression%20efficiency.%20To%20address%20these%0Alimitations%2C%20we%20propose%20SANR%2C%20a%20Scene-Aware%20Neural%20Representation%20framework%20for%0Alight%20field%20image%20compression%20with%20end-to-end%20rate-distortion%20optimization.%20For%0Ascene%20awareness%2C%20SANR%20introduces%20a%20hierarchical%20scene%20modeling%20block%20that%0Aleverages%20multi-scale%20latent%20codes%20to%20capture%20intrinsic%20scene%20structures%2C%0Athereby%20reducing%20the%20information%20gap%20between%20INR%20input%20coordinates%20and%20the%0Atarget%20light%20field%20image.%20From%20a%20compression%20perspective%2C%20SANR%20is%20the%20first%20to%0Aincorporate%20entropy-constrained%20quantization-aware%20training%20%28QAT%29%20into%20neural%0Arepresentation-based%20light%20field%20image%20compression%2C%20enabling%20end-to-end%0Arate-distortion%20optimization.%20Extensive%20experiment%20results%20demonstrate%20that%0ASANR%20significantly%20outperforms%20state-of-the-art%20techniques%20regarding%0Arate-distortion%20performance%20with%20a%2065.62%5C%25%20BD-rate%20saving%20against%20HEVC.%0A&entry.1838667208=http%3A//arxiv.org/abs/2510.15775v1&entry.124074799=Read"},
{"title": "Memory-SAM: Human-Prompt-Free Tongue Segmentation via\n  Retrieval-to-Prompt", "author": "Joongwon Chae and Lihui Luo and Xi Yuan and Dongmei Yu and Zhenglin Chen and Lian Zhang and Peiwu Qin", "abstract": "  Accurate tongue segmentation is crucial for reliable TCM analysis. Supervised\nmodels require large annotated datasets, while SAM-family models remain\nprompt-driven. We present Memory-SAM, a training-free, human-prompt-free\npipeline that automatically generates effective prompts from a small memory of\nprior cases via dense DINOv3 features and FAISS retrieval. Given a query image,\nmask-constrained correspondences to the retrieved exemplar are distilled into\nforeground/background point prompts that guide SAM2 without manual clicks or\nmodel fine-tuning. We evaluate on 600 expert-annotated images (300 controlled,\n300 in-the-wild). On the mixed test split, Memory-SAM achieves mIoU 0.9863,\nsurpassing FCN (0.8188) and a detector-to-box SAM baseline (0.1839). On\ncontrolled data, ceiling effects above 0.98 make small differences less\nmeaningful given annotation variability, while our method shows clear gains\nunder real-world conditions. Results indicate that retrieval-to-prompt enables\ndata-efficient, robust segmentation of irregular boundaries in tongue imaging.\nThe code is publicly available at https://github.com/jw-chae/memory-sam.\n", "link": "http://arxiv.org/abs/2510.15849v1", "date": "2025-10-17", "relevancy": 2.1832, "topK": [{"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.571}, {"title": "Breadcrumbs: Adversarial class-balanced sampling for long-tailed recognition", "link": "https://link.springer.com/chapter/10.1007/978-3-031-20053-3_37", "similarity": 0.5404}, {"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.4964}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Memory-SAM%3A%20Human-Prompt-Free%20Tongue%20Segmentation%20via%0A%20%20Retrieval-to-Prompt&body=Title%3A%20Memory-SAM%3A%20Human-Prompt-Free%20Tongue%20Segmentation%20via%0A%20%20Retrieval-to-Prompt%0AAuthor%3A%20Joongwon%20Chae%20and%20Lihui%20Luo%20and%20Xi%20Yuan%20and%20Dongmei%20Yu%20and%20Zhenglin%20Chen%20and%20Lian%20Zhang%20and%20Peiwu%20Qin%0AAbstract%3A%20%20%20Accurate%20tongue%20segmentation%20is%20crucial%20for%20reliable%20TCM%20analysis.%20Supervised%0Amodels%20require%20large%20annotated%20datasets%2C%20while%20SAM-family%20models%20remain%0Aprompt-driven.%20We%20present%20Memory-SAM%2C%20a%20training-free%2C%20human-prompt-free%0Apipeline%20that%20automatically%20generates%20effective%20prompts%20from%20a%20small%20memory%20of%0Aprior%20cases%20via%20dense%20DINOv3%20features%20and%20FAISS%20retrieval.%20Given%20a%20query%20image%2C%0Amask-constrained%20correspondences%20to%20the%20retrieved%20exemplar%20are%20distilled%20into%0Aforeground/background%20point%20prompts%20that%20guide%20SAM2%20without%20manual%20clicks%20or%0Amodel%20fine-tuning.%20We%20evaluate%20on%20600%20expert-annotated%20images%20%28300%20controlled%2C%0A300%20in-the-wild%29.%20On%20the%20mixed%20test%20split%2C%20Memory-SAM%20achieves%20mIoU%200.9863%2C%0Asurpassing%20FCN%20%280.8188%29%20and%20a%20detector-to-box%20SAM%20baseline%20%280.1839%29.%20On%0Acontrolled%20data%2C%20ceiling%20effects%20above%200.98%20make%20small%20differences%20less%0Ameaningful%20given%20annotation%20variability%2C%20while%20our%20method%20shows%20clear%20gains%0Aunder%20real-world%20conditions.%20Results%20indicate%20that%20retrieval-to-prompt%20enables%0Adata-efficient%2C%20robust%20segmentation%20of%20irregular%20boundaries%20in%20tongue%20imaging.%0AThe%20code%20is%20publicly%20available%20at%20https%3A//github.com/jw-chae/memory-sam.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2510.15849v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DMemory-SAM%253A%2520Human-Prompt-Free%2520Tongue%2520Segmentation%2520via%250A%2520%2520Retrieval-to-Prompt%26entry.906535625%3DJoongwon%2520Chae%2520and%2520Lihui%2520Luo%2520and%2520Xi%2520Yuan%2520and%2520Dongmei%2520Yu%2520and%2520Zhenglin%2520Chen%2520and%2520Lian%2520Zhang%2520and%2520Peiwu%2520Qin%26entry.1292438233%3D%2520%2520Accurate%2520tongue%2520segmentation%2520is%2520crucial%2520for%2520reliable%2520TCM%2520analysis.%2520Supervised%250Amodels%2520require%2520large%2520annotated%2520datasets%252C%2520while%2520SAM-family%2520models%2520remain%250Aprompt-driven.%2520We%2520present%2520Memory-SAM%252C%2520a%2520training-free%252C%2520human-prompt-free%250Apipeline%2520that%2520automatically%2520generates%2520effective%2520prompts%2520from%2520a%2520small%2520memory%2520of%250Aprior%2520cases%2520via%2520dense%2520DINOv3%2520features%2520and%2520FAISS%2520retrieval.%2520Given%2520a%2520query%2520image%252C%250Amask-constrained%2520correspondences%2520to%2520the%2520retrieved%2520exemplar%2520are%2520distilled%2520into%250Aforeground/background%2520point%2520prompts%2520that%2520guide%2520SAM2%2520without%2520manual%2520clicks%2520or%250Amodel%2520fine-tuning.%2520We%2520evaluate%2520on%2520600%2520expert-annotated%2520images%2520%2528300%2520controlled%252C%250A300%2520in-the-wild%2529.%2520On%2520the%2520mixed%2520test%2520split%252C%2520Memory-SAM%2520achieves%2520mIoU%25200.9863%252C%250Asurpassing%2520FCN%2520%25280.8188%2529%2520and%2520a%2520detector-to-box%2520SAM%2520baseline%2520%25280.1839%2529.%2520On%250Acontrolled%2520data%252C%2520ceiling%2520effects%2520above%25200.98%2520make%2520small%2520differences%2520less%250Ameaningful%2520given%2520annotation%2520variability%252C%2520while%2520our%2520method%2520shows%2520clear%2520gains%250Aunder%2520real-world%2520conditions.%2520Results%2520indicate%2520that%2520retrieval-to-prompt%2520enables%250Adata-efficient%252C%2520robust%2520segmentation%2520of%2520irregular%2520boundaries%2520in%2520tongue%2520imaging.%250AThe%2520code%2520is%2520publicly%2520available%2520at%2520https%253A//github.com/jw-chae/memory-sam.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2510.15849v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Memory-SAM%3A%20Human-Prompt-Free%20Tongue%20Segmentation%20via%0A%20%20Retrieval-to-Prompt&entry.906535625=Joongwon%20Chae%20and%20Lihui%20Luo%20and%20Xi%20Yuan%20and%20Dongmei%20Yu%20and%20Zhenglin%20Chen%20and%20Lian%20Zhang%20and%20Peiwu%20Qin&entry.1292438233=%20%20Accurate%20tongue%20segmentation%20is%20crucial%20for%20reliable%20TCM%20analysis.%20Supervised%0Amodels%20require%20large%20annotated%20datasets%2C%20while%20SAM-family%20models%20remain%0Aprompt-driven.%20We%20present%20Memory-SAM%2C%20a%20training-free%2C%20human-prompt-free%0Apipeline%20that%20automatically%20generates%20effective%20prompts%20from%20a%20small%20memory%20of%0Aprior%20cases%20via%20dense%20DINOv3%20features%20and%20FAISS%20retrieval.%20Given%20a%20query%20image%2C%0Amask-constrained%20correspondences%20to%20the%20retrieved%20exemplar%20are%20distilled%20into%0Aforeground/background%20point%20prompts%20that%20guide%20SAM2%20without%20manual%20clicks%20or%0Amodel%20fine-tuning.%20We%20evaluate%20on%20600%20expert-annotated%20images%20%28300%20controlled%2C%0A300%20in-the-wild%29.%20On%20the%20mixed%20test%20split%2C%20Memory-SAM%20achieves%20mIoU%200.9863%2C%0Asurpassing%20FCN%20%280.8188%29%20and%20a%20detector-to-box%20SAM%20baseline%20%280.1839%29.%20On%0Acontrolled%20data%2C%20ceiling%20effects%20above%200.98%20make%20small%20differences%20less%0Ameaningful%20given%20annotation%20variability%2C%20while%20our%20method%20shows%20clear%20gains%0Aunder%20real-world%20conditions.%20Results%20indicate%20that%20retrieval-to-prompt%20enables%0Adata-efficient%2C%20robust%20segmentation%20of%20irregular%20boundaries%20in%20tongue%20imaging.%0AThe%20code%20is%20publicly%20available%20at%20https%3A//github.com/jw-chae/memory-sam.%0A&entry.1838667208=http%3A//arxiv.org/abs/2510.15849v1&entry.124074799=Read"},
{"title": "Scaling Multi Agent Reinforcement Learning for Underwater Acoustic\n  Tracking via Autonomous Vehicles", "author": "Matteo Gallici and Ivan Masmitja and Mario Mart\u00edn", "abstract": "  Autonomous vehicles (AV) offer a cost-effective solution for scientific\nmissions such as underwater tracking. Recently, reinforcement learning (RL) has\nemerged as a powerful method for controlling AVs in complex marine\nenvironments. However, scaling these techniques to a fleet--essential for\nmulti-target tracking or targets with rapid, unpredictable motion--presents\nsignificant computational challenges. Multi-Agent Reinforcement Learning (MARL)\nis notoriously sample-inefficient, and while high-fidelity simulators like\nGazebo's LRAUV provide 100x faster-than-real-time single-robot simulations,\nthey offer no significant speedup for multi-vehicle scenarios, making MARL\ntraining impractical. To address these limitations, we propose an iterative\ndistillation method that transfers high-fidelity simulations into a simplified,\nGPU-accelerated environment while preserving high-level dynamics. This approach\nachieves up to a 30,000x speedup over Gazebo through parallelization, enabling\nefficient training via end-to-end GPU acceleration. Additionally, we introduce\na novel Transformer-based architecture (TransfMAPPO) that learns multi-agent\npolicies invariant to the number of agents and targets, significantly improving\nsample efficiency. Following large-scale curriculum learning conducted entirely\non GPU, we perform extensive evaluations in Gazebo, demonstrating that our\nmethod maintains tracking errors below 5 meters over extended durations, even\nin the presence of multiple fast-moving targets. This work bridges the gap\nbetween large-scale MARL training and high-fidelity deployment, providing a\nscalable framework for autonomous fleet control in real-world sea missions.\n", "link": "http://arxiv.org/abs/2505.08222v2", "date": "2025-10-17", "relevancy": 2.1719, "topK": [{"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.5596}, {"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.5401}, {"title": "RTG-SLAM: Real-time 3D Reconstruction at Scale using Gaussian Splatting", "link": "http://arxiv.org/abs/2404.19706v1", "similarity": 0.5392}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Scaling%20Multi%20Agent%20Reinforcement%20Learning%20for%20Underwater%20Acoustic%0A%20%20Tracking%20via%20Autonomous%20Vehicles&body=Title%3A%20Scaling%20Multi%20Agent%20Reinforcement%20Learning%20for%20Underwater%20Acoustic%0A%20%20Tracking%20via%20Autonomous%20Vehicles%0AAuthor%3A%20Matteo%20Gallici%20and%20Ivan%20Masmitja%20and%20Mario%20Mart%C3%ADn%0AAbstract%3A%20%20%20Autonomous%20vehicles%20%28AV%29%20offer%20a%20cost-effective%20solution%20for%20scientific%0Amissions%20such%20as%20underwater%20tracking.%20Recently%2C%20reinforcement%20learning%20%28RL%29%20has%0Aemerged%20as%20a%20powerful%20method%20for%20controlling%20AVs%20in%20complex%20marine%0Aenvironments.%20However%2C%20scaling%20these%20techniques%20to%20a%20fleet--essential%20for%0Amulti-target%20tracking%20or%20targets%20with%20rapid%2C%20unpredictable%20motion--presents%0Asignificant%20computational%20challenges.%20Multi-Agent%20Reinforcement%20Learning%20%28MARL%29%0Ais%20notoriously%20sample-inefficient%2C%20and%20while%20high-fidelity%20simulators%20like%0AGazebo%27s%20LRAUV%20provide%20100x%20faster-than-real-time%20single-robot%20simulations%2C%0Athey%20offer%20no%20significant%20speedup%20for%20multi-vehicle%20scenarios%2C%20making%20MARL%0Atraining%20impractical.%20To%20address%20these%20limitations%2C%20we%20propose%20an%20iterative%0Adistillation%20method%20that%20transfers%20high-fidelity%20simulations%20into%20a%20simplified%2C%0AGPU-accelerated%20environment%20while%20preserving%20high-level%20dynamics.%20This%20approach%0Aachieves%20up%20to%20a%2030%2C000x%20speedup%20over%20Gazebo%20through%20parallelization%2C%20enabling%0Aefficient%20training%20via%20end-to-end%20GPU%20acceleration.%20Additionally%2C%20we%20introduce%0Aa%20novel%20Transformer-based%20architecture%20%28TransfMAPPO%29%20that%20learns%20multi-agent%0Apolicies%20invariant%20to%20the%20number%20of%20agents%20and%20targets%2C%20significantly%20improving%0Asample%20efficiency.%20Following%20large-scale%20curriculum%20learning%20conducted%20entirely%0Aon%20GPU%2C%20we%20perform%20extensive%20evaluations%20in%20Gazebo%2C%20demonstrating%20that%20our%0Amethod%20maintains%20tracking%20errors%20below%205%20meters%20over%20extended%20durations%2C%20even%0Ain%20the%20presence%20of%20multiple%20fast-moving%20targets.%20This%20work%20bridges%20the%20gap%0Abetween%20large-scale%20MARL%20training%20and%20high-fidelity%20deployment%2C%20providing%20a%0Ascalable%20framework%20for%20autonomous%20fleet%20control%20in%20real-world%20sea%20missions.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2505.08222v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DScaling%2520Multi%2520Agent%2520Reinforcement%2520Learning%2520for%2520Underwater%2520Acoustic%250A%2520%2520Tracking%2520via%2520Autonomous%2520Vehicles%26entry.906535625%3DMatteo%2520Gallici%2520and%2520Ivan%2520Masmitja%2520and%2520Mario%2520Mart%25C3%25ADn%26entry.1292438233%3D%2520%2520Autonomous%2520vehicles%2520%2528AV%2529%2520offer%2520a%2520cost-effective%2520solution%2520for%2520scientific%250Amissions%2520such%2520as%2520underwater%2520tracking.%2520Recently%252C%2520reinforcement%2520learning%2520%2528RL%2529%2520has%250Aemerged%2520as%2520a%2520powerful%2520method%2520for%2520controlling%2520AVs%2520in%2520complex%2520marine%250Aenvironments.%2520However%252C%2520scaling%2520these%2520techniques%2520to%2520a%2520fleet--essential%2520for%250Amulti-target%2520tracking%2520or%2520targets%2520with%2520rapid%252C%2520unpredictable%2520motion--presents%250Asignificant%2520computational%2520challenges.%2520Multi-Agent%2520Reinforcement%2520Learning%2520%2528MARL%2529%250Ais%2520notoriously%2520sample-inefficient%252C%2520and%2520while%2520high-fidelity%2520simulators%2520like%250AGazebo%2527s%2520LRAUV%2520provide%2520100x%2520faster-than-real-time%2520single-robot%2520simulations%252C%250Athey%2520offer%2520no%2520significant%2520speedup%2520for%2520multi-vehicle%2520scenarios%252C%2520making%2520MARL%250Atraining%2520impractical.%2520To%2520address%2520these%2520limitations%252C%2520we%2520propose%2520an%2520iterative%250Adistillation%2520method%2520that%2520transfers%2520high-fidelity%2520simulations%2520into%2520a%2520simplified%252C%250AGPU-accelerated%2520environment%2520while%2520preserving%2520high-level%2520dynamics.%2520This%2520approach%250Aachieves%2520up%2520to%2520a%252030%252C000x%2520speedup%2520over%2520Gazebo%2520through%2520parallelization%252C%2520enabling%250Aefficient%2520training%2520via%2520end-to-end%2520GPU%2520acceleration.%2520Additionally%252C%2520we%2520introduce%250Aa%2520novel%2520Transformer-based%2520architecture%2520%2528TransfMAPPO%2529%2520that%2520learns%2520multi-agent%250Apolicies%2520invariant%2520to%2520the%2520number%2520of%2520agents%2520and%2520targets%252C%2520significantly%2520improving%250Asample%2520efficiency.%2520Following%2520large-scale%2520curriculum%2520learning%2520conducted%2520entirely%250Aon%2520GPU%252C%2520we%2520perform%2520extensive%2520evaluations%2520in%2520Gazebo%252C%2520demonstrating%2520that%2520our%250Amethod%2520maintains%2520tracking%2520errors%2520below%25205%2520meters%2520over%2520extended%2520durations%252C%2520even%250Ain%2520the%2520presence%2520of%2520multiple%2520fast-moving%2520targets.%2520This%2520work%2520bridges%2520the%2520gap%250Abetween%2520large-scale%2520MARL%2520training%2520and%2520high-fidelity%2520deployment%252C%2520providing%2520a%250Ascalable%2520framework%2520for%2520autonomous%2520fleet%2520control%2520in%2520real-world%2520sea%2520missions.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2505.08222v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Scaling%20Multi%20Agent%20Reinforcement%20Learning%20for%20Underwater%20Acoustic%0A%20%20Tracking%20via%20Autonomous%20Vehicles&entry.906535625=Matteo%20Gallici%20and%20Ivan%20Masmitja%20and%20Mario%20Mart%C3%ADn&entry.1292438233=%20%20Autonomous%20vehicles%20%28AV%29%20offer%20a%20cost-effective%20solution%20for%20scientific%0Amissions%20such%20as%20underwater%20tracking.%20Recently%2C%20reinforcement%20learning%20%28RL%29%20has%0Aemerged%20as%20a%20powerful%20method%20for%20controlling%20AVs%20in%20complex%20marine%0Aenvironments.%20However%2C%20scaling%20these%20techniques%20to%20a%20fleet--essential%20for%0Amulti-target%20tracking%20or%20targets%20with%20rapid%2C%20unpredictable%20motion--presents%0Asignificant%20computational%20challenges.%20Multi-Agent%20Reinforcement%20Learning%20%28MARL%29%0Ais%20notoriously%20sample-inefficient%2C%20and%20while%20high-fidelity%20simulators%20like%0AGazebo%27s%20LRAUV%20provide%20100x%20faster-than-real-time%20single-robot%20simulations%2C%0Athey%20offer%20no%20significant%20speedup%20for%20multi-vehicle%20scenarios%2C%20making%20MARL%0Atraining%20impractical.%20To%20address%20these%20limitations%2C%20we%20propose%20an%20iterative%0Adistillation%20method%20that%20transfers%20high-fidelity%20simulations%20into%20a%20simplified%2C%0AGPU-accelerated%20environment%20while%20preserving%20high-level%20dynamics.%20This%20approach%0Aachieves%20up%20to%20a%2030%2C000x%20speedup%20over%20Gazebo%20through%20parallelization%2C%20enabling%0Aefficient%20training%20via%20end-to-end%20GPU%20acceleration.%20Additionally%2C%20we%20introduce%0Aa%20novel%20Transformer-based%20architecture%20%28TransfMAPPO%29%20that%20learns%20multi-agent%0Apolicies%20invariant%20to%20the%20number%20of%20agents%20and%20targets%2C%20significantly%20improving%0Asample%20efficiency.%20Following%20large-scale%20curriculum%20learning%20conducted%20entirely%0Aon%20GPU%2C%20we%20perform%20extensive%20evaluations%20in%20Gazebo%2C%20demonstrating%20that%20our%0Amethod%20maintains%20tracking%20errors%20below%205%20meters%20over%20extended%20durations%2C%20even%0Ain%20the%20presence%20of%20multiple%20fast-moving%20targets.%20This%20work%20bridges%20the%20gap%0Abetween%20large-scale%20MARL%20training%20and%20high-fidelity%20deployment%2C%20providing%20a%0Ascalable%20framework%20for%20autonomous%20fleet%20control%20in%20real-world%20sea%20missions.%0A&entry.1838667208=http%3A//arxiv.org/abs/2505.08222v2&entry.124074799=Read"},
{"title": "Valeo Near-Field: a novel dataset for pedestrian intent detection", "author": "Antonyo Musabini and Rachid Benmokhtar and Jagdish Bhanushali and Victor Galizzi and Bertrand Luvison and Xavier Perrotton", "abstract": "  This paper presents a novel dataset aimed at detecting pedestrians'\nintentions as they approach an ego-vehicle. The dataset comprises synchronized\nmulti-modal data, including fisheye camera feeds, lidar laser scans, ultrasonic\nsensor readings, and motion capture-based 3D body poses, collected across\ndiverse real-world scenarios. Key contributions include detailed annotations of\n3D body joint positions synchronized with fisheye camera images, as well as\naccurate 3D pedestrian positions extracted from lidar data, facilitating robust\nbenchmarking for perception algorithms. We release a portion of the dataset\nalong with a comprehensive benchmark suite, featuring evaluation metrics for\naccuracy, efficiency, and scalability on embedded systems. By addressing\nreal-world challenges such as sensor occlusions, dynamic environments, and\nhardware constraints, this dataset offers a unique resource for developing and\nevaluating state-of-the-art algorithms in pedestrian detection, 3D pose\nestimation and 4D trajectory and intention prediction. Additionally, we provide\nbaseline performance metrics using custom neural network architectures and\nsuggest future research directions to encourage the adoption and enhancement of\nthe dataset. This work aims to serve as a foundation for researchers seeking to\nadvance the capabilities of intelligent vehicles in near-field scenarios.\n", "link": "http://arxiv.org/abs/2510.15673v1", "date": "2025-10-17", "relevancy": 2.1471, "topK": [{"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.56}, {"title": "Egocentric Computer Vision for Hands-Free Robotic Wheelchair Navigation", "link": "https://link.springer.com/article/10.1007/s10846-023-01807-4", "similarity": 0.5344}, {"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.5299}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Valeo%20Near-Field%3A%20a%20novel%20dataset%20for%20pedestrian%20intent%20detection&body=Title%3A%20Valeo%20Near-Field%3A%20a%20novel%20dataset%20for%20pedestrian%20intent%20detection%0AAuthor%3A%20Antonyo%20Musabini%20and%20Rachid%20Benmokhtar%20and%20Jagdish%20Bhanushali%20and%20Victor%20Galizzi%20and%20Bertrand%20Luvison%20and%20Xavier%20Perrotton%0AAbstract%3A%20%20%20This%20paper%20presents%20a%20novel%20dataset%20aimed%20at%20detecting%20pedestrians%27%0Aintentions%20as%20they%20approach%20an%20ego-vehicle.%20The%20dataset%20comprises%20synchronized%0Amulti-modal%20data%2C%20including%20fisheye%20camera%20feeds%2C%20lidar%20laser%20scans%2C%20ultrasonic%0Asensor%20readings%2C%20and%20motion%20capture-based%203D%20body%20poses%2C%20collected%20across%0Adiverse%20real-world%20scenarios.%20Key%20contributions%20include%20detailed%20annotations%20of%0A3D%20body%20joint%20positions%20synchronized%20with%20fisheye%20camera%20images%2C%20as%20well%20as%0Aaccurate%203D%20pedestrian%20positions%20extracted%20from%20lidar%20data%2C%20facilitating%20robust%0Abenchmarking%20for%20perception%20algorithms.%20We%20release%20a%20portion%20of%20the%20dataset%0Aalong%20with%20a%20comprehensive%20benchmark%20suite%2C%20featuring%20evaluation%20metrics%20for%0Aaccuracy%2C%20efficiency%2C%20and%20scalability%20on%20embedded%20systems.%20By%20addressing%0Areal-world%20challenges%20such%20as%20sensor%20occlusions%2C%20dynamic%20environments%2C%20and%0Ahardware%20constraints%2C%20this%20dataset%20offers%20a%20unique%20resource%20for%20developing%20and%0Aevaluating%20state-of-the-art%20algorithms%20in%20pedestrian%20detection%2C%203D%20pose%0Aestimation%20and%204D%20trajectory%20and%20intention%20prediction.%20Additionally%2C%20we%20provide%0Abaseline%20performance%20metrics%20using%20custom%20neural%20network%20architectures%20and%0Asuggest%20future%20research%20directions%20to%20encourage%20the%20adoption%20and%20enhancement%20of%0Athe%20dataset.%20This%20work%20aims%20to%20serve%20as%20a%20foundation%20for%20researchers%20seeking%20to%0Aadvance%20the%20capabilities%20of%20intelligent%20vehicles%20in%20near-field%20scenarios.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2510.15673v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DValeo%2520Near-Field%253A%2520a%2520novel%2520dataset%2520for%2520pedestrian%2520intent%2520detection%26entry.906535625%3DAntonyo%2520Musabini%2520and%2520Rachid%2520Benmokhtar%2520and%2520Jagdish%2520Bhanushali%2520and%2520Victor%2520Galizzi%2520and%2520Bertrand%2520Luvison%2520and%2520Xavier%2520Perrotton%26entry.1292438233%3D%2520%2520This%2520paper%2520presents%2520a%2520novel%2520dataset%2520aimed%2520at%2520detecting%2520pedestrians%2527%250Aintentions%2520as%2520they%2520approach%2520an%2520ego-vehicle.%2520The%2520dataset%2520comprises%2520synchronized%250Amulti-modal%2520data%252C%2520including%2520fisheye%2520camera%2520feeds%252C%2520lidar%2520laser%2520scans%252C%2520ultrasonic%250Asensor%2520readings%252C%2520and%2520motion%2520capture-based%25203D%2520body%2520poses%252C%2520collected%2520across%250Adiverse%2520real-world%2520scenarios.%2520Key%2520contributions%2520include%2520detailed%2520annotations%2520of%250A3D%2520body%2520joint%2520positions%2520synchronized%2520with%2520fisheye%2520camera%2520images%252C%2520as%2520well%2520as%250Aaccurate%25203D%2520pedestrian%2520positions%2520extracted%2520from%2520lidar%2520data%252C%2520facilitating%2520robust%250Abenchmarking%2520for%2520perception%2520algorithms.%2520We%2520release%2520a%2520portion%2520of%2520the%2520dataset%250Aalong%2520with%2520a%2520comprehensive%2520benchmark%2520suite%252C%2520featuring%2520evaluation%2520metrics%2520for%250Aaccuracy%252C%2520efficiency%252C%2520and%2520scalability%2520on%2520embedded%2520systems.%2520By%2520addressing%250Areal-world%2520challenges%2520such%2520as%2520sensor%2520occlusions%252C%2520dynamic%2520environments%252C%2520and%250Ahardware%2520constraints%252C%2520this%2520dataset%2520offers%2520a%2520unique%2520resource%2520for%2520developing%2520and%250Aevaluating%2520state-of-the-art%2520algorithms%2520in%2520pedestrian%2520detection%252C%25203D%2520pose%250Aestimation%2520and%25204D%2520trajectory%2520and%2520intention%2520prediction.%2520Additionally%252C%2520we%2520provide%250Abaseline%2520performance%2520metrics%2520using%2520custom%2520neural%2520network%2520architectures%2520and%250Asuggest%2520future%2520research%2520directions%2520to%2520encourage%2520the%2520adoption%2520and%2520enhancement%2520of%250Athe%2520dataset.%2520This%2520work%2520aims%2520to%2520serve%2520as%2520a%2520foundation%2520for%2520researchers%2520seeking%2520to%250Aadvance%2520the%2520capabilities%2520of%2520intelligent%2520vehicles%2520in%2520near-field%2520scenarios.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2510.15673v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Valeo%20Near-Field%3A%20a%20novel%20dataset%20for%20pedestrian%20intent%20detection&entry.906535625=Antonyo%20Musabini%20and%20Rachid%20Benmokhtar%20and%20Jagdish%20Bhanushali%20and%20Victor%20Galizzi%20and%20Bertrand%20Luvison%20and%20Xavier%20Perrotton&entry.1292438233=%20%20This%20paper%20presents%20a%20novel%20dataset%20aimed%20at%20detecting%20pedestrians%27%0Aintentions%20as%20they%20approach%20an%20ego-vehicle.%20The%20dataset%20comprises%20synchronized%0Amulti-modal%20data%2C%20including%20fisheye%20camera%20feeds%2C%20lidar%20laser%20scans%2C%20ultrasonic%0Asensor%20readings%2C%20and%20motion%20capture-based%203D%20body%20poses%2C%20collected%20across%0Adiverse%20real-world%20scenarios.%20Key%20contributions%20include%20detailed%20annotations%20of%0A3D%20body%20joint%20positions%20synchronized%20with%20fisheye%20camera%20images%2C%20as%20well%20as%0Aaccurate%203D%20pedestrian%20positions%20extracted%20from%20lidar%20data%2C%20facilitating%20robust%0Abenchmarking%20for%20perception%20algorithms.%20We%20release%20a%20portion%20of%20the%20dataset%0Aalong%20with%20a%20comprehensive%20benchmark%20suite%2C%20featuring%20evaluation%20metrics%20for%0Aaccuracy%2C%20efficiency%2C%20and%20scalability%20on%20embedded%20systems.%20By%20addressing%0Areal-world%20challenges%20such%20as%20sensor%20occlusions%2C%20dynamic%20environments%2C%20and%0Ahardware%20constraints%2C%20this%20dataset%20offers%20a%20unique%20resource%20for%20developing%20and%0Aevaluating%20state-of-the-art%20algorithms%20in%20pedestrian%20detection%2C%203D%20pose%0Aestimation%20and%204D%20trajectory%20and%20intention%20prediction.%20Additionally%2C%20we%20provide%0Abaseline%20performance%20metrics%20using%20custom%20neural%20network%20architectures%20and%0Asuggest%20future%20research%20directions%20to%20encourage%20the%20adoption%20and%20enhancement%20of%0Athe%20dataset.%20This%20work%20aims%20to%20serve%20as%20a%20foundation%20for%20researchers%20seeking%20to%0Aadvance%20the%20capabilities%20of%20intelligent%20vehicles%20in%20near-field%20scenarios.%0A&entry.1838667208=http%3A//arxiv.org/abs/2510.15673v1&entry.124074799=Read"},
{"title": "Implicit neural representations for accurate estimation of the standard\n  model of white matter", "author": "Tom Hendriks and Gerrit Arends and Edwin Versteeg and Anna Vilanova and Maxime Chamberland and Chantal M. W. Tax", "abstract": "  Diffusion magnetic resonance imaging (dMRI) enables non-invasive\ninvestigation of tissue microstructure. The Standard Model (SM) of white matter\naims to disentangle dMRI signal contributions from intra- and extra-axonal\nwater compartments. However, due to the model its high-dimensional nature,\naccurately estimating its parameters poses a complex problem and remains an\nactive field of research, in which different (machine learning) strategies have\nbeen proposed. This work introduces an estimation framework based on implicit\nneural representations (INRs), which incorporate spatial regularization through\nthe sinusoidal encoding of the input coordinates. The INR method is evaluated\non both synthetic and in vivo datasets and compared to existing methods.\nResults demonstrate superior accuracy of the INR method in estimating SM\nparameters, particularly in low signal-to-noise conditions. Additionally,\nspatial upsampling of the INR can represent the underlying dataset anatomically\nplausibly in a continuous way. The INR is self-supervised, eliminating the need\nfor labeled training data. It achieves fast inference, is robust to noise,\nsupports joint estimation of SM kernel parameters and the fiber orientation\ndistribution function with spherical harmonics orders up to at least 8, and\naccommodates gradient non-uniformity corrections. The combination of these\nproperties positions INRs as a potentially important tool for analyzing and\ninterpreting diffusion MRI data.\n", "link": "http://arxiv.org/abs/2506.15762v2", "date": "2025-10-17", "relevancy": 2.1249, "topK": [{"title": "SDXS: Real-Time One-Step Latent Diffusion Models with Image Conditions", "link": "http://arxiv.org/abs/2403.16627v1", "similarity": 0.5532}, {"title": "MiraGe: Editable 2D Images using Gaussian Splatting", "link": "http://arxiv.org/abs/2410.01521v1", "similarity": 0.5398}, {"title": "IAE: Implicit Autoencoder for Point Cloud Self-supervised Representation Learning", "link": "https://arxiv.org/abs/2201.00785", "similarity": 0.5139}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Implicit%20neural%20representations%20for%20accurate%20estimation%20of%20the%20standard%0A%20%20model%20of%20white%20matter&body=Title%3A%20Implicit%20neural%20representations%20for%20accurate%20estimation%20of%20the%20standard%0A%20%20model%20of%20white%20matter%0AAuthor%3A%20Tom%20Hendriks%20and%20Gerrit%20Arends%20and%20Edwin%20Versteeg%20and%20Anna%20Vilanova%20and%20Maxime%20Chamberland%20and%20Chantal%20M.%20W.%20Tax%0AAbstract%3A%20%20%20Diffusion%20magnetic%20resonance%20imaging%20%28dMRI%29%20enables%20non-invasive%0Ainvestigation%20of%20tissue%20microstructure.%20The%20Standard%20Model%20%28SM%29%20of%20white%20matter%0Aaims%20to%20disentangle%20dMRI%20signal%20contributions%20from%20intra-%20and%20extra-axonal%0Awater%20compartments.%20However%2C%20due%20to%20the%20model%20its%20high-dimensional%20nature%2C%0Aaccurately%20estimating%20its%20parameters%20poses%20a%20complex%20problem%20and%20remains%20an%0Aactive%20field%20of%20research%2C%20in%20which%20different%20%28machine%20learning%29%20strategies%20have%0Abeen%20proposed.%20This%20work%20introduces%20an%20estimation%20framework%20based%20on%20implicit%0Aneural%20representations%20%28INRs%29%2C%20which%20incorporate%20spatial%20regularization%20through%0Athe%20sinusoidal%20encoding%20of%20the%20input%20coordinates.%20The%20INR%20method%20is%20evaluated%0Aon%20both%20synthetic%20and%20in%20vivo%20datasets%20and%20compared%20to%20existing%20methods.%0AResults%20demonstrate%20superior%20accuracy%20of%20the%20INR%20method%20in%20estimating%20SM%0Aparameters%2C%20particularly%20in%20low%20signal-to-noise%20conditions.%20Additionally%2C%0Aspatial%20upsampling%20of%20the%20INR%20can%20represent%20the%20underlying%20dataset%20anatomically%0Aplausibly%20in%20a%20continuous%20way.%20The%20INR%20is%20self-supervised%2C%20eliminating%20the%20need%0Afor%20labeled%20training%20data.%20It%20achieves%20fast%20inference%2C%20is%20robust%20to%20noise%2C%0Asupports%20joint%20estimation%20of%20SM%20kernel%20parameters%20and%20the%20fiber%20orientation%0Adistribution%20function%20with%20spherical%20harmonics%20orders%20up%20to%20at%20least%208%2C%20and%0Aaccommodates%20gradient%20non-uniformity%20corrections.%20The%20combination%20of%20these%0Aproperties%20positions%20INRs%20as%20a%20potentially%20important%20tool%20for%20analyzing%20and%0Ainterpreting%20diffusion%20MRI%20data.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2506.15762v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DImplicit%2520neural%2520representations%2520for%2520accurate%2520estimation%2520of%2520the%2520standard%250A%2520%2520model%2520of%2520white%2520matter%26entry.906535625%3DTom%2520Hendriks%2520and%2520Gerrit%2520Arends%2520and%2520Edwin%2520Versteeg%2520and%2520Anna%2520Vilanova%2520and%2520Maxime%2520Chamberland%2520and%2520Chantal%2520M.%2520W.%2520Tax%26entry.1292438233%3D%2520%2520Diffusion%2520magnetic%2520resonance%2520imaging%2520%2528dMRI%2529%2520enables%2520non-invasive%250Ainvestigation%2520of%2520tissue%2520microstructure.%2520The%2520Standard%2520Model%2520%2528SM%2529%2520of%2520white%2520matter%250Aaims%2520to%2520disentangle%2520dMRI%2520signal%2520contributions%2520from%2520intra-%2520and%2520extra-axonal%250Awater%2520compartments.%2520However%252C%2520due%2520to%2520the%2520model%2520its%2520high-dimensional%2520nature%252C%250Aaccurately%2520estimating%2520its%2520parameters%2520poses%2520a%2520complex%2520problem%2520and%2520remains%2520an%250Aactive%2520field%2520of%2520research%252C%2520in%2520which%2520different%2520%2528machine%2520learning%2529%2520strategies%2520have%250Abeen%2520proposed.%2520This%2520work%2520introduces%2520an%2520estimation%2520framework%2520based%2520on%2520implicit%250Aneural%2520representations%2520%2528INRs%2529%252C%2520which%2520incorporate%2520spatial%2520regularization%2520through%250Athe%2520sinusoidal%2520encoding%2520of%2520the%2520input%2520coordinates.%2520The%2520INR%2520method%2520is%2520evaluated%250Aon%2520both%2520synthetic%2520and%2520in%2520vivo%2520datasets%2520and%2520compared%2520to%2520existing%2520methods.%250AResults%2520demonstrate%2520superior%2520accuracy%2520of%2520the%2520INR%2520method%2520in%2520estimating%2520SM%250Aparameters%252C%2520particularly%2520in%2520low%2520signal-to-noise%2520conditions.%2520Additionally%252C%250Aspatial%2520upsampling%2520of%2520the%2520INR%2520can%2520represent%2520the%2520underlying%2520dataset%2520anatomically%250Aplausibly%2520in%2520a%2520continuous%2520way.%2520The%2520INR%2520is%2520self-supervised%252C%2520eliminating%2520the%2520need%250Afor%2520labeled%2520training%2520data.%2520It%2520achieves%2520fast%2520inference%252C%2520is%2520robust%2520to%2520noise%252C%250Asupports%2520joint%2520estimation%2520of%2520SM%2520kernel%2520parameters%2520and%2520the%2520fiber%2520orientation%250Adistribution%2520function%2520with%2520spherical%2520harmonics%2520orders%2520up%2520to%2520at%2520least%25208%252C%2520and%250Aaccommodates%2520gradient%2520non-uniformity%2520corrections.%2520The%2520combination%2520of%2520these%250Aproperties%2520positions%2520INRs%2520as%2520a%2520potentially%2520important%2520tool%2520for%2520analyzing%2520and%250Ainterpreting%2520diffusion%2520MRI%2520data.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2506.15762v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Implicit%20neural%20representations%20for%20accurate%20estimation%20of%20the%20standard%0A%20%20model%20of%20white%20matter&entry.906535625=Tom%20Hendriks%20and%20Gerrit%20Arends%20and%20Edwin%20Versteeg%20and%20Anna%20Vilanova%20and%20Maxime%20Chamberland%20and%20Chantal%20M.%20W.%20Tax&entry.1292438233=%20%20Diffusion%20magnetic%20resonance%20imaging%20%28dMRI%29%20enables%20non-invasive%0Ainvestigation%20of%20tissue%20microstructure.%20The%20Standard%20Model%20%28SM%29%20of%20white%20matter%0Aaims%20to%20disentangle%20dMRI%20signal%20contributions%20from%20intra-%20and%20extra-axonal%0Awater%20compartments.%20However%2C%20due%20to%20the%20model%20its%20high-dimensional%20nature%2C%0Aaccurately%20estimating%20its%20parameters%20poses%20a%20complex%20problem%20and%20remains%20an%0Aactive%20field%20of%20research%2C%20in%20which%20different%20%28machine%20learning%29%20strategies%20have%0Abeen%20proposed.%20This%20work%20introduces%20an%20estimation%20framework%20based%20on%20implicit%0Aneural%20representations%20%28INRs%29%2C%20which%20incorporate%20spatial%20regularization%20through%0Athe%20sinusoidal%20encoding%20of%20the%20input%20coordinates.%20The%20INR%20method%20is%20evaluated%0Aon%20both%20synthetic%20and%20in%20vivo%20datasets%20and%20compared%20to%20existing%20methods.%0AResults%20demonstrate%20superior%20accuracy%20of%20the%20INR%20method%20in%20estimating%20SM%0Aparameters%2C%20particularly%20in%20low%20signal-to-noise%20conditions.%20Additionally%2C%0Aspatial%20upsampling%20of%20the%20INR%20can%20represent%20the%20underlying%20dataset%20anatomically%0Aplausibly%20in%20a%20continuous%20way.%20The%20INR%20is%20self-supervised%2C%20eliminating%20the%20need%0Afor%20labeled%20training%20data.%20It%20achieves%20fast%20inference%2C%20is%20robust%20to%20noise%2C%0Asupports%20joint%20estimation%20of%20SM%20kernel%20parameters%20and%20the%20fiber%20orientation%0Adistribution%20function%20with%20spherical%20harmonics%20orders%20up%20to%20at%20least%208%2C%20and%0Aaccommodates%20gradient%20non-uniformity%20corrections.%20The%20combination%20of%20these%0Aproperties%20positions%20INRs%20as%20a%20potentially%20important%20tool%20for%20analyzing%20and%0Ainterpreting%20diffusion%20MRI%20data.%0A&entry.1838667208=http%3A//arxiv.org/abs/2506.15762v2&entry.124074799=Read"},
{"title": "Expanding the Action Space of LLMs to Reason Beyond Language", "author": "Zhongqi Yue and Weishi Wang and Yundaichuan Zhan and Juncheng Li and Daniel Dahlmeier and Fredrik D. Johansson", "abstract": "  Large Language Models (LLMs) are powerful reasoners in natural language, but\ntheir actions are typically confined to outputting vocabulary tokens. As a\nresult, interactions with external environments -- such as symbolic operators\nor simulators -- must be expressed through text in predefined formats, parsed,\nand routed to external interfaces. This overloads the model's language with\nboth reasoning and control duties, and requires a hand-crafted parser, external\nto the LLM. To address this, we decouple environment interactions from language\nby internalizing them in an Expanded Action space (ExpA), beyond the\nvocabulary. The model starts reasoning in the default language environment, but\nmay trigger routing actions and switch to an external environment at any time.\nFrom there, the model can only invoke environment-specific actions, receive\nfeedback from the environment, and potentially route back to language as a\nresult. To promote effective exploration of the expanded action space and new\nenvironments, we introduce ExpA Reinforcement Learning (EARL) with\ncounterfactual policy optimization. On tasks requiring multi-turn interactions\nand contingent planning, EARL outperforms strong baselines with\nvocabulary-constrained actions. It performs robustly across calculator-based\nmulti-task learning and, in the partially observed sorting problem, achieves\nperfect Sort-4 accuracy while self-discovering an efficient algorithm\ncompetitive with classical designs.\n", "link": "http://arxiv.org/abs/2510.07581v2", "date": "2025-10-17", "relevancy": 2.0868, "topK": [{"title": "RoboEXP: Action-Conditioned Scene Graph via Interactive Exploration for\n  Robotic Manipulation", "link": "http://arxiv.org/abs/2402.15487v1", "similarity": 0.6015}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5058}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5058}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Expanding%20the%20Action%20Space%20of%20LLMs%20to%20Reason%20Beyond%20Language&body=Title%3A%20Expanding%20the%20Action%20Space%20of%20LLMs%20to%20Reason%20Beyond%20Language%0AAuthor%3A%20Zhongqi%20Yue%20and%20Weishi%20Wang%20and%20Yundaichuan%20Zhan%20and%20Juncheng%20Li%20and%20Daniel%20Dahlmeier%20and%20Fredrik%20D.%20Johansson%0AAbstract%3A%20%20%20Large%20Language%20Models%20%28LLMs%29%20are%20powerful%20reasoners%20in%20natural%20language%2C%20but%0Atheir%20actions%20are%20typically%20confined%20to%20outputting%20vocabulary%20tokens.%20As%20a%0Aresult%2C%20interactions%20with%20external%20environments%20--%20such%20as%20symbolic%20operators%0Aor%20simulators%20--%20must%20be%20expressed%20through%20text%20in%20predefined%20formats%2C%20parsed%2C%0Aand%20routed%20to%20external%20interfaces.%20This%20overloads%20the%20model%27s%20language%20with%0Aboth%20reasoning%20and%20control%20duties%2C%20and%20requires%20a%20hand-crafted%20parser%2C%20external%0Ato%20the%20LLM.%20To%20address%20this%2C%20we%20decouple%20environment%20interactions%20from%20language%0Aby%20internalizing%20them%20in%20an%20Expanded%20Action%20space%20%28ExpA%29%2C%20beyond%20the%0Avocabulary.%20The%20model%20starts%20reasoning%20in%20the%20default%20language%20environment%2C%20but%0Amay%20trigger%20routing%20actions%20and%20switch%20to%20an%20external%20environment%20at%20any%20time.%0AFrom%20there%2C%20the%20model%20can%20only%20invoke%20environment-specific%20actions%2C%20receive%0Afeedback%20from%20the%20environment%2C%20and%20potentially%20route%20back%20to%20language%20as%20a%0Aresult.%20To%20promote%20effective%20exploration%20of%20the%20expanded%20action%20space%20and%20new%0Aenvironments%2C%20we%20introduce%20ExpA%20Reinforcement%20Learning%20%28EARL%29%20with%0Acounterfactual%20policy%20optimization.%20On%20tasks%20requiring%20multi-turn%20interactions%0Aand%20contingent%20planning%2C%20EARL%20outperforms%20strong%20baselines%20with%0Avocabulary-constrained%20actions.%20It%20performs%20robustly%20across%20calculator-based%0Amulti-task%20learning%20and%2C%20in%20the%20partially%20observed%20sorting%20problem%2C%20achieves%0Aperfect%20Sort-4%20accuracy%20while%20self-discovering%20an%20efficient%20algorithm%0Acompetitive%20with%20classical%20designs.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2510.07581v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DExpanding%2520the%2520Action%2520Space%2520of%2520LLMs%2520to%2520Reason%2520Beyond%2520Language%26entry.906535625%3DZhongqi%2520Yue%2520and%2520Weishi%2520Wang%2520and%2520Yundaichuan%2520Zhan%2520and%2520Juncheng%2520Li%2520and%2520Daniel%2520Dahlmeier%2520and%2520Fredrik%2520D.%2520Johansson%26entry.1292438233%3D%2520%2520Large%2520Language%2520Models%2520%2528LLMs%2529%2520are%2520powerful%2520reasoners%2520in%2520natural%2520language%252C%2520but%250Atheir%2520actions%2520are%2520typically%2520confined%2520to%2520outputting%2520vocabulary%2520tokens.%2520As%2520a%250Aresult%252C%2520interactions%2520with%2520external%2520environments%2520--%2520such%2520as%2520symbolic%2520operators%250Aor%2520simulators%2520--%2520must%2520be%2520expressed%2520through%2520text%2520in%2520predefined%2520formats%252C%2520parsed%252C%250Aand%2520routed%2520to%2520external%2520interfaces.%2520This%2520overloads%2520the%2520model%2527s%2520language%2520with%250Aboth%2520reasoning%2520and%2520control%2520duties%252C%2520and%2520requires%2520a%2520hand-crafted%2520parser%252C%2520external%250Ato%2520the%2520LLM.%2520To%2520address%2520this%252C%2520we%2520decouple%2520environment%2520interactions%2520from%2520language%250Aby%2520internalizing%2520them%2520in%2520an%2520Expanded%2520Action%2520space%2520%2528ExpA%2529%252C%2520beyond%2520the%250Avocabulary.%2520The%2520model%2520starts%2520reasoning%2520in%2520the%2520default%2520language%2520environment%252C%2520but%250Amay%2520trigger%2520routing%2520actions%2520and%2520switch%2520to%2520an%2520external%2520environment%2520at%2520any%2520time.%250AFrom%2520there%252C%2520the%2520model%2520can%2520only%2520invoke%2520environment-specific%2520actions%252C%2520receive%250Afeedback%2520from%2520the%2520environment%252C%2520and%2520potentially%2520route%2520back%2520to%2520language%2520as%2520a%250Aresult.%2520To%2520promote%2520effective%2520exploration%2520of%2520the%2520expanded%2520action%2520space%2520and%2520new%250Aenvironments%252C%2520we%2520introduce%2520ExpA%2520Reinforcement%2520Learning%2520%2528EARL%2529%2520with%250Acounterfactual%2520policy%2520optimization.%2520On%2520tasks%2520requiring%2520multi-turn%2520interactions%250Aand%2520contingent%2520planning%252C%2520EARL%2520outperforms%2520strong%2520baselines%2520with%250Avocabulary-constrained%2520actions.%2520It%2520performs%2520robustly%2520across%2520calculator-based%250Amulti-task%2520learning%2520and%252C%2520in%2520the%2520partially%2520observed%2520sorting%2520problem%252C%2520achieves%250Aperfect%2520Sort-4%2520accuracy%2520while%2520self-discovering%2520an%2520efficient%2520algorithm%250Acompetitive%2520with%2520classical%2520designs.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2510.07581v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Expanding%20the%20Action%20Space%20of%20LLMs%20to%20Reason%20Beyond%20Language&entry.906535625=Zhongqi%20Yue%20and%20Weishi%20Wang%20and%20Yundaichuan%20Zhan%20and%20Juncheng%20Li%20and%20Daniel%20Dahlmeier%20and%20Fredrik%20D.%20Johansson&entry.1292438233=%20%20Large%20Language%20Models%20%28LLMs%29%20are%20powerful%20reasoners%20in%20natural%20language%2C%20but%0Atheir%20actions%20are%20typically%20confined%20to%20outputting%20vocabulary%20tokens.%20As%20a%0Aresult%2C%20interactions%20with%20external%20environments%20--%20such%20as%20symbolic%20operators%0Aor%20simulators%20--%20must%20be%20expressed%20through%20text%20in%20predefined%20formats%2C%20parsed%2C%0Aand%20routed%20to%20external%20interfaces.%20This%20overloads%20the%20model%27s%20language%20with%0Aboth%20reasoning%20and%20control%20duties%2C%20and%20requires%20a%20hand-crafted%20parser%2C%20external%0Ato%20the%20LLM.%20To%20address%20this%2C%20we%20decouple%20environment%20interactions%20from%20language%0Aby%20internalizing%20them%20in%20an%20Expanded%20Action%20space%20%28ExpA%29%2C%20beyond%20the%0Avocabulary.%20The%20model%20starts%20reasoning%20in%20the%20default%20language%20environment%2C%20but%0Amay%20trigger%20routing%20actions%20and%20switch%20to%20an%20external%20environment%20at%20any%20time.%0AFrom%20there%2C%20the%20model%20can%20only%20invoke%20environment-specific%20actions%2C%20receive%0Afeedback%20from%20the%20environment%2C%20and%20potentially%20route%20back%20to%20language%20as%20a%0Aresult.%20To%20promote%20effective%20exploration%20of%20the%20expanded%20action%20space%20and%20new%0Aenvironments%2C%20we%20introduce%20ExpA%20Reinforcement%20Learning%20%28EARL%29%20with%0Acounterfactual%20policy%20optimization.%20On%20tasks%20requiring%20multi-turn%20interactions%0Aand%20contingent%20planning%2C%20EARL%20outperforms%20strong%20baselines%20with%0Avocabulary-constrained%20actions.%20It%20performs%20robustly%20across%20calculator-based%0Amulti-task%20learning%20and%2C%20in%20the%20partially%20observed%20sorting%20problem%2C%20achieves%0Aperfect%20Sort-4%20accuracy%20while%20self-discovering%20an%20efficient%20algorithm%0Acompetitive%20with%20classical%20designs.%0A&entry.1838667208=http%3A//arxiv.org/abs/2510.07581v2&entry.124074799=Read"},
{"title": "Constrained Adversarial Perturbation", "author": "Virendra Nishad and Bhaskar Mukhoty and Hilal AlQuabeh and Sandeep K. Shukla and Sayak Ray Chowdhury", "abstract": "  Deep neural networks have achieved remarkable success in a wide range of\nclassification tasks. However, they remain highly susceptible to adversarial\nexamples - inputs that are subtly perturbed to induce misclassification while\nappearing unchanged to humans. Among various attack strategies, Universal\nAdversarial Perturbations (UAPs) have emerged as a powerful tool for both\nstress testing model robustness and facilitating scalable adversarial training.\nDespite their effectiveness, most existing UAP methods neglect domain specific\nconstraints that govern feature relationships. Violating such constraints, such\nas debt to income ratios in credit scoring or packet flow invariants in network\ncommunication, can render adversarial examples implausible or easily\ndetectable, thereby limiting their real world applicability.\n  In this work, we advance universal adversarial attacks to constrained feature\nspaces by formulating an augmented Lagrangian based min max optimization\nproblem that enforces multiple, potentially complex constraints of varying\nimportance. We propose Constrained Adversarial Perturbation (CAP), an efficient\nalgorithm that solves this problem using a gradient based alternating\noptimization strategy. We evaluate CAP across diverse domains including\nfinance, IT networks, and cyber physical systems, and demonstrate that it\nachieves higher attack success rates while significantly reducing runtime\ncompared to existing baselines. Our approach also generalizes seamlessly to\nindividual adversarial perturbations, where we observe similar strong\nperformance gains. Finally, we introduce a principled procedure for learning\nfeature constraints directly from data, enabling broad applicability across\ndomains with structured input spaces.\n", "link": "http://arxiv.org/abs/2510.15699v1", "date": "2025-10-17", "relevancy": 2.0797, "topK": [{"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.5385}, {"title": "Breadcrumbs: Adversarial class-balanced sampling for long-tailed recognition", "link": "https://link.springer.com/chapter/10.1007/978-3-031-20053-3_37", "similarity": 0.5121}, {"title": "Thinking Outside the BBox: Unconstrained Generative Object Compositing", "link": "http://arxiv.org/abs/2409.04559v2", "similarity": 0.493}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Constrained%20Adversarial%20Perturbation&body=Title%3A%20Constrained%20Adversarial%20Perturbation%0AAuthor%3A%20Virendra%20Nishad%20and%20Bhaskar%20Mukhoty%20and%20Hilal%20AlQuabeh%20and%20Sandeep%20K.%20Shukla%20and%20Sayak%20Ray%20Chowdhury%0AAbstract%3A%20%20%20Deep%20neural%20networks%20have%20achieved%20remarkable%20success%20in%20a%20wide%20range%20of%0Aclassification%20tasks.%20However%2C%20they%20remain%20highly%20susceptible%20to%20adversarial%0Aexamples%20-%20inputs%20that%20are%20subtly%20perturbed%20to%20induce%20misclassification%20while%0Aappearing%20unchanged%20to%20humans.%20Among%20various%20attack%20strategies%2C%20Universal%0AAdversarial%20Perturbations%20%28UAPs%29%20have%20emerged%20as%20a%20powerful%20tool%20for%20both%0Astress%20testing%20model%20robustness%20and%20facilitating%20scalable%20adversarial%20training.%0ADespite%20their%20effectiveness%2C%20most%20existing%20UAP%20methods%20neglect%20domain%20specific%0Aconstraints%20that%20govern%20feature%20relationships.%20Violating%20such%20constraints%2C%20such%0Aas%20debt%20to%20income%20ratios%20in%20credit%20scoring%20or%20packet%20flow%20invariants%20in%20network%0Acommunication%2C%20can%20render%20adversarial%20examples%20implausible%20or%20easily%0Adetectable%2C%20thereby%20limiting%20their%20real%20world%20applicability.%0A%20%20In%20this%20work%2C%20we%20advance%20universal%20adversarial%20attacks%20to%20constrained%20feature%0Aspaces%20by%20formulating%20an%20augmented%20Lagrangian%20based%20min%20max%20optimization%0Aproblem%20that%20enforces%20multiple%2C%20potentially%20complex%20constraints%20of%20varying%0Aimportance.%20We%20propose%20Constrained%20Adversarial%20Perturbation%20%28CAP%29%2C%20an%20efficient%0Aalgorithm%20that%20solves%20this%20problem%20using%20a%20gradient%20based%20alternating%0Aoptimization%20strategy.%20We%20evaluate%20CAP%20across%20diverse%20domains%20including%0Afinance%2C%20IT%20networks%2C%20and%20cyber%20physical%20systems%2C%20and%20demonstrate%20that%20it%0Aachieves%20higher%20attack%20success%20rates%20while%20significantly%20reducing%20runtime%0Acompared%20to%20existing%20baselines.%20Our%20approach%20also%20generalizes%20seamlessly%20to%0Aindividual%20adversarial%20perturbations%2C%20where%20we%20observe%20similar%20strong%0Aperformance%20gains.%20Finally%2C%20we%20introduce%20a%20principled%20procedure%20for%20learning%0Afeature%20constraints%20directly%20from%20data%2C%20enabling%20broad%20applicability%20across%0Adomains%20with%20structured%20input%20spaces.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2510.15699v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DConstrained%2520Adversarial%2520Perturbation%26entry.906535625%3DVirendra%2520Nishad%2520and%2520Bhaskar%2520Mukhoty%2520and%2520Hilal%2520AlQuabeh%2520and%2520Sandeep%2520K.%2520Shukla%2520and%2520Sayak%2520Ray%2520Chowdhury%26entry.1292438233%3D%2520%2520Deep%2520neural%2520networks%2520have%2520achieved%2520remarkable%2520success%2520in%2520a%2520wide%2520range%2520of%250Aclassification%2520tasks.%2520However%252C%2520they%2520remain%2520highly%2520susceptible%2520to%2520adversarial%250Aexamples%2520-%2520inputs%2520that%2520are%2520subtly%2520perturbed%2520to%2520induce%2520misclassification%2520while%250Aappearing%2520unchanged%2520to%2520humans.%2520Among%2520various%2520attack%2520strategies%252C%2520Universal%250AAdversarial%2520Perturbations%2520%2528UAPs%2529%2520have%2520emerged%2520as%2520a%2520powerful%2520tool%2520for%2520both%250Astress%2520testing%2520model%2520robustness%2520and%2520facilitating%2520scalable%2520adversarial%2520training.%250ADespite%2520their%2520effectiveness%252C%2520most%2520existing%2520UAP%2520methods%2520neglect%2520domain%2520specific%250Aconstraints%2520that%2520govern%2520feature%2520relationships.%2520Violating%2520such%2520constraints%252C%2520such%250Aas%2520debt%2520to%2520income%2520ratios%2520in%2520credit%2520scoring%2520or%2520packet%2520flow%2520invariants%2520in%2520network%250Acommunication%252C%2520can%2520render%2520adversarial%2520examples%2520implausible%2520or%2520easily%250Adetectable%252C%2520thereby%2520limiting%2520their%2520real%2520world%2520applicability.%250A%2520%2520In%2520this%2520work%252C%2520we%2520advance%2520universal%2520adversarial%2520attacks%2520to%2520constrained%2520feature%250Aspaces%2520by%2520formulating%2520an%2520augmented%2520Lagrangian%2520based%2520min%2520max%2520optimization%250Aproblem%2520that%2520enforces%2520multiple%252C%2520potentially%2520complex%2520constraints%2520of%2520varying%250Aimportance.%2520We%2520propose%2520Constrained%2520Adversarial%2520Perturbation%2520%2528CAP%2529%252C%2520an%2520efficient%250Aalgorithm%2520that%2520solves%2520this%2520problem%2520using%2520a%2520gradient%2520based%2520alternating%250Aoptimization%2520strategy.%2520We%2520evaluate%2520CAP%2520across%2520diverse%2520domains%2520including%250Afinance%252C%2520IT%2520networks%252C%2520and%2520cyber%2520physical%2520systems%252C%2520and%2520demonstrate%2520that%2520it%250Aachieves%2520higher%2520attack%2520success%2520rates%2520while%2520significantly%2520reducing%2520runtime%250Acompared%2520to%2520existing%2520baselines.%2520Our%2520approach%2520also%2520generalizes%2520seamlessly%2520to%250Aindividual%2520adversarial%2520perturbations%252C%2520where%2520we%2520observe%2520similar%2520strong%250Aperformance%2520gains.%2520Finally%252C%2520we%2520introduce%2520a%2520principled%2520procedure%2520for%2520learning%250Afeature%2520constraints%2520directly%2520from%2520data%252C%2520enabling%2520broad%2520applicability%2520across%250Adomains%2520with%2520structured%2520input%2520spaces.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2510.15699v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Constrained%20Adversarial%20Perturbation&entry.906535625=Virendra%20Nishad%20and%20Bhaskar%20Mukhoty%20and%20Hilal%20AlQuabeh%20and%20Sandeep%20K.%20Shukla%20and%20Sayak%20Ray%20Chowdhury&entry.1292438233=%20%20Deep%20neural%20networks%20have%20achieved%20remarkable%20success%20in%20a%20wide%20range%20of%0Aclassification%20tasks.%20However%2C%20they%20remain%20highly%20susceptible%20to%20adversarial%0Aexamples%20-%20inputs%20that%20are%20subtly%20perturbed%20to%20induce%20misclassification%20while%0Aappearing%20unchanged%20to%20humans.%20Among%20various%20attack%20strategies%2C%20Universal%0AAdversarial%20Perturbations%20%28UAPs%29%20have%20emerged%20as%20a%20powerful%20tool%20for%20both%0Astress%20testing%20model%20robustness%20and%20facilitating%20scalable%20adversarial%20training.%0ADespite%20their%20effectiveness%2C%20most%20existing%20UAP%20methods%20neglect%20domain%20specific%0Aconstraints%20that%20govern%20feature%20relationships.%20Violating%20such%20constraints%2C%20such%0Aas%20debt%20to%20income%20ratios%20in%20credit%20scoring%20or%20packet%20flow%20invariants%20in%20network%0Acommunication%2C%20can%20render%20adversarial%20examples%20implausible%20or%20easily%0Adetectable%2C%20thereby%20limiting%20their%20real%20world%20applicability.%0A%20%20In%20this%20work%2C%20we%20advance%20universal%20adversarial%20attacks%20to%20constrained%20feature%0Aspaces%20by%20formulating%20an%20augmented%20Lagrangian%20based%20min%20max%20optimization%0Aproblem%20that%20enforces%20multiple%2C%20potentially%20complex%20constraints%20of%20varying%0Aimportance.%20We%20propose%20Constrained%20Adversarial%20Perturbation%20%28CAP%29%2C%20an%20efficient%0Aalgorithm%20that%20solves%20this%20problem%20using%20a%20gradient%20based%20alternating%0Aoptimization%20strategy.%20We%20evaluate%20CAP%20across%20diverse%20domains%20including%0Afinance%2C%20IT%20networks%2C%20and%20cyber%20physical%20systems%2C%20and%20demonstrate%20that%20it%0Aachieves%20higher%20attack%20success%20rates%20while%20significantly%20reducing%20runtime%0Acompared%20to%20existing%20baselines.%20Our%20approach%20also%20generalizes%20seamlessly%20to%0Aindividual%20adversarial%20perturbations%2C%20where%20we%20observe%20similar%20strong%0Aperformance%20gains.%20Finally%2C%20we%20introduce%20a%20principled%20procedure%20for%20learning%0Afeature%20constraints%20directly%20from%20data%2C%20enabling%20broad%20applicability%20across%0Adomains%20with%20structured%20input%20spaces.%0A&entry.1838667208=http%3A//arxiv.org/abs/2510.15699v1&entry.124074799=Read"},
{"title": "InfiMed-ORBIT: Aligning LLMs on Open-Ended Complex Tasks via\n  Rubric-Based Incremental Training", "author": "Pengkai Wang and Qi Zuo and Pengwei Liu and Zhijie Sang and Congkai Xie and Hongxia Yang", "abstract": "  Large Language Models (LLMs) have shown substantial advances through\nreinforcement learning (RL), particularly in domains where rewards can be\nprogrammatically verified, such as mathematics and code. In these areas, models\nbenefit from a well-defined operational base guided by explicit rule-based\nobjectives. However, this progress reveals a significant limitation: in\nopen-ended domains where rewards are ambiguous, subjective, or\ncontext-dependent, such as creative writing, scientific reasoning, and notably\nmedical consultation, robust reward functions are lacking, making these areas\nchallenging for current RL strategies. To bridge this gap, we introduce ORBIT,\nan open-ended rubric-based incremental training framework specifically designed\nfor high-stakes medical dialogue. ORBIT integrates syn- thetic dialogue\ngeneration with the dynamic creation of rubrics, employing these rubrics to\ndirect an incremental RL process. In particular, this approach does not depend\non external medical knowledge or manual rules, instead utilizing rubric-guided\nfeedback to shape learning. When implemented on the Qwen3-4B-Instruct model,\nour method can greatly enhance its performance on the HealthBench-Hard\nbenchmark from 7.0 to 27.2 using only 2k samples, thus achieving\nstate-of-the-art results for models of this scale. Our analysis confirms that\nrubric-driven RL fos-ters consistent performance gains across diverse\nconsultation scenarios, going beyond simple numerical improvements. These\nfindings underscore rubric-based feedback as a scalable strategy for advancing\nLLMs in intricate, open-ended tasks.\n", "link": "http://arxiv.org/abs/2510.15859v1", "date": "2025-10-17", "relevancy": 2.0773, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5195}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5195}, {"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.5185}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20InfiMed-ORBIT%3A%20Aligning%20LLMs%20on%20Open-Ended%20Complex%20Tasks%20via%0A%20%20Rubric-Based%20Incremental%20Training&body=Title%3A%20InfiMed-ORBIT%3A%20Aligning%20LLMs%20on%20Open-Ended%20Complex%20Tasks%20via%0A%20%20Rubric-Based%20Incremental%20Training%0AAuthor%3A%20Pengkai%20Wang%20and%20Qi%20Zuo%20and%20Pengwei%20Liu%20and%20Zhijie%20Sang%20and%20Congkai%20Xie%20and%20Hongxia%20Yang%0AAbstract%3A%20%20%20Large%20Language%20Models%20%28LLMs%29%20have%20shown%20substantial%20advances%20through%0Areinforcement%20learning%20%28RL%29%2C%20particularly%20in%20domains%20where%20rewards%20can%20be%0Aprogrammatically%20verified%2C%20such%20as%20mathematics%20and%20code.%20In%20these%20areas%2C%20models%0Abenefit%20from%20a%20well-defined%20operational%20base%20guided%20by%20explicit%20rule-based%0Aobjectives.%20However%2C%20this%20progress%20reveals%20a%20significant%20limitation%3A%20in%0Aopen-ended%20domains%20where%20rewards%20are%20ambiguous%2C%20subjective%2C%20or%0Acontext-dependent%2C%20such%20as%20creative%20writing%2C%20scientific%20reasoning%2C%20and%20notably%0Amedical%20consultation%2C%20robust%20reward%20functions%20are%20lacking%2C%20making%20these%20areas%0Achallenging%20for%20current%20RL%20strategies.%20To%20bridge%20this%20gap%2C%20we%20introduce%20ORBIT%2C%0Aan%20open-ended%20rubric-based%20incremental%20training%20framework%20specifically%20designed%0Afor%20high-stakes%20medical%20dialogue.%20ORBIT%20integrates%20syn-%20thetic%20dialogue%0Ageneration%20with%20the%20dynamic%20creation%20of%20rubrics%2C%20employing%20these%20rubrics%20to%0Adirect%20an%20incremental%20RL%20process.%20In%20particular%2C%20this%20approach%20does%20not%20depend%0Aon%20external%20medical%20knowledge%20or%20manual%20rules%2C%20instead%20utilizing%20rubric-guided%0Afeedback%20to%20shape%20learning.%20When%20implemented%20on%20the%20Qwen3-4B-Instruct%20model%2C%0Aour%20method%20can%20greatly%20enhance%20its%20performance%20on%20the%20HealthBench-Hard%0Abenchmark%20from%207.0%20to%2027.2%20using%20only%202k%20samples%2C%20thus%20achieving%0Astate-of-the-art%20results%20for%20models%20of%20this%20scale.%20Our%20analysis%20confirms%20that%0Arubric-driven%20RL%20fos-ters%20consistent%20performance%20gains%20across%20diverse%0Aconsultation%20scenarios%2C%20going%20beyond%20simple%20numerical%20improvements.%20These%0Afindings%20underscore%20rubric-based%20feedback%20as%20a%20scalable%20strategy%20for%20advancing%0ALLMs%20in%20intricate%2C%20open-ended%20tasks.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2510.15859v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DInfiMed-ORBIT%253A%2520Aligning%2520LLMs%2520on%2520Open-Ended%2520Complex%2520Tasks%2520via%250A%2520%2520Rubric-Based%2520Incremental%2520Training%26entry.906535625%3DPengkai%2520Wang%2520and%2520Qi%2520Zuo%2520and%2520Pengwei%2520Liu%2520and%2520Zhijie%2520Sang%2520and%2520Congkai%2520Xie%2520and%2520Hongxia%2520Yang%26entry.1292438233%3D%2520%2520Large%2520Language%2520Models%2520%2528LLMs%2529%2520have%2520shown%2520substantial%2520advances%2520through%250Areinforcement%2520learning%2520%2528RL%2529%252C%2520particularly%2520in%2520domains%2520where%2520rewards%2520can%2520be%250Aprogrammatically%2520verified%252C%2520such%2520as%2520mathematics%2520and%2520code.%2520In%2520these%2520areas%252C%2520models%250Abenefit%2520from%2520a%2520well-defined%2520operational%2520base%2520guided%2520by%2520explicit%2520rule-based%250Aobjectives.%2520However%252C%2520this%2520progress%2520reveals%2520a%2520significant%2520limitation%253A%2520in%250Aopen-ended%2520domains%2520where%2520rewards%2520are%2520ambiguous%252C%2520subjective%252C%2520or%250Acontext-dependent%252C%2520such%2520as%2520creative%2520writing%252C%2520scientific%2520reasoning%252C%2520and%2520notably%250Amedical%2520consultation%252C%2520robust%2520reward%2520functions%2520are%2520lacking%252C%2520making%2520these%2520areas%250Achallenging%2520for%2520current%2520RL%2520strategies.%2520To%2520bridge%2520this%2520gap%252C%2520we%2520introduce%2520ORBIT%252C%250Aan%2520open-ended%2520rubric-based%2520incremental%2520training%2520framework%2520specifically%2520designed%250Afor%2520high-stakes%2520medical%2520dialogue.%2520ORBIT%2520integrates%2520syn-%2520thetic%2520dialogue%250Ageneration%2520with%2520the%2520dynamic%2520creation%2520of%2520rubrics%252C%2520employing%2520these%2520rubrics%2520to%250Adirect%2520an%2520incremental%2520RL%2520process.%2520In%2520particular%252C%2520this%2520approach%2520does%2520not%2520depend%250Aon%2520external%2520medical%2520knowledge%2520or%2520manual%2520rules%252C%2520instead%2520utilizing%2520rubric-guided%250Afeedback%2520to%2520shape%2520learning.%2520When%2520implemented%2520on%2520the%2520Qwen3-4B-Instruct%2520model%252C%250Aour%2520method%2520can%2520greatly%2520enhance%2520its%2520performance%2520on%2520the%2520HealthBench-Hard%250Abenchmark%2520from%25207.0%2520to%252027.2%2520using%2520only%25202k%2520samples%252C%2520thus%2520achieving%250Astate-of-the-art%2520results%2520for%2520models%2520of%2520this%2520scale.%2520Our%2520analysis%2520confirms%2520that%250Arubric-driven%2520RL%2520fos-ters%2520consistent%2520performance%2520gains%2520across%2520diverse%250Aconsultation%2520scenarios%252C%2520going%2520beyond%2520simple%2520numerical%2520improvements.%2520These%250Afindings%2520underscore%2520rubric-based%2520feedback%2520as%2520a%2520scalable%2520strategy%2520for%2520advancing%250ALLMs%2520in%2520intricate%252C%2520open-ended%2520tasks.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2510.15859v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=InfiMed-ORBIT%3A%20Aligning%20LLMs%20on%20Open-Ended%20Complex%20Tasks%20via%0A%20%20Rubric-Based%20Incremental%20Training&entry.906535625=Pengkai%20Wang%20and%20Qi%20Zuo%20and%20Pengwei%20Liu%20and%20Zhijie%20Sang%20and%20Congkai%20Xie%20and%20Hongxia%20Yang&entry.1292438233=%20%20Large%20Language%20Models%20%28LLMs%29%20have%20shown%20substantial%20advances%20through%0Areinforcement%20learning%20%28RL%29%2C%20particularly%20in%20domains%20where%20rewards%20can%20be%0Aprogrammatically%20verified%2C%20such%20as%20mathematics%20and%20code.%20In%20these%20areas%2C%20models%0Abenefit%20from%20a%20well-defined%20operational%20base%20guided%20by%20explicit%20rule-based%0Aobjectives.%20However%2C%20this%20progress%20reveals%20a%20significant%20limitation%3A%20in%0Aopen-ended%20domains%20where%20rewards%20are%20ambiguous%2C%20subjective%2C%20or%0Acontext-dependent%2C%20such%20as%20creative%20writing%2C%20scientific%20reasoning%2C%20and%20notably%0Amedical%20consultation%2C%20robust%20reward%20functions%20are%20lacking%2C%20making%20these%20areas%0Achallenging%20for%20current%20RL%20strategies.%20To%20bridge%20this%20gap%2C%20we%20introduce%20ORBIT%2C%0Aan%20open-ended%20rubric-based%20incremental%20training%20framework%20specifically%20designed%0Afor%20high-stakes%20medical%20dialogue.%20ORBIT%20integrates%20syn-%20thetic%20dialogue%0Ageneration%20with%20the%20dynamic%20creation%20of%20rubrics%2C%20employing%20these%20rubrics%20to%0Adirect%20an%20incremental%20RL%20process.%20In%20particular%2C%20this%20approach%20does%20not%20depend%0Aon%20external%20medical%20knowledge%20or%20manual%20rules%2C%20instead%20utilizing%20rubric-guided%0Afeedback%20to%20shape%20learning.%20When%20implemented%20on%20the%20Qwen3-4B-Instruct%20model%2C%0Aour%20method%20can%20greatly%20enhance%20its%20performance%20on%20the%20HealthBench-Hard%0Abenchmark%20from%207.0%20to%2027.2%20using%20only%202k%20samples%2C%20thus%20achieving%0Astate-of-the-art%20results%20for%20models%20of%20this%20scale.%20Our%20analysis%20confirms%20that%0Arubric-driven%20RL%20fos-ters%20consistent%20performance%20gains%20across%20diverse%0Aconsultation%20scenarios%2C%20going%20beyond%20simple%20numerical%20improvements.%20These%0Afindings%20underscore%20rubric-based%20feedback%20as%20a%20scalable%20strategy%20for%20advancing%0ALLMs%20in%20intricate%2C%20open-ended%20tasks.%0A&entry.1838667208=http%3A//arxiv.org/abs/2510.15859v1&entry.124074799=Read"},
{"title": "Deep Edge Filter: Return of the Human-Crafted Layer in Deep Learning", "author": "Dongkwan Lee and Junhoo Lee and Nojun Kwak", "abstract": "  We introduce the Deep Edge Filter, a novel approach that applies high-pass\nfiltering to deep neural network features to improve model generalizability.\nOur method is motivated by our hypothesis that neural networks encode\ntask-relevant semantic information in high-frequency components while storing\ndomain-specific biases in low-frequency components of deep features. By\nsubtracting low-pass filtered outputs from original features, our approach\nisolates generalizable representations while preserving architectural\nintegrity. Experimental results across diverse domains such as Vision, Text,\n3D, and Audio demonstrate consistent performance improvements regardless of\nmodel architecture and data modality. Analysis reveals that our method induces\nfeature sparsification and effectively isolates high-frequency components,\nproviding empirical validation of our core hypothesis. The code is available at\nhttps://github.com/dongkwani/DeepEdgeFilter.\n", "link": "http://arxiv.org/abs/2510.13865v2", "date": "2025-10-17", "relevancy": 2.0663, "topK": [{"title": "Boosted dynamic neural networks", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/26302", "similarity": 0.5484}, {"title": "Tuning-Free Noise Rectification for High Fidelity Image-to-Video\n  Generation", "link": "http://arxiv.org/abs/2403.02827v1", "similarity": 0.4955}, {"title": "Scaling Rectified Flow Transformers for High-Resolution Image Synthesis", "link": "http://arxiv.org/abs/2403.03206v1", "similarity": 0.4896}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Deep%20Edge%20Filter%3A%20Return%20of%20the%20Human-Crafted%20Layer%20in%20Deep%20Learning&body=Title%3A%20Deep%20Edge%20Filter%3A%20Return%20of%20the%20Human-Crafted%20Layer%20in%20Deep%20Learning%0AAuthor%3A%20Dongkwan%20Lee%20and%20Junhoo%20Lee%20and%20Nojun%20Kwak%0AAbstract%3A%20%20%20We%20introduce%20the%20Deep%20Edge%20Filter%2C%20a%20novel%20approach%20that%20applies%20high-pass%0Afiltering%20to%20deep%20neural%20network%20features%20to%20improve%20model%20generalizability.%0AOur%20method%20is%20motivated%20by%20our%20hypothesis%20that%20neural%20networks%20encode%0Atask-relevant%20semantic%20information%20in%20high-frequency%20components%20while%20storing%0Adomain-specific%20biases%20in%20low-frequency%20components%20of%20deep%20features.%20By%0Asubtracting%20low-pass%20filtered%20outputs%20from%20original%20features%2C%20our%20approach%0Aisolates%20generalizable%20representations%20while%20preserving%20architectural%0Aintegrity.%20Experimental%20results%20across%20diverse%20domains%20such%20as%20Vision%2C%20Text%2C%0A3D%2C%20and%20Audio%20demonstrate%20consistent%20performance%20improvements%20regardless%20of%0Amodel%20architecture%20and%20data%20modality.%20Analysis%20reveals%20that%20our%20method%20induces%0Afeature%20sparsification%20and%20effectively%20isolates%20high-frequency%20components%2C%0Aproviding%20empirical%20validation%20of%20our%20core%20hypothesis.%20The%20code%20is%20available%20at%0Ahttps%3A//github.com/dongkwani/DeepEdgeFilter.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2510.13865v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DDeep%2520Edge%2520Filter%253A%2520Return%2520of%2520the%2520Human-Crafted%2520Layer%2520in%2520Deep%2520Learning%26entry.906535625%3DDongkwan%2520Lee%2520and%2520Junhoo%2520Lee%2520and%2520Nojun%2520Kwak%26entry.1292438233%3D%2520%2520We%2520introduce%2520the%2520Deep%2520Edge%2520Filter%252C%2520a%2520novel%2520approach%2520that%2520applies%2520high-pass%250Afiltering%2520to%2520deep%2520neural%2520network%2520features%2520to%2520improve%2520model%2520generalizability.%250AOur%2520method%2520is%2520motivated%2520by%2520our%2520hypothesis%2520that%2520neural%2520networks%2520encode%250Atask-relevant%2520semantic%2520information%2520in%2520high-frequency%2520components%2520while%2520storing%250Adomain-specific%2520biases%2520in%2520low-frequency%2520components%2520of%2520deep%2520features.%2520By%250Asubtracting%2520low-pass%2520filtered%2520outputs%2520from%2520original%2520features%252C%2520our%2520approach%250Aisolates%2520generalizable%2520representations%2520while%2520preserving%2520architectural%250Aintegrity.%2520Experimental%2520results%2520across%2520diverse%2520domains%2520such%2520as%2520Vision%252C%2520Text%252C%250A3D%252C%2520and%2520Audio%2520demonstrate%2520consistent%2520performance%2520improvements%2520regardless%2520of%250Amodel%2520architecture%2520and%2520data%2520modality.%2520Analysis%2520reveals%2520that%2520our%2520method%2520induces%250Afeature%2520sparsification%2520and%2520effectively%2520isolates%2520high-frequency%2520components%252C%250Aproviding%2520empirical%2520validation%2520of%2520our%2520core%2520hypothesis.%2520The%2520code%2520is%2520available%2520at%250Ahttps%253A//github.com/dongkwani/DeepEdgeFilter.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2510.13865v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Deep%20Edge%20Filter%3A%20Return%20of%20the%20Human-Crafted%20Layer%20in%20Deep%20Learning&entry.906535625=Dongkwan%20Lee%20and%20Junhoo%20Lee%20and%20Nojun%20Kwak&entry.1292438233=%20%20We%20introduce%20the%20Deep%20Edge%20Filter%2C%20a%20novel%20approach%20that%20applies%20high-pass%0Afiltering%20to%20deep%20neural%20network%20features%20to%20improve%20model%20generalizability.%0AOur%20method%20is%20motivated%20by%20our%20hypothesis%20that%20neural%20networks%20encode%0Atask-relevant%20semantic%20information%20in%20high-frequency%20components%20while%20storing%0Adomain-specific%20biases%20in%20low-frequency%20components%20of%20deep%20features.%20By%0Asubtracting%20low-pass%20filtered%20outputs%20from%20original%20features%2C%20our%20approach%0Aisolates%20generalizable%20representations%20while%20preserving%20architectural%0Aintegrity.%20Experimental%20results%20across%20diverse%20domains%20such%20as%20Vision%2C%20Text%2C%0A3D%2C%20and%20Audio%20demonstrate%20consistent%20performance%20improvements%20regardless%20of%0Amodel%20architecture%20and%20data%20modality.%20Analysis%20reveals%20that%20our%20method%20induces%0Afeature%20sparsification%20and%20effectively%20isolates%20high-frequency%20components%2C%0Aproviding%20empirical%20validation%20of%20our%20core%20hypothesis.%20The%20code%20is%20available%20at%0Ahttps%3A//github.com/dongkwani/DeepEdgeFilter.%0A&entry.1838667208=http%3A//arxiv.org/abs/2510.13865v2&entry.124074799=Read"},
{"title": "CausalVerse: Benchmarking Causal Representation Learning with\n  Configurable High-Fidelity Simulations", "author": "Guangyi Chen and Yunlong Deng and Peiyuan Zhu and Yan Li and Yifan Shen and Zijian Li and Kun Zhang", "abstract": "  Causal Representation Learning (CRL) aims to uncover the data-generating\nprocess and identify the underlying causal variables and relations, whose\nevaluation remains inherently challenging due to the requirement of known\nground-truth causal variables and causal structure. Existing evaluations often\nrely on either simplistic synthetic datasets or downstream performance on\nreal-world tasks, generally suffering a dilemma between realism and evaluative\nprecision. In this paper, we introduce a new benchmark for CRL using\nhigh-fidelity simulated visual data that retains both realistic visual\ncomplexity and, more importantly, access to ground-truth causal generating\nprocesses. The dataset comprises around 200 thousand images and 3 million video\nframes across 24 sub-scenes in four domains: static image generation, dynamic\nphysical simulations, robotic manipulations, and traffic situation analysis.\nThese scenarios range from static to dynamic settings, simple to complex\nstructures, and single to multi-agent interactions, offering a comprehensive\ntestbed that hopefully bridges the gap between rigorous evaluation and\nreal-world applicability. In addition, we provide flexible access to the\nunderlying causal structures, allowing users to modify or configure them to\nalign with the required assumptions in CRL, such as available domain labels,\ntemporal dependencies, or intervention histories. Leveraging this benchmark, we\nevaluated representative CRL methods across diverse paradigms and offered\nempirical insights to assist practitioners and newcomers in choosing or\nextending appropriate CRL frameworks to properly address specific types of real\nproblems that can benefit from the CRL perspective. Welcome to visit our:\nProject page:https://causal-verse.github.io/,\nDataset:https://huggingface.co/CausalVerse.\n", "link": "http://arxiv.org/abs/2510.14049v2", "date": "2025-10-17", "relevancy": 2.0588, "topK": [{"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.5184}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.514}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.514}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20CausalVerse%3A%20Benchmarking%20Causal%20Representation%20Learning%20with%0A%20%20Configurable%20High-Fidelity%20Simulations&body=Title%3A%20CausalVerse%3A%20Benchmarking%20Causal%20Representation%20Learning%20with%0A%20%20Configurable%20High-Fidelity%20Simulations%0AAuthor%3A%20Guangyi%20Chen%20and%20Yunlong%20Deng%20and%20Peiyuan%20Zhu%20and%20Yan%20Li%20and%20Yifan%20Shen%20and%20Zijian%20Li%20and%20Kun%20Zhang%0AAbstract%3A%20%20%20Causal%20Representation%20Learning%20%28CRL%29%20aims%20to%20uncover%20the%20data-generating%0Aprocess%20and%20identify%20the%20underlying%20causal%20variables%20and%20relations%2C%20whose%0Aevaluation%20remains%20inherently%20challenging%20due%20to%20the%20requirement%20of%20known%0Aground-truth%20causal%20variables%20and%20causal%20structure.%20Existing%20evaluations%20often%0Arely%20on%20either%20simplistic%20synthetic%20datasets%20or%20downstream%20performance%20on%0Areal-world%20tasks%2C%20generally%20suffering%20a%20dilemma%20between%20realism%20and%20evaluative%0Aprecision.%20In%20this%20paper%2C%20we%20introduce%20a%20new%20benchmark%20for%20CRL%20using%0Ahigh-fidelity%20simulated%20visual%20data%20that%20retains%20both%20realistic%20visual%0Acomplexity%20and%2C%20more%20importantly%2C%20access%20to%20ground-truth%20causal%20generating%0Aprocesses.%20The%20dataset%20comprises%20around%20200%20thousand%20images%20and%203%20million%20video%0Aframes%20across%2024%20sub-scenes%20in%20four%20domains%3A%20static%20image%20generation%2C%20dynamic%0Aphysical%20simulations%2C%20robotic%20manipulations%2C%20and%20traffic%20situation%20analysis.%0AThese%20scenarios%20range%20from%20static%20to%20dynamic%20settings%2C%20simple%20to%20complex%0Astructures%2C%20and%20single%20to%20multi-agent%20interactions%2C%20offering%20a%20comprehensive%0Atestbed%20that%20hopefully%20bridges%20the%20gap%20between%20rigorous%20evaluation%20and%0Areal-world%20applicability.%20In%20addition%2C%20we%20provide%20flexible%20access%20to%20the%0Aunderlying%20causal%20structures%2C%20allowing%20users%20to%20modify%20or%20configure%20them%20to%0Aalign%20with%20the%20required%20assumptions%20in%20CRL%2C%20such%20as%20available%20domain%20labels%2C%0Atemporal%20dependencies%2C%20or%20intervention%20histories.%20Leveraging%20this%20benchmark%2C%20we%0Aevaluated%20representative%20CRL%20methods%20across%20diverse%20paradigms%20and%20offered%0Aempirical%20insights%20to%20assist%20practitioners%20and%20newcomers%20in%20choosing%20or%0Aextending%20appropriate%20CRL%20frameworks%20to%20properly%20address%20specific%20types%20of%20real%0Aproblems%20that%20can%20benefit%20from%20the%20CRL%20perspective.%20Welcome%20to%20visit%20our%3A%0AProject%20page%3Ahttps%3A//causal-verse.github.io/%2C%0ADataset%3Ahttps%3A//huggingface.co/CausalVerse.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2510.14049v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DCausalVerse%253A%2520Benchmarking%2520Causal%2520Representation%2520Learning%2520with%250A%2520%2520Configurable%2520High-Fidelity%2520Simulations%26entry.906535625%3DGuangyi%2520Chen%2520and%2520Yunlong%2520Deng%2520and%2520Peiyuan%2520Zhu%2520and%2520Yan%2520Li%2520and%2520Yifan%2520Shen%2520and%2520Zijian%2520Li%2520and%2520Kun%2520Zhang%26entry.1292438233%3D%2520%2520Causal%2520Representation%2520Learning%2520%2528CRL%2529%2520aims%2520to%2520uncover%2520the%2520data-generating%250Aprocess%2520and%2520identify%2520the%2520underlying%2520causal%2520variables%2520and%2520relations%252C%2520whose%250Aevaluation%2520remains%2520inherently%2520challenging%2520due%2520to%2520the%2520requirement%2520of%2520known%250Aground-truth%2520causal%2520variables%2520and%2520causal%2520structure.%2520Existing%2520evaluations%2520often%250Arely%2520on%2520either%2520simplistic%2520synthetic%2520datasets%2520or%2520downstream%2520performance%2520on%250Areal-world%2520tasks%252C%2520generally%2520suffering%2520a%2520dilemma%2520between%2520realism%2520and%2520evaluative%250Aprecision.%2520In%2520this%2520paper%252C%2520we%2520introduce%2520a%2520new%2520benchmark%2520for%2520CRL%2520using%250Ahigh-fidelity%2520simulated%2520visual%2520data%2520that%2520retains%2520both%2520realistic%2520visual%250Acomplexity%2520and%252C%2520more%2520importantly%252C%2520access%2520to%2520ground-truth%2520causal%2520generating%250Aprocesses.%2520The%2520dataset%2520comprises%2520around%2520200%2520thousand%2520images%2520and%25203%2520million%2520video%250Aframes%2520across%252024%2520sub-scenes%2520in%2520four%2520domains%253A%2520static%2520image%2520generation%252C%2520dynamic%250Aphysical%2520simulations%252C%2520robotic%2520manipulations%252C%2520and%2520traffic%2520situation%2520analysis.%250AThese%2520scenarios%2520range%2520from%2520static%2520to%2520dynamic%2520settings%252C%2520simple%2520to%2520complex%250Astructures%252C%2520and%2520single%2520to%2520multi-agent%2520interactions%252C%2520offering%2520a%2520comprehensive%250Atestbed%2520that%2520hopefully%2520bridges%2520the%2520gap%2520between%2520rigorous%2520evaluation%2520and%250Areal-world%2520applicability.%2520In%2520addition%252C%2520we%2520provide%2520flexible%2520access%2520to%2520the%250Aunderlying%2520causal%2520structures%252C%2520allowing%2520users%2520to%2520modify%2520or%2520configure%2520them%2520to%250Aalign%2520with%2520the%2520required%2520assumptions%2520in%2520CRL%252C%2520such%2520as%2520available%2520domain%2520labels%252C%250Atemporal%2520dependencies%252C%2520or%2520intervention%2520histories.%2520Leveraging%2520this%2520benchmark%252C%2520we%250Aevaluated%2520representative%2520CRL%2520methods%2520across%2520diverse%2520paradigms%2520and%2520offered%250Aempirical%2520insights%2520to%2520assist%2520practitioners%2520and%2520newcomers%2520in%2520choosing%2520or%250Aextending%2520appropriate%2520CRL%2520frameworks%2520to%2520properly%2520address%2520specific%2520types%2520of%2520real%250Aproblems%2520that%2520can%2520benefit%2520from%2520the%2520CRL%2520perspective.%2520Welcome%2520to%2520visit%2520our%253A%250AProject%2520page%253Ahttps%253A//causal-verse.github.io/%252C%250ADataset%253Ahttps%253A//huggingface.co/CausalVerse.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2510.14049v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=CausalVerse%3A%20Benchmarking%20Causal%20Representation%20Learning%20with%0A%20%20Configurable%20High-Fidelity%20Simulations&entry.906535625=Guangyi%20Chen%20and%20Yunlong%20Deng%20and%20Peiyuan%20Zhu%20and%20Yan%20Li%20and%20Yifan%20Shen%20and%20Zijian%20Li%20and%20Kun%20Zhang&entry.1292438233=%20%20Causal%20Representation%20Learning%20%28CRL%29%20aims%20to%20uncover%20the%20data-generating%0Aprocess%20and%20identify%20the%20underlying%20causal%20variables%20and%20relations%2C%20whose%0Aevaluation%20remains%20inherently%20challenging%20due%20to%20the%20requirement%20of%20known%0Aground-truth%20causal%20variables%20and%20causal%20structure.%20Existing%20evaluations%20often%0Arely%20on%20either%20simplistic%20synthetic%20datasets%20or%20downstream%20performance%20on%0Areal-world%20tasks%2C%20generally%20suffering%20a%20dilemma%20between%20realism%20and%20evaluative%0Aprecision.%20In%20this%20paper%2C%20we%20introduce%20a%20new%20benchmark%20for%20CRL%20using%0Ahigh-fidelity%20simulated%20visual%20data%20that%20retains%20both%20realistic%20visual%0Acomplexity%20and%2C%20more%20importantly%2C%20access%20to%20ground-truth%20causal%20generating%0Aprocesses.%20The%20dataset%20comprises%20around%20200%20thousand%20images%20and%203%20million%20video%0Aframes%20across%2024%20sub-scenes%20in%20four%20domains%3A%20static%20image%20generation%2C%20dynamic%0Aphysical%20simulations%2C%20robotic%20manipulations%2C%20and%20traffic%20situation%20analysis.%0AThese%20scenarios%20range%20from%20static%20to%20dynamic%20settings%2C%20simple%20to%20complex%0Astructures%2C%20and%20single%20to%20multi-agent%20interactions%2C%20offering%20a%20comprehensive%0Atestbed%20that%20hopefully%20bridges%20the%20gap%20between%20rigorous%20evaluation%20and%0Areal-world%20applicability.%20In%20addition%2C%20we%20provide%20flexible%20access%20to%20the%0Aunderlying%20causal%20structures%2C%20allowing%20users%20to%20modify%20or%20configure%20them%20to%0Aalign%20with%20the%20required%20assumptions%20in%20CRL%2C%20such%20as%20available%20domain%20labels%2C%0Atemporal%20dependencies%2C%20or%20intervention%20histories.%20Leveraging%20this%20benchmark%2C%20we%0Aevaluated%20representative%20CRL%20methods%20across%20diverse%20paradigms%20and%20offered%0Aempirical%20insights%20to%20assist%20practitioners%20and%20newcomers%20in%20choosing%20or%0Aextending%20appropriate%20CRL%20frameworks%20to%20properly%20address%20specific%20types%20of%20real%0Aproblems%20that%20can%20benefit%20from%20the%20CRL%20perspective.%20Welcome%20to%20visit%20our%3A%0AProject%20page%3Ahttps%3A//causal-verse.github.io/%2C%0ADataset%3Ahttps%3A//huggingface.co/CausalVerse.%0A&entry.1838667208=http%3A//arxiv.org/abs/2510.14049v2&entry.124074799=Read"},
{"title": "GRATING: Low-Latency and Memory-Efficient Semantic Selection on Device", "author": "Jiahao Zhou and Chengliang Lin and Dingji Li and Mingkai Dong and Haibo Chen", "abstract": "  Semantic top-K selection with cross-encoder rerankers underpins of on-device\nAI services, such as retrieval-augmented generation, agent memory, and\npersonalized recommendation. However, its latency and memory demands dominate\nend-to-end budgets on edge hardware. Revisiting the objective of top-K\nselection, we reveal that only relative rankings matter, not exact\nper-candidate scores. We further observe sequence-level sparsity: relative\nrankings stabilize early in intermediate layers, allowing pruning opportunities\nprior to completing full inference.\n  Building on this insight, we propose monolithic forwarding and develop a\ntraining-free inference system, GRATING. By maintaining a global view of all\ncandidates, it reduces latency through progressive cluster pruning. It also\nbounds peak memory usage by strategically overlapping I/O with computation via\ndual-layer sliding window and chunked execution. We evaluate GRATING against\nstate-of-the-art baselines on rerankers from 0.6B to 8B parameters across Apple\nM2 and RTX 5070. GRATING consistently reduces latency by up to 89.0% and peak\nmemory by up to 94.9% in microbenchmarks, without any loss in precision. Across\nthree real-world on-device AI applications, GRATING lowers latency by\n11.6%-51.0% and peak memory by 18.6%-77.8%, demonstrating substantial\nimprovements in efficiency and deployability.\n", "link": "http://arxiv.org/abs/2510.15620v1", "date": "2025-10-17", "relevancy": 2.0562, "topK": [{"title": "RTG-SLAM: Real-time 3D Reconstruction at Scale using Gaussian Splatting", "link": "http://arxiv.org/abs/2404.19706v1", "similarity": 0.5166}, {"title": "ToDo: Token Downsampling for Efficient Generation of High-Resolution\n  Images", "link": "http://arxiv.org/abs/2402.13573v2", "similarity": 0.5143}, {"title": "SDXS: Real-Time One-Step Latent Diffusion Models with Image Conditions", "link": "http://arxiv.org/abs/2403.16627v1", "similarity": 0.5072}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20GRATING%3A%20Low-Latency%20and%20Memory-Efficient%20Semantic%20Selection%20on%20Device&body=Title%3A%20GRATING%3A%20Low-Latency%20and%20Memory-Efficient%20Semantic%20Selection%20on%20Device%0AAuthor%3A%20Jiahao%20Zhou%20and%20Chengliang%20Lin%20and%20Dingji%20Li%20and%20Mingkai%20Dong%20and%20Haibo%20Chen%0AAbstract%3A%20%20%20Semantic%20top-K%20selection%20with%20cross-encoder%20rerankers%20underpins%20of%20on-device%0AAI%20services%2C%20such%20as%20retrieval-augmented%20generation%2C%20agent%20memory%2C%20and%0Apersonalized%20recommendation.%20However%2C%20its%20latency%20and%20memory%20demands%20dominate%0Aend-to-end%20budgets%20on%20edge%20hardware.%20Revisiting%20the%20objective%20of%20top-K%0Aselection%2C%20we%20reveal%20that%20only%20relative%20rankings%20matter%2C%20not%20exact%0Aper-candidate%20scores.%20We%20further%20observe%20sequence-level%20sparsity%3A%20relative%0Arankings%20stabilize%20early%20in%20intermediate%20layers%2C%20allowing%20pruning%20opportunities%0Aprior%20to%20completing%20full%20inference.%0A%20%20Building%20on%20this%20insight%2C%20we%20propose%20monolithic%20forwarding%20and%20develop%20a%0Atraining-free%20inference%20system%2C%20GRATING.%20By%20maintaining%20a%20global%20view%20of%20all%0Acandidates%2C%20it%20reduces%20latency%20through%20progressive%20cluster%20pruning.%20It%20also%0Abounds%20peak%20memory%20usage%20by%20strategically%20overlapping%20I/O%20with%20computation%20via%0Adual-layer%20sliding%20window%20and%20chunked%20execution.%20We%20evaluate%20GRATING%20against%0Astate-of-the-art%20baselines%20on%20rerankers%20from%200.6B%20to%208B%20parameters%20across%20Apple%0AM2%20and%20RTX%205070.%20GRATING%20consistently%20reduces%20latency%20by%20up%20to%2089.0%25%20and%20peak%0Amemory%20by%20up%20to%2094.9%25%20in%20microbenchmarks%2C%20without%20any%20loss%20in%20precision.%20Across%0Athree%20real-world%20on-device%20AI%20applications%2C%20GRATING%20lowers%20latency%20by%0A11.6%25-51.0%25%20and%20peak%20memory%20by%2018.6%25-77.8%25%2C%20demonstrating%20substantial%0Aimprovements%20in%20efficiency%20and%20deployability.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2510.15620v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DGRATING%253A%2520Low-Latency%2520and%2520Memory-Efficient%2520Semantic%2520Selection%2520on%2520Device%26entry.906535625%3DJiahao%2520Zhou%2520and%2520Chengliang%2520Lin%2520and%2520Dingji%2520Li%2520and%2520Mingkai%2520Dong%2520and%2520Haibo%2520Chen%26entry.1292438233%3D%2520%2520Semantic%2520top-K%2520selection%2520with%2520cross-encoder%2520rerankers%2520underpins%2520of%2520on-device%250AAI%2520services%252C%2520such%2520as%2520retrieval-augmented%2520generation%252C%2520agent%2520memory%252C%2520and%250Apersonalized%2520recommendation.%2520However%252C%2520its%2520latency%2520and%2520memory%2520demands%2520dominate%250Aend-to-end%2520budgets%2520on%2520edge%2520hardware.%2520Revisiting%2520the%2520objective%2520of%2520top-K%250Aselection%252C%2520we%2520reveal%2520that%2520only%2520relative%2520rankings%2520matter%252C%2520not%2520exact%250Aper-candidate%2520scores.%2520We%2520further%2520observe%2520sequence-level%2520sparsity%253A%2520relative%250Arankings%2520stabilize%2520early%2520in%2520intermediate%2520layers%252C%2520allowing%2520pruning%2520opportunities%250Aprior%2520to%2520completing%2520full%2520inference.%250A%2520%2520Building%2520on%2520this%2520insight%252C%2520we%2520propose%2520monolithic%2520forwarding%2520and%2520develop%2520a%250Atraining-free%2520inference%2520system%252C%2520GRATING.%2520By%2520maintaining%2520a%2520global%2520view%2520of%2520all%250Acandidates%252C%2520it%2520reduces%2520latency%2520through%2520progressive%2520cluster%2520pruning.%2520It%2520also%250Abounds%2520peak%2520memory%2520usage%2520by%2520strategically%2520overlapping%2520I/O%2520with%2520computation%2520via%250Adual-layer%2520sliding%2520window%2520and%2520chunked%2520execution.%2520We%2520evaluate%2520GRATING%2520against%250Astate-of-the-art%2520baselines%2520on%2520rerankers%2520from%25200.6B%2520to%25208B%2520parameters%2520across%2520Apple%250AM2%2520and%2520RTX%25205070.%2520GRATING%2520consistently%2520reduces%2520latency%2520by%2520up%2520to%252089.0%2525%2520and%2520peak%250Amemory%2520by%2520up%2520to%252094.9%2525%2520in%2520microbenchmarks%252C%2520without%2520any%2520loss%2520in%2520precision.%2520Across%250Athree%2520real-world%2520on-device%2520AI%2520applications%252C%2520GRATING%2520lowers%2520latency%2520by%250A11.6%2525-51.0%2525%2520and%2520peak%2520memory%2520by%252018.6%2525-77.8%2525%252C%2520demonstrating%2520substantial%250Aimprovements%2520in%2520efficiency%2520and%2520deployability.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2510.15620v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=GRATING%3A%20Low-Latency%20and%20Memory-Efficient%20Semantic%20Selection%20on%20Device&entry.906535625=Jiahao%20Zhou%20and%20Chengliang%20Lin%20and%20Dingji%20Li%20and%20Mingkai%20Dong%20and%20Haibo%20Chen&entry.1292438233=%20%20Semantic%20top-K%20selection%20with%20cross-encoder%20rerankers%20underpins%20of%20on-device%0AAI%20services%2C%20such%20as%20retrieval-augmented%20generation%2C%20agent%20memory%2C%20and%0Apersonalized%20recommendation.%20However%2C%20its%20latency%20and%20memory%20demands%20dominate%0Aend-to-end%20budgets%20on%20edge%20hardware.%20Revisiting%20the%20objective%20of%20top-K%0Aselection%2C%20we%20reveal%20that%20only%20relative%20rankings%20matter%2C%20not%20exact%0Aper-candidate%20scores.%20We%20further%20observe%20sequence-level%20sparsity%3A%20relative%0Arankings%20stabilize%20early%20in%20intermediate%20layers%2C%20allowing%20pruning%20opportunities%0Aprior%20to%20completing%20full%20inference.%0A%20%20Building%20on%20this%20insight%2C%20we%20propose%20monolithic%20forwarding%20and%20develop%20a%0Atraining-free%20inference%20system%2C%20GRATING.%20By%20maintaining%20a%20global%20view%20of%20all%0Acandidates%2C%20it%20reduces%20latency%20through%20progressive%20cluster%20pruning.%20It%20also%0Abounds%20peak%20memory%20usage%20by%20strategically%20overlapping%20I/O%20with%20computation%20via%0Adual-layer%20sliding%20window%20and%20chunked%20execution.%20We%20evaluate%20GRATING%20against%0Astate-of-the-art%20baselines%20on%20rerankers%20from%200.6B%20to%208B%20parameters%20across%20Apple%0AM2%20and%20RTX%205070.%20GRATING%20consistently%20reduces%20latency%20by%20up%20to%2089.0%25%20and%20peak%0Amemory%20by%20up%20to%2094.9%25%20in%20microbenchmarks%2C%20without%20any%20loss%20in%20precision.%20Across%0Athree%20real-world%20on-device%20AI%20applications%2C%20GRATING%20lowers%20latency%20by%0A11.6%25-51.0%25%20and%20peak%20memory%20by%2018.6%25-77.8%25%2C%20demonstrating%20substantial%0Aimprovements%20in%20efficiency%20and%20deployability.%0A&entry.1838667208=http%3A//arxiv.org/abs/2510.15620v1&entry.124074799=Read"},
{"title": "AB-UPT for Automotive and Aerospace Applications", "author": "Benedikt Alkin and Richard Kurle and Louis Serrano and Dennis Just and Johannes Brandstetter", "abstract": "  The recently proposed Anchored-Branched Universal Physics Transformers\n(AB-UPT) shows strong capabilities to replicate automotive computational fluid\ndynamics simulations requiring orders of magnitudes less compute than\ntraditional numerical solvers. In this technical report, we add two new\ndatasets to the body of empirically evaluated use-cases of AB-UPT, combining\nhigh-quality data generation with state-of-the-art neural surrogates. Both\ndatasets were generated with the Luminary Cloud platform containing automotives\n(SHIFT-SUV) and aircrafts (SHIFT-Wing). We start by detailing the data\ngeneration. Next, we show favorable performances of AB-UPT against previous\nstate-of-the-art transformer-based baselines on both datasets, followed by\nextensive qualitative and quantitative evaluations of our best AB-UPT model.\nAB-UPT shows strong performances across the board. Notably, it obtains near\nperfect prediction of integrated aerodynamic forces within seconds from a\nsimple isotopically tesselate geometry representation and is trainable within a\nday on a single GPU, paving the way for industry-scale applications.\n", "link": "http://arxiv.org/abs/2510.15808v1", "date": "2025-10-17", "relevancy": 2.0489, "topK": [{"title": "GST: Precise 3D Human Body from a Single Image with Gaussian Splatting\n  Transformers", "link": "http://arxiv.org/abs/2409.04196v1", "similarity": 0.517}, {"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.5106}, {"title": "Scaling Rectified Flow Transformers for High-Resolution Image Synthesis", "link": "http://arxiv.org/abs/2403.03206v1", "similarity": 0.5043}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20AB-UPT%20for%20Automotive%20and%20Aerospace%20Applications&body=Title%3A%20AB-UPT%20for%20Automotive%20and%20Aerospace%20Applications%0AAuthor%3A%20Benedikt%20Alkin%20and%20Richard%20Kurle%20and%20Louis%20Serrano%20and%20Dennis%20Just%20and%20Johannes%20Brandstetter%0AAbstract%3A%20%20%20The%20recently%20proposed%20Anchored-Branched%20Universal%20Physics%20Transformers%0A%28AB-UPT%29%20shows%20strong%20capabilities%20to%20replicate%20automotive%20computational%20fluid%0Adynamics%20simulations%20requiring%20orders%20of%20magnitudes%20less%20compute%20than%0Atraditional%20numerical%20solvers.%20In%20this%20technical%20report%2C%20we%20add%20two%20new%0Adatasets%20to%20the%20body%20of%20empirically%20evaluated%20use-cases%20of%20AB-UPT%2C%20combining%0Ahigh-quality%20data%20generation%20with%20state-of-the-art%20neural%20surrogates.%20Both%0Adatasets%20were%20generated%20with%20the%20Luminary%20Cloud%20platform%20containing%20automotives%0A%28SHIFT-SUV%29%20and%20aircrafts%20%28SHIFT-Wing%29.%20We%20start%20by%20detailing%20the%20data%0Ageneration.%20Next%2C%20we%20show%20favorable%20performances%20of%20AB-UPT%20against%20previous%0Astate-of-the-art%20transformer-based%20baselines%20on%20both%20datasets%2C%20followed%20by%0Aextensive%20qualitative%20and%20quantitative%20evaluations%20of%20our%20best%20AB-UPT%20model.%0AAB-UPT%20shows%20strong%20performances%20across%20the%20board.%20Notably%2C%20it%20obtains%20near%0Aperfect%20prediction%20of%20integrated%20aerodynamic%20forces%20within%20seconds%20from%20a%0Asimple%20isotopically%20tesselate%20geometry%20representation%20and%20is%20trainable%20within%20a%0Aday%20on%20a%20single%20GPU%2C%20paving%20the%20way%20for%20industry-scale%20applications.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2510.15808v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DAB-UPT%2520for%2520Automotive%2520and%2520Aerospace%2520Applications%26entry.906535625%3DBenedikt%2520Alkin%2520and%2520Richard%2520Kurle%2520and%2520Louis%2520Serrano%2520and%2520Dennis%2520Just%2520and%2520Johannes%2520Brandstetter%26entry.1292438233%3D%2520%2520The%2520recently%2520proposed%2520Anchored-Branched%2520Universal%2520Physics%2520Transformers%250A%2528AB-UPT%2529%2520shows%2520strong%2520capabilities%2520to%2520replicate%2520automotive%2520computational%2520fluid%250Adynamics%2520simulations%2520requiring%2520orders%2520of%2520magnitudes%2520less%2520compute%2520than%250Atraditional%2520numerical%2520solvers.%2520In%2520this%2520technical%2520report%252C%2520we%2520add%2520two%2520new%250Adatasets%2520to%2520the%2520body%2520of%2520empirically%2520evaluated%2520use-cases%2520of%2520AB-UPT%252C%2520combining%250Ahigh-quality%2520data%2520generation%2520with%2520state-of-the-art%2520neural%2520surrogates.%2520Both%250Adatasets%2520were%2520generated%2520with%2520the%2520Luminary%2520Cloud%2520platform%2520containing%2520automotives%250A%2528SHIFT-SUV%2529%2520and%2520aircrafts%2520%2528SHIFT-Wing%2529.%2520We%2520start%2520by%2520detailing%2520the%2520data%250Ageneration.%2520Next%252C%2520we%2520show%2520favorable%2520performances%2520of%2520AB-UPT%2520against%2520previous%250Astate-of-the-art%2520transformer-based%2520baselines%2520on%2520both%2520datasets%252C%2520followed%2520by%250Aextensive%2520qualitative%2520and%2520quantitative%2520evaluations%2520of%2520our%2520best%2520AB-UPT%2520model.%250AAB-UPT%2520shows%2520strong%2520performances%2520across%2520the%2520board.%2520Notably%252C%2520it%2520obtains%2520near%250Aperfect%2520prediction%2520of%2520integrated%2520aerodynamic%2520forces%2520within%2520seconds%2520from%2520a%250Asimple%2520isotopically%2520tesselate%2520geometry%2520representation%2520and%2520is%2520trainable%2520within%2520a%250Aday%2520on%2520a%2520single%2520GPU%252C%2520paving%2520the%2520way%2520for%2520industry-scale%2520applications.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2510.15808v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=AB-UPT%20for%20Automotive%20and%20Aerospace%20Applications&entry.906535625=Benedikt%20Alkin%20and%20Richard%20Kurle%20and%20Louis%20Serrano%20and%20Dennis%20Just%20and%20Johannes%20Brandstetter&entry.1292438233=%20%20The%20recently%20proposed%20Anchored-Branched%20Universal%20Physics%20Transformers%0A%28AB-UPT%29%20shows%20strong%20capabilities%20to%20replicate%20automotive%20computational%20fluid%0Adynamics%20simulations%20requiring%20orders%20of%20magnitudes%20less%20compute%20than%0Atraditional%20numerical%20solvers.%20In%20this%20technical%20report%2C%20we%20add%20two%20new%0Adatasets%20to%20the%20body%20of%20empirically%20evaluated%20use-cases%20of%20AB-UPT%2C%20combining%0Ahigh-quality%20data%20generation%20with%20state-of-the-art%20neural%20surrogates.%20Both%0Adatasets%20were%20generated%20with%20the%20Luminary%20Cloud%20platform%20containing%20automotives%0A%28SHIFT-SUV%29%20and%20aircrafts%20%28SHIFT-Wing%29.%20We%20start%20by%20detailing%20the%20data%0Ageneration.%20Next%2C%20we%20show%20favorable%20performances%20of%20AB-UPT%20against%20previous%0Astate-of-the-art%20transformer-based%20baselines%20on%20both%20datasets%2C%20followed%20by%0Aextensive%20qualitative%20and%20quantitative%20evaluations%20of%20our%20best%20AB-UPT%20model.%0AAB-UPT%20shows%20strong%20performances%20across%20the%20board.%20Notably%2C%20it%20obtains%20near%0Aperfect%20prediction%20of%20integrated%20aerodynamic%20forces%20within%20seconds%20from%20a%0Asimple%20isotopically%20tesselate%20geometry%20representation%20and%20is%20trainable%20within%20a%0Aday%20on%20a%20single%20GPU%2C%20paving%20the%20way%20for%20industry-scale%20applications.%0A&entry.1838667208=http%3A//arxiv.org/abs/2510.15808v1&entry.124074799=Read"},
{"title": "Self-evolving expertise in complex non-verifiable subject domains:\n  dialogue as implicit meta-RL", "author": "Richard M. Bailey", "abstract": "  So-called `wicked problems', those involving complex multi-dimensional\nsettings, non-verifiable outcomes, heterogeneous impacts and a lack of single\nobjectively correct answers, have plagued humans throughout history. Modern\nexamples include decisions over justice frameworks, solving environmental\npollution, planning for pandemic resilience and food security. The use of\nstate-of-the-art artificial intelligence systems (notably Large Language\nModel-based agents) collaborating with humans on solving such problems is being\nactively explored. While the abilities of LLMs can be improved by, for example,\nfine-tuning, hand-crafted system prompts and scaffolding with external tools,\nLLMs lack endogenous mechanisms to develop expertise through experience in such\nsettings. This work address this gap with Dialectica, a framework where agents\nengage in structured dialogue on defined topics, augmented by memory,\nself-reflection, and policy-constrained context editing. Formally, discussion\nis viewed as an implicit meta-reinforcement learning process. The\n`dialogue-trained' agents are evaluated post-hoc using judged pairwise\ncomparisons of elicited responses. Across two model architectures (locally run\nQwen3:30b and OpenAI's o4-mini) results show that enabling reflection-based\ncontext editing during discussion produces agents which dominate their baseline\ncounterparts on Elo scores, normalized Bradley-Terry-Davidson ability, and\nAlphaRank mass. The predicted signatures of learning are observed qualitatively\nin statement and reflection logs, where reflections identify weaknesses and\nreliably shape subsequent statements. Agreement between quantitative and\nqualitative evidence supports dialogue-driven context evolution as a practical\npath to targeted expertise amplification in open non-verifiable domains.\n", "link": "http://arxiv.org/abs/2510.15772v1", "date": "2025-10-17", "relevancy": 2.0378, "topK": [{"title": "RoboEXP: Action-Conditioned Scene Graph via Interactive Exploration for\n  Robotic Manipulation", "link": "http://arxiv.org/abs/2402.15487v1", "similarity": 0.5385}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5036}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5036}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Self-evolving%20expertise%20in%20complex%20non-verifiable%20subject%20domains%3A%0A%20%20dialogue%20as%20implicit%20meta-RL&body=Title%3A%20Self-evolving%20expertise%20in%20complex%20non-verifiable%20subject%20domains%3A%0A%20%20dialogue%20as%20implicit%20meta-RL%0AAuthor%3A%20Richard%20M.%20Bailey%0AAbstract%3A%20%20%20So-called%20%60wicked%20problems%27%2C%20those%20involving%20complex%20multi-dimensional%0Asettings%2C%20non-verifiable%20outcomes%2C%20heterogeneous%20impacts%20and%20a%20lack%20of%20single%0Aobjectively%20correct%20answers%2C%20have%20plagued%20humans%20throughout%20history.%20Modern%0Aexamples%20include%20decisions%20over%20justice%20frameworks%2C%20solving%20environmental%0Apollution%2C%20planning%20for%20pandemic%20resilience%20and%20food%20security.%20The%20use%20of%0Astate-of-the-art%20artificial%20intelligence%20systems%20%28notably%20Large%20Language%0AModel-based%20agents%29%20collaborating%20with%20humans%20on%20solving%20such%20problems%20is%20being%0Aactively%20explored.%20While%20the%20abilities%20of%20LLMs%20can%20be%20improved%20by%2C%20for%20example%2C%0Afine-tuning%2C%20hand-crafted%20system%20prompts%20and%20scaffolding%20with%20external%20tools%2C%0ALLMs%20lack%20endogenous%20mechanisms%20to%20develop%20expertise%20through%20experience%20in%20such%0Asettings.%20This%20work%20address%20this%20gap%20with%20Dialectica%2C%20a%20framework%20where%20agents%0Aengage%20in%20structured%20dialogue%20on%20defined%20topics%2C%20augmented%20by%20memory%2C%0Aself-reflection%2C%20and%20policy-constrained%20context%20editing.%20Formally%2C%20discussion%0Ais%20viewed%20as%20an%20implicit%20meta-reinforcement%20learning%20process.%20The%0A%60dialogue-trained%27%20agents%20are%20evaluated%20post-hoc%20using%20judged%20pairwise%0Acomparisons%20of%20elicited%20responses.%20Across%20two%20model%20architectures%20%28locally%20run%0AQwen3%3A30b%20and%20OpenAI%27s%20o4-mini%29%20results%20show%20that%20enabling%20reflection-based%0Acontext%20editing%20during%20discussion%20produces%20agents%20which%20dominate%20their%20baseline%0Acounterparts%20on%20Elo%20scores%2C%20normalized%20Bradley-Terry-Davidson%20ability%2C%20and%0AAlphaRank%20mass.%20The%20predicted%20signatures%20of%20learning%20are%20observed%20qualitatively%0Ain%20statement%20and%20reflection%20logs%2C%20where%20reflections%20identify%20weaknesses%20and%0Areliably%20shape%20subsequent%20statements.%20Agreement%20between%20quantitative%20and%0Aqualitative%20evidence%20supports%20dialogue-driven%20context%20evolution%20as%20a%20practical%0Apath%20to%20targeted%20expertise%20amplification%20in%20open%20non-verifiable%20domains.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2510.15772v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DSelf-evolving%2520expertise%2520in%2520complex%2520non-verifiable%2520subject%2520domains%253A%250A%2520%2520dialogue%2520as%2520implicit%2520meta-RL%26entry.906535625%3DRichard%2520M.%2520Bailey%26entry.1292438233%3D%2520%2520So-called%2520%2560wicked%2520problems%2527%252C%2520those%2520involving%2520complex%2520multi-dimensional%250Asettings%252C%2520non-verifiable%2520outcomes%252C%2520heterogeneous%2520impacts%2520and%2520a%2520lack%2520of%2520single%250Aobjectively%2520correct%2520answers%252C%2520have%2520plagued%2520humans%2520throughout%2520history.%2520Modern%250Aexamples%2520include%2520decisions%2520over%2520justice%2520frameworks%252C%2520solving%2520environmental%250Apollution%252C%2520planning%2520for%2520pandemic%2520resilience%2520and%2520food%2520security.%2520The%2520use%2520of%250Astate-of-the-art%2520artificial%2520intelligence%2520systems%2520%2528notably%2520Large%2520Language%250AModel-based%2520agents%2529%2520collaborating%2520with%2520humans%2520on%2520solving%2520such%2520problems%2520is%2520being%250Aactively%2520explored.%2520While%2520the%2520abilities%2520of%2520LLMs%2520can%2520be%2520improved%2520by%252C%2520for%2520example%252C%250Afine-tuning%252C%2520hand-crafted%2520system%2520prompts%2520and%2520scaffolding%2520with%2520external%2520tools%252C%250ALLMs%2520lack%2520endogenous%2520mechanisms%2520to%2520develop%2520expertise%2520through%2520experience%2520in%2520such%250Asettings.%2520This%2520work%2520address%2520this%2520gap%2520with%2520Dialectica%252C%2520a%2520framework%2520where%2520agents%250Aengage%2520in%2520structured%2520dialogue%2520on%2520defined%2520topics%252C%2520augmented%2520by%2520memory%252C%250Aself-reflection%252C%2520and%2520policy-constrained%2520context%2520editing.%2520Formally%252C%2520discussion%250Ais%2520viewed%2520as%2520an%2520implicit%2520meta-reinforcement%2520learning%2520process.%2520The%250A%2560dialogue-trained%2527%2520agents%2520are%2520evaluated%2520post-hoc%2520using%2520judged%2520pairwise%250Acomparisons%2520of%2520elicited%2520responses.%2520Across%2520two%2520model%2520architectures%2520%2528locally%2520run%250AQwen3%253A30b%2520and%2520OpenAI%2527s%2520o4-mini%2529%2520results%2520show%2520that%2520enabling%2520reflection-based%250Acontext%2520editing%2520during%2520discussion%2520produces%2520agents%2520which%2520dominate%2520their%2520baseline%250Acounterparts%2520on%2520Elo%2520scores%252C%2520normalized%2520Bradley-Terry-Davidson%2520ability%252C%2520and%250AAlphaRank%2520mass.%2520The%2520predicted%2520signatures%2520of%2520learning%2520are%2520observed%2520qualitatively%250Ain%2520statement%2520and%2520reflection%2520logs%252C%2520where%2520reflections%2520identify%2520weaknesses%2520and%250Areliably%2520shape%2520subsequent%2520statements.%2520Agreement%2520between%2520quantitative%2520and%250Aqualitative%2520evidence%2520supports%2520dialogue-driven%2520context%2520evolution%2520as%2520a%2520practical%250Apath%2520to%2520targeted%2520expertise%2520amplification%2520in%2520open%2520non-verifiable%2520domains.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2510.15772v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Self-evolving%20expertise%20in%20complex%20non-verifiable%20subject%20domains%3A%0A%20%20dialogue%20as%20implicit%20meta-RL&entry.906535625=Richard%20M.%20Bailey&entry.1292438233=%20%20So-called%20%60wicked%20problems%27%2C%20those%20involving%20complex%20multi-dimensional%0Asettings%2C%20non-verifiable%20outcomes%2C%20heterogeneous%20impacts%20and%20a%20lack%20of%20single%0Aobjectively%20correct%20answers%2C%20have%20plagued%20humans%20throughout%20history.%20Modern%0Aexamples%20include%20decisions%20over%20justice%20frameworks%2C%20solving%20environmental%0Apollution%2C%20planning%20for%20pandemic%20resilience%20and%20food%20security.%20The%20use%20of%0Astate-of-the-art%20artificial%20intelligence%20systems%20%28notably%20Large%20Language%0AModel-based%20agents%29%20collaborating%20with%20humans%20on%20solving%20such%20problems%20is%20being%0Aactively%20explored.%20While%20the%20abilities%20of%20LLMs%20can%20be%20improved%20by%2C%20for%20example%2C%0Afine-tuning%2C%20hand-crafted%20system%20prompts%20and%20scaffolding%20with%20external%20tools%2C%0ALLMs%20lack%20endogenous%20mechanisms%20to%20develop%20expertise%20through%20experience%20in%20such%0Asettings.%20This%20work%20address%20this%20gap%20with%20Dialectica%2C%20a%20framework%20where%20agents%0Aengage%20in%20structured%20dialogue%20on%20defined%20topics%2C%20augmented%20by%20memory%2C%0Aself-reflection%2C%20and%20policy-constrained%20context%20editing.%20Formally%2C%20discussion%0Ais%20viewed%20as%20an%20implicit%20meta-reinforcement%20learning%20process.%20The%0A%60dialogue-trained%27%20agents%20are%20evaluated%20post-hoc%20using%20judged%20pairwise%0Acomparisons%20of%20elicited%20responses.%20Across%20two%20model%20architectures%20%28locally%20run%0AQwen3%3A30b%20and%20OpenAI%27s%20o4-mini%29%20results%20show%20that%20enabling%20reflection-based%0Acontext%20editing%20during%20discussion%20produces%20agents%20which%20dominate%20their%20baseline%0Acounterparts%20on%20Elo%20scores%2C%20normalized%20Bradley-Terry-Davidson%20ability%2C%20and%0AAlphaRank%20mass.%20The%20predicted%20signatures%20of%20learning%20are%20observed%20qualitatively%0Ain%20statement%20and%20reflection%20logs%2C%20where%20reflections%20identify%20weaknesses%20and%0Areliably%20shape%20subsequent%20statements.%20Agreement%20between%20quantitative%20and%0Aqualitative%20evidence%20supports%20dialogue-driven%20context%20evolution%20as%20a%20practical%0Apath%20to%20targeted%20expertise%20amplification%20in%20open%20non-verifiable%20domains.%0A&entry.1838667208=http%3A//arxiv.org/abs/2510.15772v1&entry.124074799=Read"},
{"title": "Reasoning-Enhanced Large Language Models for Molecular Property\n  Prediction", "author": "Jiaxi Zhuang and Yaorui Shi and Jue Hou and Yunong He and Mingwei Ye and Mingjun Xu and Yuming Su and Linfeng Zhang and Ying Qian and Linfeng Zhang and Guolin Ke and Hengxing Cai", "abstract": "  Molecular property prediction is crucial for drug discovery and materials\nscience, yet existing approaches suffer from limited interpretability, poor\ncross-task generalization, and lack of chemical reasoning capabilities.\nTraditional machine learning models struggle with task transferability, while\nspecialized molecular language models provide little insight into their\ndecision-making processes. To address these limitations, we propose\n\\textbf{MPPReasoner}, a multimodal large language model that incorporates\nchemical reasoning for molecular property prediction. Our approach, built upon\nQwen2.5-VL-7B-Instruct, integrates molecular images with SMILES strings to\nenable comprehensive molecular understanding. We develop a two-stage training\nstrategy: supervised fine-tuning (SFT) using 16,000 high-quality reasoning\ntrajectories generated through expert knowledge and multiple teacher models,\nfollowed by Reinforcement Learning from Principle-Guided Rewards (RLPGR). RLPGR\nemploys verifiable, rule-based rewards that systematically evaluate chemical\nprinciple application, molecular structure analysis, and logical consistency\nthrough computational verification. Extensive experiments across 8 datasets\ndemonstrate significant performance improvements, with MPPReasoner\noutperforming the best baselines by 7.91\\% and 4.53\\% on in-distribution and\nout-of-distribution tasks respectively. MPPReasoner exhibits exceptional\ncross-task generalization and generates chemically sound reasoning paths that\nprovide valuable insights into molecular property analysis, substantially\nenhancing both interpretability and practical utility for chemists. Code is\navailable at https://anonymous.4open.science/r/MPPReasoner-12687.\n", "link": "http://arxiv.org/abs/2510.10248v2", "date": "2025-10-17", "relevancy": 2.0338, "topK": [{"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.5862}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.4929}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.4929}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Reasoning-Enhanced%20Large%20Language%20Models%20for%20Molecular%20Property%0A%20%20Prediction&body=Title%3A%20Reasoning-Enhanced%20Large%20Language%20Models%20for%20Molecular%20Property%0A%20%20Prediction%0AAuthor%3A%20Jiaxi%20Zhuang%20and%20Yaorui%20Shi%20and%20Jue%20Hou%20and%20Yunong%20He%20and%20Mingwei%20Ye%20and%20Mingjun%20Xu%20and%20Yuming%20Su%20and%20Linfeng%20Zhang%20and%20Ying%20Qian%20and%20Linfeng%20Zhang%20and%20Guolin%20Ke%20and%20Hengxing%20Cai%0AAbstract%3A%20%20%20Molecular%20property%20prediction%20is%20crucial%20for%20drug%20discovery%20and%20materials%0Ascience%2C%20yet%20existing%20approaches%20suffer%20from%20limited%20interpretability%2C%20poor%0Across-task%20generalization%2C%20and%20lack%20of%20chemical%20reasoning%20capabilities.%0ATraditional%20machine%20learning%20models%20struggle%20with%20task%20transferability%2C%20while%0Aspecialized%20molecular%20language%20models%20provide%20little%20insight%20into%20their%0Adecision-making%20processes.%20To%20address%20these%20limitations%2C%20we%20propose%0A%5Ctextbf%7BMPPReasoner%7D%2C%20a%20multimodal%20large%20language%20model%20that%20incorporates%0Achemical%20reasoning%20for%20molecular%20property%20prediction.%20Our%20approach%2C%20built%20upon%0AQwen2.5-VL-7B-Instruct%2C%20integrates%20molecular%20images%20with%20SMILES%20strings%20to%0Aenable%20comprehensive%20molecular%20understanding.%20We%20develop%20a%20two-stage%20training%0Astrategy%3A%20supervised%20fine-tuning%20%28SFT%29%20using%2016%2C000%20high-quality%20reasoning%0Atrajectories%20generated%20through%20expert%20knowledge%20and%20multiple%20teacher%20models%2C%0Afollowed%20by%20Reinforcement%20Learning%20from%20Principle-Guided%20Rewards%20%28RLPGR%29.%20RLPGR%0Aemploys%20verifiable%2C%20rule-based%20rewards%20that%20systematically%20evaluate%20chemical%0Aprinciple%20application%2C%20molecular%20structure%20analysis%2C%20and%20logical%20consistency%0Athrough%20computational%20verification.%20Extensive%20experiments%20across%208%20datasets%0Ademonstrate%20significant%20performance%20improvements%2C%20with%20MPPReasoner%0Aoutperforming%20the%20best%20baselines%20by%207.91%5C%25%20and%204.53%5C%25%20on%20in-distribution%20and%0Aout-of-distribution%20tasks%20respectively.%20MPPReasoner%20exhibits%20exceptional%0Across-task%20generalization%20and%20generates%20chemically%20sound%20reasoning%20paths%20that%0Aprovide%20valuable%20insights%20into%20molecular%20property%20analysis%2C%20substantially%0Aenhancing%20both%20interpretability%20and%20practical%20utility%20for%20chemists.%20Code%20is%0Aavailable%20at%20https%3A//anonymous.4open.science/r/MPPReasoner-12687.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2510.10248v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DReasoning-Enhanced%2520Large%2520Language%2520Models%2520for%2520Molecular%2520Property%250A%2520%2520Prediction%26entry.906535625%3DJiaxi%2520Zhuang%2520and%2520Yaorui%2520Shi%2520and%2520Jue%2520Hou%2520and%2520Yunong%2520He%2520and%2520Mingwei%2520Ye%2520and%2520Mingjun%2520Xu%2520and%2520Yuming%2520Su%2520and%2520Linfeng%2520Zhang%2520and%2520Ying%2520Qian%2520and%2520Linfeng%2520Zhang%2520and%2520Guolin%2520Ke%2520and%2520Hengxing%2520Cai%26entry.1292438233%3D%2520%2520Molecular%2520property%2520prediction%2520is%2520crucial%2520for%2520drug%2520discovery%2520and%2520materials%250Ascience%252C%2520yet%2520existing%2520approaches%2520suffer%2520from%2520limited%2520interpretability%252C%2520poor%250Across-task%2520generalization%252C%2520and%2520lack%2520of%2520chemical%2520reasoning%2520capabilities.%250ATraditional%2520machine%2520learning%2520models%2520struggle%2520with%2520task%2520transferability%252C%2520while%250Aspecialized%2520molecular%2520language%2520models%2520provide%2520little%2520insight%2520into%2520their%250Adecision-making%2520processes.%2520To%2520address%2520these%2520limitations%252C%2520we%2520propose%250A%255Ctextbf%257BMPPReasoner%257D%252C%2520a%2520multimodal%2520large%2520language%2520model%2520that%2520incorporates%250Achemical%2520reasoning%2520for%2520molecular%2520property%2520prediction.%2520Our%2520approach%252C%2520built%2520upon%250AQwen2.5-VL-7B-Instruct%252C%2520integrates%2520molecular%2520images%2520with%2520SMILES%2520strings%2520to%250Aenable%2520comprehensive%2520molecular%2520understanding.%2520We%2520develop%2520a%2520two-stage%2520training%250Astrategy%253A%2520supervised%2520fine-tuning%2520%2528SFT%2529%2520using%252016%252C000%2520high-quality%2520reasoning%250Atrajectories%2520generated%2520through%2520expert%2520knowledge%2520and%2520multiple%2520teacher%2520models%252C%250Afollowed%2520by%2520Reinforcement%2520Learning%2520from%2520Principle-Guided%2520Rewards%2520%2528RLPGR%2529.%2520RLPGR%250Aemploys%2520verifiable%252C%2520rule-based%2520rewards%2520that%2520systematically%2520evaluate%2520chemical%250Aprinciple%2520application%252C%2520molecular%2520structure%2520analysis%252C%2520and%2520logical%2520consistency%250Athrough%2520computational%2520verification.%2520Extensive%2520experiments%2520across%25208%2520datasets%250Ademonstrate%2520significant%2520performance%2520improvements%252C%2520with%2520MPPReasoner%250Aoutperforming%2520the%2520best%2520baselines%2520by%25207.91%255C%2525%2520and%25204.53%255C%2525%2520on%2520in-distribution%2520and%250Aout-of-distribution%2520tasks%2520respectively.%2520MPPReasoner%2520exhibits%2520exceptional%250Across-task%2520generalization%2520and%2520generates%2520chemically%2520sound%2520reasoning%2520paths%2520that%250Aprovide%2520valuable%2520insights%2520into%2520molecular%2520property%2520analysis%252C%2520substantially%250Aenhancing%2520both%2520interpretability%2520and%2520practical%2520utility%2520for%2520chemists.%2520Code%2520is%250Aavailable%2520at%2520https%253A//anonymous.4open.science/r/MPPReasoner-12687.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2510.10248v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Reasoning-Enhanced%20Large%20Language%20Models%20for%20Molecular%20Property%0A%20%20Prediction&entry.906535625=Jiaxi%20Zhuang%20and%20Yaorui%20Shi%20and%20Jue%20Hou%20and%20Yunong%20He%20and%20Mingwei%20Ye%20and%20Mingjun%20Xu%20and%20Yuming%20Su%20and%20Linfeng%20Zhang%20and%20Ying%20Qian%20and%20Linfeng%20Zhang%20and%20Guolin%20Ke%20and%20Hengxing%20Cai&entry.1292438233=%20%20Molecular%20property%20prediction%20is%20crucial%20for%20drug%20discovery%20and%20materials%0Ascience%2C%20yet%20existing%20approaches%20suffer%20from%20limited%20interpretability%2C%20poor%0Across-task%20generalization%2C%20and%20lack%20of%20chemical%20reasoning%20capabilities.%0ATraditional%20machine%20learning%20models%20struggle%20with%20task%20transferability%2C%20while%0Aspecialized%20molecular%20language%20models%20provide%20little%20insight%20into%20their%0Adecision-making%20processes.%20To%20address%20these%20limitations%2C%20we%20propose%0A%5Ctextbf%7BMPPReasoner%7D%2C%20a%20multimodal%20large%20language%20model%20that%20incorporates%0Achemical%20reasoning%20for%20molecular%20property%20prediction.%20Our%20approach%2C%20built%20upon%0AQwen2.5-VL-7B-Instruct%2C%20integrates%20molecular%20images%20with%20SMILES%20strings%20to%0Aenable%20comprehensive%20molecular%20understanding.%20We%20develop%20a%20two-stage%20training%0Astrategy%3A%20supervised%20fine-tuning%20%28SFT%29%20using%2016%2C000%20high-quality%20reasoning%0Atrajectories%20generated%20through%20expert%20knowledge%20and%20multiple%20teacher%20models%2C%0Afollowed%20by%20Reinforcement%20Learning%20from%20Principle-Guided%20Rewards%20%28RLPGR%29.%20RLPGR%0Aemploys%20verifiable%2C%20rule-based%20rewards%20that%20systematically%20evaluate%20chemical%0Aprinciple%20application%2C%20molecular%20structure%20analysis%2C%20and%20logical%20consistency%0Athrough%20computational%20verification.%20Extensive%20experiments%20across%208%20datasets%0Ademonstrate%20significant%20performance%20improvements%2C%20with%20MPPReasoner%0Aoutperforming%20the%20best%20baselines%20by%207.91%5C%25%20and%204.53%5C%25%20on%20in-distribution%20and%0Aout-of-distribution%20tasks%20respectively.%20MPPReasoner%20exhibits%20exceptional%0Across-task%20generalization%20and%20generates%20chemically%20sound%20reasoning%20paths%20that%0Aprovide%20valuable%20insights%20into%20molecular%20property%20analysis%2C%20substantially%0Aenhancing%20both%20interpretability%20and%20practical%20utility%20for%20chemists.%20Code%20is%0Aavailable%20at%20https%3A//anonymous.4open.science/r/MPPReasoner-12687.%0A&entry.1838667208=http%3A//arxiv.org/abs/2510.10248v2&entry.124074799=Read"},
{"title": "Where to Search: Measure the Prior-Structured Search Space of LLM Agents", "author": "Zhuo-Yang Song", "abstract": "  The generate-filter-refine (iterative) paradigm based on large language\nmodels (LLMs) has achieved progress in reasoning, programming, and program\ndiscovery in AI+Science. However, the effectiveness of search depends on where\nto search, namely, how to encode the domain prior into an operationally\nstructured hypothesis space. To this end, this paper proposes a compact formal\ntheory that describes and measures LLM-assisted iterative search guided by\ndomain priors. We represent an agent as a fuzzy relation operator on inputs and\noutputs to capture feasible transitions; the agent is thereby constrained by a\nfixed safety envelope. To describe multi-step reasoning/search, we weight all\nreachable paths by a single continuation parameter and sum them to obtain a\ncoverage generating function; this induces a measure of reachability\ndifficulty; and it provides a geometric interpretation of search on the graph\ninduced by the safety envelope. We further provide the simplest testable\ninferences and validate them via a majority-vote instantiation. This theory\noffers a workable language and operational tools to measure agents and their\nsearch spaces, proposing a systematic formal description of iterative search\nconstructed by LLMs.\n", "link": "http://arxiv.org/abs/2510.14846v2", "date": "2025-10-17", "relevancy": 2.0332, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5094}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5094}, {"title": "RoboEXP: Action-Conditioned Scene Graph via Interactive Exploration for\n  Robotic Manipulation", "link": "http://arxiv.org/abs/2402.15487v1", "similarity": 0.5028}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Where%20to%20Search%3A%20Measure%20the%20Prior-Structured%20Search%20Space%20of%20LLM%20Agents&body=Title%3A%20Where%20to%20Search%3A%20Measure%20the%20Prior-Structured%20Search%20Space%20of%20LLM%20Agents%0AAuthor%3A%20Zhuo-Yang%20Song%0AAbstract%3A%20%20%20The%20generate-filter-refine%20%28iterative%29%20paradigm%20based%20on%20large%20language%0Amodels%20%28LLMs%29%20has%20achieved%20progress%20in%20reasoning%2C%20programming%2C%20and%20program%0Adiscovery%20in%20AI%2BScience.%20However%2C%20the%20effectiveness%20of%20search%20depends%20on%20where%0Ato%20search%2C%20namely%2C%20how%20to%20encode%20the%20domain%20prior%20into%20an%20operationally%0Astructured%20hypothesis%20space.%20To%20this%20end%2C%20this%20paper%20proposes%20a%20compact%20formal%0Atheory%20that%20describes%20and%20measures%20LLM-assisted%20iterative%20search%20guided%20by%0Adomain%20priors.%20We%20represent%20an%20agent%20as%20a%20fuzzy%20relation%20operator%20on%20inputs%20and%0Aoutputs%20to%20capture%20feasible%20transitions%3B%20the%20agent%20is%20thereby%20constrained%20by%20a%0Afixed%20safety%20envelope.%20To%20describe%20multi-step%20reasoning/search%2C%20we%20weight%20all%0Areachable%20paths%20by%20a%20single%20continuation%20parameter%20and%20sum%20them%20to%20obtain%20a%0Acoverage%20generating%20function%3B%20this%20induces%20a%20measure%20of%20reachability%0Adifficulty%3B%20and%20it%20provides%20a%20geometric%20interpretation%20of%20search%20on%20the%20graph%0Ainduced%20by%20the%20safety%20envelope.%20We%20further%20provide%20the%20simplest%20testable%0Ainferences%20and%20validate%20them%20via%20a%20majority-vote%20instantiation.%20This%20theory%0Aoffers%20a%20workable%20language%20and%20operational%20tools%20to%20measure%20agents%20and%20their%0Asearch%20spaces%2C%20proposing%20a%20systematic%20formal%20description%20of%20iterative%20search%0Aconstructed%20by%20LLMs.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2510.14846v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DWhere%2520to%2520Search%253A%2520Measure%2520the%2520Prior-Structured%2520Search%2520Space%2520of%2520LLM%2520Agents%26entry.906535625%3DZhuo-Yang%2520Song%26entry.1292438233%3D%2520%2520The%2520generate-filter-refine%2520%2528iterative%2529%2520paradigm%2520based%2520on%2520large%2520language%250Amodels%2520%2528LLMs%2529%2520has%2520achieved%2520progress%2520in%2520reasoning%252C%2520programming%252C%2520and%2520program%250Adiscovery%2520in%2520AI%252BScience.%2520However%252C%2520the%2520effectiveness%2520of%2520search%2520depends%2520on%2520where%250Ato%2520search%252C%2520namely%252C%2520how%2520to%2520encode%2520the%2520domain%2520prior%2520into%2520an%2520operationally%250Astructured%2520hypothesis%2520space.%2520To%2520this%2520end%252C%2520this%2520paper%2520proposes%2520a%2520compact%2520formal%250Atheory%2520that%2520describes%2520and%2520measures%2520LLM-assisted%2520iterative%2520search%2520guided%2520by%250Adomain%2520priors.%2520We%2520represent%2520an%2520agent%2520as%2520a%2520fuzzy%2520relation%2520operator%2520on%2520inputs%2520and%250Aoutputs%2520to%2520capture%2520feasible%2520transitions%253B%2520the%2520agent%2520is%2520thereby%2520constrained%2520by%2520a%250Afixed%2520safety%2520envelope.%2520To%2520describe%2520multi-step%2520reasoning/search%252C%2520we%2520weight%2520all%250Areachable%2520paths%2520by%2520a%2520single%2520continuation%2520parameter%2520and%2520sum%2520them%2520to%2520obtain%2520a%250Acoverage%2520generating%2520function%253B%2520this%2520induces%2520a%2520measure%2520of%2520reachability%250Adifficulty%253B%2520and%2520it%2520provides%2520a%2520geometric%2520interpretation%2520of%2520search%2520on%2520the%2520graph%250Ainduced%2520by%2520the%2520safety%2520envelope.%2520We%2520further%2520provide%2520the%2520simplest%2520testable%250Ainferences%2520and%2520validate%2520them%2520via%2520a%2520majority-vote%2520instantiation.%2520This%2520theory%250Aoffers%2520a%2520workable%2520language%2520and%2520operational%2520tools%2520to%2520measure%2520agents%2520and%2520their%250Asearch%2520spaces%252C%2520proposing%2520a%2520systematic%2520formal%2520description%2520of%2520iterative%2520search%250Aconstructed%2520by%2520LLMs.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2510.14846v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Where%20to%20Search%3A%20Measure%20the%20Prior-Structured%20Search%20Space%20of%20LLM%20Agents&entry.906535625=Zhuo-Yang%20Song&entry.1292438233=%20%20The%20generate-filter-refine%20%28iterative%29%20paradigm%20based%20on%20large%20language%0Amodels%20%28LLMs%29%20has%20achieved%20progress%20in%20reasoning%2C%20programming%2C%20and%20program%0Adiscovery%20in%20AI%2BScience.%20However%2C%20the%20effectiveness%20of%20search%20depends%20on%20where%0Ato%20search%2C%20namely%2C%20how%20to%20encode%20the%20domain%20prior%20into%20an%20operationally%0Astructured%20hypothesis%20space.%20To%20this%20end%2C%20this%20paper%20proposes%20a%20compact%20formal%0Atheory%20that%20describes%20and%20measures%20LLM-assisted%20iterative%20search%20guided%20by%0Adomain%20priors.%20We%20represent%20an%20agent%20as%20a%20fuzzy%20relation%20operator%20on%20inputs%20and%0Aoutputs%20to%20capture%20feasible%20transitions%3B%20the%20agent%20is%20thereby%20constrained%20by%20a%0Afixed%20safety%20envelope.%20To%20describe%20multi-step%20reasoning/search%2C%20we%20weight%20all%0Areachable%20paths%20by%20a%20single%20continuation%20parameter%20and%20sum%20them%20to%20obtain%20a%0Acoverage%20generating%20function%3B%20this%20induces%20a%20measure%20of%20reachability%0Adifficulty%3B%20and%20it%20provides%20a%20geometric%20interpretation%20of%20search%20on%20the%20graph%0Ainduced%20by%20the%20safety%20envelope.%20We%20further%20provide%20the%20simplest%20testable%0Ainferences%20and%20validate%20them%20via%20a%20majority-vote%20instantiation.%20This%20theory%0Aoffers%20a%20workable%20language%20and%20operational%20tools%20to%20measure%20agents%20and%20their%0Asearch%20spaces%2C%20proposing%20a%20systematic%20formal%20description%20of%20iterative%20search%0Aconstructed%20by%20LLMs.%0A&entry.1838667208=http%3A//arxiv.org/abs/2510.14846v2&entry.124074799=Read"},
{"title": "Mixture of Experts Approaches in Dense Retrieval Tasks", "author": "Effrosyni Sokli and Pranav Kasela and Georgios Peikos and Gabriella Pasi", "abstract": "  Dense Retrieval Models (DRMs) are a prominent development in Information\nRetrieval (IR). A key challenge with these neural Transformer-based models is\nthat they often struggle to generalize beyond the specific tasks and domains\nthey were trained on. To address this challenge, prior research in IR\nincorporated the Mixture-of-Experts (MoE) framework within each Transformer\nlayer of a DRM, which, though effective, substantially increased the number of\nadditional parameters. In this paper, we propose a more efficient design, which\nintroduces a single MoE block (SB-MoE) after the final Transformer layer. To\nassess the retrieval effectiveness of SB-MoE, we perform an empirical\nevaluation across three IR tasks. Our experiments involve two evaluation\nsetups, aiming to assess both in-domain effectiveness and the model's zero-shot\ngeneralizability. In the first setup, we fine-tune SB-MoE with four different\nunderlying DRMs on seven IR benchmarks and evaluate them on their respective\ntest sets. In the second setup, we fine-tune SB-MoE on MSMARCO and perform\nzero-shot evaluation on thirteen BEIR datasets. Additionally, we perform\nfurther experiments to analyze the model's dependency on its hyperparameters\n(i.e., the number of employed and activated experts) and investigate how this\nvariation affects SB-MoE's performance. The obtained results show that SB-MoE\nis particularly effective for DRMs with lightweight base models, such as\nTinyBERT and BERT-Small, consistently exceeding standard model fine-tuning\nacross benchmarks. For DRMs with more parameters, such as BERT-Base and\nContriever, our model requires a larger number of training samples to achieve\nimproved retrieval performance. Our code is available online at:\nhttps://github.com/FaySokli/SB-MoE.\n", "link": "http://arxiv.org/abs/2510.15683v1", "date": "2025-10-17", "relevancy": 2.03, "topK": [{"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.519}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5052}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5052}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Mixture%20of%20Experts%20Approaches%20in%20Dense%20Retrieval%20Tasks&body=Title%3A%20Mixture%20of%20Experts%20Approaches%20in%20Dense%20Retrieval%20Tasks%0AAuthor%3A%20Effrosyni%20Sokli%20and%20Pranav%20Kasela%20and%20Georgios%20Peikos%20and%20Gabriella%20Pasi%0AAbstract%3A%20%20%20Dense%20Retrieval%20Models%20%28DRMs%29%20are%20a%20prominent%20development%20in%20Information%0ARetrieval%20%28IR%29.%20A%20key%20challenge%20with%20these%20neural%20Transformer-based%20models%20is%0Athat%20they%20often%20struggle%20to%20generalize%20beyond%20the%20specific%20tasks%20and%20domains%0Athey%20were%20trained%20on.%20To%20address%20this%20challenge%2C%20prior%20research%20in%20IR%0Aincorporated%20the%20Mixture-of-Experts%20%28MoE%29%20framework%20within%20each%20Transformer%0Alayer%20of%20a%20DRM%2C%20which%2C%20though%20effective%2C%20substantially%20increased%20the%20number%20of%0Aadditional%20parameters.%20In%20this%20paper%2C%20we%20propose%20a%20more%20efficient%20design%2C%20which%0Aintroduces%20a%20single%20MoE%20block%20%28SB-MoE%29%20after%20the%20final%20Transformer%20layer.%20To%0Aassess%20the%20retrieval%20effectiveness%20of%20SB-MoE%2C%20we%20perform%20an%20empirical%0Aevaluation%20across%20three%20IR%20tasks.%20Our%20experiments%20involve%20two%20evaluation%0Asetups%2C%20aiming%20to%20assess%20both%20in-domain%20effectiveness%20and%20the%20model%27s%20zero-shot%0Ageneralizability.%20In%20the%20first%20setup%2C%20we%20fine-tune%20SB-MoE%20with%20four%20different%0Aunderlying%20DRMs%20on%20seven%20IR%20benchmarks%20and%20evaluate%20them%20on%20their%20respective%0Atest%20sets.%20In%20the%20second%20setup%2C%20we%20fine-tune%20SB-MoE%20on%20MSMARCO%20and%20perform%0Azero-shot%20evaluation%20on%20thirteen%20BEIR%20datasets.%20Additionally%2C%20we%20perform%0Afurther%20experiments%20to%20analyze%20the%20model%27s%20dependency%20on%20its%20hyperparameters%0A%28i.e.%2C%20the%20number%20of%20employed%20and%20activated%20experts%29%20and%20investigate%20how%20this%0Avariation%20affects%20SB-MoE%27s%20performance.%20The%20obtained%20results%20show%20that%20SB-MoE%0Ais%20particularly%20effective%20for%20DRMs%20with%20lightweight%20base%20models%2C%20such%20as%0ATinyBERT%20and%20BERT-Small%2C%20consistently%20exceeding%20standard%20model%20fine-tuning%0Aacross%20benchmarks.%20For%20DRMs%20with%20more%20parameters%2C%20such%20as%20BERT-Base%20and%0AContriever%2C%20our%20model%20requires%20a%20larger%20number%20of%20training%20samples%20to%20achieve%0Aimproved%20retrieval%20performance.%20Our%20code%20is%20available%20online%20at%3A%0Ahttps%3A//github.com/FaySokli/SB-MoE.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2510.15683v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DMixture%2520of%2520Experts%2520Approaches%2520in%2520Dense%2520Retrieval%2520Tasks%26entry.906535625%3DEffrosyni%2520Sokli%2520and%2520Pranav%2520Kasela%2520and%2520Georgios%2520Peikos%2520and%2520Gabriella%2520Pasi%26entry.1292438233%3D%2520%2520Dense%2520Retrieval%2520Models%2520%2528DRMs%2529%2520are%2520a%2520prominent%2520development%2520in%2520Information%250ARetrieval%2520%2528IR%2529.%2520A%2520key%2520challenge%2520with%2520these%2520neural%2520Transformer-based%2520models%2520is%250Athat%2520they%2520often%2520struggle%2520to%2520generalize%2520beyond%2520the%2520specific%2520tasks%2520and%2520domains%250Athey%2520were%2520trained%2520on.%2520To%2520address%2520this%2520challenge%252C%2520prior%2520research%2520in%2520IR%250Aincorporated%2520the%2520Mixture-of-Experts%2520%2528MoE%2529%2520framework%2520within%2520each%2520Transformer%250Alayer%2520of%2520a%2520DRM%252C%2520which%252C%2520though%2520effective%252C%2520substantially%2520increased%2520the%2520number%2520of%250Aadditional%2520parameters.%2520In%2520this%2520paper%252C%2520we%2520propose%2520a%2520more%2520efficient%2520design%252C%2520which%250Aintroduces%2520a%2520single%2520MoE%2520block%2520%2528SB-MoE%2529%2520after%2520the%2520final%2520Transformer%2520layer.%2520To%250Aassess%2520the%2520retrieval%2520effectiveness%2520of%2520SB-MoE%252C%2520we%2520perform%2520an%2520empirical%250Aevaluation%2520across%2520three%2520IR%2520tasks.%2520Our%2520experiments%2520involve%2520two%2520evaluation%250Asetups%252C%2520aiming%2520to%2520assess%2520both%2520in-domain%2520effectiveness%2520and%2520the%2520model%2527s%2520zero-shot%250Ageneralizability.%2520In%2520the%2520first%2520setup%252C%2520we%2520fine-tune%2520SB-MoE%2520with%2520four%2520different%250Aunderlying%2520DRMs%2520on%2520seven%2520IR%2520benchmarks%2520and%2520evaluate%2520them%2520on%2520their%2520respective%250Atest%2520sets.%2520In%2520the%2520second%2520setup%252C%2520we%2520fine-tune%2520SB-MoE%2520on%2520MSMARCO%2520and%2520perform%250Azero-shot%2520evaluation%2520on%2520thirteen%2520BEIR%2520datasets.%2520Additionally%252C%2520we%2520perform%250Afurther%2520experiments%2520to%2520analyze%2520the%2520model%2527s%2520dependency%2520on%2520its%2520hyperparameters%250A%2528i.e.%252C%2520the%2520number%2520of%2520employed%2520and%2520activated%2520experts%2529%2520and%2520investigate%2520how%2520this%250Avariation%2520affects%2520SB-MoE%2527s%2520performance.%2520The%2520obtained%2520results%2520show%2520that%2520SB-MoE%250Ais%2520particularly%2520effective%2520for%2520DRMs%2520with%2520lightweight%2520base%2520models%252C%2520such%2520as%250ATinyBERT%2520and%2520BERT-Small%252C%2520consistently%2520exceeding%2520standard%2520model%2520fine-tuning%250Aacross%2520benchmarks.%2520For%2520DRMs%2520with%2520more%2520parameters%252C%2520such%2520as%2520BERT-Base%2520and%250AContriever%252C%2520our%2520model%2520requires%2520a%2520larger%2520number%2520of%2520training%2520samples%2520to%2520achieve%250Aimproved%2520retrieval%2520performance.%2520Our%2520code%2520is%2520available%2520online%2520at%253A%250Ahttps%253A//github.com/FaySokli/SB-MoE.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2510.15683v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Mixture%20of%20Experts%20Approaches%20in%20Dense%20Retrieval%20Tasks&entry.906535625=Effrosyni%20Sokli%20and%20Pranav%20Kasela%20and%20Georgios%20Peikos%20and%20Gabriella%20Pasi&entry.1292438233=%20%20Dense%20Retrieval%20Models%20%28DRMs%29%20are%20a%20prominent%20development%20in%20Information%0ARetrieval%20%28IR%29.%20A%20key%20challenge%20with%20these%20neural%20Transformer-based%20models%20is%0Athat%20they%20often%20struggle%20to%20generalize%20beyond%20the%20specific%20tasks%20and%20domains%0Athey%20were%20trained%20on.%20To%20address%20this%20challenge%2C%20prior%20research%20in%20IR%0Aincorporated%20the%20Mixture-of-Experts%20%28MoE%29%20framework%20within%20each%20Transformer%0Alayer%20of%20a%20DRM%2C%20which%2C%20though%20effective%2C%20substantially%20increased%20the%20number%20of%0Aadditional%20parameters.%20In%20this%20paper%2C%20we%20propose%20a%20more%20efficient%20design%2C%20which%0Aintroduces%20a%20single%20MoE%20block%20%28SB-MoE%29%20after%20the%20final%20Transformer%20layer.%20To%0Aassess%20the%20retrieval%20effectiveness%20of%20SB-MoE%2C%20we%20perform%20an%20empirical%0Aevaluation%20across%20three%20IR%20tasks.%20Our%20experiments%20involve%20two%20evaluation%0Asetups%2C%20aiming%20to%20assess%20both%20in-domain%20effectiveness%20and%20the%20model%27s%20zero-shot%0Ageneralizability.%20In%20the%20first%20setup%2C%20we%20fine-tune%20SB-MoE%20with%20four%20different%0Aunderlying%20DRMs%20on%20seven%20IR%20benchmarks%20and%20evaluate%20them%20on%20their%20respective%0Atest%20sets.%20In%20the%20second%20setup%2C%20we%20fine-tune%20SB-MoE%20on%20MSMARCO%20and%20perform%0Azero-shot%20evaluation%20on%20thirteen%20BEIR%20datasets.%20Additionally%2C%20we%20perform%0Afurther%20experiments%20to%20analyze%20the%20model%27s%20dependency%20on%20its%20hyperparameters%0A%28i.e.%2C%20the%20number%20of%20employed%20and%20activated%20experts%29%20and%20investigate%20how%20this%0Avariation%20affects%20SB-MoE%27s%20performance.%20The%20obtained%20results%20show%20that%20SB-MoE%0Ais%20particularly%20effective%20for%20DRMs%20with%20lightweight%20base%20models%2C%20such%20as%0ATinyBERT%20and%20BERT-Small%2C%20consistently%20exceeding%20standard%20model%20fine-tuning%0Aacross%20benchmarks.%20For%20DRMs%20with%20more%20parameters%2C%20such%20as%20BERT-Base%20and%0AContriever%2C%20our%20model%20requires%20a%20larger%20number%20of%20training%20samples%20to%20achieve%0Aimproved%20retrieval%20performance.%20Our%20code%20is%20available%20online%20at%3A%0Ahttps%3A//github.com/FaySokli/SB-MoE.%0A&entry.1838667208=http%3A//arxiv.org/abs/2510.15683v1&entry.124074799=Read"},
{"title": "SNOO: Step-K Nesterov Outer Optimizer - The Surprising Effectiveness of\n  Nesterov Momentum Applied to Pseudo-Gradients", "author": "Dominik Kallusky and Vinay Rao and Vishal Nandavanam and Hao-Jun Michael Shi", "abstract": "  The rapid development of large language models (LLMs) has driven the demand\nfor more efficient optimization techniques. Among these, the Lookahead family\nof optimizers employs a two-loop framework, maintaining fast and slow sets of\nmodel weights. Multiple inner optimizer steps on the fast weights produce a\ntrajectory - the pseudo-gradient - that is used to update the slow weights.\nDiLoCo, a notable example originally designed for distributed training, applies\nNesterov momentum to the averaged pseudo-gradient from multiple workers,\nclaiming to even outperform AdamW in a non-distributed setup. In this paper, we\nempirically show that DiLoCo's surprising effectiveness stems primarily from\napplying Nesterov momentum to the pseudo-gradient, which improves training in a\nnon-distributed setting. We call this Lookahead variant the Step-$K$ Nesterov\nOuter Optimizer (SNOO). We demonstrate that SNOO achieves compute factor gains\nof 1.5 - 2.5$\\times$ in a non-distributed setting up to a scale of 1e23\ntraining FLOPs, with improvements that increase with model size. Because of its\nminimal compute and memory overhead and compatibility with model sharding, SNOO\nis a practical enhancement for a variety of inner optimizers, including AdamW\nand Muon.\n", "link": "http://arxiv.org/abs/2510.15830v1", "date": "2025-10-17", "relevancy": 2.0087, "topK": [{"title": "Boosted dynamic neural networks", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/26302", "similarity": 0.51}, {"title": "ToDo: Token Downsampling for Efficient Generation of High-Resolution\n  Images", "link": "http://arxiv.org/abs/2402.13573v2", "similarity": 0.5009}, {"title": "SDXS: Real-Time One-Step Latent Diffusion Models with Image Conditions", "link": "http://arxiv.org/abs/2403.16627v1", "similarity": 0.4857}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20SNOO%3A%20Step-K%20Nesterov%20Outer%20Optimizer%20-%20The%20Surprising%20Effectiveness%20of%0A%20%20Nesterov%20Momentum%20Applied%20to%20Pseudo-Gradients&body=Title%3A%20SNOO%3A%20Step-K%20Nesterov%20Outer%20Optimizer%20-%20The%20Surprising%20Effectiveness%20of%0A%20%20Nesterov%20Momentum%20Applied%20to%20Pseudo-Gradients%0AAuthor%3A%20Dominik%20Kallusky%20and%20Vinay%20Rao%20and%20Vishal%20Nandavanam%20and%20Hao-Jun%20Michael%20Shi%0AAbstract%3A%20%20%20The%20rapid%20development%20of%20large%20language%20models%20%28LLMs%29%20has%20driven%20the%20demand%0Afor%20more%20efficient%20optimization%20techniques.%20Among%20these%2C%20the%20Lookahead%20family%0Aof%20optimizers%20employs%20a%20two-loop%20framework%2C%20maintaining%20fast%20and%20slow%20sets%20of%0Amodel%20weights.%20Multiple%20inner%20optimizer%20steps%20on%20the%20fast%20weights%20produce%20a%0Atrajectory%20-%20the%20pseudo-gradient%20-%20that%20is%20used%20to%20update%20the%20slow%20weights.%0ADiLoCo%2C%20a%20notable%20example%20originally%20designed%20for%20distributed%20training%2C%20applies%0ANesterov%20momentum%20to%20the%20averaged%20pseudo-gradient%20from%20multiple%20workers%2C%0Aclaiming%20to%20even%20outperform%20AdamW%20in%20a%20non-distributed%20setup.%20In%20this%20paper%2C%20we%0Aempirically%20show%20that%20DiLoCo%27s%20surprising%20effectiveness%20stems%20primarily%20from%0Aapplying%20Nesterov%20momentum%20to%20the%20pseudo-gradient%2C%20which%20improves%20training%20in%20a%0Anon-distributed%20setting.%20We%20call%20this%20Lookahead%20variant%20the%20Step-%24K%24%20Nesterov%0AOuter%20Optimizer%20%28SNOO%29.%20We%20demonstrate%20that%20SNOO%20achieves%20compute%20factor%20gains%0Aof%201.5%20-%202.5%24%5Ctimes%24%20in%20a%20non-distributed%20setting%20up%20to%20a%20scale%20of%201e23%0Atraining%20FLOPs%2C%20with%20improvements%20that%20increase%20with%20model%20size.%20Because%20of%20its%0Aminimal%20compute%20and%20memory%20overhead%20and%20compatibility%20with%20model%20sharding%2C%20SNOO%0Ais%20a%20practical%20enhancement%20for%20a%20variety%20of%20inner%20optimizers%2C%20including%20AdamW%0Aand%20Muon.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2510.15830v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DSNOO%253A%2520Step-K%2520Nesterov%2520Outer%2520Optimizer%2520-%2520The%2520Surprising%2520Effectiveness%2520of%250A%2520%2520Nesterov%2520Momentum%2520Applied%2520to%2520Pseudo-Gradients%26entry.906535625%3DDominik%2520Kallusky%2520and%2520Vinay%2520Rao%2520and%2520Vishal%2520Nandavanam%2520and%2520Hao-Jun%2520Michael%2520Shi%26entry.1292438233%3D%2520%2520The%2520rapid%2520development%2520of%2520large%2520language%2520models%2520%2528LLMs%2529%2520has%2520driven%2520the%2520demand%250Afor%2520more%2520efficient%2520optimization%2520techniques.%2520Among%2520these%252C%2520the%2520Lookahead%2520family%250Aof%2520optimizers%2520employs%2520a%2520two-loop%2520framework%252C%2520maintaining%2520fast%2520and%2520slow%2520sets%2520of%250Amodel%2520weights.%2520Multiple%2520inner%2520optimizer%2520steps%2520on%2520the%2520fast%2520weights%2520produce%2520a%250Atrajectory%2520-%2520the%2520pseudo-gradient%2520-%2520that%2520is%2520used%2520to%2520update%2520the%2520slow%2520weights.%250ADiLoCo%252C%2520a%2520notable%2520example%2520originally%2520designed%2520for%2520distributed%2520training%252C%2520applies%250ANesterov%2520momentum%2520to%2520the%2520averaged%2520pseudo-gradient%2520from%2520multiple%2520workers%252C%250Aclaiming%2520to%2520even%2520outperform%2520AdamW%2520in%2520a%2520non-distributed%2520setup.%2520In%2520this%2520paper%252C%2520we%250Aempirically%2520show%2520that%2520DiLoCo%2527s%2520surprising%2520effectiveness%2520stems%2520primarily%2520from%250Aapplying%2520Nesterov%2520momentum%2520to%2520the%2520pseudo-gradient%252C%2520which%2520improves%2520training%2520in%2520a%250Anon-distributed%2520setting.%2520We%2520call%2520this%2520Lookahead%2520variant%2520the%2520Step-%2524K%2524%2520Nesterov%250AOuter%2520Optimizer%2520%2528SNOO%2529.%2520We%2520demonstrate%2520that%2520SNOO%2520achieves%2520compute%2520factor%2520gains%250Aof%25201.5%2520-%25202.5%2524%255Ctimes%2524%2520in%2520a%2520non-distributed%2520setting%2520up%2520to%2520a%2520scale%2520of%25201e23%250Atraining%2520FLOPs%252C%2520with%2520improvements%2520that%2520increase%2520with%2520model%2520size.%2520Because%2520of%2520its%250Aminimal%2520compute%2520and%2520memory%2520overhead%2520and%2520compatibility%2520with%2520model%2520sharding%252C%2520SNOO%250Ais%2520a%2520practical%2520enhancement%2520for%2520a%2520variety%2520of%2520inner%2520optimizers%252C%2520including%2520AdamW%250Aand%2520Muon.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2510.15830v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=SNOO%3A%20Step-K%20Nesterov%20Outer%20Optimizer%20-%20The%20Surprising%20Effectiveness%20of%0A%20%20Nesterov%20Momentum%20Applied%20to%20Pseudo-Gradients&entry.906535625=Dominik%20Kallusky%20and%20Vinay%20Rao%20and%20Vishal%20Nandavanam%20and%20Hao-Jun%20Michael%20Shi&entry.1292438233=%20%20The%20rapid%20development%20of%20large%20language%20models%20%28LLMs%29%20has%20driven%20the%20demand%0Afor%20more%20efficient%20optimization%20techniques.%20Among%20these%2C%20the%20Lookahead%20family%0Aof%20optimizers%20employs%20a%20two-loop%20framework%2C%20maintaining%20fast%20and%20slow%20sets%20of%0Amodel%20weights.%20Multiple%20inner%20optimizer%20steps%20on%20the%20fast%20weights%20produce%20a%0Atrajectory%20-%20the%20pseudo-gradient%20-%20that%20is%20used%20to%20update%20the%20slow%20weights.%0ADiLoCo%2C%20a%20notable%20example%20originally%20designed%20for%20distributed%20training%2C%20applies%0ANesterov%20momentum%20to%20the%20averaged%20pseudo-gradient%20from%20multiple%20workers%2C%0Aclaiming%20to%20even%20outperform%20AdamW%20in%20a%20non-distributed%20setup.%20In%20this%20paper%2C%20we%0Aempirically%20show%20that%20DiLoCo%27s%20surprising%20effectiveness%20stems%20primarily%20from%0Aapplying%20Nesterov%20momentum%20to%20the%20pseudo-gradient%2C%20which%20improves%20training%20in%20a%0Anon-distributed%20setting.%20We%20call%20this%20Lookahead%20variant%20the%20Step-%24K%24%20Nesterov%0AOuter%20Optimizer%20%28SNOO%29.%20We%20demonstrate%20that%20SNOO%20achieves%20compute%20factor%20gains%0Aof%201.5%20-%202.5%24%5Ctimes%24%20in%20a%20non-distributed%20setting%20up%20to%20a%20scale%20of%201e23%0Atraining%20FLOPs%2C%20with%20improvements%20that%20increase%20with%20model%20size.%20Because%20of%20its%0Aminimal%20compute%20and%20memory%20overhead%20and%20compatibility%20with%20model%20sharding%2C%20SNOO%0Ais%20a%20practical%20enhancement%20for%20a%20variety%20of%20inner%20optimizers%2C%20including%20AdamW%0Aand%20Muon.%0A&entry.1838667208=http%3A//arxiv.org/abs/2510.15830v1&entry.124074799=Read"},
{"title": "GOGH: Correlation-Guided Orchestration of GPUs in Heterogeneous Clusters", "author": "Ahmad Raeisi and Mahdi Dolati and Sina Darabi and Sadegh Talebi and Patrick Eugster and Ahmad Khonsari", "abstract": "  The growing demand for computational resources in machine learning has made\nefficient resource allocation a critical challenge, especially in heterogeneous\nhardware clusters where devices vary in capability, age, and energy efficiency.\nUpgrading to the latest hardware is often infeasible, making sustainable use of\nexisting, mixed-generation resources essential. In this paper, we propose a\nlearning-based architecture for managing machine learning workloads in\nheterogeneous clusters. The system operates online, allocating resources to\nincoming training or inference requests while minimizing energy consumption and\nmeeting performance requirements. It uses two neural networks: the first\nprovides initial estimates of how well a new model will utilize different\nhardware types and how it will affect co-located models. An optimizer then\nallocates resources based on these estimates. After deployment, the system\nmonitors real performance and uses this data to refine its predictions via a\nsecond neural network. This updated model improves estimates not only for the\ncurrent hardware but also for hardware not initially allocated and for\nco-location scenarios not yet observed. The result is an adaptive, iterative\napproach that learns over time to make more effective resource allocation\ndecisions in heterogeneous deep learning clusters.\n", "link": "http://arxiv.org/abs/2510.15652v1", "date": "2025-10-17", "relevancy": 2.0068, "topK": [{"title": "Dynamic Gaussians Mesh: Consistent Mesh Reconstruction from Monocular\n  Videos", "link": "http://arxiv.org/abs/2404.12379v2", "similarity": 0.5067}, {"title": "Boosted dynamic neural networks", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/26302", "similarity": 0.5023}, {"title": "Thinking Outside the BBox: Unconstrained Generative Object Compositing", "link": "http://arxiv.org/abs/2409.04559v2", "similarity": 0.4878}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20GOGH%3A%20Correlation-Guided%20Orchestration%20of%20GPUs%20in%20Heterogeneous%20Clusters&body=Title%3A%20GOGH%3A%20Correlation-Guided%20Orchestration%20of%20GPUs%20in%20Heterogeneous%20Clusters%0AAuthor%3A%20Ahmad%20Raeisi%20and%20Mahdi%20Dolati%20and%20Sina%20Darabi%20and%20Sadegh%20Talebi%20and%20Patrick%20Eugster%20and%20Ahmad%20Khonsari%0AAbstract%3A%20%20%20The%20growing%20demand%20for%20computational%20resources%20in%20machine%20learning%20has%20made%0Aefficient%20resource%20allocation%20a%20critical%20challenge%2C%20especially%20in%20heterogeneous%0Ahardware%20clusters%20where%20devices%20vary%20in%20capability%2C%20age%2C%20and%20energy%20efficiency.%0AUpgrading%20to%20the%20latest%20hardware%20is%20often%20infeasible%2C%20making%20sustainable%20use%20of%0Aexisting%2C%20mixed-generation%20resources%20essential.%20In%20this%20paper%2C%20we%20propose%20a%0Alearning-based%20architecture%20for%20managing%20machine%20learning%20workloads%20in%0Aheterogeneous%20clusters.%20The%20system%20operates%20online%2C%20allocating%20resources%20to%0Aincoming%20training%20or%20inference%20requests%20while%20minimizing%20energy%20consumption%20and%0Ameeting%20performance%20requirements.%20It%20uses%20two%20neural%20networks%3A%20the%20first%0Aprovides%20initial%20estimates%20of%20how%20well%20a%20new%20model%20will%20utilize%20different%0Ahardware%20types%20and%20how%20it%20will%20affect%20co-located%20models.%20An%20optimizer%20then%0Aallocates%20resources%20based%20on%20these%20estimates.%20After%20deployment%2C%20the%20system%0Amonitors%20real%20performance%20and%20uses%20this%20data%20to%20refine%20its%20predictions%20via%20a%0Asecond%20neural%20network.%20This%20updated%20model%20improves%20estimates%20not%20only%20for%20the%0Acurrent%20hardware%20but%20also%20for%20hardware%20not%20initially%20allocated%20and%20for%0Aco-location%20scenarios%20not%20yet%20observed.%20The%20result%20is%20an%20adaptive%2C%20iterative%0Aapproach%20that%20learns%20over%20time%20to%20make%20more%20effective%20resource%20allocation%0Adecisions%20in%20heterogeneous%20deep%20learning%20clusters.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2510.15652v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DGOGH%253A%2520Correlation-Guided%2520Orchestration%2520of%2520GPUs%2520in%2520Heterogeneous%2520Clusters%26entry.906535625%3DAhmad%2520Raeisi%2520and%2520Mahdi%2520Dolati%2520and%2520Sina%2520Darabi%2520and%2520Sadegh%2520Talebi%2520and%2520Patrick%2520Eugster%2520and%2520Ahmad%2520Khonsari%26entry.1292438233%3D%2520%2520The%2520growing%2520demand%2520for%2520computational%2520resources%2520in%2520machine%2520learning%2520has%2520made%250Aefficient%2520resource%2520allocation%2520a%2520critical%2520challenge%252C%2520especially%2520in%2520heterogeneous%250Ahardware%2520clusters%2520where%2520devices%2520vary%2520in%2520capability%252C%2520age%252C%2520and%2520energy%2520efficiency.%250AUpgrading%2520to%2520the%2520latest%2520hardware%2520is%2520often%2520infeasible%252C%2520making%2520sustainable%2520use%2520of%250Aexisting%252C%2520mixed-generation%2520resources%2520essential.%2520In%2520this%2520paper%252C%2520we%2520propose%2520a%250Alearning-based%2520architecture%2520for%2520managing%2520machine%2520learning%2520workloads%2520in%250Aheterogeneous%2520clusters.%2520The%2520system%2520operates%2520online%252C%2520allocating%2520resources%2520to%250Aincoming%2520training%2520or%2520inference%2520requests%2520while%2520minimizing%2520energy%2520consumption%2520and%250Ameeting%2520performance%2520requirements.%2520It%2520uses%2520two%2520neural%2520networks%253A%2520the%2520first%250Aprovides%2520initial%2520estimates%2520of%2520how%2520well%2520a%2520new%2520model%2520will%2520utilize%2520different%250Ahardware%2520types%2520and%2520how%2520it%2520will%2520affect%2520co-located%2520models.%2520An%2520optimizer%2520then%250Aallocates%2520resources%2520based%2520on%2520these%2520estimates.%2520After%2520deployment%252C%2520the%2520system%250Amonitors%2520real%2520performance%2520and%2520uses%2520this%2520data%2520to%2520refine%2520its%2520predictions%2520via%2520a%250Asecond%2520neural%2520network.%2520This%2520updated%2520model%2520improves%2520estimates%2520not%2520only%2520for%2520the%250Acurrent%2520hardware%2520but%2520also%2520for%2520hardware%2520not%2520initially%2520allocated%2520and%2520for%250Aco-location%2520scenarios%2520not%2520yet%2520observed.%2520The%2520result%2520is%2520an%2520adaptive%252C%2520iterative%250Aapproach%2520that%2520learns%2520over%2520time%2520to%2520make%2520more%2520effective%2520resource%2520allocation%250Adecisions%2520in%2520heterogeneous%2520deep%2520learning%2520clusters.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2510.15652v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=GOGH%3A%20Correlation-Guided%20Orchestration%20of%20GPUs%20in%20Heterogeneous%20Clusters&entry.906535625=Ahmad%20Raeisi%20and%20Mahdi%20Dolati%20and%20Sina%20Darabi%20and%20Sadegh%20Talebi%20and%20Patrick%20Eugster%20and%20Ahmad%20Khonsari&entry.1292438233=%20%20The%20growing%20demand%20for%20computational%20resources%20in%20machine%20learning%20has%20made%0Aefficient%20resource%20allocation%20a%20critical%20challenge%2C%20especially%20in%20heterogeneous%0Ahardware%20clusters%20where%20devices%20vary%20in%20capability%2C%20age%2C%20and%20energy%20efficiency.%0AUpgrading%20to%20the%20latest%20hardware%20is%20often%20infeasible%2C%20making%20sustainable%20use%20of%0Aexisting%2C%20mixed-generation%20resources%20essential.%20In%20this%20paper%2C%20we%20propose%20a%0Alearning-based%20architecture%20for%20managing%20machine%20learning%20workloads%20in%0Aheterogeneous%20clusters.%20The%20system%20operates%20online%2C%20allocating%20resources%20to%0Aincoming%20training%20or%20inference%20requests%20while%20minimizing%20energy%20consumption%20and%0Ameeting%20performance%20requirements.%20It%20uses%20two%20neural%20networks%3A%20the%20first%0Aprovides%20initial%20estimates%20of%20how%20well%20a%20new%20model%20will%20utilize%20different%0Ahardware%20types%20and%20how%20it%20will%20affect%20co-located%20models.%20An%20optimizer%20then%0Aallocates%20resources%20based%20on%20these%20estimates.%20After%20deployment%2C%20the%20system%0Amonitors%20real%20performance%20and%20uses%20this%20data%20to%20refine%20its%20predictions%20via%20a%0Asecond%20neural%20network.%20This%20updated%20model%20improves%20estimates%20not%20only%20for%20the%0Acurrent%20hardware%20but%20also%20for%20hardware%20not%20initially%20allocated%20and%20for%0Aco-location%20scenarios%20not%20yet%20observed.%20The%20result%20is%20an%20adaptive%2C%20iterative%0Aapproach%20that%20learns%20over%20time%20to%20make%20more%20effective%20resource%20allocation%0Adecisions%20in%20heterogeneous%20deep%20learning%20clusters.%0A&entry.1838667208=http%3A//arxiv.org/abs/2510.15652v1&entry.124074799=Read"},
{"title": "Migration as a Probe: A Generalizable Benchmark Framework for Specialist\n  vs. Generalist Machine-Learned Force Fields", "author": "Yi Cao and Paulette Clancy", "abstract": "  Machine-learned force fields (MLFFs), especially pre-trained foundation\nmodels, are transforming computational materials science by enabling ab\ninitio-level accuracy at molecular dynamics scales. Yet their rapid rise raises\na key question: should researchers train specialist models from scratch,\nfine-tune generalist foundation models, or use hybrid approaches? The\ntrade-offs in data efficiency, accuracy, cost, and robustness to\nout-of-distribution failure remain unclear. We introduce a benchmarking\nframework using defect migration pathways, evaluated through nudged elastic\nband trajectories, as diagnostic probes that test both interpolation and\nextrapolation. Using Cr-doped Sb2Te3 as a representative two-dimensional\nmaterial, we benchmark multiple training paradigms within the MACE architecture\nacross equilibrium, kinetic (atomic migration), and mechanical (interlayer\nsliding) tasks. Fine-tuned models substantially outperform from-scratch and\nzero-shot approaches for kinetic properties but show partial loss of long-range\nphysics. Representational analysis reveals distinct, non-overlapping latent\nencodings, indicating that different training strategies learn different\naspects of system physics. This framework provides practical guidelines for\nMLFF development and establishes migration-based probes as efficient\ndiagnostics linking performance to learned representations, guiding future\nuncertainty-aware active learning.\n", "link": "http://arxiv.org/abs/2509.00090v2", "date": "2025-10-17", "relevancy": 1.9943, "topK": [{"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.5037}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.4976}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.4976}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Migration%20as%20a%20Probe%3A%20A%20Generalizable%20Benchmark%20Framework%20for%20Specialist%0A%20%20vs.%20Generalist%20Machine-Learned%20Force%20Fields&body=Title%3A%20Migration%20as%20a%20Probe%3A%20A%20Generalizable%20Benchmark%20Framework%20for%20Specialist%0A%20%20vs.%20Generalist%20Machine-Learned%20Force%20Fields%0AAuthor%3A%20Yi%20Cao%20and%20Paulette%20Clancy%0AAbstract%3A%20%20%20Machine-learned%20force%20fields%20%28MLFFs%29%2C%20especially%20pre-trained%20foundation%0Amodels%2C%20are%20transforming%20computational%20materials%20science%20by%20enabling%20ab%0Ainitio-level%20accuracy%20at%20molecular%20dynamics%20scales.%20Yet%20their%20rapid%20rise%20raises%0Aa%20key%20question%3A%20should%20researchers%20train%20specialist%20models%20from%20scratch%2C%0Afine-tune%20generalist%20foundation%20models%2C%20or%20use%20hybrid%20approaches%3F%20The%0Atrade-offs%20in%20data%20efficiency%2C%20accuracy%2C%20cost%2C%20and%20robustness%20to%0Aout-of-distribution%20failure%20remain%20unclear.%20We%20introduce%20a%20benchmarking%0Aframework%20using%20defect%20migration%20pathways%2C%20evaluated%20through%20nudged%20elastic%0Aband%20trajectories%2C%20as%20diagnostic%20probes%20that%20test%20both%20interpolation%20and%0Aextrapolation.%20Using%20Cr-doped%20Sb2Te3%20as%20a%20representative%20two-dimensional%0Amaterial%2C%20we%20benchmark%20multiple%20training%20paradigms%20within%20the%20MACE%20architecture%0Aacross%20equilibrium%2C%20kinetic%20%28atomic%20migration%29%2C%20and%20mechanical%20%28interlayer%0Asliding%29%20tasks.%20Fine-tuned%20models%20substantially%20outperform%20from-scratch%20and%0Azero-shot%20approaches%20for%20kinetic%20properties%20but%20show%20partial%20loss%20of%20long-range%0Aphysics.%20Representational%20analysis%20reveals%20distinct%2C%20non-overlapping%20latent%0Aencodings%2C%20indicating%20that%20different%20training%20strategies%20learn%20different%0Aaspects%20of%20system%20physics.%20This%20framework%20provides%20practical%20guidelines%20for%0AMLFF%20development%20and%20establishes%20migration-based%20probes%20as%20efficient%0Adiagnostics%20linking%20performance%20to%20learned%20representations%2C%20guiding%20future%0Auncertainty-aware%20active%20learning.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2509.00090v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DMigration%2520as%2520a%2520Probe%253A%2520A%2520Generalizable%2520Benchmark%2520Framework%2520for%2520Specialist%250A%2520%2520vs.%2520Generalist%2520Machine-Learned%2520Force%2520Fields%26entry.906535625%3DYi%2520Cao%2520and%2520Paulette%2520Clancy%26entry.1292438233%3D%2520%2520Machine-learned%2520force%2520fields%2520%2528MLFFs%2529%252C%2520especially%2520pre-trained%2520foundation%250Amodels%252C%2520are%2520transforming%2520computational%2520materials%2520science%2520by%2520enabling%2520ab%250Ainitio-level%2520accuracy%2520at%2520molecular%2520dynamics%2520scales.%2520Yet%2520their%2520rapid%2520rise%2520raises%250Aa%2520key%2520question%253A%2520should%2520researchers%2520train%2520specialist%2520models%2520from%2520scratch%252C%250Afine-tune%2520generalist%2520foundation%2520models%252C%2520or%2520use%2520hybrid%2520approaches%253F%2520The%250Atrade-offs%2520in%2520data%2520efficiency%252C%2520accuracy%252C%2520cost%252C%2520and%2520robustness%2520to%250Aout-of-distribution%2520failure%2520remain%2520unclear.%2520We%2520introduce%2520a%2520benchmarking%250Aframework%2520using%2520defect%2520migration%2520pathways%252C%2520evaluated%2520through%2520nudged%2520elastic%250Aband%2520trajectories%252C%2520as%2520diagnostic%2520probes%2520that%2520test%2520both%2520interpolation%2520and%250Aextrapolation.%2520Using%2520Cr-doped%2520Sb2Te3%2520as%2520a%2520representative%2520two-dimensional%250Amaterial%252C%2520we%2520benchmark%2520multiple%2520training%2520paradigms%2520within%2520the%2520MACE%2520architecture%250Aacross%2520equilibrium%252C%2520kinetic%2520%2528atomic%2520migration%2529%252C%2520and%2520mechanical%2520%2528interlayer%250Asliding%2529%2520tasks.%2520Fine-tuned%2520models%2520substantially%2520outperform%2520from-scratch%2520and%250Azero-shot%2520approaches%2520for%2520kinetic%2520properties%2520but%2520show%2520partial%2520loss%2520of%2520long-range%250Aphysics.%2520Representational%2520analysis%2520reveals%2520distinct%252C%2520non-overlapping%2520latent%250Aencodings%252C%2520indicating%2520that%2520different%2520training%2520strategies%2520learn%2520different%250Aaspects%2520of%2520system%2520physics.%2520This%2520framework%2520provides%2520practical%2520guidelines%2520for%250AMLFF%2520development%2520and%2520establishes%2520migration-based%2520probes%2520as%2520efficient%250Adiagnostics%2520linking%2520performance%2520to%2520learned%2520representations%252C%2520guiding%2520future%250Auncertainty-aware%2520active%2520learning.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2509.00090v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Migration%20as%20a%20Probe%3A%20A%20Generalizable%20Benchmark%20Framework%20for%20Specialist%0A%20%20vs.%20Generalist%20Machine-Learned%20Force%20Fields&entry.906535625=Yi%20Cao%20and%20Paulette%20Clancy&entry.1292438233=%20%20Machine-learned%20force%20fields%20%28MLFFs%29%2C%20especially%20pre-trained%20foundation%0Amodels%2C%20are%20transforming%20computational%20materials%20science%20by%20enabling%20ab%0Ainitio-level%20accuracy%20at%20molecular%20dynamics%20scales.%20Yet%20their%20rapid%20rise%20raises%0Aa%20key%20question%3A%20should%20researchers%20train%20specialist%20models%20from%20scratch%2C%0Afine-tune%20generalist%20foundation%20models%2C%20or%20use%20hybrid%20approaches%3F%20The%0Atrade-offs%20in%20data%20efficiency%2C%20accuracy%2C%20cost%2C%20and%20robustness%20to%0Aout-of-distribution%20failure%20remain%20unclear.%20We%20introduce%20a%20benchmarking%0Aframework%20using%20defect%20migration%20pathways%2C%20evaluated%20through%20nudged%20elastic%0Aband%20trajectories%2C%20as%20diagnostic%20probes%20that%20test%20both%20interpolation%20and%0Aextrapolation.%20Using%20Cr-doped%20Sb2Te3%20as%20a%20representative%20two-dimensional%0Amaterial%2C%20we%20benchmark%20multiple%20training%20paradigms%20within%20the%20MACE%20architecture%0Aacross%20equilibrium%2C%20kinetic%20%28atomic%20migration%29%2C%20and%20mechanical%20%28interlayer%0Asliding%29%20tasks.%20Fine-tuned%20models%20substantially%20outperform%20from-scratch%20and%0Azero-shot%20approaches%20for%20kinetic%20properties%20but%20show%20partial%20loss%20of%20long-range%0Aphysics.%20Representational%20analysis%20reveals%20distinct%2C%20non-overlapping%20latent%0Aencodings%2C%20indicating%20that%20different%20training%20strategies%20learn%20different%0Aaspects%20of%20system%20physics.%20This%20framework%20provides%20practical%20guidelines%20for%0AMLFF%20development%20and%20establishes%20migration-based%20probes%20as%20efficient%0Adiagnostics%20linking%20performance%20to%20learned%20representations%2C%20guiding%20future%0Auncertainty-aware%20active%20learning.%0A&entry.1838667208=http%3A//arxiv.org/abs/2509.00090v2&entry.124074799=Read"},
{"title": "ProofBridge: Auto-Formalization of Natural Language Proofs in Lean via\n  Joint Embeddings", "author": "Prithwish Jana and Kaan Kale and Ahmet Ege Tanriverdi and Cruise Song and Sriram Vishwanath and Vijay Ganesh", "abstract": "  Translating human-written mathematical theorems and proofs from natural\nlanguage (NL) into formal languages (FLs) like Lean 4 has long been a\nsignificant challenge for AI. Most state-of-the-art methods address this\nseparately, first translating theorems and then generating proofs, creating a\nfundamental disconnect vis-a-vis true proof auto-formalization. This two-step\nprocess and its limitations were evident even in AlphaProof's silver-medal\nperformance at the 2024 IMO, where problem statements needed manual translation\nbefore automated proof synthesis.\n  We present ProofBridge, a unified framework for automatically translating\nentire NL theorems and proofs into Lean 4. At its core is a joint embedding\nmodel that aligns NL and FL (NL-FL) theorem-proof pairs in a shared semantic\nspace, enabling cross-modal retrieval of semantically relevant FL examples to\nguide translation. Our training ensures that NL-FL theorems (and their proofs)\nare mapped close together in this space if and only if the NL-FL pairs are\nsemantically equivalent. ProofBridge integrates retrieval-augmented fine-tuning\nwith iterative proof repair, leveraging Lean's type checker and semantic\nequivalence feedback to ensure both syntactic correctness and semantic\nfidelity. Experiments show substantial improvements in proof auto-formalization\nover strong baselines (including GPT-5, Gemini-2.5, Kimina-Prover,\nDeepSeek-Prover), with our retrieval-augmented approach yielding significant\ngains in semantic correctness (SC, via proving bi-directional equivalence) and\ntype correctness (TC, via type-checking theorem+proof) across pass@k metrics on\nminiF2F-Test-PF, a dataset we curated. In particular, ProofBridge improves\ncross-modal retrieval quality by up to 3.28x Recall@1 over all-MiniLM-L6-v2,\nand achieves +31.14% SC and +1.64% TC (pass@32) compared to the baseline\nKimina-Prover-RL-1.7B.\n", "link": "http://arxiv.org/abs/2510.15681v1", "date": "2025-10-17", "relevancy": 1.9733, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.4941}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.4941}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.4895}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20ProofBridge%3A%20Auto-Formalization%20of%20Natural%20Language%20Proofs%20in%20Lean%20via%0A%20%20Joint%20Embeddings&body=Title%3A%20ProofBridge%3A%20Auto-Formalization%20of%20Natural%20Language%20Proofs%20in%20Lean%20via%0A%20%20Joint%20Embeddings%0AAuthor%3A%20Prithwish%20Jana%20and%20Kaan%20Kale%20and%20Ahmet%20Ege%20Tanriverdi%20and%20Cruise%20Song%20and%20Sriram%20Vishwanath%20and%20Vijay%20Ganesh%0AAbstract%3A%20%20%20Translating%20human-written%20mathematical%20theorems%20and%20proofs%20from%20natural%0Alanguage%20%28NL%29%20into%20formal%20languages%20%28FLs%29%20like%20Lean%204%20has%20long%20been%20a%0Asignificant%20challenge%20for%20AI.%20Most%20state-of-the-art%20methods%20address%20this%0Aseparately%2C%20first%20translating%20theorems%20and%20then%20generating%20proofs%2C%20creating%20a%0Afundamental%20disconnect%20vis-a-vis%20true%20proof%20auto-formalization.%20This%20two-step%0Aprocess%20and%20its%20limitations%20were%20evident%20even%20in%20AlphaProof%27s%20silver-medal%0Aperformance%20at%20the%202024%20IMO%2C%20where%20problem%20statements%20needed%20manual%20translation%0Abefore%20automated%20proof%20synthesis.%0A%20%20We%20present%20ProofBridge%2C%20a%20unified%20framework%20for%20automatically%20translating%0Aentire%20NL%20theorems%20and%20proofs%20into%20Lean%204.%20At%20its%20core%20is%20a%20joint%20embedding%0Amodel%20that%20aligns%20NL%20and%20FL%20%28NL-FL%29%20theorem-proof%20pairs%20in%20a%20shared%20semantic%0Aspace%2C%20enabling%20cross-modal%20retrieval%20of%20semantically%20relevant%20FL%20examples%20to%0Aguide%20translation.%20Our%20training%20ensures%20that%20NL-FL%20theorems%20%28and%20their%20proofs%29%0Aare%20mapped%20close%20together%20in%20this%20space%20if%20and%20only%20if%20the%20NL-FL%20pairs%20are%0Asemantically%20equivalent.%20ProofBridge%20integrates%20retrieval-augmented%20fine-tuning%0Awith%20iterative%20proof%20repair%2C%20leveraging%20Lean%27s%20type%20checker%20and%20semantic%0Aequivalence%20feedback%20to%20ensure%20both%20syntactic%20correctness%20and%20semantic%0Afidelity.%20Experiments%20show%20substantial%20improvements%20in%20proof%20auto-formalization%0Aover%20strong%20baselines%20%28including%20GPT-5%2C%20Gemini-2.5%2C%20Kimina-Prover%2C%0ADeepSeek-Prover%29%2C%20with%20our%20retrieval-augmented%20approach%20yielding%20significant%0Agains%20in%20semantic%20correctness%20%28SC%2C%20via%20proving%20bi-directional%20equivalence%29%20and%0Atype%20correctness%20%28TC%2C%20via%20type-checking%20theorem%2Bproof%29%20across%20pass%40k%20metrics%20on%0AminiF2F-Test-PF%2C%20a%20dataset%20we%20curated.%20In%20particular%2C%20ProofBridge%20improves%0Across-modal%20retrieval%20quality%20by%20up%20to%203.28x%20Recall%401%20over%20all-MiniLM-L6-v2%2C%0Aand%20achieves%20%2B31.14%25%20SC%20and%20%2B1.64%25%20TC%20%28pass%4032%29%20compared%20to%20the%20baseline%0AKimina-Prover-RL-1.7B.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2510.15681v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DProofBridge%253A%2520Auto-Formalization%2520of%2520Natural%2520Language%2520Proofs%2520in%2520Lean%2520via%250A%2520%2520Joint%2520Embeddings%26entry.906535625%3DPrithwish%2520Jana%2520and%2520Kaan%2520Kale%2520and%2520Ahmet%2520Ege%2520Tanriverdi%2520and%2520Cruise%2520Song%2520and%2520Sriram%2520Vishwanath%2520and%2520Vijay%2520Ganesh%26entry.1292438233%3D%2520%2520Translating%2520human-written%2520mathematical%2520theorems%2520and%2520proofs%2520from%2520natural%250Alanguage%2520%2528NL%2529%2520into%2520formal%2520languages%2520%2528FLs%2529%2520like%2520Lean%25204%2520has%2520long%2520been%2520a%250Asignificant%2520challenge%2520for%2520AI.%2520Most%2520state-of-the-art%2520methods%2520address%2520this%250Aseparately%252C%2520first%2520translating%2520theorems%2520and%2520then%2520generating%2520proofs%252C%2520creating%2520a%250Afundamental%2520disconnect%2520vis-a-vis%2520true%2520proof%2520auto-formalization.%2520This%2520two-step%250Aprocess%2520and%2520its%2520limitations%2520were%2520evident%2520even%2520in%2520AlphaProof%2527s%2520silver-medal%250Aperformance%2520at%2520the%25202024%2520IMO%252C%2520where%2520problem%2520statements%2520needed%2520manual%2520translation%250Abefore%2520automated%2520proof%2520synthesis.%250A%2520%2520We%2520present%2520ProofBridge%252C%2520a%2520unified%2520framework%2520for%2520automatically%2520translating%250Aentire%2520NL%2520theorems%2520and%2520proofs%2520into%2520Lean%25204.%2520At%2520its%2520core%2520is%2520a%2520joint%2520embedding%250Amodel%2520that%2520aligns%2520NL%2520and%2520FL%2520%2528NL-FL%2529%2520theorem-proof%2520pairs%2520in%2520a%2520shared%2520semantic%250Aspace%252C%2520enabling%2520cross-modal%2520retrieval%2520of%2520semantically%2520relevant%2520FL%2520examples%2520to%250Aguide%2520translation.%2520Our%2520training%2520ensures%2520that%2520NL-FL%2520theorems%2520%2528and%2520their%2520proofs%2529%250Aare%2520mapped%2520close%2520together%2520in%2520this%2520space%2520if%2520and%2520only%2520if%2520the%2520NL-FL%2520pairs%2520are%250Asemantically%2520equivalent.%2520ProofBridge%2520integrates%2520retrieval-augmented%2520fine-tuning%250Awith%2520iterative%2520proof%2520repair%252C%2520leveraging%2520Lean%2527s%2520type%2520checker%2520and%2520semantic%250Aequivalence%2520feedback%2520to%2520ensure%2520both%2520syntactic%2520correctness%2520and%2520semantic%250Afidelity.%2520Experiments%2520show%2520substantial%2520improvements%2520in%2520proof%2520auto-formalization%250Aover%2520strong%2520baselines%2520%2528including%2520GPT-5%252C%2520Gemini-2.5%252C%2520Kimina-Prover%252C%250ADeepSeek-Prover%2529%252C%2520with%2520our%2520retrieval-augmented%2520approach%2520yielding%2520significant%250Agains%2520in%2520semantic%2520correctness%2520%2528SC%252C%2520via%2520proving%2520bi-directional%2520equivalence%2529%2520and%250Atype%2520correctness%2520%2528TC%252C%2520via%2520type-checking%2520theorem%252Bproof%2529%2520across%2520pass%2540k%2520metrics%2520on%250AminiF2F-Test-PF%252C%2520a%2520dataset%2520we%2520curated.%2520In%2520particular%252C%2520ProofBridge%2520improves%250Across-modal%2520retrieval%2520quality%2520by%2520up%2520to%25203.28x%2520Recall%25401%2520over%2520all-MiniLM-L6-v2%252C%250Aand%2520achieves%2520%252B31.14%2525%2520SC%2520and%2520%252B1.64%2525%2520TC%2520%2528pass%254032%2529%2520compared%2520to%2520the%2520baseline%250AKimina-Prover-RL-1.7B.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2510.15681v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=ProofBridge%3A%20Auto-Formalization%20of%20Natural%20Language%20Proofs%20in%20Lean%20via%0A%20%20Joint%20Embeddings&entry.906535625=Prithwish%20Jana%20and%20Kaan%20Kale%20and%20Ahmet%20Ege%20Tanriverdi%20and%20Cruise%20Song%20and%20Sriram%20Vishwanath%20and%20Vijay%20Ganesh&entry.1292438233=%20%20Translating%20human-written%20mathematical%20theorems%20and%20proofs%20from%20natural%0Alanguage%20%28NL%29%20into%20formal%20languages%20%28FLs%29%20like%20Lean%204%20has%20long%20been%20a%0Asignificant%20challenge%20for%20AI.%20Most%20state-of-the-art%20methods%20address%20this%0Aseparately%2C%20first%20translating%20theorems%20and%20then%20generating%20proofs%2C%20creating%20a%0Afundamental%20disconnect%20vis-a-vis%20true%20proof%20auto-formalization.%20This%20two-step%0Aprocess%20and%20its%20limitations%20were%20evident%20even%20in%20AlphaProof%27s%20silver-medal%0Aperformance%20at%20the%202024%20IMO%2C%20where%20problem%20statements%20needed%20manual%20translation%0Abefore%20automated%20proof%20synthesis.%0A%20%20We%20present%20ProofBridge%2C%20a%20unified%20framework%20for%20automatically%20translating%0Aentire%20NL%20theorems%20and%20proofs%20into%20Lean%204.%20At%20its%20core%20is%20a%20joint%20embedding%0Amodel%20that%20aligns%20NL%20and%20FL%20%28NL-FL%29%20theorem-proof%20pairs%20in%20a%20shared%20semantic%0Aspace%2C%20enabling%20cross-modal%20retrieval%20of%20semantically%20relevant%20FL%20examples%20to%0Aguide%20translation.%20Our%20training%20ensures%20that%20NL-FL%20theorems%20%28and%20their%20proofs%29%0Aare%20mapped%20close%20together%20in%20this%20space%20if%20and%20only%20if%20the%20NL-FL%20pairs%20are%0Asemantically%20equivalent.%20ProofBridge%20integrates%20retrieval-augmented%20fine-tuning%0Awith%20iterative%20proof%20repair%2C%20leveraging%20Lean%27s%20type%20checker%20and%20semantic%0Aequivalence%20feedback%20to%20ensure%20both%20syntactic%20correctness%20and%20semantic%0Afidelity.%20Experiments%20show%20substantial%20improvements%20in%20proof%20auto-formalization%0Aover%20strong%20baselines%20%28including%20GPT-5%2C%20Gemini-2.5%2C%20Kimina-Prover%2C%0ADeepSeek-Prover%29%2C%20with%20our%20retrieval-augmented%20approach%20yielding%20significant%0Agains%20in%20semantic%20correctness%20%28SC%2C%20via%20proving%20bi-directional%20equivalence%29%20and%0Atype%20correctness%20%28TC%2C%20via%20type-checking%20theorem%2Bproof%29%20across%20pass%40k%20metrics%20on%0AminiF2F-Test-PF%2C%20a%20dataset%20we%20curated.%20In%20particular%2C%20ProofBridge%20improves%0Across-modal%20retrieval%20quality%20by%20up%20to%203.28x%20Recall%401%20over%20all-MiniLM-L6-v2%2C%0Aand%20achieves%20%2B31.14%25%20SC%20and%20%2B1.64%25%20TC%20%28pass%4032%29%20compared%20to%20the%20baseline%0AKimina-Prover-RL-1.7B.%0A&entry.1838667208=http%3A//arxiv.org/abs/2510.15681v1&entry.124074799=Read"},
{"title": "CoUn: Empowering Machine Unlearning via Contrastive Learning", "author": "Yasser H. Khalil and Mehdi Setayesh and Hongliang Li", "abstract": "  Machine unlearning (MU) aims to remove the influence of specific \"forget\"\ndata from a trained model while preserving its knowledge of the remaining\n\"retain\" data. Existing MU methods based on label manipulation or model weight\nperturbations often achieve limited unlearning effectiveness. To address this,\nwe introduce CoUn, a novel MU framework inspired by the observation that a\nmodel retrained from scratch using only retain data classifies forget data\nbased on their semantic similarity to the retain data. CoUn emulates this\nbehavior by adjusting learned data representations through contrastive learning\n(CL) and supervised learning, applied exclusively to retain data. Specifically,\nCoUn (1) leverages semantic similarity between data samples to indirectly\nadjust forget representations using CL, and (2) maintains retain\nrepresentations within their respective clusters through supervised learning.\nExtensive experiments across various datasets and model architectures show that\nCoUn consistently outperforms state-of-the-art MU baselines in unlearning\neffectiveness. Additionally, integrating our CL module into existing baselines\nempowers their unlearning effectiveness.\n", "link": "http://arxiv.org/abs/2509.16391v2", "date": "2025-10-17", "relevancy": 1.9702, "topK": [{"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.5007}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.4941}, {"title": "Self-supervised Photographic Image Layout Representation Learning", "link": "http://arxiv.org/abs/2403.03740v1", "similarity": 0.4877}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20CoUn%3A%20Empowering%20Machine%20Unlearning%20via%20Contrastive%20Learning&body=Title%3A%20CoUn%3A%20Empowering%20Machine%20Unlearning%20via%20Contrastive%20Learning%0AAuthor%3A%20Yasser%20H.%20Khalil%20and%20Mehdi%20Setayesh%20and%20Hongliang%20Li%0AAbstract%3A%20%20%20Machine%20unlearning%20%28MU%29%20aims%20to%20remove%20the%20influence%20of%20specific%20%22forget%22%0Adata%20from%20a%20trained%20model%20while%20preserving%20its%20knowledge%20of%20the%20remaining%0A%22retain%22%20data.%20Existing%20MU%20methods%20based%20on%20label%20manipulation%20or%20model%20weight%0Aperturbations%20often%20achieve%20limited%20unlearning%20effectiveness.%20To%20address%20this%2C%0Awe%20introduce%20CoUn%2C%20a%20novel%20MU%20framework%20inspired%20by%20the%20observation%20that%20a%0Amodel%20retrained%20from%20scratch%20using%20only%20retain%20data%20classifies%20forget%20data%0Abased%20on%20their%20semantic%20similarity%20to%20the%20retain%20data.%20CoUn%20emulates%20this%0Abehavior%20by%20adjusting%20learned%20data%20representations%20through%20contrastive%20learning%0A%28CL%29%20and%20supervised%20learning%2C%20applied%20exclusively%20to%20retain%20data.%20Specifically%2C%0ACoUn%20%281%29%20leverages%20semantic%20similarity%20between%20data%20samples%20to%20indirectly%0Aadjust%20forget%20representations%20using%20CL%2C%20and%20%282%29%20maintains%20retain%0Arepresentations%20within%20their%20respective%20clusters%20through%20supervised%20learning.%0AExtensive%20experiments%20across%20various%20datasets%20and%20model%20architectures%20show%20that%0ACoUn%20consistently%20outperforms%20state-of-the-art%20MU%20baselines%20in%20unlearning%0Aeffectiveness.%20Additionally%2C%20integrating%20our%20CL%20module%20into%20existing%20baselines%0Aempowers%20their%20unlearning%20effectiveness.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2509.16391v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DCoUn%253A%2520Empowering%2520Machine%2520Unlearning%2520via%2520Contrastive%2520Learning%26entry.906535625%3DYasser%2520H.%2520Khalil%2520and%2520Mehdi%2520Setayesh%2520and%2520Hongliang%2520Li%26entry.1292438233%3D%2520%2520Machine%2520unlearning%2520%2528MU%2529%2520aims%2520to%2520remove%2520the%2520influence%2520of%2520specific%2520%2522forget%2522%250Adata%2520from%2520a%2520trained%2520model%2520while%2520preserving%2520its%2520knowledge%2520of%2520the%2520remaining%250A%2522retain%2522%2520data.%2520Existing%2520MU%2520methods%2520based%2520on%2520label%2520manipulation%2520or%2520model%2520weight%250Aperturbations%2520often%2520achieve%2520limited%2520unlearning%2520effectiveness.%2520To%2520address%2520this%252C%250Awe%2520introduce%2520CoUn%252C%2520a%2520novel%2520MU%2520framework%2520inspired%2520by%2520the%2520observation%2520that%2520a%250Amodel%2520retrained%2520from%2520scratch%2520using%2520only%2520retain%2520data%2520classifies%2520forget%2520data%250Abased%2520on%2520their%2520semantic%2520similarity%2520to%2520the%2520retain%2520data.%2520CoUn%2520emulates%2520this%250Abehavior%2520by%2520adjusting%2520learned%2520data%2520representations%2520through%2520contrastive%2520learning%250A%2528CL%2529%2520and%2520supervised%2520learning%252C%2520applied%2520exclusively%2520to%2520retain%2520data.%2520Specifically%252C%250ACoUn%2520%25281%2529%2520leverages%2520semantic%2520similarity%2520between%2520data%2520samples%2520to%2520indirectly%250Aadjust%2520forget%2520representations%2520using%2520CL%252C%2520and%2520%25282%2529%2520maintains%2520retain%250Arepresentations%2520within%2520their%2520respective%2520clusters%2520through%2520supervised%2520learning.%250AExtensive%2520experiments%2520across%2520various%2520datasets%2520and%2520model%2520architectures%2520show%2520that%250ACoUn%2520consistently%2520outperforms%2520state-of-the-art%2520MU%2520baselines%2520in%2520unlearning%250Aeffectiveness.%2520Additionally%252C%2520integrating%2520our%2520CL%2520module%2520into%2520existing%2520baselines%250Aempowers%2520their%2520unlearning%2520effectiveness.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2509.16391v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=CoUn%3A%20Empowering%20Machine%20Unlearning%20via%20Contrastive%20Learning&entry.906535625=Yasser%20H.%20Khalil%20and%20Mehdi%20Setayesh%20and%20Hongliang%20Li&entry.1292438233=%20%20Machine%20unlearning%20%28MU%29%20aims%20to%20remove%20the%20influence%20of%20specific%20%22forget%22%0Adata%20from%20a%20trained%20model%20while%20preserving%20its%20knowledge%20of%20the%20remaining%0A%22retain%22%20data.%20Existing%20MU%20methods%20based%20on%20label%20manipulation%20or%20model%20weight%0Aperturbations%20often%20achieve%20limited%20unlearning%20effectiveness.%20To%20address%20this%2C%0Awe%20introduce%20CoUn%2C%20a%20novel%20MU%20framework%20inspired%20by%20the%20observation%20that%20a%0Amodel%20retrained%20from%20scratch%20using%20only%20retain%20data%20classifies%20forget%20data%0Abased%20on%20their%20semantic%20similarity%20to%20the%20retain%20data.%20CoUn%20emulates%20this%0Abehavior%20by%20adjusting%20learned%20data%20representations%20through%20contrastive%20learning%0A%28CL%29%20and%20supervised%20learning%2C%20applied%20exclusively%20to%20retain%20data.%20Specifically%2C%0ACoUn%20%281%29%20leverages%20semantic%20similarity%20between%20data%20samples%20to%20indirectly%0Aadjust%20forget%20representations%20using%20CL%2C%20and%20%282%29%20maintains%20retain%0Arepresentations%20within%20their%20respective%20clusters%20through%20supervised%20learning.%0AExtensive%20experiments%20across%20various%20datasets%20and%20model%20architectures%20show%20that%0ACoUn%20consistently%20outperforms%20state-of-the-art%20MU%20baselines%20in%20unlearning%0Aeffectiveness.%20Additionally%2C%20integrating%20our%20CL%20module%20into%20existing%20baselines%0Aempowers%20their%20unlearning%20effectiveness.%0A&entry.1838667208=http%3A//arxiv.org/abs/2509.16391v2&entry.124074799=Read"},
{"title": "ProofOptimizer: Training Language Models to Simplify Proofs without\n  Human Demonstrations", "author": "Alex Gu and Bartosz Piotrowski and Fabian Gloeckle and Kaiyu Yang and Aram H. Markosyan", "abstract": "  Neural theorem proving has advanced rapidly in the past year, reaching IMO\ngold-medalist capabilities and producing formal proofs that span thousands of\nlines. Although such proofs are mechanically verified by formal systems like\nLean, their excessive length renders them difficult for humans to comprehend\nand limits their usefulness for mathematical insight. Proof simplification is\ntherefore a critical bottleneck. Yet, training data for this task is scarce,\nand existing methods -- mainly agentic scaffolding with off-the-shelf LLMs --\nstruggle with the extremely long proofs generated by RL-trained provers. We\nintroduce ProofOptimizer, the first language model trained to simplify Lean\nproofs without requiring additional human supervision. ProofOptimizer is\ntrained via expert iteration and reinforcement learning, using Lean to verify\nsimplifications and provide training signal. At inference time, it operates\nwithin an iterative proof-shortening workflow, progressively reducing proof\nlength. Experiments show that ProofOptimizer substantially compresses proofs\ngenerated by state-of-the-art RL-trained provers on standard benchmarks,\nreducing proof length by 87% on miniF2F, 57% on PutnamBench, and 49% on\nSeed-Prover's IMO 2025 proofs. Beyond conciseness, the simplified proofs check\nfaster in Lean and further improve downstream prover performance when reused as\ntraining data for supervised finetuning.\n", "link": "http://arxiv.org/abs/2510.15700v1", "date": "2025-10-17", "relevancy": 1.9608, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.4919}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.4919}, {"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.482}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20ProofOptimizer%3A%20Training%20Language%20Models%20to%20Simplify%20Proofs%20without%0A%20%20Human%20Demonstrations&body=Title%3A%20ProofOptimizer%3A%20Training%20Language%20Models%20to%20Simplify%20Proofs%20without%0A%20%20Human%20Demonstrations%0AAuthor%3A%20Alex%20Gu%20and%20Bartosz%20Piotrowski%20and%20Fabian%20Gloeckle%20and%20Kaiyu%20Yang%20and%20Aram%20H.%20Markosyan%0AAbstract%3A%20%20%20Neural%20theorem%20proving%20has%20advanced%20rapidly%20in%20the%20past%20year%2C%20reaching%20IMO%0Agold-medalist%20capabilities%20and%20producing%20formal%20proofs%20that%20span%20thousands%20of%0Alines.%20Although%20such%20proofs%20are%20mechanically%20verified%20by%20formal%20systems%20like%0ALean%2C%20their%20excessive%20length%20renders%20them%20difficult%20for%20humans%20to%20comprehend%0Aand%20limits%20their%20usefulness%20for%20mathematical%20insight.%20Proof%20simplification%20is%0Atherefore%20a%20critical%20bottleneck.%20Yet%2C%20training%20data%20for%20this%20task%20is%20scarce%2C%0Aand%20existing%20methods%20--%20mainly%20agentic%20scaffolding%20with%20off-the-shelf%20LLMs%20--%0Astruggle%20with%20the%20extremely%20long%20proofs%20generated%20by%20RL-trained%20provers.%20We%0Aintroduce%20ProofOptimizer%2C%20the%20first%20language%20model%20trained%20to%20simplify%20Lean%0Aproofs%20without%20requiring%20additional%20human%20supervision.%20ProofOptimizer%20is%0Atrained%20via%20expert%20iteration%20and%20reinforcement%20learning%2C%20using%20Lean%20to%20verify%0Asimplifications%20and%20provide%20training%20signal.%20At%20inference%20time%2C%20it%20operates%0Awithin%20an%20iterative%20proof-shortening%20workflow%2C%20progressively%20reducing%20proof%0Alength.%20Experiments%20show%20that%20ProofOptimizer%20substantially%20compresses%20proofs%0Agenerated%20by%20state-of-the-art%20RL-trained%20provers%20on%20standard%20benchmarks%2C%0Areducing%20proof%20length%20by%2087%25%20on%20miniF2F%2C%2057%25%20on%20PutnamBench%2C%20and%2049%25%20on%0ASeed-Prover%27s%20IMO%202025%20proofs.%20Beyond%20conciseness%2C%20the%20simplified%20proofs%20check%0Afaster%20in%20Lean%20and%20further%20improve%20downstream%20prover%20performance%20when%20reused%20as%0Atraining%20data%20for%20supervised%20finetuning.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2510.15700v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DProofOptimizer%253A%2520Training%2520Language%2520Models%2520to%2520Simplify%2520Proofs%2520without%250A%2520%2520Human%2520Demonstrations%26entry.906535625%3DAlex%2520Gu%2520and%2520Bartosz%2520Piotrowski%2520and%2520Fabian%2520Gloeckle%2520and%2520Kaiyu%2520Yang%2520and%2520Aram%2520H.%2520Markosyan%26entry.1292438233%3D%2520%2520Neural%2520theorem%2520proving%2520has%2520advanced%2520rapidly%2520in%2520the%2520past%2520year%252C%2520reaching%2520IMO%250Agold-medalist%2520capabilities%2520and%2520producing%2520formal%2520proofs%2520that%2520span%2520thousands%2520of%250Alines.%2520Although%2520such%2520proofs%2520are%2520mechanically%2520verified%2520by%2520formal%2520systems%2520like%250ALean%252C%2520their%2520excessive%2520length%2520renders%2520them%2520difficult%2520for%2520humans%2520to%2520comprehend%250Aand%2520limits%2520their%2520usefulness%2520for%2520mathematical%2520insight.%2520Proof%2520simplification%2520is%250Atherefore%2520a%2520critical%2520bottleneck.%2520Yet%252C%2520training%2520data%2520for%2520this%2520task%2520is%2520scarce%252C%250Aand%2520existing%2520methods%2520--%2520mainly%2520agentic%2520scaffolding%2520with%2520off-the-shelf%2520LLMs%2520--%250Astruggle%2520with%2520the%2520extremely%2520long%2520proofs%2520generated%2520by%2520RL-trained%2520provers.%2520We%250Aintroduce%2520ProofOptimizer%252C%2520the%2520first%2520language%2520model%2520trained%2520to%2520simplify%2520Lean%250Aproofs%2520without%2520requiring%2520additional%2520human%2520supervision.%2520ProofOptimizer%2520is%250Atrained%2520via%2520expert%2520iteration%2520and%2520reinforcement%2520learning%252C%2520using%2520Lean%2520to%2520verify%250Asimplifications%2520and%2520provide%2520training%2520signal.%2520At%2520inference%2520time%252C%2520it%2520operates%250Awithin%2520an%2520iterative%2520proof-shortening%2520workflow%252C%2520progressively%2520reducing%2520proof%250Alength.%2520Experiments%2520show%2520that%2520ProofOptimizer%2520substantially%2520compresses%2520proofs%250Agenerated%2520by%2520state-of-the-art%2520RL-trained%2520provers%2520on%2520standard%2520benchmarks%252C%250Areducing%2520proof%2520length%2520by%252087%2525%2520on%2520miniF2F%252C%252057%2525%2520on%2520PutnamBench%252C%2520and%252049%2525%2520on%250ASeed-Prover%2527s%2520IMO%25202025%2520proofs.%2520Beyond%2520conciseness%252C%2520the%2520simplified%2520proofs%2520check%250Afaster%2520in%2520Lean%2520and%2520further%2520improve%2520downstream%2520prover%2520performance%2520when%2520reused%2520as%250Atraining%2520data%2520for%2520supervised%2520finetuning.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2510.15700v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=ProofOptimizer%3A%20Training%20Language%20Models%20to%20Simplify%20Proofs%20without%0A%20%20Human%20Demonstrations&entry.906535625=Alex%20Gu%20and%20Bartosz%20Piotrowski%20and%20Fabian%20Gloeckle%20and%20Kaiyu%20Yang%20and%20Aram%20H.%20Markosyan&entry.1292438233=%20%20Neural%20theorem%20proving%20has%20advanced%20rapidly%20in%20the%20past%20year%2C%20reaching%20IMO%0Agold-medalist%20capabilities%20and%20producing%20formal%20proofs%20that%20span%20thousands%20of%0Alines.%20Although%20such%20proofs%20are%20mechanically%20verified%20by%20formal%20systems%20like%0ALean%2C%20their%20excessive%20length%20renders%20them%20difficult%20for%20humans%20to%20comprehend%0Aand%20limits%20their%20usefulness%20for%20mathematical%20insight.%20Proof%20simplification%20is%0Atherefore%20a%20critical%20bottleneck.%20Yet%2C%20training%20data%20for%20this%20task%20is%20scarce%2C%0Aand%20existing%20methods%20--%20mainly%20agentic%20scaffolding%20with%20off-the-shelf%20LLMs%20--%0Astruggle%20with%20the%20extremely%20long%20proofs%20generated%20by%20RL-trained%20provers.%20We%0Aintroduce%20ProofOptimizer%2C%20the%20first%20language%20model%20trained%20to%20simplify%20Lean%0Aproofs%20without%20requiring%20additional%20human%20supervision.%20ProofOptimizer%20is%0Atrained%20via%20expert%20iteration%20and%20reinforcement%20learning%2C%20using%20Lean%20to%20verify%0Asimplifications%20and%20provide%20training%20signal.%20At%20inference%20time%2C%20it%20operates%0Awithin%20an%20iterative%20proof-shortening%20workflow%2C%20progressively%20reducing%20proof%0Alength.%20Experiments%20show%20that%20ProofOptimizer%20substantially%20compresses%20proofs%0Agenerated%20by%20state-of-the-art%20RL-trained%20provers%20on%20standard%20benchmarks%2C%0Areducing%20proof%20length%20by%2087%25%20on%20miniF2F%2C%2057%25%20on%20PutnamBench%2C%20and%2049%25%20on%0ASeed-Prover%27s%20IMO%202025%20proofs.%20Beyond%20conciseness%2C%20the%20simplified%20proofs%20check%0Afaster%20in%20Lean%20and%20further%20improve%20downstream%20prover%20performance%20when%20reused%20as%0Atraining%20data%20for%20supervised%20finetuning.%0A&entry.1838667208=http%3A//arxiv.org/abs/2510.15700v1&entry.124074799=Read"},
{"title": "Semantic segmentation with coarse annotations", "author": "Jort de Jong and Mike Holenderski", "abstract": "  Semantic segmentation is the task of classifying each pixel in an image.\nTraining a segmentation model achieves best results using annotated images,\nwhere each pixel is annotated with the corresponding class. When obtaining fine\nannotations is difficult or expensive, it may be possible to acquire coarse\nannotations, e.g. by roughly annotating pixels in an images leaving some pixels\naround the boundaries between classes unlabeled. Segmentation with coarse\nannotations is difficult, in particular when the objective is to optimize the\nalignment of boundaries between classes. This paper proposes a regularization\nmethod for models with an encoder-decoder architecture with superpixel based\nupsampling. It encourages the segmented pixels in the decoded image to be\nSLIC-superpixels, which are based on pixel color and position, independent of\nthe segmentation annotation. The method is applied to FCN-16 fully\nconvolutional network architecture and evaluated on the SUIM, Cityscapes, and\nPanNuke data sets. It is shown that the boundary recall improves significantly\ncompared to state-of-the-art models when trained on coarse annotations.\n", "link": "http://arxiv.org/abs/2510.15756v1", "date": "2025-10-17", "relevancy": 1.9567, "topK": [{"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.4944}, {"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.4904}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.4859}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Semantic%20segmentation%20with%20coarse%20annotations&body=Title%3A%20Semantic%20segmentation%20with%20coarse%20annotations%0AAuthor%3A%20Jort%20de%20Jong%20and%20Mike%20Holenderski%0AAbstract%3A%20%20%20Semantic%20segmentation%20is%20the%20task%20of%20classifying%20each%20pixel%20in%20an%20image.%0ATraining%20a%20segmentation%20model%20achieves%20best%20results%20using%20annotated%20images%2C%0Awhere%20each%20pixel%20is%20annotated%20with%20the%20corresponding%20class.%20When%20obtaining%20fine%0Aannotations%20is%20difficult%20or%20expensive%2C%20it%20may%20be%20possible%20to%20acquire%20coarse%0Aannotations%2C%20e.g.%20by%20roughly%20annotating%20pixels%20in%20an%20images%20leaving%20some%20pixels%0Aaround%20the%20boundaries%20between%20classes%20unlabeled.%20Segmentation%20with%20coarse%0Aannotations%20is%20difficult%2C%20in%20particular%20when%20the%20objective%20is%20to%20optimize%20the%0Aalignment%20of%20boundaries%20between%20classes.%20This%20paper%20proposes%20a%20regularization%0Amethod%20for%20models%20with%20an%20encoder-decoder%20architecture%20with%20superpixel%20based%0Aupsampling.%20It%20encourages%20the%20segmented%20pixels%20in%20the%20decoded%20image%20to%20be%0ASLIC-superpixels%2C%20which%20are%20based%20on%20pixel%20color%20and%20position%2C%20independent%20of%0Athe%20segmentation%20annotation.%20The%20method%20is%20applied%20to%20FCN-16%20fully%0Aconvolutional%20network%20architecture%20and%20evaluated%20on%20the%20SUIM%2C%20Cityscapes%2C%20and%0APanNuke%20data%20sets.%20It%20is%20shown%20that%20the%20boundary%20recall%20improves%20significantly%0Acompared%20to%20state-of-the-art%20models%20when%20trained%20on%20coarse%20annotations.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2510.15756v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DSemantic%2520segmentation%2520with%2520coarse%2520annotations%26entry.906535625%3DJort%2520de%2520Jong%2520and%2520Mike%2520Holenderski%26entry.1292438233%3D%2520%2520Semantic%2520segmentation%2520is%2520the%2520task%2520of%2520classifying%2520each%2520pixel%2520in%2520an%2520image.%250ATraining%2520a%2520segmentation%2520model%2520achieves%2520best%2520results%2520using%2520annotated%2520images%252C%250Awhere%2520each%2520pixel%2520is%2520annotated%2520with%2520the%2520corresponding%2520class.%2520When%2520obtaining%2520fine%250Aannotations%2520is%2520difficult%2520or%2520expensive%252C%2520it%2520may%2520be%2520possible%2520to%2520acquire%2520coarse%250Aannotations%252C%2520e.g.%2520by%2520roughly%2520annotating%2520pixels%2520in%2520an%2520images%2520leaving%2520some%2520pixels%250Aaround%2520the%2520boundaries%2520between%2520classes%2520unlabeled.%2520Segmentation%2520with%2520coarse%250Aannotations%2520is%2520difficult%252C%2520in%2520particular%2520when%2520the%2520objective%2520is%2520to%2520optimize%2520the%250Aalignment%2520of%2520boundaries%2520between%2520classes.%2520This%2520paper%2520proposes%2520a%2520regularization%250Amethod%2520for%2520models%2520with%2520an%2520encoder-decoder%2520architecture%2520with%2520superpixel%2520based%250Aupsampling.%2520It%2520encourages%2520the%2520segmented%2520pixels%2520in%2520the%2520decoded%2520image%2520to%2520be%250ASLIC-superpixels%252C%2520which%2520are%2520based%2520on%2520pixel%2520color%2520and%2520position%252C%2520independent%2520of%250Athe%2520segmentation%2520annotation.%2520The%2520method%2520is%2520applied%2520to%2520FCN-16%2520fully%250Aconvolutional%2520network%2520architecture%2520and%2520evaluated%2520on%2520the%2520SUIM%252C%2520Cityscapes%252C%2520and%250APanNuke%2520data%2520sets.%2520It%2520is%2520shown%2520that%2520the%2520boundary%2520recall%2520improves%2520significantly%250Acompared%2520to%2520state-of-the-art%2520models%2520when%2520trained%2520on%2520coarse%2520annotations.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2510.15756v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Semantic%20segmentation%20with%20coarse%20annotations&entry.906535625=Jort%20de%20Jong%20and%20Mike%20Holenderski&entry.1292438233=%20%20Semantic%20segmentation%20is%20the%20task%20of%20classifying%20each%20pixel%20in%20an%20image.%0ATraining%20a%20segmentation%20model%20achieves%20best%20results%20using%20annotated%20images%2C%0Awhere%20each%20pixel%20is%20annotated%20with%20the%20corresponding%20class.%20When%20obtaining%20fine%0Aannotations%20is%20difficult%20or%20expensive%2C%20it%20may%20be%20possible%20to%20acquire%20coarse%0Aannotations%2C%20e.g.%20by%20roughly%20annotating%20pixels%20in%20an%20images%20leaving%20some%20pixels%0Aaround%20the%20boundaries%20between%20classes%20unlabeled.%20Segmentation%20with%20coarse%0Aannotations%20is%20difficult%2C%20in%20particular%20when%20the%20objective%20is%20to%20optimize%20the%0Aalignment%20of%20boundaries%20between%20classes.%20This%20paper%20proposes%20a%20regularization%0Amethod%20for%20models%20with%20an%20encoder-decoder%20architecture%20with%20superpixel%20based%0Aupsampling.%20It%20encourages%20the%20segmented%20pixels%20in%20the%20decoded%20image%20to%20be%0ASLIC-superpixels%2C%20which%20are%20based%20on%20pixel%20color%20and%20position%2C%20independent%20of%0Athe%20segmentation%20annotation.%20The%20method%20is%20applied%20to%20FCN-16%20fully%0Aconvolutional%20network%20architecture%20and%20evaluated%20on%20the%20SUIM%2C%20Cityscapes%2C%20and%0APanNuke%20data%20sets.%20It%20is%20shown%20that%20the%20boundary%20recall%20improves%20significantly%0Acompared%20to%20state-of-the-art%20models%20when%20trained%20on%20coarse%20annotations.%0A&entry.1838667208=http%3A//arxiv.org/abs/2510.15756v1&entry.124074799=Read"},
{"title": "MegaScale-MoE: Large-Scale Communication-Efficient Training of\n  Mixture-of-Experts Models in Production", "author": "Chao Jin and Ziheng Jiang and Zhihao Bai and Zheng Zhong and Juncai Liu and Xiang Li and Ningxin Zheng and Xi Wang and Cong Xie and Qi Huang and Wen Heng and Yiyuan Ma and Wenlei Bao and Size Zheng and Yanghua Peng and Haibin Lin and Xuanzhe Liu and Xin Jin and Xin Liu", "abstract": "  We present MegaScale-MoE, a production system tailored for the efficient\ntraining of large-scale mixture-of-experts (MoE) models. MoE emerges as a\npromising architecture to scale large language models (LLMs) to unprecedented\nsizes, thereby enhancing model performance. However, existing MoE training\nsystems experience a degradation in training efficiency, exacerbated by the\nescalating scale of MoE models and the continuous evolution of hardware.\n  Recognizing the pivotal role of efficient communication in enhancing MoE\ntraining, MegaScale-MoE customizes communication-efficient parallelism\nstrategies for attention and FFNs in each MoE layer and adopts a holistic\napproach to overlap communication with computation at both inter- and\nintra-operator levels. Additionally, MegaScale-MoE applies communication\ncompression with adjusted communication patterns to lower precision, further\nimproving training efficiency. When training a 352B MoE model on 1,440 NVIDIA\nHopper GPUs, MegaScale-MoE achieves a training throughput of 1.41M tokens/s,\nimproving the efficiency by 1.88$\\times$ compared to Megatron-LM. We share our\noperational experience in accelerating MoE training and hope that by offering\nour insights in system design, this work will motivate future research in MoE\nsystems.\n", "link": "http://arxiv.org/abs/2505.11432v3", "date": "2025-10-17", "relevancy": 1.9563, "topK": [{"title": "Scaling Rectified Flow Transformers for High-Resolution Image Synthesis", "link": "http://arxiv.org/abs/2403.03206v1", "similarity": 0.5087}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.4861}, {"title": "ToDo: Token Downsampling for Efficient Generation of High-Resolution\n  Images", "link": "http://arxiv.org/abs/2402.13573v2", "similarity": 0.4842}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20MegaScale-MoE%3A%20Large-Scale%20Communication-Efficient%20Training%20of%0A%20%20Mixture-of-Experts%20Models%20in%20Production&body=Title%3A%20MegaScale-MoE%3A%20Large-Scale%20Communication-Efficient%20Training%20of%0A%20%20Mixture-of-Experts%20Models%20in%20Production%0AAuthor%3A%20Chao%20Jin%20and%20Ziheng%20Jiang%20and%20Zhihao%20Bai%20and%20Zheng%20Zhong%20and%20Juncai%20Liu%20and%20Xiang%20Li%20and%20Ningxin%20Zheng%20and%20Xi%20Wang%20and%20Cong%20Xie%20and%20Qi%20Huang%20and%20Wen%20Heng%20and%20Yiyuan%20Ma%20and%20Wenlei%20Bao%20and%20Size%20Zheng%20and%20Yanghua%20Peng%20and%20Haibin%20Lin%20and%20Xuanzhe%20Liu%20and%20Xin%20Jin%20and%20Xin%20Liu%0AAbstract%3A%20%20%20We%20present%20MegaScale-MoE%2C%20a%20production%20system%20tailored%20for%20the%20efficient%0Atraining%20of%20large-scale%20mixture-of-experts%20%28MoE%29%20models.%20MoE%20emerges%20as%20a%0Apromising%20architecture%20to%20scale%20large%20language%20models%20%28LLMs%29%20to%20unprecedented%0Asizes%2C%20thereby%20enhancing%20model%20performance.%20However%2C%20existing%20MoE%20training%0Asystems%20experience%20a%20degradation%20in%20training%20efficiency%2C%20exacerbated%20by%20the%0Aescalating%20scale%20of%20MoE%20models%20and%20the%20continuous%20evolution%20of%20hardware.%0A%20%20Recognizing%20the%20pivotal%20role%20of%20efficient%20communication%20in%20enhancing%20MoE%0Atraining%2C%20MegaScale-MoE%20customizes%20communication-efficient%20parallelism%0Astrategies%20for%20attention%20and%20FFNs%20in%20each%20MoE%20layer%20and%20adopts%20a%20holistic%0Aapproach%20to%20overlap%20communication%20with%20computation%20at%20both%20inter-%20and%0Aintra-operator%20levels.%20Additionally%2C%20MegaScale-MoE%20applies%20communication%0Acompression%20with%20adjusted%20communication%20patterns%20to%20lower%20precision%2C%20further%0Aimproving%20training%20efficiency.%20When%20training%20a%20352B%20MoE%20model%20on%201%2C440%20NVIDIA%0AHopper%20GPUs%2C%20MegaScale-MoE%20achieves%20a%20training%20throughput%20of%201.41M%20tokens/s%2C%0Aimproving%20the%20efficiency%20by%201.88%24%5Ctimes%24%20compared%20to%20Megatron-LM.%20We%20share%20our%0Aoperational%20experience%20in%20accelerating%20MoE%20training%20and%20hope%20that%20by%20offering%0Aour%20insights%20in%20system%20design%2C%20this%20work%20will%20motivate%20future%20research%20in%20MoE%0Asystems.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2505.11432v3%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DMegaScale-MoE%253A%2520Large-Scale%2520Communication-Efficient%2520Training%2520of%250A%2520%2520Mixture-of-Experts%2520Models%2520in%2520Production%26entry.906535625%3DChao%2520Jin%2520and%2520Ziheng%2520Jiang%2520and%2520Zhihao%2520Bai%2520and%2520Zheng%2520Zhong%2520and%2520Juncai%2520Liu%2520and%2520Xiang%2520Li%2520and%2520Ningxin%2520Zheng%2520and%2520Xi%2520Wang%2520and%2520Cong%2520Xie%2520and%2520Qi%2520Huang%2520and%2520Wen%2520Heng%2520and%2520Yiyuan%2520Ma%2520and%2520Wenlei%2520Bao%2520and%2520Size%2520Zheng%2520and%2520Yanghua%2520Peng%2520and%2520Haibin%2520Lin%2520and%2520Xuanzhe%2520Liu%2520and%2520Xin%2520Jin%2520and%2520Xin%2520Liu%26entry.1292438233%3D%2520%2520We%2520present%2520MegaScale-MoE%252C%2520a%2520production%2520system%2520tailored%2520for%2520the%2520efficient%250Atraining%2520of%2520large-scale%2520mixture-of-experts%2520%2528MoE%2529%2520models.%2520MoE%2520emerges%2520as%2520a%250Apromising%2520architecture%2520to%2520scale%2520large%2520language%2520models%2520%2528LLMs%2529%2520to%2520unprecedented%250Asizes%252C%2520thereby%2520enhancing%2520model%2520performance.%2520However%252C%2520existing%2520MoE%2520training%250Asystems%2520experience%2520a%2520degradation%2520in%2520training%2520efficiency%252C%2520exacerbated%2520by%2520the%250Aescalating%2520scale%2520of%2520MoE%2520models%2520and%2520the%2520continuous%2520evolution%2520of%2520hardware.%250A%2520%2520Recognizing%2520the%2520pivotal%2520role%2520of%2520efficient%2520communication%2520in%2520enhancing%2520MoE%250Atraining%252C%2520MegaScale-MoE%2520customizes%2520communication-efficient%2520parallelism%250Astrategies%2520for%2520attention%2520and%2520FFNs%2520in%2520each%2520MoE%2520layer%2520and%2520adopts%2520a%2520holistic%250Aapproach%2520to%2520overlap%2520communication%2520with%2520computation%2520at%2520both%2520inter-%2520and%250Aintra-operator%2520levels.%2520Additionally%252C%2520MegaScale-MoE%2520applies%2520communication%250Acompression%2520with%2520adjusted%2520communication%2520patterns%2520to%2520lower%2520precision%252C%2520further%250Aimproving%2520training%2520efficiency.%2520When%2520training%2520a%2520352B%2520MoE%2520model%2520on%25201%252C440%2520NVIDIA%250AHopper%2520GPUs%252C%2520MegaScale-MoE%2520achieves%2520a%2520training%2520throughput%2520of%25201.41M%2520tokens/s%252C%250Aimproving%2520the%2520efficiency%2520by%25201.88%2524%255Ctimes%2524%2520compared%2520to%2520Megatron-LM.%2520We%2520share%2520our%250Aoperational%2520experience%2520in%2520accelerating%2520MoE%2520training%2520and%2520hope%2520that%2520by%2520offering%250Aour%2520insights%2520in%2520system%2520design%252C%2520this%2520work%2520will%2520motivate%2520future%2520research%2520in%2520MoE%250Asystems.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2505.11432v3%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=MegaScale-MoE%3A%20Large-Scale%20Communication-Efficient%20Training%20of%0A%20%20Mixture-of-Experts%20Models%20in%20Production&entry.906535625=Chao%20Jin%20and%20Ziheng%20Jiang%20and%20Zhihao%20Bai%20and%20Zheng%20Zhong%20and%20Juncai%20Liu%20and%20Xiang%20Li%20and%20Ningxin%20Zheng%20and%20Xi%20Wang%20and%20Cong%20Xie%20and%20Qi%20Huang%20and%20Wen%20Heng%20and%20Yiyuan%20Ma%20and%20Wenlei%20Bao%20and%20Size%20Zheng%20and%20Yanghua%20Peng%20and%20Haibin%20Lin%20and%20Xuanzhe%20Liu%20and%20Xin%20Jin%20and%20Xin%20Liu&entry.1292438233=%20%20We%20present%20MegaScale-MoE%2C%20a%20production%20system%20tailored%20for%20the%20efficient%0Atraining%20of%20large-scale%20mixture-of-experts%20%28MoE%29%20models.%20MoE%20emerges%20as%20a%0Apromising%20architecture%20to%20scale%20large%20language%20models%20%28LLMs%29%20to%20unprecedented%0Asizes%2C%20thereby%20enhancing%20model%20performance.%20However%2C%20existing%20MoE%20training%0Asystems%20experience%20a%20degradation%20in%20training%20efficiency%2C%20exacerbated%20by%20the%0Aescalating%20scale%20of%20MoE%20models%20and%20the%20continuous%20evolution%20of%20hardware.%0A%20%20Recognizing%20the%20pivotal%20role%20of%20efficient%20communication%20in%20enhancing%20MoE%0Atraining%2C%20MegaScale-MoE%20customizes%20communication-efficient%20parallelism%0Astrategies%20for%20attention%20and%20FFNs%20in%20each%20MoE%20layer%20and%20adopts%20a%20holistic%0Aapproach%20to%20overlap%20communication%20with%20computation%20at%20both%20inter-%20and%0Aintra-operator%20levels.%20Additionally%2C%20MegaScale-MoE%20applies%20communication%0Acompression%20with%20adjusted%20communication%20patterns%20to%20lower%20precision%2C%20further%0Aimproving%20training%20efficiency.%20When%20training%20a%20352B%20MoE%20model%20on%201%2C440%20NVIDIA%0AHopper%20GPUs%2C%20MegaScale-MoE%20achieves%20a%20training%20throughput%20of%201.41M%20tokens/s%2C%0Aimproving%20the%20efficiency%20by%201.88%24%5Ctimes%24%20compared%20to%20Megatron-LM.%20We%20share%20our%0Aoperational%20experience%20in%20accelerating%20MoE%20training%20and%20hope%20that%20by%20offering%0Aour%20insights%20in%20system%20design%2C%20this%20work%20will%20motivate%20future%20research%20in%20MoE%0Asystems.%0A&entry.1838667208=http%3A//arxiv.org/abs/2505.11432v3&entry.124074799=Read"},
{"title": "CarBoN: Calibrated Best-of-N Sampling Improves Test-time Reasoning", "author": "Yung-Chen Tang and Pin-Yu Chen and Andrea Cavallaro", "abstract": "  Allocating more computation during inference time (test-time scaling)\nimproves language model performance, especially for reasoning tasks. However,\npopular methods like Best-of-$N$ sampling often show diminishing returns as $N$\nincreases. To address this inefficiency, we introduce a general test-time\ncalibration framework that adaptively modifies the model toward high-reward\nreasoning paths, with theoretical guarantees of improving the lower bound of\nexpected reward under finite sampling, all without large language model (LLM)\nretraining. Within this framework, we propose CarBoN (Calibrated Best-of-$N$),\na two-phase method that first explores the solution space and then learns a\ncalibration of the logits via an input-specific temperature $T$ and additive\nshift vector $\\delta$, guiding generation toward more reliable reasoning.\nExperiments on MATH-500 and AIME-2024 show that CarBoN improves efficiency,\nwith up to $4\\times$ fewer rollouts to reach the same accuracy, while often\nachieving higher accuracy under fixed budgets. We also analyze the\ncomplementary roles of $T$ and $\\delta$ in balancing output diversity and\ncorrectness, and demonstrate that the framework also generalizes to step-level\nsampling strategies such as beam search. For more information, please refer to\nour project page at huggingface.co/spaces/TrustSafeAI/Test-Time-Calibration.\n", "link": "http://arxiv.org/abs/2510.15674v1", "date": "2025-10-17", "relevancy": 1.9444, "topK": [{"title": "Breadcrumbs: Adversarial class-balanced sampling for long-tailed recognition", "link": "https://link.springer.com/chapter/10.1007/978-3-031-20053-3_37", "similarity": 0.4979}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.4848}, {"title": "ToDo: Token Downsampling for Efficient Generation of High-Resolution\n  Images", "link": "http://arxiv.org/abs/2402.13573v2", "similarity": 0.4748}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20CarBoN%3A%20Calibrated%20Best-of-N%20Sampling%20Improves%20Test-time%20Reasoning&body=Title%3A%20CarBoN%3A%20Calibrated%20Best-of-N%20Sampling%20Improves%20Test-time%20Reasoning%0AAuthor%3A%20Yung-Chen%20Tang%20and%20Pin-Yu%20Chen%20and%20Andrea%20Cavallaro%0AAbstract%3A%20%20%20Allocating%20more%20computation%20during%20inference%20time%20%28test-time%20scaling%29%0Aimproves%20language%20model%20performance%2C%20especially%20for%20reasoning%20tasks.%20However%2C%0Apopular%20methods%20like%20Best-of-%24N%24%20sampling%20often%20show%20diminishing%20returns%20as%20%24N%24%0Aincreases.%20To%20address%20this%20inefficiency%2C%20we%20introduce%20a%20general%20test-time%0Acalibration%20framework%20that%20adaptively%20modifies%20the%20model%20toward%20high-reward%0Areasoning%20paths%2C%20with%20theoretical%20guarantees%20of%20improving%20the%20lower%20bound%20of%0Aexpected%20reward%20under%20finite%20sampling%2C%20all%20without%20large%20language%20model%20%28LLM%29%0Aretraining.%20Within%20this%20framework%2C%20we%20propose%20CarBoN%20%28Calibrated%20Best-of-%24N%24%29%2C%0Aa%20two-phase%20method%20that%20first%20explores%20the%20solution%20space%20and%20then%20learns%20a%0Acalibration%20of%20the%20logits%20via%20an%20input-specific%20temperature%20%24T%24%20and%20additive%0Ashift%20vector%20%24%5Cdelta%24%2C%20guiding%20generation%20toward%20more%20reliable%20reasoning.%0AExperiments%20on%20MATH-500%20and%20AIME-2024%20show%20that%20CarBoN%20improves%20efficiency%2C%0Awith%20up%20to%20%244%5Ctimes%24%20fewer%20rollouts%20to%20reach%20the%20same%20accuracy%2C%20while%20often%0Aachieving%20higher%20accuracy%20under%20fixed%20budgets.%20We%20also%20analyze%20the%0Acomplementary%20roles%20of%20%24T%24%20and%20%24%5Cdelta%24%20in%20balancing%20output%20diversity%20and%0Acorrectness%2C%20and%20demonstrate%20that%20the%20framework%20also%20generalizes%20to%20step-level%0Asampling%20strategies%20such%20as%20beam%20search.%20For%20more%20information%2C%20please%20refer%20to%0Aour%20project%20page%20at%20huggingface.co/spaces/TrustSafeAI/Test-Time-Calibration.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2510.15674v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DCarBoN%253A%2520Calibrated%2520Best-of-N%2520Sampling%2520Improves%2520Test-time%2520Reasoning%26entry.906535625%3DYung-Chen%2520Tang%2520and%2520Pin-Yu%2520Chen%2520and%2520Andrea%2520Cavallaro%26entry.1292438233%3D%2520%2520Allocating%2520more%2520computation%2520during%2520inference%2520time%2520%2528test-time%2520scaling%2529%250Aimproves%2520language%2520model%2520performance%252C%2520especially%2520for%2520reasoning%2520tasks.%2520However%252C%250Apopular%2520methods%2520like%2520Best-of-%2524N%2524%2520sampling%2520often%2520show%2520diminishing%2520returns%2520as%2520%2524N%2524%250Aincreases.%2520To%2520address%2520this%2520inefficiency%252C%2520we%2520introduce%2520a%2520general%2520test-time%250Acalibration%2520framework%2520that%2520adaptively%2520modifies%2520the%2520model%2520toward%2520high-reward%250Areasoning%2520paths%252C%2520with%2520theoretical%2520guarantees%2520of%2520improving%2520the%2520lower%2520bound%2520of%250Aexpected%2520reward%2520under%2520finite%2520sampling%252C%2520all%2520without%2520large%2520language%2520model%2520%2528LLM%2529%250Aretraining.%2520Within%2520this%2520framework%252C%2520we%2520propose%2520CarBoN%2520%2528Calibrated%2520Best-of-%2524N%2524%2529%252C%250Aa%2520two-phase%2520method%2520that%2520first%2520explores%2520the%2520solution%2520space%2520and%2520then%2520learns%2520a%250Acalibration%2520of%2520the%2520logits%2520via%2520an%2520input-specific%2520temperature%2520%2524T%2524%2520and%2520additive%250Ashift%2520vector%2520%2524%255Cdelta%2524%252C%2520guiding%2520generation%2520toward%2520more%2520reliable%2520reasoning.%250AExperiments%2520on%2520MATH-500%2520and%2520AIME-2024%2520show%2520that%2520CarBoN%2520improves%2520efficiency%252C%250Awith%2520up%2520to%2520%25244%255Ctimes%2524%2520fewer%2520rollouts%2520to%2520reach%2520the%2520same%2520accuracy%252C%2520while%2520often%250Aachieving%2520higher%2520accuracy%2520under%2520fixed%2520budgets.%2520We%2520also%2520analyze%2520the%250Acomplementary%2520roles%2520of%2520%2524T%2524%2520and%2520%2524%255Cdelta%2524%2520in%2520balancing%2520output%2520diversity%2520and%250Acorrectness%252C%2520and%2520demonstrate%2520that%2520the%2520framework%2520also%2520generalizes%2520to%2520step-level%250Asampling%2520strategies%2520such%2520as%2520beam%2520search.%2520For%2520more%2520information%252C%2520please%2520refer%2520to%250Aour%2520project%2520page%2520at%2520huggingface.co/spaces/TrustSafeAI/Test-Time-Calibration.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2510.15674v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=CarBoN%3A%20Calibrated%20Best-of-N%20Sampling%20Improves%20Test-time%20Reasoning&entry.906535625=Yung-Chen%20Tang%20and%20Pin-Yu%20Chen%20and%20Andrea%20Cavallaro&entry.1292438233=%20%20Allocating%20more%20computation%20during%20inference%20time%20%28test-time%20scaling%29%0Aimproves%20language%20model%20performance%2C%20especially%20for%20reasoning%20tasks.%20However%2C%0Apopular%20methods%20like%20Best-of-%24N%24%20sampling%20often%20show%20diminishing%20returns%20as%20%24N%24%0Aincreases.%20To%20address%20this%20inefficiency%2C%20we%20introduce%20a%20general%20test-time%0Acalibration%20framework%20that%20adaptively%20modifies%20the%20model%20toward%20high-reward%0Areasoning%20paths%2C%20with%20theoretical%20guarantees%20of%20improving%20the%20lower%20bound%20of%0Aexpected%20reward%20under%20finite%20sampling%2C%20all%20without%20large%20language%20model%20%28LLM%29%0Aretraining.%20Within%20this%20framework%2C%20we%20propose%20CarBoN%20%28Calibrated%20Best-of-%24N%24%29%2C%0Aa%20two-phase%20method%20that%20first%20explores%20the%20solution%20space%20and%20then%20learns%20a%0Acalibration%20of%20the%20logits%20via%20an%20input-specific%20temperature%20%24T%24%20and%20additive%0Ashift%20vector%20%24%5Cdelta%24%2C%20guiding%20generation%20toward%20more%20reliable%20reasoning.%0AExperiments%20on%20MATH-500%20and%20AIME-2024%20show%20that%20CarBoN%20improves%20efficiency%2C%0Awith%20up%20to%20%244%5Ctimes%24%20fewer%20rollouts%20to%20reach%20the%20same%20accuracy%2C%20while%20often%0Aachieving%20higher%20accuracy%20under%20fixed%20budgets.%20We%20also%20analyze%20the%0Acomplementary%20roles%20of%20%24T%24%20and%20%24%5Cdelta%24%20in%20balancing%20output%20diversity%20and%0Acorrectness%2C%20and%20demonstrate%20that%20the%20framework%20also%20generalizes%20to%20step-level%0Asampling%20strategies%20such%20as%20beam%20search.%20For%20more%20information%2C%20please%20refer%20to%0Aour%20project%20page%20at%20huggingface.co/spaces/TrustSafeAI/Test-Time-Calibration.%0A&entry.1838667208=http%3A//arxiv.org/abs/2510.15674v1&entry.124074799=Read"},
{"title": "Reinforcement Learning with Verifiable yet Noisy Rewards under Imperfect\n  Verifiers", "author": "Xin-Qiang Cai and Wei Wang and Feng Liu and Tongliang Liu and Gang Niu and Masashi Sugiyama", "abstract": "  Reinforcement Learning with Verifiable Rewards (RLVR) trains policies against\nautomated verifiers to avoid costly human labeling. To reduce vulnerability to\nverifier hacking, many RLVR systems collapse rewards to binary $\\{0,1\\}$ during\ntraining. This choice carries a cost: it introduces \\textit{false negatives}\n(rejecting correct answers, FNs) and \\textit{false positives} (accepting\nincorrect ones, FPs). For instance, a rule-based checker may mark the correct\nfraction $\\frac{12}{36}$ as wrong when compared against the canonical\n$\\frac{1}{3}$ due to brittle parsing/equivalence rules (FN), while a large\nlanguage model (LLM) judges can be gamed by superficial cues or even a single\nadversarial token, yielding inflated correctness for wrong solutions (FP). We\nformalize verifier unreliability by modeling the verifier as a stochastic\nreward channel with asymmetric noise rates. From this abstraction, we derive\ntwo correction algorithms for verifier errors. The first is a \\textit{backward}\ncorrection that de-biases the observed binary reward to recover an\n\\textit{unbiased} estimator of the clean policy gradient. The second is a\n\\textit{forward} correction that reweights score-function terms so that the\nexpected update direction aligns with the \\textit{clean gradient}; notably, it\nrequires only the FN rate. We implement both as lightweight hooks in a group\nrelative policy optimization (GRPO)-based RLVR pipeline and evaluate them on\nmath-reasoning models and benchmarks. Across models and datasets, both\ncorrections improve over uncorrected training; the forward variant converges\nfaster and remains stable under heavier noise. Finally, we show a practical\nappeal mechanism in which a lightweight LLM verifier estimates the FN rate\nonline by rechecking rule-based negatives, obtaining outperformance compared\nwith other state-of-the-art contenders.\n", "link": "http://arxiv.org/abs/2510.00915v2", "date": "2025-10-17", "relevancy": 1.939, "topK": [{"title": "Flexible Visual Recognition by Evidential Modeling of Confusion and Ignorance", "link": "http://openaccess.thecvf.com/content/ICCV2023/html/Fan_Flexible_Visual_Recognition_by_Evidential_Modeling_of_Confusion_and_Ignorance_ICCV_2023_paper.html", "similarity": 0.5215}, {"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.4865}, {"title": "Tuning-Free Noise Rectification for High Fidelity Image-to-Video\n  Generation", "link": "http://arxiv.org/abs/2403.02827v1", "similarity": 0.4473}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Reinforcement%20Learning%20with%20Verifiable%20yet%20Noisy%20Rewards%20under%20Imperfect%0A%20%20Verifiers&body=Title%3A%20Reinforcement%20Learning%20with%20Verifiable%20yet%20Noisy%20Rewards%20under%20Imperfect%0A%20%20Verifiers%0AAuthor%3A%20Xin-Qiang%20Cai%20and%20Wei%20Wang%20and%20Feng%20Liu%20and%20Tongliang%20Liu%20and%20Gang%20Niu%20and%20Masashi%20Sugiyama%0AAbstract%3A%20%20%20Reinforcement%20Learning%20with%20Verifiable%20Rewards%20%28RLVR%29%20trains%20policies%20against%0Aautomated%20verifiers%20to%20avoid%20costly%20human%20labeling.%20To%20reduce%20vulnerability%20to%0Averifier%20hacking%2C%20many%20RLVR%20systems%20collapse%20rewards%20to%20binary%20%24%5C%7B0%2C1%5C%7D%24%20during%0Atraining.%20This%20choice%20carries%20a%20cost%3A%20it%20introduces%20%5Ctextit%7Bfalse%20negatives%7D%0A%28rejecting%20correct%20answers%2C%20FNs%29%20and%20%5Ctextit%7Bfalse%20positives%7D%20%28accepting%0Aincorrect%20ones%2C%20FPs%29.%20For%20instance%2C%20a%20rule-based%20checker%20may%20mark%20the%20correct%0Afraction%20%24%5Cfrac%7B12%7D%7B36%7D%24%20as%20wrong%20when%20compared%20against%20the%20canonical%0A%24%5Cfrac%7B1%7D%7B3%7D%24%20due%20to%20brittle%20parsing/equivalence%20rules%20%28FN%29%2C%20while%20a%20large%0Alanguage%20model%20%28LLM%29%20judges%20can%20be%20gamed%20by%20superficial%20cues%20or%20even%20a%20single%0Aadversarial%20token%2C%20yielding%20inflated%20correctness%20for%20wrong%20solutions%20%28FP%29.%20We%0Aformalize%20verifier%20unreliability%20by%20modeling%20the%20verifier%20as%20a%20stochastic%0Areward%20channel%20with%20asymmetric%20noise%20rates.%20From%20this%20abstraction%2C%20we%20derive%0Atwo%20correction%20algorithms%20for%20verifier%20errors.%20The%20first%20is%20a%20%5Ctextit%7Bbackward%7D%0Acorrection%20that%20de-biases%20the%20observed%20binary%20reward%20to%20recover%20an%0A%5Ctextit%7Bunbiased%7D%20estimator%20of%20the%20clean%20policy%20gradient.%20The%20second%20is%20a%0A%5Ctextit%7Bforward%7D%20correction%20that%20reweights%20score-function%20terms%20so%20that%20the%0Aexpected%20update%20direction%20aligns%20with%20the%20%5Ctextit%7Bclean%20gradient%7D%3B%20notably%2C%20it%0Arequires%20only%20the%20FN%20rate.%20We%20implement%20both%20as%20lightweight%20hooks%20in%20a%20group%0Arelative%20policy%20optimization%20%28GRPO%29-based%20RLVR%20pipeline%20and%20evaluate%20them%20on%0Amath-reasoning%20models%20and%20benchmarks.%20Across%20models%20and%20datasets%2C%20both%0Acorrections%20improve%20over%20uncorrected%20training%3B%20the%20forward%20variant%20converges%0Afaster%20and%20remains%20stable%20under%20heavier%20noise.%20Finally%2C%20we%20show%20a%20practical%0Aappeal%20mechanism%20in%20which%20a%20lightweight%20LLM%20verifier%20estimates%20the%20FN%20rate%0Aonline%20by%20rechecking%20rule-based%20negatives%2C%20obtaining%20outperformance%20compared%0Awith%20other%20state-of-the-art%20contenders.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2510.00915v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DReinforcement%2520Learning%2520with%2520Verifiable%2520yet%2520Noisy%2520Rewards%2520under%2520Imperfect%250A%2520%2520Verifiers%26entry.906535625%3DXin-Qiang%2520Cai%2520and%2520Wei%2520Wang%2520and%2520Feng%2520Liu%2520and%2520Tongliang%2520Liu%2520and%2520Gang%2520Niu%2520and%2520Masashi%2520Sugiyama%26entry.1292438233%3D%2520%2520Reinforcement%2520Learning%2520with%2520Verifiable%2520Rewards%2520%2528RLVR%2529%2520trains%2520policies%2520against%250Aautomated%2520verifiers%2520to%2520avoid%2520costly%2520human%2520labeling.%2520To%2520reduce%2520vulnerability%2520to%250Averifier%2520hacking%252C%2520many%2520RLVR%2520systems%2520collapse%2520rewards%2520to%2520binary%2520%2524%255C%257B0%252C1%255C%257D%2524%2520during%250Atraining.%2520This%2520choice%2520carries%2520a%2520cost%253A%2520it%2520introduces%2520%255Ctextit%257Bfalse%2520negatives%257D%250A%2528rejecting%2520correct%2520answers%252C%2520FNs%2529%2520and%2520%255Ctextit%257Bfalse%2520positives%257D%2520%2528accepting%250Aincorrect%2520ones%252C%2520FPs%2529.%2520For%2520instance%252C%2520a%2520rule-based%2520checker%2520may%2520mark%2520the%2520correct%250Afraction%2520%2524%255Cfrac%257B12%257D%257B36%257D%2524%2520as%2520wrong%2520when%2520compared%2520against%2520the%2520canonical%250A%2524%255Cfrac%257B1%257D%257B3%257D%2524%2520due%2520to%2520brittle%2520parsing/equivalence%2520rules%2520%2528FN%2529%252C%2520while%2520a%2520large%250Alanguage%2520model%2520%2528LLM%2529%2520judges%2520can%2520be%2520gamed%2520by%2520superficial%2520cues%2520or%2520even%2520a%2520single%250Aadversarial%2520token%252C%2520yielding%2520inflated%2520correctness%2520for%2520wrong%2520solutions%2520%2528FP%2529.%2520We%250Aformalize%2520verifier%2520unreliability%2520by%2520modeling%2520the%2520verifier%2520as%2520a%2520stochastic%250Areward%2520channel%2520with%2520asymmetric%2520noise%2520rates.%2520From%2520this%2520abstraction%252C%2520we%2520derive%250Atwo%2520correction%2520algorithms%2520for%2520verifier%2520errors.%2520The%2520first%2520is%2520a%2520%255Ctextit%257Bbackward%257D%250Acorrection%2520that%2520de-biases%2520the%2520observed%2520binary%2520reward%2520to%2520recover%2520an%250A%255Ctextit%257Bunbiased%257D%2520estimator%2520of%2520the%2520clean%2520policy%2520gradient.%2520The%2520second%2520is%2520a%250A%255Ctextit%257Bforward%257D%2520correction%2520that%2520reweights%2520score-function%2520terms%2520so%2520that%2520the%250Aexpected%2520update%2520direction%2520aligns%2520with%2520the%2520%255Ctextit%257Bclean%2520gradient%257D%253B%2520notably%252C%2520it%250Arequires%2520only%2520the%2520FN%2520rate.%2520We%2520implement%2520both%2520as%2520lightweight%2520hooks%2520in%2520a%2520group%250Arelative%2520policy%2520optimization%2520%2528GRPO%2529-based%2520RLVR%2520pipeline%2520and%2520evaluate%2520them%2520on%250Amath-reasoning%2520models%2520and%2520benchmarks.%2520Across%2520models%2520and%2520datasets%252C%2520both%250Acorrections%2520improve%2520over%2520uncorrected%2520training%253B%2520the%2520forward%2520variant%2520converges%250Afaster%2520and%2520remains%2520stable%2520under%2520heavier%2520noise.%2520Finally%252C%2520we%2520show%2520a%2520practical%250Aappeal%2520mechanism%2520in%2520which%2520a%2520lightweight%2520LLM%2520verifier%2520estimates%2520the%2520FN%2520rate%250Aonline%2520by%2520rechecking%2520rule-based%2520negatives%252C%2520obtaining%2520outperformance%2520compared%250Awith%2520other%2520state-of-the-art%2520contenders.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2510.00915v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Reinforcement%20Learning%20with%20Verifiable%20yet%20Noisy%20Rewards%20under%20Imperfect%0A%20%20Verifiers&entry.906535625=Xin-Qiang%20Cai%20and%20Wei%20Wang%20and%20Feng%20Liu%20and%20Tongliang%20Liu%20and%20Gang%20Niu%20and%20Masashi%20Sugiyama&entry.1292438233=%20%20Reinforcement%20Learning%20with%20Verifiable%20Rewards%20%28RLVR%29%20trains%20policies%20against%0Aautomated%20verifiers%20to%20avoid%20costly%20human%20labeling.%20To%20reduce%20vulnerability%20to%0Averifier%20hacking%2C%20many%20RLVR%20systems%20collapse%20rewards%20to%20binary%20%24%5C%7B0%2C1%5C%7D%24%20during%0Atraining.%20This%20choice%20carries%20a%20cost%3A%20it%20introduces%20%5Ctextit%7Bfalse%20negatives%7D%0A%28rejecting%20correct%20answers%2C%20FNs%29%20and%20%5Ctextit%7Bfalse%20positives%7D%20%28accepting%0Aincorrect%20ones%2C%20FPs%29.%20For%20instance%2C%20a%20rule-based%20checker%20may%20mark%20the%20correct%0Afraction%20%24%5Cfrac%7B12%7D%7B36%7D%24%20as%20wrong%20when%20compared%20against%20the%20canonical%0A%24%5Cfrac%7B1%7D%7B3%7D%24%20due%20to%20brittle%20parsing/equivalence%20rules%20%28FN%29%2C%20while%20a%20large%0Alanguage%20model%20%28LLM%29%20judges%20can%20be%20gamed%20by%20superficial%20cues%20or%20even%20a%20single%0Aadversarial%20token%2C%20yielding%20inflated%20correctness%20for%20wrong%20solutions%20%28FP%29.%20We%0Aformalize%20verifier%20unreliability%20by%20modeling%20the%20verifier%20as%20a%20stochastic%0Areward%20channel%20with%20asymmetric%20noise%20rates.%20From%20this%20abstraction%2C%20we%20derive%0Atwo%20correction%20algorithms%20for%20verifier%20errors.%20The%20first%20is%20a%20%5Ctextit%7Bbackward%7D%0Acorrection%20that%20de-biases%20the%20observed%20binary%20reward%20to%20recover%20an%0A%5Ctextit%7Bunbiased%7D%20estimator%20of%20the%20clean%20policy%20gradient.%20The%20second%20is%20a%0A%5Ctextit%7Bforward%7D%20correction%20that%20reweights%20score-function%20terms%20so%20that%20the%0Aexpected%20update%20direction%20aligns%20with%20the%20%5Ctextit%7Bclean%20gradient%7D%3B%20notably%2C%20it%0Arequires%20only%20the%20FN%20rate.%20We%20implement%20both%20as%20lightweight%20hooks%20in%20a%20group%0Arelative%20policy%20optimization%20%28GRPO%29-based%20RLVR%20pipeline%20and%20evaluate%20them%20on%0Amath-reasoning%20models%20and%20benchmarks.%20Across%20models%20and%20datasets%2C%20both%0Acorrections%20improve%20over%20uncorrected%20training%3B%20the%20forward%20variant%20converges%0Afaster%20and%20remains%20stable%20under%20heavier%20noise.%20Finally%2C%20we%20show%20a%20practical%0Aappeal%20mechanism%20in%20which%20a%20lightweight%20LLM%20verifier%20estimates%20the%20FN%20rate%0Aonline%20by%20rechecking%20rule-based%20negatives%2C%20obtaining%20outperformance%20compared%0Awith%20other%20state-of-the-art%20contenders.%0A&entry.1838667208=http%3A//arxiv.org/abs/2510.00915v2&entry.124074799=Read"},
{"title": "Exploring the Synergy of Quantitative Factors and Newsflow\n  Representations from Large Language Models for Stock Return Prediction", "author": "Tian Guo and Emmanuel Hauptmann", "abstract": "  In quantitative investing, return prediction supports various tasks,\nincluding stock selection, portfolio optimization, and risk management.\nQuantitative factors, such as valuation, quality, and growth, capture various\ncharacteristics of stocks. Unstructured financial data, like news and\ntranscripts, has attracted growing attention, driven by recent advances in\nlarge language models (LLMs). This paper examines effective methods for\nleveraging multimodal factors and newsflow in return prediction and stock\nselection. First, we introduce a fusion learning framework to learn a unified\nrepresentation from factors and newsflow representations generated by an LLM.\nWithin this framework, we compare three representative methods: representation\ncombination, representation summation, and attentive representations. Next,\nbuilding on empirical observations from fusion learning, we explore the mixture\nmodel that adaptively combines predictions made by single modalities and their\nfusion. To mitigate the training instability observed in the mixture model, we\nintroduce a decoupled training approach with theoretical insights. Finally, our\nexperiments on real investment universes yield several insights into effective\nmultimodal modeling of factors and news for stock return prediction.\n", "link": "http://arxiv.org/abs/2510.15691v1", "date": "2025-10-17", "relevancy": 1.9385, "topK": [{"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.49}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.4835}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.4835}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Exploring%20the%20Synergy%20of%20Quantitative%20Factors%20and%20Newsflow%0A%20%20Representations%20from%20Large%20Language%20Models%20for%20Stock%20Return%20Prediction&body=Title%3A%20Exploring%20the%20Synergy%20of%20Quantitative%20Factors%20and%20Newsflow%0A%20%20Representations%20from%20Large%20Language%20Models%20for%20Stock%20Return%20Prediction%0AAuthor%3A%20Tian%20Guo%20and%20Emmanuel%20Hauptmann%0AAbstract%3A%20%20%20In%20quantitative%20investing%2C%20return%20prediction%20supports%20various%20tasks%2C%0Aincluding%20stock%20selection%2C%20portfolio%20optimization%2C%20and%20risk%20management.%0AQuantitative%20factors%2C%20such%20as%20valuation%2C%20quality%2C%20and%20growth%2C%20capture%20various%0Acharacteristics%20of%20stocks.%20Unstructured%20financial%20data%2C%20like%20news%20and%0Atranscripts%2C%20has%20attracted%20growing%20attention%2C%20driven%20by%20recent%20advances%20in%0Alarge%20language%20models%20%28LLMs%29.%20This%20paper%20examines%20effective%20methods%20for%0Aleveraging%20multimodal%20factors%20and%20newsflow%20in%20return%20prediction%20and%20stock%0Aselection.%20First%2C%20we%20introduce%20a%20fusion%20learning%20framework%20to%20learn%20a%20unified%0Arepresentation%20from%20factors%20and%20newsflow%20representations%20generated%20by%20an%20LLM.%0AWithin%20this%20framework%2C%20we%20compare%20three%20representative%20methods%3A%20representation%0Acombination%2C%20representation%20summation%2C%20and%20attentive%20representations.%20Next%2C%0Abuilding%20on%20empirical%20observations%20from%20fusion%20learning%2C%20we%20explore%20the%20mixture%0Amodel%20that%20adaptively%20combines%20predictions%20made%20by%20single%20modalities%20and%20their%0Afusion.%20To%20mitigate%20the%20training%20instability%20observed%20in%20the%20mixture%20model%2C%20we%0Aintroduce%20a%20decoupled%20training%20approach%20with%20theoretical%20insights.%20Finally%2C%20our%0Aexperiments%20on%20real%20investment%20universes%20yield%20several%20insights%20into%20effective%0Amultimodal%20modeling%20of%20factors%20and%20news%20for%20stock%20return%20prediction.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2510.15691v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DExploring%2520the%2520Synergy%2520of%2520Quantitative%2520Factors%2520and%2520Newsflow%250A%2520%2520Representations%2520from%2520Large%2520Language%2520Models%2520for%2520Stock%2520Return%2520Prediction%26entry.906535625%3DTian%2520Guo%2520and%2520Emmanuel%2520Hauptmann%26entry.1292438233%3D%2520%2520In%2520quantitative%2520investing%252C%2520return%2520prediction%2520supports%2520various%2520tasks%252C%250Aincluding%2520stock%2520selection%252C%2520portfolio%2520optimization%252C%2520and%2520risk%2520management.%250AQuantitative%2520factors%252C%2520such%2520as%2520valuation%252C%2520quality%252C%2520and%2520growth%252C%2520capture%2520various%250Acharacteristics%2520of%2520stocks.%2520Unstructured%2520financial%2520data%252C%2520like%2520news%2520and%250Atranscripts%252C%2520has%2520attracted%2520growing%2520attention%252C%2520driven%2520by%2520recent%2520advances%2520in%250Alarge%2520language%2520models%2520%2528LLMs%2529.%2520This%2520paper%2520examines%2520effective%2520methods%2520for%250Aleveraging%2520multimodal%2520factors%2520and%2520newsflow%2520in%2520return%2520prediction%2520and%2520stock%250Aselection.%2520First%252C%2520we%2520introduce%2520a%2520fusion%2520learning%2520framework%2520to%2520learn%2520a%2520unified%250Arepresentation%2520from%2520factors%2520and%2520newsflow%2520representations%2520generated%2520by%2520an%2520LLM.%250AWithin%2520this%2520framework%252C%2520we%2520compare%2520three%2520representative%2520methods%253A%2520representation%250Acombination%252C%2520representation%2520summation%252C%2520and%2520attentive%2520representations.%2520Next%252C%250Abuilding%2520on%2520empirical%2520observations%2520from%2520fusion%2520learning%252C%2520we%2520explore%2520the%2520mixture%250Amodel%2520that%2520adaptively%2520combines%2520predictions%2520made%2520by%2520single%2520modalities%2520and%2520their%250Afusion.%2520To%2520mitigate%2520the%2520training%2520instability%2520observed%2520in%2520the%2520mixture%2520model%252C%2520we%250Aintroduce%2520a%2520decoupled%2520training%2520approach%2520with%2520theoretical%2520insights.%2520Finally%252C%2520our%250Aexperiments%2520on%2520real%2520investment%2520universes%2520yield%2520several%2520insights%2520into%2520effective%250Amultimodal%2520modeling%2520of%2520factors%2520and%2520news%2520for%2520stock%2520return%2520prediction.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2510.15691v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Exploring%20the%20Synergy%20of%20Quantitative%20Factors%20and%20Newsflow%0A%20%20Representations%20from%20Large%20Language%20Models%20for%20Stock%20Return%20Prediction&entry.906535625=Tian%20Guo%20and%20Emmanuel%20Hauptmann&entry.1292438233=%20%20In%20quantitative%20investing%2C%20return%20prediction%20supports%20various%20tasks%2C%0Aincluding%20stock%20selection%2C%20portfolio%20optimization%2C%20and%20risk%20management.%0AQuantitative%20factors%2C%20such%20as%20valuation%2C%20quality%2C%20and%20growth%2C%20capture%20various%0Acharacteristics%20of%20stocks.%20Unstructured%20financial%20data%2C%20like%20news%20and%0Atranscripts%2C%20has%20attracted%20growing%20attention%2C%20driven%20by%20recent%20advances%20in%0Alarge%20language%20models%20%28LLMs%29.%20This%20paper%20examines%20effective%20methods%20for%0Aleveraging%20multimodal%20factors%20and%20newsflow%20in%20return%20prediction%20and%20stock%0Aselection.%20First%2C%20we%20introduce%20a%20fusion%20learning%20framework%20to%20learn%20a%20unified%0Arepresentation%20from%20factors%20and%20newsflow%20representations%20generated%20by%20an%20LLM.%0AWithin%20this%20framework%2C%20we%20compare%20three%20representative%20methods%3A%20representation%0Acombination%2C%20representation%20summation%2C%20and%20attentive%20representations.%20Next%2C%0Abuilding%20on%20empirical%20observations%20from%20fusion%20learning%2C%20we%20explore%20the%20mixture%0Amodel%20that%20adaptively%20combines%20predictions%20made%20by%20single%20modalities%20and%20their%0Afusion.%20To%20mitigate%20the%20training%20instability%20observed%20in%20the%20mixture%20model%2C%20we%0Aintroduce%20a%20decoupled%20training%20approach%20with%20theoretical%20insights.%20Finally%2C%20our%0Aexperiments%20on%20real%20investment%20universes%20yield%20several%20insights%20into%20effective%0Amultimodal%20modeling%20of%20factors%20and%20news%20for%20stock%20return%20prediction.%0A&entry.1838667208=http%3A//arxiv.org/abs/2510.15691v1&entry.124074799=Read"},
{"title": "Deep Neural ODE Operator Networks for PDEs", "author": "Ziqian Li and Kang Liu and Yongcun Song and Hangrui Yue and Enrique Zuazua", "abstract": "  Operator learning has emerged as a promising paradigm for developing\nefficient surrogate models to solve partial differential equations (PDEs).\nHowever, existing approaches often overlook the domain knowledge inherent in\nthe underlying PDEs and hence suffer from challenges in capturing temporal\ndynamics and generalization issues beyond training time frames. This paper\nintroduces a deep neural ordinary differential equation (ODE) operator network\nframework, termed NODE-ONet, to alleviate these limitations. The framework\nadopts an encoder-decoder architecture comprising three core components: an\nencoder that spatially discretizes input functions, a neural ODE capturing\nlatent temporal dynamics, and a decoder reconstructing solutions in physical\nspaces. Theoretically, error analysis for the encoder-decoder architecture is\ninvestigated. Computationally, we propose novel physics-encoded neural ODEs to\nincorporate PDE-specific physical properties. Such well-designed neural ODEs\nsignificantly reduce the framework's complexity while enhancing numerical\nefficiency, robustness, applicability, and generalization capacity. Numerical\nexperiments on nonlinear diffusion-reaction and Navier-Stokes equations\ndemonstrate high accuracy, computational efficiency, and prediction\ncapabilities beyond training time frames. Additionally, the framework's\nflexibility to accommodate diverse encoders/decoders and its ability to\ngeneralize across related PDE families further underscore its potential as a\nscalable, physics-encoded tool for scientific machine learning.\n", "link": "http://arxiv.org/abs/2510.15651v1", "date": "2025-10-17", "relevancy": 1.9146, "topK": [{"title": "Boosted dynamic neural networks", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/26302", "similarity": 0.5009}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.4694}, {"title": "DDM-NET: End-to-end learning of keypoint feature Detection, Description and Matching for 3D localization", "link": "https://arxiv.org/abs/2212.04575", "similarity": 0.46}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Deep%20Neural%20ODE%20Operator%20Networks%20for%20PDEs&body=Title%3A%20Deep%20Neural%20ODE%20Operator%20Networks%20for%20PDEs%0AAuthor%3A%20Ziqian%20Li%20and%20Kang%20Liu%20and%20Yongcun%20Song%20and%20Hangrui%20Yue%20and%20Enrique%20Zuazua%0AAbstract%3A%20%20%20Operator%20learning%20has%20emerged%20as%20a%20promising%20paradigm%20for%20developing%0Aefficient%20surrogate%20models%20to%20solve%20partial%20differential%20equations%20%28PDEs%29.%0AHowever%2C%20existing%20approaches%20often%20overlook%20the%20domain%20knowledge%20inherent%20in%0Athe%20underlying%20PDEs%20and%20hence%20suffer%20from%20challenges%20in%20capturing%20temporal%0Adynamics%20and%20generalization%20issues%20beyond%20training%20time%20frames.%20This%20paper%0Aintroduces%20a%20deep%20neural%20ordinary%20differential%20equation%20%28ODE%29%20operator%20network%0Aframework%2C%20termed%20NODE-ONet%2C%20to%20alleviate%20these%20limitations.%20The%20framework%0Aadopts%20an%20encoder-decoder%20architecture%20comprising%20three%20core%20components%3A%20an%0Aencoder%20that%20spatially%20discretizes%20input%20functions%2C%20a%20neural%20ODE%20capturing%0Alatent%20temporal%20dynamics%2C%20and%20a%20decoder%20reconstructing%20solutions%20in%20physical%0Aspaces.%20Theoretically%2C%20error%20analysis%20for%20the%20encoder-decoder%20architecture%20is%0Ainvestigated.%20Computationally%2C%20we%20propose%20novel%20physics-encoded%20neural%20ODEs%20to%0Aincorporate%20PDE-specific%20physical%20properties.%20Such%20well-designed%20neural%20ODEs%0Asignificantly%20reduce%20the%20framework%27s%20complexity%20while%20enhancing%20numerical%0Aefficiency%2C%20robustness%2C%20applicability%2C%20and%20generalization%20capacity.%20Numerical%0Aexperiments%20on%20nonlinear%20diffusion-reaction%20and%20Navier-Stokes%20equations%0Ademonstrate%20high%20accuracy%2C%20computational%20efficiency%2C%20and%20prediction%0Acapabilities%20beyond%20training%20time%20frames.%20Additionally%2C%20the%20framework%27s%0Aflexibility%20to%20accommodate%20diverse%20encoders/decoders%20and%20its%20ability%20to%0Ageneralize%20across%20related%20PDE%20families%20further%20underscore%20its%20potential%20as%20a%0Ascalable%2C%20physics-encoded%20tool%20for%20scientific%20machine%20learning.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2510.15651v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DDeep%2520Neural%2520ODE%2520Operator%2520Networks%2520for%2520PDEs%26entry.906535625%3DZiqian%2520Li%2520and%2520Kang%2520Liu%2520and%2520Yongcun%2520Song%2520and%2520Hangrui%2520Yue%2520and%2520Enrique%2520Zuazua%26entry.1292438233%3D%2520%2520Operator%2520learning%2520has%2520emerged%2520as%2520a%2520promising%2520paradigm%2520for%2520developing%250Aefficient%2520surrogate%2520models%2520to%2520solve%2520partial%2520differential%2520equations%2520%2528PDEs%2529.%250AHowever%252C%2520existing%2520approaches%2520often%2520overlook%2520the%2520domain%2520knowledge%2520inherent%2520in%250Athe%2520underlying%2520PDEs%2520and%2520hence%2520suffer%2520from%2520challenges%2520in%2520capturing%2520temporal%250Adynamics%2520and%2520generalization%2520issues%2520beyond%2520training%2520time%2520frames.%2520This%2520paper%250Aintroduces%2520a%2520deep%2520neural%2520ordinary%2520differential%2520equation%2520%2528ODE%2529%2520operator%2520network%250Aframework%252C%2520termed%2520NODE-ONet%252C%2520to%2520alleviate%2520these%2520limitations.%2520The%2520framework%250Aadopts%2520an%2520encoder-decoder%2520architecture%2520comprising%2520three%2520core%2520components%253A%2520an%250Aencoder%2520that%2520spatially%2520discretizes%2520input%2520functions%252C%2520a%2520neural%2520ODE%2520capturing%250Alatent%2520temporal%2520dynamics%252C%2520and%2520a%2520decoder%2520reconstructing%2520solutions%2520in%2520physical%250Aspaces.%2520Theoretically%252C%2520error%2520analysis%2520for%2520the%2520encoder-decoder%2520architecture%2520is%250Ainvestigated.%2520Computationally%252C%2520we%2520propose%2520novel%2520physics-encoded%2520neural%2520ODEs%2520to%250Aincorporate%2520PDE-specific%2520physical%2520properties.%2520Such%2520well-designed%2520neural%2520ODEs%250Asignificantly%2520reduce%2520the%2520framework%2527s%2520complexity%2520while%2520enhancing%2520numerical%250Aefficiency%252C%2520robustness%252C%2520applicability%252C%2520and%2520generalization%2520capacity.%2520Numerical%250Aexperiments%2520on%2520nonlinear%2520diffusion-reaction%2520and%2520Navier-Stokes%2520equations%250Ademonstrate%2520high%2520accuracy%252C%2520computational%2520efficiency%252C%2520and%2520prediction%250Acapabilities%2520beyond%2520training%2520time%2520frames.%2520Additionally%252C%2520the%2520framework%2527s%250Aflexibility%2520to%2520accommodate%2520diverse%2520encoders/decoders%2520and%2520its%2520ability%2520to%250Ageneralize%2520across%2520related%2520PDE%2520families%2520further%2520underscore%2520its%2520potential%2520as%2520a%250Ascalable%252C%2520physics-encoded%2520tool%2520for%2520scientific%2520machine%2520learning.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2510.15651v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Deep%20Neural%20ODE%20Operator%20Networks%20for%20PDEs&entry.906535625=Ziqian%20Li%20and%20Kang%20Liu%20and%20Yongcun%20Song%20and%20Hangrui%20Yue%20and%20Enrique%20Zuazua&entry.1292438233=%20%20Operator%20learning%20has%20emerged%20as%20a%20promising%20paradigm%20for%20developing%0Aefficient%20surrogate%20models%20to%20solve%20partial%20differential%20equations%20%28PDEs%29.%0AHowever%2C%20existing%20approaches%20often%20overlook%20the%20domain%20knowledge%20inherent%20in%0Athe%20underlying%20PDEs%20and%20hence%20suffer%20from%20challenges%20in%20capturing%20temporal%0Adynamics%20and%20generalization%20issues%20beyond%20training%20time%20frames.%20This%20paper%0Aintroduces%20a%20deep%20neural%20ordinary%20differential%20equation%20%28ODE%29%20operator%20network%0Aframework%2C%20termed%20NODE-ONet%2C%20to%20alleviate%20these%20limitations.%20The%20framework%0Aadopts%20an%20encoder-decoder%20architecture%20comprising%20three%20core%20components%3A%20an%0Aencoder%20that%20spatially%20discretizes%20input%20functions%2C%20a%20neural%20ODE%20capturing%0Alatent%20temporal%20dynamics%2C%20and%20a%20decoder%20reconstructing%20solutions%20in%20physical%0Aspaces.%20Theoretically%2C%20error%20analysis%20for%20the%20encoder-decoder%20architecture%20is%0Ainvestigated.%20Computationally%2C%20we%20propose%20novel%20physics-encoded%20neural%20ODEs%20to%0Aincorporate%20PDE-specific%20physical%20properties.%20Such%20well-designed%20neural%20ODEs%0Asignificantly%20reduce%20the%20framework%27s%20complexity%20while%20enhancing%20numerical%0Aefficiency%2C%20robustness%2C%20applicability%2C%20and%20generalization%20capacity.%20Numerical%0Aexperiments%20on%20nonlinear%20diffusion-reaction%20and%20Navier-Stokes%20equations%0Ademonstrate%20high%20accuracy%2C%20computational%20efficiency%2C%20and%20prediction%0Acapabilities%20beyond%20training%20time%20frames.%20Additionally%2C%20the%20framework%27s%0Aflexibility%20to%20accommodate%20diverse%20encoders/decoders%20and%20its%20ability%20to%0Ageneralize%20across%20related%20PDE%20families%20further%20underscore%20its%20potential%20as%20a%0Ascalable%2C%20physics-encoded%20tool%20for%20scientific%20machine%20learning.%0A&entry.1838667208=http%3A//arxiv.org/abs/2510.15651v1&entry.124074799=Read"},
{"title": "LLMs Judge Themselves: A Game-Theoretic Framework for Human-Aligned\n  Evaluation", "author": "Gao Yang and Yuhang Liu and Siyu Miao and Xinyue Liang and Zhengyang Liu and Heyan Huang", "abstract": "  Ideal or real - that is the question.In this work, we explore whether\nprinciples from game theory can be effectively applied to the evaluation of\nlarge language models (LLMs). This inquiry is motivated by the growing\ninadequacy of conventional evaluation practices, which often rely on\nfixed-format tasks with reference answers and struggle to capture the nuanced,\nsubjective, and open-ended nature of modern LLM behavior. To address these\nchallenges, we propose a novel alternative: automatic mutual evaluation, where\nLLMs assess each other's output through self-play and peer review. These peer\nassessments are then systematically compared with human voting behavior to\nevaluate their alignment with human judgment. Our framework incorporates\ngame-theoretic voting algorithms to aggregate peer reviews, enabling a\nprincipled investigation into whether model-generated rankings reflect human\npreferences. Empirical results reveal both convergences and divergences between\ntheoretical predictions and human evaluations, offering valuable insights into\nthe promises and limitations of mutual evaluation. To the best of our\nknowledge, this is the first work to jointly integrate mutual evaluation,\ngame-theoretic aggregation, and human-grounded validation for evaluating the\ncapabilities of LLMs.\n", "link": "http://arxiv.org/abs/2510.15746v1", "date": "2025-10-17", "relevancy": 1.9139, "topK": [{"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.4998}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.4742}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.4742}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20LLMs%20Judge%20Themselves%3A%20A%20Game-Theoretic%20Framework%20for%20Human-Aligned%0A%20%20Evaluation&body=Title%3A%20LLMs%20Judge%20Themselves%3A%20A%20Game-Theoretic%20Framework%20for%20Human-Aligned%0A%20%20Evaluation%0AAuthor%3A%20Gao%20Yang%20and%20Yuhang%20Liu%20and%20Siyu%20Miao%20and%20Xinyue%20Liang%20and%20Zhengyang%20Liu%20and%20Heyan%20Huang%0AAbstract%3A%20%20%20Ideal%20or%20real%20-%20that%20is%20the%20question.In%20this%20work%2C%20we%20explore%20whether%0Aprinciples%20from%20game%20theory%20can%20be%20effectively%20applied%20to%20the%20evaluation%20of%0Alarge%20language%20models%20%28LLMs%29.%20This%20inquiry%20is%20motivated%20by%20the%20growing%0Ainadequacy%20of%20conventional%20evaluation%20practices%2C%20which%20often%20rely%20on%0Afixed-format%20tasks%20with%20reference%20answers%20and%20struggle%20to%20capture%20the%20nuanced%2C%0Asubjective%2C%20and%20open-ended%20nature%20of%20modern%20LLM%20behavior.%20To%20address%20these%0Achallenges%2C%20we%20propose%20a%20novel%20alternative%3A%20automatic%20mutual%20evaluation%2C%20where%0ALLMs%20assess%20each%20other%27s%20output%20through%20self-play%20and%20peer%20review.%20These%20peer%0Aassessments%20are%20then%20systematically%20compared%20with%20human%20voting%20behavior%20to%0Aevaluate%20their%20alignment%20with%20human%20judgment.%20Our%20framework%20incorporates%0Agame-theoretic%20voting%20algorithms%20to%20aggregate%20peer%20reviews%2C%20enabling%20a%0Aprincipled%20investigation%20into%20whether%20model-generated%20rankings%20reflect%20human%0Apreferences.%20Empirical%20results%20reveal%20both%20convergences%20and%20divergences%20between%0Atheoretical%20predictions%20and%20human%20evaluations%2C%20offering%20valuable%20insights%20into%0Athe%20promises%20and%20limitations%20of%20mutual%20evaluation.%20To%20the%20best%20of%20our%0Aknowledge%2C%20this%20is%20the%20first%20work%20to%20jointly%20integrate%20mutual%20evaluation%2C%0Agame-theoretic%20aggregation%2C%20and%20human-grounded%20validation%20for%20evaluating%20the%0Acapabilities%20of%20LLMs.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2510.15746v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DLLMs%2520Judge%2520Themselves%253A%2520A%2520Game-Theoretic%2520Framework%2520for%2520Human-Aligned%250A%2520%2520Evaluation%26entry.906535625%3DGao%2520Yang%2520and%2520Yuhang%2520Liu%2520and%2520Siyu%2520Miao%2520and%2520Xinyue%2520Liang%2520and%2520Zhengyang%2520Liu%2520and%2520Heyan%2520Huang%26entry.1292438233%3D%2520%2520Ideal%2520or%2520real%2520-%2520that%2520is%2520the%2520question.In%2520this%2520work%252C%2520we%2520explore%2520whether%250Aprinciples%2520from%2520game%2520theory%2520can%2520be%2520effectively%2520applied%2520to%2520the%2520evaluation%2520of%250Alarge%2520language%2520models%2520%2528LLMs%2529.%2520This%2520inquiry%2520is%2520motivated%2520by%2520the%2520growing%250Ainadequacy%2520of%2520conventional%2520evaluation%2520practices%252C%2520which%2520often%2520rely%2520on%250Afixed-format%2520tasks%2520with%2520reference%2520answers%2520and%2520struggle%2520to%2520capture%2520the%2520nuanced%252C%250Asubjective%252C%2520and%2520open-ended%2520nature%2520of%2520modern%2520LLM%2520behavior.%2520To%2520address%2520these%250Achallenges%252C%2520we%2520propose%2520a%2520novel%2520alternative%253A%2520automatic%2520mutual%2520evaluation%252C%2520where%250ALLMs%2520assess%2520each%2520other%2527s%2520output%2520through%2520self-play%2520and%2520peer%2520review.%2520These%2520peer%250Aassessments%2520are%2520then%2520systematically%2520compared%2520with%2520human%2520voting%2520behavior%2520to%250Aevaluate%2520their%2520alignment%2520with%2520human%2520judgment.%2520Our%2520framework%2520incorporates%250Agame-theoretic%2520voting%2520algorithms%2520to%2520aggregate%2520peer%2520reviews%252C%2520enabling%2520a%250Aprincipled%2520investigation%2520into%2520whether%2520model-generated%2520rankings%2520reflect%2520human%250Apreferences.%2520Empirical%2520results%2520reveal%2520both%2520convergences%2520and%2520divergences%2520between%250Atheoretical%2520predictions%2520and%2520human%2520evaluations%252C%2520offering%2520valuable%2520insights%2520into%250Athe%2520promises%2520and%2520limitations%2520of%2520mutual%2520evaluation.%2520To%2520the%2520best%2520of%2520our%250Aknowledge%252C%2520this%2520is%2520the%2520first%2520work%2520to%2520jointly%2520integrate%2520mutual%2520evaluation%252C%250Agame-theoretic%2520aggregation%252C%2520and%2520human-grounded%2520validation%2520for%2520evaluating%2520the%250Acapabilities%2520of%2520LLMs.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2510.15746v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=LLMs%20Judge%20Themselves%3A%20A%20Game-Theoretic%20Framework%20for%20Human-Aligned%0A%20%20Evaluation&entry.906535625=Gao%20Yang%20and%20Yuhang%20Liu%20and%20Siyu%20Miao%20and%20Xinyue%20Liang%20and%20Zhengyang%20Liu%20and%20Heyan%20Huang&entry.1292438233=%20%20Ideal%20or%20real%20-%20that%20is%20the%20question.In%20this%20work%2C%20we%20explore%20whether%0Aprinciples%20from%20game%20theory%20can%20be%20effectively%20applied%20to%20the%20evaluation%20of%0Alarge%20language%20models%20%28LLMs%29.%20This%20inquiry%20is%20motivated%20by%20the%20growing%0Ainadequacy%20of%20conventional%20evaluation%20practices%2C%20which%20often%20rely%20on%0Afixed-format%20tasks%20with%20reference%20answers%20and%20struggle%20to%20capture%20the%20nuanced%2C%0Asubjective%2C%20and%20open-ended%20nature%20of%20modern%20LLM%20behavior.%20To%20address%20these%0Achallenges%2C%20we%20propose%20a%20novel%20alternative%3A%20automatic%20mutual%20evaluation%2C%20where%0ALLMs%20assess%20each%20other%27s%20output%20through%20self-play%20and%20peer%20review.%20These%20peer%0Aassessments%20are%20then%20systematically%20compared%20with%20human%20voting%20behavior%20to%0Aevaluate%20their%20alignment%20with%20human%20judgment.%20Our%20framework%20incorporates%0Agame-theoretic%20voting%20algorithms%20to%20aggregate%20peer%20reviews%2C%20enabling%20a%0Aprincipled%20investigation%20into%20whether%20model-generated%20rankings%20reflect%20human%0Apreferences.%20Empirical%20results%20reveal%20both%20convergences%20and%20divergences%20between%0Atheoretical%20predictions%20and%20human%20evaluations%2C%20offering%20valuable%20insights%20into%0Athe%20promises%20and%20limitations%20of%20mutual%20evaluation.%20To%20the%20best%20of%20our%0Aknowledge%2C%20this%20is%20the%20first%20work%20to%20jointly%20integrate%20mutual%20evaluation%2C%0Agame-theoretic%20aggregation%2C%20and%20human-grounded%20validation%20for%20evaluating%20the%0Acapabilities%20of%20LLMs.%0A&entry.1838667208=http%3A//arxiv.org/abs/2510.15746v1&entry.124074799=Read"},
{"title": "All Roads Lead to Likelihood: The Value of Reinforcement Learning in\n  Fine-Tuning", "author": "Gokul Swamy and Sanjiban Choudhury and Wen Sun and Zhiwei Steven Wu and J. Andrew Bagnell", "abstract": "  From a first-principles perspective, it may seem odd that the strongest\nresults in foundation model fine-tuning (FT) are achieved via a relatively\ncomplex, two-stage training procedure. Specifically, one first trains a reward\nmodel (RM) on some dataset (e.g., human preferences) before using it to provide\nonline feedback as part of a downstream reinforcement learning (RL) procedure,\nrather than directly optimizing the policy parameters on said dataset via\noffline maximum likelihood estimation. In fact, from an information-theoretic\nperspective, we can only lose information via passing through a reward model\nand cannot create any new information via on-policy sampling. To explain this\ndiscrepancy, we scrutinize several hypotheses on the value of RL in FT through\nboth theoretical and empirical lenses. Of the hypotheses considered, we find\nthe most support for the explanation that on problems with a\ngeneration-verification gap, (1) it is relatively easy to learn the relatively\nsimple RM (verifier) from the preference data. Then, (2) the downstream RL\nprocedure only returns policies (generators) that are optimal for such\nrelatively simple verifiers. Thus, end-to-end, two-stage online FT only has to\nsearch over a reduced subset of the full space of policies, requiring less data\nthan offline FT.\n", "link": "http://arxiv.org/abs/2503.01067v2", "date": "2025-10-17", "relevancy": 1.9037, "topK": [{"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.491}, {"title": "Safe and efficient training of a control agent", "link": "https://patents.google.com/patent/US11709462B2/en", "similarity": 0.4805}, {"title": "ToDo: Token Downsampling for Efficient Generation of High-Resolution\n  Images", "link": "http://arxiv.org/abs/2402.13573v2", "similarity": 0.4653}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20All%20Roads%20Lead%20to%20Likelihood%3A%20The%20Value%20of%20Reinforcement%20Learning%20in%0A%20%20Fine-Tuning&body=Title%3A%20All%20Roads%20Lead%20to%20Likelihood%3A%20The%20Value%20of%20Reinforcement%20Learning%20in%0A%20%20Fine-Tuning%0AAuthor%3A%20Gokul%20Swamy%20and%20Sanjiban%20Choudhury%20and%20Wen%20Sun%20and%20Zhiwei%20Steven%20Wu%20and%20J.%20Andrew%20Bagnell%0AAbstract%3A%20%20%20From%20a%20first-principles%20perspective%2C%20it%20may%20seem%20odd%20that%20the%20strongest%0Aresults%20in%20foundation%20model%20fine-tuning%20%28FT%29%20are%20achieved%20via%20a%20relatively%0Acomplex%2C%20two-stage%20training%20procedure.%20Specifically%2C%20one%20first%20trains%20a%20reward%0Amodel%20%28RM%29%20on%20some%20dataset%20%28e.g.%2C%20human%20preferences%29%20before%20using%20it%20to%20provide%0Aonline%20feedback%20as%20part%20of%20a%20downstream%20reinforcement%20learning%20%28RL%29%20procedure%2C%0Arather%20than%20directly%20optimizing%20the%20policy%20parameters%20on%20said%20dataset%20via%0Aoffline%20maximum%20likelihood%20estimation.%20In%20fact%2C%20from%20an%20information-theoretic%0Aperspective%2C%20we%20can%20only%20lose%20information%20via%20passing%20through%20a%20reward%20model%0Aand%20cannot%20create%20any%20new%20information%20via%20on-policy%20sampling.%20To%20explain%20this%0Adiscrepancy%2C%20we%20scrutinize%20several%20hypotheses%20on%20the%20value%20of%20RL%20in%20FT%20through%0Aboth%20theoretical%20and%20empirical%20lenses.%20Of%20the%20hypotheses%20considered%2C%20we%20find%0Athe%20most%20support%20for%20the%20explanation%20that%20on%20problems%20with%20a%0Ageneration-verification%20gap%2C%20%281%29%20it%20is%20relatively%20easy%20to%20learn%20the%20relatively%0Asimple%20RM%20%28verifier%29%20from%20the%20preference%20data.%20Then%2C%20%282%29%20the%20downstream%20RL%0Aprocedure%20only%20returns%20policies%20%28generators%29%20that%20are%20optimal%20for%20such%0Arelatively%20simple%20verifiers.%20Thus%2C%20end-to-end%2C%20two-stage%20online%20FT%20only%20has%20to%0Asearch%20over%20a%20reduced%20subset%20of%20the%20full%20space%20of%20policies%2C%20requiring%20less%20data%0Athan%20offline%20FT.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2503.01067v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DAll%2520Roads%2520Lead%2520to%2520Likelihood%253A%2520The%2520Value%2520of%2520Reinforcement%2520Learning%2520in%250A%2520%2520Fine-Tuning%26entry.906535625%3DGokul%2520Swamy%2520and%2520Sanjiban%2520Choudhury%2520and%2520Wen%2520Sun%2520and%2520Zhiwei%2520Steven%2520Wu%2520and%2520J.%2520Andrew%2520Bagnell%26entry.1292438233%3D%2520%2520From%2520a%2520first-principles%2520perspective%252C%2520it%2520may%2520seem%2520odd%2520that%2520the%2520strongest%250Aresults%2520in%2520foundation%2520model%2520fine-tuning%2520%2528FT%2529%2520are%2520achieved%2520via%2520a%2520relatively%250Acomplex%252C%2520two-stage%2520training%2520procedure.%2520Specifically%252C%2520one%2520first%2520trains%2520a%2520reward%250Amodel%2520%2528RM%2529%2520on%2520some%2520dataset%2520%2528e.g.%252C%2520human%2520preferences%2529%2520before%2520using%2520it%2520to%2520provide%250Aonline%2520feedback%2520as%2520part%2520of%2520a%2520downstream%2520reinforcement%2520learning%2520%2528RL%2529%2520procedure%252C%250Arather%2520than%2520directly%2520optimizing%2520the%2520policy%2520parameters%2520on%2520said%2520dataset%2520via%250Aoffline%2520maximum%2520likelihood%2520estimation.%2520In%2520fact%252C%2520from%2520an%2520information-theoretic%250Aperspective%252C%2520we%2520can%2520only%2520lose%2520information%2520via%2520passing%2520through%2520a%2520reward%2520model%250Aand%2520cannot%2520create%2520any%2520new%2520information%2520via%2520on-policy%2520sampling.%2520To%2520explain%2520this%250Adiscrepancy%252C%2520we%2520scrutinize%2520several%2520hypotheses%2520on%2520the%2520value%2520of%2520RL%2520in%2520FT%2520through%250Aboth%2520theoretical%2520and%2520empirical%2520lenses.%2520Of%2520the%2520hypotheses%2520considered%252C%2520we%2520find%250Athe%2520most%2520support%2520for%2520the%2520explanation%2520that%2520on%2520problems%2520with%2520a%250Ageneration-verification%2520gap%252C%2520%25281%2529%2520it%2520is%2520relatively%2520easy%2520to%2520learn%2520the%2520relatively%250Asimple%2520RM%2520%2528verifier%2529%2520from%2520the%2520preference%2520data.%2520Then%252C%2520%25282%2529%2520the%2520downstream%2520RL%250Aprocedure%2520only%2520returns%2520policies%2520%2528generators%2529%2520that%2520are%2520optimal%2520for%2520such%250Arelatively%2520simple%2520verifiers.%2520Thus%252C%2520end-to-end%252C%2520two-stage%2520online%2520FT%2520only%2520has%2520to%250Asearch%2520over%2520a%2520reduced%2520subset%2520of%2520the%2520full%2520space%2520of%2520policies%252C%2520requiring%2520less%2520data%250Athan%2520offline%2520FT.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2503.01067v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=All%20Roads%20Lead%20to%20Likelihood%3A%20The%20Value%20of%20Reinforcement%20Learning%20in%0A%20%20Fine-Tuning&entry.906535625=Gokul%20Swamy%20and%20Sanjiban%20Choudhury%20and%20Wen%20Sun%20and%20Zhiwei%20Steven%20Wu%20and%20J.%20Andrew%20Bagnell&entry.1292438233=%20%20From%20a%20first-principles%20perspective%2C%20it%20may%20seem%20odd%20that%20the%20strongest%0Aresults%20in%20foundation%20model%20fine-tuning%20%28FT%29%20are%20achieved%20via%20a%20relatively%0Acomplex%2C%20two-stage%20training%20procedure.%20Specifically%2C%20one%20first%20trains%20a%20reward%0Amodel%20%28RM%29%20on%20some%20dataset%20%28e.g.%2C%20human%20preferences%29%20before%20using%20it%20to%20provide%0Aonline%20feedback%20as%20part%20of%20a%20downstream%20reinforcement%20learning%20%28RL%29%20procedure%2C%0Arather%20than%20directly%20optimizing%20the%20policy%20parameters%20on%20said%20dataset%20via%0Aoffline%20maximum%20likelihood%20estimation.%20In%20fact%2C%20from%20an%20information-theoretic%0Aperspective%2C%20we%20can%20only%20lose%20information%20via%20passing%20through%20a%20reward%20model%0Aand%20cannot%20create%20any%20new%20information%20via%20on-policy%20sampling.%20To%20explain%20this%0Adiscrepancy%2C%20we%20scrutinize%20several%20hypotheses%20on%20the%20value%20of%20RL%20in%20FT%20through%0Aboth%20theoretical%20and%20empirical%20lenses.%20Of%20the%20hypotheses%20considered%2C%20we%20find%0Athe%20most%20support%20for%20the%20explanation%20that%20on%20problems%20with%20a%0Ageneration-verification%20gap%2C%20%281%29%20it%20is%20relatively%20easy%20to%20learn%20the%20relatively%0Asimple%20RM%20%28verifier%29%20from%20the%20preference%20data.%20Then%2C%20%282%29%20the%20downstream%20RL%0Aprocedure%20only%20returns%20policies%20%28generators%29%20that%20are%20optimal%20for%20such%0Arelatively%20simple%20verifiers.%20Thus%2C%20end-to-end%2C%20two-stage%20online%20FT%20only%20has%20to%0Asearch%20over%20a%20reduced%20subset%20of%20the%20full%20space%20of%20policies%2C%20requiring%20less%20data%0Athan%20offline%20FT.%0A&entry.1838667208=http%3A//arxiv.org/abs/2503.01067v2&entry.124074799=Read"},
{"title": "LeMat-Traj: A Scalable and Unified Dataset of Materials Trajectories for\n  Atomistic Modeling", "author": "Ali Ramlaoui and Martin Siron and Inel Djafar and Joseph Musielewicz and Amandine Rossello and Victor Schmidt and Alexandre Duval", "abstract": "  The development of accurate machine learning interatomic potentials (MLIPs)\nis limited by the fragmented availability and inconsistent formatting of\nquantum mechanical trajectory datasets derived from Density Functional Theory\n(DFT). These datasets are expensive to generate yet difficult to combine due to\nvariations in format, metadata, and accessibility. To address this, we\nintroduce LeMat-Traj, a curated dataset comprising over 120 million atomic\nconfigurations aggregated from large-scale repositories, including the\nMaterials Project, Alexandria, and OQMD. LeMat-Traj standardizes data\nrepresentation, harmonizes results and filters for high-quality configurations\nacross widely used DFT functionals (PBE, PBESol, SCAN, r2SCAN). It\nsignificantly lowers the barrier for training transferrable and accurate MLIPs.\nLeMat-Traj spans both relaxed low-energy states and high-energy, high-force\nstructures, complementing molecular dynamics and active learning datasets. By\nfine-tuning models pre-trained on high-force data with LeMat-Traj, we achieve a\nsignificant reduction in force prediction errors on relaxation tasks. We also\npresent LeMaterial-Fetcher, a modular and extensible open-source library\ndeveloped for this work, designed to provide a reproducible framework for the\ncommunity to easily incorporate new data sources and ensure the continued\nevolution of large-scale materials datasets. LeMat-Traj and LeMaterial-Fetcher\nare publicly available at https://huggingface.co/datasets/LeMaterial/LeMat-Traj\nand https://github.com/LeMaterial/lematerial-fetcher.\n", "link": "http://arxiv.org/abs/2508.20875v2", "date": "2025-10-17", "relevancy": 1.8958, "topK": [{"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.4888}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.4677}, {"title": "Dynamic Gaussians Mesh: Consistent Mesh Reconstruction from Monocular\n  Videos", "link": "http://arxiv.org/abs/2404.12379v2", "similarity": 0.4616}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20LeMat-Traj%3A%20A%20Scalable%20and%20Unified%20Dataset%20of%20Materials%20Trajectories%20for%0A%20%20Atomistic%20Modeling&body=Title%3A%20LeMat-Traj%3A%20A%20Scalable%20and%20Unified%20Dataset%20of%20Materials%20Trajectories%20for%0A%20%20Atomistic%20Modeling%0AAuthor%3A%20Ali%20Ramlaoui%20and%20Martin%20Siron%20and%20Inel%20Djafar%20and%20Joseph%20Musielewicz%20and%20Amandine%20Rossello%20and%20Victor%20Schmidt%20and%20Alexandre%20Duval%0AAbstract%3A%20%20%20The%20development%20of%20accurate%20machine%20learning%20interatomic%20potentials%20%28MLIPs%29%0Ais%20limited%20by%20the%20fragmented%20availability%20and%20inconsistent%20formatting%20of%0Aquantum%20mechanical%20trajectory%20datasets%20derived%20from%20Density%20Functional%20Theory%0A%28DFT%29.%20These%20datasets%20are%20expensive%20to%20generate%20yet%20difficult%20to%20combine%20due%20to%0Avariations%20in%20format%2C%20metadata%2C%20and%20accessibility.%20To%20address%20this%2C%20we%0Aintroduce%20LeMat-Traj%2C%20a%20curated%20dataset%20comprising%20over%20120%20million%20atomic%0Aconfigurations%20aggregated%20from%20large-scale%20repositories%2C%20including%20the%0AMaterials%20Project%2C%20Alexandria%2C%20and%20OQMD.%20LeMat-Traj%20standardizes%20data%0Arepresentation%2C%20harmonizes%20results%20and%20filters%20for%20high-quality%20configurations%0Aacross%20widely%20used%20DFT%20functionals%20%28PBE%2C%20PBESol%2C%20SCAN%2C%20r2SCAN%29.%20It%0Asignificantly%20lowers%20the%20barrier%20for%20training%20transferrable%20and%20accurate%20MLIPs.%0ALeMat-Traj%20spans%20both%20relaxed%20low-energy%20states%20and%20high-energy%2C%20high-force%0Astructures%2C%20complementing%20molecular%20dynamics%20and%20active%20learning%20datasets.%20By%0Afine-tuning%20models%20pre-trained%20on%20high-force%20data%20with%20LeMat-Traj%2C%20we%20achieve%20a%0Asignificant%20reduction%20in%20force%20prediction%20errors%20on%20relaxation%20tasks.%20We%20also%0Apresent%20LeMaterial-Fetcher%2C%20a%20modular%20and%20extensible%20open-source%20library%0Adeveloped%20for%20this%20work%2C%20designed%20to%20provide%20a%20reproducible%20framework%20for%20the%0Acommunity%20to%20easily%20incorporate%20new%20data%20sources%20and%20ensure%20the%20continued%0Aevolution%20of%20large-scale%20materials%20datasets.%20LeMat-Traj%20and%20LeMaterial-Fetcher%0Aare%20publicly%20available%20at%20https%3A//huggingface.co/datasets/LeMaterial/LeMat-Traj%0Aand%20https%3A//github.com/LeMaterial/lematerial-fetcher.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2508.20875v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DLeMat-Traj%253A%2520A%2520Scalable%2520and%2520Unified%2520Dataset%2520of%2520Materials%2520Trajectories%2520for%250A%2520%2520Atomistic%2520Modeling%26entry.906535625%3DAli%2520Ramlaoui%2520and%2520Martin%2520Siron%2520and%2520Inel%2520Djafar%2520and%2520Joseph%2520Musielewicz%2520and%2520Amandine%2520Rossello%2520and%2520Victor%2520Schmidt%2520and%2520Alexandre%2520Duval%26entry.1292438233%3D%2520%2520The%2520development%2520of%2520accurate%2520machine%2520learning%2520interatomic%2520potentials%2520%2528MLIPs%2529%250Ais%2520limited%2520by%2520the%2520fragmented%2520availability%2520and%2520inconsistent%2520formatting%2520of%250Aquantum%2520mechanical%2520trajectory%2520datasets%2520derived%2520from%2520Density%2520Functional%2520Theory%250A%2528DFT%2529.%2520These%2520datasets%2520are%2520expensive%2520to%2520generate%2520yet%2520difficult%2520to%2520combine%2520due%2520to%250Avariations%2520in%2520format%252C%2520metadata%252C%2520and%2520accessibility.%2520To%2520address%2520this%252C%2520we%250Aintroduce%2520LeMat-Traj%252C%2520a%2520curated%2520dataset%2520comprising%2520over%2520120%2520million%2520atomic%250Aconfigurations%2520aggregated%2520from%2520large-scale%2520repositories%252C%2520including%2520the%250AMaterials%2520Project%252C%2520Alexandria%252C%2520and%2520OQMD.%2520LeMat-Traj%2520standardizes%2520data%250Arepresentation%252C%2520harmonizes%2520results%2520and%2520filters%2520for%2520high-quality%2520configurations%250Aacross%2520widely%2520used%2520DFT%2520functionals%2520%2528PBE%252C%2520PBESol%252C%2520SCAN%252C%2520r2SCAN%2529.%2520It%250Asignificantly%2520lowers%2520the%2520barrier%2520for%2520training%2520transferrable%2520and%2520accurate%2520MLIPs.%250ALeMat-Traj%2520spans%2520both%2520relaxed%2520low-energy%2520states%2520and%2520high-energy%252C%2520high-force%250Astructures%252C%2520complementing%2520molecular%2520dynamics%2520and%2520active%2520learning%2520datasets.%2520By%250Afine-tuning%2520models%2520pre-trained%2520on%2520high-force%2520data%2520with%2520LeMat-Traj%252C%2520we%2520achieve%2520a%250Asignificant%2520reduction%2520in%2520force%2520prediction%2520errors%2520on%2520relaxation%2520tasks.%2520We%2520also%250Apresent%2520LeMaterial-Fetcher%252C%2520a%2520modular%2520and%2520extensible%2520open-source%2520library%250Adeveloped%2520for%2520this%2520work%252C%2520designed%2520to%2520provide%2520a%2520reproducible%2520framework%2520for%2520the%250Acommunity%2520to%2520easily%2520incorporate%2520new%2520data%2520sources%2520and%2520ensure%2520the%2520continued%250Aevolution%2520of%2520large-scale%2520materials%2520datasets.%2520LeMat-Traj%2520and%2520LeMaterial-Fetcher%250Aare%2520publicly%2520available%2520at%2520https%253A//huggingface.co/datasets/LeMaterial/LeMat-Traj%250Aand%2520https%253A//github.com/LeMaterial/lematerial-fetcher.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2508.20875v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=LeMat-Traj%3A%20A%20Scalable%20and%20Unified%20Dataset%20of%20Materials%20Trajectories%20for%0A%20%20Atomistic%20Modeling&entry.906535625=Ali%20Ramlaoui%20and%20Martin%20Siron%20and%20Inel%20Djafar%20and%20Joseph%20Musielewicz%20and%20Amandine%20Rossello%20and%20Victor%20Schmidt%20and%20Alexandre%20Duval&entry.1292438233=%20%20The%20development%20of%20accurate%20machine%20learning%20interatomic%20potentials%20%28MLIPs%29%0Ais%20limited%20by%20the%20fragmented%20availability%20and%20inconsistent%20formatting%20of%0Aquantum%20mechanical%20trajectory%20datasets%20derived%20from%20Density%20Functional%20Theory%0A%28DFT%29.%20These%20datasets%20are%20expensive%20to%20generate%20yet%20difficult%20to%20combine%20due%20to%0Avariations%20in%20format%2C%20metadata%2C%20and%20accessibility.%20To%20address%20this%2C%20we%0Aintroduce%20LeMat-Traj%2C%20a%20curated%20dataset%20comprising%20over%20120%20million%20atomic%0Aconfigurations%20aggregated%20from%20large-scale%20repositories%2C%20including%20the%0AMaterials%20Project%2C%20Alexandria%2C%20and%20OQMD.%20LeMat-Traj%20standardizes%20data%0Arepresentation%2C%20harmonizes%20results%20and%20filters%20for%20high-quality%20configurations%0Aacross%20widely%20used%20DFT%20functionals%20%28PBE%2C%20PBESol%2C%20SCAN%2C%20r2SCAN%29.%20It%0Asignificantly%20lowers%20the%20barrier%20for%20training%20transferrable%20and%20accurate%20MLIPs.%0ALeMat-Traj%20spans%20both%20relaxed%20low-energy%20states%20and%20high-energy%2C%20high-force%0Astructures%2C%20complementing%20molecular%20dynamics%20and%20active%20learning%20datasets.%20By%0Afine-tuning%20models%20pre-trained%20on%20high-force%20data%20with%20LeMat-Traj%2C%20we%20achieve%20a%0Asignificant%20reduction%20in%20force%20prediction%20errors%20on%20relaxation%20tasks.%20We%20also%0Apresent%20LeMaterial-Fetcher%2C%20a%20modular%20and%20extensible%20open-source%20library%0Adeveloped%20for%20this%20work%2C%20designed%20to%20provide%20a%20reproducible%20framework%20for%20the%0Acommunity%20to%20easily%20incorporate%20new%20data%20sources%20and%20ensure%20the%20continued%0Aevolution%20of%20large-scale%20materials%20datasets.%20LeMat-Traj%20and%20LeMaterial-Fetcher%0Aare%20publicly%20available%20at%20https%3A//huggingface.co/datasets/LeMaterial/LeMat-Traj%0Aand%20https%3A//github.com/LeMaterial/lematerial-fetcher.%0A&entry.1838667208=http%3A//arxiv.org/abs/2508.20875v2&entry.124074799=Read"},
{"title": "Understanding Generalization in Node and Link Prediction", "author": "Antonis Vasileiou and Timo Stoll and Christopher Morris", "abstract": "  Using message-passing graph neural networks (MPNNs) for node and link\nprediction is crucial in various scientific and industrial domains, which has\nled to the development of diverse MPNN architectures. Besides working well in\npractical settings, their ability to generalize beyond the training set remains\npoorly understood. While some studies have explored MPNNs' generalization in\ngraph-level prediction tasks, much less attention has been given to node- and\nlink-level predictions. Existing works often rely on unrealistic i.i.d.\\@\nassumptions, overlooking possible correlations between nodes or links, and\nassuming fixed aggregation and impractical loss functions while neglecting the\ninfluence of graph structure. In this work, we introduce a unified framework to\nanalyze the generalization properties of MPNNs in inductive and transductive\nnode and link prediction settings, incorporating diverse architectural\nparameters and loss functions and quantifying the influence of graph structure.\nAdditionally, our proposed generalization framework can be applied beyond\ngraphs to any classification task under the inductive or transductive setting.\nOur empirical study supports our theoretical insights, deepening our\nunderstanding of MPNNs' generalization capabilities in these tasks.\n", "link": "http://arxiv.org/abs/2507.00927v2", "date": "2025-10-17", "relevancy": 1.883, "topK": [{"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.4895}, {"title": "Boosted dynamic neural networks", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/26302", "similarity": 0.4764}, {"title": "Self-supervised Photographic Image Layout Representation Learning", "link": "http://arxiv.org/abs/2403.03740v1", "similarity": 0.4575}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Understanding%20Generalization%20in%20Node%20and%20Link%20Prediction&body=Title%3A%20Understanding%20Generalization%20in%20Node%20and%20Link%20Prediction%0AAuthor%3A%20Antonis%20Vasileiou%20and%20Timo%20Stoll%20and%20Christopher%20Morris%0AAbstract%3A%20%20%20Using%20message-passing%20graph%20neural%20networks%20%28MPNNs%29%20for%20node%20and%20link%0Aprediction%20is%20crucial%20in%20various%20scientific%20and%20industrial%20domains%2C%20which%20has%0Aled%20to%20the%20development%20of%20diverse%20MPNN%20architectures.%20Besides%20working%20well%20in%0Apractical%20settings%2C%20their%20ability%20to%20generalize%20beyond%20the%20training%20set%20remains%0Apoorly%20understood.%20While%20some%20studies%20have%20explored%20MPNNs%27%20generalization%20in%0Agraph-level%20prediction%20tasks%2C%20much%20less%20attention%20has%20been%20given%20to%20node-%20and%0Alink-level%20predictions.%20Existing%20works%20often%20rely%20on%20unrealistic%20i.i.d.%5C%40%0Aassumptions%2C%20overlooking%20possible%20correlations%20between%20nodes%20or%20links%2C%20and%0Aassuming%20fixed%20aggregation%20and%20impractical%20loss%20functions%20while%20neglecting%20the%0Ainfluence%20of%20graph%20structure.%20In%20this%20work%2C%20we%20introduce%20a%20unified%20framework%20to%0Aanalyze%20the%20generalization%20properties%20of%20MPNNs%20in%20inductive%20and%20transductive%0Anode%20and%20link%20prediction%20settings%2C%20incorporating%20diverse%20architectural%0Aparameters%20and%20loss%20functions%20and%20quantifying%20the%20influence%20of%20graph%20structure.%0AAdditionally%2C%20our%20proposed%20generalization%20framework%20can%20be%20applied%20beyond%0Agraphs%20to%20any%20classification%20task%20under%20the%20inductive%20or%20transductive%20setting.%0AOur%20empirical%20study%20supports%20our%20theoretical%20insights%2C%20deepening%20our%0Aunderstanding%20of%20MPNNs%27%20generalization%20capabilities%20in%20these%20tasks.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2507.00927v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DUnderstanding%2520Generalization%2520in%2520Node%2520and%2520Link%2520Prediction%26entry.906535625%3DAntonis%2520Vasileiou%2520and%2520Timo%2520Stoll%2520and%2520Christopher%2520Morris%26entry.1292438233%3D%2520%2520Using%2520message-passing%2520graph%2520neural%2520networks%2520%2528MPNNs%2529%2520for%2520node%2520and%2520link%250Aprediction%2520is%2520crucial%2520in%2520various%2520scientific%2520and%2520industrial%2520domains%252C%2520which%2520has%250Aled%2520to%2520the%2520development%2520of%2520diverse%2520MPNN%2520architectures.%2520Besides%2520working%2520well%2520in%250Apractical%2520settings%252C%2520their%2520ability%2520to%2520generalize%2520beyond%2520the%2520training%2520set%2520remains%250Apoorly%2520understood.%2520While%2520some%2520studies%2520have%2520explored%2520MPNNs%2527%2520generalization%2520in%250Agraph-level%2520prediction%2520tasks%252C%2520much%2520less%2520attention%2520has%2520been%2520given%2520to%2520node-%2520and%250Alink-level%2520predictions.%2520Existing%2520works%2520often%2520rely%2520on%2520unrealistic%2520i.i.d.%255C%2540%250Aassumptions%252C%2520overlooking%2520possible%2520correlations%2520between%2520nodes%2520or%2520links%252C%2520and%250Aassuming%2520fixed%2520aggregation%2520and%2520impractical%2520loss%2520functions%2520while%2520neglecting%2520the%250Ainfluence%2520of%2520graph%2520structure.%2520In%2520this%2520work%252C%2520we%2520introduce%2520a%2520unified%2520framework%2520to%250Aanalyze%2520the%2520generalization%2520properties%2520of%2520MPNNs%2520in%2520inductive%2520and%2520transductive%250Anode%2520and%2520link%2520prediction%2520settings%252C%2520incorporating%2520diverse%2520architectural%250Aparameters%2520and%2520loss%2520functions%2520and%2520quantifying%2520the%2520influence%2520of%2520graph%2520structure.%250AAdditionally%252C%2520our%2520proposed%2520generalization%2520framework%2520can%2520be%2520applied%2520beyond%250Agraphs%2520to%2520any%2520classification%2520task%2520under%2520the%2520inductive%2520or%2520transductive%2520setting.%250AOur%2520empirical%2520study%2520supports%2520our%2520theoretical%2520insights%252C%2520deepening%2520our%250Aunderstanding%2520of%2520MPNNs%2527%2520generalization%2520capabilities%2520in%2520these%2520tasks.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2507.00927v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Understanding%20Generalization%20in%20Node%20and%20Link%20Prediction&entry.906535625=Antonis%20Vasileiou%20and%20Timo%20Stoll%20and%20Christopher%20Morris&entry.1292438233=%20%20Using%20message-passing%20graph%20neural%20networks%20%28MPNNs%29%20for%20node%20and%20link%0Aprediction%20is%20crucial%20in%20various%20scientific%20and%20industrial%20domains%2C%20which%20has%0Aled%20to%20the%20development%20of%20diverse%20MPNN%20architectures.%20Besides%20working%20well%20in%0Apractical%20settings%2C%20their%20ability%20to%20generalize%20beyond%20the%20training%20set%20remains%0Apoorly%20understood.%20While%20some%20studies%20have%20explored%20MPNNs%27%20generalization%20in%0Agraph-level%20prediction%20tasks%2C%20much%20less%20attention%20has%20been%20given%20to%20node-%20and%0Alink-level%20predictions.%20Existing%20works%20often%20rely%20on%20unrealistic%20i.i.d.%5C%40%0Aassumptions%2C%20overlooking%20possible%20correlations%20between%20nodes%20or%20links%2C%20and%0Aassuming%20fixed%20aggregation%20and%20impractical%20loss%20functions%20while%20neglecting%20the%0Ainfluence%20of%20graph%20structure.%20In%20this%20work%2C%20we%20introduce%20a%20unified%20framework%20to%0Aanalyze%20the%20generalization%20properties%20of%20MPNNs%20in%20inductive%20and%20transductive%0Anode%20and%20link%20prediction%20settings%2C%20incorporating%20diverse%20architectural%0Aparameters%20and%20loss%20functions%20and%20quantifying%20the%20influence%20of%20graph%20structure.%0AAdditionally%2C%20our%20proposed%20generalization%20framework%20can%20be%20applied%20beyond%0Agraphs%20to%20any%20classification%20task%20under%20the%20inductive%20or%20transductive%20setting.%0AOur%20empirical%20study%20supports%20our%20theoretical%20insights%2C%20deepening%20our%0Aunderstanding%20of%20MPNNs%27%20generalization%20capabilities%20in%20these%20tasks.%0A&entry.1838667208=http%3A//arxiv.org/abs/2507.00927v2&entry.124074799=Read"},
{"title": "On Non-interactive Evaluation of Animal Communication Translators", "author": "Orr Paradise and David F. Gruber and Adam Tauman Kalai", "abstract": "  If you had an AI Whale-to-English translator, how could you validate whether\nor not it is working? Does one need to interact with the animals or rely on\ngrounded observations such as temperature? We provide theoretical and\nproof-of-concept experimental evidence suggesting that interaction and even\nobservations may not be necessary for sufficiently complex languages. One may\nbe able to evaluate translators solely by their English outputs, offering\npotential advantages in terms of safety, ethics, and cost. This is an instance\nof machine translation quality evaluation (MTQE) without any reference\ntranslations available. A key challenge is identifying ``hallucinations,''\nfalse translations which may appear fluent and plausible. We propose using\nsegment-by-segment translation together with the classic NLP shuffle test to\nevaluate translators. The idea is to translate animal communication, turn by\nturn, and evaluate how often the resulting translations make more sense in\norder than permuted. Proof-of-concept experiments on data-scarce human\nlanguages and constructed languages demonstrate the potential utility of this\nevaluation methodology. These human-language experiments serve solely to\nvalidate our reference-free metric under data scarcity. It is found to\ncorrelate highly with a standard evaluation based on reference translations,\nwhich are available in our experiments. We also perform a theoretical analysis\nsuggesting that interaction may not be necessary nor efficient in the early\nstages of learning to translate.\n", "link": "http://arxiv.org/abs/2510.15768v1", "date": "2025-10-17", "relevancy": 1.8758, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.4705}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.4705}, {"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.4614}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20On%20Non-interactive%20Evaluation%20of%20Animal%20Communication%20Translators&body=Title%3A%20On%20Non-interactive%20Evaluation%20of%20Animal%20Communication%20Translators%0AAuthor%3A%20Orr%20Paradise%20and%20David%20F.%20Gruber%20and%20Adam%20Tauman%20Kalai%0AAbstract%3A%20%20%20If%20you%20had%20an%20AI%20Whale-to-English%20translator%2C%20how%20could%20you%20validate%20whether%0Aor%20not%20it%20is%20working%3F%20Does%20one%20need%20to%20interact%20with%20the%20animals%20or%20rely%20on%0Agrounded%20observations%20such%20as%20temperature%3F%20We%20provide%20theoretical%20and%0Aproof-of-concept%20experimental%20evidence%20suggesting%20that%20interaction%20and%20even%0Aobservations%20may%20not%20be%20necessary%20for%20sufficiently%20complex%20languages.%20One%20may%0Abe%20able%20to%20evaluate%20translators%20solely%20by%20their%20English%20outputs%2C%20offering%0Apotential%20advantages%20in%20terms%20of%20safety%2C%20ethics%2C%20and%20cost.%20This%20is%20an%20instance%0Aof%20machine%20translation%20quality%20evaluation%20%28MTQE%29%20without%20any%20reference%0Atranslations%20available.%20A%20key%20challenge%20is%20identifying%20%60%60hallucinations%2C%27%27%0Afalse%20translations%20which%20may%20appear%20fluent%20and%20plausible.%20We%20propose%20using%0Asegment-by-segment%20translation%20together%20with%20the%20classic%20NLP%20shuffle%20test%20to%0Aevaluate%20translators.%20The%20idea%20is%20to%20translate%20animal%20communication%2C%20turn%20by%0Aturn%2C%20and%20evaluate%20how%20often%20the%20resulting%20translations%20make%20more%20sense%20in%0Aorder%20than%20permuted.%20Proof-of-concept%20experiments%20on%20data-scarce%20human%0Alanguages%20and%20constructed%20languages%20demonstrate%20the%20potential%20utility%20of%20this%0Aevaluation%20methodology.%20These%20human-language%20experiments%20serve%20solely%20to%0Avalidate%20our%20reference-free%20metric%20under%20data%20scarcity.%20It%20is%20found%20to%0Acorrelate%20highly%20with%20a%20standard%20evaluation%20based%20on%20reference%20translations%2C%0Awhich%20are%20available%20in%20our%20experiments.%20We%20also%20perform%20a%20theoretical%20analysis%0Asuggesting%20that%20interaction%20may%20not%20be%20necessary%20nor%20efficient%20in%20the%20early%0Astages%20of%20learning%20to%20translate.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2510.15768v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DOn%2520Non-interactive%2520Evaluation%2520of%2520Animal%2520Communication%2520Translators%26entry.906535625%3DOrr%2520Paradise%2520and%2520David%2520F.%2520Gruber%2520and%2520Adam%2520Tauman%2520Kalai%26entry.1292438233%3D%2520%2520If%2520you%2520had%2520an%2520AI%2520Whale-to-English%2520translator%252C%2520how%2520could%2520you%2520validate%2520whether%250Aor%2520not%2520it%2520is%2520working%253F%2520Does%2520one%2520need%2520to%2520interact%2520with%2520the%2520animals%2520or%2520rely%2520on%250Agrounded%2520observations%2520such%2520as%2520temperature%253F%2520We%2520provide%2520theoretical%2520and%250Aproof-of-concept%2520experimental%2520evidence%2520suggesting%2520that%2520interaction%2520and%2520even%250Aobservations%2520may%2520not%2520be%2520necessary%2520for%2520sufficiently%2520complex%2520languages.%2520One%2520may%250Abe%2520able%2520to%2520evaluate%2520translators%2520solely%2520by%2520their%2520English%2520outputs%252C%2520offering%250Apotential%2520advantages%2520in%2520terms%2520of%2520safety%252C%2520ethics%252C%2520and%2520cost.%2520This%2520is%2520an%2520instance%250Aof%2520machine%2520translation%2520quality%2520evaluation%2520%2528MTQE%2529%2520without%2520any%2520reference%250Atranslations%2520available.%2520A%2520key%2520challenge%2520is%2520identifying%2520%2560%2560hallucinations%252C%2527%2527%250Afalse%2520translations%2520which%2520may%2520appear%2520fluent%2520and%2520plausible.%2520We%2520propose%2520using%250Asegment-by-segment%2520translation%2520together%2520with%2520the%2520classic%2520NLP%2520shuffle%2520test%2520to%250Aevaluate%2520translators.%2520The%2520idea%2520is%2520to%2520translate%2520animal%2520communication%252C%2520turn%2520by%250Aturn%252C%2520and%2520evaluate%2520how%2520often%2520the%2520resulting%2520translations%2520make%2520more%2520sense%2520in%250Aorder%2520than%2520permuted.%2520Proof-of-concept%2520experiments%2520on%2520data-scarce%2520human%250Alanguages%2520and%2520constructed%2520languages%2520demonstrate%2520the%2520potential%2520utility%2520of%2520this%250Aevaluation%2520methodology.%2520These%2520human-language%2520experiments%2520serve%2520solely%2520to%250Avalidate%2520our%2520reference-free%2520metric%2520under%2520data%2520scarcity.%2520It%2520is%2520found%2520to%250Acorrelate%2520highly%2520with%2520a%2520standard%2520evaluation%2520based%2520on%2520reference%2520translations%252C%250Awhich%2520are%2520available%2520in%2520our%2520experiments.%2520We%2520also%2520perform%2520a%2520theoretical%2520analysis%250Asuggesting%2520that%2520interaction%2520may%2520not%2520be%2520necessary%2520nor%2520efficient%2520in%2520the%2520early%250Astages%2520of%2520learning%2520to%2520translate.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2510.15768v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=On%20Non-interactive%20Evaluation%20of%20Animal%20Communication%20Translators&entry.906535625=Orr%20Paradise%20and%20David%20F.%20Gruber%20and%20Adam%20Tauman%20Kalai&entry.1292438233=%20%20If%20you%20had%20an%20AI%20Whale-to-English%20translator%2C%20how%20could%20you%20validate%20whether%0Aor%20not%20it%20is%20working%3F%20Does%20one%20need%20to%20interact%20with%20the%20animals%20or%20rely%20on%0Agrounded%20observations%20such%20as%20temperature%3F%20We%20provide%20theoretical%20and%0Aproof-of-concept%20experimental%20evidence%20suggesting%20that%20interaction%20and%20even%0Aobservations%20may%20not%20be%20necessary%20for%20sufficiently%20complex%20languages.%20One%20may%0Abe%20able%20to%20evaluate%20translators%20solely%20by%20their%20English%20outputs%2C%20offering%0Apotential%20advantages%20in%20terms%20of%20safety%2C%20ethics%2C%20and%20cost.%20This%20is%20an%20instance%0Aof%20machine%20translation%20quality%20evaluation%20%28MTQE%29%20without%20any%20reference%0Atranslations%20available.%20A%20key%20challenge%20is%20identifying%20%60%60hallucinations%2C%27%27%0Afalse%20translations%20which%20may%20appear%20fluent%20and%20plausible.%20We%20propose%20using%0Asegment-by-segment%20translation%20together%20with%20the%20classic%20NLP%20shuffle%20test%20to%0Aevaluate%20translators.%20The%20idea%20is%20to%20translate%20animal%20communication%2C%20turn%20by%0Aturn%2C%20and%20evaluate%20how%20often%20the%20resulting%20translations%20make%20more%20sense%20in%0Aorder%20than%20permuted.%20Proof-of-concept%20experiments%20on%20data-scarce%20human%0Alanguages%20and%20constructed%20languages%20demonstrate%20the%20potential%20utility%20of%20this%0Aevaluation%20methodology.%20These%20human-language%20experiments%20serve%20solely%20to%0Avalidate%20our%20reference-free%20metric%20under%20data%20scarcity.%20It%20is%20found%20to%0Acorrelate%20highly%20with%20a%20standard%20evaluation%20based%20on%20reference%20translations%2C%0Awhich%20are%20available%20in%20our%20experiments.%20We%20also%20perform%20a%20theoretical%20analysis%0Asuggesting%20that%20interaction%20may%20not%20be%20necessary%20nor%20efficient%20in%20the%20early%0Astages%20of%20learning%20to%20translate.%0A&entry.1838667208=http%3A//arxiv.org/abs/2510.15768v1&entry.124074799=Read"},
{"title": "A Framework for Rapidly Developing and Deploying Protection Against\n  Large Language Model Attacks", "author": "Adam Swanda and Amy Chang and Alexander Chen and Fraser Burch and Paul Kassianik and Konstantin Berlin", "abstract": "  The widespread adoption of Large Language Models (LLMs) has revolutionized AI\ndeployment, enabling autonomous and semi-autonomous applications across\nindustries through intuitive language interfaces and continuous improvements in\nmodel development. However, the attendant increase in autonomy and expansion of\naccess permissions among AI applications also make these systems compelling\ntargets for malicious attacks. Their inherent susceptibility to security flaws\nnecessitates robust defenses, yet no known approaches can prevent zero-day or\nnovel attacks against LLMs. This places AI protection systems in a category\nsimilar to established malware protection systems: rather than providing\nguaranteed immunity, they minimize risk through enhanced observability,\nmulti-layered defense, and rapid threat response, supported by a threat\nintelligence function designed specifically for AI-related threats.\n  Prior work on LLM protection has largely evaluated individual detection\nmodels rather than end-to-end systems designed for continuous, rapid adaptation\nto a changing threat landscape. We present a production-grade defense system\nrooted in established malware detection and threat intelligence practices. Our\nplatform integrates three components: a threat intelligence system that turns\nemerging threats into protections; a data platform that aggregates and enriches\ninformation while providing observability, monitoring, and ML operations; and a\nrelease platform enabling safe, rapid detection updates without disrupting\ncustomer workflows. Together, these components deliver layered protection\nagainst evolving LLM threats while generating training data for continuous\nmodel improvement and deploying updates without interrupting production.\n", "link": "http://arxiv.org/abs/2509.20639v2", "date": "2025-10-17", "relevancy": 1.8754, "topK": [{"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.4794}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.4667}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.4667}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20A%20Framework%20for%20Rapidly%20Developing%20and%20Deploying%20Protection%20Against%0A%20%20Large%20Language%20Model%20Attacks&body=Title%3A%20A%20Framework%20for%20Rapidly%20Developing%20and%20Deploying%20Protection%20Against%0A%20%20Large%20Language%20Model%20Attacks%0AAuthor%3A%20Adam%20Swanda%20and%20Amy%20Chang%20and%20Alexander%20Chen%20and%20Fraser%20Burch%20and%20Paul%20Kassianik%20and%20Konstantin%20Berlin%0AAbstract%3A%20%20%20The%20widespread%20adoption%20of%20Large%20Language%20Models%20%28LLMs%29%20has%20revolutionized%20AI%0Adeployment%2C%20enabling%20autonomous%20and%20semi-autonomous%20applications%20across%0Aindustries%20through%20intuitive%20language%20interfaces%20and%20continuous%20improvements%20in%0Amodel%20development.%20However%2C%20the%20attendant%20increase%20in%20autonomy%20and%20expansion%20of%0Aaccess%20permissions%20among%20AI%20applications%20also%20make%20these%20systems%20compelling%0Atargets%20for%20malicious%20attacks.%20Their%20inherent%20susceptibility%20to%20security%20flaws%0Anecessitates%20robust%20defenses%2C%20yet%20no%20known%20approaches%20can%20prevent%20zero-day%20or%0Anovel%20attacks%20against%20LLMs.%20This%20places%20AI%20protection%20systems%20in%20a%20category%0Asimilar%20to%20established%20malware%20protection%20systems%3A%20rather%20than%20providing%0Aguaranteed%20immunity%2C%20they%20minimize%20risk%20through%20enhanced%20observability%2C%0Amulti-layered%20defense%2C%20and%20rapid%20threat%20response%2C%20supported%20by%20a%20threat%0Aintelligence%20function%20designed%20specifically%20for%20AI-related%20threats.%0A%20%20Prior%20work%20on%20LLM%20protection%20has%20largely%20evaluated%20individual%20detection%0Amodels%20rather%20than%20end-to-end%20systems%20designed%20for%20continuous%2C%20rapid%20adaptation%0Ato%20a%20changing%20threat%20landscape.%20We%20present%20a%20production-grade%20defense%20system%0Arooted%20in%20established%20malware%20detection%20and%20threat%20intelligence%20practices.%20Our%0Aplatform%20integrates%20three%20components%3A%20a%20threat%20intelligence%20system%20that%20turns%0Aemerging%20threats%20into%20protections%3B%20a%20data%20platform%20that%20aggregates%20and%20enriches%0Ainformation%20while%20providing%20observability%2C%20monitoring%2C%20and%20ML%20operations%3B%20and%20a%0Arelease%20platform%20enabling%20safe%2C%20rapid%20detection%20updates%20without%20disrupting%0Acustomer%20workflows.%20Together%2C%20these%20components%20deliver%20layered%20protection%0Aagainst%20evolving%20LLM%20threats%20while%20generating%20training%20data%20for%20continuous%0Amodel%20improvement%20and%20deploying%20updates%20without%20interrupting%20production.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2509.20639v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DA%2520Framework%2520for%2520Rapidly%2520Developing%2520and%2520Deploying%2520Protection%2520Against%250A%2520%2520Large%2520Language%2520Model%2520Attacks%26entry.906535625%3DAdam%2520Swanda%2520and%2520Amy%2520Chang%2520and%2520Alexander%2520Chen%2520and%2520Fraser%2520Burch%2520and%2520Paul%2520Kassianik%2520and%2520Konstantin%2520Berlin%26entry.1292438233%3D%2520%2520The%2520widespread%2520adoption%2520of%2520Large%2520Language%2520Models%2520%2528LLMs%2529%2520has%2520revolutionized%2520AI%250Adeployment%252C%2520enabling%2520autonomous%2520and%2520semi-autonomous%2520applications%2520across%250Aindustries%2520through%2520intuitive%2520language%2520interfaces%2520and%2520continuous%2520improvements%2520in%250Amodel%2520development.%2520However%252C%2520the%2520attendant%2520increase%2520in%2520autonomy%2520and%2520expansion%2520of%250Aaccess%2520permissions%2520among%2520AI%2520applications%2520also%2520make%2520these%2520systems%2520compelling%250Atargets%2520for%2520malicious%2520attacks.%2520Their%2520inherent%2520susceptibility%2520to%2520security%2520flaws%250Anecessitates%2520robust%2520defenses%252C%2520yet%2520no%2520known%2520approaches%2520can%2520prevent%2520zero-day%2520or%250Anovel%2520attacks%2520against%2520LLMs.%2520This%2520places%2520AI%2520protection%2520systems%2520in%2520a%2520category%250Asimilar%2520to%2520established%2520malware%2520protection%2520systems%253A%2520rather%2520than%2520providing%250Aguaranteed%2520immunity%252C%2520they%2520minimize%2520risk%2520through%2520enhanced%2520observability%252C%250Amulti-layered%2520defense%252C%2520and%2520rapid%2520threat%2520response%252C%2520supported%2520by%2520a%2520threat%250Aintelligence%2520function%2520designed%2520specifically%2520for%2520AI-related%2520threats.%250A%2520%2520Prior%2520work%2520on%2520LLM%2520protection%2520has%2520largely%2520evaluated%2520individual%2520detection%250Amodels%2520rather%2520than%2520end-to-end%2520systems%2520designed%2520for%2520continuous%252C%2520rapid%2520adaptation%250Ato%2520a%2520changing%2520threat%2520landscape.%2520We%2520present%2520a%2520production-grade%2520defense%2520system%250Arooted%2520in%2520established%2520malware%2520detection%2520and%2520threat%2520intelligence%2520practices.%2520Our%250Aplatform%2520integrates%2520three%2520components%253A%2520a%2520threat%2520intelligence%2520system%2520that%2520turns%250Aemerging%2520threats%2520into%2520protections%253B%2520a%2520data%2520platform%2520that%2520aggregates%2520and%2520enriches%250Ainformation%2520while%2520providing%2520observability%252C%2520monitoring%252C%2520and%2520ML%2520operations%253B%2520and%2520a%250Arelease%2520platform%2520enabling%2520safe%252C%2520rapid%2520detection%2520updates%2520without%2520disrupting%250Acustomer%2520workflows.%2520Together%252C%2520these%2520components%2520deliver%2520layered%2520protection%250Aagainst%2520evolving%2520LLM%2520threats%2520while%2520generating%2520training%2520data%2520for%2520continuous%250Amodel%2520improvement%2520and%2520deploying%2520updates%2520without%2520interrupting%2520production.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2509.20639v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=A%20Framework%20for%20Rapidly%20Developing%20and%20Deploying%20Protection%20Against%0A%20%20Large%20Language%20Model%20Attacks&entry.906535625=Adam%20Swanda%20and%20Amy%20Chang%20and%20Alexander%20Chen%20and%20Fraser%20Burch%20and%20Paul%20Kassianik%20and%20Konstantin%20Berlin&entry.1292438233=%20%20The%20widespread%20adoption%20of%20Large%20Language%20Models%20%28LLMs%29%20has%20revolutionized%20AI%0Adeployment%2C%20enabling%20autonomous%20and%20semi-autonomous%20applications%20across%0Aindustries%20through%20intuitive%20language%20interfaces%20and%20continuous%20improvements%20in%0Amodel%20development.%20However%2C%20the%20attendant%20increase%20in%20autonomy%20and%20expansion%20of%0Aaccess%20permissions%20among%20AI%20applications%20also%20make%20these%20systems%20compelling%0Atargets%20for%20malicious%20attacks.%20Their%20inherent%20susceptibility%20to%20security%20flaws%0Anecessitates%20robust%20defenses%2C%20yet%20no%20known%20approaches%20can%20prevent%20zero-day%20or%0Anovel%20attacks%20against%20LLMs.%20This%20places%20AI%20protection%20systems%20in%20a%20category%0Asimilar%20to%20established%20malware%20protection%20systems%3A%20rather%20than%20providing%0Aguaranteed%20immunity%2C%20they%20minimize%20risk%20through%20enhanced%20observability%2C%0Amulti-layered%20defense%2C%20and%20rapid%20threat%20response%2C%20supported%20by%20a%20threat%0Aintelligence%20function%20designed%20specifically%20for%20AI-related%20threats.%0A%20%20Prior%20work%20on%20LLM%20protection%20has%20largely%20evaluated%20individual%20detection%0Amodels%20rather%20than%20end-to-end%20systems%20designed%20for%20continuous%2C%20rapid%20adaptation%0Ato%20a%20changing%20threat%20landscape.%20We%20present%20a%20production-grade%20defense%20system%0Arooted%20in%20established%20malware%20detection%20and%20threat%20intelligence%20practices.%20Our%0Aplatform%20integrates%20three%20components%3A%20a%20threat%20intelligence%20system%20that%20turns%0Aemerging%20threats%20into%20protections%3B%20a%20data%20platform%20that%20aggregates%20and%20enriches%0Ainformation%20while%20providing%20observability%2C%20monitoring%2C%20and%20ML%20operations%3B%20and%20a%0Arelease%20platform%20enabling%20safe%2C%20rapid%20detection%20updates%20without%20disrupting%0Acustomer%20workflows.%20Together%2C%20these%20components%20deliver%20layered%20protection%0Aagainst%20evolving%20LLM%20threats%20while%20generating%20training%20data%20for%20continuous%0Amodel%20improvement%20and%20deploying%20updates%20without%20interrupting%20production.%0A&entry.1838667208=http%3A//arxiv.org/abs/2509.20639v2&entry.124074799=Read"},
{"title": "Decentralized Parameter-Free Online Learning", "author": "Tomas Ortega and Hamid Jafarkhani", "abstract": "  We propose the first parameter-free decentralized online learning algorithms\nwith network regret guarantees, which achieve sublinear regret without\nrequiring hyperparameter tuning. This family of algorithms connects multi-agent\ncoin-betting and decentralized online learning via gossip steps. To enable our\ndecentralized analysis, we introduce a novel \"betting function\" formulation for\ncoin-betting that simplifies the multi-agent regret analysis. Our analysis\nshows sublinear network regret bounds and is validated through experiments on\nsynthetic and real datasets. This family of algorithms is applicable to\ndistributed sensing, decentralized optimization, and collaborative ML\napplications.\n", "link": "http://arxiv.org/abs/2510.15644v1", "date": "2025-10-17", "relevancy": 1.8631, "topK": [{"title": "Boosted dynamic neural networks", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/26302", "similarity": 0.477}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.46}, {"title": "DDM-NET: End-to-end learning of keypoint feature Detection, Description and Matching for 3D localization", "link": "https://arxiv.org/abs/2212.04575", "similarity": 0.4569}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Decentralized%20Parameter-Free%20Online%20Learning&body=Title%3A%20Decentralized%20Parameter-Free%20Online%20Learning%0AAuthor%3A%20Tomas%20Ortega%20and%20Hamid%20Jafarkhani%0AAbstract%3A%20%20%20We%20propose%20the%20first%20parameter-free%20decentralized%20online%20learning%20algorithms%0Awith%20network%20regret%20guarantees%2C%20which%20achieve%20sublinear%20regret%20without%0Arequiring%20hyperparameter%20tuning.%20This%20family%20of%20algorithms%20connects%20multi-agent%0Acoin-betting%20and%20decentralized%20online%20learning%20via%20gossip%20steps.%20To%20enable%20our%0Adecentralized%20analysis%2C%20we%20introduce%20a%20novel%20%22betting%20function%22%20formulation%20for%0Acoin-betting%20that%20simplifies%20the%20multi-agent%20regret%20analysis.%20Our%20analysis%0Ashows%20sublinear%20network%20regret%20bounds%20and%20is%20validated%20through%20experiments%20on%0Asynthetic%20and%20real%20datasets.%20This%20family%20of%20algorithms%20is%20applicable%20to%0Adistributed%20sensing%2C%20decentralized%20optimization%2C%20and%20collaborative%20ML%0Aapplications.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2510.15644v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DDecentralized%2520Parameter-Free%2520Online%2520Learning%26entry.906535625%3DTomas%2520Ortega%2520and%2520Hamid%2520Jafarkhani%26entry.1292438233%3D%2520%2520We%2520propose%2520the%2520first%2520parameter-free%2520decentralized%2520online%2520learning%2520algorithms%250Awith%2520network%2520regret%2520guarantees%252C%2520which%2520achieve%2520sublinear%2520regret%2520without%250Arequiring%2520hyperparameter%2520tuning.%2520This%2520family%2520of%2520algorithms%2520connects%2520multi-agent%250Acoin-betting%2520and%2520decentralized%2520online%2520learning%2520via%2520gossip%2520steps.%2520To%2520enable%2520our%250Adecentralized%2520analysis%252C%2520we%2520introduce%2520a%2520novel%2520%2522betting%2520function%2522%2520formulation%2520for%250Acoin-betting%2520that%2520simplifies%2520the%2520multi-agent%2520regret%2520analysis.%2520Our%2520analysis%250Ashows%2520sublinear%2520network%2520regret%2520bounds%2520and%2520is%2520validated%2520through%2520experiments%2520on%250Asynthetic%2520and%2520real%2520datasets.%2520This%2520family%2520of%2520algorithms%2520is%2520applicable%2520to%250Adistributed%2520sensing%252C%2520decentralized%2520optimization%252C%2520and%2520collaborative%2520ML%250Aapplications.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2510.15644v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Decentralized%20Parameter-Free%20Online%20Learning&entry.906535625=Tomas%20Ortega%20and%20Hamid%20Jafarkhani&entry.1292438233=%20%20We%20propose%20the%20first%20parameter-free%20decentralized%20online%20learning%20algorithms%0Awith%20network%20regret%20guarantees%2C%20which%20achieve%20sublinear%20regret%20without%0Arequiring%20hyperparameter%20tuning.%20This%20family%20of%20algorithms%20connects%20multi-agent%0Acoin-betting%20and%20decentralized%20online%20learning%20via%20gossip%20steps.%20To%20enable%20our%0Adecentralized%20analysis%2C%20we%20introduce%20a%20novel%20%22betting%20function%22%20formulation%20for%0Acoin-betting%20that%20simplifies%20the%20multi-agent%20regret%20analysis.%20Our%20analysis%0Ashows%20sublinear%20network%20regret%20bounds%20and%20is%20validated%20through%20experiments%20on%0Asynthetic%20and%20real%20datasets.%20This%20family%20of%20algorithms%20is%20applicable%20to%0Adistributed%20sensing%2C%20decentralized%20optimization%2C%20and%20collaborative%20ML%0Aapplications.%0A&entry.1838667208=http%3A//arxiv.org/abs/2510.15644v1&entry.124074799=Read"},
{"title": "Lookup multivariate Kolmogorov-Arnold Networks", "author": "Sergey Pozdnyakov and Philippe Schwaller", "abstract": "  High-dimensional linear mappings, or linear layers, dominate both the\nparameter count and the computational cost of most modern deep-learning models.\nWe introduce a general-purpose drop-in replacement, lookup multivariate\nKolmogorov-Arnold Networks (lmKANs), which deliver a substantially better\ntrade-off between capacity and inference cost. Our construction expresses a\ngeneral high-dimensional mapping through trainable low-dimensional multivariate\nfunctions. These functions can carry dozens or hundreds of trainable parameters\neach, and yet it takes only a few multiplications to compute them because they\nare implemented as spline lookup tables. Empirically, lmKANs reduce inference\nFLOPs by up to 6.0x while matching the flexibility of MLPs in general\nhigh-dimensional function approximation. In another feedforward fully connected\nbenchmark, on the tabular-like dataset of randomly displaced methane\nconfigurations, lmKANs enable more than 10x higher H100 throughput at equal\naccuracy. Within frameworks of Convolutional Neural Networks, lmKAN-based CNNs\ncut inference FLOPs at matched accuracy by 1.6-2.1x and by 1.7x on the CIFAR-10\nand ImageNet-1k datasets, respectively. Our code, including dedicated CUDA\nkernels, is available online at https://github.com/schwallergroup/lmkan.\n", "link": "http://arxiv.org/abs/2509.07103v2", "date": "2025-10-17", "relevancy": 1.8582, "topK": [{"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.4754}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.4698}, {"title": "Boosted dynamic neural networks", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/26302", "similarity": 0.4516}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Lookup%20multivariate%20Kolmogorov-Arnold%20Networks&body=Title%3A%20Lookup%20multivariate%20Kolmogorov-Arnold%20Networks%0AAuthor%3A%20Sergey%20Pozdnyakov%20and%20Philippe%20Schwaller%0AAbstract%3A%20%20%20High-dimensional%20linear%20mappings%2C%20or%20linear%20layers%2C%20dominate%20both%20the%0Aparameter%20count%20and%20the%20computational%20cost%20of%20most%20modern%20deep-learning%20models.%0AWe%20introduce%20a%20general-purpose%20drop-in%20replacement%2C%20lookup%20multivariate%0AKolmogorov-Arnold%20Networks%20%28lmKANs%29%2C%20which%20deliver%20a%20substantially%20better%0Atrade-off%20between%20capacity%20and%20inference%20cost.%20Our%20construction%20expresses%20a%0Ageneral%20high-dimensional%20mapping%20through%20trainable%20low-dimensional%20multivariate%0Afunctions.%20These%20functions%20can%20carry%20dozens%20or%20hundreds%20of%20trainable%20parameters%0Aeach%2C%20and%20yet%20it%20takes%20only%20a%20few%20multiplications%20to%20compute%20them%20because%20they%0Aare%20implemented%20as%20spline%20lookup%20tables.%20Empirically%2C%20lmKANs%20reduce%20inference%0AFLOPs%20by%20up%20to%206.0x%20while%20matching%20the%20flexibility%20of%20MLPs%20in%20general%0Ahigh-dimensional%20function%20approximation.%20In%20another%20feedforward%20fully%20connected%0Abenchmark%2C%20on%20the%20tabular-like%20dataset%20of%20randomly%20displaced%20methane%0Aconfigurations%2C%20lmKANs%20enable%20more%20than%2010x%20higher%20H100%20throughput%20at%20equal%0Aaccuracy.%20Within%20frameworks%20of%20Convolutional%20Neural%20Networks%2C%20lmKAN-based%20CNNs%0Acut%20inference%20FLOPs%20at%20matched%20accuracy%20by%201.6-2.1x%20and%20by%201.7x%20on%20the%20CIFAR-10%0Aand%20ImageNet-1k%20datasets%2C%20respectively.%20Our%20code%2C%20including%20dedicated%20CUDA%0Akernels%2C%20is%20available%20online%20at%20https%3A//github.com/schwallergroup/lmkan.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2509.07103v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DLookup%2520multivariate%2520Kolmogorov-Arnold%2520Networks%26entry.906535625%3DSergey%2520Pozdnyakov%2520and%2520Philippe%2520Schwaller%26entry.1292438233%3D%2520%2520High-dimensional%2520linear%2520mappings%252C%2520or%2520linear%2520layers%252C%2520dominate%2520both%2520the%250Aparameter%2520count%2520and%2520the%2520computational%2520cost%2520of%2520most%2520modern%2520deep-learning%2520models.%250AWe%2520introduce%2520a%2520general-purpose%2520drop-in%2520replacement%252C%2520lookup%2520multivariate%250AKolmogorov-Arnold%2520Networks%2520%2528lmKANs%2529%252C%2520which%2520deliver%2520a%2520substantially%2520better%250Atrade-off%2520between%2520capacity%2520and%2520inference%2520cost.%2520Our%2520construction%2520expresses%2520a%250Ageneral%2520high-dimensional%2520mapping%2520through%2520trainable%2520low-dimensional%2520multivariate%250Afunctions.%2520These%2520functions%2520can%2520carry%2520dozens%2520or%2520hundreds%2520of%2520trainable%2520parameters%250Aeach%252C%2520and%2520yet%2520it%2520takes%2520only%2520a%2520few%2520multiplications%2520to%2520compute%2520them%2520because%2520they%250Aare%2520implemented%2520as%2520spline%2520lookup%2520tables.%2520Empirically%252C%2520lmKANs%2520reduce%2520inference%250AFLOPs%2520by%2520up%2520to%25206.0x%2520while%2520matching%2520the%2520flexibility%2520of%2520MLPs%2520in%2520general%250Ahigh-dimensional%2520function%2520approximation.%2520In%2520another%2520feedforward%2520fully%2520connected%250Abenchmark%252C%2520on%2520the%2520tabular-like%2520dataset%2520of%2520randomly%2520displaced%2520methane%250Aconfigurations%252C%2520lmKANs%2520enable%2520more%2520than%252010x%2520higher%2520H100%2520throughput%2520at%2520equal%250Aaccuracy.%2520Within%2520frameworks%2520of%2520Convolutional%2520Neural%2520Networks%252C%2520lmKAN-based%2520CNNs%250Acut%2520inference%2520FLOPs%2520at%2520matched%2520accuracy%2520by%25201.6-2.1x%2520and%2520by%25201.7x%2520on%2520the%2520CIFAR-10%250Aand%2520ImageNet-1k%2520datasets%252C%2520respectively.%2520Our%2520code%252C%2520including%2520dedicated%2520CUDA%250Akernels%252C%2520is%2520available%2520online%2520at%2520https%253A//github.com/schwallergroup/lmkan.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2509.07103v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Lookup%20multivariate%20Kolmogorov-Arnold%20Networks&entry.906535625=Sergey%20Pozdnyakov%20and%20Philippe%20Schwaller&entry.1292438233=%20%20High-dimensional%20linear%20mappings%2C%20or%20linear%20layers%2C%20dominate%20both%20the%0Aparameter%20count%20and%20the%20computational%20cost%20of%20most%20modern%20deep-learning%20models.%0AWe%20introduce%20a%20general-purpose%20drop-in%20replacement%2C%20lookup%20multivariate%0AKolmogorov-Arnold%20Networks%20%28lmKANs%29%2C%20which%20deliver%20a%20substantially%20better%0Atrade-off%20between%20capacity%20and%20inference%20cost.%20Our%20construction%20expresses%20a%0Ageneral%20high-dimensional%20mapping%20through%20trainable%20low-dimensional%20multivariate%0Afunctions.%20These%20functions%20can%20carry%20dozens%20or%20hundreds%20of%20trainable%20parameters%0Aeach%2C%20and%20yet%20it%20takes%20only%20a%20few%20multiplications%20to%20compute%20them%20because%20they%0Aare%20implemented%20as%20spline%20lookup%20tables.%20Empirically%2C%20lmKANs%20reduce%20inference%0AFLOPs%20by%20up%20to%206.0x%20while%20matching%20the%20flexibility%20of%20MLPs%20in%20general%0Ahigh-dimensional%20function%20approximation.%20In%20another%20feedforward%20fully%20connected%0Abenchmark%2C%20on%20the%20tabular-like%20dataset%20of%20randomly%20displaced%20methane%0Aconfigurations%2C%20lmKANs%20enable%20more%20than%2010x%20higher%20H100%20throughput%20at%20equal%0Aaccuracy.%20Within%20frameworks%20of%20Convolutional%20Neural%20Networks%2C%20lmKAN-based%20CNNs%0Acut%20inference%20FLOPs%20at%20matched%20accuracy%20by%201.6-2.1x%20and%20by%201.7x%20on%20the%20CIFAR-10%0Aand%20ImageNet-1k%20datasets%2C%20respectively.%20Our%20code%2C%20including%20dedicated%20CUDA%0Akernels%2C%20is%20available%20online%20at%20https%3A//github.com/schwallergroup/lmkan.%0A&entry.1838667208=http%3A//arxiv.org/abs/2509.07103v2&entry.124074799=Read"},
{"title": "On Universality of Deep Equivariant Networks", "author": "Marco Pacini and Mircea Petrache and Bruno Lepri and Shubhendu Trivedi and Robin Walters", "abstract": "  Universality results for equivariant neural networks remain rare. Those that\ndo exist typically hold only in restrictive settings: either they rely on\nregular or higher-order tensor representations, leading to impractically\nhigh-dimensional hidden spaces, or they target specialized architectures, often\nconfined to the invariant setting. This work develops a more general account.\nFor invariant networks, we establish a universality theorem under separation\nconstraints, showing that the addition of a fully connected readout layer\nsecures approximation within the class of separation-constrained continuous\nfunctions. For equivariant networks, where results are even scarcer, we\ndemonstrate that standard separability notions are inadequate and introduce the\nsharper criterion of $\\textit{entry-wise separability}$. We show that with\nsufficient depth or with the addition of appropriate readout layers,\nequivariant networks attain universality within the entry-wise separable\nregime. Together with prior results showing the failure of universality for\nshallow models, our findings identify depth and readout layers as a decisive\nmechanism for universality, additionally offering a unified perspective that\nsubsumes and extends earlier specialized results.\n", "link": "http://arxiv.org/abs/2510.15814v1", "date": "2025-10-17", "relevancy": 1.8544, "topK": [{"title": "Boosted dynamic neural networks", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/26302", "similarity": 0.4857}, {"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.4517}, {"title": "DDM-NET: End-to-end learning of keypoint feature Detection, Description and Matching for 3D localization", "link": "https://arxiv.org/abs/2212.04575", "similarity": 0.4463}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20On%20Universality%20of%20Deep%20Equivariant%20Networks&body=Title%3A%20On%20Universality%20of%20Deep%20Equivariant%20Networks%0AAuthor%3A%20Marco%20Pacini%20and%20Mircea%20Petrache%20and%20Bruno%20Lepri%20and%20Shubhendu%20Trivedi%20and%20Robin%20Walters%0AAbstract%3A%20%20%20Universality%20results%20for%20equivariant%20neural%20networks%20remain%20rare.%20Those%20that%0Ado%20exist%20typically%20hold%20only%20in%20restrictive%20settings%3A%20either%20they%20rely%20on%0Aregular%20or%20higher-order%20tensor%20representations%2C%20leading%20to%20impractically%0Ahigh-dimensional%20hidden%20spaces%2C%20or%20they%20target%20specialized%20architectures%2C%20often%0Aconfined%20to%20the%20invariant%20setting.%20This%20work%20develops%20a%20more%20general%20account.%0AFor%20invariant%20networks%2C%20we%20establish%20a%20universality%20theorem%20under%20separation%0Aconstraints%2C%20showing%20that%20the%20addition%20of%20a%20fully%20connected%20readout%20layer%0Asecures%20approximation%20within%20the%20class%20of%20separation-constrained%20continuous%0Afunctions.%20For%20equivariant%20networks%2C%20where%20results%20are%20even%20scarcer%2C%20we%0Ademonstrate%20that%20standard%20separability%20notions%20are%20inadequate%20and%20introduce%20the%0Asharper%20criterion%20of%20%24%5Ctextit%7Bentry-wise%20separability%7D%24.%20We%20show%20that%20with%0Asufficient%20depth%20or%20with%20the%20addition%20of%20appropriate%20readout%20layers%2C%0Aequivariant%20networks%20attain%20universality%20within%20the%20entry-wise%20separable%0Aregime.%20Together%20with%20prior%20results%20showing%20the%20failure%20of%20universality%20for%0Ashallow%20models%2C%20our%20findings%20identify%20depth%20and%20readout%20layers%20as%20a%20decisive%0Amechanism%20for%20universality%2C%20additionally%20offering%20a%20unified%20perspective%20that%0Asubsumes%20and%20extends%20earlier%20specialized%20results.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2510.15814v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DOn%2520Universality%2520of%2520Deep%2520Equivariant%2520Networks%26entry.906535625%3DMarco%2520Pacini%2520and%2520Mircea%2520Petrache%2520and%2520Bruno%2520Lepri%2520and%2520Shubhendu%2520Trivedi%2520and%2520Robin%2520Walters%26entry.1292438233%3D%2520%2520Universality%2520results%2520for%2520equivariant%2520neural%2520networks%2520remain%2520rare.%2520Those%2520that%250Ado%2520exist%2520typically%2520hold%2520only%2520in%2520restrictive%2520settings%253A%2520either%2520they%2520rely%2520on%250Aregular%2520or%2520higher-order%2520tensor%2520representations%252C%2520leading%2520to%2520impractically%250Ahigh-dimensional%2520hidden%2520spaces%252C%2520or%2520they%2520target%2520specialized%2520architectures%252C%2520often%250Aconfined%2520to%2520the%2520invariant%2520setting.%2520This%2520work%2520develops%2520a%2520more%2520general%2520account.%250AFor%2520invariant%2520networks%252C%2520we%2520establish%2520a%2520universality%2520theorem%2520under%2520separation%250Aconstraints%252C%2520showing%2520that%2520the%2520addition%2520of%2520a%2520fully%2520connected%2520readout%2520layer%250Asecures%2520approximation%2520within%2520the%2520class%2520of%2520separation-constrained%2520continuous%250Afunctions.%2520For%2520equivariant%2520networks%252C%2520where%2520results%2520are%2520even%2520scarcer%252C%2520we%250Ademonstrate%2520that%2520standard%2520separability%2520notions%2520are%2520inadequate%2520and%2520introduce%2520the%250Asharper%2520criterion%2520of%2520%2524%255Ctextit%257Bentry-wise%2520separability%257D%2524.%2520We%2520show%2520that%2520with%250Asufficient%2520depth%2520or%2520with%2520the%2520addition%2520of%2520appropriate%2520readout%2520layers%252C%250Aequivariant%2520networks%2520attain%2520universality%2520within%2520the%2520entry-wise%2520separable%250Aregime.%2520Together%2520with%2520prior%2520results%2520showing%2520the%2520failure%2520of%2520universality%2520for%250Ashallow%2520models%252C%2520our%2520findings%2520identify%2520depth%2520and%2520readout%2520layers%2520as%2520a%2520decisive%250Amechanism%2520for%2520universality%252C%2520additionally%2520offering%2520a%2520unified%2520perspective%2520that%250Asubsumes%2520and%2520extends%2520earlier%2520specialized%2520results.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2510.15814v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=On%20Universality%20of%20Deep%20Equivariant%20Networks&entry.906535625=Marco%20Pacini%20and%20Mircea%20Petrache%20and%20Bruno%20Lepri%20and%20Shubhendu%20Trivedi%20and%20Robin%20Walters&entry.1292438233=%20%20Universality%20results%20for%20equivariant%20neural%20networks%20remain%20rare.%20Those%20that%0Ado%20exist%20typically%20hold%20only%20in%20restrictive%20settings%3A%20either%20they%20rely%20on%0Aregular%20or%20higher-order%20tensor%20representations%2C%20leading%20to%20impractically%0Ahigh-dimensional%20hidden%20spaces%2C%20or%20they%20target%20specialized%20architectures%2C%20often%0Aconfined%20to%20the%20invariant%20setting.%20This%20work%20develops%20a%20more%20general%20account.%0AFor%20invariant%20networks%2C%20we%20establish%20a%20universality%20theorem%20under%20separation%0Aconstraints%2C%20showing%20that%20the%20addition%20of%20a%20fully%20connected%20readout%20layer%0Asecures%20approximation%20within%20the%20class%20of%20separation-constrained%20continuous%0Afunctions.%20For%20equivariant%20networks%2C%20where%20results%20are%20even%20scarcer%2C%20we%0Ademonstrate%20that%20standard%20separability%20notions%20are%20inadequate%20and%20introduce%20the%0Asharper%20criterion%20of%20%24%5Ctextit%7Bentry-wise%20separability%7D%24.%20We%20show%20that%20with%0Asufficient%20depth%20or%20with%20the%20addition%20of%20appropriate%20readout%20layers%2C%0Aequivariant%20networks%20attain%20universality%20within%20the%20entry-wise%20separable%0Aregime.%20Together%20with%20prior%20results%20showing%20the%20failure%20of%20universality%20for%0Ashallow%20models%2C%20our%20findings%20identify%20depth%20and%20readout%20layers%20as%20a%20decisive%0Amechanism%20for%20universality%2C%20additionally%20offering%20a%20unified%20perspective%20that%0Asubsumes%20and%20extends%20earlier%20specialized%20results.%0A&entry.1838667208=http%3A//arxiv.org/abs/2510.15814v1&entry.124074799=Read"},
{"title": "Paper2Web: Let's Make Your Paper Alive!", "author": "Yuhang Chen and Tianpeng Lv and Siyi Zhang and Yixiang Yin and Yao Wan and Philip S. Yu and Dongping Chen", "abstract": "  Academic project websites can more effectively disseminate research when they\nclearly present core content and enable intuitive navigation and interaction.\nHowever, current approaches such as direct Large Language Model (LLM)\ngeneration, templates, or direct HTML conversion struggle to produce\nlayout-aware, interactive sites, and a comprehensive evaluation suite for this\ntask has been lacking. In this paper, we introduce Paper2Web, a benchmark\ndataset and multi-dimensional evaluation framework for assessing academic\nwebpage generation. It incorporates rule-based metrics like Connectivity,\nCompleteness and human-verified LLM-as-a-Judge (covering interactivity,\naesthetics, and informativeness), and PaperQuiz, which measures paper-level\nknowledge retention. We further present PWAgent, an autonomous pipeline that\nconverts scientific papers into interactive and multimedia-rich academic\nhomepages. The agent iteratively refines both content and layout through MCP\ntools that enhance emphasis, balance, and presentation quality. Our experiments\nshow that PWAgent consistently outperforms end-to-end baselines like\ntemplate-based webpages and arXiv/alphaXiv versions by a large margin while\nmaintaining low cost, achieving the Pareto-front in academic webpage\ngeneration.\n", "link": "http://arxiv.org/abs/2510.15842v1", "date": "2025-10-17", "relevancy": 1.8343, "topK": [{"title": "DressCode: Autoregressively Sewing and Generating Garments from Text\n  Guidance", "link": "http://arxiv.org/abs/2401.16465v3", "similarity": 0.4728}, {"title": "WorldExplorer: Towards Generating Fully Navigable 3D Scenes", "link": "http://arxiv.org/abs/2506.01799v2", "similarity": 0.454}, {"title": "PhysGen: Rigid-Body Physics-Grounded Image-to-Video Generation", "link": "http://arxiv.org/abs/2409.18964v1", "similarity": 0.4462}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Paper2Web%3A%20Let%27s%20Make%20Your%20Paper%20Alive%21&body=Title%3A%20Paper2Web%3A%20Let%27s%20Make%20Your%20Paper%20Alive%21%0AAuthor%3A%20Yuhang%20Chen%20and%20Tianpeng%20Lv%20and%20Siyi%20Zhang%20and%20Yixiang%20Yin%20and%20Yao%20Wan%20and%20Philip%20S.%20Yu%20and%20Dongping%20Chen%0AAbstract%3A%20%20%20Academic%20project%20websites%20can%20more%20effectively%20disseminate%20research%20when%20they%0Aclearly%20present%20core%20content%20and%20enable%20intuitive%20navigation%20and%20interaction.%0AHowever%2C%20current%20approaches%20such%20as%20direct%20Large%20Language%20Model%20%28LLM%29%0Ageneration%2C%20templates%2C%20or%20direct%20HTML%20conversion%20struggle%20to%20produce%0Alayout-aware%2C%20interactive%20sites%2C%20and%20a%20comprehensive%20evaluation%20suite%20for%20this%0Atask%20has%20been%20lacking.%20In%20this%20paper%2C%20we%20introduce%20Paper2Web%2C%20a%20benchmark%0Adataset%20and%20multi-dimensional%20evaluation%20framework%20for%20assessing%20academic%0Awebpage%20generation.%20It%20incorporates%20rule-based%20metrics%20like%20Connectivity%2C%0ACompleteness%20and%20human-verified%20LLM-as-a-Judge%20%28covering%20interactivity%2C%0Aaesthetics%2C%20and%20informativeness%29%2C%20and%20PaperQuiz%2C%20which%20measures%20paper-level%0Aknowledge%20retention.%20We%20further%20present%20PWAgent%2C%20an%20autonomous%20pipeline%20that%0Aconverts%20scientific%20papers%20into%20interactive%20and%20multimedia-rich%20academic%0Ahomepages.%20The%20agent%20iteratively%20refines%20both%20content%20and%20layout%20through%20MCP%0Atools%20that%20enhance%20emphasis%2C%20balance%2C%20and%20presentation%20quality.%20Our%20experiments%0Ashow%20that%20PWAgent%20consistently%20outperforms%20end-to-end%20baselines%20like%0Atemplate-based%20webpages%20and%20arXiv/alphaXiv%20versions%20by%20a%20large%20margin%20while%0Amaintaining%20low%20cost%2C%20achieving%20the%20Pareto-front%20in%20academic%20webpage%0Ageneration.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2510.15842v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DPaper2Web%253A%2520Let%2527s%2520Make%2520Your%2520Paper%2520Alive%2521%26entry.906535625%3DYuhang%2520Chen%2520and%2520Tianpeng%2520Lv%2520and%2520Siyi%2520Zhang%2520and%2520Yixiang%2520Yin%2520and%2520Yao%2520Wan%2520and%2520Philip%2520S.%2520Yu%2520and%2520Dongping%2520Chen%26entry.1292438233%3D%2520%2520Academic%2520project%2520websites%2520can%2520more%2520effectively%2520disseminate%2520research%2520when%2520they%250Aclearly%2520present%2520core%2520content%2520and%2520enable%2520intuitive%2520navigation%2520and%2520interaction.%250AHowever%252C%2520current%2520approaches%2520such%2520as%2520direct%2520Large%2520Language%2520Model%2520%2528LLM%2529%250Ageneration%252C%2520templates%252C%2520or%2520direct%2520HTML%2520conversion%2520struggle%2520to%2520produce%250Alayout-aware%252C%2520interactive%2520sites%252C%2520and%2520a%2520comprehensive%2520evaluation%2520suite%2520for%2520this%250Atask%2520has%2520been%2520lacking.%2520In%2520this%2520paper%252C%2520we%2520introduce%2520Paper2Web%252C%2520a%2520benchmark%250Adataset%2520and%2520multi-dimensional%2520evaluation%2520framework%2520for%2520assessing%2520academic%250Awebpage%2520generation.%2520It%2520incorporates%2520rule-based%2520metrics%2520like%2520Connectivity%252C%250ACompleteness%2520and%2520human-verified%2520LLM-as-a-Judge%2520%2528covering%2520interactivity%252C%250Aaesthetics%252C%2520and%2520informativeness%2529%252C%2520and%2520PaperQuiz%252C%2520which%2520measures%2520paper-level%250Aknowledge%2520retention.%2520We%2520further%2520present%2520PWAgent%252C%2520an%2520autonomous%2520pipeline%2520that%250Aconverts%2520scientific%2520papers%2520into%2520interactive%2520and%2520multimedia-rich%2520academic%250Ahomepages.%2520The%2520agent%2520iteratively%2520refines%2520both%2520content%2520and%2520layout%2520through%2520MCP%250Atools%2520that%2520enhance%2520emphasis%252C%2520balance%252C%2520and%2520presentation%2520quality.%2520Our%2520experiments%250Ashow%2520that%2520PWAgent%2520consistently%2520outperforms%2520end-to-end%2520baselines%2520like%250Atemplate-based%2520webpages%2520and%2520arXiv/alphaXiv%2520versions%2520by%2520a%2520large%2520margin%2520while%250Amaintaining%2520low%2520cost%252C%2520achieving%2520the%2520Pareto-front%2520in%2520academic%2520webpage%250Ageneration.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2510.15842v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Paper2Web%3A%20Let%27s%20Make%20Your%20Paper%20Alive%21&entry.906535625=Yuhang%20Chen%20and%20Tianpeng%20Lv%20and%20Siyi%20Zhang%20and%20Yixiang%20Yin%20and%20Yao%20Wan%20and%20Philip%20S.%20Yu%20and%20Dongping%20Chen&entry.1292438233=%20%20Academic%20project%20websites%20can%20more%20effectively%20disseminate%20research%20when%20they%0Aclearly%20present%20core%20content%20and%20enable%20intuitive%20navigation%20and%20interaction.%0AHowever%2C%20current%20approaches%20such%20as%20direct%20Large%20Language%20Model%20%28LLM%29%0Ageneration%2C%20templates%2C%20or%20direct%20HTML%20conversion%20struggle%20to%20produce%0Alayout-aware%2C%20interactive%20sites%2C%20and%20a%20comprehensive%20evaluation%20suite%20for%20this%0Atask%20has%20been%20lacking.%20In%20this%20paper%2C%20we%20introduce%20Paper2Web%2C%20a%20benchmark%0Adataset%20and%20multi-dimensional%20evaluation%20framework%20for%20assessing%20academic%0Awebpage%20generation.%20It%20incorporates%20rule-based%20metrics%20like%20Connectivity%2C%0ACompleteness%20and%20human-verified%20LLM-as-a-Judge%20%28covering%20interactivity%2C%0Aaesthetics%2C%20and%20informativeness%29%2C%20and%20PaperQuiz%2C%20which%20measures%20paper-level%0Aknowledge%20retention.%20We%20further%20present%20PWAgent%2C%20an%20autonomous%20pipeline%20that%0Aconverts%20scientific%20papers%20into%20interactive%20and%20multimedia-rich%20academic%0Ahomepages.%20The%20agent%20iteratively%20refines%20both%20content%20and%20layout%20through%20MCP%0Atools%20that%20enhance%20emphasis%2C%20balance%2C%20and%20presentation%20quality.%20Our%20experiments%0Ashow%20that%20PWAgent%20consistently%20outperforms%20end-to-end%20baselines%20like%0Atemplate-based%20webpages%20and%20arXiv/alphaXiv%20versions%20by%20a%20large%20margin%20while%0Amaintaining%20low%20cost%2C%20achieving%20the%20Pareto-front%20in%20academic%20webpage%0Ageneration.%0A&entry.1838667208=http%3A//arxiv.org/abs/2510.15842v1&entry.124074799=Read"},
{"title": "Operationalizing Automated Essay Scoring: A Human-Aware Approach", "author": "Yenisel Plasencia-Cala\u00f1a", "abstract": "  This paper explores the human-centric operationalization of Automated Essay\nScoring (AES) systems, addressing aspects beyond accuracy. We compare various\nmachine learning-based approaches with Large Language Models (LLMs) approaches,\nidentifying their strengths, similarities and differences. The study\ninvestigates key dimensions such as bias, robustness, and explainability,\nconsidered important for human-aware operationalization of AES systems. Our\nstudy shows that ML-based AES models outperform LLMs in accuracy but struggle\nwith explainability, whereas LLMs provide richer explanations. We also found\nthat both approaches struggle with bias and robustness to edge scores. By\nanalyzing these dimensions, the paper aims to identify challenges and\ntrade-offs between different methods, contributing to more reliable and\ntrustworthy AES methods.\n", "link": "http://arxiv.org/abs/2506.21603v2", "date": "2025-10-17", "relevancy": 1.8303, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.4584}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.4584}, {"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.4533}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Operationalizing%20Automated%20Essay%20Scoring%3A%20A%20Human-Aware%20Approach&body=Title%3A%20Operationalizing%20Automated%20Essay%20Scoring%3A%20A%20Human-Aware%20Approach%0AAuthor%3A%20Yenisel%20Plasencia-Cala%C3%B1a%0AAbstract%3A%20%20%20This%20paper%20explores%20the%20human-centric%20operationalization%20of%20Automated%20Essay%0AScoring%20%28AES%29%20systems%2C%20addressing%20aspects%20beyond%20accuracy.%20We%20compare%20various%0Amachine%20learning-based%20approaches%20with%20Large%20Language%20Models%20%28LLMs%29%20approaches%2C%0Aidentifying%20their%20strengths%2C%20similarities%20and%20differences.%20The%20study%0Ainvestigates%20key%20dimensions%20such%20as%20bias%2C%20robustness%2C%20and%20explainability%2C%0Aconsidered%20important%20for%20human-aware%20operationalization%20of%20AES%20systems.%20Our%0Astudy%20shows%20that%20ML-based%20AES%20models%20outperform%20LLMs%20in%20accuracy%20but%20struggle%0Awith%20explainability%2C%20whereas%20LLMs%20provide%20richer%20explanations.%20We%20also%20found%0Athat%20both%20approaches%20struggle%20with%20bias%20and%20robustness%20to%20edge%20scores.%20By%0Aanalyzing%20these%20dimensions%2C%20the%20paper%20aims%20to%20identify%20challenges%20and%0Atrade-offs%20between%20different%20methods%2C%20contributing%20to%20more%20reliable%20and%0Atrustworthy%20AES%20methods.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2506.21603v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DOperationalizing%2520Automated%2520Essay%2520Scoring%253A%2520A%2520Human-Aware%2520Approach%26entry.906535625%3DYenisel%2520Plasencia-Cala%25C3%25B1a%26entry.1292438233%3D%2520%2520This%2520paper%2520explores%2520the%2520human-centric%2520operationalization%2520of%2520Automated%2520Essay%250AScoring%2520%2528AES%2529%2520systems%252C%2520addressing%2520aspects%2520beyond%2520accuracy.%2520We%2520compare%2520various%250Amachine%2520learning-based%2520approaches%2520with%2520Large%2520Language%2520Models%2520%2528LLMs%2529%2520approaches%252C%250Aidentifying%2520their%2520strengths%252C%2520similarities%2520and%2520differences.%2520The%2520study%250Ainvestigates%2520key%2520dimensions%2520such%2520as%2520bias%252C%2520robustness%252C%2520and%2520explainability%252C%250Aconsidered%2520important%2520for%2520human-aware%2520operationalization%2520of%2520AES%2520systems.%2520Our%250Astudy%2520shows%2520that%2520ML-based%2520AES%2520models%2520outperform%2520LLMs%2520in%2520accuracy%2520but%2520struggle%250Awith%2520explainability%252C%2520whereas%2520LLMs%2520provide%2520richer%2520explanations.%2520We%2520also%2520found%250Athat%2520both%2520approaches%2520struggle%2520with%2520bias%2520and%2520robustness%2520to%2520edge%2520scores.%2520By%250Aanalyzing%2520these%2520dimensions%252C%2520the%2520paper%2520aims%2520to%2520identify%2520challenges%2520and%250Atrade-offs%2520between%2520different%2520methods%252C%2520contributing%2520to%2520more%2520reliable%2520and%250Atrustworthy%2520AES%2520methods.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2506.21603v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Operationalizing%20Automated%20Essay%20Scoring%3A%20A%20Human-Aware%20Approach&entry.906535625=Yenisel%20Plasencia-Cala%C3%B1a&entry.1292438233=%20%20This%20paper%20explores%20the%20human-centric%20operationalization%20of%20Automated%20Essay%0AScoring%20%28AES%29%20systems%2C%20addressing%20aspects%20beyond%20accuracy.%20We%20compare%20various%0Amachine%20learning-based%20approaches%20with%20Large%20Language%20Models%20%28LLMs%29%20approaches%2C%0Aidentifying%20their%20strengths%2C%20similarities%20and%20differences.%20The%20study%0Ainvestigates%20key%20dimensions%20such%20as%20bias%2C%20robustness%2C%20and%20explainability%2C%0Aconsidered%20important%20for%20human-aware%20operationalization%20of%20AES%20systems.%20Our%0Astudy%20shows%20that%20ML-based%20AES%20models%20outperform%20LLMs%20in%20accuracy%20but%20struggle%0Awith%20explainability%2C%20whereas%20LLMs%20provide%20richer%20explanations.%20We%20also%20found%0Athat%20both%20approaches%20struggle%20with%20bias%20and%20robustness%20to%20edge%20scores.%20By%0Aanalyzing%20these%20dimensions%2C%20the%20paper%20aims%20to%20identify%20challenges%20and%0Atrade-offs%20between%20different%20methods%2C%20contributing%20to%20more%20reliable%20and%0Atrustworthy%20AES%20methods.%0A&entry.1838667208=http%3A//arxiv.org/abs/2506.21603v2&entry.124074799=Read"},
{"title": "REX: Causal discovery based on machine learning and explainability\n  techniques", "author": "Jesus Renero and Idoia Ochoa and Roberto Maestre", "abstract": "  Explainable Artificial Intelligence (XAI) techniques hold significant\npotential for enhancing the causal discovery process, which is crucial for\nunderstanding complex systems in areas like healthcare, economics, and\nartificial intelligence. However, no causal discovery methods currently\nincorporate explainability into their models to derive the causal graphs. Thus,\nin this paper we explore this innovative approach, as it offers substantial\npotential and represents a promising new direction worth investigating.\nSpecifically, we introduce ReX, a causal discovery method that leverages\nmachine learning (ML) models coupled with explainability techniques,\nspecifically Shapley values, to identify and interpret significant causal\nrelationships among variables. Comparative evaluations on synthetic datasets\ncomprising continuous tabular data reveal that ReX outperforms state-of-the-art\ncausal discovery methods across diverse data generation processes, including\nnon-linear and additive noise models. Moreover, ReX was tested on the Sachs\nsingle-cell protein-signaling dataset, achieving a precision of 0.952 and\nrecovering key causal relationships with no incorrect edges. Taking together,\nthese results showcase ReX's effectiveness in accurately recovering true causal\nstructures while minimizing false positive predictions, its robustness across\ndiverse datasets, and its applicability to real-world problems. By combining ML\nand explainability techniques with causal discovery, ReX bridges the gap\nbetween predictive modeling and causal inference, offering an effective tool\nfor understanding complex causal structures.\n", "link": "http://arxiv.org/abs/2501.12706v3", "date": "2025-10-17", "relevancy": 1.8247, "topK": [{"title": "RoboEXP: Action-Conditioned Scene Graph via Interactive Exploration for\n  Robotic Manipulation", "link": "http://arxiv.org/abs/2402.15487v1", "similarity": 0.4734}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.4527}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.4527}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20REX%3A%20Causal%20discovery%20based%20on%20machine%20learning%20and%20explainability%0A%20%20techniques&body=Title%3A%20REX%3A%20Causal%20discovery%20based%20on%20machine%20learning%20and%20explainability%0A%20%20techniques%0AAuthor%3A%20Jesus%20Renero%20and%20Idoia%20Ochoa%20and%20Roberto%20Maestre%0AAbstract%3A%20%20%20Explainable%20Artificial%20Intelligence%20%28XAI%29%20techniques%20hold%20significant%0Apotential%20for%20enhancing%20the%20causal%20discovery%20process%2C%20which%20is%20crucial%20for%0Aunderstanding%20complex%20systems%20in%20areas%20like%20healthcare%2C%20economics%2C%20and%0Aartificial%20intelligence.%20However%2C%20no%20causal%20discovery%20methods%20currently%0Aincorporate%20explainability%20into%20their%20models%20to%20derive%20the%20causal%20graphs.%20Thus%2C%0Ain%20this%20paper%20we%20explore%20this%20innovative%20approach%2C%20as%20it%20offers%20substantial%0Apotential%20and%20represents%20a%20promising%20new%20direction%20worth%20investigating.%0ASpecifically%2C%20we%20introduce%20ReX%2C%20a%20causal%20discovery%20method%20that%20leverages%0Amachine%20learning%20%28ML%29%20models%20coupled%20with%20explainability%20techniques%2C%0Aspecifically%20Shapley%20values%2C%20to%20identify%20and%20interpret%20significant%20causal%0Arelationships%20among%20variables.%20Comparative%20evaluations%20on%20synthetic%20datasets%0Acomprising%20continuous%20tabular%20data%20reveal%20that%20ReX%20outperforms%20state-of-the-art%0Acausal%20discovery%20methods%20across%20diverse%20data%20generation%20processes%2C%20including%0Anon-linear%20and%20additive%20noise%20models.%20Moreover%2C%20ReX%20was%20tested%20on%20the%20Sachs%0Asingle-cell%20protein-signaling%20dataset%2C%20achieving%20a%20precision%20of%200.952%20and%0Arecovering%20key%20causal%20relationships%20with%20no%20incorrect%20edges.%20Taking%20together%2C%0Athese%20results%20showcase%20ReX%27s%20effectiveness%20in%20accurately%20recovering%20true%20causal%0Astructures%20while%20minimizing%20false%20positive%20predictions%2C%20its%20robustness%20across%0Adiverse%20datasets%2C%20and%20its%20applicability%20to%20real-world%20problems.%20By%20combining%20ML%0Aand%20explainability%20techniques%20with%20causal%20discovery%2C%20ReX%20bridges%20the%20gap%0Abetween%20predictive%20modeling%20and%20causal%20inference%2C%20offering%20an%20effective%20tool%0Afor%20understanding%20complex%20causal%20structures.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2501.12706v3%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DREX%253A%2520Causal%2520discovery%2520based%2520on%2520machine%2520learning%2520and%2520explainability%250A%2520%2520techniques%26entry.906535625%3DJesus%2520Renero%2520and%2520Idoia%2520Ochoa%2520and%2520Roberto%2520Maestre%26entry.1292438233%3D%2520%2520Explainable%2520Artificial%2520Intelligence%2520%2528XAI%2529%2520techniques%2520hold%2520significant%250Apotential%2520for%2520enhancing%2520the%2520causal%2520discovery%2520process%252C%2520which%2520is%2520crucial%2520for%250Aunderstanding%2520complex%2520systems%2520in%2520areas%2520like%2520healthcare%252C%2520economics%252C%2520and%250Aartificial%2520intelligence.%2520However%252C%2520no%2520causal%2520discovery%2520methods%2520currently%250Aincorporate%2520explainability%2520into%2520their%2520models%2520to%2520derive%2520the%2520causal%2520graphs.%2520Thus%252C%250Ain%2520this%2520paper%2520we%2520explore%2520this%2520innovative%2520approach%252C%2520as%2520it%2520offers%2520substantial%250Apotential%2520and%2520represents%2520a%2520promising%2520new%2520direction%2520worth%2520investigating.%250ASpecifically%252C%2520we%2520introduce%2520ReX%252C%2520a%2520causal%2520discovery%2520method%2520that%2520leverages%250Amachine%2520learning%2520%2528ML%2529%2520models%2520coupled%2520with%2520explainability%2520techniques%252C%250Aspecifically%2520Shapley%2520values%252C%2520to%2520identify%2520and%2520interpret%2520significant%2520causal%250Arelationships%2520among%2520variables.%2520Comparative%2520evaluations%2520on%2520synthetic%2520datasets%250Acomprising%2520continuous%2520tabular%2520data%2520reveal%2520that%2520ReX%2520outperforms%2520state-of-the-art%250Acausal%2520discovery%2520methods%2520across%2520diverse%2520data%2520generation%2520processes%252C%2520including%250Anon-linear%2520and%2520additive%2520noise%2520models.%2520Moreover%252C%2520ReX%2520was%2520tested%2520on%2520the%2520Sachs%250Asingle-cell%2520protein-signaling%2520dataset%252C%2520achieving%2520a%2520precision%2520of%25200.952%2520and%250Arecovering%2520key%2520causal%2520relationships%2520with%2520no%2520incorrect%2520edges.%2520Taking%2520together%252C%250Athese%2520results%2520showcase%2520ReX%2527s%2520effectiveness%2520in%2520accurately%2520recovering%2520true%2520causal%250Astructures%2520while%2520minimizing%2520false%2520positive%2520predictions%252C%2520its%2520robustness%2520across%250Adiverse%2520datasets%252C%2520and%2520its%2520applicability%2520to%2520real-world%2520problems.%2520By%2520combining%2520ML%250Aand%2520explainability%2520techniques%2520with%2520causal%2520discovery%252C%2520ReX%2520bridges%2520the%2520gap%250Abetween%2520predictive%2520modeling%2520and%2520causal%2520inference%252C%2520offering%2520an%2520effective%2520tool%250Afor%2520understanding%2520complex%2520causal%2520structures.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2501.12706v3%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=REX%3A%20Causal%20discovery%20based%20on%20machine%20learning%20and%20explainability%0A%20%20techniques&entry.906535625=Jesus%20Renero%20and%20Idoia%20Ochoa%20and%20Roberto%20Maestre&entry.1292438233=%20%20Explainable%20Artificial%20Intelligence%20%28XAI%29%20techniques%20hold%20significant%0Apotential%20for%20enhancing%20the%20causal%20discovery%20process%2C%20which%20is%20crucial%20for%0Aunderstanding%20complex%20systems%20in%20areas%20like%20healthcare%2C%20economics%2C%20and%0Aartificial%20intelligence.%20However%2C%20no%20causal%20discovery%20methods%20currently%0Aincorporate%20explainability%20into%20their%20models%20to%20derive%20the%20causal%20graphs.%20Thus%2C%0Ain%20this%20paper%20we%20explore%20this%20innovative%20approach%2C%20as%20it%20offers%20substantial%0Apotential%20and%20represents%20a%20promising%20new%20direction%20worth%20investigating.%0ASpecifically%2C%20we%20introduce%20ReX%2C%20a%20causal%20discovery%20method%20that%20leverages%0Amachine%20learning%20%28ML%29%20models%20coupled%20with%20explainability%20techniques%2C%0Aspecifically%20Shapley%20values%2C%20to%20identify%20and%20interpret%20significant%20causal%0Arelationships%20among%20variables.%20Comparative%20evaluations%20on%20synthetic%20datasets%0Acomprising%20continuous%20tabular%20data%20reveal%20that%20ReX%20outperforms%20state-of-the-art%0Acausal%20discovery%20methods%20across%20diverse%20data%20generation%20processes%2C%20including%0Anon-linear%20and%20additive%20noise%20models.%20Moreover%2C%20ReX%20was%20tested%20on%20the%20Sachs%0Asingle-cell%20protein-signaling%20dataset%2C%20achieving%20a%20precision%20of%200.952%20and%0Arecovering%20key%20causal%20relationships%20with%20no%20incorrect%20edges.%20Taking%20together%2C%0Athese%20results%20showcase%20ReX%27s%20effectiveness%20in%20accurately%20recovering%20true%20causal%0Astructures%20while%20minimizing%20false%20positive%20predictions%2C%20its%20robustness%20across%0Adiverse%20datasets%2C%20and%20its%20applicability%20to%20real-world%20problems.%20By%20combining%20ML%0Aand%20explainability%20techniques%20with%20causal%20discovery%2C%20ReX%20bridges%20the%20gap%0Abetween%20predictive%20modeling%20and%20causal%20inference%2C%20offering%20an%20effective%20tool%0Afor%20understanding%20complex%20causal%20structures.%0A&entry.1838667208=http%3A//arxiv.org/abs/2501.12706v3&entry.124074799=Read"},
{"title": "Fast and Compact Tsetlin Machine Inference on CPUs Using\n  Instruction-Level Optimization", "author": "Yefan Zeng and Shengyu Duan and Rishad Shafik and Alex Yakovlev", "abstract": "  The Tsetlin Machine (TM) offers high-speed inference on resource-constrained\ndevices such as CPUs. Its logic-driven operations naturally lend themselves to\nparallel execution on modern CPU architectures. Motivated by this, we propose\nan efficient software implementation of the TM by leveraging instruction-level\nbitwise operations for compact model representation and accelerated processing.\nTo further improve inference speed, we introduce an early exit mechanism, which\nexploits the TM's AND-based clause evaluation to avoid unnecessary\ncomputations. Building upon this, we propose a literal Reorder strategy\ndesigned to maximize the likelihood of early exits. This strategy is applied\nduring a post-training, pre-inference stage through statistical analysis of all\nliterals and the corresponding actions of their associated Tsetlin Automata\n(TA), introducing negligible runtime overhead. Experimental results using the\ngem5 simulator with an ARM processor show that our optimized implementation\nreduces inference time by up to 96.71% compared to the conventional\ninteger-based TM implementations while maintaining comparable code density.\n", "link": "http://arxiv.org/abs/2510.15653v1", "date": "2025-10-17", "relevancy": 1.8204, "topK": [{"title": "SDXS: Real-Time One-Step Latent Diffusion Models with Image Conditions", "link": "http://arxiv.org/abs/2403.16627v1", "similarity": 0.4788}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.453}, {"title": "ToDo: Token Downsampling for Efficient Generation of High-Resolution\n  Images", "link": "http://arxiv.org/abs/2402.13573v2", "similarity": 0.4478}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Fast%20and%20Compact%20Tsetlin%20Machine%20Inference%20on%20CPUs%20Using%0A%20%20Instruction-Level%20Optimization&body=Title%3A%20Fast%20and%20Compact%20Tsetlin%20Machine%20Inference%20on%20CPUs%20Using%0A%20%20Instruction-Level%20Optimization%0AAuthor%3A%20Yefan%20Zeng%20and%20Shengyu%20Duan%20and%20Rishad%20Shafik%20and%20Alex%20Yakovlev%0AAbstract%3A%20%20%20The%20Tsetlin%20Machine%20%28TM%29%20offers%20high-speed%20inference%20on%20resource-constrained%0Adevices%20such%20as%20CPUs.%20Its%20logic-driven%20operations%20naturally%20lend%20themselves%20to%0Aparallel%20execution%20on%20modern%20CPU%20architectures.%20Motivated%20by%20this%2C%20we%20propose%0Aan%20efficient%20software%20implementation%20of%20the%20TM%20by%20leveraging%20instruction-level%0Abitwise%20operations%20for%20compact%20model%20representation%20and%20accelerated%20processing.%0ATo%20further%20improve%20inference%20speed%2C%20we%20introduce%20an%20early%20exit%20mechanism%2C%20which%0Aexploits%20the%20TM%27s%20AND-based%20clause%20evaluation%20to%20avoid%20unnecessary%0Acomputations.%20Building%20upon%20this%2C%20we%20propose%20a%20literal%20Reorder%20strategy%0Adesigned%20to%20maximize%20the%20likelihood%20of%20early%20exits.%20This%20strategy%20is%20applied%0Aduring%20a%20post-training%2C%20pre-inference%20stage%20through%20statistical%20analysis%20of%20all%0Aliterals%20and%20the%20corresponding%20actions%20of%20their%20associated%20Tsetlin%20Automata%0A%28TA%29%2C%20introducing%20negligible%20runtime%20overhead.%20Experimental%20results%20using%20the%0Agem5%20simulator%20with%20an%20ARM%20processor%20show%20that%20our%20optimized%20implementation%0Areduces%20inference%20time%20by%20up%20to%2096.71%25%20compared%20to%20the%20conventional%0Ainteger-based%20TM%20implementations%20while%20maintaining%20comparable%20code%20density.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2510.15653v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DFast%2520and%2520Compact%2520Tsetlin%2520Machine%2520Inference%2520on%2520CPUs%2520Using%250A%2520%2520Instruction-Level%2520Optimization%26entry.906535625%3DYefan%2520Zeng%2520and%2520Shengyu%2520Duan%2520and%2520Rishad%2520Shafik%2520and%2520Alex%2520Yakovlev%26entry.1292438233%3D%2520%2520The%2520Tsetlin%2520Machine%2520%2528TM%2529%2520offers%2520high-speed%2520inference%2520on%2520resource-constrained%250Adevices%2520such%2520as%2520CPUs.%2520Its%2520logic-driven%2520operations%2520naturally%2520lend%2520themselves%2520to%250Aparallel%2520execution%2520on%2520modern%2520CPU%2520architectures.%2520Motivated%2520by%2520this%252C%2520we%2520propose%250Aan%2520efficient%2520software%2520implementation%2520of%2520the%2520TM%2520by%2520leveraging%2520instruction-level%250Abitwise%2520operations%2520for%2520compact%2520model%2520representation%2520and%2520accelerated%2520processing.%250ATo%2520further%2520improve%2520inference%2520speed%252C%2520we%2520introduce%2520an%2520early%2520exit%2520mechanism%252C%2520which%250Aexploits%2520the%2520TM%2527s%2520AND-based%2520clause%2520evaluation%2520to%2520avoid%2520unnecessary%250Acomputations.%2520Building%2520upon%2520this%252C%2520we%2520propose%2520a%2520literal%2520Reorder%2520strategy%250Adesigned%2520to%2520maximize%2520the%2520likelihood%2520of%2520early%2520exits.%2520This%2520strategy%2520is%2520applied%250Aduring%2520a%2520post-training%252C%2520pre-inference%2520stage%2520through%2520statistical%2520analysis%2520of%2520all%250Aliterals%2520and%2520the%2520corresponding%2520actions%2520of%2520their%2520associated%2520Tsetlin%2520Automata%250A%2528TA%2529%252C%2520introducing%2520negligible%2520runtime%2520overhead.%2520Experimental%2520results%2520using%2520the%250Agem5%2520simulator%2520with%2520an%2520ARM%2520processor%2520show%2520that%2520our%2520optimized%2520implementation%250Areduces%2520inference%2520time%2520by%2520up%2520to%252096.71%2525%2520compared%2520to%2520the%2520conventional%250Ainteger-based%2520TM%2520implementations%2520while%2520maintaining%2520comparable%2520code%2520density.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2510.15653v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Fast%20and%20Compact%20Tsetlin%20Machine%20Inference%20on%20CPUs%20Using%0A%20%20Instruction-Level%20Optimization&entry.906535625=Yefan%20Zeng%20and%20Shengyu%20Duan%20and%20Rishad%20Shafik%20and%20Alex%20Yakovlev&entry.1292438233=%20%20The%20Tsetlin%20Machine%20%28TM%29%20offers%20high-speed%20inference%20on%20resource-constrained%0Adevices%20such%20as%20CPUs.%20Its%20logic-driven%20operations%20naturally%20lend%20themselves%20to%0Aparallel%20execution%20on%20modern%20CPU%20architectures.%20Motivated%20by%20this%2C%20we%20propose%0Aan%20efficient%20software%20implementation%20of%20the%20TM%20by%20leveraging%20instruction-level%0Abitwise%20operations%20for%20compact%20model%20representation%20and%20accelerated%20processing.%0ATo%20further%20improve%20inference%20speed%2C%20we%20introduce%20an%20early%20exit%20mechanism%2C%20which%0Aexploits%20the%20TM%27s%20AND-based%20clause%20evaluation%20to%20avoid%20unnecessary%0Acomputations.%20Building%20upon%20this%2C%20we%20propose%20a%20literal%20Reorder%20strategy%0Adesigned%20to%20maximize%20the%20likelihood%20of%20early%20exits.%20This%20strategy%20is%20applied%0Aduring%20a%20post-training%2C%20pre-inference%20stage%20through%20statistical%20analysis%20of%20all%0Aliterals%20and%20the%20corresponding%20actions%20of%20their%20associated%20Tsetlin%20Automata%0A%28TA%29%2C%20introducing%20negligible%20runtime%20overhead.%20Experimental%20results%20using%20the%0Agem5%20simulator%20with%20an%20ARM%20processor%20show%20that%20our%20optimized%20implementation%0Areduces%20inference%20time%20by%20up%20to%2096.71%25%20compared%20to%20the%20conventional%0Ainteger-based%20TM%20implementations%20while%20maintaining%20comparable%20code%20density.%0A&entry.1838667208=http%3A//arxiv.org/abs/2510.15653v1&entry.124074799=Read"},
{"title": "FinChain: A Symbolic Benchmark for Verifiable Chain-of-Thought Financial\n  Reasoning", "author": "Zhuohan Xie and Daniil Orel and Rushil Thareja and Dhruv Sahnan and Hachem Madmoun and Fan Zhang and Debopriyo Banerjee and Georgi Georgiev and Xueqing Peng and Lingfei Qian and Jimin Huang and Jinyan Su and Aaryamonvikram Singh and Rui Xing and Rania Elbadry and Chen Xu and Haonan Li and Fajri Koto and Ivan Koychev and Tanmoy Chakraborty and Yuxia Wang and Salem Lahlou and Veselin Stoyanov and Sophia Ananiadou and Preslav Nakov", "abstract": "  Multi-step symbolic reasoning is essential for robust financial analysis;\nyet, current benchmarks largely overlook this capability. Existing datasets\nsuch as FinQA and ConvFinQA emphasize final numerical answers while neglecting\nthe intermediate reasoning required for transparency and verification. To\naddress this gap, we introduce FinChain, the first benchmark specifically\ndesigned for verifiable Chain-of-Thought (CoT) evaluation in finance. FinChain\nspans 58 topics across 12 financial domains, each represented by parameterized\nsymbolic templates with executable Python traces that enable fully\nmachine-verifiable reasoning and scalable, contamination-free data generation.\nTo assess reasoning capacity, we propose ChainEval, a dynamic alignment metric\nthat jointly evaluates both the final-answer correctness and the step-level\nreasoning consistency. Evaluating 26 leading LLMs reveals that even frontier\nproprietary systems exhibit clear limitations in symbolic financial reasoning,\nwhile domain-adapted and math-enhanced fine-tuned models substantially narrow\nthis gap. Overall, FinChain exposes persistent weaknesses in multi-step\nfinancial reasoning and provides a foundation for developing trustworthy,\ninterpretable, and verifiable financial AI.\n", "link": "http://arxiv.org/abs/2506.02515v2", "date": "2025-10-17", "relevancy": 1.8128, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.4564}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.4564}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.4371}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20FinChain%3A%20A%20Symbolic%20Benchmark%20for%20Verifiable%20Chain-of-Thought%20Financial%0A%20%20Reasoning&body=Title%3A%20FinChain%3A%20A%20Symbolic%20Benchmark%20for%20Verifiable%20Chain-of-Thought%20Financial%0A%20%20Reasoning%0AAuthor%3A%20Zhuohan%20Xie%20and%20Daniil%20Orel%20and%20Rushil%20Thareja%20and%20Dhruv%20Sahnan%20and%20Hachem%20Madmoun%20and%20Fan%20Zhang%20and%20Debopriyo%20Banerjee%20and%20Georgi%20Georgiev%20and%20Xueqing%20Peng%20and%20Lingfei%20Qian%20and%20Jimin%20Huang%20and%20Jinyan%20Su%20and%20Aaryamonvikram%20Singh%20and%20Rui%20Xing%20and%20Rania%20Elbadry%20and%20Chen%20Xu%20and%20Haonan%20Li%20and%20Fajri%20Koto%20and%20Ivan%20Koychev%20and%20Tanmoy%20Chakraborty%20and%20Yuxia%20Wang%20and%20Salem%20Lahlou%20and%20Veselin%20Stoyanov%20and%20Sophia%20Ananiadou%20and%20Preslav%20Nakov%0AAbstract%3A%20%20%20Multi-step%20symbolic%20reasoning%20is%20essential%20for%20robust%20financial%20analysis%3B%0Ayet%2C%20current%20benchmarks%20largely%20overlook%20this%20capability.%20Existing%20datasets%0Asuch%20as%20FinQA%20and%20ConvFinQA%20emphasize%20final%20numerical%20answers%20while%20neglecting%0Athe%20intermediate%20reasoning%20required%20for%20transparency%20and%20verification.%20To%0Aaddress%20this%20gap%2C%20we%20introduce%20FinChain%2C%20the%20first%20benchmark%20specifically%0Adesigned%20for%20verifiable%20Chain-of-Thought%20%28CoT%29%20evaluation%20in%20finance.%20FinChain%0Aspans%2058%20topics%20across%2012%20financial%20domains%2C%20each%20represented%20by%20parameterized%0Asymbolic%20templates%20with%20executable%20Python%20traces%20that%20enable%20fully%0Amachine-verifiable%20reasoning%20and%20scalable%2C%20contamination-free%20data%20generation.%0ATo%20assess%20reasoning%20capacity%2C%20we%20propose%20ChainEval%2C%20a%20dynamic%20alignment%20metric%0Athat%20jointly%20evaluates%20both%20the%20final-answer%20correctness%20and%20the%20step-level%0Areasoning%20consistency.%20Evaluating%2026%20leading%20LLMs%20reveals%20that%20even%20frontier%0Aproprietary%20systems%20exhibit%20clear%20limitations%20in%20symbolic%20financial%20reasoning%2C%0Awhile%20domain-adapted%20and%20math-enhanced%20fine-tuned%20models%20substantially%20narrow%0Athis%20gap.%20Overall%2C%20FinChain%20exposes%20persistent%20weaknesses%20in%20multi-step%0Afinancial%20reasoning%20and%20provides%20a%20foundation%20for%20developing%20trustworthy%2C%0Ainterpretable%2C%20and%20verifiable%20financial%20AI.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2506.02515v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DFinChain%253A%2520A%2520Symbolic%2520Benchmark%2520for%2520Verifiable%2520Chain-of-Thought%2520Financial%250A%2520%2520Reasoning%26entry.906535625%3DZhuohan%2520Xie%2520and%2520Daniil%2520Orel%2520and%2520Rushil%2520Thareja%2520and%2520Dhruv%2520Sahnan%2520and%2520Hachem%2520Madmoun%2520and%2520Fan%2520Zhang%2520and%2520Debopriyo%2520Banerjee%2520and%2520Georgi%2520Georgiev%2520and%2520Xueqing%2520Peng%2520and%2520Lingfei%2520Qian%2520and%2520Jimin%2520Huang%2520and%2520Jinyan%2520Su%2520and%2520Aaryamonvikram%2520Singh%2520and%2520Rui%2520Xing%2520and%2520Rania%2520Elbadry%2520and%2520Chen%2520Xu%2520and%2520Haonan%2520Li%2520and%2520Fajri%2520Koto%2520and%2520Ivan%2520Koychev%2520and%2520Tanmoy%2520Chakraborty%2520and%2520Yuxia%2520Wang%2520and%2520Salem%2520Lahlou%2520and%2520Veselin%2520Stoyanov%2520and%2520Sophia%2520Ananiadou%2520and%2520Preslav%2520Nakov%26entry.1292438233%3D%2520%2520Multi-step%2520symbolic%2520reasoning%2520is%2520essential%2520for%2520robust%2520financial%2520analysis%253B%250Ayet%252C%2520current%2520benchmarks%2520largely%2520overlook%2520this%2520capability.%2520Existing%2520datasets%250Asuch%2520as%2520FinQA%2520and%2520ConvFinQA%2520emphasize%2520final%2520numerical%2520answers%2520while%2520neglecting%250Athe%2520intermediate%2520reasoning%2520required%2520for%2520transparency%2520and%2520verification.%2520To%250Aaddress%2520this%2520gap%252C%2520we%2520introduce%2520FinChain%252C%2520the%2520first%2520benchmark%2520specifically%250Adesigned%2520for%2520verifiable%2520Chain-of-Thought%2520%2528CoT%2529%2520evaluation%2520in%2520finance.%2520FinChain%250Aspans%252058%2520topics%2520across%252012%2520financial%2520domains%252C%2520each%2520represented%2520by%2520parameterized%250Asymbolic%2520templates%2520with%2520executable%2520Python%2520traces%2520that%2520enable%2520fully%250Amachine-verifiable%2520reasoning%2520and%2520scalable%252C%2520contamination-free%2520data%2520generation.%250ATo%2520assess%2520reasoning%2520capacity%252C%2520we%2520propose%2520ChainEval%252C%2520a%2520dynamic%2520alignment%2520metric%250Athat%2520jointly%2520evaluates%2520both%2520the%2520final-answer%2520correctness%2520and%2520the%2520step-level%250Areasoning%2520consistency.%2520Evaluating%252026%2520leading%2520LLMs%2520reveals%2520that%2520even%2520frontier%250Aproprietary%2520systems%2520exhibit%2520clear%2520limitations%2520in%2520symbolic%2520financial%2520reasoning%252C%250Awhile%2520domain-adapted%2520and%2520math-enhanced%2520fine-tuned%2520models%2520substantially%2520narrow%250Athis%2520gap.%2520Overall%252C%2520FinChain%2520exposes%2520persistent%2520weaknesses%2520in%2520multi-step%250Afinancial%2520reasoning%2520and%2520provides%2520a%2520foundation%2520for%2520developing%2520trustworthy%252C%250Ainterpretable%252C%2520and%2520verifiable%2520financial%2520AI.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2506.02515v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=FinChain%3A%20A%20Symbolic%20Benchmark%20for%20Verifiable%20Chain-of-Thought%20Financial%0A%20%20Reasoning&entry.906535625=Zhuohan%20Xie%20and%20Daniil%20Orel%20and%20Rushil%20Thareja%20and%20Dhruv%20Sahnan%20and%20Hachem%20Madmoun%20and%20Fan%20Zhang%20and%20Debopriyo%20Banerjee%20and%20Georgi%20Georgiev%20and%20Xueqing%20Peng%20and%20Lingfei%20Qian%20and%20Jimin%20Huang%20and%20Jinyan%20Su%20and%20Aaryamonvikram%20Singh%20and%20Rui%20Xing%20and%20Rania%20Elbadry%20and%20Chen%20Xu%20and%20Haonan%20Li%20and%20Fajri%20Koto%20and%20Ivan%20Koychev%20and%20Tanmoy%20Chakraborty%20and%20Yuxia%20Wang%20and%20Salem%20Lahlou%20and%20Veselin%20Stoyanov%20and%20Sophia%20Ananiadou%20and%20Preslav%20Nakov&entry.1292438233=%20%20Multi-step%20symbolic%20reasoning%20is%20essential%20for%20robust%20financial%20analysis%3B%0Ayet%2C%20current%20benchmarks%20largely%20overlook%20this%20capability.%20Existing%20datasets%0Asuch%20as%20FinQA%20and%20ConvFinQA%20emphasize%20final%20numerical%20answers%20while%20neglecting%0Athe%20intermediate%20reasoning%20required%20for%20transparency%20and%20verification.%20To%0Aaddress%20this%20gap%2C%20we%20introduce%20FinChain%2C%20the%20first%20benchmark%20specifically%0Adesigned%20for%20verifiable%20Chain-of-Thought%20%28CoT%29%20evaluation%20in%20finance.%20FinChain%0Aspans%2058%20topics%20across%2012%20financial%20domains%2C%20each%20represented%20by%20parameterized%0Asymbolic%20templates%20with%20executable%20Python%20traces%20that%20enable%20fully%0Amachine-verifiable%20reasoning%20and%20scalable%2C%20contamination-free%20data%20generation.%0ATo%20assess%20reasoning%20capacity%2C%20we%20propose%20ChainEval%2C%20a%20dynamic%20alignment%20metric%0Athat%20jointly%20evaluates%20both%20the%20final-answer%20correctness%20and%20the%20step-level%0Areasoning%20consistency.%20Evaluating%2026%20leading%20LLMs%20reveals%20that%20even%20frontier%0Aproprietary%20systems%20exhibit%20clear%20limitations%20in%20symbolic%20financial%20reasoning%2C%0Awhile%20domain-adapted%20and%20math-enhanced%20fine-tuned%20models%20substantially%20narrow%0Athis%20gap.%20Overall%2C%20FinChain%20exposes%20persistent%20weaknesses%20in%20multi-step%0Afinancial%20reasoning%20and%20provides%20a%20foundation%20for%20developing%20trustworthy%2C%0Ainterpretable%2C%20and%20verifiable%20financial%20AI.%0A&entry.1838667208=http%3A//arxiv.org/abs/2506.02515v2&entry.124074799=Read"},
{"title": "A Split-Client Approach to Second-Order Optimization", "author": "El Mahdi Chayti and Martin Jaggi", "abstract": "  Second-order methods promise faster convergence but are rarely used in\npractice because Hessian computations and decompositions are far more expensive\nthan gradients. We propose a \\emph{split-client} framework where gradients and\ncurvature are computed asynchronously by separate clients. This abstraction\ncaptures realistic delays and inexact Hessian updates while avoiding the manual\ntuning required by Lazy Hessian methods. Focusing on cubic regularization, we\nshow that our approach retains strong convergence guarantees and achieves a\nprovable wall-clock speedup of order $\\sqrt{\\tau}$, where $\\tau$ is the\nrelative time needed to compute and decompose the Hessian compared to a\ngradient step. Since $\\tau$ can be orders of magnitude larger than one in\nhigh-dimensional problems, this improvement is practically significant.\nExperiments on synthetic and real datasets confirm the theory: asynchronous\ncurvature consistently outperforms vanilla and Lazy Hessian baselines, while\nmaintaining second-order accuracy.\n", "link": "http://arxiv.org/abs/2510.15714v1", "date": "2025-10-17", "relevancy": 1.8075, "topK": [{"title": "RTG-SLAM: Real-time 3D Reconstruction at Scale using Gaussian Splatting", "link": "http://arxiv.org/abs/2404.19706v1", "similarity": 0.4633}, {"title": "Dynamic Gaussians Mesh: Consistent Mesh Reconstruction from Monocular\n  Videos", "link": "http://arxiv.org/abs/2404.12379v2", "similarity": 0.4465}, {"title": "Thinking Outside the BBox: Unconstrained Generative Object Compositing", "link": "http://arxiv.org/abs/2409.04559v2", "similarity": 0.4369}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20A%20Split-Client%20Approach%20to%20Second-Order%20Optimization&body=Title%3A%20A%20Split-Client%20Approach%20to%20Second-Order%20Optimization%0AAuthor%3A%20El%20Mahdi%20Chayti%20and%20Martin%20Jaggi%0AAbstract%3A%20%20%20Second-order%20methods%20promise%20faster%20convergence%20but%20are%20rarely%20used%20in%0Apractice%20because%20Hessian%20computations%20and%20decompositions%20are%20far%20more%20expensive%0Athan%20gradients.%20We%20propose%20a%20%5Cemph%7Bsplit-client%7D%20framework%20where%20gradients%20and%0Acurvature%20are%20computed%20asynchronously%20by%20separate%20clients.%20This%20abstraction%0Acaptures%20realistic%20delays%20and%20inexact%20Hessian%20updates%20while%20avoiding%20the%20manual%0Atuning%20required%20by%20Lazy%20Hessian%20methods.%20Focusing%20on%20cubic%20regularization%2C%20we%0Ashow%20that%20our%20approach%20retains%20strong%20convergence%20guarantees%20and%20achieves%20a%0Aprovable%20wall-clock%20speedup%20of%20order%20%24%5Csqrt%7B%5Ctau%7D%24%2C%20where%20%24%5Ctau%24%20is%20the%0Arelative%20time%20needed%20to%20compute%20and%20decompose%20the%20Hessian%20compared%20to%20a%0Agradient%20step.%20Since%20%24%5Ctau%24%20can%20be%20orders%20of%20magnitude%20larger%20than%20one%20in%0Ahigh-dimensional%20problems%2C%20this%20improvement%20is%20practically%20significant.%0AExperiments%20on%20synthetic%20and%20real%20datasets%20confirm%20the%20theory%3A%20asynchronous%0Acurvature%20consistently%20outperforms%20vanilla%20and%20Lazy%20Hessian%20baselines%2C%20while%0Amaintaining%20second-order%20accuracy.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2510.15714v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DA%2520Split-Client%2520Approach%2520to%2520Second-Order%2520Optimization%26entry.906535625%3DEl%2520Mahdi%2520Chayti%2520and%2520Martin%2520Jaggi%26entry.1292438233%3D%2520%2520Second-order%2520methods%2520promise%2520faster%2520convergence%2520but%2520are%2520rarely%2520used%2520in%250Apractice%2520because%2520Hessian%2520computations%2520and%2520decompositions%2520are%2520far%2520more%2520expensive%250Athan%2520gradients.%2520We%2520propose%2520a%2520%255Cemph%257Bsplit-client%257D%2520framework%2520where%2520gradients%2520and%250Acurvature%2520are%2520computed%2520asynchronously%2520by%2520separate%2520clients.%2520This%2520abstraction%250Acaptures%2520realistic%2520delays%2520and%2520inexact%2520Hessian%2520updates%2520while%2520avoiding%2520the%2520manual%250Atuning%2520required%2520by%2520Lazy%2520Hessian%2520methods.%2520Focusing%2520on%2520cubic%2520regularization%252C%2520we%250Ashow%2520that%2520our%2520approach%2520retains%2520strong%2520convergence%2520guarantees%2520and%2520achieves%2520a%250Aprovable%2520wall-clock%2520speedup%2520of%2520order%2520%2524%255Csqrt%257B%255Ctau%257D%2524%252C%2520where%2520%2524%255Ctau%2524%2520is%2520the%250Arelative%2520time%2520needed%2520to%2520compute%2520and%2520decompose%2520the%2520Hessian%2520compared%2520to%2520a%250Agradient%2520step.%2520Since%2520%2524%255Ctau%2524%2520can%2520be%2520orders%2520of%2520magnitude%2520larger%2520than%2520one%2520in%250Ahigh-dimensional%2520problems%252C%2520this%2520improvement%2520is%2520practically%2520significant.%250AExperiments%2520on%2520synthetic%2520and%2520real%2520datasets%2520confirm%2520the%2520theory%253A%2520asynchronous%250Acurvature%2520consistently%2520outperforms%2520vanilla%2520and%2520Lazy%2520Hessian%2520baselines%252C%2520while%250Amaintaining%2520second-order%2520accuracy.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2510.15714v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=A%20Split-Client%20Approach%20to%20Second-Order%20Optimization&entry.906535625=El%20Mahdi%20Chayti%20and%20Martin%20Jaggi&entry.1292438233=%20%20Second-order%20methods%20promise%20faster%20convergence%20but%20are%20rarely%20used%20in%0Apractice%20because%20Hessian%20computations%20and%20decompositions%20are%20far%20more%20expensive%0Athan%20gradients.%20We%20propose%20a%20%5Cemph%7Bsplit-client%7D%20framework%20where%20gradients%20and%0Acurvature%20are%20computed%20asynchronously%20by%20separate%20clients.%20This%20abstraction%0Acaptures%20realistic%20delays%20and%20inexact%20Hessian%20updates%20while%20avoiding%20the%20manual%0Atuning%20required%20by%20Lazy%20Hessian%20methods.%20Focusing%20on%20cubic%20regularization%2C%20we%0Ashow%20that%20our%20approach%20retains%20strong%20convergence%20guarantees%20and%20achieves%20a%0Aprovable%20wall-clock%20speedup%20of%20order%20%24%5Csqrt%7B%5Ctau%7D%24%2C%20where%20%24%5Ctau%24%20is%20the%0Arelative%20time%20needed%20to%20compute%20and%20decompose%20the%20Hessian%20compared%20to%20a%0Agradient%20step.%20Since%20%24%5Ctau%24%20can%20be%20orders%20of%20magnitude%20larger%20than%20one%20in%0Ahigh-dimensional%20problems%2C%20this%20improvement%20is%20practically%20significant.%0AExperiments%20on%20synthetic%20and%20real%20datasets%20confirm%20the%20theory%3A%20asynchronous%0Acurvature%20consistently%20outperforms%20vanilla%20and%20Lazy%20Hessian%20baselines%2C%20while%0Amaintaining%20second-order%20accuracy.%0A&entry.1838667208=http%3A//arxiv.org/abs/2510.15714v1&entry.124074799=Read"},
{"title": "PokeeResearch: Effective Deep Research via Reinforcement Learning from\n  AI Feedback and Robust Reasoning Scaffold", "author": "Yi Wan and Jiuqi Wang and Liam Li and Jinsong Liu and Ruihao Zhu and Zheqing Zhu", "abstract": "  Tool-augmented large language models (LLMs) are emerging as deep research\nagents, systems that decompose complex queries, retrieve external evidence, and\nsynthesize grounded responses. Yet current agents remain limited by shallow\nretrieval, weak alignment metrics, and brittle tool-use behavior. We introduce\nPokeeResearch-7B, a 7B-parameter deep research agent built under a unified\nreinforcement learning framework for robustness, alignment, and scalability.\nPokeeResearch-7B is trained by an annotation-free Reinforcement Learning from\nAI Feedback (RLAIF) framework to optimize policies using LLM-based reward\nsignals that capture factual accuracy, citation faithfulness, and instruction\nadherence. A chain-of-thought-driven multi-call reasoning scaffold further\nenhances robustness through self-verification and adaptive recovery from tool\nfailures. Among 10 popular deep research benchmarks, PokeeResearch-7B achieves\nstate-of-the-art performance among 7B-scale deep research agents. This\nhighlights that careful reinforcement learning and reasoning design can produce\nefficient, resilient, and research-grade AI agents. The model and inference\ncode is open-sourced under MIT license at\nhttps://github.com/Pokee-AI/PokeeResearchOSS.\n", "link": "http://arxiv.org/abs/2510.15862v1", "date": "2025-10-17", "relevancy": 1.5327, "topK": [{"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.5205}, {"title": "RoboEXP: Action-Conditioned Scene Graph via Interactive Exploration for\n  Robotic Manipulation", "link": "http://arxiv.org/abs/2402.15487v1", "similarity": 0.5098}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5075}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20PokeeResearch%3A%20Effective%20Deep%20Research%20via%20Reinforcement%20Learning%20from%0A%20%20AI%20Feedback%20and%20Robust%20Reasoning%20Scaffold&body=Title%3A%20PokeeResearch%3A%20Effective%20Deep%20Research%20via%20Reinforcement%20Learning%20from%0A%20%20AI%20Feedback%20and%20Robust%20Reasoning%20Scaffold%0AAuthor%3A%20Yi%20Wan%20and%20Jiuqi%20Wang%20and%20Liam%20Li%20and%20Jinsong%20Liu%20and%20Ruihao%20Zhu%20and%20Zheqing%20Zhu%0AAbstract%3A%20%20%20Tool-augmented%20large%20language%20models%20%28LLMs%29%20are%20emerging%20as%20deep%20research%0Aagents%2C%20systems%20that%20decompose%20complex%20queries%2C%20retrieve%20external%20evidence%2C%20and%0Asynthesize%20grounded%20responses.%20Yet%20current%20agents%20remain%20limited%20by%20shallow%0Aretrieval%2C%20weak%20alignment%20metrics%2C%20and%20brittle%20tool-use%20behavior.%20We%20introduce%0APokeeResearch-7B%2C%20a%207B-parameter%20deep%20research%20agent%20built%20under%20a%20unified%0Areinforcement%20learning%20framework%20for%20robustness%2C%20alignment%2C%20and%20scalability.%0APokeeResearch-7B%20is%20trained%20by%20an%20annotation-free%20Reinforcement%20Learning%20from%0AAI%20Feedback%20%28RLAIF%29%20framework%20to%20optimize%20policies%20using%20LLM-based%20reward%0Asignals%20that%20capture%20factual%20accuracy%2C%20citation%20faithfulness%2C%20and%20instruction%0Aadherence.%20A%20chain-of-thought-driven%20multi-call%20reasoning%20scaffold%20further%0Aenhances%20robustness%20through%20self-verification%20and%20adaptive%20recovery%20from%20tool%0Afailures.%20Among%2010%20popular%20deep%20research%20benchmarks%2C%20PokeeResearch-7B%20achieves%0Astate-of-the-art%20performance%20among%207B-scale%20deep%20research%20agents.%20This%0Ahighlights%20that%20careful%20reinforcement%20learning%20and%20reasoning%20design%20can%20produce%0Aefficient%2C%20resilient%2C%20and%20research-grade%20AI%20agents.%20The%20model%20and%20inference%0Acode%20is%20open-sourced%20under%20MIT%20license%20at%0Ahttps%3A//github.com/Pokee-AI/PokeeResearchOSS.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2510.15862v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DPokeeResearch%253A%2520Effective%2520Deep%2520Research%2520via%2520Reinforcement%2520Learning%2520from%250A%2520%2520AI%2520Feedback%2520and%2520Robust%2520Reasoning%2520Scaffold%26entry.906535625%3DYi%2520Wan%2520and%2520Jiuqi%2520Wang%2520and%2520Liam%2520Li%2520and%2520Jinsong%2520Liu%2520and%2520Ruihao%2520Zhu%2520and%2520Zheqing%2520Zhu%26entry.1292438233%3D%2520%2520Tool-augmented%2520large%2520language%2520models%2520%2528LLMs%2529%2520are%2520emerging%2520as%2520deep%2520research%250Aagents%252C%2520systems%2520that%2520decompose%2520complex%2520queries%252C%2520retrieve%2520external%2520evidence%252C%2520and%250Asynthesize%2520grounded%2520responses.%2520Yet%2520current%2520agents%2520remain%2520limited%2520by%2520shallow%250Aretrieval%252C%2520weak%2520alignment%2520metrics%252C%2520and%2520brittle%2520tool-use%2520behavior.%2520We%2520introduce%250APokeeResearch-7B%252C%2520a%25207B-parameter%2520deep%2520research%2520agent%2520built%2520under%2520a%2520unified%250Areinforcement%2520learning%2520framework%2520for%2520robustness%252C%2520alignment%252C%2520and%2520scalability.%250APokeeResearch-7B%2520is%2520trained%2520by%2520an%2520annotation-free%2520Reinforcement%2520Learning%2520from%250AAI%2520Feedback%2520%2528RLAIF%2529%2520framework%2520to%2520optimize%2520policies%2520using%2520LLM-based%2520reward%250Asignals%2520that%2520capture%2520factual%2520accuracy%252C%2520citation%2520faithfulness%252C%2520and%2520instruction%250Aadherence.%2520A%2520chain-of-thought-driven%2520multi-call%2520reasoning%2520scaffold%2520further%250Aenhances%2520robustness%2520through%2520self-verification%2520and%2520adaptive%2520recovery%2520from%2520tool%250Afailures.%2520Among%252010%2520popular%2520deep%2520research%2520benchmarks%252C%2520PokeeResearch-7B%2520achieves%250Astate-of-the-art%2520performance%2520among%25207B-scale%2520deep%2520research%2520agents.%2520This%250Ahighlights%2520that%2520careful%2520reinforcement%2520learning%2520and%2520reasoning%2520design%2520can%2520produce%250Aefficient%252C%2520resilient%252C%2520and%2520research-grade%2520AI%2520agents.%2520The%2520model%2520and%2520inference%250Acode%2520is%2520open-sourced%2520under%2520MIT%2520license%2520at%250Ahttps%253A//github.com/Pokee-AI/PokeeResearchOSS.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2510.15862v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=PokeeResearch%3A%20Effective%20Deep%20Research%20via%20Reinforcement%20Learning%20from%0A%20%20AI%20Feedback%20and%20Robust%20Reasoning%20Scaffold&entry.906535625=Yi%20Wan%20and%20Jiuqi%20Wang%20and%20Liam%20Li%20and%20Jinsong%20Liu%20and%20Ruihao%20Zhu%20and%20Zheqing%20Zhu&entry.1292438233=%20%20Tool-augmented%20large%20language%20models%20%28LLMs%29%20are%20emerging%20as%20deep%20research%0Aagents%2C%20systems%20that%20decompose%20complex%20queries%2C%20retrieve%20external%20evidence%2C%20and%0Asynthesize%20grounded%20responses.%20Yet%20current%20agents%20remain%20limited%20by%20shallow%0Aretrieval%2C%20weak%20alignment%20metrics%2C%20and%20brittle%20tool-use%20behavior.%20We%20introduce%0APokeeResearch-7B%2C%20a%207B-parameter%20deep%20research%20agent%20built%20under%20a%20unified%0Areinforcement%20learning%20framework%20for%20robustness%2C%20alignment%2C%20and%20scalability.%0APokeeResearch-7B%20is%20trained%20by%20an%20annotation-free%20Reinforcement%20Learning%20from%0AAI%20Feedback%20%28RLAIF%29%20framework%20to%20optimize%20policies%20using%20LLM-based%20reward%0Asignals%20that%20capture%20factual%20accuracy%2C%20citation%20faithfulness%2C%20and%20instruction%0Aadherence.%20A%20chain-of-thought-driven%20multi-call%20reasoning%20scaffold%20further%0Aenhances%20robustness%20through%20self-verification%20and%20adaptive%20recovery%20from%20tool%0Afailures.%20Among%2010%20popular%20deep%20research%20benchmarks%2C%20PokeeResearch-7B%20achieves%0Astate-of-the-art%20performance%20among%207B-scale%20deep%20research%20agents.%20This%0Ahighlights%20that%20careful%20reinforcement%20learning%20and%20reasoning%20design%20can%20produce%0Aefficient%2C%20resilient%2C%20and%20research-grade%20AI%20agents.%20The%20model%20and%20inference%0Acode%20is%20open-sourced%20under%20MIT%20license%20at%0Ahttps%3A//github.com/Pokee-AI/PokeeResearchOSS.%0A&entry.1838667208=http%3A//arxiv.org/abs/2510.15862v1&entry.124074799=Read"},
{"title": "CCD: Mitigating Hallucinations in Radiology MLLMs via Clinical\n  Contrastive Decoding", "author": "Xi Zhang and Zaiqiao Meng and Jake Lever and Edmond S. L. Ho", "abstract": "  Multimodal large language models (MLLMs) have recently achieved remarkable\nprogress in radiology by integrating visual perception with natural language\nunderstanding. However, they often generate clinically unsupported\ndescriptions, known as medical hallucinations, which pose serious risks in\nmedical applications that demand accuracy and image-grounded outputs. Through\nempirical analysis, we find that prompt-induced hallucinations remain prevalent\nin radiology MLLMs, largely due to over-sensitivity to clinical sections. To\naddress this, we introduce Clinical Contrastive Decoding (CCD), a training-free\nand retrieval-free inference framework that integrates structured clinical\nsignals from task-specific radiology expert models. CCD introduces a dual-stage\ncontrastive mechanism to refine token-level logits during generation, thereby\nenhancing clinical fidelity without modifying the base MLLM. Experiments on\nthree datasets and multiple models demonstrate that CCD consistently improves\noverall performance on radiology report generation (RRG). On the MIMIC-CXR\ndataset, it yields up to a 17% improvement in RadGraph-F1 when applied to\nstate-of-the-art RRG models. Our approach provides a lightweight and\ngeneralisable solution for mitigating medical hallucinations, effectively\nbridging expert models and MLLMs in radiology.\n", "link": "http://arxiv.org/abs/2509.23379v2", "date": "2025-10-17", "relevancy": 1.6139, "topK": [{"title": "SDXS: Real-Time One-Step Latent Diffusion Models with Image Conditions", "link": "http://arxiv.org/abs/2403.16627v1", "similarity": 0.5405}, {"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.5391}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5365}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20CCD%3A%20Mitigating%20Hallucinations%20in%20Radiology%20MLLMs%20via%20Clinical%0A%20%20Contrastive%20Decoding&body=Title%3A%20CCD%3A%20Mitigating%20Hallucinations%20in%20Radiology%20MLLMs%20via%20Clinical%0A%20%20Contrastive%20Decoding%0AAuthor%3A%20Xi%20Zhang%20and%20Zaiqiao%20Meng%20and%20Jake%20Lever%20and%20Edmond%20S.%20L.%20Ho%0AAbstract%3A%20%20%20Multimodal%20large%20language%20models%20%28MLLMs%29%20have%20recently%20achieved%20remarkable%0Aprogress%20in%20radiology%20by%20integrating%20visual%20perception%20with%20natural%20language%0Aunderstanding.%20However%2C%20they%20often%20generate%20clinically%20unsupported%0Adescriptions%2C%20known%20as%20medical%20hallucinations%2C%20which%20pose%20serious%20risks%20in%0Amedical%20applications%20that%20demand%20accuracy%20and%20image-grounded%20outputs.%20Through%0Aempirical%20analysis%2C%20we%20find%20that%20prompt-induced%20hallucinations%20remain%20prevalent%0Ain%20radiology%20MLLMs%2C%20largely%20due%20to%20over-sensitivity%20to%20clinical%20sections.%20To%0Aaddress%20this%2C%20we%20introduce%20Clinical%20Contrastive%20Decoding%20%28CCD%29%2C%20a%20training-free%0Aand%20retrieval-free%20inference%20framework%20that%20integrates%20structured%20clinical%0Asignals%20from%20task-specific%20radiology%20expert%20models.%20CCD%20introduces%20a%20dual-stage%0Acontrastive%20mechanism%20to%20refine%20token-level%20logits%20during%20generation%2C%20thereby%0Aenhancing%20clinical%20fidelity%20without%20modifying%20the%20base%20MLLM.%20Experiments%20on%0Athree%20datasets%20and%20multiple%20models%20demonstrate%20that%20CCD%20consistently%20improves%0Aoverall%20performance%20on%20radiology%20report%20generation%20%28RRG%29.%20On%20the%20MIMIC-CXR%0Adataset%2C%20it%20yields%20up%20to%20a%2017%25%20improvement%20in%20RadGraph-F1%20when%20applied%20to%0Astate-of-the-art%20RRG%20models.%20Our%20approach%20provides%20a%20lightweight%20and%0Ageneralisable%20solution%20for%20mitigating%20medical%20hallucinations%2C%20effectively%0Abridging%20expert%20models%20and%20MLLMs%20in%20radiology.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2509.23379v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DCCD%253A%2520Mitigating%2520Hallucinations%2520in%2520Radiology%2520MLLMs%2520via%2520Clinical%250A%2520%2520Contrastive%2520Decoding%26entry.906535625%3DXi%2520Zhang%2520and%2520Zaiqiao%2520Meng%2520and%2520Jake%2520Lever%2520and%2520Edmond%2520S.%2520L.%2520Ho%26entry.1292438233%3D%2520%2520Multimodal%2520large%2520language%2520models%2520%2528MLLMs%2529%2520have%2520recently%2520achieved%2520remarkable%250Aprogress%2520in%2520radiology%2520by%2520integrating%2520visual%2520perception%2520with%2520natural%2520language%250Aunderstanding.%2520However%252C%2520they%2520often%2520generate%2520clinically%2520unsupported%250Adescriptions%252C%2520known%2520as%2520medical%2520hallucinations%252C%2520which%2520pose%2520serious%2520risks%2520in%250Amedical%2520applications%2520that%2520demand%2520accuracy%2520and%2520image-grounded%2520outputs.%2520Through%250Aempirical%2520analysis%252C%2520we%2520find%2520that%2520prompt-induced%2520hallucinations%2520remain%2520prevalent%250Ain%2520radiology%2520MLLMs%252C%2520largely%2520due%2520to%2520over-sensitivity%2520to%2520clinical%2520sections.%2520To%250Aaddress%2520this%252C%2520we%2520introduce%2520Clinical%2520Contrastive%2520Decoding%2520%2528CCD%2529%252C%2520a%2520training-free%250Aand%2520retrieval-free%2520inference%2520framework%2520that%2520integrates%2520structured%2520clinical%250Asignals%2520from%2520task-specific%2520radiology%2520expert%2520models.%2520CCD%2520introduces%2520a%2520dual-stage%250Acontrastive%2520mechanism%2520to%2520refine%2520token-level%2520logits%2520during%2520generation%252C%2520thereby%250Aenhancing%2520clinical%2520fidelity%2520without%2520modifying%2520the%2520base%2520MLLM.%2520Experiments%2520on%250Athree%2520datasets%2520and%2520multiple%2520models%2520demonstrate%2520that%2520CCD%2520consistently%2520improves%250Aoverall%2520performance%2520on%2520radiology%2520report%2520generation%2520%2528RRG%2529.%2520On%2520the%2520MIMIC-CXR%250Adataset%252C%2520it%2520yields%2520up%2520to%2520a%252017%2525%2520improvement%2520in%2520RadGraph-F1%2520when%2520applied%2520to%250Astate-of-the-art%2520RRG%2520models.%2520Our%2520approach%2520provides%2520a%2520lightweight%2520and%250Ageneralisable%2520solution%2520for%2520mitigating%2520medical%2520hallucinations%252C%2520effectively%250Abridging%2520expert%2520models%2520and%2520MLLMs%2520in%2520radiology.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2509.23379v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=CCD%3A%20Mitigating%20Hallucinations%20in%20Radiology%20MLLMs%20via%20Clinical%0A%20%20Contrastive%20Decoding&entry.906535625=Xi%20Zhang%20and%20Zaiqiao%20Meng%20and%20Jake%20Lever%20and%20Edmond%20S.%20L.%20Ho&entry.1292438233=%20%20Multimodal%20large%20language%20models%20%28MLLMs%29%20have%20recently%20achieved%20remarkable%0Aprogress%20in%20radiology%20by%20integrating%20visual%20perception%20with%20natural%20language%0Aunderstanding.%20However%2C%20they%20often%20generate%20clinically%20unsupported%0Adescriptions%2C%20known%20as%20medical%20hallucinations%2C%20which%20pose%20serious%20risks%20in%0Amedical%20applications%20that%20demand%20accuracy%20and%20image-grounded%20outputs.%20Through%0Aempirical%20analysis%2C%20we%20find%20that%20prompt-induced%20hallucinations%20remain%20prevalent%0Ain%20radiology%20MLLMs%2C%20largely%20due%20to%20over-sensitivity%20to%20clinical%20sections.%20To%0Aaddress%20this%2C%20we%20introduce%20Clinical%20Contrastive%20Decoding%20%28CCD%29%2C%20a%20training-free%0Aand%20retrieval-free%20inference%20framework%20that%20integrates%20structured%20clinical%0Asignals%20from%20task-specific%20radiology%20expert%20models.%20CCD%20introduces%20a%20dual-stage%0Acontrastive%20mechanism%20to%20refine%20token-level%20logits%20during%20generation%2C%20thereby%0Aenhancing%20clinical%20fidelity%20without%20modifying%20the%20base%20MLLM.%20Experiments%20on%0Athree%20datasets%20and%20multiple%20models%20demonstrate%20that%20CCD%20consistently%20improves%0Aoverall%20performance%20on%20radiology%20report%20generation%20%28RRG%29.%20On%20the%20MIMIC-CXR%0Adataset%2C%20it%20yields%20up%20to%20a%2017%25%20improvement%20in%20RadGraph-F1%20when%20applied%20to%0Astate-of-the-art%20RRG%20models.%20Our%20approach%20provides%20a%20lightweight%20and%0Ageneralisable%20solution%20for%20mitigating%20medical%20hallucinations%2C%20effectively%0Abridging%20expert%20models%20and%20MLLMs%20in%20radiology.%0A&entry.1838667208=http%3A//arxiv.org/abs/2509.23379v2&entry.124074799=Read"},
{"title": "BLIP3o-NEXT: Next Frontier of Native Image Generation", "author": "Jiuhai Chen and Le Xue and Zhiyang Xu and Xichen Pan and Shusheng Yang and Can Qin and An Yan and Honglu Zhou and Zeyuan Chen and Lifu Huang and Tianyi Zhou and Junnan Li and Silvio Savarese and Caiming Xiong and Ran Xu", "abstract": "  We present BLIP3o-NEXT, a fully open-source foundation model in the BLIP3\nseries that advances the next frontier of native image generation. BLIP3o-NEXT\nunifies text-to-image generation and image editing within a single\narchitecture, demonstrating strong image generation and image editing\ncapabilities. In developing the state-of-the-art native image generation model,\nwe identify four key insights: (1) Most architectural choices yield comparable\nperformance; an architecture can be deemed effective provided it scales\nefficiently and supports fast inference; (2) The successful application of\nreinforcement learning can further push the frontier of native image\ngeneration; (3) Image editing still remains a challenging task, yet instruction\nfollowing and the consistency between generated and reference images can be\nsignificantly enhanced through post-training and data engine; (4) Data quality\nand scale continue to be decisive factors that determine the upper bound of\nmodel performance. Building upon these insights, BLIP3o-NEXT leverages an\nAutoregressive + Diffusion architecture in which an autoregressive model first\ngenerates discrete image tokens conditioned on multimodal inputs, whose hidden\nstates are then used as conditioning signals for a diffusion model to generate\nhigh-fidelity images. This architecture integrates the reasoning strength and\ninstruction following of autoregressive models with the fine-detail rendering\nability of diffusion models, achieving a new level of coherence and realism.\nExtensive evaluations of various text-to-image and image-editing benchmarks\nshow that BLIP3o-NEXT achieves superior performance over existing models.\n", "link": "http://arxiv.org/abs/2510.15857v1", "date": "2025-10-17", "relevancy": 1.1627, "topK": [{"title": "SDXS: Real-Time One-Step Latent Diffusion Models with Image Conditions", "link": "http://arxiv.org/abs/2403.16627v1", "similarity": 0.5957}, {"title": "Thinking Outside the BBox: Unconstrained Generative Object Compositing", "link": "http://arxiv.org/abs/2409.04559v2", "similarity": 0.5744}, {"title": "Scaling Rectified Flow Transformers for High-Resolution Image Synthesis", "link": "http://arxiv.org/abs/2403.03206v1", "similarity": 0.574}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20BLIP3o-NEXT%3A%20Next%20Frontier%20of%20Native%20Image%20Generation&body=Title%3A%20BLIP3o-NEXT%3A%20Next%20Frontier%20of%20Native%20Image%20Generation%0AAuthor%3A%20Jiuhai%20Chen%20and%20Le%20Xue%20and%20Zhiyang%20Xu%20and%20Xichen%20Pan%20and%20Shusheng%20Yang%20and%20Can%20Qin%20and%20An%20Yan%20and%20Honglu%20Zhou%20and%20Zeyuan%20Chen%20and%20Lifu%20Huang%20and%20Tianyi%20Zhou%20and%20Junnan%20Li%20and%20Silvio%20Savarese%20and%20Caiming%20Xiong%20and%20Ran%20Xu%0AAbstract%3A%20%20%20We%20present%20BLIP3o-NEXT%2C%20a%20fully%20open-source%20foundation%20model%20in%20the%20BLIP3%0Aseries%20that%20advances%20the%20next%20frontier%20of%20native%20image%20generation.%20BLIP3o-NEXT%0Aunifies%20text-to-image%20generation%20and%20image%20editing%20within%20a%20single%0Aarchitecture%2C%20demonstrating%20strong%20image%20generation%20and%20image%20editing%0Acapabilities.%20In%20developing%20the%20state-of-the-art%20native%20image%20generation%20model%2C%0Awe%20identify%20four%20key%20insights%3A%20%281%29%20Most%20architectural%20choices%20yield%20comparable%0Aperformance%3B%20an%20architecture%20can%20be%20deemed%20effective%20provided%20it%20scales%0Aefficiently%20and%20supports%20fast%20inference%3B%20%282%29%20The%20successful%20application%20of%0Areinforcement%20learning%20can%20further%20push%20the%20frontier%20of%20native%20image%0Ageneration%3B%20%283%29%20Image%20editing%20still%20remains%20a%20challenging%20task%2C%20yet%20instruction%0Afollowing%20and%20the%20consistency%20between%20generated%20and%20reference%20images%20can%20be%0Asignificantly%20enhanced%20through%20post-training%20and%20data%20engine%3B%20%284%29%20Data%20quality%0Aand%20scale%20continue%20to%20be%20decisive%20factors%20that%20determine%20the%20upper%20bound%20of%0Amodel%20performance.%20Building%20upon%20these%20insights%2C%20BLIP3o-NEXT%20leverages%20an%0AAutoregressive%20%2B%20Diffusion%20architecture%20in%20which%20an%20autoregressive%20model%20first%0Agenerates%20discrete%20image%20tokens%20conditioned%20on%20multimodal%20inputs%2C%20whose%20hidden%0Astates%20are%20then%20used%20as%20conditioning%20signals%20for%20a%20diffusion%20model%20to%20generate%0Ahigh-fidelity%20images.%20This%20architecture%20integrates%20the%20reasoning%20strength%20and%0Ainstruction%20following%20of%20autoregressive%20models%20with%20the%20fine-detail%20rendering%0Aability%20of%20diffusion%20models%2C%20achieving%20a%20new%20level%20of%20coherence%20and%20realism.%0AExtensive%20evaluations%20of%20various%20text-to-image%20and%20image-editing%20benchmarks%0Ashow%20that%20BLIP3o-NEXT%20achieves%20superior%20performance%20over%20existing%20models.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2510.15857v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DBLIP3o-NEXT%253A%2520Next%2520Frontier%2520of%2520Native%2520Image%2520Generation%26entry.906535625%3DJiuhai%2520Chen%2520and%2520Le%2520Xue%2520and%2520Zhiyang%2520Xu%2520and%2520Xichen%2520Pan%2520and%2520Shusheng%2520Yang%2520and%2520Can%2520Qin%2520and%2520An%2520Yan%2520and%2520Honglu%2520Zhou%2520and%2520Zeyuan%2520Chen%2520and%2520Lifu%2520Huang%2520and%2520Tianyi%2520Zhou%2520and%2520Junnan%2520Li%2520and%2520Silvio%2520Savarese%2520and%2520Caiming%2520Xiong%2520and%2520Ran%2520Xu%26entry.1292438233%3D%2520%2520We%2520present%2520BLIP3o-NEXT%252C%2520a%2520fully%2520open-source%2520foundation%2520model%2520in%2520the%2520BLIP3%250Aseries%2520that%2520advances%2520the%2520next%2520frontier%2520of%2520native%2520image%2520generation.%2520BLIP3o-NEXT%250Aunifies%2520text-to-image%2520generation%2520and%2520image%2520editing%2520within%2520a%2520single%250Aarchitecture%252C%2520demonstrating%2520strong%2520image%2520generation%2520and%2520image%2520editing%250Acapabilities.%2520In%2520developing%2520the%2520state-of-the-art%2520native%2520image%2520generation%2520model%252C%250Awe%2520identify%2520four%2520key%2520insights%253A%2520%25281%2529%2520Most%2520architectural%2520choices%2520yield%2520comparable%250Aperformance%253B%2520an%2520architecture%2520can%2520be%2520deemed%2520effective%2520provided%2520it%2520scales%250Aefficiently%2520and%2520supports%2520fast%2520inference%253B%2520%25282%2529%2520The%2520successful%2520application%2520of%250Areinforcement%2520learning%2520can%2520further%2520push%2520the%2520frontier%2520of%2520native%2520image%250Ageneration%253B%2520%25283%2529%2520Image%2520editing%2520still%2520remains%2520a%2520challenging%2520task%252C%2520yet%2520instruction%250Afollowing%2520and%2520the%2520consistency%2520between%2520generated%2520and%2520reference%2520images%2520can%2520be%250Asignificantly%2520enhanced%2520through%2520post-training%2520and%2520data%2520engine%253B%2520%25284%2529%2520Data%2520quality%250Aand%2520scale%2520continue%2520to%2520be%2520decisive%2520factors%2520that%2520determine%2520the%2520upper%2520bound%2520of%250Amodel%2520performance.%2520Building%2520upon%2520these%2520insights%252C%2520BLIP3o-NEXT%2520leverages%2520an%250AAutoregressive%2520%252B%2520Diffusion%2520architecture%2520in%2520which%2520an%2520autoregressive%2520model%2520first%250Agenerates%2520discrete%2520image%2520tokens%2520conditioned%2520on%2520multimodal%2520inputs%252C%2520whose%2520hidden%250Astates%2520are%2520then%2520used%2520as%2520conditioning%2520signals%2520for%2520a%2520diffusion%2520model%2520to%2520generate%250Ahigh-fidelity%2520images.%2520This%2520architecture%2520integrates%2520the%2520reasoning%2520strength%2520and%250Ainstruction%2520following%2520of%2520autoregressive%2520models%2520with%2520the%2520fine-detail%2520rendering%250Aability%2520of%2520diffusion%2520models%252C%2520achieving%2520a%2520new%2520level%2520of%2520coherence%2520and%2520realism.%250AExtensive%2520evaluations%2520of%2520various%2520text-to-image%2520and%2520image-editing%2520benchmarks%250Ashow%2520that%2520BLIP3o-NEXT%2520achieves%2520superior%2520performance%2520over%2520existing%2520models.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2510.15857v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=BLIP3o-NEXT%3A%20Next%20Frontier%20of%20Native%20Image%20Generation&entry.906535625=Jiuhai%20Chen%20and%20Le%20Xue%20and%20Zhiyang%20Xu%20and%20Xichen%20Pan%20and%20Shusheng%20Yang%20and%20Can%20Qin%20and%20An%20Yan%20and%20Honglu%20Zhou%20and%20Zeyuan%20Chen%20and%20Lifu%20Huang%20and%20Tianyi%20Zhou%20and%20Junnan%20Li%20and%20Silvio%20Savarese%20and%20Caiming%20Xiong%20and%20Ran%20Xu&entry.1292438233=%20%20We%20present%20BLIP3o-NEXT%2C%20a%20fully%20open-source%20foundation%20model%20in%20the%20BLIP3%0Aseries%20that%20advances%20the%20next%20frontier%20of%20native%20image%20generation.%20BLIP3o-NEXT%0Aunifies%20text-to-image%20generation%20and%20image%20editing%20within%20a%20single%0Aarchitecture%2C%20demonstrating%20strong%20image%20generation%20and%20image%20editing%0Acapabilities.%20In%20developing%20the%20state-of-the-art%20native%20image%20generation%20model%2C%0Awe%20identify%20four%20key%20insights%3A%20%281%29%20Most%20architectural%20choices%20yield%20comparable%0Aperformance%3B%20an%20architecture%20can%20be%20deemed%20effective%20provided%20it%20scales%0Aefficiently%20and%20supports%20fast%20inference%3B%20%282%29%20The%20successful%20application%20of%0Areinforcement%20learning%20can%20further%20push%20the%20frontier%20of%20native%20image%0Ageneration%3B%20%283%29%20Image%20editing%20still%20remains%20a%20challenging%20task%2C%20yet%20instruction%0Afollowing%20and%20the%20consistency%20between%20generated%20and%20reference%20images%20can%20be%0Asignificantly%20enhanced%20through%20post-training%20and%20data%20engine%3B%20%284%29%20Data%20quality%0Aand%20scale%20continue%20to%20be%20decisive%20factors%20that%20determine%20the%20upper%20bound%20of%0Amodel%20performance.%20Building%20upon%20these%20insights%2C%20BLIP3o-NEXT%20leverages%20an%0AAutoregressive%20%2B%20Diffusion%20architecture%20in%20which%20an%20autoregressive%20model%20first%0Agenerates%20discrete%20image%20tokens%20conditioned%20on%20multimodal%20inputs%2C%20whose%20hidden%0Astates%20are%20then%20used%20as%20conditioning%20signals%20for%20a%20diffusion%20model%20to%20generate%0Ahigh-fidelity%20images.%20This%20architecture%20integrates%20the%20reasoning%20strength%20and%0Ainstruction%20following%20of%20autoregressive%20models%20with%20the%20fine-detail%20rendering%0Aability%20of%20diffusion%20models%2C%20achieving%20a%20new%20level%20of%20coherence%20and%20realism.%0AExtensive%20evaluations%20of%20various%20text-to-image%20and%20image-editing%20benchmarks%0Ashow%20that%20BLIP3o-NEXT%20achieves%20superior%20performance%20over%20existing%20models.%0A&entry.1838667208=http%3A//arxiv.org/abs/2510.15857v1&entry.124074799=Read"},
{"title": "Beyond-Diagonal RIS Under Non-Idealities: Learning-Based Architecture\n  Discovery and Optimization", "author": "Binggui Zhou and Bruno Clerckx", "abstract": "  Beyond-diagonal reconfigurable intelligent surface (BD-RIS) has recently been\nintroduced to enable advanced control over electromagnetic waves to further\nincrease the benefits of traditional RIS in enhancing signal quality and\nimproving spectral and energy efficiency for next-generation wireless networks.\nA significant issue in designing and deploying BD-RIS is the tradeoff between\nits performance and circuit complexity. Despite some efforts in exploring\noptimal architectures with the lowest circuit complexities for ideal BD-RIS,\narchitecture discovery for non-ideal BD-RIS remains uninvestigated. Therefore,\nhow non-idealities and circuit complexity jointly affect the performance of\nBD-RIS remains unclear, making it difficult to achieve the performance -\ncircuit complexity tradeoff in the presence of non-idealities. Essentially,\narchitecture discovery for non-ideal BD-RIS faces challenges from both the\ncomputational complexity of global architecture search and the difficulty in\nachieving global optima. To tackle these challenges, we propose a\nlearning-based two-tier architecture discovery framework (LTTADF) consisting of\nan architecture generator and a performance optimizer to jointly discover\noptimal architectures of non-ideal BD-RIS given specific circuit complexities,\nwhich can effectively explore over a large architecture space while avoiding\ngetting trapped in poor local optima and thus achieving near-optimal solutions\nfor the performance optimization. Numerical results provide valuable insights\nfor deploying non-ideal BD-RIS considering the performance - circuit complexity\ntradeoff.\n", "link": "http://arxiv.org/abs/2510.15701v1", "date": "2025-10-17", "relevancy": 1.479, "topK": [{"title": "Self-supervised Photographic Image Layout Representation Learning", "link": "http://arxiv.org/abs/2403.03740v1", "similarity": 0.4987}, {"title": "SDXS: Real-Time One-Step Latent Diffusion Models with Image Conditions", "link": "http://arxiv.org/abs/2403.16627v1", "similarity": 0.4971}, {"title": "Scaling Rectified Flow Transformers for High-Resolution Image Synthesis", "link": "http://arxiv.org/abs/2403.03206v1", "similarity": 0.4745}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Beyond-Diagonal%20RIS%20Under%20Non-Idealities%3A%20Learning-Based%20Architecture%0A%20%20Discovery%20and%20Optimization&body=Title%3A%20Beyond-Diagonal%20RIS%20Under%20Non-Idealities%3A%20Learning-Based%20Architecture%0A%20%20Discovery%20and%20Optimization%0AAuthor%3A%20Binggui%20Zhou%20and%20Bruno%20Clerckx%0AAbstract%3A%20%20%20Beyond-diagonal%20reconfigurable%20intelligent%20surface%20%28BD-RIS%29%20has%20recently%20been%0Aintroduced%20to%20enable%20advanced%20control%20over%20electromagnetic%20waves%20to%20further%0Aincrease%20the%20benefits%20of%20traditional%20RIS%20in%20enhancing%20signal%20quality%20and%0Aimproving%20spectral%20and%20energy%20efficiency%20for%20next-generation%20wireless%20networks.%0AA%20significant%20issue%20in%20designing%20and%20deploying%20BD-RIS%20is%20the%20tradeoff%20between%0Aits%20performance%20and%20circuit%20complexity.%20Despite%20some%20efforts%20in%20exploring%0Aoptimal%20architectures%20with%20the%20lowest%20circuit%20complexities%20for%20ideal%20BD-RIS%2C%0Aarchitecture%20discovery%20for%20non-ideal%20BD-RIS%20remains%20uninvestigated.%20Therefore%2C%0Ahow%20non-idealities%20and%20circuit%20complexity%20jointly%20affect%20the%20performance%20of%0ABD-RIS%20remains%20unclear%2C%20making%20it%20difficult%20to%20achieve%20the%20performance%20-%0Acircuit%20complexity%20tradeoff%20in%20the%20presence%20of%20non-idealities.%20Essentially%2C%0Aarchitecture%20discovery%20for%20non-ideal%20BD-RIS%20faces%20challenges%20from%20both%20the%0Acomputational%20complexity%20of%20global%20architecture%20search%20and%20the%20difficulty%20in%0Aachieving%20global%20optima.%20To%20tackle%20these%20challenges%2C%20we%20propose%20a%0Alearning-based%20two-tier%20architecture%20discovery%20framework%20%28LTTADF%29%20consisting%20of%0Aan%20architecture%20generator%20and%20a%20performance%20optimizer%20to%20jointly%20discover%0Aoptimal%20architectures%20of%20non-ideal%20BD-RIS%20given%20specific%20circuit%20complexities%2C%0Awhich%20can%20effectively%20explore%20over%20a%20large%20architecture%20space%20while%20avoiding%0Agetting%20trapped%20in%20poor%20local%20optima%20and%20thus%20achieving%20near-optimal%20solutions%0Afor%20the%20performance%20optimization.%20Numerical%20results%20provide%20valuable%20insights%0Afor%20deploying%20non-ideal%20BD-RIS%20considering%20the%20performance%20-%20circuit%20complexity%0Atradeoff.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2510.15701v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DBeyond-Diagonal%2520RIS%2520Under%2520Non-Idealities%253A%2520Learning-Based%2520Architecture%250A%2520%2520Discovery%2520and%2520Optimization%26entry.906535625%3DBinggui%2520Zhou%2520and%2520Bruno%2520Clerckx%26entry.1292438233%3D%2520%2520Beyond-diagonal%2520reconfigurable%2520intelligent%2520surface%2520%2528BD-RIS%2529%2520has%2520recently%2520been%250Aintroduced%2520to%2520enable%2520advanced%2520control%2520over%2520electromagnetic%2520waves%2520to%2520further%250Aincrease%2520the%2520benefits%2520of%2520traditional%2520RIS%2520in%2520enhancing%2520signal%2520quality%2520and%250Aimproving%2520spectral%2520and%2520energy%2520efficiency%2520for%2520next-generation%2520wireless%2520networks.%250AA%2520significant%2520issue%2520in%2520designing%2520and%2520deploying%2520BD-RIS%2520is%2520the%2520tradeoff%2520between%250Aits%2520performance%2520and%2520circuit%2520complexity.%2520Despite%2520some%2520efforts%2520in%2520exploring%250Aoptimal%2520architectures%2520with%2520the%2520lowest%2520circuit%2520complexities%2520for%2520ideal%2520BD-RIS%252C%250Aarchitecture%2520discovery%2520for%2520non-ideal%2520BD-RIS%2520remains%2520uninvestigated.%2520Therefore%252C%250Ahow%2520non-idealities%2520and%2520circuit%2520complexity%2520jointly%2520affect%2520the%2520performance%2520of%250ABD-RIS%2520remains%2520unclear%252C%2520making%2520it%2520difficult%2520to%2520achieve%2520the%2520performance%2520-%250Acircuit%2520complexity%2520tradeoff%2520in%2520the%2520presence%2520of%2520non-idealities.%2520Essentially%252C%250Aarchitecture%2520discovery%2520for%2520non-ideal%2520BD-RIS%2520faces%2520challenges%2520from%2520both%2520the%250Acomputational%2520complexity%2520of%2520global%2520architecture%2520search%2520and%2520the%2520difficulty%2520in%250Aachieving%2520global%2520optima.%2520To%2520tackle%2520these%2520challenges%252C%2520we%2520propose%2520a%250Alearning-based%2520two-tier%2520architecture%2520discovery%2520framework%2520%2528LTTADF%2529%2520consisting%2520of%250Aan%2520architecture%2520generator%2520and%2520a%2520performance%2520optimizer%2520to%2520jointly%2520discover%250Aoptimal%2520architectures%2520of%2520non-ideal%2520BD-RIS%2520given%2520specific%2520circuit%2520complexities%252C%250Awhich%2520can%2520effectively%2520explore%2520over%2520a%2520large%2520architecture%2520space%2520while%2520avoiding%250Agetting%2520trapped%2520in%2520poor%2520local%2520optima%2520and%2520thus%2520achieving%2520near-optimal%2520solutions%250Afor%2520the%2520performance%2520optimization.%2520Numerical%2520results%2520provide%2520valuable%2520insights%250Afor%2520deploying%2520non-ideal%2520BD-RIS%2520considering%2520the%2520performance%2520-%2520circuit%2520complexity%250Atradeoff.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2510.15701v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Beyond-Diagonal%20RIS%20Under%20Non-Idealities%3A%20Learning-Based%20Architecture%0A%20%20Discovery%20and%20Optimization&entry.906535625=Binggui%20Zhou%20and%20Bruno%20Clerckx&entry.1292438233=%20%20Beyond-diagonal%20reconfigurable%20intelligent%20surface%20%28BD-RIS%29%20has%20recently%20been%0Aintroduced%20to%20enable%20advanced%20control%20over%20electromagnetic%20waves%20to%20further%0Aincrease%20the%20benefits%20of%20traditional%20RIS%20in%20enhancing%20signal%20quality%20and%0Aimproving%20spectral%20and%20energy%20efficiency%20for%20next-generation%20wireless%20networks.%0AA%20significant%20issue%20in%20designing%20and%20deploying%20BD-RIS%20is%20the%20tradeoff%20between%0Aits%20performance%20and%20circuit%20complexity.%20Despite%20some%20efforts%20in%20exploring%0Aoptimal%20architectures%20with%20the%20lowest%20circuit%20complexities%20for%20ideal%20BD-RIS%2C%0Aarchitecture%20discovery%20for%20non-ideal%20BD-RIS%20remains%20uninvestigated.%20Therefore%2C%0Ahow%20non-idealities%20and%20circuit%20complexity%20jointly%20affect%20the%20performance%20of%0ABD-RIS%20remains%20unclear%2C%20making%20it%20difficult%20to%20achieve%20the%20performance%20-%0Acircuit%20complexity%20tradeoff%20in%20the%20presence%20of%20non-idealities.%20Essentially%2C%0Aarchitecture%20discovery%20for%20non-ideal%20BD-RIS%20faces%20challenges%20from%20both%20the%0Acomputational%20complexity%20of%20global%20architecture%20search%20and%20the%20difficulty%20in%0Aachieving%20global%20optima.%20To%20tackle%20these%20challenges%2C%20we%20propose%20a%0Alearning-based%20two-tier%20architecture%20discovery%20framework%20%28LTTADF%29%20consisting%20of%0Aan%20architecture%20generator%20and%20a%20performance%20optimizer%20to%20jointly%20discover%0Aoptimal%20architectures%20of%20non-ideal%20BD-RIS%20given%20specific%20circuit%20complexities%2C%0Awhich%20can%20effectively%20explore%20over%20a%20large%20architecture%20space%20while%20avoiding%0Agetting%20trapped%20in%20poor%20local%20optima%20and%20thus%20achieving%20near-optimal%20solutions%0Afor%20the%20performance%20optimization.%20Numerical%20results%20provide%20valuable%20insights%0Afor%20deploying%20non-ideal%20BD-RIS%20considering%20the%20performance%20-%20circuit%20complexity%0Atradeoff.%0A&entry.1838667208=http%3A//arxiv.org/abs/2510.15701v1&entry.124074799=Read"},
{"title": "CQD-SHAP: Explainable Complex Query Answering via Shapley Values", "author": "Parsa Abbasi and Stefan Heindorf", "abstract": "  Complex query answering (CQA) goes beyond the well-studied link prediction\ntask by addressing more sophisticated queries that require multi-hop reasoning\nover incomplete knowledge graphs (KGs). Research on neural and neurosymbolic\nCQA methods is still an emerging field. Almost all of these methods can be\nregarded as black-box models, which may raise concerns about user trust.\nAlthough neurosymbolic approaches like CQD are slightly more interpretable,\nallowing intermediate results to be tracked, the importance of different parts\nof the query remains unexplained. In this paper, we propose CQD-SHAP, a novel\nframework that computes the contribution of each query part to the ranking of a\nspecific answer. This contribution explains the value of leveraging a neural\npredictor that can infer new knowledge from an incomplete KG, rather than a\nsymbolic approach relying solely on existing facts in the KG. CQD-SHAP is\nformulated based on Shapley values from cooperative game theory and satisfies\nall the fundamental Shapley axioms. Automated evaluation of these explanations\nin terms of necessary and sufficient explanations, and comparisons with various\nbaselines, shows the effectiveness of this approach for most query types.\n", "link": "http://arxiv.org/abs/2510.15623v1", "date": "2025-10-17", "relevancy": 1.3423, "topK": [{"title": "RoboEXP: Action-Conditioned Scene Graph via Interactive Exploration for\n  Robotic Manipulation", "link": "http://arxiv.org/abs/2402.15487v1", "similarity": 0.4556}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.4514}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.4426}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20CQD-SHAP%3A%20Explainable%20Complex%20Query%20Answering%20via%20Shapley%20Values&body=Title%3A%20CQD-SHAP%3A%20Explainable%20Complex%20Query%20Answering%20via%20Shapley%20Values%0AAuthor%3A%20Parsa%20Abbasi%20and%20Stefan%20Heindorf%0AAbstract%3A%20%20%20Complex%20query%20answering%20%28CQA%29%20goes%20beyond%20the%20well-studied%20link%20prediction%0Atask%20by%20addressing%20more%20sophisticated%20queries%20that%20require%20multi-hop%20reasoning%0Aover%20incomplete%20knowledge%20graphs%20%28KGs%29.%20Research%20on%20neural%20and%20neurosymbolic%0ACQA%20methods%20is%20still%20an%20emerging%20field.%20Almost%20all%20of%20these%20methods%20can%20be%0Aregarded%20as%20black-box%20models%2C%20which%20may%20raise%20concerns%20about%20user%20trust.%0AAlthough%20neurosymbolic%20approaches%20like%20CQD%20are%20slightly%20more%20interpretable%2C%0Aallowing%20intermediate%20results%20to%20be%20tracked%2C%20the%20importance%20of%20different%20parts%0Aof%20the%20query%20remains%20unexplained.%20In%20this%20paper%2C%20we%20propose%20CQD-SHAP%2C%20a%20novel%0Aframework%20that%20computes%20the%20contribution%20of%20each%20query%20part%20to%20the%20ranking%20of%20a%0Aspecific%20answer.%20This%20contribution%20explains%20the%20value%20of%20leveraging%20a%20neural%0Apredictor%20that%20can%20infer%20new%20knowledge%20from%20an%20incomplete%20KG%2C%20rather%20than%20a%0Asymbolic%20approach%20relying%20solely%20on%20existing%20facts%20in%20the%20KG.%20CQD-SHAP%20is%0Aformulated%20based%20on%20Shapley%20values%20from%20cooperative%20game%20theory%20and%20satisfies%0Aall%20the%20fundamental%20Shapley%20axioms.%20Automated%20evaluation%20of%20these%20explanations%0Ain%20terms%20of%20necessary%20and%20sufficient%20explanations%2C%20and%20comparisons%20with%20various%0Abaselines%2C%20shows%20the%20effectiveness%20of%20this%20approach%20for%20most%20query%20types.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2510.15623v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DCQD-SHAP%253A%2520Explainable%2520Complex%2520Query%2520Answering%2520via%2520Shapley%2520Values%26entry.906535625%3DParsa%2520Abbasi%2520and%2520Stefan%2520Heindorf%26entry.1292438233%3D%2520%2520Complex%2520query%2520answering%2520%2528CQA%2529%2520goes%2520beyond%2520the%2520well-studied%2520link%2520prediction%250Atask%2520by%2520addressing%2520more%2520sophisticated%2520queries%2520that%2520require%2520multi-hop%2520reasoning%250Aover%2520incomplete%2520knowledge%2520graphs%2520%2528KGs%2529.%2520Research%2520on%2520neural%2520and%2520neurosymbolic%250ACQA%2520methods%2520is%2520still%2520an%2520emerging%2520field.%2520Almost%2520all%2520of%2520these%2520methods%2520can%2520be%250Aregarded%2520as%2520black-box%2520models%252C%2520which%2520may%2520raise%2520concerns%2520about%2520user%2520trust.%250AAlthough%2520neurosymbolic%2520approaches%2520like%2520CQD%2520are%2520slightly%2520more%2520interpretable%252C%250Aallowing%2520intermediate%2520results%2520to%2520be%2520tracked%252C%2520the%2520importance%2520of%2520different%2520parts%250Aof%2520the%2520query%2520remains%2520unexplained.%2520In%2520this%2520paper%252C%2520we%2520propose%2520CQD-SHAP%252C%2520a%2520novel%250Aframework%2520that%2520computes%2520the%2520contribution%2520of%2520each%2520query%2520part%2520to%2520the%2520ranking%2520of%2520a%250Aspecific%2520answer.%2520This%2520contribution%2520explains%2520the%2520value%2520of%2520leveraging%2520a%2520neural%250Apredictor%2520that%2520can%2520infer%2520new%2520knowledge%2520from%2520an%2520incomplete%2520KG%252C%2520rather%2520than%2520a%250Asymbolic%2520approach%2520relying%2520solely%2520on%2520existing%2520facts%2520in%2520the%2520KG.%2520CQD-SHAP%2520is%250Aformulated%2520based%2520on%2520Shapley%2520values%2520from%2520cooperative%2520game%2520theory%2520and%2520satisfies%250Aall%2520the%2520fundamental%2520Shapley%2520axioms.%2520Automated%2520evaluation%2520of%2520these%2520explanations%250Ain%2520terms%2520of%2520necessary%2520and%2520sufficient%2520explanations%252C%2520and%2520comparisons%2520with%2520various%250Abaselines%252C%2520shows%2520the%2520effectiveness%2520of%2520this%2520approach%2520for%2520most%2520query%2520types.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2510.15623v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=CQD-SHAP%3A%20Explainable%20Complex%20Query%20Answering%20via%20Shapley%20Values&entry.906535625=Parsa%20Abbasi%20and%20Stefan%20Heindorf&entry.1292438233=%20%20Complex%20query%20answering%20%28CQA%29%20goes%20beyond%20the%20well-studied%20link%20prediction%0Atask%20by%20addressing%20more%20sophisticated%20queries%20that%20require%20multi-hop%20reasoning%0Aover%20incomplete%20knowledge%20graphs%20%28KGs%29.%20Research%20on%20neural%20and%20neurosymbolic%0ACQA%20methods%20is%20still%20an%20emerging%20field.%20Almost%20all%20of%20these%20methods%20can%20be%0Aregarded%20as%20black-box%20models%2C%20which%20may%20raise%20concerns%20about%20user%20trust.%0AAlthough%20neurosymbolic%20approaches%20like%20CQD%20are%20slightly%20more%20interpretable%2C%0Aallowing%20intermediate%20results%20to%20be%20tracked%2C%20the%20importance%20of%20different%20parts%0Aof%20the%20query%20remains%20unexplained.%20In%20this%20paper%2C%20we%20propose%20CQD-SHAP%2C%20a%20novel%0Aframework%20that%20computes%20the%20contribution%20of%20each%20query%20part%20to%20the%20ranking%20of%20a%0Aspecific%20answer.%20This%20contribution%20explains%20the%20value%20of%20leveraging%20a%20neural%0Apredictor%20that%20can%20infer%20new%20knowledge%20from%20an%20incomplete%20KG%2C%20rather%20than%20a%0Asymbolic%20approach%20relying%20solely%20on%20existing%20facts%20in%20the%20KG.%20CQD-SHAP%20is%0Aformulated%20based%20on%20Shapley%20values%20from%20cooperative%20game%20theory%20and%20satisfies%0Aall%20the%20fundamental%20Shapley%20axioms.%20Automated%20evaluation%20of%20these%20explanations%0Ain%20terms%20of%20necessary%20and%20sufficient%20explanations%2C%20and%20comparisons%20with%20various%0Abaselines%2C%20shows%20the%20effectiveness%20of%20this%20approach%20for%20most%20query%20types.%0A&entry.1838667208=http%3A//arxiv.org/abs/2510.15623v1&entry.124074799=Read"},
{"title": "Unleashing Scientific Reasoning for Bio-experimental Protocol Generation\n  via Structured Component-based Reward Mechanism", "author": "Haoran Sun and Yankai Jiang and Zhenyu Tang and Yaning Pan and Shuang Gu and Zekai Lin and Lilong Wang and Wenjie Lou and Lei Liu and Lei Bai and Xiaosong Wang", "abstract": "  The foundation of reproducible science lies in protocols that are precise,\nlogically ordered, and executable. The autonomous generation of these protocols\nthrough natural language queries could greatly improve the efficiency of the\nreproduction process. However, current leading large language models (LLMs)\noften generate incomplete or inconsistent protocols, limiting their utility. To\naddress this limitation, we first introduce SciRecipe, a large-scale dataset of\nover 12K structured protocols spanning 27 biological subfields and encompassing\nboth comprehension and problem-solving tasks. To further improve protocol\ngeneration, we propose the \"Sketch-and-Fill\" paradigm, which separates\nanalysis, structuring, and expression to ensure each step is explicit and\nverifiable. Complementing this, the structured component-based reward mechanism\nevaluates step granularity, action order, and semantic fidelity, aligning model\noptimization with experimental reliability. Building on these components, we\ndevelop Thoth, trained through a staged Knowledge-to-Action process that\nprogresses from knowledge acquisition to operational reasoning and ultimately\nto robust, executable protocol generation. Across multiple benchmarks, Thoth\nconsistently surpasses both proprietary and open-source LLMs, achieving\nsignificant improvements in step alignment, logical sequencing, and semantic\naccuracy. Our approach paves the way for reliable scientific assistants that\nbridge knowledge with experimental execution. All data, code, and models will\nbe released publicly.\n", "link": "http://arxiv.org/abs/2510.15600v1", "date": "2025-10-17", "relevancy": 1.5059, "topK": [{"title": "RoboEXP: Action-Conditioned Scene Graph via Interactive Exploration for\n  Robotic Manipulation", "link": "http://arxiv.org/abs/2402.15487v1", "similarity": 0.5513}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.4886}, {"title": "Genie: Generative Interactive Environments", "link": "http://arxiv.org/abs/2402.15391v1", "similarity": 0.4876}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Unleashing%20Scientific%20Reasoning%20for%20Bio-experimental%20Protocol%20Generation%0A%20%20via%20Structured%20Component-based%20Reward%20Mechanism&body=Title%3A%20Unleashing%20Scientific%20Reasoning%20for%20Bio-experimental%20Protocol%20Generation%0A%20%20via%20Structured%20Component-based%20Reward%20Mechanism%0AAuthor%3A%20Haoran%20Sun%20and%20Yankai%20Jiang%20and%20Zhenyu%20Tang%20and%20Yaning%20Pan%20and%20Shuang%20Gu%20and%20Zekai%20Lin%20and%20Lilong%20Wang%20and%20Wenjie%20Lou%20and%20Lei%20Liu%20and%20Lei%20Bai%20and%20Xiaosong%20Wang%0AAbstract%3A%20%20%20The%20foundation%20of%20reproducible%20science%20lies%20in%20protocols%20that%20are%20precise%2C%0Alogically%20ordered%2C%20and%20executable.%20The%20autonomous%20generation%20of%20these%20protocols%0Athrough%20natural%20language%20queries%20could%20greatly%20improve%20the%20efficiency%20of%20the%0Areproduction%20process.%20However%2C%20current%20leading%20large%20language%20models%20%28LLMs%29%0Aoften%20generate%20incomplete%20or%20inconsistent%20protocols%2C%20limiting%20their%20utility.%20To%0Aaddress%20this%20limitation%2C%20we%20first%20introduce%20SciRecipe%2C%20a%20large-scale%20dataset%20of%0Aover%2012K%20structured%20protocols%20spanning%2027%20biological%20subfields%20and%20encompassing%0Aboth%20comprehension%20and%20problem-solving%20tasks.%20To%20further%20improve%20protocol%0Ageneration%2C%20we%20propose%20the%20%22Sketch-and-Fill%22%20paradigm%2C%20which%20separates%0Aanalysis%2C%20structuring%2C%20and%20expression%20to%20ensure%20each%20step%20is%20explicit%20and%0Averifiable.%20Complementing%20this%2C%20the%20structured%20component-based%20reward%20mechanism%0Aevaluates%20step%20granularity%2C%20action%20order%2C%20and%20semantic%20fidelity%2C%20aligning%20model%0Aoptimization%20with%20experimental%20reliability.%20Building%20on%20these%20components%2C%20we%0Adevelop%20Thoth%2C%20trained%20through%20a%20staged%20Knowledge-to-Action%20process%20that%0Aprogresses%20from%20knowledge%20acquisition%20to%20operational%20reasoning%20and%20ultimately%0Ato%20robust%2C%20executable%20protocol%20generation.%20Across%20multiple%20benchmarks%2C%20Thoth%0Aconsistently%20surpasses%20both%20proprietary%20and%20open-source%20LLMs%2C%20achieving%0Asignificant%20improvements%20in%20step%20alignment%2C%20logical%20sequencing%2C%20and%20semantic%0Aaccuracy.%20Our%20approach%20paves%20the%20way%20for%20reliable%20scientific%20assistants%20that%0Abridge%20knowledge%20with%20experimental%20execution.%20All%20data%2C%20code%2C%20and%20models%20will%0Abe%20released%20publicly.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2510.15600v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DUnleashing%2520Scientific%2520Reasoning%2520for%2520Bio-experimental%2520Protocol%2520Generation%250A%2520%2520via%2520Structured%2520Component-based%2520Reward%2520Mechanism%26entry.906535625%3DHaoran%2520Sun%2520and%2520Yankai%2520Jiang%2520and%2520Zhenyu%2520Tang%2520and%2520Yaning%2520Pan%2520and%2520Shuang%2520Gu%2520and%2520Zekai%2520Lin%2520and%2520Lilong%2520Wang%2520and%2520Wenjie%2520Lou%2520and%2520Lei%2520Liu%2520and%2520Lei%2520Bai%2520and%2520Xiaosong%2520Wang%26entry.1292438233%3D%2520%2520The%2520foundation%2520of%2520reproducible%2520science%2520lies%2520in%2520protocols%2520that%2520are%2520precise%252C%250Alogically%2520ordered%252C%2520and%2520executable.%2520The%2520autonomous%2520generation%2520of%2520these%2520protocols%250Athrough%2520natural%2520language%2520queries%2520could%2520greatly%2520improve%2520the%2520efficiency%2520of%2520the%250Areproduction%2520process.%2520However%252C%2520current%2520leading%2520large%2520language%2520models%2520%2528LLMs%2529%250Aoften%2520generate%2520incomplete%2520or%2520inconsistent%2520protocols%252C%2520limiting%2520their%2520utility.%2520To%250Aaddress%2520this%2520limitation%252C%2520we%2520first%2520introduce%2520SciRecipe%252C%2520a%2520large-scale%2520dataset%2520of%250Aover%252012K%2520structured%2520protocols%2520spanning%252027%2520biological%2520subfields%2520and%2520encompassing%250Aboth%2520comprehension%2520and%2520problem-solving%2520tasks.%2520To%2520further%2520improve%2520protocol%250Ageneration%252C%2520we%2520propose%2520the%2520%2522Sketch-and-Fill%2522%2520paradigm%252C%2520which%2520separates%250Aanalysis%252C%2520structuring%252C%2520and%2520expression%2520to%2520ensure%2520each%2520step%2520is%2520explicit%2520and%250Averifiable.%2520Complementing%2520this%252C%2520the%2520structured%2520component-based%2520reward%2520mechanism%250Aevaluates%2520step%2520granularity%252C%2520action%2520order%252C%2520and%2520semantic%2520fidelity%252C%2520aligning%2520model%250Aoptimization%2520with%2520experimental%2520reliability.%2520Building%2520on%2520these%2520components%252C%2520we%250Adevelop%2520Thoth%252C%2520trained%2520through%2520a%2520staged%2520Knowledge-to-Action%2520process%2520that%250Aprogresses%2520from%2520knowledge%2520acquisition%2520to%2520operational%2520reasoning%2520and%2520ultimately%250Ato%2520robust%252C%2520executable%2520protocol%2520generation.%2520Across%2520multiple%2520benchmarks%252C%2520Thoth%250Aconsistently%2520surpasses%2520both%2520proprietary%2520and%2520open-source%2520LLMs%252C%2520achieving%250Asignificant%2520improvements%2520in%2520step%2520alignment%252C%2520logical%2520sequencing%252C%2520and%2520semantic%250Aaccuracy.%2520Our%2520approach%2520paves%2520the%2520way%2520for%2520reliable%2520scientific%2520assistants%2520that%250Abridge%2520knowledge%2520with%2520experimental%2520execution.%2520All%2520data%252C%2520code%252C%2520and%2520models%2520will%250Abe%2520released%2520publicly.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2510.15600v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Unleashing%20Scientific%20Reasoning%20for%20Bio-experimental%20Protocol%20Generation%0A%20%20via%20Structured%20Component-based%20Reward%20Mechanism&entry.906535625=Haoran%20Sun%20and%20Yankai%20Jiang%20and%20Zhenyu%20Tang%20and%20Yaning%20Pan%20and%20Shuang%20Gu%20and%20Zekai%20Lin%20and%20Lilong%20Wang%20and%20Wenjie%20Lou%20and%20Lei%20Liu%20and%20Lei%20Bai%20and%20Xiaosong%20Wang&entry.1292438233=%20%20The%20foundation%20of%20reproducible%20science%20lies%20in%20protocols%20that%20are%20precise%2C%0Alogically%20ordered%2C%20and%20executable.%20The%20autonomous%20generation%20of%20these%20protocols%0Athrough%20natural%20language%20queries%20could%20greatly%20improve%20the%20efficiency%20of%20the%0Areproduction%20process.%20However%2C%20current%20leading%20large%20language%20models%20%28LLMs%29%0Aoften%20generate%20incomplete%20or%20inconsistent%20protocols%2C%20limiting%20their%20utility.%20To%0Aaddress%20this%20limitation%2C%20we%20first%20introduce%20SciRecipe%2C%20a%20large-scale%20dataset%20of%0Aover%2012K%20structured%20protocols%20spanning%2027%20biological%20subfields%20and%20encompassing%0Aboth%20comprehension%20and%20problem-solving%20tasks.%20To%20further%20improve%20protocol%0Ageneration%2C%20we%20propose%20the%20%22Sketch-and-Fill%22%20paradigm%2C%20which%20separates%0Aanalysis%2C%20structuring%2C%20and%20expression%20to%20ensure%20each%20step%20is%20explicit%20and%0Averifiable.%20Complementing%20this%2C%20the%20structured%20component-based%20reward%20mechanism%0Aevaluates%20step%20granularity%2C%20action%20order%2C%20and%20semantic%20fidelity%2C%20aligning%20model%0Aoptimization%20with%20experimental%20reliability.%20Building%20on%20these%20components%2C%20we%0Adevelop%20Thoth%2C%20trained%20through%20a%20staged%20Knowledge-to-Action%20process%20that%0Aprogresses%20from%20knowledge%20acquisition%20to%20operational%20reasoning%20and%20ultimately%0Ato%20robust%2C%20executable%20protocol%20generation.%20Across%20multiple%20benchmarks%2C%20Thoth%0Aconsistently%20surpasses%20both%20proprietary%20and%20open-source%20LLMs%2C%20achieving%0Asignificant%20improvements%20in%20step%20alignment%2C%20logical%20sequencing%2C%20and%20semantic%0Aaccuracy.%20Our%20approach%20paves%20the%20way%20for%20reliable%20scientific%20assistants%20that%0Abridge%20knowledge%20with%20experimental%20execution.%20All%20data%2C%20code%2C%20and%20models%20will%0Abe%20released%20publicly.%0A&entry.1838667208=http%3A//arxiv.org/abs/2510.15600v1&entry.124074799=Read"},
{"title": "WARP-LUTs - Walsh-Assisted Relaxation for Probabilistic Look Up Tables", "author": "Lino Gerlach and Liv V\u00e5ge and Thore Gerlach and Elliott Kauffman", "abstract": "  Fast and efficient machine learning is of growing interest to the scientific\ncommunity and has spurred significant research into novel model architectures\nand hardware-aware design. Recent hard? and software co-design approaches have\ndemonstrated impressive results with entirely multiplication-free models.\nDifferentiable Logic Gate Networks (DLGNs), for instance, provide a\ngradient-based framework for learning optimal combinations of low-level logic\ngates, setting state-of-the-art trade-offs between accuracy, resource usage,\nand latency. However, these models suffer from high computational cost during\ntraining and do not generalize well to logic blocks with more inputs. In this\nwork, we introduce Walsh-Assisted Relaxation for Probabilistic Look-Up Tables\n(WARP-LUTs) - a novel gradient-based method that efficiently learns\ncombinations of logic gates with substantially fewer trainable parameters. We\ndemonstrate that WARP-LUTs achieve significantly faster convergence on CIFAR-10\ncompared to DLGNs, while maintaining comparable accuracy. Furthermore, our\napproach suggests potential for extension to higher-input logic blocks,\nmotivating future research on extremely efficient deployment on modern FPGAs\nand its real-time science applications.\n", "link": "http://arxiv.org/abs/2510.15655v1", "date": "2025-10-17", "relevancy": 1.4812, "topK": [{"title": "SDXS: Real-Time One-Step Latent Diffusion Models with Image Conditions", "link": "http://arxiv.org/abs/2403.16627v1", "similarity": 0.5133}, {"title": "Scaling Rectified Flow Transformers for High-Resolution Image Synthesis", "link": "http://arxiv.org/abs/2403.03206v1", "similarity": 0.4935}, {"title": "ToDo: Token Downsampling for Efficient Generation of High-Resolution\n  Images", "link": "http://arxiv.org/abs/2402.13573v2", "similarity": 0.486}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20WARP-LUTs%20-%20Walsh-Assisted%20Relaxation%20for%20Probabilistic%20Look%20Up%20Tables&body=Title%3A%20WARP-LUTs%20-%20Walsh-Assisted%20Relaxation%20for%20Probabilistic%20Look%20Up%20Tables%0AAuthor%3A%20Lino%20Gerlach%20and%20Liv%20V%C3%A5ge%20and%20Thore%20Gerlach%20and%20Elliott%20Kauffman%0AAbstract%3A%20%20%20Fast%20and%20efficient%20machine%20learning%20is%20of%20growing%20interest%20to%20the%20scientific%0Acommunity%20and%20has%20spurred%20significant%20research%20into%20novel%20model%20architectures%0Aand%20hardware-aware%20design.%20Recent%20hard%3F%20and%20software%20co-design%20approaches%20have%0Ademonstrated%20impressive%20results%20with%20entirely%20multiplication-free%20models.%0ADifferentiable%20Logic%20Gate%20Networks%20%28DLGNs%29%2C%20for%20instance%2C%20provide%20a%0Agradient-based%20framework%20for%20learning%20optimal%20combinations%20of%20low-level%20logic%0Agates%2C%20setting%20state-of-the-art%20trade-offs%20between%20accuracy%2C%20resource%20usage%2C%0Aand%20latency.%20However%2C%20these%20models%20suffer%20from%20high%20computational%20cost%20during%0Atraining%20and%20do%20not%20generalize%20well%20to%20logic%20blocks%20with%20more%20inputs.%20In%20this%0Awork%2C%20we%20introduce%20Walsh-Assisted%20Relaxation%20for%20Probabilistic%20Look-Up%20Tables%0A%28WARP-LUTs%29%20-%20a%20novel%20gradient-based%20method%20that%20efficiently%20learns%0Acombinations%20of%20logic%20gates%20with%20substantially%20fewer%20trainable%20parameters.%20We%0Ademonstrate%20that%20WARP-LUTs%20achieve%20significantly%20faster%20convergence%20on%20CIFAR-10%0Acompared%20to%20DLGNs%2C%20while%20maintaining%20comparable%20accuracy.%20Furthermore%2C%20our%0Aapproach%20suggests%20potential%20for%20extension%20to%20higher-input%20logic%20blocks%2C%0Amotivating%20future%20research%20on%20extremely%20efficient%20deployment%20on%20modern%20FPGAs%0Aand%20its%20real-time%20science%20applications.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2510.15655v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DWARP-LUTs%2520-%2520Walsh-Assisted%2520Relaxation%2520for%2520Probabilistic%2520Look%2520Up%2520Tables%26entry.906535625%3DLino%2520Gerlach%2520and%2520Liv%2520V%25C3%25A5ge%2520and%2520Thore%2520Gerlach%2520and%2520Elliott%2520Kauffman%26entry.1292438233%3D%2520%2520Fast%2520and%2520efficient%2520machine%2520learning%2520is%2520of%2520growing%2520interest%2520to%2520the%2520scientific%250Acommunity%2520and%2520has%2520spurred%2520significant%2520research%2520into%2520novel%2520model%2520architectures%250Aand%2520hardware-aware%2520design.%2520Recent%2520hard%253F%2520and%2520software%2520co-design%2520approaches%2520have%250Ademonstrated%2520impressive%2520results%2520with%2520entirely%2520multiplication-free%2520models.%250ADifferentiable%2520Logic%2520Gate%2520Networks%2520%2528DLGNs%2529%252C%2520for%2520instance%252C%2520provide%2520a%250Agradient-based%2520framework%2520for%2520learning%2520optimal%2520combinations%2520of%2520low-level%2520logic%250Agates%252C%2520setting%2520state-of-the-art%2520trade-offs%2520between%2520accuracy%252C%2520resource%2520usage%252C%250Aand%2520latency.%2520However%252C%2520these%2520models%2520suffer%2520from%2520high%2520computational%2520cost%2520during%250Atraining%2520and%2520do%2520not%2520generalize%2520well%2520to%2520logic%2520blocks%2520with%2520more%2520inputs.%2520In%2520this%250Awork%252C%2520we%2520introduce%2520Walsh-Assisted%2520Relaxation%2520for%2520Probabilistic%2520Look-Up%2520Tables%250A%2528WARP-LUTs%2529%2520-%2520a%2520novel%2520gradient-based%2520method%2520that%2520efficiently%2520learns%250Acombinations%2520of%2520logic%2520gates%2520with%2520substantially%2520fewer%2520trainable%2520parameters.%2520We%250Ademonstrate%2520that%2520WARP-LUTs%2520achieve%2520significantly%2520faster%2520convergence%2520on%2520CIFAR-10%250Acompared%2520to%2520DLGNs%252C%2520while%2520maintaining%2520comparable%2520accuracy.%2520Furthermore%252C%2520our%250Aapproach%2520suggests%2520potential%2520for%2520extension%2520to%2520higher-input%2520logic%2520blocks%252C%250Amotivating%2520future%2520research%2520on%2520extremely%2520efficient%2520deployment%2520on%2520modern%2520FPGAs%250Aand%2520its%2520real-time%2520science%2520applications.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2510.15655v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=WARP-LUTs%20-%20Walsh-Assisted%20Relaxation%20for%20Probabilistic%20Look%20Up%20Tables&entry.906535625=Lino%20Gerlach%20and%20Liv%20V%C3%A5ge%20and%20Thore%20Gerlach%20and%20Elliott%20Kauffman&entry.1292438233=%20%20Fast%20and%20efficient%20machine%20learning%20is%20of%20growing%20interest%20to%20the%20scientific%0Acommunity%20and%20has%20spurred%20significant%20research%20into%20novel%20model%20architectures%0Aand%20hardware-aware%20design.%20Recent%20hard%3F%20and%20software%20co-design%20approaches%20have%0Ademonstrated%20impressive%20results%20with%20entirely%20multiplication-free%20models.%0ADifferentiable%20Logic%20Gate%20Networks%20%28DLGNs%29%2C%20for%20instance%2C%20provide%20a%0Agradient-based%20framework%20for%20learning%20optimal%20combinations%20of%20low-level%20logic%0Agates%2C%20setting%20state-of-the-art%20trade-offs%20between%20accuracy%2C%20resource%20usage%2C%0Aand%20latency.%20However%2C%20these%20models%20suffer%20from%20high%20computational%20cost%20during%0Atraining%20and%20do%20not%20generalize%20well%20to%20logic%20blocks%20with%20more%20inputs.%20In%20this%0Awork%2C%20we%20introduce%20Walsh-Assisted%20Relaxation%20for%20Probabilistic%20Look-Up%20Tables%0A%28WARP-LUTs%29%20-%20a%20novel%20gradient-based%20method%20that%20efficiently%20learns%0Acombinations%20of%20logic%20gates%20with%20substantially%20fewer%20trainable%20parameters.%20We%0Ademonstrate%20that%20WARP-LUTs%20achieve%20significantly%20faster%20convergence%20on%20CIFAR-10%0Acompared%20to%20DLGNs%2C%20while%20maintaining%20comparable%20accuracy.%20Furthermore%2C%20our%0Aapproach%20suggests%20potential%20for%20extension%20to%20higher-input%20logic%20blocks%2C%0Amotivating%20future%20research%20on%20extremely%20efficient%20deployment%20on%20modern%20FPGAs%0Aand%20its%20real-time%20science%20applications.%0A&entry.1838667208=http%3A//arxiv.org/abs/2510.15655v1&entry.124074799=Read"},
{"title": "Blackwell's Approachability for Sequential Conformal Inference", "author": "Guillaume Principato and Gilles Stoltz", "abstract": "  We study conformal inference in non-exchangeable environments through the\nlens of Blackwell's theory of approachability. We first recast adaptive\nconformal inference (ACI, Gibbs and Cand\\`es, 2021) as a repeated two-player\nvector-valued finite game and characterize attainable coverage--efficiency\ntradeoffs. We then construct coverage and efficiency objectives under potential\nrestrictions on the adversary's play, and design a calibration-based\napproachability strategy to achieve these goals. The resulting algorithm enjoys\nstrong theoretical guarantees and provides practical insights, though its\ncomputational burden may limit deployment in practice.\n", "link": "http://arxiv.org/abs/2510.15824v1", "date": "2025-10-17", "relevancy": 1.3833, "topK": [{"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.4719}, {"title": "Flexible Visual Recognition by Evidential Modeling of Confusion and Ignorance", "link": "http://openaccess.thecvf.com/content/ICCV2023/html/Fan_Flexible_Visual_Recognition_by_Evidential_Modeling_of_Confusion_and_Ignorance_ICCV_2023_paper.html", "similarity": 0.4588}, {"title": "CamCtrl3D: Single-Image Scene Exploration with Precise 3D Camera Control", "link": "http://arxiv.org/abs/2501.06006v1", "similarity": 0.4561}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Blackwell%27s%20Approachability%20for%20Sequential%20Conformal%20Inference&body=Title%3A%20Blackwell%27s%20Approachability%20for%20Sequential%20Conformal%20Inference%0AAuthor%3A%20Guillaume%20Principato%20and%20Gilles%20Stoltz%0AAbstract%3A%20%20%20We%20study%20conformal%20inference%20in%20non-exchangeable%20environments%20through%20the%0Alens%20of%20Blackwell%27s%20theory%20of%20approachability.%20We%20first%20recast%20adaptive%0Aconformal%20inference%20%28ACI%2C%20Gibbs%20and%20Cand%5C%60es%2C%202021%29%20as%20a%20repeated%20two-player%0Avector-valued%20finite%20game%20and%20characterize%20attainable%20coverage--efficiency%0Atradeoffs.%20We%20then%20construct%20coverage%20and%20efficiency%20objectives%20under%20potential%0Arestrictions%20on%20the%20adversary%27s%20play%2C%20and%20design%20a%20calibration-based%0Aapproachability%20strategy%20to%20achieve%20these%20goals.%20The%20resulting%20algorithm%20enjoys%0Astrong%20theoretical%20guarantees%20and%20provides%20practical%20insights%2C%20though%20its%0Acomputational%20burden%20may%20limit%20deployment%20in%20practice.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2510.15824v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DBlackwell%2527s%2520Approachability%2520for%2520Sequential%2520Conformal%2520Inference%26entry.906535625%3DGuillaume%2520Principato%2520and%2520Gilles%2520Stoltz%26entry.1292438233%3D%2520%2520We%2520study%2520conformal%2520inference%2520in%2520non-exchangeable%2520environments%2520through%2520the%250Alens%2520of%2520Blackwell%2527s%2520theory%2520of%2520approachability.%2520We%2520first%2520recast%2520adaptive%250Aconformal%2520inference%2520%2528ACI%252C%2520Gibbs%2520and%2520Cand%255C%2560es%252C%25202021%2529%2520as%2520a%2520repeated%2520two-player%250Avector-valued%2520finite%2520game%2520and%2520characterize%2520attainable%2520coverage--efficiency%250Atradeoffs.%2520We%2520then%2520construct%2520coverage%2520and%2520efficiency%2520objectives%2520under%2520potential%250Arestrictions%2520on%2520the%2520adversary%2527s%2520play%252C%2520and%2520design%2520a%2520calibration-based%250Aapproachability%2520strategy%2520to%2520achieve%2520these%2520goals.%2520The%2520resulting%2520algorithm%2520enjoys%250Astrong%2520theoretical%2520guarantees%2520and%2520provides%2520practical%2520insights%252C%2520though%2520its%250Acomputational%2520burden%2520may%2520limit%2520deployment%2520in%2520practice.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2510.15824v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Blackwell%27s%20Approachability%20for%20Sequential%20Conformal%20Inference&entry.906535625=Guillaume%20Principato%20and%20Gilles%20Stoltz&entry.1292438233=%20%20We%20study%20conformal%20inference%20in%20non-exchangeable%20environments%20through%20the%0Alens%20of%20Blackwell%27s%20theory%20of%20approachability.%20We%20first%20recast%20adaptive%0Aconformal%20inference%20%28ACI%2C%20Gibbs%20and%20Cand%5C%60es%2C%202021%29%20as%20a%20repeated%20two-player%0Avector-valued%20finite%20game%20and%20characterize%20attainable%20coverage--efficiency%0Atradeoffs.%20We%20then%20construct%20coverage%20and%20efficiency%20objectives%20under%20potential%0Arestrictions%20on%20the%20adversary%27s%20play%2C%20and%20design%20a%20calibration-based%0Aapproachability%20strategy%20to%20achieve%20these%20goals.%20The%20resulting%20algorithm%20enjoys%0Astrong%20theoretical%20guarantees%20and%20provides%20practical%20insights%2C%20though%20its%0Acomputational%20burden%20may%20limit%20deployment%20in%20practice.%0A&entry.1838667208=http%3A//arxiv.org/abs/2510.15824v1&entry.124074799=Read"},
{"title": "CADE 2.5 - ZeResFDG: Frequency-Decoupled, Rescaled and Zero-Projected\n  Guidance for SD/SDXL Latent Diffusion Models", "author": "Denis Rychkovskiy", "abstract": "  We introduce CADE 2.5 (Comfy Adaptive Detail Enhancer), a sampler-level\nguidance stack for SD/SDXL latent diffusion models. The central module,\nZeResFDG, unifies (i) frequency-decoupled guidance that reweights low- and\nhigh-frequency components of the guidance signal, (ii) energy rescaling that\nmatches the per-sample magnitude of the guided prediction to the positive\nbranch, and (iii) zero-projection that removes the component parallel to the\nunconditional direction. A lightweight spectral EMA with hysteresis switches\nbetween a conservative and a detail-seeking mode as structure crystallizes\nduring sampling. Across SD/SDXL samplers, ZeResFDG improves sharpness, prompt\nadherence, and artifact control at moderate guidance scales without any\nretraining. In addition, we employ a training-free inference-time stabilizer,\nQSilk Micrograin Stabilizer (quantile clamp + depth/edge-gated micro-detail\ninjection), which improves robustness and yields natural high-frequency\nmicro-texture at high resolutions with negligible overhead. For completeness we\nnote that the same rule is compatible with alternative parameterizations (e.g.,\nvelocity), which we briefly discuss in the Appendix; however, this paper\nfocuses on SD/SDXL latent diffusion models.\n", "link": "http://arxiv.org/abs/2510.12954v2", "date": "2025-10-17", "relevancy": 1.7096, "topK": [{"title": "SDXS: Real-Time One-Step Latent Diffusion Models with Image Conditions", "link": "http://arxiv.org/abs/2403.16627v1", "similarity": 0.6551}, {"title": "ToDo: Token Downsampling for Efficient Generation of High-Resolution\n  Images", "link": "http://arxiv.org/abs/2402.13573v2", "similarity": 0.5518}, {"title": "Scaling Rectified Flow Transformers for High-Resolution Image Synthesis", "link": "http://arxiv.org/abs/2403.03206v1", "similarity": 0.5297}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20CADE%202.5%20-%20ZeResFDG%3A%20Frequency-Decoupled%2C%20Rescaled%20and%20Zero-Projected%0A%20%20Guidance%20for%20SD/SDXL%20Latent%20Diffusion%20Models&body=Title%3A%20CADE%202.5%20-%20ZeResFDG%3A%20Frequency-Decoupled%2C%20Rescaled%20and%20Zero-Projected%0A%20%20Guidance%20for%20SD/SDXL%20Latent%20Diffusion%20Models%0AAuthor%3A%20Denis%20Rychkovskiy%0AAbstract%3A%20%20%20We%20introduce%20CADE%202.5%20%28Comfy%20Adaptive%20Detail%20Enhancer%29%2C%20a%20sampler-level%0Aguidance%20stack%20for%20SD/SDXL%20latent%20diffusion%20models.%20The%20central%20module%2C%0AZeResFDG%2C%20unifies%20%28i%29%20frequency-decoupled%20guidance%20that%20reweights%20low-%20and%0Ahigh-frequency%20components%20of%20the%20guidance%20signal%2C%20%28ii%29%20energy%20rescaling%20that%0Amatches%20the%20per-sample%20magnitude%20of%20the%20guided%20prediction%20to%20the%20positive%0Abranch%2C%20and%20%28iii%29%20zero-projection%20that%20removes%20the%20component%20parallel%20to%20the%0Aunconditional%20direction.%20A%20lightweight%20spectral%20EMA%20with%20hysteresis%20switches%0Abetween%20a%20conservative%20and%20a%20detail-seeking%20mode%20as%20structure%20crystallizes%0Aduring%20sampling.%20Across%20SD/SDXL%20samplers%2C%20ZeResFDG%20improves%20sharpness%2C%20prompt%0Aadherence%2C%20and%20artifact%20control%20at%20moderate%20guidance%20scales%20without%20any%0Aretraining.%20In%20addition%2C%20we%20employ%20a%20training-free%20inference-time%20stabilizer%2C%0AQSilk%20Micrograin%20Stabilizer%20%28quantile%20clamp%20%2B%20depth/edge-gated%20micro-detail%0Ainjection%29%2C%20which%20improves%20robustness%20and%20yields%20natural%20high-frequency%0Amicro-texture%20at%20high%20resolutions%20with%20negligible%20overhead.%20For%20completeness%20we%0Anote%20that%20the%20same%20rule%20is%20compatible%20with%20alternative%20parameterizations%20%28e.g.%2C%0Avelocity%29%2C%20which%20we%20briefly%20discuss%20in%20the%20Appendix%3B%20however%2C%20this%20paper%0Afocuses%20on%20SD/SDXL%20latent%20diffusion%20models.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2510.12954v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DCADE%25202.5%2520-%2520ZeResFDG%253A%2520Frequency-Decoupled%252C%2520Rescaled%2520and%2520Zero-Projected%250A%2520%2520Guidance%2520for%2520SD/SDXL%2520Latent%2520Diffusion%2520Models%26entry.906535625%3DDenis%2520Rychkovskiy%26entry.1292438233%3D%2520%2520We%2520introduce%2520CADE%25202.5%2520%2528Comfy%2520Adaptive%2520Detail%2520Enhancer%2529%252C%2520a%2520sampler-level%250Aguidance%2520stack%2520for%2520SD/SDXL%2520latent%2520diffusion%2520models.%2520The%2520central%2520module%252C%250AZeResFDG%252C%2520unifies%2520%2528i%2529%2520frequency-decoupled%2520guidance%2520that%2520reweights%2520low-%2520and%250Ahigh-frequency%2520components%2520of%2520the%2520guidance%2520signal%252C%2520%2528ii%2529%2520energy%2520rescaling%2520that%250Amatches%2520the%2520per-sample%2520magnitude%2520of%2520the%2520guided%2520prediction%2520to%2520the%2520positive%250Abranch%252C%2520and%2520%2528iii%2529%2520zero-projection%2520that%2520removes%2520the%2520component%2520parallel%2520to%2520the%250Aunconditional%2520direction.%2520A%2520lightweight%2520spectral%2520EMA%2520with%2520hysteresis%2520switches%250Abetween%2520a%2520conservative%2520and%2520a%2520detail-seeking%2520mode%2520as%2520structure%2520crystallizes%250Aduring%2520sampling.%2520Across%2520SD/SDXL%2520samplers%252C%2520ZeResFDG%2520improves%2520sharpness%252C%2520prompt%250Aadherence%252C%2520and%2520artifact%2520control%2520at%2520moderate%2520guidance%2520scales%2520without%2520any%250Aretraining.%2520In%2520addition%252C%2520we%2520employ%2520a%2520training-free%2520inference-time%2520stabilizer%252C%250AQSilk%2520Micrograin%2520Stabilizer%2520%2528quantile%2520clamp%2520%252B%2520depth/edge-gated%2520micro-detail%250Ainjection%2529%252C%2520which%2520improves%2520robustness%2520and%2520yields%2520natural%2520high-frequency%250Amicro-texture%2520at%2520high%2520resolutions%2520with%2520negligible%2520overhead.%2520For%2520completeness%2520we%250Anote%2520that%2520the%2520same%2520rule%2520is%2520compatible%2520with%2520alternative%2520parameterizations%2520%2528e.g.%252C%250Avelocity%2529%252C%2520which%2520we%2520briefly%2520discuss%2520in%2520the%2520Appendix%253B%2520however%252C%2520this%2520paper%250Afocuses%2520on%2520SD/SDXL%2520latent%2520diffusion%2520models.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2510.12954v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=CADE%202.5%20-%20ZeResFDG%3A%20Frequency-Decoupled%2C%20Rescaled%20and%20Zero-Projected%0A%20%20Guidance%20for%20SD/SDXL%20Latent%20Diffusion%20Models&entry.906535625=Denis%20Rychkovskiy&entry.1292438233=%20%20We%20introduce%20CADE%202.5%20%28Comfy%20Adaptive%20Detail%20Enhancer%29%2C%20a%20sampler-level%0Aguidance%20stack%20for%20SD/SDXL%20latent%20diffusion%20models.%20The%20central%20module%2C%0AZeResFDG%2C%20unifies%20%28i%29%20frequency-decoupled%20guidance%20that%20reweights%20low-%20and%0Ahigh-frequency%20components%20of%20the%20guidance%20signal%2C%20%28ii%29%20energy%20rescaling%20that%0Amatches%20the%20per-sample%20magnitude%20of%20the%20guided%20prediction%20to%20the%20positive%0Abranch%2C%20and%20%28iii%29%20zero-projection%20that%20removes%20the%20component%20parallel%20to%20the%0Aunconditional%20direction.%20A%20lightweight%20spectral%20EMA%20with%20hysteresis%20switches%0Abetween%20a%20conservative%20and%20a%20detail-seeking%20mode%20as%20structure%20crystallizes%0Aduring%20sampling.%20Across%20SD/SDXL%20samplers%2C%20ZeResFDG%20improves%20sharpness%2C%20prompt%0Aadherence%2C%20and%20artifact%20control%20at%20moderate%20guidance%20scales%20without%20any%0Aretraining.%20In%20addition%2C%20we%20employ%20a%20training-free%20inference-time%20stabilizer%2C%0AQSilk%20Micrograin%20Stabilizer%20%28quantile%20clamp%20%2B%20depth/edge-gated%20micro-detail%0Ainjection%29%2C%20which%20improves%20robustness%20and%20yields%20natural%20high-frequency%0Amicro-texture%20at%20high%20resolutions%20with%20negligible%20overhead.%20For%20completeness%20we%0Anote%20that%20the%20same%20rule%20is%20compatible%20with%20alternative%20parameterizations%20%28e.g.%2C%0Avelocity%29%2C%20which%20we%20briefly%20discuss%20in%20the%20Appendix%3B%20however%2C%20this%20paper%0Afocuses%20on%20SD/SDXL%20latent%20diffusion%20models.%0A&entry.1838667208=http%3A//arxiv.org/abs/2510.12954v2&entry.124074799=Read"},
{"title": "AURA: An Agent Autonomy Risk Assessment Framework", "author": "Lorenzo Satta Chiris and Ayush Mishra", "abstract": "  As autonomous agentic AI systems see increasing adoption across\norganisations, persistent challenges in alignment, governance, and risk\nmanagement threaten to impede deployment at scale. We present AURA (Agent\naUtonomy Risk Assessment), a unified framework designed to detect, quantify,\nand mitigate risks arising from agentic AI. Building on recent research and\npractical deployments, AURA introduces a gamma-based risk scoring methodology\nthat balances risk assessment accuracy with computational efficiency and\npractical considerations. AURA provides an interactive process to score,\nevaluate and mitigate the risks of running one or multiple AI Agents,\nsynchronously or asynchronously (autonomously). The framework is engineered for\nHuman-in-the-Loop (HITL) oversight and presents Agent-to-Human (A2H)\ncommunication mechanisms, allowing for seamless integration with agentic\nsystems for autonomous self-assessment, rendering it interoperable with\nestablished protocols (MCP and A2A) and tools. AURA supports a responsible and\ntransparent adoption of agentic AI and provides robust risk detection and\nmitigation while balancing computational resources, positioning it as a\ncritical enabler for large-scale, governable agentic AI in enterprise\nenvironments.\n", "link": "http://arxiv.org/abs/2510.15739v1", "date": "2025-10-17", "relevancy": 1.3606, "topK": [{"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.4636}, {"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.4508}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.431}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20AURA%3A%20An%20Agent%20Autonomy%20Risk%20Assessment%20Framework&body=Title%3A%20AURA%3A%20An%20Agent%20Autonomy%20Risk%20Assessment%20Framework%0AAuthor%3A%20Lorenzo%20Satta%20Chiris%20and%20Ayush%20Mishra%0AAbstract%3A%20%20%20As%20autonomous%20agentic%20AI%20systems%20see%20increasing%20adoption%20across%0Aorganisations%2C%20persistent%20challenges%20in%20alignment%2C%20governance%2C%20and%20risk%0Amanagement%20threaten%20to%20impede%20deployment%20at%20scale.%20We%20present%20AURA%20%28Agent%0AaUtonomy%20Risk%20Assessment%29%2C%20a%20unified%20framework%20designed%20to%20detect%2C%20quantify%2C%0Aand%20mitigate%20risks%20arising%20from%20agentic%20AI.%20Building%20on%20recent%20research%20and%0Apractical%20deployments%2C%20AURA%20introduces%20a%20gamma-based%20risk%20scoring%20methodology%0Athat%20balances%20risk%20assessment%20accuracy%20with%20computational%20efficiency%20and%0Apractical%20considerations.%20AURA%20provides%20an%20interactive%20process%20to%20score%2C%0Aevaluate%20and%20mitigate%20the%20risks%20of%20running%20one%20or%20multiple%20AI%20Agents%2C%0Asynchronously%20or%20asynchronously%20%28autonomously%29.%20The%20framework%20is%20engineered%20for%0AHuman-in-the-Loop%20%28HITL%29%20oversight%20and%20presents%20Agent-to-Human%20%28A2H%29%0Acommunication%20mechanisms%2C%20allowing%20for%20seamless%20integration%20with%20agentic%0Asystems%20for%20autonomous%20self-assessment%2C%20rendering%20it%20interoperable%20with%0Aestablished%20protocols%20%28MCP%20and%20A2A%29%20and%20tools.%20AURA%20supports%20a%20responsible%20and%0Atransparent%20adoption%20of%20agentic%20AI%20and%20provides%20robust%20risk%20detection%20and%0Amitigation%20while%20balancing%20computational%20resources%2C%20positioning%20it%20as%20a%0Acritical%20enabler%20for%20large-scale%2C%20governable%20agentic%20AI%20in%20enterprise%0Aenvironments.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2510.15739v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DAURA%253A%2520An%2520Agent%2520Autonomy%2520Risk%2520Assessment%2520Framework%26entry.906535625%3DLorenzo%2520Satta%2520Chiris%2520and%2520Ayush%2520Mishra%26entry.1292438233%3D%2520%2520As%2520autonomous%2520agentic%2520AI%2520systems%2520see%2520increasing%2520adoption%2520across%250Aorganisations%252C%2520persistent%2520challenges%2520in%2520alignment%252C%2520governance%252C%2520and%2520risk%250Amanagement%2520threaten%2520to%2520impede%2520deployment%2520at%2520scale.%2520We%2520present%2520AURA%2520%2528Agent%250AaUtonomy%2520Risk%2520Assessment%2529%252C%2520a%2520unified%2520framework%2520designed%2520to%2520detect%252C%2520quantify%252C%250Aand%2520mitigate%2520risks%2520arising%2520from%2520agentic%2520AI.%2520Building%2520on%2520recent%2520research%2520and%250Apractical%2520deployments%252C%2520AURA%2520introduces%2520a%2520gamma-based%2520risk%2520scoring%2520methodology%250Athat%2520balances%2520risk%2520assessment%2520accuracy%2520with%2520computational%2520efficiency%2520and%250Apractical%2520considerations.%2520AURA%2520provides%2520an%2520interactive%2520process%2520to%2520score%252C%250Aevaluate%2520and%2520mitigate%2520the%2520risks%2520of%2520running%2520one%2520or%2520multiple%2520AI%2520Agents%252C%250Asynchronously%2520or%2520asynchronously%2520%2528autonomously%2529.%2520The%2520framework%2520is%2520engineered%2520for%250AHuman-in-the-Loop%2520%2528HITL%2529%2520oversight%2520and%2520presents%2520Agent-to-Human%2520%2528A2H%2529%250Acommunication%2520mechanisms%252C%2520allowing%2520for%2520seamless%2520integration%2520with%2520agentic%250Asystems%2520for%2520autonomous%2520self-assessment%252C%2520rendering%2520it%2520interoperable%2520with%250Aestablished%2520protocols%2520%2528MCP%2520and%2520A2A%2529%2520and%2520tools.%2520AURA%2520supports%2520a%2520responsible%2520and%250Atransparent%2520adoption%2520of%2520agentic%2520AI%2520and%2520provides%2520robust%2520risk%2520detection%2520and%250Amitigation%2520while%2520balancing%2520computational%2520resources%252C%2520positioning%2520it%2520as%2520a%250Acritical%2520enabler%2520for%2520large-scale%252C%2520governable%2520agentic%2520AI%2520in%2520enterprise%250Aenvironments.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2510.15739v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=AURA%3A%20An%20Agent%20Autonomy%20Risk%20Assessment%20Framework&entry.906535625=Lorenzo%20Satta%20Chiris%20and%20Ayush%20Mishra&entry.1292438233=%20%20As%20autonomous%20agentic%20AI%20systems%20see%20increasing%20adoption%20across%0Aorganisations%2C%20persistent%20challenges%20in%20alignment%2C%20governance%2C%20and%20risk%0Amanagement%20threaten%20to%20impede%20deployment%20at%20scale.%20We%20present%20AURA%20%28Agent%0AaUtonomy%20Risk%20Assessment%29%2C%20a%20unified%20framework%20designed%20to%20detect%2C%20quantify%2C%0Aand%20mitigate%20risks%20arising%20from%20agentic%20AI.%20Building%20on%20recent%20research%20and%0Apractical%20deployments%2C%20AURA%20introduces%20a%20gamma-based%20risk%20scoring%20methodology%0Athat%20balances%20risk%20assessment%20accuracy%20with%20computational%20efficiency%20and%0Apractical%20considerations.%20AURA%20provides%20an%20interactive%20process%20to%20score%2C%0Aevaluate%20and%20mitigate%20the%20risks%20of%20running%20one%20or%20multiple%20AI%20Agents%2C%0Asynchronously%20or%20asynchronously%20%28autonomously%29.%20The%20framework%20is%20engineered%20for%0AHuman-in-the-Loop%20%28HITL%29%20oversight%20and%20presents%20Agent-to-Human%20%28A2H%29%0Acommunication%20mechanisms%2C%20allowing%20for%20seamless%20integration%20with%20agentic%0Asystems%20for%20autonomous%20self-assessment%2C%20rendering%20it%20interoperable%20with%0Aestablished%20protocols%20%28MCP%20and%20A2A%29%20and%20tools.%20AURA%20supports%20a%20responsible%20and%0Atransparent%20adoption%20of%20agentic%20AI%20and%20provides%20robust%20risk%20detection%20and%0Amitigation%20while%20balancing%20computational%20resources%2C%20positioning%20it%20as%20a%0Acritical%20enabler%20for%20large-scale%2C%20governable%20agentic%20AI%20in%20enterprise%0Aenvironments.%0A&entry.1838667208=http%3A//arxiv.org/abs/2510.15739v1&entry.124074799=Read"},
      ];
      const content = document.getElementById('content');
      function createPostElement(post) {
        const postElement = document.createElement('div');
        postElement.className = 'post';
        const dateElem = document.createElement('p');
        dateElem.setAttribute("class", "date");
        dateElem.textContent = post.date;
        postElement.appendChild(dateElem);

        const textElem = document.createElement('p');
        textElem.setAttribute("class", "text");
        const titleElem = document.createElement('p');
        titleElem.setAttribute("class", "title");
        titleElem.textContent = post.title;
        textElem.appendChild(titleElem);
        const authorElem = document.createElement('p');
        authorElem.setAttribute("class", "author");
        authorElem.textContent = post.author;
        textElem.appendChild(authorElem);
        const abstractElem = document.createElement('p');
        abstractElem.setAttribute("class", "abstract");
        abstractElem.textContent = post.abstract;
        textElem.appendChild(abstractElem);

        const linkElement = document.createElement('a');
        linkElement.setAttribute("class", "link");
        linkElement.href = post.link;
        linkElement.target = "_blank";
        linkElement.textContent = post.link.length > 50 ? post.link.substring(0, 50) + '...' : post.link;
        textElem.appendChild(linkElement);
        postElement.appendChild(textElem);

        const linkElementContainer = document.createElement('div');
        linkElementContainer.setAttribute("class", "comment");
        const actionElement = document.createElement('a');
        actionElement.setAttribute("class", "comment");
        actionElement.href = post.form;
        actionElement.textContent = "Action";
        actionElement.target = "_blank";
        linkElementContainer.appendChild(actionElement);
        const emailElement = document.createElement('a');
        emailElement.setAttribute("class", "comment");
        emailElement.href = post.mailto;
        emailElement.textContent = "Email";
        emailElement.target = "_blank";
        linkElementContainer.appendChild(emailElement);
        postElement.appendChild(linkElementContainer);
        const e = document.createElement('div');
        e.setAttribute("class", "clear");
        postElement.appendChild(e);

        const relevancyContainer = document.createElement('div');
        const relevancyValElem = document.createElement('p');
        relevancyValElem.textContent = "Relevancy " + post.relevancy;
        relevancyContainer.appendChild(relevancyValElem);
        post.topK.forEach((sub) => {
          const topKElem = document.createElement('a');
          topKElem.setAttribute("class", "topK");
          topKElem.href = sub.link;
          topKElem.textContent = sub.title + " (" + sub.similarity + ")";
          topKElem.target = "_blank";
          relevancyContainer.appendChild(topKElem);
        });
        postElement.appendChild(relevancyContainer);
        return postElement;
      }
      function loadPosts() {
        // Simulate loading more posts
        posts.forEach((post) => {
          const postElement = createPostElement(post);
          content.appendChild(postElement);
        });
      }
      // Load initial posts
      loadPosts();
    </script>

  </body>
</html>


