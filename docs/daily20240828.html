<!DOCTYPE html>
<html lang="en">
  <head>
    <script async src="https://www.googletagmanager.com/gtag/js?id=G-V34CNNDP8V"></script>
    <script>
      window.dataLayer = window.dataLayer || [];
      function gtag(){dataLayer.push(arguments);}
      gtag('js', new Date());
      gtag('config', 'G-V34CNNDP8V');
    </script>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Arxiv Paper Selection</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.3/css/all.min.css" crossorigin="anonymous" referrerpolicy="no-referrer" />
    <style>
    body {
      font-family: Arial, sans-serif;
      margin: 0;
      padding: 0;
      background-color: #f4f4f4;
    }
    header {
      position: fixed;
      top: 0;
      left: 0;
      width: 100%;
      background-color: #ffffff;
      color: black;
      padding: 10px;
      text-align: center;
      z-index: 1000;
      border-bottom: 1px solid #ddd;
    }
    header div {
      display: block;
      margin: 10px auto;
    }

    #home-icon {
      display: block;
      float: left;
      margin: 5px;
      text-decoration: none;
      color: black;
    }

    main {
      margin-top: 60px; /* Adjusted margin to account for fixed header */
      padding: 20px;
    }

    .post {
      background-color: white;
      border: 1px solid #ddd;
      border-radius: 5px;
      margin-bottom: 10px;
      padding: 10px 20px;
      max-height: 2000px;
      overflow: scroll;
    }
    .post img {
      display: block;
      margin-top: 5px;
      max-width: auto;
      max-height: 100px;
    }
    .post .clear {
      clear: both;
      display: block;
    }
    .post a {
      text-decoration: none;
    }
    .post a:hover {
      color: #0056b3;
    }
    .post a:visited {
      color: #0056b3;
    }
    .post div.comment {
      text-align: right;
    }
    .post div.comment a {
      margin: 1em;
    }
    .post .text {
      margin: 1em 0em;
      padding: 0;
    }
    .post .text .title {
    }
    .post .text .author {
    }
    .post .text .abstract {
    }
    .post .topK {
      display: block;
      margin: 0.5em;
    }
    .post .date {
      margin: 0;
      padding: 0;
      text-size: small; 
      color: gray;
    }
    .post .link {
      margin: 0;
      padding: 0;
    }
    @media screen and (max-width: 600px) {
      body {
        max-width: 100%; 
      }
      #home-icon {
        float: none;
        display: block;
        text-align: center;
        margin-bottom: 10px;
      }
    }
    footer {
      width: 100%;
      background-color: #ddd;
      text-align: center;
      z-index: 1000;
      padding: 20px 0px;
      margin-bottom: 20px;
      left: 0;
    }

    #next-btn,
    #prev-btn {
      background-color: #4CAF50;
      color: white;
      padding: 8px 16px;
      margin: 0 50px;
      border: none;
      border-radius: 4px;
      cursor: pointer;
    }

    .links {
      padding: 20px;
    }
    .links a {
      text-decoration: none;
    }
    .links a:hover {
      color: #0056b3;
    }
    .links a:visited {
      color: #0056b3;
    }

    #page-index {
      font-size: small;
    }
    .ads {
      width: 100%;
    }
    #prev-btn {
      background-color: #4CAF50;
      color: white;
      padding: 8px 16px;
      margin: 0 50px;
      border: none;
      border-radius: 4px;
      cursor: pointer;
    }
    </style>
  </head>
  <body>

    <header>
      <a id="prev-btn" href="daily20240827.html"><i class="fas fa-chevron-left"></i></a>
      <a href="https://haoxiang.org/">About</a>
    </header>

    <main id="content">
      <!-- Posts will be dynamically added here using JavaScript -->
    </main>

    <script>
      // Dummy data for posts
      const posts = [
{"title": "Drone-assisted Road Gaussian Splatting with Cross-view Uncertainty", "author": "Saining Zhang and Baijun Ye and Xiaoxue Chen and Yuantao Chen and Zongzheng Zhang and Cheng Peng and Yongliang Shi and Hao Zhao", "abstract": "  Robust and realistic rendering for large-scale road scenes is essential in\nautonomous driving simulation. Recently, 3D Gaussian Splatting (3D-GS) has made\ngroundbreaking progress in neural rendering, but the general fidelity of\nlarge-scale road scene renderings is often limited by the input imagery, which\nusually has a narrow field of view and focuses mainly on the street-level local\narea. Intuitively, the data from the drone's perspective can provide a\ncomplementary viewpoint for the data from the ground vehicle's perspective,\nenhancing the completeness of scene reconstruction and rendering. However,\ntraining naively with aerial and ground images, which exhibit large view\ndisparity, poses a significant convergence challenge for 3D-GS, and does not\ndemonstrate remarkable improvements in performance on road views. In order to\nenhance the novel view synthesis of road views and to effectively use the\naerial information, we design an uncertainty-aware training method that allows\naerial images to assist in the synthesis of areas where ground images have poor\nlearning outcomes instead of weighting all pixels equally in 3D-GS training\nlike prior work did. We are the first to introduce the cross-view uncertainty\nto 3D-GS by matching the car-view ensemble-based rendering uncertainty to\naerial images, weighting the contribution of each pixel to the training\nprocess. Additionally, to systematically quantify evaluation metrics, we\nassemble a high-quality synthesized dataset comprising both aerial and ground\nimages for road scenes.\n", "link": "http://arxiv.org/abs/2408.15242v1", "date": "2024-08-27", "relevancy": 3.1762, "topK": [{"title": "RTG-SLAM: Real-time 3D Reconstruction at Scale using Gaussian Splatting", "link": "http://arxiv.org/abs/2404.19706v1", "similarity": 0.6747}, {"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.6168}, {"title": "Dynamic Gaussians Mesh: Consistent Mesh Reconstruction from Monocular\n  Videos", "link": "http://arxiv.org/abs/2404.12379v2", "similarity": 0.6142}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Drone-assisted%20Road%20Gaussian%20Splatting%20with%20Cross-view%20Uncertainty&body=Title%3A%20Drone-assisted%20Road%20Gaussian%20Splatting%20with%20Cross-view%20Uncertainty%0AAuthor%3A%20Saining%20Zhang%20and%20Baijun%20Ye%20and%20Xiaoxue%20Chen%20and%20Yuantao%20Chen%20and%20Zongzheng%20Zhang%20and%20Cheng%20Peng%20and%20Yongliang%20Shi%20and%20Hao%20Zhao%0AAbstract%3A%20%20%20Robust%20and%20realistic%20rendering%20for%20large-scale%20road%20scenes%20is%20essential%20in%0Aautonomous%20driving%20simulation.%20Recently%2C%203D%20Gaussian%20Splatting%20%283D-GS%29%20has%20made%0Agroundbreaking%20progress%20in%20neural%20rendering%2C%20but%20the%20general%20fidelity%20of%0Alarge-scale%20road%20scene%20renderings%20is%20often%20limited%20by%20the%20input%20imagery%2C%20which%0Ausually%20has%20a%20narrow%20field%20of%20view%20and%20focuses%20mainly%20on%20the%20street-level%20local%0Aarea.%20Intuitively%2C%20the%20data%20from%20the%20drone%27s%20perspective%20can%20provide%20a%0Acomplementary%20viewpoint%20for%20the%20data%20from%20the%20ground%20vehicle%27s%20perspective%2C%0Aenhancing%20the%20completeness%20of%20scene%20reconstruction%20and%20rendering.%20However%2C%0Atraining%20naively%20with%20aerial%20and%20ground%20images%2C%20which%20exhibit%20large%20view%0Adisparity%2C%20poses%20a%20significant%20convergence%20challenge%20for%203D-GS%2C%20and%20does%20not%0Ademonstrate%20remarkable%20improvements%20in%20performance%20on%20road%20views.%20In%20order%20to%0Aenhance%20the%20novel%20view%20synthesis%20of%20road%20views%20and%20to%20effectively%20use%20the%0Aaerial%20information%2C%20we%20design%20an%20uncertainty-aware%20training%20method%20that%20allows%0Aaerial%20images%20to%20assist%20in%20the%20synthesis%20of%20areas%20where%20ground%20images%20have%20poor%0Alearning%20outcomes%20instead%20of%20weighting%20all%20pixels%20equally%20in%203D-GS%20training%0Alike%20prior%20work%20did.%20We%20are%20the%20first%20to%20introduce%20the%20cross-view%20uncertainty%0Ato%203D-GS%20by%20matching%20the%20car-view%20ensemble-based%20rendering%20uncertainty%20to%0Aaerial%20images%2C%20weighting%20the%20contribution%20of%20each%20pixel%20to%20the%20training%0Aprocess.%20Additionally%2C%20to%20systematically%20quantify%20evaluation%20metrics%2C%20we%0Aassemble%20a%20high-quality%20synthesized%20dataset%20comprising%20both%20aerial%20and%20ground%0Aimages%20for%20road%20scenes.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2408.15242v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DDrone-assisted%2520Road%2520Gaussian%2520Splatting%2520with%2520Cross-view%2520Uncertainty%26entry.906535625%3DSaining%2520Zhang%2520and%2520Baijun%2520Ye%2520and%2520Xiaoxue%2520Chen%2520and%2520Yuantao%2520Chen%2520and%2520Zongzheng%2520Zhang%2520and%2520Cheng%2520Peng%2520and%2520Yongliang%2520Shi%2520and%2520Hao%2520Zhao%26entry.1292438233%3D%2520%2520Robust%2520and%2520realistic%2520rendering%2520for%2520large-scale%2520road%2520scenes%2520is%2520essential%2520in%250Aautonomous%2520driving%2520simulation.%2520Recently%252C%25203D%2520Gaussian%2520Splatting%2520%25283D-GS%2529%2520has%2520made%250Agroundbreaking%2520progress%2520in%2520neural%2520rendering%252C%2520but%2520the%2520general%2520fidelity%2520of%250Alarge-scale%2520road%2520scene%2520renderings%2520is%2520often%2520limited%2520by%2520the%2520input%2520imagery%252C%2520which%250Ausually%2520has%2520a%2520narrow%2520field%2520of%2520view%2520and%2520focuses%2520mainly%2520on%2520the%2520street-level%2520local%250Aarea.%2520Intuitively%252C%2520the%2520data%2520from%2520the%2520drone%2527s%2520perspective%2520can%2520provide%2520a%250Acomplementary%2520viewpoint%2520for%2520the%2520data%2520from%2520the%2520ground%2520vehicle%2527s%2520perspective%252C%250Aenhancing%2520the%2520completeness%2520of%2520scene%2520reconstruction%2520and%2520rendering.%2520However%252C%250Atraining%2520naively%2520with%2520aerial%2520and%2520ground%2520images%252C%2520which%2520exhibit%2520large%2520view%250Adisparity%252C%2520poses%2520a%2520significant%2520convergence%2520challenge%2520for%25203D-GS%252C%2520and%2520does%2520not%250Ademonstrate%2520remarkable%2520improvements%2520in%2520performance%2520on%2520road%2520views.%2520In%2520order%2520to%250Aenhance%2520the%2520novel%2520view%2520synthesis%2520of%2520road%2520views%2520and%2520to%2520effectively%2520use%2520the%250Aaerial%2520information%252C%2520we%2520design%2520an%2520uncertainty-aware%2520training%2520method%2520that%2520allows%250Aaerial%2520images%2520to%2520assist%2520in%2520the%2520synthesis%2520of%2520areas%2520where%2520ground%2520images%2520have%2520poor%250Alearning%2520outcomes%2520instead%2520of%2520weighting%2520all%2520pixels%2520equally%2520in%25203D-GS%2520training%250Alike%2520prior%2520work%2520did.%2520We%2520are%2520the%2520first%2520to%2520introduce%2520the%2520cross-view%2520uncertainty%250Ato%25203D-GS%2520by%2520matching%2520the%2520car-view%2520ensemble-based%2520rendering%2520uncertainty%2520to%250Aaerial%2520images%252C%2520weighting%2520the%2520contribution%2520of%2520each%2520pixel%2520to%2520the%2520training%250Aprocess.%2520Additionally%252C%2520to%2520systematically%2520quantify%2520evaluation%2520metrics%252C%2520we%250Aassemble%2520a%2520high-quality%2520synthesized%2520dataset%2520comprising%2520both%2520aerial%2520and%2520ground%250Aimages%2520for%2520road%2520scenes.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2408.15242v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Drone-assisted%20Road%20Gaussian%20Splatting%20with%20Cross-view%20Uncertainty&entry.906535625=Saining%20Zhang%20and%20Baijun%20Ye%20and%20Xiaoxue%20Chen%20and%20Yuantao%20Chen%20and%20Zongzheng%20Zhang%20and%20Cheng%20Peng%20and%20Yongliang%20Shi%20and%20Hao%20Zhao&entry.1292438233=%20%20Robust%20and%20realistic%20rendering%20for%20large-scale%20road%20scenes%20is%20essential%20in%0Aautonomous%20driving%20simulation.%20Recently%2C%203D%20Gaussian%20Splatting%20%283D-GS%29%20has%20made%0Agroundbreaking%20progress%20in%20neural%20rendering%2C%20but%20the%20general%20fidelity%20of%0Alarge-scale%20road%20scene%20renderings%20is%20often%20limited%20by%20the%20input%20imagery%2C%20which%0Ausually%20has%20a%20narrow%20field%20of%20view%20and%20focuses%20mainly%20on%20the%20street-level%20local%0Aarea.%20Intuitively%2C%20the%20data%20from%20the%20drone%27s%20perspective%20can%20provide%20a%0Acomplementary%20viewpoint%20for%20the%20data%20from%20the%20ground%20vehicle%27s%20perspective%2C%0Aenhancing%20the%20completeness%20of%20scene%20reconstruction%20and%20rendering.%20However%2C%0Atraining%20naively%20with%20aerial%20and%20ground%20images%2C%20which%20exhibit%20large%20view%0Adisparity%2C%20poses%20a%20significant%20convergence%20challenge%20for%203D-GS%2C%20and%20does%20not%0Ademonstrate%20remarkable%20improvements%20in%20performance%20on%20road%20views.%20In%20order%20to%0Aenhance%20the%20novel%20view%20synthesis%20of%20road%20views%20and%20to%20effectively%20use%20the%0Aaerial%20information%2C%20we%20design%20an%20uncertainty-aware%20training%20method%20that%20allows%0Aaerial%20images%20to%20assist%20in%20the%20synthesis%20of%20areas%20where%20ground%20images%20have%20poor%0Alearning%20outcomes%20instead%20of%20weighting%20all%20pixels%20equally%20in%203D-GS%20training%0Alike%20prior%20work%20did.%20We%20are%20the%20first%20to%20introduce%20the%20cross-view%20uncertainty%0Ato%203D-GS%20by%20matching%20the%20car-view%20ensemble-based%20rendering%20uncertainty%20to%0Aaerial%20images%2C%20weighting%20the%20contribution%20of%20each%20pixel%20to%20the%20training%0Aprocess.%20Additionally%2C%20to%20systematically%20quantify%20evaluation%20metrics%2C%20we%0Aassemble%20a%20high-quality%20synthesized%20dataset%20comprising%20both%20aerial%20and%20ground%0Aimages%20for%20road%20scenes.%0A&entry.1838667208=http%3A//arxiv.org/abs/2408.15242v1&entry.124074799=Read"},
{"title": "BOX3D: Lightweight Camera-LiDAR Fusion for 3D Object Detection and\n  Localization", "author": "Mario A. V. Saucedo and Nikolaos Stathoulopoulos and Vidya Sumathy and Christoforos Kanellakis and George Nikolakopoulos", "abstract": "  Object detection and global localization play a crucial role in robotics,\nspanning across a great spectrum of applications from autonomous cars to\nmulti-layered 3D Scene Graphs for semantic scene understanding. This article\nproposes BOX3D, a novel multi-modal and lightweight scheme for localizing\nobjects of interest by fusing the information from RGB camera and 3D LiDAR.\nBOX3D is structured around a three-layered architecture, building up from the\nlocal perception of the incoming sequential sensor data to the global\nperception refinement that covers for outliers and the general consistency of\neach object's observation. More specifically, the first layer handles the\nlow-level fusion of camera and LiDAR data for initial 3D bounding box\nextraction. The second layer converts each LiDAR's scan 3D bounding boxes to\nthe world coordinate frame and applies a spatial pairing and merging mechanism\nto maintain the uniqueness of objects observed from different viewpoints.\nFinally, BOX3D integrates the third layer that supervises the consistency of\nthe results on the global map iteratively, using a point-to-voxel comparison\nfor identifying all points in the global map that belong to the object.\nBenchmarking results of the proposed novel architecture are showcased in\nmultiple experimental trials on public state-of-the-art large-scale dataset of\nurban environments.\n", "link": "http://arxiv.org/abs/2408.14941v1", "date": "2024-08-27", "relevancy": 3.0185, "topK": [{"title": "DDM-NET: End-to-end learning of keypoint feature Detection, Description and Matching for 3D localization", "link": "https://arxiv.org/abs/2212.04575", "similarity": 0.6536}, {"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.5837}, {"title": "RTG-SLAM: Real-time 3D Reconstruction at Scale using Gaussian Splatting", "link": "http://arxiv.org/abs/2404.19706v1", "similarity": 0.5738}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20BOX3D%3A%20Lightweight%20Camera-LiDAR%20Fusion%20for%203D%20Object%20Detection%20and%0A%20%20Localization&body=Title%3A%20BOX3D%3A%20Lightweight%20Camera-LiDAR%20Fusion%20for%203D%20Object%20Detection%20and%0A%20%20Localization%0AAuthor%3A%20Mario%20A.%20V.%20Saucedo%20and%20Nikolaos%20Stathoulopoulos%20and%20Vidya%20Sumathy%20and%20Christoforos%20Kanellakis%20and%20George%20Nikolakopoulos%0AAbstract%3A%20%20%20Object%20detection%20and%20global%20localization%20play%20a%20crucial%20role%20in%20robotics%2C%0Aspanning%20across%20a%20great%20spectrum%20of%20applications%20from%20autonomous%20cars%20to%0Amulti-layered%203D%20Scene%20Graphs%20for%20semantic%20scene%20understanding.%20This%20article%0Aproposes%20BOX3D%2C%20a%20novel%20multi-modal%20and%20lightweight%20scheme%20for%20localizing%0Aobjects%20of%20interest%20by%20fusing%20the%20information%20from%20RGB%20camera%20and%203D%20LiDAR.%0ABOX3D%20is%20structured%20around%20a%20three-layered%20architecture%2C%20building%20up%20from%20the%0Alocal%20perception%20of%20the%20incoming%20sequential%20sensor%20data%20to%20the%20global%0Aperception%20refinement%20that%20covers%20for%20outliers%20and%20the%20general%20consistency%20of%0Aeach%20object%27s%20observation.%20More%20specifically%2C%20the%20first%20layer%20handles%20the%0Alow-level%20fusion%20of%20camera%20and%20LiDAR%20data%20for%20initial%203D%20bounding%20box%0Aextraction.%20The%20second%20layer%20converts%20each%20LiDAR%27s%20scan%203D%20bounding%20boxes%20to%0Athe%20world%20coordinate%20frame%20and%20applies%20a%20spatial%20pairing%20and%20merging%20mechanism%0Ato%20maintain%20the%20uniqueness%20of%20objects%20observed%20from%20different%20viewpoints.%0AFinally%2C%20BOX3D%20integrates%20the%20third%20layer%20that%20supervises%20the%20consistency%20of%0Athe%20results%20on%20the%20global%20map%20iteratively%2C%20using%20a%20point-to-voxel%20comparison%0Afor%20identifying%20all%20points%20in%20the%20global%20map%20that%20belong%20to%20the%20object.%0ABenchmarking%20results%20of%20the%20proposed%20novel%20architecture%20are%20showcased%20in%0Amultiple%20experimental%20trials%20on%20public%20state-of-the-art%20large-scale%20dataset%20of%0Aurban%20environments.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2408.14941v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DBOX3D%253A%2520Lightweight%2520Camera-LiDAR%2520Fusion%2520for%25203D%2520Object%2520Detection%2520and%250A%2520%2520Localization%26entry.906535625%3DMario%2520A.%2520V.%2520Saucedo%2520and%2520Nikolaos%2520Stathoulopoulos%2520and%2520Vidya%2520Sumathy%2520and%2520Christoforos%2520Kanellakis%2520and%2520George%2520Nikolakopoulos%26entry.1292438233%3D%2520%2520Object%2520detection%2520and%2520global%2520localization%2520play%2520a%2520crucial%2520role%2520in%2520robotics%252C%250Aspanning%2520across%2520a%2520great%2520spectrum%2520of%2520applications%2520from%2520autonomous%2520cars%2520to%250Amulti-layered%25203D%2520Scene%2520Graphs%2520for%2520semantic%2520scene%2520understanding.%2520This%2520article%250Aproposes%2520BOX3D%252C%2520a%2520novel%2520multi-modal%2520and%2520lightweight%2520scheme%2520for%2520localizing%250Aobjects%2520of%2520interest%2520by%2520fusing%2520the%2520information%2520from%2520RGB%2520camera%2520and%25203D%2520LiDAR.%250ABOX3D%2520is%2520structured%2520around%2520a%2520three-layered%2520architecture%252C%2520building%2520up%2520from%2520the%250Alocal%2520perception%2520of%2520the%2520incoming%2520sequential%2520sensor%2520data%2520to%2520the%2520global%250Aperception%2520refinement%2520that%2520covers%2520for%2520outliers%2520and%2520the%2520general%2520consistency%2520of%250Aeach%2520object%2527s%2520observation.%2520More%2520specifically%252C%2520the%2520first%2520layer%2520handles%2520the%250Alow-level%2520fusion%2520of%2520camera%2520and%2520LiDAR%2520data%2520for%2520initial%25203D%2520bounding%2520box%250Aextraction.%2520The%2520second%2520layer%2520converts%2520each%2520LiDAR%2527s%2520scan%25203D%2520bounding%2520boxes%2520to%250Athe%2520world%2520coordinate%2520frame%2520and%2520applies%2520a%2520spatial%2520pairing%2520and%2520merging%2520mechanism%250Ato%2520maintain%2520the%2520uniqueness%2520of%2520objects%2520observed%2520from%2520different%2520viewpoints.%250AFinally%252C%2520BOX3D%2520integrates%2520the%2520third%2520layer%2520that%2520supervises%2520the%2520consistency%2520of%250Athe%2520results%2520on%2520the%2520global%2520map%2520iteratively%252C%2520using%2520a%2520point-to-voxel%2520comparison%250Afor%2520identifying%2520all%2520points%2520in%2520the%2520global%2520map%2520that%2520belong%2520to%2520the%2520object.%250ABenchmarking%2520results%2520of%2520the%2520proposed%2520novel%2520architecture%2520are%2520showcased%2520in%250Amultiple%2520experimental%2520trials%2520on%2520public%2520state-of-the-art%2520large-scale%2520dataset%2520of%250Aurban%2520environments.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2408.14941v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=BOX3D%3A%20Lightweight%20Camera-LiDAR%20Fusion%20for%203D%20Object%20Detection%20and%0A%20%20Localization&entry.906535625=Mario%20A.%20V.%20Saucedo%20and%20Nikolaos%20Stathoulopoulos%20and%20Vidya%20Sumathy%20and%20Christoforos%20Kanellakis%20and%20George%20Nikolakopoulos&entry.1292438233=%20%20Object%20detection%20and%20global%20localization%20play%20a%20crucial%20role%20in%20robotics%2C%0Aspanning%20across%20a%20great%20spectrum%20of%20applications%20from%20autonomous%20cars%20to%0Amulti-layered%203D%20Scene%20Graphs%20for%20semantic%20scene%20understanding.%20This%20article%0Aproposes%20BOX3D%2C%20a%20novel%20multi-modal%20and%20lightweight%20scheme%20for%20localizing%0Aobjects%20of%20interest%20by%20fusing%20the%20information%20from%20RGB%20camera%20and%203D%20LiDAR.%0ABOX3D%20is%20structured%20around%20a%20three-layered%20architecture%2C%20building%20up%20from%20the%0Alocal%20perception%20of%20the%20incoming%20sequential%20sensor%20data%20to%20the%20global%0Aperception%20refinement%20that%20covers%20for%20outliers%20and%20the%20general%20consistency%20of%0Aeach%20object%27s%20observation.%20More%20specifically%2C%20the%20first%20layer%20handles%20the%0Alow-level%20fusion%20of%20camera%20and%20LiDAR%20data%20for%20initial%203D%20bounding%20box%0Aextraction.%20The%20second%20layer%20converts%20each%20LiDAR%27s%20scan%203D%20bounding%20boxes%20to%0Athe%20world%20coordinate%20frame%20and%20applies%20a%20spatial%20pairing%20and%20merging%20mechanism%0Ato%20maintain%20the%20uniqueness%20of%20objects%20observed%20from%20different%20viewpoints.%0AFinally%2C%20BOX3D%20integrates%20the%20third%20layer%20that%20supervises%20the%20consistency%20of%0Athe%20results%20on%20the%20global%20map%20iteratively%2C%20using%20a%20point-to-voxel%20comparison%0Afor%20identifying%20all%20points%20in%20the%20global%20map%20that%20belong%20to%20the%20object.%0ABenchmarking%20results%20of%20the%20proposed%20novel%20architecture%20are%20showcased%20in%0Amultiple%20experimental%20trials%20on%20public%20state-of-the-art%20large-scale%20dataset%20of%0Aurban%20environments.%0A&entry.1838667208=http%3A//arxiv.org/abs/2408.14941v1&entry.124074799=Read"},
{"title": "Few-Shot Unsupervised Implicit Neural Shape Representation Learning with\n  Spatial Adversaries", "author": "Amine Ouasfi and Adnane Boukhayma", "abstract": "  Implicit Neural Representations have gained prominence as a powerful\nframework for capturing complex data modalities, encompassing a wide range from\n3D shapes to images and audio. Within the realm of 3D shape representation,\nNeural Signed Distance Functions (SDF) have demonstrated remarkable potential\nin faithfully encoding intricate shape geometry. However, learning SDFs from\nsparse 3D point clouds in the absence of ground truth supervision remains a\nvery challenging task. While recent methods rely on smoothness priors to\nregularize the learning, our method introduces a regularization term that\nleverages adversarial samples around the shape to improve the learned SDFs.\nThrough extensive experiments and evaluations, we illustrate the efficacy of\nour proposed method, highlighting its capacity to improve SDF learning with\nrespect to baselines and the state-of-the-art using synthetic and real data.\n", "link": "http://arxiv.org/abs/2408.15114v1", "date": "2024-08-27", "relevancy": 2.9929, "topK": [{"title": "IAE: Implicit Autoencoder for Point Cloud Self-supervised Representation Learning", "link": "https://arxiv.org/abs/2201.00785", "similarity": 0.6243}, {"title": "Self-supervised Photographic Image Layout Representation Learning", "link": "http://arxiv.org/abs/2403.03740v1", "similarity": 0.5882}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5833}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Few-Shot%20Unsupervised%20Implicit%20Neural%20Shape%20Representation%20Learning%20with%0A%20%20Spatial%20Adversaries&body=Title%3A%20Few-Shot%20Unsupervised%20Implicit%20Neural%20Shape%20Representation%20Learning%20with%0A%20%20Spatial%20Adversaries%0AAuthor%3A%20Amine%20Ouasfi%20and%20Adnane%20Boukhayma%0AAbstract%3A%20%20%20Implicit%20Neural%20Representations%20have%20gained%20prominence%20as%20a%20powerful%0Aframework%20for%20capturing%20complex%20data%20modalities%2C%20encompassing%20a%20wide%20range%20from%0A3D%20shapes%20to%20images%20and%20audio.%20Within%20the%20realm%20of%203D%20shape%20representation%2C%0ANeural%20Signed%20Distance%20Functions%20%28SDF%29%20have%20demonstrated%20remarkable%20potential%0Ain%20faithfully%20encoding%20intricate%20shape%20geometry.%20However%2C%20learning%20SDFs%20from%0Asparse%203D%20point%20clouds%20in%20the%20absence%20of%20ground%20truth%20supervision%20remains%20a%0Avery%20challenging%20task.%20While%20recent%20methods%20rely%20on%20smoothness%20priors%20to%0Aregularize%20the%20learning%2C%20our%20method%20introduces%20a%20regularization%20term%20that%0Aleverages%20adversarial%20samples%20around%20the%20shape%20to%20improve%20the%20learned%20SDFs.%0AThrough%20extensive%20experiments%20and%20evaluations%2C%20we%20illustrate%20the%20efficacy%20of%0Aour%20proposed%20method%2C%20highlighting%20its%20capacity%20to%20improve%20SDF%20learning%20with%0Arespect%20to%20baselines%20and%20the%20state-of-the-art%20using%20synthetic%20and%20real%20data.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2408.15114v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DFew-Shot%2520Unsupervised%2520Implicit%2520Neural%2520Shape%2520Representation%2520Learning%2520with%250A%2520%2520Spatial%2520Adversaries%26entry.906535625%3DAmine%2520Ouasfi%2520and%2520Adnane%2520Boukhayma%26entry.1292438233%3D%2520%2520Implicit%2520Neural%2520Representations%2520have%2520gained%2520prominence%2520as%2520a%2520powerful%250Aframework%2520for%2520capturing%2520complex%2520data%2520modalities%252C%2520encompassing%2520a%2520wide%2520range%2520from%250A3D%2520shapes%2520to%2520images%2520and%2520audio.%2520Within%2520the%2520realm%2520of%25203D%2520shape%2520representation%252C%250ANeural%2520Signed%2520Distance%2520Functions%2520%2528SDF%2529%2520have%2520demonstrated%2520remarkable%2520potential%250Ain%2520faithfully%2520encoding%2520intricate%2520shape%2520geometry.%2520However%252C%2520learning%2520SDFs%2520from%250Asparse%25203D%2520point%2520clouds%2520in%2520the%2520absence%2520of%2520ground%2520truth%2520supervision%2520remains%2520a%250Avery%2520challenging%2520task.%2520While%2520recent%2520methods%2520rely%2520on%2520smoothness%2520priors%2520to%250Aregularize%2520the%2520learning%252C%2520our%2520method%2520introduces%2520a%2520regularization%2520term%2520that%250Aleverages%2520adversarial%2520samples%2520around%2520the%2520shape%2520to%2520improve%2520the%2520learned%2520SDFs.%250AThrough%2520extensive%2520experiments%2520and%2520evaluations%252C%2520we%2520illustrate%2520the%2520efficacy%2520of%250Aour%2520proposed%2520method%252C%2520highlighting%2520its%2520capacity%2520to%2520improve%2520SDF%2520learning%2520with%250Arespect%2520to%2520baselines%2520and%2520the%2520state-of-the-art%2520using%2520synthetic%2520and%2520real%2520data.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2408.15114v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Few-Shot%20Unsupervised%20Implicit%20Neural%20Shape%20Representation%20Learning%20with%0A%20%20Spatial%20Adversaries&entry.906535625=Amine%20Ouasfi%20and%20Adnane%20Boukhayma&entry.1292438233=%20%20Implicit%20Neural%20Representations%20have%20gained%20prominence%20as%20a%20powerful%0Aframework%20for%20capturing%20complex%20data%20modalities%2C%20encompassing%20a%20wide%20range%20from%0A3D%20shapes%20to%20images%20and%20audio.%20Within%20the%20realm%20of%203D%20shape%20representation%2C%0ANeural%20Signed%20Distance%20Functions%20%28SDF%29%20have%20demonstrated%20remarkable%20potential%0Ain%20faithfully%20encoding%20intricate%20shape%20geometry.%20However%2C%20learning%20SDFs%20from%0Asparse%203D%20point%20clouds%20in%20the%20absence%20of%20ground%20truth%20supervision%20remains%20a%0Avery%20challenging%20task.%20While%20recent%20methods%20rely%20on%20smoothness%20priors%20to%0Aregularize%20the%20learning%2C%20our%20method%20introduces%20a%20regularization%20term%20that%0Aleverages%20adversarial%20samples%20around%20the%20shape%20to%20improve%20the%20learned%20SDFs.%0AThrough%20extensive%20experiments%20and%20evaluations%2C%20we%20illustrate%20the%20efficacy%20of%0Aour%20proposed%20method%2C%20highlighting%20its%20capacity%20to%20improve%20SDF%20learning%20with%0Arespect%20to%20baselines%20and%20the%20state-of-the-art%20using%20synthetic%20and%20real%20data.%0A&entry.1838667208=http%3A//arxiv.org/abs/2408.15114v1&entry.124074799=Read"},
{"title": "TAPVid-3D: A Benchmark for Tracking Any Point in 3D", "author": "Skanda Koppula and Ignacio Rocco and Yi Yang and Joe Heyward and Jo\u00e3o Carreira and Andrew Zisserman and Gabriel Brostow and Carl Doersch", "abstract": "  We introduce a new benchmark, TAPVid-3D, for evaluating the task of\nlong-range Tracking Any Point in 3D (TAP-3D). While point tracking in two\ndimensions (TAP) has many benchmarks measuring performance on real-world\nvideos, such as TAPVid-DAVIS, three-dimensional point tracking has none. To\nthis end, leveraging existing footage, we build a new benchmark for 3D point\ntracking featuring 4,000+ real-world videos, composed of three different data\nsources spanning a variety of object types, motion patterns, and indoor and\noutdoor environments. To measure performance on the TAP-3D task, we formulate a\ncollection of metrics that extend the Jaccard-based metric used in TAP to\nhandle the complexities of ambiguous depth scales across models, occlusions,\nand multi-track spatio-temporal smoothness. We manually verify a large sample\nof trajectories to ensure correct video annotations, and assess the current\nstate of the TAP-3D task by constructing competitive baselines using existing\ntracking models. We anticipate this benchmark will serve as a guidepost to\nimprove our ability to understand precise 3D motion and surface deformation\nfrom monocular video. Code for dataset download, generation, and model\nevaluation is available at https://tapvid3d.github.io\n", "link": "http://arxiv.org/abs/2407.05921v2", "date": "2024-08-27", "relevancy": 2.9534, "topK": [{"title": "CAT3D: Create Anything in 3D with Multi-View Diffusion Models", "link": "http://arxiv.org/abs/2405.10314v1", "similarity": 0.6041}, {"title": "CAT3D: Create Anything in 3D with Multi-View Diffusion Models", "link": "http://arxiv.org/abs/2405.10314v1", "similarity": 0.6041}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5638}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20TAPVid-3D%3A%20A%20Benchmark%20for%20Tracking%20Any%20Point%20in%203D&body=Title%3A%20TAPVid-3D%3A%20A%20Benchmark%20for%20Tracking%20Any%20Point%20in%203D%0AAuthor%3A%20Skanda%20Koppula%20and%20Ignacio%20Rocco%20and%20Yi%20Yang%20and%20Joe%20Heyward%20and%20Jo%C3%A3o%20Carreira%20and%20Andrew%20Zisserman%20and%20Gabriel%20Brostow%20and%20Carl%20Doersch%0AAbstract%3A%20%20%20We%20introduce%20a%20new%20benchmark%2C%20TAPVid-3D%2C%20for%20evaluating%20the%20task%20of%0Along-range%20Tracking%20Any%20Point%20in%203D%20%28TAP-3D%29.%20While%20point%20tracking%20in%20two%0Adimensions%20%28TAP%29%20has%20many%20benchmarks%20measuring%20performance%20on%20real-world%0Avideos%2C%20such%20as%20TAPVid-DAVIS%2C%20three-dimensional%20point%20tracking%20has%20none.%20To%0Athis%20end%2C%20leveraging%20existing%20footage%2C%20we%20build%20a%20new%20benchmark%20for%203D%20point%0Atracking%20featuring%204%2C000%2B%20real-world%20videos%2C%20composed%20of%20three%20different%20data%0Asources%20spanning%20a%20variety%20of%20object%20types%2C%20motion%20patterns%2C%20and%20indoor%20and%0Aoutdoor%20environments.%20To%20measure%20performance%20on%20the%20TAP-3D%20task%2C%20we%20formulate%20a%0Acollection%20of%20metrics%20that%20extend%20the%20Jaccard-based%20metric%20used%20in%20TAP%20to%0Ahandle%20the%20complexities%20of%20ambiguous%20depth%20scales%20across%20models%2C%20occlusions%2C%0Aand%20multi-track%20spatio-temporal%20smoothness.%20We%20manually%20verify%20a%20large%20sample%0Aof%20trajectories%20to%20ensure%20correct%20video%20annotations%2C%20and%20assess%20the%20current%0Astate%20of%20the%20TAP-3D%20task%20by%20constructing%20competitive%20baselines%20using%20existing%0Atracking%20models.%20We%20anticipate%20this%20benchmark%20will%20serve%20as%20a%20guidepost%20to%0Aimprove%20our%20ability%20to%20understand%20precise%203D%20motion%20and%20surface%20deformation%0Afrom%20monocular%20video.%20Code%20for%20dataset%20download%2C%20generation%2C%20and%20model%0Aevaluation%20is%20available%20at%20https%3A//tapvid3d.github.io%0A%0ALink%3A%20http%3A//arxiv.org/abs/2407.05921v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DTAPVid-3D%253A%2520A%2520Benchmark%2520for%2520Tracking%2520Any%2520Point%2520in%25203D%26entry.906535625%3DSkanda%2520Koppula%2520and%2520Ignacio%2520Rocco%2520and%2520Yi%2520Yang%2520and%2520Joe%2520Heyward%2520and%2520Jo%25C3%25A3o%2520Carreira%2520and%2520Andrew%2520Zisserman%2520and%2520Gabriel%2520Brostow%2520and%2520Carl%2520Doersch%26entry.1292438233%3D%2520%2520We%2520introduce%2520a%2520new%2520benchmark%252C%2520TAPVid-3D%252C%2520for%2520evaluating%2520the%2520task%2520of%250Along-range%2520Tracking%2520Any%2520Point%2520in%25203D%2520%2528TAP-3D%2529.%2520While%2520point%2520tracking%2520in%2520two%250Adimensions%2520%2528TAP%2529%2520has%2520many%2520benchmarks%2520measuring%2520performance%2520on%2520real-world%250Avideos%252C%2520such%2520as%2520TAPVid-DAVIS%252C%2520three-dimensional%2520point%2520tracking%2520has%2520none.%2520To%250Athis%2520end%252C%2520leveraging%2520existing%2520footage%252C%2520we%2520build%2520a%2520new%2520benchmark%2520for%25203D%2520point%250Atracking%2520featuring%25204%252C000%252B%2520real-world%2520videos%252C%2520composed%2520of%2520three%2520different%2520data%250Asources%2520spanning%2520a%2520variety%2520of%2520object%2520types%252C%2520motion%2520patterns%252C%2520and%2520indoor%2520and%250Aoutdoor%2520environments.%2520To%2520measure%2520performance%2520on%2520the%2520TAP-3D%2520task%252C%2520we%2520formulate%2520a%250Acollection%2520of%2520metrics%2520that%2520extend%2520the%2520Jaccard-based%2520metric%2520used%2520in%2520TAP%2520to%250Ahandle%2520the%2520complexities%2520of%2520ambiguous%2520depth%2520scales%2520across%2520models%252C%2520occlusions%252C%250Aand%2520multi-track%2520spatio-temporal%2520smoothness.%2520We%2520manually%2520verify%2520a%2520large%2520sample%250Aof%2520trajectories%2520to%2520ensure%2520correct%2520video%2520annotations%252C%2520and%2520assess%2520the%2520current%250Astate%2520of%2520the%2520TAP-3D%2520task%2520by%2520constructing%2520competitive%2520baselines%2520using%2520existing%250Atracking%2520models.%2520We%2520anticipate%2520this%2520benchmark%2520will%2520serve%2520as%2520a%2520guidepost%2520to%250Aimprove%2520our%2520ability%2520to%2520understand%2520precise%25203D%2520motion%2520and%2520surface%2520deformation%250Afrom%2520monocular%2520video.%2520Code%2520for%2520dataset%2520download%252C%2520generation%252C%2520and%2520model%250Aevaluation%2520is%2520available%2520at%2520https%253A//tapvid3d.github.io%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2407.05921v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=TAPVid-3D%3A%20A%20Benchmark%20for%20Tracking%20Any%20Point%20in%203D&entry.906535625=Skanda%20Koppula%20and%20Ignacio%20Rocco%20and%20Yi%20Yang%20and%20Joe%20Heyward%20and%20Jo%C3%A3o%20Carreira%20and%20Andrew%20Zisserman%20and%20Gabriel%20Brostow%20and%20Carl%20Doersch&entry.1292438233=%20%20We%20introduce%20a%20new%20benchmark%2C%20TAPVid-3D%2C%20for%20evaluating%20the%20task%20of%0Along-range%20Tracking%20Any%20Point%20in%203D%20%28TAP-3D%29.%20While%20point%20tracking%20in%20two%0Adimensions%20%28TAP%29%20has%20many%20benchmarks%20measuring%20performance%20on%20real-world%0Avideos%2C%20such%20as%20TAPVid-DAVIS%2C%20three-dimensional%20point%20tracking%20has%20none.%20To%0Athis%20end%2C%20leveraging%20existing%20footage%2C%20we%20build%20a%20new%20benchmark%20for%203D%20point%0Atracking%20featuring%204%2C000%2B%20real-world%20videos%2C%20composed%20of%20three%20different%20data%0Asources%20spanning%20a%20variety%20of%20object%20types%2C%20motion%20patterns%2C%20and%20indoor%20and%0Aoutdoor%20environments.%20To%20measure%20performance%20on%20the%20TAP-3D%20task%2C%20we%20formulate%20a%0Acollection%20of%20metrics%20that%20extend%20the%20Jaccard-based%20metric%20used%20in%20TAP%20to%0Ahandle%20the%20complexities%20of%20ambiguous%20depth%20scales%20across%20models%2C%20occlusions%2C%0Aand%20multi-track%20spatio-temporal%20smoothness.%20We%20manually%20verify%20a%20large%20sample%0Aof%20trajectories%20to%20ensure%20correct%20video%20annotations%2C%20and%20assess%20the%20current%0Astate%20of%20the%20TAP-3D%20task%20by%20constructing%20competitive%20baselines%20using%20existing%0Atracking%20models.%20We%20anticipate%20this%20benchmark%20will%20serve%20as%20a%20guidepost%20to%0Aimprove%20our%20ability%20to%20understand%20precise%203D%20motion%20and%20surface%20deformation%0Afrom%20monocular%20video.%20Code%20for%20dataset%20download%2C%20generation%2C%20and%20model%0Aevaluation%20is%20available%20at%20https%3A//tapvid3d.github.io%0A&entry.1838667208=http%3A//arxiv.org/abs/2407.05921v2&entry.124074799=Read"},
{"title": "Fundus2Video: Cross-Modal Angiography Video Generation from Static\n  Fundus Photography with Clinical Knowledge Guidance", "author": "Weiyi Zhang and Siyu Huang and Jiancheng Yang and Ruoyu Chen and Zongyuan Ge and Yingfeng Zheng and Danli Shi and Mingguang He", "abstract": "  Fundus Fluorescein Angiography (FFA) is a critical tool for assessing retinal\nvascular dynamics and aiding in the diagnosis of eye diseases. However, its\ninvasive nature and less accessibility compared to Color Fundus (CF) images\npose significant challenges. Current CF to FFA translation methods are limited\nto static generation. In this work, we pioneer dynamic FFA video generation\nfrom static CF images. We introduce an autoregressive GAN for smooth,\nmemory-saving frame-by-frame FFA synthesis. To enhance the focus on dynamic\nlesion changes in FFA regions, we design a knowledge mask based on clinical\nexperience. Leveraging this mask, our approach integrates innovative knowledge\nmask-guided techniques, including knowledge-boosted attention, knowledge-aware\ndiscriminators, and mask-enhanced patchNCE loss, aimed at refining generation\nin critical areas and addressing the pixel misalignment challenge. Our method\nachieves the best FVD of 1503.21 and PSNR of 11.81 compared to other common\nvideo generation approaches. Human assessment by an ophthalmologist confirms\nits high generation quality. Notably, our knowledge mask surpasses supervised\nlesion segmentation masks, offering a promising non-invasive alternative to\ntraditional FFA for research and clinical applications. The code is available\nat https://github.com/Michi-3000/Fundus2Video.\n", "link": "http://arxiv.org/abs/2408.15217v1", "date": "2024-08-27", "relevancy": 2.9354, "topK": [{"title": "Tuning-Free Noise Rectification for High Fidelity Image-to-Video\n  Generation", "link": "http://arxiv.org/abs/2403.02827v1", "similarity": 0.6317}, {"title": "Magic-Me: Identity-Specific Video Customized Diffusion", "link": "http://arxiv.org/abs/2402.09368v1", "similarity": 0.585}, {"title": "Dynamic Gaussians Mesh: Consistent Mesh Reconstruction from Monocular\n  Videos", "link": "http://arxiv.org/abs/2404.12379v2", "similarity": 0.5446}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Fundus2Video%3A%20Cross-Modal%20Angiography%20Video%20Generation%20from%20Static%0A%20%20Fundus%20Photography%20with%20Clinical%20Knowledge%20Guidance&body=Title%3A%20Fundus2Video%3A%20Cross-Modal%20Angiography%20Video%20Generation%20from%20Static%0A%20%20Fundus%20Photography%20with%20Clinical%20Knowledge%20Guidance%0AAuthor%3A%20Weiyi%20Zhang%20and%20Siyu%20Huang%20and%20Jiancheng%20Yang%20and%20Ruoyu%20Chen%20and%20Zongyuan%20Ge%20and%20Yingfeng%20Zheng%20and%20Danli%20Shi%20and%20Mingguang%20He%0AAbstract%3A%20%20%20Fundus%20Fluorescein%20Angiography%20%28FFA%29%20is%20a%20critical%20tool%20for%20assessing%20retinal%0Avascular%20dynamics%20and%20aiding%20in%20the%20diagnosis%20of%20eye%20diseases.%20However%2C%20its%0Ainvasive%20nature%20and%20less%20accessibility%20compared%20to%20Color%20Fundus%20%28CF%29%20images%0Apose%20significant%20challenges.%20Current%20CF%20to%20FFA%20translation%20methods%20are%20limited%0Ato%20static%20generation.%20In%20this%20work%2C%20we%20pioneer%20dynamic%20FFA%20video%20generation%0Afrom%20static%20CF%20images.%20We%20introduce%20an%20autoregressive%20GAN%20for%20smooth%2C%0Amemory-saving%20frame-by-frame%20FFA%20synthesis.%20To%20enhance%20the%20focus%20on%20dynamic%0Alesion%20changes%20in%20FFA%20regions%2C%20we%20design%20a%20knowledge%20mask%20based%20on%20clinical%0Aexperience.%20Leveraging%20this%20mask%2C%20our%20approach%20integrates%20innovative%20knowledge%0Amask-guided%20techniques%2C%20including%20knowledge-boosted%20attention%2C%20knowledge-aware%0Adiscriminators%2C%20and%20mask-enhanced%20patchNCE%20loss%2C%20aimed%20at%20refining%20generation%0Ain%20critical%20areas%20and%20addressing%20the%20pixel%20misalignment%20challenge.%20Our%20method%0Aachieves%20the%20best%20FVD%20of%201503.21%20and%20PSNR%20of%2011.81%20compared%20to%20other%20common%0Avideo%20generation%20approaches.%20Human%20assessment%20by%20an%20ophthalmologist%20confirms%0Aits%20high%20generation%20quality.%20Notably%2C%20our%20knowledge%20mask%20surpasses%20supervised%0Alesion%20segmentation%20masks%2C%20offering%20a%20promising%20non-invasive%20alternative%20to%0Atraditional%20FFA%20for%20research%20and%20clinical%20applications.%20The%20code%20is%20available%0Aat%20https%3A//github.com/Michi-3000/Fundus2Video.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2408.15217v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DFundus2Video%253A%2520Cross-Modal%2520Angiography%2520Video%2520Generation%2520from%2520Static%250A%2520%2520Fundus%2520Photography%2520with%2520Clinical%2520Knowledge%2520Guidance%26entry.906535625%3DWeiyi%2520Zhang%2520and%2520Siyu%2520Huang%2520and%2520Jiancheng%2520Yang%2520and%2520Ruoyu%2520Chen%2520and%2520Zongyuan%2520Ge%2520and%2520Yingfeng%2520Zheng%2520and%2520Danli%2520Shi%2520and%2520Mingguang%2520He%26entry.1292438233%3D%2520%2520Fundus%2520Fluorescein%2520Angiography%2520%2528FFA%2529%2520is%2520a%2520critical%2520tool%2520for%2520assessing%2520retinal%250Avascular%2520dynamics%2520and%2520aiding%2520in%2520the%2520diagnosis%2520of%2520eye%2520diseases.%2520However%252C%2520its%250Ainvasive%2520nature%2520and%2520less%2520accessibility%2520compared%2520to%2520Color%2520Fundus%2520%2528CF%2529%2520images%250Apose%2520significant%2520challenges.%2520Current%2520CF%2520to%2520FFA%2520translation%2520methods%2520are%2520limited%250Ato%2520static%2520generation.%2520In%2520this%2520work%252C%2520we%2520pioneer%2520dynamic%2520FFA%2520video%2520generation%250Afrom%2520static%2520CF%2520images.%2520We%2520introduce%2520an%2520autoregressive%2520GAN%2520for%2520smooth%252C%250Amemory-saving%2520frame-by-frame%2520FFA%2520synthesis.%2520To%2520enhance%2520the%2520focus%2520on%2520dynamic%250Alesion%2520changes%2520in%2520FFA%2520regions%252C%2520we%2520design%2520a%2520knowledge%2520mask%2520based%2520on%2520clinical%250Aexperience.%2520Leveraging%2520this%2520mask%252C%2520our%2520approach%2520integrates%2520innovative%2520knowledge%250Amask-guided%2520techniques%252C%2520including%2520knowledge-boosted%2520attention%252C%2520knowledge-aware%250Adiscriminators%252C%2520and%2520mask-enhanced%2520patchNCE%2520loss%252C%2520aimed%2520at%2520refining%2520generation%250Ain%2520critical%2520areas%2520and%2520addressing%2520the%2520pixel%2520misalignment%2520challenge.%2520Our%2520method%250Aachieves%2520the%2520best%2520FVD%2520of%25201503.21%2520and%2520PSNR%2520of%252011.81%2520compared%2520to%2520other%2520common%250Avideo%2520generation%2520approaches.%2520Human%2520assessment%2520by%2520an%2520ophthalmologist%2520confirms%250Aits%2520high%2520generation%2520quality.%2520Notably%252C%2520our%2520knowledge%2520mask%2520surpasses%2520supervised%250Alesion%2520segmentation%2520masks%252C%2520offering%2520a%2520promising%2520non-invasive%2520alternative%2520to%250Atraditional%2520FFA%2520for%2520research%2520and%2520clinical%2520applications.%2520The%2520code%2520is%2520available%250Aat%2520https%253A//github.com/Michi-3000/Fundus2Video.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2408.15217v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Fundus2Video%3A%20Cross-Modal%20Angiography%20Video%20Generation%20from%20Static%0A%20%20Fundus%20Photography%20with%20Clinical%20Knowledge%20Guidance&entry.906535625=Weiyi%20Zhang%20and%20Siyu%20Huang%20and%20Jiancheng%20Yang%20and%20Ruoyu%20Chen%20and%20Zongyuan%20Ge%20and%20Yingfeng%20Zheng%20and%20Danli%20Shi%20and%20Mingguang%20He&entry.1292438233=%20%20Fundus%20Fluorescein%20Angiography%20%28FFA%29%20is%20a%20critical%20tool%20for%20assessing%20retinal%0Avascular%20dynamics%20and%20aiding%20in%20the%20diagnosis%20of%20eye%20diseases.%20However%2C%20its%0Ainvasive%20nature%20and%20less%20accessibility%20compared%20to%20Color%20Fundus%20%28CF%29%20images%0Apose%20significant%20challenges.%20Current%20CF%20to%20FFA%20translation%20methods%20are%20limited%0Ato%20static%20generation.%20In%20this%20work%2C%20we%20pioneer%20dynamic%20FFA%20video%20generation%0Afrom%20static%20CF%20images.%20We%20introduce%20an%20autoregressive%20GAN%20for%20smooth%2C%0Amemory-saving%20frame-by-frame%20FFA%20synthesis.%20To%20enhance%20the%20focus%20on%20dynamic%0Alesion%20changes%20in%20FFA%20regions%2C%20we%20design%20a%20knowledge%20mask%20based%20on%20clinical%0Aexperience.%20Leveraging%20this%20mask%2C%20our%20approach%20integrates%20innovative%20knowledge%0Amask-guided%20techniques%2C%20including%20knowledge-boosted%20attention%2C%20knowledge-aware%0Adiscriminators%2C%20and%20mask-enhanced%20patchNCE%20loss%2C%20aimed%20at%20refining%20generation%0Ain%20critical%20areas%20and%20addressing%20the%20pixel%20misalignment%20challenge.%20Our%20method%0Aachieves%20the%20best%20FVD%20of%201503.21%20and%20PSNR%20of%2011.81%20compared%20to%20other%20common%0Avideo%20generation%20approaches.%20Human%20assessment%20by%20an%20ophthalmologist%20confirms%0Aits%20high%20generation%20quality.%20Notably%2C%20our%20knowledge%20mask%20surpasses%20supervised%0Alesion%20segmentation%20masks%2C%20offering%20a%20promising%20non-invasive%20alternative%20to%0Atraditional%20FFA%20for%20research%20and%20clinical%20applications.%20The%20code%20is%20available%0Aat%20https%3A//github.com/Michi-3000/Fundus2Video.%0A&entry.1838667208=http%3A//arxiv.org/abs/2408.15217v1&entry.124074799=Read"},
{"title": "3D Adaptive Structural Convolution Network for Domain-Invariant Point\n  Cloud Recognition", "author": "Younggun Kim and Beomsik Cho and Seonghoon Ryoo and Soomok Lee", "abstract": "  Adapting deep learning networks for point cloud data recognition in\nself-driving vehicles faces challenges due to the variability in datasets and\nsensor technologies, emphasizing the need for adaptive techniques to maintain\naccuracy across different conditions. In this paper, we introduce the 3D\nAdaptive Structural Convolution Network (3D-ASCN), a cutting-edge framework for\n3D point cloud recognition. It combines 3D convolution kernels, a structural\ntree structure, and adaptive neighborhood sampling for effective geometric\nfeature extraction. This method obtains domain-invariant features and\ndemonstrates robust, adaptable performance on a variety of point cloud\ndatasets, ensuring compatibility across diverse sensor configurations without\nthe need for parameter adjustments. This highlights its potential to\nsignificantly enhance the reliability and efficiency of self-driving vehicle\ntechnology.\n", "link": "http://arxiv.org/abs/2407.04833v3", "date": "2024-08-27", "relevancy": 2.8716, "topK": [{"title": "IAE: Implicit Autoencoder for Point Cloud Self-supervised Representation Learning", "link": "https://arxiv.org/abs/2201.00785", "similarity": 0.5961}, {"title": "DDM-NET: End-to-end learning of keypoint feature Detection, Description and Matching for 3D localization", "link": "https://arxiv.org/abs/2212.04575", "similarity": 0.5785}, {"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.5484}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%203D%20Adaptive%20Structural%20Convolution%20Network%20for%20Domain-Invariant%20Point%0A%20%20Cloud%20Recognition&body=Title%3A%203D%20Adaptive%20Structural%20Convolution%20Network%20for%20Domain-Invariant%20Point%0A%20%20Cloud%20Recognition%0AAuthor%3A%20Younggun%20Kim%20and%20Beomsik%20Cho%20and%20Seonghoon%20Ryoo%20and%20Soomok%20Lee%0AAbstract%3A%20%20%20Adapting%20deep%20learning%20networks%20for%20point%20cloud%20data%20recognition%20in%0Aself-driving%20vehicles%20faces%20challenges%20due%20to%20the%20variability%20in%20datasets%20and%0Asensor%20technologies%2C%20emphasizing%20the%20need%20for%20adaptive%20techniques%20to%20maintain%0Aaccuracy%20across%20different%20conditions.%20In%20this%20paper%2C%20we%20introduce%20the%203D%0AAdaptive%20Structural%20Convolution%20Network%20%283D-ASCN%29%2C%20a%20cutting-edge%20framework%20for%0A3D%20point%20cloud%20recognition.%20It%20combines%203D%20convolution%20kernels%2C%20a%20structural%0Atree%20structure%2C%20and%20adaptive%20neighborhood%20sampling%20for%20effective%20geometric%0Afeature%20extraction.%20This%20method%20obtains%20domain-invariant%20features%20and%0Ademonstrates%20robust%2C%20adaptable%20performance%20on%20a%20variety%20of%20point%20cloud%0Adatasets%2C%20ensuring%20compatibility%20across%20diverse%20sensor%20configurations%20without%0Athe%20need%20for%20parameter%20adjustments.%20This%20highlights%20its%20potential%20to%0Asignificantly%20enhance%20the%20reliability%20and%20efficiency%20of%20self-driving%20vehicle%0Atechnology.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2407.04833v3%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3D3D%2520Adaptive%2520Structural%2520Convolution%2520Network%2520for%2520Domain-Invariant%2520Point%250A%2520%2520Cloud%2520Recognition%26entry.906535625%3DYounggun%2520Kim%2520and%2520Beomsik%2520Cho%2520and%2520Seonghoon%2520Ryoo%2520and%2520Soomok%2520Lee%26entry.1292438233%3D%2520%2520Adapting%2520deep%2520learning%2520networks%2520for%2520point%2520cloud%2520data%2520recognition%2520in%250Aself-driving%2520vehicles%2520faces%2520challenges%2520due%2520to%2520the%2520variability%2520in%2520datasets%2520and%250Asensor%2520technologies%252C%2520emphasizing%2520the%2520need%2520for%2520adaptive%2520techniques%2520to%2520maintain%250Aaccuracy%2520across%2520different%2520conditions.%2520In%2520this%2520paper%252C%2520we%2520introduce%2520the%25203D%250AAdaptive%2520Structural%2520Convolution%2520Network%2520%25283D-ASCN%2529%252C%2520a%2520cutting-edge%2520framework%2520for%250A3D%2520point%2520cloud%2520recognition.%2520It%2520combines%25203D%2520convolution%2520kernels%252C%2520a%2520structural%250Atree%2520structure%252C%2520and%2520adaptive%2520neighborhood%2520sampling%2520for%2520effective%2520geometric%250Afeature%2520extraction.%2520This%2520method%2520obtains%2520domain-invariant%2520features%2520and%250Ademonstrates%2520robust%252C%2520adaptable%2520performance%2520on%2520a%2520variety%2520of%2520point%2520cloud%250Adatasets%252C%2520ensuring%2520compatibility%2520across%2520diverse%2520sensor%2520configurations%2520without%250Athe%2520need%2520for%2520parameter%2520adjustments.%2520This%2520highlights%2520its%2520potential%2520to%250Asignificantly%2520enhance%2520the%2520reliability%2520and%2520efficiency%2520of%2520self-driving%2520vehicle%250Atechnology.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2407.04833v3%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=3D%20Adaptive%20Structural%20Convolution%20Network%20for%20Domain-Invariant%20Point%0A%20%20Cloud%20Recognition&entry.906535625=Younggun%20Kim%20and%20Beomsik%20Cho%20and%20Seonghoon%20Ryoo%20and%20Soomok%20Lee&entry.1292438233=%20%20Adapting%20deep%20learning%20networks%20for%20point%20cloud%20data%20recognition%20in%0Aself-driving%20vehicles%20faces%20challenges%20due%20to%20the%20variability%20in%20datasets%20and%0Asensor%20technologies%2C%20emphasizing%20the%20need%20for%20adaptive%20techniques%20to%20maintain%0Aaccuracy%20across%20different%20conditions.%20In%20this%20paper%2C%20we%20introduce%20the%203D%0AAdaptive%20Structural%20Convolution%20Network%20%283D-ASCN%29%2C%20a%20cutting-edge%20framework%20for%0A3D%20point%20cloud%20recognition.%20It%20combines%203D%20convolution%20kernels%2C%20a%20structural%0Atree%20structure%2C%20and%20adaptive%20neighborhood%20sampling%20for%20effective%20geometric%0Afeature%20extraction.%20This%20method%20obtains%20domain-invariant%20features%20and%0Ademonstrates%20robust%2C%20adaptable%20performance%20on%20a%20variety%20of%20point%20cloud%0Adatasets%2C%20ensuring%20compatibility%20across%20diverse%20sensor%20configurations%20without%0Athe%20need%20for%20parameter%20adjustments.%20This%20highlights%20its%20potential%20to%0Asignificantly%20enhance%20the%20reliability%20and%20efficiency%20of%20self-driving%20vehicle%0Atechnology.%0A&entry.1838667208=http%3A//arxiv.org/abs/2407.04833v3&entry.124074799=Read"},
{"title": "Learning-based Multi-View Stereo: A Survey", "author": "Fangjinhua Wang and Qingtian Zhu and Di Chang and Quankai Gao and Junlin Han and Tong Zhang and Richard Hartley and Marc Pollefeys", "abstract": "  3D reconstruction aims to recover the dense 3D structure of a scene. It plays\nan essential role in various applications such as Augmented/Virtual Reality\n(AR/VR), autonomous driving and robotics. Leveraging multiple views of a scene\ncaptured from different viewpoints, Multi-View Stereo (MVS) algorithms\nsynthesize a comprehensive 3D representation, enabling precise reconstruction\nin complex environments. Due to its efficiency and effectiveness, MVS has\nbecome a pivotal method for image-based 3D reconstruction. Recently, with the\nsuccess of deep learning, many learning-based MVS methods have been proposed,\nachieving impressive performance against traditional methods. We categorize\nthese learning-based methods as: depth map-based, voxel-based, NeRF-based, 3D\nGaussian Splatting-based, and large feed-forward methods. Among these, we focus\nsignificantly on depth map-based methods, which are the main family of MVS due\nto their conciseness, flexibility and scalability. In this survey, we provide a\ncomprehensive review of the literature at the time of this writing. We\ninvestigate these learning-based methods, summarize their performances on\npopular benchmarks, and discuss promising future research directions in this\narea.\n", "link": "http://arxiv.org/abs/2408.15235v1", "date": "2024-08-27", "relevancy": 2.8372, "topK": [{"title": "CAT3D: Create Anything in 3D with Multi-View Diffusion Models", "link": "http://arxiv.org/abs/2405.10314v1", "similarity": 0.5706}, {"title": "CAT3D: Create Anything in 3D with Multi-View Diffusion Models", "link": "http://arxiv.org/abs/2405.10314v1", "similarity": 0.5706}, {"title": "RTG-SLAM: Real-time 3D Reconstruction at Scale using Gaussian Splatting", "link": "http://arxiv.org/abs/2404.19706v1", "similarity": 0.5612}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Learning-based%20Multi-View%20Stereo%3A%20A%20Survey&body=Title%3A%20Learning-based%20Multi-View%20Stereo%3A%20A%20Survey%0AAuthor%3A%20Fangjinhua%20Wang%20and%20Qingtian%20Zhu%20and%20Di%20Chang%20and%20Quankai%20Gao%20and%20Junlin%20Han%20and%20Tong%20Zhang%20and%20Richard%20Hartley%20and%20Marc%20Pollefeys%0AAbstract%3A%20%20%203D%20reconstruction%20aims%20to%20recover%20the%20dense%203D%20structure%20of%20a%20scene.%20It%20plays%0Aan%20essential%20role%20in%20various%20applications%20such%20as%20Augmented/Virtual%20Reality%0A%28AR/VR%29%2C%20autonomous%20driving%20and%20robotics.%20Leveraging%20multiple%20views%20of%20a%20scene%0Acaptured%20from%20different%20viewpoints%2C%20Multi-View%20Stereo%20%28MVS%29%20algorithms%0Asynthesize%20a%20comprehensive%203D%20representation%2C%20enabling%20precise%20reconstruction%0Ain%20complex%20environments.%20Due%20to%20its%20efficiency%20and%20effectiveness%2C%20MVS%20has%0Abecome%20a%20pivotal%20method%20for%20image-based%203D%20reconstruction.%20Recently%2C%20with%20the%0Asuccess%20of%20deep%20learning%2C%20many%20learning-based%20MVS%20methods%20have%20been%20proposed%2C%0Aachieving%20impressive%20performance%20against%20traditional%20methods.%20We%20categorize%0Athese%20learning-based%20methods%20as%3A%20depth%20map-based%2C%20voxel-based%2C%20NeRF-based%2C%203D%0AGaussian%20Splatting-based%2C%20and%20large%20feed-forward%20methods.%20Among%20these%2C%20we%20focus%0Asignificantly%20on%20depth%20map-based%20methods%2C%20which%20are%20the%20main%20family%20of%20MVS%20due%0Ato%20their%20conciseness%2C%20flexibility%20and%20scalability.%20In%20this%20survey%2C%20we%20provide%20a%0Acomprehensive%20review%20of%20the%20literature%20at%20the%20time%20of%20this%20writing.%20We%0Ainvestigate%20these%20learning-based%20methods%2C%20summarize%20their%20performances%20on%0Apopular%20benchmarks%2C%20and%20discuss%20promising%20future%20research%20directions%20in%20this%0Aarea.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2408.15235v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DLearning-based%2520Multi-View%2520Stereo%253A%2520A%2520Survey%26entry.906535625%3DFangjinhua%2520Wang%2520and%2520Qingtian%2520Zhu%2520and%2520Di%2520Chang%2520and%2520Quankai%2520Gao%2520and%2520Junlin%2520Han%2520and%2520Tong%2520Zhang%2520and%2520Richard%2520Hartley%2520and%2520Marc%2520Pollefeys%26entry.1292438233%3D%2520%25203D%2520reconstruction%2520aims%2520to%2520recover%2520the%2520dense%25203D%2520structure%2520of%2520a%2520scene.%2520It%2520plays%250Aan%2520essential%2520role%2520in%2520various%2520applications%2520such%2520as%2520Augmented/Virtual%2520Reality%250A%2528AR/VR%2529%252C%2520autonomous%2520driving%2520and%2520robotics.%2520Leveraging%2520multiple%2520views%2520of%2520a%2520scene%250Acaptured%2520from%2520different%2520viewpoints%252C%2520Multi-View%2520Stereo%2520%2528MVS%2529%2520algorithms%250Asynthesize%2520a%2520comprehensive%25203D%2520representation%252C%2520enabling%2520precise%2520reconstruction%250Ain%2520complex%2520environments.%2520Due%2520to%2520its%2520efficiency%2520and%2520effectiveness%252C%2520MVS%2520has%250Abecome%2520a%2520pivotal%2520method%2520for%2520image-based%25203D%2520reconstruction.%2520Recently%252C%2520with%2520the%250Asuccess%2520of%2520deep%2520learning%252C%2520many%2520learning-based%2520MVS%2520methods%2520have%2520been%2520proposed%252C%250Aachieving%2520impressive%2520performance%2520against%2520traditional%2520methods.%2520We%2520categorize%250Athese%2520learning-based%2520methods%2520as%253A%2520depth%2520map-based%252C%2520voxel-based%252C%2520NeRF-based%252C%25203D%250AGaussian%2520Splatting-based%252C%2520and%2520large%2520feed-forward%2520methods.%2520Among%2520these%252C%2520we%2520focus%250Asignificantly%2520on%2520depth%2520map-based%2520methods%252C%2520which%2520are%2520the%2520main%2520family%2520of%2520MVS%2520due%250Ato%2520their%2520conciseness%252C%2520flexibility%2520and%2520scalability.%2520In%2520this%2520survey%252C%2520we%2520provide%2520a%250Acomprehensive%2520review%2520of%2520the%2520literature%2520at%2520the%2520time%2520of%2520this%2520writing.%2520We%250Ainvestigate%2520these%2520learning-based%2520methods%252C%2520summarize%2520their%2520performances%2520on%250Apopular%2520benchmarks%252C%2520and%2520discuss%2520promising%2520future%2520research%2520directions%2520in%2520this%250Aarea.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2408.15235v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Learning-based%20Multi-View%20Stereo%3A%20A%20Survey&entry.906535625=Fangjinhua%20Wang%20and%20Qingtian%20Zhu%20and%20Di%20Chang%20and%20Quankai%20Gao%20and%20Junlin%20Han%20and%20Tong%20Zhang%20and%20Richard%20Hartley%20and%20Marc%20Pollefeys&entry.1292438233=%20%203D%20reconstruction%20aims%20to%20recover%20the%20dense%203D%20structure%20of%20a%20scene.%20It%20plays%0Aan%20essential%20role%20in%20various%20applications%20such%20as%20Augmented/Virtual%20Reality%0A%28AR/VR%29%2C%20autonomous%20driving%20and%20robotics.%20Leveraging%20multiple%20views%20of%20a%20scene%0Acaptured%20from%20different%20viewpoints%2C%20Multi-View%20Stereo%20%28MVS%29%20algorithms%0Asynthesize%20a%20comprehensive%203D%20representation%2C%20enabling%20precise%20reconstruction%0Ain%20complex%20environments.%20Due%20to%20its%20efficiency%20and%20effectiveness%2C%20MVS%20has%0Abecome%20a%20pivotal%20method%20for%20image-based%203D%20reconstruction.%20Recently%2C%20with%20the%0Asuccess%20of%20deep%20learning%2C%20many%20learning-based%20MVS%20methods%20have%20been%20proposed%2C%0Aachieving%20impressive%20performance%20against%20traditional%20methods.%20We%20categorize%0Athese%20learning-based%20methods%20as%3A%20depth%20map-based%2C%20voxel-based%2C%20NeRF-based%2C%203D%0AGaussian%20Splatting-based%2C%20and%20large%20feed-forward%20methods.%20Among%20these%2C%20we%20focus%0Asignificantly%20on%20depth%20map-based%20methods%2C%20which%20are%20the%20main%20family%20of%20MVS%20due%0Ato%20their%20conciseness%2C%20flexibility%20and%20scalability.%20In%20this%20survey%2C%20we%20provide%20a%0Acomprehensive%20review%20of%20the%20literature%20at%20the%20time%20of%20this%20writing.%20We%0Ainvestigate%20these%20learning-based%20methods%2C%20summarize%20their%20performances%20on%0Apopular%20benchmarks%2C%20and%20discuss%20promising%20future%20research%20directions%20in%20this%0Aarea.%0A&entry.1838667208=http%3A//arxiv.org/abs/2408.15235v1&entry.124074799=Read"},
{"title": "Listen, Disentangle, and Control: Controllable Speech-Driven Talking\n  Head Generation", "author": "Changpeng Cai and Guinan Guo and Jiao Li and Junhao Su and Chenghao He and Jing Xiao and Yuanxu Chen and Lei Dai and Feiyu Zhu", "abstract": "  Most earlier investigations on talking face generation have focused on the\nsynchronization of lip motion and speech content. However, human head pose and\nfacial emotions are equally important characteristics of natural human faces.\nWhile audio-driven talking face generation has seen notable advancements,\nexisting methods either overlook facial emotions or are limited to specific\nindividuals and cannot be applied to arbitrary subjects. In this paper, we\npropose a one-shot Talking Head Generation framework (SPEAK) that distinguishes\nitself from general Talking Face Generation by enabling emotional and postural\ncontrol. Specifically, we introduce the Inter-Reconstructed Feature\nDisentanglement (IRFD) method to decouple human facial features into three\nlatent spaces. We then design a face editing module that modifies speech\ncontent and facial latent codes into a single latent space. Subsequently, we\npresent a novel generator that employs modified latent codes derived from the\nediting module to regulate emotional expression, head poses, and speech content\nin synthesizing facial animations. Extensive trials demonstrate that our method\ncan generate realistic talking head with coordinated lip motions, authentic\nfacial emotions, and smooth head movements. The demo video is available at the\nanonymous link: https://anonymous.4open.science/r/SPEAK-F56E\n", "link": "http://arxiv.org/abs/2405.07257v2", "date": "2024-08-27", "relevancy": 2.8302, "topK": [{"title": "3D Gaussian Blendshapes for Head Avatar Animation", "link": "http://arxiv.org/abs/2404.19398v2", "similarity": 0.5695}, {"title": "3D Gaussian Blendshapes for Head Avatar Animation", "link": "http://arxiv.org/abs/2404.19398v2", "similarity": 0.5695}, {"title": "Magic-Me: Identity-Specific Video Customized Diffusion", "link": "http://arxiv.org/abs/2402.09368v1", "similarity": 0.5591}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Listen%2C%20Disentangle%2C%20and%20Control%3A%20Controllable%20Speech-Driven%20Talking%0A%20%20Head%20Generation&body=Title%3A%20Listen%2C%20Disentangle%2C%20and%20Control%3A%20Controllable%20Speech-Driven%20Talking%0A%20%20Head%20Generation%0AAuthor%3A%20Changpeng%20Cai%20and%20Guinan%20Guo%20and%20Jiao%20Li%20and%20Junhao%20Su%20and%20Chenghao%20He%20and%20Jing%20Xiao%20and%20Yuanxu%20Chen%20and%20Lei%20Dai%20and%20Feiyu%20Zhu%0AAbstract%3A%20%20%20Most%20earlier%20investigations%20on%20talking%20face%20generation%20have%20focused%20on%20the%0Asynchronization%20of%20lip%20motion%20and%20speech%20content.%20However%2C%20human%20head%20pose%20and%0Afacial%20emotions%20are%20equally%20important%20characteristics%20of%20natural%20human%20faces.%0AWhile%20audio-driven%20talking%20face%20generation%20has%20seen%20notable%20advancements%2C%0Aexisting%20methods%20either%20overlook%20facial%20emotions%20or%20are%20limited%20to%20specific%0Aindividuals%20and%20cannot%20be%20applied%20to%20arbitrary%20subjects.%20In%20this%20paper%2C%20we%0Apropose%20a%20one-shot%20Talking%20Head%20Generation%20framework%20%28SPEAK%29%20that%20distinguishes%0Aitself%20from%20general%20Talking%20Face%20Generation%20by%20enabling%20emotional%20and%20postural%0Acontrol.%20Specifically%2C%20we%20introduce%20the%20Inter-Reconstructed%20Feature%0ADisentanglement%20%28IRFD%29%20method%20to%20decouple%20human%20facial%20features%20into%20three%0Alatent%20spaces.%20We%20then%20design%20a%20face%20editing%20module%20that%20modifies%20speech%0Acontent%20and%20facial%20latent%20codes%20into%20a%20single%20latent%20space.%20Subsequently%2C%20we%0Apresent%20a%20novel%20generator%20that%20employs%20modified%20latent%20codes%20derived%20from%20the%0Aediting%20module%20to%20regulate%20emotional%20expression%2C%20head%20poses%2C%20and%20speech%20content%0Ain%20synthesizing%20facial%20animations.%20Extensive%20trials%20demonstrate%20that%20our%20method%0Acan%20generate%20realistic%20talking%20head%20with%20coordinated%20lip%20motions%2C%20authentic%0Afacial%20emotions%2C%20and%20smooth%20head%20movements.%20The%20demo%20video%20is%20available%20at%20the%0Aanonymous%20link%3A%20https%3A//anonymous.4open.science/r/SPEAK-F56E%0A%0ALink%3A%20http%3A//arxiv.org/abs/2405.07257v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DListen%252C%2520Disentangle%252C%2520and%2520Control%253A%2520Controllable%2520Speech-Driven%2520Talking%250A%2520%2520Head%2520Generation%26entry.906535625%3DChangpeng%2520Cai%2520and%2520Guinan%2520Guo%2520and%2520Jiao%2520Li%2520and%2520Junhao%2520Su%2520and%2520Chenghao%2520He%2520and%2520Jing%2520Xiao%2520and%2520Yuanxu%2520Chen%2520and%2520Lei%2520Dai%2520and%2520Feiyu%2520Zhu%26entry.1292438233%3D%2520%2520Most%2520earlier%2520investigations%2520on%2520talking%2520face%2520generation%2520have%2520focused%2520on%2520the%250Asynchronization%2520of%2520lip%2520motion%2520and%2520speech%2520content.%2520However%252C%2520human%2520head%2520pose%2520and%250Afacial%2520emotions%2520are%2520equally%2520important%2520characteristics%2520of%2520natural%2520human%2520faces.%250AWhile%2520audio-driven%2520talking%2520face%2520generation%2520has%2520seen%2520notable%2520advancements%252C%250Aexisting%2520methods%2520either%2520overlook%2520facial%2520emotions%2520or%2520are%2520limited%2520to%2520specific%250Aindividuals%2520and%2520cannot%2520be%2520applied%2520to%2520arbitrary%2520subjects.%2520In%2520this%2520paper%252C%2520we%250Apropose%2520a%2520one-shot%2520Talking%2520Head%2520Generation%2520framework%2520%2528SPEAK%2529%2520that%2520distinguishes%250Aitself%2520from%2520general%2520Talking%2520Face%2520Generation%2520by%2520enabling%2520emotional%2520and%2520postural%250Acontrol.%2520Specifically%252C%2520we%2520introduce%2520the%2520Inter-Reconstructed%2520Feature%250ADisentanglement%2520%2528IRFD%2529%2520method%2520to%2520decouple%2520human%2520facial%2520features%2520into%2520three%250Alatent%2520spaces.%2520We%2520then%2520design%2520a%2520face%2520editing%2520module%2520that%2520modifies%2520speech%250Acontent%2520and%2520facial%2520latent%2520codes%2520into%2520a%2520single%2520latent%2520space.%2520Subsequently%252C%2520we%250Apresent%2520a%2520novel%2520generator%2520that%2520employs%2520modified%2520latent%2520codes%2520derived%2520from%2520the%250Aediting%2520module%2520to%2520regulate%2520emotional%2520expression%252C%2520head%2520poses%252C%2520and%2520speech%2520content%250Ain%2520synthesizing%2520facial%2520animations.%2520Extensive%2520trials%2520demonstrate%2520that%2520our%2520method%250Acan%2520generate%2520realistic%2520talking%2520head%2520with%2520coordinated%2520lip%2520motions%252C%2520authentic%250Afacial%2520emotions%252C%2520and%2520smooth%2520head%2520movements.%2520The%2520demo%2520video%2520is%2520available%2520at%2520the%250Aanonymous%2520link%253A%2520https%253A//anonymous.4open.science/r/SPEAK-F56E%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2405.07257v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Listen%2C%20Disentangle%2C%20and%20Control%3A%20Controllable%20Speech-Driven%20Talking%0A%20%20Head%20Generation&entry.906535625=Changpeng%20Cai%20and%20Guinan%20Guo%20and%20Jiao%20Li%20and%20Junhao%20Su%20and%20Chenghao%20He%20and%20Jing%20Xiao%20and%20Yuanxu%20Chen%20and%20Lei%20Dai%20and%20Feiyu%20Zhu&entry.1292438233=%20%20Most%20earlier%20investigations%20on%20talking%20face%20generation%20have%20focused%20on%20the%0Asynchronization%20of%20lip%20motion%20and%20speech%20content.%20However%2C%20human%20head%20pose%20and%0Afacial%20emotions%20are%20equally%20important%20characteristics%20of%20natural%20human%20faces.%0AWhile%20audio-driven%20talking%20face%20generation%20has%20seen%20notable%20advancements%2C%0Aexisting%20methods%20either%20overlook%20facial%20emotions%20or%20are%20limited%20to%20specific%0Aindividuals%20and%20cannot%20be%20applied%20to%20arbitrary%20subjects.%20In%20this%20paper%2C%20we%0Apropose%20a%20one-shot%20Talking%20Head%20Generation%20framework%20%28SPEAK%29%20that%20distinguishes%0Aitself%20from%20general%20Talking%20Face%20Generation%20by%20enabling%20emotional%20and%20postural%0Acontrol.%20Specifically%2C%20we%20introduce%20the%20Inter-Reconstructed%20Feature%0ADisentanglement%20%28IRFD%29%20method%20to%20decouple%20human%20facial%20features%20into%20three%0Alatent%20spaces.%20We%20then%20design%20a%20face%20editing%20module%20that%20modifies%20speech%0Acontent%20and%20facial%20latent%20codes%20into%20a%20single%20latent%20space.%20Subsequently%2C%20we%0Apresent%20a%20novel%20generator%20that%20employs%20modified%20latent%20codes%20derived%20from%20the%0Aediting%20module%20to%20regulate%20emotional%20expression%2C%20head%20poses%2C%20and%20speech%20content%0Ain%20synthesizing%20facial%20animations.%20Extensive%20trials%20demonstrate%20that%20our%20method%0Acan%20generate%20realistic%20talking%20head%20with%20coordinated%20lip%20motions%2C%20authentic%0Afacial%20emotions%2C%20and%20smooth%20head%20movements.%20The%20demo%20video%20is%20available%20at%20the%0Aanonymous%20link%3A%20https%3A//anonymous.4open.science/r/SPEAK-F56E%0A&entry.1838667208=http%3A//arxiv.org/abs/2405.07257v2&entry.124074799=Read"},
{"title": "Enhancing License Plate Super-Resolution: A Layout-Aware and\n  Character-Driven Approach", "author": "Valfride Nascimento and Rayson Laroca and Rafael O. Ribeiro and William Robson Schwartz and David Menotti", "abstract": "  Despite significant advancements in License Plate Recognition (LPR) through\ndeep learning, most improvements rely on high-resolution images with clear\ncharacters. This scenario does not reflect real-world conditions where traffic\nsurveillance often captures low-resolution and blurry images. Under these\nconditions, characters tend to blend with the background or neighboring\ncharacters, making accurate LPR challenging. To address this issue, we\nintroduce a novel loss function, Layout and Character Oriented Focal Loss\n(LCOFL), which considers factors such as resolution, texture, and structural\ndetails, as well as the performance of the LPR task itself. We enhance\ncharacter feature learning using deformable convolutions and shared weights in\nan attention module and employ a GAN-based training approach with an Optical\nCharacter Recognition (OCR) model as the discriminator to guide the\nsuper-resolution process. Our experimental results show significant\nimprovements in character reconstruction quality, outperforming two\nstate-of-the-art methods in both quantitative and qualitative measures. Our\ncode is publicly available at https://github.com/valfride/lpsr-lacd\n", "link": "http://arxiv.org/abs/2408.15103v1", "date": "2024-08-27", "relevancy": 2.8076, "topK": [{"title": "Self-supervised Photographic Image Layout Representation Learning", "link": "http://arxiv.org/abs/2403.03740v1", "similarity": 0.6288}, {"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.5331}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5227}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Enhancing%20License%20Plate%20Super-Resolution%3A%20A%20Layout-Aware%20and%0A%20%20Character-Driven%20Approach&body=Title%3A%20Enhancing%20License%20Plate%20Super-Resolution%3A%20A%20Layout-Aware%20and%0A%20%20Character-Driven%20Approach%0AAuthor%3A%20Valfride%20Nascimento%20and%20Rayson%20Laroca%20and%20Rafael%20O.%20Ribeiro%20and%20William%20Robson%20Schwartz%20and%20David%20Menotti%0AAbstract%3A%20%20%20Despite%20significant%20advancements%20in%20License%20Plate%20Recognition%20%28LPR%29%20through%0Adeep%20learning%2C%20most%20improvements%20rely%20on%20high-resolution%20images%20with%20clear%0Acharacters.%20This%20scenario%20does%20not%20reflect%20real-world%20conditions%20where%20traffic%0Asurveillance%20often%20captures%20low-resolution%20and%20blurry%20images.%20Under%20these%0Aconditions%2C%20characters%20tend%20to%20blend%20with%20the%20background%20or%20neighboring%0Acharacters%2C%20making%20accurate%20LPR%20challenging.%20To%20address%20this%20issue%2C%20we%0Aintroduce%20a%20novel%20loss%20function%2C%20Layout%20and%20Character%20Oriented%20Focal%20Loss%0A%28LCOFL%29%2C%20which%20considers%20factors%20such%20as%20resolution%2C%20texture%2C%20and%20structural%0Adetails%2C%20as%20well%20as%20the%20performance%20of%20the%20LPR%20task%20itself.%20We%20enhance%0Acharacter%20feature%20learning%20using%20deformable%20convolutions%20and%20shared%20weights%20in%0Aan%20attention%20module%20and%20employ%20a%20GAN-based%20training%20approach%20with%20an%20Optical%0ACharacter%20Recognition%20%28OCR%29%20model%20as%20the%20discriminator%20to%20guide%20the%0Asuper-resolution%20process.%20Our%20experimental%20results%20show%20significant%0Aimprovements%20in%20character%20reconstruction%20quality%2C%20outperforming%20two%0Astate-of-the-art%20methods%20in%20both%20quantitative%20and%20qualitative%20measures.%20Our%0Acode%20is%20publicly%20available%20at%20https%3A//github.com/valfride/lpsr-lacd%0A%0ALink%3A%20http%3A//arxiv.org/abs/2408.15103v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DEnhancing%2520License%2520Plate%2520Super-Resolution%253A%2520A%2520Layout-Aware%2520and%250A%2520%2520Character-Driven%2520Approach%26entry.906535625%3DValfride%2520Nascimento%2520and%2520Rayson%2520Laroca%2520and%2520Rafael%2520O.%2520Ribeiro%2520and%2520William%2520Robson%2520Schwartz%2520and%2520David%2520Menotti%26entry.1292438233%3D%2520%2520Despite%2520significant%2520advancements%2520in%2520License%2520Plate%2520Recognition%2520%2528LPR%2529%2520through%250Adeep%2520learning%252C%2520most%2520improvements%2520rely%2520on%2520high-resolution%2520images%2520with%2520clear%250Acharacters.%2520This%2520scenario%2520does%2520not%2520reflect%2520real-world%2520conditions%2520where%2520traffic%250Asurveillance%2520often%2520captures%2520low-resolution%2520and%2520blurry%2520images.%2520Under%2520these%250Aconditions%252C%2520characters%2520tend%2520to%2520blend%2520with%2520the%2520background%2520or%2520neighboring%250Acharacters%252C%2520making%2520accurate%2520LPR%2520challenging.%2520To%2520address%2520this%2520issue%252C%2520we%250Aintroduce%2520a%2520novel%2520loss%2520function%252C%2520Layout%2520and%2520Character%2520Oriented%2520Focal%2520Loss%250A%2528LCOFL%2529%252C%2520which%2520considers%2520factors%2520such%2520as%2520resolution%252C%2520texture%252C%2520and%2520structural%250Adetails%252C%2520as%2520well%2520as%2520the%2520performance%2520of%2520the%2520LPR%2520task%2520itself.%2520We%2520enhance%250Acharacter%2520feature%2520learning%2520using%2520deformable%2520convolutions%2520and%2520shared%2520weights%2520in%250Aan%2520attention%2520module%2520and%2520employ%2520a%2520GAN-based%2520training%2520approach%2520with%2520an%2520Optical%250ACharacter%2520Recognition%2520%2528OCR%2529%2520model%2520as%2520the%2520discriminator%2520to%2520guide%2520the%250Asuper-resolution%2520process.%2520Our%2520experimental%2520results%2520show%2520significant%250Aimprovements%2520in%2520character%2520reconstruction%2520quality%252C%2520outperforming%2520two%250Astate-of-the-art%2520methods%2520in%2520both%2520quantitative%2520and%2520qualitative%2520measures.%2520Our%250Acode%2520is%2520publicly%2520available%2520at%2520https%253A//github.com/valfride/lpsr-lacd%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2408.15103v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Enhancing%20License%20Plate%20Super-Resolution%3A%20A%20Layout-Aware%20and%0A%20%20Character-Driven%20Approach&entry.906535625=Valfride%20Nascimento%20and%20Rayson%20Laroca%20and%20Rafael%20O.%20Ribeiro%20and%20William%20Robson%20Schwartz%20and%20David%20Menotti&entry.1292438233=%20%20Despite%20significant%20advancements%20in%20License%20Plate%20Recognition%20%28LPR%29%20through%0Adeep%20learning%2C%20most%20improvements%20rely%20on%20high-resolution%20images%20with%20clear%0Acharacters.%20This%20scenario%20does%20not%20reflect%20real-world%20conditions%20where%20traffic%0Asurveillance%20often%20captures%20low-resolution%20and%20blurry%20images.%20Under%20these%0Aconditions%2C%20characters%20tend%20to%20blend%20with%20the%20background%20or%20neighboring%0Acharacters%2C%20making%20accurate%20LPR%20challenging.%20To%20address%20this%20issue%2C%20we%0Aintroduce%20a%20novel%20loss%20function%2C%20Layout%20and%20Character%20Oriented%20Focal%20Loss%0A%28LCOFL%29%2C%20which%20considers%20factors%20such%20as%20resolution%2C%20texture%2C%20and%20structural%0Adetails%2C%20as%20well%20as%20the%20performance%20of%20the%20LPR%20task%20itself.%20We%20enhance%0Acharacter%20feature%20learning%20using%20deformable%20convolutions%20and%20shared%20weights%20in%0Aan%20attention%20module%20and%20employ%20a%20GAN-based%20training%20approach%20with%20an%20Optical%0ACharacter%20Recognition%20%28OCR%29%20model%20as%20the%20discriminator%20to%20guide%20the%0Asuper-resolution%20process.%20Our%20experimental%20results%20show%20significant%0Aimprovements%20in%20character%20reconstruction%20quality%2C%20outperforming%20two%0Astate-of-the-art%20methods%20in%20both%20quantitative%20and%20qualitative%20measures.%20Our%0Acode%20is%20publicly%20available%20at%20https%3A//github.com/valfride/lpsr-lacd%0A&entry.1838667208=http%3A//arxiv.org/abs/2408.15103v1&entry.124074799=Read"},
{"title": "Depth Restoration of Hand-Held Transparent Objects for Human-to-Robot\n  Handover", "author": "Ran Yu and Haixin Yu and Huang Yan and Ziwu Song and Shoujie Li and Wenbo Ding", "abstract": "  Transparent objects are common in daily life, while their unique optical\nproperties pose challenges for RGB-D cameras, which struggle to capture\naccurate depth information. For assistant robots, accurately perceiving\ntransparent objects held by humans is essential for effective human-robot\ninteraction. This paper presents a Hand-Aware Depth Restoration (HADR) method\nfor hand-held transparent objects based on creating an implicit neural\nrepresentation function from a single RGB-D image. The proposed method\nintroduces the hand posture as an important guidance to leverage semantic and\ngeometric information. To train and evaluate the proposed method, we create a\nhigh-fidelity synthetic dataset called TransHand-14K with a real-to-sim data\ngeneration scheme. Experiments show that our method has a better performance\nand generalization ability compared with existing methods. We further develop a\nreal-world human-to-robot handover system based on the proposed depth\nrestoration method, demonstrating its application value in human-robot\ninteraction.\n", "link": "http://arxiv.org/abs/2408.14997v1", "date": "2024-08-27", "relevancy": 2.79, "topK": [{"title": "Egocentric Computer Vision for Hands-Free Robotic Wheelchair Navigation", "link": "https://link.springer.com/article/10.1007/s10846-023-01807-4", "similarity": 0.5747}, {"title": "VirtualModel: Generating Object-ID-retentive Human-object Interaction\n  Image by Diffusion Model for E-commerce Marketing", "link": "http://arxiv.org/abs/2405.09985v1", "similarity": 0.5505}, {"title": "UGG: Unified Generative Grasping", "link": "https://arxiv.org/abs/2311.16917", "similarity": 0.5488}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Depth%20Restoration%20of%20Hand-Held%20Transparent%20Objects%20for%20Human-to-Robot%0A%20%20Handover&body=Title%3A%20Depth%20Restoration%20of%20Hand-Held%20Transparent%20Objects%20for%20Human-to-Robot%0A%20%20Handover%0AAuthor%3A%20Ran%20Yu%20and%20Haixin%20Yu%20and%20Huang%20Yan%20and%20Ziwu%20Song%20and%20Shoujie%20Li%20and%20Wenbo%20Ding%0AAbstract%3A%20%20%20Transparent%20objects%20are%20common%20in%20daily%20life%2C%20while%20their%20unique%20optical%0Aproperties%20pose%20challenges%20for%20RGB-D%20cameras%2C%20which%20struggle%20to%20capture%0Aaccurate%20depth%20information.%20For%20assistant%20robots%2C%20accurately%20perceiving%0Atransparent%20objects%20held%20by%20humans%20is%20essential%20for%20effective%20human-robot%0Ainteraction.%20This%20paper%20presents%20a%20Hand-Aware%20Depth%20Restoration%20%28HADR%29%20method%0Afor%20hand-held%20transparent%20objects%20based%20on%20creating%20an%20implicit%20neural%0Arepresentation%20function%20from%20a%20single%20RGB-D%20image.%20The%20proposed%20method%0Aintroduces%20the%20hand%20posture%20as%20an%20important%20guidance%20to%20leverage%20semantic%20and%0Ageometric%20information.%20To%20train%20and%20evaluate%20the%20proposed%20method%2C%20we%20create%20a%0Ahigh-fidelity%20synthetic%20dataset%20called%20TransHand-14K%20with%20a%20real-to-sim%20data%0Ageneration%20scheme.%20Experiments%20show%20that%20our%20method%20has%20a%20better%20performance%0Aand%20generalization%20ability%20compared%20with%20existing%20methods.%20We%20further%20develop%20a%0Areal-world%20human-to-robot%20handover%20system%20based%20on%20the%20proposed%20depth%0Arestoration%20method%2C%20demonstrating%20its%20application%20value%20in%20human-robot%0Ainteraction.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2408.14997v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DDepth%2520Restoration%2520of%2520Hand-Held%2520Transparent%2520Objects%2520for%2520Human-to-Robot%250A%2520%2520Handover%26entry.906535625%3DRan%2520Yu%2520and%2520Haixin%2520Yu%2520and%2520Huang%2520Yan%2520and%2520Ziwu%2520Song%2520and%2520Shoujie%2520Li%2520and%2520Wenbo%2520Ding%26entry.1292438233%3D%2520%2520Transparent%2520objects%2520are%2520common%2520in%2520daily%2520life%252C%2520while%2520their%2520unique%2520optical%250Aproperties%2520pose%2520challenges%2520for%2520RGB-D%2520cameras%252C%2520which%2520struggle%2520to%2520capture%250Aaccurate%2520depth%2520information.%2520For%2520assistant%2520robots%252C%2520accurately%2520perceiving%250Atransparent%2520objects%2520held%2520by%2520humans%2520is%2520essential%2520for%2520effective%2520human-robot%250Ainteraction.%2520This%2520paper%2520presents%2520a%2520Hand-Aware%2520Depth%2520Restoration%2520%2528HADR%2529%2520method%250Afor%2520hand-held%2520transparent%2520objects%2520based%2520on%2520creating%2520an%2520implicit%2520neural%250Arepresentation%2520function%2520from%2520a%2520single%2520RGB-D%2520image.%2520The%2520proposed%2520method%250Aintroduces%2520the%2520hand%2520posture%2520as%2520an%2520important%2520guidance%2520to%2520leverage%2520semantic%2520and%250Ageometric%2520information.%2520To%2520train%2520and%2520evaluate%2520the%2520proposed%2520method%252C%2520we%2520create%2520a%250Ahigh-fidelity%2520synthetic%2520dataset%2520called%2520TransHand-14K%2520with%2520a%2520real-to-sim%2520data%250Ageneration%2520scheme.%2520Experiments%2520show%2520that%2520our%2520method%2520has%2520a%2520better%2520performance%250Aand%2520generalization%2520ability%2520compared%2520with%2520existing%2520methods.%2520We%2520further%2520develop%2520a%250Areal-world%2520human-to-robot%2520handover%2520system%2520based%2520on%2520the%2520proposed%2520depth%250Arestoration%2520method%252C%2520demonstrating%2520its%2520application%2520value%2520in%2520human-robot%250Ainteraction.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2408.14997v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Depth%20Restoration%20of%20Hand-Held%20Transparent%20Objects%20for%20Human-to-Robot%0A%20%20Handover&entry.906535625=Ran%20Yu%20and%20Haixin%20Yu%20and%20Huang%20Yan%20and%20Ziwu%20Song%20and%20Shoujie%20Li%20and%20Wenbo%20Ding&entry.1292438233=%20%20Transparent%20objects%20are%20common%20in%20daily%20life%2C%20while%20their%20unique%20optical%0Aproperties%20pose%20challenges%20for%20RGB-D%20cameras%2C%20which%20struggle%20to%20capture%0Aaccurate%20depth%20information.%20For%20assistant%20robots%2C%20accurately%20perceiving%0Atransparent%20objects%20held%20by%20humans%20is%20essential%20for%20effective%20human-robot%0Ainteraction.%20This%20paper%20presents%20a%20Hand-Aware%20Depth%20Restoration%20%28HADR%29%20method%0Afor%20hand-held%20transparent%20objects%20based%20on%20creating%20an%20implicit%20neural%0Arepresentation%20function%20from%20a%20single%20RGB-D%20image.%20The%20proposed%20method%0Aintroduces%20the%20hand%20posture%20as%20an%20important%20guidance%20to%20leverage%20semantic%20and%0Ageometric%20information.%20To%20train%20and%20evaluate%20the%20proposed%20method%2C%20we%20create%20a%0Ahigh-fidelity%20synthetic%20dataset%20called%20TransHand-14K%20with%20a%20real-to-sim%20data%0Ageneration%20scheme.%20Experiments%20show%20that%20our%20method%20has%20a%20better%20performance%0Aand%20generalization%20ability%20compared%20with%20existing%20methods.%20We%20further%20develop%20a%0Areal-world%20human-to-robot%20handover%20system%20based%20on%20the%20proposed%20depth%0Arestoration%20method%2C%20demonstrating%20its%20application%20value%20in%20human-robot%0Ainteraction.%0A&entry.1838667208=http%3A//arxiv.org/abs/2408.14997v1&entry.124074799=Read"},
{"title": "Prior-free Balanced Replay: Uncertainty-guided Reservoir Sampling for\n  Long-Tailed Continual Learning", "author": "Lei Liu and Li Liu and Yawen Cui", "abstract": "  Even in the era of large models, one of the well-known issues in continual\nlearning (CL) is catastrophic forgetting, which is significantly challenging\nwhen the continual data stream exhibits a long-tailed distribution, termed as\nLong-Tailed Continual Learning (LTCL). Existing LTCL solutions generally\nrequire the label distribution of the data stream to achieve re-balance\ntraining. However, obtaining such prior information is often infeasible in real\nscenarios since the model should learn without pre-identifying the majority and\nminority classes. To this end, we propose a novel Prior-free Balanced Replay\n(PBR) framework to learn from long-tailed data stream with less forgetting.\nConcretely, motivated by our experimental finding that the minority classes are\nmore likely to be forgotten due to the higher uncertainty, we newly design an\nuncertainty-guided reservoir sampling strategy to prioritize rehearsing\nminority data without using any prior information, which is based on the mutual\ndependence between the model and samples. Additionally, we incorporate two\nprior-free components to further reduce the forgetting issue: (1) Boundary\nconstraint is to preserve uncertain boundary supporting samples for continually\nre-estimating task boundaries. (2) Prototype constraint is to maintain the\nconsistency of learned class prototypes along with training. Our approach is\nevaluated on three standard long-tailed benchmarks, demonstrating superior\nperformance to existing CL methods and previous SOTA LTCL approach in both\ntask- and class-incremental learning settings, as well as ordered- and\nshuffled-LTCL settings.\n", "link": "http://arxiv.org/abs/2408.14976v1", "date": "2024-08-27", "relevancy": 2.7789, "topK": [{"title": "Breadcrumbs: Adversarial class-balanced sampling for long-tailed recognition", "link": "https://link.springer.com/chapter/10.1007/978-3-031-20053-3_37", "similarity": 0.616}, {"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.5326}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5187}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Prior-free%20Balanced%20Replay%3A%20Uncertainty-guided%20Reservoir%20Sampling%20for%0A%20%20Long-Tailed%20Continual%20Learning&body=Title%3A%20Prior-free%20Balanced%20Replay%3A%20Uncertainty-guided%20Reservoir%20Sampling%20for%0A%20%20Long-Tailed%20Continual%20Learning%0AAuthor%3A%20Lei%20Liu%20and%20Li%20Liu%20and%20Yawen%20Cui%0AAbstract%3A%20%20%20Even%20in%20the%20era%20of%20large%20models%2C%20one%20of%20the%20well-known%20issues%20in%20continual%0Alearning%20%28CL%29%20is%20catastrophic%20forgetting%2C%20which%20is%20significantly%20challenging%0Awhen%20the%20continual%20data%20stream%20exhibits%20a%20long-tailed%20distribution%2C%20termed%20as%0ALong-Tailed%20Continual%20Learning%20%28LTCL%29.%20Existing%20LTCL%20solutions%20generally%0Arequire%20the%20label%20distribution%20of%20the%20data%20stream%20to%20achieve%20re-balance%0Atraining.%20However%2C%20obtaining%20such%20prior%20information%20is%20often%20infeasible%20in%20real%0Ascenarios%20since%20the%20model%20should%20learn%20without%20pre-identifying%20the%20majority%20and%0Aminority%20classes.%20To%20this%20end%2C%20we%20propose%20a%20novel%20Prior-free%20Balanced%20Replay%0A%28PBR%29%20framework%20to%20learn%20from%20long-tailed%20data%20stream%20with%20less%20forgetting.%0AConcretely%2C%20motivated%20by%20our%20experimental%20finding%20that%20the%20minority%20classes%20are%0Amore%20likely%20to%20be%20forgotten%20due%20to%20the%20higher%20uncertainty%2C%20we%20newly%20design%20an%0Auncertainty-guided%20reservoir%20sampling%20strategy%20to%20prioritize%20rehearsing%0Aminority%20data%20without%20using%20any%20prior%20information%2C%20which%20is%20based%20on%20the%20mutual%0Adependence%20between%20the%20model%20and%20samples.%20Additionally%2C%20we%20incorporate%20two%0Aprior-free%20components%20to%20further%20reduce%20the%20forgetting%20issue%3A%20%281%29%20Boundary%0Aconstraint%20is%20to%20preserve%20uncertain%20boundary%20supporting%20samples%20for%20continually%0Are-estimating%20task%20boundaries.%20%282%29%20Prototype%20constraint%20is%20to%20maintain%20the%0Aconsistency%20of%20learned%20class%20prototypes%20along%20with%20training.%20Our%20approach%20is%0Aevaluated%20on%20three%20standard%20long-tailed%20benchmarks%2C%20demonstrating%20superior%0Aperformance%20to%20existing%20CL%20methods%20and%20previous%20SOTA%20LTCL%20approach%20in%20both%0Atask-%20and%20class-incremental%20learning%20settings%2C%20as%20well%20as%20ordered-%20and%0Ashuffled-LTCL%20settings.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2408.14976v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DPrior-free%2520Balanced%2520Replay%253A%2520Uncertainty-guided%2520Reservoir%2520Sampling%2520for%250A%2520%2520Long-Tailed%2520Continual%2520Learning%26entry.906535625%3DLei%2520Liu%2520and%2520Li%2520Liu%2520and%2520Yawen%2520Cui%26entry.1292438233%3D%2520%2520Even%2520in%2520the%2520era%2520of%2520large%2520models%252C%2520one%2520of%2520the%2520well-known%2520issues%2520in%2520continual%250Alearning%2520%2528CL%2529%2520is%2520catastrophic%2520forgetting%252C%2520which%2520is%2520significantly%2520challenging%250Awhen%2520the%2520continual%2520data%2520stream%2520exhibits%2520a%2520long-tailed%2520distribution%252C%2520termed%2520as%250ALong-Tailed%2520Continual%2520Learning%2520%2528LTCL%2529.%2520Existing%2520LTCL%2520solutions%2520generally%250Arequire%2520the%2520label%2520distribution%2520of%2520the%2520data%2520stream%2520to%2520achieve%2520re-balance%250Atraining.%2520However%252C%2520obtaining%2520such%2520prior%2520information%2520is%2520often%2520infeasible%2520in%2520real%250Ascenarios%2520since%2520the%2520model%2520should%2520learn%2520without%2520pre-identifying%2520the%2520majority%2520and%250Aminority%2520classes.%2520To%2520this%2520end%252C%2520we%2520propose%2520a%2520novel%2520Prior-free%2520Balanced%2520Replay%250A%2528PBR%2529%2520framework%2520to%2520learn%2520from%2520long-tailed%2520data%2520stream%2520with%2520less%2520forgetting.%250AConcretely%252C%2520motivated%2520by%2520our%2520experimental%2520finding%2520that%2520the%2520minority%2520classes%2520are%250Amore%2520likely%2520to%2520be%2520forgotten%2520due%2520to%2520the%2520higher%2520uncertainty%252C%2520we%2520newly%2520design%2520an%250Auncertainty-guided%2520reservoir%2520sampling%2520strategy%2520to%2520prioritize%2520rehearsing%250Aminority%2520data%2520without%2520using%2520any%2520prior%2520information%252C%2520which%2520is%2520based%2520on%2520the%2520mutual%250Adependence%2520between%2520the%2520model%2520and%2520samples.%2520Additionally%252C%2520we%2520incorporate%2520two%250Aprior-free%2520components%2520to%2520further%2520reduce%2520the%2520forgetting%2520issue%253A%2520%25281%2529%2520Boundary%250Aconstraint%2520is%2520to%2520preserve%2520uncertain%2520boundary%2520supporting%2520samples%2520for%2520continually%250Are-estimating%2520task%2520boundaries.%2520%25282%2529%2520Prototype%2520constraint%2520is%2520to%2520maintain%2520the%250Aconsistency%2520of%2520learned%2520class%2520prototypes%2520along%2520with%2520training.%2520Our%2520approach%2520is%250Aevaluated%2520on%2520three%2520standard%2520long-tailed%2520benchmarks%252C%2520demonstrating%2520superior%250Aperformance%2520to%2520existing%2520CL%2520methods%2520and%2520previous%2520SOTA%2520LTCL%2520approach%2520in%2520both%250Atask-%2520and%2520class-incremental%2520learning%2520settings%252C%2520as%2520well%2520as%2520ordered-%2520and%250Ashuffled-LTCL%2520settings.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2408.14976v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Prior-free%20Balanced%20Replay%3A%20Uncertainty-guided%20Reservoir%20Sampling%20for%0A%20%20Long-Tailed%20Continual%20Learning&entry.906535625=Lei%20Liu%20and%20Li%20Liu%20and%20Yawen%20Cui&entry.1292438233=%20%20Even%20in%20the%20era%20of%20large%20models%2C%20one%20of%20the%20well-known%20issues%20in%20continual%0Alearning%20%28CL%29%20is%20catastrophic%20forgetting%2C%20which%20is%20significantly%20challenging%0Awhen%20the%20continual%20data%20stream%20exhibits%20a%20long-tailed%20distribution%2C%20termed%20as%0ALong-Tailed%20Continual%20Learning%20%28LTCL%29.%20Existing%20LTCL%20solutions%20generally%0Arequire%20the%20label%20distribution%20of%20the%20data%20stream%20to%20achieve%20re-balance%0Atraining.%20However%2C%20obtaining%20such%20prior%20information%20is%20often%20infeasible%20in%20real%0Ascenarios%20since%20the%20model%20should%20learn%20without%20pre-identifying%20the%20majority%20and%0Aminority%20classes.%20To%20this%20end%2C%20we%20propose%20a%20novel%20Prior-free%20Balanced%20Replay%0A%28PBR%29%20framework%20to%20learn%20from%20long-tailed%20data%20stream%20with%20less%20forgetting.%0AConcretely%2C%20motivated%20by%20our%20experimental%20finding%20that%20the%20minority%20classes%20are%0Amore%20likely%20to%20be%20forgotten%20due%20to%20the%20higher%20uncertainty%2C%20we%20newly%20design%20an%0Auncertainty-guided%20reservoir%20sampling%20strategy%20to%20prioritize%20rehearsing%0Aminority%20data%20without%20using%20any%20prior%20information%2C%20which%20is%20based%20on%20the%20mutual%0Adependence%20between%20the%20model%20and%20samples.%20Additionally%2C%20we%20incorporate%20two%0Aprior-free%20components%20to%20further%20reduce%20the%20forgetting%20issue%3A%20%281%29%20Boundary%0Aconstraint%20is%20to%20preserve%20uncertain%20boundary%20supporting%20samples%20for%20continually%0Are-estimating%20task%20boundaries.%20%282%29%20Prototype%20constraint%20is%20to%20maintain%20the%0Aconsistency%20of%20learned%20class%20prototypes%20along%20with%20training.%20Our%20approach%20is%0Aevaluated%20on%20three%20standard%20long-tailed%20benchmarks%2C%20demonstrating%20superior%0Aperformance%20to%20existing%20CL%20methods%20and%20previous%20SOTA%20LTCL%20approach%20in%20both%0Atask-%20and%20class-incremental%20learning%20settings%2C%20as%20well%20as%20ordered-%20and%0Ashuffled-LTCL%20settings.%0A&entry.1838667208=http%3A//arxiv.org/abs/2408.14976v1&entry.124074799=Read"},
{"title": "Hierarchical Graph Interaction Transformer with Dynamic Token Clustering\n  for Camouflaged Object Detection", "author": "Siyuan Yao and Hao Sun and Tian-Zhu Xiang and Xiao Wang and Xiaochun Cao", "abstract": "  Camouflaged object detection (COD) aims to identify the objects that\nseamlessly blend into the surrounding backgrounds. Due to the intrinsic\nsimilarity between the camouflaged objects and the background region, it is\nextremely challenging to precisely distinguish the camouflaged objects by\nexisting approaches. In this paper, we propose a hierarchical graph interaction\nnetwork termed HGINet for camouflaged object detection, which is capable of\ndiscovering imperceptible objects via effective graph interaction among the\nhierarchical tokenized features. Specifically, we first design a region-aware\ntoken focusing attention (RTFA) with dynamic token clustering to excavate the\npotentially distinguishable tokens in the local region. Afterwards, a\nhierarchical graph interaction transformer (HGIT) is proposed to construct\nbi-directional aligned communication between hierarchical features in the\nlatent interaction space for visual semantics enhancement. Furthermore, we\npropose a decoder network with confidence aggregated feature fusion (CAFF)\nmodules, which progressively fuses the hierarchical interacted features to\nrefine the local detail in ambiguous regions. Extensive experiments conducted\non the prevalent datasets, i.e. COD10K, CAMO, NC4K and CHAMELEON demonstrate\nthe superior performance of HGINet compared to existing state-of-the-art\nmethods. Our code is available at https://github.com/Garyson1204/HGINet.\n", "link": "http://arxiv.org/abs/2408.15020v1", "date": "2024-08-27", "relevancy": 2.741, "topK": [{"title": "Boosted dynamic neural networks", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/26302", "similarity": 0.5498}, {"title": "Magic-Me: Identity-Specific Video Customized Diffusion", "link": "http://arxiv.org/abs/2402.09368v1", "similarity": 0.549}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5458}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Hierarchical%20Graph%20Interaction%20Transformer%20with%20Dynamic%20Token%20Clustering%0A%20%20for%20Camouflaged%20Object%20Detection&body=Title%3A%20Hierarchical%20Graph%20Interaction%20Transformer%20with%20Dynamic%20Token%20Clustering%0A%20%20for%20Camouflaged%20Object%20Detection%0AAuthor%3A%20Siyuan%20Yao%20and%20Hao%20Sun%20and%20Tian-Zhu%20Xiang%20and%20Xiao%20Wang%20and%20Xiaochun%20Cao%0AAbstract%3A%20%20%20Camouflaged%20object%20detection%20%28COD%29%20aims%20to%20identify%20the%20objects%20that%0Aseamlessly%20blend%20into%20the%20surrounding%20backgrounds.%20Due%20to%20the%20intrinsic%0Asimilarity%20between%20the%20camouflaged%20objects%20and%20the%20background%20region%2C%20it%20is%0Aextremely%20challenging%20to%20precisely%20distinguish%20the%20camouflaged%20objects%20by%0Aexisting%20approaches.%20In%20this%20paper%2C%20we%20propose%20a%20hierarchical%20graph%20interaction%0Anetwork%20termed%20HGINet%20for%20camouflaged%20object%20detection%2C%20which%20is%20capable%20of%0Adiscovering%20imperceptible%20objects%20via%20effective%20graph%20interaction%20among%20the%0Ahierarchical%20tokenized%20features.%20Specifically%2C%20we%20first%20design%20a%20region-aware%0Atoken%20focusing%20attention%20%28RTFA%29%20with%20dynamic%20token%20clustering%20to%20excavate%20the%0Apotentially%20distinguishable%20tokens%20in%20the%20local%20region.%20Afterwards%2C%20a%0Ahierarchical%20graph%20interaction%20transformer%20%28HGIT%29%20is%20proposed%20to%20construct%0Abi-directional%20aligned%20communication%20between%20hierarchical%20features%20in%20the%0Alatent%20interaction%20space%20for%20visual%20semantics%20enhancement.%20Furthermore%2C%20we%0Apropose%20a%20decoder%20network%20with%20confidence%20aggregated%20feature%20fusion%20%28CAFF%29%0Amodules%2C%20which%20progressively%20fuses%20the%20hierarchical%20interacted%20features%20to%0Arefine%20the%20local%20detail%20in%20ambiguous%20regions.%20Extensive%20experiments%20conducted%0Aon%20the%20prevalent%20datasets%2C%20i.e.%20COD10K%2C%20CAMO%2C%20NC4K%20and%20CHAMELEON%20demonstrate%0Athe%20superior%20performance%20of%20HGINet%20compared%20to%20existing%20state-of-the-art%0Amethods.%20Our%20code%20is%20available%20at%20https%3A//github.com/Garyson1204/HGINet.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2408.15020v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DHierarchical%2520Graph%2520Interaction%2520Transformer%2520with%2520Dynamic%2520Token%2520Clustering%250A%2520%2520for%2520Camouflaged%2520Object%2520Detection%26entry.906535625%3DSiyuan%2520Yao%2520and%2520Hao%2520Sun%2520and%2520Tian-Zhu%2520Xiang%2520and%2520Xiao%2520Wang%2520and%2520Xiaochun%2520Cao%26entry.1292438233%3D%2520%2520Camouflaged%2520object%2520detection%2520%2528COD%2529%2520aims%2520to%2520identify%2520the%2520objects%2520that%250Aseamlessly%2520blend%2520into%2520the%2520surrounding%2520backgrounds.%2520Due%2520to%2520the%2520intrinsic%250Asimilarity%2520between%2520the%2520camouflaged%2520objects%2520and%2520the%2520background%2520region%252C%2520it%2520is%250Aextremely%2520challenging%2520to%2520precisely%2520distinguish%2520the%2520camouflaged%2520objects%2520by%250Aexisting%2520approaches.%2520In%2520this%2520paper%252C%2520we%2520propose%2520a%2520hierarchical%2520graph%2520interaction%250Anetwork%2520termed%2520HGINet%2520for%2520camouflaged%2520object%2520detection%252C%2520which%2520is%2520capable%2520of%250Adiscovering%2520imperceptible%2520objects%2520via%2520effective%2520graph%2520interaction%2520among%2520the%250Ahierarchical%2520tokenized%2520features.%2520Specifically%252C%2520we%2520first%2520design%2520a%2520region-aware%250Atoken%2520focusing%2520attention%2520%2528RTFA%2529%2520with%2520dynamic%2520token%2520clustering%2520to%2520excavate%2520the%250Apotentially%2520distinguishable%2520tokens%2520in%2520the%2520local%2520region.%2520Afterwards%252C%2520a%250Ahierarchical%2520graph%2520interaction%2520transformer%2520%2528HGIT%2529%2520is%2520proposed%2520to%2520construct%250Abi-directional%2520aligned%2520communication%2520between%2520hierarchical%2520features%2520in%2520the%250Alatent%2520interaction%2520space%2520for%2520visual%2520semantics%2520enhancement.%2520Furthermore%252C%2520we%250Apropose%2520a%2520decoder%2520network%2520with%2520confidence%2520aggregated%2520feature%2520fusion%2520%2528CAFF%2529%250Amodules%252C%2520which%2520progressively%2520fuses%2520the%2520hierarchical%2520interacted%2520features%2520to%250Arefine%2520the%2520local%2520detail%2520in%2520ambiguous%2520regions.%2520Extensive%2520experiments%2520conducted%250Aon%2520the%2520prevalent%2520datasets%252C%2520i.e.%2520COD10K%252C%2520CAMO%252C%2520NC4K%2520and%2520CHAMELEON%2520demonstrate%250Athe%2520superior%2520performance%2520of%2520HGINet%2520compared%2520to%2520existing%2520state-of-the-art%250Amethods.%2520Our%2520code%2520is%2520available%2520at%2520https%253A//github.com/Garyson1204/HGINet.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2408.15020v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Hierarchical%20Graph%20Interaction%20Transformer%20with%20Dynamic%20Token%20Clustering%0A%20%20for%20Camouflaged%20Object%20Detection&entry.906535625=Siyuan%20Yao%20and%20Hao%20Sun%20and%20Tian-Zhu%20Xiang%20and%20Xiao%20Wang%20and%20Xiaochun%20Cao&entry.1292438233=%20%20Camouflaged%20object%20detection%20%28COD%29%20aims%20to%20identify%20the%20objects%20that%0Aseamlessly%20blend%20into%20the%20surrounding%20backgrounds.%20Due%20to%20the%20intrinsic%0Asimilarity%20between%20the%20camouflaged%20objects%20and%20the%20background%20region%2C%20it%20is%0Aextremely%20challenging%20to%20precisely%20distinguish%20the%20camouflaged%20objects%20by%0Aexisting%20approaches.%20In%20this%20paper%2C%20we%20propose%20a%20hierarchical%20graph%20interaction%0Anetwork%20termed%20HGINet%20for%20camouflaged%20object%20detection%2C%20which%20is%20capable%20of%0Adiscovering%20imperceptible%20objects%20via%20effective%20graph%20interaction%20among%20the%0Ahierarchical%20tokenized%20features.%20Specifically%2C%20we%20first%20design%20a%20region-aware%0Atoken%20focusing%20attention%20%28RTFA%29%20with%20dynamic%20token%20clustering%20to%20excavate%20the%0Apotentially%20distinguishable%20tokens%20in%20the%20local%20region.%20Afterwards%2C%20a%0Ahierarchical%20graph%20interaction%20transformer%20%28HGIT%29%20is%20proposed%20to%20construct%0Abi-directional%20aligned%20communication%20between%20hierarchical%20features%20in%20the%0Alatent%20interaction%20space%20for%20visual%20semantics%20enhancement.%20Furthermore%2C%20we%0Apropose%20a%20decoder%20network%20with%20confidence%20aggregated%20feature%20fusion%20%28CAFF%29%0Amodules%2C%20which%20progressively%20fuses%20the%20hierarchical%20interacted%20features%20to%0Arefine%20the%20local%20detail%20in%20ambiguous%20regions.%20Extensive%20experiments%20conducted%0Aon%20the%20prevalent%20datasets%2C%20i.e.%20COD10K%2C%20CAMO%2C%20NC4K%20and%20CHAMELEON%20demonstrate%0Athe%20superior%20performance%20of%20HGINet%20compared%20to%20existing%20state-of-the-art%0Amethods.%20Our%20code%20is%20available%20at%20https%3A//github.com/Garyson1204/HGINet.%0A&entry.1838667208=http%3A//arxiv.org/abs/2408.15020v1&entry.124074799=Read"},
{"title": "Zero-Shot Character Identification and Speaker Prediction in Comics via\n  Iterative Multimodal Fusion", "author": "Yingxuan Li and Ryota Hinami and Kiyoharu Aizawa and Yusuke Matsui", "abstract": "  Recognizing characters and predicting speakers of dialogue are critical for\ncomic processing tasks, such as voice generation or translation. However,\nbecause characters vary by comic title, supervised learning approaches like\ntraining character classifiers which require specific annotations for each\ncomic title are infeasible. This motivates us to propose a novel zero-shot\napproach, allowing machines to identify characters and predict speaker names\nbased solely on unannotated comic images. In spite of their importance in\nreal-world applications, these task have largely remained unexplored due to\nchallenges in story comprehension and multimodal integration. Recent large\nlanguage models (LLMs) have shown great capability for text understanding and\nreasoning, while their application to multimodal content analysis is still an\nopen problem. To address this problem, we propose an iterative multimodal\nframework, the first to employ multimodal information for both character\nidentification and speaker prediction tasks. Our experiments demonstrate the\neffectiveness of the proposed framework, establishing a robust baseline for\nthese tasks. Furthermore, since our method requires no training data or\nannotations, it can be used as-is on any comic series.\n", "link": "http://arxiv.org/abs/2404.13993v3", "date": "2024-08-27", "relevancy": 2.6802, "topK": [{"title": "PoseAnimate: Zero-shot high fidelity pose controllable character\n  animation", "link": "http://arxiv.org/abs/2404.13680v2", "similarity": 0.574}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5364}, {"title": "Magic-Me: Identity-Specific Video Customized Diffusion", "link": "http://arxiv.org/abs/2402.09368v1", "similarity": 0.4978}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Zero-Shot%20Character%20Identification%20and%20Speaker%20Prediction%20in%20Comics%20via%0A%20%20Iterative%20Multimodal%20Fusion&body=Title%3A%20Zero-Shot%20Character%20Identification%20and%20Speaker%20Prediction%20in%20Comics%20via%0A%20%20Iterative%20Multimodal%20Fusion%0AAuthor%3A%20Yingxuan%20Li%20and%20Ryota%20Hinami%20and%20Kiyoharu%20Aizawa%20and%20Yusuke%20Matsui%0AAbstract%3A%20%20%20Recognizing%20characters%20and%20predicting%20speakers%20of%20dialogue%20are%20critical%20for%0Acomic%20processing%20tasks%2C%20such%20as%20voice%20generation%20or%20translation.%20However%2C%0Abecause%20characters%20vary%20by%20comic%20title%2C%20supervised%20learning%20approaches%20like%0Atraining%20character%20classifiers%20which%20require%20specific%20annotations%20for%20each%0Acomic%20title%20are%20infeasible.%20This%20motivates%20us%20to%20propose%20a%20novel%20zero-shot%0Aapproach%2C%20allowing%20machines%20to%20identify%20characters%20and%20predict%20speaker%20names%0Abased%20solely%20on%20unannotated%20comic%20images.%20In%20spite%20of%20their%20importance%20in%0Areal-world%20applications%2C%20these%20task%20have%20largely%20remained%20unexplored%20due%20to%0Achallenges%20in%20story%20comprehension%20and%20multimodal%20integration.%20Recent%20large%0Alanguage%20models%20%28LLMs%29%20have%20shown%20great%20capability%20for%20text%20understanding%20and%0Areasoning%2C%20while%20their%20application%20to%20multimodal%20content%20analysis%20is%20still%20an%0Aopen%20problem.%20To%20address%20this%20problem%2C%20we%20propose%20an%20iterative%20multimodal%0Aframework%2C%20the%20first%20to%20employ%20multimodal%20information%20for%20both%20character%0Aidentification%20and%20speaker%20prediction%20tasks.%20Our%20experiments%20demonstrate%20the%0Aeffectiveness%20of%20the%20proposed%20framework%2C%20establishing%20a%20robust%20baseline%20for%0Athese%20tasks.%20Furthermore%2C%20since%20our%20method%20requires%20no%20training%20data%20or%0Aannotations%2C%20it%20can%20be%20used%20as-is%20on%20any%20comic%20series.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2404.13993v3%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DZero-Shot%2520Character%2520Identification%2520and%2520Speaker%2520Prediction%2520in%2520Comics%2520via%250A%2520%2520Iterative%2520Multimodal%2520Fusion%26entry.906535625%3DYingxuan%2520Li%2520and%2520Ryota%2520Hinami%2520and%2520Kiyoharu%2520Aizawa%2520and%2520Yusuke%2520Matsui%26entry.1292438233%3D%2520%2520Recognizing%2520characters%2520and%2520predicting%2520speakers%2520of%2520dialogue%2520are%2520critical%2520for%250Acomic%2520processing%2520tasks%252C%2520such%2520as%2520voice%2520generation%2520or%2520translation.%2520However%252C%250Abecause%2520characters%2520vary%2520by%2520comic%2520title%252C%2520supervised%2520learning%2520approaches%2520like%250Atraining%2520character%2520classifiers%2520which%2520require%2520specific%2520annotations%2520for%2520each%250Acomic%2520title%2520are%2520infeasible.%2520This%2520motivates%2520us%2520to%2520propose%2520a%2520novel%2520zero-shot%250Aapproach%252C%2520allowing%2520machines%2520to%2520identify%2520characters%2520and%2520predict%2520speaker%2520names%250Abased%2520solely%2520on%2520unannotated%2520comic%2520images.%2520In%2520spite%2520of%2520their%2520importance%2520in%250Areal-world%2520applications%252C%2520these%2520task%2520have%2520largely%2520remained%2520unexplored%2520due%2520to%250Achallenges%2520in%2520story%2520comprehension%2520and%2520multimodal%2520integration.%2520Recent%2520large%250Alanguage%2520models%2520%2528LLMs%2529%2520have%2520shown%2520great%2520capability%2520for%2520text%2520understanding%2520and%250Areasoning%252C%2520while%2520their%2520application%2520to%2520multimodal%2520content%2520analysis%2520is%2520still%2520an%250Aopen%2520problem.%2520To%2520address%2520this%2520problem%252C%2520we%2520propose%2520an%2520iterative%2520multimodal%250Aframework%252C%2520the%2520first%2520to%2520employ%2520multimodal%2520information%2520for%2520both%2520character%250Aidentification%2520and%2520speaker%2520prediction%2520tasks.%2520Our%2520experiments%2520demonstrate%2520the%250Aeffectiveness%2520of%2520the%2520proposed%2520framework%252C%2520establishing%2520a%2520robust%2520baseline%2520for%250Athese%2520tasks.%2520Furthermore%252C%2520since%2520our%2520method%2520requires%2520no%2520training%2520data%2520or%250Aannotations%252C%2520it%2520can%2520be%2520used%2520as-is%2520on%2520any%2520comic%2520series.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2404.13993v3%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Zero-Shot%20Character%20Identification%20and%20Speaker%20Prediction%20in%20Comics%20via%0A%20%20Iterative%20Multimodal%20Fusion&entry.906535625=Yingxuan%20Li%20and%20Ryota%20Hinami%20and%20Kiyoharu%20Aizawa%20and%20Yusuke%20Matsui&entry.1292438233=%20%20Recognizing%20characters%20and%20predicting%20speakers%20of%20dialogue%20are%20critical%20for%0Acomic%20processing%20tasks%2C%20such%20as%20voice%20generation%20or%20translation.%20However%2C%0Abecause%20characters%20vary%20by%20comic%20title%2C%20supervised%20learning%20approaches%20like%0Atraining%20character%20classifiers%20which%20require%20specific%20annotations%20for%20each%0Acomic%20title%20are%20infeasible.%20This%20motivates%20us%20to%20propose%20a%20novel%20zero-shot%0Aapproach%2C%20allowing%20machines%20to%20identify%20characters%20and%20predict%20speaker%20names%0Abased%20solely%20on%20unannotated%20comic%20images.%20In%20spite%20of%20their%20importance%20in%0Areal-world%20applications%2C%20these%20task%20have%20largely%20remained%20unexplored%20due%20to%0Achallenges%20in%20story%20comprehension%20and%20multimodal%20integration.%20Recent%20large%0Alanguage%20models%20%28LLMs%29%20have%20shown%20great%20capability%20for%20text%20understanding%20and%0Areasoning%2C%20while%20their%20application%20to%20multimodal%20content%20analysis%20is%20still%20an%0Aopen%20problem.%20To%20address%20this%20problem%2C%20we%20propose%20an%20iterative%20multimodal%0Aframework%2C%20the%20first%20to%20employ%20multimodal%20information%20for%20both%20character%0Aidentification%20and%20speaker%20prediction%20tasks.%20Our%20experiments%20demonstrate%20the%0Aeffectiveness%20of%20the%20proposed%20framework%2C%20establishing%20a%20robust%20baseline%20for%0Athese%20tasks.%20Furthermore%2C%20since%20our%20method%20requires%20no%20training%20data%20or%0Aannotations%2C%20it%20can%20be%20used%20as-is%20on%20any%20comic%20series.%0A&entry.1838667208=http%3A//arxiv.org/abs/2404.13993v3&entry.124074799=Read"},
{"title": "SAM & SAM 2 in 3D Slicer: SegmentWithSAM Extension for Annotating\n  Medical Images", "author": "Zafer Yildiz and Yuwen Chen and Maciej A. Mazurowski", "abstract": "  Creating annotations for 3D medical data is time-consuming and often requires\nhighly specialized expertise. Various tools have been implemented to aid this\nprocess. Segment Anything Model 2 (SAM 2) offers a general-purpose prompt-based\nsegmentation algorithm designed to annotate videos. In this paper, we adapt\nthis model to the annotation of 3D medical images and offer our implementation\nin the form of an extension to the popular annotation software: 3D Slicer. Our\nextension allows users to place point prompts on 2D slices to generate\nannotation masks and propagate these annotations across entire volumes in\neither single-directional or bi-directional manners. Our code is publicly\navailable on https://github.com/mazurowski-lab/SlicerSegmentWithSAM and can be\neasily installed directly from the Extension Manager of 3D Slicer as well.\n", "link": "http://arxiv.org/abs/2408.15224v1", "date": "2024-08-27", "relevancy": 2.658, "topK": [{"title": "CAT3D: Create Anything in 3D with Multi-View Diffusion Models", "link": "http://arxiv.org/abs/2405.10314v1", "similarity": 0.5425}, {"title": "CAT3D: Create Anything in 3D with Multi-View Diffusion Models", "link": "http://arxiv.org/abs/2405.10314v1", "similarity": 0.5425}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5098}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20SAM%20%26%20SAM%202%20in%203D%20Slicer%3A%20SegmentWithSAM%20Extension%20for%20Annotating%0A%20%20Medical%20Images&body=Title%3A%20SAM%20%26%20SAM%202%20in%203D%20Slicer%3A%20SegmentWithSAM%20Extension%20for%20Annotating%0A%20%20Medical%20Images%0AAuthor%3A%20Zafer%20Yildiz%20and%20Yuwen%20Chen%20and%20Maciej%20A.%20Mazurowski%0AAbstract%3A%20%20%20Creating%20annotations%20for%203D%20medical%20data%20is%20time-consuming%20and%20often%20requires%0Ahighly%20specialized%20expertise.%20Various%20tools%20have%20been%20implemented%20to%20aid%20this%0Aprocess.%20Segment%20Anything%20Model%202%20%28SAM%202%29%20offers%20a%20general-purpose%20prompt-based%0Asegmentation%20algorithm%20designed%20to%20annotate%20videos.%20In%20this%20paper%2C%20we%20adapt%0Athis%20model%20to%20the%20annotation%20of%203D%20medical%20images%20and%20offer%20our%20implementation%0Ain%20the%20form%20of%20an%20extension%20to%20the%20popular%20annotation%20software%3A%203D%20Slicer.%20Our%0Aextension%20allows%20users%20to%20place%20point%20prompts%20on%202D%20slices%20to%20generate%0Aannotation%20masks%20and%20propagate%20these%20annotations%20across%20entire%20volumes%20in%0Aeither%20single-directional%20or%20bi-directional%20manners.%20Our%20code%20is%20publicly%0Aavailable%20on%20https%3A//github.com/mazurowski-lab/SlicerSegmentWithSAM%20and%20can%20be%0Aeasily%20installed%20directly%20from%20the%20Extension%20Manager%20of%203D%20Slicer%20as%20well.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2408.15224v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DSAM%2520%2526%2520SAM%25202%2520in%25203D%2520Slicer%253A%2520SegmentWithSAM%2520Extension%2520for%2520Annotating%250A%2520%2520Medical%2520Images%26entry.906535625%3DZafer%2520Yildiz%2520and%2520Yuwen%2520Chen%2520and%2520Maciej%2520A.%2520Mazurowski%26entry.1292438233%3D%2520%2520Creating%2520annotations%2520for%25203D%2520medical%2520data%2520is%2520time-consuming%2520and%2520often%2520requires%250Ahighly%2520specialized%2520expertise.%2520Various%2520tools%2520have%2520been%2520implemented%2520to%2520aid%2520this%250Aprocess.%2520Segment%2520Anything%2520Model%25202%2520%2528SAM%25202%2529%2520offers%2520a%2520general-purpose%2520prompt-based%250Asegmentation%2520algorithm%2520designed%2520to%2520annotate%2520videos.%2520In%2520this%2520paper%252C%2520we%2520adapt%250Athis%2520model%2520to%2520the%2520annotation%2520of%25203D%2520medical%2520images%2520and%2520offer%2520our%2520implementation%250Ain%2520the%2520form%2520of%2520an%2520extension%2520to%2520the%2520popular%2520annotation%2520software%253A%25203D%2520Slicer.%2520Our%250Aextension%2520allows%2520users%2520to%2520place%2520point%2520prompts%2520on%25202D%2520slices%2520to%2520generate%250Aannotation%2520masks%2520and%2520propagate%2520these%2520annotations%2520across%2520entire%2520volumes%2520in%250Aeither%2520single-directional%2520or%2520bi-directional%2520manners.%2520Our%2520code%2520is%2520publicly%250Aavailable%2520on%2520https%253A//github.com/mazurowski-lab/SlicerSegmentWithSAM%2520and%2520can%2520be%250Aeasily%2520installed%2520directly%2520from%2520the%2520Extension%2520Manager%2520of%25203D%2520Slicer%2520as%2520well.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2408.15224v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=SAM%20%26%20SAM%202%20in%203D%20Slicer%3A%20SegmentWithSAM%20Extension%20for%20Annotating%0A%20%20Medical%20Images&entry.906535625=Zafer%20Yildiz%20and%20Yuwen%20Chen%20and%20Maciej%20A.%20Mazurowski&entry.1292438233=%20%20Creating%20annotations%20for%203D%20medical%20data%20is%20time-consuming%20and%20often%20requires%0Ahighly%20specialized%20expertise.%20Various%20tools%20have%20been%20implemented%20to%20aid%20this%0Aprocess.%20Segment%20Anything%20Model%202%20%28SAM%202%29%20offers%20a%20general-purpose%20prompt-based%0Asegmentation%20algorithm%20designed%20to%20annotate%20videos.%20In%20this%20paper%2C%20we%20adapt%0Athis%20model%20to%20the%20annotation%20of%203D%20medical%20images%20and%20offer%20our%20implementation%0Ain%20the%20form%20of%20an%20extension%20to%20the%20popular%20annotation%20software%3A%203D%20Slicer.%20Our%0Aextension%20allows%20users%20to%20place%20point%20prompts%20on%202D%20slices%20to%20generate%0Aannotation%20masks%20and%20propagate%20these%20annotations%20across%20entire%20volumes%20in%0Aeither%20single-directional%20or%20bi-directional%20manners.%20Our%20code%20is%20publicly%0Aavailable%20on%20https%3A//github.com/mazurowski-lab/SlicerSegmentWithSAM%20and%20can%20be%0Aeasily%20installed%20directly%20from%20the%20Extension%20Manager%20of%203D%20Slicer%20as%20well.%0A&entry.1838667208=http%3A//arxiv.org/abs/2408.15224v1&entry.124074799=Read"},
{"title": "Dr.E Bridges Graphs with Large Language Models through Words", "author": "Zipeng Liu and Likang Wu and Ming He and Zhong Guan and Hongke Zhao and Nan Feng", "abstract": "  Significant efforts have been dedicated to integrating the powerful Large\nLanguage Models (LLMs) with diverse modalities, particularly focusing on the\nfusion of language, vision and audio data. However, the graph-structured data,\nwhich is inherently rich in structural and domain-specific knowledge, has not\nyet been gracefully adapted to LLMs. Existing methods either describe the graph\nwith raw text, suffering the loss of graph structural information, or feed\nGraph Neural Network (GNN) embeddings into LLMs at the cost of losing\nexplainable prompt semantics. To bridge this gap, we introduce an end-to-end\nmodality-aligning framework for LLM-graph alignment: Dual-Residual Vector\nQuantized-Variational AutoEncoder, namely Dr.E. Our approach is purposefully\ndesigned to facilitate token-level alignment with LLMs, enabling an effective\ntranslation of the intrinsic `language' of graphs into comprehensible natural\nlanguage. We also manage to enhance LLMs' more robust structural understanding\nof graphs by incorporating multiple views of the central nodes based on their\nsurrounding nodes at various distances. Our experimental evaluations on\nstandard graph tasks demonstrate competitive performance against other\nstate-of-the-art (SOTA) approaches. Additionally, our framework ensures certain\nvisual interpretability, efficiency, and robustness, marking the promising\nsuccessful endeavor to achieve token-level alignment between LLMs and GNNs. Our\ncode is available at: https://anonymous.4open.science/r/dre-817.\n", "link": "http://arxiv.org/abs/2406.15504v2", "date": "2024-08-27", "relevancy": 2.6427, "topK": [{"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5434}, {"title": "Self-supervised Photographic Image Layout Representation Learning", "link": "http://arxiv.org/abs/2403.03740v1", "similarity": 0.5244}, {"title": "DDM-NET: End-to-end learning of keypoint feature Detection, Description and Matching for 3D localization", "link": "https://arxiv.org/abs/2212.04575", "similarity": 0.5178}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Dr.E%20Bridges%20Graphs%20with%20Large%20Language%20Models%20through%20Words&body=Title%3A%20Dr.E%20Bridges%20Graphs%20with%20Large%20Language%20Models%20through%20Words%0AAuthor%3A%20Zipeng%20Liu%20and%20Likang%20Wu%20and%20Ming%20He%20and%20Zhong%20Guan%20and%20Hongke%20Zhao%20and%20Nan%20Feng%0AAbstract%3A%20%20%20Significant%20efforts%20have%20been%20dedicated%20to%20integrating%20the%20powerful%20Large%0ALanguage%20Models%20%28LLMs%29%20with%20diverse%20modalities%2C%20particularly%20focusing%20on%20the%0Afusion%20of%20language%2C%20vision%20and%20audio%20data.%20However%2C%20the%20graph-structured%20data%2C%0Awhich%20is%20inherently%20rich%20in%20structural%20and%20domain-specific%20knowledge%2C%20has%20not%0Ayet%20been%20gracefully%20adapted%20to%20LLMs.%20Existing%20methods%20either%20describe%20the%20graph%0Awith%20raw%20text%2C%20suffering%20the%20loss%20of%20graph%20structural%20information%2C%20or%20feed%0AGraph%20Neural%20Network%20%28GNN%29%20embeddings%20into%20LLMs%20at%20the%20cost%20of%20losing%0Aexplainable%20prompt%20semantics.%20To%20bridge%20this%20gap%2C%20we%20introduce%20an%20end-to-end%0Amodality-aligning%20framework%20for%20LLM-graph%20alignment%3A%20Dual-Residual%20Vector%0AQuantized-Variational%20AutoEncoder%2C%20namely%20Dr.E.%20Our%20approach%20is%20purposefully%0Adesigned%20to%20facilitate%20token-level%20alignment%20with%20LLMs%2C%20enabling%20an%20effective%0Atranslation%20of%20the%20intrinsic%20%60language%27%20of%20graphs%20into%20comprehensible%20natural%0Alanguage.%20We%20also%20manage%20to%20enhance%20LLMs%27%20more%20robust%20structural%20understanding%0Aof%20graphs%20by%20incorporating%20multiple%20views%20of%20the%20central%20nodes%20based%20on%20their%0Asurrounding%20nodes%20at%20various%20distances.%20Our%20experimental%20evaluations%20on%0Astandard%20graph%20tasks%20demonstrate%20competitive%20performance%20against%20other%0Astate-of-the-art%20%28SOTA%29%20approaches.%20Additionally%2C%20our%20framework%20ensures%20certain%0Avisual%20interpretability%2C%20efficiency%2C%20and%20robustness%2C%20marking%20the%20promising%0Asuccessful%20endeavor%20to%20achieve%20token-level%20alignment%20between%20LLMs%20and%20GNNs.%20Our%0Acode%20is%20available%20at%3A%20https%3A//anonymous.4open.science/r/dre-817.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2406.15504v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DDr.E%2520Bridges%2520Graphs%2520with%2520Large%2520Language%2520Models%2520through%2520Words%26entry.906535625%3DZipeng%2520Liu%2520and%2520Likang%2520Wu%2520and%2520Ming%2520He%2520and%2520Zhong%2520Guan%2520and%2520Hongke%2520Zhao%2520and%2520Nan%2520Feng%26entry.1292438233%3D%2520%2520Significant%2520efforts%2520have%2520been%2520dedicated%2520to%2520integrating%2520the%2520powerful%2520Large%250ALanguage%2520Models%2520%2528LLMs%2529%2520with%2520diverse%2520modalities%252C%2520particularly%2520focusing%2520on%2520the%250Afusion%2520of%2520language%252C%2520vision%2520and%2520audio%2520data.%2520However%252C%2520the%2520graph-structured%2520data%252C%250Awhich%2520is%2520inherently%2520rich%2520in%2520structural%2520and%2520domain-specific%2520knowledge%252C%2520has%2520not%250Ayet%2520been%2520gracefully%2520adapted%2520to%2520LLMs.%2520Existing%2520methods%2520either%2520describe%2520the%2520graph%250Awith%2520raw%2520text%252C%2520suffering%2520the%2520loss%2520of%2520graph%2520structural%2520information%252C%2520or%2520feed%250AGraph%2520Neural%2520Network%2520%2528GNN%2529%2520embeddings%2520into%2520LLMs%2520at%2520the%2520cost%2520of%2520losing%250Aexplainable%2520prompt%2520semantics.%2520To%2520bridge%2520this%2520gap%252C%2520we%2520introduce%2520an%2520end-to-end%250Amodality-aligning%2520framework%2520for%2520LLM-graph%2520alignment%253A%2520Dual-Residual%2520Vector%250AQuantized-Variational%2520AutoEncoder%252C%2520namely%2520Dr.E.%2520Our%2520approach%2520is%2520purposefully%250Adesigned%2520to%2520facilitate%2520token-level%2520alignment%2520with%2520LLMs%252C%2520enabling%2520an%2520effective%250Atranslation%2520of%2520the%2520intrinsic%2520%2560language%2527%2520of%2520graphs%2520into%2520comprehensible%2520natural%250Alanguage.%2520We%2520also%2520manage%2520to%2520enhance%2520LLMs%2527%2520more%2520robust%2520structural%2520understanding%250Aof%2520graphs%2520by%2520incorporating%2520multiple%2520views%2520of%2520the%2520central%2520nodes%2520based%2520on%2520their%250Asurrounding%2520nodes%2520at%2520various%2520distances.%2520Our%2520experimental%2520evaluations%2520on%250Astandard%2520graph%2520tasks%2520demonstrate%2520competitive%2520performance%2520against%2520other%250Astate-of-the-art%2520%2528SOTA%2529%2520approaches.%2520Additionally%252C%2520our%2520framework%2520ensures%2520certain%250Avisual%2520interpretability%252C%2520efficiency%252C%2520and%2520robustness%252C%2520marking%2520the%2520promising%250Asuccessful%2520endeavor%2520to%2520achieve%2520token-level%2520alignment%2520between%2520LLMs%2520and%2520GNNs.%2520Our%250Acode%2520is%2520available%2520at%253A%2520https%253A//anonymous.4open.science/r/dre-817.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2406.15504v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Dr.E%20Bridges%20Graphs%20with%20Large%20Language%20Models%20through%20Words&entry.906535625=Zipeng%20Liu%20and%20Likang%20Wu%20and%20Ming%20He%20and%20Zhong%20Guan%20and%20Hongke%20Zhao%20and%20Nan%20Feng&entry.1292438233=%20%20Significant%20efforts%20have%20been%20dedicated%20to%20integrating%20the%20powerful%20Large%0ALanguage%20Models%20%28LLMs%29%20with%20diverse%20modalities%2C%20particularly%20focusing%20on%20the%0Afusion%20of%20language%2C%20vision%20and%20audio%20data.%20However%2C%20the%20graph-structured%20data%2C%0Awhich%20is%20inherently%20rich%20in%20structural%20and%20domain-specific%20knowledge%2C%20has%20not%0Ayet%20been%20gracefully%20adapted%20to%20LLMs.%20Existing%20methods%20either%20describe%20the%20graph%0Awith%20raw%20text%2C%20suffering%20the%20loss%20of%20graph%20structural%20information%2C%20or%20feed%0AGraph%20Neural%20Network%20%28GNN%29%20embeddings%20into%20LLMs%20at%20the%20cost%20of%20losing%0Aexplainable%20prompt%20semantics.%20To%20bridge%20this%20gap%2C%20we%20introduce%20an%20end-to-end%0Amodality-aligning%20framework%20for%20LLM-graph%20alignment%3A%20Dual-Residual%20Vector%0AQuantized-Variational%20AutoEncoder%2C%20namely%20Dr.E.%20Our%20approach%20is%20purposefully%0Adesigned%20to%20facilitate%20token-level%20alignment%20with%20LLMs%2C%20enabling%20an%20effective%0Atranslation%20of%20the%20intrinsic%20%60language%27%20of%20graphs%20into%20comprehensible%20natural%0Alanguage.%20We%20also%20manage%20to%20enhance%20LLMs%27%20more%20robust%20structural%20understanding%0Aof%20graphs%20by%20incorporating%20multiple%20views%20of%20the%20central%20nodes%20based%20on%20their%0Asurrounding%20nodes%20at%20various%20distances.%20Our%20experimental%20evaluations%20on%0Astandard%20graph%20tasks%20demonstrate%20competitive%20performance%20against%20other%0Astate-of-the-art%20%28SOTA%29%20approaches.%20Additionally%2C%20our%20framework%20ensures%20certain%0Avisual%20interpretability%2C%20efficiency%2C%20and%20robustness%2C%20marking%20the%20promising%0Asuccessful%20endeavor%20to%20achieve%20token-level%20alignment%20between%20LLMs%20and%20GNNs.%20Our%0Acode%20is%20available%20at%3A%20https%3A//anonymous.4open.science/r/dre-817.%0A&entry.1838667208=http%3A//arxiv.org/abs/2406.15504v2&entry.124074799=Read"},
{"title": "Sequence-aware Pre-training for Echocardiography Probe Guidance", "author": "Haojun Jiang and Zhenguo Sun and Yu Sun and Ning Jia and Meng Li and Shaqi Luo and Shiji Song and Gao Huang", "abstract": "  Cardiac ultrasound probe guidance aims to help novices adjust the 6-DOF probe\npose to obtain high-quality sectional images. Cardiac ultrasound faces two\nmajor challenges: (1) the inherently complex structure of the heart, and (2)\nsignificant individual variations. Previous works have only learned the\npopulation-averaged 2D and 3D structures of the heart rather than personalized\ncardiac structural features, leading to a performance bottleneck. Clinically,\nwe observed that sonographers adjust their understanding of a patient's cardiac\nstructure based on prior scanning sequences, thereby modifying their scanning\nstrategies. Inspired by this, we propose a sequence-aware self-supervised\npre-training method. Specifically, our approach learns personalized 2D and 3D\ncardiac structural features by predicting the masked-out images and actions in\na scanning sequence. We hypothesize that if the model can predict the missing\ncontent it has acquired a good understanding of the personalized cardiac\nstructure. In the downstream probe guidance task, we also introduced a sequence\nmodeling approach that models individual cardiac structural information based\non the images and actions from historical scan data, enabling more accurate\nnavigation decisions. Experiments on a large-scale dataset with 1.36 million\nsamples demonstrated that our proposed sequence-aware paradigm can\nsignificantly reduce navigation errors, with translation errors decreasing by\n15.90% to 36.87% and rotation errors decreasing by 11.13% to 20.77%, compared\nto state-of-the-art methods.\n", "link": "http://arxiv.org/abs/2408.15026v1", "date": "2024-08-27", "relevancy": 2.6349, "topK": [{"title": "Weakly-guided self-supervised pretraining for temporal activity detection", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/25189", "similarity": 0.5323}, {"title": "DDM-NET: End-to-end learning of keypoint feature Detection, Description and Matching for 3D localization", "link": "https://arxiv.org/abs/2212.04575", "similarity": 0.5311}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5175}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Sequence-aware%20Pre-training%20for%20Echocardiography%20Probe%20Guidance&body=Title%3A%20Sequence-aware%20Pre-training%20for%20Echocardiography%20Probe%20Guidance%0AAuthor%3A%20Haojun%20Jiang%20and%20Zhenguo%20Sun%20and%20Yu%20Sun%20and%20Ning%20Jia%20and%20Meng%20Li%20and%20Shaqi%20Luo%20and%20Shiji%20Song%20and%20Gao%20Huang%0AAbstract%3A%20%20%20Cardiac%20ultrasound%20probe%20guidance%20aims%20to%20help%20novices%20adjust%20the%206-DOF%20probe%0Apose%20to%20obtain%20high-quality%20sectional%20images.%20Cardiac%20ultrasound%20faces%20two%0Amajor%20challenges%3A%20%281%29%20the%20inherently%20complex%20structure%20of%20the%20heart%2C%20and%20%282%29%0Asignificant%20individual%20variations.%20Previous%20works%20have%20only%20learned%20the%0Apopulation-averaged%202D%20and%203D%20structures%20of%20the%20heart%20rather%20than%20personalized%0Acardiac%20structural%20features%2C%20leading%20to%20a%20performance%20bottleneck.%20Clinically%2C%0Awe%20observed%20that%20sonographers%20adjust%20their%20understanding%20of%20a%20patient%27s%20cardiac%0Astructure%20based%20on%20prior%20scanning%20sequences%2C%20thereby%20modifying%20their%20scanning%0Astrategies.%20Inspired%20by%20this%2C%20we%20propose%20a%20sequence-aware%20self-supervised%0Apre-training%20method.%20Specifically%2C%20our%20approach%20learns%20personalized%202D%20and%203D%0Acardiac%20structural%20features%20by%20predicting%20the%20masked-out%20images%20and%20actions%20in%0Aa%20scanning%20sequence.%20We%20hypothesize%20that%20if%20the%20model%20can%20predict%20the%20missing%0Acontent%20it%20has%20acquired%20a%20good%20understanding%20of%20the%20personalized%20cardiac%0Astructure.%20In%20the%20downstream%20probe%20guidance%20task%2C%20we%20also%20introduced%20a%20sequence%0Amodeling%20approach%20that%20models%20individual%20cardiac%20structural%20information%20based%0Aon%20the%20images%20and%20actions%20from%20historical%20scan%20data%2C%20enabling%20more%20accurate%0Anavigation%20decisions.%20Experiments%20on%20a%20large-scale%20dataset%20with%201.36%20million%0Asamples%20demonstrated%20that%20our%20proposed%20sequence-aware%20paradigm%20can%0Asignificantly%20reduce%20navigation%20errors%2C%20with%20translation%20errors%20decreasing%20by%0A15.90%25%20to%2036.87%25%20and%20rotation%20errors%20decreasing%20by%2011.13%25%20to%2020.77%25%2C%20compared%0Ato%20state-of-the-art%20methods.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2408.15026v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DSequence-aware%2520Pre-training%2520for%2520Echocardiography%2520Probe%2520Guidance%26entry.906535625%3DHaojun%2520Jiang%2520and%2520Zhenguo%2520Sun%2520and%2520Yu%2520Sun%2520and%2520Ning%2520Jia%2520and%2520Meng%2520Li%2520and%2520Shaqi%2520Luo%2520and%2520Shiji%2520Song%2520and%2520Gao%2520Huang%26entry.1292438233%3D%2520%2520Cardiac%2520ultrasound%2520probe%2520guidance%2520aims%2520to%2520help%2520novices%2520adjust%2520the%25206-DOF%2520probe%250Apose%2520to%2520obtain%2520high-quality%2520sectional%2520images.%2520Cardiac%2520ultrasound%2520faces%2520two%250Amajor%2520challenges%253A%2520%25281%2529%2520the%2520inherently%2520complex%2520structure%2520of%2520the%2520heart%252C%2520and%2520%25282%2529%250Asignificant%2520individual%2520variations.%2520Previous%2520works%2520have%2520only%2520learned%2520the%250Apopulation-averaged%25202D%2520and%25203D%2520structures%2520of%2520the%2520heart%2520rather%2520than%2520personalized%250Acardiac%2520structural%2520features%252C%2520leading%2520to%2520a%2520performance%2520bottleneck.%2520Clinically%252C%250Awe%2520observed%2520that%2520sonographers%2520adjust%2520their%2520understanding%2520of%2520a%2520patient%2527s%2520cardiac%250Astructure%2520based%2520on%2520prior%2520scanning%2520sequences%252C%2520thereby%2520modifying%2520their%2520scanning%250Astrategies.%2520Inspired%2520by%2520this%252C%2520we%2520propose%2520a%2520sequence-aware%2520self-supervised%250Apre-training%2520method.%2520Specifically%252C%2520our%2520approach%2520learns%2520personalized%25202D%2520and%25203D%250Acardiac%2520structural%2520features%2520by%2520predicting%2520the%2520masked-out%2520images%2520and%2520actions%2520in%250Aa%2520scanning%2520sequence.%2520We%2520hypothesize%2520that%2520if%2520the%2520model%2520can%2520predict%2520the%2520missing%250Acontent%2520it%2520has%2520acquired%2520a%2520good%2520understanding%2520of%2520the%2520personalized%2520cardiac%250Astructure.%2520In%2520the%2520downstream%2520probe%2520guidance%2520task%252C%2520we%2520also%2520introduced%2520a%2520sequence%250Amodeling%2520approach%2520that%2520models%2520individual%2520cardiac%2520structural%2520information%2520based%250Aon%2520the%2520images%2520and%2520actions%2520from%2520historical%2520scan%2520data%252C%2520enabling%2520more%2520accurate%250Anavigation%2520decisions.%2520Experiments%2520on%2520a%2520large-scale%2520dataset%2520with%25201.36%2520million%250Asamples%2520demonstrated%2520that%2520our%2520proposed%2520sequence-aware%2520paradigm%2520can%250Asignificantly%2520reduce%2520navigation%2520errors%252C%2520with%2520translation%2520errors%2520decreasing%2520by%250A15.90%2525%2520to%252036.87%2525%2520and%2520rotation%2520errors%2520decreasing%2520by%252011.13%2525%2520to%252020.77%2525%252C%2520compared%250Ato%2520state-of-the-art%2520methods.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2408.15026v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Sequence-aware%20Pre-training%20for%20Echocardiography%20Probe%20Guidance&entry.906535625=Haojun%20Jiang%20and%20Zhenguo%20Sun%20and%20Yu%20Sun%20and%20Ning%20Jia%20and%20Meng%20Li%20and%20Shaqi%20Luo%20and%20Shiji%20Song%20and%20Gao%20Huang&entry.1292438233=%20%20Cardiac%20ultrasound%20probe%20guidance%20aims%20to%20help%20novices%20adjust%20the%206-DOF%20probe%0Apose%20to%20obtain%20high-quality%20sectional%20images.%20Cardiac%20ultrasound%20faces%20two%0Amajor%20challenges%3A%20%281%29%20the%20inherently%20complex%20structure%20of%20the%20heart%2C%20and%20%282%29%0Asignificant%20individual%20variations.%20Previous%20works%20have%20only%20learned%20the%0Apopulation-averaged%202D%20and%203D%20structures%20of%20the%20heart%20rather%20than%20personalized%0Acardiac%20structural%20features%2C%20leading%20to%20a%20performance%20bottleneck.%20Clinically%2C%0Awe%20observed%20that%20sonographers%20adjust%20their%20understanding%20of%20a%20patient%27s%20cardiac%0Astructure%20based%20on%20prior%20scanning%20sequences%2C%20thereby%20modifying%20their%20scanning%0Astrategies.%20Inspired%20by%20this%2C%20we%20propose%20a%20sequence-aware%20self-supervised%0Apre-training%20method.%20Specifically%2C%20our%20approach%20learns%20personalized%202D%20and%203D%0Acardiac%20structural%20features%20by%20predicting%20the%20masked-out%20images%20and%20actions%20in%0Aa%20scanning%20sequence.%20We%20hypothesize%20that%20if%20the%20model%20can%20predict%20the%20missing%0Acontent%20it%20has%20acquired%20a%20good%20understanding%20of%20the%20personalized%20cardiac%0Astructure.%20In%20the%20downstream%20probe%20guidance%20task%2C%20we%20also%20introduced%20a%20sequence%0Amodeling%20approach%20that%20models%20individual%20cardiac%20structural%20information%20based%0Aon%20the%20images%20and%20actions%20from%20historical%20scan%20data%2C%20enabling%20more%20accurate%0Anavigation%20decisions.%20Experiments%20on%20a%20large-scale%20dataset%20with%201.36%20million%0Asamples%20demonstrated%20that%20our%20proposed%20sequence-aware%20paradigm%20can%0Asignificantly%20reduce%20navigation%20errors%2C%20with%20translation%20errors%20decreasing%20by%0A15.90%25%20to%2036.87%25%20and%20rotation%20errors%20decreasing%20by%2011.13%25%20to%2020.77%25%2C%20compared%0Ato%20state-of-the-art%20methods.%0A&entry.1838667208=http%3A//arxiv.org/abs/2408.15026v1&entry.124074799=Read"},
{"title": "Geometric Artifact Correction for Symmetric Multi-Linear Trajectory CT:\n  Theory, Method, and Generalization", "author": "Zhisheng Wang and Yanxu Sun and Shangyu Li and Legeng Lin and Shunli Wang and Junning Cui", "abstract": "  For extending CT field-of-view to perform non-destructive testing, the\nSymmetric Multi-Linear trajectory Computed Tomography (SMLCT) has been\ndeveloped as a successful example of non-standard CT scanning modes. However,\ninevitable geometric errors can cause severe artifacts in the reconstructed\nimages. The existing calibration method for SMLCT is both crude and\ninefficient. It involves reconstructing hundreds of images by exhaustively\nsubstituting each potential error, and then manually identifying the images\nwith the fewest geometric artifacts to estimate the final geometric errors for\ncalibration. In this paper, we comprehensively and efficiently address the\nchallenging geometric artifacts in SMLCT, , and the corresponding works mainly\ninvolve theory, method, and generalization. In particular, after identifying\nsensitive parameters and conducting some theory analysis of geometric\nartifacts, we summarize several key properties between sensitive geometric\nparameters and artifact characteristics. Then, we further construct\nmathematical relationships that relate sensitive geometric errors to the pixel\noffsets of reconstruction images with artifact characteristics. To accurately\nextract pixel bias, we innovatively adapt the Generalized Cross-Correlation\nwith Phase Transform (GCC-PHAT) algorithm, commonly used in sound processing,\nfor our image registration task for each paired symmetric LCT. This adaptation\nleads to the design of a highly efficient rigid translation registration\nmethod. Simulation and physical experiments have validated the excellent\nperformance of this work. Additionally, our results demonstrate significant\ngeneralization to common rotated CT and a variant of SMLCT.\n", "link": "http://arxiv.org/abs/2408.15069v1", "date": "2024-08-27", "relevancy": 2.6143, "topK": [{"title": "RTG-SLAM: Real-time 3D Reconstruction at Scale using Gaussian Splatting", "link": "http://arxiv.org/abs/2404.19706v1", "similarity": 0.5396}, {"title": "CAT3D: Create Anything in 3D with Multi-View Diffusion Models", "link": "http://arxiv.org/abs/2405.10314v1", "similarity": 0.5145}, {"title": "CAT3D: Create Anything in 3D with Multi-View Diffusion Models", "link": "http://arxiv.org/abs/2405.10314v1", "similarity": 0.5145}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Geometric%20Artifact%20Correction%20for%20Symmetric%20Multi-Linear%20Trajectory%20CT%3A%0A%20%20Theory%2C%20Method%2C%20and%20Generalization&body=Title%3A%20Geometric%20Artifact%20Correction%20for%20Symmetric%20Multi-Linear%20Trajectory%20CT%3A%0A%20%20Theory%2C%20Method%2C%20and%20Generalization%0AAuthor%3A%20Zhisheng%20Wang%20and%20Yanxu%20Sun%20and%20Shangyu%20Li%20and%20Legeng%20Lin%20and%20Shunli%20Wang%20and%20Junning%20Cui%0AAbstract%3A%20%20%20For%20extending%20CT%20field-of-view%20to%20perform%20non-destructive%20testing%2C%20the%0ASymmetric%20Multi-Linear%20trajectory%20Computed%20Tomography%20%28SMLCT%29%20has%20been%0Adeveloped%20as%20a%20successful%20example%20of%20non-standard%20CT%20scanning%20modes.%20However%2C%0Ainevitable%20geometric%20errors%20can%20cause%20severe%20artifacts%20in%20the%20reconstructed%0Aimages.%20The%20existing%20calibration%20method%20for%20SMLCT%20is%20both%20crude%20and%0Ainefficient.%20It%20involves%20reconstructing%20hundreds%20of%20images%20by%20exhaustively%0Asubstituting%20each%20potential%20error%2C%20and%20then%20manually%20identifying%20the%20images%0Awith%20the%20fewest%20geometric%20artifacts%20to%20estimate%20the%20final%20geometric%20errors%20for%0Acalibration.%20In%20this%20paper%2C%20we%20comprehensively%20and%20efficiently%20address%20the%0Achallenging%20geometric%20artifacts%20in%20SMLCT%2C%20%2C%20and%20the%20corresponding%20works%20mainly%0Ainvolve%20theory%2C%20method%2C%20and%20generalization.%20In%20particular%2C%20after%20identifying%0Asensitive%20parameters%20and%20conducting%20some%20theory%20analysis%20of%20geometric%0Aartifacts%2C%20we%20summarize%20several%20key%20properties%20between%20sensitive%20geometric%0Aparameters%20and%20artifact%20characteristics.%20Then%2C%20we%20further%20construct%0Amathematical%20relationships%20that%20relate%20sensitive%20geometric%20errors%20to%20the%20pixel%0Aoffsets%20of%20reconstruction%20images%20with%20artifact%20characteristics.%20To%20accurately%0Aextract%20pixel%20bias%2C%20we%20innovatively%20adapt%20the%20Generalized%20Cross-Correlation%0Awith%20Phase%20Transform%20%28GCC-PHAT%29%20algorithm%2C%20commonly%20used%20in%20sound%20processing%2C%0Afor%20our%20image%20registration%20task%20for%20each%20paired%20symmetric%20LCT.%20This%20adaptation%0Aleads%20to%20the%20design%20of%20a%20highly%20efficient%20rigid%20translation%20registration%0Amethod.%20Simulation%20and%20physical%20experiments%20have%20validated%20the%20excellent%0Aperformance%20of%20this%20work.%20Additionally%2C%20our%20results%20demonstrate%20significant%0Ageneralization%20to%20common%20rotated%20CT%20and%20a%20variant%20of%20SMLCT.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2408.15069v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DGeometric%2520Artifact%2520Correction%2520for%2520Symmetric%2520Multi-Linear%2520Trajectory%2520CT%253A%250A%2520%2520Theory%252C%2520Method%252C%2520and%2520Generalization%26entry.906535625%3DZhisheng%2520Wang%2520and%2520Yanxu%2520Sun%2520and%2520Shangyu%2520Li%2520and%2520Legeng%2520Lin%2520and%2520Shunli%2520Wang%2520and%2520Junning%2520Cui%26entry.1292438233%3D%2520%2520For%2520extending%2520CT%2520field-of-view%2520to%2520perform%2520non-destructive%2520testing%252C%2520the%250ASymmetric%2520Multi-Linear%2520trajectory%2520Computed%2520Tomography%2520%2528SMLCT%2529%2520has%2520been%250Adeveloped%2520as%2520a%2520successful%2520example%2520of%2520non-standard%2520CT%2520scanning%2520modes.%2520However%252C%250Ainevitable%2520geometric%2520errors%2520can%2520cause%2520severe%2520artifacts%2520in%2520the%2520reconstructed%250Aimages.%2520The%2520existing%2520calibration%2520method%2520for%2520SMLCT%2520is%2520both%2520crude%2520and%250Ainefficient.%2520It%2520involves%2520reconstructing%2520hundreds%2520of%2520images%2520by%2520exhaustively%250Asubstituting%2520each%2520potential%2520error%252C%2520and%2520then%2520manually%2520identifying%2520the%2520images%250Awith%2520the%2520fewest%2520geometric%2520artifacts%2520to%2520estimate%2520the%2520final%2520geometric%2520errors%2520for%250Acalibration.%2520In%2520this%2520paper%252C%2520we%2520comprehensively%2520and%2520efficiently%2520address%2520the%250Achallenging%2520geometric%2520artifacts%2520in%2520SMLCT%252C%2520%252C%2520and%2520the%2520corresponding%2520works%2520mainly%250Ainvolve%2520theory%252C%2520method%252C%2520and%2520generalization.%2520In%2520particular%252C%2520after%2520identifying%250Asensitive%2520parameters%2520and%2520conducting%2520some%2520theory%2520analysis%2520of%2520geometric%250Aartifacts%252C%2520we%2520summarize%2520several%2520key%2520properties%2520between%2520sensitive%2520geometric%250Aparameters%2520and%2520artifact%2520characteristics.%2520Then%252C%2520we%2520further%2520construct%250Amathematical%2520relationships%2520that%2520relate%2520sensitive%2520geometric%2520errors%2520to%2520the%2520pixel%250Aoffsets%2520of%2520reconstruction%2520images%2520with%2520artifact%2520characteristics.%2520To%2520accurately%250Aextract%2520pixel%2520bias%252C%2520we%2520innovatively%2520adapt%2520the%2520Generalized%2520Cross-Correlation%250Awith%2520Phase%2520Transform%2520%2528GCC-PHAT%2529%2520algorithm%252C%2520commonly%2520used%2520in%2520sound%2520processing%252C%250Afor%2520our%2520image%2520registration%2520task%2520for%2520each%2520paired%2520symmetric%2520LCT.%2520This%2520adaptation%250Aleads%2520to%2520the%2520design%2520of%2520a%2520highly%2520efficient%2520rigid%2520translation%2520registration%250Amethod.%2520Simulation%2520and%2520physical%2520experiments%2520have%2520validated%2520the%2520excellent%250Aperformance%2520of%2520this%2520work.%2520Additionally%252C%2520our%2520results%2520demonstrate%2520significant%250Ageneralization%2520to%2520common%2520rotated%2520CT%2520and%2520a%2520variant%2520of%2520SMLCT.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2408.15069v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Geometric%20Artifact%20Correction%20for%20Symmetric%20Multi-Linear%20Trajectory%20CT%3A%0A%20%20Theory%2C%20Method%2C%20and%20Generalization&entry.906535625=Zhisheng%20Wang%20and%20Yanxu%20Sun%20and%20Shangyu%20Li%20and%20Legeng%20Lin%20and%20Shunli%20Wang%20and%20Junning%20Cui&entry.1292438233=%20%20For%20extending%20CT%20field-of-view%20to%20perform%20non-destructive%20testing%2C%20the%0ASymmetric%20Multi-Linear%20trajectory%20Computed%20Tomography%20%28SMLCT%29%20has%20been%0Adeveloped%20as%20a%20successful%20example%20of%20non-standard%20CT%20scanning%20modes.%20However%2C%0Ainevitable%20geometric%20errors%20can%20cause%20severe%20artifacts%20in%20the%20reconstructed%0Aimages.%20The%20existing%20calibration%20method%20for%20SMLCT%20is%20both%20crude%20and%0Ainefficient.%20It%20involves%20reconstructing%20hundreds%20of%20images%20by%20exhaustively%0Asubstituting%20each%20potential%20error%2C%20and%20then%20manually%20identifying%20the%20images%0Awith%20the%20fewest%20geometric%20artifacts%20to%20estimate%20the%20final%20geometric%20errors%20for%0Acalibration.%20In%20this%20paper%2C%20we%20comprehensively%20and%20efficiently%20address%20the%0Achallenging%20geometric%20artifacts%20in%20SMLCT%2C%20%2C%20and%20the%20corresponding%20works%20mainly%0Ainvolve%20theory%2C%20method%2C%20and%20generalization.%20In%20particular%2C%20after%20identifying%0Asensitive%20parameters%20and%20conducting%20some%20theory%20analysis%20of%20geometric%0Aartifacts%2C%20we%20summarize%20several%20key%20properties%20between%20sensitive%20geometric%0Aparameters%20and%20artifact%20characteristics.%20Then%2C%20we%20further%20construct%0Amathematical%20relationships%20that%20relate%20sensitive%20geometric%20errors%20to%20the%20pixel%0Aoffsets%20of%20reconstruction%20images%20with%20artifact%20characteristics.%20To%20accurately%0Aextract%20pixel%20bias%2C%20we%20innovatively%20adapt%20the%20Generalized%20Cross-Correlation%0Awith%20Phase%20Transform%20%28GCC-PHAT%29%20algorithm%2C%20commonly%20used%20in%20sound%20processing%2C%0Afor%20our%20image%20registration%20task%20for%20each%20paired%20symmetric%20LCT.%20This%20adaptation%0Aleads%20to%20the%20design%20of%20a%20highly%20efficient%20rigid%20translation%20registration%0Amethod.%20Simulation%20and%20physical%20experiments%20have%20validated%20the%20excellent%0Aperformance%20of%20this%20work.%20Additionally%2C%20our%20results%20demonstrate%20significant%0Ageneralization%20to%20common%20rotated%20CT%20and%20a%20variant%20of%20SMLCT.%0A&entry.1838667208=http%3A//arxiv.org/abs/2408.15069v1&entry.124074799=Read"},
{"title": "Pre-training Everywhere: Parameter-Efficient Fine-Tuning for Medical\n  Image Analysis via Target Parameter Pre-training", "author": "Xingliang Lei and Yiwen Ye and Ziyang Chen and Minglei Shu and Yong Xia", "abstract": "  Parameter-efficient fine-tuning (PEFT) techniques have emerged to address\nissues of overfitting and high computational costs associated with fully\nfine-tuning in the paradigm of self-supervised learning. Mainstream methods\nbased on PEFT involve adding a few trainable parameters while keeping the\npre-trained parameters of the backbone fixed. These methods achieve\ncomparative, and often superior, performance to fully fine-tuning,\ndemonstrating the powerful representation ability of the pre-trained backbone.\nDespite its success, these methods typically ignore the initialization of the\nnew parameters, often relying solely on random initialization. We argue that if\npre-training is significantly beneficial, it should be applied to all\nparameters requiring representational capacity. Motivated by this insight, we\npropose a simple yet effective fine-tuning framework based on Target Parameter\nPre-training (TPP). The target parameters refer to the new parameters\nintroduced during fine-tuning. TPP includes an additional stage before PEFT to\npre-train these target parameters. During this stage, the pre-trained backbone\nparameters are frozen, and only the target parameters are trainable. A defined\npre-text task is used to encourage the target parameters to learn specific\nrepresentations of downstream data. When PEFT is subsequently employed, the\npre-trained target parameters are loaded to enhance fine-tuning efficiency. The\nproposed TPP framework is versatile, allowing for the integration of various\npretext tasks for pre-training and supporting different PEFT methods as\nbackbones. We evaluated the fine-tining performance of our method using five\npublic datasets, including three modalities and two task types. The results\ndemonstrate that the proposed TPP can be easily integrated into existing PEFT\nmethods, significantly improving performance.\n", "link": "http://arxiv.org/abs/2408.15011v1", "date": "2024-08-27", "relevancy": 2.5854, "topK": [{"title": "Weakly-guided self-supervised pretraining for temporal activity detection", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/25189", "similarity": 0.5229}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5197}, {"title": "ToDo: Token Downsampling for Efficient Generation of High-Resolution\n  Images", "link": "http://arxiv.org/abs/2402.13573v2", "similarity": 0.5087}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Pre-training%20Everywhere%3A%20Parameter-Efficient%20Fine-Tuning%20for%20Medical%0A%20%20Image%20Analysis%20via%20Target%20Parameter%20Pre-training&body=Title%3A%20Pre-training%20Everywhere%3A%20Parameter-Efficient%20Fine-Tuning%20for%20Medical%0A%20%20Image%20Analysis%20via%20Target%20Parameter%20Pre-training%0AAuthor%3A%20Xingliang%20Lei%20and%20Yiwen%20Ye%20and%20Ziyang%20Chen%20and%20Minglei%20Shu%20and%20Yong%20Xia%0AAbstract%3A%20%20%20Parameter-efficient%20fine-tuning%20%28PEFT%29%20techniques%20have%20emerged%20to%20address%0Aissues%20of%20overfitting%20and%20high%20computational%20costs%20associated%20with%20fully%0Afine-tuning%20in%20the%20paradigm%20of%20self-supervised%20learning.%20Mainstream%20methods%0Abased%20on%20PEFT%20involve%20adding%20a%20few%20trainable%20parameters%20while%20keeping%20the%0Apre-trained%20parameters%20of%20the%20backbone%20fixed.%20These%20methods%20achieve%0Acomparative%2C%20and%20often%20superior%2C%20performance%20to%20fully%20fine-tuning%2C%0Ademonstrating%20the%20powerful%20representation%20ability%20of%20the%20pre-trained%20backbone.%0ADespite%20its%20success%2C%20these%20methods%20typically%20ignore%20the%20initialization%20of%20the%0Anew%20parameters%2C%20often%20relying%20solely%20on%20random%20initialization.%20We%20argue%20that%20if%0Apre-training%20is%20significantly%20beneficial%2C%20it%20should%20be%20applied%20to%20all%0Aparameters%20requiring%20representational%20capacity.%20Motivated%20by%20this%20insight%2C%20we%0Apropose%20a%20simple%20yet%20effective%20fine-tuning%20framework%20based%20on%20Target%20Parameter%0APre-training%20%28TPP%29.%20The%20target%20parameters%20refer%20to%20the%20new%20parameters%0Aintroduced%20during%20fine-tuning.%20TPP%20includes%20an%20additional%20stage%20before%20PEFT%20to%0Apre-train%20these%20target%20parameters.%20During%20this%20stage%2C%20the%20pre-trained%20backbone%0Aparameters%20are%20frozen%2C%20and%20only%20the%20target%20parameters%20are%20trainable.%20A%20defined%0Apre-text%20task%20is%20used%20to%20encourage%20the%20target%20parameters%20to%20learn%20specific%0Arepresentations%20of%20downstream%20data.%20When%20PEFT%20is%20subsequently%20employed%2C%20the%0Apre-trained%20target%20parameters%20are%20loaded%20to%20enhance%20fine-tuning%20efficiency.%20The%0Aproposed%20TPP%20framework%20is%20versatile%2C%20allowing%20for%20the%20integration%20of%20various%0Apretext%20tasks%20for%20pre-training%20and%20supporting%20different%20PEFT%20methods%20as%0Abackbones.%20We%20evaluated%20the%20fine-tining%20performance%20of%20our%20method%20using%20five%0Apublic%20datasets%2C%20including%20three%20modalities%20and%20two%20task%20types.%20The%20results%0Ademonstrate%20that%20the%20proposed%20TPP%20can%20be%20easily%20integrated%20into%20existing%20PEFT%0Amethods%2C%20significantly%20improving%20performance.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2408.15011v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DPre-training%2520Everywhere%253A%2520Parameter-Efficient%2520Fine-Tuning%2520for%2520Medical%250A%2520%2520Image%2520Analysis%2520via%2520Target%2520Parameter%2520Pre-training%26entry.906535625%3DXingliang%2520Lei%2520and%2520Yiwen%2520Ye%2520and%2520Ziyang%2520Chen%2520and%2520Minglei%2520Shu%2520and%2520Yong%2520Xia%26entry.1292438233%3D%2520%2520Parameter-efficient%2520fine-tuning%2520%2528PEFT%2529%2520techniques%2520have%2520emerged%2520to%2520address%250Aissues%2520of%2520overfitting%2520and%2520high%2520computational%2520costs%2520associated%2520with%2520fully%250Afine-tuning%2520in%2520the%2520paradigm%2520of%2520self-supervised%2520learning.%2520Mainstream%2520methods%250Abased%2520on%2520PEFT%2520involve%2520adding%2520a%2520few%2520trainable%2520parameters%2520while%2520keeping%2520the%250Apre-trained%2520parameters%2520of%2520the%2520backbone%2520fixed.%2520These%2520methods%2520achieve%250Acomparative%252C%2520and%2520often%2520superior%252C%2520performance%2520to%2520fully%2520fine-tuning%252C%250Ademonstrating%2520the%2520powerful%2520representation%2520ability%2520of%2520the%2520pre-trained%2520backbone.%250ADespite%2520its%2520success%252C%2520these%2520methods%2520typically%2520ignore%2520the%2520initialization%2520of%2520the%250Anew%2520parameters%252C%2520often%2520relying%2520solely%2520on%2520random%2520initialization.%2520We%2520argue%2520that%2520if%250Apre-training%2520is%2520significantly%2520beneficial%252C%2520it%2520should%2520be%2520applied%2520to%2520all%250Aparameters%2520requiring%2520representational%2520capacity.%2520Motivated%2520by%2520this%2520insight%252C%2520we%250Apropose%2520a%2520simple%2520yet%2520effective%2520fine-tuning%2520framework%2520based%2520on%2520Target%2520Parameter%250APre-training%2520%2528TPP%2529.%2520The%2520target%2520parameters%2520refer%2520to%2520the%2520new%2520parameters%250Aintroduced%2520during%2520fine-tuning.%2520TPP%2520includes%2520an%2520additional%2520stage%2520before%2520PEFT%2520to%250Apre-train%2520these%2520target%2520parameters.%2520During%2520this%2520stage%252C%2520the%2520pre-trained%2520backbone%250Aparameters%2520are%2520frozen%252C%2520and%2520only%2520the%2520target%2520parameters%2520are%2520trainable.%2520A%2520defined%250Apre-text%2520task%2520is%2520used%2520to%2520encourage%2520the%2520target%2520parameters%2520to%2520learn%2520specific%250Arepresentations%2520of%2520downstream%2520data.%2520When%2520PEFT%2520is%2520subsequently%2520employed%252C%2520the%250Apre-trained%2520target%2520parameters%2520are%2520loaded%2520to%2520enhance%2520fine-tuning%2520efficiency.%2520The%250Aproposed%2520TPP%2520framework%2520is%2520versatile%252C%2520allowing%2520for%2520the%2520integration%2520of%2520various%250Apretext%2520tasks%2520for%2520pre-training%2520and%2520supporting%2520different%2520PEFT%2520methods%2520as%250Abackbones.%2520We%2520evaluated%2520the%2520fine-tining%2520performance%2520of%2520our%2520method%2520using%2520five%250Apublic%2520datasets%252C%2520including%2520three%2520modalities%2520and%2520two%2520task%2520types.%2520The%2520results%250Ademonstrate%2520that%2520the%2520proposed%2520TPP%2520can%2520be%2520easily%2520integrated%2520into%2520existing%2520PEFT%250Amethods%252C%2520significantly%2520improving%2520performance.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2408.15011v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Pre-training%20Everywhere%3A%20Parameter-Efficient%20Fine-Tuning%20for%20Medical%0A%20%20Image%20Analysis%20via%20Target%20Parameter%20Pre-training&entry.906535625=Xingliang%20Lei%20and%20Yiwen%20Ye%20and%20Ziyang%20Chen%20and%20Minglei%20Shu%20and%20Yong%20Xia&entry.1292438233=%20%20Parameter-efficient%20fine-tuning%20%28PEFT%29%20techniques%20have%20emerged%20to%20address%0Aissues%20of%20overfitting%20and%20high%20computational%20costs%20associated%20with%20fully%0Afine-tuning%20in%20the%20paradigm%20of%20self-supervised%20learning.%20Mainstream%20methods%0Abased%20on%20PEFT%20involve%20adding%20a%20few%20trainable%20parameters%20while%20keeping%20the%0Apre-trained%20parameters%20of%20the%20backbone%20fixed.%20These%20methods%20achieve%0Acomparative%2C%20and%20often%20superior%2C%20performance%20to%20fully%20fine-tuning%2C%0Ademonstrating%20the%20powerful%20representation%20ability%20of%20the%20pre-trained%20backbone.%0ADespite%20its%20success%2C%20these%20methods%20typically%20ignore%20the%20initialization%20of%20the%0Anew%20parameters%2C%20often%20relying%20solely%20on%20random%20initialization.%20We%20argue%20that%20if%0Apre-training%20is%20significantly%20beneficial%2C%20it%20should%20be%20applied%20to%20all%0Aparameters%20requiring%20representational%20capacity.%20Motivated%20by%20this%20insight%2C%20we%0Apropose%20a%20simple%20yet%20effective%20fine-tuning%20framework%20based%20on%20Target%20Parameter%0APre-training%20%28TPP%29.%20The%20target%20parameters%20refer%20to%20the%20new%20parameters%0Aintroduced%20during%20fine-tuning.%20TPP%20includes%20an%20additional%20stage%20before%20PEFT%20to%0Apre-train%20these%20target%20parameters.%20During%20this%20stage%2C%20the%20pre-trained%20backbone%0Aparameters%20are%20frozen%2C%20and%20only%20the%20target%20parameters%20are%20trainable.%20A%20defined%0Apre-text%20task%20is%20used%20to%20encourage%20the%20target%20parameters%20to%20learn%20specific%0Arepresentations%20of%20downstream%20data.%20When%20PEFT%20is%20subsequently%20employed%2C%20the%0Apre-trained%20target%20parameters%20are%20loaded%20to%20enhance%20fine-tuning%20efficiency.%20The%0Aproposed%20TPP%20framework%20is%20versatile%2C%20allowing%20for%20the%20integration%20of%20various%0Apretext%20tasks%20for%20pre-training%20and%20supporting%20different%20PEFT%20methods%20as%0Abackbones.%20We%20evaluated%20the%20fine-tining%20performance%20of%20our%20method%20using%20five%0Apublic%20datasets%2C%20including%20three%20modalities%20and%20two%20task%20types.%20The%20results%0Ademonstrate%20that%20the%20proposed%20TPP%20can%20be%20easily%20integrated%20into%20existing%20PEFT%0Amethods%2C%20significantly%20improving%20performance.%0A&entry.1838667208=http%3A//arxiv.org/abs/2408.15011v1&entry.124074799=Read"},
{"title": "GenRec: Unifying Video Generation and Recognition with Diffusion Models", "author": "Zejia Weng and Xitong Yang and Zhen Xing and Zuxuan Wu and Yu-Gang Jiang", "abstract": "  Video diffusion models are able to generate high-quality videos by learning\nstrong spatial-temporal priors on large-scale datasets. In this paper, we aim\nto investigate whether such priors derived from a generative process are\nsuitable for video recognition, and eventually joint optimization of generation\nand recognition. Building upon Stable Video Diffusion, we introduce GenRec, the\nfirst unified framework trained with a random-frame conditioning process so as\nto learn generalized spatial-temporal representations. The resulting framework\ncan naturally supports generation and recognition, and more importantly is\nrobust even when visual inputs contain limited information. Extensive\nexperiments demonstrate the efficacy of GenRec for both recognition and\ngeneration. In particular, GenRec achieves competitive recognition performance,\noffering 75.8% and 87.2% accuracy on SSV2 and K400, respectively. GenRec also\nperforms the best class-conditioned image-to-video generation results,\nachieving 46.5 and 49.3 FVD scores on SSV2 and EK-100 datasets. Furthermore,\nGenRec demonstrates extraordinary robustness in scenarios that only limited\nframes can be observed.\n", "link": "http://arxiv.org/abs/2408.15241v1", "date": "2024-08-27", "relevancy": 2.5818, "topK": [{"title": "Magic-Me: Identity-Specific Video Customized Diffusion", "link": "http://arxiv.org/abs/2402.09368v1", "similarity": 0.6714}, {"title": "Tuning-Free Noise Rectification for High Fidelity Image-to-Video\n  Generation", "link": "http://arxiv.org/abs/2403.02827v1", "similarity": 0.6313}, {"title": "Customize-A-Video: One-Shot Motion Customization of Text-to-Video\n  Diffusion Models", "link": "http://arxiv.org/abs/2402.14780v1", "similarity": 0.6159}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20GenRec%3A%20Unifying%20Video%20Generation%20and%20Recognition%20with%20Diffusion%20Models&body=Title%3A%20GenRec%3A%20Unifying%20Video%20Generation%20and%20Recognition%20with%20Diffusion%20Models%0AAuthor%3A%20Zejia%20Weng%20and%20Xitong%20Yang%20and%20Zhen%20Xing%20and%20Zuxuan%20Wu%20and%20Yu-Gang%20Jiang%0AAbstract%3A%20%20%20Video%20diffusion%20models%20are%20able%20to%20generate%20high-quality%20videos%20by%20learning%0Astrong%20spatial-temporal%20priors%20on%20large-scale%20datasets.%20In%20this%20paper%2C%20we%20aim%0Ato%20investigate%20whether%20such%20priors%20derived%20from%20a%20generative%20process%20are%0Asuitable%20for%20video%20recognition%2C%20and%20eventually%20joint%20optimization%20of%20generation%0Aand%20recognition.%20Building%20upon%20Stable%20Video%20Diffusion%2C%20we%20introduce%20GenRec%2C%20the%0Afirst%20unified%20framework%20trained%20with%20a%20random-frame%20conditioning%20process%20so%20as%0Ato%20learn%20generalized%20spatial-temporal%20representations.%20The%20resulting%20framework%0Acan%20naturally%20supports%20generation%20and%20recognition%2C%20and%20more%20importantly%20is%0Arobust%20even%20when%20visual%20inputs%20contain%20limited%20information.%20Extensive%0Aexperiments%20demonstrate%20the%20efficacy%20of%20GenRec%20for%20both%20recognition%20and%0Ageneration.%20In%20particular%2C%20GenRec%20achieves%20competitive%20recognition%20performance%2C%0Aoffering%2075.8%25%20and%2087.2%25%20accuracy%20on%20SSV2%20and%20K400%2C%20respectively.%20GenRec%20also%0Aperforms%20the%20best%20class-conditioned%20image-to-video%20generation%20results%2C%0Aachieving%2046.5%20and%2049.3%20FVD%20scores%20on%20SSV2%20and%20EK-100%20datasets.%20Furthermore%2C%0AGenRec%20demonstrates%20extraordinary%20robustness%20in%20scenarios%20that%20only%20limited%0Aframes%20can%20be%20observed.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2408.15241v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DGenRec%253A%2520Unifying%2520Video%2520Generation%2520and%2520Recognition%2520with%2520Diffusion%2520Models%26entry.906535625%3DZejia%2520Weng%2520and%2520Xitong%2520Yang%2520and%2520Zhen%2520Xing%2520and%2520Zuxuan%2520Wu%2520and%2520Yu-Gang%2520Jiang%26entry.1292438233%3D%2520%2520Video%2520diffusion%2520models%2520are%2520able%2520to%2520generate%2520high-quality%2520videos%2520by%2520learning%250Astrong%2520spatial-temporal%2520priors%2520on%2520large-scale%2520datasets.%2520In%2520this%2520paper%252C%2520we%2520aim%250Ato%2520investigate%2520whether%2520such%2520priors%2520derived%2520from%2520a%2520generative%2520process%2520are%250Asuitable%2520for%2520video%2520recognition%252C%2520and%2520eventually%2520joint%2520optimization%2520of%2520generation%250Aand%2520recognition.%2520Building%2520upon%2520Stable%2520Video%2520Diffusion%252C%2520we%2520introduce%2520GenRec%252C%2520the%250Afirst%2520unified%2520framework%2520trained%2520with%2520a%2520random-frame%2520conditioning%2520process%2520so%2520as%250Ato%2520learn%2520generalized%2520spatial-temporal%2520representations.%2520The%2520resulting%2520framework%250Acan%2520naturally%2520supports%2520generation%2520and%2520recognition%252C%2520and%2520more%2520importantly%2520is%250Arobust%2520even%2520when%2520visual%2520inputs%2520contain%2520limited%2520information.%2520Extensive%250Aexperiments%2520demonstrate%2520the%2520efficacy%2520of%2520GenRec%2520for%2520both%2520recognition%2520and%250Ageneration.%2520In%2520particular%252C%2520GenRec%2520achieves%2520competitive%2520recognition%2520performance%252C%250Aoffering%252075.8%2525%2520and%252087.2%2525%2520accuracy%2520on%2520SSV2%2520and%2520K400%252C%2520respectively.%2520GenRec%2520also%250Aperforms%2520the%2520best%2520class-conditioned%2520image-to-video%2520generation%2520results%252C%250Aachieving%252046.5%2520and%252049.3%2520FVD%2520scores%2520on%2520SSV2%2520and%2520EK-100%2520datasets.%2520Furthermore%252C%250AGenRec%2520demonstrates%2520extraordinary%2520robustness%2520in%2520scenarios%2520that%2520only%2520limited%250Aframes%2520can%2520be%2520observed.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2408.15241v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=GenRec%3A%20Unifying%20Video%20Generation%20and%20Recognition%20with%20Diffusion%20Models&entry.906535625=Zejia%20Weng%20and%20Xitong%20Yang%20and%20Zhen%20Xing%20and%20Zuxuan%20Wu%20and%20Yu-Gang%20Jiang&entry.1292438233=%20%20Video%20diffusion%20models%20are%20able%20to%20generate%20high-quality%20videos%20by%20learning%0Astrong%20spatial-temporal%20priors%20on%20large-scale%20datasets.%20In%20this%20paper%2C%20we%20aim%0Ato%20investigate%20whether%20such%20priors%20derived%20from%20a%20generative%20process%20are%0Asuitable%20for%20video%20recognition%2C%20and%20eventually%20joint%20optimization%20of%20generation%0Aand%20recognition.%20Building%20upon%20Stable%20Video%20Diffusion%2C%20we%20introduce%20GenRec%2C%20the%0Afirst%20unified%20framework%20trained%20with%20a%20random-frame%20conditioning%20process%20so%20as%0Ato%20learn%20generalized%20spatial-temporal%20representations.%20The%20resulting%20framework%0Acan%20naturally%20supports%20generation%20and%20recognition%2C%20and%20more%20importantly%20is%0Arobust%20even%20when%20visual%20inputs%20contain%20limited%20information.%20Extensive%0Aexperiments%20demonstrate%20the%20efficacy%20of%20GenRec%20for%20both%20recognition%20and%0Ageneration.%20In%20particular%2C%20GenRec%20achieves%20competitive%20recognition%20performance%2C%0Aoffering%2075.8%25%20and%2087.2%25%20accuracy%20on%20SSV2%20and%20K400%2C%20respectively.%20GenRec%20also%0Aperforms%20the%20best%20class-conditioned%20image-to-video%20generation%20results%2C%0Aachieving%2046.5%20and%2049.3%20FVD%20scores%20on%20SSV2%20and%20EK-100%20datasets.%20Furthermore%2C%0AGenRec%20demonstrates%20extraordinary%20robustness%20in%20scenarios%20that%20only%20limited%0Aframes%20can%20be%20observed.%0A&entry.1838667208=http%3A//arxiv.org/abs/2408.15241v1&entry.124074799=Read"},
{"title": "DIFR3CT: Latent Diffusion for Probabilistic 3D CT Reconstruction from\n  Few Planar X-Rays", "author": "Yiran Sun and Hana Baroudi and Tucker Netherton and Laurence Court and Osama Mawlawi and Ashok Veeraraghavan and Guha Balakrishnan", "abstract": "  Computed Tomography (CT) scans are the standard-of-care for the visualization\nand diagnosis of many clinical ailments, and are needed for the treatment\nplanning of external beam radiotherapy. Unfortunately, the availability of CT\nscanners in low- and mid-resource settings is highly variable. Planar x-ray\nradiography units, in comparison, are far more prevalent, but can only provide\nlimited 2D observations of the 3D anatomy. In this work we propose DIFR3CT, a\n3D latent diffusion model, that can generate a distribution of plausible CT\nvolumes from one or few (<10) planar x-ray observations. DIFR3CT works by\nfusing 2D features from each x-ray into a joint 3D space, and performing\ndiffusion conditioned on these fused features in a low-dimensional latent\nspace. We conduct extensive experiments demonstrating that DIFR3CT is better\nthan recent sparse CT reconstruction baselines in terms of standard pixel-level\n(PSNR, SSIM) on both the public LIDC and in-house post-mastectomy CT datasets.\nWe also show that DIFR3CT supports uncertainty quantification via Monte Carlo\nsampling, which provides an opportunity to measure reconstruction reliability.\nFinally, we perform a preliminary pilot study evaluating DIFR3CT for automated\nbreast radiotherapy contouring and planning -- and demonstrate promising\nfeasibility. Our code is available at https://github.com/yransun/DIFR3CT.\n", "link": "http://arxiv.org/abs/2408.15118v1", "date": "2024-08-27", "relevancy": 2.5652, "topK": [{"title": "CAT3D: Create Anything in 3D with Multi-View Diffusion Models", "link": "http://arxiv.org/abs/2405.10314v1", "similarity": 0.6504}, {"title": "CAT3D: Create Anything in 3D with Multi-View Diffusion Models", "link": "http://arxiv.org/abs/2405.10314v1", "similarity": 0.6504}, {"title": "SDXS: Real-Time One-Step Latent Diffusion Models with Image Conditions", "link": "http://arxiv.org/abs/2403.16627v1", "similarity": 0.5959}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20DIFR3CT%3A%20Latent%20Diffusion%20for%20Probabilistic%203D%20CT%20Reconstruction%20from%0A%20%20Few%20Planar%20X-Rays&body=Title%3A%20DIFR3CT%3A%20Latent%20Diffusion%20for%20Probabilistic%203D%20CT%20Reconstruction%20from%0A%20%20Few%20Planar%20X-Rays%0AAuthor%3A%20Yiran%20Sun%20and%20Hana%20Baroudi%20and%20Tucker%20Netherton%20and%20Laurence%20Court%20and%20Osama%20Mawlawi%20and%20Ashok%20Veeraraghavan%20and%20Guha%20Balakrishnan%0AAbstract%3A%20%20%20Computed%20Tomography%20%28CT%29%20scans%20are%20the%20standard-of-care%20for%20the%20visualization%0Aand%20diagnosis%20of%20many%20clinical%20ailments%2C%20and%20are%20needed%20for%20the%20treatment%0Aplanning%20of%20external%20beam%20radiotherapy.%20Unfortunately%2C%20the%20availability%20of%20CT%0Ascanners%20in%20low-%20and%20mid-resource%20settings%20is%20highly%20variable.%20Planar%20x-ray%0Aradiography%20units%2C%20in%20comparison%2C%20are%20far%20more%20prevalent%2C%20but%20can%20only%20provide%0Alimited%202D%20observations%20of%20the%203D%20anatomy.%20In%20this%20work%20we%20propose%20DIFR3CT%2C%20a%0A3D%20latent%20diffusion%20model%2C%20that%20can%20generate%20a%20distribution%20of%20plausible%20CT%0Avolumes%20from%20one%20or%20few%20%28%3C10%29%20planar%20x-ray%20observations.%20DIFR3CT%20works%20by%0Afusing%202D%20features%20from%20each%20x-ray%20into%20a%20joint%203D%20space%2C%20and%20performing%0Adiffusion%20conditioned%20on%20these%20fused%20features%20in%20a%20low-dimensional%20latent%0Aspace.%20We%20conduct%20extensive%20experiments%20demonstrating%20that%20DIFR3CT%20is%20better%0Athan%20recent%20sparse%20CT%20reconstruction%20baselines%20in%20terms%20of%20standard%20pixel-level%0A%28PSNR%2C%20SSIM%29%20on%20both%20the%20public%20LIDC%20and%20in-house%20post-mastectomy%20CT%20datasets.%0AWe%20also%20show%20that%20DIFR3CT%20supports%20uncertainty%20quantification%20via%20Monte%20Carlo%0Asampling%2C%20which%20provides%20an%20opportunity%20to%20measure%20reconstruction%20reliability.%0AFinally%2C%20we%20perform%20a%20preliminary%20pilot%20study%20evaluating%20DIFR3CT%20for%20automated%0Abreast%20radiotherapy%20contouring%20and%20planning%20--%20and%20demonstrate%20promising%0Afeasibility.%20Our%20code%20is%20available%20at%20https%3A//github.com/yransun/DIFR3CT.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2408.15118v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DDIFR3CT%253A%2520Latent%2520Diffusion%2520for%2520Probabilistic%25203D%2520CT%2520Reconstruction%2520from%250A%2520%2520Few%2520Planar%2520X-Rays%26entry.906535625%3DYiran%2520Sun%2520and%2520Hana%2520Baroudi%2520and%2520Tucker%2520Netherton%2520and%2520Laurence%2520Court%2520and%2520Osama%2520Mawlawi%2520and%2520Ashok%2520Veeraraghavan%2520and%2520Guha%2520Balakrishnan%26entry.1292438233%3D%2520%2520Computed%2520Tomography%2520%2528CT%2529%2520scans%2520are%2520the%2520standard-of-care%2520for%2520the%2520visualization%250Aand%2520diagnosis%2520of%2520many%2520clinical%2520ailments%252C%2520and%2520are%2520needed%2520for%2520the%2520treatment%250Aplanning%2520of%2520external%2520beam%2520radiotherapy.%2520Unfortunately%252C%2520the%2520availability%2520of%2520CT%250Ascanners%2520in%2520low-%2520and%2520mid-resource%2520settings%2520is%2520highly%2520variable.%2520Planar%2520x-ray%250Aradiography%2520units%252C%2520in%2520comparison%252C%2520are%2520far%2520more%2520prevalent%252C%2520but%2520can%2520only%2520provide%250Alimited%25202D%2520observations%2520of%2520the%25203D%2520anatomy.%2520In%2520this%2520work%2520we%2520propose%2520DIFR3CT%252C%2520a%250A3D%2520latent%2520diffusion%2520model%252C%2520that%2520can%2520generate%2520a%2520distribution%2520of%2520plausible%2520CT%250Avolumes%2520from%2520one%2520or%2520few%2520%2528%253C10%2529%2520planar%2520x-ray%2520observations.%2520DIFR3CT%2520works%2520by%250Afusing%25202D%2520features%2520from%2520each%2520x-ray%2520into%2520a%2520joint%25203D%2520space%252C%2520and%2520performing%250Adiffusion%2520conditioned%2520on%2520these%2520fused%2520features%2520in%2520a%2520low-dimensional%2520latent%250Aspace.%2520We%2520conduct%2520extensive%2520experiments%2520demonstrating%2520that%2520DIFR3CT%2520is%2520better%250Athan%2520recent%2520sparse%2520CT%2520reconstruction%2520baselines%2520in%2520terms%2520of%2520standard%2520pixel-level%250A%2528PSNR%252C%2520SSIM%2529%2520on%2520both%2520the%2520public%2520LIDC%2520and%2520in-house%2520post-mastectomy%2520CT%2520datasets.%250AWe%2520also%2520show%2520that%2520DIFR3CT%2520supports%2520uncertainty%2520quantification%2520via%2520Monte%2520Carlo%250Asampling%252C%2520which%2520provides%2520an%2520opportunity%2520to%2520measure%2520reconstruction%2520reliability.%250AFinally%252C%2520we%2520perform%2520a%2520preliminary%2520pilot%2520study%2520evaluating%2520DIFR3CT%2520for%2520automated%250Abreast%2520radiotherapy%2520contouring%2520and%2520planning%2520--%2520and%2520demonstrate%2520promising%250Afeasibility.%2520Our%2520code%2520is%2520available%2520at%2520https%253A//github.com/yransun/DIFR3CT.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2408.15118v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=DIFR3CT%3A%20Latent%20Diffusion%20for%20Probabilistic%203D%20CT%20Reconstruction%20from%0A%20%20Few%20Planar%20X-Rays&entry.906535625=Yiran%20Sun%20and%20Hana%20Baroudi%20and%20Tucker%20Netherton%20and%20Laurence%20Court%20and%20Osama%20Mawlawi%20and%20Ashok%20Veeraraghavan%20and%20Guha%20Balakrishnan&entry.1292438233=%20%20Computed%20Tomography%20%28CT%29%20scans%20are%20the%20standard-of-care%20for%20the%20visualization%0Aand%20diagnosis%20of%20many%20clinical%20ailments%2C%20and%20are%20needed%20for%20the%20treatment%0Aplanning%20of%20external%20beam%20radiotherapy.%20Unfortunately%2C%20the%20availability%20of%20CT%0Ascanners%20in%20low-%20and%20mid-resource%20settings%20is%20highly%20variable.%20Planar%20x-ray%0Aradiography%20units%2C%20in%20comparison%2C%20are%20far%20more%20prevalent%2C%20but%20can%20only%20provide%0Alimited%202D%20observations%20of%20the%203D%20anatomy.%20In%20this%20work%20we%20propose%20DIFR3CT%2C%20a%0A3D%20latent%20diffusion%20model%2C%20that%20can%20generate%20a%20distribution%20of%20plausible%20CT%0Avolumes%20from%20one%20or%20few%20%28%3C10%29%20planar%20x-ray%20observations.%20DIFR3CT%20works%20by%0Afusing%202D%20features%20from%20each%20x-ray%20into%20a%20joint%203D%20space%2C%20and%20performing%0Adiffusion%20conditioned%20on%20these%20fused%20features%20in%20a%20low-dimensional%20latent%0Aspace.%20We%20conduct%20extensive%20experiments%20demonstrating%20that%20DIFR3CT%20is%20better%0Athan%20recent%20sparse%20CT%20reconstruction%20baselines%20in%20terms%20of%20standard%20pixel-level%0A%28PSNR%2C%20SSIM%29%20on%20both%20the%20public%20LIDC%20and%20in-house%20post-mastectomy%20CT%20datasets.%0AWe%20also%20show%20that%20DIFR3CT%20supports%20uncertainty%20quantification%20via%20Monte%20Carlo%0Asampling%2C%20which%20provides%20an%20opportunity%20to%20measure%20reconstruction%20reliability.%0AFinally%2C%20we%20perform%20a%20preliminary%20pilot%20study%20evaluating%20DIFR3CT%20for%20automated%0Abreast%20radiotherapy%20contouring%20and%20planning%20--%20and%20demonstrate%20promising%0Afeasibility.%20Our%20code%20is%20available%20at%20https%3A//github.com/yransun/DIFR3CT.%0A&entry.1838667208=http%3A//arxiv.org/abs/2408.15118v1&entry.124074799=Read"},
{"title": "Recent Event Camera Innovations: A Survey", "author": "Bharatesh Chakravarthi and Aayush Atul Verma and Kostas Daniilidis and Cornelia Fermuller and Yezhou Yang", "abstract": "  Event-based vision, inspired by the human visual system, offers\ntransformative capabilities such as low latency, high dynamic range, and\nreduced power consumption. This paper presents a comprehensive survey of event\ncameras, tracing their evolution over time. It introduces the fundamental\nprinciples of event cameras, compares them with traditional frame cameras, and\nhighlights their unique characteristics and operational differences. The survey\ncovers various event camera models from leading manufacturers, key\ntechnological milestones, and influential research contributions. It explores\ndiverse application areas across different domains and discusses essential\nreal-world and synthetic datasets for research advancement. Additionally, the\nrole of event camera simulators in testing and development is discussed. This\nsurvey aims to consolidate the current state of event cameras and inspire\nfurther innovation in this rapidly evolving field. To support the research\ncommunity, a GitHub page\n(https://github.com/chakravarthi589/Event-based-Vision_Resources) categorizes\npast and future research articles and consolidates valuable resources.\n", "link": "http://arxiv.org/abs/2408.13627v2", "date": "2024-08-27", "relevancy": 2.5327, "topK": [{"title": "CAT3D: Create Anything in 3D with Multi-View Diffusion Models", "link": "http://arxiv.org/abs/2405.10314v1", "similarity": 0.5124}, {"title": "CAT3D: Create Anything in 3D with Multi-View Diffusion Models", "link": "http://arxiv.org/abs/2405.10314v1", "similarity": 0.5124}, {"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.4948}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Recent%20Event%20Camera%20Innovations%3A%20A%20Survey&body=Title%3A%20Recent%20Event%20Camera%20Innovations%3A%20A%20Survey%0AAuthor%3A%20Bharatesh%20Chakravarthi%20and%20Aayush%20Atul%20Verma%20and%20Kostas%20Daniilidis%20and%20Cornelia%20Fermuller%20and%20Yezhou%20Yang%0AAbstract%3A%20%20%20Event-based%20vision%2C%20inspired%20by%20the%20human%20visual%20system%2C%20offers%0Atransformative%20capabilities%20such%20as%20low%20latency%2C%20high%20dynamic%20range%2C%20and%0Areduced%20power%20consumption.%20This%20paper%20presents%20a%20comprehensive%20survey%20of%20event%0Acameras%2C%20tracing%20their%20evolution%20over%20time.%20It%20introduces%20the%20fundamental%0Aprinciples%20of%20event%20cameras%2C%20compares%20them%20with%20traditional%20frame%20cameras%2C%20and%0Ahighlights%20their%20unique%20characteristics%20and%20operational%20differences.%20The%20survey%0Acovers%20various%20event%20camera%20models%20from%20leading%20manufacturers%2C%20key%0Atechnological%20milestones%2C%20and%20influential%20research%20contributions.%20It%20explores%0Adiverse%20application%20areas%20across%20different%20domains%20and%20discusses%20essential%0Areal-world%20and%20synthetic%20datasets%20for%20research%20advancement.%20Additionally%2C%20the%0Arole%20of%20event%20camera%20simulators%20in%20testing%20and%20development%20is%20discussed.%20This%0Asurvey%20aims%20to%20consolidate%20the%20current%20state%20of%20event%20cameras%20and%20inspire%0Afurther%20innovation%20in%20this%20rapidly%20evolving%20field.%20To%20support%20the%20research%0Acommunity%2C%20a%20GitHub%20page%0A%28https%3A//github.com/chakravarthi589/Event-based-Vision_Resources%29%20categorizes%0Apast%20and%20future%20research%20articles%20and%20consolidates%20valuable%20resources.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2408.13627v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DRecent%2520Event%2520Camera%2520Innovations%253A%2520A%2520Survey%26entry.906535625%3DBharatesh%2520Chakravarthi%2520and%2520Aayush%2520Atul%2520Verma%2520and%2520Kostas%2520Daniilidis%2520and%2520Cornelia%2520Fermuller%2520and%2520Yezhou%2520Yang%26entry.1292438233%3D%2520%2520Event-based%2520vision%252C%2520inspired%2520by%2520the%2520human%2520visual%2520system%252C%2520offers%250Atransformative%2520capabilities%2520such%2520as%2520low%2520latency%252C%2520high%2520dynamic%2520range%252C%2520and%250Areduced%2520power%2520consumption.%2520This%2520paper%2520presents%2520a%2520comprehensive%2520survey%2520of%2520event%250Acameras%252C%2520tracing%2520their%2520evolution%2520over%2520time.%2520It%2520introduces%2520the%2520fundamental%250Aprinciples%2520of%2520event%2520cameras%252C%2520compares%2520them%2520with%2520traditional%2520frame%2520cameras%252C%2520and%250Ahighlights%2520their%2520unique%2520characteristics%2520and%2520operational%2520differences.%2520The%2520survey%250Acovers%2520various%2520event%2520camera%2520models%2520from%2520leading%2520manufacturers%252C%2520key%250Atechnological%2520milestones%252C%2520and%2520influential%2520research%2520contributions.%2520It%2520explores%250Adiverse%2520application%2520areas%2520across%2520different%2520domains%2520and%2520discusses%2520essential%250Areal-world%2520and%2520synthetic%2520datasets%2520for%2520research%2520advancement.%2520Additionally%252C%2520the%250Arole%2520of%2520event%2520camera%2520simulators%2520in%2520testing%2520and%2520development%2520is%2520discussed.%2520This%250Asurvey%2520aims%2520to%2520consolidate%2520the%2520current%2520state%2520of%2520event%2520cameras%2520and%2520inspire%250Afurther%2520innovation%2520in%2520this%2520rapidly%2520evolving%2520field.%2520To%2520support%2520the%2520research%250Acommunity%252C%2520a%2520GitHub%2520page%250A%2528https%253A//github.com/chakravarthi589/Event-based-Vision_Resources%2529%2520categorizes%250Apast%2520and%2520future%2520research%2520articles%2520and%2520consolidates%2520valuable%2520resources.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2408.13627v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Recent%20Event%20Camera%20Innovations%3A%20A%20Survey&entry.906535625=Bharatesh%20Chakravarthi%20and%20Aayush%20Atul%20Verma%20and%20Kostas%20Daniilidis%20and%20Cornelia%20Fermuller%20and%20Yezhou%20Yang&entry.1292438233=%20%20Event-based%20vision%2C%20inspired%20by%20the%20human%20visual%20system%2C%20offers%0Atransformative%20capabilities%20such%20as%20low%20latency%2C%20high%20dynamic%20range%2C%20and%0Areduced%20power%20consumption.%20This%20paper%20presents%20a%20comprehensive%20survey%20of%20event%0Acameras%2C%20tracing%20their%20evolution%20over%20time.%20It%20introduces%20the%20fundamental%0Aprinciples%20of%20event%20cameras%2C%20compares%20them%20with%20traditional%20frame%20cameras%2C%20and%0Ahighlights%20their%20unique%20characteristics%20and%20operational%20differences.%20The%20survey%0Acovers%20various%20event%20camera%20models%20from%20leading%20manufacturers%2C%20key%0Atechnological%20milestones%2C%20and%20influential%20research%20contributions.%20It%20explores%0Adiverse%20application%20areas%20across%20different%20domains%20and%20discusses%20essential%0Areal-world%20and%20synthetic%20datasets%20for%20research%20advancement.%20Additionally%2C%20the%0Arole%20of%20event%20camera%20simulators%20in%20testing%20and%20development%20is%20discussed.%20This%0Asurvey%20aims%20to%20consolidate%20the%20current%20state%20of%20event%20cameras%20and%20inspire%0Afurther%20innovation%20in%20this%20rapidly%20evolving%20field.%20To%20support%20the%20research%0Acommunity%2C%20a%20GitHub%20page%0A%28https%3A//github.com/chakravarthi589/Event-based-Vision_Resources%29%20categorizes%0Apast%20and%20future%20research%20articles%20and%20consolidates%20valuable%20resources.%0A&entry.1838667208=http%3A//arxiv.org/abs/2408.13627v2&entry.124074799=Read"},
{"title": "Revisiting LARS for Large Batch Training Generalization of Neural\n  Networks", "author": "Khoi Do and Duong Nguyen and Hoa Nguyen and Long Tran-Thanh and Nguyen-Hoang Tran and Quoc-Viet Pham", "abstract": "  This paper explores Large Batch Training techniques using layer-wise adaptive\nscaling ratio (LARS) across diverse settings, uncovering insights. LARS\nalgorithms with warm-up tend to be trapped in sharp minimizers early on due to\nredundant ratio scaling. Additionally, a fixed steep decline in the latter\nphase restricts deep neural networks from effectively navigating early-phase\nsharp minimizers. Building on these findings, we propose Time Varying LARS\n(TVLARS), a novel algorithm that replaces warm-up with a configurable\nsigmoid-like function for robust training in the initial phase. TVLARS promotes\ngradient exploration early on, surpassing sharp optimizers and gradually\ntransitioning to LARS for robustness in later phases. Extensive experiments\ndemonstrate that TVLARS consistently outperforms LARS and LAMB in most cases,\nwith up to 2\\% improvement in classification scenarios. Notably, in all\nself-supervised learning cases, TVLARS dominates LARS and LAMB with performance\nimprovements of up to 10\\%.\n", "link": "http://arxiv.org/abs/2309.14053v5", "date": "2024-08-27", "relevancy": 2.5218, "topK": [{"title": "Boosted dynamic neural networks", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/26302", "similarity": 0.5184}, {"title": "Breadcrumbs: Adversarial class-balanced sampling for long-tailed recognition", "link": "https://link.springer.com/chapter/10.1007/978-3-031-20053-3_37", "similarity": 0.5079}, {"title": "Self-supervised Photographic Image Layout Representation Learning", "link": "http://arxiv.org/abs/2403.03740v1", "similarity": 0.4867}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Revisiting%20LARS%20for%20Large%20Batch%20Training%20Generalization%20of%20Neural%0A%20%20Networks&body=Title%3A%20Revisiting%20LARS%20for%20Large%20Batch%20Training%20Generalization%20of%20Neural%0A%20%20Networks%0AAuthor%3A%20Khoi%20Do%20and%20Duong%20Nguyen%20and%20Hoa%20Nguyen%20and%20Long%20Tran-Thanh%20and%20Nguyen-Hoang%20Tran%20and%20Quoc-Viet%20Pham%0AAbstract%3A%20%20%20This%20paper%20explores%20Large%20Batch%20Training%20techniques%20using%20layer-wise%20adaptive%0Ascaling%20ratio%20%28LARS%29%20across%20diverse%20settings%2C%20uncovering%20insights.%20LARS%0Aalgorithms%20with%20warm-up%20tend%20to%20be%20trapped%20in%20sharp%20minimizers%20early%20on%20due%20to%0Aredundant%20ratio%20scaling.%20Additionally%2C%20a%20fixed%20steep%20decline%20in%20the%20latter%0Aphase%20restricts%20deep%20neural%20networks%20from%20effectively%20navigating%20early-phase%0Asharp%20minimizers.%20Building%20on%20these%20findings%2C%20we%20propose%20Time%20Varying%20LARS%0A%28TVLARS%29%2C%20a%20novel%20algorithm%20that%20replaces%20warm-up%20with%20a%20configurable%0Asigmoid-like%20function%20for%20robust%20training%20in%20the%20initial%20phase.%20TVLARS%20promotes%0Agradient%20exploration%20early%20on%2C%20surpassing%20sharp%20optimizers%20and%20gradually%0Atransitioning%20to%20LARS%20for%20robustness%20in%20later%20phases.%20Extensive%20experiments%0Ademonstrate%20that%20TVLARS%20consistently%20outperforms%20LARS%20and%20LAMB%20in%20most%20cases%2C%0Awith%20up%20to%202%5C%25%20improvement%20in%20classification%20scenarios.%20Notably%2C%20in%20all%0Aself-supervised%20learning%20cases%2C%20TVLARS%20dominates%20LARS%20and%20LAMB%20with%20performance%0Aimprovements%20of%20up%20to%2010%5C%25.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2309.14053v5%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DRevisiting%2520LARS%2520for%2520Large%2520Batch%2520Training%2520Generalization%2520of%2520Neural%250A%2520%2520Networks%26entry.906535625%3DKhoi%2520Do%2520and%2520Duong%2520Nguyen%2520and%2520Hoa%2520Nguyen%2520and%2520Long%2520Tran-Thanh%2520and%2520Nguyen-Hoang%2520Tran%2520and%2520Quoc-Viet%2520Pham%26entry.1292438233%3D%2520%2520This%2520paper%2520explores%2520Large%2520Batch%2520Training%2520techniques%2520using%2520layer-wise%2520adaptive%250Ascaling%2520ratio%2520%2528LARS%2529%2520across%2520diverse%2520settings%252C%2520uncovering%2520insights.%2520LARS%250Aalgorithms%2520with%2520warm-up%2520tend%2520to%2520be%2520trapped%2520in%2520sharp%2520minimizers%2520early%2520on%2520due%2520to%250Aredundant%2520ratio%2520scaling.%2520Additionally%252C%2520a%2520fixed%2520steep%2520decline%2520in%2520the%2520latter%250Aphase%2520restricts%2520deep%2520neural%2520networks%2520from%2520effectively%2520navigating%2520early-phase%250Asharp%2520minimizers.%2520Building%2520on%2520these%2520findings%252C%2520we%2520propose%2520Time%2520Varying%2520LARS%250A%2528TVLARS%2529%252C%2520a%2520novel%2520algorithm%2520that%2520replaces%2520warm-up%2520with%2520a%2520configurable%250Asigmoid-like%2520function%2520for%2520robust%2520training%2520in%2520the%2520initial%2520phase.%2520TVLARS%2520promotes%250Agradient%2520exploration%2520early%2520on%252C%2520surpassing%2520sharp%2520optimizers%2520and%2520gradually%250Atransitioning%2520to%2520LARS%2520for%2520robustness%2520in%2520later%2520phases.%2520Extensive%2520experiments%250Ademonstrate%2520that%2520TVLARS%2520consistently%2520outperforms%2520LARS%2520and%2520LAMB%2520in%2520most%2520cases%252C%250Awith%2520up%2520to%25202%255C%2525%2520improvement%2520in%2520classification%2520scenarios.%2520Notably%252C%2520in%2520all%250Aself-supervised%2520learning%2520cases%252C%2520TVLARS%2520dominates%2520LARS%2520and%2520LAMB%2520with%2520performance%250Aimprovements%2520of%2520up%2520to%252010%255C%2525.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2309.14053v5%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Revisiting%20LARS%20for%20Large%20Batch%20Training%20Generalization%20of%20Neural%0A%20%20Networks&entry.906535625=Khoi%20Do%20and%20Duong%20Nguyen%20and%20Hoa%20Nguyen%20and%20Long%20Tran-Thanh%20and%20Nguyen-Hoang%20Tran%20and%20Quoc-Viet%20Pham&entry.1292438233=%20%20This%20paper%20explores%20Large%20Batch%20Training%20techniques%20using%20layer-wise%20adaptive%0Ascaling%20ratio%20%28LARS%29%20across%20diverse%20settings%2C%20uncovering%20insights.%20LARS%0Aalgorithms%20with%20warm-up%20tend%20to%20be%20trapped%20in%20sharp%20minimizers%20early%20on%20due%20to%0Aredundant%20ratio%20scaling.%20Additionally%2C%20a%20fixed%20steep%20decline%20in%20the%20latter%0Aphase%20restricts%20deep%20neural%20networks%20from%20effectively%20navigating%20early-phase%0Asharp%20minimizers.%20Building%20on%20these%20findings%2C%20we%20propose%20Time%20Varying%20LARS%0A%28TVLARS%29%2C%20a%20novel%20algorithm%20that%20replaces%20warm-up%20with%20a%20configurable%0Asigmoid-like%20function%20for%20robust%20training%20in%20the%20initial%20phase.%20TVLARS%20promotes%0Agradient%20exploration%20early%20on%2C%20surpassing%20sharp%20optimizers%20and%20gradually%0Atransitioning%20to%20LARS%20for%20robustness%20in%20later%20phases.%20Extensive%20experiments%0Ademonstrate%20that%20TVLARS%20consistently%20outperforms%20LARS%20and%20LAMB%20in%20most%20cases%2C%0Awith%20up%20to%202%5C%25%20improvement%20in%20classification%20scenarios.%20Notably%2C%20in%20all%0Aself-supervised%20learning%20cases%2C%20TVLARS%20dominates%20LARS%20and%20LAMB%20with%20performance%0Aimprovements%20of%20up%20to%2010%5C%25.%0A&entry.1838667208=http%3A//arxiv.org/abs/2309.14053v5&entry.124074799=Read"},
{"title": "CLIP-AGIQA: Boosting the Performance of AI-Generated Image Quality\n  Assessment with CLIP", "author": "Zhenchen Tang and Zichuan Wang and Bo Peng and Jing Dong", "abstract": "  With the rapid development of generative technologies, AI-Generated Images\n(AIGIs) have been widely applied in various aspects of daily life. However, due\nto the immaturity of the technology, the quality of the generated images\nvaries, so it is important to develop quality assessment techniques for the\ngenerated images. Although some models have been proposed to assess the quality\nof generated images, they are inadequate when faced with the ever-increasing\nand diverse categories of generated images. Consequently, the development of\nmore advanced and effective models for evaluating the quality of generated\nimages is urgently needed. Recent research has explored the significant\npotential of the visual language model CLIP in image quality assessment,\nfinding that it performs well in evaluating the quality of natural images.\nHowever, its application to generated images has not been thoroughly\ninvestigated. In this paper, we build on this idea and further explore the\npotential of CLIP in evaluating the quality of generated images. We design\nCLIP-AGIQA, a CLIP-based regression model for quality assessment of generated\nimages, leveraging rich visual and textual knowledge encapsulated in CLIP.\nParticularly, we implement multi-category learnable prompts to fully utilize\nthe textual knowledge in CLIP for quality assessment. Extensive experiments on\nseveral generated image quality assessment benchmarks, including AGIQA-3K and\nAIGCIQA2023, demonstrate that CLIP-AGIQA outperforms existing IQA models,\nachieving excellent results in evaluating the quality of generated images.\n", "link": "http://arxiv.org/abs/2408.15098v1", "date": "2024-08-27", "relevancy": 2.5216, "topK": [{"title": "Magic-Me: Identity-Specific Video Customized Diffusion", "link": "http://arxiv.org/abs/2402.09368v1", "similarity": 0.516}, {"title": "UGG: Unified Generative Grasping", "link": "https://arxiv.org/abs/2311.16917", "similarity": 0.4995}, {"title": "Tuning-Free Noise Rectification for High Fidelity Image-to-Video\n  Generation", "link": "http://arxiv.org/abs/2403.02827v1", "similarity": 0.4974}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20CLIP-AGIQA%3A%20Boosting%20the%20Performance%20of%20AI-Generated%20Image%20Quality%0A%20%20Assessment%20with%20CLIP&body=Title%3A%20CLIP-AGIQA%3A%20Boosting%20the%20Performance%20of%20AI-Generated%20Image%20Quality%0A%20%20Assessment%20with%20CLIP%0AAuthor%3A%20Zhenchen%20Tang%20and%20Zichuan%20Wang%20and%20Bo%20Peng%20and%20Jing%20Dong%0AAbstract%3A%20%20%20With%20the%20rapid%20development%20of%20generative%20technologies%2C%20AI-Generated%20Images%0A%28AIGIs%29%20have%20been%20widely%20applied%20in%20various%20aspects%20of%20daily%20life.%20However%2C%20due%0Ato%20the%20immaturity%20of%20the%20technology%2C%20the%20quality%20of%20the%20generated%20images%0Avaries%2C%20so%20it%20is%20important%20to%20develop%20quality%20assessment%20techniques%20for%20the%0Agenerated%20images.%20Although%20some%20models%20have%20been%20proposed%20to%20assess%20the%20quality%0Aof%20generated%20images%2C%20they%20are%20inadequate%20when%20faced%20with%20the%20ever-increasing%0Aand%20diverse%20categories%20of%20generated%20images.%20Consequently%2C%20the%20development%20of%0Amore%20advanced%20and%20effective%20models%20for%20evaluating%20the%20quality%20of%20generated%0Aimages%20is%20urgently%20needed.%20Recent%20research%20has%20explored%20the%20significant%0Apotential%20of%20the%20visual%20language%20model%20CLIP%20in%20image%20quality%20assessment%2C%0Afinding%20that%20it%20performs%20well%20in%20evaluating%20the%20quality%20of%20natural%20images.%0AHowever%2C%20its%20application%20to%20generated%20images%20has%20not%20been%20thoroughly%0Ainvestigated.%20In%20this%20paper%2C%20we%20build%20on%20this%20idea%20and%20further%20explore%20the%0Apotential%20of%20CLIP%20in%20evaluating%20the%20quality%20of%20generated%20images.%20We%20design%0ACLIP-AGIQA%2C%20a%20CLIP-based%20regression%20model%20for%20quality%20assessment%20of%20generated%0Aimages%2C%20leveraging%20rich%20visual%20and%20textual%20knowledge%20encapsulated%20in%20CLIP.%0AParticularly%2C%20we%20implement%20multi-category%20learnable%20prompts%20to%20fully%20utilize%0Athe%20textual%20knowledge%20in%20CLIP%20for%20quality%20assessment.%20Extensive%20experiments%20on%0Aseveral%20generated%20image%20quality%20assessment%20benchmarks%2C%20including%20AGIQA-3K%20and%0AAIGCIQA2023%2C%20demonstrate%20that%20CLIP-AGIQA%20outperforms%20existing%20IQA%20models%2C%0Aachieving%20excellent%20results%20in%20evaluating%20the%20quality%20of%20generated%20images.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2408.15098v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DCLIP-AGIQA%253A%2520Boosting%2520the%2520Performance%2520of%2520AI-Generated%2520Image%2520Quality%250A%2520%2520Assessment%2520with%2520CLIP%26entry.906535625%3DZhenchen%2520Tang%2520and%2520Zichuan%2520Wang%2520and%2520Bo%2520Peng%2520and%2520Jing%2520Dong%26entry.1292438233%3D%2520%2520With%2520the%2520rapid%2520development%2520of%2520generative%2520technologies%252C%2520AI-Generated%2520Images%250A%2528AIGIs%2529%2520have%2520been%2520widely%2520applied%2520in%2520various%2520aspects%2520of%2520daily%2520life.%2520However%252C%2520due%250Ato%2520the%2520immaturity%2520of%2520the%2520technology%252C%2520the%2520quality%2520of%2520the%2520generated%2520images%250Avaries%252C%2520so%2520it%2520is%2520important%2520to%2520develop%2520quality%2520assessment%2520techniques%2520for%2520the%250Agenerated%2520images.%2520Although%2520some%2520models%2520have%2520been%2520proposed%2520to%2520assess%2520the%2520quality%250Aof%2520generated%2520images%252C%2520they%2520are%2520inadequate%2520when%2520faced%2520with%2520the%2520ever-increasing%250Aand%2520diverse%2520categories%2520of%2520generated%2520images.%2520Consequently%252C%2520the%2520development%2520of%250Amore%2520advanced%2520and%2520effective%2520models%2520for%2520evaluating%2520the%2520quality%2520of%2520generated%250Aimages%2520is%2520urgently%2520needed.%2520Recent%2520research%2520has%2520explored%2520the%2520significant%250Apotential%2520of%2520the%2520visual%2520language%2520model%2520CLIP%2520in%2520image%2520quality%2520assessment%252C%250Afinding%2520that%2520it%2520performs%2520well%2520in%2520evaluating%2520the%2520quality%2520of%2520natural%2520images.%250AHowever%252C%2520its%2520application%2520to%2520generated%2520images%2520has%2520not%2520been%2520thoroughly%250Ainvestigated.%2520In%2520this%2520paper%252C%2520we%2520build%2520on%2520this%2520idea%2520and%2520further%2520explore%2520the%250Apotential%2520of%2520CLIP%2520in%2520evaluating%2520the%2520quality%2520of%2520generated%2520images.%2520We%2520design%250ACLIP-AGIQA%252C%2520a%2520CLIP-based%2520regression%2520model%2520for%2520quality%2520assessment%2520of%2520generated%250Aimages%252C%2520leveraging%2520rich%2520visual%2520and%2520textual%2520knowledge%2520encapsulated%2520in%2520CLIP.%250AParticularly%252C%2520we%2520implement%2520multi-category%2520learnable%2520prompts%2520to%2520fully%2520utilize%250Athe%2520textual%2520knowledge%2520in%2520CLIP%2520for%2520quality%2520assessment.%2520Extensive%2520experiments%2520on%250Aseveral%2520generated%2520image%2520quality%2520assessment%2520benchmarks%252C%2520including%2520AGIQA-3K%2520and%250AAIGCIQA2023%252C%2520demonstrate%2520that%2520CLIP-AGIQA%2520outperforms%2520existing%2520IQA%2520models%252C%250Aachieving%2520excellent%2520results%2520in%2520evaluating%2520the%2520quality%2520of%2520generated%2520images.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2408.15098v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=CLIP-AGIQA%3A%20Boosting%20the%20Performance%20of%20AI-Generated%20Image%20Quality%0A%20%20Assessment%20with%20CLIP&entry.906535625=Zhenchen%20Tang%20and%20Zichuan%20Wang%20and%20Bo%20Peng%20and%20Jing%20Dong&entry.1292438233=%20%20With%20the%20rapid%20development%20of%20generative%20technologies%2C%20AI-Generated%20Images%0A%28AIGIs%29%20have%20been%20widely%20applied%20in%20various%20aspects%20of%20daily%20life.%20However%2C%20due%0Ato%20the%20immaturity%20of%20the%20technology%2C%20the%20quality%20of%20the%20generated%20images%0Avaries%2C%20so%20it%20is%20important%20to%20develop%20quality%20assessment%20techniques%20for%20the%0Agenerated%20images.%20Although%20some%20models%20have%20been%20proposed%20to%20assess%20the%20quality%0Aof%20generated%20images%2C%20they%20are%20inadequate%20when%20faced%20with%20the%20ever-increasing%0Aand%20diverse%20categories%20of%20generated%20images.%20Consequently%2C%20the%20development%20of%0Amore%20advanced%20and%20effective%20models%20for%20evaluating%20the%20quality%20of%20generated%0Aimages%20is%20urgently%20needed.%20Recent%20research%20has%20explored%20the%20significant%0Apotential%20of%20the%20visual%20language%20model%20CLIP%20in%20image%20quality%20assessment%2C%0Afinding%20that%20it%20performs%20well%20in%20evaluating%20the%20quality%20of%20natural%20images.%0AHowever%2C%20its%20application%20to%20generated%20images%20has%20not%20been%20thoroughly%0Ainvestigated.%20In%20this%20paper%2C%20we%20build%20on%20this%20idea%20and%20further%20explore%20the%0Apotential%20of%20CLIP%20in%20evaluating%20the%20quality%20of%20generated%20images.%20We%20design%0ACLIP-AGIQA%2C%20a%20CLIP-based%20regression%20model%20for%20quality%20assessment%20of%20generated%0Aimages%2C%20leveraging%20rich%20visual%20and%20textual%20knowledge%20encapsulated%20in%20CLIP.%0AParticularly%2C%20we%20implement%20multi-category%20learnable%20prompts%20to%20fully%20utilize%0Athe%20textual%20knowledge%20in%20CLIP%20for%20quality%20assessment.%20Extensive%20experiments%20on%0Aseveral%20generated%20image%20quality%20assessment%20benchmarks%2C%20including%20AGIQA-3K%20and%0AAIGCIQA2023%2C%20demonstrate%20that%20CLIP-AGIQA%20outperforms%20existing%20IQA%20models%2C%0Aachieving%20excellent%20results%20in%20evaluating%20the%20quality%20of%20generated%20images.%0A&entry.1838667208=http%3A//arxiv.org/abs/2408.15098v1&entry.124074799=Read"},
{"title": "Evaluation of Local Planner-Based Stanley Control in Autonomous RC Car\n  Racing Series", "author": "M\u00e1t\u00e9 Fazekas and Zal\u00e1n Demeter and J\u00e1nos T\u00f3th and \u00c1rmin Bog\u00e1r-N\u00e9meth and Gergely B\u00e1ri", "abstract": "  This paper proposes a control technique for autonomous RC car racing. The\npresented method does not require any map-building phase beforehand since it\noperates only local path planning on the actual LiDAR point cloud. Racing\ncontrol algorithms must have the capability to be optimized to the actual track\nlayout for minimization of lap time. In the examined one, it is guaranteed with\nthe improvement of the Stanley controller with additive control components to\nstabilize the movement in both low and high-speed ranges, and with the\nintegration of an adaptive lookahead point to induce sharp and dynamic\ncornering for traveled distance reduction. The developed method is tested on a\n1/10-sized RC car, and the tuning procedure from a base solution to the optimal\nsetting in a real F1Tenth race is presented. Furthermore, the proposed method\nis evaluated with a comparison to a more simple reactive method, and in\nparallel to a more complex optimization-based technique that involves offline\nmap building the global optimal trajectory calculation. The performance of the\nproposed method compared to the latter, referring to the lap time, is that the\nproposed one has only 8% lower average speed. This demonstrates that with\nappropriate tuning, a local planning-based method can be comparable with a more\ncomplex optimization-based one. Thus, the performance gap is lower than 10%\nfrom the state-of-the-art method. Moreover, the proposed technique has\nsignificantly higher similarity to real scenarios, therefore the results can be\ninteresting in the context of automotive industry.\n", "link": "http://arxiv.org/abs/2408.15152v1", "date": "2024-08-27", "relevancy": 2.4801, "topK": [{"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.5014}, {"title": "Safe and efficient training of a control agent", "link": "https://patents.google.com/patent/US11709462B2/en", "similarity": 0.4955}, {"title": "Egocentric Computer Vision for Hands-Free Robotic Wheelchair Navigation", "link": "https://link.springer.com/article/10.1007/s10846-023-01807-4", "similarity": 0.4913}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Evaluation%20of%20Local%20Planner-Based%20Stanley%20Control%20in%20Autonomous%20RC%20Car%0A%20%20Racing%20Series&body=Title%3A%20Evaluation%20of%20Local%20Planner-Based%20Stanley%20Control%20in%20Autonomous%20RC%20Car%0A%20%20Racing%20Series%0AAuthor%3A%20M%C3%A1t%C3%A9%20Fazekas%20and%20Zal%C3%A1n%20Demeter%20and%20J%C3%A1nos%20T%C3%B3th%20and%20%C3%81rmin%20Bog%C3%A1r-N%C3%A9meth%20and%20Gergely%20B%C3%A1ri%0AAbstract%3A%20%20%20This%20paper%20proposes%20a%20control%20technique%20for%20autonomous%20RC%20car%20racing.%20The%0Apresented%20method%20does%20not%20require%20any%20map-building%20phase%20beforehand%20since%20it%0Aoperates%20only%20local%20path%20planning%20on%20the%20actual%20LiDAR%20point%20cloud.%20Racing%0Acontrol%20algorithms%20must%20have%20the%20capability%20to%20be%20optimized%20to%20the%20actual%20track%0Alayout%20for%20minimization%20of%20lap%20time.%20In%20the%20examined%20one%2C%20it%20is%20guaranteed%20with%0Athe%20improvement%20of%20the%20Stanley%20controller%20with%20additive%20control%20components%20to%0Astabilize%20the%20movement%20in%20both%20low%20and%20high-speed%20ranges%2C%20and%20with%20the%0Aintegration%20of%20an%20adaptive%20lookahead%20point%20to%20induce%20sharp%20and%20dynamic%0Acornering%20for%20traveled%20distance%20reduction.%20The%20developed%20method%20is%20tested%20on%20a%0A1/10-sized%20RC%20car%2C%20and%20the%20tuning%20procedure%20from%20a%20base%20solution%20to%20the%20optimal%0Asetting%20in%20a%20real%20F1Tenth%20race%20is%20presented.%20Furthermore%2C%20the%20proposed%20method%0Ais%20evaluated%20with%20a%20comparison%20to%20a%20more%20simple%20reactive%20method%2C%20and%20in%0Aparallel%20to%20a%20more%20complex%20optimization-based%20technique%20that%20involves%20offline%0Amap%20building%20the%20global%20optimal%20trajectory%20calculation.%20The%20performance%20of%20the%0Aproposed%20method%20compared%20to%20the%20latter%2C%20referring%20to%20the%20lap%20time%2C%20is%20that%20the%0Aproposed%20one%20has%20only%208%25%20lower%20average%20speed.%20This%20demonstrates%20that%20with%0Aappropriate%20tuning%2C%20a%20local%20planning-based%20method%20can%20be%20comparable%20with%20a%20more%0Acomplex%20optimization-based%20one.%20Thus%2C%20the%20performance%20gap%20is%20lower%20than%2010%25%0Afrom%20the%20state-of-the-art%20method.%20Moreover%2C%20the%20proposed%20technique%20has%0Asignificantly%20higher%20similarity%20to%20real%20scenarios%2C%20therefore%20the%20results%20can%20be%0Ainteresting%20in%20the%20context%20of%20automotive%20industry.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2408.15152v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DEvaluation%2520of%2520Local%2520Planner-Based%2520Stanley%2520Control%2520in%2520Autonomous%2520RC%2520Car%250A%2520%2520Racing%2520Series%26entry.906535625%3DM%25C3%25A1t%25C3%25A9%2520Fazekas%2520and%2520Zal%25C3%25A1n%2520Demeter%2520and%2520J%25C3%25A1nos%2520T%25C3%25B3th%2520and%2520%25C3%2581rmin%2520Bog%25C3%25A1r-N%25C3%25A9meth%2520and%2520Gergely%2520B%25C3%25A1ri%26entry.1292438233%3D%2520%2520This%2520paper%2520proposes%2520a%2520control%2520technique%2520for%2520autonomous%2520RC%2520car%2520racing.%2520The%250Apresented%2520method%2520does%2520not%2520require%2520any%2520map-building%2520phase%2520beforehand%2520since%2520it%250Aoperates%2520only%2520local%2520path%2520planning%2520on%2520the%2520actual%2520LiDAR%2520point%2520cloud.%2520Racing%250Acontrol%2520algorithms%2520must%2520have%2520the%2520capability%2520to%2520be%2520optimized%2520to%2520the%2520actual%2520track%250Alayout%2520for%2520minimization%2520of%2520lap%2520time.%2520In%2520the%2520examined%2520one%252C%2520it%2520is%2520guaranteed%2520with%250Athe%2520improvement%2520of%2520the%2520Stanley%2520controller%2520with%2520additive%2520control%2520components%2520to%250Astabilize%2520the%2520movement%2520in%2520both%2520low%2520and%2520high-speed%2520ranges%252C%2520and%2520with%2520the%250Aintegration%2520of%2520an%2520adaptive%2520lookahead%2520point%2520to%2520induce%2520sharp%2520and%2520dynamic%250Acornering%2520for%2520traveled%2520distance%2520reduction.%2520The%2520developed%2520method%2520is%2520tested%2520on%2520a%250A1/10-sized%2520RC%2520car%252C%2520and%2520the%2520tuning%2520procedure%2520from%2520a%2520base%2520solution%2520to%2520the%2520optimal%250Asetting%2520in%2520a%2520real%2520F1Tenth%2520race%2520is%2520presented.%2520Furthermore%252C%2520the%2520proposed%2520method%250Ais%2520evaluated%2520with%2520a%2520comparison%2520to%2520a%2520more%2520simple%2520reactive%2520method%252C%2520and%2520in%250Aparallel%2520to%2520a%2520more%2520complex%2520optimization-based%2520technique%2520that%2520involves%2520offline%250Amap%2520building%2520the%2520global%2520optimal%2520trajectory%2520calculation.%2520The%2520performance%2520of%2520the%250Aproposed%2520method%2520compared%2520to%2520the%2520latter%252C%2520referring%2520to%2520the%2520lap%2520time%252C%2520is%2520that%2520the%250Aproposed%2520one%2520has%2520only%25208%2525%2520lower%2520average%2520speed.%2520This%2520demonstrates%2520that%2520with%250Aappropriate%2520tuning%252C%2520a%2520local%2520planning-based%2520method%2520can%2520be%2520comparable%2520with%2520a%2520more%250Acomplex%2520optimization-based%2520one.%2520Thus%252C%2520the%2520performance%2520gap%2520is%2520lower%2520than%252010%2525%250Afrom%2520the%2520state-of-the-art%2520method.%2520Moreover%252C%2520the%2520proposed%2520technique%2520has%250Asignificantly%2520higher%2520similarity%2520to%2520real%2520scenarios%252C%2520therefore%2520the%2520results%2520can%2520be%250Ainteresting%2520in%2520the%2520context%2520of%2520automotive%2520industry.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2408.15152v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Evaluation%20of%20Local%20Planner-Based%20Stanley%20Control%20in%20Autonomous%20RC%20Car%0A%20%20Racing%20Series&entry.906535625=M%C3%A1t%C3%A9%20Fazekas%20and%20Zal%C3%A1n%20Demeter%20and%20J%C3%A1nos%20T%C3%B3th%20and%20%C3%81rmin%20Bog%C3%A1r-N%C3%A9meth%20and%20Gergely%20B%C3%A1ri&entry.1292438233=%20%20This%20paper%20proposes%20a%20control%20technique%20for%20autonomous%20RC%20car%20racing.%20The%0Apresented%20method%20does%20not%20require%20any%20map-building%20phase%20beforehand%20since%20it%0Aoperates%20only%20local%20path%20planning%20on%20the%20actual%20LiDAR%20point%20cloud.%20Racing%0Acontrol%20algorithms%20must%20have%20the%20capability%20to%20be%20optimized%20to%20the%20actual%20track%0Alayout%20for%20minimization%20of%20lap%20time.%20In%20the%20examined%20one%2C%20it%20is%20guaranteed%20with%0Athe%20improvement%20of%20the%20Stanley%20controller%20with%20additive%20control%20components%20to%0Astabilize%20the%20movement%20in%20both%20low%20and%20high-speed%20ranges%2C%20and%20with%20the%0Aintegration%20of%20an%20adaptive%20lookahead%20point%20to%20induce%20sharp%20and%20dynamic%0Acornering%20for%20traveled%20distance%20reduction.%20The%20developed%20method%20is%20tested%20on%20a%0A1/10-sized%20RC%20car%2C%20and%20the%20tuning%20procedure%20from%20a%20base%20solution%20to%20the%20optimal%0Asetting%20in%20a%20real%20F1Tenth%20race%20is%20presented.%20Furthermore%2C%20the%20proposed%20method%0Ais%20evaluated%20with%20a%20comparison%20to%20a%20more%20simple%20reactive%20method%2C%20and%20in%0Aparallel%20to%20a%20more%20complex%20optimization-based%20technique%20that%20involves%20offline%0Amap%20building%20the%20global%20optimal%20trajectory%20calculation.%20The%20performance%20of%20the%0Aproposed%20method%20compared%20to%20the%20latter%2C%20referring%20to%20the%20lap%20time%2C%20is%20that%20the%0Aproposed%20one%20has%20only%208%25%20lower%20average%20speed.%20This%20demonstrates%20that%20with%0Aappropriate%20tuning%2C%20a%20local%20planning-based%20method%20can%20be%20comparable%20with%20a%20more%0Acomplex%20optimization-based%20one.%20Thus%2C%20the%20performance%20gap%20is%20lower%20than%2010%25%0Afrom%20the%20state-of-the-art%20method.%20Moreover%2C%20the%20proposed%20technique%20has%0Asignificantly%20higher%20similarity%20to%20real%20scenarios%2C%20therefore%20the%20results%20can%20be%0Ainteresting%20in%20the%20context%20of%20automotive%20industry.%0A&entry.1838667208=http%3A//arxiv.org/abs/2408.15152v1&entry.124074799=Read"},
{"title": "SiHGNN: Leveraging Properties of Semantic Graphs for Efficient HGNN\n  Acceleration", "author": "Runzhen Xue and Mingyu Yan and Dengke Han and Zhimin Tang and Xiaochun Ye and Dongrui Fan", "abstract": "  Heterogeneous Graph Neural Networks (HGNNs) have expanded graph\nrepresentation learning to heterogeneous graph fields. Recent studies have\ndemonstrated their superior performance across various applications, including\nmedical analysis and recommendation systems, often surpassing existing methods.\nHowever, GPUs often experience inefficiencies when executing HGNNs due to their\nunique and complex execution patterns. Compared to traditional Graph Neural\nNetworks, these patterns further exacerbate irregularities in memory access. To\ntackle these challenges, recent studies have focused on developing\ndomain-specific accelerators for HGNNs. Nonetheless, most of these efforts have\nconcentrated on optimizing the datapath or scheduling data accesses, while\nlargely overlooking the potential benefits that could be gained from leveraging\nthe inherent properties of the semantic graph, such as its topology, layout,\nand generation.\n  In this work, we focus on leveraging the properties of semantic graphs to\nenhance HGNN performance. First, we analyze the Semantic Graph Build (SGB)\nstage and identify significant opportunities for data reuse during semantic\ngraph generation. Next, we uncover the phenomenon of buffer thrashing during\nthe Graph Feature Processing (GFP) stage, revealing potential optimization\nopportunities in semantic graph layout. Furthermore, we propose a lightweight\nhardware accelerator frontend for HGNNs, called SiHGNN. This accelerator\nfrontend incorporates a tree-based Semantic Graph Builder for efficient\nsemantic graph generation and features a novel Graph Restructurer for\noptimizing semantic graph layouts. Experimental results show that SiHGNN\nenables the state-of-the-art HGNN accelerator to achieve an average performance\nimprovement of 2.95$\\times$.\n", "link": "http://arxiv.org/abs/2408.15089v1", "date": "2024-08-27", "relevancy": 2.4239, "topK": [{"title": "Dynamic Gaussians Mesh: Consistent Mesh Reconstruction from Monocular\n  Videos", "link": "http://arxiv.org/abs/2404.12379v2", "similarity": 0.4961}, {"title": "RTG-SLAM: Real-time 3D Reconstruction at Scale using Gaussian Splatting", "link": "http://arxiv.org/abs/2404.19706v1", "similarity": 0.4873}, {"title": "Boosted dynamic neural networks", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/26302", "similarity": 0.4709}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20SiHGNN%3A%20Leveraging%20Properties%20of%20Semantic%20Graphs%20for%20Efficient%20HGNN%0A%20%20Acceleration&body=Title%3A%20SiHGNN%3A%20Leveraging%20Properties%20of%20Semantic%20Graphs%20for%20Efficient%20HGNN%0A%20%20Acceleration%0AAuthor%3A%20Runzhen%20Xue%20and%20Mingyu%20Yan%20and%20Dengke%20Han%20and%20Zhimin%20Tang%20and%20Xiaochun%20Ye%20and%20Dongrui%20Fan%0AAbstract%3A%20%20%20Heterogeneous%20Graph%20Neural%20Networks%20%28HGNNs%29%20have%20expanded%20graph%0Arepresentation%20learning%20to%20heterogeneous%20graph%20fields.%20Recent%20studies%20have%0Ademonstrated%20their%20superior%20performance%20across%20various%20applications%2C%20including%0Amedical%20analysis%20and%20recommendation%20systems%2C%20often%20surpassing%20existing%20methods.%0AHowever%2C%20GPUs%20often%20experience%20inefficiencies%20when%20executing%20HGNNs%20due%20to%20their%0Aunique%20and%20complex%20execution%20patterns.%20Compared%20to%20traditional%20Graph%20Neural%0ANetworks%2C%20these%20patterns%20further%20exacerbate%20irregularities%20in%20memory%20access.%20To%0Atackle%20these%20challenges%2C%20recent%20studies%20have%20focused%20on%20developing%0Adomain-specific%20accelerators%20for%20HGNNs.%20Nonetheless%2C%20most%20of%20these%20efforts%20have%0Aconcentrated%20on%20optimizing%20the%20datapath%20or%20scheduling%20data%20accesses%2C%20while%0Alargely%20overlooking%20the%20potential%20benefits%20that%20could%20be%20gained%20from%20leveraging%0Athe%20inherent%20properties%20of%20the%20semantic%20graph%2C%20such%20as%20its%20topology%2C%20layout%2C%0Aand%20generation.%0A%20%20In%20this%20work%2C%20we%20focus%20on%20leveraging%20the%20properties%20of%20semantic%20graphs%20to%0Aenhance%20HGNN%20performance.%20First%2C%20we%20analyze%20the%20Semantic%20Graph%20Build%20%28SGB%29%0Astage%20and%20identify%20significant%20opportunities%20for%20data%20reuse%20during%20semantic%0Agraph%20generation.%20Next%2C%20we%20uncover%20the%20phenomenon%20of%20buffer%20thrashing%20during%0Athe%20Graph%20Feature%20Processing%20%28GFP%29%20stage%2C%20revealing%20potential%20optimization%0Aopportunities%20in%20semantic%20graph%20layout.%20Furthermore%2C%20we%20propose%20a%20lightweight%0Ahardware%20accelerator%20frontend%20for%20HGNNs%2C%20called%20SiHGNN.%20This%20accelerator%0Afrontend%20incorporates%20a%20tree-based%20Semantic%20Graph%20Builder%20for%20efficient%0Asemantic%20graph%20generation%20and%20features%20a%20novel%20Graph%20Restructurer%20for%0Aoptimizing%20semantic%20graph%20layouts.%20Experimental%20results%20show%20that%20SiHGNN%0Aenables%20the%20state-of-the-art%20HGNN%20accelerator%20to%20achieve%20an%20average%20performance%0Aimprovement%20of%202.95%24%5Ctimes%24.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2408.15089v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DSiHGNN%253A%2520Leveraging%2520Properties%2520of%2520Semantic%2520Graphs%2520for%2520Efficient%2520HGNN%250A%2520%2520Acceleration%26entry.906535625%3DRunzhen%2520Xue%2520and%2520Mingyu%2520Yan%2520and%2520Dengke%2520Han%2520and%2520Zhimin%2520Tang%2520and%2520Xiaochun%2520Ye%2520and%2520Dongrui%2520Fan%26entry.1292438233%3D%2520%2520Heterogeneous%2520Graph%2520Neural%2520Networks%2520%2528HGNNs%2529%2520have%2520expanded%2520graph%250Arepresentation%2520learning%2520to%2520heterogeneous%2520graph%2520fields.%2520Recent%2520studies%2520have%250Ademonstrated%2520their%2520superior%2520performance%2520across%2520various%2520applications%252C%2520including%250Amedical%2520analysis%2520and%2520recommendation%2520systems%252C%2520often%2520surpassing%2520existing%2520methods.%250AHowever%252C%2520GPUs%2520often%2520experience%2520inefficiencies%2520when%2520executing%2520HGNNs%2520due%2520to%2520their%250Aunique%2520and%2520complex%2520execution%2520patterns.%2520Compared%2520to%2520traditional%2520Graph%2520Neural%250ANetworks%252C%2520these%2520patterns%2520further%2520exacerbate%2520irregularities%2520in%2520memory%2520access.%2520To%250Atackle%2520these%2520challenges%252C%2520recent%2520studies%2520have%2520focused%2520on%2520developing%250Adomain-specific%2520accelerators%2520for%2520HGNNs.%2520Nonetheless%252C%2520most%2520of%2520these%2520efforts%2520have%250Aconcentrated%2520on%2520optimizing%2520the%2520datapath%2520or%2520scheduling%2520data%2520accesses%252C%2520while%250Alargely%2520overlooking%2520the%2520potential%2520benefits%2520that%2520could%2520be%2520gained%2520from%2520leveraging%250Athe%2520inherent%2520properties%2520of%2520the%2520semantic%2520graph%252C%2520such%2520as%2520its%2520topology%252C%2520layout%252C%250Aand%2520generation.%250A%2520%2520In%2520this%2520work%252C%2520we%2520focus%2520on%2520leveraging%2520the%2520properties%2520of%2520semantic%2520graphs%2520to%250Aenhance%2520HGNN%2520performance.%2520First%252C%2520we%2520analyze%2520the%2520Semantic%2520Graph%2520Build%2520%2528SGB%2529%250Astage%2520and%2520identify%2520significant%2520opportunities%2520for%2520data%2520reuse%2520during%2520semantic%250Agraph%2520generation.%2520Next%252C%2520we%2520uncover%2520the%2520phenomenon%2520of%2520buffer%2520thrashing%2520during%250Athe%2520Graph%2520Feature%2520Processing%2520%2528GFP%2529%2520stage%252C%2520revealing%2520potential%2520optimization%250Aopportunities%2520in%2520semantic%2520graph%2520layout.%2520Furthermore%252C%2520we%2520propose%2520a%2520lightweight%250Ahardware%2520accelerator%2520frontend%2520for%2520HGNNs%252C%2520called%2520SiHGNN.%2520This%2520accelerator%250Afrontend%2520incorporates%2520a%2520tree-based%2520Semantic%2520Graph%2520Builder%2520for%2520efficient%250Asemantic%2520graph%2520generation%2520and%2520features%2520a%2520novel%2520Graph%2520Restructurer%2520for%250Aoptimizing%2520semantic%2520graph%2520layouts.%2520Experimental%2520results%2520show%2520that%2520SiHGNN%250Aenables%2520the%2520state-of-the-art%2520HGNN%2520accelerator%2520to%2520achieve%2520an%2520average%2520performance%250Aimprovement%2520of%25202.95%2524%255Ctimes%2524.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2408.15089v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=SiHGNN%3A%20Leveraging%20Properties%20of%20Semantic%20Graphs%20for%20Efficient%20HGNN%0A%20%20Acceleration&entry.906535625=Runzhen%20Xue%20and%20Mingyu%20Yan%20and%20Dengke%20Han%20and%20Zhimin%20Tang%20and%20Xiaochun%20Ye%20and%20Dongrui%20Fan&entry.1292438233=%20%20Heterogeneous%20Graph%20Neural%20Networks%20%28HGNNs%29%20have%20expanded%20graph%0Arepresentation%20learning%20to%20heterogeneous%20graph%20fields.%20Recent%20studies%20have%0Ademonstrated%20their%20superior%20performance%20across%20various%20applications%2C%20including%0Amedical%20analysis%20and%20recommendation%20systems%2C%20often%20surpassing%20existing%20methods.%0AHowever%2C%20GPUs%20often%20experience%20inefficiencies%20when%20executing%20HGNNs%20due%20to%20their%0Aunique%20and%20complex%20execution%20patterns.%20Compared%20to%20traditional%20Graph%20Neural%0ANetworks%2C%20these%20patterns%20further%20exacerbate%20irregularities%20in%20memory%20access.%20To%0Atackle%20these%20challenges%2C%20recent%20studies%20have%20focused%20on%20developing%0Adomain-specific%20accelerators%20for%20HGNNs.%20Nonetheless%2C%20most%20of%20these%20efforts%20have%0Aconcentrated%20on%20optimizing%20the%20datapath%20or%20scheduling%20data%20accesses%2C%20while%0Alargely%20overlooking%20the%20potential%20benefits%20that%20could%20be%20gained%20from%20leveraging%0Athe%20inherent%20properties%20of%20the%20semantic%20graph%2C%20such%20as%20its%20topology%2C%20layout%2C%0Aand%20generation.%0A%20%20In%20this%20work%2C%20we%20focus%20on%20leveraging%20the%20properties%20of%20semantic%20graphs%20to%0Aenhance%20HGNN%20performance.%20First%2C%20we%20analyze%20the%20Semantic%20Graph%20Build%20%28SGB%29%0Astage%20and%20identify%20significant%20opportunities%20for%20data%20reuse%20during%20semantic%0Agraph%20generation.%20Next%2C%20we%20uncover%20the%20phenomenon%20of%20buffer%20thrashing%20during%0Athe%20Graph%20Feature%20Processing%20%28GFP%29%20stage%2C%20revealing%20potential%20optimization%0Aopportunities%20in%20semantic%20graph%20layout.%20Furthermore%2C%20we%20propose%20a%20lightweight%0Ahardware%20accelerator%20frontend%20for%20HGNNs%2C%20called%20SiHGNN.%20This%20accelerator%0Afrontend%20incorporates%20a%20tree-based%20Semantic%20Graph%20Builder%20for%20efficient%0Asemantic%20graph%20generation%20and%20features%20a%20novel%20Graph%20Restructurer%20for%0Aoptimizing%20semantic%20graph%20layouts.%20Experimental%20results%20show%20that%20SiHGNN%0Aenables%20the%20state-of-the-art%20HGNN%20accelerator%20to%20achieve%20an%20average%20performance%0Aimprovement%20of%202.95%24%5Ctimes%24.%0A&entry.1838667208=http%3A//arxiv.org/abs/2408.15089v1&entry.124074799=Read"},
{"title": "MegActor-$\u03a3$: Unlocking Flexible Mixed-Modal Control in Portrait\n  Animation with Diffusion Transformer", "author": "Shurong Yang and Huadong Li and Juhao Wu and Minhao Jing and Linze Li and Renhe Ji and Jiajun Liang and Haoqiang Fan and Jin Wang", "abstract": "  Diffusion models have demonstrated superior performance in the field of\nportrait animation. However, current approaches relied on either visual or\naudio modality to control character movements, failing to exploit the potential\nof mixed-modal control. This challenge arises from the difficulty in balancing\nthe weak control strength of audio modality and the strong control strength of\nvisual modality. To address this issue, we introduce MegActor-$\\Sigma$: a\nmixed-modal conditional diffusion transformer (DiT), which can flexibly inject\naudio and visual modality control signals into portrait animation.\nSpecifically, we make substantial advancements over its predecessor, MegActor,\nby leveraging the promising model structure of DiT and integrating audio and\nvisual conditions through advanced modules within the DiT framework. To further\nachieve flexible combinations of mixed-modal control signals, we propose a\n``Modality Decoupling Control\" training strategy to balance the control\nstrength between visual and audio modalities, along with the ``Amplitude\nAdjustment\" inference strategy to freely regulate the motion amplitude of each\nmodality. Finally, to facilitate extensive studies in this field, we design\nseveral dataset evaluation metrics to filter out public datasets and solely use\nthis filtered dataset to train MegActor-$\\Sigma$. Extensive experiments\ndemonstrate the superiority of our approach in generating vivid portrait\nanimations, outperforming previous methods trained on private dataset.\n", "link": "http://arxiv.org/abs/2408.14975v1", "date": "2024-08-27", "relevancy": 2.4199, "topK": [{"title": "Magic-Me: Identity-Specific Video Customized Diffusion", "link": "http://arxiv.org/abs/2402.09368v1", "similarity": 0.6107}, {"title": "PoseAnimate: Zero-shot high fidelity pose controllable character\n  animation", "link": "http://arxiv.org/abs/2404.13680v2", "similarity": 0.6042}, {"title": "Multimodal-Conditioned Latent Diffusion Models for Fashion Image Editing", "link": "http://arxiv.org/abs/2403.14828v2", "similarity": 0.5925}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20MegActor-%24%CE%A3%24%3A%20Unlocking%20Flexible%20Mixed-Modal%20Control%20in%20Portrait%0A%20%20Animation%20with%20Diffusion%20Transformer&body=Title%3A%20MegActor-%24%CE%A3%24%3A%20Unlocking%20Flexible%20Mixed-Modal%20Control%20in%20Portrait%0A%20%20Animation%20with%20Diffusion%20Transformer%0AAuthor%3A%20Shurong%20Yang%20and%20Huadong%20Li%20and%20Juhao%20Wu%20and%20Minhao%20Jing%20and%20Linze%20Li%20and%20Renhe%20Ji%20and%20Jiajun%20Liang%20and%20Haoqiang%20Fan%20and%20Jin%20Wang%0AAbstract%3A%20%20%20Diffusion%20models%20have%20demonstrated%20superior%20performance%20in%20the%20field%20of%0Aportrait%20animation.%20However%2C%20current%20approaches%20relied%20on%20either%20visual%20or%0Aaudio%20modality%20to%20control%20character%20movements%2C%20failing%20to%20exploit%20the%20potential%0Aof%20mixed-modal%20control.%20This%20challenge%20arises%20from%20the%20difficulty%20in%20balancing%0Athe%20weak%20control%20strength%20of%20audio%20modality%20and%20the%20strong%20control%20strength%20of%0Avisual%20modality.%20To%20address%20this%20issue%2C%20we%20introduce%20MegActor-%24%5CSigma%24%3A%20a%0Amixed-modal%20conditional%20diffusion%20transformer%20%28DiT%29%2C%20which%20can%20flexibly%20inject%0Aaudio%20and%20visual%20modality%20control%20signals%20into%20portrait%20animation.%0ASpecifically%2C%20we%20make%20substantial%20advancements%20over%20its%20predecessor%2C%20MegActor%2C%0Aby%20leveraging%20the%20promising%20model%20structure%20of%20DiT%20and%20integrating%20audio%20and%0Avisual%20conditions%20through%20advanced%20modules%20within%20the%20DiT%20framework.%20To%20further%0Aachieve%20flexible%20combinations%20of%20mixed-modal%20control%20signals%2C%20we%20propose%20a%0A%60%60Modality%20Decoupling%20Control%22%20training%20strategy%20to%20balance%20the%20control%0Astrength%20between%20visual%20and%20audio%20modalities%2C%20along%20with%20the%20%60%60Amplitude%0AAdjustment%22%20inference%20strategy%20to%20freely%20regulate%20the%20motion%20amplitude%20of%20each%0Amodality.%20Finally%2C%20to%20facilitate%20extensive%20studies%20in%20this%20field%2C%20we%20design%0Aseveral%20dataset%20evaluation%20metrics%20to%20filter%20out%20public%20datasets%20and%20solely%20use%0Athis%20filtered%20dataset%20to%20train%20MegActor-%24%5CSigma%24.%20Extensive%20experiments%0Ademonstrate%20the%20superiority%20of%20our%20approach%20in%20generating%20vivid%20portrait%0Aanimations%2C%20outperforming%20previous%20methods%20trained%20on%20private%20dataset.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2408.14975v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DMegActor-%2524%25CE%25A3%2524%253A%2520Unlocking%2520Flexible%2520Mixed-Modal%2520Control%2520in%2520Portrait%250A%2520%2520Animation%2520with%2520Diffusion%2520Transformer%26entry.906535625%3DShurong%2520Yang%2520and%2520Huadong%2520Li%2520and%2520Juhao%2520Wu%2520and%2520Minhao%2520Jing%2520and%2520Linze%2520Li%2520and%2520Renhe%2520Ji%2520and%2520Jiajun%2520Liang%2520and%2520Haoqiang%2520Fan%2520and%2520Jin%2520Wang%26entry.1292438233%3D%2520%2520Diffusion%2520models%2520have%2520demonstrated%2520superior%2520performance%2520in%2520the%2520field%2520of%250Aportrait%2520animation.%2520However%252C%2520current%2520approaches%2520relied%2520on%2520either%2520visual%2520or%250Aaudio%2520modality%2520to%2520control%2520character%2520movements%252C%2520failing%2520to%2520exploit%2520the%2520potential%250Aof%2520mixed-modal%2520control.%2520This%2520challenge%2520arises%2520from%2520the%2520difficulty%2520in%2520balancing%250Athe%2520weak%2520control%2520strength%2520of%2520audio%2520modality%2520and%2520the%2520strong%2520control%2520strength%2520of%250Avisual%2520modality.%2520To%2520address%2520this%2520issue%252C%2520we%2520introduce%2520MegActor-%2524%255CSigma%2524%253A%2520a%250Amixed-modal%2520conditional%2520diffusion%2520transformer%2520%2528DiT%2529%252C%2520which%2520can%2520flexibly%2520inject%250Aaudio%2520and%2520visual%2520modality%2520control%2520signals%2520into%2520portrait%2520animation.%250ASpecifically%252C%2520we%2520make%2520substantial%2520advancements%2520over%2520its%2520predecessor%252C%2520MegActor%252C%250Aby%2520leveraging%2520the%2520promising%2520model%2520structure%2520of%2520DiT%2520and%2520integrating%2520audio%2520and%250Avisual%2520conditions%2520through%2520advanced%2520modules%2520within%2520the%2520DiT%2520framework.%2520To%2520further%250Aachieve%2520flexible%2520combinations%2520of%2520mixed-modal%2520control%2520signals%252C%2520we%2520propose%2520a%250A%2560%2560Modality%2520Decoupling%2520Control%2522%2520training%2520strategy%2520to%2520balance%2520the%2520control%250Astrength%2520between%2520visual%2520and%2520audio%2520modalities%252C%2520along%2520with%2520the%2520%2560%2560Amplitude%250AAdjustment%2522%2520inference%2520strategy%2520to%2520freely%2520regulate%2520the%2520motion%2520amplitude%2520of%2520each%250Amodality.%2520Finally%252C%2520to%2520facilitate%2520extensive%2520studies%2520in%2520this%2520field%252C%2520we%2520design%250Aseveral%2520dataset%2520evaluation%2520metrics%2520to%2520filter%2520out%2520public%2520datasets%2520and%2520solely%2520use%250Athis%2520filtered%2520dataset%2520to%2520train%2520MegActor-%2524%255CSigma%2524.%2520Extensive%2520experiments%250Ademonstrate%2520the%2520superiority%2520of%2520our%2520approach%2520in%2520generating%2520vivid%2520portrait%250Aanimations%252C%2520outperforming%2520previous%2520methods%2520trained%2520on%2520private%2520dataset.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2408.14975v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=MegActor-%24%CE%A3%24%3A%20Unlocking%20Flexible%20Mixed-Modal%20Control%20in%20Portrait%0A%20%20Animation%20with%20Diffusion%20Transformer&entry.906535625=Shurong%20Yang%20and%20Huadong%20Li%20and%20Juhao%20Wu%20and%20Minhao%20Jing%20and%20Linze%20Li%20and%20Renhe%20Ji%20and%20Jiajun%20Liang%20and%20Haoqiang%20Fan%20and%20Jin%20Wang&entry.1292438233=%20%20Diffusion%20models%20have%20demonstrated%20superior%20performance%20in%20the%20field%20of%0Aportrait%20animation.%20However%2C%20current%20approaches%20relied%20on%20either%20visual%20or%0Aaudio%20modality%20to%20control%20character%20movements%2C%20failing%20to%20exploit%20the%20potential%0Aof%20mixed-modal%20control.%20This%20challenge%20arises%20from%20the%20difficulty%20in%20balancing%0Athe%20weak%20control%20strength%20of%20audio%20modality%20and%20the%20strong%20control%20strength%20of%0Avisual%20modality.%20To%20address%20this%20issue%2C%20we%20introduce%20MegActor-%24%5CSigma%24%3A%20a%0Amixed-modal%20conditional%20diffusion%20transformer%20%28DiT%29%2C%20which%20can%20flexibly%20inject%0Aaudio%20and%20visual%20modality%20control%20signals%20into%20portrait%20animation.%0ASpecifically%2C%20we%20make%20substantial%20advancements%20over%20its%20predecessor%2C%20MegActor%2C%0Aby%20leveraging%20the%20promising%20model%20structure%20of%20DiT%20and%20integrating%20audio%20and%0Avisual%20conditions%20through%20advanced%20modules%20within%20the%20DiT%20framework.%20To%20further%0Aachieve%20flexible%20combinations%20of%20mixed-modal%20control%20signals%2C%20we%20propose%20a%0A%60%60Modality%20Decoupling%20Control%22%20training%20strategy%20to%20balance%20the%20control%0Astrength%20between%20visual%20and%20audio%20modalities%2C%20along%20with%20the%20%60%60Amplitude%0AAdjustment%22%20inference%20strategy%20to%20freely%20regulate%20the%20motion%20amplitude%20of%20each%0Amodality.%20Finally%2C%20to%20facilitate%20extensive%20studies%20in%20this%20field%2C%20we%20design%0Aseveral%20dataset%20evaluation%20metrics%20to%20filter%20out%20public%20datasets%20and%20solely%20use%0Athis%20filtered%20dataset%20to%20train%20MegActor-%24%5CSigma%24.%20Extensive%20experiments%0Ademonstrate%20the%20superiority%20of%20our%20approach%20in%20generating%20vivid%20portrait%0Aanimations%2C%20outperforming%20previous%20methods%20trained%20on%20private%20dataset.%0A&entry.1838667208=http%3A//arxiv.org/abs/2408.14975v1&entry.124074799=Read"},
{"title": "Say No to Freeloader: Protecting Intellectual Property of Your Deep\n  Model", "author": "Lianyu Wang and Meng Wang and Huazhu Fu and Daoqiang Zhang", "abstract": "  Model intellectual property (IP) protection has attracted growing attention\nas science and technology advancements stem from human intellectual labor and\ncomputational expenses. Ensuring IP safety for trainers and owners is of utmost\nimportance, particularly in domains where ownership verification and\napplicability authorization are required. A notable approach to safeguarding\nmodel IP involves proactively preventing the use of well-trained models of\nauthorized domains from unauthorized domains. In this paper, we introduce a\nnovel Compact Un-transferable Pyramid Isolation Domain (CUPI-Domain) which\nserves as a barrier against illegal transfers from authorized to unauthorized\ndomains. Drawing inspiration from human transitive inference and learning\nabilities, the CUPI-Domain is designed to obstruct cross-domain transfers by\nemphasizing the distinctive style features of the authorized domain. This\nemphasis leads to failure in recognizing irrelevant private style features on\nunauthorized domains. To this end, we propose novel CUPI-Domain generators,\nwhich select features from both authorized and CUPI-Domain as anchors. Then, we\nfuse the style features and semantic features of these anchors to generate\nlabeled and style-rich CUPI-Domain. Additionally, we design external\nDomain-Information Memory Banks (DIMB) for storing and updating labeled pyramid\nfeatures to obtain stable domain class features and domain class-wise style\nfeatures. Based on the proposed whole method, the novel style and\ndiscriminative loss functions are designed to effectively enhance the\ndistinction in style and discriminative features between authorized and\nunauthorized domains, respectively. Moreover, we provide two solutions for\nutilizing CUPI-Domain based on whether the unauthorized domain is known:\ntarget-specified CUPI-Domain and target-free CUPI-Domain.\n", "link": "http://arxiv.org/abs/2408.13161v2", "date": "2024-08-27", "relevancy": 2.3967, "topK": [{"title": "Self-supervised Photographic Image Layout Representation Learning", "link": "http://arxiv.org/abs/2403.03740v1", "similarity": 0.5009}, {"title": "Magic-Me: Identity-Specific Video Customized Diffusion", "link": "http://arxiv.org/abs/2402.09368v1", "similarity": 0.4731}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.4641}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Say%20No%20to%20Freeloader%3A%20Protecting%20Intellectual%20Property%20of%20Your%20Deep%0A%20%20Model&body=Title%3A%20Say%20No%20to%20Freeloader%3A%20Protecting%20Intellectual%20Property%20of%20Your%20Deep%0A%20%20Model%0AAuthor%3A%20Lianyu%20Wang%20and%20Meng%20Wang%20and%20Huazhu%20Fu%20and%20Daoqiang%20Zhang%0AAbstract%3A%20%20%20Model%20intellectual%20property%20%28IP%29%20protection%20has%20attracted%20growing%20attention%0Aas%20science%20and%20technology%20advancements%20stem%20from%20human%20intellectual%20labor%20and%0Acomputational%20expenses.%20Ensuring%20IP%20safety%20for%20trainers%20and%20owners%20is%20of%20utmost%0Aimportance%2C%20particularly%20in%20domains%20where%20ownership%20verification%20and%0Aapplicability%20authorization%20are%20required.%20A%20notable%20approach%20to%20safeguarding%0Amodel%20IP%20involves%20proactively%20preventing%20the%20use%20of%20well-trained%20models%20of%0Aauthorized%20domains%20from%20unauthorized%20domains.%20In%20this%20paper%2C%20we%20introduce%20a%0Anovel%20Compact%20Un-transferable%20Pyramid%20Isolation%20Domain%20%28CUPI-Domain%29%20which%0Aserves%20as%20a%20barrier%20against%20illegal%20transfers%20from%20authorized%20to%20unauthorized%0Adomains.%20Drawing%20inspiration%20from%20human%20transitive%20inference%20and%20learning%0Aabilities%2C%20the%20CUPI-Domain%20is%20designed%20to%20obstruct%20cross-domain%20transfers%20by%0Aemphasizing%20the%20distinctive%20style%20features%20of%20the%20authorized%20domain.%20This%0Aemphasis%20leads%20to%20failure%20in%20recognizing%20irrelevant%20private%20style%20features%20on%0Aunauthorized%20domains.%20To%20this%20end%2C%20we%20propose%20novel%20CUPI-Domain%20generators%2C%0Awhich%20select%20features%20from%20both%20authorized%20and%20CUPI-Domain%20as%20anchors.%20Then%2C%20we%0Afuse%20the%20style%20features%20and%20semantic%20features%20of%20these%20anchors%20to%20generate%0Alabeled%20and%20style-rich%20CUPI-Domain.%20Additionally%2C%20we%20design%20external%0ADomain-Information%20Memory%20Banks%20%28DIMB%29%20for%20storing%20and%20updating%20labeled%20pyramid%0Afeatures%20to%20obtain%20stable%20domain%20class%20features%20and%20domain%20class-wise%20style%0Afeatures.%20Based%20on%20the%20proposed%20whole%20method%2C%20the%20novel%20style%20and%0Adiscriminative%20loss%20functions%20are%20designed%20to%20effectively%20enhance%20the%0Adistinction%20in%20style%20and%20discriminative%20features%20between%20authorized%20and%0Aunauthorized%20domains%2C%20respectively.%20Moreover%2C%20we%20provide%20two%20solutions%20for%0Autilizing%20CUPI-Domain%20based%20on%20whether%20the%20unauthorized%20domain%20is%20known%3A%0Atarget-specified%20CUPI-Domain%20and%20target-free%20CUPI-Domain.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2408.13161v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DSay%2520No%2520to%2520Freeloader%253A%2520Protecting%2520Intellectual%2520Property%2520of%2520Your%2520Deep%250A%2520%2520Model%26entry.906535625%3DLianyu%2520Wang%2520and%2520Meng%2520Wang%2520and%2520Huazhu%2520Fu%2520and%2520Daoqiang%2520Zhang%26entry.1292438233%3D%2520%2520Model%2520intellectual%2520property%2520%2528IP%2529%2520protection%2520has%2520attracted%2520growing%2520attention%250Aas%2520science%2520and%2520technology%2520advancements%2520stem%2520from%2520human%2520intellectual%2520labor%2520and%250Acomputational%2520expenses.%2520Ensuring%2520IP%2520safety%2520for%2520trainers%2520and%2520owners%2520is%2520of%2520utmost%250Aimportance%252C%2520particularly%2520in%2520domains%2520where%2520ownership%2520verification%2520and%250Aapplicability%2520authorization%2520are%2520required.%2520A%2520notable%2520approach%2520to%2520safeguarding%250Amodel%2520IP%2520involves%2520proactively%2520preventing%2520the%2520use%2520of%2520well-trained%2520models%2520of%250Aauthorized%2520domains%2520from%2520unauthorized%2520domains.%2520In%2520this%2520paper%252C%2520we%2520introduce%2520a%250Anovel%2520Compact%2520Un-transferable%2520Pyramid%2520Isolation%2520Domain%2520%2528CUPI-Domain%2529%2520which%250Aserves%2520as%2520a%2520barrier%2520against%2520illegal%2520transfers%2520from%2520authorized%2520to%2520unauthorized%250Adomains.%2520Drawing%2520inspiration%2520from%2520human%2520transitive%2520inference%2520and%2520learning%250Aabilities%252C%2520the%2520CUPI-Domain%2520is%2520designed%2520to%2520obstruct%2520cross-domain%2520transfers%2520by%250Aemphasizing%2520the%2520distinctive%2520style%2520features%2520of%2520the%2520authorized%2520domain.%2520This%250Aemphasis%2520leads%2520to%2520failure%2520in%2520recognizing%2520irrelevant%2520private%2520style%2520features%2520on%250Aunauthorized%2520domains.%2520To%2520this%2520end%252C%2520we%2520propose%2520novel%2520CUPI-Domain%2520generators%252C%250Awhich%2520select%2520features%2520from%2520both%2520authorized%2520and%2520CUPI-Domain%2520as%2520anchors.%2520Then%252C%2520we%250Afuse%2520the%2520style%2520features%2520and%2520semantic%2520features%2520of%2520these%2520anchors%2520to%2520generate%250Alabeled%2520and%2520style-rich%2520CUPI-Domain.%2520Additionally%252C%2520we%2520design%2520external%250ADomain-Information%2520Memory%2520Banks%2520%2528DIMB%2529%2520for%2520storing%2520and%2520updating%2520labeled%2520pyramid%250Afeatures%2520to%2520obtain%2520stable%2520domain%2520class%2520features%2520and%2520domain%2520class-wise%2520style%250Afeatures.%2520Based%2520on%2520the%2520proposed%2520whole%2520method%252C%2520the%2520novel%2520style%2520and%250Adiscriminative%2520loss%2520functions%2520are%2520designed%2520to%2520effectively%2520enhance%2520the%250Adistinction%2520in%2520style%2520and%2520discriminative%2520features%2520between%2520authorized%2520and%250Aunauthorized%2520domains%252C%2520respectively.%2520Moreover%252C%2520we%2520provide%2520two%2520solutions%2520for%250Autilizing%2520CUPI-Domain%2520based%2520on%2520whether%2520the%2520unauthorized%2520domain%2520is%2520known%253A%250Atarget-specified%2520CUPI-Domain%2520and%2520target-free%2520CUPI-Domain.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2408.13161v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Say%20No%20to%20Freeloader%3A%20Protecting%20Intellectual%20Property%20of%20Your%20Deep%0A%20%20Model&entry.906535625=Lianyu%20Wang%20and%20Meng%20Wang%20and%20Huazhu%20Fu%20and%20Daoqiang%20Zhang&entry.1292438233=%20%20Model%20intellectual%20property%20%28IP%29%20protection%20has%20attracted%20growing%20attention%0Aas%20science%20and%20technology%20advancements%20stem%20from%20human%20intellectual%20labor%20and%0Acomputational%20expenses.%20Ensuring%20IP%20safety%20for%20trainers%20and%20owners%20is%20of%20utmost%0Aimportance%2C%20particularly%20in%20domains%20where%20ownership%20verification%20and%0Aapplicability%20authorization%20are%20required.%20A%20notable%20approach%20to%20safeguarding%0Amodel%20IP%20involves%20proactively%20preventing%20the%20use%20of%20well-trained%20models%20of%0Aauthorized%20domains%20from%20unauthorized%20domains.%20In%20this%20paper%2C%20we%20introduce%20a%0Anovel%20Compact%20Un-transferable%20Pyramid%20Isolation%20Domain%20%28CUPI-Domain%29%20which%0Aserves%20as%20a%20barrier%20against%20illegal%20transfers%20from%20authorized%20to%20unauthorized%0Adomains.%20Drawing%20inspiration%20from%20human%20transitive%20inference%20and%20learning%0Aabilities%2C%20the%20CUPI-Domain%20is%20designed%20to%20obstruct%20cross-domain%20transfers%20by%0Aemphasizing%20the%20distinctive%20style%20features%20of%20the%20authorized%20domain.%20This%0Aemphasis%20leads%20to%20failure%20in%20recognizing%20irrelevant%20private%20style%20features%20on%0Aunauthorized%20domains.%20To%20this%20end%2C%20we%20propose%20novel%20CUPI-Domain%20generators%2C%0Awhich%20select%20features%20from%20both%20authorized%20and%20CUPI-Domain%20as%20anchors.%20Then%2C%20we%0Afuse%20the%20style%20features%20and%20semantic%20features%20of%20these%20anchors%20to%20generate%0Alabeled%20and%20style-rich%20CUPI-Domain.%20Additionally%2C%20we%20design%20external%0ADomain-Information%20Memory%20Banks%20%28DIMB%29%20for%20storing%20and%20updating%20labeled%20pyramid%0Afeatures%20to%20obtain%20stable%20domain%20class%20features%20and%20domain%20class-wise%20style%0Afeatures.%20Based%20on%20the%20proposed%20whole%20method%2C%20the%20novel%20style%20and%0Adiscriminative%20loss%20functions%20are%20designed%20to%20effectively%20enhance%20the%0Adistinction%20in%20style%20and%20discriminative%20features%20between%20authorized%20and%0Aunauthorized%20domains%2C%20respectively.%20Moreover%2C%20we%20provide%20two%20solutions%20for%0Autilizing%20CUPI-Domain%20based%20on%20whether%20the%20unauthorized%20domain%20is%20known%3A%0Atarget-specified%20CUPI-Domain%20and%20target-free%20CUPI-Domain.%0A&entry.1838667208=http%3A//arxiv.org/abs/2408.13161v2&entry.124074799=Read"},
{"title": "PoseWatch: A Transformer-based Architecture for Human-centric Video\n  Anomaly Detection Using Spatio-temporal Pose Tokenization", "author": "Ghazal Alinezhad Noghre and Armin Danesh Pazho and Hamed Tabkhi", "abstract": "  Video Anomaly Detection (VAD) presents a significant challenge in computer\nvision, particularly due to the unpredictable and infrequent nature of\nanomalous events, coupled with the diverse and dynamic environments in which\nthey occur. Human-centric VAD, a specialized area within this domain, faces\nadditional complexities, including variations in human behavior, potential\nbiases in data, and substantial privacy concerns related to human subjects.\nThese issues complicate the development of models that are both robust and\ngeneralizable. To address these challenges, recent advancements have focused on\npose-based VAD, which leverages human pose as a high-level feature to mitigate\nprivacy concerns, reduce appearance biases, and minimize background\ninterference. In this paper, we introduce PoseWatch, a novel transformer-based\narchitecture designed specifically for human-centric pose-based VAD. PoseWatch\nfeatures an innovative Spatio-Temporal Pose and Relative Pose (ST-PRP)\ntokenization method that enhances the representation of human motion over time,\nwhich is also beneficial for broader human behavior analysis tasks. The\narchitecture's core, a Unified Encoder Twin Decoders (UETD) transformer,\nsignificantly improves the detection of anomalous behaviors in video data.\nExtensive evaluations across multiple benchmark datasets demonstrate that\nPoseWatch consistently outperforms existing methods, establishing a new\nstate-of-the-art in pose-based VAD. This work not only demonstrates the\nefficacy of PoseWatch but also highlights the potential of integrating Natural\nLanguage Processing techniques with computer vision to advance human behavior\nanalysis.\n", "link": "http://arxiv.org/abs/2408.15185v1", "date": "2024-08-27", "relevancy": 2.391, "topK": [{"title": "PoseAnimate: Zero-shot high fidelity pose controllable character\n  animation", "link": "http://arxiv.org/abs/2404.13680v2", "similarity": 0.6302}, {"title": "Customize-A-Video: One-Shot Motion Customization of Text-to-Video\n  Diffusion Models", "link": "http://arxiv.org/abs/2402.14780v1", "similarity": 0.5823}, {"title": "Weakly-guided self-supervised pretraining for temporal activity detection", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/25189", "similarity": 0.5715}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20PoseWatch%3A%20A%20Transformer-based%20Architecture%20for%20Human-centric%20Video%0A%20%20Anomaly%20Detection%20Using%20Spatio-temporal%20Pose%20Tokenization&body=Title%3A%20PoseWatch%3A%20A%20Transformer-based%20Architecture%20for%20Human-centric%20Video%0A%20%20Anomaly%20Detection%20Using%20Spatio-temporal%20Pose%20Tokenization%0AAuthor%3A%20Ghazal%20Alinezhad%20Noghre%20and%20Armin%20Danesh%20Pazho%20and%20Hamed%20Tabkhi%0AAbstract%3A%20%20%20Video%20Anomaly%20Detection%20%28VAD%29%20presents%20a%20significant%20challenge%20in%20computer%0Avision%2C%20particularly%20due%20to%20the%20unpredictable%20and%20infrequent%20nature%20of%0Aanomalous%20events%2C%20coupled%20with%20the%20diverse%20and%20dynamic%20environments%20in%20which%0Athey%20occur.%20Human-centric%20VAD%2C%20a%20specialized%20area%20within%20this%20domain%2C%20faces%0Aadditional%20complexities%2C%20including%20variations%20in%20human%20behavior%2C%20potential%0Abiases%20in%20data%2C%20and%20substantial%20privacy%20concerns%20related%20to%20human%20subjects.%0AThese%20issues%20complicate%20the%20development%20of%20models%20that%20are%20both%20robust%20and%0Ageneralizable.%20To%20address%20these%20challenges%2C%20recent%20advancements%20have%20focused%20on%0Apose-based%20VAD%2C%20which%20leverages%20human%20pose%20as%20a%20high-level%20feature%20to%20mitigate%0Aprivacy%20concerns%2C%20reduce%20appearance%20biases%2C%20and%20minimize%20background%0Ainterference.%20In%20this%20paper%2C%20we%20introduce%20PoseWatch%2C%20a%20novel%20transformer-based%0Aarchitecture%20designed%20specifically%20for%20human-centric%20pose-based%20VAD.%20PoseWatch%0Afeatures%20an%20innovative%20Spatio-Temporal%20Pose%20and%20Relative%20Pose%20%28ST-PRP%29%0Atokenization%20method%20that%20enhances%20the%20representation%20of%20human%20motion%20over%20time%2C%0Awhich%20is%20also%20beneficial%20for%20broader%20human%20behavior%20analysis%20tasks.%20The%0Aarchitecture%27s%20core%2C%20a%20Unified%20Encoder%20Twin%20Decoders%20%28UETD%29%20transformer%2C%0Asignificantly%20improves%20the%20detection%20of%20anomalous%20behaviors%20in%20video%20data.%0AExtensive%20evaluations%20across%20multiple%20benchmark%20datasets%20demonstrate%20that%0APoseWatch%20consistently%20outperforms%20existing%20methods%2C%20establishing%20a%20new%0Astate-of-the-art%20in%20pose-based%20VAD.%20This%20work%20not%20only%20demonstrates%20the%0Aefficacy%20of%20PoseWatch%20but%20also%20highlights%20the%20potential%20of%20integrating%20Natural%0ALanguage%20Processing%20techniques%20with%20computer%20vision%20to%20advance%20human%20behavior%0Aanalysis.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2408.15185v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DPoseWatch%253A%2520A%2520Transformer-based%2520Architecture%2520for%2520Human-centric%2520Video%250A%2520%2520Anomaly%2520Detection%2520Using%2520Spatio-temporal%2520Pose%2520Tokenization%26entry.906535625%3DGhazal%2520Alinezhad%2520Noghre%2520and%2520Armin%2520Danesh%2520Pazho%2520and%2520Hamed%2520Tabkhi%26entry.1292438233%3D%2520%2520Video%2520Anomaly%2520Detection%2520%2528VAD%2529%2520presents%2520a%2520significant%2520challenge%2520in%2520computer%250Avision%252C%2520particularly%2520due%2520to%2520the%2520unpredictable%2520and%2520infrequent%2520nature%2520of%250Aanomalous%2520events%252C%2520coupled%2520with%2520the%2520diverse%2520and%2520dynamic%2520environments%2520in%2520which%250Athey%2520occur.%2520Human-centric%2520VAD%252C%2520a%2520specialized%2520area%2520within%2520this%2520domain%252C%2520faces%250Aadditional%2520complexities%252C%2520including%2520variations%2520in%2520human%2520behavior%252C%2520potential%250Abiases%2520in%2520data%252C%2520and%2520substantial%2520privacy%2520concerns%2520related%2520to%2520human%2520subjects.%250AThese%2520issues%2520complicate%2520the%2520development%2520of%2520models%2520that%2520are%2520both%2520robust%2520and%250Ageneralizable.%2520To%2520address%2520these%2520challenges%252C%2520recent%2520advancements%2520have%2520focused%2520on%250Apose-based%2520VAD%252C%2520which%2520leverages%2520human%2520pose%2520as%2520a%2520high-level%2520feature%2520to%2520mitigate%250Aprivacy%2520concerns%252C%2520reduce%2520appearance%2520biases%252C%2520and%2520minimize%2520background%250Ainterference.%2520In%2520this%2520paper%252C%2520we%2520introduce%2520PoseWatch%252C%2520a%2520novel%2520transformer-based%250Aarchitecture%2520designed%2520specifically%2520for%2520human-centric%2520pose-based%2520VAD.%2520PoseWatch%250Afeatures%2520an%2520innovative%2520Spatio-Temporal%2520Pose%2520and%2520Relative%2520Pose%2520%2528ST-PRP%2529%250Atokenization%2520method%2520that%2520enhances%2520the%2520representation%2520of%2520human%2520motion%2520over%2520time%252C%250Awhich%2520is%2520also%2520beneficial%2520for%2520broader%2520human%2520behavior%2520analysis%2520tasks.%2520The%250Aarchitecture%2527s%2520core%252C%2520a%2520Unified%2520Encoder%2520Twin%2520Decoders%2520%2528UETD%2529%2520transformer%252C%250Asignificantly%2520improves%2520the%2520detection%2520of%2520anomalous%2520behaviors%2520in%2520video%2520data.%250AExtensive%2520evaluations%2520across%2520multiple%2520benchmark%2520datasets%2520demonstrate%2520that%250APoseWatch%2520consistently%2520outperforms%2520existing%2520methods%252C%2520establishing%2520a%2520new%250Astate-of-the-art%2520in%2520pose-based%2520VAD.%2520This%2520work%2520not%2520only%2520demonstrates%2520the%250Aefficacy%2520of%2520PoseWatch%2520but%2520also%2520highlights%2520the%2520potential%2520of%2520integrating%2520Natural%250ALanguage%2520Processing%2520techniques%2520with%2520computer%2520vision%2520to%2520advance%2520human%2520behavior%250Aanalysis.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2408.15185v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=PoseWatch%3A%20A%20Transformer-based%20Architecture%20for%20Human-centric%20Video%0A%20%20Anomaly%20Detection%20Using%20Spatio-temporal%20Pose%20Tokenization&entry.906535625=Ghazal%20Alinezhad%20Noghre%20and%20Armin%20Danesh%20Pazho%20and%20Hamed%20Tabkhi&entry.1292438233=%20%20Video%20Anomaly%20Detection%20%28VAD%29%20presents%20a%20significant%20challenge%20in%20computer%0Avision%2C%20particularly%20due%20to%20the%20unpredictable%20and%20infrequent%20nature%20of%0Aanomalous%20events%2C%20coupled%20with%20the%20diverse%20and%20dynamic%20environments%20in%20which%0Athey%20occur.%20Human-centric%20VAD%2C%20a%20specialized%20area%20within%20this%20domain%2C%20faces%0Aadditional%20complexities%2C%20including%20variations%20in%20human%20behavior%2C%20potential%0Abiases%20in%20data%2C%20and%20substantial%20privacy%20concerns%20related%20to%20human%20subjects.%0AThese%20issues%20complicate%20the%20development%20of%20models%20that%20are%20both%20robust%20and%0Ageneralizable.%20To%20address%20these%20challenges%2C%20recent%20advancements%20have%20focused%20on%0Apose-based%20VAD%2C%20which%20leverages%20human%20pose%20as%20a%20high-level%20feature%20to%20mitigate%0Aprivacy%20concerns%2C%20reduce%20appearance%20biases%2C%20and%20minimize%20background%0Ainterference.%20In%20this%20paper%2C%20we%20introduce%20PoseWatch%2C%20a%20novel%20transformer-based%0Aarchitecture%20designed%20specifically%20for%20human-centric%20pose-based%20VAD.%20PoseWatch%0Afeatures%20an%20innovative%20Spatio-Temporal%20Pose%20and%20Relative%20Pose%20%28ST-PRP%29%0Atokenization%20method%20that%20enhances%20the%20representation%20of%20human%20motion%20over%20time%2C%0Awhich%20is%20also%20beneficial%20for%20broader%20human%20behavior%20analysis%20tasks.%20The%0Aarchitecture%27s%20core%2C%20a%20Unified%20Encoder%20Twin%20Decoders%20%28UETD%29%20transformer%2C%0Asignificantly%20improves%20the%20detection%20of%20anomalous%20behaviors%20in%20video%20data.%0AExtensive%20evaluations%20across%20multiple%20benchmark%20datasets%20demonstrate%20that%0APoseWatch%20consistently%20outperforms%20existing%20methods%2C%20establishing%20a%20new%0Astate-of-the-art%20in%20pose-based%20VAD.%20This%20work%20not%20only%20demonstrates%20the%0Aefficacy%20of%20PoseWatch%20but%20also%20highlights%20the%20potential%20of%20integrating%20Natural%0ALanguage%20Processing%20techniques%20with%20computer%20vision%20to%20advance%20human%20behavior%0Aanalysis.%0A&entry.1838667208=http%3A//arxiv.org/abs/2408.15185v1&entry.124074799=Read"},
{"title": "Generative Inbetweening: Adapting Image-to-Video Models for Keyframe\n  Interpolation", "author": "Xiaojuan Wang and Boyang Zhou and Brian Curless and Ira Kemelmacher-Shlizerman and Aleksander Holynski and Steven M. Seitz", "abstract": "  We present a method for generating video sequences with coherent motion\nbetween a pair of input key frames. We adapt a pretrained large-scale\nimage-to-video diffusion model (originally trained to generate videos moving\nforward in time from a single input image) for key frame interpolation, i.e.,\nto produce a video in between two input frames. We accomplish this adaptation\nthrough a lightweight fine-tuning technique that produces a version of the\nmodel that instead predicts videos moving backwards in time from a single input\nimage. This model (along with the original forward-moving model) is\nsubsequently used in a dual-directional diffusion sampling process that\ncombines the overlapping model estimates starting from each of the two\nkeyframes. Our experiments show that our method outperforms both existing\ndiffusion-based methods and traditional frame interpolation techniques.\n", "link": "http://arxiv.org/abs/2408.15239v1", "date": "2024-08-27", "relevancy": 2.3383, "topK": [{"title": "Customize-A-Video: One-Shot Motion Customization of Text-to-Video\n  Diffusion Models", "link": "http://arxiv.org/abs/2402.14780v1", "similarity": 0.5953}, {"title": "Tuning-Free Noise Rectification for High Fidelity Image-to-Video\n  Generation", "link": "http://arxiv.org/abs/2403.02827v1", "similarity": 0.5921}, {"title": "Magic-Me: Identity-Specific Video Customized Diffusion", "link": "http://arxiv.org/abs/2402.09368v1", "similarity": 0.5728}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Generative%20Inbetweening%3A%20Adapting%20Image-to-Video%20Models%20for%20Keyframe%0A%20%20Interpolation&body=Title%3A%20Generative%20Inbetweening%3A%20Adapting%20Image-to-Video%20Models%20for%20Keyframe%0A%20%20Interpolation%0AAuthor%3A%20Xiaojuan%20Wang%20and%20Boyang%20Zhou%20and%20Brian%20Curless%20and%20Ira%20Kemelmacher-Shlizerman%20and%20Aleksander%20Holynski%20and%20Steven%20M.%20Seitz%0AAbstract%3A%20%20%20We%20present%20a%20method%20for%20generating%20video%20sequences%20with%20coherent%20motion%0Abetween%20a%20pair%20of%20input%20key%20frames.%20We%20adapt%20a%20pretrained%20large-scale%0Aimage-to-video%20diffusion%20model%20%28originally%20trained%20to%20generate%20videos%20moving%0Aforward%20in%20time%20from%20a%20single%20input%20image%29%20for%20key%20frame%20interpolation%2C%20i.e.%2C%0Ato%20produce%20a%20video%20in%20between%20two%20input%20frames.%20We%20accomplish%20this%20adaptation%0Athrough%20a%20lightweight%20fine-tuning%20technique%20that%20produces%20a%20version%20of%20the%0Amodel%20that%20instead%20predicts%20videos%20moving%20backwards%20in%20time%20from%20a%20single%20input%0Aimage.%20This%20model%20%28along%20with%20the%20original%20forward-moving%20model%29%20is%0Asubsequently%20used%20in%20a%20dual-directional%20diffusion%20sampling%20process%20that%0Acombines%20the%20overlapping%20model%20estimates%20starting%20from%20each%20of%20the%20two%0Akeyframes.%20Our%20experiments%20show%20that%20our%20method%20outperforms%20both%20existing%0Adiffusion-based%20methods%20and%20traditional%20frame%20interpolation%20techniques.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2408.15239v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DGenerative%2520Inbetweening%253A%2520Adapting%2520Image-to-Video%2520Models%2520for%2520Keyframe%250A%2520%2520Interpolation%26entry.906535625%3DXiaojuan%2520Wang%2520and%2520Boyang%2520Zhou%2520and%2520Brian%2520Curless%2520and%2520Ira%2520Kemelmacher-Shlizerman%2520and%2520Aleksander%2520Holynski%2520and%2520Steven%2520M.%2520Seitz%26entry.1292438233%3D%2520%2520We%2520present%2520a%2520method%2520for%2520generating%2520video%2520sequences%2520with%2520coherent%2520motion%250Abetween%2520a%2520pair%2520of%2520input%2520key%2520frames.%2520We%2520adapt%2520a%2520pretrained%2520large-scale%250Aimage-to-video%2520diffusion%2520model%2520%2528originally%2520trained%2520to%2520generate%2520videos%2520moving%250Aforward%2520in%2520time%2520from%2520a%2520single%2520input%2520image%2529%2520for%2520key%2520frame%2520interpolation%252C%2520i.e.%252C%250Ato%2520produce%2520a%2520video%2520in%2520between%2520two%2520input%2520frames.%2520We%2520accomplish%2520this%2520adaptation%250Athrough%2520a%2520lightweight%2520fine-tuning%2520technique%2520that%2520produces%2520a%2520version%2520of%2520the%250Amodel%2520that%2520instead%2520predicts%2520videos%2520moving%2520backwards%2520in%2520time%2520from%2520a%2520single%2520input%250Aimage.%2520This%2520model%2520%2528along%2520with%2520the%2520original%2520forward-moving%2520model%2529%2520is%250Asubsequently%2520used%2520in%2520a%2520dual-directional%2520diffusion%2520sampling%2520process%2520that%250Acombines%2520the%2520overlapping%2520model%2520estimates%2520starting%2520from%2520each%2520of%2520the%2520two%250Akeyframes.%2520Our%2520experiments%2520show%2520that%2520our%2520method%2520outperforms%2520both%2520existing%250Adiffusion-based%2520methods%2520and%2520traditional%2520frame%2520interpolation%2520techniques.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2408.15239v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Generative%20Inbetweening%3A%20Adapting%20Image-to-Video%20Models%20for%20Keyframe%0A%20%20Interpolation&entry.906535625=Xiaojuan%20Wang%20and%20Boyang%20Zhou%20and%20Brian%20Curless%20and%20Ira%20Kemelmacher-Shlizerman%20and%20Aleksander%20Holynski%20and%20Steven%20M.%20Seitz&entry.1292438233=%20%20We%20present%20a%20method%20for%20generating%20video%20sequences%20with%20coherent%20motion%0Abetween%20a%20pair%20of%20input%20key%20frames.%20We%20adapt%20a%20pretrained%20large-scale%0Aimage-to-video%20diffusion%20model%20%28originally%20trained%20to%20generate%20videos%20moving%0Aforward%20in%20time%20from%20a%20single%20input%20image%29%20for%20key%20frame%20interpolation%2C%20i.e.%2C%0Ato%20produce%20a%20video%20in%20between%20two%20input%20frames.%20We%20accomplish%20this%20adaptation%0Athrough%20a%20lightweight%20fine-tuning%20technique%20that%20produces%20a%20version%20of%20the%0Amodel%20that%20instead%20predicts%20videos%20moving%20backwards%20in%20time%20from%20a%20single%20input%0Aimage.%20This%20model%20%28along%20with%20the%20original%20forward-moving%20model%29%20is%0Asubsequently%20used%20in%20a%20dual-directional%20diffusion%20sampling%20process%20that%0Acombines%20the%20overlapping%20model%20estimates%20starting%20from%20each%20of%20the%20two%0Akeyframes.%20Our%20experiments%20show%20that%20our%20method%20outperforms%20both%20existing%0Adiffusion-based%20methods%20and%20traditional%20frame%20interpolation%20techniques.%0A&entry.1838667208=http%3A//arxiv.org/abs/2408.15239v1&entry.124074799=Read"},
{"title": "Adaptive Fusion of Radiomics and Deep Features for Lung Adenocarcinoma\n  Subtype Recognition", "author": "Jing Zhou and Xiaotong Fu and Xirong Li and Ying Ji", "abstract": "  The most common type of lung cancer, lung adenocarcinoma (LUAD), has been\nincreasingly detected since the advent of low-dose computed tomography\nscreening technology. In clinical practice, pre-invasive LUAD (Pre-IAs) should\nonly require regular follow-up care, while invasive LUAD (IAs) should receive\nimmediate treatment with appropriate lung cancer resection, based on the cancer\nsubtype. However, prior research on diagnosing LUAD has mainly focused on\nclassifying Pre-IAs/IAs, as techniques for distinguishing different subtypes of\nIAs have been lacking. In this study, we proposed a multi-head attentional\nfeature fusion (MHA-FF) model for not only distinguishing IAs from Pre-IAs, but\nalso for distinguishing the different subtypes of IAs. To predict the subtype\nof each nodule accurately, we leveraged both radiomics and deep features\nextracted from computed tomography images. Furthermore, those features were\naggregated through an adaptive fusion module that can learn attention-based\ndiscriminative features. The utility of our proposed method is demonstrated\nhere by means of real-world data collected from a multi-center cohort.\n", "link": "http://arxiv.org/abs/2308.13997v2", "date": "2024-08-27", "relevancy": 2.3178, "topK": [{"title": "Breadcrumbs: Adversarial class-balanced sampling for long-tailed recognition", "link": "https://link.springer.com/chapter/10.1007/978-3-031-20053-3_37", "similarity": 0.4729}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.4724}, {"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.4454}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Adaptive%20Fusion%20of%20Radiomics%20and%20Deep%20Features%20for%20Lung%20Adenocarcinoma%0A%20%20Subtype%20Recognition&body=Title%3A%20Adaptive%20Fusion%20of%20Radiomics%20and%20Deep%20Features%20for%20Lung%20Adenocarcinoma%0A%20%20Subtype%20Recognition%0AAuthor%3A%20Jing%20Zhou%20and%20Xiaotong%20Fu%20and%20Xirong%20Li%20and%20Ying%20Ji%0AAbstract%3A%20%20%20The%20most%20common%20type%20of%20lung%20cancer%2C%20lung%20adenocarcinoma%20%28LUAD%29%2C%20has%20been%0Aincreasingly%20detected%20since%20the%20advent%20of%20low-dose%20computed%20tomography%0Ascreening%20technology.%20In%20clinical%20practice%2C%20pre-invasive%20LUAD%20%28Pre-IAs%29%20should%0Aonly%20require%20regular%20follow-up%20care%2C%20while%20invasive%20LUAD%20%28IAs%29%20should%20receive%0Aimmediate%20treatment%20with%20appropriate%20lung%20cancer%20resection%2C%20based%20on%20the%20cancer%0Asubtype.%20However%2C%20prior%20research%20on%20diagnosing%20LUAD%20has%20mainly%20focused%20on%0Aclassifying%20Pre-IAs/IAs%2C%20as%20techniques%20for%20distinguishing%20different%20subtypes%20of%0AIAs%20have%20been%20lacking.%20In%20this%20study%2C%20we%20proposed%20a%20multi-head%20attentional%0Afeature%20fusion%20%28MHA-FF%29%20model%20for%20not%20only%20distinguishing%20IAs%20from%20Pre-IAs%2C%20but%0Aalso%20for%20distinguishing%20the%20different%20subtypes%20of%20IAs.%20To%20predict%20the%20subtype%0Aof%20each%20nodule%20accurately%2C%20we%20leveraged%20both%20radiomics%20and%20deep%20features%0Aextracted%20from%20computed%20tomography%20images.%20Furthermore%2C%20those%20features%20were%0Aaggregated%20through%20an%20adaptive%20fusion%20module%20that%20can%20learn%20attention-based%0Adiscriminative%20features.%20The%20utility%20of%20our%20proposed%20method%20is%20demonstrated%0Ahere%20by%20means%20of%20real-world%20data%20collected%20from%20a%20multi-center%20cohort.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2308.13997v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DAdaptive%2520Fusion%2520of%2520Radiomics%2520and%2520Deep%2520Features%2520for%2520Lung%2520Adenocarcinoma%250A%2520%2520Subtype%2520Recognition%26entry.906535625%3DJing%2520Zhou%2520and%2520Xiaotong%2520Fu%2520and%2520Xirong%2520Li%2520and%2520Ying%2520Ji%26entry.1292438233%3D%2520%2520The%2520most%2520common%2520type%2520of%2520lung%2520cancer%252C%2520lung%2520adenocarcinoma%2520%2528LUAD%2529%252C%2520has%2520been%250Aincreasingly%2520detected%2520since%2520the%2520advent%2520of%2520low-dose%2520computed%2520tomography%250Ascreening%2520technology.%2520In%2520clinical%2520practice%252C%2520pre-invasive%2520LUAD%2520%2528Pre-IAs%2529%2520should%250Aonly%2520require%2520regular%2520follow-up%2520care%252C%2520while%2520invasive%2520LUAD%2520%2528IAs%2529%2520should%2520receive%250Aimmediate%2520treatment%2520with%2520appropriate%2520lung%2520cancer%2520resection%252C%2520based%2520on%2520the%2520cancer%250Asubtype.%2520However%252C%2520prior%2520research%2520on%2520diagnosing%2520LUAD%2520has%2520mainly%2520focused%2520on%250Aclassifying%2520Pre-IAs/IAs%252C%2520as%2520techniques%2520for%2520distinguishing%2520different%2520subtypes%2520of%250AIAs%2520have%2520been%2520lacking.%2520In%2520this%2520study%252C%2520we%2520proposed%2520a%2520multi-head%2520attentional%250Afeature%2520fusion%2520%2528MHA-FF%2529%2520model%2520for%2520not%2520only%2520distinguishing%2520IAs%2520from%2520Pre-IAs%252C%2520but%250Aalso%2520for%2520distinguishing%2520the%2520different%2520subtypes%2520of%2520IAs.%2520To%2520predict%2520the%2520subtype%250Aof%2520each%2520nodule%2520accurately%252C%2520we%2520leveraged%2520both%2520radiomics%2520and%2520deep%2520features%250Aextracted%2520from%2520computed%2520tomography%2520images.%2520Furthermore%252C%2520those%2520features%2520were%250Aaggregated%2520through%2520an%2520adaptive%2520fusion%2520module%2520that%2520can%2520learn%2520attention-based%250Adiscriminative%2520features.%2520The%2520utility%2520of%2520our%2520proposed%2520method%2520is%2520demonstrated%250Ahere%2520by%2520means%2520of%2520real-world%2520data%2520collected%2520from%2520a%2520multi-center%2520cohort.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2308.13997v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Adaptive%20Fusion%20of%20Radiomics%20and%20Deep%20Features%20for%20Lung%20Adenocarcinoma%0A%20%20Subtype%20Recognition&entry.906535625=Jing%20Zhou%20and%20Xiaotong%20Fu%20and%20Xirong%20Li%20and%20Ying%20Ji&entry.1292438233=%20%20The%20most%20common%20type%20of%20lung%20cancer%2C%20lung%20adenocarcinoma%20%28LUAD%29%2C%20has%20been%0Aincreasingly%20detected%20since%20the%20advent%20of%20low-dose%20computed%20tomography%0Ascreening%20technology.%20In%20clinical%20practice%2C%20pre-invasive%20LUAD%20%28Pre-IAs%29%20should%0Aonly%20require%20regular%20follow-up%20care%2C%20while%20invasive%20LUAD%20%28IAs%29%20should%20receive%0Aimmediate%20treatment%20with%20appropriate%20lung%20cancer%20resection%2C%20based%20on%20the%20cancer%0Asubtype.%20However%2C%20prior%20research%20on%20diagnosing%20LUAD%20has%20mainly%20focused%20on%0Aclassifying%20Pre-IAs/IAs%2C%20as%20techniques%20for%20distinguishing%20different%20subtypes%20of%0AIAs%20have%20been%20lacking.%20In%20this%20study%2C%20we%20proposed%20a%20multi-head%20attentional%0Afeature%20fusion%20%28MHA-FF%29%20model%20for%20not%20only%20distinguishing%20IAs%20from%20Pre-IAs%2C%20but%0Aalso%20for%20distinguishing%20the%20different%20subtypes%20of%20IAs.%20To%20predict%20the%20subtype%0Aof%20each%20nodule%20accurately%2C%20we%20leveraged%20both%20radiomics%20and%20deep%20features%0Aextracted%20from%20computed%20tomography%20images.%20Furthermore%2C%20those%20features%20were%0Aaggregated%20through%20an%20adaptive%20fusion%20module%20that%20can%20learn%20attention-based%0Adiscriminative%20features.%20The%20utility%20of%20our%20proposed%20method%20is%20demonstrated%0Ahere%20by%20means%20of%20real-world%20data%20collected%20from%20a%20multi-center%20cohort.%0A&entry.1838667208=http%3A//arxiv.org/abs/2308.13997v2&entry.124074799=Read"},
{"title": "Text3DAug -- Prompted Instance Augmentation for LiDAR Perception", "author": "Laurenz Reichardt and Luca Uhr and Oliver Wasenm\u00fcller", "abstract": "  LiDAR data of urban scenarios poses unique challenges, such as heterogeneous\ncharacteristics and inherent class imbalance. Therefore, large-scale datasets\nare necessary to apply deep learning methods. Instance augmentation has emerged\nas an efficient method to increase dataset diversity. However, current methods\nrequire the time-consuming curation of 3D models or costly manual data\nannotation. To overcome these limitations, we propose Text3DAug, a novel\napproach leveraging generative models for instance augmentation. Text3DAug does\nnot depend on labeled data and is the first of its kind to generate instances\nand annotations from text. This allows for a fully automated pipeline,\neliminating the need for manual effort in practical applications. Additionally,\nText3DAug is sensor agnostic and can be applied regardless of the LiDAR sensor\nused. Comprehensive experimental analysis on LiDAR segmentation, detection and\nnovel class discovery demonstrates that Text3DAug is effective in supplementing\nexisting methods or as a standalone method, performing on par or better than\nestablished methods, however while overcoming their specific drawbacks. The\ncode is publicly available.\n", "link": "http://arxiv.org/abs/2408.14253v2", "date": "2024-08-27", "relevancy": 2.3074, "topK": [{"title": "CAT3D: Create Anything in 3D with Multi-View Diffusion Models", "link": "http://arxiv.org/abs/2405.10314v1", "similarity": 0.5775}, {"title": "CAT3D: Create Anything in 3D with Multi-View Diffusion Models", "link": "http://arxiv.org/abs/2405.10314v1", "similarity": 0.5775}, {"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.5736}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Text3DAug%20--%20Prompted%20Instance%20Augmentation%20for%20LiDAR%20Perception&body=Title%3A%20Text3DAug%20--%20Prompted%20Instance%20Augmentation%20for%20LiDAR%20Perception%0AAuthor%3A%20Laurenz%20Reichardt%20and%20Luca%20Uhr%20and%20Oliver%20Wasenm%C3%BCller%0AAbstract%3A%20%20%20LiDAR%20data%20of%20urban%20scenarios%20poses%20unique%20challenges%2C%20such%20as%20heterogeneous%0Acharacteristics%20and%20inherent%20class%20imbalance.%20Therefore%2C%20large-scale%20datasets%0Aare%20necessary%20to%20apply%20deep%20learning%20methods.%20Instance%20augmentation%20has%20emerged%0Aas%20an%20efficient%20method%20to%20increase%20dataset%20diversity.%20However%2C%20current%20methods%0Arequire%20the%20time-consuming%20curation%20of%203D%20models%20or%20costly%20manual%20data%0Aannotation.%20To%20overcome%20these%20limitations%2C%20we%20propose%20Text3DAug%2C%20a%20novel%0Aapproach%20leveraging%20generative%20models%20for%20instance%20augmentation.%20Text3DAug%20does%0Anot%20depend%20on%20labeled%20data%20and%20is%20the%20first%20of%20its%20kind%20to%20generate%20instances%0Aand%20annotations%20from%20text.%20This%20allows%20for%20a%20fully%20automated%20pipeline%2C%0Aeliminating%20the%20need%20for%20manual%20effort%20in%20practical%20applications.%20Additionally%2C%0AText3DAug%20is%20sensor%20agnostic%20and%20can%20be%20applied%20regardless%20of%20the%20LiDAR%20sensor%0Aused.%20Comprehensive%20experimental%20analysis%20on%20LiDAR%20segmentation%2C%20detection%20and%0Anovel%20class%20discovery%20demonstrates%20that%20Text3DAug%20is%20effective%20in%20supplementing%0Aexisting%20methods%20or%20as%20a%20standalone%20method%2C%20performing%20on%20par%20or%20better%20than%0Aestablished%20methods%2C%20however%20while%20overcoming%20their%20specific%20drawbacks.%20The%0Acode%20is%20publicly%20available.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2408.14253v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DText3DAug%2520--%2520Prompted%2520Instance%2520Augmentation%2520for%2520LiDAR%2520Perception%26entry.906535625%3DLaurenz%2520Reichardt%2520and%2520Luca%2520Uhr%2520and%2520Oliver%2520Wasenm%25C3%25BCller%26entry.1292438233%3D%2520%2520LiDAR%2520data%2520of%2520urban%2520scenarios%2520poses%2520unique%2520challenges%252C%2520such%2520as%2520heterogeneous%250Acharacteristics%2520and%2520inherent%2520class%2520imbalance.%2520Therefore%252C%2520large-scale%2520datasets%250Aare%2520necessary%2520to%2520apply%2520deep%2520learning%2520methods.%2520Instance%2520augmentation%2520has%2520emerged%250Aas%2520an%2520efficient%2520method%2520to%2520increase%2520dataset%2520diversity.%2520However%252C%2520current%2520methods%250Arequire%2520the%2520time-consuming%2520curation%2520of%25203D%2520models%2520or%2520costly%2520manual%2520data%250Aannotation.%2520To%2520overcome%2520these%2520limitations%252C%2520we%2520propose%2520Text3DAug%252C%2520a%2520novel%250Aapproach%2520leveraging%2520generative%2520models%2520for%2520instance%2520augmentation.%2520Text3DAug%2520does%250Anot%2520depend%2520on%2520labeled%2520data%2520and%2520is%2520the%2520first%2520of%2520its%2520kind%2520to%2520generate%2520instances%250Aand%2520annotations%2520from%2520text.%2520This%2520allows%2520for%2520a%2520fully%2520automated%2520pipeline%252C%250Aeliminating%2520the%2520need%2520for%2520manual%2520effort%2520in%2520practical%2520applications.%2520Additionally%252C%250AText3DAug%2520is%2520sensor%2520agnostic%2520and%2520can%2520be%2520applied%2520regardless%2520of%2520the%2520LiDAR%2520sensor%250Aused.%2520Comprehensive%2520experimental%2520analysis%2520on%2520LiDAR%2520segmentation%252C%2520detection%2520and%250Anovel%2520class%2520discovery%2520demonstrates%2520that%2520Text3DAug%2520is%2520effective%2520in%2520supplementing%250Aexisting%2520methods%2520or%2520as%2520a%2520standalone%2520method%252C%2520performing%2520on%2520par%2520or%2520better%2520than%250Aestablished%2520methods%252C%2520however%2520while%2520overcoming%2520their%2520specific%2520drawbacks.%2520The%250Acode%2520is%2520publicly%2520available.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2408.14253v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Text3DAug%20--%20Prompted%20Instance%20Augmentation%20for%20LiDAR%20Perception&entry.906535625=Laurenz%20Reichardt%20and%20Luca%20Uhr%20and%20Oliver%20Wasenm%C3%BCller&entry.1292438233=%20%20LiDAR%20data%20of%20urban%20scenarios%20poses%20unique%20challenges%2C%20such%20as%20heterogeneous%0Acharacteristics%20and%20inherent%20class%20imbalance.%20Therefore%2C%20large-scale%20datasets%0Aare%20necessary%20to%20apply%20deep%20learning%20methods.%20Instance%20augmentation%20has%20emerged%0Aas%20an%20efficient%20method%20to%20increase%20dataset%20diversity.%20However%2C%20current%20methods%0Arequire%20the%20time-consuming%20curation%20of%203D%20models%20or%20costly%20manual%20data%0Aannotation.%20To%20overcome%20these%20limitations%2C%20we%20propose%20Text3DAug%2C%20a%20novel%0Aapproach%20leveraging%20generative%20models%20for%20instance%20augmentation.%20Text3DAug%20does%0Anot%20depend%20on%20labeled%20data%20and%20is%20the%20first%20of%20its%20kind%20to%20generate%20instances%0Aand%20annotations%20from%20text.%20This%20allows%20for%20a%20fully%20automated%20pipeline%2C%0Aeliminating%20the%20need%20for%20manual%20effort%20in%20practical%20applications.%20Additionally%2C%0AText3DAug%20is%20sensor%20agnostic%20and%20can%20be%20applied%20regardless%20of%20the%20LiDAR%20sensor%0Aused.%20Comprehensive%20experimental%20analysis%20on%20LiDAR%20segmentation%2C%20detection%20and%0Anovel%20class%20discovery%20demonstrates%20that%20Text3DAug%20is%20effective%20in%20supplementing%0Aexisting%20methods%20or%20as%20a%20standalone%20method%2C%20performing%20on%20par%20or%20better%20than%0Aestablished%20methods%2C%20however%20while%20overcoming%20their%20specific%20drawbacks.%20The%0Acode%20is%20publicly%20available.%0A&entry.1838667208=http%3A//arxiv.org/abs/2408.14253v2&entry.124074799=Read"},
{"title": "Glauber Generative Model: Discrete Diffusion Models via Binary\n  Classification", "author": "Harshit Varma and Dheeraj Nagaraj and Karthikeyan Shanmugam", "abstract": "  We introduce the Glauber Generative Model (GGM), a new class of discrete\ndiffusion models, to obtain new samples from a distribution given samples from\na discrete space. GGM deploys a discrete Markov chain called the heat bath\ndynamics (or the Glauber dynamics) to denoise a sequence of noisy tokens to a\nsample from a joint distribution of discrete tokens. Our novel conceptual\nframework provides an exact reduction of the task of learning the denoising\nMarkov chain to solving a class of binary classification tasks. More\nspecifically, the model learns to classify a given token in a noisy sequence as\nsignal or noise. In contrast, prior works on discrete diffusion models either\nsolve regression problems to learn importance ratios, or minimize loss\nfunctions given by variational approximations. We apply GGM to language\nmodeling and image generation, where images are discretized using image\ntokenizers like VQGANs. We show that it outperforms existing discrete diffusion\nmodels in language generation, and demonstrates strong performance for image\ngeneration without using dataset-specific image tokenizers. We also show that\nour model is capable of performing well in zero-shot control settings like text\nand image infilling.\n", "link": "http://arxiv.org/abs/2405.17035v3", "date": "2024-08-27", "relevancy": 2.2946, "topK": [{"title": "ToDo: Token Downsampling for Efficient Generation of High-Resolution\n  Images", "link": "http://arxiv.org/abs/2402.13573v2", "similarity": 0.5771}, {"title": "Genie: Generative Interactive Environments", "link": "http://arxiv.org/abs/2402.15391v1", "similarity": 0.5716}, {"title": "SDXS: Real-Time One-Step Latent Diffusion Models with Image Conditions", "link": "http://arxiv.org/abs/2403.16627v1", "similarity": 0.5703}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Glauber%20Generative%20Model%3A%20Discrete%20Diffusion%20Models%20via%20Binary%0A%20%20Classification&body=Title%3A%20Glauber%20Generative%20Model%3A%20Discrete%20Diffusion%20Models%20via%20Binary%0A%20%20Classification%0AAuthor%3A%20Harshit%20Varma%20and%20Dheeraj%20Nagaraj%20and%20Karthikeyan%20Shanmugam%0AAbstract%3A%20%20%20We%20introduce%20the%20Glauber%20Generative%20Model%20%28GGM%29%2C%20a%20new%20class%20of%20discrete%0Adiffusion%20models%2C%20to%20obtain%20new%20samples%20from%20a%20distribution%20given%20samples%20from%0Aa%20discrete%20space.%20GGM%20deploys%20a%20discrete%20Markov%20chain%20called%20the%20heat%20bath%0Adynamics%20%28or%20the%20Glauber%20dynamics%29%20to%20denoise%20a%20sequence%20of%20noisy%20tokens%20to%20a%0Asample%20from%20a%20joint%20distribution%20of%20discrete%20tokens.%20Our%20novel%20conceptual%0Aframework%20provides%20an%20exact%20reduction%20of%20the%20task%20of%20learning%20the%20denoising%0AMarkov%20chain%20to%20solving%20a%20class%20of%20binary%20classification%20tasks.%20More%0Aspecifically%2C%20the%20model%20learns%20to%20classify%20a%20given%20token%20in%20a%20noisy%20sequence%20as%0Asignal%20or%20noise.%20In%20contrast%2C%20prior%20works%20on%20discrete%20diffusion%20models%20either%0Asolve%20regression%20problems%20to%20learn%20importance%20ratios%2C%20or%20minimize%20loss%0Afunctions%20given%20by%20variational%20approximations.%20We%20apply%20GGM%20to%20language%0Amodeling%20and%20image%20generation%2C%20where%20images%20are%20discretized%20using%20image%0Atokenizers%20like%20VQGANs.%20We%20show%20that%20it%20outperforms%20existing%20discrete%20diffusion%0Amodels%20in%20language%20generation%2C%20and%20demonstrates%20strong%20performance%20for%20image%0Ageneration%20without%20using%20dataset-specific%20image%20tokenizers.%20We%20also%20show%20that%0Aour%20model%20is%20capable%20of%20performing%20well%20in%20zero-shot%20control%20settings%20like%20text%0Aand%20image%20infilling.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2405.17035v3%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DGlauber%2520Generative%2520Model%253A%2520Discrete%2520Diffusion%2520Models%2520via%2520Binary%250A%2520%2520Classification%26entry.906535625%3DHarshit%2520Varma%2520and%2520Dheeraj%2520Nagaraj%2520and%2520Karthikeyan%2520Shanmugam%26entry.1292438233%3D%2520%2520We%2520introduce%2520the%2520Glauber%2520Generative%2520Model%2520%2528GGM%2529%252C%2520a%2520new%2520class%2520of%2520discrete%250Adiffusion%2520models%252C%2520to%2520obtain%2520new%2520samples%2520from%2520a%2520distribution%2520given%2520samples%2520from%250Aa%2520discrete%2520space.%2520GGM%2520deploys%2520a%2520discrete%2520Markov%2520chain%2520called%2520the%2520heat%2520bath%250Adynamics%2520%2528or%2520the%2520Glauber%2520dynamics%2529%2520to%2520denoise%2520a%2520sequence%2520of%2520noisy%2520tokens%2520to%2520a%250Asample%2520from%2520a%2520joint%2520distribution%2520of%2520discrete%2520tokens.%2520Our%2520novel%2520conceptual%250Aframework%2520provides%2520an%2520exact%2520reduction%2520of%2520the%2520task%2520of%2520learning%2520the%2520denoising%250AMarkov%2520chain%2520to%2520solving%2520a%2520class%2520of%2520binary%2520classification%2520tasks.%2520More%250Aspecifically%252C%2520the%2520model%2520learns%2520to%2520classify%2520a%2520given%2520token%2520in%2520a%2520noisy%2520sequence%2520as%250Asignal%2520or%2520noise.%2520In%2520contrast%252C%2520prior%2520works%2520on%2520discrete%2520diffusion%2520models%2520either%250Asolve%2520regression%2520problems%2520to%2520learn%2520importance%2520ratios%252C%2520or%2520minimize%2520loss%250Afunctions%2520given%2520by%2520variational%2520approximations.%2520We%2520apply%2520GGM%2520to%2520language%250Amodeling%2520and%2520image%2520generation%252C%2520where%2520images%2520are%2520discretized%2520using%2520image%250Atokenizers%2520like%2520VQGANs.%2520We%2520show%2520that%2520it%2520outperforms%2520existing%2520discrete%2520diffusion%250Amodels%2520in%2520language%2520generation%252C%2520and%2520demonstrates%2520strong%2520performance%2520for%2520image%250Ageneration%2520without%2520using%2520dataset-specific%2520image%2520tokenizers.%2520We%2520also%2520show%2520that%250Aour%2520model%2520is%2520capable%2520of%2520performing%2520well%2520in%2520zero-shot%2520control%2520settings%2520like%2520text%250Aand%2520image%2520infilling.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2405.17035v3%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Glauber%20Generative%20Model%3A%20Discrete%20Diffusion%20Models%20via%20Binary%0A%20%20Classification&entry.906535625=Harshit%20Varma%20and%20Dheeraj%20Nagaraj%20and%20Karthikeyan%20Shanmugam&entry.1292438233=%20%20We%20introduce%20the%20Glauber%20Generative%20Model%20%28GGM%29%2C%20a%20new%20class%20of%20discrete%0Adiffusion%20models%2C%20to%20obtain%20new%20samples%20from%20a%20distribution%20given%20samples%20from%0Aa%20discrete%20space.%20GGM%20deploys%20a%20discrete%20Markov%20chain%20called%20the%20heat%20bath%0Adynamics%20%28or%20the%20Glauber%20dynamics%29%20to%20denoise%20a%20sequence%20of%20noisy%20tokens%20to%20a%0Asample%20from%20a%20joint%20distribution%20of%20discrete%20tokens.%20Our%20novel%20conceptual%0Aframework%20provides%20an%20exact%20reduction%20of%20the%20task%20of%20learning%20the%20denoising%0AMarkov%20chain%20to%20solving%20a%20class%20of%20binary%20classification%20tasks.%20More%0Aspecifically%2C%20the%20model%20learns%20to%20classify%20a%20given%20token%20in%20a%20noisy%20sequence%20as%0Asignal%20or%20noise.%20In%20contrast%2C%20prior%20works%20on%20discrete%20diffusion%20models%20either%0Asolve%20regression%20problems%20to%20learn%20importance%20ratios%2C%20or%20minimize%20loss%0Afunctions%20given%20by%20variational%20approximations.%20We%20apply%20GGM%20to%20language%0Amodeling%20and%20image%20generation%2C%20where%20images%20are%20discretized%20using%20image%0Atokenizers%20like%20VQGANs.%20We%20show%20that%20it%20outperforms%20existing%20discrete%20diffusion%0Amodels%20in%20language%20generation%2C%20and%20demonstrates%20strong%20performance%20for%20image%0Ageneration%20without%20using%20dataset-specific%20image%20tokenizers.%20We%20also%20show%20that%0Aour%20model%20is%20capable%20of%20performing%20well%20in%20zero-shot%20control%20settings%20like%20text%0Aand%20image%20infilling.%0A&entry.1838667208=http%3A//arxiv.org/abs/2405.17035v3&entry.124074799=Read"},
{"title": "Variational Autoencoding of Dental Point Clouds", "author": "Johan Ziruo Ye and Thomas \u00d8rkild and Peter Lempel S\u00f8ndergaard and S\u00f8ren Hauberg", "abstract": "  Digital dentistry has made significant advancements, yet numerous challenges\nremain. This paper introduces the FDI 16 dataset, an extensive collection of\ntooth meshes and point clouds. Additionally, we present a novel approach:\nVariational FoldingNet (VF-Net), a fully probabilistic variational autoencoder\nfor point clouds. Notably, prior latent variable models for point clouds lack a\none-to-one correspondence between input and output points. Instead, they rely\non optimizing Chamfer distances, a metric that lacks a normalized\ndistributional counterpart, rendering it unsuitable for probabilistic modeling.\nWe replace the explicit minimization of Chamfer distances with a suitable\nencoder, increasing computational efficiency while simplifying the\nprobabilistic extension. This allows for straightforward application in various\ntasks, including mesh generation, shape completion, and representation\nlearning. Empirically, we provide evidence of lower reconstruction error in\ndental reconstruction and interpolation, showcasing state-of-the-art\nperformance in dental sample generation while identifying valuable latent\nrepresentations\n", "link": "http://arxiv.org/abs/2307.10895v4", "date": "2024-08-27", "relevancy": 2.2569, "topK": [{"title": "IAE: Implicit Autoencoder for Point Cloud Self-supervised Representation Learning", "link": "https://arxiv.org/abs/2201.00785", "similarity": 0.6103}, {"title": "Dynamic Gaussians Mesh: Consistent Mesh Reconstruction from Monocular\n  Videos", "link": "http://arxiv.org/abs/2404.12379v2", "similarity": 0.5366}, {"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.518}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Variational%20Autoencoding%20of%20Dental%20Point%20Clouds&body=Title%3A%20Variational%20Autoencoding%20of%20Dental%20Point%20Clouds%0AAuthor%3A%20Johan%20Ziruo%20Ye%20and%20Thomas%20%C3%98rkild%20and%20Peter%20Lempel%20S%C3%B8ndergaard%20and%20S%C3%B8ren%20Hauberg%0AAbstract%3A%20%20%20Digital%20dentistry%20has%20made%20significant%20advancements%2C%20yet%20numerous%20challenges%0Aremain.%20This%20paper%20introduces%20the%20FDI%2016%20dataset%2C%20an%20extensive%20collection%20of%0Atooth%20meshes%20and%20point%20clouds.%20Additionally%2C%20we%20present%20a%20novel%20approach%3A%0AVariational%20FoldingNet%20%28VF-Net%29%2C%20a%20fully%20probabilistic%20variational%20autoencoder%0Afor%20point%20clouds.%20Notably%2C%20prior%20latent%20variable%20models%20for%20point%20clouds%20lack%20a%0Aone-to-one%20correspondence%20between%20input%20and%20output%20points.%20Instead%2C%20they%20rely%0Aon%20optimizing%20Chamfer%20distances%2C%20a%20metric%20that%20lacks%20a%20normalized%0Adistributional%20counterpart%2C%20rendering%20it%20unsuitable%20for%20probabilistic%20modeling.%0AWe%20replace%20the%20explicit%20minimization%20of%20Chamfer%20distances%20with%20a%20suitable%0Aencoder%2C%20increasing%20computational%20efficiency%20while%20simplifying%20the%0Aprobabilistic%20extension.%20This%20allows%20for%20straightforward%20application%20in%20various%0Atasks%2C%20including%20mesh%20generation%2C%20shape%20completion%2C%20and%20representation%0Alearning.%20Empirically%2C%20we%20provide%20evidence%20of%20lower%20reconstruction%20error%20in%0Adental%20reconstruction%20and%20interpolation%2C%20showcasing%20state-of-the-art%0Aperformance%20in%20dental%20sample%20generation%20while%20identifying%20valuable%20latent%0Arepresentations%0A%0ALink%3A%20http%3A//arxiv.org/abs/2307.10895v4%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DVariational%2520Autoencoding%2520of%2520Dental%2520Point%2520Clouds%26entry.906535625%3DJohan%2520Ziruo%2520Ye%2520and%2520Thomas%2520%25C3%2598rkild%2520and%2520Peter%2520Lempel%2520S%25C3%25B8ndergaard%2520and%2520S%25C3%25B8ren%2520Hauberg%26entry.1292438233%3D%2520%2520Digital%2520dentistry%2520has%2520made%2520significant%2520advancements%252C%2520yet%2520numerous%2520challenges%250Aremain.%2520This%2520paper%2520introduces%2520the%2520FDI%252016%2520dataset%252C%2520an%2520extensive%2520collection%2520of%250Atooth%2520meshes%2520and%2520point%2520clouds.%2520Additionally%252C%2520we%2520present%2520a%2520novel%2520approach%253A%250AVariational%2520FoldingNet%2520%2528VF-Net%2529%252C%2520a%2520fully%2520probabilistic%2520variational%2520autoencoder%250Afor%2520point%2520clouds.%2520Notably%252C%2520prior%2520latent%2520variable%2520models%2520for%2520point%2520clouds%2520lack%2520a%250Aone-to-one%2520correspondence%2520between%2520input%2520and%2520output%2520points.%2520Instead%252C%2520they%2520rely%250Aon%2520optimizing%2520Chamfer%2520distances%252C%2520a%2520metric%2520that%2520lacks%2520a%2520normalized%250Adistributional%2520counterpart%252C%2520rendering%2520it%2520unsuitable%2520for%2520probabilistic%2520modeling.%250AWe%2520replace%2520the%2520explicit%2520minimization%2520of%2520Chamfer%2520distances%2520with%2520a%2520suitable%250Aencoder%252C%2520increasing%2520computational%2520efficiency%2520while%2520simplifying%2520the%250Aprobabilistic%2520extension.%2520This%2520allows%2520for%2520straightforward%2520application%2520in%2520various%250Atasks%252C%2520including%2520mesh%2520generation%252C%2520shape%2520completion%252C%2520and%2520representation%250Alearning.%2520Empirically%252C%2520we%2520provide%2520evidence%2520of%2520lower%2520reconstruction%2520error%2520in%250Adental%2520reconstruction%2520and%2520interpolation%252C%2520showcasing%2520state-of-the-art%250Aperformance%2520in%2520dental%2520sample%2520generation%2520while%2520identifying%2520valuable%2520latent%250Arepresentations%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2307.10895v4%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Variational%20Autoencoding%20of%20Dental%20Point%20Clouds&entry.906535625=Johan%20Ziruo%20Ye%20and%20Thomas%20%C3%98rkild%20and%20Peter%20Lempel%20S%C3%B8ndergaard%20and%20S%C3%B8ren%20Hauberg&entry.1292438233=%20%20Digital%20dentistry%20has%20made%20significant%20advancements%2C%20yet%20numerous%20challenges%0Aremain.%20This%20paper%20introduces%20the%20FDI%2016%20dataset%2C%20an%20extensive%20collection%20of%0Atooth%20meshes%20and%20point%20clouds.%20Additionally%2C%20we%20present%20a%20novel%20approach%3A%0AVariational%20FoldingNet%20%28VF-Net%29%2C%20a%20fully%20probabilistic%20variational%20autoencoder%0Afor%20point%20clouds.%20Notably%2C%20prior%20latent%20variable%20models%20for%20point%20clouds%20lack%20a%0Aone-to-one%20correspondence%20between%20input%20and%20output%20points.%20Instead%2C%20they%20rely%0Aon%20optimizing%20Chamfer%20distances%2C%20a%20metric%20that%20lacks%20a%20normalized%0Adistributional%20counterpart%2C%20rendering%20it%20unsuitable%20for%20probabilistic%20modeling.%0AWe%20replace%20the%20explicit%20minimization%20of%20Chamfer%20distances%20with%20a%20suitable%0Aencoder%2C%20increasing%20computational%20efficiency%20while%20simplifying%20the%0Aprobabilistic%20extension.%20This%20allows%20for%20straightforward%20application%20in%20various%0Atasks%2C%20including%20mesh%20generation%2C%20shape%20completion%2C%20and%20representation%0Alearning.%20Empirically%2C%20we%20provide%20evidence%20of%20lower%20reconstruction%20error%20in%0Adental%20reconstruction%20and%20interpolation%2C%20showcasing%20state-of-the-art%0Aperformance%20in%20dental%20sample%20generation%20while%20identifying%20valuable%20latent%0Arepresentations%0A&entry.1838667208=http%3A//arxiv.org/abs/2307.10895v4&entry.124074799=Read"},
{"title": "Interactive Occlusion Boundary Estimation through Exploitation of\n  Synthetic Data", "author": "Lintao Xu and Chaohui Wang", "abstract": "  Occlusion boundaries (OBs) geometrically localize the occlusion events in a\n2D image, and contain useful information for addressing various scene\nunderstanding problems. To advance their study, we have led the investigation\nin the following three aspects. Firstly, we have studied interactive estimation\nof OBs, which is the first in the literature, and proposed an efficient\ndeep-network-based method using multiple-scribble intervention, named DNMMSI,\nwhich significantly improves the performance over the state-of-the-art\nfully-automatic methods. Secondly, we propose to exploit the synthetic\nbenchmark for the training process, thanks to the particularity that OBs are\ndetermined geometrically and unambiguously from the 3D scene. To this end, we\nhave developed an efficient tool, named Mesh2OB, for the automatic generation\nof 2D images together with their ground-truth OBs, using which we have\nconstructed a synthetic benchmark, named OB-FUTURE. Abundant experimental\nresults demonstrate that leveraging such a synthetic benchmark for training\nachieves promising performance, even without the use of domain adaptation\ntechniques. Finally, to achieve a more compelling and robust evaluation in\nOB-related research, we have created a real benchmark, named OB-LabName,\nconsisting of 120 high-resolution images together with their ground-truth OBs,\nwith precision surpassing that of previous benchmarks. We will release DNMMSI\nwith pre-trained parameters, Mesh2OB, OB-FUTURE, and OB-LabName to support\nfurther research.\n", "link": "http://arxiv.org/abs/2408.15038v1", "date": "2024-08-27", "relevancy": 2.2548, "topK": [{"title": "Dynamic Gaussians Mesh: Consistent Mesh Reconstruction from Monocular\n  Videos", "link": "http://arxiv.org/abs/2404.12379v2", "similarity": 0.5708}, {"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.5647}, {"title": "VirtualModel: Generating Object-ID-retentive Human-object Interaction\n  Image by Diffusion Model for E-commerce Marketing", "link": "http://arxiv.org/abs/2405.09985v1", "similarity": 0.5562}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Interactive%20Occlusion%20Boundary%20Estimation%20through%20Exploitation%20of%0A%20%20Synthetic%20Data&body=Title%3A%20Interactive%20Occlusion%20Boundary%20Estimation%20through%20Exploitation%20of%0A%20%20Synthetic%20Data%0AAuthor%3A%20Lintao%20Xu%20and%20Chaohui%20Wang%0AAbstract%3A%20%20%20Occlusion%20boundaries%20%28OBs%29%20geometrically%20localize%20the%20occlusion%20events%20in%20a%0A2D%20image%2C%20and%20contain%20useful%20information%20for%20addressing%20various%20scene%0Aunderstanding%20problems.%20To%20advance%20their%20study%2C%20we%20have%20led%20the%20investigation%0Ain%20the%20following%20three%20aspects.%20Firstly%2C%20we%20have%20studied%20interactive%20estimation%0Aof%20OBs%2C%20which%20is%20the%20first%20in%20the%20literature%2C%20and%20proposed%20an%20efficient%0Adeep-network-based%20method%20using%20multiple-scribble%20intervention%2C%20named%20DNMMSI%2C%0Awhich%20significantly%20improves%20the%20performance%20over%20the%20state-of-the-art%0Afully-automatic%20methods.%20Secondly%2C%20we%20propose%20to%20exploit%20the%20synthetic%0Abenchmark%20for%20the%20training%20process%2C%20thanks%20to%20the%20particularity%20that%20OBs%20are%0Adetermined%20geometrically%20and%20unambiguously%20from%20the%203D%20scene.%20To%20this%20end%2C%20we%0Ahave%20developed%20an%20efficient%20tool%2C%20named%20Mesh2OB%2C%20for%20the%20automatic%20generation%0Aof%202D%20images%20together%20with%20their%20ground-truth%20OBs%2C%20using%20which%20we%20have%0Aconstructed%20a%20synthetic%20benchmark%2C%20named%20OB-FUTURE.%20Abundant%20experimental%0Aresults%20demonstrate%20that%20leveraging%20such%20a%20synthetic%20benchmark%20for%20training%0Aachieves%20promising%20performance%2C%20even%20without%20the%20use%20of%20domain%20adaptation%0Atechniques.%20Finally%2C%20to%20achieve%20a%20more%20compelling%20and%20robust%20evaluation%20in%0AOB-related%20research%2C%20we%20have%20created%20a%20real%20benchmark%2C%20named%20OB-LabName%2C%0Aconsisting%20of%20120%20high-resolution%20images%20together%20with%20their%20ground-truth%20OBs%2C%0Awith%20precision%20surpassing%20that%20of%20previous%20benchmarks.%20We%20will%20release%20DNMMSI%0Awith%20pre-trained%20parameters%2C%20Mesh2OB%2C%20OB-FUTURE%2C%20and%20OB-LabName%20to%20support%0Afurther%20research.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2408.15038v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DInteractive%2520Occlusion%2520Boundary%2520Estimation%2520through%2520Exploitation%2520of%250A%2520%2520Synthetic%2520Data%26entry.906535625%3DLintao%2520Xu%2520and%2520Chaohui%2520Wang%26entry.1292438233%3D%2520%2520Occlusion%2520boundaries%2520%2528OBs%2529%2520geometrically%2520localize%2520the%2520occlusion%2520events%2520in%2520a%250A2D%2520image%252C%2520and%2520contain%2520useful%2520information%2520for%2520addressing%2520various%2520scene%250Aunderstanding%2520problems.%2520To%2520advance%2520their%2520study%252C%2520we%2520have%2520led%2520the%2520investigation%250Ain%2520the%2520following%2520three%2520aspects.%2520Firstly%252C%2520we%2520have%2520studied%2520interactive%2520estimation%250Aof%2520OBs%252C%2520which%2520is%2520the%2520first%2520in%2520the%2520literature%252C%2520and%2520proposed%2520an%2520efficient%250Adeep-network-based%2520method%2520using%2520multiple-scribble%2520intervention%252C%2520named%2520DNMMSI%252C%250Awhich%2520significantly%2520improves%2520the%2520performance%2520over%2520the%2520state-of-the-art%250Afully-automatic%2520methods.%2520Secondly%252C%2520we%2520propose%2520to%2520exploit%2520the%2520synthetic%250Abenchmark%2520for%2520the%2520training%2520process%252C%2520thanks%2520to%2520the%2520particularity%2520that%2520OBs%2520are%250Adetermined%2520geometrically%2520and%2520unambiguously%2520from%2520the%25203D%2520scene.%2520To%2520this%2520end%252C%2520we%250Ahave%2520developed%2520an%2520efficient%2520tool%252C%2520named%2520Mesh2OB%252C%2520for%2520the%2520automatic%2520generation%250Aof%25202D%2520images%2520together%2520with%2520their%2520ground-truth%2520OBs%252C%2520using%2520which%2520we%2520have%250Aconstructed%2520a%2520synthetic%2520benchmark%252C%2520named%2520OB-FUTURE.%2520Abundant%2520experimental%250Aresults%2520demonstrate%2520that%2520leveraging%2520such%2520a%2520synthetic%2520benchmark%2520for%2520training%250Aachieves%2520promising%2520performance%252C%2520even%2520without%2520the%2520use%2520of%2520domain%2520adaptation%250Atechniques.%2520Finally%252C%2520to%2520achieve%2520a%2520more%2520compelling%2520and%2520robust%2520evaluation%2520in%250AOB-related%2520research%252C%2520we%2520have%2520created%2520a%2520real%2520benchmark%252C%2520named%2520OB-LabName%252C%250Aconsisting%2520of%2520120%2520high-resolution%2520images%2520together%2520with%2520their%2520ground-truth%2520OBs%252C%250Awith%2520precision%2520surpassing%2520that%2520of%2520previous%2520benchmarks.%2520We%2520will%2520release%2520DNMMSI%250Awith%2520pre-trained%2520parameters%252C%2520Mesh2OB%252C%2520OB-FUTURE%252C%2520and%2520OB-LabName%2520to%2520support%250Afurther%2520research.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2408.15038v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Interactive%20Occlusion%20Boundary%20Estimation%20through%20Exploitation%20of%0A%20%20Synthetic%20Data&entry.906535625=Lintao%20Xu%20and%20Chaohui%20Wang&entry.1292438233=%20%20Occlusion%20boundaries%20%28OBs%29%20geometrically%20localize%20the%20occlusion%20events%20in%20a%0A2D%20image%2C%20and%20contain%20useful%20information%20for%20addressing%20various%20scene%0Aunderstanding%20problems.%20To%20advance%20their%20study%2C%20we%20have%20led%20the%20investigation%0Ain%20the%20following%20three%20aspects.%20Firstly%2C%20we%20have%20studied%20interactive%20estimation%0Aof%20OBs%2C%20which%20is%20the%20first%20in%20the%20literature%2C%20and%20proposed%20an%20efficient%0Adeep-network-based%20method%20using%20multiple-scribble%20intervention%2C%20named%20DNMMSI%2C%0Awhich%20significantly%20improves%20the%20performance%20over%20the%20state-of-the-art%0Afully-automatic%20methods.%20Secondly%2C%20we%20propose%20to%20exploit%20the%20synthetic%0Abenchmark%20for%20the%20training%20process%2C%20thanks%20to%20the%20particularity%20that%20OBs%20are%0Adetermined%20geometrically%20and%20unambiguously%20from%20the%203D%20scene.%20To%20this%20end%2C%20we%0Ahave%20developed%20an%20efficient%20tool%2C%20named%20Mesh2OB%2C%20for%20the%20automatic%20generation%0Aof%202D%20images%20together%20with%20their%20ground-truth%20OBs%2C%20using%20which%20we%20have%0Aconstructed%20a%20synthetic%20benchmark%2C%20named%20OB-FUTURE.%20Abundant%20experimental%0Aresults%20demonstrate%20that%20leveraging%20such%20a%20synthetic%20benchmark%20for%20training%0Aachieves%20promising%20performance%2C%20even%20without%20the%20use%20of%20domain%20adaptation%0Atechniques.%20Finally%2C%20to%20achieve%20a%20more%20compelling%20and%20robust%20evaluation%20in%0AOB-related%20research%2C%20we%20have%20created%20a%20real%20benchmark%2C%20named%20OB-LabName%2C%0Aconsisting%20of%20120%20high-resolution%20images%20together%20with%20their%20ground-truth%20OBs%2C%0Awith%20precision%20surpassing%20that%20of%20previous%20benchmarks.%20We%20will%20release%20DNMMSI%0Awith%20pre-trained%20parameters%2C%20Mesh2OB%2C%20OB-FUTURE%2C%20and%20OB-LabName%20to%20support%0Afurther%20research.%0A&entry.1838667208=http%3A//arxiv.org/abs/2408.15038v1&entry.124074799=Read"},
{"title": "Distance-Forward Learning: Enhancing the Forward-Forward Algorithm\n  Towards High-Performance On-Chip Learning", "author": "Yujie Wu and Siyuan Xu and Jibin Wu and Lei Deng and Mingkun Xu and Qinghao Wen and Guoqi Li", "abstract": "  The Forward-Forward (FF) algorithm was recently proposed as a local learning\nmethod to address the limitations of backpropagation (BP), offering biological\nplausibility along with memory-efficient and highly parallelized computational\nbenefits. However, it suffers from suboptimal performance and poor\ngeneralization, largely due to inadequate theoretical support and a lack of\neffective learning strategies. In this work, we reformulate FF using distance\nmetric learning and propose a distance-forward algorithm (DF) to improve FF\nperformance in supervised vision tasks while preserving its local computational\nproperties, making it competitive for efficient on-chip learning. To achieve\nthis, we reinterpret FF through the lens of centroid-based metric learning and\ndevelop a goodness-based N-pair margin loss to facilitate the learning of\ndiscriminative features. Furthermore, we integrate layer-collaboration local\nupdate strategies to reduce information loss caused by greedy local parameter\nupdates. Our method surpasses existing FF models and other advanced local\nlearning approaches, with accuracies of 99.7\\% on MNIST, 88.2\\% on CIFAR-10,\n59\\% on CIFAR-100, 95.9\\% on SVHN, and 82.5\\% on ImageNette, respectively.\nMoreover, it achieves comparable performance with less than 40\\% memory cost\ncompared to BP training, while exhibiting stronger robustness to multiple types\nof hardware-related noise, demonstrating its potential for online learning and\nenergy-efficient computation on neuromorphic chips.\n", "link": "http://arxiv.org/abs/2408.14925v1", "date": "2024-08-27", "relevancy": 2.2369, "topK": [{"title": "DDM-NET: End-to-end learning of keypoint feature Detection, Description and Matching for 3D localization", "link": "https://arxiv.org/abs/2212.04575", "similarity": 0.5674}, {"title": "SDXS: Real-Time One-Step Latent Diffusion Models with Image Conditions", "link": "http://arxiv.org/abs/2403.16627v1", "similarity": 0.558}, {"title": "ToDo: Token Downsampling for Efficient Generation of High-Resolution\n  Images", "link": "http://arxiv.org/abs/2402.13573v2", "similarity": 0.5515}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Distance-Forward%20Learning%3A%20Enhancing%20the%20Forward-Forward%20Algorithm%0A%20%20Towards%20High-Performance%20On-Chip%20Learning&body=Title%3A%20Distance-Forward%20Learning%3A%20Enhancing%20the%20Forward-Forward%20Algorithm%0A%20%20Towards%20High-Performance%20On-Chip%20Learning%0AAuthor%3A%20Yujie%20Wu%20and%20Siyuan%20Xu%20and%20Jibin%20Wu%20and%20Lei%20Deng%20and%20Mingkun%20Xu%20and%20Qinghao%20Wen%20and%20Guoqi%20Li%0AAbstract%3A%20%20%20The%20Forward-Forward%20%28FF%29%20algorithm%20was%20recently%20proposed%20as%20a%20local%20learning%0Amethod%20to%20address%20the%20limitations%20of%20backpropagation%20%28BP%29%2C%20offering%20biological%0Aplausibility%20along%20with%20memory-efficient%20and%20highly%20parallelized%20computational%0Abenefits.%20However%2C%20it%20suffers%20from%20suboptimal%20performance%20and%20poor%0Ageneralization%2C%20largely%20due%20to%20inadequate%20theoretical%20support%20and%20a%20lack%20of%0Aeffective%20learning%20strategies.%20In%20this%20work%2C%20we%20reformulate%20FF%20using%20distance%0Ametric%20learning%20and%20propose%20a%20distance-forward%20algorithm%20%28DF%29%20to%20improve%20FF%0Aperformance%20in%20supervised%20vision%20tasks%20while%20preserving%20its%20local%20computational%0Aproperties%2C%20making%20it%20competitive%20for%20efficient%20on-chip%20learning.%20To%20achieve%0Athis%2C%20we%20reinterpret%20FF%20through%20the%20lens%20of%20centroid-based%20metric%20learning%20and%0Adevelop%20a%20goodness-based%20N-pair%20margin%20loss%20to%20facilitate%20the%20learning%20of%0Adiscriminative%20features.%20Furthermore%2C%20we%20integrate%20layer-collaboration%20local%0Aupdate%20strategies%20to%20reduce%20information%20loss%20caused%20by%20greedy%20local%20parameter%0Aupdates.%20Our%20method%20surpasses%20existing%20FF%20models%20and%20other%20advanced%20local%0Alearning%20approaches%2C%20with%20accuracies%20of%2099.7%5C%25%20on%20MNIST%2C%2088.2%5C%25%20on%20CIFAR-10%2C%0A59%5C%25%20on%20CIFAR-100%2C%2095.9%5C%25%20on%20SVHN%2C%20and%2082.5%5C%25%20on%20ImageNette%2C%20respectively.%0AMoreover%2C%20it%20achieves%20comparable%20performance%20with%20less%20than%2040%5C%25%20memory%20cost%0Acompared%20to%20BP%20training%2C%20while%20exhibiting%20stronger%20robustness%20to%20multiple%20types%0Aof%20hardware-related%20noise%2C%20demonstrating%20its%20potential%20for%20online%20learning%20and%0Aenergy-efficient%20computation%20on%20neuromorphic%20chips.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2408.14925v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DDistance-Forward%2520Learning%253A%2520Enhancing%2520the%2520Forward-Forward%2520Algorithm%250A%2520%2520Towards%2520High-Performance%2520On-Chip%2520Learning%26entry.906535625%3DYujie%2520Wu%2520and%2520Siyuan%2520Xu%2520and%2520Jibin%2520Wu%2520and%2520Lei%2520Deng%2520and%2520Mingkun%2520Xu%2520and%2520Qinghao%2520Wen%2520and%2520Guoqi%2520Li%26entry.1292438233%3D%2520%2520The%2520Forward-Forward%2520%2528FF%2529%2520algorithm%2520was%2520recently%2520proposed%2520as%2520a%2520local%2520learning%250Amethod%2520to%2520address%2520the%2520limitations%2520of%2520backpropagation%2520%2528BP%2529%252C%2520offering%2520biological%250Aplausibility%2520along%2520with%2520memory-efficient%2520and%2520highly%2520parallelized%2520computational%250Abenefits.%2520However%252C%2520it%2520suffers%2520from%2520suboptimal%2520performance%2520and%2520poor%250Ageneralization%252C%2520largely%2520due%2520to%2520inadequate%2520theoretical%2520support%2520and%2520a%2520lack%2520of%250Aeffective%2520learning%2520strategies.%2520In%2520this%2520work%252C%2520we%2520reformulate%2520FF%2520using%2520distance%250Ametric%2520learning%2520and%2520propose%2520a%2520distance-forward%2520algorithm%2520%2528DF%2529%2520to%2520improve%2520FF%250Aperformance%2520in%2520supervised%2520vision%2520tasks%2520while%2520preserving%2520its%2520local%2520computational%250Aproperties%252C%2520making%2520it%2520competitive%2520for%2520efficient%2520on-chip%2520learning.%2520To%2520achieve%250Athis%252C%2520we%2520reinterpret%2520FF%2520through%2520the%2520lens%2520of%2520centroid-based%2520metric%2520learning%2520and%250Adevelop%2520a%2520goodness-based%2520N-pair%2520margin%2520loss%2520to%2520facilitate%2520the%2520learning%2520of%250Adiscriminative%2520features.%2520Furthermore%252C%2520we%2520integrate%2520layer-collaboration%2520local%250Aupdate%2520strategies%2520to%2520reduce%2520information%2520loss%2520caused%2520by%2520greedy%2520local%2520parameter%250Aupdates.%2520Our%2520method%2520surpasses%2520existing%2520FF%2520models%2520and%2520other%2520advanced%2520local%250Alearning%2520approaches%252C%2520with%2520accuracies%2520of%252099.7%255C%2525%2520on%2520MNIST%252C%252088.2%255C%2525%2520on%2520CIFAR-10%252C%250A59%255C%2525%2520on%2520CIFAR-100%252C%252095.9%255C%2525%2520on%2520SVHN%252C%2520and%252082.5%255C%2525%2520on%2520ImageNette%252C%2520respectively.%250AMoreover%252C%2520it%2520achieves%2520comparable%2520performance%2520with%2520less%2520than%252040%255C%2525%2520memory%2520cost%250Acompared%2520to%2520BP%2520training%252C%2520while%2520exhibiting%2520stronger%2520robustness%2520to%2520multiple%2520types%250Aof%2520hardware-related%2520noise%252C%2520demonstrating%2520its%2520potential%2520for%2520online%2520learning%2520and%250Aenergy-efficient%2520computation%2520on%2520neuromorphic%2520chips.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2408.14925v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Distance-Forward%20Learning%3A%20Enhancing%20the%20Forward-Forward%20Algorithm%0A%20%20Towards%20High-Performance%20On-Chip%20Learning&entry.906535625=Yujie%20Wu%20and%20Siyuan%20Xu%20and%20Jibin%20Wu%20and%20Lei%20Deng%20and%20Mingkun%20Xu%20and%20Qinghao%20Wen%20and%20Guoqi%20Li&entry.1292438233=%20%20The%20Forward-Forward%20%28FF%29%20algorithm%20was%20recently%20proposed%20as%20a%20local%20learning%0Amethod%20to%20address%20the%20limitations%20of%20backpropagation%20%28BP%29%2C%20offering%20biological%0Aplausibility%20along%20with%20memory-efficient%20and%20highly%20parallelized%20computational%0Abenefits.%20However%2C%20it%20suffers%20from%20suboptimal%20performance%20and%20poor%0Ageneralization%2C%20largely%20due%20to%20inadequate%20theoretical%20support%20and%20a%20lack%20of%0Aeffective%20learning%20strategies.%20In%20this%20work%2C%20we%20reformulate%20FF%20using%20distance%0Ametric%20learning%20and%20propose%20a%20distance-forward%20algorithm%20%28DF%29%20to%20improve%20FF%0Aperformance%20in%20supervised%20vision%20tasks%20while%20preserving%20its%20local%20computational%0Aproperties%2C%20making%20it%20competitive%20for%20efficient%20on-chip%20learning.%20To%20achieve%0Athis%2C%20we%20reinterpret%20FF%20through%20the%20lens%20of%20centroid-based%20metric%20learning%20and%0Adevelop%20a%20goodness-based%20N-pair%20margin%20loss%20to%20facilitate%20the%20learning%20of%0Adiscriminative%20features.%20Furthermore%2C%20we%20integrate%20layer-collaboration%20local%0Aupdate%20strategies%20to%20reduce%20information%20loss%20caused%20by%20greedy%20local%20parameter%0Aupdates.%20Our%20method%20surpasses%20existing%20FF%20models%20and%20other%20advanced%20local%0Alearning%20approaches%2C%20with%20accuracies%20of%2099.7%5C%25%20on%20MNIST%2C%2088.2%5C%25%20on%20CIFAR-10%2C%0A59%5C%25%20on%20CIFAR-100%2C%2095.9%5C%25%20on%20SVHN%2C%20and%2082.5%5C%25%20on%20ImageNette%2C%20respectively.%0AMoreover%2C%20it%20achieves%20comparable%20performance%20with%20less%20than%2040%5C%25%20memory%20cost%0Acompared%20to%20BP%20training%2C%20while%20exhibiting%20stronger%20robustness%20to%20multiple%20types%0Aof%20hardware-related%20noise%2C%20demonstrating%20its%20potential%20for%20online%20learning%20and%0Aenergy-efficient%20computation%20on%20neuromorphic%20chips.%0A&entry.1838667208=http%3A//arxiv.org/abs/2408.14925v1&entry.124074799=Read"},
{"title": "Cross-Modal Temporal Alignment for Event-guided Video Deblurring", "author": "Taewoo Kim and Hoonhee Cho and Kuk-Jin Yoon", "abstract": "  Video deblurring aims to enhance the quality of restored results in\nmotion-blurred videos by effectively gathering information from adjacent video\nframes to compensate for the insufficient data in a single blurred frame.\nHowever, when faced with consecutively severe motion blur situations,\nframe-based video deblurring methods often fail to find accurate temporal\ncorrespondence among neighboring video frames, leading to diminished\nperformance. To address this limitation, we aim to solve the video deblurring\ntask by leveraging an event camera with micro-second temporal resolution. To\nfully exploit the dense temporal resolution of the event camera, we propose two\nmodules: 1) Intra-frame feature enhancement operates within the exposure time\nof a single blurred frame, iteratively enhancing cross-modality features in a\nrecurrent manner to better utilize the rich temporal information of events, 2)\nInter-frame temporal feature alignment gathers valuable long-range temporal\ninformation to target frames, aggregating sharp features leveraging the\nadvantages of the events. In addition, we present a novel dataset composed of\nreal-world blurred RGB videos, corresponding sharp videos, and event data. This\ndataset serves as a valuable resource for evaluating event-guided deblurring\nmethods. We demonstrate that our proposed methods outperform state-of-the-art\nframe-based and event-based motion deblurring methods through extensive\nexperiments conducted on both synthetic and real-world deblurring datasets. The\ncode and dataset are available at https://github.com/intelpro/CMTA.\n", "link": "http://arxiv.org/abs/2408.14930v1", "date": "2024-08-27", "relevancy": 2.2271, "topK": [{"title": "Customize-A-Video: One-Shot Motion Customization of Text-to-Video\n  Diffusion Models", "link": "http://arxiv.org/abs/2402.14780v1", "similarity": 0.5751}, {"title": "Tuning-Free Noise Rectification for High Fidelity Image-to-Video\n  Generation", "link": "http://arxiv.org/abs/2403.02827v1", "similarity": 0.5598}, {"title": "Magic-Me: Identity-Specific Video Customized Diffusion", "link": "http://arxiv.org/abs/2402.09368v1", "similarity": 0.5464}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Cross-Modal%20Temporal%20Alignment%20for%20Event-guided%20Video%20Deblurring&body=Title%3A%20Cross-Modal%20Temporal%20Alignment%20for%20Event-guided%20Video%20Deblurring%0AAuthor%3A%20Taewoo%20Kim%20and%20Hoonhee%20Cho%20and%20Kuk-Jin%20Yoon%0AAbstract%3A%20%20%20Video%20deblurring%20aims%20to%20enhance%20the%20quality%20of%20restored%20results%20in%0Amotion-blurred%20videos%20by%20effectively%20gathering%20information%20from%20adjacent%20video%0Aframes%20to%20compensate%20for%20the%20insufficient%20data%20in%20a%20single%20blurred%20frame.%0AHowever%2C%20when%20faced%20with%20consecutively%20severe%20motion%20blur%20situations%2C%0Aframe-based%20video%20deblurring%20methods%20often%20fail%20to%20find%20accurate%20temporal%0Acorrespondence%20among%20neighboring%20video%20frames%2C%20leading%20to%20diminished%0Aperformance.%20To%20address%20this%20limitation%2C%20we%20aim%20to%20solve%20the%20video%20deblurring%0Atask%20by%20leveraging%20an%20event%20camera%20with%20micro-second%20temporal%20resolution.%20To%0Afully%20exploit%20the%20dense%20temporal%20resolution%20of%20the%20event%20camera%2C%20we%20propose%20two%0Amodules%3A%201%29%20Intra-frame%20feature%20enhancement%20operates%20within%20the%20exposure%20time%0Aof%20a%20single%20blurred%20frame%2C%20iteratively%20enhancing%20cross-modality%20features%20in%20a%0Arecurrent%20manner%20to%20better%20utilize%20the%20rich%20temporal%20information%20of%20events%2C%202%29%0AInter-frame%20temporal%20feature%20alignment%20gathers%20valuable%20long-range%20temporal%0Ainformation%20to%20target%20frames%2C%20aggregating%20sharp%20features%20leveraging%20the%0Aadvantages%20of%20the%20events.%20In%20addition%2C%20we%20present%20a%20novel%20dataset%20composed%20of%0Areal-world%20blurred%20RGB%20videos%2C%20corresponding%20sharp%20videos%2C%20and%20event%20data.%20This%0Adataset%20serves%20as%20a%20valuable%20resource%20for%20evaluating%20event-guided%20deblurring%0Amethods.%20We%20demonstrate%20that%20our%20proposed%20methods%20outperform%20state-of-the-art%0Aframe-based%20and%20event-based%20motion%20deblurring%20methods%20through%20extensive%0Aexperiments%20conducted%20on%20both%20synthetic%20and%20real-world%20deblurring%20datasets.%20The%0Acode%20and%20dataset%20are%20available%20at%20https%3A//github.com/intelpro/CMTA.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2408.14930v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DCross-Modal%2520Temporal%2520Alignment%2520for%2520Event-guided%2520Video%2520Deblurring%26entry.906535625%3DTaewoo%2520Kim%2520and%2520Hoonhee%2520Cho%2520and%2520Kuk-Jin%2520Yoon%26entry.1292438233%3D%2520%2520Video%2520deblurring%2520aims%2520to%2520enhance%2520the%2520quality%2520of%2520restored%2520results%2520in%250Amotion-blurred%2520videos%2520by%2520effectively%2520gathering%2520information%2520from%2520adjacent%2520video%250Aframes%2520to%2520compensate%2520for%2520the%2520insufficient%2520data%2520in%2520a%2520single%2520blurred%2520frame.%250AHowever%252C%2520when%2520faced%2520with%2520consecutively%2520severe%2520motion%2520blur%2520situations%252C%250Aframe-based%2520video%2520deblurring%2520methods%2520often%2520fail%2520to%2520find%2520accurate%2520temporal%250Acorrespondence%2520among%2520neighboring%2520video%2520frames%252C%2520leading%2520to%2520diminished%250Aperformance.%2520To%2520address%2520this%2520limitation%252C%2520we%2520aim%2520to%2520solve%2520the%2520video%2520deblurring%250Atask%2520by%2520leveraging%2520an%2520event%2520camera%2520with%2520micro-second%2520temporal%2520resolution.%2520To%250Afully%2520exploit%2520the%2520dense%2520temporal%2520resolution%2520of%2520the%2520event%2520camera%252C%2520we%2520propose%2520two%250Amodules%253A%25201%2529%2520Intra-frame%2520feature%2520enhancement%2520operates%2520within%2520the%2520exposure%2520time%250Aof%2520a%2520single%2520blurred%2520frame%252C%2520iteratively%2520enhancing%2520cross-modality%2520features%2520in%2520a%250Arecurrent%2520manner%2520to%2520better%2520utilize%2520the%2520rich%2520temporal%2520information%2520of%2520events%252C%25202%2529%250AInter-frame%2520temporal%2520feature%2520alignment%2520gathers%2520valuable%2520long-range%2520temporal%250Ainformation%2520to%2520target%2520frames%252C%2520aggregating%2520sharp%2520features%2520leveraging%2520the%250Aadvantages%2520of%2520the%2520events.%2520In%2520addition%252C%2520we%2520present%2520a%2520novel%2520dataset%2520composed%2520of%250Areal-world%2520blurred%2520RGB%2520videos%252C%2520corresponding%2520sharp%2520videos%252C%2520and%2520event%2520data.%2520This%250Adataset%2520serves%2520as%2520a%2520valuable%2520resource%2520for%2520evaluating%2520event-guided%2520deblurring%250Amethods.%2520We%2520demonstrate%2520that%2520our%2520proposed%2520methods%2520outperform%2520state-of-the-art%250Aframe-based%2520and%2520event-based%2520motion%2520deblurring%2520methods%2520through%2520extensive%250Aexperiments%2520conducted%2520on%2520both%2520synthetic%2520and%2520real-world%2520deblurring%2520datasets.%2520The%250Acode%2520and%2520dataset%2520are%2520available%2520at%2520https%253A//github.com/intelpro/CMTA.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2408.14930v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Cross-Modal%20Temporal%20Alignment%20for%20Event-guided%20Video%20Deblurring&entry.906535625=Taewoo%20Kim%20and%20Hoonhee%20Cho%20and%20Kuk-Jin%20Yoon&entry.1292438233=%20%20Video%20deblurring%20aims%20to%20enhance%20the%20quality%20of%20restored%20results%20in%0Amotion-blurred%20videos%20by%20effectively%20gathering%20information%20from%20adjacent%20video%0Aframes%20to%20compensate%20for%20the%20insufficient%20data%20in%20a%20single%20blurred%20frame.%0AHowever%2C%20when%20faced%20with%20consecutively%20severe%20motion%20blur%20situations%2C%0Aframe-based%20video%20deblurring%20methods%20often%20fail%20to%20find%20accurate%20temporal%0Acorrespondence%20among%20neighboring%20video%20frames%2C%20leading%20to%20diminished%0Aperformance.%20To%20address%20this%20limitation%2C%20we%20aim%20to%20solve%20the%20video%20deblurring%0Atask%20by%20leveraging%20an%20event%20camera%20with%20micro-second%20temporal%20resolution.%20To%0Afully%20exploit%20the%20dense%20temporal%20resolution%20of%20the%20event%20camera%2C%20we%20propose%20two%0Amodules%3A%201%29%20Intra-frame%20feature%20enhancement%20operates%20within%20the%20exposure%20time%0Aof%20a%20single%20blurred%20frame%2C%20iteratively%20enhancing%20cross-modality%20features%20in%20a%0Arecurrent%20manner%20to%20better%20utilize%20the%20rich%20temporal%20information%20of%20events%2C%202%29%0AInter-frame%20temporal%20feature%20alignment%20gathers%20valuable%20long-range%20temporal%0Ainformation%20to%20target%20frames%2C%20aggregating%20sharp%20features%20leveraging%20the%0Aadvantages%20of%20the%20events.%20In%20addition%2C%20we%20present%20a%20novel%20dataset%20composed%20of%0Areal-world%20blurred%20RGB%20videos%2C%20corresponding%20sharp%20videos%2C%20and%20event%20data.%20This%0Adataset%20serves%20as%20a%20valuable%20resource%20for%20evaluating%20event-guided%20deblurring%0Amethods.%20We%20demonstrate%20that%20our%20proposed%20methods%20outperform%20state-of-the-art%0Aframe-based%20and%20event-based%20motion%20deblurring%20methods%20through%20extensive%0Aexperiments%20conducted%20on%20both%20synthetic%20and%20real-world%20deblurring%20datasets.%20The%0Acode%20and%20dataset%20are%20available%20at%20https%3A//github.com/intelpro/CMTA.%0A&entry.1838667208=http%3A//arxiv.org/abs/2408.14930v1&entry.124074799=Read"},
{"title": "DocLayLLM: An Efficient and Effective Multi-modal Extension of Large\n  Language Models for Text-rich Document Understanding", "author": "Wenhui Liao and Jiapeng Wang and Hongliang Li and Chengyu Wang and Jun Huang and Lianwen Jin", "abstract": "  Text-rich document understanding (TDU) refers to analyzing and comprehending\ndocuments containing substantial textual content. With the rapid evolution of\nlarge language models (LLMs), they have been widely leveraged for TDU due to\ntheir remarkable versatility and generalization. In this paper, we introduce\nDocLayLLM, an efficient and effective multi-modal extension of LLMs\nspecifically designed for TDU. By integrating visual patch tokens and 2D\npositional tokens into LLMs and encoding the document content using the LLMs\nthemselves, we fully take advantage of the document comprehension capability of\nLLMs and enhance their perception of OCR information. We have also deeply\nconsidered the role of the chain-of-thought (CoT) and innovatively proposed the\ntechniques of CoT Pre-training and CoT Annealing. Our DocLayLLM can achieve\nremarkable performances with lightweight training settings, showcasing its\nefficiency and effectiveness. Experimental results demonstrate that our\nDocLayLLM surpasses existing OCR-dependent methods and also outperforms\nOCR-free competitors.\n", "link": "http://arxiv.org/abs/2408.15045v1", "date": "2024-08-27", "relevancy": 2.2078, "topK": [{"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.589}, {"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.5641}, {"title": "ToDo: Token Downsampling for Efficient Generation of High-Resolution\n  Images", "link": "http://arxiv.org/abs/2402.13573v2", "similarity": 0.5101}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20DocLayLLM%3A%20An%20Efficient%20and%20Effective%20Multi-modal%20Extension%20of%20Large%0A%20%20Language%20Models%20for%20Text-rich%20Document%20Understanding&body=Title%3A%20DocLayLLM%3A%20An%20Efficient%20and%20Effective%20Multi-modal%20Extension%20of%20Large%0A%20%20Language%20Models%20for%20Text-rich%20Document%20Understanding%0AAuthor%3A%20Wenhui%20Liao%20and%20Jiapeng%20Wang%20and%20Hongliang%20Li%20and%20Chengyu%20Wang%20and%20Jun%20Huang%20and%20Lianwen%20Jin%0AAbstract%3A%20%20%20Text-rich%20document%20understanding%20%28TDU%29%20refers%20to%20analyzing%20and%20comprehending%0Adocuments%20containing%20substantial%20textual%20content.%20With%20the%20rapid%20evolution%20of%0Alarge%20language%20models%20%28LLMs%29%2C%20they%20have%20been%20widely%20leveraged%20for%20TDU%20due%20to%0Atheir%20remarkable%20versatility%20and%20generalization.%20In%20this%20paper%2C%20we%20introduce%0ADocLayLLM%2C%20an%20efficient%20and%20effective%20multi-modal%20extension%20of%20LLMs%0Aspecifically%20designed%20for%20TDU.%20By%20integrating%20visual%20patch%20tokens%20and%202D%0Apositional%20tokens%20into%20LLMs%20and%20encoding%20the%20document%20content%20using%20the%20LLMs%0Athemselves%2C%20we%20fully%20take%20advantage%20of%20the%20document%20comprehension%20capability%20of%0ALLMs%20and%20enhance%20their%20perception%20of%20OCR%20information.%20We%20have%20also%20deeply%0Aconsidered%20the%20role%20of%20the%20chain-of-thought%20%28CoT%29%20and%20innovatively%20proposed%20the%0Atechniques%20of%20CoT%20Pre-training%20and%20CoT%20Annealing.%20Our%20DocLayLLM%20can%20achieve%0Aremarkable%20performances%20with%20lightweight%20training%20settings%2C%20showcasing%20its%0Aefficiency%20and%20effectiveness.%20Experimental%20results%20demonstrate%20that%20our%0ADocLayLLM%20surpasses%20existing%20OCR-dependent%20methods%20and%20also%20outperforms%0AOCR-free%20competitors.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2408.15045v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DDocLayLLM%253A%2520An%2520Efficient%2520and%2520Effective%2520Multi-modal%2520Extension%2520of%2520Large%250A%2520%2520Language%2520Models%2520for%2520Text-rich%2520Document%2520Understanding%26entry.906535625%3DWenhui%2520Liao%2520and%2520Jiapeng%2520Wang%2520and%2520Hongliang%2520Li%2520and%2520Chengyu%2520Wang%2520and%2520Jun%2520Huang%2520and%2520Lianwen%2520Jin%26entry.1292438233%3D%2520%2520Text-rich%2520document%2520understanding%2520%2528TDU%2529%2520refers%2520to%2520analyzing%2520and%2520comprehending%250Adocuments%2520containing%2520substantial%2520textual%2520content.%2520With%2520the%2520rapid%2520evolution%2520of%250Alarge%2520language%2520models%2520%2528LLMs%2529%252C%2520they%2520have%2520been%2520widely%2520leveraged%2520for%2520TDU%2520due%2520to%250Atheir%2520remarkable%2520versatility%2520and%2520generalization.%2520In%2520this%2520paper%252C%2520we%2520introduce%250ADocLayLLM%252C%2520an%2520efficient%2520and%2520effective%2520multi-modal%2520extension%2520of%2520LLMs%250Aspecifically%2520designed%2520for%2520TDU.%2520By%2520integrating%2520visual%2520patch%2520tokens%2520and%25202D%250Apositional%2520tokens%2520into%2520LLMs%2520and%2520encoding%2520the%2520document%2520content%2520using%2520the%2520LLMs%250Athemselves%252C%2520we%2520fully%2520take%2520advantage%2520of%2520the%2520document%2520comprehension%2520capability%2520of%250ALLMs%2520and%2520enhance%2520their%2520perception%2520of%2520OCR%2520information.%2520We%2520have%2520also%2520deeply%250Aconsidered%2520the%2520role%2520of%2520the%2520chain-of-thought%2520%2528CoT%2529%2520and%2520innovatively%2520proposed%2520the%250Atechniques%2520of%2520CoT%2520Pre-training%2520and%2520CoT%2520Annealing.%2520Our%2520DocLayLLM%2520can%2520achieve%250Aremarkable%2520performances%2520with%2520lightweight%2520training%2520settings%252C%2520showcasing%2520its%250Aefficiency%2520and%2520effectiveness.%2520Experimental%2520results%2520demonstrate%2520that%2520our%250ADocLayLLM%2520surpasses%2520existing%2520OCR-dependent%2520methods%2520and%2520also%2520outperforms%250AOCR-free%2520competitors.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2408.15045v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=DocLayLLM%3A%20An%20Efficient%20and%20Effective%20Multi-modal%20Extension%20of%20Large%0A%20%20Language%20Models%20for%20Text-rich%20Document%20Understanding&entry.906535625=Wenhui%20Liao%20and%20Jiapeng%20Wang%20and%20Hongliang%20Li%20and%20Chengyu%20Wang%20and%20Jun%20Huang%20and%20Lianwen%20Jin&entry.1292438233=%20%20Text-rich%20document%20understanding%20%28TDU%29%20refers%20to%20analyzing%20and%20comprehending%0Adocuments%20containing%20substantial%20textual%20content.%20With%20the%20rapid%20evolution%20of%0Alarge%20language%20models%20%28LLMs%29%2C%20they%20have%20been%20widely%20leveraged%20for%20TDU%20due%20to%0Atheir%20remarkable%20versatility%20and%20generalization.%20In%20this%20paper%2C%20we%20introduce%0ADocLayLLM%2C%20an%20efficient%20and%20effective%20multi-modal%20extension%20of%20LLMs%0Aspecifically%20designed%20for%20TDU.%20By%20integrating%20visual%20patch%20tokens%20and%202D%0Apositional%20tokens%20into%20LLMs%20and%20encoding%20the%20document%20content%20using%20the%20LLMs%0Athemselves%2C%20we%20fully%20take%20advantage%20of%20the%20document%20comprehension%20capability%20of%0ALLMs%20and%20enhance%20their%20perception%20of%20OCR%20information.%20We%20have%20also%20deeply%0Aconsidered%20the%20role%20of%20the%20chain-of-thought%20%28CoT%29%20and%20innovatively%20proposed%20the%0Atechniques%20of%20CoT%20Pre-training%20and%20CoT%20Annealing.%20Our%20DocLayLLM%20can%20achieve%0Aremarkable%20performances%20with%20lightweight%20training%20settings%2C%20showcasing%20its%0Aefficiency%20and%20effectiveness.%20Experimental%20results%20demonstrate%20that%20our%0ADocLayLLM%20surpasses%20existing%20OCR-dependent%20methods%20and%20also%20outperforms%0AOCR-free%20competitors.%0A&entry.1838667208=http%3A//arxiv.org/abs/2408.15045v1&entry.124074799=Read"},
{"title": "TAAT: Think and Act from Arbitrary Texts in Text2Motion", "author": "Runqi Wang and Caoyuan Ma and Guopeng Li and Zheng Wang", "abstract": "  Text to Motion aims to generate human motions from texts. Existing settings\nassume that texts include action labels, which limits flexibility in practical\nscenarios. This paper extends this task with a more realistic assumption that\nthe texts are arbitrary. Specifically, in our setting, arbitrary texts include\nexisting action texts composed of action labels and introduce scene texts\nwithout explicit action labels. To address this practical issue, we extend the\naction texts in the HUMANML3D dataset by incorporating additional scene texts,\nthereby creating a new dataset, HUMANML3D++. Concurrently, we propose a simple\nframework that extracts action representations from arbitrary texts using a\nLarge Language Model (LLM) and subsequently generates motions. Furthermore, we\nenhance the existing evaluation methodologies to address their inadequacies.\nExtensive experiments are conducted under different application scenarios to\nvalidate the effectiveness of the proposed framework on existing and proposed\ndatasets. The results indicate that Text to Motion in this realistic setting is\nvery challenging, fostering new research in this practical direction. Our\ndataset and code will be released.\n", "link": "http://arxiv.org/abs/2404.14745v3", "date": "2024-08-27", "relevancy": 2.189, "topK": [{"title": "Customize-A-Video: One-Shot Motion Customization of Text-to-Video\n  Diffusion Models", "link": "http://arxiv.org/abs/2402.14780v1", "similarity": 0.5959}, {"title": "DressCode: Autoregressively Sewing and Generating Garments from Text\n  Guidance", "link": "http://arxiv.org/abs/2401.16465v3", "similarity": 0.5412}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5338}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20TAAT%3A%20Think%20and%20Act%20from%20Arbitrary%20Texts%20in%20Text2Motion&body=Title%3A%20TAAT%3A%20Think%20and%20Act%20from%20Arbitrary%20Texts%20in%20Text2Motion%0AAuthor%3A%20Runqi%20Wang%20and%20Caoyuan%20Ma%20and%20Guopeng%20Li%20and%20Zheng%20Wang%0AAbstract%3A%20%20%20Text%20to%20Motion%20aims%20to%20generate%20human%20motions%20from%20texts.%20Existing%20settings%0Aassume%20that%20texts%20include%20action%20labels%2C%20which%20limits%20flexibility%20in%20practical%0Ascenarios.%20This%20paper%20extends%20this%20task%20with%20a%20more%20realistic%20assumption%20that%0Athe%20texts%20are%20arbitrary.%20Specifically%2C%20in%20our%20setting%2C%20arbitrary%20texts%20include%0Aexisting%20action%20texts%20composed%20of%20action%20labels%20and%20introduce%20scene%20texts%0Awithout%20explicit%20action%20labels.%20To%20address%20this%20practical%20issue%2C%20we%20extend%20the%0Aaction%20texts%20in%20the%20HUMANML3D%20dataset%20by%20incorporating%20additional%20scene%20texts%2C%0Athereby%20creating%20a%20new%20dataset%2C%20HUMANML3D%2B%2B.%20Concurrently%2C%20we%20propose%20a%20simple%0Aframework%20that%20extracts%20action%20representations%20from%20arbitrary%20texts%20using%20a%0ALarge%20Language%20Model%20%28LLM%29%20and%20subsequently%20generates%20motions.%20Furthermore%2C%20we%0Aenhance%20the%20existing%20evaluation%20methodologies%20to%20address%20their%20inadequacies.%0AExtensive%20experiments%20are%20conducted%20under%20different%20application%20scenarios%20to%0Avalidate%20the%20effectiveness%20of%20the%20proposed%20framework%20on%20existing%20and%20proposed%0Adatasets.%20The%20results%20indicate%20that%20Text%20to%20Motion%20in%20this%20realistic%20setting%20is%0Avery%20challenging%2C%20fostering%20new%20research%20in%20this%20practical%20direction.%20Our%0Adataset%20and%20code%20will%20be%20released.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2404.14745v3%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DTAAT%253A%2520Think%2520and%2520Act%2520from%2520Arbitrary%2520Texts%2520in%2520Text2Motion%26entry.906535625%3DRunqi%2520Wang%2520and%2520Caoyuan%2520Ma%2520and%2520Guopeng%2520Li%2520and%2520Zheng%2520Wang%26entry.1292438233%3D%2520%2520Text%2520to%2520Motion%2520aims%2520to%2520generate%2520human%2520motions%2520from%2520texts.%2520Existing%2520settings%250Aassume%2520that%2520texts%2520include%2520action%2520labels%252C%2520which%2520limits%2520flexibility%2520in%2520practical%250Ascenarios.%2520This%2520paper%2520extends%2520this%2520task%2520with%2520a%2520more%2520realistic%2520assumption%2520that%250Athe%2520texts%2520are%2520arbitrary.%2520Specifically%252C%2520in%2520our%2520setting%252C%2520arbitrary%2520texts%2520include%250Aexisting%2520action%2520texts%2520composed%2520of%2520action%2520labels%2520and%2520introduce%2520scene%2520texts%250Awithout%2520explicit%2520action%2520labels.%2520To%2520address%2520this%2520practical%2520issue%252C%2520we%2520extend%2520the%250Aaction%2520texts%2520in%2520the%2520HUMANML3D%2520dataset%2520by%2520incorporating%2520additional%2520scene%2520texts%252C%250Athereby%2520creating%2520a%2520new%2520dataset%252C%2520HUMANML3D%252B%252B.%2520Concurrently%252C%2520we%2520propose%2520a%2520simple%250Aframework%2520that%2520extracts%2520action%2520representations%2520from%2520arbitrary%2520texts%2520using%2520a%250ALarge%2520Language%2520Model%2520%2528LLM%2529%2520and%2520subsequently%2520generates%2520motions.%2520Furthermore%252C%2520we%250Aenhance%2520the%2520existing%2520evaluation%2520methodologies%2520to%2520address%2520their%2520inadequacies.%250AExtensive%2520experiments%2520are%2520conducted%2520under%2520different%2520application%2520scenarios%2520to%250Avalidate%2520the%2520effectiveness%2520of%2520the%2520proposed%2520framework%2520on%2520existing%2520and%2520proposed%250Adatasets.%2520The%2520results%2520indicate%2520that%2520Text%2520to%2520Motion%2520in%2520this%2520realistic%2520setting%2520is%250Avery%2520challenging%252C%2520fostering%2520new%2520research%2520in%2520this%2520practical%2520direction.%2520Our%250Adataset%2520and%2520code%2520will%2520be%2520released.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2404.14745v3%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=TAAT%3A%20Think%20and%20Act%20from%20Arbitrary%20Texts%20in%20Text2Motion&entry.906535625=Runqi%20Wang%20and%20Caoyuan%20Ma%20and%20Guopeng%20Li%20and%20Zheng%20Wang&entry.1292438233=%20%20Text%20to%20Motion%20aims%20to%20generate%20human%20motions%20from%20texts.%20Existing%20settings%0Aassume%20that%20texts%20include%20action%20labels%2C%20which%20limits%20flexibility%20in%20practical%0Ascenarios.%20This%20paper%20extends%20this%20task%20with%20a%20more%20realistic%20assumption%20that%0Athe%20texts%20are%20arbitrary.%20Specifically%2C%20in%20our%20setting%2C%20arbitrary%20texts%20include%0Aexisting%20action%20texts%20composed%20of%20action%20labels%20and%20introduce%20scene%20texts%0Awithout%20explicit%20action%20labels.%20To%20address%20this%20practical%20issue%2C%20we%20extend%20the%0Aaction%20texts%20in%20the%20HUMANML3D%20dataset%20by%20incorporating%20additional%20scene%20texts%2C%0Athereby%20creating%20a%20new%20dataset%2C%20HUMANML3D%2B%2B.%20Concurrently%2C%20we%20propose%20a%20simple%0Aframework%20that%20extracts%20action%20representations%20from%20arbitrary%20texts%20using%20a%0ALarge%20Language%20Model%20%28LLM%29%20and%20subsequently%20generates%20motions.%20Furthermore%2C%20we%0Aenhance%20the%20existing%20evaluation%20methodologies%20to%20address%20their%20inadequacies.%0AExtensive%20experiments%20are%20conducted%20under%20different%20application%20scenarios%20to%0Avalidate%20the%20effectiveness%20of%20the%20proposed%20framework%20on%20existing%20and%20proposed%0Adatasets.%20The%20results%20indicate%20that%20Text%20to%20Motion%20in%20this%20realistic%20setting%20is%0Avery%20challenging%2C%20fostering%20new%20research%20in%20this%20practical%20direction.%20Our%0Adataset%20and%20code%20will%20be%20released.%0A&entry.1838667208=http%3A//arxiv.org/abs/2404.14745v3&entry.124074799=Read"},
{"title": "NeuralOOD: Improving Out-of-Distribution Generalization Performance with\n  Brain-machine Fusion Learning Framework", "author": "Shuangchen Zhao and Changde Du and Hui Li and Huiguang He", "abstract": "  Deep Neural Networks (DNNs) have demonstrated exceptional recognition\ncapabilities in traditional computer vision (CV) tasks. However, existing CV\nmodels often suffer a significant decrease in accuracy when confronted with\nout-of-distribution (OOD) data. In contrast to these DNN models, human can\nmaintain a consistently low error rate when facing OOD scenes, partly\nattributed to the rich prior cognitive knowledge stored in the human brain.\nPrevious OOD generalization researches only focus on the single modal,\noverlooking the advantages of multimodal learning method. In this paper, we\nutilize the multimodal learning method to improve the OOD generalization and\npropose a novel Brain-machine Fusion Learning (BMFL) framework. We adopt the\ncross-attention mechanism to fuse the visual knowledge from CV model and prior\ncognitive knowledge from the human brain. Specially, we employ a pre-trained\nvisual neural encoding model to predict the functional Magnetic Resonance\nImaging (fMRI) from visual features which eliminates the need for the fMRI data\ncollection and pre-processing, effectively reduces the workload associated with\nconventional BMFL methods. Furthermore, we construct a brain transformer to\nfacilitate the extraction of knowledge inside the fMRI data. Moreover, we\nintroduce the Pearson correlation coefficient maximization regularization\nmethod into the training process, which improves the fusion capability with\nbetter constrains. Our model outperforms the DINOv2 and baseline models on the\nImageNet-1k validation dataset as well as six curated OOD datasets, showcasing\nits superior performance in diverse scenarios.\n", "link": "http://arxiv.org/abs/2408.14950v1", "date": "2024-08-27", "relevancy": 2.1684, "topK": [{"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5441}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.5409}, {"title": "Boosted dynamic neural networks", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/26302", "similarity": 0.5405}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20NeuralOOD%3A%20Improving%20Out-of-Distribution%20Generalization%20Performance%20with%0A%20%20Brain-machine%20Fusion%20Learning%20Framework&body=Title%3A%20NeuralOOD%3A%20Improving%20Out-of-Distribution%20Generalization%20Performance%20with%0A%20%20Brain-machine%20Fusion%20Learning%20Framework%0AAuthor%3A%20Shuangchen%20Zhao%20and%20Changde%20Du%20and%20Hui%20Li%20and%20Huiguang%20He%0AAbstract%3A%20%20%20Deep%20Neural%20Networks%20%28DNNs%29%20have%20demonstrated%20exceptional%20recognition%0Acapabilities%20in%20traditional%20computer%20vision%20%28CV%29%20tasks.%20However%2C%20existing%20CV%0Amodels%20often%20suffer%20a%20significant%20decrease%20in%20accuracy%20when%20confronted%20with%0Aout-of-distribution%20%28OOD%29%20data.%20In%20contrast%20to%20these%20DNN%20models%2C%20human%20can%0Amaintain%20a%20consistently%20low%20error%20rate%20when%20facing%20OOD%20scenes%2C%20partly%0Aattributed%20to%20the%20rich%20prior%20cognitive%20knowledge%20stored%20in%20the%20human%20brain.%0APrevious%20OOD%20generalization%20researches%20only%20focus%20on%20the%20single%20modal%2C%0Aoverlooking%20the%20advantages%20of%20multimodal%20learning%20method.%20In%20this%20paper%2C%20we%0Autilize%20the%20multimodal%20learning%20method%20to%20improve%20the%20OOD%20generalization%20and%0Apropose%20a%20novel%20Brain-machine%20Fusion%20Learning%20%28BMFL%29%20framework.%20We%20adopt%20the%0Across-attention%20mechanism%20to%20fuse%20the%20visual%20knowledge%20from%20CV%20model%20and%20prior%0Acognitive%20knowledge%20from%20the%20human%20brain.%20Specially%2C%20we%20employ%20a%20pre-trained%0Avisual%20neural%20encoding%20model%20to%20predict%20the%20functional%20Magnetic%20Resonance%0AImaging%20%28fMRI%29%20from%20visual%20features%20which%20eliminates%20the%20need%20for%20the%20fMRI%20data%0Acollection%20and%20pre-processing%2C%20effectively%20reduces%20the%20workload%20associated%20with%0Aconventional%20BMFL%20methods.%20Furthermore%2C%20we%20construct%20a%20brain%20transformer%20to%0Afacilitate%20the%20extraction%20of%20knowledge%20inside%20the%20fMRI%20data.%20Moreover%2C%20we%0Aintroduce%20the%20Pearson%20correlation%20coefficient%20maximization%20regularization%0Amethod%20into%20the%20training%20process%2C%20which%20improves%20the%20fusion%20capability%20with%0Abetter%20constrains.%20Our%20model%20outperforms%20the%20DINOv2%20and%20baseline%20models%20on%20the%0AImageNet-1k%20validation%20dataset%20as%20well%20as%20six%20curated%20OOD%20datasets%2C%20showcasing%0Aits%20superior%20performance%20in%20diverse%20scenarios.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2408.14950v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DNeuralOOD%253A%2520Improving%2520Out-of-Distribution%2520Generalization%2520Performance%2520with%250A%2520%2520Brain-machine%2520Fusion%2520Learning%2520Framework%26entry.906535625%3DShuangchen%2520Zhao%2520and%2520Changde%2520Du%2520and%2520Hui%2520Li%2520and%2520Huiguang%2520He%26entry.1292438233%3D%2520%2520Deep%2520Neural%2520Networks%2520%2528DNNs%2529%2520have%2520demonstrated%2520exceptional%2520recognition%250Acapabilities%2520in%2520traditional%2520computer%2520vision%2520%2528CV%2529%2520tasks.%2520However%252C%2520existing%2520CV%250Amodels%2520often%2520suffer%2520a%2520significant%2520decrease%2520in%2520accuracy%2520when%2520confronted%2520with%250Aout-of-distribution%2520%2528OOD%2529%2520data.%2520In%2520contrast%2520to%2520these%2520DNN%2520models%252C%2520human%2520can%250Amaintain%2520a%2520consistently%2520low%2520error%2520rate%2520when%2520facing%2520OOD%2520scenes%252C%2520partly%250Aattributed%2520to%2520the%2520rich%2520prior%2520cognitive%2520knowledge%2520stored%2520in%2520the%2520human%2520brain.%250APrevious%2520OOD%2520generalization%2520researches%2520only%2520focus%2520on%2520the%2520single%2520modal%252C%250Aoverlooking%2520the%2520advantages%2520of%2520multimodal%2520learning%2520method.%2520In%2520this%2520paper%252C%2520we%250Autilize%2520the%2520multimodal%2520learning%2520method%2520to%2520improve%2520the%2520OOD%2520generalization%2520and%250Apropose%2520a%2520novel%2520Brain-machine%2520Fusion%2520Learning%2520%2528BMFL%2529%2520framework.%2520We%2520adopt%2520the%250Across-attention%2520mechanism%2520to%2520fuse%2520the%2520visual%2520knowledge%2520from%2520CV%2520model%2520and%2520prior%250Acognitive%2520knowledge%2520from%2520the%2520human%2520brain.%2520Specially%252C%2520we%2520employ%2520a%2520pre-trained%250Avisual%2520neural%2520encoding%2520model%2520to%2520predict%2520the%2520functional%2520Magnetic%2520Resonance%250AImaging%2520%2528fMRI%2529%2520from%2520visual%2520features%2520which%2520eliminates%2520the%2520need%2520for%2520the%2520fMRI%2520data%250Acollection%2520and%2520pre-processing%252C%2520effectively%2520reduces%2520the%2520workload%2520associated%2520with%250Aconventional%2520BMFL%2520methods.%2520Furthermore%252C%2520we%2520construct%2520a%2520brain%2520transformer%2520to%250Afacilitate%2520the%2520extraction%2520of%2520knowledge%2520inside%2520the%2520fMRI%2520data.%2520Moreover%252C%2520we%250Aintroduce%2520the%2520Pearson%2520correlation%2520coefficient%2520maximization%2520regularization%250Amethod%2520into%2520the%2520training%2520process%252C%2520which%2520improves%2520the%2520fusion%2520capability%2520with%250Abetter%2520constrains.%2520Our%2520model%2520outperforms%2520the%2520DINOv2%2520and%2520baseline%2520models%2520on%2520the%250AImageNet-1k%2520validation%2520dataset%2520as%2520well%2520as%2520six%2520curated%2520OOD%2520datasets%252C%2520showcasing%250Aits%2520superior%2520performance%2520in%2520diverse%2520scenarios.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2408.14950v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=NeuralOOD%3A%20Improving%20Out-of-Distribution%20Generalization%20Performance%20with%0A%20%20Brain-machine%20Fusion%20Learning%20Framework&entry.906535625=Shuangchen%20Zhao%20and%20Changde%20Du%20and%20Hui%20Li%20and%20Huiguang%20He&entry.1292438233=%20%20Deep%20Neural%20Networks%20%28DNNs%29%20have%20demonstrated%20exceptional%20recognition%0Acapabilities%20in%20traditional%20computer%20vision%20%28CV%29%20tasks.%20However%2C%20existing%20CV%0Amodels%20often%20suffer%20a%20significant%20decrease%20in%20accuracy%20when%20confronted%20with%0Aout-of-distribution%20%28OOD%29%20data.%20In%20contrast%20to%20these%20DNN%20models%2C%20human%20can%0Amaintain%20a%20consistently%20low%20error%20rate%20when%20facing%20OOD%20scenes%2C%20partly%0Aattributed%20to%20the%20rich%20prior%20cognitive%20knowledge%20stored%20in%20the%20human%20brain.%0APrevious%20OOD%20generalization%20researches%20only%20focus%20on%20the%20single%20modal%2C%0Aoverlooking%20the%20advantages%20of%20multimodal%20learning%20method.%20In%20this%20paper%2C%20we%0Autilize%20the%20multimodal%20learning%20method%20to%20improve%20the%20OOD%20generalization%20and%0Apropose%20a%20novel%20Brain-machine%20Fusion%20Learning%20%28BMFL%29%20framework.%20We%20adopt%20the%0Across-attention%20mechanism%20to%20fuse%20the%20visual%20knowledge%20from%20CV%20model%20and%20prior%0Acognitive%20knowledge%20from%20the%20human%20brain.%20Specially%2C%20we%20employ%20a%20pre-trained%0Avisual%20neural%20encoding%20model%20to%20predict%20the%20functional%20Magnetic%20Resonance%0AImaging%20%28fMRI%29%20from%20visual%20features%20which%20eliminates%20the%20need%20for%20the%20fMRI%20data%0Acollection%20and%20pre-processing%2C%20effectively%20reduces%20the%20workload%20associated%20with%0Aconventional%20BMFL%20methods.%20Furthermore%2C%20we%20construct%20a%20brain%20transformer%20to%0Afacilitate%20the%20extraction%20of%20knowledge%20inside%20the%20fMRI%20data.%20Moreover%2C%20we%0Aintroduce%20the%20Pearson%20correlation%20coefficient%20maximization%20regularization%0Amethod%20into%20the%20training%20process%2C%20which%20improves%20the%20fusion%20capability%20with%0Abetter%20constrains.%20Our%20model%20outperforms%20the%20DINOv2%20and%20baseline%20models%20on%20the%0AImageNet-1k%20validation%20dataset%20as%20well%20as%20six%20curated%20OOD%20datasets%2C%20showcasing%0Aits%20superior%20performance%20in%20diverse%20scenarios.%0A&entry.1838667208=http%3A//arxiv.org/abs/2408.14950v1&entry.124074799=Read"},
{"title": "Unsupervised Domain Adaptation via Style-Aware Self-intermediate Domain", "author": "Lianyu Wang and Meng Wang and Daoqiang Zhang and Huazhu Fu", "abstract": "  Unsupervised domain adaptation (UDA) has attracted considerable attention,\nwhich transfers knowledge from a label-rich source domain to a related but\nunlabeled target domain. Reducing inter-domain differences has always been a\ncrucial factor to improve performance in UDA, especially for tasks where there\nis a large gap between source and target domains. To this end, we propose a\nnovel style-aware feature fusion method (SAFF) to bridge the large domain gap\nand transfer knowledge while alleviating the loss of class-discriminative\ninformation. Inspired by the human transitive inference and learning ability, a\nnovel style-aware self-intermediate domain (SSID) is investigated to link two\nseemingly unrelated concepts through a series of intermediate auxiliary\nsynthesized concepts. Specifically, we propose a novel learning strategy of\nSSID, which selects samples from both source and target domains as anchors, and\nthen randomly fuses the object and style features of these anchors to generate\nlabeled and style-rich intermediate auxiliary features for knowledge transfer.\nMoreover, we design an external memory bank to store and update specified\nlabeled features to obtain stable class features and class-wise style features.\nBased on the proposed memory bank, the intra- and inter-domain loss functions\nare designed to improve the class recognition ability and feature\ncompatibility, respectively. Meanwhile, we simulate the rich latent feature\nspace of SSID by infinite sampling and the convergence of the loss function by\nmathematical theory. Finally, we conduct comprehensive experiments on commonly\nused domain adaptive benchmarks to evaluate the proposed SAFF, and the\nexperimental results show that the proposed SAFF can be easily combined with\ndifferent backbone networks and obtain better performance as a plug-in-plug-out\nmodule.\n", "link": "http://arxiv.org/abs/2209.01870v2", "date": "2024-08-27", "relevancy": 2.1611, "topK": [{"title": "Self-supervised Photographic Image Layout Representation Learning", "link": "http://arxiv.org/abs/2403.03740v1", "similarity": 0.5699}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.525}, {"title": "SDXS: Real-Time One-Step Latent Diffusion Models with Image Conditions", "link": "http://arxiv.org/abs/2403.16627v1", "similarity": 0.5045}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Unsupervised%20Domain%20Adaptation%20via%20Style-Aware%20Self-intermediate%20Domain&body=Title%3A%20Unsupervised%20Domain%20Adaptation%20via%20Style-Aware%20Self-intermediate%20Domain%0AAuthor%3A%20Lianyu%20Wang%20and%20Meng%20Wang%20and%20Daoqiang%20Zhang%20and%20Huazhu%20Fu%0AAbstract%3A%20%20%20Unsupervised%20domain%20adaptation%20%28UDA%29%20has%20attracted%20considerable%20attention%2C%0Awhich%20transfers%20knowledge%20from%20a%20label-rich%20source%20domain%20to%20a%20related%20but%0Aunlabeled%20target%20domain.%20Reducing%20inter-domain%20differences%20has%20always%20been%20a%0Acrucial%20factor%20to%20improve%20performance%20in%20UDA%2C%20especially%20for%20tasks%20where%20there%0Ais%20a%20large%20gap%20between%20source%20and%20target%20domains.%20To%20this%20end%2C%20we%20propose%20a%0Anovel%20style-aware%20feature%20fusion%20method%20%28SAFF%29%20to%20bridge%20the%20large%20domain%20gap%0Aand%20transfer%20knowledge%20while%20alleviating%20the%20loss%20of%20class-discriminative%0Ainformation.%20Inspired%20by%20the%20human%20transitive%20inference%20and%20learning%20ability%2C%20a%0Anovel%20style-aware%20self-intermediate%20domain%20%28SSID%29%20is%20investigated%20to%20link%20two%0Aseemingly%20unrelated%20concepts%20through%20a%20series%20of%20intermediate%20auxiliary%0Asynthesized%20concepts.%20Specifically%2C%20we%20propose%20a%20novel%20learning%20strategy%20of%0ASSID%2C%20which%20selects%20samples%20from%20both%20source%20and%20target%20domains%20as%20anchors%2C%20and%0Athen%20randomly%20fuses%20the%20object%20and%20style%20features%20of%20these%20anchors%20to%20generate%0Alabeled%20and%20style-rich%20intermediate%20auxiliary%20features%20for%20knowledge%20transfer.%0AMoreover%2C%20we%20design%20an%20external%20memory%20bank%20to%20store%20and%20update%20specified%0Alabeled%20features%20to%20obtain%20stable%20class%20features%20and%20class-wise%20style%20features.%0ABased%20on%20the%20proposed%20memory%20bank%2C%20the%20intra-%20and%20inter-domain%20loss%20functions%0Aare%20designed%20to%20improve%20the%20class%20recognition%20ability%20and%20feature%0Acompatibility%2C%20respectively.%20Meanwhile%2C%20we%20simulate%20the%20rich%20latent%20feature%0Aspace%20of%20SSID%20by%20infinite%20sampling%20and%20the%20convergence%20of%20the%20loss%20function%20by%0Amathematical%20theory.%20Finally%2C%20we%20conduct%20comprehensive%20experiments%20on%20commonly%0Aused%20domain%20adaptive%20benchmarks%20to%20evaluate%20the%20proposed%20SAFF%2C%20and%20the%0Aexperimental%20results%20show%20that%20the%20proposed%20SAFF%20can%20be%20easily%20combined%20with%0Adifferent%20backbone%20networks%20and%20obtain%20better%20performance%20as%20a%20plug-in-plug-out%0Amodule.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2209.01870v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DUnsupervised%2520Domain%2520Adaptation%2520via%2520Style-Aware%2520Self-intermediate%2520Domain%26entry.906535625%3DLianyu%2520Wang%2520and%2520Meng%2520Wang%2520and%2520Daoqiang%2520Zhang%2520and%2520Huazhu%2520Fu%26entry.1292438233%3D%2520%2520Unsupervised%2520domain%2520adaptation%2520%2528UDA%2529%2520has%2520attracted%2520considerable%2520attention%252C%250Awhich%2520transfers%2520knowledge%2520from%2520a%2520label-rich%2520source%2520domain%2520to%2520a%2520related%2520but%250Aunlabeled%2520target%2520domain.%2520Reducing%2520inter-domain%2520differences%2520has%2520always%2520been%2520a%250Acrucial%2520factor%2520to%2520improve%2520performance%2520in%2520UDA%252C%2520especially%2520for%2520tasks%2520where%2520there%250Ais%2520a%2520large%2520gap%2520between%2520source%2520and%2520target%2520domains.%2520To%2520this%2520end%252C%2520we%2520propose%2520a%250Anovel%2520style-aware%2520feature%2520fusion%2520method%2520%2528SAFF%2529%2520to%2520bridge%2520the%2520large%2520domain%2520gap%250Aand%2520transfer%2520knowledge%2520while%2520alleviating%2520the%2520loss%2520of%2520class-discriminative%250Ainformation.%2520Inspired%2520by%2520the%2520human%2520transitive%2520inference%2520and%2520learning%2520ability%252C%2520a%250Anovel%2520style-aware%2520self-intermediate%2520domain%2520%2528SSID%2529%2520is%2520investigated%2520to%2520link%2520two%250Aseemingly%2520unrelated%2520concepts%2520through%2520a%2520series%2520of%2520intermediate%2520auxiliary%250Asynthesized%2520concepts.%2520Specifically%252C%2520we%2520propose%2520a%2520novel%2520learning%2520strategy%2520of%250ASSID%252C%2520which%2520selects%2520samples%2520from%2520both%2520source%2520and%2520target%2520domains%2520as%2520anchors%252C%2520and%250Athen%2520randomly%2520fuses%2520the%2520object%2520and%2520style%2520features%2520of%2520these%2520anchors%2520to%2520generate%250Alabeled%2520and%2520style-rich%2520intermediate%2520auxiliary%2520features%2520for%2520knowledge%2520transfer.%250AMoreover%252C%2520we%2520design%2520an%2520external%2520memory%2520bank%2520to%2520store%2520and%2520update%2520specified%250Alabeled%2520features%2520to%2520obtain%2520stable%2520class%2520features%2520and%2520class-wise%2520style%2520features.%250ABased%2520on%2520the%2520proposed%2520memory%2520bank%252C%2520the%2520intra-%2520and%2520inter-domain%2520loss%2520functions%250Aare%2520designed%2520to%2520improve%2520the%2520class%2520recognition%2520ability%2520and%2520feature%250Acompatibility%252C%2520respectively.%2520Meanwhile%252C%2520we%2520simulate%2520the%2520rich%2520latent%2520feature%250Aspace%2520of%2520SSID%2520by%2520infinite%2520sampling%2520and%2520the%2520convergence%2520of%2520the%2520loss%2520function%2520by%250Amathematical%2520theory.%2520Finally%252C%2520we%2520conduct%2520comprehensive%2520experiments%2520on%2520commonly%250Aused%2520domain%2520adaptive%2520benchmarks%2520to%2520evaluate%2520the%2520proposed%2520SAFF%252C%2520and%2520the%250Aexperimental%2520results%2520show%2520that%2520the%2520proposed%2520SAFF%2520can%2520be%2520easily%2520combined%2520with%250Adifferent%2520backbone%2520networks%2520and%2520obtain%2520better%2520performance%2520as%2520a%2520plug-in-plug-out%250Amodule.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2209.01870v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Unsupervised%20Domain%20Adaptation%20via%20Style-Aware%20Self-intermediate%20Domain&entry.906535625=Lianyu%20Wang%20and%20Meng%20Wang%20and%20Daoqiang%20Zhang%20and%20Huazhu%20Fu&entry.1292438233=%20%20Unsupervised%20domain%20adaptation%20%28UDA%29%20has%20attracted%20considerable%20attention%2C%0Awhich%20transfers%20knowledge%20from%20a%20label-rich%20source%20domain%20to%20a%20related%20but%0Aunlabeled%20target%20domain.%20Reducing%20inter-domain%20differences%20has%20always%20been%20a%0Acrucial%20factor%20to%20improve%20performance%20in%20UDA%2C%20especially%20for%20tasks%20where%20there%0Ais%20a%20large%20gap%20between%20source%20and%20target%20domains.%20To%20this%20end%2C%20we%20propose%20a%0Anovel%20style-aware%20feature%20fusion%20method%20%28SAFF%29%20to%20bridge%20the%20large%20domain%20gap%0Aand%20transfer%20knowledge%20while%20alleviating%20the%20loss%20of%20class-discriminative%0Ainformation.%20Inspired%20by%20the%20human%20transitive%20inference%20and%20learning%20ability%2C%20a%0Anovel%20style-aware%20self-intermediate%20domain%20%28SSID%29%20is%20investigated%20to%20link%20two%0Aseemingly%20unrelated%20concepts%20through%20a%20series%20of%20intermediate%20auxiliary%0Asynthesized%20concepts.%20Specifically%2C%20we%20propose%20a%20novel%20learning%20strategy%20of%0ASSID%2C%20which%20selects%20samples%20from%20both%20source%20and%20target%20domains%20as%20anchors%2C%20and%0Athen%20randomly%20fuses%20the%20object%20and%20style%20features%20of%20these%20anchors%20to%20generate%0Alabeled%20and%20style-rich%20intermediate%20auxiliary%20features%20for%20knowledge%20transfer.%0AMoreover%2C%20we%20design%20an%20external%20memory%20bank%20to%20store%20and%20update%20specified%0Alabeled%20features%20to%20obtain%20stable%20class%20features%20and%20class-wise%20style%20features.%0ABased%20on%20the%20proposed%20memory%20bank%2C%20the%20intra-%20and%20inter-domain%20loss%20functions%0Aare%20designed%20to%20improve%20the%20class%20recognition%20ability%20and%20feature%0Acompatibility%2C%20respectively.%20Meanwhile%2C%20we%20simulate%20the%20rich%20latent%20feature%0Aspace%20of%20SSID%20by%20infinite%20sampling%20and%20the%20convergence%20of%20the%20loss%20function%20by%0Amathematical%20theory.%20Finally%2C%20we%20conduct%20comprehensive%20experiments%20on%20commonly%0Aused%20domain%20adaptive%20benchmarks%20to%20evaluate%20the%20proposed%20SAFF%2C%20and%20the%0Aexperimental%20results%20show%20that%20the%20proposed%20SAFF%20can%20be%20easily%20combined%20with%0Adifferent%20backbone%20networks%20and%20obtain%20better%20performance%20as%20a%20plug-in-plug-out%0Amodule.%0A&entry.1838667208=http%3A//arxiv.org/abs/2209.01870v2&entry.124074799=Read"},
{"title": "Probabilistic Visibility-Aware Trajectory Planning for Target Tracking\n  in Cluttered Environments", "author": "Han Gao and Pengying Wu and Yao Su and Kangjie Zhou and Ji Ma and Hangxin Liu and Chang Liu", "abstract": "  Target tracking has numerous significant civilian and military applications,\nand maintaining the visibility of the target plays a vital role in ensuring the\nsuccess of the tracking task. Existing visibility-aware planners primarily\nfocus on keeping the target within the limited field of view of an onboard\nsensor and avoiding obstacle occlusion. However, the negative impact of system\nuncertainty is often neglected, rendering the planners delicate to\nuncertainties in practice. To bridge the gap, this work proposes a real-time,\nnon-myopic trajectory planner for visibility-aware and safe target tracking in\nthe presence of system uncertainty. For more accurate target motion prediction,\nwe introduce the concept of belief-space probability of detection (BPOD) to\nmeasure the predictive visibility of the target under stochastic robot and\ntarget states. An Extended Kalman Filter variant incorporating BPOD is\ndeveloped to predict target belief state under uncertain visibility within the\nplanning horizon. To reach real-time trajectory planning, we propose a\ncomputationally efficient algorithm to uniformly calculate both BPOD and the\nchance-constrained collision risk by utilizing linearized signed distance\nfunction (SDF), and then design a two-stage strategy for lightweight\ncalculation of SDF in sequential convex programming. Extensive simulation\nresults with benchmark comparisons show the capacity of the proposed approach\nto robustly maintain the visibility of the target under high system\nuncertainty. The practicality of the proposed trajectory planner is validated\nby real-world experiments.\n", "link": "http://arxiv.org/abs/2306.06363v2", "date": "2024-08-27", "relevancy": 2.1463, "topK": [{"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.576}, {"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.5642}, {"title": "Flexible Visual Recognition by Evidential Modeling of Confusion and Ignorance", "link": "http://openaccess.thecvf.com/content/ICCV2023/html/Fan_Flexible_Visual_Recognition_by_Evidential_Modeling_of_Confusion_and_Ignorance_ICCV_2023_paper.html", "similarity": 0.4932}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Probabilistic%20Visibility-Aware%20Trajectory%20Planning%20for%20Target%20Tracking%0A%20%20in%20Cluttered%20Environments&body=Title%3A%20Probabilistic%20Visibility-Aware%20Trajectory%20Planning%20for%20Target%20Tracking%0A%20%20in%20Cluttered%20Environments%0AAuthor%3A%20Han%20Gao%20and%20Pengying%20Wu%20and%20Yao%20Su%20and%20Kangjie%20Zhou%20and%20Ji%20Ma%20and%20Hangxin%20Liu%20and%20Chang%20Liu%0AAbstract%3A%20%20%20Target%20tracking%20has%20numerous%20significant%20civilian%20and%20military%20applications%2C%0Aand%20maintaining%20the%20visibility%20of%20the%20target%20plays%20a%20vital%20role%20in%20ensuring%20the%0Asuccess%20of%20the%20tracking%20task.%20Existing%20visibility-aware%20planners%20primarily%0Afocus%20on%20keeping%20the%20target%20within%20the%20limited%20field%20of%20view%20of%20an%20onboard%0Asensor%20and%20avoiding%20obstacle%20occlusion.%20However%2C%20the%20negative%20impact%20of%20system%0Auncertainty%20is%20often%20neglected%2C%20rendering%20the%20planners%20delicate%20to%0Auncertainties%20in%20practice.%20To%20bridge%20the%20gap%2C%20this%20work%20proposes%20a%20real-time%2C%0Anon-myopic%20trajectory%20planner%20for%20visibility-aware%20and%20safe%20target%20tracking%20in%0Athe%20presence%20of%20system%20uncertainty.%20For%20more%20accurate%20target%20motion%20prediction%2C%0Awe%20introduce%20the%20concept%20of%20belief-space%20probability%20of%20detection%20%28BPOD%29%20to%0Ameasure%20the%20predictive%20visibility%20of%20the%20target%20under%20stochastic%20robot%20and%0Atarget%20states.%20An%20Extended%20Kalman%20Filter%20variant%20incorporating%20BPOD%20is%0Adeveloped%20to%20predict%20target%20belief%20state%20under%20uncertain%20visibility%20within%20the%0Aplanning%20horizon.%20To%20reach%20real-time%20trajectory%20planning%2C%20we%20propose%20a%0Acomputationally%20efficient%20algorithm%20to%20uniformly%20calculate%20both%20BPOD%20and%20the%0Achance-constrained%20collision%20risk%20by%20utilizing%20linearized%20signed%20distance%0Afunction%20%28SDF%29%2C%20and%20then%20design%20a%20two-stage%20strategy%20for%20lightweight%0Acalculation%20of%20SDF%20in%20sequential%20convex%20programming.%20Extensive%20simulation%0Aresults%20with%20benchmark%20comparisons%20show%20the%20capacity%20of%20the%20proposed%20approach%0Ato%20robustly%20maintain%20the%20visibility%20of%20the%20target%20under%20high%20system%0Auncertainty.%20The%20practicality%20of%20the%20proposed%20trajectory%20planner%20is%20validated%0Aby%20real-world%20experiments.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2306.06363v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DProbabilistic%2520Visibility-Aware%2520Trajectory%2520Planning%2520for%2520Target%2520Tracking%250A%2520%2520in%2520Cluttered%2520Environments%26entry.906535625%3DHan%2520Gao%2520and%2520Pengying%2520Wu%2520and%2520Yao%2520Su%2520and%2520Kangjie%2520Zhou%2520and%2520Ji%2520Ma%2520and%2520Hangxin%2520Liu%2520and%2520Chang%2520Liu%26entry.1292438233%3D%2520%2520Target%2520tracking%2520has%2520numerous%2520significant%2520civilian%2520and%2520military%2520applications%252C%250Aand%2520maintaining%2520the%2520visibility%2520of%2520the%2520target%2520plays%2520a%2520vital%2520role%2520in%2520ensuring%2520the%250Asuccess%2520of%2520the%2520tracking%2520task.%2520Existing%2520visibility-aware%2520planners%2520primarily%250Afocus%2520on%2520keeping%2520the%2520target%2520within%2520the%2520limited%2520field%2520of%2520view%2520of%2520an%2520onboard%250Asensor%2520and%2520avoiding%2520obstacle%2520occlusion.%2520However%252C%2520the%2520negative%2520impact%2520of%2520system%250Auncertainty%2520is%2520often%2520neglected%252C%2520rendering%2520the%2520planners%2520delicate%2520to%250Auncertainties%2520in%2520practice.%2520To%2520bridge%2520the%2520gap%252C%2520this%2520work%2520proposes%2520a%2520real-time%252C%250Anon-myopic%2520trajectory%2520planner%2520for%2520visibility-aware%2520and%2520safe%2520target%2520tracking%2520in%250Athe%2520presence%2520of%2520system%2520uncertainty.%2520For%2520more%2520accurate%2520target%2520motion%2520prediction%252C%250Awe%2520introduce%2520the%2520concept%2520of%2520belief-space%2520probability%2520of%2520detection%2520%2528BPOD%2529%2520to%250Ameasure%2520the%2520predictive%2520visibility%2520of%2520the%2520target%2520under%2520stochastic%2520robot%2520and%250Atarget%2520states.%2520An%2520Extended%2520Kalman%2520Filter%2520variant%2520incorporating%2520BPOD%2520is%250Adeveloped%2520to%2520predict%2520target%2520belief%2520state%2520under%2520uncertain%2520visibility%2520within%2520the%250Aplanning%2520horizon.%2520To%2520reach%2520real-time%2520trajectory%2520planning%252C%2520we%2520propose%2520a%250Acomputationally%2520efficient%2520algorithm%2520to%2520uniformly%2520calculate%2520both%2520BPOD%2520and%2520the%250Achance-constrained%2520collision%2520risk%2520by%2520utilizing%2520linearized%2520signed%2520distance%250Afunction%2520%2528SDF%2529%252C%2520and%2520then%2520design%2520a%2520two-stage%2520strategy%2520for%2520lightweight%250Acalculation%2520of%2520SDF%2520in%2520sequential%2520convex%2520programming.%2520Extensive%2520simulation%250Aresults%2520with%2520benchmark%2520comparisons%2520show%2520the%2520capacity%2520of%2520the%2520proposed%2520approach%250Ato%2520robustly%2520maintain%2520the%2520visibility%2520of%2520the%2520target%2520under%2520high%2520system%250Auncertainty.%2520The%2520practicality%2520of%2520the%2520proposed%2520trajectory%2520planner%2520is%2520validated%250Aby%2520real-world%2520experiments.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2306.06363v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Probabilistic%20Visibility-Aware%20Trajectory%20Planning%20for%20Target%20Tracking%0A%20%20in%20Cluttered%20Environments&entry.906535625=Han%20Gao%20and%20Pengying%20Wu%20and%20Yao%20Su%20and%20Kangjie%20Zhou%20and%20Ji%20Ma%20and%20Hangxin%20Liu%20and%20Chang%20Liu&entry.1292438233=%20%20Target%20tracking%20has%20numerous%20significant%20civilian%20and%20military%20applications%2C%0Aand%20maintaining%20the%20visibility%20of%20the%20target%20plays%20a%20vital%20role%20in%20ensuring%20the%0Asuccess%20of%20the%20tracking%20task.%20Existing%20visibility-aware%20planners%20primarily%0Afocus%20on%20keeping%20the%20target%20within%20the%20limited%20field%20of%20view%20of%20an%20onboard%0Asensor%20and%20avoiding%20obstacle%20occlusion.%20However%2C%20the%20negative%20impact%20of%20system%0Auncertainty%20is%20often%20neglected%2C%20rendering%20the%20planners%20delicate%20to%0Auncertainties%20in%20practice.%20To%20bridge%20the%20gap%2C%20this%20work%20proposes%20a%20real-time%2C%0Anon-myopic%20trajectory%20planner%20for%20visibility-aware%20and%20safe%20target%20tracking%20in%0Athe%20presence%20of%20system%20uncertainty.%20For%20more%20accurate%20target%20motion%20prediction%2C%0Awe%20introduce%20the%20concept%20of%20belief-space%20probability%20of%20detection%20%28BPOD%29%20to%0Ameasure%20the%20predictive%20visibility%20of%20the%20target%20under%20stochastic%20robot%20and%0Atarget%20states.%20An%20Extended%20Kalman%20Filter%20variant%20incorporating%20BPOD%20is%0Adeveloped%20to%20predict%20target%20belief%20state%20under%20uncertain%20visibility%20within%20the%0Aplanning%20horizon.%20To%20reach%20real-time%20trajectory%20planning%2C%20we%20propose%20a%0Acomputationally%20efficient%20algorithm%20to%20uniformly%20calculate%20both%20BPOD%20and%20the%0Achance-constrained%20collision%20risk%20by%20utilizing%20linearized%20signed%20distance%0Afunction%20%28SDF%29%2C%20and%20then%20design%20a%20two-stage%20strategy%20for%20lightweight%0Acalculation%20of%20SDF%20in%20sequential%20convex%20programming.%20Extensive%20simulation%0Aresults%20with%20benchmark%20comparisons%20show%20the%20capacity%20of%20the%20proposed%20approach%0Ato%20robustly%20maintain%20the%20visibility%20of%20the%20target%20under%20high%20system%0Auncertainty.%20The%20practicality%20of%20the%20proposed%20trajectory%20planner%20is%20validated%0Aby%20real-world%20experiments.%0A&entry.1838667208=http%3A//arxiv.org/abs/2306.06363v2&entry.124074799=Read"},
{"title": "MMASD+: A Novel Dataset for Privacy-Preserving Behavior Analysis of\n  Children with Autism Spectrum Disorder", "author": "Pavan Uttej Ravva and Behdokht Kiafar and Pinar Kullu and Jicheng Li and Anjana Bhat and Roghayeh Leila Barmaki", "abstract": "  Autism spectrum disorder (ASD) is characterized by significant challenges in\nsocial interaction and comprehending communication signals. Recently,\ntherapeutic interventions for ASD have increasingly utilized Deep learning\npowered-computer vision techniques to monitor individual progress over time.\nThese models are trained on private, non-public datasets from the autism\ncommunity, creating challenges in comparing results across different models due\nto privacy-preserving data-sharing issues. This work introduces MMASD+. MMASD+\nconsists of diverse data modalities, including 3D-Skeleton, 3D Body Mesh, and\nOptical Flow data. It integrates the capabilities of Yolov8 and Deep SORT\nalgorithms to distinguish between the therapist and children, addressing a\nsignificant barrier in the original dataset. Additionally, a Multimodal\nTransformer framework is proposed to predict 11 action types and the presence\nof ASD. This framework achieves an accuracy of 95.03% for predicting action\ntypes and 96.42% for predicting ASD presence, demonstrating over a 10%\nimprovement compared to models trained on single data modalities. These\nfindings highlight the advantages of integrating multiple data modalities\nwithin the Multimodal Transformer framework.\n", "link": "http://arxiv.org/abs/2408.15077v1", "date": "2024-08-27", "relevancy": 2.1449, "topK": [{"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.5591}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5341}, {"title": "Weakly-guided self-supervised pretraining for temporal activity detection", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/25189", "similarity": 0.5292}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20MMASD%2B%3A%20A%20Novel%20Dataset%20for%20Privacy-Preserving%20Behavior%20Analysis%20of%0A%20%20Children%20with%20Autism%20Spectrum%20Disorder&body=Title%3A%20MMASD%2B%3A%20A%20Novel%20Dataset%20for%20Privacy-Preserving%20Behavior%20Analysis%20of%0A%20%20Children%20with%20Autism%20Spectrum%20Disorder%0AAuthor%3A%20Pavan%20Uttej%20Ravva%20and%20Behdokht%20Kiafar%20and%20Pinar%20Kullu%20and%20Jicheng%20Li%20and%20Anjana%20Bhat%20and%20Roghayeh%20Leila%20Barmaki%0AAbstract%3A%20%20%20Autism%20spectrum%20disorder%20%28ASD%29%20is%20characterized%20by%20significant%20challenges%20in%0Asocial%20interaction%20and%20comprehending%20communication%20signals.%20Recently%2C%0Atherapeutic%20interventions%20for%20ASD%20have%20increasingly%20utilized%20Deep%20learning%0Apowered-computer%20vision%20techniques%20to%20monitor%20individual%20progress%20over%20time.%0AThese%20models%20are%20trained%20on%20private%2C%20non-public%20datasets%20from%20the%20autism%0Acommunity%2C%20creating%20challenges%20in%20comparing%20results%20across%20different%20models%20due%0Ato%20privacy-preserving%20data-sharing%20issues.%20This%20work%20introduces%20MMASD%2B.%20MMASD%2B%0Aconsists%20of%20diverse%20data%20modalities%2C%20including%203D-Skeleton%2C%203D%20Body%20Mesh%2C%20and%0AOptical%20Flow%20data.%20It%20integrates%20the%20capabilities%20of%20Yolov8%20and%20Deep%20SORT%0Aalgorithms%20to%20distinguish%20between%20the%20therapist%20and%20children%2C%20addressing%20a%0Asignificant%20barrier%20in%20the%20original%20dataset.%20Additionally%2C%20a%20Multimodal%0ATransformer%20framework%20is%20proposed%20to%20predict%2011%20action%20types%20and%20the%20presence%0Aof%20ASD.%20This%20framework%20achieves%20an%20accuracy%20of%2095.03%25%20for%20predicting%20action%0Atypes%20and%2096.42%25%20for%20predicting%20ASD%20presence%2C%20demonstrating%20over%20a%2010%25%0Aimprovement%20compared%20to%20models%20trained%20on%20single%20data%20modalities.%20These%0Afindings%20highlight%20the%20advantages%20of%20integrating%20multiple%20data%20modalities%0Awithin%20the%20Multimodal%20Transformer%20framework.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2408.15077v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DMMASD%252B%253A%2520A%2520Novel%2520Dataset%2520for%2520Privacy-Preserving%2520Behavior%2520Analysis%2520of%250A%2520%2520Children%2520with%2520Autism%2520Spectrum%2520Disorder%26entry.906535625%3DPavan%2520Uttej%2520Ravva%2520and%2520Behdokht%2520Kiafar%2520and%2520Pinar%2520Kullu%2520and%2520Jicheng%2520Li%2520and%2520Anjana%2520Bhat%2520and%2520Roghayeh%2520Leila%2520Barmaki%26entry.1292438233%3D%2520%2520Autism%2520spectrum%2520disorder%2520%2528ASD%2529%2520is%2520characterized%2520by%2520significant%2520challenges%2520in%250Asocial%2520interaction%2520and%2520comprehending%2520communication%2520signals.%2520Recently%252C%250Atherapeutic%2520interventions%2520for%2520ASD%2520have%2520increasingly%2520utilized%2520Deep%2520learning%250Apowered-computer%2520vision%2520techniques%2520to%2520monitor%2520individual%2520progress%2520over%2520time.%250AThese%2520models%2520are%2520trained%2520on%2520private%252C%2520non-public%2520datasets%2520from%2520the%2520autism%250Acommunity%252C%2520creating%2520challenges%2520in%2520comparing%2520results%2520across%2520different%2520models%2520due%250Ato%2520privacy-preserving%2520data-sharing%2520issues.%2520This%2520work%2520introduces%2520MMASD%252B.%2520MMASD%252B%250Aconsists%2520of%2520diverse%2520data%2520modalities%252C%2520including%25203D-Skeleton%252C%25203D%2520Body%2520Mesh%252C%2520and%250AOptical%2520Flow%2520data.%2520It%2520integrates%2520the%2520capabilities%2520of%2520Yolov8%2520and%2520Deep%2520SORT%250Aalgorithms%2520to%2520distinguish%2520between%2520the%2520therapist%2520and%2520children%252C%2520addressing%2520a%250Asignificant%2520barrier%2520in%2520the%2520original%2520dataset.%2520Additionally%252C%2520a%2520Multimodal%250ATransformer%2520framework%2520is%2520proposed%2520to%2520predict%252011%2520action%2520types%2520and%2520the%2520presence%250Aof%2520ASD.%2520This%2520framework%2520achieves%2520an%2520accuracy%2520of%252095.03%2525%2520for%2520predicting%2520action%250Atypes%2520and%252096.42%2525%2520for%2520predicting%2520ASD%2520presence%252C%2520demonstrating%2520over%2520a%252010%2525%250Aimprovement%2520compared%2520to%2520models%2520trained%2520on%2520single%2520data%2520modalities.%2520These%250Afindings%2520highlight%2520the%2520advantages%2520of%2520integrating%2520multiple%2520data%2520modalities%250Awithin%2520the%2520Multimodal%2520Transformer%2520framework.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2408.15077v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=MMASD%2B%3A%20A%20Novel%20Dataset%20for%20Privacy-Preserving%20Behavior%20Analysis%20of%0A%20%20Children%20with%20Autism%20Spectrum%20Disorder&entry.906535625=Pavan%20Uttej%20Ravva%20and%20Behdokht%20Kiafar%20and%20Pinar%20Kullu%20and%20Jicheng%20Li%20and%20Anjana%20Bhat%20and%20Roghayeh%20Leila%20Barmaki&entry.1292438233=%20%20Autism%20spectrum%20disorder%20%28ASD%29%20is%20characterized%20by%20significant%20challenges%20in%0Asocial%20interaction%20and%20comprehending%20communication%20signals.%20Recently%2C%0Atherapeutic%20interventions%20for%20ASD%20have%20increasingly%20utilized%20Deep%20learning%0Apowered-computer%20vision%20techniques%20to%20monitor%20individual%20progress%20over%20time.%0AThese%20models%20are%20trained%20on%20private%2C%20non-public%20datasets%20from%20the%20autism%0Acommunity%2C%20creating%20challenges%20in%20comparing%20results%20across%20different%20models%20due%0Ato%20privacy-preserving%20data-sharing%20issues.%20This%20work%20introduces%20MMASD%2B.%20MMASD%2B%0Aconsists%20of%20diverse%20data%20modalities%2C%20including%203D-Skeleton%2C%203D%20Body%20Mesh%2C%20and%0AOptical%20Flow%20data.%20It%20integrates%20the%20capabilities%20of%20Yolov8%20and%20Deep%20SORT%0Aalgorithms%20to%20distinguish%20between%20the%20therapist%20and%20children%2C%20addressing%20a%0Asignificant%20barrier%20in%20the%20original%20dataset.%20Additionally%2C%20a%20Multimodal%0ATransformer%20framework%20is%20proposed%20to%20predict%2011%20action%20types%20and%20the%20presence%0Aof%20ASD.%20This%20framework%20achieves%20an%20accuracy%20of%2095.03%25%20for%20predicting%20action%0Atypes%20and%2096.42%25%20for%20predicting%20ASD%20presence%2C%20demonstrating%20over%20a%2010%25%0Aimprovement%20compared%20to%20models%20trained%20on%20single%20data%20modalities.%20These%0Afindings%20highlight%20the%20advantages%20of%20integrating%20multiple%20data%20modalities%0Awithin%20the%20Multimodal%20Transformer%20framework.%0A&entry.1838667208=http%3A//arxiv.org/abs/2408.15077v1&entry.124074799=Read"},
{"title": "An Investigation on The Position Encoding in Vision-Based Dynamics\n  Prediction", "author": "Jiageng Zhu and Hanchen Xie and Jiazhi Li and Mahyar Khayatkhoei and Wael AbdAlmageed", "abstract": "  Despite the success of vision-based dynamics prediction models, which predict\nobject states by utilizing RGB images and simple object descriptions, they were\nchallenged by environment misalignments. Although the literature has\ndemonstrated that unifying visual domains with both environment context and\nobject abstract, such as semantic segmentation and bounding boxes, can\neffectively mitigate the visual domain misalignment challenge, discussions were\nfocused on the abstract of environment context, and the insight of using\nbounding box as the object abstract is under-explored. Furthermore, we notice\nthat, as empirical results shown in the literature, even when the visual\nappearance of objects is removed, object bounding boxes alone, instead of being\ndirectly fed into the network, can indirectly provide sufficient position\ninformation via the Region of Interest Pooling operation for dynamics\nprediction. However, previous literature overlooked discussions regarding how\nsuch position information is implicitly encoded in the dynamics prediction\nmodel. Thus, in this paper, we provide detailed studies to investigate the\nprocess and necessary conditions for encoding position information via using\nthe bounding box as the object abstract into output features. Furthermore, we\nstudy the limitation of solely using object abstracts, such that the dynamics\nprediction performance will be jeopardized when the environment context varies.\n", "link": "http://arxiv.org/abs/2408.15201v1", "date": "2024-08-27", "relevancy": 2.1431, "topK": [{"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.6117}, {"title": "DDM-NET: End-to-end learning of keypoint feature Detection, Description and Matching for 3D localization", "link": "https://arxiv.org/abs/2212.04575", "similarity": 0.5296}, {"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.5116}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20An%20Investigation%20on%20The%20Position%20Encoding%20in%20Vision-Based%20Dynamics%0A%20%20Prediction&body=Title%3A%20An%20Investigation%20on%20The%20Position%20Encoding%20in%20Vision-Based%20Dynamics%0A%20%20Prediction%0AAuthor%3A%20Jiageng%20Zhu%20and%20Hanchen%20Xie%20and%20Jiazhi%20Li%20and%20Mahyar%20Khayatkhoei%20and%20Wael%20AbdAlmageed%0AAbstract%3A%20%20%20Despite%20the%20success%20of%20vision-based%20dynamics%20prediction%20models%2C%20which%20predict%0Aobject%20states%20by%20utilizing%20RGB%20images%20and%20simple%20object%20descriptions%2C%20they%20were%0Achallenged%20by%20environment%20misalignments.%20Although%20the%20literature%20has%0Ademonstrated%20that%20unifying%20visual%20domains%20with%20both%20environment%20context%20and%0Aobject%20abstract%2C%20such%20as%20semantic%20segmentation%20and%20bounding%20boxes%2C%20can%0Aeffectively%20mitigate%20the%20visual%20domain%20misalignment%20challenge%2C%20discussions%20were%0Afocused%20on%20the%20abstract%20of%20environment%20context%2C%20and%20the%20insight%20of%20using%0Abounding%20box%20as%20the%20object%20abstract%20is%20under-explored.%20Furthermore%2C%20we%20notice%0Athat%2C%20as%20empirical%20results%20shown%20in%20the%20literature%2C%20even%20when%20the%20visual%0Aappearance%20of%20objects%20is%20removed%2C%20object%20bounding%20boxes%20alone%2C%20instead%20of%20being%0Adirectly%20fed%20into%20the%20network%2C%20can%20indirectly%20provide%20sufficient%20position%0Ainformation%20via%20the%20Region%20of%20Interest%20Pooling%20operation%20for%20dynamics%0Aprediction.%20However%2C%20previous%20literature%20overlooked%20discussions%20regarding%20how%0Asuch%20position%20information%20is%20implicitly%20encoded%20in%20the%20dynamics%20prediction%0Amodel.%20Thus%2C%20in%20this%20paper%2C%20we%20provide%20detailed%20studies%20to%20investigate%20the%0Aprocess%20and%20necessary%20conditions%20for%20encoding%20position%20information%20via%20using%0Athe%20bounding%20box%20as%20the%20object%20abstract%20into%20output%20features.%20Furthermore%2C%20we%0Astudy%20the%20limitation%20of%20solely%20using%20object%20abstracts%2C%20such%20that%20the%20dynamics%0Aprediction%20performance%20will%20be%20jeopardized%20when%20the%20environment%20context%20varies.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2408.15201v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DAn%2520Investigation%2520on%2520The%2520Position%2520Encoding%2520in%2520Vision-Based%2520Dynamics%250A%2520%2520Prediction%26entry.906535625%3DJiageng%2520Zhu%2520and%2520Hanchen%2520Xie%2520and%2520Jiazhi%2520Li%2520and%2520Mahyar%2520Khayatkhoei%2520and%2520Wael%2520AbdAlmageed%26entry.1292438233%3D%2520%2520Despite%2520the%2520success%2520of%2520vision-based%2520dynamics%2520prediction%2520models%252C%2520which%2520predict%250Aobject%2520states%2520by%2520utilizing%2520RGB%2520images%2520and%2520simple%2520object%2520descriptions%252C%2520they%2520were%250Achallenged%2520by%2520environment%2520misalignments.%2520Although%2520the%2520literature%2520has%250Ademonstrated%2520that%2520unifying%2520visual%2520domains%2520with%2520both%2520environment%2520context%2520and%250Aobject%2520abstract%252C%2520such%2520as%2520semantic%2520segmentation%2520and%2520bounding%2520boxes%252C%2520can%250Aeffectively%2520mitigate%2520the%2520visual%2520domain%2520misalignment%2520challenge%252C%2520discussions%2520were%250Afocused%2520on%2520the%2520abstract%2520of%2520environment%2520context%252C%2520and%2520the%2520insight%2520of%2520using%250Abounding%2520box%2520as%2520the%2520object%2520abstract%2520is%2520under-explored.%2520Furthermore%252C%2520we%2520notice%250Athat%252C%2520as%2520empirical%2520results%2520shown%2520in%2520the%2520literature%252C%2520even%2520when%2520the%2520visual%250Aappearance%2520of%2520objects%2520is%2520removed%252C%2520object%2520bounding%2520boxes%2520alone%252C%2520instead%2520of%2520being%250Adirectly%2520fed%2520into%2520the%2520network%252C%2520can%2520indirectly%2520provide%2520sufficient%2520position%250Ainformation%2520via%2520the%2520Region%2520of%2520Interest%2520Pooling%2520operation%2520for%2520dynamics%250Aprediction.%2520However%252C%2520previous%2520literature%2520overlooked%2520discussions%2520regarding%2520how%250Asuch%2520position%2520information%2520is%2520implicitly%2520encoded%2520in%2520the%2520dynamics%2520prediction%250Amodel.%2520Thus%252C%2520in%2520this%2520paper%252C%2520we%2520provide%2520detailed%2520studies%2520to%2520investigate%2520the%250Aprocess%2520and%2520necessary%2520conditions%2520for%2520encoding%2520position%2520information%2520via%2520using%250Athe%2520bounding%2520box%2520as%2520the%2520object%2520abstract%2520into%2520output%2520features.%2520Furthermore%252C%2520we%250Astudy%2520the%2520limitation%2520of%2520solely%2520using%2520object%2520abstracts%252C%2520such%2520that%2520the%2520dynamics%250Aprediction%2520performance%2520will%2520be%2520jeopardized%2520when%2520the%2520environment%2520context%2520varies.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2408.15201v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=An%20Investigation%20on%20The%20Position%20Encoding%20in%20Vision-Based%20Dynamics%0A%20%20Prediction&entry.906535625=Jiageng%20Zhu%20and%20Hanchen%20Xie%20and%20Jiazhi%20Li%20and%20Mahyar%20Khayatkhoei%20and%20Wael%20AbdAlmageed&entry.1292438233=%20%20Despite%20the%20success%20of%20vision-based%20dynamics%20prediction%20models%2C%20which%20predict%0Aobject%20states%20by%20utilizing%20RGB%20images%20and%20simple%20object%20descriptions%2C%20they%20were%0Achallenged%20by%20environment%20misalignments.%20Although%20the%20literature%20has%0Ademonstrated%20that%20unifying%20visual%20domains%20with%20both%20environment%20context%20and%0Aobject%20abstract%2C%20such%20as%20semantic%20segmentation%20and%20bounding%20boxes%2C%20can%0Aeffectively%20mitigate%20the%20visual%20domain%20misalignment%20challenge%2C%20discussions%20were%0Afocused%20on%20the%20abstract%20of%20environment%20context%2C%20and%20the%20insight%20of%20using%0Abounding%20box%20as%20the%20object%20abstract%20is%20under-explored.%20Furthermore%2C%20we%20notice%0Athat%2C%20as%20empirical%20results%20shown%20in%20the%20literature%2C%20even%20when%20the%20visual%0Aappearance%20of%20objects%20is%20removed%2C%20object%20bounding%20boxes%20alone%2C%20instead%20of%20being%0Adirectly%20fed%20into%20the%20network%2C%20can%20indirectly%20provide%20sufficient%20position%0Ainformation%20via%20the%20Region%20of%20Interest%20Pooling%20operation%20for%20dynamics%0Aprediction.%20However%2C%20previous%20literature%20overlooked%20discussions%20regarding%20how%0Asuch%20position%20information%20is%20implicitly%20encoded%20in%20the%20dynamics%20prediction%0Amodel.%20Thus%2C%20in%20this%20paper%2C%20we%20provide%20detailed%20studies%20to%20investigate%20the%0Aprocess%20and%20necessary%20conditions%20for%20encoding%20position%20information%20via%20using%0Athe%20bounding%20box%20as%20the%20object%20abstract%20into%20output%20features.%20Furthermore%2C%20we%0Astudy%20the%20limitation%20of%20solely%20using%20object%20abstracts%2C%20such%20that%20the%20dynamics%0Aprediction%20performance%20will%20be%20jeopardized%20when%20the%20environment%20context%20varies.%0A&entry.1838667208=http%3A//arxiv.org/abs/2408.15201v1&entry.124074799=Read"},
{"title": "FPO++: Efficient Encoding and Rendering of Dynamic Neural Radiance\n  Fields by Analyzing and Enhancing Fourier PlenOctrees", "author": "Saskia Rabich and Patrick Stotko and Reinhard Klein", "abstract": "  Fourier PlenOctrees have shown to be an efficient representation for\nreal-time rendering of dynamic Neural Radiance Fields (NeRF). Despite its many\nadvantages, this method suffers from artifacts introduced by the involved\ncompression when combining it with recent state-of-the-art techniques for\ntraining the static per-frame NeRF models. In this paper, we perform an\nin-depth analysis of these artifacts and leverage the resulting insights to\npropose an improved representation. In particular, we present a novel density\nencoding that adapts the Fourier-based compression to the characteristics of\nthe transfer function used by the underlying volume rendering procedure and\nleads to a substantial reduction of artifacts in the dynamic model.\nFurthermore, we show an augmentation of the training data that relaxes the\nperiodicity assumption of the compression. We demonstrate the effectiveness of\nour enhanced Fourier PlenOctrees in the scope of quantitative and qualitative\nevaluations on synthetic and real-world scenes.\n", "link": "http://arxiv.org/abs/2310.20710v2", "date": "2024-08-27", "relevancy": 2.1396, "topK": [{"title": "Tuning-Free Noise Rectification for High Fidelity Image-to-Video\n  Generation", "link": "http://arxiv.org/abs/2403.02827v1", "similarity": 0.5607}, {"title": "RTG-SLAM: Real-time 3D Reconstruction at Scale using Gaussian Splatting", "link": "http://arxiv.org/abs/2404.19706v1", "similarity": 0.5171}, {"title": "Scaling Rectified Flow Transformers for High-Resolution Image Synthesis", "link": "http://arxiv.org/abs/2403.03206v1", "similarity": 0.5151}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20FPO%2B%2B%3A%20Efficient%20Encoding%20and%20Rendering%20of%20Dynamic%20Neural%20Radiance%0A%20%20Fields%20by%20Analyzing%20and%20Enhancing%20Fourier%20PlenOctrees&body=Title%3A%20FPO%2B%2B%3A%20Efficient%20Encoding%20and%20Rendering%20of%20Dynamic%20Neural%20Radiance%0A%20%20Fields%20by%20Analyzing%20and%20Enhancing%20Fourier%20PlenOctrees%0AAuthor%3A%20Saskia%20Rabich%20and%20Patrick%20Stotko%20and%20Reinhard%20Klein%0AAbstract%3A%20%20%20Fourier%20PlenOctrees%20have%20shown%20to%20be%20an%20efficient%20representation%20for%0Areal-time%20rendering%20of%20dynamic%20Neural%20Radiance%20Fields%20%28NeRF%29.%20Despite%20its%20many%0Aadvantages%2C%20this%20method%20suffers%20from%20artifacts%20introduced%20by%20the%20involved%0Acompression%20when%20combining%20it%20with%20recent%20state-of-the-art%20techniques%20for%0Atraining%20the%20static%20per-frame%20NeRF%20models.%20In%20this%20paper%2C%20we%20perform%20an%0Ain-depth%20analysis%20of%20these%20artifacts%20and%20leverage%20the%20resulting%20insights%20to%0Apropose%20an%20improved%20representation.%20In%20particular%2C%20we%20present%20a%20novel%20density%0Aencoding%20that%20adapts%20the%20Fourier-based%20compression%20to%20the%20characteristics%20of%0Athe%20transfer%20function%20used%20by%20the%20underlying%20volume%20rendering%20procedure%20and%0Aleads%20to%20a%20substantial%20reduction%20of%20artifacts%20in%20the%20dynamic%20model.%0AFurthermore%2C%20we%20show%20an%20augmentation%20of%20the%20training%20data%20that%20relaxes%20the%0Aperiodicity%20assumption%20of%20the%20compression.%20We%20demonstrate%20the%20effectiveness%20of%0Aour%20enhanced%20Fourier%20PlenOctrees%20in%20the%20scope%20of%20quantitative%20and%20qualitative%0Aevaluations%20on%20synthetic%20and%20real-world%20scenes.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2310.20710v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DFPO%252B%252B%253A%2520Efficient%2520Encoding%2520and%2520Rendering%2520of%2520Dynamic%2520Neural%2520Radiance%250A%2520%2520Fields%2520by%2520Analyzing%2520and%2520Enhancing%2520Fourier%2520PlenOctrees%26entry.906535625%3DSaskia%2520Rabich%2520and%2520Patrick%2520Stotko%2520and%2520Reinhard%2520Klein%26entry.1292438233%3D%2520%2520Fourier%2520PlenOctrees%2520have%2520shown%2520to%2520be%2520an%2520efficient%2520representation%2520for%250Areal-time%2520rendering%2520of%2520dynamic%2520Neural%2520Radiance%2520Fields%2520%2528NeRF%2529.%2520Despite%2520its%2520many%250Aadvantages%252C%2520this%2520method%2520suffers%2520from%2520artifacts%2520introduced%2520by%2520the%2520involved%250Acompression%2520when%2520combining%2520it%2520with%2520recent%2520state-of-the-art%2520techniques%2520for%250Atraining%2520the%2520static%2520per-frame%2520NeRF%2520models.%2520In%2520this%2520paper%252C%2520we%2520perform%2520an%250Ain-depth%2520analysis%2520of%2520these%2520artifacts%2520and%2520leverage%2520the%2520resulting%2520insights%2520to%250Apropose%2520an%2520improved%2520representation.%2520In%2520particular%252C%2520we%2520present%2520a%2520novel%2520density%250Aencoding%2520that%2520adapts%2520the%2520Fourier-based%2520compression%2520to%2520the%2520characteristics%2520of%250Athe%2520transfer%2520function%2520used%2520by%2520the%2520underlying%2520volume%2520rendering%2520procedure%2520and%250Aleads%2520to%2520a%2520substantial%2520reduction%2520of%2520artifacts%2520in%2520the%2520dynamic%2520model.%250AFurthermore%252C%2520we%2520show%2520an%2520augmentation%2520of%2520the%2520training%2520data%2520that%2520relaxes%2520the%250Aperiodicity%2520assumption%2520of%2520the%2520compression.%2520We%2520demonstrate%2520the%2520effectiveness%2520of%250Aour%2520enhanced%2520Fourier%2520PlenOctrees%2520in%2520the%2520scope%2520of%2520quantitative%2520and%2520qualitative%250Aevaluations%2520on%2520synthetic%2520and%2520real-world%2520scenes.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2310.20710v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=FPO%2B%2B%3A%20Efficient%20Encoding%20and%20Rendering%20of%20Dynamic%20Neural%20Radiance%0A%20%20Fields%20by%20Analyzing%20and%20Enhancing%20Fourier%20PlenOctrees&entry.906535625=Saskia%20Rabich%20and%20Patrick%20Stotko%20and%20Reinhard%20Klein&entry.1292438233=%20%20Fourier%20PlenOctrees%20have%20shown%20to%20be%20an%20efficient%20representation%20for%0Areal-time%20rendering%20of%20dynamic%20Neural%20Radiance%20Fields%20%28NeRF%29.%20Despite%20its%20many%0Aadvantages%2C%20this%20method%20suffers%20from%20artifacts%20introduced%20by%20the%20involved%0Acompression%20when%20combining%20it%20with%20recent%20state-of-the-art%20techniques%20for%0Atraining%20the%20static%20per-frame%20NeRF%20models.%20In%20this%20paper%2C%20we%20perform%20an%0Ain-depth%20analysis%20of%20these%20artifacts%20and%20leverage%20the%20resulting%20insights%20to%0Apropose%20an%20improved%20representation.%20In%20particular%2C%20we%20present%20a%20novel%20density%0Aencoding%20that%20adapts%20the%20Fourier-based%20compression%20to%20the%20characteristics%20of%0Athe%20transfer%20function%20used%20by%20the%20underlying%20volume%20rendering%20procedure%20and%0Aleads%20to%20a%20substantial%20reduction%20of%20artifacts%20in%20the%20dynamic%20model.%0AFurthermore%2C%20we%20show%20an%20augmentation%20of%20the%20training%20data%20that%20relaxes%20the%0Aperiodicity%20assumption%20of%20the%20compression.%20We%20demonstrate%20the%20effectiveness%20of%0Aour%20enhanced%20Fourier%20PlenOctrees%20in%20the%20scope%20of%20quantitative%20and%20qualitative%0Aevaluations%20on%20synthetic%20and%20real-world%20scenes.%0A&entry.1838667208=http%3A//arxiv.org/abs/2310.20710v2&entry.124074799=Read"},
{"title": "Skill Q-Network: Learning Adaptive Skill Ensemble for Mapless Navigation\n  in Unknown Environments", "author": "Hyunki Seong and David Hyunchul Shim", "abstract": "  This paper focuses on the acquisition of mapless navigation skills within\nunknown environments. We introduce the Skill Q-Network (SQN), a novel\nreinforcement learning method featuring an adaptive skill ensemble mechanism.\nUnlike existing methods, our model concurrently learns a high-level skill\ndecision process alongside multiple low-level navigation skills, all without\nthe need for prior knowledge. Leveraging a tailored reward function for mapless\nnavigation, the SQN is capable of learning adaptive maneuvers that incorporate\nboth exploration and goal-directed skills, enabling effective navigation in new\nenvironments. Our experiments demonstrate that our SQN can effectively navigate\ncomplex environments, exhibiting a 40\\% higher performance compared to baseline\nmodels. Without explicit guidance, SQN discovers how to combine low-level skill\npolicies, showcasing both goal-directed navigations to reach destinations and\nexploration maneuvers to escape from local minimum regions in challenging\nscenarios. Remarkably, our adaptive skill ensemble method enables zero-shot\ntransfer to out-of-distribution domains, characterized by unseen observations\nfrom non-convex obstacles or uneven, subterranean-like environments. The\nproject page is available at https://sites.google.com/view/skill-q-net.\n", "link": "http://arxiv.org/abs/2403.16664v3", "date": "2024-08-27", "relevancy": 2.138, "topK": [{"title": "RoboEXP: Action-Conditioned Scene Graph via Interactive Exploration for\n  Robotic Manipulation", "link": "http://arxiv.org/abs/2402.15487v1", "similarity": 0.5504}, {"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.5373}, {"title": "Egocentric Computer Vision for Hands-Free Robotic Wheelchair Navigation", "link": "https://link.springer.com/article/10.1007/s10846-023-01807-4", "similarity": 0.5254}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Skill%20Q-Network%3A%20Learning%20Adaptive%20Skill%20Ensemble%20for%20Mapless%20Navigation%0A%20%20in%20Unknown%20Environments&body=Title%3A%20Skill%20Q-Network%3A%20Learning%20Adaptive%20Skill%20Ensemble%20for%20Mapless%20Navigation%0A%20%20in%20Unknown%20Environments%0AAuthor%3A%20Hyunki%20Seong%20and%20David%20Hyunchul%20Shim%0AAbstract%3A%20%20%20This%20paper%20focuses%20on%20the%20acquisition%20of%20mapless%20navigation%20skills%20within%0Aunknown%20environments.%20We%20introduce%20the%20Skill%20Q-Network%20%28SQN%29%2C%20a%20novel%0Areinforcement%20learning%20method%20featuring%20an%20adaptive%20skill%20ensemble%20mechanism.%0AUnlike%20existing%20methods%2C%20our%20model%20concurrently%20learns%20a%20high-level%20skill%0Adecision%20process%20alongside%20multiple%20low-level%20navigation%20skills%2C%20all%20without%0Athe%20need%20for%20prior%20knowledge.%20Leveraging%20a%20tailored%20reward%20function%20for%20mapless%0Anavigation%2C%20the%20SQN%20is%20capable%20of%20learning%20adaptive%20maneuvers%20that%20incorporate%0Aboth%20exploration%20and%20goal-directed%20skills%2C%20enabling%20effective%20navigation%20in%20new%0Aenvironments.%20Our%20experiments%20demonstrate%20that%20our%20SQN%20can%20effectively%20navigate%0Acomplex%20environments%2C%20exhibiting%20a%2040%5C%25%20higher%20performance%20compared%20to%20baseline%0Amodels.%20Without%20explicit%20guidance%2C%20SQN%20discovers%20how%20to%20combine%20low-level%20skill%0Apolicies%2C%20showcasing%20both%20goal-directed%20navigations%20to%20reach%20destinations%20and%0Aexploration%20maneuvers%20to%20escape%20from%20local%20minimum%20regions%20in%20challenging%0Ascenarios.%20Remarkably%2C%20our%20adaptive%20skill%20ensemble%20method%20enables%20zero-shot%0Atransfer%20to%20out-of-distribution%20domains%2C%20characterized%20by%20unseen%20observations%0Afrom%20non-convex%20obstacles%20or%20uneven%2C%20subterranean-like%20environments.%20The%0Aproject%20page%20is%20available%20at%20https%3A//sites.google.com/view/skill-q-net.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2403.16664v3%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DSkill%2520Q-Network%253A%2520Learning%2520Adaptive%2520Skill%2520Ensemble%2520for%2520Mapless%2520Navigation%250A%2520%2520in%2520Unknown%2520Environments%26entry.906535625%3DHyunki%2520Seong%2520and%2520David%2520Hyunchul%2520Shim%26entry.1292438233%3D%2520%2520This%2520paper%2520focuses%2520on%2520the%2520acquisition%2520of%2520mapless%2520navigation%2520skills%2520within%250Aunknown%2520environments.%2520We%2520introduce%2520the%2520Skill%2520Q-Network%2520%2528SQN%2529%252C%2520a%2520novel%250Areinforcement%2520learning%2520method%2520featuring%2520an%2520adaptive%2520skill%2520ensemble%2520mechanism.%250AUnlike%2520existing%2520methods%252C%2520our%2520model%2520concurrently%2520learns%2520a%2520high-level%2520skill%250Adecision%2520process%2520alongside%2520multiple%2520low-level%2520navigation%2520skills%252C%2520all%2520without%250Athe%2520need%2520for%2520prior%2520knowledge.%2520Leveraging%2520a%2520tailored%2520reward%2520function%2520for%2520mapless%250Anavigation%252C%2520the%2520SQN%2520is%2520capable%2520of%2520learning%2520adaptive%2520maneuvers%2520that%2520incorporate%250Aboth%2520exploration%2520and%2520goal-directed%2520skills%252C%2520enabling%2520effective%2520navigation%2520in%2520new%250Aenvironments.%2520Our%2520experiments%2520demonstrate%2520that%2520our%2520SQN%2520can%2520effectively%2520navigate%250Acomplex%2520environments%252C%2520exhibiting%2520a%252040%255C%2525%2520higher%2520performance%2520compared%2520to%2520baseline%250Amodels.%2520Without%2520explicit%2520guidance%252C%2520SQN%2520discovers%2520how%2520to%2520combine%2520low-level%2520skill%250Apolicies%252C%2520showcasing%2520both%2520goal-directed%2520navigations%2520to%2520reach%2520destinations%2520and%250Aexploration%2520maneuvers%2520to%2520escape%2520from%2520local%2520minimum%2520regions%2520in%2520challenging%250Ascenarios.%2520Remarkably%252C%2520our%2520adaptive%2520skill%2520ensemble%2520method%2520enables%2520zero-shot%250Atransfer%2520to%2520out-of-distribution%2520domains%252C%2520characterized%2520by%2520unseen%2520observations%250Afrom%2520non-convex%2520obstacles%2520or%2520uneven%252C%2520subterranean-like%2520environments.%2520The%250Aproject%2520page%2520is%2520available%2520at%2520https%253A//sites.google.com/view/skill-q-net.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2403.16664v3%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Skill%20Q-Network%3A%20Learning%20Adaptive%20Skill%20Ensemble%20for%20Mapless%20Navigation%0A%20%20in%20Unknown%20Environments&entry.906535625=Hyunki%20Seong%20and%20David%20Hyunchul%20Shim&entry.1292438233=%20%20This%20paper%20focuses%20on%20the%20acquisition%20of%20mapless%20navigation%20skills%20within%0Aunknown%20environments.%20We%20introduce%20the%20Skill%20Q-Network%20%28SQN%29%2C%20a%20novel%0Areinforcement%20learning%20method%20featuring%20an%20adaptive%20skill%20ensemble%20mechanism.%0AUnlike%20existing%20methods%2C%20our%20model%20concurrently%20learns%20a%20high-level%20skill%0Adecision%20process%20alongside%20multiple%20low-level%20navigation%20skills%2C%20all%20without%0Athe%20need%20for%20prior%20knowledge.%20Leveraging%20a%20tailored%20reward%20function%20for%20mapless%0Anavigation%2C%20the%20SQN%20is%20capable%20of%20learning%20adaptive%20maneuvers%20that%20incorporate%0Aboth%20exploration%20and%20goal-directed%20skills%2C%20enabling%20effective%20navigation%20in%20new%0Aenvironments.%20Our%20experiments%20demonstrate%20that%20our%20SQN%20can%20effectively%20navigate%0Acomplex%20environments%2C%20exhibiting%20a%2040%5C%25%20higher%20performance%20compared%20to%20baseline%0Amodels.%20Without%20explicit%20guidance%2C%20SQN%20discovers%20how%20to%20combine%20low-level%20skill%0Apolicies%2C%20showcasing%20both%20goal-directed%20navigations%20to%20reach%20destinations%20and%0Aexploration%20maneuvers%20to%20escape%20from%20local%20minimum%20regions%20in%20challenging%0Ascenarios.%20Remarkably%2C%20our%20adaptive%20skill%20ensemble%20method%20enables%20zero-shot%0Atransfer%20to%20out-of-distribution%20domains%2C%20characterized%20by%20unseen%20observations%0Afrom%20non-convex%20obstacles%20or%20uneven%2C%20subterranean-like%20environments.%20The%0Aproject%20page%20is%20available%20at%20https%3A//sites.google.com/view/skill-q-net.%0A&entry.1838667208=http%3A//arxiv.org/abs/2403.16664v3&entry.124074799=Read"},
{"title": "The Benefits of Balance: From Information Projections to Variance\n  Reduction", "author": "Lang Liu and Ronak Mehta and Soumik Pal and Zaid Harchaoui", "abstract": "  Data balancing across multiple modalities/sources appears in various forms in\nseveral foundation models (e.g., CLIP and DINO) achieving universal\nrepresentation learning. We show that this iterative algorithm, usually used to\navoid representation collapse, enjoys an unsuspected benefit: reducing the\nvariance of estimators that are functionals of the empirical distribution over\nthese sources. We provide non-asymptotic bounds quantifying this variance\nreduction effect and relate them to the eigendecays of appropriately defined\nMarkov operators. We explain how various forms of data balancing in contrastive\nmultimodal learning and self-supervised clustering can be interpreted as\ninstances of this variance reduction scheme.\n", "link": "http://arxiv.org/abs/2408.15065v1", "date": "2024-08-27", "relevancy": 2.125, "topK": [{"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.5661}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5369}, {"title": "Breadcrumbs: Adversarial class-balanced sampling for long-tailed recognition", "link": "https://link.springer.com/chapter/10.1007/978-3-031-20053-3_37", "similarity": 0.5117}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20The%20Benefits%20of%20Balance%3A%20From%20Information%20Projections%20to%20Variance%0A%20%20Reduction&body=Title%3A%20The%20Benefits%20of%20Balance%3A%20From%20Information%20Projections%20to%20Variance%0A%20%20Reduction%0AAuthor%3A%20Lang%20Liu%20and%20Ronak%20Mehta%20and%20Soumik%20Pal%20and%20Zaid%20Harchaoui%0AAbstract%3A%20%20%20Data%20balancing%20across%20multiple%20modalities/sources%20appears%20in%20various%20forms%20in%0Aseveral%20foundation%20models%20%28e.g.%2C%20CLIP%20and%20DINO%29%20achieving%20universal%0Arepresentation%20learning.%20We%20show%20that%20this%20iterative%20algorithm%2C%20usually%20used%20to%0Aavoid%20representation%20collapse%2C%20enjoys%20an%20unsuspected%20benefit%3A%20reducing%20the%0Avariance%20of%20estimators%20that%20are%20functionals%20of%20the%20empirical%20distribution%20over%0Athese%20sources.%20We%20provide%20non-asymptotic%20bounds%20quantifying%20this%20variance%0Areduction%20effect%20and%20relate%20them%20to%20the%20eigendecays%20of%20appropriately%20defined%0AMarkov%20operators.%20We%20explain%20how%20various%20forms%20of%20data%20balancing%20in%20contrastive%0Amultimodal%20learning%20and%20self-supervised%20clustering%20can%20be%20interpreted%20as%0Ainstances%20of%20this%20variance%20reduction%20scheme.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2408.15065v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DThe%2520Benefits%2520of%2520Balance%253A%2520From%2520Information%2520Projections%2520to%2520Variance%250A%2520%2520Reduction%26entry.906535625%3DLang%2520Liu%2520and%2520Ronak%2520Mehta%2520and%2520Soumik%2520Pal%2520and%2520Zaid%2520Harchaoui%26entry.1292438233%3D%2520%2520Data%2520balancing%2520across%2520multiple%2520modalities/sources%2520appears%2520in%2520various%2520forms%2520in%250Aseveral%2520foundation%2520models%2520%2528e.g.%252C%2520CLIP%2520and%2520DINO%2529%2520achieving%2520universal%250Arepresentation%2520learning.%2520We%2520show%2520that%2520this%2520iterative%2520algorithm%252C%2520usually%2520used%2520to%250Aavoid%2520representation%2520collapse%252C%2520enjoys%2520an%2520unsuspected%2520benefit%253A%2520reducing%2520the%250Avariance%2520of%2520estimators%2520that%2520are%2520functionals%2520of%2520the%2520empirical%2520distribution%2520over%250Athese%2520sources.%2520We%2520provide%2520non-asymptotic%2520bounds%2520quantifying%2520this%2520variance%250Areduction%2520effect%2520and%2520relate%2520them%2520to%2520the%2520eigendecays%2520of%2520appropriately%2520defined%250AMarkov%2520operators.%2520We%2520explain%2520how%2520various%2520forms%2520of%2520data%2520balancing%2520in%2520contrastive%250Amultimodal%2520learning%2520and%2520self-supervised%2520clustering%2520can%2520be%2520interpreted%2520as%250Ainstances%2520of%2520this%2520variance%2520reduction%2520scheme.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2408.15065v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=The%20Benefits%20of%20Balance%3A%20From%20Information%20Projections%20to%20Variance%0A%20%20Reduction&entry.906535625=Lang%20Liu%20and%20Ronak%20Mehta%20and%20Soumik%20Pal%20and%20Zaid%20Harchaoui&entry.1292438233=%20%20Data%20balancing%20across%20multiple%20modalities/sources%20appears%20in%20various%20forms%20in%0Aseveral%20foundation%20models%20%28e.g.%2C%20CLIP%20and%20DINO%29%20achieving%20universal%0Arepresentation%20learning.%20We%20show%20that%20this%20iterative%20algorithm%2C%20usually%20used%20to%0Aavoid%20representation%20collapse%2C%20enjoys%20an%20unsuspected%20benefit%3A%20reducing%20the%0Avariance%20of%20estimators%20that%20are%20functionals%20of%20the%20empirical%20distribution%20over%0Athese%20sources.%20We%20provide%20non-asymptotic%20bounds%20quantifying%20this%20variance%0Areduction%20effect%20and%20relate%20them%20to%20the%20eigendecays%20of%20appropriately%20defined%0AMarkov%20operators.%20We%20explain%20how%20various%20forms%20of%20data%20balancing%20in%20contrastive%0Amultimodal%20learning%20and%20self-supervised%20clustering%20can%20be%20interpreted%20as%0Ainstances%20of%20this%20variance%20reduction%20scheme.%0A&entry.1838667208=http%3A//arxiv.org/abs/2408.15065v1&entry.124074799=Read"},
{"title": "Creating Image Datasets in Agricultural Environments using DALL.E:\n  Generative AI-Powered Large Language Model", "author": "Ranjan Sapkota and Manoj Karkee", "abstract": "  This research investigated the role of artificial intelligence (AI),\nspecifically the DALL.E model by OpenAI, in advancing data generation and\nvisualization techniques in agriculture. DALL.E, an advanced AI image\ngenerator, works alongside ChatGPT's language processing to transform text\ndescriptions and image clues into realistic visual representations of the\ncontent. The study used both approaches of image generation: text-to-image and\nimage-to image (variation). Six types of datasets depicting fruit crop\nenvironment were generated. These AI-generated images were then compared\nagainst ground truth images captured by sensors in real agricultural fields.\nThe comparison was based on Peak Signal-to-Noise Ratio (PSNR) and Feature\nSimilarity Index (FSIM) metrics. The image-to-image generation exhibited a\n5.78% increase in average PSNR over text-to-image methods, signifying superior\nimage clarity and quality. However, this method also resulted in a 10.23%\ndecrease in average FSIM, indicating a diminished structural and textural\nsimilarity to the original images. Similar to these measures, human evaluation\nalso showed that images generated using image-to-image-based method were more\nrealistic compared to those generated with text-to-image approach. The results\nhighlighted DALL.E's potential in generating realistic agricultural image\ndatasets and thus accelerating the development and adoption of imaging-based\nprecision agricultural solutions.\n", "link": "http://arxiv.org/abs/2307.08789v4", "date": "2024-08-27", "relevancy": 2.1234, "topK": [{"title": "VirtualModel: Generating Object-ID-retentive Human-object Interaction\n  Image by Diffusion Model for E-commerce Marketing", "link": "http://arxiv.org/abs/2405.09985v1", "similarity": 0.5325}, {"title": "Genie: Generative Interactive Environments", "link": "http://arxiv.org/abs/2402.15391v1", "similarity": 0.5315}, {"title": "Scaling Rectified Flow Transformers for High-Resolution Image Synthesis", "link": "http://arxiv.org/abs/2403.03206v1", "similarity": 0.5252}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Creating%20Image%20Datasets%20in%20Agricultural%20Environments%20using%20DALL.E%3A%0A%20%20Generative%20AI-Powered%20Large%20Language%20Model&body=Title%3A%20Creating%20Image%20Datasets%20in%20Agricultural%20Environments%20using%20DALL.E%3A%0A%20%20Generative%20AI-Powered%20Large%20Language%20Model%0AAuthor%3A%20Ranjan%20Sapkota%20and%20Manoj%20Karkee%0AAbstract%3A%20%20%20This%20research%20investigated%20the%20role%20of%20artificial%20intelligence%20%28AI%29%2C%0Aspecifically%20the%20DALL.E%20model%20by%20OpenAI%2C%20in%20advancing%20data%20generation%20and%0Avisualization%20techniques%20in%20agriculture.%20DALL.E%2C%20an%20advanced%20AI%20image%0Agenerator%2C%20works%20alongside%20ChatGPT%27s%20language%20processing%20to%20transform%20text%0Adescriptions%20and%20image%20clues%20into%20realistic%20visual%20representations%20of%20the%0Acontent.%20The%20study%20used%20both%20approaches%20of%20image%20generation%3A%20text-to-image%20and%0Aimage-to%20image%20%28variation%29.%20Six%20types%20of%20datasets%20depicting%20fruit%20crop%0Aenvironment%20were%20generated.%20These%20AI-generated%20images%20were%20then%20compared%0Aagainst%20ground%20truth%20images%20captured%20by%20sensors%20in%20real%20agricultural%20fields.%0AThe%20comparison%20was%20based%20on%20Peak%20Signal-to-Noise%20Ratio%20%28PSNR%29%20and%20Feature%0ASimilarity%20Index%20%28FSIM%29%20metrics.%20The%20image-to-image%20generation%20exhibited%20a%0A5.78%25%20increase%20in%20average%20PSNR%20over%20text-to-image%20methods%2C%20signifying%20superior%0Aimage%20clarity%20and%20quality.%20However%2C%20this%20method%20also%20resulted%20in%20a%2010.23%25%0Adecrease%20in%20average%20FSIM%2C%20indicating%20a%20diminished%20structural%20and%20textural%0Asimilarity%20to%20the%20original%20images.%20Similar%20to%20these%20measures%2C%20human%20evaluation%0Aalso%20showed%20that%20images%20generated%20using%20image-to-image-based%20method%20were%20more%0Arealistic%20compared%20to%20those%20generated%20with%20text-to-image%20approach.%20The%20results%0Ahighlighted%20DALL.E%27s%20potential%20in%20generating%20realistic%20agricultural%20image%0Adatasets%20and%20thus%20accelerating%20the%20development%20and%20adoption%20of%20imaging-based%0Aprecision%20agricultural%20solutions.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2307.08789v4%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DCreating%2520Image%2520Datasets%2520in%2520Agricultural%2520Environments%2520using%2520DALL.E%253A%250A%2520%2520Generative%2520AI-Powered%2520Large%2520Language%2520Model%26entry.906535625%3DRanjan%2520Sapkota%2520and%2520Manoj%2520Karkee%26entry.1292438233%3D%2520%2520This%2520research%2520investigated%2520the%2520role%2520of%2520artificial%2520intelligence%2520%2528AI%2529%252C%250Aspecifically%2520the%2520DALL.E%2520model%2520by%2520OpenAI%252C%2520in%2520advancing%2520data%2520generation%2520and%250Avisualization%2520techniques%2520in%2520agriculture.%2520DALL.E%252C%2520an%2520advanced%2520AI%2520image%250Agenerator%252C%2520works%2520alongside%2520ChatGPT%2527s%2520language%2520processing%2520to%2520transform%2520text%250Adescriptions%2520and%2520image%2520clues%2520into%2520realistic%2520visual%2520representations%2520of%2520the%250Acontent.%2520The%2520study%2520used%2520both%2520approaches%2520of%2520image%2520generation%253A%2520text-to-image%2520and%250Aimage-to%2520image%2520%2528variation%2529.%2520Six%2520types%2520of%2520datasets%2520depicting%2520fruit%2520crop%250Aenvironment%2520were%2520generated.%2520These%2520AI-generated%2520images%2520were%2520then%2520compared%250Aagainst%2520ground%2520truth%2520images%2520captured%2520by%2520sensors%2520in%2520real%2520agricultural%2520fields.%250AThe%2520comparison%2520was%2520based%2520on%2520Peak%2520Signal-to-Noise%2520Ratio%2520%2528PSNR%2529%2520and%2520Feature%250ASimilarity%2520Index%2520%2528FSIM%2529%2520metrics.%2520The%2520image-to-image%2520generation%2520exhibited%2520a%250A5.78%2525%2520increase%2520in%2520average%2520PSNR%2520over%2520text-to-image%2520methods%252C%2520signifying%2520superior%250Aimage%2520clarity%2520and%2520quality.%2520However%252C%2520this%2520method%2520also%2520resulted%2520in%2520a%252010.23%2525%250Adecrease%2520in%2520average%2520FSIM%252C%2520indicating%2520a%2520diminished%2520structural%2520and%2520textural%250Asimilarity%2520to%2520the%2520original%2520images.%2520Similar%2520to%2520these%2520measures%252C%2520human%2520evaluation%250Aalso%2520showed%2520that%2520images%2520generated%2520using%2520image-to-image-based%2520method%2520were%2520more%250Arealistic%2520compared%2520to%2520those%2520generated%2520with%2520text-to-image%2520approach.%2520The%2520results%250Ahighlighted%2520DALL.E%2527s%2520potential%2520in%2520generating%2520realistic%2520agricultural%2520image%250Adatasets%2520and%2520thus%2520accelerating%2520the%2520development%2520and%2520adoption%2520of%2520imaging-based%250Aprecision%2520agricultural%2520solutions.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2307.08789v4%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Creating%20Image%20Datasets%20in%20Agricultural%20Environments%20using%20DALL.E%3A%0A%20%20Generative%20AI-Powered%20Large%20Language%20Model&entry.906535625=Ranjan%20Sapkota%20and%20Manoj%20Karkee&entry.1292438233=%20%20This%20research%20investigated%20the%20role%20of%20artificial%20intelligence%20%28AI%29%2C%0Aspecifically%20the%20DALL.E%20model%20by%20OpenAI%2C%20in%20advancing%20data%20generation%20and%0Avisualization%20techniques%20in%20agriculture.%20DALL.E%2C%20an%20advanced%20AI%20image%0Agenerator%2C%20works%20alongside%20ChatGPT%27s%20language%20processing%20to%20transform%20text%0Adescriptions%20and%20image%20clues%20into%20realistic%20visual%20representations%20of%20the%0Acontent.%20The%20study%20used%20both%20approaches%20of%20image%20generation%3A%20text-to-image%20and%0Aimage-to%20image%20%28variation%29.%20Six%20types%20of%20datasets%20depicting%20fruit%20crop%0Aenvironment%20were%20generated.%20These%20AI-generated%20images%20were%20then%20compared%0Aagainst%20ground%20truth%20images%20captured%20by%20sensors%20in%20real%20agricultural%20fields.%0AThe%20comparison%20was%20based%20on%20Peak%20Signal-to-Noise%20Ratio%20%28PSNR%29%20and%20Feature%0ASimilarity%20Index%20%28FSIM%29%20metrics.%20The%20image-to-image%20generation%20exhibited%20a%0A5.78%25%20increase%20in%20average%20PSNR%20over%20text-to-image%20methods%2C%20signifying%20superior%0Aimage%20clarity%20and%20quality.%20However%2C%20this%20method%20also%20resulted%20in%20a%2010.23%25%0Adecrease%20in%20average%20FSIM%2C%20indicating%20a%20diminished%20structural%20and%20textural%0Asimilarity%20to%20the%20original%20images.%20Similar%20to%20these%20measures%2C%20human%20evaluation%0Aalso%20showed%20that%20images%20generated%20using%20image-to-image-based%20method%20were%20more%0Arealistic%20compared%20to%20those%20generated%20with%20text-to-image%20approach.%20The%20results%0Ahighlighted%20DALL.E%27s%20potential%20in%20generating%20realistic%20agricultural%20image%0Adatasets%20and%20thus%20accelerating%20the%20development%20and%20adoption%20of%20imaging-based%0Aprecision%20agricultural%20solutions.%0A&entry.1838667208=http%3A//arxiv.org/abs/2307.08789v4&entry.124074799=Read"},
{"title": "Mamba2MIL: State Space Duality Based Multiple Instance Learning for\n  Computational Pathology", "author": "Yuqi Zhang and Xiaoqian Zhang and Jiakai Wang and Yuancheng Yang and Taiying Peng and Chao Tong", "abstract": "  Computational pathology (CPath) has significantly advanced the clinical\npractice of pathology. Despite the progress made, Multiple Instance Learning\n(MIL), a promising paradigm within CPath, continues to face challenges,\nparticularly related to incomplete information utilization. Existing\nframeworks, such as those based on Convolutional Neural Networks (CNNs),\nattention, and selective scan space state sequential model (SSM), lack\nsufficient flexibility and scalability in fusing diverse features, and cannot\neffectively fuse diverse features. Additionally, current approaches do not\nadequately exploit order-related and order-independent features, resulting in\nsuboptimal utilization of sequence information. To address these limitations,\nwe propose a novel MIL framework called Mamba2MIL. Our framework utilizes the\nstate space duality model (SSD) to model long sequences of patches of whole\nslide images (WSIs), which, combined with weighted feature selection, supports\nthe fusion processing of more branching features and can be extended according\nto specific application needs. Moreover, we introduce a sequence transformation\nmethod tailored to varying WSI sizes, which enhances sequence-independent\nfeatures while preserving local sequence information, thereby improving\nsequence information utilization. Extensive experiments demonstrate that\nMamba2MIL surpasses state-of-the-art MIL methods. We conducted extensive\nexperiments across multiple datasets, achieving improvements in nearly all\nperformance metrics. Specifically, on the NSCLC dataset, Mamba2MIL achieves a\nbinary tumor classification AUC of 0.9533 and an accuracy of 0.8794. On the\nBRACS dataset, it achieves a multiclass classification AUC of 0.7986 and an\naccuracy of 0.4981. The code is available at\nhttps://github.com/YuqiZhang-Buaa/Mamba2MIL.\n", "link": "http://arxiv.org/abs/2408.15032v1", "date": "2024-08-27", "relevancy": 2.1101, "topK": [{"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.5333}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5305}, {"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.5223}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Mamba2MIL%3A%20State%20Space%20Duality%20Based%20Multiple%20Instance%20Learning%20for%0A%20%20Computational%20Pathology&body=Title%3A%20Mamba2MIL%3A%20State%20Space%20Duality%20Based%20Multiple%20Instance%20Learning%20for%0A%20%20Computational%20Pathology%0AAuthor%3A%20Yuqi%20Zhang%20and%20Xiaoqian%20Zhang%20and%20Jiakai%20Wang%20and%20Yuancheng%20Yang%20and%20Taiying%20Peng%20and%20Chao%20Tong%0AAbstract%3A%20%20%20Computational%20pathology%20%28CPath%29%20has%20significantly%20advanced%20the%20clinical%0Apractice%20of%20pathology.%20Despite%20the%20progress%20made%2C%20Multiple%20Instance%20Learning%0A%28MIL%29%2C%20a%20promising%20paradigm%20within%20CPath%2C%20continues%20to%20face%20challenges%2C%0Aparticularly%20related%20to%20incomplete%20information%20utilization.%20Existing%0Aframeworks%2C%20such%20as%20those%20based%20on%20Convolutional%20Neural%20Networks%20%28CNNs%29%2C%0Aattention%2C%20and%20selective%20scan%20space%20state%20sequential%20model%20%28SSM%29%2C%20lack%0Asufficient%20flexibility%20and%20scalability%20in%20fusing%20diverse%20features%2C%20and%20cannot%0Aeffectively%20fuse%20diverse%20features.%20Additionally%2C%20current%20approaches%20do%20not%0Aadequately%20exploit%20order-related%20and%20order-independent%20features%2C%20resulting%20in%0Asuboptimal%20utilization%20of%20sequence%20information.%20To%20address%20these%20limitations%2C%0Awe%20propose%20a%20novel%20MIL%20framework%20called%20Mamba2MIL.%20Our%20framework%20utilizes%20the%0Astate%20space%20duality%20model%20%28SSD%29%20to%20model%20long%20sequences%20of%20patches%20of%20whole%0Aslide%20images%20%28WSIs%29%2C%20which%2C%20combined%20with%20weighted%20feature%20selection%2C%20supports%0Athe%20fusion%20processing%20of%20more%20branching%20features%20and%20can%20be%20extended%20according%0Ato%20specific%20application%20needs.%20Moreover%2C%20we%20introduce%20a%20sequence%20transformation%0Amethod%20tailored%20to%20varying%20WSI%20sizes%2C%20which%20enhances%20sequence-independent%0Afeatures%20while%20preserving%20local%20sequence%20information%2C%20thereby%20improving%0Asequence%20information%20utilization.%20Extensive%20experiments%20demonstrate%20that%0AMamba2MIL%20surpasses%20state-of-the-art%20MIL%20methods.%20We%20conducted%20extensive%0Aexperiments%20across%20multiple%20datasets%2C%20achieving%20improvements%20in%20nearly%20all%0Aperformance%20metrics.%20Specifically%2C%20on%20the%20NSCLC%20dataset%2C%20Mamba2MIL%20achieves%20a%0Abinary%20tumor%20classification%20AUC%20of%200.9533%20and%20an%20accuracy%20of%200.8794.%20On%20the%0ABRACS%20dataset%2C%20it%20achieves%20a%20multiclass%20classification%20AUC%20of%200.7986%20and%20an%0Aaccuracy%20of%200.4981.%20The%20code%20is%20available%20at%0Ahttps%3A//github.com/YuqiZhang-Buaa/Mamba2MIL.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2408.15032v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DMamba2MIL%253A%2520State%2520Space%2520Duality%2520Based%2520Multiple%2520Instance%2520Learning%2520for%250A%2520%2520Computational%2520Pathology%26entry.906535625%3DYuqi%2520Zhang%2520and%2520Xiaoqian%2520Zhang%2520and%2520Jiakai%2520Wang%2520and%2520Yuancheng%2520Yang%2520and%2520Taiying%2520Peng%2520and%2520Chao%2520Tong%26entry.1292438233%3D%2520%2520Computational%2520pathology%2520%2528CPath%2529%2520has%2520significantly%2520advanced%2520the%2520clinical%250Apractice%2520of%2520pathology.%2520Despite%2520the%2520progress%2520made%252C%2520Multiple%2520Instance%2520Learning%250A%2528MIL%2529%252C%2520a%2520promising%2520paradigm%2520within%2520CPath%252C%2520continues%2520to%2520face%2520challenges%252C%250Aparticularly%2520related%2520to%2520incomplete%2520information%2520utilization.%2520Existing%250Aframeworks%252C%2520such%2520as%2520those%2520based%2520on%2520Convolutional%2520Neural%2520Networks%2520%2528CNNs%2529%252C%250Aattention%252C%2520and%2520selective%2520scan%2520space%2520state%2520sequential%2520model%2520%2528SSM%2529%252C%2520lack%250Asufficient%2520flexibility%2520and%2520scalability%2520in%2520fusing%2520diverse%2520features%252C%2520and%2520cannot%250Aeffectively%2520fuse%2520diverse%2520features.%2520Additionally%252C%2520current%2520approaches%2520do%2520not%250Aadequately%2520exploit%2520order-related%2520and%2520order-independent%2520features%252C%2520resulting%2520in%250Asuboptimal%2520utilization%2520of%2520sequence%2520information.%2520To%2520address%2520these%2520limitations%252C%250Awe%2520propose%2520a%2520novel%2520MIL%2520framework%2520called%2520Mamba2MIL.%2520Our%2520framework%2520utilizes%2520the%250Astate%2520space%2520duality%2520model%2520%2528SSD%2529%2520to%2520model%2520long%2520sequences%2520of%2520patches%2520of%2520whole%250Aslide%2520images%2520%2528WSIs%2529%252C%2520which%252C%2520combined%2520with%2520weighted%2520feature%2520selection%252C%2520supports%250Athe%2520fusion%2520processing%2520of%2520more%2520branching%2520features%2520and%2520can%2520be%2520extended%2520according%250Ato%2520specific%2520application%2520needs.%2520Moreover%252C%2520we%2520introduce%2520a%2520sequence%2520transformation%250Amethod%2520tailored%2520to%2520varying%2520WSI%2520sizes%252C%2520which%2520enhances%2520sequence-independent%250Afeatures%2520while%2520preserving%2520local%2520sequence%2520information%252C%2520thereby%2520improving%250Asequence%2520information%2520utilization.%2520Extensive%2520experiments%2520demonstrate%2520that%250AMamba2MIL%2520surpasses%2520state-of-the-art%2520MIL%2520methods.%2520We%2520conducted%2520extensive%250Aexperiments%2520across%2520multiple%2520datasets%252C%2520achieving%2520improvements%2520in%2520nearly%2520all%250Aperformance%2520metrics.%2520Specifically%252C%2520on%2520the%2520NSCLC%2520dataset%252C%2520Mamba2MIL%2520achieves%2520a%250Abinary%2520tumor%2520classification%2520AUC%2520of%25200.9533%2520and%2520an%2520accuracy%2520of%25200.8794.%2520On%2520the%250ABRACS%2520dataset%252C%2520it%2520achieves%2520a%2520multiclass%2520classification%2520AUC%2520of%25200.7986%2520and%2520an%250Aaccuracy%2520of%25200.4981.%2520The%2520code%2520is%2520available%2520at%250Ahttps%253A//github.com/YuqiZhang-Buaa/Mamba2MIL.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2408.15032v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Mamba2MIL%3A%20State%20Space%20Duality%20Based%20Multiple%20Instance%20Learning%20for%0A%20%20Computational%20Pathology&entry.906535625=Yuqi%20Zhang%20and%20Xiaoqian%20Zhang%20and%20Jiakai%20Wang%20and%20Yuancheng%20Yang%20and%20Taiying%20Peng%20and%20Chao%20Tong&entry.1292438233=%20%20Computational%20pathology%20%28CPath%29%20has%20significantly%20advanced%20the%20clinical%0Apractice%20of%20pathology.%20Despite%20the%20progress%20made%2C%20Multiple%20Instance%20Learning%0A%28MIL%29%2C%20a%20promising%20paradigm%20within%20CPath%2C%20continues%20to%20face%20challenges%2C%0Aparticularly%20related%20to%20incomplete%20information%20utilization.%20Existing%0Aframeworks%2C%20such%20as%20those%20based%20on%20Convolutional%20Neural%20Networks%20%28CNNs%29%2C%0Aattention%2C%20and%20selective%20scan%20space%20state%20sequential%20model%20%28SSM%29%2C%20lack%0Asufficient%20flexibility%20and%20scalability%20in%20fusing%20diverse%20features%2C%20and%20cannot%0Aeffectively%20fuse%20diverse%20features.%20Additionally%2C%20current%20approaches%20do%20not%0Aadequately%20exploit%20order-related%20and%20order-independent%20features%2C%20resulting%20in%0Asuboptimal%20utilization%20of%20sequence%20information.%20To%20address%20these%20limitations%2C%0Awe%20propose%20a%20novel%20MIL%20framework%20called%20Mamba2MIL.%20Our%20framework%20utilizes%20the%0Astate%20space%20duality%20model%20%28SSD%29%20to%20model%20long%20sequences%20of%20patches%20of%20whole%0Aslide%20images%20%28WSIs%29%2C%20which%2C%20combined%20with%20weighted%20feature%20selection%2C%20supports%0Athe%20fusion%20processing%20of%20more%20branching%20features%20and%20can%20be%20extended%20according%0Ato%20specific%20application%20needs.%20Moreover%2C%20we%20introduce%20a%20sequence%20transformation%0Amethod%20tailored%20to%20varying%20WSI%20sizes%2C%20which%20enhances%20sequence-independent%0Afeatures%20while%20preserving%20local%20sequence%20information%2C%20thereby%20improving%0Asequence%20information%20utilization.%20Extensive%20experiments%20demonstrate%20that%0AMamba2MIL%20surpasses%20state-of-the-art%20MIL%20methods.%20We%20conducted%20extensive%0Aexperiments%20across%20multiple%20datasets%2C%20achieving%20improvements%20in%20nearly%20all%0Aperformance%20metrics.%20Specifically%2C%20on%20the%20NSCLC%20dataset%2C%20Mamba2MIL%20achieves%20a%0Abinary%20tumor%20classification%20AUC%20of%200.9533%20and%20an%20accuracy%20of%200.8794.%20On%20the%0ABRACS%20dataset%2C%20it%20achieves%20a%20multiclass%20classification%20AUC%20of%200.7986%20and%20an%0Aaccuracy%20of%200.4981.%20The%20code%20is%20available%20at%0Ahttps%3A//github.com/YuqiZhang-Buaa/Mamba2MIL.%0A&entry.1838667208=http%3A//arxiv.org/abs/2408.15032v1&entry.124074799=Read"},
{"title": "T-FAKE: Synthesizing Thermal Images for Facial Landmarking", "author": "Philipp Flotho and Moritz Piening and Anna Kukleva and Gabriele Steidl", "abstract": "  Facial analysis is a key component in a wide range of applications such as\nsecurity, autonomous driving, entertainment, and healthcare. Despite the\navailability of various facial RGB datasets, the thermal modality, which plays\na crucial role in life sciences, medicine, and biometrics, has been largely\noverlooked. To address this gap, we introduce the T-FAKE dataset, a new\nlarge-scale synthetic thermal dataset with sparse and dense landmarks. To\nfacilitate the creation of the dataset, we propose a novel RGB2Thermal loss\nfunction, which enables the transfer of thermal style to RGB faces. By\nutilizing the Wasserstein distance between thermal and RGB patches and the\nstatistical analysis of clinical temperature distributions on faces, we ensure\nthat the generated thermal images closely resemble real samples. Using\nRGB2Thermal style transfer based on our RGB2Thermal loss function, we create\nthe T-FAKE dataset, a large-scale synthetic thermal dataset of faces.\nLeveraging our novel T-FAKE dataset, probabilistic landmark prediction, and\nlabel adaptation networks, we demonstrate significant improvements in landmark\ndetection methods on thermal images across different landmark conventions. Our\nmodels show excellent performance with both sparse 70-point landmarks and dense\n478-point landmark annotations. Our code and models are available at\nhttps://github.com/phflot/tfake.\n", "link": "http://arxiv.org/abs/2408.15127v1", "date": "2024-08-27", "relevancy": 2.1087, "topK": [{"title": "RTG-SLAM: Real-time 3D Reconstruction at Scale using Gaussian Splatting", "link": "http://arxiv.org/abs/2404.19706v1", "similarity": 0.5326}, {"title": "Total Selfie: Generating Full-Body Selfies", "link": "http://arxiv.org/abs/2308.14740v2", "similarity": 0.53}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5206}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20T-FAKE%3A%20Synthesizing%20Thermal%20Images%20for%20Facial%20Landmarking&body=Title%3A%20T-FAKE%3A%20Synthesizing%20Thermal%20Images%20for%20Facial%20Landmarking%0AAuthor%3A%20Philipp%20Flotho%20and%20Moritz%20Piening%20and%20Anna%20Kukleva%20and%20Gabriele%20Steidl%0AAbstract%3A%20%20%20Facial%20analysis%20is%20a%20key%20component%20in%20a%20wide%20range%20of%20applications%20such%20as%0Asecurity%2C%20autonomous%20driving%2C%20entertainment%2C%20and%20healthcare.%20Despite%20the%0Aavailability%20of%20various%20facial%20RGB%20datasets%2C%20the%20thermal%20modality%2C%20which%20plays%0Aa%20crucial%20role%20in%20life%20sciences%2C%20medicine%2C%20and%20biometrics%2C%20has%20been%20largely%0Aoverlooked.%20To%20address%20this%20gap%2C%20we%20introduce%20the%20T-FAKE%20dataset%2C%20a%20new%0Alarge-scale%20synthetic%20thermal%20dataset%20with%20sparse%20and%20dense%20landmarks.%20To%0Afacilitate%20the%20creation%20of%20the%20dataset%2C%20we%20propose%20a%20novel%20RGB2Thermal%20loss%0Afunction%2C%20which%20enables%20the%20transfer%20of%20thermal%20style%20to%20RGB%20faces.%20By%0Autilizing%20the%20Wasserstein%20distance%20between%20thermal%20and%20RGB%20patches%20and%20the%0Astatistical%20analysis%20of%20clinical%20temperature%20distributions%20on%20faces%2C%20we%20ensure%0Athat%20the%20generated%20thermal%20images%20closely%20resemble%20real%20samples.%20Using%0ARGB2Thermal%20style%20transfer%20based%20on%20our%20RGB2Thermal%20loss%20function%2C%20we%20create%0Athe%20T-FAKE%20dataset%2C%20a%20large-scale%20synthetic%20thermal%20dataset%20of%20faces.%0ALeveraging%20our%20novel%20T-FAKE%20dataset%2C%20probabilistic%20landmark%20prediction%2C%20and%0Alabel%20adaptation%20networks%2C%20we%20demonstrate%20significant%20improvements%20in%20landmark%0Adetection%20methods%20on%20thermal%20images%20across%20different%20landmark%20conventions.%20Our%0Amodels%20show%20excellent%20performance%20with%20both%20sparse%2070-point%20landmarks%20and%20dense%0A478-point%20landmark%20annotations.%20Our%20code%20and%20models%20are%20available%20at%0Ahttps%3A//github.com/phflot/tfake.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2408.15127v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DT-FAKE%253A%2520Synthesizing%2520Thermal%2520Images%2520for%2520Facial%2520Landmarking%26entry.906535625%3DPhilipp%2520Flotho%2520and%2520Moritz%2520Piening%2520and%2520Anna%2520Kukleva%2520and%2520Gabriele%2520Steidl%26entry.1292438233%3D%2520%2520Facial%2520analysis%2520is%2520a%2520key%2520component%2520in%2520a%2520wide%2520range%2520of%2520applications%2520such%2520as%250Asecurity%252C%2520autonomous%2520driving%252C%2520entertainment%252C%2520and%2520healthcare.%2520Despite%2520the%250Aavailability%2520of%2520various%2520facial%2520RGB%2520datasets%252C%2520the%2520thermal%2520modality%252C%2520which%2520plays%250Aa%2520crucial%2520role%2520in%2520life%2520sciences%252C%2520medicine%252C%2520and%2520biometrics%252C%2520has%2520been%2520largely%250Aoverlooked.%2520To%2520address%2520this%2520gap%252C%2520we%2520introduce%2520the%2520T-FAKE%2520dataset%252C%2520a%2520new%250Alarge-scale%2520synthetic%2520thermal%2520dataset%2520with%2520sparse%2520and%2520dense%2520landmarks.%2520To%250Afacilitate%2520the%2520creation%2520of%2520the%2520dataset%252C%2520we%2520propose%2520a%2520novel%2520RGB2Thermal%2520loss%250Afunction%252C%2520which%2520enables%2520the%2520transfer%2520of%2520thermal%2520style%2520to%2520RGB%2520faces.%2520By%250Autilizing%2520the%2520Wasserstein%2520distance%2520between%2520thermal%2520and%2520RGB%2520patches%2520and%2520the%250Astatistical%2520analysis%2520of%2520clinical%2520temperature%2520distributions%2520on%2520faces%252C%2520we%2520ensure%250Athat%2520the%2520generated%2520thermal%2520images%2520closely%2520resemble%2520real%2520samples.%2520Using%250ARGB2Thermal%2520style%2520transfer%2520based%2520on%2520our%2520RGB2Thermal%2520loss%2520function%252C%2520we%2520create%250Athe%2520T-FAKE%2520dataset%252C%2520a%2520large-scale%2520synthetic%2520thermal%2520dataset%2520of%2520faces.%250ALeveraging%2520our%2520novel%2520T-FAKE%2520dataset%252C%2520probabilistic%2520landmark%2520prediction%252C%2520and%250Alabel%2520adaptation%2520networks%252C%2520we%2520demonstrate%2520significant%2520improvements%2520in%2520landmark%250Adetection%2520methods%2520on%2520thermal%2520images%2520across%2520different%2520landmark%2520conventions.%2520Our%250Amodels%2520show%2520excellent%2520performance%2520with%2520both%2520sparse%252070-point%2520landmarks%2520and%2520dense%250A478-point%2520landmark%2520annotations.%2520Our%2520code%2520and%2520models%2520are%2520available%2520at%250Ahttps%253A//github.com/phflot/tfake.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2408.15127v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=T-FAKE%3A%20Synthesizing%20Thermal%20Images%20for%20Facial%20Landmarking&entry.906535625=Philipp%20Flotho%20and%20Moritz%20Piening%20and%20Anna%20Kukleva%20and%20Gabriele%20Steidl&entry.1292438233=%20%20Facial%20analysis%20is%20a%20key%20component%20in%20a%20wide%20range%20of%20applications%20such%20as%0Asecurity%2C%20autonomous%20driving%2C%20entertainment%2C%20and%20healthcare.%20Despite%20the%0Aavailability%20of%20various%20facial%20RGB%20datasets%2C%20the%20thermal%20modality%2C%20which%20plays%0Aa%20crucial%20role%20in%20life%20sciences%2C%20medicine%2C%20and%20biometrics%2C%20has%20been%20largely%0Aoverlooked.%20To%20address%20this%20gap%2C%20we%20introduce%20the%20T-FAKE%20dataset%2C%20a%20new%0Alarge-scale%20synthetic%20thermal%20dataset%20with%20sparse%20and%20dense%20landmarks.%20To%0Afacilitate%20the%20creation%20of%20the%20dataset%2C%20we%20propose%20a%20novel%20RGB2Thermal%20loss%0Afunction%2C%20which%20enables%20the%20transfer%20of%20thermal%20style%20to%20RGB%20faces.%20By%0Autilizing%20the%20Wasserstein%20distance%20between%20thermal%20and%20RGB%20patches%20and%20the%0Astatistical%20analysis%20of%20clinical%20temperature%20distributions%20on%20faces%2C%20we%20ensure%0Athat%20the%20generated%20thermal%20images%20closely%20resemble%20real%20samples.%20Using%0ARGB2Thermal%20style%20transfer%20based%20on%20our%20RGB2Thermal%20loss%20function%2C%20we%20create%0Athe%20T-FAKE%20dataset%2C%20a%20large-scale%20synthetic%20thermal%20dataset%20of%20faces.%0ALeveraging%20our%20novel%20T-FAKE%20dataset%2C%20probabilistic%20landmark%20prediction%2C%20and%0Alabel%20adaptation%20networks%2C%20we%20demonstrate%20significant%20improvements%20in%20landmark%0Adetection%20methods%20on%20thermal%20images%20across%20different%20landmark%20conventions.%20Our%0Amodels%20show%20excellent%20performance%20with%20both%20sparse%2070-point%20landmarks%20and%20dense%0A478-point%20landmark%20annotations.%20Our%20code%20and%20models%20are%20available%20at%0Ahttps%3A//github.com/phflot/tfake.%0A&entry.1838667208=http%3A//arxiv.org/abs/2408.15127v1&entry.124074799=Read"},
{"title": "Dynamic Object Queries for Transformer-based Incremental Object\n  Detection", "author": "Jichuan Zhang and Wei Li and Shuang Cheng and Ya-Li Li and Shengjin Wang", "abstract": "  Incremental object detection (IOD) aims to sequentially learn new classes,\nwhile maintaining the capability to locate and identify old ones. As the\ntraining data arrives with annotations only with new classes, IOD suffers from\ncatastrophic forgetting. Prior methodologies mainly tackle the forgetting issue\nthrough knowledge distillation and exemplar replay, ignoring the conflict\nbetween limited model capacity and increasing knowledge. In this paper, we\nexplore \\textit{dynamic object queries} for incremental object detection built\non Transformer architecture. We propose the \\textbf{Dy}namic object\n\\textbf{Q}uery-based \\textbf{DE}tection \\textbf{TR}ansformer (DyQ-DETR), which\nincrementally expands the model representation ability to achieve\nstability-plasticity tradeoff. First, a new set of learnable object queries are\nfed into the decoder to represent new classes. These new object queries are\naggregated with those from previous phases to adapt both old and new knowledge\nwell. Second, we propose the isolated bipartite matching for object queries in\ndifferent phases, based on disentangled self-attention. The interaction among\nthe object queries at different phases is eliminated to reduce inter-class\nconfusion. Thanks to the separate supervision and computation over object\nqueries, we further present the risk-balanced partial calibration for effective\nexemplar replay. Extensive experiments demonstrate that DyQ-DETR significantly\nsurpasses the state-of-the-art methods, with limited parameter overhead. Code\nwill be made publicly available.\n", "link": "http://arxiv.org/abs/2407.21687v2", "date": "2024-08-27", "relevancy": 2.1006, "topK": [{"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.5293}, {"title": "Boosted dynamic neural networks", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/26302", "similarity": 0.5285}, {"title": "UGG: Unified Generative Grasping", "link": "https://arxiv.org/abs/2311.16917", "similarity": 0.5202}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Dynamic%20Object%20Queries%20for%20Transformer-based%20Incremental%20Object%0A%20%20Detection&body=Title%3A%20Dynamic%20Object%20Queries%20for%20Transformer-based%20Incremental%20Object%0A%20%20Detection%0AAuthor%3A%20Jichuan%20Zhang%20and%20Wei%20Li%20and%20Shuang%20Cheng%20and%20Ya-Li%20Li%20and%20Shengjin%20Wang%0AAbstract%3A%20%20%20Incremental%20object%20detection%20%28IOD%29%20aims%20to%20sequentially%20learn%20new%20classes%2C%0Awhile%20maintaining%20the%20capability%20to%20locate%20and%20identify%20old%20ones.%20As%20the%0Atraining%20data%20arrives%20with%20annotations%20only%20with%20new%20classes%2C%20IOD%20suffers%20from%0Acatastrophic%20forgetting.%20Prior%20methodologies%20mainly%20tackle%20the%20forgetting%20issue%0Athrough%20knowledge%20distillation%20and%20exemplar%20replay%2C%20ignoring%20the%20conflict%0Abetween%20limited%20model%20capacity%20and%20increasing%20knowledge.%20In%20this%20paper%2C%20we%0Aexplore%20%5Ctextit%7Bdynamic%20object%20queries%7D%20for%20incremental%20object%20detection%20built%0Aon%20Transformer%20architecture.%20We%20propose%20the%20%5Ctextbf%7BDy%7Dnamic%20object%0A%5Ctextbf%7BQ%7Duery-based%20%5Ctextbf%7BDE%7Dtection%20%5Ctextbf%7BTR%7Dansformer%20%28DyQ-DETR%29%2C%20which%0Aincrementally%20expands%20the%20model%20representation%20ability%20to%20achieve%0Astability-plasticity%20tradeoff.%20First%2C%20a%20new%20set%20of%20learnable%20object%20queries%20are%0Afed%20into%20the%20decoder%20to%20represent%20new%20classes.%20These%20new%20object%20queries%20are%0Aaggregated%20with%20those%20from%20previous%20phases%20to%20adapt%20both%20old%20and%20new%20knowledge%0Awell.%20Second%2C%20we%20propose%20the%20isolated%20bipartite%20matching%20for%20object%20queries%20in%0Adifferent%20phases%2C%20based%20on%20disentangled%20self-attention.%20The%20interaction%20among%0Athe%20object%20queries%20at%20different%20phases%20is%20eliminated%20to%20reduce%20inter-class%0Aconfusion.%20Thanks%20to%20the%20separate%20supervision%20and%20computation%20over%20object%0Aqueries%2C%20we%20further%20present%20the%20risk-balanced%20partial%20calibration%20for%20effective%0Aexemplar%20replay.%20Extensive%20experiments%20demonstrate%20that%20DyQ-DETR%20significantly%0Asurpasses%20the%20state-of-the-art%20methods%2C%20with%20limited%20parameter%20overhead.%20Code%0Awill%20be%20made%20publicly%20available.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2407.21687v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DDynamic%2520Object%2520Queries%2520for%2520Transformer-based%2520Incremental%2520Object%250A%2520%2520Detection%26entry.906535625%3DJichuan%2520Zhang%2520and%2520Wei%2520Li%2520and%2520Shuang%2520Cheng%2520and%2520Ya-Li%2520Li%2520and%2520Shengjin%2520Wang%26entry.1292438233%3D%2520%2520Incremental%2520object%2520detection%2520%2528IOD%2529%2520aims%2520to%2520sequentially%2520learn%2520new%2520classes%252C%250Awhile%2520maintaining%2520the%2520capability%2520to%2520locate%2520and%2520identify%2520old%2520ones.%2520As%2520the%250Atraining%2520data%2520arrives%2520with%2520annotations%2520only%2520with%2520new%2520classes%252C%2520IOD%2520suffers%2520from%250Acatastrophic%2520forgetting.%2520Prior%2520methodologies%2520mainly%2520tackle%2520the%2520forgetting%2520issue%250Athrough%2520knowledge%2520distillation%2520and%2520exemplar%2520replay%252C%2520ignoring%2520the%2520conflict%250Abetween%2520limited%2520model%2520capacity%2520and%2520increasing%2520knowledge.%2520In%2520this%2520paper%252C%2520we%250Aexplore%2520%255Ctextit%257Bdynamic%2520object%2520queries%257D%2520for%2520incremental%2520object%2520detection%2520built%250Aon%2520Transformer%2520architecture.%2520We%2520propose%2520the%2520%255Ctextbf%257BDy%257Dnamic%2520object%250A%255Ctextbf%257BQ%257Duery-based%2520%255Ctextbf%257BDE%257Dtection%2520%255Ctextbf%257BTR%257Dansformer%2520%2528DyQ-DETR%2529%252C%2520which%250Aincrementally%2520expands%2520the%2520model%2520representation%2520ability%2520to%2520achieve%250Astability-plasticity%2520tradeoff.%2520First%252C%2520a%2520new%2520set%2520of%2520learnable%2520object%2520queries%2520are%250Afed%2520into%2520the%2520decoder%2520to%2520represent%2520new%2520classes.%2520These%2520new%2520object%2520queries%2520are%250Aaggregated%2520with%2520those%2520from%2520previous%2520phases%2520to%2520adapt%2520both%2520old%2520and%2520new%2520knowledge%250Awell.%2520Second%252C%2520we%2520propose%2520the%2520isolated%2520bipartite%2520matching%2520for%2520object%2520queries%2520in%250Adifferent%2520phases%252C%2520based%2520on%2520disentangled%2520self-attention.%2520The%2520interaction%2520among%250Athe%2520object%2520queries%2520at%2520different%2520phases%2520is%2520eliminated%2520to%2520reduce%2520inter-class%250Aconfusion.%2520Thanks%2520to%2520the%2520separate%2520supervision%2520and%2520computation%2520over%2520object%250Aqueries%252C%2520we%2520further%2520present%2520the%2520risk-balanced%2520partial%2520calibration%2520for%2520effective%250Aexemplar%2520replay.%2520Extensive%2520experiments%2520demonstrate%2520that%2520DyQ-DETR%2520significantly%250Asurpasses%2520the%2520state-of-the-art%2520methods%252C%2520with%2520limited%2520parameter%2520overhead.%2520Code%250Awill%2520be%2520made%2520publicly%2520available.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2407.21687v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Dynamic%20Object%20Queries%20for%20Transformer-based%20Incremental%20Object%0A%20%20Detection&entry.906535625=Jichuan%20Zhang%20and%20Wei%20Li%20and%20Shuang%20Cheng%20and%20Ya-Li%20Li%20and%20Shengjin%20Wang&entry.1292438233=%20%20Incremental%20object%20detection%20%28IOD%29%20aims%20to%20sequentially%20learn%20new%20classes%2C%0Awhile%20maintaining%20the%20capability%20to%20locate%20and%20identify%20old%20ones.%20As%20the%0Atraining%20data%20arrives%20with%20annotations%20only%20with%20new%20classes%2C%20IOD%20suffers%20from%0Acatastrophic%20forgetting.%20Prior%20methodologies%20mainly%20tackle%20the%20forgetting%20issue%0Athrough%20knowledge%20distillation%20and%20exemplar%20replay%2C%20ignoring%20the%20conflict%0Abetween%20limited%20model%20capacity%20and%20increasing%20knowledge.%20In%20this%20paper%2C%20we%0Aexplore%20%5Ctextit%7Bdynamic%20object%20queries%7D%20for%20incremental%20object%20detection%20built%0Aon%20Transformer%20architecture.%20We%20propose%20the%20%5Ctextbf%7BDy%7Dnamic%20object%0A%5Ctextbf%7BQ%7Duery-based%20%5Ctextbf%7BDE%7Dtection%20%5Ctextbf%7BTR%7Dansformer%20%28DyQ-DETR%29%2C%20which%0Aincrementally%20expands%20the%20model%20representation%20ability%20to%20achieve%0Astability-plasticity%20tradeoff.%20First%2C%20a%20new%20set%20of%20learnable%20object%20queries%20are%0Afed%20into%20the%20decoder%20to%20represent%20new%20classes.%20These%20new%20object%20queries%20are%0Aaggregated%20with%20those%20from%20previous%20phases%20to%20adapt%20both%20old%20and%20new%20knowledge%0Awell.%20Second%2C%20we%20propose%20the%20isolated%20bipartite%20matching%20for%20object%20queries%20in%0Adifferent%20phases%2C%20based%20on%20disentangled%20self-attention.%20The%20interaction%20among%0Athe%20object%20queries%20at%20different%20phases%20is%20eliminated%20to%20reduce%20inter-class%0Aconfusion.%20Thanks%20to%20the%20separate%20supervision%20and%20computation%20over%20object%0Aqueries%2C%20we%20further%20present%20the%20risk-balanced%20partial%20calibration%20for%20effective%0Aexemplar%20replay.%20Extensive%20experiments%20demonstrate%20that%20DyQ-DETR%20significantly%0Asurpasses%20the%20state-of-the-art%20methods%2C%20with%20limited%20parameter%20overhead.%20Code%0Awill%20be%20made%20publicly%20available.%0A&entry.1838667208=http%3A//arxiv.org/abs/2407.21687v2&entry.124074799=Read"},
{"title": "Into the Unknown Unknowns: Engaged Human Learning through Participation\n  in Language Model Agent Conversations", "author": "Yucheng Jiang and Yijia Shao and Dekun Ma and Sina J. Semnani and Monica S. Lam", "abstract": "  While language model (LM)-powered chatbots and generative search engines\nexcel at answering concrete queries, discovering information in the terrain of\nunknown unknowns remains challenging for users. To emulate the common\neducational scenario where children/students learn by listening to and\nparticipating in conversations of their parents/teachers, we create\nCollaborative STORM (Co-STORM). Unlike QA systems that require users to ask all\nthe questions, Co-STORM lets users observe and occasionally steer the discourse\namong several LM agents. The agents ask questions on the user's behalf,\nallowing the user to discover unknown unknowns serendipitously. To facilitate\nuser interaction, Co-STORM assists users in tracking the discourse by\norganizing the uncovered information into a dynamic mind map, ultimately\ngenerating a comprehensive report as takeaways. For automatic evaluation, we\nconstruct the WildSeek dataset by collecting real information-seeking records\nwith user goals. Co-STORM outperforms baseline methods on both discourse trace\nand report quality. In a further human evaluation, 70% of participants prefer\nCo-STORM over a search engine, and 78% favor it over a RAG chatbot.\n", "link": "http://arxiv.org/abs/2408.15232v1", "date": "2024-08-27", "relevancy": 2.0926, "topK": [{"title": "RoboEXP: Action-Conditioned Scene Graph via Interactive Exploration for\n  Robotic Manipulation", "link": "http://arxiv.org/abs/2402.15487v1", "similarity": 0.5598}, {"title": "Genie: Generative Interactive Environments", "link": "http://arxiv.org/abs/2402.15391v1", "similarity": 0.5468}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.4848}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Into%20the%20Unknown%20Unknowns%3A%20Engaged%20Human%20Learning%20through%20Participation%0A%20%20in%20Language%20Model%20Agent%20Conversations&body=Title%3A%20Into%20the%20Unknown%20Unknowns%3A%20Engaged%20Human%20Learning%20through%20Participation%0A%20%20in%20Language%20Model%20Agent%20Conversations%0AAuthor%3A%20Yucheng%20Jiang%20and%20Yijia%20Shao%20and%20Dekun%20Ma%20and%20Sina%20J.%20Semnani%20and%20Monica%20S.%20Lam%0AAbstract%3A%20%20%20While%20language%20model%20%28LM%29-powered%20chatbots%20and%20generative%20search%20engines%0Aexcel%20at%20answering%20concrete%20queries%2C%20discovering%20information%20in%20the%20terrain%20of%0Aunknown%20unknowns%20remains%20challenging%20for%20users.%20To%20emulate%20the%20common%0Aeducational%20scenario%20where%20children/students%20learn%20by%20listening%20to%20and%0Aparticipating%20in%20conversations%20of%20their%20parents/teachers%2C%20we%20create%0ACollaborative%20STORM%20%28Co-STORM%29.%20Unlike%20QA%20systems%20that%20require%20users%20to%20ask%20all%0Athe%20questions%2C%20Co-STORM%20lets%20users%20observe%20and%20occasionally%20steer%20the%20discourse%0Aamong%20several%20LM%20agents.%20The%20agents%20ask%20questions%20on%20the%20user%27s%20behalf%2C%0Aallowing%20the%20user%20to%20discover%20unknown%20unknowns%20serendipitously.%20To%20facilitate%0Auser%20interaction%2C%20Co-STORM%20assists%20users%20in%20tracking%20the%20discourse%20by%0Aorganizing%20the%20uncovered%20information%20into%20a%20dynamic%20mind%20map%2C%20ultimately%0Agenerating%20a%20comprehensive%20report%20as%20takeaways.%20For%20automatic%20evaluation%2C%20we%0Aconstruct%20the%20WildSeek%20dataset%20by%20collecting%20real%20information-seeking%20records%0Awith%20user%20goals.%20Co-STORM%20outperforms%20baseline%20methods%20on%20both%20discourse%20trace%0Aand%20report%20quality.%20In%20a%20further%20human%20evaluation%2C%2070%25%20of%20participants%20prefer%0ACo-STORM%20over%20a%20search%20engine%2C%20and%2078%25%20favor%20it%20over%20a%20RAG%20chatbot.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2408.15232v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DInto%2520the%2520Unknown%2520Unknowns%253A%2520Engaged%2520Human%2520Learning%2520through%2520Participation%250A%2520%2520in%2520Language%2520Model%2520Agent%2520Conversations%26entry.906535625%3DYucheng%2520Jiang%2520and%2520Yijia%2520Shao%2520and%2520Dekun%2520Ma%2520and%2520Sina%2520J.%2520Semnani%2520and%2520Monica%2520S.%2520Lam%26entry.1292438233%3D%2520%2520While%2520language%2520model%2520%2528LM%2529-powered%2520chatbots%2520and%2520generative%2520search%2520engines%250Aexcel%2520at%2520answering%2520concrete%2520queries%252C%2520discovering%2520information%2520in%2520the%2520terrain%2520of%250Aunknown%2520unknowns%2520remains%2520challenging%2520for%2520users.%2520To%2520emulate%2520the%2520common%250Aeducational%2520scenario%2520where%2520children/students%2520learn%2520by%2520listening%2520to%2520and%250Aparticipating%2520in%2520conversations%2520of%2520their%2520parents/teachers%252C%2520we%2520create%250ACollaborative%2520STORM%2520%2528Co-STORM%2529.%2520Unlike%2520QA%2520systems%2520that%2520require%2520users%2520to%2520ask%2520all%250Athe%2520questions%252C%2520Co-STORM%2520lets%2520users%2520observe%2520and%2520occasionally%2520steer%2520the%2520discourse%250Aamong%2520several%2520LM%2520agents.%2520The%2520agents%2520ask%2520questions%2520on%2520the%2520user%2527s%2520behalf%252C%250Aallowing%2520the%2520user%2520to%2520discover%2520unknown%2520unknowns%2520serendipitously.%2520To%2520facilitate%250Auser%2520interaction%252C%2520Co-STORM%2520assists%2520users%2520in%2520tracking%2520the%2520discourse%2520by%250Aorganizing%2520the%2520uncovered%2520information%2520into%2520a%2520dynamic%2520mind%2520map%252C%2520ultimately%250Agenerating%2520a%2520comprehensive%2520report%2520as%2520takeaways.%2520For%2520automatic%2520evaluation%252C%2520we%250Aconstruct%2520the%2520WildSeek%2520dataset%2520by%2520collecting%2520real%2520information-seeking%2520records%250Awith%2520user%2520goals.%2520Co-STORM%2520outperforms%2520baseline%2520methods%2520on%2520both%2520discourse%2520trace%250Aand%2520report%2520quality.%2520In%2520a%2520further%2520human%2520evaluation%252C%252070%2525%2520of%2520participants%2520prefer%250ACo-STORM%2520over%2520a%2520search%2520engine%252C%2520and%252078%2525%2520favor%2520it%2520over%2520a%2520RAG%2520chatbot.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2408.15232v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Into%20the%20Unknown%20Unknowns%3A%20Engaged%20Human%20Learning%20through%20Participation%0A%20%20in%20Language%20Model%20Agent%20Conversations&entry.906535625=Yucheng%20Jiang%20and%20Yijia%20Shao%20and%20Dekun%20Ma%20and%20Sina%20J.%20Semnani%20and%20Monica%20S.%20Lam&entry.1292438233=%20%20While%20language%20model%20%28LM%29-powered%20chatbots%20and%20generative%20search%20engines%0Aexcel%20at%20answering%20concrete%20queries%2C%20discovering%20information%20in%20the%20terrain%20of%0Aunknown%20unknowns%20remains%20challenging%20for%20users.%20To%20emulate%20the%20common%0Aeducational%20scenario%20where%20children/students%20learn%20by%20listening%20to%20and%0Aparticipating%20in%20conversations%20of%20their%20parents/teachers%2C%20we%20create%0ACollaborative%20STORM%20%28Co-STORM%29.%20Unlike%20QA%20systems%20that%20require%20users%20to%20ask%20all%0Athe%20questions%2C%20Co-STORM%20lets%20users%20observe%20and%20occasionally%20steer%20the%20discourse%0Aamong%20several%20LM%20agents.%20The%20agents%20ask%20questions%20on%20the%20user%27s%20behalf%2C%0Aallowing%20the%20user%20to%20discover%20unknown%20unknowns%20serendipitously.%20To%20facilitate%0Auser%20interaction%2C%20Co-STORM%20assists%20users%20in%20tracking%20the%20discourse%20by%0Aorganizing%20the%20uncovered%20information%20into%20a%20dynamic%20mind%20map%2C%20ultimately%0Agenerating%20a%20comprehensive%20report%20as%20takeaways.%20For%20automatic%20evaluation%2C%20we%0Aconstruct%20the%20WildSeek%20dataset%20by%20collecting%20real%20information-seeking%20records%0Awith%20user%20goals.%20Co-STORM%20outperforms%20baseline%20methods%20on%20both%20discourse%20trace%0Aand%20report%20quality.%20In%20a%20further%20human%20evaluation%2C%2070%25%20of%20participants%20prefer%0ACo-STORM%20over%20a%20search%20engine%2C%20and%2078%25%20favor%20it%20over%20a%20RAG%20chatbot.%0A&entry.1838667208=http%3A//arxiv.org/abs/2408.15232v1&entry.124074799=Read"},
{"title": "RT-Attack: Jailbreaking Text-to-Image Models via Random Token", "author": "Sensen Gao and Xiaojun Jia and Yihao Huang and Ranjie Duan and Jindong Gu and Yang Liu and Qing Guo", "abstract": "  Recently, Text-to-Image(T2I) models have achieved remarkable success in image\ngeneration and editing, yet these models still have many potential issues,\nparticularly in generating inappropriate or Not-Safe-For-Work(NSFW) content.\nStrengthening attacks and uncovering such vulnerabilities can advance the\ndevelopment of reliable and practical T2I models. Most of the previous works\ntreat T2I models as white-box systems, using gradient optimization to generate\nadversarial prompts. However, accessing the model's gradient is often\nimpossible in real-world scenarios. Moreover, existing defense methods, those\nusing gradient masking, are designed to prevent attackers from obtaining\naccurate gradient information. While some black-box jailbreak attacks have been\nexplored, these typically rely on simply replacing sensitive words, leading to\nsuboptimal attack performance. To address this issue, we introduce a two-stage\nquery-based black-box attack method utilizing random search. In the first\nstage, we establish a preliminary prompt by maximizing the semantic similarity\nbetween the adversarial and target harmful prompts. In the second stage, we use\nthis initial prompt to refine our approach, creating a detailed adversarial\nprompt aimed at jailbreaking and maximizing the similarity in image features\nbetween the images generated from this prompt and those produced by the target\nharmful prompt. Extensive experiments validate the effectiveness of our method\nin attacking the latest prompt checkers, post-hoc image checkers, securely\ntrained T2I models, and online commercial models.\n", "link": "http://arxiv.org/abs/2408.13896v2", "date": "2024-08-27", "relevancy": 2.0849, "topK": [{"title": "ToDo: Token Downsampling for Efficient Generation of High-Resolution\n  Images", "link": "http://arxiv.org/abs/2402.13573v2", "similarity": 0.5307}, {"title": "Magic-Me: Identity-Specific Video Customized Diffusion", "link": "http://arxiv.org/abs/2402.09368v1", "similarity": 0.5145}, {"title": "Customize-A-Video: One-Shot Motion Customization of Text-to-Video\n  Diffusion Models", "link": "http://arxiv.org/abs/2402.14780v1", "similarity": 0.5143}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20RT-Attack%3A%20Jailbreaking%20Text-to-Image%20Models%20via%20Random%20Token&body=Title%3A%20RT-Attack%3A%20Jailbreaking%20Text-to-Image%20Models%20via%20Random%20Token%0AAuthor%3A%20Sensen%20Gao%20and%20Xiaojun%20Jia%20and%20Yihao%20Huang%20and%20Ranjie%20Duan%20and%20Jindong%20Gu%20and%20Yang%20Liu%20and%20Qing%20Guo%0AAbstract%3A%20%20%20Recently%2C%20Text-to-Image%28T2I%29%20models%20have%20achieved%20remarkable%20success%20in%20image%0Ageneration%20and%20editing%2C%20yet%20these%20models%20still%20have%20many%20potential%20issues%2C%0Aparticularly%20in%20generating%20inappropriate%20or%20Not-Safe-For-Work%28NSFW%29%20content.%0AStrengthening%20attacks%20and%20uncovering%20such%20vulnerabilities%20can%20advance%20the%0Adevelopment%20of%20reliable%20and%20practical%20T2I%20models.%20Most%20of%20the%20previous%20works%0Atreat%20T2I%20models%20as%20white-box%20systems%2C%20using%20gradient%20optimization%20to%20generate%0Aadversarial%20prompts.%20However%2C%20accessing%20the%20model%27s%20gradient%20is%20often%0Aimpossible%20in%20real-world%20scenarios.%20Moreover%2C%20existing%20defense%20methods%2C%20those%0Ausing%20gradient%20masking%2C%20are%20designed%20to%20prevent%20attackers%20from%20obtaining%0Aaccurate%20gradient%20information.%20While%20some%20black-box%20jailbreak%20attacks%20have%20been%0Aexplored%2C%20these%20typically%20rely%20on%20simply%20replacing%20sensitive%20words%2C%20leading%20to%0Asuboptimal%20attack%20performance.%20To%20address%20this%20issue%2C%20we%20introduce%20a%20two-stage%0Aquery-based%20black-box%20attack%20method%20utilizing%20random%20search.%20In%20the%20first%0Astage%2C%20we%20establish%20a%20preliminary%20prompt%20by%20maximizing%20the%20semantic%20similarity%0Abetween%20the%20adversarial%20and%20target%20harmful%20prompts.%20In%20the%20second%20stage%2C%20we%20use%0Athis%20initial%20prompt%20to%20refine%20our%20approach%2C%20creating%20a%20detailed%20adversarial%0Aprompt%20aimed%20at%20jailbreaking%20and%20maximizing%20the%20similarity%20in%20image%20features%0Abetween%20the%20images%20generated%20from%20this%20prompt%20and%20those%20produced%20by%20the%20target%0Aharmful%20prompt.%20Extensive%20experiments%20validate%20the%20effectiveness%20of%20our%20method%0Ain%20attacking%20the%20latest%20prompt%20checkers%2C%20post-hoc%20image%20checkers%2C%20securely%0Atrained%20T2I%20models%2C%20and%20online%20commercial%20models.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2408.13896v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DRT-Attack%253A%2520Jailbreaking%2520Text-to-Image%2520Models%2520via%2520Random%2520Token%26entry.906535625%3DSensen%2520Gao%2520and%2520Xiaojun%2520Jia%2520and%2520Yihao%2520Huang%2520and%2520Ranjie%2520Duan%2520and%2520Jindong%2520Gu%2520and%2520Yang%2520Liu%2520and%2520Qing%2520Guo%26entry.1292438233%3D%2520%2520Recently%252C%2520Text-to-Image%2528T2I%2529%2520models%2520have%2520achieved%2520remarkable%2520success%2520in%2520image%250Ageneration%2520and%2520editing%252C%2520yet%2520these%2520models%2520still%2520have%2520many%2520potential%2520issues%252C%250Aparticularly%2520in%2520generating%2520inappropriate%2520or%2520Not-Safe-For-Work%2528NSFW%2529%2520content.%250AStrengthening%2520attacks%2520and%2520uncovering%2520such%2520vulnerabilities%2520can%2520advance%2520the%250Adevelopment%2520of%2520reliable%2520and%2520practical%2520T2I%2520models.%2520Most%2520of%2520the%2520previous%2520works%250Atreat%2520T2I%2520models%2520as%2520white-box%2520systems%252C%2520using%2520gradient%2520optimization%2520to%2520generate%250Aadversarial%2520prompts.%2520However%252C%2520accessing%2520the%2520model%2527s%2520gradient%2520is%2520often%250Aimpossible%2520in%2520real-world%2520scenarios.%2520Moreover%252C%2520existing%2520defense%2520methods%252C%2520those%250Ausing%2520gradient%2520masking%252C%2520are%2520designed%2520to%2520prevent%2520attackers%2520from%2520obtaining%250Aaccurate%2520gradient%2520information.%2520While%2520some%2520black-box%2520jailbreak%2520attacks%2520have%2520been%250Aexplored%252C%2520these%2520typically%2520rely%2520on%2520simply%2520replacing%2520sensitive%2520words%252C%2520leading%2520to%250Asuboptimal%2520attack%2520performance.%2520To%2520address%2520this%2520issue%252C%2520we%2520introduce%2520a%2520two-stage%250Aquery-based%2520black-box%2520attack%2520method%2520utilizing%2520random%2520search.%2520In%2520the%2520first%250Astage%252C%2520we%2520establish%2520a%2520preliminary%2520prompt%2520by%2520maximizing%2520the%2520semantic%2520similarity%250Abetween%2520the%2520adversarial%2520and%2520target%2520harmful%2520prompts.%2520In%2520the%2520second%2520stage%252C%2520we%2520use%250Athis%2520initial%2520prompt%2520to%2520refine%2520our%2520approach%252C%2520creating%2520a%2520detailed%2520adversarial%250Aprompt%2520aimed%2520at%2520jailbreaking%2520and%2520maximizing%2520the%2520similarity%2520in%2520image%2520features%250Abetween%2520the%2520images%2520generated%2520from%2520this%2520prompt%2520and%2520those%2520produced%2520by%2520the%2520target%250Aharmful%2520prompt.%2520Extensive%2520experiments%2520validate%2520the%2520effectiveness%2520of%2520our%2520method%250Ain%2520attacking%2520the%2520latest%2520prompt%2520checkers%252C%2520post-hoc%2520image%2520checkers%252C%2520securely%250Atrained%2520T2I%2520models%252C%2520and%2520online%2520commercial%2520models.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2408.13896v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=RT-Attack%3A%20Jailbreaking%20Text-to-Image%20Models%20via%20Random%20Token&entry.906535625=Sensen%20Gao%20and%20Xiaojun%20Jia%20and%20Yihao%20Huang%20and%20Ranjie%20Duan%20and%20Jindong%20Gu%20and%20Yang%20Liu%20and%20Qing%20Guo&entry.1292438233=%20%20Recently%2C%20Text-to-Image%28T2I%29%20models%20have%20achieved%20remarkable%20success%20in%20image%0Ageneration%20and%20editing%2C%20yet%20these%20models%20still%20have%20many%20potential%20issues%2C%0Aparticularly%20in%20generating%20inappropriate%20or%20Not-Safe-For-Work%28NSFW%29%20content.%0AStrengthening%20attacks%20and%20uncovering%20such%20vulnerabilities%20can%20advance%20the%0Adevelopment%20of%20reliable%20and%20practical%20T2I%20models.%20Most%20of%20the%20previous%20works%0Atreat%20T2I%20models%20as%20white-box%20systems%2C%20using%20gradient%20optimization%20to%20generate%0Aadversarial%20prompts.%20However%2C%20accessing%20the%20model%27s%20gradient%20is%20often%0Aimpossible%20in%20real-world%20scenarios.%20Moreover%2C%20existing%20defense%20methods%2C%20those%0Ausing%20gradient%20masking%2C%20are%20designed%20to%20prevent%20attackers%20from%20obtaining%0Aaccurate%20gradient%20information.%20While%20some%20black-box%20jailbreak%20attacks%20have%20been%0Aexplored%2C%20these%20typically%20rely%20on%20simply%20replacing%20sensitive%20words%2C%20leading%20to%0Asuboptimal%20attack%20performance.%20To%20address%20this%20issue%2C%20we%20introduce%20a%20two-stage%0Aquery-based%20black-box%20attack%20method%20utilizing%20random%20search.%20In%20the%20first%0Astage%2C%20we%20establish%20a%20preliminary%20prompt%20by%20maximizing%20the%20semantic%20similarity%0Abetween%20the%20adversarial%20and%20target%20harmful%20prompts.%20In%20the%20second%20stage%2C%20we%20use%0Athis%20initial%20prompt%20to%20refine%20our%20approach%2C%20creating%20a%20detailed%20adversarial%0Aprompt%20aimed%20at%20jailbreaking%20and%20maximizing%20the%20similarity%20in%20image%20features%0Abetween%20the%20images%20generated%20from%20this%20prompt%20and%20those%20produced%20by%20the%20target%0Aharmful%20prompt.%20Extensive%20experiments%20validate%20the%20effectiveness%20of%20our%20method%0Ain%20attacking%20the%20latest%20prompt%20checkers%2C%20post-hoc%20image%20checkers%2C%20securely%0Atrained%20T2I%20models%2C%20and%20online%20commercial%20models.%0A&entry.1838667208=http%3A//arxiv.org/abs/2408.13896v2&entry.124074799=Read"},
{"title": "A Review of Transformer-Based Models for Computer Vision Tasks:\n  Capturing Global Context and Spatial Relationships", "author": "Gracile Astlin Pereira and Muhammad Hussain", "abstract": "  Transformer-based models have transformed the landscape of natural language\nprocessing (NLP) and are increasingly applied to computer vision tasks with\nremarkable success. These models, renowned for their ability to capture\nlong-range dependencies and contextual information, offer a promising\nalternative to traditional convolutional neural networks (CNNs) in computer\nvision. In this review paper, we provide an extensive overview of various\ntransformer architectures adapted for computer vision tasks. We delve into how\nthese models capture global context and spatial relationships in images,\nempowering them to excel in tasks such as image classification, object\ndetection, and segmentation. Analyzing the key components, training\nmethodologies, and performance metrics of transformer-based models, we\nhighlight their strengths, limitations, and recent advancements. Additionally,\nwe discuss potential research directions and applications of transformer-based\nmodels in computer vision, offering insights into their implications for future\nadvancements in the field.\n", "link": "http://arxiv.org/abs/2408.15178v1", "date": "2024-08-27", "relevancy": 2.0742, "topK": [{"title": "Scaling Rectified Flow Transformers for High-Resolution Image Synthesis", "link": "http://arxiv.org/abs/2403.03206v1", "similarity": 0.5651}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5264}, {"title": "Self-supervised Photographic Image Layout Representation Learning", "link": "http://arxiv.org/abs/2403.03740v1", "similarity": 0.4921}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20A%20Review%20of%20Transformer-Based%20Models%20for%20Computer%20Vision%20Tasks%3A%0A%20%20Capturing%20Global%20Context%20and%20Spatial%20Relationships&body=Title%3A%20A%20Review%20of%20Transformer-Based%20Models%20for%20Computer%20Vision%20Tasks%3A%0A%20%20Capturing%20Global%20Context%20and%20Spatial%20Relationships%0AAuthor%3A%20Gracile%20Astlin%20Pereira%20and%20Muhammad%20Hussain%0AAbstract%3A%20%20%20Transformer-based%20models%20have%20transformed%20the%20landscape%20of%20natural%20language%0Aprocessing%20%28NLP%29%20and%20are%20increasingly%20applied%20to%20computer%20vision%20tasks%20with%0Aremarkable%20success.%20These%20models%2C%20renowned%20for%20their%20ability%20to%20capture%0Along-range%20dependencies%20and%20contextual%20information%2C%20offer%20a%20promising%0Aalternative%20to%20traditional%20convolutional%20neural%20networks%20%28CNNs%29%20in%20computer%0Avision.%20In%20this%20review%20paper%2C%20we%20provide%20an%20extensive%20overview%20of%20various%0Atransformer%20architectures%20adapted%20for%20computer%20vision%20tasks.%20We%20delve%20into%20how%0Athese%20models%20capture%20global%20context%20and%20spatial%20relationships%20in%20images%2C%0Aempowering%20them%20to%20excel%20in%20tasks%20such%20as%20image%20classification%2C%20object%0Adetection%2C%20and%20segmentation.%20Analyzing%20the%20key%20components%2C%20training%0Amethodologies%2C%20and%20performance%20metrics%20of%20transformer-based%20models%2C%20we%0Ahighlight%20their%20strengths%2C%20limitations%2C%20and%20recent%20advancements.%20Additionally%2C%0Awe%20discuss%20potential%20research%20directions%20and%20applications%20of%20transformer-based%0Amodels%20in%20computer%20vision%2C%20offering%20insights%20into%20their%20implications%20for%20future%0Aadvancements%20in%20the%20field.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2408.15178v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DA%2520Review%2520of%2520Transformer-Based%2520Models%2520for%2520Computer%2520Vision%2520Tasks%253A%250A%2520%2520Capturing%2520Global%2520Context%2520and%2520Spatial%2520Relationships%26entry.906535625%3DGracile%2520Astlin%2520Pereira%2520and%2520Muhammad%2520Hussain%26entry.1292438233%3D%2520%2520Transformer-based%2520models%2520have%2520transformed%2520the%2520landscape%2520of%2520natural%2520language%250Aprocessing%2520%2528NLP%2529%2520and%2520are%2520increasingly%2520applied%2520to%2520computer%2520vision%2520tasks%2520with%250Aremarkable%2520success.%2520These%2520models%252C%2520renowned%2520for%2520their%2520ability%2520to%2520capture%250Along-range%2520dependencies%2520and%2520contextual%2520information%252C%2520offer%2520a%2520promising%250Aalternative%2520to%2520traditional%2520convolutional%2520neural%2520networks%2520%2528CNNs%2529%2520in%2520computer%250Avision.%2520In%2520this%2520review%2520paper%252C%2520we%2520provide%2520an%2520extensive%2520overview%2520of%2520various%250Atransformer%2520architectures%2520adapted%2520for%2520computer%2520vision%2520tasks.%2520We%2520delve%2520into%2520how%250Athese%2520models%2520capture%2520global%2520context%2520and%2520spatial%2520relationships%2520in%2520images%252C%250Aempowering%2520them%2520to%2520excel%2520in%2520tasks%2520such%2520as%2520image%2520classification%252C%2520object%250Adetection%252C%2520and%2520segmentation.%2520Analyzing%2520the%2520key%2520components%252C%2520training%250Amethodologies%252C%2520and%2520performance%2520metrics%2520of%2520transformer-based%2520models%252C%2520we%250Ahighlight%2520their%2520strengths%252C%2520limitations%252C%2520and%2520recent%2520advancements.%2520Additionally%252C%250Awe%2520discuss%2520potential%2520research%2520directions%2520and%2520applications%2520of%2520transformer-based%250Amodels%2520in%2520computer%2520vision%252C%2520offering%2520insights%2520into%2520their%2520implications%2520for%2520future%250Aadvancements%2520in%2520the%2520field.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2408.15178v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=A%20Review%20of%20Transformer-Based%20Models%20for%20Computer%20Vision%20Tasks%3A%0A%20%20Capturing%20Global%20Context%20and%20Spatial%20Relationships&entry.906535625=Gracile%20Astlin%20Pereira%20and%20Muhammad%20Hussain&entry.1292438233=%20%20Transformer-based%20models%20have%20transformed%20the%20landscape%20of%20natural%20language%0Aprocessing%20%28NLP%29%20and%20are%20increasingly%20applied%20to%20computer%20vision%20tasks%20with%0Aremarkable%20success.%20These%20models%2C%20renowned%20for%20their%20ability%20to%20capture%0Along-range%20dependencies%20and%20contextual%20information%2C%20offer%20a%20promising%0Aalternative%20to%20traditional%20convolutional%20neural%20networks%20%28CNNs%29%20in%20computer%0Avision.%20In%20this%20review%20paper%2C%20we%20provide%20an%20extensive%20overview%20of%20various%0Atransformer%20architectures%20adapted%20for%20computer%20vision%20tasks.%20We%20delve%20into%20how%0Athese%20models%20capture%20global%20context%20and%20spatial%20relationships%20in%20images%2C%0Aempowering%20them%20to%20excel%20in%20tasks%20such%20as%20image%20classification%2C%20object%0Adetection%2C%20and%20segmentation.%20Analyzing%20the%20key%20components%2C%20training%0Amethodologies%2C%20and%20performance%20metrics%20of%20transformer-based%20models%2C%20we%0Ahighlight%20their%20strengths%2C%20limitations%2C%20and%20recent%20advancements.%20Additionally%2C%0Awe%20discuss%20potential%20research%20directions%20and%20applications%20of%20transformer-based%0Amodels%20in%20computer%20vision%2C%20offering%20insights%20into%20their%20implications%20for%20future%0Aadvancements%20in%20the%20field.%0A&entry.1838667208=http%3A//arxiv.org/abs/2408.15178v1&entry.124074799=Read"},
{"title": "STD-PLM: Understanding Both Spatial and Temporal Properties of\n  Spatial-Temporal Data with PLM", "author": "YiHeng Huang and Xiaowei Mao and Shengnan Guo and Yubin Chen and Junfeng Shen and Tiankuo Li and Youfang Lin and Huaiyu Wan", "abstract": "  Spatial-temporal forecasting and imputation are important for real-world\nintelligent systems. Most existing methods are tailored for individual\nforecasting or imputation tasks but are not designed for both. Additionally,\nthey are less effective for zero-shot and few-shot learning. While pre-trained\nlanguage model (PLM) have exhibited strong pattern recognition and reasoning\nabilities across various tasks, including few-shot and zero-shot learning,\ntheir applications in spatial-temporal data understanding has been constrained\nby insufficient modeling of complex correlations such as the temporal\ncorrelations, spatial connectivity, non-pairwise and high-order\nspatial-temporal correlations within data. In this paper, we propose STD-PLM\nfor understanding both spatial and temporal properties of\n\\underline{S}patial-\\underline{T}emporal \\underline{D}ata with \\underline{PLM},\nwhich is capable of implementing both spatial-temporal forecasting and\nimputation tasks. STD-PLM understands spatial-temporal correlations via\nexplicitly designed spatial and temporal tokenizers. Topology-aware node\nembeddings are designed for PLM to comprehend and exploit the topology\nstructure of data in inductive manner. Furthermore, to mitigate the efficiency\nissues introduced by the PLM, we design a sandglass attention module (SGA)\ncombined with a specific constrained loss function, which significantly\nimproves the model's efficiency while ensuring performance. Extensive\nexperiments demonstrate that STD-PLM exhibits competitive performance and\ngeneralization capabilities across the forecasting and imputation tasks on\nvarious datasets. Moreover, STD-PLM achieves promising results on both few-shot\nand zero-shot tasks.\n", "link": "http://arxiv.org/abs/2407.09096v2", "date": "2024-08-27", "relevancy": 2.0656, "topK": [{"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5347}, {"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.5095}, {"title": "SDXS: Real-Time One-Step Latent Diffusion Models with Image Conditions", "link": "http://arxiv.org/abs/2403.16627v1", "similarity": 0.488}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20STD-PLM%3A%20Understanding%20Both%20Spatial%20and%20Temporal%20Properties%20of%0A%20%20Spatial-Temporal%20Data%20with%20PLM&body=Title%3A%20STD-PLM%3A%20Understanding%20Both%20Spatial%20and%20Temporal%20Properties%20of%0A%20%20Spatial-Temporal%20Data%20with%20PLM%0AAuthor%3A%20YiHeng%20Huang%20and%20Xiaowei%20Mao%20and%20Shengnan%20Guo%20and%20Yubin%20Chen%20and%20Junfeng%20Shen%20and%20Tiankuo%20Li%20and%20Youfang%20Lin%20and%20Huaiyu%20Wan%0AAbstract%3A%20%20%20Spatial-temporal%20forecasting%20and%20imputation%20are%20important%20for%20real-world%0Aintelligent%20systems.%20Most%20existing%20methods%20are%20tailored%20for%20individual%0Aforecasting%20or%20imputation%20tasks%20but%20are%20not%20designed%20for%20both.%20Additionally%2C%0Athey%20are%20less%20effective%20for%20zero-shot%20and%20few-shot%20learning.%20While%20pre-trained%0Alanguage%20model%20%28PLM%29%20have%20exhibited%20strong%20pattern%20recognition%20and%20reasoning%0Aabilities%20across%20various%20tasks%2C%20including%20few-shot%20and%20zero-shot%20learning%2C%0Atheir%20applications%20in%20spatial-temporal%20data%20understanding%20has%20been%20constrained%0Aby%20insufficient%20modeling%20of%20complex%20correlations%20such%20as%20the%20temporal%0Acorrelations%2C%20spatial%20connectivity%2C%20non-pairwise%20and%20high-order%0Aspatial-temporal%20correlations%20within%20data.%20In%20this%20paper%2C%20we%20propose%20STD-PLM%0Afor%20understanding%20both%20spatial%20and%20temporal%20properties%20of%0A%5Cunderline%7BS%7Dpatial-%5Cunderline%7BT%7Demporal%20%5Cunderline%7BD%7Data%20with%20%5Cunderline%7BPLM%7D%2C%0Awhich%20is%20capable%20of%20implementing%20both%20spatial-temporal%20forecasting%20and%0Aimputation%20tasks.%20STD-PLM%20understands%20spatial-temporal%20correlations%20via%0Aexplicitly%20designed%20spatial%20and%20temporal%20tokenizers.%20Topology-aware%20node%0Aembeddings%20are%20designed%20for%20PLM%20to%20comprehend%20and%20exploit%20the%20topology%0Astructure%20of%20data%20in%20inductive%20manner.%20Furthermore%2C%20to%20mitigate%20the%20efficiency%0Aissues%20introduced%20by%20the%20PLM%2C%20we%20design%20a%20sandglass%20attention%20module%20%28SGA%29%0Acombined%20with%20a%20specific%20constrained%20loss%20function%2C%20which%20significantly%0Aimproves%20the%20model%27s%20efficiency%20while%20ensuring%20performance.%20Extensive%0Aexperiments%20demonstrate%20that%20STD-PLM%20exhibits%20competitive%20performance%20and%0Ageneralization%20capabilities%20across%20the%20forecasting%20and%20imputation%20tasks%20on%0Avarious%20datasets.%20Moreover%2C%20STD-PLM%20achieves%20promising%20results%20on%20both%20few-shot%0Aand%20zero-shot%20tasks.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2407.09096v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DSTD-PLM%253A%2520Understanding%2520Both%2520Spatial%2520and%2520Temporal%2520Properties%2520of%250A%2520%2520Spatial-Temporal%2520Data%2520with%2520PLM%26entry.906535625%3DYiHeng%2520Huang%2520and%2520Xiaowei%2520Mao%2520and%2520Shengnan%2520Guo%2520and%2520Yubin%2520Chen%2520and%2520Junfeng%2520Shen%2520and%2520Tiankuo%2520Li%2520and%2520Youfang%2520Lin%2520and%2520Huaiyu%2520Wan%26entry.1292438233%3D%2520%2520Spatial-temporal%2520forecasting%2520and%2520imputation%2520are%2520important%2520for%2520real-world%250Aintelligent%2520systems.%2520Most%2520existing%2520methods%2520are%2520tailored%2520for%2520individual%250Aforecasting%2520or%2520imputation%2520tasks%2520but%2520are%2520not%2520designed%2520for%2520both.%2520Additionally%252C%250Athey%2520are%2520less%2520effective%2520for%2520zero-shot%2520and%2520few-shot%2520learning.%2520While%2520pre-trained%250Alanguage%2520model%2520%2528PLM%2529%2520have%2520exhibited%2520strong%2520pattern%2520recognition%2520and%2520reasoning%250Aabilities%2520across%2520various%2520tasks%252C%2520including%2520few-shot%2520and%2520zero-shot%2520learning%252C%250Atheir%2520applications%2520in%2520spatial-temporal%2520data%2520understanding%2520has%2520been%2520constrained%250Aby%2520insufficient%2520modeling%2520of%2520complex%2520correlations%2520such%2520as%2520the%2520temporal%250Acorrelations%252C%2520spatial%2520connectivity%252C%2520non-pairwise%2520and%2520high-order%250Aspatial-temporal%2520correlations%2520within%2520data.%2520In%2520this%2520paper%252C%2520we%2520propose%2520STD-PLM%250Afor%2520understanding%2520both%2520spatial%2520and%2520temporal%2520properties%2520of%250A%255Cunderline%257BS%257Dpatial-%255Cunderline%257BT%257Demporal%2520%255Cunderline%257BD%257Data%2520with%2520%255Cunderline%257BPLM%257D%252C%250Awhich%2520is%2520capable%2520of%2520implementing%2520both%2520spatial-temporal%2520forecasting%2520and%250Aimputation%2520tasks.%2520STD-PLM%2520understands%2520spatial-temporal%2520correlations%2520via%250Aexplicitly%2520designed%2520spatial%2520and%2520temporal%2520tokenizers.%2520Topology-aware%2520node%250Aembeddings%2520are%2520designed%2520for%2520PLM%2520to%2520comprehend%2520and%2520exploit%2520the%2520topology%250Astructure%2520of%2520data%2520in%2520inductive%2520manner.%2520Furthermore%252C%2520to%2520mitigate%2520the%2520efficiency%250Aissues%2520introduced%2520by%2520the%2520PLM%252C%2520we%2520design%2520a%2520sandglass%2520attention%2520module%2520%2528SGA%2529%250Acombined%2520with%2520a%2520specific%2520constrained%2520loss%2520function%252C%2520which%2520significantly%250Aimproves%2520the%2520model%2527s%2520efficiency%2520while%2520ensuring%2520performance.%2520Extensive%250Aexperiments%2520demonstrate%2520that%2520STD-PLM%2520exhibits%2520competitive%2520performance%2520and%250Ageneralization%2520capabilities%2520across%2520the%2520forecasting%2520and%2520imputation%2520tasks%2520on%250Avarious%2520datasets.%2520Moreover%252C%2520STD-PLM%2520achieves%2520promising%2520results%2520on%2520both%2520few-shot%250Aand%2520zero-shot%2520tasks.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2407.09096v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=STD-PLM%3A%20Understanding%20Both%20Spatial%20and%20Temporal%20Properties%20of%0A%20%20Spatial-Temporal%20Data%20with%20PLM&entry.906535625=YiHeng%20Huang%20and%20Xiaowei%20Mao%20and%20Shengnan%20Guo%20and%20Yubin%20Chen%20and%20Junfeng%20Shen%20and%20Tiankuo%20Li%20and%20Youfang%20Lin%20and%20Huaiyu%20Wan&entry.1292438233=%20%20Spatial-temporal%20forecasting%20and%20imputation%20are%20important%20for%20real-world%0Aintelligent%20systems.%20Most%20existing%20methods%20are%20tailored%20for%20individual%0Aforecasting%20or%20imputation%20tasks%20but%20are%20not%20designed%20for%20both.%20Additionally%2C%0Athey%20are%20less%20effective%20for%20zero-shot%20and%20few-shot%20learning.%20While%20pre-trained%0Alanguage%20model%20%28PLM%29%20have%20exhibited%20strong%20pattern%20recognition%20and%20reasoning%0Aabilities%20across%20various%20tasks%2C%20including%20few-shot%20and%20zero-shot%20learning%2C%0Atheir%20applications%20in%20spatial-temporal%20data%20understanding%20has%20been%20constrained%0Aby%20insufficient%20modeling%20of%20complex%20correlations%20such%20as%20the%20temporal%0Acorrelations%2C%20spatial%20connectivity%2C%20non-pairwise%20and%20high-order%0Aspatial-temporal%20correlations%20within%20data.%20In%20this%20paper%2C%20we%20propose%20STD-PLM%0Afor%20understanding%20both%20spatial%20and%20temporal%20properties%20of%0A%5Cunderline%7BS%7Dpatial-%5Cunderline%7BT%7Demporal%20%5Cunderline%7BD%7Data%20with%20%5Cunderline%7BPLM%7D%2C%0Awhich%20is%20capable%20of%20implementing%20both%20spatial-temporal%20forecasting%20and%0Aimputation%20tasks.%20STD-PLM%20understands%20spatial-temporal%20correlations%20via%0Aexplicitly%20designed%20spatial%20and%20temporal%20tokenizers.%20Topology-aware%20node%0Aembeddings%20are%20designed%20for%20PLM%20to%20comprehend%20and%20exploit%20the%20topology%0Astructure%20of%20data%20in%20inductive%20manner.%20Furthermore%2C%20to%20mitigate%20the%20efficiency%0Aissues%20introduced%20by%20the%20PLM%2C%20we%20design%20a%20sandglass%20attention%20module%20%28SGA%29%0Acombined%20with%20a%20specific%20constrained%20loss%20function%2C%20which%20significantly%0Aimproves%20the%20model%27s%20efficiency%20while%20ensuring%20performance.%20Extensive%0Aexperiments%20demonstrate%20that%20STD-PLM%20exhibits%20competitive%20performance%20and%0Ageneralization%20capabilities%20across%20the%20forecasting%20and%20imputation%20tasks%20on%0Avarious%20datasets.%20Moreover%2C%20STD-PLM%20achieves%20promising%20results%20on%20both%20few-shot%0Aand%20zero-shot%20tasks.%0A&entry.1838667208=http%3A//arxiv.org/abs/2407.09096v2&entry.124074799=Read"},
{"title": "Generative Verifiers: Reward Modeling as Next-Token Prediction", "author": "Lunjun Zhang and Arian Hosseini and Hritik Bansal and Mehran Kazemi and Aviral Kumar and Rishabh Agarwal", "abstract": "  Verifiers or reward models are often used to enhance the reasoning\nperformance of large language models (LLMs). A common approach is the Best-of-N\nmethod, where N candidate solutions generated by the LLM are ranked by a\nverifier, and the best one is selected. While LLM-based verifiers are typically\ntrained as discriminative classifiers to score solutions, they do not utilize\nthe text generation capabilities of pretrained LLMs. To overcome this\nlimitation, we instead propose training verifiers using the ubiquitous\nnext-token prediction objective, jointly on verification and solution\ngeneration. Compared to standard verifiers, such generative verifiers (GenRM)\ncan benefit from several advantages of LLMs: they integrate seamlessly with\ninstruction tuning, enable chain-of-thought reasoning, and can utilize\nadditional inference-time compute via majority voting for better verification.\nWe demonstrate that when using Gemma-based verifiers on algorithmic and\ngrade-school math reasoning tasks, GenRM outperforms discriminative verifiers\nand LLM-as-a-Judge, showing a 16-64% improvement in the percentage of problems\nsolved with Best-of-N. Furthermore, we show that GenRM scales favorably across\ndataset size, model capacity, and inference-time compute.\n", "link": "http://arxiv.org/abs/2408.15240v1", "date": "2024-08-27", "relevancy": 2.0461, "topK": [{"title": "Genie: Generative Interactive Environments", "link": "http://arxiv.org/abs/2402.15391v1", "similarity": 0.5513}, {"title": "ToDo: Token Downsampling for Efficient Generation of High-Resolution\n  Images", "link": "http://arxiv.org/abs/2402.13573v2", "similarity": 0.4894}, {"title": "Scaling Rectified Flow Transformers for High-Resolution Image Synthesis", "link": "http://arxiv.org/abs/2403.03206v1", "similarity": 0.4674}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Generative%20Verifiers%3A%20Reward%20Modeling%20as%20Next-Token%20Prediction&body=Title%3A%20Generative%20Verifiers%3A%20Reward%20Modeling%20as%20Next-Token%20Prediction%0AAuthor%3A%20Lunjun%20Zhang%20and%20Arian%20Hosseini%20and%20Hritik%20Bansal%20and%20Mehran%20Kazemi%20and%20Aviral%20Kumar%20and%20Rishabh%20Agarwal%0AAbstract%3A%20%20%20Verifiers%20or%20reward%20models%20are%20often%20used%20to%20enhance%20the%20reasoning%0Aperformance%20of%20large%20language%20models%20%28LLMs%29.%20A%20common%20approach%20is%20the%20Best-of-N%0Amethod%2C%20where%20N%20candidate%20solutions%20generated%20by%20the%20LLM%20are%20ranked%20by%20a%0Averifier%2C%20and%20the%20best%20one%20is%20selected.%20While%20LLM-based%20verifiers%20are%20typically%0Atrained%20as%20discriminative%20classifiers%20to%20score%20solutions%2C%20they%20do%20not%20utilize%0Athe%20text%20generation%20capabilities%20of%20pretrained%20LLMs.%20To%20overcome%20this%0Alimitation%2C%20we%20instead%20propose%20training%20verifiers%20using%20the%20ubiquitous%0Anext-token%20prediction%20objective%2C%20jointly%20on%20verification%20and%20solution%0Ageneration.%20Compared%20to%20standard%20verifiers%2C%20such%20generative%20verifiers%20%28GenRM%29%0Acan%20benefit%20from%20several%20advantages%20of%20LLMs%3A%20they%20integrate%20seamlessly%20with%0Ainstruction%20tuning%2C%20enable%20chain-of-thought%20reasoning%2C%20and%20can%20utilize%0Aadditional%20inference-time%20compute%20via%20majority%20voting%20for%20better%20verification.%0AWe%20demonstrate%20that%20when%20using%20Gemma-based%20verifiers%20on%20algorithmic%20and%0Agrade-school%20math%20reasoning%20tasks%2C%20GenRM%20outperforms%20discriminative%20verifiers%0Aand%20LLM-as-a-Judge%2C%20showing%20a%2016-64%25%20improvement%20in%20the%20percentage%20of%20problems%0Asolved%20with%20Best-of-N.%20Furthermore%2C%20we%20show%20that%20GenRM%20scales%20favorably%20across%0Adataset%20size%2C%20model%20capacity%2C%20and%20inference-time%20compute.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2408.15240v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DGenerative%2520Verifiers%253A%2520Reward%2520Modeling%2520as%2520Next-Token%2520Prediction%26entry.906535625%3DLunjun%2520Zhang%2520and%2520Arian%2520Hosseini%2520and%2520Hritik%2520Bansal%2520and%2520Mehran%2520Kazemi%2520and%2520Aviral%2520Kumar%2520and%2520Rishabh%2520Agarwal%26entry.1292438233%3D%2520%2520Verifiers%2520or%2520reward%2520models%2520are%2520often%2520used%2520to%2520enhance%2520the%2520reasoning%250Aperformance%2520of%2520large%2520language%2520models%2520%2528LLMs%2529.%2520A%2520common%2520approach%2520is%2520the%2520Best-of-N%250Amethod%252C%2520where%2520N%2520candidate%2520solutions%2520generated%2520by%2520the%2520LLM%2520are%2520ranked%2520by%2520a%250Averifier%252C%2520and%2520the%2520best%2520one%2520is%2520selected.%2520While%2520LLM-based%2520verifiers%2520are%2520typically%250Atrained%2520as%2520discriminative%2520classifiers%2520to%2520score%2520solutions%252C%2520they%2520do%2520not%2520utilize%250Athe%2520text%2520generation%2520capabilities%2520of%2520pretrained%2520LLMs.%2520To%2520overcome%2520this%250Alimitation%252C%2520we%2520instead%2520propose%2520training%2520verifiers%2520using%2520the%2520ubiquitous%250Anext-token%2520prediction%2520objective%252C%2520jointly%2520on%2520verification%2520and%2520solution%250Ageneration.%2520Compared%2520to%2520standard%2520verifiers%252C%2520such%2520generative%2520verifiers%2520%2528GenRM%2529%250Acan%2520benefit%2520from%2520several%2520advantages%2520of%2520LLMs%253A%2520they%2520integrate%2520seamlessly%2520with%250Ainstruction%2520tuning%252C%2520enable%2520chain-of-thought%2520reasoning%252C%2520and%2520can%2520utilize%250Aadditional%2520inference-time%2520compute%2520via%2520majority%2520voting%2520for%2520better%2520verification.%250AWe%2520demonstrate%2520that%2520when%2520using%2520Gemma-based%2520verifiers%2520on%2520algorithmic%2520and%250Agrade-school%2520math%2520reasoning%2520tasks%252C%2520GenRM%2520outperforms%2520discriminative%2520verifiers%250Aand%2520LLM-as-a-Judge%252C%2520showing%2520a%252016-64%2525%2520improvement%2520in%2520the%2520percentage%2520of%2520problems%250Asolved%2520with%2520Best-of-N.%2520Furthermore%252C%2520we%2520show%2520that%2520GenRM%2520scales%2520favorably%2520across%250Adataset%2520size%252C%2520model%2520capacity%252C%2520and%2520inference-time%2520compute.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2408.15240v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Generative%20Verifiers%3A%20Reward%20Modeling%20as%20Next-Token%20Prediction&entry.906535625=Lunjun%20Zhang%20and%20Arian%20Hosseini%20and%20Hritik%20Bansal%20and%20Mehran%20Kazemi%20and%20Aviral%20Kumar%20and%20Rishabh%20Agarwal&entry.1292438233=%20%20Verifiers%20or%20reward%20models%20are%20often%20used%20to%20enhance%20the%20reasoning%0Aperformance%20of%20large%20language%20models%20%28LLMs%29.%20A%20common%20approach%20is%20the%20Best-of-N%0Amethod%2C%20where%20N%20candidate%20solutions%20generated%20by%20the%20LLM%20are%20ranked%20by%20a%0Averifier%2C%20and%20the%20best%20one%20is%20selected.%20While%20LLM-based%20verifiers%20are%20typically%0Atrained%20as%20discriminative%20classifiers%20to%20score%20solutions%2C%20they%20do%20not%20utilize%0Athe%20text%20generation%20capabilities%20of%20pretrained%20LLMs.%20To%20overcome%20this%0Alimitation%2C%20we%20instead%20propose%20training%20verifiers%20using%20the%20ubiquitous%0Anext-token%20prediction%20objective%2C%20jointly%20on%20verification%20and%20solution%0Ageneration.%20Compared%20to%20standard%20verifiers%2C%20such%20generative%20verifiers%20%28GenRM%29%0Acan%20benefit%20from%20several%20advantages%20of%20LLMs%3A%20they%20integrate%20seamlessly%20with%0Ainstruction%20tuning%2C%20enable%20chain-of-thought%20reasoning%2C%20and%20can%20utilize%0Aadditional%20inference-time%20compute%20via%20majority%20voting%20for%20better%20verification.%0AWe%20demonstrate%20that%20when%20using%20Gemma-based%20verifiers%20on%20algorithmic%20and%0Agrade-school%20math%20reasoning%20tasks%2C%20GenRM%20outperforms%20discriminative%20verifiers%0Aand%20LLM-as-a-Judge%2C%20showing%20a%2016-64%25%20improvement%20in%20the%20percentage%20of%20problems%0Asolved%20with%20Best-of-N.%20Furthermore%2C%20we%20show%20that%20GenRM%20scales%20favorably%20across%0Adataset%20size%2C%20model%20capacity%2C%20and%20inference-time%20compute.%0A&entry.1838667208=http%3A//arxiv.org/abs/2408.15240v1&entry.124074799=Read"},
{"title": "FastTextSpotter: A High-Efficiency Transformer for Multilingual Scene\n  Text Spotting", "author": "Alloy Das and Sanket Biswas and Umapada Pal and Josep Llad\u00f3s and Saumik Bhattacharya", "abstract": "  The proliferation of scene text in both structured and unstructured\nenvironments presents significant challenges in optical character recognition\n(OCR), necessitating more efficient and robust text spotting solutions. This\npaper presents FastTextSpotter, a framework that integrates a Swin Transformer\nvisual backbone with a Transformer Encoder-Decoder architecture, enhanced by a\nnovel, faster self-attention unit, SAC2, to improve processing speeds while\nmaintaining accuracy. FastTextSpotter has been validated across multiple\ndatasets, including ICDAR2015 for regular texts and CTW1500 and TotalText for\narbitrary-shaped texts, benchmarking against current state-of-the-art models.\nOur results indicate that FastTextSpotter not only achieves superior accuracy\nin detecting and recognizing multilingual scene text (English and Vietnamese)\nbut also improves model efficiency, thereby setting new benchmarks in the\nfield. This study underscores the potential of advanced transformer\narchitectures in improving the adaptability and speed of text spotting\napplications in diverse real-world settings. The dataset, code, and pre-trained\nmodels have been released in our Github.\n", "link": "http://arxiv.org/abs/2408.14998v1", "date": "2024-08-27", "relevancy": 2.0425, "topK": [{"title": "Scaling Rectified Flow Transformers for High-Resolution Image Synthesis", "link": "http://arxiv.org/abs/2403.03206v1", "similarity": 0.5416}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5154}, {"title": "Weakly-guided self-supervised pretraining for temporal activity detection", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/25189", "similarity": 0.4935}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20FastTextSpotter%3A%20A%20High-Efficiency%20Transformer%20for%20Multilingual%20Scene%0A%20%20Text%20Spotting&body=Title%3A%20FastTextSpotter%3A%20A%20High-Efficiency%20Transformer%20for%20Multilingual%20Scene%0A%20%20Text%20Spotting%0AAuthor%3A%20Alloy%20Das%20and%20Sanket%20Biswas%20and%20Umapada%20Pal%20and%20Josep%20Llad%C3%B3s%20and%20Saumik%20Bhattacharya%0AAbstract%3A%20%20%20The%20proliferation%20of%20scene%20text%20in%20both%20structured%20and%20unstructured%0Aenvironments%20presents%20significant%20challenges%20in%20optical%20character%20recognition%0A%28OCR%29%2C%20necessitating%20more%20efficient%20and%20robust%20text%20spotting%20solutions.%20This%0Apaper%20presents%20FastTextSpotter%2C%20a%20framework%20that%20integrates%20a%20Swin%20Transformer%0Avisual%20backbone%20with%20a%20Transformer%20Encoder-Decoder%20architecture%2C%20enhanced%20by%20a%0Anovel%2C%20faster%20self-attention%20unit%2C%20SAC2%2C%20to%20improve%20processing%20speeds%20while%0Amaintaining%20accuracy.%20FastTextSpotter%20has%20been%20validated%20across%20multiple%0Adatasets%2C%20including%20ICDAR2015%20for%20regular%20texts%20and%20CTW1500%20and%20TotalText%20for%0Aarbitrary-shaped%20texts%2C%20benchmarking%20against%20current%20state-of-the-art%20models.%0AOur%20results%20indicate%20that%20FastTextSpotter%20not%20only%20achieves%20superior%20accuracy%0Ain%20detecting%20and%20recognizing%20multilingual%20scene%20text%20%28English%20and%20Vietnamese%29%0Abut%20also%20improves%20model%20efficiency%2C%20thereby%20setting%20new%20benchmarks%20in%20the%0Afield.%20This%20study%20underscores%20the%20potential%20of%20advanced%20transformer%0Aarchitectures%20in%20improving%20the%20adaptability%20and%20speed%20of%20text%20spotting%0Aapplications%20in%20diverse%20real-world%20settings.%20The%20dataset%2C%20code%2C%20and%20pre-trained%0Amodels%20have%20been%20released%20in%20our%20Github.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2408.14998v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DFastTextSpotter%253A%2520A%2520High-Efficiency%2520Transformer%2520for%2520Multilingual%2520Scene%250A%2520%2520Text%2520Spotting%26entry.906535625%3DAlloy%2520Das%2520and%2520Sanket%2520Biswas%2520and%2520Umapada%2520Pal%2520and%2520Josep%2520Llad%25C3%25B3s%2520and%2520Saumik%2520Bhattacharya%26entry.1292438233%3D%2520%2520The%2520proliferation%2520of%2520scene%2520text%2520in%2520both%2520structured%2520and%2520unstructured%250Aenvironments%2520presents%2520significant%2520challenges%2520in%2520optical%2520character%2520recognition%250A%2528OCR%2529%252C%2520necessitating%2520more%2520efficient%2520and%2520robust%2520text%2520spotting%2520solutions.%2520This%250Apaper%2520presents%2520FastTextSpotter%252C%2520a%2520framework%2520that%2520integrates%2520a%2520Swin%2520Transformer%250Avisual%2520backbone%2520with%2520a%2520Transformer%2520Encoder-Decoder%2520architecture%252C%2520enhanced%2520by%2520a%250Anovel%252C%2520faster%2520self-attention%2520unit%252C%2520SAC2%252C%2520to%2520improve%2520processing%2520speeds%2520while%250Amaintaining%2520accuracy.%2520FastTextSpotter%2520has%2520been%2520validated%2520across%2520multiple%250Adatasets%252C%2520including%2520ICDAR2015%2520for%2520regular%2520texts%2520and%2520CTW1500%2520and%2520TotalText%2520for%250Aarbitrary-shaped%2520texts%252C%2520benchmarking%2520against%2520current%2520state-of-the-art%2520models.%250AOur%2520results%2520indicate%2520that%2520FastTextSpotter%2520not%2520only%2520achieves%2520superior%2520accuracy%250Ain%2520detecting%2520and%2520recognizing%2520multilingual%2520scene%2520text%2520%2528English%2520and%2520Vietnamese%2529%250Abut%2520also%2520improves%2520model%2520efficiency%252C%2520thereby%2520setting%2520new%2520benchmarks%2520in%2520the%250Afield.%2520This%2520study%2520underscores%2520the%2520potential%2520of%2520advanced%2520transformer%250Aarchitectures%2520in%2520improving%2520the%2520adaptability%2520and%2520speed%2520of%2520text%2520spotting%250Aapplications%2520in%2520diverse%2520real-world%2520settings.%2520The%2520dataset%252C%2520code%252C%2520and%2520pre-trained%250Amodels%2520have%2520been%2520released%2520in%2520our%2520Github.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2408.14998v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=FastTextSpotter%3A%20A%20High-Efficiency%20Transformer%20for%20Multilingual%20Scene%0A%20%20Text%20Spotting&entry.906535625=Alloy%20Das%20and%20Sanket%20Biswas%20and%20Umapada%20Pal%20and%20Josep%20Llad%C3%B3s%20and%20Saumik%20Bhattacharya&entry.1292438233=%20%20The%20proliferation%20of%20scene%20text%20in%20both%20structured%20and%20unstructured%0Aenvironments%20presents%20significant%20challenges%20in%20optical%20character%20recognition%0A%28OCR%29%2C%20necessitating%20more%20efficient%20and%20robust%20text%20spotting%20solutions.%20This%0Apaper%20presents%20FastTextSpotter%2C%20a%20framework%20that%20integrates%20a%20Swin%20Transformer%0Avisual%20backbone%20with%20a%20Transformer%20Encoder-Decoder%20architecture%2C%20enhanced%20by%20a%0Anovel%2C%20faster%20self-attention%20unit%2C%20SAC2%2C%20to%20improve%20processing%20speeds%20while%0Amaintaining%20accuracy.%20FastTextSpotter%20has%20been%20validated%20across%20multiple%0Adatasets%2C%20including%20ICDAR2015%20for%20regular%20texts%20and%20CTW1500%20and%20TotalText%20for%0Aarbitrary-shaped%20texts%2C%20benchmarking%20against%20current%20state-of-the-art%20models.%0AOur%20results%20indicate%20that%20FastTextSpotter%20not%20only%20achieves%20superior%20accuracy%0Ain%20detecting%20and%20recognizing%20multilingual%20scene%20text%20%28English%20and%20Vietnamese%29%0Abut%20also%20improves%20model%20efficiency%2C%20thereby%20setting%20new%20benchmarks%20in%20the%0Afield.%20This%20study%20underscores%20the%20potential%20of%20advanced%20transformer%0Aarchitectures%20in%20improving%20the%20adaptability%20and%20speed%20of%20text%20spotting%0Aapplications%20in%20diverse%20real-world%20settings.%20The%20dataset%2C%20code%2C%20and%20pre-trained%0Amodels%20have%20been%20released%20in%20our%20Github.%0A&entry.1838667208=http%3A//arxiv.org/abs/2408.14998v1&entry.124074799=Read"},
{"title": "Scalable Supervisory Architecture for Autonomous Race Cars", "author": "Zal\u00e1n Demeter and P\u00e9ter Bogd\u00e1n and \u00c1rmin Bog\u00e1r-N\u00e9meth and Gergely B\u00e1ri", "abstract": "  In recent years, the number and importance of autonomous racing leagues, and\nconsequently the number of studies on them, has been growing. The seamless\nintegration between different series has gained attention due to the scene's\ndiversity. However, the high cost of full scale racing makes it a more\naccessible development model, to research at smaller form factors and scale up\nthe achieved results. This paper presents a scalable architecture designed for\nautonomous racing that emphasizes modularity, adaptability to diverse\nconfigurations, and the ability to supervise parallel execution of pipelines\nthat allows the use of different dynamic strategies. The system showcased\nconsistent racing performance across different environments, demonstrated\nthrough successful participation in two relevant competitions. The results\nconfirm the architecture's scalability and versatility, providing a robust\nfoundation for the development of competitive autonomous racing systems. The\nsuccessful application in real-world scenarios validates its practical\neffectiveness and highlights its potential for future advancements in\nautonomous racing technology.\n", "link": "http://arxiv.org/abs/2408.15049v1", "date": "2024-08-27", "relevancy": 2.034, "topK": [{"title": "RTG-SLAM: Real-time 3D Reconstruction at Scale using Gaussian Splatting", "link": "http://arxiv.org/abs/2404.19706v1", "similarity": 0.5121}, {"title": "Safe and efficient training of a control agent", "link": "https://patents.google.com/patent/US11709462B2/en", "similarity": 0.508}, {"title": "RoboEXP: Action-Conditioned Scene Graph via Interactive Exploration for\n  Robotic Manipulation", "link": "http://arxiv.org/abs/2402.15487v1", "similarity": 0.5008}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Scalable%20Supervisory%20Architecture%20for%20Autonomous%20Race%20Cars&body=Title%3A%20Scalable%20Supervisory%20Architecture%20for%20Autonomous%20Race%20Cars%0AAuthor%3A%20Zal%C3%A1n%20Demeter%20and%20P%C3%A9ter%20Bogd%C3%A1n%20and%20%C3%81rmin%20Bog%C3%A1r-N%C3%A9meth%20and%20Gergely%20B%C3%A1ri%0AAbstract%3A%20%20%20In%20recent%20years%2C%20the%20number%20and%20importance%20of%20autonomous%20racing%20leagues%2C%20and%0Aconsequently%20the%20number%20of%20studies%20on%20them%2C%20has%20been%20growing.%20The%20seamless%0Aintegration%20between%20different%20series%20has%20gained%20attention%20due%20to%20the%20scene%27s%0Adiversity.%20However%2C%20the%20high%20cost%20of%20full%20scale%20racing%20makes%20it%20a%20more%0Aaccessible%20development%20model%2C%20to%20research%20at%20smaller%20form%20factors%20and%20scale%20up%0Athe%20achieved%20results.%20This%20paper%20presents%20a%20scalable%20architecture%20designed%20for%0Aautonomous%20racing%20that%20emphasizes%20modularity%2C%20adaptability%20to%20diverse%0Aconfigurations%2C%20and%20the%20ability%20to%20supervise%20parallel%20execution%20of%20pipelines%0Athat%20allows%20the%20use%20of%20different%20dynamic%20strategies.%20The%20system%20showcased%0Aconsistent%20racing%20performance%20across%20different%20environments%2C%20demonstrated%0Athrough%20successful%20participation%20in%20two%20relevant%20competitions.%20The%20results%0Aconfirm%20the%20architecture%27s%20scalability%20and%20versatility%2C%20providing%20a%20robust%0Afoundation%20for%20the%20development%20of%20competitive%20autonomous%20racing%20systems.%20The%0Asuccessful%20application%20in%20real-world%20scenarios%20validates%20its%20practical%0Aeffectiveness%20and%20highlights%20its%20potential%20for%20future%20advancements%20in%0Aautonomous%20racing%20technology.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2408.15049v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DScalable%2520Supervisory%2520Architecture%2520for%2520Autonomous%2520Race%2520Cars%26entry.906535625%3DZal%25C3%25A1n%2520Demeter%2520and%2520P%25C3%25A9ter%2520Bogd%25C3%25A1n%2520and%2520%25C3%2581rmin%2520Bog%25C3%25A1r-N%25C3%25A9meth%2520and%2520Gergely%2520B%25C3%25A1ri%26entry.1292438233%3D%2520%2520In%2520recent%2520years%252C%2520the%2520number%2520and%2520importance%2520of%2520autonomous%2520racing%2520leagues%252C%2520and%250Aconsequently%2520the%2520number%2520of%2520studies%2520on%2520them%252C%2520has%2520been%2520growing.%2520The%2520seamless%250Aintegration%2520between%2520different%2520series%2520has%2520gained%2520attention%2520due%2520to%2520the%2520scene%2527s%250Adiversity.%2520However%252C%2520the%2520high%2520cost%2520of%2520full%2520scale%2520racing%2520makes%2520it%2520a%2520more%250Aaccessible%2520development%2520model%252C%2520to%2520research%2520at%2520smaller%2520form%2520factors%2520and%2520scale%2520up%250Athe%2520achieved%2520results.%2520This%2520paper%2520presents%2520a%2520scalable%2520architecture%2520designed%2520for%250Aautonomous%2520racing%2520that%2520emphasizes%2520modularity%252C%2520adaptability%2520to%2520diverse%250Aconfigurations%252C%2520and%2520the%2520ability%2520to%2520supervise%2520parallel%2520execution%2520of%2520pipelines%250Athat%2520allows%2520the%2520use%2520of%2520different%2520dynamic%2520strategies.%2520The%2520system%2520showcased%250Aconsistent%2520racing%2520performance%2520across%2520different%2520environments%252C%2520demonstrated%250Athrough%2520successful%2520participation%2520in%2520two%2520relevant%2520competitions.%2520The%2520results%250Aconfirm%2520the%2520architecture%2527s%2520scalability%2520and%2520versatility%252C%2520providing%2520a%2520robust%250Afoundation%2520for%2520the%2520development%2520of%2520competitive%2520autonomous%2520racing%2520systems.%2520The%250Asuccessful%2520application%2520in%2520real-world%2520scenarios%2520validates%2520its%2520practical%250Aeffectiveness%2520and%2520highlights%2520its%2520potential%2520for%2520future%2520advancements%2520in%250Aautonomous%2520racing%2520technology.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2408.15049v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Scalable%20Supervisory%20Architecture%20for%20Autonomous%20Race%20Cars&entry.906535625=Zal%C3%A1n%20Demeter%20and%20P%C3%A9ter%20Bogd%C3%A1n%20and%20%C3%81rmin%20Bog%C3%A1r-N%C3%A9meth%20and%20Gergely%20B%C3%A1ri&entry.1292438233=%20%20In%20recent%20years%2C%20the%20number%20and%20importance%20of%20autonomous%20racing%20leagues%2C%20and%0Aconsequently%20the%20number%20of%20studies%20on%20them%2C%20has%20been%20growing.%20The%20seamless%0Aintegration%20between%20different%20series%20has%20gained%20attention%20due%20to%20the%20scene%27s%0Adiversity.%20However%2C%20the%20high%20cost%20of%20full%20scale%20racing%20makes%20it%20a%20more%0Aaccessible%20development%20model%2C%20to%20research%20at%20smaller%20form%20factors%20and%20scale%20up%0Athe%20achieved%20results.%20This%20paper%20presents%20a%20scalable%20architecture%20designed%20for%0Aautonomous%20racing%20that%20emphasizes%20modularity%2C%20adaptability%20to%20diverse%0Aconfigurations%2C%20and%20the%20ability%20to%20supervise%20parallel%20execution%20of%20pipelines%0Athat%20allows%20the%20use%20of%20different%20dynamic%20strategies.%20The%20system%20showcased%0Aconsistent%20racing%20performance%20across%20different%20environments%2C%20demonstrated%0Athrough%20successful%20participation%20in%20two%20relevant%20competitions.%20The%20results%0Aconfirm%20the%20architecture%27s%20scalability%20and%20versatility%2C%20providing%20a%20robust%0Afoundation%20for%20the%20development%20of%20competitive%20autonomous%20racing%20systems.%20The%0Asuccessful%20application%20in%20real-world%20scenarios%20validates%20its%20practical%0Aeffectiveness%20and%20highlights%20its%20potential%20for%20future%20advancements%20in%0Aautonomous%20racing%20technology.%0A&entry.1838667208=http%3A//arxiv.org/abs/2408.15049v1&entry.124074799=Read"},
{"title": "ERX: A Fast Real-Time Anomaly Detection Algorithm for Hyperspectral\n  Line-Scanning", "author": "Samuel Garske and Bradley Evans and Christopher Artlett and KC Wong", "abstract": "  Detecting unexpected objects (anomalies) in real-time has great potential for\nmonitoring, managing, and protecting the environment. Hyperspectral line-scan\ncameras are a low-cost solution that enhance confidence in anomaly detection\nover RGB and multispectral imagery. However, real-time algorithms for these\ncameras must be fast when using small computers (e.g., those onboard a drone or\nsmall satellite), scalable to high dimensions, adaptable to changing scenery,\nand robust against geometric and radiometric distortions. This paper introduces\nthe Exponentially moving RX algorithm (ERX) and compares it to existing\nRX-based anomaly detection methods for real-time line-scanning. ERX was tested\nusing a Jetson Xavier NX compute module, achieving the best combination of\nspeed and detection across three novel datasets compared to the other\nalgorithms. This research paves the way for future studies in grouping and\nlocating anomalous objects, adaptive and automatic threshold selection, and\nreal-time field tests. The Python code for the algorithms and experiments is\navailable at https://github.com/WiseGamgee/HyperAD.\n", "link": "http://arxiv.org/abs/2408.14947v1", "date": "2024-08-27", "relevancy": 2.0339, "topK": [{"title": "RTG-SLAM: Real-time 3D Reconstruction at Scale using Gaussian Splatting", "link": "http://arxiv.org/abs/2404.19706v1", "similarity": 0.5509}, {"title": "SDXS: Real-Time One-Step Latent Diffusion Models with Image Conditions", "link": "http://arxiv.org/abs/2403.16627v1", "similarity": 0.5142}, {"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.4638}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20ERX%3A%20A%20Fast%20Real-Time%20Anomaly%20Detection%20Algorithm%20for%20Hyperspectral%0A%20%20Line-Scanning&body=Title%3A%20ERX%3A%20A%20Fast%20Real-Time%20Anomaly%20Detection%20Algorithm%20for%20Hyperspectral%0A%20%20Line-Scanning%0AAuthor%3A%20Samuel%20Garske%20and%20Bradley%20Evans%20and%20Christopher%20Artlett%20and%20KC%20Wong%0AAbstract%3A%20%20%20Detecting%20unexpected%20objects%20%28anomalies%29%20in%20real-time%20has%20great%20potential%20for%0Amonitoring%2C%20managing%2C%20and%20protecting%20the%20environment.%20Hyperspectral%20line-scan%0Acameras%20are%20a%20low-cost%20solution%20that%20enhance%20confidence%20in%20anomaly%20detection%0Aover%20RGB%20and%20multispectral%20imagery.%20However%2C%20real-time%20algorithms%20for%20these%0Acameras%20must%20be%20fast%20when%20using%20small%20computers%20%28e.g.%2C%20those%20onboard%20a%20drone%20or%0Asmall%20satellite%29%2C%20scalable%20to%20high%20dimensions%2C%20adaptable%20to%20changing%20scenery%2C%0Aand%20robust%20against%20geometric%20and%20radiometric%20distortions.%20This%20paper%20introduces%0Athe%20Exponentially%20moving%20RX%20algorithm%20%28ERX%29%20and%20compares%20it%20to%20existing%0ARX-based%20anomaly%20detection%20methods%20for%20real-time%20line-scanning.%20ERX%20was%20tested%0Ausing%20a%20Jetson%20Xavier%20NX%20compute%20module%2C%20achieving%20the%20best%20combination%20of%0Aspeed%20and%20detection%20across%20three%20novel%20datasets%20compared%20to%20the%20other%0Aalgorithms.%20This%20research%20paves%20the%20way%20for%20future%20studies%20in%20grouping%20and%0Alocating%20anomalous%20objects%2C%20adaptive%20and%20automatic%20threshold%20selection%2C%20and%0Areal-time%20field%20tests.%20The%20Python%20code%20for%20the%20algorithms%20and%20experiments%20is%0Aavailable%20at%20https%3A//github.com/WiseGamgee/HyperAD.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2408.14947v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DERX%253A%2520A%2520Fast%2520Real-Time%2520Anomaly%2520Detection%2520Algorithm%2520for%2520Hyperspectral%250A%2520%2520Line-Scanning%26entry.906535625%3DSamuel%2520Garske%2520and%2520Bradley%2520Evans%2520and%2520Christopher%2520Artlett%2520and%2520KC%2520Wong%26entry.1292438233%3D%2520%2520Detecting%2520unexpected%2520objects%2520%2528anomalies%2529%2520in%2520real-time%2520has%2520great%2520potential%2520for%250Amonitoring%252C%2520managing%252C%2520and%2520protecting%2520the%2520environment.%2520Hyperspectral%2520line-scan%250Acameras%2520are%2520a%2520low-cost%2520solution%2520that%2520enhance%2520confidence%2520in%2520anomaly%2520detection%250Aover%2520RGB%2520and%2520multispectral%2520imagery.%2520However%252C%2520real-time%2520algorithms%2520for%2520these%250Acameras%2520must%2520be%2520fast%2520when%2520using%2520small%2520computers%2520%2528e.g.%252C%2520those%2520onboard%2520a%2520drone%2520or%250Asmall%2520satellite%2529%252C%2520scalable%2520to%2520high%2520dimensions%252C%2520adaptable%2520to%2520changing%2520scenery%252C%250Aand%2520robust%2520against%2520geometric%2520and%2520radiometric%2520distortions.%2520This%2520paper%2520introduces%250Athe%2520Exponentially%2520moving%2520RX%2520algorithm%2520%2528ERX%2529%2520and%2520compares%2520it%2520to%2520existing%250ARX-based%2520anomaly%2520detection%2520methods%2520for%2520real-time%2520line-scanning.%2520ERX%2520was%2520tested%250Ausing%2520a%2520Jetson%2520Xavier%2520NX%2520compute%2520module%252C%2520achieving%2520the%2520best%2520combination%2520of%250Aspeed%2520and%2520detection%2520across%2520three%2520novel%2520datasets%2520compared%2520to%2520the%2520other%250Aalgorithms.%2520This%2520research%2520paves%2520the%2520way%2520for%2520future%2520studies%2520in%2520grouping%2520and%250Alocating%2520anomalous%2520objects%252C%2520adaptive%2520and%2520automatic%2520threshold%2520selection%252C%2520and%250Areal-time%2520field%2520tests.%2520The%2520Python%2520code%2520for%2520the%2520algorithms%2520and%2520experiments%2520is%250Aavailable%2520at%2520https%253A//github.com/WiseGamgee/HyperAD.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2408.14947v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=ERX%3A%20A%20Fast%20Real-Time%20Anomaly%20Detection%20Algorithm%20for%20Hyperspectral%0A%20%20Line-Scanning&entry.906535625=Samuel%20Garske%20and%20Bradley%20Evans%20and%20Christopher%20Artlett%20and%20KC%20Wong&entry.1292438233=%20%20Detecting%20unexpected%20objects%20%28anomalies%29%20in%20real-time%20has%20great%20potential%20for%0Amonitoring%2C%20managing%2C%20and%20protecting%20the%20environment.%20Hyperspectral%20line-scan%0Acameras%20are%20a%20low-cost%20solution%20that%20enhance%20confidence%20in%20anomaly%20detection%0Aover%20RGB%20and%20multispectral%20imagery.%20However%2C%20real-time%20algorithms%20for%20these%0Acameras%20must%20be%20fast%20when%20using%20small%20computers%20%28e.g.%2C%20those%20onboard%20a%20drone%20or%0Asmall%20satellite%29%2C%20scalable%20to%20high%20dimensions%2C%20adaptable%20to%20changing%20scenery%2C%0Aand%20robust%20against%20geometric%20and%20radiometric%20distortions.%20This%20paper%20introduces%0Athe%20Exponentially%20moving%20RX%20algorithm%20%28ERX%29%20and%20compares%20it%20to%20existing%0ARX-based%20anomaly%20detection%20methods%20for%20real-time%20line-scanning.%20ERX%20was%20tested%0Ausing%20a%20Jetson%20Xavier%20NX%20compute%20module%2C%20achieving%20the%20best%20combination%20of%0Aspeed%20and%20detection%20across%20three%20novel%20datasets%20compared%20to%20the%20other%0Aalgorithms.%20This%20research%20paves%20the%20way%20for%20future%20studies%20in%20grouping%20and%0Alocating%20anomalous%20objects%2C%20adaptive%20and%20automatic%20threshold%20selection%2C%20and%0Areal-time%20field%20tests.%20The%20Python%20code%20for%20the%20algorithms%20and%20experiments%20is%0Aavailable%20at%20https%3A//github.com/WiseGamgee/HyperAD.%0A&entry.1838667208=http%3A//arxiv.org/abs/2408.14947v1&entry.124074799=Read"},
{"title": "Applying ViT in Generalized Few-shot Semantic Segmentation", "author": "Liyuan Geng and Jinhong Xia and Yuanhe Guo", "abstract": "  This paper explores the capability of ViT-based models under the generalized\nfew-shot semantic segmentation (GFSS) framework. We conduct experiments with\nvarious combinations of backbone models, including ResNets and pretrained\nVision Transformer (ViT)-based models, along with decoders featuring a linear\nclassifier, UPerNet, and Mask Transformer. The structure made of DINOv2 and\nlinear classifier takes the lead on popular few-shot segmentation bench mark\nPASCAL-$5^i$, substantially outperforming the best of ResNet structure by 116%\nin one-shot scenario. We demonstrate the great potential of large pretrained\nViT-based model on GFSS task, and expect further improvement on testing\nbenchmarks. However, a potential caveat is that when applying pure ViT-based\nmodel and large scale ViT decoder, the model is easy to overfit.\n", "link": "http://arxiv.org/abs/2408.14957v1", "date": "2024-08-27", "relevancy": 2.0321, "topK": [{"title": "Scaling Rectified Flow Transformers for High-Resolution Image Synthesis", "link": "http://arxiv.org/abs/2403.03206v1", "similarity": 0.5158}, {"title": "Magic-Me: Identity-Specific Video Customized Diffusion", "link": "http://arxiv.org/abs/2402.09368v1", "similarity": 0.5091}, {"title": "UGG: Unified Generative Grasping", "link": "https://arxiv.org/abs/2311.16917", "similarity": 0.5039}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Applying%20ViT%20in%20Generalized%20Few-shot%20Semantic%20Segmentation&body=Title%3A%20Applying%20ViT%20in%20Generalized%20Few-shot%20Semantic%20Segmentation%0AAuthor%3A%20Liyuan%20Geng%20and%20Jinhong%20Xia%20and%20Yuanhe%20Guo%0AAbstract%3A%20%20%20This%20paper%20explores%20the%20capability%20of%20ViT-based%20models%20under%20the%20generalized%0Afew-shot%20semantic%20segmentation%20%28GFSS%29%20framework.%20We%20conduct%20experiments%20with%0Avarious%20combinations%20of%20backbone%20models%2C%20including%20ResNets%20and%20pretrained%0AVision%20Transformer%20%28ViT%29-based%20models%2C%20along%20with%20decoders%20featuring%20a%20linear%0Aclassifier%2C%20UPerNet%2C%20and%20Mask%20Transformer.%20The%20structure%20made%20of%20DINOv2%20and%0Alinear%20classifier%20takes%20the%20lead%20on%20popular%20few-shot%20segmentation%20bench%20mark%0APASCAL-%245%5Ei%24%2C%20substantially%20outperforming%20the%20best%20of%20ResNet%20structure%20by%20116%25%0Ain%20one-shot%20scenario.%20We%20demonstrate%20the%20great%20potential%20of%20large%20pretrained%0AViT-based%20model%20on%20GFSS%20task%2C%20and%20expect%20further%20improvement%20on%20testing%0Abenchmarks.%20However%2C%20a%20potential%20caveat%20is%20that%20when%20applying%20pure%20ViT-based%0Amodel%20and%20large%20scale%20ViT%20decoder%2C%20the%20model%20is%20easy%20to%20overfit.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2408.14957v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DApplying%2520ViT%2520in%2520Generalized%2520Few-shot%2520Semantic%2520Segmentation%26entry.906535625%3DLiyuan%2520Geng%2520and%2520Jinhong%2520Xia%2520and%2520Yuanhe%2520Guo%26entry.1292438233%3D%2520%2520This%2520paper%2520explores%2520the%2520capability%2520of%2520ViT-based%2520models%2520under%2520the%2520generalized%250Afew-shot%2520semantic%2520segmentation%2520%2528GFSS%2529%2520framework.%2520We%2520conduct%2520experiments%2520with%250Avarious%2520combinations%2520of%2520backbone%2520models%252C%2520including%2520ResNets%2520and%2520pretrained%250AVision%2520Transformer%2520%2528ViT%2529-based%2520models%252C%2520along%2520with%2520decoders%2520featuring%2520a%2520linear%250Aclassifier%252C%2520UPerNet%252C%2520and%2520Mask%2520Transformer.%2520The%2520structure%2520made%2520of%2520DINOv2%2520and%250Alinear%2520classifier%2520takes%2520the%2520lead%2520on%2520popular%2520few-shot%2520segmentation%2520bench%2520mark%250APASCAL-%25245%255Ei%2524%252C%2520substantially%2520outperforming%2520the%2520best%2520of%2520ResNet%2520structure%2520by%2520116%2525%250Ain%2520one-shot%2520scenario.%2520We%2520demonstrate%2520the%2520great%2520potential%2520of%2520large%2520pretrained%250AViT-based%2520model%2520on%2520GFSS%2520task%252C%2520and%2520expect%2520further%2520improvement%2520on%2520testing%250Abenchmarks.%2520However%252C%2520a%2520potential%2520caveat%2520is%2520that%2520when%2520applying%2520pure%2520ViT-based%250Amodel%2520and%2520large%2520scale%2520ViT%2520decoder%252C%2520the%2520model%2520is%2520easy%2520to%2520overfit.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2408.14957v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Applying%20ViT%20in%20Generalized%20Few-shot%20Semantic%20Segmentation&entry.906535625=Liyuan%20Geng%20and%20Jinhong%20Xia%20and%20Yuanhe%20Guo&entry.1292438233=%20%20This%20paper%20explores%20the%20capability%20of%20ViT-based%20models%20under%20the%20generalized%0Afew-shot%20semantic%20segmentation%20%28GFSS%29%20framework.%20We%20conduct%20experiments%20with%0Avarious%20combinations%20of%20backbone%20models%2C%20including%20ResNets%20and%20pretrained%0AVision%20Transformer%20%28ViT%29-based%20models%2C%20along%20with%20decoders%20featuring%20a%20linear%0Aclassifier%2C%20UPerNet%2C%20and%20Mask%20Transformer.%20The%20structure%20made%20of%20DINOv2%20and%0Alinear%20classifier%20takes%20the%20lead%20on%20popular%20few-shot%20segmentation%20bench%20mark%0APASCAL-%245%5Ei%24%2C%20substantially%20outperforming%20the%20best%20of%20ResNet%20structure%20by%20116%25%0Ain%20one-shot%20scenario.%20We%20demonstrate%20the%20great%20potential%20of%20large%20pretrained%0AViT-based%20model%20on%20GFSS%20task%2C%20and%20expect%20further%20improvement%20on%20testing%0Abenchmarks.%20However%2C%20a%20potential%20caveat%20is%20that%20when%20applying%20pure%20ViT-based%0Amodel%20and%20large%20scale%20ViT%20decoder%2C%20the%20model%20is%20easy%20to%20overfit.%0A&entry.1838667208=http%3A//arxiv.org/abs/2408.14957v1&entry.124074799=Read"},
{"title": "How transformers learn structured data: insights from hierarchical\n  filtering", "author": "Jerome Garnier-Brun and Marc M\u00e9zard and Emanuele Moscato and Luca Saglietti", "abstract": "  We introduce a hierarchical filtering procedure for generative models of\nsequences on trees, enabling control over the range of positional correlations\nin the data. Leveraging this controlled setting, we provide evidence that\nvanilla encoder-only transformer architectures can implement the optimal Belief\nPropagation algorithm on both root classification and masked language modeling\ntasks. Correlations at larger distances corresponding to increasing layers of\nthe hierarchy are sequentially included as the network is trained. We analyze\nhow the transformer layers succeed by focusing on attention maps from models\ntrained with varying degrees of filtering. These attention maps show clear\nevidence for iterative hierarchical reconstruction of correlations, and we can\nrelate these observations to a plausible implementation of the exact inference\nalgorithm for the network sizes considered.\n", "link": "http://arxiv.org/abs/2408.15138v1", "date": "2024-08-27", "relevancy": 2.0261, "topK": [{"title": "Scaling Rectified Flow Transformers for High-Resolution Image Synthesis", "link": "http://arxiv.org/abs/2403.03206v1", "similarity": 0.5925}, {"title": "ToDo: Token Downsampling for Efficient Generation of High-Resolution\n  Images", "link": "http://arxiv.org/abs/2402.13573v2", "similarity": 0.4931}, {"title": "Genie: Generative Interactive Environments", "link": "http://arxiv.org/abs/2402.15391v1", "similarity": 0.4856}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20How%20transformers%20learn%20structured%20data%3A%20insights%20from%20hierarchical%0A%20%20filtering&body=Title%3A%20How%20transformers%20learn%20structured%20data%3A%20insights%20from%20hierarchical%0A%20%20filtering%0AAuthor%3A%20Jerome%20Garnier-Brun%20and%20Marc%20M%C3%A9zard%20and%20Emanuele%20Moscato%20and%20Luca%20Saglietti%0AAbstract%3A%20%20%20We%20introduce%20a%20hierarchical%20filtering%20procedure%20for%20generative%20models%20of%0Asequences%20on%20trees%2C%20enabling%20control%20over%20the%20range%20of%20positional%20correlations%0Ain%20the%20data.%20Leveraging%20this%20controlled%20setting%2C%20we%20provide%20evidence%20that%0Avanilla%20encoder-only%20transformer%20architectures%20can%20implement%20the%20optimal%20Belief%0APropagation%20algorithm%20on%20both%20root%20classification%20and%20masked%20language%20modeling%0Atasks.%20Correlations%20at%20larger%20distances%20corresponding%20to%20increasing%20layers%20of%0Athe%20hierarchy%20are%20sequentially%20included%20as%20the%20network%20is%20trained.%20We%20analyze%0Ahow%20the%20transformer%20layers%20succeed%20by%20focusing%20on%20attention%20maps%20from%20models%0Atrained%20with%20varying%20degrees%20of%20filtering.%20These%20attention%20maps%20show%20clear%0Aevidence%20for%20iterative%20hierarchical%20reconstruction%20of%20correlations%2C%20and%20we%20can%0Arelate%20these%20observations%20to%20a%20plausible%20implementation%20of%20the%20exact%20inference%0Aalgorithm%20for%20the%20network%20sizes%20considered.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2408.15138v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DHow%2520transformers%2520learn%2520structured%2520data%253A%2520insights%2520from%2520hierarchical%250A%2520%2520filtering%26entry.906535625%3DJerome%2520Garnier-Brun%2520and%2520Marc%2520M%25C3%25A9zard%2520and%2520Emanuele%2520Moscato%2520and%2520Luca%2520Saglietti%26entry.1292438233%3D%2520%2520We%2520introduce%2520a%2520hierarchical%2520filtering%2520procedure%2520for%2520generative%2520models%2520of%250Asequences%2520on%2520trees%252C%2520enabling%2520control%2520over%2520the%2520range%2520of%2520positional%2520correlations%250Ain%2520the%2520data.%2520Leveraging%2520this%2520controlled%2520setting%252C%2520we%2520provide%2520evidence%2520that%250Avanilla%2520encoder-only%2520transformer%2520architectures%2520can%2520implement%2520the%2520optimal%2520Belief%250APropagation%2520algorithm%2520on%2520both%2520root%2520classification%2520and%2520masked%2520language%2520modeling%250Atasks.%2520Correlations%2520at%2520larger%2520distances%2520corresponding%2520to%2520increasing%2520layers%2520of%250Athe%2520hierarchy%2520are%2520sequentially%2520included%2520as%2520the%2520network%2520is%2520trained.%2520We%2520analyze%250Ahow%2520the%2520transformer%2520layers%2520succeed%2520by%2520focusing%2520on%2520attention%2520maps%2520from%2520models%250Atrained%2520with%2520varying%2520degrees%2520of%2520filtering.%2520These%2520attention%2520maps%2520show%2520clear%250Aevidence%2520for%2520iterative%2520hierarchical%2520reconstruction%2520of%2520correlations%252C%2520and%2520we%2520can%250Arelate%2520these%2520observations%2520to%2520a%2520plausible%2520implementation%2520of%2520the%2520exact%2520inference%250Aalgorithm%2520for%2520the%2520network%2520sizes%2520considered.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2408.15138v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=How%20transformers%20learn%20structured%20data%3A%20insights%20from%20hierarchical%0A%20%20filtering&entry.906535625=Jerome%20Garnier-Brun%20and%20Marc%20M%C3%A9zard%20and%20Emanuele%20Moscato%20and%20Luca%20Saglietti&entry.1292438233=%20%20We%20introduce%20a%20hierarchical%20filtering%20procedure%20for%20generative%20models%20of%0Asequences%20on%20trees%2C%20enabling%20control%20over%20the%20range%20of%20positional%20correlations%0Ain%20the%20data.%20Leveraging%20this%20controlled%20setting%2C%20we%20provide%20evidence%20that%0Avanilla%20encoder-only%20transformer%20architectures%20can%20implement%20the%20optimal%20Belief%0APropagation%20algorithm%20on%20both%20root%20classification%20and%20masked%20language%20modeling%0Atasks.%20Correlations%20at%20larger%20distances%20corresponding%20to%20increasing%20layers%20of%0Athe%20hierarchy%20are%20sequentially%20included%20as%20the%20network%20is%20trained.%20We%20analyze%0Ahow%20the%20transformer%20layers%20succeed%20by%20focusing%20on%20attention%20maps%20from%20models%0Atrained%20with%20varying%20degrees%20of%20filtering.%20These%20attention%20maps%20show%20clear%0Aevidence%20for%20iterative%20hierarchical%20reconstruction%20of%20correlations%2C%20and%20we%20can%0Arelate%20these%20observations%20to%20a%20plausible%20implementation%20of%20the%20exact%20inference%0Aalgorithm%20for%20the%20network%20sizes%20considered.%0A&entry.1838667208=http%3A//arxiv.org/abs/2408.15138v1&entry.124074799=Read"},
{"title": "Graph GOSPA metric: a metric to measure the discrepancy between graphs\n  of different sizes", "author": "Jinhao Gu and \u00c1ngel F. Garc\u00eda-Fern\u00e1ndez and Robert E. Firth and Lennart Svensson", "abstract": "  This paper proposes a metric to measure the dissimilarity between graphs that\nmay have a different number of nodes. The proposed metric extends the\ngeneralised optimal subpattern assignment (GOSPA) metric, which is a metric for\nsets, to graphs. The proposed graph GOSPA metric includes costs associated with\nnode attribute errors for properly assigned nodes, missed and false nodes and\nedge mismatches between graphs. The computation of this metric is based on\nfinding the optimal assignments between nodes in the two graphs, with the\npossibility of leaving some of the nodes unassigned. We also propose a lower\nbound for the metric, which is also a metric for graphs and is computable in\npolynomial time using linear programming. The metric is first derived for\nundirected unweighted graphs and it is then extended to directed and weighted\ngraphs. The properties of the metric are demonstrated via simulated and\nempirical datasets.\n", "link": "http://arxiv.org/abs/2311.07596v2", "date": "2024-08-27", "relevancy": 2.0238, "topK": [{"title": "Dynamic Gaussians Mesh: Consistent Mesh Reconstruction from Monocular\n  Videos", "link": "http://arxiv.org/abs/2404.12379v2", "similarity": 0.4139}, {"title": "Self-supervised Photographic Image Layout Representation Learning", "link": "http://arxiv.org/abs/2403.03740v1", "similarity": 0.4018}, {"title": "RTG-SLAM: Real-time 3D Reconstruction at Scale using Gaussian Splatting", "link": "http://arxiv.org/abs/2404.19706v1", "similarity": 0.3986}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Graph%20GOSPA%20metric%3A%20a%20metric%20to%20measure%20the%20discrepancy%20between%20graphs%0A%20%20of%20different%20sizes&body=Title%3A%20Graph%20GOSPA%20metric%3A%20a%20metric%20to%20measure%20the%20discrepancy%20between%20graphs%0A%20%20of%20different%20sizes%0AAuthor%3A%20Jinhao%20Gu%20and%20%C3%81ngel%20F.%20Garc%C3%ADa-Fern%C3%A1ndez%20and%20Robert%20E.%20Firth%20and%20Lennart%20Svensson%0AAbstract%3A%20%20%20This%20paper%20proposes%20a%20metric%20to%20measure%20the%20dissimilarity%20between%20graphs%20that%0Amay%20have%20a%20different%20number%20of%20nodes.%20The%20proposed%20metric%20extends%20the%0Ageneralised%20optimal%20subpattern%20assignment%20%28GOSPA%29%20metric%2C%20which%20is%20a%20metric%20for%0Asets%2C%20to%20graphs.%20The%20proposed%20graph%20GOSPA%20metric%20includes%20costs%20associated%20with%0Anode%20attribute%20errors%20for%20properly%20assigned%20nodes%2C%20missed%20and%20false%20nodes%20and%0Aedge%20mismatches%20between%20graphs.%20The%20computation%20of%20this%20metric%20is%20based%20on%0Afinding%20the%20optimal%20assignments%20between%20nodes%20in%20the%20two%20graphs%2C%20with%20the%0Apossibility%20of%20leaving%20some%20of%20the%20nodes%20unassigned.%20We%20also%20propose%20a%20lower%0Abound%20for%20the%20metric%2C%20which%20is%20also%20a%20metric%20for%20graphs%20and%20is%20computable%20in%0Apolynomial%20time%20using%20linear%20programming.%20The%20metric%20is%20first%20derived%20for%0Aundirected%20unweighted%20graphs%20and%20it%20is%20then%20extended%20to%20directed%20and%20weighted%0Agraphs.%20The%20properties%20of%20the%20metric%20are%20demonstrated%20via%20simulated%20and%0Aempirical%20datasets.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2311.07596v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DGraph%2520GOSPA%2520metric%253A%2520a%2520metric%2520to%2520measure%2520the%2520discrepancy%2520between%2520graphs%250A%2520%2520of%2520different%2520sizes%26entry.906535625%3DJinhao%2520Gu%2520and%2520%25C3%2581ngel%2520F.%2520Garc%25C3%25ADa-Fern%25C3%25A1ndez%2520and%2520Robert%2520E.%2520Firth%2520and%2520Lennart%2520Svensson%26entry.1292438233%3D%2520%2520This%2520paper%2520proposes%2520a%2520metric%2520to%2520measure%2520the%2520dissimilarity%2520between%2520graphs%2520that%250Amay%2520have%2520a%2520different%2520number%2520of%2520nodes.%2520The%2520proposed%2520metric%2520extends%2520the%250Ageneralised%2520optimal%2520subpattern%2520assignment%2520%2528GOSPA%2529%2520metric%252C%2520which%2520is%2520a%2520metric%2520for%250Asets%252C%2520to%2520graphs.%2520The%2520proposed%2520graph%2520GOSPA%2520metric%2520includes%2520costs%2520associated%2520with%250Anode%2520attribute%2520errors%2520for%2520properly%2520assigned%2520nodes%252C%2520missed%2520and%2520false%2520nodes%2520and%250Aedge%2520mismatches%2520between%2520graphs.%2520The%2520computation%2520of%2520this%2520metric%2520is%2520based%2520on%250Afinding%2520the%2520optimal%2520assignments%2520between%2520nodes%2520in%2520the%2520two%2520graphs%252C%2520with%2520the%250Apossibility%2520of%2520leaving%2520some%2520of%2520the%2520nodes%2520unassigned.%2520We%2520also%2520propose%2520a%2520lower%250Abound%2520for%2520the%2520metric%252C%2520which%2520is%2520also%2520a%2520metric%2520for%2520graphs%2520and%2520is%2520computable%2520in%250Apolynomial%2520time%2520using%2520linear%2520programming.%2520The%2520metric%2520is%2520first%2520derived%2520for%250Aundirected%2520unweighted%2520graphs%2520and%2520it%2520is%2520then%2520extended%2520to%2520directed%2520and%2520weighted%250Agraphs.%2520The%2520properties%2520of%2520the%2520metric%2520are%2520demonstrated%2520via%2520simulated%2520and%250Aempirical%2520datasets.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2311.07596v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Graph%20GOSPA%20metric%3A%20a%20metric%20to%20measure%20the%20discrepancy%20between%20graphs%0A%20%20of%20different%20sizes&entry.906535625=Jinhao%20Gu%20and%20%C3%81ngel%20F.%20Garc%C3%ADa-Fern%C3%A1ndez%20and%20Robert%20E.%20Firth%20and%20Lennart%20Svensson&entry.1292438233=%20%20This%20paper%20proposes%20a%20metric%20to%20measure%20the%20dissimilarity%20between%20graphs%20that%0Amay%20have%20a%20different%20number%20of%20nodes.%20The%20proposed%20metric%20extends%20the%0Ageneralised%20optimal%20subpattern%20assignment%20%28GOSPA%29%20metric%2C%20which%20is%20a%20metric%20for%0Asets%2C%20to%20graphs.%20The%20proposed%20graph%20GOSPA%20metric%20includes%20costs%20associated%20with%0Anode%20attribute%20errors%20for%20properly%20assigned%20nodes%2C%20missed%20and%20false%20nodes%20and%0Aedge%20mismatches%20between%20graphs.%20The%20computation%20of%20this%20metric%20is%20based%20on%0Afinding%20the%20optimal%20assignments%20between%20nodes%20in%20the%20two%20graphs%2C%20with%20the%0Apossibility%20of%20leaving%20some%20of%20the%20nodes%20unassigned.%20We%20also%20propose%20a%20lower%0Abound%20for%20the%20metric%2C%20which%20is%20also%20a%20metric%20for%20graphs%20and%20is%20computable%20in%0Apolynomial%20time%20using%20linear%20programming.%20The%20metric%20is%20first%20derived%20for%0Aundirected%20unweighted%20graphs%20and%20it%20is%20then%20extended%20to%20directed%20and%20weighted%0Agraphs.%20The%20properties%20of%20the%20metric%20are%20demonstrated%20via%20simulated%20and%0Aempirical%20datasets.%0A&entry.1838667208=http%3A//arxiv.org/abs/2311.07596v2&entry.124074799=Read"},
{"title": "CVPT: Cross-Attention help Visual Prompt Tuning adapt visual task", "author": "Lingyun Huang and Jianxu Mao and Yaonan Wang and Junfei Yi and Ziming Tao", "abstract": "  In recent years, the rapid expansion of model sizes has led to large-scale\npre-trained models demonstrating remarkable capabilities. Consequently, there\nhas been a trend towards increasing the scale of models. However, this trend\nintroduces significant challenges, including substantial computational costs of\ntraining and transfer to downstream tasks. To address these issues,\nParameter-Efficient Fine-Tuning (PEFT) methods have been introduced. These\nmethods optimize large-scale pre-trained models for specific tasks by\nfine-tuning a select group of parameters. Among these PEFT methods,\nadapter-based and prompt-based methods are the primary techniques.\nSpecifically, in the field of visual fine-tuning, adapters gain prominence over\nprompts because of the latter's relatively weaker performance and efficiency.\nUnder the circumstances, we refine the widely-used Visual Prompt Tuning (VPT)\nmethod, proposing Cross Visual Prompt Tuning (CVPT). CVPT calculates\ncross-attention between the prompt tokens and the embedded tokens, which allows\nus to compute the semantic relationship between them and conduct the\nfine-tuning of models exactly to adapt visual tasks better. Furthermore, we\nintroduce the weight-sharing mechanism to initialize the parameters of\ncross-attention, which avoids massive learnable parameters from cross-attention\nand enhances the representative capability of cross-attention. We conduct\ncomprehensive testing across 25 datasets and the result indicates that CVPT\nsignificantly improves VPT's performance and efficiency in visual tasks. For\nexample, on the VTAB-1K benchmark, CVPT outperforms VPT over 4% in average\naccuracy, rivaling the advanced adapter-based methods in performance and\nefficiency. Our experiments confirm that prompt-based methods can achieve\nexceptional results in visual fine-tuning.\n", "link": "http://arxiv.org/abs/2408.14961v1", "date": "2024-08-27", "relevancy": 2.009, "topK": [{"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5099}, {"title": "ToDo: Token Downsampling for Efficient Generation of High-Resolution\n  Images", "link": "http://arxiv.org/abs/2402.13573v2", "similarity": 0.4975}, {"title": "Multimodal-Conditioned Latent Diffusion Models for Fashion Image Editing", "link": "http://arxiv.org/abs/2403.14828v2", "similarity": 0.495}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20CVPT%3A%20Cross-Attention%20help%20Visual%20Prompt%20Tuning%20adapt%20visual%20task&body=Title%3A%20CVPT%3A%20Cross-Attention%20help%20Visual%20Prompt%20Tuning%20adapt%20visual%20task%0AAuthor%3A%20Lingyun%20Huang%20and%20Jianxu%20Mao%20and%20Yaonan%20Wang%20and%20Junfei%20Yi%20and%20Ziming%20Tao%0AAbstract%3A%20%20%20In%20recent%20years%2C%20the%20rapid%20expansion%20of%20model%20sizes%20has%20led%20to%20large-scale%0Apre-trained%20models%20demonstrating%20remarkable%20capabilities.%20Consequently%2C%20there%0Ahas%20been%20a%20trend%20towards%20increasing%20the%20scale%20of%20models.%20However%2C%20this%20trend%0Aintroduces%20significant%20challenges%2C%20including%20substantial%20computational%20costs%20of%0Atraining%20and%20transfer%20to%20downstream%20tasks.%20To%20address%20these%20issues%2C%0AParameter-Efficient%20Fine-Tuning%20%28PEFT%29%20methods%20have%20been%20introduced.%20These%0Amethods%20optimize%20large-scale%20pre-trained%20models%20for%20specific%20tasks%20by%0Afine-tuning%20a%20select%20group%20of%20parameters.%20Among%20these%20PEFT%20methods%2C%0Aadapter-based%20and%20prompt-based%20methods%20are%20the%20primary%20techniques.%0ASpecifically%2C%20in%20the%20field%20of%20visual%20fine-tuning%2C%20adapters%20gain%20prominence%20over%0Aprompts%20because%20of%20the%20latter%27s%20relatively%20weaker%20performance%20and%20efficiency.%0AUnder%20the%20circumstances%2C%20we%20refine%20the%20widely-used%20Visual%20Prompt%20Tuning%20%28VPT%29%0Amethod%2C%20proposing%20Cross%20Visual%20Prompt%20Tuning%20%28CVPT%29.%20CVPT%20calculates%0Across-attention%20between%20the%20prompt%20tokens%20and%20the%20embedded%20tokens%2C%20which%20allows%0Aus%20to%20compute%20the%20semantic%20relationship%20between%20them%20and%20conduct%20the%0Afine-tuning%20of%20models%20exactly%20to%20adapt%20visual%20tasks%20better.%20Furthermore%2C%20we%0Aintroduce%20the%20weight-sharing%20mechanism%20to%20initialize%20the%20parameters%20of%0Across-attention%2C%20which%20avoids%20massive%20learnable%20parameters%20from%20cross-attention%0Aand%20enhances%20the%20representative%20capability%20of%20cross-attention.%20We%20conduct%0Acomprehensive%20testing%20across%2025%20datasets%20and%20the%20result%20indicates%20that%20CVPT%0Asignificantly%20improves%20VPT%27s%20performance%20and%20efficiency%20in%20visual%20tasks.%20For%0Aexample%2C%20on%20the%20VTAB-1K%20benchmark%2C%20CVPT%20outperforms%20VPT%20over%204%25%20in%20average%0Aaccuracy%2C%20rivaling%20the%20advanced%20adapter-based%20methods%20in%20performance%20and%0Aefficiency.%20Our%20experiments%20confirm%20that%20prompt-based%20methods%20can%20achieve%0Aexceptional%20results%20in%20visual%20fine-tuning.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2408.14961v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DCVPT%253A%2520Cross-Attention%2520help%2520Visual%2520Prompt%2520Tuning%2520adapt%2520visual%2520task%26entry.906535625%3DLingyun%2520Huang%2520and%2520Jianxu%2520Mao%2520and%2520Yaonan%2520Wang%2520and%2520Junfei%2520Yi%2520and%2520Ziming%2520Tao%26entry.1292438233%3D%2520%2520In%2520recent%2520years%252C%2520the%2520rapid%2520expansion%2520of%2520model%2520sizes%2520has%2520led%2520to%2520large-scale%250Apre-trained%2520models%2520demonstrating%2520remarkable%2520capabilities.%2520Consequently%252C%2520there%250Ahas%2520been%2520a%2520trend%2520towards%2520increasing%2520the%2520scale%2520of%2520models.%2520However%252C%2520this%2520trend%250Aintroduces%2520significant%2520challenges%252C%2520including%2520substantial%2520computational%2520costs%2520of%250Atraining%2520and%2520transfer%2520to%2520downstream%2520tasks.%2520To%2520address%2520these%2520issues%252C%250AParameter-Efficient%2520Fine-Tuning%2520%2528PEFT%2529%2520methods%2520have%2520been%2520introduced.%2520These%250Amethods%2520optimize%2520large-scale%2520pre-trained%2520models%2520for%2520specific%2520tasks%2520by%250Afine-tuning%2520a%2520select%2520group%2520of%2520parameters.%2520Among%2520these%2520PEFT%2520methods%252C%250Aadapter-based%2520and%2520prompt-based%2520methods%2520are%2520the%2520primary%2520techniques.%250ASpecifically%252C%2520in%2520the%2520field%2520of%2520visual%2520fine-tuning%252C%2520adapters%2520gain%2520prominence%2520over%250Aprompts%2520because%2520of%2520the%2520latter%2527s%2520relatively%2520weaker%2520performance%2520and%2520efficiency.%250AUnder%2520the%2520circumstances%252C%2520we%2520refine%2520the%2520widely-used%2520Visual%2520Prompt%2520Tuning%2520%2528VPT%2529%250Amethod%252C%2520proposing%2520Cross%2520Visual%2520Prompt%2520Tuning%2520%2528CVPT%2529.%2520CVPT%2520calculates%250Across-attention%2520between%2520the%2520prompt%2520tokens%2520and%2520the%2520embedded%2520tokens%252C%2520which%2520allows%250Aus%2520to%2520compute%2520the%2520semantic%2520relationship%2520between%2520them%2520and%2520conduct%2520the%250Afine-tuning%2520of%2520models%2520exactly%2520to%2520adapt%2520visual%2520tasks%2520better.%2520Furthermore%252C%2520we%250Aintroduce%2520the%2520weight-sharing%2520mechanism%2520to%2520initialize%2520the%2520parameters%2520of%250Across-attention%252C%2520which%2520avoids%2520massive%2520learnable%2520parameters%2520from%2520cross-attention%250Aand%2520enhances%2520the%2520representative%2520capability%2520of%2520cross-attention.%2520We%2520conduct%250Acomprehensive%2520testing%2520across%252025%2520datasets%2520and%2520the%2520result%2520indicates%2520that%2520CVPT%250Asignificantly%2520improves%2520VPT%2527s%2520performance%2520and%2520efficiency%2520in%2520visual%2520tasks.%2520For%250Aexample%252C%2520on%2520the%2520VTAB-1K%2520benchmark%252C%2520CVPT%2520outperforms%2520VPT%2520over%25204%2525%2520in%2520average%250Aaccuracy%252C%2520rivaling%2520the%2520advanced%2520adapter-based%2520methods%2520in%2520performance%2520and%250Aefficiency.%2520Our%2520experiments%2520confirm%2520that%2520prompt-based%2520methods%2520can%2520achieve%250Aexceptional%2520results%2520in%2520visual%2520fine-tuning.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2408.14961v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=CVPT%3A%20Cross-Attention%20help%20Visual%20Prompt%20Tuning%20adapt%20visual%20task&entry.906535625=Lingyun%20Huang%20and%20Jianxu%20Mao%20and%20Yaonan%20Wang%20and%20Junfei%20Yi%20and%20Ziming%20Tao&entry.1292438233=%20%20In%20recent%20years%2C%20the%20rapid%20expansion%20of%20model%20sizes%20has%20led%20to%20large-scale%0Apre-trained%20models%20demonstrating%20remarkable%20capabilities.%20Consequently%2C%20there%0Ahas%20been%20a%20trend%20towards%20increasing%20the%20scale%20of%20models.%20However%2C%20this%20trend%0Aintroduces%20significant%20challenges%2C%20including%20substantial%20computational%20costs%20of%0Atraining%20and%20transfer%20to%20downstream%20tasks.%20To%20address%20these%20issues%2C%0AParameter-Efficient%20Fine-Tuning%20%28PEFT%29%20methods%20have%20been%20introduced.%20These%0Amethods%20optimize%20large-scale%20pre-trained%20models%20for%20specific%20tasks%20by%0Afine-tuning%20a%20select%20group%20of%20parameters.%20Among%20these%20PEFT%20methods%2C%0Aadapter-based%20and%20prompt-based%20methods%20are%20the%20primary%20techniques.%0ASpecifically%2C%20in%20the%20field%20of%20visual%20fine-tuning%2C%20adapters%20gain%20prominence%20over%0Aprompts%20because%20of%20the%20latter%27s%20relatively%20weaker%20performance%20and%20efficiency.%0AUnder%20the%20circumstances%2C%20we%20refine%20the%20widely-used%20Visual%20Prompt%20Tuning%20%28VPT%29%0Amethod%2C%20proposing%20Cross%20Visual%20Prompt%20Tuning%20%28CVPT%29.%20CVPT%20calculates%0Across-attention%20between%20the%20prompt%20tokens%20and%20the%20embedded%20tokens%2C%20which%20allows%0Aus%20to%20compute%20the%20semantic%20relationship%20between%20them%20and%20conduct%20the%0Afine-tuning%20of%20models%20exactly%20to%20adapt%20visual%20tasks%20better.%20Furthermore%2C%20we%0Aintroduce%20the%20weight-sharing%20mechanism%20to%20initialize%20the%20parameters%20of%0Across-attention%2C%20which%20avoids%20massive%20learnable%20parameters%20from%20cross-attention%0Aand%20enhances%20the%20representative%20capability%20of%20cross-attention.%20We%20conduct%0Acomprehensive%20testing%20across%2025%20datasets%20and%20the%20result%20indicates%20that%20CVPT%0Asignificantly%20improves%20VPT%27s%20performance%20and%20efficiency%20in%20visual%20tasks.%20For%0Aexample%2C%20on%20the%20VTAB-1K%20benchmark%2C%20CVPT%20outperforms%20VPT%20over%204%25%20in%20average%0Aaccuracy%2C%20rivaling%20the%20advanced%20adapter-based%20methods%20in%20performance%20and%0Aefficiency.%20Our%20experiments%20confirm%20that%20prompt-based%20methods%20can%20achieve%0Aexceptional%20results%20in%20visual%20fine-tuning.%0A&entry.1838667208=http%3A//arxiv.org/abs/2408.14961v1&entry.124074799=Read"},
{"title": "The virtual CAT: A tool for algorithmic thinking assessment in Swiss\n  compulsory education", "author": "Giorgia Adorni and Alberto Piatti", "abstract": "  In today's digital era, holding algorithmic thinking (AT) skills is crucial,\nnot only in computer science-related fields. These abilities enable individuals\nto break down complex problems into more manageable steps and create a sequence\nof actions to solve them. To address the increasing demand for AT assessments\nin educational settings and the limitations of current methods, this paper\nintroduces the virtual Cross Array Task (CAT), a digital adaptation of an\nunplugged assessment activity designed to evaluate algorithmic skills in Swiss\ncompulsory education. This tool offers scalable and automated assessment,\nreducing human involvement and mitigating potential data collection errors. The\nplatform features gesture-based and visual block-based programming interfaces,\nensuring its usability for diverse learners, further supported by multilingual\ncapabilities. To evaluate the virtual CAT platform, we conducted a pilot\nevaluation in Switzerland involving a heterogeneous group of students. The\nfindings show the platform's usability, proficiency and suitability for\nassessing AT skills among students of diverse ages, development stages, and\neducational backgrounds, as well as the feasibility of large-scale data\ncollection.\n", "link": "http://arxiv.org/abs/2408.01263v2", "date": "2024-08-27", "relevancy": 2.0053, "topK": [{"title": "CAT3D: Create Anything in 3D with Multi-View Diffusion Models", "link": "http://arxiv.org/abs/2405.10314v1", "similarity": 0.5088}, {"title": "CAT3D: Create Anything in 3D with Multi-View Diffusion Models", "link": "http://arxiv.org/abs/2405.10314v1", "similarity": 0.5088}, {"title": "RoboEXP: Action-Conditioned Scene Graph via Interactive Exploration for\n  Robotic Manipulation", "link": "http://arxiv.org/abs/2402.15487v1", "similarity": 0.4641}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20The%20virtual%20CAT%3A%20A%20tool%20for%20algorithmic%20thinking%20assessment%20in%20Swiss%0A%20%20compulsory%20education&body=Title%3A%20The%20virtual%20CAT%3A%20A%20tool%20for%20algorithmic%20thinking%20assessment%20in%20Swiss%0A%20%20compulsory%20education%0AAuthor%3A%20Giorgia%20Adorni%20and%20Alberto%20Piatti%0AAbstract%3A%20%20%20In%20today%27s%20digital%20era%2C%20holding%20algorithmic%20thinking%20%28AT%29%20skills%20is%20crucial%2C%0Anot%20only%20in%20computer%20science-related%20fields.%20These%20abilities%20enable%20individuals%0Ato%20break%20down%20complex%20problems%20into%20more%20manageable%20steps%20and%20create%20a%20sequence%0Aof%20actions%20to%20solve%20them.%20To%20address%20the%20increasing%20demand%20for%20AT%20assessments%0Ain%20educational%20settings%20and%20the%20limitations%20of%20current%20methods%2C%20this%20paper%0Aintroduces%20the%20virtual%20Cross%20Array%20Task%20%28CAT%29%2C%20a%20digital%20adaptation%20of%20an%0Aunplugged%20assessment%20activity%20designed%20to%20evaluate%20algorithmic%20skills%20in%20Swiss%0Acompulsory%20education.%20This%20tool%20offers%20scalable%20and%20automated%20assessment%2C%0Areducing%20human%20involvement%20and%20mitigating%20potential%20data%20collection%20errors.%20The%0Aplatform%20features%20gesture-based%20and%20visual%20block-based%20programming%20interfaces%2C%0Aensuring%20its%20usability%20for%20diverse%20learners%2C%20further%20supported%20by%20multilingual%0Acapabilities.%20To%20evaluate%20the%20virtual%20CAT%20platform%2C%20we%20conducted%20a%20pilot%0Aevaluation%20in%20Switzerland%20involving%20a%20heterogeneous%20group%20of%20students.%20The%0Afindings%20show%20the%20platform%27s%20usability%2C%20proficiency%20and%20suitability%20for%0Aassessing%20AT%20skills%20among%20students%20of%20diverse%20ages%2C%20development%20stages%2C%20and%0Aeducational%20backgrounds%2C%20as%20well%20as%20the%20feasibility%20of%20large-scale%20data%0Acollection.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2408.01263v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DThe%2520virtual%2520CAT%253A%2520A%2520tool%2520for%2520algorithmic%2520thinking%2520assessment%2520in%2520Swiss%250A%2520%2520compulsory%2520education%26entry.906535625%3DGiorgia%2520Adorni%2520and%2520Alberto%2520Piatti%26entry.1292438233%3D%2520%2520In%2520today%2527s%2520digital%2520era%252C%2520holding%2520algorithmic%2520thinking%2520%2528AT%2529%2520skills%2520is%2520crucial%252C%250Anot%2520only%2520in%2520computer%2520science-related%2520fields.%2520These%2520abilities%2520enable%2520individuals%250Ato%2520break%2520down%2520complex%2520problems%2520into%2520more%2520manageable%2520steps%2520and%2520create%2520a%2520sequence%250Aof%2520actions%2520to%2520solve%2520them.%2520To%2520address%2520the%2520increasing%2520demand%2520for%2520AT%2520assessments%250Ain%2520educational%2520settings%2520and%2520the%2520limitations%2520of%2520current%2520methods%252C%2520this%2520paper%250Aintroduces%2520the%2520virtual%2520Cross%2520Array%2520Task%2520%2528CAT%2529%252C%2520a%2520digital%2520adaptation%2520of%2520an%250Aunplugged%2520assessment%2520activity%2520designed%2520to%2520evaluate%2520algorithmic%2520skills%2520in%2520Swiss%250Acompulsory%2520education.%2520This%2520tool%2520offers%2520scalable%2520and%2520automated%2520assessment%252C%250Areducing%2520human%2520involvement%2520and%2520mitigating%2520potential%2520data%2520collection%2520errors.%2520The%250Aplatform%2520features%2520gesture-based%2520and%2520visual%2520block-based%2520programming%2520interfaces%252C%250Aensuring%2520its%2520usability%2520for%2520diverse%2520learners%252C%2520further%2520supported%2520by%2520multilingual%250Acapabilities.%2520To%2520evaluate%2520the%2520virtual%2520CAT%2520platform%252C%2520we%2520conducted%2520a%2520pilot%250Aevaluation%2520in%2520Switzerland%2520involving%2520a%2520heterogeneous%2520group%2520of%2520students.%2520The%250Afindings%2520show%2520the%2520platform%2527s%2520usability%252C%2520proficiency%2520and%2520suitability%2520for%250Aassessing%2520AT%2520skills%2520among%2520students%2520of%2520diverse%2520ages%252C%2520development%2520stages%252C%2520and%250Aeducational%2520backgrounds%252C%2520as%2520well%2520as%2520the%2520feasibility%2520of%2520large-scale%2520data%250Acollection.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2408.01263v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=The%20virtual%20CAT%3A%20A%20tool%20for%20algorithmic%20thinking%20assessment%20in%20Swiss%0A%20%20compulsory%20education&entry.906535625=Giorgia%20Adorni%20and%20Alberto%20Piatti&entry.1292438233=%20%20In%20today%27s%20digital%20era%2C%20holding%20algorithmic%20thinking%20%28AT%29%20skills%20is%20crucial%2C%0Anot%20only%20in%20computer%20science-related%20fields.%20These%20abilities%20enable%20individuals%0Ato%20break%20down%20complex%20problems%20into%20more%20manageable%20steps%20and%20create%20a%20sequence%0Aof%20actions%20to%20solve%20them.%20To%20address%20the%20increasing%20demand%20for%20AT%20assessments%0Ain%20educational%20settings%20and%20the%20limitations%20of%20current%20methods%2C%20this%20paper%0Aintroduces%20the%20virtual%20Cross%20Array%20Task%20%28CAT%29%2C%20a%20digital%20adaptation%20of%20an%0Aunplugged%20assessment%20activity%20designed%20to%20evaluate%20algorithmic%20skills%20in%20Swiss%0Acompulsory%20education.%20This%20tool%20offers%20scalable%20and%20automated%20assessment%2C%0Areducing%20human%20involvement%20and%20mitigating%20potential%20data%20collection%20errors.%20The%0Aplatform%20features%20gesture-based%20and%20visual%20block-based%20programming%20interfaces%2C%0Aensuring%20its%20usability%20for%20diverse%20learners%2C%20further%20supported%20by%20multilingual%0Acapabilities.%20To%20evaluate%20the%20virtual%20CAT%20platform%2C%20we%20conducted%20a%20pilot%0Aevaluation%20in%20Switzerland%20involving%20a%20heterogeneous%20group%20of%20students.%20The%0Afindings%20show%20the%20platform%27s%20usability%2C%20proficiency%20and%20suitability%20for%0Aassessing%20AT%20skills%20among%20students%20of%20diverse%20ages%2C%20development%20stages%2C%20and%0Aeducational%20backgrounds%2C%20as%20well%20as%20the%20feasibility%20of%20large-scale%20data%0Acollection.%0A&entry.1838667208=http%3A//arxiv.org/abs/2408.01263v2&entry.124074799=Read"},
{"title": "Targetin the partition function of chemically disordered materials with\n  a generative approach based on inverse variational autoencoders", "author": "Maciej J. Karcz and Luca Messina and Eiji Kawasaki and Emeric Bourasseau", "abstract": "  Computing atomic-scale properties of chemically disordered materials requires\nan efficient exploration of their vast configuration space. Traditional\napproaches such as Monte Carlo or Special Quasirandom Structures either entail\nsampling an excessive amount of configurations or do not ensure that the\nconfiguration space has been properly covered. In this work, we propose a novel\napproach where generative machine learning is used to yield a representative\nset of configurations for accurate property evaluation and provide accurate\nestimations of atomic-scale properties with minimal computational cost. Our\nmethod employs a specific type of variational autoencoder with inverse roles\nfor the encoder and decoder, enabling the application of an unsupervised active\nlearning scheme that does not require any initial training database. The model\niteratively generates configuration batches, whose properties are computed with\nconventional atomic-scale methods. These results are then fed back into the\nmodel to estimate the partition function, repeating the process until\nconvergence. We illustrate our approach by computing point-defect formation\nenergies and concentrations in (U, Pu)O2 mixed-oxide fuels. In addition, the ML\nmodel provides valuable insights into the physical factors influencing the\ntarget property. Our method is generally applicable to explore other\nproperties, such as atomic-scale diffusion coefficients, in ideally or\nnon-ideally disordered materials like high-entropy alloys.\n", "link": "http://arxiv.org/abs/2408.14928v1", "date": "2024-08-27", "relevancy": 1.9906, "topK": [{"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.536}, {"title": "Genie: Generative Interactive Environments", "link": "http://arxiv.org/abs/2402.15391v1", "similarity": 0.4916}, {"title": "UGG: Unified Generative Grasping", "link": "https://arxiv.org/abs/2311.16917", "similarity": 0.4884}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Targetin%20the%20partition%20function%20of%20chemically%20disordered%20materials%20with%0A%20%20a%20generative%20approach%20based%20on%20inverse%20variational%20autoencoders&body=Title%3A%20Targetin%20the%20partition%20function%20of%20chemically%20disordered%20materials%20with%0A%20%20a%20generative%20approach%20based%20on%20inverse%20variational%20autoencoders%0AAuthor%3A%20Maciej%20J.%20Karcz%20and%20Luca%20Messina%20and%20Eiji%20Kawasaki%20and%20Emeric%20Bourasseau%0AAbstract%3A%20%20%20Computing%20atomic-scale%20properties%20of%20chemically%20disordered%20materials%20requires%0Aan%20efficient%20exploration%20of%20their%20vast%20configuration%20space.%20Traditional%0Aapproaches%20such%20as%20Monte%20Carlo%20or%20Special%20Quasirandom%20Structures%20either%20entail%0Asampling%20an%20excessive%20amount%20of%20configurations%20or%20do%20not%20ensure%20that%20the%0Aconfiguration%20space%20has%20been%20properly%20covered.%20In%20this%20work%2C%20we%20propose%20a%20novel%0Aapproach%20where%20generative%20machine%20learning%20is%20used%20to%20yield%20a%20representative%0Aset%20of%20configurations%20for%20accurate%20property%20evaluation%20and%20provide%20accurate%0Aestimations%20of%20atomic-scale%20properties%20with%20minimal%20computational%20cost.%20Our%0Amethod%20employs%20a%20specific%20type%20of%20variational%20autoencoder%20with%20inverse%20roles%0Afor%20the%20encoder%20and%20decoder%2C%20enabling%20the%20application%20of%20an%20unsupervised%20active%0Alearning%20scheme%20that%20does%20not%20require%20any%20initial%20training%20database.%20The%20model%0Aiteratively%20generates%20configuration%20batches%2C%20whose%20properties%20are%20computed%20with%0Aconventional%20atomic-scale%20methods.%20These%20results%20are%20then%20fed%20back%20into%20the%0Amodel%20to%20estimate%20the%20partition%20function%2C%20repeating%20the%20process%20until%0Aconvergence.%20We%20illustrate%20our%20approach%20by%20computing%20point-defect%20formation%0Aenergies%20and%20concentrations%20in%20%28U%2C%20Pu%29O2%20mixed-oxide%20fuels.%20In%20addition%2C%20the%20ML%0Amodel%20provides%20valuable%20insights%20into%20the%20physical%20factors%20influencing%20the%0Atarget%20property.%20Our%20method%20is%20generally%20applicable%20to%20explore%20other%0Aproperties%2C%20such%20as%20atomic-scale%20diffusion%20coefficients%2C%20in%20ideally%20or%0Anon-ideally%20disordered%20materials%20like%20high-entropy%20alloys.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2408.14928v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DTargetin%2520the%2520partition%2520function%2520of%2520chemically%2520disordered%2520materials%2520with%250A%2520%2520a%2520generative%2520approach%2520based%2520on%2520inverse%2520variational%2520autoencoders%26entry.906535625%3DMaciej%2520J.%2520Karcz%2520and%2520Luca%2520Messina%2520and%2520Eiji%2520Kawasaki%2520and%2520Emeric%2520Bourasseau%26entry.1292438233%3D%2520%2520Computing%2520atomic-scale%2520properties%2520of%2520chemically%2520disordered%2520materials%2520requires%250Aan%2520efficient%2520exploration%2520of%2520their%2520vast%2520configuration%2520space.%2520Traditional%250Aapproaches%2520such%2520as%2520Monte%2520Carlo%2520or%2520Special%2520Quasirandom%2520Structures%2520either%2520entail%250Asampling%2520an%2520excessive%2520amount%2520of%2520configurations%2520or%2520do%2520not%2520ensure%2520that%2520the%250Aconfiguration%2520space%2520has%2520been%2520properly%2520covered.%2520In%2520this%2520work%252C%2520we%2520propose%2520a%2520novel%250Aapproach%2520where%2520generative%2520machine%2520learning%2520is%2520used%2520to%2520yield%2520a%2520representative%250Aset%2520of%2520configurations%2520for%2520accurate%2520property%2520evaluation%2520and%2520provide%2520accurate%250Aestimations%2520of%2520atomic-scale%2520properties%2520with%2520minimal%2520computational%2520cost.%2520Our%250Amethod%2520employs%2520a%2520specific%2520type%2520of%2520variational%2520autoencoder%2520with%2520inverse%2520roles%250Afor%2520the%2520encoder%2520and%2520decoder%252C%2520enabling%2520the%2520application%2520of%2520an%2520unsupervised%2520active%250Alearning%2520scheme%2520that%2520does%2520not%2520require%2520any%2520initial%2520training%2520database.%2520The%2520model%250Aiteratively%2520generates%2520configuration%2520batches%252C%2520whose%2520properties%2520are%2520computed%2520with%250Aconventional%2520atomic-scale%2520methods.%2520These%2520results%2520are%2520then%2520fed%2520back%2520into%2520the%250Amodel%2520to%2520estimate%2520the%2520partition%2520function%252C%2520repeating%2520the%2520process%2520until%250Aconvergence.%2520We%2520illustrate%2520our%2520approach%2520by%2520computing%2520point-defect%2520formation%250Aenergies%2520and%2520concentrations%2520in%2520%2528U%252C%2520Pu%2529O2%2520mixed-oxide%2520fuels.%2520In%2520addition%252C%2520the%2520ML%250Amodel%2520provides%2520valuable%2520insights%2520into%2520the%2520physical%2520factors%2520influencing%2520the%250Atarget%2520property.%2520Our%2520method%2520is%2520generally%2520applicable%2520to%2520explore%2520other%250Aproperties%252C%2520such%2520as%2520atomic-scale%2520diffusion%2520coefficients%252C%2520in%2520ideally%2520or%250Anon-ideally%2520disordered%2520materials%2520like%2520high-entropy%2520alloys.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2408.14928v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Targetin%20the%20partition%20function%20of%20chemically%20disordered%20materials%20with%0A%20%20a%20generative%20approach%20based%20on%20inverse%20variational%20autoencoders&entry.906535625=Maciej%20J.%20Karcz%20and%20Luca%20Messina%20and%20Eiji%20Kawasaki%20and%20Emeric%20Bourasseau&entry.1292438233=%20%20Computing%20atomic-scale%20properties%20of%20chemically%20disordered%20materials%20requires%0Aan%20efficient%20exploration%20of%20their%20vast%20configuration%20space.%20Traditional%0Aapproaches%20such%20as%20Monte%20Carlo%20or%20Special%20Quasirandom%20Structures%20either%20entail%0Asampling%20an%20excessive%20amount%20of%20configurations%20or%20do%20not%20ensure%20that%20the%0Aconfiguration%20space%20has%20been%20properly%20covered.%20In%20this%20work%2C%20we%20propose%20a%20novel%0Aapproach%20where%20generative%20machine%20learning%20is%20used%20to%20yield%20a%20representative%0Aset%20of%20configurations%20for%20accurate%20property%20evaluation%20and%20provide%20accurate%0Aestimations%20of%20atomic-scale%20properties%20with%20minimal%20computational%20cost.%20Our%0Amethod%20employs%20a%20specific%20type%20of%20variational%20autoencoder%20with%20inverse%20roles%0Afor%20the%20encoder%20and%20decoder%2C%20enabling%20the%20application%20of%20an%20unsupervised%20active%0Alearning%20scheme%20that%20does%20not%20require%20any%20initial%20training%20database.%20The%20model%0Aiteratively%20generates%20configuration%20batches%2C%20whose%20properties%20are%20computed%20with%0Aconventional%20atomic-scale%20methods.%20These%20results%20are%20then%20fed%20back%20into%20the%0Amodel%20to%20estimate%20the%20partition%20function%2C%20repeating%20the%20process%20until%0Aconvergence.%20We%20illustrate%20our%20approach%20by%20computing%20point-defect%20formation%0Aenergies%20and%20concentrations%20in%20%28U%2C%20Pu%29O2%20mixed-oxide%20fuels.%20In%20addition%2C%20the%20ML%0Amodel%20provides%20valuable%20insights%20into%20the%20physical%20factors%20influencing%20the%0Atarget%20property.%20Our%20method%20is%20generally%20applicable%20to%20explore%20other%0Aproperties%2C%20such%20as%20atomic-scale%20diffusion%20coefficients%2C%20in%20ideally%20or%0Anon-ideally%20disordered%20materials%20like%20high-entropy%20alloys.%0A&entry.1838667208=http%3A//arxiv.org/abs/2408.14928v1&entry.124074799=Read"},
{"title": "Empowering Sign Language Communication: Integrating Sentiment and\n  Semantics for Facial Expression Synthesis", "author": "Rafael Azevedo and Thiago Coutinho and Jo\u00e3o Ferreira and Thiago Gomes and Erickson Nascimento", "abstract": "  Translating written sentences from oral languages to a sequence of manual and\nnon-manual gestures plays a crucial role in building a more inclusive society\nfor deaf and hard-of-hearing people. Facial expressions (non-manual), in\nparticular, are responsible for encoding the grammar of the sentence to be\nspoken, applying punctuation, pronouns, or emphasizing signs. These non-manual\ngestures are closely related to the semantics of the sentence being spoken and\nalso to the utterance of the speaker's emotions. However, most Sign Language\nProduction (SLP) approaches are centered on synthesizing manual gestures and do\nnot focus on modeling the speakers expression. This paper introduces a new\nmethod focused in synthesizing facial expressions for sign language. Our goal\nis to improve sign language production by integrating sentiment information in\nfacial expression generation. The approach leverages a sentence sentiment and\nsemantic features to sample from a meaningful representation space, integrating\nthe bias of the non-manual components into the sign language production\nprocess. To evaluate our method, we extend the Frechet Gesture Distance (FGD)\nand propose a new metric called Frechet Expression Distance (FED) and apply an\nextensive set of metrics to assess the quality of specific regions of the face.\nThe experimental results showed that our method achieved state of the art,\nbeing superior to the competitors on How2Sign and PHOENIX14T datasets.\nMoreover, our architecture is based on a carefully designed graph pyramid that\nmakes it simpler, easier to train, and capable of leveraging emotions to\nproduce facial expressions.\n", "link": "http://arxiv.org/abs/2408.15159v1", "date": "2024-08-27", "relevancy": 1.9857, "topK": [{"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.5042}, {"title": "3D Gaussian Blendshapes for Head Avatar Animation", "link": "http://arxiv.org/abs/2404.19398v2", "similarity": 0.4949}, {"title": "3D Gaussian Blendshapes for Head Avatar Animation", "link": "http://arxiv.org/abs/2404.19398v2", "similarity": 0.4949}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Empowering%20Sign%20Language%20Communication%3A%20Integrating%20Sentiment%20and%0A%20%20Semantics%20for%20Facial%20Expression%20Synthesis&body=Title%3A%20Empowering%20Sign%20Language%20Communication%3A%20Integrating%20Sentiment%20and%0A%20%20Semantics%20for%20Facial%20Expression%20Synthesis%0AAuthor%3A%20Rafael%20Azevedo%20and%20Thiago%20Coutinho%20and%20Jo%C3%A3o%20Ferreira%20and%20Thiago%20Gomes%20and%20Erickson%20Nascimento%0AAbstract%3A%20%20%20Translating%20written%20sentences%20from%20oral%20languages%20to%20a%20sequence%20of%20manual%20and%0Anon-manual%20gestures%20plays%20a%20crucial%20role%20in%20building%20a%20more%20inclusive%20society%0Afor%20deaf%20and%20hard-of-hearing%20people.%20Facial%20expressions%20%28non-manual%29%2C%20in%0Aparticular%2C%20are%20responsible%20for%20encoding%20the%20grammar%20of%20the%20sentence%20to%20be%0Aspoken%2C%20applying%20punctuation%2C%20pronouns%2C%20or%20emphasizing%20signs.%20These%20non-manual%0Agestures%20are%20closely%20related%20to%20the%20semantics%20of%20the%20sentence%20being%20spoken%20and%0Aalso%20to%20the%20utterance%20of%20the%20speaker%27s%20emotions.%20However%2C%20most%20Sign%20Language%0AProduction%20%28SLP%29%20approaches%20are%20centered%20on%20synthesizing%20manual%20gestures%20and%20do%0Anot%20focus%20on%20modeling%20the%20speakers%20expression.%20This%20paper%20introduces%20a%20new%0Amethod%20focused%20in%20synthesizing%20facial%20expressions%20for%20sign%20language.%20Our%20goal%0Ais%20to%20improve%20sign%20language%20production%20by%20integrating%20sentiment%20information%20in%0Afacial%20expression%20generation.%20The%20approach%20leverages%20a%20sentence%20sentiment%20and%0Asemantic%20features%20to%20sample%20from%20a%20meaningful%20representation%20space%2C%20integrating%0Athe%20bias%20of%20the%20non-manual%20components%20into%20the%20sign%20language%20production%0Aprocess.%20To%20evaluate%20our%20method%2C%20we%20extend%20the%20Frechet%20Gesture%20Distance%20%28FGD%29%0Aand%20propose%20a%20new%20metric%20called%20Frechet%20Expression%20Distance%20%28FED%29%20and%20apply%20an%0Aextensive%20set%20of%20metrics%20to%20assess%20the%20quality%20of%20specific%20regions%20of%20the%20face.%0AThe%20experimental%20results%20showed%20that%20our%20method%20achieved%20state%20of%20the%20art%2C%0Abeing%20superior%20to%20the%20competitors%20on%20How2Sign%20and%20PHOENIX14T%20datasets.%0AMoreover%2C%20our%20architecture%20is%20based%20on%20a%20carefully%20designed%20graph%20pyramid%20that%0Amakes%20it%20simpler%2C%20easier%20to%20train%2C%20and%20capable%20of%20leveraging%20emotions%20to%0Aproduce%20facial%20expressions.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2408.15159v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DEmpowering%2520Sign%2520Language%2520Communication%253A%2520Integrating%2520Sentiment%2520and%250A%2520%2520Semantics%2520for%2520Facial%2520Expression%2520Synthesis%26entry.906535625%3DRafael%2520Azevedo%2520and%2520Thiago%2520Coutinho%2520and%2520Jo%25C3%25A3o%2520Ferreira%2520and%2520Thiago%2520Gomes%2520and%2520Erickson%2520Nascimento%26entry.1292438233%3D%2520%2520Translating%2520written%2520sentences%2520from%2520oral%2520languages%2520to%2520a%2520sequence%2520of%2520manual%2520and%250Anon-manual%2520gestures%2520plays%2520a%2520crucial%2520role%2520in%2520building%2520a%2520more%2520inclusive%2520society%250Afor%2520deaf%2520and%2520hard-of-hearing%2520people.%2520Facial%2520expressions%2520%2528non-manual%2529%252C%2520in%250Aparticular%252C%2520are%2520responsible%2520for%2520encoding%2520the%2520grammar%2520of%2520the%2520sentence%2520to%2520be%250Aspoken%252C%2520applying%2520punctuation%252C%2520pronouns%252C%2520or%2520emphasizing%2520signs.%2520These%2520non-manual%250Agestures%2520are%2520closely%2520related%2520to%2520the%2520semantics%2520of%2520the%2520sentence%2520being%2520spoken%2520and%250Aalso%2520to%2520the%2520utterance%2520of%2520the%2520speaker%2527s%2520emotions.%2520However%252C%2520most%2520Sign%2520Language%250AProduction%2520%2528SLP%2529%2520approaches%2520are%2520centered%2520on%2520synthesizing%2520manual%2520gestures%2520and%2520do%250Anot%2520focus%2520on%2520modeling%2520the%2520speakers%2520expression.%2520This%2520paper%2520introduces%2520a%2520new%250Amethod%2520focused%2520in%2520synthesizing%2520facial%2520expressions%2520for%2520sign%2520language.%2520Our%2520goal%250Ais%2520to%2520improve%2520sign%2520language%2520production%2520by%2520integrating%2520sentiment%2520information%2520in%250Afacial%2520expression%2520generation.%2520The%2520approach%2520leverages%2520a%2520sentence%2520sentiment%2520and%250Asemantic%2520features%2520to%2520sample%2520from%2520a%2520meaningful%2520representation%2520space%252C%2520integrating%250Athe%2520bias%2520of%2520the%2520non-manual%2520components%2520into%2520the%2520sign%2520language%2520production%250Aprocess.%2520To%2520evaluate%2520our%2520method%252C%2520we%2520extend%2520the%2520Frechet%2520Gesture%2520Distance%2520%2528FGD%2529%250Aand%2520propose%2520a%2520new%2520metric%2520called%2520Frechet%2520Expression%2520Distance%2520%2528FED%2529%2520and%2520apply%2520an%250Aextensive%2520set%2520of%2520metrics%2520to%2520assess%2520the%2520quality%2520of%2520specific%2520regions%2520of%2520the%2520face.%250AThe%2520experimental%2520results%2520showed%2520that%2520our%2520method%2520achieved%2520state%2520of%2520the%2520art%252C%250Abeing%2520superior%2520to%2520the%2520competitors%2520on%2520How2Sign%2520and%2520PHOENIX14T%2520datasets.%250AMoreover%252C%2520our%2520architecture%2520is%2520based%2520on%2520a%2520carefully%2520designed%2520graph%2520pyramid%2520that%250Amakes%2520it%2520simpler%252C%2520easier%2520to%2520train%252C%2520and%2520capable%2520of%2520leveraging%2520emotions%2520to%250Aproduce%2520facial%2520expressions.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2408.15159v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Empowering%20Sign%20Language%20Communication%3A%20Integrating%20Sentiment%20and%0A%20%20Semantics%20for%20Facial%20Expression%20Synthesis&entry.906535625=Rafael%20Azevedo%20and%20Thiago%20Coutinho%20and%20Jo%C3%A3o%20Ferreira%20and%20Thiago%20Gomes%20and%20Erickson%20Nascimento&entry.1292438233=%20%20Translating%20written%20sentences%20from%20oral%20languages%20to%20a%20sequence%20of%20manual%20and%0Anon-manual%20gestures%20plays%20a%20crucial%20role%20in%20building%20a%20more%20inclusive%20society%0Afor%20deaf%20and%20hard-of-hearing%20people.%20Facial%20expressions%20%28non-manual%29%2C%20in%0Aparticular%2C%20are%20responsible%20for%20encoding%20the%20grammar%20of%20the%20sentence%20to%20be%0Aspoken%2C%20applying%20punctuation%2C%20pronouns%2C%20or%20emphasizing%20signs.%20These%20non-manual%0Agestures%20are%20closely%20related%20to%20the%20semantics%20of%20the%20sentence%20being%20spoken%20and%0Aalso%20to%20the%20utterance%20of%20the%20speaker%27s%20emotions.%20However%2C%20most%20Sign%20Language%0AProduction%20%28SLP%29%20approaches%20are%20centered%20on%20synthesizing%20manual%20gestures%20and%20do%0Anot%20focus%20on%20modeling%20the%20speakers%20expression.%20This%20paper%20introduces%20a%20new%0Amethod%20focused%20in%20synthesizing%20facial%20expressions%20for%20sign%20language.%20Our%20goal%0Ais%20to%20improve%20sign%20language%20production%20by%20integrating%20sentiment%20information%20in%0Afacial%20expression%20generation.%20The%20approach%20leverages%20a%20sentence%20sentiment%20and%0Asemantic%20features%20to%20sample%20from%20a%20meaningful%20representation%20space%2C%20integrating%0Athe%20bias%20of%20the%20non-manual%20components%20into%20the%20sign%20language%20production%0Aprocess.%20To%20evaluate%20our%20method%2C%20we%20extend%20the%20Frechet%20Gesture%20Distance%20%28FGD%29%0Aand%20propose%20a%20new%20metric%20called%20Frechet%20Expression%20Distance%20%28FED%29%20and%20apply%20an%0Aextensive%20set%20of%20metrics%20to%20assess%20the%20quality%20of%20specific%20regions%20of%20the%20face.%0AThe%20experimental%20results%20showed%20that%20our%20method%20achieved%20state%20of%20the%20art%2C%0Abeing%20superior%20to%20the%20competitors%20on%20How2Sign%20and%20PHOENIX14T%20datasets.%0AMoreover%2C%20our%20architecture%20is%20based%20on%20a%20carefully%20designed%20graph%20pyramid%20that%0Amakes%20it%20simpler%2C%20easier%20to%20train%2C%20and%20capable%20of%20leveraging%20emotions%20to%0Aproduce%20facial%20expressions.%0A&entry.1838667208=http%3A//arxiv.org/abs/2408.15159v1&entry.124074799=Read"},
{"title": "Low-Budget Simulation-Based Inference with Bayesian Neural Networks", "author": "Arnaud Delaunoy and Maxence de la Brassinne Bonardeaux and Siddharth Mishra-Sharma and Gilles Louppe", "abstract": "  Simulation-based inference methods have been shown to be inaccurate in the\ndata-poor regime, when training simulations are limited or expensive. Under\nthese circumstances, the inference network is particularly prone to\noverfitting, and using it without accounting for the computational uncertainty\narising from the lack of identifiability of the network weights can lead to\nunreliable results. To address this issue, we propose using Bayesian neural\nnetworks in low-budget simulation-based inference, thereby explicitly\naccounting for the computational uncertainty of the posterior approximation. We\ndesign a family of Bayesian neural network priors that are tailored for\ninference and show that they lead to well-calibrated posteriors on tested\nbenchmarks, even when as few as $O(10)$ simulations are available. This opens\nup the possibility of performing reliable simulation-based inference using very\nexpensive simulators, as we demonstrate on a problem from the field of\ncosmology where single simulations are computationally expensive. We show that\nBayesian neural networks produce informative and well-calibrated posterior\nestimates with only a few hundred simulations.\n", "link": "http://arxiv.org/abs/2408.15136v1", "date": "2024-08-27", "relevancy": 1.9827, "topK": [{"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.5621}, {"title": "Boosted dynamic neural networks", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/26302", "similarity": 0.5079}, {"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.4569}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Low-Budget%20Simulation-Based%20Inference%20with%20Bayesian%20Neural%20Networks&body=Title%3A%20Low-Budget%20Simulation-Based%20Inference%20with%20Bayesian%20Neural%20Networks%0AAuthor%3A%20Arnaud%20Delaunoy%20and%20Maxence%20de%20la%20Brassinne%20Bonardeaux%20and%20Siddharth%20Mishra-Sharma%20and%20Gilles%20Louppe%0AAbstract%3A%20%20%20Simulation-based%20inference%20methods%20have%20been%20shown%20to%20be%20inaccurate%20in%20the%0Adata-poor%20regime%2C%20when%20training%20simulations%20are%20limited%20or%20expensive.%20Under%0Athese%20circumstances%2C%20the%20inference%20network%20is%20particularly%20prone%20to%0Aoverfitting%2C%20and%20using%20it%20without%20accounting%20for%20the%20computational%20uncertainty%0Aarising%20from%20the%20lack%20of%20identifiability%20of%20the%20network%20weights%20can%20lead%20to%0Aunreliable%20results.%20To%20address%20this%20issue%2C%20we%20propose%20using%20Bayesian%20neural%0Anetworks%20in%20low-budget%20simulation-based%20inference%2C%20thereby%20explicitly%0Aaccounting%20for%20the%20computational%20uncertainty%20of%20the%20posterior%20approximation.%20We%0Adesign%20a%20family%20of%20Bayesian%20neural%20network%20priors%20that%20are%20tailored%20for%0Ainference%20and%20show%20that%20they%20lead%20to%20well-calibrated%20posteriors%20on%20tested%0Abenchmarks%2C%20even%20when%20as%20few%20as%20%24O%2810%29%24%20simulations%20are%20available.%20This%20opens%0Aup%20the%20possibility%20of%20performing%20reliable%20simulation-based%20inference%20using%20very%0Aexpensive%20simulators%2C%20as%20we%20demonstrate%20on%20a%20problem%20from%20the%20field%20of%0Acosmology%20where%20single%20simulations%20are%20computationally%20expensive.%20We%20show%20that%0ABayesian%20neural%20networks%20produce%20informative%20and%20well-calibrated%20posterior%0Aestimates%20with%20only%20a%20few%20hundred%20simulations.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2408.15136v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DLow-Budget%2520Simulation-Based%2520Inference%2520with%2520Bayesian%2520Neural%2520Networks%26entry.906535625%3DArnaud%2520Delaunoy%2520and%2520Maxence%2520de%2520la%2520Brassinne%2520Bonardeaux%2520and%2520Siddharth%2520Mishra-Sharma%2520and%2520Gilles%2520Louppe%26entry.1292438233%3D%2520%2520Simulation-based%2520inference%2520methods%2520have%2520been%2520shown%2520to%2520be%2520inaccurate%2520in%2520the%250Adata-poor%2520regime%252C%2520when%2520training%2520simulations%2520are%2520limited%2520or%2520expensive.%2520Under%250Athese%2520circumstances%252C%2520the%2520inference%2520network%2520is%2520particularly%2520prone%2520to%250Aoverfitting%252C%2520and%2520using%2520it%2520without%2520accounting%2520for%2520the%2520computational%2520uncertainty%250Aarising%2520from%2520the%2520lack%2520of%2520identifiability%2520of%2520the%2520network%2520weights%2520can%2520lead%2520to%250Aunreliable%2520results.%2520To%2520address%2520this%2520issue%252C%2520we%2520propose%2520using%2520Bayesian%2520neural%250Anetworks%2520in%2520low-budget%2520simulation-based%2520inference%252C%2520thereby%2520explicitly%250Aaccounting%2520for%2520the%2520computational%2520uncertainty%2520of%2520the%2520posterior%2520approximation.%2520We%250Adesign%2520a%2520family%2520of%2520Bayesian%2520neural%2520network%2520priors%2520that%2520are%2520tailored%2520for%250Ainference%2520and%2520show%2520that%2520they%2520lead%2520to%2520well-calibrated%2520posteriors%2520on%2520tested%250Abenchmarks%252C%2520even%2520when%2520as%2520few%2520as%2520%2524O%252810%2529%2524%2520simulations%2520are%2520available.%2520This%2520opens%250Aup%2520the%2520possibility%2520of%2520performing%2520reliable%2520simulation-based%2520inference%2520using%2520very%250Aexpensive%2520simulators%252C%2520as%2520we%2520demonstrate%2520on%2520a%2520problem%2520from%2520the%2520field%2520of%250Acosmology%2520where%2520single%2520simulations%2520are%2520computationally%2520expensive.%2520We%2520show%2520that%250ABayesian%2520neural%2520networks%2520produce%2520informative%2520and%2520well-calibrated%2520posterior%250Aestimates%2520with%2520only%2520a%2520few%2520hundred%2520simulations.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2408.15136v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Low-Budget%20Simulation-Based%20Inference%20with%20Bayesian%20Neural%20Networks&entry.906535625=Arnaud%20Delaunoy%20and%20Maxence%20de%20la%20Brassinne%20Bonardeaux%20and%20Siddharth%20Mishra-Sharma%20and%20Gilles%20Louppe&entry.1292438233=%20%20Simulation-based%20inference%20methods%20have%20been%20shown%20to%20be%20inaccurate%20in%20the%0Adata-poor%20regime%2C%20when%20training%20simulations%20are%20limited%20or%20expensive.%20Under%0Athese%20circumstances%2C%20the%20inference%20network%20is%20particularly%20prone%20to%0Aoverfitting%2C%20and%20using%20it%20without%20accounting%20for%20the%20computational%20uncertainty%0Aarising%20from%20the%20lack%20of%20identifiability%20of%20the%20network%20weights%20can%20lead%20to%0Aunreliable%20results.%20To%20address%20this%20issue%2C%20we%20propose%20using%20Bayesian%20neural%0Anetworks%20in%20low-budget%20simulation-based%20inference%2C%20thereby%20explicitly%0Aaccounting%20for%20the%20computational%20uncertainty%20of%20the%20posterior%20approximation.%20We%0Adesign%20a%20family%20of%20Bayesian%20neural%20network%20priors%20that%20are%20tailored%20for%0Ainference%20and%20show%20that%20they%20lead%20to%20well-calibrated%20posteriors%20on%20tested%0Abenchmarks%2C%20even%20when%20as%20few%20as%20%24O%2810%29%24%20simulations%20are%20available.%20This%20opens%0Aup%20the%20possibility%20of%20performing%20reliable%20simulation-based%20inference%20using%20very%0Aexpensive%20simulators%2C%20as%20we%20demonstrate%20on%20a%20problem%20from%20the%20field%20of%0Acosmology%20where%20single%20simulations%20are%20computationally%20expensive.%20We%20show%20that%0ABayesian%20neural%20networks%20produce%20informative%20and%20well-calibrated%20posterior%0Aestimates%20with%20only%20a%20few%20hundred%20simulations.%0A&entry.1838667208=http%3A//arxiv.org/abs/2408.15136v1&entry.124074799=Read"},
{"title": "Consistent machine learning for topology optimization with\n  microstructure-dependent neural network material models", "author": "Harikrishnan Vijayakumaran and Jonathan B. Russ and Glaucio H. Paulino and Miguel A. Bessa", "abstract": "  Additive manufacturing methods together with topology optimization have\nenabled the creation of multiscale structures with controlled spatially-varying\nmaterial microstructure. However, topology optimization or inverse design of\nsuch structures in the presence of nonlinearities remains a challenge due to\nthe expense of computational homogenization methods and the complexity of\ndifferentiably parameterizing the microstructural response. A solution to this\nchallenge lies in machine learning techniques that offer efficient,\ndifferentiable mappings between the material response and its microstructural\ndescriptors. This work presents a framework for designing multiscale\nheterogeneous structures with spatially varying microstructures by merging a\nhomogenization-based topology optimization strategy with a consistent machine\nlearning approach grounded in hyperelasticity theory. We leverage neural\narchitectures that adhere to critical physical principles such as\npolyconvexity, objectivity, material symmetry, and thermodynamic consistency to\nsupply the framework with a reliable constitutive model that is dependent on\nmaterial microstructural descriptors. Our findings highlight the potential of\nintegrating consistent machine learning models with density-based topology\noptimization for enhancing design optimization of heterogeneous hyperelastic\nstructures under finite deformations.\n", "link": "http://arxiv.org/abs/2408.13843v2", "date": "2024-08-27", "relevancy": 1.9825, "topK": [{"title": "Dynamic Gaussians Mesh: Consistent Mesh Reconstruction from Monocular\n  Videos", "link": "http://arxiv.org/abs/2404.12379v2", "similarity": 0.4999}, {"title": "DDM-NET: End-to-end learning of keypoint feature Detection, Description and Matching for 3D localization", "link": "https://arxiv.org/abs/2212.04575", "similarity": 0.4967}, {"title": "Multimodal-Conditioned Latent Diffusion Models for Fashion Image Editing", "link": "http://arxiv.org/abs/2403.14828v2", "similarity": 0.4822}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Consistent%20machine%20learning%20for%20topology%20optimization%20with%0A%20%20microstructure-dependent%20neural%20network%20material%20models&body=Title%3A%20Consistent%20machine%20learning%20for%20topology%20optimization%20with%0A%20%20microstructure-dependent%20neural%20network%20material%20models%0AAuthor%3A%20Harikrishnan%20Vijayakumaran%20and%20Jonathan%20B.%20Russ%20and%20Glaucio%20H.%20Paulino%20and%20Miguel%20A.%20Bessa%0AAbstract%3A%20%20%20Additive%20manufacturing%20methods%20together%20with%20topology%20optimization%20have%0Aenabled%20the%20creation%20of%20multiscale%20structures%20with%20controlled%20spatially-varying%0Amaterial%20microstructure.%20However%2C%20topology%20optimization%20or%20inverse%20design%20of%0Asuch%20structures%20in%20the%20presence%20of%20nonlinearities%20remains%20a%20challenge%20due%20to%0Athe%20expense%20of%20computational%20homogenization%20methods%20and%20the%20complexity%20of%0Adifferentiably%20parameterizing%20the%20microstructural%20response.%20A%20solution%20to%20this%0Achallenge%20lies%20in%20machine%20learning%20techniques%20that%20offer%20efficient%2C%0Adifferentiable%20mappings%20between%20the%20material%20response%20and%20its%20microstructural%0Adescriptors.%20This%20work%20presents%20a%20framework%20for%20designing%20multiscale%0Aheterogeneous%20structures%20with%20spatially%20varying%20microstructures%20by%20merging%20a%0Ahomogenization-based%20topology%20optimization%20strategy%20with%20a%20consistent%20machine%0Alearning%20approach%20grounded%20in%20hyperelasticity%20theory.%20We%20leverage%20neural%0Aarchitectures%20that%20adhere%20to%20critical%20physical%20principles%20such%20as%0Apolyconvexity%2C%20objectivity%2C%20material%20symmetry%2C%20and%20thermodynamic%20consistency%20to%0Asupply%20the%20framework%20with%20a%20reliable%20constitutive%20model%20that%20is%20dependent%20on%0Amaterial%20microstructural%20descriptors.%20Our%20findings%20highlight%20the%20potential%20of%0Aintegrating%20consistent%20machine%20learning%20models%20with%20density-based%20topology%0Aoptimization%20for%20enhancing%20design%20optimization%20of%20heterogeneous%20hyperelastic%0Astructures%20under%20finite%20deformations.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2408.13843v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DConsistent%2520machine%2520learning%2520for%2520topology%2520optimization%2520with%250A%2520%2520microstructure-dependent%2520neural%2520network%2520material%2520models%26entry.906535625%3DHarikrishnan%2520Vijayakumaran%2520and%2520Jonathan%2520B.%2520Russ%2520and%2520Glaucio%2520H.%2520Paulino%2520and%2520Miguel%2520A.%2520Bessa%26entry.1292438233%3D%2520%2520Additive%2520manufacturing%2520methods%2520together%2520with%2520topology%2520optimization%2520have%250Aenabled%2520the%2520creation%2520of%2520multiscale%2520structures%2520with%2520controlled%2520spatially-varying%250Amaterial%2520microstructure.%2520However%252C%2520topology%2520optimization%2520or%2520inverse%2520design%2520of%250Asuch%2520structures%2520in%2520the%2520presence%2520of%2520nonlinearities%2520remains%2520a%2520challenge%2520due%2520to%250Athe%2520expense%2520of%2520computational%2520homogenization%2520methods%2520and%2520the%2520complexity%2520of%250Adifferentiably%2520parameterizing%2520the%2520microstructural%2520response.%2520A%2520solution%2520to%2520this%250Achallenge%2520lies%2520in%2520machine%2520learning%2520techniques%2520that%2520offer%2520efficient%252C%250Adifferentiable%2520mappings%2520between%2520the%2520material%2520response%2520and%2520its%2520microstructural%250Adescriptors.%2520This%2520work%2520presents%2520a%2520framework%2520for%2520designing%2520multiscale%250Aheterogeneous%2520structures%2520with%2520spatially%2520varying%2520microstructures%2520by%2520merging%2520a%250Ahomogenization-based%2520topology%2520optimization%2520strategy%2520with%2520a%2520consistent%2520machine%250Alearning%2520approach%2520grounded%2520in%2520hyperelasticity%2520theory.%2520We%2520leverage%2520neural%250Aarchitectures%2520that%2520adhere%2520to%2520critical%2520physical%2520principles%2520such%2520as%250Apolyconvexity%252C%2520objectivity%252C%2520material%2520symmetry%252C%2520and%2520thermodynamic%2520consistency%2520to%250Asupply%2520the%2520framework%2520with%2520a%2520reliable%2520constitutive%2520model%2520that%2520is%2520dependent%2520on%250Amaterial%2520microstructural%2520descriptors.%2520Our%2520findings%2520highlight%2520the%2520potential%2520of%250Aintegrating%2520consistent%2520machine%2520learning%2520models%2520with%2520density-based%2520topology%250Aoptimization%2520for%2520enhancing%2520design%2520optimization%2520of%2520heterogeneous%2520hyperelastic%250Astructures%2520under%2520finite%2520deformations.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2408.13843v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Consistent%20machine%20learning%20for%20topology%20optimization%20with%0A%20%20microstructure-dependent%20neural%20network%20material%20models&entry.906535625=Harikrishnan%20Vijayakumaran%20and%20Jonathan%20B.%20Russ%20and%20Glaucio%20H.%20Paulino%20and%20Miguel%20A.%20Bessa&entry.1292438233=%20%20Additive%20manufacturing%20methods%20together%20with%20topology%20optimization%20have%0Aenabled%20the%20creation%20of%20multiscale%20structures%20with%20controlled%20spatially-varying%0Amaterial%20microstructure.%20However%2C%20topology%20optimization%20or%20inverse%20design%20of%0Asuch%20structures%20in%20the%20presence%20of%20nonlinearities%20remains%20a%20challenge%20due%20to%0Athe%20expense%20of%20computational%20homogenization%20methods%20and%20the%20complexity%20of%0Adifferentiably%20parameterizing%20the%20microstructural%20response.%20A%20solution%20to%20this%0Achallenge%20lies%20in%20machine%20learning%20techniques%20that%20offer%20efficient%2C%0Adifferentiable%20mappings%20between%20the%20material%20response%20and%20its%20microstructural%0Adescriptors.%20This%20work%20presents%20a%20framework%20for%20designing%20multiscale%0Aheterogeneous%20structures%20with%20spatially%20varying%20microstructures%20by%20merging%20a%0Ahomogenization-based%20topology%20optimization%20strategy%20with%20a%20consistent%20machine%0Alearning%20approach%20grounded%20in%20hyperelasticity%20theory.%20We%20leverage%20neural%0Aarchitectures%20that%20adhere%20to%20critical%20physical%20principles%20such%20as%0Apolyconvexity%2C%20objectivity%2C%20material%20symmetry%2C%20and%20thermodynamic%20consistency%20to%0Asupply%20the%20framework%20with%20a%20reliable%20constitutive%20model%20that%20is%20dependent%20on%0Amaterial%20microstructural%20descriptors.%20Our%20findings%20highlight%20the%20potential%20of%0Aintegrating%20consistent%20machine%20learning%20models%20with%20density-based%20topology%0Aoptimization%20for%20enhancing%20design%20optimization%20of%20heterogeneous%20hyperelastic%0Astructures%20under%20finite%20deformations.%0A&entry.1838667208=http%3A//arxiv.org/abs/2408.13843v2&entry.124074799=Read"},
{"title": "Automatic 8-tissue Segmentation for 6-month Infant Brains", "author": "Yilan Dong and Vanessa Kyriakopoulou and Irina Grigorescu and Grainne McAlonan and Dafnis Batalle and Maria Deprez", "abstract": "  Numerous studies have highlighted that atypical brain development,\nparticularly during infancy and toddlerhood, is linked to an increased\nlikelihood of being diagnosed with a neurodevelopmental condition, such as\nautism. Accurate brain tissue segmentations for morphological analysis are\nessential in numerous infant studies. However, due to ongoing white matter (WM)\nmyelination changing tissue contrast in T1- and T2-weighted images, automatic\ntissue segmentation in 6-month infants is particularly difficult. On the other\nhand, manual labelling by experts is time-consuming and labor-intensive. In\nthis study, we propose the first 8-tissue segmentation pipeline for\nsix-month-old infant brains. This pipeline utilizes domain adaptation (DA)\ntechniques to leverage our longitudinal data, including neonatal images\nsegmented with the neonatal Developing Human Connectome Project structural\npipeline. Our pipeline takes raw 6-month images as inputs and generates the\n8-tissue segmentation as outputs, forming an end-to-end segmentation pipeline.\nThe segmented tissues include WM, gray matter (GM), cerebrospinal fluid (CSF),\nventricles, cerebellum, basal ganglia, brainstem, and hippocampus/amygdala.\nCycle-Consistent Generative Adversarial Network (CycleGAN) and Attention U-Net\nwere employed to achieve the image contrast transformation between neonatal and\n6-month images and perform tissue segmentation on the synthesized 6-month\nimages (neonatal images with 6-month intensity contrast), respectively.\nMoreover, we incorporated the segmentation outputs from Infant Brain Extraction\nand Analysis Toolbox (iBEAT) and another Attention U-Net to further enhance the\nperformance and construct the end-to-end segmentation pipeline. Our evaluation\nwith real 6-month images achieved a DICE score of 0.92, an HD95 of 1.6, and an\nASSD of 0.42.\n", "link": "http://arxiv.org/abs/2408.15198v1", "date": "2024-08-27", "relevancy": 1.9773, "topK": [{"title": "SDXS: Real-Time One-Step Latent Diffusion Models with Image Conditions", "link": "http://arxiv.org/abs/2403.16627v1", "similarity": 0.516}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5066}, {"title": "ToDo: Token Downsampling for Efficient Generation of High-Resolution\n  Images", "link": "http://arxiv.org/abs/2402.13573v2", "similarity": 0.4734}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Automatic%208-tissue%20Segmentation%20for%206-month%20Infant%20Brains&body=Title%3A%20Automatic%208-tissue%20Segmentation%20for%206-month%20Infant%20Brains%0AAuthor%3A%20Yilan%20Dong%20and%20Vanessa%20Kyriakopoulou%20and%20Irina%20Grigorescu%20and%20Grainne%20McAlonan%20and%20Dafnis%20Batalle%20and%20Maria%20Deprez%0AAbstract%3A%20%20%20Numerous%20studies%20have%20highlighted%20that%20atypical%20brain%20development%2C%0Aparticularly%20during%20infancy%20and%20toddlerhood%2C%20is%20linked%20to%20an%20increased%0Alikelihood%20of%20being%20diagnosed%20with%20a%20neurodevelopmental%20condition%2C%20such%20as%0Aautism.%20Accurate%20brain%20tissue%20segmentations%20for%20morphological%20analysis%20are%0Aessential%20in%20numerous%20infant%20studies.%20However%2C%20due%20to%20ongoing%20white%20matter%20%28WM%29%0Amyelination%20changing%20tissue%20contrast%20in%20T1-%20and%20T2-weighted%20images%2C%20automatic%0Atissue%20segmentation%20in%206-month%20infants%20is%20particularly%20difficult.%20On%20the%20other%0Ahand%2C%20manual%20labelling%20by%20experts%20is%20time-consuming%20and%20labor-intensive.%20In%0Athis%20study%2C%20we%20propose%20the%20first%208-tissue%20segmentation%20pipeline%20for%0Asix-month-old%20infant%20brains.%20This%20pipeline%20utilizes%20domain%20adaptation%20%28DA%29%0Atechniques%20to%20leverage%20our%20longitudinal%20data%2C%20including%20neonatal%20images%0Asegmented%20with%20the%20neonatal%20Developing%20Human%20Connectome%20Project%20structural%0Apipeline.%20Our%20pipeline%20takes%20raw%206-month%20images%20as%20inputs%20and%20generates%20the%0A8-tissue%20segmentation%20as%20outputs%2C%20forming%20an%20end-to-end%20segmentation%20pipeline.%0AThe%20segmented%20tissues%20include%20WM%2C%20gray%20matter%20%28GM%29%2C%20cerebrospinal%20fluid%20%28CSF%29%2C%0Aventricles%2C%20cerebellum%2C%20basal%20ganglia%2C%20brainstem%2C%20and%20hippocampus/amygdala.%0ACycle-Consistent%20Generative%20Adversarial%20Network%20%28CycleGAN%29%20and%20Attention%20U-Net%0Awere%20employed%20to%20achieve%20the%20image%20contrast%20transformation%20between%20neonatal%20and%0A6-month%20images%20and%20perform%20tissue%20segmentation%20on%20the%20synthesized%206-month%0Aimages%20%28neonatal%20images%20with%206-month%20intensity%20contrast%29%2C%20respectively.%0AMoreover%2C%20we%20incorporated%20the%20segmentation%20outputs%20from%20Infant%20Brain%20Extraction%0Aand%20Analysis%20Toolbox%20%28iBEAT%29%20and%20another%20Attention%20U-Net%20to%20further%20enhance%20the%0Aperformance%20and%20construct%20the%20end-to-end%20segmentation%20pipeline.%20Our%20evaluation%0Awith%20real%206-month%20images%20achieved%20a%20DICE%20score%20of%200.92%2C%20an%20HD95%20of%201.6%2C%20and%20an%0AASSD%20of%200.42.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2408.15198v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DAutomatic%25208-tissue%2520Segmentation%2520for%25206-month%2520Infant%2520Brains%26entry.906535625%3DYilan%2520Dong%2520and%2520Vanessa%2520Kyriakopoulou%2520and%2520Irina%2520Grigorescu%2520and%2520Grainne%2520McAlonan%2520and%2520Dafnis%2520Batalle%2520and%2520Maria%2520Deprez%26entry.1292438233%3D%2520%2520Numerous%2520studies%2520have%2520highlighted%2520that%2520atypical%2520brain%2520development%252C%250Aparticularly%2520during%2520infancy%2520and%2520toddlerhood%252C%2520is%2520linked%2520to%2520an%2520increased%250Alikelihood%2520of%2520being%2520diagnosed%2520with%2520a%2520neurodevelopmental%2520condition%252C%2520such%2520as%250Aautism.%2520Accurate%2520brain%2520tissue%2520segmentations%2520for%2520morphological%2520analysis%2520are%250Aessential%2520in%2520numerous%2520infant%2520studies.%2520However%252C%2520due%2520to%2520ongoing%2520white%2520matter%2520%2528WM%2529%250Amyelination%2520changing%2520tissue%2520contrast%2520in%2520T1-%2520and%2520T2-weighted%2520images%252C%2520automatic%250Atissue%2520segmentation%2520in%25206-month%2520infants%2520is%2520particularly%2520difficult.%2520On%2520the%2520other%250Ahand%252C%2520manual%2520labelling%2520by%2520experts%2520is%2520time-consuming%2520and%2520labor-intensive.%2520In%250Athis%2520study%252C%2520we%2520propose%2520the%2520first%25208-tissue%2520segmentation%2520pipeline%2520for%250Asix-month-old%2520infant%2520brains.%2520This%2520pipeline%2520utilizes%2520domain%2520adaptation%2520%2528DA%2529%250Atechniques%2520to%2520leverage%2520our%2520longitudinal%2520data%252C%2520including%2520neonatal%2520images%250Asegmented%2520with%2520the%2520neonatal%2520Developing%2520Human%2520Connectome%2520Project%2520structural%250Apipeline.%2520Our%2520pipeline%2520takes%2520raw%25206-month%2520images%2520as%2520inputs%2520and%2520generates%2520the%250A8-tissue%2520segmentation%2520as%2520outputs%252C%2520forming%2520an%2520end-to-end%2520segmentation%2520pipeline.%250AThe%2520segmented%2520tissues%2520include%2520WM%252C%2520gray%2520matter%2520%2528GM%2529%252C%2520cerebrospinal%2520fluid%2520%2528CSF%2529%252C%250Aventricles%252C%2520cerebellum%252C%2520basal%2520ganglia%252C%2520brainstem%252C%2520and%2520hippocampus/amygdala.%250ACycle-Consistent%2520Generative%2520Adversarial%2520Network%2520%2528CycleGAN%2529%2520and%2520Attention%2520U-Net%250Awere%2520employed%2520to%2520achieve%2520the%2520image%2520contrast%2520transformation%2520between%2520neonatal%2520and%250A6-month%2520images%2520and%2520perform%2520tissue%2520segmentation%2520on%2520the%2520synthesized%25206-month%250Aimages%2520%2528neonatal%2520images%2520with%25206-month%2520intensity%2520contrast%2529%252C%2520respectively.%250AMoreover%252C%2520we%2520incorporated%2520the%2520segmentation%2520outputs%2520from%2520Infant%2520Brain%2520Extraction%250Aand%2520Analysis%2520Toolbox%2520%2528iBEAT%2529%2520and%2520another%2520Attention%2520U-Net%2520to%2520further%2520enhance%2520the%250Aperformance%2520and%2520construct%2520the%2520end-to-end%2520segmentation%2520pipeline.%2520Our%2520evaluation%250Awith%2520real%25206-month%2520images%2520achieved%2520a%2520DICE%2520score%2520of%25200.92%252C%2520an%2520HD95%2520of%25201.6%252C%2520and%2520an%250AASSD%2520of%25200.42.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2408.15198v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Automatic%208-tissue%20Segmentation%20for%206-month%20Infant%20Brains&entry.906535625=Yilan%20Dong%20and%20Vanessa%20Kyriakopoulou%20and%20Irina%20Grigorescu%20and%20Grainne%20McAlonan%20and%20Dafnis%20Batalle%20and%20Maria%20Deprez&entry.1292438233=%20%20Numerous%20studies%20have%20highlighted%20that%20atypical%20brain%20development%2C%0Aparticularly%20during%20infancy%20and%20toddlerhood%2C%20is%20linked%20to%20an%20increased%0Alikelihood%20of%20being%20diagnosed%20with%20a%20neurodevelopmental%20condition%2C%20such%20as%0Aautism.%20Accurate%20brain%20tissue%20segmentations%20for%20morphological%20analysis%20are%0Aessential%20in%20numerous%20infant%20studies.%20However%2C%20due%20to%20ongoing%20white%20matter%20%28WM%29%0Amyelination%20changing%20tissue%20contrast%20in%20T1-%20and%20T2-weighted%20images%2C%20automatic%0Atissue%20segmentation%20in%206-month%20infants%20is%20particularly%20difficult.%20On%20the%20other%0Ahand%2C%20manual%20labelling%20by%20experts%20is%20time-consuming%20and%20labor-intensive.%20In%0Athis%20study%2C%20we%20propose%20the%20first%208-tissue%20segmentation%20pipeline%20for%0Asix-month-old%20infant%20brains.%20This%20pipeline%20utilizes%20domain%20adaptation%20%28DA%29%0Atechniques%20to%20leverage%20our%20longitudinal%20data%2C%20including%20neonatal%20images%0Asegmented%20with%20the%20neonatal%20Developing%20Human%20Connectome%20Project%20structural%0Apipeline.%20Our%20pipeline%20takes%20raw%206-month%20images%20as%20inputs%20and%20generates%20the%0A8-tissue%20segmentation%20as%20outputs%2C%20forming%20an%20end-to-end%20segmentation%20pipeline.%0AThe%20segmented%20tissues%20include%20WM%2C%20gray%20matter%20%28GM%29%2C%20cerebrospinal%20fluid%20%28CSF%29%2C%0Aventricles%2C%20cerebellum%2C%20basal%20ganglia%2C%20brainstem%2C%20and%20hippocampus/amygdala.%0ACycle-Consistent%20Generative%20Adversarial%20Network%20%28CycleGAN%29%20and%20Attention%20U-Net%0Awere%20employed%20to%20achieve%20the%20image%20contrast%20transformation%20between%20neonatal%20and%0A6-month%20images%20and%20perform%20tissue%20segmentation%20on%20the%20synthesized%206-month%0Aimages%20%28neonatal%20images%20with%206-month%20intensity%20contrast%29%2C%20respectively.%0AMoreover%2C%20we%20incorporated%20the%20segmentation%20outputs%20from%20Infant%20Brain%20Extraction%0Aand%20Analysis%20Toolbox%20%28iBEAT%29%20and%20another%20Attention%20U-Net%20to%20further%20enhance%20the%0Aperformance%20and%20construct%20the%20end-to-end%20segmentation%20pipeline.%20Our%20evaluation%0Awith%20real%206-month%20images%20achieved%20a%20DICE%20score%20of%200.92%2C%20an%20HD95%20of%201.6%2C%20and%20an%0AASSD%20of%200.42.%0A&entry.1838667208=http%3A//arxiv.org/abs/2408.15198v1&entry.124074799=Read"},
{"title": "PEER: Expertizing Domain-Specific Tasks with a Multi-Agent Framework and\n  Tuning Methods", "author": "Yiying Wang and Xiaojing Li and Binzhu Wang and Yueyang Zhou and Yingru Lin and Han Ji and Hong Chen and Jinshi Zhang and Fei Yu and Zewei Zhao and Song Jin and Renji Gong and Wanqing Xu", "abstract": "  In domain-specific applications, GPT-4, augmented with precise prompts or\nRetrieval-Augmented Generation (RAG), shows notable potential but faces the\ncritical tri-lemma of performance, cost, and data privacy. High performance\nrequires sophisticated processing techniques, yet managing multiple agents\nwithin a complex workflow often proves costly and challenging. To address this,\nwe introduce the PEER (Plan, Execute, Express, Review) multi-agent framework.\nThis systematizes domain-specific tasks by integrating precise question\ndecomposition, advanced information retrieval, comprehensive summarization, and\nrigorous self-assessment. Given the concerns of cost and data privacy,\nenterprises are shifting from proprietary models like GPT-4 to custom models,\nstriking a balance between cost, security, and performance. We developed\nindustrial practices leveraging online data and user feedback for efficient\nmodel tuning. This study provides best practice guidelines for applying\nmulti-agent systems in domain-specific problem-solving and implementing\neffective agent tuning strategies. Our empirical studies, particularly in the\nfinancial question-answering domain, demonstrate that our approach achieves\n95.0% of GPT-4's performance, while effectively managing costs and ensuring\ndata privacy.\n", "link": "http://arxiv.org/abs/2407.06985v3", "date": "2024-08-27", "relevancy": 1.9735, "topK": [{"title": "Safe and efficient training of a control agent", "link": "https://patents.google.com/patent/US11709462B2/en", "similarity": 0.498}, {"title": "RoboEXP: Action-Conditioned Scene Graph via Interactive Exploration for\n  Robotic Manipulation", "link": "http://arxiv.org/abs/2402.15487v1", "similarity": 0.491}, {"title": "Genie: Generative Interactive Environments", "link": "http://arxiv.org/abs/2402.15391v1", "similarity": 0.4897}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20PEER%3A%20Expertizing%20Domain-Specific%20Tasks%20with%20a%20Multi-Agent%20Framework%20and%0A%20%20Tuning%20Methods&body=Title%3A%20PEER%3A%20Expertizing%20Domain-Specific%20Tasks%20with%20a%20Multi-Agent%20Framework%20and%0A%20%20Tuning%20Methods%0AAuthor%3A%20Yiying%20Wang%20and%20Xiaojing%20Li%20and%20Binzhu%20Wang%20and%20Yueyang%20Zhou%20and%20Yingru%20Lin%20and%20Han%20Ji%20and%20Hong%20Chen%20and%20Jinshi%20Zhang%20and%20Fei%20Yu%20and%20Zewei%20Zhao%20and%20Song%20Jin%20and%20Renji%20Gong%20and%20Wanqing%20Xu%0AAbstract%3A%20%20%20In%20domain-specific%20applications%2C%20GPT-4%2C%20augmented%20with%20precise%20prompts%20or%0ARetrieval-Augmented%20Generation%20%28RAG%29%2C%20shows%20notable%20potential%20but%20faces%20the%0Acritical%20tri-lemma%20of%20performance%2C%20cost%2C%20and%20data%20privacy.%20High%20performance%0Arequires%20sophisticated%20processing%20techniques%2C%20yet%20managing%20multiple%20agents%0Awithin%20a%20complex%20workflow%20often%20proves%20costly%20and%20challenging.%20To%20address%20this%2C%0Awe%20introduce%20the%20PEER%20%28Plan%2C%20Execute%2C%20Express%2C%20Review%29%20multi-agent%20framework.%0AThis%20systematizes%20domain-specific%20tasks%20by%20integrating%20precise%20question%0Adecomposition%2C%20advanced%20information%20retrieval%2C%20comprehensive%20summarization%2C%20and%0Arigorous%20self-assessment.%20Given%20the%20concerns%20of%20cost%20and%20data%20privacy%2C%0Aenterprises%20are%20shifting%20from%20proprietary%20models%20like%20GPT-4%20to%20custom%20models%2C%0Astriking%20a%20balance%20between%20cost%2C%20security%2C%20and%20performance.%20We%20developed%0Aindustrial%20practices%20leveraging%20online%20data%20and%20user%20feedback%20for%20efficient%0Amodel%20tuning.%20This%20study%20provides%20best%20practice%20guidelines%20for%20applying%0Amulti-agent%20systems%20in%20domain-specific%20problem-solving%20and%20implementing%0Aeffective%20agent%20tuning%20strategies.%20Our%20empirical%20studies%2C%20particularly%20in%20the%0Afinancial%20question-answering%20domain%2C%20demonstrate%20that%20our%20approach%20achieves%0A95.0%25%20of%20GPT-4%27s%20performance%2C%20while%20effectively%20managing%20costs%20and%20ensuring%0Adata%20privacy.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2407.06985v3%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DPEER%253A%2520Expertizing%2520Domain-Specific%2520Tasks%2520with%2520a%2520Multi-Agent%2520Framework%2520and%250A%2520%2520Tuning%2520Methods%26entry.906535625%3DYiying%2520Wang%2520and%2520Xiaojing%2520Li%2520and%2520Binzhu%2520Wang%2520and%2520Yueyang%2520Zhou%2520and%2520Yingru%2520Lin%2520and%2520Han%2520Ji%2520and%2520Hong%2520Chen%2520and%2520Jinshi%2520Zhang%2520and%2520Fei%2520Yu%2520and%2520Zewei%2520Zhao%2520and%2520Song%2520Jin%2520and%2520Renji%2520Gong%2520and%2520Wanqing%2520Xu%26entry.1292438233%3D%2520%2520In%2520domain-specific%2520applications%252C%2520GPT-4%252C%2520augmented%2520with%2520precise%2520prompts%2520or%250ARetrieval-Augmented%2520Generation%2520%2528RAG%2529%252C%2520shows%2520notable%2520potential%2520but%2520faces%2520the%250Acritical%2520tri-lemma%2520of%2520performance%252C%2520cost%252C%2520and%2520data%2520privacy.%2520High%2520performance%250Arequires%2520sophisticated%2520processing%2520techniques%252C%2520yet%2520managing%2520multiple%2520agents%250Awithin%2520a%2520complex%2520workflow%2520often%2520proves%2520costly%2520and%2520challenging.%2520To%2520address%2520this%252C%250Awe%2520introduce%2520the%2520PEER%2520%2528Plan%252C%2520Execute%252C%2520Express%252C%2520Review%2529%2520multi-agent%2520framework.%250AThis%2520systematizes%2520domain-specific%2520tasks%2520by%2520integrating%2520precise%2520question%250Adecomposition%252C%2520advanced%2520information%2520retrieval%252C%2520comprehensive%2520summarization%252C%2520and%250Arigorous%2520self-assessment.%2520Given%2520the%2520concerns%2520of%2520cost%2520and%2520data%2520privacy%252C%250Aenterprises%2520are%2520shifting%2520from%2520proprietary%2520models%2520like%2520GPT-4%2520to%2520custom%2520models%252C%250Astriking%2520a%2520balance%2520between%2520cost%252C%2520security%252C%2520and%2520performance.%2520We%2520developed%250Aindustrial%2520practices%2520leveraging%2520online%2520data%2520and%2520user%2520feedback%2520for%2520efficient%250Amodel%2520tuning.%2520This%2520study%2520provides%2520best%2520practice%2520guidelines%2520for%2520applying%250Amulti-agent%2520systems%2520in%2520domain-specific%2520problem-solving%2520and%2520implementing%250Aeffective%2520agent%2520tuning%2520strategies.%2520Our%2520empirical%2520studies%252C%2520particularly%2520in%2520the%250Afinancial%2520question-answering%2520domain%252C%2520demonstrate%2520that%2520our%2520approach%2520achieves%250A95.0%2525%2520of%2520GPT-4%2527s%2520performance%252C%2520while%2520effectively%2520managing%2520costs%2520and%2520ensuring%250Adata%2520privacy.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2407.06985v3%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=PEER%3A%20Expertizing%20Domain-Specific%20Tasks%20with%20a%20Multi-Agent%20Framework%20and%0A%20%20Tuning%20Methods&entry.906535625=Yiying%20Wang%20and%20Xiaojing%20Li%20and%20Binzhu%20Wang%20and%20Yueyang%20Zhou%20and%20Yingru%20Lin%20and%20Han%20Ji%20and%20Hong%20Chen%20and%20Jinshi%20Zhang%20and%20Fei%20Yu%20and%20Zewei%20Zhao%20and%20Song%20Jin%20and%20Renji%20Gong%20and%20Wanqing%20Xu&entry.1292438233=%20%20In%20domain-specific%20applications%2C%20GPT-4%2C%20augmented%20with%20precise%20prompts%20or%0ARetrieval-Augmented%20Generation%20%28RAG%29%2C%20shows%20notable%20potential%20but%20faces%20the%0Acritical%20tri-lemma%20of%20performance%2C%20cost%2C%20and%20data%20privacy.%20High%20performance%0Arequires%20sophisticated%20processing%20techniques%2C%20yet%20managing%20multiple%20agents%0Awithin%20a%20complex%20workflow%20often%20proves%20costly%20and%20challenging.%20To%20address%20this%2C%0Awe%20introduce%20the%20PEER%20%28Plan%2C%20Execute%2C%20Express%2C%20Review%29%20multi-agent%20framework.%0AThis%20systematizes%20domain-specific%20tasks%20by%20integrating%20precise%20question%0Adecomposition%2C%20advanced%20information%20retrieval%2C%20comprehensive%20summarization%2C%20and%0Arigorous%20self-assessment.%20Given%20the%20concerns%20of%20cost%20and%20data%20privacy%2C%0Aenterprises%20are%20shifting%20from%20proprietary%20models%20like%20GPT-4%20to%20custom%20models%2C%0Astriking%20a%20balance%20between%20cost%2C%20security%2C%20and%20performance.%20We%20developed%0Aindustrial%20practices%20leveraging%20online%20data%20and%20user%20feedback%20for%20efficient%0Amodel%20tuning.%20This%20study%20provides%20best%20practice%20guidelines%20for%20applying%0Amulti-agent%20systems%20in%20domain-specific%20problem-solving%20and%20implementing%0Aeffective%20agent%20tuning%20strategies.%20Our%20empirical%20studies%2C%20particularly%20in%20the%0Afinancial%20question-answering%20domain%2C%20demonstrate%20that%20our%20approach%20achieves%0A95.0%25%20of%20GPT-4%27s%20performance%2C%20while%20effectively%20managing%20costs%20and%20ensuring%0Adata%20privacy.%0A&entry.1838667208=http%3A//arxiv.org/abs/2407.06985v3&entry.124074799=Read"},
{"title": "BaichuanSEED: Sharing the Potential of ExtensivE Data Collection and\n  Deduplication by Introducing a Competitive Large Language Model Baseline", "author": "Guosheng Dong and Da Pan and Yiding Sun and Shusen Zhang and Zheng Liang and Xin Wu and Yanjun Shen and Fan Yang and Haoze Sun and Tianpeng Li and Mingan Lin and Jianhua Xu and Yufan Zhang and Xiaonan Nie and Lei Su and Bingning Wang and Wentao Zhang and Jiaxin Mao and Zenan Zhou and Weipeng Chen", "abstract": "  The general capabilities of Large Language Models (LLM) highly rely on the\ncomposition and selection on extensive pretraining datasets, treated as\ncommercial secrets by several institutions. To mitigate this issue, we\nopen-source the details of a universally applicable data processing pipeline\nand validate its effectiveness and potential by introducing a competitive LLM\nbaseline. Specifically, the data processing pipeline consists of broad\ncollection to scale up and reweighting to improve quality. We then pretrain a\n7B model BaichuanSEED with 3T tokens processed by our pipeline without any\ndeliberate downstream task-related optimization, followed by an easy but\neffective supervised fine-tuning stage. BaichuanSEED demonstrates consistency\nand predictability throughout training and achieves comparable performance on\ncomprehensive benchmarks with several commercial advanced large language\nmodels, such as Qwen1.5 and Llama3. We also conduct several heuristic\nexperiments to discuss the potential for further optimization of downstream\ntasks, such as mathematics and coding.\n", "link": "http://arxiv.org/abs/2408.15079v1", "date": "2024-08-27", "relevancy": 1.9698, "topK": [{"title": "Breadcrumbs: Adversarial class-balanced sampling for long-tailed recognition", "link": "https://link.springer.com/chapter/10.1007/978-3-031-20053-3_37", "similarity": 0.5049}, {"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.4933}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.4796}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20BaichuanSEED%3A%20Sharing%20the%20Potential%20of%20ExtensivE%20Data%20Collection%20and%0A%20%20Deduplication%20by%20Introducing%20a%20Competitive%20Large%20Language%20Model%20Baseline&body=Title%3A%20BaichuanSEED%3A%20Sharing%20the%20Potential%20of%20ExtensivE%20Data%20Collection%20and%0A%20%20Deduplication%20by%20Introducing%20a%20Competitive%20Large%20Language%20Model%20Baseline%0AAuthor%3A%20Guosheng%20Dong%20and%20Da%20Pan%20and%20Yiding%20Sun%20and%20Shusen%20Zhang%20and%20Zheng%20Liang%20and%20Xin%20Wu%20and%20Yanjun%20Shen%20and%20Fan%20Yang%20and%20Haoze%20Sun%20and%20Tianpeng%20Li%20and%20Mingan%20Lin%20and%20Jianhua%20Xu%20and%20Yufan%20Zhang%20and%20Xiaonan%20Nie%20and%20Lei%20Su%20and%20Bingning%20Wang%20and%20Wentao%20Zhang%20and%20Jiaxin%20Mao%20and%20Zenan%20Zhou%20and%20Weipeng%20Chen%0AAbstract%3A%20%20%20The%20general%20capabilities%20of%20Large%20Language%20Models%20%28LLM%29%20highly%20rely%20on%20the%0Acomposition%20and%20selection%20on%20extensive%20pretraining%20datasets%2C%20treated%20as%0Acommercial%20secrets%20by%20several%20institutions.%20To%20mitigate%20this%20issue%2C%20we%0Aopen-source%20the%20details%20of%20a%20universally%20applicable%20data%20processing%20pipeline%0Aand%20validate%20its%20effectiveness%20and%20potential%20by%20introducing%20a%20competitive%20LLM%0Abaseline.%20Specifically%2C%20the%20data%20processing%20pipeline%20consists%20of%20broad%0Acollection%20to%20scale%20up%20and%20reweighting%20to%20improve%20quality.%20We%20then%20pretrain%20a%0A7B%20model%20BaichuanSEED%20with%203T%20tokens%20processed%20by%20our%20pipeline%20without%20any%0Adeliberate%20downstream%20task-related%20optimization%2C%20followed%20by%20an%20easy%20but%0Aeffective%20supervised%20fine-tuning%20stage.%20BaichuanSEED%20demonstrates%20consistency%0Aand%20predictability%20throughout%20training%20and%20achieves%20comparable%20performance%20on%0Acomprehensive%20benchmarks%20with%20several%20commercial%20advanced%20large%20language%0Amodels%2C%20such%20as%20Qwen1.5%20and%20Llama3.%20We%20also%20conduct%20several%20heuristic%0Aexperiments%20to%20discuss%20the%20potential%20for%20further%20optimization%20of%20downstream%0Atasks%2C%20such%20as%20mathematics%20and%20coding.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2408.15079v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DBaichuanSEED%253A%2520Sharing%2520the%2520Potential%2520of%2520ExtensivE%2520Data%2520Collection%2520and%250A%2520%2520Deduplication%2520by%2520Introducing%2520a%2520Competitive%2520Large%2520Language%2520Model%2520Baseline%26entry.906535625%3DGuosheng%2520Dong%2520and%2520Da%2520Pan%2520and%2520Yiding%2520Sun%2520and%2520Shusen%2520Zhang%2520and%2520Zheng%2520Liang%2520and%2520Xin%2520Wu%2520and%2520Yanjun%2520Shen%2520and%2520Fan%2520Yang%2520and%2520Haoze%2520Sun%2520and%2520Tianpeng%2520Li%2520and%2520Mingan%2520Lin%2520and%2520Jianhua%2520Xu%2520and%2520Yufan%2520Zhang%2520and%2520Xiaonan%2520Nie%2520and%2520Lei%2520Su%2520and%2520Bingning%2520Wang%2520and%2520Wentao%2520Zhang%2520and%2520Jiaxin%2520Mao%2520and%2520Zenan%2520Zhou%2520and%2520Weipeng%2520Chen%26entry.1292438233%3D%2520%2520The%2520general%2520capabilities%2520of%2520Large%2520Language%2520Models%2520%2528LLM%2529%2520highly%2520rely%2520on%2520the%250Acomposition%2520and%2520selection%2520on%2520extensive%2520pretraining%2520datasets%252C%2520treated%2520as%250Acommercial%2520secrets%2520by%2520several%2520institutions.%2520To%2520mitigate%2520this%2520issue%252C%2520we%250Aopen-source%2520the%2520details%2520of%2520a%2520universally%2520applicable%2520data%2520processing%2520pipeline%250Aand%2520validate%2520its%2520effectiveness%2520and%2520potential%2520by%2520introducing%2520a%2520competitive%2520LLM%250Abaseline.%2520Specifically%252C%2520the%2520data%2520processing%2520pipeline%2520consists%2520of%2520broad%250Acollection%2520to%2520scale%2520up%2520and%2520reweighting%2520to%2520improve%2520quality.%2520We%2520then%2520pretrain%2520a%250A7B%2520model%2520BaichuanSEED%2520with%25203T%2520tokens%2520processed%2520by%2520our%2520pipeline%2520without%2520any%250Adeliberate%2520downstream%2520task-related%2520optimization%252C%2520followed%2520by%2520an%2520easy%2520but%250Aeffective%2520supervised%2520fine-tuning%2520stage.%2520BaichuanSEED%2520demonstrates%2520consistency%250Aand%2520predictability%2520throughout%2520training%2520and%2520achieves%2520comparable%2520performance%2520on%250Acomprehensive%2520benchmarks%2520with%2520several%2520commercial%2520advanced%2520large%2520language%250Amodels%252C%2520such%2520as%2520Qwen1.5%2520and%2520Llama3.%2520We%2520also%2520conduct%2520several%2520heuristic%250Aexperiments%2520to%2520discuss%2520the%2520potential%2520for%2520further%2520optimization%2520of%2520downstream%250Atasks%252C%2520such%2520as%2520mathematics%2520and%2520coding.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2408.15079v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=BaichuanSEED%3A%20Sharing%20the%20Potential%20of%20ExtensivE%20Data%20Collection%20and%0A%20%20Deduplication%20by%20Introducing%20a%20Competitive%20Large%20Language%20Model%20Baseline&entry.906535625=Guosheng%20Dong%20and%20Da%20Pan%20and%20Yiding%20Sun%20and%20Shusen%20Zhang%20and%20Zheng%20Liang%20and%20Xin%20Wu%20and%20Yanjun%20Shen%20and%20Fan%20Yang%20and%20Haoze%20Sun%20and%20Tianpeng%20Li%20and%20Mingan%20Lin%20and%20Jianhua%20Xu%20and%20Yufan%20Zhang%20and%20Xiaonan%20Nie%20and%20Lei%20Su%20and%20Bingning%20Wang%20and%20Wentao%20Zhang%20and%20Jiaxin%20Mao%20and%20Zenan%20Zhou%20and%20Weipeng%20Chen&entry.1292438233=%20%20The%20general%20capabilities%20of%20Large%20Language%20Models%20%28LLM%29%20highly%20rely%20on%20the%0Acomposition%20and%20selection%20on%20extensive%20pretraining%20datasets%2C%20treated%20as%0Acommercial%20secrets%20by%20several%20institutions.%20To%20mitigate%20this%20issue%2C%20we%0Aopen-source%20the%20details%20of%20a%20universally%20applicable%20data%20processing%20pipeline%0Aand%20validate%20its%20effectiveness%20and%20potential%20by%20introducing%20a%20competitive%20LLM%0Abaseline.%20Specifically%2C%20the%20data%20processing%20pipeline%20consists%20of%20broad%0Acollection%20to%20scale%20up%20and%20reweighting%20to%20improve%20quality.%20We%20then%20pretrain%20a%0A7B%20model%20BaichuanSEED%20with%203T%20tokens%20processed%20by%20our%20pipeline%20without%20any%0Adeliberate%20downstream%20task-related%20optimization%2C%20followed%20by%20an%20easy%20but%0Aeffective%20supervised%20fine-tuning%20stage.%20BaichuanSEED%20demonstrates%20consistency%0Aand%20predictability%20throughout%20training%20and%20achieves%20comparable%20performance%20on%0Acomprehensive%20benchmarks%20with%20several%20commercial%20advanced%20large%20language%0Amodels%2C%20such%20as%20Qwen1.5%20and%20Llama3.%20We%20also%20conduct%20several%20heuristic%0Aexperiments%20to%20discuss%20the%20potential%20for%20further%20optimization%20of%20downstream%0Atasks%2C%20such%20as%20mathematics%20and%20coding.%0A&entry.1838667208=http%3A//arxiv.org/abs/2408.15079v1&entry.124074799=Read"},
{"title": "Polyp SAM 2: Advancing Zero shot Polyp Segmentation in Colorectal Cancer\n  Detection", "author": "Mobina Mansoori and Sajjad Shahabodini and Jamshid Abouei and Konstantinos N. Plataniotis and Arash Mohammadi", "abstract": "  Polyp segmentation plays a crucial role in the early detection and diagnosis\nof colorectal cancer. However, obtaining accurate segmentations often requires\nlabor-intensive annotations and specialized models. Recently, Meta AI Research\nreleased a general Segment Anything Model 2 (SAM 2), which has demonstrated\npromising performance in several segmentation tasks. In this manuscript, we\nevaluate the performance of SAM 2 in segmenting polyps under various prompted\nsettings. We hope this report will provide insights to advance the field of\npolyp segmentation and promote more interesting work in the future. This\nproject is publicly available at https://github.com/ sajjad-sh33/Polyp-SAM-2.\n", "link": "http://arxiv.org/abs/2408.05892v3", "date": "2024-08-27", "relevancy": 1.9666, "topK": [{"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5055}, {"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.4882}, {"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.4792}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Polyp%20SAM%202%3A%20Advancing%20Zero%20shot%20Polyp%20Segmentation%20in%20Colorectal%20Cancer%0A%20%20Detection&body=Title%3A%20Polyp%20SAM%202%3A%20Advancing%20Zero%20shot%20Polyp%20Segmentation%20in%20Colorectal%20Cancer%0A%20%20Detection%0AAuthor%3A%20Mobina%20Mansoori%20and%20Sajjad%20Shahabodini%20and%20Jamshid%20Abouei%20and%20Konstantinos%20N.%20Plataniotis%20and%20Arash%20Mohammadi%0AAbstract%3A%20%20%20Polyp%20segmentation%20plays%20a%20crucial%20role%20in%20the%20early%20detection%20and%20diagnosis%0Aof%20colorectal%20cancer.%20However%2C%20obtaining%20accurate%20segmentations%20often%20requires%0Alabor-intensive%20annotations%20and%20specialized%20models.%20Recently%2C%20Meta%20AI%20Research%0Areleased%20a%20general%20Segment%20Anything%20Model%202%20%28SAM%202%29%2C%20which%20has%20demonstrated%0Apromising%20performance%20in%20several%20segmentation%20tasks.%20In%20this%20manuscript%2C%20we%0Aevaluate%20the%20performance%20of%20SAM%202%20in%20segmenting%20polyps%20under%20various%20prompted%0Asettings.%20We%20hope%20this%20report%20will%20provide%20insights%20to%20advance%20the%20field%20of%0Apolyp%20segmentation%20and%20promote%20more%20interesting%20work%20in%20the%20future.%20This%0Aproject%20is%20publicly%20available%20at%20https%3A//github.com/%20sajjad-sh33/Polyp-SAM-2.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2408.05892v3%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DPolyp%2520SAM%25202%253A%2520Advancing%2520Zero%2520shot%2520Polyp%2520Segmentation%2520in%2520Colorectal%2520Cancer%250A%2520%2520Detection%26entry.906535625%3DMobina%2520Mansoori%2520and%2520Sajjad%2520Shahabodini%2520and%2520Jamshid%2520Abouei%2520and%2520Konstantinos%2520N.%2520Plataniotis%2520and%2520Arash%2520Mohammadi%26entry.1292438233%3D%2520%2520Polyp%2520segmentation%2520plays%2520a%2520crucial%2520role%2520in%2520the%2520early%2520detection%2520and%2520diagnosis%250Aof%2520colorectal%2520cancer.%2520However%252C%2520obtaining%2520accurate%2520segmentations%2520often%2520requires%250Alabor-intensive%2520annotations%2520and%2520specialized%2520models.%2520Recently%252C%2520Meta%2520AI%2520Research%250Areleased%2520a%2520general%2520Segment%2520Anything%2520Model%25202%2520%2528SAM%25202%2529%252C%2520which%2520has%2520demonstrated%250Apromising%2520performance%2520in%2520several%2520segmentation%2520tasks.%2520In%2520this%2520manuscript%252C%2520we%250Aevaluate%2520the%2520performance%2520of%2520SAM%25202%2520in%2520segmenting%2520polyps%2520under%2520various%2520prompted%250Asettings.%2520We%2520hope%2520this%2520report%2520will%2520provide%2520insights%2520to%2520advance%2520the%2520field%2520of%250Apolyp%2520segmentation%2520and%2520promote%2520more%2520interesting%2520work%2520in%2520the%2520future.%2520This%250Aproject%2520is%2520publicly%2520available%2520at%2520https%253A//github.com/%2520sajjad-sh33/Polyp-SAM-2.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2408.05892v3%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Polyp%20SAM%202%3A%20Advancing%20Zero%20shot%20Polyp%20Segmentation%20in%20Colorectal%20Cancer%0A%20%20Detection&entry.906535625=Mobina%20Mansoori%20and%20Sajjad%20Shahabodini%20and%20Jamshid%20Abouei%20and%20Konstantinos%20N.%20Plataniotis%20and%20Arash%20Mohammadi&entry.1292438233=%20%20Polyp%20segmentation%20plays%20a%20crucial%20role%20in%20the%20early%20detection%20and%20diagnosis%0Aof%20colorectal%20cancer.%20However%2C%20obtaining%20accurate%20segmentations%20often%20requires%0Alabor-intensive%20annotations%20and%20specialized%20models.%20Recently%2C%20Meta%20AI%20Research%0Areleased%20a%20general%20Segment%20Anything%20Model%202%20%28SAM%202%29%2C%20which%20has%20demonstrated%0Apromising%20performance%20in%20several%20segmentation%20tasks.%20In%20this%20manuscript%2C%20we%0Aevaluate%20the%20performance%20of%20SAM%202%20in%20segmenting%20polyps%20under%20various%20prompted%0Asettings.%20We%20hope%20this%20report%20will%20provide%20insights%20to%20advance%20the%20field%20of%0Apolyp%20segmentation%20and%20promote%20more%20interesting%20work%20in%20the%20future.%20This%0Aproject%20is%20publicly%20available%20at%20https%3A//github.com/%20sajjad-sh33/Polyp-SAM-2.%0A&entry.1838667208=http%3A//arxiv.org/abs/2408.05892v3&entry.124074799=Read"},
{"title": "Enhancing Sign Language Detection through Mediapipe and Convolutional\n  Neural Networks (CNN)", "author": "Aditya Raj Verma and Gagandeep Singh and Karnim Meghwal and Banawath Ramji and Praveen Kumar Dadheech", "abstract": "  This research combines MediaPipe and CNNs for the efficient and accurate\ninterpretation of ASL dataset for the real-time detection of sign language. The\nsystem presented here captures and processes hands' gestures in real time. the\nintended purpose was to create a very easy, accurate, and fast way of entering\ncommands without the necessity of touching something.MediaPipe supports one of\nthe powerful frameworks in real-time hand tracking capabilities for the ability\nto capture and preprocess hand movements, which increases the accuracy of the\ngesture recognition system. Actually, the integration of CNN with the MediaPipe\nresults in higher efficiency in using the model of real-time processing.The\naccuracy achieved by the model on ASL datasets is 99.12\\%.The model was tested\nusing American Sign Language (ASL) datasets. The results were then compared to\nthose of existing methods to evaluate how well it performed, using established\nevaluation techniques. The system will have applications in the communication,\neducation, and accessibility domains. Making systems such as described in this\npaper even better will assist people with hearing impairment and make things\naccessible to them. We tested the recognition and translation performance on an\nASL dataset and achieved better accuracy over previous models.It is meant to\nthe research is to identify the characters that American signs recognize using\nhand images taken from a web camera by based on mediapipe and CNNs\n", "link": "http://arxiv.org/abs/2406.03729v2", "date": "2024-08-27", "relevancy": 1.9465, "topK": [{"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5019}, {"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.4815}, {"title": "SDXS: Real-Time One-Step Latent Diffusion Models with Image Conditions", "link": "http://arxiv.org/abs/2403.16627v1", "similarity": 0.4612}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Enhancing%20Sign%20Language%20Detection%20through%20Mediapipe%20and%20Convolutional%0A%20%20Neural%20Networks%20%28CNN%29&body=Title%3A%20Enhancing%20Sign%20Language%20Detection%20through%20Mediapipe%20and%20Convolutional%0A%20%20Neural%20Networks%20%28CNN%29%0AAuthor%3A%20Aditya%20Raj%20Verma%20and%20Gagandeep%20Singh%20and%20Karnim%20Meghwal%20and%20Banawath%20Ramji%20and%20Praveen%20Kumar%20Dadheech%0AAbstract%3A%20%20%20This%20research%20combines%20MediaPipe%20and%20CNNs%20for%20the%20efficient%20and%20accurate%0Ainterpretation%20of%20ASL%20dataset%20for%20the%20real-time%20detection%20of%20sign%20language.%20The%0Asystem%20presented%20here%20captures%20and%20processes%20hands%27%20gestures%20in%20real%20time.%20the%0Aintended%20purpose%20was%20to%20create%20a%20very%20easy%2C%20accurate%2C%20and%20fast%20way%20of%20entering%0Acommands%20without%20the%20necessity%20of%20touching%20something.MediaPipe%20supports%20one%20of%0Athe%20powerful%20frameworks%20in%20real-time%20hand%20tracking%20capabilities%20for%20the%20ability%0Ato%20capture%20and%20preprocess%20hand%20movements%2C%20which%20increases%20the%20accuracy%20of%20the%0Agesture%20recognition%20system.%20Actually%2C%20the%20integration%20of%20CNN%20with%20the%20MediaPipe%0Aresults%20in%20higher%20efficiency%20in%20using%20the%20model%20of%20real-time%20processing.The%0Aaccuracy%20achieved%20by%20the%20model%20on%20ASL%20datasets%20is%2099.12%5C%25.The%20model%20was%20tested%0Ausing%20American%20Sign%20Language%20%28ASL%29%20datasets.%20The%20results%20were%20then%20compared%20to%0Athose%20of%20existing%20methods%20to%20evaluate%20how%20well%20it%20performed%2C%20using%20established%0Aevaluation%20techniques.%20The%20system%20will%20have%20applications%20in%20the%20communication%2C%0Aeducation%2C%20and%20accessibility%20domains.%20Making%20systems%20such%20as%20described%20in%20this%0Apaper%20even%20better%20will%20assist%20people%20with%20hearing%20impairment%20and%20make%20things%0Aaccessible%20to%20them.%20We%20tested%20the%20recognition%20and%20translation%20performance%20on%20an%0AASL%20dataset%20and%20achieved%20better%20accuracy%20over%20previous%20models.It%20is%20meant%20to%0Athe%20research%20is%20to%20identify%20the%20characters%20that%20American%20signs%20recognize%20using%0Ahand%20images%20taken%20from%20a%20web%20camera%20by%20based%20on%20mediapipe%20and%20CNNs%0A%0ALink%3A%20http%3A//arxiv.org/abs/2406.03729v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DEnhancing%2520Sign%2520Language%2520Detection%2520through%2520Mediapipe%2520and%2520Convolutional%250A%2520%2520Neural%2520Networks%2520%2528CNN%2529%26entry.906535625%3DAditya%2520Raj%2520Verma%2520and%2520Gagandeep%2520Singh%2520and%2520Karnim%2520Meghwal%2520and%2520Banawath%2520Ramji%2520and%2520Praveen%2520Kumar%2520Dadheech%26entry.1292438233%3D%2520%2520This%2520research%2520combines%2520MediaPipe%2520and%2520CNNs%2520for%2520the%2520efficient%2520and%2520accurate%250Ainterpretation%2520of%2520ASL%2520dataset%2520for%2520the%2520real-time%2520detection%2520of%2520sign%2520language.%2520The%250Asystem%2520presented%2520here%2520captures%2520and%2520processes%2520hands%2527%2520gestures%2520in%2520real%2520time.%2520the%250Aintended%2520purpose%2520was%2520to%2520create%2520a%2520very%2520easy%252C%2520accurate%252C%2520and%2520fast%2520way%2520of%2520entering%250Acommands%2520without%2520the%2520necessity%2520of%2520touching%2520something.MediaPipe%2520supports%2520one%2520of%250Athe%2520powerful%2520frameworks%2520in%2520real-time%2520hand%2520tracking%2520capabilities%2520for%2520the%2520ability%250Ato%2520capture%2520and%2520preprocess%2520hand%2520movements%252C%2520which%2520increases%2520the%2520accuracy%2520of%2520the%250Agesture%2520recognition%2520system.%2520Actually%252C%2520the%2520integration%2520of%2520CNN%2520with%2520the%2520MediaPipe%250Aresults%2520in%2520higher%2520efficiency%2520in%2520using%2520the%2520model%2520of%2520real-time%2520processing.The%250Aaccuracy%2520achieved%2520by%2520the%2520model%2520on%2520ASL%2520datasets%2520is%252099.12%255C%2525.The%2520model%2520was%2520tested%250Ausing%2520American%2520Sign%2520Language%2520%2528ASL%2529%2520datasets.%2520The%2520results%2520were%2520then%2520compared%2520to%250Athose%2520of%2520existing%2520methods%2520to%2520evaluate%2520how%2520well%2520it%2520performed%252C%2520using%2520established%250Aevaluation%2520techniques.%2520The%2520system%2520will%2520have%2520applications%2520in%2520the%2520communication%252C%250Aeducation%252C%2520and%2520accessibility%2520domains.%2520Making%2520systems%2520such%2520as%2520described%2520in%2520this%250Apaper%2520even%2520better%2520will%2520assist%2520people%2520with%2520hearing%2520impairment%2520and%2520make%2520things%250Aaccessible%2520to%2520them.%2520We%2520tested%2520the%2520recognition%2520and%2520translation%2520performance%2520on%2520an%250AASL%2520dataset%2520and%2520achieved%2520better%2520accuracy%2520over%2520previous%2520models.It%2520is%2520meant%2520to%250Athe%2520research%2520is%2520to%2520identify%2520the%2520characters%2520that%2520American%2520signs%2520recognize%2520using%250Ahand%2520images%2520taken%2520from%2520a%2520web%2520camera%2520by%2520based%2520on%2520mediapipe%2520and%2520CNNs%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2406.03729v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Enhancing%20Sign%20Language%20Detection%20through%20Mediapipe%20and%20Convolutional%0A%20%20Neural%20Networks%20%28CNN%29&entry.906535625=Aditya%20Raj%20Verma%20and%20Gagandeep%20Singh%20and%20Karnim%20Meghwal%20and%20Banawath%20Ramji%20and%20Praveen%20Kumar%20Dadheech&entry.1292438233=%20%20This%20research%20combines%20MediaPipe%20and%20CNNs%20for%20the%20efficient%20and%20accurate%0Ainterpretation%20of%20ASL%20dataset%20for%20the%20real-time%20detection%20of%20sign%20language.%20The%0Asystem%20presented%20here%20captures%20and%20processes%20hands%27%20gestures%20in%20real%20time.%20the%0Aintended%20purpose%20was%20to%20create%20a%20very%20easy%2C%20accurate%2C%20and%20fast%20way%20of%20entering%0Acommands%20without%20the%20necessity%20of%20touching%20something.MediaPipe%20supports%20one%20of%0Athe%20powerful%20frameworks%20in%20real-time%20hand%20tracking%20capabilities%20for%20the%20ability%0Ato%20capture%20and%20preprocess%20hand%20movements%2C%20which%20increases%20the%20accuracy%20of%20the%0Agesture%20recognition%20system.%20Actually%2C%20the%20integration%20of%20CNN%20with%20the%20MediaPipe%0Aresults%20in%20higher%20efficiency%20in%20using%20the%20model%20of%20real-time%20processing.The%0Aaccuracy%20achieved%20by%20the%20model%20on%20ASL%20datasets%20is%2099.12%5C%25.The%20model%20was%20tested%0Ausing%20American%20Sign%20Language%20%28ASL%29%20datasets.%20The%20results%20were%20then%20compared%20to%0Athose%20of%20existing%20methods%20to%20evaluate%20how%20well%20it%20performed%2C%20using%20established%0Aevaluation%20techniques.%20The%20system%20will%20have%20applications%20in%20the%20communication%2C%0Aeducation%2C%20and%20accessibility%20domains.%20Making%20systems%20such%20as%20described%20in%20this%0Apaper%20even%20better%20will%20assist%20people%20with%20hearing%20impairment%20and%20make%20things%0Aaccessible%20to%20them.%20We%20tested%20the%20recognition%20and%20translation%20performance%20on%20an%0AASL%20dataset%20and%20achieved%20better%20accuracy%20over%20previous%20models.It%20is%20meant%20to%0Athe%20research%20is%20to%20identify%20the%20characters%20that%20American%20signs%20recognize%20using%0Ahand%20images%20taken%20from%20a%20web%20camera%20by%20based%20on%20mediapipe%20and%20CNNs%0A&entry.1838667208=http%3A//arxiv.org/abs/2406.03729v2&entry.124074799=Read"},
{"title": "Structured Deep Neural Networks-Based Backstepping Trajectory Tracking\n  Control for Lagrangian Systems", "author": "Jiajun Qian and Liang Xu and Xiaoqiang Ren and Xiaofan Wang", "abstract": "  Deep neural networks (DNN) are increasingly being used to learn controllers\ndue to their excellent approximation capabilities. However, their black-box\nnature poses significant challenges to closed-loop stability guarantees and\nperformance analysis. In this paper, we introduce a structured DNN-based\ncontroller for the trajectory tracking control of Lagrangian systems using\nbacking techniques. By properly designing neural network structures, the\nproposed controller can ensure closed-loop stability for any compatible neural\nnetwork parameters. In addition, improved control performance can be achieved\nby further optimizing neural network parameters. Besides, we provide explicit\nupper bounds on tracking errors in terms of controller parameters, which allows\nus to achieve the desired tracking performance by properly selecting the\ncontroller parameters. Furthermore, when system models are unknown, we propose\nan improved Lagrangian neural network (LNN) structure to learn the system\ndynamics and design the controller. We show that in the presence of model\napproximation errors and external disturbances, the closed-loop stability and\ntracking control performance can still be guaranteed. The effectiveness of the\nproposed approach is demonstrated through simulations.\n", "link": "http://arxiv.org/abs/2403.00381v2", "date": "2024-08-27", "relevancy": 1.9409, "topK": [{"title": "Safe and efficient training of a control agent", "link": "https://patents.google.com/patent/US11709462B2/en", "similarity": 0.4916}, {"title": "Boosted dynamic neural networks", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/26302", "similarity": 0.4861}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.4671}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Structured%20Deep%20Neural%20Networks-Based%20Backstepping%20Trajectory%20Tracking%0A%20%20Control%20for%20Lagrangian%20Systems&body=Title%3A%20Structured%20Deep%20Neural%20Networks-Based%20Backstepping%20Trajectory%20Tracking%0A%20%20Control%20for%20Lagrangian%20Systems%0AAuthor%3A%20Jiajun%20Qian%20and%20Liang%20Xu%20and%20Xiaoqiang%20Ren%20and%20Xiaofan%20Wang%0AAbstract%3A%20%20%20Deep%20neural%20networks%20%28DNN%29%20are%20increasingly%20being%20used%20to%20learn%20controllers%0Adue%20to%20their%20excellent%20approximation%20capabilities.%20However%2C%20their%20black-box%0Anature%20poses%20significant%20challenges%20to%20closed-loop%20stability%20guarantees%20and%0Aperformance%20analysis.%20In%20this%20paper%2C%20we%20introduce%20a%20structured%20DNN-based%0Acontroller%20for%20the%20trajectory%20tracking%20control%20of%20Lagrangian%20systems%20using%0Abacking%20techniques.%20By%20properly%20designing%20neural%20network%20structures%2C%20the%0Aproposed%20controller%20can%20ensure%20closed-loop%20stability%20for%20any%20compatible%20neural%0Anetwork%20parameters.%20In%20addition%2C%20improved%20control%20performance%20can%20be%20achieved%0Aby%20further%20optimizing%20neural%20network%20parameters.%20Besides%2C%20we%20provide%20explicit%0Aupper%20bounds%20on%20tracking%20errors%20in%20terms%20of%20controller%20parameters%2C%20which%20allows%0Aus%20to%20achieve%20the%20desired%20tracking%20performance%20by%20properly%20selecting%20the%0Acontroller%20parameters.%20Furthermore%2C%20when%20system%20models%20are%20unknown%2C%20we%20propose%0Aan%20improved%20Lagrangian%20neural%20network%20%28LNN%29%20structure%20to%20learn%20the%20system%0Adynamics%20and%20design%20the%20controller.%20We%20show%20that%20in%20the%20presence%20of%20model%0Aapproximation%20errors%20and%20external%20disturbances%2C%20the%20closed-loop%20stability%20and%0Atracking%20control%20performance%20can%20still%20be%20guaranteed.%20The%20effectiveness%20of%20the%0Aproposed%20approach%20is%20demonstrated%20through%20simulations.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2403.00381v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DStructured%2520Deep%2520Neural%2520Networks-Based%2520Backstepping%2520Trajectory%2520Tracking%250A%2520%2520Control%2520for%2520Lagrangian%2520Systems%26entry.906535625%3DJiajun%2520Qian%2520and%2520Liang%2520Xu%2520and%2520Xiaoqiang%2520Ren%2520and%2520Xiaofan%2520Wang%26entry.1292438233%3D%2520%2520Deep%2520neural%2520networks%2520%2528DNN%2529%2520are%2520increasingly%2520being%2520used%2520to%2520learn%2520controllers%250Adue%2520to%2520their%2520excellent%2520approximation%2520capabilities.%2520However%252C%2520their%2520black-box%250Anature%2520poses%2520significant%2520challenges%2520to%2520closed-loop%2520stability%2520guarantees%2520and%250Aperformance%2520analysis.%2520In%2520this%2520paper%252C%2520we%2520introduce%2520a%2520structured%2520DNN-based%250Acontroller%2520for%2520the%2520trajectory%2520tracking%2520control%2520of%2520Lagrangian%2520systems%2520using%250Abacking%2520techniques.%2520By%2520properly%2520designing%2520neural%2520network%2520structures%252C%2520the%250Aproposed%2520controller%2520can%2520ensure%2520closed-loop%2520stability%2520for%2520any%2520compatible%2520neural%250Anetwork%2520parameters.%2520In%2520addition%252C%2520improved%2520control%2520performance%2520can%2520be%2520achieved%250Aby%2520further%2520optimizing%2520neural%2520network%2520parameters.%2520Besides%252C%2520we%2520provide%2520explicit%250Aupper%2520bounds%2520on%2520tracking%2520errors%2520in%2520terms%2520of%2520controller%2520parameters%252C%2520which%2520allows%250Aus%2520to%2520achieve%2520the%2520desired%2520tracking%2520performance%2520by%2520properly%2520selecting%2520the%250Acontroller%2520parameters.%2520Furthermore%252C%2520when%2520system%2520models%2520are%2520unknown%252C%2520we%2520propose%250Aan%2520improved%2520Lagrangian%2520neural%2520network%2520%2528LNN%2529%2520structure%2520to%2520learn%2520the%2520system%250Adynamics%2520and%2520design%2520the%2520controller.%2520We%2520show%2520that%2520in%2520the%2520presence%2520of%2520model%250Aapproximation%2520errors%2520and%2520external%2520disturbances%252C%2520the%2520closed-loop%2520stability%2520and%250Atracking%2520control%2520performance%2520can%2520still%2520be%2520guaranteed.%2520The%2520effectiveness%2520of%2520the%250Aproposed%2520approach%2520is%2520demonstrated%2520through%2520simulations.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2403.00381v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Structured%20Deep%20Neural%20Networks-Based%20Backstepping%20Trajectory%20Tracking%0A%20%20Control%20for%20Lagrangian%20Systems&entry.906535625=Jiajun%20Qian%20and%20Liang%20Xu%20and%20Xiaoqiang%20Ren%20and%20Xiaofan%20Wang&entry.1292438233=%20%20Deep%20neural%20networks%20%28DNN%29%20are%20increasingly%20being%20used%20to%20learn%20controllers%0Adue%20to%20their%20excellent%20approximation%20capabilities.%20However%2C%20their%20black-box%0Anature%20poses%20significant%20challenges%20to%20closed-loop%20stability%20guarantees%20and%0Aperformance%20analysis.%20In%20this%20paper%2C%20we%20introduce%20a%20structured%20DNN-based%0Acontroller%20for%20the%20trajectory%20tracking%20control%20of%20Lagrangian%20systems%20using%0Abacking%20techniques.%20By%20properly%20designing%20neural%20network%20structures%2C%20the%0Aproposed%20controller%20can%20ensure%20closed-loop%20stability%20for%20any%20compatible%20neural%0Anetwork%20parameters.%20In%20addition%2C%20improved%20control%20performance%20can%20be%20achieved%0Aby%20further%20optimizing%20neural%20network%20parameters.%20Besides%2C%20we%20provide%20explicit%0Aupper%20bounds%20on%20tracking%20errors%20in%20terms%20of%20controller%20parameters%2C%20which%20allows%0Aus%20to%20achieve%20the%20desired%20tracking%20performance%20by%20properly%20selecting%20the%0Acontroller%20parameters.%20Furthermore%2C%20when%20system%20models%20are%20unknown%2C%20we%20propose%0Aan%20improved%20Lagrangian%20neural%20network%20%28LNN%29%20structure%20to%20learn%20the%20system%0Adynamics%20and%20design%20the%20controller.%20We%20show%20that%20in%20the%20presence%20of%20model%0Aapproximation%20errors%20and%20external%20disturbances%2C%20the%20closed-loop%20stability%20and%0Atracking%20control%20performance%20can%20still%20be%20guaranteed.%20The%20effectiveness%20of%20the%0Aproposed%20approach%20is%20demonstrated%20through%20simulations.%0A&entry.1838667208=http%3A//arxiv.org/abs/2403.00381v2&entry.124074799=Read"},
{"title": "Localising the Seizure Onset Zone from Single-Pulse Electrical\n  Stimulation Responses with a CNN Transformer", "author": "Jamie Norris and Aswin Chari and Dorien van Blooijs and Gerald Cooray and Karl Friston and Martin Tisdall and Richard Rosch", "abstract": "  Epilepsy is one of the most common neurological disorders, often requiring\nsurgical intervention when medication fails to control seizures. For effective\nsurgical outcomes, precise localisation of the epileptogenic focus - often\napproximated through the Seizure Onset Zone (SOZ) - is critical yet remains a\nchallenge. Active probing through electrical stimulation is already standard\nclinical practice for identifying epileptogenic areas. Our study advances the\napplication of deep learning for SOZ localisation using Single-Pulse Electrical\nStimulation (SPES) responses, with two key contributions. Firstly, we implement\nan existing deep learning model to compare two SPES analysis paradigms:\ndivergent and convergent. These paradigms evaluate outward and inward effective\nconnections, respectively. We assess the generalisability of these models to\nunseen patients and electrode placements using held-out test sets. Our findings\nreveal a notable improvement in moving from a divergent (AUROC: 0.574) to a\nconvergent approach (AUROC: 0.666), marking the first application of the latter\nin this context. Secondly, we demonstrate the efficacy of CNN Transformers with\ncross-channel attention in handling heterogeneous electrode placements,\nincreasing the AUROC to 0.730. These findings represent a significant step in\nmodelling patient-specific intracranial EEG electrode placements in SPES.\nFuture work will explore integrating these models into clinical decision-making\nprocesses to bridge the gap between deep learning research and practical\nhealthcare applications.\n", "link": "http://arxiv.org/abs/2403.20324v3", "date": "2024-08-27", "relevancy": 1.9364, "topK": [{"title": "DDM-NET: End-to-end learning of keypoint feature Detection, Description and Matching for 3D localization", "link": "https://arxiv.org/abs/2212.04575", "similarity": 0.487}, {"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.4852}, {"title": "SDXS: Real-Time One-Step Latent Diffusion Models with Image Conditions", "link": "http://arxiv.org/abs/2403.16627v1", "similarity": 0.4741}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Localising%20the%20Seizure%20Onset%20Zone%20from%20Single-Pulse%20Electrical%0A%20%20Stimulation%20Responses%20with%20a%20CNN%20Transformer&body=Title%3A%20Localising%20the%20Seizure%20Onset%20Zone%20from%20Single-Pulse%20Electrical%0A%20%20Stimulation%20Responses%20with%20a%20CNN%20Transformer%0AAuthor%3A%20Jamie%20Norris%20and%20Aswin%20Chari%20and%20Dorien%20van%20Blooijs%20and%20Gerald%20Cooray%20and%20Karl%20Friston%20and%20Martin%20Tisdall%20and%20Richard%20Rosch%0AAbstract%3A%20%20%20Epilepsy%20is%20one%20of%20the%20most%20common%20neurological%20disorders%2C%20often%20requiring%0Asurgical%20intervention%20when%20medication%20fails%20to%20control%20seizures.%20For%20effective%0Asurgical%20outcomes%2C%20precise%20localisation%20of%20the%20epileptogenic%20focus%20-%20often%0Aapproximated%20through%20the%20Seizure%20Onset%20Zone%20%28SOZ%29%20-%20is%20critical%20yet%20remains%20a%0Achallenge.%20Active%20probing%20through%20electrical%20stimulation%20is%20already%20standard%0Aclinical%20practice%20for%20identifying%20epileptogenic%20areas.%20Our%20study%20advances%20the%0Aapplication%20of%20deep%20learning%20for%20SOZ%20localisation%20using%20Single-Pulse%20Electrical%0AStimulation%20%28SPES%29%20responses%2C%20with%20two%20key%20contributions.%20Firstly%2C%20we%20implement%0Aan%20existing%20deep%20learning%20model%20to%20compare%20two%20SPES%20analysis%20paradigms%3A%0Adivergent%20and%20convergent.%20These%20paradigms%20evaluate%20outward%20and%20inward%20effective%0Aconnections%2C%20respectively.%20We%20assess%20the%20generalisability%20of%20these%20models%20to%0Aunseen%20patients%20and%20electrode%20placements%20using%20held-out%20test%20sets.%20Our%20findings%0Areveal%20a%20notable%20improvement%20in%20moving%20from%20a%20divergent%20%28AUROC%3A%200.574%29%20to%20a%0Aconvergent%20approach%20%28AUROC%3A%200.666%29%2C%20marking%20the%20first%20application%20of%20the%20latter%0Ain%20this%20context.%20Secondly%2C%20we%20demonstrate%20the%20efficacy%20of%20CNN%20Transformers%20with%0Across-channel%20attention%20in%20handling%20heterogeneous%20electrode%20placements%2C%0Aincreasing%20the%20AUROC%20to%200.730.%20These%20findings%20represent%20a%20significant%20step%20in%0Amodelling%20patient-specific%20intracranial%20EEG%20electrode%20placements%20in%20SPES.%0AFuture%20work%20will%20explore%20integrating%20these%20models%20into%20clinical%20decision-making%0Aprocesses%20to%20bridge%20the%20gap%20between%20deep%20learning%20research%20and%20practical%0Ahealthcare%20applications.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2403.20324v3%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DLocalising%2520the%2520Seizure%2520Onset%2520Zone%2520from%2520Single-Pulse%2520Electrical%250A%2520%2520Stimulation%2520Responses%2520with%2520a%2520CNN%2520Transformer%26entry.906535625%3DJamie%2520Norris%2520and%2520Aswin%2520Chari%2520and%2520Dorien%2520van%2520Blooijs%2520and%2520Gerald%2520Cooray%2520and%2520Karl%2520Friston%2520and%2520Martin%2520Tisdall%2520and%2520Richard%2520Rosch%26entry.1292438233%3D%2520%2520Epilepsy%2520is%2520one%2520of%2520the%2520most%2520common%2520neurological%2520disorders%252C%2520often%2520requiring%250Asurgical%2520intervention%2520when%2520medication%2520fails%2520to%2520control%2520seizures.%2520For%2520effective%250Asurgical%2520outcomes%252C%2520precise%2520localisation%2520of%2520the%2520epileptogenic%2520focus%2520-%2520often%250Aapproximated%2520through%2520the%2520Seizure%2520Onset%2520Zone%2520%2528SOZ%2529%2520-%2520is%2520critical%2520yet%2520remains%2520a%250Achallenge.%2520Active%2520probing%2520through%2520electrical%2520stimulation%2520is%2520already%2520standard%250Aclinical%2520practice%2520for%2520identifying%2520epileptogenic%2520areas.%2520Our%2520study%2520advances%2520the%250Aapplication%2520of%2520deep%2520learning%2520for%2520SOZ%2520localisation%2520using%2520Single-Pulse%2520Electrical%250AStimulation%2520%2528SPES%2529%2520responses%252C%2520with%2520two%2520key%2520contributions.%2520Firstly%252C%2520we%2520implement%250Aan%2520existing%2520deep%2520learning%2520model%2520to%2520compare%2520two%2520SPES%2520analysis%2520paradigms%253A%250Adivergent%2520and%2520convergent.%2520These%2520paradigms%2520evaluate%2520outward%2520and%2520inward%2520effective%250Aconnections%252C%2520respectively.%2520We%2520assess%2520the%2520generalisability%2520of%2520these%2520models%2520to%250Aunseen%2520patients%2520and%2520electrode%2520placements%2520using%2520held-out%2520test%2520sets.%2520Our%2520findings%250Areveal%2520a%2520notable%2520improvement%2520in%2520moving%2520from%2520a%2520divergent%2520%2528AUROC%253A%25200.574%2529%2520to%2520a%250Aconvergent%2520approach%2520%2528AUROC%253A%25200.666%2529%252C%2520marking%2520the%2520first%2520application%2520of%2520the%2520latter%250Ain%2520this%2520context.%2520Secondly%252C%2520we%2520demonstrate%2520the%2520efficacy%2520of%2520CNN%2520Transformers%2520with%250Across-channel%2520attention%2520in%2520handling%2520heterogeneous%2520electrode%2520placements%252C%250Aincreasing%2520the%2520AUROC%2520to%25200.730.%2520These%2520findings%2520represent%2520a%2520significant%2520step%2520in%250Amodelling%2520patient-specific%2520intracranial%2520EEG%2520electrode%2520placements%2520in%2520SPES.%250AFuture%2520work%2520will%2520explore%2520integrating%2520these%2520models%2520into%2520clinical%2520decision-making%250Aprocesses%2520to%2520bridge%2520the%2520gap%2520between%2520deep%2520learning%2520research%2520and%2520practical%250Ahealthcare%2520applications.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2403.20324v3%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Localising%20the%20Seizure%20Onset%20Zone%20from%20Single-Pulse%20Electrical%0A%20%20Stimulation%20Responses%20with%20a%20CNN%20Transformer&entry.906535625=Jamie%20Norris%20and%20Aswin%20Chari%20and%20Dorien%20van%20Blooijs%20and%20Gerald%20Cooray%20and%20Karl%20Friston%20and%20Martin%20Tisdall%20and%20Richard%20Rosch&entry.1292438233=%20%20Epilepsy%20is%20one%20of%20the%20most%20common%20neurological%20disorders%2C%20often%20requiring%0Asurgical%20intervention%20when%20medication%20fails%20to%20control%20seizures.%20For%20effective%0Asurgical%20outcomes%2C%20precise%20localisation%20of%20the%20epileptogenic%20focus%20-%20often%0Aapproximated%20through%20the%20Seizure%20Onset%20Zone%20%28SOZ%29%20-%20is%20critical%20yet%20remains%20a%0Achallenge.%20Active%20probing%20through%20electrical%20stimulation%20is%20already%20standard%0Aclinical%20practice%20for%20identifying%20epileptogenic%20areas.%20Our%20study%20advances%20the%0Aapplication%20of%20deep%20learning%20for%20SOZ%20localisation%20using%20Single-Pulse%20Electrical%0AStimulation%20%28SPES%29%20responses%2C%20with%20two%20key%20contributions.%20Firstly%2C%20we%20implement%0Aan%20existing%20deep%20learning%20model%20to%20compare%20two%20SPES%20analysis%20paradigms%3A%0Adivergent%20and%20convergent.%20These%20paradigms%20evaluate%20outward%20and%20inward%20effective%0Aconnections%2C%20respectively.%20We%20assess%20the%20generalisability%20of%20these%20models%20to%0Aunseen%20patients%20and%20electrode%20placements%20using%20held-out%20test%20sets.%20Our%20findings%0Areveal%20a%20notable%20improvement%20in%20moving%20from%20a%20divergent%20%28AUROC%3A%200.574%29%20to%20a%0Aconvergent%20approach%20%28AUROC%3A%200.666%29%2C%20marking%20the%20first%20application%20of%20the%20latter%0Ain%20this%20context.%20Secondly%2C%20we%20demonstrate%20the%20efficacy%20of%20CNN%20Transformers%20with%0Across-channel%20attention%20in%20handling%20heterogeneous%20electrode%20placements%2C%0Aincreasing%20the%20AUROC%20to%200.730.%20These%20findings%20represent%20a%20significant%20step%20in%0Amodelling%20patient-specific%20intracranial%20EEG%20electrode%20placements%20in%20SPES.%0AFuture%20work%20will%20explore%20integrating%20these%20models%20into%20clinical%20decision-making%0Aprocesses%20to%20bridge%20the%20gap%20between%20deep%20learning%20research%20and%20practical%0Ahealthcare%20applications.%0A&entry.1838667208=http%3A//arxiv.org/abs/2403.20324v3&entry.124074799=Read"},
{"title": "AnomalousPatchCore: Exploring the Use of Anomalous Samples in Industrial\n  Anomaly Detection", "author": "Mykhailo Koshil and Tilman Wegener and Detlef Mentrup and Simone Frintrop and Christian Wilms", "abstract": "  Visual inspection, or industrial anomaly detection, is one of the most common\nquality control types in manufacturing. The task is to identify the presence of\nan anomaly given an image, e.g., a missing component on an image of a circuit\nboard, for subsequent manual inspection. While industrial anomaly detection has\nseen a surge in recent years, most anomaly detection methods still utilize\nknowledge only from normal samples, failing to leverage the information from\nthe frequently available anomalous samples. Additionally, they heavily rely on\nvery general feature extractors pre-trained on common image classification\ndatasets. In this paper, we address these shortcomings and propose the new\nanomaly detection system AnomalousPatchCore~(APC) based on a feature extractor\nfine-tuned with normal and anomalous in-domain samples and a subsequent memory\nbank for identifying unusual features. To fine-tune the feature extractor in\nAPC, we propose three auxiliary tasks that address the different aspects of\nanomaly detection~(classification vs. localization) and mitigate the effect of\nthe imbalance between normal and anomalous samples. Our extensive evaluation on\nthe MVTec dataset shows that APC outperforms state-of-the-art systems in\ndetecting anomalies, which is especially important in industrial anomaly\ndetection given the subsequent manual inspection. In detailed ablation studies,\nwe further investigate the properties of our APC.\n", "link": "http://arxiv.org/abs/2408.15113v1", "date": "2024-08-27", "relevancy": 1.9364, "topK": [{"title": "Breadcrumbs: Adversarial class-balanced sampling for long-tailed recognition", "link": "https://link.springer.com/chapter/10.1007/978-3-031-20053-3_37", "similarity": 0.5038}, {"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.4707}, {"title": "Multimodal-Conditioned Latent Diffusion Models for Fashion Image Editing", "link": "http://arxiv.org/abs/2403.14828v2", "similarity": 0.4684}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20AnomalousPatchCore%3A%20Exploring%20the%20Use%20of%20Anomalous%20Samples%20in%20Industrial%0A%20%20Anomaly%20Detection&body=Title%3A%20AnomalousPatchCore%3A%20Exploring%20the%20Use%20of%20Anomalous%20Samples%20in%20Industrial%0A%20%20Anomaly%20Detection%0AAuthor%3A%20Mykhailo%20Koshil%20and%20Tilman%20Wegener%20and%20Detlef%20Mentrup%20and%20Simone%20Frintrop%20and%20Christian%20Wilms%0AAbstract%3A%20%20%20Visual%20inspection%2C%20or%20industrial%20anomaly%20detection%2C%20is%20one%20of%20the%20most%20common%0Aquality%20control%20types%20in%20manufacturing.%20The%20task%20is%20to%20identify%20the%20presence%20of%0Aan%20anomaly%20given%20an%20image%2C%20e.g.%2C%20a%20missing%20component%20on%20an%20image%20of%20a%20circuit%0Aboard%2C%20for%20subsequent%20manual%20inspection.%20While%20industrial%20anomaly%20detection%20has%0Aseen%20a%20surge%20in%20recent%20years%2C%20most%20anomaly%20detection%20methods%20still%20utilize%0Aknowledge%20only%20from%20normal%20samples%2C%20failing%20to%20leverage%20the%20information%20from%0Athe%20frequently%20available%20anomalous%20samples.%20Additionally%2C%20they%20heavily%20rely%20on%0Avery%20general%20feature%20extractors%20pre-trained%20on%20common%20image%20classification%0Adatasets.%20In%20this%20paper%2C%20we%20address%20these%20shortcomings%20and%20propose%20the%20new%0Aanomaly%20detection%20system%20AnomalousPatchCore~%28APC%29%20based%20on%20a%20feature%20extractor%0Afine-tuned%20with%20normal%20and%20anomalous%20in-domain%20samples%20and%20a%20subsequent%20memory%0Abank%20for%20identifying%20unusual%20features.%20To%20fine-tune%20the%20feature%20extractor%20in%0AAPC%2C%20we%20propose%20three%20auxiliary%20tasks%20that%20address%20the%20different%20aspects%20of%0Aanomaly%20detection~%28classification%20vs.%20localization%29%20and%20mitigate%20the%20effect%20of%0Athe%20imbalance%20between%20normal%20and%20anomalous%20samples.%20Our%20extensive%20evaluation%20on%0Athe%20MVTec%20dataset%20shows%20that%20APC%20outperforms%20state-of-the-art%20systems%20in%0Adetecting%20anomalies%2C%20which%20is%20especially%20important%20in%20industrial%20anomaly%0Adetection%20given%20the%20subsequent%20manual%20inspection.%20In%20detailed%20ablation%20studies%2C%0Awe%20further%20investigate%20the%20properties%20of%20our%20APC.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2408.15113v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DAnomalousPatchCore%253A%2520Exploring%2520the%2520Use%2520of%2520Anomalous%2520Samples%2520in%2520Industrial%250A%2520%2520Anomaly%2520Detection%26entry.906535625%3DMykhailo%2520Koshil%2520and%2520Tilman%2520Wegener%2520and%2520Detlef%2520Mentrup%2520and%2520Simone%2520Frintrop%2520and%2520Christian%2520Wilms%26entry.1292438233%3D%2520%2520Visual%2520inspection%252C%2520or%2520industrial%2520anomaly%2520detection%252C%2520is%2520one%2520of%2520the%2520most%2520common%250Aquality%2520control%2520types%2520in%2520manufacturing.%2520The%2520task%2520is%2520to%2520identify%2520the%2520presence%2520of%250Aan%2520anomaly%2520given%2520an%2520image%252C%2520e.g.%252C%2520a%2520missing%2520component%2520on%2520an%2520image%2520of%2520a%2520circuit%250Aboard%252C%2520for%2520subsequent%2520manual%2520inspection.%2520While%2520industrial%2520anomaly%2520detection%2520has%250Aseen%2520a%2520surge%2520in%2520recent%2520years%252C%2520most%2520anomaly%2520detection%2520methods%2520still%2520utilize%250Aknowledge%2520only%2520from%2520normal%2520samples%252C%2520failing%2520to%2520leverage%2520the%2520information%2520from%250Athe%2520frequently%2520available%2520anomalous%2520samples.%2520Additionally%252C%2520they%2520heavily%2520rely%2520on%250Avery%2520general%2520feature%2520extractors%2520pre-trained%2520on%2520common%2520image%2520classification%250Adatasets.%2520In%2520this%2520paper%252C%2520we%2520address%2520these%2520shortcomings%2520and%2520propose%2520the%2520new%250Aanomaly%2520detection%2520system%2520AnomalousPatchCore~%2528APC%2529%2520based%2520on%2520a%2520feature%2520extractor%250Afine-tuned%2520with%2520normal%2520and%2520anomalous%2520in-domain%2520samples%2520and%2520a%2520subsequent%2520memory%250Abank%2520for%2520identifying%2520unusual%2520features.%2520To%2520fine-tune%2520the%2520feature%2520extractor%2520in%250AAPC%252C%2520we%2520propose%2520three%2520auxiliary%2520tasks%2520that%2520address%2520the%2520different%2520aspects%2520of%250Aanomaly%2520detection~%2528classification%2520vs.%2520localization%2529%2520and%2520mitigate%2520the%2520effect%2520of%250Athe%2520imbalance%2520between%2520normal%2520and%2520anomalous%2520samples.%2520Our%2520extensive%2520evaluation%2520on%250Athe%2520MVTec%2520dataset%2520shows%2520that%2520APC%2520outperforms%2520state-of-the-art%2520systems%2520in%250Adetecting%2520anomalies%252C%2520which%2520is%2520especially%2520important%2520in%2520industrial%2520anomaly%250Adetection%2520given%2520the%2520subsequent%2520manual%2520inspection.%2520In%2520detailed%2520ablation%2520studies%252C%250Awe%2520further%2520investigate%2520the%2520properties%2520of%2520our%2520APC.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2408.15113v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=AnomalousPatchCore%3A%20Exploring%20the%20Use%20of%20Anomalous%20Samples%20in%20Industrial%0A%20%20Anomaly%20Detection&entry.906535625=Mykhailo%20Koshil%20and%20Tilman%20Wegener%20and%20Detlef%20Mentrup%20and%20Simone%20Frintrop%20and%20Christian%20Wilms&entry.1292438233=%20%20Visual%20inspection%2C%20or%20industrial%20anomaly%20detection%2C%20is%20one%20of%20the%20most%20common%0Aquality%20control%20types%20in%20manufacturing.%20The%20task%20is%20to%20identify%20the%20presence%20of%0Aan%20anomaly%20given%20an%20image%2C%20e.g.%2C%20a%20missing%20component%20on%20an%20image%20of%20a%20circuit%0Aboard%2C%20for%20subsequent%20manual%20inspection.%20While%20industrial%20anomaly%20detection%20has%0Aseen%20a%20surge%20in%20recent%20years%2C%20most%20anomaly%20detection%20methods%20still%20utilize%0Aknowledge%20only%20from%20normal%20samples%2C%20failing%20to%20leverage%20the%20information%20from%0Athe%20frequently%20available%20anomalous%20samples.%20Additionally%2C%20they%20heavily%20rely%20on%0Avery%20general%20feature%20extractors%20pre-trained%20on%20common%20image%20classification%0Adatasets.%20In%20this%20paper%2C%20we%20address%20these%20shortcomings%20and%20propose%20the%20new%0Aanomaly%20detection%20system%20AnomalousPatchCore~%28APC%29%20based%20on%20a%20feature%20extractor%0Afine-tuned%20with%20normal%20and%20anomalous%20in-domain%20samples%20and%20a%20subsequent%20memory%0Abank%20for%20identifying%20unusual%20features.%20To%20fine-tune%20the%20feature%20extractor%20in%0AAPC%2C%20we%20propose%20three%20auxiliary%20tasks%20that%20address%20the%20different%20aspects%20of%0Aanomaly%20detection~%28classification%20vs.%20localization%29%20and%20mitigate%20the%20effect%20of%0Athe%20imbalance%20between%20normal%20and%20anomalous%20samples.%20Our%20extensive%20evaluation%20on%0Athe%20MVTec%20dataset%20shows%20that%20APC%20outperforms%20state-of-the-art%20systems%20in%0Adetecting%20anomalies%2C%20which%20is%20especially%20important%20in%20industrial%20anomaly%0Adetection%20given%20the%20subsequent%20manual%20inspection.%20In%20detailed%20ablation%20studies%2C%0Awe%20further%20investigate%20the%20properties%20of%20our%20APC.%0A&entry.1838667208=http%3A//arxiv.org/abs/2408.15113v1&entry.124074799=Read"},
{"title": "Assessing Lower Limb Strength using Internet-of-Things Enabled Chair", "author": "Chelsea Yeh and Hanna Kaitlin Dy and Phillip Schodinger and Hudson Kaleb Dy", "abstract": "  This project describes the application of the technologies of Machine\nLearning and Internet-of-Things to assess the lower limb strength of\nindividuals undergoing rehabilitation or therapy. Specifically, it seeks to\nmeasure and assess the progress of individuals by sensors attached to chairs\nand processing the data through Google GPU Tensorflow CoLab. Pressure sensors\nare attached to various locations on a chair, including but not limited to the\nseating area, backrest, hand rests, and legs. Sensor data from the individual\nperforming both sit-to-stand transition and stand-to-sit transition provides a\ntime series dataset regarding the pressure distribution and vibratory motion on\nthe chair. The dataset and timing information can then be fed into a machine\nlearning model to estimate the relative strength and weakness during various\nphases of the movement.\n", "link": "http://arxiv.org/abs/2209.04042v3", "date": "2024-08-27", "relevancy": 1.9318, "topK": [{"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.5269}, {"title": "Egocentric Computer Vision for Hands-Free Robotic Wheelchair Navigation", "link": "https://link.springer.com/article/10.1007/s10846-023-01807-4", "similarity": 0.5247}, {"title": "Weakly-guided self-supervised pretraining for temporal activity detection", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/25189", "similarity": 0.4237}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Assessing%20Lower%20Limb%20Strength%20using%20Internet-of-Things%20Enabled%20Chair&body=Title%3A%20Assessing%20Lower%20Limb%20Strength%20using%20Internet-of-Things%20Enabled%20Chair%0AAuthor%3A%20Chelsea%20Yeh%20and%20Hanna%20Kaitlin%20Dy%20and%20Phillip%20Schodinger%20and%20Hudson%20Kaleb%20Dy%0AAbstract%3A%20%20%20This%20project%20describes%20the%20application%20of%20the%20technologies%20of%20Machine%0ALearning%20and%20Internet-of-Things%20to%20assess%20the%20lower%20limb%20strength%20of%0Aindividuals%20undergoing%20rehabilitation%20or%20therapy.%20Specifically%2C%20it%20seeks%20to%0Ameasure%20and%20assess%20the%20progress%20of%20individuals%20by%20sensors%20attached%20to%20chairs%0Aand%20processing%20the%20data%20through%20Google%20GPU%20Tensorflow%20CoLab.%20Pressure%20sensors%0Aare%20attached%20to%20various%20locations%20on%20a%20chair%2C%20including%20but%20not%20limited%20to%20the%0Aseating%20area%2C%20backrest%2C%20hand%20rests%2C%20and%20legs.%20Sensor%20data%20from%20the%20individual%0Aperforming%20both%20sit-to-stand%20transition%20and%20stand-to-sit%20transition%20provides%20a%0Atime%20series%20dataset%20regarding%20the%20pressure%20distribution%20and%20vibratory%20motion%20on%0Athe%20chair.%20The%20dataset%20and%20timing%20information%20can%20then%20be%20fed%20into%20a%20machine%0Alearning%20model%20to%20estimate%20the%20relative%20strength%20and%20weakness%20during%20various%0Aphases%20of%20the%20movement.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2209.04042v3%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DAssessing%2520Lower%2520Limb%2520Strength%2520using%2520Internet-of-Things%2520Enabled%2520Chair%26entry.906535625%3DChelsea%2520Yeh%2520and%2520Hanna%2520Kaitlin%2520Dy%2520and%2520Phillip%2520Schodinger%2520and%2520Hudson%2520Kaleb%2520Dy%26entry.1292438233%3D%2520%2520This%2520project%2520describes%2520the%2520application%2520of%2520the%2520technologies%2520of%2520Machine%250ALearning%2520and%2520Internet-of-Things%2520to%2520assess%2520the%2520lower%2520limb%2520strength%2520of%250Aindividuals%2520undergoing%2520rehabilitation%2520or%2520therapy.%2520Specifically%252C%2520it%2520seeks%2520to%250Ameasure%2520and%2520assess%2520the%2520progress%2520of%2520individuals%2520by%2520sensors%2520attached%2520to%2520chairs%250Aand%2520processing%2520the%2520data%2520through%2520Google%2520GPU%2520Tensorflow%2520CoLab.%2520Pressure%2520sensors%250Aare%2520attached%2520to%2520various%2520locations%2520on%2520a%2520chair%252C%2520including%2520but%2520not%2520limited%2520to%2520the%250Aseating%2520area%252C%2520backrest%252C%2520hand%2520rests%252C%2520and%2520legs.%2520Sensor%2520data%2520from%2520the%2520individual%250Aperforming%2520both%2520sit-to-stand%2520transition%2520and%2520stand-to-sit%2520transition%2520provides%2520a%250Atime%2520series%2520dataset%2520regarding%2520the%2520pressure%2520distribution%2520and%2520vibratory%2520motion%2520on%250Athe%2520chair.%2520The%2520dataset%2520and%2520timing%2520information%2520can%2520then%2520be%2520fed%2520into%2520a%2520machine%250Alearning%2520model%2520to%2520estimate%2520the%2520relative%2520strength%2520and%2520weakness%2520during%2520various%250Aphases%2520of%2520the%2520movement.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2209.04042v3%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Assessing%20Lower%20Limb%20Strength%20using%20Internet-of-Things%20Enabled%20Chair&entry.906535625=Chelsea%20Yeh%20and%20Hanna%20Kaitlin%20Dy%20and%20Phillip%20Schodinger%20and%20Hudson%20Kaleb%20Dy&entry.1292438233=%20%20This%20project%20describes%20the%20application%20of%20the%20technologies%20of%20Machine%0ALearning%20and%20Internet-of-Things%20to%20assess%20the%20lower%20limb%20strength%20of%0Aindividuals%20undergoing%20rehabilitation%20or%20therapy.%20Specifically%2C%20it%20seeks%20to%0Ameasure%20and%20assess%20the%20progress%20of%20individuals%20by%20sensors%20attached%20to%20chairs%0Aand%20processing%20the%20data%20through%20Google%20GPU%20Tensorflow%20CoLab.%20Pressure%20sensors%0Aare%20attached%20to%20various%20locations%20on%20a%20chair%2C%20including%20but%20not%20limited%20to%20the%0Aseating%20area%2C%20backrest%2C%20hand%20rests%2C%20and%20legs.%20Sensor%20data%20from%20the%20individual%0Aperforming%20both%20sit-to-stand%20transition%20and%20stand-to-sit%20transition%20provides%20a%0Atime%20series%20dataset%20regarding%20the%20pressure%20distribution%20and%20vibratory%20motion%20on%0Athe%20chair.%20The%20dataset%20and%20timing%20information%20can%20then%20be%20fed%20into%20a%20machine%0Alearning%20model%20to%20estimate%20the%20relative%20strength%20and%20weakness%20during%20various%0Aphases%20of%20the%20movement.%0A&entry.1838667208=http%3A//arxiv.org/abs/2209.04042v3&entry.124074799=Read"},
{"title": "Compressed Federated Reinforcement Learning with a Generative Model", "author": "Ali Beikmohammadi and Sarit Khirirat and Sindri Magn\u00fasson", "abstract": "  Reinforcement learning has recently gained unprecedented popularity, yet it\nstill grapples with sample inefficiency. Addressing this challenge, federated\nreinforcement learning (FedRL) has emerged, wherein agents collaboratively\nlearn a single policy by aggregating local estimations. However, this\naggregation step incurs significant communication costs. In this paper, we\npropose CompFedRL, a communication-efficient FedRL approach incorporating both\n\\textit{periodic aggregation} and (direct/error-feedback) compression\nmechanisms. Specifically, we consider compressed federated $Q$-learning with a\ngenerative model setup, where a central server learns an optimal $Q$-function\nby periodically aggregating compressed $Q$-estimates from local agents. For the\nfirst time, we characterize the impact of these two mechanisms (which have\nremained elusive) by providing a finite-time analysis of our algorithm,\ndemonstrating strong convergence behaviors when utilizing either direct or\nerror-feedback compression. Our bounds indicate improved solution accuracy\nconcerning the number of agents and other federated hyperparameters while\nsimultaneously reducing communication costs. To corroborate our theory, we also\nconduct in-depth numerical experiments to verify our findings, considering\nTop-$K$ and Sparsified-$K$ sparsification operators.\n", "link": "http://arxiv.org/abs/2404.10635v5", "date": "2024-08-27", "relevancy": 1.931, "topK": [{"title": "Scaling Rectified Flow Transformers for High-Resolution Image Synthesis", "link": "http://arxiv.org/abs/2403.03206v1", "similarity": 0.498}, {"title": "UGG: Unified Generative Grasping", "link": "https://arxiv.org/abs/2311.16917", "similarity": 0.482}, {"title": "RTG-SLAM: Real-time 3D Reconstruction at Scale using Gaussian Splatting", "link": "http://arxiv.org/abs/2404.19706v1", "similarity": 0.4774}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Compressed%20Federated%20Reinforcement%20Learning%20with%20a%20Generative%20Model&body=Title%3A%20Compressed%20Federated%20Reinforcement%20Learning%20with%20a%20Generative%20Model%0AAuthor%3A%20Ali%20Beikmohammadi%20and%20Sarit%20Khirirat%20and%20Sindri%20Magn%C3%BAsson%0AAbstract%3A%20%20%20Reinforcement%20learning%20has%20recently%20gained%20unprecedented%20popularity%2C%20yet%20it%0Astill%20grapples%20with%20sample%20inefficiency.%20Addressing%20this%20challenge%2C%20federated%0Areinforcement%20learning%20%28FedRL%29%20has%20emerged%2C%20wherein%20agents%20collaboratively%0Alearn%20a%20single%20policy%20by%20aggregating%20local%20estimations.%20However%2C%20this%0Aaggregation%20step%20incurs%20significant%20communication%20costs.%20In%20this%20paper%2C%20we%0Apropose%20CompFedRL%2C%20a%20communication-efficient%20FedRL%20approach%20incorporating%20both%0A%5Ctextit%7Bperiodic%20aggregation%7D%20and%20%28direct/error-feedback%29%20compression%0Amechanisms.%20Specifically%2C%20we%20consider%20compressed%20federated%20%24Q%24-learning%20with%20a%0Agenerative%20model%20setup%2C%20where%20a%20central%20server%20learns%20an%20optimal%20%24Q%24-function%0Aby%20periodically%20aggregating%20compressed%20%24Q%24-estimates%20from%20local%20agents.%20For%20the%0Afirst%20time%2C%20we%20characterize%20the%20impact%20of%20these%20two%20mechanisms%20%28which%20have%0Aremained%20elusive%29%20by%20providing%20a%20finite-time%20analysis%20of%20our%20algorithm%2C%0Ademonstrating%20strong%20convergence%20behaviors%20when%20utilizing%20either%20direct%20or%0Aerror-feedback%20compression.%20Our%20bounds%20indicate%20improved%20solution%20accuracy%0Aconcerning%20the%20number%20of%20agents%20and%20other%20federated%20hyperparameters%20while%0Asimultaneously%20reducing%20communication%20costs.%20To%20corroborate%20our%20theory%2C%20we%20also%0Aconduct%20in-depth%20numerical%20experiments%20to%20verify%20our%20findings%2C%20considering%0ATop-%24K%24%20and%20Sparsified-%24K%24%20sparsification%20operators.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2404.10635v5%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DCompressed%2520Federated%2520Reinforcement%2520Learning%2520with%2520a%2520Generative%2520Model%26entry.906535625%3DAli%2520Beikmohammadi%2520and%2520Sarit%2520Khirirat%2520and%2520Sindri%2520Magn%25C3%25BAsson%26entry.1292438233%3D%2520%2520Reinforcement%2520learning%2520has%2520recently%2520gained%2520unprecedented%2520popularity%252C%2520yet%2520it%250Astill%2520grapples%2520with%2520sample%2520inefficiency.%2520Addressing%2520this%2520challenge%252C%2520federated%250Areinforcement%2520learning%2520%2528FedRL%2529%2520has%2520emerged%252C%2520wherein%2520agents%2520collaboratively%250Alearn%2520a%2520single%2520policy%2520by%2520aggregating%2520local%2520estimations.%2520However%252C%2520this%250Aaggregation%2520step%2520incurs%2520significant%2520communication%2520costs.%2520In%2520this%2520paper%252C%2520we%250Apropose%2520CompFedRL%252C%2520a%2520communication-efficient%2520FedRL%2520approach%2520incorporating%2520both%250A%255Ctextit%257Bperiodic%2520aggregation%257D%2520and%2520%2528direct/error-feedback%2529%2520compression%250Amechanisms.%2520Specifically%252C%2520we%2520consider%2520compressed%2520federated%2520%2524Q%2524-learning%2520with%2520a%250Agenerative%2520model%2520setup%252C%2520where%2520a%2520central%2520server%2520learns%2520an%2520optimal%2520%2524Q%2524-function%250Aby%2520periodically%2520aggregating%2520compressed%2520%2524Q%2524-estimates%2520from%2520local%2520agents.%2520For%2520the%250Afirst%2520time%252C%2520we%2520characterize%2520the%2520impact%2520of%2520these%2520two%2520mechanisms%2520%2528which%2520have%250Aremained%2520elusive%2529%2520by%2520providing%2520a%2520finite-time%2520analysis%2520of%2520our%2520algorithm%252C%250Ademonstrating%2520strong%2520convergence%2520behaviors%2520when%2520utilizing%2520either%2520direct%2520or%250Aerror-feedback%2520compression.%2520Our%2520bounds%2520indicate%2520improved%2520solution%2520accuracy%250Aconcerning%2520the%2520number%2520of%2520agents%2520and%2520other%2520federated%2520hyperparameters%2520while%250Asimultaneously%2520reducing%2520communication%2520costs.%2520To%2520corroborate%2520our%2520theory%252C%2520we%2520also%250Aconduct%2520in-depth%2520numerical%2520experiments%2520to%2520verify%2520our%2520findings%252C%2520considering%250ATop-%2524K%2524%2520and%2520Sparsified-%2524K%2524%2520sparsification%2520operators.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2404.10635v5%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Compressed%20Federated%20Reinforcement%20Learning%20with%20a%20Generative%20Model&entry.906535625=Ali%20Beikmohammadi%20and%20Sarit%20Khirirat%20and%20Sindri%20Magn%C3%BAsson&entry.1292438233=%20%20Reinforcement%20learning%20has%20recently%20gained%20unprecedented%20popularity%2C%20yet%20it%0Astill%20grapples%20with%20sample%20inefficiency.%20Addressing%20this%20challenge%2C%20federated%0Areinforcement%20learning%20%28FedRL%29%20has%20emerged%2C%20wherein%20agents%20collaboratively%0Alearn%20a%20single%20policy%20by%20aggregating%20local%20estimations.%20However%2C%20this%0Aaggregation%20step%20incurs%20significant%20communication%20costs.%20In%20this%20paper%2C%20we%0Apropose%20CompFedRL%2C%20a%20communication-efficient%20FedRL%20approach%20incorporating%20both%0A%5Ctextit%7Bperiodic%20aggregation%7D%20and%20%28direct/error-feedback%29%20compression%0Amechanisms.%20Specifically%2C%20we%20consider%20compressed%20federated%20%24Q%24-learning%20with%20a%0Agenerative%20model%20setup%2C%20where%20a%20central%20server%20learns%20an%20optimal%20%24Q%24-function%0Aby%20periodically%20aggregating%20compressed%20%24Q%24-estimates%20from%20local%20agents.%20For%20the%0Afirst%20time%2C%20we%20characterize%20the%20impact%20of%20these%20two%20mechanisms%20%28which%20have%0Aremained%20elusive%29%20by%20providing%20a%20finite-time%20analysis%20of%20our%20algorithm%2C%0Ademonstrating%20strong%20convergence%20behaviors%20when%20utilizing%20either%20direct%20or%0Aerror-feedback%20compression.%20Our%20bounds%20indicate%20improved%20solution%20accuracy%0Aconcerning%20the%20number%20of%20agents%20and%20other%20federated%20hyperparameters%20while%0Asimultaneously%20reducing%20communication%20costs.%20To%20corroborate%20our%20theory%2C%20we%20also%0Aconduct%20in-depth%20numerical%20experiments%20to%20verify%20our%20findings%2C%20considering%0ATop-%24K%24%20and%20Sparsified-%24K%24%20sparsification%20operators.%0A&entry.1838667208=http%3A//arxiv.org/abs/2404.10635v5&entry.124074799=Read"},
{"title": "Urdu Digital Text Word Optical Character Recognition Using Permuted Auto\n  Regressive Sequence Modeling", "author": "Ahmed Mustafa and Ijlal Baig and Hasan Sajid", "abstract": "  This research paper introduces an innovative word-level Optical Character\nRecognition (OCR) model specifically designed for digital Urdu text\nrecognition. Utilizing transformer-based architectures and attention\nmechanisms, the model was trained on a comprehensive dataset of approximately\n160,000 Urdu text images, achieving a character error rate (CER) of 0.178,\nwhich highlights its superior accuracy in recognizing Urdu characters. The\nmodel's strength lies in its unique architecture, incorporating the permuted\nautoregressive sequence (PARSeq) model, which allows for context-aware\ninference and iterative refinement by leveraging bidirectional context\ninformation to enhance recognition accuracy. Furthermore, its capability to\nhandle a diverse range of Urdu text styles, fonts, and variations enhances its\napplicability in real-world scenarios. Despite its promising results, the model\nhas some limitations, such as difficulty with blurred images, non-horizontal\norientations, and overlays of patterns, lines, or other text, which can\noccasionally lead to suboptimal performance. Additionally, trailing or\nfollowing punctuation marks can introduce noise into the recognition process.\nAddressing these challenges will be a focus of future research, aiming to\nrefine the model further, explore data augmentation techniques, optimize\nhyperparameters, and integrate contextual improvements for more accurate and\nefficient Urdu text recognition.\n", "link": "http://arxiv.org/abs/2408.15119v1", "date": "2024-08-27", "relevancy": 1.9228, "topK": [{"title": "Scaling Rectified Flow Transformers for High-Resolution Image Synthesis", "link": "http://arxiv.org/abs/2403.03206v1", "similarity": 0.513}, {"title": "DressCode: Autoregressively Sewing and Generating Garments from Text\n  Guidance", "link": "http://arxiv.org/abs/2401.16465v3", "similarity": 0.477}, {"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.4715}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Urdu%20Digital%20Text%20Word%20Optical%20Character%20Recognition%20Using%20Permuted%20Auto%0A%20%20Regressive%20Sequence%20Modeling&body=Title%3A%20Urdu%20Digital%20Text%20Word%20Optical%20Character%20Recognition%20Using%20Permuted%20Auto%0A%20%20Regressive%20Sequence%20Modeling%0AAuthor%3A%20Ahmed%20Mustafa%20and%20Ijlal%20Baig%20and%20Hasan%20Sajid%0AAbstract%3A%20%20%20This%20research%20paper%20introduces%20an%20innovative%20word-level%20Optical%20Character%0ARecognition%20%28OCR%29%20model%20specifically%20designed%20for%20digital%20Urdu%20text%0Arecognition.%20Utilizing%20transformer-based%20architectures%20and%20attention%0Amechanisms%2C%20the%20model%20was%20trained%20on%20a%20comprehensive%20dataset%20of%20approximately%0A160%2C000%20Urdu%20text%20images%2C%20achieving%20a%20character%20error%20rate%20%28CER%29%20of%200.178%2C%0Awhich%20highlights%20its%20superior%20accuracy%20in%20recognizing%20Urdu%20characters.%20The%0Amodel%27s%20strength%20lies%20in%20its%20unique%20architecture%2C%20incorporating%20the%20permuted%0Aautoregressive%20sequence%20%28PARSeq%29%20model%2C%20which%20allows%20for%20context-aware%0Ainference%20and%20iterative%20refinement%20by%20leveraging%20bidirectional%20context%0Ainformation%20to%20enhance%20recognition%20accuracy.%20Furthermore%2C%20its%20capability%20to%0Ahandle%20a%20diverse%20range%20of%20Urdu%20text%20styles%2C%20fonts%2C%20and%20variations%20enhances%20its%0Aapplicability%20in%20real-world%20scenarios.%20Despite%20its%20promising%20results%2C%20the%20model%0Ahas%20some%20limitations%2C%20such%20as%20difficulty%20with%20blurred%20images%2C%20non-horizontal%0Aorientations%2C%20and%20overlays%20of%20patterns%2C%20lines%2C%20or%20other%20text%2C%20which%20can%0Aoccasionally%20lead%20to%20suboptimal%20performance.%20Additionally%2C%20trailing%20or%0Afollowing%20punctuation%20marks%20can%20introduce%20noise%20into%20the%20recognition%20process.%0AAddressing%20these%20challenges%20will%20be%20a%20focus%20of%20future%20research%2C%20aiming%20to%0Arefine%20the%20model%20further%2C%20explore%20data%20augmentation%20techniques%2C%20optimize%0Ahyperparameters%2C%20and%20integrate%20contextual%20improvements%20for%20more%20accurate%20and%0Aefficient%20Urdu%20text%20recognition.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2408.15119v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DUrdu%2520Digital%2520Text%2520Word%2520Optical%2520Character%2520Recognition%2520Using%2520Permuted%2520Auto%250A%2520%2520Regressive%2520Sequence%2520Modeling%26entry.906535625%3DAhmed%2520Mustafa%2520and%2520Ijlal%2520Baig%2520and%2520Hasan%2520Sajid%26entry.1292438233%3D%2520%2520This%2520research%2520paper%2520introduces%2520an%2520innovative%2520word-level%2520Optical%2520Character%250ARecognition%2520%2528OCR%2529%2520model%2520specifically%2520designed%2520for%2520digital%2520Urdu%2520text%250Arecognition.%2520Utilizing%2520transformer-based%2520architectures%2520and%2520attention%250Amechanisms%252C%2520the%2520model%2520was%2520trained%2520on%2520a%2520comprehensive%2520dataset%2520of%2520approximately%250A160%252C000%2520Urdu%2520text%2520images%252C%2520achieving%2520a%2520character%2520error%2520rate%2520%2528CER%2529%2520of%25200.178%252C%250Awhich%2520highlights%2520its%2520superior%2520accuracy%2520in%2520recognizing%2520Urdu%2520characters.%2520The%250Amodel%2527s%2520strength%2520lies%2520in%2520its%2520unique%2520architecture%252C%2520incorporating%2520the%2520permuted%250Aautoregressive%2520sequence%2520%2528PARSeq%2529%2520model%252C%2520which%2520allows%2520for%2520context-aware%250Ainference%2520and%2520iterative%2520refinement%2520by%2520leveraging%2520bidirectional%2520context%250Ainformation%2520to%2520enhance%2520recognition%2520accuracy.%2520Furthermore%252C%2520its%2520capability%2520to%250Ahandle%2520a%2520diverse%2520range%2520of%2520Urdu%2520text%2520styles%252C%2520fonts%252C%2520and%2520variations%2520enhances%2520its%250Aapplicability%2520in%2520real-world%2520scenarios.%2520Despite%2520its%2520promising%2520results%252C%2520the%2520model%250Ahas%2520some%2520limitations%252C%2520such%2520as%2520difficulty%2520with%2520blurred%2520images%252C%2520non-horizontal%250Aorientations%252C%2520and%2520overlays%2520of%2520patterns%252C%2520lines%252C%2520or%2520other%2520text%252C%2520which%2520can%250Aoccasionally%2520lead%2520to%2520suboptimal%2520performance.%2520Additionally%252C%2520trailing%2520or%250Afollowing%2520punctuation%2520marks%2520can%2520introduce%2520noise%2520into%2520the%2520recognition%2520process.%250AAddressing%2520these%2520challenges%2520will%2520be%2520a%2520focus%2520of%2520future%2520research%252C%2520aiming%2520to%250Arefine%2520the%2520model%2520further%252C%2520explore%2520data%2520augmentation%2520techniques%252C%2520optimize%250Ahyperparameters%252C%2520and%2520integrate%2520contextual%2520improvements%2520for%2520more%2520accurate%2520and%250Aefficient%2520Urdu%2520text%2520recognition.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2408.15119v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Urdu%20Digital%20Text%20Word%20Optical%20Character%20Recognition%20Using%20Permuted%20Auto%0A%20%20Regressive%20Sequence%20Modeling&entry.906535625=Ahmed%20Mustafa%20and%20Ijlal%20Baig%20and%20Hasan%20Sajid&entry.1292438233=%20%20This%20research%20paper%20introduces%20an%20innovative%20word-level%20Optical%20Character%0ARecognition%20%28OCR%29%20model%20specifically%20designed%20for%20digital%20Urdu%20text%0Arecognition.%20Utilizing%20transformer-based%20architectures%20and%20attention%0Amechanisms%2C%20the%20model%20was%20trained%20on%20a%20comprehensive%20dataset%20of%20approximately%0A160%2C000%20Urdu%20text%20images%2C%20achieving%20a%20character%20error%20rate%20%28CER%29%20of%200.178%2C%0Awhich%20highlights%20its%20superior%20accuracy%20in%20recognizing%20Urdu%20characters.%20The%0Amodel%27s%20strength%20lies%20in%20its%20unique%20architecture%2C%20incorporating%20the%20permuted%0Aautoregressive%20sequence%20%28PARSeq%29%20model%2C%20which%20allows%20for%20context-aware%0Ainference%20and%20iterative%20refinement%20by%20leveraging%20bidirectional%20context%0Ainformation%20to%20enhance%20recognition%20accuracy.%20Furthermore%2C%20its%20capability%20to%0Ahandle%20a%20diverse%20range%20of%20Urdu%20text%20styles%2C%20fonts%2C%20and%20variations%20enhances%20its%0Aapplicability%20in%20real-world%20scenarios.%20Despite%20its%20promising%20results%2C%20the%20model%0Ahas%20some%20limitations%2C%20such%20as%20difficulty%20with%20blurred%20images%2C%20non-horizontal%0Aorientations%2C%20and%20overlays%20of%20patterns%2C%20lines%2C%20or%20other%20text%2C%20which%20can%0Aoccasionally%20lead%20to%20suboptimal%20performance.%20Additionally%2C%20trailing%20or%0Afollowing%20punctuation%20marks%20can%20introduce%20noise%20into%20the%20recognition%20process.%0AAddressing%20these%20challenges%20will%20be%20a%20focus%20of%20future%20research%2C%20aiming%20to%0Arefine%20the%20model%20further%2C%20explore%20data%20augmentation%20techniques%2C%20optimize%0Ahyperparameters%2C%20and%20integrate%20contextual%20improvements%20for%20more%20accurate%20and%0Aefficient%20Urdu%20text%20recognition.%0A&entry.1838667208=http%3A//arxiv.org/abs/2408.15119v1&entry.124074799=Read"},
{"title": "Earth Observation Satellite Scheduling with Graph Neural Networks", "author": "Antoine Jacquet and Guillaume Infantes and Nicolas Meuleau and Emmanuel Benazera and St\u00e9phanie Roussel and Vincent Baudoui and Jonathan Guerra", "abstract": "  The Earth Observation Satellite Planning (EOSP) is a difficult optimization\nproblem with considerable practical interest. A set of requested observations\nmust be scheduled on an agile Earth observation satellite while respecting\nconstraints on their visibility window, as well as maneuver constraints that\nimpose varying delays between successive observations. In addition, the problem\nis largely oversubscribed: there are much more candidate observations than what\ncan possibly be achieved. Therefore, one must select the set of observations\nthat will be performed while maximizing their weighted cumulative benefit, and\npropose a feasible schedule for these observations. As previous work mostly\nfocused on heuristic and iterative search algorithms, this paper presents a new\ntechnique for selecting and scheduling observations based on Graph Neural\nNetworks (GNNs) and Deep Reinforcement Learning (DRL). GNNs are used to extract\nrelevant information from the graphs representing instances of the EOSP, and\nDRL drives the search for optimal schedules. Our simulations show that it is\nable to learn on small problem instances and generalize to larger real-world\ninstances, with very competitive performance compared to traditional\napproaches.\n", "link": "http://arxiv.org/abs/2408.15041v1", "date": "2024-08-27", "relevancy": 1.9203, "topK": [{"title": "RoboEXP: Action-Conditioned Scene Graph via Interactive Exploration for\n  Robotic Manipulation", "link": "http://arxiv.org/abs/2402.15487v1", "similarity": 0.5009}, {"title": "Self-supervised Photographic Image Layout Representation Learning", "link": "http://arxiv.org/abs/2403.03740v1", "similarity": 0.4856}, {"title": "Boosted dynamic neural networks", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/26302", "similarity": 0.4662}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Earth%20Observation%20Satellite%20Scheduling%20with%20Graph%20Neural%20Networks&body=Title%3A%20Earth%20Observation%20Satellite%20Scheduling%20with%20Graph%20Neural%20Networks%0AAuthor%3A%20Antoine%20Jacquet%20and%20Guillaume%20Infantes%20and%20Nicolas%20Meuleau%20and%20Emmanuel%20Benazera%20and%20St%C3%A9phanie%20Roussel%20and%20Vincent%20Baudoui%20and%20Jonathan%20Guerra%0AAbstract%3A%20%20%20The%20Earth%20Observation%20Satellite%20Planning%20%28EOSP%29%20is%20a%20difficult%20optimization%0Aproblem%20with%20considerable%20practical%20interest.%20A%20set%20of%20requested%20observations%0Amust%20be%20scheduled%20on%20an%20agile%20Earth%20observation%20satellite%20while%20respecting%0Aconstraints%20on%20their%20visibility%20window%2C%20as%20well%20as%20maneuver%20constraints%20that%0Aimpose%20varying%20delays%20between%20successive%20observations.%20In%20addition%2C%20the%20problem%0Ais%20largely%20oversubscribed%3A%20there%20are%20much%20more%20candidate%20observations%20than%20what%0Acan%20possibly%20be%20achieved.%20Therefore%2C%20one%20must%20select%20the%20set%20of%20observations%0Athat%20will%20be%20performed%20while%20maximizing%20their%20weighted%20cumulative%20benefit%2C%20and%0Apropose%20a%20feasible%20schedule%20for%20these%20observations.%20As%20previous%20work%20mostly%0Afocused%20on%20heuristic%20and%20iterative%20search%20algorithms%2C%20this%20paper%20presents%20a%20new%0Atechnique%20for%20selecting%20and%20scheduling%20observations%20based%20on%20Graph%20Neural%0ANetworks%20%28GNNs%29%20and%20Deep%20Reinforcement%20Learning%20%28DRL%29.%20GNNs%20are%20used%20to%20extract%0Arelevant%20information%20from%20the%20graphs%20representing%20instances%20of%20the%20EOSP%2C%20and%0ADRL%20drives%20the%20search%20for%20optimal%20schedules.%20Our%20simulations%20show%20that%20it%20is%0Aable%20to%20learn%20on%20small%20problem%20instances%20and%20generalize%20to%20larger%20real-world%0Ainstances%2C%20with%20very%20competitive%20performance%20compared%20to%20traditional%0Aapproaches.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2408.15041v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DEarth%2520Observation%2520Satellite%2520Scheduling%2520with%2520Graph%2520Neural%2520Networks%26entry.906535625%3DAntoine%2520Jacquet%2520and%2520Guillaume%2520Infantes%2520and%2520Nicolas%2520Meuleau%2520and%2520Emmanuel%2520Benazera%2520and%2520St%25C3%25A9phanie%2520Roussel%2520and%2520Vincent%2520Baudoui%2520and%2520Jonathan%2520Guerra%26entry.1292438233%3D%2520%2520The%2520Earth%2520Observation%2520Satellite%2520Planning%2520%2528EOSP%2529%2520is%2520a%2520difficult%2520optimization%250Aproblem%2520with%2520considerable%2520practical%2520interest.%2520A%2520set%2520of%2520requested%2520observations%250Amust%2520be%2520scheduled%2520on%2520an%2520agile%2520Earth%2520observation%2520satellite%2520while%2520respecting%250Aconstraints%2520on%2520their%2520visibility%2520window%252C%2520as%2520well%2520as%2520maneuver%2520constraints%2520that%250Aimpose%2520varying%2520delays%2520between%2520successive%2520observations.%2520In%2520addition%252C%2520the%2520problem%250Ais%2520largely%2520oversubscribed%253A%2520there%2520are%2520much%2520more%2520candidate%2520observations%2520than%2520what%250Acan%2520possibly%2520be%2520achieved.%2520Therefore%252C%2520one%2520must%2520select%2520the%2520set%2520of%2520observations%250Athat%2520will%2520be%2520performed%2520while%2520maximizing%2520their%2520weighted%2520cumulative%2520benefit%252C%2520and%250Apropose%2520a%2520feasible%2520schedule%2520for%2520these%2520observations.%2520As%2520previous%2520work%2520mostly%250Afocused%2520on%2520heuristic%2520and%2520iterative%2520search%2520algorithms%252C%2520this%2520paper%2520presents%2520a%2520new%250Atechnique%2520for%2520selecting%2520and%2520scheduling%2520observations%2520based%2520on%2520Graph%2520Neural%250ANetworks%2520%2528GNNs%2529%2520and%2520Deep%2520Reinforcement%2520Learning%2520%2528DRL%2529.%2520GNNs%2520are%2520used%2520to%2520extract%250Arelevant%2520information%2520from%2520the%2520graphs%2520representing%2520instances%2520of%2520the%2520EOSP%252C%2520and%250ADRL%2520drives%2520the%2520search%2520for%2520optimal%2520schedules.%2520Our%2520simulations%2520show%2520that%2520it%2520is%250Aable%2520to%2520learn%2520on%2520small%2520problem%2520instances%2520and%2520generalize%2520to%2520larger%2520real-world%250Ainstances%252C%2520with%2520very%2520competitive%2520performance%2520compared%2520to%2520traditional%250Aapproaches.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2408.15041v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Earth%20Observation%20Satellite%20Scheduling%20with%20Graph%20Neural%20Networks&entry.906535625=Antoine%20Jacquet%20and%20Guillaume%20Infantes%20and%20Nicolas%20Meuleau%20and%20Emmanuel%20Benazera%20and%20St%C3%A9phanie%20Roussel%20and%20Vincent%20Baudoui%20and%20Jonathan%20Guerra&entry.1292438233=%20%20The%20Earth%20Observation%20Satellite%20Planning%20%28EOSP%29%20is%20a%20difficult%20optimization%0Aproblem%20with%20considerable%20practical%20interest.%20A%20set%20of%20requested%20observations%0Amust%20be%20scheduled%20on%20an%20agile%20Earth%20observation%20satellite%20while%20respecting%0Aconstraints%20on%20their%20visibility%20window%2C%20as%20well%20as%20maneuver%20constraints%20that%0Aimpose%20varying%20delays%20between%20successive%20observations.%20In%20addition%2C%20the%20problem%0Ais%20largely%20oversubscribed%3A%20there%20are%20much%20more%20candidate%20observations%20than%20what%0Acan%20possibly%20be%20achieved.%20Therefore%2C%20one%20must%20select%20the%20set%20of%20observations%0Athat%20will%20be%20performed%20while%20maximizing%20their%20weighted%20cumulative%20benefit%2C%20and%0Apropose%20a%20feasible%20schedule%20for%20these%20observations.%20As%20previous%20work%20mostly%0Afocused%20on%20heuristic%20and%20iterative%20search%20algorithms%2C%20this%20paper%20presents%20a%20new%0Atechnique%20for%20selecting%20and%20scheduling%20observations%20based%20on%20Graph%20Neural%0ANetworks%20%28GNNs%29%20and%20Deep%20Reinforcement%20Learning%20%28DRL%29.%20GNNs%20are%20used%20to%20extract%0Arelevant%20information%20from%20the%20graphs%20representing%20instances%20of%20the%20EOSP%2C%20and%0ADRL%20drives%20the%20search%20for%20optimal%20schedules.%20Our%20simulations%20show%20that%20it%20is%0Aable%20to%20learn%20on%20small%20problem%20instances%20and%20generalize%20to%20larger%20real-world%0Ainstances%2C%20with%20very%20competitive%20performance%20compared%20to%20traditional%0Aapproaches.%0A&entry.1838667208=http%3A//arxiv.org/abs/2408.15041v1&entry.124074799=Read"},
{"title": "Bayesian Learning in a Nonlinear Multiscale State-Space Model", "author": "Nayely V\u00e9lez-Cruz and Manfred D. Laubichler", "abstract": "  The ubiquity of multiscale interactions in complex systems is\nwell-recognized, with development and heredity serving as a prime example of\nhow processes at different temporal scales influence one another. This work\nintroduces a novel multiscale state-space model to explore the dynamic\ninterplay between systems interacting across different time scales, with\nfeedback between each scale. We propose a Bayesian learning framework to\nestimate unknown states by learning the unknown process noise covariances\nwithin this multiscale model. We develop a Particle Gibbs with Ancestor\nSampling (PGAS) algorithm for inference and demonstrate through simulations the\nefficacy of our approach.\n", "link": "http://arxiv.org/abs/2408.06425v5", "date": "2024-08-27", "relevancy": 1.92, "topK": [{"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.5959}, {"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.4602}, {"title": "Genie: Generative Interactive Environments", "link": "http://arxiv.org/abs/2402.15391v1", "similarity": 0.4535}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Bayesian%20Learning%20in%20a%20Nonlinear%20Multiscale%20State-Space%20Model&body=Title%3A%20Bayesian%20Learning%20in%20a%20Nonlinear%20Multiscale%20State-Space%20Model%0AAuthor%3A%20Nayely%20V%C3%A9lez-Cruz%20and%20Manfred%20D.%20Laubichler%0AAbstract%3A%20%20%20The%20ubiquity%20of%20multiscale%20interactions%20in%20complex%20systems%20is%0Awell-recognized%2C%20with%20development%20and%20heredity%20serving%20as%20a%20prime%20example%20of%0Ahow%20processes%20at%20different%20temporal%20scales%20influence%20one%20another.%20This%20work%0Aintroduces%20a%20novel%20multiscale%20state-space%20model%20to%20explore%20the%20dynamic%0Ainterplay%20between%20systems%20interacting%20across%20different%20time%20scales%2C%20with%0Afeedback%20between%20each%20scale.%20We%20propose%20a%20Bayesian%20learning%20framework%20to%0Aestimate%20unknown%20states%20by%20learning%20the%20unknown%20process%20noise%20covariances%0Awithin%20this%20multiscale%20model.%20We%20develop%20a%20Particle%20Gibbs%20with%20Ancestor%0ASampling%20%28PGAS%29%20algorithm%20for%20inference%20and%20demonstrate%20through%20simulations%20the%0Aefficacy%20of%20our%20approach.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2408.06425v5%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DBayesian%2520Learning%2520in%2520a%2520Nonlinear%2520Multiscale%2520State-Space%2520Model%26entry.906535625%3DNayely%2520V%25C3%25A9lez-Cruz%2520and%2520Manfred%2520D.%2520Laubichler%26entry.1292438233%3D%2520%2520The%2520ubiquity%2520of%2520multiscale%2520interactions%2520in%2520complex%2520systems%2520is%250Awell-recognized%252C%2520with%2520development%2520and%2520heredity%2520serving%2520as%2520a%2520prime%2520example%2520of%250Ahow%2520processes%2520at%2520different%2520temporal%2520scales%2520influence%2520one%2520another.%2520This%2520work%250Aintroduces%2520a%2520novel%2520multiscale%2520state-space%2520model%2520to%2520explore%2520the%2520dynamic%250Ainterplay%2520between%2520systems%2520interacting%2520across%2520different%2520time%2520scales%252C%2520with%250Afeedback%2520between%2520each%2520scale.%2520We%2520propose%2520a%2520Bayesian%2520learning%2520framework%2520to%250Aestimate%2520unknown%2520states%2520by%2520learning%2520the%2520unknown%2520process%2520noise%2520covariances%250Awithin%2520this%2520multiscale%2520model.%2520We%2520develop%2520a%2520Particle%2520Gibbs%2520with%2520Ancestor%250ASampling%2520%2528PGAS%2529%2520algorithm%2520for%2520inference%2520and%2520demonstrate%2520through%2520simulations%2520the%250Aefficacy%2520of%2520our%2520approach.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2408.06425v5%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Bayesian%20Learning%20in%20a%20Nonlinear%20Multiscale%20State-Space%20Model&entry.906535625=Nayely%20V%C3%A9lez-Cruz%20and%20Manfred%20D.%20Laubichler&entry.1292438233=%20%20The%20ubiquity%20of%20multiscale%20interactions%20in%20complex%20systems%20is%0Awell-recognized%2C%20with%20development%20and%20heredity%20serving%20as%20a%20prime%20example%20of%0Ahow%20processes%20at%20different%20temporal%20scales%20influence%20one%20another.%20This%20work%0Aintroduces%20a%20novel%20multiscale%20state-space%20model%20to%20explore%20the%20dynamic%0Ainterplay%20between%20systems%20interacting%20across%20different%20time%20scales%2C%20with%0Afeedback%20between%20each%20scale.%20We%20propose%20a%20Bayesian%20learning%20framework%20to%0Aestimate%20unknown%20states%20by%20learning%20the%20unknown%20process%20noise%20covariances%0Awithin%20this%20multiscale%20model.%20We%20develop%20a%20Particle%20Gibbs%20with%20Ancestor%0ASampling%20%28PGAS%29%20algorithm%20for%20inference%20and%20demonstrate%20through%20simulations%20the%0Aefficacy%20of%20our%20approach.%0A&entry.1838667208=http%3A//arxiv.org/abs/2408.06425v5&entry.124074799=Read"},
{"title": "Evidence-Enhanced Triplet Generation Framework for Hallucination\n  Alleviation in Generative Question Answering", "author": "Haowei Du and Huishuai Zhang and Dongyan Zhao", "abstract": "  To address the hallucination in generative question answering (GQA) where the\nanswer can not be derived from the document, we propose a novel\nevidence-enhanced triplet generation framework, EATQA, encouraging the model to\npredict all the combinations of (Question, Evidence, Answer) triplet by\nflipping the source pair and the target label to understand their logical\nrelationships, i.e., predict Answer(A), Question(Q), and Evidence(E) given a\nQE, EA, and QA pairs, respectively. Furthermore, we bridge the distribution gap\nto distill the knowledge from evidence in inference stage. Our framework\nensures the model to learn the logical relation between query, evidence and\nanswer, which simultaneously improves the evidence generation and query\nanswering. In this paper, we apply EATQA to LLama and it outperforms other\nLLMs-based methods and hallucination mitigation approaches on two challenging\nGQA benchmarks. Further analysis shows that our method not only keeps prior\nknowledge within LLM, but also mitigates hallucination and generates faithful\nanswers.\n", "link": "http://arxiv.org/abs/2408.15037v1", "date": "2024-08-27", "relevancy": 1.917, "topK": [{"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.5462}, {"title": "Flexible Visual Recognition by Evidential Modeling of Confusion and Ignorance", "link": "http://openaccess.thecvf.com/content/ICCV2023/html/Fan_Flexible_Visual_Recognition_by_Evidential_Modeling_of_Confusion_and_Ignorance_ICCV_2023_paper.html", "similarity": 0.4687}, {"title": "Genie: Generative Interactive Environments", "link": "http://arxiv.org/abs/2402.15391v1", "similarity": 0.463}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Evidence-Enhanced%20Triplet%20Generation%20Framework%20for%20Hallucination%0A%20%20Alleviation%20in%20Generative%20Question%20Answering&body=Title%3A%20Evidence-Enhanced%20Triplet%20Generation%20Framework%20for%20Hallucination%0A%20%20Alleviation%20in%20Generative%20Question%20Answering%0AAuthor%3A%20Haowei%20Du%20and%20Huishuai%20Zhang%20and%20Dongyan%20Zhao%0AAbstract%3A%20%20%20To%20address%20the%20hallucination%20in%20generative%20question%20answering%20%28GQA%29%20where%20the%0Aanswer%20can%20not%20be%20derived%20from%20the%20document%2C%20we%20propose%20a%20novel%0Aevidence-enhanced%20triplet%20generation%20framework%2C%20EATQA%2C%20encouraging%20the%20model%20to%0Apredict%20all%20the%20combinations%20of%20%28Question%2C%20Evidence%2C%20Answer%29%20triplet%20by%0Aflipping%20the%20source%20pair%20and%20the%20target%20label%20to%20understand%20their%20logical%0Arelationships%2C%20i.e.%2C%20predict%20Answer%28A%29%2C%20Question%28Q%29%2C%20and%20Evidence%28E%29%20given%20a%0AQE%2C%20EA%2C%20and%20QA%20pairs%2C%20respectively.%20Furthermore%2C%20we%20bridge%20the%20distribution%20gap%0Ato%20distill%20the%20knowledge%20from%20evidence%20in%20inference%20stage.%20Our%20framework%0Aensures%20the%20model%20to%20learn%20the%20logical%20relation%20between%20query%2C%20evidence%20and%0Aanswer%2C%20which%20simultaneously%20improves%20the%20evidence%20generation%20and%20query%0Aanswering.%20In%20this%20paper%2C%20we%20apply%20EATQA%20to%20LLama%20and%20it%20outperforms%20other%0ALLMs-based%20methods%20and%20hallucination%20mitigation%20approaches%20on%20two%20challenging%0AGQA%20benchmarks.%20Further%20analysis%20shows%20that%20our%20method%20not%20only%20keeps%20prior%0Aknowledge%20within%20LLM%2C%20but%20also%20mitigates%20hallucination%20and%20generates%20faithful%0Aanswers.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2408.15037v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DEvidence-Enhanced%2520Triplet%2520Generation%2520Framework%2520for%2520Hallucination%250A%2520%2520Alleviation%2520in%2520Generative%2520Question%2520Answering%26entry.906535625%3DHaowei%2520Du%2520and%2520Huishuai%2520Zhang%2520and%2520Dongyan%2520Zhao%26entry.1292438233%3D%2520%2520To%2520address%2520the%2520hallucination%2520in%2520generative%2520question%2520answering%2520%2528GQA%2529%2520where%2520the%250Aanswer%2520can%2520not%2520be%2520derived%2520from%2520the%2520document%252C%2520we%2520propose%2520a%2520novel%250Aevidence-enhanced%2520triplet%2520generation%2520framework%252C%2520EATQA%252C%2520encouraging%2520the%2520model%2520to%250Apredict%2520all%2520the%2520combinations%2520of%2520%2528Question%252C%2520Evidence%252C%2520Answer%2529%2520triplet%2520by%250Aflipping%2520the%2520source%2520pair%2520and%2520the%2520target%2520label%2520to%2520understand%2520their%2520logical%250Arelationships%252C%2520i.e.%252C%2520predict%2520Answer%2528A%2529%252C%2520Question%2528Q%2529%252C%2520and%2520Evidence%2528E%2529%2520given%2520a%250AQE%252C%2520EA%252C%2520and%2520QA%2520pairs%252C%2520respectively.%2520Furthermore%252C%2520we%2520bridge%2520the%2520distribution%2520gap%250Ato%2520distill%2520the%2520knowledge%2520from%2520evidence%2520in%2520inference%2520stage.%2520Our%2520framework%250Aensures%2520the%2520model%2520to%2520learn%2520the%2520logical%2520relation%2520between%2520query%252C%2520evidence%2520and%250Aanswer%252C%2520which%2520simultaneously%2520improves%2520the%2520evidence%2520generation%2520and%2520query%250Aanswering.%2520In%2520this%2520paper%252C%2520we%2520apply%2520EATQA%2520to%2520LLama%2520and%2520it%2520outperforms%2520other%250ALLMs-based%2520methods%2520and%2520hallucination%2520mitigation%2520approaches%2520on%2520two%2520challenging%250AGQA%2520benchmarks.%2520Further%2520analysis%2520shows%2520that%2520our%2520method%2520not%2520only%2520keeps%2520prior%250Aknowledge%2520within%2520LLM%252C%2520but%2520also%2520mitigates%2520hallucination%2520and%2520generates%2520faithful%250Aanswers.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2408.15037v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Evidence-Enhanced%20Triplet%20Generation%20Framework%20for%20Hallucination%0A%20%20Alleviation%20in%20Generative%20Question%20Answering&entry.906535625=Haowei%20Du%20and%20Huishuai%20Zhang%20and%20Dongyan%20Zhao&entry.1292438233=%20%20To%20address%20the%20hallucination%20in%20generative%20question%20answering%20%28GQA%29%20where%20the%0Aanswer%20can%20not%20be%20derived%20from%20the%20document%2C%20we%20propose%20a%20novel%0Aevidence-enhanced%20triplet%20generation%20framework%2C%20EATQA%2C%20encouraging%20the%20model%20to%0Apredict%20all%20the%20combinations%20of%20%28Question%2C%20Evidence%2C%20Answer%29%20triplet%20by%0Aflipping%20the%20source%20pair%20and%20the%20target%20label%20to%20understand%20their%20logical%0Arelationships%2C%20i.e.%2C%20predict%20Answer%28A%29%2C%20Question%28Q%29%2C%20and%20Evidence%28E%29%20given%20a%0AQE%2C%20EA%2C%20and%20QA%20pairs%2C%20respectively.%20Furthermore%2C%20we%20bridge%20the%20distribution%20gap%0Ato%20distill%20the%20knowledge%20from%20evidence%20in%20inference%20stage.%20Our%20framework%0Aensures%20the%20model%20to%20learn%20the%20logical%20relation%20between%20query%2C%20evidence%20and%0Aanswer%2C%20which%20simultaneously%20improves%20the%20evidence%20generation%20and%20query%0Aanswering.%20In%20this%20paper%2C%20we%20apply%20EATQA%20to%20LLama%20and%20it%20outperforms%20other%0ALLMs-based%20methods%20and%20hallucination%20mitigation%20approaches%20on%20two%20challenging%0AGQA%20benchmarks.%20Further%20analysis%20shows%20that%20our%20method%20not%20only%20keeps%20prior%0Aknowledge%20within%20LLM%2C%20but%20also%20mitigates%20hallucination%20and%20generates%20faithful%0Aanswers.%0A&entry.1838667208=http%3A//arxiv.org/abs/2408.15037v1&entry.124074799=Read"},
{"title": "NoRA: Nested Low-Rank Adaptation for Efficient Fine-Tuning Large Models", "author": "Cheng Lin and Lujun Li and Dezhi Li and Jie Zou and Wei Xue and Yike Guo", "abstract": "  In this paper, we introduce Nested Low-Rank Adaptation (NoRA), a novel\napproach to parameter-efficient fine-tuning that extends the capabilities of\nLow-Rank Adaptation (LoRA) techniques. Vanilla LoRA overlooks pre-trained\nweight inheritance and still requires fine-tuning numerous parameters. To\naddresses these issues, our NoRA adopts a dual-layer nested structure with\nSingular Value Decomposition (SVD), effectively leveraging original matrix\nknowledge while reducing tunable parameters. Specifically, NoRA freezes the\nouter LoRA weights and utilizes an inner LoRA design, providing enhanced\ncontrol over model optimization. This approach allows the model to more\nprecisely adapt to specific tasks while maintaining a compact parameter space.\nBy freezing outer LoRA weights and using an inner LoRA design, NoRA enables\nprecise task adaptation with a compact parameter space. Evaluations on tasks\nincluding commonsense reasoning with large language models, fine-tuning\nvision-language models, and subject-driven generation demonstrate NoRA's\nsuperiority over LoRA and its variants. Code will be released upon acceptance.\n", "link": "http://arxiv.org/abs/2408.10280v2", "date": "2024-08-27", "relevancy": 1.9028, "topK": [{"title": "Tuning-Free Noise Rectification for High Fidelity Image-to-Video\n  Generation", "link": "http://arxiv.org/abs/2403.02827v1", "similarity": 0.4837}, {"title": "ToDo: Token Downsampling for Efficient Generation of High-Resolution\n  Images", "link": "http://arxiv.org/abs/2402.13573v2", "similarity": 0.4762}, {"title": "Scaling Rectified Flow Transformers for High-Resolution Image Synthesis", "link": "http://arxiv.org/abs/2403.03206v1", "similarity": 0.4543}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20NoRA%3A%20Nested%20Low-Rank%20Adaptation%20for%20Efficient%20Fine-Tuning%20Large%20Models&body=Title%3A%20NoRA%3A%20Nested%20Low-Rank%20Adaptation%20for%20Efficient%20Fine-Tuning%20Large%20Models%0AAuthor%3A%20Cheng%20Lin%20and%20Lujun%20Li%20and%20Dezhi%20Li%20and%20Jie%20Zou%20and%20Wei%20Xue%20and%20Yike%20Guo%0AAbstract%3A%20%20%20In%20this%20paper%2C%20we%20introduce%20Nested%20Low-Rank%20Adaptation%20%28NoRA%29%2C%20a%20novel%0Aapproach%20to%20parameter-efficient%20fine-tuning%20that%20extends%20the%20capabilities%20of%0ALow-Rank%20Adaptation%20%28LoRA%29%20techniques.%20Vanilla%20LoRA%20overlooks%20pre-trained%0Aweight%20inheritance%20and%20still%20requires%20fine-tuning%20numerous%20parameters.%20To%0Aaddresses%20these%20issues%2C%20our%20NoRA%20adopts%20a%20dual-layer%20nested%20structure%20with%0ASingular%20Value%20Decomposition%20%28SVD%29%2C%20effectively%20leveraging%20original%20matrix%0Aknowledge%20while%20reducing%20tunable%20parameters.%20Specifically%2C%20NoRA%20freezes%20the%0Aouter%20LoRA%20weights%20and%20utilizes%20an%20inner%20LoRA%20design%2C%20providing%20enhanced%0Acontrol%20over%20model%20optimization.%20This%20approach%20allows%20the%20model%20to%20more%0Aprecisely%20adapt%20to%20specific%20tasks%20while%20maintaining%20a%20compact%20parameter%20space.%0ABy%20freezing%20outer%20LoRA%20weights%20and%20using%20an%20inner%20LoRA%20design%2C%20NoRA%20enables%0Aprecise%20task%20adaptation%20with%20a%20compact%20parameter%20space.%20Evaluations%20on%20tasks%0Aincluding%20commonsense%20reasoning%20with%20large%20language%20models%2C%20fine-tuning%0Avision-language%20models%2C%20and%20subject-driven%20generation%20demonstrate%20NoRA%27s%0Asuperiority%20over%20LoRA%20and%20its%20variants.%20Code%20will%20be%20released%20upon%20acceptance.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2408.10280v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DNoRA%253A%2520Nested%2520Low-Rank%2520Adaptation%2520for%2520Efficient%2520Fine-Tuning%2520Large%2520Models%26entry.906535625%3DCheng%2520Lin%2520and%2520Lujun%2520Li%2520and%2520Dezhi%2520Li%2520and%2520Jie%2520Zou%2520and%2520Wei%2520Xue%2520and%2520Yike%2520Guo%26entry.1292438233%3D%2520%2520In%2520this%2520paper%252C%2520we%2520introduce%2520Nested%2520Low-Rank%2520Adaptation%2520%2528NoRA%2529%252C%2520a%2520novel%250Aapproach%2520to%2520parameter-efficient%2520fine-tuning%2520that%2520extends%2520the%2520capabilities%2520of%250ALow-Rank%2520Adaptation%2520%2528LoRA%2529%2520techniques.%2520Vanilla%2520LoRA%2520overlooks%2520pre-trained%250Aweight%2520inheritance%2520and%2520still%2520requires%2520fine-tuning%2520numerous%2520parameters.%2520To%250Aaddresses%2520these%2520issues%252C%2520our%2520NoRA%2520adopts%2520a%2520dual-layer%2520nested%2520structure%2520with%250ASingular%2520Value%2520Decomposition%2520%2528SVD%2529%252C%2520effectively%2520leveraging%2520original%2520matrix%250Aknowledge%2520while%2520reducing%2520tunable%2520parameters.%2520Specifically%252C%2520NoRA%2520freezes%2520the%250Aouter%2520LoRA%2520weights%2520and%2520utilizes%2520an%2520inner%2520LoRA%2520design%252C%2520providing%2520enhanced%250Acontrol%2520over%2520model%2520optimization.%2520This%2520approach%2520allows%2520the%2520model%2520to%2520more%250Aprecisely%2520adapt%2520to%2520specific%2520tasks%2520while%2520maintaining%2520a%2520compact%2520parameter%2520space.%250ABy%2520freezing%2520outer%2520LoRA%2520weights%2520and%2520using%2520an%2520inner%2520LoRA%2520design%252C%2520NoRA%2520enables%250Aprecise%2520task%2520adaptation%2520with%2520a%2520compact%2520parameter%2520space.%2520Evaluations%2520on%2520tasks%250Aincluding%2520commonsense%2520reasoning%2520with%2520large%2520language%2520models%252C%2520fine-tuning%250Avision-language%2520models%252C%2520and%2520subject-driven%2520generation%2520demonstrate%2520NoRA%2527s%250Asuperiority%2520over%2520LoRA%2520and%2520its%2520variants.%2520Code%2520will%2520be%2520released%2520upon%2520acceptance.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2408.10280v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=NoRA%3A%20Nested%20Low-Rank%20Adaptation%20for%20Efficient%20Fine-Tuning%20Large%20Models&entry.906535625=Cheng%20Lin%20and%20Lujun%20Li%20and%20Dezhi%20Li%20and%20Jie%20Zou%20and%20Wei%20Xue%20and%20Yike%20Guo&entry.1292438233=%20%20In%20this%20paper%2C%20we%20introduce%20Nested%20Low-Rank%20Adaptation%20%28NoRA%29%2C%20a%20novel%0Aapproach%20to%20parameter-efficient%20fine-tuning%20that%20extends%20the%20capabilities%20of%0ALow-Rank%20Adaptation%20%28LoRA%29%20techniques.%20Vanilla%20LoRA%20overlooks%20pre-trained%0Aweight%20inheritance%20and%20still%20requires%20fine-tuning%20numerous%20parameters.%20To%0Aaddresses%20these%20issues%2C%20our%20NoRA%20adopts%20a%20dual-layer%20nested%20structure%20with%0ASingular%20Value%20Decomposition%20%28SVD%29%2C%20effectively%20leveraging%20original%20matrix%0Aknowledge%20while%20reducing%20tunable%20parameters.%20Specifically%2C%20NoRA%20freezes%20the%0Aouter%20LoRA%20weights%20and%20utilizes%20an%20inner%20LoRA%20design%2C%20providing%20enhanced%0Acontrol%20over%20model%20optimization.%20This%20approach%20allows%20the%20model%20to%20more%0Aprecisely%20adapt%20to%20specific%20tasks%20while%20maintaining%20a%20compact%20parameter%20space.%0ABy%20freezing%20outer%20LoRA%20weights%20and%20using%20an%20inner%20LoRA%20design%2C%20NoRA%20enables%0Aprecise%20task%20adaptation%20with%20a%20compact%20parameter%20space.%20Evaluations%20on%20tasks%0Aincluding%20commonsense%20reasoning%20with%20large%20language%20models%2C%20fine-tuning%0Avision-language%20models%2C%20and%20subject-driven%20generation%20demonstrate%20NoRA%27s%0Asuperiority%20over%20LoRA%20and%20its%20variants.%20Code%20will%20be%20released%20upon%20acceptance.%0A&entry.1838667208=http%3A//arxiv.org/abs/2408.10280v2&entry.124074799=Read"},
{"title": "Multilingual Arbitrage: Optimizing Data Pools to Accelerate Multilingual\n  Progress", "author": "Ayomide Odumakinde and Daniel D'souza and Pat Verga and Beyza Ermis and Sara Hooker", "abstract": "  The use of synthetic data has played a critical role in recent state-of-art\nbreakthroughs. However, overly relying on a single oracle teacher model to\ngenerate data has been shown to lead to model collapse and invite propagation\nof biases. These limitations are particularly evident in multilingual settings,\nwhere the absence of a universally effective teacher model that excels across\nall languages presents significant challenges. In this work, we address these\nextreme difference by introducing \"multilingual arbitrage\", which capitalizes\non performance variations between multiple models for a given language. To do\nso, we strategically route samples through a diverse pool of models, each with\nunique strengths in different languages. Across exhaustive experiments on\nstate-of-art models, our work suggests that arbitrage techniques allow for\nspectacular gains in performance that far outperform relying on a single\nteacher. In particular, compared to the best single teacher, we observe gains\nof up to 56.5% improvement in win rates averaged across all languages when\nswitching to multilingual arbitrage. We observe the most significant gains for\nthe least resourced languages in our pool.\n", "link": "http://arxiv.org/abs/2408.14960v1", "date": "2024-08-27", "relevancy": 1.9003, "topK": [{"title": "Breadcrumbs: Adversarial class-balanced sampling for long-tailed recognition", "link": "https://link.springer.com/chapter/10.1007/978-3-031-20053-3_37", "similarity": 0.4775}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.4733}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.4733}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Multilingual%20Arbitrage%3A%20Optimizing%20Data%20Pools%20to%20Accelerate%20Multilingual%0A%20%20Progress&body=Title%3A%20Multilingual%20Arbitrage%3A%20Optimizing%20Data%20Pools%20to%20Accelerate%20Multilingual%0A%20%20Progress%0AAuthor%3A%20Ayomide%20Odumakinde%20and%20Daniel%20D%27souza%20and%20Pat%20Verga%20and%20Beyza%20Ermis%20and%20Sara%20Hooker%0AAbstract%3A%20%20%20The%20use%20of%20synthetic%20data%20has%20played%20a%20critical%20role%20in%20recent%20state-of-art%0Abreakthroughs.%20However%2C%20overly%20relying%20on%20a%20single%20oracle%20teacher%20model%20to%0Agenerate%20data%20has%20been%20shown%20to%20lead%20to%20model%20collapse%20and%20invite%20propagation%0Aof%20biases.%20These%20limitations%20are%20particularly%20evident%20in%20multilingual%20settings%2C%0Awhere%20the%20absence%20of%20a%20universally%20effective%20teacher%20model%20that%20excels%20across%0Aall%20languages%20presents%20significant%20challenges.%20In%20this%20work%2C%20we%20address%20these%0Aextreme%20difference%20by%20introducing%20%22multilingual%20arbitrage%22%2C%20which%20capitalizes%0Aon%20performance%20variations%20between%20multiple%20models%20for%20a%20given%20language.%20To%20do%0Aso%2C%20we%20strategically%20route%20samples%20through%20a%20diverse%20pool%20of%20models%2C%20each%20with%0Aunique%20strengths%20in%20different%20languages.%20Across%20exhaustive%20experiments%20on%0Astate-of-art%20models%2C%20our%20work%20suggests%20that%20arbitrage%20techniques%20allow%20for%0Aspectacular%20gains%20in%20performance%20that%20far%20outperform%20relying%20on%20a%20single%0Ateacher.%20In%20particular%2C%20compared%20to%20the%20best%20single%20teacher%2C%20we%20observe%20gains%0Aof%20up%20to%2056.5%25%20improvement%20in%20win%20rates%20averaged%20across%20all%20languages%20when%0Aswitching%20to%20multilingual%20arbitrage.%20We%20observe%20the%20most%20significant%20gains%20for%0Athe%20least%20resourced%20languages%20in%20our%20pool.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2408.14960v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DMultilingual%2520Arbitrage%253A%2520Optimizing%2520Data%2520Pools%2520to%2520Accelerate%2520Multilingual%250A%2520%2520Progress%26entry.906535625%3DAyomide%2520Odumakinde%2520and%2520Daniel%2520D%2527souza%2520and%2520Pat%2520Verga%2520and%2520Beyza%2520Ermis%2520and%2520Sara%2520Hooker%26entry.1292438233%3D%2520%2520The%2520use%2520of%2520synthetic%2520data%2520has%2520played%2520a%2520critical%2520role%2520in%2520recent%2520state-of-art%250Abreakthroughs.%2520However%252C%2520overly%2520relying%2520on%2520a%2520single%2520oracle%2520teacher%2520model%2520to%250Agenerate%2520data%2520has%2520been%2520shown%2520to%2520lead%2520to%2520model%2520collapse%2520and%2520invite%2520propagation%250Aof%2520biases.%2520These%2520limitations%2520are%2520particularly%2520evident%2520in%2520multilingual%2520settings%252C%250Awhere%2520the%2520absence%2520of%2520a%2520universally%2520effective%2520teacher%2520model%2520that%2520excels%2520across%250Aall%2520languages%2520presents%2520significant%2520challenges.%2520In%2520this%2520work%252C%2520we%2520address%2520these%250Aextreme%2520difference%2520by%2520introducing%2520%2522multilingual%2520arbitrage%2522%252C%2520which%2520capitalizes%250Aon%2520performance%2520variations%2520between%2520multiple%2520models%2520for%2520a%2520given%2520language.%2520To%2520do%250Aso%252C%2520we%2520strategically%2520route%2520samples%2520through%2520a%2520diverse%2520pool%2520of%2520models%252C%2520each%2520with%250Aunique%2520strengths%2520in%2520different%2520languages.%2520Across%2520exhaustive%2520experiments%2520on%250Astate-of-art%2520models%252C%2520our%2520work%2520suggests%2520that%2520arbitrage%2520techniques%2520allow%2520for%250Aspectacular%2520gains%2520in%2520performance%2520that%2520far%2520outperform%2520relying%2520on%2520a%2520single%250Ateacher.%2520In%2520particular%252C%2520compared%2520to%2520the%2520best%2520single%2520teacher%252C%2520we%2520observe%2520gains%250Aof%2520up%2520to%252056.5%2525%2520improvement%2520in%2520win%2520rates%2520averaged%2520across%2520all%2520languages%2520when%250Aswitching%2520to%2520multilingual%2520arbitrage.%2520We%2520observe%2520the%2520most%2520significant%2520gains%2520for%250Athe%2520least%2520resourced%2520languages%2520in%2520our%2520pool.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2408.14960v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Multilingual%20Arbitrage%3A%20Optimizing%20Data%20Pools%20to%20Accelerate%20Multilingual%0A%20%20Progress&entry.906535625=Ayomide%20Odumakinde%20and%20Daniel%20D%27souza%20and%20Pat%20Verga%20and%20Beyza%20Ermis%20and%20Sara%20Hooker&entry.1292438233=%20%20The%20use%20of%20synthetic%20data%20has%20played%20a%20critical%20role%20in%20recent%20state-of-art%0Abreakthroughs.%20However%2C%20overly%20relying%20on%20a%20single%20oracle%20teacher%20model%20to%0Agenerate%20data%20has%20been%20shown%20to%20lead%20to%20model%20collapse%20and%20invite%20propagation%0Aof%20biases.%20These%20limitations%20are%20particularly%20evident%20in%20multilingual%20settings%2C%0Awhere%20the%20absence%20of%20a%20universally%20effective%20teacher%20model%20that%20excels%20across%0Aall%20languages%20presents%20significant%20challenges.%20In%20this%20work%2C%20we%20address%20these%0Aextreme%20difference%20by%20introducing%20%22multilingual%20arbitrage%22%2C%20which%20capitalizes%0Aon%20performance%20variations%20between%20multiple%20models%20for%20a%20given%20language.%20To%20do%0Aso%2C%20we%20strategically%20route%20samples%20through%20a%20diverse%20pool%20of%20models%2C%20each%20with%0Aunique%20strengths%20in%20different%20languages.%20Across%20exhaustive%20experiments%20on%0Astate-of-art%20models%2C%20our%20work%20suggests%20that%20arbitrage%20techniques%20allow%20for%0Aspectacular%20gains%20in%20performance%20that%20far%20outperform%20relying%20on%20a%20single%0Ateacher.%20In%20particular%2C%20compared%20to%20the%20best%20single%20teacher%2C%20we%20observe%20gains%0Aof%20up%20to%2056.5%25%20improvement%20in%20win%20rates%20averaged%20across%20all%20languages%20when%0Aswitching%20to%20multilingual%20arbitrage.%20We%20observe%20the%20most%20significant%20gains%20for%0Athe%20least%20resourced%20languages%20in%20our%20pool.%0A&entry.1838667208=http%3A//arxiv.org/abs/2408.14960v1&entry.124074799=Read"},
{"title": "Deep Learning-based Average Shear Wave Velocity Prediction using\n  Accelerometer Records", "author": "Bar\u0131\u015f Y\u0131lmaz and Melek T\u00fcrkmen and Sanem Meral and Erdem Akag\u00fcnd\u00fcz and Salih Tileylioglu", "abstract": "  Assessing seismic hazards and thereby designing earthquake-resilient\nstructures or evaluating structural damage that has been incurred after an\nearthquake are important objectives in earthquake engineering. Both tasks\nrequire critical evaluation of strong ground motion records, and the knowledge\nof site conditions at the earthquake stations plays a major role in achieving\nthe aforementioned objectives. Site conditions are generally represented by the\ntime-averaged shear wave velocity in the upper 30 meters of the geological\nmaterials (Vs30). Several strong motion stations lack Vs30 measurements\nresulting in potentially inaccurate assessment of seismic hazards and\nevaluation of ground motion records. In this study, we present a deep\nlearning-based approach for predicting Vs30 at strong motion station locations\nusing three-channel earthquake records. For this purpose, Convolutional Neural\nNetworks (CNNs) with dilated and causal convolutional layers are used to\nextract deep features from accelerometer records collected from over 700\nstations located in Turkey. In order to overcome the limited availability of\nlabeled data, we propose a two-phase training approach. In the first phase, a\nCNN is trained to estimate the epicenters, for which ground truth is available\nfor all records. After the CNN is trained, the pre-trained encoder is\nfine-tuned based on the Vs30 ground truth. The performance of the proposed\nmethod is compared with machine learning models that utilize hand-crafted\nfeatures. The results demonstrate that the deep convolutional encoder based\nVs30 prediction model outperforms the machine learning models that rely on\nhand-crafted features.\n", "link": "http://arxiv.org/abs/2408.14962v1", "date": "2024-08-27", "relevancy": 1.8927, "topK": [{"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.5045}, {"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.472}, {"title": "Weakly-guided self-supervised pretraining for temporal activity detection", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/25189", "similarity": 0.4619}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Deep%20Learning-based%20Average%20Shear%20Wave%20Velocity%20Prediction%20using%0A%20%20Accelerometer%20Records&body=Title%3A%20Deep%20Learning-based%20Average%20Shear%20Wave%20Velocity%20Prediction%20using%0A%20%20Accelerometer%20Records%0AAuthor%3A%20Bar%C4%B1%C5%9F%20Y%C4%B1lmaz%20and%20Melek%20T%C3%BCrkmen%20and%20Sanem%20Meral%20and%20Erdem%20Akag%C3%BCnd%C3%BCz%20and%20Salih%20Tileylioglu%0AAbstract%3A%20%20%20Assessing%20seismic%20hazards%20and%20thereby%20designing%20earthquake-resilient%0Astructures%20or%20evaluating%20structural%20damage%20that%20has%20been%20incurred%20after%20an%0Aearthquake%20are%20important%20objectives%20in%20earthquake%20engineering.%20Both%20tasks%0Arequire%20critical%20evaluation%20of%20strong%20ground%20motion%20records%2C%20and%20the%20knowledge%0Aof%20site%20conditions%20at%20the%20earthquake%20stations%20plays%20a%20major%20role%20in%20achieving%0Athe%20aforementioned%20objectives.%20Site%20conditions%20are%20generally%20represented%20by%20the%0Atime-averaged%20shear%20wave%20velocity%20in%20the%20upper%2030%20meters%20of%20the%20geological%0Amaterials%20%28Vs30%29.%20Several%20strong%20motion%20stations%20lack%20Vs30%20measurements%0Aresulting%20in%20potentially%20inaccurate%20assessment%20of%20seismic%20hazards%20and%0Aevaluation%20of%20ground%20motion%20records.%20In%20this%20study%2C%20we%20present%20a%20deep%0Alearning-based%20approach%20for%20predicting%20Vs30%20at%20strong%20motion%20station%20locations%0Ausing%20three-channel%20earthquake%20records.%20For%20this%20purpose%2C%20Convolutional%20Neural%0ANetworks%20%28CNNs%29%20with%20dilated%20and%20causal%20convolutional%20layers%20are%20used%20to%0Aextract%20deep%20features%20from%20accelerometer%20records%20collected%20from%20over%20700%0Astations%20located%20in%20Turkey.%20In%20order%20to%20overcome%20the%20limited%20availability%20of%0Alabeled%20data%2C%20we%20propose%20a%20two-phase%20training%20approach.%20In%20the%20first%20phase%2C%20a%0ACNN%20is%20trained%20to%20estimate%20the%20epicenters%2C%20for%20which%20ground%20truth%20is%20available%0Afor%20all%20records.%20After%20the%20CNN%20is%20trained%2C%20the%20pre-trained%20encoder%20is%0Afine-tuned%20based%20on%20the%20Vs30%20ground%20truth.%20The%20performance%20of%20the%20proposed%0Amethod%20is%20compared%20with%20machine%20learning%20models%20that%20utilize%20hand-crafted%0Afeatures.%20The%20results%20demonstrate%20that%20the%20deep%20convolutional%20encoder%20based%0AVs30%20prediction%20model%20outperforms%20the%20machine%20learning%20models%20that%20rely%20on%0Ahand-crafted%20features.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2408.14962v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DDeep%2520Learning-based%2520Average%2520Shear%2520Wave%2520Velocity%2520Prediction%2520using%250A%2520%2520Accelerometer%2520Records%26entry.906535625%3DBar%25C4%25B1%25C5%259F%2520Y%25C4%25B1lmaz%2520and%2520Melek%2520T%25C3%25BCrkmen%2520and%2520Sanem%2520Meral%2520and%2520Erdem%2520Akag%25C3%25BCnd%25C3%25BCz%2520and%2520Salih%2520Tileylioglu%26entry.1292438233%3D%2520%2520Assessing%2520seismic%2520hazards%2520and%2520thereby%2520designing%2520earthquake-resilient%250Astructures%2520or%2520evaluating%2520structural%2520damage%2520that%2520has%2520been%2520incurred%2520after%2520an%250Aearthquake%2520are%2520important%2520objectives%2520in%2520earthquake%2520engineering.%2520Both%2520tasks%250Arequire%2520critical%2520evaluation%2520of%2520strong%2520ground%2520motion%2520records%252C%2520and%2520the%2520knowledge%250Aof%2520site%2520conditions%2520at%2520the%2520earthquake%2520stations%2520plays%2520a%2520major%2520role%2520in%2520achieving%250Athe%2520aforementioned%2520objectives.%2520Site%2520conditions%2520are%2520generally%2520represented%2520by%2520the%250Atime-averaged%2520shear%2520wave%2520velocity%2520in%2520the%2520upper%252030%2520meters%2520of%2520the%2520geological%250Amaterials%2520%2528Vs30%2529.%2520Several%2520strong%2520motion%2520stations%2520lack%2520Vs30%2520measurements%250Aresulting%2520in%2520potentially%2520inaccurate%2520assessment%2520of%2520seismic%2520hazards%2520and%250Aevaluation%2520of%2520ground%2520motion%2520records.%2520In%2520this%2520study%252C%2520we%2520present%2520a%2520deep%250Alearning-based%2520approach%2520for%2520predicting%2520Vs30%2520at%2520strong%2520motion%2520station%2520locations%250Ausing%2520three-channel%2520earthquake%2520records.%2520For%2520this%2520purpose%252C%2520Convolutional%2520Neural%250ANetworks%2520%2528CNNs%2529%2520with%2520dilated%2520and%2520causal%2520convolutional%2520layers%2520are%2520used%2520to%250Aextract%2520deep%2520features%2520from%2520accelerometer%2520records%2520collected%2520from%2520over%2520700%250Astations%2520located%2520in%2520Turkey.%2520In%2520order%2520to%2520overcome%2520the%2520limited%2520availability%2520of%250Alabeled%2520data%252C%2520we%2520propose%2520a%2520two-phase%2520training%2520approach.%2520In%2520the%2520first%2520phase%252C%2520a%250ACNN%2520is%2520trained%2520to%2520estimate%2520the%2520epicenters%252C%2520for%2520which%2520ground%2520truth%2520is%2520available%250Afor%2520all%2520records.%2520After%2520the%2520CNN%2520is%2520trained%252C%2520the%2520pre-trained%2520encoder%2520is%250Afine-tuned%2520based%2520on%2520the%2520Vs30%2520ground%2520truth.%2520The%2520performance%2520of%2520the%2520proposed%250Amethod%2520is%2520compared%2520with%2520machine%2520learning%2520models%2520that%2520utilize%2520hand-crafted%250Afeatures.%2520The%2520results%2520demonstrate%2520that%2520the%2520deep%2520convolutional%2520encoder%2520based%250AVs30%2520prediction%2520model%2520outperforms%2520the%2520machine%2520learning%2520models%2520that%2520rely%2520on%250Ahand-crafted%2520features.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2408.14962v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Deep%20Learning-based%20Average%20Shear%20Wave%20Velocity%20Prediction%20using%0A%20%20Accelerometer%20Records&entry.906535625=Bar%C4%B1%C5%9F%20Y%C4%B1lmaz%20and%20Melek%20T%C3%BCrkmen%20and%20Sanem%20Meral%20and%20Erdem%20Akag%C3%BCnd%C3%BCz%20and%20Salih%20Tileylioglu&entry.1292438233=%20%20Assessing%20seismic%20hazards%20and%20thereby%20designing%20earthquake-resilient%0Astructures%20or%20evaluating%20structural%20damage%20that%20has%20been%20incurred%20after%20an%0Aearthquake%20are%20important%20objectives%20in%20earthquake%20engineering.%20Both%20tasks%0Arequire%20critical%20evaluation%20of%20strong%20ground%20motion%20records%2C%20and%20the%20knowledge%0Aof%20site%20conditions%20at%20the%20earthquake%20stations%20plays%20a%20major%20role%20in%20achieving%0Athe%20aforementioned%20objectives.%20Site%20conditions%20are%20generally%20represented%20by%20the%0Atime-averaged%20shear%20wave%20velocity%20in%20the%20upper%2030%20meters%20of%20the%20geological%0Amaterials%20%28Vs30%29.%20Several%20strong%20motion%20stations%20lack%20Vs30%20measurements%0Aresulting%20in%20potentially%20inaccurate%20assessment%20of%20seismic%20hazards%20and%0Aevaluation%20of%20ground%20motion%20records.%20In%20this%20study%2C%20we%20present%20a%20deep%0Alearning-based%20approach%20for%20predicting%20Vs30%20at%20strong%20motion%20station%20locations%0Ausing%20three-channel%20earthquake%20records.%20For%20this%20purpose%2C%20Convolutional%20Neural%0ANetworks%20%28CNNs%29%20with%20dilated%20and%20causal%20convolutional%20layers%20are%20used%20to%0Aextract%20deep%20features%20from%20accelerometer%20records%20collected%20from%20over%20700%0Astations%20located%20in%20Turkey.%20In%20order%20to%20overcome%20the%20limited%20availability%20of%0Alabeled%20data%2C%20we%20propose%20a%20two-phase%20training%20approach.%20In%20the%20first%20phase%2C%20a%0ACNN%20is%20trained%20to%20estimate%20the%20epicenters%2C%20for%20which%20ground%20truth%20is%20available%0Afor%20all%20records.%20After%20the%20CNN%20is%20trained%2C%20the%20pre-trained%20encoder%20is%0Afine-tuned%20based%20on%20the%20Vs30%20ground%20truth.%20The%20performance%20of%20the%20proposed%0Amethod%20is%20compared%20with%20machine%20learning%20models%20that%20utilize%20hand-crafted%0Afeatures.%20The%20results%20demonstrate%20that%20the%20deep%20convolutional%20encoder%20based%0AVs30%20prediction%20model%20outperforms%20the%20machine%20learning%20models%20that%20rely%20on%0Ahand-crafted%20features.%0A&entry.1838667208=http%3A//arxiv.org/abs/2408.14962v1&entry.124074799=Read"},
{"title": "Data-Driven Nonlinear Deformation Design of 3D-Printable Shells", "author": "Samuel Silverman and Kelsey L. Snapp and Keith A. Brown and Emily Whiting", "abstract": "  Designing and fabricating structures with specific mechanical properties\nrequires understanding the intricate relationship between design parameters and\nperformance. Understanding the design-performance relationship becomes\nincreasingly complicated for nonlinear deformations. Though successful at\nmodeling elastic deformations, simulation-based techniques struggle to model\nlarge elastoplastic deformations exhibiting plasticity and densification. We\npropose a neural network trained on experimental data to learn the\ndesign-performance relationship between 3D-printable shells and their\ncompressive force-displacement behavior. Trained on thousands of physical\nexperiments, our network aids in both forward and inverse design to generate\nshells exhibiting desired elastoplastic and hyperelastic deformations. We\nvalidate a subset of generated designs through fabrication and testing.\nFurthermore, we demonstrate the network's inverse design efficacy in generating\ncustom shells for several applications.\n", "link": "http://arxiv.org/abs/2408.15097v1", "date": "2024-08-27", "relevancy": 1.8638, "topK": [{"title": "Scaling Rectified Flow Transformers for High-Resolution Image Synthesis", "link": "http://arxiv.org/abs/2403.03206v1", "similarity": 0.4672}, {"title": "DressCode: Autoregressively Sewing and Generating Garments from Text\n  Guidance", "link": "http://arxiv.org/abs/2401.16465v3", "similarity": 0.4669}, {"title": "Dynamic Gaussians Mesh: Consistent Mesh Reconstruction from Monocular\n  Videos", "link": "http://arxiv.org/abs/2404.12379v2", "similarity": 0.4645}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Data-Driven%20Nonlinear%20Deformation%20Design%20of%203D-Printable%20Shells&body=Title%3A%20Data-Driven%20Nonlinear%20Deformation%20Design%20of%203D-Printable%20Shells%0AAuthor%3A%20Samuel%20Silverman%20and%20Kelsey%20L.%20Snapp%20and%20Keith%20A.%20Brown%20and%20Emily%20Whiting%0AAbstract%3A%20%20%20Designing%20and%20fabricating%20structures%20with%20specific%20mechanical%20properties%0Arequires%20understanding%20the%20intricate%20relationship%20between%20design%20parameters%20and%0Aperformance.%20Understanding%20the%20design-performance%20relationship%20becomes%0Aincreasingly%20complicated%20for%20nonlinear%20deformations.%20Though%20successful%20at%0Amodeling%20elastic%20deformations%2C%20simulation-based%20techniques%20struggle%20to%20model%0Alarge%20elastoplastic%20deformations%20exhibiting%20plasticity%20and%20densification.%20We%0Apropose%20a%20neural%20network%20trained%20on%20experimental%20data%20to%20learn%20the%0Adesign-performance%20relationship%20between%203D-printable%20shells%20and%20their%0Acompressive%20force-displacement%20behavior.%20Trained%20on%20thousands%20of%20physical%0Aexperiments%2C%20our%20network%20aids%20in%20both%20forward%20and%20inverse%20design%20to%20generate%0Ashells%20exhibiting%20desired%20elastoplastic%20and%20hyperelastic%20deformations.%20We%0Avalidate%20a%20subset%20of%20generated%20designs%20through%20fabrication%20and%20testing.%0AFurthermore%2C%20we%20demonstrate%20the%20network%27s%20inverse%20design%20efficacy%20in%20generating%0Acustom%20shells%20for%20several%20applications.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2408.15097v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DData-Driven%2520Nonlinear%2520Deformation%2520Design%2520of%25203D-Printable%2520Shells%26entry.906535625%3DSamuel%2520Silverman%2520and%2520Kelsey%2520L.%2520Snapp%2520and%2520Keith%2520A.%2520Brown%2520and%2520Emily%2520Whiting%26entry.1292438233%3D%2520%2520Designing%2520and%2520fabricating%2520structures%2520with%2520specific%2520mechanical%2520properties%250Arequires%2520understanding%2520the%2520intricate%2520relationship%2520between%2520design%2520parameters%2520and%250Aperformance.%2520Understanding%2520the%2520design-performance%2520relationship%2520becomes%250Aincreasingly%2520complicated%2520for%2520nonlinear%2520deformations.%2520Though%2520successful%2520at%250Amodeling%2520elastic%2520deformations%252C%2520simulation-based%2520techniques%2520struggle%2520to%2520model%250Alarge%2520elastoplastic%2520deformations%2520exhibiting%2520plasticity%2520and%2520densification.%2520We%250Apropose%2520a%2520neural%2520network%2520trained%2520on%2520experimental%2520data%2520to%2520learn%2520the%250Adesign-performance%2520relationship%2520between%25203D-printable%2520shells%2520and%2520their%250Acompressive%2520force-displacement%2520behavior.%2520Trained%2520on%2520thousands%2520of%2520physical%250Aexperiments%252C%2520our%2520network%2520aids%2520in%2520both%2520forward%2520and%2520inverse%2520design%2520to%2520generate%250Ashells%2520exhibiting%2520desired%2520elastoplastic%2520and%2520hyperelastic%2520deformations.%2520We%250Avalidate%2520a%2520subset%2520of%2520generated%2520designs%2520through%2520fabrication%2520and%2520testing.%250AFurthermore%252C%2520we%2520demonstrate%2520the%2520network%2527s%2520inverse%2520design%2520efficacy%2520in%2520generating%250Acustom%2520shells%2520for%2520several%2520applications.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2408.15097v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Data-Driven%20Nonlinear%20Deformation%20Design%20of%203D-Printable%20Shells&entry.906535625=Samuel%20Silverman%20and%20Kelsey%20L.%20Snapp%20and%20Keith%20A.%20Brown%20and%20Emily%20Whiting&entry.1292438233=%20%20Designing%20and%20fabricating%20structures%20with%20specific%20mechanical%20properties%0Arequires%20understanding%20the%20intricate%20relationship%20between%20design%20parameters%20and%0Aperformance.%20Understanding%20the%20design-performance%20relationship%20becomes%0Aincreasingly%20complicated%20for%20nonlinear%20deformations.%20Though%20successful%20at%0Amodeling%20elastic%20deformations%2C%20simulation-based%20techniques%20struggle%20to%20model%0Alarge%20elastoplastic%20deformations%20exhibiting%20plasticity%20and%20densification.%20We%0Apropose%20a%20neural%20network%20trained%20on%20experimental%20data%20to%20learn%20the%0Adesign-performance%20relationship%20between%203D-printable%20shells%20and%20their%0Acompressive%20force-displacement%20behavior.%20Trained%20on%20thousands%20of%20physical%0Aexperiments%2C%20our%20network%20aids%20in%20both%20forward%20and%20inverse%20design%20to%20generate%0Ashells%20exhibiting%20desired%20elastoplastic%20and%20hyperelastic%20deformations.%20We%0Avalidate%20a%20subset%20of%20generated%20designs%20through%20fabrication%20and%20testing.%0AFurthermore%2C%20we%20demonstrate%20the%20network%27s%20inverse%20design%20efficacy%20in%20generating%0Acustom%20shells%20for%20several%20applications.%0A&entry.1838667208=http%3A//arxiv.org/abs/2408.15097v1&entry.124074799=Read"},
{"title": "Knowledge Discovery in Optical Music Recognition: Enhancing Information\n  Retrieval with Instance Segmentation", "author": "Elona Shatri and George Fazekas", "abstract": "  Optical Music Recognition (OMR) automates the transcription of musical\nnotation from images into machine-readable formats like MusicXML, MEI, or MIDI,\nsignificantly reducing the costs and time of manual transcription. This study\nexplores knowledge discovery in OMR by applying instance segmentation using\nMask R-CNN to enhance the detection and delineation of musical symbols in sheet\nmusic. Unlike Optical Character Recognition (OCR), OMR must handle the\nintricate semantics of Common Western Music Notation (CWMN), where symbol\nmeanings depend on shape, position, and context. Our approach leverages\ninstance segmentation to manage the density and overlap of musical symbols,\nfacilitating more precise information retrieval from music scores. Evaluations\non the DoReMi and MUSCIMA++ datasets demonstrate substantial improvements, with\nour method achieving a mean Average Precision (mAP) of up to 59.70\\% in dense\nsymbol environments, achieving comparable results to object detection.\nFurthermore, using traditional computer vision techniques, we add a parallel\nstep for staff detection to infer the pitch for the recognised symbols. This\nstudy emphasises the role of pixel-wise segmentation in advancing accurate\nmusic symbol recognition, contributing to knowledge discovery in OMR. Our\nfindings indicate that instance segmentation provides more precise\nrepresentations of musical symbols, particularly in densely populated scores,\nadvancing OMR technology. We make our implementation, pre-processing scripts,\ntrained models, and evaluation results publicly available to support further\nresearch and development.\n", "link": "http://arxiv.org/abs/2408.15002v1", "date": "2024-08-27", "relevancy": 1.8531, "topK": [{"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.471}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.4642}, {"title": "Breadcrumbs: Adversarial class-balanced sampling for long-tailed recognition", "link": "https://link.springer.com/chapter/10.1007/978-3-031-20053-3_37", "similarity": 0.4593}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Knowledge%20Discovery%20in%20Optical%20Music%20Recognition%3A%20Enhancing%20Information%0A%20%20Retrieval%20with%20Instance%20Segmentation&body=Title%3A%20Knowledge%20Discovery%20in%20Optical%20Music%20Recognition%3A%20Enhancing%20Information%0A%20%20Retrieval%20with%20Instance%20Segmentation%0AAuthor%3A%20Elona%20Shatri%20and%20George%20Fazekas%0AAbstract%3A%20%20%20Optical%20Music%20Recognition%20%28OMR%29%20automates%20the%20transcription%20of%20musical%0Anotation%20from%20images%20into%20machine-readable%20formats%20like%20MusicXML%2C%20MEI%2C%20or%20MIDI%2C%0Asignificantly%20reducing%20the%20costs%20and%20time%20of%20manual%20transcription.%20This%20study%0Aexplores%20knowledge%20discovery%20in%20OMR%20by%20applying%20instance%20segmentation%20using%0AMask%20R-CNN%20to%20enhance%20the%20detection%20and%20delineation%20of%20musical%20symbols%20in%20sheet%0Amusic.%20Unlike%20Optical%20Character%20Recognition%20%28OCR%29%2C%20OMR%20must%20handle%20the%0Aintricate%20semantics%20of%20Common%20Western%20Music%20Notation%20%28CWMN%29%2C%20where%20symbol%0Ameanings%20depend%20on%20shape%2C%20position%2C%20and%20context.%20Our%20approach%20leverages%0Ainstance%20segmentation%20to%20manage%20the%20density%20and%20overlap%20of%20musical%20symbols%2C%0Afacilitating%20more%20precise%20information%20retrieval%20from%20music%20scores.%20Evaluations%0Aon%20the%20DoReMi%20and%20MUSCIMA%2B%2B%20datasets%20demonstrate%20substantial%20improvements%2C%20with%0Aour%20method%20achieving%20a%20mean%20Average%20Precision%20%28mAP%29%20of%20up%20to%2059.70%5C%25%20in%20dense%0Asymbol%20environments%2C%20achieving%20comparable%20results%20to%20object%20detection.%0AFurthermore%2C%20using%20traditional%20computer%20vision%20techniques%2C%20we%20add%20a%20parallel%0Astep%20for%20staff%20detection%20to%20infer%20the%20pitch%20for%20the%20recognised%20symbols.%20This%0Astudy%20emphasises%20the%20role%20of%20pixel-wise%20segmentation%20in%20advancing%20accurate%0Amusic%20symbol%20recognition%2C%20contributing%20to%20knowledge%20discovery%20in%20OMR.%20Our%0Afindings%20indicate%20that%20instance%20segmentation%20provides%20more%20precise%0Arepresentations%20of%20musical%20symbols%2C%20particularly%20in%20densely%20populated%20scores%2C%0Aadvancing%20OMR%20technology.%20We%20make%20our%20implementation%2C%20pre-processing%20scripts%2C%0Atrained%20models%2C%20and%20evaluation%20results%20publicly%20available%20to%20support%20further%0Aresearch%20and%20development.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2408.15002v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DKnowledge%2520Discovery%2520in%2520Optical%2520Music%2520Recognition%253A%2520Enhancing%2520Information%250A%2520%2520Retrieval%2520with%2520Instance%2520Segmentation%26entry.906535625%3DElona%2520Shatri%2520and%2520George%2520Fazekas%26entry.1292438233%3D%2520%2520Optical%2520Music%2520Recognition%2520%2528OMR%2529%2520automates%2520the%2520transcription%2520of%2520musical%250Anotation%2520from%2520images%2520into%2520machine-readable%2520formats%2520like%2520MusicXML%252C%2520MEI%252C%2520or%2520MIDI%252C%250Asignificantly%2520reducing%2520the%2520costs%2520and%2520time%2520of%2520manual%2520transcription.%2520This%2520study%250Aexplores%2520knowledge%2520discovery%2520in%2520OMR%2520by%2520applying%2520instance%2520segmentation%2520using%250AMask%2520R-CNN%2520to%2520enhance%2520the%2520detection%2520and%2520delineation%2520of%2520musical%2520symbols%2520in%2520sheet%250Amusic.%2520Unlike%2520Optical%2520Character%2520Recognition%2520%2528OCR%2529%252C%2520OMR%2520must%2520handle%2520the%250Aintricate%2520semantics%2520of%2520Common%2520Western%2520Music%2520Notation%2520%2528CWMN%2529%252C%2520where%2520symbol%250Ameanings%2520depend%2520on%2520shape%252C%2520position%252C%2520and%2520context.%2520Our%2520approach%2520leverages%250Ainstance%2520segmentation%2520to%2520manage%2520the%2520density%2520and%2520overlap%2520of%2520musical%2520symbols%252C%250Afacilitating%2520more%2520precise%2520information%2520retrieval%2520from%2520music%2520scores.%2520Evaluations%250Aon%2520the%2520DoReMi%2520and%2520MUSCIMA%252B%252B%2520datasets%2520demonstrate%2520substantial%2520improvements%252C%2520with%250Aour%2520method%2520achieving%2520a%2520mean%2520Average%2520Precision%2520%2528mAP%2529%2520of%2520up%2520to%252059.70%255C%2525%2520in%2520dense%250Asymbol%2520environments%252C%2520achieving%2520comparable%2520results%2520to%2520object%2520detection.%250AFurthermore%252C%2520using%2520traditional%2520computer%2520vision%2520techniques%252C%2520we%2520add%2520a%2520parallel%250Astep%2520for%2520staff%2520detection%2520to%2520infer%2520the%2520pitch%2520for%2520the%2520recognised%2520symbols.%2520This%250Astudy%2520emphasises%2520the%2520role%2520of%2520pixel-wise%2520segmentation%2520in%2520advancing%2520accurate%250Amusic%2520symbol%2520recognition%252C%2520contributing%2520to%2520knowledge%2520discovery%2520in%2520OMR.%2520Our%250Afindings%2520indicate%2520that%2520instance%2520segmentation%2520provides%2520more%2520precise%250Arepresentations%2520of%2520musical%2520symbols%252C%2520particularly%2520in%2520densely%2520populated%2520scores%252C%250Aadvancing%2520OMR%2520technology.%2520We%2520make%2520our%2520implementation%252C%2520pre-processing%2520scripts%252C%250Atrained%2520models%252C%2520and%2520evaluation%2520results%2520publicly%2520available%2520to%2520support%2520further%250Aresearch%2520and%2520development.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2408.15002v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Knowledge%20Discovery%20in%20Optical%20Music%20Recognition%3A%20Enhancing%20Information%0A%20%20Retrieval%20with%20Instance%20Segmentation&entry.906535625=Elona%20Shatri%20and%20George%20Fazekas&entry.1292438233=%20%20Optical%20Music%20Recognition%20%28OMR%29%20automates%20the%20transcription%20of%20musical%0Anotation%20from%20images%20into%20machine-readable%20formats%20like%20MusicXML%2C%20MEI%2C%20or%20MIDI%2C%0Asignificantly%20reducing%20the%20costs%20and%20time%20of%20manual%20transcription.%20This%20study%0Aexplores%20knowledge%20discovery%20in%20OMR%20by%20applying%20instance%20segmentation%20using%0AMask%20R-CNN%20to%20enhance%20the%20detection%20and%20delineation%20of%20musical%20symbols%20in%20sheet%0Amusic.%20Unlike%20Optical%20Character%20Recognition%20%28OCR%29%2C%20OMR%20must%20handle%20the%0Aintricate%20semantics%20of%20Common%20Western%20Music%20Notation%20%28CWMN%29%2C%20where%20symbol%0Ameanings%20depend%20on%20shape%2C%20position%2C%20and%20context.%20Our%20approach%20leverages%0Ainstance%20segmentation%20to%20manage%20the%20density%20and%20overlap%20of%20musical%20symbols%2C%0Afacilitating%20more%20precise%20information%20retrieval%20from%20music%20scores.%20Evaluations%0Aon%20the%20DoReMi%20and%20MUSCIMA%2B%2B%20datasets%20demonstrate%20substantial%20improvements%2C%20with%0Aour%20method%20achieving%20a%20mean%20Average%20Precision%20%28mAP%29%20of%20up%20to%2059.70%5C%25%20in%20dense%0Asymbol%20environments%2C%20achieving%20comparable%20results%20to%20object%20detection.%0AFurthermore%2C%20using%20traditional%20computer%20vision%20techniques%2C%20we%20add%20a%20parallel%0Astep%20for%20staff%20detection%20to%20infer%20the%20pitch%20for%20the%20recognised%20symbols.%20This%0Astudy%20emphasises%20the%20role%20of%20pixel-wise%20segmentation%20in%20advancing%20accurate%0Amusic%20symbol%20recognition%2C%20contributing%20to%20knowledge%20discovery%20in%20OMR.%20Our%0Afindings%20indicate%20that%20instance%20segmentation%20provides%20more%20precise%0Arepresentations%20of%20musical%20symbols%2C%20particularly%20in%20densely%20populated%20scores%2C%0Aadvancing%20OMR%20technology.%20We%20make%20our%20implementation%2C%20pre-processing%20scripts%2C%0Atrained%20models%2C%20and%20evaluation%20results%20publicly%20available%20to%20support%20further%0Aresearch%20and%20development.%0A&entry.1838667208=http%3A//arxiv.org/abs/2408.15002v1&entry.124074799=Read"},
{"title": "Does Audio Deepfake Detection Generalize?", "author": "Nicolas M. M\u00fcller and Pavel Czempin and Franziska Dieckmann and Adam Froghyar and Konstantin B\u00f6ttinger", "abstract": "  Current text-to-speech algorithms produce realistic fakes of human voices,\nmaking deepfake detection a much-needed area of research. While researchers\nhave presented various techniques for detecting audio spoofs, it is often\nunclear exactly why these architectures are successful: Preprocessing steps,\nhyperparameter settings, and the degree of fine-tuning are not consistent\nacross related work. Which factors contribute to success, and which are\naccidental? In this work, we address this problem: We systematize audio\nspoofing detection by re-implementing and uniformly evaluating architectures\nfrom related work. We identify overarching features for successful audio\ndeepfake detection, such as using cqtspec or logspec features instead of\nmelspec features, which improves performance by 37% EER on average, all other\nfactors constant. Additionally, we evaluate generalization capabilities: We\ncollect and publish a new dataset consisting of 37.9 hours of found audio\nrecordings of celebrities and politicians, of which 17.2 hours are deepfakes.\nWe find that related work performs poorly on such real-world data (performance\ndegradation of up to one thousand percent). This may suggest that the community\nhas tailored its solutions too closely to the prevailing ASVSpoof benchmark and\nthat deepfakes are much harder to detect outside the lab than previously\nthought.\n", "link": "http://arxiv.org/abs/2203.16263v4", "date": "2024-08-27", "relevancy": 1.8455, "topK": [{"title": "Tuning-Free Noise Rectification for High Fidelity Image-to-Video\n  Generation", "link": "http://arxiv.org/abs/2403.02827v1", "similarity": 0.4635}, {"title": "Scaling Rectified Flow Transformers for High-Resolution Image Synthesis", "link": "http://arxiv.org/abs/2403.03206v1", "similarity": 0.4606}, {"title": "Weakly-guided self-supervised pretraining for temporal activity detection", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/25189", "similarity": 0.4596}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Does%20Audio%20Deepfake%20Detection%20Generalize%3F&body=Title%3A%20Does%20Audio%20Deepfake%20Detection%20Generalize%3F%0AAuthor%3A%20Nicolas%20M.%20M%C3%BCller%20and%20Pavel%20Czempin%20and%20Franziska%20Dieckmann%20and%20Adam%20Froghyar%20and%20Konstantin%20B%C3%B6ttinger%0AAbstract%3A%20%20%20Current%20text-to-speech%20algorithms%20produce%20realistic%20fakes%20of%20human%20voices%2C%0Amaking%20deepfake%20detection%20a%20much-needed%20area%20of%20research.%20While%20researchers%0Ahave%20presented%20various%20techniques%20for%20detecting%20audio%20spoofs%2C%20it%20is%20often%0Aunclear%20exactly%20why%20these%20architectures%20are%20successful%3A%20Preprocessing%20steps%2C%0Ahyperparameter%20settings%2C%20and%20the%20degree%20of%20fine-tuning%20are%20not%20consistent%0Aacross%20related%20work.%20Which%20factors%20contribute%20to%20success%2C%20and%20which%20are%0Aaccidental%3F%20In%20this%20work%2C%20we%20address%20this%20problem%3A%20We%20systematize%20audio%0Aspoofing%20detection%20by%20re-implementing%20and%20uniformly%20evaluating%20architectures%0Afrom%20related%20work.%20We%20identify%20overarching%20features%20for%20successful%20audio%0Adeepfake%20detection%2C%20such%20as%20using%20cqtspec%20or%20logspec%20features%20instead%20of%0Amelspec%20features%2C%20which%20improves%20performance%20by%2037%25%20EER%20on%20average%2C%20all%20other%0Afactors%20constant.%20Additionally%2C%20we%20evaluate%20generalization%20capabilities%3A%20We%0Acollect%20and%20publish%20a%20new%20dataset%20consisting%20of%2037.9%20hours%20of%20found%20audio%0Arecordings%20of%20celebrities%20and%20politicians%2C%20of%20which%2017.2%20hours%20are%20deepfakes.%0AWe%20find%20that%20related%20work%20performs%20poorly%20on%20such%20real-world%20data%20%28performance%0Adegradation%20of%20up%20to%20one%20thousand%20percent%29.%20This%20may%20suggest%20that%20the%20community%0Ahas%20tailored%20its%20solutions%20too%20closely%20to%20the%20prevailing%20ASVSpoof%20benchmark%20and%0Athat%20deepfakes%20are%20much%20harder%20to%20detect%20outside%20the%20lab%20than%20previously%0Athought.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2203.16263v4%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DDoes%2520Audio%2520Deepfake%2520Detection%2520Generalize%253F%26entry.906535625%3DNicolas%2520M.%2520M%25C3%25BCller%2520and%2520Pavel%2520Czempin%2520and%2520Franziska%2520Dieckmann%2520and%2520Adam%2520Froghyar%2520and%2520Konstantin%2520B%25C3%25B6ttinger%26entry.1292438233%3D%2520%2520Current%2520text-to-speech%2520algorithms%2520produce%2520realistic%2520fakes%2520of%2520human%2520voices%252C%250Amaking%2520deepfake%2520detection%2520a%2520much-needed%2520area%2520of%2520research.%2520While%2520researchers%250Ahave%2520presented%2520various%2520techniques%2520for%2520detecting%2520audio%2520spoofs%252C%2520it%2520is%2520often%250Aunclear%2520exactly%2520why%2520these%2520architectures%2520are%2520successful%253A%2520Preprocessing%2520steps%252C%250Ahyperparameter%2520settings%252C%2520and%2520the%2520degree%2520of%2520fine-tuning%2520are%2520not%2520consistent%250Aacross%2520related%2520work.%2520Which%2520factors%2520contribute%2520to%2520success%252C%2520and%2520which%2520are%250Aaccidental%253F%2520In%2520this%2520work%252C%2520we%2520address%2520this%2520problem%253A%2520We%2520systematize%2520audio%250Aspoofing%2520detection%2520by%2520re-implementing%2520and%2520uniformly%2520evaluating%2520architectures%250Afrom%2520related%2520work.%2520We%2520identify%2520overarching%2520features%2520for%2520successful%2520audio%250Adeepfake%2520detection%252C%2520such%2520as%2520using%2520cqtspec%2520or%2520logspec%2520features%2520instead%2520of%250Amelspec%2520features%252C%2520which%2520improves%2520performance%2520by%252037%2525%2520EER%2520on%2520average%252C%2520all%2520other%250Afactors%2520constant.%2520Additionally%252C%2520we%2520evaluate%2520generalization%2520capabilities%253A%2520We%250Acollect%2520and%2520publish%2520a%2520new%2520dataset%2520consisting%2520of%252037.9%2520hours%2520of%2520found%2520audio%250Arecordings%2520of%2520celebrities%2520and%2520politicians%252C%2520of%2520which%252017.2%2520hours%2520are%2520deepfakes.%250AWe%2520find%2520that%2520related%2520work%2520performs%2520poorly%2520on%2520such%2520real-world%2520data%2520%2528performance%250Adegradation%2520of%2520up%2520to%2520one%2520thousand%2520percent%2529.%2520This%2520may%2520suggest%2520that%2520the%2520community%250Ahas%2520tailored%2520its%2520solutions%2520too%2520closely%2520to%2520the%2520prevailing%2520ASVSpoof%2520benchmark%2520and%250Athat%2520deepfakes%2520are%2520much%2520harder%2520to%2520detect%2520outside%2520the%2520lab%2520than%2520previously%250Athought.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2203.16263v4%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Does%20Audio%20Deepfake%20Detection%20Generalize%3F&entry.906535625=Nicolas%20M.%20M%C3%BCller%20and%20Pavel%20Czempin%20and%20Franziska%20Dieckmann%20and%20Adam%20Froghyar%20and%20Konstantin%20B%C3%B6ttinger&entry.1292438233=%20%20Current%20text-to-speech%20algorithms%20produce%20realistic%20fakes%20of%20human%20voices%2C%0Amaking%20deepfake%20detection%20a%20much-needed%20area%20of%20research.%20While%20researchers%0Ahave%20presented%20various%20techniques%20for%20detecting%20audio%20spoofs%2C%20it%20is%20often%0Aunclear%20exactly%20why%20these%20architectures%20are%20successful%3A%20Preprocessing%20steps%2C%0Ahyperparameter%20settings%2C%20and%20the%20degree%20of%20fine-tuning%20are%20not%20consistent%0Aacross%20related%20work.%20Which%20factors%20contribute%20to%20success%2C%20and%20which%20are%0Aaccidental%3F%20In%20this%20work%2C%20we%20address%20this%20problem%3A%20We%20systematize%20audio%0Aspoofing%20detection%20by%20re-implementing%20and%20uniformly%20evaluating%20architectures%0Afrom%20related%20work.%20We%20identify%20overarching%20features%20for%20successful%20audio%0Adeepfake%20detection%2C%20such%20as%20using%20cqtspec%20or%20logspec%20features%20instead%20of%0Amelspec%20features%2C%20which%20improves%20performance%20by%2037%25%20EER%20on%20average%2C%20all%20other%0Afactors%20constant.%20Additionally%2C%20we%20evaluate%20generalization%20capabilities%3A%20We%0Acollect%20and%20publish%20a%20new%20dataset%20consisting%20of%2037.9%20hours%20of%20found%20audio%0Arecordings%20of%20celebrities%20and%20politicians%2C%20of%20which%2017.2%20hours%20are%20deepfakes.%0AWe%20find%20that%20related%20work%20performs%20poorly%20on%20such%20real-world%20data%20%28performance%0Adegradation%20of%20up%20to%20one%20thousand%20percent%29.%20This%20may%20suggest%20that%20the%20community%0Ahas%20tailored%20its%20solutions%20too%20closely%20to%20the%20prevailing%20ASVSpoof%20benchmark%20and%0Athat%20deepfakes%20are%20much%20harder%20to%20detect%20outside%20the%20lab%20than%20previously%0Athought.%0A&entry.1838667208=http%3A//arxiv.org/abs/2203.16263v4&entry.124074799=Read"},
{"title": "On Newton's Method to Unlearn Neural Networks", "author": "Nhung Bui and Xinyang Lu and Rachael Hwee Ling Sim and See-Kiong Ng and Bryan Kian Hsiang Low", "abstract": "  With the widespread applications of neural networks (NNs) trained on personal\ndata, machine unlearning has become increasingly important for enabling\nindividuals to exercise their personal data ownership, particularly the \"right\nto be forgotten\" from trained NNs. Since retraining is computationally\nexpensive, we seek approximate unlearning algorithms for NNs that return\nidentical models to the retrained oracle. While Newton's method has been\nsuccessfully used to approximately unlearn linear models, we observe that\nadapting it for NN is challenging due to degenerate Hessians that make\ncomputing Newton's update impossible. Additionally, we show that when coupled\nwith popular techniques to resolve the degeneracy, Newton's method often incurs\noffensively large norm updates and empirically degrades model performance\npost-unlearning. To address these challenges, we propose CureNewton's method, a\nprinciple approach that leverages cubic regularization to handle the Hessian\ndegeneracy effectively. The added regularizer eliminates the need for manual\nfinetuning and affords a natural interpretation within the unlearning context.\nExperiments across different models and datasets show that our method can\nachieve competitive unlearning performance to the state-of-the-art algorithm in\npractical unlearning settings, while being theoretically justified and\nefficient in running time.\n", "link": "http://arxiv.org/abs/2406.14507v2", "date": "2024-08-27", "relevancy": 1.8352, "topK": [{"title": "Boosted dynamic neural networks", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/26302", "similarity": 0.497}, {"title": "ToDo: Token Downsampling for Efficient Generation of High-Resolution\n  Images", "link": "http://arxiv.org/abs/2402.13573v2", "similarity": 0.4322}, {"title": "Scaling Rectified Flow Transformers for High-Resolution Image Synthesis", "link": "http://arxiv.org/abs/2403.03206v1", "similarity": 0.4298}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20On%20Newton%27s%20Method%20to%20Unlearn%20Neural%20Networks&body=Title%3A%20On%20Newton%27s%20Method%20to%20Unlearn%20Neural%20Networks%0AAuthor%3A%20Nhung%20Bui%20and%20Xinyang%20Lu%20and%20Rachael%20Hwee%20Ling%20Sim%20and%20See-Kiong%20Ng%20and%20Bryan%20Kian%20Hsiang%20Low%0AAbstract%3A%20%20%20With%20the%20widespread%20applications%20of%20neural%20networks%20%28NNs%29%20trained%20on%20personal%0Adata%2C%20machine%20unlearning%20has%20become%20increasingly%20important%20for%20enabling%0Aindividuals%20to%20exercise%20their%20personal%20data%20ownership%2C%20particularly%20the%20%22right%0Ato%20be%20forgotten%22%20from%20trained%20NNs.%20Since%20retraining%20is%20computationally%0Aexpensive%2C%20we%20seek%20approximate%20unlearning%20algorithms%20for%20NNs%20that%20return%0Aidentical%20models%20to%20the%20retrained%20oracle.%20While%20Newton%27s%20method%20has%20been%0Asuccessfully%20used%20to%20approximately%20unlearn%20linear%20models%2C%20we%20observe%20that%0Aadapting%20it%20for%20NN%20is%20challenging%20due%20to%20degenerate%20Hessians%20that%20make%0Acomputing%20Newton%27s%20update%20impossible.%20Additionally%2C%20we%20show%20that%20when%20coupled%0Awith%20popular%20techniques%20to%20resolve%20the%20degeneracy%2C%20Newton%27s%20method%20often%20incurs%0Aoffensively%20large%20norm%20updates%20and%20empirically%20degrades%20model%20performance%0Apost-unlearning.%20To%20address%20these%20challenges%2C%20we%20propose%20CureNewton%27s%20method%2C%20a%0Aprinciple%20approach%20that%20leverages%20cubic%20regularization%20to%20handle%20the%20Hessian%0Adegeneracy%20effectively.%20The%20added%20regularizer%20eliminates%20the%20need%20for%20manual%0Afinetuning%20and%20affords%20a%20natural%20interpretation%20within%20the%20unlearning%20context.%0AExperiments%20across%20different%20models%20and%20datasets%20show%20that%20our%20method%20can%0Aachieve%20competitive%20unlearning%20performance%20to%20the%20state-of-the-art%20algorithm%20in%0Apractical%20unlearning%20settings%2C%20while%20being%20theoretically%20justified%20and%0Aefficient%20in%20running%20time.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2406.14507v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DOn%2520Newton%2527s%2520Method%2520to%2520Unlearn%2520Neural%2520Networks%26entry.906535625%3DNhung%2520Bui%2520and%2520Xinyang%2520Lu%2520and%2520Rachael%2520Hwee%2520Ling%2520Sim%2520and%2520See-Kiong%2520Ng%2520and%2520Bryan%2520Kian%2520Hsiang%2520Low%26entry.1292438233%3D%2520%2520With%2520the%2520widespread%2520applications%2520of%2520neural%2520networks%2520%2528NNs%2529%2520trained%2520on%2520personal%250Adata%252C%2520machine%2520unlearning%2520has%2520become%2520increasingly%2520important%2520for%2520enabling%250Aindividuals%2520to%2520exercise%2520their%2520personal%2520data%2520ownership%252C%2520particularly%2520the%2520%2522right%250Ato%2520be%2520forgotten%2522%2520from%2520trained%2520NNs.%2520Since%2520retraining%2520is%2520computationally%250Aexpensive%252C%2520we%2520seek%2520approximate%2520unlearning%2520algorithms%2520for%2520NNs%2520that%2520return%250Aidentical%2520models%2520to%2520the%2520retrained%2520oracle.%2520While%2520Newton%2527s%2520method%2520has%2520been%250Asuccessfully%2520used%2520to%2520approximately%2520unlearn%2520linear%2520models%252C%2520we%2520observe%2520that%250Aadapting%2520it%2520for%2520NN%2520is%2520challenging%2520due%2520to%2520degenerate%2520Hessians%2520that%2520make%250Acomputing%2520Newton%2527s%2520update%2520impossible.%2520Additionally%252C%2520we%2520show%2520that%2520when%2520coupled%250Awith%2520popular%2520techniques%2520to%2520resolve%2520the%2520degeneracy%252C%2520Newton%2527s%2520method%2520often%2520incurs%250Aoffensively%2520large%2520norm%2520updates%2520and%2520empirically%2520degrades%2520model%2520performance%250Apost-unlearning.%2520To%2520address%2520these%2520challenges%252C%2520we%2520propose%2520CureNewton%2527s%2520method%252C%2520a%250Aprinciple%2520approach%2520that%2520leverages%2520cubic%2520regularization%2520to%2520handle%2520the%2520Hessian%250Adegeneracy%2520effectively.%2520The%2520added%2520regularizer%2520eliminates%2520the%2520need%2520for%2520manual%250Afinetuning%2520and%2520affords%2520a%2520natural%2520interpretation%2520within%2520the%2520unlearning%2520context.%250AExperiments%2520across%2520different%2520models%2520and%2520datasets%2520show%2520that%2520our%2520method%2520can%250Aachieve%2520competitive%2520unlearning%2520performance%2520to%2520the%2520state-of-the-art%2520algorithm%2520in%250Apractical%2520unlearning%2520settings%252C%2520while%2520being%2520theoretically%2520justified%2520and%250Aefficient%2520in%2520running%2520time.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2406.14507v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=On%20Newton%27s%20Method%20to%20Unlearn%20Neural%20Networks&entry.906535625=Nhung%20Bui%20and%20Xinyang%20Lu%20and%20Rachael%20Hwee%20Ling%20Sim%20and%20See-Kiong%20Ng%20and%20Bryan%20Kian%20Hsiang%20Low&entry.1292438233=%20%20With%20the%20widespread%20applications%20of%20neural%20networks%20%28NNs%29%20trained%20on%20personal%0Adata%2C%20machine%20unlearning%20has%20become%20increasingly%20important%20for%20enabling%0Aindividuals%20to%20exercise%20their%20personal%20data%20ownership%2C%20particularly%20the%20%22right%0Ato%20be%20forgotten%22%20from%20trained%20NNs.%20Since%20retraining%20is%20computationally%0Aexpensive%2C%20we%20seek%20approximate%20unlearning%20algorithms%20for%20NNs%20that%20return%0Aidentical%20models%20to%20the%20retrained%20oracle.%20While%20Newton%27s%20method%20has%20been%0Asuccessfully%20used%20to%20approximately%20unlearn%20linear%20models%2C%20we%20observe%20that%0Aadapting%20it%20for%20NN%20is%20challenging%20due%20to%20degenerate%20Hessians%20that%20make%0Acomputing%20Newton%27s%20update%20impossible.%20Additionally%2C%20we%20show%20that%20when%20coupled%0Awith%20popular%20techniques%20to%20resolve%20the%20degeneracy%2C%20Newton%27s%20method%20often%20incurs%0Aoffensively%20large%20norm%20updates%20and%20empirically%20degrades%20model%20performance%0Apost-unlearning.%20To%20address%20these%20challenges%2C%20we%20propose%20CureNewton%27s%20method%2C%20a%0Aprinciple%20approach%20that%20leverages%20cubic%20regularization%20to%20handle%20the%20Hessian%0Adegeneracy%20effectively.%20The%20added%20regularizer%20eliminates%20the%20need%20for%20manual%0Afinetuning%20and%20affords%20a%20natural%20interpretation%20within%20the%20unlearning%20context.%0AExperiments%20across%20different%20models%20and%20datasets%20show%20that%20our%20method%20can%0Aachieve%20competitive%20unlearning%20performance%20to%20the%20state-of-the-art%20algorithm%20in%0Apractical%20unlearning%20settings%2C%20while%20being%20theoretically%20justified%20and%0Aefficient%20in%20running%20time.%0A&entry.1838667208=http%3A//arxiv.org/abs/2406.14507v2&entry.124074799=Read"},
{"title": "Brain Inspired Probabilistic Occupancy Grid Mapping with\n  Hyperdimensional Computing", "author": "Shay Snyder and Andrew Capodieci and David Gorsich and Maryam Parsa", "abstract": "  Real-time robotic systems require advanced perception, computation, and\naction capability. However, the main bottleneck in current autonomous systems\nis the trade-off between computational capability, energy efficiency and model\ndeterminism. World modeling, a key objective of many robotic systems, commonly\nuses occupancy grid mapping (OGM) as the first step towards building an\nend-to-end robotic system with perception, planning, autonomous maneuvering,\nand decision making capabilities. OGM divides the environment into discrete\ncells and assigns probability values to attributes such as occupancy and\ntraversability. Existing methods fall into two categories: traditional methods\nand neural methods. Traditional methods rely on dense statistical calculations,\nwhile neural methods employ deep learning for probabilistic information\nprocessing. Recent works formulate a deterministic theory of neural computation\nat the intersection of cognitive science and vector symbolic architectures. In\nthis study, we propose a Fourier-based hyperdimensional OGM system, VSA-OGM,\ncombined with a novel application of Shannon entropy that retains the\ninterpretability and stability of traditional methods along with the improved\ncomputational efficiency of neural methods. Our approach, validated across\nmultiple datasets, achieves similar accuracy to covariant traditional methods\nwhile approximately reducing latency by 200x and memory by 1000x. Compared to\ninvariant traditional methods, we see similar accuracy values while reducing\nlatency by 3.7x. Moreover, we achieve 1.5x latency reductions compared to\nneural methods while eliminating the need for domain-specific model training.\n", "link": "http://arxiv.org/abs/2408.09066v3", "date": "2024-08-27", "relevancy": 1.8312, "topK": [{"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.6616}, {"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.6002}, {"title": "RoboEXP: Action-Conditioned Scene Graph via Interactive Exploration for\n  Robotic Manipulation", "link": "http://arxiv.org/abs/2402.15487v1", "similarity": 0.5846}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Brain%20Inspired%20Probabilistic%20Occupancy%20Grid%20Mapping%20with%0A%20%20Hyperdimensional%20Computing&body=Title%3A%20Brain%20Inspired%20Probabilistic%20Occupancy%20Grid%20Mapping%20with%0A%20%20Hyperdimensional%20Computing%0AAuthor%3A%20Shay%20Snyder%20and%20Andrew%20Capodieci%20and%20David%20Gorsich%20and%20Maryam%20Parsa%0AAbstract%3A%20%20%20Real-time%20robotic%20systems%20require%20advanced%20perception%2C%20computation%2C%20and%0Aaction%20capability.%20However%2C%20the%20main%20bottleneck%20in%20current%20autonomous%20systems%0Ais%20the%20trade-off%20between%20computational%20capability%2C%20energy%20efficiency%20and%20model%0Adeterminism.%20World%20modeling%2C%20a%20key%20objective%20of%20many%20robotic%20systems%2C%20commonly%0Auses%20occupancy%20grid%20mapping%20%28OGM%29%20as%20the%20first%20step%20towards%20building%20an%0Aend-to-end%20robotic%20system%20with%20perception%2C%20planning%2C%20autonomous%20maneuvering%2C%0Aand%20decision%20making%20capabilities.%20OGM%20divides%20the%20environment%20into%20discrete%0Acells%20and%20assigns%20probability%20values%20to%20attributes%20such%20as%20occupancy%20and%0Atraversability.%20Existing%20methods%20fall%20into%20two%20categories%3A%20traditional%20methods%0Aand%20neural%20methods.%20Traditional%20methods%20rely%20on%20dense%20statistical%20calculations%2C%0Awhile%20neural%20methods%20employ%20deep%20learning%20for%20probabilistic%20information%0Aprocessing.%20Recent%20works%20formulate%20a%20deterministic%20theory%20of%20neural%20computation%0Aat%20the%20intersection%20of%20cognitive%20science%20and%20vector%20symbolic%20architectures.%20In%0Athis%20study%2C%20we%20propose%20a%20Fourier-based%20hyperdimensional%20OGM%20system%2C%20VSA-OGM%2C%0Acombined%20with%20a%20novel%20application%20of%20Shannon%20entropy%20that%20retains%20the%0Ainterpretability%20and%20stability%20of%20traditional%20methods%20along%20with%20the%20improved%0Acomputational%20efficiency%20of%20neural%20methods.%20Our%20approach%2C%20validated%20across%0Amultiple%20datasets%2C%20achieves%20similar%20accuracy%20to%20covariant%20traditional%20methods%0Awhile%20approximately%20reducing%20latency%20by%20200x%20and%20memory%20by%201000x.%20Compared%20to%0Ainvariant%20traditional%20methods%2C%20we%20see%20similar%20accuracy%20values%20while%20reducing%0Alatency%20by%203.7x.%20Moreover%2C%20we%20achieve%201.5x%20latency%20reductions%20compared%20to%0Aneural%20methods%20while%20eliminating%20the%20need%20for%20domain-specific%20model%20training.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2408.09066v3%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DBrain%2520Inspired%2520Probabilistic%2520Occupancy%2520Grid%2520Mapping%2520with%250A%2520%2520Hyperdimensional%2520Computing%26entry.906535625%3DShay%2520Snyder%2520and%2520Andrew%2520Capodieci%2520and%2520David%2520Gorsich%2520and%2520Maryam%2520Parsa%26entry.1292438233%3D%2520%2520Real-time%2520robotic%2520systems%2520require%2520advanced%2520perception%252C%2520computation%252C%2520and%250Aaction%2520capability.%2520However%252C%2520the%2520main%2520bottleneck%2520in%2520current%2520autonomous%2520systems%250Ais%2520the%2520trade-off%2520between%2520computational%2520capability%252C%2520energy%2520efficiency%2520and%2520model%250Adeterminism.%2520World%2520modeling%252C%2520a%2520key%2520objective%2520of%2520many%2520robotic%2520systems%252C%2520commonly%250Auses%2520occupancy%2520grid%2520mapping%2520%2528OGM%2529%2520as%2520the%2520first%2520step%2520towards%2520building%2520an%250Aend-to-end%2520robotic%2520system%2520with%2520perception%252C%2520planning%252C%2520autonomous%2520maneuvering%252C%250Aand%2520decision%2520making%2520capabilities.%2520OGM%2520divides%2520the%2520environment%2520into%2520discrete%250Acells%2520and%2520assigns%2520probability%2520values%2520to%2520attributes%2520such%2520as%2520occupancy%2520and%250Atraversability.%2520Existing%2520methods%2520fall%2520into%2520two%2520categories%253A%2520traditional%2520methods%250Aand%2520neural%2520methods.%2520Traditional%2520methods%2520rely%2520on%2520dense%2520statistical%2520calculations%252C%250Awhile%2520neural%2520methods%2520employ%2520deep%2520learning%2520for%2520probabilistic%2520information%250Aprocessing.%2520Recent%2520works%2520formulate%2520a%2520deterministic%2520theory%2520of%2520neural%2520computation%250Aat%2520the%2520intersection%2520of%2520cognitive%2520science%2520and%2520vector%2520symbolic%2520architectures.%2520In%250Athis%2520study%252C%2520we%2520propose%2520a%2520Fourier-based%2520hyperdimensional%2520OGM%2520system%252C%2520VSA-OGM%252C%250Acombined%2520with%2520a%2520novel%2520application%2520of%2520Shannon%2520entropy%2520that%2520retains%2520the%250Ainterpretability%2520and%2520stability%2520of%2520traditional%2520methods%2520along%2520with%2520the%2520improved%250Acomputational%2520efficiency%2520of%2520neural%2520methods.%2520Our%2520approach%252C%2520validated%2520across%250Amultiple%2520datasets%252C%2520achieves%2520similar%2520accuracy%2520to%2520covariant%2520traditional%2520methods%250Awhile%2520approximately%2520reducing%2520latency%2520by%2520200x%2520and%2520memory%2520by%25201000x.%2520Compared%2520to%250Ainvariant%2520traditional%2520methods%252C%2520we%2520see%2520similar%2520accuracy%2520values%2520while%2520reducing%250Alatency%2520by%25203.7x.%2520Moreover%252C%2520we%2520achieve%25201.5x%2520latency%2520reductions%2520compared%2520to%250Aneural%2520methods%2520while%2520eliminating%2520the%2520need%2520for%2520domain-specific%2520model%2520training.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2408.09066v3%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Brain%20Inspired%20Probabilistic%20Occupancy%20Grid%20Mapping%20with%0A%20%20Hyperdimensional%20Computing&entry.906535625=Shay%20Snyder%20and%20Andrew%20Capodieci%20and%20David%20Gorsich%20and%20Maryam%20Parsa&entry.1292438233=%20%20Real-time%20robotic%20systems%20require%20advanced%20perception%2C%20computation%2C%20and%0Aaction%20capability.%20However%2C%20the%20main%20bottleneck%20in%20current%20autonomous%20systems%0Ais%20the%20trade-off%20between%20computational%20capability%2C%20energy%20efficiency%20and%20model%0Adeterminism.%20World%20modeling%2C%20a%20key%20objective%20of%20many%20robotic%20systems%2C%20commonly%0Auses%20occupancy%20grid%20mapping%20%28OGM%29%20as%20the%20first%20step%20towards%20building%20an%0Aend-to-end%20robotic%20system%20with%20perception%2C%20planning%2C%20autonomous%20maneuvering%2C%0Aand%20decision%20making%20capabilities.%20OGM%20divides%20the%20environment%20into%20discrete%0Acells%20and%20assigns%20probability%20values%20to%20attributes%20such%20as%20occupancy%20and%0Atraversability.%20Existing%20methods%20fall%20into%20two%20categories%3A%20traditional%20methods%0Aand%20neural%20methods.%20Traditional%20methods%20rely%20on%20dense%20statistical%20calculations%2C%0Awhile%20neural%20methods%20employ%20deep%20learning%20for%20probabilistic%20information%0Aprocessing.%20Recent%20works%20formulate%20a%20deterministic%20theory%20of%20neural%20computation%0Aat%20the%20intersection%20of%20cognitive%20science%20and%20vector%20symbolic%20architectures.%20In%0Athis%20study%2C%20we%20propose%20a%20Fourier-based%20hyperdimensional%20OGM%20system%2C%20VSA-OGM%2C%0Acombined%20with%20a%20novel%20application%20of%20Shannon%20entropy%20that%20retains%20the%0Ainterpretability%20and%20stability%20of%20traditional%20methods%20along%20with%20the%20improved%0Acomputational%20efficiency%20of%20neural%20methods.%20Our%20approach%2C%20validated%20across%0Amultiple%20datasets%2C%20achieves%20similar%20accuracy%20to%20covariant%20traditional%20methods%0Awhile%20approximately%20reducing%20latency%20by%20200x%20and%20memory%20by%201000x.%20Compared%20to%0Ainvariant%20traditional%20methods%2C%20we%20see%20similar%20accuracy%20values%20while%20reducing%0Alatency%20by%203.7x.%20Moreover%2C%20we%20achieve%201.5x%20latency%20reductions%20compared%20to%0Aneural%20methods%20while%20eliminating%20the%20need%20for%20domain-specific%20model%20training.%0A&entry.1838667208=http%3A//arxiv.org/abs/2408.09066v3&entry.124074799=Read"},
{"title": "SelectLLM: Can LLMs Select Important Instructions to Annotate?", "author": "Ritik Sachin Parkar and Jaehyung Kim and Jong Inn Park and Dongyeop Kang", "abstract": "  Instruction tuning benefits from large and diverse datasets; however,\ncreating such datasets involves a high cost of human labeling. While synthetic\ndatasets generated by large language models (LLMs) have partly solved this\nissue, they often contain low-quality data. One effective solution is\nselectively annotating unlabelled instructions, especially given the relative\nease of acquiring unlabeled instructions or texts from various sources.\nHowever, how to select unlabelled instructions is not well-explored, especially\nin the context of LLMs. Therefore, we introduce SelectLLM, an alternative\nframework that leverages the capabilities of LLMs to select unlabeled\ninstructions more effectively. Specifically, SelectLLM consists of two key\nsteps: Coreset-based clustering of unlabelled instructions for enlarging\ndiversity and prompting of LLM to identify the most beneficial instructions\nwithin each cluster. We evaluate SelectLLM on AlpacaEval2 and MT-Bench,\ndemonstrating its ability to outperform state-of-the-art methods like\nAlpagasus. In addition, we compare the performance and compatibility of\nSelectLLM with various LLMs, such as ChatGPT, LLaMA-3.1-70B, and Gemma-2-27b.\nSelectLLM's adaptability and robustness are further evidenced by its ability to\nmaintain high performance across both human and synthetic datasets. All code\nand data are publicly available (https://github.com/minnesotanlp/select-llm).\n", "link": "http://arxiv.org/abs/2401.16553v7", "date": "2024-08-27", "relevancy": 1.828, "topK": [{"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.5014}, {"title": "Self-supervised Photographic Image Layout Representation Learning", "link": "http://arxiv.org/abs/2403.03740v1", "similarity": 0.4491}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.4472}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20SelectLLM%3A%20Can%20LLMs%20Select%20Important%20Instructions%20to%20Annotate%3F&body=Title%3A%20SelectLLM%3A%20Can%20LLMs%20Select%20Important%20Instructions%20to%20Annotate%3F%0AAuthor%3A%20Ritik%20Sachin%20Parkar%20and%20Jaehyung%20Kim%20and%20Jong%20Inn%20Park%20and%20Dongyeop%20Kang%0AAbstract%3A%20%20%20Instruction%20tuning%20benefits%20from%20large%20and%20diverse%20datasets%3B%20however%2C%0Acreating%20such%20datasets%20involves%20a%20high%20cost%20of%20human%20labeling.%20While%20synthetic%0Adatasets%20generated%20by%20large%20language%20models%20%28LLMs%29%20have%20partly%20solved%20this%0Aissue%2C%20they%20often%20contain%20low-quality%20data.%20One%20effective%20solution%20is%0Aselectively%20annotating%20unlabelled%20instructions%2C%20especially%20given%20the%20relative%0Aease%20of%20acquiring%20unlabeled%20instructions%20or%20texts%20from%20various%20sources.%0AHowever%2C%20how%20to%20select%20unlabelled%20instructions%20is%20not%20well-explored%2C%20especially%0Ain%20the%20context%20of%20LLMs.%20Therefore%2C%20we%20introduce%20SelectLLM%2C%20an%20alternative%0Aframework%20that%20leverages%20the%20capabilities%20of%20LLMs%20to%20select%20unlabeled%0Ainstructions%20more%20effectively.%20Specifically%2C%20SelectLLM%20consists%20of%20two%20key%0Asteps%3A%20Coreset-based%20clustering%20of%20unlabelled%20instructions%20for%20enlarging%0Adiversity%20and%20prompting%20of%20LLM%20to%20identify%20the%20most%20beneficial%20instructions%0Awithin%20each%20cluster.%20We%20evaluate%20SelectLLM%20on%20AlpacaEval2%20and%20MT-Bench%2C%0Ademonstrating%20its%20ability%20to%20outperform%20state-of-the-art%20methods%20like%0AAlpagasus.%20In%20addition%2C%20we%20compare%20the%20performance%20and%20compatibility%20of%0ASelectLLM%20with%20various%20LLMs%2C%20such%20as%20ChatGPT%2C%20LLaMA-3.1-70B%2C%20and%20Gemma-2-27b.%0ASelectLLM%27s%20adaptability%20and%20robustness%20are%20further%20evidenced%20by%20its%20ability%20to%0Amaintain%20high%20performance%20across%20both%20human%20and%20synthetic%20datasets.%20All%20code%0Aand%20data%20are%20publicly%20available%20%28https%3A//github.com/minnesotanlp/select-llm%29.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2401.16553v7%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DSelectLLM%253A%2520Can%2520LLMs%2520Select%2520Important%2520Instructions%2520to%2520Annotate%253F%26entry.906535625%3DRitik%2520Sachin%2520Parkar%2520and%2520Jaehyung%2520Kim%2520and%2520Jong%2520Inn%2520Park%2520and%2520Dongyeop%2520Kang%26entry.1292438233%3D%2520%2520Instruction%2520tuning%2520benefits%2520from%2520large%2520and%2520diverse%2520datasets%253B%2520however%252C%250Acreating%2520such%2520datasets%2520involves%2520a%2520high%2520cost%2520of%2520human%2520labeling.%2520While%2520synthetic%250Adatasets%2520generated%2520by%2520large%2520language%2520models%2520%2528LLMs%2529%2520have%2520partly%2520solved%2520this%250Aissue%252C%2520they%2520often%2520contain%2520low-quality%2520data.%2520One%2520effective%2520solution%2520is%250Aselectively%2520annotating%2520unlabelled%2520instructions%252C%2520especially%2520given%2520the%2520relative%250Aease%2520of%2520acquiring%2520unlabeled%2520instructions%2520or%2520texts%2520from%2520various%2520sources.%250AHowever%252C%2520how%2520to%2520select%2520unlabelled%2520instructions%2520is%2520not%2520well-explored%252C%2520especially%250Ain%2520the%2520context%2520of%2520LLMs.%2520Therefore%252C%2520we%2520introduce%2520SelectLLM%252C%2520an%2520alternative%250Aframework%2520that%2520leverages%2520the%2520capabilities%2520of%2520LLMs%2520to%2520select%2520unlabeled%250Ainstructions%2520more%2520effectively.%2520Specifically%252C%2520SelectLLM%2520consists%2520of%2520two%2520key%250Asteps%253A%2520Coreset-based%2520clustering%2520of%2520unlabelled%2520instructions%2520for%2520enlarging%250Adiversity%2520and%2520prompting%2520of%2520LLM%2520to%2520identify%2520the%2520most%2520beneficial%2520instructions%250Awithin%2520each%2520cluster.%2520We%2520evaluate%2520SelectLLM%2520on%2520AlpacaEval2%2520and%2520MT-Bench%252C%250Ademonstrating%2520its%2520ability%2520to%2520outperform%2520state-of-the-art%2520methods%2520like%250AAlpagasus.%2520In%2520addition%252C%2520we%2520compare%2520the%2520performance%2520and%2520compatibility%2520of%250ASelectLLM%2520with%2520various%2520LLMs%252C%2520such%2520as%2520ChatGPT%252C%2520LLaMA-3.1-70B%252C%2520and%2520Gemma-2-27b.%250ASelectLLM%2527s%2520adaptability%2520and%2520robustness%2520are%2520further%2520evidenced%2520by%2520its%2520ability%2520to%250Amaintain%2520high%2520performance%2520across%2520both%2520human%2520and%2520synthetic%2520datasets.%2520All%2520code%250Aand%2520data%2520are%2520publicly%2520available%2520%2528https%253A//github.com/minnesotanlp/select-llm%2529.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2401.16553v7%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=SelectLLM%3A%20Can%20LLMs%20Select%20Important%20Instructions%20to%20Annotate%3F&entry.906535625=Ritik%20Sachin%20Parkar%20and%20Jaehyung%20Kim%20and%20Jong%20Inn%20Park%20and%20Dongyeop%20Kang&entry.1292438233=%20%20Instruction%20tuning%20benefits%20from%20large%20and%20diverse%20datasets%3B%20however%2C%0Acreating%20such%20datasets%20involves%20a%20high%20cost%20of%20human%20labeling.%20While%20synthetic%0Adatasets%20generated%20by%20large%20language%20models%20%28LLMs%29%20have%20partly%20solved%20this%0Aissue%2C%20they%20often%20contain%20low-quality%20data.%20One%20effective%20solution%20is%0Aselectively%20annotating%20unlabelled%20instructions%2C%20especially%20given%20the%20relative%0Aease%20of%20acquiring%20unlabeled%20instructions%20or%20texts%20from%20various%20sources.%0AHowever%2C%20how%20to%20select%20unlabelled%20instructions%20is%20not%20well-explored%2C%20especially%0Ain%20the%20context%20of%20LLMs.%20Therefore%2C%20we%20introduce%20SelectLLM%2C%20an%20alternative%0Aframework%20that%20leverages%20the%20capabilities%20of%20LLMs%20to%20select%20unlabeled%0Ainstructions%20more%20effectively.%20Specifically%2C%20SelectLLM%20consists%20of%20two%20key%0Asteps%3A%20Coreset-based%20clustering%20of%20unlabelled%20instructions%20for%20enlarging%0Adiversity%20and%20prompting%20of%20LLM%20to%20identify%20the%20most%20beneficial%20instructions%0Awithin%20each%20cluster.%20We%20evaluate%20SelectLLM%20on%20AlpacaEval2%20and%20MT-Bench%2C%0Ademonstrating%20its%20ability%20to%20outperform%20state-of-the-art%20methods%20like%0AAlpagasus.%20In%20addition%2C%20we%20compare%20the%20performance%20and%20compatibility%20of%0ASelectLLM%20with%20various%20LLMs%2C%20such%20as%20ChatGPT%2C%20LLaMA-3.1-70B%2C%20and%20Gemma-2-27b.%0ASelectLLM%27s%20adaptability%20and%20robustness%20are%20further%20evidenced%20by%20its%20ability%20to%0Amaintain%20high%20performance%20across%20both%20human%20and%20synthetic%20datasets.%20All%20code%0Aand%20data%20are%20publicly%20available%20%28https%3A//github.com/minnesotanlp/select-llm%29.%0A&entry.1838667208=http%3A//arxiv.org/abs/2401.16553v7&entry.124074799=Read"},
{"title": "Aligning XAI with EU Regulations for Smart Biomedical Devices: A\n  Methodology for Compliance Analysis", "author": "Francesco Sovrano and Michael Lognoul and Giulia Vilone", "abstract": "  Significant investment and development have gone into integrating Artificial\nIntelligence (AI) in medical and healthcare applications, leading to advanced\ncontrol systems in medical technology. However, the opacity of AI systems\nraises concerns about essential characteristics needed in such sensitive\napplications, like transparency and trustworthiness. Our study addresses these\nconcerns by investigating a process for selecting the most adequate Explainable\nAI (XAI) methods to comply with the explanation requirements of key EU\nregulations in the context of smart bioelectronics for medical devices. The\nadopted methodology starts with categorising smart devices by their control\nmechanisms (open-loop, closed-loop, and semi-closed-loop systems) and delving\ninto their technology. Then, we analyse these regulations to define their\nexplainability requirements for the various devices and related goals.\nSimultaneously, we classify XAI methods by their explanatory objectives. This\nallows for matching legal explainability requirements with XAI explanatory\ngoals and determining the suitable XAI algorithms for achieving them. Our\nfindings provide a nuanced understanding of which XAI algorithms align better\nwith EU regulations for different types of medical devices. We demonstrate this\nthrough practical case studies on different neural implants, from chronic\ndisease management to advanced prosthetics. This study fills a crucial gap in\naligning XAI applications in bioelectronics with stringent provisions of EU\nregulations. It provides a practical framework for developers and researchers,\nensuring their AI innovations advance healthcare technology and adhere to legal\nand ethical standards.\n", "link": "http://arxiv.org/abs/2408.15121v1", "date": "2024-08-27", "relevancy": 1.3083, "topK": [{"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.4487}, {"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.4478}, {"title": "Egocentric Computer Vision for Hands-Free Robotic Wheelchair Navigation", "link": "https://link.springer.com/article/10.1007/s10846-023-01807-4", "similarity": 0.4263}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Aligning%20XAI%20with%20EU%20Regulations%20for%20Smart%20Biomedical%20Devices%3A%20A%0A%20%20Methodology%20for%20Compliance%20Analysis&body=Title%3A%20Aligning%20XAI%20with%20EU%20Regulations%20for%20Smart%20Biomedical%20Devices%3A%20A%0A%20%20Methodology%20for%20Compliance%20Analysis%0AAuthor%3A%20Francesco%20Sovrano%20and%20Michael%20Lognoul%20and%20Giulia%20Vilone%0AAbstract%3A%20%20%20Significant%20investment%20and%20development%20have%20gone%20into%20integrating%20Artificial%0AIntelligence%20%28AI%29%20in%20medical%20and%20healthcare%20applications%2C%20leading%20to%20advanced%0Acontrol%20systems%20in%20medical%20technology.%20However%2C%20the%20opacity%20of%20AI%20systems%0Araises%20concerns%20about%20essential%20characteristics%20needed%20in%20such%20sensitive%0Aapplications%2C%20like%20transparency%20and%20trustworthiness.%20Our%20study%20addresses%20these%0Aconcerns%20by%20investigating%20a%20process%20for%20selecting%20the%20most%20adequate%20Explainable%0AAI%20%28XAI%29%20methods%20to%20comply%20with%20the%20explanation%20requirements%20of%20key%20EU%0Aregulations%20in%20the%20context%20of%20smart%20bioelectronics%20for%20medical%20devices.%20The%0Aadopted%20methodology%20starts%20with%20categorising%20smart%20devices%20by%20their%20control%0Amechanisms%20%28open-loop%2C%20closed-loop%2C%20and%20semi-closed-loop%20systems%29%20and%20delving%0Ainto%20their%20technology.%20Then%2C%20we%20analyse%20these%20regulations%20to%20define%20their%0Aexplainability%20requirements%20for%20the%20various%20devices%20and%20related%20goals.%0ASimultaneously%2C%20we%20classify%20XAI%20methods%20by%20their%20explanatory%20objectives.%20This%0Aallows%20for%20matching%20legal%20explainability%20requirements%20with%20XAI%20explanatory%0Agoals%20and%20determining%20the%20suitable%20XAI%20algorithms%20for%20achieving%20them.%20Our%0Afindings%20provide%20a%20nuanced%20understanding%20of%20which%20XAI%20algorithms%20align%20better%0Awith%20EU%20regulations%20for%20different%20types%20of%20medical%20devices.%20We%20demonstrate%20this%0Athrough%20practical%20case%20studies%20on%20different%20neural%20implants%2C%20from%20chronic%0Adisease%20management%20to%20advanced%20prosthetics.%20This%20study%20fills%20a%20crucial%20gap%20in%0Aaligning%20XAI%20applications%20in%20bioelectronics%20with%20stringent%20provisions%20of%20EU%0Aregulations.%20It%20provides%20a%20practical%20framework%20for%20developers%20and%20researchers%2C%0Aensuring%20their%20AI%20innovations%20advance%20healthcare%20technology%20and%20adhere%20to%20legal%0Aand%20ethical%20standards.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2408.15121v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DAligning%2520XAI%2520with%2520EU%2520Regulations%2520for%2520Smart%2520Biomedical%2520Devices%253A%2520A%250A%2520%2520Methodology%2520for%2520Compliance%2520Analysis%26entry.906535625%3DFrancesco%2520Sovrano%2520and%2520Michael%2520Lognoul%2520and%2520Giulia%2520Vilone%26entry.1292438233%3D%2520%2520Significant%2520investment%2520and%2520development%2520have%2520gone%2520into%2520integrating%2520Artificial%250AIntelligence%2520%2528AI%2529%2520in%2520medical%2520and%2520healthcare%2520applications%252C%2520leading%2520to%2520advanced%250Acontrol%2520systems%2520in%2520medical%2520technology.%2520However%252C%2520the%2520opacity%2520of%2520AI%2520systems%250Araises%2520concerns%2520about%2520essential%2520characteristics%2520needed%2520in%2520such%2520sensitive%250Aapplications%252C%2520like%2520transparency%2520and%2520trustworthiness.%2520Our%2520study%2520addresses%2520these%250Aconcerns%2520by%2520investigating%2520a%2520process%2520for%2520selecting%2520the%2520most%2520adequate%2520Explainable%250AAI%2520%2528XAI%2529%2520methods%2520to%2520comply%2520with%2520the%2520explanation%2520requirements%2520of%2520key%2520EU%250Aregulations%2520in%2520the%2520context%2520of%2520smart%2520bioelectronics%2520for%2520medical%2520devices.%2520The%250Aadopted%2520methodology%2520starts%2520with%2520categorising%2520smart%2520devices%2520by%2520their%2520control%250Amechanisms%2520%2528open-loop%252C%2520closed-loop%252C%2520and%2520semi-closed-loop%2520systems%2529%2520and%2520delving%250Ainto%2520their%2520technology.%2520Then%252C%2520we%2520analyse%2520these%2520regulations%2520to%2520define%2520their%250Aexplainability%2520requirements%2520for%2520the%2520various%2520devices%2520and%2520related%2520goals.%250ASimultaneously%252C%2520we%2520classify%2520XAI%2520methods%2520by%2520their%2520explanatory%2520objectives.%2520This%250Aallows%2520for%2520matching%2520legal%2520explainability%2520requirements%2520with%2520XAI%2520explanatory%250Agoals%2520and%2520determining%2520the%2520suitable%2520XAI%2520algorithms%2520for%2520achieving%2520them.%2520Our%250Afindings%2520provide%2520a%2520nuanced%2520understanding%2520of%2520which%2520XAI%2520algorithms%2520align%2520better%250Awith%2520EU%2520regulations%2520for%2520different%2520types%2520of%2520medical%2520devices.%2520We%2520demonstrate%2520this%250Athrough%2520practical%2520case%2520studies%2520on%2520different%2520neural%2520implants%252C%2520from%2520chronic%250Adisease%2520management%2520to%2520advanced%2520prosthetics.%2520This%2520study%2520fills%2520a%2520crucial%2520gap%2520in%250Aaligning%2520XAI%2520applications%2520in%2520bioelectronics%2520with%2520stringent%2520provisions%2520of%2520EU%250Aregulations.%2520It%2520provides%2520a%2520practical%2520framework%2520for%2520developers%2520and%2520researchers%252C%250Aensuring%2520their%2520AI%2520innovations%2520advance%2520healthcare%2520technology%2520and%2520adhere%2520to%2520legal%250Aand%2520ethical%2520standards.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2408.15121v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Aligning%20XAI%20with%20EU%20Regulations%20for%20Smart%20Biomedical%20Devices%3A%20A%0A%20%20Methodology%20for%20Compliance%20Analysis&entry.906535625=Francesco%20Sovrano%20and%20Michael%20Lognoul%20and%20Giulia%20Vilone&entry.1292438233=%20%20Significant%20investment%20and%20development%20have%20gone%20into%20integrating%20Artificial%0AIntelligence%20%28AI%29%20in%20medical%20and%20healthcare%20applications%2C%20leading%20to%20advanced%0Acontrol%20systems%20in%20medical%20technology.%20However%2C%20the%20opacity%20of%20AI%20systems%0Araises%20concerns%20about%20essential%20characteristics%20needed%20in%20such%20sensitive%0Aapplications%2C%20like%20transparency%20and%20trustworthiness.%20Our%20study%20addresses%20these%0Aconcerns%20by%20investigating%20a%20process%20for%20selecting%20the%20most%20adequate%20Explainable%0AAI%20%28XAI%29%20methods%20to%20comply%20with%20the%20explanation%20requirements%20of%20key%20EU%0Aregulations%20in%20the%20context%20of%20smart%20bioelectronics%20for%20medical%20devices.%20The%0Aadopted%20methodology%20starts%20with%20categorising%20smart%20devices%20by%20their%20control%0Amechanisms%20%28open-loop%2C%20closed-loop%2C%20and%20semi-closed-loop%20systems%29%20and%20delving%0Ainto%20their%20technology.%20Then%2C%20we%20analyse%20these%20regulations%20to%20define%20their%0Aexplainability%20requirements%20for%20the%20various%20devices%20and%20related%20goals.%0ASimultaneously%2C%20we%20classify%20XAI%20methods%20by%20their%20explanatory%20objectives.%20This%0Aallows%20for%20matching%20legal%20explainability%20requirements%20with%20XAI%20explanatory%0Agoals%20and%20determining%20the%20suitable%20XAI%20algorithms%20for%20achieving%20them.%20Our%0Afindings%20provide%20a%20nuanced%20understanding%20of%20which%20XAI%20algorithms%20align%20better%0Awith%20EU%20regulations%20for%20different%20types%20of%20medical%20devices.%20We%20demonstrate%20this%0Athrough%20practical%20case%20studies%20on%20different%20neural%20implants%2C%20from%20chronic%0Adisease%20management%20to%20advanced%20prosthetics.%20This%20study%20fills%20a%20crucial%20gap%20in%0Aaligning%20XAI%20applications%20in%20bioelectronics%20with%20stringent%20provisions%20of%20EU%0Aregulations.%20It%20provides%20a%20practical%20framework%20for%20developers%20and%20researchers%2C%0Aensuring%20their%20AI%20innovations%20advance%20healthcare%20technology%20and%20adhere%20to%20legal%0Aand%20ethical%20standards.%0A&entry.1838667208=http%3A//arxiv.org/abs/2408.15121v1&entry.124074799=Read"},
{"title": "Foundation Models for Music: A Survey", "author": "Yinghao Ma and Anders \u00d8land and Anton Ragni and Bleiz MacSen Del Sette and Charalampos Saitis and Chris Donahue and Chenghua Lin and Christos Plachouras and Emmanouil Benetos and Elio Quinton and Elona Shatri and Fabio Morreale and Ge Zhang and Gy\u00f6rgy Fazekas and Gus Xia and Huan Zhang and Ilaria Manco and Jiawen Huang and Julien Guinot and Liwei Lin and Luca Marinelli and Max W. Y. Lam and Megha Sharma and Qiuqiang Kong and Roger B. Dannenberg and Ruibin Yuan and Shangda Wu and Shih-Lun Wu and Shuqi Dai and Shun Lei and Shiyin Kang and Simon Dixon and Wenhu Chen and Wenhao Huang and Xingjian Du and Xingwei Qu and Xu Tan and Yizhi Li and Zeyue Tian and Zhiyong Wu and Zhizheng Wu and Ziyang Ma and Ziyu Wang", "abstract": "  In recent years, foundation models (FMs) such as large language models (LLMs)\nand latent diffusion models (LDMs) have profoundly impacted diverse sectors,\nincluding music. This comprehensive review examines state-of-the-art (SOTA)\npre-trained models and foundation models in music, spanning from representation\nlearning, generative learning and multimodal learning. We first contextualise\nthe significance of music in various industries and trace the evolution of AI\nin music. By delineating the modalities targeted by foundation models, we\ndiscover many of the music representations are underexplored in FM development.\nThen, emphasis is placed on the lack of versatility of previous methods on\ndiverse music applications, along with the potential of FMs in music\nunderstanding, generation and medical application. By comprehensively exploring\nthe details of the model pre-training paradigm, architectural choices,\ntokenisation, finetuning methodologies and controllability, we emphasise the\nimportant topics that should have been well explored, like instruction tuning\nand in-context learning, scaling law and emergent ability, as well as\nlong-sequence modelling etc. A dedicated section presents insights into music\nagents, accompanied by a thorough analysis of datasets and evaluations\nessential for pre-training and downstream tasks. Finally, by underscoring the\nvital importance of ethical considerations, we advocate that following research\non FM for music should focus more on such issues as interpretability,\ntransparency, human responsibility, and copyright issues. The paper offers\ninsights into future challenges and trends on FMs for music, aiming to shape\nthe trajectory of human-AI collaboration in the music realm.\n", "link": "http://arxiv.org/abs/2408.14340v2", "date": "2024-08-27", "relevancy": 1.3924, "topK": [{"title": "Scaling Rectified Flow Transformers for High-Resolution Image Synthesis", "link": "http://arxiv.org/abs/2403.03206v1", "similarity": 0.4726}, {"title": "Multimodal-Conditioned Latent Diffusion Models for Fashion Image Editing", "link": "http://arxiv.org/abs/2403.14828v2", "similarity": 0.4677}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.4594}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Foundation%20Models%20for%20Music%3A%20A%20Survey&body=Title%3A%20Foundation%20Models%20for%20Music%3A%20A%20Survey%0AAuthor%3A%20Yinghao%20Ma%20and%20Anders%20%C3%98land%20and%20Anton%20Ragni%20and%20Bleiz%20MacSen%20Del%20Sette%20and%20Charalampos%20Saitis%20and%20Chris%20Donahue%20and%20Chenghua%20Lin%20and%20Christos%20Plachouras%20and%20Emmanouil%20Benetos%20and%20Elio%20Quinton%20and%20Elona%20Shatri%20and%20Fabio%20Morreale%20and%20Ge%20Zhang%20and%20Gy%C3%B6rgy%20Fazekas%20and%20Gus%20Xia%20and%20Huan%20Zhang%20and%20Ilaria%20Manco%20and%20Jiawen%20Huang%20and%20Julien%20Guinot%20and%20Liwei%20Lin%20and%20Luca%20Marinelli%20and%20Max%20W.%20Y.%20Lam%20and%20Megha%20Sharma%20and%20Qiuqiang%20Kong%20and%20Roger%20B.%20Dannenberg%20and%20Ruibin%20Yuan%20and%20Shangda%20Wu%20and%20Shih-Lun%20Wu%20and%20Shuqi%20Dai%20and%20Shun%20Lei%20and%20Shiyin%20Kang%20and%20Simon%20Dixon%20and%20Wenhu%20Chen%20and%20Wenhao%20Huang%20and%20Xingjian%20Du%20and%20Xingwei%20Qu%20and%20Xu%20Tan%20and%20Yizhi%20Li%20and%20Zeyue%20Tian%20and%20Zhiyong%20Wu%20and%20Zhizheng%20Wu%20and%20Ziyang%20Ma%20and%20Ziyu%20Wang%0AAbstract%3A%20%20%20In%20recent%20years%2C%20foundation%20models%20%28FMs%29%20such%20as%20large%20language%20models%20%28LLMs%29%0Aand%20latent%20diffusion%20models%20%28LDMs%29%20have%20profoundly%20impacted%20diverse%20sectors%2C%0Aincluding%20music.%20This%20comprehensive%20review%20examines%20state-of-the-art%20%28SOTA%29%0Apre-trained%20models%20and%20foundation%20models%20in%20music%2C%20spanning%20from%20representation%0Alearning%2C%20generative%20learning%20and%20multimodal%20learning.%20We%20first%20contextualise%0Athe%20significance%20of%20music%20in%20various%20industries%20and%20trace%20the%20evolution%20of%20AI%0Ain%20music.%20By%20delineating%20the%20modalities%20targeted%20by%20foundation%20models%2C%20we%0Adiscover%20many%20of%20the%20music%20representations%20are%20underexplored%20in%20FM%20development.%0AThen%2C%20emphasis%20is%20placed%20on%20the%20lack%20of%20versatility%20of%20previous%20methods%20on%0Adiverse%20music%20applications%2C%20along%20with%20the%20potential%20of%20FMs%20in%20music%0Aunderstanding%2C%20generation%20and%20medical%20application.%20By%20comprehensively%20exploring%0Athe%20details%20of%20the%20model%20pre-training%20paradigm%2C%20architectural%20choices%2C%0Atokenisation%2C%20finetuning%20methodologies%20and%20controllability%2C%20we%20emphasise%20the%0Aimportant%20topics%20that%20should%20have%20been%20well%20explored%2C%20like%20instruction%20tuning%0Aand%20in-context%20learning%2C%20scaling%20law%20and%20emergent%20ability%2C%20as%20well%20as%0Along-sequence%20modelling%20etc.%20A%20dedicated%20section%20presents%20insights%20into%20music%0Aagents%2C%20accompanied%20by%20a%20thorough%20analysis%20of%20datasets%20and%20evaluations%0Aessential%20for%20pre-training%20and%20downstream%20tasks.%20Finally%2C%20by%20underscoring%20the%0Avital%20importance%20of%20ethical%20considerations%2C%20we%20advocate%20that%20following%20research%0Aon%20FM%20for%20music%20should%20focus%20more%20on%20such%20issues%20as%20interpretability%2C%0Atransparency%2C%20human%20responsibility%2C%20and%20copyright%20issues.%20The%20paper%20offers%0Ainsights%20into%20future%20challenges%20and%20trends%20on%20FMs%20for%20music%2C%20aiming%20to%20shape%0Athe%20trajectory%20of%20human-AI%20collaboration%20in%20the%20music%20realm.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2408.14340v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DFoundation%2520Models%2520for%2520Music%253A%2520A%2520Survey%26entry.906535625%3DYinghao%2520Ma%2520and%2520Anders%2520%25C3%2598land%2520and%2520Anton%2520Ragni%2520and%2520Bleiz%2520MacSen%2520Del%2520Sette%2520and%2520Charalampos%2520Saitis%2520and%2520Chris%2520Donahue%2520and%2520Chenghua%2520Lin%2520and%2520Christos%2520Plachouras%2520and%2520Emmanouil%2520Benetos%2520and%2520Elio%2520Quinton%2520and%2520Elona%2520Shatri%2520and%2520Fabio%2520Morreale%2520and%2520Ge%2520Zhang%2520and%2520Gy%25C3%25B6rgy%2520Fazekas%2520and%2520Gus%2520Xia%2520and%2520Huan%2520Zhang%2520and%2520Ilaria%2520Manco%2520and%2520Jiawen%2520Huang%2520and%2520Julien%2520Guinot%2520and%2520Liwei%2520Lin%2520and%2520Luca%2520Marinelli%2520and%2520Max%2520W.%2520Y.%2520Lam%2520and%2520Megha%2520Sharma%2520and%2520Qiuqiang%2520Kong%2520and%2520Roger%2520B.%2520Dannenberg%2520and%2520Ruibin%2520Yuan%2520and%2520Shangda%2520Wu%2520and%2520Shih-Lun%2520Wu%2520and%2520Shuqi%2520Dai%2520and%2520Shun%2520Lei%2520and%2520Shiyin%2520Kang%2520and%2520Simon%2520Dixon%2520and%2520Wenhu%2520Chen%2520and%2520Wenhao%2520Huang%2520and%2520Xingjian%2520Du%2520and%2520Xingwei%2520Qu%2520and%2520Xu%2520Tan%2520and%2520Yizhi%2520Li%2520and%2520Zeyue%2520Tian%2520and%2520Zhiyong%2520Wu%2520and%2520Zhizheng%2520Wu%2520and%2520Ziyang%2520Ma%2520and%2520Ziyu%2520Wang%26entry.1292438233%3D%2520%2520In%2520recent%2520years%252C%2520foundation%2520models%2520%2528FMs%2529%2520such%2520as%2520large%2520language%2520models%2520%2528LLMs%2529%250Aand%2520latent%2520diffusion%2520models%2520%2528LDMs%2529%2520have%2520profoundly%2520impacted%2520diverse%2520sectors%252C%250Aincluding%2520music.%2520This%2520comprehensive%2520review%2520examines%2520state-of-the-art%2520%2528SOTA%2529%250Apre-trained%2520models%2520and%2520foundation%2520models%2520in%2520music%252C%2520spanning%2520from%2520representation%250Alearning%252C%2520generative%2520learning%2520and%2520multimodal%2520learning.%2520We%2520first%2520contextualise%250Athe%2520significance%2520of%2520music%2520in%2520various%2520industries%2520and%2520trace%2520the%2520evolution%2520of%2520AI%250Ain%2520music.%2520By%2520delineating%2520the%2520modalities%2520targeted%2520by%2520foundation%2520models%252C%2520we%250Adiscover%2520many%2520of%2520the%2520music%2520representations%2520are%2520underexplored%2520in%2520FM%2520development.%250AThen%252C%2520emphasis%2520is%2520placed%2520on%2520the%2520lack%2520of%2520versatility%2520of%2520previous%2520methods%2520on%250Adiverse%2520music%2520applications%252C%2520along%2520with%2520the%2520potential%2520of%2520FMs%2520in%2520music%250Aunderstanding%252C%2520generation%2520and%2520medical%2520application.%2520By%2520comprehensively%2520exploring%250Athe%2520details%2520of%2520the%2520model%2520pre-training%2520paradigm%252C%2520architectural%2520choices%252C%250Atokenisation%252C%2520finetuning%2520methodologies%2520and%2520controllability%252C%2520we%2520emphasise%2520the%250Aimportant%2520topics%2520that%2520should%2520have%2520been%2520well%2520explored%252C%2520like%2520instruction%2520tuning%250Aand%2520in-context%2520learning%252C%2520scaling%2520law%2520and%2520emergent%2520ability%252C%2520as%2520well%2520as%250Along-sequence%2520modelling%2520etc.%2520A%2520dedicated%2520section%2520presents%2520insights%2520into%2520music%250Aagents%252C%2520accompanied%2520by%2520a%2520thorough%2520analysis%2520of%2520datasets%2520and%2520evaluations%250Aessential%2520for%2520pre-training%2520and%2520downstream%2520tasks.%2520Finally%252C%2520by%2520underscoring%2520the%250Avital%2520importance%2520of%2520ethical%2520considerations%252C%2520we%2520advocate%2520that%2520following%2520research%250Aon%2520FM%2520for%2520music%2520should%2520focus%2520more%2520on%2520such%2520issues%2520as%2520interpretability%252C%250Atransparency%252C%2520human%2520responsibility%252C%2520and%2520copyright%2520issues.%2520The%2520paper%2520offers%250Ainsights%2520into%2520future%2520challenges%2520and%2520trends%2520on%2520FMs%2520for%2520music%252C%2520aiming%2520to%2520shape%250Athe%2520trajectory%2520of%2520human-AI%2520collaboration%2520in%2520the%2520music%2520realm.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2408.14340v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Foundation%20Models%20for%20Music%3A%20A%20Survey&entry.906535625=Yinghao%20Ma%20and%20Anders%20%C3%98land%20and%20Anton%20Ragni%20and%20Bleiz%20MacSen%20Del%20Sette%20and%20Charalampos%20Saitis%20and%20Chris%20Donahue%20and%20Chenghua%20Lin%20and%20Christos%20Plachouras%20and%20Emmanouil%20Benetos%20and%20Elio%20Quinton%20and%20Elona%20Shatri%20and%20Fabio%20Morreale%20and%20Ge%20Zhang%20and%20Gy%C3%B6rgy%20Fazekas%20and%20Gus%20Xia%20and%20Huan%20Zhang%20and%20Ilaria%20Manco%20and%20Jiawen%20Huang%20and%20Julien%20Guinot%20and%20Liwei%20Lin%20and%20Luca%20Marinelli%20and%20Max%20W.%20Y.%20Lam%20and%20Megha%20Sharma%20and%20Qiuqiang%20Kong%20and%20Roger%20B.%20Dannenberg%20and%20Ruibin%20Yuan%20and%20Shangda%20Wu%20and%20Shih-Lun%20Wu%20and%20Shuqi%20Dai%20and%20Shun%20Lei%20and%20Shiyin%20Kang%20and%20Simon%20Dixon%20and%20Wenhu%20Chen%20and%20Wenhao%20Huang%20and%20Xingjian%20Du%20and%20Xingwei%20Qu%20and%20Xu%20Tan%20and%20Yizhi%20Li%20and%20Zeyue%20Tian%20and%20Zhiyong%20Wu%20and%20Zhizheng%20Wu%20and%20Ziyang%20Ma%20and%20Ziyu%20Wang&entry.1292438233=%20%20In%20recent%20years%2C%20foundation%20models%20%28FMs%29%20such%20as%20large%20language%20models%20%28LLMs%29%0Aand%20latent%20diffusion%20models%20%28LDMs%29%20have%20profoundly%20impacted%20diverse%20sectors%2C%0Aincluding%20music.%20This%20comprehensive%20review%20examines%20state-of-the-art%20%28SOTA%29%0Apre-trained%20models%20and%20foundation%20models%20in%20music%2C%20spanning%20from%20representation%0Alearning%2C%20generative%20learning%20and%20multimodal%20learning.%20We%20first%20contextualise%0Athe%20significance%20of%20music%20in%20various%20industries%20and%20trace%20the%20evolution%20of%20AI%0Ain%20music.%20By%20delineating%20the%20modalities%20targeted%20by%20foundation%20models%2C%20we%0Adiscover%20many%20of%20the%20music%20representations%20are%20underexplored%20in%20FM%20development.%0AThen%2C%20emphasis%20is%20placed%20on%20the%20lack%20of%20versatility%20of%20previous%20methods%20on%0Adiverse%20music%20applications%2C%20along%20with%20the%20potential%20of%20FMs%20in%20music%0Aunderstanding%2C%20generation%20and%20medical%20application.%20By%20comprehensively%20exploring%0Athe%20details%20of%20the%20model%20pre-training%20paradigm%2C%20architectural%20choices%2C%0Atokenisation%2C%20finetuning%20methodologies%20and%20controllability%2C%20we%20emphasise%20the%0Aimportant%20topics%20that%20should%20have%20been%20well%20explored%2C%20like%20instruction%20tuning%0Aand%20in-context%20learning%2C%20scaling%20law%20and%20emergent%20ability%2C%20as%20well%20as%0Along-sequence%20modelling%20etc.%20A%20dedicated%20section%20presents%20insights%20into%20music%0Aagents%2C%20accompanied%20by%20a%20thorough%20analysis%20of%20datasets%20and%20evaluations%0Aessential%20for%20pre-training%20and%20downstream%20tasks.%20Finally%2C%20by%20underscoring%20the%0Avital%20importance%20of%20ethical%20considerations%2C%20we%20advocate%20that%20following%20research%0Aon%20FM%20for%20music%20should%20focus%20more%20on%20such%20issues%20as%20interpretability%2C%0Atransparency%2C%20human%20responsibility%2C%20and%20copyright%20issues.%20The%20paper%20offers%0Ainsights%20into%20future%20challenges%20and%20trends%20on%20FMs%20for%20music%2C%20aiming%20to%20shape%0Athe%20trajectory%20of%20human-AI%20collaboration%20in%20the%20music%20realm.%0A&entry.1838667208=http%3A//arxiv.org/abs/2408.14340v2&entry.124074799=Read"},
{"title": "Deep R Programming", "author": "Marek Gagolewski", "abstract": "  Deep R Programming is a comprehensive and in-depth introductory course on one\nof the most popular languages for data science. It equips ambitious students,\nprofessionals, and researchers with the knowledge and skills to become\nindependent users of this potent environment so that they can tackle any\nproblem related to data wrangling and analytics, numerical computing,\nstatistics, and machine learning. This textbook is a non-profit project. Its\nonline and PDF versions are freely available at\n<https://deepr.gagolewski.com/>.\n", "link": "http://arxiv.org/abs/2301.01188v4", "date": "2024-08-27", "relevancy": 1.5512, "topK": [{"title": "DressCode: Autoregressively Sewing and Generating Garments from Text\n  Guidance", "link": "http://arxiv.org/abs/2401.16465v3", "similarity": 0.3997}, {"title": "RTG-SLAM: Real-time 3D Reconstruction at Scale using Gaussian Splatting", "link": "http://arxiv.org/abs/2404.19706v1", "similarity": 0.386}, {"title": "RoboEXP: Action-Conditioned Scene Graph via Interactive Exploration for\n  Robotic Manipulation", "link": "http://arxiv.org/abs/2402.15487v1", "similarity": 0.3625}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Deep%20R%20Programming&body=Title%3A%20Deep%20R%20Programming%0AAuthor%3A%20Marek%20Gagolewski%0AAbstract%3A%20%20%20Deep%20R%20Programming%20is%20a%20comprehensive%20and%20in-depth%20introductory%20course%20on%20one%0Aof%20the%20most%20popular%20languages%20for%20data%20science.%20It%20equips%20ambitious%20students%2C%0Aprofessionals%2C%20and%20researchers%20with%20the%20knowledge%20and%20skills%20to%20become%0Aindependent%20users%20of%20this%20potent%20environment%20so%20that%20they%20can%20tackle%20any%0Aproblem%20related%20to%20data%20wrangling%20and%20analytics%2C%20numerical%20computing%2C%0Astatistics%2C%20and%20machine%20learning.%20This%20textbook%20is%20a%20non-profit%20project.%20Its%0Aonline%20and%20PDF%20versions%20are%20freely%20available%20at%0A%3Chttps%3A//deepr.gagolewski.com/%3E.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2301.01188v4%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DDeep%2520R%2520Programming%26entry.906535625%3DMarek%2520Gagolewski%26entry.1292438233%3D%2520%2520Deep%2520R%2520Programming%2520is%2520a%2520comprehensive%2520and%2520in-depth%2520introductory%2520course%2520on%2520one%250Aof%2520the%2520most%2520popular%2520languages%2520for%2520data%2520science.%2520It%2520equips%2520ambitious%2520students%252C%250Aprofessionals%252C%2520and%2520researchers%2520with%2520the%2520knowledge%2520and%2520skills%2520to%2520become%250Aindependent%2520users%2520of%2520this%2520potent%2520environment%2520so%2520that%2520they%2520can%2520tackle%2520any%250Aproblem%2520related%2520to%2520data%2520wrangling%2520and%2520analytics%252C%2520numerical%2520computing%252C%250Astatistics%252C%2520and%2520machine%2520learning.%2520This%2520textbook%2520is%2520a%2520non-profit%2520project.%2520Its%250Aonline%2520and%2520PDF%2520versions%2520are%2520freely%2520available%2520at%250A%253Chttps%253A//deepr.gagolewski.com/%253E.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2301.01188v4%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Deep%20R%20Programming&entry.906535625=Marek%20Gagolewski&entry.1292438233=%20%20Deep%20R%20Programming%20is%20a%20comprehensive%20and%20in-depth%20introductory%20course%20on%20one%0Aof%20the%20most%20popular%20languages%20for%20data%20science.%20It%20equips%20ambitious%20students%2C%0Aprofessionals%2C%20and%20researchers%20with%20the%20knowledge%20and%20skills%20to%20become%0Aindependent%20users%20of%20this%20potent%20environment%20so%20that%20they%20can%20tackle%20any%0Aproblem%20related%20to%20data%20wrangling%20and%20analytics%2C%20numerical%20computing%2C%0Astatistics%2C%20and%20machine%20learning.%20This%20textbook%20is%20a%20non-profit%20project.%20Its%0Aonline%20and%20PDF%20versions%20are%20freely%20available%20at%0A%3Chttps%3A//deepr.gagolewski.com/%3E.%0A&entry.1838667208=http%3A//arxiv.org/abs/2301.01188v4&entry.124074799=Read"},
{"title": "A domain decomposition-based autoregressive deep learning model for\n  unsteady and nonlinear partial differential equations", "author": "Sheel Nidhan and Haoliang Jiang and Lalit Ghule and Clancy Umphrey and Rishikesh Ranade and Jay Pathak", "abstract": "  In this paper, we propose a domain-decomposition-based deep learning (DL)\nframework, named transient-CoMLSim, for accurately modeling unsteady and\nnonlinear partial differential equations (PDEs). The framework consists of two\nkey components: (a) a convolutional neural network (CNN)-based autoencoder\narchitecture and (b) an autoregressive model composed of fully connected\nlayers. Unlike existing state-of-the-art methods that operate on the entire\ncomputational domain, our CNN-based autoencoder computes a lower-dimensional\nbasis for solution and condition fields represented on subdomains. Timestepping\nis performed entirely in the latent space, generating embeddings of the\nsolution variables from the time history of embeddings of solution and\ncondition variables. This approach not only reduces computational complexity\nbut also enhances scalability, making it well-suited for large-scale\nsimulations. Furthermore, to improve the stability of our rollouts, we employ a\ncurriculum learning (CL) approach during the training of the autoregressive\nmodel. The domain-decomposition strategy enables scaling to out-of-distribution\ndomain sizes while maintaining the accuracy of predictions -- a feature not\neasily integrated into popular DL-based approaches for physics simulations. We\nbenchmark our model against two widely-used DL architectures, Fourier Neural\nOperator (FNO) and U-Net, and demonstrate that our framework outperforms them\nin terms of accuracy, extrapolation to unseen timesteps, and stability for a\nwide range of use cases.\n", "link": "http://arxiv.org/abs/2408.14461v2", "date": "2024-08-27", "relevancy": 1.0497, "topK": [{"title": "SDXS: Real-Time One-Step Latent Diffusion Models with Image Conditions", "link": "http://arxiv.org/abs/2403.16627v1", "similarity": 0.5525}, {"title": "Scaling Rectified Flow Transformers for High-Resolution Image Synthesis", "link": "http://arxiv.org/abs/2403.03206v1", "similarity": 0.5213}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.5009}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20A%20domain%20decomposition-based%20autoregressive%20deep%20learning%20model%20for%0A%20%20unsteady%20and%20nonlinear%20partial%20differential%20equations&body=Title%3A%20A%20domain%20decomposition-based%20autoregressive%20deep%20learning%20model%20for%0A%20%20unsteady%20and%20nonlinear%20partial%20differential%20equations%0AAuthor%3A%20Sheel%20Nidhan%20and%20Haoliang%20Jiang%20and%20Lalit%20Ghule%20and%20Clancy%20Umphrey%20and%20Rishikesh%20Ranade%20and%20Jay%20Pathak%0AAbstract%3A%20%20%20In%20this%20paper%2C%20we%20propose%20a%20domain-decomposition-based%20deep%20learning%20%28DL%29%0Aframework%2C%20named%20transient-CoMLSim%2C%20for%20accurately%20modeling%20unsteady%20and%0Anonlinear%20partial%20differential%20equations%20%28PDEs%29.%20The%20framework%20consists%20of%20two%0Akey%20components%3A%20%28a%29%20a%20convolutional%20neural%20network%20%28CNN%29-based%20autoencoder%0Aarchitecture%20and%20%28b%29%20an%20autoregressive%20model%20composed%20of%20fully%20connected%0Alayers.%20Unlike%20existing%20state-of-the-art%20methods%20that%20operate%20on%20the%20entire%0Acomputational%20domain%2C%20our%20CNN-based%20autoencoder%20computes%20a%20lower-dimensional%0Abasis%20for%20solution%20and%20condition%20fields%20represented%20on%20subdomains.%20Timestepping%0Ais%20performed%20entirely%20in%20the%20latent%20space%2C%20generating%20embeddings%20of%20the%0Asolution%20variables%20from%20the%20time%20history%20of%20embeddings%20of%20solution%20and%0Acondition%20variables.%20This%20approach%20not%20only%20reduces%20computational%20complexity%0Abut%20also%20enhances%20scalability%2C%20making%20it%20well-suited%20for%20large-scale%0Asimulations.%20Furthermore%2C%20to%20improve%20the%20stability%20of%20our%20rollouts%2C%20we%20employ%20a%0Acurriculum%20learning%20%28CL%29%20approach%20during%20the%20training%20of%20the%20autoregressive%0Amodel.%20The%20domain-decomposition%20strategy%20enables%20scaling%20to%20out-of-distribution%0Adomain%20sizes%20while%20maintaining%20the%20accuracy%20of%20predictions%20--%20a%20feature%20not%0Aeasily%20integrated%20into%20popular%20DL-based%20approaches%20for%20physics%20simulations.%20We%0Abenchmark%20our%20model%20against%20two%20widely-used%20DL%20architectures%2C%20Fourier%20Neural%0AOperator%20%28FNO%29%20and%20U-Net%2C%20and%20demonstrate%20that%20our%20framework%20outperforms%20them%0Ain%20terms%20of%20accuracy%2C%20extrapolation%20to%20unseen%20timesteps%2C%20and%20stability%20for%20a%0Awide%20range%20of%20use%20cases.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2408.14461v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DA%2520domain%2520decomposition-based%2520autoregressive%2520deep%2520learning%2520model%2520for%250A%2520%2520unsteady%2520and%2520nonlinear%2520partial%2520differential%2520equations%26entry.906535625%3DSheel%2520Nidhan%2520and%2520Haoliang%2520Jiang%2520and%2520Lalit%2520Ghule%2520and%2520Clancy%2520Umphrey%2520and%2520Rishikesh%2520Ranade%2520and%2520Jay%2520Pathak%26entry.1292438233%3D%2520%2520In%2520this%2520paper%252C%2520we%2520propose%2520a%2520domain-decomposition-based%2520deep%2520learning%2520%2528DL%2529%250Aframework%252C%2520named%2520transient-CoMLSim%252C%2520for%2520accurately%2520modeling%2520unsteady%2520and%250Anonlinear%2520partial%2520differential%2520equations%2520%2528PDEs%2529.%2520The%2520framework%2520consists%2520of%2520two%250Akey%2520components%253A%2520%2528a%2529%2520a%2520convolutional%2520neural%2520network%2520%2528CNN%2529-based%2520autoencoder%250Aarchitecture%2520and%2520%2528b%2529%2520an%2520autoregressive%2520model%2520composed%2520of%2520fully%2520connected%250Alayers.%2520Unlike%2520existing%2520state-of-the-art%2520methods%2520that%2520operate%2520on%2520the%2520entire%250Acomputational%2520domain%252C%2520our%2520CNN-based%2520autoencoder%2520computes%2520a%2520lower-dimensional%250Abasis%2520for%2520solution%2520and%2520condition%2520fields%2520represented%2520on%2520subdomains.%2520Timestepping%250Ais%2520performed%2520entirely%2520in%2520the%2520latent%2520space%252C%2520generating%2520embeddings%2520of%2520the%250Asolution%2520variables%2520from%2520the%2520time%2520history%2520of%2520embeddings%2520of%2520solution%2520and%250Acondition%2520variables.%2520This%2520approach%2520not%2520only%2520reduces%2520computational%2520complexity%250Abut%2520also%2520enhances%2520scalability%252C%2520making%2520it%2520well-suited%2520for%2520large-scale%250Asimulations.%2520Furthermore%252C%2520to%2520improve%2520the%2520stability%2520of%2520our%2520rollouts%252C%2520we%2520employ%2520a%250Acurriculum%2520learning%2520%2528CL%2529%2520approach%2520during%2520the%2520training%2520of%2520the%2520autoregressive%250Amodel.%2520The%2520domain-decomposition%2520strategy%2520enables%2520scaling%2520to%2520out-of-distribution%250Adomain%2520sizes%2520while%2520maintaining%2520the%2520accuracy%2520of%2520predictions%2520--%2520a%2520feature%2520not%250Aeasily%2520integrated%2520into%2520popular%2520DL-based%2520approaches%2520for%2520physics%2520simulations.%2520We%250Abenchmark%2520our%2520model%2520against%2520two%2520widely-used%2520DL%2520architectures%252C%2520Fourier%2520Neural%250AOperator%2520%2528FNO%2529%2520and%2520U-Net%252C%2520and%2520demonstrate%2520that%2520our%2520framework%2520outperforms%2520them%250Ain%2520terms%2520of%2520accuracy%252C%2520extrapolation%2520to%2520unseen%2520timesteps%252C%2520and%2520stability%2520for%2520a%250Awide%2520range%2520of%2520use%2520cases.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2408.14461v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=A%20domain%20decomposition-based%20autoregressive%20deep%20learning%20model%20for%0A%20%20unsteady%20and%20nonlinear%20partial%20differential%20equations&entry.906535625=Sheel%20Nidhan%20and%20Haoliang%20Jiang%20and%20Lalit%20Ghule%20and%20Clancy%20Umphrey%20and%20Rishikesh%20Ranade%20and%20Jay%20Pathak&entry.1292438233=%20%20In%20this%20paper%2C%20we%20propose%20a%20domain-decomposition-based%20deep%20learning%20%28DL%29%0Aframework%2C%20named%20transient-CoMLSim%2C%20for%20accurately%20modeling%20unsteady%20and%0Anonlinear%20partial%20differential%20equations%20%28PDEs%29.%20The%20framework%20consists%20of%20two%0Akey%20components%3A%20%28a%29%20a%20convolutional%20neural%20network%20%28CNN%29-based%20autoencoder%0Aarchitecture%20and%20%28b%29%20an%20autoregressive%20model%20composed%20of%20fully%20connected%0Alayers.%20Unlike%20existing%20state-of-the-art%20methods%20that%20operate%20on%20the%20entire%0Acomputational%20domain%2C%20our%20CNN-based%20autoencoder%20computes%20a%20lower-dimensional%0Abasis%20for%20solution%20and%20condition%20fields%20represented%20on%20subdomains.%20Timestepping%0Ais%20performed%20entirely%20in%20the%20latent%20space%2C%20generating%20embeddings%20of%20the%0Asolution%20variables%20from%20the%20time%20history%20of%20embeddings%20of%20solution%20and%0Acondition%20variables.%20This%20approach%20not%20only%20reduces%20computational%20complexity%0Abut%20also%20enhances%20scalability%2C%20making%20it%20well-suited%20for%20large-scale%0Asimulations.%20Furthermore%2C%20to%20improve%20the%20stability%20of%20our%20rollouts%2C%20we%20employ%20a%0Acurriculum%20learning%20%28CL%29%20approach%20during%20the%20training%20of%20the%20autoregressive%0Amodel.%20The%20domain-decomposition%20strategy%20enables%20scaling%20to%20out-of-distribution%0Adomain%20sizes%20while%20maintaining%20the%20accuracy%20of%20predictions%20--%20a%20feature%20not%0Aeasily%20integrated%20into%20popular%20DL-based%20approaches%20for%20physics%20simulations.%20We%0Abenchmark%20our%20model%20against%20two%20widely-used%20DL%20architectures%2C%20Fourier%20Neural%0AOperator%20%28FNO%29%20and%20U-Net%2C%20and%20demonstrate%20that%20our%20framework%20outperforms%20them%0Ain%20terms%20of%20accuracy%2C%20extrapolation%20to%20unseen%20timesteps%2C%20and%20stability%20for%20a%0Awide%20range%20of%20use%20cases.%0A&entry.1838667208=http%3A//arxiv.org/abs/2408.14461v2&entry.124074799=Read"},
{"title": "Enhancing Uplift Modeling in Multi-Treatment Marketing Campaigns:\n  Leveraging Score Ranking and Calibration Techniques", "author": "Yoon Tae Park and Ting Xu and Mohamed Anany", "abstract": "  Uplift modeling is essential for optimizing marketing strategies by selecting\nindividuals likely to respond positively to specific marketing campaigns. This\nimportance escalates in multi-treatment marketing campaigns, where diverse\ntreatment is available and we may want to assign the customers to treatment\nthat can make the most impact. While there are existing approaches with\nconvenient frameworks like Causalml, there are potential spaces to enhance the\neffect of uplift modeling in multi treatment cases. This paper introduces a\nnovel approach to uplift modeling in multi-treatment campaigns, leveraging\nscore ranking and calibration techniques to improve overall performance of the\nmarketing campaign. We review existing uplift models, including Meta Learner\nframeworks (S, T, X), and their application in real-world scenarios.\nAdditionally, we delve into insights from multi-treatment studies to highlight\nthe complexities and potential advancements in the field. Our methodology\nincorporates Meta-Learner calibration and a scoring rank-based offer selection\nstrategy. Extensive experiment results with real-world datasets demonstrate the\npractical benefits and superior performance of our approach. The findings\nunderscore the critical role of integrating score ranking and calibration\ntechniques in refining the performance and reliability of uplift predictions,\nthereby advancing predictive modeling in marketing analytics and providing\nactionable insights for practitioners seeking to optimize their campaign\nstrategies.\n", "link": "http://arxiv.org/abs/2408.13628v2", "date": "2024-08-27", "relevancy": 1.2991, "topK": [{"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.4392}, {"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.4316}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.4304}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Enhancing%20Uplift%20Modeling%20in%20Multi-Treatment%20Marketing%20Campaigns%3A%0A%20%20Leveraging%20Score%20Ranking%20and%20Calibration%20Techniques&body=Title%3A%20Enhancing%20Uplift%20Modeling%20in%20Multi-Treatment%20Marketing%20Campaigns%3A%0A%20%20Leveraging%20Score%20Ranking%20and%20Calibration%20Techniques%0AAuthor%3A%20Yoon%20Tae%20Park%20and%20Ting%20Xu%20and%20Mohamed%20Anany%0AAbstract%3A%20%20%20Uplift%20modeling%20is%20essential%20for%20optimizing%20marketing%20strategies%20by%20selecting%0Aindividuals%20likely%20to%20respond%20positively%20to%20specific%20marketing%20campaigns.%20This%0Aimportance%20escalates%20in%20multi-treatment%20marketing%20campaigns%2C%20where%20diverse%0Atreatment%20is%20available%20and%20we%20may%20want%20to%20assign%20the%20customers%20to%20treatment%0Athat%20can%20make%20the%20most%20impact.%20While%20there%20are%20existing%20approaches%20with%0Aconvenient%20frameworks%20like%20Causalml%2C%20there%20are%20potential%20spaces%20to%20enhance%20the%0Aeffect%20of%20uplift%20modeling%20in%20multi%20treatment%20cases.%20This%20paper%20introduces%20a%0Anovel%20approach%20to%20uplift%20modeling%20in%20multi-treatment%20campaigns%2C%20leveraging%0Ascore%20ranking%20and%20calibration%20techniques%20to%20improve%20overall%20performance%20of%20the%0Amarketing%20campaign.%20We%20review%20existing%20uplift%20models%2C%20including%20Meta%20Learner%0Aframeworks%20%28S%2C%20T%2C%20X%29%2C%20and%20their%20application%20in%20real-world%20scenarios.%0AAdditionally%2C%20we%20delve%20into%20insights%20from%20multi-treatment%20studies%20to%20highlight%0Athe%20complexities%20and%20potential%20advancements%20in%20the%20field.%20Our%20methodology%0Aincorporates%20Meta-Learner%20calibration%20and%20a%20scoring%20rank-based%20offer%20selection%0Astrategy.%20Extensive%20experiment%20results%20with%20real-world%20datasets%20demonstrate%20the%0Apractical%20benefits%20and%20superior%20performance%20of%20our%20approach.%20The%20findings%0Aunderscore%20the%20critical%20role%20of%20integrating%20score%20ranking%20and%20calibration%0Atechniques%20in%20refining%20the%20performance%20and%20reliability%20of%20uplift%20predictions%2C%0Athereby%20advancing%20predictive%20modeling%20in%20marketing%20analytics%20and%20providing%0Aactionable%20insights%20for%20practitioners%20seeking%20to%20optimize%20their%20campaign%0Astrategies.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2408.13628v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DEnhancing%2520Uplift%2520Modeling%2520in%2520Multi-Treatment%2520Marketing%2520Campaigns%253A%250A%2520%2520Leveraging%2520Score%2520Ranking%2520and%2520Calibration%2520Techniques%26entry.906535625%3DYoon%2520Tae%2520Park%2520and%2520Ting%2520Xu%2520and%2520Mohamed%2520Anany%26entry.1292438233%3D%2520%2520Uplift%2520modeling%2520is%2520essential%2520for%2520optimizing%2520marketing%2520strategies%2520by%2520selecting%250Aindividuals%2520likely%2520to%2520respond%2520positively%2520to%2520specific%2520marketing%2520campaigns.%2520This%250Aimportance%2520escalates%2520in%2520multi-treatment%2520marketing%2520campaigns%252C%2520where%2520diverse%250Atreatment%2520is%2520available%2520and%2520we%2520may%2520want%2520to%2520assign%2520the%2520customers%2520to%2520treatment%250Athat%2520can%2520make%2520the%2520most%2520impact.%2520While%2520there%2520are%2520existing%2520approaches%2520with%250Aconvenient%2520frameworks%2520like%2520Causalml%252C%2520there%2520are%2520potential%2520spaces%2520to%2520enhance%2520the%250Aeffect%2520of%2520uplift%2520modeling%2520in%2520multi%2520treatment%2520cases.%2520This%2520paper%2520introduces%2520a%250Anovel%2520approach%2520to%2520uplift%2520modeling%2520in%2520multi-treatment%2520campaigns%252C%2520leveraging%250Ascore%2520ranking%2520and%2520calibration%2520techniques%2520to%2520improve%2520overall%2520performance%2520of%2520the%250Amarketing%2520campaign.%2520We%2520review%2520existing%2520uplift%2520models%252C%2520including%2520Meta%2520Learner%250Aframeworks%2520%2528S%252C%2520T%252C%2520X%2529%252C%2520and%2520their%2520application%2520in%2520real-world%2520scenarios.%250AAdditionally%252C%2520we%2520delve%2520into%2520insights%2520from%2520multi-treatment%2520studies%2520to%2520highlight%250Athe%2520complexities%2520and%2520potential%2520advancements%2520in%2520the%2520field.%2520Our%2520methodology%250Aincorporates%2520Meta-Learner%2520calibration%2520and%2520a%2520scoring%2520rank-based%2520offer%2520selection%250Astrategy.%2520Extensive%2520experiment%2520results%2520with%2520real-world%2520datasets%2520demonstrate%2520the%250Apractical%2520benefits%2520and%2520superior%2520performance%2520of%2520our%2520approach.%2520The%2520findings%250Aunderscore%2520the%2520critical%2520role%2520of%2520integrating%2520score%2520ranking%2520and%2520calibration%250Atechniques%2520in%2520refining%2520the%2520performance%2520and%2520reliability%2520of%2520uplift%2520predictions%252C%250Athereby%2520advancing%2520predictive%2520modeling%2520in%2520marketing%2520analytics%2520and%2520providing%250Aactionable%2520insights%2520for%2520practitioners%2520seeking%2520to%2520optimize%2520their%2520campaign%250Astrategies.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2408.13628v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Enhancing%20Uplift%20Modeling%20in%20Multi-Treatment%20Marketing%20Campaigns%3A%0A%20%20Leveraging%20Score%20Ranking%20and%20Calibration%20Techniques&entry.906535625=Yoon%20Tae%20Park%20and%20Ting%20Xu%20and%20Mohamed%20Anany&entry.1292438233=%20%20Uplift%20modeling%20is%20essential%20for%20optimizing%20marketing%20strategies%20by%20selecting%0Aindividuals%20likely%20to%20respond%20positively%20to%20specific%20marketing%20campaigns.%20This%0Aimportance%20escalates%20in%20multi-treatment%20marketing%20campaigns%2C%20where%20diverse%0Atreatment%20is%20available%20and%20we%20may%20want%20to%20assign%20the%20customers%20to%20treatment%0Athat%20can%20make%20the%20most%20impact.%20While%20there%20are%20existing%20approaches%20with%0Aconvenient%20frameworks%20like%20Causalml%2C%20there%20are%20potential%20spaces%20to%20enhance%20the%0Aeffect%20of%20uplift%20modeling%20in%20multi%20treatment%20cases.%20This%20paper%20introduces%20a%0Anovel%20approach%20to%20uplift%20modeling%20in%20multi-treatment%20campaigns%2C%20leveraging%0Ascore%20ranking%20and%20calibration%20techniques%20to%20improve%20overall%20performance%20of%20the%0Amarketing%20campaign.%20We%20review%20existing%20uplift%20models%2C%20including%20Meta%20Learner%0Aframeworks%20%28S%2C%20T%2C%20X%29%2C%20and%20their%20application%20in%20real-world%20scenarios.%0AAdditionally%2C%20we%20delve%20into%20insights%20from%20multi-treatment%20studies%20to%20highlight%0Athe%20complexities%20and%20potential%20advancements%20in%20the%20field.%20Our%20methodology%0Aincorporates%20Meta-Learner%20calibration%20and%20a%20scoring%20rank-based%20offer%20selection%0Astrategy.%20Extensive%20experiment%20results%20with%20real-world%20datasets%20demonstrate%20the%0Apractical%20benefits%20and%20superior%20performance%20of%20our%20approach.%20The%20findings%0Aunderscore%20the%20critical%20role%20of%20integrating%20score%20ranking%20and%20calibration%0Atechniques%20in%20refining%20the%20performance%20and%20reliability%20of%20uplift%20predictions%2C%0Athereby%20advancing%20predictive%20modeling%20in%20marketing%20analytics%20and%20providing%0Aactionable%20insights%20for%20practitioners%20seeking%20to%20optimize%20their%20campaign%0Astrategies.%0A&entry.1838667208=http%3A//arxiv.org/abs/2408.13628v2&entry.124074799=Read"},
{"title": "Exploiting Approximate Symmetry for Efficient Multi-Agent Reinforcement\n  Learning", "author": "Batuhan Yardim and Niao He", "abstract": "  Mean-field games (MFG) have become significant tools for solving large-scale\nmulti-agent reinforcement learning problems under symmetry. However, the\nassumption of exact symmetry limits the applicability of MFGs, as real-world\nscenarios often feature inherent heterogeneity. Furthermore, most works on MFG\nassume access to a known MFG model, which might not be readily available for\nreal-world finite-agent games. In this work, we broaden the applicability of\nMFGs by providing a methodology to extend any finite-player, possibly\nasymmetric, game to an \"induced MFG\". First, we prove that $N$-player dynamic\ngames can be symmetrized and smoothly extended to the infinite-player continuum\nvia explicit Kirszbraun extensions. Next, we propose the notion of\n$\\alpha,\\beta$-symmetric games, a new class of dynamic population games that\nincorporate approximate permutation invariance. For $\\alpha,\\beta$-symmetric\ngames, we establish explicit approximation bounds, demonstrating that a Nash\npolicy of the induced MFG is an approximate Nash of the $N$-player dynamic\ngame. We show that TD learning converges up to a small bias using trajectories\nof the $N$-player game with finite-sample guarantees, permitting symmetrized\nlearning without building an explicit MFG model. Finally, for certain games\nsatisfying monotonicity, we prove a sample complexity of\n$\\widetilde{\\mathcal{O}}(\\varepsilon^{-6})$ for the $N$-agent game to learn an\n$\\varepsilon$-Nash up to symmetrization bias. Our theory is supported by\nevaluations on MARL benchmarks with thousands of agents.\n", "link": "http://arxiv.org/abs/2408.15173v1", "date": "2024-08-27", "relevancy": 1.3505, "topK": [{"title": "Dynamic Gaussians Mesh: Consistent Mesh Reconstruction from Monocular\n  Videos", "link": "http://arxiv.org/abs/2404.12379v2", "similarity": 0.4581}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.4505}, {"title": "RoboEXP: Action-Conditioned Scene Graph via Interactive Exploration for\n  Robotic Manipulation", "link": "http://arxiv.org/abs/2402.15487v1", "similarity": 0.43}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Exploiting%20Approximate%20Symmetry%20for%20Efficient%20Multi-Agent%20Reinforcement%0A%20%20Learning&body=Title%3A%20Exploiting%20Approximate%20Symmetry%20for%20Efficient%20Multi-Agent%20Reinforcement%0A%20%20Learning%0AAuthor%3A%20Batuhan%20Yardim%20and%20Niao%20He%0AAbstract%3A%20%20%20Mean-field%20games%20%28MFG%29%20have%20become%20significant%20tools%20for%20solving%20large-scale%0Amulti-agent%20reinforcement%20learning%20problems%20under%20symmetry.%20However%2C%20the%0Aassumption%20of%20exact%20symmetry%20limits%20the%20applicability%20of%20MFGs%2C%20as%20real-world%0Ascenarios%20often%20feature%20inherent%20heterogeneity.%20Furthermore%2C%20most%20works%20on%20MFG%0Aassume%20access%20to%20a%20known%20MFG%20model%2C%20which%20might%20not%20be%20readily%20available%20for%0Areal-world%20finite-agent%20games.%20In%20this%20work%2C%20we%20broaden%20the%20applicability%20of%0AMFGs%20by%20providing%20a%20methodology%20to%20extend%20any%20finite-player%2C%20possibly%0Aasymmetric%2C%20game%20to%20an%20%22induced%20MFG%22.%20First%2C%20we%20prove%20that%20%24N%24-player%20dynamic%0Agames%20can%20be%20symmetrized%20and%20smoothly%20extended%20to%20the%20infinite-player%20continuum%0Avia%20explicit%20Kirszbraun%20extensions.%20Next%2C%20we%20propose%20the%20notion%20of%0A%24%5Calpha%2C%5Cbeta%24-symmetric%20games%2C%20a%20new%20class%20of%20dynamic%20population%20games%20that%0Aincorporate%20approximate%20permutation%20invariance.%20For%20%24%5Calpha%2C%5Cbeta%24-symmetric%0Agames%2C%20we%20establish%20explicit%20approximation%20bounds%2C%20demonstrating%20that%20a%20Nash%0Apolicy%20of%20the%20induced%20MFG%20is%20an%20approximate%20Nash%20of%20the%20%24N%24-player%20dynamic%0Agame.%20We%20show%20that%20TD%20learning%20converges%20up%20to%20a%20small%20bias%20using%20trajectories%0Aof%20the%20%24N%24-player%20game%20with%20finite-sample%20guarantees%2C%20permitting%20symmetrized%0Alearning%20without%20building%20an%20explicit%20MFG%20model.%20Finally%2C%20for%20certain%20games%0Asatisfying%20monotonicity%2C%20we%20prove%20a%20sample%20complexity%20of%0A%24%5Cwidetilde%7B%5Cmathcal%7BO%7D%7D%28%5Cvarepsilon%5E%7B-6%7D%29%24%20for%20the%20%24N%24-agent%20game%20to%20learn%20an%0A%24%5Cvarepsilon%24-Nash%20up%20to%20symmetrization%20bias.%20Our%20theory%20is%20supported%20by%0Aevaluations%20on%20MARL%20benchmarks%20with%20thousands%20of%20agents.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2408.15173v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DExploiting%2520Approximate%2520Symmetry%2520for%2520Efficient%2520Multi-Agent%2520Reinforcement%250A%2520%2520Learning%26entry.906535625%3DBatuhan%2520Yardim%2520and%2520Niao%2520He%26entry.1292438233%3D%2520%2520Mean-field%2520games%2520%2528MFG%2529%2520have%2520become%2520significant%2520tools%2520for%2520solving%2520large-scale%250Amulti-agent%2520reinforcement%2520learning%2520problems%2520under%2520symmetry.%2520However%252C%2520the%250Aassumption%2520of%2520exact%2520symmetry%2520limits%2520the%2520applicability%2520of%2520MFGs%252C%2520as%2520real-world%250Ascenarios%2520often%2520feature%2520inherent%2520heterogeneity.%2520Furthermore%252C%2520most%2520works%2520on%2520MFG%250Aassume%2520access%2520to%2520a%2520known%2520MFG%2520model%252C%2520which%2520might%2520not%2520be%2520readily%2520available%2520for%250Areal-world%2520finite-agent%2520games.%2520In%2520this%2520work%252C%2520we%2520broaden%2520the%2520applicability%2520of%250AMFGs%2520by%2520providing%2520a%2520methodology%2520to%2520extend%2520any%2520finite-player%252C%2520possibly%250Aasymmetric%252C%2520game%2520to%2520an%2520%2522induced%2520MFG%2522.%2520First%252C%2520we%2520prove%2520that%2520%2524N%2524-player%2520dynamic%250Agames%2520can%2520be%2520symmetrized%2520and%2520smoothly%2520extended%2520to%2520the%2520infinite-player%2520continuum%250Avia%2520explicit%2520Kirszbraun%2520extensions.%2520Next%252C%2520we%2520propose%2520the%2520notion%2520of%250A%2524%255Calpha%252C%255Cbeta%2524-symmetric%2520games%252C%2520a%2520new%2520class%2520of%2520dynamic%2520population%2520games%2520that%250Aincorporate%2520approximate%2520permutation%2520invariance.%2520For%2520%2524%255Calpha%252C%255Cbeta%2524-symmetric%250Agames%252C%2520we%2520establish%2520explicit%2520approximation%2520bounds%252C%2520demonstrating%2520that%2520a%2520Nash%250Apolicy%2520of%2520the%2520induced%2520MFG%2520is%2520an%2520approximate%2520Nash%2520of%2520the%2520%2524N%2524-player%2520dynamic%250Agame.%2520We%2520show%2520that%2520TD%2520learning%2520converges%2520up%2520to%2520a%2520small%2520bias%2520using%2520trajectories%250Aof%2520the%2520%2524N%2524-player%2520game%2520with%2520finite-sample%2520guarantees%252C%2520permitting%2520symmetrized%250Alearning%2520without%2520building%2520an%2520explicit%2520MFG%2520model.%2520Finally%252C%2520for%2520certain%2520games%250Asatisfying%2520monotonicity%252C%2520we%2520prove%2520a%2520sample%2520complexity%2520of%250A%2524%255Cwidetilde%257B%255Cmathcal%257BO%257D%257D%2528%255Cvarepsilon%255E%257B-6%257D%2529%2524%2520for%2520the%2520%2524N%2524-agent%2520game%2520to%2520learn%2520an%250A%2524%255Cvarepsilon%2524-Nash%2520up%2520to%2520symmetrization%2520bias.%2520Our%2520theory%2520is%2520supported%2520by%250Aevaluations%2520on%2520MARL%2520benchmarks%2520with%2520thousands%2520of%2520agents.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2408.15173v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Exploiting%20Approximate%20Symmetry%20for%20Efficient%20Multi-Agent%20Reinforcement%0A%20%20Learning&entry.906535625=Batuhan%20Yardim%20and%20Niao%20He&entry.1292438233=%20%20Mean-field%20games%20%28MFG%29%20have%20become%20significant%20tools%20for%20solving%20large-scale%0Amulti-agent%20reinforcement%20learning%20problems%20under%20symmetry.%20However%2C%20the%0Aassumption%20of%20exact%20symmetry%20limits%20the%20applicability%20of%20MFGs%2C%20as%20real-world%0Ascenarios%20often%20feature%20inherent%20heterogeneity.%20Furthermore%2C%20most%20works%20on%20MFG%0Aassume%20access%20to%20a%20known%20MFG%20model%2C%20which%20might%20not%20be%20readily%20available%20for%0Areal-world%20finite-agent%20games.%20In%20this%20work%2C%20we%20broaden%20the%20applicability%20of%0AMFGs%20by%20providing%20a%20methodology%20to%20extend%20any%20finite-player%2C%20possibly%0Aasymmetric%2C%20game%20to%20an%20%22induced%20MFG%22.%20First%2C%20we%20prove%20that%20%24N%24-player%20dynamic%0Agames%20can%20be%20symmetrized%20and%20smoothly%20extended%20to%20the%20infinite-player%20continuum%0Avia%20explicit%20Kirszbraun%20extensions.%20Next%2C%20we%20propose%20the%20notion%20of%0A%24%5Calpha%2C%5Cbeta%24-symmetric%20games%2C%20a%20new%20class%20of%20dynamic%20population%20games%20that%0Aincorporate%20approximate%20permutation%20invariance.%20For%20%24%5Calpha%2C%5Cbeta%24-symmetric%0Agames%2C%20we%20establish%20explicit%20approximation%20bounds%2C%20demonstrating%20that%20a%20Nash%0Apolicy%20of%20the%20induced%20MFG%20is%20an%20approximate%20Nash%20of%20the%20%24N%24-player%20dynamic%0Agame.%20We%20show%20that%20TD%20learning%20converges%20up%20to%20a%20small%20bias%20using%20trajectories%0Aof%20the%20%24N%24-player%20game%20with%20finite-sample%20guarantees%2C%20permitting%20symmetrized%0Alearning%20without%20building%20an%20explicit%20MFG%20model.%20Finally%2C%20for%20certain%20games%0Asatisfying%20monotonicity%2C%20we%20prove%20a%20sample%20complexity%20of%0A%24%5Cwidetilde%7B%5Cmathcal%7BO%7D%7D%28%5Cvarepsilon%5E%7B-6%7D%29%24%20for%20the%20%24N%24-agent%20game%20to%20learn%20an%0A%24%5Cvarepsilon%24-Nash%20up%20to%20symmetrization%20bias.%20Our%20theory%20is%20supported%20by%0Aevaluations%20on%20MARL%20benchmarks%20with%20thousands%20of%20agents.%0A&entry.1838667208=http%3A//arxiv.org/abs/2408.15173v1&entry.124074799=Read"},
{"title": "CSPs with Few Alien Constraints", "author": "Peter Jonsson and Victor Lagerkvist and George Osipov", "abstract": "  The constraint satisfaction problem asks to decide if a set of constraints\nover a relational structure $\\mathcal{A}$ is satisfiable (CSP$(\\mathcal{A})$).\nWe consider CSP$(\\mathcal{A} \\cup \\mathcal{B})$ where $\\mathcal{A}$ is a\nstructure and $\\mathcal{B}$ is an alien structure, and analyse its\n(parameterized) complexity when at most $k$ alien constraints are allowed. We\nestablish connections and obtain transferable complexity results to several\nwell-studied problems that previously escaped classification attempts. Our\nnovel approach, utilizing logical and algebraic methods, yields an FPT versus\npNP dichotomy for arbitrary finite structures and sharper dichotomies for\nBoolean structures and first-order reducts of $(\\mathbb{N},=)$ (equality CSPs),\ntogether with many partial results for general $\\omega$-categorical structures.\n", "link": "http://arxiv.org/abs/2408.12909v2", "date": "2024-08-27", "relevancy": 1.0207, "topK": [{"title": "RoboEXP: Action-Conditioned Scene Graph via Interactive Exploration for\n  Robotic Manipulation", "link": "http://arxiv.org/abs/2402.15487v1", "similarity": 0.3729}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.3479}, {"title": "Flexible Visual Recognition by Evidential Modeling of Confusion and Ignorance", "link": "http://openaccess.thecvf.com/content/ICCV2023/html/Fan_Flexible_Visual_Recognition_by_Evidential_Modeling_of_Confusion_and_Ignorance_ICCV_2023_paper.html", "similarity": 0.3241}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20CSPs%20with%20Few%20Alien%20Constraints&body=Title%3A%20CSPs%20with%20Few%20Alien%20Constraints%0AAuthor%3A%20Peter%20Jonsson%20and%20Victor%20Lagerkvist%20and%20George%20Osipov%0AAbstract%3A%20%20%20The%20constraint%20satisfaction%20problem%20asks%20to%20decide%20if%20a%20set%20of%20constraints%0Aover%20a%20relational%20structure%20%24%5Cmathcal%7BA%7D%24%20is%20satisfiable%20%28CSP%24%28%5Cmathcal%7BA%7D%29%24%29.%0AWe%20consider%20CSP%24%28%5Cmathcal%7BA%7D%20%5Ccup%20%5Cmathcal%7BB%7D%29%24%20where%20%24%5Cmathcal%7BA%7D%24%20is%20a%0Astructure%20and%20%24%5Cmathcal%7BB%7D%24%20is%20an%20alien%20structure%2C%20and%20analyse%20its%0A%28parameterized%29%20complexity%20when%20at%20most%20%24k%24%20alien%20constraints%20are%20allowed.%20We%0Aestablish%20connections%20and%20obtain%20transferable%20complexity%20results%20to%20several%0Awell-studied%20problems%20that%20previously%20escaped%20classification%20attempts.%20Our%0Anovel%20approach%2C%20utilizing%20logical%20and%20algebraic%20methods%2C%20yields%20an%20FPT%20versus%0ApNP%20dichotomy%20for%20arbitrary%20finite%20structures%20and%20sharper%20dichotomies%20for%0ABoolean%20structures%20and%20first-order%20reducts%20of%20%24%28%5Cmathbb%7BN%7D%2C%3D%29%24%20%28equality%20CSPs%29%2C%0Atogether%20with%20many%20partial%20results%20for%20general%20%24%5Comega%24-categorical%20structures.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2408.12909v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DCSPs%2520with%2520Few%2520Alien%2520Constraints%26entry.906535625%3DPeter%2520Jonsson%2520and%2520Victor%2520Lagerkvist%2520and%2520George%2520Osipov%26entry.1292438233%3D%2520%2520The%2520constraint%2520satisfaction%2520problem%2520asks%2520to%2520decide%2520if%2520a%2520set%2520of%2520constraints%250Aover%2520a%2520relational%2520structure%2520%2524%255Cmathcal%257BA%257D%2524%2520is%2520satisfiable%2520%2528CSP%2524%2528%255Cmathcal%257BA%257D%2529%2524%2529.%250AWe%2520consider%2520CSP%2524%2528%255Cmathcal%257BA%257D%2520%255Ccup%2520%255Cmathcal%257BB%257D%2529%2524%2520where%2520%2524%255Cmathcal%257BA%257D%2524%2520is%2520a%250Astructure%2520and%2520%2524%255Cmathcal%257BB%257D%2524%2520is%2520an%2520alien%2520structure%252C%2520and%2520analyse%2520its%250A%2528parameterized%2529%2520complexity%2520when%2520at%2520most%2520%2524k%2524%2520alien%2520constraints%2520are%2520allowed.%2520We%250Aestablish%2520connections%2520and%2520obtain%2520transferable%2520complexity%2520results%2520to%2520several%250Awell-studied%2520problems%2520that%2520previously%2520escaped%2520classification%2520attempts.%2520Our%250Anovel%2520approach%252C%2520utilizing%2520logical%2520and%2520algebraic%2520methods%252C%2520yields%2520an%2520FPT%2520versus%250ApNP%2520dichotomy%2520for%2520arbitrary%2520finite%2520structures%2520and%2520sharper%2520dichotomies%2520for%250ABoolean%2520structures%2520and%2520first-order%2520reducts%2520of%2520%2524%2528%255Cmathbb%257BN%257D%252C%253D%2529%2524%2520%2528equality%2520CSPs%2529%252C%250Atogether%2520with%2520many%2520partial%2520results%2520for%2520general%2520%2524%255Comega%2524-categorical%2520structures.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2408.12909v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=CSPs%20with%20Few%20Alien%20Constraints&entry.906535625=Peter%20Jonsson%20and%20Victor%20Lagerkvist%20and%20George%20Osipov&entry.1292438233=%20%20The%20constraint%20satisfaction%20problem%20asks%20to%20decide%20if%20a%20set%20of%20constraints%0Aover%20a%20relational%20structure%20%24%5Cmathcal%7BA%7D%24%20is%20satisfiable%20%28CSP%24%28%5Cmathcal%7BA%7D%29%24%29.%0AWe%20consider%20CSP%24%28%5Cmathcal%7BA%7D%20%5Ccup%20%5Cmathcal%7BB%7D%29%24%20where%20%24%5Cmathcal%7BA%7D%24%20is%20a%0Astructure%20and%20%24%5Cmathcal%7BB%7D%24%20is%20an%20alien%20structure%2C%20and%20analyse%20its%0A%28parameterized%29%20complexity%20when%20at%20most%20%24k%24%20alien%20constraints%20are%20allowed.%20We%0Aestablish%20connections%20and%20obtain%20transferable%20complexity%20results%20to%20several%0Awell-studied%20problems%20that%20previously%20escaped%20classification%20attempts.%20Our%0Anovel%20approach%2C%20utilizing%20logical%20and%20algebraic%20methods%2C%20yields%20an%20FPT%20versus%0ApNP%20dichotomy%20for%20arbitrary%20finite%20structures%20and%20sharper%20dichotomies%20for%0ABoolean%20structures%20and%20first-order%20reducts%20of%20%24%28%5Cmathbb%7BN%7D%2C%3D%29%24%20%28equality%20CSPs%29%2C%0Atogether%20with%20many%20partial%20results%20for%20general%20%24%5Comega%24-categorical%20structures.%0A&entry.1838667208=http%3A//arxiv.org/abs/2408.12909v2&entry.124074799=Read"},
{"title": "UWF-RI2FA: Generating Multi-frame Ultrawide-field Fluorescein\n  Angiography from Ultrawide-field Retinal Imaging Improves Diabetic\n  Retinopathy Stratification", "author": "Ruoyu Chen and Kezheng Xu and Kangyan Zheng and Weiyi Zhang and Yan Lu and Danli Shi and Mingguang He", "abstract": "  Ultrawide-field fluorescein angiography (UWF-FA) facilitates diabetic\nretinopathy (DR) detection by providing a clear visualization of peripheral\nretinal lesions. However, the intravenous dye injection with potential risks\nhamper its application. We aim to acquire dye-free UWF-FA images from\nnoninvasive UWF retinal imaging (UWF-RI) using generative artificial\nintelligence (GenAI) and evaluate its effectiveness in DR screening. A total of\n18,321 UWF-FA images of different phases were registered with corresponding\nUWF-RI images and fed into a generative adversarial networks (GAN)-based model\nfor training. The quality of generated UWF-FA images was evaluated through\nquantitative metrics and human evaluation. The DeepDRiD dataset was used to\nexternally assess the contribution of generated UWF-FA images to DR\nclassification, using area under the receiver operating characteristic curve\n(AUROC) as outcome metrics. The generated early, mid, and late phase UWF-FA\nimages achieved high authenticity, with multi-scale similarity scores ranging\nfrom 0.70 to 0.91 and qualitative visual scores ranging from 1.64 to 1.98\n(1=real UWF-FA quality). In fifty randomly selected images, 56% to 76% of the\ngenerated images were difficult to distinguish from real images in the Turing\ntest. Moreover, adding these generated UWF-FA images for DR classification\nsignificantly increased the AUROC from 0.869 to 0.904 compared to the baseline\nmodel using UWF-RI images (P < .001). The model successfully generates\nrealistic multi-frame UWF-FA images for enhancing DR stratification without\nintravenous dye injection.\n", "link": "http://arxiv.org/abs/2408.10636v2", "date": "2024-08-27", "relevancy": 1.4981, "topK": [{"title": "Scaling Rectified Flow Transformers for High-Resolution Image Synthesis", "link": "http://arxiv.org/abs/2403.03206v1", "similarity": 0.5014}, {"title": "SDXS: Real-Time One-Step Latent Diffusion Models with Image Conditions", "link": "http://arxiv.org/abs/2403.16627v1", "similarity": 0.499}, {"title": "ToDo: Token Downsampling for Efficient Generation of High-Resolution\n  Images", "link": "http://arxiv.org/abs/2402.13573v2", "similarity": 0.4987}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20UWF-RI2FA%3A%20Generating%20Multi-frame%20Ultrawide-field%20Fluorescein%0A%20%20Angiography%20from%20Ultrawide-field%20Retinal%20Imaging%20Improves%20Diabetic%0A%20%20Retinopathy%20Stratification&body=Title%3A%20UWF-RI2FA%3A%20Generating%20Multi-frame%20Ultrawide-field%20Fluorescein%0A%20%20Angiography%20from%20Ultrawide-field%20Retinal%20Imaging%20Improves%20Diabetic%0A%20%20Retinopathy%20Stratification%0AAuthor%3A%20Ruoyu%20Chen%20and%20Kezheng%20Xu%20and%20Kangyan%20Zheng%20and%20Weiyi%20Zhang%20and%20Yan%20Lu%20and%20Danli%20Shi%20and%20Mingguang%20He%0AAbstract%3A%20%20%20Ultrawide-field%20fluorescein%20angiography%20%28UWF-FA%29%20facilitates%20diabetic%0Aretinopathy%20%28DR%29%20detection%20by%20providing%20a%20clear%20visualization%20of%20peripheral%0Aretinal%20lesions.%20However%2C%20the%20intravenous%20dye%20injection%20with%20potential%20risks%0Ahamper%20its%20application.%20We%20aim%20to%20acquire%20dye-free%20UWF-FA%20images%20from%0Anoninvasive%20UWF%20retinal%20imaging%20%28UWF-RI%29%20using%20generative%20artificial%0Aintelligence%20%28GenAI%29%20and%20evaluate%20its%20effectiveness%20in%20DR%20screening.%20A%20total%20of%0A18%2C321%20UWF-FA%20images%20of%20different%20phases%20were%20registered%20with%20corresponding%0AUWF-RI%20images%20and%20fed%20into%20a%20generative%20adversarial%20networks%20%28GAN%29-based%20model%0Afor%20training.%20The%20quality%20of%20generated%20UWF-FA%20images%20was%20evaluated%20through%0Aquantitative%20metrics%20and%20human%20evaluation.%20The%20DeepDRiD%20dataset%20was%20used%20to%0Aexternally%20assess%20the%20contribution%20of%20generated%20UWF-FA%20images%20to%20DR%0Aclassification%2C%20using%20area%20under%20the%20receiver%20operating%20characteristic%20curve%0A%28AUROC%29%20as%20outcome%20metrics.%20The%20generated%20early%2C%20mid%2C%20and%20late%20phase%20UWF-FA%0Aimages%20achieved%20high%20authenticity%2C%20with%20multi-scale%20similarity%20scores%20ranging%0Afrom%200.70%20to%200.91%20and%20qualitative%20visual%20scores%20ranging%20from%201.64%20to%201.98%0A%281%3Dreal%20UWF-FA%20quality%29.%20In%20fifty%20randomly%20selected%20images%2C%2056%25%20to%2076%25%20of%20the%0Agenerated%20images%20were%20difficult%20to%20distinguish%20from%20real%20images%20in%20the%20Turing%0Atest.%20Moreover%2C%20adding%20these%20generated%20UWF-FA%20images%20for%20DR%20classification%0Asignificantly%20increased%20the%20AUROC%20from%200.869%20to%200.904%20compared%20to%20the%20baseline%0Amodel%20using%20UWF-RI%20images%20%28P%20%3C%20.001%29.%20The%20model%20successfully%20generates%0Arealistic%20multi-frame%20UWF-FA%20images%20for%20enhancing%20DR%20stratification%20without%0Aintravenous%20dye%20injection.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2408.10636v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DUWF-RI2FA%253A%2520Generating%2520Multi-frame%2520Ultrawide-field%2520Fluorescein%250A%2520%2520Angiography%2520from%2520Ultrawide-field%2520Retinal%2520Imaging%2520Improves%2520Diabetic%250A%2520%2520Retinopathy%2520Stratification%26entry.906535625%3DRuoyu%2520Chen%2520and%2520Kezheng%2520Xu%2520and%2520Kangyan%2520Zheng%2520and%2520Weiyi%2520Zhang%2520and%2520Yan%2520Lu%2520and%2520Danli%2520Shi%2520and%2520Mingguang%2520He%26entry.1292438233%3D%2520%2520Ultrawide-field%2520fluorescein%2520angiography%2520%2528UWF-FA%2529%2520facilitates%2520diabetic%250Aretinopathy%2520%2528DR%2529%2520detection%2520by%2520providing%2520a%2520clear%2520visualization%2520of%2520peripheral%250Aretinal%2520lesions.%2520However%252C%2520the%2520intravenous%2520dye%2520injection%2520with%2520potential%2520risks%250Ahamper%2520its%2520application.%2520We%2520aim%2520to%2520acquire%2520dye-free%2520UWF-FA%2520images%2520from%250Anoninvasive%2520UWF%2520retinal%2520imaging%2520%2528UWF-RI%2529%2520using%2520generative%2520artificial%250Aintelligence%2520%2528GenAI%2529%2520and%2520evaluate%2520its%2520effectiveness%2520in%2520DR%2520screening.%2520A%2520total%2520of%250A18%252C321%2520UWF-FA%2520images%2520of%2520different%2520phases%2520were%2520registered%2520with%2520corresponding%250AUWF-RI%2520images%2520and%2520fed%2520into%2520a%2520generative%2520adversarial%2520networks%2520%2528GAN%2529-based%2520model%250Afor%2520training.%2520The%2520quality%2520of%2520generated%2520UWF-FA%2520images%2520was%2520evaluated%2520through%250Aquantitative%2520metrics%2520and%2520human%2520evaluation.%2520The%2520DeepDRiD%2520dataset%2520was%2520used%2520to%250Aexternally%2520assess%2520the%2520contribution%2520of%2520generated%2520UWF-FA%2520images%2520to%2520DR%250Aclassification%252C%2520using%2520area%2520under%2520the%2520receiver%2520operating%2520characteristic%2520curve%250A%2528AUROC%2529%2520as%2520outcome%2520metrics.%2520The%2520generated%2520early%252C%2520mid%252C%2520and%2520late%2520phase%2520UWF-FA%250Aimages%2520achieved%2520high%2520authenticity%252C%2520with%2520multi-scale%2520similarity%2520scores%2520ranging%250Afrom%25200.70%2520to%25200.91%2520and%2520qualitative%2520visual%2520scores%2520ranging%2520from%25201.64%2520to%25201.98%250A%25281%253Dreal%2520UWF-FA%2520quality%2529.%2520In%2520fifty%2520randomly%2520selected%2520images%252C%252056%2525%2520to%252076%2525%2520of%2520the%250Agenerated%2520images%2520were%2520difficult%2520to%2520distinguish%2520from%2520real%2520images%2520in%2520the%2520Turing%250Atest.%2520Moreover%252C%2520adding%2520these%2520generated%2520UWF-FA%2520images%2520for%2520DR%2520classification%250Asignificantly%2520increased%2520the%2520AUROC%2520from%25200.869%2520to%25200.904%2520compared%2520to%2520the%2520baseline%250Amodel%2520using%2520UWF-RI%2520images%2520%2528P%2520%253C%2520.001%2529.%2520The%2520model%2520successfully%2520generates%250Arealistic%2520multi-frame%2520UWF-FA%2520images%2520for%2520enhancing%2520DR%2520stratification%2520without%250Aintravenous%2520dye%2520injection.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2408.10636v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=UWF-RI2FA%3A%20Generating%20Multi-frame%20Ultrawide-field%20Fluorescein%0A%20%20Angiography%20from%20Ultrawide-field%20Retinal%20Imaging%20Improves%20Diabetic%0A%20%20Retinopathy%20Stratification&entry.906535625=Ruoyu%20Chen%20and%20Kezheng%20Xu%20and%20Kangyan%20Zheng%20and%20Weiyi%20Zhang%20and%20Yan%20Lu%20and%20Danli%20Shi%20and%20Mingguang%20He&entry.1292438233=%20%20Ultrawide-field%20fluorescein%20angiography%20%28UWF-FA%29%20facilitates%20diabetic%0Aretinopathy%20%28DR%29%20detection%20by%20providing%20a%20clear%20visualization%20of%20peripheral%0Aretinal%20lesions.%20However%2C%20the%20intravenous%20dye%20injection%20with%20potential%20risks%0Ahamper%20its%20application.%20We%20aim%20to%20acquire%20dye-free%20UWF-FA%20images%20from%0Anoninvasive%20UWF%20retinal%20imaging%20%28UWF-RI%29%20using%20generative%20artificial%0Aintelligence%20%28GenAI%29%20and%20evaluate%20its%20effectiveness%20in%20DR%20screening.%20A%20total%20of%0A18%2C321%20UWF-FA%20images%20of%20different%20phases%20were%20registered%20with%20corresponding%0AUWF-RI%20images%20and%20fed%20into%20a%20generative%20adversarial%20networks%20%28GAN%29-based%20model%0Afor%20training.%20The%20quality%20of%20generated%20UWF-FA%20images%20was%20evaluated%20through%0Aquantitative%20metrics%20and%20human%20evaluation.%20The%20DeepDRiD%20dataset%20was%20used%20to%0Aexternally%20assess%20the%20contribution%20of%20generated%20UWF-FA%20images%20to%20DR%0Aclassification%2C%20using%20area%20under%20the%20receiver%20operating%20characteristic%20curve%0A%28AUROC%29%20as%20outcome%20metrics.%20The%20generated%20early%2C%20mid%2C%20and%20late%20phase%20UWF-FA%0Aimages%20achieved%20high%20authenticity%2C%20with%20multi-scale%20similarity%20scores%20ranging%0Afrom%200.70%20to%200.91%20and%20qualitative%20visual%20scores%20ranging%20from%201.64%20to%201.98%0A%281%3Dreal%20UWF-FA%20quality%29.%20In%20fifty%20randomly%20selected%20images%2C%2056%25%20to%2076%25%20of%20the%0Agenerated%20images%20were%20difficult%20to%20distinguish%20from%20real%20images%20in%20the%20Turing%0Atest.%20Moreover%2C%20adding%20these%20generated%20UWF-FA%20images%20for%20DR%20classification%0Asignificantly%20increased%20the%20AUROC%20from%200.869%20to%200.904%20compared%20to%20the%20baseline%0Amodel%20using%20UWF-RI%20images%20%28P%20%3C%20.001%29.%20The%20model%20successfully%20generates%0Arealistic%20multi-frame%20UWF-FA%20images%20for%20enhancing%20DR%20stratification%20without%0Aintravenous%20dye%20injection.%0A&entry.1838667208=http%3A//arxiv.org/abs/2408.10636v2&entry.124074799=Read"},
{"title": "From Variability to Stability: Advancing RecSys Benchmarking Practices", "author": "Valeriy Shevchenko and Nikita Belousov and Alexey Vasilev and Vladimir Zholobov and Artyom Sosedka and Natalia Semenova and Anna Volodkevich and Andrey Savchenko and Alexey Zaytsev", "abstract": "  In the rapidly evolving domain of Recommender Systems (RecSys), new\nalgorithms frequently claim state-of-the-art performance based on evaluations\nover a limited set of arbitrarily selected datasets. However, this approach may\nfail to holistically reflect their effectiveness due to the significant impact\nof dataset characteristics on algorithm performance. Addressing this\ndeficiency, this paper introduces a novel benchmarking methodology to\nfacilitate a fair and robust comparison of RecSys algorithms, thereby advancing\nevaluation practices. By utilizing a diverse set of $30$ open datasets,\nincluding two introduced in this work, and evaluating $11$ collaborative\nfiltering algorithms across $9$ metrics, we critically examine the influence of\ndataset characteristics on algorithm performance. We further investigate the\nfeasibility of aggregating outcomes from multiple datasets into a unified\nranking. Through rigorous experimental analysis, we validate the reliability of\nour methodology under the variability of datasets, offering a benchmarking\nstrategy that balances quality and computational demands. This methodology\nenables a fair yet effective means of evaluating RecSys algorithms, providing\nvaluable guidance for future research endeavors.\n", "link": "http://arxiv.org/abs/2402.09766v2", "date": "2024-08-27", "relevancy": 1.7172, "topK": [{"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.4327}, {"title": "RTG-SLAM: Real-time 3D Reconstruction at Scale using Gaussian Splatting", "link": "http://arxiv.org/abs/2404.19706v1", "similarity": 0.4301}, {"title": "Breadcrumbs: Adversarial class-balanced sampling for long-tailed recognition", "link": "https://link.springer.com/chapter/10.1007/978-3-031-20053-3_37", "similarity": 0.4272}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20From%20Variability%20to%20Stability%3A%20Advancing%20RecSys%20Benchmarking%20Practices&body=Title%3A%20From%20Variability%20to%20Stability%3A%20Advancing%20RecSys%20Benchmarking%20Practices%0AAuthor%3A%20Valeriy%20Shevchenko%20and%20Nikita%20Belousov%20and%20Alexey%20Vasilev%20and%20Vladimir%20Zholobov%20and%20Artyom%20Sosedka%20and%20Natalia%20Semenova%20and%20Anna%20Volodkevich%20and%20Andrey%20Savchenko%20and%20Alexey%20Zaytsev%0AAbstract%3A%20%20%20In%20the%20rapidly%20evolving%20domain%20of%20Recommender%20Systems%20%28RecSys%29%2C%20new%0Aalgorithms%20frequently%20claim%20state-of-the-art%20performance%20based%20on%20evaluations%0Aover%20a%20limited%20set%20of%20arbitrarily%20selected%20datasets.%20However%2C%20this%20approach%20may%0Afail%20to%20holistically%20reflect%20their%20effectiveness%20due%20to%20the%20significant%20impact%0Aof%20dataset%20characteristics%20on%20algorithm%20performance.%20Addressing%20this%0Adeficiency%2C%20this%20paper%20introduces%20a%20novel%20benchmarking%20methodology%20to%0Afacilitate%20a%20fair%20and%20robust%20comparison%20of%20RecSys%20algorithms%2C%20thereby%20advancing%0Aevaluation%20practices.%20By%20utilizing%20a%20diverse%20set%20of%20%2430%24%20open%20datasets%2C%0Aincluding%20two%20introduced%20in%20this%20work%2C%20and%20evaluating%20%2411%24%20collaborative%0Afiltering%20algorithms%20across%20%249%24%20metrics%2C%20we%20critically%20examine%20the%20influence%20of%0Adataset%20characteristics%20on%20algorithm%20performance.%20We%20further%20investigate%20the%0Afeasibility%20of%20aggregating%20outcomes%20from%20multiple%20datasets%20into%20a%20unified%0Aranking.%20Through%20rigorous%20experimental%20analysis%2C%20we%20validate%20the%20reliability%20of%0Aour%20methodology%20under%20the%20variability%20of%20datasets%2C%20offering%20a%20benchmarking%0Astrategy%20that%20balances%20quality%20and%20computational%20demands.%20This%20methodology%0Aenables%20a%20fair%20yet%20effective%20means%20of%20evaluating%20RecSys%20algorithms%2C%20providing%0Avaluable%20guidance%20for%20future%20research%20endeavors.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2402.09766v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DFrom%2520Variability%2520to%2520Stability%253A%2520Advancing%2520RecSys%2520Benchmarking%2520Practices%26entry.906535625%3DValeriy%2520Shevchenko%2520and%2520Nikita%2520Belousov%2520and%2520Alexey%2520Vasilev%2520and%2520Vladimir%2520Zholobov%2520and%2520Artyom%2520Sosedka%2520and%2520Natalia%2520Semenova%2520and%2520Anna%2520Volodkevich%2520and%2520Andrey%2520Savchenko%2520and%2520Alexey%2520Zaytsev%26entry.1292438233%3D%2520%2520In%2520the%2520rapidly%2520evolving%2520domain%2520of%2520Recommender%2520Systems%2520%2528RecSys%2529%252C%2520new%250Aalgorithms%2520frequently%2520claim%2520state-of-the-art%2520performance%2520based%2520on%2520evaluations%250Aover%2520a%2520limited%2520set%2520of%2520arbitrarily%2520selected%2520datasets.%2520However%252C%2520this%2520approach%2520may%250Afail%2520to%2520holistically%2520reflect%2520their%2520effectiveness%2520due%2520to%2520the%2520significant%2520impact%250Aof%2520dataset%2520characteristics%2520on%2520algorithm%2520performance.%2520Addressing%2520this%250Adeficiency%252C%2520this%2520paper%2520introduces%2520a%2520novel%2520benchmarking%2520methodology%2520to%250Afacilitate%2520a%2520fair%2520and%2520robust%2520comparison%2520of%2520RecSys%2520algorithms%252C%2520thereby%2520advancing%250Aevaluation%2520practices.%2520By%2520utilizing%2520a%2520diverse%2520set%2520of%2520%252430%2524%2520open%2520datasets%252C%250Aincluding%2520two%2520introduced%2520in%2520this%2520work%252C%2520and%2520evaluating%2520%252411%2524%2520collaborative%250Afiltering%2520algorithms%2520across%2520%25249%2524%2520metrics%252C%2520we%2520critically%2520examine%2520the%2520influence%2520of%250Adataset%2520characteristics%2520on%2520algorithm%2520performance.%2520We%2520further%2520investigate%2520the%250Afeasibility%2520of%2520aggregating%2520outcomes%2520from%2520multiple%2520datasets%2520into%2520a%2520unified%250Aranking.%2520Through%2520rigorous%2520experimental%2520analysis%252C%2520we%2520validate%2520the%2520reliability%2520of%250Aour%2520methodology%2520under%2520the%2520variability%2520of%2520datasets%252C%2520offering%2520a%2520benchmarking%250Astrategy%2520that%2520balances%2520quality%2520and%2520computational%2520demands.%2520This%2520methodology%250Aenables%2520a%2520fair%2520yet%2520effective%2520means%2520of%2520evaluating%2520RecSys%2520algorithms%252C%2520providing%250Avaluable%2520guidance%2520for%2520future%2520research%2520endeavors.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2402.09766v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=From%20Variability%20to%20Stability%3A%20Advancing%20RecSys%20Benchmarking%20Practices&entry.906535625=Valeriy%20Shevchenko%20and%20Nikita%20Belousov%20and%20Alexey%20Vasilev%20and%20Vladimir%20Zholobov%20and%20Artyom%20Sosedka%20and%20Natalia%20Semenova%20and%20Anna%20Volodkevich%20and%20Andrey%20Savchenko%20and%20Alexey%20Zaytsev&entry.1292438233=%20%20In%20the%20rapidly%20evolving%20domain%20of%20Recommender%20Systems%20%28RecSys%29%2C%20new%0Aalgorithms%20frequently%20claim%20state-of-the-art%20performance%20based%20on%20evaluations%0Aover%20a%20limited%20set%20of%20arbitrarily%20selected%20datasets.%20However%2C%20this%20approach%20may%0Afail%20to%20holistically%20reflect%20their%20effectiveness%20due%20to%20the%20significant%20impact%0Aof%20dataset%20characteristics%20on%20algorithm%20performance.%20Addressing%20this%0Adeficiency%2C%20this%20paper%20introduces%20a%20novel%20benchmarking%20methodology%20to%0Afacilitate%20a%20fair%20and%20robust%20comparison%20of%20RecSys%20algorithms%2C%20thereby%20advancing%0Aevaluation%20practices.%20By%20utilizing%20a%20diverse%20set%20of%20%2430%24%20open%20datasets%2C%0Aincluding%20two%20introduced%20in%20this%20work%2C%20and%20evaluating%20%2411%24%20collaborative%0Afiltering%20algorithms%20across%20%249%24%20metrics%2C%20we%20critically%20examine%20the%20influence%20of%0Adataset%20characteristics%20on%20algorithm%20performance.%20We%20further%20investigate%20the%0Afeasibility%20of%20aggregating%20outcomes%20from%20multiple%20datasets%20into%20a%20unified%0Aranking.%20Through%20rigorous%20experimental%20analysis%2C%20we%20validate%20the%20reliability%20of%0Aour%20methodology%20under%20the%20variability%20of%20datasets%2C%20offering%20a%20benchmarking%0Astrategy%20that%20balances%20quality%20and%20computational%20demands.%20This%20methodology%0Aenables%20a%20fair%20yet%20effective%20means%20of%20evaluating%20RecSys%20algorithms%2C%20providing%0Avaluable%20guidance%20for%20future%20research%20endeavors.%0A&entry.1838667208=http%3A//arxiv.org/abs/2402.09766v2&entry.124074799=Read"},
{"title": "Time Series Analysis for Education: Methods, Applications, and Future\n  Directions", "author": "Shengzhong Mao and Chaoli Zhang and Yichi Song and Jindong Wang and Xiao-Jun Zeng and Zenglin Xu and Qingsong Wen", "abstract": "  Recent advancements in the collection and analysis of sequential educational\ndata have brought time series analysis to a pivotal position in educational\nresearch, highlighting its essential role in facilitating data-driven\ndecision-making. However, there is a lack of comprehensive summaries that\nconsolidate these advancements. To the best of our knowledge, this paper is the\nfirst to provide a comprehensive review of time series analysis techniques\nspecifically within the educational context. We begin by exploring the\nlandscape of educational data analytics, categorizing various data sources and\ntypes relevant to education. We then review four prominent time series\nmethods-forecasting, classification, clustering, and anomaly\ndetection-illustrating their specific application points in educational\nsettings. Subsequently, we present a range of educational scenarios and\napplications, focusing on how these methods are employed to address diverse\neducational tasks, which highlights the practical integration of multiple time\nseries methods to solve complex educational problems. Finally, we conclude with\na discussion on future directions, including personalized learning analytics,\nmultimodal data fusion, and the role of large language models (LLMs) in\neducational time series. The contributions of this paper include a detailed\ntaxonomy of educational data, a synthesis of time series techniques with\nspecific educational applications, and a forward-looking perspective on\nemerging trends and future research opportunities in educational analysis. The\nrelated papers and resources are available and regularly updated at the project\npage.\n", "link": "http://arxiv.org/abs/2408.13960v2", "date": "2024-08-27", "relevancy": 1.217, "topK": [{"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.4165}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.4026}, {"title": "Customize-A-Video: One-Shot Motion Customization of Text-to-Video\n  Diffusion Models", "link": "http://arxiv.org/abs/2402.14780v1", "similarity": 0.4025}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Time%20Series%20Analysis%20for%20Education%3A%20Methods%2C%20Applications%2C%20and%20Future%0A%20%20Directions&body=Title%3A%20Time%20Series%20Analysis%20for%20Education%3A%20Methods%2C%20Applications%2C%20and%20Future%0A%20%20Directions%0AAuthor%3A%20Shengzhong%20Mao%20and%20Chaoli%20Zhang%20and%20Yichi%20Song%20and%20Jindong%20Wang%20and%20Xiao-Jun%20Zeng%20and%20Zenglin%20Xu%20and%20Qingsong%20Wen%0AAbstract%3A%20%20%20Recent%20advancements%20in%20the%20collection%20and%20analysis%20of%20sequential%20educational%0Adata%20have%20brought%20time%20series%20analysis%20to%20a%20pivotal%20position%20in%20educational%0Aresearch%2C%20highlighting%20its%20essential%20role%20in%20facilitating%20data-driven%0Adecision-making.%20However%2C%20there%20is%20a%20lack%20of%20comprehensive%20summaries%20that%0Aconsolidate%20these%20advancements.%20To%20the%20best%20of%20our%20knowledge%2C%20this%20paper%20is%20the%0Afirst%20to%20provide%20a%20comprehensive%20review%20of%20time%20series%20analysis%20techniques%0Aspecifically%20within%20the%20educational%20context.%20We%20begin%20by%20exploring%20the%0Alandscape%20of%20educational%20data%20analytics%2C%20categorizing%20various%20data%20sources%20and%0Atypes%20relevant%20to%20education.%20We%20then%20review%20four%20prominent%20time%20series%0Amethods-forecasting%2C%20classification%2C%20clustering%2C%20and%20anomaly%0Adetection-illustrating%20their%20specific%20application%20points%20in%20educational%0Asettings.%20Subsequently%2C%20we%20present%20a%20range%20of%20educational%20scenarios%20and%0Aapplications%2C%20focusing%20on%20how%20these%20methods%20are%20employed%20to%20address%20diverse%0Aeducational%20tasks%2C%20which%20highlights%20the%20practical%20integration%20of%20multiple%20time%0Aseries%20methods%20to%20solve%20complex%20educational%20problems.%20Finally%2C%20we%20conclude%20with%0Aa%20discussion%20on%20future%20directions%2C%20including%20personalized%20learning%20analytics%2C%0Amultimodal%20data%20fusion%2C%20and%20the%20role%20of%20large%20language%20models%20%28LLMs%29%20in%0Aeducational%20time%20series.%20The%20contributions%20of%20this%20paper%20include%20a%20detailed%0Ataxonomy%20of%20educational%20data%2C%20a%20synthesis%20of%20time%20series%20techniques%20with%0Aspecific%20educational%20applications%2C%20and%20a%20forward-looking%20perspective%20on%0Aemerging%20trends%20and%20future%20research%20opportunities%20in%20educational%20analysis.%20The%0Arelated%20papers%20and%20resources%20are%20available%20and%20regularly%20updated%20at%20the%20project%0Apage.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2408.13960v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DTime%2520Series%2520Analysis%2520for%2520Education%253A%2520Methods%252C%2520Applications%252C%2520and%2520Future%250A%2520%2520Directions%26entry.906535625%3DShengzhong%2520Mao%2520and%2520Chaoli%2520Zhang%2520and%2520Yichi%2520Song%2520and%2520Jindong%2520Wang%2520and%2520Xiao-Jun%2520Zeng%2520and%2520Zenglin%2520Xu%2520and%2520Qingsong%2520Wen%26entry.1292438233%3D%2520%2520Recent%2520advancements%2520in%2520the%2520collection%2520and%2520analysis%2520of%2520sequential%2520educational%250Adata%2520have%2520brought%2520time%2520series%2520analysis%2520to%2520a%2520pivotal%2520position%2520in%2520educational%250Aresearch%252C%2520highlighting%2520its%2520essential%2520role%2520in%2520facilitating%2520data-driven%250Adecision-making.%2520However%252C%2520there%2520is%2520a%2520lack%2520of%2520comprehensive%2520summaries%2520that%250Aconsolidate%2520these%2520advancements.%2520To%2520the%2520best%2520of%2520our%2520knowledge%252C%2520this%2520paper%2520is%2520the%250Afirst%2520to%2520provide%2520a%2520comprehensive%2520review%2520of%2520time%2520series%2520analysis%2520techniques%250Aspecifically%2520within%2520the%2520educational%2520context.%2520We%2520begin%2520by%2520exploring%2520the%250Alandscape%2520of%2520educational%2520data%2520analytics%252C%2520categorizing%2520various%2520data%2520sources%2520and%250Atypes%2520relevant%2520to%2520education.%2520We%2520then%2520review%2520four%2520prominent%2520time%2520series%250Amethods-forecasting%252C%2520classification%252C%2520clustering%252C%2520and%2520anomaly%250Adetection-illustrating%2520their%2520specific%2520application%2520points%2520in%2520educational%250Asettings.%2520Subsequently%252C%2520we%2520present%2520a%2520range%2520of%2520educational%2520scenarios%2520and%250Aapplications%252C%2520focusing%2520on%2520how%2520these%2520methods%2520are%2520employed%2520to%2520address%2520diverse%250Aeducational%2520tasks%252C%2520which%2520highlights%2520the%2520practical%2520integration%2520of%2520multiple%2520time%250Aseries%2520methods%2520to%2520solve%2520complex%2520educational%2520problems.%2520Finally%252C%2520we%2520conclude%2520with%250Aa%2520discussion%2520on%2520future%2520directions%252C%2520including%2520personalized%2520learning%2520analytics%252C%250Amultimodal%2520data%2520fusion%252C%2520and%2520the%2520role%2520of%2520large%2520language%2520models%2520%2528LLMs%2529%2520in%250Aeducational%2520time%2520series.%2520The%2520contributions%2520of%2520this%2520paper%2520include%2520a%2520detailed%250Ataxonomy%2520of%2520educational%2520data%252C%2520a%2520synthesis%2520of%2520time%2520series%2520techniques%2520with%250Aspecific%2520educational%2520applications%252C%2520and%2520a%2520forward-looking%2520perspective%2520on%250Aemerging%2520trends%2520and%2520future%2520research%2520opportunities%2520in%2520educational%2520analysis.%2520The%250Arelated%2520papers%2520and%2520resources%2520are%2520available%2520and%2520regularly%2520updated%2520at%2520the%2520project%250Apage.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2408.13960v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Time%20Series%20Analysis%20for%20Education%3A%20Methods%2C%20Applications%2C%20and%20Future%0A%20%20Directions&entry.906535625=Shengzhong%20Mao%20and%20Chaoli%20Zhang%20and%20Yichi%20Song%20and%20Jindong%20Wang%20and%20Xiao-Jun%20Zeng%20and%20Zenglin%20Xu%20and%20Qingsong%20Wen&entry.1292438233=%20%20Recent%20advancements%20in%20the%20collection%20and%20analysis%20of%20sequential%20educational%0Adata%20have%20brought%20time%20series%20analysis%20to%20a%20pivotal%20position%20in%20educational%0Aresearch%2C%20highlighting%20its%20essential%20role%20in%20facilitating%20data-driven%0Adecision-making.%20However%2C%20there%20is%20a%20lack%20of%20comprehensive%20summaries%20that%0Aconsolidate%20these%20advancements.%20To%20the%20best%20of%20our%20knowledge%2C%20this%20paper%20is%20the%0Afirst%20to%20provide%20a%20comprehensive%20review%20of%20time%20series%20analysis%20techniques%0Aspecifically%20within%20the%20educational%20context.%20We%20begin%20by%20exploring%20the%0Alandscape%20of%20educational%20data%20analytics%2C%20categorizing%20various%20data%20sources%20and%0Atypes%20relevant%20to%20education.%20We%20then%20review%20four%20prominent%20time%20series%0Amethods-forecasting%2C%20classification%2C%20clustering%2C%20and%20anomaly%0Adetection-illustrating%20their%20specific%20application%20points%20in%20educational%0Asettings.%20Subsequently%2C%20we%20present%20a%20range%20of%20educational%20scenarios%20and%0Aapplications%2C%20focusing%20on%20how%20these%20methods%20are%20employed%20to%20address%20diverse%0Aeducational%20tasks%2C%20which%20highlights%20the%20practical%20integration%20of%20multiple%20time%0Aseries%20methods%20to%20solve%20complex%20educational%20problems.%20Finally%2C%20we%20conclude%20with%0Aa%20discussion%20on%20future%20directions%2C%20including%20personalized%20learning%20analytics%2C%0Amultimodal%20data%20fusion%2C%20and%20the%20role%20of%20large%20language%20models%20%28LLMs%29%20in%0Aeducational%20time%20series.%20The%20contributions%20of%20this%20paper%20include%20a%20detailed%0Ataxonomy%20of%20educational%20data%2C%20a%20synthesis%20of%20time%20series%20techniques%20with%0Aspecific%20educational%20applications%2C%20and%20a%20forward-looking%20perspective%20on%0Aemerging%20trends%20and%20future%20research%20opportunities%20in%20educational%20analysis.%20The%0Arelated%20papers%20and%20resources%20are%20available%20and%20regularly%20updated%20at%20the%20project%0Apage.%0A&entry.1838667208=http%3A//arxiv.org/abs/2408.13960v2&entry.124074799=Read"},
      ];
      const content = document.getElementById('content');
      function createPostElement(post) {
        const postElement = document.createElement('div');
        postElement.className = 'post';
        const dateElem = document.createElement('p');
        dateElem.setAttribute("class", "date");
        dateElem.textContent = post.date;
        postElement.appendChild(dateElem);

        const textElem = document.createElement('p');
        textElem.setAttribute("class", "text");
        const titleElem = document.createElement('p');
        titleElem.setAttribute("class", "title");
        titleElem.textContent = post.title;
        textElem.appendChild(titleElem);
        const authorElem = document.createElement('p');
        authorElem.setAttribute("class", "author");
        authorElem.textContent = post.author;
        textElem.appendChild(authorElem);
        const abstractElem = document.createElement('p');
        abstractElem.setAttribute("class", "abstract");
        abstractElem.textContent = post.abstract;
        textElem.appendChild(abstractElem);

        const linkElement = document.createElement('a');
        linkElement.setAttribute("class", "link");
        linkElement.href = post.link;
        linkElement.target = "_blank";
        linkElement.textContent = post.link.length > 50 ? post.link.substring(0, 50) + '...' : post.link;
        textElem.appendChild(linkElement);
        postElement.appendChild(textElem);

        const linkElementContainer = document.createElement('div');
        linkElementContainer.setAttribute("class", "comment");
        const actionElement = document.createElement('a');
        actionElement.setAttribute("class", "comment");
        actionElement.href = post.form;
        actionElement.textContent = "Action";
        actionElement.target = "_blank";
        linkElementContainer.appendChild(actionElement);
        const emailElement = document.createElement('a');
        emailElement.setAttribute("class", "comment");
        emailElement.href = post.mailto;
        emailElement.textContent = "Email";
        emailElement.target = "_blank";
        linkElementContainer.appendChild(emailElement);
        postElement.appendChild(linkElementContainer);
        const e = document.createElement('div');
        e.setAttribute("class", "clear");
        postElement.appendChild(e);

        const relevancyContainer = document.createElement('div');
        const relevancyValElem = document.createElement('p');
        relevancyValElem.textContent = "Relevancy " + post.relevancy;
        relevancyContainer.appendChild(relevancyValElem);
        post.topK.forEach((sub) => {
          const topKElem = document.createElement('a');
          topKElem.setAttribute("class", "topK");
          topKElem.href = sub.link;
          topKElem.textContent = sub.title + " (" + sub.similarity + ")";
          topKElem.target = "_blank";
          relevancyContainer.appendChild(topKElem);
        });
        postElement.appendChild(relevancyContainer);
        return postElement;
      }
      function loadPosts() {
        // Simulate loading more posts
        posts.forEach((post) => {
          const postElement = createPostElement(post);
          content.appendChild(postElement);
        });
      }
      // Load initial posts
      loadPosts();
    </script>

  </body>
</html>


