<!DOCTYPE html>
<html lang="en">
  <head>
    <script async src="https://www.googletagmanager.com/gtag/js?id=G-V34CNNDP8V"></script>
    <script>
      window.dataLayer = window.dataLayer || [];
      function gtag(){dataLayer.push(arguments);}
      gtag('js', new Date());
      gtag('config', 'G-V34CNNDP8V');
    </script>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Arxiv Paper Selection</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.3/css/all.min.css" crossorigin="anonymous" referrerpolicy="no-referrer" />
    <style>
    body {
      font-family: Arial, sans-serif;
      margin: 0;
      padding: 0;
      background-color: #f4f4f4;
    }
    header {
      position: fixed;
      top: 0;
      left: 0;
      width: 100%;
      background-color: #ffffff;
      color: black;
      padding: 10px;
      text-align: center;
      z-index: 1000;
      border-bottom: 1px solid #ddd;
    }
    header div {
      display: block;
      margin: 10px auto;
    }

    #home-icon {
      display: block;
      float: left;
      margin: 5px;
      text-decoration: none;
      color: black;
    }

    main {
      margin-top: 60px; /* Adjusted margin to account for fixed header */
      padding: 20px;
    }

    .post {
      background-color: white;
      border: 1px solid #ddd;
      border-radius: 5px;
      margin-bottom: 10px;
      padding: 10px 20px;
      max-height: 2000px;
      overflow: scroll;
    }
    .post img {
      display: block;
      margin-top: 5px;
      max-width: auto;
      max-height: 100px;
    }
    .post .clear {
      clear: both;
      display: block;
    }
    .post a {
      text-decoration: none;
    }
    .post a:hover {
      color: #0056b3;
    }
    .post a:visited {
      color: #0056b3;
    }
    .post div.comment {
      text-align: right;
    }
    .post div.comment a {
      margin: 1em;
    }
    .post .text {
      margin: 1em 0em;
      padding: 0;
    }
    .post .text .title {
    }
    .post .text .author {
    }
    .post .text .abstract {
    }
    .post .topK {
      display: block;
      margin: 0.5em;
    }
    .post .date {
      margin: 0;
      padding: 0;
      text-size: small; 
      color: gray;
    }
    .post .link {
      margin: 0;
      padding: 0;
    }
    @media screen and (max-width: 600px) {
      body {
        max-width: 100%; 
      }
      #home-icon {
        float: none;
        display: block;
        text-align: center;
        margin-bottom: 10px;
      }
    }
    footer {
      width: 100%;
      background-color: #ddd;
      text-align: center;
      z-index: 1000;
      padding: 20px 0px;
      margin-bottom: 20px;
      left: 0;
    }

    #next-btn,
    #prev-btn {
      background-color: #4CAF50;
      color: white;
      padding: 8px 16px;
      margin: 0 50px;
      border: none;
      border-radius: 4px;
      cursor: pointer;
    }

    .links {
      padding: 20px;
    }
    .links a {
      text-decoration: none;
    }
    .links a:hover {
      color: #0056b3;
    }
    .links a:visited {
      color: #0056b3;
    }

    #page-index {
      font-size: small;
    }
    .ads {
      width: 100%;
    }
    #prev-btn {
      background-color: #4CAF50;
      color: white;
      padding: 8px 16px;
      margin: 0 50px;
      border: none;
      border-radius: 4px;
      cursor: pointer;
    }
    </style>
  </head>
  <body>

    <header>
      <a id="prev-btn" href="daily20241126.html"><i class="fas fa-chevron-left"></i></a>
      <a href="https://haoxiang.org/">About</a>
    </header>

    <main id="content">
      <!-- Posts will be dynamically added here using JavaScript -->
    </main>

    <script>
      // Dummy data for posts
      const posts = [
{"title": "Distractor-free Generalizable 3D Gaussian Splatting", "author": "Yanqi Bao and Jing Liao and Jing Huo and Yang Gao", "abstract": "  We present DGGS, a novel framework addressing the previously unexplored\nchallenge of Distractor-free Generalizable 3D Gaussian Splatting (3DGS). It\naccomplishes two key objectives: fortifying generalizable 3DGS against\ndistractor-laden data during both training and inference phases, while\nsuccessfully extending cross-scene adaptation capabilities to conventional\ndistractor-free approaches. To achieve these objectives, DGGS introduces a\nscene-agnostic reference-based mask prediction and refinement methodology\nduring training phase, coupled with a training view selection strategy,\neffectively improving distractor prediction accuracy and training stability.\nMoreover, to address distractor-induced voids and artifacts during inference\nstage, we propose a two-stage inference framework for better reference\nselection based on the predicted distractor masks, complemented by a distractor\npruning module to eliminate residual distractor effects. Extensive\ngeneralization experiments demonstrate DGGS's advantages under distractor-laden\nconditions. Additionally, experimental results show that our scene-agnostic\nmask inference achieves accuracy comparable to scene-specific trained methods.\nHomepage is \\url{https://github.com/bbbbby-99/DGGS}.\n", "link": "http://arxiv.org/abs/2411.17605v1", "date": "2024-11-26", "relevancy": 3.4294, "topK": [{"title": "GST: Precise 3D Human Body from a Single Image with Gaussian Splatting\n  Transformers", "link": "http://arxiv.org/abs/2409.04196v1", "similarity": 0.7135}, {"title": "RTG-SLAM: Real-time 3D Reconstruction at Scale using Gaussian Splatting", "link": "http://arxiv.org/abs/2404.19706v1", "similarity": 0.6876}, {"title": "Dynamic Gaussians Mesh: Consistent Mesh Reconstruction from Monocular\n  Videos", "link": "http://arxiv.org/abs/2404.12379v2", "similarity": 0.6565}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Distractor-free%20Generalizable%203D%20Gaussian%20Splatting&body=Title%3A%20Distractor-free%20Generalizable%203D%20Gaussian%20Splatting%0AAuthor%3A%20Yanqi%20Bao%20and%20Jing%20Liao%20and%20Jing%20Huo%20and%20Yang%20Gao%0AAbstract%3A%20%20%20We%20present%20DGGS%2C%20a%20novel%20framework%20addressing%20the%20previously%20unexplored%0Achallenge%20of%20Distractor-free%20Generalizable%203D%20Gaussian%20Splatting%20%283DGS%29.%20It%0Aaccomplishes%20two%20key%20objectives%3A%20fortifying%20generalizable%203DGS%20against%0Adistractor-laden%20data%20during%20both%20training%20and%20inference%20phases%2C%20while%0Asuccessfully%20extending%20cross-scene%20adaptation%20capabilities%20to%20conventional%0Adistractor-free%20approaches.%20To%20achieve%20these%20objectives%2C%20DGGS%20introduces%20a%0Ascene-agnostic%20reference-based%20mask%20prediction%20and%20refinement%20methodology%0Aduring%20training%20phase%2C%20coupled%20with%20a%20training%20view%20selection%20strategy%2C%0Aeffectively%20improving%20distractor%20prediction%20accuracy%20and%20training%20stability.%0AMoreover%2C%20to%20address%20distractor-induced%20voids%20and%20artifacts%20during%20inference%0Astage%2C%20we%20propose%20a%20two-stage%20inference%20framework%20for%20better%20reference%0Aselection%20based%20on%20the%20predicted%20distractor%20masks%2C%20complemented%20by%20a%20distractor%0Apruning%20module%20to%20eliminate%20residual%20distractor%20effects.%20Extensive%0Ageneralization%20experiments%20demonstrate%20DGGS%27s%20advantages%20under%20distractor-laden%0Aconditions.%20Additionally%2C%20experimental%20results%20show%20that%20our%20scene-agnostic%0Amask%20inference%20achieves%20accuracy%20comparable%20to%20scene-specific%20trained%20methods.%0AHomepage%20is%20%5Curl%7Bhttps%3A//github.com/bbbbby-99/DGGS%7D.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2411.17605v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DDistractor-free%2520Generalizable%25203D%2520Gaussian%2520Splatting%26entry.906535625%3DYanqi%2520Bao%2520and%2520Jing%2520Liao%2520and%2520Jing%2520Huo%2520and%2520Yang%2520Gao%26entry.1292438233%3D%2520%2520We%2520present%2520DGGS%252C%2520a%2520novel%2520framework%2520addressing%2520the%2520previously%2520unexplored%250Achallenge%2520of%2520Distractor-free%2520Generalizable%25203D%2520Gaussian%2520Splatting%2520%25283DGS%2529.%2520It%250Aaccomplishes%2520two%2520key%2520objectives%253A%2520fortifying%2520generalizable%25203DGS%2520against%250Adistractor-laden%2520data%2520during%2520both%2520training%2520and%2520inference%2520phases%252C%2520while%250Asuccessfully%2520extending%2520cross-scene%2520adaptation%2520capabilities%2520to%2520conventional%250Adistractor-free%2520approaches.%2520To%2520achieve%2520these%2520objectives%252C%2520DGGS%2520introduces%2520a%250Ascene-agnostic%2520reference-based%2520mask%2520prediction%2520and%2520refinement%2520methodology%250Aduring%2520training%2520phase%252C%2520coupled%2520with%2520a%2520training%2520view%2520selection%2520strategy%252C%250Aeffectively%2520improving%2520distractor%2520prediction%2520accuracy%2520and%2520training%2520stability.%250AMoreover%252C%2520to%2520address%2520distractor-induced%2520voids%2520and%2520artifacts%2520during%2520inference%250Astage%252C%2520we%2520propose%2520a%2520two-stage%2520inference%2520framework%2520for%2520better%2520reference%250Aselection%2520based%2520on%2520the%2520predicted%2520distractor%2520masks%252C%2520complemented%2520by%2520a%2520distractor%250Apruning%2520module%2520to%2520eliminate%2520residual%2520distractor%2520effects.%2520Extensive%250Ageneralization%2520experiments%2520demonstrate%2520DGGS%2527s%2520advantages%2520under%2520distractor-laden%250Aconditions.%2520Additionally%252C%2520experimental%2520results%2520show%2520that%2520our%2520scene-agnostic%250Amask%2520inference%2520achieves%2520accuracy%2520comparable%2520to%2520scene-specific%2520trained%2520methods.%250AHomepage%2520is%2520%255Curl%257Bhttps%253A//github.com/bbbbby-99/DGGS%257D.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2411.17605v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Distractor-free%20Generalizable%203D%20Gaussian%20Splatting&entry.906535625=Yanqi%20Bao%20and%20Jing%20Liao%20and%20Jing%20Huo%20and%20Yang%20Gao&entry.1292438233=%20%20We%20present%20DGGS%2C%20a%20novel%20framework%20addressing%20the%20previously%20unexplored%0Achallenge%20of%20Distractor-free%20Generalizable%203D%20Gaussian%20Splatting%20%283DGS%29.%20It%0Aaccomplishes%20two%20key%20objectives%3A%20fortifying%20generalizable%203DGS%20against%0Adistractor-laden%20data%20during%20both%20training%20and%20inference%20phases%2C%20while%0Asuccessfully%20extending%20cross-scene%20adaptation%20capabilities%20to%20conventional%0Adistractor-free%20approaches.%20To%20achieve%20these%20objectives%2C%20DGGS%20introduces%20a%0Ascene-agnostic%20reference-based%20mask%20prediction%20and%20refinement%20methodology%0Aduring%20training%20phase%2C%20coupled%20with%20a%20training%20view%20selection%20strategy%2C%0Aeffectively%20improving%20distractor%20prediction%20accuracy%20and%20training%20stability.%0AMoreover%2C%20to%20address%20distractor-induced%20voids%20and%20artifacts%20during%20inference%0Astage%2C%20we%20propose%20a%20two-stage%20inference%20framework%20for%20better%20reference%0Aselection%20based%20on%20the%20predicted%20distractor%20masks%2C%20complemented%20by%20a%20distractor%0Apruning%20module%20to%20eliminate%20residual%20distractor%20effects.%20Extensive%0Ageneralization%20experiments%20demonstrate%20DGGS%27s%20advantages%20under%20distractor-laden%0Aconditions.%20Additionally%2C%20experimental%20results%20show%20that%20our%20scene-agnostic%0Amask%20inference%20achieves%20accuracy%20comparable%20to%20scene-specific%20trained%20methods.%0AHomepage%20is%20%5Curl%7Bhttps%3A//github.com/bbbbby-99/DGGS%7D.%0A&entry.1838667208=http%3A//arxiv.org/abs/2411.17605v1&entry.124074799=Read"},
{"title": "DROID-Splat: Combining end-to-end SLAM with 3D Gaussian Splatting", "author": "Christian Homeyer and Leon Begiristain and Christoph Schn\u00f6rr", "abstract": "  Recent progress in scene synthesis makes standalone SLAM systems purely based\non optimizing hyperprimitives with a Rendering objective possible\n\\cite{monogs}.\n  However, the tracking performance still lacks behind traditional\n\\cite{orbslam} and end-to-end SLAM systems \\cite{droid}.\n  An optimal trade-off between robustness, speed and accuracy has not yet been\nreached, especially for monocular video.\n  In this paper, we introduce a SLAM system based on an end-to-end Tracker and\nextend it with a Renderer based on recent 3D Gaussian Splatting techniques.\n  Our framework \\textbf{DroidSplat} achieves both SotA tracking and rendering\nresults on common SLAM benchmarks.\n  We implemented multiple building blocks of modern SLAM systems to run in\nparallel, allowing for fast inference on common consumer GPU's.\n  Recent progress in monocular depth prediction and camera calibration allows\nour system to achieve strong results even on in-the-wild data without known\ncamera intrinsics.\n  Code will be available at \\url{https://github.com/ChenHoy/DROID-Splat}.\n", "link": "http://arxiv.org/abs/2411.17660v1", "date": "2024-11-26", "relevancy": 3.3122, "topK": [{"title": "RTG-SLAM: Real-time 3D Reconstruction at Scale using Gaussian Splatting", "link": "http://arxiv.org/abs/2404.19706v1", "similarity": 0.7424}, {"title": "GST: Precise 3D Human Body from a Single Image with Gaussian Splatting\n  Transformers", "link": "http://arxiv.org/abs/2409.04196v1", "similarity": 0.6383}, {"title": "Dynamic Gaussians Mesh: Consistent Mesh Reconstruction from Monocular\n  Videos", "link": "http://arxiv.org/abs/2404.12379v2", "similarity": 0.6066}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20DROID-Splat%3A%20Combining%20end-to-end%20SLAM%20with%203D%20Gaussian%20Splatting&body=Title%3A%20DROID-Splat%3A%20Combining%20end-to-end%20SLAM%20with%203D%20Gaussian%20Splatting%0AAuthor%3A%20Christian%20Homeyer%20and%20Leon%20Begiristain%20and%20Christoph%20Schn%C3%B6rr%0AAbstract%3A%20%20%20Recent%20progress%20in%20scene%20synthesis%20makes%20standalone%20SLAM%20systems%20purely%20based%0Aon%20optimizing%20hyperprimitives%20with%20a%20Rendering%20objective%20possible%0A%5Ccite%7Bmonogs%7D.%0A%20%20However%2C%20the%20tracking%20performance%20still%20lacks%20behind%20traditional%0A%5Ccite%7Borbslam%7D%20and%20end-to-end%20SLAM%20systems%20%5Ccite%7Bdroid%7D.%0A%20%20An%20optimal%20trade-off%20between%20robustness%2C%20speed%20and%20accuracy%20has%20not%20yet%20been%0Areached%2C%20especially%20for%20monocular%20video.%0A%20%20In%20this%20paper%2C%20we%20introduce%20a%20SLAM%20system%20based%20on%20an%20end-to-end%20Tracker%20and%0Aextend%20it%20with%20a%20Renderer%20based%20on%20recent%203D%20Gaussian%20Splatting%20techniques.%0A%20%20Our%20framework%20%5Ctextbf%7BDroidSplat%7D%20achieves%20both%20SotA%20tracking%20and%20rendering%0Aresults%20on%20common%20SLAM%20benchmarks.%0A%20%20We%20implemented%20multiple%20building%20blocks%20of%20modern%20SLAM%20systems%20to%20run%20in%0Aparallel%2C%20allowing%20for%20fast%20inference%20on%20common%20consumer%20GPU%27s.%0A%20%20Recent%20progress%20in%20monocular%20depth%20prediction%20and%20camera%20calibration%20allows%0Aour%20system%20to%20achieve%20strong%20results%20even%20on%20in-the-wild%20data%20without%20known%0Acamera%20intrinsics.%0A%20%20Code%20will%20be%20available%20at%20%5Curl%7Bhttps%3A//github.com/ChenHoy/DROID-Splat%7D.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2411.17660v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DDROID-Splat%253A%2520Combining%2520end-to-end%2520SLAM%2520with%25203D%2520Gaussian%2520Splatting%26entry.906535625%3DChristian%2520Homeyer%2520and%2520Leon%2520Begiristain%2520and%2520Christoph%2520Schn%25C3%25B6rr%26entry.1292438233%3D%2520%2520Recent%2520progress%2520in%2520scene%2520synthesis%2520makes%2520standalone%2520SLAM%2520systems%2520purely%2520based%250Aon%2520optimizing%2520hyperprimitives%2520with%2520a%2520Rendering%2520objective%2520possible%250A%255Ccite%257Bmonogs%257D.%250A%2520%2520However%252C%2520the%2520tracking%2520performance%2520still%2520lacks%2520behind%2520traditional%250A%255Ccite%257Borbslam%257D%2520and%2520end-to-end%2520SLAM%2520systems%2520%255Ccite%257Bdroid%257D.%250A%2520%2520An%2520optimal%2520trade-off%2520between%2520robustness%252C%2520speed%2520and%2520accuracy%2520has%2520not%2520yet%2520been%250Areached%252C%2520especially%2520for%2520monocular%2520video.%250A%2520%2520In%2520this%2520paper%252C%2520we%2520introduce%2520a%2520SLAM%2520system%2520based%2520on%2520an%2520end-to-end%2520Tracker%2520and%250Aextend%2520it%2520with%2520a%2520Renderer%2520based%2520on%2520recent%25203D%2520Gaussian%2520Splatting%2520techniques.%250A%2520%2520Our%2520framework%2520%255Ctextbf%257BDroidSplat%257D%2520achieves%2520both%2520SotA%2520tracking%2520and%2520rendering%250Aresults%2520on%2520common%2520SLAM%2520benchmarks.%250A%2520%2520We%2520implemented%2520multiple%2520building%2520blocks%2520of%2520modern%2520SLAM%2520systems%2520to%2520run%2520in%250Aparallel%252C%2520allowing%2520for%2520fast%2520inference%2520on%2520common%2520consumer%2520GPU%2527s.%250A%2520%2520Recent%2520progress%2520in%2520monocular%2520depth%2520prediction%2520and%2520camera%2520calibration%2520allows%250Aour%2520system%2520to%2520achieve%2520strong%2520results%2520even%2520on%2520in-the-wild%2520data%2520without%2520known%250Acamera%2520intrinsics.%250A%2520%2520Code%2520will%2520be%2520available%2520at%2520%255Curl%257Bhttps%253A//github.com/ChenHoy/DROID-Splat%257D.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2411.17660v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=DROID-Splat%3A%20Combining%20end-to-end%20SLAM%20with%203D%20Gaussian%20Splatting&entry.906535625=Christian%20Homeyer%20and%20Leon%20Begiristain%20and%20Christoph%20Schn%C3%B6rr&entry.1292438233=%20%20Recent%20progress%20in%20scene%20synthesis%20makes%20standalone%20SLAM%20systems%20purely%20based%0Aon%20optimizing%20hyperprimitives%20with%20a%20Rendering%20objective%20possible%0A%5Ccite%7Bmonogs%7D.%0A%20%20However%2C%20the%20tracking%20performance%20still%20lacks%20behind%20traditional%0A%5Ccite%7Borbslam%7D%20and%20end-to-end%20SLAM%20systems%20%5Ccite%7Bdroid%7D.%0A%20%20An%20optimal%20trade-off%20between%20robustness%2C%20speed%20and%20accuracy%20has%20not%20yet%20been%0Areached%2C%20especially%20for%20monocular%20video.%0A%20%20In%20this%20paper%2C%20we%20introduce%20a%20SLAM%20system%20based%20on%20an%20end-to-end%20Tracker%20and%0Aextend%20it%20with%20a%20Renderer%20based%20on%20recent%203D%20Gaussian%20Splatting%20techniques.%0A%20%20Our%20framework%20%5Ctextbf%7BDroidSplat%7D%20achieves%20both%20SotA%20tracking%20and%20rendering%0Aresults%20on%20common%20SLAM%20benchmarks.%0A%20%20We%20implemented%20multiple%20building%20blocks%20of%20modern%20SLAM%20systems%20to%20run%20in%0Aparallel%2C%20allowing%20for%20fast%20inference%20on%20common%20consumer%20GPU%27s.%0A%20%20Recent%20progress%20in%20monocular%20depth%20prediction%20and%20camera%20calibration%20allows%0Aour%20system%20to%20achieve%20strong%20results%20even%20on%20in-the-wild%20data%20without%20known%0Acamera%20intrinsics.%0A%20%20Code%20will%20be%20available%20at%20%5Curl%7Bhttps%3A//github.com/ChenHoy/DROID-Splat%7D.%0A&entry.1838667208=http%3A//arxiv.org/abs/2411.17660v1&entry.124074799=Read"},
{"title": "What's in the Image? A Deep-Dive into the Vision of Vision Language\n  Models", "author": "Omri Kaduri and Shai Bagon and Tali Dekel", "abstract": "  Vision-Language Models (VLMs) have recently demonstrated remarkable\ncapabilities in comprehending complex visual content. However, the mechanisms\nunderlying how VLMs process visual information remain largely unexplored. In\nthis paper, we conduct a thorough empirical analysis, focusing on attention\nmodules across layers. We reveal several key insights about how these models\nprocess visual data: (i) the internal representation of the query tokens (e.g.,\nrepresentations of \"describe the image\"), is utilized by VLMs to store global\nimage information; we demonstrate that these models generate surprisingly\ndescriptive responses solely from these tokens, without direct access to image\ntokens. (ii) Cross-modal information flow is predominantly influenced by the\nmiddle layers (approximately 25% of all layers), while early and late layers\ncontribute only marginally.(iii) Fine-grained visual attributes and object\ndetails are directly extracted from image tokens in a spatially localized\nmanner, i.e., the generated tokens associated with a specific object or\nattribute attend strongly to their corresponding regions in the image. We\npropose novel quantitative evaluation to validate our observations, leveraging\nreal-world complex visual scenes. Finally, we demonstrate the potential of our\nfindings in facilitating efficient visual processing in state-of-the-art VLMs.\n", "link": "http://arxiv.org/abs/2411.17491v1", "date": "2024-11-26", "relevancy": 3.2644, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.6917}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.6917}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5753}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20What%27s%20in%20the%20Image%3F%20A%20Deep-Dive%20into%20the%20Vision%20of%20Vision%20Language%0A%20%20Models&body=Title%3A%20What%27s%20in%20the%20Image%3F%20A%20Deep-Dive%20into%20the%20Vision%20of%20Vision%20Language%0A%20%20Models%0AAuthor%3A%20Omri%20Kaduri%20and%20Shai%20Bagon%20and%20Tali%20Dekel%0AAbstract%3A%20%20%20Vision-Language%20Models%20%28VLMs%29%20have%20recently%20demonstrated%20remarkable%0Acapabilities%20in%20comprehending%20complex%20visual%20content.%20However%2C%20the%20mechanisms%0Aunderlying%20how%20VLMs%20process%20visual%20information%20remain%20largely%20unexplored.%20In%0Athis%20paper%2C%20we%20conduct%20a%20thorough%20empirical%20analysis%2C%20focusing%20on%20attention%0Amodules%20across%20layers.%20We%20reveal%20several%20key%20insights%20about%20how%20these%20models%0Aprocess%20visual%20data%3A%20%28i%29%20the%20internal%20representation%20of%20the%20query%20tokens%20%28e.g.%2C%0Arepresentations%20of%20%22describe%20the%20image%22%29%2C%20is%20utilized%20by%20VLMs%20to%20store%20global%0Aimage%20information%3B%20we%20demonstrate%20that%20these%20models%20generate%20surprisingly%0Adescriptive%20responses%20solely%20from%20these%20tokens%2C%20without%20direct%20access%20to%20image%0Atokens.%20%28ii%29%20Cross-modal%20information%20flow%20is%20predominantly%20influenced%20by%20the%0Amiddle%20layers%20%28approximately%2025%25%20of%20all%20layers%29%2C%20while%20early%20and%20late%20layers%0Acontribute%20only%20marginally.%28iii%29%20Fine-grained%20visual%20attributes%20and%20object%0Adetails%20are%20directly%20extracted%20from%20image%20tokens%20in%20a%20spatially%20localized%0Amanner%2C%20i.e.%2C%20the%20generated%20tokens%20associated%20with%20a%20specific%20object%20or%0Aattribute%20attend%20strongly%20to%20their%20corresponding%20regions%20in%20the%20image.%20We%0Apropose%20novel%20quantitative%20evaluation%20to%20validate%20our%20observations%2C%20leveraging%0Areal-world%20complex%20visual%20scenes.%20Finally%2C%20we%20demonstrate%20the%20potential%20of%20our%0Afindings%20in%20facilitating%20efficient%20visual%20processing%20in%20state-of-the-art%20VLMs.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2411.17491v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DWhat%2527s%2520in%2520the%2520Image%253F%2520A%2520Deep-Dive%2520into%2520the%2520Vision%2520of%2520Vision%2520Language%250A%2520%2520Models%26entry.906535625%3DOmri%2520Kaduri%2520and%2520Shai%2520Bagon%2520and%2520Tali%2520Dekel%26entry.1292438233%3D%2520%2520Vision-Language%2520Models%2520%2528VLMs%2529%2520have%2520recently%2520demonstrated%2520remarkable%250Acapabilities%2520in%2520comprehending%2520complex%2520visual%2520content.%2520However%252C%2520the%2520mechanisms%250Aunderlying%2520how%2520VLMs%2520process%2520visual%2520information%2520remain%2520largely%2520unexplored.%2520In%250Athis%2520paper%252C%2520we%2520conduct%2520a%2520thorough%2520empirical%2520analysis%252C%2520focusing%2520on%2520attention%250Amodules%2520across%2520layers.%2520We%2520reveal%2520several%2520key%2520insights%2520about%2520how%2520these%2520models%250Aprocess%2520visual%2520data%253A%2520%2528i%2529%2520the%2520internal%2520representation%2520of%2520the%2520query%2520tokens%2520%2528e.g.%252C%250Arepresentations%2520of%2520%2522describe%2520the%2520image%2522%2529%252C%2520is%2520utilized%2520by%2520VLMs%2520to%2520store%2520global%250Aimage%2520information%253B%2520we%2520demonstrate%2520that%2520these%2520models%2520generate%2520surprisingly%250Adescriptive%2520responses%2520solely%2520from%2520these%2520tokens%252C%2520without%2520direct%2520access%2520to%2520image%250Atokens.%2520%2528ii%2529%2520Cross-modal%2520information%2520flow%2520is%2520predominantly%2520influenced%2520by%2520the%250Amiddle%2520layers%2520%2528approximately%252025%2525%2520of%2520all%2520layers%2529%252C%2520while%2520early%2520and%2520late%2520layers%250Acontribute%2520only%2520marginally.%2528iii%2529%2520Fine-grained%2520visual%2520attributes%2520and%2520object%250Adetails%2520are%2520directly%2520extracted%2520from%2520image%2520tokens%2520in%2520a%2520spatially%2520localized%250Amanner%252C%2520i.e.%252C%2520the%2520generated%2520tokens%2520associated%2520with%2520a%2520specific%2520object%2520or%250Aattribute%2520attend%2520strongly%2520to%2520their%2520corresponding%2520regions%2520in%2520the%2520image.%2520We%250Apropose%2520novel%2520quantitative%2520evaluation%2520to%2520validate%2520our%2520observations%252C%2520leveraging%250Areal-world%2520complex%2520visual%2520scenes.%2520Finally%252C%2520we%2520demonstrate%2520the%2520potential%2520of%2520our%250Afindings%2520in%2520facilitating%2520efficient%2520visual%2520processing%2520in%2520state-of-the-art%2520VLMs.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2411.17491v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=What%27s%20in%20the%20Image%3F%20A%20Deep-Dive%20into%20the%20Vision%20of%20Vision%20Language%0A%20%20Models&entry.906535625=Omri%20Kaduri%20and%20Shai%20Bagon%20and%20Tali%20Dekel&entry.1292438233=%20%20Vision-Language%20Models%20%28VLMs%29%20have%20recently%20demonstrated%20remarkable%0Acapabilities%20in%20comprehending%20complex%20visual%20content.%20However%2C%20the%20mechanisms%0Aunderlying%20how%20VLMs%20process%20visual%20information%20remain%20largely%20unexplored.%20In%0Athis%20paper%2C%20we%20conduct%20a%20thorough%20empirical%20analysis%2C%20focusing%20on%20attention%0Amodules%20across%20layers.%20We%20reveal%20several%20key%20insights%20about%20how%20these%20models%0Aprocess%20visual%20data%3A%20%28i%29%20the%20internal%20representation%20of%20the%20query%20tokens%20%28e.g.%2C%0Arepresentations%20of%20%22describe%20the%20image%22%29%2C%20is%20utilized%20by%20VLMs%20to%20store%20global%0Aimage%20information%3B%20we%20demonstrate%20that%20these%20models%20generate%20surprisingly%0Adescriptive%20responses%20solely%20from%20these%20tokens%2C%20without%20direct%20access%20to%20image%0Atokens.%20%28ii%29%20Cross-modal%20information%20flow%20is%20predominantly%20influenced%20by%20the%0Amiddle%20layers%20%28approximately%2025%25%20of%20all%20layers%29%2C%20while%20early%20and%20late%20layers%0Acontribute%20only%20marginally.%28iii%29%20Fine-grained%20visual%20attributes%20and%20object%0Adetails%20are%20directly%20extracted%20from%20image%20tokens%20in%20a%20spatially%20localized%0Amanner%2C%20i.e.%2C%20the%20generated%20tokens%20associated%20with%20a%20specific%20object%20or%0Aattribute%20attend%20strongly%20to%20their%20corresponding%20regions%20in%20the%20image.%20We%0Apropose%20novel%20quantitative%20evaluation%20to%20validate%20our%20observations%2C%20leveraging%0Areal-world%20complex%20visual%20scenes.%20Finally%2C%20we%20demonstrate%20the%20potential%20of%20our%0Afindings%20in%20facilitating%20efficient%20visual%20processing%20in%20state-of-the-art%20VLMs.%0A&entry.1838667208=http%3A//arxiv.org/abs/2411.17491v1&entry.124074799=Read"},
{"title": "Functionality understanding and segmentation in 3D scenes", "author": "Jaime Corsetti and Francesco Giuliari and Alice Fasoli and Davide Boscaini and Fabio Poiesi", "abstract": "  Understanding functionalities in 3D scenes involves interpreting natural\nlanguage descriptions to locate functional interactive objects, such as handles\nand buttons, in a 3D environment. Functionality understanding is highly\nchallenging, as it requires both world knowledge to interpret language and\nspatial perception to identify fine-grained objects. For example, given a task\nlike 'turn on the ceiling light', an embodied AI agent must infer that it needs\nto locate the light switch, even though the switch is not explicitly mentioned\nin the task description. To date, no dedicated methods have been developed for\nthis problem. In this paper, we introduce Fun3DU, the first approach designed\nfor functionality understanding in 3D scenes. Fun3DU uses a language model to\nparse the task description through Chain-of-Thought reasoning in order to\nidentify the object of interest. The identified object is segmented across\nmultiple views of the captured scene by using a vision and language model. The\nsegmentation results from each view are lifted in 3D and aggregated into the\npoint cloud using geometric information. Fun3DU is training-free, relying\nentirely on pre-trained models. We evaluate Fun3DU on SceneFun3D, the most\nrecent and only dataset to benchmark this task, which comprises over 3000 task\ndescriptions on 230 scenes. Our method significantly outperforms\nstate-of-the-art open-vocabulary 3D segmentation approaches. Project page:\nhttps://jcorsetti.github.io/fun3du\n", "link": "http://arxiv.org/abs/2411.16310v2", "date": "2024-11-26", "relevancy": 3.262, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.684}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.684}, {"title": "CAT3D: Create Anything in 3D with Multi-View Diffusion Models", "link": "http://arxiv.org/abs/2405.10314v1", "similarity": 0.5892}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Functionality%20understanding%20and%20segmentation%20in%203D%20scenes&body=Title%3A%20Functionality%20understanding%20and%20segmentation%20in%203D%20scenes%0AAuthor%3A%20Jaime%20Corsetti%20and%20Francesco%20Giuliari%20and%20Alice%20Fasoli%20and%20Davide%20Boscaini%20and%20Fabio%20Poiesi%0AAbstract%3A%20%20%20Understanding%20functionalities%20in%203D%20scenes%20involves%20interpreting%20natural%0Alanguage%20descriptions%20to%20locate%20functional%20interactive%20objects%2C%20such%20as%20handles%0Aand%20buttons%2C%20in%20a%203D%20environment.%20Functionality%20understanding%20is%20highly%0Achallenging%2C%20as%20it%20requires%20both%20world%20knowledge%20to%20interpret%20language%20and%0Aspatial%20perception%20to%20identify%20fine-grained%20objects.%20For%20example%2C%20given%20a%20task%0Alike%20%27turn%20on%20the%20ceiling%20light%27%2C%20an%20embodied%20AI%20agent%20must%20infer%20that%20it%20needs%0Ato%20locate%20the%20light%20switch%2C%20even%20though%20the%20switch%20is%20not%20explicitly%20mentioned%0Ain%20the%20task%20description.%20To%20date%2C%20no%20dedicated%20methods%20have%20been%20developed%20for%0Athis%20problem.%20In%20this%20paper%2C%20we%20introduce%20Fun3DU%2C%20the%20first%20approach%20designed%0Afor%20functionality%20understanding%20in%203D%20scenes.%20Fun3DU%20uses%20a%20language%20model%20to%0Aparse%20the%20task%20description%20through%20Chain-of-Thought%20reasoning%20in%20order%20to%0Aidentify%20the%20object%20of%20interest.%20The%20identified%20object%20is%20segmented%20across%0Amultiple%20views%20of%20the%20captured%20scene%20by%20using%20a%20vision%20and%20language%20model.%20The%0Asegmentation%20results%20from%20each%20view%20are%20lifted%20in%203D%20and%20aggregated%20into%20the%0Apoint%20cloud%20using%20geometric%20information.%20Fun3DU%20is%20training-free%2C%20relying%0Aentirely%20on%20pre-trained%20models.%20We%20evaluate%20Fun3DU%20on%20SceneFun3D%2C%20the%20most%0Arecent%20and%20only%20dataset%20to%20benchmark%20this%20task%2C%20which%20comprises%20over%203000%20task%0Adescriptions%20on%20230%20scenes.%20Our%20method%20significantly%20outperforms%0Astate-of-the-art%20open-vocabulary%203D%20segmentation%20approaches.%20Project%20page%3A%0Ahttps%3A//jcorsetti.github.io/fun3du%0A%0ALink%3A%20http%3A//arxiv.org/abs/2411.16310v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DFunctionality%2520understanding%2520and%2520segmentation%2520in%25203D%2520scenes%26entry.906535625%3DJaime%2520Corsetti%2520and%2520Francesco%2520Giuliari%2520and%2520Alice%2520Fasoli%2520and%2520Davide%2520Boscaini%2520and%2520Fabio%2520Poiesi%26entry.1292438233%3D%2520%2520Understanding%2520functionalities%2520in%25203D%2520scenes%2520involves%2520interpreting%2520natural%250Alanguage%2520descriptions%2520to%2520locate%2520functional%2520interactive%2520objects%252C%2520such%2520as%2520handles%250Aand%2520buttons%252C%2520in%2520a%25203D%2520environment.%2520Functionality%2520understanding%2520is%2520highly%250Achallenging%252C%2520as%2520it%2520requires%2520both%2520world%2520knowledge%2520to%2520interpret%2520language%2520and%250Aspatial%2520perception%2520to%2520identify%2520fine-grained%2520objects.%2520For%2520example%252C%2520given%2520a%2520task%250Alike%2520%2527turn%2520on%2520the%2520ceiling%2520light%2527%252C%2520an%2520embodied%2520AI%2520agent%2520must%2520infer%2520that%2520it%2520needs%250Ato%2520locate%2520the%2520light%2520switch%252C%2520even%2520though%2520the%2520switch%2520is%2520not%2520explicitly%2520mentioned%250Ain%2520the%2520task%2520description.%2520To%2520date%252C%2520no%2520dedicated%2520methods%2520have%2520been%2520developed%2520for%250Athis%2520problem.%2520In%2520this%2520paper%252C%2520we%2520introduce%2520Fun3DU%252C%2520the%2520first%2520approach%2520designed%250Afor%2520functionality%2520understanding%2520in%25203D%2520scenes.%2520Fun3DU%2520uses%2520a%2520language%2520model%2520to%250Aparse%2520the%2520task%2520description%2520through%2520Chain-of-Thought%2520reasoning%2520in%2520order%2520to%250Aidentify%2520the%2520object%2520of%2520interest.%2520The%2520identified%2520object%2520is%2520segmented%2520across%250Amultiple%2520views%2520of%2520the%2520captured%2520scene%2520by%2520using%2520a%2520vision%2520and%2520language%2520model.%2520The%250Asegmentation%2520results%2520from%2520each%2520view%2520are%2520lifted%2520in%25203D%2520and%2520aggregated%2520into%2520the%250Apoint%2520cloud%2520using%2520geometric%2520information.%2520Fun3DU%2520is%2520training-free%252C%2520relying%250Aentirely%2520on%2520pre-trained%2520models.%2520We%2520evaluate%2520Fun3DU%2520on%2520SceneFun3D%252C%2520the%2520most%250Arecent%2520and%2520only%2520dataset%2520to%2520benchmark%2520this%2520task%252C%2520which%2520comprises%2520over%25203000%2520task%250Adescriptions%2520on%2520230%2520scenes.%2520Our%2520method%2520significantly%2520outperforms%250Astate-of-the-art%2520open-vocabulary%25203D%2520segmentation%2520approaches.%2520Project%2520page%253A%250Ahttps%253A//jcorsetti.github.io/fun3du%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2411.16310v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Functionality%20understanding%20and%20segmentation%20in%203D%20scenes&entry.906535625=Jaime%20Corsetti%20and%20Francesco%20Giuliari%20and%20Alice%20Fasoli%20and%20Davide%20Boscaini%20and%20Fabio%20Poiesi&entry.1292438233=%20%20Understanding%20functionalities%20in%203D%20scenes%20involves%20interpreting%20natural%0Alanguage%20descriptions%20to%20locate%20functional%20interactive%20objects%2C%20such%20as%20handles%0Aand%20buttons%2C%20in%20a%203D%20environment.%20Functionality%20understanding%20is%20highly%0Achallenging%2C%20as%20it%20requires%20both%20world%20knowledge%20to%20interpret%20language%20and%0Aspatial%20perception%20to%20identify%20fine-grained%20objects.%20For%20example%2C%20given%20a%20task%0Alike%20%27turn%20on%20the%20ceiling%20light%27%2C%20an%20embodied%20AI%20agent%20must%20infer%20that%20it%20needs%0Ato%20locate%20the%20light%20switch%2C%20even%20though%20the%20switch%20is%20not%20explicitly%20mentioned%0Ain%20the%20task%20description.%20To%20date%2C%20no%20dedicated%20methods%20have%20been%20developed%20for%0Athis%20problem.%20In%20this%20paper%2C%20we%20introduce%20Fun3DU%2C%20the%20first%20approach%20designed%0Afor%20functionality%20understanding%20in%203D%20scenes.%20Fun3DU%20uses%20a%20language%20model%20to%0Aparse%20the%20task%20description%20through%20Chain-of-Thought%20reasoning%20in%20order%20to%0Aidentify%20the%20object%20of%20interest.%20The%20identified%20object%20is%20segmented%20across%0Amultiple%20views%20of%20the%20captured%20scene%20by%20using%20a%20vision%20and%20language%20model.%20The%0Asegmentation%20results%20from%20each%20view%20are%20lifted%20in%203D%20and%20aggregated%20into%20the%0Apoint%20cloud%20using%20geometric%20information.%20Fun3DU%20is%20training-free%2C%20relying%0Aentirely%20on%20pre-trained%20models.%20We%20evaluate%20Fun3DU%20on%20SceneFun3D%2C%20the%20most%0Arecent%20and%20only%20dataset%20to%20benchmark%20this%20task%2C%20which%20comprises%20over%203000%20task%0Adescriptions%20on%20230%20scenes.%20Our%20method%20significantly%20outperforms%0Astate-of-the-art%20open-vocabulary%203D%20segmentation%20approaches.%20Project%20page%3A%0Ahttps%3A//jcorsetti.github.io/fun3du%0A&entry.1838667208=http%3A//arxiv.org/abs/2411.16310v2&entry.124074799=Read"},
{"title": "StableAnimator: High-Quality Identity-Preserving Human Image Animation", "author": "Shuyuan Tu and Zhen Xing and Xintong Han and Zhi-Qi Cheng and Qi Dai and Chong Luo and Zuxuan Wu", "abstract": "  Current diffusion models for human image animation struggle to ensure\nidentity (ID) consistency. This paper presents StableAnimator, the first\nend-to-end ID-preserving video diffusion framework, which synthesizes\nhigh-quality videos without any post-processing, conditioned on a reference\nimage and a sequence of poses. Building upon a video diffusion model,\nStableAnimator contains carefully designed modules for both training and\ninference striving for identity consistency. In particular, StableAnimator\nbegins by computing image and face embeddings with off-the-shelf extractors,\nrespectively and face embeddings are further refined by interacting with image\nembeddings using a global content-aware Face Encoder. Then, StableAnimator\nintroduces a novel distribution-aware ID Adapter that prevents interference\ncaused by temporal layers while preserving ID via alignment. During inference,\nwe propose a novel Hamilton-Jacobi-Bellman (HJB) equation-based optimization to\nfurther enhance the face quality. We demonstrate that solving the HJB equation\ncan be integrated into the diffusion denoising process, and the resulting\nsolution constrains the denoising path and thus benefits ID preservation.\nExperiments on multiple benchmarks show the effectiveness of StableAnimator\nboth qualitatively and quantitatively.\n", "link": "http://arxiv.org/abs/2411.17697v1", "date": "2024-11-26", "relevancy": 3.2226, "topK": [{"title": "Magic-Me: Identity-Specific Video Customized Diffusion", "link": "http://arxiv.org/abs/2402.09368v1", "similarity": 0.676}, {"title": "PoseAnimate: Zero-shot high fidelity pose controllable character\n  animation", "link": "http://arxiv.org/abs/2404.13680v2", "similarity": 0.6392}, {"title": "Tuning-Free Noise Rectification for High Fidelity Image-to-Video\n  Generation", "link": "http://arxiv.org/abs/2403.02827v1", "similarity": 0.6183}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20StableAnimator%3A%20High-Quality%20Identity-Preserving%20Human%20Image%20Animation&body=Title%3A%20StableAnimator%3A%20High-Quality%20Identity-Preserving%20Human%20Image%20Animation%0AAuthor%3A%20Shuyuan%20Tu%20and%20Zhen%20Xing%20and%20Xintong%20Han%20and%20Zhi-Qi%20Cheng%20and%20Qi%20Dai%20and%20Chong%20Luo%20and%20Zuxuan%20Wu%0AAbstract%3A%20%20%20Current%20diffusion%20models%20for%20human%20image%20animation%20struggle%20to%20ensure%0Aidentity%20%28ID%29%20consistency.%20This%20paper%20presents%20StableAnimator%2C%20the%20first%0Aend-to-end%20ID-preserving%20video%20diffusion%20framework%2C%20which%20synthesizes%0Ahigh-quality%20videos%20without%20any%20post-processing%2C%20conditioned%20on%20a%20reference%0Aimage%20and%20a%20sequence%20of%20poses.%20Building%20upon%20a%20video%20diffusion%20model%2C%0AStableAnimator%20contains%20carefully%20designed%20modules%20for%20both%20training%20and%0Ainference%20striving%20for%20identity%20consistency.%20In%20particular%2C%20StableAnimator%0Abegins%20by%20computing%20image%20and%20face%20embeddings%20with%20off-the-shelf%20extractors%2C%0Arespectively%20and%20face%20embeddings%20are%20further%20refined%20by%20interacting%20with%20image%0Aembeddings%20using%20a%20global%20content-aware%20Face%20Encoder.%20Then%2C%20StableAnimator%0Aintroduces%20a%20novel%20distribution-aware%20ID%20Adapter%20that%20prevents%20interference%0Acaused%20by%20temporal%20layers%20while%20preserving%20ID%20via%20alignment.%20During%20inference%2C%0Awe%20propose%20a%20novel%20Hamilton-Jacobi-Bellman%20%28HJB%29%20equation-based%20optimization%20to%0Afurther%20enhance%20the%20face%20quality.%20We%20demonstrate%20that%20solving%20the%20HJB%20equation%0Acan%20be%20integrated%20into%20the%20diffusion%20denoising%20process%2C%20and%20the%20resulting%0Asolution%20constrains%20the%20denoising%20path%20and%20thus%20benefits%20ID%20preservation.%0AExperiments%20on%20multiple%20benchmarks%20show%20the%20effectiveness%20of%20StableAnimator%0Aboth%20qualitatively%20and%20quantitatively.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2411.17697v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DStableAnimator%253A%2520High-Quality%2520Identity-Preserving%2520Human%2520Image%2520Animation%26entry.906535625%3DShuyuan%2520Tu%2520and%2520Zhen%2520Xing%2520and%2520Xintong%2520Han%2520and%2520Zhi-Qi%2520Cheng%2520and%2520Qi%2520Dai%2520and%2520Chong%2520Luo%2520and%2520Zuxuan%2520Wu%26entry.1292438233%3D%2520%2520Current%2520diffusion%2520models%2520for%2520human%2520image%2520animation%2520struggle%2520to%2520ensure%250Aidentity%2520%2528ID%2529%2520consistency.%2520This%2520paper%2520presents%2520StableAnimator%252C%2520the%2520first%250Aend-to-end%2520ID-preserving%2520video%2520diffusion%2520framework%252C%2520which%2520synthesizes%250Ahigh-quality%2520videos%2520without%2520any%2520post-processing%252C%2520conditioned%2520on%2520a%2520reference%250Aimage%2520and%2520a%2520sequence%2520of%2520poses.%2520Building%2520upon%2520a%2520video%2520diffusion%2520model%252C%250AStableAnimator%2520contains%2520carefully%2520designed%2520modules%2520for%2520both%2520training%2520and%250Ainference%2520striving%2520for%2520identity%2520consistency.%2520In%2520particular%252C%2520StableAnimator%250Abegins%2520by%2520computing%2520image%2520and%2520face%2520embeddings%2520with%2520off-the-shelf%2520extractors%252C%250Arespectively%2520and%2520face%2520embeddings%2520are%2520further%2520refined%2520by%2520interacting%2520with%2520image%250Aembeddings%2520using%2520a%2520global%2520content-aware%2520Face%2520Encoder.%2520Then%252C%2520StableAnimator%250Aintroduces%2520a%2520novel%2520distribution-aware%2520ID%2520Adapter%2520that%2520prevents%2520interference%250Acaused%2520by%2520temporal%2520layers%2520while%2520preserving%2520ID%2520via%2520alignment.%2520During%2520inference%252C%250Awe%2520propose%2520a%2520novel%2520Hamilton-Jacobi-Bellman%2520%2528HJB%2529%2520equation-based%2520optimization%2520to%250Afurther%2520enhance%2520the%2520face%2520quality.%2520We%2520demonstrate%2520that%2520solving%2520the%2520HJB%2520equation%250Acan%2520be%2520integrated%2520into%2520the%2520diffusion%2520denoising%2520process%252C%2520and%2520the%2520resulting%250Asolution%2520constrains%2520the%2520denoising%2520path%2520and%2520thus%2520benefits%2520ID%2520preservation.%250AExperiments%2520on%2520multiple%2520benchmarks%2520show%2520the%2520effectiveness%2520of%2520StableAnimator%250Aboth%2520qualitatively%2520and%2520quantitatively.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2411.17697v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=StableAnimator%3A%20High-Quality%20Identity-Preserving%20Human%20Image%20Animation&entry.906535625=Shuyuan%20Tu%20and%20Zhen%20Xing%20and%20Xintong%20Han%20and%20Zhi-Qi%20Cheng%20and%20Qi%20Dai%20and%20Chong%20Luo%20and%20Zuxuan%20Wu&entry.1292438233=%20%20Current%20diffusion%20models%20for%20human%20image%20animation%20struggle%20to%20ensure%0Aidentity%20%28ID%29%20consistency.%20This%20paper%20presents%20StableAnimator%2C%20the%20first%0Aend-to-end%20ID-preserving%20video%20diffusion%20framework%2C%20which%20synthesizes%0Ahigh-quality%20videos%20without%20any%20post-processing%2C%20conditioned%20on%20a%20reference%0Aimage%20and%20a%20sequence%20of%20poses.%20Building%20upon%20a%20video%20diffusion%20model%2C%0AStableAnimator%20contains%20carefully%20designed%20modules%20for%20both%20training%20and%0Ainference%20striving%20for%20identity%20consistency.%20In%20particular%2C%20StableAnimator%0Abegins%20by%20computing%20image%20and%20face%20embeddings%20with%20off-the-shelf%20extractors%2C%0Arespectively%20and%20face%20embeddings%20are%20further%20refined%20by%20interacting%20with%20image%0Aembeddings%20using%20a%20global%20content-aware%20Face%20Encoder.%20Then%2C%20StableAnimator%0Aintroduces%20a%20novel%20distribution-aware%20ID%20Adapter%20that%20prevents%20interference%0Acaused%20by%20temporal%20layers%20while%20preserving%20ID%20via%20alignment.%20During%20inference%2C%0Awe%20propose%20a%20novel%20Hamilton-Jacobi-Bellman%20%28HJB%29%20equation-based%20optimization%20to%0Afurther%20enhance%20the%20face%20quality.%20We%20demonstrate%20that%20solving%20the%20HJB%20equation%0Acan%20be%20integrated%20into%20the%20diffusion%20denoising%20process%2C%20and%20the%20resulting%0Asolution%20constrains%20the%20denoising%20path%20and%20thus%20benefits%20ID%20preservation.%0AExperiments%20on%20multiple%20benchmarks%20show%20the%20effectiveness%20of%20StableAnimator%0Aboth%20qualitatively%20and%20quantitatively.%0A&entry.1838667208=http%3A//arxiv.org/abs/2411.17697v1&entry.124074799=Read"},
{"title": "LLM2CLIP: Powerful Language Model Unlocks Richer Visual Representation", "author": "Weiquan Huang and Aoqi Wu and Yifan Yang and Xufang Luo and Yuqing Yang and Liang Hu and Qi Dai and Xiyang Dai and Dongdong Chen and Chong Luo and Lili Qiu", "abstract": "  CLIP is a foundational multimodal model that aligns image and text features\ninto a shared space using contrastive learning on large-scale image-text pairs.\nIts strength lies in leveraging natural language as a rich supervisory signal.\nWith the rapid progress of large language models (LLMs), we explore their\npotential to further enhance CLIP's multimodal representation learning. This\nwork introduces a fine-tuning approach that integrates LLMs with the pretrained\nCLIP visual encoder, leveraging LLMs' advanced text understanding and\nopen-world knowledge to improve CLIP's ability to process long and complex\ncaptions. To address the challenge of LLMs' autoregressive nature, we propose a\ncaption-to-caption contrastive learning framework to enhance the discriminative\npower of their outputs. Our method achieves substantial performance gains on\nvarious downstream tasks, demonstrating the effectiveness of combining LLMs\nwith CLIP for enhanced multimodal learning.\n", "link": "http://arxiv.org/abs/2411.04997v3", "date": "2024-11-26", "relevancy": 3.0653, "topK": [{"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.6475}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5958}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5958}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20LLM2CLIP%3A%20Powerful%20Language%20Model%20Unlocks%20Richer%20Visual%20Representation&body=Title%3A%20LLM2CLIP%3A%20Powerful%20Language%20Model%20Unlocks%20Richer%20Visual%20Representation%0AAuthor%3A%20Weiquan%20Huang%20and%20Aoqi%20Wu%20and%20Yifan%20Yang%20and%20Xufang%20Luo%20and%20Yuqing%20Yang%20and%20Liang%20Hu%20and%20Qi%20Dai%20and%20Xiyang%20Dai%20and%20Dongdong%20Chen%20and%20Chong%20Luo%20and%20Lili%20Qiu%0AAbstract%3A%20%20%20CLIP%20is%20a%20foundational%20multimodal%20model%20that%20aligns%20image%20and%20text%20features%0Ainto%20a%20shared%20space%20using%20contrastive%20learning%20on%20large-scale%20image-text%20pairs.%0AIts%20strength%20lies%20in%20leveraging%20natural%20language%20as%20a%20rich%20supervisory%20signal.%0AWith%20the%20rapid%20progress%20of%20large%20language%20models%20%28LLMs%29%2C%20we%20explore%20their%0Apotential%20to%20further%20enhance%20CLIP%27s%20multimodal%20representation%20learning.%20This%0Awork%20introduces%20a%20fine-tuning%20approach%20that%20integrates%20LLMs%20with%20the%20pretrained%0ACLIP%20visual%20encoder%2C%20leveraging%20LLMs%27%20advanced%20text%20understanding%20and%0Aopen-world%20knowledge%20to%20improve%20CLIP%27s%20ability%20to%20process%20long%20and%20complex%0Acaptions.%20To%20address%20the%20challenge%20of%20LLMs%27%20autoregressive%20nature%2C%20we%20propose%20a%0Acaption-to-caption%20contrastive%20learning%20framework%20to%20enhance%20the%20discriminative%0Apower%20of%20their%20outputs.%20Our%20method%20achieves%20substantial%20performance%20gains%20on%0Avarious%20downstream%20tasks%2C%20demonstrating%20the%20effectiveness%20of%20combining%20LLMs%0Awith%20CLIP%20for%20enhanced%20multimodal%20learning.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2411.04997v3%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DLLM2CLIP%253A%2520Powerful%2520Language%2520Model%2520Unlocks%2520Richer%2520Visual%2520Representation%26entry.906535625%3DWeiquan%2520Huang%2520and%2520Aoqi%2520Wu%2520and%2520Yifan%2520Yang%2520and%2520Xufang%2520Luo%2520and%2520Yuqing%2520Yang%2520and%2520Liang%2520Hu%2520and%2520Qi%2520Dai%2520and%2520Xiyang%2520Dai%2520and%2520Dongdong%2520Chen%2520and%2520Chong%2520Luo%2520and%2520Lili%2520Qiu%26entry.1292438233%3D%2520%2520CLIP%2520is%2520a%2520foundational%2520multimodal%2520model%2520that%2520aligns%2520image%2520and%2520text%2520features%250Ainto%2520a%2520shared%2520space%2520using%2520contrastive%2520learning%2520on%2520large-scale%2520image-text%2520pairs.%250AIts%2520strength%2520lies%2520in%2520leveraging%2520natural%2520language%2520as%2520a%2520rich%2520supervisory%2520signal.%250AWith%2520the%2520rapid%2520progress%2520of%2520large%2520language%2520models%2520%2528LLMs%2529%252C%2520we%2520explore%2520their%250Apotential%2520to%2520further%2520enhance%2520CLIP%2527s%2520multimodal%2520representation%2520learning.%2520This%250Awork%2520introduces%2520a%2520fine-tuning%2520approach%2520that%2520integrates%2520LLMs%2520with%2520the%2520pretrained%250ACLIP%2520visual%2520encoder%252C%2520leveraging%2520LLMs%2527%2520advanced%2520text%2520understanding%2520and%250Aopen-world%2520knowledge%2520to%2520improve%2520CLIP%2527s%2520ability%2520to%2520process%2520long%2520and%2520complex%250Acaptions.%2520To%2520address%2520the%2520challenge%2520of%2520LLMs%2527%2520autoregressive%2520nature%252C%2520we%2520propose%2520a%250Acaption-to-caption%2520contrastive%2520learning%2520framework%2520to%2520enhance%2520the%2520discriminative%250Apower%2520of%2520their%2520outputs.%2520Our%2520method%2520achieves%2520substantial%2520performance%2520gains%2520on%250Avarious%2520downstream%2520tasks%252C%2520demonstrating%2520the%2520effectiveness%2520of%2520combining%2520LLMs%250Awith%2520CLIP%2520for%2520enhanced%2520multimodal%2520learning.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2411.04997v3%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=LLM2CLIP%3A%20Powerful%20Language%20Model%20Unlocks%20Richer%20Visual%20Representation&entry.906535625=Weiquan%20Huang%20and%20Aoqi%20Wu%20and%20Yifan%20Yang%20and%20Xufang%20Luo%20and%20Yuqing%20Yang%20and%20Liang%20Hu%20and%20Qi%20Dai%20and%20Xiyang%20Dai%20and%20Dongdong%20Chen%20and%20Chong%20Luo%20and%20Lili%20Qiu&entry.1292438233=%20%20CLIP%20is%20a%20foundational%20multimodal%20model%20that%20aligns%20image%20and%20text%20features%0Ainto%20a%20shared%20space%20using%20contrastive%20learning%20on%20large-scale%20image-text%20pairs.%0AIts%20strength%20lies%20in%20leveraging%20natural%20language%20as%20a%20rich%20supervisory%20signal.%0AWith%20the%20rapid%20progress%20of%20large%20language%20models%20%28LLMs%29%2C%20we%20explore%20their%0Apotential%20to%20further%20enhance%20CLIP%27s%20multimodal%20representation%20learning.%20This%0Awork%20introduces%20a%20fine-tuning%20approach%20that%20integrates%20LLMs%20with%20the%20pretrained%0ACLIP%20visual%20encoder%2C%20leveraging%20LLMs%27%20advanced%20text%20understanding%20and%0Aopen-world%20knowledge%20to%20improve%20CLIP%27s%20ability%20to%20process%20long%20and%20complex%0Acaptions.%20To%20address%20the%20challenge%20of%20LLMs%27%20autoregressive%20nature%2C%20we%20propose%20a%0Acaption-to-caption%20contrastive%20learning%20framework%20to%20enhance%20the%20discriminative%0Apower%20of%20their%20outputs.%20Our%20method%20achieves%20substantial%20performance%20gains%20on%0Avarious%20downstream%20tasks%2C%20demonstrating%20the%20effectiveness%20of%20combining%20LLMs%0Awith%20CLIP%20for%20enhanced%20multimodal%20learning.%0A&entry.1838667208=http%3A//arxiv.org/abs/2411.04997v3&entry.124074799=Read"},
{"title": "DRiVE: Diffusion-based Rigging Empowers Generation of Versatile and\n  Expressive Characters", "author": "Mingze Sun and Junhao Chen and Junting Dong and Yurun Chen and Xinyu Jiang and Shiwei Mao and Puhua Jiang and Jingbo Wang and Bo Dai and Ruqi Huang", "abstract": "  Recent advances in generative models have enabled high-quality 3D character\nreconstruction from multi-modal. However, animating these generated characters\nremains a challenging task, especially for complex elements like garments and\nhair, due to the lack of large-scale datasets and effective rigging methods. To\naddress this gap, we curate AnimeRig, a large-scale dataset with detailed\nskeleton and skinning annotations. Building upon this, we propose DRiVE, a\nnovel framework for generating and rigging 3D human characters with intricate\nstructures. Unlike existing methods, DRiVE utilizes a 3D Gaussian\nrepresentation, facilitating efficient animation and high-quality rendering. We\nfurther introduce GSDiff, a 3D Gaussian-based diffusion module that predicts\njoint positions as spatial distributions, overcoming the limitations of\nregression-based approaches. Extensive experiments demonstrate that DRiVE\nachieves precise rigging results, enabling realistic dynamics for clothing and\nhair, and surpassing previous methods in both quality and versatility. The code\nand dataset will be made public for academic use upon acceptance.\n", "link": "http://arxiv.org/abs/2411.17423v1", "date": "2024-11-26", "relevancy": 2.9398, "topK": [{"title": "DressCode: Autoregressively Sewing and Generating Garments from Text\n  Guidance", "link": "http://arxiv.org/abs/2401.16465v3", "similarity": 0.5993}, {"title": "FabricDiffusion: High-Fidelity Texture Transfer for 3D Garments\n  Generation from In-The-Wild Clothing Images", "link": "http://arxiv.org/abs/2410.01801v1", "similarity": 0.5918}, {"title": "UGG: Unified Generative Grasping", "link": "https://arxiv.org/abs/2311.16917", "similarity": 0.5728}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20DRiVE%3A%20Diffusion-based%20Rigging%20Empowers%20Generation%20of%20Versatile%20and%0A%20%20Expressive%20Characters&body=Title%3A%20DRiVE%3A%20Diffusion-based%20Rigging%20Empowers%20Generation%20of%20Versatile%20and%0A%20%20Expressive%20Characters%0AAuthor%3A%20Mingze%20Sun%20and%20Junhao%20Chen%20and%20Junting%20Dong%20and%20Yurun%20Chen%20and%20Xinyu%20Jiang%20and%20Shiwei%20Mao%20and%20Puhua%20Jiang%20and%20Jingbo%20Wang%20and%20Bo%20Dai%20and%20Ruqi%20Huang%0AAbstract%3A%20%20%20Recent%20advances%20in%20generative%20models%20have%20enabled%20high-quality%203D%20character%0Areconstruction%20from%20multi-modal.%20However%2C%20animating%20these%20generated%20characters%0Aremains%20a%20challenging%20task%2C%20especially%20for%20complex%20elements%20like%20garments%20and%0Ahair%2C%20due%20to%20the%20lack%20of%20large-scale%20datasets%20and%20effective%20rigging%20methods.%20To%0Aaddress%20this%20gap%2C%20we%20curate%20AnimeRig%2C%20a%20large-scale%20dataset%20with%20detailed%0Askeleton%20and%20skinning%20annotations.%20Building%20upon%20this%2C%20we%20propose%20DRiVE%2C%20a%0Anovel%20framework%20for%20generating%20and%20rigging%203D%20human%20characters%20with%20intricate%0Astructures.%20Unlike%20existing%20methods%2C%20DRiVE%20utilizes%20a%203D%20Gaussian%0Arepresentation%2C%20facilitating%20efficient%20animation%20and%20high-quality%20rendering.%20We%0Afurther%20introduce%20GSDiff%2C%20a%203D%20Gaussian-based%20diffusion%20module%20that%20predicts%0Ajoint%20positions%20as%20spatial%20distributions%2C%20overcoming%20the%20limitations%20of%0Aregression-based%20approaches.%20Extensive%20experiments%20demonstrate%20that%20DRiVE%0Aachieves%20precise%20rigging%20results%2C%20enabling%20realistic%20dynamics%20for%20clothing%20and%0Ahair%2C%20and%20surpassing%20previous%20methods%20in%20both%20quality%20and%20versatility.%20The%20code%0Aand%20dataset%20will%20be%20made%20public%20for%20academic%20use%20upon%20acceptance.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2411.17423v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DDRiVE%253A%2520Diffusion-based%2520Rigging%2520Empowers%2520Generation%2520of%2520Versatile%2520and%250A%2520%2520Expressive%2520Characters%26entry.906535625%3DMingze%2520Sun%2520and%2520Junhao%2520Chen%2520and%2520Junting%2520Dong%2520and%2520Yurun%2520Chen%2520and%2520Xinyu%2520Jiang%2520and%2520Shiwei%2520Mao%2520and%2520Puhua%2520Jiang%2520and%2520Jingbo%2520Wang%2520and%2520Bo%2520Dai%2520and%2520Ruqi%2520Huang%26entry.1292438233%3D%2520%2520Recent%2520advances%2520in%2520generative%2520models%2520have%2520enabled%2520high-quality%25203D%2520character%250Areconstruction%2520from%2520multi-modal.%2520However%252C%2520animating%2520these%2520generated%2520characters%250Aremains%2520a%2520challenging%2520task%252C%2520especially%2520for%2520complex%2520elements%2520like%2520garments%2520and%250Ahair%252C%2520due%2520to%2520the%2520lack%2520of%2520large-scale%2520datasets%2520and%2520effective%2520rigging%2520methods.%2520To%250Aaddress%2520this%2520gap%252C%2520we%2520curate%2520AnimeRig%252C%2520a%2520large-scale%2520dataset%2520with%2520detailed%250Askeleton%2520and%2520skinning%2520annotations.%2520Building%2520upon%2520this%252C%2520we%2520propose%2520DRiVE%252C%2520a%250Anovel%2520framework%2520for%2520generating%2520and%2520rigging%25203D%2520human%2520characters%2520with%2520intricate%250Astructures.%2520Unlike%2520existing%2520methods%252C%2520DRiVE%2520utilizes%2520a%25203D%2520Gaussian%250Arepresentation%252C%2520facilitating%2520efficient%2520animation%2520and%2520high-quality%2520rendering.%2520We%250Afurther%2520introduce%2520GSDiff%252C%2520a%25203D%2520Gaussian-based%2520diffusion%2520module%2520that%2520predicts%250Ajoint%2520positions%2520as%2520spatial%2520distributions%252C%2520overcoming%2520the%2520limitations%2520of%250Aregression-based%2520approaches.%2520Extensive%2520experiments%2520demonstrate%2520that%2520DRiVE%250Aachieves%2520precise%2520rigging%2520results%252C%2520enabling%2520realistic%2520dynamics%2520for%2520clothing%2520and%250Ahair%252C%2520and%2520surpassing%2520previous%2520methods%2520in%2520both%2520quality%2520and%2520versatility.%2520The%2520code%250Aand%2520dataset%2520will%2520be%2520made%2520public%2520for%2520academic%2520use%2520upon%2520acceptance.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2411.17423v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=DRiVE%3A%20Diffusion-based%20Rigging%20Empowers%20Generation%20of%20Versatile%20and%0A%20%20Expressive%20Characters&entry.906535625=Mingze%20Sun%20and%20Junhao%20Chen%20and%20Junting%20Dong%20and%20Yurun%20Chen%20and%20Xinyu%20Jiang%20and%20Shiwei%20Mao%20and%20Puhua%20Jiang%20and%20Jingbo%20Wang%20and%20Bo%20Dai%20and%20Ruqi%20Huang&entry.1292438233=%20%20Recent%20advances%20in%20generative%20models%20have%20enabled%20high-quality%203D%20character%0Areconstruction%20from%20multi-modal.%20However%2C%20animating%20these%20generated%20characters%0Aremains%20a%20challenging%20task%2C%20especially%20for%20complex%20elements%20like%20garments%20and%0Ahair%2C%20due%20to%20the%20lack%20of%20large-scale%20datasets%20and%20effective%20rigging%20methods.%20To%0Aaddress%20this%20gap%2C%20we%20curate%20AnimeRig%2C%20a%20large-scale%20dataset%20with%20detailed%0Askeleton%20and%20skinning%20annotations.%20Building%20upon%20this%2C%20we%20propose%20DRiVE%2C%20a%0Anovel%20framework%20for%20generating%20and%20rigging%203D%20human%20characters%20with%20intricate%0Astructures.%20Unlike%20existing%20methods%2C%20DRiVE%20utilizes%20a%203D%20Gaussian%0Arepresentation%2C%20facilitating%20efficient%20animation%20and%20high-quality%20rendering.%20We%0Afurther%20introduce%20GSDiff%2C%20a%203D%20Gaussian-based%20diffusion%20module%20that%20predicts%0Ajoint%20positions%20as%20spatial%20distributions%2C%20overcoming%20the%20limitations%20of%0Aregression-based%20approaches.%20Extensive%20experiments%20demonstrate%20that%20DRiVE%0Aachieves%20precise%20rigging%20results%2C%20enabling%20realistic%20dynamics%20for%20clothing%20and%0Ahair%2C%20and%20surpassing%20previous%20methods%20in%20both%20quality%20and%20versatility.%20The%20code%0Aand%20dataset%20will%20be%20made%20public%20for%20academic%20use%20upon%20acceptance.%0A&entry.1838667208=http%3A//arxiv.org/abs/2411.17423v1&entry.124074799=Read"},
{"title": "GaNI: Global and Near Field Illumination Aware Neural Inverse Rendering", "author": "Jiaye Wu and Saeed Hadadan and Geng Lin and Matthias Zwicker and David Jacobs and Roni Sengupta", "abstract": "  In this paper, we present GaNI, a Global and Near-field Illumination-aware\nneural inverse rendering technique that can reconstruct geometry, albedo, and\nroughness parameters from images of a scene captured with co-located light and\ncamera. Existing inverse rendering techniques with co-located light-camera\nfocus on single objects only, without modeling global illumination and\nnear-field lighting more prominent in scenes with multiple objects. We\nintroduce a system that solves this problem in two stages; we first reconstruct\nthe geometry powered by neural volumetric rendering NeuS, followed by inverse\nneural radiosity that uses the previously predicted geometry to estimate albedo\nand roughness. However, such a naive combination fails and we propose multiple\ntechnical contributions that enable this two-stage approach. We observe that\nNeuS fails to handle near-field illumination and strong specular reflections\nfrom the flashlight in a scene. We propose to implicitly model the effects of\nnear-field illumination and introduce a surface angle loss function to handle\nspecular reflections. Similarly, we observe that invNeRad assumes constant\nillumination throughout the capture and cannot handle moving flashlights during\ncapture. We propose a light position-aware radiance cache network and\nadditional smoothness priors on roughness to reconstruct reflectance.\nExperimental evaluation on synthetic and real data shows that our method\noutperforms the existing co-located light-camera-based inverse rendering\ntechniques. Our approach produces significantly better reflectance and slightly\nbetter geometry than capture strategies that do not require a dark room.\n", "link": "http://arxiv.org/abs/2403.15651v3", "date": "2024-11-26", "relevancy": 2.9249, "topK": [{"title": "MiraGe: Editable 2D Images using Gaussian Splatting", "link": "http://arxiv.org/abs/2410.01521v1", "similarity": 0.6069}, {"title": "RTG-SLAM: Real-time 3D Reconstruction at Scale using Gaussian Splatting", "link": "http://arxiv.org/abs/2404.19706v1", "similarity": 0.5891}, {"title": "Dynamic Gaussians Mesh: Consistent Mesh Reconstruction from Monocular\n  Videos", "link": "http://arxiv.org/abs/2404.12379v2", "similarity": 0.559}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20GaNI%3A%20Global%20and%20Near%20Field%20Illumination%20Aware%20Neural%20Inverse%20Rendering&body=Title%3A%20GaNI%3A%20Global%20and%20Near%20Field%20Illumination%20Aware%20Neural%20Inverse%20Rendering%0AAuthor%3A%20Jiaye%20Wu%20and%20Saeed%20Hadadan%20and%20Geng%20Lin%20and%20Matthias%20Zwicker%20and%20David%20Jacobs%20and%20Roni%20Sengupta%0AAbstract%3A%20%20%20In%20this%20paper%2C%20we%20present%20GaNI%2C%20a%20Global%20and%20Near-field%20Illumination-aware%0Aneural%20inverse%20rendering%20technique%20that%20can%20reconstruct%20geometry%2C%20albedo%2C%20and%0Aroughness%20parameters%20from%20images%20of%20a%20scene%20captured%20with%20co-located%20light%20and%0Acamera.%20Existing%20inverse%20rendering%20techniques%20with%20co-located%20light-camera%0Afocus%20on%20single%20objects%20only%2C%20without%20modeling%20global%20illumination%20and%0Anear-field%20lighting%20more%20prominent%20in%20scenes%20with%20multiple%20objects.%20We%0Aintroduce%20a%20system%20that%20solves%20this%20problem%20in%20two%20stages%3B%20we%20first%20reconstruct%0Athe%20geometry%20powered%20by%20neural%20volumetric%20rendering%20NeuS%2C%20followed%20by%20inverse%0Aneural%20radiosity%20that%20uses%20the%20previously%20predicted%20geometry%20to%20estimate%20albedo%0Aand%20roughness.%20However%2C%20such%20a%20naive%20combination%20fails%20and%20we%20propose%20multiple%0Atechnical%20contributions%20that%20enable%20this%20two-stage%20approach.%20We%20observe%20that%0ANeuS%20fails%20to%20handle%20near-field%20illumination%20and%20strong%20specular%20reflections%0Afrom%20the%20flashlight%20in%20a%20scene.%20We%20propose%20to%20implicitly%20model%20the%20effects%20of%0Anear-field%20illumination%20and%20introduce%20a%20surface%20angle%20loss%20function%20to%20handle%0Aspecular%20reflections.%20Similarly%2C%20we%20observe%20that%20invNeRad%20assumes%20constant%0Aillumination%20throughout%20the%20capture%20and%20cannot%20handle%20moving%20flashlights%20during%0Acapture.%20We%20propose%20a%20light%20position-aware%20radiance%20cache%20network%20and%0Aadditional%20smoothness%20priors%20on%20roughness%20to%20reconstruct%20reflectance.%0AExperimental%20evaluation%20on%20synthetic%20and%20real%20data%20shows%20that%20our%20method%0Aoutperforms%20the%20existing%20co-located%20light-camera-based%20inverse%20rendering%0Atechniques.%20Our%20approach%20produces%20significantly%20better%20reflectance%20and%20slightly%0Abetter%20geometry%20than%20capture%20strategies%20that%20do%20not%20require%20a%20dark%20room.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2403.15651v3%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DGaNI%253A%2520Global%2520and%2520Near%2520Field%2520Illumination%2520Aware%2520Neural%2520Inverse%2520Rendering%26entry.906535625%3DJiaye%2520Wu%2520and%2520Saeed%2520Hadadan%2520and%2520Geng%2520Lin%2520and%2520Matthias%2520Zwicker%2520and%2520David%2520Jacobs%2520and%2520Roni%2520Sengupta%26entry.1292438233%3D%2520%2520In%2520this%2520paper%252C%2520we%2520present%2520GaNI%252C%2520a%2520Global%2520and%2520Near-field%2520Illumination-aware%250Aneural%2520inverse%2520rendering%2520technique%2520that%2520can%2520reconstruct%2520geometry%252C%2520albedo%252C%2520and%250Aroughness%2520parameters%2520from%2520images%2520of%2520a%2520scene%2520captured%2520with%2520co-located%2520light%2520and%250Acamera.%2520Existing%2520inverse%2520rendering%2520techniques%2520with%2520co-located%2520light-camera%250Afocus%2520on%2520single%2520objects%2520only%252C%2520without%2520modeling%2520global%2520illumination%2520and%250Anear-field%2520lighting%2520more%2520prominent%2520in%2520scenes%2520with%2520multiple%2520objects.%2520We%250Aintroduce%2520a%2520system%2520that%2520solves%2520this%2520problem%2520in%2520two%2520stages%253B%2520we%2520first%2520reconstruct%250Athe%2520geometry%2520powered%2520by%2520neural%2520volumetric%2520rendering%2520NeuS%252C%2520followed%2520by%2520inverse%250Aneural%2520radiosity%2520that%2520uses%2520the%2520previously%2520predicted%2520geometry%2520to%2520estimate%2520albedo%250Aand%2520roughness.%2520However%252C%2520such%2520a%2520naive%2520combination%2520fails%2520and%2520we%2520propose%2520multiple%250Atechnical%2520contributions%2520that%2520enable%2520this%2520two-stage%2520approach.%2520We%2520observe%2520that%250ANeuS%2520fails%2520to%2520handle%2520near-field%2520illumination%2520and%2520strong%2520specular%2520reflections%250Afrom%2520the%2520flashlight%2520in%2520a%2520scene.%2520We%2520propose%2520to%2520implicitly%2520model%2520the%2520effects%2520of%250Anear-field%2520illumination%2520and%2520introduce%2520a%2520surface%2520angle%2520loss%2520function%2520to%2520handle%250Aspecular%2520reflections.%2520Similarly%252C%2520we%2520observe%2520that%2520invNeRad%2520assumes%2520constant%250Aillumination%2520throughout%2520the%2520capture%2520and%2520cannot%2520handle%2520moving%2520flashlights%2520during%250Acapture.%2520We%2520propose%2520a%2520light%2520position-aware%2520radiance%2520cache%2520network%2520and%250Aadditional%2520smoothness%2520priors%2520on%2520roughness%2520to%2520reconstruct%2520reflectance.%250AExperimental%2520evaluation%2520on%2520synthetic%2520and%2520real%2520data%2520shows%2520that%2520our%2520method%250Aoutperforms%2520the%2520existing%2520co-located%2520light-camera-based%2520inverse%2520rendering%250Atechniques.%2520Our%2520approach%2520produces%2520significantly%2520better%2520reflectance%2520and%2520slightly%250Abetter%2520geometry%2520than%2520capture%2520strategies%2520that%2520do%2520not%2520require%2520a%2520dark%2520room.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2403.15651v3%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=GaNI%3A%20Global%20and%20Near%20Field%20Illumination%20Aware%20Neural%20Inverse%20Rendering&entry.906535625=Jiaye%20Wu%20and%20Saeed%20Hadadan%20and%20Geng%20Lin%20and%20Matthias%20Zwicker%20and%20David%20Jacobs%20and%20Roni%20Sengupta&entry.1292438233=%20%20In%20this%20paper%2C%20we%20present%20GaNI%2C%20a%20Global%20and%20Near-field%20Illumination-aware%0Aneural%20inverse%20rendering%20technique%20that%20can%20reconstruct%20geometry%2C%20albedo%2C%20and%0Aroughness%20parameters%20from%20images%20of%20a%20scene%20captured%20with%20co-located%20light%20and%0Acamera.%20Existing%20inverse%20rendering%20techniques%20with%20co-located%20light-camera%0Afocus%20on%20single%20objects%20only%2C%20without%20modeling%20global%20illumination%20and%0Anear-field%20lighting%20more%20prominent%20in%20scenes%20with%20multiple%20objects.%20We%0Aintroduce%20a%20system%20that%20solves%20this%20problem%20in%20two%20stages%3B%20we%20first%20reconstruct%0Athe%20geometry%20powered%20by%20neural%20volumetric%20rendering%20NeuS%2C%20followed%20by%20inverse%0Aneural%20radiosity%20that%20uses%20the%20previously%20predicted%20geometry%20to%20estimate%20albedo%0Aand%20roughness.%20However%2C%20such%20a%20naive%20combination%20fails%20and%20we%20propose%20multiple%0Atechnical%20contributions%20that%20enable%20this%20two-stage%20approach.%20We%20observe%20that%0ANeuS%20fails%20to%20handle%20near-field%20illumination%20and%20strong%20specular%20reflections%0Afrom%20the%20flashlight%20in%20a%20scene.%20We%20propose%20to%20implicitly%20model%20the%20effects%20of%0Anear-field%20illumination%20and%20introduce%20a%20surface%20angle%20loss%20function%20to%20handle%0Aspecular%20reflections.%20Similarly%2C%20we%20observe%20that%20invNeRad%20assumes%20constant%0Aillumination%20throughout%20the%20capture%20and%20cannot%20handle%20moving%20flashlights%20during%0Acapture.%20We%20propose%20a%20light%20position-aware%20radiance%20cache%20network%20and%0Aadditional%20smoothness%20priors%20on%20roughness%20to%20reconstruct%20reflectance.%0AExperimental%20evaluation%20on%20synthetic%20and%20real%20data%20shows%20that%20our%20method%0Aoutperforms%20the%20existing%20co-located%20light-camera-based%20inverse%20rendering%0Atechniques.%20Our%20approach%20produces%20significantly%20better%20reflectance%20and%20slightly%0Abetter%20geometry%20than%20capture%20strategies%20that%20do%20not%20require%20a%20dark%20room.%0A&entry.1838667208=http%3A//arxiv.org/abs/2403.15651v3&entry.124074799=Read"},
{"title": "Open-Vocabulary Segmentation with Semantic-Assisted Calibration", "author": "Yong Liu and Sule Bai and Guanbin Li and Yitong Wang and Yansong Tang", "abstract": "  This paper studies open-vocabulary segmentation (OVS) through calibrating\nin-vocabulary and domain-biased embedding space with generalized contextual\nprior of CLIP. As the core of open-vocabulary understanding, alignment of\nvisual content with the semantics of unbounded text has become the bottleneck\nof this field. To address this challenge, recent works propose to utilize CLIP\nas an additional classifier and aggregate model predictions with CLIP\nclassification results. Despite their remarkable progress, performance of OVS\nmethods in relevant scenarios is still unsatisfactory compared with supervised\ncounterparts. We attribute this to the in-vocabulary embedding and\ndomain-biased CLIP prediction. To this end, we present a Semantic-assisted\nCAlibration Network (SCAN). In SCAN, we incorporate generalized semantic prior\nof CLIP into proposal embedding to avoid collapsing on known categories.\nBesides, a contextual shift strategy is applied to mitigate the lack of global\ncontext and unnatural background noise. With above designs, SCAN achieves\nstate-of-the-art performance on all popular open-vocabulary segmentation\nbenchmarks. Furthermore, we also focus on the problem of existing evaluation\nsystem that ignores semantic duplication across categories, and propose a new\nmetric called Semantic-Guided IoU (SG-IoU).\n", "link": "http://arxiv.org/abs/2312.04089v2", "date": "2024-11-26", "relevancy": 2.9195, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5961}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5961}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5596}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Open-Vocabulary%20Segmentation%20with%20Semantic-Assisted%20Calibration&body=Title%3A%20Open-Vocabulary%20Segmentation%20with%20Semantic-Assisted%20Calibration%0AAuthor%3A%20Yong%20Liu%20and%20Sule%20Bai%20and%20Guanbin%20Li%20and%20Yitong%20Wang%20and%20Yansong%20Tang%0AAbstract%3A%20%20%20This%20paper%20studies%20open-vocabulary%20segmentation%20%28OVS%29%20through%20calibrating%0Ain-vocabulary%20and%20domain-biased%20embedding%20space%20with%20generalized%20contextual%0Aprior%20of%20CLIP.%20As%20the%20core%20of%20open-vocabulary%20understanding%2C%20alignment%20of%0Avisual%20content%20with%20the%20semantics%20of%20unbounded%20text%20has%20become%20the%20bottleneck%0Aof%20this%20field.%20To%20address%20this%20challenge%2C%20recent%20works%20propose%20to%20utilize%20CLIP%0Aas%20an%20additional%20classifier%20and%20aggregate%20model%20predictions%20with%20CLIP%0Aclassification%20results.%20Despite%20their%20remarkable%20progress%2C%20performance%20of%20OVS%0Amethods%20in%20relevant%20scenarios%20is%20still%20unsatisfactory%20compared%20with%20supervised%0Acounterparts.%20We%20attribute%20this%20to%20the%20in-vocabulary%20embedding%20and%0Adomain-biased%20CLIP%20prediction.%20To%20this%20end%2C%20we%20present%20a%20Semantic-assisted%0ACAlibration%20Network%20%28SCAN%29.%20In%20SCAN%2C%20we%20incorporate%20generalized%20semantic%20prior%0Aof%20CLIP%20into%20proposal%20embedding%20to%20avoid%20collapsing%20on%20known%20categories.%0ABesides%2C%20a%20contextual%20shift%20strategy%20is%20applied%20to%20mitigate%20the%20lack%20of%20global%0Acontext%20and%20unnatural%20background%20noise.%20With%20above%20designs%2C%20SCAN%20achieves%0Astate-of-the-art%20performance%20on%20all%20popular%20open-vocabulary%20segmentation%0Abenchmarks.%20Furthermore%2C%20we%20also%20focus%20on%20the%20problem%20of%20existing%20evaluation%0Asystem%20that%20ignores%20semantic%20duplication%20across%20categories%2C%20and%20propose%20a%20new%0Ametric%20called%20Semantic-Guided%20IoU%20%28SG-IoU%29.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2312.04089v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DOpen-Vocabulary%2520Segmentation%2520with%2520Semantic-Assisted%2520Calibration%26entry.906535625%3DYong%2520Liu%2520and%2520Sule%2520Bai%2520and%2520Guanbin%2520Li%2520and%2520Yitong%2520Wang%2520and%2520Yansong%2520Tang%26entry.1292438233%3D%2520%2520This%2520paper%2520studies%2520open-vocabulary%2520segmentation%2520%2528OVS%2529%2520through%2520calibrating%250Ain-vocabulary%2520and%2520domain-biased%2520embedding%2520space%2520with%2520generalized%2520contextual%250Aprior%2520of%2520CLIP.%2520As%2520the%2520core%2520of%2520open-vocabulary%2520understanding%252C%2520alignment%2520of%250Avisual%2520content%2520with%2520the%2520semantics%2520of%2520unbounded%2520text%2520has%2520become%2520the%2520bottleneck%250Aof%2520this%2520field.%2520To%2520address%2520this%2520challenge%252C%2520recent%2520works%2520propose%2520to%2520utilize%2520CLIP%250Aas%2520an%2520additional%2520classifier%2520and%2520aggregate%2520model%2520predictions%2520with%2520CLIP%250Aclassification%2520results.%2520Despite%2520their%2520remarkable%2520progress%252C%2520performance%2520of%2520OVS%250Amethods%2520in%2520relevant%2520scenarios%2520is%2520still%2520unsatisfactory%2520compared%2520with%2520supervised%250Acounterparts.%2520We%2520attribute%2520this%2520to%2520the%2520in-vocabulary%2520embedding%2520and%250Adomain-biased%2520CLIP%2520prediction.%2520To%2520this%2520end%252C%2520we%2520present%2520a%2520Semantic-assisted%250ACAlibration%2520Network%2520%2528SCAN%2529.%2520In%2520SCAN%252C%2520we%2520incorporate%2520generalized%2520semantic%2520prior%250Aof%2520CLIP%2520into%2520proposal%2520embedding%2520to%2520avoid%2520collapsing%2520on%2520known%2520categories.%250ABesides%252C%2520a%2520contextual%2520shift%2520strategy%2520is%2520applied%2520to%2520mitigate%2520the%2520lack%2520of%2520global%250Acontext%2520and%2520unnatural%2520background%2520noise.%2520With%2520above%2520designs%252C%2520SCAN%2520achieves%250Astate-of-the-art%2520performance%2520on%2520all%2520popular%2520open-vocabulary%2520segmentation%250Abenchmarks.%2520Furthermore%252C%2520we%2520also%2520focus%2520on%2520the%2520problem%2520of%2520existing%2520evaluation%250Asystem%2520that%2520ignores%2520semantic%2520duplication%2520across%2520categories%252C%2520and%2520propose%2520a%2520new%250Ametric%2520called%2520Semantic-Guided%2520IoU%2520%2528SG-IoU%2529.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2312.04089v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Open-Vocabulary%20Segmentation%20with%20Semantic-Assisted%20Calibration&entry.906535625=Yong%20Liu%20and%20Sule%20Bai%20and%20Guanbin%20Li%20and%20Yitong%20Wang%20and%20Yansong%20Tang&entry.1292438233=%20%20This%20paper%20studies%20open-vocabulary%20segmentation%20%28OVS%29%20through%20calibrating%0Ain-vocabulary%20and%20domain-biased%20embedding%20space%20with%20generalized%20contextual%0Aprior%20of%20CLIP.%20As%20the%20core%20of%20open-vocabulary%20understanding%2C%20alignment%20of%0Avisual%20content%20with%20the%20semantics%20of%20unbounded%20text%20has%20become%20the%20bottleneck%0Aof%20this%20field.%20To%20address%20this%20challenge%2C%20recent%20works%20propose%20to%20utilize%20CLIP%0Aas%20an%20additional%20classifier%20and%20aggregate%20model%20predictions%20with%20CLIP%0Aclassification%20results.%20Despite%20their%20remarkable%20progress%2C%20performance%20of%20OVS%0Amethods%20in%20relevant%20scenarios%20is%20still%20unsatisfactory%20compared%20with%20supervised%0Acounterparts.%20We%20attribute%20this%20to%20the%20in-vocabulary%20embedding%20and%0Adomain-biased%20CLIP%20prediction.%20To%20this%20end%2C%20we%20present%20a%20Semantic-assisted%0ACAlibration%20Network%20%28SCAN%29.%20In%20SCAN%2C%20we%20incorporate%20generalized%20semantic%20prior%0Aof%20CLIP%20into%20proposal%20embedding%20to%20avoid%20collapsing%20on%20known%20categories.%0ABesides%2C%20a%20contextual%20shift%20strategy%20is%20applied%20to%20mitigate%20the%20lack%20of%20global%0Acontext%20and%20unnatural%20background%20noise.%20With%20above%20designs%2C%20SCAN%20achieves%0Astate-of-the-art%20performance%20on%20all%20popular%20open-vocabulary%20segmentation%0Abenchmarks.%20Furthermore%2C%20we%20also%20focus%20on%20the%20problem%20of%20existing%20evaluation%0Asystem%20that%20ignores%20semantic%20duplication%20across%20categories%2C%20and%20propose%20a%20new%0Ametric%20called%20Semantic-Guided%20IoU%20%28SG-IoU%29.%0A&entry.1838667208=http%3A//arxiv.org/abs/2312.04089v2&entry.124074799=Read"},
{"title": "HyperSeg: Towards Universal Visual Segmentation with Large Language\n  Model", "author": "Cong Wei and Yujie Zhong and Haoxian Tan and Yong Liu and Zheng Zhao and Jie Hu and Yujiu Yang", "abstract": "  This paper aims to address universal segmentation for image and video\nperception with the strong reasoning ability empowered by Visual Large Language\nModels (VLLMs). Despite significant progress in current unified segmentation\nmethods, limitations in adaptation to both image and video scenarios, as well\nas the complex reasoning segmentation, make it difficult for them to handle\nvarious challenging instructions and achieve an accurate understanding of\nfine-grained vision-language correlations. We propose HyperSeg, the first\nVLLM-based universal segmentation model for pixel-level image and video\nperception, encompassing generic segmentation tasks and more complex reasoning\nperception tasks requiring powerful reasoning abilities and world knowledge.\nBesides, to fully leverage the recognition capabilities of VLLMs and the\nfine-grained visual information, HyperSeg incorporates hybrid entity\nrecognition and fine-grained visual perceiver modules for various segmentation\ntasks. Combined with the temporal adapter, HyperSeg achieves a comprehensive\nunderstanding of temporal information. Experimental results validate the\neffectiveness of our insights in resolving universal image and video\nsegmentation tasks, including the more complex reasoning perception tasks. Our\ncode is available.\n", "link": "http://arxiv.org/abs/2411.17606v1", "date": "2024-11-26", "relevancy": 2.9026, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5992}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5992}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5432}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20HyperSeg%3A%20Towards%20Universal%20Visual%20Segmentation%20with%20Large%20Language%0A%20%20Model&body=Title%3A%20HyperSeg%3A%20Towards%20Universal%20Visual%20Segmentation%20with%20Large%20Language%0A%20%20Model%0AAuthor%3A%20Cong%20Wei%20and%20Yujie%20Zhong%20and%20Haoxian%20Tan%20and%20Yong%20Liu%20and%20Zheng%20Zhao%20and%20Jie%20Hu%20and%20Yujiu%20Yang%0AAbstract%3A%20%20%20This%20paper%20aims%20to%20address%20universal%20segmentation%20for%20image%20and%20video%0Aperception%20with%20the%20strong%20reasoning%20ability%20empowered%20by%20Visual%20Large%20Language%0AModels%20%28VLLMs%29.%20Despite%20significant%20progress%20in%20current%20unified%20segmentation%0Amethods%2C%20limitations%20in%20adaptation%20to%20both%20image%20and%20video%20scenarios%2C%20as%20well%0Aas%20the%20complex%20reasoning%20segmentation%2C%20make%20it%20difficult%20for%20them%20to%20handle%0Avarious%20challenging%20instructions%20and%20achieve%20an%20accurate%20understanding%20of%0Afine-grained%20vision-language%20correlations.%20We%20propose%20HyperSeg%2C%20the%20first%0AVLLM-based%20universal%20segmentation%20model%20for%20pixel-level%20image%20and%20video%0Aperception%2C%20encompassing%20generic%20segmentation%20tasks%20and%20more%20complex%20reasoning%0Aperception%20tasks%20requiring%20powerful%20reasoning%20abilities%20and%20world%20knowledge.%0ABesides%2C%20to%20fully%20leverage%20the%20recognition%20capabilities%20of%20VLLMs%20and%20the%0Afine-grained%20visual%20information%2C%20HyperSeg%20incorporates%20hybrid%20entity%0Arecognition%20and%20fine-grained%20visual%20perceiver%20modules%20for%20various%20segmentation%0Atasks.%20Combined%20with%20the%20temporal%20adapter%2C%20HyperSeg%20achieves%20a%20comprehensive%0Aunderstanding%20of%20temporal%20information.%20Experimental%20results%20validate%20the%0Aeffectiveness%20of%20our%20insights%20in%20resolving%20universal%20image%20and%20video%0Asegmentation%20tasks%2C%20including%20the%20more%20complex%20reasoning%20perception%20tasks.%20Our%0Acode%20is%20available.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2411.17606v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DHyperSeg%253A%2520Towards%2520Universal%2520Visual%2520Segmentation%2520with%2520Large%2520Language%250A%2520%2520Model%26entry.906535625%3DCong%2520Wei%2520and%2520Yujie%2520Zhong%2520and%2520Haoxian%2520Tan%2520and%2520Yong%2520Liu%2520and%2520Zheng%2520Zhao%2520and%2520Jie%2520Hu%2520and%2520Yujiu%2520Yang%26entry.1292438233%3D%2520%2520This%2520paper%2520aims%2520to%2520address%2520universal%2520segmentation%2520for%2520image%2520and%2520video%250Aperception%2520with%2520the%2520strong%2520reasoning%2520ability%2520empowered%2520by%2520Visual%2520Large%2520Language%250AModels%2520%2528VLLMs%2529.%2520Despite%2520significant%2520progress%2520in%2520current%2520unified%2520segmentation%250Amethods%252C%2520limitations%2520in%2520adaptation%2520to%2520both%2520image%2520and%2520video%2520scenarios%252C%2520as%2520well%250Aas%2520the%2520complex%2520reasoning%2520segmentation%252C%2520make%2520it%2520difficult%2520for%2520them%2520to%2520handle%250Avarious%2520challenging%2520instructions%2520and%2520achieve%2520an%2520accurate%2520understanding%2520of%250Afine-grained%2520vision-language%2520correlations.%2520We%2520propose%2520HyperSeg%252C%2520the%2520first%250AVLLM-based%2520universal%2520segmentation%2520model%2520for%2520pixel-level%2520image%2520and%2520video%250Aperception%252C%2520encompassing%2520generic%2520segmentation%2520tasks%2520and%2520more%2520complex%2520reasoning%250Aperception%2520tasks%2520requiring%2520powerful%2520reasoning%2520abilities%2520and%2520world%2520knowledge.%250ABesides%252C%2520to%2520fully%2520leverage%2520the%2520recognition%2520capabilities%2520of%2520VLLMs%2520and%2520the%250Afine-grained%2520visual%2520information%252C%2520HyperSeg%2520incorporates%2520hybrid%2520entity%250Arecognition%2520and%2520fine-grained%2520visual%2520perceiver%2520modules%2520for%2520various%2520segmentation%250Atasks.%2520Combined%2520with%2520the%2520temporal%2520adapter%252C%2520HyperSeg%2520achieves%2520a%2520comprehensive%250Aunderstanding%2520of%2520temporal%2520information.%2520Experimental%2520results%2520validate%2520the%250Aeffectiveness%2520of%2520our%2520insights%2520in%2520resolving%2520universal%2520image%2520and%2520video%250Asegmentation%2520tasks%252C%2520including%2520the%2520more%2520complex%2520reasoning%2520perception%2520tasks.%2520Our%250Acode%2520is%2520available.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2411.17606v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=HyperSeg%3A%20Towards%20Universal%20Visual%20Segmentation%20with%20Large%20Language%0A%20%20Model&entry.906535625=Cong%20Wei%20and%20Yujie%20Zhong%20and%20Haoxian%20Tan%20and%20Yong%20Liu%20and%20Zheng%20Zhao%20and%20Jie%20Hu%20and%20Yujiu%20Yang&entry.1292438233=%20%20This%20paper%20aims%20to%20address%20universal%20segmentation%20for%20image%20and%20video%0Aperception%20with%20the%20strong%20reasoning%20ability%20empowered%20by%20Visual%20Large%20Language%0AModels%20%28VLLMs%29.%20Despite%20significant%20progress%20in%20current%20unified%20segmentation%0Amethods%2C%20limitations%20in%20adaptation%20to%20both%20image%20and%20video%20scenarios%2C%20as%20well%0Aas%20the%20complex%20reasoning%20segmentation%2C%20make%20it%20difficult%20for%20them%20to%20handle%0Avarious%20challenging%20instructions%20and%20achieve%20an%20accurate%20understanding%20of%0Afine-grained%20vision-language%20correlations.%20We%20propose%20HyperSeg%2C%20the%20first%0AVLLM-based%20universal%20segmentation%20model%20for%20pixel-level%20image%20and%20video%0Aperception%2C%20encompassing%20generic%20segmentation%20tasks%20and%20more%20complex%20reasoning%0Aperception%20tasks%20requiring%20powerful%20reasoning%20abilities%20and%20world%20knowledge.%0ABesides%2C%20to%20fully%20leverage%20the%20recognition%20capabilities%20of%20VLLMs%20and%20the%0Afine-grained%20visual%20information%2C%20HyperSeg%20incorporates%20hybrid%20entity%0Arecognition%20and%20fine-grained%20visual%20perceiver%20modules%20for%20various%20segmentation%0Atasks.%20Combined%20with%20the%20temporal%20adapter%2C%20HyperSeg%20achieves%20a%20comprehensive%0Aunderstanding%20of%20temporal%20information.%20Experimental%20results%20validate%20the%0Aeffectiveness%20of%20our%20insights%20in%20resolving%20universal%20image%20and%20video%0Asegmentation%20tasks%2C%20including%20the%20more%20complex%20reasoning%20perception%20tasks.%20Our%0Acode%20is%20available.%0A&entry.1838667208=http%3A//arxiv.org/abs/2411.17606v1&entry.124074799=Read"},
{"title": "VLRewardBench: A Challenging Benchmark for Vision-Language Generative\n  Reward Models", "author": "Lei Li and Yuancheng Wei and Zhihui Xie and Xuqing Yang and Yifan Song and Peiyi Wang and Chenxin An and Tianyu Liu and Sujian Li and Bill Yuchen Lin and Lingpeng Kong and Qi Liu", "abstract": "  Vision-language generative reward models (VL-GenRMs) play a crucial role in\naligning and evaluating multimodal AI systems, yet their own evaluation remains\nunder-explored. Current assessment methods primarily rely on AI-annotated\npreference labels from traditional VL tasks, which can introduce biases and\noften fail to effectively challenge state-of-the-art models. To address these\nlimitations, we introduce VL-RewardBench, a comprehensive benchmark spanning\ngeneral multimodal queries, visual hallucination detection, and complex\nreasoning tasks. Through our AI-assisted annotation pipeline combining sample\nselection with human verification, we curate 1,250 high-quality examples\nspecifically designed to probe model limitations. Comprehensive evaluation\nacross 16 leading large vision-language models, demonstrates VL-RewardBench's\neffectiveness as a challenging testbed, where even GPT-4o achieves only 65.4%\naccuracy, and state-of-the-art open-source models such as Qwen2-VL-72B,\nstruggle to surpass random-guessing. Importantly, performance on VL-RewardBench\nstrongly correlates (Pearson's r > 0.9) with MMMU-Pro accuracy using Best-of-N\nsampling with VL-GenRMs. Analysis experiments uncover three critical insights\nfor improving VL-GenRMs: (i) models predominantly fail at basic visual\nperception tasks rather than reasoning tasks; (ii) inference-time scaling\nbenefits vary dramatically by model capacity; and (iii) training VL-GenRMs to\nlearn to judge substantially boosts judgment capability (+14.7% accuracy for a\n7B VL-GenRM). We believe VL-RewardBench along with the experimental insights\nwill become a valuable resource for advancing VL-GenRMs.\n", "link": "http://arxiv.org/abs/2411.17451v1", "date": "2024-11-26", "relevancy": 2.8971, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.6019}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.6019}, {"title": "Genie: Generative Interactive Environments", "link": "http://arxiv.org/abs/2402.15391v1", "similarity": 0.5344}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20VLRewardBench%3A%20A%20Challenging%20Benchmark%20for%20Vision-Language%20Generative%0A%20%20Reward%20Models&body=Title%3A%20VLRewardBench%3A%20A%20Challenging%20Benchmark%20for%20Vision-Language%20Generative%0A%20%20Reward%20Models%0AAuthor%3A%20Lei%20Li%20and%20Yuancheng%20Wei%20and%20Zhihui%20Xie%20and%20Xuqing%20Yang%20and%20Yifan%20Song%20and%20Peiyi%20Wang%20and%20Chenxin%20An%20and%20Tianyu%20Liu%20and%20Sujian%20Li%20and%20Bill%20Yuchen%20Lin%20and%20Lingpeng%20Kong%20and%20Qi%20Liu%0AAbstract%3A%20%20%20Vision-language%20generative%20reward%20models%20%28VL-GenRMs%29%20play%20a%20crucial%20role%20in%0Aaligning%20and%20evaluating%20multimodal%20AI%20systems%2C%20yet%20their%20own%20evaluation%20remains%0Aunder-explored.%20Current%20assessment%20methods%20primarily%20rely%20on%20AI-annotated%0Apreference%20labels%20from%20traditional%20VL%20tasks%2C%20which%20can%20introduce%20biases%20and%0Aoften%20fail%20to%20effectively%20challenge%20state-of-the-art%20models.%20To%20address%20these%0Alimitations%2C%20we%20introduce%20VL-RewardBench%2C%20a%20comprehensive%20benchmark%20spanning%0Ageneral%20multimodal%20queries%2C%20visual%20hallucination%20detection%2C%20and%20complex%0Areasoning%20tasks.%20Through%20our%20AI-assisted%20annotation%20pipeline%20combining%20sample%0Aselection%20with%20human%20verification%2C%20we%20curate%201%2C250%20high-quality%20examples%0Aspecifically%20designed%20to%20probe%20model%20limitations.%20Comprehensive%20evaluation%0Aacross%2016%20leading%20large%20vision-language%20models%2C%20demonstrates%20VL-RewardBench%27s%0Aeffectiveness%20as%20a%20challenging%20testbed%2C%20where%20even%20GPT-4o%20achieves%20only%2065.4%25%0Aaccuracy%2C%20and%20state-of-the-art%20open-source%20models%20such%20as%20Qwen2-VL-72B%2C%0Astruggle%20to%20surpass%20random-guessing.%20Importantly%2C%20performance%20on%20VL-RewardBench%0Astrongly%20correlates%20%28Pearson%27s%20r%20%3E%200.9%29%20with%20MMMU-Pro%20accuracy%20using%20Best-of-N%0Asampling%20with%20VL-GenRMs.%20Analysis%20experiments%20uncover%20three%20critical%20insights%0Afor%20improving%20VL-GenRMs%3A%20%28i%29%20models%20predominantly%20fail%20at%20basic%20visual%0Aperception%20tasks%20rather%20than%20reasoning%20tasks%3B%20%28ii%29%20inference-time%20scaling%0Abenefits%20vary%20dramatically%20by%20model%20capacity%3B%20and%20%28iii%29%20training%20VL-GenRMs%20to%0Alearn%20to%20judge%20substantially%20boosts%20judgment%20capability%20%28%2B14.7%25%20accuracy%20for%20a%0A7B%20VL-GenRM%29.%20We%20believe%20VL-RewardBench%20along%20with%20the%20experimental%20insights%0Awill%20become%20a%20valuable%20resource%20for%20advancing%20VL-GenRMs.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2411.17451v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DVLRewardBench%253A%2520A%2520Challenging%2520Benchmark%2520for%2520Vision-Language%2520Generative%250A%2520%2520Reward%2520Models%26entry.906535625%3DLei%2520Li%2520and%2520Yuancheng%2520Wei%2520and%2520Zhihui%2520Xie%2520and%2520Xuqing%2520Yang%2520and%2520Yifan%2520Song%2520and%2520Peiyi%2520Wang%2520and%2520Chenxin%2520An%2520and%2520Tianyu%2520Liu%2520and%2520Sujian%2520Li%2520and%2520Bill%2520Yuchen%2520Lin%2520and%2520Lingpeng%2520Kong%2520and%2520Qi%2520Liu%26entry.1292438233%3D%2520%2520Vision-language%2520generative%2520reward%2520models%2520%2528VL-GenRMs%2529%2520play%2520a%2520crucial%2520role%2520in%250Aaligning%2520and%2520evaluating%2520multimodal%2520AI%2520systems%252C%2520yet%2520their%2520own%2520evaluation%2520remains%250Aunder-explored.%2520Current%2520assessment%2520methods%2520primarily%2520rely%2520on%2520AI-annotated%250Apreference%2520labels%2520from%2520traditional%2520VL%2520tasks%252C%2520which%2520can%2520introduce%2520biases%2520and%250Aoften%2520fail%2520to%2520effectively%2520challenge%2520state-of-the-art%2520models.%2520To%2520address%2520these%250Alimitations%252C%2520we%2520introduce%2520VL-RewardBench%252C%2520a%2520comprehensive%2520benchmark%2520spanning%250Ageneral%2520multimodal%2520queries%252C%2520visual%2520hallucination%2520detection%252C%2520and%2520complex%250Areasoning%2520tasks.%2520Through%2520our%2520AI-assisted%2520annotation%2520pipeline%2520combining%2520sample%250Aselection%2520with%2520human%2520verification%252C%2520we%2520curate%25201%252C250%2520high-quality%2520examples%250Aspecifically%2520designed%2520to%2520probe%2520model%2520limitations.%2520Comprehensive%2520evaluation%250Aacross%252016%2520leading%2520large%2520vision-language%2520models%252C%2520demonstrates%2520VL-RewardBench%2527s%250Aeffectiveness%2520as%2520a%2520challenging%2520testbed%252C%2520where%2520even%2520GPT-4o%2520achieves%2520only%252065.4%2525%250Aaccuracy%252C%2520and%2520state-of-the-art%2520open-source%2520models%2520such%2520as%2520Qwen2-VL-72B%252C%250Astruggle%2520to%2520surpass%2520random-guessing.%2520Importantly%252C%2520performance%2520on%2520VL-RewardBench%250Astrongly%2520correlates%2520%2528Pearson%2527s%2520r%2520%253E%25200.9%2529%2520with%2520MMMU-Pro%2520accuracy%2520using%2520Best-of-N%250Asampling%2520with%2520VL-GenRMs.%2520Analysis%2520experiments%2520uncover%2520three%2520critical%2520insights%250Afor%2520improving%2520VL-GenRMs%253A%2520%2528i%2529%2520models%2520predominantly%2520fail%2520at%2520basic%2520visual%250Aperception%2520tasks%2520rather%2520than%2520reasoning%2520tasks%253B%2520%2528ii%2529%2520inference-time%2520scaling%250Abenefits%2520vary%2520dramatically%2520by%2520model%2520capacity%253B%2520and%2520%2528iii%2529%2520training%2520VL-GenRMs%2520to%250Alearn%2520to%2520judge%2520substantially%2520boosts%2520judgment%2520capability%2520%2528%252B14.7%2525%2520accuracy%2520for%2520a%250A7B%2520VL-GenRM%2529.%2520We%2520believe%2520VL-RewardBench%2520along%2520with%2520the%2520experimental%2520insights%250Awill%2520become%2520a%2520valuable%2520resource%2520for%2520advancing%2520VL-GenRMs.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2411.17451v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=VLRewardBench%3A%20A%20Challenging%20Benchmark%20for%20Vision-Language%20Generative%0A%20%20Reward%20Models&entry.906535625=Lei%20Li%20and%20Yuancheng%20Wei%20and%20Zhihui%20Xie%20and%20Xuqing%20Yang%20and%20Yifan%20Song%20and%20Peiyi%20Wang%20and%20Chenxin%20An%20and%20Tianyu%20Liu%20and%20Sujian%20Li%20and%20Bill%20Yuchen%20Lin%20and%20Lingpeng%20Kong%20and%20Qi%20Liu&entry.1292438233=%20%20Vision-language%20generative%20reward%20models%20%28VL-GenRMs%29%20play%20a%20crucial%20role%20in%0Aaligning%20and%20evaluating%20multimodal%20AI%20systems%2C%20yet%20their%20own%20evaluation%20remains%0Aunder-explored.%20Current%20assessment%20methods%20primarily%20rely%20on%20AI-annotated%0Apreference%20labels%20from%20traditional%20VL%20tasks%2C%20which%20can%20introduce%20biases%20and%0Aoften%20fail%20to%20effectively%20challenge%20state-of-the-art%20models.%20To%20address%20these%0Alimitations%2C%20we%20introduce%20VL-RewardBench%2C%20a%20comprehensive%20benchmark%20spanning%0Ageneral%20multimodal%20queries%2C%20visual%20hallucination%20detection%2C%20and%20complex%0Areasoning%20tasks.%20Through%20our%20AI-assisted%20annotation%20pipeline%20combining%20sample%0Aselection%20with%20human%20verification%2C%20we%20curate%201%2C250%20high-quality%20examples%0Aspecifically%20designed%20to%20probe%20model%20limitations.%20Comprehensive%20evaluation%0Aacross%2016%20leading%20large%20vision-language%20models%2C%20demonstrates%20VL-RewardBench%27s%0Aeffectiveness%20as%20a%20challenging%20testbed%2C%20where%20even%20GPT-4o%20achieves%20only%2065.4%25%0Aaccuracy%2C%20and%20state-of-the-art%20open-source%20models%20such%20as%20Qwen2-VL-72B%2C%0Astruggle%20to%20surpass%20random-guessing.%20Importantly%2C%20performance%20on%20VL-RewardBench%0Astrongly%20correlates%20%28Pearson%27s%20r%20%3E%200.9%29%20with%20MMMU-Pro%20accuracy%20using%20Best-of-N%0Asampling%20with%20VL-GenRMs.%20Analysis%20experiments%20uncover%20three%20critical%20insights%0Afor%20improving%20VL-GenRMs%3A%20%28i%29%20models%20predominantly%20fail%20at%20basic%20visual%0Aperception%20tasks%20rather%20than%20reasoning%20tasks%3B%20%28ii%29%20inference-time%20scaling%0Abenefits%20vary%20dramatically%20by%20model%20capacity%3B%20and%20%28iii%29%20training%20VL-GenRMs%20to%0Alearn%20to%20judge%20substantially%20boosts%20judgment%20capability%20%28%2B14.7%25%20accuracy%20for%20a%0A7B%20VL-GenRM%29.%20We%20believe%20VL-RewardBench%20along%20with%20the%20experimental%20insights%0Awill%20become%20a%20valuable%20resource%20for%20advancing%20VL-GenRMs.%0A&entry.1838667208=http%3A//arxiv.org/abs/2411.17451v1&entry.124074799=Read"},
{"title": "Enhancing Zero-Shot Facial Expression Recognition by LLM Knowledge\n  Transfer", "author": "Zengqun Zhao and Yu Cao and Shaogang Gong and Ioannis Patras", "abstract": "  Current facial expression recognition (FER) models are often designed in a\nsupervised learning manner and thus are constrained by the lack of large-scale\nfacial expression images with high-quality annotations. Consequently, these\nmodels often fail to generalize well, performing poorly on unseen images in\ninference. Vision-language-based zero-shot models demonstrate a promising\npotential for addressing such challenges. However, these models lack\ntask-specific knowledge and therefore are not optimized for the nuances of\nrecognizing facial expressions. To bridge this gap, this work proposes a novel\nmethod, Exp-CLIP, to enhance zero-shot FER by transferring the task knowledge\nfrom large language models (LLMs). Specifically, based on the pre-trained\nvision-language encoders, we incorporate a projection head designed to map the\ninitial joint vision-language space into a space that captures representations\nof facial actions. To train this projection head for subsequent zero-shot\npredictions, we propose to align the projected visual representations with\ntask-specific semantic meanings derived from the LLM encoder, and the text\ninstruction-based strategy is employed to customize the LLM knowledge. Given\nunlabelled facial data and efficient training of the projection head, Exp-CLIP\nachieves superior zero-shot results to the CLIP models and several other large\nvision-language models (LVLMs) on seven in-the-wild FER datasets. The code and\npre-trained models are available at https://github.com/zengqunzhao/Exp-CLIP.\n", "link": "http://arxiv.org/abs/2405.19100v3", "date": "2024-11-26", "relevancy": 2.877, "topK": [{"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5828}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5717}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5717}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Enhancing%20Zero-Shot%20Facial%20Expression%20Recognition%20by%20LLM%20Knowledge%0A%20%20Transfer&body=Title%3A%20Enhancing%20Zero-Shot%20Facial%20Expression%20Recognition%20by%20LLM%20Knowledge%0A%20%20Transfer%0AAuthor%3A%20Zengqun%20Zhao%20and%20Yu%20Cao%20and%20Shaogang%20Gong%20and%20Ioannis%20Patras%0AAbstract%3A%20%20%20Current%20facial%20expression%20recognition%20%28FER%29%20models%20are%20often%20designed%20in%20a%0Asupervised%20learning%20manner%20and%20thus%20are%20constrained%20by%20the%20lack%20of%20large-scale%0Afacial%20expression%20images%20with%20high-quality%20annotations.%20Consequently%2C%20these%0Amodels%20often%20fail%20to%20generalize%20well%2C%20performing%20poorly%20on%20unseen%20images%20in%0Ainference.%20Vision-language-based%20zero-shot%20models%20demonstrate%20a%20promising%0Apotential%20for%20addressing%20such%20challenges.%20However%2C%20these%20models%20lack%0Atask-specific%20knowledge%20and%20therefore%20are%20not%20optimized%20for%20the%20nuances%20of%0Arecognizing%20facial%20expressions.%20To%20bridge%20this%20gap%2C%20this%20work%20proposes%20a%20novel%0Amethod%2C%20Exp-CLIP%2C%20to%20enhance%20zero-shot%20FER%20by%20transferring%20the%20task%20knowledge%0Afrom%20large%20language%20models%20%28LLMs%29.%20Specifically%2C%20based%20on%20the%20pre-trained%0Avision-language%20encoders%2C%20we%20incorporate%20a%20projection%20head%20designed%20to%20map%20the%0Ainitial%20joint%20vision-language%20space%20into%20a%20space%20that%20captures%20representations%0Aof%20facial%20actions.%20To%20train%20this%20projection%20head%20for%20subsequent%20zero-shot%0Apredictions%2C%20we%20propose%20to%20align%20the%20projected%20visual%20representations%20with%0Atask-specific%20semantic%20meanings%20derived%20from%20the%20LLM%20encoder%2C%20and%20the%20text%0Ainstruction-based%20strategy%20is%20employed%20to%20customize%20the%20LLM%20knowledge.%20Given%0Aunlabelled%20facial%20data%20and%20efficient%20training%20of%20the%20projection%20head%2C%20Exp-CLIP%0Aachieves%20superior%20zero-shot%20results%20to%20the%20CLIP%20models%20and%20several%20other%20large%0Avision-language%20models%20%28LVLMs%29%20on%20seven%20in-the-wild%20FER%20datasets.%20The%20code%20and%0Apre-trained%20models%20are%20available%20at%20https%3A//github.com/zengqunzhao/Exp-CLIP.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2405.19100v3%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DEnhancing%2520Zero-Shot%2520Facial%2520Expression%2520Recognition%2520by%2520LLM%2520Knowledge%250A%2520%2520Transfer%26entry.906535625%3DZengqun%2520Zhao%2520and%2520Yu%2520Cao%2520and%2520Shaogang%2520Gong%2520and%2520Ioannis%2520Patras%26entry.1292438233%3D%2520%2520Current%2520facial%2520expression%2520recognition%2520%2528FER%2529%2520models%2520are%2520often%2520designed%2520in%2520a%250Asupervised%2520learning%2520manner%2520and%2520thus%2520are%2520constrained%2520by%2520the%2520lack%2520of%2520large-scale%250Afacial%2520expression%2520images%2520with%2520high-quality%2520annotations.%2520Consequently%252C%2520these%250Amodels%2520often%2520fail%2520to%2520generalize%2520well%252C%2520performing%2520poorly%2520on%2520unseen%2520images%2520in%250Ainference.%2520Vision-language-based%2520zero-shot%2520models%2520demonstrate%2520a%2520promising%250Apotential%2520for%2520addressing%2520such%2520challenges.%2520However%252C%2520these%2520models%2520lack%250Atask-specific%2520knowledge%2520and%2520therefore%2520are%2520not%2520optimized%2520for%2520the%2520nuances%2520of%250Arecognizing%2520facial%2520expressions.%2520To%2520bridge%2520this%2520gap%252C%2520this%2520work%2520proposes%2520a%2520novel%250Amethod%252C%2520Exp-CLIP%252C%2520to%2520enhance%2520zero-shot%2520FER%2520by%2520transferring%2520the%2520task%2520knowledge%250Afrom%2520large%2520language%2520models%2520%2528LLMs%2529.%2520Specifically%252C%2520based%2520on%2520the%2520pre-trained%250Avision-language%2520encoders%252C%2520we%2520incorporate%2520a%2520projection%2520head%2520designed%2520to%2520map%2520the%250Ainitial%2520joint%2520vision-language%2520space%2520into%2520a%2520space%2520that%2520captures%2520representations%250Aof%2520facial%2520actions.%2520To%2520train%2520this%2520projection%2520head%2520for%2520subsequent%2520zero-shot%250Apredictions%252C%2520we%2520propose%2520to%2520align%2520the%2520projected%2520visual%2520representations%2520with%250Atask-specific%2520semantic%2520meanings%2520derived%2520from%2520the%2520LLM%2520encoder%252C%2520and%2520the%2520text%250Ainstruction-based%2520strategy%2520is%2520employed%2520to%2520customize%2520the%2520LLM%2520knowledge.%2520Given%250Aunlabelled%2520facial%2520data%2520and%2520efficient%2520training%2520of%2520the%2520projection%2520head%252C%2520Exp-CLIP%250Aachieves%2520superior%2520zero-shot%2520results%2520to%2520the%2520CLIP%2520models%2520and%2520several%2520other%2520large%250Avision-language%2520models%2520%2528LVLMs%2529%2520on%2520seven%2520in-the-wild%2520FER%2520datasets.%2520The%2520code%2520and%250Apre-trained%2520models%2520are%2520available%2520at%2520https%253A//github.com/zengqunzhao/Exp-CLIP.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2405.19100v3%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Enhancing%20Zero-Shot%20Facial%20Expression%20Recognition%20by%20LLM%20Knowledge%0A%20%20Transfer&entry.906535625=Zengqun%20Zhao%20and%20Yu%20Cao%20and%20Shaogang%20Gong%20and%20Ioannis%20Patras&entry.1292438233=%20%20Current%20facial%20expression%20recognition%20%28FER%29%20models%20are%20often%20designed%20in%20a%0Asupervised%20learning%20manner%20and%20thus%20are%20constrained%20by%20the%20lack%20of%20large-scale%0Afacial%20expression%20images%20with%20high-quality%20annotations.%20Consequently%2C%20these%0Amodels%20often%20fail%20to%20generalize%20well%2C%20performing%20poorly%20on%20unseen%20images%20in%0Ainference.%20Vision-language-based%20zero-shot%20models%20demonstrate%20a%20promising%0Apotential%20for%20addressing%20such%20challenges.%20However%2C%20these%20models%20lack%0Atask-specific%20knowledge%20and%20therefore%20are%20not%20optimized%20for%20the%20nuances%20of%0Arecognizing%20facial%20expressions.%20To%20bridge%20this%20gap%2C%20this%20work%20proposes%20a%20novel%0Amethod%2C%20Exp-CLIP%2C%20to%20enhance%20zero-shot%20FER%20by%20transferring%20the%20task%20knowledge%0Afrom%20large%20language%20models%20%28LLMs%29.%20Specifically%2C%20based%20on%20the%20pre-trained%0Avision-language%20encoders%2C%20we%20incorporate%20a%20projection%20head%20designed%20to%20map%20the%0Ainitial%20joint%20vision-language%20space%20into%20a%20space%20that%20captures%20representations%0Aof%20facial%20actions.%20To%20train%20this%20projection%20head%20for%20subsequent%20zero-shot%0Apredictions%2C%20we%20propose%20to%20align%20the%20projected%20visual%20representations%20with%0Atask-specific%20semantic%20meanings%20derived%20from%20the%20LLM%20encoder%2C%20and%20the%20text%0Ainstruction-based%20strategy%20is%20employed%20to%20customize%20the%20LLM%20knowledge.%20Given%0Aunlabelled%20facial%20data%20and%20efficient%20training%20of%20the%20projection%20head%2C%20Exp-CLIP%0Aachieves%20superior%20zero-shot%20results%20to%20the%20CLIP%20models%20and%20several%20other%20large%0Avision-language%20models%20%28LVLMs%29%20on%20seven%20in-the-wild%20FER%20datasets.%20The%20code%20and%0Apre-trained%20models%20are%20available%20at%20https%3A//github.com/zengqunzhao/Exp-CLIP.%0A&entry.1838667208=http%3A//arxiv.org/abs/2405.19100v3&entry.124074799=Read"},
{"title": "Learning Visual Hierarchies with Hyperbolic Embeddings", "author": "Ziwei Wang and Sameera Ramasinghe and Chenchen Xu and Julien Monteil and Loris Bazzani and Thalaiyasingam Ajanthan", "abstract": "  Structuring latent representations in a hierarchical manner enables models to\nlearn patterns at multiple levels of abstraction. However, most prevalent image\nunderstanding models focus on visual similarity, and learning visual\nhierarchies is relatively unexplored. In this work, for the first time, we\nintroduce a learning paradigm that can encode user-defined multi-level visual\nhierarchies in hyperbolic space without requiring explicit hierarchical labels.\nAs a concrete example, first, we define a part-based image hierarchy using\nobject-level annotations within and across images. Then, we introduce an\napproach to enforce the hierarchy using contrastive loss with pairwise\nentailment metrics. Finally, we discuss new evaluation metrics to effectively\nmeasure hierarchical image retrieval. Encoding these complex relationships\nensures that the learned representations capture semantic and structural\ninformation that transcends mere visual similarity. Experiments in part-based\nimage retrieval show significant improvements in hierarchical retrieval tasks,\ndemonstrating the capability of our model in capturing visual hierarchies.\n", "link": "http://arxiv.org/abs/2411.17490v1", "date": "2024-11-26", "relevancy": 2.8629, "topK": [{"title": "Self-supervised Photographic Image Layout Representation Learning", "link": "http://arxiv.org/abs/2403.03740v1", "similarity": 0.599}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5593}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5593}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Learning%20Visual%20Hierarchies%20with%20Hyperbolic%20Embeddings&body=Title%3A%20Learning%20Visual%20Hierarchies%20with%20Hyperbolic%20Embeddings%0AAuthor%3A%20Ziwei%20Wang%20and%20Sameera%20Ramasinghe%20and%20Chenchen%20Xu%20and%20Julien%20Monteil%20and%20Loris%20Bazzani%20and%20Thalaiyasingam%20Ajanthan%0AAbstract%3A%20%20%20Structuring%20latent%20representations%20in%20a%20hierarchical%20manner%20enables%20models%20to%0Alearn%20patterns%20at%20multiple%20levels%20of%20abstraction.%20However%2C%20most%20prevalent%20image%0Aunderstanding%20models%20focus%20on%20visual%20similarity%2C%20and%20learning%20visual%0Ahierarchies%20is%20relatively%20unexplored.%20In%20this%20work%2C%20for%20the%20first%20time%2C%20we%0Aintroduce%20a%20learning%20paradigm%20that%20can%20encode%20user-defined%20multi-level%20visual%0Ahierarchies%20in%20hyperbolic%20space%20without%20requiring%20explicit%20hierarchical%20labels.%0AAs%20a%20concrete%20example%2C%20first%2C%20we%20define%20a%20part-based%20image%20hierarchy%20using%0Aobject-level%20annotations%20within%20and%20across%20images.%20Then%2C%20we%20introduce%20an%0Aapproach%20to%20enforce%20the%20hierarchy%20using%20contrastive%20loss%20with%20pairwise%0Aentailment%20metrics.%20Finally%2C%20we%20discuss%20new%20evaluation%20metrics%20to%20effectively%0Ameasure%20hierarchical%20image%20retrieval.%20Encoding%20these%20complex%20relationships%0Aensures%20that%20the%20learned%20representations%20capture%20semantic%20and%20structural%0Ainformation%20that%20transcends%20mere%20visual%20similarity.%20Experiments%20in%20part-based%0Aimage%20retrieval%20show%20significant%20improvements%20in%20hierarchical%20retrieval%20tasks%2C%0Ademonstrating%20the%20capability%20of%20our%20model%20in%20capturing%20visual%20hierarchies.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2411.17490v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DLearning%2520Visual%2520Hierarchies%2520with%2520Hyperbolic%2520Embeddings%26entry.906535625%3DZiwei%2520Wang%2520and%2520Sameera%2520Ramasinghe%2520and%2520Chenchen%2520Xu%2520and%2520Julien%2520Monteil%2520and%2520Loris%2520Bazzani%2520and%2520Thalaiyasingam%2520Ajanthan%26entry.1292438233%3D%2520%2520Structuring%2520latent%2520representations%2520in%2520a%2520hierarchical%2520manner%2520enables%2520models%2520to%250Alearn%2520patterns%2520at%2520multiple%2520levels%2520of%2520abstraction.%2520However%252C%2520most%2520prevalent%2520image%250Aunderstanding%2520models%2520focus%2520on%2520visual%2520similarity%252C%2520and%2520learning%2520visual%250Ahierarchies%2520is%2520relatively%2520unexplored.%2520In%2520this%2520work%252C%2520for%2520the%2520first%2520time%252C%2520we%250Aintroduce%2520a%2520learning%2520paradigm%2520that%2520can%2520encode%2520user-defined%2520multi-level%2520visual%250Ahierarchies%2520in%2520hyperbolic%2520space%2520without%2520requiring%2520explicit%2520hierarchical%2520labels.%250AAs%2520a%2520concrete%2520example%252C%2520first%252C%2520we%2520define%2520a%2520part-based%2520image%2520hierarchy%2520using%250Aobject-level%2520annotations%2520within%2520and%2520across%2520images.%2520Then%252C%2520we%2520introduce%2520an%250Aapproach%2520to%2520enforce%2520the%2520hierarchy%2520using%2520contrastive%2520loss%2520with%2520pairwise%250Aentailment%2520metrics.%2520Finally%252C%2520we%2520discuss%2520new%2520evaluation%2520metrics%2520to%2520effectively%250Ameasure%2520hierarchical%2520image%2520retrieval.%2520Encoding%2520these%2520complex%2520relationships%250Aensures%2520that%2520the%2520learned%2520representations%2520capture%2520semantic%2520and%2520structural%250Ainformation%2520that%2520transcends%2520mere%2520visual%2520similarity.%2520Experiments%2520in%2520part-based%250Aimage%2520retrieval%2520show%2520significant%2520improvements%2520in%2520hierarchical%2520retrieval%2520tasks%252C%250Ademonstrating%2520the%2520capability%2520of%2520our%2520model%2520in%2520capturing%2520visual%2520hierarchies.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2411.17490v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Learning%20Visual%20Hierarchies%20with%20Hyperbolic%20Embeddings&entry.906535625=Ziwei%20Wang%20and%20Sameera%20Ramasinghe%20and%20Chenchen%20Xu%20and%20Julien%20Monteil%20and%20Loris%20Bazzani%20and%20Thalaiyasingam%20Ajanthan&entry.1292438233=%20%20Structuring%20latent%20representations%20in%20a%20hierarchical%20manner%20enables%20models%20to%0Alearn%20patterns%20at%20multiple%20levels%20of%20abstraction.%20However%2C%20most%20prevalent%20image%0Aunderstanding%20models%20focus%20on%20visual%20similarity%2C%20and%20learning%20visual%0Ahierarchies%20is%20relatively%20unexplored.%20In%20this%20work%2C%20for%20the%20first%20time%2C%20we%0Aintroduce%20a%20learning%20paradigm%20that%20can%20encode%20user-defined%20multi-level%20visual%0Ahierarchies%20in%20hyperbolic%20space%20without%20requiring%20explicit%20hierarchical%20labels.%0AAs%20a%20concrete%20example%2C%20first%2C%20we%20define%20a%20part-based%20image%20hierarchy%20using%0Aobject-level%20annotations%20within%20and%20across%20images.%20Then%2C%20we%20introduce%20an%0Aapproach%20to%20enforce%20the%20hierarchy%20using%20contrastive%20loss%20with%20pairwise%0Aentailment%20metrics.%20Finally%2C%20we%20discuss%20new%20evaluation%20metrics%20to%20effectively%0Ameasure%20hierarchical%20image%20retrieval.%20Encoding%20these%20complex%20relationships%0Aensures%20that%20the%20learned%20representations%20capture%20semantic%20and%20structural%0Ainformation%20that%20transcends%20mere%20visual%20similarity.%20Experiments%20in%20part-based%0Aimage%20retrieval%20show%20significant%20improvements%20in%20hierarchical%20retrieval%20tasks%2C%0Ademonstrating%20the%20capability%20of%20our%20model%20in%20capturing%20visual%20hierarchies.%0A&entry.1838667208=http%3A//arxiv.org/abs/2411.17490v1&entry.124074799=Read"},
{"title": "Learning Spatially-Aware Language and Audio Embeddings", "author": "Bhavika Devnani and Skyler Seto and Zakaria Aldeneh and Alessandro Toso and Elena Menyaylenko and Barry-John Theobald and Jonathan Sheaffer and Miguel Sarabia", "abstract": "  Humans can picture a sound scene given an imprecise natural language\ndescription. For example, it is easy to imagine an acoustic environment given a\nphrase like \"the lion roar came from right behind me!\". For a machine to have\nthe same degree of comprehension, the machine must know what a lion is\n(semantic attribute), what the concept of \"behind\" is (spatial attribute) and\nhow these pieces of linguistic information align with the semantic and spatial\nattributes of the sound (what a roar sounds like when its coming from behind).\nState-of-the-art audio foundation models which learn to map between audio\nscenes and natural textual descriptions, are trained on non-spatial audio and\ntext pairs, and hence lack spatial awareness. In contrast, sound event\nlocalization and detection models are limited to recognizing sounds from a\nfixed number of classes, and they localize the source to absolute position\n(e.g., 0.2m) rather than a position described using natural language (e.g.,\n\"next to me\"). To address these gaps, we present ELSA a spatially aware-audio\nand text embedding model trained using multimodal contrastive learning. ELSA\nsupports non-spatial audio, spatial audio, and open vocabulary text captions\ndescribing both the spatial and semantic components of sound. To train ELSA:\n(a) we spatially augment the audio and captions of three open-source audio\ndatasets totaling 4,738 hours of audio, and (b) we design an encoder to capture\nthe semantics of non-spatial audio, and the semantics and spatial attributes of\nspatial audio using contrastive learning. ELSA is competitive with\nstate-of-the-art for both semantic retrieval and 3D source localization. In\nparticular, ELSA achieves +2.8% mean audio-to-text and text-to-audio R@1 above\nthe baseline, and outperforms by -11.6{\\deg} mean-absolute-error in 3D source\nlocalization over the baseline.\n", "link": "http://arxiv.org/abs/2409.11369v2", "date": "2024-11-26", "relevancy": 2.8602, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5892}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5892}, {"title": "DDM-NET: End-to-end learning of keypoint feature Detection, Description and Matching for 3D localization", "link": "https://arxiv.org/abs/2212.04575", "similarity": 0.5378}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Learning%20Spatially-Aware%20Language%20and%20Audio%20Embeddings&body=Title%3A%20Learning%20Spatially-Aware%20Language%20and%20Audio%20Embeddings%0AAuthor%3A%20Bhavika%20Devnani%20and%20Skyler%20Seto%20and%20Zakaria%20Aldeneh%20and%20Alessandro%20Toso%20and%20Elena%20Menyaylenko%20and%20Barry-John%20Theobald%20and%20Jonathan%20Sheaffer%20and%20Miguel%20Sarabia%0AAbstract%3A%20%20%20Humans%20can%20picture%20a%20sound%20scene%20given%20an%20imprecise%20natural%20language%0Adescription.%20For%20example%2C%20it%20is%20easy%20to%20imagine%20an%20acoustic%20environment%20given%20a%0Aphrase%20like%20%22the%20lion%20roar%20came%20from%20right%20behind%20me%21%22.%20For%20a%20machine%20to%20have%0Athe%20same%20degree%20of%20comprehension%2C%20the%20machine%20must%20know%20what%20a%20lion%20is%0A%28semantic%20attribute%29%2C%20what%20the%20concept%20of%20%22behind%22%20is%20%28spatial%20attribute%29%20and%0Ahow%20these%20pieces%20of%20linguistic%20information%20align%20with%20the%20semantic%20and%20spatial%0Aattributes%20of%20the%20sound%20%28what%20a%20roar%20sounds%20like%20when%20its%20coming%20from%20behind%29.%0AState-of-the-art%20audio%20foundation%20models%20which%20learn%20to%20map%20between%20audio%0Ascenes%20and%20natural%20textual%20descriptions%2C%20are%20trained%20on%20non-spatial%20audio%20and%0Atext%20pairs%2C%20and%20hence%20lack%20spatial%20awareness.%20In%20contrast%2C%20sound%20event%0Alocalization%20and%20detection%20models%20are%20limited%20to%20recognizing%20sounds%20from%20a%0Afixed%20number%20of%20classes%2C%20and%20they%20localize%20the%20source%20to%20absolute%20position%0A%28e.g.%2C%200.2m%29%20rather%20than%20a%20position%20described%20using%20natural%20language%20%28e.g.%2C%0A%22next%20to%20me%22%29.%20To%20address%20these%20gaps%2C%20we%20present%20ELSA%20a%20spatially%20aware-audio%0Aand%20text%20embedding%20model%20trained%20using%20multimodal%20contrastive%20learning.%20ELSA%0Asupports%20non-spatial%20audio%2C%20spatial%20audio%2C%20and%20open%20vocabulary%20text%20captions%0Adescribing%20both%20the%20spatial%20and%20semantic%20components%20of%20sound.%20To%20train%20ELSA%3A%0A%28a%29%20we%20spatially%20augment%20the%20audio%20and%20captions%20of%20three%20open-source%20audio%0Adatasets%20totaling%204%2C738%20hours%20of%20audio%2C%20and%20%28b%29%20we%20design%20an%20encoder%20to%20capture%0Athe%20semantics%20of%20non-spatial%20audio%2C%20and%20the%20semantics%20and%20spatial%20attributes%20of%0Aspatial%20audio%20using%20contrastive%20learning.%20ELSA%20is%20competitive%20with%0Astate-of-the-art%20for%20both%20semantic%20retrieval%20and%203D%20source%20localization.%20In%0Aparticular%2C%20ELSA%20achieves%20%2B2.8%25%20mean%20audio-to-text%20and%20text-to-audio%20R%401%20above%0Athe%20baseline%2C%20and%20outperforms%20by%20-11.6%7B%5Cdeg%7D%20mean-absolute-error%20in%203D%20source%0Alocalization%20over%20the%20baseline.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2409.11369v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DLearning%2520Spatially-Aware%2520Language%2520and%2520Audio%2520Embeddings%26entry.906535625%3DBhavika%2520Devnani%2520and%2520Skyler%2520Seto%2520and%2520Zakaria%2520Aldeneh%2520and%2520Alessandro%2520Toso%2520and%2520Elena%2520Menyaylenko%2520and%2520Barry-John%2520Theobald%2520and%2520Jonathan%2520Sheaffer%2520and%2520Miguel%2520Sarabia%26entry.1292438233%3D%2520%2520Humans%2520can%2520picture%2520a%2520sound%2520scene%2520given%2520an%2520imprecise%2520natural%2520language%250Adescription.%2520For%2520example%252C%2520it%2520is%2520easy%2520to%2520imagine%2520an%2520acoustic%2520environment%2520given%2520a%250Aphrase%2520like%2520%2522the%2520lion%2520roar%2520came%2520from%2520right%2520behind%2520me%2521%2522.%2520For%2520a%2520machine%2520to%2520have%250Athe%2520same%2520degree%2520of%2520comprehension%252C%2520the%2520machine%2520must%2520know%2520what%2520a%2520lion%2520is%250A%2528semantic%2520attribute%2529%252C%2520what%2520the%2520concept%2520of%2520%2522behind%2522%2520is%2520%2528spatial%2520attribute%2529%2520and%250Ahow%2520these%2520pieces%2520of%2520linguistic%2520information%2520align%2520with%2520the%2520semantic%2520and%2520spatial%250Aattributes%2520of%2520the%2520sound%2520%2528what%2520a%2520roar%2520sounds%2520like%2520when%2520its%2520coming%2520from%2520behind%2529.%250AState-of-the-art%2520audio%2520foundation%2520models%2520which%2520learn%2520to%2520map%2520between%2520audio%250Ascenes%2520and%2520natural%2520textual%2520descriptions%252C%2520are%2520trained%2520on%2520non-spatial%2520audio%2520and%250Atext%2520pairs%252C%2520and%2520hence%2520lack%2520spatial%2520awareness.%2520In%2520contrast%252C%2520sound%2520event%250Alocalization%2520and%2520detection%2520models%2520are%2520limited%2520to%2520recognizing%2520sounds%2520from%2520a%250Afixed%2520number%2520of%2520classes%252C%2520and%2520they%2520localize%2520the%2520source%2520to%2520absolute%2520position%250A%2528e.g.%252C%25200.2m%2529%2520rather%2520than%2520a%2520position%2520described%2520using%2520natural%2520language%2520%2528e.g.%252C%250A%2522next%2520to%2520me%2522%2529.%2520To%2520address%2520these%2520gaps%252C%2520we%2520present%2520ELSA%2520a%2520spatially%2520aware-audio%250Aand%2520text%2520embedding%2520model%2520trained%2520using%2520multimodal%2520contrastive%2520learning.%2520ELSA%250Asupports%2520non-spatial%2520audio%252C%2520spatial%2520audio%252C%2520and%2520open%2520vocabulary%2520text%2520captions%250Adescribing%2520both%2520the%2520spatial%2520and%2520semantic%2520components%2520of%2520sound.%2520To%2520train%2520ELSA%253A%250A%2528a%2529%2520we%2520spatially%2520augment%2520the%2520audio%2520and%2520captions%2520of%2520three%2520open-source%2520audio%250Adatasets%2520totaling%25204%252C738%2520hours%2520of%2520audio%252C%2520and%2520%2528b%2529%2520we%2520design%2520an%2520encoder%2520to%2520capture%250Athe%2520semantics%2520of%2520non-spatial%2520audio%252C%2520and%2520the%2520semantics%2520and%2520spatial%2520attributes%2520of%250Aspatial%2520audio%2520using%2520contrastive%2520learning.%2520ELSA%2520is%2520competitive%2520with%250Astate-of-the-art%2520for%2520both%2520semantic%2520retrieval%2520and%25203D%2520source%2520localization.%2520In%250Aparticular%252C%2520ELSA%2520achieves%2520%252B2.8%2525%2520mean%2520audio-to-text%2520and%2520text-to-audio%2520R%25401%2520above%250Athe%2520baseline%252C%2520and%2520outperforms%2520by%2520-11.6%257B%255Cdeg%257D%2520mean-absolute-error%2520in%25203D%2520source%250Alocalization%2520over%2520the%2520baseline.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2409.11369v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Learning%20Spatially-Aware%20Language%20and%20Audio%20Embeddings&entry.906535625=Bhavika%20Devnani%20and%20Skyler%20Seto%20and%20Zakaria%20Aldeneh%20and%20Alessandro%20Toso%20and%20Elena%20Menyaylenko%20and%20Barry-John%20Theobald%20and%20Jonathan%20Sheaffer%20and%20Miguel%20Sarabia&entry.1292438233=%20%20Humans%20can%20picture%20a%20sound%20scene%20given%20an%20imprecise%20natural%20language%0Adescription.%20For%20example%2C%20it%20is%20easy%20to%20imagine%20an%20acoustic%20environment%20given%20a%0Aphrase%20like%20%22the%20lion%20roar%20came%20from%20right%20behind%20me%21%22.%20For%20a%20machine%20to%20have%0Athe%20same%20degree%20of%20comprehension%2C%20the%20machine%20must%20know%20what%20a%20lion%20is%0A%28semantic%20attribute%29%2C%20what%20the%20concept%20of%20%22behind%22%20is%20%28spatial%20attribute%29%20and%0Ahow%20these%20pieces%20of%20linguistic%20information%20align%20with%20the%20semantic%20and%20spatial%0Aattributes%20of%20the%20sound%20%28what%20a%20roar%20sounds%20like%20when%20its%20coming%20from%20behind%29.%0AState-of-the-art%20audio%20foundation%20models%20which%20learn%20to%20map%20between%20audio%0Ascenes%20and%20natural%20textual%20descriptions%2C%20are%20trained%20on%20non-spatial%20audio%20and%0Atext%20pairs%2C%20and%20hence%20lack%20spatial%20awareness.%20In%20contrast%2C%20sound%20event%0Alocalization%20and%20detection%20models%20are%20limited%20to%20recognizing%20sounds%20from%20a%0Afixed%20number%20of%20classes%2C%20and%20they%20localize%20the%20source%20to%20absolute%20position%0A%28e.g.%2C%200.2m%29%20rather%20than%20a%20position%20described%20using%20natural%20language%20%28e.g.%2C%0A%22next%20to%20me%22%29.%20To%20address%20these%20gaps%2C%20we%20present%20ELSA%20a%20spatially%20aware-audio%0Aand%20text%20embedding%20model%20trained%20using%20multimodal%20contrastive%20learning.%20ELSA%0Asupports%20non-spatial%20audio%2C%20spatial%20audio%2C%20and%20open%20vocabulary%20text%20captions%0Adescribing%20both%20the%20spatial%20and%20semantic%20components%20of%20sound.%20To%20train%20ELSA%3A%0A%28a%29%20we%20spatially%20augment%20the%20audio%20and%20captions%20of%20three%20open-source%20audio%0Adatasets%20totaling%204%2C738%20hours%20of%20audio%2C%20and%20%28b%29%20we%20design%20an%20encoder%20to%20capture%0Athe%20semantics%20of%20non-spatial%20audio%2C%20and%20the%20semantics%20and%20spatial%20attributes%20of%0Aspatial%20audio%20using%20contrastive%20learning.%20ELSA%20is%20competitive%20with%0Astate-of-the-art%20for%20both%20semantic%20retrieval%20and%203D%20source%20localization.%20In%0Aparticular%2C%20ELSA%20achieves%20%2B2.8%25%20mean%20audio-to-text%20and%20text-to-audio%20R%401%20above%0Athe%20baseline%2C%20and%20outperforms%20by%20-11.6%7B%5Cdeg%7D%20mean-absolute-error%20in%203D%20source%0Alocalization%20over%20the%20baseline.%0A&entry.1838667208=http%3A//arxiv.org/abs/2409.11369v2&entry.124074799=Read"},
{"title": "Prompting Visual-Language Models for Dynamic Facial Expression\n  Recognition", "author": "Zengqun Zhao and Ioannis Patras", "abstract": "  This paper presents a novel visual-language model called DFER-CLIP, which is\nbased on the CLIP model and designed for in-the-wild Dynamic Facial Expression\nRecognition (DFER). Specifically, the proposed DFER-CLIP consists of a visual\npart and a textual part. For the visual part, based on the CLIP image encoder,\na temporal model consisting of several Transformer encoders is introduced for\nextracting temporal facial expression features, and the final feature embedding\nis obtained as a learnable \"class\" token. For the textual part, we use as\ninputs textual descriptions of the facial behaviour that is related to the\nclasses (facial expressions) that we are interested in recognising -- those\ndescriptions are generated using large language models, like ChatGPT. This, in\ncontrast to works that use only the class names and more accurately captures\nthe relationship between them. Alongside the textual description, we introduce\na learnable token which helps the model learn relevant context information for\neach expression during training. Extensive experiments demonstrate the\neffectiveness of the proposed method and show that our DFER-CLIP also achieves\nstate-of-the-art results compared with the current supervised DFER methods on\nthe DFEW, FERV39k, and MAFW benchmarks. Code is publicly available at\nhttps://github.com/zengqunzhao/DFER-CLIP.\n", "link": "http://arxiv.org/abs/2308.13382v3", "date": "2024-11-26", "relevancy": 2.8302, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.575}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.575}, {"title": "Flexible Visual Recognition by Evidential Modeling of Confusion and Ignorance", "link": "http://openaccess.thecvf.com/content/ICCV2023/html/Fan_Flexible_Visual_Recognition_by_Evidential_Modeling_of_Confusion_and_Ignorance_ICCV_2023_paper.html", "similarity": 0.5481}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Prompting%20Visual-Language%20Models%20for%20Dynamic%20Facial%20Expression%0A%20%20Recognition&body=Title%3A%20Prompting%20Visual-Language%20Models%20for%20Dynamic%20Facial%20Expression%0A%20%20Recognition%0AAuthor%3A%20Zengqun%20Zhao%20and%20Ioannis%20Patras%0AAbstract%3A%20%20%20This%20paper%20presents%20a%20novel%20visual-language%20model%20called%20DFER-CLIP%2C%20which%20is%0Abased%20on%20the%20CLIP%20model%20and%20designed%20for%20in-the-wild%20Dynamic%20Facial%20Expression%0ARecognition%20%28DFER%29.%20Specifically%2C%20the%20proposed%20DFER-CLIP%20consists%20of%20a%20visual%0Apart%20and%20a%20textual%20part.%20For%20the%20visual%20part%2C%20based%20on%20the%20CLIP%20image%20encoder%2C%0Aa%20temporal%20model%20consisting%20of%20several%20Transformer%20encoders%20is%20introduced%20for%0Aextracting%20temporal%20facial%20expression%20features%2C%20and%20the%20final%20feature%20embedding%0Ais%20obtained%20as%20a%20learnable%20%22class%22%20token.%20For%20the%20textual%20part%2C%20we%20use%20as%0Ainputs%20textual%20descriptions%20of%20the%20facial%20behaviour%20that%20is%20related%20to%20the%0Aclasses%20%28facial%20expressions%29%20that%20we%20are%20interested%20in%20recognising%20--%20those%0Adescriptions%20are%20generated%20using%20large%20language%20models%2C%20like%20ChatGPT.%20This%2C%20in%0Acontrast%20to%20works%20that%20use%20only%20the%20class%20names%20and%20more%20accurately%20captures%0Athe%20relationship%20between%20them.%20Alongside%20the%20textual%20description%2C%20we%20introduce%0Aa%20learnable%20token%20which%20helps%20the%20model%20learn%20relevant%20context%20information%20for%0Aeach%20expression%20during%20training.%20Extensive%20experiments%20demonstrate%20the%0Aeffectiveness%20of%20the%20proposed%20method%20and%20show%20that%20our%20DFER-CLIP%20also%20achieves%0Astate-of-the-art%20results%20compared%20with%20the%20current%20supervised%20DFER%20methods%20on%0Athe%20DFEW%2C%20FERV39k%2C%20and%20MAFW%20benchmarks.%20Code%20is%20publicly%20available%20at%0Ahttps%3A//github.com/zengqunzhao/DFER-CLIP.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2308.13382v3%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DPrompting%2520Visual-Language%2520Models%2520for%2520Dynamic%2520Facial%2520Expression%250A%2520%2520Recognition%26entry.906535625%3DZengqun%2520Zhao%2520and%2520Ioannis%2520Patras%26entry.1292438233%3D%2520%2520This%2520paper%2520presents%2520a%2520novel%2520visual-language%2520model%2520called%2520DFER-CLIP%252C%2520which%2520is%250Abased%2520on%2520the%2520CLIP%2520model%2520and%2520designed%2520for%2520in-the-wild%2520Dynamic%2520Facial%2520Expression%250ARecognition%2520%2528DFER%2529.%2520Specifically%252C%2520the%2520proposed%2520DFER-CLIP%2520consists%2520of%2520a%2520visual%250Apart%2520and%2520a%2520textual%2520part.%2520For%2520the%2520visual%2520part%252C%2520based%2520on%2520the%2520CLIP%2520image%2520encoder%252C%250Aa%2520temporal%2520model%2520consisting%2520of%2520several%2520Transformer%2520encoders%2520is%2520introduced%2520for%250Aextracting%2520temporal%2520facial%2520expression%2520features%252C%2520and%2520the%2520final%2520feature%2520embedding%250Ais%2520obtained%2520as%2520a%2520learnable%2520%2522class%2522%2520token.%2520For%2520the%2520textual%2520part%252C%2520we%2520use%2520as%250Ainputs%2520textual%2520descriptions%2520of%2520the%2520facial%2520behaviour%2520that%2520is%2520related%2520to%2520the%250Aclasses%2520%2528facial%2520expressions%2529%2520that%2520we%2520are%2520interested%2520in%2520recognising%2520--%2520those%250Adescriptions%2520are%2520generated%2520using%2520large%2520language%2520models%252C%2520like%2520ChatGPT.%2520This%252C%2520in%250Acontrast%2520to%2520works%2520that%2520use%2520only%2520the%2520class%2520names%2520and%2520more%2520accurately%2520captures%250Athe%2520relationship%2520between%2520them.%2520Alongside%2520the%2520textual%2520description%252C%2520we%2520introduce%250Aa%2520learnable%2520token%2520which%2520helps%2520the%2520model%2520learn%2520relevant%2520context%2520information%2520for%250Aeach%2520expression%2520during%2520training.%2520Extensive%2520experiments%2520demonstrate%2520the%250Aeffectiveness%2520of%2520the%2520proposed%2520method%2520and%2520show%2520that%2520our%2520DFER-CLIP%2520also%2520achieves%250Astate-of-the-art%2520results%2520compared%2520with%2520the%2520current%2520supervised%2520DFER%2520methods%2520on%250Athe%2520DFEW%252C%2520FERV39k%252C%2520and%2520MAFW%2520benchmarks.%2520Code%2520is%2520publicly%2520available%2520at%250Ahttps%253A//github.com/zengqunzhao/DFER-CLIP.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2308.13382v3%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Prompting%20Visual-Language%20Models%20for%20Dynamic%20Facial%20Expression%0A%20%20Recognition&entry.906535625=Zengqun%20Zhao%20and%20Ioannis%20Patras&entry.1292438233=%20%20This%20paper%20presents%20a%20novel%20visual-language%20model%20called%20DFER-CLIP%2C%20which%20is%0Abased%20on%20the%20CLIP%20model%20and%20designed%20for%20in-the-wild%20Dynamic%20Facial%20Expression%0ARecognition%20%28DFER%29.%20Specifically%2C%20the%20proposed%20DFER-CLIP%20consists%20of%20a%20visual%0Apart%20and%20a%20textual%20part.%20For%20the%20visual%20part%2C%20based%20on%20the%20CLIP%20image%20encoder%2C%0Aa%20temporal%20model%20consisting%20of%20several%20Transformer%20encoders%20is%20introduced%20for%0Aextracting%20temporal%20facial%20expression%20features%2C%20and%20the%20final%20feature%20embedding%0Ais%20obtained%20as%20a%20learnable%20%22class%22%20token.%20For%20the%20textual%20part%2C%20we%20use%20as%0Ainputs%20textual%20descriptions%20of%20the%20facial%20behaviour%20that%20is%20related%20to%20the%0Aclasses%20%28facial%20expressions%29%20that%20we%20are%20interested%20in%20recognising%20--%20those%0Adescriptions%20are%20generated%20using%20large%20language%20models%2C%20like%20ChatGPT.%20This%2C%20in%0Acontrast%20to%20works%20that%20use%20only%20the%20class%20names%20and%20more%20accurately%20captures%0Athe%20relationship%20between%20them.%20Alongside%20the%20textual%20description%2C%20we%20introduce%0Aa%20learnable%20token%20which%20helps%20the%20model%20learn%20relevant%20context%20information%20for%0Aeach%20expression%20during%20training.%20Extensive%20experiments%20demonstrate%20the%0Aeffectiveness%20of%20the%20proposed%20method%20and%20show%20that%20our%20DFER-CLIP%20also%20achieves%0Astate-of-the-art%20results%20compared%20with%20the%20current%20supervised%20DFER%20methods%20on%0Athe%20DFEW%2C%20FERV39k%2C%20and%20MAFW%20benchmarks.%20Code%20is%20publicly%20available%20at%0Ahttps%3A//github.com/zengqunzhao/DFER-CLIP.%0A&entry.1838667208=http%3A//arxiv.org/abs/2308.13382v3&entry.124074799=Read"},
{"title": "NumGrad-Pull: Numerical Gradient Guided Tri-plane Representation for\n  Surface Reconstruction from Point Clouds", "author": "Ruikai Cui and Shi Qiu and Jiawei Liu and Saeed Anwar and Nick Barnes", "abstract": "  Reconstructing continuous surfaces from unoriented and unordered 3D points is\na fundamental challenge in computer vision and graphics. Recent advancements\naddress this problem by training neural signed distance functions to pull 3D\nlocation queries to their closest points on a surface, following the predicted\nsigned distances and the analytical gradients computed by the network. In this\npaper, we introduce NumGrad-Pull, leveraging the representation capability of\ntri-plane structures to accelerate the learning of signed distance functions\nand enhance the fidelity of local details in surface reconstruction. To further\nimprove the training stability of grid-based tri-planes, we propose to exploit\nnumerical gradients, replacing conventional analytical computations.\nAdditionally, we present a progressive plane expansion strategy to facilitate\nfaster signed distance function convergence and design a data sampling strategy\nto mitigate reconstruction artifacts. Our extensive experiments across a\nvariety of benchmarks demonstrate the effectiveness and robustness of our\napproach. Code is available at https://github.com/CuiRuikai/NumGrad-Pull\n", "link": "http://arxiv.org/abs/2411.17392v1", "date": "2024-11-26", "relevancy": 2.7631, "topK": [{"title": "RTG-SLAM: Real-time 3D Reconstruction at Scale using Gaussian Splatting", "link": "http://arxiv.org/abs/2404.19706v1", "similarity": 0.5776}, {"title": "Dynamic Gaussians Mesh: Consistent Mesh Reconstruction from Monocular\n  Videos", "link": "http://arxiv.org/abs/2404.12379v2", "similarity": 0.5537}, {"title": "UGG: Unified Generative Grasping", "link": "https://arxiv.org/abs/2311.16917", "similarity": 0.5265}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20NumGrad-Pull%3A%20Numerical%20Gradient%20Guided%20Tri-plane%20Representation%20for%0A%20%20Surface%20Reconstruction%20from%20Point%20Clouds&body=Title%3A%20NumGrad-Pull%3A%20Numerical%20Gradient%20Guided%20Tri-plane%20Representation%20for%0A%20%20Surface%20Reconstruction%20from%20Point%20Clouds%0AAuthor%3A%20Ruikai%20Cui%20and%20Shi%20Qiu%20and%20Jiawei%20Liu%20and%20Saeed%20Anwar%20and%20Nick%20Barnes%0AAbstract%3A%20%20%20Reconstructing%20continuous%20surfaces%20from%20unoriented%20and%20unordered%203D%20points%20is%0Aa%20fundamental%20challenge%20in%20computer%20vision%20and%20graphics.%20Recent%20advancements%0Aaddress%20this%20problem%20by%20training%20neural%20signed%20distance%20functions%20to%20pull%203D%0Alocation%20queries%20to%20their%20closest%20points%20on%20a%20surface%2C%20following%20the%20predicted%0Asigned%20distances%20and%20the%20analytical%20gradients%20computed%20by%20the%20network.%20In%20this%0Apaper%2C%20we%20introduce%20NumGrad-Pull%2C%20leveraging%20the%20representation%20capability%20of%0Atri-plane%20structures%20to%20accelerate%20the%20learning%20of%20signed%20distance%20functions%0Aand%20enhance%20the%20fidelity%20of%20local%20details%20in%20surface%20reconstruction.%20To%20further%0Aimprove%20the%20training%20stability%20of%20grid-based%20tri-planes%2C%20we%20propose%20to%20exploit%0Anumerical%20gradients%2C%20replacing%20conventional%20analytical%20computations.%0AAdditionally%2C%20we%20present%20a%20progressive%20plane%20expansion%20strategy%20to%20facilitate%0Afaster%20signed%20distance%20function%20convergence%20and%20design%20a%20data%20sampling%20strategy%0Ato%20mitigate%20reconstruction%20artifacts.%20Our%20extensive%20experiments%20across%20a%0Avariety%20of%20benchmarks%20demonstrate%20the%20effectiveness%20and%20robustness%20of%20our%0Aapproach.%20Code%20is%20available%20at%20https%3A//github.com/CuiRuikai/NumGrad-Pull%0A%0ALink%3A%20http%3A//arxiv.org/abs/2411.17392v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DNumGrad-Pull%253A%2520Numerical%2520Gradient%2520Guided%2520Tri-plane%2520Representation%2520for%250A%2520%2520Surface%2520Reconstruction%2520from%2520Point%2520Clouds%26entry.906535625%3DRuikai%2520Cui%2520and%2520Shi%2520Qiu%2520and%2520Jiawei%2520Liu%2520and%2520Saeed%2520Anwar%2520and%2520Nick%2520Barnes%26entry.1292438233%3D%2520%2520Reconstructing%2520continuous%2520surfaces%2520from%2520unoriented%2520and%2520unordered%25203D%2520points%2520is%250Aa%2520fundamental%2520challenge%2520in%2520computer%2520vision%2520and%2520graphics.%2520Recent%2520advancements%250Aaddress%2520this%2520problem%2520by%2520training%2520neural%2520signed%2520distance%2520functions%2520to%2520pull%25203D%250Alocation%2520queries%2520to%2520their%2520closest%2520points%2520on%2520a%2520surface%252C%2520following%2520the%2520predicted%250Asigned%2520distances%2520and%2520the%2520analytical%2520gradients%2520computed%2520by%2520the%2520network.%2520In%2520this%250Apaper%252C%2520we%2520introduce%2520NumGrad-Pull%252C%2520leveraging%2520the%2520representation%2520capability%2520of%250Atri-plane%2520structures%2520to%2520accelerate%2520the%2520learning%2520of%2520signed%2520distance%2520functions%250Aand%2520enhance%2520the%2520fidelity%2520of%2520local%2520details%2520in%2520surface%2520reconstruction.%2520To%2520further%250Aimprove%2520the%2520training%2520stability%2520of%2520grid-based%2520tri-planes%252C%2520we%2520propose%2520to%2520exploit%250Anumerical%2520gradients%252C%2520replacing%2520conventional%2520analytical%2520computations.%250AAdditionally%252C%2520we%2520present%2520a%2520progressive%2520plane%2520expansion%2520strategy%2520to%2520facilitate%250Afaster%2520signed%2520distance%2520function%2520convergence%2520and%2520design%2520a%2520data%2520sampling%2520strategy%250Ato%2520mitigate%2520reconstruction%2520artifacts.%2520Our%2520extensive%2520experiments%2520across%2520a%250Avariety%2520of%2520benchmarks%2520demonstrate%2520the%2520effectiveness%2520and%2520robustness%2520of%2520our%250Aapproach.%2520Code%2520is%2520available%2520at%2520https%253A//github.com/CuiRuikai/NumGrad-Pull%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2411.17392v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=NumGrad-Pull%3A%20Numerical%20Gradient%20Guided%20Tri-plane%20Representation%20for%0A%20%20Surface%20Reconstruction%20from%20Point%20Clouds&entry.906535625=Ruikai%20Cui%20and%20Shi%20Qiu%20and%20Jiawei%20Liu%20and%20Saeed%20Anwar%20and%20Nick%20Barnes&entry.1292438233=%20%20Reconstructing%20continuous%20surfaces%20from%20unoriented%20and%20unordered%203D%20points%20is%0Aa%20fundamental%20challenge%20in%20computer%20vision%20and%20graphics.%20Recent%20advancements%0Aaddress%20this%20problem%20by%20training%20neural%20signed%20distance%20functions%20to%20pull%203D%0Alocation%20queries%20to%20their%20closest%20points%20on%20a%20surface%2C%20following%20the%20predicted%0Asigned%20distances%20and%20the%20analytical%20gradients%20computed%20by%20the%20network.%20In%20this%0Apaper%2C%20we%20introduce%20NumGrad-Pull%2C%20leveraging%20the%20representation%20capability%20of%0Atri-plane%20structures%20to%20accelerate%20the%20learning%20of%20signed%20distance%20functions%0Aand%20enhance%20the%20fidelity%20of%20local%20details%20in%20surface%20reconstruction.%20To%20further%0Aimprove%20the%20training%20stability%20of%20grid-based%20tri-planes%2C%20we%20propose%20to%20exploit%0Anumerical%20gradients%2C%20replacing%20conventional%20analytical%20computations.%0AAdditionally%2C%20we%20present%20a%20progressive%20plane%20expansion%20strategy%20to%20facilitate%0Afaster%20signed%20distance%20function%20convergence%20and%20design%20a%20data%20sampling%20strategy%0Ato%20mitigate%20reconstruction%20artifacts.%20Our%20extensive%20experiments%20across%20a%0Avariety%20of%20benchmarks%20demonstrate%20the%20effectiveness%20and%20robustness%20of%20our%0Aapproach.%20Code%20is%20available%20at%20https%3A//github.com/CuiRuikai/NumGrad-Pull%0A&entry.1838667208=http%3A//arxiv.org/abs/2411.17392v1&entry.124074799=Read"},
{"title": "CLOVER: Constrained Learning with Orthonormal Vectors for Eliminating\n  Redundancy", "author": "Fanxu Meng and Muhan Zhang", "abstract": "  To adapt a well-trained large model to downstream tasks, we propose\nconstraining learning within its original latent space by leveraging linear\ncombinations of its basis vectors. This approach ensures stable training\nwithout compromising the model's capabilities. Traditionally, constructing\northonormal bases from a matrix requires a transfer matrix, which significantly\nincreases storage and computational overhead for parameters and feature maps.\nIn this paper, we introduce Absorb and Decompose for Q, K, V, and O matrices,\nenabling their orthogonalization without the need for transfer matrices.\nFurthermore, the Absorb-Decompose operation eliminates redundant vectors,\nreducing the encoder attention parameters of Whisper-large-v3 by 46.42% without\nrequiring additional training. For parameter-efficient and stable fine-tuning,\nwe orthonormalized Q, K, V, and O and fine-tuned only the singular values,\nallowing efficient adaptation while constraining changes to the original latent\nspace. When fine-tuning LLaMA-2-7B on eight commonsense reasoning datasets, our\nmethod outperforms LoRA by 5.4% and DoRA by 4.4%.\n", "link": "http://arxiv.org/abs/2411.17426v1", "date": "2024-11-26", "relevancy": 2.7474, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5526}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5526}, {"title": "Self-supervised Photographic Image Layout Representation Learning", "link": "http://arxiv.org/abs/2403.03740v1", "similarity": 0.5432}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20CLOVER%3A%20Constrained%20Learning%20with%20Orthonormal%20Vectors%20for%20Eliminating%0A%20%20Redundancy&body=Title%3A%20CLOVER%3A%20Constrained%20Learning%20with%20Orthonormal%20Vectors%20for%20Eliminating%0A%20%20Redundancy%0AAuthor%3A%20Fanxu%20Meng%20and%20Muhan%20Zhang%0AAbstract%3A%20%20%20To%20adapt%20a%20well-trained%20large%20model%20to%20downstream%20tasks%2C%20we%20propose%0Aconstraining%20learning%20within%20its%20original%20latent%20space%20by%20leveraging%20linear%0Acombinations%20of%20its%20basis%20vectors.%20This%20approach%20ensures%20stable%20training%0Awithout%20compromising%20the%20model%27s%20capabilities.%20Traditionally%2C%20constructing%0Aorthonormal%20bases%20from%20a%20matrix%20requires%20a%20transfer%20matrix%2C%20which%20significantly%0Aincreases%20storage%20and%20computational%20overhead%20for%20parameters%20and%20feature%20maps.%0AIn%20this%20paper%2C%20we%20introduce%20Absorb%20and%20Decompose%20for%20Q%2C%20K%2C%20V%2C%20and%20O%20matrices%2C%0Aenabling%20their%20orthogonalization%20without%20the%20need%20for%20transfer%20matrices.%0AFurthermore%2C%20the%20Absorb-Decompose%20operation%20eliminates%20redundant%20vectors%2C%0Areducing%20the%20encoder%20attention%20parameters%20of%20Whisper-large-v3%20by%2046.42%25%20without%0Arequiring%20additional%20training.%20For%20parameter-efficient%20and%20stable%20fine-tuning%2C%0Awe%20orthonormalized%20Q%2C%20K%2C%20V%2C%20and%20O%20and%20fine-tuned%20only%20the%20singular%20values%2C%0Aallowing%20efficient%20adaptation%20while%20constraining%20changes%20to%20the%20original%20latent%0Aspace.%20When%20fine-tuning%20LLaMA-2-7B%20on%20eight%20commonsense%20reasoning%20datasets%2C%20our%0Amethod%20outperforms%20LoRA%20by%205.4%25%20and%20DoRA%20by%204.4%25.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2411.17426v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DCLOVER%253A%2520Constrained%2520Learning%2520with%2520Orthonormal%2520Vectors%2520for%2520Eliminating%250A%2520%2520Redundancy%26entry.906535625%3DFanxu%2520Meng%2520and%2520Muhan%2520Zhang%26entry.1292438233%3D%2520%2520To%2520adapt%2520a%2520well-trained%2520large%2520model%2520to%2520downstream%2520tasks%252C%2520we%2520propose%250Aconstraining%2520learning%2520within%2520its%2520original%2520latent%2520space%2520by%2520leveraging%2520linear%250Acombinations%2520of%2520its%2520basis%2520vectors.%2520This%2520approach%2520ensures%2520stable%2520training%250Awithout%2520compromising%2520the%2520model%2527s%2520capabilities.%2520Traditionally%252C%2520constructing%250Aorthonormal%2520bases%2520from%2520a%2520matrix%2520requires%2520a%2520transfer%2520matrix%252C%2520which%2520significantly%250Aincreases%2520storage%2520and%2520computational%2520overhead%2520for%2520parameters%2520and%2520feature%2520maps.%250AIn%2520this%2520paper%252C%2520we%2520introduce%2520Absorb%2520and%2520Decompose%2520for%2520Q%252C%2520K%252C%2520V%252C%2520and%2520O%2520matrices%252C%250Aenabling%2520their%2520orthogonalization%2520without%2520the%2520need%2520for%2520transfer%2520matrices.%250AFurthermore%252C%2520the%2520Absorb-Decompose%2520operation%2520eliminates%2520redundant%2520vectors%252C%250Areducing%2520the%2520encoder%2520attention%2520parameters%2520of%2520Whisper-large-v3%2520by%252046.42%2525%2520without%250Arequiring%2520additional%2520training.%2520For%2520parameter-efficient%2520and%2520stable%2520fine-tuning%252C%250Awe%2520orthonormalized%2520Q%252C%2520K%252C%2520V%252C%2520and%2520O%2520and%2520fine-tuned%2520only%2520the%2520singular%2520values%252C%250Aallowing%2520efficient%2520adaptation%2520while%2520constraining%2520changes%2520to%2520the%2520original%2520latent%250Aspace.%2520When%2520fine-tuning%2520LLaMA-2-7B%2520on%2520eight%2520commonsense%2520reasoning%2520datasets%252C%2520our%250Amethod%2520outperforms%2520LoRA%2520by%25205.4%2525%2520and%2520DoRA%2520by%25204.4%2525.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2411.17426v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=CLOVER%3A%20Constrained%20Learning%20with%20Orthonormal%20Vectors%20for%20Eliminating%0A%20%20Redundancy&entry.906535625=Fanxu%20Meng%20and%20Muhan%20Zhang&entry.1292438233=%20%20To%20adapt%20a%20well-trained%20large%20model%20to%20downstream%20tasks%2C%20we%20propose%0Aconstraining%20learning%20within%20its%20original%20latent%20space%20by%20leveraging%20linear%0Acombinations%20of%20its%20basis%20vectors.%20This%20approach%20ensures%20stable%20training%0Awithout%20compromising%20the%20model%27s%20capabilities.%20Traditionally%2C%20constructing%0Aorthonormal%20bases%20from%20a%20matrix%20requires%20a%20transfer%20matrix%2C%20which%20significantly%0Aincreases%20storage%20and%20computational%20overhead%20for%20parameters%20and%20feature%20maps.%0AIn%20this%20paper%2C%20we%20introduce%20Absorb%20and%20Decompose%20for%20Q%2C%20K%2C%20V%2C%20and%20O%20matrices%2C%0Aenabling%20their%20orthogonalization%20without%20the%20need%20for%20transfer%20matrices.%0AFurthermore%2C%20the%20Absorb-Decompose%20operation%20eliminates%20redundant%20vectors%2C%0Areducing%20the%20encoder%20attention%20parameters%20of%20Whisper-large-v3%20by%2046.42%25%20without%0Arequiring%20additional%20training.%20For%20parameter-efficient%20and%20stable%20fine-tuning%2C%0Awe%20orthonormalized%20Q%2C%20K%2C%20V%2C%20and%20O%20and%20fine-tuned%20only%20the%20singular%20values%2C%0Aallowing%20efficient%20adaptation%20while%20constraining%20changes%20to%20the%20original%20latent%0Aspace.%20When%20fine-tuning%20LLaMA-2-7B%20on%20eight%20commonsense%20reasoning%20datasets%2C%20our%0Amethod%20outperforms%20LoRA%20by%205.4%25%20and%20DoRA%20by%204.4%25.%0A&entry.1838667208=http%3A//arxiv.org/abs/2411.17426v1&entry.124074799=Read"},
{"title": "A Survey on Multimodal Large Language Models", "author": "Shukang Yin and Chaoyou Fu and Sirui Zhao and Ke Li and Xing Sun and Tong Xu and Enhong Chen", "abstract": "  Recently, Multimodal Large Language Model (MLLM) represented by GPT-4V has\nbeen a new rising research hotspot, which uses powerful Large Language Models\n(LLMs) as a brain to perform multimodal tasks. The surprising emergent\ncapabilities of MLLM, such as writing stories based on images and Optical\nCharacter Recognition (OCR)-free math reasoning, are rare in traditional\nmultimodal methods, suggesting a potential path to artificial general\nintelligence. To this end, both academia and industry have endeavored to\ndevelop MLLMs that can compete with or even outperform GPT-4V, pushing the\nlimit of research at a surprising speed. In this paper, we aim to trace and\nsummarize the recent progress of MLLMs. First of all, we present the basic\nformulation of MLLM and delineate its related concepts, including architecture,\ntraining strategy and data, as well as evaluation. Then, we introduce research\ntopics about how MLLMs can be extended to support more granularity, modalities,\nlanguages, and scenarios. We continue with multimodal hallucination and\nextended techniques, including Multimodal ICL (M-ICL), Multimodal CoT (M-CoT),\nand LLM-Aided Visual Reasoning (LAVR). To conclude the paper, we discuss\nexisting challenges and point out promising research directions.\n", "link": "http://arxiv.org/abs/2306.13549v3", "date": "2024-11-26", "relevancy": 2.7235, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5447}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5447}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5447}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20A%20Survey%20on%20Multimodal%20Large%20Language%20Models&body=Title%3A%20A%20Survey%20on%20Multimodal%20Large%20Language%20Models%0AAuthor%3A%20Shukang%20Yin%20and%20Chaoyou%20Fu%20and%20Sirui%20Zhao%20and%20Ke%20Li%20and%20Xing%20Sun%20and%20Tong%20Xu%20and%20Enhong%20Chen%0AAbstract%3A%20%20%20Recently%2C%20Multimodal%20Large%20Language%20Model%20%28MLLM%29%20represented%20by%20GPT-4V%20has%0Abeen%20a%20new%20rising%20research%20hotspot%2C%20which%20uses%20powerful%20Large%20Language%20Models%0A%28LLMs%29%20as%20a%20brain%20to%20perform%20multimodal%20tasks.%20The%20surprising%20emergent%0Acapabilities%20of%20MLLM%2C%20such%20as%20writing%20stories%20based%20on%20images%20and%20Optical%0ACharacter%20Recognition%20%28OCR%29-free%20math%20reasoning%2C%20are%20rare%20in%20traditional%0Amultimodal%20methods%2C%20suggesting%20a%20potential%20path%20to%20artificial%20general%0Aintelligence.%20To%20this%20end%2C%20both%20academia%20and%20industry%20have%20endeavored%20to%0Adevelop%20MLLMs%20that%20can%20compete%20with%20or%20even%20outperform%20GPT-4V%2C%20pushing%20the%0Alimit%20of%20research%20at%20a%20surprising%20speed.%20In%20this%20paper%2C%20we%20aim%20to%20trace%20and%0Asummarize%20the%20recent%20progress%20of%20MLLMs.%20First%20of%20all%2C%20we%20present%20the%20basic%0Aformulation%20of%20MLLM%20and%20delineate%20its%20related%20concepts%2C%20including%20architecture%2C%0Atraining%20strategy%20and%20data%2C%20as%20well%20as%20evaluation.%20Then%2C%20we%20introduce%20research%0Atopics%20about%20how%20MLLMs%20can%20be%20extended%20to%20support%20more%20granularity%2C%20modalities%2C%0Alanguages%2C%20and%20scenarios.%20We%20continue%20with%20multimodal%20hallucination%20and%0Aextended%20techniques%2C%20including%20Multimodal%20ICL%20%28M-ICL%29%2C%20Multimodal%20CoT%20%28M-CoT%29%2C%0Aand%20LLM-Aided%20Visual%20Reasoning%20%28LAVR%29.%20To%20conclude%20the%20paper%2C%20we%20discuss%0Aexisting%20challenges%20and%20point%20out%20promising%20research%20directions.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2306.13549v3%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DA%2520Survey%2520on%2520Multimodal%2520Large%2520Language%2520Models%26entry.906535625%3DShukang%2520Yin%2520and%2520Chaoyou%2520Fu%2520and%2520Sirui%2520Zhao%2520and%2520Ke%2520Li%2520and%2520Xing%2520Sun%2520and%2520Tong%2520Xu%2520and%2520Enhong%2520Chen%26entry.1292438233%3D%2520%2520Recently%252C%2520Multimodal%2520Large%2520Language%2520Model%2520%2528MLLM%2529%2520represented%2520by%2520GPT-4V%2520has%250Abeen%2520a%2520new%2520rising%2520research%2520hotspot%252C%2520which%2520uses%2520powerful%2520Large%2520Language%2520Models%250A%2528LLMs%2529%2520as%2520a%2520brain%2520to%2520perform%2520multimodal%2520tasks.%2520The%2520surprising%2520emergent%250Acapabilities%2520of%2520MLLM%252C%2520such%2520as%2520writing%2520stories%2520based%2520on%2520images%2520and%2520Optical%250ACharacter%2520Recognition%2520%2528OCR%2529-free%2520math%2520reasoning%252C%2520are%2520rare%2520in%2520traditional%250Amultimodal%2520methods%252C%2520suggesting%2520a%2520potential%2520path%2520to%2520artificial%2520general%250Aintelligence.%2520To%2520this%2520end%252C%2520both%2520academia%2520and%2520industry%2520have%2520endeavored%2520to%250Adevelop%2520MLLMs%2520that%2520can%2520compete%2520with%2520or%2520even%2520outperform%2520GPT-4V%252C%2520pushing%2520the%250Alimit%2520of%2520research%2520at%2520a%2520surprising%2520speed.%2520In%2520this%2520paper%252C%2520we%2520aim%2520to%2520trace%2520and%250Asummarize%2520the%2520recent%2520progress%2520of%2520MLLMs.%2520First%2520of%2520all%252C%2520we%2520present%2520the%2520basic%250Aformulation%2520of%2520MLLM%2520and%2520delineate%2520its%2520related%2520concepts%252C%2520including%2520architecture%252C%250Atraining%2520strategy%2520and%2520data%252C%2520as%2520well%2520as%2520evaluation.%2520Then%252C%2520we%2520introduce%2520research%250Atopics%2520about%2520how%2520MLLMs%2520can%2520be%2520extended%2520to%2520support%2520more%2520granularity%252C%2520modalities%252C%250Alanguages%252C%2520and%2520scenarios.%2520We%2520continue%2520with%2520multimodal%2520hallucination%2520and%250Aextended%2520techniques%252C%2520including%2520Multimodal%2520ICL%2520%2528M-ICL%2529%252C%2520Multimodal%2520CoT%2520%2528M-CoT%2529%252C%250Aand%2520LLM-Aided%2520Visual%2520Reasoning%2520%2528LAVR%2529.%2520To%2520conclude%2520the%2520paper%252C%2520we%2520discuss%250Aexisting%2520challenges%2520and%2520point%2520out%2520promising%2520research%2520directions.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2306.13549v3%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=A%20Survey%20on%20Multimodal%20Large%20Language%20Models&entry.906535625=Shukang%20Yin%20and%20Chaoyou%20Fu%20and%20Sirui%20Zhao%20and%20Ke%20Li%20and%20Xing%20Sun%20and%20Tong%20Xu%20and%20Enhong%20Chen&entry.1292438233=%20%20Recently%2C%20Multimodal%20Large%20Language%20Model%20%28MLLM%29%20represented%20by%20GPT-4V%20has%0Abeen%20a%20new%20rising%20research%20hotspot%2C%20which%20uses%20powerful%20Large%20Language%20Models%0A%28LLMs%29%20as%20a%20brain%20to%20perform%20multimodal%20tasks.%20The%20surprising%20emergent%0Acapabilities%20of%20MLLM%2C%20such%20as%20writing%20stories%20based%20on%20images%20and%20Optical%0ACharacter%20Recognition%20%28OCR%29-free%20math%20reasoning%2C%20are%20rare%20in%20traditional%0Amultimodal%20methods%2C%20suggesting%20a%20potential%20path%20to%20artificial%20general%0Aintelligence.%20To%20this%20end%2C%20both%20academia%20and%20industry%20have%20endeavored%20to%0Adevelop%20MLLMs%20that%20can%20compete%20with%20or%20even%20outperform%20GPT-4V%2C%20pushing%20the%0Alimit%20of%20research%20at%20a%20surprising%20speed.%20In%20this%20paper%2C%20we%20aim%20to%20trace%20and%0Asummarize%20the%20recent%20progress%20of%20MLLMs.%20First%20of%20all%2C%20we%20present%20the%20basic%0Aformulation%20of%20MLLM%20and%20delineate%20its%20related%20concepts%2C%20including%20architecture%2C%0Atraining%20strategy%20and%20data%2C%20as%20well%20as%20evaluation.%20Then%2C%20we%20introduce%20research%0Atopics%20about%20how%20MLLMs%20can%20be%20extended%20to%20support%20more%20granularity%2C%20modalities%2C%0Alanguages%2C%20and%20scenarios.%20We%20continue%20with%20multimodal%20hallucination%20and%0Aextended%20techniques%2C%20including%20Multimodal%20ICL%20%28M-ICL%29%2C%20Multimodal%20CoT%20%28M-CoT%29%2C%0Aand%20LLM-Aided%20Visual%20Reasoning%20%28LAVR%29.%20To%20conclude%20the%20paper%2C%20we%20discuss%0Aexisting%20challenges%20and%20point%20out%20promising%20research%20directions.%0A&entry.1838667208=http%3A//arxiv.org/abs/2306.13549v3&entry.124074799=Read"},
{"title": "vesselFM: A Foundation Model for Universal 3D Blood Vessel Segmentation", "author": "Bastian Wittmann and Yannick Wattenberg and Tamaz Amiranashvili and Suprosanna Shit and Bjoern Menze", "abstract": "  Segmenting 3D blood vessels is a critical yet challenging task in medical\nimage analysis. This is due to significant imaging modality-specific variations\nin artifacts, vascular patterns and scales, signal-to-noise ratios, and\nbackground tissues. These variations, along with domain gaps arising from\nvarying imaging protocols, limit the generalization of existing supervised\nlearning-based methods, requiring tedious voxel-level annotations for each\ndataset separately. While foundation models promise to alleviate this\nlimitation, they typically fail to generalize to the task of blood vessel\nsegmentation, posing a unique, complex problem. In this work, we present\nvesselFM, a foundation model designed specifically for the broad task of 3D\nblood vessel segmentation. Unlike previous models, vesselFM can effortlessly\ngeneralize to unseen domains. To achieve zero-shot generalization, we train\nvesselFM on three heterogeneous data sources: a large, curated annotated\ndataset, data generated by a domain randomization scheme, and data sampled from\na flow matching-based generative model. Extensive evaluations show that\nvesselFM outperforms state-of-the-art medical image segmentation foundation\nmodels across four (pre-)clinically relevant imaging modalities in zero-, one-,\nand few-shot scenarios, therefore providing a universal solution for 3D blood\nvessel segmentation.\n", "link": "http://arxiv.org/abs/2411.17386v1", "date": "2024-11-26", "relevancy": 2.714, "topK": [{"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.543}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5427}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5427}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20vesselFM%3A%20A%20Foundation%20Model%20for%20Universal%203D%20Blood%20Vessel%20Segmentation&body=Title%3A%20vesselFM%3A%20A%20Foundation%20Model%20for%20Universal%203D%20Blood%20Vessel%20Segmentation%0AAuthor%3A%20Bastian%20Wittmann%20and%20Yannick%20Wattenberg%20and%20Tamaz%20Amiranashvili%20and%20Suprosanna%20Shit%20and%20Bjoern%20Menze%0AAbstract%3A%20%20%20Segmenting%203D%20blood%20vessels%20is%20a%20critical%20yet%20challenging%20task%20in%20medical%0Aimage%20analysis.%20This%20is%20due%20to%20significant%20imaging%20modality-specific%20variations%0Ain%20artifacts%2C%20vascular%20patterns%20and%20scales%2C%20signal-to-noise%20ratios%2C%20and%0Abackground%20tissues.%20These%20variations%2C%20along%20with%20domain%20gaps%20arising%20from%0Avarying%20imaging%20protocols%2C%20limit%20the%20generalization%20of%20existing%20supervised%0Alearning-based%20methods%2C%20requiring%20tedious%20voxel-level%20annotations%20for%20each%0Adataset%20separately.%20While%20foundation%20models%20promise%20to%20alleviate%20this%0Alimitation%2C%20they%20typically%20fail%20to%20generalize%20to%20the%20task%20of%20blood%20vessel%0Asegmentation%2C%20posing%20a%20unique%2C%20complex%20problem.%20In%20this%20work%2C%20we%20present%0AvesselFM%2C%20a%20foundation%20model%20designed%20specifically%20for%20the%20broad%20task%20of%203D%0Ablood%20vessel%20segmentation.%20Unlike%20previous%20models%2C%20vesselFM%20can%20effortlessly%0Ageneralize%20to%20unseen%20domains.%20To%20achieve%20zero-shot%20generalization%2C%20we%20train%0AvesselFM%20on%20three%20heterogeneous%20data%20sources%3A%20a%20large%2C%20curated%20annotated%0Adataset%2C%20data%20generated%20by%20a%20domain%20randomization%20scheme%2C%20and%20data%20sampled%20from%0Aa%20flow%20matching-based%20generative%20model.%20Extensive%20evaluations%20show%20that%0AvesselFM%20outperforms%20state-of-the-art%20medical%20image%20segmentation%20foundation%0Amodels%20across%20four%20%28pre-%29clinically%20relevant%20imaging%20modalities%20in%20zero-%2C%20one-%2C%0Aand%20few-shot%20scenarios%2C%20therefore%20providing%20a%20universal%20solution%20for%203D%20blood%0Avessel%20segmentation.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2411.17386v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DvesselFM%253A%2520A%2520Foundation%2520Model%2520for%2520Universal%25203D%2520Blood%2520Vessel%2520Segmentation%26entry.906535625%3DBastian%2520Wittmann%2520and%2520Yannick%2520Wattenberg%2520and%2520Tamaz%2520Amiranashvili%2520and%2520Suprosanna%2520Shit%2520and%2520Bjoern%2520Menze%26entry.1292438233%3D%2520%2520Segmenting%25203D%2520blood%2520vessels%2520is%2520a%2520critical%2520yet%2520challenging%2520task%2520in%2520medical%250Aimage%2520analysis.%2520This%2520is%2520due%2520to%2520significant%2520imaging%2520modality-specific%2520variations%250Ain%2520artifacts%252C%2520vascular%2520patterns%2520and%2520scales%252C%2520signal-to-noise%2520ratios%252C%2520and%250Abackground%2520tissues.%2520These%2520variations%252C%2520along%2520with%2520domain%2520gaps%2520arising%2520from%250Avarying%2520imaging%2520protocols%252C%2520limit%2520the%2520generalization%2520of%2520existing%2520supervised%250Alearning-based%2520methods%252C%2520requiring%2520tedious%2520voxel-level%2520annotations%2520for%2520each%250Adataset%2520separately.%2520While%2520foundation%2520models%2520promise%2520to%2520alleviate%2520this%250Alimitation%252C%2520they%2520typically%2520fail%2520to%2520generalize%2520to%2520the%2520task%2520of%2520blood%2520vessel%250Asegmentation%252C%2520posing%2520a%2520unique%252C%2520complex%2520problem.%2520In%2520this%2520work%252C%2520we%2520present%250AvesselFM%252C%2520a%2520foundation%2520model%2520designed%2520specifically%2520for%2520the%2520broad%2520task%2520of%25203D%250Ablood%2520vessel%2520segmentation.%2520Unlike%2520previous%2520models%252C%2520vesselFM%2520can%2520effortlessly%250Ageneralize%2520to%2520unseen%2520domains.%2520To%2520achieve%2520zero-shot%2520generalization%252C%2520we%2520train%250AvesselFM%2520on%2520three%2520heterogeneous%2520data%2520sources%253A%2520a%2520large%252C%2520curated%2520annotated%250Adataset%252C%2520data%2520generated%2520by%2520a%2520domain%2520randomization%2520scheme%252C%2520and%2520data%2520sampled%2520from%250Aa%2520flow%2520matching-based%2520generative%2520model.%2520Extensive%2520evaluations%2520show%2520that%250AvesselFM%2520outperforms%2520state-of-the-art%2520medical%2520image%2520segmentation%2520foundation%250Amodels%2520across%2520four%2520%2528pre-%2529clinically%2520relevant%2520imaging%2520modalities%2520in%2520zero-%252C%2520one-%252C%250Aand%2520few-shot%2520scenarios%252C%2520therefore%2520providing%2520a%2520universal%2520solution%2520for%25203D%2520blood%250Avessel%2520segmentation.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2411.17386v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=vesselFM%3A%20A%20Foundation%20Model%20for%20Universal%203D%20Blood%20Vessel%20Segmentation&entry.906535625=Bastian%20Wittmann%20and%20Yannick%20Wattenberg%20and%20Tamaz%20Amiranashvili%20and%20Suprosanna%20Shit%20and%20Bjoern%20Menze&entry.1292438233=%20%20Segmenting%203D%20blood%20vessels%20is%20a%20critical%20yet%20challenging%20task%20in%20medical%0Aimage%20analysis.%20This%20is%20due%20to%20significant%20imaging%20modality-specific%20variations%0Ain%20artifacts%2C%20vascular%20patterns%20and%20scales%2C%20signal-to-noise%20ratios%2C%20and%0Abackground%20tissues.%20These%20variations%2C%20along%20with%20domain%20gaps%20arising%20from%0Avarying%20imaging%20protocols%2C%20limit%20the%20generalization%20of%20existing%20supervised%0Alearning-based%20methods%2C%20requiring%20tedious%20voxel-level%20annotations%20for%20each%0Adataset%20separately.%20While%20foundation%20models%20promise%20to%20alleviate%20this%0Alimitation%2C%20they%20typically%20fail%20to%20generalize%20to%20the%20task%20of%20blood%20vessel%0Asegmentation%2C%20posing%20a%20unique%2C%20complex%20problem.%20In%20this%20work%2C%20we%20present%0AvesselFM%2C%20a%20foundation%20model%20designed%20specifically%20for%20the%20broad%20task%20of%203D%0Ablood%20vessel%20segmentation.%20Unlike%20previous%20models%2C%20vesselFM%20can%20effortlessly%0Ageneralize%20to%20unseen%20domains.%20To%20achieve%20zero-shot%20generalization%2C%20we%20train%0AvesselFM%20on%20three%20heterogeneous%20data%20sources%3A%20a%20large%2C%20curated%20annotated%0Adataset%2C%20data%20generated%20by%20a%20domain%20randomization%20scheme%2C%20and%20data%20sampled%20from%0Aa%20flow%20matching-based%20generative%20model.%20Extensive%20evaluations%20show%20that%0AvesselFM%20outperforms%20state-of-the-art%20medical%20image%20segmentation%20foundation%0Amodels%20across%20four%20%28pre-%29clinically%20relevant%20imaging%20modalities%20in%20zero-%2C%20one-%2C%0Aand%20few-shot%20scenarios%2C%20therefore%20providing%20a%20universal%20solution%20for%203D%20blood%0Avessel%20segmentation.%0A&entry.1838667208=http%3A//arxiv.org/abs/2411.17386v1&entry.124074799=Read"},
{"title": "Self-supervised Video Instance Segmentation Can Boost Geographic Entity\n  Alignment in Historical Maps", "author": "Xue Xia and Randall Balestriero and Tao Zhang and Lorenz Hurni", "abstract": "  Tracking geographic entities from historical maps, such as buildings, offers\nvaluable insights into cultural heritage, urbanization patterns, environmental\nchanges, and various historical research endeavors. However, linking these\nentities across diverse maps remains a persistent challenge for researchers.\nTraditionally, this has been addressed through a two-step process: detecting\nentities within individual maps and then associating them via a heuristic-based\npost-processing step. In this paper, we propose a novel approach that combines\nsegmentation and association of geographic entities in historical maps using\nvideo instance segmentation (VIS). This method significantly streamlines\ngeographic entity alignment and enhances automation. However, acquiring\nhigh-quality, video-format training data for VIS models is prohibitively\nexpensive, especially for historical maps that often contain hundreds or\nthousands of geographic entities. To mitigate this challenge, we explore\nself-supervised learning (SSL) techniques to enhance VIS performance on\nhistorical maps. We evaluate the performance of VIS models under different\npretraining configurations and introduce a novel method for generating\nsynthetic videos from unlabeled historical map images for pretraining. Our\nproposed self-supervised VIS method substantially reduces the need for manual\nannotation. Experimental results demonstrate the superiority of the proposed\nself-supervised VIS approach, achieving a 24.9\\% improvement in AP and a 0.23\nincrease in F1 score compared to the model trained from scratch.\n", "link": "http://arxiv.org/abs/2411.17425v1", "date": "2024-11-26", "relevancy": 2.7087, "topK": [{"title": "Self-supervised Photographic Image Layout Representation Learning", "link": "http://arxiv.org/abs/2403.03740v1", "similarity": 0.5467}, {"title": "Weakly-guided self-supervised pretraining for temporal activity detection", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/25189", "similarity": 0.541}, {"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.5376}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Self-supervised%20Video%20Instance%20Segmentation%20Can%20Boost%20Geographic%20Entity%0A%20%20Alignment%20in%20Historical%20Maps&body=Title%3A%20Self-supervised%20Video%20Instance%20Segmentation%20Can%20Boost%20Geographic%20Entity%0A%20%20Alignment%20in%20Historical%20Maps%0AAuthor%3A%20Xue%20Xia%20and%20Randall%20Balestriero%20and%20Tao%20Zhang%20and%20Lorenz%20Hurni%0AAbstract%3A%20%20%20Tracking%20geographic%20entities%20from%20historical%20maps%2C%20such%20as%20buildings%2C%20offers%0Avaluable%20insights%20into%20cultural%20heritage%2C%20urbanization%20patterns%2C%20environmental%0Achanges%2C%20and%20various%20historical%20research%20endeavors.%20However%2C%20linking%20these%0Aentities%20across%20diverse%20maps%20remains%20a%20persistent%20challenge%20for%20researchers.%0ATraditionally%2C%20this%20has%20been%20addressed%20through%20a%20two-step%20process%3A%20detecting%0Aentities%20within%20individual%20maps%20and%20then%20associating%20them%20via%20a%20heuristic-based%0Apost-processing%20step.%20In%20this%20paper%2C%20we%20propose%20a%20novel%20approach%20that%20combines%0Asegmentation%20and%20association%20of%20geographic%20entities%20in%20historical%20maps%20using%0Avideo%20instance%20segmentation%20%28VIS%29.%20This%20method%20significantly%20streamlines%0Ageographic%20entity%20alignment%20and%20enhances%20automation.%20However%2C%20acquiring%0Ahigh-quality%2C%20video-format%20training%20data%20for%20VIS%20models%20is%20prohibitively%0Aexpensive%2C%20especially%20for%20historical%20maps%20that%20often%20contain%20hundreds%20or%0Athousands%20of%20geographic%20entities.%20To%20mitigate%20this%20challenge%2C%20we%20explore%0Aself-supervised%20learning%20%28SSL%29%20techniques%20to%20enhance%20VIS%20performance%20on%0Ahistorical%20maps.%20We%20evaluate%20the%20performance%20of%20VIS%20models%20under%20different%0Apretraining%20configurations%20and%20introduce%20a%20novel%20method%20for%20generating%0Asynthetic%20videos%20from%20unlabeled%20historical%20map%20images%20for%20pretraining.%20Our%0Aproposed%20self-supervised%20VIS%20method%20substantially%20reduces%20the%20need%20for%20manual%0Aannotation.%20Experimental%20results%20demonstrate%20the%20superiority%20of%20the%20proposed%0Aself-supervised%20VIS%20approach%2C%20achieving%20a%2024.9%5C%25%20improvement%20in%20AP%20and%20a%200.23%0Aincrease%20in%20F1%20score%20compared%20to%20the%20model%20trained%20from%20scratch.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2411.17425v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DSelf-supervised%2520Video%2520Instance%2520Segmentation%2520Can%2520Boost%2520Geographic%2520Entity%250A%2520%2520Alignment%2520in%2520Historical%2520Maps%26entry.906535625%3DXue%2520Xia%2520and%2520Randall%2520Balestriero%2520and%2520Tao%2520Zhang%2520and%2520Lorenz%2520Hurni%26entry.1292438233%3D%2520%2520Tracking%2520geographic%2520entities%2520from%2520historical%2520maps%252C%2520such%2520as%2520buildings%252C%2520offers%250Avaluable%2520insights%2520into%2520cultural%2520heritage%252C%2520urbanization%2520patterns%252C%2520environmental%250Achanges%252C%2520and%2520various%2520historical%2520research%2520endeavors.%2520However%252C%2520linking%2520these%250Aentities%2520across%2520diverse%2520maps%2520remains%2520a%2520persistent%2520challenge%2520for%2520researchers.%250ATraditionally%252C%2520this%2520has%2520been%2520addressed%2520through%2520a%2520two-step%2520process%253A%2520detecting%250Aentities%2520within%2520individual%2520maps%2520and%2520then%2520associating%2520them%2520via%2520a%2520heuristic-based%250Apost-processing%2520step.%2520In%2520this%2520paper%252C%2520we%2520propose%2520a%2520novel%2520approach%2520that%2520combines%250Asegmentation%2520and%2520association%2520of%2520geographic%2520entities%2520in%2520historical%2520maps%2520using%250Avideo%2520instance%2520segmentation%2520%2528VIS%2529.%2520This%2520method%2520significantly%2520streamlines%250Ageographic%2520entity%2520alignment%2520and%2520enhances%2520automation.%2520However%252C%2520acquiring%250Ahigh-quality%252C%2520video-format%2520training%2520data%2520for%2520VIS%2520models%2520is%2520prohibitively%250Aexpensive%252C%2520especially%2520for%2520historical%2520maps%2520that%2520often%2520contain%2520hundreds%2520or%250Athousands%2520of%2520geographic%2520entities.%2520To%2520mitigate%2520this%2520challenge%252C%2520we%2520explore%250Aself-supervised%2520learning%2520%2528SSL%2529%2520techniques%2520to%2520enhance%2520VIS%2520performance%2520on%250Ahistorical%2520maps.%2520We%2520evaluate%2520the%2520performance%2520of%2520VIS%2520models%2520under%2520different%250Apretraining%2520configurations%2520and%2520introduce%2520a%2520novel%2520method%2520for%2520generating%250Asynthetic%2520videos%2520from%2520unlabeled%2520historical%2520map%2520images%2520for%2520pretraining.%2520Our%250Aproposed%2520self-supervised%2520VIS%2520method%2520substantially%2520reduces%2520the%2520need%2520for%2520manual%250Aannotation.%2520Experimental%2520results%2520demonstrate%2520the%2520superiority%2520of%2520the%2520proposed%250Aself-supervised%2520VIS%2520approach%252C%2520achieving%2520a%252024.9%255C%2525%2520improvement%2520in%2520AP%2520and%2520a%25200.23%250Aincrease%2520in%2520F1%2520score%2520compared%2520to%2520the%2520model%2520trained%2520from%2520scratch.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2411.17425v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Self-supervised%20Video%20Instance%20Segmentation%20Can%20Boost%20Geographic%20Entity%0A%20%20Alignment%20in%20Historical%20Maps&entry.906535625=Xue%20Xia%20and%20Randall%20Balestriero%20and%20Tao%20Zhang%20and%20Lorenz%20Hurni&entry.1292438233=%20%20Tracking%20geographic%20entities%20from%20historical%20maps%2C%20such%20as%20buildings%2C%20offers%0Avaluable%20insights%20into%20cultural%20heritage%2C%20urbanization%20patterns%2C%20environmental%0Achanges%2C%20and%20various%20historical%20research%20endeavors.%20However%2C%20linking%20these%0Aentities%20across%20diverse%20maps%20remains%20a%20persistent%20challenge%20for%20researchers.%0ATraditionally%2C%20this%20has%20been%20addressed%20through%20a%20two-step%20process%3A%20detecting%0Aentities%20within%20individual%20maps%20and%20then%20associating%20them%20via%20a%20heuristic-based%0Apost-processing%20step.%20In%20this%20paper%2C%20we%20propose%20a%20novel%20approach%20that%20combines%0Asegmentation%20and%20association%20of%20geographic%20entities%20in%20historical%20maps%20using%0Avideo%20instance%20segmentation%20%28VIS%29.%20This%20method%20significantly%20streamlines%0Ageographic%20entity%20alignment%20and%20enhances%20automation.%20However%2C%20acquiring%0Ahigh-quality%2C%20video-format%20training%20data%20for%20VIS%20models%20is%20prohibitively%0Aexpensive%2C%20especially%20for%20historical%20maps%20that%20often%20contain%20hundreds%20or%0Athousands%20of%20geographic%20entities.%20To%20mitigate%20this%20challenge%2C%20we%20explore%0Aself-supervised%20learning%20%28SSL%29%20techniques%20to%20enhance%20VIS%20performance%20on%0Ahistorical%20maps.%20We%20evaluate%20the%20performance%20of%20VIS%20models%20under%20different%0Apretraining%20configurations%20and%20introduce%20a%20novel%20method%20for%20generating%0Asynthetic%20videos%20from%20unlabeled%20historical%20map%20images%20for%20pretraining.%20Our%0Aproposed%20self-supervised%20VIS%20method%20substantially%20reduces%20the%20need%20for%20manual%0Aannotation.%20Experimental%20results%20demonstrate%20the%20superiority%20of%20the%20proposed%0Aself-supervised%20VIS%20approach%2C%20achieving%20a%2024.9%5C%25%20improvement%20in%20AP%20and%20a%200.23%0Aincrease%20in%20F1%20score%20compared%20to%20the%20model%20trained%20from%20scratch.%0A&entry.1838667208=http%3A//arxiv.org/abs/2411.17425v1&entry.124074799=Read"},
{"title": "FLEX-CLIP: Feature-Level GEneration Network Enhanced CLIP for X-shot\n  Cross-modal Retrieval", "author": "Jingyou Xie and Jiayi Kuang and Zhenzhou Lin and Jiarui Ouyang and Zishuo Zhao and Ying Shen", "abstract": "  Given a query from one modality, few-shot cross-modal retrieval (CMR)\nretrieves semantically similar instances in another modality with the target\ndomain including classes that are disjoint from the source domain. Compared\nwith classical few-shot CMR methods, vision-language pretraining methods like\nCLIP have shown great few-shot or zero-shot learning performance. However, they\nstill suffer challenges due to (1) the feature degradation encountered in the\ntarget domain and (2) the extreme data imbalance. To tackle these issues, we\npropose FLEX-CLIP, a novel Feature-level Generation Network Enhanced CLIP.\nFLEX-CLIP includes two training stages. In multimodal feature generation, we\npropose a composite multimodal VAE-GAN network to capture real feature\ndistribution patterns and generate pseudo samples based on CLIP features,\naddressing data imbalance. For common space projection, we develop a gate\nresidual network to fuse CLIP features with projected features, reducing\nfeature degradation in X-shot scenarios. Experimental results on four benchmark\ndatasets show a 7%-15% improvement over state-of-the-art methods, with ablation\nstudies demonstrating enhancement of CLIP features.\n", "link": "http://arxiv.org/abs/2411.17454v1", "date": "2024-11-26", "relevancy": 2.6976, "topK": [{"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5886}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.515}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.515}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20FLEX-CLIP%3A%20Feature-Level%20GEneration%20Network%20Enhanced%20CLIP%20for%20X-shot%0A%20%20Cross-modal%20Retrieval&body=Title%3A%20FLEX-CLIP%3A%20Feature-Level%20GEneration%20Network%20Enhanced%20CLIP%20for%20X-shot%0A%20%20Cross-modal%20Retrieval%0AAuthor%3A%20Jingyou%20Xie%20and%20Jiayi%20Kuang%20and%20Zhenzhou%20Lin%20and%20Jiarui%20Ouyang%20and%20Zishuo%20Zhao%20and%20Ying%20Shen%0AAbstract%3A%20%20%20Given%20a%20query%20from%20one%20modality%2C%20few-shot%20cross-modal%20retrieval%20%28CMR%29%0Aretrieves%20semantically%20similar%20instances%20in%20another%20modality%20with%20the%20target%0Adomain%20including%20classes%20that%20are%20disjoint%20from%20the%20source%20domain.%20Compared%0Awith%20classical%20few-shot%20CMR%20methods%2C%20vision-language%20pretraining%20methods%20like%0ACLIP%20have%20shown%20great%20few-shot%20or%20zero-shot%20learning%20performance.%20However%2C%20they%0Astill%20suffer%20challenges%20due%20to%20%281%29%20the%20feature%20degradation%20encountered%20in%20the%0Atarget%20domain%20and%20%282%29%20the%20extreme%20data%20imbalance.%20To%20tackle%20these%20issues%2C%20we%0Apropose%20FLEX-CLIP%2C%20a%20novel%20Feature-level%20Generation%20Network%20Enhanced%20CLIP.%0AFLEX-CLIP%20includes%20two%20training%20stages.%20In%20multimodal%20feature%20generation%2C%20we%0Apropose%20a%20composite%20multimodal%20VAE-GAN%20network%20to%20capture%20real%20feature%0Adistribution%20patterns%20and%20generate%20pseudo%20samples%20based%20on%20CLIP%20features%2C%0Aaddressing%20data%20imbalance.%20For%20common%20space%20projection%2C%20we%20develop%20a%20gate%0Aresidual%20network%20to%20fuse%20CLIP%20features%20with%20projected%20features%2C%20reducing%0Afeature%20degradation%20in%20X-shot%20scenarios.%20Experimental%20results%20on%20four%20benchmark%0Adatasets%20show%20a%207%25-15%25%20improvement%20over%20state-of-the-art%20methods%2C%20with%20ablation%0Astudies%20demonstrating%20enhancement%20of%20CLIP%20features.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2411.17454v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DFLEX-CLIP%253A%2520Feature-Level%2520GEneration%2520Network%2520Enhanced%2520CLIP%2520for%2520X-shot%250A%2520%2520Cross-modal%2520Retrieval%26entry.906535625%3DJingyou%2520Xie%2520and%2520Jiayi%2520Kuang%2520and%2520Zhenzhou%2520Lin%2520and%2520Jiarui%2520Ouyang%2520and%2520Zishuo%2520Zhao%2520and%2520Ying%2520Shen%26entry.1292438233%3D%2520%2520Given%2520a%2520query%2520from%2520one%2520modality%252C%2520few-shot%2520cross-modal%2520retrieval%2520%2528CMR%2529%250Aretrieves%2520semantically%2520similar%2520instances%2520in%2520another%2520modality%2520with%2520the%2520target%250Adomain%2520including%2520classes%2520that%2520are%2520disjoint%2520from%2520the%2520source%2520domain.%2520Compared%250Awith%2520classical%2520few-shot%2520CMR%2520methods%252C%2520vision-language%2520pretraining%2520methods%2520like%250ACLIP%2520have%2520shown%2520great%2520few-shot%2520or%2520zero-shot%2520learning%2520performance.%2520However%252C%2520they%250Astill%2520suffer%2520challenges%2520due%2520to%2520%25281%2529%2520the%2520feature%2520degradation%2520encountered%2520in%2520the%250Atarget%2520domain%2520and%2520%25282%2529%2520the%2520extreme%2520data%2520imbalance.%2520To%2520tackle%2520these%2520issues%252C%2520we%250Apropose%2520FLEX-CLIP%252C%2520a%2520novel%2520Feature-level%2520Generation%2520Network%2520Enhanced%2520CLIP.%250AFLEX-CLIP%2520includes%2520two%2520training%2520stages.%2520In%2520multimodal%2520feature%2520generation%252C%2520we%250Apropose%2520a%2520composite%2520multimodal%2520VAE-GAN%2520network%2520to%2520capture%2520real%2520feature%250Adistribution%2520patterns%2520and%2520generate%2520pseudo%2520samples%2520based%2520on%2520CLIP%2520features%252C%250Aaddressing%2520data%2520imbalance.%2520For%2520common%2520space%2520projection%252C%2520we%2520develop%2520a%2520gate%250Aresidual%2520network%2520to%2520fuse%2520CLIP%2520features%2520with%2520projected%2520features%252C%2520reducing%250Afeature%2520degradation%2520in%2520X-shot%2520scenarios.%2520Experimental%2520results%2520on%2520four%2520benchmark%250Adatasets%2520show%2520a%25207%2525-15%2525%2520improvement%2520over%2520state-of-the-art%2520methods%252C%2520with%2520ablation%250Astudies%2520demonstrating%2520enhancement%2520of%2520CLIP%2520features.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2411.17454v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=FLEX-CLIP%3A%20Feature-Level%20GEneration%20Network%20Enhanced%20CLIP%20for%20X-shot%0A%20%20Cross-modal%20Retrieval&entry.906535625=Jingyou%20Xie%20and%20Jiayi%20Kuang%20and%20Zhenzhou%20Lin%20and%20Jiarui%20Ouyang%20and%20Zishuo%20Zhao%20and%20Ying%20Shen&entry.1292438233=%20%20Given%20a%20query%20from%20one%20modality%2C%20few-shot%20cross-modal%20retrieval%20%28CMR%29%0Aretrieves%20semantically%20similar%20instances%20in%20another%20modality%20with%20the%20target%0Adomain%20including%20classes%20that%20are%20disjoint%20from%20the%20source%20domain.%20Compared%0Awith%20classical%20few-shot%20CMR%20methods%2C%20vision-language%20pretraining%20methods%20like%0ACLIP%20have%20shown%20great%20few-shot%20or%20zero-shot%20learning%20performance.%20However%2C%20they%0Astill%20suffer%20challenges%20due%20to%20%281%29%20the%20feature%20degradation%20encountered%20in%20the%0Atarget%20domain%20and%20%282%29%20the%20extreme%20data%20imbalance.%20To%20tackle%20these%20issues%2C%20we%0Apropose%20FLEX-CLIP%2C%20a%20novel%20Feature-level%20Generation%20Network%20Enhanced%20CLIP.%0AFLEX-CLIP%20includes%20two%20training%20stages.%20In%20multimodal%20feature%20generation%2C%20we%0Apropose%20a%20composite%20multimodal%20VAE-GAN%20network%20to%20capture%20real%20feature%0Adistribution%20patterns%20and%20generate%20pseudo%20samples%20based%20on%20CLIP%20features%2C%0Aaddressing%20data%20imbalance.%20For%20common%20space%20projection%2C%20we%20develop%20a%20gate%0Aresidual%20network%20to%20fuse%20CLIP%20features%20with%20projected%20features%2C%20reducing%0Afeature%20degradation%20in%20X-shot%20scenarios.%20Experimental%20results%20on%20four%20benchmark%0Adatasets%20show%20a%207%25-15%25%20improvement%20over%20state-of-the-art%20methods%2C%20with%20ablation%0Astudies%20demonstrating%20enhancement%20of%20CLIP%20features.%0A&entry.1838667208=http%3A//arxiv.org/abs/2411.17454v1&entry.124074799=Read"},
{"title": "Identity-Preserving Text-to-Video Generation by Frequency Decomposition", "author": "Shenghai Yuan and Jinfa Huang and Xianyi He and Yunyuan Ge and Yujun Shi and Liuhan Chen and Jiebo Luo and Li Yuan", "abstract": "  Identity-preserving text-to-video (IPT2V) generation aims to create\nhigh-fidelity videos with consistent human identity. It is an important task in\nvideo generation but remains an open problem for generative models. This paper\npushes the technical frontier of IPT2V in two directions that have not been\nresolved in literature: (1) A tuning-free pipeline without tedious case-by-case\nfinetuning, and (2) A frequency-aware heuristic identity-preserving DiT-based\ncontrol scheme. We propose ConsisID, a tuning-free DiT-based controllable IPT2V\nmodel to keep human identity consistent in the generated video. Inspired by\nprior findings in frequency analysis of diffusion transformers, it employs\nidentity-control signals in the frequency domain, where facial features can be\ndecomposed into low-frequency global features and high-frequency intrinsic\nfeatures. First, from a low-frequency perspective, we introduce a global facial\nextractor, which encodes reference images and facial key points into a latent\nspace, generating features enriched with low-frequency information. These\nfeatures are then integrated into shallow layers of the network to alleviate\ntraining challenges associated with DiT. Second, from a high-frequency\nperspective, we design a local facial extractor to capture high-frequency\ndetails and inject them into transformer blocks, enhancing the model's ability\nto preserve fine-grained features. We propose a hierarchical training strategy\nto leverage frequency information for identity preservation, transforming a\nvanilla pre-trained video generation model into an IPT2V model. Extensive\nexperiments demonstrate that our frequency-aware heuristic scheme provides an\noptimal control solution for DiT-based models. Thanks to this scheme, our\nConsisID generates high-quality, identity-preserving videos, making strides\ntowards more effective IPT2V.\n", "link": "http://arxiv.org/abs/2411.17440v1", "date": "2024-11-26", "relevancy": 2.6874, "topK": [{"title": "Magic-Me: Identity-Specific Video Customized Diffusion", "link": "http://arxiv.org/abs/2402.09368v1", "similarity": 0.7243}, {"title": "Tuning-Free Noise Rectification for High Fidelity Image-to-Video\n  Generation", "link": "http://arxiv.org/abs/2403.02827v1", "similarity": 0.6451}, {"title": "Customize-A-Video: One-Shot Motion Customization of Text-to-Video\n  Diffusion Models", "link": "http://arxiv.org/abs/2402.14780v1", "similarity": 0.6075}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Identity-Preserving%20Text-to-Video%20Generation%20by%20Frequency%20Decomposition&body=Title%3A%20Identity-Preserving%20Text-to-Video%20Generation%20by%20Frequency%20Decomposition%0AAuthor%3A%20Shenghai%20Yuan%20and%20Jinfa%20Huang%20and%20Xianyi%20He%20and%20Yunyuan%20Ge%20and%20Yujun%20Shi%20and%20Liuhan%20Chen%20and%20Jiebo%20Luo%20and%20Li%20Yuan%0AAbstract%3A%20%20%20Identity-preserving%20text-to-video%20%28IPT2V%29%20generation%20aims%20to%20create%0Ahigh-fidelity%20videos%20with%20consistent%20human%20identity.%20It%20is%20an%20important%20task%20in%0Avideo%20generation%20but%20remains%20an%20open%20problem%20for%20generative%20models.%20This%20paper%0Apushes%20the%20technical%20frontier%20of%20IPT2V%20in%20two%20directions%20that%20have%20not%20been%0Aresolved%20in%20literature%3A%20%281%29%20A%20tuning-free%20pipeline%20without%20tedious%20case-by-case%0Afinetuning%2C%20and%20%282%29%20A%20frequency-aware%20heuristic%20identity-preserving%20DiT-based%0Acontrol%20scheme.%20We%20propose%20ConsisID%2C%20a%20tuning-free%20DiT-based%20controllable%20IPT2V%0Amodel%20to%20keep%20human%20identity%20consistent%20in%20the%20generated%20video.%20Inspired%20by%0Aprior%20findings%20in%20frequency%20analysis%20of%20diffusion%20transformers%2C%20it%20employs%0Aidentity-control%20signals%20in%20the%20frequency%20domain%2C%20where%20facial%20features%20can%20be%0Adecomposed%20into%20low-frequency%20global%20features%20and%20high-frequency%20intrinsic%0Afeatures.%20First%2C%20from%20a%20low-frequency%20perspective%2C%20we%20introduce%20a%20global%20facial%0Aextractor%2C%20which%20encodes%20reference%20images%20and%20facial%20key%20points%20into%20a%20latent%0Aspace%2C%20generating%20features%20enriched%20with%20low-frequency%20information.%20These%0Afeatures%20are%20then%20integrated%20into%20shallow%20layers%20of%20the%20network%20to%20alleviate%0Atraining%20challenges%20associated%20with%20DiT.%20Second%2C%20from%20a%20high-frequency%0Aperspective%2C%20we%20design%20a%20local%20facial%20extractor%20to%20capture%20high-frequency%0Adetails%20and%20inject%20them%20into%20transformer%20blocks%2C%20enhancing%20the%20model%27s%20ability%0Ato%20preserve%20fine-grained%20features.%20We%20propose%20a%20hierarchical%20training%20strategy%0Ato%20leverage%20frequency%20information%20for%20identity%20preservation%2C%20transforming%20a%0Avanilla%20pre-trained%20video%20generation%20model%20into%20an%20IPT2V%20model.%20Extensive%0Aexperiments%20demonstrate%20that%20our%20frequency-aware%20heuristic%20scheme%20provides%20an%0Aoptimal%20control%20solution%20for%20DiT-based%20models.%20Thanks%20to%20this%20scheme%2C%20our%0AConsisID%20generates%20high-quality%2C%20identity-preserving%20videos%2C%20making%20strides%0Atowards%20more%20effective%20IPT2V.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2411.17440v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DIdentity-Preserving%2520Text-to-Video%2520Generation%2520by%2520Frequency%2520Decomposition%26entry.906535625%3DShenghai%2520Yuan%2520and%2520Jinfa%2520Huang%2520and%2520Xianyi%2520He%2520and%2520Yunyuan%2520Ge%2520and%2520Yujun%2520Shi%2520and%2520Liuhan%2520Chen%2520and%2520Jiebo%2520Luo%2520and%2520Li%2520Yuan%26entry.1292438233%3D%2520%2520Identity-preserving%2520text-to-video%2520%2528IPT2V%2529%2520generation%2520aims%2520to%2520create%250Ahigh-fidelity%2520videos%2520with%2520consistent%2520human%2520identity.%2520It%2520is%2520an%2520important%2520task%2520in%250Avideo%2520generation%2520but%2520remains%2520an%2520open%2520problem%2520for%2520generative%2520models.%2520This%2520paper%250Apushes%2520the%2520technical%2520frontier%2520of%2520IPT2V%2520in%2520two%2520directions%2520that%2520have%2520not%2520been%250Aresolved%2520in%2520literature%253A%2520%25281%2529%2520A%2520tuning-free%2520pipeline%2520without%2520tedious%2520case-by-case%250Afinetuning%252C%2520and%2520%25282%2529%2520A%2520frequency-aware%2520heuristic%2520identity-preserving%2520DiT-based%250Acontrol%2520scheme.%2520We%2520propose%2520ConsisID%252C%2520a%2520tuning-free%2520DiT-based%2520controllable%2520IPT2V%250Amodel%2520to%2520keep%2520human%2520identity%2520consistent%2520in%2520the%2520generated%2520video.%2520Inspired%2520by%250Aprior%2520findings%2520in%2520frequency%2520analysis%2520of%2520diffusion%2520transformers%252C%2520it%2520employs%250Aidentity-control%2520signals%2520in%2520the%2520frequency%2520domain%252C%2520where%2520facial%2520features%2520can%2520be%250Adecomposed%2520into%2520low-frequency%2520global%2520features%2520and%2520high-frequency%2520intrinsic%250Afeatures.%2520First%252C%2520from%2520a%2520low-frequency%2520perspective%252C%2520we%2520introduce%2520a%2520global%2520facial%250Aextractor%252C%2520which%2520encodes%2520reference%2520images%2520and%2520facial%2520key%2520points%2520into%2520a%2520latent%250Aspace%252C%2520generating%2520features%2520enriched%2520with%2520low-frequency%2520information.%2520These%250Afeatures%2520are%2520then%2520integrated%2520into%2520shallow%2520layers%2520of%2520the%2520network%2520to%2520alleviate%250Atraining%2520challenges%2520associated%2520with%2520DiT.%2520Second%252C%2520from%2520a%2520high-frequency%250Aperspective%252C%2520we%2520design%2520a%2520local%2520facial%2520extractor%2520to%2520capture%2520high-frequency%250Adetails%2520and%2520inject%2520them%2520into%2520transformer%2520blocks%252C%2520enhancing%2520the%2520model%2527s%2520ability%250Ato%2520preserve%2520fine-grained%2520features.%2520We%2520propose%2520a%2520hierarchical%2520training%2520strategy%250Ato%2520leverage%2520frequency%2520information%2520for%2520identity%2520preservation%252C%2520transforming%2520a%250Avanilla%2520pre-trained%2520video%2520generation%2520model%2520into%2520an%2520IPT2V%2520model.%2520Extensive%250Aexperiments%2520demonstrate%2520that%2520our%2520frequency-aware%2520heuristic%2520scheme%2520provides%2520an%250Aoptimal%2520control%2520solution%2520for%2520DiT-based%2520models.%2520Thanks%2520to%2520this%2520scheme%252C%2520our%250AConsisID%2520generates%2520high-quality%252C%2520identity-preserving%2520videos%252C%2520making%2520strides%250Atowards%2520more%2520effective%2520IPT2V.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2411.17440v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Identity-Preserving%20Text-to-Video%20Generation%20by%20Frequency%20Decomposition&entry.906535625=Shenghai%20Yuan%20and%20Jinfa%20Huang%20and%20Xianyi%20He%20and%20Yunyuan%20Ge%20and%20Yujun%20Shi%20and%20Liuhan%20Chen%20and%20Jiebo%20Luo%20and%20Li%20Yuan&entry.1292438233=%20%20Identity-preserving%20text-to-video%20%28IPT2V%29%20generation%20aims%20to%20create%0Ahigh-fidelity%20videos%20with%20consistent%20human%20identity.%20It%20is%20an%20important%20task%20in%0Avideo%20generation%20but%20remains%20an%20open%20problem%20for%20generative%20models.%20This%20paper%0Apushes%20the%20technical%20frontier%20of%20IPT2V%20in%20two%20directions%20that%20have%20not%20been%0Aresolved%20in%20literature%3A%20%281%29%20A%20tuning-free%20pipeline%20without%20tedious%20case-by-case%0Afinetuning%2C%20and%20%282%29%20A%20frequency-aware%20heuristic%20identity-preserving%20DiT-based%0Acontrol%20scheme.%20We%20propose%20ConsisID%2C%20a%20tuning-free%20DiT-based%20controllable%20IPT2V%0Amodel%20to%20keep%20human%20identity%20consistent%20in%20the%20generated%20video.%20Inspired%20by%0Aprior%20findings%20in%20frequency%20analysis%20of%20diffusion%20transformers%2C%20it%20employs%0Aidentity-control%20signals%20in%20the%20frequency%20domain%2C%20where%20facial%20features%20can%20be%0Adecomposed%20into%20low-frequency%20global%20features%20and%20high-frequency%20intrinsic%0Afeatures.%20First%2C%20from%20a%20low-frequency%20perspective%2C%20we%20introduce%20a%20global%20facial%0Aextractor%2C%20which%20encodes%20reference%20images%20and%20facial%20key%20points%20into%20a%20latent%0Aspace%2C%20generating%20features%20enriched%20with%20low-frequency%20information.%20These%0Afeatures%20are%20then%20integrated%20into%20shallow%20layers%20of%20the%20network%20to%20alleviate%0Atraining%20challenges%20associated%20with%20DiT.%20Second%2C%20from%20a%20high-frequency%0Aperspective%2C%20we%20design%20a%20local%20facial%20extractor%20to%20capture%20high-frequency%0Adetails%20and%20inject%20them%20into%20transformer%20blocks%2C%20enhancing%20the%20model%27s%20ability%0Ato%20preserve%20fine-grained%20features.%20We%20propose%20a%20hierarchical%20training%20strategy%0Ato%20leverage%20frequency%20information%20for%20identity%20preservation%2C%20transforming%20a%0Avanilla%20pre-trained%20video%20generation%20model%20into%20an%20IPT2V%20model.%20Extensive%0Aexperiments%20demonstrate%20that%20our%20frequency-aware%20heuristic%20scheme%20provides%20an%0Aoptimal%20control%20solution%20for%20DiT-based%20models.%20Thanks%20to%20this%20scheme%2C%20our%0AConsisID%20generates%20high-quality%2C%20identity-preserving%20videos%2C%20making%20strides%0Atowards%20more%20effective%20IPT2V.%0A&entry.1838667208=http%3A//arxiv.org/abs/2411.17440v1&entry.124074799=Read"},
{"title": "Task Progressive Curriculum Learning for Robust Visual Question\n  Answering", "author": "Ahmed Akl and Abdelwahed Khamis and Zhe Wang and Ali Cheraghian and Sara Khalifa and Kewen Wang", "abstract": "  Visual Question Answering (VQA) systems are known for their poor performance\nin out-of-distribution datasets. An issue that was addressed in previous works\nthrough ensemble learning, answer re-ranking, or artificially growing the\ntraining set. In this work, we show for the first time that robust Visual\nQuestion Answering is attainable by simply enhancing the training strategy. Our\nproposed approach, Task Progressive Curriculum Learning (TPCL), breaks the main\nVQA problem into smaller, easier tasks based on the question type. Then, it\nprogressively trains the model on a (carefully crafted) sequence of tasks. We\nfurther support the method by a novel distributional-based difficulty measurer.\nOur approach is conceptually simple, model-agnostic, and easy to implement. We\ndemonstrate TPCL effectiveness through a comprehensive evaluation on standard\ndatasets. Without either data augmentation or explicit debiasing mechanism, it\nachieves state-of-the-art on VQA-CP v2, VQA-CP v1 and VQA v2 datasets.\nExtensive experiments demonstrate that TPCL outperforms the most competitive\nrobust VQA approaches by more than 5% and 7% on VQA-CP v2 and VQA-CP v1;\nrespectively. TPCL also can boost VQA baseline backbone performance by up to\n28.5%.\n", "link": "http://arxiv.org/abs/2411.17292v1", "date": "2024-11-26", "relevancy": 2.6849, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5421}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5421}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5268}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Task%20Progressive%20Curriculum%20Learning%20for%20Robust%20Visual%20Question%0A%20%20Answering&body=Title%3A%20Task%20Progressive%20Curriculum%20Learning%20for%20Robust%20Visual%20Question%0A%20%20Answering%0AAuthor%3A%20Ahmed%20Akl%20and%20Abdelwahed%20Khamis%20and%20Zhe%20Wang%20and%20Ali%20Cheraghian%20and%20Sara%20Khalifa%20and%20Kewen%20Wang%0AAbstract%3A%20%20%20Visual%20Question%20Answering%20%28VQA%29%20systems%20are%20known%20for%20their%20poor%20performance%0Ain%20out-of-distribution%20datasets.%20An%20issue%20that%20was%20addressed%20in%20previous%20works%0Athrough%20ensemble%20learning%2C%20answer%20re-ranking%2C%20or%20artificially%20growing%20the%0Atraining%20set.%20In%20this%20work%2C%20we%20show%20for%20the%20first%20time%20that%20robust%20Visual%0AQuestion%20Answering%20is%20attainable%20by%20simply%20enhancing%20the%20training%20strategy.%20Our%0Aproposed%20approach%2C%20Task%20Progressive%20Curriculum%20Learning%20%28TPCL%29%2C%20breaks%20the%20main%0AVQA%20problem%20into%20smaller%2C%20easier%20tasks%20based%20on%20the%20question%20type.%20Then%2C%20it%0Aprogressively%20trains%20the%20model%20on%20a%20%28carefully%20crafted%29%20sequence%20of%20tasks.%20We%0Afurther%20support%20the%20method%20by%20a%20novel%20distributional-based%20difficulty%20measurer.%0AOur%20approach%20is%20conceptually%20simple%2C%20model-agnostic%2C%20and%20easy%20to%20implement.%20We%0Ademonstrate%20TPCL%20effectiveness%20through%20a%20comprehensive%20evaluation%20on%20standard%0Adatasets.%20Without%20either%20data%20augmentation%20or%20explicit%20debiasing%20mechanism%2C%20it%0Aachieves%20state-of-the-art%20on%20VQA-CP%20v2%2C%20VQA-CP%20v1%20and%20VQA%20v2%20datasets.%0AExtensive%20experiments%20demonstrate%20that%20TPCL%20outperforms%20the%20most%20competitive%0Arobust%20VQA%20approaches%20by%20more%20than%205%25%20and%207%25%20on%20VQA-CP%20v2%20and%20VQA-CP%20v1%3B%0Arespectively.%20TPCL%20also%20can%20boost%20VQA%20baseline%20backbone%20performance%20by%20up%20to%0A28.5%25.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2411.17292v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DTask%2520Progressive%2520Curriculum%2520Learning%2520for%2520Robust%2520Visual%2520Question%250A%2520%2520Answering%26entry.906535625%3DAhmed%2520Akl%2520and%2520Abdelwahed%2520Khamis%2520and%2520Zhe%2520Wang%2520and%2520Ali%2520Cheraghian%2520and%2520Sara%2520Khalifa%2520and%2520Kewen%2520Wang%26entry.1292438233%3D%2520%2520Visual%2520Question%2520Answering%2520%2528VQA%2529%2520systems%2520are%2520known%2520for%2520their%2520poor%2520performance%250Ain%2520out-of-distribution%2520datasets.%2520An%2520issue%2520that%2520was%2520addressed%2520in%2520previous%2520works%250Athrough%2520ensemble%2520learning%252C%2520answer%2520re-ranking%252C%2520or%2520artificially%2520growing%2520the%250Atraining%2520set.%2520In%2520this%2520work%252C%2520we%2520show%2520for%2520the%2520first%2520time%2520that%2520robust%2520Visual%250AQuestion%2520Answering%2520is%2520attainable%2520by%2520simply%2520enhancing%2520the%2520training%2520strategy.%2520Our%250Aproposed%2520approach%252C%2520Task%2520Progressive%2520Curriculum%2520Learning%2520%2528TPCL%2529%252C%2520breaks%2520the%2520main%250AVQA%2520problem%2520into%2520smaller%252C%2520easier%2520tasks%2520based%2520on%2520the%2520question%2520type.%2520Then%252C%2520it%250Aprogressively%2520trains%2520the%2520model%2520on%2520a%2520%2528carefully%2520crafted%2529%2520sequence%2520of%2520tasks.%2520We%250Afurther%2520support%2520the%2520method%2520by%2520a%2520novel%2520distributional-based%2520difficulty%2520measurer.%250AOur%2520approach%2520is%2520conceptually%2520simple%252C%2520model-agnostic%252C%2520and%2520easy%2520to%2520implement.%2520We%250Ademonstrate%2520TPCL%2520effectiveness%2520through%2520a%2520comprehensive%2520evaluation%2520on%2520standard%250Adatasets.%2520Without%2520either%2520data%2520augmentation%2520or%2520explicit%2520debiasing%2520mechanism%252C%2520it%250Aachieves%2520state-of-the-art%2520on%2520VQA-CP%2520v2%252C%2520VQA-CP%2520v1%2520and%2520VQA%2520v2%2520datasets.%250AExtensive%2520experiments%2520demonstrate%2520that%2520TPCL%2520outperforms%2520the%2520most%2520competitive%250Arobust%2520VQA%2520approaches%2520by%2520more%2520than%25205%2525%2520and%25207%2525%2520on%2520VQA-CP%2520v2%2520and%2520VQA-CP%2520v1%253B%250Arespectively.%2520TPCL%2520also%2520can%2520boost%2520VQA%2520baseline%2520backbone%2520performance%2520by%2520up%2520to%250A28.5%2525.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2411.17292v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Task%20Progressive%20Curriculum%20Learning%20for%20Robust%20Visual%20Question%0A%20%20Answering&entry.906535625=Ahmed%20Akl%20and%20Abdelwahed%20Khamis%20and%20Zhe%20Wang%20and%20Ali%20Cheraghian%20and%20Sara%20Khalifa%20and%20Kewen%20Wang&entry.1292438233=%20%20Visual%20Question%20Answering%20%28VQA%29%20systems%20are%20known%20for%20their%20poor%20performance%0Ain%20out-of-distribution%20datasets.%20An%20issue%20that%20was%20addressed%20in%20previous%20works%0Athrough%20ensemble%20learning%2C%20answer%20re-ranking%2C%20or%20artificially%20growing%20the%0Atraining%20set.%20In%20this%20work%2C%20we%20show%20for%20the%20first%20time%20that%20robust%20Visual%0AQuestion%20Answering%20is%20attainable%20by%20simply%20enhancing%20the%20training%20strategy.%20Our%0Aproposed%20approach%2C%20Task%20Progressive%20Curriculum%20Learning%20%28TPCL%29%2C%20breaks%20the%20main%0AVQA%20problem%20into%20smaller%2C%20easier%20tasks%20based%20on%20the%20question%20type.%20Then%2C%20it%0Aprogressively%20trains%20the%20model%20on%20a%20%28carefully%20crafted%29%20sequence%20of%20tasks.%20We%0Afurther%20support%20the%20method%20by%20a%20novel%20distributional-based%20difficulty%20measurer.%0AOur%20approach%20is%20conceptually%20simple%2C%20model-agnostic%2C%20and%20easy%20to%20implement.%20We%0Ademonstrate%20TPCL%20effectiveness%20through%20a%20comprehensive%20evaluation%20on%20standard%0Adatasets.%20Without%20either%20data%20augmentation%20or%20explicit%20debiasing%20mechanism%2C%20it%0Aachieves%20state-of-the-art%20on%20VQA-CP%20v2%2C%20VQA-CP%20v1%20and%20VQA%20v2%20datasets.%0AExtensive%20experiments%20demonstrate%20that%20TPCL%20outperforms%20the%20most%20competitive%0Arobust%20VQA%20approaches%20by%20more%20than%205%25%20and%207%25%20on%20VQA-CP%20v2%20and%20VQA-CP%20v1%3B%0Arespectively.%20TPCL%20also%20can%20boost%20VQA%20baseline%20backbone%20performance%20by%20up%20to%0A28.5%25.%0A&entry.1838667208=http%3A//arxiv.org/abs/2411.17292v1&entry.124074799=Read"},
{"title": "CoA: Chain-of-Action for Generative Semantic Labels", "author": "Meng Wei and Zhongnian Li and Peng Ying and Xinzheng Xu", "abstract": "  Recent advances in vision-language models (VLM) have demonstrated remarkable\ncapability in image classification. These VLMs leverage a predefined set of\ncategories to construct text prompts for zero-shot reasoning. However, in more\nopen-ended domains like autonomous driving, using a predefined set of labels\nbecomes impractical, as the semantic label space is unknown and constantly\nevolving. Additionally, fixed embedding text prompts often tend to predict a\nsingle label (while in reality, multiple labels commonly exist per image). In\nthis paper, we introduce CoA, an innovative Chain-of-Action (CoA) method that\ngenerates labels aligned with all contextually relevant features of an image.\nCoA is designed based on the observation that enriched and valuable contextual\ninformation improves generative performance during inference. Traditional\nvision-language models tend to output singular and redundant responses.\nTherefore, we employ a tailored CoA to alleviate this problem. We first break\ndown the generative labeling task into detailed actions and construct an CoA\nleading to the final generative objective. Each action extracts and merges key\ninformation from the previous action and passes the enriched information as\ncontext to the next action, ultimately improving the VLM in generating\ncomprehensive and accurate semantic labels. We assess the effectiveness of CoA\nthrough comprehensive evaluations on widely-used benchmark datasets and the\nresults demonstrate significant improvements across key performance metrics.\n", "link": "http://arxiv.org/abs/2411.17406v1", "date": "2024-11-26", "relevancy": 2.6738, "topK": [{"title": "Genie: Generative Interactive Environments", "link": "http://arxiv.org/abs/2402.15391v1", "similarity": 0.5503}, {"title": "Self-supervised Photographic Image Layout Representation Learning", "link": "http://arxiv.org/abs/2403.03740v1", "similarity": 0.5369}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5171}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20CoA%3A%20Chain-of-Action%20for%20Generative%20Semantic%20Labels&body=Title%3A%20CoA%3A%20Chain-of-Action%20for%20Generative%20Semantic%20Labels%0AAuthor%3A%20Meng%20Wei%20and%20Zhongnian%20Li%20and%20Peng%20Ying%20and%20Xinzheng%20Xu%0AAbstract%3A%20%20%20Recent%20advances%20in%20vision-language%20models%20%28VLM%29%20have%20demonstrated%20remarkable%0Acapability%20in%20image%20classification.%20These%20VLMs%20leverage%20a%20predefined%20set%20of%0Acategories%20to%20construct%20text%20prompts%20for%20zero-shot%20reasoning.%20However%2C%20in%20more%0Aopen-ended%20domains%20like%20autonomous%20driving%2C%20using%20a%20predefined%20set%20of%20labels%0Abecomes%20impractical%2C%20as%20the%20semantic%20label%20space%20is%20unknown%20and%20constantly%0Aevolving.%20Additionally%2C%20fixed%20embedding%20text%20prompts%20often%20tend%20to%20predict%20a%0Asingle%20label%20%28while%20in%20reality%2C%20multiple%20labels%20commonly%20exist%20per%20image%29.%20In%0Athis%20paper%2C%20we%20introduce%20CoA%2C%20an%20innovative%20Chain-of-Action%20%28CoA%29%20method%20that%0Agenerates%20labels%20aligned%20with%20all%20contextually%20relevant%20features%20of%20an%20image.%0ACoA%20is%20designed%20based%20on%20the%20observation%20that%20enriched%20and%20valuable%20contextual%0Ainformation%20improves%20generative%20performance%20during%20inference.%20Traditional%0Avision-language%20models%20tend%20to%20output%20singular%20and%20redundant%20responses.%0ATherefore%2C%20we%20employ%20a%20tailored%20CoA%20to%20alleviate%20this%20problem.%20We%20first%20break%0Adown%20the%20generative%20labeling%20task%20into%20detailed%20actions%20and%20construct%20an%20CoA%0Aleading%20to%20the%20final%20generative%20objective.%20Each%20action%20extracts%20and%20merges%20key%0Ainformation%20from%20the%20previous%20action%20and%20passes%20the%20enriched%20information%20as%0Acontext%20to%20the%20next%20action%2C%20ultimately%20improving%20the%20VLM%20in%20generating%0Acomprehensive%20and%20accurate%20semantic%20labels.%20We%20assess%20the%20effectiveness%20of%20CoA%0Athrough%20comprehensive%20evaluations%20on%20widely-used%20benchmark%20datasets%20and%20the%0Aresults%20demonstrate%20significant%20improvements%20across%20key%20performance%20metrics.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2411.17406v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DCoA%253A%2520Chain-of-Action%2520for%2520Generative%2520Semantic%2520Labels%26entry.906535625%3DMeng%2520Wei%2520and%2520Zhongnian%2520Li%2520and%2520Peng%2520Ying%2520and%2520Xinzheng%2520Xu%26entry.1292438233%3D%2520%2520Recent%2520advances%2520in%2520vision-language%2520models%2520%2528VLM%2529%2520have%2520demonstrated%2520remarkable%250Acapability%2520in%2520image%2520classification.%2520These%2520VLMs%2520leverage%2520a%2520predefined%2520set%2520of%250Acategories%2520to%2520construct%2520text%2520prompts%2520for%2520zero-shot%2520reasoning.%2520However%252C%2520in%2520more%250Aopen-ended%2520domains%2520like%2520autonomous%2520driving%252C%2520using%2520a%2520predefined%2520set%2520of%2520labels%250Abecomes%2520impractical%252C%2520as%2520the%2520semantic%2520label%2520space%2520is%2520unknown%2520and%2520constantly%250Aevolving.%2520Additionally%252C%2520fixed%2520embedding%2520text%2520prompts%2520often%2520tend%2520to%2520predict%2520a%250Asingle%2520label%2520%2528while%2520in%2520reality%252C%2520multiple%2520labels%2520commonly%2520exist%2520per%2520image%2529.%2520In%250Athis%2520paper%252C%2520we%2520introduce%2520CoA%252C%2520an%2520innovative%2520Chain-of-Action%2520%2528CoA%2529%2520method%2520that%250Agenerates%2520labels%2520aligned%2520with%2520all%2520contextually%2520relevant%2520features%2520of%2520an%2520image.%250ACoA%2520is%2520designed%2520based%2520on%2520the%2520observation%2520that%2520enriched%2520and%2520valuable%2520contextual%250Ainformation%2520improves%2520generative%2520performance%2520during%2520inference.%2520Traditional%250Avision-language%2520models%2520tend%2520to%2520output%2520singular%2520and%2520redundant%2520responses.%250ATherefore%252C%2520we%2520employ%2520a%2520tailored%2520CoA%2520to%2520alleviate%2520this%2520problem.%2520We%2520first%2520break%250Adown%2520the%2520generative%2520labeling%2520task%2520into%2520detailed%2520actions%2520and%2520construct%2520an%2520CoA%250Aleading%2520to%2520the%2520final%2520generative%2520objective.%2520Each%2520action%2520extracts%2520and%2520merges%2520key%250Ainformation%2520from%2520the%2520previous%2520action%2520and%2520passes%2520the%2520enriched%2520information%2520as%250Acontext%2520to%2520the%2520next%2520action%252C%2520ultimately%2520improving%2520the%2520VLM%2520in%2520generating%250Acomprehensive%2520and%2520accurate%2520semantic%2520labels.%2520We%2520assess%2520the%2520effectiveness%2520of%2520CoA%250Athrough%2520comprehensive%2520evaluations%2520on%2520widely-used%2520benchmark%2520datasets%2520and%2520the%250Aresults%2520demonstrate%2520significant%2520improvements%2520across%2520key%2520performance%2520metrics.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2411.17406v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=CoA%3A%20Chain-of-Action%20for%20Generative%20Semantic%20Labels&entry.906535625=Meng%20Wei%20and%20Zhongnian%20Li%20and%20Peng%20Ying%20and%20Xinzheng%20Xu&entry.1292438233=%20%20Recent%20advances%20in%20vision-language%20models%20%28VLM%29%20have%20demonstrated%20remarkable%0Acapability%20in%20image%20classification.%20These%20VLMs%20leverage%20a%20predefined%20set%20of%0Acategories%20to%20construct%20text%20prompts%20for%20zero-shot%20reasoning.%20However%2C%20in%20more%0Aopen-ended%20domains%20like%20autonomous%20driving%2C%20using%20a%20predefined%20set%20of%20labels%0Abecomes%20impractical%2C%20as%20the%20semantic%20label%20space%20is%20unknown%20and%20constantly%0Aevolving.%20Additionally%2C%20fixed%20embedding%20text%20prompts%20often%20tend%20to%20predict%20a%0Asingle%20label%20%28while%20in%20reality%2C%20multiple%20labels%20commonly%20exist%20per%20image%29.%20In%0Athis%20paper%2C%20we%20introduce%20CoA%2C%20an%20innovative%20Chain-of-Action%20%28CoA%29%20method%20that%0Agenerates%20labels%20aligned%20with%20all%20contextually%20relevant%20features%20of%20an%20image.%0ACoA%20is%20designed%20based%20on%20the%20observation%20that%20enriched%20and%20valuable%20contextual%0Ainformation%20improves%20generative%20performance%20during%20inference.%20Traditional%0Avision-language%20models%20tend%20to%20output%20singular%20and%20redundant%20responses.%0ATherefore%2C%20we%20employ%20a%20tailored%20CoA%20to%20alleviate%20this%20problem.%20We%20first%20break%0Adown%20the%20generative%20labeling%20task%20into%20detailed%20actions%20and%20construct%20an%20CoA%0Aleading%20to%20the%20final%20generative%20objective.%20Each%20action%20extracts%20and%20merges%20key%0Ainformation%20from%20the%20previous%20action%20and%20passes%20the%20enriched%20information%20as%0Acontext%20to%20the%20next%20action%2C%20ultimately%20improving%20the%20VLM%20in%20generating%0Acomprehensive%20and%20accurate%20semantic%20labels.%20We%20assess%20the%20effectiveness%20of%20CoA%0Athrough%20comprehensive%20evaluations%20on%20widely-used%20benchmark%20datasets%20and%20the%0Aresults%20demonstrate%20significant%20improvements%20across%20key%20performance%20metrics.%0A&entry.1838667208=http%3A//arxiv.org/abs/2411.17406v1&entry.124074799=Read"},
{"title": "Monocular Lane Detection Based on Deep Learning: A Survey", "author": "Xin He and Haiyun Guo and Kuan Zhu and Bingke Zhu and Xu Zhao and Jianwu Fang and Jinqiao Wang", "abstract": "  Lane detection plays an important role in autonomous driving perception\nsystems. As deep learning algorithms gain popularity, monocular lane detection\nmethods based on deep learning have demonstrated superior performance and\nemerged as a key research direction in autonomous driving perception. The core\ndesign of these algorithmic frameworks can be summarized as follows: (1) Task\nparadigm, focusing on lane instance-level discrimination; (2) Lane modeling,\nrepresenting lanes as a set of learnable parameters in the neural network; (3)\nGlobal context supplementation, enhancing the detection of obscure lanes; (4)\nPerspective effect elimination, providing 3D lanes usable for downstream\napplications. From these perspectives, this paper presents a comprehensive\noverview of existing methods, encompassing both the increasingly mature 2D lane\ndetection approaches and the developing 3D lane detection works. For a\nrelatively fair comparison, in addition to comparing the performance of\nmainstream methods on different benchmarks, their inference speed is also\ninvestigated under a unified setting. Moreover, we present some extended works\non lane detection, including multi-task perception, video lane detection,\nonline high-definition map construction, and lane topology reasoning, to offer\nreaders a comprehensive roadmap for the evolution of lane detection. Finally,\nwe point out some potential future research directions in this field. We\nexhaustively collect the papers and codes of existing works at\nhttps://github.com/Core9724/Awesome-Lane-Detection and will keep tracing the\nresearch.\n", "link": "http://arxiv.org/abs/2411.16316v2", "date": "2024-11-26", "relevancy": 2.6736, "topK": [{"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.5389}, {"title": "Self-supervised Photographic Image Layout Representation Learning", "link": "http://arxiv.org/abs/2403.03740v1", "similarity": 0.5386}, {"title": "DDM-NET: End-to-end learning of keypoint feature Detection, Description and Matching for 3D localization", "link": "https://arxiv.org/abs/2212.04575", "similarity": 0.5267}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Monocular%20Lane%20Detection%20Based%20on%20Deep%20Learning%3A%20A%20Survey&body=Title%3A%20Monocular%20Lane%20Detection%20Based%20on%20Deep%20Learning%3A%20A%20Survey%0AAuthor%3A%20Xin%20He%20and%20Haiyun%20Guo%20and%20Kuan%20Zhu%20and%20Bingke%20Zhu%20and%20Xu%20Zhao%20and%20Jianwu%20Fang%20and%20Jinqiao%20Wang%0AAbstract%3A%20%20%20Lane%20detection%20plays%20an%20important%20role%20in%20autonomous%20driving%20perception%0Asystems.%20As%20deep%20learning%20algorithms%20gain%20popularity%2C%20monocular%20lane%20detection%0Amethods%20based%20on%20deep%20learning%20have%20demonstrated%20superior%20performance%20and%0Aemerged%20as%20a%20key%20research%20direction%20in%20autonomous%20driving%20perception.%20The%20core%0Adesign%20of%20these%20algorithmic%20frameworks%20can%20be%20summarized%20as%20follows%3A%20%281%29%20Task%0Aparadigm%2C%20focusing%20on%20lane%20instance-level%20discrimination%3B%20%282%29%20Lane%20modeling%2C%0Arepresenting%20lanes%20as%20a%20set%20of%20learnable%20parameters%20in%20the%20neural%20network%3B%20%283%29%0AGlobal%20context%20supplementation%2C%20enhancing%20the%20detection%20of%20obscure%20lanes%3B%20%284%29%0APerspective%20effect%20elimination%2C%20providing%203D%20lanes%20usable%20for%20downstream%0Aapplications.%20From%20these%20perspectives%2C%20this%20paper%20presents%20a%20comprehensive%0Aoverview%20of%20existing%20methods%2C%20encompassing%20both%20the%20increasingly%20mature%202D%20lane%0Adetection%20approaches%20and%20the%20developing%203D%20lane%20detection%20works.%20For%20a%0Arelatively%20fair%20comparison%2C%20in%20addition%20to%20comparing%20the%20performance%20of%0Amainstream%20methods%20on%20different%20benchmarks%2C%20their%20inference%20speed%20is%20also%0Ainvestigated%20under%20a%20unified%20setting.%20Moreover%2C%20we%20present%20some%20extended%20works%0Aon%20lane%20detection%2C%20including%20multi-task%20perception%2C%20video%20lane%20detection%2C%0Aonline%20high-definition%20map%20construction%2C%20and%20lane%20topology%20reasoning%2C%20to%20offer%0Areaders%20a%20comprehensive%20roadmap%20for%20the%20evolution%20of%20lane%20detection.%20Finally%2C%0Awe%20point%20out%20some%20potential%20future%20research%20directions%20in%20this%20field.%20We%0Aexhaustively%20collect%20the%20papers%20and%20codes%20of%20existing%20works%20at%0Ahttps%3A//github.com/Core9724/Awesome-Lane-Detection%20and%20will%20keep%20tracing%20the%0Aresearch.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2411.16316v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DMonocular%2520Lane%2520Detection%2520Based%2520on%2520Deep%2520Learning%253A%2520A%2520Survey%26entry.906535625%3DXin%2520He%2520and%2520Haiyun%2520Guo%2520and%2520Kuan%2520Zhu%2520and%2520Bingke%2520Zhu%2520and%2520Xu%2520Zhao%2520and%2520Jianwu%2520Fang%2520and%2520Jinqiao%2520Wang%26entry.1292438233%3D%2520%2520Lane%2520detection%2520plays%2520an%2520important%2520role%2520in%2520autonomous%2520driving%2520perception%250Asystems.%2520As%2520deep%2520learning%2520algorithms%2520gain%2520popularity%252C%2520monocular%2520lane%2520detection%250Amethods%2520based%2520on%2520deep%2520learning%2520have%2520demonstrated%2520superior%2520performance%2520and%250Aemerged%2520as%2520a%2520key%2520research%2520direction%2520in%2520autonomous%2520driving%2520perception.%2520The%2520core%250Adesign%2520of%2520these%2520algorithmic%2520frameworks%2520can%2520be%2520summarized%2520as%2520follows%253A%2520%25281%2529%2520Task%250Aparadigm%252C%2520focusing%2520on%2520lane%2520instance-level%2520discrimination%253B%2520%25282%2529%2520Lane%2520modeling%252C%250Arepresenting%2520lanes%2520as%2520a%2520set%2520of%2520learnable%2520parameters%2520in%2520the%2520neural%2520network%253B%2520%25283%2529%250AGlobal%2520context%2520supplementation%252C%2520enhancing%2520the%2520detection%2520of%2520obscure%2520lanes%253B%2520%25284%2529%250APerspective%2520effect%2520elimination%252C%2520providing%25203D%2520lanes%2520usable%2520for%2520downstream%250Aapplications.%2520From%2520these%2520perspectives%252C%2520this%2520paper%2520presents%2520a%2520comprehensive%250Aoverview%2520of%2520existing%2520methods%252C%2520encompassing%2520both%2520the%2520increasingly%2520mature%25202D%2520lane%250Adetection%2520approaches%2520and%2520the%2520developing%25203D%2520lane%2520detection%2520works.%2520For%2520a%250Arelatively%2520fair%2520comparison%252C%2520in%2520addition%2520to%2520comparing%2520the%2520performance%2520of%250Amainstream%2520methods%2520on%2520different%2520benchmarks%252C%2520their%2520inference%2520speed%2520is%2520also%250Ainvestigated%2520under%2520a%2520unified%2520setting.%2520Moreover%252C%2520we%2520present%2520some%2520extended%2520works%250Aon%2520lane%2520detection%252C%2520including%2520multi-task%2520perception%252C%2520video%2520lane%2520detection%252C%250Aonline%2520high-definition%2520map%2520construction%252C%2520and%2520lane%2520topology%2520reasoning%252C%2520to%2520offer%250Areaders%2520a%2520comprehensive%2520roadmap%2520for%2520the%2520evolution%2520of%2520lane%2520detection.%2520Finally%252C%250Awe%2520point%2520out%2520some%2520potential%2520future%2520research%2520directions%2520in%2520this%2520field.%2520We%250Aexhaustively%2520collect%2520the%2520papers%2520and%2520codes%2520of%2520existing%2520works%2520at%250Ahttps%253A//github.com/Core9724/Awesome-Lane-Detection%2520and%2520will%2520keep%2520tracing%2520the%250Aresearch.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2411.16316v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Monocular%20Lane%20Detection%20Based%20on%20Deep%20Learning%3A%20A%20Survey&entry.906535625=Xin%20He%20and%20Haiyun%20Guo%20and%20Kuan%20Zhu%20and%20Bingke%20Zhu%20and%20Xu%20Zhao%20and%20Jianwu%20Fang%20and%20Jinqiao%20Wang&entry.1292438233=%20%20Lane%20detection%20plays%20an%20important%20role%20in%20autonomous%20driving%20perception%0Asystems.%20As%20deep%20learning%20algorithms%20gain%20popularity%2C%20monocular%20lane%20detection%0Amethods%20based%20on%20deep%20learning%20have%20demonstrated%20superior%20performance%20and%0Aemerged%20as%20a%20key%20research%20direction%20in%20autonomous%20driving%20perception.%20The%20core%0Adesign%20of%20these%20algorithmic%20frameworks%20can%20be%20summarized%20as%20follows%3A%20%281%29%20Task%0Aparadigm%2C%20focusing%20on%20lane%20instance-level%20discrimination%3B%20%282%29%20Lane%20modeling%2C%0Arepresenting%20lanes%20as%20a%20set%20of%20learnable%20parameters%20in%20the%20neural%20network%3B%20%283%29%0AGlobal%20context%20supplementation%2C%20enhancing%20the%20detection%20of%20obscure%20lanes%3B%20%284%29%0APerspective%20effect%20elimination%2C%20providing%203D%20lanes%20usable%20for%20downstream%0Aapplications.%20From%20these%20perspectives%2C%20this%20paper%20presents%20a%20comprehensive%0Aoverview%20of%20existing%20methods%2C%20encompassing%20both%20the%20increasingly%20mature%202D%20lane%0Adetection%20approaches%20and%20the%20developing%203D%20lane%20detection%20works.%20For%20a%0Arelatively%20fair%20comparison%2C%20in%20addition%20to%20comparing%20the%20performance%20of%0Amainstream%20methods%20on%20different%20benchmarks%2C%20their%20inference%20speed%20is%20also%0Ainvestigated%20under%20a%20unified%20setting.%20Moreover%2C%20we%20present%20some%20extended%20works%0Aon%20lane%20detection%2C%20including%20multi-task%20perception%2C%20video%20lane%20detection%2C%0Aonline%20high-definition%20map%20construction%2C%20and%20lane%20topology%20reasoning%2C%20to%20offer%0Areaders%20a%20comprehensive%20roadmap%20for%20the%20evolution%20of%20lane%20detection.%20Finally%2C%0Awe%20point%20out%20some%20potential%20future%20research%20directions%20in%20this%20field.%20We%0Aexhaustively%20collect%20the%20papers%20and%20codes%20of%20existing%20works%20at%0Ahttps%3A//github.com/Core9724/Awesome-Lane-Detection%20and%20will%20keep%20tracing%20the%0Aresearch.%0A&entry.1838667208=http%3A//arxiv.org/abs/2411.16316v2&entry.124074799=Read"},
{"title": "A Generalized Unified Skew-Normal Process with Neural Bayes Inference", "author": "Kesen Wang and Marc G. Genton", "abstract": "  In recent decades, statisticians have been increasingly encountering spatial\ndata that exhibit non-Gaussian behaviors such as asymmetry and\nheavy-tailedness. As a result, the assumptions of symmetry and fixed tail\nweight in Gaussian processes have become restrictive and may fail to capture\nthe intrinsic properties of the data. To address the limitations of the\nGaussian models, a variety of skewed models has been proposed, of which the\npopularity has grown rapidly. These skewed models introduce parameters that\ngovern skewness and tail weight. Among various proposals in the literature,\nunified skewed distributions, such as the Unified Skew-Normal (SUN), have\nreceived considerable attention. In this work, we revisit a more concise and\nintepretable re-parameterization of the SUN distribution and apply the\ndistribution to random fields by constructing a generalized unified skew-normal\n(GSUN) spatial process. We demonstrate { that the GSUN is a valid spatial\nprocess by showing its vanishing correlation in large distances} and provide\nthe corresponding spatial interpolation method. In addition, we develop an\ninference mechanism for the GSUN process using the concept of neural Bayes\nestimators with deep graphical attention networks (GATs) and encoder\ntransformer. We show the superiority of our proposed estimator over the\nconventional CNN-based architectures regarding stability and accuracy by means\nof a simulation study and application to Pb-contaminated soil data.\nFurthermore, we show that the GSUN process is different from the conventional\nGaussian processes and Tukey g-and-h processes, through the probability\nintegral transform (PIT).\n", "link": "http://arxiv.org/abs/2411.17400v1", "date": "2024-11-26", "relevancy": 2.6539, "topK": [{"title": "Dynamic Gaussians Mesh: Consistent Mesh Reconstruction from Monocular\n  Videos", "link": "http://arxiv.org/abs/2404.12379v2", "similarity": 0.5368}, {"title": "GST: Precise 3D Human Body from a Single Image with Gaussian Splatting\n  Transformers", "link": "http://arxiv.org/abs/2409.04196v1", "similarity": 0.5279}, {"title": "MiraGe: Editable 2D Images using Gaussian Splatting", "link": "http://arxiv.org/abs/2410.01521v1", "similarity": 0.5276}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20A%20Generalized%20Unified%20Skew-Normal%20Process%20with%20Neural%20Bayes%20Inference&body=Title%3A%20A%20Generalized%20Unified%20Skew-Normal%20Process%20with%20Neural%20Bayes%20Inference%0AAuthor%3A%20Kesen%20Wang%20and%20Marc%20G.%20Genton%0AAbstract%3A%20%20%20In%20recent%20decades%2C%20statisticians%20have%20been%20increasingly%20encountering%20spatial%0Adata%20that%20exhibit%20non-Gaussian%20behaviors%20such%20as%20asymmetry%20and%0Aheavy-tailedness.%20As%20a%20result%2C%20the%20assumptions%20of%20symmetry%20and%20fixed%20tail%0Aweight%20in%20Gaussian%20processes%20have%20become%20restrictive%20and%20may%20fail%20to%20capture%0Athe%20intrinsic%20properties%20of%20the%20data.%20To%20address%20the%20limitations%20of%20the%0AGaussian%20models%2C%20a%20variety%20of%20skewed%20models%20has%20been%20proposed%2C%20of%20which%20the%0Apopularity%20has%20grown%20rapidly.%20These%20skewed%20models%20introduce%20parameters%20that%0Agovern%20skewness%20and%20tail%20weight.%20Among%20various%20proposals%20in%20the%20literature%2C%0Aunified%20skewed%20distributions%2C%20such%20as%20the%20Unified%20Skew-Normal%20%28SUN%29%2C%20have%0Areceived%20considerable%20attention.%20In%20this%20work%2C%20we%20revisit%20a%20more%20concise%20and%0Aintepretable%20re-parameterization%20of%20the%20SUN%20distribution%20and%20apply%20the%0Adistribution%20to%20random%20fields%20by%20constructing%20a%20generalized%20unified%20skew-normal%0A%28GSUN%29%20spatial%20process.%20We%20demonstrate%20%7B%20that%20the%20GSUN%20is%20a%20valid%20spatial%0Aprocess%20by%20showing%20its%20vanishing%20correlation%20in%20large%20distances%7D%20and%20provide%0Athe%20corresponding%20spatial%20interpolation%20method.%20In%20addition%2C%20we%20develop%20an%0Ainference%20mechanism%20for%20the%20GSUN%20process%20using%20the%20concept%20of%20neural%20Bayes%0Aestimators%20with%20deep%20graphical%20attention%20networks%20%28GATs%29%20and%20encoder%0Atransformer.%20We%20show%20the%20superiority%20of%20our%20proposed%20estimator%20over%20the%0Aconventional%20CNN-based%20architectures%20regarding%20stability%20and%20accuracy%20by%20means%0Aof%20a%20simulation%20study%20and%20application%20to%20Pb-contaminated%20soil%20data.%0AFurthermore%2C%20we%20show%20that%20the%20GSUN%20process%20is%20different%20from%20the%20conventional%0AGaussian%20processes%20and%20Tukey%20g-and-h%20processes%2C%20through%20the%20probability%0Aintegral%20transform%20%28PIT%29.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2411.17400v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DA%2520Generalized%2520Unified%2520Skew-Normal%2520Process%2520with%2520Neural%2520Bayes%2520Inference%26entry.906535625%3DKesen%2520Wang%2520and%2520Marc%2520G.%2520Genton%26entry.1292438233%3D%2520%2520In%2520recent%2520decades%252C%2520statisticians%2520have%2520been%2520increasingly%2520encountering%2520spatial%250Adata%2520that%2520exhibit%2520non-Gaussian%2520behaviors%2520such%2520as%2520asymmetry%2520and%250Aheavy-tailedness.%2520As%2520a%2520result%252C%2520the%2520assumptions%2520of%2520symmetry%2520and%2520fixed%2520tail%250Aweight%2520in%2520Gaussian%2520processes%2520have%2520become%2520restrictive%2520and%2520may%2520fail%2520to%2520capture%250Athe%2520intrinsic%2520properties%2520of%2520the%2520data.%2520To%2520address%2520the%2520limitations%2520of%2520the%250AGaussian%2520models%252C%2520a%2520variety%2520of%2520skewed%2520models%2520has%2520been%2520proposed%252C%2520of%2520which%2520the%250Apopularity%2520has%2520grown%2520rapidly.%2520These%2520skewed%2520models%2520introduce%2520parameters%2520that%250Agovern%2520skewness%2520and%2520tail%2520weight.%2520Among%2520various%2520proposals%2520in%2520the%2520literature%252C%250Aunified%2520skewed%2520distributions%252C%2520such%2520as%2520the%2520Unified%2520Skew-Normal%2520%2528SUN%2529%252C%2520have%250Areceived%2520considerable%2520attention.%2520In%2520this%2520work%252C%2520we%2520revisit%2520a%2520more%2520concise%2520and%250Aintepretable%2520re-parameterization%2520of%2520the%2520SUN%2520distribution%2520and%2520apply%2520the%250Adistribution%2520to%2520random%2520fields%2520by%2520constructing%2520a%2520generalized%2520unified%2520skew-normal%250A%2528GSUN%2529%2520spatial%2520process.%2520We%2520demonstrate%2520%257B%2520that%2520the%2520GSUN%2520is%2520a%2520valid%2520spatial%250Aprocess%2520by%2520showing%2520its%2520vanishing%2520correlation%2520in%2520large%2520distances%257D%2520and%2520provide%250Athe%2520corresponding%2520spatial%2520interpolation%2520method.%2520In%2520addition%252C%2520we%2520develop%2520an%250Ainference%2520mechanism%2520for%2520the%2520GSUN%2520process%2520using%2520the%2520concept%2520of%2520neural%2520Bayes%250Aestimators%2520with%2520deep%2520graphical%2520attention%2520networks%2520%2528GATs%2529%2520and%2520encoder%250Atransformer.%2520We%2520show%2520the%2520superiority%2520of%2520our%2520proposed%2520estimator%2520over%2520the%250Aconventional%2520CNN-based%2520architectures%2520regarding%2520stability%2520and%2520accuracy%2520by%2520means%250Aof%2520a%2520simulation%2520study%2520and%2520application%2520to%2520Pb-contaminated%2520soil%2520data.%250AFurthermore%252C%2520we%2520show%2520that%2520the%2520GSUN%2520process%2520is%2520different%2520from%2520the%2520conventional%250AGaussian%2520processes%2520and%2520Tukey%2520g-and-h%2520processes%252C%2520through%2520the%2520probability%250Aintegral%2520transform%2520%2528PIT%2529.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2411.17400v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=A%20Generalized%20Unified%20Skew-Normal%20Process%20with%20Neural%20Bayes%20Inference&entry.906535625=Kesen%20Wang%20and%20Marc%20G.%20Genton&entry.1292438233=%20%20In%20recent%20decades%2C%20statisticians%20have%20been%20increasingly%20encountering%20spatial%0Adata%20that%20exhibit%20non-Gaussian%20behaviors%20such%20as%20asymmetry%20and%0Aheavy-tailedness.%20As%20a%20result%2C%20the%20assumptions%20of%20symmetry%20and%20fixed%20tail%0Aweight%20in%20Gaussian%20processes%20have%20become%20restrictive%20and%20may%20fail%20to%20capture%0Athe%20intrinsic%20properties%20of%20the%20data.%20To%20address%20the%20limitations%20of%20the%0AGaussian%20models%2C%20a%20variety%20of%20skewed%20models%20has%20been%20proposed%2C%20of%20which%20the%0Apopularity%20has%20grown%20rapidly.%20These%20skewed%20models%20introduce%20parameters%20that%0Agovern%20skewness%20and%20tail%20weight.%20Among%20various%20proposals%20in%20the%20literature%2C%0Aunified%20skewed%20distributions%2C%20such%20as%20the%20Unified%20Skew-Normal%20%28SUN%29%2C%20have%0Areceived%20considerable%20attention.%20In%20this%20work%2C%20we%20revisit%20a%20more%20concise%20and%0Aintepretable%20re-parameterization%20of%20the%20SUN%20distribution%20and%20apply%20the%0Adistribution%20to%20random%20fields%20by%20constructing%20a%20generalized%20unified%20skew-normal%0A%28GSUN%29%20spatial%20process.%20We%20demonstrate%20%7B%20that%20the%20GSUN%20is%20a%20valid%20spatial%0Aprocess%20by%20showing%20its%20vanishing%20correlation%20in%20large%20distances%7D%20and%20provide%0Athe%20corresponding%20spatial%20interpolation%20method.%20In%20addition%2C%20we%20develop%20an%0Ainference%20mechanism%20for%20the%20GSUN%20process%20using%20the%20concept%20of%20neural%20Bayes%0Aestimators%20with%20deep%20graphical%20attention%20networks%20%28GATs%29%20and%20encoder%0Atransformer.%20We%20show%20the%20superiority%20of%20our%20proposed%20estimator%20over%20the%0Aconventional%20CNN-based%20architectures%20regarding%20stability%20and%20accuracy%20by%20means%0Aof%20a%20simulation%20study%20and%20application%20to%20Pb-contaminated%20soil%20data.%0AFurthermore%2C%20we%20show%20that%20the%20GSUN%20process%20is%20different%20from%20the%20conventional%0AGaussian%20processes%20and%20Tukey%20g-and-h%20processes%2C%20through%20the%20probability%0Aintegral%20transform%20%28PIT%29.%0A&entry.1838667208=http%3A//arxiv.org/abs/2411.17400v1&entry.124074799=Read"},
{"title": "Optimizing Brain Tumor Segmentation with MedNeXt: BraTS 2024 SSA and\n  Pediatrics", "author": "Sarim Hashmi and Juan Lugo and Abdelrahman Elsayed and Dinesh Saggurthi and Mohammed Elseiagy and Alikhan Nurkamal and Jaskaran Walia and Fadillah Adamsyah Maani and Mohammad Yaqub", "abstract": "  Identifying key pathological features in brain MRIs is crucial for the\nlong-term survival of glioma patients. However, manual segmentation is\ntime-consuming, requiring expert intervention and is susceptible to human\nerror. Therefore, significant research has been devoted to developing machine\nlearning methods that can accurately segment tumors in 3D multimodal brain MRI\nscans. Despite their progress, state-of-the-art models are often limited by the\ndata they are trained on, raising concerns about their reliability when applied\nto diverse populations that may introduce distribution shifts. Such shifts can\nstem from lower quality MRI technology (e.g., in sub-Saharan Africa) or\nvariations in patient demographics (e.g., children). The BraTS-2024 challenge\nprovides a platform to address these issues. This study presents our\nmethodology for segmenting tumors in the BraTS-2024 SSA and Pediatric Tumors\ntasks using MedNeXt, comprehensive model ensembling, and thorough\npostprocessing. Our approach demonstrated strong performance on the unseen\nvalidation set, achieving an average Dice Similarity Coefficient (DSC) of 0.896\non the BraTS-2024 SSA dataset and an average DSC of 0.830 on the BraTS\nPediatric Tumor dataset. Additionally, our method achieved an average Hausdorff\nDistance (HD95) of 14.682 on the BraTS-2024 SSA dataset and an average HD95 of\n37.508 on the BraTS Pediatric dataset. Our GitHub repository can be accessed\nhere: Project Repository :\nhttps://github.com/python-arch/BioMbz-Optimizing-Brain-Tumor-Segmentation-with-MedNeXt-BraTS-2024-SSA-and-Pediatrics\n", "link": "http://arxiv.org/abs/2411.15872v2", "date": "2024-11-26", "relevancy": 2.6512, "topK": [{"title": "GST: Precise 3D Human Body from a Single Image with Gaussian Splatting\n  Transformers", "link": "http://arxiv.org/abs/2409.04196v1", "similarity": 0.5512}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5198}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5198}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Optimizing%20Brain%20Tumor%20Segmentation%20with%20MedNeXt%3A%20BraTS%202024%20SSA%20and%0A%20%20Pediatrics&body=Title%3A%20Optimizing%20Brain%20Tumor%20Segmentation%20with%20MedNeXt%3A%20BraTS%202024%20SSA%20and%0A%20%20Pediatrics%0AAuthor%3A%20Sarim%20Hashmi%20and%20Juan%20Lugo%20and%20Abdelrahman%20Elsayed%20and%20Dinesh%20Saggurthi%20and%20Mohammed%20Elseiagy%20and%20Alikhan%20Nurkamal%20and%20Jaskaran%20Walia%20and%20Fadillah%20Adamsyah%20Maani%20and%20Mohammad%20Yaqub%0AAbstract%3A%20%20%20Identifying%20key%20pathological%20features%20in%20brain%20MRIs%20is%20crucial%20for%20the%0Along-term%20survival%20of%20glioma%20patients.%20However%2C%20manual%20segmentation%20is%0Atime-consuming%2C%20requiring%20expert%20intervention%20and%20is%20susceptible%20to%20human%0Aerror.%20Therefore%2C%20significant%20research%20has%20been%20devoted%20to%20developing%20machine%0Alearning%20methods%20that%20can%20accurately%20segment%20tumors%20in%203D%20multimodal%20brain%20MRI%0Ascans.%20Despite%20their%20progress%2C%20state-of-the-art%20models%20are%20often%20limited%20by%20the%0Adata%20they%20are%20trained%20on%2C%20raising%20concerns%20about%20their%20reliability%20when%20applied%0Ato%20diverse%20populations%20that%20may%20introduce%20distribution%20shifts.%20Such%20shifts%20can%0Astem%20from%20lower%20quality%20MRI%20technology%20%28e.g.%2C%20in%20sub-Saharan%20Africa%29%20or%0Avariations%20in%20patient%20demographics%20%28e.g.%2C%20children%29.%20The%20BraTS-2024%20challenge%0Aprovides%20a%20platform%20to%20address%20these%20issues.%20This%20study%20presents%20our%0Amethodology%20for%20segmenting%20tumors%20in%20the%20BraTS-2024%20SSA%20and%20Pediatric%20Tumors%0Atasks%20using%20MedNeXt%2C%20comprehensive%20model%20ensembling%2C%20and%20thorough%0Apostprocessing.%20Our%20approach%20demonstrated%20strong%20performance%20on%20the%20unseen%0Avalidation%20set%2C%20achieving%20an%20average%20Dice%20Similarity%20Coefficient%20%28DSC%29%20of%200.896%0Aon%20the%20BraTS-2024%20SSA%20dataset%20and%20an%20average%20DSC%20of%200.830%20on%20the%20BraTS%0APediatric%20Tumor%20dataset.%20Additionally%2C%20our%20method%20achieved%20an%20average%20Hausdorff%0ADistance%20%28HD95%29%20of%2014.682%20on%20the%20BraTS-2024%20SSA%20dataset%20and%20an%20average%20HD95%20of%0A37.508%20on%20the%20BraTS%20Pediatric%20dataset.%20Our%20GitHub%20repository%20can%20be%20accessed%0Ahere%3A%20Project%20Repository%20%3A%0Ahttps%3A//github.com/python-arch/BioMbz-Optimizing-Brain-Tumor-Segmentation-with-MedNeXt-BraTS-2024-SSA-and-Pediatrics%0A%0ALink%3A%20http%3A//arxiv.org/abs/2411.15872v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DOptimizing%2520Brain%2520Tumor%2520Segmentation%2520with%2520MedNeXt%253A%2520BraTS%25202024%2520SSA%2520and%250A%2520%2520Pediatrics%26entry.906535625%3DSarim%2520Hashmi%2520and%2520Juan%2520Lugo%2520and%2520Abdelrahman%2520Elsayed%2520and%2520Dinesh%2520Saggurthi%2520and%2520Mohammed%2520Elseiagy%2520and%2520Alikhan%2520Nurkamal%2520and%2520Jaskaran%2520Walia%2520and%2520Fadillah%2520Adamsyah%2520Maani%2520and%2520Mohammad%2520Yaqub%26entry.1292438233%3D%2520%2520Identifying%2520key%2520pathological%2520features%2520in%2520brain%2520MRIs%2520is%2520crucial%2520for%2520the%250Along-term%2520survival%2520of%2520glioma%2520patients.%2520However%252C%2520manual%2520segmentation%2520is%250Atime-consuming%252C%2520requiring%2520expert%2520intervention%2520and%2520is%2520susceptible%2520to%2520human%250Aerror.%2520Therefore%252C%2520significant%2520research%2520has%2520been%2520devoted%2520to%2520developing%2520machine%250Alearning%2520methods%2520that%2520can%2520accurately%2520segment%2520tumors%2520in%25203D%2520multimodal%2520brain%2520MRI%250Ascans.%2520Despite%2520their%2520progress%252C%2520state-of-the-art%2520models%2520are%2520often%2520limited%2520by%2520the%250Adata%2520they%2520are%2520trained%2520on%252C%2520raising%2520concerns%2520about%2520their%2520reliability%2520when%2520applied%250Ato%2520diverse%2520populations%2520that%2520may%2520introduce%2520distribution%2520shifts.%2520Such%2520shifts%2520can%250Astem%2520from%2520lower%2520quality%2520MRI%2520technology%2520%2528e.g.%252C%2520in%2520sub-Saharan%2520Africa%2529%2520or%250Avariations%2520in%2520patient%2520demographics%2520%2528e.g.%252C%2520children%2529.%2520The%2520BraTS-2024%2520challenge%250Aprovides%2520a%2520platform%2520to%2520address%2520these%2520issues.%2520This%2520study%2520presents%2520our%250Amethodology%2520for%2520segmenting%2520tumors%2520in%2520the%2520BraTS-2024%2520SSA%2520and%2520Pediatric%2520Tumors%250Atasks%2520using%2520MedNeXt%252C%2520comprehensive%2520model%2520ensembling%252C%2520and%2520thorough%250Apostprocessing.%2520Our%2520approach%2520demonstrated%2520strong%2520performance%2520on%2520the%2520unseen%250Avalidation%2520set%252C%2520achieving%2520an%2520average%2520Dice%2520Similarity%2520Coefficient%2520%2528DSC%2529%2520of%25200.896%250Aon%2520the%2520BraTS-2024%2520SSA%2520dataset%2520and%2520an%2520average%2520DSC%2520of%25200.830%2520on%2520the%2520BraTS%250APediatric%2520Tumor%2520dataset.%2520Additionally%252C%2520our%2520method%2520achieved%2520an%2520average%2520Hausdorff%250ADistance%2520%2528HD95%2529%2520of%252014.682%2520on%2520the%2520BraTS-2024%2520SSA%2520dataset%2520and%2520an%2520average%2520HD95%2520of%250A37.508%2520on%2520the%2520BraTS%2520Pediatric%2520dataset.%2520Our%2520GitHub%2520repository%2520can%2520be%2520accessed%250Ahere%253A%2520Project%2520Repository%2520%253A%250Ahttps%253A//github.com/python-arch/BioMbz-Optimizing-Brain-Tumor-Segmentation-with-MedNeXt-BraTS-2024-SSA-and-Pediatrics%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2411.15872v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Optimizing%20Brain%20Tumor%20Segmentation%20with%20MedNeXt%3A%20BraTS%202024%20SSA%20and%0A%20%20Pediatrics&entry.906535625=Sarim%20Hashmi%20and%20Juan%20Lugo%20and%20Abdelrahman%20Elsayed%20and%20Dinesh%20Saggurthi%20and%20Mohammed%20Elseiagy%20and%20Alikhan%20Nurkamal%20and%20Jaskaran%20Walia%20and%20Fadillah%20Adamsyah%20Maani%20and%20Mohammad%20Yaqub&entry.1292438233=%20%20Identifying%20key%20pathological%20features%20in%20brain%20MRIs%20is%20crucial%20for%20the%0Along-term%20survival%20of%20glioma%20patients.%20However%2C%20manual%20segmentation%20is%0Atime-consuming%2C%20requiring%20expert%20intervention%20and%20is%20susceptible%20to%20human%0Aerror.%20Therefore%2C%20significant%20research%20has%20been%20devoted%20to%20developing%20machine%0Alearning%20methods%20that%20can%20accurately%20segment%20tumors%20in%203D%20multimodal%20brain%20MRI%0Ascans.%20Despite%20their%20progress%2C%20state-of-the-art%20models%20are%20often%20limited%20by%20the%0Adata%20they%20are%20trained%20on%2C%20raising%20concerns%20about%20their%20reliability%20when%20applied%0Ato%20diverse%20populations%20that%20may%20introduce%20distribution%20shifts.%20Such%20shifts%20can%0Astem%20from%20lower%20quality%20MRI%20technology%20%28e.g.%2C%20in%20sub-Saharan%20Africa%29%20or%0Avariations%20in%20patient%20demographics%20%28e.g.%2C%20children%29.%20The%20BraTS-2024%20challenge%0Aprovides%20a%20platform%20to%20address%20these%20issues.%20This%20study%20presents%20our%0Amethodology%20for%20segmenting%20tumors%20in%20the%20BraTS-2024%20SSA%20and%20Pediatric%20Tumors%0Atasks%20using%20MedNeXt%2C%20comprehensive%20model%20ensembling%2C%20and%20thorough%0Apostprocessing.%20Our%20approach%20demonstrated%20strong%20performance%20on%20the%20unseen%0Avalidation%20set%2C%20achieving%20an%20average%20Dice%20Similarity%20Coefficient%20%28DSC%29%20of%200.896%0Aon%20the%20BraTS-2024%20SSA%20dataset%20and%20an%20average%20DSC%20of%200.830%20on%20the%20BraTS%0APediatric%20Tumor%20dataset.%20Additionally%2C%20our%20method%20achieved%20an%20average%20Hausdorff%0ADistance%20%28HD95%29%20of%2014.682%20on%20the%20BraTS-2024%20SSA%20dataset%20and%20an%20average%20HD95%20of%0A37.508%20on%20the%20BraTS%20Pediatric%20dataset.%20Our%20GitHub%20repository%20can%20be%20accessed%0Ahere%3A%20Project%20Repository%20%3A%0Ahttps%3A//github.com/python-arch/BioMbz-Optimizing-Brain-Tumor-Segmentation-with-MedNeXt-BraTS-2024-SSA-and-Pediatrics%0A&entry.1838667208=http%3A//arxiv.org/abs/2411.15872v2&entry.124074799=Read"},
{"title": "Comparison of marker-less 2D image-based methods for infant pose\n  estimation", "author": "Lennart Jahn and Sarah Fl\u00fcgge and Dajie Zhang and Luise Poustka and Sven B\u00f6lte and Florentin W\u00f6rg\u00f6tter and Peter B Marschik and Tomas Kulvicius", "abstract": "  In this study we compare the performance of available generic- and\ninfant-pose estimators for a video-based automated general movement assessment\n(GMA), and the choice of viewing angle for optimal recordings, i.e.,\nconventional diagonal view used in GMA vs. top-down view. We used 4500\nannotated video-frames from 75 recordings of infant spontaneous motor functions\nfrom 4 to 26 weeks. To determine which pose estimation method and camera angle\nyield the best pose estimation accuracy on infants in a GMA related setting,\nthe distance to human annotations and the percentage of correct key-points\n(PCK) were computed and compared. The results show that the best performing\ngeneric model trained on adults, ViTPose, also performs best on infants. We see\nno improvement from using infant-pose estimators over the generic pose\nestimators on our infant dataset. However, when retraining a generic model on\nour data, there is a significant improvement in pose estimation accuracy. The\npose estimation accuracy obtained from the top-down view is significantly\nbetter than that obtained from the diagonal view, especially for the detection\nof the hip key-points. The results also indicate limited generalization\ncapabilities of infant-pose estimators to other infant datasets, which hints\nthat one should be careful when choosing infant pose estimators and using them\non infant datasets which they were not trained on. While the standard GMA\nmethod uses a diagonal view for assessment, pose estimation accuracy\nsignificantly improves using a top-down view. This suggests that a top-down\nview should be included in recording setups for automated GMA research.\n", "link": "http://arxiv.org/abs/2410.04980v2", "date": "2024-11-26", "relevancy": 2.6415, "topK": [{"title": "GST: Precise 3D Human Body from a Single Image with Gaussian Splatting\n  Transformers", "link": "http://arxiv.org/abs/2409.04196v1", "similarity": 0.5464}, {"title": "PoseAnimate: Zero-shot high fidelity pose controllable character\n  animation", "link": "http://arxiv.org/abs/2404.13680v2", "similarity": 0.5378}, {"title": "3D Gaussian Blendshapes for Head Avatar Animation", "link": "http://arxiv.org/abs/2404.19398v2", "similarity": 0.5006}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Comparison%20of%20marker-less%202D%20image-based%20methods%20for%20infant%20pose%0A%20%20estimation&body=Title%3A%20Comparison%20of%20marker-less%202D%20image-based%20methods%20for%20infant%20pose%0A%20%20estimation%0AAuthor%3A%20Lennart%20Jahn%20and%20Sarah%20Fl%C3%BCgge%20and%20Dajie%20Zhang%20and%20Luise%20Poustka%20and%20Sven%20B%C3%B6lte%20and%20Florentin%20W%C3%B6rg%C3%B6tter%20and%20Peter%20B%20Marschik%20and%20Tomas%20Kulvicius%0AAbstract%3A%20%20%20In%20this%20study%20we%20compare%20the%20performance%20of%20available%20generic-%20and%0Ainfant-pose%20estimators%20for%20a%20video-based%20automated%20general%20movement%20assessment%0A%28GMA%29%2C%20and%20the%20choice%20of%20viewing%20angle%20for%20optimal%20recordings%2C%20i.e.%2C%0Aconventional%20diagonal%20view%20used%20in%20GMA%20vs.%20top-down%20view.%20We%20used%204500%0Aannotated%20video-frames%20from%2075%20recordings%20of%20infant%20spontaneous%20motor%20functions%0Afrom%204%20to%2026%20weeks.%20To%20determine%20which%20pose%20estimation%20method%20and%20camera%20angle%0Ayield%20the%20best%20pose%20estimation%20accuracy%20on%20infants%20in%20a%20GMA%20related%20setting%2C%0Athe%20distance%20to%20human%20annotations%20and%20the%20percentage%20of%20correct%20key-points%0A%28PCK%29%20were%20computed%20and%20compared.%20The%20results%20show%20that%20the%20best%20performing%0Ageneric%20model%20trained%20on%20adults%2C%20ViTPose%2C%20also%20performs%20best%20on%20infants.%20We%20see%0Ano%20improvement%20from%20using%20infant-pose%20estimators%20over%20the%20generic%20pose%0Aestimators%20on%20our%20infant%20dataset.%20However%2C%20when%20retraining%20a%20generic%20model%20on%0Aour%20data%2C%20there%20is%20a%20significant%20improvement%20in%20pose%20estimation%20accuracy.%20The%0Apose%20estimation%20accuracy%20obtained%20from%20the%20top-down%20view%20is%20significantly%0Abetter%20than%20that%20obtained%20from%20the%20diagonal%20view%2C%20especially%20for%20the%20detection%0Aof%20the%20hip%20key-points.%20The%20results%20also%20indicate%20limited%20generalization%0Acapabilities%20of%20infant-pose%20estimators%20to%20other%20infant%20datasets%2C%20which%20hints%0Athat%20one%20should%20be%20careful%20when%20choosing%20infant%20pose%20estimators%20and%20using%20them%0Aon%20infant%20datasets%20which%20they%20were%20not%20trained%20on.%20While%20the%20standard%20GMA%0Amethod%20uses%20a%20diagonal%20view%20for%20assessment%2C%20pose%20estimation%20accuracy%0Asignificantly%20improves%20using%20a%20top-down%20view.%20This%20suggests%20that%20a%20top-down%0Aview%20should%20be%20included%20in%20recording%20setups%20for%20automated%20GMA%20research.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2410.04980v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DComparison%2520of%2520marker-less%25202D%2520image-based%2520methods%2520for%2520infant%2520pose%250A%2520%2520estimation%26entry.906535625%3DLennart%2520Jahn%2520and%2520Sarah%2520Fl%25C3%25BCgge%2520and%2520Dajie%2520Zhang%2520and%2520Luise%2520Poustka%2520and%2520Sven%2520B%25C3%25B6lte%2520and%2520Florentin%2520W%25C3%25B6rg%25C3%25B6tter%2520and%2520Peter%2520B%2520Marschik%2520and%2520Tomas%2520Kulvicius%26entry.1292438233%3D%2520%2520In%2520this%2520study%2520we%2520compare%2520the%2520performance%2520of%2520available%2520generic-%2520and%250Ainfant-pose%2520estimators%2520for%2520a%2520video-based%2520automated%2520general%2520movement%2520assessment%250A%2528GMA%2529%252C%2520and%2520the%2520choice%2520of%2520viewing%2520angle%2520for%2520optimal%2520recordings%252C%2520i.e.%252C%250Aconventional%2520diagonal%2520view%2520used%2520in%2520GMA%2520vs.%2520top-down%2520view.%2520We%2520used%25204500%250Aannotated%2520video-frames%2520from%252075%2520recordings%2520of%2520infant%2520spontaneous%2520motor%2520functions%250Afrom%25204%2520to%252026%2520weeks.%2520To%2520determine%2520which%2520pose%2520estimation%2520method%2520and%2520camera%2520angle%250Ayield%2520the%2520best%2520pose%2520estimation%2520accuracy%2520on%2520infants%2520in%2520a%2520GMA%2520related%2520setting%252C%250Athe%2520distance%2520to%2520human%2520annotations%2520and%2520the%2520percentage%2520of%2520correct%2520key-points%250A%2528PCK%2529%2520were%2520computed%2520and%2520compared.%2520The%2520results%2520show%2520that%2520the%2520best%2520performing%250Ageneric%2520model%2520trained%2520on%2520adults%252C%2520ViTPose%252C%2520also%2520performs%2520best%2520on%2520infants.%2520We%2520see%250Ano%2520improvement%2520from%2520using%2520infant-pose%2520estimators%2520over%2520the%2520generic%2520pose%250Aestimators%2520on%2520our%2520infant%2520dataset.%2520However%252C%2520when%2520retraining%2520a%2520generic%2520model%2520on%250Aour%2520data%252C%2520there%2520is%2520a%2520significant%2520improvement%2520in%2520pose%2520estimation%2520accuracy.%2520The%250Apose%2520estimation%2520accuracy%2520obtained%2520from%2520the%2520top-down%2520view%2520is%2520significantly%250Abetter%2520than%2520that%2520obtained%2520from%2520the%2520diagonal%2520view%252C%2520especially%2520for%2520the%2520detection%250Aof%2520the%2520hip%2520key-points.%2520The%2520results%2520also%2520indicate%2520limited%2520generalization%250Acapabilities%2520of%2520infant-pose%2520estimators%2520to%2520other%2520infant%2520datasets%252C%2520which%2520hints%250Athat%2520one%2520should%2520be%2520careful%2520when%2520choosing%2520infant%2520pose%2520estimators%2520and%2520using%2520them%250Aon%2520infant%2520datasets%2520which%2520they%2520were%2520not%2520trained%2520on.%2520While%2520the%2520standard%2520GMA%250Amethod%2520uses%2520a%2520diagonal%2520view%2520for%2520assessment%252C%2520pose%2520estimation%2520accuracy%250Asignificantly%2520improves%2520using%2520a%2520top-down%2520view.%2520This%2520suggests%2520that%2520a%2520top-down%250Aview%2520should%2520be%2520included%2520in%2520recording%2520setups%2520for%2520automated%2520GMA%2520research.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2410.04980v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Comparison%20of%20marker-less%202D%20image-based%20methods%20for%20infant%20pose%0A%20%20estimation&entry.906535625=Lennart%20Jahn%20and%20Sarah%20Fl%C3%BCgge%20and%20Dajie%20Zhang%20and%20Luise%20Poustka%20and%20Sven%20B%C3%B6lte%20and%20Florentin%20W%C3%B6rg%C3%B6tter%20and%20Peter%20B%20Marschik%20and%20Tomas%20Kulvicius&entry.1292438233=%20%20In%20this%20study%20we%20compare%20the%20performance%20of%20available%20generic-%20and%0Ainfant-pose%20estimators%20for%20a%20video-based%20automated%20general%20movement%20assessment%0A%28GMA%29%2C%20and%20the%20choice%20of%20viewing%20angle%20for%20optimal%20recordings%2C%20i.e.%2C%0Aconventional%20diagonal%20view%20used%20in%20GMA%20vs.%20top-down%20view.%20We%20used%204500%0Aannotated%20video-frames%20from%2075%20recordings%20of%20infant%20spontaneous%20motor%20functions%0Afrom%204%20to%2026%20weeks.%20To%20determine%20which%20pose%20estimation%20method%20and%20camera%20angle%0Ayield%20the%20best%20pose%20estimation%20accuracy%20on%20infants%20in%20a%20GMA%20related%20setting%2C%0Athe%20distance%20to%20human%20annotations%20and%20the%20percentage%20of%20correct%20key-points%0A%28PCK%29%20were%20computed%20and%20compared.%20The%20results%20show%20that%20the%20best%20performing%0Ageneric%20model%20trained%20on%20adults%2C%20ViTPose%2C%20also%20performs%20best%20on%20infants.%20We%20see%0Ano%20improvement%20from%20using%20infant-pose%20estimators%20over%20the%20generic%20pose%0Aestimators%20on%20our%20infant%20dataset.%20However%2C%20when%20retraining%20a%20generic%20model%20on%0Aour%20data%2C%20there%20is%20a%20significant%20improvement%20in%20pose%20estimation%20accuracy.%20The%0Apose%20estimation%20accuracy%20obtained%20from%20the%20top-down%20view%20is%20significantly%0Abetter%20than%20that%20obtained%20from%20the%20diagonal%20view%2C%20especially%20for%20the%20detection%0Aof%20the%20hip%20key-points.%20The%20results%20also%20indicate%20limited%20generalization%0Acapabilities%20of%20infant-pose%20estimators%20to%20other%20infant%20datasets%2C%20which%20hints%0Athat%20one%20should%20be%20careful%20when%20choosing%20infant%20pose%20estimators%20and%20using%20them%0Aon%20infant%20datasets%20which%20they%20were%20not%20trained%20on.%20While%20the%20standard%20GMA%0Amethod%20uses%20a%20diagonal%20view%20for%20assessment%2C%20pose%20estimation%20accuracy%0Asignificantly%20improves%20using%20a%20top-down%20view.%20This%20suggests%20that%20a%20top-down%0Aview%20should%20be%20included%20in%20recording%20setups%20for%20automated%20GMA%20research.%0A&entry.1838667208=http%3A//arxiv.org/abs/2410.04980v2&entry.124074799=Read"},
{"title": "Structure-Guided MR-to-CT Synthesis with Spatial and Semantic Alignments\n  for Attenuation Correction of Whole-Body PET/MR Imaging", "author": "Jiaxu Zheng and Zhenrong Shen and Lichi Zhang and Qun Chen", "abstract": "  Deep-learning-based MR-to-CT synthesis can estimate the electron density of\ntissues, thereby facilitating PET attenuation correction in whole-body PET/MR\nimaging. However, whole-body MR-to-CT synthesis faces several challenges\nincluding the issue of spatial misalignment and the complexity of intensity\nmapping, primarily due to the variety of tissues and organs throughout the\nwhole body. Here we propose a novel whole-body MR-to-CT synthesis framework,\nwhich consists of three novel modules to tackle these challenges: (1)\nStructure-Guided Synthesis module leverages structure-guided attention gates to\nenhance synthetic image quality by diminishing unnecessary contours of soft\ntissues; (2) Spatial Alignment module yields precise registration between\npaired MR and CT images by taking into account the impacts of tissue volumes\nand respiratory movements, thus providing well-aligned ground-truth CT images\nduring training; (3) Semantic Alignment module utilizes contrastive learning to\nconstrain organ-related semantic information, thereby ensuring the semantic\nauthenticity of synthetic CT images.We conduct extensive experiments to\ndemonstrate that the proposed whole-body MR-to-CT framework can produce\nvisually plausible and semantically realistic CT images, and validate its\nutility in PET attenuation correction.\n", "link": "http://arxiv.org/abs/2411.17488v1", "date": "2024-11-26", "relevancy": 2.6309, "topK": [{"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5336}, {"title": "CAT3D: Create Anything in 3D with Multi-View Diffusion Models", "link": "http://arxiv.org/abs/2405.10314v1", "similarity": 0.5225}, {"title": "CAT3D: Create Anything in 3D with Multi-View Diffusion Models", "link": "http://arxiv.org/abs/2405.10314v1", "similarity": 0.5225}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Structure-Guided%20MR-to-CT%20Synthesis%20with%20Spatial%20and%20Semantic%20Alignments%0A%20%20for%20Attenuation%20Correction%20of%20Whole-Body%20PET/MR%20Imaging&body=Title%3A%20Structure-Guided%20MR-to-CT%20Synthesis%20with%20Spatial%20and%20Semantic%20Alignments%0A%20%20for%20Attenuation%20Correction%20of%20Whole-Body%20PET/MR%20Imaging%0AAuthor%3A%20Jiaxu%20Zheng%20and%20Zhenrong%20Shen%20and%20Lichi%20Zhang%20and%20Qun%20Chen%0AAbstract%3A%20%20%20Deep-learning-based%20MR-to-CT%20synthesis%20can%20estimate%20the%20electron%20density%20of%0Atissues%2C%20thereby%20facilitating%20PET%20attenuation%20correction%20in%20whole-body%20PET/MR%0Aimaging.%20However%2C%20whole-body%20MR-to-CT%20synthesis%20faces%20several%20challenges%0Aincluding%20the%20issue%20of%20spatial%20misalignment%20and%20the%20complexity%20of%20intensity%0Amapping%2C%20primarily%20due%20to%20the%20variety%20of%20tissues%20and%20organs%20throughout%20the%0Awhole%20body.%20Here%20we%20propose%20a%20novel%20whole-body%20MR-to-CT%20synthesis%20framework%2C%0Awhich%20consists%20of%20three%20novel%20modules%20to%20tackle%20these%20challenges%3A%20%281%29%0AStructure-Guided%20Synthesis%20module%20leverages%20structure-guided%20attention%20gates%20to%0Aenhance%20synthetic%20image%20quality%20by%20diminishing%20unnecessary%20contours%20of%20soft%0Atissues%3B%20%282%29%20Spatial%20Alignment%20module%20yields%20precise%20registration%20between%0Apaired%20MR%20and%20CT%20images%20by%20taking%20into%20account%20the%20impacts%20of%20tissue%20volumes%0Aand%20respiratory%20movements%2C%20thus%20providing%20well-aligned%20ground-truth%20CT%20images%0Aduring%20training%3B%20%283%29%20Semantic%20Alignment%20module%20utilizes%20contrastive%20learning%20to%0Aconstrain%20organ-related%20semantic%20information%2C%20thereby%20ensuring%20the%20semantic%0Aauthenticity%20of%20synthetic%20CT%20images.We%20conduct%20extensive%20experiments%20to%0Ademonstrate%20that%20the%20proposed%20whole-body%20MR-to-CT%20framework%20can%20produce%0Avisually%20plausible%20and%20semantically%20realistic%20CT%20images%2C%20and%20validate%20its%0Autility%20in%20PET%20attenuation%20correction.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2411.17488v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DStructure-Guided%2520MR-to-CT%2520Synthesis%2520with%2520Spatial%2520and%2520Semantic%2520Alignments%250A%2520%2520for%2520Attenuation%2520Correction%2520of%2520Whole-Body%2520PET/MR%2520Imaging%26entry.906535625%3DJiaxu%2520Zheng%2520and%2520Zhenrong%2520Shen%2520and%2520Lichi%2520Zhang%2520and%2520Qun%2520Chen%26entry.1292438233%3D%2520%2520Deep-learning-based%2520MR-to-CT%2520synthesis%2520can%2520estimate%2520the%2520electron%2520density%2520of%250Atissues%252C%2520thereby%2520facilitating%2520PET%2520attenuation%2520correction%2520in%2520whole-body%2520PET/MR%250Aimaging.%2520However%252C%2520whole-body%2520MR-to-CT%2520synthesis%2520faces%2520several%2520challenges%250Aincluding%2520the%2520issue%2520of%2520spatial%2520misalignment%2520and%2520the%2520complexity%2520of%2520intensity%250Amapping%252C%2520primarily%2520due%2520to%2520the%2520variety%2520of%2520tissues%2520and%2520organs%2520throughout%2520the%250Awhole%2520body.%2520Here%2520we%2520propose%2520a%2520novel%2520whole-body%2520MR-to-CT%2520synthesis%2520framework%252C%250Awhich%2520consists%2520of%2520three%2520novel%2520modules%2520to%2520tackle%2520these%2520challenges%253A%2520%25281%2529%250AStructure-Guided%2520Synthesis%2520module%2520leverages%2520structure-guided%2520attention%2520gates%2520to%250Aenhance%2520synthetic%2520image%2520quality%2520by%2520diminishing%2520unnecessary%2520contours%2520of%2520soft%250Atissues%253B%2520%25282%2529%2520Spatial%2520Alignment%2520module%2520yields%2520precise%2520registration%2520between%250Apaired%2520MR%2520and%2520CT%2520images%2520by%2520taking%2520into%2520account%2520the%2520impacts%2520of%2520tissue%2520volumes%250Aand%2520respiratory%2520movements%252C%2520thus%2520providing%2520well-aligned%2520ground-truth%2520CT%2520images%250Aduring%2520training%253B%2520%25283%2529%2520Semantic%2520Alignment%2520module%2520utilizes%2520contrastive%2520learning%2520to%250Aconstrain%2520organ-related%2520semantic%2520information%252C%2520thereby%2520ensuring%2520the%2520semantic%250Aauthenticity%2520of%2520synthetic%2520CT%2520images.We%2520conduct%2520extensive%2520experiments%2520to%250Ademonstrate%2520that%2520the%2520proposed%2520whole-body%2520MR-to-CT%2520framework%2520can%2520produce%250Avisually%2520plausible%2520and%2520semantically%2520realistic%2520CT%2520images%252C%2520and%2520validate%2520its%250Autility%2520in%2520PET%2520attenuation%2520correction.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2411.17488v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Structure-Guided%20MR-to-CT%20Synthesis%20with%20Spatial%20and%20Semantic%20Alignments%0A%20%20for%20Attenuation%20Correction%20of%20Whole-Body%20PET/MR%20Imaging&entry.906535625=Jiaxu%20Zheng%20and%20Zhenrong%20Shen%20and%20Lichi%20Zhang%20and%20Qun%20Chen&entry.1292438233=%20%20Deep-learning-based%20MR-to-CT%20synthesis%20can%20estimate%20the%20electron%20density%20of%0Atissues%2C%20thereby%20facilitating%20PET%20attenuation%20correction%20in%20whole-body%20PET/MR%0Aimaging.%20However%2C%20whole-body%20MR-to-CT%20synthesis%20faces%20several%20challenges%0Aincluding%20the%20issue%20of%20spatial%20misalignment%20and%20the%20complexity%20of%20intensity%0Amapping%2C%20primarily%20due%20to%20the%20variety%20of%20tissues%20and%20organs%20throughout%20the%0Awhole%20body.%20Here%20we%20propose%20a%20novel%20whole-body%20MR-to-CT%20synthesis%20framework%2C%0Awhich%20consists%20of%20three%20novel%20modules%20to%20tackle%20these%20challenges%3A%20%281%29%0AStructure-Guided%20Synthesis%20module%20leverages%20structure-guided%20attention%20gates%20to%0Aenhance%20synthetic%20image%20quality%20by%20diminishing%20unnecessary%20contours%20of%20soft%0Atissues%3B%20%282%29%20Spatial%20Alignment%20module%20yields%20precise%20registration%20between%0Apaired%20MR%20and%20CT%20images%20by%20taking%20into%20account%20the%20impacts%20of%20tissue%20volumes%0Aand%20respiratory%20movements%2C%20thus%20providing%20well-aligned%20ground-truth%20CT%20images%0Aduring%20training%3B%20%283%29%20Semantic%20Alignment%20module%20utilizes%20contrastive%20learning%20to%0Aconstrain%20organ-related%20semantic%20information%2C%20thereby%20ensuring%20the%20semantic%0Aauthenticity%20of%20synthetic%20CT%20images.We%20conduct%20extensive%20experiments%20to%0Ademonstrate%20that%20the%20proposed%20whole-body%20MR-to-CT%20framework%20can%20produce%0Avisually%20plausible%20and%20semantically%20realistic%20CT%20images%2C%20and%20validate%20its%0Autility%20in%20PET%20attenuation%20correction.%0A&entry.1838667208=http%3A//arxiv.org/abs/2411.17488v1&entry.124074799=Read"},
{"title": "Getting aligned on representational alignment", "author": "Ilia Sucholutsky and Lukas Muttenthaler and Adrian Weller and Andi Peng and Andreea Bobu and Been Kim and Bradley C. Love and Christopher J. Cueva and Erin Grant and Iris Groen and Jascha Achterberg and Joshua B. Tenenbaum and Katherine M. Collins and Katherine L. Hermann and Kerem Oktar and Klaus Greff and Martin N. Hebart and Nathan Cloos and Nikolaus Kriegeskorte and Nori Jacoby and Qiuyi Zhang and Raja Marjieh and Robert Geirhos and Sherol Chen and Simon Kornblith and Sunayana Rane and Talia Konkle and Thomas P. O'Connell and Thomas Unterthiner and Andrew K. Lampinen and Klaus-Robert M\u00fcller and Mariya Toneva and Thomas L. Griffiths", "abstract": "  Biological and artificial information processing systems form representations\nof the world that they can use to categorize, reason, plan, navigate, and make\ndecisions. How can we measure the similarity between the representations formed\nby these diverse systems? Do similarities in representations then translate\ninto similar behavior? If so, then how can a system's representations be\nmodified to better match those of another system? These questions pertaining to\nthe study of representational alignment are at the heart of some of the most\npromising research areas in contemporary cognitive science, neuroscience, and\nmachine learning. In this Perspective, we survey the exciting recent\ndevelopments in representational alignment research in the fields of cognitive\nscience, neuroscience, and machine learning. Despite their overlapping\ninterests, there is limited knowledge transfer between these fields, so work in\none field ends up duplicated in another, and useful innovations are not shared\neffectively. To improve communication, we propose a unifying framework that can\nserve as a common language for research on representational alignment, and map\nseveral streams of existing work across fields within our framework. We also\nlay out open problems in representational alignment where progress can benefit\nall three of these fields. We hope that this paper will catalyze\ncross-disciplinary collaboration and accelerate progress for all communities\nstudying and developing information processing systems.\n", "link": "http://arxiv.org/abs/2310.13018v3", "date": "2024-11-26", "relevancy": 2.6247, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5317}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5317}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5114}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Getting%20aligned%20on%20representational%20alignment&body=Title%3A%20Getting%20aligned%20on%20representational%20alignment%0AAuthor%3A%20Ilia%20Sucholutsky%20and%20Lukas%20Muttenthaler%20and%20Adrian%20Weller%20and%20Andi%20Peng%20and%20Andreea%20Bobu%20and%20Been%20Kim%20and%20Bradley%20C.%20Love%20and%20Christopher%20J.%20Cueva%20and%20Erin%20Grant%20and%20Iris%20Groen%20and%20Jascha%20Achterberg%20and%20Joshua%20B.%20Tenenbaum%20and%20Katherine%20M.%20Collins%20and%20Katherine%20L.%20Hermann%20and%20Kerem%20Oktar%20and%20Klaus%20Greff%20and%20Martin%20N.%20Hebart%20and%20Nathan%20Cloos%20and%20Nikolaus%20Kriegeskorte%20and%20Nori%20Jacoby%20and%20Qiuyi%20Zhang%20and%20Raja%20Marjieh%20and%20Robert%20Geirhos%20and%20Sherol%20Chen%20and%20Simon%20Kornblith%20and%20Sunayana%20Rane%20and%20Talia%20Konkle%20and%20Thomas%20P.%20O%27Connell%20and%20Thomas%20Unterthiner%20and%20Andrew%20K.%20Lampinen%20and%20Klaus-Robert%20M%C3%BCller%20and%20Mariya%20Toneva%20and%20Thomas%20L.%20Griffiths%0AAbstract%3A%20%20%20Biological%20and%20artificial%20information%20processing%20systems%20form%20representations%0Aof%20the%20world%20that%20they%20can%20use%20to%20categorize%2C%20reason%2C%20plan%2C%20navigate%2C%20and%20make%0Adecisions.%20How%20can%20we%20measure%20the%20similarity%20between%20the%20representations%20formed%0Aby%20these%20diverse%20systems%3F%20Do%20similarities%20in%20representations%20then%20translate%0Ainto%20similar%20behavior%3F%20If%20so%2C%20then%20how%20can%20a%20system%27s%20representations%20be%0Amodified%20to%20better%20match%20those%20of%20another%20system%3F%20These%20questions%20pertaining%20to%0Athe%20study%20of%20representational%20alignment%20are%20at%20the%20heart%20of%20some%20of%20the%20most%0Apromising%20research%20areas%20in%20contemporary%20cognitive%20science%2C%20neuroscience%2C%20and%0Amachine%20learning.%20In%20this%20Perspective%2C%20we%20survey%20the%20exciting%20recent%0Adevelopments%20in%20representational%20alignment%20research%20in%20the%20fields%20of%20cognitive%0Ascience%2C%20neuroscience%2C%20and%20machine%20learning.%20Despite%20their%20overlapping%0Ainterests%2C%20there%20is%20limited%20knowledge%20transfer%20between%20these%20fields%2C%20so%20work%20in%0Aone%20field%20ends%20up%20duplicated%20in%20another%2C%20and%20useful%20innovations%20are%20not%20shared%0Aeffectively.%20To%20improve%20communication%2C%20we%20propose%20a%20unifying%20framework%20that%20can%0Aserve%20as%20a%20common%20language%20for%20research%20on%20representational%20alignment%2C%20and%20map%0Aseveral%20streams%20of%20existing%20work%20across%20fields%20within%20our%20framework.%20We%20also%0Alay%20out%20open%20problems%20in%20representational%20alignment%20where%20progress%20can%20benefit%0Aall%20three%20of%20these%20fields.%20We%20hope%20that%20this%20paper%20will%20catalyze%0Across-disciplinary%20collaboration%20and%20accelerate%20progress%20for%20all%20communities%0Astudying%20and%20developing%20information%20processing%20systems.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2310.13018v3%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DGetting%2520aligned%2520on%2520representational%2520alignment%26entry.906535625%3DIlia%2520Sucholutsky%2520and%2520Lukas%2520Muttenthaler%2520and%2520Adrian%2520Weller%2520and%2520Andi%2520Peng%2520and%2520Andreea%2520Bobu%2520and%2520Been%2520Kim%2520and%2520Bradley%2520C.%2520Love%2520and%2520Christopher%2520J.%2520Cueva%2520and%2520Erin%2520Grant%2520and%2520Iris%2520Groen%2520and%2520Jascha%2520Achterberg%2520and%2520Joshua%2520B.%2520Tenenbaum%2520and%2520Katherine%2520M.%2520Collins%2520and%2520Katherine%2520L.%2520Hermann%2520and%2520Kerem%2520Oktar%2520and%2520Klaus%2520Greff%2520and%2520Martin%2520N.%2520Hebart%2520and%2520Nathan%2520Cloos%2520and%2520Nikolaus%2520Kriegeskorte%2520and%2520Nori%2520Jacoby%2520and%2520Qiuyi%2520Zhang%2520and%2520Raja%2520Marjieh%2520and%2520Robert%2520Geirhos%2520and%2520Sherol%2520Chen%2520and%2520Simon%2520Kornblith%2520and%2520Sunayana%2520Rane%2520and%2520Talia%2520Konkle%2520and%2520Thomas%2520P.%2520O%2527Connell%2520and%2520Thomas%2520Unterthiner%2520and%2520Andrew%2520K.%2520Lampinen%2520and%2520Klaus-Robert%2520M%25C3%25BCller%2520and%2520Mariya%2520Toneva%2520and%2520Thomas%2520L.%2520Griffiths%26entry.1292438233%3D%2520%2520Biological%2520and%2520artificial%2520information%2520processing%2520systems%2520form%2520representations%250Aof%2520the%2520world%2520that%2520they%2520can%2520use%2520to%2520categorize%252C%2520reason%252C%2520plan%252C%2520navigate%252C%2520and%2520make%250Adecisions.%2520How%2520can%2520we%2520measure%2520the%2520similarity%2520between%2520the%2520representations%2520formed%250Aby%2520these%2520diverse%2520systems%253F%2520Do%2520similarities%2520in%2520representations%2520then%2520translate%250Ainto%2520similar%2520behavior%253F%2520If%2520so%252C%2520then%2520how%2520can%2520a%2520system%2527s%2520representations%2520be%250Amodified%2520to%2520better%2520match%2520those%2520of%2520another%2520system%253F%2520These%2520questions%2520pertaining%2520to%250Athe%2520study%2520of%2520representational%2520alignment%2520are%2520at%2520the%2520heart%2520of%2520some%2520of%2520the%2520most%250Apromising%2520research%2520areas%2520in%2520contemporary%2520cognitive%2520science%252C%2520neuroscience%252C%2520and%250Amachine%2520learning.%2520In%2520this%2520Perspective%252C%2520we%2520survey%2520the%2520exciting%2520recent%250Adevelopments%2520in%2520representational%2520alignment%2520research%2520in%2520the%2520fields%2520of%2520cognitive%250Ascience%252C%2520neuroscience%252C%2520and%2520machine%2520learning.%2520Despite%2520their%2520overlapping%250Ainterests%252C%2520there%2520is%2520limited%2520knowledge%2520transfer%2520between%2520these%2520fields%252C%2520so%2520work%2520in%250Aone%2520field%2520ends%2520up%2520duplicated%2520in%2520another%252C%2520and%2520useful%2520innovations%2520are%2520not%2520shared%250Aeffectively.%2520To%2520improve%2520communication%252C%2520we%2520propose%2520a%2520unifying%2520framework%2520that%2520can%250Aserve%2520as%2520a%2520common%2520language%2520for%2520research%2520on%2520representational%2520alignment%252C%2520and%2520map%250Aseveral%2520streams%2520of%2520existing%2520work%2520across%2520fields%2520within%2520our%2520framework.%2520We%2520also%250Alay%2520out%2520open%2520problems%2520in%2520representational%2520alignment%2520where%2520progress%2520can%2520benefit%250Aall%2520three%2520of%2520these%2520fields.%2520We%2520hope%2520that%2520this%2520paper%2520will%2520catalyze%250Across-disciplinary%2520collaboration%2520and%2520accelerate%2520progress%2520for%2520all%2520communities%250Astudying%2520and%2520developing%2520information%2520processing%2520systems.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2310.13018v3%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Getting%20aligned%20on%20representational%20alignment&entry.906535625=Ilia%20Sucholutsky%20and%20Lukas%20Muttenthaler%20and%20Adrian%20Weller%20and%20Andi%20Peng%20and%20Andreea%20Bobu%20and%20Been%20Kim%20and%20Bradley%20C.%20Love%20and%20Christopher%20J.%20Cueva%20and%20Erin%20Grant%20and%20Iris%20Groen%20and%20Jascha%20Achterberg%20and%20Joshua%20B.%20Tenenbaum%20and%20Katherine%20M.%20Collins%20and%20Katherine%20L.%20Hermann%20and%20Kerem%20Oktar%20and%20Klaus%20Greff%20and%20Martin%20N.%20Hebart%20and%20Nathan%20Cloos%20and%20Nikolaus%20Kriegeskorte%20and%20Nori%20Jacoby%20and%20Qiuyi%20Zhang%20and%20Raja%20Marjieh%20and%20Robert%20Geirhos%20and%20Sherol%20Chen%20and%20Simon%20Kornblith%20and%20Sunayana%20Rane%20and%20Talia%20Konkle%20and%20Thomas%20P.%20O%27Connell%20and%20Thomas%20Unterthiner%20and%20Andrew%20K.%20Lampinen%20and%20Klaus-Robert%20M%C3%BCller%20and%20Mariya%20Toneva%20and%20Thomas%20L.%20Griffiths&entry.1292438233=%20%20Biological%20and%20artificial%20information%20processing%20systems%20form%20representations%0Aof%20the%20world%20that%20they%20can%20use%20to%20categorize%2C%20reason%2C%20plan%2C%20navigate%2C%20and%20make%0Adecisions.%20How%20can%20we%20measure%20the%20similarity%20between%20the%20representations%20formed%0Aby%20these%20diverse%20systems%3F%20Do%20similarities%20in%20representations%20then%20translate%0Ainto%20similar%20behavior%3F%20If%20so%2C%20then%20how%20can%20a%20system%27s%20representations%20be%0Amodified%20to%20better%20match%20those%20of%20another%20system%3F%20These%20questions%20pertaining%20to%0Athe%20study%20of%20representational%20alignment%20are%20at%20the%20heart%20of%20some%20of%20the%20most%0Apromising%20research%20areas%20in%20contemporary%20cognitive%20science%2C%20neuroscience%2C%20and%0Amachine%20learning.%20In%20this%20Perspective%2C%20we%20survey%20the%20exciting%20recent%0Adevelopments%20in%20representational%20alignment%20research%20in%20the%20fields%20of%20cognitive%0Ascience%2C%20neuroscience%2C%20and%20machine%20learning.%20Despite%20their%20overlapping%0Ainterests%2C%20there%20is%20limited%20knowledge%20transfer%20between%20these%20fields%2C%20so%20work%20in%0Aone%20field%20ends%20up%20duplicated%20in%20another%2C%20and%20useful%20innovations%20are%20not%20shared%0Aeffectively.%20To%20improve%20communication%2C%20we%20propose%20a%20unifying%20framework%20that%20can%0Aserve%20as%20a%20common%20language%20for%20research%20on%20representational%20alignment%2C%20and%20map%0Aseveral%20streams%20of%20existing%20work%20across%20fields%20within%20our%20framework.%20We%20also%0Alay%20out%20open%20problems%20in%20representational%20alignment%20where%20progress%20can%20benefit%0Aall%20three%20of%20these%20fields.%20We%20hope%20that%20this%20paper%20will%20catalyze%0Across-disciplinary%20collaboration%20and%20accelerate%20progress%20for%20all%20communities%0Astudying%20and%20developing%20information%20processing%20systems.%0A&entry.1838667208=http%3A//arxiv.org/abs/2310.13018v3&entry.124074799=Read"},
{"title": "Smoothed Graph Contrastive Learning via Seamless Proximity Integration", "author": "Maysam Behmanesh and Maks Ovsjanikov", "abstract": "  Graph contrastive learning (GCL) aligns node representations by classifying\nnode pairs into positives and negatives using a selection process that\ntypically relies on establishing correspondences within two augmented graphs.\nThe conventional GCL approaches incorporate negative samples uniformly in the\ncontrastive loss, resulting in the equal treatment of negative nodes,\nregardless of their proximity to the true positive. In this paper, we present a\nSmoothed Graph Contrastive Learning model (SGCL), which leverages the geometric\nstructure of augmented graphs to inject proximity information associated with\npositive/negative pairs in the contrastive loss, thus significantly\nregularizing the learning process. The proposed SGCL adjusts the penalties\nassociated with node pairs in contrastive loss by incorporating three distinct\nsmoothing techniques that result in proximity-aware positives and negatives. To\nenhance scalability for large-scale graphs, the proposed framework incorporates\na graph batch-generating strategy that partitions the given graphs into\nmultiple subgraphs, facilitating efficient training in separate batches.\nThrough extensive experimentation in the unsupervised setting on various\nbenchmarks, particularly those of large scale, we demonstrate the superiority\nof our proposed framework against recent baselines.\n", "link": "http://arxiv.org/abs/2402.15270v2", "date": "2024-11-26", "relevancy": 2.6188, "topK": [{"title": "UGG: Unified Generative Grasping", "link": "https://arxiv.org/abs/2311.16917", "similarity": 0.5415}, {"title": "Self-supervised Photographic Image Layout Representation Learning", "link": "http://arxiv.org/abs/2403.03740v1", "similarity": 0.5252}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5046}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Smoothed%20Graph%20Contrastive%20Learning%20via%20Seamless%20Proximity%20Integration&body=Title%3A%20Smoothed%20Graph%20Contrastive%20Learning%20via%20Seamless%20Proximity%20Integration%0AAuthor%3A%20Maysam%20Behmanesh%20and%20Maks%20Ovsjanikov%0AAbstract%3A%20%20%20Graph%20contrastive%20learning%20%28GCL%29%20aligns%20node%20representations%20by%20classifying%0Anode%20pairs%20into%20positives%20and%20negatives%20using%20a%20selection%20process%20that%0Atypically%20relies%20on%20establishing%20correspondences%20within%20two%20augmented%20graphs.%0AThe%20conventional%20GCL%20approaches%20incorporate%20negative%20samples%20uniformly%20in%20the%0Acontrastive%20loss%2C%20resulting%20in%20the%20equal%20treatment%20of%20negative%20nodes%2C%0Aregardless%20of%20their%20proximity%20to%20the%20true%20positive.%20In%20this%20paper%2C%20we%20present%20a%0ASmoothed%20Graph%20Contrastive%20Learning%20model%20%28SGCL%29%2C%20which%20leverages%20the%20geometric%0Astructure%20of%20augmented%20graphs%20to%20inject%20proximity%20information%20associated%20with%0Apositive/negative%20pairs%20in%20the%20contrastive%20loss%2C%20thus%20significantly%0Aregularizing%20the%20learning%20process.%20The%20proposed%20SGCL%20adjusts%20the%20penalties%0Aassociated%20with%20node%20pairs%20in%20contrastive%20loss%20by%20incorporating%20three%20distinct%0Asmoothing%20techniques%20that%20result%20in%20proximity-aware%20positives%20and%20negatives.%20To%0Aenhance%20scalability%20for%20large-scale%20graphs%2C%20the%20proposed%20framework%20incorporates%0Aa%20graph%20batch-generating%20strategy%20that%20partitions%20the%20given%20graphs%20into%0Amultiple%20subgraphs%2C%20facilitating%20efficient%20training%20in%20separate%20batches.%0AThrough%20extensive%20experimentation%20in%20the%20unsupervised%20setting%20on%20various%0Abenchmarks%2C%20particularly%20those%20of%20large%20scale%2C%20we%20demonstrate%20the%20superiority%0Aof%20our%20proposed%20framework%20against%20recent%20baselines.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2402.15270v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DSmoothed%2520Graph%2520Contrastive%2520Learning%2520via%2520Seamless%2520Proximity%2520Integration%26entry.906535625%3DMaysam%2520Behmanesh%2520and%2520Maks%2520Ovsjanikov%26entry.1292438233%3D%2520%2520Graph%2520contrastive%2520learning%2520%2528GCL%2529%2520aligns%2520node%2520representations%2520by%2520classifying%250Anode%2520pairs%2520into%2520positives%2520and%2520negatives%2520using%2520a%2520selection%2520process%2520that%250Atypically%2520relies%2520on%2520establishing%2520correspondences%2520within%2520two%2520augmented%2520graphs.%250AThe%2520conventional%2520GCL%2520approaches%2520incorporate%2520negative%2520samples%2520uniformly%2520in%2520the%250Acontrastive%2520loss%252C%2520resulting%2520in%2520the%2520equal%2520treatment%2520of%2520negative%2520nodes%252C%250Aregardless%2520of%2520their%2520proximity%2520to%2520the%2520true%2520positive.%2520In%2520this%2520paper%252C%2520we%2520present%2520a%250ASmoothed%2520Graph%2520Contrastive%2520Learning%2520model%2520%2528SGCL%2529%252C%2520which%2520leverages%2520the%2520geometric%250Astructure%2520of%2520augmented%2520graphs%2520to%2520inject%2520proximity%2520information%2520associated%2520with%250Apositive/negative%2520pairs%2520in%2520the%2520contrastive%2520loss%252C%2520thus%2520significantly%250Aregularizing%2520the%2520learning%2520process.%2520The%2520proposed%2520SGCL%2520adjusts%2520the%2520penalties%250Aassociated%2520with%2520node%2520pairs%2520in%2520contrastive%2520loss%2520by%2520incorporating%2520three%2520distinct%250Asmoothing%2520techniques%2520that%2520result%2520in%2520proximity-aware%2520positives%2520and%2520negatives.%2520To%250Aenhance%2520scalability%2520for%2520large-scale%2520graphs%252C%2520the%2520proposed%2520framework%2520incorporates%250Aa%2520graph%2520batch-generating%2520strategy%2520that%2520partitions%2520the%2520given%2520graphs%2520into%250Amultiple%2520subgraphs%252C%2520facilitating%2520efficient%2520training%2520in%2520separate%2520batches.%250AThrough%2520extensive%2520experimentation%2520in%2520the%2520unsupervised%2520setting%2520on%2520various%250Abenchmarks%252C%2520particularly%2520those%2520of%2520large%2520scale%252C%2520we%2520demonstrate%2520the%2520superiority%250Aof%2520our%2520proposed%2520framework%2520against%2520recent%2520baselines.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2402.15270v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Smoothed%20Graph%20Contrastive%20Learning%20via%20Seamless%20Proximity%20Integration&entry.906535625=Maysam%20Behmanesh%20and%20Maks%20Ovsjanikov&entry.1292438233=%20%20Graph%20contrastive%20learning%20%28GCL%29%20aligns%20node%20representations%20by%20classifying%0Anode%20pairs%20into%20positives%20and%20negatives%20using%20a%20selection%20process%20that%0Atypically%20relies%20on%20establishing%20correspondences%20within%20two%20augmented%20graphs.%0AThe%20conventional%20GCL%20approaches%20incorporate%20negative%20samples%20uniformly%20in%20the%0Acontrastive%20loss%2C%20resulting%20in%20the%20equal%20treatment%20of%20negative%20nodes%2C%0Aregardless%20of%20their%20proximity%20to%20the%20true%20positive.%20In%20this%20paper%2C%20we%20present%20a%0ASmoothed%20Graph%20Contrastive%20Learning%20model%20%28SGCL%29%2C%20which%20leverages%20the%20geometric%0Astructure%20of%20augmented%20graphs%20to%20inject%20proximity%20information%20associated%20with%0Apositive/negative%20pairs%20in%20the%20contrastive%20loss%2C%20thus%20significantly%0Aregularizing%20the%20learning%20process.%20The%20proposed%20SGCL%20adjusts%20the%20penalties%0Aassociated%20with%20node%20pairs%20in%20contrastive%20loss%20by%20incorporating%20three%20distinct%0Asmoothing%20techniques%20that%20result%20in%20proximity-aware%20positives%20and%20negatives.%20To%0Aenhance%20scalability%20for%20large-scale%20graphs%2C%20the%20proposed%20framework%20incorporates%0Aa%20graph%20batch-generating%20strategy%20that%20partitions%20the%20given%20graphs%20into%0Amultiple%20subgraphs%2C%20facilitating%20efficient%20training%20in%20separate%20batches.%0AThrough%20extensive%20experimentation%20in%20the%20unsupervised%20setting%20on%20various%0Abenchmarks%2C%20particularly%20those%20of%20large%20scale%2C%20we%20demonstrate%20the%20superiority%0Aof%20our%20proposed%20framework%20against%20recent%20baselines.%0A&entry.1838667208=http%3A//arxiv.org/abs/2402.15270v2&entry.124074799=Read"},
{"title": "On the Element-Wise Representation and Reasoning in Zero-Shot Image\n  Recognition: A Systematic Survey", "author": "Jingcai Guo and Zhijie Rao and Zhi Chen and Song Guo and Jingren Zhou and Dacheng Tao", "abstract": "  Zero-shot image recognition (ZSIR) aims to recognize and reason in unseen\ndomains by learning generalized knowledge from limited data in the seen domain.\nThe gist of ZSIR is constructing a well-aligned mapping between the input\nvisual space and the target semantic space, which is a bottom-up paradigm\ninspired by the process by which humans observe the world. In recent years,\nZSIR has witnessed significant progress on a broad spectrum, from theory to\nalgorithm design, as well as widespread applications. However, to the best of\nour knowledge, there remains a lack of a systematic review of ZSIR from an\nelement-wise perspective, i.e., learning fine-grained elements of data and\ntheir inferential associations. To fill the gap, this paper thoroughly\ninvestigates recent advances in element-wise ZSIR and provides a sound basis\nfor its future development. Concretely, we first integrate three basic ZSIR\ntasks, i.e., object recognition, compositional recognition, and foundation\nmodel-based open-world recognition, into a unified element-wise paradigm and\nprovide a detailed taxonomy and analysis of the main approaches. Next, we\nsummarize the benchmarks, covering technical implementations, standardized\ndatasets, and some more details as a library. Last, we sketch out related\napplications, discuss vital challenges, and suggest potential future\ndirections.\n", "link": "http://arxiv.org/abs/2408.04879v3", "date": "2024-11-26", "relevancy": 2.615, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5358}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5358}, {"title": "Flexible Visual Recognition by Evidential Modeling of Confusion and Ignorance", "link": "http://openaccess.thecvf.com/content/ICCV2023/html/Fan_Flexible_Visual_Recognition_by_Evidential_Modeling_of_Confusion_and_Ignorance_ICCV_2023_paper.html", "similarity": 0.4973}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20On%20the%20Element-Wise%20Representation%20and%20Reasoning%20in%20Zero-Shot%20Image%0A%20%20Recognition%3A%20A%20Systematic%20Survey&body=Title%3A%20On%20the%20Element-Wise%20Representation%20and%20Reasoning%20in%20Zero-Shot%20Image%0A%20%20Recognition%3A%20A%20Systematic%20Survey%0AAuthor%3A%20Jingcai%20Guo%20and%20Zhijie%20Rao%20and%20Zhi%20Chen%20and%20Song%20Guo%20and%20Jingren%20Zhou%20and%20Dacheng%20Tao%0AAbstract%3A%20%20%20Zero-shot%20image%20recognition%20%28ZSIR%29%20aims%20to%20recognize%20and%20reason%20in%20unseen%0Adomains%20by%20learning%20generalized%20knowledge%20from%20limited%20data%20in%20the%20seen%20domain.%0AThe%20gist%20of%20ZSIR%20is%20constructing%20a%20well-aligned%20mapping%20between%20the%20input%0Avisual%20space%20and%20the%20target%20semantic%20space%2C%20which%20is%20a%20bottom-up%20paradigm%0Ainspired%20by%20the%20process%20by%20which%20humans%20observe%20the%20world.%20In%20recent%20years%2C%0AZSIR%20has%20witnessed%20significant%20progress%20on%20a%20broad%20spectrum%2C%20from%20theory%20to%0Aalgorithm%20design%2C%20as%20well%20as%20widespread%20applications.%20However%2C%20to%20the%20best%20of%0Aour%20knowledge%2C%20there%20remains%20a%20lack%20of%20a%20systematic%20review%20of%20ZSIR%20from%20an%0Aelement-wise%20perspective%2C%20i.e.%2C%20learning%20fine-grained%20elements%20of%20data%20and%0Atheir%20inferential%20associations.%20To%20fill%20the%20gap%2C%20this%20paper%20thoroughly%0Ainvestigates%20recent%20advances%20in%20element-wise%20ZSIR%20and%20provides%20a%20sound%20basis%0Afor%20its%20future%20development.%20Concretely%2C%20we%20first%20integrate%20three%20basic%20ZSIR%0Atasks%2C%20i.e.%2C%20object%20recognition%2C%20compositional%20recognition%2C%20and%20foundation%0Amodel-based%20open-world%20recognition%2C%20into%20a%20unified%20element-wise%20paradigm%20and%0Aprovide%20a%20detailed%20taxonomy%20and%20analysis%20of%20the%20main%20approaches.%20Next%2C%20we%0Asummarize%20the%20benchmarks%2C%20covering%20technical%20implementations%2C%20standardized%0Adatasets%2C%20and%20some%20more%20details%20as%20a%20library.%20Last%2C%20we%20sketch%20out%20related%0Aapplications%2C%20discuss%20vital%20challenges%2C%20and%20suggest%20potential%20future%0Adirections.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2408.04879v3%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DOn%2520the%2520Element-Wise%2520Representation%2520and%2520Reasoning%2520in%2520Zero-Shot%2520Image%250A%2520%2520Recognition%253A%2520A%2520Systematic%2520Survey%26entry.906535625%3DJingcai%2520Guo%2520and%2520Zhijie%2520Rao%2520and%2520Zhi%2520Chen%2520and%2520Song%2520Guo%2520and%2520Jingren%2520Zhou%2520and%2520Dacheng%2520Tao%26entry.1292438233%3D%2520%2520Zero-shot%2520image%2520recognition%2520%2528ZSIR%2529%2520aims%2520to%2520recognize%2520and%2520reason%2520in%2520unseen%250Adomains%2520by%2520learning%2520generalized%2520knowledge%2520from%2520limited%2520data%2520in%2520the%2520seen%2520domain.%250AThe%2520gist%2520of%2520ZSIR%2520is%2520constructing%2520a%2520well-aligned%2520mapping%2520between%2520the%2520input%250Avisual%2520space%2520and%2520the%2520target%2520semantic%2520space%252C%2520which%2520is%2520a%2520bottom-up%2520paradigm%250Ainspired%2520by%2520the%2520process%2520by%2520which%2520humans%2520observe%2520the%2520world.%2520In%2520recent%2520years%252C%250AZSIR%2520has%2520witnessed%2520significant%2520progress%2520on%2520a%2520broad%2520spectrum%252C%2520from%2520theory%2520to%250Aalgorithm%2520design%252C%2520as%2520well%2520as%2520widespread%2520applications.%2520However%252C%2520to%2520the%2520best%2520of%250Aour%2520knowledge%252C%2520there%2520remains%2520a%2520lack%2520of%2520a%2520systematic%2520review%2520of%2520ZSIR%2520from%2520an%250Aelement-wise%2520perspective%252C%2520i.e.%252C%2520learning%2520fine-grained%2520elements%2520of%2520data%2520and%250Atheir%2520inferential%2520associations.%2520To%2520fill%2520the%2520gap%252C%2520this%2520paper%2520thoroughly%250Ainvestigates%2520recent%2520advances%2520in%2520element-wise%2520ZSIR%2520and%2520provides%2520a%2520sound%2520basis%250Afor%2520its%2520future%2520development.%2520Concretely%252C%2520we%2520first%2520integrate%2520three%2520basic%2520ZSIR%250Atasks%252C%2520i.e.%252C%2520object%2520recognition%252C%2520compositional%2520recognition%252C%2520and%2520foundation%250Amodel-based%2520open-world%2520recognition%252C%2520into%2520a%2520unified%2520element-wise%2520paradigm%2520and%250Aprovide%2520a%2520detailed%2520taxonomy%2520and%2520analysis%2520of%2520the%2520main%2520approaches.%2520Next%252C%2520we%250Asummarize%2520the%2520benchmarks%252C%2520covering%2520technical%2520implementations%252C%2520standardized%250Adatasets%252C%2520and%2520some%2520more%2520details%2520as%2520a%2520library.%2520Last%252C%2520we%2520sketch%2520out%2520related%250Aapplications%252C%2520discuss%2520vital%2520challenges%252C%2520and%2520suggest%2520potential%2520future%250Adirections.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2408.04879v3%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=On%20the%20Element-Wise%20Representation%20and%20Reasoning%20in%20Zero-Shot%20Image%0A%20%20Recognition%3A%20A%20Systematic%20Survey&entry.906535625=Jingcai%20Guo%20and%20Zhijie%20Rao%20and%20Zhi%20Chen%20and%20Song%20Guo%20and%20Jingren%20Zhou%20and%20Dacheng%20Tao&entry.1292438233=%20%20Zero-shot%20image%20recognition%20%28ZSIR%29%20aims%20to%20recognize%20and%20reason%20in%20unseen%0Adomains%20by%20learning%20generalized%20knowledge%20from%20limited%20data%20in%20the%20seen%20domain.%0AThe%20gist%20of%20ZSIR%20is%20constructing%20a%20well-aligned%20mapping%20between%20the%20input%0Avisual%20space%20and%20the%20target%20semantic%20space%2C%20which%20is%20a%20bottom-up%20paradigm%0Ainspired%20by%20the%20process%20by%20which%20humans%20observe%20the%20world.%20In%20recent%20years%2C%0AZSIR%20has%20witnessed%20significant%20progress%20on%20a%20broad%20spectrum%2C%20from%20theory%20to%0Aalgorithm%20design%2C%20as%20well%20as%20widespread%20applications.%20However%2C%20to%20the%20best%20of%0Aour%20knowledge%2C%20there%20remains%20a%20lack%20of%20a%20systematic%20review%20of%20ZSIR%20from%20an%0Aelement-wise%20perspective%2C%20i.e.%2C%20learning%20fine-grained%20elements%20of%20data%20and%0Atheir%20inferential%20associations.%20To%20fill%20the%20gap%2C%20this%20paper%20thoroughly%0Ainvestigates%20recent%20advances%20in%20element-wise%20ZSIR%20and%20provides%20a%20sound%20basis%0Afor%20its%20future%20development.%20Concretely%2C%20we%20first%20integrate%20three%20basic%20ZSIR%0Atasks%2C%20i.e.%2C%20object%20recognition%2C%20compositional%20recognition%2C%20and%20foundation%0Amodel-based%20open-world%20recognition%2C%20into%20a%20unified%20element-wise%20paradigm%20and%0Aprovide%20a%20detailed%20taxonomy%20and%20analysis%20of%20the%20main%20approaches.%20Next%2C%20we%0Asummarize%20the%20benchmarks%2C%20covering%20technical%20implementations%2C%20standardized%0Adatasets%2C%20and%20some%20more%20details%20as%20a%20library.%20Last%2C%20we%20sketch%20out%20related%0Aapplications%2C%20discuss%20vital%20challenges%2C%20and%20suggest%20potential%20future%0Adirections.%0A&entry.1838667208=http%3A//arxiv.org/abs/2408.04879v3&entry.124074799=Read"},
{"title": "VideoDirector: Precise Video Editing via Text-to-Video Models", "author": "Yukun Wang and Longguang Wang and Zhiyuan Ma and Qibin Hu and Kai Xu and Yulan Guo", "abstract": "  Despite the typical inversion-then-editing paradigm using text-to-image (T2I)\nmodels has demonstrated promising results, directly extending it to\ntext-to-video (T2V) models still suffers severe artifacts such as color\nflickering and content distortion. Consequently, current video editing methods\nprimarily rely on T2I models, which inherently lack temporal-coherence\ngenerative ability, often resulting in inferior editing results. In this paper,\nwe attribute the failure of the typical editing paradigm to: 1) Tightly\nSpatial-temporal Coupling. The vanilla pivotal-based inversion strategy\nstruggles to disentangle spatial-temporal information in the video diffusion\nmodel; 2) Complicated Spatial-temporal Layout. The vanilla cross-attention\ncontrol is deficient in preserving the unedited content. To address these\nlimitations, we propose a spatial-temporal decoupled guidance (STDG) and\nmulti-frame null-text optimization strategy to provide pivotal temporal cues\nfor more precise pivotal inversion. Furthermore, we introduce a self-attention\ncontrol strategy to maintain higher fidelity for precise partial content\nediting. Experimental results demonstrate that our method (termed\nVideoDirector) effectively harnesses the powerful temporal generation\ncapabilities of T2V models, producing edited videos with state-of-the-art\nperformance in accuracy, motion smoothness, realism, and fidelity to unedited\ncontent.\n", "link": "http://arxiv.org/abs/2411.17592v1", "date": "2024-11-26", "relevancy": 2.6123, "topK": [{"title": "Customize-A-Video: One-Shot Motion Customization of Text-to-Video\n  Diffusion Models", "link": "http://arxiv.org/abs/2402.14780v1", "similarity": 0.7081}, {"title": "Magic-Me: Identity-Specific Video Customized Diffusion", "link": "http://arxiv.org/abs/2402.09368v1", "similarity": 0.6675}, {"title": "Tuning-Free Noise Rectification for High Fidelity Image-to-Video\n  Generation", "link": "http://arxiv.org/abs/2403.02827v1", "similarity": 0.6166}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20VideoDirector%3A%20Precise%20Video%20Editing%20via%20Text-to-Video%20Models&body=Title%3A%20VideoDirector%3A%20Precise%20Video%20Editing%20via%20Text-to-Video%20Models%0AAuthor%3A%20Yukun%20Wang%20and%20Longguang%20Wang%20and%20Zhiyuan%20Ma%20and%20Qibin%20Hu%20and%20Kai%20Xu%20and%20Yulan%20Guo%0AAbstract%3A%20%20%20Despite%20the%20typical%20inversion-then-editing%20paradigm%20using%20text-to-image%20%28T2I%29%0Amodels%20has%20demonstrated%20promising%20results%2C%20directly%20extending%20it%20to%0Atext-to-video%20%28T2V%29%20models%20still%20suffers%20severe%20artifacts%20such%20as%20color%0Aflickering%20and%20content%20distortion.%20Consequently%2C%20current%20video%20editing%20methods%0Aprimarily%20rely%20on%20T2I%20models%2C%20which%20inherently%20lack%20temporal-coherence%0Agenerative%20ability%2C%20often%20resulting%20in%20inferior%20editing%20results.%20In%20this%20paper%2C%0Awe%20attribute%20the%20failure%20of%20the%20typical%20editing%20paradigm%20to%3A%201%29%20Tightly%0ASpatial-temporal%20Coupling.%20The%20vanilla%20pivotal-based%20inversion%20strategy%0Astruggles%20to%20disentangle%20spatial-temporal%20information%20in%20the%20video%20diffusion%0Amodel%3B%202%29%20Complicated%20Spatial-temporal%20Layout.%20The%20vanilla%20cross-attention%0Acontrol%20is%20deficient%20in%20preserving%20the%20unedited%20content.%20To%20address%20these%0Alimitations%2C%20we%20propose%20a%20spatial-temporal%20decoupled%20guidance%20%28STDG%29%20and%0Amulti-frame%20null-text%20optimization%20strategy%20to%20provide%20pivotal%20temporal%20cues%0Afor%20more%20precise%20pivotal%20inversion.%20Furthermore%2C%20we%20introduce%20a%20self-attention%0Acontrol%20strategy%20to%20maintain%20higher%20fidelity%20for%20precise%20partial%20content%0Aediting.%20Experimental%20results%20demonstrate%20that%20our%20method%20%28termed%0AVideoDirector%29%20effectively%20harnesses%20the%20powerful%20temporal%20generation%0Acapabilities%20of%20T2V%20models%2C%20producing%20edited%20videos%20with%20state-of-the-art%0Aperformance%20in%20accuracy%2C%20motion%20smoothness%2C%20realism%2C%20and%20fidelity%20to%20unedited%0Acontent.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2411.17592v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DVideoDirector%253A%2520Precise%2520Video%2520Editing%2520via%2520Text-to-Video%2520Models%26entry.906535625%3DYukun%2520Wang%2520and%2520Longguang%2520Wang%2520and%2520Zhiyuan%2520Ma%2520and%2520Qibin%2520Hu%2520and%2520Kai%2520Xu%2520and%2520Yulan%2520Guo%26entry.1292438233%3D%2520%2520Despite%2520the%2520typical%2520inversion-then-editing%2520paradigm%2520using%2520text-to-image%2520%2528T2I%2529%250Amodels%2520has%2520demonstrated%2520promising%2520results%252C%2520directly%2520extending%2520it%2520to%250Atext-to-video%2520%2528T2V%2529%2520models%2520still%2520suffers%2520severe%2520artifacts%2520such%2520as%2520color%250Aflickering%2520and%2520content%2520distortion.%2520Consequently%252C%2520current%2520video%2520editing%2520methods%250Aprimarily%2520rely%2520on%2520T2I%2520models%252C%2520which%2520inherently%2520lack%2520temporal-coherence%250Agenerative%2520ability%252C%2520often%2520resulting%2520in%2520inferior%2520editing%2520results.%2520In%2520this%2520paper%252C%250Awe%2520attribute%2520the%2520failure%2520of%2520the%2520typical%2520editing%2520paradigm%2520to%253A%25201%2529%2520Tightly%250ASpatial-temporal%2520Coupling.%2520The%2520vanilla%2520pivotal-based%2520inversion%2520strategy%250Astruggles%2520to%2520disentangle%2520spatial-temporal%2520information%2520in%2520the%2520video%2520diffusion%250Amodel%253B%25202%2529%2520Complicated%2520Spatial-temporal%2520Layout.%2520The%2520vanilla%2520cross-attention%250Acontrol%2520is%2520deficient%2520in%2520preserving%2520the%2520unedited%2520content.%2520To%2520address%2520these%250Alimitations%252C%2520we%2520propose%2520a%2520spatial-temporal%2520decoupled%2520guidance%2520%2528STDG%2529%2520and%250Amulti-frame%2520null-text%2520optimization%2520strategy%2520to%2520provide%2520pivotal%2520temporal%2520cues%250Afor%2520more%2520precise%2520pivotal%2520inversion.%2520Furthermore%252C%2520we%2520introduce%2520a%2520self-attention%250Acontrol%2520strategy%2520to%2520maintain%2520higher%2520fidelity%2520for%2520precise%2520partial%2520content%250Aediting.%2520Experimental%2520results%2520demonstrate%2520that%2520our%2520method%2520%2528termed%250AVideoDirector%2529%2520effectively%2520harnesses%2520the%2520powerful%2520temporal%2520generation%250Acapabilities%2520of%2520T2V%2520models%252C%2520producing%2520edited%2520videos%2520with%2520state-of-the-art%250Aperformance%2520in%2520accuracy%252C%2520motion%2520smoothness%252C%2520realism%252C%2520and%2520fidelity%2520to%2520unedited%250Acontent.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2411.17592v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=VideoDirector%3A%20Precise%20Video%20Editing%20via%20Text-to-Video%20Models&entry.906535625=Yukun%20Wang%20and%20Longguang%20Wang%20and%20Zhiyuan%20Ma%20and%20Qibin%20Hu%20and%20Kai%20Xu%20and%20Yulan%20Guo&entry.1292438233=%20%20Despite%20the%20typical%20inversion-then-editing%20paradigm%20using%20text-to-image%20%28T2I%29%0Amodels%20has%20demonstrated%20promising%20results%2C%20directly%20extending%20it%20to%0Atext-to-video%20%28T2V%29%20models%20still%20suffers%20severe%20artifacts%20such%20as%20color%0Aflickering%20and%20content%20distortion.%20Consequently%2C%20current%20video%20editing%20methods%0Aprimarily%20rely%20on%20T2I%20models%2C%20which%20inherently%20lack%20temporal-coherence%0Agenerative%20ability%2C%20often%20resulting%20in%20inferior%20editing%20results.%20In%20this%20paper%2C%0Awe%20attribute%20the%20failure%20of%20the%20typical%20editing%20paradigm%20to%3A%201%29%20Tightly%0ASpatial-temporal%20Coupling.%20The%20vanilla%20pivotal-based%20inversion%20strategy%0Astruggles%20to%20disentangle%20spatial-temporal%20information%20in%20the%20video%20diffusion%0Amodel%3B%202%29%20Complicated%20Spatial-temporal%20Layout.%20The%20vanilla%20cross-attention%0Acontrol%20is%20deficient%20in%20preserving%20the%20unedited%20content.%20To%20address%20these%0Alimitations%2C%20we%20propose%20a%20spatial-temporal%20decoupled%20guidance%20%28STDG%29%20and%0Amulti-frame%20null-text%20optimization%20strategy%20to%20provide%20pivotal%20temporal%20cues%0Afor%20more%20precise%20pivotal%20inversion.%20Furthermore%2C%20we%20introduce%20a%20self-attention%0Acontrol%20strategy%20to%20maintain%20higher%20fidelity%20for%20precise%20partial%20content%0Aediting.%20Experimental%20results%20demonstrate%20that%20our%20method%20%28termed%0AVideoDirector%29%20effectively%20harnesses%20the%20powerful%20temporal%20generation%0Acapabilities%20of%20T2V%20models%2C%20producing%20edited%20videos%20with%20state-of-the-art%0Aperformance%20in%20accuracy%2C%20motion%20smoothness%2C%20realism%2C%20and%20fidelity%20to%20unedited%0Acontent.%0A&entry.1838667208=http%3A//arxiv.org/abs/2411.17592v1&entry.124074799=Read"},
{"title": "BRACTIVE: A Brain Activation Approach to Human Visual Brain Learning", "author": "Xuan-Bac Nguyen and Hojin Jang and Xin Li and Samee U. Khan and Pawan Sinha and Khoa Luu", "abstract": "  The human brain is a highly efficient processing unit, and understanding how\nit works can inspire new algorithms and architectures in machine learning. In\nthis work, we introduce a novel framework named Brain Activation Network\n(BRACTIVE), a transformer-based approach to studying the human visual brain.\nThe main objective of BRACTIVE is to align the visual features of subjects with\ncorresponding brain representations via fMRI signals. It allows us to identify\nthe brain's Regions of Interest (ROI) of the subjects. Unlike previous brain\nresearch methods, which can only identify ROIs for one subject at a time and\nare limited by the number of subjects, BRACTIVE automatically extends this\nidentification to multiple subjects and ROIs. Our experiments demonstrate that\nBRACTIVE effectively identifies person-specific regions of interest, such as\nface and body-selective areas, aligning with neuroscience findings and\nindicating potential applicability to various object categories. More\nimportantly, we found that leveraging human visual brain activity to guide deep\nneural networks enhances performance across various benchmarks. It encourages\nthe potential of BRACTIVE in both neuroscience and machine intelligence\nstudies.\n", "link": "http://arxiv.org/abs/2405.18808v2", "date": "2024-11-26", "relevancy": 2.6079, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5259}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5259}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5129}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20BRACTIVE%3A%20A%20Brain%20Activation%20Approach%20to%20Human%20Visual%20Brain%20Learning&body=Title%3A%20BRACTIVE%3A%20A%20Brain%20Activation%20Approach%20to%20Human%20Visual%20Brain%20Learning%0AAuthor%3A%20Xuan-Bac%20Nguyen%20and%20Hojin%20Jang%20and%20Xin%20Li%20and%20Samee%20U.%20Khan%20and%20Pawan%20Sinha%20and%20Khoa%20Luu%0AAbstract%3A%20%20%20The%20human%20brain%20is%20a%20highly%20efficient%20processing%20unit%2C%20and%20understanding%20how%0Ait%20works%20can%20inspire%20new%20algorithms%20and%20architectures%20in%20machine%20learning.%20In%0Athis%20work%2C%20we%20introduce%20a%20novel%20framework%20named%20Brain%20Activation%20Network%0A%28BRACTIVE%29%2C%20a%20transformer-based%20approach%20to%20studying%20the%20human%20visual%20brain.%0AThe%20main%20objective%20of%20BRACTIVE%20is%20to%20align%20the%20visual%20features%20of%20subjects%20with%0Acorresponding%20brain%20representations%20via%20fMRI%20signals.%20It%20allows%20us%20to%20identify%0Athe%20brain%27s%20Regions%20of%20Interest%20%28ROI%29%20of%20the%20subjects.%20Unlike%20previous%20brain%0Aresearch%20methods%2C%20which%20can%20only%20identify%20ROIs%20for%20one%20subject%20at%20a%20time%20and%0Aare%20limited%20by%20the%20number%20of%20subjects%2C%20BRACTIVE%20automatically%20extends%20this%0Aidentification%20to%20multiple%20subjects%20and%20ROIs.%20Our%20experiments%20demonstrate%20that%0ABRACTIVE%20effectively%20identifies%20person-specific%20regions%20of%20interest%2C%20such%20as%0Aface%20and%20body-selective%20areas%2C%20aligning%20with%20neuroscience%20findings%20and%0Aindicating%20potential%20applicability%20to%20various%20object%20categories.%20More%0Aimportantly%2C%20we%20found%20that%20leveraging%20human%20visual%20brain%20activity%20to%20guide%20deep%0Aneural%20networks%20enhances%20performance%20across%20various%20benchmarks.%20It%20encourages%0Athe%20potential%20of%20BRACTIVE%20in%20both%20neuroscience%20and%20machine%20intelligence%0Astudies.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2405.18808v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DBRACTIVE%253A%2520A%2520Brain%2520Activation%2520Approach%2520to%2520Human%2520Visual%2520Brain%2520Learning%26entry.906535625%3DXuan-Bac%2520Nguyen%2520and%2520Hojin%2520Jang%2520and%2520Xin%2520Li%2520and%2520Samee%2520U.%2520Khan%2520and%2520Pawan%2520Sinha%2520and%2520Khoa%2520Luu%26entry.1292438233%3D%2520%2520The%2520human%2520brain%2520is%2520a%2520highly%2520efficient%2520processing%2520unit%252C%2520and%2520understanding%2520how%250Ait%2520works%2520can%2520inspire%2520new%2520algorithms%2520and%2520architectures%2520in%2520machine%2520learning.%2520In%250Athis%2520work%252C%2520we%2520introduce%2520a%2520novel%2520framework%2520named%2520Brain%2520Activation%2520Network%250A%2528BRACTIVE%2529%252C%2520a%2520transformer-based%2520approach%2520to%2520studying%2520the%2520human%2520visual%2520brain.%250AThe%2520main%2520objective%2520of%2520BRACTIVE%2520is%2520to%2520align%2520the%2520visual%2520features%2520of%2520subjects%2520with%250Acorresponding%2520brain%2520representations%2520via%2520fMRI%2520signals.%2520It%2520allows%2520us%2520to%2520identify%250Athe%2520brain%2527s%2520Regions%2520of%2520Interest%2520%2528ROI%2529%2520of%2520the%2520subjects.%2520Unlike%2520previous%2520brain%250Aresearch%2520methods%252C%2520which%2520can%2520only%2520identify%2520ROIs%2520for%2520one%2520subject%2520at%2520a%2520time%2520and%250Aare%2520limited%2520by%2520the%2520number%2520of%2520subjects%252C%2520BRACTIVE%2520automatically%2520extends%2520this%250Aidentification%2520to%2520multiple%2520subjects%2520and%2520ROIs.%2520Our%2520experiments%2520demonstrate%2520that%250ABRACTIVE%2520effectively%2520identifies%2520person-specific%2520regions%2520of%2520interest%252C%2520such%2520as%250Aface%2520and%2520body-selective%2520areas%252C%2520aligning%2520with%2520neuroscience%2520findings%2520and%250Aindicating%2520potential%2520applicability%2520to%2520various%2520object%2520categories.%2520More%250Aimportantly%252C%2520we%2520found%2520that%2520leveraging%2520human%2520visual%2520brain%2520activity%2520to%2520guide%2520deep%250Aneural%2520networks%2520enhances%2520performance%2520across%2520various%2520benchmarks.%2520It%2520encourages%250Athe%2520potential%2520of%2520BRACTIVE%2520in%2520both%2520neuroscience%2520and%2520machine%2520intelligence%250Astudies.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2405.18808v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=BRACTIVE%3A%20A%20Brain%20Activation%20Approach%20to%20Human%20Visual%20Brain%20Learning&entry.906535625=Xuan-Bac%20Nguyen%20and%20Hojin%20Jang%20and%20Xin%20Li%20and%20Samee%20U.%20Khan%20and%20Pawan%20Sinha%20and%20Khoa%20Luu&entry.1292438233=%20%20The%20human%20brain%20is%20a%20highly%20efficient%20processing%20unit%2C%20and%20understanding%20how%0Ait%20works%20can%20inspire%20new%20algorithms%20and%20architectures%20in%20machine%20learning.%20In%0Athis%20work%2C%20we%20introduce%20a%20novel%20framework%20named%20Brain%20Activation%20Network%0A%28BRACTIVE%29%2C%20a%20transformer-based%20approach%20to%20studying%20the%20human%20visual%20brain.%0AThe%20main%20objective%20of%20BRACTIVE%20is%20to%20align%20the%20visual%20features%20of%20subjects%20with%0Acorresponding%20brain%20representations%20via%20fMRI%20signals.%20It%20allows%20us%20to%20identify%0Athe%20brain%27s%20Regions%20of%20Interest%20%28ROI%29%20of%20the%20subjects.%20Unlike%20previous%20brain%0Aresearch%20methods%2C%20which%20can%20only%20identify%20ROIs%20for%20one%20subject%20at%20a%20time%20and%0Aare%20limited%20by%20the%20number%20of%20subjects%2C%20BRACTIVE%20automatically%20extends%20this%0Aidentification%20to%20multiple%20subjects%20and%20ROIs.%20Our%20experiments%20demonstrate%20that%0ABRACTIVE%20effectively%20identifies%20person-specific%20regions%20of%20interest%2C%20such%20as%0Aface%20and%20body-selective%20areas%2C%20aligning%20with%20neuroscience%20findings%20and%0Aindicating%20potential%20applicability%20to%20various%20object%20categories.%20More%0Aimportantly%2C%20we%20found%20that%20leveraging%20human%20visual%20brain%20activity%20to%20guide%20deep%0Aneural%20networks%20enhances%20performance%20across%20various%20benchmarks.%20It%20encourages%0Athe%20potential%20of%20BRACTIVE%20in%20both%20neuroscience%20and%20machine%20intelligence%0Astudies.%0A&entry.1838667208=http%3A//arxiv.org/abs/2405.18808v2&entry.124074799=Read"},
{"title": "DWCL: Dual-Weighted Contrastive Learning for Multi-View Clustering", "author": "Zhihui Zhang and Xiaoshuai Hao and Hanning Yuan and Lianhua Chi and Qi Guo and Qi Li and Ziqiang Yuan and Jinhui Pang and Yexin Li and Sijie Ruan", "abstract": "  Multi-view contrastive clustering (MVCC) has gained significant attention for\ngenerating consistent clustering structures from multiple views through\ncontrastive learning. However, most existing MVCC methods create cross-views by\ncombining any two views, leading to a high volume of unreliable pairs.\nFurthermore, these approaches often overlook discrepancies in multi-view\nrepresentations, resulting in representation degeneration. To address these\nchallenges, we introduce a novel model called Dual-Weighted Contrastive\nLearning (DWCL) for Multi-View Clustering. Specifically, to reduce the impact\nof unreliable cross-views, we introduce an innovative Best-Other (B-O)\ncontrastive mechanism that enhances the representation of individual views at a\nlow computational cost. Furthermore, we develop a dual weighting strategy that\ncombines a view quality weight, reflecting the quality of each view, with a\nview discrepancy weight. This approach effectively mitigates representation\ndegeneration by downplaying cross-views that are both low in quality and high\nin discrepancy. We theoretically validate the efficiency of the B-O contrastive\nmechanism and the effectiveness of the dual weighting strategy. Extensive\nexperiments demonstrate that DWCL outperforms previous methods across eight\nmulti-view datasets, showcasing superior performance and robustness in MVCC.\nSpecifically, our method achieves absolute accuracy improvements of 5.4\\% and\n5.6\\% compared to state-of-the-art methods on the Caltech6V7 and MSRCv1\ndatasets, respectively.\n", "link": "http://arxiv.org/abs/2411.17354v1", "date": "2024-11-26", "relevancy": 2.5932, "topK": [{"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5431}, {"title": "CAT3D: Create Anything in 3D with Multi-View Diffusion Models", "link": "http://arxiv.org/abs/2405.10314v1", "similarity": 0.5064}, {"title": "CAT3D: Create Anything in 3D with Multi-View Diffusion Models", "link": "http://arxiv.org/abs/2405.10314v1", "similarity": 0.5064}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20DWCL%3A%20Dual-Weighted%20Contrastive%20Learning%20for%20Multi-View%20Clustering&body=Title%3A%20DWCL%3A%20Dual-Weighted%20Contrastive%20Learning%20for%20Multi-View%20Clustering%0AAuthor%3A%20Zhihui%20Zhang%20and%20Xiaoshuai%20Hao%20and%20Hanning%20Yuan%20and%20Lianhua%20Chi%20and%20Qi%20Guo%20and%20Qi%20Li%20and%20Ziqiang%20Yuan%20and%20Jinhui%20Pang%20and%20Yexin%20Li%20and%20Sijie%20Ruan%0AAbstract%3A%20%20%20Multi-view%20contrastive%20clustering%20%28MVCC%29%20has%20gained%20significant%20attention%20for%0Agenerating%20consistent%20clustering%20structures%20from%20multiple%20views%20through%0Acontrastive%20learning.%20However%2C%20most%20existing%20MVCC%20methods%20create%20cross-views%20by%0Acombining%20any%20two%20views%2C%20leading%20to%20a%20high%20volume%20of%20unreliable%20pairs.%0AFurthermore%2C%20these%20approaches%20often%20overlook%20discrepancies%20in%20multi-view%0Arepresentations%2C%20resulting%20in%20representation%20degeneration.%20To%20address%20these%0Achallenges%2C%20we%20introduce%20a%20novel%20model%20called%20Dual-Weighted%20Contrastive%0ALearning%20%28DWCL%29%20for%20Multi-View%20Clustering.%20Specifically%2C%20to%20reduce%20the%20impact%0Aof%20unreliable%20cross-views%2C%20we%20introduce%20an%20innovative%20Best-Other%20%28B-O%29%0Acontrastive%20mechanism%20that%20enhances%20the%20representation%20of%20individual%20views%20at%20a%0Alow%20computational%20cost.%20Furthermore%2C%20we%20develop%20a%20dual%20weighting%20strategy%20that%0Acombines%20a%20view%20quality%20weight%2C%20reflecting%20the%20quality%20of%20each%20view%2C%20with%20a%0Aview%20discrepancy%20weight.%20This%20approach%20effectively%20mitigates%20representation%0Adegeneration%20by%20downplaying%20cross-views%20that%20are%20both%20low%20in%20quality%20and%20high%0Ain%20discrepancy.%20We%20theoretically%20validate%20the%20efficiency%20of%20the%20B-O%20contrastive%0Amechanism%20and%20the%20effectiveness%20of%20the%20dual%20weighting%20strategy.%20Extensive%0Aexperiments%20demonstrate%20that%20DWCL%20outperforms%20previous%20methods%20across%20eight%0Amulti-view%20datasets%2C%20showcasing%20superior%20performance%20and%20robustness%20in%20MVCC.%0ASpecifically%2C%20our%20method%20achieves%20absolute%20accuracy%20improvements%20of%205.4%5C%25%20and%0A5.6%5C%25%20compared%20to%20state-of-the-art%20methods%20on%20the%20Caltech6V7%20and%20MSRCv1%0Adatasets%2C%20respectively.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2411.17354v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DDWCL%253A%2520Dual-Weighted%2520Contrastive%2520Learning%2520for%2520Multi-View%2520Clustering%26entry.906535625%3DZhihui%2520Zhang%2520and%2520Xiaoshuai%2520Hao%2520and%2520Hanning%2520Yuan%2520and%2520Lianhua%2520Chi%2520and%2520Qi%2520Guo%2520and%2520Qi%2520Li%2520and%2520Ziqiang%2520Yuan%2520and%2520Jinhui%2520Pang%2520and%2520Yexin%2520Li%2520and%2520Sijie%2520Ruan%26entry.1292438233%3D%2520%2520Multi-view%2520contrastive%2520clustering%2520%2528MVCC%2529%2520has%2520gained%2520significant%2520attention%2520for%250Agenerating%2520consistent%2520clustering%2520structures%2520from%2520multiple%2520views%2520through%250Acontrastive%2520learning.%2520However%252C%2520most%2520existing%2520MVCC%2520methods%2520create%2520cross-views%2520by%250Acombining%2520any%2520two%2520views%252C%2520leading%2520to%2520a%2520high%2520volume%2520of%2520unreliable%2520pairs.%250AFurthermore%252C%2520these%2520approaches%2520often%2520overlook%2520discrepancies%2520in%2520multi-view%250Arepresentations%252C%2520resulting%2520in%2520representation%2520degeneration.%2520To%2520address%2520these%250Achallenges%252C%2520we%2520introduce%2520a%2520novel%2520model%2520called%2520Dual-Weighted%2520Contrastive%250ALearning%2520%2528DWCL%2529%2520for%2520Multi-View%2520Clustering.%2520Specifically%252C%2520to%2520reduce%2520the%2520impact%250Aof%2520unreliable%2520cross-views%252C%2520we%2520introduce%2520an%2520innovative%2520Best-Other%2520%2528B-O%2529%250Acontrastive%2520mechanism%2520that%2520enhances%2520the%2520representation%2520of%2520individual%2520views%2520at%2520a%250Alow%2520computational%2520cost.%2520Furthermore%252C%2520we%2520develop%2520a%2520dual%2520weighting%2520strategy%2520that%250Acombines%2520a%2520view%2520quality%2520weight%252C%2520reflecting%2520the%2520quality%2520of%2520each%2520view%252C%2520with%2520a%250Aview%2520discrepancy%2520weight.%2520This%2520approach%2520effectively%2520mitigates%2520representation%250Adegeneration%2520by%2520downplaying%2520cross-views%2520that%2520are%2520both%2520low%2520in%2520quality%2520and%2520high%250Ain%2520discrepancy.%2520We%2520theoretically%2520validate%2520the%2520efficiency%2520of%2520the%2520B-O%2520contrastive%250Amechanism%2520and%2520the%2520effectiveness%2520of%2520the%2520dual%2520weighting%2520strategy.%2520Extensive%250Aexperiments%2520demonstrate%2520that%2520DWCL%2520outperforms%2520previous%2520methods%2520across%2520eight%250Amulti-view%2520datasets%252C%2520showcasing%2520superior%2520performance%2520and%2520robustness%2520in%2520MVCC.%250ASpecifically%252C%2520our%2520method%2520achieves%2520absolute%2520accuracy%2520improvements%2520of%25205.4%255C%2525%2520and%250A5.6%255C%2525%2520compared%2520to%2520state-of-the-art%2520methods%2520on%2520the%2520Caltech6V7%2520and%2520MSRCv1%250Adatasets%252C%2520respectively.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2411.17354v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=DWCL%3A%20Dual-Weighted%20Contrastive%20Learning%20for%20Multi-View%20Clustering&entry.906535625=Zhihui%20Zhang%20and%20Xiaoshuai%20Hao%20and%20Hanning%20Yuan%20and%20Lianhua%20Chi%20and%20Qi%20Guo%20and%20Qi%20Li%20and%20Ziqiang%20Yuan%20and%20Jinhui%20Pang%20and%20Yexin%20Li%20and%20Sijie%20Ruan&entry.1292438233=%20%20Multi-view%20contrastive%20clustering%20%28MVCC%29%20has%20gained%20significant%20attention%20for%0Agenerating%20consistent%20clustering%20structures%20from%20multiple%20views%20through%0Acontrastive%20learning.%20However%2C%20most%20existing%20MVCC%20methods%20create%20cross-views%20by%0Acombining%20any%20two%20views%2C%20leading%20to%20a%20high%20volume%20of%20unreliable%20pairs.%0AFurthermore%2C%20these%20approaches%20often%20overlook%20discrepancies%20in%20multi-view%0Arepresentations%2C%20resulting%20in%20representation%20degeneration.%20To%20address%20these%0Achallenges%2C%20we%20introduce%20a%20novel%20model%20called%20Dual-Weighted%20Contrastive%0ALearning%20%28DWCL%29%20for%20Multi-View%20Clustering.%20Specifically%2C%20to%20reduce%20the%20impact%0Aof%20unreliable%20cross-views%2C%20we%20introduce%20an%20innovative%20Best-Other%20%28B-O%29%0Acontrastive%20mechanism%20that%20enhances%20the%20representation%20of%20individual%20views%20at%20a%0Alow%20computational%20cost.%20Furthermore%2C%20we%20develop%20a%20dual%20weighting%20strategy%20that%0Acombines%20a%20view%20quality%20weight%2C%20reflecting%20the%20quality%20of%20each%20view%2C%20with%20a%0Aview%20discrepancy%20weight.%20This%20approach%20effectively%20mitigates%20representation%0Adegeneration%20by%20downplaying%20cross-views%20that%20are%20both%20low%20in%20quality%20and%20high%0Ain%20discrepancy.%20We%20theoretically%20validate%20the%20efficiency%20of%20the%20B-O%20contrastive%0Amechanism%20and%20the%20effectiveness%20of%20the%20dual%20weighting%20strategy.%20Extensive%0Aexperiments%20demonstrate%20that%20DWCL%20outperforms%20previous%20methods%20across%20eight%0Amulti-view%20datasets%2C%20showcasing%20superior%20performance%20and%20robustness%20in%20MVCC.%0ASpecifically%2C%20our%20method%20achieves%20absolute%20accuracy%20improvements%20of%205.4%5C%25%20and%0A5.6%5C%25%20compared%20to%20state-of-the-art%20methods%20on%20the%20Caltech6V7%20and%20MSRCv1%0Adatasets%2C%20respectively.%0A&entry.1838667208=http%3A//arxiv.org/abs/2411.17354v1&entry.124074799=Read"},
{"title": "AnchorCrafter: Animate CyberAnchors Saling Your Products via\n  Human-Object Interacting Video Generation", "author": "Ziyi Xu and Ziyao Huang and Juan Cao and Yong Zhang and Xiaodong Cun and Qing Shuai and Yuchen Wang and Linchao Bao and Jintao Li and Fan Tang", "abstract": "  The automatic generation of anchor-style product promotion videos presents\npromising opportunities in online commerce, advertising, and consumer\nengagement. However, this remains a challenging task despite significant\nadvancements in pose-guided human video generation. In addressing this\nchallenge, we identify the integration of human-object interactions (HOI) into\npose-guided human video generation as a core issue. To this end, we introduce\nAnchorCrafter, a novel diffusion-based system designed to generate 2D videos\nfeaturing a target human and a customized object, achieving high visual\nfidelity and controllable interactions. Specifically, we propose two key\ninnovations: the HOI-appearance perception, which enhances object appearance\nrecognition from arbitrary multi-view perspectives and disentangles object and\nhuman appearance, and the HOI-motion injection, which enables complex\nhuman-object interactions by overcoming challenges in object trajectory\nconditioning and inter-occlusion management. Additionally, we introduce the\nHOI-region reweighting loss, a training objective that enhances the learning of\nobject details. Extensive experiments demonstrate that our proposed system\noutperforms existing methods in preserving object appearance and shape\nawareness, while simultaneously maintaining consistency in human appearance and\nmotion. Project page: https://cangcz.github.io/Anchor-Crafter/\n", "link": "http://arxiv.org/abs/2411.17383v1", "date": "2024-11-26", "relevancy": 2.5893, "topK": [{"title": "VirtualModel: Generating Object-ID-retentive Human-object Interaction\n  Image by Diffusion Model for E-commerce Marketing", "link": "http://arxiv.org/abs/2405.09985v1", "similarity": 0.6536}, {"title": "Customize-A-Video: One-Shot Motion Customization of Text-to-Video\n  Diffusion Models", "link": "http://arxiv.org/abs/2402.14780v1", "similarity": 0.6439}, {"title": "PoseAnimate: Zero-shot high fidelity pose controllable character\n  animation", "link": "http://arxiv.org/abs/2404.13680v2", "similarity": 0.6425}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20AnchorCrafter%3A%20Animate%20CyberAnchors%20Saling%20Your%20Products%20via%0A%20%20Human-Object%20Interacting%20Video%20Generation&body=Title%3A%20AnchorCrafter%3A%20Animate%20CyberAnchors%20Saling%20Your%20Products%20via%0A%20%20Human-Object%20Interacting%20Video%20Generation%0AAuthor%3A%20Ziyi%20Xu%20and%20Ziyao%20Huang%20and%20Juan%20Cao%20and%20Yong%20Zhang%20and%20Xiaodong%20Cun%20and%20Qing%20Shuai%20and%20Yuchen%20Wang%20and%20Linchao%20Bao%20and%20Jintao%20Li%20and%20Fan%20Tang%0AAbstract%3A%20%20%20The%20automatic%20generation%20of%20anchor-style%20product%20promotion%20videos%20presents%0Apromising%20opportunities%20in%20online%20commerce%2C%20advertising%2C%20and%20consumer%0Aengagement.%20However%2C%20this%20remains%20a%20challenging%20task%20despite%20significant%0Aadvancements%20in%20pose-guided%20human%20video%20generation.%20In%20addressing%20this%0Achallenge%2C%20we%20identify%20the%20integration%20of%20human-object%20interactions%20%28HOI%29%20into%0Apose-guided%20human%20video%20generation%20as%20a%20core%20issue.%20To%20this%20end%2C%20we%20introduce%0AAnchorCrafter%2C%20a%20novel%20diffusion-based%20system%20designed%20to%20generate%202D%20videos%0Afeaturing%20a%20target%20human%20and%20a%20customized%20object%2C%20achieving%20high%20visual%0Afidelity%20and%20controllable%20interactions.%20Specifically%2C%20we%20propose%20two%20key%0Ainnovations%3A%20the%20HOI-appearance%20perception%2C%20which%20enhances%20object%20appearance%0Arecognition%20from%20arbitrary%20multi-view%20perspectives%20and%20disentangles%20object%20and%0Ahuman%20appearance%2C%20and%20the%20HOI-motion%20injection%2C%20which%20enables%20complex%0Ahuman-object%20interactions%20by%20overcoming%20challenges%20in%20object%20trajectory%0Aconditioning%20and%20inter-occlusion%20management.%20Additionally%2C%20we%20introduce%20the%0AHOI-region%20reweighting%20loss%2C%20a%20training%20objective%20that%20enhances%20the%20learning%20of%0Aobject%20details.%20Extensive%20experiments%20demonstrate%20that%20our%20proposed%20system%0Aoutperforms%20existing%20methods%20in%20preserving%20object%20appearance%20and%20shape%0Aawareness%2C%20while%20simultaneously%20maintaining%20consistency%20in%20human%20appearance%20and%0Amotion.%20Project%20page%3A%20https%3A//cangcz.github.io/Anchor-Crafter/%0A%0ALink%3A%20http%3A//arxiv.org/abs/2411.17383v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DAnchorCrafter%253A%2520Animate%2520CyberAnchors%2520Saling%2520Your%2520Products%2520via%250A%2520%2520Human-Object%2520Interacting%2520Video%2520Generation%26entry.906535625%3DZiyi%2520Xu%2520and%2520Ziyao%2520Huang%2520and%2520Juan%2520Cao%2520and%2520Yong%2520Zhang%2520and%2520Xiaodong%2520Cun%2520and%2520Qing%2520Shuai%2520and%2520Yuchen%2520Wang%2520and%2520Linchao%2520Bao%2520and%2520Jintao%2520Li%2520and%2520Fan%2520Tang%26entry.1292438233%3D%2520%2520The%2520automatic%2520generation%2520of%2520anchor-style%2520product%2520promotion%2520videos%2520presents%250Apromising%2520opportunities%2520in%2520online%2520commerce%252C%2520advertising%252C%2520and%2520consumer%250Aengagement.%2520However%252C%2520this%2520remains%2520a%2520challenging%2520task%2520despite%2520significant%250Aadvancements%2520in%2520pose-guided%2520human%2520video%2520generation.%2520In%2520addressing%2520this%250Achallenge%252C%2520we%2520identify%2520the%2520integration%2520of%2520human-object%2520interactions%2520%2528HOI%2529%2520into%250Apose-guided%2520human%2520video%2520generation%2520as%2520a%2520core%2520issue.%2520To%2520this%2520end%252C%2520we%2520introduce%250AAnchorCrafter%252C%2520a%2520novel%2520diffusion-based%2520system%2520designed%2520to%2520generate%25202D%2520videos%250Afeaturing%2520a%2520target%2520human%2520and%2520a%2520customized%2520object%252C%2520achieving%2520high%2520visual%250Afidelity%2520and%2520controllable%2520interactions.%2520Specifically%252C%2520we%2520propose%2520two%2520key%250Ainnovations%253A%2520the%2520HOI-appearance%2520perception%252C%2520which%2520enhances%2520object%2520appearance%250Arecognition%2520from%2520arbitrary%2520multi-view%2520perspectives%2520and%2520disentangles%2520object%2520and%250Ahuman%2520appearance%252C%2520and%2520the%2520HOI-motion%2520injection%252C%2520which%2520enables%2520complex%250Ahuman-object%2520interactions%2520by%2520overcoming%2520challenges%2520in%2520object%2520trajectory%250Aconditioning%2520and%2520inter-occlusion%2520management.%2520Additionally%252C%2520we%2520introduce%2520the%250AHOI-region%2520reweighting%2520loss%252C%2520a%2520training%2520objective%2520that%2520enhances%2520the%2520learning%2520of%250Aobject%2520details.%2520Extensive%2520experiments%2520demonstrate%2520that%2520our%2520proposed%2520system%250Aoutperforms%2520existing%2520methods%2520in%2520preserving%2520object%2520appearance%2520and%2520shape%250Aawareness%252C%2520while%2520simultaneously%2520maintaining%2520consistency%2520in%2520human%2520appearance%2520and%250Amotion.%2520Project%2520page%253A%2520https%253A//cangcz.github.io/Anchor-Crafter/%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2411.17383v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=AnchorCrafter%3A%20Animate%20CyberAnchors%20Saling%20Your%20Products%20via%0A%20%20Human-Object%20Interacting%20Video%20Generation&entry.906535625=Ziyi%20Xu%20and%20Ziyao%20Huang%20and%20Juan%20Cao%20and%20Yong%20Zhang%20and%20Xiaodong%20Cun%20and%20Qing%20Shuai%20and%20Yuchen%20Wang%20and%20Linchao%20Bao%20and%20Jintao%20Li%20and%20Fan%20Tang&entry.1292438233=%20%20The%20automatic%20generation%20of%20anchor-style%20product%20promotion%20videos%20presents%0Apromising%20opportunities%20in%20online%20commerce%2C%20advertising%2C%20and%20consumer%0Aengagement.%20However%2C%20this%20remains%20a%20challenging%20task%20despite%20significant%0Aadvancements%20in%20pose-guided%20human%20video%20generation.%20In%20addressing%20this%0Achallenge%2C%20we%20identify%20the%20integration%20of%20human-object%20interactions%20%28HOI%29%20into%0Apose-guided%20human%20video%20generation%20as%20a%20core%20issue.%20To%20this%20end%2C%20we%20introduce%0AAnchorCrafter%2C%20a%20novel%20diffusion-based%20system%20designed%20to%20generate%202D%20videos%0Afeaturing%20a%20target%20human%20and%20a%20customized%20object%2C%20achieving%20high%20visual%0Afidelity%20and%20controllable%20interactions.%20Specifically%2C%20we%20propose%20two%20key%0Ainnovations%3A%20the%20HOI-appearance%20perception%2C%20which%20enhances%20object%20appearance%0Arecognition%20from%20arbitrary%20multi-view%20perspectives%20and%20disentangles%20object%20and%0Ahuman%20appearance%2C%20and%20the%20HOI-motion%20injection%2C%20which%20enables%20complex%0Ahuman-object%20interactions%20by%20overcoming%20challenges%20in%20object%20trajectory%0Aconditioning%20and%20inter-occlusion%20management.%20Additionally%2C%20we%20introduce%20the%0AHOI-region%20reweighting%20loss%2C%20a%20training%20objective%20that%20enhances%20the%20learning%20of%0Aobject%20details.%20Extensive%20experiments%20demonstrate%20that%20our%20proposed%20system%0Aoutperforms%20existing%20methods%20in%20preserving%20object%20appearance%20and%20shape%0Aawareness%2C%20while%20simultaneously%20maintaining%20consistency%20in%20human%20appearance%20and%0Amotion.%20Project%20page%3A%20https%3A//cangcz.github.io/Anchor-Crafter/%0A&entry.1838667208=http%3A//arxiv.org/abs/2411.17383v1&entry.124074799=Read"},
{"title": "Automatic Skull Reconstruction by Deep Learnable Symmetry Enforcement", "author": "Marek Wodzinski and Mateusz Daniol and Daria Hemmerling", "abstract": "  Every year, thousands of people suffer from skull damage and require\npersonalized implants to fill the cranial cavity. Unfortunately, the waiting\ntime for reconstruction surgery can extend to several weeks or even months,\nespecially in less developed countries. One factor contributing to the extended\nwaiting period is the intricate process of personalized implant modeling.\nCurrently, the preparation of these implants by experienced biomechanical\nexperts is both costly and time-consuming. Recent advances in artificial\nintelligence, especially in deep learning, offer promising potential for\nautomating the process. However, deep learning-based cranial reconstruction\nfaces several challenges: (i) the limited size of training datasets, (ii) the\nhigh resolution of the volumetric data, and (iii) significant data\nheterogeneity. In this work, we propose a novel approach to address these\nchallenges by enhancing the reconstruction through learnable symmetry\nenforcement. We demonstrate that it is possible to train a neural network\ndedicated to calculating skull symmetry, which can be utilized either as an\nadditional objective function during training or as a post-reconstruction\nobjective during the refinement step. We quantitatively evaluate the proposed\nmethod using open SkullBreak and SkullFix datasets, and qualitatively using\nreal clinical cases. The results indicate that the symmetry-preserving\nreconstruction network achieves considerably better outcomes compared to the\nbaseline (0.94/0.94/1.31 vs 0.84/0.76/2.43 in terms of DSC, bDSC, and HD95).\nMoreover, the results are comparable to the best-performing methods while\nrequiring significantly fewer computational resources (< 500 vs > 100,000 GPU\nhours). The proposed method is a considerable contribution to the field of\napplied artificial intelligence in medicine and is a step toward automatic\ncranial defect reconstruction in clinical practice.\n", "link": "http://arxiv.org/abs/2411.17342v1", "date": "2024-11-26", "relevancy": 2.5804, "topK": [{"title": "Dynamic Gaussians Mesh: Consistent Mesh Reconstruction from Monocular\n  Videos", "link": "http://arxiv.org/abs/2404.12379v2", "similarity": 0.5351}, {"title": "GST: Precise 3D Human Body from a Single Image with Gaussian Splatting\n  Transformers", "link": "http://arxiv.org/abs/2409.04196v1", "similarity": 0.5181}, {"title": "3D Gaussian Blendshapes for Head Avatar Animation", "link": "http://arxiv.org/abs/2404.19398v2", "similarity": 0.4951}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Automatic%20Skull%20Reconstruction%20by%20Deep%20Learnable%20Symmetry%20Enforcement&body=Title%3A%20Automatic%20Skull%20Reconstruction%20by%20Deep%20Learnable%20Symmetry%20Enforcement%0AAuthor%3A%20Marek%20Wodzinski%20and%20Mateusz%20Daniol%20and%20Daria%20Hemmerling%0AAbstract%3A%20%20%20Every%20year%2C%20thousands%20of%20people%20suffer%20from%20skull%20damage%20and%20require%0Apersonalized%20implants%20to%20fill%20the%20cranial%20cavity.%20Unfortunately%2C%20the%20waiting%0Atime%20for%20reconstruction%20surgery%20can%20extend%20to%20several%20weeks%20or%20even%20months%2C%0Aespecially%20in%20less%20developed%20countries.%20One%20factor%20contributing%20to%20the%20extended%0Awaiting%20period%20is%20the%20intricate%20process%20of%20personalized%20implant%20modeling.%0ACurrently%2C%20the%20preparation%20of%20these%20implants%20by%20experienced%20biomechanical%0Aexperts%20is%20both%20costly%20and%20time-consuming.%20Recent%20advances%20in%20artificial%0Aintelligence%2C%20especially%20in%20deep%20learning%2C%20offer%20promising%20potential%20for%0Aautomating%20the%20process.%20However%2C%20deep%20learning-based%20cranial%20reconstruction%0Afaces%20several%20challenges%3A%20%28i%29%20the%20limited%20size%20of%20training%20datasets%2C%20%28ii%29%20the%0Ahigh%20resolution%20of%20the%20volumetric%20data%2C%20and%20%28iii%29%20significant%20data%0Aheterogeneity.%20In%20this%20work%2C%20we%20propose%20a%20novel%20approach%20to%20address%20these%0Achallenges%20by%20enhancing%20the%20reconstruction%20through%20learnable%20symmetry%0Aenforcement.%20We%20demonstrate%20that%20it%20is%20possible%20to%20train%20a%20neural%20network%0Adedicated%20to%20calculating%20skull%20symmetry%2C%20which%20can%20be%20utilized%20either%20as%20an%0Aadditional%20objective%20function%20during%20training%20or%20as%20a%20post-reconstruction%0Aobjective%20during%20the%20refinement%20step.%20We%20quantitatively%20evaluate%20the%20proposed%0Amethod%20using%20open%20SkullBreak%20and%20SkullFix%20datasets%2C%20and%20qualitatively%20using%0Areal%20clinical%20cases.%20The%20results%20indicate%20that%20the%20symmetry-preserving%0Areconstruction%20network%20achieves%20considerably%20better%20outcomes%20compared%20to%20the%0Abaseline%20%280.94/0.94/1.31%20vs%200.84/0.76/2.43%20in%20terms%20of%20DSC%2C%20bDSC%2C%20and%20HD95%29.%0AMoreover%2C%20the%20results%20are%20comparable%20to%20the%20best-performing%20methods%20while%0Arequiring%20significantly%20fewer%20computational%20resources%20%28%3C%20500%20vs%20%3E%20100%2C000%20GPU%0Ahours%29.%20The%20proposed%20method%20is%20a%20considerable%20contribution%20to%20the%20field%20of%0Aapplied%20artificial%20intelligence%20in%20medicine%20and%20is%20a%20step%20toward%20automatic%0Acranial%20defect%20reconstruction%20in%20clinical%20practice.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2411.17342v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DAutomatic%2520Skull%2520Reconstruction%2520by%2520Deep%2520Learnable%2520Symmetry%2520Enforcement%26entry.906535625%3DMarek%2520Wodzinski%2520and%2520Mateusz%2520Daniol%2520and%2520Daria%2520Hemmerling%26entry.1292438233%3D%2520%2520Every%2520year%252C%2520thousands%2520of%2520people%2520suffer%2520from%2520skull%2520damage%2520and%2520require%250Apersonalized%2520implants%2520to%2520fill%2520the%2520cranial%2520cavity.%2520Unfortunately%252C%2520the%2520waiting%250Atime%2520for%2520reconstruction%2520surgery%2520can%2520extend%2520to%2520several%2520weeks%2520or%2520even%2520months%252C%250Aespecially%2520in%2520less%2520developed%2520countries.%2520One%2520factor%2520contributing%2520to%2520the%2520extended%250Awaiting%2520period%2520is%2520the%2520intricate%2520process%2520of%2520personalized%2520implant%2520modeling.%250ACurrently%252C%2520the%2520preparation%2520of%2520these%2520implants%2520by%2520experienced%2520biomechanical%250Aexperts%2520is%2520both%2520costly%2520and%2520time-consuming.%2520Recent%2520advances%2520in%2520artificial%250Aintelligence%252C%2520especially%2520in%2520deep%2520learning%252C%2520offer%2520promising%2520potential%2520for%250Aautomating%2520the%2520process.%2520However%252C%2520deep%2520learning-based%2520cranial%2520reconstruction%250Afaces%2520several%2520challenges%253A%2520%2528i%2529%2520the%2520limited%2520size%2520of%2520training%2520datasets%252C%2520%2528ii%2529%2520the%250Ahigh%2520resolution%2520of%2520the%2520volumetric%2520data%252C%2520and%2520%2528iii%2529%2520significant%2520data%250Aheterogeneity.%2520In%2520this%2520work%252C%2520we%2520propose%2520a%2520novel%2520approach%2520to%2520address%2520these%250Achallenges%2520by%2520enhancing%2520the%2520reconstruction%2520through%2520learnable%2520symmetry%250Aenforcement.%2520We%2520demonstrate%2520that%2520it%2520is%2520possible%2520to%2520train%2520a%2520neural%2520network%250Adedicated%2520to%2520calculating%2520skull%2520symmetry%252C%2520which%2520can%2520be%2520utilized%2520either%2520as%2520an%250Aadditional%2520objective%2520function%2520during%2520training%2520or%2520as%2520a%2520post-reconstruction%250Aobjective%2520during%2520the%2520refinement%2520step.%2520We%2520quantitatively%2520evaluate%2520the%2520proposed%250Amethod%2520using%2520open%2520SkullBreak%2520and%2520SkullFix%2520datasets%252C%2520and%2520qualitatively%2520using%250Areal%2520clinical%2520cases.%2520The%2520results%2520indicate%2520that%2520the%2520symmetry-preserving%250Areconstruction%2520network%2520achieves%2520considerably%2520better%2520outcomes%2520compared%2520to%2520the%250Abaseline%2520%25280.94/0.94/1.31%2520vs%25200.84/0.76/2.43%2520in%2520terms%2520of%2520DSC%252C%2520bDSC%252C%2520and%2520HD95%2529.%250AMoreover%252C%2520the%2520results%2520are%2520comparable%2520to%2520the%2520best-performing%2520methods%2520while%250Arequiring%2520significantly%2520fewer%2520computational%2520resources%2520%2528%253C%2520500%2520vs%2520%253E%2520100%252C000%2520GPU%250Ahours%2529.%2520The%2520proposed%2520method%2520is%2520a%2520considerable%2520contribution%2520to%2520the%2520field%2520of%250Aapplied%2520artificial%2520intelligence%2520in%2520medicine%2520and%2520is%2520a%2520step%2520toward%2520automatic%250Acranial%2520defect%2520reconstruction%2520in%2520clinical%2520practice.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2411.17342v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Automatic%20Skull%20Reconstruction%20by%20Deep%20Learnable%20Symmetry%20Enforcement&entry.906535625=Marek%20Wodzinski%20and%20Mateusz%20Daniol%20and%20Daria%20Hemmerling&entry.1292438233=%20%20Every%20year%2C%20thousands%20of%20people%20suffer%20from%20skull%20damage%20and%20require%0Apersonalized%20implants%20to%20fill%20the%20cranial%20cavity.%20Unfortunately%2C%20the%20waiting%0Atime%20for%20reconstruction%20surgery%20can%20extend%20to%20several%20weeks%20or%20even%20months%2C%0Aespecially%20in%20less%20developed%20countries.%20One%20factor%20contributing%20to%20the%20extended%0Awaiting%20period%20is%20the%20intricate%20process%20of%20personalized%20implant%20modeling.%0ACurrently%2C%20the%20preparation%20of%20these%20implants%20by%20experienced%20biomechanical%0Aexperts%20is%20both%20costly%20and%20time-consuming.%20Recent%20advances%20in%20artificial%0Aintelligence%2C%20especially%20in%20deep%20learning%2C%20offer%20promising%20potential%20for%0Aautomating%20the%20process.%20However%2C%20deep%20learning-based%20cranial%20reconstruction%0Afaces%20several%20challenges%3A%20%28i%29%20the%20limited%20size%20of%20training%20datasets%2C%20%28ii%29%20the%0Ahigh%20resolution%20of%20the%20volumetric%20data%2C%20and%20%28iii%29%20significant%20data%0Aheterogeneity.%20In%20this%20work%2C%20we%20propose%20a%20novel%20approach%20to%20address%20these%0Achallenges%20by%20enhancing%20the%20reconstruction%20through%20learnable%20symmetry%0Aenforcement.%20We%20demonstrate%20that%20it%20is%20possible%20to%20train%20a%20neural%20network%0Adedicated%20to%20calculating%20skull%20symmetry%2C%20which%20can%20be%20utilized%20either%20as%20an%0Aadditional%20objective%20function%20during%20training%20or%20as%20a%20post-reconstruction%0Aobjective%20during%20the%20refinement%20step.%20We%20quantitatively%20evaluate%20the%20proposed%0Amethod%20using%20open%20SkullBreak%20and%20SkullFix%20datasets%2C%20and%20qualitatively%20using%0Areal%20clinical%20cases.%20The%20results%20indicate%20that%20the%20symmetry-preserving%0Areconstruction%20network%20achieves%20considerably%20better%20outcomes%20compared%20to%20the%0Abaseline%20%280.94/0.94/1.31%20vs%200.84/0.76/2.43%20in%20terms%20of%20DSC%2C%20bDSC%2C%20and%20HD95%29.%0AMoreover%2C%20the%20results%20are%20comparable%20to%20the%20best-performing%20methods%20while%0Arequiring%20significantly%20fewer%20computational%20resources%20%28%3C%20500%20vs%20%3E%20100%2C000%20GPU%0Ahours%29.%20The%20proposed%20method%20is%20a%20considerable%20contribution%20to%20the%20field%20of%0Aapplied%20artificial%20intelligence%20in%20medicine%20and%20is%20a%20step%20toward%20automatic%0Acranial%20defect%20reconstruction%20in%20clinical%20practice.%0A&entry.1838667208=http%3A//arxiv.org/abs/2411.17342v1&entry.124074799=Read"},
{"title": "Instance-Aware Graph Prompt Learning", "author": "Jiazheng Li and Jundong Li and Chuxu Zhang", "abstract": "  Graph neural networks stand as the predominant technique for graph\nrepresentation learning owing to their strong expressive power, yet the\nperformance highly depends on the availability of high-quality labels in an\nend-to-end manner. Thus the pretraining and fine-tuning paradigm has been\nproposed to mitigate the label cost issue. Subsequently, the gap between the\npretext tasks and downstream tasks has spurred the development of graph prompt\nlearning which inserts a set of graph prompts into the original graph data with\nminimal parameters while preserving competitive performance. However, the\ncurrent exploratory works are still limited since they all concentrate on\nlearning fixed task-specific prompts which may not generalize well across the\ndiverse instances that the task comprises. To tackle this challenge, we\nintroduce Instance-Aware Graph Prompt Learning (IA-GPL) in this paper, aiming\nto generate distinct prompts tailored to different input instances. The process\ninvolves generating intermediate prompts for each instance using a lightweight\narchitecture, quantizing these prompts through trainable codebook vectors, and\nemploying the exponential moving average technique to ensure stable training.\nExtensive experiments conducted on multiple datasets and settings showcase the\nsuperior performance of IA-GPL compared to state-of-the-art baselines.\n", "link": "http://arxiv.org/abs/2411.17676v1", "date": "2024-11-26", "relevancy": 2.515, "topK": [{"title": "Self-supervised Photographic Image Layout Representation Learning", "link": "http://arxiv.org/abs/2403.03740v1", "similarity": 0.542}, {"title": "Genie: Generative Interactive Environments", "link": "http://arxiv.org/abs/2402.15391v1", "similarity": 0.4886}, {"title": "IAE: Implicit Autoencoder for Point Cloud Self-supervised Representation Learning", "link": "https://arxiv.org/abs/2201.00785", "similarity": 0.4783}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Instance-Aware%20Graph%20Prompt%20Learning&body=Title%3A%20Instance-Aware%20Graph%20Prompt%20Learning%0AAuthor%3A%20Jiazheng%20Li%20and%20Jundong%20Li%20and%20Chuxu%20Zhang%0AAbstract%3A%20%20%20Graph%20neural%20networks%20stand%20as%20the%20predominant%20technique%20for%20graph%0Arepresentation%20learning%20owing%20to%20their%20strong%20expressive%20power%2C%20yet%20the%0Aperformance%20highly%20depends%20on%20the%20availability%20of%20high-quality%20labels%20in%20an%0Aend-to-end%20manner.%20Thus%20the%20pretraining%20and%20fine-tuning%20paradigm%20has%20been%0Aproposed%20to%20mitigate%20the%20label%20cost%20issue.%20Subsequently%2C%20the%20gap%20between%20the%0Apretext%20tasks%20and%20downstream%20tasks%20has%20spurred%20the%20development%20of%20graph%20prompt%0Alearning%20which%20inserts%20a%20set%20of%20graph%20prompts%20into%20the%20original%20graph%20data%20with%0Aminimal%20parameters%20while%20preserving%20competitive%20performance.%20However%2C%20the%0Acurrent%20exploratory%20works%20are%20still%20limited%20since%20they%20all%20concentrate%20on%0Alearning%20fixed%20task-specific%20prompts%20which%20may%20not%20generalize%20well%20across%20the%0Adiverse%20instances%20that%20the%20task%20comprises.%20To%20tackle%20this%20challenge%2C%20we%0Aintroduce%20Instance-Aware%20Graph%20Prompt%20Learning%20%28IA-GPL%29%20in%20this%20paper%2C%20aiming%0Ato%20generate%20distinct%20prompts%20tailored%20to%20different%20input%20instances.%20The%20process%0Ainvolves%20generating%20intermediate%20prompts%20for%20each%20instance%20using%20a%20lightweight%0Aarchitecture%2C%20quantizing%20these%20prompts%20through%20trainable%20codebook%20vectors%2C%20and%0Aemploying%20the%20exponential%20moving%20average%20technique%20to%20ensure%20stable%20training.%0AExtensive%20experiments%20conducted%20on%20multiple%20datasets%20and%20settings%20showcase%20the%0Asuperior%20performance%20of%20IA-GPL%20compared%20to%20state-of-the-art%20baselines.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2411.17676v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DInstance-Aware%2520Graph%2520Prompt%2520Learning%26entry.906535625%3DJiazheng%2520Li%2520and%2520Jundong%2520Li%2520and%2520Chuxu%2520Zhang%26entry.1292438233%3D%2520%2520Graph%2520neural%2520networks%2520stand%2520as%2520the%2520predominant%2520technique%2520for%2520graph%250Arepresentation%2520learning%2520owing%2520to%2520their%2520strong%2520expressive%2520power%252C%2520yet%2520the%250Aperformance%2520highly%2520depends%2520on%2520the%2520availability%2520of%2520high-quality%2520labels%2520in%2520an%250Aend-to-end%2520manner.%2520Thus%2520the%2520pretraining%2520and%2520fine-tuning%2520paradigm%2520has%2520been%250Aproposed%2520to%2520mitigate%2520the%2520label%2520cost%2520issue.%2520Subsequently%252C%2520the%2520gap%2520between%2520the%250Apretext%2520tasks%2520and%2520downstream%2520tasks%2520has%2520spurred%2520the%2520development%2520of%2520graph%2520prompt%250Alearning%2520which%2520inserts%2520a%2520set%2520of%2520graph%2520prompts%2520into%2520the%2520original%2520graph%2520data%2520with%250Aminimal%2520parameters%2520while%2520preserving%2520competitive%2520performance.%2520However%252C%2520the%250Acurrent%2520exploratory%2520works%2520are%2520still%2520limited%2520since%2520they%2520all%2520concentrate%2520on%250Alearning%2520fixed%2520task-specific%2520prompts%2520which%2520may%2520not%2520generalize%2520well%2520across%2520the%250Adiverse%2520instances%2520that%2520the%2520task%2520comprises.%2520To%2520tackle%2520this%2520challenge%252C%2520we%250Aintroduce%2520Instance-Aware%2520Graph%2520Prompt%2520Learning%2520%2528IA-GPL%2529%2520in%2520this%2520paper%252C%2520aiming%250Ato%2520generate%2520distinct%2520prompts%2520tailored%2520to%2520different%2520input%2520instances.%2520The%2520process%250Ainvolves%2520generating%2520intermediate%2520prompts%2520for%2520each%2520instance%2520using%2520a%2520lightweight%250Aarchitecture%252C%2520quantizing%2520these%2520prompts%2520through%2520trainable%2520codebook%2520vectors%252C%2520and%250Aemploying%2520the%2520exponential%2520moving%2520average%2520technique%2520to%2520ensure%2520stable%2520training.%250AExtensive%2520experiments%2520conducted%2520on%2520multiple%2520datasets%2520and%2520settings%2520showcase%2520the%250Asuperior%2520performance%2520of%2520IA-GPL%2520compared%2520to%2520state-of-the-art%2520baselines.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2411.17676v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Instance-Aware%20Graph%20Prompt%20Learning&entry.906535625=Jiazheng%20Li%20and%20Jundong%20Li%20and%20Chuxu%20Zhang&entry.1292438233=%20%20Graph%20neural%20networks%20stand%20as%20the%20predominant%20technique%20for%20graph%0Arepresentation%20learning%20owing%20to%20their%20strong%20expressive%20power%2C%20yet%20the%0Aperformance%20highly%20depends%20on%20the%20availability%20of%20high-quality%20labels%20in%20an%0Aend-to-end%20manner.%20Thus%20the%20pretraining%20and%20fine-tuning%20paradigm%20has%20been%0Aproposed%20to%20mitigate%20the%20label%20cost%20issue.%20Subsequently%2C%20the%20gap%20between%20the%0Apretext%20tasks%20and%20downstream%20tasks%20has%20spurred%20the%20development%20of%20graph%20prompt%0Alearning%20which%20inserts%20a%20set%20of%20graph%20prompts%20into%20the%20original%20graph%20data%20with%0Aminimal%20parameters%20while%20preserving%20competitive%20performance.%20However%2C%20the%0Acurrent%20exploratory%20works%20are%20still%20limited%20since%20they%20all%20concentrate%20on%0Alearning%20fixed%20task-specific%20prompts%20which%20may%20not%20generalize%20well%20across%20the%0Adiverse%20instances%20that%20the%20task%20comprises.%20To%20tackle%20this%20challenge%2C%20we%0Aintroduce%20Instance-Aware%20Graph%20Prompt%20Learning%20%28IA-GPL%29%20in%20this%20paper%2C%20aiming%0Ato%20generate%20distinct%20prompts%20tailored%20to%20different%20input%20instances.%20The%20process%0Ainvolves%20generating%20intermediate%20prompts%20for%20each%20instance%20using%20a%20lightweight%0Aarchitecture%2C%20quantizing%20these%20prompts%20through%20trainable%20codebook%20vectors%2C%20and%0Aemploying%20the%20exponential%20moving%20average%20technique%20to%20ensure%20stable%20training.%0AExtensive%20experiments%20conducted%20on%20multiple%20datasets%20and%20settings%20showcase%20the%0Asuperior%20performance%20of%20IA-GPL%20compared%20to%20state-of-the-art%20baselines.%0A&entry.1838667208=http%3A//arxiv.org/abs/2411.17676v1&entry.124074799=Read"},
{"title": "Reward Incremental Learning in Text-to-Image Generation", "author": "Maorong Wang and Jiafeng Mao and Xueting Wang and Toshihiko Yamasaki", "abstract": "  The recent success of denoising diffusion models has significantly advanced\ntext-to-image generation. While these large-scale pretrained models show\nexcellent performance in general image synthesis, downstream objectives often\nrequire fine-tuning to meet specific criteria such as aesthetics or human\npreference. Reward gradient-based strategies are promising in this context, yet\nexisting methods are limited to single-reward tasks, restricting their\napplicability in real-world scenarios that demand adapting to multiple\nobjectives introduced incrementally over time. In this paper, we first define\nthis more realistic and unexplored problem, termed Reward Incremental Learning\n(RIL), where models are desired to adapt to multiple downstream objectives\nincrementally. Additionally, while the models adapt to the ever-emerging new\nobjectives, we observe a unique form of catastrophic forgetting in diffusion\nmodel fine-tuning, affecting both metric-wise and visual structure-wise image\nquality. To address this catastrophic forgetting challenge, we propose Reward\nIncremental Distillation (RID), a method that mitigates forgetting with minimal\ncomputational overhead, enabling stable performance across sequential reward\ntasks. The experimental results demonstrate the efficacy of RID in achieving\nconsistent, high-quality generation in RIL scenarios. The source code of our\nwork will be publicly available upon acceptance.\n", "link": "http://arxiv.org/abs/2411.17310v1", "date": "2024-11-26", "relevancy": 2.5009, "topK": [{"title": "Scaling Rectified Flow Transformers for High-Resolution Image Synthesis", "link": "http://arxiv.org/abs/2403.03206v1", "similarity": 0.6618}, {"title": "ToDo: Token Downsampling for Efficient Generation of High-Resolution\n  Images", "link": "http://arxiv.org/abs/2402.13573v2", "similarity": 0.6334}, {"title": "Tuning-Free Noise Rectification for High Fidelity Image-to-Video\n  Generation", "link": "http://arxiv.org/abs/2403.02827v1", "similarity": 0.6024}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Reward%20Incremental%20Learning%20in%20Text-to-Image%20Generation&body=Title%3A%20Reward%20Incremental%20Learning%20in%20Text-to-Image%20Generation%0AAuthor%3A%20Maorong%20Wang%20and%20Jiafeng%20Mao%20and%20Xueting%20Wang%20and%20Toshihiko%20Yamasaki%0AAbstract%3A%20%20%20The%20recent%20success%20of%20denoising%20diffusion%20models%20has%20significantly%20advanced%0Atext-to-image%20generation.%20While%20these%20large-scale%20pretrained%20models%20show%0Aexcellent%20performance%20in%20general%20image%20synthesis%2C%20downstream%20objectives%20often%0Arequire%20fine-tuning%20to%20meet%20specific%20criteria%20such%20as%20aesthetics%20or%20human%0Apreference.%20Reward%20gradient-based%20strategies%20are%20promising%20in%20this%20context%2C%20yet%0Aexisting%20methods%20are%20limited%20to%20single-reward%20tasks%2C%20restricting%20their%0Aapplicability%20in%20real-world%20scenarios%20that%20demand%20adapting%20to%20multiple%0Aobjectives%20introduced%20incrementally%20over%20time.%20In%20this%20paper%2C%20we%20first%20define%0Athis%20more%20realistic%20and%20unexplored%20problem%2C%20termed%20Reward%20Incremental%20Learning%0A%28RIL%29%2C%20where%20models%20are%20desired%20to%20adapt%20to%20multiple%20downstream%20objectives%0Aincrementally.%20Additionally%2C%20while%20the%20models%20adapt%20to%20the%20ever-emerging%20new%0Aobjectives%2C%20we%20observe%20a%20unique%20form%20of%20catastrophic%20forgetting%20in%20diffusion%0Amodel%20fine-tuning%2C%20affecting%20both%20metric-wise%20and%20visual%20structure-wise%20image%0Aquality.%20To%20address%20this%20catastrophic%20forgetting%20challenge%2C%20we%20propose%20Reward%0AIncremental%20Distillation%20%28RID%29%2C%20a%20method%20that%20mitigates%20forgetting%20with%20minimal%0Acomputational%20overhead%2C%20enabling%20stable%20performance%20across%20sequential%20reward%0Atasks.%20The%20experimental%20results%20demonstrate%20the%20efficacy%20of%20RID%20in%20achieving%0Aconsistent%2C%20high-quality%20generation%20in%20RIL%20scenarios.%20The%20source%20code%20of%20our%0Awork%20will%20be%20publicly%20available%20upon%20acceptance.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2411.17310v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DReward%2520Incremental%2520Learning%2520in%2520Text-to-Image%2520Generation%26entry.906535625%3DMaorong%2520Wang%2520and%2520Jiafeng%2520Mao%2520and%2520Xueting%2520Wang%2520and%2520Toshihiko%2520Yamasaki%26entry.1292438233%3D%2520%2520The%2520recent%2520success%2520of%2520denoising%2520diffusion%2520models%2520has%2520significantly%2520advanced%250Atext-to-image%2520generation.%2520While%2520these%2520large-scale%2520pretrained%2520models%2520show%250Aexcellent%2520performance%2520in%2520general%2520image%2520synthesis%252C%2520downstream%2520objectives%2520often%250Arequire%2520fine-tuning%2520to%2520meet%2520specific%2520criteria%2520such%2520as%2520aesthetics%2520or%2520human%250Apreference.%2520Reward%2520gradient-based%2520strategies%2520are%2520promising%2520in%2520this%2520context%252C%2520yet%250Aexisting%2520methods%2520are%2520limited%2520to%2520single-reward%2520tasks%252C%2520restricting%2520their%250Aapplicability%2520in%2520real-world%2520scenarios%2520that%2520demand%2520adapting%2520to%2520multiple%250Aobjectives%2520introduced%2520incrementally%2520over%2520time.%2520In%2520this%2520paper%252C%2520we%2520first%2520define%250Athis%2520more%2520realistic%2520and%2520unexplored%2520problem%252C%2520termed%2520Reward%2520Incremental%2520Learning%250A%2528RIL%2529%252C%2520where%2520models%2520are%2520desired%2520to%2520adapt%2520to%2520multiple%2520downstream%2520objectives%250Aincrementally.%2520Additionally%252C%2520while%2520the%2520models%2520adapt%2520to%2520the%2520ever-emerging%2520new%250Aobjectives%252C%2520we%2520observe%2520a%2520unique%2520form%2520of%2520catastrophic%2520forgetting%2520in%2520diffusion%250Amodel%2520fine-tuning%252C%2520affecting%2520both%2520metric-wise%2520and%2520visual%2520structure-wise%2520image%250Aquality.%2520To%2520address%2520this%2520catastrophic%2520forgetting%2520challenge%252C%2520we%2520propose%2520Reward%250AIncremental%2520Distillation%2520%2528RID%2529%252C%2520a%2520method%2520that%2520mitigates%2520forgetting%2520with%2520minimal%250Acomputational%2520overhead%252C%2520enabling%2520stable%2520performance%2520across%2520sequential%2520reward%250Atasks.%2520The%2520experimental%2520results%2520demonstrate%2520the%2520efficacy%2520of%2520RID%2520in%2520achieving%250Aconsistent%252C%2520high-quality%2520generation%2520in%2520RIL%2520scenarios.%2520The%2520source%2520code%2520of%2520our%250Awork%2520will%2520be%2520publicly%2520available%2520upon%2520acceptance.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2411.17310v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Reward%20Incremental%20Learning%20in%20Text-to-Image%20Generation&entry.906535625=Maorong%20Wang%20and%20Jiafeng%20Mao%20and%20Xueting%20Wang%20and%20Toshihiko%20Yamasaki&entry.1292438233=%20%20The%20recent%20success%20of%20denoising%20diffusion%20models%20has%20significantly%20advanced%0Atext-to-image%20generation.%20While%20these%20large-scale%20pretrained%20models%20show%0Aexcellent%20performance%20in%20general%20image%20synthesis%2C%20downstream%20objectives%20often%0Arequire%20fine-tuning%20to%20meet%20specific%20criteria%20such%20as%20aesthetics%20or%20human%0Apreference.%20Reward%20gradient-based%20strategies%20are%20promising%20in%20this%20context%2C%20yet%0Aexisting%20methods%20are%20limited%20to%20single-reward%20tasks%2C%20restricting%20their%0Aapplicability%20in%20real-world%20scenarios%20that%20demand%20adapting%20to%20multiple%0Aobjectives%20introduced%20incrementally%20over%20time.%20In%20this%20paper%2C%20we%20first%20define%0Athis%20more%20realistic%20and%20unexplored%20problem%2C%20termed%20Reward%20Incremental%20Learning%0A%28RIL%29%2C%20where%20models%20are%20desired%20to%20adapt%20to%20multiple%20downstream%20objectives%0Aincrementally.%20Additionally%2C%20while%20the%20models%20adapt%20to%20the%20ever-emerging%20new%0Aobjectives%2C%20we%20observe%20a%20unique%20form%20of%20catastrophic%20forgetting%20in%20diffusion%0Amodel%20fine-tuning%2C%20affecting%20both%20metric-wise%20and%20visual%20structure-wise%20image%0Aquality.%20To%20address%20this%20catastrophic%20forgetting%20challenge%2C%20we%20propose%20Reward%0AIncremental%20Distillation%20%28RID%29%2C%20a%20method%20that%20mitigates%20forgetting%20with%20minimal%0Acomputational%20overhead%2C%20enabling%20stable%20performance%20across%20sequential%20reward%0Atasks.%20The%20experimental%20results%20demonstrate%20the%20efficacy%20of%20RID%20in%20achieving%0Aconsistent%2C%20high-quality%20generation%20in%20RIL%20scenarios.%20The%20source%20code%20of%20our%0Awork%20will%20be%20publicly%20available%20upon%20acceptance.%0A&entry.1838667208=http%3A//arxiv.org/abs/2411.17310v1&entry.124074799=Read"},
{"title": "Orientation-Aware Graph Neural Networks for Protein Structure\n  Representation Learning", "author": "Jiahan Li and Shitong Luo and Congyue Deng and Chaoran Cheng and Jiaqi Guan and Leonidas Guibas and Jian Peng and Jianzhu Ma", "abstract": "  By folding to particular 3D structures, proteins play a key role in living\nbeings. To learn meaningful representation from a protein structure for\ndownstream tasks, not only the global backbone topology but the local\nfine-grained orientational relations between amino acids should also be\nconsidered. In this work, we propose the Orientation-Aware Graph Neural\nNetworks (OAGNNs) to better sense the geometric characteristics in protein\nstructure (e.g. inner-residue torsion angles, inter-residue orientations).\nExtending a single weight from a scalar to a 3D vector, we construct a rich set\nof geometric-meaningful operations to process both the classical and SO(3)\nrepresentations of a given structure. To plug our designed perceptron unit into\nexisting Graph Neural Networks, we further introduce an equivariant message\npassing paradigm, showing superior versatility in maintaining\nSO(3)-equivariance at the global scale. Experiments have shown that our OAGNNs\nhave a remarkable ability to sense geometric orientational features compared to\nclassical networks. OAGNNs have also achieved state-of-the-art performance on\nvarious computational biology applications related to protein 3D structures.\n", "link": "http://arxiv.org/abs/2201.13299v5", "date": "2024-11-26", "relevancy": 2.4755, "topK": [{"title": "Self-supervised Photographic Image Layout Representation Learning", "link": "http://arxiv.org/abs/2403.03740v1", "similarity": 0.515}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.4893}, {"title": "Dynamic Gaussians Mesh: Consistent Mesh Reconstruction from Monocular\n  Videos", "link": "http://arxiv.org/abs/2404.12379v2", "similarity": 0.481}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Orientation-Aware%20Graph%20Neural%20Networks%20for%20Protein%20Structure%0A%20%20Representation%20Learning&body=Title%3A%20Orientation-Aware%20Graph%20Neural%20Networks%20for%20Protein%20Structure%0A%20%20Representation%20Learning%0AAuthor%3A%20Jiahan%20Li%20and%20Shitong%20Luo%20and%20Congyue%20Deng%20and%20Chaoran%20Cheng%20and%20Jiaqi%20Guan%20and%20Leonidas%20Guibas%20and%20Jian%20Peng%20and%20Jianzhu%20Ma%0AAbstract%3A%20%20%20By%20folding%20to%20particular%203D%20structures%2C%20proteins%20play%20a%20key%20role%20in%20living%0Abeings.%20To%20learn%20meaningful%20representation%20from%20a%20protein%20structure%20for%0Adownstream%20tasks%2C%20not%20only%20the%20global%20backbone%20topology%20but%20the%20local%0Afine-grained%20orientational%20relations%20between%20amino%20acids%20should%20also%20be%0Aconsidered.%20In%20this%20work%2C%20we%20propose%20the%20Orientation-Aware%20Graph%20Neural%0ANetworks%20%28OAGNNs%29%20to%20better%20sense%20the%20geometric%20characteristics%20in%20protein%0Astructure%20%28e.g.%20inner-residue%20torsion%20angles%2C%20inter-residue%20orientations%29.%0AExtending%20a%20single%20weight%20from%20a%20scalar%20to%20a%203D%20vector%2C%20we%20construct%20a%20rich%20set%0Aof%20geometric-meaningful%20operations%20to%20process%20both%20the%20classical%20and%20SO%283%29%0Arepresentations%20of%20a%20given%20structure.%20To%20plug%20our%20designed%20perceptron%20unit%20into%0Aexisting%20Graph%20Neural%20Networks%2C%20we%20further%20introduce%20an%20equivariant%20message%0Apassing%20paradigm%2C%20showing%20superior%20versatility%20in%20maintaining%0ASO%283%29-equivariance%20at%20the%20global%20scale.%20Experiments%20have%20shown%20that%20our%20OAGNNs%0Ahave%20a%20remarkable%20ability%20to%20sense%20geometric%20orientational%20features%20compared%20to%0Aclassical%20networks.%20OAGNNs%20have%20also%20achieved%20state-of-the-art%20performance%20on%0Avarious%20computational%20biology%20applications%20related%20to%20protein%203D%20structures.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2201.13299v5%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DOrientation-Aware%2520Graph%2520Neural%2520Networks%2520for%2520Protein%2520Structure%250A%2520%2520Representation%2520Learning%26entry.906535625%3DJiahan%2520Li%2520and%2520Shitong%2520Luo%2520and%2520Congyue%2520Deng%2520and%2520Chaoran%2520Cheng%2520and%2520Jiaqi%2520Guan%2520and%2520Leonidas%2520Guibas%2520and%2520Jian%2520Peng%2520and%2520Jianzhu%2520Ma%26entry.1292438233%3D%2520%2520By%2520folding%2520to%2520particular%25203D%2520structures%252C%2520proteins%2520play%2520a%2520key%2520role%2520in%2520living%250Abeings.%2520To%2520learn%2520meaningful%2520representation%2520from%2520a%2520protein%2520structure%2520for%250Adownstream%2520tasks%252C%2520not%2520only%2520the%2520global%2520backbone%2520topology%2520but%2520the%2520local%250Afine-grained%2520orientational%2520relations%2520between%2520amino%2520acids%2520should%2520also%2520be%250Aconsidered.%2520In%2520this%2520work%252C%2520we%2520propose%2520the%2520Orientation-Aware%2520Graph%2520Neural%250ANetworks%2520%2528OAGNNs%2529%2520to%2520better%2520sense%2520the%2520geometric%2520characteristics%2520in%2520protein%250Astructure%2520%2528e.g.%2520inner-residue%2520torsion%2520angles%252C%2520inter-residue%2520orientations%2529.%250AExtending%2520a%2520single%2520weight%2520from%2520a%2520scalar%2520to%2520a%25203D%2520vector%252C%2520we%2520construct%2520a%2520rich%2520set%250Aof%2520geometric-meaningful%2520operations%2520to%2520process%2520both%2520the%2520classical%2520and%2520SO%25283%2529%250Arepresentations%2520of%2520a%2520given%2520structure.%2520To%2520plug%2520our%2520designed%2520perceptron%2520unit%2520into%250Aexisting%2520Graph%2520Neural%2520Networks%252C%2520we%2520further%2520introduce%2520an%2520equivariant%2520message%250Apassing%2520paradigm%252C%2520showing%2520superior%2520versatility%2520in%2520maintaining%250ASO%25283%2529-equivariance%2520at%2520the%2520global%2520scale.%2520Experiments%2520have%2520shown%2520that%2520our%2520OAGNNs%250Ahave%2520a%2520remarkable%2520ability%2520to%2520sense%2520geometric%2520orientational%2520features%2520compared%2520to%250Aclassical%2520networks.%2520OAGNNs%2520have%2520also%2520achieved%2520state-of-the-art%2520performance%2520on%250Avarious%2520computational%2520biology%2520applications%2520related%2520to%2520protein%25203D%2520structures.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2201.13299v5%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Orientation-Aware%20Graph%20Neural%20Networks%20for%20Protein%20Structure%0A%20%20Representation%20Learning&entry.906535625=Jiahan%20Li%20and%20Shitong%20Luo%20and%20Congyue%20Deng%20and%20Chaoran%20Cheng%20and%20Jiaqi%20Guan%20and%20Leonidas%20Guibas%20and%20Jian%20Peng%20and%20Jianzhu%20Ma&entry.1292438233=%20%20By%20folding%20to%20particular%203D%20structures%2C%20proteins%20play%20a%20key%20role%20in%20living%0Abeings.%20To%20learn%20meaningful%20representation%20from%20a%20protein%20structure%20for%0Adownstream%20tasks%2C%20not%20only%20the%20global%20backbone%20topology%20but%20the%20local%0Afine-grained%20orientational%20relations%20between%20amino%20acids%20should%20also%20be%0Aconsidered.%20In%20this%20work%2C%20we%20propose%20the%20Orientation-Aware%20Graph%20Neural%0ANetworks%20%28OAGNNs%29%20to%20better%20sense%20the%20geometric%20characteristics%20in%20protein%0Astructure%20%28e.g.%20inner-residue%20torsion%20angles%2C%20inter-residue%20orientations%29.%0AExtending%20a%20single%20weight%20from%20a%20scalar%20to%20a%203D%20vector%2C%20we%20construct%20a%20rich%20set%0Aof%20geometric-meaningful%20operations%20to%20process%20both%20the%20classical%20and%20SO%283%29%0Arepresentations%20of%20a%20given%20structure.%20To%20plug%20our%20designed%20perceptron%20unit%20into%0Aexisting%20Graph%20Neural%20Networks%2C%20we%20further%20introduce%20an%20equivariant%20message%0Apassing%20paradigm%2C%20showing%20superior%20versatility%20in%20maintaining%0ASO%283%29-equivariance%20at%20the%20global%20scale.%20Experiments%20have%20shown%20that%20our%20OAGNNs%0Ahave%20a%20remarkable%20ability%20to%20sense%20geometric%20orientational%20features%20compared%20to%0Aclassical%20networks.%20OAGNNs%20have%20also%20achieved%20state-of-the-art%20performance%20on%0Avarious%20computational%20biology%20applications%20related%20to%20protein%203D%20structures.%0A&entry.1838667208=http%3A//arxiv.org/abs/2201.13299v5&entry.124074799=Read"},
{"title": "Designing the virtual CAT: A digital tool for algorithmic thinking\n  assessment in compulsory education", "author": "Giorgia Adorni and Alberto Piatti", "abstract": "  Algorithmic thinking (AT) is a critical skill in today's digital society, and\nit is indispensable not only in computer science-related fields but also in\neveryday problem-solving. As a foundational component of digital education and\nliteracy, fostering AT skills is increasingly relevant for all students and\nshould become a standard part of compulsory education. However, successfully\nintegrating AT into formal education requires effective teaching strategies and\nrobust and scalable assessment procedures. In this paper, we present the design\nand development process of the virtual Cross Array Task (CAT), a digital\nadaptation of an unplugged assessment activity aimed at evaluating algorithmic\nskills in Swiss compulsory education. The development process followed\niterative design cycles, incorporating expert evaluations to refine the tool's\nusability, accessibility and functionality. A participatory design study played\na dual role in shaping the platform. First, it gathered valuable insights from\nend users, including students and teachers, to ensure the tool's relevance and\npracticality in classroom settings. Second, it facilitated the collection and\npreliminary analysis of data related to students' AT skills, providing an\ninitial evaluation of the tool's assessment capabilities across various\ndevelopmental stages. This was achieved through a pilot study involving a\ndiverse group of students aged 4 to 12, spanning preschool to lower secondary\nschool levels. The resulting instrument features multilingual support and\nincludes both gesture-based and visual block-based programming interfaces,\nmaking it accessible to a broad range of learners. Findings from the pilot\nstudy demonstrate the platform's usability and accessibility, as well as its\nsuitability for assessing AT skills, with preliminary results showing its\nability to cater to diverse age groups and educational contexts.\n", "link": "http://arxiv.org/abs/2408.01263v3", "date": "2024-11-26", "relevancy": 2.4745, "topK": [{"title": "CAT3D: Create Anything in 3D with Multi-View Diffusion Models", "link": "http://arxiv.org/abs/2405.10314v1", "similarity": 0.4972}, {"title": "CAT3D: Create Anything in 3D with Multi-View Diffusion Models", "link": "http://arxiv.org/abs/2405.10314v1", "similarity": 0.4972}, {"title": "DressCode: Autoregressively Sewing and Generating Garments from Text\n  Guidance", "link": "http://arxiv.org/abs/2401.16465v3", "similarity": 0.4903}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Designing%20the%20virtual%20CAT%3A%20A%20digital%20tool%20for%20algorithmic%20thinking%0A%20%20assessment%20in%20compulsory%20education&body=Title%3A%20Designing%20the%20virtual%20CAT%3A%20A%20digital%20tool%20for%20algorithmic%20thinking%0A%20%20assessment%20in%20compulsory%20education%0AAuthor%3A%20Giorgia%20Adorni%20and%20Alberto%20Piatti%0AAbstract%3A%20%20%20Algorithmic%20thinking%20%28AT%29%20is%20a%20critical%20skill%20in%20today%27s%20digital%20society%2C%20and%0Ait%20is%20indispensable%20not%20only%20in%20computer%20science-related%20fields%20but%20also%20in%0Aeveryday%20problem-solving.%20As%20a%20foundational%20component%20of%20digital%20education%20and%0Aliteracy%2C%20fostering%20AT%20skills%20is%20increasingly%20relevant%20for%20all%20students%20and%0Ashould%20become%20a%20standard%20part%20of%20compulsory%20education.%20However%2C%20successfully%0Aintegrating%20AT%20into%20formal%20education%20requires%20effective%20teaching%20strategies%20and%0Arobust%20and%20scalable%20assessment%20procedures.%20In%20this%20paper%2C%20we%20present%20the%20design%0Aand%20development%20process%20of%20the%20virtual%20Cross%20Array%20Task%20%28CAT%29%2C%20a%20digital%0Aadaptation%20of%20an%20unplugged%20assessment%20activity%20aimed%20at%20evaluating%20algorithmic%0Askills%20in%20Swiss%20compulsory%20education.%20The%20development%20process%20followed%0Aiterative%20design%20cycles%2C%20incorporating%20expert%20evaluations%20to%20refine%20the%20tool%27s%0Ausability%2C%20accessibility%20and%20functionality.%20A%20participatory%20design%20study%20played%0Aa%20dual%20role%20in%20shaping%20the%20platform.%20First%2C%20it%20gathered%20valuable%20insights%20from%0Aend%20users%2C%20including%20students%20and%20teachers%2C%20to%20ensure%20the%20tool%27s%20relevance%20and%0Apracticality%20in%20classroom%20settings.%20Second%2C%20it%20facilitated%20the%20collection%20and%0Apreliminary%20analysis%20of%20data%20related%20to%20students%27%20AT%20skills%2C%20providing%20an%0Ainitial%20evaluation%20of%20the%20tool%27s%20assessment%20capabilities%20across%20various%0Adevelopmental%20stages.%20This%20was%20achieved%20through%20a%20pilot%20study%20involving%20a%0Adiverse%20group%20of%20students%20aged%204%20to%2012%2C%20spanning%20preschool%20to%20lower%20secondary%0Aschool%20levels.%20The%20resulting%20instrument%20features%20multilingual%20support%20and%0Aincludes%20both%20gesture-based%20and%20visual%20block-based%20programming%20interfaces%2C%0Amaking%20it%20accessible%20to%20a%20broad%20range%20of%20learners.%20Findings%20from%20the%20pilot%0Astudy%20demonstrate%20the%20platform%27s%20usability%20and%20accessibility%2C%20as%20well%20as%20its%0Asuitability%20for%20assessing%20AT%20skills%2C%20with%20preliminary%20results%20showing%20its%0Aability%20to%20cater%20to%20diverse%20age%20groups%20and%20educational%20contexts.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2408.01263v3%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DDesigning%2520the%2520virtual%2520CAT%253A%2520A%2520digital%2520tool%2520for%2520algorithmic%2520thinking%250A%2520%2520assessment%2520in%2520compulsory%2520education%26entry.906535625%3DGiorgia%2520Adorni%2520and%2520Alberto%2520Piatti%26entry.1292438233%3D%2520%2520Algorithmic%2520thinking%2520%2528AT%2529%2520is%2520a%2520critical%2520skill%2520in%2520today%2527s%2520digital%2520society%252C%2520and%250Ait%2520is%2520indispensable%2520not%2520only%2520in%2520computer%2520science-related%2520fields%2520but%2520also%2520in%250Aeveryday%2520problem-solving.%2520As%2520a%2520foundational%2520component%2520of%2520digital%2520education%2520and%250Aliteracy%252C%2520fostering%2520AT%2520skills%2520is%2520increasingly%2520relevant%2520for%2520all%2520students%2520and%250Ashould%2520become%2520a%2520standard%2520part%2520of%2520compulsory%2520education.%2520However%252C%2520successfully%250Aintegrating%2520AT%2520into%2520formal%2520education%2520requires%2520effective%2520teaching%2520strategies%2520and%250Arobust%2520and%2520scalable%2520assessment%2520procedures.%2520In%2520this%2520paper%252C%2520we%2520present%2520the%2520design%250Aand%2520development%2520process%2520of%2520the%2520virtual%2520Cross%2520Array%2520Task%2520%2528CAT%2529%252C%2520a%2520digital%250Aadaptation%2520of%2520an%2520unplugged%2520assessment%2520activity%2520aimed%2520at%2520evaluating%2520algorithmic%250Askills%2520in%2520Swiss%2520compulsory%2520education.%2520The%2520development%2520process%2520followed%250Aiterative%2520design%2520cycles%252C%2520incorporating%2520expert%2520evaluations%2520to%2520refine%2520the%2520tool%2527s%250Ausability%252C%2520accessibility%2520and%2520functionality.%2520A%2520participatory%2520design%2520study%2520played%250Aa%2520dual%2520role%2520in%2520shaping%2520the%2520platform.%2520First%252C%2520it%2520gathered%2520valuable%2520insights%2520from%250Aend%2520users%252C%2520including%2520students%2520and%2520teachers%252C%2520to%2520ensure%2520the%2520tool%2527s%2520relevance%2520and%250Apracticality%2520in%2520classroom%2520settings.%2520Second%252C%2520it%2520facilitated%2520the%2520collection%2520and%250Apreliminary%2520analysis%2520of%2520data%2520related%2520to%2520students%2527%2520AT%2520skills%252C%2520providing%2520an%250Ainitial%2520evaluation%2520of%2520the%2520tool%2527s%2520assessment%2520capabilities%2520across%2520various%250Adevelopmental%2520stages.%2520This%2520was%2520achieved%2520through%2520a%2520pilot%2520study%2520involving%2520a%250Adiverse%2520group%2520of%2520students%2520aged%25204%2520to%252012%252C%2520spanning%2520preschool%2520to%2520lower%2520secondary%250Aschool%2520levels.%2520The%2520resulting%2520instrument%2520features%2520multilingual%2520support%2520and%250Aincludes%2520both%2520gesture-based%2520and%2520visual%2520block-based%2520programming%2520interfaces%252C%250Amaking%2520it%2520accessible%2520to%2520a%2520broad%2520range%2520of%2520learners.%2520Findings%2520from%2520the%2520pilot%250Astudy%2520demonstrate%2520the%2520platform%2527s%2520usability%2520and%2520accessibility%252C%2520as%2520well%2520as%2520its%250Asuitability%2520for%2520assessing%2520AT%2520skills%252C%2520with%2520preliminary%2520results%2520showing%2520its%250Aability%2520to%2520cater%2520to%2520diverse%2520age%2520groups%2520and%2520educational%2520contexts.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2408.01263v3%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Designing%20the%20virtual%20CAT%3A%20A%20digital%20tool%20for%20algorithmic%20thinking%0A%20%20assessment%20in%20compulsory%20education&entry.906535625=Giorgia%20Adorni%20and%20Alberto%20Piatti&entry.1292438233=%20%20Algorithmic%20thinking%20%28AT%29%20is%20a%20critical%20skill%20in%20today%27s%20digital%20society%2C%20and%0Ait%20is%20indispensable%20not%20only%20in%20computer%20science-related%20fields%20but%20also%20in%0Aeveryday%20problem-solving.%20As%20a%20foundational%20component%20of%20digital%20education%20and%0Aliteracy%2C%20fostering%20AT%20skills%20is%20increasingly%20relevant%20for%20all%20students%20and%0Ashould%20become%20a%20standard%20part%20of%20compulsory%20education.%20However%2C%20successfully%0Aintegrating%20AT%20into%20formal%20education%20requires%20effective%20teaching%20strategies%20and%0Arobust%20and%20scalable%20assessment%20procedures.%20In%20this%20paper%2C%20we%20present%20the%20design%0Aand%20development%20process%20of%20the%20virtual%20Cross%20Array%20Task%20%28CAT%29%2C%20a%20digital%0Aadaptation%20of%20an%20unplugged%20assessment%20activity%20aimed%20at%20evaluating%20algorithmic%0Askills%20in%20Swiss%20compulsory%20education.%20The%20development%20process%20followed%0Aiterative%20design%20cycles%2C%20incorporating%20expert%20evaluations%20to%20refine%20the%20tool%27s%0Ausability%2C%20accessibility%20and%20functionality.%20A%20participatory%20design%20study%20played%0Aa%20dual%20role%20in%20shaping%20the%20platform.%20First%2C%20it%20gathered%20valuable%20insights%20from%0Aend%20users%2C%20including%20students%20and%20teachers%2C%20to%20ensure%20the%20tool%27s%20relevance%20and%0Apracticality%20in%20classroom%20settings.%20Second%2C%20it%20facilitated%20the%20collection%20and%0Apreliminary%20analysis%20of%20data%20related%20to%20students%27%20AT%20skills%2C%20providing%20an%0Ainitial%20evaluation%20of%20the%20tool%27s%20assessment%20capabilities%20across%20various%0Adevelopmental%20stages.%20This%20was%20achieved%20through%20a%20pilot%20study%20involving%20a%0Adiverse%20group%20of%20students%20aged%204%20to%2012%2C%20spanning%20preschool%20to%20lower%20secondary%0Aschool%20levels.%20The%20resulting%20instrument%20features%20multilingual%20support%20and%0Aincludes%20both%20gesture-based%20and%20visual%20block-based%20programming%20interfaces%2C%0Amaking%20it%20accessible%20to%20a%20broad%20range%20of%20learners.%20Findings%20from%20the%20pilot%0Astudy%20demonstrate%20the%20platform%27s%20usability%20and%20accessibility%2C%20as%20well%20as%20its%0Asuitability%20for%20assessing%20AT%20skills%2C%20with%20preliminary%20results%20showing%20its%0Aability%20to%20cater%20to%20diverse%20age%20groups%20and%20educational%20contexts.%0A&entry.1838667208=http%3A//arxiv.org/abs/2408.01263v3&entry.124074799=Read"},
{"title": "Making History Readable", "author": "Bipasha Banerjee and Jennifer Goyne and William A. Ingram", "abstract": "  The Virginia Tech University Libraries (VTUL) Digital Library Platform (DLP)\nhosts digital collections that offer our users access to a wide variety of\ndocuments of historical and cultural importance. These collections are not only\nof academic importance but also provide our users with a glance at local\nhistorical events. Our DLP contains collections comprising digital objects\nfeaturing complex layouts, faded imagery, and hard-to-read handwritten text,\nwhich makes providing online access to these materials challenging. To address\nthese issues, we integrate AI into our DLP workflow and convert the text in the\ndigital objects into a machine-readable format. To enhance the user experience\nwith our historical collections, we use custom AI agents for handwriting\nrecognition, text extraction, and large language models (LLMs) for\nsummarization. This poster highlights three collections focusing on handwritten\nletters, newspapers, and digitized topographic maps. We discuss the challenges\nwith each collection and detail our approaches to address them. Our proposed\nmethods aim to enhance the user experience by making the contents in these\ncollections easier to search and navigate.\n", "link": "http://arxiv.org/abs/2411.17600v1", "date": "2024-11-26", "relevancy": 2.4677, "topK": [{"title": "DressCode: Autoregressively Sewing and Generating Garments from Text\n  Guidance", "link": "http://arxiv.org/abs/2401.16465v3", "similarity": 0.4948}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.4929}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.4929}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Making%20History%20Readable&body=Title%3A%20Making%20History%20Readable%0AAuthor%3A%20Bipasha%20Banerjee%20and%20Jennifer%20Goyne%20and%20William%20A.%20Ingram%0AAbstract%3A%20%20%20The%20Virginia%20Tech%20University%20Libraries%20%28VTUL%29%20Digital%20Library%20Platform%20%28DLP%29%0Ahosts%20digital%20collections%20that%20offer%20our%20users%20access%20to%20a%20wide%20variety%20of%0Adocuments%20of%20historical%20and%20cultural%20importance.%20These%20collections%20are%20not%20only%0Aof%20academic%20importance%20but%20also%20provide%20our%20users%20with%20a%20glance%20at%20local%0Ahistorical%20events.%20Our%20DLP%20contains%20collections%20comprising%20digital%20objects%0Afeaturing%20complex%20layouts%2C%20faded%20imagery%2C%20and%20hard-to-read%20handwritten%20text%2C%0Awhich%20makes%20providing%20online%20access%20to%20these%20materials%20challenging.%20To%20address%0Athese%20issues%2C%20we%20integrate%20AI%20into%20our%20DLP%20workflow%20and%20convert%20the%20text%20in%20the%0Adigital%20objects%20into%20a%20machine-readable%20format.%20To%20enhance%20the%20user%20experience%0Awith%20our%20historical%20collections%2C%20we%20use%20custom%20AI%20agents%20for%20handwriting%0Arecognition%2C%20text%20extraction%2C%20and%20large%20language%20models%20%28LLMs%29%20for%0Asummarization.%20This%20poster%20highlights%20three%20collections%20focusing%20on%20handwritten%0Aletters%2C%20newspapers%2C%20and%20digitized%20topographic%20maps.%20We%20discuss%20the%20challenges%0Awith%20each%20collection%20and%20detail%20our%20approaches%20to%20address%20them.%20Our%20proposed%0Amethods%20aim%20to%20enhance%20the%20user%20experience%20by%20making%20the%20contents%20in%20these%0Acollections%20easier%20to%20search%20and%20navigate.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2411.17600v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DMaking%2520History%2520Readable%26entry.906535625%3DBipasha%2520Banerjee%2520and%2520Jennifer%2520Goyne%2520and%2520William%2520A.%2520Ingram%26entry.1292438233%3D%2520%2520The%2520Virginia%2520Tech%2520University%2520Libraries%2520%2528VTUL%2529%2520Digital%2520Library%2520Platform%2520%2528DLP%2529%250Ahosts%2520digital%2520collections%2520that%2520offer%2520our%2520users%2520access%2520to%2520a%2520wide%2520variety%2520of%250Adocuments%2520of%2520historical%2520and%2520cultural%2520importance.%2520These%2520collections%2520are%2520not%2520only%250Aof%2520academic%2520importance%2520but%2520also%2520provide%2520our%2520users%2520with%2520a%2520glance%2520at%2520local%250Ahistorical%2520events.%2520Our%2520DLP%2520contains%2520collections%2520comprising%2520digital%2520objects%250Afeaturing%2520complex%2520layouts%252C%2520faded%2520imagery%252C%2520and%2520hard-to-read%2520handwritten%2520text%252C%250Awhich%2520makes%2520providing%2520online%2520access%2520to%2520these%2520materials%2520challenging.%2520To%2520address%250Athese%2520issues%252C%2520we%2520integrate%2520AI%2520into%2520our%2520DLP%2520workflow%2520and%2520convert%2520the%2520text%2520in%2520the%250Adigital%2520objects%2520into%2520a%2520machine-readable%2520format.%2520To%2520enhance%2520the%2520user%2520experience%250Awith%2520our%2520historical%2520collections%252C%2520we%2520use%2520custom%2520AI%2520agents%2520for%2520handwriting%250Arecognition%252C%2520text%2520extraction%252C%2520and%2520large%2520language%2520models%2520%2528LLMs%2529%2520for%250Asummarization.%2520This%2520poster%2520highlights%2520three%2520collections%2520focusing%2520on%2520handwritten%250Aletters%252C%2520newspapers%252C%2520and%2520digitized%2520topographic%2520maps.%2520We%2520discuss%2520the%2520challenges%250Awith%2520each%2520collection%2520and%2520detail%2520our%2520approaches%2520to%2520address%2520them.%2520Our%2520proposed%250Amethods%2520aim%2520to%2520enhance%2520the%2520user%2520experience%2520by%2520making%2520the%2520contents%2520in%2520these%250Acollections%2520easier%2520to%2520search%2520and%2520navigate.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2411.17600v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Making%20History%20Readable&entry.906535625=Bipasha%20Banerjee%20and%20Jennifer%20Goyne%20and%20William%20A.%20Ingram&entry.1292438233=%20%20The%20Virginia%20Tech%20University%20Libraries%20%28VTUL%29%20Digital%20Library%20Platform%20%28DLP%29%0Ahosts%20digital%20collections%20that%20offer%20our%20users%20access%20to%20a%20wide%20variety%20of%0Adocuments%20of%20historical%20and%20cultural%20importance.%20These%20collections%20are%20not%20only%0Aof%20academic%20importance%20but%20also%20provide%20our%20users%20with%20a%20glance%20at%20local%0Ahistorical%20events.%20Our%20DLP%20contains%20collections%20comprising%20digital%20objects%0Afeaturing%20complex%20layouts%2C%20faded%20imagery%2C%20and%20hard-to-read%20handwritten%20text%2C%0Awhich%20makes%20providing%20online%20access%20to%20these%20materials%20challenging.%20To%20address%0Athese%20issues%2C%20we%20integrate%20AI%20into%20our%20DLP%20workflow%20and%20convert%20the%20text%20in%20the%0Adigital%20objects%20into%20a%20machine-readable%20format.%20To%20enhance%20the%20user%20experience%0Awith%20our%20historical%20collections%2C%20we%20use%20custom%20AI%20agents%20for%20handwriting%0Arecognition%2C%20text%20extraction%2C%20and%20large%20language%20models%20%28LLMs%29%20for%0Asummarization.%20This%20poster%20highlights%20three%20collections%20focusing%20on%20handwritten%0Aletters%2C%20newspapers%2C%20and%20digitized%20topographic%20maps.%20We%20discuss%20the%20challenges%0Awith%20each%20collection%20and%20detail%20our%20approaches%20to%20address%20them.%20Our%20proposed%0Amethods%20aim%20to%20enhance%20the%20user%20experience%20by%20making%20the%20contents%20in%20these%0Acollections%20easier%20to%20search%20and%20navigate.%0A&entry.1838667208=http%3A//arxiv.org/abs/2411.17600v1&entry.124074799=Read"},
{"title": "DepthCues: Evaluating Monocular Depth Perception in Large Vision Models", "author": "Duolikun Danier and Mehmet Ayg\u00fcn and Changjian Li and Hakan Bilen and Oisin Mac Aodha", "abstract": "  Large-scale pre-trained vision models are becoming increasingly prevalent,\noffering expressive and generalizable visual representations that benefit\nvarious downstream tasks. Recent studies on the emergent properties of these\nmodels have revealed their high-level geometric understanding, in particular in\nthe context of depth perception. However, it remains unclear how depth\nperception arises in these models without explicit depth supervision provided\nduring pre-training. To investigate this, we examine whether the monocular\ndepth cues, similar to those used by the human visual system, emerge in these\nmodels. We introduce a new benchmark, DepthCues, designed to evaluate depth cue\nunderstanding, and present findings across 20 diverse and representative\npre-trained vision models. Our analysis shows that human-like depth cues emerge\nin more recent larger models. We also explore enhancing depth perception in\nlarge vision models by fine-tuning on DepthCues, and find that even without\ndense depth supervision, this improves depth estimation. To support further\nresearch, our benchmark and evaluation code will be made publicly available for\nstudying depth perception in vision models.\n", "link": "http://arxiv.org/abs/2411.17385v1", "date": "2024-11-26", "relevancy": 2.4598, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.6306}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.6306}, {"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.5366}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20DepthCues%3A%20Evaluating%20Monocular%20Depth%20Perception%20in%20Large%20Vision%20Models&body=Title%3A%20DepthCues%3A%20Evaluating%20Monocular%20Depth%20Perception%20in%20Large%20Vision%20Models%0AAuthor%3A%20Duolikun%20Danier%20and%20Mehmet%20Ayg%C3%BCn%20and%20Changjian%20Li%20and%20Hakan%20Bilen%20and%20Oisin%20Mac%20Aodha%0AAbstract%3A%20%20%20Large-scale%20pre-trained%20vision%20models%20are%20becoming%20increasingly%20prevalent%2C%0Aoffering%20expressive%20and%20generalizable%20visual%20representations%20that%20benefit%0Avarious%20downstream%20tasks.%20Recent%20studies%20on%20the%20emergent%20properties%20of%20these%0Amodels%20have%20revealed%20their%20high-level%20geometric%20understanding%2C%20in%20particular%20in%0Athe%20context%20of%20depth%20perception.%20However%2C%20it%20remains%20unclear%20how%20depth%0Aperception%20arises%20in%20these%20models%20without%20explicit%20depth%20supervision%20provided%0Aduring%20pre-training.%20To%20investigate%20this%2C%20we%20examine%20whether%20the%20monocular%0Adepth%20cues%2C%20similar%20to%20those%20used%20by%20the%20human%20visual%20system%2C%20emerge%20in%20these%0Amodels.%20We%20introduce%20a%20new%20benchmark%2C%20DepthCues%2C%20designed%20to%20evaluate%20depth%20cue%0Aunderstanding%2C%20and%20present%20findings%20across%2020%20diverse%20and%20representative%0Apre-trained%20vision%20models.%20Our%20analysis%20shows%20that%20human-like%20depth%20cues%20emerge%0Ain%20more%20recent%20larger%20models.%20We%20also%20explore%20enhancing%20depth%20perception%20in%0Alarge%20vision%20models%20by%20fine-tuning%20on%20DepthCues%2C%20and%20find%20that%20even%20without%0Adense%20depth%20supervision%2C%20this%20improves%20depth%20estimation.%20To%20support%20further%0Aresearch%2C%20our%20benchmark%20and%20evaluation%20code%20will%20be%20made%20publicly%20available%20for%0Astudying%20depth%20perception%20in%20vision%20models.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2411.17385v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DDepthCues%253A%2520Evaluating%2520Monocular%2520Depth%2520Perception%2520in%2520Large%2520Vision%2520Models%26entry.906535625%3DDuolikun%2520Danier%2520and%2520Mehmet%2520Ayg%25C3%25BCn%2520and%2520Changjian%2520Li%2520and%2520Hakan%2520Bilen%2520and%2520Oisin%2520Mac%2520Aodha%26entry.1292438233%3D%2520%2520Large-scale%2520pre-trained%2520vision%2520models%2520are%2520becoming%2520increasingly%2520prevalent%252C%250Aoffering%2520expressive%2520and%2520generalizable%2520visual%2520representations%2520that%2520benefit%250Avarious%2520downstream%2520tasks.%2520Recent%2520studies%2520on%2520the%2520emergent%2520properties%2520of%2520these%250Amodels%2520have%2520revealed%2520their%2520high-level%2520geometric%2520understanding%252C%2520in%2520particular%2520in%250Athe%2520context%2520of%2520depth%2520perception.%2520However%252C%2520it%2520remains%2520unclear%2520how%2520depth%250Aperception%2520arises%2520in%2520these%2520models%2520without%2520explicit%2520depth%2520supervision%2520provided%250Aduring%2520pre-training.%2520To%2520investigate%2520this%252C%2520we%2520examine%2520whether%2520the%2520monocular%250Adepth%2520cues%252C%2520similar%2520to%2520those%2520used%2520by%2520the%2520human%2520visual%2520system%252C%2520emerge%2520in%2520these%250Amodels.%2520We%2520introduce%2520a%2520new%2520benchmark%252C%2520DepthCues%252C%2520designed%2520to%2520evaluate%2520depth%2520cue%250Aunderstanding%252C%2520and%2520present%2520findings%2520across%252020%2520diverse%2520and%2520representative%250Apre-trained%2520vision%2520models.%2520Our%2520analysis%2520shows%2520that%2520human-like%2520depth%2520cues%2520emerge%250Ain%2520more%2520recent%2520larger%2520models.%2520We%2520also%2520explore%2520enhancing%2520depth%2520perception%2520in%250Alarge%2520vision%2520models%2520by%2520fine-tuning%2520on%2520DepthCues%252C%2520and%2520find%2520that%2520even%2520without%250Adense%2520depth%2520supervision%252C%2520this%2520improves%2520depth%2520estimation.%2520To%2520support%2520further%250Aresearch%252C%2520our%2520benchmark%2520and%2520evaluation%2520code%2520will%2520be%2520made%2520publicly%2520available%2520for%250Astudying%2520depth%2520perception%2520in%2520vision%2520models.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2411.17385v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=DepthCues%3A%20Evaluating%20Monocular%20Depth%20Perception%20in%20Large%20Vision%20Models&entry.906535625=Duolikun%20Danier%20and%20Mehmet%20Ayg%C3%BCn%20and%20Changjian%20Li%20and%20Hakan%20Bilen%20and%20Oisin%20Mac%20Aodha&entry.1292438233=%20%20Large-scale%20pre-trained%20vision%20models%20are%20becoming%20increasingly%20prevalent%2C%0Aoffering%20expressive%20and%20generalizable%20visual%20representations%20that%20benefit%0Avarious%20downstream%20tasks.%20Recent%20studies%20on%20the%20emergent%20properties%20of%20these%0Amodels%20have%20revealed%20their%20high-level%20geometric%20understanding%2C%20in%20particular%20in%0Athe%20context%20of%20depth%20perception.%20However%2C%20it%20remains%20unclear%20how%20depth%0Aperception%20arises%20in%20these%20models%20without%20explicit%20depth%20supervision%20provided%0Aduring%20pre-training.%20To%20investigate%20this%2C%20we%20examine%20whether%20the%20monocular%0Adepth%20cues%2C%20similar%20to%20those%20used%20by%20the%20human%20visual%20system%2C%20emerge%20in%20these%0Amodels.%20We%20introduce%20a%20new%20benchmark%2C%20DepthCues%2C%20designed%20to%20evaluate%20depth%20cue%0Aunderstanding%2C%20and%20present%20findings%20across%2020%20diverse%20and%20representative%0Apre-trained%20vision%20models.%20Our%20analysis%20shows%20that%20human-like%20depth%20cues%20emerge%0Ain%20more%20recent%20larger%20models.%20We%20also%20explore%20enhancing%20depth%20perception%20in%0Alarge%20vision%20models%20by%20fine-tuning%20on%20DepthCues%2C%20and%20find%20that%20even%20without%0Adense%20depth%20supervision%2C%20this%20improves%20depth%20estimation.%20To%20support%20further%0Aresearch%2C%20our%20benchmark%20and%20evaluation%20code%20will%20be%20made%20publicly%20available%20for%0Astudying%20depth%20perception%20in%20vision%20models.%0A&entry.1838667208=http%3A//arxiv.org/abs/2411.17385v1&entry.124074799=Read"},
{"title": "Spatially Visual Perception for End-to-End Robotic Learning", "author": "Travis Davies and Jiahuan Yan and Xiang Chen and Yu Tian and Yueting Zhuang and Yiqi Huang and Luhui Hu", "abstract": "  Recent advances in imitation learning have shown significant promise for\nrobotic control and embodied intelligence. However, achieving robust\ngeneralization across diverse mounted camera observations remains a critical\nchallenge. In this paper, we introduce a video-based spatial perception\nframework that leverages 3D spatial representations to address environmental\nvariability, with a focus on handling lighting changes. Our approach integrates\na novel image augmentation technique, AugBlender, with a state-of-the-art\nmonocular depth estimation model trained on internet-scale data. Together,\nthese components form a cohesive system designed to enhance robustness and\nadaptability in dynamic scenarios. Our results demonstrate that our approach\nsignificantly boosts the success rate across diverse camera exposures, where\nprevious models experience performance collapse. Our findings highlight the\npotential of video-based spatial perception models in advancing robustness for\nend-to-end robotic learning, paving the way for scalable, low-cost solutions in\nembodied intelligence.\n", "link": "http://arxiv.org/abs/2411.17458v1", "date": "2024-11-26", "relevancy": 2.4523, "topK": [{"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.6292}, {"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.6099}, {"title": "Egocentric Computer Vision for Hands-Free Robotic Wheelchair Navigation", "link": "https://link.springer.com/article/10.1007/s10846-023-01807-4", "similarity": 0.6098}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Spatially%20Visual%20Perception%20for%20End-to-End%20Robotic%20Learning&body=Title%3A%20Spatially%20Visual%20Perception%20for%20End-to-End%20Robotic%20Learning%0AAuthor%3A%20Travis%20Davies%20and%20Jiahuan%20Yan%20and%20Xiang%20Chen%20and%20Yu%20Tian%20and%20Yueting%20Zhuang%20and%20Yiqi%20Huang%20and%20Luhui%20Hu%0AAbstract%3A%20%20%20Recent%20advances%20in%20imitation%20learning%20have%20shown%20significant%20promise%20for%0Arobotic%20control%20and%20embodied%20intelligence.%20However%2C%20achieving%20robust%0Ageneralization%20across%20diverse%20mounted%20camera%20observations%20remains%20a%20critical%0Achallenge.%20In%20this%20paper%2C%20we%20introduce%20a%20video-based%20spatial%20perception%0Aframework%20that%20leverages%203D%20spatial%20representations%20to%20address%20environmental%0Avariability%2C%20with%20a%20focus%20on%20handling%20lighting%20changes.%20Our%20approach%20integrates%0Aa%20novel%20image%20augmentation%20technique%2C%20AugBlender%2C%20with%20a%20state-of-the-art%0Amonocular%20depth%20estimation%20model%20trained%20on%20internet-scale%20data.%20Together%2C%0Athese%20components%20form%20a%20cohesive%20system%20designed%20to%20enhance%20robustness%20and%0Aadaptability%20in%20dynamic%20scenarios.%20Our%20results%20demonstrate%20that%20our%20approach%0Asignificantly%20boosts%20the%20success%20rate%20across%20diverse%20camera%20exposures%2C%20where%0Aprevious%20models%20experience%20performance%20collapse.%20Our%20findings%20highlight%20the%0Apotential%20of%20video-based%20spatial%20perception%20models%20in%20advancing%20robustness%20for%0Aend-to-end%20robotic%20learning%2C%20paving%20the%20way%20for%20scalable%2C%20low-cost%20solutions%20in%0Aembodied%20intelligence.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2411.17458v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DSpatially%2520Visual%2520Perception%2520for%2520End-to-End%2520Robotic%2520Learning%26entry.906535625%3DTravis%2520Davies%2520and%2520Jiahuan%2520Yan%2520and%2520Xiang%2520Chen%2520and%2520Yu%2520Tian%2520and%2520Yueting%2520Zhuang%2520and%2520Yiqi%2520Huang%2520and%2520Luhui%2520Hu%26entry.1292438233%3D%2520%2520Recent%2520advances%2520in%2520imitation%2520learning%2520have%2520shown%2520significant%2520promise%2520for%250Arobotic%2520control%2520and%2520embodied%2520intelligence.%2520However%252C%2520achieving%2520robust%250Ageneralization%2520across%2520diverse%2520mounted%2520camera%2520observations%2520remains%2520a%2520critical%250Achallenge.%2520In%2520this%2520paper%252C%2520we%2520introduce%2520a%2520video-based%2520spatial%2520perception%250Aframework%2520that%2520leverages%25203D%2520spatial%2520representations%2520to%2520address%2520environmental%250Avariability%252C%2520with%2520a%2520focus%2520on%2520handling%2520lighting%2520changes.%2520Our%2520approach%2520integrates%250Aa%2520novel%2520image%2520augmentation%2520technique%252C%2520AugBlender%252C%2520with%2520a%2520state-of-the-art%250Amonocular%2520depth%2520estimation%2520model%2520trained%2520on%2520internet-scale%2520data.%2520Together%252C%250Athese%2520components%2520form%2520a%2520cohesive%2520system%2520designed%2520to%2520enhance%2520robustness%2520and%250Aadaptability%2520in%2520dynamic%2520scenarios.%2520Our%2520results%2520demonstrate%2520that%2520our%2520approach%250Asignificantly%2520boosts%2520the%2520success%2520rate%2520across%2520diverse%2520camera%2520exposures%252C%2520where%250Aprevious%2520models%2520experience%2520performance%2520collapse.%2520Our%2520findings%2520highlight%2520the%250Apotential%2520of%2520video-based%2520spatial%2520perception%2520models%2520in%2520advancing%2520robustness%2520for%250Aend-to-end%2520robotic%2520learning%252C%2520paving%2520the%2520way%2520for%2520scalable%252C%2520low-cost%2520solutions%2520in%250Aembodied%2520intelligence.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2411.17458v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Spatially%20Visual%20Perception%20for%20End-to-End%20Robotic%20Learning&entry.906535625=Travis%20Davies%20and%20Jiahuan%20Yan%20and%20Xiang%20Chen%20and%20Yu%20Tian%20and%20Yueting%20Zhuang%20and%20Yiqi%20Huang%20and%20Luhui%20Hu&entry.1292438233=%20%20Recent%20advances%20in%20imitation%20learning%20have%20shown%20significant%20promise%20for%0Arobotic%20control%20and%20embodied%20intelligence.%20However%2C%20achieving%20robust%0Ageneralization%20across%20diverse%20mounted%20camera%20observations%20remains%20a%20critical%0Achallenge.%20In%20this%20paper%2C%20we%20introduce%20a%20video-based%20spatial%20perception%0Aframework%20that%20leverages%203D%20spatial%20representations%20to%20address%20environmental%0Avariability%2C%20with%20a%20focus%20on%20handling%20lighting%20changes.%20Our%20approach%20integrates%0Aa%20novel%20image%20augmentation%20technique%2C%20AugBlender%2C%20with%20a%20state-of-the-art%0Amonocular%20depth%20estimation%20model%20trained%20on%20internet-scale%20data.%20Together%2C%0Athese%20components%20form%20a%20cohesive%20system%20designed%20to%20enhance%20robustness%20and%0Aadaptability%20in%20dynamic%20scenarios.%20Our%20results%20demonstrate%20that%20our%20approach%0Asignificantly%20boosts%20the%20success%20rate%20across%20diverse%20camera%20exposures%2C%20where%0Aprevious%20models%20experience%20performance%20collapse.%20Our%20findings%20highlight%20the%0Apotential%20of%20video-based%20spatial%20perception%20models%20in%20advancing%20robustness%20for%0Aend-to-end%20robotic%20learning%2C%20paving%20the%20way%20for%20scalable%2C%20low-cost%20solutions%20in%0Aembodied%20intelligence.%0A&entry.1838667208=http%3A//arxiv.org/abs/2411.17458v1&entry.124074799=Read"},
{"title": "Single-cell Curriculum Learning-based Deep Graph Embedding Clustering", "author": "Huifa Li and Jie Fu and Xinpeng Ling and Zhiyu Sun and Kuncan Wang and Zhili Chen", "abstract": "  The swift advancement of single-cell RNA sequencing (scRNA-seq) technologies\nenables the investigation of cellular-level tissue heterogeneity. Cell\nannotation significantly contributes to the extensive downstream analysis of\nscRNA-seq data. However, The analysis of scRNA-seq for biological inference\npresents challenges owing to its intricate and indeterminate data distribution,\ncharacterized by a substantial volume and a high frequency of dropout events.\nFurthermore, the quality of training samples varies greatly, and the\nperformance of the popular scRNA-seq data clustering solution GNN could be\nharmed by two types of low-quality training nodes: 1) nodes on the boundary; 2)\nnodes that contribute little additional information to the graph. To address\nthese problems, we propose a single-cell curriculum learning-based deep graph\nembedding clustering (scCLG). We first propose a Chebyshev graph convolutional\nautoencoder with multi-decoder (ChebAE) that combines three optimization\nobjectives corresponding to three decoders, including topology reconstruction\nloss of cell graphs, zero-inflated negative binomial (ZINB) loss, and\nclustering loss, to learn cell-cell topology representation. Meanwhile, we\nemploy a selective training strategy to train GNN based on the features and\nentropy of nodes and prune the difficult nodes based on the difficulty scores\nto keep the high-quality graph. Empirical results on a variety of gene\nexpression datasets show that our model outperforms state-of-the-art methods.\n", "link": "http://arxiv.org/abs/2408.10511v2", "date": "2024-11-26", "relevancy": 2.4424, "topK": [{"title": "Self-supervised Photographic Image Layout Representation Learning", "link": "http://arxiv.org/abs/2403.03740v1", "similarity": 0.5115}, {"title": "Boosted dynamic neural networks", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/26302", "similarity": 0.49}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.464}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Single-cell%20Curriculum%20Learning-based%20Deep%20Graph%20Embedding%20Clustering&body=Title%3A%20Single-cell%20Curriculum%20Learning-based%20Deep%20Graph%20Embedding%20Clustering%0AAuthor%3A%20Huifa%20Li%20and%20Jie%20Fu%20and%20Xinpeng%20Ling%20and%20Zhiyu%20Sun%20and%20Kuncan%20Wang%20and%20Zhili%20Chen%0AAbstract%3A%20%20%20The%20swift%20advancement%20of%20single-cell%20RNA%20sequencing%20%28scRNA-seq%29%20technologies%0Aenables%20the%20investigation%20of%20cellular-level%20tissue%20heterogeneity.%20Cell%0Aannotation%20significantly%20contributes%20to%20the%20extensive%20downstream%20analysis%20of%0AscRNA-seq%20data.%20However%2C%20The%20analysis%20of%20scRNA-seq%20for%20biological%20inference%0Apresents%20challenges%20owing%20to%20its%20intricate%20and%20indeterminate%20data%20distribution%2C%0Acharacterized%20by%20a%20substantial%20volume%20and%20a%20high%20frequency%20of%20dropout%20events.%0AFurthermore%2C%20the%20quality%20of%20training%20samples%20varies%20greatly%2C%20and%20the%0Aperformance%20of%20the%20popular%20scRNA-seq%20data%20clustering%20solution%20GNN%20could%20be%0Aharmed%20by%20two%20types%20of%20low-quality%20training%20nodes%3A%201%29%20nodes%20on%20the%20boundary%3B%202%29%0Anodes%20that%20contribute%20little%20additional%20information%20to%20the%20graph.%20To%20address%0Athese%20problems%2C%20we%20propose%20a%20single-cell%20curriculum%20learning-based%20deep%20graph%0Aembedding%20clustering%20%28scCLG%29.%20We%20first%20propose%20a%20Chebyshev%20graph%20convolutional%0Aautoencoder%20with%20multi-decoder%20%28ChebAE%29%20that%20combines%20three%20optimization%0Aobjectives%20corresponding%20to%20three%20decoders%2C%20including%20topology%20reconstruction%0Aloss%20of%20cell%20graphs%2C%20zero-inflated%20negative%20binomial%20%28ZINB%29%20loss%2C%20and%0Aclustering%20loss%2C%20to%20learn%20cell-cell%20topology%20representation.%20Meanwhile%2C%20we%0Aemploy%20a%20selective%20training%20strategy%20to%20train%20GNN%20based%20on%20the%20features%20and%0Aentropy%20of%20nodes%20and%20prune%20the%20difficult%20nodes%20based%20on%20the%20difficulty%20scores%0Ato%20keep%20the%20high-quality%20graph.%20Empirical%20results%20on%20a%20variety%20of%20gene%0Aexpression%20datasets%20show%20that%20our%20model%20outperforms%20state-of-the-art%20methods.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2408.10511v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DSingle-cell%2520Curriculum%2520Learning-based%2520Deep%2520Graph%2520Embedding%2520Clustering%26entry.906535625%3DHuifa%2520Li%2520and%2520Jie%2520Fu%2520and%2520Xinpeng%2520Ling%2520and%2520Zhiyu%2520Sun%2520and%2520Kuncan%2520Wang%2520and%2520Zhili%2520Chen%26entry.1292438233%3D%2520%2520The%2520swift%2520advancement%2520of%2520single-cell%2520RNA%2520sequencing%2520%2528scRNA-seq%2529%2520technologies%250Aenables%2520the%2520investigation%2520of%2520cellular-level%2520tissue%2520heterogeneity.%2520Cell%250Aannotation%2520significantly%2520contributes%2520to%2520the%2520extensive%2520downstream%2520analysis%2520of%250AscRNA-seq%2520data.%2520However%252C%2520The%2520analysis%2520of%2520scRNA-seq%2520for%2520biological%2520inference%250Apresents%2520challenges%2520owing%2520to%2520its%2520intricate%2520and%2520indeterminate%2520data%2520distribution%252C%250Acharacterized%2520by%2520a%2520substantial%2520volume%2520and%2520a%2520high%2520frequency%2520of%2520dropout%2520events.%250AFurthermore%252C%2520the%2520quality%2520of%2520training%2520samples%2520varies%2520greatly%252C%2520and%2520the%250Aperformance%2520of%2520the%2520popular%2520scRNA-seq%2520data%2520clustering%2520solution%2520GNN%2520could%2520be%250Aharmed%2520by%2520two%2520types%2520of%2520low-quality%2520training%2520nodes%253A%25201%2529%2520nodes%2520on%2520the%2520boundary%253B%25202%2529%250Anodes%2520that%2520contribute%2520little%2520additional%2520information%2520to%2520the%2520graph.%2520To%2520address%250Athese%2520problems%252C%2520we%2520propose%2520a%2520single-cell%2520curriculum%2520learning-based%2520deep%2520graph%250Aembedding%2520clustering%2520%2528scCLG%2529.%2520We%2520first%2520propose%2520a%2520Chebyshev%2520graph%2520convolutional%250Aautoencoder%2520with%2520multi-decoder%2520%2528ChebAE%2529%2520that%2520combines%2520three%2520optimization%250Aobjectives%2520corresponding%2520to%2520three%2520decoders%252C%2520including%2520topology%2520reconstruction%250Aloss%2520of%2520cell%2520graphs%252C%2520zero-inflated%2520negative%2520binomial%2520%2528ZINB%2529%2520loss%252C%2520and%250Aclustering%2520loss%252C%2520to%2520learn%2520cell-cell%2520topology%2520representation.%2520Meanwhile%252C%2520we%250Aemploy%2520a%2520selective%2520training%2520strategy%2520to%2520train%2520GNN%2520based%2520on%2520the%2520features%2520and%250Aentropy%2520of%2520nodes%2520and%2520prune%2520the%2520difficult%2520nodes%2520based%2520on%2520the%2520difficulty%2520scores%250Ato%2520keep%2520the%2520high-quality%2520graph.%2520Empirical%2520results%2520on%2520a%2520variety%2520of%2520gene%250Aexpression%2520datasets%2520show%2520that%2520our%2520model%2520outperforms%2520state-of-the-art%2520methods.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2408.10511v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Single-cell%20Curriculum%20Learning-based%20Deep%20Graph%20Embedding%20Clustering&entry.906535625=Huifa%20Li%20and%20Jie%20Fu%20and%20Xinpeng%20Ling%20and%20Zhiyu%20Sun%20and%20Kuncan%20Wang%20and%20Zhili%20Chen&entry.1292438233=%20%20The%20swift%20advancement%20of%20single-cell%20RNA%20sequencing%20%28scRNA-seq%29%20technologies%0Aenables%20the%20investigation%20of%20cellular-level%20tissue%20heterogeneity.%20Cell%0Aannotation%20significantly%20contributes%20to%20the%20extensive%20downstream%20analysis%20of%0AscRNA-seq%20data.%20However%2C%20The%20analysis%20of%20scRNA-seq%20for%20biological%20inference%0Apresents%20challenges%20owing%20to%20its%20intricate%20and%20indeterminate%20data%20distribution%2C%0Acharacterized%20by%20a%20substantial%20volume%20and%20a%20high%20frequency%20of%20dropout%20events.%0AFurthermore%2C%20the%20quality%20of%20training%20samples%20varies%20greatly%2C%20and%20the%0Aperformance%20of%20the%20popular%20scRNA-seq%20data%20clustering%20solution%20GNN%20could%20be%0Aharmed%20by%20two%20types%20of%20low-quality%20training%20nodes%3A%201%29%20nodes%20on%20the%20boundary%3B%202%29%0Anodes%20that%20contribute%20little%20additional%20information%20to%20the%20graph.%20To%20address%0Athese%20problems%2C%20we%20propose%20a%20single-cell%20curriculum%20learning-based%20deep%20graph%0Aembedding%20clustering%20%28scCLG%29.%20We%20first%20propose%20a%20Chebyshev%20graph%20convolutional%0Aautoencoder%20with%20multi-decoder%20%28ChebAE%29%20that%20combines%20three%20optimization%0Aobjectives%20corresponding%20to%20three%20decoders%2C%20including%20topology%20reconstruction%0Aloss%20of%20cell%20graphs%2C%20zero-inflated%20negative%20binomial%20%28ZINB%29%20loss%2C%20and%0Aclustering%20loss%2C%20to%20learn%20cell-cell%20topology%20representation.%20Meanwhile%2C%20we%0Aemploy%20a%20selective%20training%20strategy%20to%20train%20GNN%20based%20on%20the%20features%20and%0Aentropy%20of%20nodes%20and%20prune%20the%20difficult%20nodes%20based%20on%20the%20difficulty%20scores%0Ato%20keep%20the%20high-quality%20graph.%20Empirical%20results%20on%20a%20variety%20of%20gene%0Aexpression%20datasets%20show%20that%20our%20model%20outperforms%20state-of-the-art%20methods.%0A&entry.1838667208=http%3A//arxiv.org/abs/2408.10511v2&entry.124074799=Read"},
{"title": "Automatic Album Sequencing", "author": "Vincent Herrmann and Dylan R. Ashley and J\u00fcrgen Schmidhuber", "abstract": "  Album sequencing is a critical part of the album production process.\nRecently, a data-driven approach was proposed that sequences general\ncollections of independent media by extracting the narrative essence of the\nitems in the collections. While this approach implies an album sequencing\ntechnique, it is not widely accessible to a less technical audience, requiring\nadvanced knowledge of machine learning techniques to use. To address this, we\nintroduce a new user-friendly web-based tool that allows a less technical\naudience to upload music tracks, execute this technique in one click, and\nsubsequently presents the result in a clean visualization to the user. To both\nincrease the number of templates available to the user and address shortcomings\nof previous work, we also introduce a new direct transformer-based album\nsequencing method. We find that our more direct method outperforms a random\nbaseline but does not reach the same performance as the narrative essence\napproach. Both methods are included in our web-based user interface, and this\n-- alongside a full copy of our implementation -- is publicly available at\nhttps://github.com/dylanashley/automatic-album-sequencing\n", "link": "http://arxiv.org/abs/2411.07772v2", "date": "2024-11-26", "relevancy": 2.4367, "topK": [{"title": "DressCode: Autoregressively Sewing and Generating Garments from Text\n  Guidance", "link": "http://arxiv.org/abs/2401.16465v3", "similarity": 0.5126}, {"title": "Self-supervised Photographic Image Layout Representation Learning", "link": "http://arxiv.org/abs/2403.03740v1", "similarity": 0.4952}, {"title": "Magic-Me: Identity-Specific Video Customized Diffusion", "link": "http://arxiv.org/abs/2402.09368v1", "similarity": 0.4542}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Automatic%20Album%20Sequencing&body=Title%3A%20Automatic%20Album%20Sequencing%0AAuthor%3A%20Vincent%20Herrmann%20and%20Dylan%20R.%20Ashley%20and%20J%C3%BCrgen%20Schmidhuber%0AAbstract%3A%20%20%20Album%20sequencing%20is%20a%20critical%20part%20of%20the%20album%20production%20process.%0ARecently%2C%20a%20data-driven%20approach%20was%20proposed%20that%20sequences%20general%0Acollections%20of%20independent%20media%20by%20extracting%20the%20narrative%20essence%20of%20the%0Aitems%20in%20the%20collections.%20While%20this%20approach%20implies%20an%20album%20sequencing%0Atechnique%2C%20it%20is%20not%20widely%20accessible%20to%20a%20less%20technical%20audience%2C%20requiring%0Aadvanced%20knowledge%20of%20machine%20learning%20techniques%20to%20use.%20To%20address%20this%2C%20we%0Aintroduce%20a%20new%20user-friendly%20web-based%20tool%20that%20allows%20a%20less%20technical%0Aaudience%20to%20upload%20music%20tracks%2C%20execute%20this%20technique%20in%20one%20click%2C%20and%0Asubsequently%20presents%20the%20result%20in%20a%20clean%20visualization%20to%20the%20user.%20To%20both%0Aincrease%20the%20number%20of%20templates%20available%20to%20the%20user%20and%20address%20shortcomings%0Aof%20previous%20work%2C%20we%20also%20introduce%20a%20new%20direct%20transformer-based%20album%0Asequencing%20method.%20We%20find%20that%20our%20more%20direct%20method%20outperforms%20a%20random%0Abaseline%20but%20does%20not%20reach%20the%20same%20performance%20as%20the%20narrative%20essence%0Aapproach.%20Both%20methods%20are%20included%20in%20our%20web-based%20user%20interface%2C%20and%20this%0A--%20alongside%20a%20full%20copy%20of%20our%20implementation%20--%20is%20publicly%20available%20at%0Ahttps%3A//github.com/dylanashley/automatic-album-sequencing%0A%0ALink%3A%20http%3A//arxiv.org/abs/2411.07772v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DAutomatic%2520Album%2520Sequencing%26entry.906535625%3DVincent%2520Herrmann%2520and%2520Dylan%2520R.%2520Ashley%2520and%2520J%25C3%25BCrgen%2520Schmidhuber%26entry.1292438233%3D%2520%2520Album%2520sequencing%2520is%2520a%2520critical%2520part%2520of%2520the%2520album%2520production%2520process.%250ARecently%252C%2520a%2520data-driven%2520approach%2520was%2520proposed%2520that%2520sequences%2520general%250Acollections%2520of%2520independent%2520media%2520by%2520extracting%2520the%2520narrative%2520essence%2520of%2520the%250Aitems%2520in%2520the%2520collections.%2520While%2520this%2520approach%2520implies%2520an%2520album%2520sequencing%250Atechnique%252C%2520it%2520is%2520not%2520widely%2520accessible%2520to%2520a%2520less%2520technical%2520audience%252C%2520requiring%250Aadvanced%2520knowledge%2520of%2520machine%2520learning%2520techniques%2520to%2520use.%2520To%2520address%2520this%252C%2520we%250Aintroduce%2520a%2520new%2520user-friendly%2520web-based%2520tool%2520that%2520allows%2520a%2520less%2520technical%250Aaudience%2520to%2520upload%2520music%2520tracks%252C%2520execute%2520this%2520technique%2520in%2520one%2520click%252C%2520and%250Asubsequently%2520presents%2520the%2520result%2520in%2520a%2520clean%2520visualization%2520to%2520the%2520user.%2520To%2520both%250Aincrease%2520the%2520number%2520of%2520templates%2520available%2520to%2520the%2520user%2520and%2520address%2520shortcomings%250Aof%2520previous%2520work%252C%2520we%2520also%2520introduce%2520a%2520new%2520direct%2520transformer-based%2520album%250Asequencing%2520method.%2520We%2520find%2520that%2520our%2520more%2520direct%2520method%2520outperforms%2520a%2520random%250Abaseline%2520but%2520does%2520not%2520reach%2520the%2520same%2520performance%2520as%2520the%2520narrative%2520essence%250Aapproach.%2520Both%2520methods%2520are%2520included%2520in%2520our%2520web-based%2520user%2520interface%252C%2520and%2520this%250A--%2520alongside%2520a%2520full%2520copy%2520of%2520our%2520implementation%2520--%2520is%2520publicly%2520available%2520at%250Ahttps%253A//github.com/dylanashley/automatic-album-sequencing%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2411.07772v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Automatic%20Album%20Sequencing&entry.906535625=Vincent%20Herrmann%20and%20Dylan%20R.%20Ashley%20and%20J%C3%BCrgen%20Schmidhuber&entry.1292438233=%20%20Album%20sequencing%20is%20a%20critical%20part%20of%20the%20album%20production%20process.%0ARecently%2C%20a%20data-driven%20approach%20was%20proposed%20that%20sequences%20general%0Acollections%20of%20independent%20media%20by%20extracting%20the%20narrative%20essence%20of%20the%0Aitems%20in%20the%20collections.%20While%20this%20approach%20implies%20an%20album%20sequencing%0Atechnique%2C%20it%20is%20not%20widely%20accessible%20to%20a%20less%20technical%20audience%2C%20requiring%0Aadvanced%20knowledge%20of%20machine%20learning%20techniques%20to%20use.%20To%20address%20this%2C%20we%0Aintroduce%20a%20new%20user-friendly%20web-based%20tool%20that%20allows%20a%20less%20technical%0Aaudience%20to%20upload%20music%20tracks%2C%20execute%20this%20technique%20in%20one%20click%2C%20and%0Asubsequently%20presents%20the%20result%20in%20a%20clean%20visualization%20to%20the%20user.%20To%20both%0Aincrease%20the%20number%20of%20templates%20available%20to%20the%20user%20and%20address%20shortcomings%0Aof%20previous%20work%2C%20we%20also%20introduce%20a%20new%20direct%20transformer-based%20album%0Asequencing%20method.%20We%20find%20that%20our%20more%20direct%20method%20outperforms%20a%20random%0Abaseline%20but%20does%20not%20reach%20the%20same%20performance%20as%20the%20narrative%20essence%0Aapproach.%20Both%20methods%20are%20included%20in%20our%20web-based%20user%20interface%2C%20and%20this%0A--%20alongside%20a%20full%20copy%20of%20our%20implementation%20--%20is%20publicly%20available%20at%0Ahttps%3A//github.com/dylanashley/automatic-album-sequencing%0A&entry.1838667208=http%3A//arxiv.org/abs/2411.07772v2&entry.124074799=Read"},
{"title": "FedReMa: Improving Personalized Federated Learning via Leveraging the\n  Most Relevant Clients", "author": "Han Liang and Ziwei Zhan and Weijie Liu and Xiaoxi Zhang and Chee Wei Tan and Xu Chen", "abstract": "  Federated Learning (FL) is a distributed machine learning paradigm that\nachieves a globally robust model through decentralized computation and periodic\nmodel synthesis, primarily focusing on the global model's accuracy over\naggregated datasets of all participating clients. Personalized Federated\nLearning (PFL) instead tailors exclusive models for each client, aiming to\nenhance the accuracy of clients' individual models on specific local data\ndistributions. Despite of their wide adoption, existing FL and PFL works have\nyet to comprehensively address the class-imbalance issue, one of the most\ncritical challenges within the realm of data heterogeneity in PFL and FL\nresearch. In this paper, we propose FedReMa, an efficient PFL algorithm that\ncan tackle class-imbalance by 1) utilizing an adaptive inter-client co-learning\napproach to identify and harness different clients' expertise on different data\nclasses throughout various phases of the training process, and 2) employing\ndistinct aggregation methods for clients' feature extractors and classifiers,\nwith the choices informed by the different roles and implications of these\nmodel components. Specifically, driven by our experimental findings on\ninter-client similarity dynamics, we develop critical co-learning period (CCP),\nwherein we introduce a module named maximum difference segmentation (MDS) to\nassess and manage task relevance by analyzing the similarities between clients'\nlogits of their classifiers. Outside the CCP, we employ an additional scheme\nfor model aggregation that utilizes historical records of each client's most\nrelevant peers to further enhance the personalization stability. We demonstrate\nthe superiority of our FedReMa in extensive experiments.\n", "link": "http://arxiv.org/abs/2411.01825v2", "date": "2024-11-26", "relevancy": 2.4366, "topK": [{"title": "Breadcrumbs: Adversarial class-balanced sampling for long-tailed recognition", "link": "https://link.springer.com/chapter/10.1007/978-3-031-20053-3_37", "similarity": 0.4951}, {"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.4849}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.4819}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20FedReMa%3A%20Improving%20Personalized%20Federated%20Learning%20via%20Leveraging%20the%0A%20%20Most%20Relevant%20Clients&body=Title%3A%20FedReMa%3A%20Improving%20Personalized%20Federated%20Learning%20via%20Leveraging%20the%0A%20%20Most%20Relevant%20Clients%0AAuthor%3A%20Han%20Liang%20and%20Ziwei%20Zhan%20and%20Weijie%20Liu%20and%20Xiaoxi%20Zhang%20and%20Chee%20Wei%20Tan%20and%20Xu%20Chen%0AAbstract%3A%20%20%20Federated%20Learning%20%28FL%29%20is%20a%20distributed%20machine%20learning%20paradigm%20that%0Aachieves%20a%20globally%20robust%20model%20through%20decentralized%20computation%20and%20periodic%0Amodel%20synthesis%2C%20primarily%20focusing%20on%20the%20global%20model%27s%20accuracy%20over%0Aaggregated%20datasets%20of%20all%20participating%20clients.%20Personalized%20Federated%0ALearning%20%28PFL%29%20instead%20tailors%20exclusive%20models%20for%20each%20client%2C%20aiming%20to%0Aenhance%20the%20accuracy%20of%20clients%27%20individual%20models%20on%20specific%20local%20data%0Adistributions.%20Despite%20of%20their%20wide%20adoption%2C%20existing%20FL%20and%20PFL%20works%20have%0Ayet%20to%20comprehensively%20address%20the%20class-imbalance%20issue%2C%20one%20of%20the%20most%0Acritical%20challenges%20within%20the%20realm%20of%20data%20heterogeneity%20in%20PFL%20and%20FL%0Aresearch.%20In%20this%20paper%2C%20we%20propose%20FedReMa%2C%20an%20efficient%20PFL%20algorithm%20that%0Acan%20tackle%20class-imbalance%20by%201%29%20utilizing%20an%20adaptive%20inter-client%20co-learning%0Aapproach%20to%20identify%20and%20harness%20different%20clients%27%20expertise%20on%20different%20data%0Aclasses%20throughout%20various%20phases%20of%20the%20training%20process%2C%20and%202%29%20employing%0Adistinct%20aggregation%20methods%20for%20clients%27%20feature%20extractors%20and%20classifiers%2C%0Awith%20the%20choices%20informed%20by%20the%20different%20roles%20and%20implications%20of%20these%0Amodel%20components.%20Specifically%2C%20driven%20by%20our%20experimental%20findings%20on%0Ainter-client%20similarity%20dynamics%2C%20we%20develop%20critical%20co-learning%20period%20%28CCP%29%2C%0Awherein%20we%20introduce%20a%20module%20named%20maximum%20difference%20segmentation%20%28MDS%29%20to%0Aassess%20and%20manage%20task%20relevance%20by%20analyzing%20the%20similarities%20between%20clients%27%0Alogits%20of%20their%20classifiers.%20Outside%20the%20CCP%2C%20we%20employ%20an%20additional%20scheme%0Afor%20model%20aggregation%20that%20utilizes%20historical%20records%20of%20each%20client%27s%20most%0Arelevant%20peers%20to%20further%20enhance%20the%20personalization%20stability.%20We%20demonstrate%0Athe%20superiority%20of%20our%20FedReMa%20in%20extensive%20experiments.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2411.01825v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DFedReMa%253A%2520Improving%2520Personalized%2520Federated%2520Learning%2520via%2520Leveraging%2520the%250A%2520%2520Most%2520Relevant%2520Clients%26entry.906535625%3DHan%2520Liang%2520and%2520Ziwei%2520Zhan%2520and%2520Weijie%2520Liu%2520and%2520Xiaoxi%2520Zhang%2520and%2520Chee%2520Wei%2520Tan%2520and%2520Xu%2520Chen%26entry.1292438233%3D%2520%2520Federated%2520Learning%2520%2528FL%2529%2520is%2520a%2520distributed%2520machine%2520learning%2520paradigm%2520that%250Aachieves%2520a%2520globally%2520robust%2520model%2520through%2520decentralized%2520computation%2520and%2520periodic%250Amodel%2520synthesis%252C%2520primarily%2520focusing%2520on%2520the%2520global%2520model%2527s%2520accuracy%2520over%250Aaggregated%2520datasets%2520of%2520all%2520participating%2520clients.%2520Personalized%2520Federated%250ALearning%2520%2528PFL%2529%2520instead%2520tailors%2520exclusive%2520models%2520for%2520each%2520client%252C%2520aiming%2520to%250Aenhance%2520the%2520accuracy%2520of%2520clients%2527%2520individual%2520models%2520on%2520specific%2520local%2520data%250Adistributions.%2520Despite%2520of%2520their%2520wide%2520adoption%252C%2520existing%2520FL%2520and%2520PFL%2520works%2520have%250Ayet%2520to%2520comprehensively%2520address%2520the%2520class-imbalance%2520issue%252C%2520one%2520of%2520the%2520most%250Acritical%2520challenges%2520within%2520the%2520realm%2520of%2520data%2520heterogeneity%2520in%2520PFL%2520and%2520FL%250Aresearch.%2520In%2520this%2520paper%252C%2520we%2520propose%2520FedReMa%252C%2520an%2520efficient%2520PFL%2520algorithm%2520that%250Acan%2520tackle%2520class-imbalance%2520by%25201%2529%2520utilizing%2520an%2520adaptive%2520inter-client%2520co-learning%250Aapproach%2520to%2520identify%2520and%2520harness%2520different%2520clients%2527%2520expertise%2520on%2520different%2520data%250Aclasses%2520throughout%2520various%2520phases%2520of%2520the%2520training%2520process%252C%2520and%25202%2529%2520employing%250Adistinct%2520aggregation%2520methods%2520for%2520clients%2527%2520feature%2520extractors%2520and%2520classifiers%252C%250Awith%2520the%2520choices%2520informed%2520by%2520the%2520different%2520roles%2520and%2520implications%2520of%2520these%250Amodel%2520components.%2520Specifically%252C%2520driven%2520by%2520our%2520experimental%2520findings%2520on%250Ainter-client%2520similarity%2520dynamics%252C%2520we%2520develop%2520critical%2520co-learning%2520period%2520%2528CCP%2529%252C%250Awherein%2520we%2520introduce%2520a%2520module%2520named%2520maximum%2520difference%2520segmentation%2520%2528MDS%2529%2520to%250Aassess%2520and%2520manage%2520task%2520relevance%2520by%2520analyzing%2520the%2520similarities%2520between%2520clients%2527%250Alogits%2520of%2520their%2520classifiers.%2520Outside%2520the%2520CCP%252C%2520we%2520employ%2520an%2520additional%2520scheme%250Afor%2520model%2520aggregation%2520that%2520utilizes%2520historical%2520records%2520of%2520each%2520client%2527s%2520most%250Arelevant%2520peers%2520to%2520further%2520enhance%2520the%2520personalization%2520stability.%2520We%2520demonstrate%250Athe%2520superiority%2520of%2520our%2520FedReMa%2520in%2520extensive%2520experiments.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2411.01825v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=FedReMa%3A%20Improving%20Personalized%20Federated%20Learning%20via%20Leveraging%20the%0A%20%20Most%20Relevant%20Clients&entry.906535625=Han%20Liang%20and%20Ziwei%20Zhan%20and%20Weijie%20Liu%20and%20Xiaoxi%20Zhang%20and%20Chee%20Wei%20Tan%20and%20Xu%20Chen&entry.1292438233=%20%20Federated%20Learning%20%28FL%29%20is%20a%20distributed%20machine%20learning%20paradigm%20that%0Aachieves%20a%20globally%20robust%20model%20through%20decentralized%20computation%20and%20periodic%0Amodel%20synthesis%2C%20primarily%20focusing%20on%20the%20global%20model%27s%20accuracy%20over%0Aaggregated%20datasets%20of%20all%20participating%20clients.%20Personalized%20Federated%0ALearning%20%28PFL%29%20instead%20tailors%20exclusive%20models%20for%20each%20client%2C%20aiming%20to%0Aenhance%20the%20accuracy%20of%20clients%27%20individual%20models%20on%20specific%20local%20data%0Adistributions.%20Despite%20of%20their%20wide%20adoption%2C%20existing%20FL%20and%20PFL%20works%20have%0Ayet%20to%20comprehensively%20address%20the%20class-imbalance%20issue%2C%20one%20of%20the%20most%0Acritical%20challenges%20within%20the%20realm%20of%20data%20heterogeneity%20in%20PFL%20and%20FL%0Aresearch.%20In%20this%20paper%2C%20we%20propose%20FedReMa%2C%20an%20efficient%20PFL%20algorithm%20that%0Acan%20tackle%20class-imbalance%20by%201%29%20utilizing%20an%20adaptive%20inter-client%20co-learning%0Aapproach%20to%20identify%20and%20harness%20different%20clients%27%20expertise%20on%20different%20data%0Aclasses%20throughout%20various%20phases%20of%20the%20training%20process%2C%20and%202%29%20employing%0Adistinct%20aggregation%20methods%20for%20clients%27%20feature%20extractors%20and%20classifiers%2C%0Awith%20the%20choices%20informed%20by%20the%20different%20roles%20and%20implications%20of%20these%0Amodel%20components.%20Specifically%2C%20driven%20by%20our%20experimental%20findings%20on%0Ainter-client%20similarity%20dynamics%2C%20we%20develop%20critical%20co-learning%20period%20%28CCP%29%2C%0Awherein%20we%20introduce%20a%20module%20named%20maximum%20difference%20segmentation%20%28MDS%29%20to%0Aassess%20and%20manage%20task%20relevance%20by%20analyzing%20the%20similarities%20between%20clients%27%0Alogits%20of%20their%20classifiers.%20Outside%20the%20CCP%2C%20we%20employ%20an%20additional%20scheme%0Afor%20model%20aggregation%20that%20utilizes%20historical%20records%20of%20each%20client%27s%20most%0Arelevant%20peers%20to%20further%20enhance%20the%20personalization%20stability.%20We%20demonstrate%0Athe%20superiority%20of%20our%20FedReMa%20in%20extensive%20experiments.%0A&entry.1838667208=http%3A//arxiv.org/abs/2411.01825v2&entry.124074799=Read"},
{"title": "RealSeal: Revolutionizing Media Authentication with Real-Time Realism\n  Scoring", "author": "Bhaktipriya Radharapu and Harish Krishna", "abstract": "  The growing threat of deepfakes and manipulated media necessitates a radical\nrethinking of media authentication. Existing methods for watermarking synthetic\ndata fall short, as they can be easily removed or altered, and current deepfake\ndetection algorithms do not achieve perfect accuracy. Provenance techniques,\nwhich rely on metadata to verify content origin, fail to address the\nfundamental problem of staged or fake media.\n  This paper introduces a groundbreaking paradigm shift in media authentication\nby advocating for the watermarking of real content at its source, as opposed to\nwatermarking synthetic data. Our innovative approach employs multisensory\ninputs and machine learning to assess the realism of content in real-time and\nacross different contexts. We propose embedding a robust realism score within\nthe image metadata, fundamentally transforming how images are trusted and\ncirculated. By combining established principles of human reasoning about\nreality, rooted in firmware and hardware security, with the sophisticated\nreasoning capabilities of contemporary machine learning systems, we develop a\nholistic approach that analyzes information from multiple perspectives.\n  This ambitious, blue sky approach represents a significant leap forward in\nthe field, pushing the boundaries of media authenticity and trust. By embracing\ncutting-edge advancements in technology and interdisciplinary research, we aim\nto establish a new standard for verifying the authenticity of digital media.\n", "link": "http://arxiv.org/abs/2411.17684v1", "date": "2024-11-26", "relevancy": 2.4228, "topK": [{"title": "Magic-Me: Identity-Specific Video Customized Diffusion", "link": "http://arxiv.org/abs/2402.09368v1", "similarity": 0.4962}, {"title": "Tuning-Free Noise Rectification for High Fidelity Image-to-Video\n  Generation", "link": "http://arxiv.org/abs/2403.02827v1", "similarity": 0.4844}, {"title": "PhysGen: Rigid-Body Physics-Grounded Image-to-Video Generation", "link": "http://arxiv.org/abs/2409.18964v1", "similarity": 0.4731}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20RealSeal%3A%20Revolutionizing%20Media%20Authentication%20with%20Real-Time%20Realism%0A%20%20Scoring&body=Title%3A%20RealSeal%3A%20Revolutionizing%20Media%20Authentication%20with%20Real-Time%20Realism%0A%20%20Scoring%0AAuthor%3A%20Bhaktipriya%20Radharapu%20and%20Harish%20Krishna%0AAbstract%3A%20%20%20The%20growing%20threat%20of%20deepfakes%20and%20manipulated%20media%20necessitates%20a%20radical%0Arethinking%20of%20media%20authentication.%20Existing%20methods%20for%20watermarking%20synthetic%0Adata%20fall%20short%2C%20as%20they%20can%20be%20easily%20removed%20or%20altered%2C%20and%20current%20deepfake%0Adetection%20algorithms%20do%20not%20achieve%20perfect%20accuracy.%20Provenance%20techniques%2C%0Awhich%20rely%20on%20metadata%20to%20verify%20content%20origin%2C%20fail%20to%20address%20the%0Afundamental%20problem%20of%20staged%20or%20fake%20media.%0A%20%20This%20paper%20introduces%20a%20groundbreaking%20paradigm%20shift%20in%20media%20authentication%0Aby%20advocating%20for%20the%20watermarking%20of%20real%20content%20at%20its%20source%2C%20as%20opposed%20to%0Awatermarking%20synthetic%20data.%20Our%20innovative%20approach%20employs%20multisensory%0Ainputs%20and%20machine%20learning%20to%20assess%20the%20realism%20of%20content%20in%20real-time%20and%0Aacross%20different%20contexts.%20We%20propose%20embedding%20a%20robust%20realism%20score%20within%0Athe%20image%20metadata%2C%20fundamentally%20transforming%20how%20images%20are%20trusted%20and%0Acirculated.%20By%20combining%20established%20principles%20of%20human%20reasoning%20about%0Areality%2C%20rooted%20in%20firmware%20and%20hardware%20security%2C%20with%20the%20sophisticated%0Areasoning%20capabilities%20of%20contemporary%20machine%20learning%20systems%2C%20we%20develop%20a%0Aholistic%20approach%20that%20analyzes%20information%20from%20multiple%20perspectives.%0A%20%20This%20ambitious%2C%20blue%20sky%20approach%20represents%20a%20significant%20leap%20forward%20in%0Athe%20field%2C%20pushing%20the%20boundaries%20of%20media%20authenticity%20and%20trust.%20By%20embracing%0Acutting-edge%20advancements%20in%20technology%20and%20interdisciplinary%20research%2C%20we%20aim%0Ato%20establish%20a%20new%20standard%20for%20verifying%20the%20authenticity%20of%20digital%20media.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2411.17684v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DRealSeal%253A%2520Revolutionizing%2520Media%2520Authentication%2520with%2520Real-Time%2520Realism%250A%2520%2520Scoring%26entry.906535625%3DBhaktipriya%2520Radharapu%2520and%2520Harish%2520Krishna%26entry.1292438233%3D%2520%2520The%2520growing%2520threat%2520of%2520deepfakes%2520and%2520manipulated%2520media%2520necessitates%2520a%2520radical%250Arethinking%2520of%2520media%2520authentication.%2520Existing%2520methods%2520for%2520watermarking%2520synthetic%250Adata%2520fall%2520short%252C%2520as%2520they%2520can%2520be%2520easily%2520removed%2520or%2520altered%252C%2520and%2520current%2520deepfake%250Adetection%2520algorithms%2520do%2520not%2520achieve%2520perfect%2520accuracy.%2520Provenance%2520techniques%252C%250Awhich%2520rely%2520on%2520metadata%2520to%2520verify%2520content%2520origin%252C%2520fail%2520to%2520address%2520the%250Afundamental%2520problem%2520of%2520staged%2520or%2520fake%2520media.%250A%2520%2520This%2520paper%2520introduces%2520a%2520groundbreaking%2520paradigm%2520shift%2520in%2520media%2520authentication%250Aby%2520advocating%2520for%2520the%2520watermarking%2520of%2520real%2520content%2520at%2520its%2520source%252C%2520as%2520opposed%2520to%250Awatermarking%2520synthetic%2520data.%2520Our%2520innovative%2520approach%2520employs%2520multisensory%250Ainputs%2520and%2520machine%2520learning%2520to%2520assess%2520the%2520realism%2520of%2520content%2520in%2520real-time%2520and%250Aacross%2520different%2520contexts.%2520We%2520propose%2520embedding%2520a%2520robust%2520realism%2520score%2520within%250Athe%2520image%2520metadata%252C%2520fundamentally%2520transforming%2520how%2520images%2520are%2520trusted%2520and%250Acirculated.%2520By%2520combining%2520established%2520principles%2520of%2520human%2520reasoning%2520about%250Areality%252C%2520rooted%2520in%2520firmware%2520and%2520hardware%2520security%252C%2520with%2520the%2520sophisticated%250Areasoning%2520capabilities%2520of%2520contemporary%2520machine%2520learning%2520systems%252C%2520we%2520develop%2520a%250Aholistic%2520approach%2520that%2520analyzes%2520information%2520from%2520multiple%2520perspectives.%250A%2520%2520This%2520ambitious%252C%2520blue%2520sky%2520approach%2520represents%2520a%2520significant%2520leap%2520forward%2520in%250Athe%2520field%252C%2520pushing%2520the%2520boundaries%2520of%2520media%2520authenticity%2520and%2520trust.%2520By%2520embracing%250Acutting-edge%2520advancements%2520in%2520technology%2520and%2520interdisciplinary%2520research%252C%2520we%2520aim%250Ato%2520establish%2520a%2520new%2520standard%2520for%2520verifying%2520the%2520authenticity%2520of%2520digital%2520media.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2411.17684v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=RealSeal%3A%20Revolutionizing%20Media%20Authentication%20with%20Real-Time%20Realism%0A%20%20Scoring&entry.906535625=Bhaktipriya%20Radharapu%20and%20Harish%20Krishna&entry.1292438233=%20%20The%20growing%20threat%20of%20deepfakes%20and%20manipulated%20media%20necessitates%20a%20radical%0Arethinking%20of%20media%20authentication.%20Existing%20methods%20for%20watermarking%20synthetic%0Adata%20fall%20short%2C%20as%20they%20can%20be%20easily%20removed%20or%20altered%2C%20and%20current%20deepfake%0Adetection%20algorithms%20do%20not%20achieve%20perfect%20accuracy.%20Provenance%20techniques%2C%0Awhich%20rely%20on%20metadata%20to%20verify%20content%20origin%2C%20fail%20to%20address%20the%0Afundamental%20problem%20of%20staged%20or%20fake%20media.%0A%20%20This%20paper%20introduces%20a%20groundbreaking%20paradigm%20shift%20in%20media%20authentication%0Aby%20advocating%20for%20the%20watermarking%20of%20real%20content%20at%20its%20source%2C%20as%20opposed%20to%0Awatermarking%20synthetic%20data.%20Our%20innovative%20approach%20employs%20multisensory%0Ainputs%20and%20machine%20learning%20to%20assess%20the%20realism%20of%20content%20in%20real-time%20and%0Aacross%20different%20contexts.%20We%20propose%20embedding%20a%20robust%20realism%20score%20within%0Athe%20image%20metadata%2C%20fundamentally%20transforming%20how%20images%20are%20trusted%20and%0Acirculated.%20By%20combining%20established%20principles%20of%20human%20reasoning%20about%0Areality%2C%20rooted%20in%20firmware%20and%20hardware%20security%2C%20with%20the%20sophisticated%0Areasoning%20capabilities%20of%20contemporary%20machine%20learning%20systems%2C%20we%20develop%20a%0Aholistic%20approach%20that%20analyzes%20information%20from%20multiple%20perspectives.%0A%20%20This%20ambitious%2C%20blue%20sky%20approach%20represents%20a%20significant%20leap%20forward%20in%0Athe%20field%2C%20pushing%20the%20boundaries%20of%20media%20authenticity%20and%20trust.%20By%20embracing%0Acutting-edge%20advancements%20in%20technology%20and%20interdisciplinary%20research%2C%20we%20aim%0Ato%20establish%20a%20new%20standard%20for%20verifying%20the%20authenticity%20of%20digital%20media.%0A&entry.1838667208=http%3A//arxiv.org/abs/2411.17684v1&entry.124074799=Read"},
{"title": "WF-VAE: Enhancing Video VAE by Wavelet-Driven Energy Flow for Latent\n  Video Diffusion Model", "author": "Zongjian Li and Bin Lin and Yang Ye and Liuhan Chen and Xinhua Cheng and Shenghai Yuan and Li Yuan", "abstract": "  Video Variational Autoencoder (VAE) encodes videos into a low-dimensional\nlatent space, becoming a key component of most Latent Video Diffusion Models\n(LVDMs) to reduce model training costs. However, as the resolution and duration\nof generated videos increase, the encoding cost of Video VAEs becomes a\nlimiting bottleneck in training LVDMs. Moreover, the block-wise inference\nmethod adopted by most LVDMs can lead to discontinuities of latent space when\nprocessing long-duration videos. The key to addressing the computational\nbottleneck lies in decomposing videos into distinct components and efficiently\nencoding the critical information. Wavelet transform can decompose videos into\nmultiple frequency-domain components and improve the efficiency significantly,\nwe thus propose Wavelet Flow VAE (WF-VAE), an autoencoder that leverages\nmulti-level wavelet transform to facilitate low-frequency energy flow into\nlatent representation. Furthermore, we introduce a method called Causal Cache,\nwhich maintains the integrity of latent space during block-wise inference.\nCompared to state-of-the-art video VAEs, WF-VAE demonstrates superior\nperformance in both PSNR and LPIPS metrics, achieving 2x higher throughput and\n4x lower memory consumption while maintaining competitive reconstruction\nquality. Our code and models are available at\nhttps://github.com/PKU-YuanGroup/WF-VAE.\n", "link": "http://arxiv.org/abs/2411.17459v1", "date": "2024-11-26", "relevancy": 2.417, "topK": [{"title": "Magic-Me: Identity-Specific Video Customized Diffusion", "link": "http://arxiv.org/abs/2402.09368v1", "similarity": 0.6078}, {"title": "Tuning-Free Noise Rectification for High Fidelity Image-to-Video\n  Generation", "link": "http://arxiv.org/abs/2403.02827v1", "similarity": 0.6063}, {"title": "Customize-A-Video: One-Shot Motion Customization of Text-to-Video\n  Diffusion Models", "link": "http://arxiv.org/abs/2402.14780v1", "similarity": 0.5905}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20WF-VAE%3A%20Enhancing%20Video%20VAE%20by%20Wavelet-Driven%20Energy%20Flow%20for%20Latent%0A%20%20Video%20Diffusion%20Model&body=Title%3A%20WF-VAE%3A%20Enhancing%20Video%20VAE%20by%20Wavelet-Driven%20Energy%20Flow%20for%20Latent%0A%20%20Video%20Diffusion%20Model%0AAuthor%3A%20Zongjian%20Li%20and%20Bin%20Lin%20and%20Yang%20Ye%20and%20Liuhan%20Chen%20and%20Xinhua%20Cheng%20and%20Shenghai%20Yuan%20and%20Li%20Yuan%0AAbstract%3A%20%20%20Video%20Variational%20Autoencoder%20%28VAE%29%20encodes%20videos%20into%20a%20low-dimensional%0Alatent%20space%2C%20becoming%20a%20key%20component%20of%20most%20Latent%20Video%20Diffusion%20Models%0A%28LVDMs%29%20to%20reduce%20model%20training%20costs.%20However%2C%20as%20the%20resolution%20and%20duration%0Aof%20generated%20videos%20increase%2C%20the%20encoding%20cost%20of%20Video%20VAEs%20becomes%20a%0Alimiting%20bottleneck%20in%20training%20LVDMs.%20Moreover%2C%20the%20block-wise%20inference%0Amethod%20adopted%20by%20most%20LVDMs%20can%20lead%20to%20discontinuities%20of%20latent%20space%20when%0Aprocessing%20long-duration%20videos.%20The%20key%20to%20addressing%20the%20computational%0Abottleneck%20lies%20in%20decomposing%20videos%20into%20distinct%20components%20and%20efficiently%0Aencoding%20the%20critical%20information.%20Wavelet%20transform%20can%20decompose%20videos%20into%0Amultiple%20frequency-domain%20components%20and%20improve%20the%20efficiency%20significantly%2C%0Awe%20thus%20propose%20Wavelet%20Flow%20VAE%20%28WF-VAE%29%2C%20an%20autoencoder%20that%20leverages%0Amulti-level%20wavelet%20transform%20to%20facilitate%20low-frequency%20energy%20flow%20into%0Alatent%20representation.%20Furthermore%2C%20we%20introduce%20a%20method%20called%20Causal%20Cache%2C%0Awhich%20maintains%20the%20integrity%20of%20latent%20space%20during%20block-wise%20inference.%0ACompared%20to%20state-of-the-art%20video%20VAEs%2C%20WF-VAE%20demonstrates%20superior%0Aperformance%20in%20both%20PSNR%20and%20LPIPS%20metrics%2C%20achieving%202x%20higher%20throughput%20and%0A4x%20lower%20memory%20consumption%20while%20maintaining%20competitive%20reconstruction%0Aquality.%20Our%20code%20and%20models%20are%20available%20at%0Ahttps%3A//github.com/PKU-YuanGroup/WF-VAE.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2411.17459v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DWF-VAE%253A%2520Enhancing%2520Video%2520VAE%2520by%2520Wavelet-Driven%2520Energy%2520Flow%2520for%2520Latent%250A%2520%2520Video%2520Diffusion%2520Model%26entry.906535625%3DZongjian%2520Li%2520and%2520Bin%2520Lin%2520and%2520Yang%2520Ye%2520and%2520Liuhan%2520Chen%2520and%2520Xinhua%2520Cheng%2520and%2520Shenghai%2520Yuan%2520and%2520Li%2520Yuan%26entry.1292438233%3D%2520%2520Video%2520Variational%2520Autoencoder%2520%2528VAE%2529%2520encodes%2520videos%2520into%2520a%2520low-dimensional%250Alatent%2520space%252C%2520becoming%2520a%2520key%2520component%2520of%2520most%2520Latent%2520Video%2520Diffusion%2520Models%250A%2528LVDMs%2529%2520to%2520reduce%2520model%2520training%2520costs.%2520However%252C%2520as%2520the%2520resolution%2520and%2520duration%250Aof%2520generated%2520videos%2520increase%252C%2520the%2520encoding%2520cost%2520of%2520Video%2520VAEs%2520becomes%2520a%250Alimiting%2520bottleneck%2520in%2520training%2520LVDMs.%2520Moreover%252C%2520the%2520block-wise%2520inference%250Amethod%2520adopted%2520by%2520most%2520LVDMs%2520can%2520lead%2520to%2520discontinuities%2520of%2520latent%2520space%2520when%250Aprocessing%2520long-duration%2520videos.%2520The%2520key%2520to%2520addressing%2520the%2520computational%250Abottleneck%2520lies%2520in%2520decomposing%2520videos%2520into%2520distinct%2520components%2520and%2520efficiently%250Aencoding%2520the%2520critical%2520information.%2520Wavelet%2520transform%2520can%2520decompose%2520videos%2520into%250Amultiple%2520frequency-domain%2520components%2520and%2520improve%2520the%2520efficiency%2520significantly%252C%250Awe%2520thus%2520propose%2520Wavelet%2520Flow%2520VAE%2520%2528WF-VAE%2529%252C%2520an%2520autoencoder%2520that%2520leverages%250Amulti-level%2520wavelet%2520transform%2520to%2520facilitate%2520low-frequency%2520energy%2520flow%2520into%250Alatent%2520representation.%2520Furthermore%252C%2520we%2520introduce%2520a%2520method%2520called%2520Causal%2520Cache%252C%250Awhich%2520maintains%2520the%2520integrity%2520of%2520latent%2520space%2520during%2520block-wise%2520inference.%250ACompared%2520to%2520state-of-the-art%2520video%2520VAEs%252C%2520WF-VAE%2520demonstrates%2520superior%250Aperformance%2520in%2520both%2520PSNR%2520and%2520LPIPS%2520metrics%252C%2520achieving%25202x%2520higher%2520throughput%2520and%250A4x%2520lower%2520memory%2520consumption%2520while%2520maintaining%2520competitive%2520reconstruction%250Aquality.%2520Our%2520code%2520and%2520models%2520are%2520available%2520at%250Ahttps%253A//github.com/PKU-YuanGroup/WF-VAE.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2411.17459v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=WF-VAE%3A%20Enhancing%20Video%20VAE%20by%20Wavelet-Driven%20Energy%20Flow%20for%20Latent%0A%20%20Video%20Diffusion%20Model&entry.906535625=Zongjian%20Li%20and%20Bin%20Lin%20and%20Yang%20Ye%20and%20Liuhan%20Chen%20and%20Xinhua%20Cheng%20and%20Shenghai%20Yuan%20and%20Li%20Yuan&entry.1292438233=%20%20Video%20Variational%20Autoencoder%20%28VAE%29%20encodes%20videos%20into%20a%20low-dimensional%0Alatent%20space%2C%20becoming%20a%20key%20component%20of%20most%20Latent%20Video%20Diffusion%20Models%0A%28LVDMs%29%20to%20reduce%20model%20training%20costs.%20However%2C%20as%20the%20resolution%20and%20duration%0Aof%20generated%20videos%20increase%2C%20the%20encoding%20cost%20of%20Video%20VAEs%20becomes%20a%0Alimiting%20bottleneck%20in%20training%20LVDMs.%20Moreover%2C%20the%20block-wise%20inference%0Amethod%20adopted%20by%20most%20LVDMs%20can%20lead%20to%20discontinuities%20of%20latent%20space%20when%0Aprocessing%20long-duration%20videos.%20The%20key%20to%20addressing%20the%20computational%0Abottleneck%20lies%20in%20decomposing%20videos%20into%20distinct%20components%20and%20efficiently%0Aencoding%20the%20critical%20information.%20Wavelet%20transform%20can%20decompose%20videos%20into%0Amultiple%20frequency-domain%20components%20and%20improve%20the%20efficiency%20significantly%2C%0Awe%20thus%20propose%20Wavelet%20Flow%20VAE%20%28WF-VAE%29%2C%20an%20autoencoder%20that%20leverages%0Amulti-level%20wavelet%20transform%20to%20facilitate%20low-frequency%20energy%20flow%20into%0Alatent%20representation.%20Furthermore%2C%20we%20introduce%20a%20method%20called%20Causal%20Cache%2C%0Awhich%20maintains%20the%20integrity%20of%20latent%20space%20during%20block-wise%20inference.%0ACompared%20to%20state-of-the-art%20video%20VAEs%2C%20WF-VAE%20demonstrates%20superior%0Aperformance%20in%20both%20PSNR%20and%20LPIPS%20metrics%2C%20achieving%202x%20higher%20throughput%20and%0A4x%20lower%20memory%20consumption%20while%20maintaining%20competitive%20reconstruction%0Aquality.%20Our%20code%20and%20models%20are%20available%20at%0Ahttps%3A//github.com/PKU-YuanGroup/WF-VAE.%0A&entry.1838667208=http%3A//arxiv.org/abs/2411.17459v1&entry.124074799=Read"},
{"title": "Interpretable label-free self-guided subspace clustering", "author": "Ivica Kopriva", "abstract": "  Majority subspace clustering (SC) algorithms depend on one or more\nhyperparameters that need to be carefully tuned for the SC algorithms to\nachieve high clustering performance. Hyperparameter optimization (HPO) is often\nperformed using grid-search, assuming that some labeled data is available. In\nsome domains, such as medicine, this assumption does not hold true in many\ncases. One avenue of research focuses on developing SC algorithms that are\ninherently free of hyperparameters. For hyperparameters-dependent SC\nalgorithms, one approach to label-independent HPO tuning is based on internal\nclustering quality metrics (if available), whose performance should ideally\nmatch that of external (label-dependent) clustering quality metrics. In this\npaper, we propose a novel approach to label-independent HPO that uses\nclustering quality metrics, such as accuracy (ACC) or normalized mutual\ninformation (NMI), that are computed based on pseudo-labels obtained from the\nSC algorithm across a predefined grid of hyperparameters. Assuming that ACC (or\nNMI) is a smooth function of hyperparameter values it is possible to select\nsubintervals of hyperparameters. These subintervals are then iteratively\nfurther split into halves or thirds until a relative error criterion is\nsatisfied. In principle, the hyperparameters of any SC algorithm can be tuned\nusing the proposed method. We demonstrate this approach on several single- and\nmulti-view SC algorithms, comparing the achieved performance with their oracle\nversions across six datasets representing digits, faces and objects. The\nproposed method typically achieves clustering performance that is 5% to 7%\nlower than that of the oracle versions. We also make our proposed method\ninterpretable by visualizing subspace bases, which are estimated from the\ncomputed clustering partitions. This aids in the initial selection of the\nhyperparameter search space.\n", "link": "http://arxiv.org/abs/2411.17291v1", "date": "2024-11-26", "relevancy": 2.4109, "topK": [{"title": "Self-supervised Photographic Image Layout Representation Learning", "link": "http://arxiv.org/abs/2403.03740v1", "similarity": 0.4973}, {"title": "GST: Precise 3D Human Body from a Single Image with Gaussian Splatting\n  Transformers", "link": "http://arxiv.org/abs/2409.04196v1", "similarity": 0.482}, {"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.4673}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Interpretable%20label-free%20self-guided%20subspace%20clustering&body=Title%3A%20Interpretable%20label-free%20self-guided%20subspace%20clustering%0AAuthor%3A%20Ivica%20Kopriva%0AAbstract%3A%20%20%20Majority%20subspace%20clustering%20%28SC%29%20algorithms%20depend%20on%20one%20or%20more%0Ahyperparameters%20that%20need%20to%20be%20carefully%20tuned%20for%20the%20SC%20algorithms%20to%0Aachieve%20high%20clustering%20performance.%20Hyperparameter%20optimization%20%28HPO%29%20is%20often%0Aperformed%20using%20grid-search%2C%20assuming%20that%20some%20labeled%20data%20is%20available.%20In%0Asome%20domains%2C%20such%20as%20medicine%2C%20this%20assumption%20does%20not%20hold%20true%20in%20many%0Acases.%20One%20avenue%20of%20research%20focuses%20on%20developing%20SC%20algorithms%20that%20are%0Ainherently%20free%20of%20hyperparameters.%20For%20hyperparameters-dependent%20SC%0Aalgorithms%2C%20one%20approach%20to%20label-independent%20HPO%20tuning%20is%20based%20on%20internal%0Aclustering%20quality%20metrics%20%28if%20available%29%2C%20whose%20performance%20should%20ideally%0Amatch%20that%20of%20external%20%28label-dependent%29%20clustering%20quality%20metrics.%20In%20this%0Apaper%2C%20we%20propose%20a%20novel%20approach%20to%20label-independent%20HPO%20that%20uses%0Aclustering%20quality%20metrics%2C%20such%20as%20accuracy%20%28ACC%29%20or%20normalized%20mutual%0Ainformation%20%28NMI%29%2C%20that%20are%20computed%20based%20on%20pseudo-labels%20obtained%20from%20the%0ASC%20algorithm%20across%20a%20predefined%20grid%20of%20hyperparameters.%20Assuming%20that%20ACC%20%28or%0ANMI%29%20is%20a%20smooth%20function%20of%20hyperparameter%20values%20it%20is%20possible%20to%20select%0Asubintervals%20of%20hyperparameters.%20These%20subintervals%20are%20then%20iteratively%0Afurther%20split%20into%20halves%20or%20thirds%20until%20a%20relative%20error%20criterion%20is%0Asatisfied.%20In%20principle%2C%20the%20hyperparameters%20of%20any%20SC%20algorithm%20can%20be%20tuned%0Ausing%20the%20proposed%20method.%20We%20demonstrate%20this%20approach%20on%20several%20single-%20and%0Amulti-view%20SC%20algorithms%2C%20comparing%20the%20achieved%20performance%20with%20their%20oracle%0Aversions%20across%20six%20datasets%20representing%20digits%2C%20faces%20and%20objects.%20The%0Aproposed%20method%20typically%20achieves%20clustering%20performance%20that%20is%205%25%20to%207%25%0Alower%20than%20that%20of%20the%20oracle%20versions.%20We%20also%20make%20our%20proposed%20method%0Ainterpretable%20by%20visualizing%20subspace%20bases%2C%20which%20are%20estimated%20from%20the%0Acomputed%20clustering%20partitions.%20This%20aids%20in%20the%20initial%20selection%20of%20the%0Ahyperparameter%20search%20space.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2411.17291v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DInterpretable%2520label-free%2520self-guided%2520subspace%2520clustering%26entry.906535625%3DIvica%2520Kopriva%26entry.1292438233%3D%2520%2520Majority%2520subspace%2520clustering%2520%2528SC%2529%2520algorithms%2520depend%2520on%2520one%2520or%2520more%250Ahyperparameters%2520that%2520need%2520to%2520be%2520carefully%2520tuned%2520for%2520the%2520SC%2520algorithms%2520to%250Aachieve%2520high%2520clustering%2520performance.%2520Hyperparameter%2520optimization%2520%2528HPO%2529%2520is%2520often%250Aperformed%2520using%2520grid-search%252C%2520assuming%2520that%2520some%2520labeled%2520data%2520is%2520available.%2520In%250Asome%2520domains%252C%2520such%2520as%2520medicine%252C%2520this%2520assumption%2520does%2520not%2520hold%2520true%2520in%2520many%250Acases.%2520One%2520avenue%2520of%2520research%2520focuses%2520on%2520developing%2520SC%2520algorithms%2520that%2520are%250Ainherently%2520free%2520of%2520hyperparameters.%2520For%2520hyperparameters-dependent%2520SC%250Aalgorithms%252C%2520one%2520approach%2520to%2520label-independent%2520HPO%2520tuning%2520is%2520based%2520on%2520internal%250Aclustering%2520quality%2520metrics%2520%2528if%2520available%2529%252C%2520whose%2520performance%2520should%2520ideally%250Amatch%2520that%2520of%2520external%2520%2528label-dependent%2529%2520clustering%2520quality%2520metrics.%2520In%2520this%250Apaper%252C%2520we%2520propose%2520a%2520novel%2520approach%2520to%2520label-independent%2520HPO%2520that%2520uses%250Aclustering%2520quality%2520metrics%252C%2520such%2520as%2520accuracy%2520%2528ACC%2529%2520or%2520normalized%2520mutual%250Ainformation%2520%2528NMI%2529%252C%2520that%2520are%2520computed%2520based%2520on%2520pseudo-labels%2520obtained%2520from%2520the%250ASC%2520algorithm%2520across%2520a%2520predefined%2520grid%2520of%2520hyperparameters.%2520Assuming%2520that%2520ACC%2520%2528or%250ANMI%2529%2520is%2520a%2520smooth%2520function%2520of%2520hyperparameter%2520values%2520it%2520is%2520possible%2520to%2520select%250Asubintervals%2520of%2520hyperparameters.%2520These%2520subintervals%2520are%2520then%2520iteratively%250Afurther%2520split%2520into%2520halves%2520or%2520thirds%2520until%2520a%2520relative%2520error%2520criterion%2520is%250Asatisfied.%2520In%2520principle%252C%2520the%2520hyperparameters%2520of%2520any%2520SC%2520algorithm%2520can%2520be%2520tuned%250Ausing%2520the%2520proposed%2520method.%2520We%2520demonstrate%2520this%2520approach%2520on%2520several%2520single-%2520and%250Amulti-view%2520SC%2520algorithms%252C%2520comparing%2520the%2520achieved%2520performance%2520with%2520their%2520oracle%250Aversions%2520across%2520six%2520datasets%2520representing%2520digits%252C%2520faces%2520and%2520objects.%2520The%250Aproposed%2520method%2520typically%2520achieves%2520clustering%2520performance%2520that%2520is%25205%2525%2520to%25207%2525%250Alower%2520than%2520that%2520of%2520the%2520oracle%2520versions.%2520We%2520also%2520make%2520our%2520proposed%2520method%250Ainterpretable%2520by%2520visualizing%2520subspace%2520bases%252C%2520which%2520are%2520estimated%2520from%2520the%250Acomputed%2520clustering%2520partitions.%2520This%2520aids%2520in%2520the%2520initial%2520selection%2520of%2520the%250Ahyperparameter%2520search%2520space.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2411.17291v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Interpretable%20label-free%20self-guided%20subspace%20clustering&entry.906535625=Ivica%20Kopriva&entry.1292438233=%20%20Majority%20subspace%20clustering%20%28SC%29%20algorithms%20depend%20on%20one%20or%20more%0Ahyperparameters%20that%20need%20to%20be%20carefully%20tuned%20for%20the%20SC%20algorithms%20to%0Aachieve%20high%20clustering%20performance.%20Hyperparameter%20optimization%20%28HPO%29%20is%20often%0Aperformed%20using%20grid-search%2C%20assuming%20that%20some%20labeled%20data%20is%20available.%20In%0Asome%20domains%2C%20such%20as%20medicine%2C%20this%20assumption%20does%20not%20hold%20true%20in%20many%0Acases.%20One%20avenue%20of%20research%20focuses%20on%20developing%20SC%20algorithms%20that%20are%0Ainherently%20free%20of%20hyperparameters.%20For%20hyperparameters-dependent%20SC%0Aalgorithms%2C%20one%20approach%20to%20label-independent%20HPO%20tuning%20is%20based%20on%20internal%0Aclustering%20quality%20metrics%20%28if%20available%29%2C%20whose%20performance%20should%20ideally%0Amatch%20that%20of%20external%20%28label-dependent%29%20clustering%20quality%20metrics.%20In%20this%0Apaper%2C%20we%20propose%20a%20novel%20approach%20to%20label-independent%20HPO%20that%20uses%0Aclustering%20quality%20metrics%2C%20such%20as%20accuracy%20%28ACC%29%20or%20normalized%20mutual%0Ainformation%20%28NMI%29%2C%20that%20are%20computed%20based%20on%20pseudo-labels%20obtained%20from%20the%0ASC%20algorithm%20across%20a%20predefined%20grid%20of%20hyperparameters.%20Assuming%20that%20ACC%20%28or%0ANMI%29%20is%20a%20smooth%20function%20of%20hyperparameter%20values%20it%20is%20possible%20to%20select%0Asubintervals%20of%20hyperparameters.%20These%20subintervals%20are%20then%20iteratively%0Afurther%20split%20into%20halves%20or%20thirds%20until%20a%20relative%20error%20criterion%20is%0Asatisfied.%20In%20principle%2C%20the%20hyperparameters%20of%20any%20SC%20algorithm%20can%20be%20tuned%0Ausing%20the%20proposed%20method.%20We%20demonstrate%20this%20approach%20on%20several%20single-%20and%0Amulti-view%20SC%20algorithms%2C%20comparing%20the%20achieved%20performance%20with%20their%20oracle%0Aversions%20across%20six%20datasets%20representing%20digits%2C%20faces%20and%20objects.%20The%0Aproposed%20method%20typically%20achieves%20clustering%20performance%20that%20is%205%25%20to%207%25%0Alower%20than%20that%20of%20the%20oracle%20versions.%20We%20also%20make%20our%20proposed%20method%0Ainterpretable%20by%20visualizing%20subspace%20bases%2C%20which%20are%20estimated%20from%20the%0Acomputed%20clustering%20partitions.%20This%20aids%20in%20the%20initial%20selection%20of%20the%0Ahyperparameter%20search%20space.%0A&entry.1838667208=http%3A//arxiv.org/abs/2411.17291v1&entry.124074799=Read"},
{"title": "From Goal-Conditioned to Language-Conditioned Agents via Vision-Language\n  Models", "author": "Theo Cachet and Christopher R. Dance and Olivier Sigaud", "abstract": "  Vision-language models (VLMs) have tremendous potential for grounding\nlanguage, and thus enabling language-conditioned agents (LCAs) to perform\ndiverse tasks specified with text. This has motivated the study of LCAs based\non reinforcement learning (RL) with rewards given by rendering images of an\nenvironment and evaluating those images with VLMs. If single-task RL is\nemployed, such approaches are limited by the cost and time required to train a\npolicy for each new task. Multi-task RL (MTRL) is a natural alternative, but\nrequires a carefully designed corpus of training tasks and does not always\ngeneralize reliably to new tasks. Therefore, this paper introduces a novel\ndecomposition of the problem of building an LCA: first find an environment\nconfiguration that has a high VLM score for text describing a task; then use a\n(pretrained) goal-conditioned policy to reach that configuration. We also\nexplore several enhancements to the speed and quality of VLM-based LCAs,\nnotably, the use of distilled models, and the evaluation of configurations from\nmultiple viewpoints to resolve the ambiguities inherent in a single 2D view. We\ndemonstrate our approach on the Humanoid environment, showing that it results\nin LCAs that outperform MTRL baselines in zero-shot generalization, without\nrequiring any textual task descriptions or other forms of environment-specific\nannotation during training.\n  Videos and an interactive demo can be found at\nhttps://europe.naverlabs.com/text2control\n", "link": "http://arxiv.org/abs/2409.16024v2", "date": "2024-11-26", "relevancy": 2.4026, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.6057}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.6057}, {"title": "RoboEXP: Action-Conditioned Scene Graph via Interactive Exploration for\n  Robotic Manipulation", "link": "http://arxiv.org/abs/2402.15487v1", "similarity": 0.5755}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20From%20Goal-Conditioned%20to%20Language-Conditioned%20Agents%20via%20Vision-Language%0A%20%20Models&body=Title%3A%20From%20Goal-Conditioned%20to%20Language-Conditioned%20Agents%20via%20Vision-Language%0A%20%20Models%0AAuthor%3A%20Theo%20Cachet%20and%20Christopher%20R.%20Dance%20and%20Olivier%20Sigaud%0AAbstract%3A%20%20%20Vision-language%20models%20%28VLMs%29%20have%20tremendous%20potential%20for%20grounding%0Alanguage%2C%20and%20thus%20enabling%20language-conditioned%20agents%20%28LCAs%29%20to%20perform%0Adiverse%20tasks%20specified%20with%20text.%20This%20has%20motivated%20the%20study%20of%20LCAs%20based%0Aon%20reinforcement%20learning%20%28RL%29%20with%20rewards%20given%20by%20rendering%20images%20of%20an%0Aenvironment%20and%20evaluating%20those%20images%20with%20VLMs.%20If%20single-task%20RL%20is%0Aemployed%2C%20such%20approaches%20are%20limited%20by%20the%20cost%20and%20time%20required%20to%20train%20a%0Apolicy%20for%20each%20new%20task.%20Multi-task%20RL%20%28MTRL%29%20is%20a%20natural%20alternative%2C%20but%0Arequires%20a%20carefully%20designed%20corpus%20of%20training%20tasks%20and%20does%20not%20always%0Ageneralize%20reliably%20to%20new%20tasks.%20Therefore%2C%20this%20paper%20introduces%20a%20novel%0Adecomposition%20of%20the%20problem%20of%20building%20an%20LCA%3A%20first%20find%20an%20environment%0Aconfiguration%20that%20has%20a%20high%20VLM%20score%20for%20text%20describing%20a%20task%3B%20then%20use%20a%0A%28pretrained%29%20goal-conditioned%20policy%20to%20reach%20that%20configuration.%20We%20also%0Aexplore%20several%20enhancements%20to%20the%20speed%20and%20quality%20of%20VLM-based%20LCAs%2C%0Anotably%2C%20the%20use%20of%20distilled%20models%2C%20and%20the%20evaluation%20of%20configurations%20from%0Amultiple%20viewpoints%20to%20resolve%20the%20ambiguities%20inherent%20in%20a%20single%202D%20view.%20We%0Ademonstrate%20our%20approach%20on%20the%20Humanoid%20environment%2C%20showing%20that%20it%20results%0Ain%20LCAs%20that%20outperform%20MTRL%20baselines%20in%20zero-shot%20generalization%2C%20without%0Arequiring%20any%20textual%20task%20descriptions%20or%20other%20forms%20of%20environment-specific%0Aannotation%20during%20training.%0A%20%20Videos%20and%20an%20interactive%20demo%20can%20be%20found%20at%0Ahttps%3A//europe.naverlabs.com/text2control%0A%0ALink%3A%20http%3A//arxiv.org/abs/2409.16024v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DFrom%2520Goal-Conditioned%2520to%2520Language-Conditioned%2520Agents%2520via%2520Vision-Language%250A%2520%2520Models%26entry.906535625%3DTheo%2520Cachet%2520and%2520Christopher%2520R.%2520Dance%2520and%2520Olivier%2520Sigaud%26entry.1292438233%3D%2520%2520Vision-language%2520models%2520%2528VLMs%2529%2520have%2520tremendous%2520potential%2520for%2520grounding%250Alanguage%252C%2520and%2520thus%2520enabling%2520language-conditioned%2520agents%2520%2528LCAs%2529%2520to%2520perform%250Adiverse%2520tasks%2520specified%2520with%2520text.%2520This%2520has%2520motivated%2520the%2520study%2520of%2520LCAs%2520based%250Aon%2520reinforcement%2520learning%2520%2528RL%2529%2520with%2520rewards%2520given%2520by%2520rendering%2520images%2520of%2520an%250Aenvironment%2520and%2520evaluating%2520those%2520images%2520with%2520VLMs.%2520If%2520single-task%2520RL%2520is%250Aemployed%252C%2520such%2520approaches%2520are%2520limited%2520by%2520the%2520cost%2520and%2520time%2520required%2520to%2520train%2520a%250Apolicy%2520for%2520each%2520new%2520task.%2520Multi-task%2520RL%2520%2528MTRL%2529%2520is%2520a%2520natural%2520alternative%252C%2520but%250Arequires%2520a%2520carefully%2520designed%2520corpus%2520of%2520training%2520tasks%2520and%2520does%2520not%2520always%250Ageneralize%2520reliably%2520to%2520new%2520tasks.%2520Therefore%252C%2520this%2520paper%2520introduces%2520a%2520novel%250Adecomposition%2520of%2520the%2520problem%2520of%2520building%2520an%2520LCA%253A%2520first%2520find%2520an%2520environment%250Aconfiguration%2520that%2520has%2520a%2520high%2520VLM%2520score%2520for%2520text%2520describing%2520a%2520task%253B%2520then%2520use%2520a%250A%2528pretrained%2529%2520goal-conditioned%2520policy%2520to%2520reach%2520that%2520configuration.%2520We%2520also%250Aexplore%2520several%2520enhancements%2520to%2520the%2520speed%2520and%2520quality%2520of%2520VLM-based%2520LCAs%252C%250Anotably%252C%2520the%2520use%2520of%2520distilled%2520models%252C%2520and%2520the%2520evaluation%2520of%2520configurations%2520from%250Amultiple%2520viewpoints%2520to%2520resolve%2520the%2520ambiguities%2520inherent%2520in%2520a%2520single%25202D%2520view.%2520We%250Ademonstrate%2520our%2520approach%2520on%2520the%2520Humanoid%2520environment%252C%2520showing%2520that%2520it%2520results%250Ain%2520LCAs%2520that%2520outperform%2520MTRL%2520baselines%2520in%2520zero-shot%2520generalization%252C%2520without%250Arequiring%2520any%2520textual%2520task%2520descriptions%2520or%2520other%2520forms%2520of%2520environment-specific%250Aannotation%2520during%2520training.%250A%2520%2520Videos%2520and%2520an%2520interactive%2520demo%2520can%2520be%2520found%2520at%250Ahttps%253A//europe.naverlabs.com/text2control%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2409.16024v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=From%20Goal-Conditioned%20to%20Language-Conditioned%20Agents%20via%20Vision-Language%0A%20%20Models&entry.906535625=Theo%20Cachet%20and%20Christopher%20R.%20Dance%20and%20Olivier%20Sigaud&entry.1292438233=%20%20Vision-language%20models%20%28VLMs%29%20have%20tremendous%20potential%20for%20grounding%0Alanguage%2C%20and%20thus%20enabling%20language-conditioned%20agents%20%28LCAs%29%20to%20perform%0Adiverse%20tasks%20specified%20with%20text.%20This%20has%20motivated%20the%20study%20of%20LCAs%20based%0Aon%20reinforcement%20learning%20%28RL%29%20with%20rewards%20given%20by%20rendering%20images%20of%20an%0Aenvironment%20and%20evaluating%20those%20images%20with%20VLMs.%20If%20single-task%20RL%20is%0Aemployed%2C%20such%20approaches%20are%20limited%20by%20the%20cost%20and%20time%20required%20to%20train%20a%0Apolicy%20for%20each%20new%20task.%20Multi-task%20RL%20%28MTRL%29%20is%20a%20natural%20alternative%2C%20but%0Arequires%20a%20carefully%20designed%20corpus%20of%20training%20tasks%20and%20does%20not%20always%0Ageneralize%20reliably%20to%20new%20tasks.%20Therefore%2C%20this%20paper%20introduces%20a%20novel%0Adecomposition%20of%20the%20problem%20of%20building%20an%20LCA%3A%20first%20find%20an%20environment%0Aconfiguration%20that%20has%20a%20high%20VLM%20score%20for%20text%20describing%20a%20task%3B%20then%20use%20a%0A%28pretrained%29%20goal-conditioned%20policy%20to%20reach%20that%20configuration.%20We%20also%0Aexplore%20several%20enhancements%20to%20the%20speed%20and%20quality%20of%20VLM-based%20LCAs%2C%0Anotably%2C%20the%20use%20of%20distilled%20models%2C%20and%20the%20evaluation%20of%20configurations%20from%0Amultiple%20viewpoints%20to%20resolve%20the%20ambiguities%20inherent%20in%20a%20single%202D%20view.%20We%0Ademonstrate%20our%20approach%20on%20the%20Humanoid%20environment%2C%20showing%20that%20it%20results%0Ain%20LCAs%20that%20outperform%20MTRL%20baselines%20in%20zero-shot%20generalization%2C%20without%0Arequiring%20any%20textual%20task%20descriptions%20or%20other%20forms%20of%20environment-specific%0Aannotation%20during%20training.%0A%20%20Videos%20and%20an%20interactive%20demo%20can%20be%20found%20at%0Ahttps%3A//europe.naverlabs.com/text2control%0A&entry.1838667208=http%3A//arxiv.org/abs/2409.16024v2&entry.124074799=Read"},
{"title": "Event Ellipsometer: Event-based Mueller-Matrix Video Imaging", "author": "Ryota Maeda and Yunseong Moon and Seung-Hwan Baek", "abstract": "  Light-matter interactions modify both the intensity and polarization state of\nlight. Changes in polarization, represented by a Mueller matrix, encode\ndetailed scene information. Existing optical ellipsometers capture\nMueller-matrix images; however, they are often limited to capturing static\nscenes due to long acquisition times. Here, we introduce Event Ellipsometer, a\nmethod for acquiring a Mueller-matrix video for dynamic scenes. Our imaging\nsystem employs fast-rotating quarter-wave plates (QWPs) in front of a light\nsource and an event camera that asynchronously captures intensity changes\ninduced by the rotating QWPs. We develop an ellipsometric-event image formation\nmodel, a calibration method, and an ellipsometric-event reconstruction method.\nWe experimentally demonstrate that Event Ellipsometer enables Mueller-matrix\nvideo imaging at 30fps, extending ellipsometry to dynamic scenes.\n", "link": "http://arxiv.org/abs/2411.17313v1", "date": "2024-11-26", "relevancy": 2.3871, "topK": [{"title": "Dynamic Gaussians Mesh: Consistent Mesh Reconstruction from Monocular\n  Videos", "link": "http://arxiv.org/abs/2404.12379v2", "similarity": 0.4921}, {"title": "PhysGen: Rigid-Body Physics-Grounded Image-to-Video Generation", "link": "http://arxiv.org/abs/2409.18964v1", "similarity": 0.4743}, {"title": "RTG-SLAM: Real-time 3D Reconstruction at Scale using Gaussian Splatting", "link": "http://arxiv.org/abs/2404.19706v1", "similarity": 0.4658}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Event%20Ellipsometer%3A%20Event-based%20Mueller-Matrix%20Video%20Imaging&body=Title%3A%20Event%20Ellipsometer%3A%20Event-based%20Mueller-Matrix%20Video%20Imaging%0AAuthor%3A%20Ryota%20Maeda%20and%20Yunseong%20Moon%20and%20Seung-Hwan%20Baek%0AAbstract%3A%20%20%20Light-matter%20interactions%20modify%20both%20the%20intensity%20and%20polarization%20state%20of%0Alight.%20Changes%20in%20polarization%2C%20represented%20by%20a%20Mueller%20matrix%2C%20encode%0Adetailed%20scene%20information.%20Existing%20optical%20ellipsometers%20capture%0AMueller-matrix%20images%3B%20however%2C%20they%20are%20often%20limited%20to%20capturing%20static%0Ascenes%20due%20to%20long%20acquisition%20times.%20Here%2C%20we%20introduce%20Event%20Ellipsometer%2C%20a%0Amethod%20for%20acquiring%20a%20Mueller-matrix%20video%20for%20dynamic%20scenes.%20Our%20imaging%0Asystem%20employs%20fast-rotating%20quarter-wave%20plates%20%28QWPs%29%20in%20front%20of%20a%20light%0Asource%20and%20an%20event%20camera%20that%20asynchronously%20captures%20intensity%20changes%0Ainduced%20by%20the%20rotating%20QWPs.%20We%20develop%20an%20ellipsometric-event%20image%20formation%0Amodel%2C%20a%20calibration%20method%2C%20and%20an%20ellipsometric-event%20reconstruction%20method.%0AWe%20experimentally%20demonstrate%20that%20Event%20Ellipsometer%20enables%20Mueller-matrix%0Avideo%20imaging%20at%2030fps%2C%20extending%20ellipsometry%20to%20dynamic%20scenes.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2411.17313v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DEvent%2520Ellipsometer%253A%2520Event-based%2520Mueller-Matrix%2520Video%2520Imaging%26entry.906535625%3DRyota%2520Maeda%2520and%2520Yunseong%2520Moon%2520and%2520Seung-Hwan%2520Baek%26entry.1292438233%3D%2520%2520Light-matter%2520interactions%2520modify%2520both%2520the%2520intensity%2520and%2520polarization%2520state%2520of%250Alight.%2520Changes%2520in%2520polarization%252C%2520represented%2520by%2520a%2520Mueller%2520matrix%252C%2520encode%250Adetailed%2520scene%2520information.%2520Existing%2520optical%2520ellipsometers%2520capture%250AMueller-matrix%2520images%253B%2520however%252C%2520they%2520are%2520often%2520limited%2520to%2520capturing%2520static%250Ascenes%2520due%2520to%2520long%2520acquisition%2520times.%2520Here%252C%2520we%2520introduce%2520Event%2520Ellipsometer%252C%2520a%250Amethod%2520for%2520acquiring%2520a%2520Mueller-matrix%2520video%2520for%2520dynamic%2520scenes.%2520Our%2520imaging%250Asystem%2520employs%2520fast-rotating%2520quarter-wave%2520plates%2520%2528QWPs%2529%2520in%2520front%2520of%2520a%2520light%250Asource%2520and%2520an%2520event%2520camera%2520that%2520asynchronously%2520captures%2520intensity%2520changes%250Ainduced%2520by%2520the%2520rotating%2520QWPs.%2520We%2520develop%2520an%2520ellipsometric-event%2520image%2520formation%250Amodel%252C%2520a%2520calibration%2520method%252C%2520and%2520an%2520ellipsometric-event%2520reconstruction%2520method.%250AWe%2520experimentally%2520demonstrate%2520that%2520Event%2520Ellipsometer%2520enables%2520Mueller-matrix%250Avideo%2520imaging%2520at%252030fps%252C%2520extending%2520ellipsometry%2520to%2520dynamic%2520scenes.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2411.17313v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Event%20Ellipsometer%3A%20Event-based%20Mueller-Matrix%20Video%20Imaging&entry.906535625=Ryota%20Maeda%20and%20Yunseong%20Moon%20and%20Seung-Hwan%20Baek&entry.1292438233=%20%20Light-matter%20interactions%20modify%20both%20the%20intensity%20and%20polarization%20state%20of%0Alight.%20Changes%20in%20polarization%2C%20represented%20by%20a%20Mueller%20matrix%2C%20encode%0Adetailed%20scene%20information.%20Existing%20optical%20ellipsometers%20capture%0AMueller-matrix%20images%3B%20however%2C%20they%20are%20often%20limited%20to%20capturing%20static%0Ascenes%20due%20to%20long%20acquisition%20times.%20Here%2C%20we%20introduce%20Event%20Ellipsometer%2C%20a%0Amethod%20for%20acquiring%20a%20Mueller-matrix%20video%20for%20dynamic%20scenes.%20Our%20imaging%0Asystem%20employs%20fast-rotating%20quarter-wave%20plates%20%28QWPs%29%20in%20front%20of%20a%20light%0Asource%20and%20an%20event%20camera%20that%20asynchronously%20captures%20intensity%20changes%0Ainduced%20by%20the%20rotating%20QWPs.%20We%20develop%20an%20ellipsometric-event%20image%20formation%0Amodel%2C%20a%20calibration%20method%2C%20and%20an%20ellipsometric-event%20reconstruction%20method.%0AWe%20experimentally%20demonstrate%20that%20Event%20Ellipsometer%20enables%20Mueller-matrix%0Avideo%20imaging%20at%2030fps%2C%20extending%20ellipsometry%20to%20dynamic%20scenes.%0A&entry.1838667208=http%3A//arxiv.org/abs/2411.17313v1&entry.124074799=Read"},
{"title": "Modality-Incremental Learning with Disjoint Relevance Mapping Networks\n  for Image-based Semantic Segmentation", "author": "Niharika Hegde and Shishir Muralidhara and Ren\u00e9 Schuster and Didier Stricker", "abstract": "  In autonomous driving, environment perception has significantly advanced with\nthe utilization of deep learning techniques for diverse sensors such as\ncameras, depth sensors, or infrared sensors. The diversity in the sensor stack\nincreases the safety and contributes to robustness against adverse weather and\nlighting conditions. However, the variance in data acquired from different\nsensors poses challenges. In the context of continual learning (CL),\nincremental learning is especially challenging for considerably large domain\nshifts, e.g. different sensor modalities. This amplifies the problem of\ncatastrophic forgetting. To address this issue, we formulate the concept of\nmodality-incremental learning and examine its necessity, by contrasting it with\nexisting incremental learning paradigms. We propose the use of a modified\nRelevance Mapping Network (RMN) to incrementally learn new modalities while\npreserving performance on previously learned modalities, in which relevance\nmaps are disjoint. Experimental results demonstrate that the prevention of\nshared connections in this approach helps alleviate the problem of forgetting\nwithin the constraints of a strict continual learning framework.\n", "link": "http://arxiv.org/abs/2411.17610v1", "date": "2024-11-26", "relevancy": 2.3868, "topK": [{"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.6182}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5996}, {"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.5852}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Modality-Incremental%20Learning%20with%20Disjoint%20Relevance%20Mapping%20Networks%0A%20%20for%20Image-based%20Semantic%20Segmentation&body=Title%3A%20Modality-Incremental%20Learning%20with%20Disjoint%20Relevance%20Mapping%20Networks%0A%20%20for%20Image-based%20Semantic%20Segmentation%0AAuthor%3A%20Niharika%20Hegde%20and%20Shishir%20Muralidhara%20and%20Ren%C3%A9%20Schuster%20and%20Didier%20Stricker%0AAbstract%3A%20%20%20In%20autonomous%20driving%2C%20environment%20perception%20has%20significantly%20advanced%20with%0Athe%20utilization%20of%20deep%20learning%20techniques%20for%20diverse%20sensors%20such%20as%0Acameras%2C%20depth%20sensors%2C%20or%20infrared%20sensors.%20The%20diversity%20in%20the%20sensor%20stack%0Aincreases%20the%20safety%20and%20contributes%20to%20robustness%20against%20adverse%20weather%20and%0Alighting%20conditions.%20However%2C%20the%20variance%20in%20data%20acquired%20from%20different%0Asensors%20poses%20challenges.%20In%20the%20context%20of%20continual%20learning%20%28CL%29%2C%0Aincremental%20learning%20is%20especially%20challenging%20for%20considerably%20large%20domain%0Ashifts%2C%20e.g.%20different%20sensor%20modalities.%20This%20amplifies%20the%20problem%20of%0Acatastrophic%20forgetting.%20To%20address%20this%20issue%2C%20we%20formulate%20the%20concept%20of%0Amodality-incremental%20learning%20and%20examine%20its%20necessity%2C%20by%20contrasting%20it%20with%0Aexisting%20incremental%20learning%20paradigms.%20We%20propose%20the%20use%20of%20a%20modified%0ARelevance%20Mapping%20Network%20%28RMN%29%20to%20incrementally%20learn%20new%20modalities%20while%0Apreserving%20performance%20on%20previously%20learned%20modalities%2C%20in%20which%20relevance%0Amaps%20are%20disjoint.%20Experimental%20results%20demonstrate%20that%20the%20prevention%20of%0Ashared%20connections%20in%20this%20approach%20helps%20alleviate%20the%20problem%20of%20forgetting%0Awithin%20the%20constraints%20of%20a%20strict%20continual%20learning%20framework.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2411.17610v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DModality-Incremental%2520Learning%2520with%2520Disjoint%2520Relevance%2520Mapping%2520Networks%250A%2520%2520for%2520Image-based%2520Semantic%2520Segmentation%26entry.906535625%3DNiharika%2520Hegde%2520and%2520Shishir%2520Muralidhara%2520and%2520Ren%25C3%25A9%2520Schuster%2520and%2520Didier%2520Stricker%26entry.1292438233%3D%2520%2520In%2520autonomous%2520driving%252C%2520environment%2520perception%2520has%2520significantly%2520advanced%2520with%250Athe%2520utilization%2520of%2520deep%2520learning%2520techniques%2520for%2520diverse%2520sensors%2520such%2520as%250Acameras%252C%2520depth%2520sensors%252C%2520or%2520infrared%2520sensors.%2520The%2520diversity%2520in%2520the%2520sensor%2520stack%250Aincreases%2520the%2520safety%2520and%2520contributes%2520to%2520robustness%2520against%2520adverse%2520weather%2520and%250Alighting%2520conditions.%2520However%252C%2520the%2520variance%2520in%2520data%2520acquired%2520from%2520different%250Asensors%2520poses%2520challenges.%2520In%2520the%2520context%2520of%2520continual%2520learning%2520%2528CL%2529%252C%250Aincremental%2520learning%2520is%2520especially%2520challenging%2520for%2520considerably%2520large%2520domain%250Ashifts%252C%2520e.g.%2520different%2520sensor%2520modalities.%2520This%2520amplifies%2520the%2520problem%2520of%250Acatastrophic%2520forgetting.%2520To%2520address%2520this%2520issue%252C%2520we%2520formulate%2520the%2520concept%2520of%250Amodality-incremental%2520learning%2520and%2520examine%2520its%2520necessity%252C%2520by%2520contrasting%2520it%2520with%250Aexisting%2520incremental%2520learning%2520paradigms.%2520We%2520propose%2520the%2520use%2520of%2520a%2520modified%250ARelevance%2520Mapping%2520Network%2520%2528RMN%2529%2520to%2520incrementally%2520learn%2520new%2520modalities%2520while%250Apreserving%2520performance%2520on%2520previously%2520learned%2520modalities%252C%2520in%2520which%2520relevance%250Amaps%2520are%2520disjoint.%2520Experimental%2520results%2520demonstrate%2520that%2520the%2520prevention%2520of%250Ashared%2520connections%2520in%2520this%2520approach%2520helps%2520alleviate%2520the%2520problem%2520of%2520forgetting%250Awithin%2520the%2520constraints%2520of%2520a%2520strict%2520continual%2520learning%2520framework.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2411.17610v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Modality-Incremental%20Learning%20with%20Disjoint%20Relevance%20Mapping%20Networks%0A%20%20for%20Image-based%20Semantic%20Segmentation&entry.906535625=Niharika%20Hegde%20and%20Shishir%20Muralidhara%20and%20Ren%C3%A9%20Schuster%20and%20Didier%20Stricker&entry.1292438233=%20%20In%20autonomous%20driving%2C%20environment%20perception%20has%20significantly%20advanced%20with%0Athe%20utilization%20of%20deep%20learning%20techniques%20for%20diverse%20sensors%20such%20as%0Acameras%2C%20depth%20sensors%2C%20or%20infrared%20sensors.%20The%20diversity%20in%20the%20sensor%20stack%0Aincreases%20the%20safety%20and%20contributes%20to%20robustness%20against%20adverse%20weather%20and%0Alighting%20conditions.%20However%2C%20the%20variance%20in%20data%20acquired%20from%20different%0Asensors%20poses%20challenges.%20In%20the%20context%20of%20continual%20learning%20%28CL%29%2C%0Aincremental%20learning%20is%20especially%20challenging%20for%20considerably%20large%20domain%0Ashifts%2C%20e.g.%20different%20sensor%20modalities.%20This%20amplifies%20the%20problem%20of%0Acatastrophic%20forgetting.%20To%20address%20this%20issue%2C%20we%20formulate%20the%20concept%20of%0Amodality-incremental%20learning%20and%20examine%20its%20necessity%2C%20by%20contrasting%20it%20with%0Aexisting%20incremental%20learning%20paradigms.%20We%20propose%20the%20use%20of%20a%20modified%0ARelevance%20Mapping%20Network%20%28RMN%29%20to%20incrementally%20learn%20new%20modalities%20while%0Apreserving%20performance%20on%20previously%20learned%20modalities%2C%20in%20which%20relevance%0Amaps%20are%20disjoint.%20Experimental%20results%20demonstrate%20that%20the%20prevention%20of%0Ashared%20connections%20in%20this%20approach%20helps%20alleviate%20the%20problem%20of%20forgetting%0Awithin%20the%20constraints%20of%20a%20strict%20continual%20learning%20framework.%0A&entry.1838667208=http%3A//arxiv.org/abs/2411.17610v1&entry.124074799=Read"},
{"title": "CatNet: Effective FDR Control in LSTM with Gaussian Mirrors and SHAP\n  Feature Importance", "author": "Jiaan Han and Junxiao Chen and Yanzhe Fu", "abstract": "  We introduce CatNet, an algorithm that effectively controls False Discovery\nRate (FDR) and selects significant features in LSTM with the Gaussian Mirror\n(GM) method. To evaluate the feature importance of LSTM in time series, we\nintroduce a vector of the derivative of the SHapley Additive exPlanations\n(SHAP) to measure feature importance. We also propose a new kernel-based\ndependence measure to avoid multicollinearity in the GM algorithm, to make a\nrobust feature selection with controlled FDR. We use simulated data to evaluate\nCatNet's performance in both linear models and LSTM models with different link\nfunctions. The algorithm effectively controls the FDR while maintaining a high\nstatistical power in all cases. We also evaluate the algorithm's performance in\ndifferent low-dimensional and high-dimensional cases, demonstrating its\nrobustness in various input dimensions. To evaluate CatNet's performance in\nreal world applications, we construct a multi-factor investment portfolio to\nforecast the prices of S\\&P 500 index components. The results demonstrate that\nour model achieves superior predictive accuracy compared to traditional LSTM\nmodels without feature selection and FDR control. Additionally, CatNet\neffectively captures common market-driving features, which helps informed\ndecision-making in financial markets by enhancing the interpretability of\npredictions. Our study integrates of the Gaussian Mirror algorithm with LSTM\nmodels for the first time, and introduces SHAP values as a new feature\nimportance metric for FDR control methods, marking a significant advancement in\nfeature selection and error control for neural networks.\n", "link": "http://arxiv.org/abs/2411.16666v2", "date": "2024-11-26", "relevancy": 2.3653, "topK": [{"title": "MiraGe: Editable 2D Images using Gaussian Splatting", "link": "http://arxiv.org/abs/2410.01521v1", "similarity": 0.48}, {"title": "Dynamic Gaussians Mesh: Consistent Mesh Reconstruction from Monocular\n  Videos", "link": "http://arxiv.org/abs/2404.12379v2", "similarity": 0.4734}, {"title": "Breadcrumbs: Adversarial class-balanced sampling for long-tailed recognition", "link": "https://link.springer.com/chapter/10.1007/978-3-031-20053-3_37", "similarity": 0.4658}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20CatNet%3A%20Effective%20FDR%20Control%20in%20LSTM%20with%20Gaussian%20Mirrors%20and%20SHAP%0A%20%20Feature%20Importance&body=Title%3A%20CatNet%3A%20Effective%20FDR%20Control%20in%20LSTM%20with%20Gaussian%20Mirrors%20and%20SHAP%0A%20%20Feature%20Importance%0AAuthor%3A%20Jiaan%20Han%20and%20Junxiao%20Chen%20and%20Yanzhe%20Fu%0AAbstract%3A%20%20%20We%20introduce%20CatNet%2C%20an%20algorithm%20that%20effectively%20controls%20False%20Discovery%0ARate%20%28FDR%29%20and%20selects%20significant%20features%20in%20LSTM%20with%20the%20Gaussian%20Mirror%0A%28GM%29%20method.%20To%20evaluate%20the%20feature%20importance%20of%20LSTM%20in%20time%20series%2C%20we%0Aintroduce%20a%20vector%20of%20the%20derivative%20of%20the%20SHapley%20Additive%20exPlanations%0A%28SHAP%29%20to%20measure%20feature%20importance.%20We%20also%20propose%20a%20new%20kernel-based%0Adependence%20measure%20to%20avoid%20multicollinearity%20in%20the%20GM%20algorithm%2C%20to%20make%20a%0Arobust%20feature%20selection%20with%20controlled%20FDR.%20We%20use%20simulated%20data%20to%20evaluate%0ACatNet%27s%20performance%20in%20both%20linear%20models%20and%20LSTM%20models%20with%20different%20link%0Afunctions.%20The%20algorithm%20effectively%20controls%20the%20FDR%20while%20maintaining%20a%20high%0Astatistical%20power%20in%20all%20cases.%20We%20also%20evaluate%20the%20algorithm%27s%20performance%20in%0Adifferent%20low-dimensional%20and%20high-dimensional%20cases%2C%20demonstrating%20its%0Arobustness%20in%20various%20input%20dimensions.%20To%20evaluate%20CatNet%27s%20performance%20in%0Areal%20world%20applications%2C%20we%20construct%20a%20multi-factor%20investment%20portfolio%20to%0Aforecast%20the%20prices%20of%20S%5C%26P%20500%20index%20components.%20The%20results%20demonstrate%20that%0Aour%20model%20achieves%20superior%20predictive%20accuracy%20compared%20to%20traditional%20LSTM%0Amodels%20without%20feature%20selection%20and%20FDR%20control.%20Additionally%2C%20CatNet%0Aeffectively%20captures%20common%20market-driving%20features%2C%20which%20helps%20informed%0Adecision-making%20in%20financial%20markets%20by%20enhancing%20the%20interpretability%20of%0Apredictions.%20Our%20study%20integrates%20of%20the%20Gaussian%20Mirror%20algorithm%20with%20LSTM%0Amodels%20for%20the%20first%20time%2C%20and%20introduces%20SHAP%20values%20as%20a%20new%20feature%0Aimportance%20metric%20for%20FDR%20control%20methods%2C%20marking%20a%20significant%20advancement%20in%0Afeature%20selection%20and%20error%20control%20for%20neural%20networks.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2411.16666v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DCatNet%253A%2520Effective%2520FDR%2520Control%2520in%2520LSTM%2520with%2520Gaussian%2520Mirrors%2520and%2520SHAP%250A%2520%2520Feature%2520Importance%26entry.906535625%3DJiaan%2520Han%2520and%2520Junxiao%2520Chen%2520and%2520Yanzhe%2520Fu%26entry.1292438233%3D%2520%2520We%2520introduce%2520CatNet%252C%2520an%2520algorithm%2520that%2520effectively%2520controls%2520False%2520Discovery%250ARate%2520%2528FDR%2529%2520and%2520selects%2520significant%2520features%2520in%2520LSTM%2520with%2520the%2520Gaussian%2520Mirror%250A%2528GM%2529%2520method.%2520To%2520evaluate%2520the%2520feature%2520importance%2520of%2520LSTM%2520in%2520time%2520series%252C%2520we%250Aintroduce%2520a%2520vector%2520of%2520the%2520derivative%2520of%2520the%2520SHapley%2520Additive%2520exPlanations%250A%2528SHAP%2529%2520to%2520measure%2520feature%2520importance.%2520We%2520also%2520propose%2520a%2520new%2520kernel-based%250Adependence%2520measure%2520to%2520avoid%2520multicollinearity%2520in%2520the%2520GM%2520algorithm%252C%2520to%2520make%2520a%250Arobust%2520feature%2520selection%2520with%2520controlled%2520FDR.%2520We%2520use%2520simulated%2520data%2520to%2520evaluate%250ACatNet%2527s%2520performance%2520in%2520both%2520linear%2520models%2520and%2520LSTM%2520models%2520with%2520different%2520link%250Afunctions.%2520The%2520algorithm%2520effectively%2520controls%2520the%2520FDR%2520while%2520maintaining%2520a%2520high%250Astatistical%2520power%2520in%2520all%2520cases.%2520We%2520also%2520evaluate%2520the%2520algorithm%2527s%2520performance%2520in%250Adifferent%2520low-dimensional%2520and%2520high-dimensional%2520cases%252C%2520demonstrating%2520its%250Arobustness%2520in%2520various%2520input%2520dimensions.%2520To%2520evaluate%2520CatNet%2527s%2520performance%2520in%250Areal%2520world%2520applications%252C%2520we%2520construct%2520a%2520multi-factor%2520investment%2520portfolio%2520to%250Aforecast%2520the%2520prices%2520of%2520S%255C%2526P%2520500%2520index%2520components.%2520The%2520results%2520demonstrate%2520that%250Aour%2520model%2520achieves%2520superior%2520predictive%2520accuracy%2520compared%2520to%2520traditional%2520LSTM%250Amodels%2520without%2520feature%2520selection%2520and%2520FDR%2520control.%2520Additionally%252C%2520CatNet%250Aeffectively%2520captures%2520common%2520market-driving%2520features%252C%2520which%2520helps%2520informed%250Adecision-making%2520in%2520financial%2520markets%2520by%2520enhancing%2520the%2520interpretability%2520of%250Apredictions.%2520Our%2520study%2520integrates%2520of%2520the%2520Gaussian%2520Mirror%2520algorithm%2520with%2520LSTM%250Amodels%2520for%2520the%2520first%2520time%252C%2520and%2520introduces%2520SHAP%2520values%2520as%2520a%2520new%2520feature%250Aimportance%2520metric%2520for%2520FDR%2520control%2520methods%252C%2520marking%2520a%2520significant%2520advancement%2520in%250Afeature%2520selection%2520and%2520error%2520control%2520for%2520neural%2520networks.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2411.16666v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=CatNet%3A%20Effective%20FDR%20Control%20in%20LSTM%20with%20Gaussian%20Mirrors%20and%20SHAP%0A%20%20Feature%20Importance&entry.906535625=Jiaan%20Han%20and%20Junxiao%20Chen%20and%20Yanzhe%20Fu&entry.1292438233=%20%20We%20introduce%20CatNet%2C%20an%20algorithm%20that%20effectively%20controls%20False%20Discovery%0ARate%20%28FDR%29%20and%20selects%20significant%20features%20in%20LSTM%20with%20the%20Gaussian%20Mirror%0A%28GM%29%20method.%20To%20evaluate%20the%20feature%20importance%20of%20LSTM%20in%20time%20series%2C%20we%0Aintroduce%20a%20vector%20of%20the%20derivative%20of%20the%20SHapley%20Additive%20exPlanations%0A%28SHAP%29%20to%20measure%20feature%20importance.%20We%20also%20propose%20a%20new%20kernel-based%0Adependence%20measure%20to%20avoid%20multicollinearity%20in%20the%20GM%20algorithm%2C%20to%20make%20a%0Arobust%20feature%20selection%20with%20controlled%20FDR.%20We%20use%20simulated%20data%20to%20evaluate%0ACatNet%27s%20performance%20in%20both%20linear%20models%20and%20LSTM%20models%20with%20different%20link%0Afunctions.%20The%20algorithm%20effectively%20controls%20the%20FDR%20while%20maintaining%20a%20high%0Astatistical%20power%20in%20all%20cases.%20We%20also%20evaluate%20the%20algorithm%27s%20performance%20in%0Adifferent%20low-dimensional%20and%20high-dimensional%20cases%2C%20demonstrating%20its%0Arobustness%20in%20various%20input%20dimensions.%20To%20evaluate%20CatNet%27s%20performance%20in%0Areal%20world%20applications%2C%20we%20construct%20a%20multi-factor%20investment%20portfolio%20to%0Aforecast%20the%20prices%20of%20S%5C%26P%20500%20index%20components.%20The%20results%20demonstrate%20that%0Aour%20model%20achieves%20superior%20predictive%20accuracy%20compared%20to%20traditional%20LSTM%0Amodels%20without%20feature%20selection%20and%20FDR%20control.%20Additionally%2C%20CatNet%0Aeffectively%20captures%20common%20market-driving%20features%2C%20which%20helps%20informed%0Adecision-making%20in%20financial%20markets%20by%20enhancing%20the%20interpretability%20of%0Apredictions.%20Our%20study%20integrates%20of%20the%20Gaussian%20Mirror%20algorithm%20with%20LSTM%0Amodels%20for%20the%20first%20time%2C%20and%20introduces%20SHAP%20values%20as%20a%20new%20feature%0Aimportance%20metric%20for%20FDR%20control%20methods%2C%20marking%20a%20significant%20advancement%20in%0Afeature%20selection%20and%20error%20control%20for%20neural%20networks.%0A&entry.1838667208=http%3A//arxiv.org/abs/2411.16666v2&entry.124074799=Read"},
{"title": "The Exploration of Neural Collapse under Imbalanced Data", "author": "Haixia Liu", "abstract": "  Neural collapse, a newly identified characteristic, describes a property of\nsolutions during model training. In this paper, we explore neural collapse in\nthe context of imbalanced data. We consider the $L$-extended unconstrained\nfeature model with a bias term and provide a theoretical analysis of global\nminimizer.\n  Our findings include: (1) Features within the same class converge to their\nclass mean, similar to both the balanced case and the imbalanced case without\nbias. (2) The geometric structure is mainly on the left orthonormal\ntransformation of the product of $L$ linear classifiers and the right\ntransformation of the class-mean matrix. (3) Some rows of the left orthonormal\ntransformation of the product of $L$ linear classifiers collapse to zeros and\nothers are orthogonal, which relies on the singular values of $\\hat\nY=(I_K-1/N\\mathbf{n}1^\\top_K)D$, where $K$ is class size, $\\mathbf{n}$ is the\nvector of sample size for each class, $D$ is the diagonal matrix whose diagonal\nentries are given by $\\sqrt{\\mathbf{n}}$. Similar results are for the columns\nof the right orthonormal transformation of the product of class-mean matrix and\n$D$. (4) The $i$-th row of the left orthonormal transformation of the product\nof $L$ linear classifiers aligns with the $i$-th column of the right\northonormal transformation of the product of class-mean matrix and $D$. (5) We\nprovide the estimation of singular values about $\\hat Y$. Our numerical\nexperiments support these theoretical findings.\n", "link": "http://arxiv.org/abs/2411.17278v1", "date": "2024-11-26", "relevancy": 2.3444, "topK": [{"title": "Breadcrumbs: Adversarial class-balanced sampling for long-tailed recognition", "link": "https://link.springer.com/chapter/10.1007/978-3-031-20053-3_37", "similarity": 0.4988}, {"title": "Boosted dynamic neural networks", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/26302", "similarity": 0.4627}, {"title": "Self-supervised Photographic Image Layout Representation Learning", "link": "http://arxiv.org/abs/2403.03740v1", "similarity": 0.4451}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20The%20Exploration%20of%20Neural%20Collapse%20under%20Imbalanced%20Data&body=Title%3A%20The%20Exploration%20of%20Neural%20Collapse%20under%20Imbalanced%20Data%0AAuthor%3A%20Haixia%20Liu%0AAbstract%3A%20%20%20Neural%20collapse%2C%20a%20newly%20identified%20characteristic%2C%20describes%20a%20property%20of%0Asolutions%20during%20model%20training.%20In%20this%20paper%2C%20we%20explore%20neural%20collapse%20in%0Athe%20context%20of%20imbalanced%20data.%20We%20consider%20the%20%24L%24-extended%20unconstrained%0Afeature%20model%20with%20a%20bias%20term%20and%20provide%20a%20theoretical%20analysis%20of%20global%0Aminimizer.%0A%20%20Our%20findings%20include%3A%20%281%29%20Features%20within%20the%20same%20class%20converge%20to%20their%0Aclass%20mean%2C%20similar%20to%20both%20the%20balanced%20case%20and%20the%20imbalanced%20case%20without%0Abias.%20%282%29%20The%20geometric%20structure%20is%20mainly%20on%20the%20left%20orthonormal%0Atransformation%20of%20the%20product%20of%20%24L%24%20linear%20classifiers%20and%20the%20right%0Atransformation%20of%20the%20class-mean%20matrix.%20%283%29%20Some%20rows%20of%20the%20left%20orthonormal%0Atransformation%20of%20the%20product%20of%20%24L%24%20linear%20classifiers%20collapse%20to%20zeros%20and%0Aothers%20are%20orthogonal%2C%20which%20relies%20on%20the%20singular%20values%20of%20%24%5Chat%0AY%3D%28I_K-1/N%5Cmathbf%7Bn%7D1%5E%5Ctop_K%29D%24%2C%20where%20%24K%24%20is%20class%20size%2C%20%24%5Cmathbf%7Bn%7D%24%20is%20the%0Avector%20of%20sample%20size%20for%20each%20class%2C%20%24D%24%20is%20the%20diagonal%20matrix%20whose%20diagonal%0Aentries%20are%20given%20by%20%24%5Csqrt%7B%5Cmathbf%7Bn%7D%7D%24.%20Similar%20results%20are%20for%20the%20columns%0Aof%20the%20right%20orthonormal%20transformation%20of%20the%20product%20of%20class-mean%20matrix%20and%0A%24D%24.%20%284%29%20The%20%24i%24-th%20row%20of%20the%20left%20orthonormal%20transformation%20of%20the%20product%0Aof%20%24L%24%20linear%20classifiers%20aligns%20with%20the%20%24i%24-th%20column%20of%20the%20right%0Aorthonormal%20transformation%20of%20the%20product%20of%20class-mean%20matrix%20and%20%24D%24.%20%285%29%20We%0Aprovide%20the%20estimation%20of%20singular%20values%20about%20%24%5Chat%20Y%24.%20Our%20numerical%0Aexperiments%20support%20these%20theoretical%20findings.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2411.17278v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DThe%2520Exploration%2520of%2520Neural%2520Collapse%2520under%2520Imbalanced%2520Data%26entry.906535625%3DHaixia%2520Liu%26entry.1292438233%3D%2520%2520Neural%2520collapse%252C%2520a%2520newly%2520identified%2520characteristic%252C%2520describes%2520a%2520property%2520of%250Asolutions%2520during%2520model%2520training.%2520In%2520this%2520paper%252C%2520we%2520explore%2520neural%2520collapse%2520in%250Athe%2520context%2520of%2520imbalanced%2520data.%2520We%2520consider%2520the%2520%2524L%2524-extended%2520unconstrained%250Afeature%2520model%2520with%2520a%2520bias%2520term%2520and%2520provide%2520a%2520theoretical%2520analysis%2520of%2520global%250Aminimizer.%250A%2520%2520Our%2520findings%2520include%253A%2520%25281%2529%2520Features%2520within%2520the%2520same%2520class%2520converge%2520to%2520their%250Aclass%2520mean%252C%2520similar%2520to%2520both%2520the%2520balanced%2520case%2520and%2520the%2520imbalanced%2520case%2520without%250Abias.%2520%25282%2529%2520The%2520geometric%2520structure%2520is%2520mainly%2520on%2520the%2520left%2520orthonormal%250Atransformation%2520of%2520the%2520product%2520of%2520%2524L%2524%2520linear%2520classifiers%2520and%2520the%2520right%250Atransformation%2520of%2520the%2520class-mean%2520matrix.%2520%25283%2529%2520Some%2520rows%2520of%2520the%2520left%2520orthonormal%250Atransformation%2520of%2520the%2520product%2520of%2520%2524L%2524%2520linear%2520classifiers%2520collapse%2520to%2520zeros%2520and%250Aothers%2520are%2520orthogonal%252C%2520which%2520relies%2520on%2520the%2520singular%2520values%2520of%2520%2524%255Chat%250AY%253D%2528I_K-1/N%255Cmathbf%257Bn%257D1%255E%255Ctop_K%2529D%2524%252C%2520where%2520%2524K%2524%2520is%2520class%2520size%252C%2520%2524%255Cmathbf%257Bn%257D%2524%2520is%2520the%250Avector%2520of%2520sample%2520size%2520for%2520each%2520class%252C%2520%2524D%2524%2520is%2520the%2520diagonal%2520matrix%2520whose%2520diagonal%250Aentries%2520are%2520given%2520by%2520%2524%255Csqrt%257B%255Cmathbf%257Bn%257D%257D%2524.%2520Similar%2520results%2520are%2520for%2520the%2520columns%250Aof%2520the%2520right%2520orthonormal%2520transformation%2520of%2520the%2520product%2520of%2520class-mean%2520matrix%2520and%250A%2524D%2524.%2520%25284%2529%2520The%2520%2524i%2524-th%2520row%2520of%2520the%2520left%2520orthonormal%2520transformation%2520of%2520the%2520product%250Aof%2520%2524L%2524%2520linear%2520classifiers%2520aligns%2520with%2520the%2520%2524i%2524-th%2520column%2520of%2520the%2520right%250Aorthonormal%2520transformation%2520of%2520the%2520product%2520of%2520class-mean%2520matrix%2520and%2520%2524D%2524.%2520%25285%2529%2520We%250Aprovide%2520the%2520estimation%2520of%2520singular%2520values%2520about%2520%2524%255Chat%2520Y%2524.%2520Our%2520numerical%250Aexperiments%2520support%2520these%2520theoretical%2520findings.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2411.17278v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=The%20Exploration%20of%20Neural%20Collapse%20under%20Imbalanced%20Data&entry.906535625=Haixia%20Liu&entry.1292438233=%20%20Neural%20collapse%2C%20a%20newly%20identified%20characteristic%2C%20describes%20a%20property%20of%0Asolutions%20during%20model%20training.%20In%20this%20paper%2C%20we%20explore%20neural%20collapse%20in%0Athe%20context%20of%20imbalanced%20data.%20We%20consider%20the%20%24L%24-extended%20unconstrained%0Afeature%20model%20with%20a%20bias%20term%20and%20provide%20a%20theoretical%20analysis%20of%20global%0Aminimizer.%0A%20%20Our%20findings%20include%3A%20%281%29%20Features%20within%20the%20same%20class%20converge%20to%20their%0Aclass%20mean%2C%20similar%20to%20both%20the%20balanced%20case%20and%20the%20imbalanced%20case%20without%0Abias.%20%282%29%20The%20geometric%20structure%20is%20mainly%20on%20the%20left%20orthonormal%0Atransformation%20of%20the%20product%20of%20%24L%24%20linear%20classifiers%20and%20the%20right%0Atransformation%20of%20the%20class-mean%20matrix.%20%283%29%20Some%20rows%20of%20the%20left%20orthonormal%0Atransformation%20of%20the%20product%20of%20%24L%24%20linear%20classifiers%20collapse%20to%20zeros%20and%0Aothers%20are%20orthogonal%2C%20which%20relies%20on%20the%20singular%20values%20of%20%24%5Chat%0AY%3D%28I_K-1/N%5Cmathbf%7Bn%7D1%5E%5Ctop_K%29D%24%2C%20where%20%24K%24%20is%20class%20size%2C%20%24%5Cmathbf%7Bn%7D%24%20is%20the%0Avector%20of%20sample%20size%20for%20each%20class%2C%20%24D%24%20is%20the%20diagonal%20matrix%20whose%20diagonal%0Aentries%20are%20given%20by%20%24%5Csqrt%7B%5Cmathbf%7Bn%7D%7D%24.%20Similar%20results%20are%20for%20the%20columns%0Aof%20the%20right%20orthonormal%20transformation%20of%20the%20product%20of%20class-mean%20matrix%20and%0A%24D%24.%20%284%29%20The%20%24i%24-th%20row%20of%20the%20left%20orthonormal%20transformation%20of%20the%20product%0Aof%20%24L%24%20linear%20classifiers%20aligns%20with%20the%20%24i%24-th%20column%20of%20the%20right%0Aorthonormal%20transformation%20of%20the%20product%20of%20class-mean%20matrix%20and%20%24D%24.%20%285%29%20We%0Aprovide%20the%20estimation%20of%20singular%20values%20about%20%24%5Chat%20Y%24.%20Our%20numerical%0Aexperiments%20support%20these%20theoretical%20findings.%0A&entry.1838667208=http%3A//arxiv.org/abs/2411.17278v1&entry.124074799=Read"},
{"title": "DexTouch: Learning to Seek and Manipulate Objects with Tactile Dexterity", "author": "Kang-Won Lee and Yuzhe Qin and Xiaolong Wang and Soo-Chul Lim", "abstract": "  The sense of touch is an essential ability for skillfully performing a\nvariety of tasks, providing the capacity to search and manipulate objects\nwithout relying on visual information. In this paper, we introduce a\nmulti-finger robot system designed to manipulate objects using the sense of\ntouch, without relying on vision. For tasks that mimic daily life, the robot\nuses its sense of touch to manipulate randomly placed objects in dark. The\nobjective of this study is to enable robots to perform blind manipulation by\nusing tactile sensation to compensate for the information gap caused by the\nabsence of vision, given the presence of prior information. Training the policy\nthrough reinforcement learning in simulation and transferring the trained\npolicy to the real environment, we demonstrate that blind manipulation can be\napplied to robots without vision. In addition, the experiments showcase the\nimportance of tactile sensing in the blind manipulation tasks. Our project page\nis available at https://lee-kangwon.github.io/dextouch/\n", "link": "http://arxiv.org/abs/2401.12496v2", "date": "2024-11-26", "relevancy": 2.3404, "topK": [{"title": "RoboEXP: Action-Conditioned Scene Graph via Interactive Exploration for\n  Robotic Manipulation", "link": "http://arxiv.org/abs/2402.15487v1", "similarity": 0.6116}, {"title": "UGG: Unified Generative Grasping", "link": "https://arxiv.org/abs/2311.16917", "similarity": 0.5821}, {"title": "Egocentric Computer Vision for Hands-Free Robotic Wheelchair Navigation", "link": "https://link.springer.com/article/10.1007/s10846-023-01807-4", "similarity": 0.5774}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20DexTouch%3A%20Learning%20to%20Seek%20and%20Manipulate%20Objects%20with%20Tactile%20Dexterity&body=Title%3A%20DexTouch%3A%20Learning%20to%20Seek%20and%20Manipulate%20Objects%20with%20Tactile%20Dexterity%0AAuthor%3A%20Kang-Won%20Lee%20and%20Yuzhe%20Qin%20and%20Xiaolong%20Wang%20and%20Soo-Chul%20Lim%0AAbstract%3A%20%20%20The%20sense%20of%20touch%20is%20an%20essential%20ability%20for%20skillfully%20performing%20a%0Avariety%20of%20tasks%2C%20providing%20the%20capacity%20to%20search%20and%20manipulate%20objects%0Awithout%20relying%20on%20visual%20information.%20In%20this%20paper%2C%20we%20introduce%20a%0Amulti-finger%20robot%20system%20designed%20to%20manipulate%20objects%20using%20the%20sense%20of%0Atouch%2C%20without%20relying%20on%20vision.%20For%20tasks%20that%20mimic%20daily%20life%2C%20the%20robot%0Auses%20its%20sense%20of%20touch%20to%20manipulate%20randomly%20placed%20objects%20in%20dark.%20The%0Aobjective%20of%20this%20study%20is%20to%20enable%20robots%20to%20perform%20blind%20manipulation%20by%0Ausing%20tactile%20sensation%20to%20compensate%20for%20the%20information%20gap%20caused%20by%20the%0Aabsence%20of%20vision%2C%20given%20the%20presence%20of%20prior%20information.%20Training%20the%20policy%0Athrough%20reinforcement%20learning%20in%20simulation%20and%20transferring%20the%20trained%0Apolicy%20to%20the%20real%20environment%2C%20we%20demonstrate%20that%20blind%20manipulation%20can%20be%0Aapplied%20to%20robots%20without%20vision.%20In%20addition%2C%20the%20experiments%20showcase%20the%0Aimportance%20of%20tactile%20sensing%20in%20the%20blind%20manipulation%20tasks.%20Our%20project%20page%0Ais%20available%20at%20https%3A//lee-kangwon.github.io/dextouch/%0A%0ALink%3A%20http%3A//arxiv.org/abs/2401.12496v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DDexTouch%253A%2520Learning%2520to%2520Seek%2520and%2520Manipulate%2520Objects%2520with%2520Tactile%2520Dexterity%26entry.906535625%3DKang-Won%2520Lee%2520and%2520Yuzhe%2520Qin%2520and%2520Xiaolong%2520Wang%2520and%2520Soo-Chul%2520Lim%26entry.1292438233%3D%2520%2520The%2520sense%2520of%2520touch%2520is%2520an%2520essential%2520ability%2520for%2520skillfully%2520performing%2520a%250Avariety%2520of%2520tasks%252C%2520providing%2520the%2520capacity%2520to%2520search%2520and%2520manipulate%2520objects%250Awithout%2520relying%2520on%2520visual%2520information.%2520In%2520this%2520paper%252C%2520we%2520introduce%2520a%250Amulti-finger%2520robot%2520system%2520designed%2520to%2520manipulate%2520objects%2520using%2520the%2520sense%2520of%250Atouch%252C%2520without%2520relying%2520on%2520vision.%2520For%2520tasks%2520that%2520mimic%2520daily%2520life%252C%2520the%2520robot%250Auses%2520its%2520sense%2520of%2520touch%2520to%2520manipulate%2520randomly%2520placed%2520objects%2520in%2520dark.%2520The%250Aobjective%2520of%2520this%2520study%2520is%2520to%2520enable%2520robots%2520to%2520perform%2520blind%2520manipulation%2520by%250Ausing%2520tactile%2520sensation%2520to%2520compensate%2520for%2520the%2520information%2520gap%2520caused%2520by%2520the%250Aabsence%2520of%2520vision%252C%2520given%2520the%2520presence%2520of%2520prior%2520information.%2520Training%2520the%2520policy%250Athrough%2520reinforcement%2520learning%2520in%2520simulation%2520and%2520transferring%2520the%2520trained%250Apolicy%2520to%2520the%2520real%2520environment%252C%2520we%2520demonstrate%2520that%2520blind%2520manipulation%2520can%2520be%250Aapplied%2520to%2520robots%2520without%2520vision.%2520In%2520addition%252C%2520the%2520experiments%2520showcase%2520the%250Aimportance%2520of%2520tactile%2520sensing%2520in%2520the%2520blind%2520manipulation%2520tasks.%2520Our%2520project%2520page%250Ais%2520available%2520at%2520https%253A//lee-kangwon.github.io/dextouch/%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2401.12496v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=DexTouch%3A%20Learning%20to%20Seek%20and%20Manipulate%20Objects%20with%20Tactile%20Dexterity&entry.906535625=Kang-Won%20Lee%20and%20Yuzhe%20Qin%20and%20Xiaolong%20Wang%20and%20Soo-Chul%20Lim&entry.1292438233=%20%20The%20sense%20of%20touch%20is%20an%20essential%20ability%20for%20skillfully%20performing%20a%0Avariety%20of%20tasks%2C%20providing%20the%20capacity%20to%20search%20and%20manipulate%20objects%0Awithout%20relying%20on%20visual%20information.%20In%20this%20paper%2C%20we%20introduce%20a%0Amulti-finger%20robot%20system%20designed%20to%20manipulate%20objects%20using%20the%20sense%20of%0Atouch%2C%20without%20relying%20on%20vision.%20For%20tasks%20that%20mimic%20daily%20life%2C%20the%20robot%0Auses%20its%20sense%20of%20touch%20to%20manipulate%20randomly%20placed%20objects%20in%20dark.%20The%0Aobjective%20of%20this%20study%20is%20to%20enable%20robots%20to%20perform%20blind%20manipulation%20by%0Ausing%20tactile%20sensation%20to%20compensate%20for%20the%20information%20gap%20caused%20by%20the%0Aabsence%20of%20vision%2C%20given%20the%20presence%20of%20prior%20information.%20Training%20the%20policy%0Athrough%20reinforcement%20learning%20in%20simulation%20and%20transferring%20the%20trained%0Apolicy%20to%20the%20real%20environment%2C%20we%20demonstrate%20that%20blind%20manipulation%20can%20be%0Aapplied%20to%20robots%20without%20vision.%20In%20addition%2C%20the%20experiments%20showcase%20the%0Aimportance%20of%20tactile%20sensing%20in%20the%20blind%20manipulation%20tasks.%20Our%20project%20page%0Ais%20available%20at%20https%3A//lee-kangwon.github.io/dextouch/%0A&entry.1838667208=http%3A//arxiv.org/abs/2401.12496v2&entry.124074799=Read"},
{"title": "Visatronic: A Multimodal Decoder-Only Model for Speech Synthesis", "author": "Akshita Gupta and Tatiana Likhomanenko and Karren Dai Yang and Richard He Bai and Zakaria Aldeneh and Navdeep Jaitly", "abstract": "  In this paper, we propose a new task -- generating speech from videos of\npeople and their transcripts (VTTS) -- to motivate new techniques for\nmultimodal speech generation. This task generalizes the task of generating\nspeech from cropped lip videos, and is also more complicated than the task of\ngenerating generic audio clips (e.g., dog barking) from videos and text.\nMultilingual versions of the task could lead to new techniques for\ncross-lingual dubbing. We also present a decoder-only multimodal model for this\ntask, which we call Visatronic. This model embeds vision, text and speech\ndirectly into the common subspace of a transformer model and uses an\nautoregressive loss to learn a generative model of discretized mel-spectrograms\nconditioned on speaker videos and transcripts of their speech. By embedding all\nmodalities into a common subspace, Visatronic can achieve improved results over\nmodels that use only text or video as input. Further, it presents a much\nsimpler approach for multimodal speech generation compared to prevailing\napproaches which rely on lip-detectors and complicated architectures to fuse\nmodalities while producing better results. Since the model is flexible enough\nto accommodate different ways of ordering inputs as a sequence, we carefully\nexplore different strategies to better understand the best way to propagate\ninformation to the generative steps. To facilitate further research on VTTS, we\nwill release (i) our code, (ii) clean transcriptions for the large-scale\nVoxCeleb2 dataset, and (iii) a standardized evaluation protocol for VTTS\nincorporating both objective and subjective metrics.\n", "link": "http://arxiv.org/abs/2411.17690v1", "date": "2024-11-26", "relevancy": 2.3367, "topK": [{"title": "Magic-Me: Identity-Specific Video Customized Diffusion", "link": "http://arxiv.org/abs/2402.09368v1", "similarity": 0.6061}, {"title": "Customize-A-Video: One-Shot Motion Customization of Text-to-Video\n  Diffusion Models", "link": "http://arxiv.org/abs/2402.14780v1", "similarity": 0.5858}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5616}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Visatronic%3A%20A%20Multimodal%20Decoder-Only%20Model%20for%20Speech%20Synthesis&body=Title%3A%20Visatronic%3A%20A%20Multimodal%20Decoder-Only%20Model%20for%20Speech%20Synthesis%0AAuthor%3A%20Akshita%20Gupta%20and%20Tatiana%20Likhomanenko%20and%20Karren%20Dai%20Yang%20and%20Richard%20He%20Bai%20and%20Zakaria%20Aldeneh%20and%20Navdeep%20Jaitly%0AAbstract%3A%20%20%20In%20this%20paper%2C%20we%20propose%20a%20new%20task%20--%20generating%20speech%20from%20videos%20of%0Apeople%20and%20their%20transcripts%20%28VTTS%29%20--%20to%20motivate%20new%20techniques%20for%0Amultimodal%20speech%20generation.%20This%20task%20generalizes%20the%20task%20of%20generating%0Aspeech%20from%20cropped%20lip%20videos%2C%20and%20is%20also%20more%20complicated%20than%20the%20task%20of%0Agenerating%20generic%20audio%20clips%20%28e.g.%2C%20dog%20barking%29%20from%20videos%20and%20text.%0AMultilingual%20versions%20of%20the%20task%20could%20lead%20to%20new%20techniques%20for%0Across-lingual%20dubbing.%20We%20also%20present%20a%20decoder-only%20multimodal%20model%20for%20this%0Atask%2C%20which%20we%20call%20Visatronic.%20This%20model%20embeds%20vision%2C%20text%20and%20speech%0Adirectly%20into%20the%20common%20subspace%20of%20a%20transformer%20model%20and%20uses%20an%0Aautoregressive%20loss%20to%20learn%20a%20generative%20model%20of%20discretized%20mel-spectrograms%0Aconditioned%20on%20speaker%20videos%20and%20transcripts%20of%20their%20speech.%20By%20embedding%20all%0Amodalities%20into%20a%20common%20subspace%2C%20Visatronic%20can%20achieve%20improved%20results%20over%0Amodels%20that%20use%20only%20text%20or%20video%20as%20input.%20Further%2C%20it%20presents%20a%20much%0Asimpler%20approach%20for%20multimodal%20speech%20generation%20compared%20to%20prevailing%0Aapproaches%20which%20rely%20on%20lip-detectors%20and%20complicated%20architectures%20to%20fuse%0Amodalities%20while%20producing%20better%20results.%20Since%20the%20model%20is%20flexible%20enough%0Ato%20accommodate%20different%20ways%20of%20ordering%20inputs%20as%20a%20sequence%2C%20we%20carefully%0Aexplore%20different%20strategies%20to%20better%20understand%20the%20best%20way%20to%20propagate%0Ainformation%20to%20the%20generative%20steps.%20To%20facilitate%20further%20research%20on%20VTTS%2C%20we%0Awill%20release%20%28i%29%20our%20code%2C%20%28ii%29%20clean%20transcriptions%20for%20the%20large-scale%0AVoxCeleb2%20dataset%2C%20and%20%28iii%29%20a%20standardized%20evaluation%20protocol%20for%20VTTS%0Aincorporating%20both%20objective%20and%20subjective%20metrics.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2411.17690v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DVisatronic%253A%2520A%2520Multimodal%2520Decoder-Only%2520Model%2520for%2520Speech%2520Synthesis%26entry.906535625%3DAkshita%2520Gupta%2520and%2520Tatiana%2520Likhomanenko%2520and%2520Karren%2520Dai%2520Yang%2520and%2520Richard%2520He%2520Bai%2520and%2520Zakaria%2520Aldeneh%2520and%2520Navdeep%2520Jaitly%26entry.1292438233%3D%2520%2520In%2520this%2520paper%252C%2520we%2520propose%2520a%2520new%2520task%2520--%2520generating%2520speech%2520from%2520videos%2520of%250Apeople%2520and%2520their%2520transcripts%2520%2528VTTS%2529%2520--%2520to%2520motivate%2520new%2520techniques%2520for%250Amultimodal%2520speech%2520generation.%2520This%2520task%2520generalizes%2520the%2520task%2520of%2520generating%250Aspeech%2520from%2520cropped%2520lip%2520videos%252C%2520and%2520is%2520also%2520more%2520complicated%2520than%2520the%2520task%2520of%250Agenerating%2520generic%2520audio%2520clips%2520%2528e.g.%252C%2520dog%2520barking%2529%2520from%2520videos%2520and%2520text.%250AMultilingual%2520versions%2520of%2520the%2520task%2520could%2520lead%2520to%2520new%2520techniques%2520for%250Across-lingual%2520dubbing.%2520We%2520also%2520present%2520a%2520decoder-only%2520multimodal%2520model%2520for%2520this%250Atask%252C%2520which%2520we%2520call%2520Visatronic.%2520This%2520model%2520embeds%2520vision%252C%2520text%2520and%2520speech%250Adirectly%2520into%2520the%2520common%2520subspace%2520of%2520a%2520transformer%2520model%2520and%2520uses%2520an%250Aautoregressive%2520loss%2520to%2520learn%2520a%2520generative%2520model%2520of%2520discretized%2520mel-spectrograms%250Aconditioned%2520on%2520speaker%2520videos%2520and%2520transcripts%2520of%2520their%2520speech.%2520By%2520embedding%2520all%250Amodalities%2520into%2520a%2520common%2520subspace%252C%2520Visatronic%2520can%2520achieve%2520improved%2520results%2520over%250Amodels%2520that%2520use%2520only%2520text%2520or%2520video%2520as%2520input.%2520Further%252C%2520it%2520presents%2520a%2520much%250Asimpler%2520approach%2520for%2520multimodal%2520speech%2520generation%2520compared%2520to%2520prevailing%250Aapproaches%2520which%2520rely%2520on%2520lip-detectors%2520and%2520complicated%2520architectures%2520to%2520fuse%250Amodalities%2520while%2520producing%2520better%2520results.%2520Since%2520the%2520model%2520is%2520flexible%2520enough%250Ato%2520accommodate%2520different%2520ways%2520of%2520ordering%2520inputs%2520as%2520a%2520sequence%252C%2520we%2520carefully%250Aexplore%2520different%2520strategies%2520to%2520better%2520understand%2520the%2520best%2520way%2520to%2520propagate%250Ainformation%2520to%2520the%2520generative%2520steps.%2520To%2520facilitate%2520further%2520research%2520on%2520VTTS%252C%2520we%250Awill%2520release%2520%2528i%2529%2520our%2520code%252C%2520%2528ii%2529%2520clean%2520transcriptions%2520for%2520the%2520large-scale%250AVoxCeleb2%2520dataset%252C%2520and%2520%2528iii%2529%2520a%2520standardized%2520evaluation%2520protocol%2520for%2520VTTS%250Aincorporating%2520both%2520objective%2520and%2520subjective%2520metrics.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2411.17690v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Visatronic%3A%20A%20Multimodal%20Decoder-Only%20Model%20for%20Speech%20Synthesis&entry.906535625=Akshita%20Gupta%20and%20Tatiana%20Likhomanenko%20and%20Karren%20Dai%20Yang%20and%20Richard%20He%20Bai%20and%20Zakaria%20Aldeneh%20and%20Navdeep%20Jaitly&entry.1292438233=%20%20In%20this%20paper%2C%20we%20propose%20a%20new%20task%20--%20generating%20speech%20from%20videos%20of%0Apeople%20and%20their%20transcripts%20%28VTTS%29%20--%20to%20motivate%20new%20techniques%20for%0Amultimodal%20speech%20generation.%20This%20task%20generalizes%20the%20task%20of%20generating%0Aspeech%20from%20cropped%20lip%20videos%2C%20and%20is%20also%20more%20complicated%20than%20the%20task%20of%0Agenerating%20generic%20audio%20clips%20%28e.g.%2C%20dog%20barking%29%20from%20videos%20and%20text.%0AMultilingual%20versions%20of%20the%20task%20could%20lead%20to%20new%20techniques%20for%0Across-lingual%20dubbing.%20We%20also%20present%20a%20decoder-only%20multimodal%20model%20for%20this%0Atask%2C%20which%20we%20call%20Visatronic.%20This%20model%20embeds%20vision%2C%20text%20and%20speech%0Adirectly%20into%20the%20common%20subspace%20of%20a%20transformer%20model%20and%20uses%20an%0Aautoregressive%20loss%20to%20learn%20a%20generative%20model%20of%20discretized%20mel-spectrograms%0Aconditioned%20on%20speaker%20videos%20and%20transcripts%20of%20their%20speech.%20By%20embedding%20all%0Amodalities%20into%20a%20common%20subspace%2C%20Visatronic%20can%20achieve%20improved%20results%20over%0Amodels%20that%20use%20only%20text%20or%20video%20as%20input.%20Further%2C%20it%20presents%20a%20much%0Asimpler%20approach%20for%20multimodal%20speech%20generation%20compared%20to%20prevailing%0Aapproaches%20which%20rely%20on%20lip-detectors%20and%20complicated%20architectures%20to%20fuse%0Amodalities%20while%20producing%20better%20results.%20Since%20the%20model%20is%20flexible%20enough%0Ato%20accommodate%20different%20ways%20of%20ordering%20inputs%20as%20a%20sequence%2C%20we%20carefully%0Aexplore%20different%20strategies%20to%20better%20understand%20the%20best%20way%20to%20propagate%0Ainformation%20to%20the%20generative%20steps.%20To%20facilitate%20further%20research%20on%20VTTS%2C%20we%0Awill%20release%20%28i%29%20our%20code%2C%20%28ii%29%20clean%20transcriptions%20for%20the%20large-scale%0AVoxCeleb2%20dataset%2C%20and%20%28iii%29%20a%20standardized%20evaluation%20protocol%20for%20VTTS%0Aincorporating%20both%20objective%20and%20subjective%20metrics.%0A&entry.1838667208=http%3A//arxiv.org/abs/2411.17690v1&entry.124074799=Read"},
{"title": "IG-CFAT: An Improved GAN-Based Framework for Effectively Exploiting\n  Transformers in Real-World Image Super-Resolution", "author": "Alireza Aghelan and Ali Amiryan and Abolfazl Zarghani and Modjtaba Rouhani", "abstract": "  In the field of single image super-resolution (SISR), transformer-based\nmodels, have demonstrated significant advancements. However, the potential and\nefficiency of these models in applied fields such as real-world image\nsuper-resolution have been less noticed and there are substantial opportunities\nfor improvement. Recently, composite fusion attention transformer (CFAT),\noutperformed previous state-of-the-art (SOTA) models in classic image\nsuper-resolution. In this paper, we propose a novel GAN-based framework by\nincorporating the CFAT model to effectively exploit the performance of\ntransformers in real-world image super-resolution. In our proposed approach, we\nintegrate a semantic-aware discriminator to reconstruct fine details more\naccurately and employ an adaptive degradation model to better simulate\nreal-world degradations. Moreover, we introduce a new combination of loss\nfunctions by adding wavelet loss to loss functions of GAN-based models to\nbetter recover high-frequency details. Empirical results demonstrate that\nIG-CFAT significantly outperforms existing SOTA models in both quantitative and\nqualitative metrics. Our proposed model revolutionizes the field of real-world\nimage super-resolution and demonstrates substantially better performance in\nrecovering fine details and generating realistic textures. The introduction of\nIG-CFAT offers a robust and adaptable solution for real-world image\nsuper-resolution tasks.\n", "link": "http://arxiv.org/abs/2406.13815v3", "date": "2024-11-26", "relevancy": 2.3367, "topK": [{"title": "Scaling Rectified Flow Transformers for High-Resolution Image Synthesis", "link": "http://arxiv.org/abs/2403.03206v1", "similarity": 0.649}, {"title": "GST: Precise 3D Human Body from a Single Image with Gaussian Splatting\n  Transformers", "link": "http://arxiv.org/abs/2409.04196v1", "similarity": 0.5739}, {"title": "ToDo: Token Downsampling for Efficient Generation of High-Resolution\n  Images", "link": "http://arxiv.org/abs/2402.13573v2", "similarity": 0.5685}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20IG-CFAT%3A%20An%20Improved%20GAN-Based%20Framework%20for%20Effectively%20Exploiting%0A%20%20Transformers%20in%20Real-World%20Image%20Super-Resolution&body=Title%3A%20IG-CFAT%3A%20An%20Improved%20GAN-Based%20Framework%20for%20Effectively%20Exploiting%0A%20%20Transformers%20in%20Real-World%20Image%20Super-Resolution%0AAuthor%3A%20Alireza%20Aghelan%20and%20Ali%20Amiryan%20and%20Abolfazl%20Zarghani%20and%20Modjtaba%20Rouhani%0AAbstract%3A%20%20%20In%20the%20field%20of%20single%20image%20super-resolution%20%28SISR%29%2C%20transformer-based%0Amodels%2C%20have%20demonstrated%20significant%20advancements.%20However%2C%20the%20potential%20and%0Aefficiency%20of%20these%20models%20in%20applied%20fields%20such%20as%20real-world%20image%0Asuper-resolution%20have%20been%20less%20noticed%20and%20there%20are%20substantial%20opportunities%0Afor%20improvement.%20Recently%2C%20composite%20fusion%20attention%20transformer%20%28CFAT%29%2C%0Aoutperformed%20previous%20state-of-the-art%20%28SOTA%29%20models%20in%20classic%20image%0Asuper-resolution.%20In%20this%20paper%2C%20we%20propose%20a%20novel%20GAN-based%20framework%20by%0Aincorporating%20the%20CFAT%20model%20to%20effectively%20exploit%20the%20performance%20of%0Atransformers%20in%20real-world%20image%20super-resolution.%20In%20our%20proposed%20approach%2C%20we%0Aintegrate%20a%20semantic-aware%20discriminator%20to%20reconstruct%20fine%20details%20more%0Aaccurately%20and%20employ%20an%20adaptive%20degradation%20model%20to%20better%20simulate%0Areal-world%20degradations.%20Moreover%2C%20we%20introduce%20a%20new%20combination%20of%20loss%0Afunctions%20by%20adding%20wavelet%20loss%20to%20loss%20functions%20of%20GAN-based%20models%20to%0Abetter%20recover%20high-frequency%20details.%20Empirical%20results%20demonstrate%20that%0AIG-CFAT%20significantly%20outperforms%20existing%20SOTA%20models%20in%20both%20quantitative%20and%0Aqualitative%20metrics.%20Our%20proposed%20model%20revolutionizes%20the%20field%20of%20real-world%0Aimage%20super-resolution%20and%20demonstrates%20substantially%20better%20performance%20in%0Arecovering%20fine%20details%20and%20generating%20realistic%20textures.%20The%20introduction%20of%0AIG-CFAT%20offers%20a%20robust%20and%20adaptable%20solution%20for%20real-world%20image%0Asuper-resolution%20tasks.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2406.13815v3%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DIG-CFAT%253A%2520An%2520Improved%2520GAN-Based%2520Framework%2520for%2520Effectively%2520Exploiting%250A%2520%2520Transformers%2520in%2520Real-World%2520Image%2520Super-Resolution%26entry.906535625%3DAlireza%2520Aghelan%2520and%2520Ali%2520Amiryan%2520and%2520Abolfazl%2520Zarghani%2520and%2520Modjtaba%2520Rouhani%26entry.1292438233%3D%2520%2520In%2520the%2520field%2520of%2520single%2520image%2520super-resolution%2520%2528SISR%2529%252C%2520transformer-based%250Amodels%252C%2520have%2520demonstrated%2520significant%2520advancements.%2520However%252C%2520the%2520potential%2520and%250Aefficiency%2520of%2520these%2520models%2520in%2520applied%2520fields%2520such%2520as%2520real-world%2520image%250Asuper-resolution%2520have%2520been%2520less%2520noticed%2520and%2520there%2520are%2520substantial%2520opportunities%250Afor%2520improvement.%2520Recently%252C%2520composite%2520fusion%2520attention%2520transformer%2520%2528CFAT%2529%252C%250Aoutperformed%2520previous%2520state-of-the-art%2520%2528SOTA%2529%2520models%2520in%2520classic%2520image%250Asuper-resolution.%2520In%2520this%2520paper%252C%2520we%2520propose%2520a%2520novel%2520GAN-based%2520framework%2520by%250Aincorporating%2520the%2520CFAT%2520model%2520to%2520effectively%2520exploit%2520the%2520performance%2520of%250Atransformers%2520in%2520real-world%2520image%2520super-resolution.%2520In%2520our%2520proposed%2520approach%252C%2520we%250Aintegrate%2520a%2520semantic-aware%2520discriminator%2520to%2520reconstruct%2520fine%2520details%2520more%250Aaccurately%2520and%2520employ%2520an%2520adaptive%2520degradation%2520model%2520to%2520better%2520simulate%250Areal-world%2520degradations.%2520Moreover%252C%2520we%2520introduce%2520a%2520new%2520combination%2520of%2520loss%250Afunctions%2520by%2520adding%2520wavelet%2520loss%2520to%2520loss%2520functions%2520of%2520GAN-based%2520models%2520to%250Abetter%2520recover%2520high-frequency%2520details.%2520Empirical%2520results%2520demonstrate%2520that%250AIG-CFAT%2520significantly%2520outperforms%2520existing%2520SOTA%2520models%2520in%2520both%2520quantitative%2520and%250Aqualitative%2520metrics.%2520Our%2520proposed%2520model%2520revolutionizes%2520the%2520field%2520of%2520real-world%250Aimage%2520super-resolution%2520and%2520demonstrates%2520substantially%2520better%2520performance%2520in%250Arecovering%2520fine%2520details%2520and%2520generating%2520realistic%2520textures.%2520The%2520introduction%2520of%250AIG-CFAT%2520offers%2520a%2520robust%2520and%2520adaptable%2520solution%2520for%2520real-world%2520image%250Asuper-resolution%2520tasks.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2406.13815v3%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=IG-CFAT%3A%20An%20Improved%20GAN-Based%20Framework%20for%20Effectively%20Exploiting%0A%20%20Transformers%20in%20Real-World%20Image%20Super-Resolution&entry.906535625=Alireza%20Aghelan%20and%20Ali%20Amiryan%20and%20Abolfazl%20Zarghani%20and%20Modjtaba%20Rouhani&entry.1292438233=%20%20In%20the%20field%20of%20single%20image%20super-resolution%20%28SISR%29%2C%20transformer-based%0Amodels%2C%20have%20demonstrated%20significant%20advancements.%20However%2C%20the%20potential%20and%0Aefficiency%20of%20these%20models%20in%20applied%20fields%20such%20as%20real-world%20image%0Asuper-resolution%20have%20been%20less%20noticed%20and%20there%20are%20substantial%20opportunities%0Afor%20improvement.%20Recently%2C%20composite%20fusion%20attention%20transformer%20%28CFAT%29%2C%0Aoutperformed%20previous%20state-of-the-art%20%28SOTA%29%20models%20in%20classic%20image%0Asuper-resolution.%20In%20this%20paper%2C%20we%20propose%20a%20novel%20GAN-based%20framework%20by%0Aincorporating%20the%20CFAT%20model%20to%20effectively%20exploit%20the%20performance%20of%0Atransformers%20in%20real-world%20image%20super-resolution.%20In%20our%20proposed%20approach%2C%20we%0Aintegrate%20a%20semantic-aware%20discriminator%20to%20reconstruct%20fine%20details%20more%0Aaccurately%20and%20employ%20an%20adaptive%20degradation%20model%20to%20better%20simulate%0Areal-world%20degradations.%20Moreover%2C%20we%20introduce%20a%20new%20combination%20of%20loss%0Afunctions%20by%20adding%20wavelet%20loss%20to%20loss%20functions%20of%20GAN-based%20models%20to%0Abetter%20recover%20high-frequency%20details.%20Empirical%20results%20demonstrate%20that%0AIG-CFAT%20significantly%20outperforms%20existing%20SOTA%20models%20in%20both%20quantitative%20and%0Aqualitative%20metrics.%20Our%20proposed%20model%20revolutionizes%20the%20field%20of%20real-world%0Aimage%20super-resolution%20and%20demonstrates%20substantially%20better%20performance%20in%0Arecovering%20fine%20details%20and%20generating%20realistic%20textures.%20The%20introduction%20of%0AIG-CFAT%20offers%20a%20robust%20and%20adaptable%20solution%20for%20real-world%20image%0Asuper-resolution%20tasks.%0A&entry.1838667208=http%3A//arxiv.org/abs/2406.13815v3&entry.124074799=Read"},
{"title": "Natural Language Understanding and Inference with MLLM in Visual\n  Question Answering: A Survey", "author": "Jiayi Kuang and Jingyou Xie and Haohao Luo and Ronghao Li and Zhe Xu and Xianfeng Cheng and Yinghui Li and Xika Lin and Ying Shen", "abstract": "  Visual Question Answering (VQA) is a challenge task that combines natural\nlanguage processing and computer vision techniques and gradually becomes a\nbenchmark test task in multimodal large language models (MLLMs). The goal of\nour survey is to provide an overview of the development of VQA and a detailed\ndescription of the latest models with high timeliness. This survey gives an\nup-to-date synthesis of natural language understanding of images and text, as\nwell as the knowledge reasoning module based on image-question information on\nthe core VQA tasks. In addition, we elaborate on recent advances in extracting\nand fusing modal information with vision-language pretraining models and\nmultimodal large language models in VQA. We also exhaustively review the\nprogress of knowledge reasoning in VQA by detailing the extraction of internal\nknowledge and the introduction of external knowledge. Finally, we present the\ndatasets of VQA and different evaluation metrics and discuss possible\ndirections for future work.\n", "link": "http://arxiv.org/abs/2411.17558v1", "date": "2024-11-26", "relevancy": 2.3343, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5956}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5956}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.5236}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Natural%20Language%20Understanding%20and%20Inference%20with%20MLLM%20in%20Visual%0A%20%20Question%20Answering%3A%20A%20Survey&body=Title%3A%20Natural%20Language%20Understanding%20and%20Inference%20with%20MLLM%20in%20Visual%0A%20%20Question%20Answering%3A%20A%20Survey%0AAuthor%3A%20Jiayi%20Kuang%20and%20Jingyou%20Xie%20and%20Haohao%20Luo%20and%20Ronghao%20Li%20and%20Zhe%20Xu%20and%20Xianfeng%20Cheng%20and%20Yinghui%20Li%20and%20Xika%20Lin%20and%20Ying%20Shen%0AAbstract%3A%20%20%20Visual%20Question%20Answering%20%28VQA%29%20is%20a%20challenge%20task%20that%20combines%20natural%0Alanguage%20processing%20and%20computer%20vision%20techniques%20and%20gradually%20becomes%20a%0Abenchmark%20test%20task%20in%20multimodal%20large%20language%20models%20%28MLLMs%29.%20The%20goal%20of%0Aour%20survey%20is%20to%20provide%20an%20overview%20of%20the%20development%20of%20VQA%20and%20a%20detailed%0Adescription%20of%20the%20latest%20models%20with%20high%20timeliness.%20This%20survey%20gives%20an%0Aup-to-date%20synthesis%20of%20natural%20language%20understanding%20of%20images%20and%20text%2C%20as%0Awell%20as%20the%20knowledge%20reasoning%20module%20based%20on%20image-question%20information%20on%0Athe%20core%20VQA%20tasks.%20In%20addition%2C%20we%20elaborate%20on%20recent%20advances%20in%20extracting%0Aand%20fusing%20modal%20information%20with%20vision-language%20pretraining%20models%20and%0Amultimodal%20large%20language%20models%20in%20VQA.%20We%20also%20exhaustively%20review%20the%0Aprogress%20of%20knowledge%20reasoning%20in%20VQA%20by%20detailing%20the%20extraction%20of%20internal%0Aknowledge%20and%20the%20introduction%20of%20external%20knowledge.%20Finally%2C%20we%20present%20the%0Adatasets%20of%20VQA%20and%20different%20evaluation%20metrics%20and%20discuss%20possible%0Adirections%20for%20future%20work.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2411.17558v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DNatural%2520Language%2520Understanding%2520and%2520Inference%2520with%2520MLLM%2520in%2520Visual%250A%2520%2520Question%2520Answering%253A%2520A%2520Survey%26entry.906535625%3DJiayi%2520Kuang%2520and%2520Jingyou%2520Xie%2520and%2520Haohao%2520Luo%2520and%2520Ronghao%2520Li%2520and%2520Zhe%2520Xu%2520and%2520Xianfeng%2520Cheng%2520and%2520Yinghui%2520Li%2520and%2520Xika%2520Lin%2520and%2520Ying%2520Shen%26entry.1292438233%3D%2520%2520Visual%2520Question%2520Answering%2520%2528VQA%2529%2520is%2520a%2520challenge%2520task%2520that%2520combines%2520natural%250Alanguage%2520processing%2520and%2520computer%2520vision%2520techniques%2520and%2520gradually%2520becomes%2520a%250Abenchmark%2520test%2520task%2520in%2520multimodal%2520large%2520language%2520models%2520%2528MLLMs%2529.%2520The%2520goal%2520of%250Aour%2520survey%2520is%2520to%2520provide%2520an%2520overview%2520of%2520the%2520development%2520of%2520VQA%2520and%2520a%2520detailed%250Adescription%2520of%2520the%2520latest%2520models%2520with%2520high%2520timeliness.%2520This%2520survey%2520gives%2520an%250Aup-to-date%2520synthesis%2520of%2520natural%2520language%2520understanding%2520of%2520images%2520and%2520text%252C%2520as%250Awell%2520as%2520the%2520knowledge%2520reasoning%2520module%2520based%2520on%2520image-question%2520information%2520on%250Athe%2520core%2520VQA%2520tasks.%2520In%2520addition%252C%2520we%2520elaborate%2520on%2520recent%2520advances%2520in%2520extracting%250Aand%2520fusing%2520modal%2520information%2520with%2520vision-language%2520pretraining%2520models%2520and%250Amultimodal%2520large%2520language%2520models%2520in%2520VQA.%2520We%2520also%2520exhaustively%2520review%2520the%250Aprogress%2520of%2520knowledge%2520reasoning%2520in%2520VQA%2520by%2520detailing%2520the%2520extraction%2520of%2520internal%250Aknowledge%2520and%2520the%2520introduction%2520of%2520external%2520knowledge.%2520Finally%252C%2520we%2520present%2520the%250Adatasets%2520of%2520VQA%2520and%2520different%2520evaluation%2520metrics%2520and%2520discuss%2520possible%250Adirections%2520for%2520future%2520work.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2411.17558v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Natural%20Language%20Understanding%20and%20Inference%20with%20MLLM%20in%20Visual%0A%20%20Question%20Answering%3A%20A%20Survey&entry.906535625=Jiayi%20Kuang%20and%20Jingyou%20Xie%20and%20Haohao%20Luo%20and%20Ronghao%20Li%20and%20Zhe%20Xu%20and%20Xianfeng%20Cheng%20and%20Yinghui%20Li%20and%20Xika%20Lin%20and%20Ying%20Shen&entry.1292438233=%20%20Visual%20Question%20Answering%20%28VQA%29%20is%20a%20challenge%20task%20that%20combines%20natural%0Alanguage%20processing%20and%20computer%20vision%20techniques%20and%20gradually%20becomes%20a%0Abenchmark%20test%20task%20in%20multimodal%20large%20language%20models%20%28MLLMs%29.%20The%20goal%20of%0Aour%20survey%20is%20to%20provide%20an%20overview%20of%20the%20development%20of%20VQA%20and%20a%20detailed%0Adescription%20of%20the%20latest%20models%20with%20high%20timeliness.%20This%20survey%20gives%20an%0Aup-to-date%20synthesis%20of%20natural%20language%20understanding%20of%20images%20and%20text%2C%20as%0Awell%20as%20the%20knowledge%20reasoning%20module%20based%20on%20image-question%20information%20on%0Athe%20core%20VQA%20tasks.%20In%20addition%2C%20we%20elaborate%20on%20recent%20advances%20in%20extracting%0Aand%20fusing%20modal%20information%20with%20vision-language%20pretraining%20models%20and%0Amultimodal%20large%20language%20models%20in%20VQA.%20We%20also%20exhaustively%20review%20the%0Aprogress%20of%20knowledge%20reasoning%20in%20VQA%20by%20detailing%20the%20extraction%20of%20internal%0Aknowledge%20and%20the%20introduction%20of%20external%20knowledge.%20Finally%2C%20we%20present%20the%0Adatasets%20of%20VQA%20and%20different%20evaluation%20metrics%20and%20discuss%20possible%0Adirections%20for%20future%20work.%0A&entry.1838667208=http%3A//arxiv.org/abs/2411.17558v1&entry.124074799=Read"},
{"title": "CliquePH: Higher-Order Information for Graph Neural Networks through\n  Persistent Homology on Clique Graphs", "author": "Davide Buffelli and Farzin Soleymani and Bastian Rieck", "abstract": "  Graph neural networks have become the default choice by practitioners for\ngraph learning tasks such as graph classification and node classification.\nNevertheless, popular graph neural network models still struggle to capture\nhigher-order information, i.e., information that goes \\emph{beyond} pairwise\ninteractions. Recent work has shown that persistent homology, a tool from\ntopological data analysis, can enrich graph neural networks with topological\ninformation that they otherwise could not capture. Calculating such features is\nefficient for dimension 0 (connected components) and dimension 1 (cycles).\nHowever, when it comes to higher-order structures, it does not scale well, with\na complexity of $O(n^d)$, where $n$ is the number of nodes and $d$ is the order\nof the structures. In this work, we introduce a novel method that extracts\ninformation about higher-order structures in the graph while still using the\nefficient low-dimensional persistent homology algorithm. On standard benchmark\ndatasets, we show that our method can lead to up to $31\\%$ improvements in test\naccuracy.\n", "link": "http://arxiv.org/abs/2409.08217v2", "date": "2024-11-26", "relevancy": 2.3152, "topK": [{"title": "Self-supervised Photographic Image Layout Representation Learning", "link": "http://arxiv.org/abs/2403.03740v1", "similarity": 0.481}, {"title": "Boosted dynamic neural networks", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/26302", "similarity": 0.4556}, {"title": "Dynamic Gaussians Mesh: Consistent Mesh Reconstruction from Monocular\n  Videos", "link": "http://arxiv.org/abs/2404.12379v2", "similarity": 0.4525}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20CliquePH%3A%20Higher-Order%20Information%20for%20Graph%20Neural%20Networks%20through%0A%20%20Persistent%20Homology%20on%20Clique%20Graphs&body=Title%3A%20CliquePH%3A%20Higher-Order%20Information%20for%20Graph%20Neural%20Networks%20through%0A%20%20Persistent%20Homology%20on%20Clique%20Graphs%0AAuthor%3A%20Davide%20Buffelli%20and%20Farzin%20Soleymani%20and%20Bastian%20Rieck%0AAbstract%3A%20%20%20Graph%20neural%20networks%20have%20become%20the%20default%20choice%20by%20practitioners%20for%0Agraph%20learning%20tasks%20such%20as%20graph%20classification%20and%20node%20classification.%0ANevertheless%2C%20popular%20graph%20neural%20network%20models%20still%20struggle%20to%20capture%0Ahigher-order%20information%2C%20i.e.%2C%20information%20that%20goes%20%5Cemph%7Bbeyond%7D%20pairwise%0Ainteractions.%20Recent%20work%20has%20shown%20that%20persistent%20homology%2C%20a%20tool%20from%0Atopological%20data%20analysis%2C%20can%20enrich%20graph%20neural%20networks%20with%20topological%0Ainformation%20that%20they%20otherwise%20could%20not%20capture.%20Calculating%20such%20features%20is%0Aefficient%20for%20dimension%200%20%28connected%20components%29%20and%20dimension%201%20%28cycles%29.%0AHowever%2C%20when%20it%20comes%20to%20higher-order%20structures%2C%20it%20does%20not%20scale%20well%2C%20with%0Aa%20complexity%20of%20%24O%28n%5Ed%29%24%2C%20where%20%24n%24%20is%20the%20number%20of%20nodes%20and%20%24d%24%20is%20the%20order%0Aof%20the%20structures.%20In%20this%20work%2C%20we%20introduce%20a%20novel%20method%20that%20extracts%0Ainformation%20about%20higher-order%20structures%20in%20the%20graph%20while%20still%20using%20the%0Aefficient%20low-dimensional%20persistent%20homology%20algorithm.%20On%20standard%20benchmark%0Adatasets%2C%20we%20show%20that%20our%20method%20can%20lead%20to%20up%20to%20%2431%5C%25%24%20improvements%20in%20test%0Aaccuracy.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2409.08217v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DCliquePH%253A%2520Higher-Order%2520Information%2520for%2520Graph%2520Neural%2520Networks%2520through%250A%2520%2520Persistent%2520Homology%2520on%2520Clique%2520Graphs%26entry.906535625%3DDavide%2520Buffelli%2520and%2520Farzin%2520Soleymani%2520and%2520Bastian%2520Rieck%26entry.1292438233%3D%2520%2520Graph%2520neural%2520networks%2520have%2520become%2520the%2520default%2520choice%2520by%2520practitioners%2520for%250Agraph%2520learning%2520tasks%2520such%2520as%2520graph%2520classification%2520and%2520node%2520classification.%250ANevertheless%252C%2520popular%2520graph%2520neural%2520network%2520models%2520still%2520struggle%2520to%2520capture%250Ahigher-order%2520information%252C%2520i.e.%252C%2520information%2520that%2520goes%2520%255Cemph%257Bbeyond%257D%2520pairwise%250Ainteractions.%2520Recent%2520work%2520has%2520shown%2520that%2520persistent%2520homology%252C%2520a%2520tool%2520from%250Atopological%2520data%2520analysis%252C%2520can%2520enrich%2520graph%2520neural%2520networks%2520with%2520topological%250Ainformation%2520that%2520they%2520otherwise%2520could%2520not%2520capture.%2520Calculating%2520such%2520features%2520is%250Aefficient%2520for%2520dimension%25200%2520%2528connected%2520components%2529%2520and%2520dimension%25201%2520%2528cycles%2529.%250AHowever%252C%2520when%2520it%2520comes%2520to%2520higher-order%2520structures%252C%2520it%2520does%2520not%2520scale%2520well%252C%2520with%250Aa%2520complexity%2520of%2520%2524O%2528n%255Ed%2529%2524%252C%2520where%2520%2524n%2524%2520is%2520the%2520number%2520of%2520nodes%2520and%2520%2524d%2524%2520is%2520the%2520order%250Aof%2520the%2520structures.%2520In%2520this%2520work%252C%2520we%2520introduce%2520a%2520novel%2520method%2520that%2520extracts%250Ainformation%2520about%2520higher-order%2520structures%2520in%2520the%2520graph%2520while%2520still%2520using%2520the%250Aefficient%2520low-dimensional%2520persistent%2520homology%2520algorithm.%2520On%2520standard%2520benchmark%250Adatasets%252C%2520we%2520show%2520that%2520our%2520method%2520can%2520lead%2520to%2520up%2520to%2520%252431%255C%2525%2524%2520improvements%2520in%2520test%250Aaccuracy.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2409.08217v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=CliquePH%3A%20Higher-Order%20Information%20for%20Graph%20Neural%20Networks%20through%0A%20%20Persistent%20Homology%20on%20Clique%20Graphs&entry.906535625=Davide%20Buffelli%20and%20Farzin%20Soleymani%20and%20Bastian%20Rieck&entry.1292438233=%20%20Graph%20neural%20networks%20have%20become%20the%20default%20choice%20by%20practitioners%20for%0Agraph%20learning%20tasks%20such%20as%20graph%20classification%20and%20node%20classification.%0ANevertheless%2C%20popular%20graph%20neural%20network%20models%20still%20struggle%20to%20capture%0Ahigher-order%20information%2C%20i.e.%2C%20information%20that%20goes%20%5Cemph%7Bbeyond%7D%20pairwise%0Ainteractions.%20Recent%20work%20has%20shown%20that%20persistent%20homology%2C%20a%20tool%20from%0Atopological%20data%20analysis%2C%20can%20enrich%20graph%20neural%20networks%20with%20topological%0Ainformation%20that%20they%20otherwise%20could%20not%20capture.%20Calculating%20such%20features%20is%0Aefficient%20for%20dimension%200%20%28connected%20components%29%20and%20dimension%201%20%28cycles%29.%0AHowever%2C%20when%20it%20comes%20to%20higher-order%20structures%2C%20it%20does%20not%20scale%20well%2C%20with%0Aa%20complexity%20of%20%24O%28n%5Ed%29%24%2C%20where%20%24n%24%20is%20the%20number%20of%20nodes%20and%20%24d%24%20is%20the%20order%0Aof%20the%20structures.%20In%20this%20work%2C%20we%20introduce%20a%20novel%20method%20that%20extracts%0Ainformation%20about%20higher-order%20structures%20in%20the%20graph%20while%20still%20using%20the%0Aefficient%20low-dimensional%20persistent%20homology%20algorithm.%20On%20standard%20benchmark%0Adatasets%2C%20we%20show%20that%20our%20method%20can%20lead%20to%20up%20to%20%2431%5C%25%24%20improvements%20in%20test%0Aaccuracy.%0A&entry.1838667208=http%3A//arxiv.org/abs/2409.08217v2&entry.124074799=Read"},
{"title": "VIRES: Video Instance Repainting with Sketch and Text Guidance", "author": "Shuchen Weng and Haojie Zheng and Peixuan Zhan and Yuchen Hong and Han Jiang and Si Li and Boxin Shi", "abstract": "  We introduce VIRES, a video instance repainting method with sketch and text\nguidance, enabling video instance repainting, replacement, generation, and\nremoval. Existing approaches struggle with temporal consistency and accurate\nalignment with the provided sketch sequence. VIRES leverages the generative\npriors of text-to-video models to maintain temporal consistency and produce\nvisually pleasing results. We propose the Sequential ControlNet with the\nstandardized self-scaling, which effectively extracts structure layouts and\nadaptively captures high-contrast sketch details. We further augment the\ndiffusion transformer backbone with the sketch attention to interpret and\ninject fine-grained sketch semantics. A sketch-aware encoder ensures that\nrepainted results are aligned with the provided sketch sequence. Additionally,\nwe contribute the VireSet, a dataset with detailed annotations tailored for\ntraining and evaluating video instance editing methods. Experimental results\ndemonstrate the effectiveness of VIRES, which outperforms state-of-the-art\nmethods in visual quality, temporal consistency, condition alignment, and human\nratings. Project page:https://suimuc.github.io/suimu.github.io/projects/VIRES/\n", "link": "http://arxiv.org/abs/2411.16199v2", "date": "2024-11-26", "relevancy": 2.3127, "topK": [{"title": "Magic-Me: Identity-Specific Video Customized Diffusion", "link": "http://arxiv.org/abs/2402.09368v1", "similarity": 0.583}, {"title": "Customize-A-Video: One-Shot Motion Customization of Text-to-Video\n  Diffusion Models", "link": "http://arxiv.org/abs/2402.14780v1", "similarity": 0.58}, {"title": "Tuning-Free Noise Rectification for High Fidelity Image-to-Video\n  Generation", "link": "http://arxiv.org/abs/2403.02827v1", "similarity": 0.5726}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20VIRES%3A%20Video%20Instance%20Repainting%20with%20Sketch%20and%20Text%20Guidance&body=Title%3A%20VIRES%3A%20Video%20Instance%20Repainting%20with%20Sketch%20and%20Text%20Guidance%0AAuthor%3A%20Shuchen%20Weng%20and%20Haojie%20Zheng%20and%20Peixuan%20Zhan%20and%20Yuchen%20Hong%20and%20Han%20Jiang%20and%20Si%20Li%20and%20Boxin%20Shi%0AAbstract%3A%20%20%20We%20introduce%20VIRES%2C%20a%20video%20instance%20repainting%20method%20with%20sketch%20and%20text%0Aguidance%2C%20enabling%20video%20instance%20repainting%2C%20replacement%2C%20generation%2C%20and%0Aremoval.%20Existing%20approaches%20struggle%20with%20temporal%20consistency%20and%20accurate%0Aalignment%20with%20the%20provided%20sketch%20sequence.%20VIRES%20leverages%20the%20generative%0Apriors%20of%20text-to-video%20models%20to%20maintain%20temporal%20consistency%20and%20produce%0Avisually%20pleasing%20results.%20We%20propose%20the%20Sequential%20ControlNet%20with%20the%0Astandardized%20self-scaling%2C%20which%20effectively%20extracts%20structure%20layouts%20and%0Aadaptively%20captures%20high-contrast%20sketch%20details.%20We%20further%20augment%20the%0Adiffusion%20transformer%20backbone%20with%20the%20sketch%20attention%20to%20interpret%20and%0Ainject%20fine-grained%20sketch%20semantics.%20A%20sketch-aware%20encoder%20ensures%20that%0Arepainted%20results%20are%20aligned%20with%20the%20provided%20sketch%20sequence.%20Additionally%2C%0Awe%20contribute%20the%20VireSet%2C%20a%20dataset%20with%20detailed%20annotations%20tailored%20for%0Atraining%20and%20evaluating%20video%20instance%20editing%20methods.%20Experimental%20results%0Ademonstrate%20the%20effectiveness%20of%20VIRES%2C%20which%20outperforms%20state-of-the-art%0Amethods%20in%20visual%20quality%2C%20temporal%20consistency%2C%20condition%20alignment%2C%20and%20human%0Aratings.%20Project%20page%3Ahttps%3A//suimuc.github.io/suimu.github.io/projects/VIRES/%0A%0ALink%3A%20http%3A//arxiv.org/abs/2411.16199v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DVIRES%253A%2520Video%2520Instance%2520Repainting%2520with%2520Sketch%2520and%2520Text%2520Guidance%26entry.906535625%3DShuchen%2520Weng%2520and%2520Haojie%2520Zheng%2520and%2520Peixuan%2520Zhan%2520and%2520Yuchen%2520Hong%2520and%2520Han%2520Jiang%2520and%2520Si%2520Li%2520and%2520Boxin%2520Shi%26entry.1292438233%3D%2520%2520We%2520introduce%2520VIRES%252C%2520a%2520video%2520instance%2520repainting%2520method%2520with%2520sketch%2520and%2520text%250Aguidance%252C%2520enabling%2520video%2520instance%2520repainting%252C%2520replacement%252C%2520generation%252C%2520and%250Aremoval.%2520Existing%2520approaches%2520struggle%2520with%2520temporal%2520consistency%2520and%2520accurate%250Aalignment%2520with%2520the%2520provided%2520sketch%2520sequence.%2520VIRES%2520leverages%2520the%2520generative%250Apriors%2520of%2520text-to-video%2520models%2520to%2520maintain%2520temporal%2520consistency%2520and%2520produce%250Avisually%2520pleasing%2520results.%2520We%2520propose%2520the%2520Sequential%2520ControlNet%2520with%2520the%250Astandardized%2520self-scaling%252C%2520which%2520effectively%2520extracts%2520structure%2520layouts%2520and%250Aadaptively%2520captures%2520high-contrast%2520sketch%2520details.%2520We%2520further%2520augment%2520the%250Adiffusion%2520transformer%2520backbone%2520with%2520the%2520sketch%2520attention%2520to%2520interpret%2520and%250Ainject%2520fine-grained%2520sketch%2520semantics.%2520A%2520sketch-aware%2520encoder%2520ensures%2520that%250Arepainted%2520results%2520are%2520aligned%2520with%2520the%2520provided%2520sketch%2520sequence.%2520Additionally%252C%250Awe%2520contribute%2520the%2520VireSet%252C%2520a%2520dataset%2520with%2520detailed%2520annotations%2520tailored%2520for%250Atraining%2520and%2520evaluating%2520video%2520instance%2520editing%2520methods.%2520Experimental%2520results%250Ademonstrate%2520the%2520effectiveness%2520of%2520VIRES%252C%2520which%2520outperforms%2520state-of-the-art%250Amethods%2520in%2520visual%2520quality%252C%2520temporal%2520consistency%252C%2520condition%2520alignment%252C%2520and%2520human%250Aratings.%2520Project%2520page%253Ahttps%253A//suimuc.github.io/suimu.github.io/projects/VIRES/%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2411.16199v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=VIRES%3A%20Video%20Instance%20Repainting%20with%20Sketch%20and%20Text%20Guidance&entry.906535625=Shuchen%20Weng%20and%20Haojie%20Zheng%20and%20Peixuan%20Zhan%20and%20Yuchen%20Hong%20and%20Han%20Jiang%20and%20Si%20Li%20and%20Boxin%20Shi&entry.1292438233=%20%20We%20introduce%20VIRES%2C%20a%20video%20instance%20repainting%20method%20with%20sketch%20and%20text%0Aguidance%2C%20enabling%20video%20instance%20repainting%2C%20replacement%2C%20generation%2C%20and%0Aremoval.%20Existing%20approaches%20struggle%20with%20temporal%20consistency%20and%20accurate%0Aalignment%20with%20the%20provided%20sketch%20sequence.%20VIRES%20leverages%20the%20generative%0Apriors%20of%20text-to-video%20models%20to%20maintain%20temporal%20consistency%20and%20produce%0Avisually%20pleasing%20results.%20We%20propose%20the%20Sequential%20ControlNet%20with%20the%0Astandardized%20self-scaling%2C%20which%20effectively%20extracts%20structure%20layouts%20and%0Aadaptively%20captures%20high-contrast%20sketch%20details.%20We%20further%20augment%20the%0Adiffusion%20transformer%20backbone%20with%20the%20sketch%20attention%20to%20interpret%20and%0Ainject%20fine-grained%20sketch%20semantics.%20A%20sketch-aware%20encoder%20ensures%20that%0Arepainted%20results%20are%20aligned%20with%20the%20provided%20sketch%20sequence.%20Additionally%2C%0Awe%20contribute%20the%20VireSet%2C%20a%20dataset%20with%20detailed%20annotations%20tailored%20for%0Atraining%20and%20evaluating%20video%20instance%20editing%20methods.%20Experimental%20results%0Ademonstrate%20the%20effectiveness%20of%20VIRES%2C%20which%20outperforms%20state-of-the-art%0Amethods%20in%20visual%20quality%2C%20temporal%20consistency%2C%20condition%20alignment%2C%20and%20human%0Aratings.%20Project%20page%3Ahttps%3A//suimuc.github.io/suimu.github.io/projects/VIRES/%0A&entry.1838667208=http%3A//arxiv.org/abs/2411.16199v2&entry.124074799=Read"},
{"title": "TDAvec: Computing Vector Summaries of Persistence Diagrams for\n  Topological Data Analysis in R and Python", "author": "Aleksei Luchinsky and Umar Islambekov", "abstract": "  Persistent homology is a widely-used tool in topological data analysis (TDA)\nfor understanding the underlying shape of complex data. By constructing a\nfiltration of simplicial complexes from data points, it captures topological\nfeatures such as connected components, loops, and voids across multiple scales.\nThese features are encoded in persistence diagrams (PDs), which provide a\nconcise summary of the data's topological structure. However, the non-Hilbert\nnature of the space of PDs poses challenges for their direct use in machine\nlearning applications. To address this, kernel methods and vectorization\ntechniques have been developed to transform PDs into\nmachine-learning-compatible formats. In this paper, we introduce a new software\npackage designed to streamline the vectorization of PDs, offering an intuitive\nworkflow and advanced functionalities. We demonstrate the necessity of the\npackage through practical examples and provide a detailed discussion on its\ncontributions to applied TDA. Definitions of all vectorization summaries used\nin the package are included in the appendix.\n", "link": "http://arxiv.org/abs/2411.17340v1", "date": "2024-11-26", "relevancy": 2.295, "topK": [{"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.4697}, {"title": "CAT3D: Create Anything in 3D with Multi-View Diffusion Models", "link": "http://arxiv.org/abs/2405.10314v1", "similarity": 0.4536}, {"title": "CAT3D: Create Anything in 3D with Multi-View Diffusion Models", "link": "http://arxiv.org/abs/2405.10314v1", "similarity": 0.4536}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20TDAvec%3A%20Computing%20Vector%20Summaries%20of%20Persistence%20Diagrams%20for%0A%20%20Topological%20Data%20Analysis%20in%20R%20and%20Python&body=Title%3A%20TDAvec%3A%20Computing%20Vector%20Summaries%20of%20Persistence%20Diagrams%20for%0A%20%20Topological%20Data%20Analysis%20in%20R%20and%20Python%0AAuthor%3A%20Aleksei%20Luchinsky%20and%20Umar%20Islambekov%0AAbstract%3A%20%20%20Persistent%20homology%20is%20a%20widely-used%20tool%20in%20topological%20data%20analysis%20%28TDA%29%0Afor%20understanding%20the%20underlying%20shape%20of%20complex%20data.%20By%20constructing%20a%0Afiltration%20of%20simplicial%20complexes%20from%20data%20points%2C%20it%20captures%20topological%0Afeatures%20such%20as%20connected%20components%2C%20loops%2C%20and%20voids%20across%20multiple%20scales.%0AThese%20features%20are%20encoded%20in%20persistence%20diagrams%20%28PDs%29%2C%20which%20provide%20a%0Aconcise%20summary%20of%20the%20data%27s%20topological%20structure.%20However%2C%20the%20non-Hilbert%0Anature%20of%20the%20space%20of%20PDs%20poses%20challenges%20for%20their%20direct%20use%20in%20machine%0Alearning%20applications.%20To%20address%20this%2C%20kernel%20methods%20and%20vectorization%0Atechniques%20have%20been%20developed%20to%20transform%20PDs%20into%0Amachine-learning-compatible%20formats.%20In%20this%20paper%2C%20we%20introduce%20a%20new%20software%0Apackage%20designed%20to%20streamline%20the%20vectorization%20of%20PDs%2C%20offering%20an%20intuitive%0Aworkflow%20and%20advanced%20functionalities.%20We%20demonstrate%20the%20necessity%20of%20the%0Apackage%20through%20practical%20examples%20and%20provide%20a%20detailed%20discussion%20on%20its%0Acontributions%20to%20applied%20TDA.%20Definitions%20of%20all%20vectorization%20summaries%20used%0Ain%20the%20package%20are%20included%20in%20the%20appendix.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2411.17340v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DTDAvec%253A%2520Computing%2520Vector%2520Summaries%2520of%2520Persistence%2520Diagrams%2520for%250A%2520%2520Topological%2520Data%2520Analysis%2520in%2520R%2520and%2520Python%26entry.906535625%3DAleksei%2520Luchinsky%2520and%2520Umar%2520Islambekov%26entry.1292438233%3D%2520%2520Persistent%2520homology%2520is%2520a%2520widely-used%2520tool%2520in%2520topological%2520data%2520analysis%2520%2528TDA%2529%250Afor%2520understanding%2520the%2520underlying%2520shape%2520of%2520complex%2520data.%2520By%2520constructing%2520a%250Afiltration%2520of%2520simplicial%2520complexes%2520from%2520data%2520points%252C%2520it%2520captures%2520topological%250Afeatures%2520such%2520as%2520connected%2520components%252C%2520loops%252C%2520and%2520voids%2520across%2520multiple%2520scales.%250AThese%2520features%2520are%2520encoded%2520in%2520persistence%2520diagrams%2520%2528PDs%2529%252C%2520which%2520provide%2520a%250Aconcise%2520summary%2520of%2520the%2520data%2527s%2520topological%2520structure.%2520However%252C%2520the%2520non-Hilbert%250Anature%2520of%2520the%2520space%2520of%2520PDs%2520poses%2520challenges%2520for%2520their%2520direct%2520use%2520in%2520machine%250Alearning%2520applications.%2520To%2520address%2520this%252C%2520kernel%2520methods%2520and%2520vectorization%250Atechniques%2520have%2520been%2520developed%2520to%2520transform%2520PDs%2520into%250Amachine-learning-compatible%2520formats.%2520In%2520this%2520paper%252C%2520we%2520introduce%2520a%2520new%2520software%250Apackage%2520designed%2520to%2520streamline%2520the%2520vectorization%2520of%2520PDs%252C%2520offering%2520an%2520intuitive%250Aworkflow%2520and%2520advanced%2520functionalities.%2520We%2520demonstrate%2520the%2520necessity%2520of%2520the%250Apackage%2520through%2520practical%2520examples%2520and%2520provide%2520a%2520detailed%2520discussion%2520on%2520its%250Acontributions%2520to%2520applied%2520TDA.%2520Definitions%2520of%2520all%2520vectorization%2520summaries%2520used%250Ain%2520the%2520package%2520are%2520included%2520in%2520the%2520appendix.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2411.17340v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=TDAvec%3A%20Computing%20Vector%20Summaries%20of%20Persistence%20Diagrams%20for%0A%20%20Topological%20Data%20Analysis%20in%20R%20and%20Python&entry.906535625=Aleksei%20Luchinsky%20and%20Umar%20Islambekov&entry.1292438233=%20%20Persistent%20homology%20is%20a%20widely-used%20tool%20in%20topological%20data%20analysis%20%28TDA%29%0Afor%20understanding%20the%20underlying%20shape%20of%20complex%20data.%20By%20constructing%20a%0Afiltration%20of%20simplicial%20complexes%20from%20data%20points%2C%20it%20captures%20topological%0Afeatures%20such%20as%20connected%20components%2C%20loops%2C%20and%20voids%20across%20multiple%20scales.%0AThese%20features%20are%20encoded%20in%20persistence%20diagrams%20%28PDs%29%2C%20which%20provide%20a%0Aconcise%20summary%20of%20the%20data%27s%20topological%20structure.%20However%2C%20the%20non-Hilbert%0Anature%20of%20the%20space%20of%20PDs%20poses%20challenges%20for%20their%20direct%20use%20in%20machine%0Alearning%20applications.%20To%20address%20this%2C%20kernel%20methods%20and%20vectorization%0Atechniques%20have%20been%20developed%20to%20transform%20PDs%20into%0Amachine-learning-compatible%20formats.%20In%20this%20paper%2C%20we%20introduce%20a%20new%20software%0Apackage%20designed%20to%20streamline%20the%20vectorization%20of%20PDs%2C%20offering%20an%20intuitive%0Aworkflow%20and%20advanced%20functionalities.%20We%20demonstrate%20the%20necessity%20of%20the%0Apackage%20through%20practical%20examples%20and%20provide%20a%20detailed%20discussion%20on%20its%0Acontributions%20to%20applied%20TDA.%20Definitions%20of%20all%20vectorization%20summaries%20used%0Ain%20the%20package%20are%20included%20in%20the%20appendix.%0A&entry.1838667208=http%3A//arxiv.org/abs/2411.17340v1&entry.124074799=Read"},
{"title": "Loosely coupled 4D-Radar-Inertial Odometry for Ground Robots", "author": "Lucia Coto Elena and Fernando Caballero and Luis Merino", "abstract": "  Accurate robot odometry is essential for autonomous navigation. While\nnumerous techniques have been developed based on various sensor suites,\nodometry estimation using only radar and IMU remains an underexplored area.\nRadar proves particularly valuable in environments where traditional sensors,\nlike cameras or LiDAR, may struggle, especially in low-light conditions or when\nfaced with environmental challenges like fog, rain or smoke. However, despite\nits robustness, radar data is noisier and more prone to outliers, requiring\nspecialized processing approaches. In this paper, we propose a graph-based\noptimization approach using a sliding window for radar-based odometry, designed\nto maintain robust relationships between poses by forming a network of\nconnections, while keeping computational costs fixed (specially beneficial in\nlong trajectories). Additionally, we introduce an enhancement in the\nego-velocity estimation specifically for ground vehicles, both holonomic and\nnon-holonomic, which subsequently improves the direct odometry input required\nby the optimizer. Finally, we present a comparative study of our approach\nagainst existing algorithms, showing how our pure odometry approach inproves\nthe state of art in most trajectories of the NTU4DRadLM dataset, achieving\npromising results when evaluating key performance metrics.\n", "link": "http://arxiv.org/abs/2411.17289v1", "date": "2024-11-26", "relevancy": 2.2857, "topK": [{"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.5879}, {"title": "Egocentric Computer Vision for Hands-Free Robotic Wheelchair Navigation", "link": "https://link.springer.com/article/10.1007/s10846-023-01807-4", "similarity": 0.5796}, {"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.5566}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Loosely%20coupled%204D-Radar-Inertial%20Odometry%20for%20Ground%20Robots&body=Title%3A%20Loosely%20coupled%204D-Radar-Inertial%20Odometry%20for%20Ground%20Robots%0AAuthor%3A%20Lucia%20Coto%20Elena%20and%20Fernando%20Caballero%20and%20Luis%20Merino%0AAbstract%3A%20%20%20Accurate%20robot%20odometry%20is%20essential%20for%20autonomous%20navigation.%20While%0Anumerous%20techniques%20have%20been%20developed%20based%20on%20various%20sensor%20suites%2C%0Aodometry%20estimation%20using%20only%20radar%20and%20IMU%20remains%20an%20underexplored%20area.%0ARadar%20proves%20particularly%20valuable%20in%20environments%20where%20traditional%20sensors%2C%0Alike%20cameras%20or%20LiDAR%2C%20may%20struggle%2C%20especially%20in%20low-light%20conditions%20or%20when%0Afaced%20with%20environmental%20challenges%20like%20fog%2C%20rain%20or%20smoke.%20However%2C%20despite%0Aits%20robustness%2C%20radar%20data%20is%20noisier%20and%20more%20prone%20to%20outliers%2C%20requiring%0Aspecialized%20processing%20approaches.%20In%20this%20paper%2C%20we%20propose%20a%20graph-based%0Aoptimization%20approach%20using%20a%20sliding%20window%20for%20radar-based%20odometry%2C%20designed%0Ato%20maintain%20robust%20relationships%20between%20poses%20by%20forming%20a%20network%20of%0Aconnections%2C%20while%20keeping%20computational%20costs%20fixed%20%28specially%20beneficial%20in%0Along%20trajectories%29.%20Additionally%2C%20we%20introduce%20an%20enhancement%20in%20the%0Aego-velocity%20estimation%20specifically%20for%20ground%20vehicles%2C%20both%20holonomic%20and%0Anon-holonomic%2C%20which%20subsequently%20improves%20the%20direct%20odometry%20input%20required%0Aby%20the%20optimizer.%20Finally%2C%20we%20present%20a%20comparative%20study%20of%20our%20approach%0Aagainst%20existing%20algorithms%2C%20showing%20how%20our%20pure%20odometry%20approach%20inproves%0Athe%20state%20of%20art%20in%20most%20trajectories%20of%20the%20NTU4DRadLM%20dataset%2C%20achieving%0Apromising%20results%20when%20evaluating%20key%20performance%20metrics.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2411.17289v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DLoosely%2520coupled%25204D-Radar-Inertial%2520Odometry%2520for%2520Ground%2520Robots%26entry.906535625%3DLucia%2520Coto%2520Elena%2520and%2520Fernando%2520Caballero%2520and%2520Luis%2520Merino%26entry.1292438233%3D%2520%2520Accurate%2520robot%2520odometry%2520is%2520essential%2520for%2520autonomous%2520navigation.%2520While%250Anumerous%2520techniques%2520have%2520been%2520developed%2520based%2520on%2520various%2520sensor%2520suites%252C%250Aodometry%2520estimation%2520using%2520only%2520radar%2520and%2520IMU%2520remains%2520an%2520underexplored%2520area.%250ARadar%2520proves%2520particularly%2520valuable%2520in%2520environments%2520where%2520traditional%2520sensors%252C%250Alike%2520cameras%2520or%2520LiDAR%252C%2520may%2520struggle%252C%2520especially%2520in%2520low-light%2520conditions%2520or%2520when%250Afaced%2520with%2520environmental%2520challenges%2520like%2520fog%252C%2520rain%2520or%2520smoke.%2520However%252C%2520despite%250Aits%2520robustness%252C%2520radar%2520data%2520is%2520noisier%2520and%2520more%2520prone%2520to%2520outliers%252C%2520requiring%250Aspecialized%2520processing%2520approaches.%2520In%2520this%2520paper%252C%2520we%2520propose%2520a%2520graph-based%250Aoptimization%2520approach%2520using%2520a%2520sliding%2520window%2520for%2520radar-based%2520odometry%252C%2520designed%250Ato%2520maintain%2520robust%2520relationships%2520between%2520poses%2520by%2520forming%2520a%2520network%2520of%250Aconnections%252C%2520while%2520keeping%2520computational%2520costs%2520fixed%2520%2528specially%2520beneficial%2520in%250Along%2520trajectories%2529.%2520Additionally%252C%2520we%2520introduce%2520an%2520enhancement%2520in%2520the%250Aego-velocity%2520estimation%2520specifically%2520for%2520ground%2520vehicles%252C%2520both%2520holonomic%2520and%250Anon-holonomic%252C%2520which%2520subsequently%2520improves%2520the%2520direct%2520odometry%2520input%2520required%250Aby%2520the%2520optimizer.%2520Finally%252C%2520we%2520present%2520a%2520comparative%2520study%2520of%2520our%2520approach%250Aagainst%2520existing%2520algorithms%252C%2520showing%2520how%2520our%2520pure%2520odometry%2520approach%2520inproves%250Athe%2520state%2520of%2520art%2520in%2520most%2520trajectories%2520of%2520the%2520NTU4DRadLM%2520dataset%252C%2520achieving%250Apromising%2520results%2520when%2520evaluating%2520key%2520performance%2520metrics.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2411.17289v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Loosely%20coupled%204D-Radar-Inertial%20Odometry%20for%20Ground%20Robots&entry.906535625=Lucia%20Coto%20Elena%20and%20Fernando%20Caballero%20and%20Luis%20Merino&entry.1292438233=%20%20Accurate%20robot%20odometry%20is%20essential%20for%20autonomous%20navigation.%20While%0Anumerous%20techniques%20have%20been%20developed%20based%20on%20various%20sensor%20suites%2C%0Aodometry%20estimation%20using%20only%20radar%20and%20IMU%20remains%20an%20underexplored%20area.%0ARadar%20proves%20particularly%20valuable%20in%20environments%20where%20traditional%20sensors%2C%0Alike%20cameras%20or%20LiDAR%2C%20may%20struggle%2C%20especially%20in%20low-light%20conditions%20or%20when%0Afaced%20with%20environmental%20challenges%20like%20fog%2C%20rain%20or%20smoke.%20However%2C%20despite%0Aits%20robustness%2C%20radar%20data%20is%20noisier%20and%20more%20prone%20to%20outliers%2C%20requiring%0Aspecialized%20processing%20approaches.%20In%20this%20paper%2C%20we%20propose%20a%20graph-based%0Aoptimization%20approach%20using%20a%20sliding%20window%20for%20radar-based%20odometry%2C%20designed%0Ato%20maintain%20robust%20relationships%20between%20poses%20by%20forming%20a%20network%20of%0Aconnections%2C%20while%20keeping%20computational%20costs%20fixed%20%28specially%20beneficial%20in%0Along%20trajectories%29.%20Additionally%2C%20we%20introduce%20an%20enhancement%20in%20the%0Aego-velocity%20estimation%20specifically%20for%20ground%20vehicles%2C%20both%20holonomic%20and%0Anon-holonomic%2C%20which%20subsequently%20improves%20the%20direct%20odometry%20input%20required%0Aby%20the%20optimizer.%20Finally%2C%20we%20present%20a%20comparative%20study%20of%20our%20approach%0Aagainst%20existing%20algorithms%2C%20showing%20how%20our%20pure%20odometry%20approach%20inproves%0Athe%20state%20of%20art%20in%20most%20trajectories%20of%20the%20NTU4DRadLM%20dataset%2C%20achieving%0Apromising%20results%20when%20evaluating%20key%20performance%20metrics.%0A&entry.1838667208=http%3A//arxiv.org/abs/2411.17289v1&entry.124074799=Read"},
{"title": "AeroGen: Enhancing Remote Sensing Object Detection with Diffusion-Driven\n  Data Generation", "author": "Datao Tang and Xiangyong Cao and Xuan Wu and Jialin Li and Jing Yao and Xueru Bai and Deyu Meng", "abstract": "  Remote sensing image object detection (RSIOD) aims to identify and locate\nspecific objects within satellite or aerial imagery. However, there is a\nscarcity of labeled data in current RSIOD datasets, which significantly limits\nthe performance of current detection algorithms. Although existing techniques,\ne.g., data augmentation and semi-supervised learning, can mitigate this\nscarcity issue to some extent, they are heavily dependent on high-quality\nlabeled data and perform worse in rare object classes. To address this issue,\nthis paper proposes a layout-controllable diffusion generative model (i.e.\nAeroGen) tailored for RSIOD. To our knowledge, AeroGen is the first model to\nsimultaneously support horizontal and rotated bounding box condition\ngeneration, thus enabling the generation of high-quality synthetic images that\nmeet specific layout and object category requirements. Additionally, we propose\nan end-to-end data augmentation framework that integrates a\ndiversity-conditioned generator and a filtering mechanism to enhance both the\ndiversity and quality of generated data. Experimental results demonstrate that\nthe synthetic data produced by our method are of high quality and diversity.\nFurthermore, the synthetic RSIOD data can significantly improve the detection\nperformance of existing RSIOD models, i.e., the mAP metrics on DIOR, DIOR-R,\nand HRSC datasets are improved by 3.7%, 4.3%, and 2.43%, respectively. The code\nis available at https://github.com/Sonettoo/AeroGen.\n", "link": "http://arxiv.org/abs/2411.15497v2", "date": "2024-11-26", "relevancy": 2.2851, "topK": [{"title": "Thinking Outside the BBox: Unconstrained Generative Object Compositing", "link": "http://arxiv.org/abs/2409.04559v2", "similarity": 0.6106}, {"title": "PhysGen: Rigid-Body Physics-Grounded Image-to-Video Generation", "link": "http://arxiv.org/abs/2409.18964v1", "similarity": 0.5651}, {"title": "ToDo: Token Downsampling for Efficient Generation of High-Resolution\n  Images", "link": "http://arxiv.org/abs/2402.13573v2", "similarity": 0.5617}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20AeroGen%3A%20Enhancing%20Remote%20Sensing%20Object%20Detection%20with%20Diffusion-Driven%0A%20%20Data%20Generation&body=Title%3A%20AeroGen%3A%20Enhancing%20Remote%20Sensing%20Object%20Detection%20with%20Diffusion-Driven%0A%20%20Data%20Generation%0AAuthor%3A%20Datao%20Tang%20and%20Xiangyong%20Cao%20and%20Xuan%20Wu%20and%20Jialin%20Li%20and%20Jing%20Yao%20and%20Xueru%20Bai%20and%20Deyu%20Meng%0AAbstract%3A%20%20%20Remote%20sensing%20image%20object%20detection%20%28RSIOD%29%20aims%20to%20identify%20and%20locate%0Aspecific%20objects%20within%20satellite%20or%20aerial%20imagery.%20However%2C%20there%20is%20a%0Ascarcity%20of%20labeled%20data%20in%20current%20RSIOD%20datasets%2C%20which%20significantly%20limits%0Athe%20performance%20of%20current%20detection%20algorithms.%20Although%20existing%20techniques%2C%0Ae.g.%2C%20data%20augmentation%20and%20semi-supervised%20learning%2C%20can%20mitigate%20this%0Ascarcity%20issue%20to%20some%20extent%2C%20they%20are%20heavily%20dependent%20on%20high-quality%0Alabeled%20data%20and%20perform%20worse%20in%20rare%20object%20classes.%20To%20address%20this%20issue%2C%0Athis%20paper%20proposes%20a%20layout-controllable%20diffusion%20generative%20model%20%28i.e.%0AAeroGen%29%20tailored%20for%20RSIOD.%20To%20our%20knowledge%2C%20AeroGen%20is%20the%20first%20model%20to%0Asimultaneously%20support%20horizontal%20and%20rotated%20bounding%20box%20condition%0Ageneration%2C%20thus%20enabling%20the%20generation%20of%20high-quality%20synthetic%20images%20that%0Ameet%20specific%20layout%20and%20object%20category%20requirements.%20Additionally%2C%20we%20propose%0Aan%20end-to-end%20data%20augmentation%20framework%20that%20integrates%20a%0Adiversity-conditioned%20generator%20and%20a%20filtering%20mechanism%20to%20enhance%20both%20the%0Adiversity%20and%20quality%20of%20generated%20data.%20Experimental%20results%20demonstrate%20that%0Athe%20synthetic%20data%20produced%20by%20our%20method%20are%20of%20high%20quality%20and%20diversity.%0AFurthermore%2C%20the%20synthetic%20RSIOD%20data%20can%20significantly%20improve%20the%20detection%0Aperformance%20of%20existing%20RSIOD%20models%2C%20i.e.%2C%20the%20mAP%20metrics%20on%20DIOR%2C%20DIOR-R%2C%0Aand%20HRSC%20datasets%20are%20improved%20by%203.7%25%2C%204.3%25%2C%20and%202.43%25%2C%20respectively.%20The%20code%0Ais%20available%20at%20https%3A//github.com/Sonettoo/AeroGen.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2411.15497v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DAeroGen%253A%2520Enhancing%2520Remote%2520Sensing%2520Object%2520Detection%2520with%2520Diffusion-Driven%250A%2520%2520Data%2520Generation%26entry.906535625%3DDatao%2520Tang%2520and%2520Xiangyong%2520Cao%2520and%2520Xuan%2520Wu%2520and%2520Jialin%2520Li%2520and%2520Jing%2520Yao%2520and%2520Xueru%2520Bai%2520and%2520Deyu%2520Meng%26entry.1292438233%3D%2520%2520Remote%2520sensing%2520image%2520object%2520detection%2520%2528RSIOD%2529%2520aims%2520to%2520identify%2520and%2520locate%250Aspecific%2520objects%2520within%2520satellite%2520or%2520aerial%2520imagery.%2520However%252C%2520there%2520is%2520a%250Ascarcity%2520of%2520labeled%2520data%2520in%2520current%2520RSIOD%2520datasets%252C%2520which%2520significantly%2520limits%250Athe%2520performance%2520of%2520current%2520detection%2520algorithms.%2520Although%2520existing%2520techniques%252C%250Ae.g.%252C%2520data%2520augmentation%2520and%2520semi-supervised%2520learning%252C%2520can%2520mitigate%2520this%250Ascarcity%2520issue%2520to%2520some%2520extent%252C%2520they%2520are%2520heavily%2520dependent%2520on%2520high-quality%250Alabeled%2520data%2520and%2520perform%2520worse%2520in%2520rare%2520object%2520classes.%2520To%2520address%2520this%2520issue%252C%250Athis%2520paper%2520proposes%2520a%2520layout-controllable%2520diffusion%2520generative%2520model%2520%2528i.e.%250AAeroGen%2529%2520tailored%2520for%2520RSIOD.%2520To%2520our%2520knowledge%252C%2520AeroGen%2520is%2520the%2520first%2520model%2520to%250Asimultaneously%2520support%2520horizontal%2520and%2520rotated%2520bounding%2520box%2520condition%250Ageneration%252C%2520thus%2520enabling%2520the%2520generation%2520of%2520high-quality%2520synthetic%2520images%2520that%250Ameet%2520specific%2520layout%2520and%2520object%2520category%2520requirements.%2520Additionally%252C%2520we%2520propose%250Aan%2520end-to-end%2520data%2520augmentation%2520framework%2520that%2520integrates%2520a%250Adiversity-conditioned%2520generator%2520and%2520a%2520filtering%2520mechanism%2520to%2520enhance%2520both%2520the%250Adiversity%2520and%2520quality%2520of%2520generated%2520data.%2520Experimental%2520results%2520demonstrate%2520that%250Athe%2520synthetic%2520data%2520produced%2520by%2520our%2520method%2520are%2520of%2520high%2520quality%2520and%2520diversity.%250AFurthermore%252C%2520the%2520synthetic%2520RSIOD%2520data%2520can%2520significantly%2520improve%2520the%2520detection%250Aperformance%2520of%2520existing%2520RSIOD%2520models%252C%2520i.e.%252C%2520the%2520mAP%2520metrics%2520on%2520DIOR%252C%2520DIOR-R%252C%250Aand%2520HRSC%2520datasets%2520are%2520improved%2520by%25203.7%2525%252C%25204.3%2525%252C%2520and%25202.43%2525%252C%2520respectively.%2520The%2520code%250Ais%2520available%2520at%2520https%253A//github.com/Sonettoo/AeroGen.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2411.15497v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=AeroGen%3A%20Enhancing%20Remote%20Sensing%20Object%20Detection%20with%20Diffusion-Driven%0A%20%20Data%20Generation&entry.906535625=Datao%20Tang%20and%20Xiangyong%20Cao%20and%20Xuan%20Wu%20and%20Jialin%20Li%20and%20Jing%20Yao%20and%20Xueru%20Bai%20and%20Deyu%20Meng&entry.1292438233=%20%20Remote%20sensing%20image%20object%20detection%20%28RSIOD%29%20aims%20to%20identify%20and%20locate%0Aspecific%20objects%20within%20satellite%20or%20aerial%20imagery.%20However%2C%20there%20is%20a%0Ascarcity%20of%20labeled%20data%20in%20current%20RSIOD%20datasets%2C%20which%20significantly%20limits%0Athe%20performance%20of%20current%20detection%20algorithms.%20Although%20existing%20techniques%2C%0Ae.g.%2C%20data%20augmentation%20and%20semi-supervised%20learning%2C%20can%20mitigate%20this%0Ascarcity%20issue%20to%20some%20extent%2C%20they%20are%20heavily%20dependent%20on%20high-quality%0Alabeled%20data%20and%20perform%20worse%20in%20rare%20object%20classes.%20To%20address%20this%20issue%2C%0Athis%20paper%20proposes%20a%20layout-controllable%20diffusion%20generative%20model%20%28i.e.%0AAeroGen%29%20tailored%20for%20RSIOD.%20To%20our%20knowledge%2C%20AeroGen%20is%20the%20first%20model%20to%0Asimultaneously%20support%20horizontal%20and%20rotated%20bounding%20box%20condition%0Ageneration%2C%20thus%20enabling%20the%20generation%20of%20high-quality%20synthetic%20images%20that%0Ameet%20specific%20layout%20and%20object%20category%20requirements.%20Additionally%2C%20we%20propose%0Aan%20end-to-end%20data%20augmentation%20framework%20that%20integrates%20a%0Adiversity-conditioned%20generator%20and%20a%20filtering%20mechanism%20to%20enhance%20both%20the%0Adiversity%20and%20quality%20of%20generated%20data.%20Experimental%20results%20demonstrate%20that%0Athe%20synthetic%20data%20produced%20by%20our%20method%20are%20of%20high%20quality%20and%20diversity.%0AFurthermore%2C%20the%20synthetic%20RSIOD%20data%20can%20significantly%20improve%20the%20detection%0Aperformance%20of%20existing%20RSIOD%20models%2C%20i.e.%2C%20the%20mAP%20metrics%20on%20DIOR%2C%20DIOR-R%2C%0Aand%20HRSC%20datasets%20are%20improved%20by%203.7%25%2C%204.3%25%2C%20and%202.43%25%2C%20respectively.%20The%20code%0Ais%20available%20at%20https%3A//github.com/Sonettoo/AeroGen.%0A&entry.1838667208=http%3A//arxiv.org/abs/2411.15497v2&entry.124074799=Read"},
{"title": "Universal Segmentation at Arbitrary Granularity with Language\n  Instruction", "author": "Yong Liu and Cairong Zhang and Yitong Wang and Jiahao Wang and Yujiu Yang and Yansong Tang", "abstract": "  This paper aims to achieve universal segmentation of arbitrary semantic\nlevel. Despite significant progress in recent years, specialist segmentation\napproaches are limited to specific tasks and data distribution. Retraining a\nnew model for adaptation to new scenarios or settings takes expensive\ncomputation and time cost, which raises the demand for versatile and universal\nsegmentation model that can cater to various granularity. Although some\nattempts have been made for unifying different segmentation tasks or\ngeneralization to various scenarios, limitations in the definition of paradigms\nand input-output spaces make it difficult for them to achieve accurate\nunderstanding of content at arbitrary granularity. To this end, we present\nUniLSeg, a universal segmentation model that can perform segmentation at any\nsemantic level with the guidance of language instructions. For training\nUniLSeg, we reorganize a group of tasks from original diverse distributions\ninto a unified data format, where images with texts describing segmentation\ntargets as input and corresponding masks are output. Combined with a automatic\nannotation engine for utilizing numerous unlabeled data, UniLSeg achieves\nexcellent performance on various tasks and settings, surpassing both specialist\nand unified segmentation models.\n", "link": "http://arxiv.org/abs/2312.01623v4", "date": "2024-11-26", "relevancy": 2.2551, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5733}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5733}, {"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.5163}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Universal%20Segmentation%20at%20Arbitrary%20Granularity%20with%20Language%0A%20%20Instruction&body=Title%3A%20Universal%20Segmentation%20at%20Arbitrary%20Granularity%20with%20Language%0A%20%20Instruction%0AAuthor%3A%20Yong%20Liu%20and%20Cairong%20Zhang%20and%20Yitong%20Wang%20and%20Jiahao%20Wang%20and%20Yujiu%20Yang%20and%20Yansong%20Tang%0AAbstract%3A%20%20%20This%20paper%20aims%20to%20achieve%20universal%20segmentation%20of%20arbitrary%20semantic%0Alevel.%20Despite%20significant%20progress%20in%20recent%20years%2C%20specialist%20segmentation%0Aapproaches%20are%20limited%20to%20specific%20tasks%20and%20data%20distribution.%20Retraining%20a%0Anew%20model%20for%20adaptation%20to%20new%20scenarios%20or%20settings%20takes%20expensive%0Acomputation%20and%20time%20cost%2C%20which%20raises%20the%20demand%20for%20versatile%20and%20universal%0Asegmentation%20model%20that%20can%20cater%20to%20various%20granularity.%20Although%20some%0Aattempts%20have%20been%20made%20for%20unifying%20different%20segmentation%20tasks%20or%0Ageneralization%20to%20various%20scenarios%2C%20limitations%20in%20the%20definition%20of%20paradigms%0Aand%20input-output%20spaces%20make%20it%20difficult%20for%20them%20to%20achieve%20accurate%0Aunderstanding%20of%20content%20at%20arbitrary%20granularity.%20To%20this%20end%2C%20we%20present%0AUniLSeg%2C%20a%20universal%20segmentation%20model%20that%20can%20perform%20segmentation%20at%20any%0Asemantic%20level%20with%20the%20guidance%20of%20language%20instructions.%20For%20training%0AUniLSeg%2C%20we%20reorganize%20a%20group%20of%20tasks%20from%20original%20diverse%20distributions%0Ainto%20a%20unified%20data%20format%2C%20where%20images%20with%20texts%20describing%20segmentation%0Atargets%20as%20input%20and%20corresponding%20masks%20are%20output.%20Combined%20with%20a%20automatic%0Aannotation%20engine%20for%20utilizing%20numerous%20unlabeled%20data%2C%20UniLSeg%20achieves%0Aexcellent%20performance%20on%20various%20tasks%20and%20settings%2C%20surpassing%20both%20specialist%0Aand%20unified%20segmentation%20models.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2312.01623v4%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DUniversal%2520Segmentation%2520at%2520Arbitrary%2520Granularity%2520with%2520Language%250A%2520%2520Instruction%26entry.906535625%3DYong%2520Liu%2520and%2520Cairong%2520Zhang%2520and%2520Yitong%2520Wang%2520and%2520Jiahao%2520Wang%2520and%2520Yujiu%2520Yang%2520and%2520Yansong%2520Tang%26entry.1292438233%3D%2520%2520This%2520paper%2520aims%2520to%2520achieve%2520universal%2520segmentation%2520of%2520arbitrary%2520semantic%250Alevel.%2520Despite%2520significant%2520progress%2520in%2520recent%2520years%252C%2520specialist%2520segmentation%250Aapproaches%2520are%2520limited%2520to%2520specific%2520tasks%2520and%2520data%2520distribution.%2520Retraining%2520a%250Anew%2520model%2520for%2520adaptation%2520to%2520new%2520scenarios%2520or%2520settings%2520takes%2520expensive%250Acomputation%2520and%2520time%2520cost%252C%2520which%2520raises%2520the%2520demand%2520for%2520versatile%2520and%2520universal%250Asegmentation%2520model%2520that%2520can%2520cater%2520to%2520various%2520granularity.%2520Although%2520some%250Aattempts%2520have%2520been%2520made%2520for%2520unifying%2520different%2520segmentation%2520tasks%2520or%250Ageneralization%2520to%2520various%2520scenarios%252C%2520limitations%2520in%2520the%2520definition%2520of%2520paradigms%250Aand%2520input-output%2520spaces%2520make%2520it%2520difficult%2520for%2520them%2520to%2520achieve%2520accurate%250Aunderstanding%2520of%2520content%2520at%2520arbitrary%2520granularity.%2520To%2520this%2520end%252C%2520we%2520present%250AUniLSeg%252C%2520a%2520universal%2520segmentation%2520model%2520that%2520can%2520perform%2520segmentation%2520at%2520any%250Asemantic%2520level%2520with%2520the%2520guidance%2520of%2520language%2520instructions.%2520For%2520training%250AUniLSeg%252C%2520we%2520reorganize%2520a%2520group%2520of%2520tasks%2520from%2520original%2520diverse%2520distributions%250Ainto%2520a%2520unified%2520data%2520format%252C%2520where%2520images%2520with%2520texts%2520describing%2520segmentation%250Atargets%2520as%2520input%2520and%2520corresponding%2520masks%2520are%2520output.%2520Combined%2520with%2520a%2520automatic%250Aannotation%2520engine%2520for%2520utilizing%2520numerous%2520unlabeled%2520data%252C%2520UniLSeg%2520achieves%250Aexcellent%2520performance%2520on%2520various%2520tasks%2520and%2520settings%252C%2520surpassing%2520both%2520specialist%250Aand%2520unified%2520segmentation%2520models.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2312.01623v4%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Universal%20Segmentation%20at%20Arbitrary%20Granularity%20with%20Language%0A%20%20Instruction&entry.906535625=Yong%20Liu%20and%20Cairong%20Zhang%20and%20Yitong%20Wang%20and%20Jiahao%20Wang%20and%20Yujiu%20Yang%20and%20Yansong%20Tang&entry.1292438233=%20%20This%20paper%20aims%20to%20achieve%20universal%20segmentation%20of%20arbitrary%20semantic%0Alevel.%20Despite%20significant%20progress%20in%20recent%20years%2C%20specialist%20segmentation%0Aapproaches%20are%20limited%20to%20specific%20tasks%20and%20data%20distribution.%20Retraining%20a%0Anew%20model%20for%20adaptation%20to%20new%20scenarios%20or%20settings%20takes%20expensive%0Acomputation%20and%20time%20cost%2C%20which%20raises%20the%20demand%20for%20versatile%20and%20universal%0Asegmentation%20model%20that%20can%20cater%20to%20various%20granularity.%20Although%20some%0Aattempts%20have%20been%20made%20for%20unifying%20different%20segmentation%20tasks%20or%0Ageneralization%20to%20various%20scenarios%2C%20limitations%20in%20the%20definition%20of%20paradigms%0Aand%20input-output%20spaces%20make%20it%20difficult%20for%20them%20to%20achieve%20accurate%0Aunderstanding%20of%20content%20at%20arbitrary%20granularity.%20To%20this%20end%2C%20we%20present%0AUniLSeg%2C%20a%20universal%20segmentation%20model%20that%20can%20perform%20segmentation%20at%20any%0Asemantic%20level%20with%20the%20guidance%20of%20language%20instructions.%20For%20training%0AUniLSeg%2C%20we%20reorganize%20a%20group%20of%20tasks%20from%20original%20diverse%20distributions%0Ainto%20a%20unified%20data%20format%2C%20where%20images%20with%20texts%20describing%20segmentation%0Atargets%20as%20input%20and%20corresponding%20masks%20are%20output.%20Combined%20with%20a%20automatic%0Aannotation%20engine%20for%20utilizing%20numerous%20unlabeled%20data%2C%20UniLSeg%20achieves%0Aexcellent%20performance%20on%20various%20tasks%20and%20settings%2C%20surpassing%20both%20specialist%0Aand%20unified%20segmentation%20models.%0A&entry.1838667208=http%3A//arxiv.org/abs/2312.01623v4&entry.124074799=Read"},
{"title": "Efficient Long Video Tokenization via Coordinate-based Patch\n  Reconstruction", "author": "Huiwon Jang and Sihyun Yu and Jinwoo Shin and Pieter Abbeel and Younggyo Seo", "abstract": "  Efficient tokenization of videos remains a challenge in training vision\nmodels that can process long videos. One promising direction is to develop a\ntokenizer that can encode long video clips, as it would enable the tokenizer to\nleverage the temporal coherence of videos better for tokenization. However,\ntraining existing tokenizers on long videos often incurs a huge training cost\nas they are trained to reconstruct all the frames at once. In this paper, we\nintroduce CoordTok, a video tokenizer that learns a mapping from\ncoordinate-based representations to the corresponding patches of input videos,\ninspired by recent advances in 3D generative models. In particular, CoordTok\nencodes a video into factorized triplane representations and reconstructs\npatches that correspond to randomly sampled $(x,y,t)$ coordinates. This allows\nfor training large tokenizer models directly on long videos without requiring\nexcessive training resources. Our experiments show that CoordTok can\ndrastically reduce the number of tokens for encoding long video clips. For\ninstance, CoordTok can encode a 128-frame video with 128$\\times$128 resolution\ninto 1280 tokens, while baselines need 6144 or 8192 tokens to achieve similar\nreconstruction quality. We further show that this efficient video tokenization\nenables memory-efficient training of a diffusion transformer that can generate\n128 frames at once.\n", "link": "http://arxiv.org/abs/2411.14762v2", "date": "2024-11-26", "relevancy": 2.2435, "topK": [{"title": "ToDo: Token Downsampling for Efficient Generation of High-Resolution\n  Images", "link": "http://arxiv.org/abs/2402.13573v2", "similarity": 0.5798}, {"title": "Customize-A-Video: One-Shot Motion Customization of Text-to-Video\n  Diffusion Models", "link": "http://arxiv.org/abs/2402.14780v1", "similarity": 0.5477}, {"title": "Magic-Me: Identity-Specific Video Customized Diffusion", "link": "http://arxiv.org/abs/2402.09368v1", "similarity": 0.5472}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Efficient%20Long%20Video%20Tokenization%20via%20Coordinate-based%20Patch%0A%20%20Reconstruction&body=Title%3A%20Efficient%20Long%20Video%20Tokenization%20via%20Coordinate-based%20Patch%0A%20%20Reconstruction%0AAuthor%3A%20Huiwon%20Jang%20and%20Sihyun%20Yu%20and%20Jinwoo%20Shin%20and%20Pieter%20Abbeel%20and%20Younggyo%20Seo%0AAbstract%3A%20%20%20Efficient%20tokenization%20of%20videos%20remains%20a%20challenge%20in%20training%20vision%0Amodels%20that%20can%20process%20long%20videos.%20One%20promising%20direction%20is%20to%20develop%20a%0Atokenizer%20that%20can%20encode%20long%20video%20clips%2C%20as%20it%20would%20enable%20the%20tokenizer%20to%0Aleverage%20the%20temporal%20coherence%20of%20videos%20better%20for%20tokenization.%20However%2C%0Atraining%20existing%20tokenizers%20on%20long%20videos%20often%20incurs%20a%20huge%20training%20cost%0Aas%20they%20are%20trained%20to%20reconstruct%20all%20the%20frames%20at%20once.%20In%20this%20paper%2C%20we%0Aintroduce%20CoordTok%2C%20a%20video%20tokenizer%20that%20learns%20a%20mapping%20from%0Acoordinate-based%20representations%20to%20the%20corresponding%20patches%20of%20input%20videos%2C%0Ainspired%20by%20recent%20advances%20in%203D%20generative%20models.%20In%20particular%2C%20CoordTok%0Aencodes%20a%20video%20into%20factorized%20triplane%20representations%20and%20reconstructs%0Apatches%20that%20correspond%20to%20randomly%20sampled%20%24%28x%2Cy%2Ct%29%24%20coordinates.%20This%20allows%0Afor%20training%20large%20tokenizer%20models%20directly%20on%20long%20videos%20without%20requiring%0Aexcessive%20training%20resources.%20Our%20experiments%20show%20that%20CoordTok%20can%0Adrastically%20reduce%20the%20number%20of%20tokens%20for%20encoding%20long%20video%20clips.%20For%0Ainstance%2C%20CoordTok%20can%20encode%20a%20128-frame%20video%20with%20128%24%5Ctimes%24128%20resolution%0Ainto%201280%20tokens%2C%20while%20baselines%20need%206144%20or%208192%20tokens%20to%20achieve%20similar%0Areconstruction%20quality.%20We%20further%20show%20that%20this%20efficient%20video%20tokenization%0Aenables%20memory-efficient%20training%20of%20a%20diffusion%20transformer%20that%20can%20generate%0A128%20frames%20at%20once.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2411.14762v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DEfficient%2520Long%2520Video%2520Tokenization%2520via%2520Coordinate-based%2520Patch%250A%2520%2520Reconstruction%26entry.906535625%3DHuiwon%2520Jang%2520and%2520Sihyun%2520Yu%2520and%2520Jinwoo%2520Shin%2520and%2520Pieter%2520Abbeel%2520and%2520Younggyo%2520Seo%26entry.1292438233%3D%2520%2520Efficient%2520tokenization%2520of%2520videos%2520remains%2520a%2520challenge%2520in%2520training%2520vision%250Amodels%2520that%2520can%2520process%2520long%2520videos.%2520One%2520promising%2520direction%2520is%2520to%2520develop%2520a%250Atokenizer%2520that%2520can%2520encode%2520long%2520video%2520clips%252C%2520as%2520it%2520would%2520enable%2520the%2520tokenizer%2520to%250Aleverage%2520the%2520temporal%2520coherence%2520of%2520videos%2520better%2520for%2520tokenization.%2520However%252C%250Atraining%2520existing%2520tokenizers%2520on%2520long%2520videos%2520often%2520incurs%2520a%2520huge%2520training%2520cost%250Aas%2520they%2520are%2520trained%2520to%2520reconstruct%2520all%2520the%2520frames%2520at%2520once.%2520In%2520this%2520paper%252C%2520we%250Aintroduce%2520CoordTok%252C%2520a%2520video%2520tokenizer%2520that%2520learns%2520a%2520mapping%2520from%250Acoordinate-based%2520representations%2520to%2520the%2520corresponding%2520patches%2520of%2520input%2520videos%252C%250Ainspired%2520by%2520recent%2520advances%2520in%25203D%2520generative%2520models.%2520In%2520particular%252C%2520CoordTok%250Aencodes%2520a%2520video%2520into%2520factorized%2520triplane%2520representations%2520and%2520reconstructs%250Apatches%2520that%2520correspond%2520to%2520randomly%2520sampled%2520%2524%2528x%252Cy%252Ct%2529%2524%2520coordinates.%2520This%2520allows%250Afor%2520training%2520large%2520tokenizer%2520models%2520directly%2520on%2520long%2520videos%2520without%2520requiring%250Aexcessive%2520training%2520resources.%2520Our%2520experiments%2520show%2520that%2520CoordTok%2520can%250Adrastically%2520reduce%2520the%2520number%2520of%2520tokens%2520for%2520encoding%2520long%2520video%2520clips.%2520For%250Ainstance%252C%2520CoordTok%2520can%2520encode%2520a%2520128-frame%2520video%2520with%2520128%2524%255Ctimes%2524128%2520resolution%250Ainto%25201280%2520tokens%252C%2520while%2520baselines%2520need%25206144%2520or%25208192%2520tokens%2520to%2520achieve%2520similar%250Areconstruction%2520quality.%2520We%2520further%2520show%2520that%2520this%2520efficient%2520video%2520tokenization%250Aenables%2520memory-efficient%2520training%2520of%2520a%2520diffusion%2520transformer%2520that%2520can%2520generate%250A128%2520frames%2520at%2520once.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2411.14762v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Efficient%20Long%20Video%20Tokenization%20via%20Coordinate-based%20Patch%0A%20%20Reconstruction&entry.906535625=Huiwon%20Jang%20and%20Sihyun%20Yu%20and%20Jinwoo%20Shin%20and%20Pieter%20Abbeel%20and%20Younggyo%20Seo&entry.1292438233=%20%20Efficient%20tokenization%20of%20videos%20remains%20a%20challenge%20in%20training%20vision%0Amodels%20that%20can%20process%20long%20videos.%20One%20promising%20direction%20is%20to%20develop%20a%0Atokenizer%20that%20can%20encode%20long%20video%20clips%2C%20as%20it%20would%20enable%20the%20tokenizer%20to%0Aleverage%20the%20temporal%20coherence%20of%20videos%20better%20for%20tokenization.%20However%2C%0Atraining%20existing%20tokenizers%20on%20long%20videos%20often%20incurs%20a%20huge%20training%20cost%0Aas%20they%20are%20trained%20to%20reconstruct%20all%20the%20frames%20at%20once.%20In%20this%20paper%2C%20we%0Aintroduce%20CoordTok%2C%20a%20video%20tokenizer%20that%20learns%20a%20mapping%20from%0Acoordinate-based%20representations%20to%20the%20corresponding%20patches%20of%20input%20videos%2C%0Ainspired%20by%20recent%20advances%20in%203D%20generative%20models.%20In%20particular%2C%20CoordTok%0Aencodes%20a%20video%20into%20factorized%20triplane%20representations%20and%20reconstructs%0Apatches%20that%20correspond%20to%20randomly%20sampled%20%24%28x%2Cy%2Ct%29%24%20coordinates.%20This%20allows%0Afor%20training%20large%20tokenizer%20models%20directly%20on%20long%20videos%20without%20requiring%0Aexcessive%20training%20resources.%20Our%20experiments%20show%20that%20CoordTok%20can%0Adrastically%20reduce%20the%20number%20of%20tokens%20for%20encoding%20long%20video%20clips.%20For%0Ainstance%2C%20CoordTok%20can%20encode%20a%20128-frame%20video%20with%20128%24%5Ctimes%24128%20resolution%0Ainto%201280%20tokens%2C%20while%20baselines%20need%206144%20or%208192%20tokens%20to%20achieve%20similar%0Areconstruction%20quality.%20We%20further%20show%20that%20this%20efficient%20video%20tokenization%0Aenables%20memory-efficient%20training%20of%20a%20diffusion%20transformer%20that%20can%20generate%0A128%20frames%20at%20once.%0A&entry.1838667208=http%3A//arxiv.org/abs/2411.14762v2&entry.124074799=Read"},
{"title": "SuperMat: Physically Consistent PBR Material Estimation at Interactive\n  Rates", "author": "Yijia Hong and Yuan-Chen Guo and Ran Yi and Yulong Chen and Yan-Pei Cao and Lizhuang Ma", "abstract": "  Decomposing physically-based materials from images into their constituent\nproperties remains challenging, particularly when maintaining both\ncomputational efficiency and physical consistency. While recent diffusion-based\napproaches have shown promise, they face substantial computational overhead due\nto multiple denoising steps and separate models for different material\nproperties. We present SuperMat, a single-step framework that achieves\nhigh-quality material decomposition with one-step inference. This enables\nend-to-end training with perceptual and re-render losses while decomposing\nalbedo, metallic, and roughness maps at millisecond-scale speeds. We further\nextend our framework to 3D objects through a UV refinement network, enabling\nconsistent material estimation across viewpoints while maintaining efficiency.\nExperiments demonstrate that SuperMat achieves state-of-the-art PBR material\ndecomposition quality while reducing inference time from seconds to\nmilliseconds per image, and completes PBR material estimation for 3D objects in\napproximately 3 seconds.\n", "link": "http://arxiv.org/abs/2411.17515v1", "date": "2024-11-26", "relevancy": 2.2234, "topK": [{"title": "FabricDiffusion: High-Fidelity Texture Transfer for 3D Garments\n  Generation from In-The-Wild Clothing Images", "link": "http://arxiv.org/abs/2410.01801v1", "similarity": 0.5758}, {"title": "SDXS: Real-Time One-Step Latent Diffusion Models with Image Conditions", "link": "http://arxiv.org/abs/2403.16627v1", "similarity": 0.5647}, {"title": "Dynamic Gaussians Mesh: Consistent Mesh Reconstruction from Monocular\n  Videos", "link": "http://arxiv.org/abs/2404.12379v2", "similarity": 0.5323}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20SuperMat%3A%20Physically%20Consistent%20PBR%20Material%20Estimation%20at%20Interactive%0A%20%20Rates&body=Title%3A%20SuperMat%3A%20Physically%20Consistent%20PBR%20Material%20Estimation%20at%20Interactive%0A%20%20Rates%0AAuthor%3A%20Yijia%20Hong%20and%20Yuan-Chen%20Guo%20and%20Ran%20Yi%20and%20Yulong%20Chen%20and%20Yan-Pei%20Cao%20and%20Lizhuang%20Ma%0AAbstract%3A%20%20%20Decomposing%20physically-based%20materials%20from%20images%20into%20their%20constituent%0Aproperties%20remains%20challenging%2C%20particularly%20when%20maintaining%20both%0Acomputational%20efficiency%20and%20physical%20consistency.%20While%20recent%20diffusion-based%0Aapproaches%20have%20shown%20promise%2C%20they%20face%20substantial%20computational%20overhead%20due%0Ato%20multiple%20denoising%20steps%20and%20separate%20models%20for%20different%20material%0Aproperties.%20We%20present%20SuperMat%2C%20a%20single-step%20framework%20that%20achieves%0Ahigh-quality%20material%20decomposition%20with%20one-step%20inference.%20This%20enables%0Aend-to-end%20training%20with%20perceptual%20and%20re-render%20losses%20while%20decomposing%0Aalbedo%2C%20metallic%2C%20and%20roughness%20maps%20at%20millisecond-scale%20speeds.%20We%20further%0Aextend%20our%20framework%20to%203D%20objects%20through%20a%20UV%20refinement%20network%2C%20enabling%0Aconsistent%20material%20estimation%20across%20viewpoints%20while%20maintaining%20efficiency.%0AExperiments%20demonstrate%20that%20SuperMat%20achieves%20state-of-the-art%20PBR%20material%0Adecomposition%20quality%20while%20reducing%20inference%20time%20from%20seconds%20to%0Amilliseconds%20per%20image%2C%20and%20completes%20PBR%20material%20estimation%20for%203D%20objects%20in%0Aapproximately%203%20seconds.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2411.17515v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DSuperMat%253A%2520Physically%2520Consistent%2520PBR%2520Material%2520Estimation%2520at%2520Interactive%250A%2520%2520Rates%26entry.906535625%3DYijia%2520Hong%2520and%2520Yuan-Chen%2520Guo%2520and%2520Ran%2520Yi%2520and%2520Yulong%2520Chen%2520and%2520Yan-Pei%2520Cao%2520and%2520Lizhuang%2520Ma%26entry.1292438233%3D%2520%2520Decomposing%2520physically-based%2520materials%2520from%2520images%2520into%2520their%2520constituent%250Aproperties%2520remains%2520challenging%252C%2520particularly%2520when%2520maintaining%2520both%250Acomputational%2520efficiency%2520and%2520physical%2520consistency.%2520While%2520recent%2520diffusion-based%250Aapproaches%2520have%2520shown%2520promise%252C%2520they%2520face%2520substantial%2520computational%2520overhead%2520due%250Ato%2520multiple%2520denoising%2520steps%2520and%2520separate%2520models%2520for%2520different%2520material%250Aproperties.%2520We%2520present%2520SuperMat%252C%2520a%2520single-step%2520framework%2520that%2520achieves%250Ahigh-quality%2520material%2520decomposition%2520with%2520one-step%2520inference.%2520This%2520enables%250Aend-to-end%2520training%2520with%2520perceptual%2520and%2520re-render%2520losses%2520while%2520decomposing%250Aalbedo%252C%2520metallic%252C%2520and%2520roughness%2520maps%2520at%2520millisecond-scale%2520speeds.%2520We%2520further%250Aextend%2520our%2520framework%2520to%25203D%2520objects%2520through%2520a%2520UV%2520refinement%2520network%252C%2520enabling%250Aconsistent%2520material%2520estimation%2520across%2520viewpoints%2520while%2520maintaining%2520efficiency.%250AExperiments%2520demonstrate%2520that%2520SuperMat%2520achieves%2520state-of-the-art%2520PBR%2520material%250Adecomposition%2520quality%2520while%2520reducing%2520inference%2520time%2520from%2520seconds%2520to%250Amilliseconds%2520per%2520image%252C%2520and%2520completes%2520PBR%2520material%2520estimation%2520for%25203D%2520objects%2520in%250Aapproximately%25203%2520seconds.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2411.17515v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=SuperMat%3A%20Physically%20Consistent%20PBR%20Material%20Estimation%20at%20Interactive%0A%20%20Rates&entry.906535625=Yijia%20Hong%20and%20Yuan-Chen%20Guo%20and%20Ran%20Yi%20and%20Yulong%20Chen%20and%20Yan-Pei%20Cao%20and%20Lizhuang%20Ma&entry.1292438233=%20%20Decomposing%20physically-based%20materials%20from%20images%20into%20their%20constituent%0Aproperties%20remains%20challenging%2C%20particularly%20when%20maintaining%20both%0Acomputational%20efficiency%20and%20physical%20consistency.%20While%20recent%20diffusion-based%0Aapproaches%20have%20shown%20promise%2C%20they%20face%20substantial%20computational%20overhead%20due%0Ato%20multiple%20denoising%20steps%20and%20separate%20models%20for%20different%20material%0Aproperties.%20We%20present%20SuperMat%2C%20a%20single-step%20framework%20that%20achieves%0Ahigh-quality%20material%20decomposition%20with%20one-step%20inference.%20This%20enables%0Aend-to-end%20training%20with%20perceptual%20and%20re-render%20losses%20while%20decomposing%0Aalbedo%2C%20metallic%2C%20and%20roughness%20maps%20at%20millisecond-scale%20speeds.%20We%20further%0Aextend%20our%20framework%20to%203D%20objects%20through%20a%20UV%20refinement%20network%2C%20enabling%0Aconsistent%20material%20estimation%20across%20viewpoints%20while%20maintaining%20efficiency.%0AExperiments%20demonstrate%20that%20SuperMat%20achieves%20state-of-the-art%20PBR%20material%0Adecomposition%20quality%20while%20reducing%20inference%20time%20from%20seconds%20to%0Amilliseconds%20per%20image%2C%20and%20completes%20PBR%20material%20estimation%20for%203D%20objects%20in%0Aapproximately%203%20seconds.%0A&entry.1838667208=http%3A//arxiv.org/abs/2411.17515v1&entry.124074799=Read"},
{"title": "Video-Guided Foley Sound Generation with Multimodal Controls", "author": "Ziyang Chen and Prem Seetharaman and Bryan Russell and Oriol Nieto and David Bourgin and Andrew Owens and Justin Salamon", "abstract": "  Generating sound effects for videos often requires creating artistic sound\neffects that diverge significantly from real-life sources and flexible control\nin the sound design. To address this problem, we introduce MultiFoley, a model\ndesigned for video-guided sound generation that supports multimodal\nconditioning through text, audio, and video. Given a silent video and a text\nprompt, MultiFoley allows users to create clean sounds (e.g., skateboard wheels\nspinning without wind noise) or more whimsical sounds (e.g., making a lion's\nroar sound like a cat's meow). MultiFoley also allows users to choose reference\naudio from sound effects (SFX) libraries or partial videos for conditioning. A\nkey novelty of our model lies in its joint training on both internet video\ndatasets with low-quality audio and professional SFX recordings, enabling\nhigh-quality, full-bandwidth (48kHz) audio generation. Through automated\nevaluations and human studies, we demonstrate that MultiFoley successfully\ngenerates synchronized high-quality sounds across varied conditional inputs and\noutperforms existing methods. Please see our project page for video results:\nhttps://ificl.github.io/MultiFoley/\n", "link": "http://arxiv.org/abs/2411.17698v1", "date": "2024-11-26", "relevancy": 2.2224, "topK": [{"title": "Tuning-Free Noise Rectification for High Fidelity Image-to-Video\n  Generation", "link": "http://arxiv.org/abs/2403.02827v1", "similarity": 0.572}, {"title": "PhysGen: Rigid-Body Physics-Grounded Image-to-Video Generation", "link": "http://arxiv.org/abs/2409.18964v1", "similarity": 0.5487}, {"title": "Customize-A-Video: One-Shot Motion Customization of Text-to-Video\n  Diffusion Models", "link": "http://arxiv.org/abs/2402.14780v1", "similarity": 0.5319}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Video-Guided%20Foley%20Sound%20Generation%20with%20Multimodal%20Controls&body=Title%3A%20Video-Guided%20Foley%20Sound%20Generation%20with%20Multimodal%20Controls%0AAuthor%3A%20Ziyang%20Chen%20and%20Prem%20Seetharaman%20and%20Bryan%20Russell%20and%20Oriol%20Nieto%20and%20David%20Bourgin%20and%20Andrew%20Owens%20and%20Justin%20Salamon%0AAbstract%3A%20%20%20Generating%20sound%20effects%20for%20videos%20often%20requires%20creating%20artistic%20sound%0Aeffects%20that%20diverge%20significantly%20from%20real-life%20sources%20and%20flexible%20control%0Ain%20the%20sound%20design.%20To%20address%20this%20problem%2C%20we%20introduce%20MultiFoley%2C%20a%20model%0Adesigned%20for%20video-guided%20sound%20generation%20that%20supports%20multimodal%0Aconditioning%20through%20text%2C%20audio%2C%20and%20video.%20Given%20a%20silent%20video%20and%20a%20text%0Aprompt%2C%20MultiFoley%20allows%20users%20to%20create%20clean%20sounds%20%28e.g.%2C%20skateboard%20wheels%0Aspinning%20without%20wind%20noise%29%20or%20more%20whimsical%20sounds%20%28e.g.%2C%20making%20a%20lion%27s%0Aroar%20sound%20like%20a%20cat%27s%20meow%29.%20MultiFoley%20also%20allows%20users%20to%20choose%20reference%0Aaudio%20from%20sound%20effects%20%28SFX%29%20libraries%20or%20partial%20videos%20for%20conditioning.%20A%0Akey%20novelty%20of%20our%20model%20lies%20in%20its%20joint%20training%20on%20both%20internet%20video%0Adatasets%20with%20low-quality%20audio%20and%20professional%20SFX%20recordings%2C%20enabling%0Ahigh-quality%2C%20full-bandwidth%20%2848kHz%29%20audio%20generation.%20Through%20automated%0Aevaluations%20and%20human%20studies%2C%20we%20demonstrate%20that%20MultiFoley%20successfully%0Agenerates%20synchronized%20high-quality%20sounds%20across%20varied%20conditional%20inputs%20and%0Aoutperforms%20existing%20methods.%20Please%20see%20our%20project%20page%20for%20video%20results%3A%0Ahttps%3A//ificl.github.io/MultiFoley/%0A%0ALink%3A%20http%3A//arxiv.org/abs/2411.17698v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DVideo-Guided%2520Foley%2520Sound%2520Generation%2520with%2520Multimodal%2520Controls%26entry.906535625%3DZiyang%2520Chen%2520and%2520Prem%2520Seetharaman%2520and%2520Bryan%2520Russell%2520and%2520Oriol%2520Nieto%2520and%2520David%2520Bourgin%2520and%2520Andrew%2520Owens%2520and%2520Justin%2520Salamon%26entry.1292438233%3D%2520%2520Generating%2520sound%2520effects%2520for%2520videos%2520often%2520requires%2520creating%2520artistic%2520sound%250Aeffects%2520that%2520diverge%2520significantly%2520from%2520real-life%2520sources%2520and%2520flexible%2520control%250Ain%2520the%2520sound%2520design.%2520To%2520address%2520this%2520problem%252C%2520we%2520introduce%2520MultiFoley%252C%2520a%2520model%250Adesigned%2520for%2520video-guided%2520sound%2520generation%2520that%2520supports%2520multimodal%250Aconditioning%2520through%2520text%252C%2520audio%252C%2520and%2520video.%2520Given%2520a%2520silent%2520video%2520and%2520a%2520text%250Aprompt%252C%2520MultiFoley%2520allows%2520users%2520to%2520create%2520clean%2520sounds%2520%2528e.g.%252C%2520skateboard%2520wheels%250Aspinning%2520without%2520wind%2520noise%2529%2520or%2520more%2520whimsical%2520sounds%2520%2528e.g.%252C%2520making%2520a%2520lion%2527s%250Aroar%2520sound%2520like%2520a%2520cat%2527s%2520meow%2529.%2520MultiFoley%2520also%2520allows%2520users%2520to%2520choose%2520reference%250Aaudio%2520from%2520sound%2520effects%2520%2528SFX%2529%2520libraries%2520or%2520partial%2520videos%2520for%2520conditioning.%2520A%250Akey%2520novelty%2520of%2520our%2520model%2520lies%2520in%2520its%2520joint%2520training%2520on%2520both%2520internet%2520video%250Adatasets%2520with%2520low-quality%2520audio%2520and%2520professional%2520SFX%2520recordings%252C%2520enabling%250Ahigh-quality%252C%2520full-bandwidth%2520%252848kHz%2529%2520audio%2520generation.%2520Through%2520automated%250Aevaluations%2520and%2520human%2520studies%252C%2520we%2520demonstrate%2520that%2520MultiFoley%2520successfully%250Agenerates%2520synchronized%2520high-quality%2520sounds%2520across%2520varied%2520conditional%2520inputs%2520and%250Aoutperforms%2520existing%2520methods.%2520Please%2520see%2520our%2520project%2520page%2520for%2520video%2520results%253A%250Ahttps%253A//ificl.github.io/MultiFoley/%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2411.17698v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Video-Guided%20Foley%20Sound%20Generation%20with%20Multimodal%20Controls&entry.906535625=Ziyang%20Chen%20and%20Prem%20Seetharaman%20and%20Bryan%20Russell%20and%20Oriol%20Nieto%20and%20David%20Bourgin%20and%20Andrew%20Owens%20and%20Justin%20Salamon&entry.1292438233=%20%20Generating%20sound%20effects%20for%20videos%20often%20requires%20creating%20artistic%20sound%0Aeffects%20that%20diverge%20significantly%20from%20real-life%20sources%20and%20flexible%20control%0Ain%20the%20sound%20design.%20To%20address%20this%20problem%2C%20we%20introduce%20MultiFoley%2C%20a%20model%0Adesigned%20for%20video-guided%20sound%20generation%20that%20supports%20multimodal%0Aconditioning%20through%20text%2C%20audio%2C%20and%20video.%20Given%20a%20silent%20video%20and%20a%20text%0Aprompt%2C%20MultiFoley%20allows%20users%20to%20create%20clean%20sounds%20%28e.g.%2C%20skateboard%20wheels%0Aspinning%20without%20wind%20noise%29%20or%20more%20whimsical%20sounds%20%28e.g.%2C%20making%20a%20lion%27s%0Aroar%20sound%20like%20a%20cat%27s%20meow%29.%20MultiFoley%20also%20allows%20users%20to%20choose%20reference%0Aaudio%20from%20sound%20effects%20%28SFX%29%20libraries%20or%20partial%20videos%20for%20conditioning.%20A%0Akey%20novelty%20of%20our%20model%20lies%20in%20its%20joint%20training%20on%20both%20internet%20video%0Adatasets%20with%20low-quality%20audio%20and%20professional%20SFX%20recordings%2C%20enabling%0Ahigh-quality%2C%20full-bandwidth%20%2848kHz%29%20audio%20generation.%20Through%20automated%0Aevaluations%20and%20human%20studies%2C%20we%20demonstrate%20that%20MultiFoley%20successfully%0Agenerates%20synchronized%20high-quality%20sounds%20across%20varied%20conditional%20inputs%20and%0Aoutperforms%20existing%20methods.%20Please%20see%20our%20project%20page%20for%20video%20results%3A%0Ahttps%3A//ificl.github.io/MultiFoley/%0A&entry.1838667208=http%3A//arxiv.org/abs/2411.17698v1&entry.124074799=Read"},
{"title": "TAFM-Net: A Novel Approach to Skin Lesion Segmentation Using Transformer\n  Attention and Focal Modulation", "author": "Tariq M Khan and Dawn Lin and Shahzaib Iqbal and Eirk Meijering", "abstract": "  Incorporating modern computer vision techniques into clinical protocols shows\npromise in improving skin lesion segmentation. The U-Net architecture has been\na key model in this area, iteratively improved to address challenges arising\nfrom the heterogeneity of dermatologic images due to varying clinical settings,\nlighting, patient attributes, and hair density. To further improve skin lesion\nsegmentation, we developed TAFM-Net, an innovative model leveraging\nself-adaptive transformer attention (TA) coupled with focal modulation (FM).\nOur model integrates an EfficientNetV2B1 encoder, which employs TA to enhance\nspatial and channel-related saliency, while a densely connected decoder\nintegrates FM within skip connections, enhancing feature emphasis, segmentation\nperformance, and interpretability crucial for medical image analysis. A novel\ndynamic loss function amalgamates region and boundary information, guiding\neffective model training. Our model achieves competitive performance, with\nJaccard coefficients of 93.64\\%, 86.88\\% and 92.88\\% in the ISIC2016, ISIC2017\nand ISIC2018 datasets, respectively, demonstrating its potential in real-world\nscenarios.\n", "link": "http://arxiv.org/abs/2411.17556v1", "date": "2024-11-26", "relevancy": 2.2181, "topK": [{"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5751}, {"title": "GST: Precise 3D Human Body from a Single Image with Gaussian Splatting\n  Transformers", "link": "http://arxiv.org/abs/2409.04196v1", "similarity": 0.5413}, {"title": "SDXS: Real-Time One-Step Latent Diffusion Models with Image Conditions", "link": "http://arxiv.org/abs/2403.16627v1", "similarity": 0.5362}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20TAFM-Net%3A%20A%20Novel%20Approach%20to%20Skin%20Lesion%20Segmentation%20Using%20Transformer%0A%20%20Attention%20and%20Focal%20Modulation&body=Title%3A%20TAFM-Net%3A%20A%20Novel%20Approach%20to%20Skin%20Lesion%20Segmentation%20Using%20Transformer%0A%20%20Attention%20and%20Focal%20Modulation%0AAuthor%3A%20Tariq%20M%20Khan%20and%20Dawn%20Lin%20and%20Shahzaib%20Iqbal%20and%20Eirk%20Meijering%0AAbstract%3A%20%20%20Incorporating%20modern%20computer%20vision%20techniques%20into%20clinical%20protocols%20shows%0Apromise%20in%20improving%20skin%20lesion%20segmentation.%20The%20U-Net%20architecture%20has%20been%0Aa%20key%20model%20in%20this%20area%2C%20iteratively%20improved%20to%20address%20challenges%20arising%0Afrom%20the%20heterogeneity%20of%20dermatologic%20images%20due%20to%20varying%20clinical%20settings%2C%0Alighting%2C%20patient%20attributes%2C%20and%20hair%20density.%20To%20further%20improve%20skin%20lesion%0Asegmentation%2C%20we%20developed%20TAFM-Net%2C%20an%20innovative%20model%20leveraging%0Aself-adaptive%20transformer%20attention%20%28TA%29%20coupled%20with%20focal%20modulation%20%28FM%29.%0AOur%20model%20integrates%20an%20EfficientNetV2B1%20encoder%2C%20which%20employs%20TA%20to%20enhance%0Aspatial%20and%20channel-related%20saliency%2C%20while%20a%20densely%20connected%20decoder%0Aintegrates%20FM%20within%20skip%20connections%2C%20enhancing%20feature%20emphasis%2C%20segmentation%0Aperformance%2C%20and%20interpretability%20crucial%20for%20medical%20image%20analysis.%20A%20novel%0Adynamic%20loss%20function%20amalgamates%20region%20and%20boundary%20information%2C%20guiding%0Aeffective%20model%20training.%20Our%20model%20achieves%20competitive%20performance%2C%20with%0AJaccard%20coefficients%20of%2093.64%5C%25%2C%2086.88%5C%25%20and%2092.88%5C%25%20in%20the%20ISIC2016%2C%20ISIC2017%0Aand%20ISIC2018%20datasets%2C%20respectively%2C%20demonstrating%20its%20potential%20in%20real-world%0Ascenarios.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2411.17556v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DTAFM-Net%253A%2520A%2520Novel%2520Approach%2520to%2520Skin%2520Lesion%2520Segmentation%2520Using%2520Transformer%250A%2520%2520Attention%2520and%2520Focal%2520Modulation%26entry.906535625%3DTariq%2520M%2520Khan%2520and%2520Dawn%2520Lin%2520and%2520Shahzaib%2520Iqbal%2520and%2520Eirk%2520Meijering%26entry.1292438233%3D%2520%2520Incorporating%2520modern%2520computer%2520vision%2520techniques%2520into%2520clinical%2520protocols%2520shows%250Apromise%2520in%2520improving%2520skin%2520lesion%2520segmentation.%2520The%2520U-Net%2520architecture%2520has%2520been%250Aa%2520key%2520model%2520in%2520this%2520area%252C%2520iteratively%2520improved%2520to%2520address%2520challenges%2520arising%250Afrom%2520the%2520heterogeneity%2520of%2520dermatologic%2520images%2520due%2520to%2520varying%2520clinical%2520settings%252C%250Alighting%252C%2520patient%2520attributes%252C%2520and%2520hair%2520density.%2520To%2520further%2520improve%2520skin%2520lesion%250Asegmentation%252C%2520we%2520developed%2520TAFM-Net%252C%2520an%2520innovative%2520model%2520leveraging%250Aself-adaptive%2520transformer%2520attention%2520%2528TA%2529%2520coupled%2520with%2520focal%2520modulation%2520%2528FM%2529.%250AOur%2520model%2520integrates%2520an%2520EfficientNetV2B1%2520encoder%252C%2520which%2520employs%2520TA%2520to%2520enhance%250Aspatial%2520and%2520channel-related%2520saliency%252C%2520while%2520a%2520densely%2520connected%2520decoder%250Aintegrates%2520FM%2520within%2520skip%2520connections%252C%2520enhancing%2520feature%2520emphasis%252C%2520segmentation%250Aperformance%252C%2520and%2520interpretability%2520crucial%2520for%2520medical%2520image%2520analysis.%2520A%2520novel%250Adynamic%2520loss%2520function%2520amalgamates%2520region%2520and%2520boundary%2520information%252C%2520guiding%250Aeffective%2520model%2520training.%2520Our%2520model%2520achieves%2520competitive%2520performance%252C%2520with%250AJaccard%2520coefficients%2520of%252093.64%255C%2525%252C%252086.88%255C%2525%2520and%252092.88%255C%2525%2520in%2520the%2520ISIC2016%252C%2520ISIC2017%250Aand%2520ISIC2018%2520datasets%252C%2520respectively%252C%2520demonstrating%2520its%2520potential%2520in%2520real-world%250Ascenarios.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2411.17556v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=TAFM-Net%3A%20A%20Novel%20Approach%20to%20Skin%20Lesion%20Segmentation%20Using%20Transformer%0A%20%20Attention%20and%20Focal%20Modulation&entry.906535625=Tariq%20M%20Khan%20and%20Dawn%20Lin%20and%20Shahzaib%20Iqbal%20and%20Eirk%20Meijering&entry.1292438233=%20%20Incorporating%20modern%20computer%20vision%20techniques%20into%20clinical%20protocols%20shows%0Apromise%20in%20improving%20skin%20lesion%20segmentation.%20The%20U-Net%20architecture%20has%20been%0Aa%20key%20model%20in%20this%20area%2C%20iteratively%20improved%20to%20address%20challenges%20arising%0Afrom%20the%20heterogeneity%20of%20dermatologic%20images%20due%20to%20varying%20clinical%20settings%2C%0Alighting%2C%20patient%20attributes%2C%20and%20hair%20density.%20To%20further%20improve%20skin%20lesion%0Asegmentation%2C%20we%20developed%20TAFM-Net%2C%20an%20innovative%20model%20leveraging%0Aself-adaptive%20transformer%20attention%20%28TA%29%20coupled%20with%20focal%20modulation%20%28FM%29.%0AOur%20model%20integrates%20an%20EfficientNetV2B1%20encoder%2C%20which%20employs%20TA%20to%20enhance%0Aspatial%20and%20channel-related%20saliency%2C%20while%20a%20densely%20connected%20decoder%0Aintegrates%20FM%20within%20skip%20connections%2C%20enhancing%20feature%20emphasis%2C%20segmentation%0Aperformance%2C%20and%20interpretability%20crucial%20for%20medical%20image%20analysis.%20A%20novel%0Adynamic%20loss%20function%20amalgamates%20region%20and%20boundary%20information%2C%20guiding%0Aeffective%20model%20training.%20Our%20model%20achieves%20competitive%20performance%2C%20with%0AJaccard%20coefficients%20of%2093.64%5C%25%2C%2086.88%5C%25%20and%2092.88%5C%25%20in%20the%20ISIC2016%2C%20ISIC2017%0Aand%20ISIC2018%20datasets%2C%20respectively%2C%20demonstrating%20its%20potential%20in%20real-world%0Ascenarios.%0A&entry.1838667208=http%3A//arxiv.org/abs/2411.17556v1&entry.124074799=Read"},
{"title": "Joint Combinatorial Node Selection and Resource Allocations in the\n  Lightning Network using Attention-based Reinforcement Learning", "author": "Mahdi Salahshour and Amirahmad Shafiee and Mojtaba Tefagh", "abstract": "  The Lightning Network (LN) has emerged as a second-layer solution to\nBitcoin's scalability challenges. The rise of Payment Channel Networks (PCNs)\nand their specific mechanisms incentivize individuals to join the network for\nprofit-making opportunities. According to the latest statistics, the total\nvalue locked within the Lightning Network is approximately \\$500 million.\nMeanwhile, joining the LN with the profit-making incentives presents several\nobstacles, as it involves solving a complex combinatorial problem that\nencompasses both discrete and continuous control variables related to node\nselection and resource allocation, respectively. Current research inadequately\ncaptures the critical role of resource allocation and lacks realistic\nsimulations of the LN routing mechanism. In this paper, we propose a Deep\nReinforcement Learning (DRL) framework, enhanced by the power of transformers,\nto address the Joint Combinatorial Node Selection and Resource Allocation\n(JCNSRA) problem. We have improved upon an existing environment by introducing\nmodules that enhance its routing mechanism, thereby narrowing the gap with the\nactual LN routing system and ensuring compatibility with the JCNSRA problem. We\ncompare our model against several baselines and heuristics, demonstrating its\nsuperior performance across various settings. Additionally, we address concerns\nregarding centralization in the LN by deploying our agent within the network\nand monitoring the centrality measures of the evolved graph. Our findings\nsuggest not only an absence of conflict between LN's decentralization goals and\nindividuals' revenue-maximization incentives but also a positive association\nbetween the two.\n", "link": "http://arxiv.org/abs/2411.17353v1", "date": "2024-11-26", "relevancy": 2.2093, "topK": [{"title": "Boosted dynamic neural networks", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/26302", "similarity": 0.4507}, {"title": "Self-supervised Photographic Image Layout Representation Learning", "link": "http://arxiv.org/abs/2403.03740v1", "similarity": 0.4403}, {"title": "Safe and efficient training of a control agent", "link": "https://patents.google.com/patent/US11709462B2/en", "similarity": 0.4346}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Joint%20Combinatorial%20Node%20Selection%20and%20Resource%20Allocations%20in%20the%0A%20%20Lightning%20Network%20using%20Attention-based%20Reinforcement%20Learning&body=Title%3A%20Joint%20Combinatorial%20Node%20Selection%20and%20Resource%20Allocations%20in%20the%0A%20%20Lightning%20Network%20using%20Attention-based%20Reinforcement%20Learning%0AAuthor%3A%20Mahdi%20Salahshour%20and%20Amirahmad%20Shafiee%20and%20Mojtaba%20Tefagh%0AAbstract%3A%20%20%20The%20Lightning%20Network%20%28LN%29%20has%20emerged%20as%20a%20second-layer%20solution%20to%0ABitcoin%27s%20scalability%20challenges.%20The%20rise%20of%20Payment%20Channel%20Networks%20%28PCNs%29%0Aand%20their%20specific%20mechanisms%20incentivize%20individuals%20to%20join%20the%20network%20for%0Aprofit-making%20opportunities.%20According%20to%20the%20latest%20statistics%2C%20the%20total%0Avalue%20locked%20within%20the%20Lightning%20Network%20is%20approximately%20%5C%24500%20million.%0AMeanwhile%2C%20joining%20the%20LN%20with%20the%20profit-making%20incentives%20presents%20several%0Aobstacles%2C%20as%20it%20involves%20solving%20a%20complex%20combinatorial%20problem%20that%0Aencompasses%20both%20discrete%20and%20continuous%20control%20variables%20related%20to%20node%0Aselection%20and%20resource%20allocation%2C%20respectively.%20Current%20research%20inadequately%0Acaptures%20the%20critical%20role%20of%20resource%20allocation%20and%20lacks%20realistic%0Asimulations%20of%20the%20LN%20routing%20mechanism.%20In%20this%20paper%2C%20we%20propose%20a%20Deep%0AReinforcement%20Learning%20%28DRL%29%20framework%2C%20enhanced%20by%20the%20power%20of%20transformers%2C%0Ato%20address%20the%20Joint%20Combinatorial%20Node%20Selection%20and%20Resource%20Allocation%0A%28JCNSRA%29%20problem.%20We%20have%20improved%20upon%20an%20existing%20environment%20by%20introducing%0Amodules%20that%20enhance%20its%20routing%20mechanism%2C%20thereby%20narrowing%20the%20gap%20with%20the%0Aactual%20LN%20routing%20system%20and%20ensuring%20compatibility%20with%20the%20JCNSRA%20problem.%20We%0Acompare%20our%20model%20against%20several%20baselines%20and%20heuristics%2C%20demonstrating%20its%0Asuperior%20performance%20across%20various%20settings.%20Additionally%2C%20we%20address%20concerns%0Aregarding%20centralization%20in%20the%20LN%20by%20deploying%20our%20agent%20within%20the%20network%0Aand%20monitoring%20the%20centrality%20measures%20of%20the%20evolved%20graph.%20Our%20findings%0Asuggest%20not%20only%20an%20absence%20of%20conflict%20between%20LN%27s%20decentralization%20goals%20and%0Aindividuals%27%20revenue-maximization%20incentives%20but%20also%20a%20positive%20association%0Abetween%20the%20two.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2411.17353v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DJoint%2520Combinatorial%2520Node%2520Selection%2520and%2520Resource%2520Allocations%2520in%2520the%250A%2520%2520Lightning%2520Network%2520using%2520Attention-based%2520Reinforcement%2520Learning%26entry.906535625%3DMahdi%2520Salahshour%2520and%2520Amirahmad%2520Shafiee%2520and%2520Mojtaba%2520Tefagh%26entry.1292438233%3D%2520%2520The%2520Lightning%2520Network%2520%2528LN%2529%2520has%2520emerged%2520as%2520a%2520second-layer%2520solution%2520to%250ABitcoin%2527s%2520scalability%2520challenges.%2520The%2520rise%2520of%2520Payment%2520Channel%2520Networks%2520%2528PCNs%2529%250Aand%2520their%2520specific%2520mechanisms%2520incentivize%2520individuals%2520to%2520join%2520the%2520network%2520for%250Aprofit-making%2520opportunities.%2520According%2520to%2520the%2520latest%2520statistics%252C%2520the%2520total%250Avalue%2520locked%2520within%2520the%2520Lightning%2520Network%2520is%2520approximately%2520%255C%2524500%2520million.%250AMeanwhile%252C%2520joining%2520the%2520LN%2520with%2520the%2520profit-making%2520incentives%2520presents%2520several%250Aobstacles%252C%2520as%2520it%2520involves%2520solving%2520a%2520complex%2520combinatorial%2520problem%2520that%250Aencompasses%2520both%2520discrete%2520and%2520continuous%2520control%2520variables%2520related%2520to%2520node%250Aselection%2520and%2520resource%2520allocation%252C%2520respectively.%2520Current%2520research%2520inadequately%250Acaptures%2520the%2520critical%2520role%2520of%2520resource%2520allocation%2520and%2520lacks%2520realistic%250Asimulations%2520of%2520the%2520LN%2520routing%2520mechanism.%2520In%2520this%2520paper%252C%2520we%2520propose%2520a%2520Deep%250AReinforcement%2520Learning%2520%2528DRL%2529%2520framework%252C%2520enhanced%2520by%2520the%2520power%2520of%2520transformers%252C%250Ato%2520address%2520the%2520Joint%2520Combinatorial%2520Node%2520Selection%2520and%2520Resource%2520Allocation%250A%2528JCNSRA%2529%2520problem.%2520We%2520have%2520improved%2520upon%2520an%2520existing%2520environment%2520by%2520introducing%250Amodules%2520that%2520enhance%2520its%2520routing%2520mechanism%252C%2520thereby%2520narrowing%2520the%2520gap%2520with%2520the%250Aactual%2520LN%2520routing%2520system%2520and%2520ensuring%2520compatibility%2520with%2520the%2520JCNSRA%2520problem.%2520We%250Acompare%2520our%2520model%2520against%2520several%2520baselines%2520and%2520heuristics%252C%2520demonstrating%2520its%250Asuperior%2520performance%2520across%2520various%2520settings.%2520Additionally%252C%2520we%2520address%2520concerns%250Aregarding%2520centralization%2520in%2520the%2520LN%2520by%2520deploying%2520our%2520agent%2520within%2520the%2520network%250Aand%2520monitoring%2520the%2520centrality%2520measures%2520of%2520the%2520evolved%2520graph.%2520Our%2520findings%250Asuggest%2520not%2520only%2520an%2520absence%2520of%2520conflict%2520between%2520LN%2527s%2520decentralization%2520goals%2520and%250Aindividuals%2527%2520revenue-maximization%2520incentives%2520but%2520also%2520a%2520positive%2520association%250Abetween%2520the%2520two.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2411.17353v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Joint%20Combinatorial%20Node%20Selection%20and%20Resource%20Allocations%20in%20the%0A%20%20Lightning%20Network%20using%20Attention-based%20Reinforcement%20Learning&entry.906535625=Mahdi%20Salahshour%20and%20Amirahmad%20Shafiee%20and%20Mojtaba%20Tefagh&entry.1292438233=%20%20The%20Lightning%20Network%20%28LN%29%20has%20emerged%20as%20a%20second-layer%20solution%20to%0ABitcoin%27s%20scalability%20challenges.%20The%20rise%20of%20Payment%20Channel%20Networks%20%28PCNs%29%0Aand%20their%20specific%20mechanisms%20incentivize%20individuals%20to%20join%20the%20network%20for%0Aprofit-making%20opportunities.%20According%20to%20the%20latest%20statistics%2C%20the%20total%0Avalue%20locked%20within%20the%20Lightning%20Network%20is%20approximately%20%5C%24500%20million.%0AMeanwhile%2C%20joining%20the%20LN%20with%20the%20profit-making%20incentives%20presents%20several%0Aobstacles%2C%20as%20it%20involves%20solving%20a%20complex%20combinatorial%20problem%20that%0Aencompasses%20both%20discrete%20and%20continuous%20control%20variables%20related%20to%20node%0Aselection%20and%20resource%20allocation%2C%20respectively.%20Current%20research%20inadequately%0Acaptures%20the%20critical%20role%20of%20resource%20allocation%20and%20lacks%20realistic%0Asimulations%20of%20the%20LN%20routing%20mechanism.%20In%20this%20paper%2C%20we%20propose%20a%20Deep%0AReinforcement%20Learning%20%28DRL%29%20framework%2C%20enhanced%20by%20the%20power%20of%20transformers%2C%0Ato%20address%20the%20Joint%20Combinatorial%20Node%20Selection%20and%20Resource%20Allocation%0A%28JCNSRA%29%20problem.%20We%20have%20improved%20upon%20an%20existing%20environment%20by%20introducing%0Amodules%20that%20enhance%20its%20routing%20mechanism%2C%20thereby%20narrowing%20the%20gap%20with%20the%0Aactual%20LN%20routing%20system%20and%20ensuring%20compatibility%20with%20the%20JCNSRA%20problem.%20We%0Acompare%20our%20model%20against%20several%20baselines%20and%20heuristics%2C%20demonstrating%20its%0Asuperior%20performance%20across%20various%20settings.%20Additionally%2C%20we%20address%20concerns%0Aregarding%20centralization%20in%20the%20LN%20by%20deploying%20our%20agent%20within%20the%20network%0Aand%20monitoring%20the%20centrality%20measures%20of%20the%20evolved%20graph.%20Our%20findings%0Asuggest%20not%20only%20an%20absence%20of%20conflict%20between%20LN%27s%20decentralization%20goals%20and%0Aindividuals%27%20revenue-maximization%20incentives%20but%20also%20a%20positive%20association%0Abetween%20the%20two.%0A&entry.1838667208=http%3A//arxiv.org/abs/2411.17353v1&entry.124074799=Read"},
{"title": "Brainformer: Mimic Human Visual Brain Functions to Machine Vision Models\n  via fMRI", "author": "Xuan-Bac Nguyen and Xin Li and Pawan Sinha and Samee U. Khan and Khoa Luu", "abstract": "  Human perception plays a vital role in forming beliefs and understanding\nreality. A deeper understanding of brain functionality will lead to the\ndevelopment of novel deep neural networks. In this work, we introduce a novel\nframework named Brainformer, a straightforward yet effective Transformer-based\nframework, to analyze Functional Magnetic Resonance Imaging (fMRI) patterns in\nthe human perception system from a machine-learning perspective. Specifically,\nwe present the Multi-scale fMRI Transformer to explore brain activity patterns\nthrough fMRI signals. This architecture includes a simple yet efficient module\nfor high-dimensional fMRI signal encoding and incorporates a novel embedding\ntechnique called 3D Voxels Embedding. Secondly, drawing inspiration from the\nfunctionality of the brain's Region of Interest, we introduce a novel loss\nfunction called Brain fMRI Guidance Loss. This loss function mimics brain\nactivity patterns from these regions in the deep neural network using fMRI\ndata. This work introduces a prospective approach to transferring knowledge\nfrom human perception to neural networks. Our experiments demonstrate that\nleveraging fMRI information allows the machine vision model to achieve results\ncomparable to State-of-the-Art methods in various image recognition tasks.\n", "link": "http://arxiv.org/abs/2312.00236v4", "date": "2024-11-26", "relevancy": 2.2052, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5529}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5529}, {"title": "Scaling Rectified Flow Transformers for High-Resolution Image Synthesis", "link": "http://arxiv.org/abs/2403.03206v1", "similarity": 0.5435}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Brainformer%3A%20Mimic%20Human%20Visual%20Brain%20Functions%20to%20Machine%20Vision%20Models%0A%20%20via%20fMRI&body=Title%3A%20Brainformer%3A%20Mimic%20Human%20Visual%20Brain%20Functions%20to%20Machine%20Vision%20Models%0A%20%20via%20fMRI%0AAuthor%3A%20Xuan-Bac%20Nguyen%20and%20Xin%20Li%20and%20Pawan%20Sinha%20and%20Samee%20U.%20Khan%20and%20Khoa%20Luu%0AAbstract%3A%20%20%20Human%20perception%20plays%20a%20vital%20role%20in%20forming%20beliefs%20and%20understanding%0Areality.%20A%20deeper%20understanding%20of%20brain%20functionality%20will%20lead%20to%20the%0Adevelopment%20of%20novel%20deep%20neural%20networks.%20In%20this%20work%2C%20we%20introduce%20a%20novel%0Aframework%20named%20Brainformer%2C%20a%20straightforward%20yet%20effective%20Transformer-based%0Aframework%2C%20to%20analyze%20Functional%20Magnetic%20Resonance%20Imaging%20%28fMRI%29%20patterns%20in%0Athe%20human%20perception%20system%20from%20a%20machine-learning%20perspective.%20Specifically%2C%0Awe%20present%20the%20Multi-scale%20fMRI%20Transformer%20to%20explore%20brain%20activity%20patterns%0Athrough%20fMRI%20signals.%20This%20architecture%20includes%20a%20simple%20yet%20efficient%20module%0Afor%20high-dimensional%20fMRI%20signal%20encoding%20and%20incorporates%20a%20novel%20embedding%0Atechnique%20called%203D%20Voxels%20Embedding.%20Secondly%2C%20drawing%20inspiration%20from%20the%0Afunctionality%20of%20the%20brain%27s%20Region%20of%20Interest%2C%20we%20introduce%20a%20novel%20loss%0Afunction%20called%20Brain%20fMRI%20Guidance%20Loss.%20This%20loss%20function%20mimics%20brain%0Aactivity%20patterns%20from%20these%20regions%20in%20the%20deep%20neural%20network%20using%20fMRI%0Adata.%20This%20work%20introduces%20a%20prospective%20approach%20to%20transferring%20knowledge%0Afrom%20human%20perception%20to%20neural%20networks.%20Our%20experiments%20demonstrate%20that%0Aleveraging%20fMRI%20information%20allows%20the%20machine%20vision%20model%20to%20achieve%20results%0Acomparable%20to%20State-of-the-Art%20methods%20in%20various%20image%20recognition%20tasks.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2312.00236v4%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DBrainformer%253A%2520Mimic%2520Human%2520Visual%2520Brain%2520Functions%2520to%2520Machine%2520Vision%2520Models%250A%2520%2520via%2520fMRI%26entry.906535625%3DXuan-Bac%2520Nguyen%2520and%2520Xin%2520Li%2520and%2520Pawan%2520Sinha%2520and%2520Samee%2520U.%2520Khan%2520and%2520Khoa%2520Luu%26entry.1292438233%3D%2520%2520Human%2520perception%2520plays%2520a%2520vital%2520role%2520in%2520forming%2520beliefs%2520and%2520understanding%250Areality.%2520A%2520deeper%2520understanding%2520of%2520brain%2520functionality%2520will%2520lead%2520to%2520the%250Adevelopment%2520of%2520novel%2520deep%2520neural%2520networks.%2520In%2520this%2520work%252C%2520we%2520introduce%2520a%2520novel%250Aframework%2520named%2520Brainformer%252C%2520a%2520straightforward%2520yet%2520effective%2520Transformer-based%250Aframework%252C%2520to%2520analyze%2520Functional%2520Magnetic%2520Resonance%2520Imaging%2520%2528fMRI%2529%2520patterns%2520in%250Athe%2520human%2520perception%2520system%2520from%2520a%2520machine-learning%2520perspective.%2520Specifically%252C%250Awe%2520present%2520the%2520Multi-scale%2520fMRI%2520Transformer%2520to%2520explore%2520brain%2520activity%2520patterns%250Athrough%2520fMRI%2520signals.%2520This%2520architecture%2520includes%2520a%2520simple%2520yet%2520efficient%2520module%250Afor%2520high-dimensional%2520fMRI%2520signal%2520encoding%2520and%2520incorporates%2520a%2520novel%2520embedding%250Atechnique%2520called%25203D%2520Voxels%2520Embedding.%2520Secondly%252C%2520drawing%2520inspiration%2520from%2520the%250Afunctionality%2520of%2520the%2520brain%2527s%2520Region%2520of%2520Interest%252C%2520we%2520introduce%2520a%2520novel%2520loss%250Afunction%2520called%2520Brain%2520fMRI%2520Guidance%2520Loss.%2520This%2520loss%2520function%2520mimics%2520brain%250Aactivity%2520patterns%2520from%2520these%2520regions%2520in%2520the%2520deep%2520neural%2520network%2520using%2520fMRI%250Adata.%2520This%2520work%2520introduces%2520a%2520prospective%2520approach%2520to%2520transferring%2520knowledge%250Afrom%2520human%2520perception%2520to%2520neural%2520networks.%2520Our%2520experiments%2520demonstrate%2520that%250Aleveraging%2520fMRI%2520information%2520allows%2520the%2520machine%2520vision%2520model%2520to%2520achieve%2520results%250Acomparable%2520to%2520State-of-the-Art%2520methods%2520in%2520various%2520image%2520recognition%2520tasks.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2312.00236v4%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Brainformer%3A%20Mimic%20Human%20Visual%20Brain%20Functions%20to%20Machine%20Vision%20Models%0A%20%20via%20fMRI&entry.906535625=Xuan-Bac%20Nguyen%20and%20Xin%20Li%20and%20Pawan%20Sinha%20and%20Samee%20U.%20Khan%20and%20Khoa%20Luu&entry.1292438233=%20%20Human%20perception%20plays%20a%20vital%20role%20in%20forming%20beliefs%20and%20understanding%0Areality.%20A%20deeper%20understanding%20of%20brain%20functionality%20will%20lead%20to%20the%0Adevelopment%20of%20novel%20deep%20neural%20networks.%20In%20this%20work%2C%20we%20introduce%20a%20novel%0Aframework%20named%20Brainformer%2C%20a%20straightforward%20yet%20effective%20Transformer-based%0Aframework%2C%20to%20analyze%20Functional%20Magnetic%20Resonance%20Imaging%20%28fMRI%29%20patterns%20in%0Athe%20human%20perception%20system%20from%20a%20machine-learning%20perspective.%20Specifically%2C%0Awe%20present%20the%20Multi-scale%20fMRI%20Transformer%20to%20explore%20brain%20activity%20patterns%0Athrough%20fMRI%20signals.%20This%20architecture%20includes%20a%20simple%20yet%20efficient%20module%0Afor%20high-dimensional%20fMRI%20signal%20encoding%20and%20incorporates%20a%20novel%20embedding%0Atechnique%20called%203D%20Voxels%20Embedding.%20Secondly%2C%20drawing%20inspiration%20from%20the%0Afunctionality%20of%20the%20brain%27s%20Region%20of%20Interest%2C%20we%20introduce%20a%20novel%20loss%0Afunction%20called%20Brain%20fMRI%20Guidance%20Loss.%20This%20loss%20function%20mimics%20brain%0Aactivity%20patterns%20from%20these%20regions%20in%20the%20deep%20neural%20network%20using%20fMRI%0Adata.%20This%20work%20introduces%20a%20prospective%20approach%20to%20transferring%20knowledge%0Afrom%20human%20perception%20to%20neural%20networks.%20Our%20experiments%20demonstrate%20that%0Aleveraging%20fMRI%20information%20allows%20the%20machine%20vision%20model%20to%20achieve%20results%0Acomparable%20to%20State-of-the-Art%20methods%20in%20various%20image%20recognition%20tasks.%0A&entry.1838667208=http%3A//arxiv.org/abs/2312.00236v4&entry.124074799=Read"},
{"title": "InsightEdit: Towards Better Instruction Following for Image Editing", "author": "Yingjing Xu and Jie Kong and Jiazhi Wang and Xiao Pan and Bo Lin and Qiang Liu", "abstract": "  In this paper, we focus on the task of instruction-based image editing.\nPrevious works like InstructPix2Pix, InstructDiffusion, and SmartEdit have\nexplored end-to-end editing. However, two limitations still remain: First,\nexisting datasets suffer from low resolution, poor background consistency, and\noverly simplistic instructions. Second, current approaches mainly condition on\nthe text while the rich image information is underexplored, therefore inferior\nin complex instruction following and maintaining background consistency.\nTargeting these issues, we first curated the AdvancedEdit dataset using a novel\ndata construction pipeline, formulating a large-scale dataset with high visual\nquality, complex instructions, and good background consistency. Then, to\nfurther inject the rich image information, we introduce a two-stream bridging\nmechanism utilizing both the textual and visual features reasoned by the\npowerful Multimodal Large Language Models (MLLM) to guide the image editing\nprocess more precisely. Extensive results demonstrate that our approach,\nInsightEdit, achieves state-of-the-art performance, excelling in complex\ninstruction following and maintaining high background consistency with the\noriginal image.\n", "link": "http://arxiv.org/abs/2411.17323v1", "date": "2024-11-26", "relevancy": 2.2043, "topK": [{"title": "Multimodal-Conditioned Latent Diffusion Models for Fashion Image Editing", "link": "http://arxiv.org/abs/2403.14828v2", "similarity": 0.5948}, {"title": "MiraGe: Editable 2D Images using Gaussian Splatting", "link": "http://arxiv.org/abs/2410.01521v1", "similarity": 0.5463}, {"title": "Self-supervised Photographic Image Layout Representation Learning", "link": "http://arxiv.org/abs/2403.03740v1", "similarity": 0.5384}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20InsightEdit%3A%20Towards%20Better%20Instruction%20Following%20for%20Image%20Editing&body=Title%3A%20InsightEdit%3A%20Towards%20Better%20Instruction%20Following%20for%20Image%20Editing%0AAuthor%3A%20Yingjing%20Xu%20and%20Jie%20Kong%20and%20Jiazhi%20Wang%20and%20Xiao%20Pan%20and%20Bo%20Lin%20and%20Qiang%20Liu%0AAbstract%3A%20%20%20In%20this%20paper%2C%20we%20focus%20on%20the%20task%20of%20instruction-based%20image%20editing.%0APrevious%20works%20like%20InstructPix2Pix%2C%20InstructDiffusion%2C%20and%20SmartEdit%20have%0Aexplored%20end-to-end%20editing.%20However%2C%20two%20limitations%20still%20remain%3A%20First%2C%0Aexisting%20datasets%20suffer%20from%20low%20resolution%2C%20poor%20background%20consistency%2C%20and%0Aoverly%20simplistic%20instructions.%20Second%2C%20current%20approaches%20mainly%20condition%20on%0Athe%20text%20while%20the%20rich%20image%20information%20is%20underexplored%2C%20therefore%20inferior%0Ain%20complex%20instruction%20following%20and%20maintaining%20background%20consistency.%0ATargeting%20these%20issues%2C%20we%20first%20curated%20the%20AdvancedEdit%20dataset%20using%20a%20novel%0Adata%20construction%20pipeline%2C%20formulating%20a%20large-scale%20dataset%20with%20high%20visual%0Aquality%2C%20complex%20instructions%2C%20and%20good%20background%20consistency.%20Then%2C%20to%0Afurther%20inject%20the%20rich%20image%20information%2C%20we%20introduce%20a%20two-stream%20bridging%0Amechanism%20utilizing%20both%20the%20textual%20and%20visual%20features%20reasoned%20by%20the%0Apowerful%20Multimodal%20Large%20Language%20Models%20%28MLLM%29%20to%20guide%20the%20image%20editing%0Aprocess%20more%20precisely.%20Extensive%20results%20demonstrate%20that%20our%20approach%2C%0AInsightEdit%2C%20achieves%20state-of-the-art%20performance%2C%20excelling%20in%20complex%0Ainstruction%20following%20and%20maintaining%20high%20background%20consistency%20with%20the%0Aoriginal%20image.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2411.17323v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DInsightEdit%253A%2520Towards%2520Better%2520Instruction%2520Following%2520for%2520Image%2520Editing%26entry.906535625%3DYingjing%2520Xu%2520and%2520Jie%2520Kong%2520and%2520Jiazhi%2520Wang%2520and%2520Xiao%2520Pan%2520and%2520Bo%2520Lin%2520and%2520Qiang%2520Liu%26entry.1292438233%3D%2520%2520In%2520this%2520paper%252C%2520we%2520focus%2520on%2520the%2520task%2520of%2520instruction-based%2520image%2520editing.%250APrevious%2520works%2520like%2520InstructPix2Pix%252C%2520InstructDiffusion%252C%2520and%2520SmartEdit%2520have%250Aexplored%2520end-to-end%2520editing.%2520However%252C%2520two%2520limitations%2520still%2520remain%253A%2520First%252C%250Aexisting%2520datasets%2520suffer%2520from%2520low%2520resolution%252C%2520poor%2520background%2520consistency%252C%2520and%250Aoverly%2520simplistic%2520instructions.%2520Second%252C%2520current%2520approaches%2520mainly%2520condition%2520on%250Athe%2520text%2520while%2520the%2520rich%2520image%2520information%2520is%2520underexplored%252C%2520therefore%2520inferior%250Ain%2520complex%2520instruction%2520following%2520and%2520maintaining%2520background%2520consistency.%250ATargeting%2520these%2520issues%252C%2520we%2520first%2520curated%2520the%2520AdvancedEdit%2520dataset%2520using%2520a%2520novel%250Adata%2520construction%2520pipeline%252C%2520formulating%2520a%2520large-scale%2520dataset%2520with%2520high%2520visual%250Aquality%252C%2520complex%2520instructions%252C%2520and%2520good%2520background%2520consistency.%2520Then%252C%2520to%250Afurther%2520inject%2520the%2520rich%2520image%2520information%252C%2520we%2520introduce%2520a%2520two-stream%2520bridging%250Amechanism%2520utilizing%2520both%2520the%2520textual%2520and%2520visual%2520features%2520reasoned%2520by%2520the%250Apowerful%2520Multimodal%2520Large%2520Language%2520Models%2520%2528MLLM%2529%2520to%2520guide%2520the%2520image%2520editing%250Aprocess%2520more%2520precisely.%2520Extensive%2520results%2520demonstrate%2520that%2520our%2520approach%252C%250AInsightEdit%252C%2520achieves%2520state-of-the-art%2520performance%252C%2520excelling%2520in%2520complex%250Ainstruction%2520following%2520and%2520maintaining%2520high%2520background%2520consistency%2520with%2520the%250Aoriginal%2520image.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2411.17323v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=InsightEdit%3A%20Towards%20Better%20Instruction%20Following%20for%20Image%20Editing&entry.906535625=Yingjing%20Xu%20and%20Jie%20Kong%20and%20Jiazhi%20Wang%20and%20Xiao%20Pan%20and%20Bo%20Lin%20and%20Qiang%20Liu&entry.1292438233=%20%20In%20this%20paper%2C%20we%20focus%20on%20the%20task%20of%20instruction-based%20image%20editing.%0APrevious%20works%20like%20InstructPix2Pix%2C%20InstructDiffusion%2C%20and%20SmartEdit%20have%0Aexplored%20end-to-end%20editing.%20However%2C%20two%20limitations%20still%20remain%3A%20First%2C%0Aexisting%20datasets%20suffer%20from%20low%20resolution%2C%20poor%20background%20consistency%2C%20and%0Aoverly%20simplistic%20instructions.%20Second%2C%20current%20approaches%20mainly%20condition%20on%0Athe%20text%20while%20the%20rich%20image%20information%20is%20underexplored%2C%20therefore%20inferior%0Ain%20complex%20instruction%20following%20and%20maintaining%20background%20consistency.%0ATargeting%20these%20issues%2C%20we%20first%20curated%20the%20AdvancedEdit%20dataset%20using%20a%20novel%0Adata%20construction%20pipeline%2C%20formulating%20a%20large-scale%20dataset%20with%20high%20visual%0Aquality%2C%20complex%20instructions%2C%20and%20good%20background%20consistency.%20Then%2C%20to%0Afurther%20inject%20the%20rich%20image%20information%2C%20we%20introduce%20a%20two-stream%20bridging%0Amechanism%20utilizing%20both%20the%20textual%20and%20visual%20features%20reasoned%20by%20the%0Apowerful%20Multimodal%20Large%20Language%20Models%20%28MLLM%29%20to%20guide%20the%20image%20editing%0Aprocess%20more%20precisely.%20Extensive%20results%20demonstrate%20that%20our%20approach%2C%0AInsightEdit%2C%20achieves%20state-of-the-art%20performance%2C%20excelling%20in%20complex%0Ainstruction%20following%20and%20maintaining%20high%20background%20consistency%20with%20the%0Aoriginal%20image.%0A&entry.1838667208=http%3A//arxiv.org/abs/2411.17323v1&entry.124074799=Read"},
{"title": "Box for Mask and Mask for Box: weak losses for multi-task partially\n  supervised learning", "author": "Ho\u00e0ng-\u00c2n L\u00ea and Paul Berg and Minh-Tan Pham", "abstract": "  Object detection and semantic segmentation are both scene understanding tasks\nyet they differ in data structure and information level. Object detection\nrequires box coordinates for object instances while semantic segmentation\nrequires pixel-wise class labels. Making use of one task's information to train\nthe other would be beneficial for multi-task partially supervised learning\nwhere each training example is annotated only for a single task, having the\npotential to expand training sets with different-task datasets. This paper\nstudies various weak losses for partially annotated data in combination with\nexisting supervised losses. We propose Box-for-Mask and Mask-for-Box\nstrategies, and their combination BoMBo, to distil necessary information from\none task annotations to train the other. Ablation studies and experimental\nresults on VOC and COCO datasets show favorable results for the proposed idea.\nSource code and data splits can be found at https://github.com/lhoangan/multas.\n", "link": "http://arxiv.org/abs/2411.17536v1", "date": "2024-11-26", "relevancy": 2.1974, "topK": [{"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5658}, {"title": "Weakly-guided self-supervised pretraining for temporal activity detection", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/25189", "similarity": 0.5399}, {"title": "Thinking Outside the BBox: Unconstrained Generative Object Compositing", "link": "http://arxiv.org/abs/2409.04559v2", "similarity": 0.5318}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Box%20for%20Mask%20and%20Mask%20for%20Box%3A%20weak%20losses%20for%20multi-task%20partially%0A%20%20supervised%20learning&body=Title%3A%20Box%20for%20Mask%20and%20Mask%20for%20Box%3A%20weak%20losses%20for%20multi-task%20partially%0A%20%20supervised%20learning%0AAuthor%3A%20Ho%C3%A0ng-%C3%82n%20L%C3%AA%20and%20Paul%20Berg%20and%20Minh-Tan%20Pham%0AAbstract%3A%20%20%20Object%20detection%20and%20semantic%20segmentation%20are%20both%20scene%20understanding%20tasks%0Ayet%20they%20differ%20in%20data%20structure%20and%20information%20level.%20Object%20detection%0Arequires%20box%20coordinates%20for%20object%20instances%20while%20semantic%20segmentation%0Arequires%20pixel-wise%20class%20labels.%20Making%20use%20of%20one%20task%27s%20information%20to%20train%0Athe%20other%20would%20be%20beneficial%20for%20multi-task%20partially%20supervised%20learning%0Awhere%20each%20training%20example%20is%20annotated%20only%20for%20a%20single%20task%2C%20having%20the%0Apotential%20to%20expand%20training%20sets%20with%20different-task%20datasets.%20This%20paper%0Astudies%20various%20weak%20losses%20for%20partially%20annotated%20data%20in%20combination%20with%0Aexisting%20supervised%20losses.%20We%20propose%20Box-for-Mask%20and%20Mask-for-Box%0Astrategies%2C%20and%20their%20combination%20BoMBo%2C%20to%20distil%20necessary%20information%20from%0Aone%20task%20annotations%20to%20train%20the%20other.%20Ablation%20studies%20and%20experimental%0Aresults%20on%20VOC%20and%20COCO%20datasets%20show%20favorable%20results%20for%20the%20proposed%20idea.%0ASource%20code%20and%20data%20splits%20can%20be%20found%20at%20https%3A//github.com/lhoangan/multas.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2411.17536v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DBox%2520for%2520Mask%2520and%2520Mask%2520for%2520Box%253A%2520weak%2520losses%2520for%2520multi-task%2520partially%250A%2520%2520supervised%2520learning%26entry.906535625%3DHo%25C3%25A0ng-%25C3%2582n%2520L%25C3%25AA%2520and%2520Paul%2520Berg%2520and%2520Minh-Tan%2520Pham%26entry.1292438233%3D%2520%2520Object%2520detection%2520and%2520semantic%2520segmentation%2520are%2520both%2520scene%2520understanding%2520tasks%250Ayet%2520they%2520differ%2520in%2520data%2520structure%2520and%2520information%2520level.%2520Object%2520detection%250Arequires%2520box%2520coordinates%2520for%2520object%2520instances%2520while%2520semantic%2520segmentation%250Arequires%2520pixel-wise%2520class%2520labels.%2520Making%2520use%2520of%2520one%2520task%2527s%2520information%2520to%2520train%250Athe%2520other%2520would%2520be%2520beneficial%2520for%2520multi-task%2520partially%2520supervised%2520learning%250Awhere%2520each%2520training%2520example%2520is%2520annotated%2520only%2520for%2520a%2520single%2520task%252C%2520having%2520the%250Apotential%2520to%2520expand%2520training%2520sets%2520with%2520different-task%2520datasets.%2520This%2520paper%250Astudies%2520various%2520weak%2520losses%2520for%2520partially%2520annotated%2520data%2520in%2520combination%2520with%250Aexisting%2520supervised%2520losses.%2520We%2520propose%2520Box-for-Mask%2520and%2520Mask-for-Box%250Astrategies%252C%2520and%2520their%2520combination%2520BoMBo%252C%2520to%2520distil%2520necessary%2520information%2520from%250Aone%2520task%2520annotations%2520to%2520train%2520the%2520other.%2520Ablation%2520studies%2520and%2520experimental%250Aresults%2520on%2520VOC%2520and%2520COCO%2520datasets%2520show%2520favorable%2520results%2520for%2520the%2520proposed%2520idea.%250ASource%2520code%2520and%2520data%2520splits%2520can%2520be%2520found%2520at%2520https%253A//github.com/lhoangan/multas.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2411.17536v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Box%20for%20Mask%20and%20Mask%20for%20Box%3A%20weak%20losses%20for%20multi-task%20partially%0A%20%20supervised%20learning&entry.906535625=Ho%C3%A0ng-%C3%82n%20L%C3%AA%20and%20Paul%20Berg%20and%20Minh-Tan%20Pham&entry.1292438233=%20%20Object%20detection%20and%20semantic%20segmentation%20are%20both%20scene%20understanding%20tasks%0Ayet%20they%20differ%20in%20data%20structure%20and%20information%20level.%20Object%20detection%0Arequires%20box%20coordinates%20for%20object%20instances%20while%20semantic%20segmentation%0Arequires%20pixel-wise%20class%20labels.%20Making%20use%20of%20one%20task%27s%20information%20to%20train%0Athe%20other%20would%20be%20beneficial%20for%20multi-task%20partially%20supervised%20learning%0Awhere%20each%20training%20example%20is%20annotated%20only%20for%20a%20single%20task%2C%20having%20the%0Apotential%20to%20expand%20training%20sets%20with%20different-task%20datasets.%20This%20paper%0Astudies%20various%20weak%20losses%20for%20partially%20annotated%20data%20in%20combination%20with%0Aexisting%20supervised%20losses.%20We%20propose%20Box-for-Mask%20and%20Mask-for-Box%0Astrategies%2C%20and%20their%20combination%20BoMBo%2C%20to%20distil%20necessary%20information%20from%0Aone%20task%20annotations%20to%20train%20the%20other.%20Ablation%20studies%20and%20experimental%0Aresults%20on%20VOC%20and%20COCO%20datasets%20show%20favorable%20results%20for%20the%20proposed%20idea.%0ASource%20code%20and%20data%20splits%20can%20be%20found%20at%20https%3A//github.com/lhoangan/multas.%0A&entry.1838667208=http%3A//arxiv.org/abs/2411.17536v1&entry.124074799=Read"},
{"title": "LRSAA: Large-scale Remote Sensing Image Target Recognition and Automatic\n  Annotation", "author": "Wuzheng Dong and Yujuan Zhu", "abstract": "  This paper presents a method for object recognition and automatic labeling in\nlarge-area remote sensing images called LRSAA. The method integrates YOLOv11\nand MobileNetV3-SSD object detection algorithms through ensemble learning to\nenhance model performance. Furthermore, it employs Poisson disk sampling\nsegmentation techniques and the EIOU metric to optimize the training and\ninference processes of segmented images, followed by the integration of\nresults. This approach not only reduces the demand for computational resources\nbut also achieves a good balance between accuracy and speed. The source code\nfor this project has been made publicly available on\nhttps://github.com/anaerovane/LRSAA.\n", "link": "http://arxiv.org/abs/2411.15808v2", "date": "2024-11-26", "relevancy": 2.1946, "topK": [{"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.5547}, {"title": "Self-supervised Photographic Image Layout Representation Learning", "link": "http://arxiv.org/abs/2403.03740v1", "similarity": 0.5488}, {"title": "RTG-SLAM: Real-time 3D Reconstruction at Scale using Gaussian Splatting", "link": "http://arxiv.org/abs/2404.19706v1", "similarity": 0.5461}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20LRSAA%3A%20Large-scale%20Remote%20Sensing%20Image%20Target%20Recognition%20and%20Automatic%0A%20%20Annotation&body=Title%3A%20LRSAA%3A%20Large-scale%20Remote%20Sensing%20Image%20Target%20Recognition%20and%20Automatic%0A%20%20Annotation%0AAuthor%3A%20Wuzheng%20Dong%20and%20Yujuan%20Zhu%0AAbstract%3A%20%20%20This%20paper%20presents%20a%20method%20for%20object%20recognition%20and%20automatic%20labeling%20in%0Alarge-area%20remote%20sensing%20images%20called%20LRSAA.%20The%20method%20integrates%20YOLOv11%0Aand%20MobileNetV3-SSD%20object%20detection%20algorithms%20through%20ensemble%20learning%20to%0Aenhance%20model%20performance.%20Furthermore%2C%20it%20employs%20Poisson%20disk%20sampling%0Asegmentation%20techniques%20and%20the%20EIOU%20metric%20to%20optimize%20the%20training%20and%0Ainference%20processes%20of%20segmented%20images%2C%20followed%20by%20the%20integration%20of%0Aresults.%20This%20approach%20not%20only%20reduces%20the%20demand%20for%20computational%20resources%0Abut%20also%20achieves%20a%20good%20balance%20between%20accuracy%20and%20speed.%20The%20source%20code%0Afor%20this%20project%20has%20been%20made%20publicly%20available%20on%0Ahttps%3A//github.com/anaerovane/LRSAA.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2411.15808v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DLRSAA%253A%2520Large-scale%2520Remote%2520Sensing%2520Image%2520Target%2520Recognition%2520and%2520Automatic%250A%2520%2520Annotation%26entry.906535625%3DWuzheng%2520Dong%2520and%2520Yujuan%2520Zhu%26entry.1292438233%3D%2520%2520This%2520paper%2520presents%2520a%2520method%2520for%2520object%2520recognition%2520and%2520automatic%2520labeling%2520in%250Alarge-area%2520remote%2520sensing%2520images%2520called%2520LRSAA.%2520The%2520method%2520integrates%2520YOLOv11%250Aand%2520MobileNetV3-SSD%2520object%2520detection%2520algorithms%2520through%2520ensemble%2520learning%2520to%250Aenhance%2520model%2520performance.%2520Furthermore%252C%2520it%2520employs%2520Poisson%2520disk%2520sampling%250Asegmentation%2520techniques%2520and%2520the%2520EIOU%2520metric%2520to%2520optimize%2520the%2520training%2520and%250Ainference%2520processes%2520of%2520segmented%2520images%252C%2520followed%2520by%2520the%2520integration%2520of%250Aresults.%2520This%2520approach%2520not%2520only%2520reduces%2520the%2520demand%2520for%2520computational%2520resources%250Abut%2520also%2520achieves%2520a%2520good%2520balance%2520between%2520accuracy%2520and%2520speed.%2520The%2520source%2520code%250Afor%2520this%2520project%2520has%2520been%2520made%2520publicly%2520available%2520on%250Ahttps%253A//github.com/anaerovane/LRSAA.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2411.15808v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=LRSAA%3A%20Large-scale%20Remote%20Sensing%20Image%20Target%20Recognition%20and%20Automatic%0A%20%20Annotation&entry.906535625=Wuzheng%20Dong%20and%20Yujuan%20Zhu&entry.1292438233=%20%20This%20paper%20presents%20a%20method%20for%20object%20recognition%20and%20automatic%20labeling%20in%0Alarge-area%20remote%20sensing%20images%20called%20LRSAA.%20The%20method%20integrates%20YOLOv11%0Aand%20MobileNetV3-SSD%20object%20detection%20algorithms%20through%20ensemble%20learning%20to%0Aenhance%20model%20performance.%20Furthermore%2C%20it%20employs%20Poisson%20disk%20sampling%0Asegmentation%20techniques%20and%20the%20EIOU%20metric%20to%20optimize%20the%20training%20and%0Ainference%20processes%20of%20segmented%20images%2C%20followed%20by%20the%20integration%20of%0Aresults.%20This%20approach%20not%20only%20reduces%20the%20demand%20for%20computational%20resources%0Abut%20also%20achieves%20a%20good%20balance%20between%20accuracy%20and%20speed.%20The%20source%20code%0Afor%20this%20project%20has%20been%20made%20publicly%20available%20on%0Ahttps%3A//github.com/anaerovane/LRSAA.%0A&entry.1838667208=http%3A//arxiv.org/abs/2411.15808v2&entry.124074799=Read"},
{"title": "SAMWISE: Infusing wisdom in SAM2 for Text-Driven Video Segmentation", "author": "Claudia Cuttano and Gabriele Trivigno and Gabriele Rosi and Carlo Masone and Giuseppe Averta", "abstract": "  Referring Video Object Segmentation (RVOS) relies on natural language\nexpressions to segment an object in a video clip. Existing methods restrict\nreasoning either to independent short clips, losing global context, or process\nthe entire video offline, impairing their application in a streaming fashion.\nIn this work, we aim to surpass these limitations and design an RVOS method\ncapable of effectively operating in streaming-like scenarios while retaining\ncontextual information from past frames. We build upon the Segment-Anything 2\n(SAM2) model, that provides robust segmentation and tracking capabilities and\nis naturally suited for streaming processing. We make SAM2 wiser, by empowering\nit with natural language understanding and explicit temporal modeling at the\nfeature extraction stage, without fine-tuning its weights, and without\noutsourcing modality interaction to external models. To this end, we introduce\na novel adapter module that injects temporal information and multi-modal cues\nin the feature extraction process. We further reveal the phenomenon of tracking\nbias in SAM2 and propose a learnable module to adjust its tracking focus when\nthe current frame features suggest a new object more aligned with the caption.\nOur proposed method, SAMWISE, achieves state-of-the-art across various\nbenchmarks, by adding a negligible overhead of just 4.2 M parameters. The code\nis available at https://github.com/ClaudiaCuttano/SAMWISE\n", "link": "http://arxiv.org/abs/2411.17646v1", "date": "2024-11-26", "relevancy": 2.1943, "topK": [{"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5545}, {"title": "Magic-Me: Identity-Specific Video Customized Diffusion", "link": "http://arxiv.org/abs/2402.09368v1", "similarity": 0.5455}, {"title": "Customize-A-Video: One-Shot Motion Customization of Text-to-Video\n  Diffusion Models", "link": "http://arxiv.org/abs/2402.14780v1", "similarity": 0.5413}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20SAMWISE%3A%20Infusing%20wisdom%20in%20SAM2%20for%20Text-Driven%20Video%20Segmentation&body=Title%3A%20SAMWISE%3A%20Infusing%20wisdom%20in%20SAM2%20for%20Text-Driven%20Video%20Segmentation%0AAuthor%3A%20Claudia%20Cuttano%20and%20Gabriele%20Trivigno%20and%20Gabriele%20Rosi%20and%20Carlo%20Masone%20and%20Giuseppe%20Averta%0AAbstract%3A%20%20%20Referring%20Video%20Object%20Segmentation%20%28RVOS%29%20relies%20on%20natural%20language%0Aexpressions%20to%20segment%20an%20object%20in%20a%20video%20clip.%20Existing%20methods%20restrict%0Areasoning%20either%20to%20independent%20short%20clips%2C%20losing%20global%20context%2C%20or%20process%0Athe%20entire%20video%20offline%2C%20impairing%20their%20application%20in%20a%20streaming%20fashion.%0AIn%20this%20work%2C%20we%20aim%20to%20surpass%20these%20limitations%20and%20design%20an%20RVOS%20method%0Acapable%20of%20effectively%20operating%20in%20streaming-like%20scenarios%20while%20retaining%0Acontextual%20information%20from%20past%20frames.%20We%20build%20upon%20the%20Segment-Anything%202%0A%28SAM2%29%20model%2C%20that%20provides%20robust%20segmentation%20and%20tracking%20capabilities%20and%0Ais%20naturally%20suited%20for%20streaming%20processing.%20We%20make%20SAM2%20wiser%2C%20by%20empowering%0Ait%20with%20natural%20language%20understanding%20and%20explicit%20temporal%20modeling%20at%20the%0Afeature%20extraction%20stage%2C%20without%20fine-tuning%20its%20weights%2C%20and%20without%0Aoutsourcing%20modality%20interaction%20to%20external%20models.%20To%20this%20end%2C%20we%20introduce%0Aa%20novel%20adapter%20module%20that%20injects%20temporal%20information%20and%20multi-modal%20cues%0Ain%20the%20feature%20extraction%20process.%20We%20further%20reveal%20the%20phenomenon%20of%20tracking%0Abias%20in%20SAM2%20and%20propose%20a%20learnable%20module%20to%20adjust%20its%20tracking%20focus%20when%0Athe%20current%20frame%20features%20suggest%20a%20new%20object%20more%20aligned%20with%20the%20caption.%0AOur%20proposed%20method%2C%20SAMWISE%2C%20achieves%20state-of-the-art%20across%20various%0Abenchmarks%2C%20by%20adding%20a%20negligible%20overhead%20of%20just%204.2%20M%20parameters.%20The%20code%0Ais%20available%20at%20https%3A//github.com/ClaudiaCuttano/SAMWISE%0A%0ALink%3A%20http%3A//arxiv.org/abs/2411.17646v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DSAMWISE%253A%2520Infusing%2520wisdom%2520in%2520SAM2%2520for%2520Text-Driven%2520Video%2520Segmentation%26entry.906535625%3DClaudia%2520Cuttano%2520and%2520Gabriele%2520Trivigno%2520and%2520Gabriele%2520Rosi%2520and%2520Carlo%2520Masone%2520and%2520Giuseppe%2520Averta%26entry.1292438233%3D%2520%2520Referring%2520Video%2520Object%2520Segmentation%2520%2528RVOS%2529%2520relies%2520on%2520natural%2520language%250Aexpressions%2520to%2520segment%2520an%2520object%2520in%2520a%2520video%2520clip.%2520Existing%2520methods%2520restrict%250Areasoning%2520either%2520to%2520independent%2520short%2520clips%252C%2520losing%2520global%2520context%252C%2520or%2520process%250Athe%2520entire%2520video%2520offline%252C%2520impairing%2520their%2520application%2520in%2520a%2520streaming%2520fashion.%250AIn%2520this%2520work%252C%2520we%2520aim%2520to%2520surpass%2520these%2520limitations%2520and%2520design%2520an%2520RVOS%2520method%250Acapable%2520of%2520effectively%2520operating%2520in%2520streaming-like%2520scenarios%2520while%2520retaining%250Acontextual%2520information%2520from%2520past%2520frames.%2520We%2520build%2520upon%2520the%2520Segment-Anything%25202%250A%2528SAM2%2529%2520model%252C%2520that%2520provides%2520robust%2520segmentation%2520and%2520tracking%2520capabilities%2520and%250Ais%2520naturally%2520suited%2520for%2520streaming%2520processing.%2520We%2520make%2520SAM2%2520wiser%252C%2520by%2520empowering%250Ait%2520with%2520natural%2520language%2520understanding%2520and%2520explicit%2520temporal%2520modeling%2520at%2520the%250Afeature%2520extraction%2520stage%252C%2520without%2520fine-tuning%2520its%2520weights%252C%2520and%2520without%250Aoutsourcing%2520modality%2520interaction%2520to%2520external%2520models.%2520To%2520this%2520end%252C%2520we%2520introduce%250Aa%2520novel%2520adapter%2520module%2520that%2520injects%2520temporal%2520information%2520and%2520multi-modal%2520cues%250Ain%2520the%2520feature%2520extraction%2520process.%2520We%2520further%2520reveal%2520the%2520phenomenon%2520of%2520tracking%250Abias%2520in%2520SAM2%2520and%2520propose%2520a%2520learnable%2520module%2520to%2520adjust%2520its%2520tracking%2520focus%2520when%250Athe%2520current%2520frame%2520features%2520suggest%2520a%2520new%2520object%2520more%2520aligned%2520with%2520the%2520caption.%250AOur%2520proposed%2520method%252C%2520SAMWISE%252C%2520achieves%2520state-of-the-art%2520across%2520various%250Abenchmarks%252C%2520by%2520adding%2520a%2520negligible%2520overhead%2520of%2520just%25204.2%2520M%2520parameters.%2520The%2520code%250Ais%2520available%2520at%2520https%253A//github.com/ClaudiaCuttano/SAMWISE%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2411.17646v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=SAMWISE%3A%20Infusing%20wisdom%20in%20SAM2%20for%20Text-Driven%20Video%20Segmentation&entry.906535625=Claudia%20Cuttano%20and%20Gabriele%20Trivigno%20and%20Gabriele%20Rosi%20and%20Carlo%20Masone%20and%20Giuseppe%20Averta&entry.1292438233=%20%20Referring%20Video%20Object%20Segmentation%20%28RVOS%29%20relies%20on%20natural%20language%0Aexpressions%20to%20segment%20an%20object%20in%20a%20video%20clip.%20Existing%20methods%20restrict%0Areasoning%20either%20to%20independent%20short%20clips%2C%20losing%20global%20context%2C%20or%20process%0Athe%20entire%20video%20offline%2C%20impairing%20their%20application%20in%20a%20streaming%20fashion.%0AIn%20this%20work%2C%20we%20aim%20to%20surpass%20these%20limitations%20and%20design%20an%20RVOS%20method%0Acapable%20of%20effectively%20operating%20in%20streaming-like%20scenarios%20while%20retaining%0Acontextual%20information%20from%20past%20frames.%20We%20build%20upon%20the%20Segment-Anything%202%0A%28SAM2%29%20model%2C%20that%20provides%20robust%20segmentation%20and%20tracking%20capabilities%20and%0Ais%20naturally%20suited%20for%20streaming%20processing.%20We%20make%20SAM2%20wiser%2C%20by%20empowering%0Ait%20with%20natural%20language%20understanding%20and%20explicit%20temporal%20modeling%20at%20the%0Afeature%20extraction%20stage%2C%20without%20fine-tuning%20its%20weights%2C%20and%20without%0Aoutsourcing%20modality%20interaction%20to%20external%20models.%20To%20this%20end%2C%20we%20introduce%0Aa%20novel%20adapter%20module%20that%20injects%20temporal%20information%20and%20multi-modal%20cues%0Ain%20the%20feature%20extraction%20process.%20We%20further%20reveal%20the%20phenomenon%20of%20tracking%0Abias%20in%20SAM2%20and%20propose%20a%20learnable%20module%20to%20adjust%20its%20tracking%20focus%20when%0Athe%20current%20frame%20features%20suggest%20a%20new%20object%20more%20aligned%20with%20the%20caption.%0AOur%20proposed%20method%2C%20SAMWISE%2C%20achieves%20state-of-the-art%20across%20various%0Abenchmarks%2C%20by%20adding%20a%20negligible%20overhead%20of%20just%204.2%20M%20parameters.%20The%20code%0Ais%20available%20at%20https%3A//github.com/ClaudiaCuttano/SAMWISE%0A&entry.1838667208=http%3A//arxiv.org/abs/2411.17646v1&entry.124074799=Read"},
{"title": "Communication-Efficient Cooperative SLAMMOT via Determining the Number\n  of Collaboration Vehicles", "author": "Susu Fang and Hao Li", "abstract": "  The SLAMMOT, i.e. simultaneous localization, mapping, and moving object\n(detection and) tracking, represents an emerging technology for autonomous\nvehicles in dynamic environments. Such single-vehicle systems still have\ninherent limitations, such as occlusion issues. Inspired by SLAMMOT and rapidly\nevolving cooperative technologies, it is natural to explore cooperative\nsimultaneous localization, mapping, moving object (detection and) tracking\n(C-SLAMMOT) to enhance state estimation for ego-vehicles and moving objects.\nC-SLAMMOT could significantly upgrade the single-vehicle performance by\nutilizing and integrating the shared information through communication among\nthe multiple vehicles. This inevitably leads to a fundamental trade-off between\nperformance and communication cost, especially in a scalable manner as the\nnumber of collaboration vehicles increases. To address this challenge, we\npropose a LiDAR-based communication-efficient C-SLAMMOT (CE C-SLAMMOT) method\nby determining the number of collaboration vehicles. In CE C-SLAMMOT, we adopt\ndescriptor-based methods for enhancing ego-vehicle pose estimation and spatial\nconfidence map-based methods for cooperative object perception, allowing for\nthe continuous and dynamic selection of the corresponding critical\ncollaboration vehicles and interaction content. This approach avoids the waste\nof precious communication costs by preventing the sharing of information from\ncertain collaborative vehicles that may contribute little or no performance\ngain, compared to the baseline method of exchanging raw observation information\namong all vehicles. Comparative experiments in various aspects have confirmed\nthat the proposed method achieves a good trade-off between performance and\ncommunication costs, while also outperforms previous state-of-the-art methods\nin cooperative perception performance.\n", "link": "http://arxiv.org/abs/2411.17432v1", "date": "2024-11-26", "relevancy": 2.1889, "topK": [{"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.5831}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.5461}, {"title": "RTG-SLAM: Real-time 3D Reconstruction at Scale using Gaussian Splatting", "link": "http://arxiv.org/abs/2404.19706v1", "similarity": 0.5118}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Communication-Efficient%20Cooperative%20SLAMMOT%20via%20Determining%20the%20Number%0A%20%20of%20Collaboration%20Vehicles&body=Title%3A%20Communication-Efficient%20Cooperative%20SLAMMOT%20via%20Determining%20the%20Number%0A%20%20of%20Collaboration%20Vehicles%0AAuthor%3A%20Susu%20Fang%20and%20Hao%20Li%0AAbstract%3A%20%20%20The%20SLAMMOT%2C%20i.e.%20simultaneous%20localization%2C%20mapping%2C%20and%20moving%20object%0A%28detection%20and%29%20tracking%2C%20represents%20an%20emerging%20technology%20for%20autonomous%0Avehicles%20in%20dynamic%20environments.%20Such%20single-vehicle%20systems%20still%20have%0Ainherent%20limitations%2C%20such%20as%20occlusion%20issues.%20Inspired%20by%20SLAMMOT%20and%20rapidly%0Aevolving%20cooperative%20technologies%2C%20it%20is%20natural%20to%20explore%20cooperative%0Asimultaneous%20localization%2C%20mapping%2C%20moving%20object%20%28detection%20and%29%20tracking%0A%28C-SLAMMOT%29%20to%20enhance%20state%20estimation%20for%20ego-vehicles%20and%20moving%20objects.%0AC-SLAMMOT%20could%20significantly%20upgrade%20the%20single-vehicle%20performance%20by%0Autilizing%20and%20integrating%20the%20shared%20information%20through%20communication%20among%0Athe%20multiple%20vehicles.%20This%20inevitably%20leads%20to%20a%20fundamental%20trade-off%20between%0Aperformance%20and%20communication%20cost%2C%20especially%20in%20a%20scalable%20manner%20as%20the%0Anumber%20of%20collaboration%20vehicles%20increases.%20To%20address%20this%20challenge%2C%20we%0Apropose%20a%20LiDAR-based%20communication-efficient%20C-SLAMMOT%20%28CE%20C-SLAMMOT%29%20method%0Aby%20determining%20the%20number%20of%20collaboration%20vehicles.%20In%20CE%20C-SLAMMOT%2C%20we%20adopt%0Adescriptor-based%20methods%20for%20enhancing%20ego-vehicle%20pose%20estimation%20and%20spatial%0Aconfidence%20map-based%20methods%20for%20cooperative%20object%20perception%2C%20allowing%20for%0Athe%20continuous%20and%20dynamic%20selection%20of%20the%20corresponding%20critical%0Acollaboration%20vehicles%20and%20interaction%20content.%20This%20approach%20avoids%20the%20waste%0Aof%20precious%20communication%20costs%20by%20preventing%20the%20sharing%20of%20information%20from%0Acertain%20collaborative%20vehicles%20that%20may%20contribute%20little%20or%20no%20performance%0Again%2C%20compared%20to%20the%20baseline%20method%20of%20exchanging%20raw%20observation%20information%0Aamong%20all%20vehicles.%20Comparative%20experiments%20in%20various%20aspects%20have%20confirmed%0Athat%20the%20proposed%20method%20achieves%20a%20good%20trade-off%20between%20performance%20and%0Acommunication%20costs%2C%20while%20also%20outperforms%20previous%20state-of-the-art%20methods%0Ain%20cooperative%20perception%20performance.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2411.17432v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DCommunication-Efficient%2520Cooperative%2520SLAMMOT%2520via%2520Determining%2520the%2520Number%250A%2520%2520of%2520Collaboration%2520Vehicles%26entry.906535625%3DSusu%2520Fang%2520and%2520Hao%2520Li%26entry.1292438233%3D%2520%2520The%2520SLAMMOT%252C%2520i.e.%2520simultaneous%2520localization%252C%2520mapping%252C%2520and%2520moving%2520object%250A%2528detection%2520and%2529%2520tracking%252C%2520represents%2520an%2520emerging%2520technology%2520for%2520autonomous%250Avehicles%2520in%2520dynamic%2520environments.%2520Such%2520single-vehicle%2520systems%2520still%2520have%250Ainherent%2520limitations%252C%2520such%2520as%2520occlusion%2520issues.%2520Inspired%2520by%2520SLAMMOT%2520and%2520rapidly%250Aevolving%2520cooperative%2520technologies%252C%2520it%2520is%2520natural%2520to%2520explore%2520cooperative%250Asimultaneous%2520localization%252C%2520mapping%252C%2520moving%2520object%2520%2528detection%2520and%2529%2520tracking%250A%2528C-SLAMMOT%2529%2520to%2520enhance%2520state%2520estimation%2520for%2520ego-vehicles%2520and%2520moving%2520objects.%250AC-SLAMMOT%2520could%2520significantly%2520upgrade%2520the%2520single-vehicle%2520performance%2520by%250Autilizing%2520and%2520integrating%2520the%2520shared%2520information%2520through%2520communication%2520among%250Athe%2520multiple%2520vehicles.%2520This%2520inevitably%2520leads%2520to%2520a%2520fundamental%2520trade-off%2520between%250Aperformance%2520and%2520communication%2520cost%252C%2520especially%2520in%2520a%2520scalable%2520manner%2520as%2520the%250Anumber%2520of%2520collaboration%2520vehicles%2520increases.%2520To%2520address%2520this%2520challenge%252C%2520we%250Apropose%2520a%2520LiDAR-based%2520communication-efficient%2520C-SLAMMOT%2520%2528CE%2520C-SLAMMOT%2529%2520method%250Aby%2520determining%2520the%2520number%2520of%2520collaboration%2520vehicles.%2520In%2520CE%2520C-SLAMMOT%252C%2520we%2520adopt%250Adescriptor-based%2520methods%2520for%2520enhancing%2520ego-vehicle%2520pose%2520estimation%2520and%2520spatial%250Aconfidence%2520map-based%2520methods%2520for%2520cooperative%2520object%2520perception%252C%2520allowing%2520for%250Athe%2520continuous%2520and%2520dynamic%2520selection%2520of%2520the%2520corresponding%2520critical%250Acollaboration%2520vehicles%2520and%2520interaction%2520content.%2520This%2520approach%2520avoids%2520the%2520waste%250Aof%2520precious%2520communication%2520costs%2520by%2520preventing%2520the%2520sharing%2520of%2520information%2520from%250Acertain%2520collaborative%2520vehicles%2520that%2520may%2520contribute%2520little%2520or%2520no%2520performance%250Again%252C%2520compared%2520to%2520the%2520baseline%2520method%2520of%2520exchanging%2520raw%2520observation%2520information%250Aamong%2520all%2520vehicles.%2520Comparative%2520experiments%2520in%2520various%2520aspects%2520have%2520confirmed%250Athat%2520the%2520proposed%2520method%2520achieves%2520a%2520good%2520trade-off%2520between%2520performance%2520and%250Acommunication%2520costs%252C%2520while%2520also%2520outperforms%2520previous%2520state-of-the-art%2520methods%250Ain%2520cooperative%2520perception%2520performance.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2411.17432v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Communication-Efficient%20Cooperative%20SLAMMOT%20via%20Determining%20the%20Number%0A%20%20of%20Collaboration%20Vehicles&entry.906535625=Susu%20Fang%20and%20Hao%20Li&entry.1292438233=%20%20The%20SLAMMOT%2C%20i.e.%20simultaneous%20localization%2C%20mapping%2C%20and%20moving%20object%0A%28detection%20and%29%20tracking%2C%20represents%20an%20emerging%20technology%20for%20autonomous%0Avehicles%20in%20dynamic%20environments.%20Such%20single-vehicle%20systems%20still%20have%0Ainherent%20limitations%2C%20such%20as%20occlusion%20issues.%20Inspired%20by%20SLAMMOT%20and%20rapidly%0Aevolving%20cooperative%20technologies%2C%20it%20is%20natural%20to%20explore%20cooperative%0Asimultaneous%20localization%2C%20mapping%2C%20moving%20object%20%28detection%20and%29%20tracking%0A%28C-SLAMMOT%29%20to%20enhance%20state%20estimation%20for%20ego-vehicles%20and%20moving%20objects.%0AC-SLAMMOT%20could%20significantly%20upgrade%20the%20single-vehicle%20performance%20by%0Autilizing%20and%20integrating%20the%20shared%20information%20through%20communication%20among%0Athe%20multiple%20vehicles.%20This%20inevitably%20leads%20to%20a%20fundamental%20trade-off%20between%0Aperformance%20and%20communication%20cost%2C%20especially%20in%20a%20scalable%20manner%20as%20the%0Anumber%20of%20collaboration%20vehicles%20increases.%20To%20address%20this%20challenge%2C%20we%0Apropose%20a%20LiDAR-based%20communication-efficient%20C-SLAMMOT%20%28CE%20C-SLAMMOT%29%20method%0Aby%20determining%20the%20number%20of%20collaboration%20vehicles.%20In%20CE%20C-SLAMMOT%2C%20we%20adopt%0Adescriptor-based%20methods%20for%20enhancing%20ego-vehicle%20pose%20estimation%20and%20spatial%0Aconfidence%20map-based%20methods%20for%20cooperative%20object%20perception%2C%20allowing%20for%0Athe%20continuous%20and%20dynamic%20selection%20of%20the%20corresponding%20critical%0Acollaboration%20vehicles%20and%20interaction%20content.%20This%20approach%20avoids%20the%20waste%0Aof%20precious%20communication%20costs%20by%20preventing%20the%20sharing%20of%20information%20from%0Acertain%20collaborative%20vehicles%20that%20may%20contribute%20little%20or%20no%20performance%0Again%2C%20compared%20to%20the%20baseline%20method%20of%20exchanging%20raw%20observation%20information%0Aamong%20all%20vehicles.%20Comparative%20experiments%20in%20various%20aspects%20have%20confirmed%0Athat%20the%20proposed%20method%20achieves%20a%20good%20trade-off%20between%20performance%20and%0Acommunication%20costs%2C%20while%20also%20outperforms%20previous%20state-of-the-art%20methods%0Ain%20cooperative%20perception%20performance.%0A&entry.1838667208=http%3A//arxiv.org/abs/2411.17432v1&entry.124074799=Read"},
{"title": "Puzzle Similarity: A Perceptually-guided No-Reference Metric for\n  Artifact Detection in 3D Scene Reconstructions", "author": "Nicolai Hermann and Jorge Condor and Piotr Didyk", "abstract": "  Modern reconstruction techniques can effectively model complex 3D scenes from\nsparse 2D views. However, automatically assessing the quality of novel views\nand identifying artifacts is challenging due to the lack of ground truth images\nand the limitations of no-reference image metrics in predicting detailed\nartifact maps. The absence of such quality metrics hinders accurate predictions\nof the quality of generated views and limits the adoption of post-processing\ntechniques, such as inpainting, to enhance reconstruction quality. In this\nwork, we propose a new no-reference metric, Puzzle Similarity, which is\ndesigned to localize artifacts in novel views. Our approach utilizes image\npatch statistics from the input views to establish a scene-specific\ndistribution that is later used to identify poorly reconstructed regions in the\nnovel views. We test and evaluate our method in the context of 3D\nreconstruction; to this end, we collected a novel dataset of human quality\nassessment in unseen reconstructed views. Through this dataset, we demonstrate\nthat our method can not only successfully localize artifacts in novel views,\ncorrelating with human assessment, but do so without direct references.\nSurprisingly, our metric outperforms both no-reference metrics and popular\nfull-reference image metrics. We can leverage our new metric to enhance\napplications like automatic image restoration, guided acquisition, or 3D\nreconstruction from sparse inputs.\n", "link": "http://arxiv.org/abs/2411.17489v1", "date": "2024-11-26", "relevancy": 2.188, "topK": [{"title": "CAT3D: Create Anything in 3D with Multi-View Diffusion Models", "link": "http://arxiv.org/abs/2405.10314v1", "similarity": 0.5497}, {"title": "CAT3D: Create Anything in 3D with Multi-View Diffusion Models", "link": "http://arxiv.org/abs/2405.10314v1", "similarity": 0.5497}, {"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.5335}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Puzzle%20Similarity%3A%20A%20Perceptually-guided%20No-Reference%20Metric%20for%0A%20%20Artifact%20Detection%20in%203D%20Scene%20Reconstructions&body=Title%3A%20Puzzle%20Similarity%3A%20A%20Perceptually-guided%20No-Reference%20Metric%20for%0A%20%20Artifact%20Detection%20in%203D%20Scene%20Reconstructions%0AAuthor%3A%20Nicolai%20Hermann%20and%20Jorge%20Condor%20and%20Piotr%20Didyk%0AAbstract%3A%20%20%20Modern%20reconstruction%20techniques%20can%20effectively%20model%20complex%203D%20scenes%20from%0Asparse%202D%20views.%20However%2C%20automatically%20assessing%20the%20quality%20of%20novel%20views%0Aand%20identifying%20artifacts%20is%20challenging%20due%20to%20the%20lack%20of%20ground%20truth%20images%0Aand%20the%20limitations%20of%20no-reference%20image%20metrics%20in%20predicting%20detailed%0Aartifact%20maps.%20The%20absence%20of%20such%20quality%20metrics%20hinders%20accurate%20predictions%0Aof%20the%20quality%20of%20generated%20views%20and%20limits%20the%20adoption%20of%20post-processing%0Atechniques%2C%20such%20as%20inpainting%2C%20to%20enhance%20reconstruction%20quality.%20In%20this%0Awork%2C%20we%20propose%20a%20new%20no-reference%20metric%2C%20Puzzle%20Similarity%2C%20which%20is%0Adesigned%20to%20localize%20artifacts%20in%20novel%20views.%20Our%20approach%20utilizes%20image%0Apatch%20statistics%20from%20the%20input%20views%20to%20establish%20a%20scene-specific%0Adistribution%20that%20is%20later%20used%20to%20identify%20poorly%20reconstructed%20regions%20in%20the%0Anovel%20views.%20We%20test%20and%20evaluate%20our%20method%20in%20the%20context%20of%203D%0Areconstruction%3B%20to%20this%20end%2C%20we%20collected%20a%20novel%20dataset%20of%20human%20quality%0Aassessment%20in%20unseen%20reconstructed%20views.%20Through%20this%20dataset%2C%20we%20demonstrate%0Athat%20our%20method%20can%20not%20only%20successfully%20localize%20artifacts%20in%20novel%20views%2C%0Acorrelating%20with%20human%20assessment%2C%20but%20do%20so%20without%20direct%20references.%0ASurprisingly%2C%20our%20metric%20outperforms%20both%20no-reference%20metrics%20and%20popular%0Afull-reference%20image%20metrics.%20We%20can%20leverage%20our%20new%20metric%20to%20enhance%0Aapplications%20like%20automatic%20image%20restoration%2C%20guided%20acquisition%2C%20or%203D%0Areconstruction%20from%20sparse%20inputs.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2411.17489v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DPuzzle%2520Similarity%253A%2520A%2520Perceptually-guided%2520No-Reference%2520Metric%2520for%250A%2520%2520Artifact%2520Detection%2520in%25203D%2520Scene%2520Reconstructions%26entry.906535625%3DNicolai%2520Hermann%2520and%2520Jorge%2520Condor%2520and%2520Piotr%2520Didyk%26entry.1292438233%3D%2520%2520Modern%2520reconstruction%2520techniques%2520can%2520effectively%2520model%2520complex%25203D%2520scenes%2520from%250Asparse%25202D%2520views.%2520However%252C%2520automatically%2520assessing%2520the%2520quality%2520of%2520novel%2520views%250Aand%2520identifying%2520artifacts%2520is%2520challenging%2520due%2520to%2520the%2520lack%2520of%2520ground%2520truth%2520images%250Aand%2520the%2520limitations%2520of%2520no-reference%2520image%2520metrics%2520in%2520predicting%2520detailed%250Aartifact%2520maps.%2520The%2520absence%2520of%2520such%2520quality%2520metrics%2520hinders%2520accurate%2520predictions%250Aof%2520the%2520quality%2520of%2520generated%2520views%2520and%2520limits%2520the%2520adoption%2520of%2520post-processing%250Atechniques%252C%2520such%2520as%2520inpainting%252C%2520to%2520enhance%2520reconstruction%2520quality.%2520In%2520this%250Awork%252C%2520we%2520propose%2520a%2520new%2520no-reference%2520metric%252C%2520Puzzle%2520Similarity%252C%2520which%2520is%250Adesigned%2520to%2520localize%2520artifacts%2520in%2520novel%2520views.%2520Our%2520approach%2520utilizes%2520image%250Apatch%2520statistics%2520from%2520the%2520input%2520views%2520to%2520establish%2520a%2520scene-specific%250Adistribution%2520that%2520is%2520later%2520used%2520to%2520identify%2520poorly%2520reconstructed%2520regions%2520in%2520the%250Anovel%2520views.%2520We%2520test%2520and%2520evaluate%2520our%2520method%2520in%2520the%2520context%2520of%25203D%250Areconstruction%253B%2520to%2520this%2520end%252C%2520we%2520collected%2520a%2520novel%2520dataset%2520of%2520human%2520quality%250Aassessment%2520in%2520unseen%2520reconstructed%2520views.%2520Through%2520this%2520dataset%252C%2520we%2520demonstrate%250Athat%2520our%2520method%2520can%2520not%2520only%2520successfully%2520localize%2520artifacts%2520in%2520novel%2520views%252C%250Acorrelating%2520with%2520human%2520assessment%252C%2520but%2520do%2520so%2520without%2520direct%2520references.%250ASurprisingly%252C%2520our%2520metric%2520outperforms%2520both%2520no-reference%2520metrics%2520and%2520popular%250Afull-reference%2520image%2520metrics.%2520We%2520can%2520leverage%2520our%2520new%2520metric%2520to%2520enhance%250Aapplications%2520like%2520automatic%2520image%2520restoration%252C%2520guided%2520acquisition%252C%2520or%25203D%250Areconstruction%2520from%2520sparse%2520inputs.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2411.17489v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Puzzle%20Similarity%3A%20A%20Perceptually-guided%20No-Reference%20Metric%20for%0A%20%20Artifact%20Detection%20in%203D%20Scene%20Reconstructions&entry.906535625=Nicolai%20Hermann%20and%20Jorge%20Condor%20and%20Piotr%20Didyk&entry.1292438233=%20%20Modern%20reconstruction%20techniques%20can%20effectively%20model%20complex%203D%20scenes%20from%0Asparse%202D%20views.%20However%2C%20automatically%20assessing%20the%20quality%20of%20novel%20views%0Aand%20identifying%20artifacts%20is%20challenging%20due%20to%20the%20lack%20of%20ground%20truth%20images%0Aand%20the%20limitations%20of%20no-reference%20image%20metrics%20in%20predicting%20detailed%0Aartifact%20maps.%20The%20absence%20of%20such%20quality%20metrics%20hinders%20accurate%20predictions%0Aof%20the%20quality%20of%20generated%20views%20and%20limits%20the%20adoption%20of%20post-processing%0Atechniques%2C%20such%20as%20inpainting%2C%20to%20enhance%20reconstruction%20quality.%20In%20this%0Awork%2C%20we%20propose%20a%20new%20no-reference%20metric%2C%20Puzzle%20Similarity%2C%20which%20is%0Adesigned%20to%20localize%20artifacts%20in%20novel%20views.%20Our%20approach%20utilizes%20image%0Apatch%20statistics%20from%20the%20input%20views%20to%20establish%20a%20scene-specific%0Adistribution%20that%20is%20later%20used%20to%20identify%20poorly%20reconstructed%20regions%20in%20the%0Anovel%20views.%20We%20test%20and%20evaluate%20our%20method%20in%20the%20context%20of%203D%0Areconstruction%3B%20to%20this%20end%2C%20we%20collected%20a%20novel%20dataset%20of%20human%20quality%0Aassessment%20in%20unseen%20reconstructed%20views.%20Through%20this%20dataset%2C%20we%20demonstrate%0Athat%20our%20method%20can%20not%20only%20successfully%20localize%20artifacts%20in%20novel%20views%2C%0Acorrelating%20with%20human%20assessment%2C%20but%20do%20so%20without%20direct%20references.%0ASurprisingly%2C%20our%20metric%20outperforms%20both%20no-reference%20metrics%20and%20popular%0Afull-reference%20image%20metrics.%20We%20can%20leverage%20our%20new%20metric%20to%20enhance%0Aapplications%20like%20automatic%20image%20restoration%2C%20guided%20acquisition%2C%20or%203D%0Areconstruction%20from%20sparse%20inputs.%0A&entry.1838667208=http%3A//arxiv.org/abs/2411.17489v1&entry.124074799=Read"},
{"title": "ShowUI: One Vision-Language-Action Model for GUI Visual Agent", "author": "Kevin Qinghong Lin and Linjie Li and Difei Gao and Zhengyuan Yang and Shiwei Wu and Zechen Bai and Weixian Lei and Lijuan Wang and Mike Zheng Shou", "abstract": "  Building Graphical User Interface (GUI) assistants holds significant promise\nfor enhancing human workflow productivity. While most agents are\nlanguage-based, relying on closed-source API with text-rich meta-information\n(e.g., HTML or accessibility tree), they show limitations in perceiving UI\nvisuals as humans do, highlighting the need for GUI visual agents. In this\nwork, we develop a vision-language-action model in digital world, namely\nShowUI, which features the following innovations: (i) UI-Guided Visual Token\nSelection to reduce computational costs by formulating screenshots as an UI\nconnected graph, adaptively identifying their redundant relationship and serve\nas the criteria for token selection during self-attention blocks; (ii)\nInterleaved Vision-Language-Action Streaming that flexibly unifies diverse\nneeds within GUI tasks, enabling effective management of visual-action history\nin navigation or pairing multi-turn query-action sequences per screenshot to\nenhance training efficiency; (iii) Small-scale High-quality GUI\nInstruction-following Datasets by careful data curation and employing a\nresampling strategy to address significant data type imbalances. With above\ncomponents, ShowUI, a lightweight 2B model using 256K data, achieves a strong\n75.1% accuracy in zero-shot screenshot grounding. Its UI-guided token selection\nfurther reduces 33% of redundant visual tokens during training and speeds up\nthe performance by 1.4x. Navigation experiments across web Mind2Web, mobile\nAITW, and online MiniWob environments further underscore the effectiveness and\npotential of our model in advancing GUI visual agents. The models are available\nat https://github.com/showlab/ShowUI.\n", "link": "http://arxiv.org/abs/2411.17465v1", "date": "2024-11-26", "relevancy": 2.1807, "topK": [{"title": "Genie: Generative Interactive Environments", "link": "http://arxiv.org/abs/2402.15391v1", "similarity": 0.5649}, {"title": "RoboEXP: Action-Conditioned Scene Graph via Interactive Exploration for\n  Robotic Manipulation", "link": "http://arxiv.org/abs/2402.15487v1", "similarity": 0.5322}, {"title": "UGG: Unified Generative Grasping", "link": "https://arxiv.org/abs/2311.16917", "similarity": 0.5307}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20ShowUI%3A%20One%20Vision-Language-Action%20Model%20for%20GUI%20Visual%20Agent&body=Title%3A%20ShowUI%3A%20One%20Vision-Language-Action%20Model%20for%20GUI%20Visual%20Agent%0AAuthor%3A%20Kevin%20Qinghong%20Lin%20and%20Linjie%20Li%20and%20Difei%20Gao%20and%20Zhengyuan%20Yang%20and%20Shiwei%20Wu%20and%20Zechen%20Bai%20and%20Weixian%20Lei%20and%20Lijuan%20Wang%20and%20Mike%20Zheng%20Shou%0AAbstract%3A%20%20%20Building%20Graphical%20User%20Interface%20%28GUI%29%20assistants%20holds%20significant%20promise%0Afor%20enhancing%20human%20workflow%20productivity.%20While%20most%20agents%20are%0Alanguage-based%2C%20relying%20on%20closed-source%20API%20with%20text-rich%20meta-information%0A%28e.g.%2C%20HTML%20or%20accessibility%20tree%29%2C%20they%20show%20limitations%20in%20perceiving%20UI%0Avisuals%20as%20humans%20do%2C%20highlighting%20the%20need%20for%20GUI%20visual%20agents.%20In%20this%0Awork%2C%20we%20develop%20a%20vision-language-action%20model%20in%20digital%20world%2C%20namely%0AShowUI%2C%20which%20features%20the%20following%20innovations%3A%20%28i%29%20UI-Guided%20Visual%20Token%0ASelection%20to%20reduce%20computational%20costs%20by%20formulating%20screenshots%20as%20an%20UI%0Aconnected%20graph%2C%20adaptively%20identifying%20their%20redundant%20relationship%20and%20serve%0Aas%20the%20criteria%20for%20token%20selection%20during%20self-attention%20blocks%3B%20%28ii%29%0AInterleaved%20Vision-Language-Action%20Streaming%20that%20flexibly%20unifies%20diverse%0Aneeds%20within%20GUI%20tasks%2C%20enabling%20effective%20management%20of%20visual-action%20history%0Ain%20navigation%20or%20pairing%20multi-turn%20query-action%20sequences%20per%20screenshot%20to%0Aenhance%20training%20efficiency%3B%20%28iii%29%20Small-scale%20High-quality%20GUI%0AInstruction-following%20Datasets%20by%20careful%20data%20curation%20and%20employing%20a%0Aresampling%20strategy%20to%20address%20significant%20data%20type%20imbalances.%20With%20above%0Acomponents%2C%20ShowUI%2C%20a%20lightweight%202B%20model%20using%20256K%20data%2C%20achieves%20a%20strong%0A75.1%25%20accuracy%20in%20zero-shot%20screenshot%20grounding.%20Its%20UI-guided%20token%20selection%0Afurther%20reduces%2033%25%20of%20redundant%20visual%20tokens%20during%20training%20and%20speeds%20up%0Athe%20performance%20by%201.4x.%20Navigation%20experiments%20across%20web%20Mind2Web%2C%20mobile%0AAITW%2C%20and%20online%20MiniWob%20environments%20further%20underscore%20the%20effectiveness%20and%0Apotential%20of%20our%20model%20in%20advancing%20GUI%20visual%20agents.%20The%20models%20are%20available%0Aat%20https%3A//github.com/showlab/ShowUI.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2411.17465v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DShowUI%253A%2520One%2520Vision-Language-Action%2520Model%2520for%2520GUI%2520Visual%2520Agent%26entry.906535625%3DKevin%2520Qinghong%2520Lin%2520and%2520Linjie%2520Li%2520and%2520Difei%2520Gao%2520and%2520Zhengyuan%2520Yang%2520and%2520Shiwei%2520Wu%2520and%2520Zechen%2520Bai%2520and%2520Weixian%2520Lei%2520and%2520Lijuan%2520Wang%2520and%2520Mike%2520Zheng%2520Shou%26entry.1292438233%3D%2520%2520Building%2520Graphical%2520User%2520Interface%2520%2528GUI%2529%2520assistants%2520holds%2520significant%2520promise%250Afor%2520enhancing%2520human%2520workflow%2520productivity.%2520While%2520most%2520agents%2520are%250Alanguage-based%252C%2520relying%2520on%2520closed-source%2520API%2520with%2520text-rich%2520meta-information%250A%2528e.g.%252C%2520HTML%2520or%2520accessibility%2520tree%2529%252C%2520they%2520show%2520limitations%2520in%2520perceiving%2520UI%250Avisuals%2520as%2520humans%2520do%252C%2520highlighting%2520the%2520need%2520for%2520GUI%2520visual%2520agents.%2520In%2520this%250Awork%252C%2520we%2520develop%2520a%2520vision-language-action%2520model%2520in%2520digital%2520world%252C%2520namely%250AShowUI%252C%2520which%2520features%2520the%2520following%2520innovations%253A%2520%2528i%2529%2520UI-Guided%2520Visual%2520Token%250ASelection%2520to%2520reduce%2520computational%2520costs%2520by%2520formulating%2520screenshots%2520as%2520an%2520UI%250Aconnected%2520graph%252C%2520adaptively%2520identifying%2520their%2520redundant%2520relationship%2520and%2520serve%250Aas%2520the%2520criteria%2520for%2520token%2520selection%2520during%2520self-attention%2520blocks%253B%2520%2528ii%2529%250AInterleaved%2520Vision-Language-Action%2520Streaming%2520that%2520flexibly%2520unifies%2520diverse%250Aneeds%2520within%2520GUI%2520tasks%252C%2520enabling%2520effective%2520management%2520of%2520visual-action%2520history%250Ain%2520navigation%2520or%2520pairing%2520multi-turn%2520query-action%2520sequences%2520per%2520screenshot%2520to%250Aenhance%2520training%2520efficiency%253B%2520%2528iii%2529%2520Small-scale%2520High-quality%2520GUI%250AInstruction-following%2520Datasets%2520by%2520careful%2520data%2520curation%2520and%2520employing%2520a%250Aresampling%2520strategy%2520to%2520address%2520significant%2520data%2520type%2520imbalances.%2520With%2520above%250Acomponents%252C%2520ShowUI%252C%2520a%2520lightweight%25202B%2520model%2520using%2520256K%2520data%252C%2520achieves%2520a%2520strong%250A75.1%2525%2520accuracy%2520in%2520zero-shot%2520screenshot%2520grounding.%2520Its%2520UI-guided%2520token%2520selection%250Afurther%2520reduces%252033%2525%2520of%2520redundant%2520visual%2520tokens%2520during%2520training%2520and%2520speeds%2520up%250Athe%2520performance%2520by%25201.4x.%2520Navigation%2520experiments%2520across%2520web%2520Mind2Web%252C%2520mobile%250AAITW%252C%2520and%2520online%2520MiniWob%2520environments%2520further%2520underscore%2520the%2520effectiveness%2520and%250Apotential%2520of%2520our%2520model%2520in%2520advancing%2520GUI%2520visual%2520agents.%2520The%2520models%2520are%2520available%250Aat%2520https%253A//github.com/showlab/ShowUI.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2411.17465v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=ShowUI%3A%20One%20Vision-Language-Action%20Model%20for%20GUI%20Visual%20Agent&entry.906535625=Kevin%20Qinghong%20Lin%20and%20Linjie%20Li%20and%20Difei%20Gao%20and%20Zhengyuan%20Yang%20and%20Shiwei%20Wu%20and%20Zechen%20Bai%20and%20Weixian%20Lei%20and%20Lijuan%20Wang%20and%20Mike%20Zheng%20Shou&entry.1292438233=%20%20Building%20Graphical%20User%20Interface%20%28GUI%29%20assistants%20holds%20significant%20promise%0Afor%20enhancing%20human%20workflow%20productivity.%20While%20most%20agents%20are%0Alanguage-based%2C%20relying%20on%20closed-source%20API%20with%20text-rich%20meta-information%0A%28e.g.%2C%20HTML%20or%20accessibility%20tree%29%2C%20they%20show%20limitations%20in%20perceiving%20UI%0Avisuals%20as%20humans%20do%2C%20highlighting%20the%20need%20for%20GUI%20visual%20agents.%20In%20this%0Awork%2C%20we%20develop%20a%20vision-language-action%20model%20in%20digital%20world%2C%20namely%0AShowUI%2C%20which%20features%20the%20following%20innovations%3A%20%28i%29%20UI-Guided%20Visual%20Token%0ASelection%20to%20reduce%20computational%20costs%20by%20formulating%20screenshots%20as%20an%20UI%0Aconnected%20graph%2C%20adaptively%20identifying%20their%20redundant%20relationship%20and%20serve%0Aas%20the%20criteria%20for%20token%20selection%20during%20self-attention%20blocks%3B%20%28ii%29%0AInterleaved%20Vision-Language-Action%20Streaming%20that%20flexibly%20unifies%20diverse%0Aneeds%20within%20GUI%20tasks%2C%20enabling%20effective%20management%20of%20visual-action%20history%0Ain%20navigation%20or%20pairing%20multi-turn%20query-action%20sequences%20per%20screenshot%20to%0Aenhance%20training%20efficiency%3B%20%28iii%29%20Small-scale%20High-quality%20GUI%0AInstruction-following%20Datasets%20by%20careful%20data%20curation%20and%20employing%20a%0Aresampling%20strategy%20to%20address%20significant%20data%20type%20imbalances.%20With%20above%0Acomponents%2C%20ShowUI%2C%20a%20lightweight%202B%20model%20using%20256K%20data%2C%20achieves%20a%20strong%0A75.1%25%20accuracy%20in%20zero-shot%20screenshot%20grounding.%20Its%20UI-guided%20token%20selection%0Afurther%20reduces%2033%25%20of%20redundant%20visual%20tokens%20during%20training%20and%20speeds%20up%0Athe%20performance%20by%201.4x.%20Navigation%20experiments%20across%20web%20Mind2Web%2C%20mobile%0AAITW%2C%20and%20online%20MiniWob%20environments%20further%20underscore%20the%20effectiveness%20and%0Apotential%20of%20our%20model%20in%20advancing%20GUI%20visual%20agents.%20The%20models%20are%20available%0Aat%20https%3A//github.com/showlab/ShowUI.%0A&entry.1838667208=http%3A//arxiv.org/abs/2411.17465v1&entry.124074799=Read"},
{"title": "A Graph Neural Network deep-dive into successful counterattacks", "author": "Joris Bekkers and Amod Sahasrabudhe", "abstract": "  A counterattack in soccer is a high speed, high intensity direct attack that\ncan occur when a team transitions from a defensive state to an attacking state\nafter regaining possession of the ball. The aim is to create a goal-scoring\nopportunity by convering a lot of ground with minimal passes before the\nopposing team can recover their defensive shape. The purpose of this research\nis to build gender-specific Graph Neural Networks to model the likelihood of a\ncounterattack being successful and uncover what factors make them successful in\nprofessional soccer. These models are trained on a total of 20863 frames of\nsynchronized on-ball event and spatiotemporal (broadcast) tracking data. This\ndataset is derived from 632 games of MLS (2022), NWSL (2022) and international\nsoccer (2020-2022). With this data we demonstrate that gender-specific Graph\nNeural Networks outperform architecturally identical gender-ambiguous models in\npredicting the successful outcome of counterattacks. We show, using Permutation\nFeature Importance, that byline to byline speed, angle to the goal, angle to\nthe ball and sideline to sideline speed are the node features with the highest\nimpact on model performance.\n  Additionally, we offer some illustrative examples on how to navigate the\ninfinite solution search space to aid in identifying improvements for player\ndecision making.\n  This research is accompanied by an open-source repository containing all data\nand code, and it is also accompanied by an open-source Python package which\nsimplifies converting spatiotemporal data into graphs. This package also\nfacilitates testing, validation, training and prediction with this data. This\nshould allow the reader to replicate and improve upon our research more easily.\n", "link": "http://arxiv.org/abs/2411.17450v1", "date": "2024-11-26", "relevancy": 2.1793, "topK": [{"title": "Boosted dynamic neural networks", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/26302", "similarity": 0.441}, {"title": "Dynamic Gaussians Mesh: Consistent Mesh Reconstruction from Monocular\n  Videos", "link": "http://arxiv.org/abs/2404.12379v2", "similarity": 0.4369}, {"title": "UGG: Unified Generative Grasping", "link": "https://arxiv.org/abs/2311.16917", "similarity": 0.4297}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20A%20Graph%20Neural%20Network%20deep-dive%20into%20successful%20counterattacks&body=Title%3A%20A%20Graph%20Neural%20Network%20deep-dive%20into%20successful%20counterattacks%0AAuthor%3A%20Joris%20Bekkers%20and%20Amod%20Sahasrabudhe%0AAbstract%3A%20%20%20A%20counterattack%20in%20soccer%20is%20a%20high%20speed%2C%20high%20intensity%20direct%20attack%20that%0Acan%20occur%20when%20a%20team%20transitions%20from%20a%20defensive%20state%20to%20an%20attacking%20state%0Aafter%20regaining%20possession%20of%20the%20ball.%20The%20aim%20is%20to%20create%20a%20goal-scoring%0Aopportunity%20by%20convering%20a%20lot%20of%20ground%20with%20minimal%20passes%20before%20the%0Aopposing%20team%20can%20recover%20their%20defensive%20shape.%20The%20purpose%20of%20this%20research%0Ais%20to%20build%20gender-specific%20Graph%20Neural%20Networks%20to%20model%20the%20likelihood%20of%20a%0Acounterattack%20being%20successful%20and%20uncover%20what%20factors%20make%20them%20successful%20in%0Aprofessional%20soccer.%20These%20models%20are%20trained%20on%20a%20total%20of%2020863%20frames%20of%0Asynchronized%20on-ball%20event%20and%20spatiotemporal%20%28broadcast%29%20tracking%20data.%20This%0Adataset%20is%20derived%20from%20632%20games%20of%20MLS%20%282022%29%2C%20NWSL%20%282022%29%20and%20international%0Asoccer%20%282020-2022%29.%20With%20this%20data%20we%20demonstrate%20that%20gender-specific%20Graph%0ANeural%20Networks%20outperform%20architecturally%20identical%20gender-ambiguous%20models%20in%0Apredicting%20the%20successful%20outcome%20of%20counterattacks.%20We%20show%2C%20using%20Permutation%0AFeature%20Importance%2C%20that%20byline%20to%20byline%20speed%2C%20angle%20to%20the%20goal%2C%20angle%20to%0Athe%20ball%20and%20sideline%20to%20sideline%20speed%20are%20the%20node%20features%20with%20the%20highest%0Aimpact%20on%20model%20performance.%0A%20%20Additionally%2C%20we%20offer%20some%20illustrative%20examples%20on%20how%20to%20navigate%20the%0Ainfinite%20solution%20search%20space%20to%20aid%20in%20identifying%20improvements%20for%20player%0Adecision%20making.%0A%20%20This%20research%20is%20accompanied%20by%20an%20open-source%20repository%20containing%20all%20data%0Aand%20code%2C%20and%20it%20is%20also%20accompanied%20by%20an%20open-source%20Python%20package%20which%0Asimplifies%20converting%20spatiotemporal%20data%20into%20graphs.%20This%20package%20also%0Afacilitates%20testing%2C%20validation%2C%20training%20and%20prediction%20with%20this%20data.%20This%0Ashould%20allow%20the%20reader%20to%20replicate%20and%20improve%20upon%20our%20research%20more%20easily.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2411.17450v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DA%2520Graph%2520Neural%2520Network%2520deep-dive%2520into%2520successful%2520counterattacks%26entry.906535625%3DJoris%2520Bekkers%2520and%2520Amod%2520Sahasrabudhe%26entry.1292438233%3D%2520%2520A%2520counterattack%2520in%2520soccer%2520is%2520a%2520high%2520speed%252C%2520high%2520intensity%2520direct%2520attack%2520that%250Acan%2520occur%2520when%2520a%2520team%2520transitions%2520from%2520a%2520defensive%2520state%2520to%2520an%2520attacking%2520state%250Aafter%2520regaining%2520possession%2520of%2520the%2520ball.%2520The%2520aim%2520is%2520to%2520create%2520a%2520goal-scoring%250Aopportunity%2520by%2520convering%2520a%2520lot%2520of%2520ground%2520with%2520minimal%2520passes%2520before%2520the%250Aopposing%2520team%2520can%2520recover%2520their%2520defensive%2520shape.%2520The%2520purpose%2520of%2520this%2520research%250Ais%2520to%2520build%2520gender-specific%2520Graph%2520Neural%2520Networks%2520to%2520model%2520the%2520likelihood%2520of%2520a%250Acounterattack%2520being%2520successful%2520and%2520uncover%2520what%2520factors%2520make%2520them%2520successful%2520in%250Aprofessional%2520soccer.%2520These%2520models%2520are%2520trained%2520on%2520a%2520total%2520of%252020863%2520frames%2520of%250Asynchronized%2520on-ball%2520event%2520and%2520spatiotemporal%2520%2528broadcast%2529%2520tracking%2520data.%2520This%250Adataset%2520is%2520derived%2520from%2520632%2520games%2520of%2520MLS%2520%25282022%2529%252C%2520NWSL%2520%25282022%2529%2520and%2520international%250Asoccer%2520%25282020-2022%2529.%2520With%2520this%2520data%2520we%2520demonstrate%2520that%2520gender-specific%2520Graph%250ANeural%2520Networks%2520outperform%2520architecturally%2520identical%2520gender-ambiguous%2520models%2520in%250Apredicting%2520the%2520successful%2520outcome%2520of%2520counterattacks.%2520We%2520show%252C%2520using%2520Permutation%250AFeature%2520Importance%252C%2520that%2520byline%2520to%2520byline%2520speed%252C%2520angle%2520to%2520the%2520goal%252C%2520angle%2520to%250Athe%2520ball%2520and%2520sideline%2520to%2520sideline%2520speed%2520are%2520the%2520node%2520features%2520with%2520the%2520highest%250Aimpact%2520on%2520model%2520performance.%250A%2520%2520Additionally%252C%2520we%2520offer%2520some%2520illustrative%2520examples%2520on%2520how%2520to%2520navigate%2520the%250Ainfinite%2520solution%2520search%2520space%2520to%2520aid%2520in%2520identifying%2520improvements%2520for%2520player%250Adecision%2520making.%250A%2520%2520This%2520research%2520is%2520accompanied%2520by%2520an%2520open-source%2520repository%2520containing%2520all%2520data%250Aand%2520code%252C%2520and%2520it%2520is%2520also%2520accompanied%2520by%2520an%2520open-source%2520Python%2520package%2520which%250Asimplifies%2520converting%2520spatiotemporal%2520data%2520into%2520graphs.%2520This%2520package%2520also%250Afacilitates%2520testing%252C%2520validation%252C%2520training%2520and%2520prediction%2520with%2520this%2520data.%2520This%250Ashould%2520allow%2520the%2520reader%2520to%2520replicate%2520and%2520improve%2520upon%2520our%2520research%2520more%2520easily.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2411.17450v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=A%20Graph%20Neural%20Network%20deep-dive%20into%20successful%20counterattacks&entry.906535625=Joris%20Bekkers%20and%20Amod%20Sahasrabudhe&entry.1292438233=%20%20A%20counterattack%20in%20soccer%20is%20a%20high%20speed%2C%20high%20intensity%20direct%20attack%20that%0Acan%20occur%20when%20a%20team%20transitions%20from%20a%20defensive%20state%20to%20an%20attacking%20state%0Aafter%20regaining%20possession%20of%20the%20ball.%20The%20aim%20is%20to%20create%20a%20goal-scoring%0Aopportunity%20by%20convering%20a%20lot%20of%20ground%20with%20minimal%20passes%20before%20the%0Aopposing%20team%20can%20recover%20their%20defensive%20shape.%20The%20purpose%20of%20this%20research%0Ais%20to%20build%20gender-specific%20Graph%20Neural%20Networks%20to%20model%20the%20likelihood%20of%20a%0Acounterattack%20being%20successful%20and%20uncover%20what%20factors%20make%20them%20successful%20in%0Aprofessional%20soccer.%20These%20models%20are%20trained%20on%20a%20total%20of%2020863%20frames%20of%0Asynchronized%20on-ball%20event%20and%20spatiotemporal%20%28broadcast%29%20tracking%20data.%20This%0Adataset%20is%20derived%20from%20632%20games%20of%20MLS%20%282022%29%2C%20NWSL%20%282022%29%20and%20international%0Asoccer%20%282020-2022%29.%20With%20this%20data%20we%20demonstrate%20that%20gender-specific%20Graph%0ANeural%20Networks%20outperform%20architecturally%20identical%20gender-ambiguous%20models%20in%0Apredicting%20the%20successful%20outcome%20of%20counterattacks.%20We%20show%2C%20using%20Permutation%0AFeature%20Importance%2C%20that%20byline%20to%20byline%20speed%2C%20angle%20to%20the%20goal%2C%20angle%20to%0Athe%20ball%20and%20sideline%20to%20sideline%20speed%20are%20the%20node%20features%20with%20the%20highest%0Aimpact%20on%20model%20performance.%0A%20%20Additionally%2C%20we%20offer%20some%20illustrative%20examples%20on%20how%20to%20navigate%20the%0Ainfinite%20solution%20search%20space%20to%20aid%20in%20identifying%20improvements%20for%20player%0Adecision%20making.%0A%20%20This%20research%20is%20accompanied%20by%20an%20open-source%20repository%20containing%20all%20data%0Aand%20code%2C%20and%20it%20is%20also%20accompanied%20by%20an%20open-source%20Python%20package%20which%0Asimplifies%20converting%20spatiotemporal%20data%20into%20graphs.%20This%20package%20also%0Afacilitates%20testing%2C%20validation%2C%20training%20and%20prediction%20with%20this%20data.%20This%0Ashould%20allow%20the%20reader%20to%20replicate%20and%20improve%20upon%20our%20research%20more%20easily.%0A&entry.1838667208=http%3A//arxiv.org/abs/2411.17450v1&entry.124074799=Read"},
{"title": "Pre-training for Action Recognition with Automatically Generated Fractal\n  Datasets", "author": "Davyd Svyezhentsev and George Retsinas and Petros Maragos", "abstract": "  In recent years, interest in synthetic data has grown, particularly in the\ncontext of pre-training the image modality to support a range of computer\nvision tasks, including object classification, medical imaging etc. Previous\nwork has demonstrated that synthetic samples, automatically produced by various\ngenerative processes, can replace real counterparts and yield strong visual\nrepresentations. This approach resolves issues associated with real data such\nas collection and labeling costs, copyright and privacy.\n  We extend this trend to the video domain applying it to the task of action\nrecognition. Employing fractal geometry, we present methods to automatically\nproduce large-scale datasets of short synthetic video clips, which can be\nutilized for pre-training neural models. The generated video clips are\ncharacterized by notable variety, stemmed by the innate ability of fractals to\ngenerate complex multi-scale structures. To narrow the domain gap, we further\nidentify key properties of real videos and carefully emulate them during\npre-training. Through thorough ablations, we determine the attributes that\nstrengthen downstream results and offer general guidelines for pre-training\nwith synthetic videos. The proposed approach is evaluated by fine-tuning\npre-trained models on established action recognition datasets HMDB51 and UCF101\nas well as four other video benchmarks related to group action recognition,\nfine-grained action recognition and dynamic scenes. Compared to standard\nKinetics pre-training, our reported results come close and are even superior on\na portion of downstream datasets. Code and samples of synthetic videos are\navailable at https://github.com/davidsvy/fractal_video .\n", "link": "http://arxiv.org/abs/2411.17584v1", "date": "2024-11-26", "relevancy": 2.1792, "topK": [{"title": "Weakly-guided self-supervised pretraining for temporal activity detection", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/25189", "similarity": 0.5803}, {"title": "Tuning-Free Noise Rectification for High Fidelity Image-to-Video\n  Generation", "link": "http://arxiv.org/abs/2403.02827v1", "similarity": 0.5201}, {"title": "Customize-A-Video: One-Shot Motion Customization of Text-to-Video\n  Diffusion Models", "link": "http://arxiv.org/abs/2402.14780v1", "similarity": 0.5179}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Pre-training%20for%20Action%20Recognition%20with%20Automatically%20Generated%20Fractal%0A%20%20Datasets&body=Title%3A%20Pre-training%20for%20Action%20Recognition%20with%20Automatically%20Generated%20Fractal%0A%20%20Datasets%0AAuthor%3A%20Davyd%20Svyezhentsev%20and%20George%20Retsinas%20and%20Petros%20Maragos%0AAbstract%3A%20%20%20In%20recent%20years%2C%20interest%20in%20synthetic%20data%20has%20grown%2C%20particularly%20in%20the%0Acontext%20of%20pre-training%20the%20image%20modality%20to%20support%20a%20range%20of%20computer%0Avision%20tasks%2C%20including%20object%20classification%2C%20medical%20imaging%20etc.%20Previous%0Awork%20has%20demonstrated%20that%20synthetic%20samples%2C%20automatically%20produced%20by%20various%0Agenerative%20processes%2C%20can%20replace%20real%20counterparts%20and%20yield%20strong%20visual%0Arepresentations.%20This%20approach%20resolves%20issues%20associated%20with%20real%20data%20such%0Aas%20collection%20and%20labeling%20costs%2C%20copyright%20and%20privacy.%0A%20%20We%20extend%20this%20trend%20to%20the%20video%20domain%20applying%20it%20to%20the%20task%20of%20action%0Arecognition.%20Employing%20fractal%20geometry%2C%20we%20present%20methods%20to%20automatically%0Aproduce%20large-scale%20datasets%20of%20short%20synthetic%20video%20clips%2C%20which%20can%20be%0Autilized%20for%20pre-training%20neural%20models.%20The%20generated%20video%20clips%20are%0Acharacterized%20by%20notable%20variety%2C%20stemmed%20by%20the%20innate%20ability%20of%20fractals%20to%0Agenerate%20complex%20multi-scale%20structures.%20To%20narrow%20the%20domain%20gap%2C%20we%20further%0Aidentify%20key%20properties%20of%20real%20videos%20and%20carefully%20emulate%20them%20during%0Apre-training.%20Through%20thorough%20ablations%2C%20we%20determine%20the%20attributes%20that%0Astrengthen%20downstream%20results%20and%20offer%20general%20guidelines%20for%20pre-training%0Awith%20synthetic%20videos.%20The%20proposed%20approach%20is%20evaluated%20by%20fine-tuning%0Apre-trained%20models%20on%20established%20action%20recognition%20datasets%20HMDB51%20and%20UCF101%0Aas%20well%20as%20four%20other%20video%20benchmarks%20related%20to%20group%20action%20recognition%2C%0Afine-grained%20action%20recognition%20and%20dynamic%20scenes.%20Compared%20to%20standard%0AKinetics%20pre-training%2C%20our%20reported%20results%20come%20close%20and%20are%20even%20superior%20on%0Aa%20portion%20of%20downstream%20datasets.%20Code%20and%20samples%20of%20synthetic%20videos%20are%0Aavailable%20at%20https%3A//github.com/davidsvy/fractal_video%20.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2411.17584v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DPre-training%2520for%2520Action%2520Recognition%2520with%2520Automatically%2520Generated%2520Fractal%250A%2520%2520Datasets%26entry.906535625%3DDavyd%2520Svyezhentsev%2520and%2520George%2520Retsinas%2520and%2520Petros%2520Maragos%26entry.1292438233%3D%2520%2520In%2520recent%2520years%252C%2520interest%2520in%2520synthetic%2520data%2520has%2520grown%252C%2520particularly%2520in%2520the%250Acontext%2520of%2520pre-training%2520the%2520image%2520modality%2520to%2520support%2520a%2520range%2520of%2520computer%250Avision%2520tasks%252C%2520including%2520object%2520classification%252C%2520medical%2520imaging%2520etc.%2520Previous%250Awork%2520has%2520demonstrated%2520that%2520synthetic%2520samples%252C%2520automatically%2520produced%2520by%2520various%250Agenerative%2520processes%252C%2520can%2520replace%2520real%2520counterparts%2520and%2520yield%2520strong%2520visual%250Arepresentations.%2520This%2520approach%2520resolves%2520issues%2520associated%2520with%2520real%2520data%2520such%250Aas%2520collection%2520and%2520labeling%2520costs%252C%2520copyright%2520and%2520privacy.%250A%2520%2520We%2520extend%2520this%2520trend%2520to%2520the%2520video%2520domain%2520applying%2520it%2520to%2520the%2520task%2520of%2520action%250Arecognition.%2520Employing%2520fractal%2520geometry%252C%2520we%2520present%2520methods%2520to%2520automatically%250Aproduce%2520large-scale%2520datasets%2520of%2520short%2520synthetic%2520video%2520clips%252C%2520which%2520can%2520be%250Autilized%2520for%2520pre-training%2520neural%2520models.%2520The%2520generated%2520video%2520clips%2520are%250Acharacterized%2520by%2520notable%2520variety%252C%2520stemmed%2520by%2520the%2520innate%2520ability%2520of%2520fractals%2520to%250Agenerate%2520complex%2520multi-scale%2520structures.%2520To%2520narrow%2520the%2520domain%2520gap%252C%2520we%2520further%250Aidentify%2520key%2520properties%2520of%2520real%2520videos%2520and%2520carefully%2520emulate%2520them%2520during%250Apre-training.%2520Through%2520thorough%2520ablations%252C%2520we%2520determine%2520the%2520attributes%2520that%250Astrengthen%2520downstream%2520results%2520and%2520offer%2520general%2520guidelines%2520for%2520pre-training%250Awith%2520synthetic%2520videos.%2520The%2520proposed%2520approach%2520is%2520evaluated%2520by%2520fine-tuning%250Apre-trained%2520models%2520on%2520established%2520action%2520recognition%2520datasets%2520HMDB51%2520and%2520UCF101%250Aas%2520well%2520as%2520four%2520other%2520video%2520benchmarks%2520related%2520to%2520group%2520action%2520recognition%252C%250Afine-grained%2520action%2520recognition%2520and%2520dynamic%2520scenes.%2520Compared%2520to%2520standard%250AKinetics%2520pre-training%252C%2520our%2520reported%2520results%2520come%2520close%2520and%2520are%2520even%2520superior%2520on%250Aa%2520portion%2520of%2520downstream%2520datasets.%2520Code%2520and%2520samples%2520of%2520synthetic%2520videos%2520are%250Aavailable%2520at%2520https%253A//github.com/davidsvy/fractal_video%2520.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2411.17584v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Pre-training%20for%20Action%20Recognition%20with%20Automatically%20Generated%20Fractal%0A%20%20Datasets&entry.906535625=Davyd%20Svyezhentsev%20and%20George%20Retsinas%20and%20Petros%20Maragos&entry.1292438233=%20%20In%20recent%20years%2C%20interest%20in%20synthetic%20data%20has%20grown%2C%20particularly%20in%20the%0Acontext%20of%20pre-training%20the%20image%20modality%20to%20support%20a%20range%20of%20computer%0Avision%20tasks%2C%20including%20object%20classification%2C%20medical%20imaging%20etc.%20Previous%0Awork%20has%20demonstrated%20that%20synthetic%20samples%2C%20automatically%20produced%20by%20various%0Agenerative%20processes%2C%20can%20replace%20real%20counterparts%20and%20yield%20strong%20visual%0Arepresentations.%20This%20approach%20resolves%20issues%20associated%20with%20real%20data%20such%0Aas%20collection%20and%20labeling%20costs%2C%20copyright%20and%20privacy.%0A%20%20We%20extend%20this%20trend%20to%20the%20video%20domain%20applying%20it%20to%20the%20task%20of%20action%0Arecognition.%20Employing%20fractal%20geometry%2C%20we%20present%20methods%20to%20automatically%0Aproduce%20large-scale%20datasets%20of%20short%20synthetic%20video%20clips%2C%20which%20can%20be%0Autilized%20for%20pre-training%20neural%20models.%20The%20generated%20video%20clips%20are%0Acharacterized%20by%20notable%20variety%2C%20stemmed%20by%20the%20innate%20ability%20of%20fractals%20to%0Agenerate%20complex%20multi-scale%20structures.%20To%20narrow%20the%20domain%20gap%2C%20we%20further%0Aidentify%20key%20properties%20of%20real%20videos%20and%20carefully%20emulate%20them%20during%0Apre-training.%20Through%20thorough%20ablations%2C%20we%20determine%20the%20attributes%20that%0Astrengthen%20downstream%20results%20and%20offer%20general%20guidelines%20for%20pre-training%0Awith%20synthetic%20videos.%20The%20proposed%20approach%20is%20evaluated%20by%20fine-tuning%0Apre-trained%20models%20on%20established%20action%20recognition%20datasets%20HMDB51%20and%20UCF101%0Aas%20well%20as%20four%20other%20video%20benchmarks%20related%20to%20group%20action%20recognition%2C%0Afine-grained%20action%20recognition%20and%20dynamic%20scenes.%20Compared%20to%20standard%0AKinetics%20pre-training%2C%20our%20reported%20results%20come%20close%20and%20are%20even%20superior%20on%0Aa%20portion%20of%20downstream%20datasets.%20Code%20and%20samples%20of%20synthetic%20videos%20are%0Aavailable%20at%20https%3A//github.com/davidsvy/fractal_video%20.%0A&entry.1838667208=http%3A//arxiv.org/abs/2411.17584v1&entry.124074799=Read"},
{"title": "Rethinking Token Reduction in MLLMs: Towards a Unified Paradigm for\n  Training-Free Acceleration", "author": "Yuhang Han and Xuyang Liu and Pengxiang Ding and Donglin Wang and Honggang Chen and Qingsen Yan and Siteng Huang", "abstract": "  To accelerate the inference of heavy Multimodal Large Language Models\n(MLLMs), this study rethinks the current landscape of training-free token\nreduction research. We regret to find that the critical components of existing\nmethods are tightly intertwined, with their interconnections and effects\nremaining unclear for comparison, transfer, and expansion. Therefore, we\npropose a unified ''filter-correlate-compress'' paradigm that decomposes the\ntoken reduction into three distinct stages within a pipeline, maintaining\nconsistent design objectives and elements while allowing for unique\nimplementations. We additionally demystify the popular works and subsume them\ninto our paradigm to showcase its universality. Finally, we offer a suite of\nmethods grounded in the paradigm, striking a balance between speed and accuracy\nthroughout different phases of the inference. Experimental results across 10\nbenchmarks indicate that our methods can achieve up to an 82.4% reduction in\nFLOPs with a minimal impact on performance, simultaneously surpassing\nstate-of-the-art training-free methods. Our project page is at\nhttps://ficoco-accelerate.github.io/.\n", "link": "http://arxiv.org/abs/2411.17686v1", "date": "2024-11-26", "relevancy": 2.1786, "topK": [{"title": "ToDo: Token Downsampling for Efficient Generation of High-Resolution\n  Images", "link": "http://arxiv.org/abs/2402.13573v2", "similarity": 0.5623}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5467}, {"title": "SDXS: Real-Time One-Step Latent Diffusion Models with Image Conditions", "link": "http://arxiv.org/abs/2403.16627v1", "similarity": 0.4956}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Rethinking%20Token%20Reduction%20in%20MLLMs%3A%20Towards%20a%20Unified%20Paradigm%20for%0A%20%20Training-Free%20Acceleration&body=Title%3A%20Rethinking%20Token%20Reduction%20in%20MLLMs%3A%20Towards%20a%20Unified%20Paradigm%20for%0A%20%20Training-Free%20Acceleration%0AAuthor%3A%20Yuhang%20Han%20and%20Xuyang%20Liu%20and%20Pengxiang%20Ding%20and%20Donglin%20Wang%20and%20Honggang%20Chen%20and%20Qingsen%20Yan%20and%20Siteng%20Huang%0AAbstract%3A%20%20%20To%20accelerate%20the%20inference%20of%20heavy%20Multimodal%20Large%20Language%20Models%0A%28MLLMs%29%2C%20this%20study%20rethinks%20the%20current%20landscape%20of%20training-free%20token%0Areduction%20research.%20We%20regret%20to%20find%20that%20the%20critical%20components%20of%20existing%0Amethods%20are%20tightly%20intertwined%2C%20with%20their%20interconnections%20and%20effects%0Aremaining%20unclear%20for%20comparison%2C%20transfer%2C%20and%20expansion.%20Therefore%2C%20we%0Apropose%20a%20unified%20%27%27filter-correlate-compress%27%27%20paradigm%20that%20decomposes%20the%0Atoken%20reduction%20into%20three%20distinct%20stages%20within%20a%20pipeline%2C%20maintaining%0Aconsistent%20design%20objectives%20and%20elements%20while%20allowing%20for%20unique%0Aimplementations.%20We%20additionally%20demystify%20the%20popular%20works%20and%20subsume%20them%0Ainto%20our%20paradigm%20to%20showcase%20its%20universality.%20Finally%2C%20we%20offer%20a%20suite%20of%0Amethods%20grounded%20in%20the%20paradigm%2C%20striking%20a%20balance%20between%20speed%20and%20accuracy%0Athroughout%20different%20phases%20of%20the%20inference.%20Experimental%20results%20across%2010%0Abenchmarks%20indicate%20that%20our%20methods%20can%20achieve%20up%20to%20an%2082.4%25%20reduction%20in%0AFLOPs%20with%20a%20minimal%20impact%20on%20performance%2C%20simultaneously%20surpassing%0Astate-of-the-art%20training-free%20methods.%20Our%20project%20page%20is%20at%0Ahttps%3A//ficoco-accelerate.github.io/.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2411.17686v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DRethinking%2520Token%2520Reduction%2520in%2520MLLMs%253A%2520Towards%2520a%2520Unified%2520Paradigm%2520for%250A%2520%2520Training-Free%2520Acceleration%26entry.906535625%3DYuhang%2520Han%2520and%2520Xuyang%2520Liu%2520and%2520Pengxiang%2520Ding%2520and%2520Donglin%2520Wang%2520and%2520Honggang%2520Chen%2520and%2520Qingsen%2520Yan%2520and%2520Siteng%2520Huang%26entry.1292438233%3D%2520%2520To%2520accelerate%2520the%2520inference%2520of%2520heavy%2520Multimodal%2520Large%2520Language%2520Models%250A%2528MLLMs%2529%252C%2520this%2520study%2520rethinks%2520the%2520current%2520landscape%2520of%2520training-free%2520token%250Areduction%2520research.%2520We%2520regret%2520to%2520find%2520that%2520the%2520critical%2520components%2520of%2520existing%250Amethods%2520are%2520tightly%2520intertwined%252C%2520with%2520their%2520interconnections%2520and%2520effects%250Aremaining%2520unclear%2520for%2520comparison%252C%2520transfer%252C%2520and%2520expansion.%2520Therefore%252C%2520we%250Apropose%2520a%2520unified%2520%2527%2527filter-correlate-compress%2527%2527%2520paradigm%2520that%2520decomposes%2520the%250Atoken%2520reduction%2520into%2520three%2520distinct%2520stages%2520within%2520a%2520pipeline%252C%2520maintaining%250Aconsistent%2520design%2520objectives%2520and%2520elements%2520while%2520allowing%2520for%2520unique%250Aimplementations.%2520We%2520additionally%2520demystify%2520the%2520popular%2520works%2520and%2520subsume%2520them%250Ainto%2520our%2520paradigm%2520to%2520showcase%2520its%2520universality.%2520Finally%252C%2520we%2520offer%2520a%2520suite%2520of%250Amethods%2520grounded%2520in%2520the%2520paradigm%252C%2520striking%2520a%2520balance%2520between%2520speed%2520and%2520accuracy%250Athroughout%2520different%2520phases%2520of%2520the%2520inference.%2520Experimental%2520results%2520across%252010%250Abenchmarks%2520indicate%2520that%2520our%2520methods%2520can%2520achieve%2520up%2520to%2520an%252082.4%2525%2520reduction%2520in%250AFLOPs%2520with%2520a%2520minimal%2520impact%2520on%2520performance%252C%2520simultaneously%2520surpassing%250Astate-of-the-art%2520training-free%2520methods.%2520Our%2520project%2520page%2520is%2520at%250Ahttps%253A//ficoco-accelerate.github.io/.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2411.17686v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Rethinking%20Token%20Reduction%20in%20MLLMs%3A%20Towards%20a%20Unified%20Paradigm%20for%0A%20%20Training-Free%20Acceleration&entry.906535625=Yuhang%20Han%20and%20Xuyang%20Liu%20and%20Pengxiang%20Ding%20and%20Donglin%20Wang%20and%20Honggang%20Chen%20and%20Qingsen%20Yan%20and%20Siteng%20Huang&entry.1292438233=%20%20To%20accelerate%20the%20inference%20of%20heavy%20Multimodal%20Large%20Language%20Models%0A%28MLLMs%29%2C%20this%20study%20rethinks%20the%20current%20landscape%20of%20training-free%20token%0Areduction%20research.%20We%20regret%20to%20find%20that%20the%20critical%20components%20of%20existing%0Amethods%20are%20tightly%20intertwined%2C%20with%20their%20interconnections%20and%20effects%0Aremaining%20unclear%20for%20comparison%2C%20transfer%2C%20and%20expansion.%20Therefore%2C%20we%0Apropose%20a%20unified%20%27%27filter-correlate-compress%27%27%20paradigm%20that%20decomposes%20the%0Atoken%20reduction%20into%20three%20distinct%20stages%20within%20a%20pipeline%2C%20maintaining%0Aconsistent%20design%20objectives%20and%20elements%20while%20allowing%20for%20unique%0Aimplementations.%20We%20additionally%20demystify%20the%20popular%20works%20and%20subsume%20them%0Ainto%20our%20paradigm%20to%20showcase%20its%20universality.%20Finally%2C%20we%20offer%20a%20suite%20of%0Amethods%20grounded%20in%20the%20paradigm%2C%20striking%20a%20balance%20between%20speed%20and%20accuracy%0Athroughout%20different%20phases%20of%20the%20inference.%20Experimental%20results%20across%2010%0Abenchmarks%20indicate%20that%20our%20methods%20can%20achieve%20up%20to%20an%2082.4%25%20reduction%20in%0AFLOPs%20with%20a%20minimal%20impact%20on%20performance%2C%20simultaneously%20surpassing%0Astate-of-the-art%20training-free%20methods.%20Our%20project%20page%20is%20at%0Ahttps%3A//ficoco-accelerate.github.io/.%0A&entry.1838667208=http%3A//arxiv.org/abs/2411.17686v1&entry.124074799=Read"},
{"title": "Attamba: Attending To Multi-Token States", "author": "Yash Akhauri and Safeen Huda and Mohamed S. Abdelfattah", "abstract": "  When predicting the next token in a sequence, vanilla transformers compute\nattention over all previous tokens, resulting in quadratic scaling of compute\nwith sequence length. State-space models compress the entire sequence of tokens\ninto a fixed-dimensional representation to improve efficiency, while other\narchitectures achieve sub-quadratic complexity via low-rank projections or\nsparse attention patterns over the sequence. In this paper, we introduce\nAttamba, a novel architecture that uses state-space models to compress chunks\nof tokens and applies attention on these compressed key-value representations.\nWe find that replacing key and value projections in a transformer with SSMs can\nimprove model quality and enable flexible token chunking, resulting in 24%\nimproved perplexity with transformer of similar KV-Cache and attention\nfootprint, and ~4 times smaller KV-Cache and Attention FLOPs for 5% perplexity\ntrade-off. Attamba can perform attention on chunked-sequences of variable\nlength, enabling a smooth transition between quadratic and linear scaling,\noffering adaptable efficiency gains.\n", "link": "http://arxiv.org/abs/2411.17685v1", "date": "2024-11-26", "relevancy": 2.1685, "topK": [{"title": "ToDo: Token Downsampling for Efficient Generation of High-Resolution\n  Images", "link": "http://arxiv.org/abs/2402.13573v2", "similarity": 0.5901}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5138}, {"title": "Scaling Rectified Flow Transformers for High-Resolution Image Synthesis", "link": "http://arxiv.org/abs/2403.03206v1", "similarity": 0.4931}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Attamba%3A%20Attending%20To%20Multi-Token%20States&body=Title%3A%20Attamba%3A%20Attending%20To%20Multi-Token%20States%0AAuthor%3A%20Yash%20Akhauri%20and%20Safeen%20Huda%20and%20Mohamed%20S.%20Abdelfattah%0AAbstract%3A%20%20%20When%20predicting%20the%20next%20token%20in%20a%20sequence%2C%20vanilla%20transformers%20compute%0Aattention%20over%20all%20previous%20tokens%2C%20resulting%20in%20quadratic%20scaling%20of%20compute%0Awith%20sequence%20length.%20State-space%20models%20compress%20the%20entire%20sequence%20of%20tokens%0Ainto%20a%20fixed-dimensional%20representation%20to%20improve%20efficiency%2C%20while%20other%0Aarchitectures%20achieve%20sub-quadratic%20complexity%20via%20low-rank%20projections%20or%0Asparse%20attention%20patterns%20over%20the%20sequence.%20In%20this%20paper%2C%20we%20introduce%0AAttamba%2C%20a%20novel%20architecture%20that%20uses%20state-space%20models%20to%20compress%20chunks%0Aof%20tokens%20and%20applies%20attention%20on%20these%20compressed%20key-value%20representations.%0AWe%20find%20that%20replacing%20key%20and%20value%20projections%20in%20a%20transformer%20with%20SSMs%20can%0Aimprove%20model%20quality%20and%20enable%20flexible%20token%20chunking%2C%20resulting%20in%2024%25%0Aimproved%20perplexity%20with%20transformer%20of%20similar%20KV-Cache%20and%20attention%0Afootprint%2C%20and%20~4%20times%20smaller%20KV-Cache%20and%20Attention%20FLOPs%20for%205%25%20perplexity%0Atrade-off.%20Attamba%20can%20perform%20attention%20on%20chunked-sequences%20of%20variable%0Alength%2C%20enabling%20a%20smooth%20transition%20between%20quadratic%20and%20linear%20scaling%2C%0Aoffering%20adaptable%20efficiency%20gains.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2411.17685v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DAttamba%253A%2520Attending%2520To%2520Multi-Token%2520States%26entry.906535625%3DYash%2520Akhauri%2520and%2520Safeen%2520Huda%2520and%2520Mohamed%2520S.%2520Abdelfattah%26entry.1292438233%3D%2520%2520When%2520predicting%2520the%2520next%2520token%2520in%2520a%2520sequence%252C%2520vanilla%2520transformers%2520compute%250Aattention%2520over%2520all%2520previous%2520tokens%252C%2520resulting%2520in%2520quadratic%2520scaling%2520of%2520compute%250Awith%2520sequence%2520length.%2520State-space%2520models%2520compress%2520the%2520entire%2520sequence%2520of%2520tokens%250Ainto%2520a%2520fixed-dimensional%2520representation%2520to%2520improve%2520efficiency%252C%2520while%2520other%250Aarchitectures%2520achieve%2520sub-quadratic%2520complexity%2520via%2520low-rank%2520projections%2520or%250Asparse%2520attention%2520patterns%2520over%2520the%2520sequence.%2520In%2520this%2520paper%252C%2520we%2520introduce%250AAttamba%252C%2520a%2520novel%2520architecture%2520that%2520uses%2520state-space%2520models%2520to%2520compress%2520chunks%250Aof%2520tokens%2520and%2520applies%2520attention%2520on%2520these%2520compressed%2520key-value%2520representations.%250AWe%2520find%2520that%2520replacing%2520key%2520and%2520value%2520projections%2520in%2520a%2520transformer%2520with%2520SSMs%2520can%250Aimprove%2520model%2520quality%2520and%2520enable%2520flexible%2520token%2520chunking%252C%2520resulting%2520in%252024%2525%250Aimproved%2520perplexity%2520with%2520transformer%2520of%2520similar%2520KV-Cache%2520and%2520attention%250Afootprint%252C%2520and%2520~4%2520times%2520smaller%2520KV-Cache%2520and%2520Attention%2520FLOPs%2520for%25205%2525%2520perplexity%250Atrade-off.%2520Attamba%2520can%2520perform%2520attention%2520on%2520chunked-sequences%2520of%2520variable%250Alength%252C%2520enabling%2520a%2520smooth%2520transition%2520between%2520quadratic%2520and%2520linear%2520scaling%252C%250Aoffering%2520adaptable%2520efficiency%2520gains.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2411.17685v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Attamba%3A%20Attending%20To%20Multi-Token%20States&entry.906535625=Yash%20Akhauri%20and%20Safeen%20Huda%20and%20Mohamed%20S.%20Abdelfattah&entry.1292438233=%20%20When%20predicting%20the%20next%20token%20in%20a%20sequence%2C%20vanilla%20transformers%20compute%0Aattention%20over%20all%20previous%20tokens%2C%20resulting%20in%20quadratic%20scaling%20of%20compute%0Awith%20sequence%20length.%20State-space%20models%20compress%20the%20entire%20sequence%20of%20tokens%0Ainto%20a%20fixed-dimensional%20representation%20to%20improve%20efficiency%2C%20while%20other%0Aarchitectures%20achieve%20sub-quadratic%20complexity%20via%20low-rank%20projections%20or%0Asparse%20attention%20patterns%20over%20the%20sequence.%20In%20this%20paper%2C%20we%20introduce%0AAttamba%2C%20a%20novel%20architecture%20that%20uses%20state-space%20models%20to%20compress%20chunks%0Aof%20tokens%20and%20applies%20attention%20on%20these%20compressed%20key-value%20representations.%0AWe%20find%20that%20replacing%20key%20and%20value%20projections%20in%20a%20transformer%20with%20SSMs%20can%0Aimprove%20model%20quality%20and%20enable%20flexible%20token%20chunking%2C%20resulting%20in%2024%25%0Aimproved%20perplexity%20with%20transformer%20of%20similar%20KV-Cache%20and%20attention%0Afootprint%2C%20and%20~4%20times%20smaller%20KV-Cache%20and%20Attention%20FLOPs%20for%205%25%20perplexity%0Atrade-off.%20Attamba%20can%20perform%20attention%20on%20chunked-sequences%20of%20variable%0Alength%2C%20enabling%20a%20smooth%20transition%20between%20quadratic%20and%20linear%20scaling%2C%0Aoffering%20adaptable%20efficiency%20gains.%0A&entry.1838667208=http%3A//arxiv.org/abs/2411.17685v1&entry.124074799=Read"},
{"title": "ECG-Based Patient Identification: A Comprehensive Evaluation Across\n  Health and Activity Conditions", "author": "Caterina Fuster-Barcel\u00f3 and Carmen C\u00e1mara and Pedro Peris-L\u00f3pez", "abstract": "  Over the course of the past two decades, a substantial body of research has\nsubstantiated the viability of utilising cardiac signals as a biometric\nmodality. This paper presents a novel approach for patient identification in\nhealthcare systems using electrocardiogram signals. A convolutional neural\nnetwork (CNN) is employed to classify users based on electrocardiomatrices, a\nspecific type of image derived from ECG signals. The proposed identification\nsystem is evaluated in multiple databases, achieving up to 99.84\\% accuracy on\nhealthy subjects, 97.09\\% on patients with cardiovascular diseases, and 97.89%\non mixed populations including both healthy and arrhythmic patients. The system\nalso performs robustly under varying activity conditions, achieving 91.32%\naccuracy in scenarios involving different physical activities. These consistent\nand reliable results, with low error rates such as a FAR of 0.01% and FRR of\n0.157% in the best cases, demonstrate the method's significant advancement in\nsubject identification within healthcare systems. By considering patients'\ncardiovascular conditions and activity levels, the proposed approach addresses\ngaps in the existing literature, positioning it as a strong candidate for\npractical applications in real-world healthcare settings.\n", "link": "http://arxiv.org/abs/2302.06529v3", "date": "2024-11-26", "relevancy": 2.1581, "topK": [{"title": "Magic-Me: Identity-Specific Video Customized Diffusion", "link": "http://arxiv.org/abs/2402.09368v1", "similarity": 0.4425}, {"title": "Weakly-guided self-supervised pretraining for temporal activity detection", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/25189", "similarity": 0.4299}, {"title": "Flexible Visual Recognition by Evidential Modeling of Confusion and Ignorance", "link": "http://openaccess.thecvf.com/content/ICCV2023/html/Fan_Flexible_Visual_Recognition_by_Evidential_Modeling_of_Confusion_and_Ignorance_ICCV_2023_paper.html", "similarity": 0.4224}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20ECG-Based%20Patient%20Identification%3A%20A%20Comprehensive%20Evaluation%20Across%0A%20%20Health%20and%20Activity%20Conditions&body=Title%3A%20ECG-Based%20Patient%20Identification%3A%20A%20Comprehensive%20Evaluation%20Across%0A%20%20Health%20and%20Activity%20Conditions%0AAuthor%3A%20Caterina%20Fuster-Barcel%C3%B3%20and%20Carmen%20C%C3%A1mara%20and%20Pedro%20Peris-L%C3%B3pez%0AAbstract%3A%20%20%20Over%20the%20course%20of%20the%20past%20two%20decades%2C%20a%20substantial%20body%20of%20research%20has%0Asubstantiated%20the%20viability%20of%20utilising%20cardiac%20signals%20as%20a%20biometric%0Amodality.%20This%20paper%20presents%20a%20novel%20approach%20for%20patient%20identification%20in%0Ahealthcare%20systems%20using%20electrocardiogram%20signals.%20A%20convolutional%20neural%0Anetwork%20%28CNN%29%20is%20employed%20to%20classify%20users%20based%20on%20electrocardiomatrices%2C%20a%0Aspecific%20type%20of%20image%20derived%20from%20ECG%20signals.%20The%20proposed%20identification%0Asystem%20is%20evaluated%20in%20multiple%20databases%2C%20achieving%20up%20to%2099.84%5C%25%20accuracy%20on%0Ahealthy%20subjects%2C%2097.09%5C%25%20on%20patients%20with%20cardiovascular%20diseases%2C%20and%2097.89%25%0Aon%20mixed%20populations%20including%20both%20healthy%20and%20arrhythmic%20patients.%20The%20system%0Aalso%20performs%20robustly%20under%20varying%20activity%20conditions%2C%20achieving%2091.32%25%0Aaccuracy%20in%20scenarios%20involving%20different%20physical%20activities.%20These%20consistent%0Aand%20reliable%20results%2C%20with%20low%20error%20rates%20such%20as%20a%20FAR%20of%200.01%25%20and%20FRR%20of%0A0.157%25%20in%20the%20best%20cases%2C%20demonstrate%20the%20method%27s%20significant%20advancement%20in%0Asubject%20identification%20within%20healthcare%20systems.%20By%20considering%20patients%27%0Acardiovascular%20conditions%20and%20activity%20levels%2C%20the%20proposed%20approach%20addresses%0Agaps%20in%20the%20existing%20literature%2C%20positioning%20it%20as%20a%20strong%20candidate%20for%0Apractical%20applications%20in%20real-world%20healthcare%20settings.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2302.06529v3%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DECG-Based%2520Patient%2520Identification%253A%2520A%2520Comprehensive%2520Evaluation%2520Across%250A%2520%2520Health%2520and%2520Activity%2520Conditions%26entry.906535625%3DCaterina%2520Fuster-Barcel%25C3%25B3%2520and%2520Carmen%2520C%25C3%25A1mara%2520and%2520Pedro%2520Peris-L%25C3%25B3pez%26entry.1292438233%3D%2520%2520Over%2520the%2520course%2520of%2520the%2520past%2520two%2520decades%252C%2520a%2520substantial%2520body%2520of%2520research%2520has%250Asubstantiated%2520the%2520viability%2520of%2520utilising%2520cardiac%2520signals%2520as%2520a%2520biometric%250Amodality.%2520This%2520paper%2520presents%2520a%2520novel%2520approach%2520for%2520patient%2520identification%2520in%250Ahealthcare%2520systems%2520using%2520electrocardiogram%2520signals.%2520A%2520convolutional%2520neural%250Anetwork%2520%2528CNN%2529%2520is%2520employed%2520to%2520classify%2520users%2520based%2520on%2520electrocardiomatrices%252C%2520a%250Aspecific%2520type%2520of%2520image%2520derived%2520from%2520ECG%2520signals.%2520The%2520proposed%2520identification%250Asystem%2520is%2520evaluated%2520in%2520multiple%2520databases%252C%2520achieving%2520up%2520to%252099.84%255C%2525%2520accuracy%2520on%250Ahealthy%2520subjects%252C%252097.09%255C%2525%2520on%2520patients%2520with%2520cardiovascular%2520diseases%252C%2520and%252097.89%2525%250Aon%2520mixed%2520populations%2520including%2520both%2520healthy%2520and%2520arrhythmic%2520patients.%2520The%2520system%250Aalso%2520performs%2520robustly%2520under%2520varying%2520activity%2520conditions%252C%2520achieving%252091.32%2525%250Aaccuracy%2520in%2520scenarios%2520involving%2520different%2520physical%2520activities.%2520These%2520consistent%250Aand%2520reliable%2520results%252C%2520with%2520low%2520error%2520rates%2520such%2520as%2520a%2520FAR%2520of%25200.01%2525%2520and%2520FRR%2520of%250A0.157%2525%2520in%2520the%2520best%2520cases%252C%2520demonstrate%2520the%2520method%2527s%2520significant%2520advancement%2520in%250Asubject%2520identification%2520within%2520healthcare%2520systems.%2520By%2520considering%2520patients%2527%250Acardiovascular%2520conditions%2520and%2520activity%2520levels%252C%2520the%2520proposed%2520approach%2520addresses%250Agaps%2520in%2520the%2520existing%2520literature%252C%2520positioning%2520it%2520as%2520a%2520strong%2520candidate%2520for%250Apractical%2520applications%2520in%2520real-world%2520healthcare%2520settings.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2302.06529v3%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=ECG-Based%20Patient%20Identification%3A%20A%20Comprehensive%20Evaluation%20Across%0A%20%20Health%20and%20Activity%20Conditions&entry.906535625=Caterina%20Fuster-Barcel%C3%B3%20and%20Carmen%20C%C3%A1mara%20and%20Pedro%20Peris-L%C3%B3pez&entry.1292438233=%20%20Over%20the%20course%20of%20the%20past%20two%20decades%2C%20a%20substantial%20body%20of%20research%20has%0Asubstantiated%20the%20viability%20of%20utilising%20cardiac%20signals%20as%20a%20biometric%0Amodality.%20This%20paper%20presents%20a%20novel%20approach%20for%20patient%20identification%20in%0Ahealthcare%20systems%20using%20electrocardiogram%20signals.%20A%20convolutional%20neural%0Anetwork%20%28CNN%29%20is%20employed%20to%20classify%20users%20based%20on%20electrocardiomatrices%2C%20a%0Aspecific%20type%20of%20image%20derived%20from%20ECG%20signals.%20The%20proposed%20identification%0Asystem%20is%20evaluated%20in%20multiple%20databases%2C%20achieving%20up%20to%2099.84%5C%25%20accuracy%20on%0Ahealthy%20subjects%2C%2097.09%5C%25%20on%20patients%20with%20cardiovascular%20diseases%2C%20and%2097.89%25%0Aon%20mixed%20populations%20including%20both%20healthy%20and%20arrhythmic%20patients.%20The%20system%0Aalso%20performs%20robustly%20under%20varying%20activity%20conditions%2C%20achieving%2091.32%25%0Aaccuracy%20in%20scenarios%20involving%20different%20physical%20activities.%20These%20consistent%0Aand%20reliable%20results%2C%20with%20low%20error%20rates%20such%20as%20a%20FAR%20of%200.01%25%20and%20FRR%20of%0A0.157%25%20in%20the%20best%20cases%2C%20demonstrate%20the%20method%27s%20significant%20advancement%20in%0Asubject%20identification%20within%20healthcare%20systems.%20By%20considering%20patients%27%0Acardiovascular%20conditions%20and%20activity%20levels%2C%20the%20proposed%20approach%20addresses%0Agaps%20in%20the%20existing%20literature%2C%20positioning%20it%20as%20a%20strong%20candidate%20for%0Apractical%20applications%20in%20real-world%20healthcare%20settings.%0A&entry.1838667208=http%3A//arxiv.org/abs/2302.06529v3&entry.124074799=Read"},
{"title": "Mixed-State Quantum Denoising Diffusion Probabilistic Model", "author": "Gino Kwun and Bingzhi Zhang and Quntao Zhuang", "abstract": "  Generative quantum machine learning has gained significant attention for its\nability to produce quantum states with desired distributions. Among various\nquantum generative models, quantum denoising diffusion probabilistic models\n(QuDDPMs) [Phys. Rev. Lett. 132, 100602 (2024)] provide a promising approach\nwith stepwise learning that resolves the training issues. However, the\nrequirement of high-fidelity scrambling unitaries in QuDDPM poses a challenge\nin near-term implementation. We propose the \\textit{mixed-state quantum\ndenoising diffusion probabilistic model} (MSQuDDPM) to eliminate the need for\nscrambling unitaries. Our approach focuses on adapting the quantum noise\nchannels to the model architecture, which integrates depolarizing noise\nchannels in the forward diffusion process and parameterized quantum circuits\nwith projective measurements in the backward denoising steps. We also introduce\nseveral techniques to improve MSQuDDPM, including a cosine-exponent schedule of\nnoise interpolation, the use of single-qubit random ancilla, and\nsuperfidelity-based cost functions to enhance the convergence. We evaluate\nMSQuDDPM on quantum ensemble generation tasks, demonstrating its successful\nperformance.\n", "link": "http://arxiv.org/abs/2411.17608v1", "date": "2024-11-26", "relevancy": 2.1474, "topK": [{"title": "SDXS: Real-Time One-Step Latent Diffusion Models with Image Conditions", "link": "http://arxiv.org/abs/2403.16627v1", "similarity": 0.5768}, {"title": "ToDo: Token Downsampling for Efficient Generation of High-Resolution\n  Images", "link": "http://arxiv.org/abs/2402.13573v2", "similarity": 0.5323}, {"title": "Tuning-Free Noise Rectification for High Fidelity Image-to-Video\n  Generation", "link": "http://arxiv.org/abs/2403.02827v1", "similarity": 0.5254}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Mixed-State%20Quantum%20Denoising%20Diffusion%20Probabilistic%20Model&body=Title%3A%20Mixed-State%20Quantum%20Denoising%20Diffusion%20Probabilistic%20Model%0AAuthor%3A%20Gino%20Kwun%20and%20Bingzhi%20Zhang%20and%20Quntao%20Zhuang%0AAbstract%3A%20%20%20Generative%20quantum%20machine%20learning%20has%20gained%20significant%20attention%20for%20its%0Aability%20to%20produce%20quantum%20states%20with%20desired%20distributions.%20Among%20various%0Aquantum%20generative%20models%2C%20quantum%20denoising%20diffusion%20probabilistic%20models%0A%28QuDDPMs%29%20%5BPhys.%20Rev.%20Lett.%20132%2C%20100602%20%282024%29%5D%20provide%20a%20promising%20approach%0Awith%20stepwise%20learning%20that%20resolves%20the%20training%20issues.%20However%2C%20the%0Arequirement%20of%20high-fidelity%20scrambling%20unitaries%20in%20QuDDPM%20poses%20a%20challenge%0Ain%20near-term%20implementation.%20We%20propose%20the%20%5Ctextit%7Bmixed-state%20quantum%0Adenoising%20diffusion%20probabilistic%20model%7D%20%28MSQuDDPM%29%20to%20eliminate%20the%20need%20for%0Ascrambling%20unitaries.%20Our%20approach%20focuses%20on%20adapting%20the%20quantum%20noise%0Achannels%20to%20the%20model%20architecture%2C%20which%20integrates%20depolarizing%20noise%0Achannels%20in%20the%20forward%20diffusion%20process%20and%20parameterized%20quantum%20circuits%0Awith%20projective%20measurements%20in%20the%20backward%20denoising%20steps.%20We%20also%20introduce%0Aseveral%20techniques%20to%20improve%20MSQuDDPM%2C%20including%20a%20cosine-exponent%20schedule%20of%0Anoise%20interpolation%2C%20the%20use%20of%20single-qubit%20random%20ancilla%2C%20and%0Asuperfidelity-based%20cost%20functions%20to%20enhance%20the%20convergence.%20We%20evaluate%0AMSQuDDPM%20on%20quantum%20ensemble%20generation%20tasks%2C%20demonstrating%20its%20successful%0Aperformance.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2411.17608v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DMixed-State%2520Quantum%2520Denoising%2520Diffusion%2520Probabilistic%2520Model%26entry.906535625%3DGino%2520Kwun%2520and%2520Bingzhi%2520Zhang%2520and%2520Quntao%2520Zhuang%26entry.1292438233%3D%2520%2520Generative%2520quantum%2520machine%2520learning%2520has%2520gained%2520significant%2520attention%2520for%2520its%250Aability%2520to%2520produce%2520quantum%2520states%2520with%2520desired%2520distributions.%2520Among%2520various%250Aquantum%2520generative%2520models%252C%2520quantum%2520denoising%2520diffusion%2520probabilistic%2520models%250A%2528QuDDPMs%2529%2520%255BPhys.%2520Rev.%2520Lett.%2520132%252C%2520100602%2520%25282024%2529%255D%2520provide%2520a%2520promising%2520approach%250Awith%2520stepwise%2520learning%2520that%2520resolves%2520the%2520training%2520issues.%2520However%252C%2520the%250Arequirement%2520of%2520high-fidelity%2520scrambling%2520unitaries%2520in%2520QuDDPM%2520poses%2520a%2520challenge%250Ain%2520near-term%2520implementation.%2520We%2520propose%2520the%2520%255Ctextit%257Bmixed-state%2520quantum%250Adenoising%2520diffusion%2520probabilistic%2520model%257D%2520%2528MSQuDDPM%2529%2520to%2520eliminate%2520the%2520need%2520for%250Ascrambling%2520unitaries.%2520Our%2520approach%2520focuses%2520on%2520adapting%2520the%2520quantum%2520noise%250Achannels%2520to%2520the%2520model%2520architecture%252C%2520which%2520integrates%2520depolarizing%2520noise%250Achannels%2520in%2520the%2520forward%2520diffusion%2520process%2520and%2520parameterized%2520quantum%2520circuits%250Awith%2520projective%2520measurements%2520in%2520the%2520backward%2520denoising%2520steps.%2520We%2520also%2520introduce%250Aseveral%2520techniques%2520to%2520improve%2520MSQuDDPM%252C%2520including%2520a%2520cosine-exponent%2520schedule%2520of%250Anoise%2520interpolation%252C%2520the%2520use%2520of%2520single-qubit%2520random%2520ancilla%252C%2520and%250Asuperfidelity-based%2520cost%2520functions%2520to%2520enhance%2520the%2520convergence.%2520We%2520evaluate%250AMSQuDDPM%2520on%2520quantum%2520ensemble%2520generation%2520tasks%252C%2520demonstrating%2520its%2520successful%250Aperformance.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2411.17608v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Mixed-State%20Quantum%20Denoising%20Diffusion%20Probabilistic%20Model&entry.906535625=Gino%20Kwun%20and%20Bingzhi%20Zhang%20and%20Quntao%20Zhuang&entry.1292438233=%20%20Generative%20quantum%20machine%20learning%20has%20gained%20significant%20attention%20for%20its%0Aability%20to%20produce%20quantum%20states%20with%20desired%20distributions.%20Among%20various%0Aquantum%20generative%20models%2C%20quantum%20denoising%20diffusion%20probabilistic%20models%0A%28QuDDPMs%29%20%5BPhys.%20Rev.%20Lett.%20132%2C%20100602%20%282024%29%5D%20provide%20a%20promising%20approach%0Awith%20stepwise%20learning%20that%20resolves%20the%20training%20issues.%20However%2C%20the%0Arequirement%20of%20high-fidelity%20scrambling%20unitaries%20in%20QuDDPM%20poses%20a%20challenge%0Ain%20near-term%20implementation.%20We%20propose%20the%20%5Ctextit%7Bmixed-state%20quantum%0Adenoising%20diffusion%20probabilistic%20model%7D%20%28MSQuDDPM%29%20to%20eliminate%20the%20need%20for%0Ascrambling%20unitaries.%20Our%20approach%20focuses%20on%20adapting%20the%20quantum%20noise%0Achannels%20to%20the%20model%20architecture%2C%20which%20integrates%20depolarizing%20noise%0Achannels%20in%20the%20forward%20diffusion%20process%20and%20parameterized%20quantum%20circuits%0Awith%20projective%20measurements%20in%20the%20backward%20denoising%20steps.%20We%20also%20introduce%0Aseveral%20techniques%20to%20improve%20MSQuDDPM%2C%20including%20a%20cosine-exponent%20schedule%20of%0Anoise%20interpolation%2C%20the%20use%20of%20single-qubit%20random%20ancilla%2C%20and%0Asuperfidelity-based%20cost%20functions%20to%20enhance%20the%20convergence.%20We%20evaluate%0AMSQuDDPM%20on%20quantum%20ensemble%20generation%20tasks%2C%20demonstrating%20its%20successful%0Aperformance.%0A&entry.1838667208=http%3A//arxiv.org/abs/2411.17608v1&entry.124074799=Read"},
{"title": "BPP-Search: Enhancing Tree of Thought Reasoning for Mathematical\n  Modeling Problem Solving", "author": "Teng Wang and Wing-Yin Yu and Zhenqi He and Zehua Liu and Xiongwei Han and Hailei Gong and Han Wu and Wei Shi and Ruifeng She and Fangzhou Zhu and Tao Zhong", "abstract": "  LLMs exhibit advanced reasoning capabilities, offering the potential to\ntransform natural language questions into mathematical models. However,\nexisting open-source operations research datasets lack detailed annotations of\nthe modeling process, such as variable definitions, focusing solely on\nobjective values, which hinders reinforcement learning applications. To address\nthis, we release the StructuredOR dataset, annotated with comprehensive labels\nthat capture the complete mathematical modeling process. We further propose\nBPP-Search, a algorithm that integrates reinforcement learning into a\ntree-of-thought structure using Beam search, a Process reward model, and a\npairwise Preference algorithm. This approach enables efficient exploration of\ntree structures, avoiding exhaustive search while improving accuracy. Extensive\nexperiments on StructuredOR, NL4OPT, and MAMO-ComplexLP datasets show that\nBPP-Search significantly outperforms state-of-the-art methods, including\nChain-of-Thought, Self-Consistency, and Tree-of-Thought. In tree-based\nreasoning, BPP-Search also surpasses Process Reward Model combined with Greedy\nor Beam Search, demonstrating superior accuracy and efficiency, and enabling\nfaster retrieval of correct solutions.\n", "link": "http://arxiv.org/abs/2411.17404v1", "date": "2024-11-26", "relevancy": 2.145, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5376}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5376}, {"title": "RoboEXP: Action-Conditioned Scene Graph via Interactive Exploration for\n  Robotic Manipulation", "link": "http://arxiv.org/abs/2402.15487v1", "similarity": 0.5297}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20BPP-Search%3A%20Enhancing%20Tree%20of%20Thought%20Reasoning%20for%20Mathematical%0A%20%20Modeling%20Problem%20Solving&body=Title%3A%20BPP-Search%3A%20Enhancing%20Tree%20of%20Thought%20Reasoning%20for%20Mathematical%0A%20%20Modeling%20Problem%20Solving%0AAuthor%3A%20Teng%20Wang%20and%20Wing-Yin%20Yu%20and%20Zhenqi%20He%20and%20Zehua%20Liu%20and%20Xiongwei%20Han%20and%20Hailei%20Gong%20and%20Han%20Wu%20and%20Wei%20Shi%20and%20Ruifeng%20She%20and%20Fangzhou%20Zhu%20and%20Tao%20Zhong%0AAbstract%3A%20%20%20LLMs%20exhibit%20advanced%20reasoning%20capabilities%2C%20offering%20the%20potential%20to%0Atransform%20natural%20language%20questions%20into%20mathematical%20models.%20However%2C%0Aexisting%20open-source%20operations%20research%20datasets%20lack%20detailed%20annotations%20of%0Athe%20modeling%20process%2C%20such%20as%20variable%20definitions%2C%20focusing%20solely%20on%0Aobjective%20values%2C%20which%20hinders%20reinforcement%20learning%20applications.%20To%20address%0Athis%2C%20we%20release%20the%20StructuredOR%20dataset%2C%20annotated%20with%20comprehensive%20labels%0Athat%20capture%20the%20complete%20mathematical%20modeling%20process.%20We%20further%20propose%0ABPP-Search%2C%20a%20algorithm%20that%20integrates%20reinforcement%20learning%20into%20a%0Atree-of-thought%20structure%20using%20Beam%20search%2C%20a%20Process%20reward%20model%2C%20and%20a%0Apairwise%20Preference%20algorithm.%20This%20approach%20enables%20efficient%20exploration%20of%0Atree%20structures%2C%20avoiding%20exhaustive%20search%20while%20improving%20accuracy.%20Extensive%0Aexperiments%20on%20StructuredOR%2C%20NL4OPT%2C%20and%20MAMO-ComplexLP%20datasets%20show%20that%0ABPP-Search%20significantly%20outperforms%20state-of-the-art%20methods%2C%20including%0AChain-of-Thought%2C%20Self-Consistency%2C%20and%20Tree-of-Thought.%20In%20tree-based%0Areasoning%2C%20BPP-Search%20also%20surpasses%20Process%20Reward%20Model%20combined%20with%20Greedy%0Aor%20Beam%20Search%2C%20demonstrating%20superior%20accuracy%20and%20efficiency%2C%20and%20enabling%0Afaster%20retrieval%20of%20correct%20solutions.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2411.17404v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DBPP-Search%253A%2520Enhancing%2520Tree%2520of%2520Thought%2520Reasoning%2520for%2520Mathematical%250A%2520%2520Modeling%2520Problem%2520Solving%26entry.906535625%3DTeng%2520Wang%2520and%2520Wing-Yin%2520Yu%2520and%2520Zhenqi%2520He%2520and%2520Zehua%2520Liu%2520and%2520Xiongwei%2520Han%2520and%2520Hailei%2520Gong%2520and%2520Han%2520Wu%2520and%2520Wei%2520Shi%2520and%2520Ruifeng%2520She%2520and%2520Fangzhou%2520Zhu%2520and%2520Tao%2520Zhong%26entry.1292438233%3D%2520%2520LLMs%2520exhibit%2520advanced%2520reasoning%2520capabilities%252C%2520offering%2520the%2520potential%2520to%250Atransform%2520natural%2520language%2520questions%2520into%2520mathematical%2520models.%2520However%252C%250Aexisting%2520open-source%2520operations%2520research%2520datasets%2520lack%2520detailed%2520annotations%2520of%250Athe%2520modeling%2520process%252C%2520such%2520as%2520variable%2520definitions%252C%2520focusing%2520solely%2520on%250Aobjective%2520values%252C%2520which%2520hinders%2520reinforcement%2520learning%2520applications.%2520To%2520address%250Athis%252C%2520we%2520release%2520the%2520StructuredOR%2520dataset%252C%2520annotated%2520with%2520comprehensive%2520labels%250Athat%2520capture%2520the%2520complete%2520mathematical%2520modeling%2520process.%2520We%2520further%2520propose%250ABPP-Search%252C%2520a%2520algorithm%2520that%2520integrates%2520reinforcement%2520learning%2520into%2520a%250Atree-of-thought%2520structure%2520using%2520Beam%2520search%252C%2520a%2520Process%2520reward%2520model%252C%2520and%2520a%250Apairwise%2520Preference%2520algorithm.%2520This%2520approach%2520enables%2520efficient%2520exploration%2520of%250Atree%2520structures%252C%2520avoiding%2520exhaustive%2520search%2520while%2520improving%2520accuracy.%2520Extensive%250Aexperiments%2520on%2520StructuredOR%252C%2520NL4OPT%252C%2520and%2520MAMO-ComplexLP%2520datasets%2520show%2520that%250ABPP-Search%2520significantly%2520outperforms%2520state-of-the-art%2520methods%252C%2520including%250AChain-of-Thought%252C%2520Self-Consistency%252C%2520and%2520Tree-of-Thought.%2520In%2520tree-based%250Areasoning%252C%2520BPP-Search%2520also%2520surpasses%2520Process%2520Reward%2520Model%2520combined%2520with%2520Greedy%250Aor%2520Beam%2520Search%252C%2520demonstrating%2520superior%2520accuracy%2520and%2520efficiency%252C%2520and%2520enabling%250Afaster%2520retrieval%2520of%2520correct%2520solutions.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2411.17404v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=BPP-Search%3A%20Enhancing%20Tree%20of%20Thought%20Reasoning%20for%20Mathematical%0A%20%20Modeling%20Problem%20Solving&entry.906535625=Teng%20Wang%20and%20Wing-Yin%20Yu%20and%20Zhenqi%20He%20and%20Zehua%20Liu%20and%20Xiongwei%20Han%20and%20Hailei%20Gong%20and%20Han%20Wu%20and%20Wei%20Shi%20and%20Ruifeng%20She%20and%20Fangzhou%20Zhu%20and%20Tao%20Zhong&entry.1292438233=%20%20LLMs%20exhibit%20advanced%20reasoning%20capabilities%2C%20offering%20the%20potential%20to%0Atransform%20natural%20language%20questions%20into%20mathematical%20models.%20However%2C%0Aexisting%20open-source%20operations%20research%20datasets%20lack%20detailed%20annotations%20of%0Athe%20modeling%20process%2C%20such%20as%20variable%20definitions%2C%20focusing%20solely%20on%0Aobjective%20values%2C%20which%20hinders%20reinforcement%20learning%20applications.%20To%20address%0Athis%2C%20we%20release%20the%20StructuredOR%20dataset%2C%20annotated%20with%20comprehensive%20labels%0Athat%20capture%20the%20complete%20mathematical%20modeling%20process.%20We%20further%20propose%0ABPP-Search%2C%20a%20algorithm%20that%20integrates%20reinforcement%20learning%20into%20a%0Atree-of-thought%20structure%20using%20Beam%20search%2C%20a%20Process%20reward%20model%2C%20and%20a%0Apairwise%20Preference%20algorithm.%20This%20approach%20enables%20efficient%20exploration%20of%0Atree%20structures%2C%20avoiding%20exhaustive%20search%20while%20improving%20accuracy.%20Extensive%0Aexperiments%20on%20StructuredOR%2C%20NL4OPT%2C%20and%20MAMO-ComplexLP%20datasets%20show%20that%0ABPP-Search%20significantly%20outperforms%20state-of-the-art%20methods%2C%20including%0AChain-of-Thought%2C%20Self-Consistency%2C%20and%20Tree-of-Thought.%20In%20tree-based%0Areasoning%2C%20BPP-Search%20also%20surpasses%20Process%20Reward%20Model%20combined%20with%20Greedy%0Aor%20Beam%20Search%2C%20demonstrating%20superior%20accuracy%20and%20efficiency%2C%20and%20enabling%0Afaster%20retrieval%20of%20correct%20solutions.%0A&entry.1838667208=http%3A//arxiv.org/abs/2411.17404v1&entry.124074799=Read"},
{"title": "Snake-Inspired Mobile Robot Positioning with Hybrid Learning", "author": "Aviad Etzion and Itzik Klein", "abstract": "  Mobile robots are used in various fields, from deliveries to search and\nrescue applications. Different types of sensors are mounted on the robot to\nprovide accurate navigation and, thus, allow successful completion of its task.\nIn real-world scenarios, due to environmental constraints, the robot frequently\nrelies only on its inertial sensors. Therefore, due to noises and other error\nterms associated with the inertial readings, the navigation solution drifts in\ntime. To mitigate the inertial solution drift, we propose the MoRPINet\nframework consisting of a neural network to regress the robot's travelled\ndistance. To this end, we require the mobile robot to maneuver in a snake-like\nslithering motion to encourage nonlinear behavior. MoRPINet was evaluated using\na dataset of 290 minutes of inertial recordings during field experiments and\nshowed an improvement of 33\\% in the positioning error over other\nstate-of-the-art methods for pure inertial navigation.\n", "link": "http://arxiv.org/abs/2411.17430v1", "date": "2024-11-26", "relevancy": 2.1448, "topK": [{"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.5767}, {"title": "Egocentric Computer Vision for Hands-Free Robotic Wheelchair Navigation", "link": "https://link.springer.com/article/10.1007/s10846-023-01807-4", "similarity": 0.5421}, {"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.5141}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Snake-Inspired%20Mobile%20Robot%20Positioning%20with%20Hybrid%20Learning&body=Title%3A%20Snake-Inspired%20Mobile%20Robot%20Positioning%20with%20Hybrid%20Learning%0AAuthor%3A%20Aviad%20Etzion%20and%20Itzik%20Klein%0AAbstract%3A%20%20%20Mobile%20robots%20are%20used%20in%20various%20fields%2C%20from%20deliveries%20to%20search%20and%0Arescue%20applications.%20Different%20types%20of%20sensors%20are%20mounted%20on%20the%20robot%20to%0Aprovide%20accurate%20navigation%20and%2C%20thus%2C%20allow%20successful%20completion%20of%20its%20task.%0AIn%20real-world%20scenarios%2C%20due%20to%20environmental%20constraints%2C%20the%20robot%20frequently%0Arelies%20only%20on%20its%20inertial%20sensors.%20Therefore%2C%20due%20to%20noises%20and%20other%20error%0Aterms%20associated%20with%20the%20inertial%20readings%2C%20the%20navigation%20solution%20drifts%20in%0Atime.%20To%20mitigate%20the%20inertial%20solution%20drift%2C%20we%20propose%20the%20MoRPINet%0Aframework%20consisting%20of%20a%20neural%20network%20to%20regress%20the%20robot%27s%20travelled%0Adistance.%20To%20this%20end%2C%20we%20require%20the%20mobile%20robot%20to%20maneuver%20in%20a%20snake-like%0Aslithering%20motion%20to%20encourage%20nonlinear%20behavior.%20MoRPINet%20was%20evaluated%20using%0Aa%20dataset%20of%20290%20minutes%20of%20inertial%20recordings%20during%20field%20experiments%20and%0Ashowed%20an%20improvement%20of%2033%5C%25%20in%20the%20positioning%20error%20over%20other%0Astate-of-the-art%20methods%20for%20pure%20inertial%20navigation.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2411.17430v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DSnake-Inspired%2520Mobile%2520Robot%2520Positioning%2520with%2520Hybrid%2520Learning%26entry.906535625%3DAviad%2520Etzion%2520and%2520Itzik%2520Klein%26entry.1292438233%3D%2520%2520Mobile%2520robots%2520are%2520used%2520in%2520various%2520fields%252C%2520from%2520deliveries%2520to%2520search%2520and%250Arescue%2520applications.%2520Different%2520types%2520of%2520sensors%2520are%2520mounted%2520on%2520the%2520robot%2520to%250Aprovide%2520accurate%2520navigation%2520and%252C%2520thus%252C%2520allow%2520successful%2520completion%2520of%2520its%2520task.%250AIn%2520real-world%2520scenarios%252C%2520due%2520to%2520environmental%2520constraints%252C%2520the%2520robot%2520frequently%250Arelies%2520only%2520on%2520its%2520inertial%2520sensors.%2520Therefore%252C%2520due%2520to%2520noises%2520and%2520other%2520error%250Aterms%2520associated%2520with%2520the%2520inertial%2520readings%252C%2520the%2520navigation%2520solution%2520drifts%2520in%250Atime.%2520To%2520mitigate%2520the%2520inertial%2520solution%2520drift%252C%2520we%2520propose%2520the%2520MoRPINet%250Aframework%2520consisting%2520of%2520a%2520neural%2520network%2520to%2520regress%2520the%2520robot%2527s%2520travelled%250Adistance.%2520To%2520this%2520end%252C%2520we%2520require%2520the%2520mobile%2520robot%2520to%2520maneuver%2520in%2520a%2520snake-like%250Aslithering%2520motion%2520to%2520encourage%2520nonlinear%2520behavior.%2520MoRPINet%2520was%2520evaluated%2520using%250Aa%2520dataset%2520of%2520290%2520minutes%2520of%2520inertial%2520recordings%2520during%2520field%2520experiments%2520and%250Ashowed%2520an%2520improvement%2520of%252033%255C%2525%2520in%2520the%2520positioning%2520error%2520over%2520other%250Astate-of-the-art%2520methods%2520for%2520pure%2520inertial%2520navigation.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2411.17430v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Snake-Inspired%20Mobile%20Robot%20Positioning%20with%20Hybrid%20Learning&entry.906535625=Aviad%20Etzion%20and%20Itzik%20Klein&entry.1292438233=%20%20Mobile%20robots%20are%20used%20in%20various%20fields%2C%20from%20deliveries%20to%20search%20and%0Arescue%20applications.%20Different%20types%20of%20sensors%20are%20mounted%20on%20the%20robot%20to%0Aprovide%20accurate%20navigation%20and%2C%20thus%2C%20allow%20successful%20completion%20of%20its%20task.%0AIn%20real-world%20scenarios%2C%20due%20to%20environmental%20constraints%2C%20the%20robot%20frequently%0Arelies%20only%20on%20its%20inertial%20sensors.%20Therefore%2C%20due%20to%20noises%20and%20other%20error%0Aterms%20associated%20with%20the%20inertial%20readings%2C%20the%20navigation%20solution%20drifts%20in%0Atime.%20To%20mitigate%20the%20inertial%20solution%20drift%2C%20we%20propose%20the%20MoRPINet%0Aframework%20consisting%20of%20a%20neural%20network%20to%20regress%20the%20robot%27s%20travelled%0Adistance.%20To%20this%20end%2C%20we%20require%20the%20mobile%20robot%20to%20maneuver%20in%20a%20snake-like%0Aslithering%20motion%20to%20encourage%20nonlinear%20behavior.%20MoRPINet%20was%20evaluated%20using%0Aa%20dataset%20of%20290%20minutes%20of%20inertial%20recordings%20during%20field%20experiments%20and%0Ashowed%20an%20improvement%20of%2033%5C%25%20in%20the%20positioning%20error%20over%20other%0Astate-of-the-art%20methods%20for%20pure%20inertial%20navigation.%0A&entry.1838667208=http%3A//arxiv.org/abs/2411.17430v1&entry.124074799=Read"},
{"title": "Evaluating Tokenizer Performance of Large Language Models Across\n  Official Indian Languages", "author": "S. Tamang and D. J. Bora", "abstract": "  Large Language Models (LLMs) based on transformer architectures have\nrevolutionized a variety of domains, with tokenization playing a pivotal role\nin their pre-processing and fine-tuning stages. In multilingual models,\nparticularly those tailored for Indic languages, effective tokenization is\ncrucial for optimizing performance. This paper presents a comprehensive\nevaluation of tokenizers used by 12 LLMs across all 22 official languages of\nIndia, with a focus on comparing the efficiency of their tokenization\nprocesses. We employed the Normalized Sequence Length (NSL) as a key metric in\nour analysis. Our findings reveal that the SUTRA tokenizer outperforms all\nother models, including several Indic-specific models, excelling in 14\nlanguages. Notable insights include the SUTRA tokenizer's superior handling of\nIndic languages, GPT-4o's advancement over its predecessor GPT-4 in processing\nIndian languages, and the limited performance of Project Indus in certain\nlanguages. This study underscores the critical importance of developing\ntargeted tokenization strategies for multilingual and Indic-centric models,\nlaying the groundwork for future improvements in tokenizer design to enhance\nlinguistic coverage and model efficiency.\n", "link": "http://arxiv.org/abs/2411.12240v2", "date": "2024-11-26", "relevancy": 2.1439, "topK": [{"title": "ToDo: Token Downsampling for Efficient Generation of High-Resolution\n  Images", "link": "http://arxiv.org/abs/2402.13573v2", "similarity": 0.4311}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.4277}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.4277}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Evaluating%20Tokenizer%20Performance%20of%20Large%20Language%20Models%20Across%0A%20%20Official%20Indian%20Languages&body=Title%3A%20Evaluating%20Tokenizer%20Performance%20of%20Large%20Language%20Models%20Across%0A%20%20Official%20Indian%20Languages%0AAuthor%3A%20S.%20Tamang%20and%20D.%20J.%20Bora%0AAbstract%3A%20%20%20Large%20Language%20Models%20%28LLMs%29%20based%20on%20transformer%20architectures%20have%0Arevolutionized%20a%20variety%20of%20domains%2C%20with%20tokenization%20playing%20a%20pivotal%20role%0Ain%20their%20pre-processing%20and%20fine-tuning%20stages.%20In%20multilingual%20models%2C%0Aparticularly%20those%20tailored%20for%20Indic%20languages%2C%20effective%20tokenization%20is%0Acrucial%20for%20optimizing%20performance.%20This%20paper%20presents%20a%20comprehensive%0Aevaluation%20of%20tokenizers%20used%20by%2012%20LLMs%20across%20all%2022%20official%20languages%20of%0AIndia%2C%20with%20a%20focus%20on%20comparing%20the%20efficiency%20of%20their%20tokenization%0Aprocesses.%20We%20employed%20the%20Normalized%20Sequence%20Length%20%28NSL%29%20as%20a%20key%20metric%20in%0Aour%20analysis.%20Our%20findings%20reveal%20that%20the%20SUTRA%20tokenizer%20outperforms%20all%0Aother%20models%2C%20including%20several%20Indic-specific%20models%2C%20excelling%20in%2014%0Alanguages.%20Notable%20insights%20include%20the%20SUTRA%20tokenizer%27s%20superior%20handling%20of%0AIndic%20languages%2C%20GPT-4o%27s%20advancement%20over%20its%20predecessor%20GPT-4%20in%20processing%0AIndian%20languages%2C%20and%20the%20limited%20performance%20of%20Project%20Indus%20in%20certain%0Alanguages.%20This%20study%20underscores%20the%20critical%20importance%20of%20developing%0Atargeted%20tokenization%20strategies%20for%20multilingual%20and%20Indic-centric%20models%2C%0Alaying%20the%20groundwork%20for%20future%20improvements%20in%20tokenizer%20design%20to%20enhance%0Alinguistic%20coverage%20and%20model%20efficiency.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2411.12240v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DEvaluating%2520Tokenizer%2520Performance%2520of%2520Large%2520Language%2520Models%2520Across%250A%2520%2520Official%2520Indian%2520Languages%26entry.906535625%3DS.%2520Tamang%2520and%2520D.%2520J.%2520Bora%26entry.1292438233%3D%2520%2520Large%2520Language%2520Models%2520%2528LLMs%2529%2520based%2520on%2520transformer%2520architectures%2520have%250Arevolutionized%2520a%2520variety%2520of%2520domains%252C%2520with%2520tokenization%2520playing%2520a%2520pivotal%2520role%250Ain%2520their%2520pre-processing%2520and%2520fine-tuning%2520stages.%2520In%2520multilingual%2520models%252C%250Aparticularly%2520those%2520tailored%2520for%2520Indic%2520languages%252C%2520effective%2520tokenization%2520is%250Acrucial%2520for%2520optimizing%2520performance.%2520This%2520paper%2520presents%2520a%2520comprehensive%250Aevaluation%2520of%2520tokenizers%2520used%2520by%252012%2520LLMs%2520across%2520all%252022%2520official%2520languages%2520of%250AIndia%252C%2520with%2520a%2520focus%2520on%2520comparing%2520the%2520efficiency%2520of%2520their%2520tokenization%250Aprocesses.%2520We%2520employed%2520the%2520Normalized%2520Sequence%2520Length%2520%2528NSL%2529%2520as%2520a%2520key%2520metric%2520in%250Aour%2520analysis.%2520Our%2520findings%2520reveal%2520that%2520the%2520SUTRA%2520tokenizer%2520outperforms%2520all%250Aother%2520models%252C%2520including%2520several%2520Indic-specific%2520models%252C%2520excelling%2520in%252014%250Alanguages.%2520Notable%2520insights%2520include%2520the%2520SUTRA%2520tokenizer%2527s%2520superior%2520handling%2520of%250AIndic%2520languages%252C%2520GPT-4o%2527s%2520advancement%2520over%2520its%2520predecessor%2520GPT-4%2520in%2520processing%250AIndian%2520languages%252C%2520and%2520the%2520limited%2520performance%2520of%2520Project%2520Indus%2520in%2520certain%250Alanguages.%2520This%2520study%2520underscores%2520the%2520critical%2520importance%2520of%2520developing%250Atargeted%2520tokenization%2520strategies%2520for%2520multilingual%2520and%2520Indic-centric%2520models%252C%250Alaying%2520the%2520groundwork%2520for%2520future%2520improvements%2520in%2520tokenizer%2520design%2520to%2520enhance%250Alinguistic%2520coverage%2520and%2520model%2520efficiency.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2411.12240v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Evaluating%20Tokenizer%20Performance%20of%20Large%20Language%20Models%20Across%0A%20%20Official%20Indian%20Languages&entry.906535625=S.%20Tamang%20and%20D.%20J.%20Bora&entry.1292438233=%20%20Large%20Language%20Models%20%28LLMs%29%20based%20on%20transformer%20architectures%20have%0Arevolutionized%20a%20variety%20of%20domains%2C%20with%20tokenization%20playing%20a%20pivotal%20role%0Ain%20their%20pre-processing%20and%20fine-tuning%20stages.%20In%20multilingual%20models%2C%0Aparticularly%20those%20tailored%20for%20Indic%20languages%2C%20effective%20tokenization%20is%0Acrucial%20for%20optimizing%20performance.%20This%20paper%20presents%20a%20comprehensive%0Aevaluation%20of%20tokenizers%20used%20by%2012%20LLMs%20across%20all%2022%20official%20languages%20of%0AIndia%2C%20with%20a%20focus%20on%20comparing%20the%20efficiency%20of%20their%20tokenization%0Aprocesses.%20We%20employed%20the%20Normalized%20Sequence%20Length%20%28NSL%29%20as%20a%20key%20metric%20in%0Aour%20analysis.%20Our%20findings%20reveal%20that%20the%20SUTRA%20tokenizer%20outperforms%20all%0Aother%20models%2C%20including%20several%20Indic-specific%20models%2C%20excelling%20in%2014%0Alanguages.%20Notable%20insights%20include%20the%20SUTRA%20tokenizer%27s%20superior%20handling%20of%0AIndic%20languages%2C%20GPT-4o%27s%20advancement%20over%20its%20predecessor%20GPT-4%20in%20processing%0AIndian%20languages%2C%20and%20the%20limited%20performance%20of%20Project%20Indus%20in%20certain%0Alanguages.%20This%20study%20underscores%20the%20critical%20importance%20of%20developing%0Atargeted%20tokenization%20strategies%20for%20multilingual%20and%20Indic-centric%20models%2C%0Alaying%20the%20groundwork%20for%20future%20improvements%20in%20tokenizer%20design%20to%20enhance%0Alinguistic%20coverage%20and%20model%20efficiency.%0A&entry.1838667208=http%3A//arxiv.org/abs/2411.12240v2&entry.124074799=Read"},
{"title": "MAROON: A Framework for the Joint Characterization of Near-Field\n  High-Resolution Radar and Optical Depth Imaging Techniques", "author": "Vanessa Wirth and Johanna Br\u00e4unig and Martin Vossiek and Tim Weyrich and Marc Stamminger", "abstract": "  Utilizing the complementary strengths of wavelength-specific range or depth\nsensors is crucial for robust computer-assisted tasks such as autonomous\ndriving. Despite this, there is still little research done at the intersection\nof optical depth sensors and radars operating close range, where the target is\ndecimeters away from the sensors. Together with a growing interest in\nhigh-resolution imaging radars operating in the near field, the question arises\nhow these sensors behave in comparison to their traditional optical\ncounterparts.\n  In this work, we take on the unique challenge of jointly characterizing depth\nimagers from both, the optical and radio-frequency domain using a multimodal\nspatial calibration. We collect data from four depth imagers, with three\noptical sensors of varying operation principle and an imaging radar. We provide\na comprehensive evaluation of their depth measurements with respect to distinct\nobject materials, geometries, and object-to-sensor distances. Specifically, we\nreveal scattering effects of partially transmissive materials and investigate\nthe response of radio-frequency signals. All object measurements will be made\npublic in form of a multimodal dataset, called MAROON.\n", "link": "http://arxiv.org/abs/2411.00527v2", "date": "2024-11-26", "relevancy": 2.142, "topK": [{"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.5434}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.5382}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5265}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20MAROON%3A%20A%20Framework%20for%20the%20Joint%20Characterization%20of%20Near-Field%0A%20%20High-Resolution%20Radar%20and%20Optical%20Depth%20Imaging%20Techniques&body=Title%3A%20MAROON%3A%20A%20Framework%20for%20the%20Joint%20Characterization%20of%20Near-Field%0A%20%20High-Resolution%20Radar%20and%20Optical%20Depth%20Imaging%20Techniques%0AAuthor%3A%20Vanessa%20Wirth%20and%20Johanna%20Br%C3%A4unig%20and%20Martin%20Vossiek%20and%20Tim%20Weyrich%20and%20Marc%20Stamminger%0AAbstract%3A%20%20%20Utilizing%20the%20complementary%20strengths%20of%20wavelength-specific%20range%20or%20depth%0Asensors%20is%20crucial%20for%20robust%20computer-assisted%20tasks%20such%20as%20autonomous%0Adriving.%20Despite%20this%2C%20there%20is%20still%20little%20research%20done%20at%20the%20intersection%0Aof%20optical%20depth%20sensors%20and%20radars%20operating%20close%20range%2C%20where%20the%20target%20is%0Adecimeters%20away%20from%20the%20sensors.%20Together%20with%20a%20growing%20interest%20in%0Ahigh-resolution%20imaging%20radars%20operating%20in%20the%20near%20field%2C%20the%20question%20arises%0Ahow%20these%20sensors%20behave%20in%20comparison%20to%20their%20traditional%20optical%0Acounterparts.%0A%20%20In%20this%20work%2C%20we%20take%20on%20the%20unique%20challenge%20of%20jointly%20characterizing%20depth%0Aimagers%20from%20both%2C%20the%20optical%20and%20radio-frequency%20domain%20using%20a%20multimodal%0Aspatial%20calibration.%20We%20collect%20data%20from%20four%20depth%20imagers%2C%20with%20three%0Aoptical%20sensors%20of%20varying%20operation%20principle%20and%20an%20imaging%20radar.%20We%20provide%0Aa%20comprehensive%20evaluation%20of%20their%20depth%20measurements%20with%20respect%20to%20distinct%0Aobject%20materials%2C%20geometries%2C%20and%20object-to-sensor%20distances.%20Specifically%2C%20we%0Areveal%20scattering%20effects%20of%20partially%20transmissive%20materials%20and%20investigate%0Athe%20response%20of%20radio-frequency%20signals.%20All%20object%20measurements%20will%20be%20made%0Apublic%20in%20form%20of%20a%20multimodal%20dataset%2C%20called%20MAROON.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2411.00527v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DMAROON%253A%2520A%2520Framework%2520for%2520the%2520Joint%2520Characterization%2520of%2520Near-Field%250A%2520%2520High-Resolution%2520Radar%2520and%2520Optical%2520Depth%2520Imaging%2520Techniques%26entry.906535625%3DVanessa%2520Wirth%2520and%2520Johanna%2520Br%25C3%25A4unig%2520and%2520Martin%2520Vossiek%2520and%2520Tim%2520Weyrich%2520and%2520Marc%2520Stamminger%26entry.1292438233%3D%2520%2520Utilizing%2520the%2520complementary%2520strengths%2520of%2520wavelength-specific%2520range%2520or%2520depth%250Asensors%2520is%2520crucial%2520for%2520robust%2520computer-assisted%2520tasks%2520such%2520as%2520autonomous%250Adriving.%2520Despite%2520this%252C%2520there%2520is%2520still%2520little%2520research%2520done%2520at%2520the%2520intersection%250Aof%2520optical%2520depth%2520sensors%2520and%2520radars%2520operating%2520close%2520range%252C%2520where%2520the%2520target%2520is%250Adecimeters%2520away%2520from%2520the%2520sensors.%2520Together%2520with%2520a%2520growing%2520interest%2520in%250Ahigh-resolution%2520imaging%2520radars%2520operating%2520in%2520the%2520near%2520field%252C%2520the%2520question%2520arises%250Ahow%2520these%2520sensors%2520behave%2520in%2520comparison%2520to%2520their%2520traditional%2520optical%250Acounterparts.%250A%2520%2520In%2520this%2520work%252C%2520we%2520take%2520on%2520the%2520unique%2520challenge%2520of%2520jointly%2520characterizing%2520depth%250Aimagers%2520from%2520both%252C%2520the%2520optical%2520and%2520radio-frequency%2520domain%2520using%2520a%2520multimodal%250Aspatial%2520calibration.%2520We%2520collect%2520data%2520from%2520four%2520depth%2520imagers%252C%2520with%2520three%250Aoptical%2520sensors%2520of%2520varying%2520operation%2520principle%2520and%2520an%2520imaging%2520radar.%2520We%2520provide%250Aa%2520comprehensive%2520evaluation%2520of%2520their%2520depth%2520measurements%2520with%2520respect%2520to%2520distinct%250Aobject%2520materials%252C%2520geometries%252C%2520and%2520object-to-sensor%2520distances.%2520Specifically%252C%2520we%250Areveal%2520scattering%2520effects%2520of%2520partially%2520transmissive%2520materials%2520and%2520investigate%250Athe%2520response%2520of%2520radio-frequency%2520signals.%2520All%2520object%2520measurements%2520will%2520be%2520made%250Apublic%2520in%2520form%2520of%2520a%2520multimodal%2520dataset%252C%2520called%2520MAROON.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2411.00527v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=MAROON%3A%20A%20Framework%20for%20the%20Joint%20Characterization%20of%20Near-Field%0A%20%20High-Resolution%20Radar%20and%20Optical%20Depth%20Imaging%20Techniques&entry.906535625=Vanessa%20Wirth%20and%20Johanna%20Br%C3%A4unig%20and%20Martin%20Vossiek%20and%20Tim%20Weyrich%20and%20Marc%20Stamminger&entry.1292438233=%20%20Utilizing%20the%20complementary%20strengths%20of%20wavelength-specific%20range%20or%20depth%0Asensors%20is%20crucial%20for%20robust%20computer-assisted%20tasks%20such%20as%20autonomous%0Adriving.%20Despite%20this%2C%20there%20is%20still%20little%20research%20done%20at%20the%20intersection%0Aof%20optical%20depth%20sensors%20and%20radars%20operating%20close%20range%2C%20where%20the%20target%20is%0Adecimeters%20away%20from%20the%20sensors.%20Together%20with%20a%20growing%20interest%20in%0Ahigh-resolution%20imaging%20radars%20operating%20in%20the%20near%20field%2C%20the%20question%20arises%0Ahow%20these%20sensors%20behave%20in%20comparison%20to%20their%20traditional%20optical%0Acounterparts.%0A%20%20In%20this%20work%2C%20we%20take%20on%20the%20unique%20challenge%20of%20jointly%20characterizing%20depth%0Aimagers%20from%20both%2C%20the%20optical%20and%20radio-frequency%20domain%20using%20a%20multimodal%0Aspatial%20calibration.%20We%20collect%20data%20from%20four%20depth%20imagers%2C%20with%20three%0Aoptical%20sensors%20of%20varying%20operation%20principle%20and%20an%20imaging%20radar.%20We%20provide%0Aa%20comprehensive%20evaluation%20of%20their%20depth%20measurements%20with%20respect%20to%20distinct%0Aobject%20materials%2C%20geometries%2C%20and%20object-to-sensor%20distances.%20Specifically%2C%20we%0Areveal%20scattering%20effects%20of%20partially%20transmissive%20materials%20and%20investigate%0Athe%20response%20of%20radio-frequency%20signals.%20All%20object%20measurements%20will%20be%20made%0Apublic%20in%20form%20of%20a%20multimodal%20dataset%2C%20called%20MAROON.%0A&entry.1838667208=http%3A//arxiv.org/abs/2411.00527v2&entry.124074799=Read"},
{"title": "Dual-task Mutual Reinforcing Embedded Joint Video Paragraph Retrieval\n  and Grounding", "author": "Mengzhao Wang and Huafeng Li and Yafei Zhang and Jinxing Li and Minghong Xie and Dapeng Tao", "abstract": "  Video Paragraph Grounding (VPG) aims to precisely locate the most appropriate\nmoments within a video that are relevant to a given textual paragraph query.\nHowever, existing methods typically rely on large-scale annotated temporal\nlabels and assume that the correspondence between videos and paragraphs is\nknown. This is impractical in real-world applications, as constructing temporal\nlabels requires significant labor costs, and the correspondence is often\nunknown. To address this issue, we propose a Dual-task Mutual Reinforcing\nEmbedded Joint Video Paragraph Retrieval and Grounding method (DMR-JRG). In\nthis method, retrieval and grounding tasks are mutually reinforced rather than\nbeing treated as separate issues. DMR-JRG mainly consists of two branches: a\nretrieval branch and a grounding branch. The retrieval branch uses inter-video\ncontrastive learning to roughly align the global features of paragraphs and\nvideos, reducing modality differences and constructing a coarse-grained feature\nspace to break free from the need for correspondence between paragraphs and\nvideos. Additionally, this coarse-grained feature space further facilitates the\ngrounding branch in extracting fine-grained contextual representations. In the\ngrounding branch, we achieve precise cross-modal matching and grounding by\nexploring the consistency between local, global, and temporal dimensions of\nvideo segments and textual paragraphs. By synergizing these dimensions, we\nconstruct a fine-grained feature space for video and textual features, greatly\nreducing the need for large-scale annotated temporal labels.\n", "link": "http://arxiv.org/abs/2411.17481v1", "date": "2024-11-26", "relevancy": 2.1186, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.53}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.53}, {"title": "Customize-A-Video: One-Shot Motion Customization of Text-to-Video\n  Diffusion Models", "link": "http://arxiv.org/abs/2402.14780v1", "similarity": 0.5278}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Dual-task%20Mutual%20Reinforcing%20Embedded%20Joint%20Video%20Paragraph%20Retrieval%0A%20%20and%20Grounding&body=Title%3A%20Dual-task%20Mutual%20Reinforcing%20Embedded%20Joint%20Video%20Paragraph%20Retrieval%0A%20%20and%20Grounding%0AAuthor%3A%20Mengzhao%20Wang%20and%20Huafeng%20Li%20and%20Yafei%20Zhang%20and%20Jinxing%20Li%20and%20Minghong%20Xie%20and%20Dapeng%20Tao%0AAbstract%3A%20%20%20Video%20Paragraph%20Grounding%20%28VPG%29%20aims%20to%20precisely%20locate%20the%20most%20appropriate%0Amoments%20within%20a%20video%20that%20are%20relevant%20to%20a%20given%20textual%20paragraph%20query.%0AHowever%2C%20existing%20methods%20typically%20rely%20on%20large-scale%20annotated%20temporal%0Alabels%20and%20assume%20that%20the%20correspondence%20between%20videos%20and%20paragraphs%20is%0Aknown.%20This%20is%20impractical%20in%20real-world%20applications%2C%20as%20constructing%20temporal%0Alabels%20requires%20significant%20labor%20costs%2C%20and%20the%20correspondence%20is%20often%0Aunknown.%20To%20address%20this%20issue%2C%20we%20propose%20a%20Dual-task%20Mutual%20Reinforcing%0AEmbedded%20Joint%20Video%20Paragraph%20Retrieval%20and%20Grounding%20method%20%28DMR-JRG%29.%20In%0Athis%20method%2C%20retrieval%20and%20grounding%20tasks%20are%20mutually%20reinforced%20rather%20than%0Abeing%20treated%20as%20separate%20issues.%20DMR-JRG%20mainly%20consists%20of%20two%20branches%3A%20a%0Aretrieval%20branch%20and%20a%20grounding%20branch.%20The%20retrieval%20branch%20uses%20inter-video%0Acontrastive%20learning%20to%20roughly%20align%20the%20global%20features%20of%20paragraphs%20and%0Avideos%2C%20reducing%20modality%20differences%20and%20constructing%20a%20coarse-grained%20feature%0Aspace%20to%20break%20free%20from%20the%20need%20for%20correspondence%20between%20paragraphs%20and%0Avideos.%20Additionally%2C%20this%20coarse-grained%20feature%20space%20further%20facilitates%20the%0Agrounding%20branch%20in%20extracting%20fine-grained%20contextual%20representations.%20In%20the%0Agrounding%20branch%2C%20we%20achieve%20precise%20cross-modal%20matching%20and%20grounding%20by%0Aexploring%20the%20consistency%20between%20local%2C%20global%2C%20and%20temporal%20dimensions%20of%0Avideo%20segments%20and%20textual%20paragraphs.%20By%20synergizing%20these%20dimensions%2C%20we%0Aconstruct%20a%20fine-grained%20feature%20space%20for%20video%20and%20textual%20features%2C%20greatly%0Areducing%20the%20need%20for%20large-scale%20annotated%20temporal%20labels.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2411.17481v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DDual-task%2520Mutual%2520Reinforcing%2520Embedded%2520Joint%2520Video%2520Paragraph%2520Retrieval%250A%2520%2520and%2520Grounding%26entry.906535625%3DMengzhao%2520Wang%2520and%2520Huafeng%2520Li%2520and%2520Yafei%2520Zhang%2520and%2520Jinxing%2520Li%2520and%2520Minghong%2520Xie%2520and%2520Dapeng%2520Tao%26entry.1292438233%3D%2520%2520Video%2520Paragraph%2520Grounding%2520%2528VPG%2529%2520aims%2520to%2520precisely%2520locate%2520the%2520most%2520appropriate%250Amoments%2520within%2520a%2520video%2520that%2520are%2520relevant%2520to%2520a%2520given%2520textual%2520paragraph%2520query.%250AHowever%252C%2520existing%2520methods%2520typically%2520rely%2520on%2520large-scale%2520annotated%2520temporal%250Alabels%2520and%2520assume%2520that%2520the%2520correspondence%2520between%2520videos%2520and%2520paragraphs%2520is%250Aknown.%2520This%2520is%2520impractical%2520in%2520real-world%2520applications%252C%2520as%2520constructing%2520temporal%250Alabels%2520requires%2520significant%2520labor%2520costs%252C%2520and%2520the%2520correspondence%2520is%2520often%250Aunknown.%2520To%2520address%2520this%2520issue%252C%2520we%2520propose%2520a%2520Dual-task%2520Mutual%2520Reinforcing%250AEmbedded%2520Joint%2520Video%2520Paragraph%2520Retrieval%2520and%2520Grounding%2520method%2520%2528DMR-JRG%2529.%2520In%250Athis%2520method%252C%2520retrieval%2520and%2520grounding%2520tasks%2520are%2520mutually%2520reinforced%2520rather%2520than%250Abeing%2520treated%2520as%2520separate%2520issues.%2520DMR-JRG%2520mainly%2520consists%2520of%2520two%2520branches%253A%2520a%250Aretrieval%2520branch%2520and%2520a%2520grounding%2520branch.%2520The%2520retrieval%2520branch%2520uses%2520inter-video%250Acontrastive%2520learning%2520to%2520roughly%2520align%2520the%2520global%2520features%2520of%2520paragraphs%2520and%250Avideos%252C%2520reducing%2520modality%2520differences%2520and%2520constructing%2520a%2520coarse-grained%2520feature%250Aspace%2520to%2520break%2520free%2520from%2520the%2520need%2520for%2520correspondence%2520between%2520paragraphs%2520and%250Avideos.%2520Additionally%252C%2520this%2520coarse-grained%2520feature%2520space%2520further%2520facilitates%2520the%250Agrounding%2520branch%2520in%2520extracting%2520fine-grained%2520contextual%2520representations.%2520In%2520the%250Agrounding%2520branch%252C%2520we%2520achieve%2520precise%2520cross-modal%2520matching%2520and%2520grounding%2520by%250Aexploring%2520the%2520consistency%2520between%2520local%252C%2520global%252C%2520and%2520temporal%2520dimensions%2520of%250Avideo%2520segments%2520and%2520textual%2520paragraphs.%2520By%2520synergizing%2520these%2520dimensions%252C%2520we%250Aconstruct%2520a%2520fine-grained%2520feature%2520space%2520for%2520video%2520and%2520textual%2520features%252C%2520greatly%250Areducing%2520the%2520need%2520for%2520large-scale%2520annotated%2520temporal%2520labels.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2411.17481v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Dual-task%20Mutual%20Reinforcing%20Embedded%20Joint%20Video%20Paragraph%20Retrieval%0A%20%20and%20Grounding&entry.906535625=Mengzhao%20Wang%20and%20Huafeng%20Li%20and%20Yafei%20Zhang%20and%20Jinxing%20Li%20and%20Minghong%20Xie%20and%20Dapeng%20Tao&entry.1292438233=%20%20Video%20Paragraph%20Grounding%20%28VPG%29%20aims%20to%20precisely%20locate%20the%20most%20appropriate%0Amoments%20within%20a%20video%20that%20are%20relevant%20to%20a%20given%20textual%20paragraph%20query.%0AHowever%2C%20existing%20methods%20typically%20rely%20on%20large-scale%20annotated%20temporal%0Alabels%20and%20assume%20that%20the%20correspondence%20between%20videos%20and%20paragraphs%20is%0Aknown.%20This%20is%20impractical%20in%20real-world%20applications%2C%20as%20constructing%20temporal%0Alabels%20requires%20significant%20labor%20costs%2C%20and%20the%20correspondence%20is%20often%0Aunknown.%20To%20address%20this%20issue%2C%20we%20propose%20a%20Dual-task%20Mutual%20Reinforcing%0AEmbedded%20Joint%20Video%20Paragraph%20Retrieval%20and%20Grounding%20method%20%28DMR-JRG%29.%20In%0Athis%20method%2C%20retrieval%20and%20grounding%20tasks%20are%20mutually%20reinforced%20rather%20than%0Abeing%20treated%20as%20separate%20issues.%20DMR-JRG%20mainly%20consists%20of%20two%20branches%3A%20a%0Aretrieval%20branch%20and%20a%20grounding%20branch.%20The%20retrieval%20branch%20uses%20inter-video%0Acontrastive%20learning%20to%20roughly%20align%20the%20global%20features%20of%20paragraphs%20and%0Avideos%2C%20reducing%20modality%20differences%20and%20constructing%20a%20coarse-grained%20feature%0Aspace%20to%20break%20free%20from%20the%20need%20for%20correspondence%20between%20paragraphs%20and%0Avideos.%20Additionally%2C%20this%20coarse-grained%20feature%20space%20further%20facilitates%20the%0Agrounding%20branch%20in%20extracting%20fine-grained%20contextual%20representations.%20In%20the%0Agrounding%20branch%2C%20we%20achieve%20precise%20cross-modal%20matching%20and%20grounding%20by%0Aexploring%20the%20consistency%20between%20local%2C%20global%2C%20and%20temporal%20dimensions%20of%0Avideo%20segments%20and%20textual%20paragraphs.%20By%20synergizing%20these%20dimensions%2C%20we%0Aconstruct%20a%20fine-grained%20feature%20space%20for%20video%20and%20textual%20features%2C%20greatly%0Areducing%20the%20need%20for%20large-scale%20annotated%20temporal%20labels.%0A&entry.1838667208=http%3A//arxiv.org/abs/2411.17481v1&entry.124074799=Read"},
{"title": "Maximally Separated Active Learning", "author": "Tejaswi Kasarla and Abhishek Jha and Faye Tervoort and Rita Cucchiara and Pascal Mettes", "abstract": "  Active Learning aims to optimize performance while minimizing annotation\ncosts by selecting the most informative samples from an unlabelled pool.\nTraditional uncertainty sampling often leads to sampling bias by choosing\nsimilar uncertain samples. We propose an active learning method that utilizes\nfixed equiangular hyperspherical points as class prototypes, ensuring\nconsistent inter-class separation and robust feature representations. Our\napproach introduces Maximally Separated Active Learning (MSAL) for uncertainty\nsampling and a combined strategy (MSAL-D) for incorporating diversity. This\nmethod eliminates the need for costly clustering steps, while maintaining\ndiversity through hyperspherical uniformity. We demonstrate strong performance\nover existing active learning techniques across five benchmark datasets,\nhighlighting the method's effectiveness and integration ease. The code is\navailable on GitHub.\n", "link": "http://arxiv.org/abs/2411.17444v1", "date": "2024-11-26", "relevancy": 2.1174, "topK": [{"title": "Breadcrumbs: Adversarial class-balanced sampling for long-tailed recognition", "link": "https://link.springer.com/chapter/10.1007/978-3-031-20053-3_37", "similarity": 0.5554}, {"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.5231}, {"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.5057}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Maximally%20Separated%20Active%20Learning&body=Title%3A%20Maximally%20Separated%20Active%20Learning%0AAuthor%3A%20Tejaswi%20Kasarla%20and%20Abhishek%20Jha%20and%20Faye%20Tervoort%20and%20Rita%20Cucchiara%20and%20Pascal%20Mettes%0AAbstract%3A%20%20%20Active%20Learning%20aims%20to%20optimize%20performance%20while%20minimizing%20annotation%0Acosts%20by%20selecting%20the%20most%20informative%20samples%20from%20an%20unlabelled%20pool.%0ATraditional%20uncertainty%20sampling%20often%20leads%20to%20sampling%20bias%20by%20choosing%0Asimilar%20uncertain%20samples.%20We%20propose%20an%20active%20learning%20method%20that%20utilizes%0Afixed%20equiangular%20hyperspherical%20points%20as%20class%20prototypes%2C%20ensuring%0Aconsistent%20inter-class%20separation%20and%20robust%20feature%20representations.%20Our%0Aapproach%20introduces%20Maximally%20Separated%20Active%20Learning%20%28MSAL%29%20for%20uncertainty%0Asampling%20and%20a%20combined%20strategy%20%28MSAL-D%29%20for%20incorporating%20diversity.%20This%0Amethod%20eliminates%20the%20need%20for%20costly%20clustering%20steps%2C%20while%20maintaining%0Adiversity%20through%20hyperspherical%20uniformity.%20We%20demonstrate%20strong%20performance%0Aover%20existing%20active%20learning%20techniques%20across%20five%20benchmark%20datasets%2C%0Ahighlighting%20the%20method%27s%20effectiveness%20and%20integration%20ease.%20The%20code%20is%0Aavailable%20on%20GitHub.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2411.17444v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DMaximally%2520Separated%2520Active%2520Learning%26entry.906535625%3DTejaswi%2520Kasarla%2520and%2520Abhishek%2520Jha%2520and%2520Faye%2520Tervoort%2520and%2520Rita%2520Cucchiara%2520and%2520Pascal%2520Mettes%26entry.1292438233%3D%2520%2520Active%2520Learning%2520aims%2520to%2520optimize%2520performance%2520while%2520minimizing%2520annotation%250Acosts%2520by%2520selecting%2520the%2520most%2520informative%2520samples%2520from%2520an%2520unlabelled%2520pool.%250ATraditional%2520uncertainty%2520sampling%2520often%2520leads%2520to%2520sampling%2520bias%2520by%2520choosing%250Asimilar%2520uncertain%2520samples.%2520We%2520propose%2520an%2520active%2520learning%2520method%2520that%2520utilizes%250Afixed%2520equiangular%2520hyperspherical%2520points%2520as%2520class%2520prototypes%252C%2520ensuring%250Aconsistent%2520inter-class%2520separation%2520and%2520robust%2520feature%2520representations.%2520Our%250Aapproach%2520introduces%2520Maximally%2520Separated%2520Active%2520Learning%2520%2528MSAL%2529%2520for%2520uncertainty%250Asampling%2520and%2520a%2520combined%2520strategy%2520%2528MSAL-D%2529%2520for%2520incorporating%2520diversity.%2520This%250Amethod%2520eliminates%2520the%2520need%2520for%2520costly%2520clustering%2520steps%252C%2520while%2520maintaining%250Adiversity%2520through%2520hyperspherical%2520uniformity.%2520We%2520demonstrate%2520strong%2520performance%250Aover%2520existing%2520active%2520learning%2520techniques%2520across%2520five%2520benchmark%2520datasets%252C%250Ahighlighting%2520the%2520method%2527s%2520effectiveness%2520and%2520integration%2520ease.%2520The%2520code%2520is%250Aavailable%2520on%2520GitHub.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2411.17444v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Maximally%20Separated%20Active%20Learning&entry.906535625=Tejaswi%20Kasarla%20and%20Abhishek%20Jha%20and%20Faye%20Tervoort%20and%20Rita%20Cucchiara%20and%20Pascal%20Mettes&entry.1292438233=%20%20Active%20Learning%20aims%20to%20optimize%20performance%20while%20minimizing%20annotation%0Acosts%20by%20selecting%20the%20most%20informative%20samples%20from%20an%20unlabelled%20pool.%0ATraditional%20uncertainty%20sampling%20often%20leads%20to%20sampling%20bias%20by%20choosing%0Asimilar%20uncertain%20samples.%20We%20propose%20an%20active%20learning%20method%20that%20utilizes%0Afixed%20equiangular%20hyperspherical%20points%20as%20class%20prototypes%2C%20ensuring%0Aconsistent%20inter-class%20separation%20and%20robust%20feature%20representations.%20Our%0Aapproach%20introduces%20Maximally%20Separated%20Active%20Learning%20%28MSAL%29%20for%20uncertainty%0Asampling%20and%20a%20combined%20strategy%20%28MSAL-D%29%20for%20incorporating%20diversity.%20This%0Amethod%20eliminates%20the%20need%20for%20costly%20clustering%20steps%2C%20while%20maintaining%0Adiversity%20through%20hyperspherical%20uniformity.%20We%20demonstrate%20strong%20performance%0Aover%20existing%20active%20learning%20techniques%20across%20five%20benchmark%20datasets%2C%0Ahighlighting%20the%20method%27s%20effectiveness%20and%20integration%20ease.%20The%20code%20is%0Aavailable%20on%20GitHub.%0A&entry.1838667208=http%3A//arxiv.org/abs/2411.17444v1&entry.124074799=Read"},
{"title": "RSL-SQL: Robust Schema Linking in Text-to-SQL Generation", "author": "Zhenbiao Cao and Yuanlei Zheng and Zhihao Fan and Xiaojin Zhang and Wei Chen and Xiang Bai", "abstract": "  Text-to-SQL generation aims to translate natural language questions into SQL\nstatements. In Text-to-SQL based on large language models, schema linking is a\nwidely adopted strategy to streamline the input for LLMs by selecting only\nrelevant schema elements, therefore reducing noise and computational overhead.\nHowever, schema linking faces risks that require caution, including the\npotential omission of necessary elements and disruption of database structural\nintegrity. To address these challenges, we propose a novel framework called\nRSL-SQL that combines bidirectional schema linking, contextual information\naugmentation, binary selection strategy, and multi-turn self-correction. We\nimprove the recall of pattern linking using forward and backward pruning\nmethods, achieving a strict recall of 94% while reducing the number of input\ncolumns by 83%. Furthermore, it hedges the risk by voting between a full mode\nand a simplified mode enhanced with contextual information. Experiments on the\nBIRD and Spider benchmarks demonstrate that our approach achieves SOTA\nexecution accuracy among open-source solutions, with 67.2% on BIRD and 87.9% on\nSpider using GPT-4o. Furthermore, our approach outperforms a series of GPT-4\nbased Text-to-SQL systems when adopting DeepSeek (much cheaper) with same\nintact prompts. Extensive analysis and ablation studies confirm the\neffectiveness of each component in our framework. The codes are available at\nhttps://github.com/Laqcce-cao/RSL-SQL.\n", "link": "http://arxiv.org/abs/2411.00073v2", "date": "2024-11-26", "relevancy": 1.9744, "topK": [{"title": "DressCode: Autoregressively Sewing and Generating Garments from Text\n  Guidance", "link": "http://arxiv.org/abs/2401.16465v3", "similarity": 0.5043}, {"title": "RoboEXP: Action-Conditioned Scene Graph via Interactive Exploration for\n  Robotic Manipulation", "link": "http://arxiv.org/abs/2402.15487v1", "similarity": 0.4883}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.485}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20RSL-SQL%3A%20Robust%20Schema%20Linking%20in%20Text-to-SQL%20Generation&body=Title%3A%20RSL-SQL%3A%20Robust%20Schema%20Linking%20in%20Text-to-SQL%20Generation%0AAuthor%3A%20Zhenbiao%20Cao%20and%20Yuanlei%20Zheng%20and%20Zhihao%20Fan%20and%20Xiaojin%20Zhang%20and%20Wei%20Chen%20and%20Xiang%20Bai%0AAbstract%3A%20%20%20Text-to-SQL%20generation%20aims%20to%20translate%20natural%20language%20questions%20into%20SQL%0Astatements.%20In%20Text-to-SQL%20based%20on%20large%20language%20models%2C%20schema%20linking%20is%20a%0Awidely%20adopted%20strategy%20to%20streamline%20the%20input%20for%20LLMs%20by%20selecting%20only%0Arelevant%20schema%20elements%2C%20therefore%20reducing%20noise%20and%20computational%20overhead.%0AHowever%2C%20schema%20linking%20faces%20risks%20that%20require%20caution%2C%20including%20the%0Apotential%20omission%20of%20necessary%20elements%20and%20disruption%20of%20database%20structural%0Aintegrity.%20To%20address%20these%20challenges%2C%20we%20propose%20a%20novel%20framework%20called%0ARSL-SQL%20that%20combines%20bidirectional%20schema%20linking%2C%20contextual%20information%0Aaugmentation%2C%20binary%20selection%20strategy%2C%20and%20multi-turn%20self-correction.%20We%0Aimprove%20the%20recall%20of%20pattern%20linking%20using%20forward%20and%20backward%20pruning%0Amethods%2C%20achieving%20a%20strict%20recall%20of%2094%25%20while%20reducing%20the%20number%20of%20input%0Acolumns%20by%2083%25.%20Furthermore%2C%20it%20hedges%20the%20risk%20by%20voting%20between%20a%20full%20mode%0Aand%20a%20simplified%20mode%20enhanced%20with%20contextual%20information.%20Experiments%20on%20the%0ABIRD%20and%20Spider%20benchmarks%20demonstrate%20that%20our%20approach%20achieves%20SOTA%0Aexecution%20accuracy%20among%20open-source%20solutions%2C%20with%2067.2%25%20on%20BIRD%20and%2087.9%25%20on%0ASpider%20using%20GPT-4o.%20Furthermore%2C%20our%20approach%20outperforms%20a%20series%20of%20GPT-4%0Abased%20Text-to-SQL%20systems%20when%20adopting%20DeepSeek%20%28much%20cheaper%29%20with%20same%0Aintact%20prompts.%20Extensive%20analysis%20and%20ablation%20studies%20confirm%20the%0Aeffectiveness%20of%20each%20component%20in%20our%20framework.%20The%20codes%20are%20available%20at%0Ahttps%3A//github.com/Laqcce-cao/RSL-SQL.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2411.00073v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DRSL-SQL%253A%2520Robust%2520Schema%2520Linking%2520in%2520Text-to-SQL%2520Generation%26entry.906535625%3DZhenbiao%2520Cao%2520and%2520Yuanlei%2520Zheng%2520and%2520Zhihao%2520Fan%2520and%2520Xiaojin%2520Zhang%2520and%2520Wei%2520Chen%2520and%2520Xiang%2520Bai%26entry.1292438233%3D%2520%2520Text-to-SQL%2520generation%2520aims%2520to%2520translate%2520natural%2520language%2520questions%2520into%2520SQL%250Astatements.%2520In%2520Text-to-SQL%2520based%2520on%2520large%2520language%2520models%252C%2520schema%2520linking%2520is%2520a%250Awidely%2520adopted%2520strategy%2520to%2520streamline%2520the%2520input%2520for%2520LLMs%2520by%2520selecting%2520only%250Arelevant%2520schema%2520elements%252C%2520therefore%2520reducing%2520noise%2520and%2520computational%2520overhead.%250AHowever%252C%2520schema%2520linking%2520faces%2520risks%2520that%2520require%2520caution%252C%2520including%2520the%250Apotential%2520omission%2520of%2520necessary%2520elements%2520and%2520disruption%2520of%2520database%2520structural%250Aintegrity.%2520To%2520address%2520these%2520challenges%252C%2520we%2520propose%2520a%2520novel%2520framework%2520called%250ARSL-SQL%2520that%2520combines%2520bidirectional%2520schema%2520linking%252C%2520contextual%2520information%250Aaugmentation%252C%2520binary%2520selection%2520strategy%252C%2520and%2520multi-turn%2520self-correction.%2520We%250Aimprove%2520the%2520recall%2520of%2520pattern%2520linking%2520using%2520forward%2520and%2520backward%2520pruning%250Amethods%252C%2520achieving%2520a%2520strict%2520recall%2520of%252094%2525%2520while%2520reducing%2520the%2520number%2520of%2520input%250Acolumns%2520by%252083%2525.%2520Furthermore%252C%2520it%2520hedges%2520the%2520risk%2520by%2520voting%2520between%2520a%2520full%2520mode%250Aand%2520a%2520simplified%2520mode%2520enhanced%2520with%2520contextual%2520information.%2520Experiments%2520on%2520the%250ABIRD%2520and%2520Spider%2520benchmarks%2520demonstrate%2520that%2520our%2520approach%2520achieves%2520SOTA%250Aexecution%2520accuracy%2520among%2520open-source%2520solutions%252C%2520with%252067.2%2525%2520on%2520BIRD%2520and%252087.9%2525%2520on%250ASpider%2520using%2520GPT-4o.%2520Furthermore%252C%2520our%2520approach%2520outperforms%2520a%2520series%2520of%2520GPT-4%250Abased%2520Text-to-SQL%2520systems%2520when%2520adopting%2520DeepSeek%2520%2528much%2520cheaper%2529%2520with%2520same%250Aintact%2520prompts.%2520Extensive%2520analysis%2520and%2520ablation%2520studies%2520confirm%2520the%250Aeffectiveness%2520of%2520each%2520component%2520in%2520our%2520framework.%2520The%2520codes%2520are%2520available%2520at%250Ahttps%253A//github.com/Laqcce-cao/RSL-SQL.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2411.00073v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=RSL-SQL%3A%20Robust%20Schema%20Linking%20in%20Text-to-SQL%20Generation&entry.906535625=Zhenbiao%20Cao%20and%20Yuanlei%20Zheng%20and%20Zhihao%20Fan%20and%20Xiaojin%20Zhang%20and%20Wei%20Chen%20and%20Xiang%20Bai&entry.1292438233=%20%20Text-to-SQL%20generation%20aims%20to%20translate%20natural%20language%20questions%20into%20SQL%0Astatements.%20In%20Text-to-SQL%20based%20on%20large%20language%20models%2C%20schema%20linking%20is%20a%0Awidely%20adopted%20strategy%20to%20streamline%20the%20input%20for%20LLMs%20by%20selecting%20only%0Arelevant%20schema%20elements%2C%20therefore%20reducing%20noise%20and%20computational%20overhead.%0AHowever%2C%20schema%20linking%20faces%20risks%20that%20require%20caution%2C%20including%20the%0Apotential%20omission%20of%20necessary%20elements%20and%20disruption%20of%20database%20structural%0Aintegrity.%20To%20address%20these%20challenges%2C%20we%20propose%20a%20novel%20framework%20called%0ARSL-SQL%20that%20combines%20bidirectional%20schema%20linking%2C%20contextual%20information%0Aaugmentation%2C%20binary%20selection%20strategy%2C%20and%20multi-turn%20self-correction.%20We%0Aimprove%20the%20recall%20of%20pattern%20linking%20using%20forward%20and%20backward%20pruning%0Amethods%2C%20achieving%20a%20strict%20recall%20of%2094%25%20while%20reducing%20the%20number%20of%20input%0Acolumns%20by%2083%25.%20Furthermore%2C%20it%20hedges%20the%20risk%20by%20voting%20between%20a%20full%20mode%0Aand%20a%20simplified%20mode%20enhanced%20with%20contextual%20information.%20Experiments%20on%20the%0ABIRD%20and%20Spider%20benchmarks%20demonstrate%20that%20our%20approach%20achieves%20SOTA%0Aexecution%20accuracy%20among%20open-source%20solutions%2C%20with%2067.2%25%20on%20BIRD%20and%2087.9%25%20on%0ASpider%20using%20GPT-4o.%20Furthermore%2C%20our%20approach%20outperforms%20a%20series%20of%20GPT-4%0Abased%20Text-to-SQL%20systems%20when%20adopting%20DeepSeek%20%28much%20cheaper%29%20with%20same%0Aintact%20prompts.%20Extensive%20analysis%20and%20ablation%20studies%20confirm%20the%0Aeffectiveness%20of%20each%20component%20in%20our%20framework.%20The%20codes%20are%20available%20at%0Ahttps%3A//github.com/Laqcce-cao/RSL-SQL.%0A&entry.1838667208=http%3A//arxiv.org/abs/2411.00073v2&entry.124074799=Read"},
{"title": "Can LLMs be Good Graph Judger for Knowledge Graph Construction?", "author": "Haoyu Huang and Chong Chen and Conghui He and Yang Li and Jiawei Jiang and Wentao Zhang", "abstract": "  In real-world scenarios, most of the data obtained from information retrieval\n(IR) system is unstructured. Converting natural language sentences into\nstructured Knowledge Graphs (KGs) remains a critical challenge. The quality of\nconstructed KGs may also impact the performance of some KG-dependent domains\nlike GraphRAG systems and recommendation systems. Recently, Large Language\nModels (LLMs) have demonstrated impressive capabilities in addressing a wide\nrange of natural language processing tasks. However, there are still challenges\nwhen utilizing LLMs to address the task of generating structured KGs. And we\nhave identified three limitations with respect to existing KG construction\nmethods. (1)There is a large amount of information and excessive noise in\nreal-world documents, which could result in extracting messy information.\n(2)Native LLMs struggle to effectively extract accuracy knowledge from some\ndomain-specific documents. (3)Hallucinations phenomenon cannot be overlooked\nwhen utilizing LLMs directly as an unsupervised method for constructing KGs.\n  In this paper, we propose GraphJudger, a knowledge graph construction\nframework to address the aforementioned challenges. We introduce three\ninnovative modules in our method, which are entity-centric iterative text\ndenoising, knowledge aware instruction tuning and graph judgement,\nrespectively. We seek to utilize the capacity of LLMs to function as a graph\njudger, a capability superior to their role only as a predictor for KG\nconstruction problems. Experiments conducted on two general text-graph pair\ndatasets and one domain-specific text-graph pair dataset show superior\nperformances compared to baseline methods. The code of our proposed method is\navailable at https://github.com/hhy-huang/GraphJudger.\n", "link": "http://arxiv.org/abs/2411.17388v1", "date": "2024-11-26", "relevancy": 1.9329, "topK": [{"title": "Self-supervised Photographic Image Layout Representation Learning", "link": "http://arxiv.org/abs/2403.03740v1", "similarity": 0.4955}, {"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.4925}, {"title": "UGG: Unified Generative Grasping", "link": "https://arxiv.org/abs/2311.16917", "similarity": 0.4673}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Can%20LLMs%20be%20Good%20Graph%20Judger%20for%20Knowledge%20Graph%20Construction%3F&body=Title%3A%20Can%20LLMs%20be%20Good%20Graph%20Judger%20for%20Knowledge%20Graph%20Construction%3F%0AAuthor%3A%20Haoyu%20Huang%20and%20Chong%20Chen%20and%20Conghui%20He%20and%20Yang%20Li%20and%20Jiawei%20Jiang%20and%20Wentao%20Zhang%0AAbstract%3A%20%20%20In%20real-world%20scenarios%2C%20most%20of%20the%20data%20obtained%20from%20information%20retrieval%0A%28IR%29%20system%20is%20unstructured.%20Converting%20natural%20language%20sentences%20into%0Astructured%20Knowledge%20Graphs%20%28KGs%29%20remains%20a%20critical%20challenge.%20The%20quality%20of%0Aconstructed%20KGs%20may%20also%20impact%20the%20performance%20of%20some%20KG-dependent%20domains%0Alike%20GraphRAG%20systems%20and%20recommendation%20systems.%20Recently%2C%20Large%20Language%0AModels%20%28LLMs%29%20have%20demonstrated%20impressive%20capabilities%20in%20addressing%20a%20wide%0Arange%20of%20natural%20language%20processing%20tasks.%20However%2C%20there%20are%20still%20challenges%0Awhen%20utilizing%20LLMs%20to%20address%20the%20task%20of%20generating%20structured%20KGs.%20And%20we%0Ahave%20identified%20three%20limitations%20with%20respect%20to%20existing%20KG%20construction%0Amethods.%20%281%29There%20is%20a%20large%20amount%20of%20information%20and%20excessive%20noise%20in%0Areal-world%20documents%2C%20which%20could%20result%20in%20extracting%20messy%20information.%0A%282%29Native%20LLMs%20struggle%20to%20effectively%20extract%20accuracy%20knowledge%20from%20some%0Adomain-specific%20documents.%20%283%29Hallucinations%20phenomenon%20cannot%20be%20overlooked%0Awhen%20utilizing%20LLMs%20directly%20as%20an%20unsupervised%20method%20for%20constructing%20KGs.%0A%20%20In%20this%20paper%2C%20we%20propose%20GraphJudger%2C%20a%20knowledge%20graph%20construction%0Aframework%20to%20address%20the%20aforementioned%20challenges.%20We%20introduce%20three%0Ainnovative%20modules%20in%20our%20method%2C%20which%20are%20entity-centric%20iterative%20text%0Adenoising%2C%20knowledge%20aware%20instruction%20tuning%20and%20graph%20judgement%2C%0Arespectively.%20We%20seek%20to%20utilize%20the%20capacity%20of%20LLMs%20to%20function%20as%20a%20graph%0Ajudger%2C%20a%20capability%20superior%20to%20their%20role%20only%20as%20a%20predictor%20for%20KG%0Aconstruction%20problems.%20Experiments%20conducted%20on%20two%20general%20text-graph%20pair%0Adatasets%20and%20one%20domain-specific%20text-graph%20pair%20dataset%20show%20superior%0Aperformances%20compared%20to%20baseline%20methods.%20The%20code%20of%20our%20proposed%20method%20is%0Aavailable%20at%20https%3A//github.com/hhy-huang/GraphJudger.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2411.17388v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DCan%2520LLMs%2520be%2520Good%2520Graph%2520Judger%2520for%2520Knowledge%2520Graph%2520Construction%253F%26entry.906535625%3DHaoyu%2520Huang%2520and%2520Chong%2520Chen%2520and%2520Conghui%2520He%2520and%2520Yang%2520Li%2520and%2520Jiawei%2520Jiang%2520and%2520Wentao%2520Zhang%26entry.1292438233%3D%2520%2520In%2520real-world%2520scenarios%252C%2520most%2520of%2520the%2520data%2520obtained%2520from%2520information%2520retrieval%250A%2528IR%2529%2520system%2520is%2520unstructured.%2520Converting%2520natural%2520language%2520sentences%2520into%250Astructured%2520Knowledge%2520Graphs%2520%2528KGs%2529%2520remains%2520a%2520critical%2520challenge.%2520The%2520quality%2520of%250Aconstructed%2520KGs%2520may%2520also%2520impact%2520the%2520performance%2520of%2520some%2520KG-dependent%2520domains%250Alike%2520GraphRAG%2520systems%2520and%2520recommendation%2520systems.%2520Recently%252C%2520Large%2520Language%250AModels%2520%2528LLMs%2529%2520have%2520demonstrated%2520impressive%2520capabilities%2520in%2520addressing%2520a%2520wide%250Arange%2520of%2520natural%2520language%2520processing%2520tasks.%2520However%252C%2520there%2520are%2520still%2520challenges%250Awhen%2520utilizing%2520LLMs%2520to%2520address%2520the%2520task%2520of%2520generating%2520structured%2520KGs.%2520And%2520we%250Ahave%2520identified%2520three%2520limitations%2520with%2520respect%2520to%2520existing%2520KG%2520construction%250Amethods.%2520%25281%2529There%2520is%2520a%2520large%2520amount%2520of%2520information%2520and%2520excessive%2520noise%2520in%250Areal-world%2520documents%252C%2520which%2520could%2520result%2520in%2520extracting%2520messy%2520information.%250A%25282%2529Native%2520LLMs%2520struggle%2520to%2520effectively%2520extract%2520accuracy%2520knowledge%2520from%2520some%250Adomain-specific%2520documents.%2520%25283%2529Hallucinations%2520phenomenon%2520cannot%2520be%2520overlooked%250Awhen%2520utilizing%2520LLMs%2520directly%2520as%2520an%2520unsupervised%2520method%2520for%2520constructing%2520KGs.%250A%2520%2520In%2520this%2520paper%252C%2520we%2520propose%2520GraphJudger%252C%2520a%2520knowledge%2520graph%2520construction%250Aframework%2520to%2520address%2520the%2520aforementioned%2520challenges.%2520We%2520introduce%2520three%250Ainnovative%2520modules%2520in%2520our%2520method%252C%2520which%2520are%2520entity-centric%2520iterative%2520text%250Adenoising%252C%2520knowledge%2520aware%2520instruction%2520tuning%2520and%2520graph%2520judgement%252C%250Arespectively.%2520We%2520seek%2520to%2520utilize%2520the%2520capacity%2520of%2520LLMs%2520to%2520function%2520as%2520a%2520graph%250Ajudger%252C%2520a%2520capability%2520superior%2520to%2520their%2520role%2520only%2520as%2520a%2520predictor%2520for%2520KG%250Aconstruction%2520problems.%2520Experiments%2520conducted%2520on%2520two%2520general%2520text-graph%2520pair%250Adatasets%2520and%2520one%2520domain-specific%2520text-graph%2520pair%2520dataset%2520show%2520superior%250Aperformances%2520compared%2520to%2520baseline%2520methods.%2520The%2520code%2520of%2520our%2520proposed%2520method%2520is%250Aavailable%2520at%2520https%253A//github.com/hhy-huang/GraphJudger.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2411.17388v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Can%20LLMs%20be%20Good%20Graph%20Judger%20for%20Knowledge%20Graph%20Construction%3F&entry.906535625=Haoyu%20Huang%20and%20Chong%20Chen%20and%20Conghui%20He%20and%20Yang%20Li%20and%20Jiawei%20Jiang%20and%20Wentao%20Zhang&entry.1292438233=%20%20In%20real-world%20scenarios%2C%20most%20of%20the%20data%20obtained%20from%20information%20retrieval%0A%28IR%29%20system%20is%20unstructured.%20Converting%20natural%20language%20sentences%20into%0Astructured%20Knowledge%20Graphs%20%28KGs%29%20remains%20a%20critical%20challenge.%20The%20quality%20of%0Aconstructed%20KGs%20may%20also%20impact%20the%20performance%20of%20some%20KG-dependent%20domains%0Alike%20GraphRAG%20systems%20and%20recommendation%20systems.%20Recently%2C%20Large%20Language%0AModels%20%28LLMs%29%20have%20demonstrated%20impressive%20capabilities%20in%20addressing%20a%20wide%0Arange%20of%20natural%20language%20processing%20tasks.%20However%2C%20there%20are%20still%20challenges%0Awhen%20utilizing%20LLMs%20to%20address%20the%20task%20of%20generating%20structured%20KGs.%20And%20we%0Ahave%20identified%20three%20limitations%20with%20respect%20to%20existing%20KG%20construction%0Amethods.%20%281%29There%20is%20a%20large%20amount%20of%20information%20and%20excessive%20noise%20in%0Areal-world%20documents%2C%20which%20could%20result%20in%20extracting%20messy%20information.%0A%282%29Native%20LLMs%20struggle%20to%20effectively%20extract%20accuracy%20knowledge%20from%20some%0Adomain-specific%20documents.%20%283%29Hallucinations%20phenomenon%20cannot%20be%20overlooked%0Awhen%20utilizing%20LLMs%20directly%20as%20an%20unsupervised%20method%20for%20constructing%20KGs.%0A%20%20In%20this%20paper%2C%20we%20propose%20GraphJudger%2C%20a%20knowledge%20graph%20construction%0Aframework%20to%20address%20the%20aforementioned%20challenges.%20We%20introduce%20three%0Ainnovative%20modules%20in%20our%20method%2C%20which%20are%20entity-centric%20iterative%20text%0Adenoising%2C%20knowledge%20aware%20instruction%20tuning%20and%20graph%20judgement%2C%0Arespectively.%20We%20seek%20to%20utilize%20the%20capacity%20of%20LLMs%20to%20function%20as%20a%20graph%0Ajudger%2C%20a%20capability%20superior%20to%20their%20role%20only%20as%20a%20predictor%20for%20KG%0Aconstruction%20problems.%20Experiments%20conducted%20on%20two%20general%20text-graph%20pair%0Adatasets%20and%20one%20domain-specific%20text-graph%20pair%20dataset%20show%20superior%0Aperformances%20compared%20to%20baseline%20methods.%20The%20code%20of%20our%20proposed%20method%20is%0Aavailable%20at%20https%3A//github.com/hhy-huang/GraphJudger.%0A&entry.1838667208=http%3A//arxiv.org/abs/2411.17388v1&entry.124074799=Read"},
{"title": "Using Large Language Models for Expert Prior Elicitation in Predictive\n  Modelling", "author": "Alexander Capstick and Rahul G. Krishnan and Payam Barnaghi", "abstract": "  Large language models (LLMs), trained on diverse data effectively acquire a\nbreadth of information across various domains. However, their computational\ncomplexity, cost, and lack of transparency hinder their direct application for\nspecialised tasks. In fields such as clinical research, acquiring expert\nannotations or prior knowledge about predictive models is often costly and\ntime-consuming. This study proposes using LLMs to elicit expert prior\ndistributions for predictive models. This approach also provides an alternative\nto in-context learning, where language models are tasked with making\npredictions directly. We compare LLM-elicited and uninformative priors,\nevaluate whether LLMs truthfully generate parameter distributions, and propose\na model selection strategy for in-context learning and prior elicitation. Our\nfindings show that LLM-elicited prior parameter distributions significantly\nreduce predictive error compared to uninformative priors in low-data settings.\nApplied to clinical problems, this translates to fewer required biological\nsamples, lowering cost and resources. Prior elicitation also consistently\noutperforms and proves more reliable than in-context learning at a lower cost,\nmaking it a preferred alternative in our setting. We demonstrate the utility of\nthis method across various use cases, including clinical applications. For\ninfection prediction, using LLM-elicited priors reduced the number of required\nlabels to achieve the same accuracy as an uninformative prior by 55%, at 200\ndays earlier in the study.\n", "link": "http://arxiv.org/abs/2411.17284v1", "date": "2024-11-26", "relevancy": 1.9786, "topK": [{"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.5179}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.49}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.49}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Using%20Large%20Language%20Models%20for%20Expert%20Prior%20Elicitation%20in%20Predictive%0A%20%20Modelling&body=Title%3A%20Using%20Large%20Language%20Models%20for%20Expert%20Prior%20Elicitation%20in%20Predictive%0A%20%20Modelling%0AAuthor%3A%20Alexander%20Capstick%20and%20Rahul%20G.%20Krishnan%20and%20Payam%20Barnaghi%0AAbstract%3A%20%20%20Large%20language%20models%20%28LLMs%29%2C%20trained%20on%20diverse%20data%20effectively%20acquire%20a%0Abreadth%20of%20information%20across%20various%20domains.%20However%2C%20their%20computational%0Acomplexity%2C%20cost%2C%20and%20lack%20of%20transparency%20hinder%20their%20direct%20application%20for%0Aspecialised%20tasks.%20In%20fields%20such%20as%20clinical%20research%2C%20acquiring%20expert%0Aannotations%20or%20prior%20knowledge%20about%20predictive%20models%20is%20often%20costly%20and%0Atime-consuming.%20This%20study%20proposes%20using%20LLMs%20to%20elicit%20expert%20prior%0Adistributions%20for%20predictive%20models.%20This%20approach%20also%20provides%20an%20alternative%0Ato%20in-context%20learning%2C%20where%20language%20models%20are%20tasked%20with%20making%0Apredictions%20directly.%20We%20compare%20LLM-elicited%20and%20uninformative%20priors%2C%0Aevaluate%20whether%20LLMs%20truthfully%20generate%20parameter%20distributions%2C%20and%20propose%0Aa%20model%20selection%20strategy%20for%20in-context%20learning%20and%20prior%20elicitation.%20Our%0Afindings%20show%20that%20LLM-elicited%20prior%20parameter%20distributions%20significantly%0Areduce%20predictive%20error%20compared%20to%20uninformative%20priors%20in%20low-data%20settings.%0AApplied%20to%20clinical%20problems%2C%20this%20translates%20to%20fewer%20required%20biological%0Asamples%2C%20lowering%20cost%20and%20resources.%20Prior%20elicitation%20also%20consistently%0Aoutperforms%20and%20proves%20more%20reliable%20than%20in-context%20learning%20at%20a%20lower%20cost%2C%0Amaking%20it%20a%20preferred%20alternative%20in%20our%20setting.%20We%20demonstrate%20the%20utility%20of%0Athis%20method%20across%20various%20use%20cases%2C%20including%20clinical%20applications.%20For%0Ainfection%20prediction%2C%20using%20LLM-elicited%20priors%20reduced%20the%20number%20of%20required%0Alabels%20to%20achieve%20the%20same%20accuracy%20as%20an%20uninformative%20prior%20by%2055%25%2C%20at%20200%0Adays%20earlier%20in%20the%20study.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2411.17284v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DUsing%2520Large%2520Language%2520Models%2520for%2520Expert%2520Prior%2520Elicitation%2520in%2520Predictive%250A%2520%2520Modelling%26entry.906535625%3DAlexander%2520Capstick%2520and%2520Rahul%2520G.%2520Krishnan%2520and%2520Payam%2520Barnaghi%26entry.1292438233%3D%2520%2520Large%2520language%2520models%2520%2528LLMs%2529%252C%2520trained%2520on%2520diverse%2520data%2520effectively%2520acquire%2520a%250Abreadth%2520of%2520information%2520across%2520various%2520domains.%2520However%252C%2520their%2520computational%250Acomplexity%252C%2520cost%252C%2520and%2520lack%2520of%2520transparency%2520hinder%2520their%2520direct%2520application%2520for%250Aspecialised%2520tasks.%2520In%2520fields%2520such%2520as%2520clinical%2520research%252C%2520acquiring%2520expert%250Aannotations%2520or%2520prior%2520knowledge%2520about%2520predictive%2520models%2520is%2520often%2520costly%2520and%250Atime-consuming.%2520This%2520study%2520proposes%2520using%2520LLMs%2520to%2520elicit%2520expert%2520prior%250Adistributions%2520for%2520predictive%2520models.%2520This%2520approach%2520also%2520provides%2520an%2520alternative%250Ato%2520in-context%2520learning%252C%2520where%2520language%2520models%2520are%2520tasked%2520with%2520making%250Apredictions%2520directly.%2520We%2520compare%2520LLM-elicited%2520and%2520uninformative%2520priors%252C%250Aevaluate%2520whether%2520LLMs%2520truthfully%2520generate%2520parameter%2520distributions%252C%2520and%2520propose%250Aa%2520model%2520selection%2520strategy%2520for%2520in-context%2520learning%2520and%2520prior%2520elicitation.%2520Our%250Afindings%2520show%2520that%2520LLM-elicited%2520prior%2520parameter%2520distributions%2520significantly%250Areduce%2520predictive%2520error%2520compared%2520to%2520uninformative%2520priors%2520in%2520low-data%2520settings.%250AApplied%2520to%2520clinical%2520problems%252C%2520this%2520translates%2520to%2520fewer%2520required%2520biological%250Asamples%252C%2520lowering%2520cost%2520and%2520resources.%2520Prior%2520elicitation%2520also%2520consistently%250Aoutperforms%2520and%2520proves%2520more%2520reliable%2520than%2520in-context%2520learning%2520at%2520a%2520lower%2520cost%252C%250Amaking%2520it%2520a%2520preferred%2520alternative%2520in%2520our%2520setting.%2520We%2520demonstrate%2520the%2520utility%2520of%250Athis%2520method%2520across%2520various%2520use%2520cases%252C%2520including%2520clinical%2520applications.%2520For%250Ainfection%2520prediction%252C%2520using%2520LLM-elicited%2520priors%2520reduced%2520the%2520number%2520of%2520required%250Alabels%2520to%2520achieve%2520the%2520same%2520accuracy%2520as%2520an%2520uninformative%2520prior%2520by%252055%2525%252C%2520at%2520200%250Adays%2520earlier%2520in%2520the%2520study.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2411.17284v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Using%20Large%20Language%20Models%20for%20Expert%20Prior%20Elicitation%20in%20Predictive%0A%20%20Modelling&entry.906535625=Alexander%20Capstick%20and%20Rahul%20G.%20Krishnan%20and%20Payam%20Barnaghi&entry.1292438233=%20%20Large%20language%20models%20%28LLMs%29%2C%20trained%20on%20diverse%20data%20effectively%20acquire%20a%0Abreadth%20of%20information%20across%20various%20domains.%20However%2C%20their%20computational%0Acomplexity%2C%20cost%2C%20and%20lack%20of%20transparency%20hinder%20their%20direct%20application%20for%0Aspecialised%20tasks.%20In%20fields%20such%20as%20clinical%20research%2C%20acquiring%20expert%0Aannotations%20or%20prior%20knowledge%20about%20predictive%20models%20is%20often%20costly%20and%0Atime-consuming.%20This%20study%20proposes%20using%20LLMs%20to%20elicit%20expert%20prior%0Adistributions%20for%20predictive%20models.%20This%20approach%20also%20provides%20an%20alternative%0Ato%20in-context%20learning%2C%20where%20language%20models%20are%20tasked%20with%20making%0Apredictions%20directly.%20We%20compare%20LLM-elicited%20and%20uninformative%20priors%2C%0Aevaluate%20whether%20LLMs%20truthfully%20generate%20parameter%20distributions%2C%20and%20propose%0Aa%20model%20selection%20strategy%20for%20in-context%20learning%20and%20prior%20elicitation.%20Our%0Afindings%20show%20that%20LLM-elicited%20prior%20parameter%20distributions%20significantly%0Areduce%20predictive%20error%20compared%20to%20uninformative%20priors%20in%20low-data%20settings.%0AApplied%20to%20clinical%20problems%2C%20this%20translates%20to%20fewer%20required%20biological%0Asamples%2C%20lowering%20cost%20and%20resources.%20Prior%20elicitation%20also%20consistently%0Aoutperforms%20and%20proves%20more%20reliable%20than%20in-context%20learning%20at%20a%20lower%20cost%2C%0Amaking%20it%20a%20preferred%20alternative%20in%20our%20setting.%20We%20demonstrate%20the%20utility%20of%0Athis%20method%20across%20various%20use%20cases%2C%20including%20clinical%20applications.%20For%0Ainfection%20prediction%2C%20using%20LLM-elicited%20priors%20reduced%20the%20number%20of%20required%0Alabels%20to%20achieve%20the%20same%20accuracy%20as%20an%20uninformative%20prior%20by%2055%25%2C%20at%20200%0Adays%20earlier%20in%20the%20study.%0A&entry.1838667208=http%3A//arxiv.org/abs/2411.17284v1&entry.124074799=Read"},
{"title": "SIL-RRT*: Learning Sampling Distribution through Self Imitation Learning", "author": "Xuzhe Dang and Stefan Edelkamp", "abstract": "  Efficiently finding safe and feasible trajectories for mobile objects is a\ncritical field in robotics and computer science. In this paper, we propose\nSIL-RRT*, a novel learning-based motion planning algorithm that extends the\nRRT* algorithm by using a deep neural network to predict a distribution for\nsampling at each iteration. We evaluate SIL-RRT* on various 2D and 3D\nenvironments and establish that it can efficiently solve high-dimensional\nmotion planning problems with fewer samples than traditional sampling-based\nalgorithms. Moreover, SIL-RRT* is able to scale to more complex environments,\nmaking it a promising approach for solving challenging robotic motion planning\nproblems.\n", "link": "http://arxiv.org/abs/2411.17293v1", "date": "2024-11-26", "relevancy": 1.5397, "topK": [{"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.5198}, {"title": "RTG-SLAM: Real-time 3D Reconstruction at Scale using Gaussian Splatting", "link": "http://arxiv.org/abs/2404.19706v1", "similarity": 0.5134}, {"title": "RoboEXP: Action-Conditioned Scene Graph via Interactive Exploration for\n  Robotic Manipulation", "link": "http://arxiv.org/abs/2402.15487v1", "similarity": 0.5063}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20SIL-RRT%2A%3A%20Learning%20Sampling%20Distribution%20through%20Self%20Imitation%20Learning&body=Title%3A%20SIL-RRT%2A%3A%20Learning%20Sampling%20Distribution%20through%20Self%20Imitation%20Learning%0AAuthor%3A%20Xuzhe%20Dang%20and%20Stefan%20Edelkamp%0AAbstract%3A%20%20%20Efficiently%20finding%20safe%20and%20feasible%20trajectories%20for%20mobile%20objects%20is%20a%0Acritical%20field%20in%20robotics%20and%20computer%20science.%20In%20this%20paper%2C%20we%20propose%0ASIL-RRT%2A%2C%20a%20novel%20learning-based%20motion%20planning%20algorithm%20that%20extends%20the%0ARRT%2A%20algorithm%20by%20using%20a%20deep%20neural%20network%20to%20predict%20a%20distribution%20for%0Asampling%20at%20each%20iteration.%20We%20evaluate%20SIL-RRT%2A%20on%20various%202D%20and%203D%0Aenvironments%20and%20establish%20that%20it%20can%20efficiently%20solve%20high-dimensional%0Amotion%20planning%20problems%20with%20fewer%20samples%20than%20traditional%20sampling-based%0Aalgorithms.%20Moreover%2C%20SIL-RRT%2A%20is%20able%20to%20scale%20to%20more%20complex%20environments%2C%0Amaking%20it%20a%20promising%20approach%20for%20solving%20challenging%20robotic%20motion%20planning%0Aproblems.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2411.17293v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DSIL-RRT%252A%253A%2520Learning%2520Sampling%2520Distribution%2520through%2520Self%2520Imitation%2520Learning%26entry.906535625%3DXuzhe%2520Dang%2520and%2520Stefan%2520Edelkamp%26entry.1292438233%3D%2520%2520Efficiently%2520finding%2520safe%2520and%2520feasible%2520trajectories%2520for%2520mobile%2520objects%2520is%2520a%250Acritical%2520field%2520in%2520robotics%2520and%2520computer%2520science.%2520In%2520this%2520paper%252C%2520we%2520propose%250ASIL-RRT%252A%252C%2520a%2520novel%2520learning-based%2520motion%2520planning%2520algorithm%2520that%2520extends%2520the%250ARRT%252A%2520algorithm%2520by%2520using%2520a%2520deep%2520neural%2520network%2520to%2520predict%2520a%2520distribution%2520for%250Asampling%2520at%2520each%2520iteration.%2520We%2520evaluate%2520SIL-RRT%252A%2520on%2520various%25202D%2520and%25203D%250Aenvironments%2520and%2520establish%2520that%2520it%2520can%2520efficiently%2520solve%2520high-dimensional%250Amotion%2520planning%2520problems%2520with%2520fewer%2520samples%2520than%2520traditional%2520sampling-based%250Aalgorithms.%2520Moreover%252C%2520SIL-RRT%252A%2520is%2520able%2520to%2520scale%2520to%2520more%2520complex%2520environments%252C%250Amaking%2520it%2520a%2520promising%2520approach%2520for%2520solving%2520challenging%2520robotic%2520motion%2520planning%250Aproblems.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2411.17293v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=SIL-RRT%2A%3A%20Learning%20Sampling%20Distribution%20through%20Self%20Imitation%20Learning&entry.906535625=Xuzhe%20Dang%20and%20Stefan%20Edelkamp&entry.1292438233=%20%20Efficiently%20finding%20safe%20and%20feasible%20trajectories%20for%20mobile%20objects%20is%20a%0Acritical%20field%20in%20robotics%20and%20computer%20science.%20In%20this%20paper%2C%20we%20propose%0ASIL-RRT%2A%2C%20a%20novel%20learning-based%20motion%20planning%20algorithm%20that%20extends%20the%0ARRT%2A%20algorithm%20by%20using%20a%20deep%20neural%20network%20to%20predict%20a%20distribution%20for%0Asampling%20at%20each%20iteration.%20We%20evaluate%20SIL-RRT%2A%20on%20various%202D%20and%203D%0Aenvironments%20and%20establish%20that%20it%20can%20efficiently%20solve%20high-dimensional%0Amotion%20planning%20problems%20with%20fewer%20samples%20than%20traditional%20sampling-based%0Aalgorithms.%20Moreover%2C%20SIL-RRT%2A%20is%20able%20to%20scale%20to%20more%20complex%20environments%2C%0Amaking%20it%20a%20promising%20approach%20for%20solving%20challenging%20robotic%20motion%20planning%0Aproblems.%0A&entry.1838667208=http%3A//arxiv.org/abs/2411.17293v1&entry.124074799=Read"},
{"title": "Improving the Convergence Rates of Forward Gradient Descent with\n  Repeated Sampling", "author": "Niklas Dexheimer and Johannes Schmidt-Hieber", "abstract": "  Forward gradient descent (FGD) has been proposed as a biologically more\nplausible alternative of gradient descent as it can be computed without\nbackward pass. Considering the linear model with $d$ parameters, previous work\nhas found that the prediction error of FGD is, however, by a factor $d$ slower\nthan the prediction error of stochastic gradient descent (SGD). In this paper\nwe show that by computing $\\ell$ FGD steps based on each training sample, this\nsuboptimality factor becomes $d/(\\ell \\wedge d)$ and thus the suboptimality of\nthe rate disappears if $\\ell \\gtrsim d.$ We also show that FGD with repeated\nsampling can adapt to low-dimensional structure in the input distribution. The\nmain mathematical challenge lies in controlling the dependencies arising from\nthe repeated sampling process.\n", "link": "http://arxiv.org/abs/2411.17567v1", "date": "2024-11-26", "relevancy": 2.0677, "topK": [{"title": "ToDo: Token Downsampling for Efficient Generation of High-Resolution\n  Images", "link": "http://arxiv.org/abs/2402.13573v2", "similarity": 0.5465}, {"title": "SDXS: Real-Time One-Step Latent Diffusion Models with Image Conditions", "link": "http://arxiv.org/abs/2403.16627v1", "similarity": 0.5107}, {"title": "RTG-SLAM: Real-time 3D Reconstruction at Scale using Gaussian Splatting", "link": "http://arxiv.org/abs/2404.19706v1", "similarity": 0.4899}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Improving%20the%20Convergence%20Rates%20of%20Forward%20Gradient%20Descent%20with%0A%20%20Repeated%20Sampling&body=Title%3A%20Improving%20the%20Convergence%20Rates%20of%20Forward%20Gradient%20Descent%20with%0A%20%20Repeated%20Sampling%0AAuthor%3A%20Niklas%20Dexheimer%20and%20Johannes%20Schmidt-Hieber%0AAbstract%3A%20%20%20Forward%20gradient%20descent%20%28FGD%29%20has%20been%20proposed%20as%20a%20biologically%20more%0Aplausible%20alternative%20of%20gradient%20descent%20as%20it%20can%20be%20computed%20without%0Abackward%20pass.%20Considering%20the%20linear%20model%20with%20%24d%24%20parameters%2C%20previous%20work%0Ahas%20found%20that%20the%20prediction%20error%20of%20FGD%20is%2C%20however%2C%20by%20a%20factor%20%24d%24%20slower%0Athan%20the%20prediction%20error%20of%20stochastic%20gradient%20descent%20%28SGD%29.%20In%20this%20paper%0Awe%20show%20that%20by%20computing%20%24%5Cell%24%20FGD%20steps%20based%20on%20each%20training%20sample%2C%20this%0Asuboptimality%20factor%20becomes%20%24d/%28%5Cell%20%5Cwedge%20d%29%24%20and%20thus%20the%20suboptimality%20of%0Athe%20rate%20disappears%20if%20%24%5Cell%20%5Cgtrsim%20d.%24%20We%20also%20show%20that%20FGD%20with%20repeated%0Asampling%20can%20adapt%20to%20low-dimensional%20structure%20in%20the%20input%20distribution.%20The%0Amain%20mathematical%20challenge%20lies%20in%20controlling%20the%20dependencies%20arising%20from%0Athe%20repeated%20sampling%20process.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2411.17567v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DImproving%2520the%2520Convergence%2520Rates%2520of%2520Forward%2520Gradient%2520Descent%2520with%250A%2520%2520Repeated%2520Sampling%26entry.906535625%3DNiklas%2520Dexheimer%2520and%2520Johannes%2520Schmidt-Hieber%26entry.1292438233%3D%2520%2520Forward%2520gradient%2520descent%2520%2528FGD%2529%2520has%2520been%2520proposed%2520as%2520a%2520biologically%2520more%250Aplausible%2520alternative%2520of%2520gradient%2520descent%2520as%2520it%2520can%2520be%2520computed%2520without%250Abackward%2520pass.%2520Considering%2520the%2520linear%2520model%2520with%2520%2524d%2524%2520parameters%252C%2520previous%2520work%250Ahas%2520found%2520that%2520the%2520prediction%2520error%2520of%2520FGD%2520is%252C%2520however%252C%2520by%2520a%2520factor%2520%2524d%2524%2520slower%250Athan%2520the%2520prediction%2520error%2520of%2520stochastic%2520gradient%2520descent%2520%2528SGD%2529.%2520In%2520this%2520paper%250Awe%2520show%2520that%2520by%2520computing%2520%2524%255Cell%2524%2520FGD%2520steps%2520based%2520on%2520each%2520training%2520sample%252C%2520this%250Asuboptimality%2520factor%2520becomes%2520%2524d/%2528%255Cell%2520%255Cwedge%2520d%2529%2524%2520and%2520thus%2520the%2520suboptimality%2520of%250Athe%2520rate%2520disappears%2520if%2520%2524%255Cell%2520%255Cgtrsim%2520d.%2524%2520We%2520also%2520show%2520that%2520FGD%2520with%2520repeated%250Asampling%2520can%2520adapt%2520to%2520low-dimensional%2520structure%2520in%2520the%2520input%2520distribution.%2520The%250Amain%2520mathematical%2520challenge%2520lies%2520in%2520controlling%2520the%2520dependencies%2520arising%2520from%250Athe%2520repeated%2520sampling%2520process.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2411.17567v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Improving%20the%20Convergence%20Rates%20of%20Forward%20Gradient%20Descent%20with%0A%20%20Repeated%20Sampling&entry.906535625=Niklas%20Dexheimer%20and%20Johannes%20Schmidt-Hieber&entry.1292438233=%20%20Forward%20gradient%20descent%20%28FGD%29%20has%20been%20proposed%20as%20a%20biologically%20more%0Aplausible%20alternative%20of%20gradient%20descent%20as%20it%20can%20be%20computed%20without%0Abackward%20pass.%20Considering%20the%20linear%20model%20with%20%24d%24%20parameters%2C%20previous%20work%0Ahas%20found%20that%20the%20prediction%20error%20of%20FGD%20is%2C%20however%2C%20by%20a%20factor%20%24d%24%20slower%0Athan%20the%20prediction%20error%20of%20stochastic%20gradient%20descent%20%28SGD%29.%20In%20this%20paper%0Awe%20show%20that%20by%20computing%20%24%5Cell%24%20FGD%20steps%20based%20on%20each%20training%20sample%2C%20this%0Asuboptimality%20factor%20becomes%20%24d/%28%5Cell%20%5Cwedge%20d%29%24%20and%20thus%20the%20suboptimality%20of%0Athe%20rate%20disappears%20if%20%24%5Cell%20%5Cgtrsim%20d.%24%20We%20also%20show%20that%20FGD%20with%20repeated%0Asampling%20can%20adapt%20to%20low-dimensional%20structure%20in%20the%20input%20distribution.%20The%0Amain%20mathematical%20challenge%20lies%20in%20controlling%20the%20dependencies%20arising%20from%0Athe%20repeated%20sampling%20process.%0A&entry.1838667208=http%3A//arxiv.org/abs/2411.17567v1&entry.124074799=Read"},
{"title": "MotionLLaMA: A Unified Framework for Motion Synthesis and Comprehension", "author": "Zeyu Ling and Bo Han and Shiyang Li and Hongdeng Shen and Jikang Cheng and Changqing Zou", "abstract": "  This paper introduces MotionLLaMA, a unified framework for motion synthesis\nand comprehension, along with a novel full-body motion tokenizer called the\nHoMi Tokenizer. MotionLLaMA is developed based on three core principles. First,\nit establishes a powerful unified representation space through the HoMi\nTokenizer. Using a single codebook, the HoMi Tokenizer in MotionLLaMA achieves\nreconstruction accuracy comparable to residual vector quantization tokenizers\nutilizing six codebooks, outperforming all existing single-codebook tokenizers.\nSecond, MotionLLaMA integrates a large language model to tackle various\nmotion-related tasks. This integration bridges various modalities, facilitating\nboth comprehensive and intricate motion synthesis and comprehension. Third,\nMotionLLaMA introduces the MotionHub dataset, currently the most extensive\nmultimodal, multitask motion dataset, which enables fine-tuning of large\nlanguage models. Extensive experimental results demonstrate that MotionLLaMA\nnot only covers the widest range of motion-related tasks but also achieves\nstate-of-the-art (SOTA) performance in motion completion, interaction\ndual-person text-to-motion, and all comprehension tasks while reaching\nperformance comparable to SOTA in the remaining tasks. The code and MotionHub\ndataset are publicly available.\n", "link": "http://arxiv.org/abs/2411.17335v1", "date": "2024-11-26", "relevancy": 1.5855, "topK": [{"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5392}, {"title": "Customize-A-Video: One-Shot Motion Customization of Text-to-Video\n  Diffusion Models", "link": "http://arxiv.org/abs/2402.14780v1", "similarity": 0.5191}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.511}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20MotionLLaMA%3A%20A%20Unified%20Framework%20for%20Motion%20Synthesis%20and%20Comprehension&body=Title%3A%20MotionLLaMA%3A%20A%20Unified%20Framework%20for%20Motion%20Synthesis%20and%20Comprehension%0AAuthor%3A%20Zeyu%20Ling%20and%20Bo%20Han%20and%20Shiyang%20Li%20and%20Hongdeng%20Shen%20and%20Jikang%20Cheng%20and%20Changqing%20Zou%0AAbstract%3A%20%20%20This%20paper%20introduces%20MotionLLaMA%2C%20a%20unified%20framework%20for%20motion%20synthesis%0Aand%20comprehension%2C%20along%20with%20a%20novel%20full-body%20motion%20tokenizer%20called%20the%0AHoMi%20Tokenizer.%20MotionLLaMA%20is%20developed%20based%20on%20three%20core%20principles.%20First%2C%0Ait%20establishes%20a%20powerful%20unified%20representation%20space%20through%20the%20HoMi%0ATokenizer.%20Using%20a%20single%20codebook%2C%20the%20HoMi%20Tokenizer%20in%20MotionLLaMA%20achieves%0Areconstruction%20accuracy%20comparable%20to%20residual%20vector%20quantization%20tokenizers%0Autilizing%20six%20codebooks%2C%20outperforming%20all%20existing%20single-codebook%20tokenizers.%0ASecond%2C%20MotionLLaMA%20integrates%20a%20large%20language%20model%20to%20tackle%20various%0Amotion-related%20tasks.%20This%20integration%20bridges%20various%20modalities%2C%20facilitating%0Aboth%20comprehensive%20and%20intricate%20motion%20synthesis%20and%20comprehension.%20Third%2C%0AMotionLLaMA%20introduces%20the%20MotionHub%20dataset%2C%20currently%20the%20most%20extensive%0Amultimodal%2C%20multitask%20motion%20dataset%2C%20which%20enables%20fine-tuning%20of%20large%0Alanguage%20models.%20Extensive%20experimental%20results%20demonstrate%20that%20MotionLLaMA%0Anot%20only%20covers%20the%20widest%20range%20of%20motion-related%20tasks%20but%20also%20achieves%0Astate-of-the-art%20%28SOTA%29%20performance%20in%20motion%20completion%2C%20interaction%0Adual-person%20text-to-motion%2C%20and%20all%20comprehension%20tasks%20while%20reaching%0Aperformance%20comparable%20to%20SOTA%20in%20the%20remaining%20tasks.%20The%20code%20and%20MotionHub%0Adataset%20are%20publicly%20available.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2411.17335v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DMotionLLaMA%253A%2520A%2520Unified%2520Framework%2520for%2520Motion%2520Synthesis%2520and%2520Comprehension%26entry.906535625%3DZeyu%2520Ling%2520and%2520Bo%2520Han%2520and%2520Shiyang%2520Li%2520and%2520Hongdeng%2520Shen%2520and%2520Jikang%2520Cheng%2520and%2520Changqing%2520Zou%26entry.1292438233%3D%2520%2520This%2520paper%2520introduces%2520MotionLLaMA%252C%2520a%2520unified%2520framework%2520for%2520motion%2520synthesis%250Aand%2520comprehension%252C%2520along%2520with%2520a%2520novel%2520full-body%2520motion%2520tokenizer%2520called%2520the%250AHoMi%2520Tokenizer.%2520MotionLLaMA%2520is%2520developed%2520based%2520on%2520three%2520core%2520principles.%2520First%252C%250Ait%2520establishes%2520a%2520powerful%2520unified%2520representation%2520space%2520through%2520the%2520HoMi%250ATokenizer.%2520Using%2520a%2520single%2520codebook%252C%2520the%2520HoMi%2520Tokenizer%2520in%2520MotionLLaMA%2520achieves%250Areconstruction%2520accuracy%2520comparable%2520to%2520residual%2520vector%2520quantization%2520tokenizers%250Autilizing%2520six%2520codebooks%252C%2520outperforming%2520all%2520existing%2520single-codebook%2520tokenizers.%250ASecond%252C%2520MotionLLaMA%2520integrates%2520a%2520large%2520language%2520model%2520to%2520tackle%2520various%250Amotion-related%2520tasks.%2520This%2520integration%2520bridges%2520various%2520modalities%252C%2520facilitating%250Aboth%2520comprehensive%2520and%2520intricate%2520motion%2520synthesis%2520and%2520comprehension.%2520Third%252C%250AMotionLLaMA%2520introduces%2520the%2520MotionHub%2520dataset%252C%2520currently%2520the%2520most%2520extensive%250Amultimodal%252C%2520multitask%2520motion%2520dataset%252C%2520which%2520enables%2520fine-tuning%2520of%2520large%250Alanguage%2520models.%2520Extensive%2520experimental%2520results%2520demonstrate%2520that%2520MotionLLaMA%250Anot%2520only%2520covers%2520the%2520widest%2520range%2520of%2520motion-related%2520tasks%2520but%2520also%2520achieves%250Astate-of-the-art%2520%2528SOTA%2529%2520performance%2520in%2520motion%2520completion%252C%2520interaction%250Adual-person%2520text-to-motion%252C%2520and%2520all%2520comprehension%2520tasks%2520while%2520reaching%250Aperformance%2520comparable%2520to%2520SOTA%2520in%2520the%2520remaining%2520tasks.%2520The%2520code%2520and%2520MotionHub%250Adataset%2520are%2520publicly%2520available.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2411.17335v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=MotionLLaMA%3A%20A%20Unified%20Framework%20for%20Motion%20Synthesis%20and%20Comprehension&entry.906535625=Zeyu%20Ling%20and%20Bo%20Han%20and%20Shiyang%20Li%20and%20Hongdeng%20Shen%20and%20Jikang%20Cheng%20and%20Changqing%20Zou&entry.1292438233=%20%20This%20paper%20introduces%20MotionLLaMA%2C%20a%20unified%20framework%20for%20motion%20synthesis%0Aand%20comprehension%2C%20along%20with%20a%20novel%20full-body%20motion%20tokenizer%20called%20the%0AHoMi%20Tokenizer.%20MotionLLaMA%20is%20developed%20based%20on%20three%20core%20principles.%20First%2C%0Ait%20establishes%20a%20powerful%20unified%20representation%20space%20through%20the%20HoMi%0ATokenizer.%20Using%20a%20single%20codebook%2C%20the%20HoMi%20Tokenizer%20in%20MotionLLaMA%20achieves%0Areconstruction%20accuracy%20comparable%20to%20residual%20vector%20quantization%20tokenizers%0Autilizing%20six%20codebooks%2C%20outperforming%20all%20existing%20single-codebook%20tokenizers.%0ASecond%2C%20MotionLLaMA%20integrates%20a%20large%20language%20model%20to%20tackle%20various%0Amotion-related%20tasks.%20This%20integration%20bridges%20various%20modalities%2C%20facilitating%0Aboth%20comprehensive%20and%20intricate%20motion%20synthesis%20and%20comprehension.%20Third%2C%0AMotionLLaMA%20introduces%20the%20MotionHub%20dataset%2C%20currently%20the%20most%20extensive%0Amultimodal%2C%20multitask%20motion%20dataset%2C%20which%20enables%20fine-tuning%20of%20large%0Alanguage%20models.%20Extensive%20experimental%20results%20demonstrate%20that%20MotionLLaMA%0Anot%20only%20covers%20the%20widest%20range%20of%20motion-related%20tasks%20but%20also%20achieves%0Astate-of-the-art%20%28SOTA%29%20performance%20in%20motion%20completion%2C%20interaction%0Adual-person%20text-to-motion%2C%20and%20all%20comprehension%20tasks%20while%20reaching%0Aperformance%20comparable%20to%20SOTA%20in%20the%20remaining%20tasks.%20The%20code%20and%20MotionHub%0Adataset%20are%20publicly%20available.%0A&entry.1838667208=http%3A//arxiv.org/abs/2411.17335v1&entry.124074799=Read"},
{"title": "Data-driven development of cycle prediction models for lithium metal\n  batteries using multi modal mining", "author": "Jaewoong Lee and Junhee Woo and Sejin Kim and Cinthya Paulina and Hyunmin Park and Hee-Tak Kim and Steve Park and Jihan Kim", "abstract": "  Recent advances in data-driven research have shown great potential in\nunderstanding the intricate relationships between materials and their\nperformances. Herein, we introduce a novel multi modal data-driven approach\nemploying an Automatic Battery data Collector (ABC) that integrates a large\nlanguage model (LLM) with an automatic graph mining tool, Material Graph\nDigitizer (MatGD). This platform enables state-of-the-art accurate extraction\nof battery material data and cyclability performance metrics from diverse\ntextual and graphical data sources. From the database derived through the ABC\nplatform, we developed machine learning models that can accurately predict the\ncapacity and stability of lithium metal batteries, which is the first-ever\nmodel developed to achieve such predictions. Our models were also\nexperimentally validated, confirming practical applicability and reliability of\nour data-driven approach.\n", "link": "http://arxiv.org/abs/2411.17625v1", "date": "2024-11-26", "relevancy": 0.904, "topK": [{"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.4927}, {"title": "Multimodal-Conditioned Latent Diffusion Models for Fashion Image Editing", "link": "http://arxiv.org/abs/2403.14828v2", "similarity": 0.4349}, {"title": "SDXS: Real-Time One-Step Latent Diffusion Models with Image Conditions", "link": "http://arxiv.org/abs/2403.16627v1", "similarity": 0.4285}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Data-driven%20development%20of%20cycle%20prediction%20models%20for%20lithium%20metal%0A%20%20batteries%20using%20multi%20modal%20mining&body=Title%3A%20Data-driven%20development%20of%20cycle%20prediction%20models%20for%20lithium%20metal%0A%20%20batteries%20using%20multi%20modal%20mining%0AAuthor%3A%20Jaewoong%20Lee%20and%20Junhee%20Woo%20and%20Sejin%20Kim%20and%20Cinthya%20Paulina%20and%20Hyunmin%20Park%20and%20Hee-Tak%20Kim%20and%20Steve%20Park%20and%20Jihan%20Kim%0AAbstract%3A%20%20%20Recent%20advances%20in%20data-driven%20research%20have%20shown%20great%20potential%20in%0Aunderstanding%20the%20intricate%20relationships%20between%20materials%20and%20their%0Aperformances.%20Herein%2C%20we%20introduce%20a%20novel%20multi%20modal%20data-driven%20approach%0Aemploying%20an%20Automatic%20Battery%20data%20Collector%20%28ABC%29%20that%20integrates%20a%20large%0Alanguage%20model%20%28LLM%29%20with%20an%20automatic%20graph%20mining%20tool%2C%20Material%20Graph%0ADigitizer%20%28MatGD%29.%20This%20platform%20enables%20state-of-the-art%20accurate%20extraction%0Aof%20battery%20material%20data%20and%20cyclability%20performance%20metrics%20from%20diverse%0Atextual%20and%20graphical%20data%20sources.%20From%20the%20database%20derived%20through%20the%20ABC%0Aplatform%2C%20we%20developed%20machine%20learning%20models%20that%20can%20accurately%20predict%20the%0Acapacity%20and%20stability%20of%20lithium%20metal%20batteries%2C%20which%20is%20the%20first-ever%0Amodel%20developed%20to%20achieve%20such%20predictions.%20Our%20models%20were%20also%0Aexperimentally%20validated%2C%20confirming%20practical%20applicability%20and%20reliability%20of%0Aour%20data-driven%20approach.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2411.17625v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DData-driven%2520development%2520of%2520cycle%2520prediction%2520models%2520for%2520lithium%2520metal%250A%2520%2520batteries%2520using%2520multi%2520modal%2520mining%26entry.906535625%3DJaewoong%2520Lee%2520and%2520Junhee%2520Woo%2520and%2520Sejin%2520Kim%2520and%2520Cinthya%2520Paulina%2520and%2520Hyunmin%2520Park%2520and%2520Hee-Tak%2520Kim%2520and%2520Steve%2520Park%2520and%2520Jihan%2520Kim%26entry.1292438233%3D%2520%2520Recent%2520advances%2520in%2520data-driven%2520research%2520have%2520shown%2520great%2520potential%2520in%250Aunderstanding%2520the%2520intricate%2520relationships%2520between%2520materials%2520and%2520their%250Aperformances.%2520Herein%252C%2520we%2520introduce%2520a%2520novel%2520multi%2520modal%2520data-driven%2520approach%250Aemploying%2520an%2520Automatic%2520Battery%2520data%2520Collector%2520%2528ABC%2529%2520that%2520integrates%2520a%2520large%250Alanguage%2520model%2520%2528LLM%2529%2520with%2520an%2520automatic%2520graph%2520mining%2520tool%252C%2520Material%2520Graph%250ADigitizer%2520%2528MatGD%2529.%2520This%2520platform%2520enables%2520state-of-the-art%2520accurate%2520extraction%250Aof%2520battery%2520material%2520data%2520and%2520cyclability%2520performance%2520metrics%2520from%2520diverse%250Atextual%2520and%2520graphical%2520data%2520sources.%2520From%2520the%2520database%2520derived%2520through%2520the%2520ABC%250Aplatform%252C%2520we%2520developed%2520machine%2520learning%2520models%2520that%2520can%2520accurately%2520predict%2520the%250Acapacity%2520and%2520stability%2520of%2520lithium%2520metal%2520batteries%252C%2520which%2520is%2520the%2520first-ever%250Amodel%2520developed%2520to%2520achieve%2520such%2520predictions.%2520Our%2520models%2520were%2520also%250Aexperimentally%2520validated%252C%2520confirming%2520practical%2520applicability%2520and%2520reliability%2520of%250Aour%2520data-driven%2520approach.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2411.17625v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Data-driven%20development%20of%20cycle%20prediction%20models%20for%20lithium%20metal%0A%20%20batteries%20using%20multi%20modal%20mining&entry.906535625=Jaewoong%20Lee%20and%20Junhee%20Woo%20and%20Sejin%20Kim%20and%20Cinthya%20Paulina%20and%20Hyunmin%20Park%20and%20Hee-Tak%20Kim%20and%20Steve%20Park%20and%20Jihan%20Kim&entry.1292438233=%20%20Recent%20advances%20in%20data-driven%20research%20have%20shown%20great%20potential%20in%0Aunderstanding%20the%20intricate%20relationships%20between%20materials%20and%20their%0Aperformances.%20Herein%2C%20we%20introduce%20a%20novel%20multi%20modal%20data-driven%20approach%0Aemploying%20an%20Automatic%20Battery%20data%20Collector%20%28ABC%29%20that%20integrates%20a%20large%0Alanguage%20model%20%28LLM%29%20with%20an%20automatic%20graph%20mining%20tool%2C%20Material%20Graph%0ADigitizer%20%28MatGD%29.%20This%20platform%20enables%20state-of-the-art%20accurate%20extraction%0Aof%20battery%20material%20data%20and%20cyclability%20performance%20metrics%20from%20diverse%0Atextual%20and%20graphical%20data%20sources.%20From%20the%20database%20derived%20through%20the%20ABC%0Aplatform%2C%20we%20developed%20machine%20learning%20models%20that%20can%20accurately%20predict%20the%0Acapacity%20and%20stability%20of%20lithium%20metal%20batteries%2C%20which%20is%20the%20first-ever%0Amodel%20developed%20to%20achieve%20such%20predictions.%20Our%20models%20were%20also%0Aexperimentally%20validated%2C%20confirming%20practical%20applicability%20and%20reliability%20of%0Aour%20data-driven%20approach.%0A&entry.1838667208=http%3A//arxiv.org/abs/2411.17625v1&entry.124074799=Read"},
{"title": "ER2Score: LLM-based Explainable and Customizable Metric for Assessing\n  Radiology Reports with Reward-Control Loss", "author": "Yunyi Liu and Yingshu Li and Zhanyu Wang and Xinyu Liang and Lingqiao Liu and Lei Wang and Luping Zhou", "abstract": "  Automated radiology report generation (R2Gen) has advanced significantly,\nintroducing challenges in accurate evaluation due to its complexity.\nTraditional metrics often fall short by relying on rigid word-matching or\nfocusing only on pathological entities, leading to inconsistencies with human\nassessments. To bridge this gap, we introduce ER2Score, an automatic evaluation\nmetric designed specifically for R2Gen. Our metric utilizes a reward model,\nguided by our margin-based reward enforcement loss, along with a tailored\ntraining data design that enables customization of evaluation criteria to suit\nuser-defined needs. It not only scores reports according to user-specified\ncriteria but also provides detailed sub-scores, enhancing interpretability and\nallowing users to adjust the criteria between different aspects of reports.\nLeveraging GPT-4, we designed an easy-to-use data generation pipeline, enabling\nus to produce extensive training data based on two distinct scoring systems,\neach containing reports of varying quality along with corresponding scores.\nThese GPT-generated reports are then paired as accepted and rejected samples\nthrough our pairing rule to train an LLM towards our fine-grained reward model,\nwhich assigns higher rewards to the report with high quality. Our\nreward-control loss enables this model to simultaneously output multiple\nindividual rewards corresponding to the number of evaluation criteria, with\ntheir summation as our final ER2Score. Our experiments demonstrate ER2Score's\nheightened correlation with human judgments and superior performance in model\nselection compared to traditional metrics. Notably, our model provides both an\noverall score and individual scores for each evaluation item, enhancing\ninterpretability. We also demonstrate its flexible training across various\nevaluation systems.\n", "link": "http://arxiv.org/abs/2411.17301v1", "date": "2024-11-26", "relevancy": 1.4223, "topK": [{"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.5206}, {"title": "Self-supervised Photographic Image Layout Representation Learning", "link": "http://arxiv.org/abs/2403.03740v1", "similarity": 0.4631}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.4553}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20ER2Score%3A%20LLM-based%20Explainable%20and%20Customizable%20Metric%20for%20Assessing%0A%20%20Radiology%20Reports%20with%20Reward-Control%20Loss&body=Title%3A%20ER2Score%3A%20LLM-based%20Explainable%20and%20Customizable%20Metric%20for%20Assessing%0A%20%20Radiology%20Reports%20with%20Reward-Control%20Loss%0AAuthor%3A%20Yunyi%20Liu%20and%20Yingshu%20Li%20and%20Zhanyu%20Wang%20and%20Xinyu%20Liang%20and%20Lingqiao%20Liu%20and%20Lei%20Wang%20and%20Luping%20Zhou%0AAbstract%3A%20%20%20Automated%20radiology%20report%20generation%20%28R2Gen%29%20has%20advanced%20significantly%2C%0Aintroducing%20challenges%20in%20accurate%20evaluation%20due%20to%20its%20complexity.%0ATraditional%20metrics%20often%20fall%20short%20by%20relying%20on%20rigid%20word-matching%20or%0Afocusing%20only%20on%20pathological%20entities%2C%20leading%20to%20inconsistencies%20with%20human%0Aassessments.%20To%20bridge%20this%20gap%2C%20we%20introduce%20ER2Score%2C%20an%20automatic%20evaluation%0Ametric%20designed%20specifically%20for%20R2Gen.%20Our%20metric%20utilizes%20a%20reward%20model%2C%0Aguided%20by%20our%20margin-based%20reward%20enforcement%20loss%2C%20along%20with%20a%20tailored%0Atraining%20data%20design%20that%20enables%20customization%20of%20evaluation%20criteria%20to%20suit%0Auser-defined%20needs.%20It%20not%20only%20scores%20reports%20according%20to%20user-specified%0Acriteria%20but%20also%20provides%20detailed%20sub-scores%2C%20enhancing%20interpretability%20and%0Aallowing%20users%20to%20adjust%20the%20criteria%20between%20different%20aspects%20of%20reports.%0ALeveraging%20GPT-4%2C%20we%20designed%20an%20easy-to-use%20data%20generation%20pipeline%2C%20enabling%0Aus%20to%20produce%20extensive%20training%20data%20based%20on%20two%20distinct%20scoring%20systems%2C%0Aeach%20containing%20reports%20of%20varying%20quality%20along%20with%20corresponding%20scores.%0AThese%20GPT-generated%20reports%20are%20then%20paired%20as%20accepted%20and%20rejected%20samples%0Athrough%20our%20pairing%20rule%20to%20train%20an%20LLM%20towards%20our%20fine-grained%20reward%20model%2C%0Awhich%20assigns%20higher%20rewards%20to%20the%20report%20with%20high%20quality.%20Our%0Areward-control%20loss%20enables%20this%20model%20to%20simultaneously%20output%20multiple%0Aindividual%20rewards%20corresponding%20to%20the%20number%20of%20evaluation%20criteria%2C%20with%0Atheir%20summation%20as%20our%20final%20ER2Score.%20Our%20experiments%20demonstrate%20ER2Score%27s%0Aheightened%20correlation%20with%20human%20judgments%20and%20superior%20performance%20in%20model%0Aselection%20compared%20to%20traditional%20metrics.%20Notably%2C%20our%20model%20provides%20both%20an%0Aoverall%20score%20and%20individual%20scores%20for%20each%20evaluation%20item%2C%20enhancing%0Ainterpretability.%20We%20also%20demonstrate%20its%20flexible%20training%20across%20various%0Aevaluation%20systems.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2411.17301v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DER2Score%253A%2520LLM-based%2520Explainable%2520and%2520Customizable%2520Metric%2520for%2520Assessing%250A%2520%2520Radiology%2520Reports%2520with%2520Reward-Control%2520Loss%26entry.906535625%3DYunyi%2520Liu%2520and%2520Yingshu%2520Li%2520and%2520Zhanyu%2520Wang%2520and%2520Xinyu%2520Liang%2520and%2520Lingqiao%2520Liu%2520and%2520Lei%2520Wang%2520and%2520Luping%2520Zhou%26entry.1292438233%3D%2520%2520Automated%2520radiology%2520report%2520generation%2520%2528R2Gen%2529%2520has%2520advanced%2520significantly%252C%250Aintroducing%2520challenges%2520in%2520accurate%2520evaluation%2520due%2520to%2520its%2520complexity.%250ATraditional%2520metrics%2520often%2520fall%2520short%2520by%2520relying%2520on%2520rigid%2520word-matching%2520or%250Afocusing%2520only%2520on%2520pathological%2520entities%252C%2520leading%2520to%2520inconsistencies%2520with%2520human%250Aassessments.%2520To%2520bridge%2520this%2520gap%252C%2520we%2520introduce%2520ER2Score%252C%2520an%2520automatic%2520evaluation%250Ametric%2520designed%2520specifically%2520for%2520R2Gen.%2520Our%2520metric%2520utilizes%2520a%2520reward%2520model%252C%250Aguided%2520by%2520our%2520margin-based%2520reward%2520enforcement%2520loss%252C%2520along%2520with%2520a%2520tailored%250Atraining%2520data%2520design%2520that%2520enables%2520customization%2520of%2520evaluation%2520criteria%2520to%2520suit%250Auser-defined%2520needs.%2520It%2520not%2520only%2520scores%2520reports%2520according%2520to%2520user-specified%250Acriteria%2520but%2520also%2520provides%2520detailed%2520sub-scores%252C%2520enhancing%2520interpretability%2520and%250Aallowing%2520users%2520to%2520adjust%2520the%2520criteria%2520between%2520different%2520aspects%2520of%2520reports.%250ALeveraging%2520GPT-4%252C%2520we%2520designed%2520an%2520easy-to-use%2520data%2520generation%2520pipeline%252C%2520enabling%250Aus%2520to%2520produce%2520extensive%2520training%2520data%2520based%2520on%2520two%2520distinct%2520scoring%2520systems%252C%250Aeach%2520containing%2520reports%2520of%2520varying%2520quality%2520along%2520with%2520corresponding%2520scores.%250AThese%2520GPT-generated%2520reports%2520are%2520then%2520paired%2520as%2520accepted%2520and%2520rejected%2520samples%250Athrough%2520our%2520pairing%2520rule%2520to%2520train%2520an%2520LLM%2520towards%2520our%2520fine-grained%2520reward%2520model%252C%250Awhich%2520assigns%2520higher%2520rewards%2520to%2520the%2520report%2520with%2520high%2520quality.%2520Our%250Areward-control%2520loss%2520enables%2520this%2520model%2520to%2520simultaneously%2520output%2520multiple%250Aindividual%2520rewards%2520corresponding%2520to%2520the%2520number%2520of%2520evaluation%2520criteria%252C%2520with%250Atheir%2520summation%2520as%2520our%2520final%2520ER2Score.%2520Our%2520experiments%2520demonstrate%2520ER2Score%2527s%250Aheightened%2520correlation%2520with%2520human%2520judgments%2520and%2520superior%2520performance%2520in%2520model%250Aselection%2520compared%2520to%2520traditional%2520metrics.%2520Notably%252C%2520our%2520model%2520provides%2520both%2520an%250Aoverall%2520score%2520and%2520individual%2520scores%2520for%2520each%2520evaluation%2520item%252C%2520enhancing%250Ainterpretability.%2520We%2520also%2520demonstrate%2520its%2520flexible%2520training%2520across%2520various%250Aevaluation%2520systems.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2411.17301v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=ER2Score%3A%20LLM-based%20Explainable%20and%20Customizable%20Metric%20for%20Assessing%0A%20%20Radiology%20Reports%20with%20Reward-Control%20Loss&entry.906535625=Yunyi%20Liu%20and%20Yingshu%20Li%20and%20Zhanyu%20Wang%20and%20Xinyu%20Liang%20and%20Lingqiao%20Liu%20and%20Lei%20Wang%20and%20Luping%20Zhou&entry.1292438233=%20%20Automated%20radiology%20report%20generation%20%28R2Gen%29%20has%20advanced%20significantly%2C%0Aintroducing%20challenges%20in%20accurate%20evaluation%20due%20to%20its%20complexity.%0ATraditional%20metrics%20often%20fall%20short%20by%20relying%20on%20rigid%20word-matching%20or%0Afocusing%20only%20on%20pathological%20entities%2C%20leading%20to%20inconsistencies%20with%20human%0Aassessments.%20To%20bridge%20this%20gap%2C%20we%20introduce%20ER2Score%2C%20an%20automatic%20evaluation%0Ametric%20designed%20specifically%20for%20R2Gen.%20Our%20metric%20utilizes%20a%20reward%20model%2C%0Aguided%20by%20our%20margin-based%20reward%20enforcement%20loss%2C%20along%20with%20a%20tailored%0Atraining%20data%20design%20that%20enables%20customization%20of%20evaluation%20criteria%20to%20suit%0Auser-defined%20needs.%20It%20not%20only%20scores%20reports%20according%20to%20user-specified%0Acriteria%20but%20also%20provides%20detailed%20sub-scores%2C%20enhancing%20interpretability%20and%0Aallowing%20users%20to%20adjust%20the%20criteria%20between%20different%20aspects%20of%20reports.%0ALeveraging%20GPT-4%2C%20we%20designed%20an%20easy-to-use%20data%20generation%20pipeline%2C%20enabling%0Aus%20to%20produce%20extensive%20training%20data%20based%20on%20two%20distinct%20scoring%20systems%2C%0Aeach%20containing%20reports%20of%20varying%20quality%20along%20with%20corresponding%20scores.%0AThese%20GPT-generated%20reports%20are%20then%20paired%20as%20accepted%20and%20rejected%20samples%0Athrough%20our%20pairing%20rule%20to%20train%20an%20LLM%20towards%20our%20fine-grained%20reward%20model%2C%0Awhich%20assigns%20higher%20rewards%20to%20the%20report%20with%20high%20quality.%20Our%0Areward-control%20loss%20enables%20this%20model%20to%20simultaneously%20output%20multiple%0Aindividual%20rewards%20corresponding%20to%20the%20number%20of%20evaluation%20criteria%2C%20with%0Atheir%20summation%20as%20our%20final%20ER2Score.%20Our%20experiments%20demonstrate%20ER2Score%27s%0Aheightened%20correlation%20with%20human%20judgments%20and%20superior%20performance%20in%20model%0Aselection%20compared%20to%20traditional%20metrics.%20Notably%2C%20our%20model%20provides%20both%20an%0Aoverall%20score%20and%20individual%20scores%20for%20each%20evaluation%20item%2C%20enhancing%0Ainterpretability.%20We%20also%20demonstrate%20its%20flexible%20training%20across%20various%0Aevaluation%20systems.%0A&entry.1838667208=http%3A//arxiv.org/abs/2411.17301v1&entry.124074799=Read"},
{"title": "Storing overlapping associative memories on latent manifolds in low-rank\n  spiking networks", "author": "William F. Podlaski and Christian K. Machens", "abstract": "  Associative memory architectures such as the Hopfield network have long been\nimportant conceptual and theoretical models for neuroscience and artificial\nintelligence. However, translating these abstract models into spiking neural\nnetworks has been surprisingly difficult. Indeed, much previous work has been\nrestricted to storing a small number of primarily non-overlapping memories in\nlarge networks, thereby limiting their scalability. Here, we revisit the\nassociative memory problem in light of recent advances in understanding\nspike-based computation. Using a recently-established geometric framework, we\nshow that the spiking activity for a large class of all-inhibitory networks is\nsituated on a low-dimensional, convex, and piecewise-linear manifold, with\ndynamics that move along the manifold. We then map the associative memory\nproblem onto these dynamics, and demonstrate how the vertices of a hypercubic\nmanifold can be used to store stable, overlapping activity patterns with a\ndirect correspondence to the original Hopfield model. We propose several\nlearning rules, and demonstrate a linear scaling of the storage capacity with\nthe number of neurons, as well as robust pattern completion abilities. Overall,\nthis work serves as a case study to demonstrate the effectiveness of using a\ngeometrical perspective to design dynamics on neural manifolds, with\nimplications for neuroscience and machine learning.\n", "link": "http://arxiv.org/abs/2411.17485v1", "date": "2024-11-26", "relevancy": 1.8881, "topK": [{"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.4756}, {"title": "Self-supervised Photographic Image Layout Representation Learning", "link": "http://arxiv.org/abs/2403.03740v1", "similarity": 0.4738}, {"title": "Dynamic Gaussians Mesh: Consistent Mesh Reconstruction from Monocular\n  Videos", "link": "http://arxiv.org/abs/2404.12379v2", "similarity": 0.4688}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Storing%20overlapping%20associative%20memories%20on%20latent%20manifolds%20in%20low-rank%0A%20%20spiking%20networks&body=Title%3A%20Storing%20overlapping%20associative%20memories%20on%20latent%20manifolds%20in%20low-rank%0A%20%20spiking%20networks%0AAuthor%3A%20William%20F.%20Podlaski%20and%20Christian%20K.%20Machens%0AAbstract%3A%20%20%20Associative%20memory%20architectures%20such%20as%20the%20Hopfield%20network%20have%20long%20been%0Aimportant%20conceptual%20and%20theoretical%20models%20for%20neuroscience%20and%20artificial%0Aintelligence.%20However%2C%20translating%20these%20abstract%20models%20into%20spiking%20neural%0Anetworks%20has%20been%20surprisingly%20difficult.%20Indeed%2C%20much%20previous%20work%20has%20been%0Arestricted%20to%20storing%20a%20small%20number%20of%20primarily%20non-overlapping%20memories%20in%0Alarge%20networks%2C%20thereby%20limiting%20their%20scalability.%20Here%2C%20we%20revisit%20the%0Aassociative%20memory%20problem%20in%20light%20of%20recent%20advances%20in%20understanding%0Aspike-based%20computation.%20Using%20a%20recently-established%20geometric%20framework%2C%20we%0Ashow%20that%20the%20spiking%20activity%20for%20a%20large%20class%20of%20all-inhibitory%20networks%20is%0Asituated%20on%20a%20low-dimensional%2C%20convex%2C%20and%20piecewise-linear%20manifold%2C%20with%0Adynamics%20that%20move%20along%20the%20manifold.%20We%20then%20map%20the%20associative%20memory%0Aproblem%20onto%20these%20dynamics%2C%20and%20demonstrate%20how%20the%20vertices%20of%20a%20hypercubic%0Amanifold%20can%20be%20used%20to%20store%20stable%2C%20overlapping%20activity%20patterns%20with%20a%0Adirect%20correspondence%20to%20the%20original%20Hopfield%20model.%20We%20propose%20several%0Alearning%20rules%2C%20and%20demonstrate%20a%20linear%20scaling%20of%20the%20storage%20capacity%20with%0Athe%20number%20of%20neurons%2C%20as%20well%20as%20robust%20pattern%20completion%20abilities.%20Overall%2C%0Athis%20work%20serves%20as%20a%20case%20study%20to%20demonstrate%20the%20effectiveness%20of%20using%20a%0Ageometrical%20perspective%20to%20design%20dynamics%20on%20neural%20manifolds%2C%20with%0Aimplications%20for%20neuroscience%20and%20machine%20learning.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2411.17485v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DStoring%2520overlapping%2520associative%2520memories%2520on%2520latent%2520manifolds%2520in%2520low-rank%250A%2520%2520spiking%2520networks%26entry.906535625%3DWilliam%2520F.%2520Podlaski%2520and%2520Christian%2520K.%2520Machens%26entry.1292438233%3D%2520%2520Associative%2520memory%2520architectures%2520such%2520as%2520the%2520Hopfield%2520network%2520have%2520long%2520been%250Aimportant%2520conceptual%2520and%2520theoretical%2520models%2520for%2520neuroscience%2520and%2520artificial%250Aintelligence.%2520However%252C%2520translating%2520these%2520abstract%2520models%2520into%2520spiking%2520neural%250Anetworks%2520has%2520been%2520surprisingly%2520difficult.%2520Indeed%252C%2520much%2520previous%2520work%2520has%2520been%250Arestricted%2520to%2520storing%2520a%2520small%2520number%2520of%2520primarily%2520non-overlapping%2520memories%2520in%250Alarge%2520networks%252C%2520thereby%2520limiting%2520their%2520scalability.%2520Here%252C%2520we%2520revisit%2520the%250Aassociative%2520memory%2520problem%2520in%2520light%2520of%2520recent%2520advances%2520in%2520understanding%250Aspike-based%2520computation.%2520Using%2520a%2520recently-established%2520geometric%2520framework%252C%2520we%250Ashow%2520that%2520the%2520spiking%2520activity%2520for%2520a%2520large%2520class%2520of%2520all-inhibitory%2520networks%2520is%250Asituated%2520on%2520a%2520low-dimensional%252C%2520convex%252C%2520and%2520piecewise-linear%2520manifold%252C%2520with%250Adynamics%2520that%2520move%2520along%2520the%2520manifold.%2520We%2520then%2520map%2520the%2520associative%2520memory%250Aproblem%2520onto%2520these%2520dynamics%252C%2520and%2520demonstrate%2520how%2520the%2520vertices%2520of%2520a%2520hypercubic%250Amanifold%2520can%2520be%2520used%2520to%2520store%2520stable%252C%2520overlapping%2520activity%2520patterns%2520with%2520a%250Adirect%2520correspondence%2520to%2520the%2520original%2520Hopfield%2520model.%2520We%2520propose%2520several%250Alearning%2520rules%252C%2520and%2520demonstrate%2520a%2520linear%2520scaling%2520of%2520the%2520storage%2520capacity%2520with%250Athe%2520number%2520of%2520neurons%252C%2520as%2520well%2520as%2520robust%2520pattern%2520completion%2520abilities.%2520Overall%252C%250Athis%2520work%2520serves%2520as%2520a%2520case%2520study%2520to%2520demonstrate%2520the%2520effectiveness%2520of%2520using%2520a%250Ageometrical%2520perspective%2520to%2520design%2520dynamics%2520on%2520neural%2520manifolds%252C%2520with%250Aimplications%2520for%2520neuroscience%2520and%2520machine%2520learning.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2411.17485v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Storing%20overlapping%20associative%20memories%20on%20latent%20manifolds%20in%20low-rank%0A%20%20spiking%20networks&entry.906535625=William%20F.%20Podlaski%20and%20Christian%20K.%20Machens&entry.1292438233=%20%20Associative%20memory%20architectures%20such%20as%20the%20Hopfield%20network%20have%20long%20been%0Aimportant%20conceptual%20and%20theoretical%20models%20for%20neuroscience%20and%20artificial%0Aintelligence.%20However%2C%20translating%20these%20abstract%20models%20into%20spiking%20neural%0Anetworks%20has%20been%20surprisingly%20difficult.%20Indeed%2C%20much%20previous%20work%20has%20been%0Arestricted%20to%20storing%20a%20small%20number%20of%20primarily%20non-overlapping%20memories%20in%0Alarge%20networks%2C%20thereby%20limiting%20their%20scalability.%20Here%2C%20we%20revisit%20the%0Aassociative%20memory%20problem%20in%20light%20of%20recent%20advances%20in%20understanding%0Aspike-based%20computation.%20Using%20a%20recently-established%20geometric%20framework%2C%20we%0Ashow%20that%20the%20spiking%20activity%20for%20a%20large%20class%20of%20all-inhibitory%20networks%20is%0Asituated%20on%20a%20low-dimensional%2C%20convex%2C%20and%20piecewise-linear%20manifold%2C%20with%0Adynamics%20that%20move%20along%20the%20manifold.%20We%20then%20map%20the%20associative%20memory%0Aproblem%20onto%20these%20dynamics%2C%20and%20demonstrate%20how%20the%20vertices%20of%20a%20hypercubic%0Amanifold%20can%20be%20used%20to%20store%20stable%2C%20overlapping%20activity%20patterns%20with%20a%0Adirect%20correspondence%20to%20the%20original%20Hopfield%20model.%20We%20propose%20several%0Alearning%20rules%2C%20and%20demonstrate%20a%20linear%20scaling%20of%20the%20storage%20capacity%20with%0Athe%20number%20of%20neurons%2C%20as%20well%20as%20robust%20pattern%20completion%20abilities.%20Overall%2C%0Athis%20work%20serves%20as%20a%20case%20study%20to%20demonstrate%20the%20effectiveness%20of%20using%20a%0Ageometrical%20perspective%20to%20design%20dynamics%20on%20neural%20manifolds%2C%20with%0Aimplications%20for%20neuroscience%20and%20machine%20learning.%0A&entry.1838667208=http%3A//arxiv.org/abs/2411.17485v1&entry.124074799=Read"},
{"title": "Accelerating Vision Diffusion Transformers with Skip Branches", "author": "Guanjie Chen and Xinyu Zhao and Yucheng Zhou and Tianlong Chen and Cheng Yu", "abstract": "  Diffusion Transformers (DiT), an emerging image and video generation model\narchitecture, has demonstrated great potential because of its high generation\nquality and scalability properties. Despite the impressive performance, its\npractical deployment is constrained by computational complexity and redundancy\nin the sequential denoising process. While feature caching across timesteps has\nproven effective in accelerating diffusion models, its application to DiT is\nlimited by fundamental architectural differences from U-Net-based approaches.\nThrough empirical analysis of DiT feature dynamics, we identify that\nsignificant feature variation between DiT blocks presents a key challenge for\nfeature reusability. To address this, we convert standard DiT into Skip-DiT\nwith skip branches to enhance feature smoothness. Further, we introduce\nSkip-Cache which utilizes the skip branches to cache DiT features across\ntimesteps at the inference time. We validated effectiveness of our proposal on\ndifferent DiT backbones for video and image generation, showcasing skip\nbranches to help preserve generation quality and achieve higher speedup.\nExperimental results indicate that Skip-DiT achieves a 1.5x speedup almost for\nfree and a 2.2x speedup with only a minor reduction in quantitative metrics.\nCode is available at https://github.com/OpenSparseLLMs/Skip-DiT.git.\n", "link": "http://arxiv.org/abs/2411.17616v1", "date": "2024-11-26", "relevancy": 1.9783, "topK": [{"title": "SDXS: Real-Time One-Step Latent Diffusion Models with Image Conditions", "link": "http://arxiv.org/abs/2403.16627v1", "similarity": 0.7015}, {"title": "ToDo: Token Downsampling for Efficient Generation of High-Resolution\n  Images", "link": "http://arxiv.org/abs/2402.13573v2", "similarity": 0.6497}, {"title": "Scaling Rectified Flow Transformers for High-Resolution Image Synthesis", "link": "http://arxiv.org/abs/2403.03206v1", "similarity": 0.6417}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Accelerating%20Vision%20Diffusion%20Transformers%20with%20Skip%20Branches&body=Title%3A%20Accelerating%20Vision%20Diffusion%20Transformers%20with%20Skip%20Branches%0AAuthor%3A%20Guanjie%20Chen%20and%20Xinyu%20Zhao%20and%20Yucheng%20Zhou%20and%20Tianlong%20Chen%20and%20Cheng%20Yu%0AAbstract%3A%20%20%20Diffusion%20Transformers%20%28DiT%29%2C%20an%20emerging%20image%20and%20video%20generation%20model%0Aarchitecture%2C%20has%20demonstrated%20great%20potential%20because%20of%20its%20high%20generation%0Aquality%20and%20scalability%20properties.%20Despite%20the%20impressive%20performance%2C%20its%0Apractical%20deployment%20is%20constrained%20by%20computational%20complexity%20and%20redundancy%0Ain%20the%20sequential%20denoising%20process.%20While%20feature%20caching%20across%20timesteps%20has%0Aproven%20effective%20in%20accelerating%20diffusion%20models%2C%20its%20application%20to%20DiT%20is%0Alimited%20by%20fundamental%20architectural%20differences%20from%20U-Net-based%20approaches.%0AThrough%20empirical%20analysis%20of%20DiT%20feature%20dynamics%2C%20we%20identify%20that%0Asignificant%20feature%20variation%20between%20DiT%20blocks%20presents%20a%20key%20challenge%20for%0Afeature%20reusability.%20To%20address%20this%2C%20we%20convert%20standard%20DiT%20into%20Skip-DiT%0Awith%20skip%20branches%20to%20enhance%20feature%20smoothness.%20Further%2C%20we%20introduce%0ASkip-Cache%20which%20utilizes%20the%20skip%20branches%20to%20cache%20DiT%20features%20across%0Atimesteps%20at%20the%20inference%20time.%20We%20validated%20effectiveness%20of%20our%20proposal%20on%0Adifferent%20DiT%20backbones%20for%20video%20and%20image%20generation%2C%20showcasing%20skip%0Abranches%20to%20help%20preserve%20generation%20quality%20and%20achieve%20higher%20speedup.%0AExperimental%20results%20indicate%20that%20Skip-DiT%20achieves%20a%201.5x%20speedup%20almost%20for%0Afree%20and%20a%202.2x%20speedup%20with%20only%20a%20minor%20reduction%20in%20quantitative%20metrics.%0ACode%20is%20available%20at%20https%3A//github.com/OpenSparseLLMs/Skip-DiT.git.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2411.17616v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DAccelerating%2520Vision%2520Diffusion%2520Transformers%2520with%2520Skip%2520Branches%26entry.906535625%3DGuanjie%2520Chen%2520and%2520Xinyu%2520Zhao%2520and%2520Yucheng%2520Zhou%2520and%2520Tianlong%2520Chen%2520and%2520Cheng%2520Yu%26entry.1292438233%3D%2520%2520Diffusion%2520Transformers%2520%2528DiT%2529%252C%2520an%2520emerging%2520image%2520and%2520video%2520generation%2520model%250Aarchitecture%252C%2520has%2520demonstrated%2520great%2520potential%2520because%2520of%2520its%2520high%2520generation%250Aquality%2520and%2520scalability%2520properties.%2520Despite%2520the%2520impressive%2520performance%252C%2520its%250Apractical%2520deployment%2520is%2520constrained%2520by%2520computational%2520complexity%2520and%2520redundancy%250Ain%2520the%2520sequential%2520denoising%2520process.%2520While%2520feature%2520caching%2520across%2520timesteps%2520has%250Aproven%2520effective%2520in%2520accelerating%2520diffusion%2520models%252C%2520its%2520application%2520to%2520DiT%2520is%250Alimited%2520by%2520fundamental%2520architectural%2520differences%2520from%2520U-Net-based%2520approaches.%250AThrough%2520empirical%2520analysis%2520of%2520DiT%2520feature%2520dynamics%252C%2520we%2520identify%2520that%250Asignificant%2520feature%2520variation%2520between%2520DiT%2520blocks%2520presents%2520a%2520key%2520challenge%2520for%250Afeature%2520reusability.%2520To%2520address%2520this%252C%2520we%2520convert%2520standard%2520DiT%2520into%2520Skip-DiT%250Awith%2520skip%2520branches%2520to%2520enhance%2520feature%2520smoothness.%2520Further%252C%2520we%2520introduce%250ASkip-Cache%2520which%2520utilizes%2520the%2520skip%2520branches%2520to%2520cache%2520DiT%2520features%2520across%250Atimesteps%2520at%2520the%2520inference%2520time.%2520We%2520validated%2520effectiveness%2520of%2520our%2520proposal%2520on%250Adifferent%2520DiT%2520backbones%2520for%2520video%2520and%2520image%2520generation%252C%2520showcasing%2520skip%250Abranches%2520to%2520help%2520preserve%2520generation%2520quality%2520and%2520achieve%2520higher%2520speedup.%250AExperimental%2520results%2520indicate%2520that%2520Skip-DiT%2520achieves%2520a%25201.5x%2520speedup%2520almost%2520for%250Afree%2520and%2520a%25202.2x%2520speedup%2520with%2520only%2520a%2520minor%2520reduction%2520in%2520quantitative%2520metrics.%250ACode%2520is%2520available%2520at%2520https%253A//github.com/OpenSparseLLMs/Skip-DiT.git.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2411.17616v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Accelerating%20Vision%20Diffusion%20Transformers%20with%20Skip%20Branches&entry.906535625=Guanjie%20Chen%20and%20Xinyu%20Zhao%20and%20Yucheng%20Zhou%20and%20Tianlong%20Chen%20and%20Cheng%20Yu&entry.1292438233=%20%20Diffusion%20Transformers%20%28DiT%29%2C%20an%20emerging%20image%20and%20video%20generation%20model%0Aarchitecture%2C%20has%20demonstrated%20great%20potential%20because%20of%20its%20high%20generation%0Aquality%20and%20scalability%20properties.%20Despite%20the%20impressive%20performance%2C%20its%0Apractical%20deployment%20is%20constrained%20by%20computational%20complexity%20and%20redundancy%0Ain%20the%20sequential%20denoising%20process.%20While%20feature%20caching%20across%20timesteps%20has%0Aproven%20effective%20in%20accelerating%20diffusion%20models%2C%20its%20application%20to%20DiT%20is%0Alimited%20by%20fundamental%20architectural%20differences%20from%20U-Net-based%20approaches.%0AThrough%20empirical%20analysis%20of%20DiT%20feature%20dynamics%2C%20we%20identify%20that%0Asignificant%20feature%20variation%20between%20DiT%20blocks%20presents%20a%20key%20challenge%20for%0Afeature%20reusability.%20To%20address%20this%2C%20we%20convert%20standard%20DiT%20into%20Skip-DiT%0Awith%20skip%20branches%20to%20enhance%20feature%20smoothness.%20Further%2C%20we%20introduce%0ASkip-Cache%20which%20utilizes%20the%20skip%20branches%20to%20cache%20DiT%20features%20across%0Atimesteps%20at%20the%20inference%20time.%20We%20validated%20effectiveness%20of%20our%20proposal%20on%0Adifferent%20DiT%20backbones%20for%20video%20and%20image%20generation%2C%20showcasing%20skip%0Abranches%20to%20help%20preserve%20generation%20quality%20and%20achieve%20higher%20speedup.%0AExperimental%20results%20indicate%20that%20Skip-DiT%20achieves%20a%201.5x%20speedup%20almost%20for%0Afree%20and%20a%202.2x%20speedup%20with%20only%20a%20minor%20reduction%20in%20quantitative%20metrics.%0ACode%20is%20available%20at%20https%3A//github.com/OpenSparseLLMs/Skip-DiT.git.%0A&entry.1838667208=http%3A//arxiv.org/abs/2411.17616v1&entry.124074799=Read"},
      ];
      const content = document.getElementById('content');
      function createPostElement(post) {
        const postElement = document.createElement('div');
        postElement.className = 'post';
        const dateElem = document.createElement('p');
        dateElem.setAttribute("class", "date");
        dateElem.textContent = post.date;
        postElement.appendChild(dateElem);

        const textElem = document.createElement('p');
        textElem.setAttribute("class", "text");
        const titleElem = document.createElement('p');
        titleElem.setAttribute("class", "title");
        titleElem.textContent = post.title;
        textElem.appendChild(titleElem);
        const authorElem = document.createElement('p');
        authorElem.setAttribute("class", "author");
        authorElem.textContent = post.author;
        textElem.appendChild(authorElem);
        const abstractElem = document.createElement('p');
        abstractElem.setAttribute("class", "abstract");
        abstractElem.textContent = post.abstract;
        textElem.appendChild(abstractElem);

        const linkElement = document.createElement('a');
        linkElement.setAttribute("class", "link");
        linkElement.href = post.link;
        linkElement.target = "_blank";
        linkElement.textContent = post.link.length > 50 ? post.link.substring(0, 50) + '...' : post.link;
        textElem.appendChild(linkElement);
        postElement.appendChild(textElem);

        const linkElementContainer = document.createElement('div');
        linkElementContainer.setAttribute("class", "comment");
        const actionElement = document.createElement('a');
        actionElement.setAttribute("class", "comment");
        actionElement.href = post.form;
        actionElement.textContent = "Action";
        actionElement.target = "_blank";
        linkElementContainer.appendChild(actionElement);
        const emailElement = document.createElement('a');
        emailElement.setAttribute("class", "comment");
        emailElement.href = post.mailto;
        emailElement.textContent = "Email";
        emailElement.target = "_blank";
        linkElementContainer.appendChild(emailElement);
        postElement.appendChild(linkElementContainer);
        const e = document.createElement('div');
        e.setAttribute("class", "clear");
        postElement.appendChild(e);

        const relevancyContainer = document.createElement('div');
        const relevancyValElem = document.createElement('p');
        relevancyValElem.textContent = "Relevancy " + post.relevancy;
        relevancyContainer.appendChild(relevancyValElem);
        post.topK.forEach((sub) => {
          const topKElem = document.createElement('a');
          topKElem.setAttribute("class", "topK");
          topKElem.href = sub.link;
          topKElem.textContent = sub.title + " (" + sub.similarity + ")";
          topKElem.target = "_blank";
          relevancyContainer.appendChild(topKElem);
        });
        postElement.appendChild(relevancyContainer);
        return postElement;
      }
      function loadPosts() {
        // Simulate loading more posts
        posts.forEach((post) => {
          const postElement = createPostElement(post);
          content.appendChild(postElement);
        });
      }
      // Load initial posts
      loadPosts();
    </script>

  </body>
</html>


