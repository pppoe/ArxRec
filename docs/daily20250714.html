<!DOCTYPE html>
<html lang="en">
  <head>
    <script async src="https://www.googletagmanager.com/gtag/js?id=G-V34CNNDP8V"></script>
    <script>
      window.dataLayer = window.dataLayer || [];
      function gtag(){dataLayer.push(arguments);}
      gtag('js', new Date());
      gtag('config', 'G-V34CNNDP8V');
    </script>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Arxiv Paper Selection</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.3/css/all.min.css" crossorigin="anonymous" referrerpolicy="no-referrer" />
    <style>
    body {
      font-family: Arial, sans-serif;
      margin: 0;
      padding: 0;
      background-color: #f4f4f4;
    }
    header {
      position: fixed;
      top: 0;
      left: 0;
      width: 100%;
      background-color: #ffffff;
      color: black;
      padding: 10px;
      text-align: center;
      z-index: 1000;
      border-bottom: 1px solid #ddd;
    }
    header div {
      display: block;
      margin: 10px auto;
    }

    #home-icon {
      display: block;
      float: left;
      margin: 5px;
      text-decoration: none;
      color: black;
    }

    main {
      margin-top: 60px; /* Adjusted margin to account for fixed header */
      padding: 20px;
    }

    .post {
      background-color: white;
      border: 1px solid #ddd;
      border-radius: 5px;
      margin-bottom: 10px;
      padding: 10px 20px;
      max-height: 2000px;
      overflow: scroll;
    }
    .post img {
      display: block;
      margin-top: 5px;
      max-width: auto;
      max-height: 100px;
    }
    .post .clear {
      clear: both;
      display: block;
    }
    .post a {
      text-decoration: none;
    }
    .post a:hover {
      color: #0056b3;
    }
    .post a:visited {
      color: #0056b3;
    }
    .post div.comment {
      text-align: right;
    }
    .post div.comment a {
      margin: 1em;
    }
    .post .text {
      margin: 1em 0em;
      padding: 0;
    }
    .post .text .title {
    }
    .post .text .author {
    }
    .post .text .abstract {
    }
    .post .topK {
      display: block;
      margin: 0.5em;
    }
    .post .date {
      margin: 0;
      padding: 0;
      text-size: small; 
      color: gray;
    }
    .post .link {
      margin: 0;
      padding: 0;
    }
    @media screen and (max-width: 600px) {
      body {
        max-width: 100%; 
      }
      #home-icon {
        float: none;
        display: block;
        text-align: center;
        margin-bottom: 10px;
      }
    }
    footer {
      width: 100%;
      background-color: #ddd;
      text-align: center;
      z-index: 1000;
      padding: 20px 0px;
      margin-bottom: 20px;
      left: 0;
    }

    #next-btn,
    #prev-btn {
      background-color: #4CAF50;
      color: white;
      padding: 8px 16px;
      margin: 0 50px;
      border: none;
      border-radius: 4px;
      cursor: pointer;
    }

    .links {
      padding: 20px;
    }
    .links a {
      text-decoration: none;
    }
    .links a:hover {
      color: #0056b3;
    }
    .links a:visited {
      color: #0056b3;
    }

    #page-index {
      font-size: small;
    }
    .ads {
      width: 100%;
    }
    #prev-btn {
      background-color: #4CAF50;
      color: white;
      padding: 8px 16px;
      margin: 0 50px;
      border: none;
      border-radius: 4px;
      cursor: pointer;
    }
    </style>
  </head>
  <body>

    <header>
      <a id="prev-btn" href="daily20250713.html"><i class="fas fa-chevron-left"></i></a>
      <a href="https://haoxiang.org/">About</a>
    </header>

    <main id="content">
      <!-- Posts will be dynamically added here using JavaScript -->
    </main>

    <script>
      // Dummy data for posts
      const posts = [
{"title": "X-Dancer: Expressive Music to Human Dance Video Generation", "author": "Zeyuan Chen and Hongyi Xu and Guoxian Song and You Xie and Chenxu Zhang and Xin Chen and Chao Wang and Di Chang and Linjie Luo", "abstract": "  We present X-Dancer, a novel zero-shot music-driven image animation pipeline\nthat creates diverse and long-range lifelike human dance videos from a single\nstatic image. As its core, we introduce a unified transformer-diffusion\nframework, featuring an autoregressive transformer model that synthesize\nextended and music-synchronized token sequences for 2D body, head and hands\nposes, which then guide a diffusion model to produce coherent and realistic\ndance video frames. Unlike traditional methods that primarily generate human\nmotion in 3D, X-Dancer addresses data limitations and enhances scalability by\nmodeling a wide spectrum of 2D dance motions, capturing their nuanced alignment\nwith musical beats through readily available monocular videos. To achieve this,\nwe first build a spatially compositional token representation from 2D human\npose labels associated with keypoint confidences, encoding both large\narticulated body movements (e.g., upper and lower body) and fine-grained\nmotions (e.g., head and hands). We then design a music-to-motion transformer\nmodel that autoregressively generates music-aligned dance pose token sequences,\nincorporating global attention to both musical style and prior motion context.\nFinally we leverage a diffusion backbone to animate the reference image with\nthese synthesized pose tokens through AdaIN, forming a fully differentiable\nend-to-end framework. Experimental results demonstrate that X-Dancer is able to\nproduce both diverse and characterized dance videos, substantially\noutperforming state-of-the-art methods in term of diversity, expressiveness and\nrealism. Code and model will be available for research purposes.\n", "link": "http://arxiv.org/abs/2502.17414v2", "date": "2025-07-11", "relevancy": 3.2437, "topK": [{"title": "Synthesizing Moving People with 3D Control", "link": "http://arxiv.org/abs/2401.10889v2", "similarity": 0.6988}, {"title": "PoseAnimate: Zero-shot high fidelity pose controllable character\n  animation", "link": "http://arxiv.org/abs/2404.13680v2", "similarity": 0.6304}, {"title": "Magic-Me: Identity-Specific Video Customized Diffusion", "link": "http://arxiv.org/abs/2402.09368v1", "similarity": 0.6171}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20X-Dancer%3A%20Expressive%20Music%20to%20Human%20Dance%20Video%20Generation&body=Title%3A%20X-Dancer%3A%20Expressive%20Music%20to%20Human%20Dance%20Video%20Generation%0AAuthor%3A%20Zeyuan%20Chen%20and%20Hongyi%20Xu%20and%20Guoxian%20Song%20and%20You%20Xie%20and%20Chenxu%20Zhang%20and%20Xin%20Chen%20and%20Chao%20Wang%20and%20Di%20Chang%20and%20Linjie%20Luo%0AAbstract%3A%20%20%20We%20present%20X-Dancer%2C%20a%20novel%20zero-shot%20music-driven%20image%20animation%20pipeline%0Athat%20creates%20diverse%20and%20long-range%20lifelike%20human%20dance%20videos%20from%20a%20single%0Astatic%20image.%20As%20its%20core%2C%20we%20introduce%20a%20unified%20transformer-diffusion%0Aframework%2C%20featuring%20an%20autoregressive%20transformer%20model%20that%20synthesize%0Aextended%20and%20music-synchronized%20token%20sequences%20for%202D%20body%2C%20head%20and%20hands%0Aposes%2C%20which%20then%20guide%20a%20diffusion%20model%20to%20produce%20coherent%20and%20realistic%0Adance%20video%20frames.%20Unlike%20traditional%20methods%20that%20primarily%20generate%20human%0Amotion%20in%203D%2C%20X-Dancer%20addresses%20data%20limitations%20and%20enhances%20scalability%20by%0Amodeling%20a%20wide%20spectrum%20of%202D%20dance%20motions%2C%20capturing%20their%20nuanced%20alignment%0Awith%20musical%20beats%20through%20readily%20available%20monocular%20videos.%20To%20achieve%20this%2C%0Awe%20first%20build%20a%20spatially%20compositional%20token%20representation%20from%202D%20human%0Apose%20labels%20associated%20with%20keypoint%20confidences%2C%20encoding%20both%20large%0Aarticulated%20body%20movements%20%28e.g.%2C%20upper%20and%20lower%20body%29%20and%20fine-grained%0Amotions%20%28e.g.%2C%20head%20and%20hands%29.%20We%20then%20design%20a%20music-to-motion%20transformer%0Amodel%20that%20autoregressively%20generates%20music-aligned%20dance%20pose%20token%20sequences%2C%0Aincorporating%20global%20attention%20to%20both%20musical%20style%20and%20prior%20motion%20context.%0AFinally%20we%20leverage%20a%20diffusion%20backbone%20to%20animate%20the%20reference%20image%20with%0Athese%20synthesized%20pose%20tokens%20through%20AdaIN%2C%20forming%20a%20fully%20differentiable%0Aend-to-end%20framework.%20Experimental%20results%20demonstrate%20that%20X-Dancer%20is%20able%20to%0Aproduce%20both%20diverse%20and%20characterized%20dance%20videos%2C%20substantially%0Aoutperforming%20state-of-the-art%20methods%20in%20term%20of%20diversity%2C%20expressiveness%20and%0Arealism.%20Code%20and%20model%20will%20be%20available%20for%20research%20purposes.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2502.17414v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DX-Dancer%253A%2520Expressive%2520Music%2520to%2520Human%2520Dance%2520Video%2520Generation%26entry.906535625%3DZeyuan%2520Chen%2520and%2520Hongyi%2520Xu%2520and%2520Guoxian%2520Song%2520and%2520You%2520Xie%2520and%2520Chenxu%2520Zhang%2520and%2520Xin%2520Chen%2520and%2520Chao%2520Wang%2520and%2520Di%2520Chang%2520and%2520Linjie%2520Luo%26entry.1292438233%3D%2520%2520We%2520present%2520X-Dancer%252C%2520a%2520novel%2520zero-shot%2520music-driven%2520image%2520animation%2520pipeline%250Athat%2520creates%2520diverse%2520and%2520long-range%2520lifelike%2520human%2520dance%2520videos%2520from%2520a%2520single%250Astatic%2520image.%2520As%2520its%2520core%252C%2520we%2520introduce%2520a%2520unified%2520transformer-diffusion%250Aframework%252C%2520featuring%2520an%2520autoregressive%2520transformer%2520model%2520that%2520synthesize%250Aextended%2520and%2520music-synchronized%2520token%2520sequences%2520for%25202D%2520body%252C%2520head%2520and%2520hands%250Aposes%252C%2520which%2520then%2520guide%2520a%2520diffusion%2520model%2520to%2520produce%2520coherent%2520and%2520realistic%250Adance%2520video%2520frames.%2520Unlike%2520traditional%2520methods%2520that%2520primarily%2520generate%2520human%250Amotion%2520in%25203D%252C%2520X-Dancer%2520addresses%2520data%2520limitations%2520and%2520enhances%2520scalability%2520by%250Amodeling%2520a%2520wide%2520spectrum%2520of%25202D%2520dance%2520motions%252C%2520capturing%2520their%2520nuanced%2520alignment%250Awith%2520musical%2520beats%2520through%2520readily%2520available%2520monocular%2520videos.%2520To%2520achieve%2520this%252C%250Awe%2520first%2520build%2520a%2520spatially%2520compositional%2520token%2520representation%2520from%25202D%2520human%250Apose%2520labels%2520associated%2520with%2520keypoint%2520confidences%252C%2520encoding%2520both%2520large%250Aarticulated%2520body%2520movements%2520%2528e.g.%252C%2520upper%2520and%2520lower%2520body%2529%2520and%2520fine-grained%250Amotions%2520%2528e.g.%252C%2520head%2520and%2520hands%2529.%2520We%2520then%2520design%2520a%2520music-to-motion%2520transformer%250Amodel%2520that%2520autoregressively%2520generates%2520music-aligned%2520dance%2520pose%2520token%2520sequences%252C%250Aincorporating%2520global%2520attention%2520to%2520both%2520musical%2520style%2520and%2520prior%2520motion%2520context.%250AFinally%2520we%2520leverage%2520a%2520diffusion%2520backbone%2520to%2520animate%2520the%2520reference%2520image%2520with%250Athese%2520synthesized%2520pose%2520tokens%2520through%2520AdaIN%252C%2520forming%2520a%2520fully%2520differentiable%250Aend-to-end%2520framework.%2520Experimental%2520results%2520demonstrate%2520that%2520X-Dancer%2520is%2520able%2520to%250Aproduce%2520both%2520diverse%2520and%2520characterized%2520dance%2520videos%252C%2520substantially%250Aoutperforming%2520state-of-the-art%2520methods%2520in%2520term%2520of%2520diversity%252C%2520expressiveness%2520and%250Arealism.%2520Code%2520and%2520model%2520will%2520be%2520available%2520for%2520research%2520purposes.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2502.17414v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=X-Dancer%3A%20Expressive%20Music%20to%20Human%20Dance%20Video%20Generation&entry.906535625=Zeyuan%20Chen%20and%20Hongyi%20Xu%20and%20Guoxian%20Song%20and%20You%20Xie%20and%20Chenxu%20Zhang%20and%20Xin%20Chen%20and%20Chao%20Wang%20and%20Di%20Chang%20and%20Linjie%20Luo&entry.1292438233=%20%20We%20present%20X-Dancer%2C%20a%20novel%20zero-shot%20music-driven%20image%20animation%20pipeline%0Athat%20creates%20diverse%20and%20long-range%20lifelike%20human%20dance%20videos%20from%20a%20single%0Astatic%20image.%20As%20its%20core%2C%20we%20introduce%20a%20unified%20transformer-diffusion%0Aframework%2C%20featuring%20an%20autoregressive%20transformer%20model%20that%20synthesize%0Aextended%20and%20music-synchronized%20token%20sequences%20for%202D%20body%2C%20head%20and%20hands%0Aposes%2C%20which%20then%20guide%20a%20diffusion%20model%20to%20produce%20coherent%20and%20realistic%0Adance%20video%20frames.%20Unlike%20traditional%20methods%20that%20primarily%20generate%20human%0Amotion%20in%203D%2C%20X-Dancer%20addresses%20data%20limitations%20and%20enhances%20scalability%20by%0Amodeling%20a%20wide%20spectrum%20of%202D%20dance%20motions%2C%20capturing%20their%20nuanced%20alignment%0Awith%20musical%20beats%20through%20readily%20available%20monocular%20videos.%20To%20achieve%20this%2C%0Awe%20first%20build%20a%20spatially%20compositional%20token%20representation%20from%202D%20human%0Apose%20labels%20associated%20with%20keypoint%20confidences%2C%20encoding%20both%20large%0Aarticulated%20body%20movements%20%28e.g.%2C%20upper%20and%20lower%20body%29%20and%20fine-grained%0Amotions%20%28e.g.%2C%20head%20and%20hands%29.%20We%20then%20design%20a%20music-to-motion%20transformer%0Amodel%20that%20autoregressively%20generates%20music-aligned%20dance%20pose%20token%20sequences%2C%0Aincorporating%20global%20attention%20to%20both%20musical%20style%20and%20prior%20motion%20context.%0AFinally%20we%20leverage%20a%20diffusion%20backbone%20to%20animate%20the%20reference%20image%20with%0Athese%20synthesized%20pose%20tokens%20through%20AdaIN%2C%20forming%20a%20fully%20differentiable%0Aend-to-end%20framework.%20Experimental%20results%20demonstrate%20that%20X-Dancer%20is%20able%20to%0Aproduce%20both%20diverse%20and%20characterized%20dance%20videos%2C%20substantially%0Aoutperforming%20state-of-the-art%20methods%20in%20term%20of%20diversity%2C%20expressiveness%20and%0Arealism.%20Code%20and%20model%20will%20be%20available%20for%20research%20purposes.%0A&entry.1838667208=http%3A//arxiv.org/abs/2502.17414v2&entry.124074799=Read"},
{"title": "SpatialCrafter: Unleashing the Imagination of Video Diffusion Models for\n  Scene Reconstruction from Limited Observations", "author": "Songchun Zhang and Huiyao Xu and Sitong Guo and Zhongwei Xie and Hujun Bao and Weiwei Xu and Changqing Zou", "abstract": "  Novel view synthesis (NVS) boosts immersive experiences in computer vision\nand graphics. Existing techniques, though progressed, rely on dense multi-view\nobservations, restricting their application. This work takes on the challenge\nof reconstructing photorealistic 3D scenes from sparse or single-view inputs.\nWe introduce SpatialCrafter, a framework that leverages the rich knowledge in\nvideo diffusion models to generate plausible additional observations, thereby\nalleviating reconstruction ambiguity. Through a trainable camera encoder and an\nepipolar attention mechanism for explicit geometric constraints, we achieve\nprecise camera control and 3D consistency, further reinforced by a unified\nscale estimation strategy to handle scale discrepancies across datasets.\nFurthermore, by integrating monocular depth priors with semantic features in\nthe video latent space, our framework directly regresses 3D Gaussian primitives\nand efficiently processes long-sequence features using a hybrid network\nstructure. Extensive experiments show our method enhances sparse view\nreconstruction and restores the realistic appearance of 3D scenes.\n", "link": "http://arxiv.org/abs/2505.11992v2", "date": "2025-07-11", "relevancy": 2.7955, "topK": [{"title": "CamCtrl3D: Single-Image Scene Exploration with Precise 3D Camera Control", "link": "http://arxiv.org/abs/2501.06006v1", "similarity": 0.7078}, {"title": "CAT3D: Create Anything in 3D with Multi-View Diffusion Models", "link": "http://arxiv.org/abs/2405.10314v1", "similarity": 0.6971}, {"title": "CAT3D: Create Anything in 3D with Multi-View Diffusion Models", "link": "http://arxiv.org/abs/2405.10314v1", "similarity": 0.6971}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20SpatialCrafter%3A%20Unleashing%20the%20Imagination%20of%20Video%20Diffusion%20Models%20for%0A%20%20Scene%20Reconstruction%20from%20Limited%20Observations&body=Title%3A%20SpatialCrafter%3A%20Unleashing%20the%20Imagination%20of%20Video%20Diffusion%20Models%20for%0A%20%20Scene%20Reconstruction%20from%20Limited%20Observations%0AAuthor%3A%20Songchun%20Zhang%20and%20Huiyao%20Xu%20and%20Sitong%20Guo%20and%20Zhongwei%20Xie%20and%20Hujun%20Bao%20and%20Weiwei%20Xu%20and%20Changqing%20Zou%0AAbstract%3A%20%20%20Novel%20view%20synthesis%20%28NVS%29%20boosts%20immersive%20experiences%20in%20computer%20vision%0Aand%20graphics.%20Existing%20techniques%2C%20though%20progressed%2C%20rely%20on%20dense%20multi-view%0Aobservations%2C%20restricting%20their%20application.%20This%20work%20takes%20on%20the%20challenge%0Aof%20reconstructing%20photorealistic%203D%20scenes%20from%20sparse%20or%20single-view%20inputs.%0AWe%20introduce%20SpatialCrafter%2C%20a%20framework%20that%20leverages%20the%20rich%20knowledge%20in%0Avideo%20diffusion%20models%20to%20generate%20plausible%20additional%20observations%2C%20thereby%0Aalleviating%20reconstruction%20ambiguity.%20Through%20a%20trainable%20camera%20encoder%20and%20an%0Aepipolar%20attention%20mechanism%20for%20explicit%20geometric%20constraints%2C%20we%20achieve%0Aprecise%20camera%20control%20and%203D%20consistency%2C%20further%20reinforced%20by%20a%20unified%0Ascale%20estimation%20strategy%20to%20handle%20scale%20discrepancies%20across%20datasets.%0AFurthermore%2C%20by%20integrating%20monocular%20depth%20priors%20with%20semantic%20features%20in%0Athe%20video%20latent%20space%2C%20our%20framework%20directly%20regresses%203D%20Gaussian%20primitives%0Aand%20efficiently%20processes%20long-sequence%20features%20using%20a%20hybrid%20network%0Astructure.%20Extensive%20experiments%20show%20our%20method%20enhances%20sparse%20view%0Areconstruction%20and%20restores%20the%20realistic%20appearance%20of%203D%20scenes.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2505.11992v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DSpatialCrafter%253A%2520Unleashing%2520the%2520Imagination%2520of%2520Video%2520Diffusion%2520Models%2520for%250A%2520%2520Scene%2520Reconstruction%2520from%2520Limited%2520Observations%26entry.906535625%3DSongchun%2520Zhang%2520and%2520Huiyao%2520Xu%2520and%2520Sitong%2520Guo%2520and%2520Zhongwei%2520Xie%2520and%2520Hujun%2520Bao%2520and%2520Weiwei%2520Xu%2520and%2520Changqing%2520Zou%26entry.1292438233%3D%2520%2520Novel%2520view%2520synthesis%2520%2528NVS%2529%2520boosts%2520immersive%2520experiences%2520in%2520computer%2520vision%250Aand%2520graphics.%2520Existing%2520techniques%252C%2520though%2520progressed%252C%2520rely%2520on%2520dense%2520multi-view%250Aobservations%252C%2520restricting%2520their%2520application.%2520This%2520work%2520takes%2520on%2520the%2520challenge%250Aof%2520reconstructing%2520photorealistic%25203D%2520scenes%2520from%2520sparse%2520or%2520single-view%2520inputs.%250AWe%2520introduce%2520SpatialCrafter%252C%2520a%2520framework%2520that%2520leverages%2520the%2520rich%2520knowledge%2520in%250Avideo%2520diffusion%2520models%2520to%2520generate%2520plausible%2520additional%2520observations%252C%2520thereby%250Aalleviating%2520reconstruction%2520ambiguity.%2520Through%2520a%2520trainable%2520camera%2520encoder%2520and%2520an%250Aepipolar%2520attention%2520mechanism%2520for%2520explicit%2520geometric%2520constraints%252C%2520we%2520achieve%250Aprecise%2520camera%2520control%2520and%25203D%2520consistency%252C%2520further%2520reinforced%2520by%2520a%2520unified%250Ascale%2520estimation%2520strategy%2520to%2520handle%2520scale%2520discrepancies%2520across%2520datasets.%250AFurthermore%252C%2520by%2520integrating%2520monocular%2520depth%2520priors%2520with%2520semantic%2520features%2520in%250Athe%2520video%2520latent%2520space%252C%2520our%2520framework%2520directly%2520regresses%25203D%2520Gaussian%2520primitives%250Aand%2520efficiently%2520processes%2520long-sequence%2520features%2520using%2520a%2520hybrid%2520network%250Astructure.%2520Extensive%2520experiments%2520show%2520our%2520method%2520enhances%2520sparse%2520view%250Areconstruction%2520and%2520restores%2520the%2520realistic%2520appearance%2520of%25203D%2520scenes.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2505.11992v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=SpatialCrafter%3A%20Unleashing%20the%20Imagination%20of%20Video%20Diffusion%20Models%20for%0A%20%20Scene%20Reconstruction%20from%20Limited%20Observations&entry.906535625=Songchun%20Zhang%20and%20Huiyao%20Xu%20and%20Sitong%20Guo%20and%20Zhongwei%20Xie%20and%20Hujun%20Bao%20and%20Weiwei%20Xu%20and%20Changqing%20Zou&entry.1292438233=%20%20Novel%20view%20synthesis%20%28NVS%29%20boosts%20immersive%20experiences%20in%20computer%20vision%0Aand%20graphics.%20Existing%20techniques%2C%20though%20progressed%2C%20rely%20on%20dense%20multi-view%0Aobservations%2C%20restricting%20their%20application.%20This%20work%20takes%20on%20the%20challenge%0Aof%20reconstructing%20photorealistic%203D%20scenes%20from%20sparse%20or%20single-view%20inputs.%0AWe%20introduce%20SpatialCrafter%2C%20a%20framework%20that%20leverages%20the%20rich%20knowledge%20in%0Avideo%20diffusion%20models%20to%20generate%20plausible%20additional%20observations%2C%20thereby%0Aalleviating%20reconstruction%20ambiguity.%20Through%20a%20trainable%20camera%20encoder%20and%20an%0Aepipolar%20attention%20mechanism%20for%20explicit%20geometric%20constraints%2C%20we%20achieve%0Aprecise%20camera%20control%20and%203D%20consistency%2C%20further%20reinforced%20by%20a%20unified%0Ascale%20estimation%20strategy%20to%20handle%20scale%20discrepancies%20across%20datasets.%0AFurthermore%2C%20by%20integrating%20monocular%20depth%20priors%20with%20semantic%20features%20in%0Athe%20video%20latent%20space%2C%20our%20framework%20directly%20regresses%203D%20Gaussian%20primitives%0Aand%20efficiently%20processes%20long-sequence%20features%20using%20a%20hybrid%20network%0Astructure.%20Extensive%20experiments%20show%20our%20method%20enhances%20sparse%20view%0Areconstruction%20and%20restores%20the%20realistic%20appearance%20of%203D%20scenes.%0A&entry.1838667208=http%3A//arxiv.org/abs/2505.11992v2&entry.124074799=Read"},
{"title": "Feature Learning beyond the Lazy-Rich Dichotomy: Insights from\n  Representational Geometry", "author": "Chi-Ning Chou and Hang Le and Yichen Wang and SueYeon Chung", "abstract": "  Integrating task-relevant information into neural representations is a\nfundamental ability of both biological and artificial intelligence systems.\nRecent theories have categorized learning into two regimes: the rich regime,\nwhere neural networks actively learn task-relevant features, and the lazy\nregime, where networks behave like random feature models. Yet this simple\nlazy-rich dichotomy overlooks a diverse underlying taxonomy of feature\nlearning, shaped by differences in learning algorithms, network architectures,\nand data properties. To address this gap, we introduce an analysis framework to\nstudy feature learning via the geometry of neural representations. Rather than\ninspecting individual learned features, we characterize how task-relevant\nrepresentational manifolds evolve throughout the learning process. We show, in\nboth theoretical and empirical settings, that as networks learn features,\ntask-relevant manifolds untangle, with changes in manifold geometry revealing\ndistinct learning stages and strategies beyond the lazy-rich dichotomy. This\nframework provides novel insights into feature learning across neuroscience and\nmachine learning, shedding light on structural inductive biases in neural\ncircuits and the mechanisms underlying out-of-distribution generalization.\n", "link": "http://arxiv.org/abs/2503.18114v2", "date": "2025-07-11", "relevancy": 2.7428, "topK": [{"title": "Self-supervised Photographic Image Layout Representation Learning", "link": "http://arxiv.org/abs/2403.03740v1", "similarity": 0.5918}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5273}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5266}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Feature%20Learning%20beyond%20the%20Lazy-Rich%20Dichotomy%3A%20Insights%20from%0A%20%20Representational%20Geometry&body=Title%3A%20Feature%20Learning%20beyond%20the%20Lazy-Rich%20Dichotomy%3A%20Insights%20from%0A%20%20Representational%20Geometry%0AAuthor%3A%20Chi-Ning%20Chou%20and%20Hang%20Le%20and%20Yichen%20Wang%20and%20SueYeon%20Chung%0AAbstract%3A%20%20%20Integrating%20task-relevant%20information%20into%20neural%20representations%20is%20a%0Afundamental%20ability%20of%20both%20biological%20and%20artificial%20intelligence%20systems.%0ARecent%20theories%20have%20categorized%20learning%20into%20two%20regimes%3A%20the%20rich%20regime%2C%0Awhere%20neural%20networks%20actively%20learn%20task-relevant%20features%2C%20and%20the%20lazy%0Aregime%2C%20where%20networks%20behave%20like%20random%20feature%20models.%20Yet%20this%20simple%0Alazy-rich%20dichotomy%20overlooks%20a%20diverse%20underlying%20taxonomy%20of%20feature%0Alearning%2C%20shaped%20by%20differences%20in%20learning%20algorithms%2C%20network%20architectures%2C%0Aand%20data%20properties.%20To%20address%20this%20gap%2C%20we%20introduce%20an%20analysis%20framework%20to%0Astudy%20feature%20learning%20via%20the%20geometry%20of%20neural%20representations.%20Rather%20than%0Ainspecting%20individual%20learned%20features%2C%20we%20characterize%20how%20task-relevant%0Arepresentational%20manifolds%20evolve%20throughout%20the%20learning%20process.%20We%20show%2C%20in%0Aboth%20theoretical%20and%20empirical%20settings%2C%20that%20as%20networks%20learn%20features%2C%0Atask-relevant%20manifolds%20untangle%2C%20with%20changes%20in%20manifold%20geometry%20revealing%0Adistinct%20learning%20stages%20and%20strategies%20beyond%20the%20lazy-rich%20dichotomy.%20This%0Aframework%20provides%20novel%20insights%20into%20feature%20learning%20across%20neuroscience%20and%0Amachine%20learning%2C%20shedding%20light%20on%20structural%20inductive%20biases%20in%20neural%0Acircuits%20and%20the%20mechanisms%20underlying%20out-of-distribution%20generalization.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2503.18114v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DFeature%2520Learning%2520beyond%2520the%2520Lazy-Rich%2520Dichotomy%253A%2520Insights%2520from%250A%2520%2520Representational%2520Geometry%26entry.906535625%3DChi-Ning%2520Chou%2520and%2520Hang%2520Le%2520and%2520Yichen%2520Wang%2520and%2520SueYeon%2520Chung%26entry.1292438233%3D%2520%2520Integrating%2520task-relevant%2520information%2520into%2520neural%2520representations%2520is%2520a%250Afundamental%2520ability%2520of%2520both%2520biological%2520and%2520artificial%2520intelligence%2520systems.%250ARecent%2520theories%2520have%2520categorized%2520learning%2520into%2520two%2520regimes%253A%2520the%2520rich%2520regime%252C%250Awhere%2520neural%2520networks%2520actively%2520learn%2520task-relevant%2520features%252C%2520and%2520the%2520lazy%250Aregime%252C%2520where%2520networks%2520behave%2520like%2520random%2520feature%2520models.%2520Yet%2520this%2520simple%250Alazy-rich%2520dichotomy%2520overlooks%2520a%2520diverse%2520underlying%2520taxonomy%2520of%2520feature%250Alearning%252C%2520shaped%2520by%2520differences%2520in%2520learning%2520algorithms%252C%2520network%2520architectures%252C%250Aand%2520data%2520properties.%2520To%2520address%2520this%2520gap%252C%2520we%2520introduce%2520an%2520analysis%2520framework%2520to%250Astudy%2520feature%2520learning%2520via%2520the%2520geometry%2520of%2520neural%2520representations.%2520Rather%2520than%250Ainspecting%2520individual%2520learned%2520features%252C%2520we%2520characterize%2520how%2520task-relevant%250Arepresentational%2520manifolds%2520evolve%2520throughout%2520the%2520learning%2520process.%2520We%2520show%252C%2520in%250Aboth%2520theoretical%2520and%2520empirical%2520settings%252C%2520that%2520as%2520networks%2520learn%2520features%252C%250Atask-relevant%2520manifolds%2520untangle%252C%2520with%2520changes%2520in%2520manifold%2520geometry%2520revealing%250Adistinct%2520learning%2520stages%2520and%2520strategies%2520beyond%2520the%2520lazy-rich%2520dichotomy.%2520This%250Aframework%2520provides%2520novel%2520insights%2520into%2520feature%2520learning%2520across%2520neuroscience%2520and%250Amachine%2520learning%252C%2520shedding%2520light%2520on%2520structural%2520inductive%2520biases%2520in%2520neural%250Acircuits%2520and%2520the%2520mechanisms%2520underlying%2520out-of-distribution%2520generalization.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2503.18114v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Feature%20Learning%20beyond%20the%20Lazy-Rich%20Dichotomy%3A%20Insights%20from%0A%20%20Representational%20Geometry&entry.906535625=Chi-Ning%20Chou%20and%20Hang%20Le%20and%20Yichen%20Wang%20and%20SueYeon%20Chung&entry.1292438233=%20%20Integrating%20task-relevant%20information%20into%20neural%20representations%20is%20a%0Afundamental%20ability%20of%20both%20biological%20and%20artificial%20intelligence%20systems.%0ARecent%20theories%20have%20categorized%20learning%20into%20two%20regimes%3A%20the%20rich%20regime%2C%0Awhere%20neural%20networks%20actively%20learn%20task-relevant%20features%2C%20and%20the%20lazy%0Aregime%2C%20where%20networks%20behave%20like%20random%20feature%20models.%20Yet%20this%20simple%0Alazy-rich%20dichotomy%20overlooks%20a%20diverse%20underlying%20taxonomy%20of%20feature%0Alearning%2C%20shaped%20by%20differences%20in%20learning%20algorithms%2C%20network%20architectures%2C%0Aand%20data%20properties.%20To%20address%20this%20gap%2C%20we%20introduce%20an%20analysis%20framework%20to%0Astudy%20feature%20learning%20via%20the%20geometry%20of%20neural%20representations.%20Rather%20than%0Ainspecting%20individual%20learned%20features%2C%20we%20characterize%20how%20task-relevant%0Arepresentational%20manifolds%20evolve%20throughout%20the%20learning%20process.%20We%20show%2C%20in%0Aboth%20theoretical%20and%20empirical%20settings%2C%20that%20as%20networks%20learn%20features%2C%0Atask-relevant%20manifolds%20untangle%2C%20with%20changes%20in%20manifold%20geometry%20revealing%0Adistinct%20learning%20stages%20and%20strategies%20beyond%20the%20lazy-rich%20dichotomy.%20This%0Aframework%20provides%20novel%20insights%20into%20feature%20learning%20across%20neuroscience%20and%0Amachine%20learning%2C%20shedding%20light%20on%20structural%20inductive%20biases%20in%20neural%0Acircuits%20and%20the%20mechanisms%20underlying%20out-of-distribution%20generalization.%0A&entry.1838667208=http%3A//arxiv.org/abs/2503.18114v2&entry.124074799=Read"},
{"title": "ByDeWay: Boost Your multimodal LLM with DEpth prompting in a\n  Training-Free Way", "author": "Rajarshi Roy and Devleena Das and Ankesh Banerjee and Arjya Bhattacharjee and Kousik Dasgupta and Subarna Tripathi", "abstract": "  We introduce ByDeWay, a training-free framework designed to enhance the\nperformance of Multimodal Large Language Models (MLLMs). ByDeWay uses a novel\nprompting strategy called Layered-Depth-Based Prompting (LDP), which improves\nspatial reasoning and grounding without modifying any model parameters. It\nsegments the scene into closest, mid-range, and farthest layers using monocular\ndepth estimation, then generates region-specific captions with a grounded\nvision-language model. These structured, depth-aware captions are appended to\nthe image-question prompt, enriching it with spatial context. This guides MLLMs\nto produce more grounded and less hallucinated responses. Our method is\nlightweight, modular, and compatible with black-box MLLMs. Experiments on\nhallucination-sensitive (POPE) and reasoning-intensive (GQA) benchmarks show\nconsistent improvements across multiple MLLMs, validating the effectiveness of\ndepth-aware prompting in a zero-training setting.\n", "link": "http://arxiv.org/abs/2507.08679v1", "date": "2025-07-11", "relevancy": 2.7295, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5569}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5569}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5239}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20ByDeWay%3A%20Boost%20Your%20multimodal%20LLM%20with%20DEpth%20prompting%20in%20a%0A%20%20Training-Free%20Way&body=Title%3A%20ByDeWay%3A%20Boost%20Your%20multimodal%20LLM%20with%20DEpth%20prompting%20in%20a%0A%20%20Training-Free%20Way%0AAuthor%3A%20Rajarshi%20Roy%20and%20Devleena%20Das%20and%20Ankesh%20Banerjee%20and%20Arjya%20Bhattacharjee%20and%20Kousik%20Dasgupta%20and%20Subarna%20Tripathi%0AAbstract%3A%20%20%20We%20introduce%20ByDeWay%2C%20a%20training-free%20framework%20designed%20to%20enhance%20the%0Aperformance%20of%20Multimodal%20Large%20Language%20Models%20%28MLLMs%29.%20ByDeWay%20uses%20a%20novel%0Aprompting%20strategy%20called%20Layered-Depth-Based%20Prompting%20%28LDP%29%2C%20which%20improves%0Aspatial%20reasoning%20and%20grounding%20without%20modifying%20any%20model%20parameters.%20It%0Asegments%20the%20scene%20into%20closest%2C%20mid-range%2C%20and%20farthest%20layers%20using%20monocular%0Adepth%20estimation%2C%20then%20generates%20region-specific%20captions%20with%20a%20grounded%0Avision-language%20model.%20These%20structured%2C%20depth-aware%20captions%20are%20appended%20to%0Athe%20image-question%20prompt%2C%20enriching%20it%20with%20spatial%20context.%20This%20guides%20MLLMs%0Ato%20produce%20more%20grounded%20and%20less%20hallucinated%20responses.%20Our%20method%20is%0Alightweight%2C%20modular%2C%20and%20compatible%20with%20black-box%20MLLMs.%20Experiments%20on%0Ahallucination-sensitive%20%28POPE%29%20and%20reasoning-intensive%20%28GQA%29%20benchmarks%20show%0Aconsistent%20improvements%20across%20multiple%20MLLMs%2C%20validating%20the%20effectiveness%20of%0Adepth-aware%20prompting%20in%20a%20zero-training%20setting.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2507.08679v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DByDeWay%253A%2520Boost%2520Your%2520multimodal%2520LLM%2520with%2520DEpth%2520prompting%2520in%2520a%250A%2520%2520Training-Free%2520Way%26entry.906535625%3DRajarshi%2520Roy%2520and%2520Devleena%2520Das%2520and%2520Ankesh%2520Banerjee%2520and%2520Arjya%2520Bhattacharjee%2520and%2520Kousik%2520Dasgupta%2520and%2520Subarna%2520Tripathi%26entry.1292438233%3D%2520%2520We%2520introduce%2520ByDeWay%252C%2520a%2520training-free%2520framework%2520designed%2520to%2520enhance%2520the%250Aperformance%2520of%2520Multimodal%2520Large%2520Language%2520Models%2520%2528MLLMs%2529.%2520ByDeWay%2520uses%2520a%2520novel%250Aprompting%2520strategy%2520called%2520Layered-Depth-Based%2520Prompting%2520%2528LDP%2529%252C%2520which%2520improves%250Aspatial%2520reasoning%2520and%2520grounding%2520without%2520modifying%2520any%2520model%2520parameters.%2520It%250Asegments%2520the%2520scene%2520into%2520closest%252C%2520mid-range%252C%2520and%2520farthest%2520layers%2520using%2520monocular%250Adepth%2520estimation%252C%2520then%2520generates%2520region-specific%2520captions%2520with%2520a%2520grounded%250Avision-language%2520model.%2520These%2520structured%252C%2520depth-aware%2520captions%2520are%2520appended%2520to%250Athe%2520image-question%2520prompt%252C%2520enriching%2520it%2520with%2520spatial%2520context.%2520This%2520guides%2520MLLMs%250Ato%2520produce%2520more%2520grounded%2520and%2520less%2520hallucinated%2520responses.%2520Our%2520method%2520is%250Alightweight%252C%2520modular%252C%2520and%2520compatible%2520with%2520black-box%2520MLLMs.%2520Experiments%2520on%250Ahallucination-sensitive%2520%2528POPE%2529%2520and%2520reasoning-intensive%2520%2528GQA%2529%2520benchmarks%2520show%250Aconsistent%2520improvements%2520across%2520multiple%2520MLLMs%252C%2520validating%2520the%2520effectiveness%2520of%250Adepth-aware%2520prompting%2520in%2520a%2520zero-training%2520setting.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2507.08679v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=ByDeWay%3A%20Boost%20Your%20multimodal%20LLM%20with%20DEpth%20prompting%20in%20a%0A%20%20Training-Free%20Way&entry.906535625=Rajarshi%20Roy%20and%20Devleena%20Das%20and%20Ankesh%20Banerjee%20and%20Arjya%20Bhattacharjee%20and%20Kousik%20Dasgupta%20and%20Subarna%20Tripathi&entry.1292438233=%20%20We%20introduce%20ByDeWay%2C%20a%20training-free%20framework%20designed%20to%20enhance%20the%0Aperformance%20of%20Multimodal%20Large%20Language%20Models%20%28MLLMs%29.%20ByDeWay%20uses%20a%20novel%0Aprompting%20strategy%20called%20Layered-Depth-Based%20Prompting%20%28LDP%29%2C%20which%20improves%0Aspatial%20reasoning%20and%20grounding%20without%20modifying%20any%20model%20parameters.%20It%0Asegments%20the%20scene%20into%20closest%2C%20mid-range%2C%20and%20farthest%20layers%20using%20monocular%0Adepth%20estimation%2C%20then%20generates%20region-specific%20captions%20with%20a%20grounded%0Avision-language%20model.%20These%20structured%2C%20depth-aware%20captions%20are%20appended%20to%0Athe%20image-question%20prompt%2C%20enriching%20it%20with%20spatial%20context.%20This%20guides%20MLLMs%0Ato%20produce%20more%20grounded%20and%20less%20hallucinated%20responses.%20Our%20method%20is%0Alightweight%2C%20modular%2C%20and%20compatible%20with%20black-box%20MLLMs.%20Experiments%20on%0Ahallucination-sensitive%20%28POPE%29%20and%20reasoning-intensive%20%28GQA%29%20benchmarks%20show%0Aconsistent%20improvements%20across%20multiple%20MLLMs%2C%20validating%20the%20effectiveness%20of%0Adepth-aware%20prompting%20in%20a%20zero-training%20setting.%0A&entry.1838667208=http%3A//arxiv.org/abs/2507.08679v1&entry.124074799=Read"},
{"title": "Emergent Natural Language with Communication Games for Improving Image\n  Captioning Capabilities without Additional Data", "author": "Parag Dutta and Ambedkar Dukkipati", "abstract": "  Image captioning is an important problem in developing various AI systems,\nand these tasks require large volumes of annotated images to train the models.\nSince all existing labelled datasets are already used for training the large\nVision Language Models (VLMs), it becomes challenging to improve the\nperformance of the same. Considering this, it is essential to consider the\nunsupervised image captioning performance, which remains relatively\nunder-explored. To that end, we propose LoGIC (Lewis Communication Game for\nImage Captioning), a Multi-agent Reinforcement Learning game. The proposed\nmethod consists of two agents, a 'speaker' and a 'listener', with the objective\nof learning a strategy for communicating in natural language. We train agents\nin the cooperative common-reward setting using the GRPO algorithm and show that\nimprovement in image captioning performance emerges as a consequence of the\nagents learning to play the game. We show that using pre-trained VLMs as the\n'speaker' and Large Language Model (LLM) for language understanding in the\n'listener', we achieved a $46$ BLEU score after fine-tuning using LoGIC without\nadditional labels, a $2$ units advantage in absolute metrics compared to the\n$44$ BLEU score of the vanilla VLM. Additionally, we replace the VLM from the\n'speaker' with lightweight components: (i) a ViT for image perception and (ii)\na GPT2 language generation, and train them from scratch using LoGIC, obtaining\na $31$ BLEU score in the unsupervised setting, a $10$ points advantage over\nexisting unsupervised image-captioning methods.\n", "link": "http://arxiv.org/abs/2507.08610v1", "date": "2025-07-11", "relevancy": 2.7206, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5518}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5518}, {"title": "Genie: Generative Interactive Environments", "link": "http://arxiv.org/abs/2402.15391v1", "similarity": 0.5287}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Emergent%20Natural%20Language%20with%20Communication%20Games%20for%20Improving%20Image%0A%20%20Captioning%20Capabilities%20without%20Additional%20Data&body=Title%3A%20Emergent%20Natural%20Language%20with%20Communication%20Games%20for%20Improving%20Image%0A%20%20Captioning%20Capabilities%20without%20Additional%20Data%0AAuthor%3A%20Parag%20Dutta%20and%20Ambedkar%20Dukkipati%0AAbstract%3A%20%20%20Image%20captioning%20is%20an%20important%20problem%20in%20developing%20various%20AI%20systems%2C%0Aand%20these%20tasks%20require%20large%20volumes%20of%20annotated%20images%20to%20train%20the%20models.%0ASince%20all%20existing%20labelled%20datasets%20are%20already%20used%20for%20training%20the%20large%0AVision%20Language%20Models%20%28VLMs%29%2C%20it%20becomes%20challenging%20to%20improve%20the%0Aperformance%20of%20the%20same.%20Considering%20this%2C%20it%20is%20essential%20to%20consider%20the%0Aunsupervised%20image%20captioning%20performance%2C%20which%20remains%20relatively%0Aunder-explored.%20To%20that%20end%2C%20we%20propose%20LoGIC%20%28Lewis%20Communication%20Game%20for%0AImage%20Captioning%29%2C%20a%20Multi-agent%20Reinforcement%20Learning%20game.%20The%20proposed%0Amethod%20consists%20of%20two%20agents%2C%20a%20%27speaker%27%20and%20a%20%27listener%27%2C%20with%20the%20objective%0Aof%20learning%20a%20strategy%20for%20communicating%20in%20natural%20language.%20We%20train%20agents%0Ain%20the%20cooperative%20common-reward%20setting%20using%20the%20GRPO%20algorithm%20and%20show%20that%0Aimprovement%20in%20image%20captioning%20performance%20emerges%20as%20a%20consequence%20of%20the%0Aagents%20learning%20to%20play%20the%20game.%20We%20show%20that%20using%20pre-trained%20VLMs%20as%20the%0A%27speaker%27%20and%20Large%20Language%20Model%20%28LLM%29%20for%20language%20understanding%20in%20the%0A%27listener%27%2C%20we%20achieved%20a%20%2446%24%20BLEU%20score%20after%20fine-tuning%20using%20LoGIC%20without%0Aadditional%20labels%2C%20a%20%242%24%20units%20advantage%20in%20absolute%20metrics%20compared%20to%20the%0A%2444%24%20BLEU%20score%20of%20the%20vanilla%20VLM.%20Additionally%2C%20we%20replace%20the%20VLM%20from%20the%0A%27speaker%27%20with%20lightweight%20components%3A%20%28i%29%20a%20ViT%20for%20image%20perception%20and%20%28ii%29%0Aa%20GPT2%20language%20generation%2C%20and%20train%20them%20from%20scratch%20using%20LoGIC%2C%20obtaining%0Aa%20%2431%24%20BLEU%20score%20in%20the%20unsupervised%20setting%2C%20a%20%2410%24%20points%20advantage%20over%0Aexisting%20unsupervised%20image-captioning%20methods.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2507.08610v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DEmergent%2520Natural%2520Language%2520with%2520Communication%2520Games%2520for%2520Improving%2520Image%250A%2520%2520Captioning%2520Capabilities%2520without%2520Additional%2520Data%26entry.906535625%3DParag%2520Dutta%2520and%2520Ambedkar%2520Dukkipati%26entry.1292438233%3D%2520%2520Image%2520captioning%2520is%2520an%2520important%2520problem%2520in%2520developing%2520various%2520AI%2520systems%252C%250Aand%2520these%2520tasks%2520require%2520large%2520volumes%2520of%2520annotated%2520images%2520to%2520train%2520the%2520models.%250ASince%2520all%2520existing%2520labelled%2520datasets%2520are%2520already%2520used%2520for%2520training%2520the%2520large%250AVision%2520Language%2520Models%2520%2528VLMs%2529%252C%2520it%2520becomes%2520challenging%2520to%2520improve%2520the%250Aperformance%2520of%2520the%2520same.%2520Considering%2520this%252C%2520it%2520is%2520essential%2520to%2520consider%2520the%250Aunsupervised%2520image%2520captioning%2520performance%252C%2520which%2520remains%2520relatively%250Aunder-explored.%2520To%2520that%2520end%252C%2520we%2520propose%2520LoGIC%2520%2528Lewis%2520Communication%2520Game%2520for%250AImage%2520Captioning%2529%252C%2520a%2520Multi-agent%2520Reinforcement%2520Learning%2520game.%2520The%2520proposed%250Amethod%2520consists%2520of%2520two%2520agents%252C%2520a%2520%2527speaker%2527%2520and%2520a%2520%2527listener%2527%252C%2520with%2520the%2520objective%250Aof%2520learning%2520a%2520strategy%2520for%2520communicating%2520in%2520natural%2520language.%2520We%2520train%2520agents%250Ain%2520the%2520cooperative%2520common-reward%2520setting%2520using%2520the%2520GRPO%2520algorithm%2520and%2520show%2520that%250Aimprovement%2520in%2520image%2520captioning%2520performance%2520emerges%2520as%2520a%2520consequence%2520of%2520the%250Aagents%2520learning%2520to%2520play%2520the%2520game.%2520We%2520show%2520that%2520using%2520pre-trained%2520VLMs%2520as%2520the%250A%2527speaker%2527%2520and%2520Large%2520Language%2520Model%2520%2528LLM%2529%2520for%2520language%2520understanding%2520in%2520the%250A%2527listener%2527%252C%2520we%2520achieved%2520a%2520%252446%2524%2520BLEU%2520score%2520after%2520fine-tuning%2520using%2520LoGIC%2520without%250Aadditional%2520labels%252C%2520a%2520%25242%2524%2520units%2520advantage%2520in%2520absolute%2520metrics%2520compared%2520to%2520the%250A%252444%2524%2520BLEU%2520score%2520of%2520the%2520vanilla%2520VLM.%2520Additionally%252C%2520we%2520replace%2520the%2520VLM%2520from%2520the%250A%2527speaker%2527%2520with%2520lightweight%2520components%253A%2520%2528i%2529%2520a%2520ViT%2520for%2520image%2520perception%2520and%2520%2528ii%2529%250Aa%2520GPT2%2520language%2520generation%252C%2520and%2520train%2520them%2520from%2520scratch%2520using%2520LoGIC%252C%2520obtaining%250Aa%2520%252431%2524%2520BLEU%2520score%2520in%2520the%2520unsupervised%2520setting%252C%2520a%2520%252410%2524%2520points%2520advantage%2520over%250Aexisting%2520unsupervised%2520image-captioning%2520methods.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2507.08610v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Emergent%20Natural%20Language%20with%20Communication%20Games%20for%20Improving%20Image%0A%20%20Captioning%20Capabilities%20without%20Additional%20Data&entry.906535625=Parag%20Dutta%20and%20Ambedkar%20Dukkipati&entry.1292438233=%20%20Image%20captioning%20is%20an%20important%20problem%20in%20developing%20various%20AI%20systems%2C%0Aand%20these%20tasks%20require%20large%20volumes%20of%20annotated%20images%20to%20train%20the%20models.%0ASince%20all%20existing%20labelled%20datasets%20are%20already%20used%20for%20training%20the%20large%0AVision%20Language%20Models%20%28VLMs%29%2C%20it%20becomes%20challenging%20to%20improve%20the%0Aperformance%20of%20the%20same.%20Considering%20this%2C%20it%20is%20essential%20to%20consider%20the%0Aunsupervised%20image%20captioning%20performance%2C%20which%20remains%20relatively%0Aunder-explored.%20To%20that%20end%2C%20we%20propose%20LoGIC%20%28Lewis%20Communication%20Game%20for%0AImage%20Captioning%29%2C%20a%20Multi-agent%20Reinforcement%20Learning%20game.%20The%20proposed%0Amethod%20consists%20of%20two%20agents%2C%20a%20%27speaker%27%20and%20a%20%27listener%27%2C%20with%20the%20objective%0Aof%20learning%20a%20strategy%20for%20communicating%20in%20natural%20language.%20We%20train%20agents%0Ain%20the%20cooperative%20common-reward%20setting%20using%20the%20GRPO%20algorithm%20and%20show%20that%0Aimprovement%20in%20image%20captioning%20performance%20emerges%20as%20a%20consequence%20of%20the%0Aagents%20learning%20to%20play%20the%20game.%20We%20show%20that%20using%20pre-trained%20VLMs%20as%20the%0A%27speaker%27%20and%20Large%20Language%20Model%20%28LLM%29%20for%20language%20understanding%20in%20the%0A%27listener%27%2C%20we%20achieved%20a%20%2446%24%20BLEU%20score%20after%20fine-tuning%20using%20LoGIC%20without%0Aadditional%20labels%2C%20a%20%242%24%20units%20advantage%20in%20absolute%20metrics%20compared%20to%20the%0A%2444%24%20BLEU%20score%20of%20the%20vanilla%20VLM.%20Additionally%2C%20we%20replace%20the%20VLM%20from%20the%0A%27speaker%27%20with%20lightweight%20components%3A%20%28i%29%20a%20ViT%20for%20image%20perception%20and%20%28ii%29%0Aa%20GPT2%20language%20generation%2C%20and%20train%20them%20from%20scratch%20using%20LoGIC%2C%20obtaining%0Aa%20%2431%24%20BLEU%20score%20in%20the%20unsupervised%20setting%2C%20a%20%2410%24%20points%20advantage%20over%0Aexisting%20unsupervised%20image-captioning%20methods.%0A&entry.1838667208=http%3A//arxiv.org/abs/2507.08610v1&entry.124074799=Read"},
{"title": "An Efficient Approach for Muscle Segmentation and 3D Reconstruction\n  Using Keypoint Tracking in MRI Scan", "author": "Mengyuan Liu and Jeongkyu Lee", "abstract": "  Magnetic resonance imaging (MRI) enables non-invasive, high-resolution\nanalysis of muscle structures. However, automated segmentation remains limited\nby high computational costs, reliance on large training datasets, and reduced\naccuracy in segmenting smaller muscles. Convolutional neural network\n(CNN)-based methods, while powerful, often suffer from substantial\ncomputational overhead, limited generalizability, and poor interpretability\nacross diverse populations. This study proposes a training-free segmentation\napproach based on keypoint tracking, which integrates keypoint selection with\nLucas-Kanade optical flow. The proposed method achieves a mean Dice similarity\ncoefficient (DSC) ranging from 0.6 to 0.7, depending on the keypoint selection\nstrategy, performing comparably to state-of-the-art CNN-based models while\nsubstantially reducing computational demands and enhancing interpretability.\nThis scalable framework presents a robust and explainable alternative for\nmuscle segmentation in clinical and research applications.\n", "link": "http://arxiv.org/abs/2507.08690v1", "date": "2025-07-11", "relevancy": 2.6943, "topK": [{"title": "DDM-NET: End-to-end learning of keypoint feature Detection, Description and Matching for 3D localization", "link": "https://arxiv.org/abs/2212.04575", "similarity": 0.5778}, {"title": "GST: Precise 3D Human Body from a Single Image with Gaussian Splatting\n  Transformers", "link": "http://arxiv.org/abs/2409.04196v1", "similarity": 0.5261}, {"title": "RTG-SLAM: Real-time 3D Reconstruction at Scale using Gaussian Splatting", "link": "http://arxiv.org/abs/2404.19706v1", "similarity": 0.5126}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20An%20Efficient%20Approach%20for%20Muscle%20Segmentation%20and%203D%20Reconstruction%0A%20%20Using%20Keypoint%20Tracking%20in%20MRI%20Scan&body=Title%3A%20An%20Efficient%20Approach%20for%20Muscle%20Segmentation%20and%203D%20Reconstruction%0A%20%20Using%20Keypoint%20Tracking%20in%20MRI%20Scan%0AAuthor%3A%20Mengyuan%20Liu%20and%20Jeongkyu%20Lee%0AAbstract%3A%20%20%20Magnetic%20resonance%20imaging%20%28MRI%29%20enables%20non-invasive%2C%20high-resolution%0Aanalysis%20of%20muscle%20structures.%20However%2C%20automated%20segmentation%20remains%20limited%0Aby%20high%20computational%20costs%2C%20reliance%20on%20large%20training%20datasets%2C%20and%20reduced%0Aaccuracy%20in%20segmenting%20smaller%20muscles.%20Convolutional%20neural%20network%0A%28CNN%29-based%20methods%2C%20while%20powerful%2C%20often%20suffer%20from%20substantial%0Acomputational%20overhead%2C%20limited%20generalizability%2C%20and%20poor%20interpretability%0Aacross%20diverse%20populations.%20This%20study%20proposes%20a%20training-free%20segmentation%0Aapproach%20based%20on%20keypoint%20tracking%2C%20which%20integrates%20keypoint%20selection%20with%0ALucas-Kanade%20optical%20flow.%20The%20proposed%20method%20achieves%20a%20mean%20Dice%20similarity%0Acoefficient%20%28DSC%29%20ranging%20from%200.6%20to%200.7%2C%20depending%20on%20the%20keypoint%20selection%0Astrategy%2C%20performing%20comparably%20to%20state-of-the-art%20CNN-based%20models%20while%0Asubstantially%20reducing%20computational%20demands%20and%20enhancing%20interpretability.%0AThis%20scalable%20framework%20presents%20a%20robust%20and%20explainable%20alternative%20for%0Amuscle%20segmentation%20in%20clinical%20and%20research%20applications.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2507.08690v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DAn%2520Efficient%2520Approach%2520for%2520Muscle%2520Segmentation%2520and%25203D%2520Reconstruction%250A%2520%2520Using%2520Keypoint%2520Tracking%2520in%2520MRI%2520Scan%26entry.906535625%3DMengyuan%2520Liu%2520and%2520Jeongkyu%2520Lee%26entry.1292438233%3D%2520%2520Magnetic%2520resonance%2520imaging%2520%2528MRI%2529%2520enables%2520non-invasive%252C%2520high-resolution%250Aanalysis%2520of%2520muscle%2520structures.%2520However%252C%2520automated%2520segmentation%2520remains%2520limited%250Aby%2520high%2520computational%2520costs%252C%2520reliance%2520on%2520large%2520training%2520datasets%252C%2520and%2520reduced%250Aaccuracy%2520in%2520segmenting%2520smaller%2520muscles.%2520Convolutional%2520neural%2520network%250A%2528CNN%2529-based%2520methods%252C%2520while%2520powerful%252C%2520often%2520suffer%2520from%2520substantial%250Acomputational%2520overhead%252C%2520limited%2520generalizability%252C%2520and%2520poor%2520interpretability%250Aacross%2520diverse%2520populations.%2520This%2520study%2520proposes%2520a%2520training-free%2520segmentation%250Aapproach%2520based%2520on%2520keypoint%2520tracking%252C%2520which%2520integrates%2520keypoint%2520selection%2520with%250ALucas-Kanade%2520optical%2520flow.%2520The%2520proposed%2520method%2520achieves%2520a%2520mean%2520Dice%2520similarity%250Acoefficient%2520%2528DSC%2529%2520ranging%2520from%25200.6%2520to%25200.7%252C%2520depending%2520on%2520the%2520keypoint%2520selection%250Astrategy%252C%2520performing%2520comparably%2520to%2520state-of-the-art%2520CNN-based%2520models%2520while%250Asubstantially%2520reducing%2520computational%2520demands%2520and%2520enhancing%2520interpretability.%250AThis%2520scalable%2520framework%2520presents%2520a%2520robust%2520and%2520explainable%2520alternative%2520for%250Amuscle%2520segmentation%2520in%2520clinical%2520and%2520research%2520applications.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2507.08690v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=An%20Efficient%20Approach%20for%20Muscle%20Segmentation%20and%203D%20Reconstruction%0A%20%20Using%20Keypoint%20Tracking%20in%20MRI%20Scan&entry.906535625=Mengyuan%20Liu%20and%20Jeongkyu%20Lee&entry.1292438233=%20%20Magnetic%20resonance%20imaging%20%28MRI%29%20enables%20non-invasive%2C%20high-resolution%0Aanalysis%20of%20muscle%20structures.%20However%2C%20automated%20segmentation%20remains%20limited%0Aby%20high%20computational%20costs%2C%20reliance%20on%20large%20training%20datasets%2C%20and%20reduced%0Aaccuracy%20in%20segmenting%20smaller%20muscles.%20Convolutional%20neural%20network%0A%28CNN%29-based%20methods%2C%20while%20powerful%2C%20often%20suffer%20from%20substantial%0Acomputational%20overhead%2C%20limited%20generalizability%2C%20and%20poor%20interpretability%0Aacross%20diverse%20populations.%20This%20study%20proposes%20a%20training-free%20segmentation%0Aapproach%20based%20on%20keypoint%20tracking%2C%20which%20integrates%20keypoint%20selection%20with%0ALucas-Kanade%20optical%20flow.%20The%20proposed%20method%20achieves%20a%20mean%20Dice%20similarity%0Acoefficient%20%28DSC%29%20ranging%20from%200.6%20to%200.7%2C%20depending%20on%20the%20keypoint%20selection%0Astrategy%2C%20performing%20comparably%20to%20state-of-the-art%20CNN-based%20models%20while%0Asubstantially%20reducing%20computational%20demands%20and%20enhancing%20interpretability.%0AThis%20scalable%20framework%20presents%20a%20robust%20and%20explainable%20alternative%20for%0Amuscle%20segmentation%20in%20clinical%20and%20research%20applications.%0A&entry.1838667208=http%3A//arxiv.org/abs/2507.08690v1&entry.124074799=Read"},
{"title": "SEREP: Semantic Facial Expression Representation for Robust In-the-Wild\n  Capture and Retargeting", "author": "Arthur Josi and Luiz Gustavo Hafemann and Abdallah Dib and Emeline Got and Rafael M. O. Cruz and Marc-Andre Carbonneau", "abstract": "  Monocular facial performance capture in-the-wild is challenging due to varied\ncapture conditions, face shapes, and expressions. Most current methods rely on\nlinear 3D Morphable Models, which represent facial expressions independently of\nidentity at the vertex displacement level. We propose SEREP (Semantic\nExpression Representation), a model that disentangles expression from identity\nat the semantic level. We start by learning an expression representation from\nhigh-quality 3D data of unpaired facial expressions. Then, we train a model to\npredict expression from monocular images relying on a novel semi-supervised\nscheme using low quality synthetic data. In addition, we introduce MultiREX, a\nbenchmark addressing the lack of evaluation resources for the expression\ncapture task. Our experiments show that SEREP outperforms state-of-the-art\nmethods, capturing challenging expressions and transferring them to new\nidentities.\n", "link": "http://arxiv.org/abs/2412.14371v3", "date": "2025-07-11", "relevancy": 2.688, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5489}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5489}, {"title": "Synthesizing Moving People with 3D Control", "link": "http://arxiv.org/abs/2401.10889v2", "similarity": 0.5151}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20SEREP%3A%20Semantic%20Facial%20Expression%20Representation%20for%20Robust%20In-the-Wild%0A%20%20Capture%20and%20Retargeting&body=Title%3A%20SEREP%3A%20Semantic%20Facial%20Expression%20Representation%20for%20Robust%20In-the-Wild%0A%20%20Capture%20and%20Retargeting%0AAuthor%3A%20Arthur%20Josi%20and%20Luiz%20Gustavo%20Hafemann%20and%20Abdallah%20Dib%20and%20Emeline%20Got%20and%20Rafael%20M.%20O.%20Cruz%20and%20Marc-Andre%20Carbonneau%0AAbstract%3A%20%20%20Monocular%20facial%20performance%20capture%20in-the-wild%20is%20challenging%20due%20to%20varied%0Acapture%20conditions%2C%20face%20shapes%2C%20and%20expressions.%20Most%20current%20methods%20rely%20on%0Alinear%203D%20Morphable%20Models%2C%20which%20represent%20facial%20expressions%20independently%20of%0Aidentity%20at%20the%20vertex%20displacement%20level.%20We%20propose%20SEREP%20%28Semantic%0AExpression%20Representation%29%2C%20a%20model%20that%20disentangles%20expression%20from%20identity%0Aat%20the%20semantic%20level.%20We%20start%20by%20learning%20an%20expression%20representation%20from%0Ahigh-quality%203D%20data%20of%20unpaired%20facial%20expressions.%20Then%2C%20we%20train%20a%20model%20to%0Apredict%20expression%20from%20monocular%20images%20relying%20on%20a%20novel%20semi-supervised%0Ascheme%20using%20low%20quality%20synthetic%20data.%20In%20addition%2C%20we%20introduce%20MultiREX%2C%20a%0Abenchmark%20addressing%20the%20lack%20of%20evaluation%20resources%20for%20the%20expression%0Acapture%20task.%20Our%20experiments%20show%20that%20SEREP%20outperforms%20state-of-the-art%0Amethods%2C%20capturing%20challenging%20expressions%20and%20transferring%20them%20to%20new%0Aidentities.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2412.14371v3%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DSEREP%253A%2520Semantic%2520Facial%2520Expression%2520Representation%2520for%2520Robust%2520In-the-Wild%250A%2520%2520Capture%2520and%2520Retargeting%26entry.906535625%3DArthur%2520Josi%2520and%2520Luiz%2520Gustavo%2520Hafemann%2520and%2520Abdallah%2520Dib%2520and%2520Emeline%2520Got%2520and%2520Rafael%2520M.%2520O.%2520Cruz%2520and%2520Marc-Andre%2520Carbonneau%26entry.1292438233%3D%2520%2520Monocular%2520facial%2520performance%2520capture%2520in-the-wild%2520is%2520challenging%2520due%2520to%2520varied%250Acapture%2520conditions%252C%2520face%2520shapes%252C%2520and%2520expressions.%2520Most%2520current%2520methods%2520rely%2520on%250Alinear%25203D%2520Morphable%2520Models%252C%2520which%2520represent%2520facial%2520expressions%2520independently%2520of%250Aidentity%2520at%2520the%2520vertex%2520displacement%2520level.%2520We%2520propose%2520SEREP%2520%2528Semantic%250AExpression%2520Representation%2529%252C%2520a%2520model%2520that%2520disentangles%2520expression%2520from%2520identity%250Aat%2520the%2520semantic%2520level.%2520We%2520start%2520by%2520learning%2520an%2520expression%2520representation%2520from%250Ahigh-quality%25203D%2520data%2520of%2520unpaired%2520facial%2520expressions.%2520Then%252C%2520we%2520train%2520a%2520model%2520to%250Apredict%2520expression%2520from%2520monocular%2520images%2520relying%2520on%2520a%2520novel%2520semi-supervised%250Ascheme%2520using%2520low%2520quality%2520synthetic%2520data.%2520In%2520addition%252C%2520we%2520introduce%2520MultiREX%252C%2520a%250Abenchmark%2520addressing%2520the%2520lack%2520of%2520evaluation%2520resources%2520for%2520the%2520expression%250Acapture%2520task.%2520Our%2520experiments%2520show%2520that%2520SEREP%2520outperforms%2520state-of-the-art%250Amethods%252C%2520capturing%2520challenging%2520expressions%2520and%2520transferring%2520them%2520to%2520new%250Aidentities.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2412.14371v3%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=SEREP%3A%20Semantic%20Facial%20Expression%20Representation%20for%20Robust%20In-the-Wild%0A%20%20Capture%20and%20Retargeting&entry.906535625=Arthur%20Josi%20and%20Luiz%20Gustavo%20Hafemann%20and%20Abdallah%20Dib%20and%20Emeline%20Got%20and%20Rafael%20M.%20O.%20Cruz%20and%20Marc-Andre%20Carbonneau&entry.1292438233=%20%20Monocular%20facial%20performance%20capture%20in-the-wild%20is%20challenging%20due%20to%20varied%0Acapture%20conditions%2C%20face%20shapes%2C%20and%20expressions.%20Most%20current%20methods%20rely%20on%0Alinear%203D%20Morphable%20Models%2C%20which%20represent%20facial%20expressions%20independently%20of%0Aidentity%20at%20the%20vertex%20displacement%20level.%20We%20propose%20SEREP%20%28Semantic%0AExpression%20Representation%29%2C%20a%20model%20that%20disentangles%20expression%20from%20identity%0Aat%20the%20semantic%20level.%20We%20start%20by%20learning%20an%20expression%20representation%20from%0Ahigh-quality%203D%20data%20of%20unpaired%20facial%20expressions.%20Then%2C%20we%20train%20a%20model%20to%0Apredict%20expression%20from%20monocular%20images%20relying%20on%20a%20novel%20semi-supervised%0Ascheme%20using%20low%20quality%20synthetic%20data.%20In%20addition%2C%20we%20introduce%20MultiREX%2C%20a%0Abenchmark%20addressing%20the%20lack%20of%20evaluation%20resources%20for%20the%20expression%0Acapture%20task.%20Our%20experiments%20show%20that%20SEREP%20outperforms%20state-of-the-art%0Amethods%2C%20capturing%20challenging%20expressions%20and%20transferring%20them%20to%20new%0Aidentities.%0A&entry.1838667208=http%3A//arxiv.org/abs/2412.14371v3&entry.124074799=Read"},
{"title": "Text2BIM: Generating Building Models Using a Large Language Model-based\n  Multi-Agent Framework", "author": "Changyu Du and Sebastian Esser and Stavros Nousias and Andr\u00e9 Borrmann", "abstract": "  The conventional BIM authoring process typically requires designers to master\ncomplex and tedious modeling commands in order to materialize their design\nintentions within BIM authoring tools. This additional cognitive burden\ncomplicates the design process and hinders the adoption of BIM and model-based\ndesign in the AEC (Architecture, Engineering, and Construction) industry. To\nfacilitate the expression of design intentions more intuitively, we propose\nText2BIM, an LLM-based multi-agent framework that can generate 3D building\nmodels from natural language instructions. This framework orchestrates multiple\nLLM agents to collaborate and reason, transforming textual user input into\nimperative code that invokes the BIM authoring tool's APIs, thereby generating\neditable BIM models with internal layouts, external envelopes, and semantic\ninformation directly in the software. Furthermore, a rule-based model checker\nis introduced into the agentic workflow, utilizing predefined domain knowledge\nto guide the LLM agents in resolving issues within the generated models and\niteratively improving model quality. Extensive experiments were conducted to\ncompare and analyze the performance of three different LLMs under the proposed\nframework. The evaluation results demonstrate that our approach can effectively\ngenerate high-quality, structurally rational building models that are aligned\nwith the abstract concepts specified by user input. Finally, an interactive\nsoftware prototype was developed to integrate the framework into the BIM\nauthoring software Vectorworks, showcasing the potential of modeling by\nchatting. The code is available at: https://github.com/dcy0577/Text2BIM\n", "link": "http://arxiv.org/abs/2408.08054v2", "date": "2025-07-11", "relevancy": 2.6507, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5308}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5308}, {"title": "DressCode: Autoregressively Sewing and Generating Garments from Text\n  Guidance", "link": "http://arxiv.org/abs/2401.16465v3", "similarity": 0.5289}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Text2BIM%3A%20Generating%20Building%20Models%20Using%20a%20Large%20Language%20Model-based%0A%20%20Multi-Agent%20Framework&body=Title%3A%20Text2BIM%3A%20Generating%20Building%20Models%20Using%20a%20Large%20Language%20Model-based%0A%20%20Multi-Agent%20Framework%0AAuthor%3A%20Changyu%20Du%20and%20Sebastian%20Esser%20and%20Stavros%20Nousias%20and%20Andr%C3%A9%20Borrmann%0AAbstract%3A%20%20%20The%20conventional%20BIM%20authoring%20process%20typically%20requires%20designers%20to%20master%0Acomplex%20and%20tedious%20modeling%20commands%20in%20order%20to%20materialize%20their%20design%0Aintentions%20within%20BIM%20authoring%20tools.%20This%20additional%20cognitive%20burden%0Acomplicates%20the%20design%20process%20and%20hinders%20the%20adoption%20of%20BIM%20and%20model-based%0Adesign%20in%20the%20AEC%20%28Architecture%2C%20Engineering%2C%20and%20Construction%29%20industry.%20To%0Afacilitate%20the%20expression%20of%20design%20intentions%20more%20intuitively%2C%20we%20propose%0AText2BIM%2C%20an%20LLM-based%20multi-agent%20framework%20that%20can%20generate%203D%20building%0Amodels%20from%20natural%20language%20instructions.%20This%20framework%20orchestrates%20multiple%0ALLM%20agents%20to%20collaborate%20and%20reason%2C%20transforming%20textual%20user%20input%20into%0Aimperative%20code%20that%20invokes%20the%20BIM%20authoring%20tool%27s%20APIs%2C%20thereby%20generating%0Aeditable%20BIM%20models%20with%20internal%20layouts%2C%20external%20envelopes%2C%20and%20semantic%0Ainformation%20directly%20in%20the%20software.%20Furthermore%2C%20a%20rule-based%20model%20checker%0Ais%20introduced%20into%20the%20agentic%20workflow%2C%20utilizing%20predefined%20domain%20knowledge%0Ato%20guide%20the%20LLM%20agents%20in%20resolving%20issues%20within%20the%20generated%20models%20and%0Aiteratively%20improving%20model%20quality.%20Extensive%20experiments%20were%20conducted%20to%0Acompare%20and%20analyze%20the%20performance%20of%20three%20different%20LLMs%20under%20the%20proposed%0Aframework.%20The%20evaluation%20results%20demonstrate%20that%20our%20approach%20can%20effectively%0Agenerate%20high-quality%2C%20structurally%20rational%20building%20models%20that%20are%20aligned%0Awith%20the%20abstract%20concepts%20specified%20by%20user%20input.%20Finally%2C%20an%20interactive%0Asoftware%20prototype%20was%20developed%20to%20integrate%20the%20framework%20into%20the%20BIM%0Aauthoring%20software%20Vectorworks%2C%20showcasing%20the%20potential%20of%20modeling%20by%0Achatting.%20The%20code%20is%20available%20at%3A%20https%3A//github.com/dcy0577/Text2BIM%0A%0ALink%3A%20http%3A//arxiv.org/abs/2408.08054v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DText2BIM%253A%2520Generating%2520Building%2520Models%2520Using%2520a%2520Large%2520Language%2520Model-based%250A%2520%2520Multi-Agent%2520Framework%26entry.906535625%3DChangyu%2520Du%2520and%2520Sebastian%2520Esser%2520and%2520Stavros%2520Nousias%2520and%2520Andr%25C3%25A9%2520Borrmann%26entry.1292438233%3D%2520%2520The%2520conventional%2520BIM%2520authoring%2520process%2520typically%2520requires%2520designers%2520to%2520master%250Acomplex%2520and%2520tedious%2520modeling%2520commands%2520in%2520order%2520to%2520materialize%2520their%2520design%250Aintentions%2520within%2520BIM%2520authoring%2520tools.%2520This%2520additional%2520cognitive%2520burden%250Acomplicates%2520the%2520design%2520process%2520and%2520hinders%2520the%2520adoption%2520of%2520BIM%2520and%2520model-based%250Adesign%2520in%2520the%2520AEC%2520%2528Architecture%252C%2520Engineering%252C%2520and%2520Construction%2529%2520industry.%2520To%250Afacilitate%2520the%2520expression%2520of%2520design%2520intentions%2520more%2520intuitively%252C%2520we%2520propose%250AText2BIM%252C%2520an%2520LLM-based%2520multi-agent%2520framework%2520that%2520can%2520generate%25203D%2520building%250Amodels%2520from%2520natural%2520language%2520instructions.%2520This%2520framework%2520orchestrates%2520multiple%250ALLM%2520agents%2520to%2520collaborate%2520and%2520reason%252C%2520transforming%2520textual%2520user%2520input%2520into%250Aimperative%2520code%2520that%2520invokes%2520the%2520BIM%2520authoring%2520tool%2527s%2520APIs%252C%2520thereby%2520generating%250Aeditable%2520BIM%2520models%2520with%2520internal%2520layouts%252C%2520external%2520envelopes%252C%2520and%2520semantic%250Ainformation%2520directly%2520in%2520the%2520software.%2520Furthermore%252C%2520a%2520rule-based%2520model%2520checker%250Ais%2520introduced%2520into%2520the%2520agentic%2520workflow%252C%2520utilizing%2520predefined%2520domain%2520knowledge%250Ato%2520guide%2520the%2520LLM%2520agents%2520in%2520resolving%2520issues%2520within%2520the%2520generated%2520models%2520and%250Aiteratively%2520improving%2520model%2520quality.%2520Extensive%2520experiments%2520were%2520conducted%2520to%250Acompare%2520and%2520analyze%2520the%2520performance%2520of%2520three%2520different%2520LLMs%2520under%2520the%2520proposed%250Aframework.%2520The%2520evaluation%2520results%2520demonstrate%2520that%2520our%2520approach%2520can%2520effectively%250Agenerate%2520high-quality%252C%2520structurally%2520rational%2520building%2520models%2520that%2520are%2520aligned%250Awith%2520the%2520abstract%2520concepts%2520specified%2520by%2520user%2520input.%2520Finally%252C%2520an%2520interactive%250Asoftware%2520prototype%2520was%2520developed%2520to%2520integrate%2520the%2520framework%2520into%2520the%2520BIM%250Aauthoring%2520software%2520Vectorworks%252C%2520showcasing%2520the%2520potential%2520of%2520modeling%2520by%250Achatting.%2520The%2520code%2520is%2520available%2520at%253A%2520https%253A//github.com/dcy0577/Text2BIM%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2408.08054v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Text2BIM%3A%20Generating%20Building%20Models%20Using%20a%20Large%20Language%20Model-based%0A%20%20Multi-Agent%20Framework&entry.906535625=Changyu%20Du%20and%20Sebastian%20Esser%20and%20Stavros%20Nousias%20and%20Andr%C3%A9%20Borrmann&entry.1292438233=%20%20The%20conventional%20BIM%20authoring%20process%20typically%20requires%20designers%20to%20master%0Acomplex%20and%20tedious%20modeling%20commands%20in%20order%20to%20materialize%20their%20design%0Aintentions%20within%20BIM%20authoring%20tools.%20This%20additional%20cognitive%20burden%0Acomplicates%20the%20design%20process%20and%20hinders%20the%20adoption%20of%20BIM%20and%20model-based%0Adesign%20in%20the%20AEC%20%28Architecture%2C%20Engineering%2C%20and%20Construction%29%20industry.%20To%0Afacilitate%20the%20expression%20of%20design%20intentions%20more%20intuitively%2C%20we%20propose%0AText2BIM%2C%20an%20LLM-based%20multi-agent%20framework%20that%20can%20generate%203D%20building%0Amodels%20from%20natural%20language%20instructions.%20This%20framework%20orchestrates%20multiple%0ALLM%20agents%20to%20collaborate%20and%20reason%2C%20transforming%20textual%20user%20input%20into%0Aimperative%20code%20that%20invokes%20the%20BIM%20authoring%20tool%27s%20APIs%2C%20thereby%20generating%0Aeditable%20BIM%20models%20with%20internal%20layouts%2C%20external%20envelopes%2C%20and%20semantic%0Ainformation%20directly%20in%20the%20software.%20Furthermore%2C%20a%20rule-based%20model%20checker%0Ais%20introduced%20into%20the%20agentic%20workflow%2C%20utilizing%20predefined%20domain%20knowledge%0Ato%20guide%20the%20LLM%20agents%20in%20resolving%20issues%20within%20the%20generated%20models%20and%0Aiteratively%20improving%20model%20quality.%20Extensive%20experiments%20were%20conducted%20to%0Acompare%20and%20analyze%20the%20performance%20of%20three%20different%20LLMs%20under%20the%20proposed%0Aframework.%20The%20evaluation%20results%20demonstrate%20that%20our%20approach%20can%20effectively%0Agenerate%20high-quality%2C%20structurally%20rational%20building%20models%20that%20are%20aligned%0Awith%20the%20abstract%20concepts%20specified%20by%20user%20input.%20Finally%2C%20an%20interactive%0Asoftware%20prototype%20was%20developed%20to%20integrate%20the%20framework%20into%20the%20BIM%0Aauthoring%20software%20Vectorworks%2C%20showcasing%20the%20potential%20of%20modeling%20by%0Achatting.%20The%20code%20is%20available%20at%3A%20https%3A//github.com/dcy0577/Text2BIM%0A&entry.1838667208=http%3A//arxiv.org/abs/2408.08054v2&entry.124074799=Read"},
{"title": "Multilingual Multimodal Software Developer for Code Generation", "author": "Linzheng Chai and Jian Yang and Shukai Liu and Wei Zhang and Liran Wang and Ke Jin and Tao Sun and Congnan Liu and Chenchen Zhang and Hualei Zhu and Jiaheng Liu and Xianjie Wu and Ge Zhang and Tianyu Liu and Zhoujun Li", "abstract": "  The rapid advancement of Large Language Models (LLMs) has significantly\nimproved code generation, yet most models remain text-only, neglecting crucial\nvisual aids like diagrams and flowcharts used in real-world software\ndevelopment. To bridge this gap, we introduce MM-Coder, a Multilingual\nMultimodal software developer. MM-Coder integrates visual design inputs-Unified\nModeling Language (UML) diagrams and flowcharts (termed Visual Workflow)-with\ntextual instructions to enhance code generation accuracy and architectural\nalignment. To enable this, we developed MMc-Instruct, a diverse multimodal\ninstruction-tuning dataset including visual-workflow-based code generation,\nallowing MM-Coder to synthesize textual and graphical information like human\ndevelopers, distinct from prior work on narrow tasks. Furthermore, we introduce\nMMEval, a new benchmark for evaluating multimodal code generation, addressing\nexisting text-only limitations. Our evaluations using MMEval highlight\nsignificant remaining challenges for models in precise visual information\ncapture, instruction following, and advanced programming knowledge. Our work\naims to revolutionize industrial programming by enabling LLMs to interpret and\nimplement complex specifications conveyed through both text and visual designs.\n", "link": "http://arxiv.org/abs/2507.08719v1", "date": "2025-07-11", "relevancy": 2.6425, "topK": [{"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5352}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5252}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5252}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Multilingual%20Multimodal%20Software%20Developer%20for%20Code%20Generation&body=Title%3A%20Multilingual%20Multimodal%20Software%20Developer%20for%20Code%20Generation%0AAuthor%3A%20Linzheng%20Chai%20and%20Jian%20Yang%20and%20Shukai%20Liu%20and%20Wei%20Zhang%20and%20Liran%20Wang%20and%20Ke%20Jin%20and%20Tao%20Sun%20and%20Congnan%20Liu%20and%20Chenchen%20Zhang%20and%20Hualei%20Zhu%20and%20Jiaheng%20Liu%20and%20Xianjie%20Wu%20and%20Ge%20Zhang%20and%20Tianyu%20Liu%20and%20Zhoujun%20Li%0AAbstract%3A%20%20%20The%20rapid%20advancement%20of%20Large%20Language%20Models%20%28LLMs%29%20has%20significantly%0Aimproved%20code%20generation%2C%20yet%20most%20models%20remain%20text-only%2C%20neglecting%20crucial%0Avisual%20aids%20like%20diagrams%20and%20flowcharts%20used%20in%20real-world%20software%0Adevelopment.%20To%20bridge%20this%20gap%2C%20we%20introduce%20MM-Coder%2C%20a%20Multilingual%0AMultimodal%20software%20developer.%20MM-Coder%20integrates%20visual%20design%20inputs-Unified%0AModeling%20Language%20%28UML%29%20diagrams%20and%20flowcharts%20%28termed%20Visual%20Workflow%29-with%0Atextual%20instructions%20to%20enhance%20code%20generation%20accuracy%20and%20architectural%0Aalignment.%20To%20enable%20this%2C%20we%20developed%20MMc-Instruct%2C%20a%20diverse%20multimodal%0Ainstruction-tuning%20dataset%20including%20visual-workflow-based%20code%20generation%2C%0Aallowing%20MM-Coder%20to%20synthesize%20textual%20and%20graphical%20information%20like%20human%0Adevelopers%2C%20distinct%20from%20prior%20work%20on%20narrow%20tasks.%20Furthermore%2C%20we%20introduce%0AMMEval%2C%20a%20new%20benchmark%20for%20evaluating%20multimodal%20code%20generation%2C%20addressing%0Aexisting%20text-only%20limitations.%20Our%20evaluations%20using%20MMEval%20highlight%0Asignificant%20remaining%20challenges%20for%20models%20in%20precise%20visual%20information%0Acapture%2C%20instruction%20following%2C%20and%20advanced%20programming%20knowledge.%20Our%20work%0Aaims%20to%20revolutionize%20industrial%20programming%20by%20enabling%20LLMs%20to%20interpret%20and%0Aimplement%20complex%20specifications%20conveyed%20through%20both%20text%20and%20visual%20designs.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2507.08719v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DMultilingual%2520Multimodal%2520Software%2520Developer%2520for%2520Code%2520Generation%26entry.906535625%3DLinzheng%2520Chai%2520and%2520Jian%2520Yang%2520and%2520Shukai%2520Liu%2520and%2520Wei%2520Zhang%2520and%2520Liran%2520Wang%2520and%2520Ke%2520Jin%2520and%2520Tao%2520Sun%2520and%2520Congnan%2520Liu%2520and%2520Chenchen%2520Zhang%2520and%2520Hualei%2520Zhu%2520and%2520Jiaheng%2520Liu%2520and%2520Xianjie%2520Wu%2520and%2520Ge%2520Zhang%2520and%2520Tianyu%2520Liu%2520and%2520Zhoujun%2520Li%26entry.1292438233%3D%2520%2520The%2520rapid%2520advancement%2520of%2520Large%2520Language%2520Models%2520%2528LLMs%2529%2520has%2520significantly%250Aimproved%2520code%2520generation%252C%2520yet%2520most%2520models%2520remain%2520text-only%252C%2520neglecting%2520crucial%250Avisual%2520aids%2520like%2520diagrams%2520and%2520flowcharts%2520used%2520in%2520real-world%2520software%250Adevelopment.%2520To%2520bridge%2520this%2520gap%252C%2520we%2520introduce%2520MM-Coder%252C%2520a%2520Multilingual%250AMultimodal%2520software%2520developer.%2520MM-Coder%2520integrates%2520visual%2520design%2520inputs-Unified%250AModeling%2520Language%2520%2528UML%2529%2520diagrams%2520and%2520flowcharts%2520%2528termed%2520Visual%2520Workflow%2529-with%250Atextual%2520instructions%2520to%2520enhance%2520code%2520generation%2520accuracy%2520and%2520architectural%250Aalignment.%2520To%2520enable%2520this%252C%2520we%2520developed%2520MMc-Instruct%252C%2520a%2520diverse%2520multimodal%250Ainstruction-tuning%2520dataset%2520including%2520visual-workflow-based%2520code%2520generation%252C%250Aallowing%2520MM-Coder%2520to%2520synthesize%2520textual%2520and%2520graphical%2520information%2520like%2520human%250Adevelopers%252C%2520distinct%2520from%2520prior%2520work%2520on%2520narrow%2520tasks.%2520Furthermore%252C%2520we%2520introduce%250AMMEval%252C%2520a%2520new%2520benchmark%2520for%2520evaluating%2520multimodal%2520code%2520generation%252C%2520addressing%250Aexisting%2520text-only%2520limitations.%2520Our%2520evaluations%2520using%2520MMEval%2520highlight%250Asignificant%2520remaining%2520challenges%2520for%2520models%2520in%2520precise%2520visual%2520information%250Acapture%252C%2520instruction%2520following%252C%2520and%2520advanced%2520programming%2520knowledge.%2520Our%2520work%250Aaims%2520to%2520revolutionize%2520industrial%2520programming%2520by%2520enabling%2520LLMs%2520to%2520interpret%2520and%250Aimplement%2520complex%2520specifications%2520conveyed%2520through%2520both%2520text%2520and%2520visual%2520designs.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2507.08719v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Multilingual%20Multimodal%20Software%20Developer%20for%20Code%20Generation&entry.906535625=Linzheng%20Chai%20and%20Jian%20Yang%20and%20Shukai%20Liu%20and%20Wei%20Zhang%20and%20Liran%20Wang%20and%20Ke%20Jin%20and%20Tao%20Sun%20and%20Congnan%20Liu%20and%20Chenchen%20Zhang%20and%20Hualei%20Zhu%20and%20Jiaheng%20Liu%20and%20Xianjie%20Wu%20and%20Ge%20Zhang%20and%20Tianyu%20Liu%20and%20Zhoujun%20Li&entry.1292438233=%20%20The%20rapid%20advancement%20of%20Large%20Language%20Models%20%28LLMs%29%20has%20significantly%0Aimproved%20code%20generation%2C%20yet%20most%20models%20remain%20text-only%2C%20neglecting%20crucial%0Avisual%20aids%20like%20diagrams%20and%20flowcharts%20used%20in%20real-world%20software%0Adevelopment.%20To%20bridge%20this%20gap%2C%20we%20introduce%20MM-Coder%2C%20a%20Multilingual%0AMultimodal%20software%20developer.%20MM-Coder%20integrates%20visual%20design%20inputs-Unified%0AModeling%20Language%20%28UML%29%20diagrams%20and%20flowcharts%20%28termed%20Visual%20Workflow%29-with%0Atextual%20instructions%20to%20enhance%20code%20generation%20accuracy%20and%20architectural%0Aalignment.%20To%20enable%20this%2C%20we%20developed%20MMc-Instruct%2C%20a%20diverse%20multimodal%0Ainstruction-tuning%20dataset%20including%20visual-workflow-based%20code%20generation%2C%0Aallowing%20MM-Coder%20to%20synthesize%20textual%20and%20graphical%20information%20like%20human%0Adevelopers%2C%20distinct%20from%20prior%20work%20on%20narrow%20tasks.%20Furthermore%2C%20we%20introduce%0AMMEval%2C%20a%20new%20benchmark%20for%20evaluating%20multimodal%20code%20generation%2C%20addressing%0Aexisting%20text-only%20limitations.%20Our%20evaluations%20using%20MMEval%20highlight%0Asignificant%20remaining%20challenges%20for%20models%20in%20precise%20visual%20information%0Acapture%2C%20instruction%20following%2C%20and%20advanced%20programming%20knowledge.%20Our%20work%0Aaims%20to%20revolutionize%20industrial%20programming%20by%20enabling%20LLMs%20to%20interpret%20and%0Aimplement%20complex%20specifications%20conveyed%20through%20both%20text%20and%20visual%20designs.%0A&entry.1838667208=http%3A//arxiv.org/abs/2507.08719v1&entry.124074799=Read"},
{"title": "L-CLIPScore: a Lightweight Embedding-based Captioning Metric for\n  Evaluating and Training", "author": "Li Li and Yingzhe Peng and Xu Yang and Ruoxi Cheng and Haiyang Xu and Ming Yan and Fei Huang", "abstract": "  We propose a novel embedding-based captioning metric termed as L-CLIPScore\nthat can be used for efficiently evaluating caption quality and training\ncaptioning model. L-CLIPScore is calculated from a lightweight CLIP (L-CLIP),\nwhich is a dual-encoder architecture compressed and distilled from CLIP. To\ncompress, we apply two powerful techniques which are weight multiplexing and\nmatrix decomposition for reducing the parameters of encoders and word embedding\nmatrix, respectively. To distill, we design a novel multi-modal Similarity\nRegulator (SR) loss to transfer more vision-language alignment knowledge.\nSpecifically, SR loss amplifies the multi-modal embedding similarity if the\ngiven image-text pair is matched and diminishes the similarity if the pair is\nnon-matched. By compressing and distilling by this novel SR loss, our L-CLIP\nachieves comparable multi-modal alignment ability to the original CLIP while it\nrequires fewer computation resources and running time. We carry out exhaustive\nexperiments to validate the efficiency and effectiveness of L-CLIPScore when\nusing it as the judge to evaluate caption quality. We also discover that when\nusing L-CLIPScore as the supervisor to train the captioning model, it should be\nmixed up by an n-gram-based metric and meanwhile analyze why using L-CLIPScore\nonly will cause fail training.\n", "link": "http://arxiv.org/abs/2507.08710v1", "date": "2025-07-11", "relevancy": 2.5987, "topK": [{"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5493}, {"title": "Self-supervised Photographic Image Layout Representation Learning", "link": "http://arxiv.org/abs/2403.03740v1", "similarity": 0.5164}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.4936}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20L-CLIPScore%3A%20a%20Lightweight%20Embedding-based%20Captioning%20Metric%20for%0A%20%20Evaluating%20and%20Training&body=Title%3A%20L-CLIPScore%3A%20a%20Lightweight%20Embedding-based%20Captioning%20Metric%20for%0A%20%20Evaluating%20and%20Training%0AAuthor%3A%20Li%20Li%20and%20Yingzhe%20Peng%20and%20Xu%20Yang%20and%20Ruoxi%20Cheng%20and%20Haiyang%20Xu%20and%20Ming%20Yan%20and%20Fei%20Huang%0AAbstract%3A%20%20%20We%20propose%20a%20novel%20embedding-based%20captioning%20metric%20termed%20as%20L-CLIPScore%0Athat%20can%20be%20used%20for%20efficiently%20evaluating%20caption%20quality%20and%20training%0Acaptioning%20model.%20L-CLIPScore%20is%20calculated%20from%20a%20lightweight%20CLIP%20%28L-CLIP%29%2C%0Awhich%20is%20a%20dual-encoder%20architecture%20compressed%20and%20distilled%20from%20CLIP.%20To%0Acompress%2C%20we%20apply%20two%20powerful%20techniques%20which%20are%20weight%20multiplexing%20and%0Amatrix%20decomposition%20for%20reducing%20the%20parameters%20of%20encoders%20and%20word%20embedding%0Amatrix%2C%20respectively.%20To%20distill%2C%20we%20design%20a%20novel%20multi-modal%20Similarity%0ARegulator%20%28SR%29%20loss%20to%20transfer%20more%20vision-language%20alignment%20knowledge.%0ASpecifically%2C%20SR%20loss%20amplifies%20the%20multi-modal%20embedding%20similarity%20if%20the%0Agiven%20image-text%20pair%20is%20matched%20and%20diminishes%20the%20similarity%20if%20the%20pair%20is%0Anon-matched.%20By%20compressing%20and%20distilling%20by%20this%20novel%20SR%20loss%2C%20our%20L-CLIP%0Aachieves%20comparable%20multi-modal%20alignment%20ability%20to%20the%20original%20CLIP%20while%20it%0Arequires%20fewer%20computation%20resources%20and%20running%20time.%20We%20carry%20out%20exhaustive%0Aexperiments%20to%20validate%20the%20efficiency%20and%20effectiveness%20of%20L-CLIPScore%20when%0Ausing%20it%20as%20the%20judge%20to%20evaluate%20caption%20quality.%20We%20also%20discover%20that%20when%0Ausing%20L-CLIPScore%20as%20the%20supervisor%20to%20train%20the%20captioning%20model%2C%20it%20should%20be%0Amixed%20up%20by%20an%20n-gram-based%20metric%20and%20meanwhile%20analyze%20why%20using%20L-CLIPScore%0Aonly%20will%20cause%20fail%20training.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2507.08710v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DL-CLIPScore%253A%2520a%2520Lightweight%2520Embedding-based%2520Captioning%2520Metric%2520for%250A%2520%2520Evaluating%2520and%2520Training%26entry.906535625%3DLi%2520Li%2520and%2520Yingzhe%2520Peng%2520and%2520Xu%2520Yang%2520and%2520Ruoxi%2520Cheng%2520and%2520Haiyang%2520Xu%2520and%2520Ming%2520Yan%2520and%2520Fei%2520Huang%26entry.1292438233%3D%2520%2520We%2520propose%2520a%2520novel%2520embedding-based%2520captioning%2520metric%2520termed%2520as%2520L-CLIPScore%250Athat%2520can%2520be%2520used%2520for%2520efficiently%2520evaluating%2520caption%2520quality%2520and%2520training%250Acaptioning%2520model.%2520L-CLIPScore%2520is%2520calculated%2520from%2520a%2520lightweight%2520CLIP%2520%2528L-CLIP%2529%252C%250Awhich%2520is%2520a%2520dual-encoder%2520architecture%2520compressed%2520and%2520distilled%2520from%2520CLIP.%2520To%250Acompress%252C%2520we%2520apply%2520two%2520powerful%2520techniques%2520which%2520are%2520weight%2520multiplexing%2520and%250Amatrix%2520decomposition%2520for%2520reducing%2520the%2520parameters%2520of%2520encoders%2520and%2520word%2520embedding%250Amatrix%252C%2520respectively.%2520To%2520distill%252C%2520we%2520design%2520a%2520novel%2520multi-modal%2520Similarity%250ARegulator%2520%2528SR%2529%2520loss%2520to%2520transfer%2520more%2520vision-language%2520alignment%2520knowledge.%250ASpecifically%252C%2520SR%2520loss%2520amplifies%2520the%2520multi-modal%2520embedding%2520similarity%2520if%2520the%250Agiven%2520image-text%2520pair%2520is%2520matched%2520and%2520diminishes%2520the%2520similarity%2520if%2520the%2520pair%2520is%250Anon-matched.%2520By%2520compressing%2520and%2520distilling%2520by%2520this%2520novel%2520SR%2520loss%252C%2520our%2520L-CLIP%250Aachieves%2520comparable%2520multi-modal%2520alignment%2520ability%2520to%2520the%2520original%2520CLIP%2520while%2520it%250Arequires%2520fewer%2520computation%2520resources%2520and%2520running%2520time.%2520We%2520carry%2520out%2520exhaustive%250Aexperiments%2520to%2520validate%2520the%2520efficiency%2520and%2520effectiveness%2520of%2520L-CLIPScore%2520when%250Ausing%2520it%2520as%2520the%2520judge%2520to%2520evaluate%2520caption%2520quality.%2520We%2520also%2520discover%2520that%2520when%250Ausing%2520L-CLIPScore%2520as%2520the%2520supervisor%2520to%2520train%2520the%2520captioning%2520model%252C%2520it%2520should%2520be%250Amixed%2520up%2520by%2520an%2520n-gram-based%2520metric%2520and%2520meanwhile%2520analyze%2520why%2520using%2520L-CLIPScore%250Aonly%2520will%2520cause%2520fail%2520training.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2507.08710v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=L-CLIPScore%3A%20a%20Lightweight%20Embedding-based%20Captioning%20Metric%20for%0A%20%20Evaluating%20and%20Training&entry.906535625=Li%20Li%20and%20Yingzhe%20Peng%20and%20Xu%20Yang%20and%20Ruoxi%20Cheng%20and%20Haiyang%20Xu%20and%20Ming%20Yan%20and%20Fei%20Huang&entry.1292438233=%20%20We%20propose%20a%20novel%20embedding-based%20captioning%20metric%20termed%20as%20L-CLIPScore%0Athat%20can%20be%20used%20for%20efficiently%20evaluating%20caption%20quality%20and%20training%0Acaptioning%20model.%20L-CLIPScore%20is%20calculated%20from%20a%20lightweight%20CLIP%20%28L-CLIP%29%2C%0Awhich%20is%20a%20dual-encoder%20architecture%20compressed%20and%20distilled%20from%20CLIP.%20To%0Acompress%2C%20we%20apply%20two%20powerful%20techniques%20which%20are%20weight%20multiplexing%20and%0Amatrix%20decomposition%20for%20reducing%20the%20parameters%20of%20encoders%20and%20word%20embedding%0Amatrix%2C%20respectively.%20To%20distill%2C%20we%20design%20a%20novel%20multi-modal%20Similarity%0ARegulator%20%28SR%29%20loss%20to%20transfer%20more%20vision-language%20alignment%20knowledge.%0ASpecifically%2C%20SR%20loss%20amplifies%20the%20multi-modal%20embedding%20similarity%20if%20the%0Agiven%20image-text%20pair%20is%20matched%20and%20diminishes%20the%20similarity%20if%20the%20pair%20is%0Anon-matched.%20By%20compressing%20and%20distilling%20by%20this%20novel%20SR%20loss%2C%20our%20L-CLIP%0Aachieves%20comparable%20multi-modal%20alignment%20ability%20to%20the%20original%20CLIP%20while%20it%0Arequires%20fewer%20computation%20resources%20and%20running%20time.%20We%20carry%20out%20exhaustive%0Aexperiments%20to%20validate%20the%20efficiency%20and%20effectiveness%20of%20L-CLIPScore%20when%0Ausing%20it%20as%20the%20judge%20to%20evaluate%20caption%20quality.%20We%20also%20discover%20that%20when%0Ausing%20L-CLIPScore%20as%20the%20supervisor%20to%20train%20the%20captioning%20model%2C%20it%20should%20be%0Amixed%20up%20by%20an%20n-gram-based%20metric%20and%20meanwhile%20analyze%20why%20using%20L-CLIPScore%0Aonly%20will%20cause%20fail%20training.%0A&entry.1838667208=http%3A//arxiv.org/abs/2507.08710v1&entry.124074799=Read"},
{"title": "One Token to Fool LLM-as-a-Judge", "author": "Yulai Zhao and Haolin Liu and Dian Yu and S. Y. Kung and Haitao Mi and Dong Yu", "abstract": "  Generative reward models (also known as LLMs-as-judges), which use large\nlanguage models (LLMs) to evaluate answer quality, are increasingly adopted in\nreinforcement learning with verifiable rewards (RLVR). They are often preferred\nover rigid rule-based metrics, especially for complex reasoning tasks involving\nfree-form outputs. In this paradigm, an LLM is typically prompted to compare a\ncandidate answer against a ground-truth reference and assign a binary reward\nindicating correctness. Despite the seeming simplicity of this comparison task,\nwe find that generative reward models exhibit surprising vulnerabilities to\nsuperficial manipulations: non-word symbols (e.g., \":\" or \".\") or reasoning\nopeners like \"Thought process:\" and \"Let's solve this problem step by step.\"\ncan often lead to false positive rewards. We demonstrate that this weakness is\nwidespread across LLMs, datasets, and prompt formats, posing a serious threat\nfor core algorithmic paradigms that rely on generative reward models, such as\nrejection sampling, preference optimization, and RLVR. To mitigate this issue,\nwe introduce a simple yet effective data augmentation strategy and train a new\ngenerative reward model with substantially improved robustness. Our findings\nhighlight the urgent need for more reliable LLM-based evaluation methods. We\nrelease our robust, general-domain reward model and its synthetic training data\nat https://huggingface.co/sarosavo/Master-RM and\nhttps://huggingface.co/datasets/sarosavo/Master-RM.\n", "link": "http://arxiv.org/abs/2507.08794v1", "date": "2025-07-11", "relevancy": 2.5798, "topK": [{"title": "Genie: Generative Interactive Environments", "link": "http://arxiv.org/abs/2402.15391v1", "similarity": 0.5218}, {"title": "Flexible Visual Recognition by Evidential Modeling of Confusion and Ignorance", "link": "http://openaccess.thecvf.com/content/ICCV2023/html/Fan_Flexible_Visual_Recognition_by_Evidential_Modeling_of_Confusion_and_Ignorance_ICCV_2023_paper.html", "similarity": 0.5196}, {"title": "ToDo: Token Downsampling for Efficient Generation of High-Resolution\n  Images", "link": "http://arxiv.org/abs/2402.13573v2", "similarity": 0.5065}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20One%20Token%20to%20Fool%20LLM-as-a-Judge&body=Title%3A%20One%20Token%20to%20Fool%20LLM-as-a-Judge%0AAuthor%3A%20Yulai%20Zhao%20and%20Haolin%20Liu%20and%20Dian%20Yu%20and%20S.%20Y.%20Kung%20and%20Haitao%20Mi%20and%20Dong%20Yu%0AAbstract%3A%20%20%20Generative%20reward%20models%20%28also%20known%20as%20LLMs-as-judges%29%2C%20which%20use%20large%0Alanguage%20models%20%28LLMs%29%20to%20evaluate%20answer%20quality%2C%20are%20increasingly%20adopted%20in%0Areinforcement%20learning%20with%20verifiable%20rewards%20%28RLVR%29.%20They%20are%20often%20preferred%0Aover%20rigid%20rule-based%20metrics%2C%20especially%20for%20complex%20reasoning%20tasks%20involving%0Afree-form%20outputs.%20In%20this%20paradigm%2C%20an%20LLM%20is%20typically%20prompted%20to%20compare%20a%0Acandidate%20answer%20against%20a%20ground-truth%20reference%20and%20assign%20a%20binary%20reward%0Aindicating%20correctness.%20Despite%20the%20seeming%20simplicity%20of%20this%20comparison%20task%2C%0Awe%20find%20that%20generative%20reward%20models%20exhibit%20surprising%20vulnerabilities%20to%0Asuperficial%20manipulations%3A%20non-word%20symbols%20%28e.g.%2C%20%22%3A%22%20or%20%22.%22%29%20or%20reasoning%0Aopeners%20like%20%22Thought%20process%3A%22%20and%20%22Let%27s%20solve%20this%20problem%20step%20by%20step.%22%0Acan%20often%20lead%20to%20false%20positive%20rewards.%20We%20demonstrate%20that%20this%20weakness%20is%0Awidespread%20across%20LLMs%2C%20datasets%2C%20and%20prompt%20formats%2C%20posing%20a%20serious%20threat%0Afor%20core%20algorithmic%20paradigms%20that%20rely%20on%20generative%20reward%20models%2C%20such%20as%0Arejection%20sampling%2C%20preference%20optimization%2C%20and%20RLVR.%20To%20mitigate%20this%20issue%2C%0Awe%20introduce%20a%20simple%20yet%20effective%20data%20augmentation%20strategy%20and%20train%20a%20new%0Agenerative%20reward%20model%20with%20substantially%20improved%20robustness.%20Our%20findings%0Ahighlight%20the%20urgent%20need%20for%20more%20reliable%20LLM-based%20evaluation%20methods.%20We%0Arelease%20our%20robust%2C%20general-domain%20reward%20model%20and%20its%20synthetic%20training%20data%0Aat%20https%3A//huggingface.co/sarosavo/Master-RM%20and%0Ahttps%3A//huggingface.co/datasets/sarosavo/Master-RM.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2507.08794v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DOne%2520Token%2520to%2520Fool%2520LLM-as-a-Judge%26entry.906535625%3DYulai%2520Zhao%2520and%2520Haolin%2520Liu%2520and%2520Dian%2520Yu%2520and%2520S.%2520Y.%2520Kung%2520and%2520Haitao%2520Mi%2520and%2520Dong%2520Yu%26entry.1292438233%3D%2520%2520Generative%2520reward%2520models%2520%2528also%2520known%2520as%2520LLMs-as-judges%2529%252C%2520which%2520use%2520large%250Alanguage%2520models%2520%2528LLMs%2529%2520to%2520evaluate%2520answer%2520quality%252C%2520are%2520increasingly%2520adopted%2520in%250Areinforcement%2520learning%2520with%2520verifiable%2520rewards%2520%2528RLVR%2529.%2520They%2520are%2520often%2520preferred%250Aover%2520rigid%2520rule-based%2520metrics%252C%2520especially%2520for%2520complex%2520reasoning%2520tasks%2520involving%250Afree-form%2520outputs.%2520In%2520this%2520paradigm%252C%2520an%2520LLM%2520is%2520typically%2520prompted%2520to%2520compare%2520a%250Acandidate%2520answer%2520against%2520a%2520ground-truth%2520reference%2520and%2520assign%2520a%2520binary%2520reward%250Aindicating%2520correctness.%2520Despite%2520the%2520seeming%2520simplicity%2520of%2520this%2520comparison%2520task%252C%250Awe%2520find%2520that%2520generative%2520reward%2520models%2520exhibit%2520surprising%2520vulnerabilities%2520to%250Asuperficial%2520manipulations%253A%2520non-word%2520symbols%2520%2528e.g.%252C%2520%2522%253A%2522%2520or%2520%2522.%2522%2529%2520or%2520reasoning%250Aopeners%2520like%2520%2522Thought%2520process%253A%2522%2520and%2520%2522Let%2527s%2520solve%2520this%2520problem%2520step%2520by%2520step.%2522%250Acan%2520often%2520lead%2520to%2520false%2520positive%2520rewards.%2520We%2520demonstrate%2520that%2520this%2520weakness%2520is%250Awidespread%2520across%2520LLMs%252C%2520datasets%252C%2520and%2520prompt%2520formats%252C%2520posing%2520a%2520serious%2520threat%250Afor%2520core%2520algorithmic%2520paradigms%2520that%2520rely%2520on%2520generative%2520reward%2520models%252C%2520such%2520as%250Arejection%2520sampling%252C%2520preference%2520optimization%252C%2520and%2520RLVR.%2520To%2520mitigate%2520this%2520issue%252C%250Awe%2520introduce%2520a%2520simple%2520yet%2520effective%2520data%2520augmentation%2520strategy%2520and%2520train%2520a%2520new%250Agenerative%2520reward%2520model%2520with%2520substantially%2520improved%2520robustness.%2520Our%2520findings%250Ahighlight%2520the%2520urgent%2520need%2520for%2520more%2520reliable%2520LLM-based%2520evaluation%2520methods.%2520We%250Arelease%2520our%2520robust%252C%2520general-domain%2520reward%2520model%2520and%2520its%2520synthetic%2520training%2520data%250Aat%2520https%253A//huggingface.co/sarosavo/Master-RM%2520and%250Ahttps%253A//huggingface.co/datasets/sarosavo/Master-RM.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2507.08794v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=One%20Token%20to%20Fool%20LLM-as-a-Judge&entry.906535625=Yulai%20Zhao%20and%20Haolin%20Liu%20and%20Dian%20Yu%20and%20S.%20Y.%20Kung%20and%20Haitao%20Mi%20and%20Dong%20Yu&entry.1292438233=%20%20Generative%20reward%20models%20%28also%20known%20as%20LLMs-as-judges%29%2C%20which%20use%20large%0Alanguage%20models%20%28LLMs%29%20to%20evaluate%20answer%20quality%2C%20are%20increasingly%20adopted%20in%0Areinforcement%20learning%20with%20verifiable%20rewards%20%28RLVR%29.%20They%20are%20often%20preferred%0Aover%20rigid%20rule-based%20metrics%2C%20especially%20for%20complex%20reasoning%20tasks%20involving%0Afree-form%20outputs.%20In%20this%20paradigm%2C%20an%20LLM%20is%20typically%20prompted%20to%20compare%20a%0Acandidate%20answer%20against%20a%20ground-truth%20reference%20and%20assign%20a%20binary%20reward%0Aindicating%20correctness.%20Despite%20the%20seeming%20simplicity%20of%20this%20comparison%20task%2C%0Awe%20find%20that%20generative%20reward%20models%20exhibit%20surprising%20vulnerabilities%20to%0Asuperficial%20manipulations%3A%20non-word%20symbols%20%28e.g.%2C%20%22%3A%22%20or%20%22.%22%29%20or%20reasoning%0Aopeners%20like%20%22Thought%20process%3A%22%20and%20%22Let%27s%20solve%20this%20problem%20step%20by%20step.%22%0Acan%20often%20lead%20to%20false%20positive%20rewards.%20We%20demonstrate%20that%20this%20weakness%20is%0Awidespread%20across%20LLMs%2C%20datasets%2C%20and%20prompt%20formats%2C%20posing%20a%20serious%20threat%0Afor%20core%20algorithmic%20paradigms%20that%20rely%20on%20generative%20reward%20models%2C%20such%20as%0Arejection%20sampling%2C%20preference%20optimization%2C%20and%20RLVR.%20To%20mitigate%20this%20issue%2C%0Awe%20introduce%20a%20simple%20yet%20effective%20data%20augmentation%20strategy%20and%20train%20a%20new%0Agenerative%20reward%20model%20with%20substantially%20improved%20robustness.%20Our%20findings%0Ahighlight%20the%20urgent%20need%20for%20more%20reliable%20LLM-based%20evaluation%20methods.%20We%0Arelease%20our%20robust%2C%20general-domain%20reward%20model%20and%20its%20synthetic%20training%20data%0Aat%20https%3A//huggingface.co/sarosavo/Master-RM%20and%0Ahttps%3A//huggingface.co/datasets/sarosavo/Master-RM.%0A&entry.1838667208=http%3A//arxiv.org/abs/2507.08794v1&entry.124074799=Read"},
{"title": "SGPMIL: Sparse Gaussian Process Multiple Instance Learning", "author": "Andreas Lolos and Stergios Christodoulidis and Maria Vakalopoulou and Jose Dolz and Aris Moustakas", "abstract": "  Multiple Instance Learning (MIL) offers a natural solution for settings where\nonly coarse, bag-level labels are available, without having access to\ninstance-level annotations. This is usually the case in digital pathology,\nwhich consists of gigapixel sized images. While deterministic attention-based\nMIL approaches achieve strong bag-level performance, they often overlook the\nuncertainty inherent in instance relevance. In this paper, we address the lack\nof uncertainty quantification in instance-level attention scores by introducing\n\\textbf{SGPMIL}, a new probabilistic attention-based MIL framework grounded in\nSparse Gaussian Processes (SGP). By learning a posterior distribution over\nattention scores, SGPMIL enables principled uncertainty estimation, resulting\nin more reliable and calibrated instance relevance maps. Our approach not only\npreserves competitive bag-level performance but also significantly improves the\nquality and interpretability of instance-level predictions under uncertainty.\nSGPMIL extends prior work by introducing feature scaling in the SGP predictive\nmean function, leading to faster training, improved efficiency, and enhanced\ninstance-level performance. Extensive experiments on multiple well-established\ndigital pathology datasets highlight the effectiveness of our approach across\nboth bag- and instance-level evaluations. Our code will be made publicly\navailable.\n", "link": "http://arxiv.org/abs/2507.08711v1", "date": "2025-07-11", "relevancy": 2.5661, "topK": [{"title": "GST: Precise 3D Human Body from a Single Image with Gaussian Splatting\n  Transformers", "link": "http://arxiv.org/abs/2409.04196v1", "similarity": 0.536}, {"title": "RTG-SLAM: Real-time 3D Reconstruction at Scale using Gaussian Splatting", "link": "http://arxiv.org/abs/2404.19706v1", "similarity": 0.5034}, {"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.5004}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20SGPMIL%3A%20Sparse%20Gaussian%20Process%20Multiple%20Instance%20Learning&body=Title%3A%20SGPMIL%3A%20Sparse%20Gaussian%20Process%20Multiple%20Instance%20Learning%0AAuthor%3A%20Andreas%20Lolos%20and%20Stergios%20Christodoulidis%20and%20Maria%20Vakalopoulou%20and%20Jose%20Dolz%20and%20Aris%20Moustakas%0AAbstract%3A%20%20%20Multiple%20Instance%20Learning%20%28MIL%29%20offers%20a%20natural%20solution%20for%20settings%20where%0Aonly%20coarse%2C%20bag-level%20labels%20are%20available%2C%20without%20having%20access%20to%0Ainstance-level%20annotations.%20This%20is%20usually%20the%20case%20in%20digital%20pathology%2C%0Awhich%20consists%20of%20gigapixel%20sized%20images.%20While%20deterministic%20attention-based%0AMIL%20approaches%20achieve%20strong%20bag-level%20performance%2C%20they%20often%20overlook%20the%0Auncertainty%20inherent%20in%20instance%20relevance.%20In%20this%20paper%2C%20we%20address%20the%20lack%0Aof%20uncertainty%20quantification%20in%20instance-level%20attention%20scores%20by%20introducing%0A%5Ctextbf%7BSGPMIL%7D%2C%20a%20new%20probabilistic%20attention-based%20MIL%20framework%20grounded%20in%0ASparse%20Gaussian%20Processes%20%28SGP%29.%20By%20learning%20a%20posterior%20distribution%20over%0Aattention%20scores%2C%20SGPMIL%20enables%20principled%20uncertainty%20estimation%2C%20resulting%0Ain%20more%20reliable%20and%20calibrated%20instance%20relevance%20maps.%20Our%20approach%20not%20only%0Apreserves%20competitive%20bag-level%20performance%20but%20also%20significantly%20improves%20the%0Aquality%20and%20interpretability%20of%20instance-level%20predictions%20under%20uncertainty.%0ASGPMIL%20extends%20prior%20work%20by%20introducing%20feature%20scaling%20in%20the%20SGP%20predictive%0Amean%20function%2C%20leading%20to%20faster%20training%2C%20improved%20efficiency%2C%20and%20enhanced%0Ainstance-level%20performance.%20Extensive%20experiments%20on%20multiple%20well-established%0Adigital%20pathology%20datasets%20highlight%20the%20effectiveness%20of%20our%20approach%20across%0Aboth%20bag-%20and%20instance-level%20evaluations.%20Our%20code%20will%20be%20made%20publicly%0Aavailable.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2507.08711v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DSGPMIL%253A%2520Sparse%2520Gaussian%2520Process%2520Multiple%2520Instance%2520Learning%26entry.906535625%3DAndreas%2520Lolos%2520and%2520Stergios%2520Christodoulidis%2520and%2520Maria%2520Vakalopoulou%2520and%2520Jose%2520Dolz%2520and%2520Aris%2520Moustakas%26entry.1292438233%3D%2520%2520Multiple%2520Instance%2520Learning%2520%2528MIL%2529%2520offers%2520a%2520natural%2520solution%2520for%2520settings%2520where%250Aonly%2520coarse%252C%2520bag-level%2520labels%2520are%2520available%252C%2520without%2520having%2520access%2520to%250Ainstance-level%2520annotations.%2520This%2520is%2520usually%2520the%2520case%2520in%2520digital%2520pathology%252C%250Awhich%2520consists%2520of%2520gigapixel%2520sized%2520images.%2520While%2520deterministic%2520attention-based%250AMIL%2520approaches%2520achieve%2520strong%2520bag-level%2520performance%252C%2520they%2520often%2520overlook%2520the%250Auncertainty%2520inherent%2520in%2520instance%2520relevance.%2520In%2520this%2520paper%252C%2520we%2520address%2520the%2520lack%250Aof%2520uncertainty%2520quantification%2520in%2520instance-level%2520attention%2520scores%2520by%2520introducing%250A%255Ctextbf%257BSGPMIL%257D%252C%2520a%2520new%2520probabilistic%2520attention-based%2520MIL%2520framework%2520grounded%2520in%250ASparse%2520Gaussian%2520Processes%2520%2528SGP%2529.%2520By%2520learning%2520a%2520posterior%2520distribution%2520over%250Aattention%2520scores%252C%2520SGPMIL%2520enables%2520principled%2520uncertainty%2520estimation%252C%2520resulting%250Ain%2520more%2520reliable%2520and%2520calibrated%2520instance%2520relevance%2520maps.%2520Our%2520approach%2520not%2520only%250Apreserves%2520competitive%2520bag-level%2520performance%2520but%2520also%2520significantly%2520improves%2520the%250Aquality%2520and%2520interpretability%2520of%2520instance-level%2520predictions%2520under%2520uncertainty.%250ASGPMIL%2520extends%2520prior%2520work%2520by%2520introducing%2520feature%2520scaling%2520in%2520the%2520SGP%2520predictive%250Amean%2520function%252C%2520leading%2520to%2520faster%2520training%252C%2520improved%2520efficiency%252C%2520and%2520enhanced%250Ainstance-level%2520performance.%2520Extensive%2520experiments%2520on%2520multiple%2520well-established%250Adigital%2520pathology%2520datasets%2520highlight%2520the%2520effectiveness%2520of%2520our%2520approach%2520across%250Aboth%2520bag-%2520and%2520instance-level%2520evaluations.%2520Our%2520code%2520will%2520be%2520made%2520publicly%250Aavailable.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2507.08711v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=SGPMIL%3A%20Sparse%20Gaussian%20Process%20Multiple%20Instance%20Learning&entry.906535625=Andreas%20Lolos%20and%20Stergios%20Christodoulidis%20and%20Maria%20Vakalopoulou%20and%20Jose%20Dolz%20and%20Aris%20Moustakas&entry.1292438233=%20%20Multiple%20Instance%20Learning%20%28MIL%29%20offers%20a%20natural%20solution%20for%20settings%20where%0Aonly%20coarse%2C%20bag-level%20labels%20are%20available%2C%20without%20having%20access%20to%0Ainstance-level%20annotations.%20This%20is%20usually%20the%20case%20in%20digital%20pathology%2C%0Awhich%20consists%20of%20gigapixel%20sized%20images.%20While%20deterministic%20attention-based%0AMIL%20approaches%20achieve%20strong%20bag-level%20performance%2C%20they%20often%20overlook%20the%0Auncertainty%20inherent%20in%20instance%20relevance.%20In%20this%20paper%2C%20we%20address%20the%20lack%0Aof%20uncertainty%20quantification%20in%20instance-level%20attention%20scores%20by%20introducing%0A%5Ctextbf%7BSGPMIL%7D%2C%20a%20new%20probabilistic%20attention-based%20MIL%20framework%20grounded%20in%0ASparse%20Gaussian%20Processes%20%28SGP%29.%20By%20learning%20a%20posterior%20distribution%20over%0Aattention%20scores%2C%20SGPMIL%20enables%20principled%20uncertainty%20estimation%2C%20resulting%0Ain%20more%20reliable%20and%20calibrated%20instance%20relevance%20maps.%20Our%20approach%20not%20only%0Apreserves%20competitive%20bag-level%20performance%20but%20also%20significantly%20improves%20the%0Aquality%20and%20interpretability%20of%20instance-level%20predictions%20under%20uncertainty.%0ASGPMIL%20extends%20prior%20work%20by%20introducing%20feature%20scaling%20in%20the%20SGP%20predictive%0Amean%20function%2C%20leading%20to%20faster%20training%2C%20improved%20efficiency%2C%20and%20enhanced%0Ainstance-level%20performance.%20Extensive%20experiments%20on%20multiple%20well-established%0Adigital%20pathology%20datasets%20highlight%20the%20effectiveness%20of%20our%20approach%20across%0Aboth%20bag-%20and%20instance-level%20evaluations.%20Our%20code%20will%20be%20made%20publicly%0Aavailable.%0A&entry.1838667208=http%3A//arxiv.org/abs/2507.08711v1&entry.124074799=Read"},
{"title": "Learning human-to-robot handovers through 3D scene reconstruction", "author": "Yuekun Wu and Yik Lung Pang and Andrea Cavallaro and Changjae Oh", "abstract": "  Learning robot manipulation policies from raw, real-world image data requires\na large number of robot-action trials in the physical environment. Although\ntraining using simulations offers a cost-effective alternative, the visual\ndomain gap between simulation and robot workspace remains a major limitation.\nGaussian Splatting visual reconstruction methods have recently provided new\ndirections for robot manipulation by generating realistic environments. In this\npaper, we propose the first method for learning supervised-based robot\nhandovers solely from RGB images without the need of real-robot training or\nreal-robot data collection. The proposed policy learner, Human-to-Robot\nHandover using Sparse-View Gaussian Splatting (H2RH-SGS), leverages sparse-view\nGaussian Splatting reconstruction of human-to-robot handover scenes to generate\nrobot demonstrations containing image-action pairs captured with a camera\nmounted on the robot gripper. As a result, the simulated camera pose changes in\nthe reconstructed scene can be directly translated into gripper pose changes.\nWe train a robot policy on demonstrations collected with 16 household objects\nand {\\em directly} deploy this policy in the real environment. Experiments in\nboth Gaussian Splatting reconstructed scene and real-world human-to-robot\nhandover experiments demonstrate that H2RH-SGS serves as a new and effective\nrepresentation for the human-to-robot handover task.\n", "link": "http://arxiv.org/abs/2507.08726v1", "date": "2025-07-11", "relevancy": 2.5325, "topK": [{"title": "RTG-SLAM: Real-time 3D Reconstruction at Scale using Gaussian Splatting", "link": "http://arxiv.org/abs/2404.19706v1", "similarity": 0.6361}, {"title": "GST: Precise 3D Human Body from a Single Image with Gaussian Splatting\n  Transformers", "link": "http://arxiv.org/abs/2409.04196v1", "similarity": 0.6316}, {"title": "RoboEXP: Action-Conditioned Scene Graph via Interactive Exploration for\n  Robotic Manipulation", "link": "http://arxiv.org/abs/2402.15487v1", "similarity": 0.6294}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Learning%20human-to-robot%20handovers%20through%203D%20scene%20reconstruction&body=Title%3A%20Learning%20human-to-robot%20handovers%20through%203D%20scene%20reconstruction%0AAuthor%3A%20Yuekun%20Wu%20and%20Yik%20Lung%20Pang%20and%20Andrea%20Cavallaro%20and%20Changjae%20Oh%0AAbstract%3A%20%20%20Learning%20robot%20manipulation%20policies%20from%20raw%2C%20real-world%20image%20data%20requires%0Aa%20large%20number%20of%20robot-action%20trials%20in%20the%20physical%20environment.%20Although%0Atraining%20using%20simulations%20offers%20a%20cost-effective%20alternative%2C%20the%20visual%0Adomain%20gap%20between%20simulation%20and%20robot%20workspace%20remains%20a%20major%20limitation.%0AGaussian%20Splatting%20visual%20reconstruction%20methods%20have%20recently%20provided%20new%0Adirections%20for%20robot%20manipulation%20by%20generating%20realistic%20environments.%20In%20this%0Apaper%2C%20we%20propose%20the%20first%20method%20for%20learning%20supervised-based%20robot%0Ahandovers%20solely%20from%20RGB%20images%20without%20the%20need%20of%20real-robot%20training%20or%0Areal-robot%20data%20collection.%20The%20proposed%20policy%20learner%2C%20Human-to-Robot%0AHandover%20using%20Sparse-View%20Gaussian%20Splatting%20%28H2RH-SGS%29%2C%20leverages%20sparse-view%0AGaussian%20Splatting%20reconstruction%20of%20human-to-robot%20handover%20scenes%20to%20generate%0Arobot%20demonstrations%20containing%20image-action%20pairs%20captured%20with%20a%20camera%0Amounted%20on%20the%20robot%20gripper.%20As%20a%20result%2C%20the%20simulated%20camera%20pose%20changes%20in%0Athe%20reconstructed%20scene%20can%20be%20directly%20translated%20into%20gripper%20pose%20changes.%0AWe%20train%20a%20robot%20policy%20on%20demonstrations%20collected%20with%2016%20household%20objects%0Aand%20%7B%5Cem%20directly%7D%20deploy%20this%20policy%20in%20the%20real%20environment.%20Experiments%20in%0Aboth%20Gaussian%20Splatting%20reconstructed%20scene%20and%20real-world%20human-to-robot%0Ahandover%20experiments%20demonstrate%20that%20H2RH-SGS%20serves%20as%20a%20new%20and%20effective%0Arepresentation%20for%20the%20human-to-robot%20handover%20task.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2507.08726v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DLearning%2520human-to-robot%2520handovers%2520through%25203D%2520scene%2520reconstruction%26entry.906535625%3DYuekun%2520Wu%2520and%2520Yik%2520Lung%2520Pang%2520and%2520Andrea%2520Cavallaro%2520and%2520Changjae%2520Oh%26entry.1292438233%3D%2520%2520Learning%2520robot%2520manipulation%2520policies%2520from%2520raw%252C%2520real-world%2520image%2520data%2520requires%250Aa%2520large%2520number%2520of%2520robot-action%2520trials%2520in%2520the%2520physical%2520environment.%2520Although%250Atraining%2520using%2520simulations%2520offers%2520a%2520cost-effective%2520alternative%252C%2520the%2520visual%250Adomain%2520gap%2520between%2520simulation%2520and%2520robot%2520workspace%2520remains%2520a%2520major%2520limitation.%250AGaussian%2520Splatting%2520visual%2520reconstruction%2520methods%2520have%2520recently%2520provided%2520new%250Adirections%2520for%2520robot%2520manipulation%2520by%2520generating%2520realistic%2520environments.%2520In%2520this%250Apaper%252C%2520we%2520propose%2520the%2520first%2520method%2520for%2520learning%2520supervised-based%2520robot%250Ahandovers%2520solely%2520from%2520RGB%2520images%2520without%2520the%2520need%2520of%2520real-robot%2520training%2520or%250Areal-robot%2520data%2520collection.%2520The%2520proposed%2520policy%2520learner%252C%2520Human-to-Robot%250AHandover%2520using%2520Sparse-View%2520Gaussian%2520Splatting%2520%2528H2RH-SGS%2529%252C%2520leverages%2520sparse-view%250AGaussian%2520Splatting%2520reconstruction%2520of%2520human-to-robot%2520handover%2520scenes%2520to%2520generate%250Arobot%2520demonstrations%2520containing%2520image-action%2520pairs%2520captured%2520with%2520a%2520camera%250Amounted%2520on%2520the%2520robot%2520gripper.%2520As%2520a%2520result%252C%2520the%2520simulated%2520camera%2520pose%2520changes%2520in%250Athe%2520reconstructed%2520scene%2520can%2520be%2520directly%2520translated%2520into%2520gripper%2520pose%2520changes.%250AWe%2520train%2520a%2520robot%2520policy%2520on%2520demonstrations%2520collected%2520with%252016%2520household%2520objects%250Aand%2520%257B%255Cem%2520directly%257D%2520deploy%2520this%2520policy%2520in%2520the%2520real%2520environment.%2520Experiments%2520in%250Aboth%2520Gaussian%2520Splatting%2520reconstructed%2520scene%2520and%2520real-world%2520human-to-robot%250Ahandover%2520experiments%2520demonstrate%2520that%2520H2RH-SGS%2520serves%2520as%2520a%2520new%2520and%2520effective%250Arepresentation%2520for%2520the%2520human-to-robot%2520handover%2520task.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2507.08726v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Learning%20human-to-robot%20handovers%20through%203D%20scene%20reconstruction&entry.906535625=Yuekun%20Wu%20and%20Yik%20Lung%20Pang%20and%20Andrea%20Cavallaro%20and%20Changjae%20Oh&entry.1292438233=%20%20Learning%20robot%20manipulation%20policies%20from%20raw%2C%20real-world%20image%20data%20requires%0Aa%20large%20number%20of%20robot-action%20trials%20in%20the%20physical%20environment.%20Although%0Atraining%20using%20simulations%20offers%20a%20cost-effective%20alternative%2C%20the%20visual%0Adomain%20gap%20between%20simulation%20and%20robot%20workspace%20remains%20a%20major%20limitation.%0AGaussian%20Splatting%20visual%20reconstruction%20methods%20have%20recently%20provided%20new%0Adirections%20for%20robot%20manipulation%20by%20generating%20realistic%20environments.%20In%20this%0Apaper%2C%20we%20propose%20the%20first%20method%20for%20learning%20supervised-based%20robot%0Ahandovers%20solely%20from%20RGB%20images%20without%20the%20need%20of%20real-robot%20training%20or%0Areal-robot%20data%20collection.%20The%20proposed%20policy%20learner%2C%20Human-to-Robot%0AHandover%20using%20Sparse-View%20Gaussian%20Splatting%20%28H2RH-SGS%29%2C%20leverages%20sparse-view%0AGaussian%20Splatting%20reconstruction%20of%20human-to-robot%20handover%20scenes%20to%20generate%0Arobot%20demonstrations%20containing%20image-action%20pairs%20captured%20with%20a%20camera%0Amounted%20on%20the%20robot%20gripper.%20As%20a%20result%2C%20the%20simulated%20camera%20pose%20changes%20in%0Athe%20reconstructed%20scene%20can%20be%20directly%20translated%20into%20gripper%20pose%20changes.%0AWe%20train%20a%20robot%20policy%20on%20demonstrations%20collected%20with%2016%20household%20objects%0Aand%20%7B%5Cem%20directly%7D%20deploy%20this%20policy%20in%20the%20real%20environment.%20Experiments%20in%0Aboth%20Gaussian%20Splatting%20reconstructed%20scene%20and%20real-world%20human-to-robot%0Ahandover%20experiments%20demonstrate%20that%20H2RH-SGS%20serves%20as%20a%20new%20and%20effective%0Arepresentation%20for%20the%20human-to-robot%20handover%20task.%0A&entry.1838667208=http%3A//arxiv.org/abs/2507.08726v1&entry.124074799=Read"},
{"title": "From One to More: Contextual Part Latents for 3D Generation", "author": "Shaocong Dong and Lihe Ding and Xiao Chen and Yaokun Li and Yuxin Wang and Yucheng Wang and Qi Wang and Jaehyeok Kim and Chenjian Gao and Zhanpeng Huang and Zibin Wang and Tianfan Xue and Dan Xu", "abstract": "  Recent advances in 3D generation have transitioned from multi-view 2D\nrendering approaches to 3D-native latent diffusion frameworks that exploit\ngeometric priors in ground truth data. Despite progress, three key limitations\npersist: (1) Single-latent representations fail to capture complex multi-part\ngeometries, causing detail degradation; (2) Holistic latent coding neglects\npart independence and interrelationships critical for compositional design; (3)\nGlobal conditioning mechanisms lack fine-grained controllability. Inspired by\nhuman 3D design workflows, we propose CoPart - a part-aware diffusion framework\nthat decomposes 3D objects into contextual part latents for coherent multi-part\ngeneration. This paradigm offers three advantages: i) Reduces encoding\ncomplexity through part decomposition; ii) Enables explicit part relationship\nmodeling; iii) Supports part-level conditioning. We further develop a mutual\nguidance strategy to fine-tune pre-trained diffusion models for joint part\nlatent denoising, ensuring both geometric coherence and foundation model\npriors. To enable large-scale training, we construct Partverse - a novel 3D\npart dataset derived from Objaverse through automated mesh segmentation and\nhuman-verified annotations. Extensive experiments demonstrate CoPart's superior\ncapabilities in part-level editing, articulated object generation, and scene\ncomposition with unprecedented controllability.\n", "link": "http://arxiv.org/abs/2507.08772v1", "date": "2025-07-11", "relevancy": 2.5284, "topK": [{"title": "Thinking Outside the BBox: Unconstrained Generative Object Compositing", "link": "http://arxiv.org/abs/2409.04559v2", "similarity": 0.6643}, {"title": "Synthesizing Moving People with 3D Control", "link": "http://arxiv.org/abs/2401.10889v2", "similarity": 0.6294}, {"title": "CAT3D: Create Anything in 3D with Multi-View Diffusion Models", "link": "http://arxiv.org/abs/2405.10314v1", "similarity": 0.622}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20From%20One%20to%20More%3A%20Contextual%20Part%20Latents%20for%203D%20Generation&body=Title%3A%20From%20One%20to%20More%3A%20Contextual%20Part%20Latents%20for%203D%20Generation%0AAuthor%3A%20Shaocong%20Dong%20and%20Lihe%20Ding%20and%20Xiao%20Chen%20and%20Yaokun%20Li%20and%20Yuxin%20Wang%20and%20Yucheng%20Wang%20and%20Qi%20Wang%20and%20Jaehyeok%20Kim%20and%20Chenjian%20Gao%20and%20Zhanpeng%20Huang%20and%20Zibin%20Wang%20and%20Tianfan%20Xue%20and%20Dan%20Xu%0AAbstract%3A%20%20%20Recent%20advances%20in%203D%20generation%20have%20transitioned%20from%20multi-view%202D%0Arendering%20approaches%20to%203D-native%20latent%20diffusion%20frameworks%20that%20exploit%0Ageometric%20priors%20in%20ground%20truth%20data.%20Despite%20progress%2C%20three%20key%20limitations%0Apersist%3A%20%281%29%20Single-latent%20representations%20fail%20to%20capture%20complex%20multi-part%0Ageometries%2C%20causing%20detail%20degradation%3B%20%282%29%20Holistic%20latent%20coding%20neglects%0Apart%20independence%20and%20interrelationships%20critical%20for%20compositional%20design%3B%20%283%29%0AGlobal%20conditioning%20mechanisms%20lack%20fine-grained%20controllability.%20Inspired%20by%0Ahuman%203D%20design%20workflows%2C%20we%20propose%20CoPart%20-%20a%20part-aware%20diffusion%20framework%0Athat%20decomposes%203D%20objects%20into%20contextual%20part%20latents%20for%20coherent%20multi-part%0Ageneration.%20This%20paradigm%20offers%20three%20advantages%3A%20i%29%20Reduces%20encoding%0Acomplexity%20through%20part%20decomposition%3B%20ii%29%20Enables%20explicit%20part%20relationship%0Amodeling%3B%20iii%29%20Supports%20part-level%20conditioning.%20We%20further%20develop%20a%20mutual%0Aguidance%20strategy%20to%20fine-tune%20pre-trained%20diffusion%20models%20for%20joint%20part%0Alatent%20denoising%2C%20ensuring%20both%20geometric%20coherence%20and%20foundation%20model%0Apriors.%20To%20enable%20large-scale%20training%2C%20we%20construct%20Partverse%20-%20a%20novel%203D%0Apart%20dataset%20derived%20from%20Objaverse%20through%20automated%20mesh%20segmentation%20and%0Ahuman-verified%20annotations.%20Extensive%20experiments%20demonstrate%20CoPart%27s%20superior%0Acapabilities%20in%20part-level%20editing%2C%20articulated%20object%20generation%2C%20and%20scene%0Acomposition%20with%20unprecedented%20controllability.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2507.08772v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DFrom%2520One%2520to%2520More%253A%2520Contextual%2520Part%2520Latents%2520for%25203D%2520Generation%26entry.906535625%3DShaocong%2520Dong%2520and%2520Lihe%2520Ding%2520and%2520Xiao%2520Chen%2520and%2520Yaokun%2520Li%2520and%2520Yuxin%2520Wang%2520and%2520Yucheng%2520Wang%2520and%2520Qi%2520Wang%2520and%2520Jaehyeok%2520Kim%2520and%2520Chenjian%2520Gao%2520and%2520Zhanpeng%2520Huang%2520and%2520Zibin%2520Wang%2520and%2520Tianfan%2520Xue%2520and%2520Dan%2520Xu%26entry.1292438233%3D%2520%2520Recent%2520advances%2520in%25203D%2520generation%2520have%2520transitioned%2520from%2520multi-view%25202D%250Arendering%2520approaches%2520to%25203D-native%2520latent%2520diffusion%2520frameworks%2520that%2520exploit%250Ageometric%2520priors%2520in%2520ground%2520truth%2520data.%2520Despite%2520progress%252C%2520three%2520key%2520limitations%250Apersist%253A%2520%25281%2529%2520Single-latent%2520representations%2520fail%2520to%2520capture%2520complex%2520multi-part%250Ageometries%252C%2520causing%2520detail%2520degradation%253B%2520%25282%2529%2520Holistic%2520latent%2520coding%2520neglects%250Apart%2520independence%2520and%2520interrelationships%2520critical%2520for%2520compositional%2520design%253B%2520%25283%2529%250AGlobal%2520conditioning%2520mechanisms%2520lack%2520fine-grained%2520controllability.%2520Inspired%2520by%250Ahuman%25203D%2520design%2520workflows%252C%2520we%2520propose%2520CoPart%2520-%2520a%2520part-aware%2520diffusion%2520framework%250Athat%2520decomposes%25203D%2520objects%2520into%2520contextual%2520part%2520latents%2520for%2520coherent%2520multi-part%250Ageneration.%2520This%2520paradigm%2520offers%2520three%2520advantages%253A%2520i%2529%2520Reduces%2520encoding%250Acomplexity%2520through%2520part%2520decomposition%253B%2520ii%2529%2520Enables%2520explicit%2520part%2520relationship%250Amodeling%253B%2520iii%2529%2520Supports%2520part-level%2520conditioning.%2520We%2520further%2520develop%2520a%2520mutual%250Aguidance%2520strategy%2520to%2520fine-tune%2520pre-trained%2520diffusion%2520models%2520for%2520joint%2520part%250Alatent%2520denoising%252C%2520ensuring%2520both%2520geometric%2520coherence%2520and%2520foundation%2520model%250Apriors.%2520To%2520enable%2520large-scale%2520training%252C%2520we%2520construct%2520Partverse%2520-%2520a%2520novel%25203D%250Apart%2520dataset%2520derived%2520from%2520Objaverse%2520through%2520automated%2520mesh%2520segmentation%2520and%250Ahuman-verified%2520annotations.%2520Extensive%2520experiments%2520demonstrate%2520CoPart%2527s%2520superior%250Acapabilities%2520in%2520part-level%2520editing%252C%2520articulated%2520object%2520generation%252C%2520and%2520scene%250Acomposition%2520with%2520unprecedented%2520controllability.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2507.08772v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=From%20One%20to%20More%3A%20Contextual%20Part%20Latents%20for%203D%20Generation&entry.906535625=Shaocong%20Dong%20and%20Lihe%20Ding%20and%20Xiao%20Chen%20and%20Yaokun%20Li%20and%20Yuxin%20Wang%20and%20Yucheng%20Wang%20and%20Qi%20Wang%20and%20Jaehyeok%20Kim%20and%20Chenjian%20Gao%20and%20Zhanpeng%20Huang%20and%20Zibin%20Wang%20and%20Tianfan%20Xue%20and%20Dan%20Xu&entry.1292438233=%20%20Recent%20advances%20in%203D%20generation%20have%20transitioned%20from%20multi-view%202D%0Arendering%20approaches%20to%203D-native%20latent%20diffusion%20frameworks%20that%20exploit%0Ageometric%20priors%20in%20ground%20truth%20data.%20Despite%20progress%2C%20three%20key%20limitations%0Apersist%3A%20%281%29%20Single-latent%20representations%20fail%20to%20capture%20complex%20multi-part%0Ageometries%2C%20causing%20detail%20degradation%3B%20%282%29%20Holistic%20latent%20coding%20neglects%0Apart%20independence%20and%20interrelationships%20critical%20for%20compositional%20design%3B%20%283%29%0AGlobal%20conditioning%20mechanisms%20lack%20fine-grained%20controllability.%20Inspired%20by%0Ahuman%203D%20design%20workflows%2C%20we%20propose%20CoPart%20-%20a%20part-aware%20diffusion%20framework%0Athat%20decomposes%203D%20objects%20into%20contextual%20part%20latents%20for%20coherent%20multi-part%0Ageneration.%20This%20paradigm%20offers%20three%20advantages%3A%20i%29%20Reduces%20encoding%0Acomplexity%20through%20part%20decomposition%3B%20ii%29%20Enables%20explicit%20part%20relationship%0Amodeling%3B%20iii%29%20Supports%20part-level%20conditioning.%20We%20further%20develop%20a%20mutual%0Aguidance%20strategy%20to%20fine-tune%20pre-trained%20diffusion%20models%20for%20joint%20part%0Alatent%20denoising%2C%20ensuring%20both%20geometric%20coherence%20and%20foundation%20model%0Apriors.%20To%20enable%20large-scale%20training%2C%20we%20construct%20Partverse%20-%20a%20novel%203D%0Apart%20dataset%20derived%20from%20Objaverse%20through%20automated%20mesh%20segmentation%20and%0Ahuman-verified%20annotations.%20Extensive%20experiments%20demonstrate%20CoPart%27s%20superior%0Acapabilities%20in%20part-level%20editing%2C%20articulated%20object%20generation%2C%20and%20scene%0Acomposition%20with%20unprecedented%20controllability.%0A&entry.1838667208=http%3A//arxiv.org/abs/2507.08772v1&entry.124074799=Read"},
{"title": "Forget Me Not: Fighting Local Overfitting with Knowledge Fusion and\n  Distillation", "author": "Uri Stern and Eli Corn and Daphna Weinshall", "abstract": "  Overfitting in deep neural networks occurs less frequently than expected.\nThis is a puzzling observation, as theory predicts that greater model capacity\nshould eventually lead to overfitting -- yet this is rarely seen in practice.\nBut what if overfitting does occur, not globally, but in specific sub-regions\nof the data space? In this work, we introduce a novel score that measures the\nforgetting rate of deep models on validation data, capturing what we term local\noverfitting: a performance degradation confined to certain regions of the input\nspace. We demonstrate that local overfitting can arise even without\nconventional overfitting, and is closely linked to the double descent\nphenomenon.\n  Building on these insights, we introduce a two-stage approach that leverages\nthe training history of a single model to recover and retain forgotten\nknowledge: first, by aggregating checkpoints into an ensemble, and then by\ndistilling it into a single model of the original size, thus enhancing\nperformance without added inference cost.\n  Extensive experiments across multiple datasets, modern architectures, and\ntraining regimes validate the effectiveness of our approach. Notably, in the\npresence of label noise, our method -- Knowledge Fusion followed by Knowledge\nDistillation -- outperforms both the original model and independently trained\nensembles, achieving a rare win-win scenario: reduced training and inference\ncomplexity.\n", "link": "http://arxiv.org/abs/2507.08686v1", "date": "2025-07-11", "relevancy": 2.5118, "topK": [{"title": "Boosted dynamic neural networks", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/26302", "similarity": 0.5189}, {"title": "ToDo: Token Downsampling for Efficient Generation of High-Resolution\n  Images", "link": "http://arxiv.org/abs/2402.13573v2", "similarity": 0.5039}, {"title": "DDM-NET: End-to-end learning of keypoint feature Detection, Description and Matching for 3D localization", "link": "https://arxiv.org/abs/2212.04575", "similarity": 0.4842}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Forget%20Me%20Not%3A%20Fighting%20Local%20Overfitting%20with%20Knowledge%20Fusion%20and%0A%20%20Distillation&body=Title%3A%20Forget%20Me%20Not%3A%20Fighting%20Local%20Overfitting%20with%20Knowledge%20Fusion%20and%0A%20%20Distillation%0AAuthor%3A%20Uri%20Stern%20and%20Eli%20Corn%20and%20Daphna%20Weinshall%0AAbstract%3A%20%20%20Overfitting%20in%20deep%20neural%20networks%20occurs%20less%20frequently%20than%20expected.%0AThis%20is%20a%20puzzling%20observation%2C%20as%20theory%20predicts%20that%20greater%20model%20capacity%0Ashould%20eventually%20lead%20to%20overfitting%20--%20yet%20this%20is%20rarely%20seen%20in%20practice.%0ABut%20what%20if%20overfitting%20does%20occur%2C%20not%20globally%2C%20but%20in%20specific%20sub-regions%0Aof%20the%20data%20space%3F%20In%20this%20work%2C%20we%20introduce%20a%20novel%20score%20that%20measures%20the%0Aforgetting%20rate%20of%20deep%20models%20on%20validation%20data%2C%20capturing%20what%20we%20term%20local%0Aoverfitting%3A%20a%20performance%20degradation%20confined%20to%20certain%20regions%20of%20the%20input%0Aspace.%20We%20demonstrate%20that%20local%20overfitting%20can%20arise%20even%20without%0Aconventional%20overfitting%2C%20and%20is%20closely%20linked%20to%20the%20double%20descent%0Aphenomenon.%0A%20%20Building%20on%20these%20insights%2C%20we%20introduce%20a%20two-stage%20approach%20that%20leverages%0Athe%20training%20history%20of%20a%20single%20model%20to%20recover%20and%20retain%20forgotten%0Aknowledge%3A%20first%2C%20by%20aggregating%20checkpoints%20into%20an%20ensemble%2C%20and%20then%20by%0Adistilling%20it%20into%20a%20single%20model%20of%20the%20original%20size%2C%20thus%20enhancing%0Aperformance%20without%20added%20inference%20cost.%0A%20%20Extensive%20experiments%20across%20multiple%20datasets%2C%20modern%20architectures%2C%20and%0Atraining%20regimes%20validate%20the%20effectiveness%20of%20our%20approach.%20Notably%2C%20in%20the%0Apresence%20of%20label%20noise%2C%20our%20method%20--%20Knowledge%20Fusion%20followed%20by%20Knowledge%0ADistillation%20--%20outperforms%20both%20the%20original%20model%20and%20independently%20trained%0Aensembles%2C%20achieving%20a%20rare%20win-win%20scenario%3A%20reduced%20training%20and%20inference%0Acomplexity.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2507.08686v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DForget%2520Me%2520Not%253A%2520Fighting%2520Local%2520Overfitting%2520with%2520Knowledge%2520Fusion%2520and%250A%2520%2520Distillation%26entry.906535625%3DUri%2520Stern%2520and%2520Eli%2520Corn%2520and%2520Daphna%2520Weinshall%26entry.1292438233%3D%2520%2520Overfitting%2520in%2520deep%2520neural%2520networks%2520occurs%2520less%2520frequently%2520than%2520expected.%250AThis%2520is%2520a%2520puzzling%2520observation%252C%2520as%2520theory%2520predicts%2520that%2520greater%2520model%2520capacity%250Ashould%2520eventually%2520lead%2520to%2520overfitting%2520--%2520yet%2520this%2520is%2520rarely%2520seen%2520in%2520practice.%250ABut%2520what%2520if%2520overfitting%2520does%2520occur%252C%2520not%2520globally%252C%2520but%2520in%2520specific%2520sub-regions%250Aof%2520the%2520data%2520space%253F%2520In%2520this%2520work%252C%2520we%2520introduce%2520a%2520novel%2520score%2520that%2520measures%2520the%250Aforgetting%2520rate%2520of%2520deep%2520models%2520on%2520validation%2520data%252C%2520capturing%2520what%2520we%2520term%2520local%250Aoverfitting%253A%2520a%2520performance%2520degradation%2520confined%2520to%2520certain%2520regions%2520of%2520the%2520input%250Aspace.%2520We%2520demonstrate%2520that%2520local%2520overfitting%2520can%2520arise%2520even%2520without%250Aconventional%2520overfitting%252C%2520and%2520is%2520closely%2520linked%2520to%2520the%2520double%2520descent%250Aphenomenon.%250A%2520%2520Building%2520on%2520these%2520insights%252C%2520we%2520introduce%2520a%2520two-stage%2520approach%2520that%2520leverages%250Athe%2520training%2520history%2520of%2520a%2520single%2520model%2520to%2520recover%2520and%2520retain%2520forgotten%250Aknowledge%253A%2520first%252C%2520by%2520aggregating%2520checkpoints%2520into%2520an%2520ensemble%252C%2520and%2520then%2520by%250Adistilling%2520it%2520into%2520a%2520single%2520model%2520of%2520the%2520original%2520size%252C%2520thus%2520enhancing%250Aperformance%2520without%2520added%2520inference%2520cost.%250A%2520%2520Extensive%2520experiments%2520across%2520multiple%2520datasets%252C%2520modern%2520architectures%252C%2520and%250Atraining%2520regimes%2520validate%2520the%2520effectiveness%2520of%2520our%2520approach.%2520Notably%252C%2520in%2520the%250Apresence%2520of%2520label%2520noise%252C%2520our%2520method%2520--%2520Knowledge%2520Fusion%2520followed%2520by%2520Knowledge%250ADistillation%2520--%2520outperforms%2520both%2520the%2520original%2520model%2520and%2520independently%2520trained%250Aensembles%252C%2520achieving%2520a%2520rare%2520win-win%2520scenario%253A%2520reduced%2520training%2520and%2520inference%250Acomplexity.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2507.08686v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Forget%20Me%20Not%3A%20Fighting%20Local%20Overfitting%20with%20Knowledge%20Fusion%20and%0A%20%20Distillation&entry.906535625=Uri%20Stern%20and%20Eli%20Corn%20and%20Daphna%20Weinshall&entry.1292438233=%20%20Overfitting%20in%20deep%20neural%20networks%20occurs%20less%20frequently%20than%20expected.%0AThis%20is%20a%20puzzling%20observation%2C%20as%20theory%20predicts%20that%20greater%20model%20capacity%0Ashould%20eventually%20lead%20to%20overfitting%20--%20yet%20this%20is%20rarely%20seen%20in%20practice.%0ABut%20what%20if%20overfitting%20does%20occur%2C%20not%20globally%2C%20but%20in%20specific%20sub-regions%0Aof%20the%20data%20space%3F%20In%20this%20work%2C%20we%20introduce%20a%20novel%20score%20that%20measures%20the%0Aforgetting%20rate%20of%20deep%20models%20on%20validation%20data%2C%20capturing%20what%20we%20term%20local%0Aoverfitting%3A%20a%20performance%20degradation%20confined%20to%20certain%20regions%20of%20the%20input%0Aspace.%20We%20demonstrate%20that%20local%20overfitting%20can%20arise%20even%20without%0Aconventional%20overfitting%2C%20and%20is%20closely%20linked%20to%20the%20double%20descent%0Aphenomenon.%0A%20%20Building%20on%20these%20insights%2C%20we%20introduce%20a%20two-stage%20approach%20that%20leverages%0Athe%20training%20history%20of%20a%20single%20model%20to%20recover%20and%20retain%20forgotten%0Aknowledge%3A%20first%2C%20by%20aggregating%20checkpoints%20into%20an%20ensemble%2C%20and%20then%20by%0Adistilling%20it%20into%20a%20single%20model%20of%20the%20original%20size%2C%20thus%20enhancing%0Aperformance%20without%20added%20inference%20cost.%0A%20%20Extensive%20experiments%20across%20multiple%20datasets%2C%20modern%20architectures%2C%20and%0Atraining%20regimes%20validate%20the%20effectiveness%20of%20our%20approach.%20Notably%2C%20in%20the%0Apresence%20of%20label%20noise%2C%20our%20method%20--%20Knowledge%20Fusion%20followed%20by%20Knowledge%0ADistillation%20--%20outperforms%20both%20the%20original%20model%20and%20independently%20trained%0Aensembles%2C%20achieving%20a%20rare%20win-win%20scenario%3A%20reduced%20training%20and%20inference%0Acomplexity.%0A&entry.1838667208=http%3A//arxiv.org/abs/2507.08686v1&entry.124074799=Read"},
{"title": "A comprehensive study of LLM-based argument classification: from LLAMA\n  through GPT-4o to Deepseek-R1", "author": "Marcin Pietro\u0144 and Rafa\u0142 Olszowski and Jakub Gomu\u0142ka and Filip Gampel and Andrzej Tomski", "abstract": "  Argument mining (AM) is an interdisciplinary research field that integrates\ninsights from logic, philosophy, linguistics, rhetoric, law, psychology, and\ncomputer science. It involves the automatic identification and extraction of\nargumentative components, such as premises and claims, and the detection of\nrelationships between them, such as support, attack, or neutrality. Recently,\nthe field has advanced significantly, especially with the advent of large\nlanguage models (LLMs), which have enhanced the efficiency of analyzing and\nextracting argument semantics compared to traditional methods and other deep\nlearning models. There are many benchmarks for testing and verifying the\nquality of LLM, but there is still a lack of research and results on the\noperation of these models in publicly available argument classification\ndatabases. This paper presents a study of a selection of LLM's, using diverse\ndatasets such as Args.me and UKP. The models tested include versions of GPT,\nLlama, and DeepSeek, along with reasoning-enhanced variants incorporating the\nChain-of-Thoughts algorithm. The results indicate that ChatGPT-4o outperforms\nthe others in the argument classification benchmarks. In case of models\nincorporated with reasoning capabilities, the Deepseek-R1 shows its\nsuperiority. However, despite their superiority, GPT-4o and Deepseek-R1 still\nmake errors. The most common errors are discussed for all models. To our\nknowledge, the presented work is the first broader analysis of the mentioned\ndatasets using LLM and prompt algorithms. The work also shows some weaknesses\nof known prompt algorithms in argument analysis, while indicating directions\nfor their improvement. The added value of the work is the in-depth analysis of\nthe available argument datasets and the demonstration of their shortcomings.\n", "link": "http://arxiv.org/abs/2507.08621v1", "date": "2025-07-11", "relevancy": 2.4756, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5049}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5049}, {"title": "Breadcrumbs: Adversarial class-balanced sampling for long-tailed recognition", "link": "https://link.springer.com/chapter/10.1007/978-3-031-20053-3_37", "similarity": 0.4756}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20A%20comprehensive%20study%20of%20LLM-based%20argument%20classification%3A%20from%20LLAMA%0A%20%20through%20GPT-4o%20to%20Deepseek-R1&body=Title%3A%20A%20comprehensive%20study%20of%20LLM-based%20argument%20classification%3A%20from%20LLAMA%0A%20%20through%20GPT-4o%20to%20Deepseek-R1%0AAuthor%3A%20Marcin%20Pietro%C5%84%20and%20Rafa%C5%82%20Olszowski%20and%20Jakub%20Gomu%C5%82ka%20and%20Filip%20Gampel%20and%20Andrzej%20Tomski%0AAbstract%3A%20%20%20Argument%20mining%20%28AM%29%20is%20an%20interdisciplinary%20research%20field%20that%20integrates%0Ainsights%20from%20logic%2C%20philosophy%2C%20linguistics%2C%20rhetoric%2C%20law%2C%20psychology%2C%20and%0Acomputer%20science.%20It%20involves%20the%20automatic%20identification%20and%20extraction%20of%0Aargumentative%20components%2C%20such%20as%20premises%20and%20claims%2C%20and%20the%20detection%20of%0Arelationships%20between%20them%2C%20such%20as%20support%2C%20attack%2C%20or%20neutrality.%20Recently%2C%0Athe%20field%20has%20advanced%20significantly%2C%20especially%20with%20the%20advent%20of%20large%0Alanguage%20models%20%28LLMs%29%2C%20which%20have%20enhanced%20the%20efficiency%20of%20analyzing%20and%0Aextracting%20argument%20semantics%20compared%20to%20traditional%20methods%20and%20other%20deep%0Alearning%20models.%20There%20are%20many%20benchmarks%20for%20testing%20and%20verifying%20the%0Aquality%20of%20LLM%2C%20but%20there%20is%20still%20a%20lack%20of%20research%20and%20results%20on%20the%0Aoperation%20of%20these%20models%20in%20publicly%20available%20argument%20classification%0Adatabases.%20This%20paper%20presents%20a%20study%20of%20a%20selection%20of%20LLM%27s%2C%20using%20diverse%0Adatasets%20such%20as%20Args.me%20and%20UKP.%20The%20models%20tested%20include%20versions%20of%20GPT%2C%0ALlama%2C%20and%20DeepSeek%2C%20along%20with%20reasoning-enhanced%20variants%20incorporating%20the%0AChain-of-Thoughts%20algorithm.%20The%20results%20indicate%20that%20ChatGPT-4o%20outperforms%0Athe%20others%20in%20the%20argument%20classification%20benchmarks.%20In%20case%20of%20models%0Aincorporated%20with%20reasoning%20capabilities%2C%20the%20Deepseek-R1%20shows%20its%0Asuperiority.%20However%2C%20despite%20their%20superiority%2C%20GPT-4o%20and%20Deepseek-R1%20still%0Amake%20errors.%20The%20most%20common%20errors%20are%20discussed%20for%20all%20models.%20To%20our%0Aknowledge%2C%20the%20presented%20work%20is%20the%20first%20broader%20analysis%20of%20the%20mentioned%0Adatasets%20using%20LLM%20and%20prompt%20algorithms.%20The%20work%20also%20shows%20some%20weaknesses%0Aof%20known%20prompt%20algorithms%20in%20argument%20analysis%2C%20while%20indicating%20directions%0Afor%20their%20improvement.%20The%20added%20value%20of%20the%20work%20is%20the%20in-depth%20analysis%20of%0Athe%20available%20argument%20datasets%20and%20the%20demonstration%20of%20their%20shortcomings.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2507.08621v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DA%2520comprehensive%2520study%2520of%2520LLM-based%2520argument%2520classification%253A%2520from%2520LLAMA%250A%2520%2520through%2520GPT-4o%2520to%2520Deepseek-R1%26entry.906535625%3DMarcin%2520Pietro%25C5%2584%2520and%2520Rafa%25C5%2582%2520Olszowski%2520and%2520Jakub%2520Gomu%25C5%2582ka%2520and%2520Filip%2520Gampel%2520and%2520Andrzej%2520Tomski%26entry.1292438233%3D%2520%2520Argument%2520mining%2520%2528AM%2529%2520is%2520an%2520interdisciplinary%2520research%2520field%2520that%2520integrates%250Ainsights%2520from%2520logic%252C%2520philosophy%252C%2520linguistics%252C%2520rhetoric%252C%2520law%252C%2520psychology%252C%2520and%250Acomputer%2520science.%2520It%2520involves%2520the%2520automatic%2520identification%2520and%2520extraction%2520of%250Aargumentative%2520components%252C%2520such%2520as%2520premises%2520and%2520claims%252C%2520and%2520the%2520detection%2520of%250Arelationships%2520between%2520them%252C%2520such%2520as%2520support%252C%2520attack%252C%2520or%2520neutrality.%2520Recently%252C%250Athe%2520field%2520has%2520advanced%2520significantly%252C%2520especially%2520with%2520the%2520advent%2520of%2520large%250Alanguage%2520models%2520%2528LLMs%2529%252C%2520which%2520have%2520enhanced%2520the%2520efficiency%2520of%2520analyzing%2520and%250Aextracting%2520argument%2520semantics%2520compared%2520to%2520traditional%2520methods%2520and%2520other%2520deep%250Alearning%2520models.%2520There%2520are%2520many%2520benchmarks%2520for%2520testing%2520and%2520verifying%2520the%250Aquality%2520of%2520LLM%252C%2520but%2520there%2520is%2520still%2520a%2520lack%2520of%2520research%2520and%2520results%2520on%2520the%250Aoperation%2520of%2520these%2520models%2520in%2520publicly%2520available%2520argument%2520classification%250Adatabases.%2520This%2520paper%2520presents%2520a%2520study%2520of%2520a%2520selection%2520of%2520LLM%2527s%252C%2520using%2520diverse%250Adatasets%2520such%2520as%2520Args.me%2520and%2520UKP.%2520The%2520models%2520tested%2520include%2520versions%2520of%2520GPT%252C%250ALlama%252C%2520and%2520DeepSeek%252C%2520along%2520with%2520reasoning-enhanced%2520variants%2520incorporating%2520the%250AChain-of-Thoughts%2520algorithm.%2520The%2520results%2520indicate%2520that%2520ChatGPT-4o%2520outperforms%250Athe%2520others%2520in%2520the%2520argument%2520classification%2520benchmarks.%2520In%2520case%2520of%2520models%250Aincorporated%2520with%2520reasoning%2520capabilities%252C%2520the%2520Deepseek-R1%2520shows%2520its%250Asuperiority.%2520However%252C%2520despite%2520their%2520superiority%252C%2520GPT-4o%2520and%2520Deepseek-R1%2520still%250Amake%2520errors.%2520The%2520most%2520common%2520errors%2520are%2520discussed%2520for%2520all%2520models.%2520To%2520our%250Aknowledge%252C%2520the%2520presented%2520work%2520is%2520the%2520first%2520broader%2520analysis%2520of%2520the%2520mentioned%250Adatasets%2520using%2520LLM%2520and%2520prompt%2520algorithms.%2520The%2520work%2520also%2520shows%2520some%2520weaknesses%250Aof%2520known%2520prompt%2520algorithms%2520in%2520argument%2520analysis%252C%2520while%2520indicating%2520directions%250Afor%2520their%2520improvement.%2520The%2520added%2520value%2520of%2520the%2520work%2520is%2520the%2520in-depth%2520analysis%2520of%250Athe%2520available%2520argument%2520datasets%2520and%2520the%2520demonstration%2520of%2520their%2520shortcomings.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2507.08621v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=A%20comprehensive%20study%20of%20LLM-based%20argument%20classification%3A%20from%20LLAMA%0A%20%20through%20GPT-4o%20to%20Deepseek-R1&entry.906535625=Marcin%20Pietro%C5%84%20and%20Rafa%C5%82%20Olszowski%20and%20Jakub%20Gomu%C5%82ka%20and%20Filip%20Gampel%20and%20Andrzej%20Tomski&entry.1292438233=%20%20Argument%20mining%20%28AM%29%20is%20an%20interdisciplinary%20research%20field%20that%20integrates%0Ainsights%20from%20logic%2C%20philosophy%2C%20linguistics%2C%20rhetoric%2C%20law%2C%20psychology%2C%20and%0Acomputer%20science.%20It%20involves%20the%20automatic%20identification%20and%20extraction%20of%0Aargumentative%20components%2C%20such%20as%20premises%20and%20claims%2C%20and%20the%20detection%20of%0Arelationships%20between%20them%2C%20such%20as%20support%2C%20attack%2C%20or%20neutrality.%20Recently%2C%0Athe%20field%20has%20advanced%20significantly%2C%20especially%20with%20the%20advent%20of%20large%0Alanguage%20models%20%28LLMs%29%2C%20which%20have%20enhanced%20the%20efficiency%20of%20analyzing%20and%0Aextracting%20argument%20semantics%20compared%20to%20traditional%20methods%20and%20other%20deep%0Alearning%20models.%20There%20are%20many%20benchmarks%20for%20testing%20and%20verifying%20the%0Aquality%20of%20LLM%2C%20but%20there%20is%20still%20a%20lack%20of%20research%20and%20results%20on%20the%0Aoperation%20of%20these%20models%20in%20publicly%20available%20argument%20classification%0Adatabases.%20This%20paper%20presents%20a%20study%20of%20a%20selection%20of%20LLM%27s%2C%20using%20diverse%0Adatasets%20such%20as%20Args.me%20and%20UKP.%20The%20models%20tested%20include%20versions%20of%20GPT%2C%0ALlama%2C%20and%20DeepSeek%2C%20along%20with%20reasoning-enhanced%20variants%20incorporating%20the%0AChain-of-Thoughts%20algorithm.%20The%20results%20indicate%20that%20ChatGPT-4o%20outperforms%0Athe%20others%20in%20the%20argument%20classification%20benchmarks.%20In%20case%20of%20models%0Aincorporated%20with%20reasoning%20capabilities%2C%20the%20Deepseek-R1%20shows%20its%0Asuperiority.%20However%2C%20despite%20their%20superiority%2C%20GPT-4o%20and%20Deepseek-R1%20still%0Amake%20errors.%20The%20most%20common%20errors%20are%20discussed%20for%20all%20models.%20To%20our%0Aknowledge%2C%20the%20presented%20work%20is%20the%20first%20broader%20analysis%20of%20the%20mentioned%0Adatasets%20using%20LLM%20and%20prompt%20algorithms.%20The%20work%20also%20shows%20some%20weaknesses%0Aof%20known%20prompt%20algorithms%20in%20argument%20analysis%2C%20while%20indicating%20directions%0Afor%20their%20improvement.%20The%20added%20value%20of%20the%20work%20is%20the%20in-depth%20analysis%20of%0Athe%20available%20argument%20datasets%20and%20the%20demonstration%20of%20their%20shortcomings.%0A&entry.1838667208=http%3A//arxiv.org/abs/2507.08621v1&entry.124074799=Read"},
{"title": "Adaptive Nonlinear Vector Autoregression: Robust Forecasting for Noisy\n  Chaotic Time Series", "author": "Azimov Sherkhon and Susana Lopez-Moreno and Eric Dolores-Cuenca and Sieun Lee and Sangil Kim", "abstract": "  Nonlinear vector autoregression (NVAR) and reservoir computing (RC) have\nshown promise in forecasting chaotic dynamical systems, such as the Lorenz-63\nmodel and El Nino-Southern Oscillation. However, their reliance on fixed\nnonlinearities - polynomial expansions in NVAR or random feature maps in RC -\nlimits their adaptability to high noise or real-world data. These methods also\nscale poorly in high-dimensional settings due to costly matrix inversion during\nreadout computation. We propose an adaptive NVAR model that combines\ndelay-embedded linear inputs with features generated by a shallow, learnable\nmulti-layer perceptron (MLP). The MLP and linear readout are jointly trained\nusing gradient-based optimization, enabling the model to learn data-driven\nnonlinearities while preserving a simple readout structure. Unlike standard\nNVAR, our approach avoids the need for an exhaustive and sensitive grid search\nover ridge and delay parameters. Instead, tuning is restricted to neural\nnetwork hyperparameters, improving scalability. Initial experiments on chaotic\nsystems tested under noise-free and synthetically noisy conditions showed that\nthe adaptive model outperformed the standard NVAR in predictive accuracy and\nshowed robust forecasting under noisy conditions with a lower observation\nfrequency.\n", "link": "http://arxiv.org/abs/2507.08738v1", "date": "2025-07-11", "relevancy": 2.3503, "topK": [{"title": "Tuning-Free Noise Rectification for High Fidelity Image-to-Video\n  Generation", "link": "http://arxiv.org/abs/2403.02827v1", "similarity": 0.4705}, {"title": "Boosted dynamic neural networks", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/26302", "similarity": 0.47}, {"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.4696}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Adaptive%20Nonlinear%20Vector%20Autoregression%3A%20Robust%20Forecasting%20for%20Noisy%0A%20%20Chaotic%20Time%20Series&body=Title%3A%20Adaptive%20Nonlinear%20Vector%20Autoregression%3A%20Robust%20Forecasting%20for%20Noisy%0A%20%20Chaotic%20Time%20Series%0AAuthor%3A%20Azimov%20Sherkhon%20and%20Susana%20Lopez-Moreno%20and%20Eric%20Dolores-Cuenca%20and%20Sieun%20Lee%20and%20Sangil%20Kim%0AAbstract%3A%20%20%20Nonlinear%20vector%20autoregression%20%28NVAR%29%20and%20reservoir%20computing%20%28RC%29%20have%0Ashown%20promise%20in%20forecasting%20chaotic%20dynamical%20systems%2C%20such%20as%20the%20Lorenz-63%0Amodel%20and%20El%20Nino-Southern%20Oscillation.%20However%2C%20their%20reliance%20on%20fixed%0Anonlinearities%20-%20polynomial%20expansions%20in%20NVAR%20or%20random%20feature%20maps%20in%20RC%20-%0Alimits%20their%20adaptability%20to%20high%20noise%20or%20real-world%20data.%20These%20methods%20also%0Ascale%20poorly%20in%20high-dimensional%20settings%20due%20to%20costly%20matrix%20inversion%20during%0Areadout%20computation.%20We%20propose%20an%20adaptive%20NVAR%20model%20that%20combines%0Adelay-embedded%20linear%20inputs%20with%20features%20generated%20by%20a%20shallow%2C%20learnable%0Amulti-layer%20perceptron%20%28MLP%29.%20The%20MLP%20and%20linear%20readout%20are%20jointly%20trained%0Ausing%20gradient-based%20optimization%2C%20enabling%20the%20model%20to%20learn%20data-driven%0Anonlinearities%20while%20preserving%20a%20simple%20readout%20structure.%20Unlike%20standard%0ANVAR%2C%20our%20approach%20avoids%20the%20need%20for%20an%20exhaustive%20and%20sensitive%20grid%20search%0Aover%20ridge%20and%20delay%20parameters.%20Instead%2C%20tuning%20is%20restricted%20to%20neural%0Anetwork%20hyperparameters%2C%20improving%20scalability.%20Initial%20experiments%20on%20chaotic%0Asystems%20tested%20under%20noise-free%20and%20synthetically%20noisy%20conditions%20showed%20that%0Athe%20adaptive%20model%20outperformed%20the%20standard%20NVAR%20in%20predictive%20accuracy%20and%0Ashowed%20robust%20forecasting%20under%20noisy%20conditions%20with%20a%20lower%20observation%0Afrequency.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2507.08738v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DAdaptive%2520Nonlinear%2520Vector%2520Autoregression%253A%2520Robust%2520Forecasting%2520for%2520Noisy%250A%2520%2520Chaotic%2520Time%2520Series%26entry.906535625%3DAzimov%2520Sherkhon%2520and%2520Susana%2520Lopez-Moreno%2520and%2520Eric%2520Dolores-Cuenca%2520and%2520Sieun%2520Lee%2520and%2520Sangil%2520Kim%26entry.1292438233%3D%2520%2520Nonlinear%2520vector%2520autoregression%2520%2528NVAR%2529%2520and%2520reservoir%2520computing%2520%2528RC%2529%2520have%250Ashown%2520promise%2520in%2520forecasting%2520chaotic%2520dynamical%2520systems%252C%2520such%2520as%2520the%2520Lorenz-63%250Amodel%2520and%2520El%2520Nino-Southern%2520Oscillation.%2520However%252C%2520their%2520reliance%2520on%2520fixed%250Anonlinearities%2520-%2520polynomial%2520expansions%2520in%2520NVAR%2520or%2520random%2520feature%2520maps%2520in%2520RC%2520-%250Alimits%2520their%2520adaptability%2520to%2520high%2520noise%2520or%2520real-world%2520data.%2520These%2520methods%2520also%250Ascale%2520poorly%2520in%2520high-dimensional%2520settings%2520due%2520to%2520costly%2520matrix%2520inversion%2520during%250Areadout%2520computation.%2520We%2520propose%2520an%2520adaptive%2520NVAR%2520model%2520that%2520combines%250Adelay-embedded%2520linear%2520inputs%2520with%2520features%2520generated%2520by%2520a%2520shallow%252C%2520learnable%250Amulti-layer%2520perceptron%2520%2528MLP%2529.%2520The%2520MLP%2520and%2520linear%2520readout%2520are%2520jointly%2520trained%250Ausing%2520gradient-based%2520optimization%252C%2520enabling%2520the%2520model%2520to%2520learn%2520data-driven%250Anonlinearities%2520while%2520preserving%2520a%2520simple%2520readout%2520structure.%2520Unlike%2520standard%250ANVAR%252C%2520our%2520approach%2520avoids%2520the%2520need%2520for%2520an%2520exhaustive%2520and%2520sensitive%2520grid%2520search%250Aover%2520ridge%2520and%2520delay%2520parameters.%2520Instead%252C%2520tuning%2520is%2520restricted%2520to%2520neural%250Anetwork%2520hyperparameters%252C%2520improving%2520scalability.%2520Initial%2520experiments%2520on%2520chaotic%250Asystems%2520tested%2520under%2520noise-free%2520and%2520synthetically%2520noisy%2520conditions%2520showed%2520that%250Athe%2520adaptive%2520model%2520outperformed%2520the%2520standard%2520NVAR%2520in%2520predictive%2520accuracy%2520and%250Ashowed%2520robust%2520forecasting%2520under%2520noisy%2520conditions%2520with%2520a%2520lower%2520observation%250Afrequency.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2507.08738v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Adaptive%20Nonlinear%20Vector%20Autoregression%3A%20Robust%20Forecasting%20for%20Noisy%0A%20%20Chaotic%20Time%20Series&entry.906535625=Azimov%20Sherkhon%20and%20Susana%20Lopez-Moreno%20and%20Eric%20Dolores-Cuenca%20and%20Sieun%20Lee%20and%20Sangil%20Kim&entry.1292438233=%20%20Nonlinear%20vector%20autoregression%20%28NVAR%29%20and%20reservoir%20computing%20%28RC%29%20have%0Ashown%20promise%20in%20forecasting%20chaotic%20dynamical%20systems%2C%20such%20as%20the%20Lorenz-63%0Amodel%20and%20El%20Nino-Southern%20Oscillation.%20However%2C%20their%20reliance%20on%20fixed%0Anonlinearities%20-%20polynomial%20expansions%20in%20NVAR%20or%20random%20feature%20maps%20in%20RC%20-%0Alimits%20their%20adaptability%20to%20high%20noise%20or%20real-world%20data.%20These%20methods%20also%0Ascale%20poorly%20in%20high-dimensional%20settings%20due%20to%20costly%20matrix%20inversion%20during%0Areadout%20computation.%20We%20propose%20an%20adaptive%20NVAR%20model%20that%20combines%0Adelay-embedded%20linear%20inputs%20with%20features%20generated%20by%20a%20shallow%2C%20learnable%0Amulti-layer%20perceptron%20%28MLP%29.%20The%20MLP%20and%20linear%20readout%20are%20jointly%20trained%0Ausing%20gradient-based%20optimization%2C%20enabling%20the%20model%20to%20learn%20data-driven%0Anonlinearities%20while%20preserving%20a%20simple%20readout%20structure.%20Unlike%20standard%0ANVAR%2C%20our%20approach%20avoids%20the%20need%20for%20an%20exhaustive%20and%20sensitive%20grid%20search%0Aover%20ridge%20and%20delay%20parameters.%20Instead%2C%20tuning%20is%20restricted%20to%20neural%0Anetwork%20hyperparameters%2C%20improving%20scalability.%20Initial%20experiments%20on%20chaotic%0Asystems%20tested%20under%20noise-free%20and%20synthetically%20noisy%20conditions%20showed%20that%0Athe%20adaptive%20model%20outperformed%20the%20standard%20NVAR%20in%20predictive%20accuracy%20and%0Ashowed%20robust%20forecasting%20under%20noisy%20conditions%20with%20a%20lower%20observation%0Afrequency.%0A&entry.1838667208=http%3A//arxiv.org/abs/2507.08738v1&entry.124074799=Read"},
{"title": "ViLU: Learning Vision-Language Uncertainties for Failure Prediction", "author": "Marc Lafon and Yannis Karmim and Julio Silva-Rodr\u00edguez and Paul Couairon and Cl\u00e9ment Rambour and Rapha\u00ebl Fournier-Sniehotta and Ismail Ben Ayed and Jose Dolz and Nicolas Thome", "abstract": "  Reliable Uncertainty Quantification (UQ) and failure prediction remain open\nchallenges for Vision-Language Models (VLMs). We introduce ViLU, a new\nVision-Language Uncertainty quantification framework that contextualizes\nuncertainty estimates by leveraging all task-relevant textual representations.\nViLU constructs an uncertainty-aware multi-modal representation by integrating\nthe visual embedding, the predicted textual embedding, and an image-conditioned\ntextual representation via cross-attention. Unlike traditional UQ methods based\non loss prediction, ViLU trains an uncertainty predictor as a binary classifier\nto distinguish correct from incorrect predictions using a weighted binary\ncross-entropy loss, making it loss-agnostic. In particular, our proposed\napproach is well-suited for post-hoc settings, where only vision and text\nembeddings are available without direct access to the model itself. Extensive\nexperiments on diverse datasets show the significant gains of our method\ncompared to state-of-the-art failure prediction methods. We apply our method to\nstandard classification datasets, such as ImageNet-1k, as well as large-scale\nimage-caption datasets like CC12M and LAION-400M. Ablation studies highlight\nthe critical role of our architecture and training in achieving effective\nuncertainty quantification. Our code is publicly available and can be found\nhere: https://github.com/ykrmm/ViLU.\n", "link": "http://arxiv.org/abs/2507.07620v2", "date": "2025-07-11", "relevancy": 2.3403, "topK": [{"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.6032}, {"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.6023}, {"title": "Flexible Visual Recognition by Evidential Modeling of Confusion and Ignorance", "link": "http://openaccess.thecvf.com/content/ICCV2023/html/Fan_Flexible_Visual_Recognition_by_Evidential_Modeling_of_Confusion_and_Ignorance_ICCV_2023_paper.html", "similarity": 0.5606}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20ViLU%3A%20Learning%20Vision-Language%20Uncertainties%20for%20Failure%20Prediction&body=Title%3A%20ViLU%3A%20Learning%20Vision-Language%20Uncertainties%20for%20Failure%20Prediction%0AAuthor%3A%20Marc%20Lafon%20and%20Yannis%20Karmim%20and%20Julio%20Silva-Rodr%C3%ADguez%20and%20Paul%20Couairon%20and%20Cl%C3%A9ment%20Rambour%20and%20Rapha%C3%ABl%20Fournier-Sniehotta%20and%20Ismail%20Ben%20Ayed%20and%20Jose%20Dolz%20and%20Nicolas%20Thome%0AAbstract%3A%20%20%20Reliable%20Uncertainty%20Quantification%20%28UQ%29%20and%20failure%20prediction%20remain%20open%0Achallenges%20for%20Vision-Language%20Models%20%28VLMs%29.%20We%20introduce%20ViLU%2C%20a%20new%0AVision-Language%20Uncertainty%20quantification%20framework%20that%20contextualizes%0Auncertainty%20estimates%20by%20leveraging%20all%20task-relevant%20textual%20representations.%0AViLU%20constructs%20an%20uncertainty-aware%20multi-modal%20representation%20by%20integrating%0Athe%20visual%20embedding%2C%20the%20predicted%20textual%20embedding%2C%20and%20an%20image-conditioned%0Atextual%20representation%20via%20cross-attention.%20Unlike%20traditional%20UQ%20methods%20based%0Aon%20loss%20prediction%2C%20ViLU%20trains%20an%20uncertainty%20predictor%20as%20a%20binary%20classifier%0Ato%20distinguish%20correct%20from%20incorrect%20predictions%20using%20a%20weighted%20binary%0Across-entropy%20loss%2C%20making%20it%20loss-agnostic.%20In%20particular%2C%20our%20proposed%0Aapproach%20is%20well-suited%20for%20post-hoc%20settings%2C%20where%20only%20vision%20and%20text%0Aembeddings%20are%20available%20without%20direct%20access%20to%20the%20model%20itself.%20Extensive%0Aexperiments%20on%20diverse%20datasets%20show%20the%20significant%20gains%20of%20our%20method%0Acompared%20to%20state-of-the-art%20failure%20prediction%20methods.%20We%20apply%20our%20method%20to%0Astandard%20classification%20datasets%2C%20such%20as%20ImageNet-1k%2C%20as%20well%20as%20large-scale%0Aimage-caption%20datasets%20like%20CC12M%20and%20LAION-400M.%20Ablation%20studies%20highlight%0Athe%20critical%20role%20of%20our%20architecture%20and%20training%20in%20achieving%20effective%0Auncertainty%20quantification.%20Our%20code%20is%20publicly%20available%20and%20can%20be%20found%0Ahere%3A%20https%3A//github.com/ykrmm/ViLU.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2507.07620v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DViLU%253A%2520Learning%2520Vision-Language%2520Uncertainties%2520for%2520Failure%2520Prediction%26entry.906535625%3DMarc%2520Lafon%2520and%2520Yannis%2520Karmim%2520and%2520Julio%2520Silva-Rodr%25C3%25ADguez%2520and%2520Paul%2520Couairon%2520and%2520Cl%25C3%25A9ment%2520Rambour%2520and%2520Rapha%25C3%25ABl%2520Fournier-Sniehotta%2520and%2520Ismail%2520Ben%2520Ayed%2520and%2520Jose%2520Dolz%2520and%2520Nicolas%2520Thome%26entry.1292438233%3D%2520%2520Reliable%2520Uncertainty%2520Quantification%2520%2528UQ%2529%2520and%2520failure%2520prediction%2520remain%2520open%250Achallenges%2520for%2520Vision-Language%2520Models%2520%2528VLMs%2529.%2520We%2520introduce%2520ViLU%252C%2520a%2520new%250AVision-Language%2520Uncertainty%2520quantification%2520framework%2520that%2520contextualizes%250Auncertainty%2520estimates%2520by%2520leveraging%2520all%2520task-relevant%2520textual%2520representations.%250AViLU%2520constructs%2520an%2520uncertainty-aware%2520multi-modal%2520representation%2520by%2520integrating%250Athe%2520visual%2520embedding%252C%2520the%2520predicted%2520textual%2520embedding%252C%2520and%2520an%2520image-conditioned%250Atextual%2520representation%2520via%2520cross-attention.%2520Unlike%2520traditional%2520UQ%2520methods%2520based%250Aon%2520loss%2520prediction%252C%2520ViLU%2520trains%2520an%2520uncertainty%2520predictor%2520as%2520a%2520binary%2520classifier%250Ato%2520distinguish%2520correct%2520from%2520incorrect%2520predictions%2520using%2520a%2520weighted%2520binary%250Across-entropy%2520loss%252C%2520making%2520it%2520loss-agnostic.%2520In%2520particular%252C%2520our%2520proposed%250Aapproach%2520is%2520well-suited%2520for%2520post-hoc%2520settings%252C%2520where%2520only%2520vision%2520and%2520text%250Aembeddings%2520are%2520available%2520without%2520direct%2520access%2520to%2520the%2520model%2520itself.%2520Extensive%250Aexperiments%2520on%2520diverse%2520datasets%2520show%2520the%2520significant%2520gains%2520of%2520our%2520method%250Acompared%2520to%2520state-of-the-art%2520failure%2520prediction%2520methods.%2520We%2520apply%2520our%2520method%2520to%250Astandard%2520classification%2520datasets%252C%2520such%2520as%2520ImageNet-1k%252C%2520as%2520well%2520as%2520large-scale%250Aimage-caption%2520datasets%2520like%2520CC12M%2520and%2520LAION-400M.%2520Ablation%2520studies%2520highlight%250Athe%2520critical%2520role%2520of%2520our%2520architecture%2520and%2520training%2520in%2520achieving%2520effective%250Auncertainty%2520quantification.%2520Our%2520code%2520is%2520publicly%2520available%2520and%2520can%2520be%2520found%250Ahere%253A%2520https%253A//github.com/ykrmm/ViLU.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2507.07620v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=ViLU%3A%20Learning%20Vision-Language%20Uncertainties%20for%20Failure%20Prediction&entry.906535625=Marc%20Lafon%20and%20Yannis%20Karmim%20and%20Julio%20Silva-Rodr%C3%ADguez%20and%20Paul%20Couairon%20and%20Cl%C3%A9ment%20Rambour%20and%20Rapha%C3%ABl%20Fournier-Sniehotta%20and%20Ismail%20Ben%20Ayed%20and%20Jose%20Dolz%20and%20Nicolas%20Thome&entry.1292438233=%20%20Reliable%20Uncertainty%20Quantification%20%28UQ%29%20and%20failure%20prediction%20remain%20open%0Achallenges%20for%20Vision-Language%20Models%20%28VLMs%29.%20We%20introduce%20ViLU%2C%20a%20new%0AVision-Language%20Uncertainty%20quantification%20framework%20that%20contextualizes%0Auncertainty%20estimates%20by%20leveraging%20all%20task-relevant%20textual%20representations.%0AViLU%20constructs%20an%20uncertainty-aware%20multi-modal%20representation%20by%20integrating%0Athe%20visual%20embedding%2C%20the%20predicted%20textual%20embedding%2C%20and%20an%20image-conditioned%0Atextual%20representation%20via%20cross-attention.%20Unlike%20traditional%20UQ%20methods%20based%0Aon%20loss%20prediction%2C%20ViLU%20trains%20an%20uncertainty%20predictor%20as%20a%20binary%20classifier%0Ato%20distinguish%20correct%20from%20incorrect%20predictions%20using%20a%20weighted%20binary%0Across-entropy%20loss%2C%20making%20it%20loss-agnostic.%20In%20particular%2C%20our%20proposed%0Aapproach%20is%20well-suited%20for%20post-hoc%20settings%2C%20where%20only%20vision%20and%20text%0Aembeddings%20are%20available%20without%20direct%20access%20to%20the%20model%20itself.%20Extensive%0Aexperiments%20on%20diverse%20datasets%20show%20the%20significant%20gains%20of%20our%20method%0Acompared%20to%20state-of-the-art%20failure%20prediction%20methods.%20We%20apply%20our%20method%20to%0Astandard%20classification%20datasets%2C%20such%20as%20ImageNet-1k%2C%20as%20well%20as%20large-scale%0Aimage-caption%20datasets%20like%20CC12M%20and%20LAION-400M.%20Ablation%20studies%20highlight%0Athe%20critical%20role%20of%20our%20architecture%20and%20training%20in%20achieving%20effective%0Auncertainty%20quantification.%20Our%20code%20is%20publicly%20available%20and%20can%20be%20found%0Ahere%3A%20https%3A//github.com/ykrmm/ViLU.%0A&entry.1838667208=http%3A//arxiv.org/abs/2507.07620v2&entry.124074799=Read"},
{"title": "ONION: A Multi-Layered Framework for Participatory ER Design", "author": "Viktoriia Makovska and George Fletcher and Julia Stoyanovich", "abstract": "  We present ONION, a multi-layered framework for participatory\nEntity-Relationship (ER) modeling that integrates insights from design justice,\nparticipatory AI, and conceptual modeling. ONION introduces a five-stage\nmethodology: Observe, Nurture, Integrate, Optimize, Normalize. It supports\nprogressive abstraction from unstructured stakeholder input to structured ER\ndiagrams.\n  Our approach aims to reduce designer bias, promote inclusive participation,\nand increase transparency through the modeling process. We evaluate ONION\nthrough real-world workshops focused on sociotechnical systems in Ukraine,\nhighlighting how diverse stakeholder engagement leads to richer data models and\ndeeper mutual understanding. Early results demonstrate ONION's potential to\nhost diversity in early-stage data modeling. We conclude with lessons learned,\nlimitations and challenges involved in scaling and refining the framework for\nbroader adoption.\n", "link": "http://arxiv.org/abs/2507.08702v1", "date": "2025-07-11", "relevancy": 2.3399, "topK": [{"title": "VirtualModel: Generating Object-ID-retentive Human-object Interaction\n  Image by Diffusion Model for E-commerce Marketing", "link": "http://arxiv.org/abs/2405.09985v1", "similarity": 0.4722}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.4658}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.4658}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20ONION%3A%20A%20Multi-Layered%20Framework%20for%20Participatory%20ER%20Design&body=Title%3A%20ONION%3A%20A%20Multi-Layered%20Framework%20for%20Participatory%20ER%20Design%0AAuthor%3A%20Viktoriia%20Makovska%20and%20George%20Fletcher%20and%20Julia%20Stoyanovich%0AAbstract%3A%20%20%20We%20present%20ONION%2C%20a%20multi-layered%20framework%20for%20participatory%0AEntity-Relationship%20%28ER%29%20modeling%20that%20integrates%20insights%20from%20design%20justice%2C%0Aparticipatory%20AI%2C%20and%20conceptual%20modeling.%20ONION%20introduces%20a%20five-stage%0Amethodology%3A%20Observe%2C%20Nurture%2C%20Integrate%2C%20Optimize%2C%20Normalize.%20It%20supports%0Aprogressive%20abstraction%20from%20unstructured%20stakeholder%20input%20to%20structured%20ER%0Adiagrams.%0A%20%20Our%20approach%20aims%20to%20reduce%20designer%20bias%2C%20promote%20inclusive%20participation%2C%0Aand%20increase%20transparency%20through%20the%20modeling%20process.%20We%20evaluate%20ONION%0Athrough%20real-world%20workshops%20focused%20on%20sociotechnical%20systems%20in%20Ukraine%2C%0Ahighlighting%20how%20diverse%20stakeholder%20engagement%20leads%20to%20richer%20data%20models%20and%0Adeeper%20mutual%20understanding.%20Early%20results%20demonstrate%20ONION%27s%20potential%20to%0Ahost%20diversity%20in%20early-stage%20data%20modeling.%20We%20conclude%20with%20lessons%20learned%2C%0Alimitations%20and%20challenges%20involved%20in%20scaling%20and%20refining%20the%20framework%20for%0Abroader%20adoption.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2507.08702v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DONION%253A%2520A%2520Multi-Layered%2520Framework%2520for%2520Participatory%2520ER%2520Design%26entry.906535625%3DViktoriia%2520Makovska%2520and%2520George%2520Fletcher%2520and%2520Julia%2520Stoyanovich%26entry.1292438233%3D%2520%2520We%2520present%2520ONION%252C%2520a%2520multi-layered%2520framework%2520for%2520participatory%250AEntity-Relationship%2520%2528ER%2529%2520modeling%2520that%2520integrates%2520insights%2520from%2520design%2520justice%252C%250Aparticipatory%2520AI%252C%2520and%2520conceptual%2520modeling.%2520ONION%2520introduces%2520a%2520five-stage%250Amethodology%253A%2520Observe%252C%2520Nurture%252C%2520Integrate%252C%2520Optimize%252C%2520Normalize.%2520It%2520supports%250Aprogressive%2520abstraction%2520from%2520unstructured%2520stakeholder%2520input%2520to%2520structured%2520ER%250Adiagrams.%250A%2520%2520Our%2520approach%2520aims%2520to%2520reduce%2520designer%2520bias%252C%2520promote%2520inclusive%2520participation%252C%250Aand%2520increase%2520transparency%2520through%2520the%2520modeling%2520process.%2520We%2520evaluate%2520ONION%250Athrough%2520real-world%2520workshops%2520focused%2520on%2520sociotechnical%2520systems%2520in%2520Ukraine%252C%250Ahighlighting%2520how%2520diverse%2520stakeholder%2520engagement%2520leads%2520to%2520richer%2520data%2520models%2520and%250Adeeper%2520mutual%2520understanding.%2520Early%2520results%2520demonstrate%2520ONION%2527s%2520potential%2520to%250Ahost%2520diversity%2520in%2520early-stage%2520data%2520modeling.%2520We%2520conclude%2520with%2520lessons%2520learned%252C%250Alimitations%2520and%2520challenges%2520involved%2520in%2520scaling%2520and%2520refining%2520the%2520framework%2520for%250Abroader%2520adoption.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2507.08702v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=ONION%3A%20A%20Multi-Layered%20Framework%20for%20Participatory%20ER%20Design&entry.906535625=Viktoriia%20Makovska%20and%20George%20Fletcher%20and%20Julia%20Stoyanovich&entry.1292438233=%20%20We%20present%20ONION%2C%20a%20multi-layered%20framework%20for%20participatory%0AEntity-Relationship%20%28ER%29%20modeling%20that%20integrates%20insights%20from%20design%20justice%2C%0Aparticipatory%20AI%2C%20and%20conceptual%20modeling.%20ONION%20introduces%20a%20five-stage%0Amethodology%3A%20Observe%2C%20Nurture%2C%20Integrate%2C%20Optimize%2C%20Normalize.%20It%20supports%0Aprogressive%20abstraction%20from%20unstructured%20stakeholder%20input%20to%20structured%20ER%0Adiagrams.%0A%20%20Our%20approach%20aims%20to%20reduce%20designer%20bias%2C%20promote%20inclusive%20participation%2C%0Aand%20increase%20transparency%20through%20the%20modeling%20process.%20We%20evaluate%20ONION%0Athrough%20real-world%20workshops%20focused%20on%20sociotechnical%20systems%20in%20Ukraine%2C%0Ahighlighting%20how%20diverse%20stakeholder%20engagement%20leads%20to%20richer%20data%20models%20and%0Adeeper%20mutual%20understanding.%20Early%20results%20demonstrate%20ONION%27s%20potential%20to%0Ahost%20diversity%20in%20early-stage%20data%20modeling.%20We%20conclude%20with%20lessons%20learned%2C%0Alimitations%20and%20challenges%20involved%20in%20scaling%20and%20refining%20the%20framework%20for%0Abroader%20adoption.%0A&entry.1838667208=http%3A//arxiv.org/abs/2507.08702v1&entry.124074799=Read"},
{"title": "OnlineBEV: Recurrent Temporal Fusion in Bird's Eye View Representations\n  for Multi-Camera 3D Perception", "author": "Junho Koh and Youngwoo Lee and Jungho Kim and Dongyoung Lee and Jun Won Choi", "abstract": "  Multi-view camera-based 3D perception can be conducted using bird's eye view\n(BEV) features obtained through perspective view-to-BEV transformations.\nSeveral studies have shown that the performance of these 3D perception methods\ncan be further enhanced by combining sequential BEV features obtained from\nmultiple camera frames. However, even after compensating for the ego-motion of\nan autonomous agent, the performance gain from temporal aggregation is limited\nwhen combining a large number of image frames. This limitation arises due to\ndynamic changes in BEV features over time caused by object motion. In this\npaper, we introduce a novel temporal 3D perception method called OnlineBEV,\nwhich combines BEV features over time using a recurrent structure. This\nstructure increases the effective number of combined features with minimal\nmemory usage. However, it is critical to spatially align the features over time\nto maintain strong performance. OnlineBEV employs the Motion-guided BEV Fusion\nNetwork (MBFNet) to achieve temporal feature alignment. MBFNet extracts motion\nfeatures from consecutive BEV frames and dynamically aligns historical BEV\nfeatures with current ones using these motion features. To enforce temporal\nfeature alignment explicitly, we use Temporal Consistency Learning Loss, which\ncaptures discrepancies between historical and target BEV features. Experiments\nconducted on the nuScenes benchmark demonstrate that OnlineBEV achieves\nsignificant performance gains over the current best method, SOLOFusion.\nOnlineBEV achieves 63.9% NDS on the nuScenes test set, recording\nstate-of-the-art performance in the camera-only 3D object detection task.\n", "link": "http://arxiv.org/abs/2507.08644v1", "date": "2025-07-11", "relevancy": 2.3341, "topK": [{"title": "CamCtrl3D: Single-Image Scene Exploration with Precise 3D Camera Control", "link": "http://arxiv.org/abs/2501.06006v1", "similarity": 0.643}, {"title": "CAT3D: Create Anything in 3D with Multi-View Diffusion Models", "link": "http://arxiv.org/abs/2405.10314v1", "similarity": 0.5716}, {"title": "CAT3D: Create Anything in 3D with Multi-View Diffusion Models", "link": "http://arxiv.org/abs/2405.10314v1", "similarity": 0.5716}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20OnlineBEV%3A%20Recurrent%20Temporal%20Fusion%20in%20Bird%27s%20Eye%20View%20Representations%0A%20%20for%20Multi-Camera%203D%20Perception&body=Title%3A%20OnlineBEV%3A%20Recurrent%20Temporal%20Fusion%20in%20Bird%27s%20Eye%20View%20Representations%0A%20%20for%20Multi-Camera%203D%20Perception%0AAuthor%3A%20Junho%20Koh%20and%20Youngwoo%20Lee%20and%20Jungho%20Kim%20and%20Dongyoung%20Lee%20and%20Jun%20Won%20Choi%0AAbstract%3A%20%20%20Multi-view%20camera-based%203D%20perception%20can%20be%20conducted%20using%20bird%27s%20eye%20view%0A%28BEV%29%20features%20obtained%20through%20perspective%20view-to-BEV%20transformations.%0ASeveral%20studies%20have%20shown%20that%20the%20performance%20of%20these%203D%20perception%20methods%0Acan%20be%20further%20enhanced%20by%20combining%20sequential%20BEV%20features%20obtained%20from%0Amultiple%20camera%20frames.%20However%2C%20even%20after%20compensating%20for%20the%20ego-motion%20of%0Aan%20autonomous%20agent%2C%20the%20performance%20gain%20from%20temporal%20aggregation%20is%20limited%0Awhen%20combining%20a%20large%20number%20of%20image%20frames.%20This%20limitation%20arises%20due%20to%0Adynamic%20changes%20in%20BEV%20features%20over%20time%20caused%20by%20object%20motion.%20In%20this%0Apaper%2C%20we%20introduce%20a%20novel%20temporal%203D%20perception%20method%20called%20OnlineBEV%2C%0Awhich%20combines%20BEV%20features%20over%20time%20using%20a%20recurrent%20structure.%20This%0Astructure%20increases%20the%20effective%20number%20of%20combined%20features%20with%20minimal%0Amemory%20usage.%20However%2C%20it%20is%20critical%20to%20spatially%20align%20the%20features%20over%20time%0Ato%20maintain%20strong%20performance.%20OnlineBEV%20employs%20the%20Motion-guided%20BEV%20Fusion%0ANetwork%20%28MBFNet%29%20to%20achieve%20temporal%20feature%20alignment.%20MBFNet%20extracts%20motion%0Afeatures%20from%20consecutive%20BEV%20frames%20and%20dynamically%20aligns%20historical%20BEV%0Afeatures%20with%20current%20ones%20using%20these%20motion%20features.%20To%20enforce%20temporal%0Afeature%20alignment%20explicitly%2C%20we%20use%20Temporal%20Consistency%20Learning%20Loss%2C%20which%0Acaptures%20discrepancies%20between%20historical%20and%20target%20BEV%20features.%20Experiments%0Aconducted%20on%20the%20nuScenes%20benchmark%20demonstrate%20that%20OnlineBEV%20achieves%0Asignificant%20performance%20gains%20over%20the%20current%20best%20method%2C%20SOLOFusion.%0AOnlineBEV%20achieves%2063.9%25%20NDS%20on%20the%20nuScenes%20test%20set%2C%20recording%0Astate-of-the-art%20performance%20in%20the%20camera-only%203D%20object%20detection%20task.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2507.08644v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DOnlineBEV%253A%2520Recurrent%2520Temporal%2520Fusion%2520in%2520Bird%2527s%2520Eye%2520View%2520Representations%250A%2520%2520for%2520Multi-Camera%25203D%2520Perception%26entry.906535625%3DJunho%2520Koh%2520and%2520Youngwoo%2520Lee%2520and%2520Jungho%2520Kim%2520and%2520Dongyoung%2520Lee%2520and%2520Jun%2520Won%2520Choi%26entry.1292438233%3D%2520%2520Multi-view%2520camera-based%25203D%2520perception%2520can%2520be%2520conducted%2520using%2520bird%2527s%2520eye%2520view%250A%2528BEV%2529%2520features%2520obtained%2520through%2520perspective%2520view-to-BEV%2520transformations.%250ASeveral%2520studies%2520have%2520shown%2520that%2520the%2520performance%2520of%2520these%25203D%2520perception%2520methods%250Acan%2520be%2520further%2520enhanced%2520by%2520combining%2520sequential%2520BEV%2520features%2520obtained%2520from%250Amultiple%2520camera%2520frames.%2520However%252C%2520even%2520after%2520compensating%2520for%2520the%2520ego-motion%2520of%250Aan%2520autonomous%2520agent%252C%2520the%2520performance%2520gain%2520from%2520temporal%2520aggregation%2520is%2520limited%250Awhen%2520combining%2520a%2520large%2520number%2520of%2520image%2520frames.%2520This%2520limitation%2520arises%2520due%2520to%250Adynamic%2520changes%2520in%2520BEV%2520features%2520over%2520time%2520caused%2520by%2520object%2520motion.%2520In%2520this%250Apaper%252C%2520we%2520introduce%2520a%2520novel%2520temporal%25203D%2520perception%2520method%2520called%2520OnlineBEV%252C%250Awhich%2520combines%2520BEV%2520features%2520over%2520time%2520using%2520a%2520recurrent%2520structure.%2520This%250Astructure%2520increases%2520the%2520effective%2520number%2520of%2520combined%2520features%2520with%2520minimal%250Amemory%2520usage.%2520However%252C%2520it%2520is%2520critical%2520to%2520spatially%2520align%2520the%2520features%2520over%2520time%250Ato%2520maintain%2520strong%2520performance.%2520OnlineBEV%2520employs%2520the%2520Motion-guided%2520BEV%2520Fusion%250ANetwork%2520%2528MBFNet%2529%2520to%2520achieve%2520temporal%2520feature%2520alignment.%2520MBFNet%2520extracts%2520motion%250Afeatures%2520from%2520consecutive%2520BEV%2520frames%2520and%2520dynamically%2520aligns%2520historical%2520BEV%250Afeatures%2520with%2520current%2520ones%2520using%2520these%2520motion%2520features.%2520To%2520enforce%2520temporal%250Afeature%2520alignment%2520explicitly%252C%2520we%2520use%2520Temporal%2520Consistency%2520Learning%2520Loss%252C%2520which%250Acaptures%2520discrepancies%2520between%2520historical%2520and%2520target%2520BEV%2520features.%2520Experiments%250Aconducted%2520on%2520the%2520nuScenes%2520benchmark%2520demonstrate%2520that%2520OnlineBEV%2520achieves%250Asignificant%2520performance%2520gains%2520over%2520the%2520current%2520best%2520method%252C%2520SOLOFusion.%250AOnlineBEV%2520achieves%252063.9%2525%2520NDS%2520on%2520the%2520nuScenes%2520test%2520set%252C%2520recording%250Astate-of-the-art%2520performance%2520in%2520the%2520camera-only%25203D%2520object%2520detection%2520task.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2507.08644v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=OnlineBEV%3A%20Recurrent%20Temporal%20Fusion%20in%20Bird%27s%20Eye%20View%20Representations%0A%20%20for%20Multi-Camera%203D%20Perception&entry.906535625=Junho%20Koh%20and%20Youngwoo%20Lee%20and%20Jungho%20Kim%20and%20Dongyoung%20Lee%20and%20Jun%20Won%20Choi&entry.1292438233=%20%20Multi-view%20camera-based%203D%20perception%20can%20be%20conducted%20using%20bird%27s%20eye%20view%0A%28BEV%29%20features%20obtained%20through%20perspective%20view-to-BEV%20transformations.%0ASeveral%20studies%20have%20shown%20that%20the%20performance%20of%20these%203D%20perception%20methods%0Acan%20be%20further%20enhanced%20by%20combining%20sequential%20BEV%20features%20obtained%20from%0Amultiple%20camera%20frames.%20However%2C%20even%20after%20compensating%20for%20the%20ego-motion%20of%0Aan%20autonomous%20agent%2C%20the%20performance%20gain%20from%20temporal%20aggregation%20is%20limited%0Awhen%20combining%20a%20large%20number%20of%20image%20frames.%20This%20limitation%20arises%20due%20to%0Adynamic%20changes%20in%20BEV%20features%20over%20time%20caused%20by%20object%20motion.%20In%20this%0Apaper%2C%20we%20introduce%20a%20novel%20temporal%203D%20perception%20method%20called%20OnlineBEV%2C%0Awhich%20combines%20BEV%20features%20over%20time%20using%20a%20recurrent%20structure.%20This%0Astructure%20increases%20the%20effective%20number%20of%20combined%20features%20with%20minimal%0Amemory%20usage.%20However%2C%20it%20is%20critical%20to%20spatially%20align%20the%20features%20over%20time%0Ato%20maintain%20strong%20performance.%20OnlineBEV%20employs%20the%20Motion-guided%20BEV%20Fusion%0ANetwork%20%28MBFNet%29%20to%20achieve%20temporal%20feature%20alignment.%20MBFNet%20extracts%20motion%0Afeatures%20from%20consecutive%20BEV%20frames%20and%20dynamically%20aligns%20historical%20BEV%0Afeatures%20with%20current%20ones%20using%20these%20motion%20features.%20To%20enforce%20temporal%0Afeature%20alignment%20explicitly%2C%20we%20use%20Temporal%20Consistency%20Learning%20Loss%2C%20which%0Acaptures%20discrepancies%20between%20historical%20and%20target%20BEV%20features.%20Experiments%0Aconducted%20on%20the%20nuScenes%20benchmark%20demonstrate%20that%20OnlineBEV%20achieves%0Asignificant%20performance%20gains%20over%20the%20current%20best%20method%2C%20SOLOFusion.%0AOnlineBEV%20achieves%2063.9%25%20NDS%20on%20the%20nuScenes%20test%20set%2C%20recording%0Astate-of-the-art%20performance%20in%20the%20camera-only%203D%20object%20detection%20task.%0A&entry.1838667208=http%3A//arxiv.org/abs/2507.08644v1&entry.124074799=Read"},
{"title": "BayesTTA: Continual-Temporal Test-Time Adaptation for Vision-Language\n  Models via Gaussian Discriminant Analysis", "author": "Shuang Cui and Jinglin Xu and Yi Li and Xiongxin Tang and Jiangmeng Li and Jiahuan Zhou and Fanjiang Xu and Fuchun Sun and Hui Xiong", "abstract": "  Vision-language models (VLMs) such as CLIP achieve strong zero-shot\nrecognition but degrade significantly under \\textit{temporally evolving\ndistribution shifts} common in real-world scenarios (e.g., gradual illumination\nor seasonal changes). Existing continual test-time adaptation (CTTA) methods\nare typically built around sudden and severe distribution shifts and neglect\ntemporal continuity, leading to three core defects: limited memory cache\nrestricts long-range distribution modeling, causing catastrophic forgetting;\nentropy-based confidence becomes unreliable under temporal drift, worsening\nerror accumulation; and static visual representations misalign with evolving\ninputs. We formalize this practical problem as \\textit{Continual-Temporal\nTest-Time Adaptation (CT-TTA)}, where test distributions evolve gradually over\ntime. To address it, we propose \\textit{BayesTTA}, a Bayesian adaptation\nframework that enforces temporally consistent predictions and dynamically\naligns visual representations. Specifically, BayesTTA incrementally estimates\nclass-conditional Gaussian mixture distributions without storing raw data,\nadaptively selects covariance structures through statistical hypothesis\ntesting, and performs calibrated inference using Gaussian discriminant analysis\n(GDA). These calibrated predictions supervise self-paced adaptation of\nnormalization layers, ensuring efficient and stable representation alignment.\nWe establish a comprehensive CT-TTA benchmark across four temporally evolving\ndatasets and further evaluate generalization on ten standard TTA datasets.\nExtensive experiments show that BayesTTA consistently outperforms\nstate-of-the-art methods, achieving significant gains while maintaining\nefficiency. Code is available at\n\\href{https://github.com/cuishuang99/BayesTTA}{https://github.com/cuishuang99/BayesTTA}.\n", "link": "http://arxiv.org/abs/2507.08607v1", "date": "2025-07-11", "relevancy": 2.3338, "topK": [{"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.6197}, {"title": "Customize-A-Video: One-Shot Motion Customization of Text-to-Video\n  Diffusion Models", "link": "http://arxiv.org/abs/2402.14780v1", "similarity": 0.5848}, {"title": "Magic-Me: Identity-Specific Video Customized Diffusion", "link": "http://arxiv.org/abs/2402.09368v1", "similarity": 0.5467}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20BayesTTA%3A%20Continual-Temporal%20Test-Time%20Adaptation%20for%20Vision-Language%0A%20%20Models%20via%20Gaussian%20Discriminant%20Analysis&body=Title%3A%20BayesTTA%3A%20Continual-Temporal%20Test-Time%20Adaptation%20for%20Vision-Language%0A%20%20Models%20via%20Gaussian%20Discriminant%20Analysis%0AAuthor%3A%20Shuang%20Cui%20and%20Jinglin%20Xu%20and%20Yi%20Li%20and%20Xiongxin%20Tang%20and%20Jiangmeng%20Li%20and%20Jiahuan%20Zhou%20and%20Fanjiang%20Xu%20and%20Fuchun%20Sun%20and%20Hui%20Xiong%0AAbstract%3A%20%20%20Vision-language%20models%20%28VLMs%29%20such%20as%20CLIP%20achieve%20strong%20zero-shot%0Arecognition%20but%20degrade%20significantly%20under%20%5Ctextit%7Btemporally%20evolving%0Adistribution%20shifts%7D%20common%20in%20real-world%20scenarios%20%28e.g.%2C%20gradual%20illumination%0Aor%20seasonal%20changes%29.%20Existing%20continual%20test-time%20adaptation%20%28CTTA%29%20methods%0Aare%20typically%20built%20around%20sudden%20and%20severe%20distribution%20shifts%20and%20neglect%0Atemporal%20continuity%2C%20leading%20to%20three%20core%20defects%3A%20limited%20memory%20cache%0Arestricts%20long-range%20distribution%20modeling%2C%20causing%20catastrophic%20forgetting%3B%0Aentropy-based%20confidence%20becomes%20unreliable%20under%20temporal%20drift%2C%20worsening%0Aerror%20accumulation%3B%20and%20static%20visual%20representations%20misalign%20with%20evolving%0Ainputs.%20We%20formalize%20this%20practical%20problem%20as%20%5Ctextit%7BContinual-Temporal%0ATest-Time%20Adaptation%20%28CT-TTA%29%7D%2C%20where%20test%20distributions%20evolve%20gradually%20over%0Atime.%20To%20address%20it%2C%20we%20propose%20%5Ctextit%7BBayesTTA%7D%2C%20a%20Bayesian%20adaptation%0Aframework%20that%20enforces%20temporally%20consistent%20predictions%20and%20dynamically%0Aaligns%20visual%20representations.%20Specifically%2C%20BayesTTA%20incrementally%20estimates%0Aclass-conditional%20Gaussian%20mixture%20distributions%20without%20storing%20raw%20data%2C%0Aadaptively%20selects%20covariance%20structures%20through%20statistical%20hypothesis%0Atesting%2C%20and%20performs%20calibrated%20inference%20using%20Gaussian%20discriminant%20analysis%0A%28GDA%29.%20These%20calibrated%20predictions%20supervise%20self-paced%20adaptation%20of%0Anormalization%20layers%2C%20ensuring%20efficient%20and%20stable%20representation%20alignment.%0AWe%20establish%20a%20comprehensive%20CT-TTA%20benchmark%20across%20four%20temporally%20evolving%0Adatasets%20and%20further%20evaluate%20generalization%20on%20ten%20standard%20TTA%20datasets.%0AExtensive%20experiments%20show%20that%20BayesTTA%20consistently%20outperforms%0Astate-of-the-art%20methods%2C%20achieving%20significant%20gains%20while%20maintaining%0Aefficiency.%20Code%20is%20available%20at%0A%5Chref%7Bhttps%3A//github.com/cuishuang99/BayesTTA%7D%7Bhttps%3A//github.com/cuishuang99/BayesTTA%7D.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2507.08607v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DBayesTTA%253A%2520Continual-Temporal%2520Test-Time%2520Adaptation%2520for%2520Vision-Language%250A%2520%2520Models%2520via%2520Gaussian%2520Discriminant%2520Analysis%26entry.906535625%3DShuang%2520Cui%2520and%2520Jinglin%2520Xu%2520and%2520Yi%2520Li%2520and%2520Xiongxin%2520Tang%2520and%2520Jiangmeng%2520Li%2520and%2520Jiahuan%2520Zhou%2520and%2520Fanjiang%2520Xu%2520and%2520Fuchun%2520Sun%2520and%2520Hui%2520Xiong%26entry.1292438233%3D%2520%2520Vision-language%2520models%2520%2528VLMs%2529%2520such%2520as%2520CLIP%2520achieve%2520strong%2520zero-shot%250Arecognition%2520but%2520degrade%2520significantly%2520under%2520%255Ctextit%257Btemporally%2520evolving%250Adistribution%2520shifts%257D%2520common%2520in%2520real-world%2520scenarios%2520%2528e.g.%252C%2520gradual%2520illumination%250Aor%2520seasonal%2520changes%2529.%2520Existing%2520continual%2520test-time%2520adaptation%2520%2528CTTA%2529%2520methods%250Aare%2520typically%2520built%2520around%2520sudden%2520and%2520severe%2520distribution%2520shifts%2520and%2520neglect%250Atemporal%2520continuity%252C%2520leading%2520to%2520three%2520core%2520defects%253A%2520limited%2520memory%2520cache%250Arestricts%2520long-range%2520distribution%2520modeling%252C%2520causing%2520catastrophic%2520forgetting%253B%250Aentropy-based%2520confidence%2520becomes%2520unreliable%2520under%2520temporal%2520drift%252C%2520worsening%250Aerror%2520accumulation%253B%2520and%2520static%2520visual%2520representations%2520misalign%2520with%2520evolving%250Ainputs.%2520We%2520formalize%2520this%2520practical%2520problem%2520as%2520%255Ctextit%257BContinual-Temporal%250ATest-Time%2520Adaptation%2520%2528CT-TTA%2529%257D%252C%2520where%2520test%2520distributions%2520evolve%2520gradually%2520over%250Atime.%2520To%2520address%2520it%252C%2520we%2520propose%2520%255Ctextit%257BBayesTTA%257D%252C%2520a%2520Bayesian%2520adaptation%250Aframework%2520that%2520enforces%2520temporally%2520consistent%2520predictions%2520and%2520dynamically%250Aaligns%2520visual%2520representations.%2520Specifically%252C%2520BayesTTA%2520incrementally%2520estimates%250Aclass-conditional%2520Gaussian%2520mixture%2520distributions%2520without%2520storing%2520raw%2520data%252C%250Aadaptively%2520selects%2520covariance%2520structures%2520through%2520statistical%2520hypothesis%250Atesting%252C%2520and%2520performs%2520calibrated%2520inference%2520using%2520Gaussian%2520discriminant%2520analysis%250A%2528GDA%2529.%2520These%2520calibrated%2520predictions%2520supervise%2520self-paced%2520adaptation%2520of%250Anormalization%2520layers%252C%2520ensuring%2520efficient%2520and%2520stable%2520representation%2520alignment.%250AWe%2520establish%2520a%2520comprehensive%2520CT-TTA%2520benchmark%2520across%2520four%2520temporally%2520evolving%250Adatasets%2520and%2520further%2520evaluate%2520generalization%2520on%2520ten%2520standard%2520TTA%2520datasets.%250AExtensive%2520experiments%2520show%2520that%2520BayesTTA%2520consistently%2520outperforms%250Astate-of-the-art%2520methods%252C%2520achieving%2520significant%2520gains%2520while%2520maintaining%250Aefficiency.%2520Code%2520is%2520available%2520at%250A%255Chref%257Bhttps%253A//github.com/cuishuang99/BayesTTA%257D%257Bhttps%253A//github.com/cuishuang99/BayesTTA%257D.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2507.08607v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=BayesTTA%3A%20Continual-Temporal%20Test-Time%20Adaptation%20for%20Vision-Language%0A%20%20Models%20via%20Gaussian%20Discriminant%20Analysis&entry.906535625=Shuang%20Cui%20and%20Jinglin%20Xu%20and%20Yi%20Li%20and%20Xiongxin%20Tang%20and%20Jiangmeng%20Li%20and%20Jiahuan%20Zhou%20and%20Fanjiang%20Xu%20and%20Fuchun%20Sun%20and%20Hui%20Xiong&entry.1292438233=%20%20Vision-language%20models%20%28VLMs%29%20such%20as%20CLIP%20achieve%20strong%20zero-shot%0Arecognition%20but%20degrade%20significantly%20under%20%5Ctextit%7Btemporally%20evolving%0Adistribution%20shifts%7D%20common%20in%20real-world%20scenarios%20%28e.g.%2C%20gradual%20illumination%0Aor%20seasonal%20changes%29.%20Existing%20continual%20test-time%20adaptation%20%28CTTA%29%20methods%0Aare%20typically%20built%20around%20sudden%20and%20severe%20distribution%20shifts%20and%20neglect%0Atemporal%20continuity%2C%20leading%20to%20three%20core%20defects%3A%20limited%20memory%20cache%0Arestricts%20long-range%20distribution%20modeling%2C%20causing%20catastrophic%20forgetting%3B%0Aentropy-based%20confidence%20becomes%20unreliable%20under%20temporal%20drift%2C%20worsening%0Aerror%20accumulation%3B%20and%20static%20visual%20representations%20misalign%20with%20evolving%0Ainputs.%20We%20formalize%20this%20practical%20problem%20as%20%5Ctextit%7BContinual-Temporal%0ATest-Time%20Adaptation%20%28CT-TTA%29%7D%2C%20where%20test%20distributions%20evolve%20gradually%20over%0Atime.%20To%20address%20it%2C%20we%20propose%20%5Ctextit%7BBayesTTA%7D%2C%20a%20Bayesian%20adaptation%0Aframework%20that%20enforces%20temporally%20consistent%20predictions%20and%20dynamically%0Aaligns%20visual%20representations.%20Specifically%2C%20BayesTTA%20incrementally%20estimates%0Aclass-conditional%20Gaussian%20mixture%20distributions%20without%20storing%20raw%20data%2C%0Aadaptively%20selects%20covariance%20structures%20through%20statistical%20hypothesis%0Atesting%2C%20and%20performs%20calibrated%20inference%20using%20Gaussian%20discriminant%20analysis%0A%28GDA%29.%20These%20calibrated%20predictions%20supervise%20self-paced%20adaptation%20of%0Anormalization%20layers%2C%20ensuring%20efficient%20and%20stable%20representation%20alignment.%0AWe%20establish%20a%20comprehensive%20CT-TTA%20benchmark%20across%20four%20temporally%20evolving%0Adatasets%20and%20further%20evaluate%20generalization%20on%20ten%20standard%20TTA%20datasets.%0AExtensive%20experiments%20show%20that%20BayesTTA%20consistently%20outperforms%0Astate-of-the-art%20methods%2C%20achieving%20significant%20gains%20while%20maintaining%0Aefficiency.%20Code%20is%20available%20at%0A%5Chref%7Bhttps%3A//github.com/cuishuang99/BayesTTA%7D%7Bhttps%3A//github.com/cuishuang99/BayesTTA%7D.%0A&entry.1838667208=http%3A//arxiv.org/abs/2507.08607v1&entry.124074799=Read"},
{"title": "Lumos-1: On Autoregressive Video Generation from a Unified Model\n  Perspective", "author": "Hangjie Yuan and Weihua Chen and Jun Cen and Hu Yu and Jingyun Liang and Shuning Chang and Zhihui Lin and Tao Feng and Pengwei Liu and Jiazheng Xing and Hao Luo and Jiasheng Tang and Fan Wang and Yi Yang", "abstract": "  Autoregressive large language models (LLMs) have unified a vast range of\nlanguage tasks, inspiring preliminary efforts in autoregressive video\ngeneration. Existing autoregressive video generators either diverge from\nstandard LLM architectures, depend on bulky external text encoders, or incur\nprohibitive latency due to next-token decoding. In this paper, we introduce\nLumos-1, an autoregressive video generator that retains the LLM architecture\nwith minimal architectural modifications. To inject spatiotemporal correlations\nin LLMs, we identify the efficacy of incorporating 3D RoPE and diagnose its\nimbalanced frequency spectrum ranges. Therefore, we propose MM-RoPE, a RoPE\nscheme that preserves the original textual RoPE while providing comprehensive\nfrequency spectra and scaled 3D positions for modeling multimodal\nspatiotemporal data. Moreover, Lumos-1 resorts to a token dependency strategy\nthat obeys intra-frame bidirectionality and inter-frame temporal causality.\nBased on this dependency strategy, we identify the issue of frame-wise loss\nimbalance caused by spatial information redundancy and solve it by proposing\nAutoregressive Discrete Diffusion Forcing (AR-DF). AR-DF introduces temporal\ntube masking during training with a compatible inference-time masking policy to\navoid quality degradation. By using memory-efficient training techniques, we\npre-train Lumos-1 on only 48 GPUs, achieving performance comparable to EMU3 on\nGenEval, COSMOS-Video2World on VBench-I2V, and OpenSoraPlan on VBench-T2V. Code\nand models are available at https://github.com/alibaba-damo-academy/Lumos.\n", "link": "http://arxiv.org/abs/2507.08801v1", "date": "2025-07-11", "relevancy": 2.3094, "topK": [{"title": "Magic-Me: Identity-Specific Video Customized Diffusion", "link": "http://arxiv.org/abs/2402.09368v1", "similarity": 0.587}, {"title": "Customize-A-Video: One-Shot Motion Customization of Text-to-Video\n  Diffusion Models", "link": "http://arxiv.org/abs/2402.14780v1", "similarity": 0.5842}, {"title": "Genie: Generative Interactive Environments", "link": "http://arxiv.org/abs/2402.15391v1", "similarity": 0.565}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Lumos-1%3A%20On%20Autoregressive%20Video%20Generation%20from%20a%20Unified%20Model%0A%20%20Perspective&body=Title%3A%20Lumos-1%3A%20On%20Autoregressive%20Video%20Generation%20from%20a%20Unified%20Model%0A%20%20Perspective%0AAuthor%3A%20Hangjie%20Yuan%20and%20Weihua%20Chen%20and%20Jun%20Cen%20and%20Hu%20Yu%20and%20Jingyun%20Liang%20and%20Shuning%20Chang%20and%20Zhihui%20Lin%20and%20Tao%20Feng%20and%20Pengwei%20Liu%20and%20Jiazheng%20Xing%20and%20Hao%20Luo%20and%20Jiasheng%20Tang%20and%20Fan%20Wang%20and%20Yi%20Yang%0AAbstract%3A%20%20%20Autoregressive%20large%20language%20models%20%28LLMs%29%20have%20unified%20a%20vast%20range%20of%0Alanguage%20tasks%2C%20inspiring%20preliminary%20efforts%20in%20autoregressive%20video%0Ageneration.%20Existing%20autoregressive%20video%20generators%20either%20diverge%20from%0Astandard%20LLM%20architectures%2C%20depend%20on%20bulky%20external%20text%20encoders%2C%20or%20incur%0Aprohibitive%20latency%20due%20to%20next-token%20decoding.%20In%20this%20paper%2C%20we%20introduce%0ALumos-1%2C%20an%20autoregressive%20video%20generator%20that%20retains%20the%20LLM%20architecture%0Awith%20minimal%20architectural%20modifications.%20To%20inject%20spatiotemporal%20correlations%0Ain%20LLMs%2C%20we%20identify%20the%20efficacy%20of%20incorporating%203D%20RoPE%20and%20diagnose%20its%0Aimbalanced%20frequency%20spectrum%20ranges.%20Therefore%2C%20we%20propose%20MM-RoPE%2C%20a%20RoPE%0Ascheme%20that%20preserves%20the%20original%20textual%20RoPE%20while%20providing%20comprehensive%0Afrequency%20spectra%20and%20scaled%203D%20positions%20for%20modeling%20multimodal%0Aspatiotemporal%20data.%20Moreover%2C%20Lumos-1%20resorts%20to%20a%20token%20dependency%20strategy%0Athat%20obeys%20intra-frame%20bidirectionality%20and%20inter-frame%20temporal%20causality.%0ABased%20on%20this%20dependency%20strategy%2C%20we%20identify%20the%20issue%20of%20frame-wise%20loss%0Aimbalance%20caused%20by%20spatial%20information%20redundancy%20and%20solve%20it%20by%20proposing%0AAutoregressive%20Discrete%20Diffusion%20Forcing%20%28AR-DF%29.%20AR-DF%20introduces%20temporal%0Atube%20masking%20during%20training%20with%20a%20compatible%20inference-time%20masking%20policy%20to%0Aavoid%20quality%20degradation.%20By%20using%20memory-efficient%20training%20techniques%2C%20we%0Apre-train%20Lumos-1%20on%20only%2048%20GPUs%2C%20achieving%20performance%20comparable%20to%20EMU3%20on%0AGenEval%2C%20COSMOS-Video2World%20on%20VBench-I2V%2C%20and%20OpenSoraPlan%20on%20VBench-T2V.%20Code%0Aand%20models%20are%20available%20at%20https%3A//github.com/alibaba-damo-academy/Lumos.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2507.08801v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DLumos-1%253A%2520On%2520Autoregressive%2520Video%2520Generation%2520from%2520a%2520Unified%2520Model%250A%2520%2520Perspective%26entry.906535625%3DHangjie%2520Yuan%2520and%2520Weihua%2520Chen%2520and%2520Jun%2520Cen%2520and%2520Hu%2520Yu%2520and%2520Jingyun%2520Liang%2520and%2520Shuning%2520Chang%2520and%2520Zhihui%2520Lin%2520and%2520Tao%2520Feng%2520and%2520Pengwei%2520Liu%2520and%2520Jiazheng%2520Xing%2520and%2520Hao%2520Luo%2520and%2520Jiasheng%2520Tang%2520and%2520Fan%2520Wang%2520and%2520Yi%2520Yang%26entry.1292438233%3D%2520%2520Autoregressive%2520large%2520language%2520models%2520%2528LLMs%2529%2520have%2520unified%2520a%2520vast%2520range%2520of%250Alanguage%2520tasks%252C%2520inspiring%2520preliminary%2520efforts%2520in%2520autoregressive%2520video%250Ageneration.%2520Existing%2520autoregressive%2520video%2520generators%2520either%2520diverge%2520from%250Astandard%2520LLM%2520architectures%252C%2520depend%2520on%2520bulky%2520external%2520text%2520encoders%252C%2520or%2520incur%250Aprohibitive%2520latency%2520due%2520to%2520next-token%2520decoding.%2520In%2520this%2520paper%252C%2520we%2520introduce%250ALumos-1%252C%2520an%2520autoregressive%2520video%2520generator%2520that%2520retains%2520the%2520LLM%2520architecture%250Awith%2520minimal%2520architectural%2520modifications.%2520To%2520inject%2520spatiotemporal%2520correlations%250Ain%2520LLMs%252C%2520we%2520identify%2520the%2520efficacy%2520of%2520incorporating%25203D%2520RoPE%2520and%2520diagnose%2520its%250Aimbalanced%2520frequency%2520spectrum%2520ranges.%2520Therefore%252C%2520we%2520propose%2520MM-RoPE%252C%2520a%2520RoPE%250Ascheme%2520that%2520preserves%2520the%2520original%2520textual%2520RoPE%2520while%2520providing%2520comprehensive%250Afrequency%2520spectra%2520and%2520scaled%25203D%2520positions%2520for%2520modeling%2520multimodal%250Aspatiotemporal%2520data.%2520Moreover%252C%2520Lumos-1%2520resorts%2520to%2520a%2520token%2520dependency%2520strategy%250Athat%2520obeys%2520intra-frame%2520bidirectionality%2520and%2520inter-frame%2520temporal%2520causality.%250ABased%2520on%2520this%2520dependency%2520strategy%252C%2520we%2520identify%2520the%2520issue%2520of%2520frame-wise%2520loss%250Aimbalance%2520caused%2520by%2520spatial%2520information%2520redundancy%2520and%2520solve%2520it%2520by%2520proposing%250AAutoregressive%2520Discrete%2520Diffusion%2520Forcing%2520%2528AR-DF%2529.%2520AR-DF%2520introduces%2520temporal%250Atube%2520masking%2520during%2520training%2520with%2520a%2520compatible%2520inference-time%2520masking%2520policy%2520to%250Aavoid%2520quality%2520degradation.%2520By%2520using%2520memory-efficient%2520training%2520techniques%252C%2520we%250Apre-train%2520Lumos-1%2520on%2520only%252048%2520GPUs%252C%2520achieving%2520performance%2520comparable%2520to%2520EMU3%2520on%250AGenEval%252C%2520COSMOS-Video2World%2520on%2520VBench-I2V%252C%2520and%2520OpenSoraPlan%2520on%2520VBench-T2V.%2520Code%250Aand%2520models%2520are%2520available%2520at%2520https%253A//github.com/alibaba-damo-academy/Lumos.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2507.08801v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Lumos-1%3A%20On%20Autoregressive%20Video%20Generation%20from%20a%20Unified%20Model%0A%20%20Perspective&entry.906535625=Hangjie%20Yuan%20and%20Weihua%20Chen%20and%20Jun%20Cen%20and%20Hu%20Yu%20and%20Jingyun%20Liang%20and%20Shuning%20Chang%20and%20Zhihui%20Lin%20and%20Tao%20Feng%20and%20Pengwei%20Liu%20and%20Jiazheng%20Xing%20and%20Hao%20Luo%20and%20Jiasheng%20Tang%20and%20Fan%20Wang%20and%20Yi%20Yang&entry.1292438233=%20%20Autoregressive%20large%20language%20models%20%28LLMs%29%20have%20unified%20a%20vast%20range%20of%0Alanguage%20tasks%2C%20inspiring%20preliminary%20efforts%20in%20autoregressive%20video%0Ageneration.%20Existing%20autoregressive%20video%20generators%20either%20diverge%20from%0Astandard%20LLM%20architectures%2C%20depend%20on%20bulky%20external%20text%20encoders%2C%20or%20incur%0Aprohibitive%20latency%20due%20to%20next-token%20decoding.%20In%20this%20paper%2C%20we%20introduce%0ALumos-1%2C%20an%20autoregressive%20video%20generator%20that%20retains%20the%20LLM%20architecture%0Awith%20minimal%20architectural%20modifications.%20To%20inject%20spatiotemporal%20correlations%0Ain%20LLMs%2C%20we%20identify%20the%20efficacy%20of%20incorporating%203D%20RoPE%20and%20diagnose%20its%0Aimbalanced%20frequency%20spectrum%20ranges.%20Therefore%2C%20we%20propose%20MM-RoPE%2C%20a%20RoPE%0Ascheme%20that%20preserves%20the%20original%20textual%20RoPE%20while%20providing%20comprehensive%0Afrequency%20spectra%20and%20scaled%203D%20positions%20for%20modeling%20multimodal%0Aspatiotemporal%20data.%20Moreover%2C%20Lumos-1%20resorts%20to%20a%20token%20dependency%20strategy%0Athat%20obeys%20intra-frame%20bidirectionality%20and%20inter-frame%20temporal%20causality.%0ABased%20on%20this%20dependency%20strategy%2C%20we%20identify%20the%20issue%20of%20frame-wise%20loss%0Aimbalance%20caused%20by%20spatial%20information%20redundancy%20and%20solve%20it%20by%20proposing%0AAutoregressive%20Discrete%20Diffusion%20Forcing%20%28AR-DF%29.%20AR-DF%20introduces%20temporal%0Atube%20masking%20during%20training%20with%20a%20compatible%20inference-time%20masking%20policy%20to%0Aavoid%20quality%20degradation.%20By%20using%20memory-efficient%20training%20techniques%2C%20we%0Apre-train%20Lumos-1%20on%20only%2048%20GPUs%2C%20achieving%20performance%20comparable%20to%20EMU3%20on%0AGenEval%2C%20COSMOS-Video2World%20on%20VBench-I2V%2C%20and%20OpenSoraPlan%20on%20VBench-T2V.%20Code%0Aand%20models%20are%20available%20at%20https%3A//github.com/alibaba-damo-academy/Lumos.%0A&entry.1838667208=http%3A//arxiv.org/abs/2507.08801v1&entry.124074799=Read"},
{"title": "Catastrophic Forgetting Mitigation Through Plateau Phase Activity\n  Profiling", "author": "Idan Mashiach and Oren Glickman and Tom Tirer", "abstract": "  Catastrophic forgetting in deep neural networks occurs when learning new\ntasks degrades performance on previously learned tasks due to knowledge\noverwriting. Among the approaches to mitigate this issue, regularization\ntechniques aim to identify and constrain \"important\" parameters to preserve\nprevious knowledge. In the highly nonconvex optimization landscape of deep\nlearning, we propose a novel perspective: tracking parameters during the final\ntraining plateau is more effective than monitoring them throughout the entire\ntraining process. We argue that parameters that exhibit higher activity\n(movement and variability) during this plateau reveal directions in the loss\nlandscape that are relatively flat, making them suitable for adaptation to new\ntasks while preserving knowledge from previous ones. Our comprehensive\nexperiments demonstrate that this approach achieves superior performance in\nbalancing catastrophic forgetting mitigation with strong performance on newly\nlearned tasks.\n", "link": "http://arxiv.org/abs/2507.08736v1", "date": "2025-07-11", "relevancy": 2.2826, "topK": [{"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.4629}, {"title": "ToDo: Token Downsampling for Efficient Generation of High-Resolution\n  Images", "link": "http://arxiv.org/abs/2402.13573v2", "similarity": 0.4567}, {"title": "Weakly-guided self-supervised pretraining for temporal activity detection", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/25189", "similarity": 0.45}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Catastrophic%20Forgetting%20Mitigation%20Through%20Plateau%20Phase%20Activity%0A%20%20Profiling&body=Title%3A%20Catastrophic%20Forgetting%20Mitigation%20Through%20Plateau%20Phase%20Activity%0A%20%20Profiling%0AAuthor%3A%20Idan%20Mashiach%20and%20Oren%20Glickman%20and%20Tom%20Tirer%0AAbstract%3A%20%20%20Catastrophic%20forgetting%20in%20deep%20neural%20networks%20occurs%20when%20learning%20new%0Atasks%20degrades%20performance%20on%20previously%20learned%20tasks%20due%20to%20knowledge%0Aoverwriting.%20Among%20the%20approaches%20to%20mitigate%20this%20issue%2C%20regularization%0Atechniques%20aim%20to%20identify%20and%20constrain%20%22important%22%20parameters%20to%20preserve%0Aprevious%20knowledge.%20In%20the%20highly%20nonconvex%20optimization%20landscape%20of%20deep%0Alearning%2C%20we%20propose%20a%20novel%20perspective%3A%20tracking%20parameters%20during%20the%20final%0Atraining%20plateau%20is%20more%20effective%20than%20monitoring%20them%20throughout%20the%20entire%0Atraining%20process.%20We%20argue%20that%20parameters%20that%20exhibit%20higher%20activity%0A%28movement%20and%20variability%29%20during%20this%20plateau%20reveal%20directions%20in%20the%20loss%0Alandscape%20that%20are%20relatively%20flat%2C%20making%20them%20suitable%20for%20adaptation%20to%20new%0Atasks%20while%20preserving%20knowledge%20from%20previous%20ones.%20Our%20comprehensive%0Aexperiments%20demonstrate%20that%20this%20approach%20achieves%20superior%20performance%20in%0Abalancing%20catastrophic%20forgetting%20mitigation%20with%20strong%20performance%20on%20newly%0Alearned%20tasks.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2507.08736v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DCatastrophic%2520Forgetting%2520Mitigation%2520Through%2520Plateau%2520Phase%2520Activity%250A%2520%2520Profiling%26entry.906535625%3DIdan%2520Mashiach%2520and%2520Oren%2520Glickman%2520and%2520Tom%2520Tirer%26entry.1292438233%3D%2520%2520Catastrophic%2520forgetting%2520in%2520deep%2520neural%2520networks%2520occurs%2520when%2520learning%2520new%250Atasks%2520degrades%2520performance%2520on%2520previously%2520learned%2520tasks%2520due%2520to%2520knowledge%250Aoverwriting.%2520Among%2520the%2520approaches%2520to%2520mitigate%2520this%2520issue%252C%2520regularization%250Atechniques%2520aim%2520to%2520identify%2520and%2520constrain%2520%2522important%2522%2520parameters%2520to%2520preserve%250Aprevious%2520knowledge.%2520In%2520the%2520highly%2520nonconvex%2520optimization%2520landscape%2520of%2520deep%250Alearning%252C%2520we%2520propose%2520a%2520novel%2520perspective%253A%2520tracking%2520parameters%2520during%2520the%2520final%250Atraining%2520plateau%2520is%2520more%2520effective%2520than%2520monitoring%2520them%2520throughout%2520the%2520entire%250Atraining%2520process.%2520We%2520argue%2520that%2520parameters%2520that%2520exhibit%2520higher%2520activity%250A%2528movement%2520and%2520variability%2529%2520during%2520this%2520plateau%2520reveal%2520directions%2520in%2520the%2520loss%250Alandscape%2520that%2520are%2520relatively%2520flat%252C%2520making%2520them%2520suitable%2520for%2520adaptation%2520to%2520new%250Atasks%2520while%2520preserving%2520knowledge%2520from%2520previous%2520ones.%2520Our%2520comprehensive%250Aexperiments%2520demonstrate%2520that%2520this%2520approach%2520achieves%2520superior%2520performance%2520in%250Abalancing%2520catastrophic%2520forgetting%2520mitigation%2520with%2520strong%2520performance%2520on%2520newly%250Alearned%2520tasks.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2507.08736v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Catastrophic%20Forgetting%20Mitigation%20Through%20Plateau%20Phase%20Activity%0A%20%20Profiling&entry.906535625=Idan%20Mashiach%20and%20Oren%20Glickman%20and%20Tom%20Tirer&entry.1292438233=%20%20Catastrophic%20forgetting%20in%20deep%20neural%20networks%20occurs%20when%20learning%20new%0Atasks%20degrades%20performance%20on%20previously%20learned%20tasks%20due%20to%20knowledge%0Aoverwriting.%20Among%20the%20approaches%20to%20mitigate%20this%20issue%2C%20regularization%0Atechniques%20aim%20to%20identify%20and%20constrain%20%22important%22%20parameters%20to%20preserve%0Aprevious%20knowledge.%20In%20the%20highly%20nonconvex%20optimization%20landscape%20of%20deep%0Alearning%2C%20we%20propose%20a%20novel%20perspective%3A%20tracking%20parameters%20during%20the%20final%0Atraining%20plateau%20is%20more%20effective%20than%20monitoring%20them%20throughout%20the%20entire%0Atraining%20process.%20We%20argue%20that%20parameters%20that%20exhibit%20higher%20activity%0A%28movement%20and%20variability%29%20during%20this%20plateau%20reveal%20directions%20in%20the%20loss%0Alandscape%20that%20are%20relatively%20flat%2C%20making%20them%20suitable%20for%20adaptation%20to%20new%0Atasks%20while%20preserving%20knowledge%20from%20previous%20ones.%20Our%20comprehensive%0Aexperiments%20demonstrate%20that%20this%20approach%20achieves%20superior%20performance%20in%0Abalancing%20catastrophic%20forgetting%20mitigation%20with%20strong%20performance%20on%20newly%0Alearned%20tasks.%0A&entry.1838667208=http%3A//arxiv.org/abs/2507.08736v1&entry.124074799=Read"},
{"title": "USAD: End-to-End Human Activity Recognition via Diffusion Model with\n  Spatiotemporal Attention", "author": "Hang Xiao and Ying Yu and Jiarui Li and Zhifan Yang and Haotian Tang and Hanyu Liu and Chao Li", "abstract": "  The primary objective of human activity recognition (HAR) is to infer ongoing\nhuman actions from sensor data, a task that finds broad applications in health\nmonitoring, safety protection, and sports analysis. Despite proliferating\nresearch, HAR still faces key challenges, including the scarcity of labeled\nsamples for rare activities, insufficient extraction of high-level features,\nand suboptimal model performance on lightweight devices. To address these\nissues, this paper proposes a comprehensive optimization approach centered on\nmulti-attention interaction mechanisms. First, an unsupervised,\nstatistics-guided diffusion model is employed to perform data augmentation,\nthereby alleviating the problems of labeled data scarcity and severe class\nimbalance. Second, a multi-branch spatio-temporal interaction network is\ndesigned, which captures multi-scale features of sequential data through\nparallel residual branches with 3*3, 5*5, and 7*7 convolutional kernels.\nSimultaneously, temporal attention mechanisms are incorporated to identify\ncritical time points, while spatial attention enhances inter-sensor\ninteractions. A cross-branch feature fusion unit is further introduced to\nimprove the overall feature representation capability. Finally, an adaptive\nmulti-loss function fusion strategy is integrated, allowing for dynamic\nadjustment of loss weights and overall model optimization. Experimental results\non three public datasets, WISDM, PAMAP2, and OPPORTUNITY, demonstrate that the\nproposed unsupervised data augmentation spatio-temporal attention diffusion\nnetwork (USAD) achieves accuracies of 98.84%, 93.81%, and 80.92% respectively,\nsignificantly outperforming existing approaches. Furthermore, practical\ndeployment on embedded devices verifies the efficiency and feasibility of the\nproposed method.\n", "link": "http://arxiv.org/abs/2507.02827v2", "date": "2025-07-11", "relevancy": 2.2797, "topK": [{"title": "Weakly-guided self-supervised pretraining for temporal activity detection", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/25189", "similarity": 0.5883}, {"title": "SDXS: Real-Time One-Step Latent Diffusion Models with Image Conditions", "link": "http://arxiv.org/abs/2403.16627v1", "similarity": 0.5607}, {"title": "ToDo: Token Downsampling for Efficient Generation of High-Resolution\n  Images", "link": "http://arxiv.org/abs/2402.13573v2", "similarity": 0.5553}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20USAD%3A%20End-to-End%20Human%20Activity%20Recognition%20via%20Diffusion%20Model%20with%0A%20%20Spatiotemporal%20Attention&body=Title%3A%20USAD%3A%20End-to-End%20Human%20Activity%20Recognition%20via%20Diffusion%20Model%20with%0A%20%20Spatiotemporal%20Attention%0AAuthor%3A%20Hang%20Xiao%20and%20Ying%20Yu%20and%20Jiarui%20Li%20and%20Zhifan%20Yang%20and%20Haotian%20Tang%20and%20Hanyu%20Liu%20and%20Chao%20Li%0AAbstract%3A%20%20%20The%20primary%20objective%20of%20human%20activity%20recognition%20%28HAR%29%20is%20to%20infer%20ongoing%0Ahuman%20actions%20from%20sensor%20data%2C%20a%20task%20that%20finds%20broad%20applications%20in%20health%0Amonitoring%2C%20safety%20protection%2C%20and%20sports%20analysis.%20Despite%20proliferating%0Aresearch%2C%20HAR%20still%20faces%20key%20challenges%2C%20including%20the%20scarcity%20of%20labeled%0Asamples%20for%20rare%20activities%2C%20insufficient%20extraction%20of%20high-level%20features%2C%0Aand%20suboptimal%20model%20performance%20on%20lightweight%20devices.%20To%20address%20these%0Aissues%2C%20this%20paper%20proposes%20a%20comprehensive%20optimization%20approach%20centered%20on%0Amulti-attention%20interaction%20mechanisms.%20First%2C%20an%20unsupervised%2C%0Astatistics-guided%20diffusion%20model%20is%20employed%20to%20perform%20data%20augmentation%2C%0Athereby%20alleviating%20the%20problems%20of%20labeled%20data%20scarcity%20and%20severe%20class%0Aimbalance.%20Second%2C%20a%20multi-branch%20spatio-temporal%20interaction%20network%20is%0Adesigned%2C%20which%20captures%20multi-scale%20features%20of%20sequential%20data%20through%0Aparallel%20residual%20branches%20with%203%2A3%2C%205%2A5%2C%20and%207%2A7%20convolutional%20kernels.%0ASimultaneously%2C%20temporal%20attention%20mechanisms%20are%20incorporated%20to%20identify%0Acritical%20time%20points%2C%20while%20spatial%20attention%20enhances%20inter-sensor%0Ainteractions.%20A%20cross-branch%20feature%20fusion%20unit%20is%20further%20introduced%20to%0Aimprove%20the%20overall%20feature%20representation%20capability.%20Finally%2C%20an%20adaptive%0Amulti-loss%20function%20fusion%20strategy%20is%20integrated%2C%20allowing%20for%20dynamic%0Aadjustment%20of%20loss%20weights%20and%20overall%20model%20optimization.%20Experimental%20results%0Aon%20three%20public%20datasets%2C%20WISDM%2C%20PAMAP2%2C%20and%20OPPORTUNITY%2C%20demonstrate%20that%20the%0Aproposed%20unsupervised%20data%20augmentation%20spatio-temporal%20attention%20diffusion%0Anetwork%20%28USAD%29%20achieves%20accuracies%20of%2098.84%25%2C%2093.81%25%2C%20and%2080.92%25%20respectively%2C%0Asignificantly%20outperforming%20existing%20approaches.%20Furthermore%2C%20practical%0Adeployment%20on%20embedded%20devices%20verifies%20the%20efficiency%20and%20feasibility%20of%20the%0Aproposed%20method.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2507.02827v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DUSAD%253A%2520End-to-End%2520Human%2520Activity%2520Recognition%2520via%2520Diffusion%2520Model%2520with%250A%2520%2520Spatiotemporal%2520Attention%26entry.906535625%3DHang%2520Xiao%2520and%2520Ying%2520Yu%2520and%2520Jiarui%2520Li%2520and%2520Zhifan%2520Yang%2520and%2520Haotian%2520Tang%2520and%2520Hanyu%2520Liu%2520and%2520Chao%2520Li%26entry.1292438233%3D%2520%2520The%2520primary%2520objective%2520of%2520human%2520activity%2520recognition%2520%2528HAR%2529%2520is%2520to%2520infer%2520ongoing%250Ahuman%2520actions%2520from%2520sensor%2520data%252C%2520a%2520task%2520that%2520finds%2520broad%2520applications%2520in%2520health%250Amonitoring%252C%2520safety%2520protection%252C%2520and%2520sports%2520analysis.%2520Despite%2520proliferating%250Aresearch%252C%2520HAR%2520still%2520faces%2520key%2520challenges%252C%2520including%2520the%2520scarcity%2520of%2520labeled%250Asamples%2520for%2520rare%2520activities%252C%2520insufficient%2520extraction%2520of%2520high-level%2520features%252C%250Aand%2520suboptimal%2520model%2520performance%2520on%2520lightweight%2520devices.%2520To%2520address%2520these%250Aissues%252C%2520this%2520paper%2520proposes%2520a%2520comprehensive%2520optimization%2520approach%2520centered%2520on%250Amulti-attention%2520interaction%2520mechanisms.%2520First%252C%2520an%2520unsupervised%252C%250Astatistics-guided%2520diffusion%2520model%2520is%2520employed%2520to%2520perform%2520data%2520augmentation%252C%250Athereby%2520alleviating%2520the%2520problems%2520of%2520labeled%2520data%2520scarcity%2520and%2520severe%2520class%250Aimbalance.%2520Second%252C%2520a%2520multi-branch%2520spatio-temporal%2520interaction%2520network%2520is%250Adesigned%252C%2520which%2520captures%2520multi-scale%2520features%2520of%2520sequential%2520data%2520through%250Aparallel%2520residual%2520branches%2520with%25203%252A3%252C%25205%252A5%252C%2520and%25207%252A7%2520convolutional%2520kernels.%250ASimultaneously%252C%2520temporal%2520attention%2520mechanisms%2520are%2520incorporated%2520to%2520identify%250Acritical%2520time%2520points%252C%2520while%2520spatial%2520attention%2520enhances%2520inter-sensor%250Ainteractions.%2520A%2520cross-branch%2520feature%2520fusion%2520unit%2520is%2520further%2520introduced%2520to%250Aimprove%2520the%2520overall%2520feature%2520representation%2520capability.%2520Finally%252C%2520an%2520adaptive%250Amulti-loss%2520function%2520fusion%2520strategy%2520is%2520integrated%252C%2520allowing%2520for%2520dynamic%250Aadjustment%2520of%2520loss%2520weights%2520and%2520overall%2520model%2520optimization.%2520Experimental%2520results%250Aon%2520three%2520public%2520datasets%252C%2520WISDM%252C%2520PAMAP2%252C%2520and%2520OPPORTUNITY%252C%2520demonstrate%2520that%2520the%250Aproposed%2520unsupervised%2520data%2520augmentation%2520spatio-temporal%2520attention%2520diffusion%250Anetwork%2520%2528USAD%2529%2520achieves%2520accuracies%2520of%252098.84%2525%252C%252093.81%2525%252C%2520and%252080.92%2525%2520respectively%252C%250Asignificantly%2520outperforming%2520existing%2520approaches.%2520Furthermore%252C%2520practical%250Adeployment%2520on%2520embedded%2520devices%2520verifies%2520the%2520efficiency%2520and%2520feasibility%2520of%2520the%250Aproposed%2520method.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2507.02827v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=USAD%3A%20End-to-End%20Human%20Activity%20Recognition%20via%20Diffusion%20Model%20with%0A%20%20Spatiotemporal%20Attention&entry.906535625=Hang%20Xiao%20and%20Ying%20Yu%20and%20Jiarui%20Li%20and%20Zhifan%20Yang%20and%20Haotian%20Tang%20and%20Hanyu%20Liu%20and%20Chao%20Li&entry.1292438233=%20%20The%20primary%20objective%20of%20human%20activity%20recognition%20%28HAR%29%20is%20to%20infer%20ongoing%0Ahuman%20actions%20from%20sensor%20data%2C%20a%20task%20that%20finds%20broad%20applications%20in%20health%0Amonitoring%2C%20safety%20protection%2C%20and%20sports%20analysis.%20Despite%20proliferating%0Aresearch%2C%20HAR%20still%20faces%20key%20challenges%2C%20including%20the%20scarcity%20of%20labeled%0Asamples%20for%20rare%20activities%2C%20insufficient%20extraction%20of%20high-level%20features%2C%0Aand%20suboptimal%20model%20performance%20on%20lightweight%20devices.%20To%20address%20these%0Aissues%2C%20this%20paper%20proposes%20a%20comprehensive%20optimization%20approach%20centered%20on%0Amulti-attention%20interaction%20mechanisms.%20First%2C%20an%20unsupervised%2C%0Astatistics-guided%20diffusion%20model%20is%20employed%20to%20perform%20data%20augmentation%2C%0Athereby%20alleviating%20the%20problems%20of%20labeled%20data%20scarcity%20and%20severe%20class%0Aimbalance.%20Second%2C%20a%20multi-branch%20spatio-temporal%20interaction%20network%20is%0Adesigned%2C%20which%20captures%20multi-scale%20features%20of%20sequential%20data%20through%0Aparallel%20residual%20branches%20with%203%2A3%2C%205%2A5%2C%20and%207%2A7%20convolutional%20kernels.%0ASimultaneously%2C%20temporal%20attention%20mechanisms%20are%20incorporated%20to%20identify%0Acritical%20time%20points%2C%20while%20spatial%20attention%20enhances%20inter-sensor%0Ainteractions.%20A%20cross-branch%20feature%20fusion%20unit%20is%20further%20introduced%20to%0Aimprove%20the%20overall%20feature%20representation%20capability.%20Finally%2C%20an%20adaptive%0Amulti-loss%20function%20fusion%20strategy%20is%20integrated%2C%20allowing%20for%20dynamic%0Aadjustment%20of%20loss%20weights%20and%20overall%20model%20optimization.%20Experimental%20results%0Aon%20three%20public%20datasets%2C%20WISDM%2C%20PAMAP2%2C%20and%20OPPORTUNITY%2C%20demonstrate%20that%20the%0Aproposed%20unsupervised%20data%20augmentation%20spatio-temporal%20attention%20diffusion%0Anetwork%20%28USAD%29%20achieves%20accuracies%20of%2098.84%25%2C%2093.81%25%2C%20and%2080.92%25%20respectively%2C%0Asignificantly%20outperforming%20existing%20approaches.%20Furthermore%2C%20practical%0Adeployment%20on%20embedded%20devices%20verifies%20the%20efficiency%20and%20feasibility%20of%20the%0Aproposed%20method.%0A&entry.1838667208=http%3A//arxiv.org/abs/2507.02827v2&entry.124074799=Read"},
{"title": "On Barriers to Archival Audio Processing", "author": "Peter Sullivan and Muhammad Abdul-Mageed", "abstract": "  In this study, we leverage a unique UNESCO collection of mid-20th century\nradio recordings to probe the robustness of modern off-the-shelf language\nidentification (LID) and speaker recognition (SR) methods, especially with\nrespect to the impact of multilingual speakers and cross-age recordings. Our\nfindings suggest that LID systems, such as Whisper, are increasingly adept at\nhandling second-language and accented speech. However, speaker embeddings\nremain a fragile component of speech processing pipelines that is prone to\nbiases related to the channel, age, and language. Issues which will need to be\novercome should archives aim to employ SR methods for speaker indexing.\n", "link": "http://arxiv.org/abs/2507.08768v1", "date": "2025-07-11", "relevancy": 2.245, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.4751}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.4751}, {"title": "Breadcrumbs: Adversarial class-balanced sampling for long-tailed recognition", "link": "https://link.springer.com/chapter/10.1007/978-3-031-20053-3_37", "similarity": 0.3968}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20On%20Barriers%20to%20Archival%20Audio%20Processing&body=Title%3A%20On%20Barriers%20to%20Archival%20Audio%20Processing%0AAuthor%3A%20Peter%20Sullivan%20and%20Muhammad%20Abdul-Mageed%0AAbstract%3A%20%20%20In%20this%20study%2C%20we%20leverage%20a%20unique%20UNESCO%20collection%20of%20mid-20th%20century%0Aradio%20recordings%20to%20probe%20the%20robustness%20of%20modern%20off-the-shelf%20language%0Aidentification%20%28LID%29%20and%20speaker%20recognition%20%28SR%29%20methods%2C%20especially%20with%0Arespect%20to%20the%20impact%20of%20multilingual%20speakers%20and%20cross-age%20recordings.%20Our%0Afindings%20suggest%20that%20LID%20systems%2C%20such%20as%20Whisper%2C%20are%20increasingly%20adept%20at%0Ahandling%20second-language%20and%20accented%20speech.%20However%2C%20speaker%20embeddings%0Aremain%20a%20fragile%20component%20of%20speech%20processing%20pipelines%20that%20is%20prone%20to%0Abiases%20related%20to%20the%20channel%2C%20age%2C%20and%20language.%20Issues%20which%20will%20need%20to%20be%0Aovercome%20should%20archives%20aim%20to%20employ%20SR%20methods%20for%20speaker%20indexing.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2507.08768v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DOn%2520Barriers%2520to%2520Archival%2520Audio%2520Processing%26entry.906535625%3DPeter%2520Sullivan%2520and%2520Muhammad%2520Abdul-Mageed%26entry.1292438233%3D%2520%2520In%2520this%2520study%252C%2520we%2520leverage%2520a%2520unique%2520UNESCO%2520collection%2520of%2520mid-20th%2520century%250Aradio%2520recordings%2520to%2520probe%2520the%2520robustness%2520of%2520modern%2520off-the-shelf%2520language%250Aidentification%2520%2528LID%2529%2520and%2520speaker%2520recognition%2520%2528SR%2529%2520methods%252C%2520especially%2520with%250Arespect%2520to%2520the%2520impact%2520of%2520multilingual%2520speakers%2520and%2520cross-age%2520recordings.%2520Our%250Afindings%2520suggest%2520that%2520LID%2520systems%252C%2520such%2520as%2520Whisper%252C%2520are%2520increasingly%2520adept%2520at%250Ahandling%2520second-language%2520and%2520accented%2520speech.%2520However%252C%2520speaker%2520embeddings%250Aremain%2520a%2520fragile%2520component%2520of%2520speech%2520processing%2520pipelines%2520that%2520is%2520prone%2520to%250Abiases%2520related%2520to%2520the%2520channel%252C%2520age%252C%2520and%2520language.%2520Issues%2520which%2520will%2520need%2520to%2520be%250Aovercome%2520should%2520archives%2520aim%2520to%2520employ%2520SR%2520methods%2520for%2520speaker%2520indexing.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2507.08768v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=On%20Barriers%20to%20Archival%20Audio%20Processing&entry.906535625=Peter%20Sullivan%20and%20Muhammad%20Abdul-Mageed&entry.1292438233=%20%20In%20this%20study%2C%20we%20leverage%20a%20unique%20UNESCO%20collection%20of%20mid-20th%20century%0Aradio%20recordings%20to%20probe%20the%20robustness%20of%20modern%20off-the-shelf%20language%0Aidentification%20%28LID%29%20and%20speaker%20recognition%20%28SR%29%20methods%2C%20especially%20with%0Arespect%20to%20the%20impact%20of%20multilingual%20speakers%20and%20cross-age%20recordings.%20Our%0Afindings%20suggest%20that%20LID%20systems%2C%20such%20as%20Whisper%2C%20are%20increasingly%20adept%20at%0Ahandling%20second-language%20and%20accented%20speech.%20However%2C%20speaker%20embeddings%0Aremain%20a%20fragile%20component%20of%20speech%20processing%20pipelines%20that%20is%20prone%20to%0Abiases%20related%20to%20the%20channel%2C%20age%2C%20and%20language.%20Issues%20which%20will%20need%20to%20be%0Aovercome%20should%20archives%20aim%20to%20employ%20SR%20methods%20for%20speaker%20indexing.%0A&entry.1838667208=http%3A//arxiv.org/abs/2507.08768v1&entry.124074799=Read"},
{"title": "Multi-critic Learning for Whole-body End-effector Twist Tracking", "author": "Aravind Elanjimattathil Vijayan and Andrei Cramariuc and Mattia Risiglione and Christian Gehring and Marco Hutter", "abstract": "  Learning whole-body control for locomotion and arm motions in a single policy\nhas challenges, as the two tasks have conflicting goals. For instance,\nefficient locomotion typically favors a horizontal base orientation, while\nend-effector tracking may benefit from base tilting to extend reachability.\nAdditionally, current Reinforcement Learning (RL) approaches using a pose-based\ntask specification lack the ability to directly control the end-effector\nvelocity, making smoothly executing trajectories very challenging. To address\nthese limitations, we propose an RL-based framework that allows for dynamic,\nvelocity-aware whole-body end-effector control. Our method introduces a\nmulti-critic actor architecture that decouples the reward signals for\nlocomotion and manipulation, simplifying reward tuning and allowing the policy\nto resolve task conflicts more effectively. Furthermore, we design a\ntwist-based end-effector task formulation that can track both discrete poses\nand motion trajectories. We validate our approach through a set of simulation\nand hardware experiments using a quadruped robot equipped with a robotic arm.\nThe resulting controller can simultaneously walk and move its end-effector and\nshows emergent whole-body behaviors, where the base assists the arm in\nextending the workspace, despite a lack of explicit formulations.\n", "link": "http://arxiv.org/abs/2507.08656v1", "date": "2025-07-11", "relevancy": 2.2444, "topK": [{"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.5929}, {"title": "Egocentric Computer Vision for Hands-Free Robotic Wheelchair Navigation", "link": "https://link.springer.com/article/10.1007/s10846-023-01807-4", "similarity": 0.5636}, {"title": "Synthesizing Moving People with 3D Control", "link": "http://arxiv.org/abs/2401.10889v2", "similarity": 0.5459}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Multi-critic%20Learning%20for%20Whole-body%20End-effector%20Twist%20Tracking&body=Title%3A%20Multi-critic%20Learning%20for%20Whole-body%20End-effector%20Twist%20Tracking%0AAuthor%3A%20Aravind%20Elanjimattathil%20Vijayan%20and%20Andrei%20Cramariuc%20and%20Mattia%20Risiglione%20and%20Christian%20Gehring%20and%20Marco%20Hutter%0AAbstract%3A%20%20%20Learning%20whole-body%20control%20for%20locomotion%20and%20arm%20motions%20in%20a%20single%20policy%0Ahas%20challenges%2C%20as%20the%20two%20tasks%20have%20conflicting%20goals.%20For%20instance%2C%0Aefficient%20locomotion%20typically%20favors%20a%20horizontal%20base%20orientation%2C%20while%0Aend-effector%20tracking%20may%20benefit%20from%20base%20tilting%20to%20extend%20reachability.%0AAdditionally%2C%20current%20Reinforcement%20Learning%20%28RL%29%20approaches%20using%20a%20pose-based%0Atask%20specification%20lack%20the%20ability%20to%20directly%20control%20the%20end-effector%0Avelocity%2C%20making%20smoothly%20executing%20trajectories%20very%20challenging.%20To%20address%0Athese%20limitations%2C%20we%20propose%20an%20RL-based%20framework%20that%20allows%20for%20dynamic%2C%0Avelocity-aware%20whole-body%20end-effector%20control.%20Our%20method%20introduces%20a%0Amulti-critic%20actor%20architecture%20that%20decouples%20the%20reward%20signals%20for%0Alocomotion%20and%20manipulation%2C%20simplifying%20reward%20tuning%20and%20allowing%20the%20policy%0Ato%20resolve%20task%20conflicts%20more%20effectively.%20Furthermore%2C%20we%20design%20a%0Atwist-based%20end-effector%20task%20formulation%20that%20can%20track%20both%20discrete%20poses%0Aand%20motion%20trajectories.%20We%20validate%20our%20approach%20through%20a%20set%20of%20simulation%0Aand%20hardware%20experiments%20using%20a%20quadruped%20robot%20equipped%20with%20a%20robotic%20arm.%0AThe%20resulting%20controller%20can%20simultaneously%20walk%20and%20move%20its%20end-effector%20and%0Ashows%20emergent%20whole-body%20behaviors%2C%20where%20the%20base%20assists%20the%20arm%20in%0Aextending%20the%20workspace%2C%20despite%20a%20lack%20of%20explicit%20formulations.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2507.08656v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DMulti-critic%2520Learning%2520for%2520Whole-body%2520End-effector%2520Twist%2520Tracking%26entry.906535625%3DAravind%2520Elanjimattathil%2520Vijayan%2520and%2520Andrei%2520Cramariuc%2520and%2520Mattia%2520Risiglione%2520and%2520Christian%2520Gehring%2520and%2520Marco%2520Hutter%26entry.1292438233%3D%2520%2520Learning%2520whole-body%2520control%2520for%2520locomotion%2520and%2520arm%2520motions%2520in%2520a%2520single%2520policy%250Ahas%2520challenges%252C%2520as%2520the%2520two%2520tasks%2520have%2520conflicting%2520goals.%2520For%2520instance%252C%250Aefficient%2520locomotion%2520typically%2520favors%2520a%2520horizontal%2520base%2520orientation%252C%2520while%250Aend-effector%2520tracking%2520may%2520benefit%2520from%2520base%2520tilting%2520to%2520extend%2520reachability.%250AAdditionally%252C%2520current%2520Reinforcement%2520Learning%2520%2528RL%2529%2520approaches%2520using%2520a%2520pose-based%250Atask%2520specification%2520lack%2520the%2520ability%2520to%2520directly%2520control%2520the%2520end-effector%250Avelocity%252C%2520making%2520smoothly%2520executing%2520trajectories%2520very%2520challenging.%2520To%2520address%250Athese%2520limitations%252C%2520we%2520propose%2520an%2520RL-based%2520framework%2520that%2520allows%2520for%2520dynamic%252C%250Avelocity-aware%2520whole-body%2520end-effector%2520control.%2520Our%2520method%2520introduces%2520a%250Amulti-critic%2520actor%2520architecture%2520that%2520decouples%2520the%2520reward%2520signals%2520for%250Alocomotion%2520and%2520manipulation%252C%2520simplifying%2520reward%2520tuning%2520and%2520allowing%2520the%2520policy%250Ato%2520resolve%2520task%2520conflicts%2520more%2520effectively.%2520Furthermore%252C%2520we%2520design%2520a%250Atwist-based%2520end-effector%2520task%2520formulation%2520that%2520can%2520track%2520both%2520discrete%2520poses%250Aand%2520motion%2520trajectories.%2520We%2520validate%2520our%2520approach%2520through%2520a%2520set%2520of%2520simulation%250Aand%2520hardware%2520experiments%2520using%2520a%2520quadruped%2520robot%2520equipped%2520with%2520a%2520robotic%2520arm.%250AThe%2520resulting%2520controller%2520can%2520simultaneously%2520walk%2520and%2520move%2520its%2520end-effector%2520and%250Ashows%2520emergent%2520whole-body%2520behaviors%252C%2520where%2520the%2520base%2520assists%2520the%2520arm%2520in%250Aextending%2520the%2520workspace%252C%2520despite%2520a%2520lack%2520of%2520explicit%2520formulations.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2507.08656v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Multi-critic%20Learning%20for%20Whole-body%20End-effector%20Twist%20Tracking&entry.906535625=Aravind%20Elanjimattathil%20Vijayan%20and%20Andrei%20Cramariuc%20and%20Mattia%20Risiglione%20and%20Christian%20Gehring%20and%20Marco%20Hutter&entry.1292438233=%20%20Learning%20whole-body%20control%20for%20locomotion%20and%20arm%20motions%20in%20a%20single%20policy%0Ahas%20challenges%2C%20as%20the%20two%20tasks%20have%20conflicting%20goals.%20For%20instance%2C%0Aefficient%20locomotion%20typically%20favors%20a%20horizontal%20base%20orientation%2C%20while%0Aend-effector%20tracking%20may%20benefit%20from%20base%20tilting%20to%20extend%20reachability.%0AAdditionally%2C%20current%20Reinforcement%20Learning%20%28RL%29%20approaches%20using%20a%20pose-based%0Atask%20specification%20lack%20the%20ability%20to%20directly%20control%20the%20end-effector%0Avelocity%2C%20making%20smoothly%20executing%20trajectories%20very%20challenging.%20To%20address%0Athese%20limitations%2C%20we%20propose%20an%20RL-based%20framework%20that%20allows%20for%20dynamic%2C%0Avelocity-aware%20whole-body%20end-effector%20control.%20Our%20method%20introduces%20a%0Amulti-critic%20actor%20architecture%20that%20decouples%20the%20reward%20signals%20for%0Alocomotion%20and%20manipulation%2C%20simplifying%20reward%20tuning%20and%20allowing%20the%20policy%0Ato%20resolve%20task%20conflicts%20more%20effectively.%20Furthermore%2C%20we%20design%20a%0Atwist-based%20end-effector%20task%20formulation%20that%20can%20track%20both%20discrete%20poses%0Aand%20motion%20trajectories.%20We%20validate%20our%20approach%20through%20a%20set%20of%20simulation%0Aand%20hardware%20experiments%20using%20a%20quadruped%20robot%20equipped%20with%20a%20robotic%20arm.%0AThe%20resulting%20controller%20can%20simultaneously%20walk%20and%20move%20its%20end-effector%20and%0Ashows%20emergent%20whole-body%20behaviors%2C%20where%20the%20base%20assists%20the%20arm%20in%0Aextending%20the%20workspace%2C%20despite%20a%20lack%20of%20explicit%20formulations.%0A&entry.1838667208=http%3A//arxiv.org/abs/2507.08656v1&entry.124074799=Read"},
{"title": "MoSAiC: Multi-Modal Multi-Label Supervision-Aware Contrastive Learning\n  for Remote Sensing", "author": "Debashis Gupta and Aditi Golder and Rongkhun Zhu and Kangning Cui and Wei Tang and Fan Yang and Ovidiu Csillik and Sarra Alaqahtani and V. Paul Pauca", "abstract": "  Contrastive learning (CL) has emerged as a powerful paradigm for learning\ntransferable representations without the reliance on large labeled datasets.\nIts ability to capture intrinsic similarities and differences among data\nsamples has led to state-of-the-art results in computer vision tasks. These\nstrengths make CL particularly well-suited for Earth System Observation (ESO),\nwhere diverse satellite modalities such as optical and SAR imagery offer\nnaturally aligned views of the same geospatial regions. However, ESO presents\nunique challenges, including high inter-class similarity, scene clutter, and\nambiguous boundaries, which complicate representation learning -- especially in\nlow-label, multi-label settings. Existing CL frameworks often focus on\nintra-modality self-supervision or lack mechanisms for multi-label alignment\nand semantic precision across modalities. In this work, we introduce MoSAiC, a\nunified framework that jointly optimizes intra- and inter-modality contrastive\nlearning with a multi-label supervised contrastive loss. Designed specifically\nfor multi-modal satellite imagery, MoSAiC enables finer semantic\ndisentanglement and more robust representation learning across spectrally\nsimilar and spatially complex classes. Experiments on two benchmark datasets,\nBigEarthNet V2.0 and Sent12MS, show that MoSAiC consistently outperforms both\nfully supervised and self-supervised baselines in terms of accuracy, cluster\ncoherence, and generalization in low-label and high-class-overlap scenarios.\n", "link": "http://arxiv.org/abs/2507.08683v1", "date": "2025-07-11", "relevancy": 2.2332, "topK": [{"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5788}, {"title": "Self-supervised Photographic Image Layout Representation Learning", "link": "http://arxiv.org/abs/2403.03740v1", "similarity": 0.5454}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.5393}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20MoSAiC%3A%20Multi-Modal%20Multi-Label%20Supervision-Aware%20Contrastive%20Learning%0A%20%20for%20Remote%20Sensing&body=Title%3A%20MoSAiC%3A%20Multi-Modal%20Multi-Label%20Supervision-Aware%20Contrastive%20Learning%0A%20%20for%20Remote%20Sensing%0AAuthor%3A%20Debashis%20Gupta%20and%20Aditi%20Golder%20and%20Rongkhun%20Zhu%20and%20Kangning%20Cui%20and%20Wei%20Tang%20and%20Fan%20Yang%20and%20Ovidiu%20Csillik%20and%20Sarra%20Alaqahtani%20and%20V.%20Paul%20Pauca%0AAbstract%3A%20%20%20Contrastive%20learning%20%28CL%29%20has%20emerged%20as%20a%20powerful%20paradigm%20for%20learning%0Atransferable%20representations%20without%20the%20reliance%20on%20large%20labeled%20datasets.%0AIts%20ability%20to%20capture%20intrinsic%20similarities%20and%20differences%20among%20data%0Asamples%20has%20led%20to%20state-of-the-art%20results%20in%20computer%20vision%20tasks.%20These%0Astrengths%20make%20CL%20particularly%20well-suited%20for%20Earth%20System%20Observation%20%28ESO%29%2C%0Awhere%20diverse%20satellite%20modalities%20such%20as%20optical%20and%20SAR%20imagery%20offer%0Anaturally%20aligned%20views%20of%20the%20same%20geospatial%20regions.%20However%2C%20ESO%20presents%0Aunique%20challenges%2C%20including%20high%20inter-class%20similarity%2C%20scene%20clutter%2C%20and%0Aambiguous%20boundaries%2C%20which%20complicate%20representation%20learning%20--%20especially%20in%0Alow-label%2C%20multi-label%20settings.%20Existing%20CL%20frameworks%20often%20focus%20on%0Aintra-modality%20self-supervision%20or%20lack%20mechanisms%20for%20multi-label%20alignment%0Aand%20semantic%20precision%20across%20modalities.%20In%20this%20work%2C%20we%20introduce%20MoSAiC%2C%20a%0Aunified%20framework%20that%20jointly%20optimizes%20intra-%20and%20inter-modality%20contrastive%0Alearning%20with%20a%20multi-label%20supervised%20contrastive%20loss.%20Designed%20specifically%0Afor%20multi-modal%20satellite%20imagery%2C%20MoSAiC%20enables%20finer%20semantic%0Adisentanglement%20and%20more%20robust%20representation%20learning%20across%20spectrally%0Asimilar%20and%20spatially%20complex%20classes.%20Experiments%20on%20two%20benchmark%20datasets%2C%0ABigEarthNet%20V2.0%20and%20Sent12MS%2C%20show%20that%20MoSAiC%20consistently%20outperforms%20both%0Afully%20supervised%20and%20self-supervised%20baselines%20in%20terms%20of%20accuracy%2C%20cluster%0Acoherence%2C%20and%20generalization%20in%20low-label%20and%20high-class-overlap%20scenarios.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2507.08683v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DMoSAiC%253A%2520Multi-Modal%2520Multi-Label%2520Supervision-Aware%2520Contrastive%2520Learning%250A%2520%2520for%2520Remote%2520Sensing%26entry.906535625%3DDebashis%2520Gupta%2520and%2520Aditi%2520Golder%2520and%2520Rongkhun%2520Zhu%2520and%2520Kangning%2520Cui%2520and%2520Wei%2520Tang%2520and%2520Fan%2520Yang%2520and%2520Ovidiu%2520Csillik%2520and%2520Sarra%2520Alaqahtani%2520and%2520V.%2520Paul%2520Pauca%26entry.1292438233%3D%2520%2520Contrastive%2520learning%2520%2528CL%2529%2520has%2520emerged%2520as%2520a%2520powerful%2520paradigm%2520for%2520learning%250Atransferable%2520representations%2520without%2520the%2520reliance%2520on%2520large%2520labeled%2520datasets.%250AIts%2520ability%2520to%2520capture%2520intrinsic%2520similarities%2520and%2520differences%2520among%2520data%250Asamples%2520has%2520led%2520to%2520state-of-the-art%2520results%2520in%2520computer%2520vision%2520tasks.%2520These%250Astrengths%2520make%2520CL%2520particularly%2520well-suited%2520for%2520Earth%2520System%2520Observation%2520%2528ESO%2529%252C%250Awhere%2520diverse%2520satellite%2520modalities%2520such%2520as%2520optical%2520and%2520SAR%2520imagery%2520offer%250Anaturally%2520aligned%2520views%2520of%2520the%2520same%2520geospatial%2520regions.%2520However%252C%2520ESO%2520presents%250Aunique%2520challenges%252C%2520including%2520high%2520inter-class%2520similarity%252C%2520scene%2520clutter%252C%2520and%250Aambiguous%2520boundaries%252C%2520which%2520complicate%2520representation%2520learning%2520--%2520especially%2520in%250Alow-label%252C%2520multi-label%2520settings.%2520Existing%2520CL%2520frameworks%2520often%2520focus%2520on%250Aintra-modality%2520self-supervision%2520or%2520lack%2520mechanisms%2520for%2520multi-label%2520alignment%250Aand%2520semantic%2520precision%2520across%2520modalities.%2520In%2520this%2520work%252C%2520we%2520introduce%2520MoSAiC%252C%2520a%250Aunified%2520framework%2520that%2520jointly%2520optimizes%2520intra-%2520and%2520inter-modality%2520contrastive%250Alearning%2520with%2520a%2520multi-label%2520supervised%2520contrastive%2520loss.%2520Designed%2520specifically%250Afor%2520multi-modal%2520satellite%2520imagery%252C%2520MoSAiC%2520enables%2520finer%2520semantic%250Adisentanglement%2520and%2520more%2520robust%2520representation%2520learning%2520across%2520spectrally%250Asimilar%2520and%2520spatially%2520complex%2520classes.%2520Experiments%2520on%2520two%2520benchmark%2520datasets%252C%250ABigEarthNet%2520V2.0%2520and%2520Sent12MS%252C%2520show%2520that%2520MoSAiC%2520consistently%2520outperforms%2520both%250Afully%2520supervised%2520and%2520self-supervised%2520baselines%2520in%2520terms%2520of%2520accuracy%252C%2520cluster%250Acoherence%252C%2520and%2520generalization%2520in%2520low-label%2520and%2520high-class-overlap%2520scenarios.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2507.08683v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=MoSAiC%3A%20Multi-Modal%20Multi-Label%20Supervision-Aware%20Contrastive%20Learning%0A%20%20for%20Remote%20Sensing&entry.906535625=Debashis%20Gupta%20and%20Aditi%20Golder%20and%20Rongkhun%20Zhu%20and%20Kangning%20Cui%20and%20Wei%20Tang%20and%20Fan%20Yang%20and%20Ovidiu%20Csillik%20and%20Sarra%20Alaqahtani%20and%20V.%20Paul%20Pauca&entry.1292438233=%20%20Contrastive%20learning%20%28CL%29%20has%20emerged%20as%20a%20powerful%20paradigm%20for%20learning%0Atransferable%20representations%20without%20the%20reliance%20on%20large%20labeled%20datasets.%0AIts%20ability%20to%20capture%20intrinsic%20similarities%20and%20differences%20among%20data%0Asamples%20has%20led%20to%20state-of-the-art%20results%20in%20computer%20vision%20tasks.%20These%0Astrengths%20make%20CL%20particularly%20well-suited%20for%20Earth%20System%20Observation%20%28ESO%29%2C%0Awhere%20diverse%20satellite%20modalities%20such%20as%20optical%20and%20SAR%20imagery%20offer%0Anaturally%20aligned%20views%20of%20the%20same%20geospatial%20regions.%20However%2C%20ESO%20presents%0Aunique%20challenges%2C%20including%20high%20inter-class%20similarity%2C%20scene%20clutter%2C%20and%0Aambiguous%20boundaries%2C%20which%20complicate%20representation%20learning%20--%20especially%20in%0Alow-label%2C%20multi-label%20settings.%20Existing%20CL%20frameworks%20often%20focus%20on%0Aintra-modality%20self-supervision%20or%20lack%20mechanisms%20for%20multi-label%20alignment%0Aand%20semantic%20precision%20across%20modalities.%20In%20this%20work%2C%20we%20introduce%20MoSAiC%2C%20a%0Aunified%20framework%20that%20jointly%20optimizes%20intra-%20and%20inter-modality%20contrastive%0Alearning%20with%20a%20multi-label%20supervised%20contrastive%20loss.%20Designed%20specifically%0Afor%20multi-modal%20satellite%20imagery%2C%20MoSAiC%20enables%20finer%20semantic%0Adisentanglement%20and%20more%20robust%20representation%20learning%20across%20spectrally%0Asimilar%20and%20spatially%20complex%20classes.%20Experiments%20on%20two%20benchmark%20datasets%2C%0ABigEarthNet%20V2.0%20and%20Sent12MS%2C%20show%20that%20MoSAiC%20consistently%20outperforms%20both%0Afully%20supervised%20and%20self-supervised%20baselines%20in%20terms%20of%20accuracy%2C%20cluster%0Acoherence%2C%20and%20generalization%20in%20low-label%20and%20high-class-overlap%20scenarios.%0A&entry.1838667208=http%3A//arxiv.org/abs/2507.08683v1&entry.124074799=Read"},
{"title": "Unreal is all you need: Multimodal ISAC Data Simulation with Only One\n  Engine", "author": "Kongwu Huang and Shiyi Mu and Jun Jiang and Yuan Gao and Shugong Xu", "abstract": "  Scaling laws have achieved success in LLM and foundation models. To explore\ntheir potential in ISAC research, we propose Great-X. This single-engine\nmultimodal data twin platform reconstructs the ray-tracing computation of\nSionna within Unreal Engine and is deeply integrated with autonomous driving\ntools. This enables efficient and synchronized simulation of multimodal data,\nincluding CSI, RGB, Radar, and LiDAR. Based on this platform, we construct an\nopen-source, large-scale, low-altitude UAV multimodal synaesthesia dataset\nnamed Great-MSD, and propose a baseline CSI-based UAV 3D localization\nalgorithm, demonstrating its feasibility and generalizability across different\nCSI simulation engines. The related code and dataset are publicly available at:\nhttps://github.com/hkw-xg/Great-MCD.\n", "link": "http://arxiv.org/abs/2507.08716v1", "date": "2025-07-11", "relevancy": 2.232, "topK": [{"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.5682}, {"title": "RTG-SLAM: Real-time 3D Reconstruction at Scale using Gaussian Splatting", "link": "http://arxiv.org/abs/2404.19706v1", "similarity": 0.5612}, {"title": "CAT3D: Create Anything in 3D with Multi-View Diffusion Models", "link": "http://arxiv.org/abs/2405.10314v1", "similarity": 0.5507}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Unreal%20is%20all%20you%20need%3A%20Multimodal%20ISAC%20Data%20Simulation%20with%20Only%20One%0A%20%20Engine&body=Title%3A%20Unreal%20is%20all%20you%20need%3A%20Multimodal%20ISAC%20Data%20Simulation%20with%20Only%20One%0A%20%20Engine%0AAuthor%3A%20Kongwu%20Huang%20and%20Shiyi%20Mu%20and%20Jun%20Jiang%20and%20Yuan%20Gao%20and%20Shugong%20Xu%0AAbstract%3A%20%20%20Scaling%20laws%20have%20achieved%20success%20in%20LLM%20and%20foundation%20models.%20To%20explore%0Atheir%20potential%20in%20ISAC%20research%2C%20we%20propose%20Great-X.%20This%20single-engine%0Amultimodal%20data%20twin%20platform%20reconstructs%20the%20ray-tracing%20computation%20of%0ASionna%20within%20Unreal%20Engine%20and%20is%20deeply%20integrated%20with%20autonomous%20driving%0Atools.%20This%20enables%20efficient%20and%20synchronized%20simulation%20of%20multimodal%20data%2C%0Aincluding%20CSI%2C%20RGB%2C%20Radar%2C%20and%20LiDAR.%20Based%20on%20this%20platform%2C%20we%20construct%20an%0Aopen-source%2C%20large-scale%2C%20low-altitude%20UAV%20multimodal%20synaesthesia%20dataset%0Anamed%20Great-MSD%2C%20and%20propose%20a%20baseline%20CSI-based%20UAV%203D%20localization%0Aalgorithm%2C%20demonstrating%20its%20feasibility%20and%20generalizability%20across%20different%0ACSI%20simulation%20engines.%20The%20related%20code%20and%20dataset%20are%20publicly%20available%20at%3A%0Ahttps%3A//github.com/hkw-xg/Great-MCD.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2507.08716v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DUnreal%2520is%2520all%2520you%2520need%253A%2520Multimodal%2520ISAC%2520Data%2520Simulation%2520with%2520Only%2520One%250A%2520%2520Engine%26entry.906535625%3DKongwu%2520Huang%2520and%2520Shiyi%2520Mu%2520and%2520Jun%2520Jiang%2520and%2520Yuan%2520Gao%2520and%2520Shugong%2520Xu%26entry.1292438233%3D%2520%2520Scaling%2520laws%2520have%2520achieved%2520success%2520in%2520LLM%2520and%2520foundation%2520models.%2520To%2520explore%250Atheir%2520potential%2520in%2520ISAC%2520research%252C%2520we%2520propose%2520Great-X.%2520This%2520single-engine%250Amultimodal%2520data%2520twin%2520platform%2520reconstructs%2520the%2520ray-tracing%2520computation%2520of%250ASionna%2520within%2520Unreal%2520Engine%2520and%2520is%2520deeply%2520integrated%2520with%2520autonomous%2520driving%250Atools.%2520This%2520enables%2520efficient%2520and%2520synchronized%2520simulation%2520of%2520multimodal%2520data%252C%250Aincluding%2520CSI%252C%2520RGB%252C%2520Radar%252C%2520and%2520LiDAR.%2520Based%2520on%2520this%2520platform%252C%2520we%2520construct%2520an%250Aopen-source%252C%2520large-scale%252C%2520low-altitude%2520UAV%2520multimodal%2520synaesthesia%2520dataset%250Anamed%2520Great-MSD%252C%2520and%2520propose%2520a%2520baseline%2520CSI-based%2520UAV%25203D%2520localization%250Aalgorithm%252C%2520demonstrating%2520its%2520feasibility%2520and%2520generalizability%2520across%2520different%250ACSI%2520simulation%2520engines.%2520The%2520related%2520code%2520and%2520dataset%2520are%2520publicly%2520available%2520at%253A%250Ahttps%253A//github.com/hkw-xg/Great-MCD.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2507.08716v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Unreal%20is%20all%20you%20need%3A%20Multimodal%20ISAC%20Data%20Simulation%20with%20Only%20One%0A%20%20Engine&entry.906535625=Kongwu%20Huang%20and%20Shiyi%20Mu%20and%20Jun%20Jiang%20and%20Yuan%20Gao%20and%20Shugong%20Xu&entry.1292438233=%20%20Scaling%20laws%20have%20achieved%20success%20in%20LLM%20and%20foundation%20models.%20To%20explore%0Atheir%20potential%20in%20ISAC%20research%2C%20we%20propose%20Great-X.%20This%20single-engine%0Amultimodal%20data%20twin%20platform%20reconstructs%20the%20ray-tracing%20computation%20of%0ASionna%20within%20Unreal%20Engine%20and%20is%20deeply%20integrated%20with%20autonomous%20driving%0Atools.%20This%20enables%20efficient%20and%20synchronized%20simulation%20of%20multimodal%20data%2C%0Aincluding%20CSI%2C%20RGB%2C%20Radar%2C%20and%20LiDAR.%20Based%20on%20this%20platform%2C%20we%20construct%20an%0Aopen-source%2C%20large-scale%2C%20low-altitude%20UAV%20multimodal%20synaesthesia%20dataset%0Anamed%20Great-MSD%2C%20and%20propose%20a%20baseline%20CSI-based%20UAV%203D%20localization%0Aalgorithm%2C%20demonstrating%20its%20feasibility%20and%20generalizability%20across%20different%0ACSI%20simulation%20engines.%20The%20related%20code%20and%20dataset%20are%20publicly%20available%20at%3A%0Ahttps%3A//github.com/hkw-xg/Great-MCD.%0A&entry.1838667208=http%3A//arxiv.org/abs/2507.08716v1&entry.124074799=Read"},
{"title": "KV Cache Steering for Inducing Reasoning in Small Language Models", "author": "Max Belitsky and Dawid J. Kopiczko and Michael Dorkenwald and M. Jehanzeb Mirza and Cees G. M. Snoek and Yuki M. Asano", "abstract": "  We propose cache steering, a lightweight method for implicit steering of\nlanguage models via a one-shot intervention applied directly to the key-value\ncache. To validate its effectiveness, we apply cache steering to induce\nchain-of-thought reasoning in small language models. Our approach leverages\nGPT-4o-generated reasoning traces to construct steering vectors that shift\nmodel behavior toward more explicit, multi-step reasoning without fine-tuning\nor prompt modifications. Experimental evaluations on diverse reasoning\nbenchmarks demonstrate that cache steering improves both the qualitative\nstructure of model reasoning and quantitative task performance. Compared to\nprior activation steering techniques that require continuous interventions, our\none-shot cache steering offers substantial advantages in terms of\nhyperparameter stability, inference-time efficiency, and ease of integration,\nmaking it a more robust and practical solution for controlled generation.\n", "link": "http://arxiv.org/abs/2507.08799v1", "date": "2025-07-11", "relevancy": 2.2064, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.4476}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.4476}, {"title": "IAE: Implicit Autoencoder for Point Cloud Self-supervised Representation Learning", "link": "https://arxiv.org/abs/2201.00785", "similarity": 0.4287}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20KV%20Cache%20Steering%20for%20Inducing%20Reasoning%20in%20Small%20Language%20Models&body=Title%3A%20KV%20Cache%20Steering%20for%20Inducing%20Reasoning%20in%20Small%20Language%20Models%0AAuthor%3A%20Max%20Belitsky%20and%20Dawid%20J.%20Kopiczko%20and%20Michael%20Dorkenwald%20and%20M.%20Jehanzeb%20Mirza%20and%20Cees%20G.%20M.%20Snoek%20and%20Yuki%20M.%20Asano%0AAbstract%3A%20%20%20We%20propose%20cache%20steering%2C%20a%20lightweight%20method%20for%20implicit%20steering%20of%0Alanguage%20models%20via%20a%20one-shot%20intervention%20applied%20directly%20to%20the%20key-value%0Acache.%20To%20validate%20its%20effectiveness%2C%20we%20apply%20cache%20steering%20to%20induce%0Achain-of-thought%20reasoning%20in%20small%20language%20models.%20Our%20approach%20leverages%0AGPT-4o-generated%20reasoning%20traces%20to%20construct%20steering%20vectors%20that%20shift%0Amodel%20behavior%20toward%20more%20explicit%2C%20multi-step%20reasoning%20without%20fine-tuning%0Aor%20prompt%20modifications.%20Experimental%20evaluations%20on%20diverse%20reasoning%0Abenchmarks%20demonstrate%20that%20cache%20steering%20improves%20both%20the%20qualitative%0Astructure%20of%20model%20reasoning%20and%20quantitative%20task%20performance.%20Compared%20to%0Aprior%20activation%20steering%20techniques%20that%20require%20continuous%20interventions%2C%20our%0Aone-shot%20cache%20steering%20offers%20substantial%20advantages%20in%20terms%20of%0Ahyperparameter%20stability%2C%20inference-time%20efficiency%2C%20and%20ease%20of%20integration%2C%0Amaking%20it%20a%20more%20robust%20and%20practical%20solution%20for%20controlled%20generation.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2507.08799v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DKV%2520Cache%2520Steering%2520for%2520Inducing%2520Reasoning%2520in%2520Small%2520Language%2520Models%26entry.906535625%3DMax%2520Belitsky%2520and%2520Dawid%2520J.%2520Kopiczko%2520and%2520Michael%2520Dorkenwald%2520and%2520M.%2520Jehanzeb%2520Mirza%2520and%2520Cees%2520G.%2520M.%2520Snoek%2520and%2520Yuki%2520M.%2520Asano%26entry.1292438233%3D%2520%2520We%2520propose%2520cache%2520steering%252C%2520a%2520lightweight%2520method%2520for%2520implicit%2520steering%2520of%250Alanguage%2520models%2520via%2520a%2520one-shot%2520intervention%2520applied%2520directly%2520to%2520the%2520key-value%250Acache.%2520To%2520validate%2520its%2520effectiveness%252C%2520we%2520apply%2520cache%2520steering%2520to%2520induce%250Achain-of-thought%2520reasoning%2520in%2520small%2520language%2520models.%2520Our%2520approach%2520leverages%250AGPT-4o-generated%2520reasoning%2520traces%2520to%2520construct%2520steering%2520vectors%2520that%2520shift%250Amodel%2520behavior%2520toward%2520more%2520explicit%252C%2520multi-step%2520reasoning%2520without%2520fine-tuning%250Aor%2520prompt%2520modifications.%2520Experimental%2520evaluations%2520on%2520diverse%2520reasoning%250Abenchmarks%2520demonstrate%2520that%2520cache%2520steering%2520improves%2520both%2520the%2520qualitative%250Astructure%2520of%2520model%2520reasoning%2520and%2520quantitative%2520task%2520performance.%2520Compared%2520to%250Aprior%2520activation%2520steering%2520techniques%2520that%2520require%2520continuous%2520interventions%252C%2520our%250Aone-shot%2520cache%2520steering%2520offers%2520substantial%2520advantages%2520in%2520terms%2520of%250Ahyperparameter%2520stability%252C%2520inference-time%2520efficiency%252C%2520and%2520ease%2520of%2520integration%252C%250Amaking%2520it%2520a%2520more%2520robust%2520and%2520practical%2520solution%2520for%2520controlled%2520generation.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2507.08799v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=KV%20Cache%20Steering%20for%20Inducing%20Reasoning%20in%20Small%20Language%20Models&entry.906535625=Max%20Belitsky%20and%20Dawid%20J.%20Kopiczko%20and%20Michael%20Dorkenwald%20and%20M.%20Jehanzeb%20Mirza%20and%20Cees%20G.%20M.%20Snoek%20and%20Yuki%20M.%20Asano&entry.1292438233=%20%20We%20propose%20cache%20steering%2C%20a%20lightweight%20method%20for%20implicit%20steering%20of%0Alanguage%20models%20via%20a%20one-shot%20intervention%20applied%20directly%20to%20the%20key-value%0Acache.%20To%20validate%20its%20effectiveness%2C%20we%20apply%20cache%20steering%20to%20induce%0Achain-of-thought%20reasoning%20in%20small%20language%20models.%20Our%20approach%20leverages%0AGPT-4o-generated%20reasoning%20traces%20to%20construct%20steering%20vectors%20that%20shift%0Amodel%20behavior%20toward%20more%20explicit%2C%20multi-step%20reasoning%20without%20fine-tuning%0Aor%20prompt%20modifications.%20Experimental%20evaluations%20on%20diverse%20reasoning%0Abenchmarks%20demonstrate%20that%20cache%20steering%20improves%20both%20the%20qualitative%0Astructure%20of%20model%20reasoning%20and%20quantitative%20task%20performance.%20Compared%20to%0Aprior%20activation%20steering%20techniques%20that%20require%20continuous%20interventions%2C%20our%0Aone-shot%20cache%20steering%20offers%20substantial%20advantages%20in%20terms%20of%0Ahyperparameter%20stability%2C%20inference-time%20efficiency%2C%20and%20ease%20of%20integration%2C%0Amaking%20it%20a%20more%20robust%20and%20practical%20solution%20for%20controlled%20generation.%0A&entry.1838667208=http%3A//arxiv.org/abs/2507.08799v1&entry.124074799=Read"},
{"title": "DatasetAgent: A Novel Multi-Agent System for Auto-Constructing Datasets\n  from Real-World Images", "author": "Haoran Sun and Haoyu Bian and Shaoning Zeng and Yunbo Rao and Xu Xu and Lin Mei and Jianping Gou", "abstract": "  Common knowledge indicates that the process of constructing image datasets\nusually depends on the time-intensive and inefficient method of manual\ncollection and annotation. Large models offer a solution via data generation.\nNonetheless, real-world data are obviously more valuable comparing to\nartificially intelligence generated data, particularly in constructing image\ndatasets. For this reason, we propose a novel method for auto-constructing\ndatasets from real-world images by a multiagent collaborative system, named as\nDatasetAgent. By coordinating four different agents equipped with Multi-modal\nLarge Language Models (MLLMs), as well as a tool package for image\noptimization, DatasetAgent is able to construct high-quality image datasets\naccording to user-specified requirements. In particular, two types of\nexperiments are conducted, including expanding existing datasets and creating\nnew ones from scratch, on a variety of open-source datasets. In both cases,\nmultiple image datasets constructed by DatasetAgent are used to train various\nvision models for image classification, object detection, and image\nsegmentation.\n", "link": "http://arxiv.org/abs/2507.08648v1", "date": "2025-07-11", "relevancy": 2.1975, "topK": [{"title": "RoboEXP: Action-Conditioned Scene Graph via Interactive Exploration for\n  Robotic Manipulation", "link": "http://arxiv.org/abs/2402.15487v1", "similarity": 0.5546}, {"title": "Genie: Generative Interactive Environments", "link": "http://arxiv.org/abs/2402.15391v1", "similarity": 0.5539}, {"title": "Self-supervised Photographic Image Layout Representation Learning", "link": "http://arxiv.org/abs/2403.03740v1", "similarity": 0.5427}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20DatasetAgent%3A%20A%20Novel%20Multi-Agent%20System%20for%20Auto-Constructing%20Datasets%0A%20%20from%20Real-World%20Images&body=Title%3A%20DatasetAgent%3A%20A%20Novel%20Multi-Agent%20System%20for%20Auto-Constructing%20Datasets%0A%20%20from%20Real-World%20Images%0AAuthor%3A%20Haoran%20Sun%20and%20Haoyu%20Bian%20and%20Shaoning%20Zeng%20and%20Yunbo%20Rao%20and%20Xu%20Xu%20and%20Lin%20Mei%20and%20Jianping%20Gou%0AAbstract%3A%20%20%20Common%20knowledge%20indicates%20that%20the%20process%20of%20constructing%20image%20datasets%0Ausually%20depends%20on%20the%20time-intensive%20and%20inefficient%20method%20of%20manual%0Acollection%20and%20annotation.%20Large%20models%20offer%20a%20solution%20via%20data%20generation.%0ANonetheless%2C%20real-world%20data%20are%20obviously%20more%20valuable%20comparing%20to%0Aartificially%20intelligence%20generated%20data%2C%20particularly%20in%20constructing%20image%0Adatasets.%20For%20this%20reason%2C%20we%20propose%20a%20novel%20method%20for%20auto-constructing%0Adatasets%20from%20real-world%20images%20by%20a%20multiagent%20collaborative%20system%2C%20named%20as%0ADatasetAgent.%20By%20coordinating%20four%20different%20agents%20equipped%20with%20Multi-modal%0ALarge%20Language%20Models%20%28MLLMs%29%2C%20as%20well%20as%20a%20tool%20package%20for%20image%0Aoptimization%2C%20DatasetAgent%20is%20able%20to%20construct%20high-quality%20image%20datasets%0Aaccording%20to%20user-specified%20requirements.%20In%20particular%2C%20two%20types%20of%0Aexperiments%20are%20conducted%2C%20including%20expanding%20existing%20datasets%20and%20creating%0Anew%20ones%20from%20scratch%2C%20on%20a%20variety%20of%20open-source%20datasets.%20In%20both%20cases%2C%0Amultiple%20image%20datasets%20constructed%20by%20DatasetAgent%20are%20used%20to%20train%20various%0Avision%20models%20for%20image%20classification%2C%20object%20detection%2C%20and%20image%0Asegmentation.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2507.08648v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DDatasetAgent%253A%2520A%2520Novel%2520Multi-Agent%2520System%2520for%2520Auto-Constructing%2520Datasets%250A%2520%2520from%2520Real-World%2520Images%26entry.906535625%3DHaoran%2520Sun%2520and%2520Haoyu%2520Bian%2520and%2520Shaoning%2520Zeng%2520and%2520Yunbo%2520Rao%2520and%2520Xu%2520Xu%2520and%2520Lin%2520Mei%2520and%2520Jianping%2520Gou%26entry.1292438233%3D%2520%2520Common%2520knowledge%2520indicates%2520that%2520the%2520process%2520of%2520constructing%2520image%2520datasets%250Ausually%2520depends%2520on%2520the%2520time-intensive%2520and%2520inefficient%2520method%2520of%2520manual%250Acollection%2520and%2520annotation.%2520Large%2520models%2520offer%2520a%2520solution%2520via%2520data%2520generation.%250ANonetheless%252C%2520real-world%2520data%2520are%2520obviously%2520more%2520valuable%2520comparing%2520to%250Aartificially%2520intelligence%2520generated%2520data%252C%2520particularly%2520in%2520constructing%2520image%250Adatasets.%2520For%2520this%2520reason%252C%2520we%2520propose%2520a%2520novel%2520method%2520for%2520auto-constructing%250Adatasets%2520from%2520real-world%2520images%2520by%2520a%2520multiagent%2520collaborative%2520system%252C%2520named%2520as%250ADatasetAgent.%2520By%2520coordinating%2520four%2520different%2520agents%2520equipped%2520with%2520Multi-modal%250ALarge%2520Language%2520Models%2520%2528MLLMs%2529%252C%2520as%2520well%2520as%2520a%2520tool%2520package%2520for%2520image%250Aoptimization%252C%2520DatasetAgent%2520is%2520able%2520to%2520construct%2520high-quality%2520image%2520datasets%250Aaccording%2520to%2520user-specified%2520requirements.%2520In%2520particular%252C%2520two%2520types%2520of%250Aexperiments%2520are%2520conducted%252C%2520including%2520expanding%2520existing%2520datasets%2520and%2520creating%250Anew%2520ones%2520from%2520scratch%252C%2520on%2520a%2520variety%2520of%2520open-source%2520datasets.%2520In%2520both%2520cases%252C%250Amultiple%2520image%2520datasets%2520constructed%2520by%2520DatasetAgent%2520are%2520used%2520to%2520train%2520various%250Avision%2520models%2520for%2520image%2520classification%252C%2520object%2520detection%252C%2520and%2520image%250Asegmentation.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2507.08648v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=DatasetAgent%3A%20A%20Novel%20Multi-Agent%20System%20for%20Auto-Constructing%20Datasets%0A%20%20from%20Real-World%20Images&entry.906535625=Haoran%20Sun%20and%20Haoyu%20Bian%20and%20Shaoning%20Zeng%20and%20Yunbo%20Rao%20and%20Xu%20Xu%20and%20Lin%20Mei%20and%20Jianping%20Gou&entry.1292438233=%20%20Common%20knowledge%20indicates%20that%20the%20process%20of%20constructing%20image%20datasets%0Ausually%20depends%20on%20the%20time-intensive%20and%20inefficient%20method%20of%20manual%0Acollection%20and%20annotation.%20Large%20models%20offer%20a%20solution%20via%20data%20generation.%0ANonetheless%2C%20real-world%20data%20are%20obviously%20more%20valuable%20comparing%20to%0Aartificially%20intelligence%20generated%20data%2C%20particularly%20in%20constructing%20image%0Adatasets.%20For%20this%20reason%2C%20we%20propose%20a%20novel%20method%20for%20auto-constructing%0Adatasets%20from%20real-world%20images%20by%20a%20multiagent%20collaborative%20system%2C%20named%20as%0ADatasetAgent.%20By%20coordinating%20four%20different%20agents%20equipped%20with%20Multi-modal%0ALarge%20Language%20Models%20%28MLLMs%29%2C%20as%20well%20as%20a%20tool%20package%20for%20image%0Aoptimization%2C%20DatasetAgent%20is%20able%20to%20construct%20high-quality%20image%20datasets%0Aaccording%20to%20user-specified%20requirements.%20In%20particular%2C%20two%20types%20of%0Aexperiments%20are%20conducted%2C%20including%20expanding%20existing%20datasets%20and%20creating%0Anew%20ones%20from%20scratch%2C%20on%20a%20variety%20of%20open-source%20datasets.%20In%20both%20cases%2C%0Amultiple%20image%20datasets%20constructed%20by%20DatasetAgent%20are%20used%20to%20train%20various%0Avision%20models%20for%20image%20classification%2C%20object%20detection%2C%20and%20image%0Asegmentation.%0A&entry.1838667208=http%3A//arxiv.org/abs/2507.08648v1&entry.124074799=Read"},
{"title": "NeuralOS: Towards Simulating Operating Systems via Neural Generative\n  Models", "author": "Luke Rivard and Sun Sun and Hongyu Guo and Wenhu Chen and Yuntian Deng", "abstract": "  We introduce NeuralOS, a neural framework that simulates graphical user\ninterfaces (GUIs) of operating systems by directly predicting screen frames in\nresponse to user inputs such as mouse movements, clicks, and keyboard events.\nNeuralOS combines a recurrent neural network (RNN), which tracks computer\nstate, with a diffusion-based neural renderer that generates screen images. The\nmodel is trained on a large-scale dataset of Ubuntu XFCE recordings, which\ninclude both randomly generated interactions and realistic interactions\nproduced by AI agents. Experiments show that NeuralOS successfully renders\nrealistic GUI sequences, accurately captures mouse interactions, and reliably\npredicts state transitions like application launches. Although modeling\nfine-grained keyboard interactions precisely remains challenging, NeuralOS\noffers a step toward creating fully adaptive, generative neural interfaces for\nfuture human-computer interaction systems.\n", "link": "http://arxiv.org/abs/2507.08800v1", "date": "2025-07-11", "relevancy": 2.1839, "topK": [{"title": "Genie: Generative Interactive Environments", "link": "http://arxiv.org/abs/2402.15391v1", "similarity": 0.5577}, {"title": "Thinking Outside the BBox: Unconstrained Generative Object Compositing", "link": "http://arxiv.org/abs/2409.04559v2", "similarity": 0.5419}, {"title": "PhysGen: Rigid-Body Physics-Grounded Image-to-Video Generation", "link": "http://arxiv.org/abs/2409.18964v1", "similarity": 0.5359}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20NeuralOS%3A%20Towards%20Simulating%20Operating%20Systems%20via%20Neural%20Generative%0A%20%20Models&body=Title%3A%20NeuralOS%3A%20Towards%20Simulating%20Operating%20Systems%20via%20Neural%20Generative%0A%20%20Models%0AAuthor%3A%20Luke%20Rivard%20and%20Sun%20Sun%20and%20Hongyu%20Guo%20and%20Wenhu%20Chen%20and%20Yuntian%20Deng%0AAbstract%3A%20%20%20We%20introduce%20NeuralOS%2C%20a%20neural%20framework%20that%20simulates%20graphical%20user%0Ainterfaces%20%28GUIs%29%20of%20operating%20systems%20by%20directly%20predicting%20screen%20frames%20in%0Aresponse%20to%20user%20inputs%20such%20as%20mouse%20movements%2C%20clicks%2C%20and%20keyboard%20events.%0ANeuralOS%20combines%20a%20recurrent%20neural%20network%20%28RNN%29%2C%20which%20tracks%20computer%0Astate%2C%20with%20a%20diffusion-based%20neural%20renderer%20that%20generates%20screen%20images.%20The%0Amodel%20is%20trained%20on%20a%20large-scale%20dataset%20of%20Ubuntu%20XFCE%20recordings%2C%20which%0Ainclude%20both%20randomly%20generated%20interactions%20and%20realistic%20interactions%0Aproduced%20by%20AI%20agents.%20Experiments%20show%20that%20NeuralOS%20successfully%20renders%0Arealistic%20GUI%20sequences%2C%20accurately%20captures%20mouse%20interactions%2C%20and%20reliably%0Apredicts%20state%20transitions%20like%20application%20launches.%20Although%20modeling%0Afine-grained%20keyboard%20interactions%20precisely%20remains%20challenging%2C%20NeuralOS%0Aoffers%20a%20step%20toward%20creating%20fully%20adaptive%2C%20generative%20neural%20interfaces%20for%0Afuture%20human-computer%20interaction%20systems.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2507.08800v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DNeuralOS%253A%2520Towards%2520Simulating%2520Operating%2520Systems%2520via%2520Neural%2520Generative%250A%2520%2520Models%26entry.906535625%3DLuke%2520Rivard%2520and%2520Sun%2520Sun%2520and%2520Hongyu%2520Guo%2520and%2520Wenhu%2520Chen%2520and%2520Yuntian%2520Deng%26entry.1292438233%3D%2520%2520We%2520introduce%2520NeuralOS%252C%2520a%2520neural%2520framework%2520that%2520simulates%2520graphical%2520user%250Ainterfaces%2520%2528GUIs%2529%2520of%2520operating%2520systems%2520by%2520directly%2520predicting%2520screen%2520frames%2520in%250Aresponse%2520to%2520user%2520inputs%2520such%2520as%2520mouse%2520movements%252C%2520clicks%252C%2520and%2520keyboard%2520events.%250ANeuralOS%2520combines%2520a%2520recurrent%2520neural%2520network%2520%2528RNN%2529%252C%2520which%2520tracks%2520computer%250Astate%252C%2520with%2520a%2520diffusion-based%2520neural%2520renderer%2520that%2520generates%2520screen%2520images.%2520The%250Amodel%2520is%2520trained%2520on%2520a%2520large-scale%2520dataset%2520of%2520Ubuntu%2520XFCE%2520recordings%252C%2520which%250Ainclude%2520both%2520randomly%2520generated%2520interactions%2520and%2520realistic%2520interactions%250Aproduced%2520by%2520AI%2520agents.%2520Experiments%2520show%2520that%2520NeuralOS%2520successfully%2520renders%250Arealistic%2520GUI%2520sequences%252C%2520accurately%2520captures%2520mouse%2520interactions%252C%2520and%2520reliably%250Apredicts%2520state%2520transitions%2520like%2520application%2520launches.%2520Although%2520modeling%250Afine-grained%2520keyboard%2520interactions%2520precisely%2520remains%2520challenging%252C%2520NeuralOS%250Aoffers%2520a%2520step%2520toward%2520creating%2520fully%2520adaptive%252C%2520generative%2520neural%2520interfaces%2520for%250Afuture%2520human-computer%2520interaction%2520systems.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2507.08800v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=NeuralOS%3A%20Towards%20Simulating%20Operating%20Systems%20via%20Neural%20Generative%0A%20%20Models&entry.906535625=Luke%20Rivard%20and%20Sun%20Sun%20and%20Hongyu%20Guo%20and%20Wenhu%20Chen%20and%20Yuntian%20Deng&entry.1292438233=%20%20We%20introduce%20NeuralOS%2C%20a%20neural%20framework%20that%20simulates%20graphical%20user%0Ainterfaces%20%28GUIs%29%20of%20operating%20systems%20by%20directly%20predicting%20screen%20frames%20in%0Aresponse%20to%20user%20inputs%20such%20as%20mouse%20movements%2C%20clicks%2C%20and%20keyboard%20events.%0ANeuralOS%20combines%20a%20recurrent%20neural%20network%20%28RNN%29%2C%20which%20tracks%20computer%0Astate%2C%20with%20a%20diffusion-based%20neural%20renderer%20that%20generates%20screen%20images.%20The%0Amodel%20is%20trained%20on%20a%20large-scale%20dataset%20of%20Ubuntu%20XFCE%20recordings%2C%20which%0Ainclude%20both%20randomly%20generated%20interactions%20and%20realistic%20interactions%0Aproduced%20by%20AI%20agents.%20Experiments%20show%20that%20NeuralOS%20successfully%20renders%0Arealistic%20GUI%20sequences%2C%20accurately%20captures%20mouse%20interactions%2C%20and%20reliably%0Apredicts%20state%20transitions%20like%20application%20launches.%20Although%20modeling%0Afine-grained%20keyboard%20interactions%20precisely%20remains%20challenging%2C%20NeuralOS%0Aoffers%20a%20step%20toward%20creating%20fully%20adaptive%2C%20generative%20neural%20interfaces%20for%0Afuture%20human-computer%20interaction%20systems.%0A&entry.1838667208=http%3A//arxiv.org/abs/2507.08800v1&entry.124074799=Read"},
{"title": "HieraRS: A Hierarchical Segmentation Paradigm for Remote Sensing\n  Enabling Multi-Granularity Interpretation and Cross-Domain Transfer", "author": "Tianlong Ai and Tianzhu Liu and Haochen Jiang and Yanfeng Gu", "abstract": "  Hierarchical land cover and land use (LCLU) classification aims to assign\npixel-wise labels with multiple levels of semantic granularity to remote\nsensing (RS) imagery. However, existing deep learning-based methods face two\nmajor challenges: 1) They predominantly adopt a flat classification paradigm,\nwhich limits their ability to generate end-to-end multi-granularity\nhierarchical predictions aligned with tree-structured hierarchies used in\npractice. 2) Most cross-domain studies focus on performance degradation caused\nby sensor or scene variations, with limited attention to transferring LCLU\nmodels to cross-domain tasks with heterogeneous hierarchies (e.g., LCLU to crop\nclassification). These limitations hinder the flexibility and generalization of\nLCLU models in practical applications. To address these challenges, we propose\nHieraRS, a novel hierarchical interpretation paradigm that enables\nmulti-granularity predictions and supports the efficient transfer of LCLU\nmodels to cross-domain tasks with heterogeneous tree-structured hierarchies. We\nintroduce the Bidirectional Hierarchical Consistency Constraint Mechanism\n(BHCCM), which can be seamlessly integrated into mainstream flat classification\nmodels to generate hierarchical predictions, while improving both semantic\nconsistency and classification accuracy. Furthermore, we present TransLU, a\ndual-branch cross-domain transfer framework comprising two key components:\nCross-Domain Knowledge Sharing (CDKS) and Cross-Domain Semantic Alignment\n(CDSA). TransLU supports dynamic category expansion and facilitates the\neffective adaptation of LCLU models to heterogeneous hierarchies. In addition,\nwe construct MM-5B, a large-scale multi-modal hierarchical land use dataset\nfeaturing pixel-wise annotations. The code and MM-5B dataset will be released\nat: https://github.com/AI-Tianlong/HieraRS.\n", "link": "http://arxiv.org/abs/2507.08741v1", "date": "2025-07-11", "relevancy": 2.1817, "topK": [{"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.55}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5479}, {"title": "Self-supervised Photographic Image Layout Representation Learning", "link": "http://arxiv.org/abs/2403.03740v1", "similarity": 0.5411}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20HieraRS%3A%20A%20Hierarchical%20Segmentation%20Paradigm%20for%20Remote%20Sensing%0A%20%20Enabling%20Multi-Granularity%20Interpretation%20and%20Cross-Domain%20Transfer&body=Title%3A%20HieraRS%3A%20A%20Hierarchical%20Segmentation%20Paradigm%20for%20Remote%20Sensing%0A%20%20Enabling%20Multi-Granularity%20Interpretation%20and%20Cross-Domain%20Transfer%0AAuthor%3A%20Tianlong%20Ai%20and%20Tianzhu%20Liu%20and%20Haochen%20Jiang%20and%20Yanfeng%20Gu%0AAbstract%3A%20%20%20Hierarchical%20land%20cover%20and%20land%20use%20%28LCLU%29%20classification%20aims%20to%20assign%0Apixel-wise%20labels%20with%20multiple%20levels%20of%20semantic%20granularity%20to%20remote%0Asensing%20%28RS%29%20imagery.%20However%2C%20existing%20deep%20learning-based%20methods%20face%20two%0Amajor%20challenges%3A%201%29%20They%20predominantly%20adopt%20a%20flat%20classification%20paradigm%2C%0Awhich%20limits%20their%20ability%20to%20generate%20end-to-end%20multi-granularity%0Ahierarchical%20predictions%20aligned%20with%20tree-structured%20hierarchies%20used%20in%0Apractice.%202%29%20Most%20cross-domain%20studies%20focus%20on%20performance%20degradation%20caused%0Aby%20sensor%20or%20scene%20variations%2C%20with%20limited%20attention%20to%20transferring%20LCLU%0Amodels%20to%20cross-domain%20tasks%20with%20heterogeneous%20hierarchies%20%28e.g.%2C%20LCLU%20to%20crop%0Aclassification%29.%20These%20limitations%20hinder%20the%20flexibility%20and%20generalization%20of%0ALCLU%20models%20in%20practical%20applications.%20To%20address%20these%20challenges%2C%20we%20propose%0AHieraRS%2C%20a%20novel%20hierarchical%20interpretation%20paradigm%20that%20enables%0Amulti-granularity%20predictions%20and%20supports%20the%20efficient%20transfer%20of%20LCLU%0Amodels%20to%20cross-domain%20tasks%20with%20heterogeneous%20tree-structured%20hierarchies.%20We%0Aintroduce%20the%20Bidirectional%20Hierarchical%20Consistency%20Constraint%20Mechanism%0A%28BHCCM%29%2C%20which%20can%20be%20seamlessly%20integrated%20into%20mainstream%20flat%20classification%0Amodels%20to%20generate%20hierarchical%20predictions%2C%20while%20improving%20both%20semantic%0Aconsistency%20and%20classification%20accuracy.%20Furthermore%2C%20we%20present%20TransLU%2C%20a%0Adual-branch%20cross-domain%20transfer%20framework%20comprising%20two%20key%20components%3A%0ACross-Domain%20Knowledge%20Sharing%20%28CDKS%29%20and%20Cross-Domain%20Semantic%20Alignment%0A%28CDSA%29.%20TransLU%20supports%20dynamic%20category%20expansion%20and%20facilitates%20the%0Aeffective%20adaptation%20of%20LCLU%20models%20to%20heterogeneous%20hierarchies.%20In%20addition%2C%0Awe%20construct%20MM-5B%2C%20a%20large-scale%20multi-modal%20hierarchical%20land%20use%20dataset%0Afeaturing%20pixel-wise%20annotations.%20The%20code%20and%20MM-5B%20dataset%20will%20be%20released%0Aat%3A%20https%3A//github.com/AI-Tianlong/HieraRS.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2507.08741v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DHieraRS%253A%2520A%2520Hierarchical%2520Segmentation%2520Paradigm%2520for%2520Remote%2520Sensing%250A%2520%2520Enabling%2520Multi-Granularity%2520Interpretation%2520and%2520Cross-Domain%2520Transfer%26entry.906535625%3DTianlong%2520Ai%2520and%2520Tianzhu%2520Liu%2520and%2520Haochen%2520Jiang%2520and%2520Yanfeng%2520Gu%26entry.1292438233%3D%2520%2520Hierarchical%2520land%2520cover%2520and%2520land%2520use%2520%2528LCLU%2529%2520classification%2520aims%2520to%2520assign%250Apixel-wise%2520labels%2520with%2520multiple%2520levels%2520of%2520semantic%2520granularity%2520to%2520remote%250Asensing%2520%2528RS%2529%2520imagery.%2520However%252C%2520existing%2520deep%2520learning-based%2520methods%2520face%2520two%250Amajor%2520challenges%253A%25201%2529%2520They%2520predominantly%2520adopt%2520a%2520flat%2520classification%2520paradigm%252C%250Awhich%2520limits%2520their%2520ability%2520to%2520generate%2520end-to-end%2520multi-granularity%250Ahierarchical%2520predictions%2520aligned%2520with%2520tree-structured%2520hierarchies%2520used%2520in%250Apractice.%25202%2529%2520Most%2520cross-domain%2520studies%2520focus%2520on%2520performance%2520degradation%2520caused%250Aby%2520sensor%2520or%2520scene%2520variations%252C%2520with%2520limited%2520attention%2520to%2520transferring%2520LCLU%250Amodels%2520to%2520cross-domain%2520tasks%2520with%2520heterogeneous%2520hierarchies%2520%2528e.g.%252C%2520LCLU%2520to%2520crop%250Aclassification%2529.%2520These%2520limitations%2520hinder%2520the%2520flexibility%2520and%2520generalization%2520of%250ALCLU%2520models%2520in%2520practical%2520applications.%2520To%2520address%2520these%2520challenges%252C%2520we%2520propose%250AHieraRS%252C%2520a%2520novel%2520hierarchical%2520interpretation%2520paradigm%2520that%2520enables%250Amulti-granularity%2520predictions%2520and%2520supports%2520the%2520efficient%2520transfer%2520of%2520LCLU%250Amodels%2520to%2520cross-domain%2520tasks%2520with%2520heterogeneous%2520tree-structured%2520hierarchies.%2520We%250Aintroduce%2520the%2520Bidirectional%2520Hierarchical%2520Consistency%2520Constraint%2520Mechanism%250A%2528BHCCM%2529%252C%2520which%2520can%2520be%2520seamlessly%2520integrated%2520into%2520mainstream%2520flat%2520classification%250Amodels%2520to%2520generate%2520hierarchical%2520predictions%252C%2520while%2520improving%2520both%2520semantic%250Aconsistency%2520and%2520classification%2520accuracy.%2520Furthermore%252C%2520we%2520present%2520TransLU%252C%2520a%250Adual-branch%2520cross-domain%2520transfer%2520framework%2520comprising%2520two%2520key%2520components%253A%250ACross-Domain%2520Knowledge%2520Sharing%2520%2528CDKS%2529%2520and%2520Cross-Domain%2520Semantic%2520Alignment%250A%2528CDSA%2529.%2520TransLU%2520supports%2520dynamic%2520category%2520expansion%2520and%2520facilitates%2520the%250Aeffective%2520adaptation%2520of%2520LCLU%2520models%2520to%2520heterogeneous%2520hierarchies.%2520In%2520addition%252C%250Awe%2520construct%2520MM-5B%252C%2520a%2520large-scale%2520multi-modal%2520hierarchical%2520land%2520use%2520dataset%250Afeaturing%2520pixel-wise%2520annotations.%2520The%2520code%2520and%2520MM-5B%2520dataset%2520will%2520be%2520released%250Aat%253A%2520https%253A//github.com/AI-Tianlong/HieraRS.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2507.08741v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=HieraRS%3A%20A%20Hierarchical%20Segmentation%20Paradigm%20for%20Remote%20Sensing%0A%20%20Enabling%20Multi-Granularity%20Interpretation%20and%20Cross-Domain%20Transfer&entry.906535625=Tianlong%20Ai%20and%20Tianzhu%20Liu%20and%20Haochen%20Jiang%20and%20Yanfeng%20Gu&entry.1292438233=%20%20Hierarchical%20land%20cover%20and%20land%20use%20%28LCLU%29%20classification%20aims%20to%20assign%0Apixel-wise%20labels%20with%20multiple%20levels%20of%20semantic%20granularity%20to%20remote%0Asensing%20%28RS%29%20imagery.%20However%2C%20existing%20deep%20learning-based%20methods%20face%20two%0Amajor%20challenges%3A%201%29%20They%20predominantly%20adopt%20a%20flat%20classification%20paradigm%2C%0Awhich%20limits%20their%20ability%20to%20generate%20end-to-end%20multi-granularity%0Ahierarchical%20predictions%20aligned%20with%20tree-structured%20hierarchies%20used%20in%0Apractice.%202%29%20Most%20cross-domain%20studies%20focus%20on%20performance%20degradation%20caused%0Aby%20sensor%20or%20scene%20variations%2C%20with%20limited%20attention%20to%20transferring%20LCLU%0Amodels%20to%20cross-domain%20tasks%20with%20heterogeneous%20hierarchies%20%28e.g.%2C%20LCLU%20to%20crop%0Aclassification%29.%20These%20limitations%20hinder%20the%20flexibility%20and%20generalization%20of%0ALCLU%20models%20in%20practical%20applications.%20To%20address%20these%20challenges%2C%20we%20propose%0AHieraRS%2C%20a%20novel%20hierarchical%20interpretation%20paradigm%20that%20enables%0Amulti-granularity%20predictions%20and%20supports%20the%20efficient%20transfer%20of%20LCLU%0Amodels%20to%20cross-domain%20tasks%20with%20heterogeneous%20tree-structured%20hierarchies.%20We%0Aintroduce%20the%20Bidirectional%20Hierarchical%20Consistency%20Constraint%20Mechanism%0A%28BHCCM%29%2C%20which%20can%20be%20seamlessly%20integrated%20into%20mainstream%20flat%20classification%0Amodels%20to%20generate%20hierarchical%20predictions%2C%20while%20improving%20both%20semantic%0Aconsistency%20and%20classification%20accuracy.%20Furthermore%2C%20we%20present%20TransLU%2C%20a%0Adual-branch%20cross-domain%20transfer%20framework%20comprising%20two%20key%20components%3A%0ACross-Domain%20Knowledge%20Sharing%20%28CDKS%29%20and%20Cross-Domain%20Semantic%20Alignment%0A%28CDSA%29.%20TransLU%20supports%20dynamic%20category%20expansion%20and%20facilitates%20the%0Aeffective%20adaptation%20of%20LCLU%20models%20to%20heterogeneous%20hierarchies.%20In%20addition%2C%0Awe%20construct%20MM-5B%2C%20a%20large-scale%20multi-modal%20hierarchical%20land%20use%20dataset%0Afeaturing%20pixel-wise%20annotations.%20The%20code%20and%20MM-5B%20dataset%20will%20be%20released%0Aat%3A%20https%3A//github.com/AI-Tianlong/HieraRS.%0A&entry.1838667208=http%3A//arxiv.org/abs/2507.08741v1&entry.124074799=Read"},
{"title": "RoundaboutHD: High-Resolution Real-World Urban Environment Benchmark for\n  Multi-Camera Vehicle Tracking", "author": "Yuqiang Lin and Sam Lockyer and Mingxuan Sui and Li Gan and Florian Stanek and Markus Zarbock and Wenbin Li and Adrian Evans and Nic Zhang", "abstract": "  The multi-camera vehicle tracking (MCVT) framework holds significant\npotential for smart city applications, including anomaly detection, traffic\ndensity estimation, and suspect vehicle tracking. However, current publicly\navailable datasets exhibit limitations, such as overly simplistic scenarios,\nlow-resolution footage, and insufficiently diverse conditions, creating a\nconsiderable gap between academic research and real-world scenario. To fill\nthis gap, we introduce RoundaboutHD, a comprehensive, high-resolution\nmulti-camera vehicle tracking benchmark dataset specifically designed to\nrepresent real-world roundabout scenarios. RoundaboutHD provides a total of 40\nminutes of labelled video footage captured by four non-overlapping,\nhigh-resolution (4K resolution, 15 fps) cameras. In total, 512 unique vehicle\nidentities are annotated across different camera views, offering rich\ncross-camera association data. RoundaboutHD offers temporal consistency video\nfootage and enhanced challenges, including increased occlusions and nonlinear\nmovement inside the roundabout. In addition to the full MCVT dataset, several\nsubsets are also available for object detection, single camera tracking, and\nimage-based vehicle re-identification (ReID) tasks. Vehicle model information\nand camera modelling/ geometry information are also included to support further\nanalysis. We provide baseline results for vehicle detection, single-camera\ntracking, image-based vehicle re-identification, and multi-camera tracking. The\ndataset and the evaluation code are publicly available at:\nhttps://github.com/siri-rouser/RoundaboutHD.git\n", "link": "http://arxiv.org/abs/2507.08729v1", "date": "2025-07-11", "relevancy": 2.1793, "topK": [{"title": "CamCtrl3D: Single-Image Scene Exploration with Precise 3D Camera Control", "link": "http://arxiv.org/abs/2501.06006v1", "similarity": 0.5792}, {"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.5394}, {"title": "CAT3D: Create Anything in 3D with Multi-View Diffusion Models", "link": "http://arxiv.org/abs/2405.10314v1", "similarity": 0.5364}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20RoundaboutHD%3A%20High-Resolution%20Real-World%20Urban%20Environment%20Benchmark%20for%0A%20%20Multi-Camera%20Vehicle%20Tracking&body=Title%3A%20RoundaboutHD%3A%20High-Resolution%20Real-World%20Urban%20Environment%20Benchmark%20for%0A%20%20Multi-Camera%20Vehicle%20Tracking%0AAuthor%3A%20Yuqiang%20Lin%20and%20Sam%20Lockyer%20and%20Mingxuan%20Sui%20and%20Li%20Gan%20and%20Florian%20Stanek%20and%20Markus%20Zarbock%20and%20Wenbin%20Li%20and%20Adrian%20Evans%20and%20Nic%20Zhang%0AAbstract%3A%20%20%20The%20multi-camera%20vehicle%20tracking%20%28MCVT%29%20framework%20holds%20significant%0Apotential%20for%20smart%20city%20applications%2C%20including%20anomaly%20detection%2C%20traffic%0Adensity%20estimation%2C%20and%20suspect%20vehicle%20tracking.%20However%2C%20current%20publicly%0Aavailable%20datasets%20exhibit%20limitations%2C%20such%20as%20overly%20simplistic%20scenarios%2C%0Alow-resolution%20footage%2C%20and%20insufficiently%20diverse%20conditions%2C%20creating%20a%0Aconsiderable%20gap%20between%20academic%20research%20and%20real-world%20scenario.%20To%20fill%0Athis%20gap%2C%20we%20introduce%20RoundaboutHD%2C%20a%20comprehensive%2C%20high-resolution%0Amulti-camera%20vehicle%20tracking%20benchmark%20dataset%20specifically%20designed%20to%0Arepresent%20real-world%20roundabout%20scenarios.%20RoundaboutHD%20provides%20a%20total%20of%2040%0Aminutes%20of%20labelled%20video%20footage%20captured%20by%20four%20non-overlapping%2C%0Ahigh-resolution%20%284K%20resolution%2C%2015%20fps%29%20cameras.%20In%20total%2C%20512%20unique%20vehicle%0Aidentities%20are%20annotated%20across%20different%20camera%20views%2C%20offering%20rich%0Across-camera%20association%20data.%20RoundaboutHD%20offers%20temporal%20consistency%20video%0Afootage%20and%20enhanced%20challenges%2C%20including%20increased%20occlusions%20and%20nonlinear%0Amovement%20inside%20the%20roundabout.%20In%20addition%20to%20the%20full%20MCVT%20dataset%2C%20several%0Asubsets%20are%20also%20available%20for%20object%20detection%2C%20single%20camera%20tracking%2C%20and%0Aimage-based%20vehicle%20re-identification%20%28ReID%29%20tasks.%20Vehicle%20model%20information%0Aand%20camera%20modelling/%20geometry%20information%20are%20also%20included%20to%20support%20further%0Aanalysis.%20We%20provide%20baseline%20results%20for%20vehicle%20detection%2C%20single-camera%0Atracking%2C%20image-based%20vehicle%20re-identification%2C%20and%20multi-camera%20tracking.%20The%0Adataset%20and%20the%20evaluation%20code%20are%20publicly%20available%20at%3A%0Ahttps%3A//github.com/siri-rouser/RoundaboutHD.git%0A%0ALink%3A%20http%3A//arxiv.org/abs/2507.08729v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DRoundaboutHD%253A%2520High-Resolution%2520Real-World%2520Urban%2520Environment%2520Benchmark%2520for%250A%2520%2520Multi-Camera%2520Vehicle%2520Tracking%26entry.906535625%3DYuqiang%2520Lin%2520and%2520Sam%2520Lockyer%2520and%2520Mingxuan%2520Sui%2520and%2520Li%2520Gan%2520and%2520Florian%2520Stanek%2520and%2520Markus%2520Zarbock%2520and%2520Wenbin%2520Li%2520and%2520Adrian%2520Evans%2520and%2520Nic%2520Zhang%26entry.1292438233%3D%2520%2520The%2520multi-camera%2520vehicle%2520tracking%2520%2528MCVT%2529%2520framework%2520holds%2520significant%250Apotential%2520for%2520smart%2520city%2520applications%252C%2520including%2520anomaly%2520detection%252C%2520traffic%250Adensity%2520estimation%252C%2520and%2520suspect%2520vehicle%2520tracking.%2520However%252C%2520current%2520publicly%250Aavailable%2520datasets%2520exhibit%2520limitations%252C%2520such%2520as%2520overly%2520simplistic%2520scenarios%252C%250Alow-resolution%2520footage%252C%2520and%2520insufficiently%2520diverse%2520conditions%252C%2520creating%2520a%250Aconsiderable%2520gap%2520between%2520academic%2520research%2520and%2520real-world%2520scenario.%2520To%2520fill%250Athis%2520gap%252C%2520we%2520introduce%2520RoundaboutHD%252C%2520a%2520comprehensive%252C%2520high-resolution%250Amulti-camera%2520vehicle%2520tracking%2520benchmark%2520dataset%2520specifically%2520designed%2520to%250Arepresent%2520real-world%2520roundabout%2520scenarios.%2520RoundaboutHD%2520provides%2520a%2520total%2520of%252040%250Aminutes%2520of%2520labelled%2520video%2520footage%2520captured%2520by%2520four%2520non-overlapping%252C%250Ahigh-resolution%2520%25284K%2520resolution%252C%252015%2520fps%2529%2520cameras.%2520In%2520total%252C%2520512%2520unique%2520vehicle%250Aidentities%2520are%2520annotated%2520across%2520different%2520camera%2520views%252C%2520offering%2520rich%250Across-camera%2520association%2520data.%2520RoundaboutHD%2520offers%2520temporal%2520consistency%2520video%250Afootage%2520and%2520enhanced%2520challenges%252C%2520including%2520increased%2520occlusions%2520and%2520nonlinear%250Amovement%2520inside%2520the%2520roundabout.%2520In%2520addition%2520to%2520the%2520full%2520MCVT%2520dataset%252C%2520several%250Asubsets%2520are%2520also%2520available%2520for%2520object%2520detection%252C%2520single%2520camera%2520tracking%252C%2520and%250Aimage-based%2520vehicle%2520re-identification%2520%2528ReID%2529%2520tasks.%2520Vehicle%2520model%2520information%250Aand%2520camera%2520modelling/%2520geometry%2520information%2520are%2520also%2520included%2520to%2520support%2520further%250Aanalysis.%2520We%2520provide%2520baseline%2520results%2520for%2520vehicle%2520detection%252C%2520single-camera%250Atracking%252C%2520image-based%2520vehicle%2520re-identification%252C%2520and%2520multi-camera%2520tracking.%2520The%250Adataset%2520and%2520the%2520evaluation%2520code%2520are%2520publicly%2520available%2520at%253A%250Ahttps%253A//github.com/siri-rouser/RoundaboutHD.git%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2507.08729v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=RoundaboutHD%3A%20High-Resolution%20Real-World%20Urban%20Environment%20Benchmark%20for%0A%20%20Multi-Camera%20Vehicle%20Tracking&entry.906535625=Yuqiang%20Lin%20and%20Sam%20Lockyer%20and%20Mingxuan%20Sui%20and%20Li%20Gan%20and%20Florian%20Stanek%20and%20Markus%20Zarbock%20and%20Wenbin%20Li%20and%20Adrian%20Evans%20and%20Nic%20Zhang&entry.1292438233=%20%20The%20multi-camera%20vehicle%20tracking%20%28MCVT%29%20framework%20holds%20significant%0Apotential%20for%20smart%20city%20applications%2C%20including%20anomaly%20detection%2C%20traffic%0Adensity%20estimation%2C%20and%20suspect%20vehicle%20tracking.%20However%2C%20current%20publicly%0Aavailable%20datasets%20exhibit%20limitations%2C%20such%20as%20overly%20simplistic%20scenarios%2C%0Alow-resolution%20footage%2C%20and%20insufficiently%20diverse%20conditions%2C%20creating%20a%0Aconsiderable%20gap%20between%20academic%20research%20and%20real-world%20scenario.%20To%20fill%0Athis%20gap%2C%20we%20introduce%20RoundaboutHD%2C%20a%20comprehensive%2C%20high-resolution%0Amulti-camera%20vehicle%20tracking%20benchmark%20dataset%20specifically%20designed%20to%0Arepresent%20real-world%20roundabout%20scenarios.%20RoundaboutHD%20provides%20a%20total%20of%2040%0Aminutes%20of%20labelled%20video%20footage%20captured%20by%20four%20non-overlapping%2C%0Ahigh-resolution%20%284K%20resolution%2C%2015%20fps%29%20cameras.%20In%20total%2C%20512%20unique%20vehicle%0Aidentities%20are%20annotated%20across%20different%20camera%20views%2C%20offering%20rich%0Across-camera%20association%20data.%20RoundaboutHD%20offers%20temporal%20consistency%20video%0Afootage%20and%20enhanced%20challenges%2C%20including%20increased%20occlusions%20and%20nonlinear%0Amovement%20inside%20the%20roundabout.%20In%20addition%20to%20the%20full%20MCVT%20dataset%2C%20several%0Asubsets%20are%20also%20available%20for%20object%20detection%2C%20single%20camera%20tracking%2C%20and%0Aimage-based%20vehicle%20re-identification%20%28ReID%29%20tasks.%20Vehicle%20model%20information%0Aand%20camera%20modelling/%20geometry%20information%20are%20also%20included%20to%20support%20further%0Aanalysis.%20We%20provide%20baseline%20results%20for%20vehicle%20detection%2C%20single-camera%0Atracking%2C%20image-based%20vehicle%20re-identification%2C%20and%20multi-camera%20tracking.%20The%0Adataset%20and%20the%20evaluation%20code%20are%20publicly%20available%20at%3A%0Ahttps%3A//github.com/siri-rouser/RoundaboutHD.git%0A&entry.1838667208=http%3A//arxiv.org/abs/2507.08729v1&entry.124074799=Read"},
{"title": "Upgrade or Switch: Do We Need a Next-Gen Trusted Architecture for the\n  Internet of AI Agents?", "author": "Ramesh Raskar and Pradyumna Chari and Jared James Grogan and Mahesh Lambe and Robert Lincourt and Raghu Bala and Aditi Joshi and Abhishek Singh and Ayush Chopra and Rajesh Ranjan and Shailja Gupta and Dimitris Stripelis and Maria Gorskikh and Sichao Wang", "abstract": "  The emerging Internet of AI Agents challenges existing web infrastructure\ndesigned for human-scale, reactive interactions. Unlike traditional web\nresources, autonomous AI agents initiate actions, maintain persistent state,\nspawn sub-agents, and negotiate directly with peers: demanding\nmillisecond-level discovery, instant credential revocation, and cryptographic\nbehavioral proofs that exceed current DNS/PKI capabilities. This paper analyzes\nwhether to upgrade existing infrastructure or implement purpose-built index\narchitectures for autonomous agents. We identify critical failure points: DNS\npropagation (24-48 hours vs. required milliseconds), certificate revocation\nunable to scale to trillions of entities, and IPv4/IPv6 addressing inadequate\nfor agent-scale routing. We evaluate three approaches: (1) Upgrade paths, (2)\nSwitch options, (3) Hybrid index/registries. Drawing parallels to\ndialup-to-broadband transitions, we find that agent requirements constitute\nqualitative, and not incremental, changes. While upgrades offer compatibility\nand faster deployment, clean-slate solutions provide better performance but\nrequire longer for adoption. Our analysis suggests hybrid approaches will\nemerge, with centralized indexes for critical agents and federated meshes for\nspecialized use cases.\n", "link": "http://arxiv.org/abs/2506.12003v2", "date": "2025-07-11", "relevancy": 2.1282, "topK": [{"title": "Safe and efficient training of a control agent", "link": "https://patents.google.com/patent/US11709462B2/en", "similarity": 0.4525}, {"title": "Genie: Generative Interactive Environments", "link": "http://arxiv.org/abs/2402.15391v1", "similarity": 0.4128}, {"title": "ToDo: Token Downsampling for Efficient Generation of High-Resolution\n  Images", "link": "http://arxiv.org/abs/2402.13573v2", "similarity": 0.4116}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Upgrade%20or%20Switch%3A%20Do%20We%20Need%20a%20Next-Gen%20Trusted%20Architecture%20for%20the%0A%20%20Internet%20of%20AI%20Agents%3F&body=Title%3A%20Upgrade%20or%20Switch%3A%20Do%20We%20Need%20a%20Next-Gen%20Trusted%20Architecture%20for%20the%0A%20%20Internet%20of%20AI%20Agents%3F%0AAuthor%3A%20Ramesh%20Raskar%20and%20Pradyumna%20Chari%20and%20Jared%20James%20Grogan%20and%20Mahesh%20Lambe%20and%20Robert%20Lincourt%20and%20Raghu%20Bala%20and%20Aditi%20Joshi%20and%20Abhishek%20Singh%20and%20Ayush%20Chopra%20and%20Rajesh%20Ranjan%20and%20Shailja%20Gupta%20and%20Dimitris%20Stripelis%20and%20Maria%20Gorskikh%20and%20Sichao%20Wang%0AAbstract%3A%20%20%20The%20emerging%20Internet%20of%20AI%20Agents%20challenges%20existing%20web%20infrastructure%0Adesigned%20for%20human-scale%2C%20reactive%20interactions.%20Unlike%20traditional%20web%0Aresources%2C%20autonomous%20AI%20agents%20initiate%20actions%2C%20maintain%20persistent%20state%2C%0Aspawn%20sub-agents%2C%20and%20negotiate%20directly%20with%20peers%3A%20demanding%0Amillisecond-level%20discovery%2C%20instant%20credential%20revocation%2C%20and%20cryptographic%0Abehavioral%20proofs%20that%20exceed%20current%20DNS/PKI%20capabilities.%20This%20paper%20analyzes%0Awhether%20to%20upgrade%20existing%20infrastructure%20or%20implement%20purpose-built%20index%0Aarchitectures%20for%20autonomous%20agents.%20We%20identify%20critical%20failure%20points%3A%20DNS%0Apropagation%20%2824-48%20hours%20vs.%20required%20milliseconds%29%2C%20certificate%20revocation%0Aunable%20to%20scale%20to%20trillions%20of%20entities%2C%20and%20IPv4/IPv6%20addressing%20inadequate%0Afor%20agent-scale%20routing.%20We%20evaluate%20three%20approaches%3A%20%281%29%20Upgrade%20paths%2C%20%282%29%0ASwitch%20options%2C%20%283%29%20Hybrid%20index/registries.%20Drawing%20parallels%20to%0Adialup-to-broadband%20transitions%2C%20we%20find%20that%20agent%20requirements%20constitute%0Aqualitative%2C%20and%20not%20incremental%2C%20changes.%20While%20upgrades%20offer%20compatibility%0Aand%20faster%20deployment%2C%20clean-slate%20solutions%20provide%20better%20performance%20but%0Arequire%20longer%20for%20adoption.%20Our%20analysis%20suggests%20hybrid%20approaches%20will%0Aemerge%2C%20with%20centralized%20indexes%20for%20critical%20agents%20and%20federated%20meshes%20for%0Aspecialized%20use%20cases.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2506.12003v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DUpgrade%2520or%2520Switch%253A%2520Do%2520We%2520Need%2520a%2520Next-Gen%2520Trusted%2520Architecture%2520for%2520the%250A%2520%2520Internet%2520of%2520AI%2520Agents%253F%26entry.906535625%3DRamesh%2520Raskar%2520and%2520Pradyumna%2520Chari%2520and%2520Jared%2520James%2520Grogan%2520and%2520Mahesh%2520Lambe%2520and%2520Robert%2520Lincourt%2520and%2520Raghu%2520Bala%2520and%2520Aditi%2520Joshi%2520and%2520Abhishek%2520Singh%2520and%2520Ayush%2520Chopra%2520and%2520Rajesh%2520Ranjan%2520and%2520Shailja%2520Gupta%2520and%2520Dimitris%2520Stripelis%2520and%2520Maria%2520Gorskikh%2520and%2520Sichao%2520Wang%26entry.1292438233%3D%2520%2520The%2520emerging%2520Internet%2520of%2520AI%2520Agents%2520challenges%2520existing%2520web%2520infrastructure%250Adesigned%2520for%2520human-scale%252C%2520reactive%2520interactions.%2520Unlike%2520traditional%2520web%250Aresources%252C%2520autonomous%2520AI%2520agents%2520initiate%2520actions%252C%2520maintain%2520persistent%2520state%252C%250Aspawn%2520sub-agents%252C%2520and%2520negotiate%2520directly%2520with%2520peers%253A%2520demanding%250Amillisecond-level%2520discovery%252C%2520instant%2520credential%2520revocation%252C%2520and%2520cryptographic%250Abehavioral%2520proofs%2520that%2520exceed%2520current%2520DNS/PKI%2520capabilities.%2520This%2520paper%2520analyzes%250Awhether%2520to%2520upgrade%2520existing%2520infrastructure%2520or%2520implement%2520purpose-built%2520index%250Aarchitectures%2520for%2520autonomous%2520agents.%2520We%2520identify%2520critical%2520failure%2520points%253A%2520DNS%250Apropagation%2520%252824-48%2520hours%2520vs.%2520required%2520milliseconds%2529%252C%2520certificate%2520revocation%250Aunable%2520to%2520scale%2520to%2520trillions%2520of%2520entities%252C%2520and%2520IPv4/IPv6%2520addressing%2520inadequate%250Afor%2520agent-scale%2520routing.%2520We%2520evaluate%2520three%2520approaches%253A%2520%25281%2529%2520Upgrade%2520paths%252C%2520%25282%2529%250ASwitch%2520options%252C%2520%25283%2529%2520Hybrid%2520index/registries.%2520Drawing%2520parallels%2520to%250Adialup-to-broadband%2520transitions%252C%2520we%2520find%2520that%2520agent%2520requirements%2520constitute%250Aqualitative%252C%2520and%2520not%2520incremental%252C%2520changes.%2520While%2520upgrades%2520offer%2520compatibility%250Aand%2520faster%2520deployment%252C%2520clean-slate%2520solutions%2520provide%2520better%2520performance%2520but%250Arequire%2520longer%2520for%2520adoption.%2520Our%2520analysis%2520suggests%2520hybrid%2520approaches%2520will%250Aemerge%252C%2520with%2520centralized%2520indexes%2520for%2520critical%2520agents%2520and%2520federated%2520meshes%2520for%250Aspecialized%2520use%2520cases.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2506.12003v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Upgrade%20or%20Switch%3A%20Do%20We%20Need%20a%20Next-Gen%20Trusted%20Architecture%20for%20the%0A%20%20Internet%20of%20AI%20Agents%3F&entry.906535625=Ramesh%20Raskar%20and%20Pradyumna%20Chari%20and%20Jared%20James%20Grogan%20and%20Mahesh%20Lambe%20and%20Robert%20Lincourt%20and%20Raghu%20Bala%20and%20Aditi%20Joshi%20and%20Abhishek%20Singh%20and%20Ayush%20Chopra%20and%20Rajesh%20Ranjan%20and%20Shailja%20Gupta%20and%20Dimitris%20Stripelis%20and%20Maria%20Gorskikh%20and%20Sichao%20Wang&entry.1292438233=%20%20The%20emerging%20Internet%20of%20AI%20Agents%20challenges%20existing%20web%20infrastructure%0Adesigned%20for%20human-scale%2C%20reactive%20interactions.%20Unlike%20traditional%20web%0Aresources%2C%20autonomous%20AI%20agents%20initiate%20actions%2C%20maintain%20persistent%20state%2C%0Aspawn%20sub-agents%2C%20and%20negotiate%20directly%20with%20peers%3A%20demanding%0Amillisecond-level%20discovery%2C%20instant%20credential%20revocation%2C%20and%20cryptographic%0Abehavioral%20proofs%20that%20exceed%20current%20DNS/PKI%20capabilities.%20This%20paper%20analyzes%0Awhether%20to%20upgrade%20existing%20infrastructure%20or%20implement%20purpose-built%20index%0Aarchitectures%20for%20autonomous%20agents.%20We%20identify%20critical%20failure%20points%3A%20DNS%0Apropagation%20%2824-48%20hours%20vs.%20required%20milliseconds%29%2C%20certificate%20revocation%0Aunable%20to%20scale%20to%20trillions%20of%20entities%2C%20and%20IPv4/IPv6%20addressing%20inadequate%0Afor%20agent-scale%20routing.%20We%20evaluate%20three%20approaches%3A%20%281%29%20Upgrade%20paths%2C%20%282%29%0ASwitch%20options%2C%20%283%29%20Hybrid%20index/registries.%20Drawing%20parallels%20to%0Adialup-to-broadband%20transitions%2C%20we%20find%20that%20agent%20requirements%20constitute%0Aqualitative%2C%20and%20not%20incremental%2C%20changes.%20While%20upgrades%20offer%20compatibility%0Aand%20faster%20deployment%2C%20clean-slate%20solutions%20provide%20better%20performance%20but%0Arequire%20longer%20for%20adoption.%20Our%20analysis%20suggests%20hybrid%20approaches%20will%0Aemerge%2C%20with%20centralized%20indexes%20for%20critical%20agents%20and%20federated%20meshes%20for%0Aspecialized%20use%20cases.%0A&entry.1838667208=http%3A//arxiv.org/abs/2506.12003v2&entry.124074799=Read"},
{"title": "Geo-ORBIT: A Federated Digital Twin Framework for Scene-Adaptive Lane\n  Geometry Detection", "author": "Rei Tamaru and Pei Li and Bin Ran", "abstract": "  Digital Twins (DT) have the potential to transform traffic management and\noperations by creating dynamic, virtual representations of transportation\nsystems that sense conditions, analyze operations, and support decision-making.\nA key component for DT of the transportation system is dynamic roadway geometry\nsensing. However, existing approaches often rely on static maps or costly\nsensors, limiting scalability and adaptability. Additionally, large-scale DTs\nthat collect and analyze data from multiple sources face challenges in privacy,\ncommunication, and computational efficiency. To address these challenges, we\nintroduce Geo-ORBIT (Geometrical Operational Roadway Blueprint with Integrated\nTwin), a unified framework that combines real-time lane detection, DT\nsynchronization, and federated meta-learning. At the core of Geo-ORBIT is\nGeoLane, a lightweight lane detection model that learns lane geometries from\nvehicle trajectory data using roadside cameras. We extend this model through\nMeta-GeoLane, which learns to personalize detection parameters for local\nentities, and FedMeta-GeoLane, a federated learning strategy that ensures\nscalable and privacy-preserving adaptation across roadside deployments. Our\nsystem is integrated with CARLA and SUMO to create a high-fidelity DT that\nrenders highway scenarios and captures traffic flows in real-time. Extensive\nexperiments across diverse urban scenes show that FedMeta-GeoLane consistently\noutperforms baseline and meta-learning approaches, achieving lower geometric\nerror and stronger generalization to unseen locations while drastically\nreducing communication overhead. This work lays the foundation for flexible,\ncontext-aware infrastructure modeling in DTs. The framework is publicly\navailable at https://github.com/raynbowy23/FedMeta-GeoLane.git.\n", "link": "http://arxiv.org/abs/2507.08743v1", "date": "2025-07-11", "relevancy": 2.1103, "topK": [{"title": "RTG-SLAM: Real-time 3D Reconstruction at Scale using Gaussian Splatting", "link": "http://arxiv.org/abs/2404.19706v1", "similarity": 0.5298}, {"title": "Dynamic Gaussians Mesh: Consistent Mesh Reconstruction from Monocular\n  Videos", "link": "http://arxiv.org/abs/2404.12379v2", "similarity": 0.5289}, {"title": "CamCtrl3D: Single-Image Scene Exploration with Precise 3D Camera Control", "link": "http://arxiv.org/abs/2501.06006v1", "similarity": 0.5189}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Geo-ORBIT%3A%20A%20Federated%20Digital%20Twin%20Framework%20for%20Scene-Adaptive%20Lane%0A%20%20Geometry%20Detection&body=Title%3A%20Geo-ORBIT%3A%20A%20Federated%20Digital%20Twin%20Framework%20for%20Scene-Adaptive%20Lane%0A%20%20Geometry%20Detection%0AAuthor%3A%20Rei%20Tamaru%20and%20Pei%20Li%20and%20Bin%20Ran%0AAbstract%3A%20%20%20Digital%20Twins%20%28DT%29%20have%20the%20potential%20to%20transform%20traffic%20management%20and%0Aoperations%20by%20creating%20dynamic%2C%20virtual%20representations%20of%20transportation%0Asystems%20that%20sense%20conditions%2C%20analyze%20operations%2C%20and%20support%20decision-making.%0AA%20key%20component%20for%20DT%20of%20the%20transportation%20system%20is%20dynamic%20roadway%20geometry%0Asensing.%20However%2C%20existing%20approaches%20often%20rely%20on%20static%20maps%20or%20costly%0Asensors%2C%20limiting%20scalability%20and%20adaptability.%20Additionally%2C%20large-scale%20DTs%0Athat%20collect%20and%20analyze%20data%20from%20multiple%20sources%20face%20challenges%20in%20privacy%2C%0Acommunication%2C%20and%20computational%20efficiency.%20To%20address%20these%20challenges%2C%20we%0Aintroduce%20Geo-ORBIT%20%28Geometrical%20Operational%20Roadway%20Blueprint%20with%20Integrated%0ATwin%29%2C%20a%20unified%20framework%20that%20combines%20real-time%20lane%20detection%2C%20DT%0Asynchronization%2C%20and%20federated%20meta-learning.%20At%20the%20core%20of%20Geo-ORBIT%20is%0AGeoLane%2C%20a%20lightweight%20lane%20detection%20model%20that%20learns%20lane%20geometries%20from%0Avehicle%20trajectory%20data%20using%20roadside%20cameras.%20We%20extend%20this%20model%20through%0AMeta-GeoLane%2C%20which%20learns%20to%20personalize%20detection%20parameters%20for%20local%0Aentities%2C%20and%20FedMeta-GeoLane%2C%20a%20federated%20learning%20strategy%20that%20ensures%0Ascalable%20and%20privacy-preserving%20adaptation%20across%20roadside%20deployments.%20Our%0Asystem%20is%20integrated%20with%20CARLA%20and%20SUMO%20to%20create%20a%20high-fidelity%20DT%20that%0Arenders%20highway%20scenarios%20and%20captures%20traffic%20flows%20in%20real-time.%20Extensive%0Aexperiments%20across%20diverse%20urban%20scenes%20show%20that%20FedMeta-GeoLane%20consistently%0Aoutperforms%20baseline%20and%20meta-learning%20approaches%2C%20achieving%20lower%20geometric%0Aerror%20and%20stronger%20generalization%20to%20unseen%20locations%20while%20drastically%0Areducing%20communication%20overhead.%20This%20work%20lays%20the%20foundation%20for%20flexible%2C%0Acontext-aware%20infrastructure%20modeling%20in%20DTs.%20The%20framework%20is%20publicly%0Aavailable%20at%20https%3A//github.com/raynbowy23/FedMeta-GeoLane.git.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2507.08743v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DGeo-ORBIT%253A%2520A%2520Federated%2520Digital%2520Twin%2520Framework%2520for%2520Scene-Adaptive%2520Lane%250A%2520%2520Geometry%2520Detection%26entry.906535625%3DRei%2520Tamaru%2520and%2520Pei%2520Li%2520and%2520Bin%2520Ran%26entry.1292438233%3D%2520%2520Digital%2520Twins%2520%2528DT%2529%2520have%2520the%2520potential%2520to%2520transform%2520traffic%2520management%2520and%250Aoperations%2520by%2520creating%2520dynamic%252C%2520virtual%2520representations%2520of%2520transportation%250Asystems%2520that%2520sense%2520conditions%252C%2520analyze%2520operations%252C%2520and%2520support%2520decision-making.%250AA%2520key%2520component%2520for%2520DT%2520of%2520the%2520transportation%2520system%2520is%2520dynamic%2520roadway%2520geometry%250Asensing.%2520However%252C%2520existing%2520approaches%2520often%2520rely%2520on%2520static%2520maps%2520or%2520costly%250Asensors%252C%2520limiting%2520scalability%2520and%2520adaptability.%2520Additionally%252C%2520large-scale%2520DTs%250Athat%2520collect%2520and%2520analyze%2520data%2520from%2520multiple%2520sources%2520face%2520challenges%2520in%2520privacy%252C%250Acommunication%252C%2520and%2520computational%2520efficiency.%2520To%2520address%2520these%2520challenges%252C%2520we%250Aintroduce%2520Geo-ORBIT%2520%2528Geometrical%2520Operational%2520Roadway%2520Blueprint%2520with%2520Integrated%250ATwin%2529%252C%2520a%2520unified%2520framework%2520that%2520combines%2520real-time%2520lane%2520detection%252C%2520DT%250Asynchronization%252C%2520and%2520federated%2520meta-learning.%2520At%2520the%2520core%2520of%2520Geo-ORBIT%2520is%250AGeoLane%252C%2520a%2520lightweight%2520lane%2520detection%2520model%2520that%2520learns%2520lane%2520geometries%2520from%250Avehicle%2520trajectory%2520data%2520using%2520roadside%2520cameras.%2520We%2520extend%2520this%2520model%2520through%250AMeta-GeoLane%252C%2520which%2520learns%2520to%2520personalize%2520detection%2520parameters%2520for%2520local%250Aentities%252C%2520and%2520FedMeta-GeoLane%252C%2520a%2520federated%2520learning%2520strategy%2520that%2520ensures%250Ascalable%2520and%2520privacy-preserving%2520adaptation%2520across%2520roadside%2520deployments.%2520Our%250Asystem%2520is%2520integrated%2520with%2520CARLA%2520and%2520SUMO%2520to%2520create%2520a%2520high-fidelity%2520DT%2520that%250Arenders%2520highway%2520scenarios%2520and%2520captures%2520traffic%2520flows%2520in%2520real-time.%2520Extensive%250Aexperiments%2520across%2520diverse%2520urban%2520scenes%2520show%2520that%2520FedMeta-GeoLane%2520consistently%250Aoutperforms%2520baseline%2520and%2520meta-learning%2520approaches%252C%2520achieving%2520lower%2520geometric%250Aerror%2520and%2520stronger%2520generalization%2520to%2520unseen%2520locations%2520while%2520drastically%250Areducing%2520communication%2520overhead.%2520This%2520work%2520lays%2520the%2520foundation%2520for%2520flexible%252C%250Acontext-aware%2520infrastructure%2520modeling%2520in%2520DTs.%2520The%2520framework%2520is%2520publicly%250Aavailable%2520at%2520https%253A//github.com/raynbowy23/FedMeta-GeoLane.git.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2507.08743v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Geo-ORBIT%3A%20A%20Federated%20Digital%20Twin%20Framework%20for%20Scene-Adaptive%20Lane%0A%20%20Geometry%20Detection&entry.906535625=Rei%20Tamaru%20and%20Pei%20Li%20and%20Bin%20Ran&entry.1292438233=%20%20Digital%20Twins%20%28DT%29%20have%20the%20potential%20to%20transform%20traffic%20management%20and%0Aoperations%20by%20creating%20dynamic%2C%20virtual%20representations%20of%20transportation%0Asystems%20that%20sense%20conditions%2C%20analyze%20operations%2C%20and%20support%20decision-making.%0AA%20key%20component%20for%20DT%20of%20the%20transportation%20system%20is%20dynamic%20roadway%20geometry%0Asensing.%20However%2C%20existing%20approaches%20often%20rely%20on%20static%20maps%20or%20costly%0Asensors%2C%20limiting%20scalability%20and%20adaptability.%20Additionally%2C%20large-scale%20DTs%0Athat%20collect%20and%20analyze%20data%20from%20multiple%20sources%20face%20challenges%20in%20privacy%2C%0Acommunication%2C%20and%20computational%20efficiency.%20To%20address%20these%20challenges%2C%20we%0Aintroduce%20Geo-ORBIT%20%28Geometrical%20Operational%20Roadway%20Blueprint%20with%20Integrated%0ATwin%29%2C%20a%20unified%20framework%20that%20combines%20real-time%20lane%20detection%2C%20DT%0Asynchronization%2C%20and%20federated%20meta-learning.%20At%20the%20core%20of%20Geo-ORBIT%20is%0AGeoLane%2C%20a%20lightweight%20lane%20detection%20model%20that%20learns%20lane%20geometries%20from%0Avehicle%20trajectory%20data%20using%20roadside%20cameras.%20We%20extend%20this%20model%20through%0AMeta-GeoLane%2C%20which%20learns%20to%20personalize%20detection%20parameters%20for%20local%0Aentities%2C%20and%20FedMeta-GeoLane%2C%20a%20federated%20learning%20strategy%20that%20ensures%0Ascalable%20and%20privacy-preserving%20adaptation%20across%20roadside%20deployments.%20Our%0Asystem%20is%20integrated%20with%20CARLA%20and%20SUMO%20to%20create%20a%20high-fidelity%20DT%20that%0Arenders%20highway%20scenarios%20and%20captures%20traffic%20flows%20in%20real-time.%20Extensive%0Aexperiments%20across%20diverse%20urban%20scenes%20show%20that%20FedMeta-GeoLane%20consistently%0Aoutperforms%20baseline%20and%20meta-learning%20approaches%2C%20achieving%20lower%20geometric%0Aerror%20and%20stronger%20generalization%20to%20unseen%20locations%20while%20drastically%0Areducing%20communication%20overhead.%20This%20work%20lays%20the%20foundation%20for%20flexible%2C%0Acontext-aware%20infrastructure%20modeling%20in%20DTs.%20The%20framework%20is%20publicly%0Aavailable%20at%20https%3A//github.com/raynbowy23/FedMeta-GeoLane.git.%0A&entry.1838667208=http%3A//arxiv.org/abs/2507.08743v1&entry.124074799=Read"},
{"title": "Scaling Attention to Very Long Sequences in Linear Time with\n  Wavelet-Enhanced Random Spectral Attention (WERSA)", "author": "Vincenzo Dentamaro", "abstract": "  Transformer models are computationally costly on long sequences since regular\nattention has quadratic $O(n^2)$ time complexity. We introduce Wavelet-Enhanced\nRandom Spectral Attention (WERSA), a novel mechanism of linear $O(n)$ time\ncomplexity that is pivotal to enable successful long-sequence processing\nwithout the performance trade-off. WERSA merges content-adaptive random\nspectral features together with multi-resolution Haar wavelets and learnable\nparameters to selectively attend to informative scales of data while preserving\nlinear efficiency.\n  Large-scale comparisons \\textbf{on single GPU} and across various benchmarks\n(vision, NLP, hierarchical reasoning) and various attention mechanisms (like\nMultiheaded Attention, Flash-Attention-2, FNet, Linformer, Performer,\nWaveformer), reveal uniform advantages of WERSA. It achieves best accuracy in\nall tests. On ArXiv classification, WERSA improves accuracy over vanilla\nattention by 1.2\\% (86.2\\% vs 85.0\\%) while cutting training time by 81\\% (296s\nvs 1554s) and FLOPS by 73.4\\% (26.2G vs 98.4G). Significantly, WERSA excels\nwhere vanilla and FlashAttention-2 fail: on ArXiv-128k's extremely lengthy\nsequences, it achieves best accuracy (79.1\\%) and AUC (0.979) among viable\nmethods, operating on data that gives Out-Of-Memory errors to quadratic methods\nwhile being \\textbf{twice as fast} as Waveformer, its next-best competitor.\n  By significantly reducing computational loads without compromising accuracy,\nWERSA makes possible more practical, more affordable, long-context models, in\nparticular on low-resource hardware, for more sustainable and more scalable AI\ndevelopment.\n", "link": "http://arxiv.org/abs/2507.08637v1", "date": "2025-07-11", "relevancy": 2.1022, "topK": [{"title": "Scaling Rectified Flow Transformers for High-Resolution Image Synthesis", "link": "http://arxiv.org/abs/2403.03206v1", "similarity": 0.5724}, {"title": "ToDo: Token Downsampling for Efficient Generation of High-Resolution\n  Images", "link": "http://arxiv.org/abs/2402.13573v2", "similarity": 0.5248}, {"title": "Breadcrumbs: Adversarial class-balanced sampling for long-tailed recognition", "link": "https://link.springer.com/chapter/10.1007/978-3-031-20053-3_37", "similarity": 0.5075}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Scaling%20Attention%20to%20Very%20Long%20Sequences%20in%20Linear%20Time%20with%0A%20%20Wavelet-Enhanced%20Random%20Spectral%20Attention%20%28WERSA%29&body=Title%3A%20Scaling%20Attention%20to%20Very%20Long%20Sequences%20in%20Linear%20Time%20with%0A%20%20Wavelet-Enhanced%20Random%20Spectral%20Attention%20%28WERSA%29%0AAuthor%3A%20Vincenzo%20Dentamaro%0AAbstract%3A%20%20%20Transformer%20models%20are%20computationally%20costly%20on%20long%20sequences%20since%20regular%0Aattention%20has%20quadratic%20%24O%28n%5E2%29%24%20time%20complexity.%20We%20introduce%20Wavelet-Enhanced%0ARandom%20Spectral%20Attention%20%28WERSA%29%2C%20a%20novel%20mechanism%20of%20linear%20%24O%28n%29%24%20time%0Acomplexity%20that%20is%20pivotal%20to%20enable%20successful%20long-sequence%20processing%0Awithout%20the%20performance%20trade-off.%20WERSA%20merges%20content-adaptive%20random%0Aspectral%20features%20together%20with%20multi-resolution%20Haar%20wavelets%20and%20learnable%0Aparameters%20to%20selectively%20attend%20to%20informative%20scales%20of%20data%20while%20preserving%0Alinear%20efficiency.%0A%20%20Large-scale%20comparisons%20%5Ctextbf%7Bon%20single%20GPU%7D%20and%20across%20various%20benchmarks%0A%28vision%2C%20NLP%2C%20hierarchical%20reasoning%29%20and%20various%20attention%20mechanisms%20%28like%0AMultiheaded%20Attention%2C%20Flash-Attention-2%2C%20FNet%2C%20Linformer%2C%20Performer%2C%0AWaveformer%29%2C%20reveal%20uniform%20advantages%20of%20WERSA.%20It%20achieves%20best%20accuracy%20in%0Aall%20tests.%20On%20ArXiv%20classification%2C%20WERSA%20improves%20accuracy%20over%20vanilla%0Aattention%20by%201.2%5C%25%20%2886.2%5C%25%20vs%2085.0%5C%25%29%20while%20cutting%20training%20time%20by%2081%5C%25%20%28296s%0Avs%201554s%29%20and%20FLOPS%20by%2073.4%5C%25%20%2826.2G%20vs%2098.4G%29.%20Significantly%2C%20WERSA%20excels%0Awhere%20vanilla%20and%20FlashAttention-2%20fail%3A%20on%20ArXiv-128k%27s%20extremely%20lengthy%0Asequences%2C%20it%20achieves%20best%20accuracy%20%2879.1%5C%25%29%20and%20AUC%20%280.979%29%20among%20viable%0Amethods%2C%20operating%20on%20data%20that%20gives%20Out-Of-Memory%20errors%20to%20quadratic%20methods%0Awhile%20being%20%5Ctextbf%7Btwice%20as%20fast%7D%20as%20Waveformer%2C%20its%20next-best%20competitor.%0A%20%20By%20significantly%20reducing%20computational%20loads%20without%20compromising%20accuracy%2C%0AWERSA%20makes%20possible%20more%20practical%2C%20more%20affordable%2C%20long-context%20models%2C%20in%0Aparticular%20on%20low-resource%20hardware%2C%20for%20more%20sustainable%20and%20more%20scalable%20AI%0Adevelopment.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2507.08637v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DScaling%2520Attention%2520to%2520Very%2520Long%2520Sequences%2520in%2520Linear%2520Time%2520with%250A%2520%2520Wavelet-Enhanced%2520Random%2520Spectral%2520Attention%2520%2528WERSA%2529%26entry.906535625%3DVincenzo%2520Dentamaro%26entry.1292438233%3D%2520%2520Transformer%2520models%2520are%2520computationally%2520costly%2520on%2520long%2520sequences%2520since%2520regular%250Aattention%2520has%2520quadratic%2520%2524O%2528n%255E2%2529%2524%2520time%2520complexity.%2520We%2520introduce%2520Wavelet-Enhanced%250ARandom%2520Spectral%2520Attention%2520%2528WERSA%2529%252C%2520a%2520novel%2520mechanism%2520of%2520linear%2520%2524O%2528n%2529%2524%2520time%250Acomplexity%2520that%2520is%2520pivotal%2520to%2520enable%2520successful%2520long-sequence%2520processing%250Awithout%2520the%2520performance%2520trade-off.%2520WERSA%2520merges%2520content-adaptive%2520random%250Aspectral%2520features%2520together%2520with%2520multi-resolution%2520Haar%2520wavelets%2520and%2520learnable%250Aparameters%2520to%2520selectively%2520attend%2520to%2520informative%2520scales%2520of%2520data%2520while%2520preserving%250Alinear%2520efficiency.%250A%2520%2520Large-scale%2520comparisons%2520%255Ctextbf%257Bon%2520single%2520GPU%257D%2520and%2520across%2520various%2520benchmarks%250A%2528vision%252C%2520NLP%252C%2520hierarchical%2520reasoning%2529%2520and%2520various%2520attention%2520mechanisms%2520%2528like%250AMultiheaded%2520Attention%252C%2520Flash-Attention-2%252C%2520FNet%252C%2520Linformer%252C%2520Performer%252C%250AWaveformer%2529%252C%2520reveal%2520uniform%2520advantages%2520of%2520WERSA.%2520It%2520achieves%2520best%2520accuracy%2520in%250Aall%2520tests.%2520On%2520ArXiv%2520classification%252C%2520WERSA%2520improves%2520accuracy%2520over%2520vanilla%250Aattention%2520by%25201.2%255C%2525%2520%252886.2%255C%2525%2520vs%252085.0%255C%2525%2529%2520while%2520cutting%2520training%2520time%2520by%252081%255C%2525%2520%2528296s%250Avs%25201554s%2529%2520and%2520FLOPS%2520by%252073.4%255C%2525%2520%252826.2G%2520vs%252098.4G%2529.%2520Significantly%252C%2520WERSA%2520excels%250Awhere%2520vanilla%2520and%2520FlashAttention-2%2520fail%253A%2520on%2520ArXiv-128k%2527s%2520extremely%2520lengthy%250Asequences%252C%2520it%2520achieves%2520best%2520accuracy%2520%252879.1%255C%2525%2529%2520and%2520AUC%2520%25280.979%2529%2520among%2520viable%250Amethods%252C%2520operating%2520on%2520data%2520that%2520gives%2520Out-Of-Memory%2520errors%2520to%2520quadratic%2520methods%250Awhile%2520being%2520%255Ctextbf%257Btwice%2520as%2520fast%257D%2520as%2520Waveformer%252C%2520its%2520next-best%2520competitor.%250A%2520%2520By%2520significantly%2520reducing%2520computational%2520loads%2520without%2520compromising%2520accuracy%252C%250AWERSA%2520makes%2520possible%2520more%2520practical%252C%2520more%2520affordable%252C%2520long-context%2520models%252C%2520in%250Aparticular%2520on%2520low-resource%2520hardware%252C%2520for%2520more%2520sustainable%2520and%2520more%2520scalable%2520AI%250Adevelopment.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2507.08637v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Scaling%20Attention%20to%20Very%20Long%20Sequences%20in%20Linear%20Time%20with%0A%20%20Wavelet-Enhanced%20Random%20Spectral%20Attention%20%28WERSA%29&entry.906535625=Vincenzo%20Dentamaro&entry.1292438233=%20%20Transformer%20models%20are%20computationally%20costly%20on%20long%20sequences%20since%20regular%0Aattention%20has%20quadratic%20%24O%28n%5E2%29%24%20time%20complexity.%20We%20introduce%20Wavelet-Enhanced%0ARandom%20Spectral%20Attention%20%28WERSA%29%2C%20a%20novel%20mechanism%20of%20linear%20%24O%28n%29%24%20time%0Acomplexity%20that%20is%20pivotal%20to%20enable%20successful%20long-sequence%20processing%0Awithout%20the%20performance%20trade-off.%20WERSA%20merges%20content-adaptive%20random%0Aspectral%20features%20together%20with%20multi-resolution%20Haar%20wavelets%20and%20learnable%0Aparameters%20to%20selectively%20attend%20to%20informative%20scales%20of%20data%20while%20preserving%0Alinear%20efficiency.%0A%20%20Large-scale%20comparisons%20%5Ctextbf%7Bon%20single%20GPU%7D%20and%20across%20various%20benchmarks%0A%28vision%2C%20NLP%2C%20hierarchical%20reasoning%29%20and%20various%20attention%20mechanisms%20%28like%0AMultiheaded%20Attention%2C%20Flash-Attention-2%2C%20FNet%2C%20Linformer%2C%20Performer%2C%0AWaveformer%29%2C%20reveal%20uniform%20advantages%20of%20WERSA.%20It%20achieves%20best%20accuracy%20in%0Aall%20tests.%20On%20ArXiv%20classification%2C%20WERSA%20improves%20accuracy%20over%20vanilla%0Aattention%20by%201.2%5C%25%20%2886.2%5C%25%20vs%2085.0%5C%25%29%20while%20cutting%20training%20time%20by%2081%5C%25%20%28296s%0Avs%201554s%29%20and%20FLOPS%20by%2073.4%5C%25%20%2826.2G%20vs%2098.4G%29.%20Significantly%2C%20WERSA%20excels%0Awhere%20vanilla%20and%20FlashAttention-2%20fail%3A%20on%20ArXiv-128k%27s%20extremely%20lengthy%0Asequences%2C%20it%20achieves%20best%20accuracy%20%2879.1%5C%25%29%20and%20AUC%20%280.979%29%20among%20viable%0Amethods%2C%20operating%20on%20data%20that%20gives%20Out-Of-Memory%20errors%20to%20quadratic%20methods%0Awhile%20being%20%5Ctextbf%7Btwice%20as%20fast%7D%20as%20Waveformer%2C%20its%20next-best%20competitor.%0A%20%20By%20significantly%20reducing%20computational%20loads%20without%20compromising%20accuracy%2C%0AWERSA%20makes%20possible%20more%20practical%2C%20more%20affordable%2C%20long-context%20models%2C%20in%0Aparticular%20on%20low-resource%20hardware%2C%20for%20more%20sustainable%20and%20more%20scalable%20AI%0Adevelopment.%0A&entry.1838667208=http%3A//arxiv.org/abs/2507.08637v1&entry.124074799=Read"},
{"title": "GoalNet: Goal Areas Oriented Pedestrian Trajectory Prediction", "author": "Amar Fadillah and Ching-Lin Lee and Zhi-Xuan Wang and Kuan-Ting Lai", "abstract": "  Predicting the future trajectories of pedestrians on the road is an important\ntask for autonomous driving. The pedestrian trajectory prediction is affected\nby scene paths, pedestrian's intentions and decision-making, which is a\nmulti-modal problem. Most recent studies use past trajectories to predict a\nvariety of potential future trajectory distributions, which do not account for\nthe scene context and pedestrian targets. Instead of predicting the future\ntrajectory directly, we propose to use scene context and observed trajectory to\npredict the goal points first, and then reuse the goal points to predict the\nfuture trajectories. By leveraging the information from scene context and\nobserved trajectory, the uncertainty can be limited to a few target areas,\nwhich represent the \"goals\" of the pedestrians. In this paper, we propose\nGoalNet, a new trajectory prediction neural network based on the goal areas of\na pedestrian. Our network can predict both pedestrian's trajectories and\nbounding boxes. The overall model is efficient and modular, and its outputs can\nbe changed according to the usage scenario. Experimental results show that\nGoalNet significantly improves the previous state-of-the-art performance by\n48.7% on the JAAD and 40.8% on the PIE dataset.\n", "link": "http://arxiv.org/abs/2402.19002v2", "date": "2025-07-11", "relevancy": 2.0895, "topK": [{"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.5671}, {"title": "Boosted dynamic neural networks", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/26302", "similarity": 0.5233}, {"title": "DDM-NET: End-to-end learning of keypoint feature Detection, Description and Matching for 3D localization", "link": "https://arxiv.org/abs/2212.04575", "similarity": 0.5036}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20GoalNet%3A%20Goal%20Areas%20Oriented%20Pedestrian%20Trajectory%20Prediction&body=Title%3A%20GoalNet%3A%20Goal%20Areas%20Oriented%20Pedestrian%20Trajectory%20Prediction%0AAuthor%3A%20Amar%20Fadillah%20and%20Ching-Lin%20Lee%20and%20Zhi-Xuan%20Wang%20and%20Kuan-Ting%20Lai%0AAbstract%3A%20%20%20Predicting%20the%20future%20trajectories%20of%20pedestrians%20on%20the%20road%20is%20an%20important%0Atask%20for%20autonomous%20driving.%20The%20pedestrian%20trajectory%20prediction%20is%20affected%0Aby%20scene%20paths%2C%20pedestrian%27s%20intentions%20and%20decision-making%2C%20which%20is%20a%0Amulti-modal%20problem.%20Most%20recent%20studies%20use%20past%20trajectories%20to%20predict%20a%0Avariety%20of%20potential%20future%20trajectory%20distributions%2C%20which%20do%20not%20account%20for%0Athe%20scene%20context%20and%20pedestrian%20targets.%20Instead%20of%20predicting%20the%20future%0Atrajectory%20directly%2C%20we%20propose%20to%20use%20scene%20context%20and%20observed%20trajectory%20to%0Apredict%20the%20goal%20points%20first%2C%20and%20then%20reuse%20the%20goal%20points%20to%20predict%20the%0Afuture%20trajectories.%20By%20leveraging%20the%20information%20from%20scene%20context%20and%0Aobserved%20trajectory%2C%20the%20uncertainty%20can%20be%20limited%20to%20a%20few%20target%20areas%2C%0Awhich%20represent%20the%20%22goals%22%20of%20the%20pedestrians.%20In%20this%20paper%2C%20we%20propose%0AGoalNet%2C%20a%20new%20trajectory%20prediction%20neural%20network%20based%20on%20the%20goal%20areas%20of%0Aa%20pedestrian.%20Our%20network%20can%20predict%20both%20pedestrian%27s%20trajectories%20and%0Abounding%20boxes.%20The%20overall%20model%20is%20efficient%20and%20modular%2C%20and%20its%20outputs%20can%0Abe%20changed%20according%20to%20the%20usage%20scenario.%20Experimental%20results%20show%20that%0AGoalNet%20significantly%20improves%20the%20previous%20state-of-the-art%20performance%20by%0A48.7%25%20on%20the%20JAAD%20and%2040.8%25%20on%20the%20PIE%20dataset.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2402.19002v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DGoalNet%253A%2520Goal%2520Areas%2520Oriented%2520Pedestrian%2520Trajectory%2520Prediction%26entry.906535625%3DAmar%2520Fadillah%2520and%2520Ching-Lin%2520Lee%2520and%2520Zhi-Xuan%2520Wang%2520and%2520Kuan-Ting%2520Lai%26entry.1292438233%3D%2520%2520Predicting%2520the%2520future%2520trajectories%2520of%2520pedestrians%2520on%2520the%2520road%2520is%2520an%2520important%250Atask%2520for%2520autonomous%2520driving.%2520The%2520pedestrian%2520trajectory%2520prediction%2520is%2520affected%250Aby%2520scene%2520paths%252C%2520pedestrian%2527s%2520intentions%2520and%2520decision-making%252C%2520which%2520is%2520a%250Amulti-modal%2520problem.%2520Most%2520recent%2520studies%2520use%2520past%2520trajectories%2520to%2520predict%2520a%250Avariety%2520of%2520potential%2520future%2520trajectory%2520distributions%252C%2520which%2520do%2520not%2520account%2520for%250Athe%2520scene%2520context%2520and%2520pedestrian%2520targets.%2520Instead%2520of%2520predicting%2520the%2520future%250Atrajectory%2520directly%252C%2520we%2520propose%2520to%2520use%2520scene%2520context%2520and%2520observed%2520trajectory%2520to%250Apredict%2520the%2520goal%2520points%2520first%252C%2520and%2520then%2520reuse%2520the%2520goal%2520points%2520to%2520predict%2520the%250Afuture%2520trajectories.%2520By%2520leveraging%2520the%2520information%2520from%2520scene%2520context%2520and%250Aobserved%2520trajectory%252C%2520the%2520uncertainty%2520can%2520be%2520limited%2520to%2520a%2520few%2520target%2520areas%252C%250Awhich%2520represent%2520the%2520%2522goals%2522%2520of%2520the%2520pedestrians.%2520In%2520this%2520paper%252C%2520we%2520propose%250AGoalNet%252C%2520a%2520new%2520trajectory%2520prediction%2520neural%2520network%2520based%2520on%2520the%2520goal%2520areas%2520of%250Aa%2520pedestrian.%2520Our%2520network%2520can%2520predict%2520both%2520pedestrian%2527s%2520trajectories%2520and%250Abounding%2520boxes.%2520The%2520overall%2520model%2520is%2520efficient%2520and%2520modular%252C%2520and%2520its%2520outputs%2520can%250Abe%2520changed%2520according%2520to%2520the%2520usage%2520scenario.%2520Experimental%2520results%2520show%2520that%250AGoalNet%2520significantly%2520improves%2520the%2520previous%2520state-of-the-art%2520performance%2520by%250A48.7%2525%2520on%2520the%2520JAAD%2520and%252040.8%2525%2520on%2520the%2520PIE%2520dataset.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2402.19002v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=GoalNet%3A%20Goal%20Areas%20Oriented%20Pedestrian%20Trajectory%20Prediction&entry.906535625=Amar%20Fadillah%20and%20Ching-Lin%20Lee%20and%20Zhi-Xuan%20Wang%20and%20Kuan-Ting%20Lai&entry.1292438233=%20%20Predicting%20the%20future%20trajectories%20of%20pedestrians%20on%20the%20road%20is%20an%20important%0Atask%20for%20autonomous%20driving.%20The%20pedestrian%20trajectory%20prediction%20is%20affected%0Aby%20scene%20paths%2C%20pedestrian%27s%20intentions%20and%20decision-making%2C%20which%20is%20a%0Amulti-modal%20problem.%20Most%20recent%20studies%20use%20past%20trajectories%20to%20predict%20a%0Avariety%20of%20potential%20future%20trajectory%20distributions%2C%20which%20do%20not%20account%20for%0Athe%20scene%20context%20and%20pedestrian%20targets.%20Instead%20of%20predicting%20the%20future%0Atrajectory%20directly%2C%20we%20propose%20to%20use%20scene%20context%20and%20observed%20trajectory%20to%0Apredict%20the%20goal%20points%20first%2C%20and%20then%20reuse%20the%20goal%20points%20to%20predict%20the%0Afuture%20trajectories.%20By%20leveraging%20the%20information%20from%20scene%20context%20and%0Aobserved%20trajectory%2C%20the%20uncertainty%20can%20be%20limited%20to%20a%20few%20target%20areas%2C%0Awhich%20represent%20the%20%22goals%22%20of%20the%20pedestrians.%20In%20this%20paper%2C%20we%20propose%0AGoalNet%2C%20a%20new%20trajectory%20prediction%20neural%20network%20based%20on%20the%20goal%20areas%20of%0Aa%20pedestrian.%20Our%20network%20can%20predict%20both%20pedestrian%27s%20trajectories%20and%0Abounding%20boxes.%20The%20overall%20model%20is%20efficient%20and%20modular%2C%20and%20its%20outputs%20can%0Abe%20changed%20according%20to%20the%20usage%20scenario.%20Experimental%20results%20show%20that%0AGoalNet%20significantly%20improves%20the%20previous%20state-of-the-art%20performance%20by%0A48.7%25%20on%20the%20JAAD%20and%2040.8%25%20on%20the%20PIE%20dataset.%0A&entry.1838667208=http%3A//arxiv.org/abs/2402.19002v2&entry.124074799=Read"},
{"title": "Quantifying Context Bias in Domain Adaptation for Object Detection", "author": "Hojun Son and Asma Almutairi and Arpan Kusari", "abstract": "  Domain adaptation for object detection (DAOD) has become essential to counter\nperformance degradation caused by distribution shifts between training and\ndeployment domains. However, a critical factor influencing DAOD - context bias\nresulting from learned foreground-background (FG-BG) associations - has\nremained underexplored. We address three key questions regarding FG BG\nassociations in object detection: are FG-BG associations encoded during the\ntraining, is there a causal relationship between FG-BG associations and\ndetection performance, and is there an effect of FG-BG association on DAOD. To\nexamine how models capture FG BG associations, we analyze class-wise and\nfeature-wise performance degradation using background masking and feature\nperturbation, measured via change in accuracies (defined as drop rate). To\nexplore the causal role of FG-BG associations, we apply do-calculus on FG-BG\npairs guided by class activation mapping (CAM). To quantify the causal\ninfluence of FG-BG associations across domains, we propose a novel metric -\ndomain association gradient - defined as the ratio of drop rate to maximum mean\ndiscrepancy (MMD). Through systematic experiments involving background masking,\nfeature-level perturbations, and CAM, we reveal that convolution-based object\ndetection models encode FG-BG associations. Our results demonstrate that\ncontext bias not only exists but causally undermines the generalization\ncapabilities of object detection models across domains. Furthermore, we\nvalidate these findings across multiple models and datasets, including\nstate-of-the-art architectures such as ALDI++. This study highlights the\nnecessity of addressing context bias explicitly in DAOD frameworks, providing\ninsights that pave the way for developing more robust and generalizable object\ndetection systems.\n", "link": "http://arxiv.org/abs/2409.14679v3", "date": "2025-07-11", "relevancy": 2.0809, "topK": [{"title": "Thinking Outside the BBox: Unconstrained Generative Object Compositing", "link": "http://arxiv.org/abs/2409.04559v2", "similarity": 0.542}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5159}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5159}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Quantifying%20Context%20Bias%20in%20Domain%20Adaptation%20for%20Object%20Detection&body=Title%3A%20Quantifying%20Context%20Bias%20in%20Domain%20Adaptation%20for%20Object%20Detection%0AAuthor%3A%20Hojun%20Son%20and%20Asma%20Almutairi%20and%20Arpan%20Kusari%0AAbstract%3A%20%20%20Domain%20adaptation%20for%20object%20detection%20%28DAOD%29%20has%20become%20essential%20to%20counter%0Aperformance%20degradation%20caused%20by%20distribution%20shifts%20between%20training%20and%0Adeployment%20domains.%20However%2C%20a%20critical%20factor%20influencing%20DAOD%20-%20context%20bias%0Aresulting%20from%20learned%20foreground-background%20%28FG-BG%29%20associations%20-%20has%0Aremained%20underexplored.%20We%20address%20three%20key%20questions%20regarding%20FG%20BG%0Aassociations%20in%20object%20detection%3A%20are%20FG-BG%20associations%20encoded%20during%20the%0Atraining%2C%20is%20there%20a%20causal%20relationship%20between%20FG-BG%20associations%20and%0Adetection%20performance%2C%20and%20is%20there%20an%20effect%20of%20FG-BG%20association%20on%20DAOD.%20To%0Aexamine%20how%20models%20capture%20FG%20BG%20associations%2C%20we%20analyze%20class-wise%20and%0Afeature-wise%20performance%20degradation%20using%20background%20masking%20and%20feature%0Aperturbation%2C%20measured%20via%20change%20in%20accuracies%20%28defined%20as%20drop%20rate%29.%20To%0Aexplore%20the%20causal%20role%20of%20FG-BG%20associations%2C%20we%20apply%20do-calculus%20on%20FG-BG%0Apairs%20guided%20by%20class%20activation%20mapping%20%28CAM%29.%20To%20quantify%20the%20causal%0Ainfluence%20of%20FG-BG%20associations%20across%20domains%2C%20we%20propose%20a%20novel%20metric%20-%0Adomain%20association%20gradient%20-%20defined%20as%20the%20ratio%20of%20drop%20rate%20to%20maximum%20mean%0Adiscrepancy%20%28MMD%29.%20Through%20systematic%20experiments%20involving%20background%20masking%2C%0Afeature-level%20perturbations%2C%20and%20CAM%2C%20we%20reveal%20that%20convolution-based%20object%0Adetection%20models%20encode%20FG-BG%20associations.%20Our%20results%20demonstrate%20that%0Acontext%20bias%20not%20only%20exists%20but%20causally%20undermines%20the%20generalization%0Acapabilities%20of%20object%20detection%20models%20across%20domains.%20Furthermore%2C%20we%0Avalidate%20these%20findings%20across%20multiple%20models%20and%20datasets%2C%20including%0Astate-of-the-art%20architectures%20such%20as%20ALDI%2B%2B.%20This%20study%20highlights%20the%0Anecessity%20of%20addressing%20context%20bias%20explicitly%20in%20DAOD%20frameworks%2C%20providing%0Ainsights%20that%20pave%20the%20way%20for%20developing%20more%20robust%20and%20generalizable%20object%0Adetection%20systems.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2409.14679v3%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DQuantifying%2520Context%2520Bias%2520in%2520Domain%2520Adaptation%2520for%2520Object%2520Detection%26entry.906535625%3DHojun%2520Son%2520and%2520Asma%2520Almutairi%2520and%2520Arpan%2520Kusari%26entry.1292438233%3D%2520%2520Domain%2520adaptation%2520for%2520object%2520detection%2520%2528DAOD%2529%2520has%2520become%2520essential%2520to%2520counter%250Aperformance%2520degradation%2520caused%2520by%2520distribution%2520shifts%2520between%2520training%2520and%250Adeployment%2520domains.%2520However%252C%2520a%2520critical%2520factor%2520influencing%2520DAOD%2520-%2520context%2520bias%250Aresulting%2520from%2520learned%2520foreground-background%2520%2528FG-BG%2529%2520associations%2520-%2520has%250Aremained%2520underexplored.%2520We%2520address%2520three%2520key%2520questions%2520regarding%2520FG%2520BG%250Aassociations%2520in%2520object%2520detection%253A%2520are%2520FG-BG%2520associations%2520encoded%2520during%2520the%250Atraining%252C%2520is%2520there%2520a%2520causal%2520relationship%2520between%2520FG-BG%2520associations%2520and%250Adetection%2520performance%252C%2520and%2520is%2520there%2520an%2520effect%2520of%2520FG-BG%2520association%2520on%2520DAOD.%2520To%250Aexamine%2520how%2520models%2520capture%2520FG%2520BG%2520associations%252C%2520we%2520analyze%2520class-wise%2520and%250Afeature-wise%2520performance%2520degradation%2520using%2520background%2520masking%2520and%2520feature%250Aperturbation%252C%2520measured%2520via%2520change%2520in%2520accuracies%2520%2528defined%2520as%2520drop%2520rate%2529.%2520To%250Aexplore%2520the%2520causal%2520role%2520of%2520FG-BG%2520associations%252C%2520we%2520apply%2520do-calculus%2520on%2520FG-BG%250Apairs%2520guided%2520by%2520class%2520activation%2520mapping%2520%2528CAM%2529.%2520To%2520quantify%2520the%2520causal%250Ainfluence%2520of%2520FG-BG%2520associations%2520across%2520domains%252C%2520we%2520propose%2520a%2520novel%2520metric%2520-%250Adomain%2520association%2520gradient%2520-%2520defined%2520as%2520the%2520ratio%2520of%2520drop%2520rate%2520to%2520maximum%2520mean%250Adiscrepancy%2520%2528MMD%2529.%2520Through%2520systematic%2520experiments%2520involving%2520background%2520masking%252C%250Afeature-level%2520perturbations%252C%2520and%2520CAM%252C%2520we%2520reveal%2520that%2520convolution-based%2520object%250Adetection%2520models%2520encode%2520FG-BG%2520associations.%2520Our%2520results%2520demonstrate%2520that%250Acontext%2520bias%2520not%2520only%2520exists%2520but%2520causally%2520undermines%2520the%2520generalization%250Acapabilities%2520of%2520object%2520detection%2520models%2520across%2520domains.%2520Furthermore%252C%2520we%250Avalidate%2520these%2520findings%2520across%2520multiple%2520models%2520and%2520datasets%252C%2520including%250Astate-of-the-art%2520architectures%2520such%2520as%2520ALDI%252B%252B.%2520This%2520study%2520highlights%2520the%250Anecessity%2520of%2520addressing%2520context%2520bias%2520explicitly%2520in%2520DAOD%2520frameworks%252C%2520providing%250Ainsights%2520that%2520pave%2520the%2520way%2520for%2520developing%2520more%2520robust%2520and%2520generalizable%2520object%250Adetection%2520systems.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2409.14679v3%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Quantifying%20Context%20Bias%20in%20Domain%20Adaptation%20for%20Object%20Detection&entry.906535625=Hojun%20Son%20and%20Asma%20Almutairi%20and%20Arpan%20Kusari&entry.1292438233=%20%20Domain%20adaptation%20for%20object%20detection%20%28DAOD%29%20has%20become%20essential%20to%20counter%0Aperformance%20degradation%20caused%20by%20distribution%20shifts%20between%20training%20and%0Adeployment%20domains.%20However%2C%20a%20critical%20factor%20influencing%20DAOD%20-%20context%20bias%0Aresulting%20from%20learned%20foreground-background%20%28FG-BG%29%20associations%20-%20has%0Aremained%20underexplored.%20We%20address%20three%20key%20questions%20regarding%20FG%20BG%0Aassociations%20in%20object%20detection%3A%20are%20FG-BG%20associations%20encoded%20during%20the%0Atraining%2C%20is%20there%20a%20causal%20relationship%20between%20FG-BG%20associations%20and%0Adetection%20performance%2C%20and%20is%20there%20an%20effect%20of%20FG-BG%20association%20on%20DAOD.%20To%0Aexamine%20how%20models%20capture%20FG%20BG%20associations%2C%20we%20analyze%20class-wise%20and%0Afeature-wise%20performance%20degradation%20using%20background%20masking%20and%20feature%0Aperturbation%2C%20measured%20via%20change%20in%20accuracies%20%28defined%20as%20drop%20rate%29.%20To%0Aexplore%20the%20causal%20role%20of%20FG-BG%20associations%2C%20we%20apply%20do-calculus%20on%20FG-BG%0Apairs%20guided%20by%20class%20activation%20mapping%20%28CAM%29.%20To%20quantify%20the%20causal%0Ainfluence%20of%20FG-BG%20associations%20across%20domains%2C%20we%20propose%20a%20novel%20metric%20-%0Adomain%20association%20gradient%20-%20defined%20as%20the%20ratio%20of%20drop%20rate%20to%20maximum%20mean%0Adiscrepancy%20%28MMD%29.%20Through%20systematic%20experiments%20involving%20background%20masking%2C%0Afeature-level%20perturbations%2C%20and%20CAM%2C%20we%20reveal%20that%20convolution-based%20object%0Adetection%20models%20encode%20FG-BG%20associations.%20Our%20results%20demonstrate%20that%0Acontext%20bias%20not%20only%20exists%20but%20causally%20undermines%20the%20generalization%0Acapabilities%20of%20object%20detection%20models%20across%20domains.%20Furthermore%2C%20we%0Avalidate%20these%20findings%20across%20multiple%20models%20and%20datasets%2C%20including%0Astate-of-the-art%20architectures%20such%20as%20ALDI%2B%2B.%20This%20study%20highlights%20the%0Anecessity%20of%20addressing%20context%20bias%20explicitly%20in%20DAOD%20frameworks%2C%20providing%0Ainsights%20that%20pave%20the%20way%20for%20developing%20more%20robust%20and%20generalizable%20object%0Adetection%20systems.%0A&entry.1838667208=http%3A//arxiv.org/abs/2409.14679v3&entry.124074799=Read"},
{"title": "Open Materials Generation with Stochastic Interpolants", "author": "Philipp Hoellmer and Thomas Egg and Maya M. Martirossyan and Eric Fuemmeler and Zeren Shui and Amit Gupta and Pawan Prakash and Adrian Roitberg and Mingjie Liu and George Karypis and Mark Transtrum and Richard G. Hennig and Ellad B. Tadmor and Stefano Martiniani", "abstract": "  The discovery of new materials is essential for enabling technological\nadvancements. Computational approaches for predicting novel materials must\neffectively learn the manifold of stable crystal structures within an infinite\ndesign space. We introduce Open Materials Generation (OMatG), a unifying\nframework for the generative design and discovery of inorganic crystalline\nmaterials. OMatG employs stochastic interpolants (SI) to bridge an arbitrary\nbase distribution to the target distribution of inorganic crystals via a broad\nclass of tunable stochastic processes, encompassing both diffusion models and\nflow matching as special cases. In this work, we adapt the SI framework by\nintegrating an equivariant graph representation of crystal structures and\nextending it to account for periodic boundary conditions in unit cell\nrepresentations. Additionally, we couple the SI flow over spatial coordinates\nand lattice vectors with discrete flow matching for atomic species. We\nbenchmark OMatG's performance on two tasks: Crystal Structure Prediction (CSP)\nfor specified compositions, and 'de novo' generation (DNG) aimed at discovering\nstable, novel, and unique structures. In our ground-up implementation of OMatG,\nwe refine and extend both CSP and DNG metrics compared to previous works. OMatG\nestablishes a new state of the art in generative modeling for materials\ndiscovery, outperforming purely flow-based and diffusion-based implementations.\nThese results underscore the importance of designing flexible deep learning\nframeworks to accelerate progress in materials science. The OMatG code is\navailable at https://github.com/FERMat-ML/OMatG.\n", "link": "http://arxiv.org/abs/2502.02582v2", "date": "2025-07-11", "relevancy": 2.0791, "topK": [{"title": "Thinking Outside the BBox: Unconstrained Generative Object Compositing", "link": "http://arxiv.org/abs/2409.04559v2", "similarity": 0.5491}, {"title": "Dynamic Gaussians Mesh: Consistent Mesh Reconstruction from Monocular\n  Videos", "link": "http://arxiv.org/abs/2404.12379v2", "similarity": 0.5238}, {"title": "UGG: Unified Generative Grasping", "link": "https://arxiv.org/abs/2311.16917", "similarity": 0.504}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Open%20Materials%20Generation%20with%20Stochastic%20Interpolants&body=Title%3A%20Open%20Materials%20Generation%20with%20Stochastic%20Interpolants%0AAuthor%3A%20Philipp%20Hoellmer%20and%20Thomas%20Egg%20and%20Maya%20M.%20Martirossyan%20and%20Eric%20Fuemmeler%20and%20Zeren%20Shui%20and%20Amit%20Gupta%20and%20Pawan%20Prakash%20and%20Adrian%20Roitberg%20and%20Mingjie%20Liu%20and%20George%20Karypis%20and%20Mark%20Transtrum%20and%20Richard%20G.%20Hennig%20and%20Ellad%20B.%20Tadmor%20and%20Stefano%20Martiniani%0AAbstract%3A%20%20%20The%20discovery%20of%20new%20materials%20is%20essential%20for%20enabling%20technological%0Aadvancements.%20Computational%20approaches%20for%20predicting%20novel%20materials%20must%0Aeffectively%20learn%20the%20manifold%20of%20stable%20crystal%20structures%20within%20an%20infinite%0Adesign%20space.%20We%20introduce%20Open%20Materials%20Generation%20%28OMatG%29%2C%20a%20unifying%0Aframework%20for%20the%20generative%20design%20and%20discovery%20of%20inorganic%20crystalline%0Amaterials.%20OMatG%20employs%20stochastic%20interpolants%20%28SI%29%20to%20bridge%20an%20arbitrary%0Abase%20distribution%20to%20the%20target%20distribution%20of%20inorganic%20crystals%20via%20a%20broad%0Aclass%20of%20tunable%20stochastic%20processes%2C%20encompassing%20both%20diffusion%20models%20and%0Aflow%20matching%20as%20special%20cases.%20In%20this%20work%2C%20we%20adapt%20the%20SI%20framework%20by%0Aintegrating%20an%20equivariant%20graph%20representation%20of%20crystal%20structures%20and%0Aextending%20it%20to%20account%20for%20periodic%20boundary%20conditions%20in%20unit%20cell%0Arepresentations.%20Additionally%2C%20we%20couple%20the%20SI%20flow%20over%20spatial%20coordinates%0Aand%20lattice%20vectors%20with%20discrete%20flow%20matching%20for%20atomic%20species.%20We%0Abenchmark%20OMatG%27s%20performance%20on%20two%20tasks%3A%20Crystal%20Structure%20Prediction%20%28CSP%29%0Afor%20specified%20compositions%2C%20and%20%27de%20novo%27%20generation%20%28DNG%29%20aimed%20at%20discovering%0Astable%2C%20novel%2C%20and%20unique%20structures.%20In%20our%20ground-up%20implementation%20of%20OMatG%2C%0Awe%20refine%20and%20extend%20both%20CSP%20and%20DNG%20metrics%20compared%20to%20previous%20works.%20OMatG%0Aestablishes%20a%20new%20state%20of%20the%20art%20in%20generative%20modeling%20for%20materials%0Adiscovery%2C%20outperforming%20purely%20flow-based%20and%20diffusion-based%20implementations.%0AThese%20results%20underscore%20the%20importance%20of%20designing%20flexible%20deep%20learning%0Aframeworks%20to%20accelerate%20progress%20in%20materials%20science.%20The%20OMatG%20code%20is%0Aavailable%20at%20https%3A//github.com/FERMat-ML/OMatG.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2502.02582v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DOpen%2520Materials%2520Generation%2520with%2520Stochastic%2520Interpolants%26entry.906535625%3DPhilipp%2520Hoellmer%2520and%2520Thomas%2520Egg%2520and%2520Maya%2520M.%2520Martirossyan%2520and%2520Eric%2520Fuemmeler%2520and%2520Zeren%2520Shui%2520and%2520Amit%2520Gupta%2520and%2520Pawan%2520Prakash%2520and%2520Adrian%2520Roitberg%2520and%2520Mingjie%2520Liu%2520and%2520George%2520Karypis%2520and%2520Mark%2520Transtrum%2520and%2520Richard%2520G.%2520Hennig%2520and%2520Ellad%2520B.%2520Tadmor%2520and%2520Stefano%2520Martiniani%26entry.1292438233%3D%2520%2520The%2520discovery%2520of%2520new%2520materials%2520is%2520essential%2520for%2520enabling%2520technological%250Aadvancements.%2520Computational%2520approaches%2520for%2520predicting%2520novel%2520materials%2520must%250Aeffectively%2520learn%2520the%2520manifold%2520of%2520stable%2520crystal%2520structures%2520within%2520an%2520infinite%250Adesign%2520space.%2520We%2520introduce%2520Open%2520Materials%2520Generation%2520%2528OMatG%2529%252C%2520a%2520unifying%250Aframework%2520for%2520the%2520generative%2520design%2520and%2520discovery%2520of%2520inorganic%2520crystalline%250Amaterials.%2520OMatG%2520employs%2520stochastic%2520interpolants%2520%2528SI%2529%2520to%2520bridge%2520an%2520arbitrary%250Abase%2520distribution%2520to%2520the%2520target%2520distribution%2520of%2520inorganic%2520crystals%2520via%2520a%2520broad%250Aclass%2520of%2520tunable%2520stochastic%2520processes%252C%2520encompassing%2520both%2520diffusion%2520models%2520and%250Aflow%2520matching%2520as%2520special%2520cases.%2520In%2520this%2520work%252C%2520we%2520adapt%2520the%2520SI%2520framework%2520by%250Aintegrating%2520an%2520equivariant%2520graph%2520representation%2520of%2520crystal%2520structures%2520and%250Aextending%2520it%2520to%2520account%2520for%2520periodic%2520boundary%2520conditions%2520in%2520unit%2520cell%250Arepresentations.%2520Additionally%252C%2520we%2520couple%2520the%2520SI%2520flow%2520over%2520spatial%2520coordinates%250Aand%2520lattice%2520vectors%2520with%2520discrete%2520flow%2520matching%2520for%2520atomic%2520species.%2520We%250Abenchmark%2520OMatG%2527s%2520performance%2520on%2520two%2520tasks%253A%2520Crystal%2520Structure%2520Prediction%2520%2528CSP%2529%250Afor%2520specified%2520compositions%252C%2520and%2520%2527de%2520novo%2527%2520generation%2520%2528DNG%2529%2520aimed%2520at%2520discovering%250Astable%252C%2520novel%252C%2520and%2520unique%2520structures.%2520In%2520our%2520ground-up%2520implementation%2520of%2520OMatG%252C%250Awe%2520refine%2520and%2520extend%2520both%2520CSP%2520and%2520DNG%2520metrics%2520compared%2520to%2520previous%2520works.%2520OMatG%250Aestablishes%2520a%2520new%2520state%2520of%2520the%2520art%2520in%2520generative%2520modeling%2520for%2520materials%250Adiscovery%252C%2520outperforming%2520purely%2520flow-based%2520and%2520diffusion-based%2520implementations.%250AThese%2520results%2520underscore%2520the%2520importance%2520of%2520designing%2520flexible%2520deep%2520learning%250Aframeworks%2520to%2520accelerate%2520progress%2520in%2520materials%2520science.%2520The%2520OMatG%2520code%2520is%250Aavailable%2520at%2520https%253A//github.com/FERMat-ML/OMatG.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2502.02582v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Open%20Materials%20Generation%20with%20Stochastic%20Interpolants&entry.906535625=Philipp%20Hoellmer%20and%20Thomas%20Egg%20and%20Maya%20M.%20Martirossyan%20and%20Eric%20Fuemmeler%20and%20Zeren%20Shui%20and%20Amit%20Gupta%20and%20Pawan%20Prakash%20and%20Adrian%20Roitberg%20and%20Mingjie%20Liu%20and%20George%20Karypis%20and%20Mark%20Transtrum%20and%20Richard%20G.%20Hennig%20and%20Ellad%20B.%20Tadmor%20and%20Stefano%20Martiniani&entry.1292438233=%20%20The%20discovery%20of%20new%20materials%20is%20essential%20for%20enabling%20technological%0Aadvancements.%20Computational%20approaches%20for%20predicting%20novel%20materials%20must%0Aeffectively%20learn%20the%20manifold%20of%20stable%20crystal%20structures%20within%20an%20infinite%0Adesign%20space.%20We%20introduce%20Open%20Materials%20Generation%20%28OMatG%29%2C%20a%20unifying%0Aframework%20for%20the%20generative%20design%20and%20discovery%20of%20inorganic%20crystalline%0Amaterials.%20OMatG%20employs%20stochastic%20interpolants%20%28SI%29%20to%20bridge%20an%20arbitrary%0Abase%20distribution%20to%20the%20target%20distribution%20of%20inorganic%20crystals%20via%20a%20broad%0Aclass%20of%20tunable%20stochastic%20processes%2C%20encompassing%20both%20diffusion%20models%20and%0Aflow%20matching%20as%20special%20cases.%20In%20this%20work%2C%20we%20adapt%20the%20SI%20framework%20by%0Aintegrating%20an%20equivariant%20graph%20representation%20of%20crystal%20structures%20and%0Aextending%20it%20to%20account%20for%20periodic%20boundary%20conditions%20in%20unit%20cell%0Arepresentations.%20Additionally%2C%20we%20couple%20the%20SI%20flow%20over%20spatial%20coordinates%0Aand%20lattice%20vectors%20with%20discrete%20flow%20matching%20for%20atomic%20species.%20We%0Abenchmark%20OMatG%27s%20performance%20on%20two%20tasks%3A%20Crystal%20Structure%20Prediction%20%28CSP%29%0Afor%20specified%20compositions%2C%20and%20%27de%20novo%27%20generation%20%28DNG%29%20aimed%20at%20discovering%0Astable%2C%20novel%2C%20and%20unique%20structures.%20In%20our%20ground-up%20implementation%20of%20OMatG%2C%0Awe%20refine%20and%20extend%20both%20CSP%20and%20DNG%20metrics%20compared%20to%20previous%20works.%20OMatG%0Aestablishes%20a%20new%20state%20of%20the%20art%20in%20generative%20modeling%20for%20materials%0Adiscovery%2C%20outperforming%20purely%20flow-based%20and%20diffusion-based%20implementations.%0AThese%20results%20underscore%20the%20importance%20of%20designing%20flexible%20deep%20learning%0Aframeworks%20to%20accelerate%20progress%20in%20materials%20science.%20The%20OMatG%20code%20is%0Aavailable%20at%20https%3A//github.com/FERMat-ML/OMatG.%0A&entry.1838667208=http%3A//arxiv.org/abs/2502.02582v2&entry.124074799=Read"},
{"title": "Generalizable 7T T1-map Synthesis from 1.5T and 3T T1 MRI with an\n  Efficient Transformer Model", "author": "Zach Eidex and Mojtaba Safari and Tonghe Wang and Vanessa Wildman and David S. Yu and Hui Mao and Erik Middlebrooks and Aparna Kesewala and Xiaofeng Yang", "abstract": "  Purpose: Ultra-high-field 7T MRI offers improved resolution and contrast over\nstandard clinical field strengths (1.5T, 3T). However, 7T scanners are costly,\nscarce, and introduce additional challenges such as susceptibility artifacts.\nWe propose an efficient transformer-based model (7T-Restormer) to synthesize\n7T-quality T1-maps from routine 1.5T or 3T T1-weighted (T1W) images. Methods:\nOur model was validated on 35 1.5T and 108 3T T1w MRI paired with corresponding\n7T T1 maps of patients with confirmed MS. A total of 141 patient cases (32,128\nslices) were randomly divided into 105 (25; 80) training cases (19,204 slices),\n19 (5; 14) validation cases (3,476 slices), and 17 (5; 14) test cases (3,145\nslices) where (X; Y) denotes the patients with 1.5T and 3T T1W scans,\nrespectively. The synthetic 7T T1 maps were compared against the ResViT and\nResShift models. Results: The 7T-Restormer model achieved a PSNR of 26.0 +/-\n4.6 dB, SSIM of 0.861 +/- 0.072, and NMSE of 0.019 +/- 0.011 for 1.5T inputs,\nand 25.9 +/- 4.9 dB, and 0.866 +/- 0.077 for 3T inputs, respectively. Using\n10.5 M parameters, our model reduced NMSE by 64 % relative to 56.7M parameter\nResShift (0.019 vs 0.052, p = <.001 and by 41 % relative to 70.4M parameter\nResViT (0.019 vs 0.032, p = <.001) at 1.5T, with similar advantages at 3T\n(0.021 vs 0.060 and 0.033; p < .001). Training with a mixed 1.5 T + 3 T corpus\nwas superior to single-field strategies. Restricting the model to 1.5T\nincreased the 1.5T NMSE from 0.019 to 0.021 (p = 1.1E-3) while training solely\non 3T resulted in lower performance on input 1.5T T1W MRI. Conclusion: We\npropose a novel method for predicting quantitative 7T MP2RAGE maps from 1.5T\nand 3T T1W scans with higher quality than existing state-of-the-art methods.\nOur approach makes the benefits of 7T MRI more accessible to standard clinical\nworkflows.\n", "link": "http://arxiv.org/abs/2507.08655v1", "date": "2025-07-11", "relevancy": 2.0742, "topK": [{"title": "Scaling Rectified Flow Transformers for High-Resolution Image Synthesis", "link": "http://arxiv.org/abs/2403.03206v1", "similarity": 0.5773}, {"title": "GST: Precise 3D Human Body from a Single Image with Gaussian Splatting\n  Transformers", "link": "http://arxiv.org/abs/2409.04196v1", "similarity": 0.5297}, {"title": "RTG-SLAM: Real-time 3D Reconstruction at Scale using Gaussian Splatting", "link": "http://arxiv.org/abs/2404.19706v1", "similarity": 0.4839}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Generalizable%207T%20T1-map%20Synthesis%20from%201.5T%20and%203T%20T1%20MRI%20with%20an%0A%20%20Efficient%20Transformer%20Model&body=Title%3A%20Generalizable%207T%20T1-map%20Synthesis%20from%201.5T%20and%203T%20T1%20MRI%20with%20an%0A%20%20Efficient%20Transformer%20Model%0AAuthor%3A%20Zach%20Eidex%20and%20Mojtaba%20Safari%20and%20Tonghe%20Wang%20and%20Vanessa%20Wildman%20and%20David%20S.%20Yu%20and%20Hui%20Mao%20and%20Erik%20Middlebrooks%20and%20Aparna%20Kesewala%20and%20Xiaofeng%20Yang%0AAbstract%3A%20%20%20Purpose%3A%20Ultra-high-field%207T%20MRI%20offers%20improved%20resolution%20and%20contrast%20over%0Astandard%20clinical%20field%20strengths%20%281.5T%2C%203T%29.%20However%2C%207T%20scanners%20are%20costly%2C%0Ascarce%2C%20and%20introduce%20additional%20challenges%20such%20as%20susceptibility%20artifacts.%0AWe%20propose%20an%20efficient%20transformer-based%20model%20%287T-Restormer%29%20to%20synthesize%0A7T-quality%20T1-maps%20from%20routine%201.5T%20or%203T%20T1-weighted%20%28T1W%29%20images.%20Methods%3A%0AOur%20model%20was%20validated%20on%2035%201.5T%20and%20108%203T%20T1w%20MRI%20paired%20with%20corresponding%0A7T%20T1%20maps%20of%20patients%20with%20confirmed%20MS.%20A%20total%20of%20141%20patient%20cases%20%2832%2C128%0Aslices%29%20were%20randomly%20divided%20into%20105%20%2825%3B%2080%29%20training%20cases%20%2819%2C204%20slices%29%2C%0A19%20%285%3B%2014%29%20validation%20cases%20%283%2C476%20slices%29%2C%20and%2017%20%285%3B%2014%29%20test%20cases%20%283%2C145%0Aslices%29%20where%20%28X%3B%20Y%29%20denotes%20the%20patients%20with%201.5T%20and%203T%20T1W%20scans%2C%0Arespectively.%20The%20synthetic%207T%20T1%20maps%20were%20compared%20against%20the%20ResViT%20and%0AResShift%20models.%20Results%3A%20The%207T-Restormer%20model%20achieved%20a%20PSNR%20of%2026.0%20%2B/-%0A4.6%20dB%2C%20SSIM%20of%200.861%20%2B/-%200.072%2C%20and%20NMSE%20of%200.019%20%2B/-%200.011%20for%201.5T%20inputs%2C%0Aand%2025.9%20%2B/-%204.9%20dB%2C%20and%200.866%20%2B/-%200.077%20for%203T%20inputs%2C%20respectively.%20Using%0A10.5%20M%20parameters%2C%20our%20model%20reduced%20NMSE%20by%2064%20%25%20relative%20to%2056.7M%20parameter%0AResShift%20%280.019%20vs%200.052%2C%20p%20%3D%20%3C.001%20and%20by%2041%20%25%20relative%20to%2070.4M%20parameter%0AResViT%20%280.019%20vs%200.032%2C%20p%20%3D%20%3C.001%29%20at%201.5T%2C%20with%20similar%20advantages%20at%203T%0A%280.021%20vs%200.060%20and%200.033%3B%20p%20%3C%20.001%29.%20Training%20with%20a%20mixed%201.5%20T%20%2B%203%20T%20corpus%0Awas%20superior%20to%20single-field%20strategies.%20Restricting%20the%20model%20to%201.5T%0Aincreased%20the%201.5T%20NMSE%20from%200.019%20to%200.021%20%28p%20%3D%201.1E-3%29%20while%20training%20solely%0Aon%203T%20resulted%20in%20lower%20performance%20on%20input%201.5T%20T1W%20MRI.%20Conclusion%3A%20We%0Apropose%20a%20novel%20method%20for%20predicting%20quantitative%207T%20MP2RAGE%20maps%20from%201.5T%0Aand%203T%20T1W%20scans%20with%20higher%20quality%20than%20existing%20state-of-the-art%20methods.%0AOur%20approach%20makes%20the%20benefits%20of%207T%20MRI%20more%20accessible%20to%20standard%20clinical%0Aworkflows.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2507.08655v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DGeneralizable%25207T%2520T1-map%2520Synthesis%2520from%25201.5T%2520and%25203T%2520T1%2520MRI%2520with%2520an%250A%2520%2520Efficient%2520Transformer%2520Model%26entry.906535625%3DZach%2520Eidex%2520and%2520Mojtaba%2520Safari%2520and%2520Tonghe%2520Wang%2520and%2520Vanessa%2520Wildman%2520and%2520David%2520S.%2520Yu%2520and%2520Hui%2520Mao%2520and%2520Erik%2520Middlebrooks%2520and%2520Aparna%2520Kesewala%2520and%2520Xiaofeng%2520Yang%26entry.1292438233%3D%2520%2520Purpose%253A%2520Ultra-high-field%25207T%2520MRI%2520offers%2520improved%2520resolution%2520and%2520contrast%2520over%250Astandard%2520clinical%2520field%2520strengths%2520%25281.5T%252C%25203T%2529.%2520However%252C%25207T%2520scanners%2520are%2520costly%252C%250Ascarce%252C%2520and%2520introduce%2520additional%2520challenges%2520such%2520as%2520susceptibility%2520artifacts.%250AWe%2520propose%2520an%2520efficient%2520transformer-based%2520model%2520%25287T-Restormer%2529%2520to%2520synthesize%250A7T-quality%2520T1-maps%2520from%2520routine%25201.5T%2520or%25203T%2520T1-weighted%2520%2528T1W%2529%2520images.%2520Methods%253A%250AOur%2520model%2520was%2520validated%2520on%252035%25201.5T%2520and%2520108%25203T%2520T1w%2520MRI%2520paired%2520with%2520corresponding%250A7T%2520T1%2520maps%2520of%2520patients%2520with%2520confirmed%2520MS.%2520A%2520total%2520of%2520141%2520patient%2520cases%2520%252832%252C128%250Aslices%2529%2520were%2520randomly%2520divided%2520into%2520105%2520%252825%253B%252080%2529%2520training%2520cases%2520%252819%252C204%2520slices%2529%252C%250A19%2520%25285%253B%252014%2529%2520validation%2520cases%2520%25283%252C476%2520slices%2529%252C%2520and%252017%2520%25285%253B%252014%2529%2520test%2520cases%2520%25283%252C145%250Aslices%2529%2520where%2520%2528X%253B%2520Y%2529%2520denotes%2520the%2520patients%2520with%25201.5T%2520and%25203T%2520T1W%2520scans%252C%250Arespectively.%2520The%2520synthetic%25207T%2520T1%2520maps%2520were%2520compared%2520against%2520the%2520ResViT%2520and%250AResShift%2520models.%2520Results%253A%2520The%25207T-Restormer%2520model%2520achieved%2520a%2520PSNR%2520of%252026.0%2520%252B/-%250A4.6%2520dB%252C%2520SSIM%2520of%25200.861%2520%252B/-%25200.072%252C%2520and%2520NMSE%2520of%25200.019%2520%252B/-%25200.011%2520for%25201.5T%2520inputs%252C%250Aand%252025.9%2520%252B/-%25204.9%2520dB%252C%2520and%25200.866%2520%252B/-%25200.077%2520for%25203T%2520inputs%252C%2520respectively.%2520Using%250A10.5%2520M%2520parameters%252C%2520our%2520model%2520reduced%2520NMSE%2520by%252064%2520%2525%2520relative%2520to%252056.7M%2520parameter%250AResShift%2520%25280.019%2520vs%25200.052%252C%2520p%2520%253D%2520%253C.001%2520and%2520by%252041%2520%2525%2520relative%2520to%252070.4M%2520parameter%250AResViT%2520%25280.019%2520vs%25200.032%252C%2520p%2520%253D%2520%253C.001%2529%2520at%25201.5T%252C%2520with%2520similar%2520advantages%2520at%25203T%250A%25280.021%2520vs%25200.060%2520and%25200.033%253B%2520p%2520%253C%2520.001%2529.%2520Training%2520with%2520a%2520mixed%25201.5%2520T%2520%252B%25203%2520T%2520corpus%250Awas%2520superior%2520to%2520single-field%2520strategies.%2520Restricting%2520the%2520model%2520to%25201.5T%250Aincreased%2520the%25201.5T%2520NMSE%2520from%25200.019%2520to%25200.021%2520%2528p%2520%253D%25201.1E-3%2529%2520while%2520training%2520solely%250Aon%25203T%2520resulted%2520in%2520lower%2520performance%2520on%2520input%25201.5T%2520T1W%2520MRI.%2520Conclusion%253A%2520We%250Apropose%2520a%2520novel%2520method%2520for%2520predicting%2520quantitative%25207T%2520MP2RAGE%2520maps%2520from%25201.5T%250Aand%25203T%2520T1W%2520scans%2520with%2520higher%2520quality%2520than%2520existing%2520state-of-the-art%2520methods.%250AOur%2520approach%2520makes%2520the%2520benefits%2520of%25207T%2520MRI%2520more%2520accessible%2520to%2520standard%2520clinical%250Aworkflows.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2507.08655v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Generalizable%207T%20T1-map%20Synthesis%20from%201.5T%20and%203T%20T1%20MRI%20with%20an%0A%20%20Efficient%20Transformer%20Model&entry.906535625=Zach%20Eidex%20and%20Mojtaba%20Safari%20and%20Tonghe%20Wang%20and%20Vanessa%20Wildman%20and%20David%20S.%20Yu%20and%20Hui%20Mao%20and%20Erik%20Middlebrooks%20and%20Aparna%20Kesewala%20and%20Xiaofeng%20Yang&entry.1292438233=%20%20Purpose%3A%20Ultra-high-field%207T%20MRI%20offers%20improved%20resolution%20and%20contrast%20over%0Astandard%20clinical%20field%20strengths%20%281.5T%2C%203T%29.%20However%2C%207T%20scanners%20are%20costly%2C%0Ascarce%2C%20and%20introduce%20additional%20challenges%20such%20as%20susceptibility%20artifacts.%0AWe%20propose%20an%20efficient%20transformer-based%20model%20%287T-Restormer%29%20to%20synthesize%0A7T-quality%20T1-maps%20from%20routine%201.5T%20or%203T%20T1-weighted%20%28T1W%29%20images.%20Methods%3A%0AOur%20model%20was%20validated%20on%2035%201.5T%20and%20108%203T%20T1w%20MRI%20paired%20with%20corresponding%0A7T%20T1%20maps%20of%20patients%20with%20confirmed%20MS.%20A%20total%20of%20141%20patient%20cases%20%2832%2C128%0Aslices%29%20were%20randomly%20divided%20into%20105%20%2825%3B%2080%29%20training%20cases%20%2819%2C204%20slices%29%2C%0A19%20%285%3B%2014%29%20validation%20cases%20%283%2C476%20slices%29%2C%20and%2017%20%285%3B%2014%29%20test%20cases%20%283%2C145%0Aslices%29%20where%20%28X%3B%20Y%29%20denotes%20the%20patients%20with%201.5T%20and%203T%20T1W%20scans%2C%0Arespectively.%20The%20synthetic%207T%20T1%20maps%20were%20compared%20against%20the%20ResViT%20and%0AResShift%20models.%20Results%3A%20The%207T-Restormer%20model%20achieved%20a%20PSNR%20of%2026.0%20%2B/-%0A4.6%20dB%2C%20SSIM%20of%200.861%20%2B/-%200.072%2C%20and%20NMSE%20of%200.019%20%2B/-%200.011%20for%201.5T%20inputs%2C%0Aand%2025.9%20%2B/-%204.9%20dB%2C%20and%200.866%20%2B/-%200.077%20for%203T%20inputs%2C%20respectively.%20Using%0A10.5%20M%20parameters%2C%20our%20model%20reduced%20NMSE%20by%2064%20%25%20relative%20to%2056.7M%20parameter%0AResShift%20%280.019%20vs%200.052%2C%20p%20%3D%20%3C.001%20and%20by%2041%20%25%20relative%20to%2070.4M%20parameter%0AResViT%20%280.019%20vs%200.032%2C%20p%20%3D%20%3C.001%29%20at%201.5T%2C%20with%20similar%20advantages%20at%203T%0A%280.021%20vs%200.060%20and%200.033%3B%20p%20%3C%20.001%29.%20Training%20with%20a%20mixed%201.5%20T%20%2B%203%20T%20corpus%0Awas%20superior%20to%20single-field%20strategies.%20Restricting%20the%20model%20to%201.5T%0Aincreased%20the%201.5T%20NMSE%20from%200.019%20to%200.021%20%28p%20%3D%201.1E-3%29%20while%20training%20solely%0Aon%203T%20resulted%20in%20lower%20performance%20on%20input%201.5T%20T1W%20MRI.%20Conclusion%3A%20We%0Apropose%20a%20novel%20method%20for%20predicting%20quantitative%207T%20MP2RAGE%20maps%20from%201.5T%0Aand%203T%20T1W%20scans%20with%20higher%20quality%20than%20existing%20state-of-the-art%20methods.%0AOur%20approach%20makes%20the%20benefits%20of%207T%20MRI%20more%20accessible%20to%20standard%20clinical%0Aworkflows.%0A&entry.1838667208=http%3A//arxiv.org/abs/2507.08655v1&entry.124074799=Read"},
{"title": "A Hybrid Multi-Well Hopfield-CNN with Feature Extraction and K-Means for\n  MNIST Classification", "author": "Ahmed Farooq", "abstract": "  This study presents a hybrid model for classifying handwritten digits in the\nMNIST dataset, combining convolutional neural networks (CNNs) with a multi-well\nHopfield network. The approach employs a CNN to extract high-dimensional\nfeatures from input images, which are then clustered into class-specific\nprototypes using k-means clustering. These prototypes serve as attractors in a\nmulti-well energy landscape, where a Hopfield network performs classification\nby minimizing an energy function that balances feature similarity and class\nassignment.The model's design enables robust handling of intraclass\nvariability, such as diverse handwriting styles, while providing an\ninterpretable framework through its energy-based decision process. Through\nsystematic optimization of the CNN architecture and the number of wells, the\nmodel achieves a high test accuracy of 99.2% on 10,000 MNIST images,\ndemonstrating its effectiveness for image classification tasks. The findings\nhighlight the critical role of deep feature extraction and sufficient prototype\ncoverage in achieving high performance, with potential for broader applications\nin pattern recognition.\n", "link": "http://arxiv.org/abs/2507.08766v1", "date": "2025-07-11", "relevancy": 2.0602, "topK": [{"title": "Boosted dynamic neural networks", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/26302", "similarity": 0.5314}, {"title": "DDM-NET: End-to-end learning of keypoint feature Detection, Description and Matching for 3D localization", "link": "https://arxiv.org/abs/2212.04575", "similarity": 0.5047}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.5}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20A%20Hybrid%20Multi-Well%20Hopfield-CNN%20with%20Feature%20Extraction%20and%20K-Means%20for%0A%20%20MNIST%20Classification&body=Title%3A%20A%20Hybrid%20Multi-Well%20Hopfield-CNN%20with%20Feature%20Extraction%20and%20K-Means%20for%0A%20%20MNIST%20Classification%0AAuthor%3A%20Ahmed%20Farooq%0AAbstract%3A%20%20%20This%20study%20presents%20a%20hybrid%20model%20for%20classifying%20handwritten%20digits%20in%20the%0AMNIST%20dataset%2C%20combining%20convolutional%20neural%20networks%20%28CNNs%29%20with%20a%20multi-well%0AHopfield%20network.%20The%20approach%20employs%20a%20CNN%20to%20extract%20high-dimensional%0Afeatures%20from%20input%20images%2C%20which%20are%20then%20clustered%20into%20class-specific%0Aprototypes%20using%20k-means%20clustering.%20These%20prototypes%20serve%20as%20attractors%20in%20a%0Amulti-well%20energy%20landscape%2C%20where%20a%20Hopfield%20network%20performs%20classification%0Aby%20minimizing%20an%20energy%20function%20that%20balances%20feature%20similarity%20and%20class%0Aassignment.The%20model%27s%20design%20enables%20robust%20handling%20of%20intraclass%0Avariability%2C%20such%20as%20diverse%20handwriting%20styles%2C%20while%20providing%20an%0Ainterpretable%20framework%20through%20its%20energy-based%20decision%20process.%20Through%0Asystematic%20optimization%20of%20the%20CNN%20architecture%20and%20the%20number%20of%20wells%2C%20the%0Amodel%20achieves%20a%20high%20test%20accuracy%20of%2099.2%25%20on%2010%2C000%20MNIST%20images%2C%0Ademonstrating%20its%20effectiveness%20for%20image%20classification%20tasks.%20The%20findings%0Ahighlight%20the%20critical%20role%20of%20deep%20feature%20extraction%20and%20sufficient%20prototype%0Acoverage%20in%20achieving%20high%20performance%2C%20with%20potential%20for%20broader%20applications%0Ain%20pattern%20recognition.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2507.08766v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DA%2520Hybrid%2520Multi-Well%2520Hopfield-CNN%2520with%2520Feature%2520Extraction%2520and%2520K-Means%2520for%250A%2520%2520MNIST%2520Classification%26entry.906535625%3DAhmed%2520Farooq%26entry.1292438233%3D%2520%2520This%2520study%2520presents%2520a%2520hybrid%2520model%2520for%2520classifying%2520handwritten%2520digits%2520in%2520the%250AMNIST%2520dataset%252C%2520combining%2520convolutional%2520neural%2520networks%2520%2528CNNs%2529%2520with%2520a%2520multi-well%250AHopfield%2520network.%2520The%2520approach%2520employs%2520a%2520CNN%2520to%2520extract%2520high-dimensional%250Afeatures%2520from%2520input%2520images%252C%2520which%2520are%2520then%2520clustered%2520into%2520class-specific%250Aprototypes%2520using%2520k-means%2520clustering.%2520These%2520prototypes%2520serve%2520as%2520attractors%2520in%2520a%250Amulti-well%2520energy%2520landscape%252C%2520where%2520a%2520Hopfield%2520network%2520performs%2520classification%250Aby%2520minimizing%2520an%2520energy%2520function%2520that%2520balances%2520feature%2520similarity%2520and%2520class%250Aassignment.The%2520model%2527s%2520design%2520enables%2520robust%2520handling%2520of%2520intraclass%250Avariability%252C%2520such%2520as%2520diverse%2520handwriting%2520styles%252C%2520while%2520providing%2520an%250Ainterpretable%2520framework%2520through%2520its%2520energy-based%2520decision%2520process.%2520Through%250Asystematic%2520optimization%2520of%2520the%2520CNN%2520architecture%2520and%2520the%2520number%2520of%2520wells%252C%2520the%250Amodel%2520achieves%2520a%2520high%2520test%2520accuracy%2520of%252099.2%2525%2520on%252010%252C000%2520MNIST%2520images%252C%250Ademonstrating%2520its%2520effectiveness%2520for%2520image%2520classification%2520tasks.%2520The%2520findings%250Ahighlight%2520the%2520critical%2520role%2520of%2520deep%2520feature%2520extraction%2520and%2520sufficient%2520prototype%250Acoverage%2520in%2520achieving%2520high%2520performance%252C%2520with%2520potential%2520for%2520broader%2520applications%250Ain%2520pattern%2520recognition.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2507.08766v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=A%20Hybrid%20Multi-Well%20Hopfield-CNN%20with%20Feature%20Extraction%20and%20K-Means%20for%0A%20%20MNIST%20Classification&entry.906535625=Ahmed%20Farooq&entry.1292438233=%20%20This%20study%20presents%20a%20hybrid%20model%20for%20classifying%20handwritten%20digits%20in%20the%0AMNIST%20dataset%2C%20combining%20convolutional%20neural%20networks%20%28CNNs%29%20with%20a%20multi-well%0AHopfield%20network.%20The%20approach%20employs%20a%20CNN%20to%20extract%20high-dimensional%0Afeatures%20from%20input%20images%2C%20which%20are%20then%20clustered%20into%20class-specific%0Aprototypes%20using%20k-means%20clustering.%20These%20prototypes%20serve%20as%20attractors%20in%20a%0Amulti-well%20energy%20landscape%2C%20where%20a%20Hopfield%20network%20performs%20classification%0Aby%20minimizing%20an%20energy%20function%20that%20balances%20feature%20similarity%20and%20class%0Aassignment.The%20model%27s%20design%20enables%20robust%20handling%20of%20intraclass%0Avariability%2C%20such%20as%20diverse%20handwriting%20styles%2C%20while%20providing%20an%0Ainterpretable%20framework%20through%20its%20energy-based%20decision%20process.%20Through%0Asystematic%20optimization%20of%20the%20CNN%20architecture%20and%20the%20number%20of%20wells%2C%20the%0Amodel%20achieves%20a%20high%20test%20accuracy%20of%2099.2%25%20on%2010%2C000%20MNIST%20images%2C%0Ademonstrating%20its%20effectiveness%20for%20image%20classification%20tasks.%20The%20findings%0Ahighlight%20the%20critical%20role%20of%20deep%20feature%20extraction%20and%20sufficient%20prototype%0Acoverage%20in%20achieving%20high%20performance%2C%20with%20potential%20for%20broader%20applications%0Ain%20pattern%20recognition.%0A&entry.1838667208=http%3A//arxiv.org/abs/2507.08766v1&entry.124074799=Read"},
{"title": "Ensemble of Weak Spectral Total Variation Learners: a PET-CT Case Study", "author": "Anna Rosenberg and John Kennedy and Zohar Keidar and Yehoshua Y. Zeevi and Guy Gilboa", "abstract": "  Solving computer vision problems through machine learning, one often\nencounters lack of sufficient training data. To mitigate this we propose the\nuse of ensembles of weak learners based on spectral total-variation (STV)\nfeatures (Gilboa 2014). The features are related to nonlinear eigenfunctions of\nthe total-variation subgradient and can characterize well textures at various\nscales. It was shown (Burger et-al 2016) that, in the one-dimensional case,\northogonal features are generated, whereas in two-dimensions the features are\nempirically lowly correlated. Ensemble learning theory advocates the use of\nlowly correlated weak learners. We thus propose here to design ensembles using\nlearners based on STV features. To show the effectiveness of this paradigm we\nexamine a hard real-world medical imaging problem: the predictive value of\ncomputed tomography (CT) data for high uptake in positron emission tomography\n(PET) for patients suspected of skeletal metastases. The database consists of\n457 scans with 1524 unique pairs of registered CT and PET slices. Our approach\nis compared to deep-learning methods and to Radiomics features, showing STV\nlearners perform best (AUC=0.87), compared to neural nets (AUC=0.75) and\nRadiomics (AUC=0.79). We observe that fine STV scales in CT images are\nespecially indicative for the presence of high uptake in PET.\n", "link": "http://arxiv.org/abs/2507.08735v1", "date": "2025-07-11", "relevancy": 2.046, "topK": [{"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.5161}, {"title": "GST: Precise 3D Human Body from a Single Image with Gaussian Splatting\n  Transformers", "link": "http://arxiv.org/abs/2409.04196v1", "similarity": 0.5115}, {"title": "CAT3D: Create Anything in 3D with Multi-View Diffusion Models", "link": "http://arxiv.org/abs/2405.10314v1", "similarity": 0.5097}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Ensemble%20of%20Weak%20Spectral%20Total%20Variation%20Learners%3A%20a%20PET-CT%20Case%20Study&body=Title%3A%20Ensemble%20of%20Weak%20Spectral%20Total%20Variation%20Learners%3A%20a%20PET-CT%20Case%20Study%0AAuthor%3A%20Anna%20Rosenberg%20and%20John%20Kennedy%20and%20Zohar%20Keidar%20and%20Yehoshua%20Y.%20Zeevi%20and%20Guy%20Gilboa%0AAbstract%3A%20%20%20Solving%20computer%20vision%20problems%20through%20machine%20learning%2C%20one%20often%0Aencounters%20lack%20of%20sufficient%20training%20data.%20To%20mitigate%20this%20we%20propose%20the%0Ause%20of%20ensembles%20of%20weak%20learners%20based%20on%20spectral%20total-variation%20%28STV%29%0Afeatures%20%28Gilboa%202014%29.%20The%20features%20are%20related%20to%20nonlinear%20eigenfunctions%20of%0Athe%20total-variation%20subgradient%20and%20can%20characterize%20well%20textures%20at%20various%0Ascales.%20It%20was%20shown%20%28Burger%20et-al%202016%29%20that%2C%20in%20the%20one-dimensional%20case%2C%0Aorthogonal%20features%20are%20generated%2C%20whereas%20in%20two-dimensions%20the%20features%20are%0Aempirically%20lowly%20correlated.%20Ensemble%20learning%20theory%20advocates%20the%20use%20of%0Alowly%20correlated%20weak%20learners.%20We%20thus%20propose%20here%20to%20design%20ensembles%20using%0Alearners%20based%20on%20STV%20features.%20To%20show%20the%20effectiveness%20of%20this%20paradigm%20we%0Aexamine%20a%20hard%20real-world%20medical%20imaging%20problem%3A%20the%20predictive%20value%20of%0Acomputed%20tomography%20%28CT%29%20data%20for%20high%20uptake%20in%20positron%20emission%20tomography%0A%28PET%29%20for%20patients%20suspected%20of%20skeletal%20metastases.%20The%20database%20consists%20of%0A457%20scans%20with%201524%20unique%20pairs%20of%20registered%20CT%20and%20PET%20slices.%20Our%20approach%0Ais%20compared%20to%20deep-learning%20methods%20and%20to%20Radiomics%20features%2C%20showing%20STV%0Alearners%20perform%20best%20%28AUC%3D0.87%29%2C%20compared%20to%20neural%20nets%20%28AUC%3D0.75%29%20and%0ARadiomics%20%28AUC%3D0.79%29.%20We%20observe%20that%20fine%20STV%20scales%20in%20CT%20images%20are%0Aespecially%20indicative%20for%20the%20presence%20of%20high%20uptake%20in%20PET.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2507.08735v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DEnsemble%2520of%2520Weak%2520Spectral%2520Total%2520Variation%2520Learners%253A%2520a%2520PET-CT%2520Case%2520Study%26entry.906535625%3DAnna%2520Rosenberg%2520and%2520John%2520Kennedy%2520and%2520Zohar%2520Keidar%2520and%2520Yehoshua%2520Y.%2520Zeevi%2520and%2520Guy%2520Gilboa%26entry.1292438233%3D%2520%2520Solving%2520computer%2520vision%2520problems%2520through%2520machine%2520learning%252C%2520one%2520often%250Aencounters%2520lack%2520of%2520sufficient%2520training%2520data.%2520To%2520mitigate%2520this%2520we%2520propose%2520the%250Ause%2520of%2520ensembles%2520of%2520weak%2520learners%2520based%2520on%2520spectral%2520total-variation%2520%2528STV%2529%250Afeatures%2520%2528Gilboa%25202014%2529.%2520The%2520features%2520are%2520related%2520to%2520nonlinear%2520eigenfunctions%2520of%250Athe%2520total-variation%2520subgradient%2520and%2520can%2520characterize%2520well%2520textures%2520at%2520various%250Ascales.%2520It%2520was%2520shown%2520%2528Burger%2520et-al%25202016%2529%2520that%252C%2520in%2520the%2520one-dimensional%2520case%252C%250Aorthogonal%2520features%2520are%2520generated%252C%2520whereas%2520in%2520two-dimensions%2520the%2520features%2520are%250Aempirically%2520lowly%2520correlated.%2520Ensemble%2520learning%2520theory%2520advocates%2520the%2520use%2520of%250Alowly%2520correlated%2520weak%2520learners.%2520We%2520thus%2520propose%2520here%2520to%2520design%2520ensembles%2520using%250Alearners%2520based%2520on%2520STV%2520features.%2520To%2520show%2520the%2520effectiveness%2520of%2520this%2520paradigm%2520we%250Aexamine%2520a%2520hard%2520real-world%2520medical%2520imaging%2520problem%253A%2520the%2520predictive%2520value%2520of%250Acomputed%2520tomography%2520%2528CT%2529%2520data%2520for%2520high%2520uptake%2520in%2520positron%2520emission%2520tomography%250A%2528PET%2529%2520for%2520patients%2520suspected%2520of%2520skeletal%2520metastases.%2520The%2520database%2520consists%2520of%250A457%2520scans%2520with%25201524%2520unique%2520pairs%2520of%2520registered%2520CT%2520and%2520PET%2520slices.%2520Our%2520approach%250Ais%2520compared%2520to%2520deep-learning%2520methods%2520and%2520to%2520Radiomics%2520features%252C%2520showing%2520STV%250Alearners%2520perform%2520best%2520%2528AUC%253D0.87%2529%252C%2520compared%2520to%2520neural%2520nets%2520%2528AUC%253D0.75%2529%2520and%250ARadiomics%2520%2528AUC%253D0.79%2529.%2520We%2520observe%2520that%2520fine%2520STV%2520scales%2520in%2520CT%2520images%2520are%250Aespecially%2520indicative%2520for%2520the%2520presence%2520of%2520high%2520uptake%2520in%2520PET.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2507.08735v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Ensemble%20of%20Weak%20Spectral%20Total%20Variation%20Learners%3A%20a%20PET-CT%20Case%20Study&entry.906535625=Anna%20Rosenberg%20and%20John%20Kennedy%20and%20Zohar%20Keidar%20and%20Yehoshua%20Y.%20Zeevi%20and%20Guy%20Gilboa&entry.1292438233=%20%20Solving%20computer%20vision%20problems%20through%20machine%20learning%2C%20one%20often%0Aencounters%20lack%20of%20sufficient%20training%20data.%20To%20mitigate%20this%20we%20propose%20the%0Ause%20of%20ensembles%20of%20weak%20learners%20based%20on%20spectral%20total-variation%20%28STV%29%0Afeatures%20%28Gilboa%202014%29.%20The%20features%20are%20related%20to%20nonlinear%20eigenfunctions%20of%0Athe%20total-variation%20subgradient%20and%20can%20characterize%20well%20textures%20at%20various%0Ascales.%20It%20was%20shown%20%28Burger%20et-al%202016%29%20that%2C%20in%20the%20one-dimensional%20case%2C%0Aorthogonal%20features%20are%20generated%2C%20whereas%20in%20two-dimensions%20the%20features%20are%0Aempirically%20lowly%20correlated.%20Ensemble%20learning%20theory%20advocates%20the%20use%20of%0Alowly%20correlated%20weak%20learners.%20We%20thus%20propose%20here%20to%20design%20ensembles%20using%0Alearners%20based%20on%20STV%20features.%20To%20show%20the%20effectiveness%20of%20this%20paradigm%20we%0Aexamine%20a%20hard%20real-world%20medical%20imaging%20problem%3A%20the%20predictive%20value%20of%0Acomputed%20tomography%20%28CT%29%20data%20for%20high%20uptake%20in%20positron%20emission%20tomography%0A%28PET%29%20for%20patients%20suspected%20of%20skeletal%20metastases.%20The%20database%20consists%20of%0A457%20scans%20with%201524%20unique%20pairs%20of%20registered%20CT%20and%20PET%20slices.%20Our%20approach%0Ais%20compared%20to%20deep-learning%20methods%20and%20to%20Radiomics%20features%2C%20showing%20STV%0Alearners%20perform%20best%20%28AUC%3D0.87%29%2C%20compared%20to%20neural%20nets%20%28AUC%3D0.75%29%20and%0ARadiomics%20%28AUC%3D0.79%29.%20We%20observe%20that%20fine%20STV%20scales%20in%20CT%20images%20are%0Aespecially%20indicative%20for%20the%20presence%20of%20high%20uptake%20in%20PET.%0A&entry.1838667208=http%3A//arxiv.org/abs/2507.08735v1&entry.124074799=Read"},
{"title": "InsViE-1M: Effective Instruction-based Video Editing with Elaborate\n  Dataset Construction", "author": "Yuhui Wu and Liyi Chen and Ruibin Li and Shihao Wang and Chenxi Xie and Lei Zhang", "abstract": "  Instruction-based video editing allows effective and interactive editing of\nvideos using only instructions without extra inputs such as masks or\nattributes. However, collecting high-quality training triplets (source video,\nedited video, instruction) is a challenging task. Existing datasets mostly\nconsist of low-resolution, short duration, and limited amount of source videos\nwith unsatisfactory editing quality, limiting the performance of trained\nediting models. In this work, we present a high-quality Instruction-based Video\nEditing dataset with 1M triplets, namely InsViE-1M. We first curate\nhigh-resolution and high-quality source videos and images, then design an\neffective editing-filtering pipeline to construct high-quality editing triplets\nfor model training. For a source video, we generate multiple edited samples of\nits first frame with different intensities of classifier-free guidance, which\nare automatically filtered by GPT-4o with carefully crafted guidelines. The\nedited first frame is propagated to subsequent frames to produce the edited\nvideo, followed by another round of filtering for frame quality and motion\nevaluation. We also generate and filter a variety of video editing triplets\nfrom high-quality images. With the InsViE-1M dataset, we propose a multi-stage\nlearning strategy to train our InsViE model, progressively enhancing its\ninstruction following and editing ability. Extensive experiments demonstrate\nthe advantages of our InsViE-1M dataset and the trained model over\nstate-of-the-art works. Codes are available at\n\\href{https://github.com/langmanbusi/InsViE}{InsViE}.\n", "link": "http://arxiv.org/abs/2503.20287v2", "date": "2025-07-11", "relevancy": 2.044, "topK": [{"title": "Magic-Me: Identity-Specific Video Customized Diffusion", "link": "http://arxiv.org/abs/2402.09368v1", "similarity": 0.5252}, {"title": "Customize-A-Video: One-Shot Motion Customization of Text-to-Video\n  Diffusion Models", "link": "http://arxiv.org/abs/2402.14780v1", "similarity": 0.5195}, {"title": "Tuning-Free Noise Rectification for High Fidelity Image-to-Video\n  Generation", "link": "http://arxiv.org/abs/2403.02827v1", "similarity": 0.4934}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20InsViE-1M%3A%20Effective%20Instruction-based%20Video%20Editing%20with%20Elaborate%0A%20%20Dataset%20Construction&body=Title%3A%20InsViE-1M%3A%20Effective%20Instruction-based%20Video%20Editing%20with%20Elaborate%0A%20%20Dataset%20Construction%0AAuthor%3A%20Yuhui%20Wu%20and%20Liyi%20Chen%20and%20Ruibin%20Li%20and%20Shihao%20Wang%20and%20Chenxi%20Xie%20and%20Lei%20Zhang%0AAbstract%3A%20%20%20Instruction-based%20video%20editing%20allows%20effective%20and%20interactive%20editing%20of%0Avideos%20using%20only%20instructions%20without%20extra%20inputs%20such%20as%20masks%20or%0Aattributes.%20However%2C%20collecting%20high-quality%20training%20triplets%20%28source%20video%2C%0Aedited%20video%2C%20instruction%29%20is%20a%20challenging%20task.%20Existing%20datasets%20mostly%0Aconsist%20of%20low-resolution%2C%20short%20duration%2C%20and%20limited%20amount%20of%20source%20videos%0Awith%20unsatisfactory%20editing%20quality%2C%20limiting%20the%20performance%20of%20trained%0Aediting%20models.%20In%20this%20work%2C%20we%20present%20a%20high-quality%20Instruction-based%20Video%0AEditing%20dataset%20with%201M%20triplets%2C%20namely%20InsViE-1M.%20We%20first%20curate%0Ahigh-resolution%20and%20high-quality%20source%20videos%20and%20images%2C%20then%20design%20an%0Aeffective%20editing-filtering%20pipeline%20to%20construct%20high-quality%20editing%20triplets%0Afor%20model%20training.%20For%20a%20source%20video%2C%20we%20generate%20multiple%20edited%20samples%20of%0Aits%20first%20frame%20with%20different%20intensities%20of%20classifier-free%20guidance%2C%20which%0Aare%20automatically%20filtered%20by%20GPT-4o%20with%20carefully%20crafted%20guidelines.%20The%0Aedited%20first%20frame%20is%20propagated%20to%20subsequent%20frames%20to%20produce%20the%20edited%0Avideo%2C%20followed%20by%20another%20round%20of%20filtering%20for%20frame%20quality%20and%20motion%0Aevaluation.%20We%20also%20generate%20and%20filter%20a%20variety%20of%20video%20editing%20triplets%0Afrom%20high-quality%20images.%20With%20the%20InsViE-1M%20dataset%2C%20we%20propose%20a%20multi-stage%0Alearning%20strategy%20to%20train%20our%20InsViE%20model%2C%20progressively%20enhancing%20its%0Ainstruction%20following%20and%20editing%20ability.%20Extensive%20experiments%20demonstrate%0Athe%20advantages%20of%20our%20InsViE-1M%20dataset%20and%20the%20trained%20model%20over%0Astate-of-the-art%20works.%20Codes%20are%20available%20at%0A%5Chref%7Bhttps%3A//github.com/langmanbusi/InsViE%7D%7BInsViE%7D.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2503.20287v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DInsViE-1M%253A%2520Effective%2520Instruction-based%2520Video%2520Editing%2520with%2520Elaborate%250A%2520%2520Dataset%2520Construction%26entry.906535625%3DYuhui%2520Wu%2520and%2520Liyi%2520Chen%2520and%2520Ruibin%2520Li%2520and%2520Shihao%2520Wang%2520and%2520Chenxi%2520Xie%2520and%2520Lei%2520Zhang%26entry.1292438233%3D%2520%2520Instruction-based%2520video%2520editing%2520allows%2520effective%2520and%2520interactive%2520editing%2520of%250Avideos%2520using%2520only%2520instructions%2520without%2520extra%2520inputs%2520such%2520as%2520masks%2520or%250Aattributes.%2520However%252C%2520collecting%2520high-quality%2520training%2520triplets%2520%2528source%2520video%252C%250Aedited%2520video%252C%2520instruction%2529%2520is%2520a%2520challenging%2520task.%2520Existing%2520datasets%2520mostly%250Aconsist%2520of%2520low-resolution%252C%2520short%2520duration%252C%2520and%2520limited%2520amount%2520of%2520source%2520videos%250Awith%2520unsatisfactory%2520editing%2520quality%252C%2520limiting%2520the%2520performance%2520of%2520trained%250Aediting%2520models.%2520In%2520this%2520work%252C%2520we%2520present%2520a%2520high-quality%2520Instruction-based%2520Video%250AEditing%2520dataset%2520with%25201M%2520triplets%252C%2520namely%2520InsViE-1M.%2520We%2520first%2520curate%250Ahigh-resolution%2520and%2520high-quality%2520source%2520videos%2520and%2520images%252C%2520then%2520design%2520an%250Aeffective%2520editing-filtering%2520pipeline%2520to%2520construct%2520high-quality%2520editing%2520triplets%250Afor%2520model%2520training.%2520For%2520a%2520source%2520video%252C%2520we%2520generate%2520multiple%2520edited%2520samples%2520of%250Aits%2520first%2520frame%2520with%2520different%2520intensities%2520of%2520classifier-free%2520guidance%252C%2520which%250Aare%2520automatically%2520filtered%2520by%2520GPT-4o%2520with%2520carefully%2520crafted%2520guidelines.%2520The%250Aedited%2520first%2520frame%2520is%2520propagated%2520to%2520subsequent%2520frames%2520to%2520produce%2520the%2520edited%250Avideo%252C%2520followed%2520by%2520another%2520round%2520of%2520filtering%2520for%2520frame%2520quality%2520and%2520motion%250Aevaluation.%2520We%2520also%2520generate%2520and%2520filter%2520a%2520variety%2520of%2520video%2520editing%2520triplets%250Afrom%2520high-quality%2520images.%2520With%2520the%2520InsViE-1M%2520dataset%252C%2520we%2520propose%2520a%2520multi-stage%250Alearning%2520strategy%2520to%2520train%2520our%2520InsViE%2520model%252C%2520progressively%2520enhancing%2520its%250Ainstruction%2520following%2520and%2520editing%2520ability.%2520Extensive%2520experiments%2520demonstrate%250Athe%2520advantages%2520of%2520our%2520InsViE-1M%2520dataset%2520and%2520the%2520trained%2520model%2520over%250Astate-of-the-art%2520works.%2520Codes%2520are%2520available%2520at%250A%255Chref%257Bhttps%253A//github.com/langmanbusi/InsViE%257D%257BInsViE%257D.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2503.20287v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=InsViE-1M%3A%20Effective%20Instruction-based%20Video%20Editing%20with%20Elaborate%0A%20%20Dataset%20Construction&entry.906535625=Yuhui%20Wu%20and%20Liyi%20Chen%20and%20Ruibin%20Li%20and%20Shihao%20Wang%20and%20Chenxi%20Xie%20and%20Lei%20Zhang&entry.1292438233=%20%20Instruction-based%20video%20editing%20allows%20effective%20and%20interactive%20editing%20of%0Avideos%20using%20only%20instructions%20without%20extra%20inputs%20such%20as%20masks%20or%0Aattributes.%20However%2C%20collecting%20high-quality%20training%20triplets%20%28source%20video%2C%0Aedited%20video%2C%20instruction%29%20is%20a%20challenging%20task.%20Existing%20datasets%20mostly%0Aconsist%20of%20low-resolution%2C%20short%20duration%2C%20and%20limited%20amount%20of%20source%20videos%0Awith%20unsatisfactory%20editing%20quality%2C%20limiting%20the%20performance%20of%20trained%0Aediting%20models.%20In%20this%20work%2C%20we%20present%20a%20high-quality%20Instruction-based%20Video%0AEditing%20dataset%20with%201M%20triplets%2C%20namely%20InsViE-1M.%20We%20first%20curate%0Ahigh-resolution%20and%20high-quality%20source%20videos%20and%20images%2C%20then%20design%20an%0Aeffective%20editing-filtering%20pipeline%20to%20construct%20high-quality%20editing%20triplets%0Afor%20model%20training.%20For%20a%20source%20video%2C%20we%20generate%20multiple%20edited%20samples%20of%0Aits%20first%20frame%20with%20different%20intensities%20of%20classifier-free%20guidance%2C%20which%0Aare%20automatically%20filtered%20by%20GPT-4o%20with%20carefully%20crafted%20guidelines.%20The%0Aedited%20first%20frame%20is%20propagated%20to%20subsequent%20frames%20to%20produce%20the%20edited%0Avideo%2C%20followed%20by%20another%20round%20of%20filtering%20for%20frame%20quality%20and%20motion%0Aevaluation.%20We%20also%20generate%20and%20filter%20a%20variety%20of%20video%20editing%20triplets%0Afrom%20high-quality%20images.%20With%20the%20InsViE-1M%20dataset%2C%20we%20propose%20a%20multi-stage%0Alearning%20strategy%20to%20train%20our%20InsViE%20model%2C%20progressively%20enhancing%20its%0Ainstruction%20following%20and%20editing%20ability.%20Extensive%20experiments%20demonstrate%0Athe%20advantages%20of%20our%20InsViE-1M%20dataset%20and%20the%20trained%20model%20over%0Astate-of-the-art%20works.%20Codes%20are%20available%20at%0A%5Chref%7Bhttps%3A//github.com/langmanbusi/InsViE%7D%7BInsViE%7D.%0A&entry.1838667208=http%3A//arxiv.org/abs/2503.20287v2&entry.124074799=Read"},
{"title": "BlockFFN: Towards End-Side Acceleration-Friendly Mixture-of-Experts with\n  Chunk-Level Activation Sparsity", "author": "Chenyang Song and Weilin Zhao and Xu Han and Chaojun Xiao and Yingfa Chen and Yuxuan Li and Zhiyuan Liu and Maosong Sun", "abstract": "  To alleviate the computational burden of large language models (LLMs),\narchitectures with activation sparsity, represented by mixture-of-experts\n(MoE), have attracted increasing attention. However, the non-differentiable and\ninflexible routing of vanilla MoE hurts model performance. Moreover, while each\ntoken activates only a few parameters, these sparsely-activated architectures\nexhibit low chunk-level sparsity, indicating that the union of multiple\nconsecutive tokens activates a large ratio of parameters. Such a sparsity\npattern is unfriendly for acceleration under low-resource conditions (e.g.,\nend-side devices) and incompatible with mainstream acceleration techniques\n(e.g., speculative decoding). To address these challenges, we introduce a novel\nMoE architecture, BlockFFN, as well as its efficient training and deployment\ntechniques. Specifically, we use a router integrating ReLU activation and\nRMSNorm for differentiable and flexible routing. Next, to promote both\ntoken-level sparsity (TLS) and chunk-level sparsity (CLS), CLS-aware training\nobjectives are designed, making BlockFFN more acceleration-friendly. Finally,\nwe implement efficient acceleration kernels, combining activation sparsity and\nspeculative decoding for the first time. The experimental results demonstrate\nthe superior performance of BlockFFN over other MoE baselines, achieving over\n80% TLS and 70% 8-token CLS. Our kernels achieve up to 3.67$\\times$ speedup on\nreal end-side devices than dense models. All codes and checkpoints are\navailable publicly (https://github.com/thunlp/BlockFFN).\n", "link": "http://arxiv.org/abs/2507.08771v1", "date": "2025-07-11", "relevancy": 2.0399, "topK": [{"title": "ToDo: Token Downsampling for Efficient Generation of High-Resolution\n  Images", "link": "http://arxiv.org/abs/2402.13573v2", "similarity": 0.5237}, {"title": "SDXS: Real-Time One-Step Latent Diffusion Models with Image Conditions", "link": "http://arxiv.org/abs/2403.16627v1", "similarity": 0.508}, {"title": "Boosted dynamic neural networks", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/26302", "similarity": 0.497}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20BlockFFN%3A%20Towards%20End-Side%20Acceleration-Friendly%20Mixture-of-Experts%20with%0A%20%20Chunk-Level%20Activation%20Sparsity&body=Title%3A%20BlockFFN%3A%20Towards%20End-Side%20Acceleration-Friendly%20Mixture-of-Experts%20with%0A%20%20Chunk-Level%20Activation%20Sparsity%0AAuthor%3A%20Chenyang%20Song%20and%20Weilin%20Zhao%20and%20Xu%20Han%20and%20Chaojun%20Xiao%20and%20Yingfa%20Chen%20and%20Yuxuan%20Li%20and%20Zhiyuan%20Liu%20and%20Maosong%20Sun%0AAbstract%3A%20%20%20To%20alleviate%20the%20computational%20burden%20of%20large%20language%20models%20%28LLMs%29%2C%0Aarchitectures%20with%20activation%20sparsity%2C%20represented%20by%20mixture-of-experts%0A%28MoE%29%2C%20have%20attracted%20increasing%20attention.%20However%2C%20the%20non-differentiable%20and%0Ainflexible%20routing%20of%20vanilla%20MoE%20hurts%20model%20performance.%20Moreover%2C%20while%20each%0Atoken%20activates%20only%20a%20few%20parameters%2C%20these%20sparsely-activated%20architectures%0Aexhibit%20low%20chunk-level%20sparsity%2C%20indicating%20that%20the%20union%20of%20multiple%0Aconsecutive%20tokens%20activates%20a%20large%20ratio%20of%20parameters.%20Such%20a%20sparsity%0Apattern%20is%20unfriendly%20for%20acceleration%20under%20low-resource%20conditions%20%28e.g.%2C%0Aend-side%20devices%29%20and%20incompatible%20with%20mainstream%20acceleration%20techniques%0A%28e.g.%2C%20speculative%20decoding%29.%20To%20address%20these%20challenges%2C%20we%20introduce%20a%20novel%0AMoE%20architecture%2C%20BlockFFN%2C%20as%20well%20as%20its%20efficient%20training%20and%20deployment%0Atechniques.%20Specifically%2C%20we%20use%20a%20router%20integrating%20ReLU%20activation%20and%0ARMSNorm%20for%20differentiable%20and%20flexible%20routing.%20Next%2C%20to%20promote%20both%0Atoken-level%20sparsity%20%28TLS%29%20and%20chunk-level%20sparsity%20%28CLS%29%2C%20CLS-aware%20training%0Aobjectives%20are%20designed%2C%20making%20BlockFFN%20more%20acceleration-friendly.%20Finally%2C%0Awe%20implement%20efficient%20acceleration%20kernels%2C%20combining%20activation%20sparsity%20and%0Aspeculative%20decoding%20for%20the%20first%20time.%20The%20experimental%20results%20demonstrate%0Athe%20superior%20performance%20of%20BlockFFN%20over%20other%20MoE%20baselines%2C%20achieving%20over%0A80%25%20TLS%20and%2070%25%208-token%20CLS.%20Our%20kernels%20achieve%20up%20to%203.67%24%5Ctimes%24%20speedup%20on%0Areal%20end-side%20devices%20than%20dense%20models.%20All%20codes%20and%20checkpoints%20are%0Aavailable%20publicly%20%28https%3A//github.com/thunlp/BlockFFN%29.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2507.08771v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DBlockFFN%253A%2520Towards%2520End-Side%2520Acceleration-Friendly%2520Mixture-of-Experts%2520with%250A%2520%2520Chunk-Level%2520Activation%2520Sparsity%26entry.906535625%3DChenyang%2520Song%2520and%2520Weilin%2520Zhao%2520and%2520Xu%2520Han%2520and%2520Chaojun%2520Xiao%2520and%2520Yingfa%2520Chen%2520and%2520Yuxuan%2520Li%2520and%2520Zhiyuan%2520Liu%2520and%2520Maosong%2520Sun%26entry.1292438233%3D%2520%2520To%2520alleviate%2520the%2520computational%2520burden%2520of%2520large%2520language%2520models%2520%2528LLMs%2529%252C%250Aarchitectures%2520with%2520activation%2520sparsity%252C%2520represented%2520by%2520mixture-of-experts%250A%2528MoE%2529%252C%2520have%2520attracted%2520increasing%2520attention.%2520However%252C%2520the%2520non-differentiable%2520and%250Ainflexible%2520routing%2520of%2520vanilla%2520MoE%2520hurts%2520model%2520performance.%2520Moreover%252C%2520while%2520each%250Atoken%2520activates%2520only%2520a%2520few%2520parameters%252C%2520these%2520sparsely-activated%2520architectures%250Aexhibit%2520low%2520chunk-level%2520sparsity%252C%2520indicating%2520that%2520the%2520union%2520of%2520multiple%250Aconsecutive%2520tokens%2520activates%2520a%2520large%2520ratio%2520of%2520parameters.%2520Such%2520a%2520sparsity%250Apattern%2520is%2520unfriendly%2520for%2520acceleration%2520under%2520low-resource%2520conditions%2520%2528e.g.%252C%250Aend-side%2520devices%2529%2520and%2520incompatible%2520with%2520mainstream%2520acceleration%2520techniques%250A%2528e.g.%252C%2520speculative%2520decoding%2529.%2520To%2520address%2520these%2520challenges%252C%2520we%2520introduce%2520a%2520novel%250AMoE%2520architecture%252C%2520BlockFFN%252C%2520as%2520well%2520as%2520its%2520efficient%2520training%2520and%2520deployment%250Atechniques.%2520Specifically%252C%2520we%2520use%2520a%2520router%2520integrating%2520ReLU%2520activation%2520and%250ARMSNorm%2520for%2520differentiable%2520and%2520flexible%2520routing.%2520Next%252C%2520to%2520promote%2520both%250Atoken-level%2520sparsity%2520%2528TLS%2529%2520and%2520chunk-level%2520sparsity%2520%2528CLS%2529%252C%2520CLS-aware%2520training%250Aobjectives%2520are%2520designed%252C%2520making%2520BlockFFN%2520more%2520acceleration-friendly.%2520Finally%252C%250Awe%2520implement%2520efficient%2520acceleration%2520kernels%252C%2520combining%2520activation%2520sparsity%2520and%250Aspeculative%2520decoding%2520for%2520the%2520first%2520time.%2520The%2520experimental%2520results%2520demonstrate%250Athe%2520superior%2520performance%2520of%2520BlockFFN%2520over%2520other%2520MoE%2520baselines%252C%2520achieving%2520over%250A80%2525%2520TLS%2520and%252070%2525%25208-token%2520CLS.%2520Our%2520kernels%2520achieve%2520up%2520to%25203.67%2524%255Ctimes%2524%2520speedup%2520on%250Areal%2520end-side%2520devices%2520than%2520dense%2520models.%2520All%2520codes%2520and%2520checkpoints%2520are%250Aavailable%2520publicly%2520%2528https%253A//github.com/thunlp/BlockFFN%2529.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2507.08771v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=BlockFFN%3A%20Towards%20End-Side%20Acceleration-Friendly%20Mixture-of-Experts%20with%0A%20%20Chunk-Level%20Activation%20Sparsity&entry.906535625=Chenyang%20Song%20and%20Weilin%20Zhao%20and%20Xu%20Han%20and%20Chaojun%20Xiao%20and%20Yingfa%20Chen%20and%20Yuxuan%20Li%20and%20Zhiyuan%20Liu%20and%20Maosong%20Sun&entry.1292438233=%20%20To%20alleviate%20the%20computational%20burden%20of%20large%20language%20models%20%28LLMs%29%2C%0Aarchitectures%20with%20activation%20sparsity%2C%20represented%20by%20mixture-of-experts%0A%28MoE%29%2C%20have%20attracted%20increasing%20attention.%20However%2C%20the%20non-differentiable%20and%0Ainflexible%20routing%20of%20vanilla%20MoE%20hurts%20model%20performance.%20Moreover%2C%20while%20each%0Atoken%20activates%20only%20a%20few%20parameters%2C%20these%20sparsely-activated%20architectures%0Aexhibit%20low%20chunk-level%20sparsity%2C%20indicating%20that%20the%20union%20of%20multiple%0Aconsecutive%20tokens%20activates%20a%20large%20ratio%20of%20parameters.%20Such%20a%20sparsity%0Apattern%20is%20unfriendly%20for%20acceleration%20under%20low-resource%20conditions%20%28e.g.%2C%0Aend-side%20devices%29%20and%20incompatible%20with%20mainstream%20acceleration%20techniques%0A%28e.g.%2C%20speculative%20decoding%29.%20To%20address%20these%20challenges%2C%20we%20introduce%20a%20novel%0AMoE%20architecture%2C%20BlockFFN%2C%20as%20well%20as%20its%20efficient%20training%20and%20deployment%0Atechniques.%20Specifically%2C%20we%20use%20a%20router%20integrating%20ReLU%20activation%20and%0ARMSNorm%20for%20differentiable%20and%20flexible%20routing.%20Next%2C%20to%20promote%20both%0Atoken-level%20sparsity%20%28TLS%29%20and%20chunk-level%20sparsity%20%28CLS%29%2C%20CLS-aware%20training%0Aobjectives%20are%20designed%2C%20making%20BlockFFN%20more%20acceleration-friendly.%20Finally%2C%0Awe%20implement%20efficient%20acceleration%20kernels%2C%20combining%20activation%20sparsity%20and%0Aspeculative%20decoding%20for%20the%20first%20time.%20The%20experimental%20results%20demonstrate%0Athe%20superior%20performance%20of%20BlockFFN%20over%20other%20MoE%20baselines%2C%20achieving%20over%0A80%25%20TLS%20and%2070%25%208-token%20CLS.%20Our%20kernels%20achieve%20up%20to%203.67%24%5Ctimes%24%20speedup%20on%0Areal%20end-side%20devices%20than%20dense%20models.%20All%20codes%20and%20checkpoints%20are%0Aavailable%20publicly%20%28https%3A//github.com/thunlp/BlockFFN%29.%0A&entry.1838667208=http%3A//arxiv.org/abs/2507.08771v1&entry.124074799=Read"},
{"title": "Greedy Low-Rank Gradient Compression for Distributed Learning with\n  Convergence Guarantees", "author": "Chuyan Chen and Yutong He and Pengrui Li and Weichen Jia and Kun Yuan", "abstract": "  Distributed optimization is pivotal for large-scale signal processing and\nmachine learning, yet communication overhead remains a major bottleneck.\nLow-rank gradient compression, in which the transmitted gradients are\napproximated by low-rank matrices to reduce communication, offers a promising\nremedy. Existing methods typically adopt either randomized or greedy\ncompression strategies: randomized approaches project gradients onto randomly\nchosen subspaces, introducing high variance and degrading empirical\nperformance; greedy methods select the most informative subspaces, achieving\nstrong empirical results but lacking convergence guarantees. To address this\ngap, we propose GreedyLore--the first Greedy Low-Rank gradient compression\nalgorithm for distributed learning with rigorous convergence guarantees.\nGreedyLore incorporates error feedback to correct the bias introduced by greedy\ncompression and introduces a semi-lazy subspace update that ensures the\ncompression operator remains contractive throughout all iterations. With these\ntechniques, we prove that GreedyLore achieves a convergence rate of\n$\\mathcal{O}(\\sigma/\\sqrt{NT} + 1/T)$ under standard optimizers such as MSGD\nand Adam--marking the first linear speedup convergence rate for low-rank\ngradient compression. Extensive experiments are conducted to validate our\ntheoretical findings.\n", "link": "http://arxiv.org/abs/2507.08784v1", "date": "2025-07-11", "relevancy": 2.0204, "topK": [{"title": "RTG-SLAM: Real-time 3D Reconstruction at Scale using Gaussian Splatting", "link": "http://arxiv.org/abs/2404.19706v1", "similarity": 0.5202}, {"title": "Self-supervised Photographic Image Layout Representation Learning", "link": "http://arxiv.org/abs/2403.03740v1", "similarity": 0.4965}, {"title": "SDXS: Real-Time One-Step Latent Diffusion Models with Image Conditions", "link": "http://arxiv.org/abs/2403.16627v1", "similarity": 0.4889}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Greedy%20Low-Rank%20Gradient%20Compression%20for%20Distributed%20Learning%20with%0A%20%20Convergence%20Guarantees&body=Title%3A%20Greedy%20Low-Rank%20Gradient%20Compression%20for%20Distributed%20Learning%20with%0A%20%20Convergence%20Guarantees%0AAuthor%3A%20Chuyan%20Chen%20and%20Yutong%20He%20and%20Pengrui%20Li%20and%20Weichen%20Jia%20and%20Kun%20Yuan%0AAbstract%3A%20%20%20Distributed%20optimization%20is%20pivotal%20for%20large-scale%20signal%20processing%20and%0Amachine%20learning%2C%20yet%20communication%20overhead%20remains%20a%20major%20bottleneck.%0ALow-rank%20gradient%20compression%2C%20in%20which%20the%20transmitted%20gradients%20are%0Aapproximated%20by%20low-rank%20matrices%20to%20reduce%20communication%2C%20offers%20a%20promising%0Aremedy.%20Existing%20methods%20typically%20adopt%20either%20randomized%20or%20greedy%0Acompression%20strategies%3A%20randomized%20approaches%20project%20gradients%20onto%20randomly%0Achosen%20subspaces%2C%20introducing%20high%20variance%20and%20degrading%20empirical%0Aperformance%3B%20greedy%20methods%20select%20the%20most%20informative%20subspaces%2C%20achieving%0Astrong%20empirical%20results%20but%20lacking%20convergence%20guarantees.%20To%20address%20this%0Agap%2C%20we%20propose%20GreedyLore--the%20first%20Greedy%20Low-Rank%20gradient%20compression%0Aalgorithm%20for%20distributed%20learning%20with%20rigorous%20convergence%20guarantees.%0AGreedyLore%20incorporates%20error%20feedback%20to%20correct%20the%20bias%20introduced%20by%20greedy%0Acompression%20and%20introduces%20a%20semi-lazy%20subspace%20update%20that%20ensures%20the%0Acompression%20operator%20remains%20contractive%20throughout%20all%20iterations.%20With%20these%0Atechniques%2C%20we%20prove%20that%20GreedyLore%20achieves%20a%20convergence%20rate%20of%0A%24%5Cmathcal%7BO%7D%28%5Csigma/%5Csqrt%7BNT%7D%20%2B%201/T%29%24%20under%20standard%20optimizers%20such%20as%20MSGD%0Aand%20Adam--marking%20the%20first%20linear%20speedup%20convergence%20rate%20for%20low-rank%0Agradient%20compression.%20Extensive%20experiments%20are%20conducted%20to%20validate%20our%0Atheoretical%20findings.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2507.08784v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DGreedy%2520Low-Rank%2520Gradient%2520Compression%2520for%2520Distributed%2520Learning%2520with%250A%2520%2520Convergence%2520Guarantees%26entry.906535625%3DChuyan%2520Chen%2520and%2520Yutong%2520He%2520and%2520Pengrui%2520Li%2520and%2520Weichen%2520Jia%2520and%2520Kun%2520Yuan%26entry.1292438233%3D%2520%2520Distributed%2520optimization%2520is%2520pivotal%2520for%2520large-scale%2520signal%2520processing%2520and%250Amachine%2520learning%252C%2520yet%2520communication%2520overhead%2520remains%2520a%2520major%2520bottleneck.%250ALow-rank%2520gradient%2520compression%252C%2520in%2520which%2520the%2520transmitted%2520gradients%2520are%250Aapproximated%2520by%2520low-rank%2520matrices%2520to%2520reduce%2520communication%252C%2520offers%2520a%2520promising%250Aremedy.%2520Existing%2520methods%2520typically%2520adopt%2520either%2520randomized%2520or%2520greedy%250Acompression%2520strategies%253A%2520randomized%2520approaches%2520project%2520gradients%2520onto%2520randomly%250Achosen%2520subspaces%252C%2520introducing%2520high%2520variance%2520and%2520degrading%2520empirical%250Aperformance%253B%2520greedy%2520methods%2520select%2520the%2520most%2520informative%2520subspaces%252C%2520achieving%250Astrong%2520empirical%2520results%2520but%2520lacking%2520convergence%2520guarantees.%2520To%2520address%2520this%250Agap%252C%2520we%2520propose%2520GreedyLore--the%2520first%2520Greedy%2520Low-Rank%2520gradient%2520compression%250Aalgorithm%2520for%2520distributed%2520learning%2520with%2520rigorous%2520convergence%2520guarantees.%250AGreedyLore%2520incorporates%2520error%2520feedback%2520to%2520correct%2520the%2520bias%2520introduced%2520by%2520greedy%250Acompression%2520and%2520introduces%2520a%2520semi-lazy%2520subspace%2520update%2520that%2520ensures%2520the%250Acompression%2520operator%2520remains%2520contractive%2520throughout%2520all%2520iterations.%2520With%2520these%250Atechniques%252C%2520we%2520prove%2520that%2520GreedyLore%2520achieves%2520a%2520convergence%2520rate%2520of%250A%2524%255Cmathcal%257BO%257D%2528%255Csigma/%255Csqrt%257BNT%257D%2520%252B%25201/T%2529%2524%2520under%2520standard%2520optimizers%2520such%2520as%2520MSGD%250Aand%2520Adam--marking%2520the%2520first%2520linear%2520speedup%2520convergence%2520rate%2520for%2520low-rank%250Agradient%2520compression.%2520Extensive%2520experiments%2520are%2520conducted%2520to%2520validate%2520our%250Atheoretical%2520findings.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2507.08784v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Greedy%20Low-Rank%20Gradient%20Compression%20for%20Distributed%20Learning%20with%0A%20%20Convergence%20Guarantees&entry.906535625=Chuyan%20Chen%20and%20Yutong%20He%20and%20Pengrui%20Li%20and%20Weichen%20Jia%20and%20Kun%20Yuan&entry.1292438233=%20%20Distributed%20optimization%20is%20pivotal%20for%20large-scale%20signal%20processing%20and%0Amachine%20learning%2C%20yet%20communication%20overhead%20remains%20a%20major%20bottleneck.%0ALow-rank%20gradient%20compression%2C%20in%20which%20the%20transmitted%20gradients%20are%0Aapproximated%20by%20low-rank%20matrices%20to%20reduce%20communication%2C%20offers%20a%20promising%0Aremedy.%20Existing%20methods%20typically%20adopt%20either%20randomized%20or%20greedy%0Acompression%20strategies%3A%20randomized%20approaches%20project%20gradients%20onto%20randomly%0Achosen%20subspaces%2C%20introducing%20high%20variance%20and%20degrading%20empirical%0Aperformance%3B%20greedy%20methods%20select%20the%20most%20informative%20subspaces%2C%20achieving%0Astrong%20empirical%20results%20but%20lacking%20convergence%20guarantees.%20To%20address%20this%0Agap%2C%20we%20propose%20GreedyLore--the%20first%20Greedy%20Low-Rank%20gradient%20compression%0Aalgorithm%20for%20distributed%20learning%20with%20rigorous%20convergence%20guarantees.%0AGreedyLore%20incorporates%20error%20feedback%20to%20correct%20the%20bias%20introduced%20by%20greedy%0Acompression%20and%20introduces%20a%20semi-lazy%20subspace%20update%20that%20ensures%20the%0Acompression%20operator%20remains%20contractive%20throughout%20all%20iterations.%20With%20these%0Atechniques%2C%20we%20prove%20that%20GreedyLore%20achieves%20a%20convergence%20rate%20of%0A%24%5Cmathcal%7BO%7D%28%5Csigma/%5Csqrt%7BNT%7D%20%2B%201/T%29%24%20under%20standard%20optimizers%20such%20as%20MSGD%0Aand%20Adam--marking%20the%20first%20linear%20speedup%20convergence%20rate%20for%20low-rank%0Agradient%20compression.%20Extensive%20experiments%20are%20conducted%20to%20validate%20our%0Atheoretical%20findings.%0A&entry.1838667208=http%3A//arxiv.org/abs/2507.08784v1&entry.124074799=Read"},
{"title": "Attribution assignment for deep-generative sequence models enables\n  interpretability analysis using positive-only data", "author": "Robert Frank and Michael Widrich and Rahmad Akbar and G\u00fcnter Klambauer and Geir Kjetil Sandve and Philippe A. Robert and Victor Greiff", "abstract": "  Generative machine learning models offer a powerful framework for therapeutic\ndesign by efficiently exploring large spaces of biological sequences enriched\nfor desirable properties. Unlike supervised learning methods, which require\nboth positive and negative labeled data, generative models such as LSTMs can be\ntrained solely on positively labeled sequences, for example, high-affinity\nantibodies. This is particularly advantageous in biological settings where\nnegative data are scarce, unreliable, or biologically ill-defined. However, the\nlack of attribution methods for generative models has hindered the ability to\nextract interpretable biological insights from such models. To address this\ngap, we developed Generative Attribution Metric Analysis (GAMA), an attribution\nmethod for autoregressive generative models based on Integrated Gradients. We\nassessed GAMA using synthetic datasets with known ground truths to characterize\nits statistical behavior and validate its ability to recover biologically\nrelevant features. We further demonstrated the utility of GAMA by applying it\nto experimental antibody-antigen binding data. GAMA enables model\ninterpretability and the validation of generative sequence design strategies\nwithout the need for negative training data.\n", "link": "http://arxiv.org/abs/2506.23182v2", "date": "2025-07-11", "relevancy": 1.9962, "topK": [{"title": "Genie: Generative Interactive Environments", "link": "http://arxiv.org/abs/2402.15391v1", "similarity": 0.5084}, {"title": "Thinking Outside the BBox: Unconstrained Generative Object Compositing", "link": "http://arxiv.org/abs/2409.04559v2", "similarity": 0.493}, {"title": "UGG: Unified Generative Grasping", "link": "https://arxiv.org/abs/2311.16917", "similarity": 0.4922}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Attribution%20assignment%20for%20deep-generative%20sequence%20models%20enables%0A%20%20interpretability%20analysis%20using%20positive-only%20data&body=Title%3A%20Attribution%20assignment%20for%20deep-generative%20sequence%20models%20enables%0A%20%20interpretability%20analysis%20using%20positive-only%20data%0AAuthor%3A%20Robert%20Frank%20and%20Michael%20Widrich%20and%20Rahmad%20Akbar%20and%20G%C3%BCnter%20Klambauer%20and%20Geir%20Kjetil%20Sandve%20and%20Philippe%20A.%20Robert%20and%20Victor%20Greiff%0AAbstract%3A%20%20%20Generative%20machine%20learning%20models%20offer%20a%20powerful%20framework%20for%20therapeutic%0Adesign%20by%20efficiently%20exploring%20large%20spaces%20of%20biological%20sequences%20enriched%0Afor%20desirable%20properties.%20Unlike%20supervised%20learning%20methods%2C%20which%20require%0Aboth%20positive%20and%20negative%20labeled%20data%2C%20generative%20models%20such%20as%20LSTMs%20can%20be%0Atrained%20solely%20on%20positively%20labeled%20sequences%2C%20for%20example%2C%20high-affinity%0Aantibodies.%20This%20is%20particularly%20advantageous%20in%20biological%20settings%20where%0Anegative%20data%20are%20scarce%2C%20unreliable%2C%20or%20biologically%20ill-defined.%20However%2C%20the%0Alack%20of%20attribution%20methods%20for%20generative%20models%20has%20hindered%20the%20ability%20to%0Aextract%20interpretable%20biological%20insights%20from%20such%20models.%20To%20address%20this%0Agap%2C%20we%20developed%20Generative%20Attribution%20Metric%20Analysis%20%28GAMA%29%2C%20an%20attribution%0Amethod%20for%20autoregressive%20generative%20models%20based%20on%20Integrated%20Gradients.%20We%0Aassessed%20GAMA%20using%20synthetic%20datasets%20with%20known%20ground%20truths%20to%20characterize%0Aits%20statistical%20behavior%20and%20validate%20its%20ability%20to%20recover%20biologically%0Arelevant%20features.%20We%20further%20demonstrated%20the%20utility%20of%20GAMA%20by%20applying%20it%0Ato%20experimental%20antibody-antigen%20binding%20data.%20GAMA%20enables%20model%0Ainterpretability%20and%20the%20validation%20of%20generative%20sequence%20design%20strategies%0Awithout%20the%20need%20for%20negative%20training%20data.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2506.23182v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DAttribution%2520assignment%2520for%2520deep-generative%2520sequence%2520models%2520enables%250A%2520%2520interpretability%2520analysis%2520using%2520positive-only%2520data%26entry.906535625%3DRobert%2520Frank%2520and%2520Michael%2520Widrich%2520and%2520Rahmad%2520Akbar%2520and%2520G%25C3%25BCnter%2520Klambauer%2520and%2520Geir%2520Kjetil%2520Sandve%2520and%2520Philippe%2520A.%2520Robert%2520and%2520Victor%2520Greiff%26entry.1292438233%3D%2520%2520Generative%2520machine%2520learning%2520models%2520offer%2520a%2520powerful%2520framework%2520for%2520therapeutic%250Adesign%2520by%2520efficiently%2520exploring%2520large%2520spaces%2520of%2520biological%2520sequences%2520enriched%250Afor%2520desirable%2520properties.%2520Unlike%2520supervised%2520learning%2520methods%252C%2520which%2520require%250Aboth%2520positive%2520and%2520negative%2520labeled%2520data%252C%2520generative%2520models%2520such%2520as%2520LSTMs%2520can%2520be%250Atrained%2520solely%2520on%2520positively%2520labeled%2520sequences%252C%2520for%2520example%252C%2520high-affinity%250Aantibodies.%2520This%2520is%2520particularly%2520advantageous%2520in%2520biological%2520settings%2520where%250Anegative%2520data%2520are%2520scarce%252C%2520unreliable%252C%2520or%2520biologically%2520ill-defined.%2520However%252C%2520the%250Alack%2520of%2520attribution%2520methods%2520for%2520generative%2520models%2520has%2520hindered%2520the%2520ability%2520to%250Aextract%2520interpretable%2520biological%2520insights%2520from%2520such%2520models.%2520To%2520address%2520this%250Agap%252C%2520we%2520developed%2520Generative%2520Attribution%2520Metric%2520Analysis%2520%2528GAMA%2529%252C%2520an%2520attribution%250Amethod%2520for%2520autoregressive%2520generative%2520models%2520based%2520on%2520Integrated%2520Gradients.%2520We%250Aassessed%2520GAMA%2520using%2520synthetic%2520datasets%2520with%2520known%2520ground%2520truths%2520to%2520characterize%250Aits%2520statistical%2520behavior%2520and%2520validate%2520its%2520ability%2520to%2520recover%2520biologically%250Arelevant%2520features.%2520We%2520further%2520demonstrated%2520the%2520utility%2520of%2520GAMA%2520by%2520applying%2520it%250Ato%2520experimental%2520antibody-antigen%2520binding%2520data.%2520GAMA%2520enables%2520model%250Ainterpretability%2520and%2520the%2520validation%2520of%2520generative%2520sequence%2520design%2520strategies%250Awithout%2520the%2520need%2520for%2520negative%2520training%2520data.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2506.23182v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Attribution%20assignment%20for%20deep-generative%20sequence%20models%20enables%0A%20%20interpretability%20analysis%20using%20positive-only%20data&entry.906535625=Robert%20Frank%20and%20Michael%20Widrich%20and%20Rahmad%20Akbar%20and%20G%C3%BCnter%20Klambauer%20and%20Geir%20Kjetil%20Sandve%20and%20Philippe%20A.%20Robert%20and%20Victor%20Greiff&entry.1292438233=%20%20Generative%20machine%20learning%20models%20offer%20a%20powerful%20framework%20for%20therapeutic%0Adesign%20by%20efficiently%20exploring%20large%20spaces%20of%20biological%20sequences%20enriched%0Afor%20desirable%20properties.%20Unlike%20supervised%20learning%20methods%2C%20which%20require%0Aboth%20positive%20and%20negative%20labeled%20data%2C%20generative%20models%20such%20as%20LSTMs%20can%20be%0Atrained%20solely%20on%20positively%20labeled%20sequences%2C%20for%20example%2C%20high-affinity%0Aantibodies.%20This%20is%20particularly%20advantageous%20in%20biological%20settings%20where%0Anegative%20data%20are%20scarce%2C%20unreliable%2C%20or%20biologically%20ill-defined.%20However%2C%20the%0Alack%20of%20attribution%20methods%20for%20generative%20models%20has%20hindered%20the%20ability%20to%0Aextract%20interpretable%20biological%20insights%20from%20such%20models.%20To%20address%20this%0Agap%2C%20we%20developed%20Generative%20Attribution%20Metric%20Analysis%20%28GAMA%29%2C%20an%20attribution%0Amethod%20for%20autoregressive%20generative%20models%20based%20on%20Integrated%20Gradients.%20We%0Aassessed%20GAMA%20using%20synthetic%20datasets%20with%20known%20ground%20truths%20to%20characterize%0Aits%20statistical%20behavior%20and%20validate%20its%20ability%20to%20recover%20biologically%0Arelevant%20features.%20We%20further%20demonstrated%20the%20utility%20of%20GAMA%20by%20applying%20it%0Ato%20experimental%20antibody-antigen%20binding%20data.%20GAMA%20enables%20model%0Ainterpretability%20and%20the%20validation%20of%20generative%20sequence%20design%20strategies%0Awithout%20the%20need%20for%20negative%20training%20data.%0A&entry.1838667208=http%3A//arxiv.org/abs/2506.23182v2&entry.124074799=Read"},
{"title": "Introspection of Thought Helps AI Agents", "author": "Haoran Sun and Shaoning Zeng", "abstract": "  AI Agents rely on Large Language Models (LLMs) and Multimodal-LLMs (MLLMs) to\nperform interpretation and inference in text and image tasks without\npost-training, where LLMs and MLLMs play the most critical role and determine\nthe initial ability and limitations of AI Agents. Usually, AI Agents utilize\nsophisticated prompt engineering and external reasoning framework to obtain a\npromising interaction with LLMs, e.g., Chain-of-Thought, Iteration of Thought\nand Image-of-Thought. However, they are still constrained by the inherent\nlimitations of LLM in understanding natural language, and the iterative\nreasoning process will generate a large amount of inference cost. To this end,\nwe propose a novel AI Agent Reasoning Framework with Introspection of Thought\n(INoT) by designing a new LLM-Read code in prompt. It enables LLM to execute\nprogrammatic dialogue reasoning processes following the code in prompt.\nTherefore, self-denial and reflection occur within LLM instead of outside LLM,\nwhich can reduce token cost effectively. Through our experiments on six\nbenchmarks for three different tasks, the effectiveness of INoT is verified,\nwith an average improvement of 7.95\\% in performance, exceeding the baselines.\nFurthermore, the token cost of INoT is lower on average than the best\nperforming method at baseline by 58.3\\%. In addition, we demonstrate the\nversatility of INoT in image interpretation and inference through verification\nexperiments.\n", "link": "http://arxiv.org/abs/2507.08664v1", "date": "2025-07-11", "relevancy": 1.9931, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5018}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5018}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.4807}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Introspection%20of%20Thought%20Helps%20AI%20Agents&body=Title%3A%20Introspection%20of%20Thought%20Helps%20AI%20Agents%0AAuthor%3A%20Haoran%20Sun%20and%20Shaoning%20Zeng%0AAbstract%3A%20%20%20AI%20Agents%20rely%20on%20Large%20Language%20Models%20%28LLMs%29%20and%20Multimodal-LLMs%20%28MLLMs%29%20to%0Aperform%20interpretation%20and%20inference%20in%20text%20and%20image%20tasks%20without%0Apost-training%2C%20where%20LLMs%20and%20MLLMs%20play%20the%20most%20critical%20role%20and%20determine%0Athe%20initial%20ability%20and%20limitations%20of%20AI%20Agents.%20Usually%2C%20AI%20Agents%20utilize%0Asophisticated%20prompt%20engineering%20and%20external%20reasoning%20framework%20to%20obtain%20a%0Apromising%20interaction%20with%20LLMs%2C%20e.g.%2C%20Chain-of-Thought%2C%20Iteration%20of%20Thought%0Aand%20Image-of-Thought.%20However%2C%20they%20are%20still%20constrained%20by%20the%20inherent%0Alimitations%20of%20LLM%20in%20understanding%20natural%20language%2C%20and%20the%20iterative%0Areasoning%20process%20will%20generate%20a%20large%20amount%20of%20inference%20cost.%20To%20this%20end%2C%0Awe%20propose%20a%20novel%20AI%20Agent%20Reasoning%20Framework%20with%20Introspection%20of%20Thought%0A%28INoT%29%20by%20designing%20a%20new%20LLM-Read%20code%20in%20prompt.%20It%20enables%20LLM%20to%20execute%0Aprogrammatic%20dialogue%20reasoning%20processes%20following%20the%20code%20in%20prompt.%0ATherefore%2C%20self-denial%20and%20reflection%20occur%20within%20LLM%20instead%20of%20outside%20LLM%2C%0Awhich%20can%20reduce%20token%20cost%20effectively.%20Through%20our%20experiments%20on%20six%0Abenchmarks%20for%20three%20different%20tasks%2C%20the%20effectiveness%20of%20INoT%20is%20verified%2C%0Awith%20an%20average%20improvement%20of%207.95%5C%25%20in%20performance%2C%20exceeding%20the%20baselines.%0AFurthermore%2C%20the%20token%20cost%20of%20INoT%20is%20lower%20on%20average%20than%20the%20best%0Aperforming%20method%20at%20baseline%20by%2058.3%5C%25.%20In%20addition%2C%20we%20demonstrate%20the%0Aversatility%20of%20INoT%20in%20image%20interpretation%20and%20inference%20through%20verification%0Aexperiments.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2507.08664v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DIntrospection%2520of%2520Thought%2520Helps%2520AI%2520Agents%26entry.906535625%3DHaoran%2520Sun%2520and%2520Shaoning%2520Zeng%26entry.1292438233%3D%2520%2520AI%2520Agents%2520rely%2520on%2520Large%2520Language%2520Models%2520%2528LLMs%2529%2520and%2520Multimodal-LLMs%2520%2528MLLMs%2529%2520to%250Aperform%2520interpretation%2520and%2520inference%2520in%2520text%2520and%2520image%2520tasks%2520without%250Apost-training%252C%2520where%2520LLMs%2520and%2520MLLMs%2520play%2520the%2520most%2520critical%2520role%2520and%2520determine%250Athe%2520initial%2520ability%2520and%2520limitations%2520of%2520AI%2520Agents.%2520Usually%252C%2520AI%2520Agents%2520utilize%250Asophisticated%2520prompt%2520engineering%2520and%2520external%2520reasoning%2520framework%2520to%2520obtain%2520a%250Apromising%2520interaction%2520with%2520LLMs%252C%2520e.g.%252C%2520Chain-of-Thought%252C%2520Iteration%2520of%2520Thought%250Aand%2520Image-of-Thought.%2520However%252C%2520they%2520are%2520still%2520constrained%2520by%2520the%2520inherent%250Alimitations%2520of%2520LLM%2520in%2520understanding%2520natural%2520language%252C%2520and%2520the%2520iterative%250Areasoning%2520process%2520will%2520generate%2520a%2520large%2520amount%2520of%2520inference%2520cost.%2520To%2520this%2520end%252C%250Awe%2520propose%2520a%2520novel%2520AI%2520Agent%2520Reasoning%2520Framework%2520with%2520Introspection%2520of%2520Thought%250A%2528INoT%2529%2520by%2520designing%2520a%2520new%2520LLM-Read%2520code%2520in%2520prompt.%2520It%2520enables%2520LLM%2520to%2520execute%250Aprogrammatic%2520dialogue%2520reasoning%2520processes%2520following%2520the%2520code%2520in%2520prompt.%250ATherefore%252C%2520self-denial%2520and%2520reflection%2520occur%2520within%2520LLM%2520instead%2520of%2520outside%2520LLM%252C%250Awhich%2520can%2520reduce%2520token%2520cost%2520effectively.%2520Through%2520our%2520experiments%2520on%2520six%250Abenchmarks%2520for%2520three%2520different%2520tasks%252C%2520the%2520effectiveness%2520of%2520INoT%2520is%2520verified%252C%250Awith%2520an%2520average%2520improvement%2520of%25207.95%255C%2525%2520in%2520performance%252C%2520exceeding%2520the%2520baselines.%250AFurthermore%252C%2520the%2520token%2520cost%2520of%2520INoT%2520is%2520lower%2520on%2520average%2520than%2520the%2520best%250Aperforming%2520method%2520at%2520baseline%2520by%252058.3%255C%2525.%2520In%2520addition%252C%2520we%2520demonstrate%2520the%250Aversatility%2520of%2520INoT%2520in%2520image%2520interpretation%2520and%2520inference%2520through%2520verification%250Aexperiments.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2507.08664v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Introspection%20of%20Thought%20Helps%20AI%20Agents&entry.906535625=Haoran%20Sun%20and%20Shaoning%20Zeng&entry.1292438233=%20%20AI%20Agents%20rely%20on%20Large%20Language%20Models%20%28LLMs%29%20and%20Multimodal-LLMs%20%28MLLMs%29%20to%0Aperform%20interpretation%20and%20inference%20in%20text%20and%20image%20tasks%20without%0Apost-training%2C%20where%20LLMs%20and%20MLLMs%20play%20the%20most%20critical%20role%20and%20determine%0Athe%20initial%20ability%20and%20limitations%20of%20AI%20Agents.%20Usually%2C%20AI%20Agents%20utilize%0Asophisticated%20prompt%20engineering%20and%20external%20reasoning%20framework%20to%20obtain%20a%0Apromising%20interaction%20with%20LLMs%2C%20e.g.%2C%20Chain-of-Thought%2C%20Iteration%20of%20Thought%0Aand%20Image-of-Thought.%20However%2C%20they%20are%20still%20constrained%20by%20the%20inherent%0Alimitations%20of%20LLM%20in%20understanding%20natural%20language%2C%20and%20the%20iterative%0Areasoning%20process%20will%20generate%20a%20large%20amount%20of%20inference%20cost.%20To%20this%20end%2C%0Awe%20propose%20a%20novel%20AI%20Agent%20Reasoning%20Framework%20with%20Introspection%20of%20Thought%0A%28INoT%29%20by%20designing%20a%20new%20LLM-Read%20code%20in%20prompt.%20It%20enables%20LLM%20to%20execute%0Aprogrammatic%20dialogue%20reasoning%20processes%20following%20the%20code%20in%20prompt.%0ATherefore%2C%20self-denial%20and%20reflection%20occur%20within%20LLM%20instead%20of%20outside%20LLM%2C%0Awhich%20can%20reduce%20token%20cost%20effectively.%20Through%20our%20experiments%20on%20six%0Abenchmarks%20for%20three%20different%20tasks%2C%20the%20effectiveness%20of%20INoT%20is%20verified%2C%0Awith%20an%20average%20improvement%20of%207.95%5C%25%20in%20performance%2C%20exceeding%20the%20baselines.%0AFurthermore%2C%20the%20token%20cost%20of%20INoT%20is%20lower%20on%20average%20than%20the%20best%0Aperforming%20method%20at%20baseline%20by%2058.3%5C%25.%20In%20addition%2C%20we%20demonstrate%20the%0Aversatility%20of%20INoT%20in%20image%20interpretation%20and%20inference%20through%20verification%0Aexperiments.%0A&entry.1838667208=http%3A//arxiv.org/abs/2507.08664v1&entry.124074799=Read"},
{"title": "Safe Deep Reinforcement Learning for Resource Allocation with Peak Age\n  of Information Violation Guarantees", "author": "Berire Gunes Reyhan and Sinem Coleri", "abstract": "  In Wireless Networked Control Systems (WNCSs), control and communication\nsystems must be co-designed due to their strong interdependence. This paper\npresents a novel optimization theory-based safe deep reinforcement learning\n(DRL) framework for ultra-reliable WNCSs, ensuring constraint satisfaction\nwhile optimizing performance, for the first time in the literature. The\napproach minimizes power consumption under key constraints, including Peak Age\nof Information (PAoI) violation probability, transmit power, and schedulability\nin the finite blocklength regime. PAoI violation probability is uniquely\nderived by combining stochastic maximum allowable transfer interval (MATI) and\nmaximum allowable packet delay (MAD) constraints in a multi-sensor network. The\nframework consists of two stages: optimization theory and safe DRL. The first\nstage derives optimality conditions to establish mathematical relationships\namong variables, simplifying and decomposing the problem. The second stage\nemploys a safe DRL model where a teacher-student framework guides the DRL agent\n(student). The control mechanism (teacher) evaluates compliance with system\nconstraints and suggests the nearest feasible action when needed. Extensive\nsimulations show that the proposed framework outperforms rule-based and other\noptimization theory based DRL benchmarks, achieving faster convergence, higher\nrewards, and greater stability.\n", "link": "http://arxiv.org/abs/2507.08653v1", "date": "2025-07-11", "relevancy": 1.9834, "topK": [{"title": "Safe and efficient training of a control agent", "link": "https://patents.google.com/patent/US11709462B2/en", "similarity": 0.5589}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.4686}, {"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.4437}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Safe%20Deep%20Reinforcement%20Learning%20for%20Resource%20Allocation%20with%20Peak%20Age%0A%20%20of%20Information%20Violation%20Guarantees&body=Title%3A%20Safe%20Deep%20Reinforcement%20Learning%20for%20Resource%20Allocation%20with%20Peak%20Age%0A%20%20of%20Information%20Violation%20Guarantees%0AAuthor%3A%20Berire%20Gunes%20Reyhan%20and%20Sinem%20Coleri%0AAbstract%3A%20%20%20In%20Wireless%20Networked%20Control%20Systems%20%28WNCSs%29%2C%20control%20and%20communication%0Asystems%20must%20be%20co-designed%20due%20to%20their%20strong%20interdependence.%20This%20paper%0Apresents%20a%20novel%20optimization%20theory-based%20safe%20deep%20reinforcement%20learning%0A%28DRL%29%20framework%20for%20ultra-reliable%20WNCSs%2C%20ensuring%20constraint%20satisfaction%0Awhile%20optimizing%20performance%2C%20for%20the%20first%20time%20in%20the%20literature.%20The%0Aapproach%20minimizes%20power%20consumption%20under%20key%20constraints%2C%20including%20Peak%20Age%0Aof%20Information%20%28PAoI%29%20violation%20probability%2C%20transmit%20power%2C%20and%20schedulability%0Ain%20the%20finite%20blocklength%20regime.%20PAoI%20violation%20probability%20is%20uniquely%0Aderived%20by%20combining%20stochastic%20maximum%20allowable%20transfer%20interval%20%28MATI%29%20and%0Amaximum%20allowable%20packet%20delay%20%28MAD%29%20constraints%20in%20a%20multi-sensor%20network.%20The%0Aframework%20consists%20of%20two%20stages%3A%20optimization%20theory%20and%20safe%20DRL.%20The%20first%0Astage%20derives%20optimality%20conditions%20to%20establish%20mathematical%20relationships%0Aamong%20variables%2C%20simplifying%20and%20decomposing%20the%20problem.%20The%20second%20stage%0Aemploys%20a%20safe%20DRL%20model%20where%20a%20teacher-student%20framework%20guides%20the%20DRL%20agent%0A%28student%29.%20The%20control%20mechanism%20%28teacher%29%20evaluates%20compliance%20with%20system%0Aconstraints%20and%20suggests%20the%20nearest%20feasible%20action%20when%20needed.%20Extensive%0Asimulations%20show%20that%20the%20proposed%20framework%20outperforms%20rule-based%20and%20other%0Aoptimization%20theory%20based%20DRL%20benchmarks%2C%20achieving%20faster%20convergence%2C%20higher%0Arewards%2C%20and%20greater%20stability.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2507.08653v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DSafe%2520Deep%2520Reinforcement%2520Learning%2520for%2520Resource%2520Allocation%2520with%2520Peak%2520Age%250A%2520%2520of%2520Information%2520Violation%2520Guarantees%26entry.906535625%3DBerire%2520Gunes%2520Reyhan%2520and%2520Sinem%2520Coleri%26entry.1292438233%3D%2520%2520In%2520Wireless%2520Networked%2520Control%2520Systems%2520%2528WNCSs%2529%252C%2520control%2520and%2520communication%250Asystems%2520must%2520be%2520co-designed%2520due%2520to%2520their%2520strong%2520interdependence.%2520This%2520paper%250Apresents%2520a%2520novel%2520optimization%2520theory-based%2520safe%2520deep%2520reinforcement%2520learning%250A%2528DRL%2529%2520framework%2520for%2520ultra-reliable%2520WNCSs%252C%2520ensuring%2520constraint%2520satisfaction%250Awhile%2520optimizing%2520performance%252C%2520for%2520the%2520first%2520time%2520in%2520the%2520literature.%2520The%250Aapproach%2520minimizes%2520power%2520consumption%2520under%2520key%2520constraints%252C%2520including%2520Peak%2520Age%250Aof%2520Information%2520%2528PAoI%2529%2520violation%2520probability%252C%2520transmit%2520power%252C%2520and%2520schedulability%250Ain%2520the%2520finite%2520blocklength%2520regime.%2520PAoI%2520violation%2520probability%2520is%2520uniquely%250Aderived%2520by%2520combining%2520stochastic%2520maximum%2520allowable%2520transfer%2520interval%2520%2528MATI%2529%2520and%250Amaximum%2520allowable%2520packet%2520delay%2520%2528MAD%2529%2520constraints%2520in%2520a%2520multi-sensor%2520network.%2520The%250Aframework%2520consists%2520of%2520two%2520stages%253A%2520optimization%2520theory%2520and%2520safe%2520DRL.%2520The%2520first%250Astage%2520derives%2520optimality%2520conditions%2520to%2520establish%2520mathematical%2520relationships%250Aamong%2520variables%252C%2520simplifying%2520and%2520decomposing%2520the%2520problem.%2520The%2520second%2520stage%250Aemploys%2520a%2520safe%2520DRL%2520model%2520where%2520a%2520teacher-student%2520framework%2520guides%2520the%2520DRL%2520agent%250A%2528student%2529.%2520The%2520control%2520mechanism%2520%2528teacher%2529%2520evaluates%2520compliance%2520with%2520system%250Aconstraints%2520and%2520suggests%2520the%2520nearest%2520feasible%2520action%2520when%2520needed.%2520Extensive%250Asimulations%2520show%2520that%2520the%2520proposed%2520framework%2520outperforms%2520rule-based%2520and%2520other%250Aoptimization%2520theory%2520based%2520DRL%2520benchmarks%252C%2520achieving%2520faster%2520convergence%252C%2520higher%250Arewards%252C%2520and%2520greater%2520stability.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2507.08653v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Safe%20Deep%20Reinforcement%20Learning%20for%20Resource%20Allocation%20with%20Peak%20Age%0A%20%20of%20Information%20Violation%20Guarantees&entry.906535625=Berire%20Gunes%20Reyhan%20and%20Sinem%20Coleri&entry.1292438233=%20%20In%20Wireless%20Networked%20Control%20Systems%20%28WNCSs%29%2C%20control%20and%20communication%0Asystems%20must%20be%20co-designed%20due%20to%20their%20strong%20interdependence.%20This%20paper%0Apresents%20a%20novel%20optimization%20theory-based%20safe%20deep%20reinforcement%20learning%0A%28DRL%29%20framework%20for%20ultra-reliable%20WNCSs%2C%20ensuring%20constraint%20satisfaction%0Awhile%20optimizing%20performance%2C%20for%20the%20first%20time%20in%20the%20literature.%20The%0Aapproach%20minimizes%20power%20consumption%20under%20key%20constraints%2C%20including%20Peak%20Age%0Aof%20Information%20%28PAoI%29%20violation%20probability%2C%20transmit%20power%2C%20and%20schedulability%0Ain%20the%20finite%20blocklength%20regime.%20PAoI%20violation%20probability%20is%20uniquely%0Aderived%20by%20combining%20stochastic%20maximum%20allowable%20transfer%20interval%20%28MATI%29%20and%0Amaximum%20allowable%20packet%20delay%20%28MAD%29%20constraints%20in%20a%20multi-sensor%20network.%20The%0Aframework%20consists%20of%20two%20stages%3A%20optimization%20theory%20and%20safe%20DRL.%20The%20first%0Astage%20derives%20optimality%20conditions%20to%20establish%20mathematical%20relationships%0Aamong%20variables%2C%20simplifying%20and%20decomposing%20the%20problem.%20The%20second%20stage%0Aemploys%20a%20safe%20DRL%20model%20where%20a%20teacher-student%20framework%20guides%20the%20DRL%20agent%0A%28student%29.%20The%20control%20mechanism%20%28teacher%29%20evaluates%20compliance%20with%20system%0Aconstraints%20and%20suggests%20the%20nearest%20feasible%20action%20when%20needed.%20Extensive%0Asimulations%20show%20that%20the%20proposed%20framework%20outperforms%20rule-based%20and%20other%0Aoptimization%20theory%20based%20DRL%20benchmarks%2C%20achieving%20faster%20convergence%2C%20higher%0Arewards%2C%20and%20greater%20stability.%0A&entry.1838667208=http%3A//arxiv.org/abs/2507.08653v1&entry.124074799=Read"},
{"title": "Leanabell-Prover-V2: Verifier-integrated Reasoning for Formal Theorem\n  Proving via Reinforcement Learning", "author": "Xingguang Ji and Yahui Liu and Qi Wang and Jingyuan Zhang and Yang Yue and Rui Shi and Chenxi Sun and Fuzheng Zhang and Guorui Zhou and Kun Gai", "abstract": "  We introduce our Leanabell-Prover-V2, a 7B large language models (LLMs) that\ncan produce formal theorem proofs in Lean 4, with verifier-integrated Long\nChain-of-Thoughts (CoT). Following our previous work Leanabell-Prover-V1, we\ncontinual to choose to posttrain existing strong prover models for further\nperformance improvement. In our V2 version, we mainly upgrade the Reinforcement\nLearning (RL) with feedback provided by the Lean 4 verifier. Crucially,\nverifier feedback, such as indicating success or detailing specific errors,\nallows the LLM to become ``self-aware'' of the correctness of its own reasoning\nprocess and learn to reflexively correct errors. Leanabell-Prover-V2 directly\noptimizes LLM reasoning trajectories with multi-turn verifier interactions,\ntogether with feedback token masking for stable RL training and a simple reward\nstrategy. Experiments show that Leanabell-Prover-V2 improves performance by\n3.2% (pass@128) with Kimina-Prover-Preview-Distill-7B and 2.0% (pass@128) with\nDeepSeek-Prover-V2-7B on the MiniF2F test set. The source codes, curated data\nand models are available at:\nhttps://github.com/Leanabell-LM/Leanabell-Prover-V2.\n", "link": "http://arxiv.org/abs/2507.08649v1", "date": "2025-07-11", "relevancy": 1.983, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5027}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5027}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.4609}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Leanabell-Prover-V2%3A%20Verifier-integrated%20Reasoning%20for%20Formal%20Theorem%0A%20%20Proving%20via%20Reinforcement%20Learning&body=Title%3A%20Leanabell-Prover-V2%3A%20Verifier-integrated%20Reasoning%20for%20Formal%20Theorem%0A%20%20Proving%20via%20Reinforcement%20Learning%0AAuthor%3A%20Xingguang%20Ji%20and%20Yahui%20Liu%20and%20Qi%20Wang%20and%20Jingyuan%20Zhang%20and%20Yang%20Yue%20and%20Rui%20Shi%20and%20Chenxi%20Sun%20and%20Fuzheng%20Zhang%20and%20Guorui%20Zhou%20and%20Kun%20Gai%0AAbstract%3A%20%20%20We%20introduce%20our%20Leanabell-Prover-V2%2C%20a%207B%20large%20language%20models%20%28LLMs%29%20that%0Acan%20produce%20formal%20theorem%20proofs%20in%20Lean%204%2C%20with%20verifier-integrated%20Long%0AChain-of-Thoughts%20%28CoT%29.%20Following%20our%20previous%20work%20Leanabell-Prover-V1%2C%20we%0Acontinual%20to%20choose%20to%20posttrain%20existing%20strong%20prover%20models%20for%20further%0Aperformance%20improvement.%20In%20our%20V2%20version%2C%20we%20mainly%20upgrade%20the%20Reinforcement%0ALearning%20%28RL%29%20with%20feedback%20provided%20by%20the%20Lean%204%20verifier.%20Crucially%2C%0Averifier%20feedback%2C%20such%20as%20indicating%20success%20or%20detailing%20specific%20errors%2C%0Aallows%20the%20LLM%20to%20become%20%60%60self-aware%27%27%20of%20the%20correctness%20of%20its%20own%20reasoning%0Aprocess%20and%20learn%20to%20reflexively%20correct%20errors.%20Leanabell-Prover-V2%20directly%0Aoptimizes%20LLM%20reasoning%20trajectories%20with%20multi-turn%20verifier%20interactions%2C%0Atogether%20with%20feedback%20token%20masking%20for%20stable%20RL%20training%20and%20a%20simple%20reward%0Astrategy.%20Experiments%20show%20that%20Leanabell-Prover-V2%20improves%20performance%20by%0A3.2%25%20%28pass%40128%29%20with%20Kimina-Prover-Preview-Distill-7B%20and%202.0%25%20%28pass%40128%29%20with%0ADeepSeek-Prover-V2-7B%20on%20the%20MiniF2F%20test%20set.%20The%20source%20codes%2C%20curated%20data%0Aand%20models%20are%20available%20at%3A%0Ahttps%3A//github.com/Leanabell-LM/Leanabell-Prover-V2.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2507.08649v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DLeanabell-Prover-V2%253A%2520Verifier-integrated%2520Reasoning%2520for%2520Formal%2520Theorem%250A%2520%2520Proving%2520via%2520Reinforcement%2520Learning%26entry.906535625%3DXingguang%2520Ji%2520and%2520Yahui%2520Liu%2520and%2520Qi%2520Wang%2520and%2520Jingyuan%2520Zhang%2520and%2520Yang%2520Yue%2520and%2520Rui%2520Shi%2520and%2520Chenxi%2520Sun%2520and%2520Fuzheng%2520Zhang%2520and%2520Guorui%2520Zhou%2520and%2520Kun%2520Gai%26entry.1292438233%3D%2520%2520We%2520introduce%2520our%2520Leanabell-Prover-V2%252C%2520a%25207B%2520large%2520language%2520models%2520%2528LLMs%2529%2520that%250Acan%2520produce%2520formal%2520theorem%2520proofs%2520in%2520Lean%25204%252C%2520with%2520verifier-integrated%2520Long%250AChain-of-Thoughts%2520%2528CoT%2529.%2520Following%2520our%2520previous%2520work%2520Leanabell-Prover-V1%252C%2520we%250Acontinual%2520to%2520choose%2520to%2520posttrain%2520existing%2520strong%2520prover%2520models%2520for%2520further%250Aperformance%2520improvement.%2520In%2520our%2520V2%2520version%252C%2520we%2520mainly%2520upgrade%2520the%2520Reinforcement%250ALearning%2520%2528RL%2529%2520with%2520feedback%2520provided%2520by%2520the%2520Lean%25204%2520verifier.%2520Crucially%252C%250Averifier%2520feedback%252C%2520such%2520as%2520indicating%2520success%2520or%2520detailing%2520specific%2520errors%252C%250Aallows%2520the%2520LLM%2520to%2520become%2520%2560%2560self-aware%2527%2527%2520of%2520the%2520correctness%2520of%2520its%2520own%2520reasoning%250Aprocess%2520and%2520learn%2520to%2520reflexively%2520correct%2520errors.%2520Leanabell-Prover-V2%2520directly%250Aoptimizes%2520LLM%2520reasoning%2520trajectories%2520with%2520multi-turn%2520verifier%2520interactions%252C%250Atogether%2520with%2520feedback%2520token%2520masking%2520for%2520stable%2520RL%2520training%2520and%2520a%2520simple%2520reward%250Astrategy.%2520Experiments%2520show%2520that%2520Leanabell-Prover-V2%2520improves%2520performance%2520by%250A3.2%2525%2520%2528pass%2540128%2529%2520with%2520Kimina-Prover-Preview-Distill-7B%2520and%25202.0%2525%2520%2528pass%2540128%2529%2520with%250ADeepSeek-Prover-V2-7B%2520on%2520the%2520MiniF2F%2520test%2520set.%2520The%2520source%2520codes%252C%2520curated%2520data%250Aand%2520models%2520are%2520available%2520at%253A%250Ahttps%253A//github.com/Leanabell-LM/Leanabell-Prover-V2.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2507.08649v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Leanabell-Prover-V2%3A%20Verifier-integrated%20Reasoning%20for%20Formal%20Theorem%0A%20%20Proving%20via%20Reinforcement%20Learning&entry.906535625=Xingguang%20Ji%20and%20Yahui%20Liu%20and%20Qi%20Wang%20and%20Jingyuan%20Zhang%20and%20Yang%20Yue%20and%20Rui%20Shi%20and%20Chenxi%20Sun%20and%20Fuzheng%20Zhang%20and%20Guorui%20Zhou%20and%20Kun%20Gai&entry.1292438233=%20%20We%20introduce%20our%20Leanabell-Prover-V2%2C%20a%207B%20large%20language%20models%20%28LLMs%29%20that%0Acan%20produce%20formal%20theorem%20proofs%20in%20Lean%204%2C%20with%20verifier-integrated%20Long%0AChain-of-Thoughts%20%28CoT%29.%20Following%20our%20previous%20work%20Leanabell-Prover-V1%2C%20we%0Acontinual%20to%20choose%20to%20posttrain%20existing%20strong%20prover%20models%20for%20further%0Aperformance%20improvement.%20In%20our%20V2%20version%2C%20we%20mainly%20upgrade%20the%20Reinforcement%0ALearning%20%28RL%29%20with%20feedback%20provided%20by%20the%20Lean%204%20verifier.%20Crucially%2C%0Averifier%20feedback%2C%20such%20as%20indicating%20success%20or%20detailing%20specific%20errors%2C%0Aallows%20the%20LLM%20to%20become%20%60%60self-aware%27%27%20of%20the%20correctness%20of%20its%20own%20reasoning%0Aprocess%20and%20learn%20to%20reflexively%20correct%20errors.%20Leanabell-Prover-V2%20directly%0Aoptimizes%20LLM%20reasoning%20trajectories%20with%20multi-turn%20verifier%20interactions%2C%0Atogether%20with%20feedback%20token%20masking%20for%20stable%20RL%20training%20and%20a%20simple%20reward%0Astrategy.%20Experiments%20show%20that%20Leanabell-Prover-V2%20improves%20performance%20by%0A3.2%25%20%28pass%40128%29%20with%20Kimina-Prover-Preview-Distill-7B%20and%202.0%25%20%28pass%40128%29%20with%0ADeepSeek-Prover-V2-7B%20on%20the%20MiniF2F%20test%20set.%20The%20source%20codes%2C%20curated%20data%0Aand%20models%20are%20available%20at%3A%0Ahttps%3A//github.com/Leanabell-LM/Leanabell-Prover-V2.%0A&entry.1838667208=http%3A//arxiv.org/abs/2507.08649v1&entry.124074799=Read"},
{"title": "The Non-Linear Representation Dilemma: Is Causal Abstraction Enough for\n  Mechanistic Interpretability?", "author": "Denis Sutter and Julian Minder and Thomas Hofmann and Tiago Pimentel", "abstract": "  The concept of causal abstraction got recently popularised to demystify the\nopaque decision-making processes of machine learning models; in short, a neural\nnetwork can be abstracted as a higher-level algorithm if there exists a\nfunction which allows us to map between them. Notably, most interpretability\npapers implement these maps as linear functions, motivated by the linear\nrepresentation hypothesis: the idea that features are encoded linearly in a\nmodel's representations. However, this linearity constraint is not required by\nthe definition of causal abstraction. In this work, we critically examine the\nconcept of causal abstraction by considering arbitrarily powerful alignment\nmaps. In particular, we prove that under reasonable assumptions, any neural\nnetwork can be mapped to any algorithm, rendering this unrestricted notion of\ncausal abstraction trivial and uninformative. We complement these theoretical\nfindings with empirical evidence, demonstrating that it is possible to\nperfectly map models to algorithms even when these models are incapable of\nsolving the actual task; e.g., on an experiment using randomly initialised\nlanguage models, our alignment maps reach 100% interchange-intervention\naccuracy on the indirect object identification task. This raises the non-linear\nrepresentation dilemma: if we lift the linearity constraint imposed to\nalignment maps in causal abstraction analyses, we are left with no principled\nway to balance the inherent trade-off between these maps' complexity and\naccuracy. Together, these results suggest an answer to our title's question:\ncausal abstraction is not enough for mechanistic interpretability, as it\nbecomes vacuous without assumptions about how models encode information.\nStudying the connection between this information-encoding assumption and causal\nabstraction should lead to exciting future work.\n", "link": "http://arxiv.org/abs/2507.08802v1", "date": "2025-07-11", "relevancy": 1.9823, "topK": [{"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.5252}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.4896}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.4896}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20The%20Non-Linear%20Representation%20Dilemma%3A%20Is%20Causal%20Abstraction%20Enough%20for%0A%20%20Mechanistic%20Interpretability%3F&body=Title%3A%20The%20Non-Linear%20Representation%20Dilemma%3A%20Is%20Causal%20Abstraction%20Enough%20for%0A%20%20Mechanistic%20Interpretability%3F%0AAuthor%3A%20Denis%20Sutter%20and%20Julian%20Minder%20and%20Thomas%20Hofmann%20and%20Tiago%20Pimentel%0AAbstract%3A%20%20%20The%20concept%20of%20causal%20abstraction%20got%20recently%20popularised%20to%20demystify%20the%0Aopaque%20decision-making%20processes%20of%20machine%20learning%20models%3B%20in%20short%2C%20a%20neural%0Anetwork%20can%20be%20abstracted%20as%20a%20higher-level%20algorithm%20if%20there%20exists%20a%0Afunction%20which%20allows%20us%20to%20map%20between%20them.%20Notably%2C%20most%20interpretability%0Apapers%20implement%20these%20maps%20as%20linear%20functions%2C%20motivated%20by%20the%20linear%0Arepresentation%20hypothesis%3A%20the%20idea%20that%20features%20are%20encoded%20linearly%20in%20a%0Amodel%27s%20representations.%20However%2C%20this%20linearity%20constraint%20is%20not%20required%20by%0Athe%20definition%20of%20causal%20abstraction.%20In%20this%20work%2C%20we%20critically%20examine%20the%0Aconcept%20of%20causal%20abstraction%20by%20considering%20arbitrarily%20powerful%20alignment%0Amaps.%20In%20particular%2C%20we%20prove%20that%20under%20reasonable%20assumptions%2C%20any%20neural%0Anetwork%20can%20be%20mapped%20to%20any%20algorithm%2C%20rendering%20this%20unrestricted%20notion%20of%0Acausal%20abstraction%20trivial%20and%20uninformative.%20We%20complement%20these%20theoretical%0Afindings%20with%20empirical%20evidence%2C%20demonstrating%20that%20it%20is%20possible%20to%0Aperfectly%20map%20models%20to%20algorithms%20even%20when%20these%20models%20are%20incapable%20of%0Asolving%20the%20actual%20task%3B%20e.g.%2C%20on%20an%20experiment%20using%20randomly%20initialised%0Alanguage%20models%2C%20our%20alignment%20maps%20reach%20100%25%20interchange-intervention%0Aaccuracy%20on%20the%20indirect%20object%20identification%20task.%20This%20raises%20the%20non-linear%0Arepresentation%20dilemma%3A%20if%20we%20lift%20the%20linearity%20constraint%20imposed%20to%0Aalignment%20maps%20in%20causal%20abstraction%20analyses%2C%20we%20are%20left%20with%20no%20principled%0Away%20to%20balance%20the%20inherent%20trade-off%20between%20these%20maps%27%20complexity%20and%0Aaccuracy.%20Together%2C%20these%20results%20suggest%20an%20answer%20to%20our%20title%27s%20question%3A%0Acausal%20abstraction%20is%20not%20enough%20for%20mechanistic%20interpretability%2C%20as%20it%0Abecomes%20vacuous%20without%20assumptions%20about%20how%20models%20encode%20information.%0AStudying%20the%20connection%20between%20this%20information-encoding%20assumption%20and%20causal%0Aabstraction%20should%20lead%20to%20exciting%20future%20work.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2507.08802v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DThe%2520Non-Linear%2520Representation%2520Dilemma%253A%2520Is%2520Causal%2520Abstraction%2520Enough%2520for%250A%2520%2520Mechanistic%2520Interpretability%253F%26entry.906535625%3DDenis%2520Sutter%2520and%2520Julian%2520Minder%2520and%2520Thomas%2520Hofmann%2520and%2520Tiago%2520Pimentel%26entry.1292438233%3D%2520%2520The%2520concept%2520of%2520causal%2520abstraction%2520got%2520recently%2520popularised%2520to%2520demystify%2520the%250Aopaque%2520decision-making%2520processes%2520of%2520machine%2520learning%2520models%253B%2520in%2520short%252C%2520a%2520neural%250Anetwork%2520can%2520be%2520abstracted%2520as%2520a%2520higher-level%2520algorithm%2520if%2520there%2520exists%2520a%250Afunction%2520which%2520allows%2520us%2520to%2520map%2520between%2520them.%2520Notably%252C%2520most%2520interpretability%250Apapers%2520implement%2520these%2520maps%2520as%2520linear%2520functions%252C%2520motivated%2520by%2520the%2520linear%250Arepresentation%2520hypothesis%253A%2520the%2520idea%2520that%2520features%2520are%2520encoded%2520linearly%2520in%2520a%250Amodel%2527s%2520representations.%2520However%252C%2520this%2520linearity%2520constraint%2520is%2520not%2520required%2520by%250Athe%2520definition%2520of%2520causal%2520abstraction.%2520In%2520this%2520work%252C%2520we%2520critically%2520examine%2520the%250Aconcept%2520of%2520causal%2520abstraction%2520by%2520considering%2520arbitrarily%2520powerful%2520alignment%250Amaps.%2520In%2520particular%252C%2520we%2520prove%2520that%2520under%2520reasonable%2520assumptions%252C%2520any%2520neural%250Anetwork%2520can%2520be%2520mapped%2520to%2520any%2520algorithm%252C%2520rendering%2520this%2520unrestricted%2520notion%2520of%250Acausal%2520abstraction%2520trivial%2520and%2520uninformative.%2520We%2520complement%2520these%2520theoretical%250Afindings%2520with%2520empirical%2520evidence%252C%2520demonstrating%2520that%2520it%2520is%2520possible%2520to%250Aperfectly%2520map%2520models%2520to%2520algorithms%2520even%2520when%2520these%2520models%2520are%2520incapable%2520of%250Asolving%2520the%2520actual%2520task%253B%2520e.g.%252C%2520on%2520an%2520experiment%2520using%2520randomly%2520initialised%250Alanguage%2520models%252C%2520our%2520alignment%2520maps%2520reach%2520100%2525%2520interchange-intervention%250Aaccuracy%2520on%2520the%2520indirect%2520object%2520identification%2520task.%2520This%2520raises%2520the%2520non-linear%250Arepresentation%2520dilemma%253A%2520if%2520we%2520lift%2520the%2520linearity%2520constraint%2520imposed%2520to%250Aalignment%2520maps%2520in%2520causal%2520abstraction%2520analyses%252C%2520we%2520are%2520left%2520with%2520no%2520principled%250Away%2520to%2520balance%2520the%2520inherent%2520trade-off%2520between%2520these%2520maps%2527%2520complexity%2520and%250Aaccuracy.%2520Together%252C%2520these%2520results%2520suggest%2520an%2520answer%2520to%2520our%2520title%2527s%2520question%253A%250Acausal%2520abstraction%2520is%2520not%2520enough%2520for%2520mechanistic%2520interpretability%252C%2520as%2520it%250Abecomes%2520vacuous%2520without%2520assumptions%2520about%2520how%2520models%2520encode%2520information.%250AStudying%2520the%2520connection%2520between%2520this%2520information-encoding%2520assumption%2520and%2520causal%250Aabstraction%2520should%2520lead%2520to%2520exciting%2520future%2520work.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2507.08802v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=The%20Non-Linear%20Representation%20Dilemma%3A%20Is%20Causal%20Abstraction%20Enough%20for%0A%20%20Mechanistic%20Interpretability%3F&entry.906535625=Denis%20Sutter%20and%20Julian%20Minder%20and%20Thomas%20Hofmann%20and%20Tiago%20Pimentel&entry.1292438233=%20%20The%20concept%20of%20causal%20abstraction%20got%20recently%20popularised%20to%20demystify%20the%0Aopaque%20decision-making%20processes%20of%20machine%20learning%20models%3B%20in%20short%2C%20a%20neural%0Anetwork%20can%20be%20abstracted%20as%20a%20higher-level%20algorithm%20if%20there%20exists%20a%0Afunction%20which%20allows%20us%20to%20map%20between%20them.%20Notably%2C%20most%20interpretability%0Apapers%20implement%20these%20maps%20as%20linear%20functions%2C%20motivated%20by%20the%20linear%0Arepresentation%20hypothesis%3A%20the%20idea%20that%20features%20are%20encoded%20linearly%20in%20a%0Amodel%27s%20representations.%20However%2C%20this%20linearity%20constraint%20is%20not%20required%20by%0Athe%20definition%20of%20causal%20abstraction.%20In%20this%20work%2C%20we%20critically%20examine%20the%0Aconcept%20of%20causal%20abstraction%20by%20considering%20arbitrarily%20powerful%20alignment%0Amaps.%20In%20particular%2C%20we%20prove%20that%20under%20reasonable%20assumptions%2C%20any%20neural%0Anetwork%20can%20be%20mapped%20to%20any%20algorithm%2C%20rendering%20this%20unrestricted%20notion%20of%0Acausal%20abstraction%20trivial%20and%20uninformative.%20We%20complement%20these%20theoretical%0Afindings%20with%20empirical%20evidence%2C%20demonstrating%20that%20it%20is%20possible%20to%0Aperfectly%20map%20models%20to%20algorithms%20even%20when%20these%20models%20are%20incapable%20of%0Asolving%20the%20actual%20task%3B%20e.g.%2C%20on%20an%20experiment%20using%20randomly%20initialised%0Alanguage%20models%2C%20our%20alignment%20maps%20reach%20100%25%20interchange-intervention%0Aaccuracy%20on%20the%20indirect%20object%20identification%20task.%20This%20raises%20the%20non-linear%0Arepresentation%20dilemma%3A%20if%20we%20lift%20the%20linearity%20constraint%20imposed%20to%0Aalignment%20maps%20in%20causal%20abstraction%20analyses%2C%20we%20are%20left%20with%20no%20principled%0Away%20to%20balance%20the%20inherent%20trade-off%20between%20these%20maps%27%20complexity%20and%0Aaccuracy.%20Together%2C%20these%20results%20suggest%20an%20answer%20to%20our%20title%27s%20question%3A%0Acausal%20abstraction%20is%20not%20enough%20for%20mechanistic%20interpretability%2C%20as%20it%0Abecomes%20vacuous%20without%20assumptions%20about%20how%20models%20encode%20information.%0AStudying%20the%20connection%20between%20this%20information-encoding%20assumption%20and%20causal%0Aabstraction%20should%20lead%20to%20exciting%20future%20work.%0A&entry.1838667208=http%3A//arxiv.org/abs/2507.08802v1&entry.124074799=Read"},
{"title": "AgentsNet: Coordination and Collaborative Reasoning in Multi-Agent LLMs", "author": "Florian Gr\u00f6tschla and Luis M\u00fcller and Jan T\u00f6nshoff and Mikhail Galkin and Bryan Perozzi", "abstract": "  Large-language models (LLMs) have demonstrated powerful problem-solving\ncapabilities, in particular when organized in multi-agent systems. However, the\nadvent of such systems also raises several questions on the ability of a\ncomplex network of agents to effectively self-organize and collaborate. While\nmeasuring performance on standard reasoning benchmarks indicates how well\nmulti-agent systems can solve reasoning tasks, it is unclear whether these\nsystems are able to leverage their topology effectively. Here, we propose\nAgentsNet, a new benchmark for multi-agent reasoning. By drawing inspiration\nfrom classical problems in distributed systems and graph theory, AgentsNet\nmeasures the ability of multi-agent systems to collaboratively form strategies\nfor problem-solving, self-organization, and effective communication given a\nnetwork topology. We evaluate a variety of baseline methods on AgentsNet\nincluding homogeneous networks of agents which first have to agree on basic\nprotocols for organization and communication. We find that some frontier LLMs\nare already demonstrating strong performance for small networks but begin to\nfall off once the size of the network scales. While existing multi-agent\nbenchmarks cover at most 2-5 agents, AgentsNet is practically unlimited in size\nand can scale with new generations of LLMs. As such, we also probe frontier\nmodels in a setup with up to 100 agents.\n", "link": "http://arxiv.org/abs/2507.08616v1", "date": "2025-07-11", "relevancy": 1.9786, "topK": [{"title": "RoboEXP: Action-Conditioned Scene Graph via Interactive Exploration for\n  Robotic Manipulation", "link": "http://arxiv.org/abs/2402.15487v1", "similarity": 0.4958}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.4944}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.4944}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20AgentsNet%3A%20Coordination%20and%20Collaborative%20Reasoning%20in%20Multi-Agent%20LLMs&body=Title%3A%20AgentsNet%3A%20Coordination%20and%20Collaborative%20Reasoning%20in%20Multi-Agent%20LLMs%0AAuthor%3A%20Florian%20Gr%C3%B6tschla%20and%20Luis%20M%C3%BCller%20and%20Jan%20T%C3%B6nshoff%20and%20Mikhail%20Galkin%20and%20Bryan%20Perozzi%0AAbstract%3A%20%20%20Large-language%20models%20%28LLMs%29%20have%20demonstrated%20powerful%20problem-solving%0Acapabilities%2C%20in%20particular%20when%20organized%20in%20multi-agent%20systems.%20However%2C%20the%0Aadvent%20of%20such%20systems%20also%20raises%20several%20questions%20on%20the%20ability%20of%20a%0Acomplex%20network%20of%20agents%20to%20effectively%20self-organize%20and%20collaborate.%20While%0Ameasuring%20performance%20on%20standard%20reasoning%20benchmarks%20indicates%20how%20well%0Amulti-agent%20systems%20can%20solve%20reasoning%20tasks%2C%20it%20is%20unclear%20whether%20these%0Asystems%20are%20able%20to%20leverage%20their%20topology%20effectively.%20Here%2C%20we%20propose%0AAgentsNet%2C%20a%20new%20benchmark%20for%20multi-agent%20reasoning.%20By%20drawing%20inspiration%0Afrom%20classical%20problems%20in%20distributed%20systems%20and%20graph%20theory%2C%20AgentsNet%0Ameasures%20the%20ability%20of%20multi-agent%20systems%20to%20collaboratively%20form%20strategies%0Afor%20problem-solving%2C%20self-organization%2C%20and%20effective%20communication%20given%20a%0Anetwork%20topology.%20We%20evaluate%20a%20variety%20of%20baseline%20methods%20on%20AgentsNet%0Aincluding%20homogeneous%20networks%20of%20agents%20which%20first%20have%20to%20agree%20on%20basic%0Aprotocols%20for%20organization%20and%20communication.%20We%20find%20that%20some%20frontier%20LLMs%0Aare%20already%20demonstrating%20strong%20performance%20for%20small%20networks%20but%20begin%20to%0Afall%20off%20once%20the%20size%20of%20the%20network%20scales.%20While%20existing%20multi-agent%0Abenchmarks%20cover%20at%20most%202-5%20agents%2C%20AgentsNet%20is%20practically%20unlimited%20in%20size%0Aand%20can%20scale%20with%20new%20generations%20of%20LLMs.%20As%20such%2C%20we%20also%20probe%20frontier%0Amodels%20in%20a%20setup%20with%20up%20to%20100%20agents.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2507.08616v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DAgentsNet%253A%2520Coordination%2520and%2520Collaborative%2520Reasoning%2520in%2520Multi-Agent%2520LLMs%26entry.906535625%3DFlorian%2520Gr%25C3%25B6tschla%2520and%2520Luis%2520M%25C3%25BCller%2520and%2520Jan%2520T%25C3%25B6nshoff%2520and%2520Mikhail%2520Galkin%2520and%2520Bryan%2520Perozzi%26entry.1292438233%3D%2520%2520Large-language%2520models%2520%2528LLMs%2529%2520have%2520demonstrated%2520powerful%2520problem-solving%250Acapabilities%252C%2520in%2520particular%2520when%2520organized%2520in%2520multi-agent%2520systems.%2520However%252C%2520the%250Aadvent%2520of%2520such%2520systems%2520also%2520raises%2520several%2520questions%2520on%2520the%2520ability%2520of%2520a%250Acomplex%2520network%2520of%2520agents%2520to%2520effectively%2520self-organize%2520and%2520collaborate.%2520While%250Ameasuring%2520performance%2520on%2520standard%2520reasoning%2520benchmarks%2520indicates%2520how%2520well%250Amulti-agent%2520systems%2520can%2520solve%2520reasoning%2520tasks%252C%2520it%2520is%2520unclear%2520whether%2520these%250Asystems%2520are%2520able%2520to%2520leverage%2520their%2520topology%2520effectively.%2520Here%252C%2520we%2520propose%250AAgentsNet%252C%2520a%2520new%2520benchmark%2520for%2520multi-agent%2520reasoning.%2520By%2520drawing%2520inspiration%250Afrom%2520classical%2520problems%2520in%2520distributed%2520systems%2520and%2520graph%2520theory%252C%2520AgentsNet%250Ameasures%2520the%2520ability%2520of%2520multi-agent%2520systems%2520to%2520collaboratively%2520form%2520strategies%250Afor%2520problem-solving%252C%2520self-organization%252C%2520and%2520effective%2520communication%2520given%2520a%250Anetwork%2520topology.%2520We%2520evaluate%2520a%2520variety%2520of%2520baseline%2520methods%2520on%2520AgentsNet%250Aincluding%2520homogeneous%2520networks%2520of%2520agents%2520which%2520first%2520have%2520to%2520agree%2520on%2520basic%250Aprotocols%2520for%2520organization%2520and%2520communication.%2520We%2520find%2520that%2520some%2520frontier%2520LLMs%250Aare%2520already%2520demonstrating%2520strong%2520performance%2520for%2520small%2520networks%2520but%2520begin%2520to%250Afall%2520off%2520once%2520the%2520size%2520of%2520the%2520network%2520scales.%2520While%2520existing%2520multi-agent%250Abenchmarks%2520cover%2520at%2520most%25202-5%2520agents%252C%2520AgentsNet%2520is%2520practically%2520unlimited%2520in%2520size%250Aand%2520can%2520scale%2520with%2520new%2520generations%2520of%2520LLMs.%2520As%2520such%252C%2520we%2520also%2520probe%2520frontier%250Amodels%2520in%2520a%2520setup%2520with%2520up%2520to%2520100%2520agents.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2507.08616v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=AgentsNet%3A%20Coordination%20and%20Collaborative%20Reasoning%20in%20Multi-Agent%20LLMs&entry.906535625=Florian%20Gr%C3%B6tschla%20and%20Luis%20M%C3%BCller%20and%20Jan%20T%C3%B6nshoff%20and%20Mikhail%20Galkin%20and%20Bryan%20Perozzi&entry.1292438233=%20%20Large-language%20models%20%28LLMs%29%20have%20demonstrated%20powerful%20problem-solving%0Acapabilities%2C%20in%20particular%20when%20organized%20in%20multi-agent%20systems.%20However%2C%20the%0Aadvent%20of%20such%20systems%20also%20raises%20several%20questions%20on%20the%20ability%20of%20a%0Acomplex%20network%20of%20agents%20to%20effectively%20self-organize%20and%20collaborate.%20While%0Ameasuring%20performance%20on%20standard%20reasoning%20benchmarks%20indicates%20how%20well%0Amulti-agent%20systems%20can%20solve%20reasoning%20tasks%2C%20it%20is%20unclear%20whether%20these%0Asystems%20are%20able%20to%20leverage%20their%20topology%20effectively.%20Here%2C%20we%20propose%0AAgentsNet%2C%20a%20new%20benchmark%20for%20multi-agent%20reasoning.%20By%20drawing%20inspiration%0Afrom%20classical%20problems%20in%20distributed%20systems%20and%20graph%20theory%2C%20AgentsNet%0Ameasures%20the%20ability%20of%20multi-agent%20systems%20to%20collaboratively%20form%20strategies%0Afor%20problem-solving%2C%20self-organization%2C%20and%20effective%20communication%20given%20a%0Anetwork%20topology.%20We%20evaluate%20a%20variety%20of%20baseline%20methods%20on%20AgentsNet%0Aincluding%20homogeneous%20networks%20of%20agents%20which%20first%20have%20to%20agree%20on%20basic%0Aprotocols%20for%20organization%20and%20communication.%20We%20find%20that%20some%20frontier%20LLMs%0Aare%20already%20demonstrating%20strong%20performance%20for%20small%20networks%20but%20begin%20to%0Afall%20off%20once%20the%20size%20of%20the%20network%20scales.%20While%20existing%20multi-agent%0Abenchmarks%20cover%20at%20most%202-5%20agents%2C%20AgentsNet%20is%20practically%20unlimited%20in%20size%0Aand%20can%20scale%20with%20new%20generations%20of%20LLMs.%20As%20such%2C%20we%20also%20probe%20frontier%0Amodels%20in%20a%20setup%20with%20up%20to%20100%20agents.%0A&entry.1838667208=http%3A//arxiv.org/abs/2507.08616v1&entry.124074799=Read"},
{"title": "On learning functions over biological sequence space: relating Gaussian\n  process priors, regularization, and gauge fixing", "author": "Samantha Petti and Carlos Mart\u00ed-G\u00f3mez and Justin B. Kinney and Juannan Zhou and David M. McCandlish", "abstract": "  Mappings from biological sequences (DNA, RNA, protein) to quantitative\nmeasures of sequence functionality play an important role in contemporary\nbiology. We are interested in the related tasks of (i) inferring predictive\nsequence-to-function maps and (ii) decomposing sequence-function maps to\nelucidate the contributions of individual subsequences. Because each\nsequence-function map can be written as a weighted sum over subsequences in\nmultiple ways, meaningfully interpreting these weights requires \"gauge-fixing,\"\ni.e., defining a unique representation for each map. Recent work has\nestablished that most existing gauge-fixed representations arise as the unique\nsolutions to $L_2$-regularized regression in an overparameterized \"weight\nspace\" where the choice of regularizer defines the gauge. Here, we establish\nthe relationship between regularized regression in overparameterized weight\nspace and Gaussian process approaches that operate in \"function space,\" i.e.\nthe space of all real-valued functions on a finite set of sequences. We\ndisentangle how weight space regularizers both impose an implicit prior on the\nlearned function and restrict the optimal weights to a particular gauge. We\nalso show how to construct regularizers that correspond to arbitrary explicit\nGaussian process priors combined with a wide variety of gauges. Next, we derive\nthe distribution of gauge-fixed weights implied by the Gaussian process\nposterior and demonstrate that even for long sequences this distribution can be\nefficiently computed for product-kernel priors using a kernel trick. Finally,\nwe characterize the implicit function space priors associated with the most\ncommon weight space regularizers. Overall, our framework unifies and extends\nour ability to infer and interpret sequence-function relationships.\n", "link": "http://arxiv.org/abs/2504.19034v2", "date": "2025-07-11", "relevancy": 1.9685, "topK": [{"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.5019}, {"title": "GST: Precise 3D Human Body from a Single Image with Gaussian Splatting\n  Transformers", "link": "http://arxiv.org/abs/2409.04196v1", "similarity": 0.4913}, {"title": "MiraGe: Editable 2D Images using Gaussian Splatting", "link": "http://arxiv.org/abs/2410.01521v1", "similarity": 0.4891}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20On%20learning%20functions%20over%20biological%20sequence%20space%3A%20relating%20Gaussian%0A%20%20process%20priors%2C%20regularization%2C%20and%20gauge%20fixing&body=Title%3A%20On%20learning%20functions%20over%20biological%20sequence%20space%3A%20relating%20Gaussian%0A%20%20process%20priors%2C%20regularization%2C%20and%20gauge%20fixing%0AAuthor%3A%20Samantha%20Petti%20and%20Carlos%20Mart%C3%AD-G%C3%B3mez%20and%20Justin%20B.%20Kinney%20and%20Juannan%20Zhou%20and%20David%20M.%20McCandlish%0AAbstract%3A%20%20%20Mappings%20from%20biological%20sequences%20%28DNA%2C%20RNA%2C%20protein%29%20to%20quantitative%0Ameasures%20of%20sequence%20functionality%20play%20an%20important%20role%20in%20contemporary%0Abiology.%20We%20are%20interested%20in%20the%20related%20tasks%20of%20%28i%29%20inferring%20predictive%0Asequence-to-function%20maps%20and%20%28ii%29%20decomposing%20sequence-function%20maps%20to%0Aelucidate%20the%20contributions%20of%20individual%20subsequences.%20Because%20each%0Asequence-function%20map%20can%20be%20written%20as%20a%20weighted%20sum%20over%20subsequences%20in%0Amultiple%20ways%2C%20meaningfully%20interpreting%20these%20weights%20requires%20%22gauge-fixing%2C%22%0Ai.e.%2C%20defining%20a%20unique%20representation%20for%20each%20map.%20Recent%20work%20has%0Aestablished%20that%20most%20existing%20gauge-fixed%20representations%20arise%20as%20the%20unique%0Asolutions%20to%20%24L_2%24-regularized%20regression%20in%20an%20overparameterized%20%22weight%0Aspace%22%20where%20the%20choice%20of%20regularizer%20defines%20the%20gauge.%20Here%2C%20we%20establish%0Athe%20relationship%20between%20regularized%20regression%20in%20overparameterized%20weight%0Aspace%20and%20Gaussian%20process%20approaches%20that%20operate%20in%20%22function%20space%2C%22%20i.e.%0Athe%20space%20of%20all%20real-valued%20functions%20on%20a%20finite%20set%20of%20sequences.%20We%0Adisentangle%20how%20weight%20space%20regularizers%20both%20impose%20an%20implicit%20prior%20on%20the%0Alearned%20function%20and%20restrict%20the%20optimal%20weights%20to%20a%20particular%20gauge.%20We%0Aalso%20show%20how%20to%20construct%20regularizers%20that%20correspond%20to%20arbitrary%20explicit%0AGaussian%20process%20priors%20combined%20with%20a%20wide%20variety%20of%20gauges.%20Next%2C%20we%20derive%0Athe%20distribution%20of%20gauge-fixed%20weights%20implied%20by%20the%20Gaussian%20process%0Aposterior%20and%20demonstrate%20that%20even%20for%20long%20sequences%20this%20distribution%20can%20be%0Aefficiently%20computed%20for%20product-kernel%20priors%20using%20a%20kernel%20trick.%20Finally%2C%0Awe%20characterize%20the%20implicit%20function%20space%20priors%20associated%20with%20the%20most%0Acommon%20weight%20space%20regularizers.%20Overall%2C%20our%20framework%20unifies%20and%20extends%0Aour%20ability%20to%20infer%20and%20interpret%20sequence-function%20relationships.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2504.19034v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DOn%2520learning%2520functions%2520over%2520biological%2520sequence%2520space%253A%2520relating%2520Gaussian%250A%2520%2520process%2520priors%252C%2520regularization%252C%2520and%2520gauge%2520fixing%26entry.906535625%3DSamantha%2520Petti%2520and%2520Carlos%2520Mart%25C3%25AD-G%25C3%25B3mez%2520and%2520Justin%2520B.%2520Kinney%2520and%2520Juannan%2520Zhou%2520and%2520David%2520M.%2520McCandlish%26entry.1292438233%3D%2520%2520Mappings%2520from%2520biological%2520sequences%2520%2528DNA%252C%2520RNA%252C%2520protein%2529%2520to%2520quantitative%250Ameasures%2520of%2520sequence%2520functionality%2520play%2520an%2520important%2520role%2520in%2520contemporary%250Abiology.%2520We%2520are%2520interested%2520in%2520the%2520related%2520tasks%2520of%2520%2528i%2529%2520inferring%2520predictive%250Asequence-to-function%2520maps%2520and%2520%2528ii%2529%2520decomposing%2520sequence-function%2520maps%2520to%250Aelucidate%2520the%2520contributions%2520of%2520individual%2520subsequences.%2520Because%2520each%250Asequence-function%2520map%2520can%2520be%2520written%2520as%2520a%2520weighted%2520sum%2520over%2520subsequences%2520in%250Amultiple%2520ways%252C%2520meaningfully%2520interpreting%2520these%2520weights%2520requires%2520%2522gauge-fixing%252C%2522%250Ai.e.%252C%2520defining%2520a%2520unique%2520representation%2520for%2520each%2520map.%2520Recent%2520work%2520has%250Aestablished%2520that%2520most%2520existing%2520gauge-fixed%2520representations%2520arise%2520as%2520the%2520unique%250Asolutions%2520to%2520%2524L_2%2524-regularized%2520regression%2520in%2520an%2520overparameterized%2520%2522weight%250Aspace%2522%2520where%2520the%2520choice%2520of%2520regularizer%2520defines%2520the%2520gauge.%2520Here%252C%2520we%2520establish%250Athe%2520relationship%2520between%2520regularized%2520regression%2520in%2520overparameterized%2520weight%250Aspace%2520and%2520Gaussian%2520process%2520approaches%2520that%2520operate%2520in%2520%2522function%2520space%252C%2522%2520i.e.%250Athe%2520space%2520of%2520all%2520real-valued%2520functions%2520on%2520a%2520finite%2520set%2520of%2520sequences.%2520We%250Adisentangle%2520how%2520weight%2520space%2520regularizers%2520both%2520impose%2520an%2520implicit%2520prior%2520on%2520the%250Alearned%2520function%2520and%2520restrict%2520the%2520optimal%2520weights%2520to%2520a%2520particular%2520gauge.%2520We%250Aalso%2520show%2520how%2520to%2520construct%2520regularizers%2520that%2520correspond%2520to%2520arbitrary%2520explicit%250AGaussian%2520process%2520priors%2520combined%2520with%2520a%2520wide%2520variety%2520of%2520gauges.%2520Next%252C%2520we%2520derive%250Athe%2520distribution%2520of%2520gauge-fixed%2520weights%2520implied%2520by%2520the%2520Gaussian%2520process%250Aposterior%2520and%2520demonstrate%2520that%2520even%2520for%2520long%2520sequences%2520this%2520distribution%2520can%2520be%250Aefficiently%2520computed%2520for%2520product-kernel%2520priors%2520using%2520a%2520kernel%2520trick.%2520Finally%252C%250Awe%2520characterize%2520the%2520implicit%2520function%2520space%2520priors%2520associated%2520with%2520the%2520most%250Acommon%2520weight%2520space%2520regularizers.%2520Overall%252C%2520our%2520framework%2520unifies%2520and%2520extends%250Aour%2520ability%2520to%2520infer%2520and%2520interpret%2520sequence-function%2520relationships.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2504.19034v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=On%20learning%20functions%20over%20biological%20sequence%20space%3A%20relating%20Gaussian%0A%20%20process%20priors%2C%20regularization%2C%20and%20gauge%20fixing&entry.906535625=Samantha%20Petti%20and%20Carlos%20Mart%C3%AD-G%C3%B3mez%20and%20Justin%20B.%20Kinney%20and%20Juannan%20Zhou%20and%20David%20M.%20McCandlish&entry.1292438233=%20%20Mappings%20from%20biological%20sequences%20%28DNA%2C%20RNA%2C%20protein%29%20to%20quantitative%0Ameasures%20of%20sequence%20functionality%20play%20an%20important%20role%20in%20contemporary%0Abiology.%20We%20are%20interested%20in%20the%20related%20tasks%20of%20%28i%29%20inferring%20predictive%0Asequence-to-function%20maps%20and%20%28ii%29%20decomposing%20sequence-function%20maps%20to%0Aelucidate%20the%20contributions%20of%20individual%20subsequences.%20Because%20each%0Asequence-function%20map%20can%20be%20written%20as%20a%20weighted%20sum%20over%20subsequences%20in%0Amultiple%20ways%2C%20meaningfully%20interpreting%20these%20weights%20requires%20%22gauge-fixing%2C%22%0Ai.e.%2C%20defining%20a%20unique%20representation%20for%20each%20map.%20Recent%20work%20has%0Aestablished%20that%20most%20existing%20gauge-fixed%20representations%20arise%20as%20the%20unique%0Asolutions%20to%20%24L_2%24-regularized%20regression%20in%20an%20overparameterized%20%22weight%0Aspace%22%20where%20the%20choice%20of%20regularizer%20defines%20the%20gauge.%20Here%2C%20we%20establish%0Athe%20relationship%20between%20regularized%20regression%20in%20overparameterized%20weight%0Aspace%20and%20Gaussian%20process%20approaches%20that%20operate%20in%20%22function%20space%2C%22%20i.e.%0Athe%20space%20of%20all%20real-valued%20functions%20on%20a%20finite%20set%20of%20sequences.%20We%0Adisentangle%20how%20weight%20space%20regularizers%20both%20impose%20an%20implicit%20prior%20on%20the%0Alearned%20function%20and%20restrict%20the%20optimal%20weights%20to%20a%20particular%20gauge.%20We%0Aalso%20show%20how%20to%20construct%20regularizers%20that%20correspond%20to%20arbitrary%20explicit%0AGaussian%20process%20priors%20combined%20with%20a%20wide%20variety%20of%20gauges.%20Next%2C%20we%20derive%0Athe%20distribution%20of%20gauge-fixed%20weights%20implied%20by%20the%20Gaussian%20process%0Aposterior%20and%20demonstrate%20that%20even%20for%20long%20sequences%20this%20distribution%20can%20be%0Aefficiently%20computed%20for%20product-kernel%20priors%20using%20a%20kernel%20trick.%20Finally%2C%0Awe%20characterize%20the%20implicit%20function%20space%20priors%20associated%20with%20the%20most%0Acommon%20weight%20space%20regularizers.%20Overall%2C%20our%20framework%20unifies%20and%20extends%0Aour%20ability%20to%20infer%20and%20interpret%20sequence-function%20relationships.%0A&entry.1838667208=http%3A//arxiv.org/abs/2504.19034v2&entry.124074799=Read"},
{"title": "The Impact of Automatic Speech Transcription on Speaker Attribution", "author": "Cristina Aggazzotti and Matthew Wiesner and Elizabeth Allyn Smith and Nicholas Andrews", "abstract": "  Speaker attribution from speech transcripts is the task of identifying a\nspeaker from the transcript of their speech based on patterns in their language\nuse. This task is especially useful when the audio is unavailable (e.g.\ndeleted) or unreliable (e.g. anonymized speech). Prior work in this area has\nprimarily focused on the feasibility of attributing speakers using transcripts\nproduced by human annotators. However, in real-world settings, one often only\nhas more errorful transcripts produced by automatic speech recognition (ASR)\nsystems. In this paper, we conduct what is, to our knowledge, the first\ncomprehensive study of the impact of automatic transcription on speaker\nattribution performance. In particular, we study the extent to which speaker\nattribution performance degrades in the face of transcription errors, as well\nas how properties of the ASR system impact attribution. We find that\nattribution is surprisingly resilient to word-level transcription errors and\nthat the objective of recovering the true transcript is minimally correlated\nwith attribution performance. Overall, our findings suggest that speaker\nattribution on more errorful transcripts produced by ASR is as good, if not\nbetter, than attribution based on human-transcribed data, possibly because ASR\ntranscription errors can capture speaker-specific features revealing of speaker\nidentity.\n", "link": "http://arxiv.org/abs/2507.08660v1", "date": "2025-07-11", "relevancy": 1.9645, "topK": [{"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.3955}, {"title": "DressCode: Autoregressively Sewing and Generating Garments from Text\n  Guidance", "link": "http://arxiv.org/abs/2401.16465v3", "similarity": 0.3931}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.3901}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20The%20Impact%20of%20Automatic%20Speech%20Transcription%20on%20Speaker%20Attribution&body=Title%3A%20The%20Impact%20of%20Automatic%20Speech%20Transcription%20on%20Speaker%20Attribution%0AAuthor%3A%20Cristina%20Aggazzotti%20and%20Matthew%20Wiesner%20and%20Elizabeth%20Allyn%20Smith%20and%20Nicholas%20Andrews%0AAbstract%3A%20%20%20Speaker%20attribution%20from%20speech%20transcripts%20is%20the%20task%20of%20identifying%20a%0Aspeaker%20from%20the%20transcript%20of%20their%20speech%20based%20on%20patterns%20in%20their%20language%0Ause.%20This%20task%20is%20especially%20useful%20when%20the%20audio%20is%20unavailable%20%28e.g.%0Adeleted%29%20or%20unreliable%20%28e.g.%20anonymized%20speech%29.%20Prior%20work%20in%20this%20area%20has%0Aprimarily%20focused%20on%20the%20feasibility%20of%20attributing%20speakers%20using%20transcripts%0Aproduced%20by%20human%20annotators.%20However%2C%20in%20real-world%20settings%2C%20one%20often%20only%0Ahas%20more%20errorful%20transcripts%20produced%20by%20automatic%20speech%20recognition%20%28ASR%29%0Asystems.%20In%20this%20paper%2C%20we%20conduct%20what%20is%2C%20to%20our%20knowledge%2C%20the%20first%0Acomprehensive%20study%20of%20the%20impact%20of%20automatic%20transcription%20on%20speaker%0Aattribution%20performance.%20In%20particular%2C%20we%20study%20the%20extent%20to%20which%20speaker%0Aattribution%20performance%20degrades%20in%20the%20face%20of%20transcription%20errors%2C%20as%20well%0Aas%20how%20properties%20of%20the%20ASR%20system%20impact%20attribution.%20We%20find%20that%0Aattribution%20is%20surprisingly%20resilient%20to%20word-level%20transcription%20errors%20and%0Athat%20the%20objective%20of%20recovering%20the%20true%20transcript%20is%20minimally%20correlated%0Awith%20attribution%20performance.%20Overall%2C%20our%20findings%20suggest%20that%20speaker%0Aattribution%20on%20more%20errorful%20transcripts%20produced%20by%20ASR%20is%20as%20good%2C%20if%20not%0Abetter%2C%20than%20attribution%20based%20on%20human-transcribed%20data%2C%20possibly%20because%20ASR%0Atranscription%20errors%20can%20capture%20speaker-specific%20features%20revealing%20of%20speaker%0Aidentity.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2507.08660v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DThe%2520Impact%2520of%2520Automatic%2520Speech%2520Transcription%2520on%2520Speaker%2520Attribution%26entry.906535625%3DCristina%2520Aggazzotti%2520and%2520Matthew%2520Wiesner%2520and%2520Elizabeth%2520Allyn%2520Smith%2520and%2520Nicholas%2520Andrews%26entry.1292438233%3D%2520%2520Speaker%2520attribution%2520from%2520speech%2520transcripts%2520is%2520the%2520task%2520of%2520identifying%2520a%250Aspeaker%2520from%2520the%2520transcript%2520of%2520their%2520speech%2520based%2520on%2520patterns%2520in%2520their%2520language%250Ause.%2520This%2520task%2520is%2520especially%2520useful%2520when%2520the%2520audio%2520is%2520unavailable%2520%2528e.g.%250Adeleted%2529%2520or%2520unreliable%2520%2528e.g.%2520anonymized%2520speech%2529.%2520Prior%2520work%2520in%2520this%2520area%2520has%250Aprimarily%2520focused%2520on%2520the%2520feasibility%2520of%2520attributing%2520speakers%2520using%2520transcripts%250Aproduced%2520by%2520human%2520annotators.%2520However%252C%2520in%2520real-world%2520settings%252C%2520one%2520often%2520only%250Ahas%2520more%2520errorful%2520transcripts%2520produced%2520by%2520automatic%2520speech%2520recognition%2520%2528ASR%2529%250Asystems.%2520In%2520this%2520paper%252C%2520we%2520conduct%2520what%2520is%252C%2520to%2520our%2520knowledge%252C%2520the%2520first%250Acomprehensive%2520study%2520of%2520the%2520impact%2520of%2520automatic%2520transcription%2520on%2520speaker%250Aattribution%2520performance.%2520In%2520particular%252C%2520we%2520study%2520the%2520extent%2520to%2520which%2520speaker%250Aattribution%2520performance%2520degrades%2520in%2520the%2520face%2520of%2520transcription%2520errors%252C%2520as%2520well%250Aas%2520how%2520properties%2520of%2520the%2520ASR%2520system%2520impact%2520attribution.%2520We%2520find%2520that%250Aattribution%2520is%2520surprisingly%2520resilient%2520to%2520word-level%2520transcription%2520errors%2520and%250Athat%2520the%2520objective%2520of%2520recovering%2520the%2520true%2520transcript%2520is%2520minimally%2520correlated%250Awith%2520attribution%2520performance.%2520Overall%252C%2520our%2520findings%2520suggest%2520that%2520speaker%250Aattribution%2520on%2520more%2520errorful%2520transcripts%2520produced%2520by%2520ASR%2520is%2520as%2520good%252C%2520if%2520not%250Abetter%252C%2520than%2520attribution%2520based%2520on%2520human-transcribed%2520data%252C%2520possibly%2520because%2520ASR%250Atranscription%2520errors%2520can%2520capture%2520speaker-specific%2520features%2520revealing%2520of%2520speaker%250Aidentity.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2507.08660v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=The%20Impact%20of%20Automatic%20Speech%20Transcription%20on%20Speaker%20Attribution&entry.906535625=Cristina%20Aggazzotti%20and%20Matthew%20Wiesner%20and%20Elizabeth%20Allyn%20Smith%20and%20Nicholas%20Andrews&entry.1292438233=%20%20Speaker%20attribution%20from%20speech%20transcripts%20is%20the%20task%20of%20identifying%20a%0Aspeaker%20from%20the%20transcript%20of%20their%20speech%20based%20on%20patterns%20in%20their%20language%0Ause.%20This%20task%20is%20especially%20useful%20when%20the%20audio%20is%20unavailable%20%28e.g.%0Adeleted%29%20or%20unreliable%20%28e.g.%20anonymized%20speech%29.%20Prior%20work%20in%20this%20area%20has%0Aprimarily%20focused%20on%20the%20feasibility%20of%20attributing%20speakers%20using%20transcripts%0Aproduced%20by%20human%20annotators.%20However%2C%20in%20real-world%20settings%2C%20one%20often%20only%0Ahas%20more%20errorful%20transcripts%20produced%20by%20automatic%20speech%20recognition%20%28ASR%29%0Asystems.%20In%20this%20paper%2C%20we%20conduct%20what%20is%2C%20to%20our%20knowledge%2C%20the%20first%0Acomprehensive%20study%20of%20the%20impact%20of%20automatic%20transcription%20on%20speaker%0Aattribution%20performance.%20In%20particular%2C%20we%20study%20the%20extent%20to%20which%20speaker%0Aattribution%20performance%20degrades%20in%20the%20face%20of%20transcription%20errors%2C%20as%20well%0Aas%20how%20properties%20of%20the%20ASR%20system%20impact%20attribution.%20We%20find%20that%0Aattribution%20is%20surprisingly%20resilient%20to%20word-level%20transcription%20errors%20and%0Athat%20the%20objective%20of%20recovering%20the%20true%20transcript%20is%20minimally%20correlated%0Awith%20attribution%20performance.%20Overall%2C%20our%20findings%20suggest%20that%20speaker%0Aattribution%20on%20more%20errorful%20transcripts%20produced%20by%20ASR%20is%20as%20good%2C%20if%20not%0Abetter%2C%20than%20attribution%20based%20on%20human-transcribed%20data%2C%20possibly%20because%20ASR%0Atranscription%20errors%20can%20capture%20speaker-specific%20features%20revealing%20of%20speaker%0Aidentity.%0A&entry.1838667208=http%3A//arxiv.org/abs/2507.08660v1&entry.124074799=Read"},
{"title": "Rethinking Approximate Gaussian Inference in Classification", "author": "B\u00e1lint Mucs\u00e1nyi and Natha\u00ebl Da Costa and Philipp Hennig", "abstract": "  In classification tasks, softmax functions are ubiquitously used as output\nactivations to produce predictive probabilities. Such outputs only capture\naleatoric uncertainty. To capture epistemic uncertainty, approximate Gaussian\ninference methods have been proposed. We develop a common formalism to describe\nsuch methods, which we view as outputting Gaussian distributions over the logit\nspace. Predictives are then obtained as the expectations of the Gaussian\ndistributions pushed forward through the softmax. However, such softmax\nGaussian integrals cannot be solved analytically, and Monte Carlo (MC)\napproximations can be costly and noisy. We propose to replace the softmax\nactivation by element-wise normCDF or sigmoid, which allows for the accurate\nsampling-free approximation of predictives. This also enables the approximation\nof the Gaussian pushforwards by Dirichlet distributions with moment matching.\nThis approach entirely eliminates the runtime and memory overhead associated\nwith MC sampling. We evaluate it combined with several approximate Gaussian\ninference methods (Laplace, HET, SNGP) on large- and small-scale datasets\n(ImageNet, CIFAR-100, CIFAR-10), demonstrating improved uncertainty\nquantification capabilities compared to softmax MC sampling.\n", "link": "http://arxiv.org/abs/2502.03366v2", "date": "2025-07-11", "relevancy": 1.9584, "topK": [{"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.524}, {"title": "Flexible Visual Recognition by Evidential Modeling of Confusion and Ignorance", "link": "http://openaccess.thecvf.com/content/ICCV2023/html/Fan_Flexible_Visual_Recognition_by_Evidential_Modeling_of_Confusion_and_Ignorance_ICCV_2023_paper.html", "similarity": 0.4944}, {"title": "GST: Precise 3D Human Body from a Single Image with Gaussian Splatting\n  Transformers", "link": "http://arxiv.org/abs/2409.04196v1", "similarity": 0.471}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Rethinking%20Approximate%20Gaussian%20Inference%20in%20Classification&body=Title%3A%20Rethinking%20Approximate%20Gaussian%20Inference%20in%20Classification%0AAuthor%3A%20B%C3%A1lint%20Mucs%C3%A1nyi%20and%20Natha%C3%ABl%20Da%20Costa%20and%20Philipp%20Hennig%0AAbstract%3A%20%20%20In%20classification%20tasks%2C%20softmax%20functions%20are%20ubiquitously%20used%20as%20output%0Aactivations%20to%20produce%20predictive%20probabilities.%20Such%20outputs%20only%20capture%0Aaleatoric%20uncertainty.%20To%20capture%20epistemic%20uncertainty%2C%20approximate%20Gaussian%0Ainference%20methods%20have%20been%20proposed.%20We%20develop%20a%20common%20formalism%20to%20describe%0Asuch%20methods%2C%20which%20we%20view%20as%20outputting%20Gaussian%20distributions%20over%20the%20logit%0Aspace.%20Predictives%20are%20then%20obtained%20as%20the%20expectations%20of%20the%20Gaussian%0Adistributions%20pushed%20forward%20through%20the%20softmax.%20However%2C%20such%20softmax%0AGaussian%20integrals%20cannot%20be%20solved%20analytically%2C%20and%20Monte%20Carlo%20%28MC%29%0Aapproximations%20can%20be%20costly%20and%20noisy.%20We%20propose%20to%20replace%20the%20softmax%0Aactivation%20by%20element-wise%20normCDF%20or%20sigmoid%2C%20which%20allows%20for%20the%20accurate%0Asampling-free%20approximation%20of%20predictives.%20This%20also%20enables%20the%20approximation%0Aof%20the%20Gaussian%20pushforwards%20by%20Dirichlet%20distributions%20with%20moment%20matching.%0AThis%20approach%20entirely%20eliminates%20the%20runtime%20and%20memory%20overhead%20associated%0Awith%20MC%20sampling.%20We%20evaluate%20it%20combined%20with%20several%20approximate%20Gaussian%0Ainference%20methods%20%28Laplace%2C%20HET%2C%20SNGP%29%20on%20large-%20and%20small-scale%20datasets%0A%28ImageNet%2C%20CIFAR-100%2C%20CIFAR-10%29%2C%20demonstrating%20improved%20uncertainty%0Aquantification%20capabilities%20compared%20to%20softmax%20MC%20sampling.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2502.03366v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DRethinking%2520Approximate%2520Gaussian%2520Inference%2520in%2520Classification%26entry.906535625%3DB%25C3%25A1lint%2520Mucs%25C3%25A1nyi%2520and%2520Natha%25C3%25ABl%2520Da%2520Costa%2520and%2520Philipp%2520Hennig%26entry.1292438233%3D%2520%2520In%2520classification%2520tasks%252C%2520softmax%2520functions%2520are%2520ubiquitously%2520used%2520as%2520output%250Aactivations%2520to%2520produce%2520predictive%2520probabilities.%2520Such%2520outputs%2520only%2520capture%250Aaleatoric%2520uncertainty.%2520To%2520capture%2520epistemic%2520uncertainty%252C%2520approximate%2520Gaussian%250Ainference%2520methods%2520have%2520been%2520proposed.%2520We%2520develop%2520a%2520common%2520formalism%2520to%2520describe%250Asuch%2520methods%252C%2520which%2520we%2520view%2520as%2520outputting%2520Gaussian%2520distributions%2520over%2520the%2520logit%250Aspace.%2520Predictives%2520are%2520then%2520obtained%2520as%2520the%2520expectations%2520of%2520the%2520Gaussian%250Adistributions%2520pushed%2520forward%2520through%2520the%2520softmax.%2520However%252C%2520such%2520softmax%250AGaussian%2520integrals%2520cannot%2520be%2520solved%2520analytically%252C%2520and%2520Monte%2520Carlo%2520%2528MC%2529%250Aapproximations%2520can%2520be%2520costly%2520and%2520noisy.%2520We%2520propose%2520to%2520replace%2520the%2520softmax%250Aactivation%2520by%2520element-wise%2520normCDF%2520or%2520sigmoid%252C%2520which%2520allows%2520for%2520the%2520accurate%250Asampling-free%2520approximation%2520of%2520predictives.%2520This%2520also%2520enables%2520the%2520approximation%250Aof%2520the%2520Gaussian%2520pushforwards%2520by%2520Dirichlet%2520distributions%2520with%2520moment%2520matching.%250AThis%2520approach%2520entirely%2520eliminates%2520the%2520runtime%2520and%2520memory%2520overhead%2520associated%250Awith%2520MC%2520sampling.%2520We%2520evaluate%2520it%2520combined%2520with%2520several%2520approximate%2520Gaussian%250Ainference%2520methods%2520%2528Laplace%252C%2520HET%252C%2520SNGP%2529%2520on%2520large-%2520and%2520small-scale%2520datasets%250A%2528ImageNet%252C%2520CIFAR-100%252C%2520CIFAR-10%2529%252C%2520demonstrating%2520improved%2520uncertainty%250Aquantification%2520capabilities%2520compared%2520to%2520softmax%2520MC%2520sampling.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2502.03366v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Rethinking%20Approximate%20Gaussian%20Inference%20in%20Classification&entry.906535625=B%C3%A1lint%20Mucs%C3%A1nyi%20and%20Natha%C3%ABl%20Da%20Costa%20and%20Philipp%20Hennig&entry.1292438233=%20%20In%20classification%20tasks%2C%20softmax%20functions%20are%20ubiquitously%20used%20as%20output%0Aactivations%20to%20produce%20predictive%20probabilities.%20Such%20outputs%20only%20capture%0Aaleatoric%20uncertainty.%20To%20capture%20epistemic%20uncertainty%2C%20approximate%20Gaussian%0Ainference%20methods%20have%20been%20proposed.%20We%20develop%20a%20common%20formalism%20to%20describe%0Asuch%20methods%2C%20which%20we%20view%20as%20outputting%20Gaussian%20distributions%20over%20the%20logit%0Aspace.%20Predictives%20are%20then%20obtained%20as%20the%20expectations%20of%20the%20Gaussian%0Adistributions%20pushed%20forward%20through%20the%20softmax.%20However%2C%20such%20softmax%0AGaussian%20integrals%20cannot%20be%20solved%20analytically%2C%20and%20Monte%20Carlo%20%28MC%29%0Aapproximations%20can%20be%20costly%20and%20noisy.%20We%20propose%20to%20replace%20the%20softmax%0Aactivation%20by%20element-wise%20normCDF%20or%20sigmoid%2C%20which%20allows%20for%20the%20accurate%0Asampling-free%20approximation%20of%20predictives.%20This%20also%20enables%20the%20approximation%0Aof%20the%20Gaussian%20pushforwards%20by%20Dirichlet%20distributions%20with%20moment%20matching.%0AThis%20approach%20entirely%20eliminates%20the%20runtime%20and%20memory%20overhead%20associated%0Awith%20MC%20sampling.%20We%20evaluate%20it%20combined%20with%20several%20approximate%20Gaussian%0Ainference%20methods%20%28Laplace%2C%20HET%2C%20SNGP%29%20on%20large-%20and%20small-scale%20datasets%0A%28ImageNet%2C%20CIFAR-100%2C%20CIFAR-10%29%2C%20demonstrating%20improved%20uncertainty%0Aquantification%20capabilities%20compared%20to%20softmax%20MC%20sampling.%0A&entry.1838667208=http%3A//arxiv.org/abs/2502.03366v2&entry.124074799=Read"},
{"title": "Alternating Gradient Flows: A Theory of Feature Learning in Two-layer\n  Neural Networks", "author": "Daniel Kunin and Giovanni Luca Marchetti and Feng Chen and Dhruva Karkada and James B. Simon and Michael R. DeWeese and Surya Ganguli and Nina Miolane", "abstract": "  What features neural networks learn, and how, remains an open question. In\nthis paper, we introduce Alternating Gradient Flows (AGF), an algorithmic\nframework that describes the dynamics of feature learning in two-layer networks\ntrained from small initialization. Prior works have shown that gradient flow in\nthis regime exhibits a staircase-like loss curve, alternating between plateaus\nwhere neurons slowly align to useful directions and sharp drops where neurons\nrapidly grow in norm. AGF approximates this behavior as an alternating two-step\nprocess: maximizing a utility function over dormant neurons and minimizing a\ncost function over active ones. AGF begins with all neurons dormant. At each\nround, a dormant neuron activates, triggering the acquisition of a feature and\na drop in the loss. AGF quantifies the order, timing, and magnitude of these\ndrops, matching experiments across architectures. We show that AGF unifies and\nextends existing saddle-to-saddle analyses in fully connected linear networks\nand attention-only linear transformers, where the learned features are singular\nmodes and principal components, respectively. In diagonal linear networks, we\nprove AGF converges to gradient flow in the limit of vanishing initialization.\nApplying AGF to quadratic networks trained to perform modular addition, we give\nthe first complete characterization of the training dynamics, revealing that\nnetworks learn Fourier features in decreasing order of coefficient magnitude.\nAltogether, AGF offers a promising step towards understanding feature learning\nin neural networks.\n", "link": "http://arxiv.org/abs/2506.06489v2", "date": "2025-07-11", "relevancy": 1.9545, "topK": [{"title": "Scaling Rectified Flow Transformers for High-Resolution Image Synthesis", "link": "http://arxiv.org/abs/2403.03206v1", "similarity": 0.5465}, {"title": "Boosted dynamic neural networks", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/26302", "similarity": 0.4901}, {"title": "ToDo: Token Downsampling for Efficient Generation of High-Resolution\n  Images", "link": "http://arxiv.org/abs/2402.13573v2", "similarity": 0.464}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Alternating%20Gradient%20Flows%3A%20A%20Theory%20of%20Feature%20Learning%20in%20Two-layer%0A%20%20Neural%20Networks&body=Title%3A%20Alternating%20Gradient%20Flows%3A%20A%20Theory%20of%20Feature%20Learning%20in%20Two-layer%0A%20%20Neural%20Networks%0AAuthor%3A%20Daniel%20Kunin%20and%20Giovanni%20Luca%20Marchetti%20and%20Feng%20Chen%20and%20Dhruva%20Karkada%20and%20James%20B.%20Simon%20and%20Michael%20R.%20DeWeese%20and%20Surya%20Ganguli%20and%20Nina%20Miolane%0AAbstract%3A%20%20%20What%20features%20neural%20networks%20learn%2C%20and%20how%2C%20remains%20an%20open%20question.%20In%0Athis%20paper%2C%20we%20introduce%20Alternating%20Gradient%20Flows%20%28AGF%29%2C%20an%20algorithmic%0Aframework%20that%20describes%20the%20dynamics%20of%20feature%20learning%20in%20two-layer%20networks%0Atrained%20from%20small%20initialization.%20Prior%20works%20have%20shown%20that%20gradient%20flow%20in%0Athis%20regime%20exhibits%20a%20staircase-like%20loss%20curve%2C%20alternating%20between%20plateaus%0Awhere%20neurons%20slowly%20align%20to%20useful%20directions%20and%20sharp%20drops%20where%20neurons%0Arapidly%20grow%20in%20norm.%20AGF%20approximates%20this%20behavior%20as%20an%20alternating%20two-step%0Aprocess%3A%20maximizing%20a%20utility%20function%20over%20dormant%20neurons%20and%20minimizing%20a%0Acost%20function%20over%20active%20ones.%20AGF%20begins%20with%20all%20neurons%20dormant.%20At%20each%0Around%2C%20a%20dormant%20neuron%20activates%2C%20triggering%20the%20acquisition%20of%20a%20feature%20and%0Aa%20drop%20in%20the%20loss.%20AGF%20quantifies%20the%20order%2C%20timing%2C%20and%20magnitude%20of%20these%0Adrops%2C%20matching%20experiments%20across%20architectures.%20We%20show%20that%20AGF%20unifies%20and%0Aextends%20existing%20saddle-to-saddle%20analyses%20in%20fully%20connected%20linear%20networks%0Aand%20attention-only%20linear%20transformers%2C%20where%20the%20learned%20features%20are%20singular%0Amodes%20and%20principal%20components%2C%20respectively.%20In%20diagonal%20linear%20networks%2C%20we%0Aprove%20AGF%20converges%20to%20gradient%20flow%20in%20the%20limit%20of%20vanishing%20initialization.%0AApplying%20AGF%20to%20quadratic%20networks%20trained%20to%20perform%20modular%20addition%2C%20we%20give%0Athe%20first%20complete%20characterization%20of%20the%20training%20dynamics%2C%20revealing%20that%0Anetworks%20learn%20Fourier%20features%20in%20decreasing%20order%20of%20coefficient%20magnitude.%0AAltogether%2C%20AGF%20offers%20a%20promising%20step%20towards%20understanding%20feature%20learning%0Ain%20neural%20networks.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2506.06489v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DAlternating%2520Gradient%2520Flows%253A%2520A%2520Theory%2520of%2520Feature%2520Learning%2520in%2520Two-layer%250A%2520%2520Neural%2520Networks%26entry.906535625%3DDaniel%2520Kunin%2520and%2520Giovanni%2520Luca%2520Marchetti%2520and%2520Feng%2520Chen%2520and%2520Dhruva%2520Karkada%2520and%2520James%2520B.%2520Simon%2520and%2520Michael%2520R.%2520DeWeese%2520and%2520Surya%2520Ganguli%2520and%2520Nina%2520Miolane%26entry.1292438233%3D%2520%2520What%2520features%2520neural%2520networks%2520learn%252C%2520and%2520how%252C%2520remains%2520an%2520open%2520question.%2520In%250Athis%2520paper%252C%2520we%2520introduce%2520Alternating%2520Gradient%2520Flows%2520%2528AGF%2529%252C%2520an%2520algorithmic%250Aframework%2520that%2520describes%2520the%2520dynamics%2520of%2520feature%2520learning%2520in%2520two-layer%2520networks%250Atrained%2520from%2520small%2520initialization.%2520Prior%2520works%2520have%2520shown%2520that%2520gradient%2520flow%2520in%250Athis%2520regime%2520exhibits%2520a%2520staircase-like%2520loss%2520curve%252C%2520alternating%2520between%2520plateaus%250Awhere%2520neurons%2520slowly%2520align%2520to%2520useful%2520directions%2520and%2520sharp%2520drops%2520where%2520neurons%250Arapidly%2520grow%2520in%2520norm.%2520AGF%2520approximates%2520this%2520behavior%2520as%2520an%2520alternating%2520two-step%250Aprocess%253A%2520maximizing%2520a%2520utility%2520function%2520over%2520dormant%2520neurons%2520and%2520minimizing%2520a%250Acost%2520function%2520over%2520active%2520ones.%2520AGF%2520begins%2520with%2520all%2520neurons%2520dormant.%2520At%2520each%250Around%252C%2520a%2520dormant%2520neuron%2520activates%252C%2520triggering%2520the%2520acquisition%2520of%2520a%2520feature%2520and%250Aa%2520drop%2520in%2520the%2520loss.%2520AGF%2520quantifies%2520the%2520order%252C%2520timing%252C%2520and%2520magnitude%2520of%2520these%250Adrops%252C%2520matching%2520experiments%2520across%2520architectures.%2520We%2520show%2520that%2520AGF%2520unifies%2520and%250Aextends%2520existing%2520saddle-to-saddle%2520analyses%2520in%2520fully%2520connected%2520linear%2520networks%250Aand%2520attention-only%2520linear%2520transformers%252C%2520where%2520the%2520learned%2520features%2520are%2520singular%250Amodes%2520and%2520principal%2520components%252C%2520respectively.%2520In%2520diagonal%2520linear%2520networks%252C%2520we%250Aprove%2520AGF%2520converges%2520to%2520gradient%2520flow%2520in%2520the%2520limit%2520of%2520vanishing%2520initialization.%250AApplying%2520AGF%2520to%2520quadratic%2520networks%2520trained%2520to%2520perform%2520modular%2520addition%252C%2520we%2520give%250Athe%2520first%2520complete%2520characterization%2520of%2520the%2520training%2520dynamics%252C%2520revealing%2520that%250Anetworks%2520learn%2520Fourier%2520features%2520in%2520decreasing%2520order%2520of%2520coefficient%2520magnitude.%250AAltogether%252C%2520AGF%2520offers%2520a%2520promising%2520step%2520towards%2520understanding%2520feature%2520learning%250Ain%2520neural%2520networks.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2506.06489v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Alternating%20Gradient%20Flows%3A%20A%20Theory%20of%20Feature%20Learning%20in%20Two-layer%0A%20%20Neural%20Networks&entry.906535625=Daniel%20Kunin%20and%20Giovanni%20Luca%20Marchetti%20and%20Feng%20Chen%20and%20Dhruva%20Karkada%20and%20James%20B.%20Simon%20and%20Michael%20R.%20DeWeese%20and%20Surya%20Ganguli%20and%20Nina%20Miolane&entry.1292438233=%20%20What%20features%20neural%20networks%20learn%2C%20and%20how%2C%20remains%20an%20open%20question.%20In%0Athis%20paper%2C%20we%20introduce%20Alternating%20Gradient%20Flows%20%28AGF%29%2C%20an%20algorithmic%0Aframework%20that%20describes%20the%20dynamics%20of%20feature%20learning%20in%20two-layer%20networks%0Atrained%20from%20small%20initialization.%20Prior%20works%20have%20shown%20that%20gradient%20flow%20in%0Athis%20regime%20exhibits%20a%20staircase-like%20loss%20curve%2C%20alternating%20between%20plateaus%0Awhere%20neurons%20slowly%20align%20to%20useful%20directions%20and%20sharp%20drops%20where%20neurons%0Arapidly%20grow%20in%20norm.%20AGF%20approximates%20this%20behavior%20as%20an%20alternating%20two-step%0Aprocess%3A%20maximizing%20a%20utility%20function%20over%20dormant%20neurons%20and%20minimizing%20a%0Acost%20function%20over%20active%20ones.%20AGF%20begins%20with%20all%20neurons%20dormant.%20At%20each%0Around%2C%20a%20dormant%20neuron%20activates%2C%20triggering%20the%20acquisition%20of%20a%20feature%20and%0Aa%20drop%20in%20the%20loss.%20AGF%20quantifies%20the%20order%2C%20timing%2C%20and%20magnitude%20of%20these%0Adrops%2C%20matching%20experiments%20across%20architectures.%20We%20show%20that%20AGF%20unifies%20and%0Aextends%20existing%20saddle-to-saddle%20analyses%20in%20fully%20connected%20linear%20networks%0Aand%20attention-only%20linear%20transformers%2C%20where%20the%20learned%20features%20are%20singular%0Amodes%20and%20principal%20components%2C%20respectively.%20In%20diagonal%20linear%20networks%2C%20we%0Aprove%20AGF%20converges%20to%20gradient%20flow%20in%20the%20limit%20of%20vanishing%20initialization.%0AApplying%20AGF%20to%20quadratic%20networks%20trained%20to%20perform%20modular%20addition%2C%20we%20give%0Athe%20first%20complete%20characterization%20of%20the%20training%20dynamics%2C%20revealing%20that%0Anetworks%20learn%20Fourier%20features%20in%20decreasing%20order%20of%20coefficient%20magnitude.%0AAltogether%2C%20AGF%20offers%20a%20promising%20step%20towards%20understanding%20feature%20learning%0Ain%20neural%20networks.%0A&entry.1838667208=http%3A//arxiv.org/abs/2506.06489v2&entry.124074799=Read"},
{"title": "Drowning in Documents: Consequences of Scaling Reranker Inference", "author": "Mathew Jacob and Erik Lindgren and Matei Zaharia and Michael Carbin and Omar Khattab and Andrew Drozdov", "abstract": "  Rerankers, typically cross-encoders, are computationally intensive but are\nfrequently used because they are widely assumed to outperform cheaper initial\nIR systems. We challenge this assumption by measuring reranker performance for\nfull retrieval, not just re-scoring first-stage retrieval. To provide a more\nrobust evaluation, we prioritize strong first-stage retrieval using modern\ndense embeddings and test rerankers on a variety of carefully chosen,\nchallenging tasks, including internally curated datasets to avoid\ncontamination, and out-of-domain ones. Our empirical results reveal a\nsurprising trend: the best existing rerankers provide initial improvements when\nscoring progressively more documents, but their effectiveness gradually\ndeclines and can even degrade quality beyond a certain limit. We hope that our\nfindings will spur future research to improve reranking.\n", "link": "http://arxiv.org/abs/2411.11767v2", "date": "2025-07-11", "relevancy": 1.9525, "topK": [{"title": "Scaling Rectified Flow Transformers for High-Resolution Image Synthesis", "link": "http://arxiv.org/abs/2403.03206v1", "similarity": 0.4966}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.4864}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.4864}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Drowning%20in%20Documents%3A%20Consequences%20of%20Scaling%20Reranker%20Inference&body=Title%3A%20Drowning%20in%20Documents%3A%20Consequences%20of%20Scaling%20Reranker%20Inference%0AAuthor%3A%20Mathew%20Jacob%20and%20Erik%20Lindgren%20and%20Matei%20Zaharia%20and%20Michael%20Carbin%20and%20Omar%20Khattab%20and%20Andrew%20Drozdov%0AAbstract%3A%20%20%20Rerankers%2C%20typically%20cross-encoders%2C%20are%20computationally%20intensive%20but%20are%0Afrequently%20used%20because%20they%20are%20widely%20assumed%20to%20outperform%20cheaper%20initial%0AIR%20systems.%20We%20challenge%20this%20assumption%20by%20measuring%20reranker%20performance%20for%0Afull%20retrieval%2C%20not%20just%20re-scoring%20first-stage%20retrieval.%20To%20provide%20a%20more%0Arobust%20evaluation%2C%20we%20prioritize%20strong%20first-stage%20retrieval%20using%20modern%0Adense%20embeddings%20and%20test%20rerankers%20on%20a%20variety%20of%20carefully%20chosen%2C%0Achallenging%20tasks%2C%20including%20internally%20curated%20datasets%20to%20avoid%0Acontamination%2C%20and%20out-of-domain%20ones.%20Our%20empirical%20results%20reveal%20a%0Asurprising%20trend%3A%20the%20best%20existing%20rerankers%20provide%20initial%20improvements%20when%0Ascoring%20progressively%20more%20documents%2C%20but%20their%20effectiveness%20gradually%0Adeclines%20and%20can%20even%20degrade%20quality%20beyond%20a%20certain%20limit.%20We%20hope%20that%20our%0Afindings%20will%20spur%20future%20research%20to%20improve%20reranking.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2411.11767v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DDrowning%2520in%2520Documents%253A%2520Consequences%2520of%2520Scaling%2520Reranker%2520Inference%26entry.906535625%3DMathew%2520Jacob%2520and%2520Erik%2520Lindgren%2520and%2520Matei%2520Zaharia%2520and%2520Michael%2520Carbin%2520and%2520Omar%2520Khattab%2520and%2520Andrew%2520Drozdov%26entry.1292438233%3D%2520%2520Rerankers%252C%2520typically%2520cross-encoders%252C%2520are%2520computationally%2520intensive%2520but%2520are%250Afrequently%2520used%2520because%2520they%2520are%2520widely%2520assumed%2520to%2520outperform%2520cheaper%2520initial%250AIR%2520systems.%2520We%2520challenge%2520this%2520assumption%2520by%2520measuring%2520reranker%2520performance%2520for%250Afull%2520retrieval%252C%2520not%2520just%2520re-scoring%2520first-stage%2520retrieval.%2520To%2520provide%2520a%2520more%250Arobust%2520evaluation%252C%2520we%2520prioritize%2520strong%2520first-stage%2520retrieval%2520using%2520modern%250Adense%2520embeddings%2520and%2520test%2520rerankers%2520on%2520a%2520variety%2520of%2520carefully%2520chosen%252C%250Achallenging%2520tasks%252C%2520including%2520internally%2520curated%2520datasets%2520to%2520avoid%250Acontamination%252C%2520and%2520out-of-domain%2520ones.%2520Our%2520empirical%2520results%2520reveal%2520a%250Asurprising%2520trend%253A%2520the%2520best%2520existing%2520rerankers%2520provide%2520initial%2520improvements%2520when%250Ascoring%2520progressively%2520more%2520documents%252C%2520but%2520their%2520effectiveness%2520gradually%250Adeclines%2520and%2520can%2520even%2520degrade%2520quality%2520beyond%2520a%2520certain%2520limit.%2520We%2520hope%2520that%2520our%250Afindings%2520will%2520spur%2520future%2520research%2520to%2520improve%2520reranking.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2411.11767v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Drowning%20in%20Documents%3A%20Consequences%20of%20Scaling%20Reranker%20Inference&entry.906535625=Mathew%20Jacob%20and%20Erik%20Lindgren%20and%20Matei%20Zaharia%20and%20Michael%20Carbin%20and%20Omar%20Khattab%20and%20Andrew%20Drozdov&entry.1292438233=%20%20Rerankers%2C%20typically%20cross-encoders%2C%20are%20computationally%20intensive%20but%20are%0Afrequently%20used%20because%20they%20are%20widely%20assumed%20to%20outperform%20cheaper%20initial%0AIR%20systems.%20We%20challenge%20this%20assumption%20by%20measuring%20reranker%20performance%20for%0Afull%20retrieval%2C%20not%20just%20re-scoring%20first-stage%20retrieval.%20To%20provide%20a%20more%0Arobust%20evaluation%2C%20we%20prioritize%20strong%20first-stage%20retrieval%20using%20modern%0Adense%20embeddings%20and%20test%20rerankers%20on%20a%20variety%20of%20carefully%20chosen%2C%0Achallenging%20tasks%2C%20including%20internally%20curated%20datasets%20to%20avoid%0Acontamination%2C%20and%20out-of-domain%20ones.%20Our%20empirical%20results%20reveal%20a%0Asurprising%20trend%3A%20the%20best%20existing%20rerankers%20provide%20initial%20improvements%20when%0Ascoring%20progressively%20more%20documents%2C%20but%20their%20effectiveness%20gradually%0Adeclines%20and%20can%20even%20degrade%20quality%20beyond%20a%20certain%20limit.%20We%20hope%20that%20our%0Afindings%20will%20spur%20future%20research%20to%20improve%20reranking.%0A&entry.1838667208=http%3A//arxiv.org/abs/2411.11767v2&entry.124074799=Read"},
{"title": "elsciRL: Integrating Language Solutions into Reinforcement Learning\n  Problem Settings", "author": "Philip Osborne and Danilo S. Carvalho and Andr\u00e9 Freitas", "abstract": "  We present elsciRL, an open-source Python library to facilitate the\napplication of language solutions on reinforcement learning problems. We\ndemonstrate the potential of our software by extending the Language Adapter\nwith Self-Completing Instruction framework defined in (Osborne, 2024) with the\nuse of LLMs. Our approach can be re-applied to new applications with minimal\nsetup requirements. We provide a novel GUI that allows a user to provide text\ninput for an LLM to generate instructions which it can then self-complete.\nEmpirical results indicate that these instructions \\textit{can} improve a\nreinforcement learning agent's performance. Therefore, we present this work to\naccelerate the evaluation of language solutions on reward based environments to\nenable new opportunities for scientific discovery.\n", "link": "http://arxiv.org/abs/2507.08705v1", "date": "2025-07-11", "relevancy": 1.9246, "topK": [{"title": "RoboEXP: Action-Conditioned Scene Graph via Interactive Exploration for\n  Robotic Manipulation", "link": "http://arxiv.org/abs/2402.15487v1", "similarity": 0.4987}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.4776}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.4776}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20elsciRL%3A%20Integrating%20Language%20Solutions%20into%20Reinforcement%20Learning%0A%20%20Problem%20Settings&body=Title%3A%20elsciRL%3A%20Integrating%20Language%20Solutions%20into%20Reinforcement%20Learning%0A%20%20Problem%20Settings%0AAuthor%3A%20Philip%20Osborne%20and%20Danilo%20S.%20Carvalho%20and%20Andr%C3%A9%20Freitas%0AAbstract%3A%20%20%20We%20present%20elsciRL%2C%20an%20open-source%20Python%20library%20to%20facilitate%20the%0Aapplication%20of%20language%20solutions%20on%20reinforcement%20learning%20problems.%20We%0Ademonstrate%20the%20potential%20of%20our%20software%20by%20extending%20the%20Language%20Adapter%0Awith%20Self-Completing%20Instruction%20framework%20defined%20in%20%28Osborne%2C%202024%29%20with%20the%0Ause%20of%20LLMs.%20Our%20approach%20can%20be%20re-applied%20to%20new%20applications%20with%20minimal%0Asetup%20requirements.%20We%20provide%20a%20novel%20GUI%20that%20allows%20a%20user%20to%20provide%20text%0Ainput%20for%20an%20LLM%20to%20generate%20instructions%20which%20it%20can%20then%20self-complete.%0AEmpirical%20results%20indicate%20that%20these%20instructions%20%5Ctextit%7Bcan%7D%20improve%20a%0Areinforcement%20learning%20agent%27s%20performance.%20Therefore%2C%20we%20present%20this%20work%20to%0Aaccelerate%20the%20evaluation%20of%20language%20solutions%20on%20reward%20based%20environments%20to%0Aenable%20new%20opportunities%20for%20scientific%20discovery.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2507.08705v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DelsciRL%253A%2520Integrating%2520Language%2520Solutions%2520into%2520Reinforcement%2520Learning%250A%2520%2520Problem%2520Settings%26entry.906535625%3DPhilip%2520Osborne%2520and%2520Danilo%2520S.%2520Carvalho%2520and%2520Andr%25C3%25A9%2520Freitas%26entry.1292438233%3D%2520%2520We%2520present%2520elsciRL%252C%2520an%2520open-source%2520Python%2520library%2520to%2520facilitate%2520the%250Aapplication%2520of%2520language%2520solutions%2520on%2520reinforcement%2520learning%2520problems.%2520We%250Ademonstrate%2520the%2520potential%2520of%2520our%2520software%2520by%2520extending%2520the%2520Language%2520Adapter%250Awith%2520Self-Completing%2520Instruction%2520framework%2520defined%2520in%2520%2528Osborne%252C%25202024%2529%2520with%2520the%250Ause%2520of%2520LLMs.%2520Our%2520approach%2520can%2520be%2520re-applied%2520to%2520new%2520applications%2520with%2520minimal%250Asetup%2520requirements.%2520We%2520provide%2520a%2520novel%2520GUI%2520that%2520allows%2520a%2520user%2520to%2520provide%2520text%250Ainput%2520for%2520an%2520LLM%2520to%2520generate%2520instructions%2520which%2520it%2520can%2520then%2520self-complete.%250AEmpirical%2520results%2520indicate%2520that%2520these%2520instructions%2520%255Ctextit%257Bcan%257D%2520improve%2520a%250Areinforcement%2520learning%2520agent%2527s%2520performance.%2520Therefore%252C%2520we%2520present%2520this%2520work%2520to%250Aaccelerate%2520the%2520evaluation%2520of%2520language%2520solutions%2520on%2520reward%2520based%2520environments%2520to%250Aenable%2520new%2520opportunities%2520for%2520scientific%2520discovery.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2507.08705v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=elsciRL%3A%20Integrating%20Language%20Solutions%20into%20Reinforcement%20Learning%0A%20%20Problem%20Settings&entry.906535625=Philip%20Osborne%20and%20Danilo%20S.%20Carvalho%20and%20Andr%C3%A9%20Freitas&entry.1292438233=%20%20We%20present%20elsciRL%2C%20an%20open-source%20Python%20library%20to%20facilitate%20the%0Aapplication%20of%20language%20solutions%20on%20reinforcement%20learning%20problems.%20We%0Ademonstrate%20the%20potential%20of%20our%20software%20by%20extending%20the%20Language%20Adapter%0Awith%20Self-Completing%20Instruction%20framework%20defined%20in%20%28Osborne%2C%202024%29%20with%20the%0Ause%20of%20LLMs.%20Our%20approach%20can%20be%20re-applied%20to%20new%20applications%20with%20minimal%0Asetup%20requirements.%20We%20provide%20a%20novel%20GUI%20that%20allows%20a%20user%20to%20provide%20text%0Ainput%20for%20an%20LLM%20to%20generate%20instructions%20which%20it%20can%20then%20self-complete.%0AEmpirical%20results%20indicate%20that%20these%20instructions%20%5Ctextit%7Bcan%7D%20improve%20a%0Areinforcement%20learning%20agent%27s%20performance.%20Therefore%2C%20we%20present%20this%20work%20to%0Aaccelerate%20the%20evaluation%20of%20language%20solutions%20on%20reward%20based%20environments%20to%0Aenable%20new%20opportunities%20for%20scientific%20discovery.%0A&entry.1838667208=http%3A//arxiv.org/abs/2507.08705v1&entry.124074799=Read"},
{"title": "KG-Attention: Knowledge Graph-Guided Attention at Test-Time via\n  Bidirectional Information Aggregation", "author": "Songlin Zhai and Guilin Qi and Yuan Meng", "abstract": "  Knowledge graphs (KGs) play a critical role in enhancing large language\nmodels (LLMs) by introducing structured and grounded knowledge into the\nlearning process. However, most existing KG-enhanced approaches rely on\nparameter-intensive fine-tuning, which risks catastrophic forgetting and\ndegrades the pretrained model's generalization. Moreover, they exhibit limited\nadaptability to real-time knowledge updates due to their static integration\nframeworks. To address these issues, we introduce the first test-time\nKG-augmented framework for LLMs, built around a dedicated knowledge\ngraph-guided attention (KGA) module that enables dynamic knowledge fusion\nwithout any parameter updates. The proposed KGA module augments the standard\nself-attention mechanism with two synergistic pathways: outward and inward\naggregation. Specifically, the outward pathway dynamically integrates external\nknowledge into input representations via input-driven KG fusion. This inward\naggregation complements the outward pathway by refining input representations\nthrough KG-guided filtering, suppressing task-irrelevant signals and amplifying\nknowledge-relevant patterns. Importantly, while the outward pathway handles\nknowledge fusion, the inward path selects the most relevant triples and feeds\nthem back into the fusion process, forming a closed-loop enhancement mechanism.\nBy synergistically combining these two pathways, the proposed method supports\nreal-time knowledge fusion exclusively at test-time, without any parameter\nmodification. Extensive experiments on five benchmarks verify the comparable\nknowledge fusion performance of KGA.\n", "link": "http://arxiv.org/abs/2507.08704v1", "date": "2025-07-11", "relevancy": 1.9202, "topK": [{"title": "UGG: Unified Generative Grasping", "link": "https://arxiv.org/abs/2311.16917", "similarity": 0.4965}, {"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.4775}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.4646}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20KG-Attention%3A%20Knowledge%20Graph-Guided%20Attention%20at%20Test-Time%20via%0A%20%20Bidirectional%20Information%20Aggregation&body=Title%3A%20KG-Attention%3A%20Knowledge%20Graph-Guided%20Attention%20at%20Test-Time%20via%0A%20%20Bidirectional%20Information%20Aggregation%0AAuthor%3A%20Songlin%20Zhai%20and%20Guilin%20Qi%20and%20Yuan%20Meng%0AAbstract%3A%20%20%20Knowledge%20graphs%20%28KGs%29%20play%20a%20critical%20role%20in%20enhancing%20large%20language%0Amodels%20%28LLMs%29%20by%20introducing%20structured%20and%20grounded%20knowledge%20into%20the%0Alearning%20process.%20However%2C%20most%20existing%20KG-enhanced%20approaches%20rely%20on%0Aparameter-intensive%20fine-tuning%2C%20which%20risks%20catastrophic%20forgetting%20and%0Adegrades%20the%20pretrained%20model%27s%20generalization.%20Moreover%2C%20they%20exhibit%20limited%0Aadaptability%20to%20real-time%20knowledge%20updates%20due%20to%20their%20static%20integration%0Aframeworks.%20To%20address%20these%20issues%2C%20we%20introduce%20the%20first%20test-time%0AKG-augmented%20framework%20for%20LLMs%2C%20built%20around%20a%20dedicated%20knowledge%0Agraph-guided%20attention%20%28KGA%29%20module%20that%20enables%20dynamic%20knowledge%20fusion%0Awithout%20any%20parameter%20updates.%20The%20proposed%20KGA%20module%20augments%20the%20standard%0Aself-attention%20mechanism%20with%20two%20synergistic%20pathways%3A%20outward%20and%20inward%0Aaggregation.%20Specifically%2C%20the%20outward%20pathway%20dynamically%20integrates%20external%0Aknowledge%20into%20input%20representations%20via%20input-driven%20KG%20fusion.%20This%20inward%0Aaggregation%20complements%20the%20outward%20pathway%20by%20refining%20input%20representations%0Athrough%20KG-guided%20filtering%2C%20suppressing%20task-irrelevant%20signals%20and%20amplifying%0Aknowledge-relevant%20patterns.%20Importantly%2C%20while%20the%20outward%20pathway%20handles%0Aknowledge%20fusion%2C%20the%20inward%20path%20selects%20the%20most%20relevant%20triples%20and%20feeds%0Athem%20back%20into%20the%20fusion%20process%2C%20forming%20a%20closed-loop%20enhancement%20mechanism.%0ABy%20synergistically%20combining%20these%20two%20pathways%2C%20the%20proposed%20method%20supports%0Areal-time%20knowledge%20fusion%20exclusively%20at%20test-time%2C%20without%20any%20parameter%0Amodification.%20Extensive%20experiments%20on%20five%20benchmarks%20verify%20the%20comparable%0Aknowledge%20fusion%20performance%20of%20KGA.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2507.08704v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DKG-Attention%253A%2520Knowledge%2520Graph-Guided%2520Attention%2520at%2520Test-Time%2520via%250A%2520%2520Bidirectional%2520Information%2520Aggregation%26entry.906535625%3DSonglin%2520Zhai%2520and%2520Guilin%2520Qi%2520and%2520Yuan%2520Meng%26entry.1292438233%3D%2520%2520Knowledge%2520graphs%2520%2528KGs%2529%2520play%2520a%2520critical%2520role%2520in%2520enhancing%2520large%2520language%250Amodels%2520%2528LLMs%2529%2520by%2520introducing%2520structured%2520and%2520grounded%2520knowledge%2520into%2520the%250Alearning%2520process.%2520However%252C%2520most%2520existing%2520KG-enhanced%2520approaches%2520rely%2520on%250Aparameter-intensive%2520fine-tuning%252C%2520which%2520risks%2520catastrophic%2520forgetting%2520and%250Adegrades%2520the%2520pretrained%2520model%2527s%2520generalization.%2520Moreover%252C%2520they%2520exhibit%2520limited%250Aadaptability%2520to%2520real-time%2520knowledge%2520updates%2520due%2520to%2520their%2520static%2520integration%250Aframeworks.%2520To%2520address%2520these%2520issues%252C%2520we%2520introduce%2520the%2520first%2520test-time%250AKG-augmented%2520framework%2520for%2520LLMs%252C%2520built%2520around%2520a%2520dedicated%2520knowledge%250Agraph-guided%2520attention%2520%2528KGA%2529%2520module%2520that%2520enables%2520dynamic%2520knowledge%2520fusion%250Awithout%2520any%2520parameter%2520updates.%2520The%2520proposed%2520KGA%2520module%2520augments%2520the%2520standard%250Aself-attention%2520mechanism%2520with%2520two%2520synergistic%2520pathways%253A%2520outward%2520and%2520inward%250Aaggregation.%2520Specifically%252C%2520the%2520outward%2520pathway%2520dynamically%2520integrates%2520external%250Aknowledge%2520into%2520input%2520representations%2520via%2520input-driven%2520KG%2520fusion.%2520This%2520inward%250Aaggregation%2520complements%2520the%2520outward%2520pathway%2520by%2520refining%2520input%2520representations%250Athrough%2520KG-guided%2520filtering%252C%2520suppressing%2520task-irrelevant%2520signals%2520and%2520amplifying%250Aknowledge-relevant%2520patterns.%2520Importantly%252C%2520while%2520the%2520outward%2520pathway%2520handles%250Aknowledge%2520fusion%252C%2520the%2520inward%2520path%2520selects%2520the%2520most%2520relevant%2520triples%2520and%2520feeds%250Athem%2520back%2520into%2520the%2520fusion%2520process%252C%2520forming%2520a%2520closed-loop%2520enhancement%2520mechanism.%250ABy%2520synergistically%2520combining%2520these%2520two%2520pathways%252C%2520the%2520proposed%2520method%2520supports%250Areal-time%2520knowledge%2520fusion%2520exclusively%2520at%2520test-time%252C%2520without%2520any%2520parameter%250Amodification.%2520Extensive%2520experiments%2520on%2520five%2520benchmarks%2520verify%2520the%2520comparable%250Aknowledge%2520fusion%2520performance%2520of%2520KGA.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2507.08704v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=KG-Attention%3A%20Knowledge%20Graph-Guided%20Attention%20at%20Test-Time%20via%0A%20%20Bidirectional%20Information%20Aggregation&entry.906535625=Songlin%20Zhai%20and%20Guilin%20Qi%20and%20Yuan%20Meng&entry.1292438233=%20%20Knowledge%20graphs%20%28KGs%29%20play%20a%20critical%20role%20in%20enhancing%20large%20language%0Amodels%20%28LLMs%29%20by%20introducing%20structured%20and%20grounded%20knowledge%20into%20the%0Alearning%20process.%20However%2C%20most%20existing%20KG-enhanced%20approaches%20rely%20on%0Aparameter-intensive%20fine-tuning%2C%20which%20risks%20catastrophic%20forgetting%20and%0Adegrades%20the%20pretrained%20model%27s%20generalization.%20Moreover%2C%20they%20exhibit%20limited%0Aadaptability%20to%20real-time%20knowledge%20updates%20due%20to%20their%20static%20integration%0Aframeworks.%20To%20address%20these%20issues%2C%20we%20introduce%20the%20first%20test-time%0AKG-augmented%20framework%20for%20LLMs%2C%20built%20around%20a%20dedicated%20knowledge%0Agraph-guided%20attention%20%28KGA%29%20module%20that%20enables%20dynamic%20knowledge%20fusion%0Awithout%20any%20parameter%20updates.%20The%20proposed%20KGA%20module%20augments%20the%20standard%0Aself-attention%20mechanism%20with%20two%20synergistic%20pathways%3A%20outward%20and%20inward%0Aaggregation.%20Specifically%2C%20the%20outward%20pathway%20dynamically%20integrates%20external%0Aknowledge%20into%20input%20representations%20via%20input-driven%20KG%20fusion.%20This%20inward%0Aaggregation%20complements%20the%20outward%20pathway%20by%20refining%20input%20representations%0Athrough%20KG-guided%20filtering%2C%20suppressing%20task-irrelevant%20signals%20and%20amplifying%0Aknowledge-relevant%20patterns.%20Importantly%2C%20while%20the%20outward%20pathway%20handles%0Aknowledge%20fusion%2C%20the%20inward%20path%20selects%20the%20most%20relevant%20triples%20and%20feeds%0Athem%20back%20into%20the%20fusion%20process%2C%20forming%20a%20closed-loop%20enhancement%20mechanism.%0ABy%20synergistically%20combining%20these%20two%20pathways%2C%20the%20proposed%20method%20supports%0Areal-time%20knowledge%20fusion%20exclusively%20at%20test-time%2C%20without%20any%20parameter%0Amodification.%20Extensive%20experiments%20on%20five%20benchmarks%20verify%20the%20comparable%0Aknowledge%20fusion%20performance%20of%20KGA.%0A&entry.1838667208=http%3A//arxiv.org/abs/2507.08704v1&entry.124074799=Read"},
{"title": "A Hybrid SMT-NRA Solver: Integrating 2D Cell-Jump-Based Local Search,\n  MCSAT and OpenCAD", "author": "Tianyi Ding and Haokun Li and Xinpeng Ni and Bican Xia and Tianqi Zhao", "abstract": "  In this paper, we propose a hybrid framework for Satisfiability Modulo the\nTheory of Nonlinear Real Arithmetic (SMT-NRA for short). First, we introduce a\ntwo-dimensional cell-jump move, called \\emph{$2d$-cell-jump}, generalizing the\nkey operation, cell-jump, of the local search method for SMT-NRA. Then, we\npropose an extended local search framework, named \\emph{$2d$-LS} (following the\nlocal search framework, LS, for SMT-NRA), integrating the model constructing\nsatisfiability calculus (MCSAT) framework to improve search efficiency. To\nfurther improve the efficiency of MCSAT, we implement a recently proposed\ntechnique called \\emph{sample-cell projection operator} for MCSAT, which is\nwell suited for CDCL-style search in the real domain and helps guide the search\naway from conflicting states. Finally, we present a hybrid framework for\nSMT-NRA integrating MCSAT, $2d$-LS and OpenCAD, to improve search efficiency\nthrough information exchange. The experimental results demonstrate improvements\nin local search performance, highlighting the effectiveness of the proposed\nmethods.\n", "link": "http://arxiv.org/abs/2507.00557v2", "date": "2025-07-11", "relevancy": 1.9059, "topK": [{"title": "RoboEXP: Action-Conditioned Scene Graph via Interactive Exploration for\n  Robotic Manipulation", "link": "http://arxiv.org/abs/2402.15487v1", "similarity": 0.4897}, {"title": "RTG-SLAM: Real-time 3D Reconstruction at Scale using Gaussian Splatting", "link": "http://arxiv.org/abs/2404.19706v1", "similarity": 0.4788}, {"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.4689}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20A%20Hybrid%20SMT-NRA%20Solver%3A%20Integrating%202D%20Cell-Jump-Based%20Local%20Search%2C%0A%20%20MCSAT%20and%20OpenCAD&body=Title%3A%20A%20Hybrid%20SMT-NRA%20Solver%3A%20Integrating%202D%20Cell-Jump-Based%20Local%20Search%2C%0A%20%20MCSAT%20and%20OpenCAD%0AAuthor%3A%20Tianyi%20Ding%20and%20Haokun%20Li%20and%20Xinpeng%20Ni%20and%20Bican%20Xia%20and%20Tianqi%20Zhao%0AAbstract%3A%20%20%20In%20this%20paper%2C%20we%20propose%20a%20hybrid%20framework%20for%20Satisfiability%20Modulo%20the%0ATheory%20of%20Nonlinear%20Real%20Arithmetic%20%28SMT-NRA%20for%20short%29.%20First%2C%20we%20introduce%20a%0Atwo-dimensional%20cell-jump%20move%2C%20called%20%5Cemph%7B%242d%24-cell-jump%7D%2C%20generalizing%20the%0Akey%20operation%2C%20cell-jump%2C%20of%20the%20local%20search%20method%20for%20SMT-NRA.%20Then%2C%20we%0Apropose%20an%20extended%20local%20search%20framework%2C%20named%20%5Cemph%7B%242d%24-LS%7D%20%28following%20the%0Alocal%20search%20framework%2C%20LS%2C%20for%20SMT-NRA%29%2C%20integrating%20the%20model%20constructing%0Asatisfiability%20calculus%20%28MCSAT%29%20framework%20to%20improve%20search%20efficiency.%20To%0Afurther%20improve%20the%20efficiency%20of%20MCSAT%2C%20we%20implement%20a%20recently%20proposed%0Atechnique%20called%20%5Cemph%7Bsample-cell%20projection%20operator%7D%20for%20MCSAT%2C%20which%20is%0Awell%20suited%20for%20CDCL-style%20search%20in%20the%20real%20domain%20and%20helps%20guide%20the%20search%0Aaway%20from%20conflicting%20states.%20Finally%2C%20we%20present%20a%20hybrid%20framework%20for%0ASMT-NRA%20integrating%20MCSAT%2C%20%242d%24-LS%20and%20OpenCAD%2C%20to%20improve%20search%20efficiency%0Athrough%20information%20exchange.%20The%20experimental%20results%20demonstrate%20improvements%0Ain%20local%20search%20performance%2C%20highlighting%20the%20effectiveness%20of%20the%20proposed%0Amethods.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2507.00557v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DA%2520Hybrid%2520SMT-NRA%2520Solver%253A%2520Integrating%25202D%2520Cell-Jump-Based%2520Local%2520Search%252C%250A%2520%2520MCSAT%2520and%2520OpenCAD%26entry.906535625%3DTianyi%2520Ding%2520and%2520Haokun%2520Li%2520and%2520Xinpeng%2520Ni%2520and%2520Bican%2520Xia%2520and%2520Tianqi%2520Zhao%26entry.1292438233%3D%2520%2520In%2520this%2520paper%252C%2520we%2520propose%2520a%2520hybrid%2520framework%2520for%2520Satisfiability%2520Modulo%2520the%250ATheory%2520of%2520Nonlinear%2520Real%2520Arithmetic%2520%2528SMT-NRA%2520for%2520short%2529.%2520First%252C%2520we%2520introduce%2520a%250Atwo-dimensional%2520cell-jump%2520move%252C%2520called%2520%255Cemph%257B%25242d%2524-cell-jump%257D%252C%2520generalizing%2520the%250Akey%2520operation%252C%2520cell-jump%252C%2520of%2520the%2520local%2520search%2520method%2520for%2520SMT-NRA.%2520Then%252C%2520we%250Apropose%2520an%2520extended%2520local%2520search%2520framework%252C%2520named%2520%255Cemph%257B%25242d%2524-LS%257D%2520%2528following%2520the%250Alocal%2520search%2520framework%252C%2520LS%252C%2520for%2520SMT-NRA%2529%252C%2520integrating%2520the%2520model%2520constructing%250Asatisfiability%2520calculus%2520%2528MCSAT%2529%2520framework%2520to%2520improve%2520search%2520efficiency.%2520To%250Afurther%2520improve%2520the%2520efficiency%2520of%2520MCSAT%252C%2520we%2520implement%2520a%2520recently%2520proposed%250Atechnique%2520called%2520%255Cemph%257Bsample-cell%2520projection%2520operator%257D%2520for%2520MCSAT%252C%2520which%2520is%250Awell%2520suited%2520for%2520CDCL-style%2520search%2520in%2520the%2520real%2520domain%2520and%2520helps%2520guide%2520the%2520search%250Aaway%2520from%2520conflicting%2520states.%2520Finally%252C%2520we%2520present%2520a%2520hybrid%2520framework%2520for%250ASMT-NRA%2520integrating%2520MCSAT%252C%2520%25242d%2524-LS%2520and%2520OpenCAD%252C%2520to%2520improve%2520search%2520efficiency%250Athrough%2520information%2520exchange.%2520The%2520experimental%2520results%2520demonstrate%2520improvements%250Ain%2520local%2520search%2520performance%252C%2520highlighting%2520the%2520effectiveness%2520of%2520the%2520proposed%250Amethods.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2507.00557v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=A%20Hybrid%20SMT-NRA%20Solver%3A%20Integrating%202D%20Cell-Jump-Based%20Local%20Search%2C%0A%20%20MCSAT%20and%20OpenCAD&entry.906535625=Tianyi%20Ding%20and%20Haokun%20Li%20and%20Xinpeng%20Ni%20and%20Bican%20Xia%20and%20Tianqi%20Zhao&entry.1292438233=%20%20In%20this%20paper%2C%20we%20propose%20a%20hybrid%20framework%20for%20Satisfiability%20Modulo%20the%0ATheory%20of%20Nonlinear%20Real%20Arithmetic%20%28SMT-NRA%20for%20short%29.%20First%2C%20we%20introduce%20a%0Atwo-dimensional%20cell-jump%20move%2C%20called%20%5Cemph%7B%242d%24-cell-jump%7D%2C%20generalizing%20the%0Akey%20operation%2C%20cell-jump%2C%20of%20the%20local%20search%20method%20for%20SMT-NRA.%20Then%2C%20we%0Apropose%20an%20extended%20local%20search%20framework%2C%20named%20%5Cemph%7B%242d%24-LS%7D%20%28following%20the%0Alocal%20search%20framework%2C%20LS%2C%20for%20SMT-NRA%29%2C%20integrating%20the%20model%20constructing%0Asatisfiability%20calculus%20%28MCSAT%29%20framework%20to%20improve%20search%20efficiency.%20To%0Afurther%20improve%20the%20efficiency%20of%20MCSAT%2C%20we%20implement%20a%20recently%20proposed%0Atechnique%20called%20%5Cemph%7Bsample-cell%20projection%20operator%7D%20for%20MCSAT%2C%20which%20is%0Awell%20suited%20for%20CDCL-style%20search%20in%20the%20real%20domain%20and%20helps%20guide%20the%20search%0Aaway%20from%20conflicting%20states.%20Finally%2C%20we%20present%20a%20hybrid%20framework%20for%0ASMT-NRA%20integrating%20MCSAT%2C%20%242d%24-LS%20and%20OpenCAD%2C%20to%20improve%20search%20efficiency%0Athrough%20information%20exchange.%20The%20experimental%20results%20demonstrate%20improvements%0Ain%20local%20search%20performance%2C%20highlighting%20the%20effectiveness%20of%20the%20proposed%0Amethods.%0A&entry.1838667208=http%3A//arxiv.org/abs/2507.00557v2&entry.124074799=Read"},
{"title": "Large Language Models in Mental Health Care: a Scoping Review", "author": "Yining Hua and Fenglin Liu and Kailai Yang and Zehan Li and Hongbin Na and Yi-han Sheu and Peilin Zhou and Lauren V. Moran and Sophia Ananiadou and David A. Clifton and Andrew Beam and John Torous", "abstract": "  Objectieve:This review aims to deliver a comprehensive analysis of Large\nLanguage Models (LLMs) utilization in mental health care, evaluating their\neffectiveness, identifying challenges, and exploring their potential for future\napplication. Materials and Methods: A systematic search was performed across\nmultiple databases including PubMed, Web of Science, Google Scholar, arXiv,\nmedRxiv, and PsyArXiv in November 2023. The review includes all types of\noriginal research, regardless of peer-review status, published or disseminated\nbetween October 1, 2019, and December 2, 2023. Studies were included without\nlanguage restrictions if they employed LLMs developed after T5 and directly\ninvestigated research questions within mental health care settings. Results:\nOut of an initial 313 articles, 34 were selected based on their relevance to\nLLMs applications in mental health care and the rigor of their reported\noutcomes. The review identified various LLMs applications in mental health\ncare, including diagnostics, therapy, and enhancing patient engagement. Key\nchallenges highlighted were related to data availability and reliability, the\nnuanced handling of mental states, and effective evaluation methods. While LLMs\nshowed promise in improving accuracy and accessibility, significant gaps in\nclinical applicability and ethical considerations were noted. Conclusion: LLMs\nhold substantial promise for enhancing mental health care. For their full\npotential to be realized, emphasis must be placed on developing robust\ndatasets, development and evaluation frameworks, ethical guidelines, and\ninterdisciplinary collaborations to address current limitations.\n", "link": "http://arxiv.org/abs/2401.02984v3", "date": "2025-07-11", "relevancy": 1.895, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.4859}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.4859}, {"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.4129}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Large%20Language%20Models%20in%20Mental%20Health%20Care%3A%20a%20Scoping%20Review&body=Title%3A%20Large%20Language%20Models%20in%20Mental%20Health%20Care%3A%20a%20Scoping%20Review%0AAuthor%3A%20Yining%20Hua%20and%20Fenglin%20Liu%20and%20Kailai%20Yang%20and%20Zehan%20Li%20and%20Hongbin%20Na%20and%20Yi-han%20Sheu%20and%20Peilin%20Zhou%20and%20Lauren%20V.%20Moran%20and%20Sophia%20Ananiadou%20and%20David%20A.%20Clifton%20and%20Andrew%20Beam%20and%20John%20Torous%0AAbstract%3A%20%20%20Objectieve%3AThis%20review%20aims%20to%20deliver%20a%20comprehensive%20analysis%20of%20Large%0ALanguage%20Models%20%28LLMs%29%20utilization%20in%20mental%20health%20care%2C%20evaluating%20their%0Aeffectiveness%2C%20identifying%20challenges%2C%20and%20exploring%20their%20potential%20for%20future%0Aapplication.%20Materials%20and%20Methods%3A%20A%20systematic%20search%20was%20performed%20across%0Amultiple%20databases%20including%20PubMed%2C%20Web%20of%20Science%2C%20Google%20Scholar%2C%20arXiv%2C%0AmedRxiv%2C%20and%20PsyArXiv%20in%20November%202023.%20The%20review%20includes%20all%20types%20of%0Aoriginal%20research%2C%20regardless%20of%20peer-review%20status%2C%20published%20or%20disseminated%0Abetween%20October%201%2C%202019%2C%20and%20December%202%2C%202023.%20Studies%20were%20included%20without%0Alanguage%20restrictions%20if%20they%20employed%20LLMs%20developed%20after%20T5%20and%20directly%0Ainvestigated%20research%20questions%20within%20mental%20health%20care%20settings.%20Results%3A%0AOut%20of%20an%20initial%20313%20articles%2C%2034%20were%20selected%20based%20on%20their%20relevance%20to%0ALLMs%20applications%20in%20mental%20health%20care%20and%20the%20rigor%20of%20their%20reported%0Aoutcomes.%20The%20review%20identified%20various%20LLMs%20applications%20in%20mental%20health%0Acare%2C%20including%20diagnostics%2C%20therapy%2C%20and%20enhancing%20patient%20engagement.%20Key%0Achallenges%20highlighted%20were%20related%20to%20data%20availability%20and%20reliability%2C%20the%0Anuanced%20handling%20of%20mental%20states%2C%20and%20effective%20evaluation%20methods.%20While%20LLMs%0Ashowed%20promise%20in%20improving%20accuracy%20and%20accessibility%2C%20significant%20gaps%20in%0Aclinical%20applicability%20and%20ethical%20considerations%20were%20noted.%20Conclusion%3A%20LLMs%0Ahold%20substantial%20promise%20for%20enhancing%20mental%20health%20care.%20For%20their%20full%0Apotential%20to%20be%20realized%2C%20emphasis%20must%20be%20placed%20on%20developing%20robust%0Adatasets%2C%20development%20and%20evaluation%20frameworks%2C%20ethical%20guidelines%2C%20and%0Ainterdisciplinary%20collaborations%20to%20address%20current%20limitations.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2401.02984v3%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DLarge%2520Language%2520Models%2520in%2520Mental%2520Health%2520Care%253A%2520a%2520Scoping%2520Review%26entry.906535625%3DYining%2520Hua%2520and%2520Fenglin%2520Liu%2520and%2520Kailai%2520Yang%2520and%2520Zehan%2520Li%2520and%2520Hongbin%2520Na%2520and%2520Yi-han%2520Sheu%2520and%2520Peilin%2520Zhou%2520and%2520Lauren%2520V.%2520Moran%2520and%2520Sophia%2520Ananiadou%2520and%2520David%2520A.%2520Clifton%2520and%2520Andrew%2520Beam%2520and%2520John%2520Torous%26entry.1292438233%3D%2520%2520Objectieve%253AThis%2520review%2520aims%2520to%2520deliver%2520a%2520comprehensive%2520analysis%2520of%2520Large%250ALanguage%2520Models%2520%2528LLMs%2529%2520utilization%2520in%2520mental%2520health%2520care%252C%2520evaluating%2520their%250Aeffectiveness%252C%2520identifying%2520challenges%252C%2520and%2520exploring%2520their%2520potential%2520for%2520future%250Aapplication.%2520Materials%2520and%2520Methods%253A%2520A%2520systematic%2520search%2520was%2520performed%2520across%250Amultiple%2520databases%2520including%2520PubMed%252C%2520Web%2520of%2520Science%252C%2520Google%2520Scholar%252C%2520arXiv%252C%250AmedRxiv%252C%2520and%2520PsyArXiv%2520in%2520November%25202023.%2520The%2520review%2520includes%2520all%2520types%2520of%250Aoriginal%2520research%252C%2520regardless%2520of%2520peer-review%2520status%252C%2520published%2520or%2520disseminated%250Abetween%2520October%25201%252C%25202019%252C%2520and%2520December%25202%252C%25202023.%2520Studies%2520were%2520included%2520without%250Alanguage%2520restrictions%2520if%2520they%2520employed%2520LLMs%2520developed%2520after%2520T5%2520and%2520directly%250Ainvestigated%2520research%2520questions%2520within%2520mental%2520health%2520care%2520settings.%2520Results%253A%250AOut%2520of%2520an%2520initial%2520313%2520articles%252C%252034%2520were%2520selected%2520based%2520on%2520their%2520relevance%2520to%250ALLMs%2520applications%2520in%2520mental%2520health%2520care%2520and%2520the%2520rigor%2520of%2520their%2520reported%250Aoutcomes.%2520The%2520review%2520identified%2520various%2520LLMs%2520applications%2520in%2520mental%2520health%250Acare%252C%2520including%2520diagnostics%252C%2520therapy%252C%2520and%2520enhancing%2520patient%2520engagement.%2520Key%250Achallenges%2520highlighted%2520were%2520related%2520to%2520data%2520availability%2520and%2520reliability%252C%2520the%250Anuanced%2520handling%2520of%2520mental%2520states%252C%2520and%2520effective%2520evaluation%2520methods.%2520While%2520LLMs%250Ashowed%2520promise%2520in%2520improving%2520accuracy%2520and%2520accessibility%252C%2520significant%2520gaps%2520in%250Aclinical%2520applicability%2520and%2520ethical%2520considerations%2520were%2520noted.%2520Conclusion%253A%2520LLMs%250Ahold%2520substantial%2520promise%2520for%2520enhancing%2520mental%2520health%2520care.%2520For%2520their%2520full%250Apotential%2520to%2520be%2520realized%252C%2520emphasis%2520must%2520be%2520placed%2520on%2520developing%2520robust%250Adatasets%252C%2520development%2520and%2520evaluation%2520frameworks%252C%2520ethical%2520guidelines%252C%2520and%250Ainterdisciplinary%2520collaborations%2520to%2520address%2520current%2520limitations.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2401.02984v3%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Large%20Language%20Models%20in%20Mental%20Health%20Care%3A%20a%20Scoping%20Review&entry.906535625=Yining%20Hua%20and%20Fenglin%20Liu%20and%20Kailai%20Yang%20and%20Zehan%20Li%20and%20Hongbin%20Na%20and%20Yi-han%20Sheu%20and%20Peilin%20Zhou%20and%20Lauren%20V.%20Moran%20and%20Sophia%20Ananiadou%20and%20David%20A.%20Clifton%20and%20Andrew%20Beam%20and%20John%20Torous&entry.1292438233=%20%20Objectieve%3AThis%20review%20aims%20to%20deliver%20a%20comprehensive%20analysis%20of%20Large%0ALanguage%20Models%20%28LLMs%29%20utilization%20in%20mental%20health%20care%2C%20evaluating%20their%0Aeffectiveness%2C%20identifying%20challenges%2C%20and%20exploring%20their%20potential%20for%20future%0Aapplication.%20Materials%20and%20Methods%3A%20A%20systematic%20search%20was%20performed%20across%0Amultiple%20databases%20including%20PubMed%2C%20Web%20of%20Science%2C%20Google%20Scholar%2C%20arXiv%2C%0AmedRxiv%2C%20and%20PsyArXiv%20in%20November%202023.%20The%20review%20includes%20all%20types%20of%0Aoriginal%20research%2C%20regardless%20of%20peer-review%20status%2C%20published%20or%20disseminated%0Abetween%20October%201%2C%202019%2C%20and%20December%202%2C%202023.%20Studies%20were%20included%20without%0Alanguage%20restrictions%20if%20they%20employed%20LLMs%20developed%20after%20T5%20and%20directly%0Ainvestigated%20research%20questions%20within%20mental%20health%20care%20settings.%20Results%3A%0AOut%20of%20an%20initial%20313%20articles%2C%2034%20were%20selected%20based%20on%20their%20relevance%20to%0ALLMs%20applications%20in%20mental%20health%20care%20and%20the%20rigor%20of%20their%20reported%0Aoutcomes.%20The%20review%20identified%20various%20LLMs%20applications%20in%20mental%20health%0Acare%2C%20including%20diagnostics%2C%20therapy%2C%20and%20enhancing%20patient%20engagement.%20Key%0Achallenges%20highlighted%20were%20related%20to%20data%20availability%20and%20reliability%2C%20the%0Anuanced%20handling%20of%20mental%20states%2C%20and%20effective%20evaluation%20methods.%20While%20LLMs%0Ashowed%20promise%20in%20improving%20accuracy%20and%20accessibility%2C%20significant%20gaps%20in%0Aclinical%20applicability%20and%20ethical%20considerations%20were%20noted.%20Conclusion%3A%20LLMs%0Ahold%20substantial%20promise%20for%20enhancing%20mental%20health%20care.%20For%20their%20full%0Apotential%20to%20be%20realized%2C%20emphasis%20must%20be%20placed%20on%20developing%20robust%0Adatasets%2C%20development%20and%20evaluation%20frameworks%2C%20ethical%20guidelines%2C%20and%0Ainterdisciplinary%20collaborations%20to%20address%20current%20limitations.%0A&entry.1838667208=http%3A//arxiv.org/abs/2401.02984v3&entry.124074799=Read"},
{"title": "Riemannian Time Warping: Multiple Sequence Alignment in Curved Spaces", "author": "Julian Richter and Christopher Erd\u00f6s and Christian Scheurer and Jochen J. Steil and Niels Dehio", "abstract": "  Temporal alignment of multiple signals through time warping is crucial in\nmany fields, such as classification within speech recognition or robot motion\nlearning. Almost all related works are limited to data in Euclidean space.\nAlthough an attempt was made in 2011 to adapt this concept to unit quaternions,\na general extension to Riemannian manifolds remains absent. Given its\nimportance for numerous applications in robotics and beyond, we introduce\nRiemannian Time Warping (RTW). This novel approach efficiently aligns multiple\nsignals by considering the geometric structure of the Riemannian manifold in\nwhich the data is embedded. Extensive experiments on synthetic and real-world\ndata, including tests with an LBR iiwa robot, demonstrate that RTW consistently\noutperforms state-of-the-art baselines in both averaging and classification\ntasks.\n", "link": "http://arxiv.org/abs/2506.01635v2", "date": "2025-07-11", "relevancy": 1.8941, "topK": [{"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.4751}, {"title": "RTG-SLAM: Real-time 3D Reconstruction at Scale using Gaussian Splatting", "link": "http://arxiv.org/abs/2404.19706v1", "similarity": 0.4745}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.4672}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Riemannian%20Time%20Warping%3A%20Multiple%20Sequence%20Alignment%20in%20Curved%20Spaces&body=Title%3A%20Riemannian%20Time%20Warping%3A%20Multiple%20Sequence%20Alignment%20in%20Curved%20Spaces%0AAuthor%3A%20Julian%20Richter%20and%20Christopher%20Erd%C3%B6s%20and%20Christian%20Scheurer%20and%20Jochen%20J.%20Steil%20and%20Niels%20Dehio%0AAbstract%3A%20%20%20Temporal%20alignment%20of%20multiple%20signals%20through%20time%20warping%20is%20crucial%20in%0Amany%20fields%2C%20such%20as%20classification%20within%20speech%20recognition%20or%20robot%20motion%0Alearning.%20Almost%20all%20related%20works%20are%20limited%20to%20data%20in%20Euclidean%20space.%0AAlthough%20an%20attempt%20was%20made%20in%202011%20to%20adapt%20this%20concept%20to%20unit%20quaternions%2C%0Aa%20general%20extension%20to%20Riemannian%20manifolds%20remains%20absent.%20Given%20its%0Aimportance%20for%20numerous%20applications%20in%20robotics%20and%20beyond%2C%20we%20introduce%0ARiemannian%20Time%20Warping%20%28RTW%29.%20This%20novel%20approach%20efficiently%20aligns%20multiple%0Asignals%20by%20considering%20the%20geometric%20structure%20of%20the%20Riemannian%20manifold%20in%0Awhich%20the%20data%20is%20embedded.%20Extensive%20experiments%20on%20synthetic%20and%20real-world%0Adata%2C%20including%20tests%20with%20an%20LBR%20iiwa%20robot%2C%20demonstrate%20that%20RTW%20consistently%0Aoutperforms%20state-of-the-art%20baselines%20in%20both%20averaging%20and%20classification%0Atasks.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2506.01635v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DRiemannian%2520Time%2520Warping%253A%2520Multiple%2520Sequence%2520Alignment%2520in%2520Curved%2520Spaces%26entry.906535625%3DJulian%2520Richter%2520and%2520Christopher%2520Erd%25C3%25B6s%2520and%2520Christian%2520Scheurer%2520and%2520Jochen%2520J.%2520Steil%2520and%2520Niels%2520Dehio%26entry.1292438233%3D%2520%2520Temporal%2520alignment%2520of%2520multiple%2520signals%2520through%2520time%2520warping%2520is%2520crucial%2520in%250Amany%2520fields%252C%2520such%2520as%2520classification%2520within%2520speech%2520recognition%2520or%2520robot%2520motion%250Alearning.%2520Almost%2520all%2520related%2520works%2520are%2520limited%2520to%2520data%2520in%2520Euclidean%2520space.%250AAlthough%2520an%2520attempt%2520was%2520made%2520in%25202011%2520to%2520adapt%2520this%2520concept%2520to%2520unit%2520quaternions%252C%250Aa%2520general%2520extension%2520to%2520Riemannian%2520manifolds%2520remains%2520absent.%2520Given%2520its%250Aimportance%2520for%2520numerous%2520applications%2520in%2520robotics%2520and%2520beyond%252C%2520we%2520introduce%250ARiemannian%2520Time%2520Warping%2520%2528RTW%2529.%2520This%2520novel%2520approach%2520efficiently%2520aligns%2520multiple%250Asignals%2520by%2520considering%2520the%2520geometric%2520structure%2520of%2520the%2520Riemannian%2520manifold%2520in%250Awhich%2520the%2520data%2520is%2520embedded.%2520Extensive%2520experiments%2520on%2520synthetic%2520and%2520real-world%250Adata%252C%2520including%2520tests%2520with%2520an%2520LBR%2520iiwa%2520robot%252C%2520demonstrate%2520that%2520RTW%2520consistently%250Aoutperforms%2520state-of-the-art%2520baselines%2520in%2520both%2520averaging%2520and%2520classification%250Atasks.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2506.01635v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Riemannian%20Time%20Warping%3A%20Multiple%20Sequence%20Alignment%20in%20Curved%20Spaces&entry.906535625=Julian%20Richter%20and%20Christopher%20Erd%C3%B6s%20and%20Christian%20Scheurer%20and%20Jochen%20J.%20Steil%20and%20Niels%20Dehio&entry.1292438233=%20%20Temporal%20alignment%20of%20multiple%20signals%20through%20time%20warping%20is%20crucial%20in%0Amany%20fields%2C%20such%20as%20classification%20within%20speech%20recognition%20or%20robot%20motion%0Alearning.%20Almost%20all%20related%20works%20are%20limited%20to%20data%20in%20Euclidean%20space.%0AAlthough%20an%20attempt%20was%20made%20in%202011%20to%20adapt%20this%20concept%20to%20unit%20quaternions%2C%0Aa%20general%20extension%20to%20Riemannian%20manifolds%20remains%20absent.%20Given%20its%0Aimportance%20for%20numerous%20applications%20in%20robotics%20and%20beyond%2C%20we%20introduce%0ARiemannian%20Time%20Warping%20%28RTW%29.%20This%20novel%20approach%20efficiently%20aligns%20multiple%0Asignals%20by%20considering%20the%20geometric%20structure%20of%20the%20Riemannian%20manifold%20in%0Awhich%20the%20data%20is%20embedded.%20Extensive%20experiments%20on%20synthetic%20and%20real-world%0Adata%2C%20including%20tests%20with%20an%20LBR%20iiwa%20robot%2C%20demonstrate%20that%20RTW%20consistently%0Aoutperforms%20state-of-the-art%20baselines%20in%20both%20averaging%20and%20classification%0Atasks.%0A&entry.1838667208=http%3A//arxiv.org/abs/2506.01635v2&entry.124074799=Read"},
{"title": "Hashing for Fast Pattern Set Selection", "author": "Maiju Karjalainen and Pauli Miettinen", "abstract": "  Pattern set mining, which is the task of finding a good set of patterns\ninstead of all patterns, is a fundamental problem in data mining. Many\ndifferent definitions of what constitutes a good set have been proposed in\nrecent years. In this paper, we consider the reconstruction error as a proxy\nmeasure for the goodness of the set, and concentrate on the adjacent problem of\nhow to find a good set efficiently. We propose a method based on bottom-k\nhashing for efficiently selecting the set and extend the method for the common\ncase where the patterns might only appear in approximate form in the data. Our\napproach has applications in tiling databases, Boolean matrix factorization,\nand redescription mining, among others. We show that our hashing-based approach\nis significantly faster than the standard greedy algorithm while obtaining\nalmost equally good results in both synthetic and real-world data sets.\n", "link": "http://arxiv.org/abs/2507.08745v1", "date": "2025-07-11", "relevancy": 1.8832, "topK": [{"title": "Breadcrumbs: Adversarial class-balanced sampling for long-tailed recognition", "link": "https://link.springer.com/chapter/10.1007/978-3-031-20053-3_37", "similarity": 0.3996}, {"title": "ToDo: Token Downsampling for Efficient Generation of High-Resolution\n  Images", "link": "http://arxiv.org/abs/2402.13573v2", "similarity": 0.3728}, {"title": "DressCode: Autoregressively Sewing and Generating Garments from Text\n  Guidance", "link": "http://arxiv.org/abs/2401.16465v3", "similarity": 0.3575}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Hashing%20for%20Fast%20Pattern%20Set%20Selection&body=Title%3A%20Hashing%20for%20Fast%20Pattern%20Set%20Selection%0AAuthor%3A%20Maiju%20Karjalainen%20and%20Pauli%20Miettinen%0AAbstract%3A%20%20%20Pattern%20set%20mining%2C%20which%20is%20the%20task%20of%20finding%20a%20good%20set%20of%20patterns%0Ainstead%20of%20all%20patterns%2C%20is%20a%20fundamental%20problem%20in%20data%20mining.%20Many%0Adifferent%20definitions%20of%20what%20constitutes%20a%20good%20set%20have%20been%20proposed%20in%0Arecent%20years.%20In%20this%20paper%2C%20we%20consider%20the%20reconstruction%20error%20as%20a%20proxy%0Ameasure%20for%20the%20goodness%20of%20the%20set%2C%20and%20concentrate%20on%20the%20adjacent%20problem%20of%0Ahow%20to%20find%20a%20good%20set%20efficiently.%20We%20propose%20a%20method%20based%20on%20bottom-k%0Ahashing%20for%20efficiently%20selecting%20the%20set%20and%20extend%20the%20method%20for%20the%20common%0Acase%20where%20the%20patterns%20might%20only%20appear%20in%20approximate%20form%20in%20the%20data.%20Our%0Aapproach%20has%20applications%20in%20tiling%20databases%2C%20Boolean%20matrix%20factorization%2C%0Aand%20redescription%20mining%2C%20among%20others.%20We%20show%20that%20our%20hashing-based%20approach%0Ais%20significantly%20faster%20than%20the%20standard%20greedy%20algorithm%20while%20obtaining%0Aalmost%20equally%20good%20results%20in%20both%20synthetic%20and%20real-world%20data%20sets.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2507.08745v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DHashing%2520for%2520Fast%2520Pattern%2520Set%2520Selection%26entry.906535625%3DMaiju%2520Karjalainen%2520and%2520Pauli%2520Miettinen%26entry.1292438233%3D%2520%2520Pattern%2520set%2520mining%252C%2520which%2520is%2520the%2520task%2520of%2520finding%2520a%2520good%2520set%2520of%2520patterns%250Ainstead%2520of%2520all%2520patterns%252C%2520is%2520a%2520fundamental%2520problem%2520in%2520data%2520mining.%2520Many%250Adifferent%2520definitions%2520of%2520what%2520constitutes%2520a%2520good%2520set%2520have%2520been%2520proposed%2520in%250Arecent%2520years.%2520In%2520this%2520paper%252C%2520we%2520consider%2520the%2520reconstruction%2520error%2520as%2520a%2520proxy%250Ameasure%2520for%2520the%2520goodness%2520of%2520the%2520set%252C%2520and%2520concentrate%2520on%2520the%2520adjacent%2520problem%2520of%250Ahow%2520to%2520find%2520a%2520good%2520set%2520efficiently.%2520We%2520propose%2520a%2520method%2520based%2520on%2520bottom-k%250Ahashing%2520for%2520efficiently%2520selecting%2520the%2520set%2520and%2520extend%2520the%2520method%2520for%2520the%2520common%250Acase%2520where%2520the%2520patterns%2520might%2520only%2520appear%2520in%2520approximate%2520form%2520in%2520the%2520data.%2520Our%250Aapproach%2520has%2520applications%2520in%2520tiling%2520databases%252C%2520Boolean%2520matrix%2520factorization%252C%250Aand%2520redescription%2520mining%252C%2520among%2520others.%2520We%2520show%2520that%2520our%2520hashing-based%2520approach%250Ais%2520significantly%2520faster%2520than%2520the%2520standard%2520greedy%2520algorithm%2520while%2520obtaining%250Aalmost%2520equally%2520good%2520results%2520in%2520both%2520synthetic%2520and%2520real-world%2520data%2520sets.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2507.08745v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Hashing%20for%20Fast%20Pattern%20Set%20Selection&entry.906535625=Maiju%20Karjalainen%20and%20Pauli%20Miettinen&entry.1292438233=%20%20Pattern%20set%20mining%2C%20which%20is%20the%20task%20of%20finding%20a%20good%20set%20of%20patterns%0Ainstead%20of%20all%20patterns%2C%20is%20a%20fundamental%20problem%20in%20data%20mining.%20Many%0Adifferent%20definitions%20of%20what%20constitutes%20a%20good%20set%20have%20been%20proposed%20in%0Arecent%20years.%20In%20this%20paper%2C%20we%20consider%20the%20reconstruction%20error%20as%20a%20proxy%0Ameasure%20for%20the%20goodness%20of%20the%20set%2C%20and%20concentrate%20on%20the%20adjacent%20problem%20of%0Ahow%20to%20find%20a%20good%20set%20efficiently.%20We%20propose%20a%20method%20based%20on%20bottom-k%0Ahashing%20for%20efficiently%20selecting%20the%20set%20and%20extend%20the%20method%20for%20the%20common%0Acase%20where%20the%20patterns%20might%20only%20appear%20in%20approximate%20form%20in%20the%20data.%20Our%0Aapproach%20has%20applications%20in%20tiling%20databases%2C%20Boolean%20matrix%20factorization%2C%0Aand%20redescription%20mining%2C%20among%20others.%20We%20show%20that%20our%20hashing-based%20approach%0Ais%20significantly%20faster%20than%20the%20standard%20greedy%20algorithm%20while%20obtaining%0Aalmost%20equally%20good%20results%20in%20both%20synthetic%20and%20real-world%20data%20sets.%0A&entry.1838667208=http%3A//arxiv.org/abs/2507.08745v1&entry.124074799=Read"},
{"title": "A Personalised Formal Verification Framework for Monitoring Activities\n  of Daily Living of Older Adults Living Independently in Their Homes", "author": "Ricardo Contreras and Filip Smola and Nu\u0161a Fari\u010d and Jiawei Zheng and Jane Hillston and Jacques D. Fleuriot", "abstract": "  There is an imperative need to provide quality of life to a growing\npopulation of older adults living independently. Personalised solutions that\nfocus on the person and take into consideration their preferences and context\nare key. In this work, we introduce a framework for representing and reasoning\nabout the Activities of Daily Living of older adults living independently at\nhome. The framework integrates data from sensors and contextual information\nthat aggregates semi-structured interviews, home layouts and sociological\nobservations from the participants. We use these data to create formal models,\npersonalised for each participant according to their preferences and context.\nWe formulate requirements that are specific to each individual as properties\nencoded in Linear Temporal Logic and use a model checker to verify whether each\nproperty is satisfied by the model. When a property is violated, a\ncounterexample is generated giving the cause of the violation. We demonstrate\nthe framework's generalisability by applying it to different participants,\nhighlighting its potential to enhance the safety and well-being of older adults\nageing in place.\n", "link": "http://arxiv.org/abs/2507.08701v1", "date": "2025-07-11", "relevancy": 1.8399, "topK": [{"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.4946}, {"title": "Egocentric Computer Vision for Hands-Free Robotic Wheelchair Navigation", "link": "https://link.springer.com/article/10.1007/s10846-023-01807-4", "similarity": 0.4576}, {"title": "Weakly-guided self-supervised pretraining for temporal activity detection", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/25189", "similarity": 0.4485}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20A%20Personalised%20Formal%20Verification%20Framework%20for%20Monitoring%20Activities%0A%20%20of%20Daily%20Living%20of%20Older%20Adults%20Living%20Independently%20in%20Their%20Homes&body=Title%3A%20A%20Personalised%20Formal%20Verification%20Framework%20for%20Monitoring%20Activities%0A%20%20of%20Daily%20Living%20of%20Older%20Adults%20Living%20Independently%20in%20Their%20Homes%0AAuthor%3A%20Ricardo%20Contreras%20and%20Filip%20Smola%20and%20Nu%C5%A1a%20Fari%C4%8D%20and%20Jiawei%20Zheng%20and%20Jane%20Hillston%20and%20Jacques%20D.%20Fleuriot%0AAbstract%3A%20%20%20There%20is%20an%20imperative%20need%20to%20provide%20quality%20of%20life%20to%20a%20growing%0Apopulation%20of%20older%20adults%20living%20independently.%20Personalised%20solutions%20that%0Afocus%20on%20the%20person%20and%20take%20into%20consideration%20their%20preferences%20and%20context%0Aare%20key.%20In%20this%20work%2C%20we%20introduce%20a%20framework%20for%20representing%20and%20reasoning%0Aabout%20the%20Activities%20of%20Daily%20Living%20of%20older%20adults%20living%20independently%20at%0Ahome.%20The%20framework%20integrates%20data%20from%20sensors%20and%20contextual%20information%0Athat%20aggregates%20semi-structured%20interviews%2C%20home%20layouts%20and%20sociological%0Aobservations%20from%20the%20participants.%20We%20use%20these%20data%20to%20create%20formal%20models%2C%0Apersonalised%20for%20each%20participant%20according%20to%20their%20preferences%20and%20context.%0AWe%20formulate%20requirements%20that%20are%20specific%20to%20each%20individual%20as%20properties%0Aencoded%20in%20Linear%20Temporal%20Logic%20and%20use%20a%20model%20checker%20to%20verify%20whether%20each%0Aproperty%20is%20satisfied%20by%20the%20model.%20When%20a%20property%20is%20violated%2C%20a%0Acounterexample%20is%20generated%20giving%20the%20cause%20of%20the%20violation.%20We%20demonstrate%0Athe%20framework%27s%20generalisability%20by%20applying%20it%20to%20different%20participants%2C%0Ahighlighting%20its%20potential%20to%20enhance%20the%20safety%20and%20well-being%20of%20older%20adults%0Aageing%20in%20place.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2507.08701v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DA%2520Personalised%2520Formal%2520Verification%2520Framework%2520for%2520Monitoring%2520Activities%250A%2520%2520of%2520Daily%2520Living%2520of%2520Older%2520Adults%2520Living%2520Independently%2520in%2520Their%2520Homes%26entry.906535625%3DRicardo%2520Contreras%2520and%2520Filip%2520Smola%2520and%2520Nu%25C5%25A1a%2520Fari%25C4%258D%2520and%2520Jiawei%2520Zheng%2520and%2520Jane%2520Hillston%2520and%2520Jacques%2520D.%2520Fleuriot%26entry.1292438233%3D%2520%2520There%2520is%2520an%2520imperative%2520need%2520to%2520provide%2520quality%2520of%2520life%2520to%2520a%2520growing%250Apopulation%2520of%2520older%2520adults%2520living%2520independently.%2520Personalised%2520solutions%2520that%250Afocus%2520on%2520the%2520person%2520and%2520take%2520into%2520consideration%2520their%2520preferences%2520and%2520context%250Aare%2520key.%2520In%2520this%2520work%252C%2520we%2520introduce%2520a%2520framework%2520for%2520representing%2520and%2520reasoning%250Aabout%2520the%2520Activities%2520of%2520Daily%2520Living%2520of%2520older%2520adults%2520living%2520independently%2520at%250Ahome.%2520The%2520framework%2520integrates%2520data%2520from%2520sensors%2520and%2520contextual%2520information%250Athat%2520aggregates%2520semi-structured%2520interviews%252C%2520home%2520layouts%2520and%2520sociological%250Aobservations%2520from%2520the%2520participants.%2520We%2520use%2520these%2520data%2520to%2520create%2520formal%2520models%252C%250Apersonalised%2520for%2520each%2520participant%2520according%2520to%2520their%2520preferences%2520and%2520context.%250AWe%2520formulate%2520requirements%2520that%2520are%2520specific%2520to%2520each%2520individual%2520as%2520properties%250Aencoded%2520in%2520Linear%2520Temporal%2520Logic%2520and%2520use%2520a%2520model%2520checker%2520to%2520verify%2520whether%2520each%250Aproperty%2520is%2520satisfied%2520by%2520the%2520model.%2520When%2520a%2520property%2520is%2520violated%252C%2520a%250Acounterexample%2520is%2520generated%2520giving%2520the%2520cause%2520of%2520the%2520violation.%2520We%2520demonstrate%250Athe%2520framework%2527s%2520generalisability%2520by%2520applying%2520it%2520to%2520different%2520participants%252C%250Ahighlighting%2520its%2520potential%2520to%2520enhance%2520the%2520safety%2520and%2520well-being%2520of%2520older%2520adults%250Aageing%2520in%2520place.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2507.08701v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=A%20Personalised%20Formal%20Verification%20Framework%20for%20Monitoring%20Activities%0A%20%20of%20Daily%20Living%20of%20Older%20Adults%20Living%20Independently%20in%20Their%20Homes&entry.906535625=Ricardo%20Contreras%20and%20Filip%20Smola%20and%20Nu%C5%A1a%20Fari%C4%8D%20and%20Jiawei%20Zheng%20and%20Jane%20Hillston%20and%20Jacques%20D.%20Fleuriot&entry.1292438233=%20%20There%20is%20an%20imperative%20need%20to%20provide%20quality%20of%20life%20to%20a%20growing%0Apopulation%20of%20older%20adults%20living%20independently.%20Personalised%20solutions%20that%0Afocus%20on%20the%20person%20and%20take%20into%20consideration%20their%20preferences%20and%20context%0Aare%20key.%20In%20this%20work%2C%20we%20introduce%20a%20framework%20for%20representing%20and%20reasoning%0Aabout%20the%20Activities%20of%20Daily%20Living%20of%20older%20adults%20living%20independently%20at%0Ahome.%20The%20framework%20integrates%20data%20from%20sensors%20and%20contextual%20information%0Athat%20aggregates%20semi-structured%20interviews%2C%20home%20layouts%20and%20sociological%0Aobservations%20from%20the%20participants.%20We%20use%20these%20data%20to%20create%20formal%20models%2C%0Apersonalised%20for%20each%20participant%20according%20to%20their%20preferences%20and%20context.%0AWe%20formulate%20requirements%20that%20are%20specific%20to%20each%20individual%20as%20properties%0Aencoded%20in%20Linear%20Temporal%20Logic%20and%20use%20a%20model%20checker%20to%20verify%20whether%20each%0Aproperty%20is%20satisfied%20by%20the%20model.%20When%20a%20property%20is%20violated%2C%20a%0Acounterexample%20is%20generated%20giving%20the%20cause%20of%20the%20violation.%20We%20demonstrate%0Athe%20framework%27s%20generalisability%20by%20applying%20it%20to%20different%20participants%2C%0Ahighlighting%20its%20potential%20to%20enhance%20the%20safety%20and%20well-being%20of%20older%20adults%0Aageing%20in%20place.%0A&entry.1838667208=http%3A//arxiv.org/abs/2507.08701v1&entry.124074799=Read"},
{"title": "Predicting Barge Presence and Quantity on Inland Waterways using Vessel\n  Tracking Data: A Machine Learning Approach", "author": "Geoffery Agorku and Sarah Hernandez and Maria Falquez and Subhadipto Poddar and Shihao Pang", "abstract": "  This study presents a machine learning approach to predict the number of\nbarges transported by vessels on inland waterways using tracking data from the\nAutomatic Identification System (AIS). While AIS tracks the location of tug and\ntow vessels, it does not monitor the presence or number of barges transported\nby those vessels. Understanding the number and types of barges conveyed along\nriver segments, between ports, and at ports is crucial for estimating the\nquantities of freight transported on the nation's waterways. This insight is\nalso valuable for waterway management and infrastructure operations impacting\nareas such as targeted dredging operations, and data-driven resource\nallocation. Labeled sample data was generated using observations from traffic\ncameras located along key river segments and matched to AIS data records. A\nsample of 164 vessels representing up to 42 barge convoys per vessel was used\nfor model development. The methodology involved first predicting barge presence\nand then predicting barge quantity. Features derived from the AIS data included\nspeed measures, vessel characteristics, turning measures, and interaction\nterms. For predicting barge presence, the AdaBoost model achieved an F1 score\nof 0.932. For predicting barge quantity, the Random Forest combined with an\nAdaBoost ensemble model achieved an F1 score of 0.886. Bayesian optimization\nwas used for hyperparameter tuning. By advancing predictive modeling for inland\nwaterways, this study offers valuable insights for transportation planners and\norganizations, which require detailed knowledge of traffic volumes, including\nthe flow of commodities, their destinations, and the tonnage moving in and out\nof ports.\n", "link": "http://arxiv.org/abs/2501.00615v2", "date": "2025-07-11", "relevancy": 1.8358, "topK": [{"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.5331}, {"title": "Breadcrumbs: Adversarial class-balanced sampling for long-tailed recognition", "link": "https://link.springer.com/chapter/10.1007/978-3-031-20053-3_37", "similarity": 0.4469}, {"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.4414}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Predicting%20Barge%20Presence%20and%20Quantity%20on%20Inland%20Waterways%20using%20Vessel%0A%20%20Tracking%20Data%3A%20A%20Machine%20Learning%20Approach&body=Title%3A%20Predicting%20Barge%20Presence%20and%20Quantity%20on%20Inland%20Waterways%20using%20Vessel%0A%20%20Tracking%20Data%3A%20A%20Machine%20Learning%20Approach%0AAuthor%3A%20Geoffery%20Agorku%20and%20Sarah%20Hernandez%20and%20Maria%20Falquez%20and%20Subhadipto%20Poddar%20and%20Shihao%20Pang%0AAbstract%3A%20%20%20This%20study%20presents%20a%20machine%20learning%20approach%20to%20predict%20the%20number%20of%0Abarges%20transported%20by%20vessels%20on%20inland%20waterways%20using%20tracking%20data%20from%20the%0AAutomatic%20Identification%20System%20%28AIS%29.%20While%20AIS%20tracks%20the%20location%20of%20tug%20and%0Atow%20vessels%2C%20it%20does%20not%20monitor%20the%20presence%20or%20number%20of%20barges%20transported%0Aby%20those%20vessels.%20Understanding%20the%20number%20and%20types%20of%20barges%20conveyed%20along%0Ariver%20segments%2C%20between%20ports%2C%20and%20at%20ports%20is%20crucial%20for%20estimating%20the%0Aquantities%20of%20freight%20transported%20on%20the%20nation%27s%20waterways.%20This%20insight%20is%0Aalso%20valuable%20for%20waterway%20management%20and%20infrastructure%20operations%20impacting%0Aareas%20such%20as%20targeted%20dredging%20operations%2C%20and%20data-driven%20resource%0Aallocation.%20Labeled%20sample%20data%20was%20generated%20using%20observations%20from%20traffic%0Acameras%20located%20along%20key%20river%20segments%20and%20matched%20to%20AIS%20data%20records.%20A%0Asample%20of%20164%20vessels%20representing%20up%20to%2042%20barge%20convoys%20per%20vessel%20was%20used%0Afor%20model%20development.%20The%20methodology%20involved%20first%20predicting%20barge%20presence%0Aand%20then%20predicting%20barge%20quantity.%20Features%20derived%20from%20the%20AIS%20data%20included%0Aspeed%20measures%2C%20vessel%20characteristics%2C%20turning%20measures%2C%20and%20interaction%0Aterms.%20For%20predicting%20barge%20presence%2C%20the%20AdaBoost%20model%20achieved%20an%20F1%20score%0Aof%200.932.%20For%20predicting%20barge%20quantity%2C%20the%20Random%20Forest%20combined%20with%20an%0AAdaBoost%20ensemble%20model%20achieved%20an%20F1%20score%20of%200.886.%20Bayesian%20optimization%0Awas%20used%20for%20hyperparameter%20tuning.%20By%20advancing%20predictive%20modeling%20for%20inland%0Awaterways%2C%20this%20study%20offers%20valuable%20insights%20for%20transportation%20planners%20and%0Aorganizations%2C%20which%20require%20detailed%20knowledge%20of%20traffic%20volumes%2C%20including%0Athe%20flow%20of%20commodities%2C%20their%20destinations%2C%20and%20the%20tonnage%20moving%20in%20and%20out%0Aof%20ports.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2501.00615v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DPredicting%2520Barge%2520Presence%2520and%2520Quantity%2520on%2520Inland%2520Waterways%2520using%2520Vessel%250A%2520%2520Tracking%2520Data%253A%2520A%2520Machine%2520Learning%2520Approach%26entry.906535625%3DGeoffery%2520Agorku%2520and%2520Sarah%2520Hernandez%2520and%2520Maria%2520Falquez%2520and%2520Subhadipto%2520Poddar%2520and%2520Shihao%2520Pang%26entry.1292438233%3D%2520%2520This%2520study%2520presents%2520a%2520machine%2520learning%2520approach%2520to%2520predict%2520the%2520number%2520of%250Abarges%2520transported%2520by%2520vessels%2520on%2520inland%2520waterways%2520using%2520tracking%2520data%2520from%2520the%250AAutomatic%2520Identification%2520System%2520%2528AIS%2529.%2520While%2520AIS%2520tracks%2520the%2520location%2520of%2520tug%2520and%250Atow%2520vessels%252C%2520it%2520does%2520not%2520monitor%2520the%2520presence%2520or%2520number%2520of%2520barges%2520transported%250Aby%2520those%2520vessels.%2520Understanding%2520the%2520number%2520and%2520types%2520of%2520barges%2520conveyed%2520along%250Ariver%2520segments%252C%2520between%2520ports%252C%2520and%2520at%2520ports%2520is%2520crucial%2520for%2520estimating%2520the%250Aquantities%2520of%2520freight%2520transported%2520on%2520the%2520nation%2527s%2520waterways.%2520This%2520insight%2520is%250Aalso%2520valuable%2520for%2520waterway%2520management%2520and%2520infrastructure%2520operations%2520impacting%250Aareas%2520such%2520as%2520targeted%2520dredging%2520operations%252C%2520and%2520data-driven%2520resource%250Aallocation.%2520Labeled%2520sample%2520data%2520was%2520generated%2520using%2520observations%2520from%2520traffic%250Acameras%2520located%2520along%2520key%2520river%2520segments%2520and%2520matched%2520to%2520AIS%2520data%2520records.%2520A%250Asample%2520of%2520164%2520vessels%2520representing%2520up%2520to%252042%2520barge%2520convoys%2520per%2520vessel%2520was%2520used%250Afor%2520model%2520development.%2520The%2520methodology%2520involved%2520first%2520predicting%2520barge%2520presence%250Aand%2520then%2520predicting%2520barge%2520quantity.%2520Features%2520derived%2520from%2520the%2520AIS%2520data%2520included%250Aspeed%2520measures%252C%2520vessel%2520characteristics%252C%2520turning%2520measures%252C%2520and%2520interaction%250Aterms.%2520For%2520predicting%2520barge%2520presence%252C%2520the%2520AdaBoost%2520model%2520achieved%2520an%2520F1%2520score%250Aof%25200.932.%2520For%2520predicting%2520barge%2520quantity%252C%2520the%2520Random%2520Forest%2520combined%2520with%2520an%250AAdaBoost%2520ensemble%2520model%2520achieved%2520an%2520F1%2520score%2520of%25200.886.%2520Bayesian%2520optimization%250Awas%2520used%2520for%2520hyperparameter%2520tuning.%2520By%2520advancing%2520predictive%2520modeling%2520for%2520inland%250Awaterways%252C%2520this%2520study%2520offers%2520valuable%2520insights%2520for%2520transportation%2520planners%2520and%250Aorganizations%252C%2520which%2520require%2520detailed%2520knowledge%2520of%2520traffic%2520volumes%252C%2520including%250Athe%2520flow%2520of%2520commodities%252C%2520their%2520destinations%252C%2520and%2520the%2520tonnage%2520moving%2520in%2520and%2520out%250Aof%2520ports.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2501.00615v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Predicting%20Barge%20Presence%20and%20Quantity%20on%20Inland%20Waterways%20using%20Vessel%0A%20%20Tracking%20Data%3A%20A%20Machine%20Learning%20Approach&entry.906535625=Geoffery%20Agorku%20and%20Sarah%20Hernandez%20and%20Maria%20Falquez%20and%20Subhadipto%20Poddar%20and%20Shihao%20Pang&entry.1292438233=%20%20This%20study%20presents%20a%20machine%20learning%20approach%20to%20predict%20the%20number%20of%0Abarges%20transported%20by%20vessels%20on%20inland%20waterways%20using%20tracking%20data%20from%20the%0AAutomatic%20Identification%20System%20%28AIS%29.%20While%20AIS%20tracks%20the%20location%20of%20tug%20and%0Atow%20vessels%2C%20it%20does%20not%20monitor%20the%20presence%20or%20number%20of%20barges%20transported%0Aby%20those%20vessels.%20Understanding%20the%20number%20and%20types%20of%20barges%20conveyed%20along%0Ariver%20segments%2C%20between%20ports%2C%20and%20at%20ports%20is%20crucial%20for%20estimating%20the%0Aquantities%20of%20freight%20transported%20on%20the%20nation%27s%20waterways.%20This%20insight%20is%0Aalso%20valuable%20for%20waterway%20management%20and%20infrastructure%20operations%20impacting%0Aareas%20such%20as%20targeted%20dredging%20operations%2C%20and%20data-driven%20resource%0Aallocation.%20Labeled%20sample%20data%20was%20generated%20using%20observations%20from%20traffic%0Acameras%20located%20along%20key%20river%20segments%20and%20matched%20to%20AIS%20data%20records.%20A%0Asample%20of%20164%20vessels%20representing%20up%20to%2042%20barge%20convoys%20per%20vessel%20was%20used%0Afor%20model%20development.%20The%20methodology%20involved%20first%20predicting%20barge%20presence%0Aand%20then%20predicting%20barge%20quantity.%20Features%20derived%20from%20the%20AIS%20data%20included%0Aspeed%20measures%2C%20vessel%20characteristics%2C%20turning%20measures%2C%20and%20interaction%0Aterms.%20For%20predicting%20barge%20presence%2C%20the%20AdaBoost%20model%20achieved%20an%20F1%20score%0Aof%200.932.%20For%20predicting%20barge%20quantity%2C%20the%20Random%20Forest%20combined%20with%20an%0AAdaBoost%20ensemble%20model%20achieved%20an%20F1%20score%20of%200.886.%20Bayesian%20optimization%0Awas%20used%20for%20hyperparameter%20tuning.%20By%20advancing%20predictive%20modeling%20for%20inland%0Awaterways%2C%20this%20study%20offers%20valuable%20insights%20for%20transportation%20planners%20and%0Aorganizations%2C%20which%20require%20detailed%20knowledge%20of%20traffic%20volumes%2C%20including%0Athe%20flow%20of%20commodities%2C%20their%20destinations%2C%20and%20the%20tonnage%20moving%20in%20and%20out%0Aof%20ports.%0A&entry.1838667208=http%3A//arxiv.org/abs/2501.00615v2&entry.124074799=Read"},
{"title": "Learnable quantum spectral filters for hybrid graph neural networks", "author": "Ammar Daskin", "abstract": "  In this paper, we describe a parameterized quantum circuit that can be\nconsidered as convolutional and pooling layers for graph neural networks. The\ncircuit incorporates the parameterized quantum Fourier circuit where the qubit\nconnections for the controlled gates derived from the Laplacian operator.\nSpecifically, we show that the eigenspace of the Laplacian operator of a graph\ncan be approximated by using QFT based circuit whose connections are determined\nfrom the adjacency matrix. For an $N\\times N$ Laplacian, this approach yields\nan approximate polynomial-depth circuit requiring only $n=log(N)$ qubits. These\ntypes of circuits can eliminate the expensive classical computations for\napproximating the learnable functions of the Laplacian through Chebyshev\npolynomial or Taylor expansions.\n  Using this circuit as a convolutional layer provides an $n-$ dimensional\nprobability vector that can be considered as the filtered and compressed graph\nsignal. Therefore, the circuit along with the measurement can be considered a\nvery efficient convolution plus pooling layer that transforms an\n$N$-dimensional signal input into $n-$dimensional signal with an exponential\ncompression. We then apply a classical neural network prediction head to the\noutput of the circuit to construct a complete graph neural network. Since the\ncircuit incorporates geometric structure through its graph connection-based\napproach, we present graph classification results for the benchmark datasets\nlisted in TUDataset library. Using only [1-100] learnable parameters for the\nquantum circuit and minimal classical layers (1000-5000 parameters) in a\ngeneric setting, the obtained results are comparable to and in some cases\nbetter than many of the baseline results, particularly for the cases when\ngeometric structure plays a significant role.\n", "link": "http://arxiv.org/abs/2507.05640v2", "date": "2025-07-11", "relevancy": 1.8229, "topK": [{"title": "Self-supervised Photographic Image Layout Representation Learning", "link": "http://arxiv.org/abs/2403.03740v1", "similarity": 0.4759}, {"title": "Boosted dynamic neural networks", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/26302", "similarity": 0.443}, {"title": "Scaling Rectified Flow Transformers for High-Resolution Image Synthesis", "link": "http://arxiv.org/abs/2403.03206v1", "similarity": 0.4372}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Learnable%20quantum%20spectral%20filters%20for%20hybrid%20graph%20neural%20networks&body=Title%3A%20Learnable%20quantum%20spectral%20filters%20for%20hybrid%20graph%20neural%20networks%0AAuthor%3A%20Ammar%20Daskin%0AAbstract%3A%20%20%20In%20this%20paper%2C%20we%20describe%20a%20parameterized%20quantum%20circuit%20that%20can%20be%0Aconsidered%20as%20convolutional%20and%20pooling%20layers%20for%20graph%20neural%20networks.%20The%0Acircuit%20incorporates%20the%20parameterized%20quantum%20Fourier%20circuit%20where%20the%20qubit%0Aconnections%20for%20the%20controlled%20gates%20derived%20from%20the%20Laplacian%20operator.%0ASpecifically%2C%20we%20show%20that%20the%20eigenspace%20of%20the%20Laplacian%20operator%20of%20a%20graph%0Acan%20be%20approximated%20by%20using%20QFT%20based%20circuit%20whose%20connections%20are%20determined%0Afrom%20the%20adjacency%20matrix.%20For%20an%20%24N%5Ctimes%20N%24%20Laplacian%2C%20this%20approach%20yields%0Aan%20approximate%20polynomial-depth%20circuit%20requiring%20only%20%24n%3Dlog%28N%29%24%20qubits.%20These%0Atypes%20of%20circuits%20can%20eliminate%20the%20expensive%20classical%20computations%20for%0Aapproximating%20the%20learnable%20functions%20of%20the%20Laplacian%20through%20Chebyshev%0Apolynomial%20or%20Taylor%20expansions.%0A%20%20Using%20this%20circuit%20as%20a%20convolutional%20layer%20provides%20an%20%24n-%24%20dimensional%0Aprobability%20vector%20that%20can%20be%20considered%20as%20the%20filtered%20and%20compressed%20graph%0Asignal.%20Therefore%2C%20the%20circuit%20along%20with%20the%20measurement%20can%20be%20considered%20a%0Avery%20efficient%20convolution%20plus%20pooling%20layer%20that%20transforms%20an%0A%24N%24-dimensional%20signal%20input%20into%20%24n-%24dimensional%20signal%20with%20an%20exponential%0Acompression.%20We%20then%20apply%20a%20classical%20neural%20network%20prediction%20head%20to%20the%0Aoutput%20of%20the%20circuit%20to%20construct%20a%20complete%20graph%20neural%20network.%20Since%20the%0Acircuit%20incorporates%20geometric%20structure%20through%20its%20graph%20connection-based%0Aapproach%2C%20we%20present%20graph%20classification%20results%20for%20the%20benchmark%20datasets%0Alisted%20in%20TUDataset%20library.%20Using%20only%20%5B1-100%5D%20learnable%20parameters%20for%20the%0Aquantum%20circuit%20and%20minimal%20classical%20layers%20%281000-5000%20parameters%29%20in%20a%0Ageneric%20setting%2C%20the%20obtained%20results%20are%20comparable%20to%20and%20in%20some%20cases%0Abetter%20than%20many%20of%20the%20baseline%20results%2C%20particularly%20for%20the%20cases%20when%0Ageometric%20structure%20plays%20a%20significant%20role.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2507.05640v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DLearnable%2520quantum%2520spectral%2520filters%2520for%2520hybrid%2520graph%2520neural%2520networks%26entry.906535625%3DAmmar%2520Daskin%26entry.1292438233%3D%2520%2520In%2520this%2520paper%252C%2520we%2520describe%2520a%2520parameterized%2520quantum%2520circuit%2520that%2520can%2520be%250Aconsidered%2520as%2520convolutional%2520and%2520pooling%2520layers%2520for%2520graph%2520neural%2520networks.%2520The%250Acircuit%2520incorporates%2520the%2520parameterized%2520quantum%2520Fourier%2520circuit%2520where%2520the%2520qubit%250Aconnections%2520for%2520the%2520controlled%2520gates%2520derived%2520from%2520the%2520Laplacian%2520operator.%250ASpecifically%252C%2520we%2520show%2520that%2520the%2520eigenspace%2520of%2520the%2520Laplacian%2520operator%2520of%2520a%2520graph%250Acan%2520be%2520approximated%2520by%2520using%2520QFT%2520based%2520circuit%2520whose%2520connections%2520are%2520determined%250Afrom%2520the%2520adjacency%2520matrix.%2520For%2520an%2520%2524N%255Ctimes%2520N%2524%2520Laplacian%252C%2520this%2520approach%2520yields%250Aan%2520approximate%2520polynomial-depth%2520circuit%2520requiring%2520only%2520%2524n%253Dlog%2528N%2529%2524%2520qubits.%2520These%250Atypes%2520of%2520circuits%2520can%2520eliminate%2520the%2520expensive%2520classical%2520computations%2520for%250Aapproximating%2520the%2520learnable%2520functions%2520of%2520the%2520Laplacian%2520through%2520Chebyshev%250Apolynomial%2520or%2520Taylor%2520expansions.%250A%2520%2520Using%2520this%2520circuit%2520as%2520a%2520convolutional%2520layer%2520provides%2520an%2520%2524n-%2524%2520dimensional%250Aprobability%2520vector%2520that%2520can%2520be%2520considered%2520as%2520the%2520filtered%2520and%2520compressed%2520graph%250Asignal.%2520Therefore%252C%2520the%2520circuit%2520along%2520with%2520the%2520measurement%2520can%2520be%2520considered%2520a%250Avery%2520efficient%2520convolution%2520plus%2520pooling%2520layer%2520that%2520transforms%2520an%250A%2524N%2524-dimensional%2520signal%2520input%2520into%2520%2524n-%2524dimensional%2520signal%2520with%2520an%2520exponential%250Acompression.%2520We%2520then%2520apply%2520a%2520classical%2520neural%2520network%2520prediction%2520head%2520to%2520the%250Aoutput%2520of%2520the%2520circuit%2520to%2520construct%2520a%2520complete%2520graph%2520neural%2520network.%2520Since%2520the%250Acircuit%2520incorporates%2520geometric%2520structure%2520through%2520its%2520graph%2520connection-based%250Aapproach%252C%2520we%2520present%2520graph%2520classification%2520results%2520for%2520the%2520benchmark%2520datasets%250Alisted%2520in%2520TUDataset%2520library.%2520Using%2520only%2520%255B1-100%255D%2520learnable%2520parameters%2520for%2520the%250Aquantum%2520circuit%2520and%2520minimal%2520classical%2520layers%2520%25281000-5000%2520parameters%2529%2520in%2520a%250Ageneric%2520setting%252C%2520the%2520obtained%2520results%2520are%2520comparable%2520to%2520and%2520in%2520some%2520cases%250Abetter%2520than%2520many%2520of%2520the%2520baseline%2520results%252C%2520particularly%2520for%2520the%2520cases%2520when%250Ageometric%2520structure%2520plays%2520a%2520significant%2520role.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2507.05640v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Learnable%20quantum%20spectral%20filters%20for%20hybrid%20graph%20neural%20networks&entry.906535625=Ammar%20Daskin&entry.1292438233=%20%20In%20this%20paper%2C%20we%20describe%20a%20parameterized%20quantum%20circuit%20that%20can%20be%0Aconsidered%20as%20convolutional%20and%20pooling%20layers%20for%20graph%20neural%20networks.%20The%0Acircuit%20incorporates%20the%20parameterized%20quantum%20Fourier%20circuit%20where%20the%20qubit%0Aconnections%20for%20the%20controlled%20gates%20derived%20from%20the%20Laplacian%20operator.%0ASpecifically%2C%20we%20show%20that%20the%20eigenspace%20of%20the%20Laplacian%20operator%20of%20a%20graph%0Acan%20be%20approximated%20by%20using%20QFT%20based%20circuit%20whose%20connections%20are%20determined%0Afrom%20the%20adjacency%20matrix.%20For%20an%20%24N%5Ctimes%20N%24%20Laplacian%2C%20this%20approach%20yields%0Aan%20approximate%20polynomial-depth%20circuit%20requiring%20only%20%24n%3Dlog%28N%29%24%20qubits.%20These%0Atypes%20of%20circuits%20can%20eliminate%20the%20expensive%20classical%20computations%20for%0Aapproximating%20the%20learnable%20functions%20of%20the%20Laplacian%20through%20Chebyshev%0Apolynomial%20or%20Taylor%20expansions.%0A%20%20Using%20this%20circuit%20as%20a%20convolutional%20layer%20provides%20an%20%24n-%24%20dimensional%0Aprobability%20vector%20that%20can%20be%20considered%20as%20the%20filtered%20and%20compressed%20graph%0Asignal.%20Therefore%2C%20the%20circuit%20along%20with%20the%20measurement%20can%20be%20considered%20a%0Avery%20efficient%20convolution%20plus%20pooling%20layer%20that%20transforms%20an%0A%24N%24-dimensional%20signal%20input%20into%20%24n-%24dimensional%20signal%20with%20an%20exponential%0Acompression.%20We%20then%20apply%20a%20classical%20neural%20network%20prediction%20head%20to%20the%0Aoutput%20of%20the%20circuit%20to%20construct%20a%20complete%20graph%20neural%20network.%20Since%20the%0Acircuit%20incorporates%20geometric%20structure%20through%20its%20graph%20connection-based%0Aapproach%2C%20we%20present%20graph%20classification%20results%20for%20the%20benchmark%20datasets%0Alisted%20in%20TUDataset%20library.%20Using%20only%20%5B1-100%5D%20learnable%20parameters%20for%20the%0Aquantum%20circuit%20and%20minimal%20classical%20layers%20%281000-5000%20parameters%29%20in%20a%0Ageneric%20setting%2C%20the%20obtained%20results%20are%20comparable%20to%20and%20in%20some%20cases%0Abetter%20than%20many%20of%20the%20baseline%20results%2C%20particularly%20for%20the%20cases%20when%0Ageometric%20structure%20plays%20a%20significant%20role.%0A&entry.1838667208=http%3A//arxiv.org/abs/2507.05640v2&entry.124074799=Read"},
{"title": "Compress Any Segment Anything Model (SAM)", "author": "Juntong Fan and Zhiwei Hao and Jianqiang Shen and Shang-Ling Jui and Yi Zhang and Jing-Xiao Liao and Feng-Lei Fan", "abstract": "  Due to the excellent performance in yielding high-quality, zero-shot\nsegmentation, Segment Anything Model (SAM) and its variants have been widely\napplied in diverse scenarios such as healthcare and intelligent manufacturing.\nTherefore, effectively compressing SAMs has become an increasingly pressing\npractical need. In this study, we propose Birkhoff, a novel data-free\ncompression algorithm for SAM and its variants. Unlike quantization, pruning,\ndistillation, and other compression methods, Birkhoff embodies versatility\nacross model types, agility in deployment, faithfulness to the original model,\nand compactness in model size. Specifically, Birkhoff introduces a novel\ncompression algorithm: Hyper-Compression, whose core principle is to find a\ndense trajectory to turn a high-dimensional parameter vector into a\nlow-dimensional scalar. Furthermore, Birkhoff designs a dedicated linear layer\noperator, HyperLinear, to fuse decompression and matrix multiplication to\nsignificantly accelerate inference of the compressed SAMs. Extensive\nexperiments on 18 SAMs in the COCO, LVIS, and SA-1B datasets show that Birkhoff\nperforms consistently and competitively in compression time, compression ratio,\npost-compression performance, and inference speed. For example, Birkhoff can\nachieve a compression ratio of 5.17x on SAM2-B, with less than 1% performance\ndrop without using any fine-tuning data. Moreover, the compression is finished\nwithin 60 seconds for all models.\n", "link": "http://arxiv.org/abs/2507.08765v1", "date": "2025-07-11", "relevancy": 1.8001, "topK": [{"title": "SDXS: Real-Time One-Step Latent Diffusion Models with Image Conditions", "link": "http://arxiv.org/abs/2403.16627v1", "similarity": 0.4689}, {"title": "GST: Precise 3D Human Body from a Single Image with Gaussian Splatting\n  Transformers", "link": "http://arxiv.org/abs/2409.04196v1", "similarity": 0.4526}, {"title": "Breadcrumbs: Adversarial class-balanced sampling for long-tailed recognition", "link": "https://link.springer.com/chapter/10.1007/978-3-031-20053-3_37", "similarity": 0.4399}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Compress%20Any%20Segment%20Anything%20Model%20%28SAM%29&body=Title%3A%20Compress%20Any%20Segment%20Anything%20Model%20%28SAM%29%0AAuthor%3A%20Juntong%20Fan%20and%20Zhiwei%20Hao%20and%20Jianqiang%20Shen%20and%20Shang-Ling%20Jui%20and%20Yi%20Zhang%20and%20Jing-Xiao%20Liao%20and%20Feng-Lei%20Fan%0AAbstract%3A%20%20%20Due%20to%20the%20excellent%20performance%20in%20yielding%20high-quality%2C%20zero-shot%0Asegmentation%2C%20Segment%20Anything%20Model%20%28SAM%29%20and%20its%20variants%20have%20been%20widely%0Aapplied%20in%20diverse%20scenarios%20such%20as%20healthcare%20and%20intelligent%20manufacturing.%0ATherefore%2C%20effectively%20compressing%20SAMs%20has%20become%20an%20increasingly%20pressing%0Apractical%20need.%20In%20this%20study%2C%20we%20propose%20Birkhoff%2C%20a%20novel%20data-free%0Acompression%20algorithm%20for%20SAM%20and%20its%20variants.%20Unlike%20quantization%2C%20pruning%2C%0Adistillation%2C%20and%20other%20compression%20methods%2C%20Birkhoff%20embodies%20versatility%0Aacross%20model%20types%2C%20agility%20in%20deployment%2C%20faithfulness%20to%20the%20original%20model%2C%0Aand%20compactness%20in%20model%20size.%20Specifically%2C%20Birkhoff%20introduces%20a%20novel%0Acompression%20algorithm%3A%20Hyper-Compression%2C%20whose%20core%20principle%20is%20to%20find%20a%0Adense%20trajectory%20to%20turn%20a%20high-dimensional%20parameter%20vector%20into%20a%0Alow-dimensional%20scalar.%20Furthermore%2C%20Birkhoff%20designs%20a%20dedicated%20linear%20layer%0Aoperator%2C%20HyperLinear%2C%20to%20fuse%20decompression%20and%20matrix%20multiplication%20to%0Asignificantly%20accelerate%20inference%20of%20the%20compressed%20SAMs.%20Extensive%0Aexperiments%20on%2018%20SAMs%20in%20the%20COCO%2C%20LVIS%2C%20and%20SA-1B%20datasets%20show%20that%20Birkhoff%0Aperforms%20consistently%20and%20competitively%20in%20compression%20time%2C%20compression%20ratio%2C%0Apost-compression%20performance%2C%20and%20inference%20speed.%20For%20example%2C%20Birkhoff%20can%0Aachieve%20a%20compression%20ratio%20of%205.17x%20on%20SAM2-B%2C%20with%20less%20than%201%25%20performance%0Adrop%20without%20using%20any%20fine-tuning%20data.%20Moreover%2C%20the%20compression%20is%20finished%0Awithin%2060%20seconds%20for%20all%20models.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2507.08765v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DCompress%2520Any%2520Segment%2520Anything%2520Model%2520%2528SAM%2529%26entry.906535625%3DJuntong%2520Fan%2520and%2520Zhiwei%2520Hao%2520and%2520Jianqiang%2520Shen%2520and%2520Shang-Ling%2520Jui%2520and%2520Yi%2520Zhang%2520and%2520Jing-Xiao%2520Liao%2520and%2520Feng-Lei%2520Fan%26entry.1292438233%3D%2520%2520Due%2520to%2520the%2520excellent%2520performance%2520in%2520yielding%2520high-quality%252C%2520zero-shot%250Asegmentation%252C%2520Segment%2520Anything%2520Model%2520%2528SAM%2529%2520and%2520its%2520variants%2520have%2520been%2520widely%250Aapplied%2520in%2520diverse%2520scenarios%2520such%2520as%2520healthcare%2520and%2520intelligent%2520manufacturing.%250ATherefore%252C%2520effectively%2520compressing%2520SAMs%2520has%2520become%2520an%2520increasingly%2520pressing%250Apractical%2520need.%2520In%2520this%2520study%252C%2520we%2520propose%2520Birkhoff%252C%2520a%2520novel%2520data-free%250Acompression%2520algorithm%2520for%2520SAM%2520and%2520its%2520variants.%2520Unlike%2520quantization%252C%2520pruning%252C%250Adistillation%252C%2520and%2520other%2520compression%2520methods%252C%2520Birkhoff%2520embodies%2520versatility%250Aacross%2520model%2520types%252C%2520agility%2520in%2520deployment%252C%2520faithfulness%2520to%2520the%2520original%2520model%252C%250Aand%2520compactness%2520in%2520model%2520size.%2520Specifically%252C%2520Birkhoff%2520introduces%2520a%2520novel%250Acompression%2520algorithm%253A%2520Hyper-Compression%252C%2520whose%2520core%2520principle%2520is%2520to%2520find%2520a%250Adense%2520trajectory%2520to%2520turn%2520a%2520high-dimensional%2520parameter%2520vector%2520into%2520a%250Alow-dimensional%2520scalar.%2520Furthermore%252C%2520Birkhoff%2520designs%2520a%2520dedicated%2520linear%2520layer%250Aoperator%252C%2520HyperLinear%252C%2520to%2520fuse%2520decompression%2520and%2520matrix%2520multiplication%2520to%250Asignificantly%2520accelerate%2520inference%2520of%2520the%2520compressed%2520SAMs.%2520Extensive%250Aexperiments%2520on%252018%2520SAMs%2520in%2520the%2520COCO%252C%2520LVIS%252C%2520and%2520SA-1B%2520datasets%2520show%2520that%2520Birkhoff%250Aperforms%2520consistently%2520and%2520competitively%2520in%2520compression%2520time%252C%2520compression%2520ratio%252C%250Apost-compression%2520performance%252C%2520and%2520inference%2520speed.%2520For%2520example%252C%2520Birkhoff%2520can%250Aachieve%2520a%2520compression%2520ratio%2520of%25205.17x%2520on%2520SAM2-B%252C%2520with%2520less%2520than%25201%2525%2520performance%250Adrop%2520without%2520using%2520any%2520fine-tuning%2520data.%2520Moreover%252C%2520the%2520compression%2520is%2520finished%250Awithin%252060%2520seconds%2520for%2520all%2520models.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2507.08765v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Compress%20Any%20Segment%20Anything%20Model%20%28SAM%29&entry.906535625=Juntong%20Fan%20and%20Zhiwei%20Hao%20and%20Jianqiang%20Shen%20and%20Shang-Ling%20Jui%20and%20Yi%20Zhang%20and%20Jing-Xiao%20Liao%20and%20Feng-Lei%20Fan&entry.1292438233=%20%20Due%20to%20the%20excellent%20performance%20in%20yielding%20high-quality%2C%20zero-shot%0Asegmentation%2C%20Segment%20Anything%20Model%20%28SAM%29%20and%20its%20variants%20have%20been%20widely%0Aapplied%20in%20diverse%20scenarios%20such%20as%20healthcare%20and%20intelligent%20manufacturing.%0ATherefore%2C%20effectively%20compressing%20SAMs%20has%20become%20an%20increasingly%20pressing%0Apractical%20need.%20In%20this%20study%2C%20we%20propose%20Birkhoff%2C%20a%20novel%20data-free%0Acompression%20algorithm%20for%20SAM%20and%20its%20variants.%20Unlike%20quantization%2C%20pruning%2C%0Adistillation%2C%20and%20other%20compression%20methods%2C%20Birkhoff%20embodies%20versatility%0Aacross%20model%20types%2C%20agility%20in%20deployment%2C%20faithfulness%20to%20the%20original%20model%2C%0Aand%20compactness%20in%20model%20size.%20Specifically%2C%20Birkhoff%20introduces%20a%20novel%0Acompression%20algorithm%3A%20Hyper-Compression%2C%20whose%20core%20principle%20is%20to%20find%20a%0Adense%20trajectory%20to%20turn%20a%20high-dimensional%20parameter%20vector%20into%20a%0Alow-dimensional%20scalar.%20Furthermore%2C%20Birkhoff%20designs%20a%20dedicated%20linear%20layer%0Aoperator%2C%20HyperLinear%2C%20to%20fuse%20decompression%20and%20matrix%20multiplication%20to%0Asignificantly%20accelerate%20inference%20of%20the%20compressed%20SAMs.%20Extensive%0Aexperiments%20on%2018%20SAMs%20in%20the%20COCO%2C%20LVIS%2C%20and%20SA-1B%20datasets%20show%20that%20Birkhoff%0Aperforms%20consistently%20and%20competitively%20in%20compression%20time%2C%20compression%20ratio%2C%0Apost-compression%20performance%2C%20and%20inference%20speed.%20For%20example%2C%20Birkhoff%20can%0Aachieve%20a%20compression%20ratio%20of%205.17x%20on%20SAM2-B%2C%20with%20less%20than%201%25%20performance%0Adrop%20without%20using%20any%20fine-tuning%20data.%20Moreover%2C%20the%20compression%20is%20finished%0Awithin%2060%20seconds%20for%20all%20models.%0A&entry.1838667208=http%3A//arxiv.org/abs/2507.08765v1&entry.124074799=Read"},
{"title": "Experimental Setup and Software Pipeline to Evaluate Optimization based\n  Autonomous Multi-Robot Search Algorithms", "author": "Aditya Bhatt and Mary Katherine Corra and Franklin Merlo and Prajit KrisshnaKumar and Souma Chowdhury", "abstract": "  Signal source localization has been a problem of interest in the multi-robot\nsystems domain given its applications in search & rescue and hazard\nlocalization in various industrial and outdoor settings. A variety of\nmulti-robot search algorithms exist that usually formulate and solve the\nassociated autonomous motion planning problem as a heuristic model-free or\nbelief model-based optimization process. Most of these algorithms however\nremains tested only in simulation, thereby losing the opportunity to generate\nknowledge about how such algorithms would compare/contrast in a real physical\nsetting in terms of search performance and real-time computing performance. To\naddress this gap, this paper presents a new lab-scale physical setup and\nassociated open-source software pipeline to evaluate and benchmark multi-robot\nsearch algorithms. The presented physical setup innovatively uses an acoustic\nsource (that is safe and inexpensive) and small ground robots (e-pucks)\noperating in a standard motion-capture environment. This setup can be easily\nrecreated and used by most robotics researchers. The acoustic source also\npresents interesting uncertainty in terms of its noise-to-signal ratio, which\nis useful to assess sim-to-real gaps. The overall software pipeline is designed\nto readily interface with any multi-robot search algorithm with minimal effort\nand is executable in parallel asynchronous form. This pipeline includes a\nframework for distributed implementation of multi-robot or swarm search\nalgorithms, integrated with a ROS (Robotics Operating System)-based software\nstack for motion capture supported localization. The utility of this novel\nsetup is demonstrated by using it to evaluate two state-of-the-art multi-robot\nsearch algorithms, based on swarm optimization and batch-Bayesian Optimization\n(called Bayes-Swarm), as well as a random walk baseline.\n", "link": "http://arxiv.org/abs/2506.16710v3", "date": "2025-07-11", "relevancy": 1.7861, "topK": [{"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.6221}, {"title": "RoboEXP: Action-Conditioned Scene Graph via Interactive Exploration for\n  Robotic Manipulation", "link": "http://arxiv.org/abs/2402.15487v1", "similarity": 0.612}, {"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.578}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Experimental%20Setup%20and%20Software%20Pipeline%20to%20Evaluate%20Optimization%20based%0A%20%20Autonomous%20Multi-Robot%20Search%20Algorithms&body=Title%3A%20Experimental%20Setup%20and%20Software%20Pipeline%20to%20Evaluate%20Optimization%20based%0A%20%20Autonomous%20Multi-Robot%20Search%20Algorithms%0AAuthor%3A%20Aditya%20Bhatt%20and%20Mary%20Katherine%20Corra%20and%20Franklin%20Merlo%20and%20Prajit%20KrisshnaKumar%20and%20Souma%20Chowdhury%0AAbstract%3A%20%20%20Signal%20source%20localization%20has%20been%20a%20problem%20of%20interest%20in%20the%20multi-robot%0Asystems%20domain%20given%20its%20applications%20in%20search%20%26%20rescue%20and%20hazard%0Alocalization%20in%20various%20industrial%20and%20outdoor%20settings.%20A%20variety%20of%0Amulti-robot%20search%20algorithms%20exist%20that%20usually%20formulate%20and%20solve%20the%0Aassociated%20autonomous%20motion%20planning%20problem%20as%20a%20heuristic%20model-free%20or%0Abelief%20model-based%20optimization%20process.%20Most%20of%20these%20algorithms%20however%0Aremains%20tested%20only%20in%20simulation%2C%20thereby%20losing%20the%20opportunity%20to%20generate%0Aknowledge%20about%20how%20such%20algorithms%20would%20compare/contrast%20in%20a%20real%20physical%0Asetting%20in%20terms%20of%20search%20performance%20and%20real-time%20computing%20performance.%20To%0Aaddress%20this%20gap%2C%20this%20paper%20presents%20a%20new%20lab-scale%20physical%20setup%20and%0Aassociated%20open-source%20software%20pipeline%20to%20evaluate%20and%20benchmark%20multi-robot%0Asearch%20algorithms.%20The%20presented%20physical%20setup%20innovatively%20uses%20an%20acoustic%0Asource%20%28that%20is%20safe%20and%20inexpensive%29%20and%20small%20ground%20robots%20%28e-pucks%29%0Aoperating%20in%20a%20standard%20motion-capture%20environment.%20This%20setup%20can%20be%20easily%0Arecreated%20and%20used%20by%20most%20robotics%20researchers.%20The%20acoustic%20source%20also%0Apresents%20interesting%20uncertainty%20in%20terms%20of%20its%20noise-to-signal%20ratio%2C%20which%0Ais%20useful%20to%20assess%20sim-to-real%20gaps.%20The%20overall%20software%20pipeline%20is%20designed%0Ato%20readily%20interface%20with%20any%20multi-robot%20search%20algorithm%20with%20minimal%20effort%0Aand%20is%20executable%20in%20parallel%20asynchronous%20form.%20This%20pipeline%20includes%20a%0Aframework%20for%20distributed%20implementation%20of%20multi-robot%20or%20swarm%20search%0Aalgorithms%2C%20integrated%20with%20a%20ROS%20%28Robotics%20Operating%20System%29-based%20software%0Astack%20for%20motion%20capture%20supported%20localization.%20The%20utility%20of%20this%20novel%0Asetup%20is%20demonstrated%20by%20using%20it%20to%20evaluate%20two%20state-of-the-art%20multi-robot%0Asearch%20algorithms%2C%20based%20on%20swarm%20optimization%20and%20batch-Bayesian%20Optimization%0A%28called%20Bayes-Swarm%29%2C%20as%20well%20as%20a%20random%20walk%20baseline.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2506.16710v3%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DExperimental%2520Setup%2520and%2520Software%2520Pipeline%2520to%2520Evaluate%2520Optimization%2520based%250A%2520%2520Autonomous%2520Multi-Robot%2520Search%2520Algorithms%26entry.906535625%3DAditya%2520Bhatt%2520and%2520Mary%2520Katherine%2520Corra%2520and%2520Franklin%2520Merlo%2520and%2520Prajit%2520KrisshnaKumar%2520and%2520Souma%2520Chowdhury%26entry.1292438233%3D%2520%2520Signal%2520source%2520localization%2520has%2520been%2520a%2520problem%2520of%2520interest%2520in%2520the%2520multi-robot%250Asystems%2520domain%2520given%2520its%2520applications%2520in%2520search%2520%2526%2520rescue%2520and%2520hazard%250Alocalization%2520in%2520various%2520industrial%2520and%2520outdoor%2520settings.%2520A%2520variety%2520of%250Amulti-robot%2520search%2520algorithms%2520exist%2520that%2520usually%2520formulate%2520and%2520solve%2520the%250Aassociated%2520autonomous%2520motion%2520planning%2520problem%2520as%2520a%2520heuristic%2520model-free%2520or%250Abelief%2520model-based%2520optimization%2520process.%2520Most%2520of%2520these%2520algorithms%2520however%250Aremains%2520tested%2520only%2520in%2520simulation%252C%2520thereby%2520losing%2520the%2520opportunity%2520to%2520generate%250Aknowledge%2520about%2520how%2520such%2520algorithms%2520would%2520compare/contrast%2520in%2520a%2520real%2520physical%250Asetting%2520in%2520terms%2520of%2520search%2520performance%2520and%2520real-time%2520computing%2520performance.%2520To%250Aaddress%2520this%2520gap%252C%2520this%2520paper%2520presents%2520a%2520new%2520lab-scale%2520physical%2520setup%2520and%250Aassociated%2520open-source%2520software%2520pipeline%2520to%2520evaluate%2520and%2520benchmark%2520multi-robot%250Asearch%2520algorithms.%2520The%2520presented%2520physical%2520setup%2520innovatively%2520uses%2520an%2520acoustic%250Asource%2520%2528that%2520is%2520safe%2520and%2520inexpensive%2529%2520and%2520small%2520ground%2520robots%2520%2528e-pucks%2529%250Aoperating%2520in%2520a%2520standard%2520motion-capture%2520environment.%2520This%2520setup%2520can%2520be%2520easily%250Arecreated%2520and%2520used%2520by%2520most%2520robotics%2520researchers.%2520The%2520acoustic%2520source%2520also%250Apresents%2520interesting%2520uncertainty%2520in%2520terms%2520of%2520its%2520noise-to-signal%2520ratio%252C%2520which%250Ais%2520useful%2520to%2520assess%2520sim-to-real%2520gaps.%2520The%2520overall%2520software%2520pipeline%2520is%2520designed%250Ato%2520readily%2520interface%2520with%2520any%2520multi-robot%2520search%2520algorithm%2520with%2520minimal%2520effort%250Aand%2520is%2520executable%2520in%2520parallel%2520asynchronous%2520form.%2520This%2520pipeline%2520includes%2520a%250Aframework%2520for%2520distributed%2520implementation%2520of%2520multi-robot%2520or%2520swarm%2520search%250Aalgorithms%252C%2520integrated%2520with%2520a%2520ROS%2520%2528Robotics%2520Operating%2520System%2529-based%2520software%250Astack%2520for%2520motion%2520capture%2520supported%2520localization.%2520The%2520utility%2520of%2520this%2520novel%250Asetup%2520is%2520demonstrated%2520by%2520using%2520it%2520to%2520evaluate%2520two%2520state-of-the-art%2520multi-robot%250Asearch%2520algorithms%252C%2520based%2520on%2520swarm%2520optimization%2520and%2520batch-Bayesian%2520Optimization%250A%2528called%2520Bayes-Swarm%2529%252C%2520as%2520well%2520as%2520a%2520random%2520walk%2520baseline.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2506.16710v3%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Experimental%20Setup%20and%20Software%20Pipeline%20to%20Evaluate%20Optimization%20based%0A%20%20Autonomous%20Multi-Robot%20Search%20Algorithms&entry.906535625=Aditya%20Bhatt%20and%20Mary%20Katherine%20Corra%20and%20Franklin%20Merlo%20and%20Prajit%20KrisshnaKumar%20and%20Souma%20Chowdhury&entry.1292438233=%20%20Signal%20source%20localization%20has%20been%20a%20problem%20of%20interest%20in%20the%20multi-robot%0Asystems%20domain%20given%20its%20applications%20in%20search%20%26%20rescue%20and%20hazard%0Alocalization%20in%20various%20industrial%20and%20outdoor%20settings.%20A%20variety%20of%0Amulti-robot%20search%20algorithms%20exist%20that%20usually%20formulate%20and%20solve%20the%0Aassociated%20autonomous%20motion%20planning%20problem%20as%20a%20heuristic%20model-free%20or%0Abelief%20model-based%20optimization%20process.%20Most%20of%20these%20algorithms%20however%0Aremains%20tested%20only%20in%20simulation%2C%20thereby%20losing%20the%20opportunity%20to%20generate%0Aknowledge%20about%20how%20such%20algorithms%20would%20compare/contrast%20in%20a%20real%20physical%0Asetting%20in%20terms%20of%20search%20performance%20and%20real-time%20computing%20performance.%20To%0Aaddress%20this%20gap%2C%20this%20paper%20presents%20a%20new%20lab-scale%20physical%20setup%20and%0Aassociated%20open-source%20software%20pipeline%20to%20evaluate%20and%20benchmark%20multi-robot%0Asearch%20algorithms.%20The%20presented%20physical%20setup%20innovatively%20uses%20an%20acoustic%0Asource%20%28that%20is%20safe%20and%20inexpensive%29%20and%20small%20ground%20robots%20%28e-pucks%29%0Aoperating%20in%20a%20standard%20motion-capture%20environment.%20This%20setup%20can%20be%20easily%0Arecreated%20and%20used%20by%20most%20robotics%20researchers.%20The%20acoustic%20source%20also%0Apresents%20interesting%20uncertainty%20in%20terms%20of%20its%20noise-to-signal%20ratio%2C%20which%0Ais%20useful%20to%20assess%20sim-to-real%20gaps.%20The%20overall%20software%20pipeline%20is%20designed%0Ato%20readily%20interface%20with%20any%20multi-robot%20search%20algorithm%20with%20minimal%20effort%0Aand%20is%20executable%20in%20parallel%20asynchronous%20form.%20This%20pipeline%20includes%20a%0Aframework%20for%20distributed%20implementation%20of%20multi-robot%20or%20swarm%20search%0Aalgorithms%2C%20integrated%20with%20a%20ROS%20%28Robotics%20Operating%20System%29-based%20software%0Astack%20for%20motion%20capture%20supported%20localization.%20The%20utility%20of%20this%20novel%0Asetup%20is%20demonstrated%20by%20using%20it%20to%20evaluate%20two%20state-of-the-art%20multi-robot%0Asearch%20algorithms%2C%20based%20on%20swarm%20optimization%20and%20batch-Bayesian%20Optimization%0A%28called%20Bayes-Swarm%29%2C%20as%20well%20as%20a%20random%20walk%20baseline.%0A&entry.1838667208=http%3A//arxiv.org/abs/2506.16710v3&entry.124074799=Read"},
{"title": "Red Teaming Large Language Models for Healthcare", "author": "Vahid Balazadeh and Michael Cooper and David Pellow and Atousa Assadi and Jennifer Bell and Mark Coatsworth and Kaivalya Deshpande and Jim Fackler and Gabriel Funingana and Spencer Gable-Cook and Anirudh Gangadhar and Abhishek Jaiswal and Sumanth Kaja and Christopher Khoury and Amrit Krishnan and Randy Lin and Kaden McKeen and Sara Naimimohasses and Khashayar Namdar and Aviraj Newatia and Allan Pang and Anshul Pattoo and Sameer Peesapati and Diana Prepelita and Bogdana Rakova and Saba Sadatamin and Rafael Schulman and Ajay Shah and Syed Azhar Shah and Syed Ahmar Shah and Babak Taati and Balagopal Unnikrishnan and I\u00f1igo Urteaga and Stephanie Williams and Rahul G Krishnan", "abstract": "  We present the design process and findings of the pre-conference workshop at\nthe Machine Learning for Healthcare Conference (2024) entitled Red Teaming\nLarge Language Models for Healthcare, which took place on August 15, 2024.\nConference participants, comprising a mix of computational and clinical\nexpertise, attempted to discover vulnerabilities -- realistic clinical prompts\nfor which a large language model (LLM) outputs a response that could cause\nclinical harm. Red-teaming with clinicians enables the identification of LLM\nvulnerabilities that may not be recognised by LLM developers lacking clinical\nexpertise. We report the vulnerabilities found, categorise them, and present\nthe results of a replication study assessing the vulnerabilities across all\nLLMs provided.\n", "link": "http://arxiv.org/abs/2505.00467v2", "date": "2025-07-11", "relevancy": 1.786, "topK": [{"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.4599}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.4438}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.4438}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Red%20Teaming%20Large%20Language%20Models%20for%20Healthcare&body=Title%3A%20Red%20Teaming%20Large%20Language%20Models%20for%20Healthcare%0AAuthor%3A%20Vahid%20Balazadeh%20and%20Michael%20Cooper%20and%20David%20Pellow%20and%20Atousa%20Assadi%20and%20Jennifer%20Bell%20and%20Mark%20Coatsworth%20and%20Kaivalya%20Deshpande%20and%20Jim%20Fackler%20and%20Gabriel%20Funingana%20and%20Spencer%20Gable-Cook%20and%20Anirudh%20Gangadhar%20and%20Abhishek%20Jaiswal%20and%20Sumanth%20Kaja%20and%20Christopher%20Khoury%20and%20Amrit%20Krishnan%20and%20Randy%20Lin%20and%20Kaden%20McKeen%20and%20Sara%20Naimimohasses%20and%20Khashayar%20Namdar%20and%20Aviraj%20Newatia%20and%20Allan%20Pang%20and%20Anshul%20Pattoo%20and%20Sameer%20Peesapati%20and%20Diana%20Prepelita%20and%20Bogdana%20Rakova%20and%20Saba%20Sadatamin%20and%20Rafael%20Schulman%20and%20Ajay%20Shah%20and%20Syed%20Azhar%20Shah%20and%20Syed%20Ahmar%20Shah%20and%20Babak%20Taati%20and%20Balagopal%20Unnikrishnan%20and%20I%C3%B1igo%20Urteaga%20and%20Stephanie%20Williams%20and%20Rahul%20G%20Krishnan%0AAbstract%3A%20%20%20We%20present%20the%20design%20process%20and%20findings%20of%20the%20pre-conference%20workshop%20at%0Athe%20Machine%20Learning%20for%20Healthcare%20Conference%20%282024%29%20entitled%20Red%20Teaming%0ALarge%20Language%20Models%20for%20Healthcare%2C%20which%20took%20place%20on%20August%2015%2C%202024.%0AConference%20participants%2C%20comprising%20a%20mix%20of%20computational%20and%20clinical%0Aexpertise%2C%20attempted%20to%20discover%20vulnerabilities%20--%20realistic%20clinical%20prompts%0Afor%20which%20a%20large%20language%20model%20%28LLM%29%20outputs%20a%20response%20that%20could%20cause%0Aclinical%20harm.%20Red-teaming%20with%20clinicians%20enables%20the%20identification%20of%20LLM%0Avulnerabilities%20that%20may%20not%20be%20recognised%20by%20LLM%20developers%20lacking%20clinical%0Aexpertise.%20We%20report%20the%20vulnerabilities%20found%2C%20categorise%20them%2C%20and%20present%0Athe%20results%20of%20a%20replication%20study%20assessing%20the%20vulnerabilities%20across%20all%0ALLMs%20provided.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2505.00467v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DRed%2520Teaming%2520Large%2520Language%2520Models%2520for%2520Healthcare%26entry.906535625%3DVahid%2520Balazadeh%2520and%2520Michael%2520Cooper%2520and%2520David%2520Pellow%2520and%2520Atousa%2520Assadi%2520and%2520Jennifer%2520Bell%2520and%2520Mark%2520Coatsworth%2520and%2520Kaivalya%2520Deshpande%2520and%2520Jim%2520Fackler%2520and%2520Gabriel%2520Funingana%2520and%2520Spencer%2520Gable-Cook%2520and%2520Anirudh%2520Gangadhar%2520and%2520Abhishek%2520Jaiswal%2520and%2520Sumanth%2520Kaja%2520and%2520Christopher%2520Khoury%2520and%2520Amrit%2520Krishnan%2520and%2520Randy%2520Lin%2520and%2520Kaden%2520McKeen%2520and%2520Sara%2520Naimimohasses%2520and%2520Khashayar%2520Namdar%2520and%2520Aviraj%2520Newatia%2520and%2520Allan%2520Pang%2520and%2520Anshul%2520Pattoo%2520and%2520Sameer%2520Peesapati%2520and%2520Diana%2520Prepelita%2520and%2520Bogdana%2520Rakova%2520and%2520Saba%2520Sadatamin%2520and%2520Rafael%2520Schulman%2520and%2520Ajay%2520Shah%2520and%2520Syed%2520Azhar%2520Shah%2520and%2520Syed%2520Ahmar%2520Shah%2520and%2520Babak%2520Taati%2520and%2520Balagopal%2520Unnikrishnan%2520and%2520I%25C3%25B1igo%2520Urteaga%2520and%2520Stephanie%2520Williams%2520and%2520Rahul%2520G%2520Krishnan%26entry.1292438233%3D%2520%2520We%2520present%2520the%2520design%2520process%2520and%2520findings%2520of%2520the%2520pre-conference%2520workshop%2520at%250Athe%2520Machine%2520Learning%2520for%2520Healthcare%2520Conference%2520%25282024%2529%2520entitled%2520Red%2520Teaming%250ALarge%2520Language%2520Models%2520for%2520Healthcare%252C%2520which%2520took%2520place%2520on%2520August%252015%252C%25202024.%250AConference%2520participants%252C%2520comprising%2520a%2520mix%2520of%2520computational%2520and%2520clinical%250Aexpertise%252C%2520attempted%2520to%2520discover%2520vulnerabilities%2520--%2520realistic%2520clinical%2520prompts%250Afor%2520which%2520a%2520large%2520language%2520model%2520%2528LLM%2529%2520outputs%2520a%2520response%2520that%2520could%2520cause%250Aclinical%2520harm.%2520Red-teaming%2520with%2520clinicians%2520enables%2520the%2520identification%2520of%2520LLM%250Avulnerabilities%2520that%2520may%2520not%2520be%2520recognised%2520by%2520LLM%2520developers%2520lacking%2520clinical%250Aexpertise.%2520We%2520report%2520the%2520vulnerabilities%2520found%252C%2520categorise%2520them%252C%2520and%2520present%250Athe%2520results%2520of%2520a%2520replication%2520study%2520assessing%2520the%2520vulnerabilities%2520across%2520all%250ALLMs%2520provided.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2505.00467v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Red%20Teaming%20Large%20Language%20Models%20for%20Healthcare&entry.906535625=Vahid%20Balazadeh%20and%20Michael%20Cooper%20and%20David%20Pellow%20and%20Atousa%20Assadi%20and%20Jennifer%20Bell%20and%20Mark%20Coatsworth%20and%20Kaivalya%20Deshpande%20and%20Jim%20Fackler%20and%20Gabriel%20Funingana%20and%20Spencer%20Gable-Cook%20and%20Anirudh%20Gangadhar%20and%20Abhishek%20Jaiswal%20and%20Sumanth%20Kaja%20and%20Christopher%20Khoury%20and%20Amrit%20Krishnan%20and%20Randy%20Lin%20and%20Kaden%20McKeen%20and%20Sara%20Naimimohasses%20and%20Khashayar%20Namdar%20and%20Aviraj%20Newatia%20and%20Allan%20Pang%20and%20Anshul%20Pattoo%20and%20Sameer%20Peesapati%20and%20Diana%20Prepelita%20and%20Bogdana%20Rakova%20and%20Saba%20Sadatamin%20and%20Rafael%20Schulman%20and%20Ajay%20Shah%20and%20Syed%20Azhar%20Shah%20and%20Syed%20Ahmar%20Shah%20and%20Babak%20Taati%20and%20Balagopal%20Unnikrishnan%20and%20I%C3%B1igo%20Urteaga%20and%20Stephanie%20Williams%20and%20Rahul%20G%20Krishnan&entry.1292438233=%20%20We%20present%20the%20design%20process%20and%20findings%20of%20the%20pre-conference%20workshop%20at%0Athe%20Machine%20Learning%20for%20Healthcare%20Conference%20%282024%29%20entitled%20Red%20Teaming%0ALarge%20Language%20Models%20for%20Healthcare%2C%20which%20took%20place%20on%20August%2015%2C%202024.%0AConference%20participants%2C%20comprising%20a%20mix%20of%20computational%20and%20clinical%0Aexpertise%2C%20attempted%20to%20discover%20vulnerabilities%20--%20realistic%20clinical%20prompts%0Afor%20which%20a%20large%20language%20model%20%28LLM%29%20outputs%20a%20response%20that%20could%20cause%0Aclinical%20harm.%20Red-teaming%20with%20clinicians%20enables%20the%20identification%20of%20LLM%0Avulnerabilities%20that%20may%20not%20be%20recognised%20by%20LLM%20developers%20lacking%20clinical%0Aexpertise.%20We%20report%20the%20vulnerabilities%20found%2C%20categorise%20them%2C%20and%20present%0Athe%20results%20of%20a%20replication%20study%20assessing%20the%20vulnerabilities%20across%20all%0ALLMs%20provided.%0A&entry.1838667208=http%3A//arxiv.org/abs/2505.00467v2&entry.124074799=Read"},
{"title": "Exploring Efficient Quantification of Modeling Uncertainties with\n  Differentiable Physics-Informed Machine Learning Architectures", "author": "Manaswin Oddiraju and Bharath Varma Penumatsa and Divyang Amin and Michael Piedmonte and Souma Chowdhury", "abstract": "  Quantifying and propagating modeling uncertainties is crucial for reliability\nanalysis, robust optimization, and other model-based algorithmic processes in\nengineering design and control. Now, physics-informed machine learning (PIML)\nmethods have emerged in recent years as a new alternative to traditional\ncomputational modeling and surrogate modeling methods, offering a balance\nbetween computing efficiency, modeling accuracy, and interpretability. However,\ntheir ability to predict and propagate modeling uncertainties remains mostly\nunexplored. In this paper, a promising class of auto-differentiable hybrid PIML\narchitectures that combine partial physics and neural networks or ANNs (for\ninput transformation or adaptive parameter estimation) is integrated with\nBayesian Neural networks (replacing the ANNs); this is done with the goal to\nexplore whether BNNs can successfully provision uncertainty propagation\ncapabilities in the PIML architectures as well, further supported by the\nauto-differentiability of these architectures. A two-stage training process is\nused to alleviate the challenges traditionally encountered in training\nprobabilistic ML models. The resulting BNN-integrated PIML architecture is\nevaluated on an analytical benchmark problem and flight experiments data for a\nfixed-wing RC aircraft, with prediction performance observed to be slightly\nworse or at par with purely data-driven ML and original PIML models. Moreover,\nMonte Carlo sampling of probabilistic BNN weights was found to be most\neffective in propagating uncertainty in the BNN-integrated PIML architectures.\n", "link": "http://arxiv.org/abs/2506.18247v2", "date": "2025-07-11", "relevancy": 1.7568, "topK": [{"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.5971}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.5933}, {"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.549}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Exploring%20Efficient%20Quantification%20of%20Modeling%20Uncertainties%20with%0A%20%20Differentiable%20Physics-Informed%20Machine%20Learning%20Architectures&body=Title%3A%20Exploring%20Efficient%20Quantification%20of%20Modeling%20Uncertainties%20with%0A%20%20Differentiable%20Physics-Informed%20Machine%20Learning%20Architectures%0AAuthor%3A%20Manaswin%20Oddiraju%20and%20Bharath%20Varma%20Penumatsa%20and%20Divyang%20Amin%20and%20Michael%20Piedmonte%20and%20Souma%20Chowdhury%0AAbstract%3A%20%20%20Quantifying%20and%20propagating%20modeling%20uncertainties%20is%20crucial%20for%20reliability%0Aanalysis%2C%20robust%20optimization%2C%20and%20other%20model-based%20algorithmic%20processes%20in%0Aengineering%20design%20and%20control.%20Now%2C%20physics-informed%20machine%20learning%20%28PIML%29%0Amethods%20have%20emerged%20in%20recent%20years%20as%20a%20new%20alternative%20to%20traditional%0Acomputational%20modeling%20and%20surrogate%20modeling%20methods%2C%20offering%20a%20balance%0Abetween%20computing%20efficiency%2C%20modeling%20accuracy%2C%20and%20interpretability.%20However%2C%0Atheir%20ability%20to%20predict%20and%20propagate%20modeling%20uncertainties%20remains%20mostly%0Aunexplored.%20In%20this%20paper%2C%20a%20promising%20class%20of%20auto-differentiable%20hybrid%20PIML%0Aarchitectures%20that%20combine%20partial%20physics%20and%20neural%20networks%20or%20ANNs%20%28for%0Ainput%20transformation%20or%20adaptive%20parameter%20estimation%29%20is%20integrated%20with%0ABayesian%20Neural%20networks%20%28replacing%20the%20ANNs%29%3B%20this%20is%20done%20with%20the%20goal%20to%0Aexplore%20whether%20BNNs%20can%20successfully%20provision%20uncertainty%20propagation%0Acapabilities%20in%20the%20PIML%20architectures%20as%20well%2C%20further%20supported%20by%20the%0Aauto-differentiability%20of%20these%20architectures.%20A%20two-stage%20training%20process%20is%0Aused%20to%20alleviate%20the%20challenges%20traditionally%20encountered%20in%20training%0Aprobabilistic%20ML%20models.%20The%20resulting%20BNN-integrated%20PIML%20architecture%20is%0Aevaluated%20on%20an%20analytical%20benchmark%20problem%20and%20flight%20experiments%20data%20for%20a%0Afixed-wing%20RC%20aircraft%2C%20with%20prediction%20performance%20observed%20to%20be%20slightly%0Aworse%20or%20at%20par%20with%20purely%20data-driven%20ML%20and%20original%20PIML%20models.%20Moreover%2C%0AMonte%20Carlo%20sampling%20of%20probabilistic%20BNN%20weights%20was%20found%20to%20be%20most%0Aeffective%20in%20propagating%20uncertainty%20in%20the%20BNN-integrated%20PIML%20architectures.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2506.18247v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DExploring%2520Efficient%2520Quantification%2520of%2520Modeling%2520Uncertainties%2520with%250A%2520%2520Differentiable%2520Physics-Informed%2520Machine%2520Learning%2520Architectures%26entry.906535625%3DManaswin%2520Oddiraju%2520and%2520Bharath%2520Varma%2520Penumatsa%2520and%2520Divyang%2520Amin%2520and%2520Michael%2520Piedmonte%2520and%2520Souma%2520Chowdhury%26entry.1292438233%3D%2520%2520Quantifying%2520and%2520propagating%2520modeling%2520uncertainties%2520is%2520crucial%2520for%2520reliability%250Aanalysis%252C%2520robust%2520optimization%252C%2520and%2520other%2520model-based%2520algorithmic%2520processes%2520in%250Aengineering%2520design%2520and%2520control.%2520Now%252C%2520physics-informed%2520machine%2520learning%2520%2528PIML%2529%250Amethods%2520have%2520emerged%2520in%2520recent%2520years%2520as%2520a%2520new%2520alternative%2520to%2520traditional%250Acomputational%2520modeling%2520and%2520surrogate%2520modeling%2520methods%252C%2520offering%2520a%2520balance%250Abetween%2520computing%2520efficiency%252C%2520modeling%2520accuracy%252C%2520and%2520interpretability.%2520However%252C%250Atheir%2520ability%2520to%2520predict%2520and%2520propagate%2520modeling%2520uncertainties%2520remains%2520mostly%250Aunexplored.%2520In%2520this%2520paper%252C%2520a%2520promising%2520class%2520of%2520auto-differentiable%2520hybrid%2520PIML%250Aarchitectures%2520that%2520combine%2520partial%2520physics%2520and%2520neural%2520networks%2520or%2520ANNs%2520%2528for%250Ainput%2520transformation%2520or%2520adaptive%2520parameter%2520estimation%2529%2520is%2520integrated%2520with%250ABayesian%2520Neural%2520networks%2520%2528replacing%2520the%2520ANNs%2529%253B%2520this%2520is%2520done%2520with%2520the%2520goal%2520to%250Aexplore%2520whether%2520BNNs%2520can%2520successfully%2520provision%2520uncertainty%2520propagation%250Acapabilities%2520in%2520the%2520PIML%2520architectures%2520as%2520well%252C%2520further%2520supported%2520by%2520the%250Aauto-differentiability%2520of%2520these%2520architectures.%2520A%2520two-stage%2520training%2520process%2520is%250Aused%2520to%2520alleviate%2520the%2520challenges%2520traditionally%2520encountered%2520in%2520training%250Aprobabilistic%2520ML%2520models.%2520The%2520resulting%2520BNN-integrated%2520PIML%2520architecture%2520is%250Aevaluated%2520on%2520an%2520analytical%2520benchmark%2520problem%2520and%2520flight%2520experiments%2520data%2520for%2520a%250Afixed-wing%2520RC%2520aircraft%252C%2520with%2520prediction%2520performance%2520observed%2520to%2520be%2520slightly%250Aworse%2520or%2520at%2520par%2520with%2520purely%2520data-driven%2520ML%2520and%2520original%2520PIML%2520models.%2520Moreover%252C%250AMonte%2520Carlo%2520sampling%2520of%2520probabilistic%2520BNN%2520weights%2520was%2520found%2520to%2520be%2520most%250Aeffective%2520in%2520propagating%2520uncertainty%2520in%2520the%2520BNN-integrated%2520PIML%2520architectures.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2506.18247v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Exploring%20Efficient%20Quantification%20of%20Modeling%20Uncertainties%20with%0A%20%20Differentiable%20Physics-Informed%20Machine%20Learning%20Architectures&entry.906535625=Manaswin%20Oddiraju%20and%20Bharath%20Varma%20Penumatsa%20and%20Divyang%20Amin%20and%20Michael%20Piedmonte%20and%20Souma%20Chowdhury&entry.1292438233=%20%20Quantifying%20and%20propagating%20modeling%20uncertainties%20is%20crucial%20for%20reliability%0Aanalysis%2C%20robust%20optimization%2C%20and%20other%20model-based%20algorithmic%20processes%20in%0Aengineering%20design%20and%20control.%20Now%2C%20physics-informed%20machine%20learning%20%28PIML%29%0Amethods%20have%20emerged%20in%20recent%20years%20as%20a%20new%20alternative%20to%20traditional%0Acomputational%20modeling%20and%20surrogate%20modeling%20methods%2C%20offering%20a%20balance%0Abetween%20computing%20efficiency%2C%20modeling%20accuracy%2C%20and%20interpretability.%20However%2C%0Atheir%20ability%20to%20predict%20and%20propagate%20modeling%20uncertainties%20remains%20mostly%0Aunexplored.%20In%20this%20paper%2C%20a%20promising%20class%20of%20auto-differentiable%20hybrid%20PIML%0Aarchitectures%20that%20combine%20partial%20physics%20and%20neural%20networks%20or%20ANNs%20%28for%0Ainput%20transformation%20or%20adaptive%20parameter%20estimation%29%20is%20integrated%20with%0ABayesian%20Neural%20networks%20%28replacing%20the%20ANNs%29%3B%20this%20is%20done%20with%20the%20goal%20to%0Aexplore%20whether%20BNNs%20can%20successfully%20provision%20uncertainty%20propagation%0Acapabilities%20in%20the%20PIML%20architectures%20as%20well%2C%20further%20supported%20by%20the%0Aauto-differentiability%20of%20these%20architectures.%20A%20two-stage%20training%20process%20is%0Aused%20to%20alleviate%20the%20challenges%20traditionally%20encountered%20in%20training%0Aprobabilistic%20ML%20models.%20The%20resulting%20BNN-integrated%20PIML%20architecture%20is%0Aevaluated%20on%20an%20analytical%20benchmark%20problem%20and%20flight%20experiments%20data%20for%20a%0Afixed-wing%20RC%20aircraft%2C%20with%20prediction%20performance%20observed%20to%20be%20slightly%0Aworse%20or%20at%20par%20with%20purely%20data-driven%20ML%20and%20original%20PIML%20models.%20Moreover%2C%0AMonte%20Carlo%20sampling%20of%20probabilistic%20BNN%20weights%20was%20found%20to%20be%20most%0Aeffective%20in%20propagating%20uncertainty%20in%20the%20BNN-integrated%20PIML%20architectures.%0A&entry.1838667208=http%3A//arxiv.org/abs/2506.18247v2&entry.124074799=Read"},
{"title": "On the Effect of Regularization in Policy Mirror Descent", "author": "Jan Felix Kleuker and Aske Plaat and Thomas Moerland", "abstract": "  Policy Mirror Descent (PMD) has emerged as a unifying framework in\nreinforcement learning (RL) by linking policy gradient methods with a\nfirst-order optimization method known as mirror descent. At its core, PMD\nincorporates two key regularization components: (i) a distance term that\nenforces a trust region for stable policy updates and (ii) an MDP regularizer\nthat augments the reward function to promote structure and robustness. While\nPMD has been extensively studied in theory, empirical investigations remain\nscarce. This work provides a large-scale empirical analysis of the interplay\nbetween these two regularization techniques, running over 500k training seeds\non small RL environments. Our results demonstrate that, although the two\nregularizers can partially substitute each other, their precise combination is\ncritical for achieving robust performance. These findings highlight the\npotential for advancing research on more robust algorithms in RL, particularly\nwith respect to hyperparameter sensitivity.\n", "link": "http://arxiv.org/abs/2507.08718v1", "date": "2025-07-11", "relevancy": 1.7385, "topK": [{"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.4479}, {"title": "MiraGe: Editable 2D Images using Gaussian Splatting", "link": "http://arxiv.org/abs/2410.01521v1", "similarity": 0.4337}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.4302}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20On%20the%20Effect%20of%20Regularization%20in%20Policy%20Mirror%20Descent&body=Title%3A%20On%20the%20Effect%20of%20Regularization%20in%20Policy%20Mirror%20Descent%0AAuthor%3A%20Jan%20Felix%20Kleuker%20and%20Aske%20Plaat%20and%20Thomas%20Moerland%0AAbstract%3A%20%20%20Policy%20Mirror%20Descent%20%28PMD%29%20has%20emerged%20as%20a%20unifying%20framework%20in%0Areinforcement%20learning%20%28RL%29%20by%20linking%20policy%20gradient%20methods%20with%20a%0Afirst-order%20optimization%20method%20known%20as%20mirror%20descent.%20At%20its%20core%2C%20PMD%0Aincorporates%20two%20key%20regularization%20components%3A%20%28i%29%20a%20distance%20term%20that%0Aenforces%20a%20trust%20region%20for%20stable%20policy%20updates%20and%20%28ii%29%20an%20MDP%20regularizer%0Athat%20augments%20the%20reward%20function%20to%20promote%20structure%20and%20robustness.%20While%0APMD%20has%20been%20extensively%20studied%20in%20theory%2C%20empirical%20investigations%20remain%0Ascarce.%20This%20work%20provides%20a%20large-scale%20empirical%20analysis%20of%20the%20interplay%0Abetween%20these%20two%20regularization%20techniques%2C%20running%20over%20500k%20training%20seeds%0Aon%20small%20RL%20environments.%20Our%20results%20demonstrate%20that%2C%20although%20the%20two%0Aregularizers%20can%20partially%20substitute%20each%20other%2C%20their%20precise%20combination%20is%0Acritical%20for%20achieving%20robust%20performance.%20These%20findings%20highlight%20the%0Apotential%20for%20advancing%20research%20on%20more%20robust%20algorithms%20in%20RL%2C%20particularly%0Awith%20respect%20to%20hyperparameter%20sensitivity.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2507.08718v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DOn%2520the%2520Effect%2520of%2520Regularization%2520in%2520Policy%2520Mirror%2520Descent%26entry.906535625%3DJan%2520Felix%2520Kleuker%2520and%2520Aske%2520Plaat%2520and%2520Thomas%2520Moerland%26entry.1292438233%3D%2520%2520Policy%2520Mirror%2520Descent%2520%2528PMD%2529%2520has%2520emerged%2520as%2520a%2520unifying%2520framework%2520in%250Areinforcement%2520learning%2520%2528RL%2529%2520by%2520linking%2520policy%2520gradient%2520methods%2520with%2520a%250Afirst-order%2520optimization%2520method%2520known%2520as%2520mirror%2520descent.%2520At%2520its%2520core%252C%2520PMD%250Aincorporates%2520two%2520key%2520regularization%2520components%253A%2520%2528i%2529%2520a%2520distance%2520term%2520that%250Aenforces%2520a%2520trust%2520region%2520for%2520stable%2520policy%2520updates%2520and%2520%2528ii%2529%2520an%2520MDP%2520regularizer%250Athat%2520augments%2520the%2520reward%2520function%2520to%2520promote%2520structure%2520and%2520robustness.%2520While%250APMD%2520has%2520been%2520extensively%2520studied%2520in%2520theory%252C%2520empirical%2520investigations%2520remain%250Ascarce.%2520This%2520work%2520provides%2520a%2520large-scale%2520empirical%2520analysis%2520of%2520the%2520interplay%250Abetween%2520these%2520two%2520regularization%2520techniques%252C%2520running%2520over%2520500k%2520training%2520seeds%250Aon%2520small%2520RL%2520environments.%2520Our%2520results%2520demonstrate%2520that%252C%2520although%2520the%2520two%250Aregularizers%2520can%2520partially%2520substitute%2520each%2520other%252C%2520their%2520precise%2520combination%2520is%250Acritical%2520for%2520achieving%2520robust%2520performance.%2520These%2520findings%2520highlight%2520the%250Apotential%2520for%2520advancing%2520research%2520on%2520more%2520robust%2520algorithms%2520in%2520RL%252C%2520particularly%250Awith%2520respect%2520to%2520hyperparameter%2520sensitivity.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2507.08718v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=On%20the%20Effect%20of%20Regularization%20in%20Policy%20Mirror%20Descent&entry.906535625=Jan%20Felix%20Kleuker%20and%20Aske%20Plaat%20and%20Thomas%20Moerland&entry.1292438233=%20%20Policy%20Mirror%20Descent%20%28PMD%29%20has%20emerged%20as%20a%20unifying%20framework%20in%0Areinforcement%20learning%20%28RL%29%20by%20linking%20policy%20gradient%20methods%20with%20a%0Afirst-order%20optimization%20method%20known%20as%20mirror%20descent.%20At%20its%20core%2C%20PMD%0Aincorporates%20two%20key%20regularization%20components%3A%20%28i%29%20a%20distance%20term%20that%0Aenforces%20a%20trust%20region%20for%20stable%20policy%20updates%20and%20%28ii%29%20an%20MDP%20regularizer%0Athat%20augments%20the%20reward%20function%20to%20promote%20structure%20and%20robustness.%20While%0APMD%20has%20been%20extensively%20studied%20in%20theory%2C%20empirical%20investigations%20remain%0Ascarce.%20This%20work%20provides%20a%20large-scale%20empirical%20analysis%20of%20the%20interplay%0Abetween%20these%20two%20regularization%20techniques%2C%20running%20over%20500k%20training%20seeds%0Aon%20small%20RL%20environments.%20Our%20results%20demonstrate%20that%2C%20although%20the%20two%0Aregularizers%20can%20partially%20substitute%20each%20other%2C%20their%20precise%20combination%20is%0Acritical%20for%20achieving%20robust%20performance.%20These%20findings%20highlight%20the%0Apotential%20for%20advancing%20research%20on%20more%20robust%20algorithms%20in%20RL%2C%20particularly%0Awith%20respect%20to%20hyperparameter%20sensitivity.%0A&entry.1838667208=http%3A//arxiv.org/abs/2507.08718v1&entry.124074799=Read"},
{"title": "Entangled Threats: A Unified Kill Chain Model for Quantum Machine\n  Learning Security", "author": "Pascal Debus and Maximilian Wendlinger and Kilian Tscharke and Daniel Herr and Cedric Br\u00fcgmann and Daniel Ohl de Mello and Juris Ulmanis and Alexander Erhard and Arthur Schmidt and Fabian Petsch", "abstract": "  Quantum Machine Learning (QML) systems inherit vulnerabilities from classical\nmachine learning while introducing new attack surfaces rooted in the physical\nand algorithmic layers of quantum computing. Despite a growing body of research\non individual attack vectors - ranging from adversarial poisoning and evasion\nto circuit-level backdoors, side-channel leakage, and model extraction - these\nthreats are often analyzed in isolation, with unrealistic assumptions about\nattacker capabilities and system environments. This fragmentation hampers the\ndevelopment of effective, holistic defense strategies. In this work, we argue\nthat QML security requires more structured modeling of the attack surface,\ncapturing not only individual techniques but also their relationships,\nprerequisites, and potential impact across the QML pipeline. We propose\nadapting kill chain models, widely used in classical IT and cybersecurity, to\nthe quantum machine learning context. Such models allow for structured\nreasoning about attacker objectives, capabilities, and possible multi-stage\nattack paths - spanning reconnaissance, initial access, manipulation,\npersistence, and exfiltration. Based on extensive literature analysis, we\npresent a detailed taxonomy of QML attack vectors mapped to corresponding\nstages in a quantum-aware kill chain framework that is inspired by the MITRE\nATLAS for classical machine learning. We highlight interdependencies between\nphysical-level threats (like side-channel leakage and crosstalk faults), data\nand algorithm manipulation (such as poisoning or circuit backdoors), and\nprivacy attacks (including model extraction and training data inference). This\nwork provides a foundation for more realistic threat modeling and proactive\nsecurity-in-depth design in the emerging field of quantum machine learning.\n", "link": "http://arxiv.org/abs/2507.08623v1", "date": "2025-07-11", "relevancy": 1.7366, "topK": [{"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.4343}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.4341}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.4341}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Entangled%20Threats%3A%20A%20Unified%20Kill%20Chain%20Model%20for%20Quantum%20Machine%0A%20%20Learning%20Security&body=Title%3A%20Entangled%20Threats%3A%20A%20Unified%20Kill%20Chain%20Model%20for%20Quantum%20Machine%0A%20%20Learning%20Security%0AAuthor%3A%20Pascal%20Debus%20and%20Maximilian%20Wendlinger%20and%20Kilian%20Tscharke%20and%20Daniel%20Herr%20and%20Cedric%20Br%C3%BCgmann%20and%20Daniel%20Ohl%20de%20Mello%20and%20Juris%20Ulmanis%20and%20Alexander%20Erhard%20and%20Arthur%20Schmidt%20and%20Fabian%20Petsch%0AAbstract%3A%20%20%20Quantum%20Machine%20Learning%20%28QML%29%20systems%20inherit%20vulnerabilities%20from%20classical%0Amachine%20learning%20while%20introducing%20new%20attack%20surfaces%20rooted%20in%20the%20physical%0Aand%20algorithmic%20layers%20of%20quantum%20computing.%20Despite%20a%20growing%20body%20of%20research%0Aon%20individual%20attack%20vectors%20-%20ranging%20from%20adversarial%20poisoning%20and%20evasion%0Ato%20circuit-level%20backdoors%2C%20side-channel%20leakage%2C%20and%20model%20extraction%20-%20these%0Athreats%20are%20often%20analyzed%20in%20isolation%2C%20with%20unrealistic%20assumptions%20about%0Aattacker%20capabilities%20and%20system%20environments.%20This%20fragmentation%20hampers%20the%0Adevelopment%20of%20effective%2C%20holistic%20defense%20strategies.%20In%20this%20work%2C%20we%20argue%0Athat%20QML%20security%20requires%20more%20structured%20modeling%20of%20the%20attack%20surface%2C%0Acapturing%20not%20only%20individual%20techniques%20but%20also%20their%20relationships%2C%0Aprerequisites%2C%20and%20potential%20impact%20across%20the%20QML%20pipeline.%20We%20propose%0Aadapting%20kill%20chain%20models%2C%20widely%20used%20in%20classical%20IT%20and%20cybersecurity%2C%20to%0Athe%20quantum%20machine%20learning%20context.%20Such%20models%20allow%20for%20structured%0Areasoning%20about%20attacker%20objectives%2C%20capabilities%2C%20and%20possible%20multi-stage%0Aattack%20paths%20-%20spanning%20reconnaissance%2C%20initial%20access%2C%20manipulation%2C%0Apersistence%2C%20and%20exfiltration.%20Based%20on%20extensive%20literature%20analysis%2C%20we%0Apresent%20a%20detailed%20taxonomy%20of%20QML%20attack%20vectors%20mapped%20to%20corresponding%0Astages%20in%20a%20quantum-aware%20kill%20chain%20framework%20that%20is%20inspired%20by%20the%20MITRE%0AATLAS%20for%20classical%20machine%20learning.%20We%20highlight%20interdependencies%20between%0Aphysical-level%20threats%20%28like%20side-channel%20leakage%20and%20crosstalk%20faults%29%2C%20data%0Aand%20algorithm%20manipulation%20%28such%20as%20poisoning%20or%20circuit%20backdoors%29%2C%20and%0Aprivacy%20attacks%20%28including%20model%20extraction%20and%20training%20data%20inference%29.%20This%0Awork%20provides%20a%20foundation%20for%20more%20realistic%20threat%20modeling%20and%20proactive%0Asecurity-in-depth%20design%20in%20the%20emerging%20field%20of%20quantum%20machine%20learning.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2507.08623v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DEntangled%2520Threats%253A%2520A%2520Unified%2520Kill%2520Chain%2520Model%2520for%2520Quantum%2520Machine%250A%2520%2520Learning%2520Security%26entry.906535625%3DPascal%2520Debus%2520and%2520Maximilian%2520Wendlinger%2520and%2520Kilian%2520Tscharke%2520and%2520Daniel%2520Herr%2520and%2520Cedric%2520Br%25C3%25BCgmann%2520and%2520Daniel%2520Ohl%2520de%2520Mello%2520and%2520Juris%2520Ulmanis%2520and%2520Alexander%2520Erhard%2520and%2520Arthur%2520Schmidt%2520and%2520Fabian%2520Petsch%26entry.1292438233%3D%2520%2520Quantum%2520Machine%2520Learning%2520%2528QML%2529%2520systems%2520inherit%2520vulnerabilities%2520from%2520classical%250Amachine%2520learning%2520while%2520introducing%2520new%2520attack%2520surfaces%2520rooted%2520in%2520the%2520physical%250Aand%2520algorithmic%2520layers%2520of%2520quantum%2520computing.%2520Despite%2520a%2520growing%2520body%2520of%2520research%250Aon%2520individual%2520attack%2520vectors%2520-%2520ranging%2520from%2520adversarial%2520poisoning%2520and%2520evasion%250Ato%2520circuit-level%2520backdoors%252C%2520side-channel%2520leakage%252C%2520and%2520model%2520extraction%2520-%2520these%250Athreats%2520are%2520often%2520analyzed%2520in%2520isolation%252C%2520with%2520unrealistic%2520assumptions%2520about%250Aattacker%2520capabilities%2520and%2520system%2520environments.%2520This%2520fragmentation%2520hampers%2520the%250Adevelopment%2520of%2520effective%252C%2520holistic%2520defense%2520strategies.%2520In%2520this%2520work%252C%2520we%2520argue%250Athat%2520QML%2520security%2520requires%2520more%2520structured%2520modeling%2520of%2520the%2520attack%2520surface%252C%250Acapturing%2520not%2520only%2520individual%2520techniques%2520but%2520also%2520their%2520relationships%252C%250Aprerequisites%252C%2520and%2520potential%2520impact%2520across%2520the%2520QML%2520pipeline.%2520We%2520propose%250Aadapting%2520kill%2520chain%2520models%252C%2520widely%2520used%2520in%2520classical%2520IT%2520and%2520cybersecurity%252C%2520to%250Athe%2520quantum%2520machine%2520learning%2520context.%2520Such%2520models%2520allow%2520for%2520structured%250Areasoning%2520about%2520attacker%2520objectives%252C%2520capabilities%252C%2520and%2520possible%2520multi-stage%250Aattack%2520paths%2520-%2520spanning%2520reconnaissance%252C%2520initial%2520access%252C%2520manipulation%252C%250Apersistence%252C%2520and%2520exfiltration.%2520Based%2520on%2520extensive%2520literature%2520analysis%252C%2520we%250Apresent%2520a%2520detailed%2520taxonomy%2520of%2520QML%2520attack%2520vectors%2520mapped%2520to%2520corresponding%250Astages%2520in%2520a%2520quantum-aware%2520kill%2520chain%2520framework%2520that%2520is%2520inspired%2520by%2520the%2520MITRE%250AATLAS%2520for%2520classical%2520machine%2520learning.%2520We%2520highlight%2520interdependencies%2520between%250Aphysical-level%2520threats%2520%2528like%2520side-channel%2520leakage%2520and%2520crosstalk%2520faults%2529%252C%2520data%250Aand%2520algorithm%2520manipulation%2520%2528such%2520as%2520poisoning%2520or%2520circuit%2520backdoors%2529%252C%2520and%250Aprivacy%2520attacks%2520%2528including%2520model%2520extraction%2520and%2520training%2520data%2520inference%2529.%2520This%250Awork%2520provides%2520a%2520foundation%2520for%2520more%2520realistic%2520threat%2520modeling%2520and%2520proactive%250Asecurity-in-depth%2520design%2520in%2520the%2520emerging%2520field%2520of%2520quantum%2520machine%2520learning.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2507.08623v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Entangled%20Threats%3A%20A%20Unified%20Kill%20Chain%20Model%20for%20Quantum%20Machine%0A%20%20Learning%20Security&entry.906535625=Pascal%20Debus%20and%20Maximilian%20Wendlinger%20and%20Kilian%20Tscharke%20and%20Daniel%20Herr%20and%20Cedric%20Br%C3%BCgmann%20and%20Daniel%20Ohl%20de%20Mello%20and%20Juris%20Ulmanis%20and%20Alexander%20Erhard%20and%20Arthur%20Schmidt%20and%20Fabian%20Petsch&entry.1292438233=%20%20Quantum%20Machine%20Learning%20%28QML%29%20systems%20inherit%20vulnerabilities%20from%20classical%0Amachine%20learning%20while%20introducing%20new%20attack%20surfaces%20rooted%20in%20the%20physical%0Aand%20algorithmic%20layers%20of%20quantum%20computing.%20Despite%20a%20growing%20body%20of%20research%0Aon%20individual%20attack%20vectors%20-%20ranging%20from%20adversarial%20poisoning%20and%20evasion%0Ato%20circuit-level%20backdoors%2C%20side-channel%20leakage%2C%20and%20model%20extraction%20-%20these%0Athreats%20are%20often%20analyzed%20in%20isolation%2C%20with%20unrealistic%20assumptions%20about%0Aattacker%20capabilities%20and%20system%20environments.%20This%20fragmentation%20hampers%20the%0Adevelopment%20of%20effective%2C%20holistic%20defense%20strategies.%20In%20this%20work%2C%20we%20argue%0Athat%20QML%20security%20requires%20more%20structured%20modeling%20of%20the%20attack%20surface%2C%0Acapturing%20not%20only%20individual%20techniques%20but%20also%20their%20relationships%2C%0Aprerequisites%2C%20and%20potential%20impact%20across%20the%20QML%20pipeline.%20We%20propose%0Aadapting%20kill%20chain%20models%2C%20widely%20used%20in%20classical%20IT%20and%20cybersecurity%2C%20to%0Athe%20quantum%20machine%20learning%20context.%20Such%20models%20allow%20for%20structured%0Areasoning%20about%20attacker%20objectives%2C%20capabilities%2C%20and%20possible%20multi-stage%0Aattack%20paths%20-%20spanning%20reconnaissance%2C%20initial%20access%2C%20manipulation%2C%0Apersistence%2C%20and%20exfiltration.%20Based%20on%20extensive%20literature%20analysis%2C%20we%0Apresent%20a%20detailed%20taxonomy%20of%20QML%20attack%20vectors%20mapped%20to%20corresponding%0Astages%20in%20a%20quantum-aware%20kill%20chain%20framework%20that%20is%20inspired%20by%20the%20MITRE%0AATLAS%20for%20classical%20machine%20learning.%20We%20highlight%20interdependencies%20between%0Aphysical-level%20threats%20%28like%20side-channel%20leakage%20and%20crosstalk%20faults%29%2C%20data%0Aand%20algorithm%20manipulation%20%28such%20as%20poisoning%20or%20circuit%20backdoors%29%2C%20and%0Aprivacy%20attacks%20%28including%20model%20extraction%20and%20training%20data%20inference%29.%20This%0Awork%20provides%20a%20foundation%20for%20more%20realistic%20threat%20modeling%20and%20proactive%0Asecurity-in-depth%20design%20in%20the%20emerging%20field%20of%20quantum%20machine%20learning.%0A&entry.1838667208=http%3A//arxiv.org/abs/2507.08623v1&entry.124074799=Read"},
{"title": "Synergistic Prompting for Robust Visual Recognition with Missing\n  Modalities", "author": "Zhihui Zhang and Luanyuan Dai and Qika Lin and Yunfeng Diao and Guangyin Jin and Yufei Guo and Jing Zhang and Xiaoshuai Hao", "abstract": "  Large-scale multi-modal models have demonstrated remarkable performance\nacross various visual recognition tasks by leveraging extensive paired\nmulti-modal training data. However, in real-world applications, the presence of\nmissing or incomplete modality inputs often leads to significant performance\ndegradation. Recent research has focused on prompt-based strategies to tackle\nthis issue; however, existing methods are hindered by two major limitations:\n(1) static prompts lack the flexibility to adapt to varying missing-data\nconditions, and (2) basic prompt-tuning methods struggle to ensure reliable\nperformance when critical modalities are missing.To address these challenges,\nwe propose a novel Synergistic Prompting (SyP) framework for robust visual\nrecognition with missing modalities. The proposed SyP introduces two key\ninnovations: (I) a Dynamic Adapter, which computes adaptive scaling factors to\ndynamically generate prompts, replacing static parameters for flexible\nmulti-modal adaptation, and (II) a Synergistic Prompting Strategy, which\ncombines static and dynamic prompts to balance information across modalities,\nensuring robust reasoning even when key modalities are missing. The proposed\nSyP achieves significant performance improvements over existing approaches\nacross three widely-used visual recognition datasets, demonstrating robustness\nunder diverse missing rates and conditions. Extensive experiments and ablation\nstudies validate its effectiveness in handling missing modalities, highlighting\nits superior adaptability and reliability.\n", "link": "http://arxiv.org/abs/2507.07802v2", "date": "2025-07-11", "relevancy": 1.712, "topK": [{"title": "Multimodal-Conditioned Latent Diffusion Models for Fashion Image Editing", "link": "http://arxiv.org/abs/2403.14828v2", "similarity": 0.5777}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5712}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.5622}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Synergistic%20Prompting%20for%20Robust%20Visual%20Recognition%20with%20Missing%0A%20%20Modalities&body=Title%3A%20Synergistic%20Prompting%20for%20Robust%20Visual%20Recognition%20with%20Missing%0A%20%20Modalities%0AAuthor%3A%20Zhihui%20Zhang%20and%20Luanyuan%20Dai%20and%20Qika%20Lin%20and%20Yunfeng%20Diao%20and%20Guangyin%20Jin%20and%20Yufei%20Guo%20and%20Jing%20Zhang%20and%20Xiaoshuai%20Hao%0AAbstract%3A%20%20%20Large-scale%20multi-modal%20models%20have%20demonstrated%20remarkable%20performance%0Aacross%20various%20visual%20recognition%20tasks%20by%20leveraging%20extensive%20paired%0Amulti-modal%20training%20data.%20However%2C%20in%20real-world%20applications%2C%20the%20presence%20of%0Amissing%20or%20incomplete%20modality%20inputs%20often%20leads%20to%20significant%20performance%0Adegradation.%20Recent%20research%20has%20focused%20on%20prompt-based%20strategies%20to%20tackle%0Athis%20issue%3B%20however%2C%20existing%20methods%20are%20hindered%20by%20two%20major%20limitations%3A%0A%281%29%20static%20prompts%20lack%20the%20flexibility%20to%20adapt%20to%20varying%20missing-data%0Aconditions%2C%20and%20%282%29%20basic%20prompt-tuning%20methods%20struggle%20to%20ensure%20reliable%0Aperformance%20when%20critical%20modalities%20are%20missing.To%20address%20these%20challenges%2C%0Awe%20propose%20a%20novel%20Synergistic%20Prompting%20%28SyP%29%20framework%20for%20robust%20visual%0Arecognition%20with%20missing%20modalities.%20The%20proposed%20SyP%20introduces%20two%20key%0Ainnovations%3A%20%28I%29%20a%20Dynamic%20Adapter%2C%20which%20computes%20adaptive%20scaling%20factors%20to%0Adynamically%20generate%20prompts%2C%20replacing%20static%20parameters%20for%20flexible%0Amulti-modal%20adaptation%2C%20and%20%28II%29%20a%20Synergistic%20Prompting%20Strategy%2C%20which%0Acombines%20static%20and%20dynamic%20prompts%20to%20balance%20information%20across%20modalities%2C%0Aensuring%20robust%20reasoning%20even%20when%20key%20modalities%20are%20missing.%20The%20proposed%0ASyP%20achieves%20significant%20performance%20improvements%20over%20existing%20approaches%0Aacross%20three%20widely-used%20visual%20recognition%20datasets%2C%20demonstrating%20robustness%0Aunder%20diverse%20missing%20rates%20and%20conditions.%20Extensive%20experiments%20and%20ablation%0Astudies%20validate%20its%20effectiveness%20in%20handling%20missing%20modalities%2C%20highlighting%0Aits%20superior%20adaptability%20and%20reliability.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2507.07802v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DSynergistic%2520Prompting%2520for%2520Robust%2520Visual%2520Recognition%2520with%2520Missing%250A%2520%2520Modalities%26entry.906535625%3DZhihui%2520Zhang%2520and%2520Luanyuan%2520Dai%2520and%2520Qika%2520Lin%2520and%2520Yunfeng%2520Diao%2520and%2520Guangyin%2520Jin%2520and%2520Yufei%2520Guo%2520and%2520Jing%2520Zhang%2520and%2520Xiaoshuai%2520Hao%26entry.1292438233%3D%2520%2520Large-scale%2520multi-modal%2520models%2520have%2520demonstrated%2520remarkable%2520performance%250Aacross%2520various%2520visual%2520recognition%2520tasks%2520by%2520leveraging%2520extensive%2520paired%250Amulti-modal%2520training%2520data.%2520However%252C%2520in%2520real-world%2520applications%252C%2520the%2520presence%2520of%250Amissing%2520or%2520incomplete%2520modality%2520inputs%2520often%2520leads%2520to%2520significant%2520performance%250Adegradation.%2520Recent%2520research%2520has%2520focused%2520on%2520prompt-based%2520strategies%2520to%2520tackle%250Athis%2520issue%253B%2520however%252C%2520existing%2520methods%2520are%2520hindered%2520by%2520two%2520major%2520limitations%253A%250A%25281%2529%2520static%2520prompts%2520lack%2520the%2520flexibility%2520to%2520adapt%2520to%2520varying%2520missing-data%250Aconditions%252C%2520and%2520%25282%2529%2520basic%2520prompt-tuning%2520methods%2520struggle%2520to%2520ensure%2520reliable%250Aperformance%2520when%2520critical%2520modalities%2520are%2520missing.To%2520address%2520these%2520challenges%252C%250Awe%2520propose%2520a%2520novel%2520Synergistic%2520Prompting%2520%2528SyP%2529%2520framework%2520for%2520robust%2520visual%250Arecognition%2520with%2520missing%2520modalities.%2520The%2520proposed%2520SyP%2520introduces%2520two%2520key%250Ainnovations%253A%2520%2528I%2529%2520a%2520Dynamic%2520Adapter%252C%2520which%2520computes%2520adaptive%2520scaling%2520factors%2520to%250Adynamically%2520generate%2520prompts%252C%2520replacing%2520static%2520parameters%2520for%2520flexible%250Amulti-modal%2520adaptation%252C%2520and%2520%2528II%2529%2520a%2520Synergistic%2520Prompting%2520Strategy%252C%2520which%250Acombines%2520static%2520and%2520dynamic%2520prompts%2520to%2520balance%2520information%2520across%2520modalities%252C%250Aensuring%2520robust%2520reasoning%2520even%2520when%2520key%2520modalities%2520are%2520missing.%2520The%2520proposed%250ASyP%2520achieves%2520significant%2520performance%2520improvements%2520over%2520existing%2520approaches%250Aacross%2520three%2520widely-used%2520visual%2520recognition%2520datasets%252C%2520demonstrating%2520robustness%250Aunder%2520diverse%2520missing%2520rates%2520and%2520conditions.%2520Extensive%2520experiments%2520and%2520ablation%250Astudies%2520validate%2520its%2520effectiveness%2520in%2520handling%2520missing%2520modalities%252C%2520highlighting%250Aits%2520superior%2520adaptability%2520and%2520reliability.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2507.07802v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Synergistic%20Prompting%20for%20Robust%20Visual%20Recognition%20with%20Missing%0A%20%20Modalities&entry.906535625=Zhihui%20Zhang%20and%20Luanyuan%20Dai%20and%20Qika%20Lin%20and%20Yunfeng%20Diao%20and%20Guangyin%20Jin%20and%20Yufei%20Guo%20and%20Jing%20Zhang%20and%20Xiaoshuai%20Hao&entry.1292438233=%20%20Large-scale%20multi-modal%20models%20have%20demonstrated%20remarkable%20performance%0Aacross%20various%20visual%20recognition%20tasks%20by%20leveraging%20extensive%20paired%0Amulti-modal%20training%20data.%20However%2C%20in%20real-world%20applications%2C%20the%20presence%20of%0Amissing%20or%20incomplete%20modality%20inputs%20often%20leads%20to%20significant%20performance%0Adegradation.%20Recent%20research%20has%20focused%20on%20prompt-based%20strategies%20to%20tackle%0Athis%20issue%3B%20however%2C%20existing%20methods%20are%20hindered%20by%20two%20major%20limitations%3A%0A%281%29%20static%20prompts%20lack%20the%20flexibility%20to%20adapt%20to%20varying%20missing-data%0Aconditions%2C%20and%20%282%29%20basic%20prompt-tuning%20methods%20struggle%20to%20ensure%20reliable%0Aperformance%20when%20critical%20modalities%20are%20missing.To%20address%20these%20challenges%2C%0Awe%20propose%20a%20novel%20Synergistic%20Prompting%20%28SyP%29%20framework%20for%20robust%20visual%0Arecognition%20with%20missing%20modalities.%20The%20proposed%20SyP%20introduces%20two%20key%0Ainnovations%3A%20%28I%29%20a%20Dynamic%20Adapter%2C%20which%20computes%20adaptive%20scaling%20factors%20to%0Adynamically%20generate%20prompts%2C%20replacing%20static%20parameters%20for%20flexible%0Amulti-modal%20adaptation%2C%20and%20%28II%29%20a%20Synergistic%20Prompting%20Strategy%2C%20which%0Acombines%20static%20and%20dynamic%20prompts%20to%20balance%20information%20across%20modalities%2C%0Aensuring%20robust%20reasoning%20even%20when%20key%20modalities%20are%20missing.%20The%20proposed%0ASyP%20achieves%20significant%20performance%20improvements%20over%20existing%20approaches%0Aacross%20three%20widely-used%20visual%20recognition%20datasets%2C%20demonstrating%20robustness%0Aunder%20diverse%20missing%20rates%20and%20conditions.%20Extensive%20experiments%20and%20ablation%0Astudies%20validate%20its%20effectiveness%20in%20handling%20missing%20modalities%2C%20highlighting%0Aits%20superior%20adaptability%20and%20reliability.%0A&entry.1838667208=http%3A//arxiv.org/abs/2507.07802v2&entry.124074799=Read"},
{"title": "CLiFT: Compressive Light-Field Tokens for Compute-Efficient and Adaptive\n  Neural Rendering", "author": "Zhengqing Wang and Yuefan Wu and Jiacheng Chen and Fuyang Zhang and Yasutaka Furukawa", "abstract": "  This paper proposes a neural rendering approach that represents a scene as\n\"compressed light-field tokens (CLiFTs)\", retaining rich appearance and\ngeometric information of a scene. CLiFT enables compute-efficient rendering by\ncompressed tokens, while being capable of changing the number of tokens to\nrepresent a scene or render a novel view with one trained network. Concretely,\ngiven a set of images, multi-view encoder tokenizes the images with the camera\nposes. Latent-space K-means selects a reduced set of rays as cluster centroids\nusing the tokens. The multi-view ``condenser'' compresses the information of\nall the tokens into the centroid tokens to construct CLiFTs. At test time,\ngiven a target view and a compute budget (i.e., the number of CLiFTs), the\nsystem collects the specified number of nearby tokens and synthesizes a novel\nview using a compute-adaptive renderer. Extensive experiments on RealEstate10K\nand DL3DV datasets quantitatively and qualitatively validate our approach,\nachieving significant data reduction with comparable rendering quality and the\nhighest overall rendering score, while providing trade-offs of data size,\nrendering quality, and rendering speed.\n", "link": "http://arxiv.org/abs/2507.08776v1", "date": "2025-07-11", "relevancy": 1.6883, "topK": [{"title": "CamCtrl3D: Single-Image Scene Exploration with Precise 3D Camera Control", "link": "http://arxiv.org/abs/2501.06006v1", "similarity": 0.5692}, {"title": "ToDo: Token Downsampling for Efficient Generation of High-Resolution\n  Images", "link": "http://arxiv.org/abs/2402.13573v2", "similarity": 0.5687}, {"title": "Thinking Outside the BBox: Unconstrained Generative Object Compositing", "link": "http://arxiv.org/abs/2409.04559v2", "similarity": 0.5416}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20CLiFT%3A%20Compressive%20Light-Field%20Tokens%20for%20Compute-Efficient%20and%20Adaptive%0A%20%20Neural%20Rendering&body=Title%3A%20CLiFT%3A%20Compressive%20Light-Field%20Tokens%20for%20Compute-Efficient%20and%20Adaptive%0A%20%20Neural%20Rendering%0AAuthor%3A%20Zhengqing%20Wang%20and%20Yuefan%20Wu%20and%20Jiacheng%20Chen%20and%20Fuyang%20Zhang%20and%20Yasutaka%20Furukawa%0AAbstract%3A%20%20%20This%20paper%20proposes%20a%20neural%20rendering%20approach%20that%20represents%20a%20scene%20as%0A%22compressed%20light-field%20tokens%20%28CLiFTs%29%22%2C%20retaining%20rich%20appearance%20and%0Ageometric%20information%20of%20a%20scene.%20CLiFT%20enables%20compute-efficient%20rendering%20by%0Acompressed%20tokens%2C%20while%20being%20capable%20of%20changing%20the%20number%20of%20tokens%20to%0Arepresent%20a%20scene%20or%20render%20a%20novel%20view%20with%20one%20trained%20network.%20Concretely%2C%0Agiven%20a%20set%20of%20images%2C%20multi-view%20encoder%20tokenizes%20the%20images%20with%20the%20camera%0Aposes.%20Latent-space%20K-means%20selects%20a%20reduced%20set%20of%20rays%20as%20cluster%20centroids%0Ausing%20the%20tokens.%20The%20multi-view%20%60%60condenser%27%27%20compresses%20the%20information%20of%0Aall%20the%20tokens%20into%20the%20centroid%20tokens%20to%20construct%20CLiFTs.%20At%20test%20time%2C%0Agiven%20a%20target%20view%20and%20a%20compute%20budget%20%28i.e.%2C%20the%20number%20of%20CLiFTs%29%2C%20the%0Asystem%20collects%20the%20specified%20number%20of%20nearby%20tokens%20and%20synthesizes%20a%20novel%0Aview%20using%20a%20compute-adaptive%20renderer.%20Extensive%20experiments%20on%20RealEstate10K%0Aand%20DL3DV%20datasets%20quantitatively%20and%20qualitatively%20validate%20our%20approach%2C%0Aachieving%20significant%20data%20reduction%20with%20comparable%20rendering%20quality%20and%20the%0Ahighest%20overall%20rendering%20score%2C%20while%20providing%20trade-offs%20of%20data%20size%2C%0Arendering%20quality%2C%20and%20rendering%20speed.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2507.08776v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DCLiFT%253A%2520Compressive%2520Light-Field%2520Tokens%2520for%2520Compute-Efficient%2520and%2520Adaptive%250A%2520%2520Neural%2520Rendering%26entry.906535625%3DZhengqing%2520Wang%2520and%2520Yuefan%2520Wu%2520and%2520Jiacheng%2520Chen%2520and%2520Fuyang%2520Zhang%2520and%2520Yasutaka%2520Furukawa%26entry.1292438233%3D%2520%2520This%2520paper%2520proposes%2520a%2520neural%2520rendering%2520approach%2520that%2520represents%2520a%2520scene%2520as%250A%2522compressed%2520light-field%2520tokens%2520%2528CLiFTs%2529%2522%252C%2520retaining%2520rich%2520appearance%2520and%250Ageometric%2520information%2520of%2520a%2520scene.%2520CLiFT%2520enables%2520compute-efficient%2520rendering%2520by%250Acompressed%2520tokens%252C%2520while%2520being%2520capable%2520of%2520changing%2520the%2520number%2520of%2520tokens%2520to%250Arepresent%2520a%2520scene%2520or%2520render%2520a%2520novel%2520view%2520with%2520one%2520trained%2520network.%2520Concretely%252C%250Agiven%2520a%2520set%2520of%2520images%252C%2520multi-view%2520encoder%2520tokenizes%2520the%2520images%2520with%2520the%2520camera%250Aposes.%2520Latent-space%2520K-means%2520selects%2520a%2520reduced%2520set%2520of%2520rays%2520as%2520cluster%2520centroids%250Ausing%2520the%2520tokens.%2520The%2520multi-view%2520%2560%2560condenser%2527%2527%2520compresses%2520the%2520information%2520of%250Aall%2520the%2520tokens%2520into%2520the%2520centroid%2520tokens%2520to%2520construct%2520CLiFTs.%2520At%2520test%2520time%252C%250Agiven%2520a%2520target%2520view%2520and%2520a%2520compute%2520budget%2520%2528i.e.%252C%2520the%2520number%2520of%2520CLiFTs%2529%252C%2520the%250Asystem%2520collects%2520the%2520specified%2520number%2520of%2520nearby%2520tokens%2520and%2520synthesizes%2520a%2520novel%250Aview%2520using%2520a%2520compute-adaptive%2520renderer.%2520Extensive%2520experiments%2520on%2520RealEstate10K%250Aand%2520DL3DV%2520datasets%2520quantitatively%2520and%2520qualitatively%2520validate%2520our%2520approach%252C%250Aachieving%2520significant%2520data%2520reduction%2520with%2520comparable%2520rendering%2520quality%2520and%2520the%250Ahighest%2520overall%2520rendering%2520score%252C%2520while%2520providing%2520trade-offs%2520of%2520data%2520size%252C%250Arendering%2520quality%252C%2520and%2520rendering%2520speed.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2507.08776v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=CLiFT%3A%20Compressive%20Light-Field%20Tokens%20for%20Compute-Efficient%20and%20Adaptive%0A%20%20Neural%20Rendering&entry.906535625=Zhengqing%20Wang%20and%20Yuefan%20Wu%20and%20Jiacheng%20Chen%20and%20Fuyang%20Zhang%20and%20Yasutaka%20Furukawa&entry.1292438233=%20%20This%20paper%20proposes%20a%20neural%20rendering%20approach%20that%20represents%20a%20scene%20as%0A%22compressed%20light-field%20tokens%20%28CLiFTs%29%22%2C%20retaining%20rich%20appearance%20and%0Ageometric%20information%20of%20a%20scene.%20CLiFT%20enables%20compute-efficient%20rendering%20by%0Acompressed%20tokens%2C%20while%20being%20capable%20of%20changing%20the%20number%20of%20tokens%20to%0Arepresent%20a%20scene%20or%20render%20a%20novel%20view%20with%20one%20trained%20network.%20Concretely%2C%0Agiven%20a%20set%20of%20images%2C%20multi-view%20encoder%20tokenizes%20the%20images%20with%20the%20camera%0Aposes.%20Latent-space%20K-means%20selects%20a%20reduced%20set%20of%20rays%20as%20cluster%20centroids%0Ausing%20the%20tokens.%20The%20multi-view%20%60%60condenser%27%27%20compresses%20the%20information%20of%0Aall%20the%20tokens%20into%20the%20centroid%20tokens%20to%20construct%20CLiFTs.%20At%20test%20time%2C%0Agiven%20a%20target%20view%20and%20a%20compute%20budget%20%28i.e.%2C%20the%20number%20of%20CLiFTs%29%2C%20the%0Asystem%20collects%20the%20specified%20number%20of%20nearby%20tokens%20and%20synthesizes%20a%20novel%0Aview%20using%20a%20compute-adaptive%20renderer.%20Extensive%20experiments%20on%20RealEstate10K%0Aand%20DL3DV%20datasets%20quantitatively%20and%20qualitatively%20validate%20our%20approach%2C%0Aachieving%20significant%20data%20reduction%20with%20comparable%20rendering%20quality%20and%20the%0Ahighest%20overall%20rendering%20score%2C%20while%20providing%20trade-offs%20of%20data%20size%2C%0Arendering%20quality%2C%20and%20rendering%20speed.%0A&entry.1838667208=http%3A//arxiv.org/abs/2507.08776v1&entry.124074799=Read"},
{"title": "Adaptive Framework for Ambient Intelligence in Rehabilitation Assistance", "author": "G\u00e1bor Baranyi and Zsolt Csibi and Kristian Fenech and \u00c1ron F\u00f3thi and Zs\u00f3fia Ga\u00e1l and Joul Skaf and Andr\u00e1s L\u0151rincz", "abstract": "  This paper introduces the Ambient Intelligence Rehabilitation Support (AIRS)\nframework, an advanced artificial intelligence-based solution tailored for home\nrehabilitation environments. AIRS integrates cutting-edge technologies,\nincluding Real-Time 3D Reconstruction (RT-3DR), intelligent navigation, and\nlarge Vision-Language Models (VLMs), to create a comprehensive system for\nmachine-guided physical rehabilitation. The general AIRS framework is\ndemonstrated in rehabilitation scenarios following total knee replacement\n(TKR), utilizing a database of 263 video recordings for evaluation. A\nsmartphone is employed within AIRS to perform RT-3DR of living spaces and has a\nbody-matched avatar to provide visual feedback about the excercise. This avatar\nis necessary in (a) optimizing exercise configurations, including camera\nplacement, patient positioning, and initial poses, and (b) addressing privacy\nconcerns and promoting compliance with the AI Act. The system guides users\nthrough the recording process to ensure the collection of properly recorded\nvideos. AIRS employs two feedback mechanisms: (i) visual 3D feedback, enabling\ndirect comparisons between prerecorded clinical exercises and patient home\nrecordings and (ii) VLM-generated feedback, providing detailed explanations and\ncorrections for exercise errors. The framework also supports people with visual\nand hearing impairments. It also features a modular design that can be adapted\nto broader rehabilitation contexts. AIRS software components are available for\nfurther use and customization.\n", "link": "http://arxiv.org/abs/2507.08624v1", "date": "2025-07-11", "relevancy": 1.6707, "topK": [{"title": "Egocentric Computer Vision for Hands-Free Robotic Wheelchair Navigation", "link": "https://link.springer.com/article/10.1007/s10846-023-01807-4", "similarity": 0.5829}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.5307}, {"title": "RoboEXP: Action-Conditioned Scene Graph via Interactive Exploration for\n  Robotic Manipulation", "link": "http://arxiv.org/abs/2402.15487v1", "similarity": 0.518}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Adaptive%20Framework%20for%20Ambient%20Intelligence%20in%20Rehabilitation%20Assistance&body=Title%3A%20Adaptive%20Framework%20for%20Ambient%20Intelligence%20in%20Rehabilitation%20Assistance%0AAuthor%3A%20G%C3%A1bor%20Baranyi%20and%20Zsolt%20Csibi%20and%20Kristian%20Fenech%20and%20%C3%81ron%20F%C3%B3thi%20and%20Zs%C3%B3fia%20Ga%C3%A1l%20and%20Joul%20Skaf%20and%20Andr%C3%A1s%20L%C5%91rincz%0AAbstract%3A%20%20%20This%20paper%20introduces%20the%20Ambient%20Intelligence%20Rehabilitation%20Support%20%28AIRS%29%0Aframework%2C%20an%20advanced%20artificial%20intelligence-based%20solution%20tailored%20for%20home%0Arehabilitation%20environments.%20AIRS%20integrates%20cutting-edge%20technologies%2C%0Aincluding%20Real-Time%203D%20Reconstruction%20%28RT-3DR%29%2C%20intelligent%20navigation%2C%20and%0Alarge%20Vision-Language%20Models%20%28VLMs%29%2C%20to%20create%20a%20comprehensive%20system%20for%0Amachine-guided%20physical%20rehabilitation.%20The%20general%20AIRS%20framework%20is%0Ademonstrated%20in%20rehabilitation%20scenarios%20following%20total%20knee%20replacement%0A%28TKR%29%2C%20utilizing%20a%20database%20of%20263%20video%20recordings%20for%20evaluation.%20A%0Asmartphone%20is%20employed%20within%20AIRS%20to%20perform%20RT-3DR%20of%20living%20spaces%20and%20has%20a%0Abody-matched%20avatar%20to%20provide%20visual%20feedback%20about%20the%20excercise.%20This%20avatar%0Ais%20necessary%20in%20%28a%29%20optimizing%20exercise%20configurations%2C%20including%20camera%0Aplacement%2C%20patient%20positioning%2C%20and%20initial%20poses%2C%20and%20%28b%29%20addressing%20privacy%0Aconcerns%20and%20promoting%20compliance%20with%20the%20AI%20Act.%20The%20system%20guides%20users%0Athrough%20the%20recording%20process%20to%20ensure%20the%20collection%20of%20properly%20recorded%0Avideos.%20AIRS%20employs%20two%20feedback%20mechanisms%3A%20%28i%29%20visual%203D%20feedback%2C%20enabling%0Adirect%20comparisons%20between%20prerecorded%20clinical%20exercises%20and%20patient%20home%0Arecordings%20and%20%28ii%29%20VLM-generated%20feedback%2C%20providing%20detailed%20explanations%20and%0Acorrections%20for%20exercise%20errors.%20The%20framework%20also%20supports%20people%20with%20visual%0Aand%20hearing%20impairments.%20It%20also%20features%20a%20modular%20design%20that%20can%20be%20adapted%0Ato%20broader%20rehabilitation%20contexts.%20AIRS%20software%20components%20are%20available%20for%0Afurther%20use%20and%20customization.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2507.08624v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DAdaptive%2520Framework%2520for%2520Ambient%2520Intelligence%2520in%2520Rehabilitation%2520Assistance%26entry.906535625%3DG%25C3%25A1bor%2520Baranyi%2520and%2520Zsolt%2520Csibi%2520and%2520Kristian%2520Fenech%2520and%2520%25C3%2581ron%2520F%25C3%25B3thi%2520and%2520Zs%25C3%25B3fia%2520Ga%25C3%25A1l%2520and%2520Joul%2520Skaf%2520and%2520Andr%25C3%25A1s%2520L%25C5%2591rincz%26entry.1292438233%3D%2520%2520This%2520paper%2520introduces%2520the%2520Ambient%2520Intelligence%2520Rehabilitation%2520Support%2520%2528AIRS%2529%250Aframework%252C%2520an%2520advanced%2520artificial%2520intelligence-based%2520solution%2520tailored%2520for%2520home%250Arehabilitation%2520environments.%2520AIRS%2520integrates%2520cutting-edge%2520technologies%252C%250Aincluding%2520Real-Time%25203D%2520Reconstruction%2520%2528RT-3DR%2529%252C%2520intelligent%2520navigation%252C%2520and%250Alarge%2520Vision-Language%2520Models%2520%2528VLMs%2529%252C%2520to%2520create%2520a%2520comprehensive%2520system%2520for%250Amachine-guided%2520physical%2520rehabilitation.%2520The%2520general%2520AIRS%2520framework%2520is%250Ademonstrated%2520in%2520rehabilitation%2520scenarios%2520following%2520total%2520knee%2520replacement%250A%2528TKR%2529%252C%2520utilizing%2520a%2520database%2520of%2520263%2520video%2520recordings%2520for%2520evaluation.%2520A%250Asmartphone%2520is%2520employed%2520within%2520AIRS%2520to%2520perform%2520RT-3DR%2520of%2520living%2520spaces%2520and%2520has%2520a%250Abody-matched%2520avatar%2520to%2520provide%2520visual%2520feedback%2520about%2520the%2520excercise.%2520This%2520avatar%250Ais%2520necessary%2520in%2520%2528a%2529%2520optimizing%2520exercise%2520configurations%252C%2520including%2520camera%250Aplacement%252C%2520patient%2520positioning%252C%2520and%2520initial%2520poses%252C%2520and%2520%2528b%2529%2520addressing%2520privacy%250Aconcerns%2520and%2520promoting%2520compliance%2520with%2520the%2520AI%2520Act.%2520The%2520system%2520guides%2520users%250Athrough%2520the%2520recording%2520process%2520to%2520ensure%2520the%2520collection%2520of%2520properly%2520recorded%250Avideos.%2520AIRS%2520employs%2520two%2520feedback%2520mechanisms%253A%2520%2528i%2529%2520visual%25203D%2520feedback%252C%2520enabling%250Adirect%2520comparisons%2520between%2520prerecorded%2520clinical%2520exercises%2520and%2520patient%2520home%250Arecordings%2520and%2520%2528ii%2529%2520VLM-generated%2520feedback%252C%2520providing%2520detailed%2520explanations%2520and%250Acorrections%2520for%2520exercise%2520errors.%2520The%2520framework%2520also%2520supports%2520people%2520with%2520visual%250Aand%2520hearing%2520impairments.%2520It%2520also%2520features%2520a%2520modular%2520design%2520that%2520can%2520be%2520adapted%250Ato%2520broader%2520rehabilitation%2520contexts.%2520AIRS%2520software%2520components%2520are%2520available%2520for%250Afurther%2520use%2520and%2520customization.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2507.08624v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Adaptive%20Framework%20for%20Ambient%20Intelligence%20in%20Rehabilitation%20Assistance&entry.906535625=G%C3%A1bor%20Baranyi%20and%20Zsolt%20Csibi%20and%20Kristian%20Fenech%20and%20%C3%81ron%20F%C3%B3thi%20and%20Zs%C3%B3fia%20Ga%C3%A1l%20and%20Joul%20Skaf%20and%20Andr%C3%A1s%20L%C5%91rincz&entry.1292438233=%20%20This%20paper%20introduces%20the%20Ambient%20Intelligence%20Rehabilitation%20Support%20%28AIRS%29%0Aframework%2C%20an%20advanced%20artificial%20intelligence-based%20solution%20tailored%20for%20home%0Arehabilitation%20environments.%20AIRS%20integrates%20cutting-edge%20technologies%2C%0Aincluding%20Real-Time%203D%20Reconstruction%20%28RT-3DR%29%2C%20intelligent%20navigation%2C%20and%0Alarge%20Vision-Language%20Models%20%28VLMs%29%2C%20to%20create%20a%20comprehensive%20system%20for%0Amachine-guided%20physical%20rehabilitation.%20The%20general%20AIRS%20framework%20is%0Ademonstrated%20in%20rehabilitation%20scenarios%20following%20total%20knee%20replacement%0A%28TKR%29%2C%20utilizing%20a%20database%20of%20263%20video%20recordings%20for%20evaluation.%20A%0Asmartphone%20is%20employed%20within%20AIRS%20to%20perform%20RT-3DR%20of%20living%20spaces%20and%20has%20a%0Abody-matched%20avatar%20to%20provide%20visual%20feedback%20about%20the%20excercise.%20This%20avatar%0Ais%20necessary%20in%20%28a%29%20optimizing%20exercise%20configurations%2C%20including%20camera%0Aplacement%2C%20patient%20positioning%2C%20and%20initial%20poses%2C%20and%20%28b%29%20addressing%20privacy%0Aconcerns%20and%20promoting%20compliance%20with%20the%20AI%20Act.%20The%20system%20guides%20users%0Athrough%20the%20recording%20process%20to%20ensure%20the%20collection%20of%20properly%20recorded%0Avideos.%20AIRS%20employs%20two%20feedback%20mechanisms%3A%20%28i%29%20visual%203D%20feedback%2C%20enabling%0Adirect%20comparisons%20between%20prerecorded%20clinical%20exercises%20and%20patient%20home%0Arecordings%20and%20%28ii%29%20VLM-generated%20feedback%2C%20providing%20detailed%20explanations%20and%0Acorrections%20for%20exercise%20errors.%20The%20framework%20also%20supports%20people%20with%20visual%0Aand%20hearing%20impairments.%20It%20also%20features%20a%20modular%20design%20that%20can%20be%20adapted%0Ato%20broader%20rehabilitation%20contexts.%20AIRS%20software%20components%20are%20available%20for%0Afurther%20use%20and%20customization.%0A&entry.1838667208=http%3A//arxiv.org/abs/2507.08624v1&entry.124074799=Read"},
{"title": "Modeling Partially Observed Nonlinear Dynamical Systems and Efficient\n  Data Assimilation via Discrete-Time Conditional Gaussian Koopman Network", "author": "Chuanqi Chen and Zhongrui Wang and Nan Chen and Jin-Long Wu", "abstract": "  A discrete-time conditional Gaussian Koopman network (CGKN) is developed in\nthis work to learn surrogate models that can perform efficient state forecast\nand data assimilation (DA) for high-dimensional complex dynamical systems,\ne.g., systems governed by nonlinear partial differential equations (PDEs).\nFocusing on nonlinear partially observed systems that are common in many\nengineering and earth science applications, this work exploits Koopman\nembedding to discover a proper latent representation of the unobserved system\nstates, such that the dynamics of the latent states are conditional linear,\ni.e., linear with the given observed system states. The modeled system of the\nobserved and latent states then becomes a conditional Gaussian system, for\nwhich the posterior distribution of the latent states is Gaussian and can be\nefficiently evaluated via analytical formulae. The analytical formulae of DA\nfacilitate the incorporation of DA performance into the learning process of the\nmodeled system, which leads to a framework that unifies scientific machine\nlearning (SciML) and data assimilation. The performance of discrete-time CGKN\nis demonstrated on several canonical problems governed by nonlinear PDEs with\nintermittency and turbulent features, including the viscous Burgers' equation,\nthe Kuramoto-Sivashinsky equation, and the 2-D Navier-Stokes equations, with\nwhich we show that the discrete-time CGKN framework achieves comparable\nperformance as the state-of-the-art SciML methods in state forecast and\nprovides efficient and accurate DA results. The discrete-time CGKN framework\nalso serves as an example to illustrate unifying the development of SciML\nmodels and their other outer-loop applications such as design optimization,\ninverse problems, and optimal control.\n", "link": "http://arxiv.org/abs/2507.08749v1", "date": "2025-07-11", "relevancy": 1.6543, "topK": [{"title": "Dynamic Gaussians Mesh: Consistent Mesh Reconstruction from Monocular\n  Videos", "link": "http://arxiv.org/abs/2404.12379v2", "similarity": 0.5646}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.5463}, {"title": "SDXS: Real-Time One-Step Latent Diffusion Models with Image Conditions", "link": "http://arxiv.org/abs/2403.16627v1", "similarity": 0.5235}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Modeling%20Partially%20Observed%20Nonlinear%20Dynamical%20Systems%20and%20Efficient%0A%20%20Data%20Assimilation%20via%20Discrete-Time%20Conditional%20Gaussian%20Koopman%20Network&body=Title%3A%20Modeling%20Partially%20Observed%20Nonlinear%20Dynamical%20Systems%20and%20Efficient%0A%20%20Data%20Assimilation%20via%20Discrete-Time%20Conditional%20Gaussian%20Koopman%20Network%0AAuthor%3A%20Chuanqi%20Chen%20and%20Zhongrui%20Wang%20and%20Nan%20Chen%20and%20Jin-Long%20Wu%0AAbstract%3A%20%20%20A%20discrete-time%20conditional%20Gaussian%20Koopman%20network%20%28CGKN%29%20is%20developed%20in%0Athis%20work%20to%20learn%20surrogate%20models%20that%20can%20perform%20efficient%20state%20forecast%0Aand%20data%20assimilation%20%28DA%29%20for%20high-dimensional%20complex%20dynamical%20systems%2C%0Ae.g.%2C%20systems%20governed%20by%20nonlinear%20partial%20differential%20equations%20%28PDEs%29.%0AFocusing%20on%20nonlinear%20partially%20observed%20systems%20that%20are%20common%20in%20many%0Aengineering%20and%20earth%20science%20applications%2C%20this%20work%20exploits%20Koopman%0Aembedding%20to%20discover%20a%20proper%20latent%20representation%20of%20the%20unobserved%20system%0Astates%2C%20such%20that%20the%20dynamics%20of%20the%20latent%20states%20are%20conditional%20linear%2C%0Ai.e.%2C%20linear%20with%20the%20given%20observed%20system%20states.%20The%20modeled%20system%20of%20the%0Aobserved%20and%20latent%20states%20then%20becomes%20a%20conditional%20Gaussian%20system%2C%20for%0Awhich%20the%20posterior%20distribution%20of%20the%20latent%20states%20is%20Gaussian%20and%20can%20be%0Aefficiently%20evaluated%20via%20analytical%20formulae.%20The%20analytical%20formulae%20of%20DA%0Afacilitate%20the%20incorporation%20of%20DA%20performance%20into%20the%20learning%20process%20of%20the%0Amodeled%20system%2C%20which%20leads%20to%20a%20framework%20that%20unifies%20scientific%20machine%0Alearning%20%28SciML%29%20and%20data%20assimilation.%20The%20performance%20of%20discrete-time%20CGKN%0Ais%20demonstrated%20on%20several%20canonical%20problems%20governed%20by%20nonlinear%20PDEs%20with%0Aintermittency%20and%20turbulent%20features%2C%20including%20the%20viscous%20Burgers%27%20equation%2C%0Athe%20Kuramoto-Sivashinsky%20equation%2C%20and%20the%202-D%20Navier-Stokes%20equations%2C%20with%0Awhich%20we%20show%20that%20the%20discrete-time%20CGKN%20framework%20achieves%20comparable%0Aperformance%20as%20the%20state-of-the-art%20SciML%20methods%20in%20state%20forecast%20and%0Aprovides%20efficient%20and%20accurate%20DA%20results.%20The%20discrete-time%20CGKN%20framework%0Aalso%20serves%20as%20an%20example%20to%20illustrate%20unifying%20the%20development%20of%20SciML%0Amodels%20and%20their%20other%20outer-loop%20applications%20such%20as%20design%20optimization%2C%0Ainverse%20problems%2C%20and%20optimal%20control.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2507.08749v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DModeling%2520Partially%2520Observed%2520Nonlinear%2520Dynamical%2520Systems%2520and%2520Efficient%250A%2520%2520Data%2520Assimilation%2520via%2520Discrete-Time%2520Conditional%2520Gaussian%2520Koopman%2520Network%26entry.906535625%3DChuanqi%2520Chen%2520and%2520Zhongrui%2520Wang%2520and%2520Nan%2520Chen%2520and%2520Jin-Long%2520Wu%26entry.1292438233%3D%2520%2520A%2520discrete-time%2520conditional%2520Gaussian%2520Koopman%2520network%2520%2528CGKN%2529%2520is%2520developed%2520in%250Athis%2520work%2520to%2520learn%2520surrogate%2520models%2520that%2520can%2520perform%2520efficient%2520state%2520forecast%250Aand%2520data%2520assimilation%2520%2528DA%2529%2520for%2520high-dimensional%2520complex%2520dynamical%2520systems%252C%250Ae.g.%252C%2520systems%2520governed%2520by%2520nonlinear%2520partial%2520differential%2520equations%2520%2528PDEs%2529.%250AFocusing%2520on%2520nonlinear%2520partially%2520observed%2520systems%2520that%2520are%2520common%2520in%2520many%250Aengineering%2520and%2520earth%2520science%2520applications%252C%2520this%2520work%2520exploits%2520Koopman%250Aembedding%2520to%2520discover%2520a%2520proper%2520latent%2520representation%2520of%2520the%2520unobserved%2520system%250Astates%252C%2520such%2520that%2520the%2520dynamics%2520of%2520the%2520latent%2520states%2520are%2520conditional%2520linear%252C%250Ai.e.%252C%2520linear%2520with%2520the%2520given%2520observed%2520system%2520states.%2520The%2520modeled%2520system%2520of%2520the%250Aobserved%2520and%2520latent%2520states%2520then%2520becomes%2520a%2520conditional%2520Gaussian%2520system%252C%2520for%250Awhich%2520the%2520posterior%2520distribution%2520of%2520the%2520latent%2520states%2520is%2520Gaussian%2520and%2520can%2520be%250Aefficiently%2520evaluated%2520via%2520analytical%2520formulae.%2520The%2520analytical%2520formulae%2520of%2520DA%250Afacilitate%2520the%2520incorporation%2520of%2520DA%2520performance%2520into%2520the%2520learning%2520process%2520of%2520the%250Amodeled%2520system%252C%2520which%2520leads%2520to%2520a%2520framework%2520that%2520unifies%2520scientific%2520machine%250Alearning%2520%2528SciML%2529%2520and%2520data%2520assimilation.%2520The%2520performance%2520of%2520discrete-time%2520CGKN%250Ais%2520demonstrated%2520on%2520several%2520canonical%2520problems%2520governed%2520by%2520nonlinear%2520PDEs%2520with%250Aintermittency%2520and%2520turbulent%2520features%252C%2520including%2520the%2520viscous%2520Burgers%2527%2520equation%252C%250Athe%2520Kuramoto-Sivashinsky%2520equation%252C%2520and%2520the%25202-D%2520Navier-Stokes%2520equations%252C%2520with%250Awhich%2520we%2520show%2520that%2520the%2520discrete-time%2520CGKN%2520framework%2520achieves%2520comparable%250Aperformance%2520as%2520the%2520state-of-the-art%2520SciML%2520methods%2520in%2520state%2520forecast%2520and%250Aprovides%2520efficient%2520and%2520accurate%2520DA%2520results.%2520The%2520discrete-time%2520CGKN%2520framework%250Aalso%2520serves%2520as%2520an%2520example%2520to%2520illustrate%2520unifying%2520the%2520development%2520of%2520SciML%250Amodels%2520and%2520their%2520other%2520outer-loop%2520applications%2520such%2520as%2520design%2520optimization%252C%250Ainverse%2520problems%252C%2520and%2520optimal%2520control.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2507.08749v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Modeling%20Partially%20Observed%20Nonlinear%20Dynamical%20Systems%20and%20Efficient%0A%20%20Data%20Assimilation%20via%20Discrete-Time%20Conditional%20Gaussian%20Koopman%20Network&entry.906535625=Chuanqi%20Chen%20and%20Zhongrui%20Wang%20and%20Nan%20Chen%20and%20Jin-Long%20Wu&entry.1292438233=%20%20A%20discrete-time%20conditional%20Gaussian%20Koopman%20network%20%28CGKN%29%20is%20developed%20in%0Athis%20work%20to%20learn%20surrogate%20models%20that%20can%20perform%20efficient%20state%20forecast%0Aand%20data%20assimilation%20%28DA%29%20for%20high-dimensional%20complex%20dynamical%20systems%2C%0Ae.g.%2C%20systems%20governed%20by%20nonlinear%20partial%20differential%20equations%20%28PDEs%29.%0AFocusing%20on%20nonlinear%20partially%20observed%20systems%20that%20are%20common%20in%20many%0Aengineering%20and%20earth%20science%20applications%2C%20this%20work%20exploits%20Koopman%0Aembedding%20to%20discover%20a%20proper%20latent%20representation%20of%20the%20unobserved%20system%0Astates%2C%20such%20that%20the%20dynamics%20of%20the%20latent%20states%20are%20conditional%20linear%2C%0Ai.e.%2C%20linear%20with%20the%20given%20observed%20system%20states.%20The%20modeled%20system%20of%20the%0Aobserved%20and%20latent%20states%20then%20becomes%20a%20conditional%20Gaussian%20system%2C%20for%0Awhich%20the%20posterior%20distribution%20of%20the%20latent%20states%20is%20Gaussian%20and%20can%20be%0Aefficiently%20evaluated%20via%20analytical%20formulae.%20The%20analytical%20formulae%20of%20DA%0Afacilitate%20the%20incorporation%20of%20DA%20performance%20into%20the%20learning%20process%20of%20the%0Amodeled%20system%2C%20which%20leads%20to%20a%20framework%20that%20unifies%20scientific%20machine%0Alearning%20%28SciML%29%20and%20data%20assimilation.%20The%20performance%20of%20discrete-time%20CGKN%0Ais%20demonstrated%20on%20several%20canonical%20problems%20governed%20by%20nonlinear%20PDEs%20with%0Aintermittency%20and%20turbulent%20features%2C%20including%20the%20viscous%20Burgers%27%20equation%2C%0Athe%20Kuramoto-Sivashinsky%20equation%2C%20and%20the%202-D%20Navier-Stokes%20equations%2C%20with%0Awhich%20we%20show%20that%20the%20discrete-time%20CGKN%20framework%20achieves%20comparable%0Aperformance%20as%20the%20state-of-the-art%20SciML%20methods%20in%20state%20forecast%20and%0Aprovides%20efficient%20and%20accurate%20DA%20results.%20The%20discrete-time%20CGKN%20framework%0Aalso%20serves%20as%20an%20example%20to%20illustrate%20unifying%20the%20development%20of%20SciML%0Amodels%20and%20their%20other%20outer-loop%20applications%20such%20as%20design%20optimization%2C%0Ainverse%20problems%2C%20and%20optimal%20control.%0A&entry.1838667208=http%3A//arxiv.org/abs/2507.08749v1&entry.124074799=Read"},
{"title": "Open Source Planning & Control System with Language Agents for\n  Autonomous Scientific Discovery", "author": "Licong Xu and Milind Sarkar and Anto I. Lonappan and \u00cd\u00f1igo Zubeldia and Pablo Villanueva-Domingo and Santiago Casas and Christian Fidler and Chetana Amancharla and Ujjwal Tiwari and Adrian Bayer and Chadi Ait Ekioui and Miles Cranmer and Adrian Dimitrov and James Fergusson and Kahaan Gandhi and Sven Krippendorf and Andrew Laverick and Julien Lesgourgues and Antony Lewis and Thomas Meier and Blake Sherwin and Kristen Surrao and Francisco Villaescusa-Navarro and Chi Wang and Xueqing Xu and Boris Bolliet", "abstract": "  We present a multi-agent system for automation of scientific research tasks,\ncmbagent (https://github.com/CMBAgents/cmbagent). The system is formed by about\n30 Large Language Model (LLM) agents and implements a Planning & Control\nstrategy to orchestrate the agentic workflow, with no human-in-the-loop at any\npoint. Each agent specializes in a different task (performing retrieval on\nscientific papers and codebases, writing code, interpreting results, critiquing\nthe output of other agents) and the system is able to execute code locally. We\nsuccessfully apply cmbagent to carry out a PhD level cosmology task (the\nmeasurement of cosmological parameters using supernova data) and evaluate its\nperformance on two benchmark sets, finding superior performance over\nstate-of-the-art LLMs. The source code is available on GitHub, demonstration\nvideos are also available, and the system is deployed on HuggingFace and will\nbe available on the cloud.\n", "link": "http://arxiv.org/abs/2507.07257v2", "date": "2025-07-11", "relevancy": 1.6356, "topK": [{"title": "RoboEXP: Action-Conditioned Scene Graph via Interactive Exploration for\n  Robotic Manipulation", "link": "http://arxiv.org/abs/2402.15487v1", "similarity": 0.5702}, {"title": "Safe and efficient training of a control agent", "link": "https://patents.google.com/patent/US11709462B2/en", "similarity": 0.5397}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.534}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Open%20Source%20Planning%20%26%20Control%20System%20with%20Language%20Agents%20for%0A%20%20Autonomous%20Scientific%20Discovery&body=Title%3A%20Open%20Source%20Planning%20%26%20Control%20System%20with%20Language%20Agents%20for%0A%20%20Autonomous%20Scientific%20Discovery%0AAuthor%3A%20Licong%20Xu%20and%20Milind%20Sarkar%20and%20Anto%20I.%20Lonappan%20and%20%C3%8D%C3%B1igo%20Zubeldia%20and%20Pablo%20Villanueva-Domingo%20and%20Santiago%20Casas%20and%20Christian%20Fidler%20and%20Chetana%20Amancharla%20and%20Ujjwal%20Tiwari%20and%20Adrian%20Bayer%20and%20Chadi%20Ait%20Ekioui%20and%20Miles%20Cranmer%20and%20Adrian%20Dimitrov%20and%20James%20Fergusson%20and%20Kahaan%20Gandhi%20and%20Sven%20Krippendorf%20and%20Andrew%20Laverick%20and%20Julien%20Lesgourgues%20and%20Antony%20Lewis%20and%20Thomas%20Meier%20and%20Blake%20Sherwin%20and%20Kristen%20Surrao%20and%20Francisco%20Villaescusa-Navarro%20and%20Chi%20Wang%20and%20Xueqing%20Xu%20and%20Boris%20Bolliet%0AAbstract%3A%20%20%20We%20present%20a%20multi-agent%20system%20for%20automation%20of%20scientific%20research%20tasks%2C%0Acmbagent%20%28https%3A//github.com/CMBAgents/cmbagent%29.%20The%20system%20is%20formed%20by%20about%0A30%20Large%20Language%20Model%20%28LLM%29%20agents%20and%20implements%20a%20Planning%20%26%20Control%0Astrategy%20to%20orchestrate%20the%20agentic%20workflow%2C%20with%20no%20human-in-the-loop%20at%20any%0Apoint.%20Each%20agent%20specializes%20in%20a%20different%20task%20%28performing%20retrieval%20on%0Ascientific%20papers%20and%20codebases%2C%20writing%20code%2C%20interpreting%20results%2C%20critiquing%0Athe%20output%20of%20other%20agents%29%20and%20the%20system%20is%20able%20to%20execute%20code%20locally.%20We%0Asuccessfully%20apply%20cmbagent%20to%20carry%20out%20a%20PhD%20level%20cosmology%20task%20%28the%0Ameasurement%20of%20cosmological%20parameters%20using%20supernova%20data%29%20and%20evaluate%20its%0Aperformance%20on%20two%20benchmark%20sets%2C%20finding%20superior%20performance%20over%0Astate-of-the-art%20LLMs.%20The%20source%20code%20is%20available%20on%20GitHub%2C%20demonstration%0Avideos%20are%20also%20available%2C%20and%20the%20system%20is%20deployed%20on%20HuggingFace%20and%20will%0Abe%20available%20on%20the%20cloud.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2507.07257v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DOpen%2520Source%2520Planning%2520%2526%2520Control%2520System%2520with%2520Language%2520Agents%2520for%250A%2520%2520Autonomous%2520Scientific%2520Discovery%26entry.906535625%3DLicong%2520Xu%2520and%2520Milind%2520Sarkar%2520and%2520Anto%2520I.%2520Lonappan%2520and%2520%25C3%258D%25C3%25B1igo%2520Zubeldia%2520and%2520Pablo%2520Villanueva-Domingo%2520and%2520Santiago%2520Casas%2520and%2520Christian%2520Fidler%2520and%2520Chetana%2520Amancharla%2520and%2520Ujjwal%2520Tiwari%2520and%2520Adrian%2520Bayer%2520and%2520Chadi%2520Ait%2520Ekioui%2520and%2520Miles%2520Cranmer%2520and%2520Adrian%2520Dimitrov%2520and%2520James%2520Fergusson%2520and%2520Kahaan%2520Gandhi%2520and%2520Sven%2520Krippendorf%2520and%2520Andrew%2520Laverick%2520and%2520Julien%2520Lesgourgues%2520and%2520Antony%2520Lewis%2520and%2520Thomas%2520Meier%2520and%2520Blake%2520Sherwin%2520and%2520Kristen%2520Surrao%2520and%2520Francisco%2520Villaescusa-Navarro%2520and%2520Chi%2520Wang%2520and%2520Xueqing%2520Xu%2520and%2520Boris%2520Bolliet%26entry.1292438233%3D%2520%2520We%2520present%2520a%2520multi-agent%2520system%2520for%2520automation%2520of%2520scientific%2520research%2520tasks%252C%250Acmbagent%2520%2528https%253A//github.com/CMBAgents/cmbagent%2529.%2520The%2520system%2520is%2520formed%2520by%2520about%250A30%2520Large%2520Language%2520Model%2520%2528LLM%2529%2520agents%2520and%2520implements%2520a%2520Planning%2520%2526%2520Control%250Astrategy%2520to%2520orchestrate%2520the%2520agentic%2520workflow%252C%2520with%2520no%2520human-in-the-loop%2520at%2520any%250Apoint.%2520Each%2520agent%2520specializes%2520in%2520a%2520different%2520task%2520%2528performing%2520retrieval%2520on%250Ascientific%2520papers%2520and%2520codebases%252C%2520writing%2520code%252C%2520interpreting%2520results%252C%2520critiquing%250Athe%2520output%2520of%2520other%2520agents%2529%2520and%2520the%2520system%2520is%2520able%2520to%2520execute%2520code%2520locally.%2520We%250Asuccessfully%2520apply%2520cmbagent%2520to%2520carry%2520out%2520a%2520PhD%2520level%2520cosmology%2520task%2520%2528the%250Ameasurement%2520of%2520cosmological%2520parameters%2520using%2520supernova%2520data%2529%2520and%2520evaluate%2520its%250Aperformance%2520on%2520two%2520benchmark%2520sets%252C%2520finding%2520superior%2520performance%2520over%250Astate-of-the-art%2520LLMs.%2520The%2520source%2520code%2520is%2520available%2520on%2520GitHub%252C%2520demonstration%250Avideos%2520are%2520also%2520available%252C%2520and%2520the%2520system%2520is%2520deployed%2520on%2520HuggingFace%2520and%2520will%250Abe%2520available%2520on%2520the%2520cloud.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2507.07257v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Open%20Source%20Planning%20%26%20Control%20System%20with%20Language%20Agents%20for%0A%20%20Autonomous%20Scientific%20Discovery&entry.906535625=Licong%20Xu%20and%20Milind%20Sarkar%20and%20Anto%20I.%20Lonappan%20and%20%C3%8D%C3%B1igo%20Zubeldia%20and%20Pablo%20Villanueva-Domingo%20and%20Santiago%20Casas%20and%20Christian%20Fidler%20and%20Chetana%20Amancharla%20and%20Ujjwal%20Tiwari%20and%20Adrian%20Bayer%20and%20Chadi%20Ait%20Ekioui%20and%20Miles%20Cranmer%20and%20Adrian%20Dimitrov%20and%20James%20Fergusson%20and%20Kahaan%20Gandhi%20and%20Sven%20Krippendorf%20and%20Andrew%20Laverick%20and%20Julien%20Lesgourgues%20and%20Antony%20Lewis%20and%20Thomas%20Meier%20and%20Blake%20Sherwin%20and%20Kristen%20Surrao%20and%20Francisco%20Villaescusa-Navarro%20and%20Chi%20Wang%20and%20Xueqing%20Xu%20and%20Boris%20Bolliet&entry.1292438233=%20%20We%20present%20a%20multi-agent%20system%20for%20automation%20of%20scientific%20research%20tasks%2C%0Acmbagent%20%28https%3A//github.com/CMBAgents/cmbagent%29.%20The%20system%20is%20formed%20by%20about%0A30%20Large%20Language%20Model%20%28LLM%29%20agents%20and%20implements%20a%20Planning%20%26%20Control%0Astrategy%20to%20orchestrate%20the%20agentic%20workflow%2C%20with%20no%20human-in-the-loop%20at%20any%0Apoint.%20Each%20agent%20specializes%20in%20a%20different%20task%20%28performing%20retrieval%20on%0Ascientific%20papers%20and%20codebases%2C%20writing%20code%2C%20interpreting%20results%2C%20critiquing%0Athe%20output%20of%20other%20agents%29%20and%20the%20system%20is%20able%20to%20execute%20code%20locally.%20We%0Asuccessfully%20apply%20cmbagent%20to%20carry%20out%20a%20PhD%20level%20cosmology%20task%20%28the%0Ameasurement%20of%20cosmological%20parameters%20using%20supernova%20data%29%20and%20evaluate%20its%0Aperformance%20on%20two%20benchmark%20sets%2C%20finding%20superior%20performance%20over%0Astate-of-the-art%20LLMs.%20The%20source%20code%20is%20available%20on%20GitHub%2C%20demonstration%0Avideos%20are%20also%20available%2C%20and%20the%20system%20is%20deployed%20on%20HuggingFace%20and%20will%0Abe%20available%20on%20the%20cloud.%0A&entry.1838667208=http%3A//arxiv.org/abs/2507.07257v2&entry.124074799=Read"},
{"title": "The Value of Prediction in Identifying the Worst-Off", "author": "Unai Fischer-Abaigar and Christoph Kern and Juan Carlos Perdomo", "abstract": "  Machine learning is increasingly used in government programs to identify and\nsupport the most vulnerable individuals, prioritizing assistance for those at\ngreatest risk over optimizing aggregate outcomes. This paper examines the\nwelfare impacts of prediction in equity-driven contexts, and how they compare\nto other policy levers, such as expanding bureaucratic capacity. Through\nmathematical models and a real-world case study on long-term unemployment\namongst German residents, we develop a comprehensive understanding of the\nrelative effectiveness of prediction in surfacing the worst-off. Our findings\nprovide clear analytical frameworks and practical, data-driven tools that\nempower policymakers to make principled decisions when designing these systems.\n", "link": "http://arxiv.org/abs/2501.19334v3", "date": "2025-07-11", "relevancy": 1.6288, "topK": [{"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.4524}, {"title": "Flexible Visual Recognition by Evidential Modeling of Confusion and Ignorance", "link": "http://openaccess.thecvf.com/content/ICCV2023/html/Fan_Flexible_Visual_Recognition_by_Evidential_Modeling_of_Confusion_and_Ignorance_ICCV_2023_paper.html", "similarity": 0.3985}, {"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.3979}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20The%20Value%20of%20Prediction%20in%20Identifying%20the%20Worst-Off&body=Title%3A%20The%20Value%20of%20Prediction%20in%20Identifying%20the%20Worst-Off%0AAuthor%3A%20Unai%20Fischer-Abaigar%20and%20Christoph%20Kern%20and%20Juan%20Carlos%20Perdomo%0AAbstract%3A%20%20%20Machine%20learning%20is%20increasingly%20used%20in%20government%20programs%20to%20identify%20and%0Asupport%20the%20most%20vulnerable%20individuals%2C%20prioritizing%20assistance%20for%20those%20at%0Agreatest%20risk%20over%20optimizing%20aggregate%20outcomes.%20This%20paper%20examines%20the%0Awelfare%20impacts%20of%20prediction%20in%20equity-driven%20contexts%2C%20and%20how%20they%20compare%0Ato%20other%20policy%20levers%2C%20such%20as%20expanding%20bureaucratic%20capacity.%20Through%0Amathematical%20models%20and%20a%20real-world%20case%20study%20on%20long-term%20unemployment%0Aamongst%20German%20residents%2C%20we%20develop%20a%20comprehensive%20understanding%20of%20the%0Arelative%20effectiveness%20of%20prediction%20in%20surfacing%20the%20worst-off.%20Our%20findings%0Aprovide%20clear%20analytical%20frameworks%20and%20practical%2C%20data-driven%20tools%20that%0Aempower%20policymakers%20to%20make%20principled%20decisions%20when%20designing%20these%20systems.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2501.19334v3%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DThe%2520Value%2520of%2520Prediction%2520in%2520Identifying%2520the%2520Worst-Off%26entry.906535625%3DUnai%2520Fischer-Abaigar%2520and%2520Christoph%2520Kern%2520and%2520Juan%2520Carlos%2520Perdomo%26entry.1292438233%3D%2520%2520Machine%2520learning%2520is%2520increasingly%2520used%2520in%2520government%2520programs%2520to%2520identify%2520and%250Asupport%2520the%2520most%2520vulnerable%2520individuals%252C%2520prioritizing%2520assistance%2520for%2520those%2520at%250Agreatest%2520risk%2520over%2520optimizing%2520aggregate%2520outcomes.%2520This%2520paper%2520examines%2520the%250Awelfare%2520impacts%2520of%2520prediction%2520in%2520equity-driven%2520contexts%252C%2520and%2520how%2520they%2520compare%250Ato%2520other%2520policy%2520levers%252C%2520such%2520as%2520expanding%2520bureaucratic%2520capacity.%2520Through%250Amathematical%2520models%2520and%2520a%2520real-world%2520case%2520study%2520on%2520long-term%2520unemployment%250Aamongst%2520German%2520residents%252C%2520we%2520develop%2520a%2520comprehensive%2520understanding%2520of%2520the%250Arelative%2520effectiveness%2520of%2520prediction%2520in%2520surfacing%2520the%2520worst-off.%2520Our%2520findings%250Aprovide%2520clear%2520analytical%2520frameworks%2520and%2520practical%252C%2520data-driven%2520tools%2520that%250Aempower%2520policymakers%2520to%2520make%2520principled%2520decisions%2520when%2520designing%2520these%2520systems.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2501.19334v3%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=The%20Value%20of%20Prediction%20in%20Identifying%20the%20Worst-Off&entry.906535625=Unai%20Fischer-Abaigar%20and%20Christoph%20Kern%20and%20Juan%20Carlos%20Perdomo&entry.1292438233=%20%20Machine%20learning%20is%20increasingly%20used%20in%20government%20programs%20to%20identify%20and%0Asupport%20the%20most%20vulnerable%20individuals%2C%20prioritizing%20assistance%20for%20those%20at%0Agreatest%20risk%20over%20optimizing%20aggregate%20outcomes.%20This%20paper%20examines%20the%0Awelfare%20impacts%20of%20prediction%20in%20equity-driven%20contexts%2C%20and%20how%20they%20compare%0Ato%20other%20policy%20levers%2C%20such%20as%20expanding%20bureaucratic%20capacity.%20Through%0Amathematical%20models%20and%20a%20real-world%20case%20study%20on%20long-term%20unemployment%0Aamongst%20German%20residents%2C%20we%20develop%20a%20comprehensive%20understanding%20of%20the%0Arelative%20effectiveness%20of%20prediction%20in%20surfacing%20the%20worst-off.%20Our%20findings%0Aprovide%20clear%20analytical%20frameworks%20and%20practical%2C%20data-driven%20tools%20that%0Aempower%20policymakers%20to%20make%20principled%20decisions%20when%20designing%20these%20systems.%0A&entry.1838667208=http%3A//arxiv.org/abs/2501.19334v3&entry.124074799=Read"},
{"title": "Local Flow Matching Generative Models", "author": "Chen Xu and Xiuyuan Cheng and Yao Xie", "abstract": "  Flow Matching (FM) is a simulation-free method for learning a continuous and\ninvertible flow to interpolate between two distributions, and in particular to\ngenerate data from noise. Inspired by the variational nature of the diffusion\nprocess as a gradient flow, we introduce a stepwise FM model called Local Flow\nMatching (LFM), which consecutively learns a sequence of FM sub-models, each\nmatching a diffusion process up to the time of the step size in the\ndata-to-noise direction. In each step, the two distributions to be interpolated\nby the sub-flow model are closer to each other than data vs. noise, and this\nenables the use of smaller models with faster training. This variational\nperspective also allows us to theoretically prove a generation guarantee of the\nproposed flow model in terms of the $\\chi^2$-divergence between the generated\nand true data distributions, utilizing the contraction property of the\ndiffusion process. In practice, the stepwise structure of LFM is natural to be\ndistilled and different distillation techniques can be adopted to speed up\ngeneration. We empirically demonstrate improved training efficiency and\ncompetitive generative performance of LFM compared to FM on the unconditional\ngeneration of tabular data and image datasets, and also on the conditional\ngeneration of robotic manipulation policies.\n", "link": "http://arxiv.org/abs/2410.02548v3", "date": "2025-07-11", "relevancy": 1.6124, "topK": [{"title": "Scaling Rectified Flow Transformers for High-Resolution Image Synthesis", "link": "http://arxiv.org/abs/2403.03206v1", "similarity": 0.6177}, {"title": "Thinking Outside the BBox: Unconstrained Generative Object Compositing", "link": "http://arxiv.org/abs/2409.04559v2", "similarity": 0.5204}, {"title": "Tuning-Free Noise Rectification for High Fidelity Image-to-Video\n  Generation", "link": "http://arxiv.org/abs/2403.02827v1", "similarity": 0.5122}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Local%20Flow%20Matching%20Generative%20Models&body=Title%3A%20Local%20Flow%20Matching%20Generative%20Models%0AAuthor%3A%20Chen%20Xu%20and%20Xiuyuan%20Cheng%20and%20Yao%20Xie%0AAbstract%3A%20%20%20Flow%20Matching%20%28FM%29%20is%20a%20simulation-free%20method%20for%20learning%20a%20continuous%20and%0Ainvertible%20flow%20to%20interpolate%20between%20two%20distributions%2C%20and%20in%20particular%20to%0Agenerate%20data%20from%20noise.%20Inspired%20by%20the%20variational%20nature%20of%20the%20diffusion%0Aprocess%20as%20a%20gradient%20flow%2C%20we%20introduce%20a%20stepwise%20FM%20model%20called%20Local%20Flow%0AMatching%20%28LFM%29%2C%20which%20consecutively%20learns%20a%20sequence%20of%20FM%20sub-models%2C%20each%0Amatching%20a%20diffusion%20process%20up%20to%20the%20time%20of%20the%20step%20size%20in%20the%0Adata-to-noise%20direction.%20In%20each%20step%2C%20the%20two%20distributions%20to%20be%20interpolated%0Aby%20the%20sub-flow%20model%20are%20closer%20to%20each%20other%20than%20data%20vs.%20noise%2C%20and%20this%0Aenables%20the%20use%20of%20smaller%20models%20with%20faster%20training.%20This%20variational%0Aperspective%20also%20allows%20us%20to%20theoretically%20prove%20a%20generation%20guarantee%20of%20the%0Aproposed%20flow%20model%20in%20terms%20of%20the%20%24%5Cchi%5E2%24-divergence%20between%20the%20generated%0Aand%20true%20data%20distributions%2C%20utilizing%20the%20contraction%20property%20of%20the%0Adiffusion%20process.%20In%20practice%2C%20the%20stepwise%20structure%20of%20LFM%20is%20natural%20to%20be%0Adistilled%20and%20different%20distillation%20techniques%20can%20be%20adopted%20to%20speed%20up%0Ageneration.%20We%20empirically%20demonstrate%20improved%20training%20efficiency%20and%0Acompetitive%20generative%20performance%20of%20LFM%20compared%20to%20FM%20on%20the%20unconditional%0Ageneration%20of%20tabular%20data%20and%20image%20datasets%2C%20and%20also%20on%20the%20conditional%0Ageneration%20of%20robotic%20manipulation%20policies.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2410.02548v3%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DLocal%2520Flow%2520Matching%2520Generative%2520Models%26entry.906535625%3DChen%2520Xu%2520and%2520Xiuyuan%2520Cheng%2520and%2520Yao%2520Xie%26entry.1292438233%3D%2520%2520Flow%2520Matching%2520%2528FM%2529%2520is%2520a%2520simulation-free%2520method%2520for%2520learning%2520a%2520continuous%2520and%250Ainvertible%2520flow%2520to%2520interpolate%2520between%2520two%2520distributions%252C%2520and%2520in%2520particular%2520to%250Agenerate%2520data%2520from%2520noise.%2520Inspired%2520by%2520the%2520variational%2520nature%2520of%2520the%2520diffusion%250Aprocess%2520as%2520a%2520gradient%2520flow%252C%2520we%2520introduce%2520a%2520stepwise%2520FM%2520model%2520called%2520Local%2520Flow%250AMatching%2520%2528LFM%2529%252C%2520which%2520consecutively%2520learns%2520a%2520sequence%2520of%2520FM%2520sub-models%252C%2520each%250Amatching%2520a%2520diffusion%2520process%2520up%2520to%2520the%2520time%2520of%2520the%2520step%2520size%2520in%2520the%250Adata-to-noise%2520direction.%2520In%2520each%2520step%252C%2520the%2520two%2520distributions%2520to%2520be%2520interpolated%250Aby%2520the%2520sub-flow%2520model%2520are%2520closer%2520to%2520each%2520other%2520than%2520data%2520vs.%2520noise%252C%2520and%2520this%250Aenables%2520the%2520use%2520of%2520smaller%2520models%2520with%2520faster%2520training.%2520This%2520variational%250Aperspective%2520also%2520allows%2520us%2520to%2520theoretically%2520prove%2520a%2520generation%2520guarantee%2520of%2520the%250Aproposed%2520flow%2520model%2520in%2520terms%2520of%2520the%2520%2524%255Cchi%255E2%2524-divergence%2520between%2520the%2520generated%250Aand%2520true%2520data%2520distributions%252C%2520utilizing%2520the%2520contraction%2520property%2520of%2520the%250Adiffusion%2520process.%2520In%2520practice%252C%2520the%2520stepwise%2520structure%2520of%2520LFM%2520is%2520natural%2520to%2520be%250Adistilled%2520and%2520different%2520distillation%2520techniques%2520can%2520be%2520adopted%2520to%2520speed%2520up%250Ageneration.%2520We%2520empirically%2520demonstrate%2520improved%2520training%2520efficiency%2520and%250Acompetitive%2520generative%2520performance%2520of%2520LFM%2520compared%2520to%2520FM%2520on%2520the%2520unconditional%250Ageneration%2520of%2520tabular%2520data%2520and%2520image%2520datasets%252C%2520and%2520also%2520on%2520the%2520conditional%250Ageneration%2520of%2520robotic%2520manipulation%2520policies.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2410.02548v3%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Local%20Flow%20Matching%20Generative%20Models&entry.906535625=Chen%20Xu%20and%20Xiuyuan%20Cheng%20and%20Yao%20Xie&entry.1292438233=%20%20Flow%20Matching%20%28FM%29%20is%20a%20simulation-free%20method%20for%20learning%20a%20continuous%20and%0Ainvertible%20flow%20to%20interpolate%20between%20two%20distributions%2C%20and%20in%20particular%20to%0Agenerate%20data%20from%20noise.%20Inspired%20by%20the%20variational%20nature%20of%20the%20diffusion%0Aprocess%20as%20a%20gradient%20flow%2C%20we%20introduce%20a%20stepwise%20FM%20model%20called%20Local%20Flow%0AMatching%20%28LFM%29%2C%20which%20consecutively%20learns%20a%20sequence%20of%20FM%20sub-models%2C%20each%0Amatching%20a%20diffusion%20process%20up%20to%20the%20time%20of%20the%20step%20size%20in%20the%0Adata-to-noise%20direction.%20In%20each%20step%2C%20the%20two%20distributions%20to%20be%20interpolated%0Aby%20the%20sub-flow%20model%20are%20closer%20to%20each%20other%20than%20data%20vs.%20noise%2C%20and%20this%0Aenables%20the%20use%20of%20smaller%20models%20with%20faster%20training.%20This%20variational%0Aperspective%20also%20allows%20us%20to%20theoretically%20prove%20a%20generation%20guarantee%20of%20the%0Aproposed%20flow%20model%20in%20terms%20of%20the%20%24%5Cchi%5E2%24-divergence%20between%20the%20generated%0Aand%20true%20data%20distributions%2C%20utilizing%20the%20contraction%20property%20of%20the%0Adiffusion%20process.%20In%20practice%2C%20the%20stepwise%20structure%20of%20LFM%20is%20natural%20to%20be%0Adistilled%20and%20different%20distillation%20techniques%20can%20be%20adopted%20to%20speed%20up%0Ageneration.%20We%20empirically%20demonstrate%20improved%20training%20efficiency%20and%0Acompetitive%20generative%20performance%20of%20LFM%20compared%20to%20FM%20on%20the%20unconditional%0Ageneration%20of%20tabular%20data%20and%20image%20datasets%2C%20and%20also%20on%20the%20conditional%0Ageneration%20of%20robotic%20manipulation%20policies.%0A&entry.1838667208=http%3A//arxiv.org/abs/2410.02548v3&entry.124074799=Read"},
{"title": "MedSegFactory: Text-Guided Generation of Medical Image-Mask Pairs", "author": "Jiawei Mao and Yuhan Wang and Yucheng Tang and Daguang Xu and Kang Wang and Yang Yang and Zongwei Zhou and Yuyin Zhou", "abstract": "  This paper presents MedSegFactory, a versatile medical synthesis framework\nthat generates high-quality paired medical images and segmentation masks across\nmodalities and tasks. It aims to serve as an unlimited data repository,\nsupplying image-mask pairs to enhance existing segmentation tools. The core of\nMedSegFactory is a dual-stream diffusion model, where one stream synthesizes\nmedical images and the other generates corresponding segmentation masks. To\nensure precise alignment between image-mask pairs, we introduce Joint\nCross-Attention (JCA), enabling a collaborative denoising paradigm by dynamic\ncross-conditioning between streams. This bidirectional interaction allows both\nrepresentations to guide each other's generation, enhancing consistency between\ngenerated pairs. MedSegFactory unlocks on-demand generation of paired medical\nimages and segmentation masks through user-defined prompts that specify the\ntarget labels, imaging modalities, anatomical regions, and pathological\nconditions, facilitating scalable and high-quality data generation. This new\nparadigm of medical image synthesis enables seamless integration into diverse\nmedical imaging workflows, enhancing both efficiency and accuracy. Extensive\nexperiments show that MedSegFactory generates data of superior quality and\nusability, achieving competitive or state-of-the-art performance in 2D and 3D\nsegmentation tasks while addressing data scarcity and regulatory constraints.\n", "link": "http://arxiv.org/abs/2504.06897v2", "date": "2025-07-11", "relevancy": 1.6012, "topK": [{"title": "Thinking Outside the BBox: Unconstrained Generative Object Compositing", "link": "http://arxiv.org/abs/2409.04559v2", "similarity": 0.5572}, {"title": "VirtualModel: Generating Object-ID-retentive Human-object Interaction\n  Image by Diffusion Model for E-commerce Marketing", "link": "http://arxiv.org/abs/2405.09985v1", "similarity": 0.5311}, {"title": "Multimodal-Conditioned Latent Diffusion Models for Fashion Image Editing", "link": "http://arxiv.org/abs/2403.14828v2", "similarity": 0.5169}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20MedSegFactory%3A%20Text-Guided%20Generation%20of%20Medical%20Image-Mask%20Pairs&body=Title%3A%20MedSegFactory%3A%20Text-Guided%20Generation%20of%20Medical%20Image-Mask%20Pairs%0AAuthor%3A%20Jiawei%20Mao%20and%20Yuhan%20Wang%20and%20Yucheng%20Tang%20and%20Daguang%20Xu%20and%20Kang%20Wang%20and%20Yang%20Yang%20and%20Zongwei%20Zhou%20and%20Yuyin%20Zhou%0AAbstract%3A%20%20%20This%20paper%20presents%20MedSegFactory%2C%20a%20versatile%20medical%20synthesis%20framework%0Athat%20generates%20high-quality%20paired%20medical%20images%20and%20segmentation%20masks%20across%0Amodalities%20and%20tasks.%20It%20aims%20to%20serve%20as%20an%20unlimited%20data%20repository%2C%0Asupplying%20image-mask%20pairs%20to%20enhance%20existing%20segmentation%20tools.%20The%20core%20of%0AMedSegFactory%20is%20a%20dual-stream%20diffusion%20model%2C%20where%20one%20stream%20synthesizes%0Amedical%20images%20and%20the%20other%20generates%20corresponding%20segmentation%20masks.%20To%0Aensure%20precise%20alignment%20between%20image-mask%20pairs%2C%20we%20introduce%20Joint%0ACross-Attention%20%28JCA%29%2C%20enabling%20a%20collaborative%20denoising%20paradigm%20by%20dynamic%0Across-conditioning%20between%20streams.%20This%20bidirectional%20interaction%20allows%20both%0Arepresentations%20to%20guide%20each%20other%27s%20generation%2C%20enhancing%20consistency%20between%0Agenerated%20pairs.%20MedSegFactory%20unlocks%20on-demand%20generation%20of%20paired%20medical%0Aimages%20and%20segmentation%20masks%20through%20user-defined%20prompts%20that%20specify%20the%0Atarget%20labels%2C%20imaging%20modalities%2C%20anatomical%20regions%2C%20and%20pathological%0Aconditions%2C%20facilitating%20scalable%20and%20high-quality%20data%20generation.%20This%20new%0Aparadigm%20of%20medical%20image%20synthesis%20enables%20seamless%20integration%20into%20diverse%0Amedical%20imaging%20workflows%2C%20enhancing%20both%20efficiency%20and%20accuracy.%20Extensive%0Aexperiments%20show%20that%20MedSegFactory%20generates%20data%20of%20superior%20quality%20and%0Ausability%2C%20achieving%20competitive%20or%20state-of-the-art%20performance%20in%202D%20and%203D%0Asegmentation%20tasks%20while%20addressing%20data%20scarcity%20and%20regulatory%20constraints.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2504.06897v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DMedSegFactory%253A%2520Text-Guided%2520Generation%2520of%2520Medical%2520Image-Mask%2520Pairs%26entry.906535625%3DJiawei%2520Mao%2520and%2520Yuhan%2520Wang%2520and%2520Yucheng%2520Tang%2520and%2520Daguang%2520Xu%2520and%2520Kang%2520Wang%2520and%2520Yang%2520Yang%2520and%2520Zongwei%2520Zhou%2520and%2520Yuyin%2520Zhou%26entry.1292438233%3D%2520%2520This%2520paper%2520presents%2520MedSegFactory%252C%2520a%2520versatile%2520medical%2520synthesis%2520framework%250Athat%2520generates%2520high-quality%2520paired%2520medical%2520images%2520and%2520segmentation%2520masks%2520across%250Amodalities%2520and%2520tasks.%2520It%2520aims%2520to%2520serve%2520as%2520an%2520unlimited%2520data%2520repository%252C%250Asupplying%2520image-mask%2520pairs%2520to%2520enhance%2520existing%2520segmentation%2520tools.%2520The%2520core%2520of%250AMedSegFactory%2520is%2520a%2520dual-stream%2520diffusion%2520model%252C%2520where%2520one%2520stream%2520synthesizes%250Amedical%2520images%2520and%2520the%2520other%2520generates%2520corresponding%2520segmentation%2520masks.%2520To%250Aensure%2520precise%2520alignment%2520between%2520image-mask%2520pairs%252C%2520we%2520introduce%2520Joint%250ACross-Attention%2520%2528JCA%2529%252C%2520enabling%2520a%2520collaborative%2520denoising%2520paradigm%2520by%2520dynamic%250Across-conditioning%2520between%2520streams.%2520This%2520bidirectional%2520interaction%2520allows%2520both%250Arepresentations%2520to%2520guide%2520each%2520other%2527s%2520generation%252C%2520enhancing%2520consistency%2520between%250Agenerated%2520pairs.%2520MedSegFactory%2520unlocks%2520on-demand%2520generation%2520of%2520paired%2520medical%250Aimages%2520and%2520segmentation%2520masks%2520through%2520user-defined%2520prompts%2520that%2520specify%2520the%250Atarget%2520labels%252C%2520imaging%2520modalities%252C%2520anatomical%2520regions%252C%2520and%2520pathological%250Aconditions%252C%2520facilitating%2520scalable%2520and%2520high-quality%2520data%2520generation.%2520This%2520new%250Aparadigm%2520of%2520medical%2520image%2520synthesis%2520enables%2520seamless%2520integration%2520into%2520diverse%250Amedical%2520imaging%2520workflows%252C%2520enhancing%2520both%2520efficiency%2520and%2520accuracy.%2520Extensive%250Aexperiments%2520show%2520that%2520MedSegFactory%2520generates%2520data%2520of%2520superior%2520quality%2520and%250Ausability%252C%2520achieving%2520competitive%2520or%2520state-of-the-art%2520performance%2520in%25202D%2520and%25203D%250Asegmentation%2520tasks%2520while%2520addressing%2520data%2520scarcity%2520and%2520regulatory%2520constraints.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2504.06897v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=MedSegFactory%3A%20Text-Guided%20Generation%20of%20Medical%20Image-Mask%20Pairs&entry.906535625=Jiawei%20Mao%20and%20Yuhan%20Wang%20and%20Yucheng%20Tang%20and%20Daguang%20Xu%20and%20Kang%20Wang%20and%20Yang%20Yang%20and%20Zongwei%20Zhou%20and%20Yuyin%20Zhou&entry.1292438233=%20%20This%20paper%20presents%20MedSegFactory%2C%20a%20versatile%20medical%20synthesis%20framework%0Athat%20generates%20high-quality%20paired%20medical%20images%20and%20segmentation%20masks%20across%0Amodalities%20and%20tasks.%20It%20aims%20to%20serve%20as%20an%20unlimited%20data%20repository%2C%0Asupplying%20image-mask%20pairs%20to%20enhance%20existing%20segmentation%20tools.%20The%20core%20of%0AMedSegFactory%20is%20a%20dual-stream%20diffusion%20model%2C%20where%20one%20stream%20synthesizes%0Amedical%20images%20and%20the%20other%20generates%20corresponding%20segmentation%20masks.%20To%0Aensure%20precise%20alignment%20between%20image-mask%20pairs%2C%20we%20introduce%20Joint%0ACross-Attention%20%28JCA%29%2C%20enabling%20a%20collaborative%20denoising%20paradigm%20by%20dynamic%0Across-conditioning%20between%20streams.%20This%20bidirectional%20interaction%20allows%20both%0Arepresentations%20to%20guide%20each%20other%27s%20generation%2C%20enhancing%20consistency%20between%0Agenerated%20pairs.%20MedSegFactory%20unlocks%20on-demand%20generation%20of%20paired%20medical%0Aimages%20and%20segmentation%20masks%20through%20user-defined%20prompts%20that%20specify%20the%0Atarget%20labels%2C%20imaging%20modalities%2C%20anatomical%20regions%2C%20and%20pathological%0Aconditions%2C%20facilitating%20scalable%20and%20high-quality%20data%20generation.%20This%20new%0Aparadigm%20of%20medical%20image%20synthesis%20enables%20seamless%20integration%20into%20diverse%0Amedical%20imaging%20workflows%2C%20enhancing%20both%20efficiency%20and%20accuracy.%20Extensive%0Aexperiments%20show%20that%20MedSegFactory%20generates%20data%20of%20superior%20quality%20and%0Ausability%2C%20achieving%20competitive%20or%20state-of-the-art%20performance%20in%202D%20and%203D%0Asegmentation%20tasks%20while%20addressing%20data%20scarcity%20and%20regulatory%20constraints.%0A&entry.1838667208=http%3A//arxiv.org/abs/2504.06897v2&entry.124074799=Read"},
{"title": "SPLASH! Sample-efficient Preference-based inverse reinforcement learning\n  for Long-horizon Adversarial tasks from Suboptimal Hierarchical\n  demonstrations", "author": "Peter Crowley and Zachary Serlin and Tyler Paine and Makai Mann and Michael Benjamin and Calin Belta", "abstract": "  Inverse Reinforcement Learning (IRL) presents a powerful paradigm for\nlearning complex robotic tasks from human demonstrations. However, most\napproaches make the assumption that expert demonstrations are available, which\nis often not the case. Those that allow for suboptimality in the demonstrations\nare not designed for long-horizon goals or adversarial tasks. Many desirable\nrobot capabilities fall into one or both of these categories, thus highlighting\na critical shortcoming in the ability of IRL to produce field-ready robotic\nagents. We introduce Sample-efficient Preference-based inverse reinforcement\nlearning for Long-horizon Adversarial tasks from Suboptimal Hierarchical\ndemonstrations (SPLASH), which advances the state-of-the-art in learning from\nsuboptimal demonstrations to long-horizon and adversarial settings. We\nempirically validate SPLASH on a maritime capture-the-flag task in simulation,\nand demonstrate real-world applicability with sim-to-real translation\nexperiments on autonomous unmanned surface vehicles. We show that our proposed\nmethods allow SPLASH to significantly outperform the state-of-the-art in reward\nlearning from suboptimal demonstrations.\n", "link": "http://arxiv.org/abs/2507.08707v1", "date": "2025-07-11", "relevancy": 1.5845, "topK": [{"title": "RoboEXP: Action-Conditioned Scene Graph via Interactive Exploration for\n  Robotic Manipulation", "link": "http://arxiv.org/abs/2402.15487v1", "similarity": 0.5302}, {"title": "Breadcrumbs: Adversarial class-balanced sampling for long-tailed recognition", "link": "https://link.springer.com/chapter/10.1007/978-3-031-20053-3_37", "similarity": 0.5282}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.526}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20SPLASH%21%20Sample-efficient%20Preference-based%20inverse%20reinforcement%20learning%0A%20%20for%20Long-horizon%20Adversarial%20tasks%20from%20Suboptimal%20Hierarchical%0A%20%20demonstrations&body=Title%3A%20SPLASH%21%20Sample-efficient%20Preference-based%20inverse%20reinforcement%20learning%0A%20%20for%20Long-horizon%20Adversarial%20tasks%20from%20Suboptimal%20Hierarchical%0A%20%20demonstrations%0AAuthor%3A%20Peter%20Crowley%20and%20Zachary%20Serlin%20and%20Tyler%20Paine%20and%20Makai%20Mann%20and%20Michael%20Benjamin%20and%20Calin%20Belta%0AAbstract%3A%20%20%20Inverse%20Reinforcement%20Learning%20%28IRL%29%20presents%20a%20powerful%20paradigm%20for%0Alearning%20complex%20robotic%20tasks%20from%20human%20demonstrations.%20However%2C%20most%0Aapproaches%20make%20the%20assumption%20that%20expert%20demonstrations%20are%20available%2C%20which%0Ais%20often%20not%20the%20case.%20Those%20that%20allow%20for%20suboptimality%20in%20the%20demonstrations%0Aare%20not%20designed%20for%20long-horizon%20goals%20or%20adversarial%20tasks.%20Many%20desirable%0Arobot%20capabilities%20fall%20into%20one%20or%20both%20of%20these%20categories%2C%20thus%20highlighting%0Aa%20critical%20shortcoming%20in%20the%20ability%20of%20IRL%20to%20produce%20field-ready%20robotic%0Aagents.%20We%20introduce%20Sample-efficient%20Preference-based%20inverse%20reinforcement%0Alearning%20for%20Long-horizon%20Adversarial%20tasks%20from%20Suboptimal%20Hierarchical%0Ademonstrations%20%28SPLASH%29%2C%20which%20advances%20the%20state-of-the-art%20in%20learning%20from%0Asuboptimal%20demonstrations%20to%20long-horizon%20and%20adversarial%20settings.%20We%0Aempirically%20validate%20SPLASH%20on%20a%20maritime%20capture-the-flag%20task%20in%20simulation%2C%0Aand%20demonstrate%20real-world%20applicability%20with%20sim-to-real%20translation%0Aexperiments%20on%20autonomous%20unmanned%20surface%20vehicles.%20We%20show%20that%20our%20proposed%0Amethods%20allow%20SPLASH%20to%20significantly%20outperform%20the%20state-of-the-art%20in%20reward%0Alearning%20from%20suboptimal%20demonstrations.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2507.08707v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DSPLASH%2521%2520Sample-efficient%2520Preference-based%2520inverse%2520reinforcement%2520learning%250A%2520%2520for%2520Long-horizon%2520Adversarial%2520tasks%2520from%2520Suboptimal%2520Hierarchical%250A%2520%2520demonstrations%26entry.906535625%3DPeter%2520Crowley%2520and%2520Zachary%2520Serlin%2520and%2520Tyler%2520Paine%2520and%2520Makai%2520Mann%2520and%2520Michael%2520Benjamin%2520and%2520Calin%2520Belta%26entry.1292438233%3D%2520%2520Inverse%2520Reinforcement%2520Learning%2520%2528IRL%2529%2520presents%2520a%2520powerful%2520paradigm%2520for%250Alearning%2520complex%2520robotic%2520tasks%2520from%2520human%2520demonstrations.%2520However%252C%2520most%250Aapproaches%2520make%2520the%2520assumption%2520that%2520expert%2520demonstrations%2520are%2520available%252C%2520which%250Ais%2520often%2520not%2520the%2520case.%2520Those%2520that%2520allow%2520for%2520suboptimality%2520in%2520the%2520demonstrations%250Aare%2520not%2520designed%2520for%2520long-horizon%2520goals%2520or%2520adversarial%2520tasks.%2520Many%2520desirable%250Arobot%2520capabilities%2520fall%2520into%2520one%2520or%2520both%2520of%2520these%2520categories%252C%2520thus%2520highlighting%250Aa%2520critical%2520shortcoming%2520in%2520the%2520ability%2520of%2520IRL%2520to%2520produce%2520field-ready%2520robotic%250Aagents.%2520We%2520introduce%2520Sample-efficient%2520Preference-based%2520inverse%2520reinforcement%250Alearning%2520for%2520Long-horizon%2520Adversarial%2520tasks%2520from%2520Suboptimal%2520Hierarchical%250Ademonstrations%2520%2528SPLASH%2529%252C%2520which%2520advances%2520the%2520state-of-the-art%2520in%2520learning%2520from%250Asuboptimal%2520demonstrations%2520to%2520long-horizon%2520and%2520adversarial%2520settings.%2520We%250Aempirically%2520validate%2520SPLASH%2520on%2520a%2520maritime%2520capture-the-flag%2520task%2520in%2520simulation%252C%250Aand%2520demonstrate%2520real-world%2520applicability%2520with%2520sim-to-real%2520translation%250Aexperiments%2520on%2520autonomous%2520unmanned%2520surface%2520vehicles.%2520We%2520show%2520that%2520our%2520proposed%250Amethods%2520allow%2520SPLASH%2520to%2520significantly%2520outperform%2520the%2520state-of-the-art%2520in%2520reward%250Alearning%2520from%2520suboptimal%2520demonstrations.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2507.08707v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=SPLASH%21%20Sample-efficient%20Preference-based%20inverse%20reinforcement%20learning%0A%20%20for%20Long-horizon%20Adversarial%20tasks%20from%20Suboptimal%20Hierarchical%0A%20%20demonstrations&entry.906535625=Peter%20Crowley%20and%20Zachary%20Serlin%20and%20Tyler%20Paine%20and%20Makai%20Mann%20and%20Michael%20Benjamin%20and%20Calin%20Belta&entry.1292438233=%20%20Inverse%20Reinforcement%20Learning%20%28IRL%29%20presents%20a%20powerful%20paradigm%20for%0Alearning%20complex%20robotic%20tasks%20from%20human%20demonstrations.%20However%2C%20most%0Aapproaches%20make%20the%20assumption%20that%20expert%20demonstrations%20are%20available%2C%20which%0Ais%20often%20not%20the%20case.%20Those%20that%20allow%20for%20suboptimality%20in%20the%20demonstrations%0Aare%20not%20designed%20for%20long-horizon%20goals%20or%20adversarial%20tasks.%20Many%20desirable%0Arobot%20capabilities%20fall%20into%20one%20or%20both%20of%20these%20categories%2C%20thus%20highlighting%0Aa%20critical%20shortcoming%20in%20the%20ability%20of%20IRL%20to%20produce%20field-ready%20robotic%0Aagents.%20We%20introduce%20Sample-efficient%20Preference-based%20inverse%20reinforcement%0Alearning%20for%20Long-horizon%20Adversarial%20tasks%20from%20Suboptimal%20Hierarchical%0Ademonstrations%20%28SPLASH%29%2C%20which%20advances%20the%20state-of-the-art%20in%20learning%20from%0Asuboptimal%20demonstrations%20to%20long-horizon%20and%20adversarial%20settings.%20We%0Aempirically%20validate%20SPLASH%20on%20a%20maritime%20capture-the-flag%20task%20in%20simulation%2C%0Aand%20demonstrate%20real-world%20applicability%20with%20sim-to-real%20translation%0Aexperiments%20on%20autonomous%20unmanned%20surface%20vehicles.%20We%20show%20that%20our%20proposed%0Amethods%20allow%20SPLASH%20to%20significantly%20outperform%20the%20state-of-the-art%20in%20reward%0Alearning%20from%20suboptimal%20demonstrations.%0A&entry.1838667208=http%3A//arxiv.org/abs/2507.08707v1&entry.124074799=Read"},
{"title": "Agentic Large Language Models for Conceptual Systems Engineering and\n  Design", "author": "Soheyl Massoudi and Mark Fuge", "abstract": "  Early-stage engineering design involves complex, iterative reasoning, yet\nexisting large language model (LLM) workflows struggle to maintain task\ncontinuity and generate executable models. We evaluate whether a structured\nmulti-agent system (MAS) can more effectively manage requirements extraction,\nfunctional decomposition, and simulator code generation than a simpler\ntwo-agent system (2AS). The target application is a solar-powered water\nfiltration system as described in a cahier des charges. We introduce the\nDesign-State Graph (DSG), a JSON-serializable representation that bundles\nrequirements, physical embodiments, and Python-based physics models into graph\nnodes. A nine-role MAS iteratively builds and refines the DSG, while the 2AS\ncollapses the process to a Generator-Reflector loop. Both systems run a total\nof 60 experiments (2 LLMs - Llama 3.3 70B vs reasoning-distilled DeepSeek R1\n70B x 2 agent configurations x 3 temperatures x 5 seeds). We report a JSON\nvalidity, requirement coverage, embodiment presence, code compatibility,\nworkflow completion, runtime, and graph size. Across all runs, both MAS and 2AS\nmaintained perfect JSON integrity and embodiment tagging. Requirement coverage\nremained minimal (less than 20\\%). Code compatibility peaked at 100\\% under\nspecific 2AS settings but averaged below 50\\% for MAS. Only the\nreasoning-distilled model reliably flagged workflow completion. Powered by\nDeepSeek R1 70B, the MAS generated more granular DSGs (average 5-6 nodes)\nwhereas 2AS mode-collapsed. Structured multi-agent orchestration enhanced\ndesign detail. Reasoning-distilled LLM improved completion rates, yet low\nrequirements and fidelity gaps in coding persisted.\n", "link": "http://arxiv.org/abs/2507.08619v1", "date": "2025-07-11", "relevancy": 1.5737, "topK": [{"title": "RoboEXP: Action-Conditioned Scene Graph via Interactive Exploration for\n  Robotic Manipulation", "link": "http://arxiv.org/abs/2402.15487v1", "similarity": 0.5831}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.521}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5026}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Agentic%20Large%20Language%20Models%20for%20Conceptual%20Systems%20Engineering%20and%0A%20%20Design&body=Title%3A%20Agentic%20Large%20Language%20Models%20for%20Conceptual%20Systems%20Engineering%20and%0A%20%20Design%0AAuthor%3A%20Soheyl%20Massoudi%20and%20Mark%20Fuge%0AAbstract%3A%20%20%20Early-stage%20engineering%20design%20involves%20complex%2C%20iterative%20reasoning%2C%20yet%0Aexisting%20large%20language%20model%20%28LLM%29%20workflows%20struggle%20to%20maintain%20task%0Acontinuity%20and%20generate%20executable%20models.%20We%20evaluate%20whether%20a%20structured%0Amulti-agent%20system%20%28MAS%29%20can%20more%20effectively%20manage%20requirements%20extraction%2C%0Afunctional%20decomposition%2C%20and%20simulator%20code%20generation%20than%20a%20simpler%0Atwo-agent%20system%20%282AS%29.%20The%20target%20application%20is%20a%20solar-powered%20water%0Afiltration%20system%20as%20described%20in%20a%20cahier%20des%20charges.%20We%20introduce%20the%0ADesign-State%20Graph%20%28DSG%29%2C%20a%20JSON-serializable%20representation%20that%20bundles%0Arequirements%2C%20physical%20embodiments%2C%20and%20Python-based%20physics%20models%20into%20graph%0Anodes.%20A%20nine-role%20MAS%20iteratively%20builds%20and%20refines%20the%20DSG%2C%20while%20the%202AS%0Acollapses%20the%20process%20to%20a%20Generator-Reflector%20loop.%20Both%20systems%20run%20a%20total%0Aof%2060%20experiments%20%282%20LLMs%20-%20Llama%203.3%2070B%20vs%20reasoning-distilled%20DeepSeek%20R1%0A70B%20x%202%20agent%20configurations%20x%203%20temperatures%20x%205%20seeds%29.%20We%20report%20a%20JSON%0Avalidity%2C%20requirement%20coverage%2C%20embodiment%20presence%2C%20code%20compatibility%2C%0Aworkflow%20completion%2C%20runtime%2C%20and%20graph%20size.%20Across%20all%20runs%2C%20both%20MAS%20and%202AS%0Amaintained%20perfect%20JSON%20integrity%20and%20embodiment%20tagging.%20Requirement%20coverage%0Aremained%20minimal%20%28less%20than%2020%5C%25%29.%20Code%20compatibility%20peaked%20at%20100%5C%25%20under%0Aspecific%202AS%20settings%20but%20averaged%20below%2050%5C%25%20for%20MAS.%20Only%20the%0Areasoning-distilled%20model%20reliably%20flagged%20workflow%20completion.%20Powered%20by%0ADeepSeek%20R1%2070B%2C%20the%20MAS%20generated%20more%20granular%20DSGs%20%28average%205-6%20nodes%29%0Awhereas%202AS%20mode-collapsed.%20Structured%20multi-agent%20orchestration%20enhanced%0Adesign%20detail.%20Reasoning-distilled%20LLM%20improved%20completion%20rates%2C%20yet%20low%0Arequirements%20and%20fidelity%20gaps%20in%20coding%20persisted.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2507.08619v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DAgentic%2520Large%2520Language%2520Models%2520for%2520Conceptual%2520Systems%2520Engineering%2520and%250A%2520%2520Design%26entry.906535625%3DSoheyl%2520Massoudi%2520and%2520Mark%2520Fuge%26entry.1292438233%3D%2520%2520Early-stage%2520engineering%2520design%2520involves%2520complex%252C%2520iterative%2520reasoning%252C%2520yet%250Aexisting%2520large%2520language%2520model%2520%2528LLM%2529%2520workflows%2520struggle%2520to%2520maintain%2520task%250Acontinuity%2520and%2520generate%2520executable%2520models.%2520We%2520evaluate%2520whether%2520a%2520structured%250Amulti-agent%2520system%2520%2528MAS%2529%2520can%2520more%2520effectively%2520manage%2520requirements%2520extraction%252C%250Afunctional%2520decomposition%252C%2520and%2520simulator%2520code%2520generation%2520than%2520a%2520simpler%250Atwo-agent%2520system%2520%25282AS%2529.%2520The%2520target%2520application%2520is%2520a%2520solar-powered%2520water%250Afiltration%2520system%2520as%2520described%2520in%2520a%2520cahier%2520des%2520charges.%2520We%2520introduce%2520the%250ADesign-State%2520Graph%2520%2528DSG%2529%252C%2520a%2520JSON-serializable%2520representation%2520that%2520bundles%250Arequirements%252C%2520physical%2520embodiments%252C%2520and%2520Python-based%2520physics%2520models%2520into%2520graph%250Anodes.%2520A%2520nine-role%2520MAS%2520iteratively%2520builds%2520and%2520refines%2520the%2520DSG%252C%2520while%2520the%25202AS%250Acollapses%2520the%2520process%2520to%2520a%2520Generator-Reflector%2520loop.%2520Both%2520systems%2520run%2520a%2520total%250Aof%252060%2520experiments%2520%25282%2520LLMs%2520-%2520Llama%25203.3%252070B%2520vs%2520reasoning-distilled%2520DeepSeek%2520R1%250A70B%2520x%25202%2520agent%2520configurations%2520x%25203%2520temperatures%2520x%25205%2520seeds%2529.%2520We%2520report%2520a%2520JSON%250Avalidity%252C%2520requirement%2520coverage%252C%2520embodiment%2520presence%252C%2520code%2520compatibility%252C%250Aworkflow%2520completion%252C%2520runtime%252C%2520and%2520graph%2520size.%2520Across%2520all%2520runs%252C%2520both%2520MAS%2520and%25202AS%250Amaintained%2520perfect%2520JSON%2520integrity%2520and%2520embodiment%2520tagging.%2520Requirement%2520coverage%250Aremained%2520minimal%2520%2528less%2520than%252020%255C%2525%2529.%2520Code%2520compatibility%2520peaked%2520at%2520100%255C%2525%2520under%250Aspecific%25202AS%2520settings%2520but%2520averaged%2520below%252050%255C%2525%2520for%2520MAS.%2520Only%2520the%250Areasoning-distilled%2520model%2520reliably%2520flagged%2520workflow%2520completion.%2520Powered%2520by%250ADeepSeek%2520R1%252070B%252C%2520the%2520MAS%2520generated%2520more%2520granular%2520DSGs%2520%2528average%25205-6%2520nodes%2529%250Awhereas%25202AS%2520mode-collapsed.%2520Structured%2520multi-agent%2520orchestration%2520enhanced%250Adesign%2520detail.%2520Reasoning-distilled%2520LLM%2520improved%2520completion%2520rates%252C%2520yet%2520low%250Arequirements%2520and%2520fidelity%2520gaps%2520in%2520coding%2520persisted.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2507.08619v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Agentic%20Large%20Language%20Models%20for%20Conceptual%20Systems%20Engineering%20and%0A%20%20Design&entry.906535625=Soheyl%20Massoudi%20and%20Mark%20Fuge&entry.1292438233=%20%20Early-stage%20engineering%20design%20involves%20complex%2C%20iterative%20reasoning%2C%20yet%0Aexisting%20large%20language%20model%20%28LLM%29%20workflows%20struggle%20to%20maintain%20task%0Acontinuity%20and%20generate%20executable%20models.%20We%20evaluate%20whether%20a%20structured%0Amulti-agent%20system%20%28MAS%29%20can%20more%20effectively%20manage%20requirements%20extraction%2C%0Afunctional%20decomposition%2C%20and%20simulator%20code%20generation%20than%20a%20simpler%0Atwo-agent%20system%20%282AS%29.%20The%20target%20application%20is%20a%20solar-powered%20water%0Afiltration%20system%20as%20described%20in%20a%20cahier%20des%20charges.%20We%20introduce%20the%0ADesign-State%20Graph%20%28DSG%29%2C%20a%20JSON-serializable%20representation%20that%20bundles%0Arequirements%2C%20physical%20embodiments%2C%20and%20Python-based%20physics%20models%20into%20graph%0Anodes.%20A%20nine-role%20MAS%20iteratively%20builds%20and%20refines%20the%20DSG%2C%20while%20the%202AS%0Acollapses%20the%20process%20to%20a%20Generator-Reflector%20loop.%20Both%20systems%20run%20a%20total%0Aof%2060%20experiments%20%282%20LLMs%20-%20Llama%203.3%2070B%20vs%20reasoning-distilled%20DeepSeek%20R1%0A70B%20x%202%20agent%20configurations%20x%203%20temperatures%20x%205%20seeds%29.%20We%20report%20a%20JSON%0Avalidity%2C%20requirement%20coverage%2C%20embodiment%20presence%2C%20code%20compatibility%2C%0Aworkflow%20completion%2C%20runtime%2C%20and%20graph%20size.%20Across%20all%20runs%2C%20both%20MAS%20and%202AS%0Amaintained%20perfect%20JSON%20integrity%20and%20embodiment%20tagging.%20Requirement%20coverage%0Aremained%20minimal%20%28less%20than%2020%5C%25%29.%20Code%20compatibility%20peaked%20at%20100%5C%25%20under%0Aspecific%202AS%20settings%20but%20averaged%20below%2050%5C%25%20for%20MAS.%20Only%20the%0Areasoning-distilled%20model%20reliably%20flagged%20workflow%20completion.%20Powered%20by%0ADeepSeek%20R1%2070B%2C%20the%20MAS%20generated%20more%20granular%20DSGs%20%28average%205-6%20nodes%29%0Awhereas%202AS%20mode-collapsed.%20Structured%20multi-agent%20orchestration%20enhanced%0Adesign%20detail.%20Reasoning-distilled%20LLM%20improved%20completion%20rates%2C%20yet%20low%0Arequirements%20and%20fidelity%20gaps%20in%20coding%20persisted.%0A&entry.1838667208=http%3A//arxiv.org/abs/2507.08619v1&entry.124074799=Read"},
{"title": "Optimistic Exploration for Risk-Averse Constrained Reinforcement\n  Learning", "author": "James McCarthy and Radu Marinescu and Elizabeth Daly and Ivana Dusparic", "abstract": "  Risk-averse Constrained Reinforcement Learning (RaCRL) aims to learn policies\nthat minimise the likelihood of rare and catastrophic constraint violations\ncaused by an environment's inherent randomness. In general, risk-aversion leads\nto conservative exploration of the environment which typically results in\nconverging to sub-optimal policies that fail to adequately maximise reward or,\nin some cases, fail to achieve the goal. In this paper, we propose an\nexploration-based approach for RaCRL called Optimistic Risk-averse Actor Critic\n(ORAC), which constructs an exploratory policy by maximising a local upper\nconfidence bound of the state-action reward value function whilst minimising a\nlocal lower confidence bound of the risk-averse state-action cost value\nfunction. Specifically, at each step, the weighting assigned to the cost value\nis increased or decreased if it exceeds or falls below the safety constraint\nvalue. This way the policy is encouraged to explore uncertain regions of the\nenvironment to discover high reward states whilst still satisfying the safety\nconstraints. Our experimental results demonstrate that the ORAC approach\nprevents convergence to sub-optimal policies and improves significantly the\nreward-cost trade-off in various continuous control tasks such as\nSafety-Gymnasium and a complex building energy management environment\nCityLearn.\n", "link": "http://arxiv.org/abs/2507.08793v1", "date": "2025-07-11", "relevancy": 1.541, "topK": [{"title": "RoboEXP: Action-Conditioned Scene Graph via Interactive Exploration for\n  Robotic Manipulation", "link": "http://arxiv.org/abs/2402.15487v1", "similarity": 0.5734}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.5023}, {"title": "Safe and efficient training of a control agent", "link": "https://patents.google.com/patent/US11709462B2/en", "similarity": 0.4943}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Optimistic%20Exploration%20for%20Risk-Averse%20Constrained%20Reinforcement%0A%20%20Learning&body=Title%3A%20Optimistic%20Exploration%20for%20Risk-Averse%20Constrained%20Reinforcement%0A%20%20Learning%0AAuthor%3A%20James%20McCarthy%20and%20Radu%20Marinescu%20and%20Elizabeth%20Daly%20and%20Ivana%20Dusparic%0AAbstract%3A%20%20%20Risk-averse%20Constrained%20Reinforcement%20Learning%20%28RaCRL%29%20aims%20to%20learn%20policies%0Athat%20minimise%20the%20likelihood%20of%20rare%20and%20catastrophic%20constraint%20violations%0Acaused%20by%20an%20environment%27s%20inherent%20randomness.%20In%20general%2C%20risk-aversion%20leads%0Ato%20conservative%20exploration%20of%20the%20environment%20which%20typically%20results%20in%0Aconverging%20to%20sub-optimal%20policies%20that%20fail%20to%20adequately%20maximise%20reward%20or%2C%0Ain%20some%20cases%2C%20fail%20to%20achieve%20the%20goal.%20In%20this%20paper%2C%20we%20propose%20an%0Aexploration-based%20approach%20for%20RaCRL%20called%20Optimistic%20Risk-averse%20Actor%20Critic%0A%28ORAC%29%2C%20which%20constructs%20an%20exploratory%20policy%20by%20maximising%20a%20local%20upper%0Aconfidence%20bound%20of%20the%20state-action%20reward%20value%20function%20whilst%20minimising%20a%0Alocal%20lower%20confidence%20bound%20of%20the%20risk-averse%20state-action%20cost%20value%0Afunction.%20Specifically%2C%20at%20each%20step%2C%20the%20weighting%20assigned%20to%20the%20cost%20value%0Ais%20increased%20or%20decreased%20if%20it%20exceeds%20or%20falls%20below%20the%20safety%20constraint%0Avalue.%20This%20way%20the%20policy%20is%20encouraged%20to%20explore%20uncertain%20regions%20of%20the%0Aenvironment%20to%20discover%20high%20reward%20states%20whilst%20still%20satisfying%20the%20safety%0Aconstraints.%20Our%20experimental%20results%20demonstrate%20that%20the%20ORAC%20approach%0Aprevents%20convergence%20to%20sub-optimal%20policies%20and%20improves%20significantly%20the%0Areward-cost%20trade-off%20in%20various%20continuous%20control%20tasks%20such%20as%0ASafety-Gymnasium%20and%20a%20complex%20building%20energy%20management%20environment%0ACityLearn.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2507.08793v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DOptimistic%2520Exploration%2520for%2520Risk-Averse%2520Constrained%2520Reinforcement%250A%2520%2520Learning%26entry.906535625%3DJames%2520McCarthy%2520and%2520Radu%2520Marinescu%2520and%2520Elizabeth%2520Daly%2520and%2520Ivana%2520Dusparic%26entry.1292438233%3D%2520%2520Risk-averse%2520Constrained%2520Reinforcement%2520Learning%2520%2528RaCRL%2529%2520aims%2520to%2520learn%2520policies%250Athat%2520minimise%2520the%2520likelihood%2520of%2520rare%2520and%2520catastrophic%2520constraint%2520violations%250Acaused%2520by%2520an%2520environment%2527s%2520inherent%2520randomness.%2520In%2520general%252C%2520risk-aversion%2520leads%250Ato%2520conservative%2520exploration%2520of%2520the%2520environment%2520which%2520typically%2520results%2520in%250Aconverging%2520to%2520sub-optimal%2520policies%2520that%2520fail%2520to%2520adequately%2520maximise%2520reward%2520or%252C%250Ain%2520some%2520cases%252C%2520fail%2520to%2520achieve%2520the%2520goal.%2520In%2520this%2520paper%252C%2520we%2520propose%2520an%250Aexploration-based%2520approach%2520for%2520RaCRL%2520called%2520Optimistic%2520Risk-averse%2520Actor%2520Critic%250A%2528ORAC%2529%252C%2520which%2520constructs%2520an%2520exploratory%2520policy%2520by%2520maximising%2520a%2520local%2520upper%250Aconfidence%2520bound%2520of%2520the%2520state-action%2520reward%2520value%2520function%2520whilst%2520minimising%2520a%250Alocal%2520lower%2520confidence%2520bound%2520of%2520the%2520risk-averse%2520state-action%2520cost%2520value%250Afunction.%2520Specifically%252C%2520at%2520each%2520step%252C%2520the%2520weighting%2520assigned%2520to%2520the%2520cost%2520value%250Ais%2520increased%2520or%2520decreased%2520if%2520it%2520exceeds%2520or%2520falls%2520below%2520the%2520safety%2520constraint%250Avalue.%2520This%2520way%2520the%2520policy%2520is%2520encouraged%2520to%2520explore%2520uncertain%2520regions%2520of%2520the%250Aenvironment%2520to%2520discover%2520high%2520reward%2520states%2520whilst%2520still%2520satisfying%2520the%2520safety%250Aconstraints.%2520Our%2520experimental%2520results%2520demonstrate%2520that%2520the%2520ORAC%2520approach%250Aprevents%2520convergence%2520to%2520sub-optimal%2520policies%2520and%2520improves%2520significantly%2520the%250Areward-cost%2520trade-off%2520in%2520various%2520continuous%2520control%2520tasks%2520such%2520as%250ASafety-Gymnasium%2520and%2520a%2520complex%2520building%2520energy%2520management%2520environment%250ACityLearn.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2507.08793v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Optimistic%20Exploration%20for%20Risk-Averse%20Constrained%20Reinforcement%0A%20%20Learning&entry.906535625=James%20McCarthy%20and%20Radu%20Marinescu%20and%20Elizabeth%20Daly%20and%20Ivana%20Dusparic&entry.1292438233=%20%20Risk-averse%20Constrained%20Reinforcement%20Learning%20%28RaCRL%29%20aims%20to%20learn%20policies%0Athat%20minimise%20the%20likelihood%20of%20rare%20and%20catastrophic%20constraint%20violations%0Acaused%20by%20an%20environment%27s%20inherent%20randomness.%20In%20general%2C%20risk-aversion%20leads%0Ato%20conservative%20exploration%20of%20the%20environment%20which%20typically%20results%20in%0Aconverging%20to%20sub-optimal%20policies%20that%20fail%20to%20adequately%20maximise%20reward%20or%2C%0Ain%20some%20cases%2C%20fail%20to%20achieve%20the%20goal.%20In%20this%20paper%2C%20we%20propose%20an%0Aexploration-based%20approach%20for%20RaCRL%20called%20Optimistic%20Risk-averse%20Actor%20Critic%0A%28ORAC%29%2C%20which%20constructs%20an%20exploratory%20policy%20by%20maximising%20a%20local%20upper%0Aconfidence%20bound%20of%20the%20state-action%20reward%20value%20function%20whilst%20minimising%20a%0Alocal%20lower%20confidence%20bound%20of%20the%20risk-averse%20state-action%20cost%20value%0Afunction.%20Specifically%2C%20at%20each%20step%2C%20the%20weighting%20assigned%20to%20the%20cost%20value%0Ais%20increased%20or%20decreased%20if%20it%20exceeds%20or%20falls%20below%20the%20safety%20constraint%0Avalue.%20This%20way%20the%20policy%20is%20encouraged%20to%20explore%20uncertain%20regions%20of%20the%0Aenvironment%20to%20discover%20high%20reward%20states%20whilst%20still%20satisfying%20the%20safety%0Aconstraints.%20Our%20experimental%20results%20demonstrate%20that%20the%20ORAC%20approach%0Aprevents%20convergence%20to%20sub-optimal%20policies%20and%20improves%20significantly%20the%0Areward-cost%20trade-off%20in%20various%20continuous%20control%20tasks%20such%20as%0ASafety-Gymnasium%20and%20a%20complex%20building%20energy%20management%20environment%0ACityLearn.%0A&entry.1838667208=http%3A//arxiv.org/abs/2507.08793v1&entry.124074799=Read"},
{"title": "KELPS: A Framework for Verified Multi-Language Autoformalization via\n  Semantic-Syntactic Alignment", "author": "Jiyao Zhang and Chengli Zhong and Hui Xu and Qige Li and Yi Zhou", "abstract": "  Modern large language models (LLMs) show promising progress in formalizing\ninformal mathematics into machine-verifiable theorems. However, these methods\nstill face bottlenecks due to the limited quantity and quality of multilingual\nparallel corpora. In this paper, we propose a novel neuro-symbolic framework\nKELPS (Knowledge-Equation based Logical Processing System) to address these\nproblems. KELPS is an iterative framework for translating, synthesizing, and\nfiltering informal data into multiple formal languages (Lean, Coq, and\nIsabelle). First, we translate natural language into Knowledge Equations (KEs),\na novel language that we designed, theoretically grounded in assertional logic.\nNext, we convert them to target languages through rigorously defined rules that\npreserve both syntactic structure and semantic meaning. This process yielded a\nparallel corpus of over 60,000 problems. Our framework achieves 88.9% syntactic\naccuracy (pass@1) on MiniF2F, outperforming SOTA models such as Deepseek-V3\n(81%) and Herald (81.3%) across multiple datasets. All datasets and codes are\navailable in the supplementary materials.\n", "link": "http://arxiv.org/abs/2507.08665v1", "date": "2025-07-11", "relevancy": 1.5315, "topK": [{"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.5224}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.5077}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5069}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20KELPS%3A%20A%20Framework%20for%20Verified%20Multi-Language%20Autoformalization%20via%0A%20%20Semantic-Syntactic%20Alignment&body=Title%3A%20KELPS%3A%20A%20Framework%20for%20Verified%20Multi-Language%20Autoformalization%20via%0A%20%20Semantic-Syntactic%20Alignment%0AAuthor%3A%20Jiyao%20Zhang%20and%20Chengli%20Zhong%20and%20Hui%20Xu%20and%20Qige%20Li%20and%20Yi%20Zhou%0AAbstract%3A%20%20%20Modern%20large%20language%20models%20%28LLMs%29%20show%20promising%20progress%20in%20formalizing%0Ainformal%20mathematics%20into%20machine-verifiable%20theorems.%20However%2C%20these%20methods%0Astill%20face%20bottlenecks%20due%20to%20the%20limited%20quantity%20and%20quality%20of%20multilingual%0Aparallel%20corpora.%20In%20this%20paper%2C%20we%20propose%20a%20novel%20neuro-symbolic%20framework%0AKELPS%20%28Knowledge-Equation%20based%20Logical%20Processing%20System%29%20to%20address%20these%0Aproblems.%20KELPS%20is%20an%20iterative%20framework%20for%20translating%2C%20synthesizing%2C%20and%0Afiltering%20informal%20data%20into%20multiple%20formal%20languages%20%28Lean%2C%20Coq%2C%20and%0AIsabelle%29.%20First%2C%20we%20translate%20natural%20language%20into%20Knowledge%20Equations%20%28KEs%29%2C%0Aa%20novel%20language%20that%20we%20designed%2C%20theoretically%20grounded%20in%20assertional%20logic.%0ANext%2C%20we%20convert%20them%20to%20target%20languages%20through%20rigorously%20defined%20rules%20that%0Apreserve%20both%20syntactic%20structure%20and%20semantic%20meaning.%20This%20process%20yielded%20a%0Aparallel%20corpus%20of%20over%2060%2C000%20problems.%20Our%20framework%20achieves%2088.9%25%20syntactic%0Aaccuracy%20%28pass%401%29%20on%20MiniF2F%2C%20outperforming%20SOTA%20models%20such%20as%20Deepseek-V3%0A%2881%25%29%20and%20Herald%20%2881.3%25%29%20across%20multiple%20datasets.%20All%20datasets%20and%20codes%20are%0Aavailable%20in%20the%20supplementary%20materials.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2507.08665v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DKELPS%253A%2520A%2520Framework%2520for%2520Verified%2520Multi-Language%2520Autoformalization%2520via%250A%2520%2520Semantic-Syntactic%2520Alignment%26entry.906535625%3DJiyao%2520Zhang%2520and%2520Chengli%2520Zhong%2520and%2520Hui%2520Xu%2520and%2520Qige%2520Li%2520and%2520Yi%2520Zhou%26entry.1292438233%3D%2520%2520Modern%2520large%2520language%2520models%2520%2528LLMs%2529%2520show%2520promising%2520progress%2520in%2520formalizing%250Ainformal%2520mathematics%2520into%2520machine-verifiable%2520theorems.%2520However%252C%2520these%2520methods%250Astill%2520face%2520bottlenecks%2520due%2520to%2520the%2520limited%2520quantity%2520and%2520quality%2520of%2520multilingual%250Aparallel%2520corpora.%2520In%2520this%2520paper%252C%2520we%2520propose%2520a%2520novel%2520neuro-symbolic%2520framework%250AKELPS%2520%2528Knowledge-Equation%2520based%2520Logical%2520Processing%2520System%2529%2520to%2520address%2520these%250Aproblems.%2520KELPS%2520is%2520an%2520iterative%2520framework%2520for%2520translating%252C%2520synthesizing%252C%2520and%250Afiltering%2520informal%2520data%2520into%2520multiple%2520formal%2520languages%2520%2528Lean%252C%2520Coq%252C%2520and%250AIsabelle%2529.%2520First%252C%2520we%2520translate%2520natural%2520language%2520into%2520Knowledge%2520Equations%2520%2528KEs%2529%252C%250Aa%2520novel%2520language%2520that%2520we%2520designed%252C%2520theoretically%2520grounded%2520in%2520assertional%2520logic.%250ANext%252C%2520we%2520convert%2520them%2520to%2520target%2520languages%2520through%2520rigorously%2520defined%2520rules%2520that%250Apreserve%2520both%2520syntactic%2520structure%2520and%2520semantic%2520meaning.%2520This%2520process%2520yielded%2520a%250Aparallel%2520corpus%2520of%2520over%252060%252C000%2520problems.%2520Our%2520framework%2520achieves%252088.9%2525%2520syntactic%250Aaccuracy%2520%2528pass%25401%2529%2520on%2520MiniF2F%252C%2520outperforming%2520SOTA%2520models%2520such%2520as%2520Deepseek-V3%250A%252881%2525%2529%2520and%2520Herald%2520%252881.3%2525%2529%2520across%2520multiple%2520datasets.%2520All%2520datasets%2520and%2520codes%2520are%250Aavailable%2520in%2520the%2520supplementary%2520materials.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2507.08665v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=KELPS%3A%20A%20Framework%20for%20Verified%20Multi-Language%20Autoformalization%20via%0A%20%20Semantic-Syntactic%20Alignment&entry.906535625=Jiyao%20Zhang%20and%20Chengli%20Zhong%20and%20Hui%20Xu%20and%20Qige%20Li%20and%20Yi%20Zhou&entry.1292438233=%20%20Modern%20large%20language%20models%20%28LLMs%29%20show%20promising%20progress%20in%20formalizing%0Ainformal%20mathematics%20into%20machine-verifiable%20theorems.%20However%2C%20these%20methods%0Astill%20face%20bottlenecks%20due%20to%20the%20limited%20quantity%20and%20quality%20of%20multilingual%0Aparallel%20corpora.%20In%20this%20paper%2C%20we%20propose%20a%20novel%20neuro-symbolic%20framework%0AKELPS%20%28Knowledge-Equation%20based%20Logical%20Processing%20System%29%20to%20address%20these%0Aproblems.%20KELPS%20is%20an%20iterative%20framework%20for%20translating%2C%20synthesizing%2C%20and%0Afiltering%20informal%20data%20into%20multiple%20formal%20languages%20%28Lean%2C%20Coq%2C%20and%0AIsabelle%29.%20First%2C%20we%20translate%20natural%20language%20into%20Knowledge%20Equations%20%28KEs%29%2C%0Aa%20novel%20language%20that%20we%20designed%2C%20theoretically%20grounded%20in%20assertional%20logic.%0ANext%2C%20we%20convert%20them%20to%20target%20languages%20through%20rigorously%20defined%20rules%20that%0Apreserve%20both%20syntactic%20structure%20and%20semantic%20meaning.%20This%20process%20yielded%20a%0Aparallel%20corpus%20of%20over%2060%2C000%20problems.%20Our%20framework%20achieves%2088.9%25%20syntactic%0Aaccuracy%20%28pass%401%29%20on%20MiniF2F%2C%20outperforming%20SOTA%20models%20such%20as%20Deepseek-V3%0A%2881%25%29%20and%20Herald%20%2881.3%25%29%20across%20multiple%20datasets.%20All%20datasets%20and%20codes%20are%0Aavailable%20in%20the%20supplementary%20materials.%0A&entry.1838667208=http%3A//arxiv.org/abs/2507.08665v1&entry.124074799=Read"},
{"title": "Remote Sensing Reveals Adoption of Sustainable Rice Farming Practices\n  Across Punjab, India", "author": "Ando Shah and Rajveer Singh and Akram Zaytar and Girmaw Abebe Tadesse and Caleb Robinson and Negar Tafti and Stephen A. Wood and Rahul Dodhia and Juan M. Lavista Ferres", "abstract": "  Rice cultivation consumes 24-30% of global freshwater, creating critical\nwater management challenges in major rice-producing regions. Sustainable\nirrigation practices like direct seeded rice (DSR) and alternate wetting and\ndrying (AWD) can reduce water use by 20-40% while maintaining yields, helping\nsecure long-term agricultural productivity as water scarcity intensifies - a\nkey component of the Zero Hunger Sustainable Development Goal. However, limited\ndata on adoption rates of these practices prevents evidence-based policymaking\nand targeted resource allocation. We developed a novel remote sensing framework\nto monitor sustainable water management practices at scale in Punjab, India - a\nregion facing severe groundwater depletion of 41.6 cm/year. To collect\nessential ground truth data, we partnered with the Nature Conservancy's\nPromoting Regenerative and No-burn Agriculture (PRANA) program, which trained\napproximately 1,400 farmers on water-saving techniques while documenting their\nfield-level practices. Using this data, we created a classification system with\nSentinel-1 satellite imagery that separates water management along sowing and\nirrigation dimensions. Our approach achieved a 78% F1-score in distinguishing\nDSR from traditional puddled transplanted rice without requiring prior\nknowledge of planting dates. We demonstrated scalability by mapping DSR\nadoption across approximately 3 million agricultural plots in Punjab, with\ndistrict-level predictions showing strong correlation (Pearson=0.77, RBO= 0.77)\nwith government records. This study provides policymakers with a powerful tool\nto track sustainable water management adoption, target interventions, and\nmeasure program impacts at scale.\n", "link": "http://arxiv.org/abs/2507.08605v1", "date": "2025-07-11", "relevancy": 1.5201, "topK": [{"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.3869}, {"title": "Scaling Rectified Flow Transformers for High-Resolution Image Synthesis", "link": "http://arxiv.org/abs/2403.03206v1", "similarity": 0.3752}, {"title": "RTG-SLAM: Real-time 3D Reconstruction at Scale using Gaussian Splatting", "link": "http://arxiv.org/abs/2404.19706v1", "similarity": 0.3751}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Remote%20Sensing%20Reveals%20Adoption%20of%20Sustainable%20Rice%20Farming%20Practices%0A%20%20Across%20Punjab%2C%20India&body=Title%3A%20Remote%20Sensing%20Reveals%20Adoption%20of%20Sustainable%20Rice%20Farming%20Practices%0A%20%20Across%20Punjab%2C%20India%0AAuthor%3A%20Ando%20Shah%20and%20Rajveer%20Singh%20and%20Akram%20Zaytar%20and%20Girmaw%20Abebe%20Tadesse%20and%20Caleb%20Robinson%20and%20Negar%20Tafti%20and%20Stephen%20A.%20Wood%20and%20Rahul%20Dodhia%20and%20Juan%20M.%20Lavista%20Ferres%0AAbstract%3A%20%20%20Rice%20cultivation%20consumes%2024-30%25%20of%20global%20freshwater%2C%20creating%20critical%0Awater%20management%20challenges%20in%20major%20rice-producing%20regions.%20Sustainable%0Airrigation%20practices%20like%20direct%20seeded%20rice%20%28DSR%29%20and%20alternate%20wetting%20and%0Adrying%20%28AWD%29%20can%20reduce%20water%20use%20by%2020-40%25%20while%20maintaining%20yields%2C%20helping%0Asecure%20long-term%20agricultural%20productivity%20as%20water%20scarcity%20intensifies%20-%20a%0Akey%20component%20of%20the%20Zero%20Hunger%20Sustainable%20Development%20Goal.%20However%2C%20limited%0Adata%20on%20adoption%20rates%20of%20these%20practices%20prevents%20evidence-based%20policymaking%0Aand%20targeted%20resource%20allocation.%20We%20developed%20a%20novel%20remote%20sensing%20framework%0Ato%20monitor%20sustainable%20water%20management%20practices%20at%20scale%20in%20Punjab%2C%20India%20-%20a%0Aregion%20facing%20severe%20groundwater%20depletion%20of%2041.6%20cm/year.%20To%20collect%0Aessential%20ground%20truth%20data%2C%20we%20partnered%20with%20the%20Nature%20Conservancy%27s%0APromoting%20Regenerative%20and%20No-burn%20Agriculture%20%28PRANA%29%20program%2C%20which%20trained%0Aapproximately%201%2C400%20farmers%20on%20water-saving%20techniques%20while%20documenting%20their%0Afield-level%20practices.%20Using%20this%20data%2C%20we%20created%20a%20classification%20system%20with%0ASentinel-1%20satellite%20imagery%20that%20separates%20water%20management%20along%20sowing%20and%0Airrigation%20dimensions.%20Our%20approach%20achieved%20a%2078%25%20F1-score%20in%20distinguishing%0ADSR%20from%20traditional%20puddled%20transplanted%20rice%20without%20requiring%20prior%0Aknowledge%20of%20planting%20dates.%20We%20demonstrated%20scalability%20by%20mapping%20DSR%0Aadoption%20across%20approximately%203%20million%20agricultural%20plots%20in%20Punjab%2C%20with%0Adistrict-level%20predictions%20showing%20strong%20correlation%20%28Pearson%3D0.77%2C%20RBO%3D%200.77%29%0Awith%20government%20records.%20This%20study%20provides%20policymakers%20with%20a%20powerful%20tool%0Ato%20track%20sustainable%20water%20management%20adoption%2C%20target%20interventions%2C%20and%0Ameasure%20program%20impacts%20at%20scale.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2507.08605v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DRemote%2520Sensing%2520Reveals%2520Adoption%2520of%2520Sustainable%2520Rice%2520Farming%2520Practices%250A%2520%2520Across%2520Punjab%252C%2520India%26entry.906535625%3DAndo%2520Shah%2520and%2520Rajveer%2520Singh%2520and%2520Akram%2520Zaytar%2520and%2520Girmaw%2520Abebe%2520Tadesse%2520and%2520Caleb%2520Robinson%2520and%2520Negar%2520Tafti%2520and%2520Stephen%2520A.%2520Wood%2520and%2520Rahul%2520Dodhia%2520and%2520Juan%2520M.%2520Lavista%2520Ferres%26entry.1292438233%3D%2520%2520Rice%2520cultivation%2520consumes%252024-30%2525%2520of%2520global%2520freshwater%252C%2520creating%2520critical%250Awater%2520management%2520challenges%2520in%2520major%2520rice-producing%2520regions.%2520Sustainable%250Airrigation%2520practices%2520like%2520direct%2520seeded%2520rice%2520%2528DSR%2529%2520and%2520alternate%2520wetting%2520and%250Adrying%2520%2528AWD%2529%2520can%2520reduce%2520water%2520use%2520by%252020-40%2525%2520while%2520maintaining%2520yields%252C%2520helping%250Asecure%2520long-term%2520agricultural%2520productivity%2520as%2520water%2520scarcity%2520intensifies%2520-%2520a%250Akey%2520component%2520of%2520the%2520Zero%2520Hunger%2520Sustainable%2520Development%2520Goal.%2520However%252C%2520limited%250Adata%2520on%2520adoption%2520rates%2520of%2520these%2520practices%2520prevents%2520evidence-based%2520policymaking%250Aand%2520targeted%2520resource%2520allocation.%2520We%2520developed%2520a%2520novel%2520remote%2520sensing%2520framework%250Ato%2520monitor%2520sustainable%2520water%2520management%2520practices%2520at%2520scale%2520in%2520Punjab%252C%2520India%2520-%2520a%250Aregion%2520facing%2520severe%2520groundwater%2520depletion%2520of%252041.6%2520cm/year.%2520To%2520collect%250Aessential%2520ground%2520truth%2520data%252C%2520we%2520partnered%2520with%2520the%2520Nature%2520Conservancy%2527s%250APromoting%2520Regenerative%2520and%2520No-burn%2520Agriculture%2520%2528PRANA%2529%2520program%252C%2520which%2520trained%250Aapproximately%25201%252C400%2520farmers%2520on%2520water-saving%2520techniques%2520while%2520documenting%2520their%250Afield-level%2520practices.%2520Using%2520this%2520data%252C%2520we%2520created%2520a%2520classification%2520system%2520with%250ASentinel-1%2520satellite%2520imagery%2520that%2520separates%2520water%2520management%2520along%2520sowing%2520and%250Airrigation%2520dimensions.%2520Our%2520approach%2520achieved%2520a%252078%2525%2520F1-score%2520in%2520distinguishing%250ADSR%2520from%2520traditional%2520puddled%2520transplanted%2520rice%2520without%2520requiring%2520prior%250Aknowledge%2520of%2520planting%2520dates.%2520We%2520demonstrated%2520scalability%2520by%2520mapping%2520DSR%250Aadoption%2520across%2520approximately%25203%2520million%2520agricultural%2520plots%2520in%2520Punjab%252C%2520with%250Adistrict-level%2520predictions%2520showing%2520strong%2520correlation%2520%2528Pearson%253D0.77%252C%2520RBO%253D%25200.77%2529%250Awith%2520government%2520records.%2520This%2520study%2520provides%2520policymakers%2520with%2520a%2520powerful%2520tool%250Ato%2520track%2520sustainable%2520water%2520management%2520adoption%252C%2520target%2520interventions%252C%2520and%250Ameasure%2520program%2520impacts%2520at%2520scale.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2507.08605v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Remote%20Sensing%20Reveals%20Adoption%20of%20Sustainable%20Rice%20Farming%20Practices%0A%20%20Across%20Punjab%2C%20India&entry.906535625=Ando%20Shah%20and%20Rajveer%20Singh%20and%20Akram%20Zaytar%20and%20Girmaw%20Abebe%20Tadesse%20and%20Caleb%20Robinson%20and%20Negar%20Tafti%20and%20Stephen%20A.%20Wood%20and%20Rahul%20Dodhia%20and%20Juan%20M.%20Lavista%20Ferres&entry.1292438233=%20%20Rice%20cultivation%20consumes%2024-30%25%20of%20global%20freshwater%2C%20creating%20critical%0Awater%20management%20challenges%20in%20major%20rice-producing%20regions.%20Sustainable%0Airrigation%20practices%20like%20direct%20seeded%20rice%20%28DSR%29%20and%20alternate%20wetting%20and%0Adrying%20%28AWD%29%20can%20reduce%20water%20use%20by%2020-40%25%20while%20maintaining%20yields%2C%20helping%0Asecure%20long-term%20agricultural%20productivity%20as%20water%20scarcity%20intensifies%20-%20a%0Akey%20component%20of%20the%20Zero%20Hunger%20Sustainable%20Development%20Goal.%20However%2C%20limited%0Adata%20on%20adoption%20rates%20of%20these%20practices%20prevents%20evidence-based%20policymaking%0Aand%20targeted%20resource%20allocation.%20We%20developed%20a%20novel%20remote%20sensing%20framework%0Ato%20monitor%20sustainable%20water%20management%20practices%20at%20scale%20in%20Punjab%2C%20India%20-%20a%0Aregion%20facing%20severe%20groundwater%20depletion%20of%2041.6%20cm/year.%20To%20collect%0Aessential%20ground%20truth%20data%2C%20we%20partnered%20with%20the%20Nature%20Conservancy%27s%0APromoting%20Regenerative%20and%20No-burn%20Agriculture%20%28PRANA%29%20program%2C%20which%20trained%0Aapproximately%201%2C400%20farmers%20on%20water-saving%20techniques%20while%20documenting%20their%0Afield-level%20practices.%20Using%20this%20data%2C%20we%20created%20a%20classification%20system%20with%0ASentinel-1%20satellite%20imagery%20that%20separates%20water%20management%20along%20sowing%20and%0Airrigation%20dimensions.%20Our%20approach%20achieved%20a%2078%25%20F1-score%20in%20distinguishing%0ADSR%20from%20traditional%20puddled%20transplanted%20rice%20without%20requiring%20prior%0Aknowledge%20of%20planting%20dates.%20We%20demonstrated%20scalability%20by%20mapping%20DSR%0Aadoption%20across%20approximately%203%20million%20agricultural%20plots%20in%20Punjab%2C%20with%0Adistrict-level%20predictions%20showing%20strong%20correlation%20%28Pearson%3D0.77%2C%20RBO%3D%200.77%29%0Awith%20government%20records.%20This%20study%20provides%20policymakers%20with%20a%20powerful%20tool%0Ato%20track%20sustainable%20water%20management%20adoption%2C%20target%20interventions%2C%20and%0Ameasure%20program%20impacts%20at%20scale.%0A&entry.1838667208=http%3A//arxiv.org/abs/2507.08605v1&entry.124074799=Read"},
{"title": "Dually Hierarchical Drift Adaptation for Online Configuration\n  Performance Learning", "author": "Zezhen Xiang and Jingzhi Gong and Tao Chen", "abstract": "  Modern configurable software systems need to learn models that correlate\nconfiguration and performance. However, when the system operates in dynamic\nenvironments, the workload variations, hardware changes, and system updates\nwill inevitably introduce concept drifts at different levels - global drifts,\nwhich reshape the performance landscape of the entire configuration space; and\nlocal drifts, which only affect certain sub-regions of that space. As such,\nexisting offline and transfer learning approaches can struggle to adapt to\nthese implicit and unpredictable changes in real-time, rendering configuration\nperformance learning challenging. To address this, we propose DHDA, an online\nconfiguration performance learning framework designed to capture and adapt to\nthese drifts at different levels. The key idea is that DHDA adapts to both the\nlocal and global drifts using dually hierarchical adaptation: at the upper\nlevel, we redivide the data into different divisions, within each of which the\nlocal model is retrained, to handle global drifts only when necessary. At the\nlower level, the local models of the divisions can detect local drifts and\nadapt themselves asynchronously. To balance responsiveness and efficiency, DHDA\ncombines incremental updates with periodic full retraining to minimize\nredundant computation when no drifts are detected. Through evaluating eight\nsoftware systems and against state-of-the-art approaches, we show that DHDA\nachieves considerably better accuracy and can effectively adapt to drifts with\nup to 2x improvements, while incurring reasonable overhead and is able to\nimprove different local models in handling concept drift.\n", "link": "http://arxiv.org/abs/2507.08730v1", "date": "2025-07-11", "relevancy": 1.4841, "topK": [{"title": "SDXS: Real-Time One-Step Latent Diffusion Models with Image Conditions", "link": "http://arxiv.org/abs/2403.16627v1", "similarity": 0.5061}, {"title": "Boosted dynamic neural networks", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/26302", "similarity": 0.5028}, {"title": "Multimodal-Conditioned Latent Diffusion Models for Fashion Image Editing", "link": "http://arxiv.org/abs/2403.14828v2", "similarity": 0.463}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Dually%20Hierarchical%20Drift%20Adaptation%20for%20Online%20Configuration%0A%20%20Performance%20Learning&body=Title%3A%20Dually%20Hierarchical%20Drift%20Adaptation%20for%20Online%20Configuration%0A%20%20Performance%20Learning%0AAuthor%3A%20Zezhen%20Xiang%20and%20Jingzhi%20Gong%20and%20Tao%20Chen%0AAbstract%3A%20%20%20Modern%20configurable%20software%20systems%20need%20to%20learn%20models%20that%20correlate%0Aconfiguration%20and%20performance.%20However%2C%20when%20the%20system%20operates%20in%20dynamic%0Aenvironments%2C%20the%20workload%20variations%2C%20hardware%20changes%2C%20and%20system%20updates%0Awill%20inevitably%20introduce%20concept%20drifts%20at%20different%20levels%20-%20global%20drifts%2C%0Awhich%20reshape%20the%20performance%20landscape%20of%20the%20entire%20configuration%20space%3B%20and%0Alocal%20drifts%2C%20which%20only%20affect%20certain%20sub-regions%20of%20that%20space.%20As%20such%2C%0Aexisting%20offline%20and%20transfer%20learning%20approaches%20can%20struggle%20to%20adapt%20to%0Athese%20implicit%20and%20unpredictable%20changes%20in%20real-time%2C%20rendering%20configuration%0Aperformance%20learning%20challenging.%20To%20address%20this%2C%20we%20propose%20DHDA%2C%20an%20online%0Aconfiguration%20performance%20learning%20framework%20designed%20to%20capture%20and%20adapt%20to%0Athese%20drifts%20at%20different%20levels.%20The%20key%20idea%20is%20that%20DHDA%20adapts%20to%20both%20the%0Alocal%20and%20global%20drifts%20using%20dually%20hierarchical%20adaptation%3A%20at%20the%20upper%0Alevel%2C%20we%20redivide%20the%20data%20into%20different%20divisions%2C%20within%20each%20of%20which%20the%0Alocal%20model%20is%20retrained%2C%20to%20handle%20global%20drifts%20only%20when%20necessary.%20At%20the%0Alower%20level%2C%20the%20local%20models%20of%20the%20divisions%20can%20detect%20local%20drifts%20and%0Aadapt%20themselves%20asynchronously.%20To%20balance%20responsiveness%20and%20efficiency%2C%20DHDA%0Acombines%20incremental%20updates%20with%20periodic%20full%20retraining%20to%20minimize%0Aredundant%20computation%20when%20no%20drifts%20are%20detected.%20Through%20evaluating%20eight%0Asoftware%20systems%20and%20against%20state-of-the-art%20approaches%2C%20we%20show%20that%20DHDA%0Aachieves%20considerably%20better%20accuracy%20and%20can%20effectively%20adapt%20to%20drifts%20with%0Aup%20to%202x%20improvements%2C%20while%20incurring%20reasonable%20overhead%20and%20is%20able%20to%0Aimprove%20different%20local%20models%20in%20handling%20concept%20drift.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2507.08730v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DDually%2520Hierarchical%2520Drift%2520Adaptation%2520for%2520Online%2520Configuration%250A%2520%2520Performance%2520Learning%26entry.906535625%3DZezhen%2520Xiang%2520and%2520Jingzhi%2520Gong%2520and%2520Tao%2520Chen%26entry.1292438233%3D%2520%2520Modern%2520configurable%2520software%2520systems%2520need%2520to%2520learn%2520models%2520that%2520correlate%250Aconfiguration%2520and%2520performance.%2520However%252C%2520when%2520the%2520system%2520operates%2520in%2520dynamic%250Aenvironments%252C%2520the%2520workload%2520variations%252C%2520hardware%2520changes%252C%2520and%2520system%2520updates%250Awill%2520inevitably%2520introduce%2520concept%2520drifts%2520at%2520different%2520levels%2520-%2520global%2520drifts%252C%250Awhich%2520reshape%2520the%2520performance%2520landscape%2520of%2520the%2520entire%2520configuration%2520space%253B%2520and%250Alocal%2520drifts%252C%2520which%2520only%2520affect%2520certain%2520sub-regions%2520of%2520that%2520space.%2520As%2520such%252C%250Aexisting%2520offline%2520and%2520transfer%2520learning%2520approaches%2520can%2520struggle%2520to%2520adapt%2520to%250Athese%2520implicit%2520and%2520unpredictable%2520changes%2520in%2520real-time%252C%2520rendering%2520configuration%250Aperformance%2520learning%2520challenging.%2520To%2520address%2520this%252C%2520we%2520propose%2520DHDA%252C%2520an%2520online%250Aconfiguration%2520performance%2520learning%2520framework%2520designed%2520to%2520capture%2520and%2520adapt%2520to%250Athese%2520drifts%2520at%2520different%2520levels.%2520The%2520key%2520idea%2520is%2520that%2520DHDA%2520adapts%2520to%2520both%2520the%250Alocal%2520and%2520global%2520drifts%2520using%2520dually%2520hierarchical%2520adaptation%253A%2520at%2520the%2520upper%250Alevel%252C%2520we%2520redivide%2520the%2520data%2520into%2520different%2520divisions%252C%2520within%2520each%2520of%2520which%2520the%250Alocal%2520model%2520is%2520retrained%252C%2520to%2520handle%2520global%2520drifts%2520only%2520when%2520necessary.%2520At%2520the%250Alower%2520level%252C%2520the%2520local%2520models%2520of%2520the%2520divisions%2520can%2520detect%2520local%2520drifts%2520and%250Aadapt%2520themselves%2520asynchronously.%2520To%2520balance%2520responsiveness%2520and%2520efficiency%252C%2520DHDA%250Acombines%2520incremental%2520updates%2520with%2520periodic%2520full%2520retraining%2520to%2520minimize%250Aredundant%2520computation%2520when%2520no%2520drifts%2520are%2520detected.%2520Through%2520evaluating%2520eight%250Asoftware%2520systems%2520and%2520against%2520state-of-the-art%2520approaches%252C%2520we%2520show%2520that%2520DHDA%250Aachieves%2520considerably%2520better%2520accuracy%2520and%2520can%2520effectively%2520adapt%2520to%2520drifts%2520with%250Aup%2520to%25202x%2520improvements%252C%2520while%2520incurring%2520reasonable%2520overhead%2520and%2520is%2520able%2520to%250Aimprove%2520different%2520local%2520models%2520in%2520handling%2520concept%2520drift.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2507.08730v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Dually%20Hierarchical%20Drift%20Adaptation%20for%20Online%20Configuration%0A%20%20Performance%20Learning&entry.906535625=Zezhen%20Xiang%20and%20Jingzhi%20Gong%20and%20Tao%20Chen&entry.1292438233=%20%20Modern%20configurable%20software%20systems%20need%20to%20learn%20models%20that%20correlate%0Aconfiguration%20and%20performance.%20However%2C%20when%20the%20system%20operates%20in%20dynamic%0Aenvironments%2C%20the%20workload%20variations%2C%20hardware%20changes%2C%20and%20system%20updates%0Awill%20inevitably%20introduce%20concept%20drifts%20at%20different%20levels%20-%20global%20drifts%2C%0Awhich%20reshape%20the%20performance%20landscape%20of%20the%20entire%20configuration%20space%3B%20and%0Alocal%20drifts%2C%20which%20only%20affect%20certain%20sub-regions%20of%20that%20space.%20As%20such%2C%0Aexisting%20offline%20and%20transfer%20learning%20approaches%20can%20struggle%20to%20adapt%20to%0Athese%20implicit%20and%20unpredictable%20changes%20in%20real-time%2C%20rendering%20configuration%0Aperformance%20learning%20challenging.%20To%20address%20this%2C%20we%20propose%20DHDA%2C%20an%20online%0Aconfiguration%20performance%20learning%20framework%20designed%20to%20capture%20and%20adapt%20to%0Athese%20drifts%20at%20different%20levels.%20The%20key%20idea%20is%20that%20DHDA%20adapts%20to%20both%20the%0Alocal%20and%20global%20drifts%20using%20dually%20hierarchical%20adaptation%3A%20at%20the%20upper%0Alevel%2C%20we%20redivide%20the%20data%20into%20different%20divisions%2C%20within%20each%20of%20which%20the%0Alocal%20model%20is%20retrained%2C%20to%20handle%20global%20drifts%20only%20when%20necessary.%20At%20the%0Alower%20level%2C%20the%20local%20models%20of%20the%20divisions%20can%20detect%20local%20drifts%20and%0Aadapt%20themselves%20asynchronously.%20To%20balance%20responsiveness%20and%20efficiency%2C%20DHDA%0Acombines%20incremental%20updates%20with%20periodic%20full%20retraining%20to%20minimize%0Aredundant%20computation%20when%20no%20drifts%20are%20detected.%20Through%20evaluating%20eight%0Asoftware%20systems%20and%20against%20state-of-the-art%20approaches%2C%20we%20show%20that%20DHDA%0Aachieves%20considerably%20better%20accuracy%20and%20can%20effectively%20adapt%20to%20drifts%20with%0Aup%20to%202x%20improvements%2C%20while%20incurring%20reasonable%20overhead%20and%20is%20able%20to%0Aimprove%20different%20local%20models%20in%20handling%20concept%20drift.%0A&entry.1838667208=http%3A//arxiv.org/abs/2507.08730v1&entry.124074799=Read"},
{"title": "Towards Collaborative Fairness in Federated Learning Under Imbalanced\n  Covariate Shift", "author": "Tianrun Yu and Jiaqi Wang and Haoyu Wang and Mingquan Lin and Han Liu and Nelson S. Yee and Fenglong Ma", "abstract": "  Collaborative fairness is a crucial challenge in federated learning. However,\nexisting approaches often overlook a practical yet complex form of\nheterogeneity: imbalanced covariate shift. We provide a theoretical analysis of\nthis setting, which motivates the design of FedAKD (Federated Asynchronous\nKnowledge Distillation)- simple yet effective approach that balances accurate\nprediction with collaborative fairness. FedAKD consists of client and server\nupdates. In the client update, we introduce a novel asynchronous knowledge\ndistillation strategy based on our preliminary analysis, which reveals that\nwhile correctly predicted samples exhibit similar feature distributions across\nclients, incorrectly predicted samples show significant variability. This\nsuggests that imbalanced covariate shift primarily arises from misclassified\nsamples. Leveraging this insight, our approach first applies traditional\nknowledge distillation to update client models while keeping the global model\nfixed. Next, we select correctly predicted high-confidence samples and update\nthe global model using these samples while keeping client models fixed. The\nserver update simply aggregates all client models. We further provide a\ntheoretical proof of FedAKD's convergence. Experimental results on public\ndatasets (FashionMNIST and CIFAR10) and a real-world Electronic Health Records\n(EHR) dataset demonstrate that FedAKD significantly improves collaborative\nfairness, enhances predictive accuracy, and fosters client participation even\nunder highly heterogeneous data distributions.\n", "link": "http://arxiv.org/abs/2507.08617v1", "date": "2025-07-11", "relevancy": 1.4273, "topK": [{"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.4792}, {"title": "Multimodal-Conditioned Latent Diffusion Models for Fashion Image Editing", "link": "http://arxiv.org/abs/2403.14828v2", "similarity": 0.4754}, {"title": "Flexible Visual Recognition by Evidential Modeling of Confusion and Ignorance", "link": "http://openaccess.thecvf.com/content/ICCV2023/html/Fan_Flexible_Visual_Recognition_by_Evidential_Modeling_of_Confusion_and_Ignorance_ICCV_2023_paper.html", "similarity": 0.4746}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Towards%20Collaborative%20Fairness%20in%20Federated%20Learning%20Under%20Imbalanced%0A%20%20Covariate%20Shift&body=Title%3A%20Towards%20Collaborative%20Fairness%20in%20Federated%20Learning%20Under%20Imbalanced%0A%20%20Covariate%20Shift%0AAuthor%3A%20Tianrun%20Yu%20and%20Jiaqi%20Wang%20and%20Haoyu%20Wang%20and%20Mingquan%20Lin%20and%20Han%20Liu%20and%20Nelson%20S.%20Yee%20and%20Fenglong%20Ma%0AAbstract%3A%20%20%20Collaborative%20fairness%20is%20a%20crucial%20challenge%20in%20federated%20learning.%20However%2C%0Aexisting%20approaches%20often%20overlook%20a%20practical%20yet%20complex%20form%20of%0Aheterogeneity%3A%20imbalanced%20covariate%20shift.%20We%20provide%20a%20theoretical%20analysis%20of%0Athis%20setting%2C%20which%20motivates%20the%20design%20of%20FedAKD%20%28Federated%20Asynchronous%0AKnowledge%20Distillation%29-%20simple%20yet%20effective%20approach%20that%20balances%20accurate%0Aprediction%20with%20collaborative%20fairness.%20FedAKD%20consists%20of%20client%20and%20server%0Aupdates.%20In%20the%20client%20update%2C%20we%20introduce%20a%20novel%20asynchronous%20knowledge%0Adistillation%20strategy%20based%20on%20our%20preliminary%20analysis%2C%20which%20reveals%20that%0Awhile%20correctly%20predicted%20samples%20exhibit%20similar%20feature%20distributions%20across%0Aclients%2C%20incorrectly%20predicted%20samples%20show%20significant%20variability.%20This%0Asuggests%20that%20imbalanced%20covariate%20shift%20primarily%20arises%20from%20misclassified%0Asamples.%20Leveraging%20this%20insight%2C%20our%20approach%20first%20applies%20traditional%0Aknowledge%20distillation%20to%20update%20client%20models%20while%20keeping%20the%20global%20model%0Afixed.%20Next%2C%20we%20select%20correctly%20predicted%20high-confidence%20samples%20and%20update%0Athe%20global%20model%20using%20these%20samples%20while%20keeping%20client%20models%20fixed.%20The%0Aserver%20update%20simply%20aggregates%20all%20client%20models.%20We%20further%20provide%20a%0Atheoretical%20proof%20of%20FedAKD%27s%20convergence.%20Experimental%20results%20on%20public%0Adatasets%20%28FashionMNIST%20and%20CIFAR10%29%20and%20a%20real-world%20Electronic%20Health%20Records%0A%28EHR%29%20dataset%20demonstrate%20that%20FedAKD%20significantly%20improves%20collaborative%0Afairness%2C%20enhances%20predictive%20accuracy%2C%20and%20fosters%20client%20participation%20even%0Aunder%20highly%20heterogeneous%20data%20distributions.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2507.08617v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DTowards%2520Collaborative%2520Fairness%2520in%2520Federated%2520Learning%2520Under%2520Imbalanced%250A%2520%2520Covariate%2520Shift%26entry.906535625%3DTianrun%2520Yu%2520and%2520Jiaqi%2520Wang%2520and%2520Haoyu%2520Wang%2520and%2520Mingquan%2520Lin%2520and%2520Han%2520Liu%2520and%2520Nelson%2520S.%2520Yee%2520and%2520Fenglong%2520Ma%26entry.1292438233%3D%2520%2520Collaborative%2520fairness%2520is%2520a%2520crucial%2520challenge%2520in%2520federated%2520learning.%2520However%252C%250Aexisting%2520approaches%2520often%2520overlook%2520a%2520practical%2520yet%2520complex%2520form%2520of%250Aheterogeneity%253A%2520imbalanced%2520covariate%2520shift.%2520We%2520provide%2520a%2520theoretical%2520analysis%2520of%250Athis%2520setting%252C%2520which%2520motivates%2520the%2520design%2520of%2520FedAKD%2520%2528Federated%2520Asynchronous%250AKnowledge%2520Distillation%2529-%2520simple%2520yet%2520effective%2520approach%2520that%2520balances%2520accurate%250Aprediction%2520with%2520collaborative%2520fairness.%2520FedAKD%2520consists%2520of%2520client%2520and%2520server%250Aupdates.%2520In%2520the%2520client%2520update%252C%2520we%2520introduce%2520a%2520novel%2520asynchronous%2520knowledge%250Adistillation%2520strategy%2520based%2520on%2520our%2520preliminary%2520analysis%252C%2520which%2520reveals%2520that%250Awhile%2520correctly%2520predicted%2520samples%2520exhibit%2520similar%2520feature%2520distributions%2520across%250Aclients%252C%2520incorrectly%2520predicted%2520samples%2520show%2520significant%2520variability.%2520This%250Asuggests%2520that%2520imbalanced%2520covariate%2520shift%2520primarily%2520arises%2520from%2520misclassified%250Asamples.%2520Leveraging%2520this%2520insight%252C%2520our%2520approach%2520first%2520applies%2520traditional%250Aknowledge%2520distillation%2520to%2520update%2520client%2520models%2520while%2520keeping%2520the%2520global%2520model%250Afixed.%2520Next%252C%2520we%2520select%2520correctly%2520predicted%2520high-confidence%2520samples%2520and%2520update%250Athe%2520global%2520model%2520using%2520these%2520samples%2520while%2520keeping%2520client%2520models%2520fixed.%2520The%250Aserver%2520update%2520simply%2520aggregates%2520all%2520client%2520models.%2520We%2520further%2520provide%2520a%250Atheoretical%2520proof%2520of%2520FedAKD%2527s%2520convergence.%2520Experimental%2520results%2520on%2520public%250Adatasets%2520%2528FashionMNIST%2520and%2520CIFAR10%2529%2520and%2520a%2520real-world%2520Electronic%2520Health%2520Records%250A%2528EHR%2529%2520dataset%2520demonstrate%2520that%2520FedAKD%2520significantly%2520improves%2520collaborative%250Afairness%252C%2520enhances%2520predictive%2520accuracy%252C%2520and%2520fosters%2520client%2520participation%2520even%250Aunder%2520highly%2520heterogeneous%2520data%2520distributions.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2507.08617v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Towards%20Collaborative%20Fairness%20in%20Federated%20Learning%20Under%20Imbalanced%0A%20%20Covariate%20Shift&entry.906535625=Tianrun%20Yu%20and%20Jiaqi%20Wang%20and%20Haoyu%20Wang%20and%20Mingquan%20Lin%20and%20Han%20Liu%20and%20Nelson%20S.%20Yee%20and%20Fenglong%20Ma&entry.1292438233=%20%20Collaborative%20fairness%20is%20a%20crucial%20challenge%20in%20federated%20learning.%20However%2C%0Aexisting%20approaches%20often%20overlook%20a%20practical%20yet%20complex%20form%20of%0Aheterogeneity%3A%20imbalanced%20covariate%20shift.%20We%20provide%20a%20theoretical%20analysis%20of%0Athis%20setting%2C%20which%20motivates%20the%20design%20of%20FedAKD%20%28Federated%20Asynchronous%0AKnowledge%20Distillation%29-%20simple%20yet%20effective%20approach%20that%20balances%20accurate%0Aprediction%20with%20collaborative%20fairness.%20FedAKD%20consists%20of%20client%20and%20server%0Aupdates.%20In%20the%20client%20update%2C%20we%20introduce%20a%20novel%20asynchronous%20knowledge%0Adistillation%20strategy%20based%20on%20our%20preliminary%20analysis%2C%20which%20reveals%20that%0Awhile%20correctly%20predicted%20samples%20exhibit%20similar%20feature%20distributions%20across%0Aclients%2C%20incorrectly%20predicted%20samples%20show%20significant%20variability.%20This%0Asuggests%20that%20imbalanced%20covariate%20shift%20primarily%20arises%20from%20misclassified%0Asamples.%20Leveraging%20this%20insight%2C%20our%20approach%20first%20applies%20traditional%0Aknowledge%20distillation%20to%20update%20client%20models%20while%20keeping%20the%20global%20model%0Afixed.%20Next%2C%20we%20select%20correctly%20predicted%20high-confidence%20samples%20and%20update%0Athe%20global%20model%20using%20these%20samples%20while%20keeping%20client%20models%20fixed.%20The%0Aserver%20update%20simply%20aggregates%20all%20client%20models.%20We%20further%20provide%20a%0Atheoretical%20proof%20of%20FedAKD%27s%20convergence.%20Experimental%20results%20on%20public%0Adatasets%20%28FashionMNIST%20and%20CIFAR10%29%20and%20a%20real-world%20Electronic%20Health%20Records%0A%28EHR%29%20dataset%20demonstrate%20that%20FedAKD%20significantly%20improves%20collaborative%0Afairness%2C%20enhances%20predictive%20accuracy%2C%20and%20fosters%20client%20participation%20even%0Aunder%20highly%20heterogeneous%20data%20distributions.%0A&entry.1838667208=http%3A//arxiv.org/abs/2507.08617v1&entry.124074799=Read"},
{"title": "System-of-systems Modeling and Optimization: An Integrated Framework for\n  Intermodal Mobility", "author": "Paul Saves and Jasper Bussemaker and R\u00e9mi Lafage and Thierry Lefebvre and Nathalie Bartoli and Youssef Diouane and Joseph Morlier", "abstract": "  For developing innovative systems architectures, modeling and optimization\ntechniques have been central to frame the architecting process and define the\noptimization and modeling problems. In this context, for system-of-systems the\nuse of efficient dedicated approaches (often physics-based simulations) is\nhighly recommended to reduce the computational complexity of the targeted\napplications. However, exploring novel architectures using such dedicated\napproaches might pose challenges for optimization algorithms, including\nincreased evaluation costs and potential failures. To address these challenges,\nsurrogate-based optimization algorithms, such as Bayesian optimization\nutilizing Gaussian process models have emerged.\n", "link": "http://arxiv.org/abs/2507.08715v1", "date": "2025-07-11", "relevancy": 1.4154, "topK": [{"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.5236}, {"title": "Thinking Outside the BBox: Unconstrained Generative Object Compositing", "link": "http://arxiv.org/abs/2409.04559v2", "similarity": 0.486}, {"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.4455}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20System-of-systems%20Modeling%20and%20Optimization%3A%20An%20Integrated%20Framework%20for%0A%20%20Intermodal%20Mobility&body=Title%3A%20System-of-systems%20Modeling%20and%20Optimization%3A%20An%20Integrated%20Framework%20for%0A%20%20Intermodal%20Mobility%0AAuthor%3A%20Paul%20Saves%20and%20Jasper%20Bussemaker%20and%20R%C3%A9mi%20Lafage%20and%20Thierry%20Lefebvre%20and%20Nathalie%20Bartoli%20and%20Youssef%20Diouane%20and%20Joseph%20Morlier%0AAbstract%3A%20%20%20For%20developing%20innovative%20systems%20architectures%2C%20modeling%20and%20optimization%0Atechniques%20have%20been%20central%20to%20frame%20the%20architecting%20process%20and%20define%20the%0Aoptimization%20and%20modeling%20problems.%20In%20this%20context%2C%20for%20system-of-systems%20the%0Ause%20of%20efficient%20dedicated%20approaches%20%28often%20physics-based%20simulations%29%20is%0Ahighly%20recommended%20to%20reduce%20the%20computational%20complexity%20of%20the%20targeted%0Aapplications.%20However%2C%20exploring%20novel%20architectures%20using%20such%20dedicated%0Aapproaches%20might%20pose%20challenges%20for%20optimization%20algorithms%2C%20including%0Aincreased%20evaluation%20costs%20and%20potential%20failures.%20To%20address%20these%20challenges%2C%0Asurrogate-based%20optimization%20algorithms%2C%20such%20as%20Bayesian%20optimization%0Autilizing%20Gaussian%20process%20models%20have%20emerged.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2507.08715v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DSystem-of-systems%2520Modeling%2520and%2520Optimization%253A%2520An%2520Integrated%2520Framework%2520for%250A%2520%2520Intermodal%2520Mobility%26entry.906535625%3DPaul%2520Saves%2520and%2520Jasper%2520Bussemaker%2520and%2520R%25C3%25A9mi%2520Lafage%2520and%2520Thierry%2520Lefebvre%2520and%2520Nathalie%2520Bartoli%2520and%2520Youssef%2520Diouane%2520and%2520Joseph%2520Morlier%26entry.1292438233%3D%2520%2520For%2520developing%2520innovative%2520systems%2520architectures%252C%2520modeling%2520and%2520optimization%250Atechniques%2520have%2520been%2520central%2520to%2520frame%2520the%2520architecting%2520process%2520and%2520define%2520the%250Aoptimization%2520and%2520modeling%2520problems.%2520In%2520this%2520context%252C%2520for%2520system-of-systems%2520the%250Ause%2520of%2520efficient%2520dedicated%2520approaches%2520%2528often%2520physics-based%2520simulations%2529%2520is%250Ahighly%2520recommended%2520to%2520reduce%2520the%2520computational%2520complexity%2520of%2520the%2520targeted%250Aapplications.%2520However%252C%2520exploring%2520novel%2520architectures%2520using%2520such%2520dedicated%250Aapproaches%2520might%2520pose%2520challenges%2520for%2520optimization%2520algorithms%252C%2520including%250Aincreased%2520evaluation%2520costs%2520and%2520potential%2520failures.%2520To%2520address%2520these%2520challenges%252C%250Asurrogate-based%2520optimization%2520algorithms%252C%2520such%2520as%2520Bayesian%2520optimization%250Autilizing%2520Gaussian%2520process%2520models%2520have%2520emerged.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2507.08715v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=System-of-systems%20Modeling%20and%20Optimization%3A%20An%20Integrated%20Framework%20for%0A%20%20Intermodal%20Mobility&entry.906535625=Paul%20Saves%20and%20Jasper%20Bussemaker%20and%20R%C3%A9mi%20Lafage%20and%20Thierry%20Lefebvre%20and%20Nathalie%20Bartoli%20and%20Youssef%20Diouane%20and%20Joseph%20Morlier&entry.1292438233=%20%20For%20developing%20innovative%20systems%20architectures%2C%20modeling%20and%20optimization%0Atechniques%20have%20been%20central%20to%20frame%20the%20architecting%20process%20and%20define%20the%0Aoptimization%20and%20modeling%20problems.%20In%20this%20context%2C%20for%20system-of-systems%20the%0Ause%20of%20efficient%20dedicated%20approaches%20%28often%20physics-based%20simulations%29%20is%0Ahighly%20recommended%20to%20reduce%20the%20computational%20complexity%20of%20the%20targeted%0Aapplications.%20However%2C%20exploring%20novel%20architectures%20using%20such%20dedicated%0Aapproaches%20might%20pose%20challenges%20for%20optimization%20algorithms%2C%20including%0Aincreased%20evaluation%20costs%20and%20potential%20failures.%20To%20address%20these%20challenges%2C%0Asurrogate-based%20optimization%20algorithms%2C%20such%20as%20Bayesian%20optimization%0Autilizing%20Gaussian%20process%20models%20have%20emerged.%0A&entry.1838667208=http%3A//arxiv.org/abs/2507.08715v1&entry.124074799=Read"},
{"title": "Discovering Algorithms with Computational Language Processing", "author": "Theo Bourdais and Abeynaya Gnanasekaran and Houman Owhadi and Tuhin Sahai", "abstract": "  Algorithms are the engine for reproducible problem-solving. We present a\nframework automating algorithm discovery by conceptualizing them as sequences\nof operations, represented as tokens. These computational tokens are chained\nusing a grammar, enabling the formation of increasingly sophisticated\nprocedures. Our ensemble Monte Carlo tree search (MCTS) guided by reinforcement\nlearning (RL) explores token chaining and drives the creation of new tokens.\nThis methodology rediscovers, improves, and generates new algorithms that\nsubstantially outperform existing methods for strongly NP-hard combinatorial\noptimization problems and foundational quantum computing approaches such as\nGrover's and Quantum Approximate Optimization Algorithm. Operating at the\ncomputational rather than code-generation level, our framework produces\nalgorithms that can be tailored specifically to problem instances, not merely\nclasses.\n", "link": "http://arxiv.org/abs/2507.03190v2", "date": "2025-07-11", "relevancy": 1.4084, "topK": [{"title": "RoboEXP: Action-Conditioned Scene Graph via Interactive Exploration for\n  Robotic Manipulation", "link": "http://arxiv.org/abs/2402.15487v1", "similarity": 0.5228}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.4591}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.4523}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Discovering%20Algorithms%20with%20Computational%20Language%20Processing&body=Title%3A%20Discovering%20Algorithms%20with%20Computational%20Language%20Processing%0AAuthor%3A%20Theo%20Bourdais%20and%20Abeynaya%20Gnanasekaran%20and%20Houman%20Owhadi%20and%20Tuhin%20Sahai%0AAbstract%3A%20%20%20Algorithms%20are%20the%20engine%20for%20reproducible%20problem-solving.%20We%20present%20a%0Aframework%20automating%20algorithm%20discovery%20by%20conceptualizing%20them%20as%20sequences%0Aof%20operations%2C%20represented%20as%20tokens.%20These%20computational%20tokens%20are%20chained%0Ausing%20a%20grammar%2C%20enabling%20the%20formation%20of%20increasingly%20sophisticated%0Aprocedures.%20Our%20ensemble%20Monte%20Carlo%20tree%20search%20%28MCTS%29%20guided%20by%20reinforcement%0Alearning%20%28RL%29%20explores%20token%20chaining%20and%20drives%20the%20creation%20of%20new%20tokens.%0AThis%20methodology%20rediscovers%2C%20improves%2C%20and%20generates%20new%20algorithms%20that%0Asubstantially%20outperform%20existing%20methods%20for%20strongly%20NP-hard%20combinatorial%0Aoptimization%20problems%20and%20foundational%20quantum%20computing%20approaches%20such%20as%0AGrover%27s%20and%20Quantum%20Approximate%20Optimization%20Algorithm.%20Operating%20at%20the%0Acomputational%20rather%20than%20code-generation%20level%2C%20our%20framework%20produces%0Aalgorithms%20that%20can%20be%20tailored%20specifically%20to%20problem%20instances%2C%20not%20merely%0Aclasses.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2507.03190v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DDiscovering%2520Algorithms%2520with%2520Computational%2520Language%2520Processing%26entry.906535625%3DTheo%2520Bourdais%2520and%2520Abeynaya%2520Gnanasekaran%2520and%2520Houman%2520Owhadi%2520and%2520Tuhin%2520Sahai%26entry.1292438233%3D%2520%2520Algorithms%2520are%2520the%2520engine%2520for%2520reproducible%2520problem-solving.%2520We%2520present%2520a%250Aframework%2520automating%2520algorithm%2520discovery%2520by%2520conceptualizing%2520them%2520as%2520sequences%250Aof%2520operations%252C%2520represented%2520as%2520tokens.%2520These%2520computational%2520tokens%2520are%2520chained%250Ausing%2520a%2520grammar%252C%2520enabling%2520the%2520formation%2520of%2520increasingly%2520sophisticated%250Aprocedures.%2520Our%2520ensemble%2520Monte%2520Carlo%2520tree%2520search%2520%2528MCTS%2529%2520guided%2520by%2520reinforcement%250Alearning%2520%2528RL%2529%2520explores%2520token%2520chaining%2520and%2520drives%2520the%2520creation%2520of%2520new%2520tokens.%250AThis%2520methodology%2520rediscovers%252C%2520improves%252C%2520and%2520generates%2520new%2520algorithms%2520that%250Asubstantially%2520outperform%2520existing%2520methods%2520for%2520strongly%2520NP-hard%2520combinatorial%250Aoptimization%2520problems%2520and%2520foundational%2520quantum%2520computing%2520approaches%2520such%2520as%250AGrover%2527s%2520and%2520Quantum%2520Approximate%2520Optimization%2520Algorithm.%2520Operating%2520at%2520the%250Acomputational%2520rather%2520than%2520code-generation%2520level%252C%2520our%2520framework%2520produces%250Aalgorithms%2520that%2520can%2520be%2520tailored%2520specifically%2520to%2520problem%2520instances%252C%2520not%2520merely%250Aclasses.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2507.03190v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Discovering%20Algorithms%20with%20Computational%20Language%20Processing&entry.906535625=Theo%20Bourdais%20and%20Abeynaya%20Gnanasekaran%20and%20Houman%20Owhadi%20and%20Tuhin%20Sahai&entry.1292438233=%20%20Algorithms%20are%20the%20engine%20for%20reproducible%20problem-solving.%20We%20present%20a%0Aframework%20automating%20algorithm%20discovery%20by%20conceptualizing%20them%20as%20sequences%0Aof%20operations%2C%20represented%20as%20tokens.%20These%20computational%20tokens%20are%20chained%0Ausing%20a%20grammar%2C%20enabling%20the%20formation%20of%20increasingly%20sophisticated%0Aprocedures.%20Our%20ensemble%20Monte%20Carlo%20tree%20search%20%28MCTS%29%20guided%20by%20reinforcement%0Alearning%20%28RL%29%20explores%20token%20chaining%20and%20drives%20the%20creation%20of%20new%20tokens.%0AThis%20methodology%20rediscovers%2C%20improves%2C%20and%20generates%20new%20algorithms%20that%0Asubstantially%20outperform%20existing%20methods%20for%20strongly%20NP-hard%20combinatorial%0Aoptimization%20problems%20and%20foundational%20quantum%20computing%20approaches%20such%20as%0AGrover%27s%20and%20Quantum%20Approximate%20Optimization%20Algorithm.%20Operating%20at%20the%0Acomputational%20rather%20than%20code-generation%20level%2C%20our%20framework%20produces%0Aalgorithms%20that%20can%20be%20tailored%20specifically%20to%20problem%20instances%2C%20not%20merely%0Aclasses.%0A&entry.1838667208=http%3A//arxiv.org/abs/2507.03190v2&entry.124074799=Read"},
{"title": "Penalizing Infeasible Actions and Reward Scaling in Reinforcement\n  Learning with Offline Data", "author": "Jeonghye Kim and Yongjae Shin and Whiyoung Jung and Sunghoon Hong and Deunsol Yoon and Youngchul Sung and Kanghoon Lee and Woohyung Lim", "abstract": "  Reinforcement learning with offline data suffers from Q-value extrapolation\nerrors. To address this issue, we first demonstrate that linear extrapolation\nof the Q-function beyond the data range is particularly problematic. To\nmitigate this, we propose guiding the gradual decrease of Q-values outside the\ndata range, which is achieved through reward scaling with layer normalization\n(RS-LN) and a penalization mechanism for infeasible actions (PA). By combining\nRS-LN and PA, we develop a new algorithm called PARS. We evaluate PARS across a\nrange of tasks, demonstrating superior performance compared to state-of-the-art\nalgorithms in both offline training and online fine-tuning on the D4RL\nbenchmark, with notable success in the challenging AntMaze Ultra task.\n", "link": "http://arxiv.org/abs/2507.08761v1", "date": "2025-07-11", "relevancy": 1.3839, "topK": [{"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.4801}, {"title": "Safe and efficient training of a control agent", "link": "https://patents.google.com/patent/US11709462B2/en", "similarity": 0.4564}, {"title": "RoboEXP: Action-Conditioned Scene Graph via Interactive Exploration for\n  Robotic Manipulation", "link": "http://arxiv.org/abs/2402.15487v1", "similarity": 0.4548}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Penalizing%20Infeasible%20Actions%20and%20Reward%20Scaling%20in%20Reinforcement%0A%20%20Learning%20with%20Offline%20Data&body=Title%3A%20Penalizing%20Infeasible%20Actions%20and%20Reward%20Scaling%20in%20Reinforcement%0A%20%20Learning%20with%20Offline%20Data%0AAuthor%3A%20Jeonghye%20Kim%20and%20Yongjae%20Shin%20and%20Whiyoung%20Jung%20and%20Sunghoon%20Hong%20and%20Deunsol%20Yoon%20and%20Youngchul%20Sung%20and%20Kanghoon%20Lee%20and%20Woohyung%20Lim%0AAbstract%3A%20%20%20Reinforcement%20learning%20with%20offline%20data%20suffers%20from%20Q-value%20extrapolation%0Aerrors.%20To%20address%20this%20issue%2C%20we%20first%20demonstrate%20that%20linear%20extrapolation%0Aof%20the%20Q-function%20beyond%20the%20data%20range%20is%20particularly%20problematic.%20To%0Amitigate%20this%2C%20we%20propose%20guiding%20the%20gradual%20decrease%20of%20Q-values%20outside%20the%0Adata%20range%2C%20which%20is%20achieved%20through%20reward%20scaling%20with%20layer%20normalization%0A%28RS-LN%29%20and%20a%20penalization%20mechanism%20for%20infeasible%20actions%20%28PA%29.%20By%20combining%0ARS-LN%20and%20PA%2C%20we%20develop%20a%20new%20algorithm%20called%20PARS.%20We%20evaluate%20PARS%20across%20a%0Arange%20of%20tasks%2C%20demonstrating%20superior%20performance%20compared%20to%20state-of-the-art%0Aalgorithms%20in%20both%20offline%20training%20and%20online%20fine-tuning%20on%20the%20D4RL%0Abenchmark%2C%20with%20notable%20success%20in%20the%20challenging%20AntMaze%20Ultra%20task.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2507.08761v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DPenalizing%2520Infeasible%2520Actions%2520and%2520Reward%2520Scaling%2520in%2520Reinforcement%250A%2520%2520Learning%2520with%2520Offline%2520Data%26entry.906535625%3DJeonghye%2520Kim%2520and%2520Yongjae%2520Shin%2520and%2520Whiyoung%2520Jung%2520and%2520Sunghoon%2520Hong%2520and%2520Deunsol%2520Yoon%2520and%2520Youngchul%2520Sung%2520and%2520Kanghoon%2520Lee%2520and%2520Woohyung%2520Lim%26entry.1292438233%3D%2520%2520Reinforcement%2520learning%2520with%2520offline%2520data%2520suffers%2520from%2520Q-value%2520extrapolation%250Aerrors.%2520To%2520address%2520this%2520issue%252C%2520we%2520first%2520demonstrate%2520that%2520linear%2520extrapolation%250Aof%2520the%2520Q-function%2520beyond%2520the%2520data%2520range%2520is%2520particularly%2520problematic.%2520To%250Amitigate%2520this%252C%2520we%2520propose%2520guiding%2520the%2520gradual%2520decrease%2520of%2520Q-values%2520outside%2520the%250Adata%2520range%252C%2520which%2520is%2520achieved%2520through%2520reward%2520scaling%2520with%2520layer%2520normalization%250A%2528RS-LN%2529%2520and%2520a%2520penalization%2520mechanism%2520for%2520infeasible%2520actions%2520%2528PA%2529.%2520By%2520combining%250ARS-LN%2520and%2520PA%252C%2520we%2520develop%2520a%2520new%2520algorithm%2520called%2520PARS.%2520We%2520evaluate%2520PARS%2520across%2520a%250Arange%2520of%2520tasks%252C%2520demonstrating%2520superior%2520performance%2520compared%2520to%2520state-of-the-art%250Aalgorithms%2520in%2520both%2520offline%2520training%2520and%2520online%2520fine-tuning%2520on%2520the%2520D4RL%250Abenchmark%252C%2520with%2520notable%2520success%2520in%2520the%2520challenging%2520AntMaze%2520Ultra%2520task.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2507.08761v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Penalizing%20Infeasible%20Actions%20and%20Reward%20Scaling%20in%20Reinforcement%0A%20%20Learning%20with%20Offline%20Data&entry.906535625=Jeonghye%20Kim%20and%20Yongjae%20Shin%20and%20Whiyoung%20Jung%20and%20Sunghoon%20Hong%20and%20Deunsol%20Yoon%20and%20Youngchul%20Sung%20and%20Kanghoon%20Lee%20and%20Woohyung%20Lim&entry.1292438233=%20%20Reinforcement%20learning%20with%20offline%20data%20suffers%20from%20Q-value%20extrapolation%0Aerrors.%20To%20address%20this%20issue%2C%20we%20first%20demonstrate%20that%20linear%20extrapolation%0Aof%20the%20Q-function%20beyond%20the%20data%20range%20is%20particularly%20problematic.%20To%0Amitigate%20this%2C%20we%20propose%20guiding%20the%20gradual%20decrease%20of%20Q-values%20outside%20the%0Adata%20range%2C%20which%20is%20achieved%20through%20reward%20scaling%20with%20layer%20normalization%0A%28RS-LN%29%20and%20a%20penalization%20mechanism%20for%20infeasible%20actions%20%28PA%29.%20By%20combining%0ARS-LN%20and%20PA%2C%20we%20develop%20a%20new%20algorithm%20called%20PARS.%20We%20evaluate%20PARS%20across%20a%0Arange%20of%20tasks%2C%20demonstrating%20superior%20performance%20compared%20to%20state-of-the-art%0Aalgorithms%20in%20both%20offline%20training%20and%20online%20fine-tuning%20on%20the%20D4RL%0Abenchmark%2C%20with%20notable%20success%20in%20the%20challenging%20AntMaze%20Ultra%20task.%0A&entry.1838667208=http%3A//arxiv.org/abs/2507.08761v1&entry.124074799=Read"},
{"title": "Learning-aided Bigraph Matching Approach to Multi-Crew Restoration of\n  Damaged Power Networks Coupled with Road Transportation Networks", "author": "Nathan Maurer and Harshal Kaushik and Roshni Anna Jacob and Jie Zhang and Souma Chowdhury", "abstract": "  The resilience of critical infrastructure networks (CINs) after disruptions,\nsuch as those caused by natural hazards, depends on both the speed of\nrestoration and the extent to which operational functionality can be regained.\nAllocating resources for restoration is a combinatorial optimal planning\nproblem that involves determining which crews will repair specific network\nnodes and in what order. This paper presents a novel graph-based formulation\nthat merges two interconnected graphs, representing crew and transportation\nnodes and power grid nodes, into a single heterogeneous graph. To enable\nefficient planning, graph reinforcement learning (GRL) is integrated with\nbigraph matching. GRL is utilized to design the incentive function for\nassigning crews to repair tasks based on the graph-abstracted state of the\nenvironment, ensuring generalization across damage scenarios. Two learning\ntechniques are employed: a graph neural network trained using Proximal Policy\nOptimization and another trained via Neuroevolution. The learned incentive\nfunctions inform a bipartite graph that links crews to repair tasks, enabling\nweighted maximum matching for crew-to-task allocations. An efficient simulation\nenvironment that pre-computes optimal node-to-node path plans is used to train\nthe proposed restoration planning methods. An IEEE 8500-bus power distribution\ntest network coupled with a 21 square km transportation network is used as the\ncase study, with scenarios varying in terms of numbers of damaged nodes,\ndepots, and crews. Results demonstrate the approach's generalizability and\nscalability across scenarios, with learned policies providing 3-fold better\nperformance than random policies, while also outperforming optimization-based\nsolutions in both computation time (by several orders of magnitude) and power\nrestored.\n", "link": "http://arxiv.org/abs/2506.19703v2", "date": "2025-07-11", "relevancy": 1.378, "topK": [{"title": "Self-supervised Photographic Image Layout Representation Learning", "link": "http://arxiv.org/abs/2403.03740v1", "similarity": 0.4661}, {"title": "RoboEXP: Action-Conditioned Scene Graph via Interactive Exploration for\n  Robotic Manipulation", "link": "http://arxiv.org/abs/2402.15487v1", "similarity": 0.4525}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.4494}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Learning-aided%20Bigraph%20Matching%20Approach%20to%20Multi-Crew%20Restoration%20of%0A%20%20Damaged%20Power%20Networks%20Coupled%20with%20Road%20Transportation%20Networks&body=Title%3A%20Learning-aided%20Bigraph%20Matching%20Approach%20to%20Multi-Crew%20Restoration%20of%0A%20%20Damaged%20Power%20Networks%20Coupled%20with%20Road%20Transportation%20Networks%0AAuthor%3A%20Nathan%20Maurer%20and%20Harshal%20Kaushik%20and%20Roshni%20Anna%20Jacob%20and%20Jie%20Zhang%20and%20Souma%20Chowdhury%0AAbstract%3A%20%20%20The%20resilience%20of%20critical%20infrastructure%20networks%20%28CINs%29%20after%20disruptions%2C%0Asuch%20as%20those%20caused%20by%20natural%20hazards%2C%20depends%20on%20both%20the%20speed%20of%0Arestoration%20and%20the%20extent%20to%20which%20operational%20functionality%20can%20be%20regained.%0AAllocating%20resources%20for%20restoration%20is%20a%20combinatorial%20optimal%20planning%0Aproblem%20that%20involves%20determining%20which%20crews%20will%20repair%20specific%20network%0Anodes%20and%20in%20what%20order.%20This%20paper%20presents%20a%20novel%20graph-based%20formulation%0Athat%20merges%20two%20interconnected%20graphs%2C%20representing%20crew%20and%20transportation%0Anodes%20and%20power%20grid%20nodes%2C%20into%20a%20single%20heterogeneous%20graph.%20To%20enable%0Aefficient%20planning%2C%20graph%20reinforcement%20learning%20%28GRL%29%20is%20integrated%20with%0Abigraph%20matching.%20GRL%20is%20utilized%20to%20design%20the%20incentive%20function%20for%0Aassigning%20crews%20to%20repair%20tasks%20based%20on%20the%20graph-abstracted%20state%20of%20the%0Aenvironment%2C%20ensuring%20generalization%20across%20damage%20scenarios.%20Two%20learning%0Atechniques%20are%20employed%3A%20a%20graph%20neural%20network%20trained%20using%20Proximal%20Policy%0AOptimization%20and%20another%20trained%20via%20Neuroevolution.%20The%20learned%20incentive%0Afunctions%20inform%20a%20bipartite%20graph%20that%20links%20crews%20to%20repair%20tasks%2C%20enabling%0Aweighted%20maximum%20matching%20for%20crew-to-task%20allocations.%20An%20efficient%20simulation%0Aenvironment%20that%20pre-computes%20optimal%20node-to-node%20path%20plans%20is%20used%20to%20train%0Athe%20proposed%20restoration%20planning%20methods.%20An%20IEEE%208500-bus%20power%20distribution%0Atest%20network%20coupled%20with%20a%2021%20square%20km%20transportation%20network%20is%20used%20as%20the%0Acase%20study%2C%20with%20scenarios%20varying%20in%20terms%20of%20numbers%20of%20damaged%20nodes%2C%0Adepots%2C%20and%20crews.%20Results%20demonstrate%20the%20approach%27s%20generalizability%20and%0Ascalability%20across%20scenarios%2C%20with%20learned%20policies%20providing%203-fold%20better%0Aperformance%20than%20random%20policies%2C%20while%20also%20outperforming%20optimization-based%0Asolutions%20in%20both%20computation%20time%20%28by%20several%20orders%20of%20magnitude%29%20and%20power%0Arestored.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2506.19703v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DLearning-aided%2520Bigraph%2520Matching%2520Approach%2520to%2520Multi-Crew%2520Restoration%2520of%250A%2520%2520Damaged%2520Power%2520Networks%2520Coupled%2520with%2520Road%2520Transportation%2520Networks%26entry.906535625%3DNathan%2520Maurer%2520and%2520Harshal%2520Kaushik%2520and%2520Roshni%2520Anna%2520Jacob%2520and%2520Jie%2520Zhang%2520and%2520Souma%2520Chowdhury%26entry.1292438233%3D%2520%2520The%2520resilience%2520of%2520critical%2520infrastructure%2520networks%2520%2528CINs%2529%2520after%2520disruptions%252C%250Asuch%2520as%2520those%2520caused%2520by%2520natural%2520hazards%252C%2520depends%2520on%2520both%2520the%2520speed%2520of%250Arestoration%2520and%2520the%2520extent%2520to%2520which%2520operational%2520functionality%2520can%2520be%2520regained.%250AAllocating%2520resources%2520for%2520restoration%2520is%2520a%2520combinatorial%2520optimal%2520planning%250Aproblem%2520that%2520involves%2520determining%2520which%2520crews%2520will%2520repair%2520specific%2520network%250Anodes%2520and%2520in%2520what%2520order.%2520This%2520paper%2520presents%2520a%2520novel%2520graph-based%2520formulation%250Athat%2520merges%2520two%2520interconnected%2520graphs%252C%2520representing%2520crew%2520and%2520transportation%250Anodes%2520and%2520power%2520grid%2520nodes%252C%2520into%2520a%2520single%2520heterogeneous%2520graph.%2520To%2520enable%250Aefficient%2520planning%252C%2520graph%2520reinforcement%2520learning%2520%2528GRL%2529%2520is%2520integrated%2520with%250Abigraph%2520matching.%2520GRL%2520is%2520utilized%2520to%2520design%2520the%2520incentive%2520function%2520for%250Aassigning%2520crews%2520to%2520repair%2520tasks%2520based%2520on%2520the%2520graph-abstracted%2520state%2520of%2520the%250Aenvironment%252C%2520ensuring%2520generalization%2520across%2520damage%2520scenarios.%2520Two%2520learning%250Atechniques%2520are%2520employed%253A%2520a%2520graph%2520neural%2520network%2520trained%2520using%2520Proximal%2520Policy%250AOptimization%2520and%2520another%2520trained%2520via%2520Neuroevolution.%2520The%2520learned%2520incentive%250Afunctions%2520inform%2520a%2520bipartite%2520graph%2520that%2520links%2520crews%2520to%2520repair%2520tasks%252C%2520enabling%250Aweighted%2520maximum%2520matching%2520for%2520crew-to-task%2520allocations.%2520An%2520efficient%2520simulation%250Aenvironment%2520that%2520pre-computes%2520optimal%2520node-to-node%2520path%2520plans%2520is%2520used%2520to%2520train%250Athe%2520proposed%2520restoration%2520planning%2520methods.%2520An%2520IEEE%25208500-bus%2520power%2520distribution%250Atest%2520network%2520coupled%2520with%2520a%252021%2520square%2520km%2520transportation%2520network%2520is%2520used%2520as%2520the%250Acase%2520study%252C%2520with%2520scenarios%2520varying%2520in%2520terms%2520of%2520numbers%2520of%2520damaged%2520nodes%252C%250Adepots%252C%2520and%2520crews.%2520Results%2520demonstrate%2520the%2520approach%2527s%2520generalizability%2520and%250Ascalability%2520across%2520scenarios%252C%2520with%2520learned%2520policies%2520providing%25203-fold%2520better%250Aperformance%2520than%2520random%2520policies%252C%2520while%2520also%2520outperforming%2520optimization-based%250Asolutions%2520in%2520both%2520computation%2520time%2520%2528by%2520several%2520orders%2520of%2520magnitude%2529%2520and%2520power%250Arestored.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2506.19703v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Learning-aided%20Bigraph%20Matching%20Approach%20to%20Multi-Crew%20Restoration%20of%0A%20%20Damaged%20Power%20Networks%20Coupled%20with%20Road%20Transportation%20Networks&entry.906535625=Nathan%20Maurer%20and%20Harshal%20Kaushik%20and%20Roshni%20Anna%20Jacob%20and%20Jie%20Zhang%20and%20Souma%20Chowdhury&entry.1292438233=%20%20The%20resilience%20of%20critical%20infrastructure%20networks%20%28CINs%29%20after%20disruptions%2C%0Asuch%20as%20those%20caused%20by%20natural%20hazards%2C%20depends%20on%20both%20the%20speed%20of%0Arestoration%20and%20the%20extent%20to%20which%20operational%20functionality%20can%20be%20regained.%0AAllocating%20resources%20for%20restoration%20is%20a%20combinatorial%20optimal%20planning%0Aproblem%20that%20involves%20determining%20which%20crews%20will%20repair%20specific%20network%0Anodes%20and%20in%20what%20order.%20This%20paper%20presents%20a%20novel%20graph-based%20formulation%0Athat%20merges%20two%20interconnected%20graphs%2C%20representing%20crew%20and%20transportation%0Anodes%20and%20power%20grid%20nodes%2C%20into%20a%20single%20heterogeneous%20graph.%20To%20enable%0Aefficient%20planning%2C%20graph%20reinforcement%20learning%20%28GRL%29%20is%20integrated%20with%0Abigraph%20matching.%20GRL%20is%20utilized%20to%20design%20the%20incentive%20function%20for%0Aassigning%20crews%20to%20repair%20tasks%20based%20on%20the%20graph-abstracted%20state%20of%20the%0Aenvironment%2C%20ensuring%20generalization%20across%20damage%20scenarios.%20Two%20learning%0Atechniques%20are%20employed%3A%20a%20graph%20neural%20network%20trained%20using%20Proximal%20Policy%0AOptimization%20and%20another%20trained%20via%20Neuroevolution.%20The%20learned%20incentive%0Afunctions%20inform%20a%20bipartite%20graph%20that%20links%20crews%20to%20repair%20tasks%2C%20enabling%0Aweighted%20maximum%20matching%20for%20crew-to-task%20allocations.%20An%20efficient%20simulation%0Aenvironment%20that%20pre-computes%20optimal%20node-to-node%20path%20plans%20is%20used%20to%20train%0Athe%20proposed%20restoration%20planning%20methods.%20An%20IEEE%208500-bus%20power%20distribution%0Atest%20network%20coupled%20with%20a%2021%20square%20km%20transportation%20network%20is%20used%20as%20the%0Acase%20study%2C%20with%20scenarios%20varying%20in%20terms%20of%20numbers%20of%20damaged%20nodes%2C%0Adepots%2C%20and%20crews.%20Results%20demonstrate%20the%20approach%27s%20generalizability%20and%0Ascalability%20across%20scenarios%2C%20with%20learned%20policies%20providing%203-fold%20better%0Aperformance%20than%20random%20policies%2C%20while%20also%20outperforming%20optimization-based%0Asolutions%20in%20both%20computation%20time%20%28by%20several%20orders%20of%20magnitude%29%20and%20power%0Arestored.%0A&entry.1838667208=http%3A//arxiv.org/abs/2506.19703v2&entry.124074799=Read"},
{"title": "Conditional regression for the Nonlinear Single-Variable Model", "author": "Yantao Wu and Mauro Maggioni", "abstract": "  Regressing a function $F$ on $\\mathbb{R}^d$ without the statistical and\ncomputational curse of dimensionality requires special statistical models, for\nexample that impose geometric assumptions on the distribution of the data\n(e.g., that its support is low-dimensional), or strong smoothness assumptions\non $F$, or a special structure $F$. Among the latter, compositional models\n$F=f\\circ g$ with $g$ mapping to $\\mathbb{R}^r$ with $r\\ll d$ include classical\nsingle- and multi-index models, as well as neural networks. While the case\nwhere $g$ is linear is well-understood, less is known when $g$ is nonlinear,\nand in particular for which $g$'s the curse of dimensionality in estimating\n$F$, or both $f$ and $g$, may be circumvented. Here we consider a model\n$F(X):=f(\\Pi_\\gamma X)$ where\n$\\Pi_\\gamma:\\mathbb{R}^d\\to[0,\\textrm{len}_\\gamma]$ is the closest-point\nprojection onto the parameter of a regular curve $\\gamma:[0,\n\\textrm{len}_\\gamma]\\to\\mathbb{R}^d$, and $f:[0,\\textrm{len}_\\gamma]\\to\n\\mathbb{R}^1$. The input data $X$ is not low-dimensional: it can be as far from\n$\\gamma$ as the condition that $\\Pi_\\gamma(X)$ is well-defined allows. The\ndistribution $X$, the curve $\\gamma$ and the function $f$ are all unknown. This\nmodel is a natural nonlinear generalization of the single-index model,\ncorresponding to $\\gamma$ being a line. We propose a nonparametric estimator,\nbased on conditional regression, that under suitable assumptions, the strongest\nof which being that $f$ is coarsely monotone, achieves, up to log factors, the\n$\\textit{one-dimensional}$ optimal min-max rate for non-parametric regression,\nup to the level of noise in the observations, and be constructed in time\n$\\mathcal{O}(d^2 n\\log n)$. All the constants in the learning bounds, in the\nminimal number of samples required for our bounds to hold, and in the\ncomputational complexity are at most low-order polynomials in $d$.\n", "link": "http://arxiv.org/abs/2411.09686v2", "date": "2025-07-11", "relevancy": 0.9589, "topK": [{"title": "SDXS: Real-Time One-Step Latent Diffusion Models with Image Conditions", "link": "http://arxiv.org/abs/2403.16627v1", "similarity": 0.4933}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.4781}, {"title": "Multimodal-Conditioned Latent Diffusion Models for Fashion Image Editing", "link": "http://arxiv.org/abs/2403.14828v2", "similarity": 0.4669}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Conditional%20regression%20for%20the%20Nonlinear%20Single-Variable%20Model&body=Title%3A%20Conditional%20regression%20for%20the%20Nonlinear%20Single-Variable%20Model%0AAuthor%3A%20Yantao%20Wu%20and%20Mauro%20Maggioni%0AAbstract%3A%20%20%20Regressing%20a%20function%20%24F%24%20on%20%24%5Cmathbb%7BR%7D%5Ed%24%20without%20the%20statistical%20and%0Acomputational%20curse%20of%20dimensionality%20requires%20special%20statistical%20models%2C%20for%0Aexample%20that%20impose%20geometric%20assumptions%20on%20the%20distribution%20of%20the%20data%0A%28e.g.%2C%20that%20its%20support%20is%20low-dimensional%29%2C%20or%20strong%20smoothness%20assumptions%0Aon%20%24F%24%2C%20or%20a%20special%20structure%20%24F%24.%20Among%20the%20latter%2C%20compositional%20models%0A%24F%3Df%5Ccirc%20g%24%20with%20%24g%24%20mapping%20to%20%24%5Cmathbb%7BR%7D%5Er%24%20with%20%24r%5Cll%20d%24%20include%20classical%0Asingle-%20and%20multi-index%20models%2C%20as%20well%20as%20neural%20networks.%20While%20the%20case%0Awhere%20%24g%24%20is%20linear%20is%20well-understood%2C%20less%20is%20known%20when%20%24g%24%20is%20nonlinear%2C%0Aand%20in%20particular%20for%20which%20%24g%24%27s%20the%20curse%20of%20dimensionality%20in%20estimating%0A%24F%24%2C%20or%20both%20%24f%24%20and%20%24g%24%2C%20may%20be%20circumvented.%20Here%20we%20consider%20a%20model%0A%24F%28X%29%3A%3Df%28%5CPi_%5Cgamma%20X%29%24%20where%0A%24%5CPi_%5Cgamma%3A%5Cmathbb%7BR%7D%5Ed%5Cto%5B0%2C%5Ctextrm%7Blen%7D_%5Cgamma%5D%24%20is%20the%20closest-point%0Aprojection%20onto%20the%20parameter%20of%20a%20regular%20curve%20%24%5Cgamma%3A%5B0%2C%0A%5Ctextrm%7Blen%7D_%5Cgamma%5D%5Cto%5Cmathbb%7BR%7D%5Ed%24%2C%20and%20%24f%3A%5B0%2C%5Ctextrm%7Blen%7D_%5Cgamma%5D%5Cto%0A%5Cmathbb%7BR%7D%5E1%24.%20The%20input%20data%20%24X%24%20is%20not%20low-dimensional%3A%20it%20can%20be%20as%20far%20from%0A%24%5Cgamma%24%20as%20the%20condition%20that%20%24%5CPi_%5Cgamma%28X%29%24%20is%20well-defined%20allows.%20The%0Adistribution%20%24X%24%2C%20the%20curve%20%24%5Cgamma%24%20and%20the%20function%20%24f%24%20are%20all%20unknown.%20This%0Amodel%20is%20a%20natural%20nonlinear%20generalization%20of%20the%20single-index%20model%2C%0Acorresponding%20to%20%24%5Cgamma%24%20being%20a%20line.%20We%20propose%20a%20nonparametric%20estimator%2C%0Abased%20on%20conditional%20regression%2C%20that%20under%20suitable%20assumptions%2C%20the%20strongest%0Aof%20which%20being%20that%20%24f%24%20is%20coarsely%20monotone%2C%20achieves%2C%20up%20to%20log%20factors%2C%20the%0A%24%5Ctextit%7Bone-dimensional%7D%24%20optimal%20min-max%20rate%20for%20non-parametric%20regression%2C%0Aup%20to%20the%20level%20of%20noise%20in%20the%20observations%2C%20and%20be%20constructed%20in%20time%0A%24%5Cmathcal%7BO%7D%28d%5E2%20n%5Clog%20n%29%24.%20All%20the%20constants%20in%20the%20learning%20bounds%2C%20in%20the%0Aminimal%20number%20of%20samples%20required%20for%20our%20bounds%20to%20hold%2C%20and%20in%20the%0Acomputational%20complexity%20are%20at%20most%20low-order%20polynomials%20in%20%24d%24.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2411.09686v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DConditional%2520regression%2520for%2520the%2520Nonlinear%2520Single-Variable%2520Model%26entry.906535625%3DYantao%2520Wu%2520and%2520Mauro%2520Maggioni%26entry.1292438233%3D%2520%2520Regressing%2520a%2520function%2520%2524F%2524%2520on%2520%2524%255Cmathbb%257BR%257D%255Ed%2524%2520without%2520the%2520statistical%2520and%250Acomputational%2520curse%2520of%2520dimensionality%2520requires%2520special%2520statistical%2520models%252C%2520for%250Aexample%2520that%2520impose%2520geometric%2520assumptions%2520on%2520the%2520distribution%2520of%2520the%2520data%250A%2528e.g.%252C%2520that%2520its%2520support%2520is%2520low-dimensional%2529%252C%2520or%2520strong%2520smoothness%2520assumptions%250Aon%2520%2524F%2524%252C%2520or%2520a%2520special%2520structure%2520%2524F%2524.%2520Among%2520the%2520latter%252C%2520compositional%2520models%250A%2524F%253Df%255Ccirc%2520g%2524%2520with%2520%2524g%2524%2520mapping%2520to%2520%2524%255Cmathbb%257BR%257D%255Er%2524%2520with%2520%2524r%255Cll%2520d%2524%2520include%2520classical%250Asingle-%2520and%2520multi-index%2520models%252C%2520as%2520well%2520as%2520neural%2520networks.%2520While%2520the%2520case%250Awhere%2520%2524g%2524%2520is%2520linear%2520is%2520well-understood%252C%2520less%2520is%2520known%2520when%2520%2524g%2524%2520is%2520nonlinear%252C%250Aand%2520in%2520particular%2520for%2520which%2520%2524g%2524%2527s%2520the%2520curse%2520of%2520dimensionality%2520in%2520estimating%250A%2524F%2524%252C%2520or%2520both%2520%2524f%2524%2520and%2520%2524g%2524%252C%2520may%2520be%2520circumvented.%2520Here%2520we%2520consider%2520a%2520model%250A%2524F%2528X%2529%253A%253Df%2528%255CPi_%255Cgamma%2520X%2529%2524%2520where%250A%2524%255CPi_%255Cgamma%253A%255Cmathbb%257BR%257D%255Ed%255Cto%255B0%252C%255Ctextrm%257Blen%257D_%255Cgamma%255D%2524%2520is%2520the%2520closest-point%250Aprojection%2520onto%2520the%2520parameter%2520of%2520a%2520regular%2520curve%2520%2524%255Cgamma%253A%255B0%252C%250A%255Ctextrm%257Blen%257D_%255Cgamma%255D%255Cto%255Cmathbb%257BR%257D%255Ed%2524%252C%2520and%2520%2524f%253A%255B0%252C%255Ctextrm%257Blen%257D_%255Cgamma%255D%255Cto%250A%255Cmathbb%257BR%257D%255E1%2524.%2520The%2520input%2520data%2520%2524X%2524%2520is%2520not%2520low-dimensional%253A%2520it%2520can%2520be%2520as%2520far%2520from%250A%2524%255Cgamma%2524%2520as%2520the%2520condition%2520that%2520%2524%255CPi_%255Cgamma%2528X%2529%2524%2520is%2520well-defined%2520allows.%2520The%250Adistribution%2520%2524X%2524%252C%2520the%2520curve%2520%2524%255Cgamma%2524%2520and%2520the%2520function%2520%2524f%2524%2520are%2520all%2520unknown.%2520This%250Amodel%2520is%2520a%2520natural%2520nonlinear%2520generalization%2520of%2520the%2520single-index%2520model%252C%250Acorresponding%2520to%2520%2524%255Cgamma%2524%2520being%2520a%2520line.%2520We%2520propose%2520a%2520nonparametric%2520estimator%252C%250Abased%2520on%2520conditional%2520regression%252C%2520that%2520under%2520suitable%2520assumptions%252C%2520the%2520strongest%250Aof%2520which%2520being%2520that%2520%2524f%2524%2520is%2520coarsely%2520monotone%252C%2520achieves%252C%2520up%2520to%2520log%2520factors%252C%2520the%250A%2524%255Ctextit%257Bone-dimensional%257D%2524%2520optimal%2520min-max%2520rate%2520for%2520non-parametric%2520regression%252C%250Aup%2520to%2520the%2520level%2520of%2520noise%2520in%2520the%2520observations%252C%2520and%2520be%2520constructed%2520in%2520time%250A%2524%255Cmathcal%257BO%257D%2528d%255E2%2520n%255Clog%2520n%2529%2524.%2520All%2520the%2520constants%2520in%2520the%2520learning%2520bounds%252C%2520in%2520the%250Aminimal%2520number%2520of%2520samples%2520required%2520for%2520our%2520bounds%2520to%2520hold%252C%2520and%2520in%2520the%250Acomputational%2520complexity%2520are%2520at%2520most%2520low-order%2520polynomials%2520in%2520%2524d%2524.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2411.09686v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Conditional%20regression%20for%20the%20Nonlinear%20Single-Variable%20Model&entry.906535625=Yantao%20Wu%20and%20Mauro%20Maggioni&entry.1292438233=%20%20Regressing%20a%20function%20%24F%24%20on%20%24%5Cmathbb%7BR%7D%5Ed%24%20without%20the%20statistical%20and%0Acomputational%20curse%20of%20dimensionality%20requires%20special%20statistical%20models%2C%20for%0Aexample%20that%20impose%20geometric%20assumptions%20on%20the%20distribution%20of%20the%20data%0A%28e.g.%2C%20that%20its%20support%20is%20low-dimensional%29%2C%20or%20strong%20smoothness%20assumptions%0Aon%20%24F%24%2C%20or%20a%20special%20structure%20%24F%24.%20Among%20the%20latter%2C%20compositional%20models%0A%24F%3Df%5Ccirc%20g%24%20with%20%24g%24%20mapping%20to%20%24%5Cmathbb%7BR%7D%5Er%24%20with%20%24r%5Cll%20d%24%20include%20classical%0Asingle-%20and%20multi-index%20models%2C%20as%20well%20as%20neural%20networks.%20While%20the%20case%0Awhere%20%24g%24%20is%20linear%20is%20well-understood%2C%20less%20is%20known%20when%20%24g%24%20is%20nonlinear%2C%0Aand%20in%20particular%20for%20which%20%24g%24%27s%20the%20curse%20of%20dimensionality%20in%20estimating%0A%24F%24%2C%20or%20both%20%24f%24%20and%20%24g%24%2C%20may%20be%20circumvented.%20Here%20we%20consider%20a%20model%0A%24F%28X%29%3A%3Df%28%5CPi_%5Cgamma%20X%29%24%20where%0A%24%5CPi_%5Cgamma%3A%5Cmathbb%7BR%7D%5Ed%5Cto%5B0%2C%5Ctextrm%7Blen%7D_%5Cgamma%5D%24%20is%20the%20closest-point%0Aprojection%20onto%20the%20parameter%20of%20a%20regular%20curve%20%24%5Cgamma%3A%5B0%2C%0A%5Ctextrm%7Blen%7D_%5Cgamma%5D%5Cto%5Cmathbb%7BR%7D%5Ed%24%2C%20and%20%24f%3A%5B0%2C%5Ctextrm%7Blen%7D_%5Cgamma%5D%5Cto%0A%5Cmathbb%7BR%7D%5E1%24.%20The%20input%20data%20%24X%24%20is%20not%20low-dimensional%3A%20it%20can%20be%20as%20far%20from%0A%24%5Cgamma%24%20as%20the%20condition%20that%20%24%5CPi_%5Cgamma%28X%29%24%20is%20well-defined%20allows.%20The%0Adistribution%20%24X%24%2C%20the%20curve%20%24%5Cgamma%24%20and%20the%20function%20%24f%24%20are%20all%20unknown.%20This%0Amodel%20is%20a%20natural%20nonlinear%20generalization%20of%20the%20single-index%20model%2C%0Acorresponding%20to%20%24%5Cgamma%24%20being%20a%20line.%20We%20propose%20a%20nonparametric%20estimator%2C%0Abased%20on%20conditional%20regression%2C%20that%20under%20suitable%20assumptions%2C%20the%20strongest%0Aof%20which%20being%20that%20%24f%24%20is%20coarsely%20monotone%2C%20achieves%2C%20up%20to%20log%20factors%2C%20the%0A%24%5Ctextit%7Bone-dimensional%7D%24%20optimal%20min-max%20rate%20for%20non-parametric%20regression%2C%0Aup%20to%20the%20level%20of%20noise%20in%20the%20observations%2C%20and%20be%20constructed%20in%20time%0A%24%5Cmathcal%7BO%7D%28d%5E2%20n%5Clog%20n%29%24.%20All%20the%20constants%20in%20the%20learning%20bounds%2C%20in%20the%0Aminimal%20number%20of%20samples%20required%20for%20our%20bounds%20to%20hold%2C%20and%20in%20the%0Acomputational%20complexity%20are%20at%20most%20low-order%20polynomials%20in%20%24d%24.%0A&entry.1838667208=http%3A//arxiv.org/abs/2411.09686v2&entry.124074799=Read"},
{"title": "Partitioned Hybrid Quantum Fourier Neural Operators for Scientific\n  Quantum Machine Learning", "author": "Paolo Marcandelli and Yuanchun He and Stefano Mariani and Martina Siena and Stefano Markidis", "abstract": "  We introduce the Partitioned Hybrid Quantum Fourier Neural Operator (PHQFNO),\na generalization of the Quantum Fourier Neural Operator (QFNO) for scientific\nmachine learning. PHQFNO partitions the Fourier operator computation across\nclassical and quantum resources, enabling tunable quantum-classical\nhybridization and distributed execution across quantum and classical devices.\nThe method extends QFNOs to higher dimensions and incorporates a\nmessage-passing framework to distribute data across different partitions. Input\ndata are encoded into quantum states using unary encoding, and quantum circuit\nparameters are optimized using a variational scheme. We implement PHQFNO using\nPennyLane with PyTorch integration and evaluate it on Burgers' equation,\nincompressible and compressible Navier-Stokes equations. We show that PHQFNO\nrecovers classical FNO accuracy. On incompressible Navier-Stokes, PHQFNO\nachieves higher accuracy than its classical counterparts. Finally, we perform a\nsensitivity analysis under input noise, confirming improved stability of PHQFNO\nover classical baselines.\n", "link": "http://arxiv.org/abs/2507.08746v1", "date": "2025-07-11", "relevancy": 1.3047, "topK": [{"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.4437}, {"title": "Tuning-Free Noise Rectification for High Fidelity Image-to-Video\n  Generation", "link": "http://arxiv.org/abs/2403.02827v1", "similarity": 0.4353}, {"title": "Scaling Rectified Flow Transformers for High-Resolution Image Synthesis", "link": "http://arxiv.org/abs/2403.03206v1", "similarity": 0.4251}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Partitioned%20Hybrid%20Quantum%20Fourier%20Neural%20Operators%20for%20Scientific%0A%20%20Quantum%20Machine%20Learning&body=Title%3A%20Partitioned%20Hybrid%20Quantum%20Fourier%20Neural%20Operators%20for%20Scientific%0A%20%20Quantum%20Machine%20Learning%0AAuthor%3A%20Paolo%20Marcandelli%20and%20Yuanchun%20He%20and%20Stefano%20Mariani%20and%20Martina%20Siena%20and%20Stefano%20Markidis%0AAbstract%3A%20%20%20We%20introduce%20the%20Partitioned%20Hybrid%20Quantum%20Fourier%20Neural%20Operator%20%28PHQFNO%29%2C%0Aa%20generalization%20of%20the%20Quantum%20Fourier%20Neural%20Operator%20%28QFNO%29%20for%20scientific%0Amachine%20learning.%20PHQFNO%20partitions%20the%20Fourier%20operator%20computation%20across%0Aclassical%20and%20quantum%20resources%2C%20enabling%20tunable%20quantum-classical%0Ahybridization%20and%20distributed%20execution%20across%20quantum%20and%20classical%20devices.%0AThe%20method%20extends%20QFNOs%20to%20higher%20dimensions%20and%20incorporates%20a%0Amessage-passing%20framework%20to%20distribute%20data%20across%20different%20partitions.%20Input%0Adata%20are%20encoded%20into%20quantum%20states%20using%20unary%20encoding%2C%20and%20quantum%20circuit%0Aparameters%20are%20optimized%20using%20a%20variational%20scheme.%20We%20implement%20PHQFNO%20using%0APennyLane%20with%20PyTorch%20integration%20and%20evaluate%20it%20on%20Burgers%27%20equation%2C%0Aincompressible%20and%20compressible%20Navier-Stokes%20equations.%20We%20show%20that%20PHQFNO%0Arecovers%20classical%20FNO%20accuracy.%20On%20incompressible%20Navier-Stokes%2C%20PHQFNO%0Aachieves%20higher%20accuracy%20than%20its%20classical%20counterparts.%20Finally%2C%20we%20perform%20a%0Asensitivity%20analysis%20under%20input%20noise%2C%20confirming%20improved%20stability%20of%20PHQFNO%0Aover%20classical%20baselines.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2507.08746v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DPartitioned%2520Hybrid%2520Quantum%2520Fourier%2520Neural%2520Operators%2520for%2520Scientific%250A%2520%2520Quantum%2520Machine%2520Learning%26entry.906535625%3DPaolo%2520Marcandelli%2520and%2520Yuanchun%2520He%2520and%2520Stefano%2520Mariani%2520and%2520Martina%2520Siena%2520and%2520Stefano%2520Markidis%26entry.1292438233%3D%2520%2520We%2520introduce%2520the%2520Partitioned%2520Hybrid%2520Quantum%2520Fourier%2520Neural%2520Operator%2520%2528PHQFNO%2529%252C%250Aa%2520generalization%2520of%2520the%2520Quantum%2520Fourier%2520Neural%2520Operator%2520%2528QFNO%2529%2520for%2520scientific%250Amachine%2520learning.%2520PHQFNO%2520partitions%2520the%2520Fourier%2520operator%2520computation%2520across%250Aclassical%2520and%2520quantum%2520resources%252C%2520enabling%2520tunable%2520quantum-classical%250Ahybridization%2520and%2520distributed%2520execution%2520across%2520quantum%2520and%2520classical%2520devices.%250AThe%2520method%2520extends%2520QFNOs%2520to%2520higher%2520dimensions%2520and%2520incorporates%2520a%250Amessage-passing%2520framework%2520to%2520distribute%2520data%2520across%2520different%2520partitions.%2520Input%250Adata%2520are%2520encoded%2520into%2520quantum%2520states%2520using%2520unary%2520encoding%252C%2520and%2520quantum%2520circuit%250Aparameters%2520are%2520optimized%2520using%2520a%2520variational%2520scheme.%2520We%2520implement%2520PHQFNO%2520using%250APennyLane%2520with%2520PyTorch%2520integration%2520and%2520evaluate%2520it%2520on%2520Burgers%2527%2520equation%252C%250Aincompressible%2520and%2520compressible%2520Navier-Stokes%2520equations.%2520We%2520show%2520that%2520PHQFNO%250Arecovers%2520classical%2520FNO%2520accuracy.%2520On%2520incompressible%2520Navier-Stokes%252C%2520PHQFNO%250Aachieves%2520higher%2520accuracy%2520than%2520its%2520classical%2520counterparts.%2520Finally%252C%2520we%2520perform%2520a%250Asensitivity%2520analysis%2520under%2520input%2520noise%252C%2520confirming%2520improved%2520stability%2520of%2520PHQFNO%250Aover%2520classical%2520baselines.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2507.08746v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Partitioned%20Hybrid%20Quantum%20Fourier%20Neural%20Operators%20for%20Scientific%0A%20%20Quantum%20Machine%20Learning&entry.906535625=Paolo%20Marcandelli%20and%20Yuanchun%20He%20and%20Stefano%20Mariani%20and%20Martina%20Siena%20and%20Stefano%20Markidis&entry.1292438233=%20%20We%20introduce%20the%20Partitioned%20Hybrid%20Quantum%20Fourier%20Neural%20Operator%20%28PHQFNO%29%2C%0Aa%20generalization%20of%20the%20Quantum%20Fourier%20Neural%20Operator%20%28QFNO%29%20for%20scientific%0Amachine%20learning.%20PHQFNO%20partitions%20the%20Fourier%20operator%20computation%20across%0Aclassical%20and%20quantum%20resources%2C%20enabling%20tunable%20quantum-classical%0Ahybridization%20and%20distributed%20execution%20across%20quantum%20and%20classical%20devices.%0AThe%20method%20extends%20QFNOs%20to%20higher%20dimensions%20and%20incorporates%20a%0Amessage-passing%20framework%20to%20distribute%20data%20across%20different%20partitions.%20Input%0Adata%20are%20encoded%20into%20quantum%20states%20using%20unary%20encoding%2C%20and%20quantum%20circuit%0Aparameters%20are%20optimized%20using%20a%20variational%20scheme.%20We%20implement%20PHQFNO%20using%0APennyLane%20with%20PyTorch%20integration%20and%20evaluate%20it%20on%20Burgers%27%20equation%2C%0Aincompressible%20and%20compressible%20Navier-Stokes%20equations.%20We%20show%20that%20PHQFNO%0Arecovers%20classical%20FNO%20accuracy.%20On%20incompressible%20Navier-Stokes%2C%20PHQFNO%0Aachieves%20higher%20accuracy%20than%20its%20classical%20counterparts.%20Finally%2C%20we%20perform%20a%0Asensitivity%20analysis%20under%20input%20noise%2C%20confirming%20improved%20stability%20of%20PHQFNO%0Aover%20classical%20baselines.%0A&entry.1838667208=http%3A//arxiv.org/abs/2507.08746v1&entry.124074799=Read"},
{"title": "Domain-Informed Operation Excellence of Gas Turbine System with Machine\n  Learning", "author": "Waqar Muhammad Ashraf and Amir H. Keshavarzzadeh and Abdulelah S. Alshehri and Abdulrahman bin Jumah and Ramit Debnath and Vivek Dua", "abstract": "  The domain-consistent adoption of artificial intelligence (AI) remains low in\nthermal power plants due to the black-box nature of AI algorithms and low\nrepresentation of domain knowledge in conventional data-centric analytics. In\nthis paper, we develop a MAhalanobis Distance-based OPTimization (MAD-OPT)\nframework that incorporates the Mahalanobis distance-based constraint to\nintroduce domain knowledge into data-centric analytics. The developed MAD-OPT\nframework is applied to maximize thermal efficiency and minimize turbine heat\nrate for a 395 MW capacity gas turbine system. We demonstrate that the MAD-OPT\nframework can estimate domain-informed optimal process conditions under\ndifferent ambient conditions, and the optimal solutions are found to be robust\nas evaluated by Monte Carlo simulations. We also apply the MAD-OPT framework to\nestimate optimal process conditions beyond the design power generation limit of\nthe gas turbine system, and have found comparable results with the actual data\nof the power plant. We demonstrate that implementing data-centric optimization\nanalytics without incorporating domain-informed constraints may provide\nineffective solutions that may not be implementable in the real operation of\nthe gas turbine system. This research advances the integration of the\ndata-driven domain knowledge into machine learning-powered analytics that\nenhances the domain-informed operation excellence and paves the way for safe AI\nadoption in thermal power systems.\n", "link": "http://arxiv.org/abs/2507.08697v1", "date": "2025-07-11", "relevancy": 1.2952, "topK": [{"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.4356}, {"title": "Scaling Rectified Flow Transformers for High-Resolution Image Synthesis", "link": "http://arxiv.org/abs/2403.03206v1", "similarity": 0.4336}, {"title": "Tuning-Free Noise Rectification for High Fidelity Image-to-Video\n  Generation", "link": "http://arxiv.org/abs/2403.02827v1", "similarity": 0.4294}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Domain-Informed%20Operation%20Excellence%20of%20Gas%20Turbine%20System%20with%20Machine%0A%20%20Learning&body=Title%3A%20Domain-Informed%20Operation%20Excellence%20of%20Gas%20Turbine%20System%20with%20Machine%0A%20%20Learning%0AAuthor%3A%20Waqar%20Muhammad%20Ashraf%20and%20Amir%20H.%20Keshavarzzadeh%20and%20Abdulelah%20S.%20Alshehri%20and%20Abdulrahman%20bin%20Jumah%20and%20Ramit%20Debnath%20and%20Vivek%20Dua%0AAbstract%3A%20%20%20The%20domain-consistent%20adoption%20of%20artificial%20intelligence%20%28AI%29%20remains%20low%20in%0Athermal%20power%20plants%20due%20to%20the%20black-box%20nature%20of%20AI%20algorithms%20and%20low%0Arepresentation%20of%20domain%20knowledge%20in%20conventional%20data-centric%20analytics.%20In%0Athis%20paper%2C%20we%20develop%20a%20MAhalanobis%20Distance-based%20OPTimization%20%28MAD-OPT%29%0Aframework%20that%20incorporates%20the%20Mahalanobis%20distance-based%20constraint%20to%0Aintroduce%20domain%20knowledge%20into%20data-centric%20analytics.%20The%20developed%20MAD-OPT%0Aframework%20is%20applied%20to%20maximize%20thermal%20efficiency%20and%20minimize%20turbine%20heat%0Arate%20for%20a%20395%20MW%20capacity%20gas%20turbine%20system.%20We%20demonstrate%20that%20the%20MAD-OPT%0Aframework%20can%20estimate%20domain-informed%20optimal%20process%20conditions%20under%0Adifferent%20ambient%20conditions%2C%20and%20the%20optimal%20solutions%20are%20found%20to%20be%20robust%0Aas%20evaluated%20by%20Monte%20Carlo%20simulations.%20We%20also%20apply%20the%20MAD-OPT%20framework%20to%0Aestimate%20optimal%20process%20conditions%20beyond%20the%20design%20power%20generation%20limit%20of%0Athe%20gas%20turbine%20system%2C%20and%20have%20found%20comparable%20results%20with%20the%20actual%20data%0Aof%20the%20power%20plant.%20We%20demonstrate%20that%20implementing%20data-centric%20optimization%0Aanalytics%20without%20incorporating%20domain-informed%20constraints%20may%20provide%0Aineffective%20solutions%20that%20may%20not%20be%20implementable%20in%20the%20real%20operation%20of%0Athe%20gas%20turbine%20system.%20This%20research%20advances%20the%20integration%20of%20the%0Adata-driven%20domain%20knowledge%20into%20machine%20learning-powered%20analytics%20that%0Aenhances%20the%20domain-informed%20operation%20excellence%20and%20paves%20the%20way%20for%20safe%20AI%0Aadoption%20in%20thermal%20power%20systems.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2507.08697v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DDomain-Informed%2520Operation%2520Excellence%2520of%2520Gas%2520Turbine%2520System%2520with%2520Machine%250A%2520%2520Learning%26entry.906535625%3DWaqar%2520Muhammad%2520Ashraf%2520and%2520Amir%2520H.%2520Keshavarzzadeh%2520and%2520Abdulelah%2520S.%2520Alshehri%2520and%2520Abdulrahman%2520bin%2520Jumah%2520and%2520Ramit%2520Debnath%2520and%2520Vivek%2520Dua%26entry.1292438233%3D%2520%2520The%2520domain-consistent%2520adoption%2520of%2520artificial%2520intelligence%2520%2528AI%2529%2520remains%2520low%2520in%250Athermal%2520power%2520plants%2520due%2520to%2520the%2520black-box%2520nature%2520of%2520AI%2520algorithms%2520and%2520low%250Arepresentation%2520of%2520domain%2520knowledge%2520in%2520conventional%2520data-centric%2520analytics.%2520In%250Athis%2520paper%252C%2520we%2520develop%2520a%2520MAhalanobis%2520Distance-based%2520OPTimization%2520%2528MAD-OPT%2529%250Aframework%2520that%2520incorporates%2520the%2520Mahalanobis%2520distance-based%2520constraint%2520to%250Aintroduce%2520domain%2520knowledge%2520into%2520data-centric%2520analytics.%2520The%2520developed%2520MAD-OPT%250Aframework%2520is%2520applied%2520to%2520maximize%2520thermal%2520efficiency%2520and%2520minimize%2520turbine%2520heat%250Arate%2520for%2520a%2520395%2520MW%2520capacity%2520gas%2520turbine%2520system.%2520We%2520demonstrate%2520that%2520the%2520MAD-OPT%250Aframework%2520can%2520estimate%2520domain-informed%2520optimal%2520process%2520conditions%2520under%250Adifferent%2520ambient%2520conditions%252C%2520and%2520the%2520optimal%2520solutions%2520are%2520found%2520to%2520be%2520robust%250Aas%2520evaluated%2520by%2520Monte%2520Carlo%2520simulations.%2520We%2520also%2520apply%2520the%2520MAD-OPT%2520framework%2520to%250Aestimate%2520optimal%2520process%2520conditions%2520beyond%2520the%2520design%2520power%2520generation%2520limit%2520of%250Athe%2520gas%2520turbine%2520system%252C%2520and%2520have%2520found%2520comparable%2520results%2520with%2520the%2520actual%2520data%250Aof%2520the%2520power%2520plant.%2520We%2520demonstrate%2520that%2520implementing%2520data-centric%2520optimization%250Aanalytics%2520without%2520incorporating%2520domain-informed%2520constraints%2520may%2520provide%250Aineffective%2520solutions%2520that%2520may%2520not%2520be%2520implementable%2520in%2520the%2520real%2520operation%2520of%250Athe%2520gas%2520turbine%2520system.%2520This%2520research%2520advances%2520the%2520integration%2520of%2520the%250Adata-driven%2520domain%2520knowledge%2520into%2520machine%2520learning-powered%2520analytics%2520that%250Aenhances%2520the%2520domain-informed%2520operation%2520excellence%2520and%2520paves%2520the%2520way%2520for%2520safe%2520AI%250Aadoption%2520in%2520thermal%2520power%2520systems.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2507.08697v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Domain-Informed%20Operation%20Excellence%20of%20Gas%20Turbine%20System%20with%20Machine%0A%20%20Learning&entry.906535625=Waqar%20Muhammad%20Ashraf%20and%20Amir%20H.%20Keshavarzzadeh%20and%20Abdulelah%20S.%20Alshehri%20and%20Abdulrahman%20bin%20Jumah%20and%20Ramit%20Debnath%20and%20Vivek%20Dua&entry.1292438233=%20%20The%20domain-consistent%20adoption%20of%20artificial%20intelligence%20%28AI%29%20remains%20low%20in%0Athermal%20power%20plants%20due%20to%20the%20black-box%20nature%20of%20AI%20algorithms%20and%20low%0Arepresentation%20of%20domain%20knowledge%20in%20conventional%20data-centric%20analytics.%20In%0Athis%20paper%2C%20we%20develop%20a%20MAhalanobis%20Distance-based%20OPTimization%20%28MAD-OPT%29%0Aframework%20that%20incorporates%20the%20Mahalanobis%20distance-based%20constraint%20to%0Aintroduce%20domain%20knowledge%20into%20data-centric%20analytics.%20The%20developed%20MAD-OPT%0Aframework%20is%20applied%20to%20maximize%20thermal%20efficiency%20and%20minimize%20turbine%20heat%0Arate%20for%20a%20395%20MW%20capacity%20gas%20turbine%20system.%20We%20demonstrate%20that%20the%20MAD-OPT%0Aframework%20can%20estimate%20domain-informed%20optimal%20process%20conditions%20under%0Adifferent%20ambient%20conditions%2C%20and%20the%20optimal%20solutions%20are%20found%20to%20be%20robust%0Aas%20evaluated%20by%20Monte%20Carlo%20simulations.%20We%20also%20apply%20the%20MAD-OPT%20framework%20to%0Aestimate%20optimal%20process%20conditions%20beyond%20the%20design%20power%20generation%20limit%20of%0Athe%20gas%20turbine%20system%2C%20and%20have%20found%20comparable%20results%20with%20the%20actual%20data%0Aof%20the%20power%20plant.%20We%20demonstrate%20that%20implementing%20data-centric%20optimization%0Aanalytics%20without%20incorporating%20domain-informed%20constraints%20may%20provide%0Aineffective%20solutions%20that%20may%20not%20be%20implementable%20in%20the%20real%20operation%20of%0Athe%20gas%20turbine%20system.%20This%20research%20advances%20the%20integration%20of%20the%0Adata-driven%20domain%20knowledge%20into%20machine%20learning-powered%20analytics%20that%0Aenhances%20the%20domain-informed%20operation%20excellence%20and%20paves%20the%20way%20for%20safe%20AI%0Aadoption%20in%20thermal%20power%20systems.%0A&entry.1838667208=http%3A//arxiv.org/abs/2507.08697v1&entry.124074799=Read"},
{"title": "AI Safety Should Prioritize the Future of Work", "author": "Sanchaita Hazra and Bodhisattwa Prasad Majumder and Tuhin Chakrabarty", "abstract": "  Current efforts in AI safety prioritize filtering harmful content, preventing\nmanipulation of human behavior, and eliminating existential risks in\ncybersecurity or biosecurity. While pressing, this narrow focus overlooks\ncritical human-centric considerations that shape the long-term trajectory of a\nsociety. In this position paper, we identify the risks of overlooking the\nimpact of AI on the future of work and recommend comprehensive transition\nsupport towards the evolution of meaningful labor with human agency. Through\nthe lens of economic theories, we highlight the intertemporal impacts of AI on\nhuman livelihood and the structural changes in labor markets that exacerbate\nincome inequality. Additionally, the closed-source approach of major\nstakeholders in AI development resembles rent-seeking behavior through\nexploiting resources, breeding mediocrity in creative labor, and monopolizing\ninnovation. To address this, we argue in favor of a robust international\ncopyright anatomy supported by implementing collective licensing that ensures\nfair compensation mechanisms for using data to train AI models. We strongly\nrecommend a pro-worker framework of global AI governance to enhance shared\nprosperity and economic justice while reducing technical debt.\n", "link": "http://arxiv.org/abs/2504.13959v2", "date": "2025-07-11", "relevancy": 1.2122, "topK": [{"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.4146}, {"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.4064}, {"title": "DressCode: Autoregressively Sewing and Generating Garments from Text\n  Guidance", "link": "http://arxiv.org/abs/2401.16465v3", "similarity": 0.3989}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20AI%20Safety%20Should%20Prioritize%20the%20Future%20of%20Work&body=Title%3A%20AI%20Safety%20Should%20Prioritize%20the%20Future%20of%20Work%0AAuthor%3A%20Sanchaita%20Hazra%20and%20Bodhisattwa%20Prasad%20Majumder%20and%20Tuhin%20Chakrabarty%0AAbstract%3A%20%20%20Current%20efforts%20in%20AI%20safety%20prioritize%20filtering%20harmful%20content%2C%20preventing%0Amanipulation%20of%20human%20behavior%2C%20and%20eliminating%20existential%20risks%20in%0Acybersecurity%20or%20biosecurity.%20While%20pressing%2C%20this%20narrow%20focus%20overlooks%0Acritical%20human-centric%20considerations%20that%20shape%20the%20long-term%20trajectory%20of%20a%0Asociety.%20In%20this%20position%20paper%2C%20we%20identify%20the%20risks%20of%20overlooking%20the%0Aimpact%20of%20AI%20on%20the%20future%20of%20work%20and%20recommend%20comprehensive%20transition%0Asupport%20towards%20the%20evolution%20of%20meaningful%20labor%20with%20human%20agency.%20Through%0Athe%20lens%20of%20economic%20theories%2C%20we%20highlight%20the%20intertemporal%20impacts%20of%20AI%20on%0Ahuman%20livelihood%20and%20the%20structural%20changes%20in%20labor%20markets%20that%20exacerbate%0Aincome%20inequality.%20Additionally%2C%20the%20closed-source%20approach%20of%20major%0Astakeholders%20in%20AI%20development%20resembles%20rent-seeking%20behavior%20through%0Aexploiting%20resources%2C%20breeding%20mediocrity%20in%20creative%20labor%2C%20and%20monopolizing%0Ainnovation.%20To%20address%20this%2C%20we%20argue%20in%20favor%20of%20a%20robust%20international%0Acopyright%20anatomy%20supported%20by%20implementing%20collective%20licensing%20that%20ensures%0Afair%20compensation%20mechanisms%20for%20using%20data%20to%20train%20AI%20models.%20We%20strongly%0Arecommend%20a%20pro-worker%20framework%20of%20global%20AI%20governance%20to%20enhance%20shared%0Aprosperity%20and%20economic%20justice%20while%20reducing%20technical%20debt.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2504.13959v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DAI%2520Safety%2520Should%2520Prioritize%2520the%2520Future%2520of%2520Work%26entry.906535625%3DSanchaita%2520Hazra%2520and%2520Bodhisattwa%2520Prasad%2520Majumder%2520and%2520Tuhin%2520Chakrabarty%26entry.1292438233%3D%2520%2520Current%2520efforts%2520in%2520AI%2520safety%2520prioritize%2520filtering%2520harmful%2520content%252C%2520preventing%250Amanipulation%2520of%2520human%2520behavior%252C%2520and%2520eliminating%2520existential%2520risks%2520in%250Acybersecurity%2520or%2520biosecurity.%2520While%2520pressing%252C%2520this%2520narrow%2520focus%2520overlooks%250Acritical%2520human-centric%2520considerations%2520that%2520shape%2520the%2520long-term%2520trajectory%2520of%2520a%250Asociety.%2520In%2520this%2520position%2520paper%252C%2520we%2520identify%2520the%2520risks%2520of%2520overlooking%2520the%250Aimpact%2520of%2520AI%2520on%2520the%2520future%2520of%2520work%2520and%2520recommend%2520comprehensive%2520transition%250Asupport%2520towards%2520the%2520evolution%2520of%2520meaningful%2520labor%2520with%2520human%2520agency.%2520Through%250Athe%2520lens%2520of%2520economic%2520theories%252C%2520we%2520highlight%2520the%2520intertemporal%2520impacts%2520of%2520AI%2520on%250Ahuman%2520livelihood%2520and%2520the%2520structural%2520changes%2520in%2520labor%2520markets%2520that%2520exacerbate%250Aincome%2520inequality.%2520Additionally%252C%2520the%2520closed-source%2520approach%2520of%2520major%250Astakeholders%2520in%2520AI%2520development%2520resembles%2520rent-seeking%2520behavior%2520through%250Aexploiting%2520resources%252C%2520breeding%2520mediocrity%2520in%2520creative%2520labor%252C%2520and%2520monopolizing%250Ainnovation.%2520To%2520address%2520this%252C%2520we%2520argue%2520in%2520favor%2520of%2520a%2520robust%2520international%250Acopyright%2520anatomy%2520supported%2520by%2520implementing%2520collective%2520licensing%2520that%2520ensures%250Afair%2520compensation%2520mechanisms%2520for%2520using%2520data%2520to%2520train%2520AI%2520models.%2520We%2520strongly%250Arecommend%2520a%2520pro-worker%2520framework%2520of%2520global%2520AI%2520governance%2520to%2520enhance%2520shared%250Aprosperity%2520and%2520economic%2520justice%2520while%2520reducing%2520technical%2520debt.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2504.13959v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=AI%20Safety%20Should%20Prioritize%20the%20Future%20of%20Work&entry.906535625=Sanchaita%20Hazra%20and%20Bodhisattwa%20Prasad%20Majumder%20and%20Tuhin%20Chakrabarty&entry.1292438233=%20%20Current%20efforts%20in%20AI%20safety%20prioritize%20filtering%20harmful%20content%2C%20preventing%0Amanipulation%20of%20human%20behavior%2C%20and%20eliminating%20existential%20risks%20in%0Acybersecurity%20or%20biosecurity.%20While%20pressing%2C%20this%20narrow%20focus%20overlooks%0Acritical%20human-centric%20considerations%20that%20shape%20the%20long-term%20trajectory%20of%20a%0Asociety.%20In%20this%20position%20paper%2C%20we%20identify%20the%20risks%20of%20overlooking%20the%0Aimpact%20of%20AI%20on%20the%20future%20of%20work%20and%20recommend%20comprehensive%20transition%0Asupport%20towards%20the%20evolution%20of%20meaningful%20labor%20with%20human%20agency.%20Through%0Athe%20lens%20of%20economic%20theories%2C%20we%20highlight%20the%20intertemporal%20impacts%20of%20AI%20on%0Ahuman%20livelihood%20and%20the%20structural%20changes%20in%20labor%20markets%20that%20exacerbate%0Aincome%20inequality.%20Additionally%2C%20the%20closed-source%20approach%20of%20major%0Astakeholders%20in%20AI%20development%20resembles%20rent-seeking%20behavior%20through%0Aexploiting%20resources%2C%20breeding%20mediocrity%20in%20creative%20labor%2C%20and%20monopolizing%0Ainnovation.%20To%20address%20this%2C%20we%20argue%20in%20favor%20of%20a%20robust%20international%0Acopyright%20anatomy%20supported%20by%20implementing%20collective%20licensing%20that%20ensures%0Afair%20compensation%20mechanisms%20for%20using%20data%20to%20train%20AI%20models.%20We%20strongly%0Arecommend%20a%20pro-worker%20framework%20of%20global%20AI%20governance%20to%20enhance%20shared%0Aprosperity%20and%20economic%20justice%20while%20reducing%20technical%20debt.%0A&entry.1838667208=http%3A//arxiv.org/abs/2504.13959v2&entry.124074799=Read"},
{"title": "Filter Equivariant Functions: A symmetric account of length-general\n  extrapolation on lists", "author": "Owen Lewis and Neil Ghani and Andrew Dudzik and Christos Perivolaropoulos and Razvan Pascanu and Petar Veli\u010dkovi\u0107", "abstract": "  What should a function that extrapolates beyond known input/output examples\nlook like? This is a tricky question to answer in general, as any function\nmatching the outputs on those examples can in principle be a correct\nextrapolant. We argue that a \"good\" extrapolant should follow certain kinds of\nrules, and here we study a particularly appealing criterion for rule-following\nin list functions: that the function should behave predictably even when\ncertain elements are removed. In functional programming, a standard way to\nexpress such removal operations is by using a filter function. Accordingly, our\npaper introduces a new semantic class of functions -- the filter equivariant\nfunctions. We show that this class contains interesting examples, prove some\nbasic theorems about it, and relate it to the well-known class of map\nequivariant functions. We also present a geometric account of filter\nequivariants, showing how they correspond naturally to certain simplicial\nstructures. Our highlight result is the amalgamation algorithm, which\nconstructs any filter-equivariant function's output by first studying how it\nbehaves on sublists of the input, in a way that extrapolates perfectly.\n", "link": "http://arxiv.org/abs/2507.08796v1", "date": "2025-07-11", "relevancy": 1.1678, "topK": [{"title": "Breadcrumbs: Adversarial class-balanced sampling for long-tailed recognition", "link": "https://link.springer.com/chapter/10.1007/978-3-031-20053-3_37", "similarity": 0.3929}, {"title": "Scaling Rectified Flow Transformers for High-Resolution Image Synthesis", "link": "http://arxiv.org/abs/2403.03206v1", "similarity": 0.3855}, {"title": "Thinking Outside the BBox: Unconstrained Generative Object Compositing", "link": "http://arxiv.org/abs/2409.04559v2", "similarity": 0.3839}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Filter%20Equivariant%20Functions%3A%20A%20symmetric%20account%20of%20length-general%0A%20%20extrapolation%20on%20lists&body=Title%3A%20Filter%20Equivariant%20Functions%3A%20A%20symmetric%20account%20of%20length-general%0A%20%20extrapolation%20on%20lists%0AAuthor%3A%20Owen%20Lewis%20and%20Neil%20Ghani%20and%20Andrew%20Dudzik%20and%20Christos%20Perivolaropoulos%20and%20Razvan%20Pascanu%20and%20Petar%20Veli%C4%8Dkovi%C4%87%0AAbstract%3A%20%20%20What%20should%20a%20function%20that%20extrapolates%20beyond%20known%20input/output%20examples%0Alook%20like%3F%20This%20is%20a%20tricky%20question%20to%20answer%20in%20general%2C%20as%20any%20function%0Amatching%20the%20outputs%20on%20those%20examples%20can%20in%20principle%20be%20a%20correct%0Aextrapolant.%20We%20argue%20that%20a%20%22good%22%20extrapolant%20should%20follow%20certain%20kinds%20of%0Arules%2C%20and%20here%20we%20study%20a%20particularly%20appealing%20criterion%20for%20rule-following%0Ain%20list%20functions%3A%20that%20the%20function%20should%20behave%20predictably%20even%20when%0Acertain%20elements%20are%20removed.%20In%20functional%20programming%2C%20a%20standard%20way%20to%0Aexpress%20such%20removal%20operations%20is%20by%20using%20a%20filter%20function.%20Accordingly%2C%20our%0Apaper%20introduces%20a%20new%20semantic%20class%20of%20functions%20--%20the%20filter%20equivariant%0Afunctions.%20We%20show%20that%20this%20class%20contains%20interesting%20examples%2C%20prove%20some%0Abasic%20theorems%20about%20it%2C%20and%20relate%20it%20to%20the%20well-known%20class%20of%20map%0Aequivariant%20functions.%20We%20also%20present%20a%20geometric%20account%20of%20filter%0Aequivariants%2C%20showing%20how%20they%20correspond%20naturally%20to%20certain%20simplicial%0Astructures.%20Our%20highlight%20result%20is%20the%20amalgamation%20algorithm%2C%20which%0Aconstructs%20any%20filter-equivariant%20function%27s%20output%20by%20first%20studying%20how%20it%0Abehaves%20on%20sublists%20of%20the%20input%2C%20in%20a%20way%20that%20extrapolates%20perfectly.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2507.08796v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DFilter%2520Equivariant%2520Functions%253A%2520A%2520symmetric%2520account%2520of%2520length-general%250A%2520%2520extrapolation%2520on%2520lists%26entry.906535625%3DOwen%2520Lewis%2520and%2520Neil%2520Ghani%2520and%2520Andrew%2520Dudzik%2520and%2520Christos%2520Perivolaropoulos%2520and%2520Razvan%2520Pascanu%2520and%2520Petar%2520Veli%25C4%258Dkovi%25C4%2587%26entry.1292438233%3D%2520%2520What%2520should%2520a%2520function%2520that%2520extrapolates%2520beyond%2520known%2520input/output%2520examples%250Alook%2520like%253F%2520This%2520is%2520a%2520tricky%2520question%2520to%2520answer%2520in%2520general%252C%2520as%2520any%2520function%250Amatching%2520the%2520outputs%2520on%2520those%2520examples%2520can%2520in%2520principle%2520be%2520a%2520correct%250Aextrapolant.%2520We%2520argue%2520that%2520a%2520%2522good%2522%2520extrapolant%2520should%2520follow%2520certain%2520kinds%2520of%250Arules%252C%2520and%2520here%2520we%2520study%2520a%2520particularly%2520appealing%2520criterion%2520for%2520rule-following%250Ain%2520list%2520functions%253A%2520that%2520the%2520function%2520should%2520behave%2520predictably%2520even%2520when%250Acertain%2520elements%2520are%2520removed.%2520In%2520functional%2520programming%252C%2520a%2520standard%2520way%2520to%250Aexpress%2520such%2520removal%2520operations%2520is%2520by%2520using%2520a%2520filter%2520function.%2520Accordingly%252C%2520our%250Apaper%2520introduces%2520a%2520new%2520semantic%2520class%2520of%2520functions%2520--%2520the%2520filter%2520equivariant%250Afunctions.%2520We%2520show%2520that%2520this%2520class%2520contains%2520interesting%2520examples%252C%2520prove%2520some%250Abasic%2520theorems%2520about%2520it%252C%2520and%2520relate%2520it%2520to%2520the%2520well-known%2520class%2520of%2520map%250Aequivariant%2520functions.%2520We%2520also%2520present%2520a%2520geometric%2520account%2520of%2520filter%250Aequivariants%252C%2520showing%2520how%2520they%2520correspond%2520naturally%2520to%2520certain%2520simplicial%250Astructures.%2520Our%2520highlight%2520result%2520is%2520the%2520amalgamation%2520algorithm%252C%2520which%250Aconstructs%2520any%2520filter-equivariant%2520function%2527s%2520output%2520by%2520first%2520studying%2520how%2520it%250Abehaves%2520on%2520sublists%2520of%2520the%2520input%252C%2520in%2520a%2520way%2520that%2520extrapolates%2520perfectly.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2507.08796v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Filter%20Equivariant%20Functions%3A%20A%20symmetric%20account%20of%20length-general%0A%20%20extrapolation%20on%20lists&entry.906535625=Owen%20Lewis%20and%20Neil%20Ghani%20and%20Andrew%20Dudzik%20and%20Christos%20Perivolaropoulos%20and%20Razvan%20Pascanu%20and%20Petar%20Veli%C4%8Dkovi%C4%87&entry.1292438233=%20%20What%20should%20a%20function%20that%20extrapolates%20beyond%20known%20input/output%20examples%0Alook%20like%3F%20This%20is%20a%20tricky%20question%20to%20answer%20in%20general%2C%20as%20any%20function%0Amatching%20the%20outputs%20on%20those%20examples%20can%20in%20principle%20be%20a%20correct%0Aextrapolant.%20We%20argue%20that%20a%20%22good%22%20extrapolant%20should%20follow%20certain%20kinds%20of%0Arules%2C%20and%20here%20we%20study%20a%20particularly%20appealing%20criterion%20for%20rule-following%0Ain%20list%20functions%3A%20that%20the%20function%20should%20behave%20predictably%20even%20when%0Acertain%20elements%20are%20removed.%20In%20functional%20programming%2C%20a%20standard%20way%20to%0Aexpress%20such%20removal%20operations%20is%20by%20using%20a%20filter%20function.%20Accordingly%2C%20our%0Apaper%20introduces%20a%20new%20semantic%20class%20of%20functions%20--%20the%20filter%20equivariant%0Afunctions.%20We%20show%20that%20this%20class%20contains%20interesting%20examples%2C%20prove%20some%0Abasic%20theorems%20about%20it%2C%20and%20relate%20it%20to%20the%20well-known%20class%20of%20map%0Aequivariant%20functions.%20We%20also%20present%20a%20geometric%20account%20of%20filter%0Aequivariants%2C%20showing%20how%20they%20correspond%20naturally%20to%20certain%20simplicial%0Astructures.%20Our%20highlight%20result%20is%20the%20amalgamation%20algorithm%2C%20which%0Aconstructs%20any%20filter-equivariant%20function%27s%20output%20by%20first%20studying%20how%20it%0Abehaves%20on%20sublists%20of%20the%20input%2C%20in%20a%20way%20that%20extrapolates%20perfectly.%0A&entry.1838667208=http%3A//arxiv.org/abs/2507.08796v1&entry.124074799=Read"},
{"title": "Monitoring Risks in Test-Time Adaptation", "author": "Mona Schirmer and Metod Jazbec and Christian A. Naesseth and Eric Nalisnick", "abstract": "  Encountering shifted data at test time is a ubiquitous challenge when\ndeploying predictive models. Test-time adaptation (TTA) methods address this\nissue by continuously adapting a deployed model using only unlabeled test data.\nWhile TTA can extend the model's lifespan, it is only a temporary solution.\nEventually the model might degrade to the point that it must be taken offline\nand retrained. To detect such points of ultimate failure, we propose pairing\nTTA with risk monitoring frameworks that track predictive performance and raise\nalerts when predefined performance criteria are violated. Specifically, we\nextend existing monitoring tools based on sequential testing with confidence\nsequences to accommodate scenarios in which the model is updated at test time\nand no test labels are available to estimate the performance metrics of\ninterest. Our extensions unlock the application of rigorous statistical risk\nmonitoring to TTA, and we demonstrate the effectiveness of our proposed TTA\nmonitoring framework across a representative set of datasets, distribution\nshift types, and TTA methods.\n", "link": "http://arxiv.org/abs/2507.08721v1", "date": "2025-07-11", "relevancy": 1.352, "topK": [{"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.4587}, {"title": "Customize-A-Video: One-Shot Motion Customization of Text-to-Video\n  Diffusion Models", "link": "http://arxiv.org/abs/2402.14780v1", "similarity": 0.4572}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.4242}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Monitoring%20Risks%20in%20Test-Time%20Adaptation&body=Title%3A%20Monitoring%20Risks%20in%20Test-Time%20Adaptation%0AAuthor%3A%20Mona%20Schirmer%20and%20Metod%20Jazbec%20and%20Christian%20A.%20Naesseth%20and%20Eric%20Nalisnick%0AAbstract%3A%20%20%20Encountering%20shifted%20data%20at%20test%20time%20is%20a%20ubiquitous%20challenge%20when%0Adeploying%20predictive%20models.%20Test-time%20adaptation%20%28TTA%29%20methods%20address%20this%0Aissue%20by%20continuously%20adapting%20a%20deployed%20model%20using%20only%20unlabeled%20test%20data.%0AWhile%20TTA%20can%20extend%20the%20model%27s%20lifespan%2C%20it%20is%20only%20a%20temporary%20solution.%0AEventually%20the%20model%20might%20degrade%20to%20the%20point%20that%20it%20must%20be%20taken%20offline%0Aand%20retrained.%20To%20detect%20such%20points%20of%20ultimate%20failure%2C%20we%20propose%20pairing%0ATTA%20with%20risk%20monitoring%20frameworks%20that%20track%20predictive%20performance%20and%20raise%0Aalerts%20when%20predefined%20performance%20criteria%20are%20violated.%20Specifically%2C%20we%0Aextend%20existing%20monitoring%20tools%20based%20on%20sequential%20testing%20with%20confidence%0Asequences%20to%20accommodate%20scenarios%20in%20which%20the%20model%20is%20updated%20at%20test%20time%0Aand%20no%20test%20labels%20are%20available%20to%20estimate%20the%20performance%20metrics%20of%0Ainterest.%20Our%20extensions%20unlock%20the%20application%20of%20rigorous%20statistical%20risk%0Amonitoring%20to%20TTA%2C%20and%20we%20demonstrate%20the%20effectiveness%20of%20our%20proposed%20TTA%0Amonitoring%20framework%20across%20a%20representative%20set%20of%20datasets%2C%20distribution%0Ashift%20types%2C%20and%20TTA%20methods.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2507.08721v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DMonitoring%2520Risks%2520in%2520Test-Time%2520Adaptation%26entry.906535625%3DMona%2520Schirmer%2520and%2520Metod%2520Jazbec%2520and%2520Christian%2520A.%2520Naesseth%2520and%2520Eric%2520Nalisnick%26entry.1292438233%3D%2520%2520Encountering%2520shifted%2520data%2520at%2520test%2520time%2520is%2520a%2520ubiquitous%2520challenge%2520when%250Adeploying%2520predictive%2520models.%2520Test-time%2520adaptation%2520%2528TTA%2529%2520methods%2520address%2520this%250Aissue%2520by%2520continuously%2520adapting%2520a%2520deployed%2520model%2520using%2520only%2520unlabeled%2520test%2520data.%250AWhile%2520TTA%2520can%2520extend%2520the%2520model%2527s%2520lifespan%252C%2520it%2520is%2520only%2520a%2520temporary%2520solution.%250AEventually%2520the%2520model%2520might%2520degrade%2520to%2520the%2520point%2520that%2520it%2520must%2520be%2520taken%2520offline%250Aand%2520retrained.%2520To%2520detect%2520such%2520points%2520of%2520ultimate%2520failure%252C%2520we%2520propose%2520pairing%250ATTA%2520with%2520risk%2520monitoring%2520frameworks%2520that%2520track%2520predictive%2520performance%2520and%2520raise%250Aalerts%2520when%2520predefined%2520performance%2520criteria%2520are%2520violated.%2520Specifically%252C%2520we%250Aextend%2520existing%2520monitoring%2520tools%2520based%2520on%2520sequential%2520testing%2520with%2520confidence%250Asequences%2520to%2520accommodate%2520scenarios%2520in%2520which%2520the%2520model%2520is%2520updated%2520at%2520test%2520time%250Aand%2520no%2520test%2520labels%2520are%2520available%2520to%2520estimate%2520the%2520performance%2520metrics%2520of%250Ainterest.%2520Our%2520extensions%2520unlock%2520the%2520application%2520of%2520rigorous%2520statistical%2520risk%250Amonitoring%2520to%2520TTA%252C%2520and%2520we%2520demonstrate%2520the%2520effectiveness%2520of%2520our%2520proposed%2520TTA%250Amonitoring%2520framework%2520across%2520a%2520representative%2520set%2520of%2520datasets%252C%2520distribution%250Ashift%2520types%252C%2520and%2520TTA%2520methods.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2507.08721v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Monitoring%20Risks%20in%20Test-Time%20Adaptation&entry.906535625=Mona%20Schirmer%20and%20Metod%20Jazbec%20and%20Christian%20A.%20Naesseth%20and%20Eric%20Nalisnick&entry.1292438233=%20%20Encountering%20shifted%20data%20at%20test%20time%20is%20a%20ubiquitous%20challenge%20when%0Adeploying%20predictive%20models.%20Test-time%20adaptation%20%28TTA%29%20methods%20address%20this%0Aissue%20by%20continuously%20adapting%20a%20deployed%20model%20using%20only%20unlabeled%20test%20data.%0AWhile%20TTA%20can%20extend%20the%20model%27s%20lifespan%2C%20it%20is%20only%20a%20temporary%20solution.%0AEventually%20the%20model%20might%20degrade%20to%20the%20point%20that%20it%20must%20be%20taken%20offline%0Aand%20retrained.%20To%20detect%20such%20points%20of%20ultimate%20failure%2C%20we%20propose%20pairing%0ATTA%20with%20risk%20monitoring%20frameworks%20that%20track%20predictive%20performance%20and%20raise%0Aalerts%20when%20predefined%20performance%20criteria%20are%20violated.%20Specifically%2C%20we%0Aextend%20existing%20monitoring%20tools%20based%20on%20sequential%20testing%20with%20confidence%0Asequences%20to%20accommodate%20scenarios%20in%20which%20the%20model%20is%20updated%20at%20test%20time%0Aand%20no%20test%20labels%20are%20available%20to%20estimate%20the%20performance%20metrics%20of%0Ainterest.%20Our%20extensions%20unlock%20the%20application%20of%20rigorous%20statistical%20risk%0Amonitoring%20to%20TTA%2C%20and%20we%20demonstrate%20the%20effectiveness%20of%20our%20proposed%20TTA%0Amonitoring%20framework%20across%20a%20representative%20set%20of%20datasets%2C%20distribution%0Ashift%20types%2C%20and%20TTA%20methods.%0A&entry.1838667208=http%3A//arxiv.org/abs/2507.08721v1&entry.124074799=Read"},
{"title": "Mind the Memory Gap: Unveiling GPU Bottlenecks in Large-Batch LLM\n  Inference", "author": "Pol G. Recasens and Ferran Agullo and Yue Zhu and Chen Wang and Eun Kyung Lee and Olivier Tardieu and Jordi Torres and Josep Ll. Berral", "abstract": "  Large language models have been widely adopted across different tasks, but\ntheir auto-regressive generation nature often leads to inefficient resource\nutilization during inference. While batching is commonly used to increase\nthroughput, performance gains plateau beyond a certain batch size, especially\nwith smaller models, a phenomenon that existing literature typically explains\nas a shift to the compute-bound regime. In this paper, through an in-depth\nGPU-level analysis, we reveal that large-batch inference remains memory-bound,\nwith most GPU compute capabilities underutilized due to DRAM bandwidth\nsaturation as the primary bottleneck. To address this, we propose a Batching\nConfiguration Advisor (BCA) that optimizes memory allocation, reducing GPU\nmemory requirements with minimal impact on throughput. The freed memory and\nunderutilized GPU compute capabilities can then be leveraged by concurrent\nworkloads. Specifically, we use model replication to improve serving throughput\nand GPU utilization. Our findings challenge conventional assumptions about LLM\ninference, offering new insights and practical strategies for improving\nresource utilization, particularly for smaller language models. The code is\npublicly available at\nhttps://github.com/FerranAgulloLopez/vLLMBatchingMemoryGap.\n", "link": "http://arxiv.org/abs/2503.08311v2", "date": "2025-07-11", "relevancy": 1.0208, "topK": [{"title": "SDXS: Real-Time One-Step Latent Diffusion Models with Image Conditions", "link": "http://arxiv.org/abs/2403.16627v1", "similarity": 0.5271}, {"title": "Scaling Rectified Flow Transformers for High-Resolution Image Synthesis", "link": "http://arxiv.org/abs/2403.03206v1", "similarity": 0.5099}, {"title": "Thinking Outside the BBox: Unconstrained Generative Object Compositing", "link": "http://arxiv.org/abs/2409.04559v2", "similarity": 0.4943}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Mind%20the%20Memory%20Gap%3A%20Unveiling%20GPU%20Bottlenecks%20in%20Large-Batch%20LLM%0A%20%20Inference&body=Title%3A%20Mind%20the%20Memory%20Gap%3A%20Unveiling%20GPU%20Bottlenecks%20in%20Large-Batch%20LLM%0A%20%20Inference%0AAuthor%3A%20Pol%20G.%20Recasens%20and%20Ferran%20Agullo%20and%20Yue%20Zhu%20and%20Chen%20Wang%20and%20Eun%20Kyung%20Lee%20and%20Olivier%20Tardieu%20and%20Jordi%20Torres%20and%20Josep%20Ll.%20Berral%0AAbstract%3A%20%20%20Large%20language%20models%20have%20been%20widely%20adopted%20across%20different%20tasks%2C%20but%0Atheir%20auto-regressive%20generation%20nature%20often%20leads%20to%20inefficient%20resource%0Autilization%20during%20inference.%20While%20batching%20is%20commonly%20used%20to%20increase%0Athroughput%2C%20performance%20gains%20plateau%20beyond%20a%20certain%20batch%20size%2C%20especially%0Awith%20smaller%20models%2C%20a%20phenomenon%20that%20existing%20literature%20typically%20explains%0Aas%20a%20shift%20to%20the%20compute-bound%20regime.%20In%20this%20paper%2C%20through%20an%20in-depth%0AGPU-level%20analysis%2C%20we%20reveal%20that%20large-batch%20inference%20remains%20memory-bound%2C%0Awith%20most%20GPU%20compute%20capabilities%20underutilized%20due%20to%20DRAM%20bandwidth%0Asaturation%20as%20the%20primary%20bottleneck.%20To%20address%20this%2C%20we%20propose%20a%20Batching%0AConfiguration%20Advisor%20%28BCA%29%20that%20optimizes%20memory%20allocation%2C%20reducing%20GPU%0Amemory%20requirements%20with%20minimal%20impact%20on%20throughput.%20The%20freed%20memory%20and%0Aunderutilized%20GPU%20compute%20capabilities%20can%20then%20be%20leveraged%20by%20concurrent%0Aworkloads.%20Specifically%2C%20we%20use%20model%20replication%20to%20improve%20serving%20throughput%0Aand%20GPU%20utilization.%20Our%20findings%20challenge%20conventional%20assumptions%20about%20LLM%0Ainference%2C%20offering%20new%20insights%20and%20practical%20strategies%20for%20improving%0Aresource%20utilization%2C%20particularly%20for%20smaller%20language%20models.%20The%20code%20is%0Apublicly%20available%20at%0Ahttps%3A//github.com/FerranAgulloLopez/vLLMBatchingMemoryGap.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2503.08311v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DMind%2520the%2520Memory%2520Gap%253A%2520Unveiling%2520GPU%2520Bottlenecks%2520in%2520Large-Batch%2520LLM%250A%2520%2520Inference%26entry.906535625%3DPol%2520G.%2520Recasens%2520and%2520Ferran%2520Agullo%2520and%2520Yue%2520Zhu%2520and%2520Chen%2520Wang%2520and%2520Eun%2520Kyung%2520Lee%2520and%2520Olivier%2520Tardieu%2520and%2520Jordi%2520Torres%2520and%2520Josep%2520Ll.%2520Berral%26entry.1292438233%3D%2520%2520Large%2520language%2520models%2520have%2520been%2520widely%2520adopted%2520across%2520different%2520tasks%252C%2520but%250Atheir%2520auto-regressive%2520generation%2520nature%2520often%2520leads%2520to%2520inefficient%2520resource%250Autilization%2520during%2520inference.%2520While%2520batching%2520is%2520commonly%2520used%2520to%2520increase%250Athroughput%252C%2520performance%2520gains%2520plateau%2520beyond%2520a%2520certain%2520batch%2520size%252C%2520especially%250Awith%2520smaller%2520models%252C%2520a%2520phenomenon%2520that%2520existing%2520literature%2520typically%2520explains%250Aas%2520a%2520shift%2520to%2520the%2520compute-bound%2520regime.%2520In%2520this%2520paper%252C%2520through%2520an%2520in-depth%250AGPU-level%2520analysis%252C%2520we%2520reveal%2520that%2520large-batch%2520inference%2520remains%2520memory-bound%252C%250Awith%2520most%2520GPU%2520compute%2520capabilities%2520underutilized%2520due%2520to%2520DRAM%2520bandwidth%250Asaturation%2520as%2520the%2520primary%2520bottleneck.%2520To%2520address%2520this%252C%2520we%2520propose%2520a%2520Batching%250AConfiguration%2520Advisor%2520%2528BCA%2529%2520that%2520optimizes%2520memory%2520allocation%252C%2520reducing%2520GPU%250Amemory%2520requirements%2520with%2520minimal%2520impact%2520on%2520throughput.%2520The%2520freed%2520memory%2520and%250Aunderutilized%2520GPU%2520compute%2520capabilities%2520can%2520then%2520be%2520leveraged%2520by%2520concurrent%250Aworkloads.%2520Specifically%252C%2520we%2520use%2520model%2520replication%2520to%2520improve%2520serving%2520throughput%250Aand%2520GPU%2520utilization.%2520Our%2520findings%2520challenge%2520conventional%2520assumptions%2520about%2520LLM%250Ainference%252C%2520offering%2520new%2520insights%2520and%2520practical%2520strategies%2520for%2520improving%250Aresource%2520utilization%252C%2520particularly%2520for%2520smaller%2520language%2520models.%2520The%2520code%2520is%250Apublicly%2520available%2520at%250Ahttps%253A//github.com/FerranAgulloLopez/vLLMBatchingMemoryGap.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2503.08311v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Mind%20the%20Memory%20Gap%3A%20Unveiling%20GPU%20Bottlenecks%20in%20Large-Batch%20LLM%0A%20%20Inference&entry.906535625=Pol%20G.%20Recasens%20and%20Ferran%20Agullo%20and%20Yue%20Zhu%20and%20Chen%20Wang%20and%20Eun%20Kyung%20Lee%20and%20Olivier%20Tardieu%20and%20Jordi%20Torres%20and%20Josep%20Ll.%20Berral&entry.1292438233=%20%20Large%20language%20models%20have%20been%20widely%20adopted%20across%20different%20tasks%2C%20but%0Atheir%20auto-regressive%20generation%20nature%20often%20leads%20to%20inefficient%20resource%0Autilization%20during%20inference.%20While%20batching%20is%20commonly%20used%20to%20increase%0Athroughput%2C%20performance%20gains%20plateau%20beyond%20a%20certain%20batch%20size%2C%20especially%0Awith%20smaller%20models%2C%20a%20phenomenon%20that%20existing%20literature%20typically%20explains%0Aas%20a%20shift%20to%20the%20compute-bound%20regime.%20In%20this%20paper%2C%20through%20an%20in-depth%0AGPU-level%20analysis%2C%20we%20reveal%20that%20large-batch%20inference%20remains%20memory-bound%2C%0Awith%20most%20GPU%20compute%20capabilities%20underutilized%20due%20to%20DRAM%20bandwidth%0Asaturation%20as%20the%20primary%20bottleneck.%20To%20address%20this%2C%20we%20propose%20a%20Batching%0AConfiguration%20Advisor%20%28BCA%29%20that%20optimizes%20memory%20allocation%2C%20reducing%20GPU%0Amemory%20requirements%20with%20minimal%20impact%20on%20throughput.%20The%20freed%20memory%20and%0Aunderutilized%20GPU%20compute%20capabilities%20can%20then%20be%20leveraged%20by%20concurrent%0Aworkloads.%20Specifically%2C%20we%20use%20model%20replication%20to%20improve%20serving%20throughput%0Aand%20GPU%20utilization.%20Our%20findings%20challenge%20conventional%20assumptions%20about%20LLM%0Ainference%2C%20offering%20new%20insights%20and%20practical%20strategies%20for%20improving%0Aresource%20utilization%2C%20particularly%20for%20smaller%20language%20models.%20The%20code%20is%0Apublicly%20available%20at%0Ahttps%3A//github.com/FerranAgulloLopez/vLLMBatchingMemoryGap.%0A&entry.1838667208=http%3A//arxiv.org/abs/2503.08311v2&entry.124074799=Read"},
{"title": "Normalized vs Diplomatic Annotation: A Case Study of Automatic\n  Information Extraction from Handwritten Uruguayan Birth Certificates", "author": "Natalia Bottaioli and Sol\u00e8ne Tarride and J\u00e9r\u00e9my Anger and Seginus Mowlavi and Marina Gardella and Antoine Tadros and Gabriele Facciolo and Rafael Grompone von Gioi and Christopher Kermorvant and Jean-Michel Morel and Javier Preciozzi", "abstract": "  This study evaluates the recently proposed Document Attention Network (DAN)\nfor extracting key-value information from Uruguayan birth certificates,\nhandwritten in Spanish. We investigate two annotation strategies for\nautomatically transcribing handwritten documents, fine-tuning DAN with minimal\ntraining data and annotation effort. Experiments were conducted on two datasets\ncontaining the same images (201 scans of birth certificates written by more\nthan 15 different writers) but with different annotation methods. Our findings\nindicate that normalized annotation is more effective for fields that can be\nstandardized, such as dates and places of birth, whereas diplomatic annotation\nperforms much better for fields containing names and surnames, which can not be\nstandardized.\n", "link": "http://arxiv.org/abs/2507.08636v1", "date": "2025-07-11", "relevancy": 1.3086, "topK": [{"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.4729}, {"title": "Multimodal-Conditioned Latent Diffusion Models for Fashion Image Editing", "link": "http://arxiv.org/abs/2403.14828v2", "similarity": 0.4361}, {"title": "UGG: Unified Generative Grasping", "link": "https://arxiv.org/abs/2311.16917", "similarity": 0.4215}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Normalized%20vs%20Diplomatic%20Annotation%3A%20A%20Case%20Study%20of%20Automatic%0A%20%20Information%20Extraction%20from%20Handwritten%20Uruguayan%20Birth%20Certificates&body=Title%3A%20Normalized%20vs%20Diplomatic%20Annotation%3A%20A%20Case%20Study%20of%20Automatic%0A%20%20Information%20Extraction%20from%20Handwritten%20Uruguayan%20Birth%20Certificates%0AAuthor%3A%20Natalia%20Bottaioli%20and%20Sol%C3%A8ne%20Tarride%20and%20J%C3%A9r%C3%A9my%20Anger%20and%20Seginus%20Mowlavi%20and%20Marina%20Gardella%20and%20Antoine%20Tadros%20and%20Gabriele%20Facciolo%20and%20Rafael%20Grompone%20von%20Gioi%20and%20Christopher%20Kermorvant%20and%20Jean-Michel%20Morel%20and%20Javier%20Preciozzi%0AAbstract%3A%20%20%20This%20study%20evaluates%20the%20recently%20proposed%20Document%20Attention%20Network%20%28DAN%29%0Afor%20extracting%20key-value%20information%20from%20Uruguayan%20birth%20certificates%2C%0Ahandwritten%20in%20Spanish.%20We%20investigate%20two%20annotation%20strategies%20for%0Aautomatically%20transcribing%20handwritten%20documents%2C%20fine-tuning%20DAN%20with%20minimal%0Atraining%20data%20and%20annotation%20effort.%20Experiments%20were%20conducted%20on%20two%20datasets%0Acontaining%20the%20same%20images%20%28201%20scans%20of%20birth%20certificates%20written%20by%20more%0Athan%2015%20different%20writers%29%20but%20with%20different%20annotation%20methods.%20Our%20findings%0Aindicate%20that%20normalized%20annotation%20is%20more%20effective%20for%20fields%20that%20can%20be%0Astandardized%2C%20such%20as%20dates%20and%20places%20of%20birth%2C%20whereas%20diplomatic%20annotation%0Aperforms%20much%20better%20for%20fields%20containing%20names%20and%20surnames%2C%20which%20can%20not%20be%0Astandardized.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2507.08636v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DNormalized%2520vs%2520Diplomatic%2520Annotation%253A%2520A%2520Case%2520Study%2520of%2520Automatic%250A%2520%2520Information%2520Extraction%2520from%2520Handwritten%2520Uruguayan%2520Birth%2520Certificates%26entry.906535625%3DNatalia%2520Bottaioli%2520and%2520Sol%25C3%25A8ne%2520Tarride%2520and%2520J%25C3%25A9r%25C3%25A9my%2520Anger%2520and%2520Seginus%2520Mowlavi%2520and%2520Marina%2520Gardella%2520and%2520Antoine%2520Tadros%2520and%2520Gabriele%2520Facciolo%2520and%2520Rafael%2520Grompone%2520von%2520Gioi%2520and%2520Christopher%2520Kermorvant%2520and%2520Jean-Michel%2520Morel%2520and%2520Javier%2520Preciozzi%26entry.1292438233%3D%2520%2520This%2520study%2520evaluates%2520the%2520recently%2520proposed%2520Document%2520Attention%2520Network%2520%2528DAN%2529%250Afor%2520extracting%2520key-value%2520information%2520from%2520Uruguayan%2520birth%2520certificates%252C%250Ahandwritten%2520in%2520Spanish.%2520We%2520investigate%2520two%2520annotation%2520strategies%2520for%250Aautomatically%2520transcribing%2520handwritten%2520documents%252C%2520fine-tuning%2520DAN%2520with%2520minimal%250Atraining%2520data%2520and%2520annotation%2520effort.%2520Experiments%2520were%2520conducted%2520on%2520two%2520datasets%250Acontaining%2520the%2520same%2520images%2520%2528201%2520scans%2520of%2520birth%2520certificates%2520written%2520by%2520more%250Athan%252015%2520different%2520writers%2529%2520but%2520with%2520different%2520annotation%2520methods.%2520Our%2520findings%250Aindicate%2520that%2520normalized%2520annotation%2520is%2520more%2520effective%2520for%2520fields%2520that%2520can%2520be%250Astandardized%252C%2520such%2520as%2520dates%2520and%2520places%2520of%2520birth%252C%2520whereas%2520diplomatic%2520annotation%250Aperforms%2520much%2520better%2520for%2520fields%2520containing%2520names%2520and%2520surnames%252C%2520which%2520can%2520not%2520be%250Astandardized.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2507.08636v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Normalized%20vs%20Diplomatic%20Annotation%3A%20A%20Case%20Study%20of%20Automatic%0A%20%20Information%20Extraction%20from%20Handwritten%20Uruguayan%20Birth%20Certificates&entry.906535625=Natalia%20Bottaioli%20and%20Sol%C3%A8ne%20Tarride%20and%20J%C3%A9r%C3%A9my%20Anger%20and%20Seginus%20Mowlavi%20and%20Marina%20Gardella%20and%20Antoine%20Tadros%20and%20Gabriele%20Facciolo%20and%20Rafael%20Grompone%20von%20Gioi%20and%20Christopher%20Kermorvant%20and%20Jean-Michel%20Morel%20and%20Javier%20Preciozzi&entry.1292438233=%20%20This%20study%20evaluates%20the%20recently%20proposed%20Document%20Attention%20Network%20%28DAN%29%0Afor%20extracting%20key-value%20information%20from%20Uruguayan%20birth%20certificates%2C%0Ahandwritten%20in%20Spanish.%20We%20investigate%20two%20annotation%20strategies%20for%0Aautomatically%20transcribing%20handwritten%20documents%2C%20fine-tuning%20DAN%20with%20minimal%0Atraining%20data%20and%20annotation%20effort.%20Experiments%20were%20conducted%20on%20two%20datasets%0Acontaining%20the%20same%20images%20%28201%20scans%20of%20birth%20certificates%20written%20by%20more%0Athan%2015%20different%20writers%29%20but%20with%20different%20annotation%20methods.%20Our%20findings%0Aindicate%20that%20normalized%20annotation%20is%20more%20effective%20for%20fields%20that%20can%20be%0Astandardized%2C%20such%20as%20dates%20and%20places%20of%20birth%2C%20whereas%20diplomatic%20annotation%0Aperforms%20much%20better%20for%20fields%20containing%20names%20and%20surnames%2C%20which%20can%20not%20be%0Astandardized.%0A&entry.1838667208=http%3A//arxiv.org/abs/2507.08636v1&entry.124074799=Read"},
{"title": "A Malliavin calculus approach to score functions in diffusion generative\n  models", "author": "Ehsan Mirafzali and Frank Proske and Utkarsh Gupta and Daniele Venturi and Razvan Marinescu", "abstract": "  Score-based diffusion generative models have recently emerged as a powerful\ntool for modelling complex data distributions. These models aim at learning the\nscore function, which defines a map from a known probability distribution to\nthe target data distribution via deterministic or stochastic differential\nequations (SDEs). The score function is typically estimated from data using a\nvariety of approximation techniques, such as denoising or sliced score\nmatching, Hyv\\\"arien's method, or Schr\\\"odinger bridges. In this paper, we\nderive an exact, closed form, expression for the score function for a broad\nclass of nonlinear diffusion generative models. Our approach combines modern\nstochastic analysis tools such as Malliavin derivatives and their adjoint\noperators (Skorokhod integrals or Malliavin Divergence) with a new Bismut-type\nformula. The resulting expression for the score function can be written\nentirely in terms of the first and second variation processes, with all\nMalliavin derivatives systematically eliminated, thereby enhancing its\npractical applicability. The theoretical framework presented in this work\noffers a principled foundation for advancing score estimation methods in\ngenerative modelling, enabling the design of new sampling algorithms for\ncomplex probability distributions. Our results can be extended to broader\nclasses of stochastic differential equations, opening new directions for the\ndevelopment of score-based diffusion generative models.\n", "link": "http://arxiv.org/abs/2507.05550v2", "date": "2025-07-11", "relevancy": 0.9928, "topK": [{"title": "SDXS: Real-Time One-Step Latent Diffusion Models with Image Conditions", "link": "http://arxiv.org/abs/2403.16627v1", "similarity": 0.5093}, {"title": "Thinking Outside the BBox: Unconstrained Generative Object Compositing", "link": "http://arxiv.org/abs/2409.04559v2", "similarity": 0.4928}, {"title": "Scaling Rectified Flow Transformers for High-Resolution Image Synthesis", "link": "http://arxiv.org/abs/2403.03206v1", "similarity": 0.4871}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20A%20Malliavin%20calculus%20approach%20to%20score%20functions%20in%20diffusion%20generative%0A%20%20models&body=Title%3A%20A%20Malliavin%20calculus%20approach%20to%20score%20functions%20in%20diffusion%20generative%0A%20%20models%0AAuthor%3A%20Ehsan%20Mirafzali%20and%20Frank%20Proske%20and%20Utkarsh%20Gupta%20and%20Daniele%20Venturi%20and%20Razvan%20Marinescu%0AAbstract%3A%20%20%20Score-based%20diffusion%20generative%20models%20have%20recently%20emerged%20as%20a%20powerful%0Atool%20for%20modelling%20complex%20data%20distributions.%20These%20models%20aim%20at%20learning%20the%0Ascore%20function%2C%20which%20defines%20a%20map%20from%20a%20known%20probability%20distribution%20to%0Athe%20target%20data%20distribution%20via%20deterministic%20or%20stochastic%20differential%0Aequations%20%28SDEs%29.%20The%20score%20function%20is%20typically%20estimated%20from%20data%20using%20a%0Avariety%20of%20approximation%20techniques%2C%20such%20as%20denoising%20or%20sliced%20score%0Amatching%2C%20Hyv%5C%22arien%27s%20method%2C%20or%20Schr%5C%22odinger%20bridges.%20In%20this%20paper%2C%20we%0Aderive%20an%20exact%2C%20closed%20form%2C%20expression%20for%20the%20score%20function%20for%20a%20broad%0Aclass%20of%20nonlinear%20diffusion%20generative%20models.%20Our%20approach%20combines%20modern%0Astochastic%20analysis%20tools%20such%20as%20Malliavin%20derivatives%20and%20their%20adjoint%0Aoperators%20%28Skorokhod%20integrals%20or%20Malliavin%20Divergence%29%20with%20a%20new%20Bismut-type%0Aformula.%20The%20resulting%20expression%20for%20the%20score%20function%20can%20be%20written%0Aentirely%20in%20terms%20of%20the%20first%20and%20second%20variation%20processes%2C%20with%20all%0AMalliavin%20derivatives%20systematically%20eliminated%2C%20thereby%20enhancing%20its%0Apractical%20applicability.%20The%20theoretical%20framework%20presented%20in%20this%20work%0Aoffers%20a%20principled%20foundation%20for%20advancing%20score%20estimation%20methods%20in%0Agenerative%20modelling%2C%20enabling%20the%20design%20of%20new%20sampling%20algorithms%20for%0Acomplex%20probability%20distributions.%20Our%20results%20can%20be%20extended%20to%20broader%0Aclasses%20of%20stochastic%20differential%20equations%2C%20opening%20new%20directions%20for%20the%0Adevelopment%20of%20score-based%20diffusion%20generative%20models.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2507.05550v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DA%2520Malliavin%2520calculus%2520approach%2520to%2520score%2520functions%2520in%2520diffusion%2520generative%250A%2520%2520models%26entry.906535625%3DEhsan%2520Mirafzali%2520and%2520Frank%2520Proske%2520and%2520Utkarsh%2520Gupta%2520and%2520Daniele%2520Venturi%2520and%2520Razvan%2520Marinescu%26entry.1292438233%3D%2520%2520Score-based%2520diffusion%2520generative%2520models%2520have%2520recently%2520emerged%2520as%2520a%2520powerful%250Atool%2520for%2520modelling%2520complex%2520data%2520distributions.%2520These%2520models%2520aim%2520at%2520learning%2520the%250Ascore%2520function%252C%2520which%2520defines%2520a%2520map%2520from%2520a%2520known%2520probability%2520distribution%2520to%250Athe%2520target%2520data%2520distribution%2520via%2520deterministic%2520or%2520stochastic%2520differential%250Aequations%2520%2528SDEs%2529.%2520The%2520score%2520function%2520is%2520typically%2520estimated%2520from%2520data%2520using%2520a%250Avariety%2520of%2520approximation%2520techniques%252C%2520such%2520as%2520denoising%2520or%2520sliced%2520score%250Amatching%252C%2520Hyv%255C%2522arien%2527s%2520method%252C%2520or%2520Schr%255C%2522odinger%2520bridges.%2520In%2520this%2520paper%252C%2520we%250Aderive%2520an%2520exact%252C%2520closed%2520form%252C%2520expression%2520for%2520the%2520score%2520function%2520for%2520a%2520broad%250Aclass%2520of%2520nonlinear%2520diffusion%2520generative%2520models.%2520Our%2520approach%2520combines%2520modern%250Astochastic%2520analysis%2520tools%2520such%2520as%2520Malliavin%2520derivatives%2520and%2520their%2520adjoint%250Aoperators%2520%2528Skorokhod%2520integrals%2520or%2520Malliavin%2520Divergence%2529%2520with%2520a%2520new%2520Bismut-type%250Aformula.%2520The%2520resulting%2520expression%2520for%2520the%2520score%2520function%2520can%2520be%2520written%250Aentirely%2520in%2520terms%2520of%2520the%2520first%2520and%2520second%2520variation%2520processes%252C%2520with%2520all%250AMalliavin%2520derivatives%2520systematically%2520eliminated%252C%2520thereby%2520enhancing%2520its%250Apractical%2520applicability.%2520The%2520theoretical%2520framework%2520presented%2520in%2520this%2520work%250Aoffers%2520a%2520principled%2520foundation%2520for%2520advancing%2520score%2520estimation%2520methods%2520in%250Agenerative%2520modelling%252C%2520enabling%2520the%2520design%2520of%2520new%2520sampling%2520algorithms%2520for%250Acomplex%2520probability%2520distributions.%2520Our%2520results%2520can%2520be%2520extended%2520to%2520broader%250Aclasses%2520of%2520stochastic%2520differential%2520equations%252C%2520opening%2520new%2520directions%2520for%2520the%250Adevelopment%2520of%2520score-based%2520diffusion%2520generative%2520models.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2507.05550v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=A%20Malliavin%20calculus%20approach%20to%20score%20functions%20in%20diffusion%20generative%0A%20%20models&entry.906535625=Ehsan%20Mirafzali%20and%20Frank%20Proske%20and%20Utkarsh%20Gupta%20and%20Daniele%20Venturi%20and%20Razvan%20Marinescu&entry.1292438233=%20%20Score-based%20diffusion%20generative%20models%20have%20recently%20emerged%20as%20a%20powerful%0Atool%20for%20modelling%20complex%20data%20distributions.%20These%20models%20aim%20at%20learning%20the%0Ascore%20function%2C%20which%20defines%20a%20map%20from%20a%20known%20probability%20distribution%20to%0Athe%20target%20data%20distribution%20via%20deterministic%20or%20stochastic%20differential%0Aequations%20%28SDEs%29.%20The%20score%20function%20is%20typically%20estimated%20from%20data%20using%20a%0Avariety%20of%20approximation%20techniques%2C%20such%20as%20denoising%20or%20sliced%20score%0Amatching%2C%20Hyv%5C%22arien%27s%20method%2C%20or%20Schr%5C%22odinger%20bridges.%20In%20this%20paper%2C%20we%0Aderive%20an%20exact%2C%20closed%20form%2C%20expression%20for%20the%20score%20function%20for%20a%20broad%0Aclass%20of%20nonlinear%20diffusion%20generative%20models.%20Our%20approach%20combines%20modern%0Astochastic%20analysis%20tools%20such%20as%20Malliavin%20derivatives%20and%20their%20adjoint%0Aoperators%20%28Skorokhod%20integrals%20or%20Malliavin%20Divergence%29%20with%20a%20new%20Bismut-type%0Aformula.%20The%20resulting%20expression%20for%20the%20score%20function%20can%20be%20written%0Aentirely%20in%20terms%20of%20the%20first%20and%20second%20variation%20processes%2C%20with%20all%0AMalliavin%20derivatives%20systematically%20eliminated%2C%20thereby%20enhancing%20its%0Apractical%20applicability.%20The%20theoretical%20framework%20presented%20in%20this%20work%0Aoffers%20a%20principled%20foundation%20for%20advancing%20score%20estimation%20methods%20in%0Agenerative%20modelling%2C%20enabling%20the%20design%20of%20new%20sampling%20algorithms%20for%0Acomplex%20probability%20distributions.%20Our%20results%20can%20be%20extended%20to%20broader%0Aclasses%20of%20stochastic%20differential%20equations%2C%20opening%20new%20directions%20for%20the%0Adevelopment%20of%20score-based%20diffusion%20generative%20models.%0A&entry.1838667208=http%3A//arxiv.org/abs/2507.05550v2&entry.124074799=Read"},
{"title": "ML-Based Automata Simplification for Symbolic Accelerators", "author": "Tiffany Yu and Rye Stahle-Smith and Darssan Eswaramoorthi and Rasha Karakchi", "abstract": "  Symbolic accelerators are increasingly used for symbolic data processing in\ndomains such as genomics, NLP, and cybersecurity. However, these accelerators\nface scalability issues due to excessive memory use and routing complexity,\nespecially when targeting a large set. We present AutoSlim, a machine\nlearning-based graph simplification framework designed to reduce the complexity\nof symbolic accelerators built on Non-deterministic Finite Automata (NFA)\ndeployed on FPGA-based overlays such as NAPOLY+. AutoSlim uses Random Forest\nclassification to prune low-impact transitions based on edge scores and\nstructural features, significantly reducing automata graph density while\npreserving semantic correctness. Unlike prior tools, AutoSlim targets automated\nscore-aware simplification with weighted transitions, enabling efficient\nranking-based sequence analysis. We evaluated data sets (1K to 64K nodes) in\nNAPOLY+ and conducted performance measurements including latency, throughput,\nand resource usage. AutoSlim achieves up to 40 percent reduction in FPGA LUTs\nand over 30 percent pruning in transitions, while scaling to graphs an order of\nmagnitude larger than existing benchmarks. Our results also demonstrate how\nhardware interconnection (fanout) heavily influences hardware cost and that\nAutoSlim's pruning mitigates resource blowup.\n", "link": "http://arxiv.org/abs/2507.08751v1", "date": "2025-07-11", "relevancy": 1.2867, "topK": [{"title": "ToDo: Token Downsampling for Efficient Generation of High-Resolution\n  Images", "link": "http://arxiv.org/abs/2402.13573v2", "similarity": 0.4355}, {"title": "RoboEXP: Action-Conditioned Scene Graph via Interactive Exploration for\n  Robotic Manipulation", "link": "http://arxiv.org/abs/2402.15487v1", "similarity": 0.4215}, {"title": "SDXS: Real-Time One-Step Latent Diffusion Models with Image Conditions", "link": "http://arxiv.org/abs/2403.16627v1", "similarity": 0.4199}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20ML-Based%20Automata%20Simplification%20for%20Symbolic%20Accelerators&body=Title%3A%20ML-Based%20Automata%20Simplification%20for%20Symbolic%20Accelerators%0AAuthor%3A%20Tiffany%20Yu%20and%20Rye%20Stahle-Smith%20and%20Darssan%20Eswaramoorthi%20and%20Rasha%20Karakchi%0AAbstract%3A%20%20%20Symbolic%20accelerators%20are%20increasingly%20used%20for%20symbolic%20data%20processing%20in%0Adomains%20such%20as%20genomics%2C%20NLP%2C%20and%20cybersecurity.%20However%2C%20these%20accelerators%0Aface%20scalability%20issues%20due%20to%20excessive%20memory%20use%20and%20routing%20complexity%2C%0Aespecially%20when%20targeting%20a%20large%20set.%20We%20present%20AutoSlim%2C%20a%20machine%0Alearning-based%20graph%20simplification%20framework%20designed%20to%20reduce%20the%20complexity%0Aof%20symbolic%20accelerators%20built%20on%20Non-deterministic%20Finite%20Automata%20%28NFA%29%0Adeployed%20on%20FPGA-based%20overlays%20such%20as%20NAPOLY%2B.%20AutoSlim%20uses%20Random%20Forest%0Aclassification%20to%20prune%20low-impact%20transitions%20based%20on%20edge%20scores%20and%0Astructural%20features%2C%20significantly%20reducing%20automata%20graph%20density%20while%0Apreserving%20semantic%20correctness.%20Unlike%20prior%20tools%2C%20AutoSlim%20targets%20automated%0Ascore-aware%20simplification%20with%20weighted%20transitions%2C%20enabling%20efficient%0Aranking-based%20sequence%20analysis.%20We%20evaluated%20data%20sets%20%281K%20to%2064K%20nodes%29%20in%0ANAPOLY%2B%20and%20conducted%20performance%20measurements%20including%20latency%2C%20throughput%2C%0Aand%20resource%20usage.%20AutoSlim%20achieves%20up%20to%2040%20percent%20reduction%20in%20FPGA%20LUTs%0Aand%20over%2030%20percent%20pruning%20in%20transitions%2C%20while%20scaling%20to%20graphs%20an%20order%20of%0Amagnitude%20larger%20than%20existing%20benchmarks.%20Our%20results%20also%20demonstrate%20how%0Ahardware%20interconnection%20%28fanout%29%20heavily%20influences%20hardware%20cost%20and%20that%0AAutoSlim%27s%20pruning%20mitigates%20resource%20blowup.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2507.08751v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DML-Based%2520Automata%2520Simplification%2520for%2520Symbolic%2520Accelerators%26entry.906535625%3DTiffany%2520Yu%2520and%2520Rye%2520Stahle-Smith%2520and%2520Darssan%2520Eswaramoorthi%2520and%2520Rasha%2520Karakchi%26entry.1292438233%3D%2520%2520Symbolic%2520accelerators%2520are%2520increasingly%2520used%2520for%2520symbolic%2520data%2520processing%2520in%250Adomains%2520such%2520as%2520genomics%252C%2520NLP%252C%2520and%2520cybersecurity.%2520However%252C%2520these%2520accelerators%250Aface%2520scalability%2520issues%2520due%2520to%2520excessive%2520memory%2520use%2520and%2520routing%2520complexity%252C%250Aespecially%2520when%2520targeting%2520a%2520large%2520set.%2520We%2520present%2520AutoSlim%252C%2520a%2520machine%250Alearning-based%2520graph%2520simplification%2520framework%2520designed%2520to%2520reduce%2520the%2520complexity%250Aof%2520symbolic%2520accelerators%2520built%2520on%2520Non-deterministic%2520Finite%2520Automata%2520%2528NFA%2529%250Adeployed%2520on%2520FPGA-based%2520overlays%2520such%2520as%2520NAPOLY%252B.%2520AutoSlim%2520uses%2520Random%2520Forest%250Aclassification%2520to%2520prune%2520low-impact%2520transitions%2520based%2520on%2520edge%2520scores%2520and%250Astructural%2520features%252C%2520significantly%2520reducing%2520automata%2520graph%2520density%2520while%250Apreserving%2520semantic%2520correctness.%2520Unlike%2520prior%2520tools%252C%2520AutoSlim%2520targets%2520automated%250Ascore-aware%2520simplification%2520with%2520weighted%2520transitions%252C%2520enabling%2520efficient%250Aranking-based%2520sequence%2520analysis.%2520We%2520evaluated%2520data%2520sets%2520%25281K%2520to%252064K%2520nodes%2529%2520in%250ANAPOLY%252B%2520and%2520conducted%2520performance%2520measurements%2520including%2520latency%252C%2520throughput%252C%250Aand%2520resource%2520usage.%2520AutoSlim%2520achieves%2520up%2520to%252040%2520percent%2520reduction%2520in%2520FPGA%2520LUTs%250Aand%2520over%252030%2520percent%2520pruning%2520in%2520transitions%252C%2520while%2520scaling%2520to%2520graphs%2520an%2520order%2520of%250Amagnitude%2520larger%2520than%2520existing%2520benchmarks.%2520Our%2520results%2520also%2520demonstrate%2520how%250Ahardware%2520interconnection%2520%2528fanout%2529%2520heavily%2520influences%2520hardware%2520cost%2520and%2520that%250AAutoSlim%2527s%2520pruning%2520mitigates%2520resource%2520blowup.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2507.08751v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=ML-Based%20Automata%20Simplification%20for%20Symbolic%20Accelerators&entry.906535625=Tiffany%20Yu%20and%20Rye%20Stahle-Smith%20and%20Darssan%20Eswaramoorthi%20and%20Rasha%20Karakchi&entry.1292438233=%20%20Symbolic%20accelerators%20are%20increasingly%20used%20for%20symbolic%20data%20processing%20in%0Adomains%20such%20as%20genomics%2C%20NLP%2C%20and%20cybersecurity.%20However%2C%20these%20accelerators%0Aface%20scalability%20issues%20due%20to%20excessive%20memory%20use%20and%20routing%20complexity%2C%0Aespecially%20when%20targeting%20a%20large%20set.%20We%20present%20AutoSlim%2C%20a%20machine%0Alearning-based%20graph%20simplification%20framework%20designed%20to%20reduce%20the%20complexity%0Aof%20symbolic%20accelerators%20built%20on%20Non-deterministic%20Finite%20Automata%20%28NFA%29%0Adeployed%20on%20FPGA-based%20overlays%20such%20as%20NAPOLY%2B.%20AutoSlim%20uses%20Random%20Forest%0Aclassification%20to%20prune%20low-impact%20transitions%20based%20on%20edge%20scores%20and%0Astructural%20features%2C%20significantly%20reducing%20automata%20graph%20density%20while%0Apreserving%20semantic%20correctness.%20Unlike%20prior%20tools%2C%20AutoSlim%20targets%20automated%0Ascore-aware%20simplification%20with%20weighted%20transitions%2C%20enabling%20efficient%0Aranking-based%20sequence%20analysis.%20We%20evaluated%20data%20sets%20%281K%20to%2064K%20nodes%29%20in%0ANAPOLY%2B%20and%20conducted%20performance%20measurements%20including%20latency%2C%20throughput%2C%0Aand%20resource%20usage.%20AutoSlim%20achieves%20up%20to%2040%20percent%20reduction%20in%20FPGA%20LUTs%0Aand%20over%2030%20percent%20pruning%20in%20transitions%2C%20while%20scaling%20to%20graphs%20an%20order%20of%0Amagnitude%20larger%20than%20existing%20benchmarks.%20Our%20results%20also%20demonstrate%20how%0Ahardware%20interconnection%20%28fanout%29%20heavily%20influences%20hardware%20cost%20and%20that%0AAutoSlim%27s%20pruning%20mitigates%20resource%20blowup.%0A&entry.1838667208=http%3A//arxiv.org/abs/2507.08751v1&entry.124074799=Read"},
      ];
      const content = document.getElementById('content');
      function createPostElement(post) {
        const postElement = document.createElement('div');
        postElement.className = 'post';
        const dateElem = document.createElement('p');
        dateElem.setAttribute("class", "date");
        dateElem.textContent = post.date;
        postElement.appendChild(dateElem);

        const textElem = document.createElement('p');
        textElem.setAttribute("class", "text");
        const titleElem = document.createElement('p');
        titleElem.setAttribute("class", "title");
        titleElem.textContent = post.title;
        textElem.appendChild(titleElem);
        const authorElem = document.createElement('p');
        authorElem.setAttribute("class", "author");
        authorElem.textContent = post.author;
        textElem.appendChild(authorElem);
        const abstractElem = document.createElement('p');
        abstractElem.setAttribute("class", "abstract");
        abstractElem.textContent = post.abstract;
        textElem.appendChild(abstractElem);

        const linkElement = document.createElement('a');
        linkElement.setAttribute("class", "link");
        linkElement.href = post.link;
        linkElement.target = "_blank";
        linkElement.textContent = post.link.length > 50 ? post.link.substring(0, 50) + '...' : post.link;
        textElem.appendChild(linkElement);
        postElement.appendChild(textElem);

        const linkElementContainer = document.createElement('div');
        linkElementContainer.setAttribute("class", "comment");
        const actionElement = document.createElement('a');
        actionElement.setAttribute("class", "comment");
        actionElement.href = post.form;
        actionElement.textContent = "Action";
        actionElement.target = "_blank";
        linkElementContainer.appendChild(actionElement);
        const emailElement = document.createElement('a');
        emailElement.setAttribute("class", "comment");
        emailElement.href = post.mailto;
        emailElement.textContent = "Email";
        emailElement.target = "_blank";
        linkElementContainer.appendChild(emailElement);
        postElement.appendChild(linkElementContainer);
        const e = document.createElement('div');
        e.setAttribute("class", "clear");
        postElement.appendChild(e);

        const relevancyContainer = document.createElement('div');
        const relevancyValElem = document.createElement('p');
        relevancyValElem.textContent = "Relevancy " + post.relevancy;
        relevancyContainer.appendChild(relevancyValElem);
        post.topK.forEach((sub) => {
          const topKElem = document.createElement('a');
          topKElem.setAttribute("class", "topK");
          topKElem.href = sub.link;
          topKElem.textContent = sub.title + " (" + sub.similarity + ")";
          topKElem.target = "_blank";
          relevancyContainer.appendChild(topKElem);
        });
        postElement.appendChild(relevancyContainer);
        return postElement;
      }
      function loadPosts() {
        // Simulate loading more posts
        posts.forEach((post) => {
          const postElement = createPostElement(post);
          content.appendChild(postElement);
        });
      }
      // Load initial posts
      loadPosts();
    </script>

  </body>
</html>


