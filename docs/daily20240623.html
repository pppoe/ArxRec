<!DOCTYPE html>
<html lang="en">
  <head>
    <script async src="https://www.googletagmanager.com/gtag/js?id=G-V34CNNDP8V"></script>
    <script>
      window.dataLayer = window.dataLayer || [];
      function gtag(){dataLayer.push(arguments);}
      gtag('js', new Date());
      gtag('config', 'G-V34CNNDP8V');
    </script>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Arxiv Paper Selection</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.3/css/all.min.css" crossorigin="anonymous" referrerpolicy="no-referrer" />
    <style>
    body {
      font-family: Arial, sans-serif;
      margin: 0;
      padding: 0;
      background-color: #f4f4f4;
    }
    header {
      position: fixed;
      top: 0;
      left: 0;
      width: 100%;
      background-color: #ffffff;
      color: black;
      padding: 10px;
      text-align: center;
      z-index: 1000;
      border-bottom: 1px solid #ddd;
    }
    header div {
      display: block;
      margin: 10px auto;
    }

    #home-icon {
      display: block;
      float: left;
      margin: 5px;
      text-decoration: none;
      color: black;
    }

    main {
      margin-top: 60px; /* Adjusted margin to account for fixed header */
      padding: 20px;
    }

    .post {
      background-color: white;
      border: 1px solid #ddd;
      border-radius: 5px;
      margin-bottom: 10px;
      padding: 10px 20px;
      max-height: 2000px;
      overflow: scroll;
    }
    .post img {
      display: block;
      margin-top: 5px;
      max-width: auto;
      max-height: 100px;
    }
    .post .clear {
      clear: both;
      display: block;
    }
    .post a {
      text-decoration: none;
    }
    .post a:hover {
      color: #0056b3;
    }
    .post a:visited {
      color: #0056b3;
    }
    .post div.comment {
      text-align: right;
    }
    .post div.comment a {
      margin: 1em;
    }
    .post .text {
      margin: 1em 0em;
      padding: 0;
    }
    .post .text .title {
    }
    .post .text .author {
    }
    .post .text .abstract {
    }
    .post .topK {
      display: block;
      margin: 0.5em;
    }
    .post .date {
      margin: 0;
      padding: 0;
      text-size: small; 
      color: gray;
    }
    .post .link {
      margin: 0;
      padding: 0;
    }
    @media screen and (max-width: 600px) {
      body {
        max-width: 100%; 
      }
      #home-icon {
        float: none;
        display: block;
        text-align: center;
        margin-bottom: 10px;
      }
    }
    footer {
      width: 100%;
      background-color: #ddd;
      text-align: center;
      z-index: 1000;
      padding: 20px 0px;
      margin-bottom: 20px;
      left: 0;
    }

    #next-btn,
    #prev-btn {
      background-color: #4CAF50;
      color: white;
      padding: 8px 16px;
      margin: 0 50px;
      border: none;
      border-radius: 4px;
      cursor: pointer;
    }

    .links {
      padding: 20px;
    }
    .links a {
      text-decoration: none;
    }
    .links a:hover {
      color: #0056b3;
    }
    .links a:visited {
      color: #0056b3;
    }

    #page-index {
      font-size: small;
    }
    .ads {
      width: 100%;
    }
    #prev-btn {
      background-color: #4CAF50;
      color: white;
      padding: 8px 16px;
      margin: 0 50px;
      border: none;
      border-radius: 4px;
      cursor: pointer;
    }
    </style>
  </head>
  <body>

    <header>
      <a id="prev-btn" href="daily20240619.html"><i class="fas fa-chevron-left"></i></a>
      <a href="https://haoxiang.org/">About</a>
    </header>

    <main id="content">
      <!-- Posts will be dynamically added here using JavaScript -->
    </main>

    <script>
      // Dummy data for posts
      const posts = [
{"title": "Action2Sound: Ambient-Aware Generation of Action Sounds from Egocentric\n  Videos", "author": "Changan Chen and Puyuan Peng and Ami Baid and Zihui Xue and Wei-Ning Hsu and David Harwath and Kristen Grauman", "abstract": "  Generating realistic audio for human interactions is important for many\napplications, such as creating sound effects for films or virtual reality\ngames. Existing approaches implicitly assume total correspondence between the\nvideo and audio during training, yet many sounds happen off-screen and have\nweak to no correspondence with the visuals -- resulting in uncontrolled ambient\nsounds or hallucinations at test time. We propose a novel ambient-aware audio\ngeneration model, AV-LDM. We devise a novel audio-conditioning mechanism to\nlearn to disentangle foreground action sounds from the ambient background\nsounds in in-the-wild training videos. Given a novel silent video, our model\nuses retrieval-augmented generation to create audio that matches the visual\ncontent both semantically and temporally. We train and evaluate our model on\ntwo in-the-wild egocentric video datasets Ego4D and EPIC-KITCHENS. Our model\noutperforms an array of existing methods, allows controllable generation of the\nambient sound, and even shows promise for generalizing to computer graphics\ngame clips. Overall, our work is the first to focus video-to-audio generation\nfaithfully on the observed visual content despite training from uncurated clips\nwith natural background sounds.\n", "link": "http://arxiv.org/abs/2406.09272v2", "date": "2024-06-20", "relevancy": 2.9006, "topK": [{"title": "Genie: Generative Interactive Environments", "link": "http://arxiv.org/abs/2402.15391v1", "similarity": 0.6183}, {"title": "Magic-Me: Identity-Specific Video Customized Diffusion", "link": "http://arxiv.org/abs/2402.09368v1", "similarity": 0.5639}, {"title": "Tuning-Free Noise Rectification for High Fidelity Image-to-Video\n  Generation", "link": "http://arxiv.org/abs/2403.02827v1", "similarity": 0.5581}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Action2Sound%3A%20Ambient-Aware%20Generation%20of%20Action%20Sounds%20from%20Egocentric%0A%20%20Videos&body=Title%3A%20Action2Sound%3A%20Ambient-Aware%20Generation%20of%20Action%20Sounds%20from%20Egocentric%0A%20%20Videos%0AAuthor%3A%20Changan%20Chen%20and%20Puyuan%20Peng%20and%20Ami%20Baid%20and%20Zihui%20Xue%20and%20Wei-Ning%20Hsu%20and%20David%20Harwath%20and%20Kristen%20Grauman%0AAbstract%3A%20%20%20Generating%20realistic%20audio%20for%20human%20interactions%20is%20important%20for%20many%0Aapplications%2C%20such%20as%20creating%20sound%20effects%20for%20films%20or%20virtual%20reality%0Agames.%20Existing%20approaches%20implicitly%20assume%20total%20correspondence%20between%20the%0Avideo%20and%20audio%20during%20training%2C%20yet%20many%20sounds%20happen%20off-screen%20and%20have%0Aweak%20to%20no%20correspondence%20with%20the%20visuals%20--%20resulting%20in%20uncontrolled%20ambient%0Asounds%20or%20hallucinations%20at%20test%20time.%20We%20propose%20a%20novel%20ambient-aware%20audio%0Ageneration%20model%2C%20AV-LDM.%20We%20devise%20a%20novel%20audio-conditioning%20mechanism%20to%0Alearn%20to%20disentangle%20foreground%20action%20sounds%20from%20the%20ambient%20background%0Asounds%20in%20in-the-wild%20training%20videos.%20Given%20a%20novel%20silent%20video%2C%20our%20model%0Auses%20retrieval-augmented%20generation%20to%20create%20audio%20that%20matches%20the%20visual%0Acontent%20both%20semantically%20and%20temporally.%20We%20train%20and%20evaluate%20our%20model%20on%0Atwo%20in-the-wild%20egocentric%20video%20datasets%20Ego4D%20and%20EPIC-KITCHENS.%20Our%20model%0Aoutperforms%20an%20array%20of%20existing%20methods%2C%20allows%20controllable%20generation%20of%20the%0Aambient%20sound%2C%20and%20even%20shows%20promise%20for%20generalizing%20to%20computer%20graphics%0Agame%20clips.%20Overall%2C%20our%20work%20is%20the%20first%20to%20focus%20video-to-audio%20generation%0Afaithfully%20on%20the%20observed%20visual%20content%20despite%20training%20from%20uncurated%20clips%0Awith%20natural%20background%20sounds.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2406.09272v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DAction2Sound%253A%2520Ambient-Aware%2520Generation%2520of%2520Action%2520Sounds%2520from%2520Egocentric%250A%2520%2520Videos%26entry.906535625%3DChangan%2520Chen%2520and%2520Puyuan%2520Peng%2520and%2520Ami%2520Baid%2520and%2520Zihui%2520Xue%2520and%2520Wei-Ning%2520Hsu%2520and%2520David%2520Harwath%2520and%2520Kristen%2520Grauman%26entry.1292438233%3D%2520%2520Generating%2520realistic%2520audio%2520for%2520human%2520interactions%2520is%2520important%2520for%2520many%250Aapplications%252C%2520such%2520as%2520creating%2520sound%2520effects%2520for%2520films%2520or%2520virtual%2520reality%250Agames.%2520Existing%2520approaches%2520implicitly%2520assume%2520total%2520correspondence%2520between%2520the%250Avideo%2520and%2520audio%2520during%2520training%252C%2520yet%2520many%2520sounds%2520happen%2520off-screen%2520and%2520have%250Aweak%2520to%2520no%2520correspondence%2520with%2520the%2520visuals%2520--%2520resulting%2520in%2520uncontrolled%2520ambient%250Asounds%2520or%2520hallucinations%2520at%2520test%2520time.%2520We%2520propose%2520a%2520novel%2520ambient-aware%2520audio%250Ageneration%2520model%252C%2520AV-LDM.%2520We%2520devise%2520a%2520novel%2520audio-conditioning%2520mechanism%2520to%250Alearn%2520to%2520disentangle%2520foreground%2520action%2520sounds%2520from%2520the%2520ambient%2520background%250Asounds%2520in%2520in-the-wild%2520training%2520videos.%2520Given%2520a%2520novel%2520silent%2520video%252C%2520our%2520model%250Auses%2520retrieval-augmented%2520generation%2520to%2520create%2520audio%2520that%2520matches%2520the%2520visual%250Acontent%2520both%2520semantically%2520and%2520temporally.%2520We%2520train%2520and%2520evaluate%2520our%2520model%2520on%250Atwo%2520in-the-wild%2520egocentric%2520video%2520datasets%2520Ego4D%2520and%2520EPIC-KITCHENS.%2520Our%2520model%250Aoutperforms%2520an%2520array%2520of%2520existing%2520methods%252C%2520allows%2520controllable%2520generation%2520of%2520the%250Aambient%2520sound%252C%2520and%2520even%2520shows%2520promise%2520for%2520generalizing%2520to%2520computer%2520graphics%250Agame%2520clips.%2520Overall%252C%2520our%2520work%2520is%2520the%2520first%2520to%2520focus%2520video-to-audio%2520generation%250Afaithfully%2520on%2520the%2520observed%2520visual%2520content%2520despite%2520training%2520from%2520uncurated%2520clips%250Awith%2520natural%2520background%2520sounds.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2406.09272v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Action2Sound%3A%20Ambient-Aware%20Generation%20of%20Action%20Sounds%20from%20Egocentric%0A%20%20Videos&entry.906535625=Changan%20Chen%20and%20Puyuan%20Peng%20and%20Ami%20Baid%20and%20Zihui%20Xue%20and%20Wei-Ning%20Hsu%20and%20David%20Harwath%20and%20Kristen%20Grauman&entry.1292438233=%20%20Generating%20realistic%20audio%20for%20human%20interactions%20is%20important%20for%20many%0Aapplications%2C%20such%20as%20creating%20sound%20effects%20for%20films%20or%20virtual%20reality%0Agames.%20Existing%20approaches%20implicitly%20assume%20total%20correspondence%20between%20the%0Avideo%20and%20audio%20during%20training%2C%20yet%20many%20sounds%20happen%20off-screen%20and%20have%0Aweak%20to%20no%20correspondence%20with%20the%20visuals%20--%20resulting%20in%20uncontrolled%20ambient%0Asounds%20or%20hallucinations%20at%20test%20time.%20We%20propose%20a%20novel%20ambient-aware%20audio%0Ageneration%20model%2C%20AV-LDM.%20We%20devise%20a%20novel%20audio-conditioning%20mechanism%20to%0Alearn%20to%20disentangle%20foreground%20action%20sounds%20from%20the%20ambient%20background%0Asounds%20in%20in-the-wild%20training%20videos.%20Given%20a%20novel%20silent%20video%2C%20our%20model%0Auses%20retrieval-augmented%20generation%20to%20create%20audio%20that%20matches%20the%20visual%0Acontent%20both%20semantically%20and%20temporally.%20We%20train%20and%20evaluate%20our%20model%20on%0Atwo%20in-the-wild%20egocentric%20video%20datasets%20Ego4D%20and%20EPIC-KITCHENS.%20Our%20model%0Aoutperforms%20an%20array%20of%20existing%20methods%2C%20allows%20controllable%20generation%20of%20the%0Aambient%20sound%2C%20and%20even%20shows%20promise%20for%20generalizing%20to%20computer%20graphics%0Agame%20clips.%20Overall%2C%20our%20work%20is%20the%20first%20to%20focus%20video-to-audio%20generation%0Afaithfully%20on%20the%20observed%20visual%20content%20despite%20training%20from%20uncurated%20clips%0Awith%20natural%20background%20sounds.%0A&entry.1838667208=http%3A//arxiv.org/abs/2406.09272v2&entry.124074799=Read"},
{"title": "GLIMPSE: Generalized Local Imaging with MLPs", "author": "AmirEhsan Khorashadizadeh and Valentin Debarnot and Tianlin Liu and Ivan Dokmani\u0107", "abstract": "  Deep learning is the current de facto state of the art in tomographic\nimaging. A common approach is to feed the result of a simple inversion, for\nexample the backprojection, to a convolutional neural network (CNN) which then\ncomputes the reconstruction. Despite strong results on 'in-distribution' test\ndata similar to the training data, backprojection from sparse-view data\ndelocalizes singularities, so these approaches require a large receptive field\nto perform well. As a consequence, they overfit to certain global structures\nwhich leads to poor generalization on out-of-distribution (OOD) samples.\nMoreover, their memory complexity and training time scale unfavorably with\nimage resolution, making them impractical for application at realistic clinical\nresolutions, especially in 3D: a standard U-Net requires a substantial 140GB of\nmemory and 2600 seconds per epoch on a research-grade GPU when training on\n1024x1024 images. In this paper, we introduce GLIMPSE, a local processing\nneural network for computed tomography which reconstructs a pixel value by\nfeeding only the measurements associated with the neighborhood of the pixel to\na simple MLP. While achieving comparable or better performance with successful\nCNNs like the U-Net on in-distribution test data, GLIMPSE significantly\noutperforms them on OOD samples while maintaining a memory footprint almost\nindependent of image resolution; 5GB memory suffices to train on 1024x1024\nimages. Further, we built GLIMPSE to be fully differentiable, which enables\nfeats such as recovery of accurate projection angles if they are out of\ncalibration.\n", "link": "http://arxiv.org/abs/2401.00816v2", "date": "2024-06-20", "relevancy": 2.8764, "topK": [{"title": "RTG-SLAM: Real-time 3D Reconstruction at Scale using Gaussian Splatting", "link": "http://arxiv.org/abs/2404.19706v1", "similarity": 0.5836}, {"title": "DDM-NET: End-to-end learning of keypoint feature Detection, Description and Matching for 3D localization", "link": "https://arxiv.org/abs/2212.04575", "similarity": 0.5742}, {"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.568}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20GLIMPSE%3A%20Generalized%20Local%20Imaging%20with%20MLPs&body=Title%3A%20GLIMPSE%3A%20Generalized%20Local%20Imaging%20with%20MLPs%0AAuthor%3A%20AmirEhsan%20Khorashadizadeh%20and%20Valentin%20Debarnot%20and%20Tianlin%20Liu%20and%20Ivan%20Dokmani%C4%87%0AAbstract%3A%20%20%20Deep%20learning%20is%20the%20current%20de%20facto%20state%20of%20the%20art%20in%20tomographic%0Aimaging.%20A%20common%20approach%20is%20to%20feed%20the%20result%20of%20a%20simple%20inversion%2C%20for%0Aexample%20the%20backprojection%2C%20to%20a%20convolutional%20neural%20network%20%28CNN%29%20which%20then%0Acomputes%20the%20reconstruction.%20Despite%20strong%20results%20on%20%27in-distribution%27%20test%0Adata%20similar%20to%20the%20training%20data%2C%20backprojection%20from%20sparse-view%20data%0Adelocalizes%20singularities%2C%20so%20these%20approaches%20require%20a%20large%20receptive%20field%0Ato%20perform%20well.%20As%20a%20consequence%2C%20they%20overfit%20to%20certain%20global%20structures%0Awhich%20leads%20to%20poor%20generalization%20on%20out-of-distribution%20%28OOD%29%20samples.%0AMoreover%2C%20their%20memory%20complexity%20and%20training%20time%20scale%20unfavorably%20with%0Aimage%20resolution%2C%20making%20them%20impractical%20for%20application%20at%20realistic%20clinical%0Aresolutions%2C%20especially%20in%203D%3A%20a%20standard%20U-Net%20requires%20a%20substantial%20140GB%20of%0Amemory%20and%202600%20seconds%20per%20epoch%20on%20a%20research-grade%20GPU%20when%20training%20on%0A1024x1024%20images.%20In%20this%20paper%2C%20we%20introduce%20GLIMPSE%2C%20a%20local%20processing%0Aneural%20network%20for%20computed%20tomography%20which%20reconstructs%20a%20pixel%20value%20by%0Afeeding%20only%20the%20measurements%20associated%20with%20the%20neighborhood%20of%20the%20pixel%20to%0Aa%20simple%20MLP.%20While%20achieving%20comparable%20or%20better%20performance%20with%20successful%0ACNNs%20like%20the%20U-Net%20on%20in-distribution%20test%20data%2C%20GLIMPSE%20significantly%0Aoutperforms%20them%20on%20OOD%20samples%20while%20maintaining%20a%20memory%20footprint%20almost%0Aindependent%20of%20image%20resolution%3B%205GB%20memory%20suffices%20to%20train%20on%201024x1024%0Aimages.%20Further%2C%20we%20built%20GLIMPSE%20to%20be%20fully%20differentiable%2C%20which%20enables%0Afeats%20such%20as%20recovery%20of%20accurate%20projection%20angles%20if%20they%20are%20out%20of%0Acalibration.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2401.00816v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DGLIMPSE%253A%2520Generalized%2520Local%2520Imaging%2520with%2520MLPs%26entry.906535625%3DAmirEhsan%2520Khorashadizadeh%2520and%2520Valentin%2520Debarnot%2520and%2520Tianlin%2520Liu%2520and%2520Ivan%2520Dokmani%25C4%2587%26entry.1292438233%3D%2520%2520Deep%2520learning%2520is%2520the%2520current%2520de%2520facto%2520state%2520of%2520the%2520art%2520in%2520tomographic%250Aimaging.%2520A%2520common%2520approach%2520is%2520to%2520feed%2520the%2520result%2520of%2520a%2520simple%2520inversion%252C%2520for%250Aexample%2520the%2520backprojection%252C%2520to%2520a%2520convolutional%2520neural%2520network%2520%2528CNN%2529%2520which%2520then%250Acomputes%2520the%2520reconstruction.%2520Despite%2520strong%2520results%2520on%2520%2527in-distribution%2527%2520test%250Adata%2520similar%2520to%2520the%2520training%2520data%252C%2520backprojection%2520from%2520sparse-view%2520data%250Adelocalizes%2520singularities%252C%2520so%2520these%2520approaches%2520require%2520a%2520large%2520receptive%2520field%250Ato%2520perform%2520well.%2520As%2520a%2520consequence%252C%2520they%2520overfit%2520to%2520certain%2520global%2520structures%250Awhich%2520leads%2520to%2520poor%2520generalization%2520on%2520out-of-distribution%2520%2528OOD%2529%2520samples.%250AMoreover%252C%2520their%2520memory%2520complexity%2520and%2520training%2520time%2520scale%2520unfavorably%2520with%250Aimage%2520resolution%252C%2520making%2520them%2520impractical%2520for%2520application%2520at%2520realistic%2520clinical%250Aresolutions%252C%2520especially%2520in%25203D%253A%2520a%2520standard%2520U-Net%2520requires%2520a%2520substantial%2520140GB%2520of%250Amemory%2520and%25202600%2520seconds%2520per%2520epoch%2520on%2520a%2520research-grade%2520GPU%2520when%2520training%2520on%250A1024x1024%2520images.%2520In%2520this%2520paper%252C%2520we%2520introduce%2520GLIMPSE%252C%2520a%2520local%2520processing%250Aneural%2520network%2520for%2520computed%2520tomography%2520which%2520reconstructs%2520a%2520pixel%2520value%2520by%250Afeeding%2520only%2520the%2520measurements%2520associated%2520with%2520the%2520neighborhood%2520of%2520the%2520pixel%2520to%250Aa%2520simple%2520MLP.%2520While%2520achieving%2520comparable%2520or%2520better%2520performance%2520with%2520successful%250ACNNs%2520like%2520the%2520U-Net%2520on%2520in-distribution%2520test%2520data%252C%2520GLIMPSE%2520significantly%250Aoutperforms%2520them%2520on%2520OOD%2520samples%2520while%2520maintaining%2520a%2520memory%2520footprint%2520almost%250Aindependent%2520of%2520image%2520resolution%253B%25205GB%2520memory%2520suffices%2520to%2520train%2520on%25201024x1024%250Aimages.%2520Further%252C%2520we%2520built%2520GLIMPSE%2520to%2520be%2520fully%2520differentiable%252C%2520which%2520enables%250Afeats%2520such%2520as%2520recovery%2520of%2520accurate%2520projection%2520angles%2520if%2520they%2520are%2520out%2520of%250Acalibration.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2401.00816v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=GLIMPSE%3A%20Generalized%20Local%20Imaging%20with%20MLPs&entry.906535625=AmirEhsan%20Khorashadizadeh%20and%20Valentin%20Debarnot%20and%20Tianlin%20Liu%20and%20Ivan%20Dokmani%C4%87&entry.1292438233=%20%20Deep%20learning%20is%20the%20current%20de%20facto%20state%20of%20the%20art%20in%20tomographic%0Aimaging.%20A%20common%20approach%20is%20to%20feed%20the%20result%20of%20a%20simple%20inversion%2C%20for%0Aexample%20the%20backprojection%2C%20to%20a%20convolutional%20neural%20network%20%28CNN%29%20which%20then%0Acomputes%20the%20reconstruction.%20Despite%20strong%20results%20on%20%27in-distribution%27%20test%0Adata%20similar%20to%20the%20training%20data%2C%20backprojection%20from%20sparse-view%20data%0Adelocalizes%20singularities%2C%20so%20these%20approaches%20require%20a%20large%20receptive%20field%0Ato%20perform%20well.%20As%20a%20consequence%2C%20they%20overfit%20to%20certain%20global%20structures%0Awhich%20leads%20to%20poor%20generalization%20on%20out-of-distribution%20%28OOD%29%20samples.%0AMoreover%2C%20their%20memory%20complexity%20and%20training%20time%20scale%20unfavorably%20with%0Aimage%20resolution%2C%20making%20them%20impractical%20for%20application%20at%20realistic%20clinical%0Aresolutions%2C%20especially%20in%203D%3A%20a%20standard%20U-Net%20requires%20a%20substantial%20140GB%20of%0Amemory%20and%202600%20seconds%20per%20epoch%20on%20a%20research-grade%20GPU%20when%20training%20on%0A1024x1024%20images.%20In%20this%20paper%2C%20we%20introduce%20GLIMPSE%2C%20a%20local%20processing%0Aneural%20network%20for%20computed%20tomography%20which%20reconstructs%20a%20pixel%20value%20by%0Afeeding%20only%20the%20measurements%20associated%20with%20the%20neighborhood%20of%20the%20pixel%20to%0Aa%20simple%20MLP.%20While%20achieving%20comparable%20or%20better%20performance%20with%20successful%0ACNNs%20like%20the%20U-Net%20on%20in-distribution%20test%20data%2C%20GLIMPSE%20significantly%0Aoutperforms%20them%20on%20OOD%20samples%20while%20maintaining%20a%20memory%20footprint%20almost%0Aindependent%20of%20image%20resolution%3B%205GB%20memory%20suffices%20to%20train%20on%201024x1024%0Aimages.%20Further%2C%20we%20built%20GLIMPSE%20to%20be%20fully%20differentiable%2C%20which%20enables%0Afeats%20such%20as%20recovery%20of%20accurate%20projection%20angles%20if%20they%20are%20out%20of%0Acalibration.%0A&entry.1838667208=http%3A//arxiv.org/abs/2401.00816v2&entry.124074799=Read"},
{"title": "Self-Supervised Pretext Tasks for Alzheimer's Disease Classification\n  using 3D Convolutional Neural Networks on Large-Scale Synthetic Neuroimaging\n  Dataset", "author": "Chen Zheng", "abstract": "  Structural magnetic resonance imaging (MRI) studies have shown that\nAlzheimer's Disease (AD) induces both localised and widespread neural\ndegenerative changes throughout the brain. However, the absence of segmentation\nthat highlights brain degenerative changes presents unique challenges for\ntraining CNN-based classifiers in a supervised fashion. In this work, we\nevaluated several unsupervised methods to train a feature extractor for\ndownstream AD vs. CN classification. Using the 3D T1-weighted MRI data of\ncognitive normal (CN) subjects from the synthetic neuroimaging LDM100K dataset,\nlightweight 3D CNN-based models are trained for brain age prediction, brain\nimage rotation classification, brain image reconstruction and a multi-head task\ncombining all three tasks into one. Feature extractors trained on the LDM100K\nsynthetic dataset achieved similar performance compared to the same model using\nreal-world data. This supports the feasibility of utilising large-scale\nsynthetic data for pretext task training. All the training and testing splits\nare performed on the subject-level to prevent data leakage issues. Alongside\nthe simple preprocessing steps, the random cropping data augmentation technique\nshows consistent improvement across all experiments.\n", "link": "http://arxiv.org/abs/2406.14210v1", "date": "2024-06-20", "relevancy": 2.7625, "topK": [{"title": "Weakly-guided self-supervised pretraining for temporal activity detection", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/25189", "similarity": 0.5615}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5538}, {"title": "DDM-NET: End-to-end learning of keypoint feature Detection, Description and Matching for 3D localization", "link": "https://arxiv.org/abs/2212.04575", "similarity": 0.5422}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Self-Supervised%20Pretext%20Tasks%20for%20Alzheimer%27s%20Disease%20Classification%0A%20%20using%203D%20Convolutional%20Neural%20Networks%20on%20Large-Scale%20Synthetic%20Neuroimaging%0A%20%20Dataset&body=Title%3A%20Self-Supervised%20Pretext%20Tasks%20for%20Alzheimer%27s%20Disease%20Classification%0A%20%20using%203D%20Convolutional%20Neural%20Networks%20on%20Large-Scale%20Synthetic%20Neuroimaging%0A%20%20Dataset%0AAuthor%3A%20Chen%20Zheng%0AAbstract%3A%20%20%20Structural%20magnetic%20resonance%20imaging%20%28MRI%29%20studies%20have%20shown%20that%0AAlzheimer%27s%20Disease%20%28AD%29%20induces%20both%20localised%20and%20widespread%20neural%0Adegenerative%20changes%20throughout%20the%20brain.%20However%2C%20the%20absence%20of%20segmentation%0Athat%20highlights%20brain%20degenerative%20changes%20presents%20unique%20challenges%20for%0Atraining%20CNN-based%20classifiers%20in%20a%20supervised%20fashion.%20In%20this%20work%2C%20we%0Aevaluated%20several%20unsupervised%20methods%20to%20train%20a%20feature%20extractor%20for%0Adownstream%20AD%20vs.%20CN%20classification.%20Using%20the%203D%20T1-weighted%20MRI%20data%20of%0Acognitive%20normal%20%28CN%29%20subjects%20from%20the%20synthetic%20neuroimaging%20LDM100K%20dataset%2C%0Alightweight%203D%20CNN-based%20models%20are%20trained%20for%20brain%20age%20prediction%2C%20brain%0Aimage%20rotation%20classification%2C%20brain%20image%20reconstruction%20and%20a%20multi-head%20task%0Acombining%20all%20three%20tasks%20into%20one.%20Feature%20extractors%20trained%20on%20the%20LDM100K%0Asynthetic%20dataset%20achieved%20similar%20performance%20compared%20to%20the%20same%20model%20using%0Areal-world%20data.%20This%20supports%20the%20feasibility%20of%20utilising%20large-scale%0Asynthetic%20data%20for%20pretext%20task%20training.%20All%20the%20training%20and%20testing%20splits%0Aare%20performed%20on%20the%20subject-level%20to%20prevent%20data%20leakage%20issues.%20Alongside%0Athe%20simple%20preprocessing%20steps%2C%20the%20random%20cropping%20data%20augmentation%20technique%0Ashows%20consistent%20improvement%20across%20all%20experiments.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2406.14210v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DSelf-Supervised%2520Pretext%2520Tasks%2520for%2520Alzheimer%2527s%2520Disease%2520Classification%250A%2520%2520using%25203D%2520Convolutional%2520Neural%2520Networks%2520on%2520Large-Scale%2520Synthetic%2520Neuroimaging%250A%2520%2520Dataset%26entry.906535625%3DChen%2520Zheng%26entry.1292438233%3D%2520%2520Structural%2520magnetic%2520resonance%2520imaging%2520%2528MRI%2529%2520studies%2520have%2520shown%2520that%250AAlzheimer%2527s%2520Disease%2520%2528AD%2529%2520induces%2520both%2520localised%2520and%2520widespread%2520neural%250Adegenerative%2520changes%2520throughout%2520the%2520brain.%2520However%252C%2520the%2520absence%2520of%2520segmentation%250Athat%2520highlights%2520brain%2520degenerative%2520changes%2520presents%2520unique%2520challenges%2520for%250Atraining%2520CNN-based%2520classifiers%2520in%2520a%2520supervised%2520fashion.%2520In%2520this%2520work%252C%2520we%250Aevaluated%2520several%2520unsupervised%2520methods%2520to%2520train%2520a%2520feature%2520extractor%2520for%250Adownstream%2520AD%2520vs.%2520CN%2520classification.%2520Using%2520the%25203D%2520T1-weighted%2520MRI%2520data%2520of%250Acognitive%2520normal%2520%2528CN%2529%2520subjects%2520from%2520the%2520synthetic%2520neuroimaging%2520LDM100K%2520dataset%252C%250Alightweight%25203D%2520CNN-based%2520models%2520are%2520trained%2520for%2520brain%2520age%2520prediction%252C%2520brain%250Aimage%2520rotation%2520classification%252C%2520brain%2520image%2520reconstruction%2520and%2520a%2520multi-head%2520task%250Acombining%2520all%2520three%2520tasks%2520into%2520one.%2520Feature%2520extractors%2520trained%2520on%2520the%2520LDM100K%250Asynthetic%2520dataset%2520achieved%2520similar%2520performance%2520compared%2520to%2520the%2520same%2520model%2520using%250Areal-world%2520data.%2520This%2520supports%2520the%2520feasibility%2520of%2520utilising%2520large-scale%250Asynthetic%2520data%2520for%2520pretext%2520task%2520training.%2520All%2520the%2520training%2520and%2520testing%2520splits%250Aare%2520performed%2520on%2520the%2520subject-level%2520to%2520prevent%2520data%2520leakage%2520issues.%2520Alongside%250Athe%2520simple%2520preprocessing%2520steps%252C%2520the%2520random%2520cropping%2520data%2520augmentation%2520technique%250Ashows%2520consistent%2520improvement%2520across%2520all%2520experiments.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2406.14210v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Self-Supervised%20Pretext%20Tasks%20for%20Alzheimer%27s%20Disease%20Classification%0A%20%20using%203D%20Convolutional%20Neural%20Networks%20on%20Large-Scale%20Synthetic%20Neuroimaging%0A%20%20Dataset&entry.906535625=Chen%20Zheng&entry.1292438233=%20%20Structural%20magnetic%20resonance%20imaging%20%28MRI%29%20studies%20have%20shown%20that%0AAlzheimer%27s%20Disease%20%28AD%29%20induces%20both%20localised%20and%20widespread%20neural%0Adegenerative%20changes%20throughout%20the%20brain.%20However%2C%20the%20absence%20of%20segmentation%0Athat%20highlights%20brain%20degenerative%20changes%20presents%20unique%20challenges%20for%0Atraining%20CNN-based%20classifiers%20in%20a%20supervised%20fashion.%20In%20this%20work%2C%20we%0Aevaluated%20several%20unsupervised%20methods%20to%20train%20a%20feature%20extractor%20for%0Adownstream%20AD%20vs.%20CN%20classification.%20Using%20the%203D%20T1-weighted%20MRI%20data%20of%0Acognitive%20normal%20%28CN%29%20subjects%20from%20the%20synthetic%20neuroimaging%20LDM100K%20dataset%2C%0Alightweight%203D%20CNN-based%20models%20are%20trained%20for%20brain%20age%20prediction%2C%20brain%0Aimage%20rotation%20classification%2C%20brain%20image%20reconstruction%20and%20a%20multi-head%20task%0Acombining%20all%20three%20tasks%20into%20one.%20Feature%20extractors%20trained%20on%20the%20LDM100K%0Asynthetic%20dataset%20achieved%20similar%20performance%20compared%20to%20the%20same%20model%20using%0Areal-world%20data.%20This%20supports%20the%20feasibility%20of%20utilising%20large-scale%0Asynthetic%20data%20for%20pretext%20task%20training.%20All%20the%20training%20and%20testing%20splits%0Aare%20performed%20on%20the%20subject-level%20to%20prevent%20data%20leakage%20issues.%20Alongside%0Athe%20simple%20preprocessing%20steps%2C%20the%20random%20cropping%20data%20augmentation%20technique%0Ashows%20consistent%20improvement%20across%20all%20experiments.%0A&entry.1838667208=http%3A//arxiv.org/abs/2406.14210v1&entry.124074799=Read"},
{"title": "Towards evolution of Deep Neural Networks through contrastive\n  Self-Supervised learning", "author": "Adriano Vinhas and Jo\u00e3o Correia and Penousal Machado", "abstract": "  Deep Neural Networks (DNNs) have been successfully applied to a wide range of\nproblems. However, two main limitations are commonly pointed out. The first one\nis that they require long time to design. The other is that they heavily rely\non labelled data, which can sometimes be costly and hard to obtain. In order to\naddress the first problem, neuroevolution has been proved to be a plausible\noption to automate the design of DNNs. As for the second problem,\nself-supervised learning has been used to leverage unlabelled data to learn\nrepresentations. Our goal is to study how neuroevolution can help\nself-supervised learning to bridge the gap to supervised learning in terms of\nperformance. In this work, we propose a framework that is able to evolve deep\nneural networks using self-supervised learning. Our results on the CIFAR-10\ndataset show that it is possible to evolve adequate neural networks while\nreducing the reliance on labelled data. Moreover, an analysis to the structure\nof the evolved networks suggests that the amount of labelled data fed to them\nhas less effect on the structure of networks that learned via self-supervised\nlearning, when compared to individuals that relied on supervised learning.\n", "link": "http://arxiv.org/abs/2406.14525v1", "date": "2024-06-20", "relevancy": 2.7321, "topK": [{"title": "Self-supervised Photographic Image Layout Representation Learning", "link": "http://arxiv.org/abs/2403.03740v1", "similarity": 0.5693}, {"title": "Boosted dynamic neural networks", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/26302", "similarity": 0.559}, {"title": "IAE: Implicit Autoencoder for Point Cloud Self-supervised Representation Learning", "link": "https://arxiv.org/abs/2201.00785", "similarity": 0.511}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Towards%20evolution%20of%20Deep%20Neural%20Networks%20through%20contrastive%0A%20%20Self-Supervised%20learning&body=Title%3A%20Towards%20evolution%20of%20Deep%20Neural%20Networks%20through%20contrastive%0A%20%20Self-Supervised%20learning%0AAuthor%3A%20Adriano%20Vinhas%20and%20Jo%C3%A3o%20Correia%20and%20Penousal%20Machado%0AAbstract%3A%20%20%20Deep%20Neural%20Networks%20%28DNNs%29%20have%20been%20successfully%20applied%20to%20a%20wide%20range%20of%0Aproblems.%20However%2C%20two%20main%20limitations%20are%20commonly%20pointed%20out.%20The%20first%20one%0Ais%20that%20they%20require%20long%20time%20to%20design.%20The%20other%20is%20that%20they%20heavily%20rely%0Aon%20labelled%20data%2C%20which%20can%20sometimes%20be%20costly%20and%20hard%20to%20obtain.%20In%20order%20to%0Aaddress%20the%20first%20problem%2C%20neuroevolution%20has%20been%20proved%20to%20be%20a%20plausible%0Aoption%20to%20automate%20the%20design%20of%20DNNs.%20As%20for%20the%20second%20problem%2C%0Aself-supervised%20learning%20has%20been%20used%20to%20leverage%20unlabelled%20data%20to%20learn%0Arepresentations.%20Our%20goal%20is%20to%20study%20how%20neuroevolution%20can%20help%0Aself-supervised%20learning%20to%20bridge%20the%20gap%20to%20supervised%20learning%20in%20terms%20of%0Aperformance.%20In%20this%20work%2C%20we%20propose%20a%20framework%20that%20is%20able%20to%20evolve%20deep%0Aneural%20networks%20using%20self-supervised%20learning.%20Our%20results%20on%20the%20CIFAR-10%0Adataset%20show%20that%20it%20is%20possible%20to%20evolve%20adequate%20neural%20networks%20while%0Areducing%20the%20reliance%20on%20labelled%20data.%20Moreover%2C%20an%20analysis%20to%20the%20structure%0Aof%20the%20evolved%20networks%20suggests%20that%20the%20amount%20of%20labelled%20data%20fed%20to%20them%0Ahas%20less%20effect%20on%20the%20structure%20of%20networks%20that%20learned%20via%20self-supervised%0Alearning%2C%20when%20compared%20to%20individuals%20that%20relied%20on%20supervised%20learning.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2406.14525v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DTowards%2520evolution%2520of%2520Deep%2520Neural%2520Networks%2520through%2520contrastive%250A%2520%2520Self-Supervised%2520learning%26entry.906535625%3DAdriano%2520Vinhas%2520and%2520Jo%25C3%25A3o%2520Correia%2520and%2520Penousal%2520Machado%26entry.1292438233%3D%2520%2520Deep%2520Neural%2520Networks%2520%2528DNNs%2529%2520have%2520been%2520successfully%2520applied%2520to%2520a%2520wide%2520range%2520of%250Aproblems.%2520However%252C%2520two%2520main%2520limitations%2520are%2520commonly%2520pointed%2520out.%2520The%2520first%2520one%250Ais%2520that%2520they%2520require%2520long%2520time%2520to%2520design.%2520The%2520other%2520is%2520that%2520they%2520heavily%2520rely%250Aon%2520labelled%2520data%252C%2520which%2520can%2520sometimes%2520be%2520costly%2520and%2520hard%2520to%2520obtain.%2520In%2520order%2520to%250Aaddress%2520the%2520first%2520problem%252C%2520neuroevolution%2520has%2520been%2520proved%2520to%2520be%2520a%2520plausible%250Aoption%2520to%2520automate%2520the%2520design%2520of%2520DNNs.%2520As%2520for%2520the%2520second%2520problem%252C%250Aself-supervised%2520learning%2520has%2520been%2520used%2520to%2520leverage%2520unlabelled%2520data%2520to%2520learn%250Arepresentations.%2520Our%2520goal%2520is%2520to%2520study%2520how%2520neuroevolution%2520can%2520help%250Aself-supervised%2520learning%2520to%2520bridge%2520the%2520gap%2520to%2520supervised%2520learning%2520in%2520terms%2520of%250Aperformance.%2520In%2520this%2520work%252C%2520we%2520propose%2520a%2520framework%2520that%2520is%2520able%2520to%2520evolve%2520deep%250Aneural%2520networks%2520using%2520self-supervised%2520learning.%2520Our%2520results%2520on%2520the%2520CIFAR-10%250Adataset%2520show%2520that%2520it%2520is%2520possible%2520to%2520evolve%2520adequate%2520neural%2520networks%2520while%250Areducing%2520the%2520reliance%2520on%2520labelled%2520data.%2520Moreover%252C%2520an%2520analysis%2520to%2520the%2520structure%250Aof%2520the%2520evolved%2520networks%2520suggests%2520that%2520the%2520amount%2520of%2520labelled%2520data%2520fed%2520to%2520them%250Ahas%2520less%2520effect%2520on%2520the%2520structure%2520of%2520networks%2520that%2520learned%2520via%2520self-supervised%250Alearning%252C%2520when%2520compared%2520to%2520individuals%2520that%2520relied%2520on%2520supervised%2520learning.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2406.14525v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Towards%20evolution%20of%20Deep%20Neural%20Networks%20through%20contrastive%0A%20%20Self-Supervised%20learning&entry.906535625=Adriano%20Vinhas%20and%20Jo%C3%A3o%20Correia%20and%20Penousal%20Machado&entry.1292438233=%20%20Deep%20Neural%20Networks%20%28DNNs%29%20have%20been%20successfully%20applied%20to%20a%20wide%20range%20of%0Aproblems.%20However%2C%20two%20main%20limitations%20are%20commonly%20pointed%20out.%20The%20first%20one%0Ais%20that%20they%20require%20long%20time%20to%20design.%20The%20other%20is%20that%20they%20heavily%20rely%0Aon%20labelled%20data%2C%20which%20can%20sometimes%20be%20costly%20and%20hard%20to%20obtain.%20In%20order%20to%0Aaddress%20the%20first%20problem%2C%20neuroevolution%20has%20been%20proved%20to%20be%20a%20plausible%0Aoption%20to%20automate%20the%20design%20of%20DNNs.%20As%20for%20the%20second%20problem%2C%0Aself-supervised%20learning%20has%20been%20used%20to%20leverage%20unlabelled%20data%20to%20learn%0Arepresentations.%20Our%20goal%20is%20to%20study%20how%20neuroevolution%20can%20help%0Aself-supervised%20learning%20to%20bridge%20the%20gap%20to%20supervised%20learning%20in%20terms%20of%0Aperformance.%20In%20this%20work%2C%20we%20propose%20a%20framework%20that%20is%20able%20to%20evolve%20deep%0Aneural%20networks%20using%20self-supervised%20learning.%20Our%20results%20on%20the%20CIFAR-10%0Adataset%20show%20that%20it%20is%20possible%20to%20evolve%20adequate%20neural%20networks%20while%0Areducing%20the%20reliance%20on%20labelled%20data.%20Moreover%2C%20an%20analysis%20to%20the%20structure%0Aof%20the%20evolved%20networks%20suggests%20that%20the%20amount%20of%20labelled%20data%20fed%20to%20them%0Ahas%20less%20effect%20on%20the%20structure%20of%20networks%20that%20learned%20via%20self-supervised%0Alearning%2C%20when%20compared%20to%20individuals%20that%20relied%20on%20supervised%20learning.%0A&entry.1838667208=http%3A//arxiv.org/abs/2406.14525v1&entry.124074799=Read"},
{"title": "CityNav: Language-Goal Aerial Navigation Dataset with Geographic\n  Information", "author": "Jungdae Lee and Taiki Miyanishi and Shuhei Kurita and Koya Sakamoto and Daichi Azuma and Yutaka Matsuo and Nakamasa Inoue", "abstract": "  Vision-and-language navigation (VLN) aims to guide autonomous agents through\nreal-world environments by integrating visual and linguistic cues. While\nsubstantial progress has been made in understanding these interactive\nmodalities in ground-level navigation, aerial navigation remains largely\nunderexplored. This is primarily due to the scarcity of resources suitable for\nreal-world, city-scale aerial navigation studies. To bridge this gap, we\nintroduce CityNav, a new dataset for language-goal aerial navigation using a 3D\npoint cloud representation from real-world cities. CityNav includes 32,637\nnatural language descriptions paired with human demonstration trajectories,\ncollected from participants via a new web-based 3D simulator developed for this\nresearch. Each description specifies a navigation goal, leveraging the names\nand locations of landmarks within real-world cities. We also provide baseline\nmodels of navigation agents that incorporate an internal 2D spatial map\nrepresenting landmarks referenced in the descriptions. We benchmark the latest\naerial navigation baselines and our proposed model on the CityNav dataset. The\nresults using this dataset reveal the following key findings: (i) Our aerial\nagent models trained on human demonstration trajectories outperform those\ntrained on shortest path trajectories, highlighting the importance of\nhuman-driven navigation strategies; (ii) The integration of a 2D spatial map\nsignificantly enhances navigation efficiency at city scale. Our dataset and\ncode are available at https://water-cookie.github.io/city-nav-proj/\n", "link": "http://arxiv.org/abs/2406.14240v1", "date": "2024-06-20", "relevancy": 2.7223, "topK": [{"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.5782}, {"title": "DDM-NET: End-to-end learning of keypoint feature Detection, Description and Matching for 3D localization", "link": "https://arxiv.org/abs/2212.04575", "similarity": 0.5278}, {"title": "Egocentric Computer Vision for Hands-Free Robotic Wheelchair Navigation", "link": "https://link.springer.com/article/10.1007/s10846-023-01807-4", "similarity": 0.5274}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20CityNav%3A%20Language-Goal%20Aerial%20Navigation%20Dataset%20with%20Geographic%0A%20%20Information&body=Title%3A%20CityNav%3A%20Language-Goal%20Aerial%20Navigation%20Dataset%20with%20Geographic%0A%20%20Information%0AAuthor%3A%20Jungdae%20Lee%20and%20Taiki%20Miyanishi%20and%20Shuhei%20Kurita%20and%20Koya%20Sakamoto%20and%20Daichi%20Azuma%20and%20Yutaka%20Matsuo%20and%20Nakamasa%20Inoue%0AAbstract%3A%20%20%20Vision-and-language%20navigation%20%28VLN%29%20aims%20to%20guide%20autonomous%20agents%20through%0Areal-world%20environments%20by%20integrating%20visual%20and%20linguistic%20cues.%20While%0Asubstantial%20progress%20has%20been%20made%20in%20understanding%20these%20interactive%0Amodalities%20in%20ground-level%20navigation%2C%20aerial%20navigation%20remains%20largely%0Aunderexplored.%20This%20is%20primarily%20due%20to%20the%20scarcity%20of%20resources%20suitable%20for%0Areal-world%2C%20city-scale%20aerial%20navigation%20studies.%20To%20bridge%20this%20gap%2C%20we%0Aintroduce%20CityNav%2C%20a%20new%20dataset%20for%20language-goal%20aerial%20navigation%20using%20a%203D%0Apoint%20cloud%20representation%20from%20real-world%20cities.%20CityNav%20includes%2032%2C637%0Anatural%20language%20descriptions%20paired%20with%20human%20demonstration%20trajectories%2C%0Acollected%20from%20participants%20via%20a%20new%20web-based%203D%20simulator%20developed%20for%20this%0Aresearch.%20Each%20description%20specifies%20a%20navigation%20goal%2C%20leveraging%20the%20names%0Aand%20locations%20of%20landmarks%20within%20real-world%20cities.%20We%20also%20provide%20baseline%0Amodels%20of%20navigation%20agents%20that%20incorporate%20an%20internal%202D%20spatial%20map%0Arepresenting%20landmarks%20referenced%20in%20the%20descriptions.%20We%20benchmark%20the%20latest%0Aaerial%20navigation%20baselines%20and%20our%20proposed%20model%20on%20the%20CityNav%20dataset.%20The%0Aresults%20using%20this%20dataset%20reveal%20the%20following%20key%20findings%3A%20%28i%29%20Our%20aerial%0Aagent%20models%20trained%20on%20human%20demonstration%20trajectories%20outperform%20those%0Atrained%20on%20shortest%20path%20trajectories%2C%20highlighting%20the%20importance%20of%0Ahuman-driven%20navigation%20strategies%3B%20%28ii%29%20The%20integration%20of%20a%202D%20spatial%20map%0Asignificantly%20enhances%20navigation%20efficiency%20at%20city%20scale.%20Our%20dataset%20and%0Acode%20are%20available%20at%20https%3A//water-cookie.github.io/city-nav-proj/%0A%0ALink%3A%20http%3A//arxiv.org/abs/2406.14240v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DCityNav%253A%2520Language-Goal%2520Aerial%2520Navigation%2520Dataset%2520with%2520Geographic%250A%2520%2520Information%26entry.906535625%3DJungdae%2520Lee%2520and%2520Taiki%2520Miyanishi%2520and%2520Shuhei%2520Kurita%2520and%2520Koya%2520Sakamoto%2520and%2520Daichi%2520Azuma%2520and%2520Yutaka%2520Matsuo%2520and%2520Nakamasa%2520Inoue%26entry.1292438233%3D%2520%2520Vision-and-language%2520navigation%2520%2528VLN%2529%2520aims%2520to%2520guide%2520autonomous%2520agents%2520through%250Areal-world%2520environments%2520by%2520integrating%2520visual%2520and%2520linguistic%2520cues.%2520While%250Asubstantial%2520progress%2520has%2520been%2520made%2520in%2520understanding%2520these%2520interactive%250Amodalities%2520in%2520ground-level%2520navigation%252C%2520aerial%2520navigation%2520remains%2520largely%250Aunderexplored.%2520This%2520is%2520primarily%2520due%2520to%2520the%2520scarcity%2520of%2520resources%2520suitable%2520for%250Areal-world%252C%2520city-scale%2520aerial%2520navigation%2520studies.%2520To%2520bridge%2520this%2520gap%252C%2520we%250Aintroduce%2520CityNav%252C%2520a%2520new%2520dataset%2520for%2520language-goal%2520aerial%2520navigation%2520using%2520a%25203D%250Apoint%2520cloud%2520representation%2520from%2520real-world%2520cities.%2520CityNav%2520includes%252032%252C637%250Anatural%2520language%2520descriptions%2520paired%2520with%2520human%2520demonstration%2520trajectories%252C%250Acollected%2520from%2520participants%2520via%2520a%2520new%2520web-based%25203D%2520simulator%2520developed%2520for%2520this%250Aresearch.%2520Each%2520description%2520specifies%2520a%2520navigation%2520goal%252C%2520leveraging%2520the%2520names%250Aand%2520locations%2520of%2520landmarks%2520within%2520real-world%2520cities.%2520We%2520also%2520provide%2520baseline%250Amodels%2520of%2520navigation%2520agents%2520that%2520incorporate%2520an%2520internal%25202D%2520spatial%2520map%250Arepresenting%2520landmarks%2520referenced%2520in%2520the%2520descriptions.%2520We%2520benchmark%2520the%2520latest%250Aaerial%2520navigation%2520baselines%2520and%2520our%2520proposed%2520model%2520on%2520the%2520CityNav%2520dataset.%2520The%250Aresults%2520using%2520this%2520dataset%2520reveal%2520the%2520following%2520key%2520findings%253A%2520%2528i%2529%2520Our%2520aerial%250Aagent%2520models%2520trained%2520on%2520human%2520demonstration%2520trajectories%2520outperform%2520those%250Atrained%2520on%2520shortest%2520path%2520trajectories%252C%2520highlighting%2520the%2520importance%2520of%250Ahuman-driven%2520navigation%2520strategies%253B%2520%2528ii%2529%2520The%2520integration%2520of%2520a%25202D%2520spatial%2520map%250Asignificantly%2520enhances%2520navigation%2520efficiency%2520at%2520city%2520scale.%2520Our%2520dataset%2520and%250Acode%2520are%2520available%2520at%2520https%253A//water-cookie.github.io/city-nav-proj/%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2406.14240v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=CityNav%3A%20Language-Goal%20Aerial%20Navigation%20Dataset%20with%20Geographic%0A%20%20Information&entry.906535625=Jungdae%20Lee%20and%20Taiki%20Miyanishi%20and%20Shuhei%20Kurita%20and%20Koya%20Sakamoto%20and%20Daichi%20Azuma%20and%20Yutaka%20Matsuo%20and%20Nakamasa%20Inoue&entry.1292438233=%20%20Vision-and-language%20navigation%20%28VLN%29%20aims%20to%20guide%20autonomous%20agents%20through%0Areal-world%20environments%20by%20integrating%20visual%20and%20linguistic%20cues.%20While%0Asubstantial%20progress%20has%20been%20made%20in%20understanding%20these%20interactive%0Amodalities%20in%20ground-level%20navigation%2C%20aerial%20navigation%20remains%20largely%0Aunderexplored.%20This%20is%20primarily%20due%20to%20the%20scarcity%20of%20resources%20suitable%20for%0Areal-world%2C%20city-scale%20aerial%20navigation%20studies.%20To%20bridge%20this%20gap%2C%20we%0Aintroduce%20CityNav%2C%20a%20new%20dataset%20for%20language-goal%20aerial%20navigation%20using%20a%203D%0Apoint%20cloud%20representation%20from%20real-world%20cities.%20CityNav%20includes%2032%2C637%0Anatural%20language%20descriptions%20paired%20with%20human%20demonstration%20trajectories%2C%0Acollected%20from%20participants%20via%20a%20new%20web-based%203D%20simulator%20developed%20for%20this%0Aresearch.%20Each%20description%20specifies%20a%20navigation%20goal%2C%20leveraging%20the%20names%0Aand%20locations%20of%20landmarks%20within%20real-world%20cities.%20We%20also%20provide%20baseline%0Amodels%20of%20navigation%20agents%20that%20incorporate%20an%20internal%202D%20spatial%20map%0Arepresenting%20landmarks%20referenced%20in%20the%20descriptions.%20We%20benchmark%20the%20latest%0Aaerial%20navigation%20baselines%20and%20our%20proposed%20model%20on%20the%20CityNav%20dataset.%20The%0Aresults%20using%20this%20dataset%20reveal%20the%20following%20key%20findings%3A%20%28i%29%20Our%20aerial%0Aagent%20models%20trained%20on%20human%20demonstration%20trajectories%20outperform%20those%0Atrained%20on%20shortest%20path%20trajectories%2C%20highlighting%20the%20importance%20of%0Ahuman-driven%20navigation%20strategies%3B%20%28ii%29%20The%20integration%20of%20a%202D%20spatial%20map%0Asignificantly%20enhances%20navigation%20efficiency%20at%20city%20scale.%20Our%20dataset%20and%0Acode%20are%20available%20at%20https%3A//water-cookie.github.io/city-nav-proj/%0A&entry.1838667208=http%3A//arxiv.org/abs/2406.14240v1&entry.124074799=Read"},
{"title": "DuMapNet: An End-to-End Vectorization System for City-Scale Lane-Level\n  Map Generation", "author": "Deguo Xia and Weiming Zhang and Xiyan Liu and Wei Zhang and Chenting Gong and Jizhou Huang and Mengmeng Yang and Diange Yang", "abstract": "  Generating city-scale lane-level maps faces significant challenges due to the\nintricate urban environments, such as blurred or absent lane markings.\nAdditionally, a standard lane-level map requires a comprehensive organization\nof lane groupings, encompassing lane direction, style, boundary, and topology,\nyet has not been thoroughly examined in prior research. These obstacles result\nin labor-intensive human annotation and high maintenance costs. This paper\novercomes these limitations and presents an industrial-grade solution named\nDuMapNet that outputs standardized, vectorized map elements and their topology\nin an end-to-end paradigm. To this end, we propose a group-wise lane prediction\n(GLP) system that outputs vectorized results of lane groups by meticulously\ntailoring a transformer-based network. Meanwhile, to enhance generalization in\nchallenging scenarios, such as road wear and occlusions, as well as to improve\nglobal consistency, a contextual prompts encoder (CPE) module is proposed,\nwhich leverages the predicted results of spatial neighborhoods as contextual\ninformation. Extensive experiments conducted on large-scale real-world datasets\ndemonstrate the superiority and effectiveness of DuMapNet. Additionally,\nDuMap-Net has already been deployed in production at Baidu Maps since June\n2023, supporting lane-level map generation tasks for over 360 cities while\nbringing a 95% reduction in costs. This demonstrates that DuMapNet serves as a\npractical and cost-effective industrial solution for city-scale lane-level map\ngeneration.\n", "link": "http://arxiv.org/abs/2406.14255v1", "date": "2024-06-20", "relevancy": 2.716, "topK": [{"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.5717}, {"title": "DDM-NET: End-to-end learning of keypoint feature Detection, Description and Matching for 3D localization", "link": "https://arxiv.org/abs/2212.04575", "similarity": 0.5524}, {"title": "Self-supervised Photographic Image Layout Representation Learning", "link": "http://arxiv.org/abs/2403.03740v1", "similarity": 0.5055}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20DuMapNet%3A%20An%20End-to-End%20Vectorization%20System%20for%20City-Scale%20Lane-Level%0A%20%20Map%20Generation&body=Title%3A%20DuMapNet%3A%20An%20End-to-End%20Vectorization%20System%20for%20City-Scale%20Lane-Level%0A%20%20Map%20Generation%0AAuthor%3A%20Deguo%20Xia%20and%20Weiming%20Zhang%20and%20Xiyan%20Liu%20and%20Wei%20Zhang%20and%20Chenting%20Gong%20and%20Jizhou%20Huang%20and%20Mengmeng%20Yang%20and%20Diange%20Yang%0AAbstract%3A%20%20%20Generating%20city-scale%20lane-level%20maps%20faces%20significant%20challenges%20due%20to%20the%0Aintricate%20urban%20environments%2C%20such%20as%20blurred%20or%20absent%20lane%20markings.%0AAdditionally%2C%20a%20standard%20lane-level%20map%20requires%20a%20comprehensive%20organization%0Aof%20lane%20groupings%2C%20encompassing%20lane%20direction%2C%20style%2C%20boundary%2C%20and%20topology%2C%0Ayet%20has%20not%20been%20thoroughly%20examined%20in%20prior%20research.%20These%20obstacles%20result%0Ain%20labor-intensive%20human%20annotation%20and%20high%20maintenance%20costs.%20This%20paper%0Aovercomes%20these%20limitations%20and%20presents%20an%20industrial-grade%20solution%20named%0ADuMapNet%20that%20outputs%20standardized%2C%20vectorized%20map%20elements%20and%20their%20topology%0Ain%20an%20end-to-end%20paradigm.%20To%20this%20end%2C%20we%20propose%20a%20group-wise%20lane%20prediction%0A%28GLP%29%20system%20that%20outputs%20vectorized%20results%20of%20lane%20groups%20by%20meticulously%0Atailoring%20a%20transformer-based%20network.%20Meanwhile%2C%20to%20enhance%20generalization%20in%0Achallenging%20scenarios%2C%20such%20as%20road%20wear%20and%20occlusions%2C%20as%20well%20as%20to%20improve%0Aglobal%20consistency%2C%20a%20contextual%20prompts%20encoder%20%28CPE%29%20module%20is%20proposed%2C%0Awhich%20leverages%20the%20predicted%20results%20of%20spatial%20neighborhoods%20as%20contextual%0Ainformation.%20Extensive%20experiments%20conducted%20on%20large-scale%20real-world%20datasets%0Ademonstrate%20the%20superiority%20and%20effectiveness%20of%20DuMapNet.%20Additionally%2C%0ADuMap-Net%20has%20already%20been%20deployed%20in%20production%20at%20Baidu%20Maps%20since%20June%0A2023%2C%20supporting%20lane-level%20map%20generation%20tasks%20for%20over%20360%20cities%20while%0Abringing%20a%2095%25%20reduction%20in%20costs.%20This%20demonstrates%20that%20DuMapNet%20serves%20as%20a%0Apractical%20and%20cost-effective%20industrial%20solution%20for%20city-scale%20lane-level%20map%0Ageneration.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2406.14255v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DDuMapNet%253A%2520An%2520End-to-End%2520Vectorization%2520System%2520for%2520City-Scale%2520Lane-Level%250A%2520%2520Map%2520Generation%26entry.906535625%3DDeguo%2520Xia%2520and%2520Weiming%2520Zhang%2520and%2520Xiyan%2520Liu%2520and%2520Wei%2520Zhang%2520and%2520Chenting%2520Gong%2520and%2520Jizhou%2520Huang%2520and%2520Mengmeng%2520Yang%2520and%2520Diange%2520Yang%26entry.1292438233%3D%2520%2520Generating%2520city-scale%2520lane-level%2520maps%2520faces%2520significant%2520challenges%2520due%2520to%2520the%250Aintricate%2520urban%2520environments%252C%2520such%2520as%2520blurred%2520or%2520absent%2520lane%2520markings.%250AAdditionally%252C%2520a%2520standard%2520lane-level%2520map%2520requires%2520a%2520comprehensive%2520organization%250Aof%2520lane%2520groupings%252C%2520encompassing%2520lane%2520direction%252C%2520style%252C%2520boundary%252C%2520and%2520topology%252C%250Ayet%2520has%2520not%2520been%2520thoroughly%2520examined%2520in%2520prior%2520research.%2520These%2520obstacles%2520result%250Ain%2520labor-intensive%2520human%2520annotation%2520and%2520high%2520maintenance%2520costs.%2520This%2520paper%250Aovercomes%2520these%2520limitations%2520and%2520presents%2520an%2520industrial-grade%2520solution%2520named%250ADuMapNet%2520that%2520outputs%2520standardized%252C%2520vectorized%2520map%2520elements%2520and%2520their%2520topology%250Ain%2520an%2520end-to-end%2520paradigm.%2520To%2520this%2520end%252C%2520we%2520propose%2520a%2520group-wise%2520lane%2520prediction%250A%2528GLP%2529%2520system%2520that%2520outputs%2520vectorized%2520results%2520of%2520lane%2520groups%2520by%2520meticulously%250Atailoring%2520a%2520transformer-based%2520network.%2520Meanwhile%252C%2520to%2520enhance%2520generalization%2520in%250Achallenging%2520scenarios%252C%2520such%2520as%2520road%2520wear%2520and%2520occlusions%252C%2520as%2520well%2520as%2520to%2520improve%250Aglobal%2520consistency%252C%2520a%2520contextual%2520prompts%2520encoder%2520%2528CPE%2529%2520module%2520is%2520proposed%252C%250Awhich%2520leverages%2520the%2520predicted%2520results%2520of%2520spatial%2520neighborhoods%2520as%2520contextual%250Ainformation.%2520Extensive%2520experiments%2520conducted%2520on%2520large-scale%2520real-world%2520datasets%250Ademonstrate%2520the%2520superiority%2520and%2520effectiveness%2520of%2520DuMapNet.%2520Additionally%252C%250ADuMap-Net%2520has%2520already%2520been%2520deployed%2520in%2520production%2520at%2520Baidu%2520Maps%2520since%2520June%250A2023%252C%2520supporting%2520lane-level%2520map%2520generation%2520tasks%2520for%2520over%2520360%2520cities%2520while%250Abringing%2520a%252095%2525%2520reduction%2520in%2520costs.%2520This%2520demonstrates%2520that%2520DuMapNet%2520serves%2520as%2520a%250Apractical%2520and%2520cost-effective%2520industrial%2520solution%2520for%2520city-scale%2520lane-level%2520map%250Ageneration.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2406.14255v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=DuMapNet%3A%20An%20End-to-End%20Vectorization%20System%20for%20City-Scale%20Lane-Level%0A%20%20Map%20Generation&entry.906535625=Deguo%20Xia%20and%20Weiming%20Zhang%20and%20Xiyan%20Liu%20and%20Wei%20Zhang%20and%20Chenting%20Gong%20and%20Jizhou%20Huang%20and%20Mengmeng%20Yang%20and%20Diange%20Yang&entry.1292438233=%20%20Generating%20city-scale%20lane-level%20maps%20faces%20significant%20challenges%20due%20to%20the%0Aintricate%20urban%20environments%2C%20such%20as%20blurred%20or%20absent%20lane%20markings.%0AAdditionally%2C%20a%20standard%20lane-level%20map%20requires%20a%20comprehensive%20organization%0Aof%20lane%20groupings%2C%20encompassing%20lane%20direction%2C%20style%2C%20boundary%2C%20and%20topology%2C%0Ayet%20has%20not%20been%20thoroughly%20examined%20in%20prior%20research.%20These%20obstacles%20result%0Ain%20labor-intensive%20human%20annotation%20and%20high%20maintenance%20costs.%20This%20paper%0Aovercomes%20these%20limitations%20and%20presents%20an%20industrial-grade%20solution%20named%0ADuMapNet%20that%20outputs%20standardized%2C%20vectorized%20map%20elements%20and%20their%20topology%0Ain%20an%20end-to-end%20paradigm.%20To%20this%20end%2C%20we%20propose%20a%20group-wise%20lane%20prediction%0A%28GLP%29%20system%20that%20outputs%20vectorized%20results%20of%20lane%20groups%20by%20meticulously%0Atailoring%20a%20transformer-based%20network.%20Meanwhile%2C%20to%20enhance%20generalization%20in%0Achallenging%20scenarios%2C%20such%20as%20road%20wear%20and%20occlusions%2C%20as%20well%20as%20to%20improve%0Aglobal%20consistency%2C%20a%20contextual%20prompts%20encoder%20%28CPE%29%20module%20is%20proposed%2C%0Awhich%20leverages%20the%20predicted%20results%20of%20spatial%20neighborhoods%20as%20contextual%0Ainformation.%20Extensive%20experiments%20conducted%20on%20large-scale%20real-world%20datasets%0Ademonstrate%20the%20superiority%20and%20effectiveness%20of%20DuMapNet.%20Additionally%2C%0ADuMap-Net%20has%20already%20been%20deployed%20in%20production%20at%20Baidu%20Maps%20since%20June%0A2023%2C%20supporting%20lane-level%20map%20generation%20tasks%20for%20over%20360%20cities%20while%0Abringing%20a%2095%25%20reduction%20in%20costs.%20This%20demonstrates%20that%20DuMapNet%20serves%20as%20a%0Apractical%20and%20cost-effective%20industrial%20solution%20for%20city-scale%20lane-level%20map%0Ageneration.%0A&entry.1838667208=http%3A//arxiv.org/abs/2406.14255v1&entry.124074799=Read"},
{"title": "Advancing Fine-Grained Classification by Structure and Subject\n  Preserving Augmentation", "author": "Eyal Michaeli and Ohad Fried", "abstract": "  Fine-grained visual classification (FGVC) involves classifying closely\nrelated sub-classes. This task is difficult due to the subtle differences\nbetween classes and the high intra-class variance. Moreover, FGVC datasets are\ntypically small and challenging to gather, thus highlighting a significant need\nfor effective data augmentation. Recent advancements in text-to-image diffusion\nmodels offer new possibilities for augmenting classification datasets. While\nthese models have been used to generate training data for classification tasks,\ntheir effectiveness in full-dataset training of FGVC models remains\nunder-explored. Recent techniques that rely on Text2Image generation or Img2Img\nmethods, often struggle to generate images that accurately represent the class\nwhile modifying them to a degree that significantly increases the dataset's\ndiversity. To address these challenges, we present SaSPA: Structure and Subject\nPreserving Augmentation. Contrary to recent methods, our method does not use\nreal images as guidance, thereby increasing generation flexibility and\npromoting greater diversity. To ensure accurate class representation, we employ\nconditioning mechanisms, specifically by conditioning on image edges and\nsubject representation. We conduct extensive experiments and benchmark SaSPA\nagainst both traditional and recent generative data augmentation methods. SaSPA\nconsistently outperforms all established baselines across multiple settings,\nincluding full dataset training, contextual bias, and few-shot classification.\nAdditionally, our results reveal interesting patterns in using synthetic data\nfor FGVC models; for instance, we find a relationship between the amount of\nreal data used and the optimal proportion of synthetic data. Code is available\nat https://github.com/EyalMichaeli/SaSPA-Aug.\n", "link": "http://arxiv.org/abs/2406.14551v1", "date": "2024-06-20", "relevancy": 2.7026, "topK": [{"title": "Magic-Me: Identity-Specific Video Customized Diffusion", "link": "http://arxiv.org/abs/2402.09368v1", "similarity": 0.5539}, {"title": "UGG: Unified Generative Grasping", "link": "https://arxiv.org/abs/2311.16917", "similarity": 0.5343}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5334}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Advancing%20Fine-Grained%20Classification%20by%20Structure%20and%20Subject%0A%20%20Preserving%20Augmentation&body=Title%3A%20Advancing%20Fine-Grained%20Classification%20by%20Structure%20and%20Subject%0A%20%20Preserving%20Augmentation%0AAuthor%3A%20Eyal%20Michaeli%20and%20Ohad%20Fried%0AAbstract%3A%20%20%20Fine-grained%20visual%20classification%20%28FGVC%29%20involves%20classifying%20closely%0Arelated%20sub-classes.%20This%20task%20is%20difficult%20due%20to%20the%20subtle%20differences%0Abetween%20classes%20and%20the%20high%20intra-class%20variance.%20Moreover%2C%20FGVC%20datasets%20are%0Atypically%20small%20and%20challenging%20to%20gather%2C%20thus%20highlighting%20a%20significant%20need%0Afor%20effective%20data%20augmentation.%20Recent%20advancements%20in%20text-to-image%20diffusion%0Amodels%20offer%20new%20possibilities%20for%20augmenting%20classification%20datasets.%20While%0Athese%20models%20have%20been%20used%20to%20generate%20training%20data%20for%20classification%20tasks%2C%0Atheir%20effectiveness%20in%20full-dataset%20training%20of%20FGVC%20models%20remains%0Aunder-explored.%20Recent%20techniques%20that%20rely%20on%20Text2Image%20generation%20or%20Img2Img%0Amethods%2C%20often%20struggle%20to%20generate%20images%20that%20accurately%20represent%20the%20class%0Awhile%20modifying%20them%20to%20a%20degree%20that%20significantly%20increases%20the%20dataset%27s%0Adiversity.%20To%20address%20these%20challenges%2C%20we%20present%20SaSPA%3A%20Structure%20and%20Subject%0APreserving%20Augmentation.%20Contrary%20to%20recent%20methods%2C%20our%20method%20does%20not%20use%0Areal%20images%20as%20guidance%2C%20thereby%20increasing%20generation%20flexibility%20and%0Apromoting%20greater%20diversity.%20To%20ensure%20accurate%20class%20representation%2C%20we%20employ%0Aconditioning%20mechanisms%2C%20specifically%20by%20conditioning%20on%20image%20edges%20and%0Asubject%20representation.%20We%20conduct%20extensive%20experiments%20and%20benchmark%20SaSPA%0Aagainst%20both%20traditional%20and%20recent%20generative%20data%20augmentation%20methods.%20SaSPA%0Aconsistently%20outperforms%20all%20established%20baselines%20across%20multiple%20settings%2C%0Aincluding%20full%20dataset%20training%2C%20contextual%20bias%2C%20and%20few-shot%20classification.%0AAdditionally%2C%20our%20results%20reveal%20interesting%20patterns%20in%20using%20synthetic%20data%0Afor%20FGVC%20models%3B%20for%20instance%2C%20we%20find%20a%20relationship%20between%20the%20amount%20of%0Areal%20data%20used%20and%20the%20optimal%20proportion%20of%20synthetic%20data.%20Code%20is%20available%0Aat%20https%3A//github.com/EyalMichaeli/SaSPA-Aug.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2406.14551v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DAdvancing%2520Fine-Grained%2520Classification%2520by%2520Structure%2520and%2520Subject%250A%2520%2520Preserving%2520Augmentation%26entry.906535625%3DEyal%2520Michaeli%2520and%2520Ohad%2520Fried%26entry.1292438233%3D%2520%2520Fine-grained%2520visual%2520classification%2520%2528FGVC%2529%2520involves%2520classifying%2520closely%250Arelated%2520sub-classes.%2520This%2520task%2520is%2520difficult%2520due%2520to%2520the%2520subtle%2520differences%250Abetween%2520classes%2520and%2520the%2520high%2520intra-class%2520variance.%2520Moreover%252C%2520FGVC%2520datasets%2520are%250Atypically%2520small%2520and%2520challenging%2520to%2520gather%252C%2520thus%2520highlighting%2520a%2520significant%2520need%250Afor%2520effective%2520data%2520augmentation.%2520Recent%2520advancements%2520in%2520text-to-image%2520diffusion%250Amodels%2520offer%2520new%2520possibilities%2520for%2520augmenting%2520classification%2520datasets.%2520While%250Athese%2520models%2520have%2520been%2520used%2520to%2520generate%2520training%2520data%2520for%2520classification%2520tasks%252C%250Atheir%2520effectiveness%2520in%2520full-dataset%2520training%2520of%2520FGVC%2520models%2520remains%250Aunder-explored.%2520Recent%2520techniques%2520that%2520rely%2520on%2520Text2Image%2520generation%2520or%2520Img2Img%250Amethods%252C%2520often%2520struggle%2520to%2520generate%2520images%2520that%2520accurately%2520represent%2520the%2520class%250Awhile%2520modifying%2520them%2520to%2520a%2520degree%2520that%2520significantly%2520increases%2520the%2520dataset%2527s%250Adiversity.%2520To%2520address%2520these%2520challenges%252C%2520we%2520present%2520SaSPA%253A%2520Structure%2520and%2520Subject%250APreserving%2520Augmentation.%2520Contrary%2520to%2520recent%2520methods%252C%2520our%2520method%2520does%2520not%2520use%250Areal%2520images%2520as%2520guidance%252C%2520thereby%2520increasing%2520generation%2520flexibility%2520and%250Apromoting%2520greater%2520diversity.%2520To%2520ensure%2520accurate%2520class%2520representation%252C%2520we%2520employ%250Aconditioning%2520mechanisms%252C%2520specifically%2520by%2520conditioning%2520on%2520image%2520edges%2520and%250Asubject%2520representation.%2520We%2520conduct%2520extensive%2520experiments%2520and%2520benchmark%2520SaSPA%250Aagainst%2520both%2520traditional%2520and%2520recent%2520generative%2520data%2520augmentation%2520methods.%2520SaSPA%250Aconsistently%2520outperforms%2520all%2520established%2520baselines%2520across%2520multiple%2520settings%252C%250Aincluding%2520full%2520dataset%2520training%252C%2520contextual%2520bias%252C%2520and%2520few-shot%2520classification.%250AAdditionally%252C%2520our%2520results%2520reveal%2520interesting%2520patterns%2520in%2520using%2520synthetic%2520data%250Afor%2520FGVC%2520models%253B%2520for%2520instance%252C%2520we%2520find%2520a%2520relationship%2520between%2520the%2520amount%2520of%250Areal%2520data%2520used%2520and%2520the%2520optimal%2520proportion%2520of%2520synthetic%2520data.%2520Code%2520is%2520available%250Aat%2520https%253A//github.com/EyalMichaeli/SaSPA-Aug.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2406.14551v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Advancing%20Fine-Grained%20Classification%20by%20Structure%20and%20Subject%0A%20%20Preserving%20Augmentation&entry.906535625=Eyal%20Michaeli%20and%20Ohad%20Fried&entry.1292438233=%20%20Fine-grained%20visual%20classification%20%28FGVC%29%20involves%20classifying%20closely%0Arelated%20sub-classes.%20This%20task%20is%20difficult%20due%20to%20the%20subtle%20differences%0Abetween%20classes%20and%20the%20high%20intra-class%20variance.%20Moreover%2C%20FGVC%20datasets%20are%0Atypically%20small%20and%20challenging%20to%20gather%2C%20thus%20highlighting%20a%20significant%20need%0Afor%20effective%20data%20augmentation.%20Recent%20advancements%20in%20text-to-image%20diffusion%0Amodels%20offer%20new%20possibilities%20for%20augmenting%20classification%20datasets.%20While%0Athese%20models%20have%20been%20used%20to%20generate%20training%20data%20for%20classification%20tasks%2C%0Atheir%20effectiveness%20in%20full-dataset%20training%20of%20FGVC%20models%20remains%0Aunder-explored.%20Recent%20techniques%20that%20rely%20on%20Text2Image%20generation%20or%20Img2Img%0Amethods%2C%20often%20struggle%20to%20generate%20images%20that%20accurately%20represent%20the%20class%0Awhile%20modifying%20them%20to%20a%20degree%20that%20significantly%20increases%20the%20dataset%27s%0Adiversity.%20To%20address%20these%20challenges%2C%20we%20present%20SaSPA%3A%20Structure%20and%20Subject%0APreserving%20Augmentation.%20Contrary%20to%20recent%20methods%2C%20our%20method%20does%20not%20use%0Areal%20images%20as%20guidance%2C%20thereby%20increasing%20generation%20flexibility%20and%0Apromoting%20greater%20diversity.%20To%20ensure%20accurate%20class%20representation%2C%20we%20employ%0Aconditioning%20mechanisms%2C%20specifically%20by%20conditioning%20on%20image%20edges%20and%0Asubject%20representation.%20We%20conduct%20extensive%20experiments%20and%20benchmark%20SaSPA%0Aagainst%20both%20traditional%20and%20recent%20generative%20data%20augmentation%20methods.%20SaSPA%0Aconsistently%20outperforms%20all%20established%20baselines%20across%20multiple%20settings%2C%0Aincluding%20full%20dataset%20training%2C%20contextual%20bias%2C%20and%20few-shot%20classification.%0AAdditionally%2C%20our%20results%20reveal%20interesting%20patterns%20in%20using%20synthetic%20data%0Afor%20FGVC%20models%3B%20for%20instance%2C%20we%20find%20a%20relationship%20between%20the%20amount%20of%0Areal%20data%20used%20and%20the%20optimal%20proportion%20of%20synthetic%20data.%20Code%20is%20available%0Aat%20https%3A//github.com/EyalMichaeli/SaSPA-Aug.%0A&entry.1838667208=http%3A//arxiv.org/abs/2406.14551v1&entry.124074799=Read"},
{"title": "African or European Swallow? Benchmarking Large Vision-Language Models\n  for Fine-Grained Object Classification", "author": "Gregor Geigle and Radu Timofte and Goran Glava\u0161", "abstract": "  Recent Large Vision-Language Models (LVLMs) demonstrate impressive abilities\non numerous image understanding and reasoning tasks. The task of fine-grained\nobject classification (e.g., distinction between \\textit{animal species}),\nhowever, has been probed insufficiently, despite its downstream importance. We\nfill this evaluation gap by creating \\texttt{FOCI} (\\textbf{F}ine-grained\n\\textbf{O}bject \\textbf{C}lass\\textbf{I}fication), a difficult multiple-choice\nbenchmark for fine-grained object classification, from existing object\nclassification datasets: (1) multiple-choice avoids ambiguous answers\nassociated with casting classification as open-ended QA task; (2) we retain\nclassification difficulty by mining negative labels with a CLIP model.\n\\texttt{FOCI}\\xspace complements five popular classification datasets with four\ndomain-specific subsets from ImageNet-21k. We benchmark 12 public LVLMs on\n\\texttt{FOCI} and show that it tests for a \\textit{complementary skill} to\nestablished image understanding and reasoning benchmarks. Crucially, CLIP\nmodels exhibit dramatically better performance than LVLMs. Since the image\nencoders of LVLMs come from these CLIP models, this points to inadequate\nalignment for fine-grained object distinction between the encoder and the LLM\nand warrants (pre)training data with more fine-grained annotation. We release\nour code at \\url{https://github.com/gregor-ge/FOCI-Benchmark}.\n", "link": "http://arxiv.org/abs/2406.14496v1", "date": "2024-06-20", "relevancy": 2.691, "topK": [{"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5603}, {"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.5315}, {"title": "Self-supervised Photographic Image Layout Representation Learning", "link": "http://arxiv.org/abs/2403.03740v1", "similarity": 0.5227}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20African%20or%20European%20Swallow%3F%20Benchmarking%20Large%20Vision-Language%20Models%0A%20%20for%20Fine-Grained%20Object%20Classification&body=Title%3A%20African%20or%20European%20Swallow%3F%20Benchmarking%20Large%20Vision-Language%20Models%0A%20%20for%20Fine-Grained%20Object%20Classification%0AAuthor%3A%20Gregor%20Geigle%20and%20Radu%20Timofte%20and%20Goran%20Glava%C5%A1%0AAbstract%3A%20%20%20Recent%20Large%20Vision-Language%20Models%20%28LVLMs%29%20demonstrate%20impressive%20abilities%0Aon%20numerous%20image%20understanding%20and%20reasoning%20tasks.%20The%20task%20of%20fine-grained%0Aobject%20classification%20%28e.g.%2C%20distinction%20between%20%5Ctextit%7Banimal%20species%7D%29%2C%0Ahowever%2C%20has%20been%20probed%20insufficiently%2C%20despite%20its%20downstream%20importance.%20We%0Afill%20this%20evaluation%20gap%20by%20creating%20%5Ctexttt%7BFOCI%7D%20%28%5Ctextbf%7BF%7Dine-grained%0A%5Ctextbf%7BO%7Dbject%20%5Ctextbf%7BC%7Dlass%5Ctextbf%7BI%7Dfication%29%2C%20a%20difficult%20multiple-choice%0Abenchmark%20for%20fine-grained%20object%20classification%2C%20from%20existing%20object%0Aclassification%20datasets%3A%20%281%29%20multiple-choice%20avoids%20ambiguous%20answers%0Aassociated%20with%20casting%20classification%20as%20open-ended%20QA%20task%3B%20%282%29%20we%20retain%0Aclassification%20difficulty%20by%20mining%20negative%20labels%20with%20a%20CLIP%20model.%0A%5Ctexttt%7BFOCI%7D%5Cxspace%20complements%20five%20popular%20classification%20datasets%20with%20four%0Adomain-specific%20subsets%20from%20ImageNet-21k.%20We%20benchmark%2012%20public%20LVLMs%20on%0A%5Ctexttt%7BFOCI%7D%20and%20show%20that%20it%20tests%20for%20a%20%5Ctextit%7Bcomplementary%20skill%7D%20to%0Aestablished%20image%20understanding%20and%20reasoning%20benchmarks.%20Crucially%2C%20CLIP%0Amodels%20exhibit%20dramatically%20better%20performance%20than%20LVLMs.%20Since%20the%20image%0Aencoders%20of%20LVLMs%20come%20from%20these%20CLIP%20models%2C%20this%20points%20to%20inadequate%0Aalignment%20for%20fine-grained%20object%20distinction%20between%20the%20encoder%20and%20the%20LLM%0Aand%20warrants%20%28pre%29training%20data%20with%20more%20fine-grained%20annotation.%20We%20release%0Aour%20code%20at%20%5Curl%7Bhttps%3A//github.com/gregor-ge/FOCI-Benchmark%7D.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2406.14496v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DAfrican%2520or%2520European%2520Swallow%253F%2520Benchmarking%2520Large%2520Vision-Language%2520Models%250A%2520%2520for%2520Fine-Grained%2520Object%2520Classification%26entry.906535625%3DGregor%2520Geigle%2520and%2520Radu%2520Timofte%2520and%2520Goran%2520Glava%25C5%25A1%26entry.1292438233%3D%2520%2520Recent%2520Large%2520Vision-Language%2520Models%2520%2528LVLMs%2529%2520demonstrate%2520impressive%2520abilities%250Aon%2520numerous%2520image%2520understanding%2520and%2520reasoning%2520tasks.%2520The%2520task%2520of%2520fine-grained%250Aobject%2520classification%2520%2528e.g.%252C%2520distinction%2520between%2520%255Ctextit%257Banimal%2520species%257D%2529%252C%250Ahowever%252C%2520has%2520been%2520probed%2520insufficiently%252C%2520despite%2520its%2520downstream%2520importance.%2520We%250Afill%2520this%2520evaluation%2520gap%2520by%2520creating%2520%255Ctexttt%257BFOCI%257D%2520%2528%255Ctextbf%257BF%257Dine-grained%250A%255Ctextbf%257BO%257Dbject%2520%255Ctextbf%257BC%257Dlass%255Ctextbf%257BI%257Dfication%2529%252C%2520a%2520difficult%2520multiple-choice%250Abenchmark%2520for%2520fine-grained%2520object%2520classification%252C%2520from%2520existing%2520object%250Aclassification%2520datasets%253A%2520%25281%2529%2520multiple-choice%2520avoids%2520ambiguous%2520answers%250Aassociated%2520with%2520casting%2520classification%2520as%2520open-ended%2520QA%2520task%253B%2520%25282%2529%2520we%2520retain%250Aclassification%2520difficulty%2520by%2520mining%2520negative%2520labels%2520with%2520a%2520CLIP%2520model.%250A%255Ctexttt%257BFOCI%257D%255Cxspace%2520complements%2520five%2520popular%2520classification%2520datasets%2520with%2520four%250Adomain-specific%2520subsets%2520from%2520ImageNet-21k.%2520We%2520benchmark%252012%2520public%2520LVLMs%2520on%250A%255Ctexttt%257BFOCI%257D%2520and%2520show%2520that%2520it%2520tests%2520for%2520a%2520%255Ctextit%257Bcomplementary%2520skill%257D%2520to%250Aestablished%2520image%2520understanding%2520and%2520reasoning%2520benchmarks.%2520Crucially%252C%2520CLIP%250Amodels%2520exhibit%2520dramatically%2520better%2520performance%2520than%2520LVLMs.%2520Since%2520the%2520image%250Aencoders%2520of%2520LVLMs%2520come%2520from%2520these%2520CLIP%2520models%252C%2520this%2520points%2520to%2520inadequate%250Aalignment%2520for%2520fine-grained%2520object%2520distinction%2520between%2520the%2520encoder%2520and%2520the%2520LLM%250Aand%2520warrants%2520%2528pre%2529training%2520data%2520with%2520more%2520fine-grained%2520annotation.%2520We%2520release%250Aour%2520code%2520at%2520%255Curl%257Bhttps%253A//github.com/gregor-ge/FOCI-Benchmark%257D.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2406.14496v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=African%20or%20European%20Swallow%3F%20Benchmarking%20Large%20Vision-Language%20Models%0A%20%20for%20Fine-Grained%20Object%20Classification&entry.906535625=Gregor%20Geigle%20and%20Radu%20Timofte%20and%20Goran%20Glava%C5%A1&entry.1292438233=%20%20Recent%20Large%20Vision-Language%20Models%20%28LVLMs%29%20demonstrate%20impressive%20abilities%0Aon%20numerous%20image%20understanding%20and%20reasoning%20tasks.%20The%20task%20of%20fine-grained%0Aobject%20classification%20%28e.g.%2C%20distinction%20between%20%5Ctextit%7Banimal%20species%7D%29%2C%0Ahowever%2C%20has%20been%20probed%20insufficiently%2C%20despite%20its%20downstream%20importance.%20We%0Afill%20this%20evaluation%20gap%20by%20creating%20%5Ctexttt%7BFOCI%7D%20%28%5Ctextbf%7BF%7Dine-grained%0A%5Ctextbf%7BO%7Dbject%20%5Ctextbf%7BC%7Dlass%5Ctextbf%7BI%7Dfication%29%2C%20a%20difficult%20multiple-choice%0Abenchmark%20for%20fine-grained%20object%20classification%2C%20from%20existing%20object%0Aclassification%20datasets%3A%20%281%29%20multiple-choice%20avoids%20ambiguous%20answers%0Aassociated%20with%20casting%20classification%20as%20open-ended%20QA%20task%3B%20%282%29%20we%20retain%0Aclassification%20difficulty%20by%20mining%20negative%20labels%20with%20a%20CLIP%20model.%0A%5Ctexttt%7BFOCI%7D%5Cxspace%20complements%20five%20popular%20classification%20datasets%20with%20four%0Adomain-specific%20subsets%20from%20ImageNet-21k.%20We%20benchmark%2012%20public%20LVLMs%20on%0A%5Ctexttt%7BFOCI%7D%20and%20show%20that%20it%20tests%20for%20a%20%5Ctextit%7Bcomplementary%20skill%7D%20to%0Aestablished%20image%20understanding%20and%20reasoning%20benchmarks.%20Crucially%2C%20CLIP%0Amodels%20exhibit%20dramatically%20better%20performance%20than%20LVLMs.%20Since%20the%20image%0Aencoders%20of%20LVLMs%20come%20from%20these%20CLIP%20models%2C%20this%20points%20to%20inadequate%0Aalignment%20for%20fine-grained%20object%20distinction%20between%20the%20encoder%20and%20the%20LLM%0Aand%20warrants%20%28pre%29training%20data%20with%20more%20fine-grained%20annotation.%20We%20release%0Aour%20code%20at%20%5Curl%7Bhttps%3A//github.com/gregor-ge/FOCI-Benchmark%7D.%0A&entry.1838667208=http%3A//arxiv.org/abs/2406.14496v1&entry.124074799=Read"},
{"title": "Learning to Discover Knowledge: A Weakly-Supervised Partial Domain\n  Adaptation Approach", "author": "Mengcheng Lan and Min Meng and Jun Yu and Jigang Wu", "abstract": "  Domain adaptation has shown appealing performance by leveraging knowledge\nfrom a source domain with rich annotations. However, for a specific target\ntask, it is cumbersome to collect related and high-quality source domains. In\nreal-world scenarios, large-scale datasets corrupted with noisy labels are easy\nto collect, stimulating a great demand for automatic recognition in a\ngeneralized setting, i.e., weakly-supervised partial domain adaptation\n(WS-PDA), which transfers a classifier from a large source domain with noises\nin labels to a small unlabeled target domain. As such, the key issues of WS-PDA\nare: 1) how to sufficiently discover the knowledge from the noisy labeled\nsource domain and the unlabeled target domain, and 2) how to successfully adapt\nthe knowledge across domains. In this paper, we propose a simple yet effective\ndomain adaptation approach, termed as self-paced transfer classifier learning\n(SP-TCL), to address the above issues, which could be regarded as a\nwell-performing baseline for several generalized domain adaptation tasks. The\nproposed model is established upon the self-paced learning scheme, seeking a\npreferable classifier for the target domain. Specifically, SP-TCL learns to\ndiscover faithful knowledge via a carefully designed prudent loss function and\nsimultaneously adapts the learned knowledge to the target domain by iteratively\nexcluding source examples from training under the self-paced fashion. Extensive\nevaluations on several benchmark datasets demonstrate that SP-TCL significantly\noutperforms state-of-the-art approaches on several generalized domain\nadaptation tasks.\n", "link": "http://arxiv.org/abs/2406.14274v1", "date": "2024-06-20", "relevancy": 2.6727, "topK": [{"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5454}, {"title": "Weakly-guided self-supervised pretraining for temporal activity detection", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/25189", "similarity": 0.5327}, {"title": "Self-supervised Photographic Image Layout Representation Learning", "link": "http://arxiv.org/abs/2403.03740v1", "similarity": 0.5255}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Learning%20to%20Discover%20Knowledge%3A%20A%20Weakly-Supervised%20Partial%20Domain%0A%20%20Adaptation%20Approach&body=Title%3A%20Learning%20to%20Discover%20Knowledge%3A%20A%20Weakly-Supervised%20Partial%20Domain%0A%20%20Adaptation%20Approach%0AAuthor%3A%20Mengcheng%20Lan%20and%20Min%20Meng%20and%20Jun%20Yu%20and%20Jigang%20Wu%0AAbstract%3A%20%20%20Domain%20adaptation%20has%20shown%20appealing%20performance%20by%20leveraging%20knowledge%0Afrom%20a%20source%20domain%20with%20rich%20annotations.%20However%2C%20for%20a%20specific%20target%0Atask%2C%20it%20is%20cumbersome%20to%20collect%20related%20and%20high-quality%20source%20domains.%20In%0Areal-world%20scenarios%2C%20large-scale%20datasets%20corrupted%20with%20noisy%20labels%20are%20easy%0Ato%20collect%2C%20stimulating%20a%20great%20demand%20for%20automatic%20recognition%20in%20a%0Ageneralized%20setting%2C%20i.e.%2C%20weakly-supervised%20partial%20domain%20adaptation%0A%28WS-PDA%29%2C%20which%20transfers%20a%20classifier%20from%20a%20large%20source%20domain%20with%20noises%0Ain%20labels%20to%20a%20small%20unlabeled%20target%20domain.%20As%20such%2C%20the%20key%20issues%20of%20WS-PDA%0Aare%3A%201%29%20how%20to%20sufficiently%20discover%20the%20knowledge%20from%20the%20noisy%20labeled%0Asource%20domain%20and%20the%20unlabeled%20target%20domain%2C%20and%202%29%20how%20to%20successfully%20adapt%0Athe%20knowledge%20across%20domains.%20In%20this%20paper%2C%20we%20propose%20a%20simple%20yet%20effective%0Adomain%20adaptation%20approach%2C%20termed%20as%20self-paced%20transfer%20classifier%20learning%0A%28SP-TCL%29%2C%20to%20address%20the%20above%20issues%2C%20which%20could%20be%20regarded%20as%20a%0Awell-performing%20baseline%20for%20several%20generalized%20domain%20adaptation%20tasks.%20The%0Aproposed%20model%20is%20established%20upon%20the%20self-paced%20learning%20scheme%2C%20seeking%20a%0Apreferable%20classifier%20for%20the%20target%20domain.%20Specifically%2C%20SP-TCL%20learns%20to%0Adiscover%20faithful%20knowledge%20via%20a%20carefully%20designed%20prudent%20loss%20function%20and%0Asimultaneously%20adapts%20the%20learned%20knowledge%20to%20the%20target%20domain%20by%20iteratively%0Aexcluding%20source%20examples%20from%20training%20under%20the%20self-paced%20fashion.%20Extensive%0Aevaluations%20on%20several%20benchmark%20datasets%20demonstrate%20that%20SP-TCL%20significantly%0Aoutperforms%20state-of-the-art%20approaches%20on%20several%20generalized%20domain%0Aadaptation%20tasks.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2406.14274v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DLearning%2520to%2520Discover%2520Knowledge%253A%2520A%2520Weakly-Supervised%2520Partial%2520Domain%250A%2520%2520Adaptation%2520Approach%26entry.906535625%3DMengcheng%2520Lan%2520and%2520Min%2520Meng%2520and%2520Jun%2520Yu%2520and%2520Jigang%2520Wu%26entry.1292438233%3D%2520%2520Domain%2520adaptation%2520has%2520shown%2520appealing%2520performance%2520by%2520leveraging%2520knowledge%250Afrom%2520a%2520source%2520domain%2520with%2520rich%2520annotations.%2520However%252C%2520for%2520a%2520specific%2520target%250Atask%252C%2520it%2520is%2520cumbersome%2520to%2520collect%2520related%2520and%2520high-quality%2520source%2520domains.%2520In%250Areal-world%2520scenarios%252C%2520large-scale%2520datasets%2520corrupted%2520with%2520noisy%2520labels%2520are%2520easy%250Ato%2520collect%252C%2520stimulating%2520a%2520great%2520demand%2520for%2520automatic%2520recognition%2520in%2520a%250Ageneralized%2520setting%252C%2520i.e.%252C%2520weakly-supervised%2520partial%2520domain%2520adaptation%250A%2528WS-PDA%2529%252C%2520which%2520transfers%2520a%2520classifier%2520from%2520a%2520large%2520source%2520domain%2520with%2520noises%250Ain%2520labels%2520to%2520a%2520small%2520unlabeled%2520target%2520domain.%2520As%2520such%252C%2520the%2520key%2520issues%2520of%2520WS-PDA%250Aare%253A%25201%2529%2520how%2520to%2520sufficiently%2520discover%2520the%2520knowledge%2520from%2520the%2520noisy%2520labeled%250Asource%2520domain%2520and%2520the%2520unlabeled%2520target%2520domain%252C%2520and%25202%2529%2520how%2520to%2520successfully%2520adapt%250Athe%2520knowledge%2520across%2520domains.%2520In%2520this%2520paper%252C%2520we%2520propose%2520a%2520simple%2520yet%2520effective%250Adomain%2520adaptation%2520approach%252C%2520termed%2520as%2520self-paced%2520transfer%2520classifier%2520learning%250A%2528SP-TCL%2529%252C%2520to%2520address%2520the%2520above%2520issues%252C%2520which%2520could%2520be%2520regarded%2520as%2520a%250Awell-performing%2520baseline%2520for%2520several%2520generalized%2520domain%2520adaptation%2520tasks.%2520The%250Aproposed%2520model%2520is%2520established%2520upon%2520the%2520self-paced%2520learning%2520scheme%252C%2520seeking%2520a%250Apreferable%2520classifier%2520for%2520the%2520target%2520domain.%2520Specifically%252C%2520SP-TCL%2520learns%2520to%250Adiscover%2520faithful%2520knowledge%2520via%2520a%2520carefully%2520designed%2520prudent%2520loss%2520function%2520and%250Asimultaneously%2520adapts%2520the%2520learned%2520knowledge%2520to%2520the%2520target%2520domain%2520by%2520iteratively%250Aexcluding%2520source%2520examples%2520from%2520training%2520under%2520the%2520self-paced%2520fashion.%2520Extensive%250Aevaluations%2520on%2520several%2520benchmark%2520datasets%2520demonstrate%2520that%2520SP-TCL%2520significantly%250Aoutperforms%2520state-of-the-art%2520approaches%2520on%2520several%2520generalized%2520domain%250Aadaptation%2520tasks.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2406.14274v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Learning%20to%20Discover%20Knowledge%3A%20A%20Weakly-Supervised%20Partial%20Domain%0A%20%20Adaptation%20Approach&entry.906535625=Mengcheng%20Lan%20and%20Min%20Meng%20and%20Jun%20Yu%20and%20Jigang%20Wu&entry.1292438233=%20%20Domain%20adaptation%20has%20shown%20appealing%20performance%20by%20leveraging%20knowledge%0Afrom%20a%20source%20domain%20with%20rich%20annotations.%20However%2C%20for%20a%20specific%20target%0Atask%2C%20it%20is%20cumbersome%20to%20collect%20related%20and%20high-quality%20source%20domains.%20In%0Areal-world%20scenarios%2C%20large-scale%20datasets%20corrupted%20with%20noisy%20labels%20are%20easy%0Ato%20collect%2C%20stimulating%20a%20great%20demand%20for%20automatic%20recognition%20in%20a%0Ageneralized%20setting%2C%20i.e.%2C%20weakly-supervised%20partial%20domain%20adaptation%0A%28WS-PDA%29%2C%20which%20transfers%20a%20classifier%20from%20a%20large%20source%20domain%20with%20noises%0Ain%20labels%20to%20a%20small%20unlabeled%20target%20domain.%20As%20such%2C%20the%20key%20issues%20of%20WS-PDA%0Aare%3A%201%29%20how%20to%20sufficiently%20discover%20the%20knowledge%20from%20the%20noisy%20labeled%0Asource%20domain%20and%20the%20unlabeled%20target%20domain%2C%20and%202%29%20how%20to%20successfully%20adapt%0Athe%20knowledge%20across%20domains.%20In%20this%20paper%2C%20we%20propose%20a%20simple%20yet%20effective%0Adomain%20adaptation%20approach%2C%20termed%20as%20self-paced%20transfer%20classifier%20learning%0A%28SP-TCL%29%2C%20to%20address%20the%20above%20issues%2C%20which%20could%20be%20regarded%20as%20a%0Awell-performing%20baseline%20for%20several%20generalized%20domain%20adaptation%20tasks.%20The%0Aproposed%20model%20is%20established%20upon%20the%20self-paced%20learning%20scheme%2C%20seeking%20a%0Apreferable%20classifier%20for%20the%20target%20domain.%20Specifically%2C%20SP-TCL%20learns%20to%0Adiscover%20faithful%20knowledge%20via%20a%20carefully%20designed%20prudent%20loss%20function%20and%0Asimultaneously%20adapts%20the%20learned%20knowledge%20to%20the%20target%20domain%20by%20iteratively%0Aexcluding%20source%20examples%20from%20training%20under%20the%20self-paced%20fashion.%20Extensive%0Aevaluations%20on%20several%20benchmark%20datasets%20demonstrate%20that%20SP-TCL%20significantly%0Aoutperforms%20state-of-the-art%20approaches%20on%20several%20generalized%20domain%0Aadaptation%20tasks.%0A&entry.1838667208=http%3A//arxiv.org/abs/2406.14274v1&entry.124074799=Read"},
{"title": "ATAC-Net: Zoomed view works better for Anomaly Detection", "author": "Shaurya Gupta and Neil Gautam and Anurag Malyala", "abstract": "  The application of deep learning in visual anomaly detection has gained\nwidespread popularity due to its potential use in quality control and\nmanufacturing. Current standard methods are Unsupervised, where a clean dataset\nis utilised to detect deviations and flag anomalies during testing. However,\nincorporating a few samples when the type of anomalies is known beforehand can\nsignificantly enhance performance. Thus, we propose ATAC-Net, a framework that\ntrains to detect anomalies from a minimal set of known prior anomalies.\nFurthermore, we introduce attention-guided cropping, which provides a closer\nview of suspect regions during the training phase. Our framework is a reliable\nand easy-to-understand system for detecting anomalies, and we substantiate its\nsuperiority to some of the current state-of-the-art techniques in a comparable\nsetting.\n", "link": "http://arxiv.org/abs/2406.14398v1", "date": "2024-06-20", "relevancy": 2.6333, "topK": [{"title": "Weakly-guided self-supervised pretraining for temporal activity detection", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/25189", "similarity": 0.5437}, {"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.5201}, {"title": "ToDo: Token Downsampling for Efficient Generation of High-Resolution\n  Images", "link": "http://arxiv.org/abs/2402.13573v2", "similarity": 0.5162}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20ATAC-Net%3A%20Zoomed%20view%20works%20better%20for%20Anomaly%20Detection&body=Title%3A%20ATAC-Net%3A%20Zoomed%20view%20works%20better%20for%20Anomaly%20Detection%0AAuthor%3A%20Shaurya%20Gupta%20and%20Neil%20Gautam%20and%20Anurag%20Malyala%0AAbstract%3A%20%20%20The%20application%20of%20deep%20learning%20in%20visual%20anomaly%20detection%20has%20gained%0Awidespread%20popularity%20due%20to%20its%20potential%20use%20in%20quality%20control%20and%0Amanufacturing.%20Current%20standard%20methods%20are%20Unsupervised%2C%20where%20a%20clean%20dataset%0Ais%20utilised%20to%20detect%20deviations%20and%20flag%20anomalies%20during%20testing.%20However%2C%0Aincorporating%20a%20few%20samples%20when%20the%20type%20of%20anomalies%20is%20known%20beforehand%20can%0Asignificantly%20enhance%20performance.%20Thus%2C%20we%20propose%20ATAC-Net%2C%20a%20framework%20that%0Atrains%20to%20detect%20anomalies%20from%20a%20minimal%20set%20of%20known%20prior%20anomalies.%0AFurthermore%2C%20we%20introduce%20attention-guided%20cropping%2C%20which%20provides%20a%20closer%0Aview%20of%20suspect%20regions%20during%20the%20training%20phase.%20Our%20framework%20is%20a%20reliable%0Aand%20easy-to-understand%20system%20for%20detecting%20anomalies%2C%20and%20we%20substantiate%20its%0Asuperiority%20to%20some%20of%20the%20current%20state-of-the-art%20techniques%20in%20a%20comparable%0Asetting.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2406.14398v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DATAC-Net%253A%2520Zoomed%2520view%2520works%2520better%2520for%2520Anomaly%2520Detection%26entry.906535625%3DShaurya%2520Gupta%2520and%2520Neil%2520Gautam%2520and%2520Anurag%2520Malyala%26entry.1292438233%3D%2520%2520The%2520application%2520of%2520deep%2520learning%2520in%2520visual%2520anomaly%2520detection%2520has%2520gained%250Awidespread%2520popularity%2520due%2520to%2520its%2520potential%2520use%2520in%2520quality%2520control%2520and%250Amanufacturing.%2520Current%2520standard%2520methods%2520are%2520Unsupervised%252C%2520where%2520a%2520clean%2520dataset%250Ais%2520utilised%2520to%2520detect%2520deviations%2520and%2520flag%2520anomalies%2520during%2520testing.%2520However%252C%250Aincorporating%2520a%2520few%2520samples%2520when%2520the%2520type%2520of%2520anomalies%2520is%2520known%2520beforehand%2520can%250Asignificantly%2520enhance%2520performance.%2520Thus%252C%2520we%2520propose%2520ATAC-Net%252C%2520a%2520framework%2520that%250Atrains%2520to%2520detect%2520anomalies%2520from%2520a%2520minimal%2520set%2520of%2520known%2520prior%2520anomalies.%250AFurthermore%252C%2520we%2520introduce%2520attention-guided%2520cropping%252C%2520which%2520provides%2520a%2520closer%250Aview%2520of%2520suspect%2520regions%2520during%2520the%2520training%2520phase.%2520Our%2520framework%2520is%2520a%2520reliable%250Aand%2520easy-to-understand%2520system%2520for%2520detecting%2520anomalies%252C%2520and%2520we%2520substantiate%2520its%250Asuperiority%2520to%2520some%2520of%2520the%2520current%2520state-of-the-art%2520techniques%2520in%2520a%2520comparable%250Asetting.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2406.14398v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=ATAC-Net%3A%20Zoomed%20view%20works%20better%20for%20Anomaly%20Detection&entry.906535625=Shaurya%20Gupta%20and%20Neil%20Gautam%20and%20Anurag%20Malyala&entry.1292438233=%20%20The%20application%20of%20deep%20learning%20in%20visual%20anomaly%20detection%20has%20gained%0Awidespread%20popularity%20due%20to%20its%20potential%20use%20in%20quality%20control%20and%0Amanufacturing.%20Current%20standard%20methods%20are%20Unsupervised%2C%20where%20a%20clean%20dataset%0Ais%20utilised%20to%20detect%20deviations%20and%20flag%20anomalies%20during%20testing.%20However%2C%0Aincorporating%20a%20few%20samples%20when%20the%20type%20of%20anomalies%20is%20known%20beforehand%20can%0Asignificantly%20enhance%20performance.%20Thus%2C%20we%20propose%20ATAC-Net%2C%20a%20framework%20that%0Atrains%20to%20detect%20anomalies%20from%20a%20minimal%20set%20of%20known%20prior%20anomalies.%0AFurthermore%2C%20we%20introduce%20attention-guided%20cropping%2C%20which%20provides%20a%20closer%0Aview%20of%20suspect%20regions%20during%20the%20training%20phase.%20Our%20framework%20is%20a%20reliable%0Aand%20easy-to-understand%20system%20for%20detecting%20anomalies%2C%20and%20we%20substantiate%20its%0Asuperiority%20to%20some%20of%20the%20current%20state-of-the-art%20techniques%20in%20a%20comparable%0Asetting.%0A&entry.1838667208=http%3A//arxiv.org/abs/2406.14398v1&entry.124074799=Read"},
{"title": "Computation-Efficient Semi-Supervised Learning for ECG-based\n  Cardiovascular Diseases Detection", "author": "Rushuang Zhou and Zijun Liu and Lei Clifton and David A. Clifton and Kannie W. Y. Chan and Yuan-Ting Zhang and Yining Dong", "abstract": "  Label scarcity problem is the main challenge that hinders the wide\napplication of deep learning systems in automatic cardiovascular diseases\n(CVDs) detection using electrocardiography (ECG). Tuning pre-trained models\nalleviates this problem by transferring knowledge learned from large datasets\nto downstream small datasets. However, bottlenecks in computational efficiency\nand CVDs detection performance limit its clinical applications. It is difficult\nto improve the detection performance without significantly sacrificing model\ncomputational efficiency. Here, we propose a computation-efficient\nsemi-supervised learning paradigm (FastECG) for robust and\ncomputation-efficient CVDs detection using ECG. It enables a robust adaptation\nof pre-trained models on downstream datasets with limited supervision and high\ncomputational efficiency. First, a random-deactivation technique is developed\nto achieve robust and fast low-rank adaptation of pre-trained weights.\nSubsequently, we propose a one-shot rank allocation module to determine the\noptimal ranks for the update matrices of the pre-trained weights. Finally, a\nlightweight semi-supervised learning pipeline is introduced to enhance model\nperformance by leveraging labeled and unlabeled data with high computational\nefficiency. Extensive experiments on four downstream ECG datasets demonstrate\nthat FastECG not only outperforms the state-of-the-art methods in multi-label\nCVDs detection but also consumes fewer GPU footprints, training time, and\nparameter storage space. As such, this paradigm provides an effective solution\nfor achieving high computational efficiency and robust detection performance in\nthe clinical applications of pre-trained models under limited supervision.\n", "link": "http://arxiv.org/abs/2406.14377v1", "date": "2024-06-20", "relevancy": 2.5764, "topK": [{"title": "Weakly-guided self-supervised pretraining for temporal activity detection", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/25189", "similarity": 0.5259}, {"title": "Self-supervised Photographic Image Layout Representation Learning", "link": "http://arxiv.org/abs/2403.03740v1", "similarity": 0.5209}, {"title": "ToDo: Token Downsampling for Efficient Generation of High-Resolution\n  Images", "link": "http://arxiv.org/abs/2402.13573v2", "similarity": 0.4991}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Computation-Efficient%20Semi-Supervised%20Learning%20for%20ECG-based%0A%20%20Cardiovascular%20Diseases%20Detection&body=Title%3A%20Computation-Efficient%20Semi-Supervised%20Learning%20for%20ECG-based%0A%20%20Cardiovascular%20Diseases%20Detection%0AAuthor%3A%20Rushuang%20Zhou%20and%20Zijun%20Liu%20and%20Lei%20Clifton%20and%20David%20A.%20Clifton%20and%20Kannie%20W.%20Y.%20Chan%20and%20Yuan-Ting%20Zhang%20and%20Yining%20Dong%0AAbstract%3A%20%20%20Label%20scarcity%20problem%20is%20the%20main%20challenge%20that%20hinders%20the%20wide%0Aapplication%20of%20deep%20learning%20systems%20in%20automatic%20cardiovascular%20diseases%0A%28CVDs%29%20detection%20using%20electrocardiography%20%28ECG%29.%20Tuning%20pre-trained%20models%0Aalleviates%20this%20problem%20by%20transferring%20knowledge%20learned%20from%20large%20datasets%0Ato%20downstream%20small%20datasets.%20However%2C%20bottlenecks%20in%20computational%20efficiency%0Aand%20CVDs%20detection%20performance%20limit%20its%20clinical%20applications.%20It%20is%20difficult%0Ato%20improve%20the%20detection%20performance%20without%20significantly%20sacrificing%20model%0Acomputational%20efficiency.%20Here%2C%20we%20propose%20a%20computation-efficient%0Asemi-supervised%20learning%20paradigm%20%28FastECG%29%20for%20robust%20and%0Acomputation-efficient%20CVDs%20detection%20using%20ECG.%20It%20enables%20a%20robust%20adaptation%0Aof%20pre-trained%20models%20on%20downstream%20datasets%20with%20limited%20supervision%20and%20high%0Acomputational%20efficiency.%20First%2C%20a%20random-deactivation%20technique%20is%20developed%0Ato%20achieve%20robust%20and%20fast%20low-rank%20adaptation%20of%20pre-trained%20weights.%0ASubsequently%2C%20we%20propose%20a%20one-shot%20rank%20allocation%20module%20to%20determine%20the%0Aoptimal%20ranks%20for%20the%20update%20matrices%20of%20the%20pre-trained%20weights.%20Finally%2C%20a%0Alightweight%20semi-supervised%20learning%20pipeline%20is%20introduced%20to%20enhance%20model%0Aperformance%20by%20leveraging%20labeled%20and%20unlabeled%20data%20with%20high%20computational%0Aefficiency.%20Extensive%20experiments%20on%20four%20downstream%20ECG%20datasets%20demonstrate%0Athat%20FastECG%20not%20only%20outperforms%20the%20state-of-the-art%20methods%20in%20multi-label%0ACVDs%20detection%20but%20also%20consumes%20fewer%20GPU%20footprints%2C%20training%20time%2C%20and%0Aparameter%20storage%20space.%20As%20such%2C%20this%20paradigm%20provides%20an%20effective%20solution%0Afor%20achieving%20high%20computational%20efficiency%20and%20robust%20detection%20performance%20in%0Athe%20clinical%20applications%20of%20pre-trained%20models%20under%20limited%20supervision.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2406.14377v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DComputation-Efficient%2520Semi-Supervised%2520Learning%2520for%2520ECG-based%250A%2520%2520Cardiovascular%2520Diseases%2520Detection%26entry.906535625%3DRushuang%2520Zhou%2520and%2520Zijun%2520Liu%2520and%2520Lei%2520Clifton%2520and%2520David%2520A.%2520Clifton%2520and%2520Kannie%2520W.%2520Y.%2520Chan%2520and%2520Yuan-Ting%2520Zhang%2520and%2520Yining%2520Dong%26entry.1292438233%3D%2520%2520Label%2520scarcity%2520problem%2520is%2520the%2520main%2520challenge%2520that%2520hinders%2520the%2520wide%250Aapplication%2520of%2520deep%2520learning%2520systems%2520in%2520automatic%2520cardiovascular%2520diseases%250A%2528CVDs%2529%2520detection%2520using%2520electrocardiography%2520%2528ECG%2529.%2520Tuning%2520pre-trained%2520models%250Aalleviates%2520this%2520problem%2520by%2520transferring%2520knowledge%2520learned%2520from%2520large%2520datasets%250Ato%2520downstream%2520small%2520datasets.%2520However%252C%2520bottlenecks%2520in%2520computational%2520efficiency%250Aand%2520CVDs%2520detection%2520performance%2520limit%2520its%2520clinical%2520applications.%2520It%2520is%2520difficult%250Ato%2520improve%2520the%2520detection%2520performance%2520without%2520significantly%2520sacrificing%2520model%250Acomputational%2520efficiency.%2520Here%252C%2520we%2520propose%2520a%2520computation-efficient%250Asemi-supervised%2520learning%2520paradigm%2520%2528FastECG%2529%2520for%2520robust%2520and%250Acomputation-efficient%2520CVDs%2520detection%2520using%2520ECG.%2520It%2520enables%2520a%2520robust%2520adaptation%250Aof%2520pre-trained%2520models%2520on%2520downstream%2520datasets%2520with%2520limited%2520supervision%2520and%2520high%250Acomputational%2520efficiency.%2520First%252C%2520a%2520random-deactivation%2520technique%2520is%2520developed%250Ato%2520achieve%2520robust%2520and%2520fast%2520low-rank%2520adaptation%2520of%2520pre-trained%2520weights.%250ASubsequently%252C%2520we%2520propose%2520a%2520one-shot%2520rank%2520allocation%2520module%2520to%2520determine%2520the%250Aoptimal%2520ranks%2520for%2520the%2520update%2520matrices%2520of%2520the%2520pre-trained%2520weights.%2520Finally%252C%2520a%250Alightweight%2520semi-supervised%2520learning%2520pipeline%2520is%2520introduced%2520to%2520enhance%2520model%250Aperformance%2520by%2520leveraging%2520labeled%2520and%2520unlabeled%2520data%2520with%2520high%2520computational%250Aefficiency.%2520Extensive%2520experiments%2520on%2520four%2520downstream%2520ECG%2520datasets%2520demonstrate%250Athat%2520FastECG%2520not%2520only%2520outperforms%2520the%2520state-of-the-art%2520methods%2520in%2520multi-label%250ACVDs%2520detection%2520but%2520also%2520consumes%2520fewer%2520GPU%2520footprints%252C%2520training%2520time%252C%2520and%250Aparameter%2520storage%2520space.%2520As%2520such%252C%2520this%2520paradigm%2520provides%2520an%2520effective%2520solution%250Afor%2520achieving%2520high%2520computational%2520efficiency%2520and%2520robust%2520detection%2520performance%2520in%250Athe%2520clinical%2520applications%2520of%2520pre-trained%2520models%2520under%2520limited%2520supervision.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2406.14377v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Computation-Efficient%20Semi-Supervised%20Learning%20for%20ECG-based%0A%20%20Cardiovascular%20Diseases%20Detection&entry.906535625=Rushuang%20Zhou%20and%20Zijun%20Liu%20and%20Lei%20Clifton%20and%20David%20A.%20Clifton%20and%20Kannie%20W.%20Y.%20Chan%20and%20Yuan-Ting%20Zhang%20and%20Yining%20Dong&entry.1292438233=%20%20Label%20scarcity%20problem%20is%20the%20main%20challenge%20that%20hinders%20the%20wide%0Aapplication%20of%20deep%20learning%20systems%20in%20automatic%20cardiovascular%20diseases%0A%28CVDs%29%20detection%20using%20electrocardiography%20%28ECG%29.%20Tuning%20pre-trained%20models%0Aalleviates%20this%20problem%20by%20transferring%20knowledge%20learned%20from%20large%20datasets%0Ato%20downstream%20small%20datasets.%20However%2C%20bottlenecks%20in%20computational%20efficiency%0Aand%20CVDs%20detection%20performance%20limit%20its%20clinical%20applications.%20It%20is%20difficult%0Ato%20improve%20the%20detection%20performance%20without%20significantly%20sacrificing%20model%0Acomputational%20efficiency.%20Here%2C%20we%20propose%20a%20computation-efficient%0Asemi-supervised%20learning%20paradigm%20%28FastECG%29%20for%20robust%20and%0Acomputation-efficient%20CVDs%20detection%20using%20ECG.%20It%20enables%20a%20robust%20adaptation%0Aof%20pre-trained%20models%20on%20downstream%20datasets%20with%20limited%20supervision%20and%20high%0Acomputational%20efficiency.%20First%2C%20a%20random-deactivation%20technique%20is%20developed%0Ato%20achieve%20robust%20and%20fast%20low-rank%20adaptation%20of%20pre-trained%20weights.%0ASubsequently%2C%20we%20propose%20a%20one-shot%20rank%20allocation%20module%20to%20determine%20the%0Aoptimal%20ranks%20for%20the%20update%20matrices%20of%20the%20pre-trained%20weights.%20Finally%2C%20a%0Alightweight%20semi-supervised%20learning%20pipeline%20is%20introduced%20to%20enhance%20model%0Aperformance%20by%20leveraging%20labeled%20and%20unlabeled%20data%20with%20high%20computational%0Aefficiency.%20Extensive%20experiments%20on%20four%20downstream%20ECG%20datasets%20demonstrate%0Athat%20FastECG%20not%20only%20outperforms%20the%20state-of-the-art%20methods%20in%20multi-label%0ACVDs%20detection%20but%20also%20consumes%20fewer%20GPU%20footprints%2C%20training%20time%2C%20and%0Aparameter%20storage%20space.%20As%20such%2C%20this%20paradigm%20provides%20an%20effective%20solution%0Afor%20achieving%20high%20computational%20efficiency%20and%20robust%20detection%20performance%20in%0Athe%20clinical%20applications%20of%20pre-trained%20models%20under%20limited%20supervision.%0A&entry.1838667208=http%3A//arxiv.org/abs/2406.14377v1&entry.124074799=Read"},
{"title": "Fantastic Copyrighted Beasts and How (Not) to Generate Them", "author": "Luxi He and Yangsibo Huang and Weijia Shi and Tinghao Xie and Haotian Liu and Yue Wang and Luke Zettlemoyer and Chiyuan Zhang and Danqi Chen and Peter Henderson", "abstract": "  Recent studies show that image and video generation models can be prompted to\nreproduce copyrighted content from their training data, raising serious legal\nconcerns around copyright infringement. Copyrighted characters, in particular,\npose a difficult challenge for image generation services, with at least one\nlawsuit already awarding damages based on the generation of these characters.\nYet, little research has empirically examined this issue. We conduct a\nsystematic evaluation to fill this gap. First, we build CopyCat, an evaluation\nsuite consisting of diverse copyrighted characters and a novel evaluation\npipeline. Our evaluation considers both the detection of similarity to\ncopyrighted characters and generated image's consistency with user input. Our\nevaluation systematically shows that both image and video generation models can\nstill generate characters even if characters' names are not explicitly\nmentioned in the prompt, sometimes with only two generic keywords (e.g.,\nprompting with \"videogame, plumber\" consistently generates Nintendo's Mario\ncharacter). We then introduce techniques to semi-automatically identify such\nkeywords or descriptions that trigger character generation. Using our\nevaluation suite, we study runtime mitigation strategies, including both\nexisting methods and new strategies we propose. Our findings reveal that\ncommonly employed strategies, such as prompt rewriting in the DALL-E system,\nare not sufficient as standalone guardrails. These strategies must be coupled\nwith other approaches, like negative prompting, to effectively reduce the\nunintended generation of copyrighted characters. Our work provides empirical\ngrounding to the discussion of copyright mitigation strategies and offers\nactionable insights for model deployers actively implementing them.\n", "link": "http://arxiv.org/abs/2406.14526v1", "date": "2024-06-20", "relevancy": 2.5556, "topK": [{"title": "Genie: Generative Interactive Environments", "link": "http://arxiv.org/abs/2402.15391v1", "similarity": 0.5336}, {"title": "Magic-Me: Identity-Specific Video Customized Diffusion", "link": "http://arxiv.org/abs/2402.09368v1", "similarity": 0.5072}, {"title": "DressCode: Autoregressively Sewing and Generating Garments from Text\n  Guidance", "link": "http://arxiv.org/abs/2401.16465v3", "similarity": 0.4926}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Fantastic%20Copyrighted%20Beasts%20and%20How%20%28Not%29%20to%20Generate%20Them&body=Title%3A%20Fantastic%20Copyrighted%20Beasts%20and%20How%20%28Not%29%20to%20Generate%20Them%0AAuthor%3A%20Luxi%20He%20and%20Yangsibo%20Huang%20and%20Weijia%20Shi%20and%20Tinghao%20Xie%20and%20Haotian%20Liu%20and%20Yue%20Wang%20and%20Luke%20Zettlemoyer%20and%20Chiyuan%20Zhang%20and%20Danqi%20Chen%20and%20Peter%20Henderson%0AAbstract%3A%20%20%20Recent%20studies%20show%20that%20image%20and%20video%20generation%20models%20can%20be%20prompted%20to%0Areproduce%20copyrighted%20content%20from%20their%20training%20data%2C%20raising%20serious%20legal%0Aconcerns%20around%20copyright%20infringement.%20Copyrighted%20characters%2C%20in%20particular%2C%0Apose%20a%20difficult%20challenge%20for%20image%20generation%20services%2C%20with%20at%20least%20one%0Alawsuit%20already%20awarding%20damages%20based%20on%20the%20generation%20of%20these%20characters.%0AYet%2C%20little%20research%20has%20empirically%20examined%20this%20issue.%20We%20conduct%20a%0Asystematic%20evaluation%20to%20fill%20this%20gap.%20First%2C%20we%20build%20CopyCat%2C%20an%20evaluation%0Asuite%20consisting%20of%20diverse%20copyrighted%20characters%20and%20a%20novel%20evaluation%0Apipeline.%20Our%20evaluation%20considers%20both%20the%20detection%20of%20similarity%20to%0Acopyrighted%20characters%20and%20generated%20image%27s%20consistency%20with%20user%20input.%20Our%0Aevaluation%20systematically%20shows%20that%20both%20image%20and%20video%20generation%20models%20can%0Astill%20generate%20characters%20even%20if%20characters%27%20names%20are%20not%20explicitly%0Amentioned%20in%20the%20prompt%2C%20sometimes%20with%20only%20two%20generic%20keywords%20%28e.g.%2C%0Aprompting%20with%20%22videogame%2C%20plumber%22%20consistently%20generates%20Nintendo%27s%20Mario%0Acharacter%29.%20We%20then%20introduce%20techniques%20to%20semi-automatically%20identify%20such%0Akeywords%20or%20descriptions%20that%20trigger%20character%20generation.%20Using%20our%0Aevaluation%20suite%2C%20we%20study%20runtime%20mitigation%20strategies%2C%20including%20both%0Aexisting%20methods%20and%20new%20strategies%20we%20propose.%20Our%20findings%20reveal%20that%0Acommonly%20employed%20strategies%2C%20such%20as%20prompt%20rewriting%20in%20the%20DALL-E%20system%2C%0Aare%20not%20sufficient%20as%20standalone%20guardrails.%20These%20strategies%20must%20be%20coupled%0Awith%20other%20approaches%2C%20like%20negative%20prompting%2C%20to%20effectively%20reduce%20the%0Aunintended%20generation%20of%20copyrighted%20characters.%20Our%20work%20provides%20empirical%0Agrounding%20to%20the%20discussion%20of%20copyright%20mitigation%20strategies%20and%20offers%0Aactionable%20insights%20for%20model%20deployers%20actively%20implementing%20them.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2406.14526v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DFantastic%2520Copyrighted%2520Beasts%2520and%2520How%2520%2528Not%2529%2520to%2520Generate%2520Them%26entry.906535625%3DLuxi%2520He%2520and%2520Yangsibo%2520Huang%2520and%2520Weijia%2520Shi%2520and%2520Tinghao%2520Xie%2520and%2520Haotian%2520Liu%2520and%2520Yue%2520Wang%2520and%2520Luke%2520Zettlemoyer%2520and%2520Chiyuan%2520Zhang%2520and%2520Danqi%2520Chen%2520and%2520Peter%2520Henderson%26entry.1292438233%3D%2520%2520Recent%2520studies%2520show%2520that%2520image%2520and%2520video%2520generation%2520models%2520can%2520be%2520prompted%2520to%250Areproduce%2520copyrighted%2520content%2520from%2520their%2520training%2520data%252C%2520raising%2520serious%2520legal%250Aconcerns%2520around%2520copyright%2520infringement.%2520Copyrighted%2520characters%252C%2520in%2520particular%252C%250Apose%2520a%2520difficult%2520challenge%2520for%2520image%2520generation%2520services%252C%2520with%2520at%2520least%2520one%250Alawsuit%2520already%2520awarding%2520damages%2520based%2520on%2520the%2520generation%2520of%2520these%2520characters.%250AYet%252C%2520little%2520research%2520has%2520empirically%2520examined%2520this%2520issue.%2520We%2520conduct%2520a%250Asystematic%2520evaluation%2520to%2520fill%2520this%2520gap.%2520First%252C%2520we%2520build%2520CopyCat%252C%2520an%2520evaluation%250Asuite%2520consisting%2520of%2520diverse%2520copyrighted%2520characters%2520and%2520a%2520novel%2520evaluation%250Apipeline.%2520Our%2520evaluation%2520considers%2520both%2520the%2520detection%2520of%2520similarity%2520to%250Acopyrighted%2520characters%2520and%2520generated%2520image%2527s%2520consistency%2520with%2520user%2520input.%2520Our%250Aevaluation%2520systematically%2520shows%2520that%2520both%2520image%2520and%2520video%2520generation%2520models%2520can%250Astill%2520generate%2520characters%2520even%2520if%2520characters%2527%2520names%2520are%2520not%2520explicitly%250Amentioned%2520in%2520the%2520prompt%252C%2520sometimes%2520with%2520only%2520two%2520generic%2520keywords%2520%2528e.g.%252C%250Aprompting%2520with%2520%2522videogame%252C%2520plumber%2522%2520consistently%2520generates%2520Nintendo%2527s%2520Mario%250Acharacter%2529.%2520We%2520then%2520introduce%2520techniques%2520to%2520semi-automatically%2520identify%2520such%250Akeywords%2520or%2520descriptions%2520that%2520trigger%2520character%2520generation.%2520Using%2520our%250Aevaluation%2520suite%252C%2520we%2520study%2520runtime%2520mitigation%2520strategies%252C%2520including%2520both%250Aexisting%2520methods%2520and%2520new%2520strategies%2520we%2520propose.%2520Our%2520findings%2520reveal%2520that%250Acommonly%2520employed%2520strategies%252C%2520such%2520as%2520prompt%2520rewriting%2520in%2520the%2520DALL-E%2520system%252C%250Aare%2520not%2520sufficient%2520as%2520standalone%2520guardrails.%2520These%2520strategies%2520must%2520be%2520coupled%250Awith%2520other%2520approaches%252C%2520like%2520negative%2520prompting%252C%2520to%2520effectively%2520reduce%2520the%250Aunintended%2520generation%2520of%2520copyrighted%2520characters.%2520Our%2520work%2520provides%2520empirical%250Agrounding%2520to%2520the%2520discussion%2520of%2520copyright%2520mitigation%2520strategies%2520and%2520offers%250Aactionable%2520insights%2520for%2520model%2520deployers%2520actively%2520implementing%2520them.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2406.14526v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Fantastic%20Copyrighted%20Beasts%20and%20How%20%28Not%29%20to%20Generate%20Them&entry.906535625=Luxi%20He%20and%20Yangsibo%20Huang%20and%20Weijia%20Shi%20and%20Tinghao%20Xie%20and%20Haotian%20Liu%20and%20Yue%20Wang%20and%20Luke%20Zettlemoyer%20and%20Chiyuan%20Zhang%20and%20Danqi%20Chen%20and%20Peter%20Henderson&entry.1292438233=%20%20Recent%20studies%20show%20that%20image%20and%20video%20generation%20models%20can%20be%20prompted%20to%0Areproduce%20copyrighted%20content%20from%20their%20training%20data%2C%20raising%20serious%20legal%0Aconcerns%20around%20copyright%20infringement.%20Copyrighted%20characters%2C%20in%20particular%2C%0Apose%20a%20difficult%20challenge%20for%20image%20generation%20services%2C%20with%20at%20least%20one%0Alawsuit%20already%20awarding%20damages%20based%20on%20the%20generation%20of%20these%20characters.%0AYet%2C%20little%20research%20has%20empirically%20examined%20this%20issue.%20We%20conduct%20a%0Asystematic%20evaluation%20to%20fill%20this%20gap.%20First%2C%20we%20build%20CopyCat%2C%20an%20evaluation%0Asuite%20consisting%20of%20diverse%20copyrighted%20characters%20and%20a%20novel%20evaluation%0Apipeline.%20Our%20evaluation%20considers%20both%20the%20detection%20of%20similarity%20to%0Acopyrighted%20characters%20and%20generated%20image%27s%20consistency%20with%20user%20input.%20Our%0Aevaluation%20systematically%20shows%20that%20both%20image%20and%20video%20generation%20models%20can%0Astill%20generate%20characters%20even%20if%20characters%27%20names%20are%20not%20explicitly%0Amentioned%20in%20the%20prompt%2C%20sometimes%20with%20only%20two%20generic%20keywords%20%28e.g.%2C%0Aprompting%20with%20%22videogame%2C%20plumber%22%20consistently%20generates%20Nintendo%27s%20Mario%0Acharacter%29.%20We%20then%20introduce%20techniques%20to%20semi-automatically%20identify%20such%0Akeywords%20or%20descriptions%20that%20trigger%20character%20generation.%20Using%20our%0Aevaluation%20suite%2C%20we%20study%20runtime%20mitigation%20strategies%2C%20including%20both%0Aexisting%20methods%20and%20new%20strategies%20we%20propose.%20Our%20findings%20reveal%20that%0Acommonly%20employed%20strategies%2C%20such%20as%20prompt%20rewriting%20in%20the%20DALL-E%20system%2C%0Aare%20not%20sufficient%20as%20standalone%20guardrails.%20These%20strategies%20must%20be%20coupled%0Awith%20other%20approaches%2C%20like%20negative%20prompting%2C%20to%20effectively%20reduce%20the%0Aunintended%20generation%20of%20copyrighted%20characters.%20Our%20work%20provides%20empirical%0Agrounding%20to%20the%20discussion%20of%20copyright%20mitigation%20strategies%20and%20offers%0Aactionable%20insights%20for%20model%20deployers%20actively%20implementing%20them.%0A&entry.1838667208=http%3A//arxiv.org/abs/2406.14526v1&entry.124074799=Read"},
{"title": "Adaptive Adversarial Cross-Entropy Loss for Sharpness-Aware Minimization", "author": "Tanapat Ratchatorn and Masayuki Tanaka", "abstract": "  Recent advancements in learning algorithms have demonstrated that the\nsharpness of the loss surface is an effective measure for improving the\ngeneralization gap. Building upon this concept, Sharpness-Aware Minimization\n(SAM) was proposed to enhance model generalization and achieved\nstate-of-the-art performance. SAM consists of two main steps, the weight\nperturbation step and the weight updating step. However, the perturbation in\nSAM is determined by only the gradient of the training loss, or cross-entropy\nloss. As the model approaches a stationary point, this gradient becomes small\nand oscillates, leading to inconsistent perturbation directions and also has a\nchance of diminishing the gradient. Our research introduces an innovative\napproach to further enhancing model generalization. We propose the Adaptive\nAdversarial Cross-Entropy (AACE) loss function to replace standard\ncross-entropy loss for SAM's perturbation. AACE loss and its gradient uniquely\nincrease as the model nears convergence, ensuring consistent perturbation\ndirection and addressing the gradient diminishing issue. Additionally, a novel\nperturbation-generating function utilizing AACE loss without normalization is\nproposed, enhancing the model's exploratory capabilities in near-optimum\nstages. Empirical testing confirms the effectiveness of AACE, with experiments\ndemonstrating improved performance in image classification tasks using Wide\nResNet and PyramidNet across various datasets. The reproduction code is\navailable online\n", "link": "http://arxiv.org/abs/2406.14329v1", "date": "2024-06-20", "relevancy": 2.5523, "topK": [{"title": "Breadcrumbs: Adversarial class-balanced sampling for long-tailed recognition", "link": "https://link.springer.com/chapter/10.1007/978-3-031-20053-3_37", "similarity": 0.5274}, {"title": "Boosted dynamic neural networks", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/26302", "similarity": 0.511}, {"title": "ToDo: Token Downsampling for Efficient Generation of High-Resolution\n  Images", "link": "http://arxiv.org/abs/2402.13573v2", "similarity": 0.493}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Adaptive%20Adversarial%20Cross-Entropy%20Loss%20for%20Sharpness-Aware%20Minimization&body=Title%3A%20Adaptive%20Adversarial%20Cross-Entropy%20Loss%20for%20Sharpness-Aware%20Minimization%0AAuthor%3A%20Tanapat%20Ratchatorn%20and%20Masayuki%20Tanaka%0AAbstract%3A%20%20%20Recent%20advancements%20in%20learning%20algorithms%20have%20demonstrated%20that%20the%0Asharpness%20of%20the%20loss%20surface%20is%20an%20effective%20measure%20for%20improving%20the%0Ageneralization%20gap.%20Building%20upon%20this%20concept%2C%20Sharpness-Aware%20Minimization%0A%28SAM%29%20was%20proposed%20to%20enhance%20model%20generalization%20and%20achieved%0Astate-of-the-art%20performance.%20SAM%20consists%20of%20two%20main%20steps%2C%20the%20weight%0Aperturbation%20step%20and%20the%20weight%20updating%20step.%20However%2C%20the%20perturbation%20in%0ASAM%20is%20determined%20by%20only%20the%20gradient%20of%20the%20training%20loss%2C%20or%20cross-entropy%0Aloss.%20As%20the%20model%20approaches%20a%20stationary%20point%2C%20this%20gradient%20becomes%20small%0Aand%20oscillates%2C%20leading%20to%20inconsistent%20perturbation%20directions%20and%20also%20has%20a%0Achance%20of%20diminishing%20the%20gradient.%20Our%20research%20introduces%20an%20innovative%0Aapproach%20to%20further%20enhancing%20model%20generalization.%20We%20propose%20the%20Adaptive%0AAdversarial%20Cross-Entropy%20%28AACE%29%20loss%20function%20to%20replace%20standard%0Across-entropy%20loss%20for%20SAM%27s%20perturbation.%20AACE%20loss%20and%20its%20gradient%20uniquely%0Aincrease%20as%20the%20model%20nears%20convergence%2C%20ensuring%20consistent%20perturbation%0Adirection%20and%20addressing%20the%20gradient%20diminishing%20issue.%20Additionally%2C%20a%20novel%0Aperturbation-generating%20function%20utilizing%20AACE%20loss%20without%20normalization%20is%0Aproposed%2C%20enhancing%20the%20model%27s%20exploratory%20capabilities%20in%20near-optimum%0Astages.%20Empirical%20testing%20confirms%20the%20effectiveness%20of%20AACE%2C%20with%20experiments%0Ademonstrating%20improved%20performance%20in%20image%20classification%20tasks%20using%20Wide%0AResNet%20and%20PyramidNet%20across%20various%20datasets.%20The%20reproduction%20code%20is%0Aavailable%20online%0A%0ALink%3A%20http%3A//arxiv.org/abs/2406.14329v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DAdaptive%2520Adversarial%2520Cross-Entropy%2520Loss%2520for%2520Sharpness-Aware%2520Minimization%26entry.906535625%3DTanapat%2520Ratchatorn%2520and%2520Masayuki%2520Tanaka%26entry.1292438233%3D%2520%2520Recent%2520advancements%2520in%2520learning%2520algorithms%2520have%2520demonstrated%2520that%2520the%250Asharpness%2520of%2520the%2520loss%2520surface%2520is%2520an%2520effective%2520measure%2520for%2520improving%2520the%250Ageneralization%2520gap.%2520Building%2520upon%2520this%2520concept%252C%2520Sharpness-Aware%2520Minimization%250A%2528SAM%2529%2520was%2520proposed%2520to%2520enhance%2520model%2520generalization%2520and%2520achieved%250Astate-of-the-art%2520performance.%2520SAM%2520consists%2520of%2520two%2520main%2520steps%252C%2520the%2520weight%250Aperturbation%2520step%2520and%2520the%2520weight%2520updating%2520step.%2520However%252C%2520the%2520perturbation%2520in%250ASAM%2520is%2520determined%2520by%2520only%2520the%2520gradient%2520of%2520the%2520training%2520loss%252C%2520or%2520cross-entropy%250Aloss.%2520As%2520the%2520model%2520approaches%2520a%2520stationary%2520point%252C%2520this%2520gradient%2520becomes%2520small%250Aand%2520oscillates%252C%2520leading%2520to%2520inconsistent%2520perturbation%2520directions%2520and%2520also%2520has%2520a%250Achance%2520of%2520diminishing%2520the%2520gradient.%2520Our%2520research%2520introduces%2520an%2520innovative%250Aapproach%2520to%2520further%2520enhancing%2520model%2520generalization.%2520We%2520propose%2520the%2520Adaptive%250AAdversarial%2520Cross-Entropy%2520%2528AACE%2529%2520loss%2520function%2520to%2520replace%2520standard%250Across-entropy%2520loss%2520for%2520SAM%2527s%2520perturbation.%2520AACE%2520loss%2520and%2520its%2520gradient%2520uniquely%250Aincrease%2520as%2520the%2520model%2520nears%2520convergence%252C%2520ensuring%2520consistent%2520perturbation%250Adirection%2520and%2520addressing%2520the%2520gradient%2520diminishing%2520issue.%2520Additionally%252C%2520a%2520novel%250Aperturbation-generating%2520function%2520utilizing%2520AACE%2520loss%2520without%2520normalization%2520is%250Aproposed%252C%2520enhancing%2520the%2520model%2527s%2520exploratory%2520capabilities%2520in%2520near-optimum%250Astages.%2520Empirical%2520testing%2520confirms%2520the%2520effectiveness%2520of%2520AACE%252C%2520with%2520experiments%250Ademonstrating%2520improved%2520performance%2520in%2520image%2520classification%2520tasks%2520using%2520Wide%250AResNet%2520and%2520PyramidNet%2520across%2520various%2520datasets.%2520The%2520reproduction%2520code%2520is%250Aavailable%2520online%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2406.14329v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Adaptive%20Adversarial%20Cross-Entropy%20Loss%20for%20Sharpness-Aware%20Minimization&entry.906535625=Tanapat%20Ratchatorn%20and%20Masayuki%20Tanaka&entry.1292438233=%20%20Recent%20advancements%20in%20learning%20algorithms%20have%20demonstrated%20that%20the%0Asharpness%20of%20the%20loss%20surface%20is%20an%20effective%20measure%20for%20improving%20the%0Ageneralization%20gap.%20Building%20upon%20this%20concept%2C%20Sharpness-Aware%20Minimization%0A%28SAM%29%20was%20proposed%20to%20enhance%20model%20generalization%20and%20achieved%0Astate-of-the-art%20performance.%20SAM%20consists%20of%20two%20main%20steps%2C%20the%20weight%0Aperturbation%20step%20and%20the%20weight%20updating%20step.%20However%2C%20the%20perturbation%20in%0ASAM%20is%20determined%20by%20only%20the%20gradient%20of%20the%20training%20loss%2C%20or%20cross-entropy%0Aloss.%20As%20the%20model%20approaches%20a%20stationary%20point%2C%20this%20gradient%20becomes%20small%0Aand%20oscillates%2C%20leading%20to%20inconsistent%20perturbation%20directions%20and%20also%20has%20a%0Achance%20of%20diminishing%20the%20gradient.%20Our%20research%20introduces%20an%20innovative%0Aapproach%20to%20further%20enhancing%20model%20generalization.%20We%20propose%20the%20Adaptive%0AAdversarial%20Cross-Entropy%20%28AACE%29%20loss%20function%20to%20replace%20standard%0Across-entropy%20loss%20for%20SAM%27s%20perturbation.%20AACE%20loss%20and%20its%20gradient%20uniquely%0Aincrease%20as%20the%20model%20nears%20convergence%2C%20ensuring%20consistent%20perturbation%0Adirection%20and%20addressing%20the%20gradient%20diminishing%20issue.%20Additionally%2C%20a%20novel%0Aperturbation-generating%20function%20utilizing%20AACE%20loss%20without%20normalization%20is%0Aproposed%2C%20enhancing%20the%20model%27s%20exploratory%20capabilities%20in%20near-optimum%0Astages.%20Empirical%20testing%20confirms%20the%20effectiveness%20of%20AACE%2C%20with%20experiments%0Ademonstrating%20improved%20performance%20in%20image%20classification%20tasks%20using%20Wide%0AResNet%20and%20PyramidNet%20across%20various%20datasets.%20The%20reproduction%20code%20is%0Aavailable%20online%0A&entry.1838667208=http%3A//arxiv.org/abs/2406.14329v1&entry.124074799=Read"},
{"title": "Epicardium Prompt-guided Real-time Cardiac Ultrasound Frame-to-volume\n  Registration", "author": "Long Lei and Jun Zhou and Jialun Pei and Baoliang Zhao and Yueming Jin and Yuen-Chun Jeremy Teoh and Jing Qin and Pheng-Ann Heng", "abstract": "  A comprehensive guidance view for cardiac interventional surgery can be\nprovided by the real-time fusion of the intraoperative 2D images and\npreoperative 3D volume based on the ultrasound frame-to-volume registration.\nHowever, cardiac ultrasound images are characterized by a low signal-to-noise\nratio and small differences between adjacent frames, coupled with significant\ndimension variations between 2D frames and 3D volumes to be registered,\nresulting in real-time and accurate cardiac ultrasound frame-to-volume\nregistration being a very challenging task. This paper introduces a lightweight\nend-to-end Cardiac Ultrasound frame-to-volume Registration network, termed\nCU-Reg. Specifically, the proposed model leverages epicardium prompt-guided\nanatomical clues to reinforce the interaction of 2D sparse and 3D dense\nfeatures, followed by a voxel-wise local-global aggregation of enhanced\nfeatures, thereby boosting the cross-dimensional matching effectiveness of\nlow-quality ultrasound modalities. We further embed an inter-frame\ndiscriminative regularization term within the hybrid supervised learning to\nincrease the distinction between adjacent slices in the same ultrasound volume\nto ensure registration stability. Experimental results on the reprocessed CAMUS\ndataset demonstrate that our CU-Reg surpasses existing methods in terms of\nregistration accuracy and efficiency, meeting the guidance requirements of\nclinical cardiac interventional surgery.\n", "link": "http://arxiv.org/abs/2406.14534v1", "date": "2024-06-20", "relevancy": 2.5422, "topK": [{"title": "DDM-NET: End-to-end learning of keypoint feature Detection, Description and Matching for 3D localization", "link": "https://arxiv.org/abs/2212.04575", "similarity": 0.5219}, {"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.5071}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.4963}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Epicardium%20Prompt-guided%20Real-time%20Cardiac%20Ultrasound%20Frame-to-volume%0A%20%20Registration&body=Title%3A%20Epicardium%20Prompt-guided%20Real-time%20Cardiac%20Ultrasound%20Frame-to-volume%0A%20%20Registration%0AAuthor%3A%20Long%20Lei%20and%20Jun%20Zhou%20and%20Jialun%20Pei%20and%20Baoliang%20Zhao%20and%20Yueming%20Jin%20and%20Yuen-Chun%20Jeremy%20Teoh%20and%20Jing%20Qin%20and%20Pheng-Ann%20Heng%0AAbstract%3A%20%20%20A%20comprehensive%20guidance%20view%20for%20cardiac%20interventional%20surgery%20can%20be%0Aprovided%20by%20the%20real-time%20fusion%20of%20the%20intraoperative%202D%20images%20and%0Apreoperative%203D%20volume%20based%20on%20the%20ultrasound%20frame-to-volume%20registration.%0AHowever%2C%20cardiac%20ultrasound%20images%20are%20characterized%20by%20a%20low%20signal-to-noise%0Aratio%20and%20small%20differences%20between%20adjacent%20frames%2C%20coupled%20with%20significant%0Adimension%20variations%20between%202D%20frames%20and%203D%20volumes%20to%20be%20registered%2C%0Aresulting%20in%20real-time%20and%20accurate%20cardiac%20ultrasound%20frame-to-volume%0Aregistration%20being%20a%20very%20challenging%20task.%20This%20paper%20introduces%20a%20lightweight%0Aend-to-end%20Cardiac%20Ultrasound%20frame-to-volume%20Registration%20network%2C%20termed%0ACU-Reg.%20Specifically%2C%20the%20proposed%20model%20leverages%20epicardium%20prompt-guided%0Aanatomical%20clues%20to%20reinforce%20the%20interaction%20of%202D%20sparse%20and%203D%20dense%0Afeatures%2C%20followed%20by%20a%20voxel-wise%20local-global%20aggregation%20of%20enhanced%0Afeatures%2C%20thereby%20boosting%20the%20cross-dimensional%20matching%20effectiveness%20of%0Alow-quality%20ultrasound%20modalities.%20We%20further%20embed%20an%20inter-frame%0Adiscriminative%20regularization%20term%20within%20the%20hybrid%20supervised%20learning%20to%0Aincrease%20the%20distinction%20between%20adjacent%20slices%20in%20the%20same%20ultrasound%20volume%0Ato%20ensure%20registration%20stability.%20Experimental%20results%20on%20the%20reprocessed%20CAMUS%0Adataset%20demonstrate%20that%20our%20CU-Reg%20surpasses%20existing%20methods%20in%20terms%20of%0Aregistration%20accuracy%20and%20efficiency%2C%20meeting%20the%20guidance%20requirements%20of%0Aclinical%20cardiac%20interventional%20surgery.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2406.14534v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DEpicardium%2520Prompt-guided%2520Real-time%2520Cardiac%2520Ultrasound%2520Frame-to-volume%250A%2520%2520Registration%26entry.906535625%3DLong%2520Lei%2520and%2520Jun%2520Zhou%2520and%2520Jialun%2520Pei%2520and%2520Baoliang%2520Zhao%2520and%2520Yueming%2520Jin%2520and%2520Yuen-Chun%2520Jeremy%2520Teoh%2520and%2520Jing%2520Qin%2520and%2520Pheng-Ann%2520Heng%26entry.1292438233%3D%2520%2520A%2520comprehensive%2520guidance%2520view%2520for%2520cardiac%2520interventional%2520surgery%2520can%2520be%250Aprovided%2520by%2520the%2520real-time%2520fusion%2520of%2520the%2520intraoperative%25202D%2520images%2520and%250Apreoperative%25203D%2520volume%2520based%2520on%2520the%2520ultrasound%2520frame-to-volume%2520registration.%250AHowever%252C%2520cardiac%2520ultrasound%2520images%2520are%2520characterized%2520by%2520a%2520low%2520signal-to-noise%250Aratio%2520and%2520small%2520differences%2520between%2520adjacent%2520frames%252C%2520coupled%2520with%2520significant%250Adimension%2520variations%2520between%25202D%2520frames%2520and%25203D%2520volumes%2520to%2520be%2520registered%252C%250Aresulting%2520in%2520real-time%2520and%2520accurate%2520cardiac%2520ultrasound%2520frame-to-volume%250Aregistration%2520being%2520a%2520very%2520challenging%2520task.%2520This%2520paper%2520introduces%2520a%2520lightweight%250Aend-to-end%2520Cardiac%2520Ultrasound%2520frame-to-volume%2520Registration%2520network%252C%2520termed%250ACU-Reg.%2520Specifically%252C%2520the%2520proposed%2520model%2520leverages%2520epicardium%2520prompt-guided%250Aanatomical%2520clues%2520to%2520reinforce%2520the%2520interaction%2520of%25202D%2520sparse%2520and%25203D%2520dense%250Afeatures%252C%2520followed%2520by%2520a%2520voxel-wise%2520local-global%2520aggregation%2520of%2520enhanced%250Afeatures%252C%2520thereby%2520boosting%2520the%2520cross-dimensional%2520matching%2520effectiveness%2520of%250Alow-quality%2520ultrasound%2520modalities.%2520We%2520further%2520embed%2520an%2520inter-frame%250Adiscriminative%2520regularization%2520term%2520within%2520the%2520hybrid%2520supervised%2520learning%2520to%250Aincrease%2520the%2520distinction%2520between%2520adjacent%2520slices%2520in%2520the%2520same%2520ultrasound%2520volume%250Ato%2520ensure%2520registration%2520stability.%2520Experimental%2520results%2520on%2520the%2520reprocessed%2520CAMUS%250Adataset%2520demonstrate%2520that%2520our%2520CU-Reg%2520surpasses%2520existing%2520methods%2520in%2520terms%2520of%250Aregistration%2520accuracy%2520and%2520efficiency%252C%2520meeting%2520the%2520guidance%2520requirements%2520of%250Aclinical%2520cardiac%2520interventional%2520surgery.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2406.14534v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Epicardium%20Prompt-guided%20Real-time%20Cardiac%20Ultrasound%20Frame-to-volume%0A%20%20Registration&entry.906535625=Long%20Lei%20and%20Jun%20Zhou%20and%20Jialun%20Pei%20and%20Baoliang%20Zhao%20and%20Yueming%20Jin%20and%20Yuen-Chun%20Jeremy%20Teoh%20and%20Jing%20Qin%20and%20Pheng-Ann%20Heng&entry.1292438233=%20%20A%20comprehensive%20guidance%20view%20for%20cardiac%20interventional%20surgery%20can%20be%0Aprovided%20by%20the%20real-time%20fusion%20of%20the%20intraoperative%202D%20images%20and%0Apreoperative%203D%20volume%20based%20on%20the%20ultrasound%20frame-to-volume%20registration.%0AHowever%2C%20cardiac%20ultrasound%20images%20are%20characterized%20by%20a%20low%20signal-to-noise%0Aratio%20and%20small%20differences%20between%20adjacent%20frames%2C%20coupled%20with%20significant%0Adimension%20variations%20between%202D%20frames%20and%203D%20volumes%20to%20be%20registered%2C%0Aresulting%20in%20real-time%20and%20accurate%20cardiac%20ultrasound%20frame-to-volume%0Aregistration%20being%20a%20very%20challenging%20task.%20This%20paper%20introduces%20a%20lightweight%0Aend-to-end%20Cardiac%20Ultrasound%20frame-to-volume%20Registration%20network%2C%20termed%0ACU-Reg.%20Specifically%2C%20the%20proposed%20model%20leverages%20epicardium%20prompt-guided%0Aanatomical%20clues%20to%20reinforce%20the%20interaction%20of%202D%20sparse%20and%203D%20dense%0Afeatures%2C%20followed%20by%20a%20voxel-wise%20local-global%20aggregation%20of%20enhanced%0Afeatures%2C%20thereby%20boosting%20the%20cross-dimensional%20matching%20effectiveness%20of%0Alow-quality%20ultrasound%20modalities.%20We%20further%20embed%20an%20inter-frame%0Adiscriminative%20regularization%20term%20within%20the%20hybrid%20supervised%20learning%20to%0Aincrease%20the%20distinction%20between%20adjacent%20slices%20in%20the%20same%20ultrasound%20volume%0Ato%20ensure%20registration%20stability.%20Experimental%20results%20on%20the%20reprocessed%20CAMUS%0Adataset%20demonstrate%20that%20our%20CU-Reg%20surpasses%20existing%20methods%20in%20terms%20of%0Aregistration%20accuracy%20and%20efficiency%2C%20meeting%20the%20guidance%20requirements%20of%0Aclinical%20cardiac%20interventional%20surgery.%0A&entry.1838667208=http%3A//arxiv.org/abs/2406.14534v1&entry.124074799=Read"},
{"title": "Capturing Temporal Components for Time Series Classification", "author": "Venkata Ragavendra Vavilthota and Ranjith Ramanathan and Sathyanarayanan N. Aakur", "abstract": "  Analyzing sequential data is crucial in many domains, particularly due to the\nabundance of data collected from the Internet of Things paradigm. Time series\nclassification, the task of categorizing sequential data, has gained\nprominence, with machine learning approaches demonstrating remarkable\nperformance on public benchmark datasets. However, progress has primarily been\nin designing architectures for learning representations from raw data at fixed\n(or ideal) time scales, which can fail to generalize to longer sequences. This\nwork introduces a \\textit{compositional representation learning} approach\ntrained on statistically coherent components extracted from sequential data.\nBased on a multi-scale change space, an unsupervised approach is proposed to\nsegment the sequential data into chunks with similar statistical properties. A\nsequence-based encoder model is trained in a multi-task setting to learn\ncompositional representations from these temporal components for time series\nclassification. We demonstrate its effectiveness through extensive experiments\non publicly available time series classification benchmarks. Evaluating the\ncoherence of segmented components shows its competitive performance on the\nunsupervised segmentation task.\n", "link": "http://arxiv.org/abs/2406.14456v1", "date": "2024-06-20", "relevancy": 2.5132, "topK": [{"title": "Weakly-guided self-supervised pretraining for temporal activity detection", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/25189", "similarity": 0.5282}, {"title": "Breadcrumbs: Adversarial class-balanced sampling for long-tailed recognition", "link": "https://link.springer.com/chapter/10.1007/978-3-031-20053-3_37", "similarity": 0.4899}, {"title": "Self-supervised Photographic Image Layout Representation Learning", "link": "http://arxiv.org/abs/2403.03740v1", "similarity": 0.4898}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Capturing%20Temporal%20Components%20for%20Time%20Series%20Classification&body=Title%3A%20Capturing%20Temporal%20Components%20for%20Time%20Series%20Classification%0AAuthor%3A%20Venkata%20Ragavendra%20Vavilthota%20and%20Ranjith%20Ramanathan%20and%20Sathyanarayanan%20N.%20Aakur%0AAbstract%3A%20%20%20Analyzing%20sequential%20data%20is%20crucial%20in%20many%20domains%2C%20particularly%20due%20to%20the%0Aabundance%20of%20data%20collected%20from%20the%20Internet%20of%20Things%20paradigm.%20Time%20series%0Aclassification%2C%20the%20task%20of%20categorizing%20sequential%20data%2C%20has%20gained%0Aprominence%2C%20with%20machine%20learning%20approaches%20demonstrating%20remarkable%0Aperformance%20on%20public%20benchmark%20datasets.%20However%2C%20progress%20has%20primarily%20been%0Ain%20designing%20architectures%20for%20learning%20representations%20from%20raw%20data%20at%20fixed%0A%28or%20ideal%29%20time%20scales%2C%20which%20can%20fail%20to%20generalize%20to%20longer%20sequences.%20This%0Awork%20introduces%20a%20%5Ctextit%7Bcompositional%20representation%20learning%7D%20approach%0Atrained%20on%20statistically%20coherent%20components%20extracted%20from%20sequential%20data.%0ABased%20on%20a%20multi-scale%20change%20space%2C%20an%20unsupervised%20approach%20is%20proposed%20to%0Asegment%20the%20sequential%20data%20into%20chunks%20with%20similar%20statistical%20properties.%20A%0Asequence-based%20encoder%20model%20is%20trained%20in%20a%20multi-task%20setting%20to%20learn%0Acompositional%20representations%20from%20these%20temporal%20components%20for%20time%20series%0Aclassification.%20We%20demonstrate%20its%20effectiveness%20through%20extensive%20experiments%0Aon%20publicly%20available%20time%20series%20classification%20benchmarks.%20Evaluating%20the%0Acoherence%20of%20segmented%20components%20shows%20its%20competitive%20performance%20on%20the%0Aunsupervised%20segmentation%20task.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2406.14456v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DCapturing%2520Temporal%2520Components%2520for%2520Time%2520Series%2520Classification%26entry.906535625%3DVenkata%2520Ragavendra%2520Vavilthota%2520and%2520Ranjith%2520Ramanathan%2520and%2520Sathyanarayanan%2520N.%2520Aakur%26entry.1292438233%3D%2520%2520Analyzing%2520sequential%2520data%2520is%2520crucial%2520in%2520many%2520domains%252C%2520particularly%2520due%2520to%2520the%250Aabundance%2520of%2520data%2520collected%2520from%2520the%2520Internet%2520of%2520Things%2520paradigm.%2520Time%2520series%250Aclassification%252C%2520the%2520task%2520of%2520categorizing%2520sequential%2520data%252C%2520has%2520gained%250Aprominence%252C%2520with%2520machine%2520learning%2520approaches%2520demonstrating%2520remarkable%250Aperformance%2520on%2520public%2520benchmark%2520datasets.%2520However%252C%2520progress%2520has%2520primarily%2520been%250Ain%2520designing%2520architectures%2520for%2520learning%2520representations%2520from%2520raw%2520data%2520at%2520fixed%250A%2528or%2520ideal%2529%2520time%2520scales%252C%2520which%2520can%2520fail%2520to%2520generalize%2520to%2520longer%2520sequences.%2520This%250Awork%2520introduces%2520a%2520%255Ctextit%257Bcompositional%2520representation%2520learning%257D%2520approach%250Atrained%2520on%2520statistically%2520coherent%2520components%2520extracted%2520from%2520sequential%2520data.%250ABased%2520on%2520a%2520multi-scale%2520change%2520space%252C%2520an%2520unsupervised%2520approach%2520is%2520proposed%2520to%250Asegment%2520the%2520sequential%2520data%2520into%2520chunks%2520with%2520similar%2520statistical%2520properties.%2520A%250Asequence-based%2520encoder%2520model%2520is%2520trained%2520in%2520a%2520multi-task%2520setting%2520to%2520learn%250Acompositional%2520representations%2520from%2520these%2520temporal%2520components%2520for%2520time%2520series%250Aclassification.%2520We%2520demonstrate%2520its%2520effectiveness%2520through%2520extensive%2520experiments%250Aon%2520publicly%2520available%2520time%2520series%2520classification%2520benchmarks.%2520Evaluating%2520the%250Acoherence%2520of%2520segmented%2520components%2520shows%2520its%2520competitive%2520performance%2520on%2520the%250Aunsupervised%2520segmentation%2520task.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2406.14456v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Capturing%20Temporal%20Components%20for%20Time%20Series%20Classification&entry.906535625=Venkata%20Ragavendra%20Vavilthota%20and%20Ranjith%20Ramanathan%20and%20Sathyanarayanan%20N.%20Aakur&entry.1292438233=%20%20Analyzing%20sequential%20data%20is%20crucial%20in%20many%20domains%2C%20particularly%20due%20to%20the%0Aabundance%20of%20data%20collected%20from%20the%20Internet%20of%20Things%20paradigm.%20Time%20series%0Aclassification%2C%20the%20task%20of%20categorizing%20sequential%20data%2C%20has%20gained%0Aprominence%2C%20with%20machine%20learning%20approaches%20demonstrating%20remarkable%0Aperformance%20on%20public%20benchmark%20datasets.%20However%2C%20progress%20has%20primarily%20been%0Ain%20designing%20architectures%20for%20learning%20representations%20from%20raw%20data%20at%20fixed%0A%28or%20ideal%29%20time%20scales%2C%20which%20can%20fail%20to%20generalize%20to%20longer%20sequences.%20This%0Awork%20introduces%20a%20%5Ctextit%7Bcompositional%20representation%20learning%7D%20approach%0Atrained%20on%20statistically%20coherent%20components%20extracted%20from%20sequential%20data.%0ABased%20on%20a%20multi-scale%20change%20space%2C%20an%20unsupervised%20approach%20is%20proposed%20to%0Asegment%20the%20sequential%20data%20into%20chunks%20with%20similar%20statistical%20properties.%20A%0Asequence-based%20encoder%20model%20is%20trained%20in%20a%20multi-task%20setting%20to%20learn%0Acompositional%20representations%20from%20these%20temporal%20components%20for%20time%20series%0Aclassification.%20We%20demonstrate%20its%20effectiveness%20through%20extensive%20experiments%0Aon%20publicly%20available%20time%20series%20classification%20benchmarks.%20Evaluating%20the%0Acoherence%20of%20segmented%20components%20shows%20its%20competitive%20performance%20on%20the%0Aunsupervised%20segmentation%20task.%0A&entry.1838667208=http%3A//arxiv.org/abs/2406.14456v1&entry.124074799=Read"},
{"title": "V-LASIK: Consistent Glasses-Removal from Videos Using Synthetic Data", "author": "Rotem Shalev-Arkushin and Aharon Azulay and Tavi Halperin and Eitan Richardson and Amit H. Bermano and Ohad Fried", "abstract": "  Diffusion-based generative models have recently shown remarkable image and\nvideo editing capabilities. However, local video editing, particularly removal\nof small attributes like glasses, remains a challenge. Existing methods either\nalter the videos excessively, generate unrealistic artifacts, or fail to\nperform the requested edit consistently throughout the video. In this work, we\nfocus on consistent and identity-preserving removal of glasses in videos, using\nit as a case study for consistent local attribute removal in videos. Due to the\nlack of paired data, we adopt a weakly supervised approach and generate\nsynthetic imperfect data, using an adjusted pretrained diffusion model. We show\nthat despite data imperfection, by learning from our generated data and\nleveraging the prior of pretrained diffusion models, our model is able to\nperform the desired edit consistently while preserving the original video\ncontent. Furthermore, we exemplify the generalization ability of our method to\nother local video editing tasks by applying it successfully to facial\nsticker-removal. Our approach demonstrates significant improvement over\nexisting methods, showcasing the potential of leveraging synthetic data and\nstrong video priors for local video editing tasks.\n", "link": "http://arxiv.org/abs/2406.14510v1", "date": "2024-06-20", "relevancy": 2.5088, "topK": [{"title": "Magic-Me: Identity-Specific Video Customized Diffusion", "link": "http://arxiv.org/abs/2402.09368v1", "similarity": 0.6518}, {"title": "Customize-A-Video: One-Shot Motion Customization of Text-to-Video\n  Diffusion Models", "link": "http://arxiv.org/abs/2402.14780v1", "similarity": 0.6345}, {"title": "Tuning-Free Noise Rectification for High Fidelity Image-to-Video\n  Generation", "link": "http://arxiv.org/abs/2403.02827v1", "similarity": 0.5998}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20V-LASIK%3A%20Consistent%20Glasses-Removal%20from%20Videos%20Using%20Synthetic%20Data&body=Title%3A%20V-LASIK%3A%20Consistent%20Glasses-Removal%20from%20Videos%20Using%20Synthetic%20Data%0AAuthor%3A%20Rotem%20Shalev-Arkushin%20and%20Aharon%20Azulay%20and%20Tavi%20Halperin%20and%20Eitan%20Richardson%20and%20Amit%20H.%20Bermano%20and%20Ohad%20Fried%0AAbstract%3A%20%20%20Diffusion-based%20generative%20models%20have%20recently%20shown%20remarkable%20image%20and%0Avideo%20editing%20capabilities.%20However%2C%20local%20video%20editing%2C%20particularly%20removal%0Aof%20small%20attributes%20like%20glasses%2C%20remains%20a%20challenge.%20Existing%20methods%20either%0Aalter%20the%20videos%20excessively%2C%20generate%20unrealistic%20artifacts%2C%20or%20fail%20to%0Aperform%20the%20requested%20edit%20consistently%20throughout%20the%20video.%20In%20this%20work%2C%20we%0Afocus%20on%20consistent%20and%20identity-preserving%20removal%20of%20glasses%20in%20videos%2C%20using%0Ait%20as%20a%20case%20study%20for%20consistent%20local%20attribute%20removal%20in%20videos.%20Due%20to%20the%0Alack%20of%20paired%20data%2C%20we%20adopt%20a%20weakly%20supervised%20approach%20and%20generate%0Asynthetic%20imperfect%20data%2C%20using%20an%20adjusted%20pretrained%20diffusion%20model.%20We%20show%0Athat%20despite%20data%20imperfection%2C%20by%20learning%20from%20our%20generated%20data%20and%0Aleveraging%20the%20prior%20of%20pretrained%20diffusion%20models%2C%20our%20model%20is%20able%20to%0Aperform%20the%20desired%20edit%20consistently%20while%20preserving%20the%20original%20video%0Acontent.%20Furthermore%2C%20we%20exemplify%20the%20generalization%20ability%20of%20our%20method%20to%0Aother%20local%20video%20editing%20tasks%20by%20applying%20it%20successfully%20to%20facial%0Asticker-removal.%20Our%20approach%20demonstrates%20significant%20improvement%20over%0Aexisting%20methods%2C%20showcasing%20the%20potential%20of%20leveraging%20synthetic%20data%20and%0Astrong%20video%20priors%20for%20local%20video%20editing%20tasks.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2406.14510v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DV-LASIK%253A%2520Consistent%2520Glasses-Removal%2520from%2520Videos%2520Using%2520Synthetic%2520Data%26entry.906535625%3DRotem%2520Shalev-Arkushin%2520and%2520Aharon%2520Azulay%2520and%2520Tavi%2520Halperin%2520and%2520Eitan%2520Richardson%2520and%2520Amit%2520H.%2520Bermano%2520and%2520Ohad%2520Fried%26entry.1292438233%3D%2520%2520Diffusion-based%2520generative%2520models%2520have%2520recently%2520shown%2520remarkable%2520image%2520and%250Avideo%2520editing%2520capabilities.%2520However%252C%2520local%2520video%2520editing%252C%2520particularly%2520removal%250Aof%2520small%2520attributes%2520like%2520glasses%252C%2520remains%2520a%2520challenge.%2520Existing%2520methods%2520either%250Aalter%2520the%2520videos%2520excessively%252C%2520generate%2520unrealistic%2520artifacts%252C%2520or%2520fail%2520to%250Aperform%2520the%2520requested%2520edit%2520consistently%2520throughout%2520the%2520video.%2520In%2520this%2520work%252C%2520we%250Afocus%2520on%2520consistent%2520and%2520identity-preserving%2520removal%2520of%2520glasses%2520in%2520videos%252C%2520using%250Ait%2520as%2520a%2520case%2520study%2520for%2520consistent%2520local%2520attribute%2520removal%2520in%2520videos.%2520Due%2520to%2520the%250Alack%2520of%2520paired%2520data%252C%2520we%2520adopt%2520a%2520weakly%2520supervised%2520approach%2520and%2520generate%250Asynthetic%2520imperfect%2520data%252C%2520using%2520an%2520adjusted%2520pretrained%2520diffusion%2520model.%2520We%2520show%250Athat%2520despite%2520data%2520imperfection%252C%2520by%2520learning%2520from%2520our%2520generated%2520data%2520and%250Aleveraging%2520the%2520prior%2520of%2520pretrained%2520diffusion%2520models%252C%2520our%2520model%2520is%2520able%2520to%250Aperform%2520the%2520desired%2520edit%2520consistently%2520while%2520preserving%2520the%2520original%2520video%250Acontent.%2520Furthermore%252C%2520we%2520exemplify%2520the%2520generalization%2520ability%2520of%2520our%2520method%2520to%250Aother%2520local%2520video%2520editing%2520tasks%2520by%2520applying%2520it%2520successfully%2520to%2520facial%250Asticker-removal.%2520Our%2520approach%2520demonstrates%2520significant%2520improvement%2520over%250Aexisting%2520methods%252C%2520showcasing%2520the%2520potential%2520of%2520leveraging%2520synthetic%2520data%2520and%250Astrong%2520video%2520priors%2520for%2520local%2520video%2520editing%2520tasks.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2406.14510v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=V-LASIK%3A%20Consistent%20Glasses-Removal%20from%20Videos%20Using%20Synthetic%20Data&entry.906535625=Rotem%20Shalev-Arkushin%20and%20Aharon%20Azulay%20and%20Tavi%20Halperin%20and%20Eitan%20Richardson%20and%20Amit%20H.%20Bermano%20and%20Ohad%20Fried&entry.1292438233=%20%20Diffusion-based%20generative%20models%20have%20recently%20shown%20remarkable%20image%20and%0Avideo%20editing%20capabilities.%20However%2C%20local%20video%20editing%2C%20particularly%20removal%0Aof%20small%20attributes%20like%20glasses%2C%20remains%20a%20challenge.%20Existing%20methods%20either%0Aalter%20the%20videos%20excessively%2C%20generate%20unrealistic%20artifacts%2C%20or%20fail%20to%0Aperform%20the%20requested%20edit%20consistently%20throughout%20the%20video.%20In%20this%20work%2C%20we%0Afocus%20on%20consistent%20and%20identity-preserving%20removal%20of%20glasses%20in%20videos%2C%20using%0Ait%20as%20a%20case%20study%20for%20consistent%20local%20attribute%20removal%20in%20videos.%20Due%20to%20the%0Alack%20of%20paired%20data%2C%20we%20adopt%20a%20weakly%20supervised%20approach%20and%20generate%0Asynthetic%20imperfect%20data%2C%20using%20an%20adjusted%20pretrained%20diffusion%20model.%20We%20show%0Athat%20despite%20data%20imperfection%2C%20by%20learning%20from%20our%20generated%20data%20and%0Aleveraging%20the%20prior%20of%20pretrained%20diffusion%20models%2C%20our%20model%20is%20able%20to%0Aperform%20the%20desired%20edit%20consistently%20while%20preserving%20the%20original%20video%0Acontent.%20Furthermore%2C%20we%20exemplify%20the%20generalization%20ability%20of%20our%20method%20to%0Aother%20local%20video%20editing%20tasks%20by%20applying%20it%20successfully%20to%20facial%0Asticker-removal.%20Our%20approach%20demonstrates%20significant%20improvement%20over%0Aexisting%20methods%2C%20showcasing%20the%20potential%20of%20leveraging%20synthetic%20data%20and%0Astrong%20video%20priors%20for%20local%20video%20editing%20tasks.%0A&entry.1838667208=http%3A//arxiv.org/abs/2406.14510v1&entry.124074799=Read"},
{"title": "CoT-BERT: Enhancing Unsupervised Sentence Representation through\n  Chain-of-Thought", "author": "Bowen Zhang and Kehua Chang and Chunping Li", "abstract": "  Unsupervised sentence representation learning aims to transform input\nsentences into fixed-length vectors enriched with intricate semantic\ninformation while obviating the reliance on labeled data. Recent strides within\nthis domain have been significantly propelled by breakthroughs in contrastive\nlearning and prompt engineering. Despite these advancements, the field has\nreached a plateau, leading some researchers to incorporate external components\nto enhance the quality of sentence embeddings. Such integration, though\nbeneficial, complicates solutions and inflates demands for computational\nresources. In response to these challenges, this paper presents CoT-BERT, an\ninnovative method that harnesses the progressive thinking of Chain-of-Thought\nreasoning to tap into the latent potential of pre-trained models like BERT.\nAdditionally, we develop an advanced contrastive learning loss function and\npropose a novel template denoising strategy. Rigorous experimentation\ndemonstrates that CoT-BERT surpasses a range of well-established baselines by\nrelying exclusively on the intrinsic strengths of pre-trained models.\n", "link": "http://arxiv.org/abs/2309.11143v4", "date": "2024-06-20", "relevancy": 2.5064, "topK": [{"title": "Self-supervised Photographic Image Layout Representation Learning", "link": "http://arxiv.org/abs/2403.03740v1", "similarity": 0.5164}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5026}, {"title": "Genie: Generative Interactive Environments", "link": "http://arxiv.org/abs/2402.15391v1", "similarity": 0.4848}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20CoT-BERT%3A%20Enhancing%20Unsupervised%20Sentence%20Representation%20through%0A%20%20Chain-of-Thought&body=Title%3A%20CoT-BERT%3A%20Enhancing%20Unsupervised%20Sentence%20Representation%20through%0A%20%20Chain-of-Thought%0AAuthor%3A%20Bowen%20Zhang%20and%20Kehua%20Chang%20and%20Chunping%20Li%0AAbstract%3A%20%20%20Unsupervised%20sentence%20representation%20learning%20aims%20to%20transform%20input%0Asentences%20into%20fixed-length%20vectors%20enriched%20with%20intricate%20semantic%0Ainformation%20while%20obviating%20the%20reliance%20on%20labeled%20data.%20Recent%20strides%20within%0Athis%20domain%20have%20been%20significantly%20propelled%20by%20breakthroughs%20in%20contrastive%0Alearning%20and%20prompt%20engineering.%20Despite%20these%20advancements%2C%20the%20field%20has%0Areached%20a%20plateau%2C%20leading%20some%20researchers%20to%20incorporate%20external%20components%0Ato%20enhance%20the%20quality%20of%20sentence%20embeddings.%20Such%20integration%2C%20though%0Abeneficial%2C%20complicates%20solutions%20and%20inflates%20demands%20for%20computational%0Aresources.%20In%20response%20to%20these%20challenges%2C%20this%20paper%20presents%20CoT-BERT%2C%20an%0Ainnovative%20method%20that%20harnesses%20the%20progressive%20thinking%20of%20Chain-of-Thought%0Areasoning%20to%20tap%20into%20the%20latent%20potential%20of%20pre-trained%20models%20like%20BERT.%0AAdditionally%2C%20we%20develop%20an%20advanced%20contrastive%20learning%20loss%20function%20and%0Apropose%20a%20novel%20template%20denoising%20strategy.%20Rigorous%20experimentation%0Ademonstrates%20that%20CoT-BERT%20surpasses%20a%20range%20of%20well-established%20baselines%20by%0Arelying%20exclusively%20on%20the%20intrinsic%20strengths%20of%20pre-trained%20models.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2309.11143v4%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DCoT-BERT%253A%2520Enhancing%2520Unsupervised%2520Sentence%2520Representation%2520through%250A%2520%2520Chain-of-Thought%26entry.906535625%3DBowen%2520Zhang%2520and%2520Kehua%2520Chang%2520and%2520Chunping%2520Li%26entry.1292438233%3D%2520%2520Unsupervised%2520sentence%2520representation%2520learning%2520aims%2520to%2520transform%2520input%250Asentences%2520into%2520fixed-length%2520vectors%2520enriched%2520with%2520intricate%2520semantic%250Ainformation%2520while%2520obviating%2520the%2520reliance%2520on%2520labeled%2520data.%2520Recent%2520strides%2520within%250Athis%2520domain%2520have%2520been%2520significantly%2520propelled%2520by%2520breakthroughs%2520in%2520contrastive%250Alearning%2520and%2520prompt%2520engineering.%2520Despite%2520these%2520advancements%252C%2520the%2520field%2520has%250Areached%2520a%2520plateau%252C%2520leading%2520some%2520researchers%2520to%2520incorporate%2520external%2520components%250Ato%2520enhance%2520the%2520quality%2520of%2520sentence%2520embeddings.%2520Such%2520integration%252C%2520though%250Abeneficial%252C%2520complicates%2520solutions%2520and%2520inflates%2520demands%2520for%2520computational%250Aresources.%2520In%2520response%2520to%2520these%2520challenges%252C%2520this%2520paper%2520presents%2520CoT-BERT%252C%2520an%250Ainnovative%2520method%2520that%2520harnesses%2520the%2520progressive%2520thinking%2520of%2520Chain-of-Thought%250Areasoning%2520to%2520tap%2520into%2520the%2520latent%2520potential%2520of%2520pre-trained%2520models%2520like%2520BERT.%250AAdditionally%252C%2520we%2520develop%2520an%2520advanced%2520contrastive%2520learning%2520loss%2520function%2520and%250Apropose%2520a%2520novel%2520template%2520denoising%2520strategy.%2520Rigorous%2520experimentation%250Ademonstrates%2520that%2520CoT-BERT%2520surpasses%2520a%2520range%2520of%2520well-established%2520baselines%2520by%250Arelying%2520exclusively%2520on%2520the%2520intrinsic%2520strengths%2520of%2520pre-trained%2520models.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2309.11143v4%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=CoT-BERT%3A%20Enhancing%20Unsupervised%20Sentence%20Representation%20through%0A%20%20Chain-of-Thought&entry.906535625=Bowen%20Zhang%20and%20Kehua%20Chang%20and%20Chunping%20Li&entry.1292438233=%20%20Unsupervised%20sentence%20representation%20learning%20aims%20to%20transform%20input%0Asentences%20into%20fixed-length%20vectors%20enriched%20with%20intricate%20semantic%0Ainformation%20while%20obviating%20the%20reliance%20on%20labeled%20data.%20Recent%20strides%20within%0Athis%20domain%20have%20been%20significantly%20propelled%20by%20breakthroughs%20in%20contrastive%0Alearning%20and%20prompt%20engineering.%20Despite%20these%20advancements%2C%20the%20field%20has%0Areached%20a%20plateau%2C%20leading%20some%20researchers%20to%20incorporate%20external%20components%0Ato%20enhance%20the%20quality%20of%20sentence%20embeddings.%20Such%20integration%2C%20though%0Abeneficial%2C%20complicates%20solutions%20and%20inflates%20demands%20for%20computational%0Aresources.%20In%20response%20to%20these%20challenges%2C%20this%20paper%20presents%20CoT-BERT%2C%20an%0Ainnovative%20method%20that%20harnesses%20the%20progressive%20thinking%20of%20Chain-of-Thought%0Areasoning%20to%20tap%20into%20the%20latent%20potential%20of%20pre-trained%20models%20like%20BERT.%0AAdditionally%2C%20we%20develop%20an%20advanced%20contrastive%20learning%20loss%20function%20and%0Apropose%20a%20novel%20template%20denoising%20strategy.%20Rigorous%20experimentation%0Ademonstrates%20that%20CoT-BERT%20surpasses%20a%20range%20of%20well-established%20baselines%20by%0Arelying%20exclusively%20on%20the%20intrinsic%20strengths%20of%20pre-trained%20models.%0A&entry.1838667208=http%3A//arxiv.org/abs/2309.11143v4&entry.124074799=Read"},
{"title": "Active Few-Shot Fine-Tuning", "author": "Jonas H\u00fcbotter and Bhavya Sukhija and Lenart Treven and Yarden As and Andreas Krause", "abstract": "  We study the question: How can we select the right data for fine-tuning to a\nspecific task? We call this data selection problem active fine-tuning and show\nthat it is an instance of transductive active learning, a novel generalization\nof classical active learning. We propose ITL, short for information-based\ntransductive learning, an approach which samples adaptively to maximize\ninformation gained about the specified task. We are the first to show, under\ngeneral regularity assumptions, that such decision rules converge uniformly to\nthe smallest possible uncertainty obtainable from the accessible data. We apply\nITL to the few-shot fine-tuning of large neural networks and show that\nfine-tuning with ITL learns the task with significantly fewer examples than the\nstate-of-the-art.\n", "link": "http://arxiv.org/abs/2402.15441v3", "date": "2024-06-20", "relevancy": 2.4769, "topK": [{"title": "Boosted dynamic neural networks", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/26302", "similarity": 0.4996}, {"title": "ToDo: Token Downsampling for Efficient Generation of High-Resolution\n  Images", "link": "http://arxiv.org/abs/2402.13573v2", "similarity": 0.4965}, {"title": "Tuning-Free Noise Rectification for High Fidelity Image-to-Video\n  Generation", "link": "http://arxiv.org/abs/2403.02827v1", "similarity": 0.4901}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Active%20Few-Shot%20Fine-Tuning&body=Title%3A%20Active%20Few-Shot%20Fine-Tuning%0AAuthor%3A%20Jonas%20H%C3%BCbotter%20and%20Bhavya%20Sukhija%20and%20Lenart%20Treven%20and%20Yarden%20As%20and%20Andreas%20Krause%0AAbstract%3A%20%20%20We%20study%20the%20question%3A%20How%20can%20we%20select%20the%20right%20data%20for%20fine-tuning%20to%20a%0Aspecific%20task%3F%20We%20call%20this%20data%20selection%20problem%20active%20fine-tuning%20and%20show%0Athat%20it%20is%20an%20instance%20of%20transductive%20active%20learning%2C%20a%20novel%20generalization%0Aof%20classical%20active%20learning.%20We%20propose%20ITL%2C%20short%20for%20information-based%0Atransductive%20learning%2C%20an%20approach%20which%20samples%20adaptively%20to%20maximize%0Ainformation%20gained%20about%20the%20specified%20task.%20We%20are%20the%20first%20to%20show%2C%20under%0Ageneral%20regularity%20assumptions%2C%20that%20such%20decision%20rules%20converge%20uniformly%20to%0Athe%20smallest%20possible%20uncertainty%20obtainable%20from%20the%20accessible%20data.%20We%20apply%0AITL%20to%20the%20few-shot%20fine-tuning%20of%20large%20neural%20networks%20and%20show%20that%0Afine-tuning%20with%20ITL%20learns%20the%20task%20with%20significantly%20fewer%20examples%20than%20the%0Astate-of-the-art.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2402.15441v3%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DActive%2520Few-Shot%2520Fine-Tuning%26entry.906535625%3DJonas%2520H%25C3%25BCbotter%2520and%2520Bhavya%2520Sukhija%2520and%2520Lenart%2520Treven%2520and%2520Yarden%2520As%2520and%2520Andreas%2520Krause%26entry.1292438233%3D%2520%2520We%2520study%2520the%2520question%253A%2520How%2520can%2520we%2520select%2520the%2520right%2520data%2520for%2520fine-tuning%2520to%2520a%250Aspecific%2520task%253F%2520We%2520call%2520this%2520data%2520selection%2520problem%2520active%2520fine-tuning%2520and%2520show%250Athat%2520it%2520is%2520an%2520instance%2520of%2520transductive%2520active%2520learning%252C%2520a%2520novel%2520generalization%250Aof%2520classical%2520active%2520learning.%2520We%2520propose%2520ITL%252C%2520short%2520for%2520information-based%250Atransductive%2520learning%252C%2520an%2520approach%2520which%2520samples%2520adaptively%2520to%2520maximize%250Ainformation%2520gained%2520about%2520the%2520specified%2520task.%2520We%2520are%2520the%2520first%2520to%2520show%252C%2520under%250Ageneral%2520regularity%2520assumptions%252C%2520that%2520such%2520decision%2520rules%2520converge%2520uniformly%2520to%250Athe%2520smallest%2520possible%2520uncertainty%2520obtainable%2520from%2520the%2520accessible%2520data.%2520We%2520apply%250AITL%2520to%2520the%2520few-shot%2520fine-tuning%2520of%2520large%2520neural%2520networks%2520and%2520show%2520that%250Afine-tuning%2520with%2520ITL%2520learns%2520the%2520task%2520with%2520significantly%2520fewer%2520examples%2520than%2520the%250Astate-of-the-art.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2402.15441v3%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Active%20Few-Shot%20Fine-Tuning&entry.906535625=Jonas%20H%C3%BCbotter%20and%20Bhavya%20Sukhija%20and%20Lenart%20Treven%20and%20Yarden%20As%20and%20Andreas%20Krause&entry.1292438233=%20%20We%20study%20the%20question%3A%20How%20can%20we%20select%20the%20right%20data%20for%20fine-tuning%20to%20a%0Aspecific%20task%3F%20We%20call%20this%20data%20selection%20problem%20active%20fine-tuning%20and%20show%0Athat%20it%20is%20an%20instance%20of%20transductive%20active%20learning%2C%20a%20novel%20generalization%0Aof%20classical%20active%20learning.%20We%20propose%20ITL%2C%20short%20for%20information-based%0Atransductive%20learning%2C%20an%20approach%20which%20samples%20adaptively%20to%20maximize%0Ainformation%20gained%20about%20the%20specified%20task.%20We%20are%20the%20first%20to%20show%2C%20under%0Ageneral%20regularity%20assumptions%2C%20that%20such%20decision%20rules%20converge%20uniformly%20to%0Athe%20smallest%20possible%20uncertainty%20obtainable%20from%20the%20accessible%20data.%20We%20apply%0AITL%20to%20the%20few-shot%20fine-tuning%20of%20large%20neural%20networks%20and%20show%20that%0Afine-tuning%20with%20ITL%20learns%20the%20task%20with%20significantly%20fewer%20examples%20than%20the%0Astate-of-the-art.%0A&entry.1838667208=http%3A//arxiv.org/abs/2402.15441v3&entry.124074799=Read"},
{"title": "Contractive Systems Improve Graph Neural Networks Against Adversarial\n  Attacks", "author": "Moshe Eliasof and Davide Murari and Ferdia Sherry and Carola-Bibiane Sch\u00f6nlieb", "abstract": "  Graph Neural Networks (GNNs) have established themselves as a key component\nin addressing diverse graph-based tasks. Despite their notable successes, GNNs\nremain susceptible to input perturbations in the form of adversarial attacks.\nThis paper introduces an innovative approach to fortify GNNs against\nadversarial perturbations through the lens of contractive dynamical systems.\nOur method introduces graph neural layers based on differential equations with\ncontractive properties, which, as we show, improve the robustness of GNNs. A\ndistinctive feature of the proposed approach is the simultaneous learned\nevolution of both the node features and the adjacency matrix, yielding an\nintrinsic enhancement of model robustness to perturbations in the input\nfeatures and the connectivity of the graph. We mathematically derive the\nunderpinnings of our novel architecture and provide theoretical insights to\nreason about its expected behavior. We demonstrate the efficacy of our method\nthrough numerous real-world benchmarks, reading on par or improved performance\ncompared to existing methods.\n", "link": "http://arxiv.org/abs/2311.06942v2", "date": "2024-06-20", "relevancy": 2.4415, "topK": [{"title": "Boosted dynamic neural networks", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/26302", "similarity": 0.5321}, {"title": "Dynamic Gaussians Mesh: Consistent Mesh Reconstruction from Monocular\n  Videos", "link": "http://arxiv.org/abs/2404.12379v2", "similarity": 0.4763}, {"title": "UGG: Unified Generative Grasping", "link": "https://arxiv.org/abs/2311.16917", "similarity": 0.4565}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Contractive%20Systems%20Improve%20Graph%20Neural%20Networks%20Against%20Adversarial%0A%20%20Attacks&body=Title%3A%20Contractive%20Systems%20Improve%20Graph%20Neural%20Networks%20Against%20Adversarial%0A%20%20Attacks%0AAuthor%3A%20Moshe%20Eliasof%20and%20Davide%20Murari%20and%20Ferdia%20Sherry%20and%20Carola-Bibiane%20Sch%C3%B6nlieb%0AAbstract%3A%20%20%20Graph%20Neural%20Networks%20%28GNNs%29%20have%20established%20themselves%20as%20a%20key%20component%0Ain%20addressing%20diverse%20graph-based%20tasks.%20Despite%20their%20notable%20successes%2C%20GNNs%0Aremain%20susceptible%20to%20input%20perturbations%20in%20the%20form%20of%20adversarial%20attacks.%0AThis%20paper%20introduces%20an%20innovative%20approach%20to%20fortify%20GNNs%20against%0Aadversarial%20perturbations%20through%20the%20lens%20of%20contractive%20dynamical%20systems.%0AOur%20method%20introduces%20graph%20neural%20layers%20based%20on%20differential%20equations%20with%0Acontractive%20properties%2C%20which%2C%20as%20we%20show%2C%20improve%20the%20robustness%20of%20GNNs.%20A%0Adistinctive%20feature%20of%20the%20proposed%20approach%20is%20the%20simultaneous%20learned%0Aevolution%20of%20both%20the%20node%20features%20and%20the%20adjacency%20matrix%2C%20yielding%20an%0Aintrinsic%20enhancement%20of%20model%20robustness%20to%20perturbations%20in%20the%20input%0Afeatures%20and%20the%20connectivity%20of%20the%20graph.%20We%20mathematically%20derive%20the%0Aunderpinnings%20of%20our%20novel%20architecture%20and%20provide%20theoretical%20insights%20to%0Areason%20about%20its%20expected%20behavior.%20We%20demonstrate%20the%20efficacy%20of%20our%20method%0Athrough%20numerous%20real-world%20benchmarks%2C%20reading%20on%20par%20or%20improved%20performance%0Acompared%20to%20existing%20methods.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2311.06942v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DContractive%2520Systems%2520Improve%2520Graph%2520Neural%2520Networks%2520Against%2520Adversarial%250A%2520%2520Attacks%26entry.906535625%3DMoshe%2520Eliasof%2520and%2520Davide%2520Murari%2520and%2520Ferdia%2520Sherry%2520and%2520Carola-Bibiane%2520Sch%25C3%25B6nlieb%26entry.1292438233%3D%2520%2520Graph%2520Neural%2520Networks%2520%2528GNNs%2529%2520have%2520established%2520themselves%2520as%2520a%2520key%2520component%250Ain%2520addressing%2520diverse%2520graph-based%2520tasks.%2520Despite%2520their%2520notable%2520successes%252C%2520GNNs%250Aremain%2520susceptible%2520to%2520input%2520perturbations%2520in%2520the%2520form%2520of%2520adversarial%2520attacks.%250AThis%2520paper%2520introduces%2520an%2520innovative%2520approach%2520to%2520fortify%2520GNNs%2520against%250Aadversarial%2520perturbations%2520through%2520the%2520lens%2520of%2520contractive%2520dynamical%2520systems.%250AOur%2520method%2520introduces%2520graph%2520neural%2520layers%2520based%2520on%2520differential%2520equations%2520with%250Acontractive%2520properties%252C%2520which%252C%2520as%2520we%2520show%252C%2520improve%2520the%2520robustness%2520of%2520GNNs.%2520A%250Adistinctive%2520feature%2520of%2520the%2520proposed%2520approach%2520is%2520the%2520simultaneous%2520learned%250Aevolution%2520of%2520both%2520the%2520node%2520features%2520and%2520the%2520adjacency%2520matrix%252C%2520yielding%2520an%250Aintrinsic%2520enhancement%2520of%2520model%2520robustness%2520to%2520perturbations%2520in%2520the%2520input%250Afeatures%2520and%2520the%2520connectivity%2520of%2520the%2520graph.%2520We%2520mathematically%2520derive%2520the%250Aunderpinnings%2520of%2520our%2520novel%2520architecture%2520and%2520provide%2520theoretical%2520insights%2520to%250Areason%2520about%2520its%2520expected%2520behavior.%2520We%2520demonstrate%2520the%2520efficacy%2520of%2520our%2520method%250Athrough%2520numerous%2520real-world%2520benchmarks%252C%2520reading%2520on%2520par%2520or%2520improved%2520performance%250Acompared%2520to%2520existing%2520methods.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2311.06942v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Contractive%20Systems%20Improve%20Graph%20Neural%20Networks%20Against%20Adversarial%0A%20%20Attacks&entry.906535625=Moshe%20Eliasof%20and%20Davide%20Murari%20and%20Ferdia%20Sherry%20and%20Carola-Bibiane%20Sch%C3%B6nlieb&entry.1292438233=%20%20Graph%20Neural%20Networks%20%28GNNs%29%20have%20established%20themselves%20as%20a%20key%20component%0Ain%20addressing%20diverse%20graph-based%20tasks.%20Despite%20their%20notable%20successes%2C%20GNNs%0Aremain%20susceptible%20to%20input%20perturbations%20in%20the%20form%20of%20adversarial%20attacks.%0AThis%20paper%20introduces%20an%20innovative%20approach%20to%20fortify%20GNNs%20against%0Aadversarial%20perturbations%20through%20the%20lens%20of%20contractive%20dynamical%20systems.%0AOur%20method%20introduces%20graph%20neural%20layers%20based%20on%20differential%20equations%20with%0Acontractive%20properties%2C%20which%2C%20as%20we%20show%2C%20improve%20the%20robustness%20of%20GNNs.%20A%0Adistinctive%20feature%20of%20the%20proposed%20approach%20is%20the%20simultaneous%20learned%0Aevolution%20of%20both%20the%20node%20features%20and%20the%20adjacency%20matrix%2C%20yielding%20an%0Aintrinsic%20enhancement%20of%20model%20robustness%20to%20perturbations%20in%20the%20input%0Afeatures%20and%20the%20connectivity%20of%20the%20graph.%20We%20mathematically%20derive%20the%0Aunderpinnings%20of%20our%20novel%20architecture%20and%20provide%20theoretical%20insights%20to%0Areason%20about%20its%20expected%20behavior.%20We%20demonstrate%20the%20efficacy%20of%20our%20method%0Athrough%20numerous%20real-world%20benchmarks%2C%20reading%20on%20par%20or%20improved%20performance%0Acompared%20to%20existing%20methods.%0A&entry.1838667208=http%3A//arxiv.org/abs/2311.06942v2&entry.124074799=Read"},
{"title": "Revisiting Modularity Maximization for Graph Clustering: A Contrastive\n  Learning Perspective", "author": "Yunfei Liu and Jintang Li and Yuehe Chen and Ruofan Wu and Ericbk Wang and Jing Zhou and Sheng Tian and Shuheng Shen and Xing Fu and Changhua Meng and Weiqiang Wang and Liang Chen", "abstract": "  Graph clustering, a fundamental and challenging task in graph mining, aims to\nclassify nodes in a graph into several disjoint clusters. In recent years,\ngraph contrastive learning (GCL) has emerged as a dominant line of research in\ngraph clustering and advances the new state-of-the-art. However, GCL-based\nmethods heavily rely on graph augmentations and contrastive schemes, which may\npotentially introduce challenges such as semantic drift and scalability issues.\nAnother promising line of research involves the adoption of modularity\nmaximization, a popular and effective measure for community detection, as the\nguiding principle for clustering tasks. Despite the recent progress, the\nunderlying mechanism of modularity maximization is still not well understood.\nIn this work, we dig into the hidden success of modularity maximization for\ngraph clustering. Our analysis reveals the strong connections between\nmodularity maximization and graph contrastive learning, where positive and\nnegative examples are naturally defined by modularity. In light of our results,\nwe propose a community-aware graph clustering framework, coined MAGI, which\nleverages modularity maximization as a contrastive pretext task to effectively\nuncover the underlying information of communities in graphs, while avoiding the\nproblem of semantic drift. Extensive experiments on multiple graph datasets\nverify the effectiveness of MAGI in terms of scalability and clustering\nperformance compared to state-of-the-art graph clustering methods. Notably,\nMAGI easily scales a sufficiently large graph with 100M nodes while\noutperforming strong baselines.\n", "link": "http://arxiv.org/abs/2406.14288v1", "date": "2024-06-20", "relevancy": 2.3762, "topK": [{"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.4787}, {"title": "Self-supervised Photographic Image Layout Representation Learning", "link": "http://arxiv.org/abs/2403.03740v1", "similarity": 0.4767}, {"title": "UGG: Unified Generative Grasping", "link": "https://arxiv.org/abs/2311.16917", "similarity": 0.4703}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Revisiting%20Modularity%20Maximization%20for%20Graph%20Clustering%3A%20A%20Contrastive%0A%20%20Learning%20Perspective&body=Title%3A%20Revisiting%20Modularity%20Maximization%20for%20Graph%20Clustering%3A%20A%20Contrastive%0A%20%20Learning%20Perspective%0AAuthor%3A%20Yunfei%20Liu%20and%20Jintang%20Li%20and%20Yuehe%20Chen%20and%20Ruofan%20Wu%20and%20Ericbk%20Wang%20and%20Jing%20Zhou%20and%20Sheng%20Tian%20and%20Shuheng%20Shen%20and%20Xing%20Fu%20and%20Changhua%20Meng%20and%20Weiqiang%20Wang%20and%20Liang%20Chen%0AAbstract%3A%20%20%20Graph%20clustering%2C%20a%20fundamental%20and%20challenging%20task%20in%20graph%20mining%2C%20aims%20to%0Aclassify%20nodes%20in%20a%20graph%20into%20several%20disjoint%20clusters.%20In%20recent%20years%2C%0Agraph%20contrastive%20learning%20%28GCL%29%20has%20emerged%20as%20a%20dominant%20line%20of%20research%20in%0Agraph%20clustering%20and%20advances%20the%20new%20state-of-the-art.%20However%2C%20GCL-based%0Amethods%20heavily%20rely%20on%20graph%20augmentations%20and%20contrastive%20schemes%2C%20which%20may%0Apotentially%20introduce%20challenges%20such%20as%20semantic%20drift%20and%20scalability%20issues.%0AAnother%20promising%20line%20of%20research%20involves%20the%20adoption%20of%20modularity%0Amaximization%2C%20a%20popular%20and%20effective%20measure%20for%20community%20detection%2C%20as%20the%0Aguiding%20principle%20for%20clustering%20tasks.%20Despite%20the%20recent%20progress%2C%20the%0Aunderlying%20mechanism%20of%20modularity%20maximization%20is%20still%20not%20well%20understood.%0AIn%20this%20work%2C%20we%20dig%20into%20the%20hidden%20success%20of%20modularity%20maximization%20for%0Agraph%20clustering.%20Our%20analysis%20reveals%20the%20strong%20connections%20between%0Amodularity%20maximization%20and%20graph%20contrastive%20learning%2C%20where%20positive%20and%0Anegative%20examples%20are%20naturally%20defined%20by%20modularity.%20In%20light%20of%20our%20results%2C%0Awe%20propose%20a%20community-aware%20graph%20clustering%20framework%2C%20coined%20MAGI%2C%20which%0Aleverages%20modularity%20maximization%20as%20a%20contrastive%20pretext%20task%20to%20effectively%0Auncover%20the%20underlying%20information%20of%20communities%20in%20graphs%2C%20while%20avoiding%20the%0Aproblem%20of%20semantic%20drift.%20Extensive%20experiments%20on%20multiple%20graph%20datasets%0Averify%20the%20effectiveness%20of%20MAGI%20in%20terms%20of%20scalability%20and%20clustering%0Aperformance%20compared%20to%20state-of-the-art%20graph%20clustering%20methods.%20Notably%2C%0AMAGI%20easily%20scales%20a%20sufficiently%20large%20graph%20with%20100M%20nodes%20while%0Aoutperforming%20strong%20baselines.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2406.14288v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DRevisiting%2520Modularity%2520Maximization%2520for%2520Graph%2520Clustering%253A%2520A%2520Contrastive%250A%2520%2520Learning%2520Perspective%26entry.906535625%3DYunfei%2520Liu%2520and%2520Jintang%2520Li%2520and%2520Yuehe%2520Chen%2520and%2520Ruofan%2520Wu%2520and%2520Ericbk%2520Wang%2520and%2520Jing%2520Zhou%2520and%2520Sheng%2520Tian%2520and%2520Shuheng%2520Shen%2520and%2520Xing%2520Fu%2520and%2520Changhua%2520Meng%2520and%2520Weiqiang%2520Wang%2520and%2520Liang%2520Chen%26entry.1292438233%3D%2520%2520Graph%2520clustering%252C%2520a%2520fundamental%2520and%2520challenging%2520task%2520in%2520graph%2520mining%252C%2520aims%2520to%250Aclassify%2520nodes%2520in%2520a%2520graph%2520into%2520several%2520disjoint%2520clusters.%2520In%2520recent%2520years%252C%250Agraph%2520contrastive%2520learning%2520%2528GCL%2529%2520has%2520emerged%2520as%2520a%2520dominant%2520line%2520of%2520research%2520in%250Agraph%2520clustering%2520and%2520advances%2520the%2520new%2520state-of-the-art.%2520However%252C%2520GCL-based%250Amethods%2520heavily%2520rely%2520on%2520graph%2520augmentations%2520and%2520contrastive%2520schemes%252C%2520which%2520may%250Apotentially%2520introduce%2520challenges%2520such%2520as%2520semantic%2520drift%2520and%2520scalability%2520issues.%250AAnother%2520promising%2520line%2520of%2520research%2520involves%2520the%2520adoption%2520of%2520modularity%250Amaximization%252C%2520a%2520popular%2520and%2520effective%2520measure%2520for%2520community%2520detection%252C%2520as%2520the%250Aguiding%2520principle%2520for%2520clustering%2520tasks.%2520Despite%2520the%2520recent%2520progress%252C%2520the%250Aunderlying%2520mechanism%2520of%2520modularity%2520maximization%2520is%2520still%2520not%2520well%2520understood.%250AIn%2520this%2520work%252C%2520we%2520dig%2520into%2520the%2520hidden%2520success%2520of%2520modularity%2520maximization%2520for%250Agraph%2520clustering.%2520Our%2520analysis%2520reveals%2520the%2520strong%2520connections%2520between%250Amodularity%2520maximization%2520and%2520graph%2520contrastive%2520learning%252C%2520where%2520positive%2520and%250Anegative%2520examples%2520are%2520naturally%2520defined%2520by%2520modularity.%2520In%2520light%2520of%2520our%2520results%252C%250Awe%2520propose%2520a%2520community-aware%2520graph%2520clustering%2520framework%252C%2520coined%2520MAGI%252C%2520which%250Aleverages%2520modularity%2520maximization%2520as%2520a%2520contrastive%2520pretext%2520task%2520to%2520effectively%250Auncover%2520the%2520underlying%2520information%2520of%2520communities%2520in%2520graphs%252C%2520while%2520avoiding%2520the%250Aproblem%2520of%2520semantic%2520drift.%2520Extensive%2520experiments%2520on%2520multiple%2520graph%2520datasets%250Averify%2520the%2520effectiveness%2520of%2520MAGI%2520in%2520terms%2520of%2520scalability%2520and%2520clustering%250Aperformance%2520compared%2520to%2520state-of-the-art%2520graph%2520clustering%2520methods.%2520Notably%252C%250AMAGI%2520easily%2520scales%2520a%2520sufficiently%2520large%2520graph%2520with%2520100M%2520nodes%2520while%250Aoutperforming%2520strong%2520baselines.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2406.14288v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Revisiting%20Modularity%20Maximization%20for%20Graph%20Clustering%3A%20A%20Contrastive%0A%20%20Learning%20Perspective&entry.906535625=Yunfei%20Liu%20and%20Jintang%20Li%20and%20Yuehe%20Chen%20and%20Ruofan%20Wu%20and%20Ericbk%20Wang%20and%20Jing%20Zhou%20and%20Sheng%20Tian%20and%20Shuheng%20Shen%20and%20Xing%20Fu%20and%20Changhua%20Meng%20and%20Weiqiang%20Wang%20and%20Liang%20Chen&entry.1292438233=%20%20Graph%20clustering%2C%20a%20fundamental%20and%20challenging%20task%20in%20graph%20mining%2C%20aims%20to%0Aclassify%20nodes%20in%20a%20graph%20into%20several%20disjoint%20clusters.%20In%20recent%20years%2C%0Agraph%20contrastive%20learning%20%28GCL%29%20has%20emerged%20as%20a%20dominant%20line%20of%20research%20in%0Agraph%20clustering%20and%20advances%20the%20new%20state-of-the-art.%20However%2C%20GCL-based%0Amethods%20heavily%20rely%20on%20graph%20augmentations%20and%20contrastive%20schemes%2C%20which%20may%0Apotentially%20introduce%20challenges%20such%20as%20semantic%20drift%20and%20scalability%20issues.%0AAnother%20promising%20line%20of%20research%20involves%20the%20adoption%20of%20modularity%0Amaximization%2C%20a%20popular%20and%20effective%20measure%20for%20community%20detection%2C%20as%20the%0Aguiding%20principle%20for%20clustering%20tasks.%20Despite%20the%20recent%20progress%2C%20the%0Aunderlying%20mechanism%20of%20modularity%20maximization%20is%20still%20not%20well%20understood.%0AIn%20this%20work%2C%20we%20dig%20into%20the%20hidden%20success%20of%20modularity%20maximization%20for%0Agraph%20clustering.%20Our%20analysis%20reveals%20the%20strong%20connections%20between%0Amodularity%20maximization%20and%20graph%20contrastive%20learning%2C%20where%20positive%20and%0Anegative%20examples%20are%20naturally%20defined%20by%20modularity.%20In%20light%20of%20our%20results%2C%0Awe%20propose%20a%20community-aware%20graph%20clustering%20framework%2C%20coined%20MAGI%2C%20which%0Aleverages%20modularity%20maximization%20as%20a%20contrastive%20pretext%20task%20to%20effectively%0Auncover%20the%20underlying%20information%20of%20communities%20in%20graphs%2C%20while%20avoiding%20the%0Aproblem%20of%20semantic%20drift.%20Extensive%20experiments%20on%20multiple%20graph%20datasets%0Averify%20the%20effectiveness%20of%20MAGI%20in%20terms%20of%20scalability%20and%20clustering%0Aperformance%20compared%20to%20state-of-the-art%20graph%20clustering%20methods.%20Notably%2C%0AMAGI%20easily%20scales%20a%20sufficiently%20large%20graph%20with%20100M%20nodes%20while%0Aoutperforming%20strong%20baselines.%0A&entry.1838667208=http%3A//arxiv.org/abs/2406.14288v1&entry.124074799=Read"},
{"title": "RobMOT: Robust 3D Multi-Object Tracking by Observational Noise and State\n  Estimation Drift Mitigation on LiDAR PointCloud", "author": "Mohamed Nagy and Naoufel Werghi and Bilal Hassan and Jorge Dias and Majid Khonji", "abstract": "  This work addresses limitations in recent 3D tracking-by-detection methods,\nfocusing on identifying legitimate trajectories and addressing state estimation\ndrift in Kalman filters. Current methods rely heavily on threshold-based\nfiltering of false positive detections using detection scores to prevent ghost\ntrajectories. However, this approach is inadequate for distant and partially\noccluded objects, where detection scores tend to drop, potentially leading to\nfalse positives exceeding the threshold. Additionally, the literature generally\ntreats detections as precise localizations of objects. Our research reveals\nthat noise in detections impacts localization information, causing trajectory\ndrift for occluded objects and hindering recovery. To this end, we propose a\nnovel online track validity mechanism that temporally distinguishes between\nlegitimate and ghost tracks, along with a multi-stage observational gating\nprocess for incoming observations. This mechanism significantly improves\ntracking performance, with a $6.28\\%$ in HOTA and a $17.87\\%$ increase in MOTA.\nWe also introduce a refinement to the Kalman filter that enhances noise\nmitigation in trajectory drift, leading to more robust state estimation for\noccluded objects. Our framework, RobMOT, outperforms state-of-the-art methods,\nincluding deep learning approaches, across various detectors, achieving up to a\n$4\\%$ margin in HOTA and $6\\%$ in MOTA. RobMOT excels under challenging\nconditions, such as prolonged occlusions and tracking distant objects, with up\nto a 59\\% improvement in processing latency.\n", "link": "http://arxiv.org/abs/2405.11536v2", "date": "2024-06-20", "relevancy": 2.3577, "topK": [{"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.6091}, {"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.603}, {"title": "DDM-NET: End-to-end learning of keypoint feature Detection, Description and Matching for 3D localization", "link": "https://arxiv.org/abs/2212.04575", "similarity": 0.5679}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20RobMOT%3A%20Robust%203D%20Multi-Object%20Tracking%20by%20Observational%20Noise%20and%20State%0A%20%20Estimation%20Drift%20Mitigation%20on%20LiDAR%20PointCloud&body=Title%3A%20RobMOT%3A%20Robust%203D%20Multi-Object%20Tracking%20by%20Observational%20Noise%20and%20State%0A%20%20Estimation%20Drift%20Mitigation%20on%20LiDAR%20PointCloud%0AAuthor%3A%20Mohamed%20Nagy%20and%20Naoufel%20Werghi%20and%20Bilal%20Hassan%20and%20Jorge%20Dias%20and%20Majid%20Khonji%0AAbstract%3A%20%20%20This%20work%20addresses%20limitations%20in%20recent%203D%20tracking-by-detection%20methods%2C%0Afocusing%20on%20identifying%20legitimate%20trajectories%20and%20addressing%20state%20estimation%0Adrift%20in%20Kalman%20filters.%20Current%20methods%20rely%20heavily%20on%20threshold-based%0Afiltering%20of%20false%20positive%20detections%20using%20detection%20scores%20to%20prevent%20ghost%0Atrajectories.%20However%2C%20this%20approach%20is%20inadequate%20for%20distant%20and%20partially%0Aoccluded%20objects%2C%20where%20detection%20scores%20tend%20to%20drop%2C%20potentially%20leading%20to%0Afalse%20positives%20exceeding%20the%20threshold.%20Additionally%2C%20the%20literature%20generally%0Atreats%20detections%20as%20precise%20localizations%20of%20objects.%20Our%20research%20reveals%0Athat%20noise%20in%20detections%20impacts%20localization%20information%2C%20causing%20trajectory%0Adrift%20for%20occluded%20objects%20and%20hindering%20recovery.%20To%20this%20end%2C%20we%20propose%20a%0Anovel%20online%20track%20validity%20mechanism%20that%20temporally%20distinguishes%20between%0Alegitimate%20and%20ghost%20tracks%2C%20along%20with%20a%20multi-stage%20observational%20gating%0Aprocess%20for%20incoming%20observations.%20This%20mechanism%20significantly%20improves%0Atracking%20performance%2C%20with%20a%20%246.28%5C%25%24%20in%20HOTA%20and%20a%20%2417.87%5C%25%24%20increase%20in%20MOTA.%0AWe%20also%20introduce%20a%20refinement%20to%20the%20Kalman%20filter%20that%20enhances%20noise%0Amitigation%20in%20trajectory%20drift%2C%20leading%20to%20more%20robust%20state%20estimation%20for%0Aoccluded%20objects.%20Our%20framework%2C%20RobMOT%2C%20outperforms%20state-of-the-art%20methods%2C%0Aincluding%20deep%20learning%20approaches%2C%20across%20various%20detectors%2C%20achieving%20up%20to%20a%0A%244%5C%25%24%20margin%20in%20HOTA%20and%20%246%5C%25%24%20in%20MOTA.%20RobMOT%20excels%20under%20challenging%0Aconditions%2C%20such%20as%20prolonged%20occlusions%20and%20tracking%20distant%20objects%2C%20with%20up%0Ato%20a%2059%5C%25%20improvement%20in%20processing%20latency.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2405.11536v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DRobMOT%253A%2520Robust%25203D%2520Multi-Object%2520Tracking%2520by%2520Observational%2520Noise%2520and%2520State%250A%2520%2520Estimation%2520Drift%2520Mitigation%2520on%2520LiDAR%2520PointCloud%26entry.906535625%3DMohamed%2520Nagy%2520and%2520Naoufel%2520Werghi%2520and%2520Bilal%2520Hassan%2520and%2520Jorge%2520Dias%2520and%2520Majid%2520Khonji%26entry.1292438233%3D%2520%2520This%2520work%2520addresses%2520limitations%2520in%2520recent%25203D%2520tracking-by-detection%2520methods%252C%250Afocusing%2520on%2520identifying%2520legitimate%2520trajectories%2520and%2520addressing%2520state%2520estimation%250Adrift%2520in%2520Kalman%2520filters.%2520Current%2520methods%2520rely%2520heavily%2520on%2520threshold-based%250Afiltering%2520of%2520false%2520positive%2520detections%2520using%2520detection%2520scores%2520to%2520prevent%2520ghost%250Atrajectories.%2520However%252C%2520this%2520approach%2520is%2520inadequate%2520for%2520distant%2520and%2520partially%250Aoccluded%2520objects%252C%2520where%2520detection%2520scores%2520tend%2520to%2520drop%252C%2520potentially%2520leading%2520to%250Afalse%2520positives%2520exceeding%2520the%2520threshold.%2520Additionally%252C%2520the%2520literature%2520generally%250Atreats%2520detections%2520as%2520precise%2520localizations%2520of%2520objects.%2520Our%2520research%2520reveals%250Athat%2520noise%2520in%2520detections%2520impacts%2520localization%2520information%252C%2520causing%2520trajectory%250Adrift%2520for%2520occluded%2520objects%2520and%2520hindering%2520recovery.%2520To%2520this%2520end%252C%2520we%2520propose%2520a%250Anovel%2520online%2520track%2520validity%2520mechanism%2520that%2520temporally%2520distinguishes%2520between%250Alegitimate%2520and%2520ghost%2520tracks%252C%2520along%2520with%2520a%2520multi-stage%2520observational%2520gating%250Aprocess%2520for%2520incoming%2520observations.%2520This%2520mechanism%2520significantly%2520improves%250Atracking%2520performance%252C%2520with%2520a%2520%25246.28%255C%2525%2524%2520in%2520HOTA%2520and%2520a%2520%252417.87%255C%2525%2524%2520increase%2520in%2520MOTA.%250AWe%2520also%2520introduce%2520a%2520refinement%2520to%2520the%2520Kalman%2520filter%2520that%2520enhances%2520noise%250Amitigation%2520in%2520trajectory%2520drift%252C%2520leading%2520to%2520more%2520robust%2520state%2520estimation%2520for%250Aoccluded%2520objects.%2520Our%2520framework%252C%2520RobMOT%252C%2520outperforms%2520state-of-the-art%2520methods%252C%250Aincluding%2520deep%2520learning%2520approaches%252C%2520across%2520various%2520detectors%252C%2520achieving%2520up%2520to%2520a%250A%25244%255C%2525%2524%2520margin%2520in%2520HOTA%2520and%2520%25246%255C%2525%2524%2520in%2520MOTA.%2520RobMOT%2520excels%2520under%2520challenging%250Aconditions%252C%2520such%2520as%2520prolonged%2520occlusions%2520and%2520tracking%2520distant%2520objects%252C%2520with%2520up%250Ato%2520a%252059%255C%2525%2520improvement%2520in%2520processing%2520latency.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2405.11536v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=RobMOT%3A%20Robust%203D%20Multi-Object%20Tracking%20by%20Observational%20Noise%20and%20State%0A%20%20Estimation%20Drift%20Mitigation%20on%20LiDAR%20PointCloud&entry.906535625=Mohamed%20Nagy%20and%20Naoufel%20Werghi%20and%20Bilal%20Hassan%20and%20Jorge%20Dias%20and%20Majid%20Khonji&entry.1292438233=%20%20This%20work%20addresses%20limitations%20in%20recent%203D%20tracking-by-detection%20methods%2C%0Afocusing%20on%20identifying%20legitimate%20trajectories%20and%20addressing%20state%20estimation%0Adrift%20in%20Kalman%20filters.%20Current%20methods%20rely%20heavily%20on%20threshold-based%0Afiltering%20of%20false%20positive%20detections%20using%20detection%20scores%20to%20prevent%20ghost%0Atrajectories.%20However%2C%20this%20approach%20is%20inadequate%20for%20distant%20and%20partially%0Aoccluded%20objects%2C%20where%20detection%20scores%20tend%20to%20drop%2C%20potentially%20leading%20to%0Afalse%20positives%20exceeding%20the%20threshold.%20Additionally%2C%20the%20literature%20generally%0Atreats%20detections%20as%20precise%20localizations%20of%20objects.%20Our%20research%20reveals%0Athat%20noise%20in%20detections%20impacts%20localization%20information%2C%20causing%20trajectory%0Adrift%20for%20occluded%20objects%20and%20hindering%20recovery.%20To%20this%20end%2C%20we%20propose%20a%0Anovel%20online%20track%20validity%20mechanism%20that%20temporally%20distinguishes%20between%0Alegitimate%20and%20ghost%20tracks%2C%20along%20with%20a%20multi-stage%20observational%20gating%0Aprocess%20for%20incoming%20observations.%20This%20mechanism%20significantly%20improves%0Atracking%20performance%2C%20with%20a%20%246.28%5C%25%24%20in%20HOTA%20and%20a%20%2417.87%5C%25%24%20increase%20in%20MOTA.%0AWe%20also%20introduce%20a%20refinement%20to%20the%20Kalman%20filter%20that%20enhances%20noise%0Amitigation%20in%20trajectory%20drift%2C%20leading%20to%20more%20robust%20state%20estimation%20for%0Aoccluded%20objects.%20Our%20framework%2C%20RobMOT%2C%20outperforms%20state-of-the-art%20methods%2C%0Aincluding%20deep%20learning%20approaches%2C%20across%20various%20detectors%2C%20achieving%20up%20to%20a%0A%244%5C%25%24%20margin%20in%20HOTA%20and%20%246%5C%25%24%20in%20MOTA.%20RobMOT%20excels%20under%20challenging%0Aconditions%2C%20such%20as%20prolonged%20occlusions%20and%20tracking%20distant%20objects%2C%20with%20up%0Ato%20a%2059%5C%25%20improvement%20in%20processing%20latency.%0A&entry.1838667208=http%3A//arxiv.org/abs/2405.11536v2&entry.124074799=Read"},
{"title": "Video Generation with Learned Action Prior", "author": "Meenakshi Sarkar and Devansh Bhardwaj and Debasish Ghose", "abstract": "  Stochastic video generation is particularly challenging when the camera is\nmounted on a moving platform, as camera motion interacts with observed image\npixels, creating complex spatio-temporal dynamics and making the problem\npartially observable. Existing methods typically address this by focusing on\nraw pixel-level image reconstruction without explicitly modelling camera motion\ndynamics. We propose a solution by considering camera motion or action as part\nof the observed image state, modelling both image and action within a\nmulti-modal learning framework. We introduce three models: Video Generation\nwith Learning Action Prior (VG-LeAP) treats the image-action pair as an\naugmented state generated from a single latent stochastic process and uses\nvariational inference to learn the image-action latent prior; Causal-LeAP,\nwhich establishes a causal relationship between action and the observed image\nframe at time $t$, learning an action prior conditioned on the observed image\nstates; and RAFI, which integrates the augmented image-action state concept\ninto flow matching with diffusion generative processes, demonstrating that this\naction-conditioned image generation concept can be extended to other\ndiffusion-based models. We emphasize the importance of multi-modal training in\npartially observable video generation problems through detailed empirical\nstudies on our new video action dataset, RoAM.\n", "link": "http://arxiv.org/abs/2406.14436v1", "date": "2024-06-20", "relevancy": 2.3498, "topK": [{"title": "Customize-A-Video: One-Shot Motion Customization of Text-to-Video\n  Diffusion Models", "link": "http://arxiv.org/abs/2402.14780v1", "similarity": 0.6044}, {"title": "Genie: Generative Interactive Environments", "link": "http://arxiv.org/abs/2402.15391v1", "similarity": 0.6009}, {"title": "Magic-Me: Identity-Specific Video Customized Diffusion", "link": "http://arxiv.org/abs/2402.09368v1", "similarity": 0.5672}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Video%20Generation%20with%20Learned%20Action%20Prior&body=Title%3A%20Video%20Generation%20with%20Learned%20Action%20Prior%0AAuthor%3A%20Meenakshi%20Sarkar%20and%20Devansh%20Bhardwaj%20and%20Debasish%20Ghose%0AAbstract%3A%20%20%20Stochastic%20video%20generation%20is%20particularly%20challenging%20when%20the%20camera%20is%0Amounted%20on%20a%20moving%20platform%2C%20as%20camera%20motion%20interacts%20with%20observed%20image%0Apixels%2C%20creating%20complex%20spatio-temporal%20dynamics%20and%20making%20the%20problem%0Apartially%20observable.%20Existing%20methods%20typically%20address%20this%20by%20focusing%20on%0Araw%20pixel-level%20image%20reconstruction%20without%20explicitly%20modelling%20camera%20motion%0Adynamics.%20We%20propose%20a%20solution%20by%20considering%20camera%20motion%20or%20action%20as%20part%0Aof%20the%20observed%20image%20state%2C%20modelling%20both%20image%20and%20action%20within%20a%0Amulti-modal%20learning%20framework.%20We%20introduce%20three%20models%3A%20Video%20Generation%0Awith%20Learning%20Action%20Prior%20%28VG-LeAP%29%20treats%20the%20image-action%20pair%20as%20an%0Aaugmented%20state%20generated%20from%20a%20single%20latent%20stochastic%20process%20and%20uses%0Avariational%20inference%20to%20learn%20the%20image-action%20latent%20prior%3B%20Causal-LeAP%2C%0Awhich%20establishes%20a%20causal%20relationship%20between%20action%20and%20the%20observed%20image%0Aframe%20at%20time%20%24t%24%2C%20learning%20an%20action%20prior%20conditioned%20on%20the%20observed%20image%0Astates%3B%20and%20RAFI%2C%20which%20integrates%20the%20augmented%20image-action%20state%20concept%0Ainto%20flow%20matching%20with%20diffusion%20generative%20processes%2C%20demonstrating%20that%20this%0Aaction-conditioned%20image%20generation%20concept%20can%20be%20extended%20to%20other%0Adiffusion-based%20models.%20We%20emphasize%20the%20importance%20of%20multi-modal%20training%20in%0Apartially%20observable%20video%20generation%20problems%20through%20detailed%20empirical%0Astudies%20on%20our%20new%20video%20action%20dataset%2C%20RoAM.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2406.14436v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DVideo%2520Generation%2520with%2520Learned%2520Action%2520Prior%26entry.906535625%3DMeenakshi%2520Sarkar%2520and%2520Devansh%2520Bhardwaj%2520and%2520Debasish%2520Ghose%26entry.1292438233%3D%2520%2520Stochastic%2520video%2520generation%2520is%2520particularly%2520challenging%2520when%2520the%2520camera%2520is%250Amounted%2520on%2520a%2520moving%2520platform%252C%2520as%2520camera%2520motion%2520interacts%2520with%2520observed%2520image%250Apixels%252C%2520creating%2520complex%2520spatio-temporal%2520dynamics%2520and%2520making%2520the%2520problem%250Apartially%2520observable.%2520Existing%2520methods%2520typically%2520address%2520this%2520by%2520focusing%2520on%250Araw%2520pixel-level%2520image%2520reconstruction%2520without%2520explicitly%2520modelling%2520camera%2520motion%250Adynamics.%2520We%2520propose%2520a%2520solution%2520by%2520considering%2520camera%2520motion%2520or%2520action%2520as%2520part%250Aof%2520the%2520observed%2520image%2520state%252C%2520modelling%2520both%2520image%2520and%2520action%2520within%2520a%250Amulti-modal%2520learning%2520framework.%2520We%2520introduce%2520three%2520models%253A%2520Video%2520Generation%250Awith%2520Learning%2520Action%2520Prior%2520%2528VG-LeAP%2529%2520treats%2520the%2520image-action%2520pair%2520as%2520an%250Aaugmented%2520state%2520generated%2520from%2520a%2520single%2520latent%2520stochastic%2520process%2520and%2520uses%250Avariational%2520inference%2520to%2520learn%2520the%2520image-action%2520latent%2520prior%253B%2520Causal-LeAP%252C%250Awhich%2520establishes%2520a%2520causal%2520relationship%2520between%2520action%2520and%2520the%2520observed%2520image%250Aframe%2520at%2520time%2520%2524t%2524%252C%2520learning%2520an%2520action%2520prior%2520conditioned%2520on%2520the%2520observed%2520image%250Astates%253B%2520and%2520RAFI%252C%2520which%2520integrates%2520the%2520augmented%2520image-action%2520state%2520concept%250Ainto%2520flow%2520matching%2520with%2520diffusion%2520generative%2520processes%252C%2520demonstrating%2520that%2520this%250Aaction-conditioned%2520image%2520generation%2520concept%2520can%2520be%2520extended%2520to%2520other%250Adiffusion-based%2520models.%2520We%2520emphasize%2520the%2520importance%2520of%2520multi-modal%2520training%2520in%250Apartially%2520observable%2520video%2520generation%2520problems%2520through%2520detailed%2520empirical%250Astudies%2520on%2520our%2520new%2520video%2520action%2520dataset%252C%2520RoAM.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2406.14436v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Video%20Generation%20with%20Learned%20Action%20Prior&entry.906535625=Meenakshi%20Sarkar%20and%20Devansh%20Bhardwaj%20and%20Debasish%20Ghose&entry.1292438233=%20%20Stochastic%20video%20generation%20is%20particularly%20challenging%20when%20the%20camera%20is%0Amounted%20on%20a%20moving%20platform%2C%20as%20camera%20motion%20interacts%20with%20observed%20image%0Apixels%2C%20creating%20complex%20spatio-temporal%20dynamics%20and%20making%20the%20problem%0Apartially%20observable.%20Existing%20methods%20typically%20address%20this%20by%20focusing%20on%0Araw%20pixel-level%20image%20reconstruction%20without%20explicitly%20modelling%20camera%20motion%0Adynamics.%20We%20propose%20a%20solution%20by%20considering%20camera%20motion%20or%20action%20as%20part%0Aof%20the%20observed%20image%20state%2C%20modelling%20both%20image%20and%20action%20within%20a%0Amulti-modal%20learning%20framework.%20We%20introduce%20three%20models%3A%20Video%20Generation%0Awith%20Learning%20Action%20Prior%20%28VG-LeAP%29%20treats%20the%20image-action%20pair%20as%20an%0Aaugmented%20state%20generated%20from%20a%20single%20latent%20stochastic%20process%20and%20uses%0Avariational%20inference%20to%20learn%20the%20image-action%20latent%20prior%3B%20Causal-LeAP%2C%0Awhich%20establishes%20a%20causal%20relationship%20between%20action%20and%20the%20observed%20image%0Aframe%20at%20time%20%24t%24%2C%20learning%20an%20action%20prior%20conditioned%20on%20the%20observed%20image%0Astates%3B%20and%20RAFI%2C%20which%20integrates%20the%20augmented%20image-action%20state%20concept%0Ainto%20flow%20matching%20with%20diffusion%20generative%20processes%2C%20demonstrating%20that%20this%0Aaction-conditioned%20image%20generation%20concept%20can%20be%20extended%20to%20other%0Adiffusion-based%20models.%20We%20emphasize%20the%20importance%20of%20multi-modal%20training%20in%0Apartially%20observable%20video%20generation%20problems%20through%20detailed%20empirical%0Astudies%20on%20our%20new%20video%20action%20dataset%2C%20RoAM.%0A&entry.1838667208=http%3A//arxiv.org/abs/2406.14436v1&entry.124074799=Read"},
{"title": "STimage-1K4M: A histopathology image-gene expression dataset for spatial\n  transcriptomics", "author": "Jiawen Chen and Muqing Zhou and Wenrong Wu and Jinwei Zhang and Yun Li and Didong Li", "abstract": "  Recent advances in multi-modal algorithms have driven and been driven by the\nincreasing availability of large image-text datasets, leading to significant\nstrides in various fields, including computational pathology. However, in most\nexisting medical image-text datasets, the text typically provides high-level\nsummaries that may not sufficiently describe sub-tile regions within a large\npathology image. For example, an image might cover an extensive tissue area\ncontaining cancerous and healthy regions, but the accompanying text might only\nspecify that this image is a cancer slide, lacking the nuanced details needed\nfor in-depth analysis. In this study, we introduce STimage-1K4M, a novel\ndataset designed to bridge this gap by providing genomic features for sub-tile\nimages. STimage-1K4M contains 1,149 images derived from spatial transcriptomics\ndata, which captures gene expression information at the level of individual\nspatial spots within a pathology image. Specifically, each image in the dataset\nis broken down into smaller sub-image tiles, with each tile paired with\n15,000-30,000 dimensional gene expressions. With 4,293,195 pairs of sub-tile\nimages and gene expressions, STimage-1K4M offers unprecedented granularity,\npaving the way for a wide range of advanced research in multi-modal data\nanalysis an innovative applications in computational pathology, and beyond.\n", "link": "http://arxiv.org/abs/2406.06393v2", "date": "2024-06-20", "relevancy": 2.3435, "topK": [{"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.479}, {"title": "CAT3D: Create Anything in 3D with Multi-View Diffusion Models", "link": "http://arxiv.org/abs/2405.10314v1", "similarity": 0.4636}, {"title": "CAT3D: Create Anything in 3D with Multi-View Diffusion Models", "link": "http://arxiv.org/abs/2405.10314v1", "similarity": 0.4636}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20STimage-1K4M%3A%20A%20histopathology%20image-gene%20expression%20dataset%20for%20spatial%0A%20%20transcriptomics&body=Title%3A%20STimage-1K4M%3A%20A%20histopathology%20image-gene%20expression%20dataset%20for%20spatial%0A%20%20transcriptomics%0AAuthor%3A%20Jiawen%20Chen%20and%20Muqing%20Zhou%20and%20Wenrong%20Wu%20and%20Jinwei%20Zhang%20and%20Yun%20Li%20and%20Didong%20Li%0AAbstract%3A%20%20%20Recent%20advances%20in%20multi-modal%20algorithms%20have%20driven%20and%20been%20driven%20by%20the%0Aincreasing%20availability%20of%20large%20image-text%20datasets%2C%20leading%20to%20significant%0Astrides%20in%20various%20fields%2C%20including%20computational%20pathology.%20However%2C%20in%20most%0Aexisting%20medical%20image-text%20datasets%2C%20the%20text%20typically%20provides%20high-level%0Asummaries%20that%20may%20not%20sufficiently%20describe%20sub-tile%20regions%20within%20a%20large%0Apathology%20image.%20For%20example%2C%20an%20image%20might%20cover%20an%20extensive%20tissue%20area%0Acontaining%20cancerous%20and%20healthy%20regions%2C%20but%20the%20accompanying%20text%20might%20only%0Aspecify%20that%20this%20image%20is%20a%20cancer%20slide%2C%20lacking%20the%20nuanced%20details%20needed%0Afor%20in-depth%20analysis.%20In%20this%20study%2C%20we%20introduce%20STimage-1K4M%2C%20a%20novel%0Adataset%20designed%20to%20bridge%20this%20gap%20by%20providing%20genomic%20features%20for%20sub-tile%0Aimages.%20STimage-1K4M%20contains%201%2C149%20images%20derived%20from%20spatial%20transcriptomics%0Adata%2C%20which%20captures%20gene%20expression%20information%20at%20the%20level%20of%20individual%0Aspatial%20spots%20within%20a%20pathology%20image.%20Specifically%2C%20each%20image%20in%20the%20dataset%0Ais%20broken%20down%20into%20smaller%20sub-image%20tiles%2C%20with%20each%20tile%20paired%20with%0A15%2C000-30%2C000%20dimensional%20gene%20expressions.%20With%204%2C293%2C195%20pairs%20of%20sub-tile%0Aimages%20and%20gene%20expressions%2C%20STimage-1K4M%20offers%20unprecedented%20granularity%2C%0Apaving%20the%20way%20for%20a%20wide%20range%20of%20advanced%20research%20in%20multi-modal%20data%0Aanalysis%20an%20innovative%20applications%20in%20computational%20pathology%2C%20and%20beyond.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2406.06393v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DSTimage-1K4M%253A%2520A%2520histopathology%2520image-gene%2520expression%2520dataset%2520for%2520spatial%250A%2520%2520transcriptomics%26entry.906535625%3DJiawen%2520Chen%2520and%2520Muqing%2520Zhou%2520and%2520Wenrong%2520Wu%2520and%2520Jinwei%2520Zhang%2520and%2520Yun%2520Li%2520and%2520Didong%2520Li%26entry.1292438233%3D%2520%2520Recent%2520advances%2520in%2520multi-modal%2520algorithms%2520have%2520driven%2520and%2520been%2520driven%2520by%2520the%250Aincreasing%2520availability%2520of%2520large%2520image-text%2520datasets%252C%2520leading%2520to%2520significant%250Astrides%2520in%2520various%2520fields%252C%2520including%2520computational%2520pathology.%2520However%252C%2520in%2520most%250Aexisting%2520medical%2520image-text%2520datasets%252C%2520the%2520text%2520typically%2520provides%2520high-level%250Asummaries%2520that%2520may%2520not%2520sufficiently%2520describe%2520sub-tile%2520regions%2520within%2520a%2520large%250Apathology%2520image.%2520For%2520example%252C%2520an%2520image%2520might%2520cover%2520an%2520extensive%2520tissue%2520area%250Acontaining%2520cancerous%2520and%2520healthy%2520regions%252C%2520but%2520the%2520accompanying%2520text%2520might%2520only%250Aspecify%2520that%2520this%2520image%2520is%2520a%2520cancer%2520slide%252C%2520lacking%2520the%2520nuanced%2520details%2520needed%250Afor%2520in-depth%2520analysis.%2520In%2520this%2520study%252C%2520we%2520introduce%2520STimage-1K4M%252C%2520a%2520novel%250Adataset%2520designed%2520to%2520bridge%2520this%2520gap%2520by%2520providing%2520genomic%2520features%2520for%2520sub-tile%250Aimages.%2520STimage-1K4M%2520contains%25201%252C149%2520images%2520derived%2520from%2520spatial%2520transcriptomics%250Adata%252C%2520which%2520captures%2520gene%2520expression%2520information%2520at%2520the%2520level%2520of%2520individual%250Aspatial%2520spots%2520within%2520a%2520pathology%2520image.%2520Specifically%252C%2520each%2520image%2520in%2520the%2520dataset%250Ais%2520broken%2520down%2520into%2520smaller%2520sub-image%2520tiles%252C%2520with%2520each%2520tile%2520paired%2520with%250A15%252C000-30%252C000%2520dimensional%2520gene%2520expressions.%2520With%25204%252C293%252C195%2520pairs%2520of%2520sub-tile%250Aimages%2520and%2520gene%2520expressions%252C%2520STimage-1K4M%2520offers%2520unprecedented%2520granularity%252C%250Apaving%2520the%2520way%2520for%2520a%2520wide%2520range%2520of%2520advanced%2520research%2520in%2520multi-modal%2520data%250Aanalysis%2520an%2520innovative%2520applications%2520in%2520computational%2520pathology%252C%2520and%2520beyond.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2406.06393v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=STimage-1K4M%3A%20A%20histopathology%20image-gene%20expression%20dataset%20for%20spatial%0A%20%20transcriptomics&entry.906535625=Jiawen%20Chen%20and%20Muqing%20Zhou%20and%20Wenrong%20Wu%20and%20Jinwei%20Zhang%20and%20Yun%20Li%20and%20Didong%20Li&entry.1292438233=%20%20Recent%20advances%20in%20multi-modal%20algorithms%20have%20driven%20and%20been%20driven%20by%20the%0Aincreasing%20availability%20of%20large%20image-text%20datasets%2C%20leading%20to%20significant%0Astrides%20in%20various%20fields%2C%20including%20computational%20pathology.%20However%2C%20in%20most%0Aexisting%20medical%20image-text%20datasets%2C%20the%20text%20typically%20provides%20high-level%0Asummaries%20that%20may%20not%20sufficiently%20describe%20sub-tile%20regions%20within%20a%20large%0Apathology%20image.%20For%20example%2C%20an%20image%20might%20cover%20an%20extensive%20tissue%20area%0Acontaining%20cancerous%20and%20healthy%20regions%2C%20but%20the%20accompanying%20text%20might%20only%0Aspecify%20that%20this%20image%20is%20a%20cancer%20slide%2C%20lacking%20the%20nuanced%20details%20needed%0Afor%20in-depth%20analysis.%20In%20this%20study%2C%20we%20introduce%20STimage-1K4M%2C%20a%20novel%0Adataset%20designed%20to%20bridge%20this%20gap%20by%20providing%20genomic%20features%20for%20sub-tile%0Aimages.%20STimage-1K4M%20contains%201%2C149%20images%20derived%20from%20spatial%20transcriptomics%0Adata%2C%20which%20captures%20gene%20expression%20information%20at%20the%20level%20of%20individual%0Aspatial%20spots%20within%20a%20pathology%20image.%20Specifically%2C%20each%20image%20in%20the%20dataset%0Ais%20broken%20down%20into%20smaller%20sub-image%20tiles%2C%20with%20each%20tile%20paired%20with%0A15%2C000-30%2C000%20dimensional%20gene%20expressions.%20With%204%2C293%2C195%20pairs%20of%20sub-tile%0Aimages%20and%20gene%20expressions%2C%20STimage-1K4M%20offers%20unprecedented%20granularity%2C%0Apaving%20the%20way%20for%20a%20wide%20range%20of%20advanced%20research%20in%20multi-modal%20data%0Aanalysis%20an%20innovative%20applications%20in%20computational%20pathology%2C%20and%20beyond.%0A&entry.1838667208=http%3A//arxiv.org/abs/2406.06393v2&entry.124074799=Read"},
{"title": "Graph Neural Networks in Histopathology: Emerging Trends and Future\n  Directions", "author": "Siemen Brussee and Giorgio Buzzanca and Anne M. R. Schrader and Jesper Kers", "abstract": "  Histopathological analysis of Whole Slide Images (WSIs) has seen a surge in\nthe utilization of deep learning methods, particularly Convolutional Neural\nNetworks (CNNs). However, CNNs often fall short in capturing the intricate\nspatial dependencies inherent in WSIs. Graph Neural Networks (GNNs) present a\npromising alternative, adept at directly modeling pairwise interactions and\neffectively discerning the topological tissue and cellular structures within\nWSIs. Recognizing the pressing need for deep learning techniques that harness\nthe topological structure of WSIs, the application of GNNs in histopathology\nhas experienced rapid growth. In this comprehensive review, we survey GNNs in\nhistopathology, discuss their applications, and explore emerging trends that\npave the way for future advancements in the field. We begin by elucidating the\nfundamentals of GNNs and their potential applications in histopathology.\nLeveraging quantitative literature analysis, we identify four emerging trends:\nHierarchical GNNs, Adaptive Graph Structure Learning, Multimodal GNNs, and\nHigher-order GNNs. Through an in-depth exploration of these trends, we offer\ninsights into the evolving landscape of GNNs in histopathological analysis.\nBased on our findings, we propose future directions to propel the field\nforward. Our analysis serves to guide researchers and practitioners towards\ninnovative approaches and methodologies, fostering advancements in\nhistopathological analysis through the lens of graph neural networks.\n", "link": "http://arxiv.org/abs/2406.12808v2", "date": "2024-06-20", "relevancy": 2.3243, "topK": [{"title": "Self-supervised Photographic Image Layout Representation Learning", "link": "http://arxiv.org/abs/2403.03740v1", "similarity": 0.4872}, {"title": "Boosted dynamic neural networks", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/26302", "similarity": 0.4655}, {"title": "Dynamic Gaussians Mesh: Consistent Mesh Reconstruction from Monocular\n  Videos", "link": "http://arxiv.org/abs/2404.12379v2", "similarity": 0.4419}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Graph%20Neural%20Networks%20in%20Histopathology%3A%20Emerging%20Trends%20and%20Future%0A%20%20Directions&body=Title%3A%20Graph%20Neural%20Networks%20in%20Histopathology%3A%20Emerging%20Trends%20and%20Future%0A%20%20Directions%0AAuthor%3A%20Siemen%20Brussee%20and%20Giorgio%20Buzzanca%20and%20Anne%20M.%20R.%20Schrader%20and%20Jesper%20Kers%0AAbstract%3A%20%20%20Histopathological%20analysis%20of%20Whole%20Slide%20Images%20%28WSIs%29%20has%20seen%20a%20surge%20in%0Athe%20utilization%20of%20deep%20learning%20methods%2C%20particularly%20Convolutional%20Neural%0ANetworks%20%28CNNs%29.%20However%2C%20CNNs%20often%20fall%20short%20in%20capturing%20the%20intricate%0Aspatial%20dependencies%20inherent%20in%20WSIs.%20Graph%20Neural%20Networks%20%28GNNs%29%20present%20a%0Apromising%20alternative%2C%20adept%20at%20directly%20modeling%20pairwise%20interactions%20and%0Aeffectively%20discerning%20the%20topological%20tissue%20and%20cellular%20structures%20within%0AWSIs.%20Recognizing%20the%20pressing%20need%20for%20deep%20learning%20techniques%20that%20harness%0Athe%20topological%20structure%20of%20WSIs%2C%20the%20application%20of%20GNNs%20in%20histopathology%0Ahas%20experienced%20rapid%20growth.%20In%20this%20comprehensive%20review%2C%20we%20survey%20GNNs%20in%0Ahistopathology%2C%20discuss%20their%20applications%2C%20and%20explore%20emerging%20trends%20that%0Apave%20the%20way%20for%20future%20advancements%20in%20the%20field.%20We%20begin%20by%20elucidating%20the%0Afundamentals%20of%20GNNs%20and%20their%20potential%20applications%20in%20histopathology.%0ALeveraging%20quantitative%20literature%20analysis%2C%20we%20identify%20four%20emerging%20trends%3A%0AHierarchical%20GNNs%2C%20Adaptive%20Graph%20Structure%20Learning%2C%20Multimodal%20GNNs%2C%20and%0AHigher-order%20GNNs.%20Through%20an%20in-depth%20exploration%20of%20these%20trends%2C%20we%20offer%0Ainsights%20into%20the%20evolving%20landscape%20of%20GNNs%20in%20histopathological%20analysis.%0ABased%20on%20our%20findings%2C%20we%20propose%20future%20directions%20to%20propel%20the%20field%0Aforward.%20Our%20analysis%20serves%20to%20guide%20researchers%20and%20practitioners%20towards%0Ainnovative%20approaches%20and%20methodologies%2C%20fostering%20advancements%20in%0Ahistopathological%20analysis%20through%20the%20lens%20of%20graph%20neural%20networks.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2406.12808v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DGraph%2520Neural%2520Networks%2520in%2520Histopathology%253A%2520Emerging%2520Trends%2520and%2520Future%250A%2520%2520Directions%26entry.906535625%3DSiemen%2520Brussee%2520and%2520Giorgio%2520Buzzanca%2520and%2520Anne%2520M.%2520R.%2520Schrader%2520and%2520Jesper%2520Kers%26entry.1292438233%3D%2520%2520Histopathological%2520analysis%2520of%2520Whole%2520Slide%2520Images%2520%2528WSIs%2529%2520has%2520seen%2520a%2520surge%2520in%250Athe%2520utilization%2520of%2520deep%2520learning%2520methods%252C%2520particularly%2520Convolutional%2520Neural%250ANetworks%2520%2528CNNs%2529.%2520However%252C%2520CNNs%2520often%2520fall%2520short%2520in%2520capturing%2520the%2520intricate%250Aspatial%2520dependencies%2520inherent%2520in%2520WSIs.%2520Graph%2520Neural%2520Networks%2520%2528GNNs%2529%2520present%2520a%250Apromising%2520alternative%252C%2520adept%2520at%2520directly%2520modeling%2520pairwise%2520interactions%2520and%250Aeffectively%2520discerning%2520the%2520topological%2520tissue%2520and%2520cellular%2520structures%2520within%250AWSIs.%2520Recognizing%2520the%2520pressing%2520need%2520for%2520deep%2520learning%2520techniques%2520that%2520harness%250Athe%2520topological%2520structure%2520of%2520WSIs%252C%2520the%2520application%2520of%2520GNNs%2520in%2520histopathology%250Ahas%2520experienced%2520rapid%2520growth.%2520In%2520this%2520comprehensive%2520review%252C%2520we%2520survey%2520GNNs%2520in%250Ahistopathology%252C%2520discuss%2520their%2520applications%252C%2520and%2520explore%2520emerging%2520trends%2520that%250Apave%2520the%2520way%2520for%2520future%2520advancements%2520in%2520the%2520field.%2520We%2520begin%2520by%2520elucidating%2520the%250Afundamentals%2520of%2520GNNs%2520and%2520their%2520potential%2520applications%2520in%2520histopathology.%250ALeveraging%2520quantitative%2520literature%2520analysis%252C%2520we%2520identify%2520four%2520emerging%2520trends%253A%250AHierarchical%2520GNNs%252C%2520Adaptive%2520Graph%2520Structure%2520Learning%252C%2520Multimodal%2520GNNs%252C%2520and%250AHigher-order%2520GNNs.%2520Through%2520an%2520in-depth%2520exploration%2520of%2520these%2520trends%252C%2520we%2520offer%250Ainsights%2520into%2520the%2520evolving%2520landscape%2520of%2520GNNs%2520in%2520histopathological%2520analysis.%250ABased%2520on%2520our%2520findings%252C%2520we%2520propose%2520future%2520directions%2520to%2520propel%2520the%2520field%250Aforward.%2520Our%2520analysis%2520serves%2520to%2520guide%2520researchers%2520and%2520practitioners%2520towards%250Ainnovative%2520approaches%2520and%2520methodologies%252C%2520fostering%2520advancements%2520in%250Ahistopathological%2520analysis%2520through%2520the%2520lens%2520of%2520graph%2520neural%2520networks.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2406.12808v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Graph%20Neural%20Networks%20in%20Histopathology%3A%20Emerging%20Trends%20and%20Future%0A%20%20Directions&entry.906535625=Siemen%20Brussee%20and%20Giorgio%20Buzzanca%20and%20Anne%20M.%20R.%20Schrader%20and%20Jesper%20Kers&entry.1292438233=%20%20Histopathological%20analysis%20of%20Whole%20Slide%20Images%20%28WSIs%29%20has%20seen%20a%20surge%20in%0Athe%20utilization%20of%20deep%20learning%20methods%2C%20particularly%20Convolutional%20Neural%0ANetworks%20%28CNNs%29.%20However%2C%20CNNs%20often%20fall%20short%20in%20capturing%20the%20intricate%0Aspatial%20dependencies%20inherent%20in%20WSIs.%20Graph%20Neural%20Networks%20%28GNNs%29%20present%20a%0Apromising%20alternative%2C%20adept%20at%20directly%20modeling%20pairwise%20interactions%20and%0Aeffectively%20discerning%20the%20topological%20tissue%20and%20cellular%20structures%20within%0AWSIs.%20Recognizing%20the%20pressing%20need%20for%20deep%20learning%20techniques%20that%20harness%0Athe%20topological%20structure%20of%20WSIs%2C%20the%20application%20of%20GNNs%20in%20histopathology%0Ahas%20experienced%20rapid%20growth.%20In%20this%20comprehensive%20review%2C%20we%20survey%20GNNs%20in%0Ahistopathology%2C%20discuss%20their%20applications%2C%20and%20explore%20emerging%20trends%20that%0Apave%20the%20way%20for%20future%20advancements%20in%20the%20field.%20We%20begin%20by%20elucidating%20the%0Afundamentals%20of%20GNNs%20and%20their%20potential%20applications%20in%20histopathology.%0ALeveraging%20quantitative%20literature%20analysis%2C%20we%20identify%20four%20emerging%20trends%3A%0AHierarchical%20GNNs%2C%20Adaptive%20Graph%20Structure%20Learning%2C%20Multimodal%20GNNs%2C%20and%0AHigher-order%20GNNs.%20Through%20an%20in-depth%20exploration%20of%20these%20trends%2C%20we%20offer%0Ainsights%20into%20the%20evolving%20landscape%20of%20GNNs%20in%20histopathological%20analysis.%0ABased%20on%20our%20findings%2C%20we%20propose%20future%20directions%20to%20propel%20the%20field%0Aforward.%20Our%20analysis%20serves%20to%20guide%20researchers%20and%20practitioners%20towards%0Ainnovative%20approaches%20and%20methodologies%2C%20fostering%20advancements%20in%0Ahistopathological%20analysis%20through%20the%20lens%20of%20graph%20neural%20networks.%0A&entry.1838667208=http%3A//arxiv.org/abs/2406.12808v2&entry.124074799=Read"},
{"title": "Identifiable Exchangeable Mechanisms for Causal Structure and\n  Representation Learning", "author": "Patrik Reizinger and Siyuan Guo and Ferenc Husz\u00e1r and Bernhard Sch\u00f6lkopf and Wieland Brendel", "abstract": "  Identifying latent representations or causal structures is important for good\ngeneralization and downstream task performance. However, both fields have been\ndeveloped rather independently. We observe that several methods in both\nrepresentation and causal structure learning rely on the same data-generating\nprocess (DGP), namely, exchangeable but not i.i.d. (independent and identically\ndistributed) data. We provide a unified framework, termed Identifiable\nExchangeable Mechanisms (IEM), for representation and structure learning under\nthe lens of exchangeability. IEM provides new insights that let us relax the\nnecessary conditions for causal structure identification in exchangeable\nnon--i.i.d. data. We also demonstrate the existence of a duality condition in\nidentifiable representation learning, leading to new identifiability results.\nWe hope this work will pave the way for further research in causal\nrepresentation learning.\n", "link": "http://arxiv.org/abs/2406.14302v1", "date": "2024-06-20", "relevancy": 2.309, "topK": [{"title": "IAE: Implicit Autoencoder for Point Cloud Self-supervised Representation Learning", "link": "https://arxiv.org/abs/2201.00785", "similarity": 0.465}, {"title": "Flexible Visual Recognition by Evidential Modeling of Confusion and Ignorance", "link": "http://openaccess.thecvf.com/content/ICCV2023/html/Fan_Flexible_Visual_Recognition_by_Evidential_Modeling_of_Confusion_and_Ignorance_ICCV_2023_paper.html", "similarity": 0.4612}, {"title": "Magic-Me: Identity-Specific Video Customized Diffusion", "link": "http://arxiv.org/abs/2402.09368v1", "similarity": 0.4592}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Identifiable%20Exchangeable%20Mechanisms%20for%20Causal%20Structure%20and%0A%20%20Representation%20Learning&body=Title%3A%20Identifiable%20Exchangeable%20Mechanisms%20for%20Causal%20Structure%20and%0A%20%20Representation%20Learning%0AAuthor%3A%20Patrik%20Reizinger%20and%20Siyuan%20Guo%20and%20Ferenc%20Husz%C3%A1r%20and%20Bernhard%20Sch%C3%B6lkopf%20and%20Wieland%20Brendel%0AAbstract%3A%20%20%20Identifying%20latent%20representations%20or%20causal%20structures%20is%20important%20for%20good%0Ageneralization%20and%20downstream%20task%20performance.%20However%2C%20both%20fields%20have%20been%0Adeveloped%20rather%20independently.%20We%20observe%20that%20several%20methods%20in%20both%0Arepresentation%20and%20causal%20structure%20learning%20rely%20on%20the%20same%20data-generating%0Aprocess%20%28DGP%29%2C%20namely%2C%20exchangeable%20but%20not%20i.i.d.%20%28independent%20and%20identically%0Adistributed%29%20data.%20We%20provide%20a%20unified%20framework%2C%20termed%20Identifiable%0AExchangeable%20Mechanisms%20%28IEM%29%2C%20for%20representation%20and%20structure%20learning%20under%0Athe%20lens%20of%20exchangeability.%20IEM%20provides%20new%20insights%20that%20let%20us%20relax%20the%0Anecessary%20conditions%20for%20causal%20structure%20identification%20in%20exchangeable%0Anon--i.i.d.%20data.%20We%20also%20demonstrate%20the%20existence%20of%20a%20duality%20condition%20in%0Aidentifiable%20representation%20learning%2C%20leading%20to%20new%20identifiability%20results.%0AWe%20hope%20this%20work%20will%20pave%20the%20way%20for%20further%20research%20in%20causal%0Arepresentation%20learning.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2406.14302v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DIdentifiable%2520Exchangeable%2520Mechanisms%2520for%2520Causal%2520Structure%2520and%250A%2520%2520Representation%2520Learning%26entry.906535625%3DPatrik%2520Reizinger%2520and%2520Siyuan%2520Guo%2520and%2520Ferenc%2520Husz%25C3%25A1r%2520and%2520Bernhard%2520Sch%25C3%25B6lkopf%2520and%2520Wieland%2520Brendel%26entry.1292438233%3D%2520%2520Identifying%2520latent%2520representations%2520or%2520causal%2520structures%2520is%2520important%2520for%2520good%250Ageneralization%2520and%2520downstream%2520task%2520performance.%2520However%252C%2520both%2520fields%2520have%2520been%250Adeveloped%2520rather%2520independently.%2520We%2520observe%2520that%2520several%2520methods%2520in%2520both%250Arepresentation%2520and%2520causal%2520structure%2520learning%2520rely%2520on%2520the%2520same%2520data-generating%250Aprocess%2520%2528DGP%2529%252C%2520namely%252C%2520exchangeable%2520but%2520not%2520i.i.d.%2520%2528independent%2520and%2520identically%250Adistributed%2529%2520data.%2520We%2520provide%2520a%2520unified%2520framework%252C%2520termed%2520Identifiable%250AExchangeable%2520Mechanisms%2520%2528IEM%2529%252C%2520for%2520representation%2520and%2520structure%2520learning%2520under%250Athe%2520lens%2520of%2520exchangeability.%2520IEM%2520provides%2520new%2520insights%2520that%2520let%2520us%2520relax%2520the%250Anecessary%2520conditions%2520for%2520causal%2520structure%2520identification%2520in%2520exchangeable%250Anon--i.i.d.%2520data.%2520We%2520also%2520demonstrate%2520the%2520existence%2520of%2520a%2520duality%2520condition%2520in%250Aidentifiable%2520representation%2520learning%252C%2520leading%2520to%2520new%2520identifiability%2520results.%250AWe%2520hope%2520this%2520work%2520will%2520pave%2520the%2520way%2520for%2520further%2520research%2520in%2520causal%250Arepresentation%2520learning.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2406.14302v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Identifiable%20Exchangeable%20Mechanisms%20for%20Causal%20Structure%20and%0A%20%20Representation%20Learning&entry.906535625=Patrik%20Reizinger%20and%20Siyuan%20Guo%20and%20Ferenc%20Husz%C3%A1r%20and%20Bernhard%20Sch%C3%B6lkopf%20and%20Wieland%20Brendel&entry.1292438233=%20%20Identifying%20latent%20representations%20or%20causal%20structures%20is%20important%20for%20good%0Ageneralization%20and%20downstream%20task%20performance.%20However%2C%20both%20fields%20have%20been%0Adeveloped%20rather%20independently.%20We%20observe%20that%20several%20methods%20in%20both%0Arepresentation%20and%20causal%20structure%20learning%20rely%20on%20the%20same%20data-generating%0Aprocess%20%28DGP%29%2C%20namely%2C%20exchangeable%20but%20not%20i.i.d.%20%28independent%20and%20identically%0Adistributed%29%20data.%20We%20provide%20a%20unified%20framework%2C%20termed%20Identifiable%0AExchangeable%20Mechanisms%20%28IEM%29%2C%20for%20representation%20and%20structure%20learning%20under%0Athe%20lens%20of%20exchangeability.%20IEM%20provides%20new%20insights%20that%20let%20us%20relax%20the%0Anecessary%20conditions%20for%20causal%20structure%20identification%20in%20exchangeable%0Anon--i.i.d.%20data.%20We%20also%20demonstrate%20the%20existence%20of%20a%20duality%20condition%20in%0Aidentifiable%20representation%20learning%2C%20leading%20to%20new%20identifiability%20results.%0AWe%20hope%20this%20work%20will%20pave%20the%20way%20for%20further%20research%20in%20causal%0Arepresentation%20learning.%0A&entry.1838667208=http%3A//arxiv.org/abs/2406.14302v1&entry.124074799=Read"},
{"title": "MultiTalk: Enhancing 3D Talking Head Generation Across Languages with\n  Multilingual Video Dataset", "author": "Kim Sung-Bin and Lee Chae-Yeon and Gihun Son and Oh Hyun-Bin and Janghoon Ju and Suekyeong Nam and Tae-Hyun Oh", "abstract": "  Recent studies in speech-driven 3D talking head generation have achieved\nconvincing results in verbal articulations. However, generating accurate\nlip-syncs degrades when applied to input speech in other languages, possibly\ndue to the lack of datasets covering a broad spectrum of facial movements\nacross languages. In this work, we introduce a novel task to generate 3D\ntalking heads from speeches of diverse languages. We collect a new multilingual\n2D video dataset comprising over 420 hours of talking videos in 20 languages.\nWith our proposed dataset, we present a multilingually enhanced model that\nincorporates language-specific style embeddings, enabling it to capture the\nunique mouth movements associated with each language. Additionally, we present\na metric for assessing lip-sync accuracy in multilingual settings. We\ndemonstrate that training a 3D talking head model with our proposed dataset\nsignificantly enhances its multilingual performance. Codes and datasets are\navailable at https://multi-talk.github.io/.\n", "link": "http://arxiv.org/abs/2406.14272v1", "date": "2024-06-20", "relevancy": 2.2913, "topK": [{"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.601}, {"title": "Customize-A-Video: One-Shot Motion Customization of Text-to-Video\n  Diffusion Models", "link": "http://arxiv.org/abs/2402.14780v1", "similarity": 0.5529}, {"title": "3D Gaussian Blendshapes for Head Avatar Animation", "link": "http://arxiv.org/abs/2404.19398v2", "similarity": 0.5526}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20MultiTalk%3A%20Enhancing%203D%20Talking%20Head%20Generation%20Across%20Languages%20with%0A%20%20Multilingual%20Video%20Dataset&body=Title%3A%20MultiTalk%3A%20Enhancing%203D%20Talking%20Head%20Generation%20Across%20Languages%20with%0A%20%20Multilingual%20Video%20Dataset%0AAuthor%3A%20Kim%20Sung-Bin%20and%20Lee%20Chae-Yeon%20and%20Gihun%20Son%20and%20Oh%20Hyun-Bin%20and%20Janghoon%20Ju%20and%20Suekyeong%20Nam%20and%20Tae-Hyun%20Oh%0AAbstract%3A%20%20%20Recent%20studies%20in%20speech-driven%203D%20talking%20head%20generation%20have%20achieved%0Aconvincing%20results%20in%20verbal%20articulations.%20However%2C%20generating%20accurate%0Alip-syncs%20degrades%20when%20applied%20to%20input%20speech%20in%20other%20languages%2C%20possibly%0Adue%20to%20the%20lack%20of%20datasets%20covering%20a%20broad%20spectrum%20of%20facial%20movements%0Aacross%20languages.%20In%20this%20work%2C%20we%20introduce%20a%20novel%20task%20to%20generate%203D%0Atalking%20heads%20from%20speeches%20of%20diverse%20languages.%20We%20collect%20a%20new%20multilingual%0A2D%20video%20dataset%20comprising%20over%20420%20hours%20of%20talking%20videos%20in%2020%20languages.%0AWith%20our%20proposed%20dataset%2C%20we%20present%20a%20multilingually%20enhanced%20model%20that%0Aincorporates%20language-specific%20style%20embeddings%2C%20enabling%20it%20to%20capture%20the%0Aunique%20mouth%20movements%20associated%20with%20each%20language.%20Additionally%2C%20we%20present%0Aa%20metric%20for%20assessing%20lip-sync%20accuracy%20in%20multilingual%20settings.%20We%0Ademonstrate%20that%20training%20a%203D%20talking%20head%20model%20with%20our%20proposed%20dataset%0Asignificantly%20enhances%20its%20multilingual%20performance.%20Codes%20and%20datasets%20are%0Aavailable%20at%20https%3A//multi-talk.github.io/.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2406.14272v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DMultiTalk%253A%2520Enhancing%25203D%2520Talking%2520Head%2520Generation%2520Across%2520Languages%2520with%250A%2520%2520Multilingual%2520Video%2520Dataset%26entry.906535625%3DKim%2520Sung-Bin%2520and%2520Lee%2520Chae-Yeon%2520and%2520Gihun%2520Son%2520and%2520Oh%2520Hyun-Bin%2520and%2520Janghoon%2520Ju%2520and%2520Suekyeong%2520Nam%2520and%2520Tae-Hyun%2520Oh%26entry.1292438233%3D%2520%2520Recent%2520studies%2520in%2520speech-driven%25203D%2520talking%2520head%2520generation%2520have%2520achieved%250Aconvincing%2520results%2520in%2520verbal%2520articulations.%2520However%252C%2520generating%2520accurate%250Alip-syncs%2520degrades%2520when%2520applied%2520to%2520input%2520speech%2520in%2520other%2520languages%252C%2520possibly%250Adue%2520to%2520the%2520lack%2520of%2520datasets%2520covering%2520a%2520broad%2520spectrum%2520of%2520facial%2520movements%250Aacross%2520languages.%2520In%2520this%2520work%252C%2520we%2520introduce%2520a%2520novel%2520task%2520to%2520generate%25203D%250Atalking%2520heads%2520from%2520speeches%2520of%2520diverse%2520languages.%2520We%2520collect%2520a%2520new%2520multilingual%250A2D%2520video%2520dataset%2520comprising%2520over%2520420%2520hours%2520of%2520talking%2520videos%2520in%252020%2520languages.%250AWith%2520our%2520proposed%2520dataset%252C%2520we%2520present%2520a%2520multilingually%2520enhanced%2520model%2520that%250Aincorporates%2520language-specific%2520style%2520embeddings%252C%2520enabling%2520it%2520to%2520capture%2520the%250Aunique%2520mouth%2520movements%2520associated%2520with%2520each%2520language.%2520Additionally%252C%2520we%2520present%250Aa%2520metric%2520for%2520assessing%2520lip-sync%2520accuracy%2520in%2520multilingual%2520settings.%2520We%250Ademonstrate%2520that%2520training%2520a%25203D%2520talking%2520head%2520model%2520with%2520our%2520proposed%2520dataset%250Asignificantly%2520enhances%2520its%2520multilingual%2520performance.%2520Codes%2520and%2520datasets%2520are%250Aavailable%2520at%2520https%253A//multi-talk.github.io/.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2406.14272v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=MultiTalk%3A%20Enhancing%203D%20Talking%20Head%20Generation%20Across%20Languages%20with%0A%20%20Multilingual%20Video%20Dataset&entry.906535625=Kim%20Sung-Bin%20and%20Lee%20Chae-Yeon%20and%20Gihun%20Son%20and%20Oh%20Hyun-Bin%20and%20Janghoon%20Ju%20and%20Suekyeong%20Nam%20and%20Tae-Hyun%20Oh&entry.1292438233=%20%20Recent%20studies%20in%20speech-driven%203D%20talking%20head%20generation%20have%20achieved%0Aconvincing%20results%20in%20verbal%20articulations.%20However%2C%20generating%20accurate%0Alip-syncs%20degrades%20when%20applied%20to%20input%20speech%20in%20other%20languages%2C%20possibly%0Adue%20to%20the%20lack%20of%20datasets%20covering%20a%20broad%20spectrum%20of%20facial%20movements%0Aacross%20languages.%20In%20this%20work%2C%20we%20introduce%20a%20novel%20task%20to%20generate%203D%0Atalking%20heads%20from%20speeches%20of%20diverse%20languages.%20We%20collect%20a%20new%20multilingual%0A2D%20video%20dataset%20comprising%20over%20420%20hours%20of%20talking%20videos%20in%2020%20languages.%0AWith%20our%20proposed%20dataset%2C%20we%20present%20a%20multilingually%20enhanced%20model%20that%0Aincorporates%20language-specific%20style%20embeddings%2C%20enabling%20it%20to%20capture%20the%0Aunique%20mouth%20movements%20associated%20with%20each%20language.%20Additionally%2C%20we%20present%0Aa%20metric%20for%20assessing%20lip-sync%20accuracy%20in%20multilingual%20settings.%20We%0Ademonstrate%20that%20training%20a%203D%20talking%20head%20model%20with%20our%20proposed%20dataset%0Asignificantly%20enhances%20its%20multilingual%20performance.%20Codes%20and%20datasets%20are%0Aavailable%20at%20https%3A//multi-talk.github.io/.%0A&entry.1838667208=http%3A//arxiv.org/abs/2406.14272v1&entry.124074799=Read"},
{"title": "Compressed representation of brain genetic transcription", "author": "James K Ruffle and Henry Watkins and Robert J Gray and Harpreet Hyare and Michel Thiebaut de Schotten and Parashkev Nachev", "abstract": "  The architecture of the brain is too complex to be intuitively surveyable\nwithout the use of compressed representations that project its variation into a\ncompact, navigable space. The task is especially challenging with\nhigh-dimensional data, such as gene expression, where the joint complexity of\nanatomical and transcriptional patterns demands maximum compression.\nEstablished practice is to use standard principal component analysis (PCA),\nwhose computational felicity is offset by limited expressivity, especially at\ngreat compression ratios. Employing whole-brain, voxel-wise Allen Brain Atlas\ntranscription data, here we systematically compare compressed representations\nbased on the most widely supported linear and non-linear methods-PCA, kernel\nPCA, non-negative matrix factorization (NMF), t-stochastic neighbour embedding\n(t-SNE), uniform manifold approximation and projection (UMAP), and deep\nauto-encoding-quantifying reconstruction fidelity, anatomical coherence, and\npredictive utility with respect to signalling, microstructural, and metabolic\ntargets. We show that deep auto-encoders yield superior representations across\nall metrics of performance and target domains, supporting their use as the\nreference standard for representing transcription patterns in the human brain.\n", "link": "http://arxiv.org/abs/2310.16113v3", "date": "2024-06-20", "relevancy": 2.2884, "topK": [{"title": "Self-supervised Photographic Image Layout Representation Learning", "link": "http://arxiv.org/abs/2403.03740v1", "similarity": 0.4758}, {"title": "IAE: Implicit Autoencoder for Point Cloud Self-supervised Representation Learning", "link": "https://arxiv.org/abs/2201.00785", "similarity": 0.4586}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.4386}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Compressed%20representation%20of%20brain%20genetic%20transcription&body=Title%3A%20Compressed%20representation%20of%20brain%20genetic%20transcription%0AAuthor%3A%20James%20K%20Ruffle%20and%20Henry%20Watkins%20and%20Robert%20J%20Gray%20and%20Harpreet%20Hyare%20and%20Michel%20Thiebaut%20de%20Schotten%20and%20Parashkev%20Nachev%0AAbstract%3A%20%20%20The%20architecture%20of%20the%20brain%20is%20too%20complex%20to%20be%20intuitively%20surveyable%0Awithout%20the%20use%20of%20compressed%20representations%20that%20project%20its%20variation%20into%20a%0Acompact%2C%20navigable%20space.%20The%20task%20is%20especially%20challenging%20with%0Ahigh-dimensional%20data%2C%20such%20as%20gene%20expression%2C%20where%20the%20joint%20complexity%20of%0Aanatomical%20and%20transcriptional%20patterns%20demands%20maximum%20compression.%0AEstablished%20practice%20is%20to%20use%20standard%20principal%20component%20analysis%20%28PCA%29%2C%0Awhose%20computational%20felicity%20is%20offset%20by%20limited%20expressivity%2C%20especially%20at%0Agreat%20compression%20ratios.%20Employing%20whole-brain%2C%20voxel-wise%20Allen%20Brain%20Atlas%0Atranscription%20data%2C%20here%20we%20systematically%20compare%20compressed%20representations%0Abased%20on%20the%20most%20widely%20supported%20linear%20and%20non-linear%20methods-PCA%2C%20kernel%0APCA%2C%20non-negative%20matrix%20factorization%20%28NMF%29%2C%20t-stochastic%20neighbour%20embedding%0A%28t-SNE%29%2C%20uniform%20manifold%20approximation%20and%20projection%20%28UMAP%29%2C%20and%20deep%0Aauto-encoding-quantifying%20reconstruction%20fidelity%2C%20anatomical%20coherence%2C%20and%0Apredictive%20utility%20with%20respect%20to%20signalling%2C%20microstructural%2C%20and%20metabolic%0Atargets.%20We%20show%20that%20deep%20auto-encoders%20yield%20superior%20representations%20across%0Aall%20metrics%20of%20performance%20and%20target%20domains%2C%20supporting%20their%20use%20as%20the%0Areference%20standard%20for%20representing%20transcription%20patterns%20in%20the%20human%20brain.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2310.16113v3%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DCompressed%2520representation%2520of%2520brain%2520genetic%2520transcription%26entry.906535625%3DJames%2520K%2520Ruffle%2520and%2520Henry%2520Watkins%2520and%2520Robert%2520J%2520Gray%2520and%2520Harpreet%2520Hyare%2520and%2520Michel%2520Thiebaut%2520de%2520Schotten%2520and%2520Parashkev%2520Nachev%26entry.1292438233%3D%2520%2520The%2520architecture%2520of%2520the%2520brain%2520is%2520too%2520complex%2520to%2520be%2520intuitively%2520surveyable%250Awithout%2520the%2520use%2520of%2520compressed%2520representations%2520that%2520project%2520its%2520variation%2520into%2520a%250Acompact%252C%2520navigable%2520space.%2520The%2520task%2520is%2520especially%2520challenging%2520with%250Ahigh-dimensional%2520data%252C%2520such%2520as%2520gene%2520expression%252C%2520where%2520the%2520joint%2520complexity%2520of%250Aanatomical%2520and%2520transcriptional%2520patterns%2520demands%2520maximum%2520compression.%250AEstablished%2520practice%2520is%2520to%2520use%2520standard%2520principal%2520component%2520analysis%2520%2528PCA%2529%252C%250Awhose%2520computational%2520felicity%2520is%2520offset%2520by%2520limited%2520expressivity%252C%2520especially%2520at%250Agreat%2520compression%2520ratios.%2520Employing%2520whole-brain%252C%2520voxel-wise%2520Allen%2520Brain%2520Atlas%250Atranscription%2520data%252C%2520here%2520we%2520systematically%2520compare%2520compressed%2520representations%250Abased%2520on%2520the%2520most%2520widely%2520supported%2520linear%2520and%2520non-linear%2520methods-PCA%252C%2520kernel%250APCA%252C%2520non-negative%2520matrix%2520factorization%2520%2528NMF%2529%252C%2520t-stochastic%2520neighbour%2520embedding%250A%2528t-SNE%2529%252C%2520uniform%2520manifold%2520approximation%2520and%2520projection%2520%2528UMAP%2529%252C%2520and%2520deep%250Aauto-encoding-quantifying%2520reconstruction%2520fidelity%252C%2520anatomical%2520coherence%252C%2520and%250Apredictive%2520utility%2520with%2520respect%2520to%2520signalling%252C%2520microstructural%252C%2520and%2520metabolic%250Atargets.%2520We%2520show%2520that%2520deep%2520auto-encoders%2520yield%2520superior%2520representations%2520across%250Aall%2520metrics%2520of%2520performance%2520and%2520target%2520domains%252C%2520supporting%2520their%2520use%2520as%2520the%250Areference%2520standard%2520for%2520representing%2520transcription%2520patterns%2520in%2520the%2520human%2520brain.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2310.16113v3%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Compressed%20representation%20of%20brain%20genetic%20transcription&entry.906535625=James%20K%20Ruffle%20and%20Henry%20Watkins%20and%20Robert%20J%20Gray%20and%20Harpreet%20Hyare%20and%20Michel%20Thiebaut%20de%20Schotten%20and%20Parashkev%20Nachev&entry.1292438233=%20%20The%20architecture%20of%20the%20brain%20is%20too%20complex%20to%20be%20intuitively%20surveyable%0Awithout%20the%20use%20of%20compressed%20representations%20that%20project%20its%20variation%20into%20a%0Acompact%2C%20navigable%20space.%20The%20task%20is%20especially%20challenging%20with%0Ahigh-dimensional%20data%2C%20such%20as%20gene%20expression%2C%20where%20the%20joint%20complexity%20of%0Aanatomical%20and%20transcriptional%20patterns%20demands%20maximum%20compression.%0AEstablished%20practice%20is%20to%20use%20standard%20principal%20component%20analysis%20%28PCA%29%2C%0Awhose%20computational%20felicity%20is%20offset%20by%20limited%20expressivity%2C%20especially%20at%0Agreat%20compression%20ratios.%20Employing%20whole-brain%2C%20voxel-wise%20Allen%20Brain%20Atlas%0Atranscription%20data%2C%20here%20we%20systematically%20compare%20compressed%20representations%0Abased%20on%20the%20most%20widely%20supported%20linear%20and%20non-linear%20methods-PCA%2C%20kernel%0APCA%2C%20non-negative%20matrix%20factorization%20%28NMF%29%2C%20t-stochastic%20neighbour%20embedding%0A%28t-SNE%29%2C%20uniform%20manifold%20approximation%20and%20projection%20%28UMAP%29%2C%20and%20deep%0Aauto-encoding-quantifying%20reconstruction%20fidelity%2C%20anatomical%20coherence%2C%20and%0Apredictive%20utility%20with%20respect%20to%20signalling%2C%20microstructural%2C%20and%20metabolic%0Atargets.%20We%20show%20that%20deep%20auto-encoders%20yield%20superior%20representations%20across%0Aall%20metrics%20of%20performance%20and%20target%20domains%2C%20supporting%20their%20use%20as%20the%0Areference%20standard%20for%20representing%20transcription%20patterns%20in%20the%20human%20brain.%0A&entry.1838667208=http%3A//arxiv.org/abs/2310.16113v3&entry.124074799=Read"},
{"title": "Advancements in Translation Accuracy for Stereo Visual-Inertial\n  Initialization", "author": "Han Song and Zhongche Qu and Zhi Zhang and Zihan Ye and Cong Liu", "abstract": "  As the current initialization method in the state-of-the-art Stereo\nVisual-Inertial SLAM framework, ORB-SLAM3 has limitations. Its success depends\non the performance of the pure stereo SLAM system and is based on the\nunderlying assumption that pure visual SLAM can accurately estimate the camera\ntrajectory, which is essential for inertial parameter estimation. Meanwhile,\nthe further improved initialization method for ORB-SLAM3, known as Stereo-NEC,\nis time-consuming due to applying keypoint tracking to estimate gyroscope bias\nwith normal epipolar constraints. To address the limitations of previous\nmethods, this paper proposes a method aimed at enhancing translation accuracy\nduring the initialization stage. The fundamental concept of our method is to\nimprove the translation estimate with a 3 Degree-of-Freedom (DoF) Bundle\nAdjustment (BA), independently, while the rotation estimate is fixed, instead\nof using ORB-SLAM3's 6-DoF BA. Additionally, the rotation estimate will be\nupdated by considering IMU measurements and gyroscope bias, unlike ORB-SLAM3's\nrotation, which is directly obtained from stereo visual odometry and may yield\ninferior results when operating in challenging scenarios. We also conduct\nextensive evaluations on the public benchmark, the EuRoC dataset, demonstrating\nthat our method excels in accuracy.\n", "link": "http://arxiv.org/abs/2405.15082v3", "date": "2024-06-20", "relevancy": 2.2391, "topK": [{"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.5684}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.554}, {"title": "DDM-NET: End-to-end learning of keypoint feature Detection, Description and Matching for 3D localization", "link": "https://arxiv.org/abs/2212.04575", "similarity": 0.5535}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Advancements%20in%20Translation%20Accuracy%20for%20Stereo%20Visual-Inertial%0A%20%20Initialization&body=Title%3A%20Advancements%20in%20Translation%20Accuracy%20for%20Stereo%20Visual-Inertial%0A%20%20Initialization%0AAuthor%3A%20Han%20Song%20and%20Zhongche%20Qu%20and%20Zhi%20Zhang%20and%20Zihan%20Ye%20and%20Cong%20Liu%0AAbstract%3A%20%20%20As%20the%20current%20initialization%20method%20in%20the%20state-of-the-art%20Stereo%0AVisual-Inertial%20SLAM%20framework%2C%20ORB-SLAM3%20has%20limitations.%20Its%20success%20depends%0Aon%20the%20performance%20of%20the%20pure%20stereo%20SLAM%20system%20and%20is%20based%20on%20the%0Aunderlying%20assumption%20that%20pure%20visual%20SLAM%20can%20accurately%20estimate%20the%20camera%0Atrajectory%2C%20which%20is%20essential%20for%20inertial%20parameter%20estimation.%20Meanwhile%2C%0Athe%20further%20improved%20initialization%20method%20for%20ORB-SLAM3%2C%20known%20as%20Stereo-NEC%2C%0Ais%20time-consuming%20due%20to%20applying%20keypoint%20tracking%20to%20estimate%20gyroscope%20bias%0Awith%20normal%20epipolar%20constraints.%20To%20address%20the%20limitations%20of%20previous%0Amethods%2C%20this%20paper%20proposes%20a%20method%20aimed%20at%20enhancing%20translation%20accuracy%0Aduring%20the%20initialization%20stage.%20The%20fundamental%20concept%20of%20our%20method%20is%20to%0Aimprove%20the%20translation%20estimate%20with%20a%203%20Degree-of-Freedom%20%28DoF%29%20Bundle%0AAdjustment%20%28BA%29%2C%20independently%2C%20while%20the%20rotation%20estimate%20is%20fixed%2C%20instead%0Aof%20using%20ORB-SLAM3%27s%206-DoF%20BA.%20Additionally%2C%20the%20rotation%20estimate%20will%20be%0Aupdated%20by%20considering%20IMU%20measurements%20and%20gyroscope%20bias%2C%20unlike%20ORB-SLAM3%27s%0Arotation%2C%20which%20is%20directly%20obtained%20from%20stereo%20visual%20odometry%20and%20may%20yield%0Ainferior%20results%20when%20operating%20in%20challenging%20scenarios.%20We%20also%20conduct%0Aextensive%20evaluations%20on%20the%20public%20benchmark%2C%20the%20EuRoC%20dataset%2C%20demonstrating%0Athat%20our%20method%20excels%20in%20accuracy.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2405.15082v3%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DAdvancements%2520in%2520Translation%2520Accuracy%2520for%2520Stereo%2520Visual-Inertial%250A%2520%2520Initialization%26entry.906535625%3DHan%2520Song%2520and%2520Zhongche%2520Qu%2520and%2520Zhi%2520Zhang%2520and%2520Zihan%2520Ye%2520and%2520Cong%2520Liu%26entry.1292438233%3D%2520%2520As%2520the%2520current%2520initialization%2520method%2520in%2520the%2520state-of-the-art%2520Stereo%250AVisual-Inertial%2520SLAM%2520framework%252C%2520ORB-SLAM3%2520has%2520limitations.%2520Its%2520success%2520depends%250Aon%2520the%2520performance%2520of%2520the%2520pure%2520stereo%2520SLAM%2520system%2520and%2520is%2520based%2520on%2520the%250Aunderlying%2520assumption%2520that%2520pure%2520visual%2520SLAM%2520can%2520accurately%2520estimate%2520the%2520camera%250Atrajectory%252C%2520which%2520is%2520essential%2520for%2520inertial%2520parameter%2520estimation.%2520Meanwhile%252C%250Athe%2520further%2520improved%2520initialization%2520method%2520for%2520ORB-SLAM3%252C%2520known%2520as%2520Stereo-NEC%252C%250Ais%2520time-consuming%2520due%2520to%2520applying%2520keypoint%2520tracking%2520to%2520estimate%2520gyroscope%2520bias%250Awith%2520normal%2520epipolar%2520constraints.%2520To%2520address%2520the%2520limitations%2520of%2520previous%250Amethods%252C%2520this%2520paper%2520proposes%2520a%2520method%2520aimed%2520at%2520enhancing%2520translation%2520accuracy%250Aduring%2520the%2520initialization%2520stage.%2520The%2520fundamental%2520concept%2520of%2520our%2520method%2520is%2520to%250Aimprove%2520the%2520translation%2520estimate%2520with%2520a%25203%2520Degree-of-Freedom%2520%2528DoF%2529%2520Bundle%250AAdjustment%2520%2528BA%2529%252C%2520independently%252C%2520while%2520the%2520rotation%2520estimate%2520is%2520fixed%252C%2520instead%250Aof%2520using%2520ORB-SLAM3%2527s%25206-DoF%2520BA.%2520Additionally%252C%2520the%2520rotation%2520estimate%2520will%2520be%250Aupdated%2520by%2520considering%2520IMU%2520measurements%2520and%2520gyroscope%2520bias%252C%2520unlike%2520ORB-SLAM3%2527s%250Arotation%252C%2520which%2520is%2520directly%2520obtained%2520from%2520stereo%2520visual%2520odometry%2520and%2520may%2520yield%250Ainferior%2520results%2520when%2520operating%2520in%2520challenging%2520scenarios.%2520We%2520also%2520conduct%250Aextensive%2520evaluations%2520on%2520the%2520public%2520benchmark%252C%2520the%2520EuRoC%2520dataset%252C%2520demonstrating%250Athat%2520our%2520method%2520excels%2520in%2520accuracy.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2405.15082v3%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Advancements%20in%20Translation%20Accuracy%20for%20Stereo%20Visual-Inertial%0A%20%20Initialization&entry.906535625=Han%20Song%20and%20Zhongche%20Qu%20and%20Zhi%20Zhang%20and%20Zihan%20Ye%20and%20Cong%20Liu&entry.1292438233=%20%20As%20the%20current%20initialization%20method%20in%20the%20state-of-the-art%20Stereo%0AVisual-Inertial%20SLAM%20framework%2C%20ORB-SLAM3%20has%20limitations.%20Its%20success%20depends%0Aon%20the%20performance%20of%20the%20pure%20stereo%20SLAM%20system%20and%20is%20based%20on%20the%0Aunderlying%20assumption%20that%20pure%20visual%20SLAM%20can%20accurately%20estimate%20the%20camera%0Atrajectory%2C%20which%20is%20essential%20for%20inertial%20parameter%20estimation.%20Meanwhile%2C%0Athe%20further%20improved%20initialization%20method%20for%20ORB-SLAM3%2C%20known%20as%20Stereo-NEC%2C%0Ais%20time-consuming%20due%20to%20applying%20keypoint%20tracking%20to%20estimate%20gyroscope%20bias%0Awith%20normal%20epipolar%20constraints.%20To%20address%20the%20limitations%20of%20previous%0Amethods%2C%20this%20paper%20proposes%20a%20method%20aimed%20at%20enhancing%20translation%20accuracy%0Aduring%20the%20initialization%20stage.%20The%20fundamental%20concept%20of%20our%20method%20is%20to%0Aimprove%20the%20translation%20estimate%20with%20a%203%20Degree-of-Freedom%20%28DoF%29%20Bundle%0AAdjustment%20%28BA%29%2C%20independently%2C%20while%20the%20rotation%20estimate%20is%20fixed%2C%20instead%0Aof%20using%20ORB-SLAM3%27s%206-DoF%20BA.%20Additionally%2C%20the%20rotation%20estimate%20will%20be%0Aupdated%20by%20considering%20IMU%20measurements%20and%20gyroscope%20bias%2C%20unlike%20ORB-SLAM3%27s%0Arotation%2C%20which%20is%20directly%20obtained%20from%20stereo%20visual%20odometry%20and%20may%20yield%0Ainferior%20results%20when%20operating%20in%20challenging%20scenarios.%20We%20also%20conduct%0Aextensive%20evaluations%20on%20the%20public%20benchmark%2C%20the%20EuRoC%20dataset%2C%20demonstrating%0Athat%20our%20method%20excels%20in%20accuracy.%0A&entry.1838667208=http%3A//arxiv.org/abs/2405.15082v3&entry.124074799=Read"},
{"title": "Asynchronous Multi-Server Federated Learning for Geo-Distributed Clients", "author": "Yuncong Zuo and Bart Cox and Lydia Y. Chen and J\u00e9r\u00e9mie Decouchant", "abstract": "  Federated learning (FL) systems enable multiple clients to train a machine\nlearning model iteratively through synchronously exchanging the intermediate\nmodel weights with a single server. The scalability of such FL systems can be\nlimited by two factors: server idle time due to synchronous communication and\nthe risk of a single server becoming the bottleneck. In this paper, we propose\na new FL architecture, to our knowledge, the first multi-server FL system that\nis entirely asynchronous, and therefore addresses these two limitations\nsimultaneously. Our solution keeps both servers and clients continuously\nactive. As in previous multi-server methods, clients interact solely with their\nnearest server, ensuring efficient update integration into the model.\nDifferently, however, servers also periodically update each other\nasynchronously, and never postpone interactions with clients. We compare our\nsolution to three representative baselines - FedAvg, FedAsync and HierFAVG - on\nthe MNIST and CIFAR-10 image classification datasets and on the WikiText-2\nlanguage modeling dataset. Our solution converges to similar or higher accuracy\nlevels than previous baselines and requires 61% less time to do so in\ngeo-distributed settings.\n", "link": "http://arxiv.org/abs/2406.01439v2", "date": "2024-06-20", "relevancy": 2.2329, "topK": [{"title": "DDM-NET: End-to-end learning of keypoint feature Detection, Description and Matching for 3D localization", "link": "https://arxiv.org/abs/2212.04575", "similarity": 0.4522}, {"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.4455}, {"title": "Dynamic Gaussians Mesh: Consistent Mesh Reconstruction from Monocular\n  Videos", "link": "http://arxiv.org/abs/2404.12379v2", "similarity": 0.442}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Asynchronous%20Multi-Server%20Federated%20Learning%20for%20Geo-Distributed%20Clients&body=Title%3A%20Asynchronous%20Multi-Server%20Federated%20Learning%20for%20Geo-Distributed%20Clients%0AAuthor%3A%20Yuncong%20Zuo%20and%20Bart%20Cox%20and%20Lydia%20Y.%20Chen%20and%20J%C3%A9r%C3%A9mie%20Decouchant%0AAbstract%3A%20%20%20Federated%20learning%20%28FL%29%20systems%20enable%20multiple%20clients%20to%20train%20a%20machine%0Alearning%20model%20iteratively%20through%20synchronously%20exchanging%20the%20intermediate%0Amodel%20weights%20with%20a%20single%20server.%20The%20scalability%20of%20such%20FL%20systems%20can%20be%0Alimited%20by%20two%20factors%3A%20server%20idle%20time%20due%20to%20synchronous%20communication%20and%0Athe%20risk%20of%20a%20single%20server%20becoming%20the%20bottleneck.%20In%20this%20paper%2C%20we%20propose%0Aa%20new%20FL%20architecture%2C%20to%20our%20knowledge%2C%20the%20first%20multi-server%20FL%20system%20that%0Ais%20entirely%20asynchronous%2C%20and%20therefore%20addresses%20these%20two%20limitations%0Asimultaneously.%20Our%20solution%20keeps%20both%20servers%20and%20clients%20continuously%0Aactive.%20As%20in%20previous%20multi-server%20methods%2C%20clients%20interact%20solely%20with%20their%0Anearest%20server%2C%20ensuring%20efficient%20update%20integration%20into%20the%20model.%0ADifferently%2C%20however%2C%20servers%20also%20periodically%20update%20each%20other%0Aasynchronously%2C%20and%20never%20postpone%20interactions%20with%20clients.%20We%20compare%20our%0Asolution%20to%20three%20representative%20baselines%20-%20FedAvg%2C%20FedAsync%20and%20HierFAVG%20-%20on%0Athe%20MNIST%20and%20CIFAR-10%20image%20classification%20datasets%20and%20on%20the%20WikiText-2%0Alanguage%20modeling%20dataset.%20Our%20solution%20converges%20to%20similar%20or%20higher%20accuracy%0Alevels%20than%20previous%20baselines%20and%20requires%2061%25%20less%20time%20to%20do%20so%20in%0Ageo-distributed%20settings.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2406.01439v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DAsynchronous%2520Multi-Server%2520Federated%2520Learning%2520for%2520Geo-Distributed%2520Clients%26entry.906535625%3DYuncong%2520Zuo%2520and%2520Bart%2520Cox%2520and%2520Lydia%2520Y.%2520Chen%2520and%2520J%25C3%25A9r%25C3%25A9mie%2520Decouchant%26entry.1292438233%3D%2520%2520Federated%2520learning%2520%2528FL%2529%2520systems%2520enable%2520multiple%2520clients%2520to%2520train%2520a%2520machine%250Alearning%2520model%2520iteratively%2520through%2520synchronously%2520exchanging%2520the%2520intermediate%250Amodel%2520weights%2520with%2520a%2520single%2520server.%2520The%2520scalability%2520of%2520such%2520FL%2520systems%2520can%2520be%250Alimited%2520by%2520two%2520factors%253A%2520server%2520idle%2520time%2520due%2520to%2520synchronous%2520communication%2520and%250Athe%2520risk%2520of%2520a%2520single%2520server%2520becoming%2520the%2520bottleneck.%2520In%2520this%2520paper%252C%2520we%2520propose%250Aa%2520new%2520FL%2520architecture%252C%2520to%2520our%2520knowledge%252C%2520the%2520first%2520multi-server%2520FL%2520system%2520that%250Ais%2520entirely%2520asynchronous%252C%2520and%2520therefore%2520addresses%2520these%2520two%2520limitations%250Asimultaneously.%2520Our%2520solution%2520keeps%2520both%2520servers%2520and%2520clients%2520continuously%250Aactive.%2520As%2520in%2520previous%2520multi-server%2520methods%252C%2520clients%2520interact%2520solely%2520with%2520their%250Anearest%2520server%252C%2520ensuring%2520efficient%2520update%2520integration%2520into%2520the%2520model.%250ADifferently%252C%2520however%252C%2520servers%2520also%2520periodically%2520update%2520each%2520other%250Aasynchronously%252C%2520and%2520never%2520postpone%2520interactions%2520with%2520clients.%2520We%2520compare%2520our%250Asolution%2520to%2520three%2520representative%2520baselines%2520-%2520FedAvg%252C%2520FedAsync%2520and%2520HierFAVG%2520-%2520on%250Athe%2520MNIST%2520and%2520CIFAR-10%2520image%2520classification%2520datasets%2520and%2520on%2520the%2520WikiText-2%250Alanguage%2520modeling%2520dataset.%2520Our%2520solution%2520converges%2520to%2520similar%2520or%2520higher%2520accuracy%250Alevels%2520than%2520previous%2520baselines%2520and%2520requires%252061%2525%2520less%2520time%2520to%2520do%2520so%2520in%250Ageo-distributed%2520settings.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2406.01439v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Asynchronous%20Multi-Server%20Federated%20Learning%20for%20Geo-Distributed%20Clients&entry.906535625=Yuncong%20Zuo%20and%20Bart%20Cox%20and%20Lydia%20Y.%20Chen%20and%20J%C3%A9r%C3%A9mie%20Decouchant&entry.1292438233=%20%20Federated%20learning%20%28FL%29%20systems%20enable%20multiple%20clients%20to%20train%20a%20machine%0Alearning%20model%20iteratively%20through%20synchronously%20exchanging%20the%20intermediate%0Amodel%20weights%20with%20a%20single%20server.%20The%20scalability%20of%20such%20FL%20systems%20can%20be%0Alimited%20by%20two%20factors%3A%20server%20idle%20time%20due%20to%20synchronous%20communication%20and%0Athe%20risk%20of%20a%20single%20server%20becoming%20the%20bottleneck.%20In%20this%20paper%2C%20we%20propose%0Aa%20new%20FL%20architecture%2C%20to%20our%20knowledge%2C%20the%20first%20multi-server%20FL%20system%20that%0Ais%20entirely%20asynchronous%2C%20and%20therefore%20addresses%20these%20two%20limitations%0Asimultaneously.%20Our%20solution%20keeps%20both%20servers%20and%20clients%20continuously%0Aactive.%20As%20in%20previous%20multi-server%20methods%2C%20clients%20interact%20solely%20with%20their%0Anearest%20server%2C%20ensuring%20efficient%20update%20integration%20into%20the%20model.%0ADifferently%2C%20however%2C%20servers%20also%20periodically%20update%20each%20other%0Aasynchronously%2C%20and%20never%20postpone%20interactions%20with%20clients.%20We%20compare%20our%0Asolution%20to%20three%20representative%20baselines%20-%20FedAvg%2C%20FedAsync%20and%20HierFAVG%20-%20on%0Athe%20MNIST%20and%20CIFAR-10%20image%20classification%20datasets%20and%20on%20the%20WikiText-2%0Alanguage%20modeling%20dataset.%20Our%20solution%20converges%20to%20similar%20or%20higher%20accuracy%0Alevels%20than%20previous%20baselines%20and%20requires%2061%25%20less%20time%20to%20do%20so%20in%0Ageo-distributed%20settings.%0A&entry.1838667208=http%3A//arxiv.org/abs/2406.01439v2&entry.124074799=Read"},
{"title": "Stable Phase Retrieval with Mirror Descent", "author": "Jean-Jacques Godeme and Jalal Fadili and Claude Amra and Myriam Zerrad", "abstract": "  In this paper, we aim to reconstruct an n-dimensional real vector from m\nphaseless measurements corrupted by an additive noise. We extend the noiseless\nframework developed in [15], based on mirror descent (or Bregman gradient\ndescent), to deal with noisy measurements and prove that the procedure is\nstable to (small enough) additive noise. In the deterministic case, we show\nthat mirror descent converges to a critical point of the phase retrieval\nproblem, and if the algorithm is well initialized and the noise is small\nenough, the critical point is near the true vector up to a global sign change.\nWhen the measurements are i.i.d Gaussian and the signal-to-noise ratio is large\nenough, we provide global convergence guarantees that ensure that with high\nprobability, mirror descent converges to a global minimizer near the true\nvector (up to a global sign change), as soon as the number of measurements m is\nlarge enough. The sample complexity bound can be improved if a spectral method\nis used to provide a good initial guess. We complement our theoretical study\nwith several numerical results showing that mirror descent is both a\ncomputationally and statistically efficient scheme to solve the phase retrieval\nproblem.\n", "link": "http://arxiv.org/abs/2405.10754v2", "date": "2024-06-20", "relevancy": 2.2267, "topK": [{"title": "Tuning-Free Noise Rectification for High Fidelity Image-to-Video\n  Generation", "link": "http://arxiv.org/abs/2403.02827v1", "similarity": 0.4496}, {"title": "Dynamic Gaussians Mesh: Consistent Mesh Reconstruction from Monocular\n  Videos", "link": "http://arxiv.org/abs/2404.12379v2", "similarity": 0.4486}, {"title": "DDM-NET: End-to-end learning of keypoint feature Detection, Description and Matching for 3D localization", "link": "https://arxiv.org/abs/2212.04575", "similarity": 0.4378}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Stable%20Phase%20Retrieval%20with%20Mirror%20Descent&body=Title%3A%20Stable%20Phase%20Retrieval%20with%20Mirror%20Descent%0AAuthor%3A%20Jean-Jacques%20Godeme%20and%20Jalal%20Fadili%20and%20Claude%20Amra%20and%20Myriam%20Zerrad%0AAbstract%3A%20%20%20In%20this%20paper%2C%20we%20aim%20to%20reconstruct%20an%20n-dimensional%20real%20vector%20from%20m%0Aphaseless%20measurements%20corrupted%20by%20an%20additive%20noise.%20We%20extend%20the%20noiseless%0Aframework%20developed%20in%20%5B15%5D%2C%20based%20on%20mirror%20descent%20%28or%20Bregman%20gradient%0Adescent%29%2C%20to%20deal%20with%20noisy%20measurements%20and%20prove%20that%20the%20procedure%20is%0Astable%20to%20%28small%20enough%29%20additive%20noise.%20In%20the%20deterministic%20case%2C%20we%20show%0Athat%20mirror%20descent%20converges%20to%20a%20critical%20point%20of%20the%20phase%20retrieval%0Aproblem%2C%20and%20if%20the%20algorithm%20is%20well%20initialized%20and%20the%20noise%20is%20small%0Aenough%2C%20the%20critical%20point%20is%20near%20the%20true%20vector%20up%20to%20a%20global%20sign%20change.%0AWhen%20the%20measurements%20are%20i.i.d%20Gaussian%20and%20the%20signal-to-noise%20ratio%20is%20large%0Aenough%2C%20we%20provide%20global%20convergence%20guarantees%20that%20ensure%20that%20with%20high%0Aprobability%2C%20mirror%20descent%20converges%20to%20a%20global%20minimizer%20near%20the%20true%0Avector%20%28up%20to%20a%20global%20sign%20change%29%2C%20as%20soon%20as%20the%20number%20of%20measurements%20m%20is%0Alarge%20enough.%20The%20sample%20complexity%20bound%20can%20be%20improved%20if%20a%20spectral%20method%0Ais%20used%20to%20provide%20a%20good%20initial%20guess.%20We%20complement%20our%20theoretical%20study%0Awith%20several%20numerical%20results%20showing%20that%20mirror%20descent%20is%20both%20a%0Acomputationally%20and%20statistically%20efficient%20scheme%20to%20solve%20the%20phase%20retrieval%0Aproblem.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2405.10754v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DStable%2520Phase%2520Retrieval%2520with%2520Mirror%2520Descent%26entry.906535625%3DJean-Jacques%2520Godeme%2520and%2520Jalal%2520Fadili%2520and%2520Claude%2520Amra%2520and%2520Myriam%2520Zerrad%26entry.1292438233%3D%2520%2520In%2520this%2520paper%252C%2520we%2520aim%2520to%2520reconstruct%2520an%2520n-dimensional%2520real%2520vector%2520from%2520m%250Aphaseless%2520measurements%2520corrupted%2520by%2520an%2520additive%2520noise.%2520We%2520extend%2520the%2520noiseless%250Aframework%2520developed%2520in%2520%255B15%255D%252C%2520based%2520on%2520mirror%2520descent%2520%2528or%2520Bregman%2520gradient%250Adescent%2529%252C%2520to%2520deal%2520with%2520noisy%2520measurements%2520and%2520prove%2520that%2520the%2520procedure%2520is%250Astable%2520to%2520%2528small%2520enough%2529%2520additive%2520noise.%2520In%2520the%2520deterministic%2520case%252C%2520we%2520show%250Athat%2520mirror%2520descent%2520converges%2520to%2520a%2520critical%2520point%2520of%2520the%2520phase%2520retrieval%250Aproblem%252C%2520and%2520if%2520the%2520algorithm%2520is%2520well%2520initialized%2520and%2520the%2520noise%2520is%2520small%250Aenough%252C%2520the%2520critical%2520point%2520is%2520near%2520the%2520true%2520vector%2520up%2520to%2520a%2520global%2520sign%2520change.%250AWhen%2520the%2520measurements%2520are%2520i.i.d%2520Gaussian%2520and%2520the%2520signal-to-noise%2520ratio%2520is%2520large%250Aenough%252C%2520we%2520provide%2520global%2520convergence%2520guarantees%2520that%2520ensure%2520that%2520with%2520high%250Aprobability%252C%2520mirror%2520descent%2520converges%2520to%2520a%2520global%2520minimizer%2520near%2520the%2520true%250Avector%2520%2528up%2520to%2520a%2520global%2520sign%2520change%2529%252C%2520as%2520soon%2520as%2520the%2520number%2520of%2520measurements%2520m%2520is%250Alarge%2520enough.%2520The%2520sample%2520complexity%2520bound%2520can%2520be%2520improved%2520if%2520a%2520spectral%2520method%250Ais%2520used%2520to%2520provide%2520a%2520good%2520initial%2520guess.%2520We%2520complement%2520our%2520theoretical%2520study%250Awith%2520several%2520numerical%2520results%2520showing%2520that%2520mirror%2520descent%2520is%2520both%2520a%250Acomputationally%2520and%2520statistically%2520efficient%2520scheme%2520to%2520solve%2520the%2520phase%2520retrieval%250Aproblem.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2405.10754v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Stable%20Phase%20Retrieval%20with%20Mirror%20Descent&entry.906535625=Jean-Jacques%20Godeme%20and%20Jalal%20Fadili%20and%20Claude%20Amra%20and%20Myriam%20Zerrad&entry.1292438233=%20%20In%20this%20paper%2C%20we%20aim%20to%20reconstruct%20an%20n-dimensional%20real%20vector%20from%20m%0Aphaseless%20measurements%20corrupted%20by%20an%20additive%20noise.%20We%20extend%20the%20noiseless%0Aframework%20developed%20in%20%5B15%5D%2C%20based%20on%20mirror%20descent%20%28or%20Bregman%20gradient%0Adescent%29%2C%20to%20deal%20with%20noisy%20measurements%20and%20prove%20that%20the%20procedure%20is%0Astable%20to%20%28small%20enough%29%20additive%20noise.%20In%20the%20deterministic%20case%2C%20we%20show%0Athat%20mirror%20descent%20converges%20to%20a%20critical%20point%20of%20the%20phase%20retrieval%0Aproblem%2C%20and%20if%20the%20algorithm%20is%20well%20initialized%20and%20the%20noise%20is%20small%0Aenough%2C%20the%20critical%20point%20is%20near%20the%20true%20vector%20up%20to%20a%20global%20sign%20change.%0AWhen%20the%20measurements%20are%20i.i.d%20Gaussian%20and%20the%20signal-to-noise%20ratio%20is%20large%0Aenough%2C%20we%20provide%20global%20convergence%20guarantees%20that%20ensure%20that%20with%20high%0Aprobability%2C%20mirror%20descent%20converges%20to%20a%20global%20minimizer%20near%20the%20true%0Avector%20%28up%20to%20a%20global%20sign%20change%29%2C%20as%20soon%20as%20the%20number%20of%20measurements%20m%20is%0Alarge%20enough.%20The%20sample%20complexity%20bound%20can%20be%20improved%20if%20a%20spectral%20method%0Ais%20used%20to%20provide%20a%20good%20initial%20guess.%20We%20complement%20our%20theoretical%20study%0Awith%20several%20numerical%20results%20showing%20that%20mirror%20descent%20is%20both%20a%0Acomputationally%20and%20statistically%20efficient%20scheme%20to%20solve%20the%20phase%20retrieval%0Aproblem.%0A&entry.1838667208=http%3A//arxiv.org/abs/2405.10754v2&entry.124074799=Read"},
{"title": "Evaluation of Deep Learning Semantic Segmentation for Land Cover Mapping\n  on Multispectral, Hyperspectral and High Spatial Aerial Imagery", "author": "Ilham Adi Panuntun and Ying-Nong Chen and Ilham Jamaluddin and Thi Linh Chi Tran", "abstract": "  In the rise of climate change, land cover mapping has become such an urgent\nneed in environmental monitoring. The accuracy of land cover classification has\ngotten increasingly based on the improvement of remote sensing data. Land cover\nclassification using satellite imageries has been explored and become more\nprevalent in recent years, but the methodologies remain some drawbacks of\nsubjective and time-consuming. Some deep learning techniques have been utilized\nto overcome these limitations. However, most studies implemented just one image\ntype to evaluate algorithms for land cover mapping. Therefore, our study\nconducted deep learning semantic segmentation in multispectral, hyperspectral,\nand high spatial aerial image datasets for landcover mapping. This research\nimplemented a semantic segmentation method such as Unet, Linknet, FPN, and\nPSPnet for categorizing vegetation, water, and others (i.e., soil and\nimpervious surface). The LinkNet model obtained high accuracy in IoU\n(Intersection Over Union) at 0.92 in all datasets, which is comparable with\nother mentioned techniques. In evaluation with different image types, the\nmultispectral images showed higher performance with the IoU, and F1-score are\n0.993 and 0.997, respectively. Our outcome highlighted the efficiency and broad\napplicability of LinkNet and multispectral image on land cover classification.\nThis research contributes to establishing an approach on landcover segmentation\nvia open source for long-term future application.\n", "link": "http://arxiv.org/abs/2406.14220v1", "date": "2024-06-20", "relevancy": 2.2231, "topK": [{"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.5898}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.5559}, {"title": "Self-supervised Photographic Image Layout Representation Learning", "link": "http://arxiv.org/abs/2403.03740v1", "similarity": 0.5217}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Evaluation%20of%20Deep%20Learning%20Semantic%20Segmentation%20for%20Land%20Cover%20Mapping%0A%20%20on%20Multispectral%2C%20Hyperspectral%20and%20High%20Spatial%20Aerial%20Imagery&body=Title%3A%20Evaluation%20of%20Deep%20Learning%20Semantic%20Segmentation%20for%20Land%20Cover%20Mapping%0A%20%20on%20Multispectral%2C%20Hyperspectral%20and%20High%20Spatial%20Aerial%20Imagery%0AAuthor%3A%20Ilham%20Adi%20Panuntun%20and%20Ying-Nong%20Chen%20and%20Ilham%20Jamaluddin%20and%20Thi%20Linh%20Chi%20Tran%0AAbstract%3A%20%20%20In%20the%20rise%20of%20climate%20change%2C%20land%20cover%20mapping%20has%20become%20such%20an%20urgent%0Aneed%20in%20environmental%20monitoring.%20The%20accuracy%20of%20land%20cover%20classification%20has%0Agotten%20increasingly%20based%20on%20the%20improvement%20of%20remote%20sensing%20data.%20Land%20cover%0Aclassification%20using%20satellite%20imageries%20has%20been%20explored%20and%20become%20more%0Aprevalent%20in%20recent%20years%2C%20but%20the%20methodologies%20remain%20some%20drawbacks%20of%0Asubjective%20and%20time-consuming.%20Some%20deep%20learning%20techniques%20have%20been%20utilized%0Ato%20overcome%20these%20limitations.%20However%2C%20most%20studies%20implemented%20just%20one%20image%0Atype%20to%20evaluate%20algorithms%20for%20land%20cover%20mapping.%20Therefore%2C%20our%20study%0Aconducted%20deep%20learning%20semantic%20segmentation%20in%20multispectral%2C%20hyperspectral%2C%0Aand%20high%20spatial%20aerial%20image%20datasets%20for%20landcover%20mapping.%20This%20research%0Aimplemented%20a%20semantic%20segmentation%20method%20such%20as%20Unet%2C%20Linknet%2C%20FPN%2C%20and%0APSPnet%20for%20categorizing%20vegetation%2C%20water%2C%20and%20others%20%28i.e.%2C%20soil%20and%0Aimpervious%20surface%29.%20The%20LinkNet%20model%20obtained%20high%20accuracy%20in%20IoU%0A%28Intersection%20Over%20Union%29%20at%200.92%20in%20all%20datasets%2C%20which%20is%20comparable%20with%0Aother%20mentioned%20techniques.%20In%20evaluation%20with%20different%20image%20types%2C%20the%0Amultispectral%20images%20showed%20higher%20performance%20with%20the%20IoU%2C%20and%20F1-score%20are%0A0.993%20and%200.997%2C%20respectively.%20Our%20outcome%20highlighted%20the%20efficiency%20and%20broad%0Aapplicability%20of%20LinkNet%20and%20multispectral%20image%20on%20land%20cover%20classification.%0AThis%20research%20contributes%20to%20establishing%20an%20approach%20on%20landcover%20segmentation%0Avia%20open%20source%20for%20long-term%20future%20application.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2406.14220v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DEvaluation%2520of%2520Deep%2520Learning%2520Semantic%2520Segmentation%2520for%2520Land%2520Cover%2520Mapping%250A%2520%2520on%2520Multispectral%252C%2520Hyperspectral%2520and%2520High%2520Spatial%2520Aerial%2520Imagery%26entry.906535625%3DIlham%2520Adi%2520Panuntun%2520and%2520Ying-Nong%2520Chen%2520and%2520Ilham%2520Jamaluddin%2520and%2520Thi%2520Linh%2520Chi%2520Tran%26entry.1292438233%3D%2520%2520In%2520the%2520rise%2520of%2520climate%2520change%252C%2520land%2520cover%2520mapping%2520has%2520become%2520such%2520an%2520urgent%250Aneed%2520in%2520environmental%2520monitoring.%2520The%2520accuracy%2520of%2520land%2520cover%2520classification%2520has%250Agotten%2520increasingly%2520based%2520on%2520the%2520improvement%2520of%2520remote%2520sensing%2520data.%2520Land%2520cover%250Aclassification%2520using%2520satellite%2520imageries%2520has%2520been%2520explored%2520and%2520become%2520more%250Aprevalent%2520in%2520recent%2520years%252C%2520but%2520the%2520methodologies%2520remain%2520some%2520drawbacks%2520of%250Asubjective%2520and%2520time-consuming.%2520Some%2520deep%2520learning%2520techniques%2520have%2520been%2520utilized%250Ato%2520overcome%2520these%2520limitations.%2520However%252C%2520most%2520studies%2520implemented%2520just%2520one%2520image%250Atype%2520to%2520evaluate%2520algorithms%2520for%2520land%2520cover%2520mapping.%2520Therefore%252C%2520our%2520study%250Aconducted%2520deep%2520learning%2520semantic%2520segmentation%2520in%2520multispectral%252C%2520hyperspectral%252C%250Aand%2520high%2520spatial%2520aerial%2520image%2520datasets%2520for%2520landcover%2520mapping.%2520This%2520research%250Aimplemented%2520a%2520semantic%2520segmentation%2520method%2520such%2520as%2520Unet%252C%2520Linknet%252C%2520FPN%252C%2520and%250APSPnet%2520for%2520categorizing%2520vegetation%252C%2520water%252C%2520and%2520others%2520%2528i.e.%252C%2520soil%2520and%250Aimpervious%2520surface%2529.%2520The%2520LinkNet%2520model%2520obtained%2520high%2520accuracy%2520in%2520IoU%250A%2528Intersection%2520Over%2520Union%2529%2520at%25200.92%2520in%2520all%2520datasets%252C%2520which%2520is%2520comparable%2520with%250Aother%2520mentioned%2520techniques.%2520In%2520evaluation%2520with%2520different%2520image%2520types%252C%2520the%250Amultispectral%2520images%2520showed%2520higher%2520performance%2520with%2520the%2520IoU%252C%2520and%2520F1-score%2520are%250A0.993%2520and%25200.997%252C%2520respectively.%2520Our%2520outcome%2520highlighted%2520the%2520efficiency%2520and%2520broad%250Aapplicability%2520of%2520LinkNet%2520and%2520multispectral%2520image%2520on%2520land%2520cover%2520classification.%250AThis%2520research%2520contributes%2520to%2520establishing%2520an%2520approach%2520on%2520landcover%2520segmentation%250Avia%2520open%2520source%2520for%2520long-term%2520future%2520application.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2406.14220v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Evaluation%20of%20Deep%20Learning%20Semantic%20Segmentation%20for%20Land%20Cover%20Mapping%0A%20%20on%20Multispectral%2C%20Hyperspectral%20and%20High%20Spatial%20Aerial%20Imagery&entry.906535625=Ilham%20Adi%20Panuntun%20and%20Ying-Nong%20Chen%20and%20Ilham%20Jamaluddin%20and%20Thi%20Linh%20Chi%20Tran&entry.1292438233=%20%20In%20the%20rise%20of%20climate%20change%2C%20land%20cover%20mapping%20has%20become%20such%20an%20urgent%0Aneed%20in%20environmental%20monitoring.%20The%20accuracy%20of%20land%20cover%20classification%20has%0Agotten%20increasingly%20based%20on%20the%20improvement%20of%20remote%20sensing%20data.%20Land%20cover%0Aclassification%20using%20satellite%20imageries%20has%20been%20explored%20and%20become%20more%0Aprevalent%20in%20recent%20years%2C%20but%20the%20methodologies%20remain%20some%20drawbacks%20of%0Asubjective%20and%20time-consuming.%20Some%20deep%20learning%20techniques%20have%20been%20utilized%0Ato%20overcome%20these%20limitations.%20However%2C%20most%20studies%20implemented%20just%20one%20image%0Atype%20to%20evaluate%20algorithms%20for%20land%20cover%20mapping.%20Therefore%2C%20our%20study%0Aconducted%20deep%20learning%20semantic%20segmentation%20in%20multispectral%2C%20hyperspectral%2C%0Aand%20high%20spatial%20aerial%20image%20datasets%20for%20landcover%20mapping.%20This%20research%0Aimplemented%20a%20semantic%20segmentation%20method%20such%20as%20Unet%2C%20Linknet%2C%20FPN%2C%20and%0APSPnet%20for%20categorizing%20vegetation%2C%20water%2C%20and%20others%20%28i.e.%2C%20soil%20and%0Aimpervious%20surface%29.%20The%20LinkNet%20model%20obtained%20high%20accuracy%20in%20IoU%0A%28Intersection%20Over%20Union%29%20at%200.92%20in%20all%20datasets%2C%20which%20is%20comparable%20with%0Aother%20mentioned%20techniques.%20In%20evaluation%20with%20different%20image%20types%2C%20the%0Amultispectral%20images%20showed%20higher%20performance%20with%20the%20IoU%2C%20and%20F1-score%20are%0A0.993%20and%200.997%2C%20respectively.%20Our%20outcome%20highlighted%20the%20efficiency%20and%20broad%0Aapplicability%20of%20LinkNet%20and%20multispectral%20image%20on%20land%20cover%20classification.%0AThis%20research%20contributes%20to%20establishing%20an%20approach%20on%20landcover%20segmentation%0Avia%20open%20source%20for%20long-term%20future%20application.%0A&entry.1838667208=http%3A//arxiv.org/abs/2406.14220v1&entry.124074799=Read"},
{"title": "Benchmarking Monocular 3D Dog Pose Estimation Using In-The-Wild Motion\n  Capture Data", "author": "Moira Shooter and Charles Malleson and Adrian Hilton", "abstract": "  We introduce a new benchmark analysis focusing on 3D canine pose estimation\nfrom monocular in-the-wild images. A multi-modal dataset 3DDogs-Lab was\ncaptured indoors, featuring various dog breeds trotting on a walkway. It\nincludes data from optical marker-based mocap systems, RGBD cameras, IMUs, and\na pressure mat. While providing high-quality motion data, the presence of\noptical markers and limited background diversity make the captured video less\nrepresentative of real-world conditions. To address this, we created\n3DDogs-Wild, a naturalised version of the dataset where the optical markers are\nin-painted and the subjects are placed in diverse environments, enhancing its\nutility for training RGB image-based pose detectors. We show that using the\n3DDogs-Wild to train the models leads to improved performance when evaluating\non in-the-wild data. Additionally, we provide a thorough analysis using various\npose estimation models, revealing their respective strengths and weaknesses. We\nbelieve that our findings, coupled with the datasets provided, offer valuable\ninsights for advancing 3D animal pose estimation.\n", "link": "http://arxiv.org/abs/2406.14412v1", "date": "2024-06-20", "relevancy": 2.2223, "topK": [{"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.5687}, {"title": "DDM-NET: End-to-end learning of keypoint feature Detection, Description and Matching for 3D localization", "link": "https://arxiv.org/abs/2212.04575", "similarity": 0.5549}, {"title": "CAT3D: Create Anything in 3D with Multi-View Diffusion Models", "link": "http://arxiv.org/abs/2405.10314v1", "similarity": 0.551}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Benchmarking%20Monocular%203D%20Dog%20Pose%20Estimation%20Using%20In-The-Wild%20Motion%0A%20%20Capture%20Data&body=Title%3A%20Benchmarking%20Monocular%203D%20Dog%20Pose%20Estimation%20Using%20In-The-Wild%20Motion%0A%20%20Capture%20Data%0AAuthor%3A%20Moira%20Shooter%20and%20Charles%20Malleson%20and%20Adrian%20Hilton%0AAbstract%3A%20%20%20We%20introduce%20a%20new%20benchmark%20analysis%20focusing%20on%203D%20canine%20pose%20estimation%0Afrom%20monocular%20in-the-wild%20images.%20A%20multi-modal%20dataset%203DDogs-Lab%20was%0Acaptured%20indoors%2C%20featuring%20various%20dog%20breeds%20trotting%20on%20a%20walkway.%20It%0Aincludes%20data%20from%20optical%20marker-based%20mocap%20systems%2C%20RGBD%20cameras%2C%20IMUs%2C%20and%0Aa%20pressure%20mat.%20While%20providing%20high-quality%20motion%20data%2C%20the%20presence%20of%0Aoptical%20markers%20and%20limited%20background%20diversity%20make%20the%20captured%20video%20less%0Arepresentative%20of%20real-world%20conditions.%20To%20address%20this%2C%20we%20created%0A3DDogs-Wild%2C%20a%20naturalised%20version%20of%20the%20dataset%20where%20the%20optical%20markers%20are%0Ain-painted%20and%20the%20subjects%20are%20placed%20in%20diverse%20environments%2C%20enhancing%20its%0Autility%20for%20training%20RGB%20image-based%20pose%20detectors.%20We%20show%20that%20using%20the%0A3DDogs-Wild%20to%20train%20the%20models%20leads%20to%20improved%20performance%20when%20evaluating%0Aon%20in-the-wild%20data.%20Additionally%2C%20we%20provide%20a%20thorough%20analysis%20using%20various%0Apose%20estimation%20models%2C%20revealing%20their%20respective%20strengths%20and%20weaknesses.%20We%0Abelieve%20that%20our%20findings%2C%20coupled%20with%20the%20datasets%20provided%2C%20offer%20valuable%0Ainsights%20for%20advancing%203D%20animal%20pose%20estimation.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2406.14412v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DBenchmarking%2520Monocular%25203D%2520Dog%2520Pose%2520Estimation%2520Using%2520In-The-Wild%2520Motion%250A%2520%2520Capture%2520Data%26entry.906535625%3DMoira%2520Shooter%2520and%2520Charles%2520Malleson%2520and%2520Adrian%2520Hilton%26entry.1292438233%3D%2520%2520We%2520introduce%2520a%2520new%2520benchmark%2520analysis%2520focusing%2520on%25203D%2520canine%2520pose%2520estimation%250Afrom%2520monocular%2520in-the-wild%2520images.%2520A%2520multi-modal%2520dataset%25203DDogs-Lab%2520was%250Acaptured%2520indoors%252C%2520featuring%2520various%2520dog%2520breeds%2520trotting%2520on%2520a%2520walkway.%2520It%250Aincludes%2520data%2520from%2520optical%2520marker-based%2520mocap%2520systems%252C%2520RGBD%2520cameras%252C%2520IMUs%252C%2520and%250Aa%2520pressure%2520mat.%2520While%2520providing%2520high-quality%2520motion%2520data%252C%2520the%2520presence%2520of%250Aoptical%2520markers%2520and%2520limited%2520background%2520diversity%2520make%2520the%2520captured%2520video%2520less%250Arepresentative%2520of%2520real-world%2520conditions.%2520To%2520address%2520this%252C%2520we%2520created%250A3DDogs-Wild%252C%2520a%2520naturalised%2520version%2520of%2520the%2520dataset%2520where%2520the%2520optical%2520markers%2520are%250Ain-painted%2520and%2520the%2520subjects%2520are%2520placed%2520in%2520diverse%2520environments%252C%2520enhancing%2520its%250Autility%2520for%2520training%2520RGB%2520image-based%2520pose%2520detectors.%2520We%2520show%2520that%2520using%2520the%250A3DDogs-Wild%2520to%2520train%2520the%2520models%2520leads%2520to%2520improved%2520performance%2520when%2520evaluating%250Aon%2520in-the-wild%2520data.%2520Additionally%252C%2520we%2520provide%2520a%2520thorough%2520analysis%2520using%2520various%250Apose%2520estimation%2520models%252C%2520revealing%2520their%2520respective%2520strengths%2520and%2520weaknesses.%2520We%250Abelieve%2520that%2520our%2520findings%252C%2520coupled%2520with%2520the%2520datasets%2520provided%252C%2520offer%2520valuable%250Ainsights%2520for%2520advancing%25203D%2520animal%2520pose%2520estimation.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2406.14412v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Benchmarking%20Monocular%203D%20Dog%20Pose%20Estimation%20Using%20In-The-Wild%20Motion%0A%20%20Capture%20Data&entry.906535625=Moira%20Shooter%20and%20Charles%20Malleson%20and%20Adrian%20Hilton&entry.1292438233=%20%20We%20introduce%20a%20new%20benchmark%20analysis%20focusing%20on%203D%20canine%20pose%20estimation%0Afrom%20monocular%20in-the-wild%20images.%20A%20multi-modal%20dataset%203DDogs-Lab%20was%0Acaptured%20indoors%2C%20featuring%20various%20dog%20breeds%20trotting%20on%20a%20walkway.%20It%0Aincludes%20data%20from%20optical%20marker-based%20mocap%20systems%2C%20RGBD%20cameras%2C%20IMUs%2C%20and%0Aa%20pressure%20mat.%20While%20providing%20high-quality%20motion%20data%2C%20the%20presence%20of%0Aoptical%20markers%20and%20limited%20background%20diversity%20make%20the%20captured%20video%20less%0Arepresentative%20of%20real-world%20conditions.%20To%20address%20this%2C%20we%20created%0A3DDogs-Wild%2C%20a%20naturalised%20version%20of%20the%20dataset%20where%20the%20optical%20markers%20are%0Ain-painted%20and%20the%20subjects%20are%20placed%20in%20diverse%20environments%2C%20enhancing%20its%0Autility%20for%20training%20RGB%20image-based%20pose%20detectors.%20We%20show%20that%20using%20the%0A3DDogs-Wild%20to%20train%20the%20models%20leads%20to%20improved%20performance%20when%20evaluating%0Aon%20in-the-wild%20data.%20Additionally%2C%20we%20provide%20a%20thorough%20analysis%20using%20various%0Apose%20estimation%20models%2C%20revealing%20their%20respective%20strengths%20and%20weaknesses.%20We%0Abelieve%20that%20our%20findings%2C%20coupled%20with%20the%20datasets%20provided%2C%20offer%20valuable%0Ainsights%20for%20advancing%203D%20animal%20pose%20estimation.%0A&entry.1838667208=http%3A//arxiv.org/abs/2406.14412v1&entry.124074799=Read"},
{"title": "RankCLIP: Ranking-Consistent Language-Image Pretraining", "author": "Yiming Zhang and Zhuokai Zhao and Zhaorun Chen and Zhili Feng and Zenghui Ding and Yining Sun", "abstract": "  Self-supervised contrastive learning models, such as CLIP, have set new\nbenchmarks for vision-language models in many downstream tasks. However, their\ndependency on rigid one-to-one mappings overlooks the complex and often\nmultifaceted relationships between and within texts and images. To this end, we\nintroduce RANKCLIP, a novel pretraining method that extends beyond the rigid\none-to-one matching framework of CLIP and its variants. By extending the\ntraditional pair-wise loss to list-wise, and leveraging both in-modal and\ncross-modal ranking consistency, RANKCLIP improves the alignment process,\nenabling it to capture the nuanced many-to-many relationships between and\nwithin each modality. Through comprehensive experiments, we demonstrate the\neffectiveness of RANKCLIP in various downstream tasks, notably achieving\nsignificant gains in zero-shot classifications over state-of-the-art methods,\nunderscoring the importance of this enhanced learning process.\n", "link": "http://arxiv.org/abs/2404.09387v2", "date": "2024-06-20", "relevancy": 2.2141, "topK": [{"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5919}, {"title": "Self-supervised Photographic Image Layout Representation Learning", "link": "http://arxiv.org/abs/2403.03740v1", "similarity": 0.5433}, {"title": "Customize-A-Video: One-Shot Motion Customization of Text-to-Video\n  Diffusion Models", "link": "http://arxiv.org/abs/2402.14780v1", "similarity": 0.4831}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20RankCLIP%3A%20Ranking-Consistent%20Language-Image%20Pretraining&body=Title%3A%20RankCLIP%3A%20Ranking-Consistent%20Language-Image%20Pretraining%0AAuthor%3A%20Yiming%20Zhang%20and%20Zhuokai%20Zhao%20and%20Zhaorun%20Chen%20and%20Zhili%20Feng%20and%20Zenghui%20Ding%20and%20Yining%20Sun%0AAbstract%3A%20%20%20Self-supervised%20contrastive%20learning%20models%2C%20such%20as%20CLIP%2C%20have%20set%20new%0Abenchmarks%20for%20vision-language%20models%20in%20many%20downstream%20tasks.%20However%2C%20their%0Adependency%20on%20rigid%20one-to-one%20mappings%20overlooks%20the%20complex%20and%20often%0Amultifaceted%20relationships%20between%20and%20within%20texts%20and%20images.%20To%20this%20end%2C%20we%0Aintroduce%20RANKCLIP%2C%20a%20novel%20pretraining%20method%20that%20extends%20beyond%20the%20rigid%0Aone-to-one%20matching%20framework%20of%20CLIP%20and%20its%20variants.%20By%20extending%20the%0Atraditional%20pair-wise%20loss%20to%20list-wise%2C%20and%20leveraging%20both%20in-modal%20and%0Across-modal%20ranking%20consistency%2C%20RANKCLIP%20improves%20the%20alignment%20process%2C%0Aenabling%20it%20to%20capture%20the%20nuanced%20many-to-many%20relationships%20between%20and%0Awithin%20each%20modality.%20Through%20comprehensive%20experiments%2C%20we%20demonstrate%20the%0Aeffectiveness%20of%20RANKCLIP%20in%20various%20downstream%20tasks%2C%20notably%20achieving%0Asignificant%20gains%20in%20zero-shot%20classifications%20over%20state-of-the-art%20methods%2C%0Aunderscoring%20the%20importance%20of%20this%20enhanced%20learning%20process.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2404.09387v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DRankCLIP%253A%2520Ranking-Consistent%2520Language-Image%2520Pretraining%26entry.906535625%3DYiming%2520Zhang%2520and%2520Zhuokai%2520Zhao%2520and%2520Zhaorun%2520Chen%2520and%2520Zhili%2520Feng%2520and%2520Zenghui%2520Ding%2520and%2520Yining%2520Sun%26entry.1292438233%3D%2520%2520Self-supervised%2520contrastive%2520learning%2520models%252C%2520such%2520as%2520CLIP%252C%2520have%2520set%2520new%250Abenchmarks%2520for%2520vision-language%2520models%2520in%2520many%2520downstream%2520tasks.%2520However%252C%2520their%250Adependency%2520on%2520rigid%2520one-to-one%2520mappings%2520overlooks%2520the%2520complex%2520and%2520often%250Amultifaceted%2520relationships%2520between%2520and%2520within%2520texts%2520and%2520images.%2520To%2520this%2520end%252C%2520we%250Aintroduce%2520RANKCLIP%252C%2520a%2520novel%2520pretraining%2520method%2520that%2520extends%2520beyond%2520the%2520rigid%250Aone-to-one%2520matching%2520framework%2520of%2520CLIP%2520and%2520its%2520variants.%2520By%2520extending%2520the%250Atraditional%2520pair-wise%2520loss%2520to%2520list-wise%252C%2520and%2520leveraging%2520both%2520in-modal%2520and%250Across-modal%2520ranking%2520consistency%252C%2520RANKCLIP%2520improves%2520the%2520alignment%2520process%252C%250Aenabling%2520it%2520to%2520capture%2520the%2520nuanced%2520many-to-many%2520relationships%2520between%2520and%250Awithin%2520each%2520modality.%2520Through%2520comprehensive%2520experiments%252C%2520we%2520demonstrate%2520the%250Aeffectiveness%2520of%2520RANKCLIP%2520in%2520various%2520downstream%2520tasks%252C%2520notably%2520achieving%250Asignificant%2520gains%2520in%2520zero-shot%2520classifications%2520over%2520state-of-the-art%2520methods%252C%250Aunderscoring%2520the%2520importance%2520of%2520this%2520enhanced%2520learning%2520process.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2404.09387v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=RankCLIP%3A%20Ranking-Consistent%20Language-Image%20Pretraining&entry.906535625=Yiming%20Zhang%20and%20Zhuokai%20Zhao%20and%20Zhaorun%20Chen%20and%20Zhili%20Feng%20and%20Zenghui%20Ding%20and%20Yining%20Sun&entry.1292438233=%20%20Self-supervised%20contrastive%20learning%20models%2C%20such%20as%20CLIP%2C%20have%20set%20new%0Abenchmarks%20for%20vision-language%20models%20in%20many%20downstream%20tasks.%20However%2C%20their%0Adependency%20on%20rigid%20one-to-one%20mappings%20overlooks%20the%20complex%20and%20often%0Amultifaceted%20relationships%20between%20and%20within%20texts%20and%20images.%20To%20this%20end%2C%20we%0Aintroduce%20RANKCLIP%2C%20a%20novel%20pretraining%20method%20that%20extends%20beyond%20the%20rigid%0Aone-to-one%20matching%20framework%20of%20CLIP%20and%20its%20variants.%20By%20extending%20the%0Atraditional%20pair-wise%20loss%20to%20list-wise%2C%20and%20leveraging%20both%20in-modal%20and%0Across-modal%20ranking%20consistency%2C%20RANKCLIP%20improves%20the%20alignment%20process%2C%0Aenabling%20it%20to%20capture%20the%20nuanced%20many-to-many%20relationships%20between%20and%0Awithin%20each%20modality.%20Through%20comprehensive%20experiments%2C%20we%20demonstrate%20the%0Aeffectiveness%20of%20RANKCLIP%20in%20various%20downstream%20tasks%2C%20notably%20achieving%0Asignificant%20gains%20in%20zero-shot%20classifications%20over%20state-of-the-art%20methods%2C%0Aunderscoring%20the%20importance%20of%20this%20enhanced%20learning%20process.%0A&entry.1838667208=http%3A//arxiv.org/abs/2404.09387v2&entry.124074799=Read"},
{"title": "Communication-Efficient Byzantine-Resilient Federated Zero-Order\n  Optimization", "author": "Afonso de S\u00e1 Delgado Neto and Maximilian Egger and Mayank Bakshi and Rawad Bitar", "abstract": "  We introduce CYBER-0, the first zero-order optimization algorithm for\nmemory-and-communication efficient Federated Learning, resilient to Byzantine\nfaults. We show through extensive numerical experiments on the MNIST dataset\nand finetuning RoBERTa-Large that CYBER-0 outperforms state-of-the-art\nalgorithms in terms of communication and memory efficiency while reaching\nsimilar accuracy. We provide theoretical guarantees on its convergence for\nconvex loss functions.\n", "link": "http://arxiv.org/abs/2406.14362v1", "date": "2024-06-20", "relevancy": 2.2063, "topK": [{"title": "Tuning-Free Noise Rectification for High Fidelity Image-to-Video\n  Generation", "link": "http://arxiv.org/abs/2403.02827v1", "similarity": 0.4427}, {"title": "ToDo: Token Downsampling for Efficient Generation of High-Resolution\n  Images", "link": "http://arxiv.org/abs/2402.13573v2", "similarity": 0.4425}, {"title": "Boosted dynamic neural networks", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/26302", "similarity": 0.4386}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Communication-Efficient%20Byzantine-Resilient%20Federated%20Zero-Order%0A%20%20Optimization&body=Title%3A%20Communication-Efficient%20Byzantine-Resilient%20Federated%20Zero-Order%0A%20%20Optimization%0AAuthor%3A%20Afonso%20de%20S%C3%A1%20Delgado%20Neto%20and%20Maximilian%20Egger%20and%20Mayank%20Bakshi%20and%20Rawad%20Bitar%0AAbstract%3A%20%20%20We%20introduce%20CYBER-0%2C%20the%20first%20zero-order%20optimization%20algorithm%20for%0Amemory-and-communication%20efficient%20Federated%20Learning%2C%20resilient%20to%20Byzantine%0Afaults.%20We%20show%20through%20extensive%20numerical%20experiments%20on%20the%20MNIST%20dataset%0Aand%20finetuning%20RoBERTa-Large%20that%20CYBER-0%20outperforms%20state-of-the-art%0Aalgorithms%20in%20terms%20of%20communication%20and%20memory%20efficiency%20while%20reaching%0Asimilar%20accuracy.%20We%20provide%20theoretical%20guarantees%20on%20its%20convergence%20for%0Aconvex%20loss%20functions.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2406.14362v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DCommunication-Efficient%2520Byzantine-Resilient%2520Federated%2520Zero-Order%250A%2520%2520Optimization%26entry.906535625%3DAfonso%2520de%2520S%25C3%25A1%2520Delgado%2520Neto%2520and%2520Maximilian%2520Egger%2520and%2520Mayank%2520Bakshi%2520and%2520Rawad%2520Bitar%26entry.1292438233%3D%2520%2520We%2520introduce%2520CYBER-0%252C%2520the%2520first%2520zero-order%2520optimization%2520algorithm%2520for%250Amemory-and-communication%2520efficient%2520Federated%2520Learning%252C%2520resilient%2520to%2520Byzantine%250Afaults.%2520We%2520show%2520through%2520extensive%2520numerical%2520experiments%2520on%2520the%2520MNIST%2520dataset%250Aand%2520finetuning%2520RoBERTa-Large%2520that%2520CYBER-0%2520outperforms%2520state-of-the-art%250Aalgorithms%2520in%2520terms%2520of%2520communication%2520and%2520memory%2520efficiency%2520while%2520reaching%250Asimilar%2520accuracy.%2520We%2520provide%2520theoretical%2520guarantees%2520on%2520its%2520convergence%2520for%250Aconvex%2520loss%2520functions.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2406.14362v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Communication-Efficient%20Byzantine-Resilient%20Federated%20Zero-Order%0A%20%20Optimization&entry.906535625=Afonso%20de%20S%C3%A1%20Delgado%20Neto%20and%20Maximilian%20Egger%20and%20Mayank%20Bakshi%20and%20Rawad%20Bitar&entry.1292438233=%20%20We%20introduce%20CYBER-0%2C%20the%20first%20zero-order%20optimization%20algorithm%20for%0Amemory-and-communication%20efficient%20Federated%20Learning%2C%20resilient%20to%20Byzantine%0Afaults.%20We%20show%20through%20extensive%20numerical%20experiments%20on%20the%20MNIST%20dataset%0Aand%20finetuning%20RoBERTa-Large%20that%20CYBER-0%20outperforms%20state-of-the-art%0Aalgorithms%20in%20terms%20of%20communication%20and%20memory%20efficiency%20while%20reaching%0Asimilar%20accuracy.%20We%20provide%20theoretical%20guarantees%20on%20its%20convergence%20for%0Aconvex%20loss%20functions.%0A&entry.1838667208=http%3A//arxiv.org/abs/2406.14362v1&entry.124074799=Read"},
{"title": "Unleashing the Potential of Tracklets for Unsupervised Video Person\n  Re-Identification", "author": "Nanxing Meng and Qizao Wang and Bin Li and Xiangyang Xue", "abstract": "  With rich temporal-spatial information, video-based person re-identification\nmethods have shown broad prospects. Although tracklets can be easily obtained\nwith ready-made tracking models, annotating identities is still expensive and\nimpractical. Therefore, some video-based methods propose using only a few\nidentity annotations or camera labels to facilitate feature learning. They also\nsimply average the frame features of each tracklet, overlooking unexpected\nvariations and inherent identity consistency within tracklets. In this paper,\nwe propose the Self-Supervised Refined Clustering (SSR-C) framework without\nrelying on any annotation or auxiliary information to promote unsupervised\nvideo person re-identification. Specifically, we first propose the\nNoise-Filtered Tracklet Partition (NFTP) module to reduce the feature bias of\ntracklets caused by noisy tracking results, and sequentially partition the\nnoise-filtered tracklets into \"sub-tracklets\". Then, we cluster and further\nmerge sub-tracklets using the self-supervised signal from tracklet partition,\nwhich is enhanced through a progressive strategy to generate reliable pseudo\nlabels, facilitating intra-class cross-tracklet aggregation. Moreover, we\npropose the Class Smoothing Classification (CSC) loss to efficiently promote\nmodel learning. Extensive experiments on the MARS and DukeMTMC-VideoReID\ndatasets demonstrate that our proposed SSR-C for unsupervised video person\nre-identification achieves state-of-the-art results and is comparable to\nadvanced supervised methods.\n", "link": "http://arxiv.org/abs/2406.14261v1", "date": "2024-06-20", "relevancy": 2.2046, "topK": [{"title": "Weakly-guided self-supervised pretraining for temporal activity detection", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/25189", "similarity": 0.5553}, {"title": "Magic-Me: Identity-Specific Video Customized Diffusion", "link": "http://arxiv.org/abs/2402.09368v1", "similarity": 0.5514}, {"title": "Customize-A-Video: One-Shot Motion Customization of Text-to-Video\n  Diffusion Models", "link": "http://arxiv.org/abs/2402.14780v1", "similarity": 0.5404}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Unleashing%20the%20Potential%20of%20Tracklets%20for%20Unsupervised%20Video%20Person%0A%20%20Re-Identification&body=Title%3A%20Unleashing%20the%20Potential%20of%20Tracklets%20for%20Unsupervised%20Video%20Person%0A%20%20Re-Identification%0AAuthor%3A%20Nanxing%20Meng%20and%20Qizao%20Wang%20and%20Bin%20Li%20and%20Xiangyang%20Xue%0AAbstract%3A%20%20%20With%20rich%20temporal-spatial%20information%2C%20video-based%20person%20re-identification%0Amethods%20have%20shown%20broad%20prospects.%20Although%20tracklets%20can%20be%20easily%20obtained%0Awith%20ready-made%20tracking%20models%2C%20annotating%20identities%20is%20still%20expensive%20and%0Aimpractical.%20Therefore%2C%20some%20video-based%20methods%20propose%20using%20only%20a%20few%0Aidentity%20annotations%20or%20camera%20labels%20to%20facilitate%20feature%20learning.%20They%20also%0Asimply%20average%20the%20frame%20features%20of%20each%20tracklet%2C%20overlooking%20unexpected%0Avariations%20and%20inherent%20identity%20consistency%20within%20tracklets.%20In%20this%20paper%2C%0Awe%20propose%20the%20Self-Supervised%20Refined%20Clustering%20%28SSR-C%29%20framework%20without%0Arelying%20on%20any%20annotation%20or%20auxiliary%20information%20to%20promote%20unsupervised%0Avideo%20person%20re-identification.%20Specifically%2C%20we%20first%20propose%20the%0ANoise-Filtered%20Tracklet%20Partition%20%28NFTP%29%20module%20to%20reduce%20the%20feature%20bias%20of%0Atracklets%20caused%20by%20noisy%20tracking%20results%2C%20and%20sequentially%20partition%20the%0Anoise-filtered%20tracklets%20into%20%22sub-tracklets%22.%20Then%2C%20we%20cluster%20and%20further%0Amerge%20sub-tracklets%20using%20the%20self-supervised%20signal%20from%20tracklet%20partition%2C%0Awhich%20is%20enhanced%20through%20a%20progressive%20strategy%20to%20generate%20reliable%20pseudo%0Alabels%2C%20facilitating%20intra-class%20cross-tracklet%20aggregation.%20Moreover%2C%20we%0Apropose%20the%20Class%20Smoothing%20Classification%20%28CSC%29%20loss%20to%20efficiently%20promote%0Amodel%20learning.%20Extensive%20experiments%20on%20the%20MARS%20and%20DukeMTMC-VideoReID%0Adatasets%20demonstrate%20that%20our%20proposed%20SSR-C%20for%20unsupervised%20video%20person%0Are-identification%20achieves%20state-of-the-art%20results%20and%20is%20comparable%20to%0Aadvanced%20supervised%20methods.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2406.14261v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DUnleashing%2520the%2520Potential%2520of%2520Tracklets%2520for%2520Unsupervised%2520Video%2520Person%250A%2520%2520Re-Identification%26entry.906535625%3DNanxing%2520Meng%2520and%2520Qizao%2520Wang%2520and%2520Bin%2520Li%2520and%2520Xiangyang%2520Xue%26entry.1292438233%3D%2520%2520With%2520rich%2520temporal-spatial%2520information%252C%2520video-based%2520person%2520re-identification%250Amethods%2520have%2520shown%2520broad%2520prospects.%2520Although%2520tracklets%2520can%2520be%2520easily%2520obtained%250Awith%2520ready-made%2520tracking%2520models%252C%2520annotating%2520identities%2520is%2520still%2520expensive%2520and%250Aimpractical.%2520Therefore%252C%2520some%2520video-based%2520methods%2520propose%2520using%2520only%2520a%2520few%250Aidentity%2520annotations%2520or%2520camera%2520labels%2520to%2520facilitate%2520feature%2520learning.%2520They%2520also%250Asimply%2520average%2520the%2520frame%2520features%2520of%2520each%2520tracklet%252C%2520overlooking%2520unexpected%250Avariations%2520and%2520inherent%2520identity%2520consistency%2520within%2520tracklets.%2520In%2520this%2520paper%252C%250Awe%2520propose%2520the%2520Self-Supervised%2520Refined%2520Clustering%2520%2528SSR-C%2529%2520framework%2520without%250Arelying%2520on%2520any%2520annotation%2520or%2520auxiliary%2520information%2520to%2520promote%2520unsupervised%250Avideo%2520person%2520re-identification.%2520Specifically%252C%2520we%2520first%2520propose%2520the%250ANoise-Filtered%2520Tracklet%2520Partition%2520%2528NFTP%2529%2520module%2520to%2520reduce%2520the%2520feature%2520bias%2520of%250Atracklets%2520caused%2520by%2520noisy%2520tracking%2520results%252C%2520and%2520sequentially%2520partition%2520the%250Anoise-filtered%2520tracklets%2520into%2520%2522sub-tracklets%2522.%2520Then%252C%2520we%2520cluster%2520and%2520further%250Amerge%2520sub-tracklets%2520using%2520the%2520self-supervised%2520signal%2520from%2520tracklet%2520partition%252C%250Awhich%2520is%2520enhanced%2520through%2520a%2520progressive%2520strategy%2520to%2520generate%2520reliable%2520pseudo%250Alabels%252C%2520facilitating%2520intra-class%2520cross-tracklet%2520aggregation.%2520Moreover%252C%2520we%250Apropose%2520the%2520Class%2520Smoothing%2520Classification%2520%2528CSC%2529%2520loss%2520to%2520efficiently%2520promote%250Amodel%2520learning.%2520Extensive%2520experiments%2520on%2520the%2520MARS%2520and%2520DukeMTMC-VideoReID%250Adatasets%2520demonstrate%2520that%2520our%2520proposed%2520SSR-C%2520for%2520unsupervised%2520video%2520person%250Are-identification%2520achieves%2520state-of-the-art%2520results%2520and%2520is%2520comparable%2520to%250Aadvanced%2520supervised%2520methods.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2406.14261v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Unleashing%20the%20Potential%20of%20Tracklets%20for%20Unsupervised%20Video%20Person%0A%20%20Re-Identification&entry.906535625=Nanxing%20Meng%20and%20Qizao%20Wang%20and%20Bin%20Li%20and%20Xiangyang%20Xue&entry.1292438233=%20%20With%20rich%20temporal-spatial%20information%2C%20video-based%20person%20re-identification%0Amethods%20have%20shown%20broad%20prospects.%20Although%20tracklets%20can%20be%20easily%20obtained%0Awith%20ready-made%20tracking%20models%2C%20annotating%20identities%20is%20still%20expensive%20and%0Aimpractical.%20Therefore%2C%20some%20video-based%20methods%20propose%20using%20only%20a%20few%0Aidentity%20annotations%20or%20camera%20labels%20to%20facilitate%20feature%20learning.%20They%20also%0Asimply%20average%20the%20frame%20features%20of%20each%20tracklet%2C%20overlooking%20unexpected%0Avariations%20and%20inherent%20identity%20consistency%20within%20tracklets.%20In%20this%20paper%2C%0Awe%20propose%20the%20Self-Supervised%20Refined%20Clustering%20%28SSR-C%29%20framework%20without%0Arelying%20on%20any%20annotation%20or%20auxiliary%20information%20to%20promote%20unsupervised%0Avideo%20person%20re-identification.%20Specifically%2C%20we%20first%20propose%20the%0ANoise-Filtered%20Tracklet%20Partition%20%28NFTP%29%20module%20to%20reduce%20the%20feature%20bias%20of%0Atracklets%20caused%20by%20noisy%20tracking%20results%2C%20and%20sequentially%20partition%20the%0Anoise-filtered%20tracklets%20into%20%22sub-tracklets%22.%20Then%2C%20we%20cluster%20and%20further%0Amerge%20sub-tracklets%20using%20the%20self-supervised%20signal%20from%20tracklet%20partition%2C%0Awhich%20is%20enhanced%20through%20a%20progressive%20strategy%20to%20generate%20reliable%20pseudo%0Alabels%2C%20facilitating%20intra-class%20cross-tracklet%20aggregation.%20Moreover%2C%20we%0Apropose%20the%20Class%20Smoothing%20Classification%20%28CSC%29%20loss%20to%20efficiently%20promote%0Amodel%20learning.%20Extensive%20experiments%20on%20the%20MARS%20and%20DukeMTMC-VideoReID%0Adatasets%20demonstrate%20that%20our%20proposed%20SSR-C%20for%20unsupervised%20video%20person%0Are-identification%20achieves%20state-of-the-art%20results%20and%20is%20comparable%20to%0Aadvanced%20supervised%20methods.%0A&entry.1838667208=http%3A//arxiv.org/abs/2406.14261v1&entry.124074799=Read"},
{"title": "REVEAL-IT: REinforcement learning with Visibility of Evolving Agent\n  poLicy for InTerpretability", "author": "Shuang Ao and Simon Khan and Haris Aziz and Flora D. Salim", "abstract": "  Understanding the agent's learning process, particularly the factors that\ncontribute to its success or failure post-training, is crucial for\ncomprehending the rationale behind the agent's decision-making process. Prior\nmethods clarify the learning process by creating a structural causal model\n(SCM) or visually representing the distribution of value functions.\nNevertheless, these approaches have constraints as they exclusively function in\n2D-environments or with uncomplicated transition dynamics. Understanding the\nagent's learning process in complicated environments or tasks is more\nchallenging. In this paper, we propose REVEAL-IT, a novel framework for\nexplaining the learning process of an agent in complex environments. Initially,\nwe visualize the policy structure and the agent's learning process for various\ntraining tasks. By visualizing these findings, we can understand how much a\nparticular training task or stage affects the agent's performance in test.\nThen, a GNN-based explainer learns to highlight the most important section of\nthe policy, providing a more clear and robust explanation of the agent's\nlearning process. The experiments demonstrate that explanations derived from\nthis framework can effectively help in the optimization of the\n", "link": "http://arxiv.org/abs/2406.14214v1", "date": "2024-06-20", "relevancy": 2.1921, "topK": [{"title": "RoboEXP: Action-Conditioned Scene Graph via Interactive Exploration for\n  Robotic Manipulation", "link": "http://arxiv.org/abs/2402.15487v1", "similarity": 0.5496}, {"title": "Safe and efficient training of a control agent", "link": "https://patents.google.com/patent/US11709462B2/en", "similarity": 0.5491}, {"title": "Genie: Generative Interactive Environments", "link": "http://arxiv.org/abs/2402.15391v1", "similarity": 0.5463}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20REVEAL-IT%3A%20REinforcement%20learning%20with%20Visibility%20of%20Evolving%20Agent%0A%20%20poLicy%20for%20InTerpretability&body=Title%3A%20REVEAL-IT%3A%20REinforcement%20learning%20with%20Visibility%20of%20Evolving%20Agent%0A%20%20poLicy%20for%20InTerpretability%0AAuthor%3A%20Shuang%20Ao%20and%20Simon%20Khan%20and%20Haris%20Aziz%20and%20Flora%20D.%20Salim%0AAbstract%3A%20%20%20Understanding%20the%20agent%27s%20learning%20process%2C%20particularly%20the%20factors%20that%0Acontribute%20to%20its%20success%20or%20failure%20post-training%2C%20is%20crucial%20for%0Acomprehending%20the%20rationale%20behind%20the%20agent%27s%20decision-making%20process.%20Prior%0Amethods%20clarify%20the%20learning%20process%20by%20creating%20a%20structural%20causal%20model%0A%28SCM%29%20or%20visually%20representing%20the%20distribution%20of%20value%20functions.%0ANevertheless%2C%20these%20approaches%20have%20constraints%20as%20they%20exclusively%20function%20in%0A2D-environments%20or%20with%20uncomplicated%20transition%20dynamics.%20Understanding%20the%0Aagent%27s%20learning%20process%20in%20complicated%20environments%20or%20tasks%20is%20more%0Achallenging.%20In%20this%20paper%2C%20we%20propose%20REVEAL-IT%2C%20a%20novel%20framework%20for%0Aexplaining%20the%20learning%20process%20of%20an%20agent%20in%20complex%20environments.%20Initially%2C%0Awe%20visualize%20the%20policy%20structure%20and%20the%20agent%27s%20learning%20process%20for%20various%0Atraining%20tasks.%20By%20visualizing%20these%20findings%2C%20we%20can%20understand%20how%20much%20a%0Aparticular%20training%20task%20or%20stage%20affects%20the%20agent%27s%20performance%20in%20test.%0AThen%2C%20a%20GNN-based%20explainer%20learns%20to%20highlight%20the%20most%20important%20section%20of%0Athe%20policy%2C%20providing%20a%20more%20clear%20and%20robust%20explanation%20of%20the%20agent%27s%0Alearning%20process.%20The%20experiments%20demonstrate%20that%20explanations%20derived%20from%0Athis%20framework%20can%20effectively%20help%20in%20the%20optimization%20of%20the%0A%0ALink%3A%20http%3A//arxiv.org/abs/2406.14214v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DREVEAL-IT%253A%2520REinforcement%2520learning%2520with%2520Visibility%2520of%2520Evolving%2520Agent%250A%2520%2520poLicy%2520for%2520InTerpretability%26entry.906535625%3DShuang%2520Ao%2520and%2520Simon%2520Khan%2520and%2520Haris%2520Aziz%2520and%2520Flora%2520D.%2520Salim%26entry.1292438233%3D%2520%2520Understanding%2520the%2520agent%2527s%2520learning%2520process%252C%2520particularly%2520the%2520factors%2520that%250Acontribute%2520to%2520its%2520success%2520or%2520failure%2520post-training%252C%2520is%2520crucial%2520for%250Acomprehending%2520the%2520rationale%2520behind%2520the%2520agent%2527s%2520decision-making%2520process.%2520Prior%250Amethods%2520clarify%2520the%2520learning%2520process%2520by%2520creating%2520a%2520structural%2520causal%2520model%250A%2528SCM%2529%2520or%2520visually%2520representing%2520the%2520distribution%2520of%2520value%2520functions.%250ANevertheless%252C%2520these%2520approaches%2520have%2520constraints%2520as%2520they%2520exclusively%2520function%2520in%250A2D-environments%2520or%2520with%2520uncomplicated%2520transition%2520dynamics.%2520Understanding%2520the%250Aagent%2527s%2520learning%2520process%2520in%2520complicated%2520environments%2520or%2520tasks%2520is%2520more%250Achallenging.%2520In%2520this%2520paper%252C%2520we%2520propose%2520REVEAL-IT%252C%2520a%2520novel%2520framework%2520for%250Aexplaining%2520the%2520learning%2520process%2520of%2520an%2520agent%2520in%2520complex%2520environments.%2520Initially%252C%250Awe%2520visualize%2520the%2520policy%2520structure%2520and%2520the%2520agent%2527s%2520learning%2520process%2520for%2520various%250Atraining%2520tasks.%2520By%2520visualizing%2520these%2520findings%252C%2520we%2520can%2520understand%2520how%2520much%2520a%250Aparticular%2520training%2520task%2520or%2520stage%2520affects%2520the%2520agent%2527s%2520performance%2520in%2520test.%250AThen%252C%2520a%2520GNN-based%2520explainer%2520learns%2520to%2520highlight%2520the%2520most%2520important%2520section%2520of%250Athe%2520policy%252C%2520providing%2520a%2520more%2520clear%2520and%2520robust%2520explanation%2520of%2520the%2520agent%2527s%250Alearning%2520process.%2520The%2520experiments%2520demonstrate%2520that%2520explanations%2520derived%2520from%250Athis%2520framework%2520can%2520effectively%2520help%2520in%2520the%2520optimization%2520of%2520the%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2406.14214v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=REVEAL-IT%3A%20REinforcement%20learning%20with%20Visibility%20of%20Evolving%20Agent%0A%20%20poLicy%20for%20InTerpretability&entry.906535625=Shuang%20Ao%20and%20Simon%20Khan%20and%20Haris%20Aziz%20and%20Flora%20D.%20Salim&entry.1292438233=%20%20Understanding%20the%20agent%27s%20learning%20process%2C%20particularly%20the%20factors%20that%0Acontribute%20to%20its%20success%20or%20failure%20post-training%2C%20is%20crucial%20for%0Acomprehending%20the%20rationale%20behind%20the%20agent%27s%20decision-making%20process.%20Prior%0Amethods%20clarify%20the%20learning%20process%20by%20creating%20a%20structural%20causal%20model%0A%28SCM%29%20or%20visually%20representing%20the%20distribution%20of%20value%20functions.%0ANevertheless%2C%20these%20approaches%20have%20constraints%20as%20they%20exclusively%20function%20in%0A2D-environments%20or%20with%20uncomplicated%20transition%20dynamics.%20Understanding%20the%0Aagent%27s%20learning%20process%20in%20complicated%20environments%20or%20tasks%20is%20more%0Achallenging.%20In%20this%20paper%2C%20we%20propose%20REVEAL-IT%2C%20a%20novel%20framework%20for%0Aexplaining%20the%20learning%20process%20of%20an%20agent%20in%20complex%20environments.%20Initially%2C%0Awe%20visualize%20the%20policy%20structure%20and%20the%20agent%27s%20learning%20process%20for%20various%0Atraining%20tasks.%20By%20visualizing%20these%20findings%2C%20we%20can%20understand%20how%20much%20a%0Aparticular%20training%20task%20or%20stage%20affects%20the%20agent%27s%20performance%20in%20test.%0AThen%2C%20a%20GNN-based%20explainer%20learns%20to%20highlight%20the%20most%20important%20section%20of%0Athe%20policy%2C%20providing%20a%20more%20clear%20and%20robust%20explanation%20of%20the%20agent%27s%0Alearning%20process.%20The%20experiments%20demonstrate%20that%20explanations%20derived%20from%0Athis%20framework%20can%20effectively%20help%20in%20the%20optimization%20of%20the%0A&entry.1838667208=http%3A//arxiv.org/abs/2406.14214v1&entry.124074799=Read"},
{"title": "Iterative Sizing Field Prediction for Adaptive Mesh Generation From\n  Expert Demonstrations", "author": "Niklas Freymuth and Philipp Dahlinger and Tobias W\u00fcrth and Philipp Becker and Aleksandar Taranovic and Onno Gr\u00f6nheim and Luise K\u00e4rger and Gerhard Neumann", "abstract": "  Many engineering systems require accurate simulations of complex physical\nsystems. Yet, analytical solutions are only available for simple problems,\nnecessitating numerical approximations such as the Finite Element Method (FEM).\nThe cost and accuracy of the FEM scale with the resolution of the underlying\ncomputational mesh. To balance computational speed and accuracy meshes with\nadaptive resolution are used, allocating more resources to critical parts of\nthe geometry. Currently, practitioners often resort to hand-crafted meshes,\nwhich require extensive expert knowledge and are thus costly to obtain. Our\napproach, Adaptive Meshing By Expert Reconstruction (AMBER), views mesh\ngeneration as an imitation learning problem. AMBER combines a graph neural\nnetwork with an online data acquisition scheme to predict the projected sizing\nfield of an expert mesh on a given intermediate mesh, creating a more accurate\nsubsequent mesh. This iterative process ensures efficient and accurate\nimitation of expert mesh resolutions on arbitrary new geometries during\ninference. We experimentally validate AMBER on heuristic 2D meshes and 3D\nmeshes provided by a human expert, closely matching the provided demonstrations\nand outperforming a single-step CNN baseline.\n", "link": "http://arxiv.org/abs/2406.14161v1", "date": "2024-06-20", "relevancy": 2.1815, "topK": [{"title": "Dynamic Gaussians Mesh: Consistent Mesh Reconstruction from Monocular\n  Videos", "link": "http://arxiv.org/abs/2404.12379v2", "similarity": 0.5691}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5325}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.5183}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Iterative%20Sizing%20Field%20Prediction%20for%20Adaptive%20Mesh%20Generation%20From%0A%20%20Expert%20Demonstrations&body=Title%3A%20Iterative%20Sizing%20Field%20Prediction%20for%20Adaptive%20Mesh%20Generation%20From%0A%20%20Expert%20Demonstrations%0AAuthor%3A%20Niklas%20Freymuth%20and%20Philipp%20Dahlinger%20and%20Tobias%20W%C3%BCrth%20and%20Philipp%20Becker%20and%20Aleksandar%20Taranovic%20and%20Onno%20Gr%C3%B6nheim%20and%20Luise%20K%C3%A4rger%20and%20Gerhard%20Neumann%0AAbstract%3A%20%20%20Many%20engineering%20systems%20require%20accurate%20simulations%20of%20complex%20physical%0Asystems.%20Yet%2C%20analytical%20solutions%20are%20only%20available%20for%20simple%20problems%2C%0Anecessitating%20numerical%20approximations%20such%20as%20the%20Finite%20Element%20Method%20%28FEM%29.%0AThe%20cost%20and%20accuracy%20of%20the%20FEM%20scale%20with%20the%20resolution%20of%20the%20underlying%0Acomputational%20mesh.%20To%20balance%20computational%20speed%20and%20accuracy%20meshes%20with%0Aadaptive%20resolution%20are%20used%2C%20allocating%20more%20resources%20to%20critical%20parts%20of%0Athe%20geometry.%20Currently%2C%20practitioners%20often%20resort%20to%20hand-crafted%20meshes%2C%0Awhich%20require%20extensive%20expert%20knowledge%20and%20are%20thus%20costly%20to%20obtain.%20Our%0Aapproach%2C%20Adaptive%20Meshing%20By%20Expert%20Reconstruction%20%28AMBER%29%2C%20views%20mesh%0Ageneration%20as%20an%20imitation%20learning%20problem.%20AMBER%20combines%20a%20graph%20neural%0Anetwork%20with%20an%20online%20data%20acquisition%20scheme%20to%20predict%20the%20projected%20sizing%0Afield%20of%20an%20expert%20mesh%20on%20a%20given%20intermediate%20mesh%2C%20creating%20a%20more%20accurate%0Asubsequent%20mesh.%20This%20iterative%20process%20ensures%20efficient%20and%20accurate%0Aimitation%20of%20expert%20mesh%20resolutions%20on%20arbitrary%20new%20geometries%20during%0Ainference.%20We%20experimentally%20validate%20AMBER%20on%20heuristic%202D%20meshes%20and%203D%0Ameshes%20provided%20by%20a%20human%20expert%2C%20closely%20matching%20the%20provided%20demonstrations%0Aand%20outperforming%20a%20single-step%20CNN%20baseline.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2406.14161v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DIterative%2520Sizing%2520Field%2520Prediction%2520for%2520Adaptive%2520Mesh%2520Generation%2520From%250A%2520%2520Expert%2520Demonstrations%26entry.906535625%3DNiklas%2520Freymuth%2520and%2520Philipp%2520Dahlinger%2520and%2520Tobias%2520W%25C3%25BCrth%2520and%2520Philipp%2520Becker%2520and%2520Aleksandar%2520Taranovic%2520and%2520Onno%2520Gr%25C3%25B6nheim%2520and%2520Luise%2520K%25C3%25A4rger%2520and%2520Gerhard%2520Neumann%26entry.1292438233%3D%2520%2520Many%2520engineering%2520systems%2520require%2520accurate%2520simulations%2520of%2520complex%2520physical%250Asystems.%2520Yet%252C%2520analytical%2520solutions%2520are%2520only%2520available%2520for%2520simple%2520problems%252C%250Anecessitating%2520numerical%2520approximations%2520such%2520as%2520the%2520Finite%2520Element%2520Method%2520%2528FEM%2529.%250AThe%2520cost%2520and%2520accuracy%2520of%2520the%2520FEM%2520scale%2520with%2520the%2520resolution%2520of%2520the%2520underlying%250Acomputational%2520mesh.%2520To%2520balance%2520computational%2520speed%2520and%2520accuracy%2520meshes%2520with%250Aadaptive%2520resolution%2520are%2520used%252C%2520allocating%2520more%2520resources%2520to%2520critical%2520parts%2520of%250Athe%2520geometry.%2520Currently%252C%2520practitioners%2520often%2520resort%2520to%2520hand-crafted%2520meshes%252C%250Awhich%2520require%2520extensive%2520expert%2520knowledge%2520and%2520are%2520thus%2520costly%2520to%2520obtain.%2520Our%250Aapproach%252C%2520Adaptive%2520Meshing%2520By%2520Expert%2520Reconstruction%2520%2528AMBER%2529%252C%2520views%2520mesh%250Ageneration%2520as%2520an%2520imitation%2520learning%2520problem.%2520AMBER%2520combines%2520a%2520graph%2520neural%250Anetwork%2520with%2520an%2520online%2520data%2520acquisition%2520scheme%2520to%2520predict%2520the%2520projected%2520sizing%250Afield%2520of%2520an%2520expert%2520mesh%2520on%2520a%2520given%2520intermediate%2520mesh%252C%2520creating%2520a%2520more%2520accurate%250Asubsequent%2520mesh.%2520This%2520iterative%2520process%2520ensures%2520efficient%2520and%2520accurate%250Aimitation%2520of%2520expert%2520mesh%2520resolutions%2520on%2520arbitrary%2520new%2520geometries%2520during%250Ainference.%2520We%2520experimentally%2520validate%2520AMBER%2520on%2520heuristic%25202D%2520meshes%2520and%25203D%250Ameshes%2520provided%2520by%2520a%2520human%2520expert%252C%2520closely%2520matching%2520the%2520provided%2520demonstrations%250Aand%2520outperforming%2520a%2520single-step%2520CNN%2520baseline.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2406.14161v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Iterative%20Sizing%20Field%20Prediction%20for%20Adaptive%20Mesh%20Generation%20From%0A%20%20Expert%20Demonstrations&entry.906535625=Niklas%20Freymuth%20and%20Philipp%20Dahlinger%20and%20Tobias%20W%C3%BCrth%20and%20Philipp%20Becker%20and%20Aleksandar%20Taranovic%20and%20Onno%20Gr%C3%B6nheim%20and%20Luise%20K%C3%A4rger%20and%20Gerhard%20Neumann&entry.1292438233=%20%20Many%20engineering%20systems%20require%20accurate%20simulations%20of%20complex%20physical%0Asystems.%20Yet%2C%20analytical%20solutions%20are%20only%20available%20for%20simple%20problems%2C%0Anecessitating%20numerical%20approximations%20such%20as%20the%20Finite%20Element%20Method%20%28FEM%29.%0AThe%20cost%20and%20accuracy%20of%20the%20FEM%20scale%20with%20the%20resolution%20of%20the%20underlying%0Acomputational%20mesh.%20To%20balance%20computational%20speed%20and%20accuracy%20meshes%20with%0Aadaptive%20resolution%20are%20used%2C%20allocating%20more%20resources%20to%20critical%20parts%20of%0Athe%20geometry.%20Currently%2C%20practitioners%20often%20resort%20to%20hand-crafted%20meshes%2C%0Awhich%20require%20extensive%20expert%20knowledge%20and%20are%20thus%20costly%20to%20obtain.%20Our%0Aapproach%2C%20Adaptive%20Meshing%20By%20Expert%20Reconstruction%20%28AMBER%29%2C%20views%20mesh%0Ageneration%20as%20an%20imitation%20learning%20problem.%20AMBER%20combines%20a%20graph%20neural%0Anetwork%20with%20an%20online%20data%20acquisition%20scheme%20to%20predict%20the%20projected%20sizing%0Afield%20of%20an%20expert%20mesh%20on%20a%20given%20intermediate%20mesh%2C%20creating%20a%20more%20accurate%0Asubsequent%20mesh.%20This%20iterative%20process%20ensures%20efficient%20and%20accurate%0Aimitation%20of%20expert%20mesh%20resolutions%20on%20arbitrary%20new%20geometries%20during%0Ainference.%20We%20experimentally%20validate%20AMBER%20on%20heuristic%202D%20meshes%20and%203D%0Ameshes%20provided%20by%20a%20human%20expert%2C%20closely%20matching%20the%20provided%20demonstrations%0Aand%20outperforming%20a%20single-step%20CNN%20baseline.%0A&entry.1838667208=http%3A//arxiv.org/abs/2406.14161v1&entry.124074799=Read"},
{"title": "Consistency Models Made Easy", "author": "Zhengyang Geng and Ashwini Pokle and William Luo and Justin Lin and J. Zico Kolter", "abstract": "  Consistency models (CMs) are an emerging class of generative models that\noffer faster sampling than traditional diffusion models. CMs enforce that all\npoints along a sampling trajectory are mapped to the same initial point. But\nthis target leads to resource-intensive training: for example, as of 2024,\ntraining a SoTA CM on CIFAR-10 takes one week on 8 GPUs. In this work, we\npropose an alternative scheme for training CMs, vastly improving the efficiency\nof building such models. Specifically, by expressing CM trajectories via a\nparticular differential equation, we argue that diffusion models can be viewed\nas a special case of CMs with a specific discretization. We can thus fine-tune\na consistency model starting from a pre-trained diffusion model and\nprogressively approximate the full consistency condition to stronger degrees\nover the training process. Our resulting method, which we term Easy Consistency\nTuning (ECT), achieves vastly improved training times while indeed improving\nupon the quality of previous methods: for example, ECT achieves a 2-step FID of\n2.73 on CIFAR10 within 1 hour on a single A100 GPU, matching Consistency\nDistillation trained of hundreds of GPU hours. Owing to this computational\nefficiency, we investigate the scaling law of CMs under ECT, showing that they\nseem to obey classic power law scaling, hinting at their ability to improve\nefficiency and performance at larger scales. Code\n(https://github.com/locuslab/ect) is available.\n", "link": "http://arxiv.org/abs/2406.14548v1", "date": "2024-06-20", "relevancy": 2.18, "topK": [{"title": "SDXS: Real-Time One-Step Latent Diffusion Models with Image Conditions", "link": "http://arxiv.org/abs/2403.16627v1", "similarity": 0.5856}, {"title": "ToDo: Token Downsampling for Efficient Generation of High-Resolution\n  Images", "link": "http://arxiv.org/abs/2402.13573v2", "similarity": 0.5526}, {"title": "Magic-Me: Identity-Specific Video Customized Diffusion", "link": "http://arxiv.org/abs/2402.09368v1", "similarity": 0.5211}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Consistency%20Models%20Made%20Easy&body=Title%3A%20Consistency%20Models%20Made%20Easy%0AAuthor%3A%20Zhengyang%20Geng%20and%20Ashwini%20Pokle%20and%20William%20Luo%20and%20Justin%20Lin%20and%20J.%20Zico%20Kolter%0AAbstract%3A%20%20%20Consistency%20models%20%28CMs%29%20are%20an%20emerging%20class%20of%20generative%20models%20that%0Aoffer%20faster%20sampling%20than%20traditional%20diffusion%20models.%20CMs%20enforce%20that%20all%0Apoints%20along%20a%20sampling%20trajectory%20are%20mapped%20to%20the%20same%20initial%20point.%20But%0Athis%20target%20leads%20to%20resource-intensive%20training%3A%20for%20example%2C%20as%20of%202024%2C%0Atraining%20a%20SoTA%20CM%20on%20CIFAR-10%20takes%20one%20week%20on%208%20GPUs.%20In%20this%20work%2C%20we%0Apropose%20an%20alternative%20scheme%20for%20training%20CMs%2C%20vastly%20improving%20the%20efficiency%0Aof%20building%20such%20models.%20Specifically%2C%20by%20expressing%20CM%20trajectories%20via%20a%0Aparticular%20differential%20equation%2C%20we%20argue%20that%20diffusion%20models%20can%20be%20viewed%0Aas%20a%20special%20case%20of%20CMs%20with%20a%20specific%20discretization.%20We%20can%20thus%20fine-tune%0Aa%20consistency%20model%20starting%20from%20a%20pre-trained%20diffusion%20model%20and%0Aprogressively%20approximate%20the%20full%20consistency%20condition%20to%20stronger%20degrees%0Aover%20the%20training%20process.%20Our%20resulting%20method%2C%20which%20we%20term%20Easy%20Consistency%0ATuning%20%28ECT%29%2C%20achieves%20vastly%20improved%20training%20times%20while%20indeed%20improving%0Aupon%20the%20quality%20of%20previous%20methods%3A%20for%20example%2C%20ECT%20achieves%20a%202-step%20FID%20of%0A2.73%20on%20CIFAR10%20within%201%20hour%20on%20a%20single%20A100%20GPU%2C%20matching%20Consistency%0ADistillation%20trained%20of%20hundreds%20of%20GPU%20hours.%20Owing%20to%20this%20computational%0Aefficiency%2C%20we%20investigate%20the%20scaling%20law%20of%20CMs%20under%20ECT%2C%20showing%20that%20they%0Aseem%20to%20obey%20classic%20power%20law%20scaling%2C%20hinting%20at%20their%20ability%20to%20improve%0Aefficiency%20and%20performance%20at%20larger%20scales.%20Code%0A%28https%3A//github.com/locuslab/ect%29%20is%20available.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2406.14548v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DConsistency%2520Models%2520Made%2520Easy%26entry.906535625%3DZhengyang%2520Geng%2520and%2520Ashwini%2520Pokle%2520and%2520William%2520Luo%2520and%2520Justin%2520Lin%2520and%2520J.%2520Zico%2520Kolter%26entry.1292438233%3D%2520%2520Consistency%2520models%2520%2528CMs%2529%2520are%2520an%2520emerging%2520class%2520of%2520generative%2520models%2520that%250Aoffer%2520faster%2520sampling%2520than%2520traditional%2520diffusion%2520models.%2520CMs%2520enforce%2520that%2520all%250Apoints%2520along%2520a%2520sampling%2520trajectory%2520are%2520mapped%2520to%2520the%2520same%2520initial%2520point.%2520But%250Athis%2520target%2520leads%2520to%2520resource-intensive%2520training%253A%2520for%2520example%252C%2520as%2520of%25202024%252C%250Atraining%2520a%2520SoTA%2520CM%2520on%2520CIFAR-10%2520takes%2520one%2520week%2520on%25208%2520GPUs.%2520In%2520this%2520work%252C%2520we%250Apropose%2520an%2520alternative%2520scheme%2520for%2520training%2520CMs%252C%2520vastly%2520improving%2520the%2520efficiency%250Aof%2520building%2520such%2520models.%2520Specifically%252C%2520by%2520expressing%2520CM%2520trajectories%2520via%2520a%250Aparticular%2520differential%2520equation%252C%2520we%2520argue%2520that%2520diffusion%2520models%2520can%2520be%2520viewed%250Aas%2520a%2520special%2520case%2520of%2520CMs%2520with%2520a%2520specific%2520discretization.%2520We%2520can%2520thus%2520fine-tune%250Aa%2520consistency%2520model%2520starting%2520from%2520a%2520pre-trained%2520diffusion%2520model%2520and%250Aprogressively%2520approximate%2520the%2520full%2520consistency%2520condition%2520to%2520stronger%2520degrees%250Aover%2520the%2520training%2520process.%2520Our%2520resulting%2520method%252C%2520which%2520we%2520term%2520Easy%2520Consistency%250ATuning%2520%2528ECT%2529%252C%2520achieves%2520vastly%2520improved%2520training%2520times%2520while%2520indeed%2520improving%250Aupon%2520the%2520quality%2520of%2520previous%2520methods%253A%2520for%2520example%252C%2520ECT%2520achieves%2520a%25202-step%2520FID%2520of%250A2.73%2520on%2520CIFAR10%2520within%25201%2520hour%2520on%2520a%2520single%2520A100%2520GPU%252C%2520matching%2520Consistency%250ADistillation%2520trained%2520of%2520hundreds%2520of%2520GPU%2520hours.%2520Owing%2520to%2520this%2520computational%250Aefficiency%252C%2520we%2520investigate%2520the%2520scaling%2520law%2520of%2520CMs%2520under%2520ECT%252C%2520showing%2520that%2520they%250Aseem%2520to%2520obey%2520classic%2520power%2520law%2520scaling%252C%2520hinting%2520at%2520their%2520ability%2520to%2520improve%250Aefficiency%2520and%2520performance%2520at%2520larger%2520scales.%2520Code%250A%2528https%253A//github.com/locuslab/ect%2529%2520is%2520available.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2406.14548v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Consistency%20Models%20Made%20Easy&entry.906535625=Zhengyang%20Geng%20and%20Ashwini%20Pokle%20and%20William%20Luo%20and%20Justin%20Lin%20and%20J.%20Zico%20Kolter&entry.1292438233=%20%20Consistency%20models%20%28CMs%29%20are%20an%20emerging%20class%20of%20generative%20models%20that%0Aoffer%20faster%20sampling%20than%20traditional%20diffusion%20models.%20CMs%20enforce%20that%20all%0Apoints%20along%20a%20sampling%20trajectory%20are%20mapped%20to%20the%20same%20initial%20point.%20But%0Athis%20target%20leads%20to%20resource-intensive%20training%3A%20for%20example%2C%20as%20of%202024%2C%0Atraining%20a%20SoTA%20CM%20on%20CIFAR-10%20takes%20one%20week%20on%208%20GPUs.%20In%20this%20work%2C%20we%0Apropose%20an%20alternative%20scheme%20for%20training%20CMs%2C%20vastly%20improving%20the%20efficiency%0Aof%20building%20such%20models.%20Specifically%2C%20by%20expressing%20CM%20trajectories%20via%20a%0Aparticular%20differential%20equation%2C%20we%20argue%20that%20diffusion%20models%20can%20be%20viewed%0Aas%20a%20special%20case%20of%20CMs%20with%20a%20specific%20discretization.%20We%20can%20thus%20fine-tune%0Aa%20consistency%20model%20starting%20from%20a%20pre-trained%20diffusion%20model%20and%0Aprogressively%20approximate%20the%20full%20consistency%20condition%20to%20stronger%20degrees%0Aover%20the%20training%20process.%20Our%20resulting%20method%2C%20which%20we%20term%20Easy%20Consistency%0ATuning%20%28ECT%29%2C%20achieves%20vastly%20improved%20training%20times%20while%20indeed%20improving%0Aupon%20the%20quality%20of%20previous%20methods%3A%20for%20example%2C%20ECT%20achieves%20a%202-step%20FID%20of%0A2.73%20on%20CIFAR10%20within%201%20hour%20on%20a%20single%20A100%20GPU%2C%20matching%20Consistency%0ADistillation%20trained%20of%20hundreds%20of%20GPU%20hours.%20Owing%20to%20this%20computational%0Aefficiency%2C%20we%20investigate%20the%20scaling%20law%20of%20CMs%20under%20ECT%2C%20showing%20that%20they%0Aseem%20to%20obey%20classic%20power%20law%20scaling%2C%20hinting%20at%20their%20ability%20to%20improve%0Aefficiency%20and%20performance%20at%20larger%20scales.%20Code%0A%28https%3A//github.com/locuslab/ect%29%20is%20available.%0A&entry.1838667208=http%3A//arxiv.org/abs/2406.14548v1&entry.124074799=Read"},
{"title": "SafeSora: Towards Safety Alignment of Text2Video Generation via a Human\n  Preference Dataset", "author": "Josef Dai and Tianle Chen and Xuyao Wang and Ziran Yang and Taiye Chen and Jiaming Ji and Yaodong Yang", "abstract": "  To mitigate the risk of harmful outputs from large vision models (LVMs), we\nintroduce the SafeSora dataset to promote research on aligning text-to-video\ngeneration with human values. This dataset encompasses human preferences in\ntext-to-video generation tasks along two primary dimensions: helpfulness and\nharmlessness. To capture in-depth human preferences and facilitate structured\nreasoning by crowdworkers, we subdivide helpfulness into 4 sub-dimensions and\nharmlessness into 12 sub-categories, serving as the basis for pilot\nannotations. The SafeSora dataset includes 14,711 unique prompts, 57,333 unique\nvideos generated by 4 distinct LVMs, and 51,691 pairs of preference annotations\nlabeled by humans. We further demonstrate the utility of the SafeSora dataset\nthrough several applications, including training the text-video moderation\nmodel and aligning LVMs with human preference by fine-tuning a prompt\naugmentation module or the diffusion model. These applications highlight its\npotential as the foundation for text-to-video alignment research, such as human\npreference modeling and the development and validation of alignment algorithms.\n", "link": "http://arxiv.org/abs/2406.14477v1", "date": "2024-06-20", "relevancy": 2.1728, "topK": [{"title": "Customize-A-Video: One-Shot Motion Customization of Text-to-Video\n  Diffusion Models", "link": "http://arxiv.org/abs/2402.14780v1", "similarity": 0.5913}, {"title": "Magic-Me: Identity-Specific Video Customized Diffusion", "link": "http://arxiv.org/abs/2402.09368v1", "similarity": 0.5509}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5162}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20SafeSora%3A%20Towards%20Safety%20Alignment%20of%20Text2Video%20Generation%20via%20a%20Human%0A%20%20Preference%20Dataset&body=Title%3A%20SafeSora%3A%20Towards%20Safety%20Alignment%20of%20Text2Video%20Generation%20via%20a%20Human%0A%20%20Preference%20Dataset%0AAuthor%3A%20Josef%20Dai%20and%20Tianle%20Chen%20and%20Xuyao%20Wang%20and%20Ziran%20Yang%20and%20Taiye%20Chen%20and%20Jiaming%20Ji%20and%20Yaodong%20Yang%0AAbstract%3A%20%20%20To%20mitigate%20the%20risk%20of%20harmful%20outputs%20from%20large%20vision%20models%20%28LVMs%29%2C%20we%0Aintroduce%20the%20SafeSora%20dataset%20to%20promote%20research%20on%20aligning%20text-to-video%0Ageneration%20with%20human%20values.%20This%20dataset%20encompasses%20human%20preferences%20in%0Atext-to-video%20generation%20tasks%20along%20two%20primary%20dimensions%3A%20helpfulness%20and%0Aharmlessness.%20To%20capture%20in-depth%20human%20preferences%20and%20facilitate%20structured%0Areasoning%20by%20crowdworkers%2C%20we%20subdivide%20helpfulness%20into%204%20sub-dimensions%20and%0Aharmlessness%20into%2012%20sub-categories%2C%20serving%20as%20the%20basis%20for%20pilot%0Aannotations.%20The%20SafeSora%20dataset%20includes%2014%2C711%20unique%20prompts%2C%2057%2C333%20unique%0Avideos%20generated%20by%204%20distinct%20LVMs%2C%20and%2051%2C691%20pairs%20of%20preference%20annotations%0Alabeled%20by%20humans.%20We%20further%20demonstrate%20the%20utility%20of%20the%20SafeSora%20dataset%0Athrough%20several%20applications%2C%20including%20training%20the%20text-video%20moderation%0Amodel%20and%20aligning%20LVMs%20with%20human%20preference%20by%20fine-tuning%20a%20prompt%0Aaugmentation%20module%20or%20the%20diffusion%20model.%20These%20applications%20highlight%20its%0Apotential%20as%20the%20foundation%20for%20text-to-video%20alignment%20research%2C%20such%20as%20human%0Apreference%20modeling%20and%20the%20development%20and%20validation%20of%20alignment%20algorithms.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2406.14477v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DSafeSora%253A%2520Towards%2520Safety%2520Alignment%2520of%2520Text2Video%2520Generation%2520via%2520a%2520Human%250A%2520%2520Preference%2520Dataset%26entry.906535625%3DJosef%2520Dai%2520and%2520Tianle%2520Chen%2520and%2520Xuyao%2520Wang%2520and%2520Ziran%2520Yang%2520and%2520Taiye%2520Chen%2520and%2520Jiaming%2520Ji%2520and%2520Yaodong%2520Yang%26entry.1292438233%3D%2520%2520To%2520mitigate%2520the%2520risk%2520of%2520harmful%2520outputs%2520from%2520large%2520vision%2520models%2520%2528LVMs%2529%252C%2520we%250Aintroduce%2520the%2520SafeSora%2520dataset%2520to%2520promote%2520research%2520on%2520aligning%2520text-to-video%250Ageneration%2520with%2520human%2520values.%2520This%2520dataset%2520encompasses%2520human%2520preferences%2520in%250Atext-to-video%2520generation%2520tasks%2520along%2520two%2520primary%2520dimensions%253A%2520helpfulness%2520and%250Aharmlessness.%2520To%2520capture%2520in-depth%2520human%2520preferences%2520and%2520facilitate%2520structured%250Areasoning%2520by%2520crowdworkers%252C%2520we%2520subdivide%2520helpfulness%2520into%25204%2520sub-dimensions%2520and%250Aharmlessness%2520into%252012%2520sub-categories%252C%2520serving%2520as%2520the%2520basis%2520for%2520pilot%250Aannotations.%2520The%2520SafeSora%2520dataset%2520includes%252014%252C711%2520unique%2520prompts%252C%252057%252C333%2520unique%250Avideos%2520generated%2520by%25204%2520distinct%2520LVMs%252C%2520and%252051%252C691%2520pairs%2520of%2520preference%2520annotations%250Alabeled%2520by%2520humans.%2520We%2520further%2520demonstrate%2520the%2520utility%2520of%2520the%2520SafeSora%2520dataset%250Athrough%2520several%2520applications%252C%2520including%2520training%2520the%2520text-video%2520moderation%250Amodel%2520and%2520aligning%2520LVMs%2520with%2520human%2520preference%2520by%2520fine-tuning%2520a%2520prompt%250Aaugmentation%2520module%2520or%2520the%2520diffusion%2520model.%2520These%2520applications%2520highlight%2520its%250Apotential%2520as%2520the%2520foundation%2520for%2520text-to-video%2520alignment%2520research%252C%2520such%2520as%2520human%250Apreference%2520modeling%2520and%2520the%2520development%2520and%2520validation%2520of%2520alignment%2520algorithms.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2406.14477v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=SafeSora%3A%20Towards%20Safety%20Alignment%20of%20Text2Video%20Generation%20via%20a%20Human%0A%20%20Preference%20Dataset&entry.906535625=Josef%20Dai%20and%20Tianle%20Chen%20and%20Xuyao%20Wang%20and%20Ziran%20Yang%20and%20Taiye%20Chen%20and%20Jiaming%20Ji%20and%20Yaodong%20Yang&entry.1292438233=%20%20To%20mitigate%20the%20risk%20of%20harmful%20outputs%20from%20large%20vision%20models%20%28LVMs%29%2C%20we%0Aintroduce%20the%20SafeSora%20dataset%20to%20promote%20research%20on%20aligning%20text-to-video%0Ageneration%20with%20human%20values.%20This%20dataset%20encompasses%20human%20preferences%20in%0Atext-to-video%20generation%20tasks%20along%20two%20primary%20dimensions%3A%20helpfulness%20and%0Aharmlessness.%20To%20capture%20in-depth%20human%20preferences%20and%20facilitate%20structured%0Areasoning%20by%20crowdworkers%2C%20we%20subdivide%20helpfulness%20into%204%20sub-dimensions%20and%0Aharmlessness%20into%2012%20sub-categories%2C%20serving%20as%20the%20basis%20for%20pilot%0Aannotations.%20The%20SafeSora%20dataset%20includes%2014%2C711%20unique%20prompts%2C%2057%2C333%20unique%0Avideos%20generated%20by%204%20distinct%20LVMs%2C%20and%2051%2C691%20pairs%20of%20preference%20annotations%0Alabeled%20by%20humans.%20We%20further%20demonstrate%20the%20utility%20of%20the%20SafeSora%20dataset%0Athrough%20several%20applications%2C%20including%20training%20the%20text-video%20moderation%0Amodel%20and%20aligning%20LVMs%20with%20human%20preference%20by%20fine-tuning%20a%20prompt%0Aaugmentation%20module%20or%20the%20diffusion%20model.%20These%20applications%20highlight%20its%0Apotential%20as%20the%20foundation%20for%20text-to-video%20alignment%20research%2C%20such%20as%20human%0Apreference%20modeling%20and%20the%20development%20and%20validation%20of%20alignment%20algorithms.%0A&entry.1838667208=http%3A//arxiv.org/abs/2406.14477v1&entry.124074799=Read"},
{"title": "Exploring Fine-Grained Representation and Recomposition for\n  Cloth-Changing Person Re-Identification", "author": "Qizao Wang and Xuelin Qian and Bin Li and Xiangyang Xue and Yanwei Fu", "abstract": "  Cloth-changing person Re-IDentification (Re-ID) is a particularly challenging\ntask, suffering from two limitations of inferior discriminative features and\nlimited training samples. Existing methods mainly leverage auxiliary\ninformation to facilitate identity-relevant feature learning, including\nsoft-biometrics features of shapes or gaits, and additional labels of clothing.\nHowever, this information may be unavailable in real-world applications. In\nthis paper, we propose a novel FIne-grained Representation and Recomposition\n(FIRe$^{2}$) framework to tackle both limitations without any auxiliary\nannotation or data. Specifically, we first design a Fine-grained Feature Mining\n(FFM) module to separately cluster images of each person. Images with similar\nso-called fine-grained attributes (e.g., clothes and viewpoints) are encouraged\nto cluster together. An attribute-aware classification loss is introduced to\nperform fine-grained learning based on cluster labels, which are not shared\namong different people, promoting the model to learn identity-relevant\nfeatures. Furthermore, to take full advantage of fine-grained attributes, we\npresent a Fine-grained Attribute Recomposition (FAR) module by recomposing\nimage features with different attributes in the latent space. It significantly\nenhances robust feature learning. Extensive experiments demonstrate that\nFIRe$^{2}$ can achieve state-of-the-art performance on five widely-used\ncloth-changing person Re-ID benchmarks. The code is available at\nhttps://github.com/QizaoWang/FIRe-CCReID.\n", "link": "http://arxiv.org/abs/2308.10692v2", "date": "2024-06-20", "relevancy": 2.1637, "topK": [{"title": "Magic-Me: Identity-Specific Video Customized Diffusion", "link": "http://arxiv.org/abs/2402.09368v1", "similarity": 0.5689}, {"title": "Multimodal-Conditioned Latent Diffusion Models for Fashion Image Editing", "link": "http://arxiv.org/abs/2403.14828v2", "similarity": 0.5321}, {"title": "DressCode: Autoregressively Sewing and Generating Garments from Text\n  Guidance", "link": "http://arxiv.org/abs/2401.16465v3", "similarity": 0.5165}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Exploring%20Fine-Grained%20Representation%20and%20Recomposition%20for%0A%20%20Cloth-Changing%20Person%20Re-Identification&body=Title%3A%20Exploring%20Fine-Grained%20Representation%20and%20Recomposition%20for%0A%20%20Cloth-Changing%20Person%20Re-Identification%0AAuthor%3A%20Qizao%20Wang%20and%20Xuelin%20Qian%20and%20Bin%20Li%20and%20Xiangyang%20Xue%20and%20Yanwei%20Fu%0AAbstract%3A%20%20%20Cloth-changing%20person%20Re-IDentification%20%28Re-ID%29%20is%20a%20particularly%20challenging%0Atask%2C%20suffering%20from%20two%20limitations%20of%20inferior%20discriminative%20features%20and%0Alimited%20training%20samples.%20Existing%20methods%20mainly%20leverage%20auxiliary%0Ainformation%20to%20facilitate%20identity-relevant%20feature%20learning%2C%20including%0Asoft-biometrics%20features%20of%20shapes%20or%20gaits%2C%20and%20additional%20labels%20of%20clothing.%0AHowever%2C%20this%20information%20may%20be%20unavailable%20in%20real-world%20applications.%20In%0Athis%20paper%2C%20we%20propose%20a%20novel%20FIne-grained%20Representation%20and%20Recomposition%0A%28FIRe%24%5E%7B2%7D%24%29%20framework%20to%20tackle%20both%20limitations%20without%20any%20auxiliary%0Aannotation%20or%20data.%20Specifically%2C%20we%20first%20design%20a%20Fine-grained%20Feature%20Mining%0A%28FFM%29%20module%20to%20separately%20cluster%20images%20of%20each%20person.%20Images%20with%20similar%0Aso-called%20fine-grained%20attributes%20%28e.g.%2C%20clothes%20and%20viewpoints%29%20are%20encouraged%0Ato%20cluster%20together.%20An%20attribute-aware%20classification%20loss%20is%20introduced%20to%0Aperform%20fine-grained%20learning%20based%20on%20cluster%20labels%2C%20which%20are%20not%20shared%0Aamong%20different%20people%2C%20promoting%20the%20model%20to%20learn%20identity-relevant%0Afeatures.%20Furthermore%2C%20to%20take%20full%20advantage%20of%20fine-grained%20attributes%2C%20we%0Apresent%20a%20Fine-grained%20Attribute%20Recomposition%20%28FAR%29%20module%20by%20recomposing%0Aimage%20features%20with%20different%20attributes%20in%20the%20latent%20space.%20It%20significantly%0Aenhances%20robust%20feature%20learning.%20Extensive%20experiments%20demonstrate%20that%0AFIRe%24%5E%7B2%7D%24%20can%20achieve%20state-of-the-art%20performance%20on%20five%20widely-used%0Acloth-changing%20person%20Re-ID%20benchmarks.%20The%20code%20is%20available%20at%0Ahttps%3A//github.com/QizaoWang/FIRe-CCReID.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2308.10692v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DExploring%2520Fine-Grained%2520Representation%2520and%2520Recomposition%2520for%250A%2520%2520Cloth-Changing%2520Person%2520Re-Identification%26entry.906535625%3DQizao%2520Wang%2520and%2520Xuelin%2520Qian%2520and%2520Bin%2520Li%2520and%2520Xiangyang%2520Xue%2520and%2520Yanwei%2520Fu%26entry.1292438233%3D%2520%2520Cloth-changing%2520person%2520Re-IDentification%2520%2528Re-ID%2529%2520is%2520a%2520particularly%2520challenging%250Atask%252C%2520suffering%2520from%2520two%2520limitations%2520of%2520inferior%2520discriminative%2520features%2520and%250Alimited%2520training%2520samples.%2520Existing%2520methods%2520mainly%2520leverage%2520auxiliary%250Ainformation%2520to%2520facilitate%2520identity-relevant%2520feature%2520learning%252C%2520including%250Asoft-biometrics%2520features%2520of%2520shapes%2520or%2520gaits%252C%2520and%2520additional%2520labels%2520of%2520clothing.%250AHowever%252C%2520this%2520information%2520may%2520be%2520unavailable%2520in%2520real-world%2520applications.%2520In%250Athis%2520paper%252C%2520we%2520propose%2520a%2520novel%2520FIne-grained%2520Representation%2520and%2520Recomposition%250A%2528FIRe%2524%255E%257B2%257D%2524%2529%2520framework%2520to%2520tackle%2520both%2520limitations%2520without%2520any%2520auxiliary%250Aannotation%2520or%2520data.%2520Specifically%252C%2520we%2520first%2520design%2520a%2520Fine-grained%2520Feature%2520Mining%250A%2528FFM%2529%2520module%2520to%2520separately%2520cluster%2520images%2520of%2520each%2520person.%2520Images%2520with%2520similar%250Aso-called%2520fine-grained%2520attributes%2520%2528e.g.%252C%2520clothes%2520and%2520viewpoints%2529%2520are%2520encouraged%250Ato%2520cluster%2520together.%2520An%2520attribute-aware%2520classification%2520loss%2520is%2520introduced%2520to%250Aperform%2520fine-grained%2520learning%2520based%2520on%2520cluster%2520labels%252C%2520which%2520are%2520not%2520shared%250Aamong%2520different%2520people%252C%2520promoting%2520the%2520model%2520to%2520learn%2520identity-relevant%250Afeatures.%2520Furthermore%252C%2520to%2520take%2520full%2520advantage%2520of%2520fine-grained%2520attributes%252C%2520we%250Apresent%2520a%2520Fine-grained%2520Attribute%2520Recomposition%2520%2528FAR%2529%2520module%2520by%2520recomposing%250Aimage%2520features%2520with%2520different%2520attributes%2520in%2520the%2520latent%2520space.%2520It%2520significantly%250Aenhances%2520robust%2520feature%2520learning.%2520Extensive%2520experiments%2520demonstrate%2520that%250AFIRe%2524%255E%257B2%257D%2524%2520can%2520achieve%2520state-of-the-art%2520performance%2520on%2520five%2520widely-used%250Acloth-changing%2520person%2520Re-ID%2520benchmarks.%2520The%2520code%2520is%2520available%2520at%250Ahttps%253A//github.com/QizaoWang/FIRe-CCReID.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2308.10692v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Exploring%20Fine-Grained%20Representation%20and%20Recomposition%20for%0A%20%20Cloth-Changing%20Person%20Re-Identification&entry.906535625=Qizao%20Wang%20and%20Xuelin%20Qian%20and%20Bin%20Li%20and%20Xiangyang%20Xue%20and%20Yanwei%20Fu&entry.1292438233=%20%20Cloth-changing%20person%20Re-IDentification%20%28Re-ID%29%20is%20a%20particularly%20challenging%0Atask%2C%20suffering%20from%20two%20limitations%20of%20inferior%20discriminative%20features%20and%0Alimited%20training%20samples.%20Existing%20methods%20mainly%20leverage%20auxiliary%0Ainformation%20to%20facilitate%20identity-relevant%20feature%20learning%2C%20including%0Asoft-biometrics%20features%20of%20shapes%20or%20gaits%2C%20and%20additional%20labels%20of%20clothing.%0AHowever%2C%20this%20information%20may%20be%20unavailable%20in%20real-world%20applications.%20In%0Athis%20paper%2C%20we%20propose%20a%20novel%20FIne-grained%20Representation%20and%20Recomposition%0A%28FIRe%24%5E%7B2%7D%24%29%20framework%20to%20tackle%20both%20limitations%20without%20any%20auxiliary%0Aannotation%20or%20data.%20Specifically%2C%20we%20first%20design%20a%20Fine-grained%20Feature%20Mining%0A%28FFM%29%20module%20to%20separately%20cluster%20images%20of%20each%20person.%20Images%20with%20similar%0Aso-called%20fine-grained%20attributes%20%28e.g.%2C%20clothes%20and%20viewpoints%29%20are%20encouraged%0Ato%20cluster%20together.%20An%20attribute-aware%20classification%20loss%20is%20introduced%20to%0Aperform%20fine-grained%20learning%20based%20on%20cluster%20labels%2C%20which%20are%20not%20shared%0Aamong%20different%20people%2C%20promoting%20the%20model%20to%20learn%20identity-relevant%0Afeatures.%20Furthermore%2C%20to%20take%20full%20advantage%20of%20fine-grained%20attributes%2C%20we%0Apresent%20a%20Fine-grained%20Attribute%20Recomposition%20%28FAR%29%20module%20by%20recomposing%0Aimage%20features%20with%20different%20attributes%20in%20the%20latent%20space.%20It%20significantly%0Aenhances%20robust%20feature%20learning.%20Extensive%20experiments%20demonstrate%20that%0AFIRe%24%5E%7B2%7D%24%20can%20achieve%20state-of-the-art%20performance%20on%20five%20widely-used%0Acloth-changing%20person%20Re-ID%20benchmarks.%20The%20code%20is%20available%20at%0Ahttps%3A//github.com/QizaoWang/FIRe-CCReID.%0A&entry.1838667208=http%3A//arxiv.org/abs/2308.10692v2&entry.124074799=Read"},
{"title": "MM-GTUNets: Unified Multi-Modal Graph Deep Learning for Brain Disorders\n  Prediction", "author": "Luhui Cai and Weiming Zeng and Hongyu Chen and Hua Zhang and Yueyang Li and Hongjie Yan and Lingbin Bian and Nizhuan Wang", "abstract": "  Graph deep learning (GDL) has demonstrated impressive performance in\npredicting population-based brain disorders (BDs) through the integration of\nboth imaging and non-imaging data. However, the effectiveness of GDL based\nmethods heavily depends on the quality of modeling the multi-modal population\ngraphs and tends to degrade as the graph scale increases. Furthermore, these\nmethods often constrain interactions between imaging and non-imaging data to\nnode-edge interactions within the graph, overlooking complex inter-modal\ncorrelations, leading to suboptimal outcomes. To overcome these challenges, we\npropose MM-GTUNets, an end-to-end graph transformer based multi-modal graph\ndeep learning (MMGDL) framework designed for brain disorders prediction at\nlarge scale. Specifically, to effectively leverage rich multi-modal information\nrelated to diseases, we introduce Modality Reward Representation Learning\n(MRRL) which adaptively constructs population graphs using a reward system.\nAdditionally, we employ variational autoencoder to reconstruct latent\nrepresentations of non-imaging features aligned with imaging features. Based on\nthis, we propose Adaptive Cross-Modal Graph Learning (ACMGL), which captures\ncritical modality-specific and modality-shared features through a unified\nGTUNet encoder taking advantages of Graph UNet and Graph Transformer, and\nfeature fusion module. We validated our method on two public multi-modal\ndatasets ABIDE and ADHD-200, demonstrating its superior performance in\ndiagnosing BDs. Our code is available at https://github.com/NZWANG/MM-GTUNets.\n", "link": "http://arxiv.org/abs/2406.14455v1", "date": "2024-06-20", "relevancy": 2.1454, "topK": [{"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5503}, {"title": "UGG: Unified Generative Grasping", "link": "https://arxiv.org/abs/2311.16917", "similarity": 0.5317}, {"title": "Multimodal-Conditioned Latent Diffusion Models for Fashion Image Editing", "link": "http://arxiv.org/abs/2403.14828v2", "similarity": 0.5129}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20MM-GTUNets%3A%20Unified%20Multi-Modal%20Graph%20Deep%20Learning%20for%20Brain%20Disorders%0A%20%20Prediction&body=Title%3A%20MM-GTUNets%3A%20Unified%20Multi-Modal%20Graph%20Deep%20Learning%20for%20Brain%20Disorders%0A%20%20Prediction%0AAuthor%3A%20Luhui%20Cai%20and%20Weiming%20Zeng%20and%20Hongyu%20Chen%20and%20Hua%20Zhang%20and%20Yueyang%20Li%20and%20Hongjie%20Yan%20and%20Lingbin%20Bian%20and%20Nizhuan%20Wang%0AAbstract%3A%20%20%20Graph%20deep%20learning%20%28GDL%29%20has%20demonstrated%20impressive%20performance%20in%0Apredicting%20population-based%20brain%20disorders%20%28BDs%29%20through%20the%20integration%20of%0Aboth%20imaging%20and%20non-imaging%20data.%20However%2C%20the%20effectiveness%20of%20GDL%20based%0Amethods%20heavily%20depends%20on%20the%20quality%20of%20modeling%20the%20multi-modal%20population%0Agraphs%20and%20tends%20to%20degrade%20as%20the%20graph%20scale%20increases.%20Furthermore%2C%20these%0Amethods%20often%20constrain%20interactions%20between%20imaging%20and%20non-imaging%20data%20to%0Anode-edge%20interactions%20within%20the%20graph%2C%20overlooking%20complex%20inter-modal%0Acorrelations%2C%20leading%20to%20suboptimal%20outcomes.%20To%20overcome%20these%20challenges%2C%20we%0Apropose%20MM-GTUNets%2C%20an%20end-to-end%20graph%20transformer%20based%20multi-modal%20graph%0Adeep%20learning%20%28MMGDL%29%20framework%20designed%20for%20brain%20disorders%20prediction%20at%0Alarge%20scale.%20Specifically%2C%20to%20effectively%20leverage%20rich%20multi-modal%20information%0Arelated%20to%20diseases%2C%20we%20introduce%20Modality%20Reward%20Representation%20Learning%0A%28MRRL%29%20which%20adaptively%20constructs%20population%20graphs%20using%20a%20reward%20system.%0AAdditionally%2C%20we%20employ%20variational%20autoencoder%20to%20reconstruct%20latent%0Arepresentations%20of%20non-imaging%20features%20aligned%20with%20imaging%20features.%20Based%20on%0Athis%2C%20we%20propose%20Adaptive%20Cross-Modal%20Graph%20Learning%20%28ACMGL%29%2C%20which%20captures%0Acritical%20modality-specific%20and%20modality-shared%20features%20through%20a%20unified%0AGTUNet%20encoder%20taking%20advantages%20of%20Graph%20UNet%20and%20Graph%20Transformer%2C%20and%0Afeature%20fusion%20module.%20We%20validated%20our%20method%20on%20two%20public%20multi-modal%0Adatasets%20ABIDE%20and%20ADHD-200%2C%20demonstrating%20its%20superior%20performance%20in%0Adiagnosing%20BDs.%20Our%20code%20is%20available%20at%20https%3A//github.com/NZWANG/MM-GTUNets.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2406.14455v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DMM-GTUNets%253A%2520Unified%2520Multi-Modal%2520Graph%2520Deep%2520Learning%2520for%2520Brain%2520Disorders%250A%2520%2520Prediction%26entry.906535625%3DLuhui%2520Cai%2520and%2520Weiming%2520Zeng%2520and%2520Hongyu%2520Chen%2520and%2520Hua%2520Zhang%2520and%2520Yueyang%2520Li%2520and%2520Hongjie%2520Yan%2520and%2520Lingbin%2520Bian%2520and%2520Nizhuan%2520Wang%26entry.1292438233%3D%2520%2520Graph%2520deep%2520learning%2520%2528GDL%2529%2520has%2520demonstrated%2520impressive%2520performance%2520in%250Apredicting%2520population-based%2520brain%2520disorders%2520%2528BDs%2529%2520through%2520the%2520integration%2520of%250Aboth%2520imaging%2520and%2520non-imaging%2520data.%2520However%252C%2520the%2520effectiveness%2520of%2520GDL%2520based%250Amethods%2520heavily%2520depends%2520on%2520the%2520quality%2520of%2520modeling%2520the%2520multi-modal%2520population%250Agraphs%2520and%2520tends%2520to%2520degrade%2520as%2520the%2520graph%2520scale%2520increases.%2520Furthermore%252C%2520these%250Amethods%2520often%2520constrain%2520interactions%2520between%2520imaging%2520and%2520non-imaging%2520data%2520to%250Anode-edge%2520interactions%2520within%2520the%2520graph%252C%2520overlooking%2520complex%2520inter-modal%250Acorrelations%252C%2520leading%2520to%2520suboptimal%2520outcomes.%2520To%2520overcome%2520these%2520challenges%252C%2520we%250Apropose%2520MM-GTUNets%252C%2520an%2520end-to-end%2520graph%2520transformer%2520based%2520multi-modal%2520graph%250Adeep%2520learning%2520%2528MMGDL%2529%2520framework%2520designed%2520for%2520brain%2520disorders%2520prediction%2520at%250Alarge%2520scale.%2520Specifically%252C%2520to%2520effectively%2520leverage%2520rich%2520multi-modal%2520information%250Arelated%2520to%2520diseases%252C%2520we%2520introduce%2520Modality%2520Reward%2520Representation%2520Learning%250A%2528MRRL%2529%2520which%2520adaptively%2520constructs%2520population%2520graphs%2520using%2520a%2520reward%2520system.%250AAdditionally%252C%2520we%2520employ%2520variational%2520autoencoder%2520to%2520reconstruct%2520latent%250Arepresentations%2520of%2520non-imaging%2520features%2520aligned%2520with%2520imaging%2520features.%2520Based%2520on%250Athis%252C%2520we%2520propose%2520Adaptive%2520Cross-Modal%2520Graph%2520Learning%2520%2528ACMGL%2529%252C%2520which%2520captures%250Acritical%2520modality-specific%2520and%2520modality-shared%2520features%2520through%2520a%2520unified%250AGTUNet%2520encoder%2520taking%2520advantages%2520of%2520Graph%2520UNet%2520and%2520Graph%2520Transformer%252C%2520and%250Afeature%2520fusion%2520module.%2520We%2520validated%2520our%2520method%2520on%2520two%2520public%2520multi-modal%250Adatasets%2520ABIDE%2520and%2520ADHD-200%252C%2520demonstrating%2520its%2520superior%2520performance%2520in%250Adiagnosing%2520BDs.%2520Our%2520code%2520is%2520available%2520at%2520https%253A//github.com/NZWANG/MM-GTUNets.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2406.14455v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=MM-GTUNets%3A%20Unified%20Multi-Modal%20Graph%20Deep%20Learning%20for%20Brain%20Disorders%0A%20%20Prediction&entry.906535625=Luhui%20Cai%20and%20Weiming%20Zeng%20and%20Hongyu%20Chen%20and%20Hua%20Zhang%20and%20Yueyang%20Li%20and%20Hongjie%20Yan%20and%20Lingbin%20Bian%20and%20Nizhuan%20Wang&entry.1292438233=%20%20Graph%20deep%20learning%20%28GDL%29%20has%20demonstrated%20impressive%20performance%20in%0Apredicting%20population-based%20brain%20disorders%20%28BDs%29%20through%20the%20integration%20of%0Aboth%20imaging%20and%20non-imaging%20data.%20However%2C%20the%20effectiveness%20of%20GDL%20based%0Amethods%20heavily%20depends%20on%20the%20quality%20of%20modeling%20the%20multi-modal%20population%0Agraphs%20and%20tends%20to%20degrade%20as%20the%20graph%20scale%20increases.%20Furthermore%2C%20these%0Amethods%20often%20constrain%20interactions%20between%20imaging%20and%20non-imaging%20data%20to%0Anode-edge%20interactions%20within%20the%20graph%2C%20overlooking%20complex%20inter-modal%0Acorrelations%2C%20leading%20to%20suboptimal%20outcomes.%20To%20overcome%20these%20challenges%2C%20we%0Apropose%20MM-GTUNets%2C%20an%20end-to-end%20graph%20transformer%20based%20multi-modal%20graph%0Adeep%20learning%20%28MMGDL%29%20framework%20designed%20for%20brain%20disorders%20prediction%20at%0Alarge%20scale.%20Specifically%2C%20to%20effectively%20leverage%20rich%20multi-modal%20information%0Arelated%20to%20diseases%2C%20we%20introduce%20Modality%20Reward%20Representation%20Learning%0A%28MRRL%29%20which%20adaptively%20constructs%20population%20graphs%20using%20a%20reward%20system.%0AAdditionally%2C%20we%20employ%20variational%20autoencoder%20to%20reconstruct%20latent%0Arepresentations%20of%20non-imaging%20features%20aligned%20with%20imaging%20features.%20Based%20on%0Athis%2C%20we%20propose%20Adaptive%20Cross-Modal%20Graph%20Learning%20%28ACMGL%29%2C%20which%20captures%0Acritical%20modality-specific%20and%20modality-shared%20features%20through%20a%20unified%0AGTUNet%20encoder%20taking%20advantages%20of%20Graph%20UNet%20and%20Graph%20Transformer%2C%20and%0Afeature%20fusion%20module.%20We%20validated%20our%20method%20on%20two%20public%20multi-modal%0Adatasets%20ABIDE%20and%20ADHD-200%2C%20demonstrating%20its%20superior%20performance%20in%0Adiagnosing%20BDs.%20Our%20code%20is%20available%20at%20https%3A//github.com/NZWANG/MM-GTUNets.%0A&entry.1838667208=http%3A//arxiv.org/abs/2406.14455v1&entry.124074799=Read"},
{"title": "Self-supervised Learning for Human Activity Recognition Using 700,000\n  Person-days of Wearable Data", "author": "Hang Yuan and Shing Chan and Andrew P. Creagh and Catherine Tong and Aidan Acquah and David A. Clifton and Aiden Doherty", "abstract": "  Advances in deep learning for human activity recognition have been relatively\nlimited due to the lack of large labelled datasets. In this study, we leverage\nself-supervised learning techniques on the UK-Biobank activity tracker\ndataset--the largest of its kind to date--containing more than 700,000\nperson-days of unlabelled wearable sensor data. Our resulting activity\nrecognition model consistently outperformed strong baselines across seven\nbenchmark datasets, with an F1 relative improvement of 2.5%-100% (median\n18.4%), the largest improvements occurring in the smaller datasets. In contrast\nto previous studies, our results generalise across external datasets, devices,\nand environments. Our open-source model will help researchers and developers to\nbuild customisable and generalisable activity classifiers with high\nperformance.\n", "link": "http://arxiv.org/abs/2206.02909v3", "date": "2024-06-20", "relevancy": 2.1452, "topK": [{"title": "Weakly-guided self-supervised pretraining for temporal activity detection", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/25189", "similarity": 0.6083}, {"title": "Self-supervised Photographic Image Layout Representation Learning", "link": "http://arxiv.org/abs/2403.03740v1", "similarity": 0.4911}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.4693}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Self-supervised%20Learning%20for%20Human%20Activity%20Recognition%20Using%20700%2C000%0A%20%20Person-days%20of%20Wearable%20Data&body=Title%3A%20Self-supervised%20Learning%20for%20Human%20Activity%20Recognition%20Using%20700%2C000%0A%20%20Person-days%20of%20Wearable%20Data%0AAuthor%3A%20Hang%20Yuan%20and%20Shing%20Chan%20and%20Andrew%20P.%20Creagh%20and%20Catherine%20Tong%20and%20Aidan%20Acquah%20and%20David%20A.%20Clifton%20and%20Aiden%20Doherty%0AAbstract%3A%20%20%20Advances%20in%20deep%20learning%20for%20human%20activity%20recognition%20have%20been%20relatively%0Alimited%20due%20to%20the%20lack%20of%20large%20labelled%20datasets.%20In%20this%20study%2C%20we%20leverage%0Aself-supervised%20learning%20techniques%20on%20the%20UK-Biobank%20activity%20tracker%0Adataset--the%20largest%20of%20its%20kind%20to%20date--containing%20more%20than%20700%2C000%0Aperson-days%20of%20unlabelled%20wearable%20sensor%20data.%20Our%20resulting%20activity%0Arecognition%20model%20consistently%20outperformed%20strong%20baselines%20across%20seven%0Abenchmark%20datasets%2C%20with%20an%20F1%20relative%20improvement%20of%202.5%25-100%25%20%28median%0A18.4%25%29%2C%20the%20largest%20improvements%20occurring%20in%20the%20smaller%20datasets.%20In%20contrast%0Ato%20previous%20studies%2C%20our%20results%20generalise%20across%20external%20datasets%2C%20devices%2C%0Aand%20environments.%20Our%20open-source%20model%20will%20help%20researchers%20and%20developers%20to%0Abuild%20customisable%20and%20generalisable%20activity%20classifiers%20with%20high%0Aperformance.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2206.02909v3%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DSelf-supervised%2520Learning%2520for%2520Human%2520Activity%2520Recognition%2520Using%2520700%252C000%250A%2520%2520Person-days%2520of%2520Wearable%2520Data%26entry.906535625%3DHang%2520Yuan%2520and%2520Shing%2520Chan%2520and%2520Andrew%2520P.%2520Creagh%2520and%2520Catherine%2520Tong%2520and%2520Aidan%2520Acquah%2520and%2520David%2520A.%2520Clifton%2520and%2520Aiden%2520Doherty%26entry.1292438233%3D%2520%2520Advances%2520in%2520deep%2520learning%2520for%2520human%2520activity%2520recognition%2520have%2520been%2520relatively%250Alimited%2520due%2520to%2520the%2520lack%2520of%2520large%2520labelled%2520datasets.%2520In%2520this%2520study%252C%2520we%2520leverage%250Aself-supervised%2520learning%2520techniques%2520on%2520the%2520UK-Biobank%2520activity%2520tracker%250Adataset--the%2520largest%2520of%2520its%2520kind%2520to%2520date--containing%2520more%2520than%2520700%252C000%250Aperson-days%2520of%2520unlabelled%2520wearable%2520sensor%2520data.%2520Our%2520resulting%2520activity%250Arecognition%2520model%2520consistently%2520outperformed%2520strong%2520baselines%2520across%2520seven%250Abenchmark%2520datasets%252C%2520with%2520an%2520F1%2520relative%2520improvement%2520of%25202.5%2525-100%2525%2520%2528median%250A18.4%2525%2529%252C%2520the%2520largest%2520improvements%2520occurring%2520in%2520the%2520smaller%2520datasets.%2520In%2520contrast%250Ato%2520previous%2520studies%252C%2520our%2520results%2520generalise%2520across%2520external%2520datasets%252C%2520devices%252C%250Aand%2520environments.%2520Our%2520open-source%2520model%2520will%2520help%2520researchers%2520and%2520developers%2520to%250Abuild%2520customisable%2520and%2520generalisable%2520activity%2520classifiers%2520with%2520high%250Aperformance.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2206.02909v3%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Self-supervised%20Learning%20for%20Human%20Activity%20Recognition%20Using%20700%2C000%0A%20%20Person-days%20of%20Wearable%20Data&entry.906535625=Hang%20Yuan%20and%20Shing%20Chan%20and%20Andrew%20P.%20Creagh%20and%20Catherine%20Tong%20and%20Aidan%20Acquah%20and%20David%20A.%20Clifton%20and%20Aiden%20Doherty&entry.1292438233=%20%20Advances%20in%20deep%20learning%20for%20human%20activity%20recognition%20have%20been%20relatively%0Alimited%20due%20to%20the%20lack%20of%20large%20labelled%20datasets.%20In%20this%20study%2C%20we%20leverage%0Aself-supervised%20learning%20techniques%20on%20the%20UK-Biobank%20activity%20tracker%0Adataset--the%20largest%20of%20its%20kind%20to%20date--containing%20more%20than%20700%2C000%0Aperson-days%20of%20unlabelled%20wearable%20sensor%20data.%20Our%20resulting%20activity%0Arecognition%20model%20consistently%20outperformed%20strong%20baselines%20across%20seven%0Abenchmark%20datasets%2C%20with%20an%20F1%20relative%20improvement%20of%202.5%25-100%25%20%28median%0A18.4%25%29%2C%20the%20largest%20improvements%20occurring%20in%20the%20smaller%20datasets.%20In%20contrast%0Ato%20previous%20studies%2C%20our%20results%20generalise%20across%20external%20datasets%2C%20devices%2C%0Aand%20environments.%20Our%20open-source%20model%20will%20help%20researchers%20and%20developers%20to%0Abuild%20customisable%20and%20generalisable%20activity%20classifiers%20with%20high%0Aperformance.%0A&entry.1838667208=http%3A//arxiv.org/abs/2206.02909v3&entry.124074799=Read"},
{"title": "A Multi-Stream Fusion Approach with One-Class Learning for Audio-Visual\n  Deepfake Detection", "author": "Kyungbok Lee and You Zhang and Zhiyao Duan", "abstract": "  This paper addresses the challenge of developing a robust audio-visual\ndeepfake detection model. In practical use cases, new generation algorithms are\ncontinually emerging, and these algorithms are not encountered during the\ndevelopment of detection methods. This calls for the generalization ability of\nthe method. Additionally, to ensure the credibility of detection methods, it is\nbeneficial for the model to interpret which cues from the video indicate it is\nfake. Motivated by these considerations, we then propose a multi-stream fusion\napproach with one-class learning as a representation-level regularization\ntechnique. We study the generalization problem of audio-visual deepfake\ndetection by creating a new benchmark by extending and re-splitting the\nexisting FakeAVCeleb dataset. The benchmark contains four categories of fake\nvideo(Real Audio-Fake Visual, Fake Audio-Fake Visual, Fake Audio-Real Visual,\nand unsynchronized video). The experimental results show that our approach\nimproves the model's detection of unseen attacks by an average of 7.31% across\nfour test sets, compared to the baseline model. Additionally, our proposed\nframework offers interpretability, indicating which modality the model\nidentifies as fake.\n", "link": "http://arxiv.org/abs/2406.14176v1", "date": "2024-06-20", "relevancy": 2.1435, "topK": [{"title": "Customize-A-Video: One-Shot Motion Customization of Text-to-Video\n  Diffusion Models", "link": "http://arxiv.org/abs/2402.14780v1", "similarity": 0.5461}, {"title": "Tuning-Free Noise Rectification for High Fidelity Image-to-Video\n  Generation", "link": "http://arxiv.org/abs/2403.02827v1", "similarity": 0.5352}, {"title": "Magic-Me: Identity-Specific Video Customized Diffusion", "link": "http://arxiv.org/abs/2402.09368v1", "similarity": 0.5324}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20A%20Multi-Stream%20Fusion%20Approach%20with%20One-Class%20Learning%20for%20Audio-Visual%0A%20%20Deepfake%20Detection&body=Title%3A%20A%20Multi-Stream%20Fusion%20Approach%20with%20One-Class%20Learning%20for%20Audio-Visual%0A%20%20Deepfake%20Detection%0AAuthor%3A%20Kyungbok%20Lee%20and%20You%20Zhang%20and%20Zhiyao%20Duan%0AAbstract%3A%20%20%20This%20paper%20addresses%20the%20challenge%20of%20developing%20a%20robust%20audio-visual%0Adeepfake%20detection%20model.%20In%20practical%20use%20cases%2C%20new%20generation%20algorithms%20are%0Acontinually%20emerging%2C%20and%20these%20algorithms%20are%20not%20encountered%20during%20the%0Adevelopment%20of%20detection%20methods.%20This%20calls%20for%20the%20generalization%20ability%20of%0Athe%20method.%20Additionally%2C%20to%20ensure%20the%20credibility%20of%20detection%20methods%2C%20it%20is%0Abeneficial%20for%20the%20model%20to%20interpret%20which%20cues%20from%20the%20video%20indicate%20it%20is%0Afake.%20Motivated%20by%20these%20considerations%2C%20we%20then%20propose%20a%20multi-stream%20fusion%0Aapproach%20with%20one-class%20learning%20as%20a%20representation-level%20regularization%0Atechnique.%20We%20study%20the%20generalization%20problem%20of%20audio-visual%20deepfake%0Adetection%20by%20creating%20a%20new%20benchmark%20by%20extending%20and%20re-splitting%20the%0Aexisting%20FakeAVCeleb%20dataset.%20The%20benchmark%20contains%20four%20categories%20of%20fake%0Avideo%28Real%20Audio-Fake%20Visual%2C%20Fake%20Audio-Fake%20Visual%2C%20Fake%20Audio-Real%20Visual%2C%0Aand%20unsynchronized%20video%29.%20The%20experimental%20results%20show%20that%20our%20approach%0Aimproves%20the%20model%27s%20detection%20of%20unseen%20attacks%20by%20an%20average%20of%207.31%25%20across%0Afour%20test%20sets%2C%20compared%20to%20the%20baseline%20model.%20Additionally%2C%20our%20proposed%0Aframework%20offers%20interpretability%2C%20indicating%20which%20modality%20the%20model%0Aidentifies%20as%20fake.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2406.14176v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DA%2520Multi-Stream%2520Fusion%2520Approach%2520with%2520One-Class%2520Learning%2520for%2520Audio-Visual%250A%2520%2520Deepfake%2520Detection%26entry.906535625%3DKyungbok%2520Lee%2520and%2520You%2520Zhang%2520and%2520Zhiyao%2520Duan%26entry.1292438233%3D%2520%2520This%2520paper%2520addresses%2520the%2520challenge%2520of%2520developing%2520a%2520robust%2520audio-visual%250Adeepfake%2520detection%2520model.%2520In%2520practical%2520use%2520cases%252C%2520new%2520generation%2520algorithms%2520are%250Acontinually%2520emerging%252C%2520and%2520these%2520algorithms%2520are%2520not%2520encountered%2520during%2520the%250Adevelopment%2520of%2520detection%2520methods.%2520This%2520calls%2520for%2520the%2520generalization%2520ability%2520of%250Athe%2520method.%2520Additionally%252C%2520to%2520ensure%2520the%2520credibility%2520of%2520detection%2520methods%252C%2520it%2520is%250Abeneficial%2520for%2520the%2520model%2520to%2520interpret%2520which%2520cues%2520from%2520the%2520video%2520indicate%2520it%2520is%250Afake.%2520Motivated%2520by%2520these%2520considerations%252C%2520we%2520then%2520propose%2520a%2520multi-stream%2520fusion%250Aapproach%2520with%2520one-class%2520learning%2520as%2520a%2520representation-level%2520regularization%250Atechnique.%2520We%2520study%2520the%2520generalization%2520problem%2520of%2520audio-visual%2520deepfake%250Adetection%2520by%2520creating%2520a%2520new%2520benchmark%2520by%2520extending%2520and%2520re-splitting%2520the%250Aexisting%2520FakeAVCeleb%2520dataset.%2520The%2520benchmark%2520contains%2520four%2520categories%2520of%2520fake%250Avideo%2528Real%2520Audio-Fake%2520Visual%252C%2520Fake%2520Audio-Fake%2520Visual%252C%2520Fake%2520Audio-Real%2520Visual%252C%250Aand%2520unsynchronized%2520video%2529.%2520The%2520experimental%2520results%2520show%2520that%2520our%2520approach%250Aimproves%2520the%2520model%2527s%2520detection%2520of%2520unseen%2520attacks%2520by%2520an%2520average%2520of%25207.31%2525%2520across%250Afour%2520test%2520sets%252C%2520compared%2520to%2520the%2520baseline%2520model.%2520Additionally%252C%2520our%2520proposed%250Aframework%2520offers%2520interpretability%252C%2520indicating%2520which%2520modality%2520the%2520model%250Aidentifies%2520as%2520fake.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2406.14176v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=A%20Multi-Stream%20Fusion%20Approach%20with%20One-Class%20Learning%20for%20Audio-Visual%0A%20%20Deepfake%20Detection&entry.906535625=Kyungbok%20Lee%20and%20You%20Zhang%20and%20Zhiyao%20Duan&entry.1292438233=%20%20This%20paper%20addresses%20the%20challenge%20of%20developing%20a%20robust%20audio-visual%0Adeepfake%20detection%20model.%20In%20practical%20use%20cases%2C%20new%20generation%20algorithms%20are%0Acontinually%20emerging%2C%20and%20these%20algorithms%20are%20not%20encountered%20during%20the%0Adevelopment%20of%20detection%20methods.%20This%20calls%20for%20the%20generalization%20ability%20of%0Athe%20method.%20Additionally%2C%20to%20ensure%20the%20credibility%20of%20detection%20methods%2C%20it%20is%0Abeneficial%20for%20the%20model%20to%20interpret%20which%20cues%20from%20the%20video%20indicate%20it%20is%0Afake.%20Motivated%20by%20these%20considerations%2C%20we%20then%20propose%20a%20multi-stream%20fusion%0Aapproach%20with%20one-class%20learning%20as%20a%20representation-level%20regularization%0Atechnique.%20We%20study%20the%20generalization%20problem%20of%20audio-visual%20deepfake%0Adetection%20by%20creating%20a%20new%20benchmark%20by%20extending%20and%20re-splitting%20the%0Aexisting%20FakeAVCeleb%20dataset.%20The%20benchmark%20contains%20four%20categories%20of%20fake%0Avideo%28Real%20Audio-Fake%20Visual%2C%20Fake%20Audio-Fake%20Visual%2C%20Fake%20Audio-Real%20Visual%2C%0Aand%20unsynchronized%20video%29.%20The%20experimental%20results%20show%20that%20our%20approach%0Aimproves%20the%20model%27s%20detection%20of%20unseen%20attacks%20by%20an%20average%20of%207.31%25%20across%0Afour%20test%20sets%2C%20compared%20to%20the%20baseline%20model.%20Additionally%2C%20our%20proposed%0Aframework%20offers%20interpretability%2C%20indicating%20which%20modality%20the%20model%0Aidentifies%20as%20fake.%0A&entry.1838667208=http%3A//arxiv.org/abs/2406.14176v1&entry.124074799=Read"},
{"title": "Active Diffusion Subsampling", "author": "Oisin Nolan and Tristan S. W. Stevens and Wessel L. van Nierop and Ruud J. G. van Sloun", "abstract": "  Subsampling is commonly used to mitigate costs associated with data\nacquisition, such as time or energy requirements, motivating the development of\nalgorithms for estimating the fully-sampled signal of interest $x$ from\npartially observed measurements $y$. In maximum-entropy sampling, one selects\nmeasurement locations that are expected to have the highest entropy, so as to\nminimize uncertainty about $x$. This approach relies on an accurate model of\nthe posterior distribution over future measurements, given the measurements\nobserved so far. Recently, diffusion models have been shown to produce\nhigh-quality posterior samples of high-dimensional signals using guided\ndiffusion. In this work, we propose Active Diffusion Subsampling (ADS), a\nmethod for performing active subsampling using guided diffusion in which the\nmodel tracks a distribution of beliefs over the true state of $x$ throughout\nthe reverse diffusion process, progressively decreasing its uncertainty by\nchoosing to acquire measurements with maximum expected entropy, and ultimately\ngenerating the posterior distribution $p(x | y)$. ADS can be applied using\npre-trained diffusion models for any subsampling rate, and does not require\ntask-specific retraining - just the specification of a measurement model.\nFurthermore, the maximum entropy sampling policy employed by ADS is\ninterpretable, enhancing transparency relative to existing methods using\nblack-box policies. Experimentally, we show that ADS outperforms fixed sampling\nstrategies, and study an application of ADS in Magnetic Resonance Imaging\nacceleration using the fastMRI dataset, finding that ADS performs competitively\nwith supervised methods. Code available at\nhttps://active-diffusion-subsampling.github.io/.\n", "link": "http://arxiv.org/abs/2406.14388v1", "date": "2024-06-20", "relevancy": 2.1433, "topK": [{"title": "SDXS: Real-Time One-Step Latent Diffusion Models with Image Conditions", "link": "http://arxiv.org/abs/2403.16627v1", "similarity": 0.5952}, {"title": "ToDo: Token Downsampling for Efficient Generation of High-Resolution\n  Images", "link": "http://arxiv.org/abs/2402.13573v2", "similarity": 0.5366}, {"title": "Breadcrumbs: Adversarial class-balanced sampling for long-tailed recognition", "link": "https://link.springer.com/chapter/10.1007/978-3-031-20053-3_37", "similarity": 0.5113}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Active%20Diffusion%20Subsampling&body=Title%3A%20Active%20Diffusion%20Subsampling%0AAuthor%3A%20Oisin%20Nolan%20and%20Tristan%20S.%20W.%20Stevens%20and%20Wessel%20L.%20van%20Nierop%20and%20Ruud%20J.%20G.%20van%20Sloun%0AAbstract%3A%20%20%20Subsampling%20is%20commonly%20used%20to%20mitigate%20costs%20associated%20with%20data%0Aacquisition%2C%20such%20as%20time%20or%20energy%20requirements%2C%20motivating%20the%20development%20of%0Aalgorithms%20for%20estimating%20the%20fully-sampled%20signal%20of%20interest%20%24x%24%20from%0Apartially%20observed%20measurements%20%24y%24.%20In%20maximum-entropy%20sampling%2C%20one%20selects%0Ameasurement%20locations%20that%20are%20expected%20to%20have%20the%20highest%20entropy%2C%20so%20as%20to%0Aminimize%20uncertainty%20about%20%24x%24.%20This%20approach%20relies%20on%20an%20accurate%20model%20of%0Athe%20posterior%20distribution%20over%20future%20measurements%2C%20given%20the%20measurements%0Aobserved%20so%20far.%20Recently%2C%20diffusion%20models%20have%20been%20shown%20to%20produce%0Ahigh-quality%20posterior%20samples%20of%20high-dimensional%20signals%20using%20guided%0Adiffusion.%20In%20this%20work%2C%20we%20propose%20Active%20Diffusion%20Subsampling%20%28ADS%29%2C%20a%0Amethod%20for%20performing%20active%20subsampling%20using%20guided%20diffusion%20in%20which%20the%0Amodel%20tracks%20a%20distribution%20of%20beliefs%20over%20the%20true%20state%20of%20%24x%24%20throughout%0Athe%20reverse%20diffusion%20process%2C%20progressively%20decreasing%20its%20uncertainty%20by%0Achoosing%20to%20acquire%20measurements%20with%20maximum%20expected%20entropy%2C%20and%20ultimately%0Agenerating%20the%20posterior%20distribution%20%24p%28x%20%7C%20y%29%24.%20ADS%20can%20be%20applied%20using%0Apre-trained%20diffusion%20models%20for%20any%20subsampling%20rate%2C%20and%20does%20not%20require%0Atask-specific%20retraining%20-%20just%20the%20specification%20of%20a%20measurement%20model.%0AFurthermore%2C%20the%20maximum%20entropy%20sampling%20policy%20employed%20by%20ADS%20is%0Ainterpretable%2C%20enhancing%20transparency%20relative%20to%20existing%20methods%20using%0Ablack-box%20policies.%20Experimentally%2C%20we%20show%20that%20ADS%20outperforms%20fixed%20sampling%0Astrategies%2C%20and%20study%20an%20application%20of%20ADS%20in%20Magnetic%20Resonance%20Imaging%0Aacceleration%20using%20the%20fastMRI%20dataset%2C%20finding%20that%20ADS%20performs%20competitively%0Awith%20supervised%20methods.%20Code%20available%20at%0Ahttps%3A//active-diffusion-subsampling.github.io/.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2406.14388v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DActive%2520Diffusion%2520Subsampling%26entry.906535625%3DOisin%2520Nolan%2520and%2520Tristan%2520S.%2520W.%2520Stevens%2520and%2520Wessel%2520L.%2520van%2520Nierop%2520and%2520Ruud%2520J.%2520G.%2520van%2520Sloun%26entry.1292438233%3D%2520%2520Subsampling%2520is%2520commonly%2520used%2520to%2520mitigate%2520costs%2520associated%2520with%2520data%250Aacquisition%252C%2520such%2520as%2520time%2520or%2520energy%2520requirements%252C%2520motivating%2520the%2520development%2520of%250Aalgorithms%2520for%2520estimating%2520the%2520fully-sampled%2520signal%2520of%2520interest%2520%2524x%2524%2520from%250Apartially%2520observed%2520measurements%2520%2524y%2524.%2520In%2520maximum-entropy%2520sampling%252C%2520one%2520selects%250Ameasurement%2520locations%2520that%2520are%2520expected%2520to%2520have%2520the%2520highest%2520entropy%252C%2520so%2520as%2520to%250Aminimize%2520uncertainty%2520about%2520%2524x%2524.%2520This%2520approach%2520relies%2520on%2520an%2520accurate%2520model%2520of%250Athe%2520posterior%2520distribution%2520over%2520future%2520measurements%252C%2520given%2520the%2520measurements%250Aobserved%2520so%2520far.%2520Recently%252C%2520diffusion%2520models%2520have%2520been%2520shown%2520to%2520produce%250Ahigh-quality%2520posterior%2520samples%2520of%2520high-dimensional%2520signals%2520using%2520guided%250Adiffusion.%2520In%2520this%2520work%252C%2520we%2520propose%2520Active%2520Diffusion%2520Subsampling%2520%2528ADS%2529%252C%2520a%250Amethod%2520for%2520performing%2520active%2520subsampling%2520using%2520guided%2520diffusion%2520in%2520which%2520the%250Amodel%2520tracks%2520a%2520distribution%2520of%2520beliefs%2520over%2520the%2520true%2520state%2520of%2520%2524x%2524%2520throughout%250Athe%2520reverse%2520diffusion%2520process%252C%2520progressively%2520decreasing%2520its%2520uncertainty%2520by%250Achoosing%2520to%2520acquire%2520measurements%2520with%2520maximum%2520expected%2520entropy%252C%2520and%2520ultimately%250Agenerating%2520the%2520posterior%2520distribution%2520%2524p%2528x%2520%257C%2520y%2529%2524.%2520ADS%2520can%2520be%2520applied%2520using%250Apre-trained%2520diffusion%2520models%2520for%2520any%2520subsampling%2520rate%252C%2520and%2520does%2520not%2520require%250Atask-specific%2520retraining%2520-%2520just%2520the%2520specification%2520of%2520a%2520measurement%2520model.%250AFurthermore%252C%2520the%2520maximum%2520entropy%2520sampling%2520policy%2520employed%2520by%2520ADS%2520is%250Ainterpretable%252C%2520enhancing%2520transparency%2520relative%2520to%2520existing%2520methods%2520using%250Ablack-box%2520policies.%2520Experimentally%252C%2520we%2520show%2520that%2520ADS%2520outperforms%2520fixed%2520sampling%250Astrategies%252C%2520and%2520study%2520an%2520application%2520of%2520ADS%2520in%2520Magnetic%2520Resonance%2520Imaging%250Aacceleration%2520using%2520the%2520fastMRI%2520dataset%252C%2520finding%2520that%2520ADS%2520performs%2520competitively%250Awith%2520supervised%2520methods.%2520Code%2520available%2520at%250Ahttps%253A//active-diffusion-subsampling.github.io/.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2406.14388v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Active%20Diffusion%20Subsampling&entry.906535625=Oisin%20Nolan%20and%20Tristan%20S.%20W.%20Stevens%20and%20Wessel%20L.%20van%20Nierop%20and%20Ruud%20J.%20G.%20van%20Sloun&entry.1292438233=%20%20Subsampling%20is%20commonly%20used%20to%20mitigate%20costs%20associated%20with%20data%0Aacquisition%2C%20such%20as%20time%20or%20energy%20requirements%2C%20motivating%20the%20development%20of%0Aalgorithms%20for%20estimating%20the%20fully-sampled%20signal%20of%20interest%20%24x%24%20from%0Apartially%20observed%20measurements%20%24y%24.%20In%20maximum-entropy%20sampling%2C%20one%20selects%0Ameasurement%20locations%20that%20are%20expected%20to%20have%20the%20highest%20entropy%2C%20so%20as%20to%0Aminimize%20uncertainty%20about%20%24x%24.%20This%20approach%20relies%20on%20an%20accurate%20model%20of%0Athe%20posterior%20distribution%20over%20future%20measurements%2C%20given%20the%20measurements%0Aobserved%20so%20far.%20Recently%2C%20diffusion%20models%20have%20been%20shown%20to%20produce%0Ahigh-quality%20posterior%20samples%20of%20high-dimensional%20signals%20using%20guided%0Adiffusion.%20In%20this%20work%2C%20we%20propose%20Active%20Diffusion%20Subsampling%20%28ADS%29%2C%20a%0Amethod%20for%20performing%20active%20subsampling%20using%20guided%20diffusion%20in%20which%20the%0Amodel%20tracks%20a%20distribution%20of%20beliefs%20over%20the%20true%20state%20of%20%24x%24%20throughout%0Athe%20reverse%20diffusion%20process%2C%20progressively%20decreasing%20its%20uncertainty%20by%0Achoosing%20to%20acquire%20measurements%20with%20maximum%20expected%20entropy%2C%20and%20ultimately%0Agenerating%20the%20posterior%20distribution%20%24p%28x%20%7C%20y%29%24.%20ADS%20can%20be%20applied%20using%0Apre-trained%20diffusion%20models%20for%20any%20subsampling%20rate%2C%20and%20does%20not%20require%0Atask-specific%20retraining%20-%20just%20the%20specification%20of%20a%20measurement%20model.%0AFurthermore%2C%20the%20maximum%20entropy%20sampling%20policy%20employed%20by%20ADS%20is%0Ainterpretable%2C%20enhancing%20transparency%20relative%20to%20existing%20methods%20using%0Ablack-box%20policies.%20Experimentally%2C%20we%20show%20that%20ADS%20outperforms%20fixed%20sampling%0Astrategies%2C%20and%20study%20an%20application%20of%20ADS%20in%20Magnetic%20Resonance%20Imaging%0Aacceleration%20using%20the%20fastMRI%20dataset%2C%20finding%20that%20ADS%20performs%20competitively%0Awith%20supervised%20methods.%20Code%20available%20at%0Ahttps%3A//active-diffusion-subsampling.github.io/.%0A&entry.1838667208=http%3A//arxiv.org/abs/2406.14388v1&entry.124074799=Read"},
{"title": "FutureNet-LOF: Joint Trajectory Prediction and Lane Occupancy Field\n  Prediction with Future Context Encoding", "author": "Mingkun Wang and Xiaoguang Ren and Ruochun Jin and Minglong Li and Xiaochuan Zhang and Changqian Yu and Mingxu Wang and Wenjing Yang", "abstract": "  Most prior motion prediction endeavors in autonomous driving have\ninadequately encoded future scenarios, leading to predictions that may fail to\naccurately capture the diverse movements of agents (e.g., vehicles or\npedestrians). To address this, we propose FutureNet, which explicitly\nintegrates initially predicted trajectories into the future scenario and\nfurther encodes these future contexts to enhance subsequent forecasting.\nAdditionally, most previous motion forecasting works have focused on predicting\nindependent futures for each agent. However, safe and smooth autonomous driving\nrequires accurately predicting the diverse future behaviors of numerous\nsurrounding agents jointly in complex dynamic environments. Given that all\nagents occupy certain potential travel spaces and possess lane driving\npriority, we propose Lane Occupancy Field (LOF), a new representation with lane\nsemantics for motion forecasting in autonomous driving. LOF can simultaneously\ncapture the joint probability distribution of all road participants' future\nspatial-temporal positions. Due to the high compatibility between lane\noccupancy field prediction and trajectory prediction, we propose a novel\nnetwork with future context encoding for the joint prediction of these two\ntasks. Our approach ranks 1st on two large-scale motion forecasting benchmarks:\nArgoverse 1 and Argoverse 2.\n", "link": "http://arxiv.org/abs/2406.14422v1", "date": "2024-06-20", "relevancy": 2.1313, "topK": [{"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.5715}, {"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.5427}, {"title": "Self-supervised Photographic Image Layout Representation Learning", "link": "http://arxiv.org/abs/2403.03740v1", "similarity": 0.5075}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20FutureNet-LOF%3A%20Joint%20Trajectory%20Prediction%20and%20Lane%20Occupancy%20Field%0A%20%20Prediction%20with%20Future%20Context%20Encoding&body=Title%3A%20FutureNet-LOF%3A%20Joint%20Trajectory%20Prediction%20and%20Lane%20Occupancy%20Field%0A%20%20Prediction%20with%20Future%20Context%20Encoding%0AAuthor%3A%20Mingkun%20Wang%20and%20Xiaoguang%20Ren%20and%20Ruochun%20Jin%20and%20Minglong%20Li%20and%20Xiaochuan%20Zhang%20and%20Changqian%20Yu%20and%20Mingxu%20Wang%20and%20Wenjing%20Yang%0AAbstract%3A%20%20%20Most%20prior%20motion%20prediction%20endeavors%20in%20autonomous%20driving%20have%0Ainadequately%20encoded%20future%20scenarios%2C%20leading%20to%20predictions%20that%20may%20fail%20to%0Aaccurately%20capture%20the%20diverse%20movements%20of%20agents%20%28e.g.%2C%20vehicles%20or%0Apedestrians%29.%20To%20address%20this%2C%20we%20propose%20FutureNet%2C%20which%20explicitly%0Aintegrates%20initially%20predicted%20trajectories%20into%20the%20future%20scenario%20and%0Afurther%20encodes%20these%20future%20contexts%20to%20enhance%20subsequent%20forecasting.%0AAdditionally%2C%20most%20previous%20motion%20forecasting%20works%20have%20focused%20on%20predicting%0Aindependent%20futures%20for%20each%20agent.%20However%2C%20safe%20and%20smooth%20autonomous%20driving%0Arequires%20accurately%20predicting%20the%20diverse%20future%20behaviors%20of%20numerous%0Asurrounding%20agents%20jointly%20in%20complex%20dynamic%20environments.%20Given%20that%20all%0Aagents%20occupy%20certain%20potential%20travel%20spaces%20and%20possess%20lane%20driving%0Apriority%2C%20we%20propose%20Lane%20Occupancy%20Field%20%28LOF%29%2C%20a%20new%20representation%20with%20lane%0Asemantics%20for%20motion%20forecasting%20in%20autonomous%20driving.%20LOF%20can%20simultaneously%0Acapture%20the%20joint%20probability%20distribution%20of%20all%20road%20participants%27%20future%0Aspatial-temporal%20positions.%20Due%20to%20the%20high%20compatibility%20between%20lane%0Aoccupancy%20field%20prediction%20and%20trajectory%20prediction%2C%20we%20propose%20a%20novel%0Anetwork%20with%20future%20context%20encoding%20for%20the%20joint%20prediction%20of%20these%20two%0Atasks.%20Our%20approach%20ranks%201st%20on%20two%20large-scale%20motion%20forecasting%20benchmarks%3A%0AArgoverse%201%20and%20Argoverse%202.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2406.14422v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DFutureNet-LOF%253A%2520Joint%2520Trajectory%2520Prediction%2520and%2520Lane%2520Occupancy%2520Field%250A%2520%2520Prediction%2520with%2520Future%2520Context%2520Encoding%26entry.906535625%3DMingkun%2520Wang%2520and%2520Xiaoguang%2520Ren%2520and%2520Ruochun%2520Jin%2520and%2520Minglong%2520Li%2520and%2520Xiaochuan%2520Zhang%2520and%2520Changqian%2520Yu%2520and%2520Mingxu%2520Wang%2520and%2520Wenjing%2520Yang%26entry.1292438233%3D%2520%2520Most%2520prior%2520motion%2520prediction%2520endeavors%2520in%2520autonomous%2520driving%2520have%250Ainadequately%2520encoded%2520future%2520scenarios%252C%2520leading%2520to%2520predictions%2520that%2520may%2520fail%2520to%250Aaccurately%2520capture%2520the%2520diverse%2520movements%2520of%2520agents%2520%2528e.g.%252C%2520vehicles%2520or%250Apedestrians%2529.%2520To%2520address%2520this%252C%2520we%2520propose%2520FutureNet%252C%2520which%2520explicitly%250Aintegrates%2520initially%2520predicted%2520trajectories%2520into%2520the%2520future%2520scenario%2520and%250Afurther%2520encodes%2520these%2520future%2520contexts%2520to%2520enhance%2520subsequent%2520forecasting.%250AAdditionally%252C%2520most%2520previous%2520motion%2520forecasting%2520works%2520have%2520focused%2520on%2520predicting%250Aindependent%2520futures%2520for%2520each%2520agent.%2520However%252C%2520safe%2520and%2520smooth%2520autonomous%2520driving%250Arequires%2520accurately%2520predicting%2520the%2520diverse%2520future%2520behaviors%2520of%2520numerous%250Asurrounding%2520agents%2520jointly%2520in%2520complex%2520dynamic%2520environments.%2520Given%2520that%2520all%250Aagents%2520occupy%2520certain%2520potential%2520travel%2520spaces%2520and%2520possess%2520lane%2520driving%250Apriority%252C%2520we%2520propose%2520Lane%2520Occupancy%2520Field%2520%2528LOF%2529%252C%2520a%2520new%2520representation%2520with%2520lane%250Asemantics%2520for%2520motion%2520forecasting%2520in%2520autonomous%2520driving.%2520LOF%2520can%2520simultaneously%250Acapture%2520the%2520joint%2520probability%2520distribution%2520of%2520all%2520road%2520participants%2527%2520future%250Aspatial-temporal%2520positions.%2520Due%2520to%2520the%2520high%2520compatibility%2520between%2520lane%250Aoccupancy%2520field%2520prediction%2520and%2520trajectory%2520prediction%252C%2520we%2520propose%2520a%2520novel%250Anetwork%2520with%2520future%2520context%2520encoding%2520for%2520the%2520joint%2520prediction%2520of%2520these%2520two%250Atasks.%2520Our%2520approach%2520ranks%25201st%2520on%2520two%2520large-scale%2520motion%2520forecasting%2520benchmarks%253A%250AArgoverse%25201%2520and%2520Argoverse%25202.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2406.14422v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=FutureNet-LOF%3A%20Joint%20Trajectory%20Prediction%20and%20Lane%20Occupancy%20Field%0A%20%20Prediction%20with%20Future%20Context%20Encoding&entry.906535625=Mingkun%20Wang%20and%20Xiaoguang%20Ren%20and%20Ruochun%20Jin%20and%20Minglong%20Li%20and%20Xiaochuan%20Zhang%20and%20Changqian%20Yu%20and%20Mingxu%20Wang%20and%20Wenjing%20Yang&entry.1292438233=%20%20Most%20prior%20motion%20prediction%20endeavors%20in%20autonomous%20driving%20have%0Ainadequately%20encoded%20future%20scenarios%2C%20leading%20to%20predictions%20that%20may%20fail%20to%0Aaccurately%20capture%20the%20diverse%20movements%20of%20agents%20%28e.g.%2C%20vehicles%20or%0Apedestrians%29.%20To%20address%20this%2C%20we%20propose%20FutureNet%2C%20which%20explicitly%0Aintegrates%20initially%20predicted%20trajectories%20into%20the%20future%20scenario%20and%0Afurther%20encodes%20these%20future%20contexts%20to%20enhance%20subsequent%20forecasting.%0AAdditionally%2C%20most%20previous%20motion%20forecasting%20works%20have%20focused%20on%20predicting%0Aindependent%20futures%20for%20each%20agent.%20However%2C%20safe%20and%20smooth%20autonomous%20driving%0Arequires%20accurately%20predicting%20the%20diverse%20future%20behaviors%20of%20numerous%0Asurrounding%20agents%20jointly%20in%20complex%20dynamic%20environments.%20Given%20that%20all%0Aagents%20occupy%20certain%20potential%20travel%20spaces%20and%20possess%20lane%20driving%0Apriority%2C%20we%20propose%20Lane%20Occupancy%20Field%20%28LOF%29%2C%20a%20new%20representation%20with%20lane%0Asemantics%20for%20motion%20forecasting%20in%20autonomous%20driving.%20LOF%20can%20simultaneously%0Acapture%20the%20joint%20probability%20distribution%20of%20all%20road%20participants%27%20future%0Aspatial-temporal%20positions.%20Due%20to%20the%20high%20compatibility%20between%20lane%0Aoccupancy%20field%20prediction%20and%20trajectory%20prediction%2C%20we%20propose%20a%20novel%0Anetwork%20with%20future%20context%20encoding%20for%20the%20joint%20prediction%20of%20these%20two%0Atasks.%20Our%20approach%20ranks%201st%20on%20two%20large-scale%20motion%20forecasting%20benchmarks%3A%0AArgoverse%201%20and%20Argoverse%202.%0A&entry.1838667208=http%3A//arxiv.org/abs/2406.14422v1&entry.124074799=Read"},
{"title": "Predicting Probabilities of Error to Combine Quantization and Early\n  Exiting: QuEE", "author": "Florence Regol and Joud Chataoui and Bertrand Charpentier and Mark Coates and Pablo Piantanida and Stephan Gunnemann", "abstract": "  Machine learning models can solve complex tasks but often require significant\ncomputational resources during inference. This has led to the development of\nvarious post-training computation reduction methods that tackle this issue in\ndifferent ways, such as quantization which reduces the precision of weights and\narithmetic operations, and dynamic networks which adapt computation to the\nsample at hand. In this work, we propose a more general dynamic network that\ncan combine both quantization and early exit dynamic network: QuEE. Our\nalgorithm can be seen as a form of soft early exiting or input-dependent\ncompression. Rather than a binary decision between exiting or continuing, we\nintroduce the possibility of continuing with reduced computation. This\ncomplicates the traditionally considered early exiting problem, which we solve\nthrough a principled formulation. The crucial factor of our approach is\naccurate prediction of the potential accuracy improvement achievable through\nfurther computation. We demonstrate the effectiveness of our method through\nempirical evaluation, as well as exploring the conditions for its success on 4\nclassification datasets.\n", "link": "http://arxiv.org/abs/2406.14404v1", "date": "2024-06-20", "relevancy": 2.1293, "topK": [{"title": "Boosted dynamic neural networks", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/26302", "similarity": 0.5694}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.514}, {"title": "ToDo: Token Downsampling for Efficient Generation of High-Resolution\n  Images", "link": "http://arxiv.org/abs/2402.13573v2", "similarity": 0.5025}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Predicting%20Probabilities%20of%20Error%20to%20Combine%20Quantization%20and%20Early%0A%20%20Exiting%3A%20QuEE&body=Title%3A%20Predicting%20Probabilities%20of%20Error%20to%20Combine%20Quantization%20and%20Early%0A%20%20Exiting%3A%20QuEE%0AAuthor%3A%20Florence%20Regol%20and%20Joud%20Chataoui%20and%20Bertrand%20Charpentier%20and%20Mark%20Coates%20and%20Pablo%20Piantanida%20and%20Stephan%20Gunnemann%0AAbstract%3A%20%20%20Machine%20learning%20models%20can%20solve%20complex%20tasks%20but%20often%20require%20significant%0Acomputational%20resources%20during%20inference.%20This%20has%20led%20to%20the%20development%20of%0Avarious%20post-training%20computation%20reduction%20methods%20that%20tackle%20this%20issue%20in%0Adifferent%20ways%2C%20such%20as%20quantization%20which%20reduces%20the%20precision%20of%20weights%20and%0Aarithmetic%20operations%2C%20and%20dynamic%20networks%20which%20adapt%20computation%20to%20the%0Asample%20at%20hand.%20In%20this%20work%2C%20we%20propose%20a%20more%20general%20dynamic%20network%20that%0Acan%20combine%20both%20quantization%20and%20early%20exit%20dynamic%20network%3A%20QuEE.%20Our%0Aalgorithm%20can%20be%20seen%20as%20a%20form%20of%20soft%20early%20exiting%20or%20input-dependent%0Acompression.%20Rather%20than%20a%20binary%20decision%20between%20exiting%20or%20continuing%2C%20we%0Aintroduce%20the%20possibility%20of%20continuing%20with%20reduced%20computation.%20This%0Acomplicates%20the%20traditionally%20considered%20early%20exiting%20problem%2C%20which%20we%20solve%0Athrough%20a%20principled%20formulation.%20The%20crucial%20factor%20of%20our%20approach%20is%0Aaccurate%20prediction%20of%20the%20potential%20accuracy%20improvement%20achievable%20through%0Afurther%20computation.%20We%20demonstrate%20the%20effectiveness%20of%20our%20method%20through%0Aempirical%20evaluation%2C%20as%20well%20as%20exploring%20the%20conditions%20for%20its%20success%20on%204%0Aclassification%20datasets.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2406.14404v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DPredicting%2520Probabilities%2520of%2520Error%2520to%2520Combine%2520Quantization%2520and%2520Early%250A%2520%2520Exiting%253A%2520QuEE%26entry.906535625%3DFlorence%2520Regol%2520and%2520Joud%2520Chataoui%2520and%2520Bertrand%2520Charpentier%2520and%2520Mark%2520Coates%2520and%2520Pablo%2520Piantanida%2520and%2520Stephan%2520Gunnemann%26entry.1292438233%3D%2520%2520Machine%2520learning%2520models%2520can%2520solve%2520complex%2520tasks%2520but%2520often%2520require%2520significant%250Acomputational%2520resources%2520during%2520inference.%2520This%2520has%2520led%2520to%2520the%2520development%2520of%250Avarious%2520post-training%2520computation%2520reduction%2520methods%2520that%2520tackle%2520this%2520issue%2520in%250Adifferent%2520ways%252C%2520such%2520as%2520quantization%2520which%2520reduces%2520the%2520precision%2520of%2520weights%2520and%250Aarithmetic%2520operations%252C%2520and%2520dynamic%2520networks%2520which%2520adapt%2520computation%2520to%2520the%250Asample%2520at%2520hand.%2520In%2520this%2520work%252C%2520we%2520propose%2520a%2520more%2520general%2520dynamic%2520network%2520that%250Acan%2520combine%2520both%2520quantization%2520and%2520early%2520exit%2520dynamic%2520network%253A%2520QuEE.%2520Our%250Aalgorithm%2520can%2520be%2520seen%2520as%2520a%2520form%2520of%2520soft%2520early%2520exiting%2520or%2520input-dependent%250Acompression.%2520Rather%2520than%2520a%2520binary%2520decision%2520between%2520exiting%2520or%2520continuing%252C%2520we%250Aintroduce%2520the%2520possibility%2520of%2520continuing%2520with%2520reduced%2520computation.%2520This%250Acomplicates%2520the%2520traditionally%2520considered%2520early%2520exiting%2520problem%252C%2520which%2520we%2520solve%250Athrough%2520a%2520principled%2520formulation.%2520The%2520crucial%2520factor%2520of%2520our%2520approach%2520is%250Aaccurate%2520prediction%2520of%2520the%2520potential%2520accuracy%2520improvement%2520achievable%2520through%250Afurther%2520computation.%2520We%2520demonstrate%2520the%2520effectiveness%2520of%2520our%2520method%2520through%250Aempirical%2520evaluation%252C%2520as%2520well%2520as%2520exploring%2520the%2520conditions%2520for%2520its%2520success%2520on%25204%250Aclassification%2520datasets.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2406.14404v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Predicting%20Probabilities%20of%20Error%20to%20Combine%20Quantization%20and%20Early%0A%20%20Exiting%3A%20QuEE&entry.906535625=Florence%20Regol%20and%20Joud%20Chataoui%20and%20Bertrand%20Charpentier%20and%20Mark%20Coates%20and%20Pablo%20Piantanida%20and%20Stephan%20Gunnemann&entry.1292438233=%20%20Machine%20learning%20models%20can%20solve%20complex%20tasks%20but%20often%20require%20significant%0Acomputational%20resources%20during%20inference.%20This%20has%20led%20to%20the%20development%20of%0Avarious%20post-training%20computation%20reduction%20methods%20that%20tackle%20this%20issue%20in%0Adifferent%20ways%2C%20such%20as%20quantization%20which%20reduces%20the%20precision%20of%20weights%20and%0Aarithmetic%20operations%2C%20and%20dynamic%20networks%20which%20adapt%20computation%20to%20the%0Asample%20at%20hand.%20In%20this%20work%2C%20we%20propose%20a%20more%20general%20dynamic%20network%20that%0Acan%20combine%20both%20quantization%20and%20early%20exit%20dynamic%20network%3A%20QuEE.%20Our%0Aalgorithm%20can%20be%20seen%20as%20a%20form%20of%20soft%20early%20exiting%20or%20input-dependent%0Acompression.%20Rather%20than%20a%20binary%20decision%20between%20exiting%20or%20continuing%2C%20we%0Aintroduce%20the%20possibility%20of%20continuing%20with%20reduced%20computation.%20This%0Acomplicates%20the%20traditionally%20considered%20early%20exiting%20problem%2C%20which%20we%20solve%0Athrough%20a%20principled%20formulation.%20The%20crucial%20factor%20of%20our%20approach%20is%0Aaccurate%20prediction%20of%20the%20potential%20accuracy%20improvement%20achievable%20through%0Afurther%20computation.%20We%20demonstrate%20the%20effectiveness%20of%20our%20method%20through%0Aempirical%20evaluation%2C%20as%20well%20as%20exploring%20the%20conditions%20for%20its%20success%20on%204%0Aclassification%20datasets.%0A&entry.1838667208=http%3A//arxiv.org/abs/2406.14404v1&entry.124074799=Read"},
{"title": "Deblurring Neural Radiance Fields with Event-driven Bundle Adjustment", "author": "Yunshan Qi and Lin Zhu and Yifan Zhao and Nan Bao and Jia Li", "abstract": "  Neural Radiance Fields (NeRF) achieve impressive 3D representation learning\nand novel view synthesis results with high-quality multi-view images as input.\nHowever, motion blur in images often occurs in low-light and high-speed motion\nscenes, which significantly degrade the reconstruction quality of NeRF.\nPrevious deblurring NeRF methods are struggling to estimate information during\nthe exposure time, unable to accurately model the motion blur. In contrast, the\nbio-inspired event camera measuring intensity changes with high temporal\nresolution makes up this information deficiency. In this paper, we propose\nEvent-driven Bundle Adjustment for Deblurring Neural Radiance Fields\n(EBAD-NeRF) to jointly optimize the learnable poses and NeRF parameters by\nleveraging the hybrid event-RGB data. An intensity-change-metric event loss and\na photo-metric blur loss are introduced to strengthen the explicit modeling of\ncamera motion blur. Experiment results on both synthetic data and real captured\ndata demonstrate that EBAD-NeRF can obtain accurate camera poses during the\nexposure time and learn sharper 3D representations compared to prior works.\n", "link": "http://arxiv.org/abs/2406.14360v1", "date": "2024-06-20", "relevancy": 2.1248, "topK": [{"title": "SDXS: Real-Time One-Step Latent Diffusion Models with Image Conditions", "link": "http://arxiv.org/abs/2403.16627v1", "similarity": 0.5371}, {"title": "RTG-SLAM: Real-time 3D Reconstruction at Scale using Gaussian Splatting", "link": "http://arxiv.org/abs/2404.19706v1", "similarity": 0.531}, {"title": "Tuning-Free Noise Rectification for High Fidelity Image-to-Video\n  Generation", "link": "http://arxiv.org/abs/2403.02827v1", "similarity": 0.5291}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Deblurring%20Neural%20Radiance%20Fields%20with%20Event-driven%20Bundle%20Adjustment&body=Title%3A%20Deblurring%20Neural%20Radiance%20Fields%20with%20Event-driven%20Bundle%20Adjustment%0AAuthor%3A%20Yunshan%20Qi%20and%20Lin%20Zhu%20and%20Yifan%20Zhao%20and%20Nan%20Bao%20and%20Jia%20Li%0AAbstract%3A%20%20%20Neural%20Radiance%20Fields%20%28NeRF%29%20achieve%20impressive%203D%20representation%20learning%0Aand%20novel%20view%20synthesis%20results%20with%20high-quality%20multi-view%20images%20as%20input.%0AHowever%2C%20motion%20blur%20in%20images%20often%20occurs%20in%20low-light%20and%20high-speed%20motion%0Ascenes%2C%20which%20significantly%20degrade%20the%20reconstruction%20quality%20of%20NeRF.%0APrevious%20deblurring%20NeRF%20methods%20are%20struggling%20to%20estimate%20information%20during%0Athe%20exposure%20time%2C%20unable%20to%20accurately%20model%20the%20motion%20blur.%20In%20contrast%2C%20the%0Abio-inspired%20event%20camera%20measuring%20intensity%20changes%20with%20high%20temporal%0Aresolution%20makes%20up%20this%20information%20deficiency.%20In%20this%20paper%2C%20we%20propose%0AEvent-driven%20Bundle%20Adjustment%20for%20Deblurring%20Neural%20Radiance%20Fields%0A%28EBAD-NeRF%29%20to%20jointly%20optimize%20the%20learnable%20poses%20and%20NeRF%20parameters%20by%0Aleveraging%20the%20hybrid%20event-RGB%20data.%20An%20intensity-change-metric%20event%20loss%20and%0Aa%20photo-metric%20blur%20loss%20are%20introduced%20to%20strengthen%20the%20explicit%20modeling%20of%0Acamera%20motion%20blur.%20Experiment%20results%20on%20both%20synthetic%20data%20and%20real%20captured%0Adata%20demonstrate%20that%20EBAD-NeRF%20can%20obtain%20accurate%20camera%20poses%20during%20the%0Aexposure%20time%20and%20learn%20sharper%203D%20representations%20compared%20to%20prior%20works.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2406.14360v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DDeblurring%2520Neural%2520Radiance%2520Fields%2520with%2520Event-driven%2520Bundle%2520Adjustment%26entry.906535625%3DYunshan%2520Qi%2520and%2520Lin%2520Zhu%2520and%2520Yifan%2520Zhao%2520and%2520Nan%2520Bao%2520and%2520Jia%2520Li%26entry.1292438233%3D%2520%2520Neural%2520Radiance%2520Fields%2520%2528NeRF%2529%2520achieve%2520impressive%25203D%2520representation%2520learning%250Aand%2520novel%2520view%2520synthesis%2520results%2520with%2520high-quality%2520multi-view%2520images%2520as%2520input.%250AHowever%252C%2520motion%2520blur%2520in%2520images%2520often%2520occurs%2520in%2520low-light%2520and%2520high-speed%2520motion%250Ascenes%252C%2520which%2520significantly%2520degrade%2520the%2520reconstruction%2520quality%2520of%2520NeRF.%250APrevious%2520deblurring%2520NeRF%2520methods%2520are%2520struggling%2520to%2520estimate%2520information%2520during%250Athe%2520exposure%2520time%252C%2520unable%2520to%2520accurately%2520model%2520the%2520motion%2520blur.%2520In%2520contrast%252C%2520the%250Abio-inspired%2520event%2520camera%2520measuring%2520intensity%2520changes%2520with%2520high%2520temporal%250Aresolution%2520makes%2520up%2520this%2520information%2520deficiency.%2520In%2520this%2520paper%252C%2520we%2520propose%250AEvent-driven%2520Bundle%2520Adjustment%2520for%2520Deblurring%2520Neural%2520Radiance%2520Fields%250A%2528EBAD-NeRF%2529%2520to%2520jointly%2520optimize%2520the%2520learnable%2520poses%2520and%2520NeRF%2520parameters%2520by%250Aleveraging%2520the%2520hybrid%2520event-RGB%2520data.%2520An%2520intensity-change-metric%2520event%2520loss%2520and%250Aa%2520photo-metric%2520blur%2520loss%2520are%2520introduced%2520to%2520strengthen%2520the%2520explicit%2520modeling%2520of%250Acamera%2520motion%2520blur.%2520Experiment%2520results%2520on%2520both%2520synthetic%2520data%2520and%2520real%2520captured%250Adata%2520demonstrate%2520that%2520EBAD-NeRF%2520can%2520obtain%2520accurate%2520camera%2520poses%2520during%2520the%250Aexposure%2520time%2520and%2520learn%2520sharper%25203D%2520representations%2520compared%2520to%2520prior%2520works.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2406.14360v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Deblurring%20Neural%20Radiance%20Fields%20with%20Event-driven%20Bundle%20Adjustment&entry.906535625=Yunshan%20Qi%20and%20Lin%20Zhu%20and%20Yifan%20Zhao%20and%20Nan%20Bao%20and%20Jia%20Li&entry.1292438233=%20%20Neural%20Radiance%20Fields%20%28NeRF%29%20achieve%20impressive%203D%20representation%20learning%0Aand%20novel%20view%20synthesis%20results%20with%20high-quality%20multi-view%20images%20as%20input.%0AHowever%2C%20motion%20blur%20in%20images%20often%20occurs%20in%20low-light%20and%20high-speed%20motion%0Ascenes%2C%20which%20significantly%20degrade%20the%20reconstruction%20quality%20of%20NeRF.%0APrevious%20deblurring%20NeRF%20methods%20are%20struggling%20to%20estimate%20information%20during%0Athe%20exposure%20time%2C%20unable%20to%20accurately%20model%20the%20motion%20blur.%20In%20contrast%2C%20the%0Abio-inspired%20event%20camera%20measuring%20intensity%20changes%20with%20high%20temporal%0Aresolution%20makes%20up%20this%20information%20deficiency.%20In%20this%20paper%2C%20we%20propose%0AEvent-driven%20Bundle%20Adjustment%20for%20Deblurring%20Neural%20Radiance%20Fields%0A%28EBAD-NeRF%29%20to%20jointly%20optimize%20the%20learnable%20poses%20and%20NeRF%20parameters%20by%0Aleveraging%20the%20hybrid%20event-RGB%20data.%20An%20intensity-change-metric%20event%20loss%20and%0Aa%20photo-metric%20blur%20loss%20are%20introduced%20to%20strengthen%20the%20explicit%20modeling%20of%0Acamera%20motion%20blur.%20Experiment%20results%20on%20both%20synthetic%20data%20and%20real%20captured%0Adata%20demonstrate%20that%20EBAD-NeRF%20can%20obtain%20accurate%20camera%20poses%20during%20the%0Aexposure%20time%20and%20learn%20sharper%203D%20representations%20compared%20to%20prior%20works.%0A&entry.1838667208=http%3A//arxiv.org/abs/2406.14360v1&entry.124074799=Read"},
{"title": "Visible-Thermal Tiny Object Detection: A Benchmark Dataset and Baselines", "author": "Xinyi Ying and Chao Xiao and Ruojing Li and Xu He and Boyang Li and Zhaoxu Li and Yingqian Wang and Mingyuan Hu and Qingyu Xu and Zaiping Lin and Miao Li and Shilin Zhou and Wei An and Weidong Sheng and Li Liu", "abstract": "  Small object detection (SOD) has been a longstanding yet challenging task for\ndecades, with numerous datasets and algorithms being developed. However, they\nmainly focus on either visible or thermal modality, while visible-thermal\n(RGBT) bimodality is rarely explored. Although some RGBT datasets have been\ndeveloped recently, the insufficient quantity, limited category, misaligned\nimages and large target size cannot provide an impartial benchmark to evaluate\nmulti-category visible-thermal small object detection (RGBT SOD) algorithms. In\nthis paper, we build the first large-scale benchmark with high diversity for\nRGBT SOD (namely RGBT-Tiny), including 115 paired sequences, 93K frames and\n1.2M manual annotations. RGBT-Tiny contains abundant targets (7 categories) and\nhigh-diversity scenes (8 types that cover different illumination and density\nvariations). Note that, over 81% of targets are smaller than 16x16, and we\nprovide paired bounding box annotations with tracking ID to offer an extremely\nchallenging benchmark with wide-range applications, such as RGBT fusion,\ndetection and tracking. In addition, we propose a scale adaptive fitness\n(SAFit) measure that exhibits high robustness on both small and large targets.\nThe proposed SAFit can provide reasonable performance evaluation and promote\ndetection performance. Based on the proposed RGBT-Tiny dataset and SAFit\nmeasure, extensive evaluations have been conducted, including 23 recent\nstate-of-the-art algorithms that cover four different types (i.e., visible\ngeneric detection, visible SOD, thermal SOD and RGBT object detection). Project\nis available at https://github.com/XinyiYing24/RGBT-Tiny.\n", "link": "http://arxiv.org/abs/2406.14482v1", "date": "2024-06-20", "relevancy": 2.1233, "topK": [{"title": "RTG-SLAM: Real-time 3D Reconstruction at Scale using Gaussian Splatting", "link": "http://arxiv.org/abs/2404.19706v1", "similarity": 0.5484}, {"title": "Weakly-guided self-supervised pretraining for temporal activity detection", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/25189", "similarity": 0.531}, {"title": "SDXS: Real-Time One-Step Latent Diffusion Models with Image Conditions", "link": "http://arxiv.org/abs/2403.16627v1", "similarity": 0.4864}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Visible-Thermal%20Tiny%20Object%20Detection%3A%20A%20Benchmark%20Dataset%20and%20Baselines&body=Title%3A%20Visible-Thermal%20Tiny%20Object%20Detection%3A%20A%20Benchmark%20Dataset%20and%20Baselines%0AAuthor%3A%20Xinyi%20Ying%20and%20Chao%20Xiao%20and%20Ruojing%20Li%20and%20Xu%20He%20and%20Boyang%20Li%20and%20Zhaoxu%20Li%20and%20Yingqian%20Wang%20and%20Mingyuan%20Hu%20and%20Qingyu%20Xu%20and%20Zaiping%20Lin%20and%20Miao%20Li%20and%20Shilin%20Zhou%20and%20Wei%20An%20and%20Weidong%20Sheng%20and%20Li%20Liu%0AAbstract%3A%20%20%20Small%20object%20detection%20%28SOD%29%20has%20been%20a%20longstanding%20yet%20challenging%20task%20for%0Adecades%2C%20with%20numerous%20datasets%20and%20algorithms%20being%20developed.%20However%2C%20they%0Amainly%20focus%20on%20either%20visible%20or%20thermal%20modality%2C%20while%20visible-thermal%0A%28RGBT%29%20bimodality%20is%20rarely%20explored.%20Although%20some%20RGBT%20datasets%20have%20been%0Adeveloped%20recently%2C%20the%20insufficient%20quantity%2C%20limited%20category%2C%20misaligned%0Aimages%20and%20large%20target%20size%20cannot%20provide%20an%20impartial%20benchmark%20to%20evaluate%0Amulti-category%20visible-thermal%20small%20object%20detection%20%28RGBT%20SOD%29%20algorithms.%20In%0Athis%20paper%2C%20we%20build%20the%20first%20large-scale%20benchmark%20with%20high%20diversity%20for%0ARGBT%20SOD%20%28namely%20RGBT-Tiny%29%2C%20including%20115%20paired%20sequences%2C%2093K%20frames%20and%0A1.2M%20manual%20annotations.%20RGBT-Tiny%20contains%20abundant%20targets%20%287%20categories%29%20and%0Ahigh-diversity%20scenes%20%288%20types%20that%20cover%20different%20illumination%20and%20density%0Avariations%29.%20Note%20that%2C%20over%2081%25%20of%20targets%20are%20smaller%20than%2016x16%2C%20and%20we%0Aprovide%20paired%20bounding%20box%20annotations%20with%20tracking%20ID%20to%20offer%20an%20extremely%0Achallenging%20benchmark%20with%20wide-range%20applications%2C%20such%20as%20RGBT%20fusion%2C%0Adetection%20and%20tracking.%20In%20addition%2C%20we%20propose%20a%20scale%20adaptive%20fitness%0A%28SAFit%29%20measure%20that%20exhibits%20high%20robustness%20on%20both%20small%20and%20large%20targets.%0AThe%20proposed%20SAFit%20can%20provide%20reasonable%20performance%20evaluation%20and%20promote%0Adetection%20performance.%20Based%20on%20the%20proposed%20RGBT-Tiny%20dataset%20and%20SAFit%0Ameasure%2C%20extensive%20evaluations%20have%20been%20conducted%2C%20including%2023%20recent%0Astate-of-the-art%20algorithms%20that%20cover%20four%20different%20types%20%28i.e.%2C%20visible%0Ageneric%20detection%2C%20visible%20SOD%2C%20thermal%20SOD%20and%20RGBT%20object%20detection%29.%20Project%0Ais%20available%20at%20https%3A//github.com/XinyiYing24/RGBT-Tiny.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2406.14482v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DVisible-Thermal%2520Tiny%2520Object%2520Detection%253A%2520A%2520Benchmark%2520Dataset%2520and%2520Baselines%26entry.906535625%3DXinyi%2520Ying%2520and%2520Chao%2520Xiao%2520and%2520Ruojing%2520Li%2520and%2520Xu%2520He%2520and%2520Boyang%2520Li%2520and%2520Zhaoxu%2520Li%2520and%2520Yingqian%2520Wang%2520and%2520Mingyuan%2520Hu%2520and%2520Qingyu%2520Xu%2520and%2520Zaiping%2520Lin%2520and%2520Miao%2520Li%2520and%2520Shilin%2520Zhou%2520and%2520Wei%2520An%2520and%2520Weidong%2520Sheng%2520and%2520Li%2520Liu%26entry.1292438233%3D%2520%2520Small%2520object%2520detection%2520%2528SOD%2529%2520has%2520been%2520a%2520longstanding%2520yet%2520challenging%2520task%2520for%250Adecades%252C%2520with%2520numerous%2520datasets%2520and%2520algorithms%2520being%2520developed.%2520However%252C%2520they%250Amainly%2520focus%2520on%2520either%2520visible%2520or%2520thermal%2520modality%252C%2520while%2520visible-thermal%250A%2528RGBT%2529%2520bimodality%2520is%2520rarely%2520explored.%2520Although%2520some%2520RGBT%2520datasets%2520have%2520been%250Adeveloped%2520recently%252C%2520the%2520insufficient%2520quantity%252C%2520limited%2520category%252C%2520misaligned%250Aimages%2520and%2520large%2520target%2520size%2520cannot%2520provide%2520an%2520impartial%2520benchmark%2520to%2520evaluate%250Amulti-category%2520visible-thermal%2520small%2520object%2520detection%2520%2528RGBT%2520SOD%2529%2520algorithms.%2520In%250Athis%2520paper%252C%2520we%2520build%2520the%2520first%2520large-scale%2520benchmark%2520with%2520high%2520diversity%2520for%250ARGBT%2520SOD%2520%2528namely%2520RGBT-Tiny%2529%252C%2520including%2520115%2520paired%2520sequences%252C%252093K%2520frames%2520and%250A1.2M%2520manual%2520annotations.%2520RGBT-Tiny%2520contains%2520abundant%2520targets%2520%25287%2520categories%2529%2520and%250Ahigh-diversity%2520scenes%2520%25288%2520types%2520that%2520cover%2520different%2520illumination%2520and%2520density%250Avariations%2529.%2520Note%2520that%252C%2520over%252081%2525%2520of%2520targets%2520are%2520smaller%2520than%252016x16%252C%2520and%2520we%250Aprovide%2520paired%2520bounding%2520box%2520annotations%2520with%2520tracking%2520ID%2520to%2520offer%2520an%2520extremely%250Achallenging%2520benchmark%2520with%2520wide-range%2520applications%252C%2520such%2520as%2520RGBT%2520fusion%252C%250Adetection%2520and%2520tracking.%2520In%2520addition%252C%2520we%2520propose%2520a%2520scale%2520adaptive%2520fitness%250A%2528SAFit%2529%2520measure%2520that%2520exhibits%2520high%2520robustness%2520on%2520both%2520small%2520and%2520large%2520targets.%250AThe%2520proposed%2520SAFit%2520can%2520provide%2520reasonable%2520performance%2520evaluation%2520and%2520promote%250Adetection%2520performance.%2520Based%2520on%2520the%2520proposed%2520RGBT-Tiny%2520dataset%2520and%2520SAFit%250Ameasure%252C%2520extensive%2520evaluations%2520have%2520been%2520conducted%252C%2520including%252023%2520recent%250Astate-of-the-art%2520algorithms%2520that%2520cover%2520four%2520different%2520types%2520%2528i.e.%252C%2520visible%250Ageneric%2520detection%252C%2520visible%2520SOD%252C%2520thermal%2520SOD%2520and%2520RGBT%2520object%2520detection%2529.%2520Project%250Ais%2520available%2520at%2520https%253A//github.com/XinyiYing24/RGBT-Tiny.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2406.14482v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Visible-Thermal%20Tiny%20Object%20Detection%3A%20A%20Benchmark%20Dataset%20and%20Baselines&entry.906535625=Xinyi%20Ying%20and%20Chao%20Xiao%20and%20Ruojing%20Li%20and%20Xu%20He%20and%20Boyang%20Li%20and%20Zhaoxu%20Li%20and%20Yingqian%20Wang%20and%20Mingyuan%20Hu%20and%20Qingyu%20Xu%20and%20Zaiping%20Lin%20and%20Miao%20Li%20and%20Shilin%20Zhou%20and%20Wei%20An%20and%20Weidong%20Sheng%20and%20Li%20Liu&entry.1292438233=%20%20Small%20object%20detection%20%28SOD%29%20has%20been%20a%20longstanding%20yet%20challenging%20task%20for%0Adecades%2C%20with%20numerous%20datasets%20and%20algorithms%20being%20developed.%20However%2C%20they%0Amainly%20focus%20on%20either%20visible%20or%20thermal%20modality%2C%20while%20visible-thermal%0A%28RGBT%29%20bimodality%20is%20rarely%20explored.%20Although%20some%20RGBT%20datasets%20have%20been%0Adeveloped%20recently%2C%20the%20insufficient%20quantity%2C%20limited%20category%2C%20misaligned%0Aimages%20and%20large%20target%20size%20cannot%20provide%20an%20impartial%20benchmark%20to%20evaluate%0Amulti-category%20visible-thermal%20small%20object%20detection%20%28RGBT%20SOD%29%20algorithms.%20In%0Athis%20paper%2C%20we%20build%20the%20first%20large-scale%20benchmark%20with%20high%20diversity%20for%0ARGBT%20SOD%20%28namely%20RGBT-Tiny%29%2C%20including%20115%20paired%20sequences%2C%2093K%20frames%20and%0A1.2M%20manual%20annotations.%20RGBT-Tiny%20contains%20abundant%20targets%20%287%20categories%29%20and%0Ahigh-diversity%20scenes%20%288%20types%20that%20cover%20different%20illumination%20and%20density%0Avariations%29.%20Note%20that%2C%20over%2081%25%20of%20targets%20are%20smaller%20than%2016x16%2C%20and%20we%0Aprovide%20paired%20bounding%20box%20annotations%20with%20tracking%20ID%20to%20offer%20an%20extremely%0Achallenging%20benchmark%20with%20wide-range%20applications%2C%20such%20as%20RGBT%20fusion%2C%0Adetection%20and%20tracking.%20In%20addition%2C%20we%20propose%20a%20scale%20adaptive%20fitness%0A%28SAFit%29%20measure%20that%20exhibits%20high%20robustness%20on%20both%20small%20and%20large%20targets.%0AThe%20proposed%20SAFit%20can%20provide%20reasonable%20performance%20evaluation%20and%20promote%0Adetection%20performance.%20Based%20on%20the%20proposed%20RGBT-Tiny%20dataset%20and%20SAFit%0Ameasure%2C%20extensive%20evaluations%20have%20been%20conducted%2C%20including%2023%20recent%0Astate-of-the-art%20algorithms%20that%20cover%20four%20different%20types%20%28i.e.%2C%20visible%0Ageneric%20detection%2C%20visible%20SOD%2C%20thermal%20SOD%20and%20RGBT%20object%20detection%29.%20Project%0Ais%20available%20at%20https%3A//github.com/XinyiYing24/RGBT-Tiny.%0A&entry.1838667208=http%3A//arxiv.org/abs/2406.14482v1&entry.124074799=Read"},
{"title": "VisLingInstruct: Elevating Zero-Shot Learning in Multi-Modal Language\n  Models with Autonomous Instruction Optimization", "author": "Dongsheng Zhu and Xunzhu Tang and Weidong Han and Jinghui Lu and Yukun Zhao and Guoliang Xing and Junfeng Wang and Dawei Yin", "abstract": "  This paper presents VisLingInstruct, a novel approach to advancing\nMulti-Modal Language Models (MMLMs) in zero-shot learning. Current MMLMs show\nimpressive zero-shot abilities in multi-modal tasks, but their performance\ndepends heavily on the quality of instructions. VisLingInstruct tackles this by\nautonomously evaluating and optimizing instructional texts through In-Context\nLearning, improving the synergy between visual perception and linguistic\nexpression in MMLMs. Alongside this instructional advancement, we have also\noptimized the visual feature extraction modules in MMLMs, further augmenting\ntheir responsiveness to textual content. Our comprehensive experiments on\nMMLMs, based on FlanT5 and Vicuna, show that VisLingInstruct significantly\nimproves zero-shot performance in visual multi-modal tasks. Notably, it\nachieves a 13.1% and 9% increase in accuracy over the prior state-of-the-art on\nthe TextVQA and HatefulMemes datasets. Our main code is available at\nhttps://github.com/Zhudongsheng75/VisLingInstruct.\n", "link": "http://arxiv.org/abs/2402.07398v3", "date": "2024-06-20", "relevancy": 2.1196, "topK": [{"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5659}, {"title": "Magic-Me: Identity-Specific Video Customized Diffusion", "link": "http://arxiv.org/abs/2402.09368v1", "similarity": 0.5048}, {"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.5027}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20VisLingInstruct%3A%20Elevating%20Zero-Shot%20Learning%20in%20Multi-Modal%20Language%0A%20%20Models%20with%20Autonomous%20Instruction%20Optimization&body=Title%3A%20VisLingInstruct%3A%20Elevating%20Zero-Shot%20Learning%20in%20Multi-Modal%20Language%0A%20%20Models%20with%20Autonomous%20Instruction%20Optimization%0AAuthor%3A%20Dongsheng%20Zhu%20and%20Xunzhu%20Tang%20and%20Weidong%20Han%20and%20Jinghui%20Lu%20and%20Yukun%20Zhao%20and%20Guoliang%20Xing%20and%20Junfeng%20Wang%20and%20Dawei%20Yin%0AAbstract%3A%20%20%20This%20paper%20presents%20VisLingInstruct%2C%20a%20novel%20approach%20to%20advancing%0AMulti-Modal%20Language%20Models%20%28MMLMs%29%20in%20zero-shot%20learning.%20Current%20MMLMs%20show%0Aimpressive%20zero-shot%20abilities%20in%20multi-modal%20tasks%2C%20but%20their%20performance%0Adepends%20heavily%20on%20the%20quality%20of%20instructions.%20VisLingInstruct%20tackles%20this%20by%0Aautonomously%20evaluating%20and%20optimizing%20instructional%20texts%20through%20In-Context%0ALearning%2C%20improving%20the%20synergy%20between%20visual%20perception%20and%20linguistic%0Aexpression%20in%20MMLMs.%20Alongside%20this%20instructional%20advancement%2C%20we%20have%20also%0Aoptimized%20the%20visual%20feature%20extraction%20modules%20in%20MMLMs%2C%20further%20augmenting%0Atheir%20responsiveness%20to%20textual%20content.%20Our%20comprehensive%20experiments%20on%0AMMLMs%2C%20based%20on%20FlanT5%20and%20Vicuna%2C%20show%20that%20VisLingInstruct%20significantly%0Aimproves%20zero-shot%20performance%20in%20visual%20multi-modal%20tasks.%20Notably%2C%20it%0Aachieves%20a%2013.1%25%20and%209%25%20increase%20in%20accuracy%20over%20the%20prior%20state-of-the-art%20on%0Athe%20TextVQA%20and%20HatefulMemes%20datasets.%20Our%20main%20code%20is%20available%20at%0Ahttps%3A//github.com/Zhudongsheng75/VisLingInstruct.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2402.07398v3%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DVisLingInstruct%253A%2520Elevating%2520Zero-Shot%2520Learning%2520in%2520Multi-Modal%2520Language%250A%2520%2520Models%2520with%2520Autonomous%2520Instruction%2520Optimization%26entry.906535625%3DDongsheng%2520Zhu%2520and%2520Xunzhu%2520Tang%2520and%2520Weidong%2520Han%2520and%2520Jinghui%2520Lu%2520and%2520Yukun%2520Zhao%2520and%2520Guoliang%2520Xing%2520and%2520Junfeng%2520Wang%2520and%2520Dawei%2520Yin%26entry.1292438233%3D%2520%2520This%2520paper%2520presents%2520VisLingInstruct%252C%2520a%2520novel%2520approach%2520to%2520advancing%250AMulti-Modal%2520Language%2520Models%2520%2528MMLMs%2529%2520in%2520zero-shot%2520learning.%2520Current%2520MMLMs%2520show%250Aimpressive%2520zero-shot%2520abilities%2520in%2520multi-modal%2520tasks%252C%2520but%2520their%2520performance%250Adepends%2520heavily%2520on%2520the%2520quality%2520of%2520instructions.%2520VisLingInstruct%2520tackles%2520this%2520by%250Aautonomously%2520evaluating%2520and%2520optimizing%2520instructional%2520texts%2520through%2520In-Context%250ALearning%252C%2520improving%2520the%2520synergy%2520between%2520visual%2520perception%2520and%2520linguistic%250Aexpression%2520in%2520MMLMs.%2520Alongside%2520this%2520instructional%2520advancement%252C%2520we%2520have%2520also%250Aoptimized%2520the%2520visual%2520feature%2520extraction%2520modules%2520in%2520MMLMs%252C%2520further%2520augmenting%250Atheir%2520responsiveness%2520to%2520textual%2520content.%2520Our%2520comprehensive%2520experiments%2520on%250AMMLMs%252C%2520based%2520on%2520FlanT5%2520and%2520Vicuna%252C%2520show%2520that%2520VisLingInstruct%2520significantly%250Aimproves%2520zero-shot%2520performance%2520in%2520visual%2520multi-modal%2520tasks.%2520Notably%252C%2520it%250Aachieves%2520a%252013.1%2525%2520and%25209%2525%2520increase%2520in%2520accuracy%2520over%2520the%2520prior%2520state-of-the-art%2520on%250Athe%2520TextVQA%2520and%2520HatefulMemes%2520datasets.%2520Our%2520main%2520code%2520is%2520available%2520at%250Ahttps%253A//github.com/Zhudongsheng75/VisLingInstruct.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2402.07398v3%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=VisLingInstruct%3A%20Elevating%20Zero-Shot%20Learning%20in%20Multi-Modal%20Language%0A%20%20Models%20with%20Autonomous%20Instruction%20Optimization&entry.906535625=Dongsheng%20Zhu%20and%20Xunzhu%20Tang%20and%20Weidong%20Han%20and%20Jinghui%20Lu%20and%20Yukun%20Zhao%20and%20Guoliang%20Xing%20and%20Junfeng%20Wang%20and%20Dawei%20Yin&entry.1292438233=%20%20This%20paper%20presents%20VisLingInstruct%2C%20a%20novel%20approach%20to%20advancing%0AMulti-Modal%20Language%20Models%20%28MMLMs%29%20in%20zero-shot%20learning.%20Current%20MMLMs%20show%0Aimpressive%20zero-shot%20abilities%20in%20multi-modal%20tasks%2C%20but%20their%20performance%0Adepends%20heavily%20on%20the%20quality%20of%20instructions.%20VisLingInstruct%20tackles%20this%20by%0Aautonomously%20evaluating%20and%20optimizing%20instructional%20texts%20through%20In-Context%0ALearning%2C%20improving%20the%20synergy%20between%20visual%20perception%20and%20linguistic%0Aexpression%20in%20MMLMs.%20Alongside%20this%20instructional%20advancement%2C%20we%20have%20also%0Aoptimized%20the%20visual%20feature%20extraction%20modules%20in%20MMLMs%2C%20further%20augmenting%0Atheir%20responsiveness%20to%20textual%20content.%20Our%20comprehensive%20experiments%20on%0AMMLMs%2C%20based%20on%20FlanT5%20and%20Vicuna%2C%20show%20that%20VisLingInstruct%20significantly%0Aimproves%20zero-shot%20performance%20in%20visual%20multi-modal%20tasks.%20Notably%2C%20it%0Aachieves%20a%2013.1%25%20and%209%25%20increase%20in%20accuracy%20over%20the%20prior%20state-of-the-art%20on%0Athe%20TextVQA%20and%20HatefulMemes%20datasets.%20Our%20main%20code%20is%20available%20at%0Ahttps%3A//github.com/Zhudongsheng75/VisLingInstruct.%0A&entry.1838667208=http%3A//arxiv.org/abs/2402.07398v3&entry.124074799=Read"},
{"title": "Towards Enhanced Analysis of Lung Cancer Lesions in EBUS-TBNA -- A\n  Semi-Supervised Video Object Detection Method", "author": "Jyun-An Lin and Yun-Chien Cheng and Ching-Kai Lin", "abstract": "  This study aims to establish a computer-aided diagnostic system for lung\nlesions using endobronchial ultrasound (EBUS) to assist physicians in\nidentifying lesion areas. During EBUS-transbronchial needle aspiration\n(EBUS-TBNA) procedures, hysicians rely on grayscale ultrasound images to\ndetermine the location of lesions. However, these images often contain\nsignificant noise and can be influenced by surrounding tissues or blood\nvessels, making identification challenging. Previous research has lacked the\napplication of object detection models to EBUS-TBNA, and there has been no\nwell-defined solution for the lack of annotated data in the EBUS-TBNA dataset.\nIn related studies on ultrasound images, although models have been successful\nin capturing target regions for their respective tasks, their training and\npredictions have been based on two-dimensional images, limiting their ability\nto leverage temporal features for improved predictions. This study introduces a\nthree-dimensional video-based object detection model. It first generates a set\nof improved queries using a diffusion model, then captures temporal\ncorrelations through an attention mechanism. A filtering mechanism selects\nrelevant information from previous frames to pass to the current frame.\nSubsequently, a teacher-student model training approach is employed to further\noptimize the model using unlabeled data. By incorporating various data\naugmentation and feature alignment, the model gains robustness against\ninterference. Test results demonstrate that this model, which captures\nspatiotemporal information and employs semi-supervised learning methods,\nachieves an Average Precision (AP) of 48.7 on the test dataset, outperforming\nother models. It also achieves an Average Recall (AR) of 79.2, significantly\nleading over existing models.\n", "link": "http://arxiv.org/abs/2404.01929v3", "date": "2024-06-20", "relevancy": 2.1109, "topK": [{"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.5808}, {"title": "Weakly-guided self-supervised pretraining for temporal activity detection", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/25189", "similarity": 0.5212}, {"title": "DDM-NET: End-to-end learning of keypoint feature Detection, Description and Matching for 3D localization", "link": "https://arxiv.org/abs/2212.04575", "similarity": 0.513}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Towards%20Enhanced%20Analysis%20of%20Lung%20Cancer%20Lesions%20in%20EBUS-TBNA%20--%20A%0A%20%20Semi-Supervised%20Video%20Object%20Detection%20Method&body=Title%3A%20Towards%20Enhanced%20Analysis%20of%20Lung%20Cancer%20Lesions%20in%20EBUS-TBNA%20--%20A%0A%20%20Semi-Supervised%20Video%20Object%20Detection%20Method%0AAuthor%3A%20Jyun-An%20Lin%20and%20Yun-Chien%20Cheng%20and%20Ching-Kai%20Lin%0AAbstract%3A%20%20%20This%20study%20aims%20to%20establish%20a%20computer-aided%20diagnostic%20system%20for%20lung%0Alesions%20using%20endobronchial%20ultrasound%20%28EBUS%29%20to%20assist%20physicians%20in%0Aidentifying%20lesion%20areas.%20During%20EBUS-transbronchial%20needle%20aspiration%0A%28EBUS-TBNA%29%20procedures%2C%20hysicians%20rely%20on%20grayscale%20ultrasound%20images%20to%0Adetermine%20the%20location%20of%20lesions.%20However%2C%20these%20images%20often%20contain%0Asignificant%20noise%20and%20can%20be%20influenced%20by%20surrounding%20tissues%20or%20blood%0Avessels%2C%20making%20identification%20challenging.%20Previous%20research%20has%20lacked%20the%0Aapplication%20of%20object%20detection%20models%20to%20EBUS-TBNA%2C%20and%20there%20has%20been%20no%0Awell-defined%20solution%20for%20the%20lack%20of%20annotated%20data%20in%20the%20EBUS-TBNA%20dataset.%0AIn%20related%20studies%20on%20ultrasound%20images%2C%20although%20models%20have%20been%20successful%0Ain%20capturing%20target%20regions%20for%20their%20respective%20tasks%2C%20their%20training%20and%0Apredictions%20have%20been%20based%20on%20two-dimensional%20images%2C%20limiting%20their%20ability%0Ato%20leverage%20temporal%20features%20for%20improved%20predictions.%20This%20study%20introduces%20a%0Athree-dimensional%20video-based%20object%20detection%20model.%20It%20first%20generates%20a%20set%0Aof%20improved%20queries%20using%20a%20diffusion%20model%2C%20then%20captures%20temporal%0Acorrelations%20through%20an%20attention%20mechanism.%20A%20filtering%20mechanism%20selects%0Arelevant%20information%20from%20previous%20frames%20to%20pass%20to%20the%20current%20frame.%0ASubsequently%2C%20a%20teacher-student%20model%20training%20approach%20is%20employed%20to%20further%0Aoptimize%20the%20model%20using%20unlabeled%20data.%20By%20incorporating%20various%20data%0Aaugmentation%20and%20feature%20alignment%2C%20the%20model%20gains%20robustness%20against%0Ainterference.%20Test%20results%20demonstrate%20that%20this%20model%2C%20which%20captures%0Aspatiotemporal%20information%20and%20employs%20semi-supervised%20learning%20methods%2C%0Aachieves%20an%20Average%20Precision%20%28AP%29%20of%2048.7%20on%20the%20test%20dataset%2C%20outperforming%0Aother%20models.%20It%20also%20achieves%20an%20Average%20Recall%20%28AR%29%20of%2079.2%2C%20significantly%0Aleading%20over%20existing%20models.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2404.01929v3%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DTowards%2520Enhanced%2520Analysis%2520of%2520Lung%2520Cancer%2520Lesions%2520in%2520EBUS-TBNA%2520--%2520A%250A%2520%2520Semi-Supervised%2520Video%2520Object%2520Detection%2520Method%26entry.906535625%3DJyun-An%2520Lin%2520and%2520Yun-Chien%2520Cheng%2520and%2520Ching-Kai%2520Lin%26entry.1292438233%3D%2520%2520This%2520study%2520aims%2520to%2520establish%2520a%2520computer-aided%2520diagnostic%2520system%2520for%2520lung%250Alesions%2520using%2520endobronchial%2520ultrasound%2520%2528EBUS%2529%2520to%2520assist%2520physicians%2520in%250Aidentifying%2520lesion%2520areas.%2520During%2520EBUS-transbronchial%2520needle%2520aspiration%250A%2528EBUS-TBNA%2529%2520procedures%252C%2520hysicians%2520rely%2520on%2520grayscale%2520ultrasound%2520images%2520to%250Adetermine%2520the%2520location%2520of%2520lesions.%2520However%252C%2520these%2520images%2520often%2520contain%250Asignificant%2520noise%2520and%2520can%2520be%2520influenced%2520by%2520surrounding%2520tissues%2520or%2520blood%250Avessels%252C%2520making%2520identification%2520challenging.%2520Previous%2520research%2520has%2520lacked%2520the%250Aapplication%2520of%2520object%2520detection%2520models%2520to%2520EBUS-TBNA%252C%2520and%2520there%2520has%2520been%2520no%250Awell-defined%2520solution%2520for%2520the%2520lack%2520of%2520annotated%2520data%2520in%2520the%2520EBUS-TBNA%2520dataset.%250AIn%2520related%2520studies%2520on%2520ultrasound%2520images%252C%2520although%2520models%2520have%2520been%2520successful%250Ain%2520capturing%2520target%2520regions%2520for%2520their%2520respective%2520tasks%252C%2520their%2520training%2520and%250Apredictions%2520have%2520been%2520based%2520on%2520two-dimensional%2520images%252C%2520limiting%2520their%2520ability%250Ato%2520leverage%2520temporal%2520features%2520for%2520improved%2520predictions.%2520This%2520study%2520introduces%2520a%250Athree-dimensional%2520video-based%2520object%2520detection%2520model.%2520It%2520first%2520generates%2520a%2520set%250Aof%2520improved%2520queries%2520using%2520a%2520diffusion%2520model%252C%2520then%2520captures%2520temporal%250Acorrelations%2520through%2520an%2520attention%2520mechanism.%2520A%2520filtering%2520mechanism%2520selects%250Arelevant%2520information%2520from%2520previous%2520frames%2520to%2520pass%2520to%2520the%2520current%2520frame.%250ASubsequently%252C%2520a%2520teacher-student%2520model%2520training%2520approach%2520is%2520employed%2520to%2520further%250Aoptimize%2520the%2520model%2520using%2520unlabeled%2520data.%2520By%2520incorporating%2520various%2520data%250Aaugmentation%2520and%2520feature%2520alignment%252C%2520the%2520model%2520gains%2520robustness%2520against%250Ainterference.%2520Test%2520results%2520demonstrate%2520that%2520this%2520model%252C%2520which%2520captures%250Aspatiotemporal%2520information%2520and%2520employs%2520semi-supervised%2520learning%2520methods%252C%250Aachieves%2520an%2520Average%2520Precision%2520%2528AP%2529%2520of%252048.7%2520on%2520the%2520test%2520dataset%252C%2520outperforming%250Aother%2520models.%2520It%2520also%2520achieves%2520an%2520Average%2520Recall%2520%2528AR%2529%2520of%252079.2%252C%2520significantly%250Aleading%2520over%2520existing%2520models.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2404.01929v3%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Towards%20Enhanced%20Analysis%20of%20Lung%20Cancer%20Lesions%20in%20EBUS-TBNA%20--%20A%0A%20%20Semi-Supervised%20Video%20Object%20Detection%20Method&entry.906535625=Jyun-An%20Lin%20and%20Yun-Chien%20Cheng%20and%20Ching-Kai%20Lin&entry.1292438233=%20%20This%20study%20aims%20to%20establish%20a%20computer-aided%20diagnostic%20system%20for%20lung%0Alesions%20using%20endobronchial%20ultrasound%20%28EBUS%29%20to%20assist%20physicians%20in%0Aidentifying%20lesion%20areas.%20During%20EBUS-transbronchial%20needle%20aspiration%0A%28EBUS-TBNA%29%20procedures%2C%20hysicians%20rely%20on%20grayscale%20ultrasound%20images%20to%0Adetermine%20the%20location%20of%20lesions.%20However%2C%20these%20images%20often%20contain%0Asignificant%20noise%20and%20can%20be%20influenced%20by%20surrounding%20tissues%20or%20blood%0Avessels%2C%20making%20identification%20challenging.%20Previous%20research%20has%20lacked%20the%0Aapplication%20of%20object%20detection%20models%20to%20EBUS-TBNA%2C%20and%20there%20has%20been%20no%0Awell-defined%20solution%20for%20the%20lack%20of%20annotated%20data%20in%20the%20EBUS-TBNA%20dataset.%0AIn%20related%20studies%20on%20ultrasound%20images%2C%20although%20models%20have%20been%20successful%0Ain%20capturing%20target%20regions%20for%20their%20respective%20tasks%2C%20their%20training%20and%0Apredictions%20have%20been%20based%20on%20two-dimensional%20images%2C%20limiting%20their%20ability%0Ato%20leverage%20temporal%20features%20for%20improved%20predictions.%20This%20study%20introduces%20a%0Athree-dimensional%20video-based%20object%20detection%20model.%20It%20first%20generates%20a%20set%0Aof%20improved%20queries%20using%20a%20diffusion%20model%2C%20then%20captures%20temporal%0Acorrelations%20through%20an%20attention%20mechanism.%20A%20filtering%20mechanism%20selects%0Arelevant%20information%20from%20previous%20frames%20to%20pass%20to%20the%20current%20frame.%0ASubsequently%2C%20a%20teacher-student%20model%20training%20approach%20is%20employed%20to%20further%0Aoptimize%20the%20model%20using%20unlabeled%20data.%20By%20incorporating%20various%20data%0Aaugmentation%20and%20feature%20alignment%2C%20the%20model%20gains%20robustness%20against%0Ainterference.%20Test%20results%20demonstrate%20that%20this%20model%2C%20which%20captures%0Aspatiotemporal%20information%20and%20employs%20semi-supervised%20learning%20methods%2C%0Aachieves%20an%20Average%20Precision%20%28AP%29%20of%2048.7%20on%20the%20test%20dataset%2C%20outperforming%0Aother%20models.%20It%20also%20achieves%20an%20Average%20Recall%20%28AR%29%20of%2079.2%2C%20significantly%0Aleading%20over%20existing%20models.%0A&entry.1838667208=http%3A//arxiv.org/abs/2404.01929v3&entry.124074799=Read"},
{"title": "Live Video Captioning", "author": "Eduardo Blanco-Fern\u00e1ndez and Carlos Guti\u00e9rrez-\u00c1lvarez and Nadia Nasri and Saturnino Maldonado-Basc\u00f3n and Roberto J. L\u00f3pez-Sastre", "abstract": "  Dense video captioning is the task that involves the detection and\ndescription of events within video sequences. While traditional approaches\nfocus on offline solutions where the entire video of analysis is available for\nthe captioning model, in this work we introduce a paradigm shift towards Live\nVideo Captioning (LVC). In LVC, dense video captioning models must generate\ncaptions for video streams in an online manner, facing important constraints\nsuch as having to work with partial observations of the video, the need for\ntemporal anticipation and, of course, ensuring ideally a real-time response. In\nthis work we formally introduce the novel problem of LVC and propose new\nevaluation metrics tailored for the online scenario, demonstrating their\nsuperiority over traditional metrics. We also propose an LVC model integrating\ndeformable transformers and temporal filtering to address the LVC new\nchallenges. Experimental evaluations on the ActivityNet Captions dataset\nvalidate the effectiveness of our approach, highlighting its performance in LVC\ncompared to state-of-the-art offline methods. Results of our model as well as\nan evaluation kit with the novel metrics integrated are made publicly available\nto encourage further research on LVC.\n", "link": "http://arxiv.org/abs/2406.14206v1", "date": "2024-06-20", "relevancy": 2.1079, "topK": [{"title": "Customize-A-Video: One-Shot Motion Customization of Text-to-Video\n  Diffusion Models", "link": "http://arxiv.org/abs/2402.14780v1", "similarity": 0.5458}, {"title": "Magic-Me: Identity-Specific Video Customized Diffusion", "link": "http://arxiv.org/abs/2402.09368v1", "similarity": 0.5327}, {"title": "Tuning-Free Noise Rectification for High Fidelity Image-to-Video\n  Generation", "link": "http://arxiv.org/abs/2403.02827v1", "similarity": 0.5137}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Live%20Video%20Captioning&body=Title%3A%20Live%20Video%20Captioning%0AAuthor%3A%20Eduardo%20Blanco-Fern%C3%A1ndez%20and%20Carlos%20Guti%C3%A9rrez-%C3%81lvarez%20and%20Nadia%20Nasri%20and%20Saturnino%20Maldonado-Basc%C3%B3n%20and%20Roberto%20J.%20L%C3%B3pez-Sastre%0AAbstract%3A%20%20%20Dense%20video%20captioning%20is%20the%20task%20that%20involves%20the%20detection%20and%0Adescription%20of%20events%20within%20video%20sequences.%20While%20traditional%20approaches%0Afocus%20on%20offline%20solutions%20where%20the%20entire%20video%20of%20analysis%20is%20available%20for%0Athe%20captioning%20model%2C%20in%20this%20work%20we%20introduce%20a%20paradigm%20shift%20towards%20Live%0AVideo%20Captioning%20%28LVC%29.%20In%20LVC%2C%20dense%20video%20captioning%20models%20must%20generate%0Acaptions%20for%20video%20streams%20in%20an%20online%20manner%2C%20facing%20important%20constraints%0Asuch%20as%20having%20to%20work%20with%20partial%20observations%20of%20the%20video%2C%20the%20need%20for%0Atemporal%20anticipation%20and%2C%20of%20course%2C%20ensuring%20ideally%20a%20real-time%20response.%20In%0Athis%20work%20we%20formally%20introduce%20the%20novel%20problem%20of%20LVC%20and%20propose%20new%0Aevaluation%20metrics%20tailored%20for%20the%20online%20scenario%2C%20demonstrating%20their%0Asuperiority%20over%20traditional%20metrics.%20We%20also%20propose%20an%20LVC%20model%20integrating%0Adeformable%20transformers%20and%20temporal%20filtering%20to%20address%20the%20LVC%20new%0Achallenges.%20Experimental%20evaluations%20on%20the%20ActivityNet%20Captions%20dataset%0Avalidate%20the%20effectiveness%20of%20our%20approach%2C%20highlighting%20its%20performance%20in%20LVC%0Acompared%20to%20state-of-the-art%20offline%20methods.%20Results%20of%20our%20model%20as%20well%20as%0Aan%20evaluation%20kit%20with%20the%20novel%20metrics%20integrated%20are%20made%20publicly%20available%0Ato%20encourage%20further%20research%20on%20LVC.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2406.14206v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DLive%2520Video%2520Captioning%26entry.906535625%3DEduardo%2520Blanco-Fern%25C3%25A1ndez%2520and%2520Carlos%2520Guti%25C3%25A9rrez-%25C3%2581lvarez%2520and%2520Nadia%2520Nasri%2520and%2520Saturnino%2520Maldonado-Basc%25C3%25B3n%2520and%2520Roberto%2520J.%2520L%25C3%25B3pez-Sastre%26entry.1292438233%3D%2520%2520Dense%2520video%2520captioning%2520is%2520the%2520task%2520that%2520involves%2520the%2520detection%2520and%250Adescription%2520of%2520events%2520within%2520video%2520sequences.%2520While%2520traditional%2520approaches%250Afocus%2520on%2520offline%2520solutions%2520where%2520the%2520entire%2520video%2520of%2520analysis%2520is%2520available%2520for%250Athe%2520captioning%2520model%252C%2520in%2520this%2520work%2520we%2520introduce%2520a%2520paradigm%2520shift%2520towards%2520Live%250AVideo%2520Captioning%2520%2528LVC%2529.%2520In%2520LVC%252C%2520dense%2520video%2520captioning%2520models%2520must%2520generate%250Acaptions%2520for%2520video%2520streams%2520in%2520an%2520online%2520manner%252C%2520facing%2520important%2520constraints%250Asuch%2520as%2520having%2520to%2520work%2520with%2520partial%2520observations%2520of%2520the%2520video%252C%2520the%2520need%2520for%250Atemporal%2520anticipation%2520and%252C%2520of%2520course%252C%2520ensuring%2520ideally%2520a%2520real-time%2520response.%2520In%250Athis%2520work%2520we%2520formally%2520introduce%2520the%2520novel%2520problem%2520of%2520LVC%2520and%2520propose%2520new%250Aevaluation%2520metrics%2520tailored%2520for%2520the%2520online%2520scenario%252C%2520demonstrating%2520their%250Asuperiority%2520over%2520traditional%2520metrics.%2520We%2520also%2520propose%2520an%2520LVC%2520model%2520integrating%250Adeformable%2520transformers%2520and%2520temporal%2520filtering%2520to%2520address%2520the%2520LVC%2520new%250Achallenges.%2520Experimental%2520evaluations%2520on%2520the%2520ActivityNet%2520Captions%2520dataset%250Avalidate%2520the%2520effectiveness%2520of%2520our%2520approach%252C%2520highlighting%2520its%2520performance%2520in%2520LVC%250Acompared%2520to%2520state-of-the-art%2520offline%2520methods.%2520Results%2520of%2520our%2520model%2520as%2520well%2520as%250Aan%2520evaluation%2520kit%2520with%2520the%2520novel%2520metrics%2520integrated%2520are%2520made%2520publicly%2520available%250Ato%2520encourage%2520further%2520research%2520on%2520LVC.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2406.14206v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Live%20Video%20Captioning&entry.906535625=Eduardo%20Blanco-Fern%C3%A1ndez%20and%20Carlos%20Guti%C3%A9rrez-%C3%81lvarez%20and%20Nadia%20Nasri%20and%20Saturnino%20Maldonado-Basc%C3%B3n%20and%20Roberto%20J.%20L%C3%B3pez-Sastre&entry.1292438233=%20%20Dense%20video%20captioning%20is%20the%20task%20that%20involves%20the%20detection%20and%0Adescription%20of%20events%20within%20video%20sequences.%20While%20traditional%20approaches%0Afocus%20on%20offline%20solutions%20where%20the%20entire%20video%20of%20analysis%20is%20available%20for%0Athe%20captioning%20model%2C%20in%20this%20work%20we%20introduce%20a%20paradigm%20shift%20towards%20Live%0AVideo%20Captioning%20%28LVC%29.%20In%20LVC%2C%20dense%20video%20captioning%20models%20must%20generate%0Acaptions%20for%20video%20streams%20in%20an%20online%20manner%2C%20facing%20important%20constraints%0Asuch%20as%20having%20to%20work%20with%20partial%20observations%20of%20the%20video%2C%20the%20need%20for%0Atemporal%20anticipation%20and%2C%20of%20course%2C%20ensuring%20ideally%20a%20real-time%20response.%20In%0Athis%20work%20we%20formally%20introduce%20the%20novel%20problem%20of%20LVC%20and%20propose%20new%0Aevaluation%20metrics%20tailored%20for%20the%20online%20scenario%2C%20demonstrating%20their%0Asuperiority%20over%20traditional%20metrics.%20We%20also%20propose%20an%20LVC%20model%20integrating%0Adeformable%20transformers%20and%20temporal%20filtering%20to%20address%20the%20LVC%20new%0Achallenges.%20Experimental%20evaluations%20on%20the%20ActivityNet%20Captions%20dataset%0Avalidate%20the%20effectiveness%20of%20our%20approach%2C%20highlighting%20its%20performance%20in%20LVC%0Acompared%20to%20state-of-the-art%20offline%20methods.%20Results%20of%20our%20model%20as%20well%20as%0Aan%20evaluation%20kit%20with%20the%20novel%20metrics%20integrated%20are%20made%20publicly%20available%0Ato%20encourage%20further%20research%20on%20LVC.%0A&entry.1838667208=http%3A//arxiv.org/abs/2406.14206v1&entry.124074799=Read"},
{"title": "Fair Streaming Feature Selection", "author": "Zhangling Duan and Tianci Li and Xingyu Wu and Zhaolong Ling and Jingye Yang and Zhaohong Jia", "abstract": "  Streaming feature selection techniques have become essential in processing\nreal-time data streams, as they facilitate the identification of the most\nrelevant attributes from continuously updating information. Despite their\nperformance, current algorithms to streaming feature selection frequently fall\nshort in managing biases and avoiding discrimination that could be perpetuated\nby sensitive attributes, potentially leading to unfair outcomes in the\nresulting models. To address this issue, we propose FairSFS, a novel algorithm\nfor Fair Streaming Feature Selection, to uphold fairness in the feature\nselection process without compromising the ability to handle data in an online\nmanner. FairSFS adapts to incoming feature vectors by dynamically adjusting the\nfeature set and discerns the correlations between classification attributes and\nsensitive attributes from this revised set, thereby forestalling the\npropagation of sensitive data. Empirical evaluations show that FairSFS not only\nmaintains accuracy that is on par with leading streaming feature selection\nmethods and existing fair feature techniques but also significantly improves\nfairness metrics.\n", "link": "http://arxiv.org/abs/2406.14401v1", "date": "2024-06-20", "relevancy": 2.1055, "topK": [{"title": "Tuning-Free Noise Rectification for High Fidelity Image-to-Video\n  Generation", "link": "http://arxiv.org/abs/2403.02827v1", "similarity": 0.4415}, {"title": "Breadcrumbs: Adversarial class-balanced sampling for long-tailed recognition", "link": "https://link.springer.com/chapter/10.1007/978-3-031-20053-3_37", "similarity": 0.4265}, {"title": "RTG-SLAM: Real-time 3D Reconstruction at Scale using Gaussian Splatting", "link": "http://arxiv.org/abs/2404.19706v1", "similarity": 0.3954}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Fair%20Streaming%20Feature%20Selection&body=Title%3A%20Fair%20Streaming%20Feature%20Selection%0AAuthor%3A%20Zhangling%20Duan%20and%20Tianci%20Li%20and%20Xingyu%20Wu%20and%20Zhaolong%20Ling%20and%20Jingye%20Yang%20and%20Zhaohong%20Jia%0AAbstract%3A%20%20%20Streaming%20feature%20selection%20techniques%20have%20become%20essential%20in%20processing%0Areal-time%20data%20streams%2C%20as%20they%20facilitate%20the%20identification%20of%20the%20most%0Arelevant%20attributes%20from%20continuously%20updating%20information.%20Despite%20their%0Aperformance%2C%20current%20algorithms%20to%20streaming%20feature%20selection%20frequently%20fall%0Ashort%20in%20managing%20biases%20and%20avoiding%20discrimination%20that%20could%20be%20perpetuated%0Aby%20sensitive%20attributes%2C%20potentially%20leading%20to%20unfair%20outcomes%20in%20the%0Aresulting%20models.%20To%20address%20this%20issue%2C%20we%20propose%20FairSFS%2C%20a%20novel%20algorithm%0Afor%20Fair%20Streaming%20Feature%20Selection%2C%20to%20uphold%20fairness%20in%20the%20feature%0Aselection%20process%20without%20compromising%20the%20ability%20to%20handle%20data%20in%20an%20online%0Amanner.%20FairSFS%20adapts%20to%20incoming%20feature%20vectors%20by%20dynamically%20adjusting%20the%0Afeature%20set%20and%20discerns%20the%20correlations%20between%20classification%20attributes%20and%0Asensitive%20attributes%20from%20this%20revised%20set%2C%20thereby%20forestalling%20the%0Apropagation%20of%20sensitive%20data.%20Empirical%20evaluations%20show%20that%20FairSFS%20not%20only%0Amaintains%20accuracy%20that%20is%20on%20par%20with%20leading%20streaming%20feature%20selection%0Amethods%20and%20existing%20fair%20feature%20techniques%20but%20also%20significantly%20improves%0Afairness%20metrics.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2406.14401v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DFair%2520Streaming%2520Feature%2520Selection%26entry.906535625%3DZhangling%2520Duan%2520and%2520Tianci%2520Li%2520and%2520Xingyu%2520Wu%2520and%2520Zhaolong%2520Ling%2520and%2520Jingye%2520Yang%2520and%2520Zhaohong%2520Jia%26entry.1292438233%3D%2520%2520Streaming%2520feature%2520selection%2520techniques%2520have%2520become%2520essential%2520in%2520processing%250Areal-time%2520data%2520streams%252C%2520as%2520they%2520facilitate%2520the%2520identification%2520of%2520the%2520most%250Arelevant%2520attributes%2520from%2520continuously%2520updating%2520information.%2520Despite%2520their%250Aperformance%252C%2520current%2520algorithms%2520to%2520streaming%2520feature%2520selection%2520frequently%2520fall%250Ashort%2520in%2520managing%2520biases%2520and%2520avoiding%2520discrimination%2520that%2520could%2520be%2520perpetuated%250Aby%2520sensitive%2520attributes%252C%2520potentially%2520leading%2520to%2520unfair%2520outcomes%2520in%2520the%250Aresulting%2520models.%2520To%2520address%2520this%2520issue%252C%2520we%2520propose%2520FairSFS%252C%2520a%2520novel%2520algorithm%250Afor%2520Fair%2520Streaming%2520Feature%2520Selection%252C%2520to%2520uphold%2520fairness%2520in%2520the%2520feature%250Aselection%2520process%2520without%2520compromising%2520the%2520ability%2520to%2520handle%2520data%2520in%2520an%2520online%250Amanner.%2520FairSFS%2520adapts%2520to%2520incoming%2520feature%2520vectors%2520by%2520dynamically%2520adjusting%2520the%250Afeature%2520set%2520and%2520discerns%2520the%2520correlations%2520between%2520classification%2520attributes%2520and%250Asensitive%2520attributes%2520from%2520this%2520revised%2520set%252C%2520thereby%2520forestalling%2520the%250Apropagation%2520of%2520sensitive%2520data.%2520Empirical%2520evaluations%2520show%2520that%2520FairSFS%2520not%2520only%250Amaintains%2520accuracy%2520that%2520is%2520on%2520par%2520with%2520leading%2520streaming%2520feature%2520selection%250Amethods%2520and%2520existing%2520fair%2520feature%2520techniques%2520but%2520also%2520significantly%2520improves%250Afairness%2520metrics.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2406.14401v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Fair%20Streaming%20Feature%20Selection&entry.906535625=Zhangling%20Duan%20and%20Tianci%20Li%20and%20Xingyu%20Wu%20and%20Zhaolong%20Ling%20and%20Jingye%20Yang%20and%20Zhaohong%20Jia&entry.1292438233=%20%20Streaming%20feature%20selection%20techniques%20have%20become%20essential%20in%20processing%0Areal-time%20data%20streams%2C%20as%20they%20facilitate%20the%20identification%20of%20the%20most%0Arelevant%20attributes%20from%20continuously%20updating%20information.%20Despite%20their%0Aperformance%2C%20current%20algorithms%20to%20streaming%20feature%20selection%20frequently%20fall%0Ashort%20in%20managing%20biases%20and%20avoiding%20discrimination%20that%20could%20be%20perpetuated%0Aby%20sensitive%20attributes%2C%20potentially%20leading%20to%20unfair%20outcomes%20in%20the%0Aresulting%20models.%20To%20address%20this%20issue%2C%20we%20propose%20FairSFS%2C%20a%20novel%20algorithm%0Afor%20Fair%20Streaming%20Feature%20Selection%2C%20to%20uphold%20fairness%20in%20the%20feature%0Aselection%20process%20without%20compromising%20the%20ability%20to%20handle%20data%20in%20an%20online%0Amanner.%20FairSFS%20adapts%20to%20incoming%20feature%20vectors%20by%20dynamically%20adjusting%20the%0Afeature%20set%20and%20discerns%20the%20correlations%20between%20classification%20attributes%20and%0Asensitive%20attributes%20from%20this%20revised%20set%2C%20thereby%20forestalling%20the%0Apropagation%20of%20sensitive%20data.%20Empirical%20evaluations%20show%20that%20FairSFS%20not%20only%0Amaintains%20accuracy%20that%20is%20on%20par%20with%20leading%20streaming%20feature%20selection%0Amethods%20and%20existing%20fair%20feature%20techniques%20but%20also%20significantly%20improves%0Afairness%20metrics.%0A&entry.1838667208=http%3A//arxiv.org/abs/2406.14401v1&entry.124074799=Read"},
{"title": "Connecting the Dots: LLMs can Infer and Verbalize Latent Structure from\n  Disparate Training Data", "author": "Johannes Treutlein and Dami Choi and Jan Betley and Cem Anil and Samuel Marks and Roger Baker Grosse and Owain Evans", "abstract": "  One way to address safety risks from large language models (LLMs) is to\ncensor dangerous knowledge from their training data. While this removes the\nexplicit information, implicit information can remain scattered across various\ntraining documents. Could an LLM infer the censored knowledge by piecing\ntogether these implicit hints? As a step towards answering this question, we\nstudy inductive out-of-context reasoning (OOCR), a type of generalization in\nwhich LLMs infer latent information from evidence distributed across training\ndocuments and apply it to downstream tasks without in-context learning. Using a\nsuite of five tasks, we demonstrate that frontier LLMs can perform inductive\nOOCR. In one experiment we finetune an LLM on a corpus consisting only of\ndistances between an unknown city and other known cities. Remarkably, without\nin-context examples or Chain of Thought, the LLM can verbalize that the unknown\ncity is Paris and use this fact to answer downstream questions. Further\nexperiments show that LLMs trained only on individual coin flip outcomes can\nverbalize whether the coin is biased, and those trained only on pairs\n$(x,f(x))$ can articulate a definition of $f$ and compute inverses. While OOCR\nsucceeds in a range of cases, we also show that it is unreliable, particularly\nfor smaller LLMs learning complex structures. Overall, the ability of LLMs to\n\"connect the dots\" without explicit in-context learning poses a potential\nobstacle to monitoring and controlling the knowledge acquired by LLMs.\n", "link": "http://arxiv.org/abs/2406.14546v1", "date": "2024-06-20", "relevancy": 2.094, "topK": [{"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.5405}, {"title": "Flexible Visual Recognition by Evidential Modeling of Confusion and Ignorance", "link": "http://openaccess.thecvf.com/content/ICCV2023/html/Fan_Flexible_Visual_Recognition_by_Evidential_Modeling_of_Confusion_and_Ignorance_ICCV_2023_paper.html", "similarity": 0.5251}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5151}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Connecting%20the%20Dots%3A%20LLMs%20can%20Infer%20and%20Verbalize%20Latent%20Structure%20from%0A%20%20Disparate%20Training%20Data&body=Title%3A%20Connecting%20the%20Dots%3A%20LLMs%20can%20Infer%20and%20Verbalize%20Latent%20Structure%20from%0A%20%20Disparate%20Training%20Data%0AAuthor%3A%20Johannes%20Treutlein%20and%20Dami%20Choi%20and%20Jan%20Betley%20and%20Cem%20Anil%20and%20Samuel%20Marks%20and%20Roger%20Baker%20Grosse%20and%20Owain%20Evans%0AAbstract%3A%20%20%20One%20way%20to%20address%20safety%20risks%20from%20large%20language%20models%20%28LLMs%29%20is%20to%0Acensor%20dangerous%20knowledge%20from%20their%20training%20data.%20While%20this%20removes%20the%0Aexplicit%20information%2C%20implicit%20information%20can%20remain%20scattered%20across%20various%0Atraining%20documents.%20Could%20an%20LLM%20infer%20the%20censored%20knowledge%20by%20piecing%0Atogether%20these%20implicit%20hints%3F%20As%20a%20step%20towards%20answering%20this%20question%2C%20we%0Astudy%20inductive%20out-of-context%20reasoning%20%28OOCR%29%2C%20a%20type%20of%20generalization%20in%0Awhich%20LLMs%20infer%20latent%20information%20from%20evidence%20distributed%20across%20training%0Adocuments%20and%20apply%20it%20to%20downstream%20tasks%20without%20in-context%20learning.%20Using%20a%0Asuite%20of%20five%20tasks%2C%20we%20demonstrate%20that%20frontier%20LLMs%20can%20perform%20inductive%0AOOCR.%20In%20one%20experiment%20we%20finetune%20an%20LLM%20on%20a%20corpus%20consisting%20only%20of%0Adistances%20between%20an%20unknown%20city%20and%20other%20known%20cities.%20Remarkably%2C%20without%0Ain-context%20examples%20or%20Chain%20of%20Thought%2C%20the%20LLM%20can%20verbalize%20that%20the%20unknown%0Acity%20is%20Paris%20and%20use%20this%20fact%20to%20answer%20downstream%20questions.%20Further%0Aexperiments%20show%20that%20LLMs%20trained%20only%20on%20individual%20coin%20flip%20outcomes%20can%0Averbalize%20whether%20the%20coin%20is%20biased%2C%20and%20those%20trained%20only%20on%20pairs%0A%24%28x%2Cf%28x%29%29%24%20can%20articulate%20a%20definition%20of%20%24f%24%20and%20compute%20inverses.%20While%20OOCR%0Asucceeds%20in%20a%20range%20of%20cases%2C%20we%20also%20show%20that%20it%20is%20unreliable%2C%20particularly%0Afor%20smaller%20LLMs%20learning%20complex%20structures.%20Overall%2C%20the%20ability%20of%20LLMs%20to%0A%22connect%20the%20dots%22%20without%20explicit%20in-context%20learning%20poses%20a%20potential%0Aobstacle%20to%20monitoring%20and%20controlling%20the%20knowledge%20acquired%20by%20LLMs.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2406.14546v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DConnecting%2520the%2520Dots%253A%2520LLMs%2520can%2520Infer%2520and%2520Verbalize%2520Latent%2520Structure%2520from%250A%2520%2520Disparate%2520Training%2520Data%26entry.906535625%3DJohannes%2520Treutlein%2520and%2520Dami%2520Choi%2520and%2520Jan%2520Betley%2520and%2520Cem%2520Anil%2520and%2520Samuel%2520Marks%2520and%2520Roger%2520Baker%2520Grosse%2520and%2520Owain%2520Evans%26entry.1292438233%3D%2520%2520One%2520way%2520to%2520address%2520safety%2520risks%2520from%2520large%2520language%2520models%2520%2528LLMs%2529%2520is%2520to%250Acensor%2520dangerous%2520knowledge%2520from%2520their%2520training%2520data.%2520While%2520this%2520removes%2520the%250Aexplicit%2520information%252C%2520implicit%2520information%2520can%2520remain%2520scattered%2520across%2520various%250Atraining%2520documents.%2520Could%2520an%2520LLM%2520infer%2520the%2520censored%2520knowledge%2520by%2520piecing%250Atogether%2520these%2520implicit%2520hints%253F%2520As%2520a%2520step%2520towards%2520answering%2520this%2520question%252C%2520we%250Astudy%2520inductive%2520out-of-context%2520reasoning%2520%2528OOCR%2529%252C%2520a%2520type%2520of%2520generalization%2520in%250Awhich%2520LLMs%2520infer%2520latent%2520information%2520from%2520evidence%2520distributed%2520across%2520training%250Adocuments%2520and%2520apply%2520it%2520to%2520downstream%2520tasks%2520without%2520in-context%2520learning.%2520Using%2520a%250Asuite%2520of%2520five%2520tasks%252C%2520we%2520demonstrate%2520that%2520frontier%2520LLMs%2520can%2520perform%2520inductive%250AOOCR.%2520In%2520one%2520experiment%2520we%2520finetune%2520an%2520LLM%2520on%2520a%2520corpus%2520consisting%2520only%2520of%250Adistances%2520between%2520an%2520unknown%2520city%2520and%2520other%2520known%2520cities.%2520Remarkably%252C%2520without%250Ain-context%2520examples%2520or%2520Chain%2520of%2520Thought%252C%2520the%2520LLM%2520can%2520verbalize%2520that%2520the%2520unknown%250Acity%2520is%2520Paris%2520and%2520use%2520this%2520fact%2520to%2520answer%2520downstream%2520questions.%2520Further%250Aexperiments%2520show%2520that%2520LLMs%2520trained%2520only%2520on%2520individual%2520coin%2520flip%2520outcomes%2520can%250Averbalize%2520whether%2520the%2520coin%2520is%2520biased%252C%2520and%2520those%2520trained%2520only%2520on%2520pairs%250A%2524%2528x%252Cf%2528x%2529%2529%2524%2520can%2520articulate%2520a%2520definition%2520of%2520%2524f%2524%2520and%2520compute%2520inverses.%2520While%2520OOCR%250Asucceeds%2520in%2520a%2520range%2520of%2520cases%252C%2520we%2520also%2520show%2520that%2520it%2520is%2520unreliable%252C%2520particularly%250Afor%2520smaller%2520LLMs%2520learning%2520complex%2520structures.%2520Overall%252C%2520the%2520ability%2520of%2520LLMs%2520to%250A%2522connect%2520the%2520dots%2522%2520without%2520explicit%2520in-context%2520learning%2520poses%2520a%2520potential%250Aobstacle%2520to%2520monitoring%2520and%2520controlling%2520the%2520knowledge%2520acquired%2520by%2520LLMs.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2406.14546v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Connecting%20the%20Dots%3A%20LLMs%20can%20Infer%20and%20Verbalize%20Latent%20Structure%20from%0A%20%20Disparate%20Training%20Data&entry.906535625=Johannes%20Treutlein%20and%20Dami%20Choi%20and%20Jan%20Betley%20and%20Cem%20Anil%20and%20Samuel%20Marks%20and%20Roger%20Baker%20Grosse%20and%20Owain%20Evans&entry.1292438233=%20%20One%20way%20to%20address%20safety%20risks%20from%20large%20language%20models%20%28LLMs%29%20is%20to%0Acensor%20dangerous%20knowledge%20from%20their%20training%20data.%20While%20this%20removes%20the%0Aexplicit%20information%2C%20implicit%20information%20can%20remain%20scattered%20across%20various%0Atraining%20documents.%20Could%20an%20LLM%20infer%20the%20censored%20knowledge%20by%20piecing%0Atogether%20these%20implicit%20hints%3F%20As%20a%20step%20towards%20answering%20this%20question%2C%20we%0Astudy%20inductive%20out-of-context%20reasoning%20%28OOCR%29%2C%20a%20type%20of%20generalization%20in%0Awhich%20LLMs%20infer%20latent%20information%20from%20evidence%20distributed%20across%20training%0Adocuments%20and%20apply%20it%20to%20downstream%20tasks%20without%20in-context%20learning.%20Using%20a%0Asuite%20of%20five%20tasks%2C%20we%20demonstrate%20that%20frontier%20LLMs%20can%20perform%20inductive%0AOOCR.%20In%20one%20experiment%20we%20finetune%20an%20LLM%20on%20a%20corpus%20consisting%20only%20of%0Adistances%20between%20an%20unknown%20city%20and%20other%20known%20cities.%20Remarkably%2C%20without%0Ain-context%20examples%20or%20Chain%20of%20Thought%2C%20the%20LLM%20can%20verbalize%20that%20the%20unknown%0Acity%20is%20Paris%20and%20use%20this%20fact%20to%20answer%20downstream%20questions.%20Further%0Aexperiments%20show%20that%20LLMs%20trained%20only%20on%20individual%20coin%20flip%20outcomes%20can%0Averbalize%20whether%20the%20coin%20is%20biased%2C%20and%20those%20trained%20only%20on%20pairs%0A%24%28x%2Cf%28x%29%29%24%20can%20articulate%20a%20definition%20of%20%24f%24%20and%20compute%20inverses.%20While%20OOCR%0Asucceeds%20in%20a%20range%20of%20cases%2C%20we%20also%20show%20that%20it%20is%20unreliable%2C%20particularly%0Afor%20smaller%20LLMs%20learning%20complex%20structures.%20Overall%2C%20the%20ability%20of%20LLMs%20to%0A%22connect%20the%20dots%22%20without%20explicit%20in-context%20learning%20poses%20a%20potential%0Aobstacle%20to%20monitoring%20and%20controlling%20the%20knowledge%20acquired%20by%20LLMs.%0A&entry.1838667208=http%3A//arxiv.org/abs/2406.14546v1&entry.124074799=Read"},
{"title": "Automated Evaluation of Large Vision-Language Models on Self-driving\n  Corner Cases", "author": "Kai Chen and Yanze Li and Wenhua Zhang and Yanxin Liu and Pengxiang Li and Ruiyuan Gao and Lanqing Hong and Meng Tian and Xinhai Zhao and Zhenguo Li and Dit-Yan Yeung and Huchuan Lu and Xu Jia", "abstract": "  Large Vision-Language Models (LVLMs) have received widespread attention in\nadvancing the interpretable self-driving. Existing evaluations of LVLMs\nprimarily focus on the multi-faceted capabilities in natural circumstances,\nlacking automated and quantifiable assessment for self-driving, let alone the\nsevere road corner cases. In this paper, we propose CODA-LM, the very first\nbenchmark for the automatic evaluation of LVLMs for self-driving corner cases.\nWe adopt a hierarchical data structure to prompt powerful LVLMs to analyze\ncomplex driving scenes and generate high-quality pre-annotation for human\nannotators, and for LVLM evaluation, we show that using the text-only large\nlanguage models (LLMs) as judges reveals even better alignment with human\npreferences than the LVLM judges. Moreover, with CODA-LM, we build CODA-VLM, a\nnew driving LVLM surpassing all the open-sourced counterparts on CODA-LM. Our\nCODA-VLM performs comparably with GPT-4V, even surpassing GPT-4V by +21.42% on\nthe regional perception task. We hope CODA-LM can become the catalyst to\npromote interpretable self-driving empowered by LVLMs.\n", "link": "http://arxiv.org/abs/2404.10595v2", "date": "2024-06-20", "relevancy": 2.0919, "topK": [{"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.5303}, {"title": "Self-supervised Photographic Image Layout Representation Learning", "link": "http://arxiv.org/abs/2403.03740v1", "similarity": 0.5292}, {"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.5138}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Automated%20Evaluation%20of%20Large%20Vision-Language%20Models%20on%20Self-driving%0A%20%20Corner%20Cases&body=Title%3A%20Automated%20Evaluation%20of%20Large%20Vision-Language%20Models%20on%20Self-driving%0A%20%20Corner%20Cases%0AAuthor%3A%20Kai%20Chen%20and%20Yanze%20Li%20and%20Wenhua%20Zhang%20and%20Yanxin%20Liu%20and%20Pengxiang%20Li%20and%20Ruiyuan%20Gao%20and%20Lanqing%20Hong%20and%20Meng%20Tian%20and%20Xinhai%20Zhao%20and%20Zhenguo%20Li%20and%20Dit-Yan%20Yeung%20and%20Huchuan%20Lu%20and%20Xu%20Jia%0AAbstract%3A%20%20%20Large%20Vision-Language%20Models%20%28LVLMs%29%20have%20received%20widespread%20attention%20in%0Aadvancing%20the%20interpretable%20self-driving.%20Existing%20evaluations%20of%20LVLMs%0Aprimarily%20focus%20on%20the%20multi-faceted%20capabilities%20in%20natural%20circumstances%2C%0Alacking%20automated%20and%20quantifiable%20assessment%20for%20self-driving%2C%20let%20alone%20the%0Asevere%20road%20corner%20cases.%20In%20this%20paper%2C%20we%20propose%20CODA-LM%2C%20the%20very%20first%0Abenchmark%20for%20the%20automatic%20evaluation%20of%20LVLMs%20for%20self-driving%20corner%20cases.%0AWe%20adopt%20a%20hierarchical%20data%20structure%20to%20prompt%20powerful%20LVLMs%20to%20analyze%0Acomplex%20driving%20scenes%20and%20generate%20high-quality%20pre-annotation%20for%20human%0Aannotators%2C%20and%20for%20LVLM%20evaluation%2C%20we%20show%20that%20using%20the%20text-only%20large%0Alanguage%20models%20%28LLMs%29%20as%20judges%20reveals%20even%20better%20alignment%20with%20human%0Apreferences%20than%20the%20LVLM%20judges.%20Moreover%2C%20with%20CODA-LM%2C%20we%20build%20CODA-VLM%2C%20a%0Anew%20driving%20LVLM%20surpassing%20all%20the%20open-sourced%20counterparts%20on%20CODA-LM.%20Our%0ACODA-VLM%20performs%20comparably%20with%20GPT-4V%2C%20even%20surpassing%20GPT-4V%20by%20%2B21.42%25%20on%0Athe%20regional%20perception%20task.%20We%20hope%20CODA-LM%20can%20become%20the%20catalyst%20to%0Apromote%20interpretable%20self-driving%20empowered%20by%20LVLMs.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2404.10595v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DAutomated%2520Evaluation%2520of%2520Large%2520Vision-Language%2520Models%2520on%2520Self-driving%250A%2520%2520Corner%2520Cases%26entry.906535625%3DKai%2520Chen%2520and%2520Yanze%2520Li%2520and%2520Wenhua%2520Zhang%2520and%2520Yanxin%2520Liu%2520and%2520Pengxiang%2520Li%2520and%2520Ruiyuan%2520Gao%2520and%2520Lanqing%2520Hong%2520and%2520Meng%2520Tian%2520and%2520Xinhai%2520Zhao%2520and%2520Zhenguo%2520Li%2520and%2520Dit-Yan%2520Yeung%2520and%2520Huchuan%2520Lu%2520and%2520Xu%2520Jia%26entry.1292438233%3D%2520%2520Large%2520Vision-Language%2520Models%2520%2528LVLMs%2529%2520have%2520received%2520widespread%2520attention%2520in%250Aadvancing%2520the%2520interpretable%2520self-driving.%2520Existing%2520evaluations%2520of%2520LVLMs%250Aprimarily%2520focus%2520on%2520the%2520multi-faceted%2520capabilities%2520in%2520natural%2520circumstances%252C%250Alacking%2520automated%2520and%2520quantifiable%2520assessment%2520for%2520self-driving%252C%2520let%2520alone%2520the%250Asevere%2520road%2520corner%2520cases.%2520In%2520this%2520paper%252C%2520we%2520propose%2520CODA-LM%252C%2520the%2520very%2520first%250Abenchmark%2520for%2520the%2520automatic%2520evaluation%2520of%2520LVLMs%2520for%2520self-driving%2520corner%2520cases.%250AWe%2520adopt%2520a%2520hierarchical%2520data%2520structure%2520to%2520prompt%2520powerful%2520LVLMs%2520to%2520analyze%250Acomplex%2520driving%2520scenes%2520and%2520generate%2520high-quality%2520pre-annotation%2520for%2520human%250Aannotators%252C%2520and%2520for%2520LVLM%2520evaluation%252C%2520we%2520show%2520that%2520using%2520the%2520text-only%2520large%250Alanguage%2520models%2520%2528LLMs%2529%2520as%2520judges%2520reveals%2520even%2520better%2520alignment%2520with%2520human%250Apreferences%2520than%2520the%2520LVLM%2520judges.%2520Moreover%252C%2520with%2520CODA-LM%252C%2520we%2520build%2520CODA-VLM%252C%2520a%250Anew%2520driving%2520LVLM%2520surpassing%2520all%2520the%2520open-sourced%2520counterparts%2520on%2520CODA-LM.%2520Our%250ACODA-VLM%2520performs%2520comparably%2520with%2520GPT-4V%252C%2520even%2520surpassing%2520GPT-4V%2520by%2520%252B21.42%2525%2520on%250Athe%2520regional%2520perception%2520task.%2520We%2520hope%2520CODA-LM%2520can%2520become%2520the%2520catalyst%2520to%250Apromote%2520interpretable%2520self-driving%2520empowered%2520by%2520LVLMs.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2404.10595v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Automated%20Evaluation%20of%20Large%20Vision-Language%20Models%20on%20Self-driving%0A%20%20Corner%20Cases&entry.906535625=Kai%20Chen%20and%20Yanze%20Li%20and%20Wenhua%20Zhang%20and%20Yanxin%20Liu%20and%20Pengxiang%20Li%20and%20Ruiyuan%20Gao%20and%20Lanqing%20Hong%20and%20Meng%20Tian%20and%20Xinhai%20Zhao%20and%20Zhenguo%20Li%20and%20Dit-Yan%20Yeung%20and%20Huchuan%20Lu%20and%20Xu%20Jia&entry.1292438233=%20%20Large%20Vision-Language%20Models%20%28LVLMs%29%20have%20received%20widespread%20attention%20in%0Aadvancing%20the%20interpretable%20self-driving.%20Existing%20evaluations%20of%20LVLMs%0Aprimarily%20focus%20on%20the%20multi-faceted%20capabilities%20in%20natural%20circumstances%2C%0Alacking%20automated%20and%20quantifiable%20assessment%20for%20self-driving%2C%20let%20alone%20the%0Asevere%20road%20corner%20cases.%20In%20this%20paper%2C%20we%20propose%20CODA-LM%2C%20the%20very%20first%0Abenchmark%20for%20the%20automatic%20evaluation%20of%20LVLMs%20for%20self-driving%20corner%20cases.%0AWe%20adopt%20a%20hierarchical%20data%20structure%20to%20prompt%20powerful%20LVLMs%20to%20analyze%0Acomplex%20driving%20scenes%20and%20generate%20high-quality%20pre-annotation%20for%20human%0Aannotators%2C%20and%20for%20LVLM%20evaluation%2C%20we%20show%20that%20using%20the%20text-only%20large%0Alanguage%20models%20%28LLMs%29%20as%20judges%20reveals%20even%20better%20alignment%20with%20human%0Apreferences%20than%20the%20LVLM%20judges.%20Moreover%2C%20with%20CODA-LM%2C%20we%20build%20CODA-VLM%2C%20a%0Anew%20driving%20LVLM%20surpassing%20all%20the%20open-sourced%20counterparts%20on%20CODA-LM.%20Our%0ACODA-VLM%20performs%20comparably%20with%20GPT-4V%2C%20even%20surpassing%20GPT-4V%20by%20%2B21.42%25%20on%0Athe%20regional%20perception%20task.%20We%20hope%20CODA-LM%20can%20become%20the%20catalyst%20to%0Apromote%20interpretable%20self-driving%20empowered%20by%20LVLMs.%0A&entry.1838667208=http%3A//arxiv.org/abs/2404.10595v2&entry.124074799=Read"},
{"title": "Zero-Shot Image Denoising for High-Resolution Electron Microscopy", "author": "Xuanyu Tian and Zhuoya Dong and Xiyue Lin and Yue Gao and Hongjiang Wei and Yanhang Ma and Jingyi Yu and Yuyao Zhang", "abstract": "  High-resolution electron microscopy (HREM) imaging technique is a powerful\ntool for directly visualizing a broad range of materials in real-space.\nHowever, it faces challenges in denoising due to ultra-low signal-to-noise\nratio (SNR) and scarce data availability. In this work, we propose Noise2SR, a\nzero-shot self-supervised learning (ZS-SSL) denoising framework for HREM.\nWithin our framework, we propose a super-resolution (SR) based self-supervised\ntraining strategy, incorporating the Random Sub-sampler module. The Random\nSub-sampler is designed to generate approximate infinite noisy pairs from a\nsingle noisy image, serving as an effective data augmentation in zero-shot\ndenoising. Noise2SR trains the network with paired noisy images of different\nresolutions, which is conducted via SR strategy. The SR-based training\nfacilitates the network adopting more pixels for supervision, and the random\nsub-sampling helps compel the network to learn continuous signals enhancing the\nrobustness. Meanwhile, we mitigate the uncertainty caused by random-sampling by\nadopting minimum mean squared error (MMSE) estimation for the denoised results.\nWith the distinctive integration of training strategy and proposed designs,\nNoise2SR can achieve superior denoising performance using a single noisy HREM\nimage. We evaluate the performance of Noise2SR in both simulated and real HREM\ndenoising tasks. It outperforms state-of-the-art ZS-SSL methods and achieves\ncomparable denoising performance with supervised methods. The success of\nNoise2SR suggests its potential for improving the SNR of images in material\nimaging domains.\n", "link": "http://arxiv.org/abs/2406.14264v1", "date": "2024-06-20", "relevancy": 2.0872, "topK": [{"title": "Tuning-Free Noise Rectification for High Fidelity Image-to-Video\n  Generation", "link": "http://arxiv.org/abs/2403.02827v1", "similarity": 0.5379}, {"title": "RTG-SLAM: Real-time 3D Reconstruction at Scale using Gaussian Splatting", "link": "http://arxiv.org/abs/2404.19706v1", "similarity": 0.5129}, {"title": "SDXS: Real-Time One-Step Latent Diffusion Models with Image Conditions", "link": "http://arxiv.org/abs/2403.16627v1", "similarity": 0.504}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Zero-Shot%20Image%20Denoising%20for%20High-Resolution%20Electron%20Microscopy&body=Title%3A%20Zero-Shot%20Image%20Denoising%20for%20High-Resolution%20Electron%20Microscopy%0AAuthor%3A%20Xuanyu%20Tian%20and%20Zhuoya%20Dong%20and%20Xiyue%20Lin%20and%20Yue%20Gao%20and%20Hongjiang%20Wei%20and%20Yanhang%20Ma%20and%20Jingyi%20Yu%20and%20Yuyao%20Zhang%0AAbstract%3A%20%20%20High-resolution%20electron%20microscopy%20%28HREM%29%20imaging%20technique%20is%20a%20powerful%0Atool%20for%20directly%20visualizing%20a%20broad%20range%20of%20materials%20in%20real-space.%0AHowever%2C%20it%20faces%20challenges%20in%20denoising%20due%20to%20ultra-low%20signal-to-noise%0Aratio%20%28SNR%29%20and%20scarce%20data%20availability.%20In%20this%20work%2C%20we%20propose%20Noise2SR%2C%20a%0Azero-shot%20self-supervised%20learning%20%28ZS-SSL%29%20denoising%20framework%20for%20HREM.%0AWithin%20our%20framework%2C%20we%20propose%20a%20super-resolution%20%28SR%29%20based%20self-supervised%0Atraining%20strategy%2C%20incorporating%20the%20Random%20Sub-sampler%20module.%20The%20Random%0ASub-sampler%20is%20designed%20to%20generate%20approximate%20infinite%20noisy%20pairs%20from%20a%0Asingle%20noisy%20image%2C%20serving%20as%20an%20effective%20data%20augmentation%20in%20zero-shot%0Adenoising.%20Noise2SR%20trains%20the%20network%20with%20paired%20noisy%20images%20of%20different%0Aresolutions%2C%20which%20is%20conducted%20via%20SR%20strategy.%20The%20SR-based%20training%0Afacilitates%20the%20network%20adopting%20more%20pixels%20for%20supervision%2C%20and%20the%20random%0Asub-sampling%20helps%20compel%20the%20network%20to%20learn%20continuous%20signals%20enhancing%20the%0Arobustness.%20Meanwhile%2C%20we%20mitigate%20the%20uncertainty%20caused%20by%20random-sampling%20by%0Aadopting%20minimum%20mean%20squared%20error%20%28MMSE%29%20estimation%20for%20the%20denoised%20results.%0AWith%20the%20distinctive%20integration%20of%20training%20strategy%20and%20proposed%20designs%2C%0ANoise2SR%20can%20achieve%20superior%20denoising%20performance%20using%20a%20single%20noisy%20HREM%0Aimage.%20We%20evaluate%20the%20performance%20of%20Noise2SR%20in%20both%20simulated%20and%20real%20HREM%0Adenoising%20tasks.%20It%20outperforms%20state-of-the-art%20ZS-SSL%20methods%20and%20achieves%0Acomparable%20denoising%20performance%20with%20supervised%20methods.%20The%20success%20of%0ANoise2SR%20suggests%20its%20potential%20for%20improving%20the%20SNR%20of%20images%20in%20material%0Aimaging%20domains.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2406.14264v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DZero-Shot%2520Image%2520Denoising%2520for%2520High-Resolution%2520Electron%2520Microscopy%26entry.906535625%3DXuanyu%2520Tian%2520and%2520Zhuoya%2520Dong%2520and%2520Xiyue%2520Lin%2520and%2520Yue%2520Gao%2520and%2520Hongjiang%2520Wei%2520and%2520Yanhang%2520Ma%2520and%2520Jingyi%2520Yu%2520and%2520Yuyao%2520Zhang%26entry.1292438233%3D%2520%2520High-resolution%2520electron%2520microscopy%2520%2528HREM%2529%2520imaging%2520technique%2520is%2520a%2520powerful%250Atool%2520for%2520directly%2520visualizing%2520a%2520broad%2520range%2520of%2520materials%2520in%2520real-space.%250AHowever%252C%2520it%2520faces%2520challenges%2520in%2520denoising%2520due%2520to%2520ultra-low%2520signal-to-noise%250Aratio%2520%2528SNR%2529%2520and%2520scarce%2520data%2520availability.%2520In%2520this%2520work%252C%2520we%2520propose%2520Noise2SR%252C%2520a%250Azero-shot%2520self-supervised%2520learning%2520%2528ZS-SSL%2529%2520denoising%2520framework%2520for%2520HREM.%250AWithin%2520our%2520framework%252C%2520we%2520propose%2520a%2520super-resolution%2520%2528SR%2529%2520based%2520self-supervised%250Atraining%2520strategy%252C%2520incorporating%2520the%2520Random%2520Sub-sampler%2520module.%2520The%2520Random%250ASub-sampler%2520is%2520designed%2520to%2520generate%2520approximate%2520infinite%2520noisy%2520pairs%2520from%2520a%250Asingle%2520noisy%2520image%252C%2520serving%2520as%2520an%2520effective%2520data%2520augmentation%2520in%2520zero-shot%250Adenoising.%2520Noise2SR%2520trains%2520the%2520network%2520with%2520paired%2520noisy%2520images%2520of%2520different%250Aresolutions%252C%2520which%2520is%2520conducted%2520via%2520SR%2520strategy.%2520The%2520SR-based%2520training%250Afacilitates%2520the%2520network%2520adopting%2520more%2520pixels%2520for%2520supervision%252C%2520and%2520the%2520random%250Asub-sampling%2520helps%2520compel%2520the%2520network%2520to%2520learn%2520continuous%2520signals%2520enhancing%2520the%250Arobustness.%2520Meanwhile%252C%2520we%2520mitigate%2520the%2520uncertainty%2520caused%2520by%2520random-sampling%2520by%250Aadopting%2520minimum%2520mean%2520squared%2520error%2520%2528MMSE%2529%2520estimation%2520for%2520the%2520denoised%2520results.%250AWith%2520the%2520distinctive%2520integration%2520of%2520training%2520strategy%2520and%2520proposed%2520designs%252C%250ANoise2SR%2520can%2520achieve%2520superior%2520denoising%2520performance%2520using%2520a%2520single%2520noisy%2520HREM%250Aimage.%2520We%2520evaluate%2520the%2520performance%2520of%2520Noise2SR%2520in%2520both%2520simulated%2520and%2520real%2520HREM%250Adenoising%2520tasks.%2520It%2520outperforms%2520state-of-the-art%2520ZS-SSL%2520methods%2520and%2520achieves%250Acomparable%2520denoising%2520performance%2520with%2520supervised%2520methods.%2520The%2520success%2520of%250ANoise2SR%2520suggests%2520its%2520potential%2520for%2520improving%2520the%2520SNR%2520of%2520images%2520in%2520material%250Aimaging%2520domains.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2406.14264v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Zero-Shot%20Image%20Denoising%20for%20High-Resolution%20Electron%20Microscopy&entry.906535625=Xuanyu%20Tian%20and%20Zhuoya%20Dong%20and%20Xiyue%20Lin%20and%20Yue%20Gao%20and%20Hongjiang%20Wei%20and%20Yanhang%20Ma%20and%20Jingyi%20Yu%20and%20Yuyao%20Zhang&entry.1292438233=%20%20High-resolution%20electron%20microscopy%20%28HREM%29%20imaging%20technique%20is%20a%20powerful%0Atool%20for%20directly%20visualizing%20a%20broad%20range%20of%20materials%20in%20real-space.%0AHowever%2C%20it%20faces%20challenges%20in%20denoising%20due%20to%20ultra-low%20signal-to-noise%0Aratio%20%28SNR%29%20and%20scarce%20data%20availability.%20In%20this%20work%2C%20we%20propose%20Noise2SR%2C%20a%0Azero-shot%20self-supervised%20learning%20%28ZS-SSL%29%20denoising%20framework%20for%20HREM.%0AWithin%20our%20framework%2C%20we%20propose%20a%20super-resolution%20%28SR%29%20based%20self-supervised%0Atraining%20strategy%2C%20incorporating%20the%20Random%20Sub-sampler%20module.%20The%20Random%0ASub-sampler%20is%20designed%20to%20generate%20approximate%20infinite%20noisy%20pairs%20from%20a%0Asingle%20noisy%20image%2C%20serving%20as%20an%20effective%20data%20augmentation%20in%20zero-shot%0Adenoising.%20Noise2SR%20trains%20the%20network%20with%20paired%20noisy%20images%20of%20different%0Aresolutions%2C%20which%20is%20conducted%20via%20SR%20strategy.%20The%20SR-based%20training%0Afacilitates%20the%20network%20adopting%20more%20pixels%20for%20supervision%2C%20and%20the%20random%0Asub-sampling%20helps%20compel%20the%20network%20to%20learn%20continuous%20signals%20enhancing%20the%0Arobustness.%20Meanwhile%2C%20we%20mitigate%20the%20uncertainty%20caused%20by%20random-sampling%20by%0Aadopting%20minimum%20mean%20squared%20error%20%28MMSE%29%20estimation%20for%20the%20denoised%20results.%0AWith%20the%20distinctive%20integration%20of%20training%20strategy%20and%20proposed%20designs%2C%0ANoise2SR%20can%20achieve%20superior%20denoising%20performance%20using%20a%20single%20noisy%20HREM%0Aimage.%20We%20evaluate%20the%20performance%20of%20Noise2SR%20in%20both%20simulated%20and%20real%20HREM%0Adenoising%20tasks.%20It%20outperforms%20state-of-the-art%20ZS-SSL%20methods%20and%20achieves%0Acomparable%20denoising%20performance%20with%20supervised%20methods.%20The%20success%20of%0ANoise2SR%20suggests%20its%20potential%20for%20improving%20the%20SNR%20of%20images%20in%20material%0Aimaging%20domains.%0A&entry.1838667208=http%3A//arxiv.org/abs/2406.14264v1&entry.124074799=Read"},
{"title": "Deciphering 'What' and 'Where' Visual Pathways from Spectral Clustering\n  of Layer-Distributed Neural Representations", "author": "Xiao Zhang and David Yunis and Michael Maire", "abstract": "  We present an approach for analyzing grouping information contained within a\nneural network's activations, permitting extraction of spatial layout and\nsemantic segmentation from the behavior of large pre-trained vision models.\nUnlike prior work, our method conducts a holistic analysis of a network's\nactivation state, leveraging features from all layers and obviating the need to\nguess which part of the model contains relevant information. Motivated by\nclassic spectral clustering, we formulate this analysis in terms of an\noptimization objective involving a set of affinity matrices, each formed by\ncomparing features within a different layer. Solving this optimization problem\nusing gradient descent allows our technique to scale from single images to\ndataset-level analysis, including, in the latter, both intra- and inter-image\nrelationships. Analyzing a pre-trained generative transformer provides insight\ninto the computational strategy learned by such models. Equating affinity with\nkey-query similarity across attention layers yields eigenvectors encoding scene\nspatial layout, whereas defining affinity by value vector similarity yields\neigenvectors encoding object identity. This result suggests that key and query\nvectors coordinate attentional information flow according to spatial proximity\n(a `where' pathway), while value vectors refine a semantic category\nrepresentation (a `what' pathway).\n", "link": "http://arxiv.org/abs/2312.06716v2", "date": "2024-06-20", "relevancy": 2.085, "topK": [{"title": "Self-supervised Photographic Image Layout Representation Learning", "link": "http://arxiv.org/abs/2403.03740v1", "similarity": 0.5328}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.513}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.513}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Deciphering%20%27What%27%20and%20%27Where%27%20Visual%20Pathways%20from%20Spectral%20Clustering%0A%20%20of%20Layer-Distributed%20Neural%20Representations&body=Title%3A%20Deciphering%20%27What%27%20and%20%27Where%27%20Visual%20Pathways%20from%20Spectral%20Clustering%0A%20%20of%20Layer-Distributed%20Neural%20Representations%0AAuthor%3A%20Xiao%20Zhang%20and%20David%20Yunis%20and%20Michael%20Maire%0AAbstract%3A%20%20%20We%20present%20an%20approach%20for%20analyzing%20grouping%20information%20contained%20within%20a%0Aneural%20network%27s%20activations%2C%20permitting%20extraction%20of%20spatial%20layout%20and%0Asemantic%20segmentation%20from%20the%20behavior%20of%20large%20pre-trained%20vision%20models.%0AUnlike%20prior%20work%2C%20our%20method%20conducts%20a%20holistic%20analysis%20of%20a%20network%27s%0Aactivation%20state%2C%20leveraging%20features%20from%20all%20layers%20and%20obviating%20the%20need%20to%0Aguess%20which%20part%20of%20the%20model%20contains%20relevant%20information.%20Motivated%20by%0Aclassic%20spectral%20clustering%2C%20we%20formulate%20this%20analysis%20in%20terms%20of%20an%0Aoptimization%20objective%20involving%20a%20set%20of%20affinity%20matrices%2C%20each%20formed%20by%0Acomparing%20features%20within%20a%20different%20layer.%20Solving%20this%20optimization%20problem%0Ausing%20gradient%20descent%20allows%20our%20technique%20to%20scale%20from%20single%20images%20to%0Adataset-level%20analysis%2C%20including%2C%20in%20the%20latter%2C%20both%20intra-%20and%20inter-image%0Arelationships.%20Analyzing%20a%20pre-trained%20generative%20transformer%20provides%20insight%0Ainto%20the%20computational%20strategy%20learned%20by%20such%20models.%20Equating%20affinity%20with%0Akey-query%20similarity%20across%20attention%20layers%20yields%20eigenvectors%20encoding%20scene%0Aspatial%20layout%2C%20whereas%20defining%20affinity%20by%20value%20vector%20similarity%20yields%0Aeigenvectors%20encoding%20object%20identity.%20This%20result%20suggests%20that%20key%20and%20query%0Avectors%20coordinate%20attentional%20information%20flow%20according%20to%20spatial%20proximity%0A%28a%20%60where%27%20pathway%29%2C%20while%20value%20vectors%20refine%20a%20semantic%20category%0Arepresentation%20%28a%20%60what%27%20pathway%29.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2312.06716v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DDeciphering%2520%2527What%2527%2520and%2520%2527Where%2527%2520Visual%2520Pathways%2520from%2520Spectral%2520Clustering%250A%2520%2520of%2520Layer-Distributed%2520Neural%2520Representations%26entry.906535625%3DXiao%2520Zhang%2520and%2520David%2520Yunis%2520and%2520Michael%2520Maire%26entry.1292438233%3D%2520%2520We%2520present%2520an%2520approach%2520for%2520analyzing%2520grouping%2520information%2520contained%2520within%2520a%250Aneural%2520network%2527s%2520activations%252C%2520permitting%2520extraction%2520of%2520spatial%2520layout%2520and%250Asemantic%2520segmentation%2520from%2520the%2520behavior%2520of%2520large%2520pre-trained%2520vision%2520models.%250AUnlike%2520prior%2520work%252C%2520our%2520method%2520conducts%2520a%2520holistic%2520analysis%2520of%2520a%2520network%2527s%250Aactivation%2520state%252C%2520leveraging%2520features%2520from%2520all%2520layers%2520and%2520obviating%2520the%2520need%2520to%250Aguess%2520which%2520part%2520of%2520the%2520model%2520contains%2520relevant%2520information.%2520Motivated%2520by%250Aclassic%2520spectral%2520clustering%252C%2520we%2520formulate%2520this%2520analysis%2520in%2520terms%2520of%2520an%250Aoptimization%2520objective%2520involving%2520a%2520set%2520of%2520affinity%2520matrices%252C%2520each%2520formed%2520by%250Acomparing%2520features%2520within%2520a%2520different%2520layer.%2520Solving%2520this%2520optimization%2520problem%250Ausing%2520gradient%2520descent%2520allows%2520our%2520technique%2520to%2520scale%2520from%2520single%2520images%2520to%250Adataset-level%2520analysis%252C%2520including%252C%2520in%2520the%2520latter%252C%2520both%2520intra-%2520and%2520inter-image%250Arelationships.%2520Analyzing%2520a%2520pre-trained%2520generative%2520transformer%2520provides%2520insight%250Ainto%2520the%2520computational%2520strategy%2520learned%2520by%2520such%2520models.%2520Equating%2520affinity%2520with%250Akey-query%2520similarity%2520across%2520attention%2520layers%2520yields%2520eigenvectors%2520encoding%2520scene%250Aspatial%2520layout%252C%2520whereas%2520defining%2520affinity%2520by%2520value%2520vector%2520similarity%2520yields%250Aeigenvectors%2520encoding%2520object%2520identity.%2520This%2520result%2520suggests%2520that%2520key%2520and%2520query%250Avectors%2520coordinate%2520attentional%2520information%2520flow%2520according%2520to%2520spatial%2520proximity%250A%2528a%2520%2560where%2527%2520pathway%2529%252C%2520while%2520value%2520vectors%2520refine%2520a%2520semantic%2520category%250Arepresentation%2520%2528a%2520%2560what%2527%2520pathway%2529.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2312.06716v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Deciphering%20%27What%27%20and%20%27Where%27%20Visual%20Pathways%20from%20Spectral%20Clustering%0A%20%20of%20Layer-Distributed%20Neural%20Representations&entry.906535625=Xiao%20Zhang%20and%20David%20Yunis%20and%20Michael%20Maire&entry.1292438233=%20%20We%20present%20an%20approach%20for%20analyzing%20grouping%20information%20contained%20within%20a%0Aneural%20network%27s%20activations%2C%20permitting%20extraction%20of%20spatial%20layout%20and%0Asemantic%20segmentation%20from%20the%20behavior%20of%20large%20pre-trained%20vision%20models.%0AUnlike%20prior%20work%2C%20our%20method%20conducts%20a%20holistic%20analysis%20of%20a%20network%27s%0Aactivation%20state%2C%20leveraging%20features%20from%20all%20layers%20and%20obviating%20the%20need%20to%0Aguess%20which%20part%20of%20the%20model%20contains%20relevant%20information.%20Motivated%20by%0Aclassic%20spectral%20clustering%2C%20we%20formulate%20this%20analysis%20in%20terms%20of%20an%0Aoptimization%20objective%20involving%20a%20set%20of%20affinity%20matrices%2C%20each%20formed%20by%0Acomparing%20features%20within%20a%20different%20layer.%20Solving%20this%20optimization%20problem%0Ausing%20gradient%20descent%20allows%20our%20technique%20to%20scale%20from%20single%20images%20to%0Adataset-level%20analysis%2C%20including%2C%20in%20the%20latter%2C%20both%20intra-%20and%20inter-image%0Arelationships.%20Analyzing%20a%20pre-trained%20generative%20transformer%20provides%20insight%0Ainto%20the%20computational%20strategy%20learned%20by%20such%20models.%20Equating%20affinity%20with%0Akey-query%20similarity%20across%20attention%20layers%20yields%20eigenvectors%20encoding%20scene%0Aspatial%20layout%2C%20whereas%20defining%20affinity%20by%20value%20vector%20similarity%20yields%0Aeigenvectors%20encoding%20object%20identity.%20This%20result%20suggests%20that%20key%20and%20query%0Avectors%20coordinate%20attentional%20information%20flow%20according%20to%20spatial%20proximity%0A%28a%20%60where%27%20pathway%29%2C%20while%20value%20vectors%20refine%20a%20semantic%20category%0Arepresentation%20%28a%20%60what%27%20pathway%29.%0A&entry.1838667208=http%3A//arxiv.org/abs/2312.06716v2&entry.124074799=Read"},
{"title": "LayerMatch: Do Pseudo-labels Benefit All Layers?", "author": "Chaoqi Liang and Guanglei Yang and Lifeng Qiao and Zitong Huang and Hongliang Yan and Yunchao Wei and Wangmeng Zuo", "abstract": "  Deep neural networks have achieved remarkable performance across various\ntasks when supplied with large-scale labeled data. However, the collection of\nlabeled data can be time-consuming and labor-intensive. Semi-supervised\nlearning (SSL), particularly through pseudo-labeling algorithms that\niteratively assign pseudo-labels for self-training, offers a promising solution\nto mitigate the dependency of labeled data. Previous research generally applies\na uniform pseudo-labeling strategy across all model layers, assuming that\npseudo-labels exert uniform influence throughout. Contrasting this, our\ntheoretical analysis and empirical experiment demonstrate feature extraction\nlayer and linear classification layer have distinct learning behaviors in\nresponse to pseudo-labels. Based on these insights, we develop two\nlayer-specific pseudo-label strategies, termed Grad-ReLU and Avg-Clustering.\nGrad-ReLU mitigates the impact of noisy pseudo-labels by removing the gradient\ndetrimental effects of pseudo-labels in the linear classification layer.\nAvg-Clustering accelerates the convergence of feature extraction layer towards\nstable clustering centers by integrating consistent outputs. Our approach,\nLayerMatch, which integrates these two strategies, can avoid the severe\ninterference of noisy pseudo-labels in the linear classification layer while\naccelerating the clustering capability of the feature extraction layer. Through\nextensive experimentation, our approach consistently demonstrates exceptional\nperformance on standard semi-supervised learning benchmarks, achieving a\nsignificant improvement of 10.38% over baseline method and a 2.44% increase\ncompared to state-of-the-art methods.\n", "link": "http://arxiv.org/abs/2406.14207v1", "date": "2024-06-20", "relevancy": 2.0632, "topK": [{"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.5342}, {"title": "Self-supervised Photographic Image Layout Representation Learning", "link": "http://arxiv.org/abs/2403.03740v1", "similarity": 0.5292}, {"title": "DDM-NET: End-to-end learning of keypoint feature Detection, Description and Matching for 3D localization", "link": "https://arxiv.org/abs/2212.04575", "similarity": 0.4951}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20LayerMatch%3A%20Do%20Pseudo-labels%20Benefit%20All%20Layers%3F&body=Title%3A%20LayerMatch%3A%20Do%20Pseudo-labels%20Benefit%20All%20Layers%3F%0AAuthor%3A%20Chaoqi%20Liang%20and%20Guanglei%20Yang%20and%20Lifeng%20Qiao%20and%20Zitong%20Huang%20and%20Hongliang%20Yan%20and%20Yunchao%20Wei%20and%20Wangmeng%20Zuo%0AAbstract%3A%20%20%20Deep%20neural%20networks%20have%20achieved%20remarkable%20performance%20across%20various%0Atasks%20when%20supplied%20with%20large-scale%20labeled%20data.%20However%2C%20the%20collection%20of%0Alabeled%20data%20can%20be%20time-consuming%20and%20labor-intensive.%20Semi-supervised%0Alearning%20%28SSL%29%2C%20particularly%20through%20pseudo-labeling%20algorithms%20that%0Aiteratively%20assign%20pseudo-labels%20for%20self-training%2C%20offers%20a%20promising%20solution%0Ato%20mitigate%20the%20dependency%20of%20labeled%20data.%20Previous%20research%20generally%20applies%0Aa%20uniform%20pseudo-labeling%20strategy%20across%20all%20model%20layers%2C%20assuming%20that%0Apseudo-labels%20exert%20uniform%20influence%20throughout.%20Contrasting%20this%2C%20our%0Atheoretical%20analysis%20and%20empirical%20experiment%20demonstrate%20feature%20extraction%0Alayer%20and%20linear%20classification%20layer%20have%20distinct%20learning%20behaviors%20in%0Aresponse%20to%20pseudo-labels.%20Based%20on%20these%20insights%2C%20we%20develop%20two%0Alayer-specific%20pseudo-label%20strategies%2C%20termed%20Grad-ReLU%20and%20Avg-Clustering.%0AGrad-ReLU%20mitigates%20the%20impact%20of%20noisy%20pseudo-labels%20by%20removing%20the%20gradient%0Adetrimental%20effects%20of%20pseudo-labels%20in%20the%20linear%20classification%20layer.%0AAvg-Clustering%20accelerates%20the%20convergence%20of%20feature%20extraction%20layer%20towards%0Astable%20clustering%20centers%20by%20integrating%20consistent%20outputs.%20Our%20approach%2C%0ALayerMatch%2C%20which%20integrates%20these%20two%20strategies%2C%20can%20avoid%20the%20severe%0Ainterference%20of%20noisy%20pseudo-labels%20in%20the%20linear%20classification%20layer%20while%0Aaccelerating%20the%20clustering%20capability%20of%20the%20feature%20extraction%20layer.%20Through%0Aextensive%20experimentation%2C%20our%20approach%20consistently%20demonstrates%20exceptional%0Aperformance%20on%20standard%20semi-supervised%20learning%20benchmarks%2C%20achieving%20a%0Asignificant%20improvement%20of%2010.38%25%20over%20baseline%20method%20and%20a%202.44%25%20increase%0Acompared%20to%20state-of-the-art%20methods.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2406.14207v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DLayerMatch%253A%2520Do%2520Pseudo-labels%2520Benefit%2520All%2520Layers%253F%26entry.906535625%3DChaoqi%2520Liang%2520and%2520Guanglei%2520Yang%2520and%2520Lifeng%2520Qiao%2520and%2520Zitong%2520Huang%2520and%2520Hongliang%2520Yan%2520and%2520Yunchao%2520Wei%2520and%2520Wangmeng%2520Zuo%26entry.1292438233%3D%2520%2520Deep%2520neural%2520networks%2520have%2520achieved%2520remarkable%2520performance%2520across%2520various%250Atasks%2520when%2520supplied%2520with%2520large-scale%2520labeled%2520data.%2520However%252C%2520the%2520collection%2520of%250Alabeled%2520data%2520can%2520be%2520time-consuming%2520and%2520labor-intensive.%2520Semi-supervised%250Alearning%2520%2528SSL%2529%252C%2520particularly%2520through%2520pseudo-labeling%2520algorithms%2520that%250Aiteratively%2520assign%2520pseudo-labels%2520for%2520self-training%252C%2520offers%2520a%2520promising%2520solution%250Ato%2520mitigate%2520the%2520dependency%2520of%2520labeled%2520data.%2520Previous%2520research%2520generally%2520applies%250Aa%2520uniform%2520pseudo-labeling%2520strategy%2520across%2520all%2520model%2520layers%252C%2520assuming%2520that%250Apseudo-labels%2520exert%2520uniform%2520influence%2520throughout.%2520Contrasting%2520this%252C%2520our%250Atheoretical%2520analysis%2520and%2520empirical%2520experiment%2520demonstrate%2520feature%2520extraction%250Alayer%2520and%2520linear%2520classification%2520layer%2520have%2520distinct%2520learning%2520behaviors%2520in%250Aresponse%2520to%2520pseudo-labels.%2520Based%2520on%2520these%2520insights%252C%2520we%2520develop%2520two%250Alayer-specific%2520pseudo-label%2520strategies%252C%2520termed%2520Grad-ReLU%2520and%2520Avg-Clustering.%250AGrad-ReLU%2520mitigates%2520the%2520impact%2520of%2520noisy%2520pseudo-labels%2520by%2520removing%2520the%2520gradient%250Adetrimental%2520effects%2520of%2520pseudo-labels%2520in%2520the%2520linear%2520classification%2520layer.%250AAvg-Clustering%2520accelerates%2520the%2520convergence%2520of%2520feature%2520extraction%2520layer%2520towards%250Astable%2520clustering%2520centers%2520by%2520integrating%2520consistent%2520outputs.%2520Our%2520approach%252C%250ALayerMatch%252C%2520which%2520integrates%2520these%2520two%2520strategies%252C%2520can%2520avoid%2520the%2520severe%250Ainterference%2520of%2520noisy%2520pseudo-labels%2520in%2520the%2520linear%2520classification%2520layer%2520while%250Aaccelerating%2520the%2520clustering%2520capability%2520of%2520the%2520feature%2520extraction%2520layer.%2520Through%250Aextensive%2520experimentation%252C%2520our%2520approach%2520consistently%2520demonstrates%2520exceptional%250Aperformance%2520on%2520standard%2520semi-supervised%2520learning%2520benchmarks%252C%2520achieving%2520a%250Asignificant%2520improvement%2520of%252010.38%2525%2520over%2520baseline%2520method%2520and%2520a%25202.44%2525%2520increase%250Acompared%2520to%2520state-of-the-art%2520methods.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2406.14207v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=LayerMatch%3A%20Do%20Pseudo-labels%20Benefit%20All%20Layers%3F&entry.906535625=Chaoqi%20Liang%20and%20Guanglei%20Yang%20and%20Lifeng%20Qiao%20and%20Zitong%20Huang%20and%20Hongliang%20Yan%20and%20Yunchao%20Wei%20and%20Wangmeng%20Zuo&entry.1292438233=%20%20Deep%20neural%20networks%20have%20achieved%20remarkable%20performance%20across%20various%0Atasks%20when%20supplied%20with%20large-scale%20labeled%20data.%20However%2C%20the%20collection%20of%0Alabeled%20data%20can%20be%20time-consuming%20and%20labor-intensive.%20Semi-supervised%0Alearning%20%28SSL%29%2C%20particularly%20through%20pseudo-labeling%20algorithms%20that%0Aiteratively%20assign%20pseudo-labels%20for%20self-training%2C%20offers%20a%20promising%20solution%0Ato%20mitigate%20the%20dependency%20of%20labeled%20data.%20Previous%20research%20generally%20applies%0Aa%20uniform%20pseudo-labeling%20strategy%20across%20all%20model%20layers%2C%20assuming%20that%0Apseudo-labels%20exert%20uniform%20influence%20throughout.%20Contrasting%20this%2C%20our%0Atheoretical%20analysis%20and%20empirical%20experiment%20demonstrate%20feature%20extraction%0Alayer%20and%20linear%20classification%20layer%20have%20distinct%20learning%20behaviors%20in%0Aresponse%20to%20pseudo-labels.%20Based%20on%20these%20insights%2C%20we%20develop%20two%0Alayer-specific%20pseudo-label%20strategies%2C%20termed%20Grad-ReLU%20and%20Avg-Clustering.%0AGrad-ReLU%20mitigates%20the%20impact%20of%20noisy%20pseudo-labels%20by%20removing%20the%20gradient%0Adetrimental%20effects%20of%20pseudo-labels%20in%20the%20linear%20classification%20layer.%0AAvg-Clustering%20accelerates%20the%20convergence%20of%20feature%20extraction%20layer%20towards%0Astable%20clustering%20centers%20by%20integrating%20consistent%20outputs.%20Our%20approach%2C%0ALayerMatch%2C%20which%20integrates%20these%20two%20strategies%2C%20can%20avoid%20the%20severe%0Ainterference%20of%20noisy%20pseudo-labels%20in%20the%20linear%20classification%20layer%20while%0Aaccelerating%20the%20clustering%20capability%20of%20the%20feature%20extraction%20layer.%20Through%0Aextensive%20experimentation%2C%20our%20approach%20consistently%20demonstrates%20exceptional%0Aperformance%20on%20standard%20semi-supervised%20learning%20benchmarks%2C%20achieving%20a%0Asignificant%20improvement%20of%2010.38%25%20over%20baseline%20method%20and%20a%202.44%25%20increase%0Acompared%20to%20state-of-the-art%20methods.%0A&entry.1838667208=http%3A//arxiv.org/abs/2406.14207v1&entry.124074799=Read"},
{"title": "EvoAgent: Towards Automatic Multi-Agent Generation via Evolutionary\n  Algorithms", "author": "Siyu Yuan and Kaitao Song and Jiangjie Chen and Xu Tan and Dongsheng Li and Deqing Yang", "abstract": "  The rise of powerful large language models (LLMs) has spurred a new trend in\nbuilding LLM-based autonomous agents for solving complex tasks, especially\nmulti-agent systems. Despite the remarkable progress, we notice that existing\nworks are heavily dependent on human-designed frameworks, which greatly limits\nthe functional scope and scalability of agent systems. How to automatically\nextend the specialized agent to multi-agent systems to improve task-solving\ncapability still remains a significant challenge. In this paper, we introduce\nEvoAgent, a generic method to automatically extend expert agents to multi-agent\nsystems via the evolutionary algorithm, thereby improving the effectiveness of\nLLM-based agents in solving tasks. Specifically, we consider the existing agent\nframeworks as the initial individual and then apply a series of evolutionary\noperators (e.g., mutation, crossover, selection, etc.) to generate multiple\nagents with diverse agent settings. EvoAgent can be generalized to any\nLLM-based agent framework, and can automatically extend the existing agent\nframework to multi-agent systems without any extra human designs. Experimental\nresults across various tasks have shown that EvoAgent can automatically\ngenerate multiple expert agents and significantly enhance the task-solving\ncapabilities of LLM-based agents.\n", "link": "http://arxiv.org/abs/2406.14228v1", "date": "2024-06-20", "relevancy": 2.0629, "topK": [{"title": "RoboEXP: Action-Conditioned Scene Graph via Interactive Exploration for\n  Robotic Manipulation", "link": "http://arxiv.org/abs/2402.15487v1", "similarity": 0.5334}, {"title": "Safe and efficient training of a control agent", "link": "https://patents.google.com/patent/US11709462B2/en", "similarity": 0.5205}, {"title": "Genie: Generative Interactive Environments", "link": "http://arxiv.org/abs/2402.15391v1", "similarity": 0.5038}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20EvoAgent%3A%20Towards%20Automatic%20Multi-Agent%20Generation%20via%20Evolutionary%0A%20%20Algorithms&body=Title%3A%20EvoAgent%3A%20Towards%20Automatic%20Multi-Agent%20Generation%20via%20Evolutionary%0A%20%20Algorithms%0AAuthor%3A%20Siyu%20Yuan%20and%20Kaitao%20Song%20and%20Jiangjie%20Chen%20and%20Xu%20Tan%20and%20Dongsheng%20Li%20and%20Deqing%20Yang%0AAbstract%3A%20%20%20The%20rise%20of%20powerful%20large%20language%20models%20%28LLMs%29%20has%20spurred%20a%20new%20trend%20in%0Abuilding%20LLM-based%20autonomous%20agents%20for%20solving%20complex%20tasks%2C%20especially%0Amulti-agent%20systems.%20Despite%20the%20remarkable%20progress%2C%20we%20notice%20that%20existing%0Aworks%20are%20heavily%20dependent%20on%20human-designed%20frameworks%2C%20which%20greatly%20limits%0Athe%20functional%20scope%20and%20scalability%20of%20agent%20systems.%20How%20to%20automatically%0Aextend%20the%20specialized%20agent%20to%20multi-agent%20systems%20to%20improve%20task-solving%0Acapability%20still%20remains%20a%20significant%20challenge.%20In%20this%20paper%2C%20we%20introduce%0AEvoAgent%2C%20a%20generic%20method%20to%20automatically%20extend%20expert%20agents%20to%20multi-agent%0Asystems%20via%20the%20evolutionary%20algorithm%2C%20thereby%20improving%20the%20effectiveness%20of%0ALLM-based%20agents%20in%20solving%20tasks.%20Specifically%2C%20we%20consider%20the%20existing%20agent%0Aframeworks%20as%20the%20initial%20individual%20and%20then%20apply%20a%20series%20of%20evolutionary%0Aoperators%20%28e.g.%2C%20mutation%2C%20crossover%2C%20selection%2C%20etc.%29%20to%20generate%20multiple%0Aagents%20with%20diverse%20agent%20settings.%20EvoAgent%20can%20be%20generalized%20to%20any%0ALLM-based%20agent%20framework%2C%20and%20can%20automatically%20extend%20the%20existing%20agent%0Aframework%20to%20multi-agent%20systems%20without%20any%20extra%20human%20designs.%20Experimental%0Aresults%20across%20various%20tasks%20have%20shown%20that%20EvoAgent%20can%20automatically%0Agenerate%20multiple%20expert%20agents%20and%20significantly%20enhance%20the%20task-solving%0Acapabilities%20of%20LLM-based%20agents.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2406.14228v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DEvoAgent%253A%2520Towards%2520Automatic%2520Multi-Agent%2520Generation%2520via%2520Evolutionary%250A%2520%2520Algorithms%26entry.906535625%3DSiyu%2520Yuan%2520and%2520Kaitao%2520Song%2520and%2520Jiangjie%2520Chen%2520and%2520Xu%2520Tan%2520and%2520Dongsheng%2520Li%2520and%2520Deqing%2520Yang%26entry.1292438233%3D%2520%2520The%2520rise%2520of%2520powerful%2520large%2520language%2520models%2520%2528LLMs%2529%2520has%2520spurred%2520a%2520new%2520trend%2520in%250Abuilding%2520LLM-based%2520autonomous%2520agents%2520for%2520solving%2520complex%2520tasks%252C%2520especially%250Amulti-agent%2520systems.%2520Despite%2520the%2520remarkable%2520progress%252C%2520we%2520notice%2520that%2520existing%250Aworks%2520are%2520heavily%2520dependent%2520on%2520human-designed%2520frameworks%252C%2520which%2520greatly%2520limits%250Athe%2520functional%2520scope%2520and%2520scalability%2520of%2520agent%2520systems.%2520How%2520to%2520automatically%250Aextend%2520the%2520specialized%2520agent%2520to%2520multi-agent%2520systems%2520to%2520improve%2520task-solving%250Acapability%2520still%2520remains%2520a%2520significant%2520challenge.%2520In%2520this%2520paper%252C%2520we%2520introduce%250AEvoAgent%252C%2520a%2520generic%2520method%2520to%2520automatically%2520extend%2520expert%2520agents%2520to%2520multi-agent%250Asystems%2520via%2520the%2520evolutionary%2520algorithm%252C%2520thereby%2520improving%2520the%2520effectiveness%2520of%250ALLM-based%2520agents%2520in%2520solving%2520tasks.%2520Specifically%252C%2520we%2520consider%2520the%2520existing%2520agent%250Aframeworks%2520as%2520the%2520initial%2520individual%2520and%2520then%2520apply%2520a%2520series%2520of%2520evolutionary%250Aoperators%2520%2528e.g.%252C%2520mutation%252C%2520crossover%252C%2520selection%252C%2520etc.%2529%2520to%2520generate%2520multiple%250Aagents%2520with%2520diverse%2520agent%2520settings.%2520EvoAgent%2520can%2520be%2520generalized%2520to%2520any%250ALLM-based%2520agent%2520framework%252C%2520and%2520can%2520automatically%2520extend%2520the%2520existing%2520agent%250Aframework%2520to%2520multi-agent%2520systems%2520without%2520any%2520extra%2520human%2520designs.%2520Experimental%250Aresults%2520across%2520various%2520tasks%2520have%2520shown%2520that%2520EvoAgent%2520can%2520automatically%250Agenerate%2520multiple%2520expert%2520agents%2520and%2520significantly%2520enhance%2520the%2520task-solving%250Acapabilities%2520of%2520LLM-based%2520agents.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2406.14228v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=EvoAgent%3A%20Towards%20Automatic%20Multi-Agent%20Generation%20via%20Evolutionary%0A%20%20Algorithms&entry.906535625=Siyu%20Yuan%20and%20Kaitao%20Song%20and%20Jiangjie%20Chen%20and%20Xu%20Tan%20and%20Dongsheng%20Li%20and%20Deqing%20Yang&entry.1292438233=%20%20The%20rise%20of%20powerful%20large%20language%20models%20%28LLMs%29%20has%20spurred%20a%20new%20trend%20in%0Abuilding%20LLM-based%20autonomous%20agents%20for%20solving%20complex%20tasks%2C%20especially%0Amulti-agent%20systems.%20Despite%20the%20remarkable%20progress%2C%20we%20notice%20that%20existing%0Aworks%20are%20heavily%20dependent%20on%20human-designed%20frameworks%2C%20which%20greatly%20limits%0Athe%20functional%20scope%20and%20scalability%20of%20agent%20systems.%20How%20to%20automatically%0Aextend%20the%20specialized%20agent%20to%20multi-agent%20systems%20to%20improve%20task-solving%0Acapability%20still%20remains%20a%20significant%20challenge.%20In%20this%20paper%2C%20we%20introduce%0AEvoAgent%2C%20a%20generic%20method%20to%20automatically%20extend%20expert%20agents%20to%20multi-agent%0Asystems%20via%20the%20evolutionary%20algorithm%2C%20thereby%20improving%20the%20effectiveness%20of%0ALLM-based%20agents%20in%20solving%20tasks.%20Specifically%2C%20we%20consider%20the%20existing%20agent%0Aframeworks%20as%20the%20initial%20individual%20and%20then%20apply%20a%20series%20of%20evolutionary%0Aoperators%20%28e.g.%2C%20mutation%2C%20crossover%2C%20selection%2C%20etc.%29%20to%20generate%20multiple%0Aagents%20with%20diverse%20agent%20settings.%20EvoAgent%20can%20be%20generalized%20to%20any%0ALLM-based%20agent%20framework%2C%20and%20can%20automatically%20extend%20the%20existing%20agent%0Aframework%20to%20multi-agent%20systems%20without%20any%20extra%20human%20designs.%20Experimental%0Aresults%20across%20various%20tasks%20have%20shown%20that%20EvoAgent%20can%20automatically%0Agenerate%20multiple%20expert%20agents%20and%20significantly%20enhance%20the%20task-solving%0Acapabilities%20of%20LLM-based%20agents.%0A&entry.1838667208=http%3A//arxiv.org/abs/2406.14228v1&entry.124074799=Read"},
{"title": "NNPP: A Learning-Based Heuristic Model for Accelerating Optimal Path\n  Planning on Uneven Terrain", "author": "Yiming Ji and Yang Liu and Guanghu Xie and Boyu Ma and Zongwu Xie and Baoshi Cao", "abstract": "  Intelligent autonomous path planning is essential for enhancing the\nexploration efficiency of mobile robots operating in uneven terrains like\nplanetary surfaces and off-road environments.In this paper, we propose the NNPP\nmodel for computing the heuristic region, enabling foundation algorithms like\nAstar to find the optimal path solely within this reduced search space,\neffectively decreasing the search time. The NNPP model learns semantic\ninformation about start and goal locations, as well as map representations,\nfrom numerous pre-annotated optimal path demonstrations, and produces a\nprobabilistic distribution over each pixel representing the likelihood of it\nbelonging to an optimal path on the map. More specifically, the paper computes\nthe traversal cost for each grid cell from the slope, roughness and elevation\ndifference obtained from the digital elevation model. Subsequently, the start\nand goal locations are encoded using a Gaussian distribution and different\nlocation encoding parameters are analyzed for their effect on model\nperformance. After training, the NNPP model is able to\n\\textcolor{revision}{accelerate} path planning on novel maps.\n", "link": "http://arxiv.org/abs/2308.04792v3", "date": "2024-06-20", "relevancy": 2.0622, "topK": [{"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.5421}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.536}, {"title": "Egocentric Computer Vision for Hands-Free Robotic Wheelchair Navigation", "link": "https://link.springer.com/article/10.1007/s10846-023-01807-4", "similarity": 0.4809}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20NNPP%3A%20A%20Learning-Based%20Heuristic%20Model%20for%20Accelerating%20Optimal%20Path%0A%20%20Planning%20on%20Uneven%20Terrain&body=Title%3A%20NNPP%3A%20A%20Learning-Based%20Heuristic%20Model%20for%20Accelerating%20Optimal%20Path%0A%20%20Planning%20on%20Uneven%20Terrain%0AAuthor%3A%20Yiming%20Ji%20and%20Yang%20Liu%20and%20Guanghu%20Xie%20and%20Boyu%20Ma%20and%20Zongwu%20Xie%20and%20Baoshi%20Cao%0AAbstract%3A%20%20%20Intelligent%20autonomous%20path%20planning%20is%20essential%20for%20enhancing%20the%0Aexploration%20efficiency%20of%20mobile%20robots%20operating%20in%20uneven%20terrains%20like%0Aplanetary%20surfaces%20and%20off-road%20environments.In%20this%20paper%2C%20we%20propose%20the%20NNPP%0Amodel%20for%20computing%20the%20heuristic%20region%2C%20enabling%20foundation%20algorithms%20like%0AAstar%20to%20find%20the%20optimal%20path%20solely%20within%20this%20reduced%20search%20space%2C%0Aeffectively%20decreasing%20the%20search%20time.%20The%20NNPP%20model%20learns%20semantic%0Ainformation%20about%20start%20and%20goal%20locations%2C%20as%20well%20as%20map%20representations%2C%0Afrom%20numerous%20pre-annotated%20optimal%20path%20demonstrations%2C%20and%20produces%20a%0Aprobabilistic%20distribution%20over%20each%20pixel%20representing%20the%20likelihood%20of%20it%0Abelonging%20to%20an%20optimal%20path%20on%20the%20map.%20More%20specifically%2C%20the%20paper%20computes%0Athe%20traversal%20cost%20for%20each%20grid%20cell%20from%20the%20slope%2C%20roughness%20and%20elevation%0Adifference%20obtained%20from%20the%20digital%20elevation%20model.%20Subsequently%2C%20the%20start%0Aand%20goal%20locations%20are%20encoded%20using%20a%20Gaussian%20distribution%20and%20different%0Alocation%20encoding%20parameters%20are%20analyzed%20for%20their%20effect%20on%20model%0Aperformance.%20After%20training%2C%20the%20NNPP%20model%20is%20able%20to%0A%5Ctextcolor%7Brevision%7D%7Baccelerate%7D%20path%20planning%20on%20novel%20maps.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2308.04792v3%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DNNPP%253A%2520A%2520Learning-Based%2520Heuristic%2520Model%2520for%2520Accelerating%2520Optimal%2520Path%250A%2520%2520Planning%2520on%2520Uneven%2520Terrain%26entry.906535625%3DYiming%2520Ji%2520and%2520Yang%2520Liu%2520and%2520Guanghu%2520Xie%2520and%2520Boyu%2520Ma%2520and%2520Zongwu%2520Xie%2520and%2520Baoshi%2520Cao%26entry.1292438233%3D%2520%2520Intelligent%2520autonomous%2520path%2520planning%2520is%2520essential%2520for%2520enhancing%2520the%250Aexploration%2520efficiency%2520of%2520mobile%2520robots%2520operating%2520in%2520uneven%2520terrains%2520like%250Aplanetary%2520surfaces%2520and%2520off-road%2520environments.In%2520this%2520paper%252C%2520we%2520propose%2520the%2520NNPP%250Amodel%2520for%2520computing%2520the%2520heuristic%2520region%252C%2520enabling%2520foundation%2520algorithms%2520like%250AAstar%2520to%2520find%2520the%2520optimal%2520path%2520solely%2520within%2520this%2520reduced%2520search%2520space%252C%250Aeffectively%2520decreasing%2520the%2520search%2520time.%2520The%2520NNPP%2520model%2520learns%2520semantic%250Ainformation%2520about%2520start%2520and%2520goal%2520locations%252C%2520as%2520well%2520as%2520map%2520representations%252C%250Afrom%2520numerous%2520pre-annotated%2520optimal%2520path%2520demonstrations%252C%2520and%2520produces%2520a%250Aprobabilistic%2520distribution%2520over%2520each%2520pixel%2520representing%2520the%2520likelihood%2520of%2520it%250Abelonging%2520to%2520an%2520optimal%2520path%2520on%2520the%2520map.%2520More%2520specifically%252C%2520the%2520paper%2520computes%250Athe%2520traversal%2520cost%2520for%2520each%2520grid%2520cell%2520from%2520the%2520slope%252C%2520roughness%2520and%2520elevation%250Adifference%2520obtained%2520from%2520the%2520digital%2520elevation%2520model.%2520Subsequently%252C%2520the%2520start%250Aand%2520goal%2520locations%2520are%2520encoded%2520using%2520a%2520Gaussian%2520distribution%2520and%2520different%250Alocation%2520encoding%2520parameters%2520are%2520analyzed%2520for%2520their%2520effect%2520on%2520model%250Aperformance.%2520After%2520training%252C%2520the%2520NNPP%2520model%2520is%2520able%2520to%250A%255Ctextcolor%257Brevision%257D%257Baccelerate%257D%2520path%2520planning%2520on%2520novel%2520maps.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2308.04792v3%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=NNPP%3A%20A%20Learning-Based%20Heuristic%20Model%20for%20Accelerating%20Optimal%20Path%0A%20%20Planning%20on%20Uneven%20Terrain&entry.906535625=Yiming%20Ji%20and%20Yang%20Liu%20and%20Guanghu%20Xie%20and%20Boyu%20Ma%20and%20Zongwu%20Xie%20and%20Baoshi%20Cao&entry.1292438233=%20%20Intelligent%20autonomous%20path%20planning%20is%20essential%20for%20enhancing%20the%0Aexploration%20efficiency%20of%20mobile%20robots%20operating%20in%20uneven%20terrains%20like%0Aplanetary%20surfaces%20and%20off-road%20environments.In%20this%20paper%2C%20we%20propose%20the%20NNPP%0Amodel%20for%20computing%20the%20heuristic%20region%2C%20enabling%20foundation%20algorithms%20like%0AAstar%20to%20find%20the%20optimal%20path%20solely%20within%20this%20reduced%20search%20space%2C%0Aeffectively%20decreasing%20the%20search%20time.%20The%20NNPP%20model%20learns%20semantic%0Ainformation%20about%20start%20and%20goal%20locations%2C%20as%20well%20as%20map%20representations%2C%0Afrom%20numerous%20pre-annotated%20optimal%20path%20demonstrations%2C%20and%20produces%20a%0Aprobabilistic%20distribution%20over%20each%20pixel%20representing%20the%20likelihood%20of%20it%0Abelonging%20to%20an%20optimal%20path%20on%20the%20map.%20More%20specifically%2C%20the%20paper%20computes%0Athe%20traversal%20cost%20for%20each%20grid%20cell%20from%20the%20slope%2C%20roughness%20and%20elevation%0Adifference%20obtained%20from%20the%20digital%20elevation%20model.%20Subsequently%2C%20the%20start%0Aand%20goal%20locations%20are%20encoded%20using%20a%20Gaussian%20distribution%20and%20different%0Alocation%20encoding%20parameters%20are%20analyzed%20for%20their%20effect%20on%20model%0Aperformance.%20After%20training%2C%20the%20NNPP%20model%20is%20able%20to%0A%5Ctextcolor%7Brevision%7D%7Baccelerate%7D%20path%20planning%20on%20novel%20maps.%0A&entry.1838667208=http%3A//arxiv.org/abs/2308.04792v3&entry.124074799=Read"},
{"title": "Latent. Functional Map", "author": "Marco Fumero and Marco Pegoraro and Valentino Maiorca and Francesco Locatello and Emanuele Rodol\u00e0", "abstract": "  Neural models learn data representations that lie on low-dimensional\nmanifolds, yet modeling the relation between these representational spaces is\nan ongoing challenge. By integrating spectral geometry principles into neural\nmodeling, we show that this problem can be better addressed in the functional\ndomain, mitigating complexity, while enhancing interpretability and\nperformances on downstream tasks. To this end, we introduce a multi-purpose\nframework to the representation learning community, which allows to: (i)\ncompare different spaces in an interpretable way and measure their intrinsic\nsimilarity; (ii) find correspondences between them, both in unsupervised and\nweakly supervised settings, and (iii) to effectively transfer representations\nbetween distinct spaces. We validate our framework on various applications,\nranging from stitching to retrieval tasks, demonstrating that latent functional\nmaps can serve as a swiss-army knife for representation alignment.\n", "link": "http://arxiv.org/abs/2406.14183v1", "date": "2024-06-20", "relevancy": 2.0522, "topK": [{"title": "Self-supervised Photographic Image Layout Representation Learning", "link": "http://arxiv.org/abs/2403.03740v1", "similarity": 0.5204}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5094}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.504}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Latent.%20Functional%20Map&body=Title%3A%20Latent.%20Functional%20Map%0AAuthor%3A%20Marco%20Fumero%20and%20Marco%20Pegoraro%20and%20Valentino%20Maiorca%20and%20Francesco%20Locatello%20and%20Emanuele%20Rodol%C3%A0%0AAbstract%3A%20%20%20Neural%20models%20learn%20data%20representations%20that%20lie%20on%20low-dimensional%0Amanifolds%2C%20yet%20modeling%20the%20relation%20between%20these%20representational%20spaces%20is%0Aan%20ongoing%20challenge.%20By%20integrating%20spectral%20geometry%20principles%20into%20neural%0Amodeling%2C%20we%20show%20that%20this%20problem%20can%20be%20better%20addressed%20in%20the%20functional%0Adomain%2C%20mitigating%20complexity%2C%20while%20enhancing%20interpretability%20and%0Aperformances%20on%20downstream%20tasks.%20To%20this%20end%2C%20we%20introduce%20a%20multi-purpose%0Aframework%20to%20the%20representation%20learning%20community%2C%20which%20allows%20to%3A%20%28i%29%0Acompare%20different%20spaces%20in%20an%20interpretable%20way%20and%20measure%20their%20intrinsic%0Asimilarity%3B%20%28ii%29%20find%20correspondences%20between%20them%2C%20both%20in%20unsupervised%20and%0Aweakly%20supervised%20settings%2C%20and%20%28iii%29%20to%20effectively%20transfer%20representations%0Abetween%20distinct%20spaces.%20We%20validate%20our%20framework%20on%20various%20applications%2C%0Aranging%20from%20stitching%20to%20retrieval%20tasks%2C%20demonstrating%20that%20latent%20functional%0Amaps%20can%20serve%20as%20a%20swiss-army%20knife%20for%20representation%20alignment.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2406.14183v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DLatent.%2520Functional%2520Map%26entry.906535625%3DMarco%2520Fumero%2520and%2520Marco%2520Pegoraro%2520and%2520Valentino%2520Maiorca%2520and%2520Francesco%2520Locatello%2520and%2520Emanuele%2520Rodol%25C3%25A0%26entry.1292438233%3D%2520%2520Neural%2520models%2520learn%2520data%2520representations%2520that%2520lie%2520on%2520low-dimensional%250Amanifolds%252C%2520yet%2520modeling%2520the%2520relation%2520between%2520these%2520representational%2520spaces%2520is%250Aan%2520ongoing%2520challenge.%2520By%2520integrating%2520spectral%2520geometry%2520principles%2520into%2520neural%250Amodeling%252C%2520we%2520show%2520that%2520this%2520problem%2520can%2520be%2520better%2520addressed%2520in%2520the%2520functional%250Adomain%252C%2520mitigating%2520complexity%252C%2520while%2520enhancing%2520interpretability%2520and%250Aperformances%2520on%2520downstream%2520tasks.%2520To%2520this%2520end%252C%2520we%2520introduce%2520a%2520multi-purpose%250Aframework%2520to%2520the%2520representation%2520learning%2520community%252C%2520which%2520allows%2520to%253A%2520%2528i%2529%250Acompare%2520different%2520spaces%2520in%2520an%2520interpretable%2520way%2520and%2520measure%2520their%2520intrinsic%250Asimilarity%253B%2520%2528ii%2529%2520find%2520correspondences%2520between%2520them%252C%2520both%2520in%2520unsupervised%2520and%250Aweakly%2520supervised%2520settings%252C%2520and%2520%2528iii%2529%2520to%2520effectively%2520transfer%2520representations%250Abetween%2520distinct%2520spaces.%2520We%2520validate%2520our%2520framework%2520on%2520various%2520applications%252C%250Aranging%2520from%2520stitching%2520to%2520retrieval%2520tasks%252C%2520demonstrating%2520that%2520latent%2520functional%250Amaps%2520can%2520serve%2520as%2520a%2520swiss-army%2520knife%2520for%2520representation%2520alignment.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2406.14183v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Latent.%20Functional%20Map&entry.906535625=Marco%20Fumero%20and%20Marco%20Pegoraro%20and%20Valentino%20Maiorca%20and%20Francesco%20Locatello%20and%20Emanuele%20Rodol%C3%A0&entry.1292438233=%20%20Neural%20models%20learn%20data%20representations%20that%20lie%20on%20low-dimensional%0Amanifolds%2C%20yet%20modeling%20the%20relation%20between%20these%20representational%20spaces%20is%0Aan%20ongoing%20challenge.%20By%20integrating%20spectral%20geometry%20principles%20into%20neural%0Amodeling%2C%20we%20show%20that%20this%20problem%20can%20be%20better%20addressed%20in%20the%20functional%0Adomain%2C%20mitigating%20complexity%2C%20while%20enhancing%20interpretability%20and%0Aperformances%20on%20downstream%20tasks.%20To%20this%20end%2C%20we%20introduce%20a%20multi-purpose%0Aframework%20to%20the%20representation%20learning%20community%2C%20which%20allows%20to%3A%20%28i%29%0Acompare%20different%20spaces%20in%20an%20interpretable%20way%20and%20measure%20their%20intrinsic%0Asimilarity%3B%20%28ii%29%20find%20correspondences%20between%20them%2C%20both%20in%20unsupervised%20and%0Aweakly%20supervised%20settings%2C%20and%20%28iii%29%20to%20effectively%20transfer%20representations%0Abetween%20distinct%20spaces.%20We%20validate%20our%20framework%20on%20various%20applications%2C%0Aranging%20from%20stitching%20to%20retrieval%20tasks%2C%20demonstrating%20that%20latent%20functional%0Amaps%20can%20serve%20as%20a%20swiss-army%20knife%20for%20representation%20alignment.%0A&entry.1838667208=http%3A//arxiv.org/abs/2406.14183v1&entry.124074799=Read"},
{"title": "Self-supervised Interpretable Concept-based Models for Text\n  Classification", "author": "Francesco De Santis and Philippe Bich and Gabriele Ciravegna and Pietro Barbiero and Danilo Giordano and Tania Cerquitelli", "abstract": "  Despite their success, Large-Language Models (LLMs) still face criticism as\ntheir lack of interpretability limits their controllability and reliability.\nTraditional post-hoc interpretation methods, based on attention and\ngradient-based analysis, offer limited insight into the model's decision-making\nprocesses. In the image field, Concept-based models have emerged as\nexplainable-by-design architectures, employing human-interpretable features as\nintermediate representations. However, these methods have not been yet adapted\nto textual data, mainly because they require expensive concept annotations,\nwhich are impractical for real-world text data. This paper addresses this\nchallenge by proposing a self-supervised Interpretable Concept Embedding Models\n(ICEMs). We leverage the generalization abilities of LLMs to predict the\nconcepts labels in a self-supervised way, while we deliver the final\npredictions with an interpretable function. The results of our experiments show\nthat ICEMs can be trained in a self-supervised way achieving similar\nperformance to fully supervised concept-based models and end-to-end black-box\nones. Additionally, we show that our models are (i) interpretable, offering\nmeaningful logical explanations for their predictions; (ii) interactable,\nallowing humans to modify intermediate predictions through concept\ninterventions; and (iii) controllable, guiding the LLMs' decoding process to\nfollow a required decision-making path.\n", "link": "http://arxiv.org/abs/2406.14335v1", "date": "2024-06-20", "relevancy": 2.0455, "topK": [{"title": "Self-supervised Photographic Image Layout Representation Learning", "link": "http://arxiv.org/abs/2403.03740v1", "similarity": 0.5229}, {"title": "Flexible Visual Recognition by Evidential Modeling of Confusion and Ignorance", "link": "http://openaccess.thecvf.com/content/ICCV2023/html/Fan_Flexible_Visual_Recognition_by_Evidential_Modeling_of_Confusion_and_Ignorance_ICCV_2023_paper.html", "similarity": 0.5032}, {"title": "Multimodal-Conditioned Latent Diffusion Models for Fashion Image Editing", "link": "http://arxiv.org/abs/2403.14828v2", "similarity": 0.503}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Self-supervised%20Interpretable%20Concept-based%20Models%20for%20Text%0A%20%20Classification&body=Title%3A%20Self-supervised%20Interpretable%20Concept-based%20Models%20for%20Text%0A%20%20Classification%0AAuthor%3A%20Francesco%20De%20Santis%20and%20Philippe%20Bich%20and%20Gabriele%20Ciravegna%20and%20Pietro%20Barbiero%20and%20Danilo%20Giordano%20and%20Tania%20Cerquitelli%0AAbstract%3A%20%20%20Despite%20their%20success%2C%20Large-Language%20Models%20%28LLMs%29%20still%20face%20criticism%20as%0Atheir%20lack%20of%20interpretability%20limits%20their%20controllability%20and%20reliability.%0ATraditional%20post-hoc%20interpretation%20methods%2C%20based%20on%20attention%20and%0Agradient-based%20analysis%2C%20offer%20limited%20insight%20into%20the%20model%27s%20decision-making%0Aprocesses.%20In%20the%20image%20field%2C%20Concept-based%20models%20have%20emerged%20as%0Aexplainable-by-design%20architectures%2C%20employing%20human-interpretable%20features%20as%0Aintermediate%20representations.%20However%2C%20these%20methods%20have%20not%20been%20yet%20adapted%0Ato%20textual%20data%2C%20mainly%20because%20they%20require%20expensive%20concept%20annotations%2C%0Awhich%20are%20impractical%20for%20real-world%20text%20data.%20This%20paper%20addresses%20this%0Achallenge%20by%20proposing%20a%20self-supervised%20Interpretable%20Concept%20Embedding%20Models%0A%28ICEMs%29.%20We%20leverage%20the%20generalization%20abilities%20of%20LLMs%20to%20predict%20the%0Aconcepts%20labels%20in%20a%20self-supervised%20way%2C%20while%20we%20deliver%20the%20final%0Apredictions%20with%20an%20interpretable%20function.%20The%20results%20of%20our%20experiments%20show%0Athat%20ICEMs%20can%20be%20trained%20in%20a%20self-supervised%20way%20achieving%20similar%0Aperformance%20to%20fully%20supervised%20concept-based%20models%20and%20end-to-end%20black-box%0Aones.%20Additionally%2C%20we%20show%20that%20our%20models%20are%20%28i%29%20interpretable%2C%20offering%0Ameaningful%20logical%20explanations%20for%20their%20predictions%3B%20%28ii%29%20interactable%2C%0Aallowing%20humans%20to%20modify%20intermediate%20predictions%20through%20concept%0Ainterventions%3B%20and%20%28iii%29%20controllable%2C%20guiding%20the%20LLMs%27%20decoding%20process%20to%0Afollow%20a%20required%20decision-making%20path.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2406.14335v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DSelf-supervised%2520Interpretable%2520Concept-based%2520Models%2520for%2520Text%250A%2520%2520Classification%26entry.906535625%3DFrancesco%2520De%2520Santis%2520and%2520Philippe%2520Bich%2520and%2520Gabriele%2520Ciravegna%2520and%2520Pietro%2520Barbiero%2520and%2520Danilo%2520Giordano%2520and%2520Tania%2520Cerquitelli%26entry.1292438233%3D%2520%2520Despite%2520their%2520success%252C%2520Large-Language%2520Models%2520%2528LLMs%2529%2520still%2520face%2520criticism%2520as%250Atheir%2520lack%2520of%2520interpretability%2520limits%2520their%2520controllability%2520and%2520reliability.%250ATraditional%2520post-hoc%2520interpretation%2520methods%252C%2520based%2520on%2520attention%2520and%250Agradient-based%2520analysis%252C%2520offer%2520limited%2520insight%2520into%2520the%2520model%2527s%2520decision-making%250Aprocesses.%2520In%2520the%2520image%2520field%252C%2520Concept-based%2520models%2520have%2520emerged%2520as%250Aexplainable-by-design%2520architectures%252C%2520employing%2520human-interpretable%2520features%2520as%250Aintermediate%2520representations.%2520However%252C%2520these%2520methods%2520have%2520not%2520been%2520yet%2520adapted%250Ato%2520textual%2520data%252C%2520mainly%2520because%2520they%2520require%2520expensive%2520concept%2520annotations%252C%250Awhich%2520are%2520impractical%2520for%2520real-world%2520text%2520data.%2520This%2520paper%2520addresses%2520this%250Achallenge%2520by%2520proposing%2520a%2520self-supervised%2520Interpretable%2520Concept%2520Embedding%2520Models%250A%2528ICEMs%2529.%2520We%2520leverage%2520the%2520generalization%2520abilities%2520of%2520LLMs%2520to%2520predict%2520the%250Aconcepts%2520labels%2520in%2520a%2520self-supervised%2520way%252C%2520while%2520we%2520deliver%2520the%2520final%250Apredictions%2520with%2520an%2520interpretable%2520function.%2520The%2520results%2520of%2520our%2520experiments%2520show%250Athat%2520ICEMs%2520can%2520be%2520trained%2520in%2520a%2520self-supervised%2520way%2520achieving%2520similar%250Aperformance%2520to%2520fully%2520supervised%2520concept-based%2520models%2520and%2520end-to-end%2520black-box%250Aones.%2520Additionally%252C%2520we%2520show%2520that%2520our%2520models%2520are%2520%2528i%2529%2520interpretable%252C%2520offering%250Ameaningful%2520logical%2520explanations%2520for%2520their%2520predictions%253B%2520%2528ii%2529%2520interactable%252C%250Aallowing%2520humans%2520to%2520modify%2520intermediate%2520predictions%2520through%2520concept%250Ainterventions%253B%2520and%2520%2528iii%2529%2520controllable%252C%2520guiding%2520the%2520LLMs%2527%2520decoding%2520process%2520to%250Afollow%2520a%2520required%2520decision-making%2520path.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2406.14335v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Self-supervised%20Interpretable%20Concept-based%20Models%20for%20Text%0A%20%20Classification&entry.906535625=Francesco%20De%20Santis%20and%20Philippe%20Bich%20and%20Gabriele%20Ciravegna%20and%20Pietro%20Barbiero%20and%20Danilo%20Giordano%20and%20Tania%20Cerquitelli&entry.1292438233=%20%20Despite%20their%20success%2C%20Large-Language%20Models%20%28LLMs%29%20still%20face%20criticism%20as%0Atheir%20lack%20of%20interpretability%20limits%20their%20controllability%20and%20reliability.%0ATraditional%20post-hoc%20interpretation%20methods%2C%20based%20on%20attention%20and%0Agradient-based%20analysis%2C%20offer%20limited%20insight%20into%20the%20model%27s%20decision-making%0Aprocesses.%20In%20the%20image%20field%2C%20Concept-based%20models%20have%20emerged%20as%0Aexplainable-by-design%20architectures%2C%20employing%20human-interpretable%20features%20as%0Aintermediate%20representations.%20However%2C%20these%20methods%20have%20not%20been%20yet%20adapted%0Ato%20textual%20data%2C%20mainly%20because%20they%20require%20expensive%20concept%20annotations%2C%0Awhich%20are%20impractical%20for%20real-world%20text%20data.%20This%20paper%20addresses%20this%0Achallenge%20by%20proposing%20a%20self-supervised%20Interpretable%20Concept%20Embedding%20Models%0A%28ICEMs%29.%20We%20leverage%20the%20generalization%20abilities%20of%20LLMs%20to%20predict%20the%0Aconcepts%20labels%20in%20a%20self-supervised%20way%2C%20while%20we%20deliver%20the%20final%0Apredictions%20with%20an%20interpretable%20function.%20The%20results%20of%20our%20experiments%20show%0Athat%20ICEMs%20can%20be%20trained%20in%20a%20self-supervised%20way%20achieving%20similar%0Aperformance%20to%20fully%20supervised%20concept-based%20models%20and%20end-to-end%20black-box%0Aones.%20Additionally%2C%20we%20show%20that%20our%20models%20are%20%28i%29%20interpretable%2C%20offering%0Ameaningful%20logical%20explanations%20for%20their%20predictions%3B%20%28ii%29%20interactable%2C%0Aallowing%20humans%20to%20modify%20intermediate%20predictions%20through%20concept%0Ainterventions%3B%20and%20%28iii%29%20controllable%2C%20guiding%20the%20LLMs%27%20decoding%20process%20to%0Afollow%20a%20required%20decision-making%20path.%0A&entry.1838667208=http%3A//arxiv.org/abs/2406.14335v1&entry.124074799=Read"},
{"title": "On Layer-wise Representation Similarity: Application for Multi-Exit\n  Models with a Single Classifier", "author": "Jiachen Jiang and Jinxin Zhou and Zhihui Zhu", "abstract": "  Analyzing the similarity of internal representations within and across\ndifferent models has been an important technique for understanding the behavior\nof deep neural networks. Most existing methods for analyzing the similarity\nbetween representations of high dimensions, such as those based on Canonical\nCorrelation Analysis (CCA) and widely used Centered Kernel Alignment (CKA),\nrely on statistical properties of the representations for a set of data points.\nIn this paper, we focus on transformer models and study the similarity of\nrepresentations between the hidden layers of individual transformers. In this\ncontext, we show that a simple sample-wise cosine similarity metric is capable\nof capturing the similarity and aligns with the complicated CKA. Our\nexperimental results on common transformers reveal that representations across\nlayers are positively correlated, albeit the similarity decreases when layers\nare far apart. We then propose an aligned training approach to enhance the\nsimilarity between internal representations, with trained models that enjoy the\nfollowing properties: (1) the last-layer classifier can be directly applied\nright after any hidden layers, yielding intermediate layer accuracies much\nhigher than those under standard training, (2) the layer-wise accuracies\nmonotonically increase and reveal the minimal depth needed for the given task,\n(3) when served as multi-exit models, they achieve on-par performance with\nstandard multi-exit architectures which consist of additional classifiers\ndesigned for early exiting in shallow layers. To our knowledge, our work is the\nfirst to show that one common classifier is sufficient for multi-exit models.\nWe conduct experiments on both vision and NLP tasks to demonstrate the\nperformance of the proposed aligned training.\n", "link": "http://arxiv.org/abs/2406.14479v1", "date": "2024-06-20", "relevancy": 2.0415, "topK": [{"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5518}, {"title": "Scaling Rectified Flow Transformers for High-Resolution Image Synthesis", "link": "http://arxiv.org/abs/2403.03206v1", "similarity": 0.4872}, {"title": "DDM-NET: End-to-end learning of keypoint feature Detection, Description and Matching for 3D localization", "link": "https://arxiv.org/abs/2212.04575", "similarity": 0.4783}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20On%20Layer-wise%20Representation%20Similarity%3A%20Application%20for%20Multi-Exit%0A%20%20Models%20with%20a%20Single%20Classifier&body=Title%3A%20On%20Layer-wise%20Representation%20Similarity%3A%20Application%20for%20Multi-Exit%0A%20%20Models%20with%20a%20Single%20Classifier%0AAuthor%3A%20Jiachen%20Jiang%20and%20Jinxin%20Zhou%20and%20Zhihui%20Zhu%0AAbstract%3A%20%20%20Analyzing%20the%20similarity%20of%20internal%20representations%20within%20and%20across%0Adifferent%20models%20has%20been%20an%20important%20technique%20for%20understanding%20the%20behavior%0Aof%20deep%20neural%20networks.%20Most%20existing%20methods%20for%20analyzing%20the%20similarity%0Abetween%20representations%20of%20high%20dimensions%2C%20such%20as%20those%20based%20on%20Canonical%0ACorrelation%20Analysis%20%28CCA%29%20and%20widely%20used%20Centered%20Kernel%20Alignment%20%28CKA%29%2C%0Arely%20on%20statistical%20properties%20of%20the%20representations%20for%20a%20set%20of%20data%20points.%0AIn%20this%20paper%2C%20we%20focus%20on%20transformer%20models%20and%20study%20the%20similarity%20of%0Arepresentations%20between%20the%20hidden%20layers%20of%20individual%20transformers.%20In%20this%0Acontext%2C%20we%20show%20that%20a%20simple%20sample-wise%20cosine%20similarity%20metric%20is%20capable%0Aof%20capturing%20the%20similarity%20and%20aligns%20with%20the%20complicated%20CKA.%20Our%0Aexperimental%20results%20on%20common%20transformers%20reveal%20that%20representations%20across%0Alayers%20are%20positively%20correlated%2C%20albeit%20the%20similarity%20decreases%20when%20layers%0Aare%20far%20apart.%20We%20then%20propose%20an%20aligned%20training%20approach%20to%20enhance%20the%0Asimilarity%20between%20internal%20representations%2C%20with%20trained%20models%20that%20enjoy%20the%0Afollowing%20properties%3A%20%281%29%20the%20last-layer%20classifier%20can%20be%20directly%20applied%0Aright%20after%20any%20hidden%20layers%2C%20yielding%20intermediate%20layer%20accuracies%20much%0Ahigher%20than%20those%20under%20standard%20training%2C%20%282%29%20the%20layer-wise%20accuracies%0Amonotonically%20increase%20and%20reveal%20the%20minimal%20depth%20needed%20for%20the%20given%20task%2C%0A%283%29%20when%20served%20as%20multi-exit%20models%2C%20they%20achieve%20on-par%20performance%20with%0Astandard%20multi-exit%20architectures%20which%20consist%20of%20additional%20classifiers%0Adesigned%20for%20early%20exiting%20in%20shallow%20layers.%20To%20our%20knowledge%2C%20our%20work%20is%20the%0Afirst%20to%20show%20that%20one%20common%20classifier%20is%20sufficient%20for%20multi-exit%20models.%0AWe%20conduct%20experiments%20on%20both%20vision%20and%20NLP%20tasks%20to%20demonstrate%20the%0Aperformance%20of%20the%20proposed%20aligned%20training.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2406.14479v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DOn%2520Layer-wise%2520Representation%2520Similarity%253A%2520Application%2520for%2520Multi-Exit%250A%2520%2520Models%2520with%2520a%2520Single%2520Classifier%26entry.906535625%3DJiachen%2520Jiang%2520and%2520Jinxin%2520Zhou%2520and%2520Zhihui%2520Zhu%26entry.1292438233%3D%2520%2520Analyzing%2520the%2520similarity%2520of%2520internal%2520representations%2520within%2520and%2520across%250Adifferent%2520models%2520has%2520been%2520an%2520important%2520technique%2520for%2520understanding%2520the%2520behavior%250Aof%2520deep%2520neural%2520networks.%2520Most%2520existing%2520methods%2520for%2520analyzing%2520the%2520similarity%250Abetween%2520representations%2520of%2520high%2520dimensions%252C%2520such%2520as%2520those%2520based%2520on%2520Canonical%250ACorrelation%2520Analysis%2520%2528CCA%2529%2520and%2520widely%2520used%2520Centered%2520Kernel%2520Alignment%2520%2528CKA%2529%252C%250Arely%2520on%2520statistical%2520properties%2520of%2520the%2520representations%2520for%2520a%2520set%2520of%2520data%2520points.%250AIn%2520this%2520paper%252C%2520we%2520focus%2520on%2520transformer%2520models%2520and%2520study%2520the%2520similarity%2520of%250Arepresentations%2520between%2520the%2520hidden%2520layers%2520of%2520individual%2520transformers.%2520In%2520this%250Acontext%252C%2520we%2520show%2520that%2520a%2520simple%2520sample-wise%2520cosine%2520similarity%2520metric%2520is%2520capable%250Aof%2520capturing%2520the%2520similarity%2520and%2520aligns%2520with%2520the%2520complicated%2520CKA.%2520Our%250Aexperimental%2520results%2520on%2520common%2520transformers%2520reveal%2520that%2520representations%2520across%250Alayers%2520are%2520positively%2520correlated%252C%2520albeit%2520the%2520similarity%2520decreases%2520when%2520layers%250Aare%2520far%2520apart.%2520We%2520then%2520propose%2520an%2520aligned%2520training%2520approach%2520to%2520enhance%2520the%250Asimilarity%2520between%2520internal%2520representations%252C%2520with%2520trained%2520models%2520that%2520enjoy%2520the%250Afollowing%2520properties%253A%2520%25281%2529%2520the%2520last-layer%2520classifier%2520can%2520be%2520directly%2520applied%250Aright%2520after%2520any%2520hidden%2520layers%252C%2520yielding%2520intermediate%2520layer%2520accuracies%2520much%250Ahigher%2520than%2520those%2520under%2520standard%2520training%252C%2520%25282%2529%2520the%2520layer-wise%2520accuracies%250Amonotonically%2520increase%2520and%2520reveal%2520the%2520minimal%2520depth%2520needed%2520for%2520the%2520given%2520task%252C%250A%25283%2529%2520when%2520served%2520as%2520multi-exit%2520models%252C%2520they%2520achieve%2520on-par%2520performance%2520with%250Astandard%2520multi-exit%2520architectures%2520which%2520consist%2520of%2520additional%2520classifiers%250Adesigned%2520for%2520early%2520exiting%2520in%2520shallow%2520layers.%2520To%2520our%2520knowledge%252C%2520our%2520work%2520is%2520the%250Afirst%2520to%2520show%2520that%2520one%2520common%2520classifier%2520is%2520sufficient%2520for%2520multi-exit%2520models.%250AWe%2520conduct%2520experiments%2520on%2520both%2520vision%2520and%2520NLP%2520tasks%2520to%2520demonstrate%2520the%250Aperformance%2520of%2520the%2520proposed%2520aligned%2520training.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2406.14479v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=On%20Layer-wise%20Representation%20Similarity%3A%20Application%20for%20Multi-Exit%0A%20%20Models%20with%20a%20Single%20Classifier&entry.906535625=Jiachen%20Jiang%20and%20Jinxin%20Zhou%20and%20Zhihui%20Zhu&entry.1292438233=%20%20Analyzing%20the%20similarity%20of%20internal%20representations%20within%20and%20across%0Adifferent%20models%20has%20been%20an%20important%20technique%20for%20understanding%20the%20behavior%0Aof%20deep%20neural%20networks.%20Most%20existing%20methods%20for%20analyzing%20the%20similarity%0Abetween%20representations%20of%20high%20dimensions%2C%20such%20as%20those%20based%20on%20Canonical%0ACorrelation%20Analysis%20%28CCA%29%20and%20widely%20used%20Centered%20Kernel%20Alignment%20%28CKA%29%2C%0Arely%20on%20statistical%20properties%20of%20the%20representations%20for%20a%20set%20of%20data%20points.%0AIn%20this%20paper%2C%20we%20focus%20on%20transformer%20models%20and%20study%20the%20similarity%20of%0Arepresentations%20between%20the%20hidden%20layers%20of%20individual%20transformers.%20In%20this%0Acontext%2C%20we%20show%20that%20a%20simple%20sample-wise%20cosine%20similarity%20metric%20is%20capable%0Aof%20capturing%20the%20similarity%20and%20aligns%20with%20the%20complicated%20CKA.%20Our%0Aexperimental%20results%20on%20common%20transformers%20reveal%20that%20representations%20across%0Alayers%20are%20positively%20correlated%2C%20albeit%20the%20similarity%20decreases%20when%20layers%0Aare%20far%20apart.%20We%20then%20propose%20an%20aligned%20training%20approach%20to%20enhance%20the%0Asimilarity%20between%20internal%20representations%2C%20with%20trained%20models%20that%20enjoy%20the%0Afollowing%20properties%3A%20%281%29%20the%20last-layer%20classifier%20can%20be%20directly%20applied%0Aright%20after%20any%20hidden%20layers%2C%20yielding%20intermediate%20layer%20accuracies%20much%0Ahigher%20than%20those%20under%20standard%20training%2C%20%282%29%20the%20layer-wise%20accuracies%0Amonotonically%20increase%20and%20reveal%20the%20minimal%20depth%20needed%20for%20the%20given%20task%2C%0A%283%29%20when%20served%20as%20multi-exit%20models%2C%20they%20achieve%20on-par%20performance%20with%0Astandard%20multi-exit%20architectures%20which%20consist%20of%20additional%20classifiers%0Adesigned%20for%20early%20exiting%20in%20shallow%20layers.%20To%20our%20knowledge%2C%20our%20work%20is%20the%0Afirst%20to%20show%20that%20one%20common%20classifier%20is%20sufficient%20for%20multi-exit%20models.%0AWe%20conduct%20experiments%20on%20both%20vision%20and%20NLP%20tasks%20to%20demonstrate%20the%0Aperformance%20of%20the%20proposed%20aligned%20training.%0A&entry.1838667208=http%3A//arxiv.org/abs/2406.14479v1&entry.124074799=Read"},
{"title": "A Single Graph Convolution Is All You Need: Efficient Grayscale Image\n  Classification", "author": "Jacob Fein-Ashley and Tian Ye and Sachini Wickramasinghe and Bingyi Zhang and Rajgopal Kannan and Viktor Prasanna", "abstract": "  Image classifiers often rely on convolutional neural networks (CNN) for their\ntasks, which, for image classification, experience high latency due to the\nnumber of operations they perform, which can be problematic in real-time\napplications. Additionally, many image classification models work on both RGB\nand grayscale datasets. Classifiers that operate solely on grayscale images are\nmuch less common. Grayscale image classification has diverse applications,\nincluding but not limited to medical image classification and synthetic\naperture radar (SAR) automatic target recognition (ATR). Thus, we present a\nnovel grayscale image classification approach using a vectorized view of\nimages. We exploit the lightweightness of MLPs by viewing images as vectors and\nreducing our problem setting to the grayscale image classification setting. We\nfind that using a single graph convolutional layer batch-wise increases\naccuracy and reduces variance in the performance of our model. Moreover, we\ndevelop a customized accelerator on FPGA for the proposed model with several\noptimizations to improve its performance. Our experimental results on benchmark\ngrayscale image datasets demonstrate the effectiveness of the proposed model,\nachieving vastly lower latency (up to 16$\\times$ less) and competitive or\nleading performance compared to other state-of-the-art image classification\nmodels on various domain-specific grayscale image classification datasets.\n", "link": "http://arxiv.org/abs/2402.00564v5", "date": "2024-06-20", "relevancy": 2.0384, "topK": [{"title": "SDXS: Real-Time One-Step Latent Diffusion Models with Image Conditions", "link": "http://arxiv.org/abs/2403.16627v1", "similarity": 0.5418}, {"title": "ToDo: Token Downsampling for Efficient Generation of High-Resolution\n  Images", "link": "http://arxiv.org/abs/2402.13573v2", "similarity": 0.5173}, {"title": "Self-supervised Photographic Image Layout Representation Learning", "link": "http://arxiv.org/abs/2403.03740v1", "similarity": 0.489}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20A%20Single%20Graph%20Convolution%20Is%20All%20You%20Need%3A%20Efficient%20Grayscale%20Image%0A%20%20Classification&body=Title%3A%20A%20Single%20Graph%20Convolution%20Is%20All%20You%20Need%3A%20Efficient%20Grayscale%20Image%0A%20%20Classification%0AAuthor%3A%20Jacob%20Fein-Ashley%20and%20Tian%20Ye%20and%20Sachini%20Wickramasinghe%20and%20Bingyi%20Zhang%20and%20Rajgopal%20Kannan%20and%20Viktor%20Prasanna%0AAbstract%3A%20%20%20Image%20classifiers%20often%20rely%20on%20convolutional%20neural%20networks%20%28CNN%29%20for%20their%0Atasks%2C%20which%2C%20for%20image%20classification%2C%20experience%20high%20latency%20due%20to%20the%0Anumber%20of%20operations%20they%20perform%2C%20which%20can%20be%20problematic%20in%20real-time%0Aapplications.%20Additionally%2C%20many%20image%20classification%20models%20work%20on%20both%20RGB%0Aand%20grayscale%20datasets.%20Classifiers%20that%20operate%20solely%20on%20grayscale%20images%20are%0Amuch%20less%20common.%20Grayscale%20image%20classification%20has%20diverse%20applications%2C%0Aincluding%20but%20not%20limited%20to%20medical%20image%20classification%20and%20synthetic%0Aaperture%20radar%20%28SAR%29%20automatic%20target%20recognition%20%28ATR%29.%20Thus%2C%20we%20present%20a%0Anovel%20grayscale%20image%20classification%20approach%20using%20a%20vectorized%20view%20of%0Aimages.%20We%20exploit%20the%20lightweightness%20of%20MLPs%20by%20viewing%20images%20as%20vectors%20and%0Areducing%20our%20problem%20setting%20to%20the%20grayscale%20image%20classification%20setting.%20We%0Afind%20that%20using%20a%20single%20graph%20convolutional%20layer%20batch-wise%20increases%0Aaccuracy%20and%20reduces%20variance%20in%20the%20performance%20of%20our%20model.%20Moreover%2C%20we%0Adevelop%20a%20customized%20accelerator%20on%20FPGA%20for%20the%20proposed%20model%20with%20several%0Aoptimizations%20to%20improve%20its%20performance.%20Our%20experimental%20results%20on%20benchmark%0Agrayscale%20image%20datasets%20demonstrate%20the%20effectiveness%20of%20the%20proposed%20model%2C%0Aachieving%20vastly%20lower%20latency%20%28up%20to%2016%24%5Ctimes%24%20less%29%20and%20competitive%20or%0Aleading%20performance%20compared%20to%20other%20state-of-the-art%20image%20classification%0Amodels%20on%20various%20domain-specific%20grayscale%20image%20classification%20datasets.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2402.00564v5%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DA%2520Single%2520Graph%2520Convolution%2520Is%2520All%2520You%2520Need%253A%2520Efficient%2520Grayscale%2520Image%250A%2520%2520Classification%26entry.906535625%3DJacob%2520Fein-Ashley%2520and%2520Tian%2520Ye%2520and%2520Sachini%2520Wickramasinghe%2520and%2520Bingyi%2520Zhang%2520and%2520Rajgopal%2520Kannan%2520and%2520Viktor%2520Prasanna%26entry.1292438233%3D%2520%2520Image%2520classifiers%2520often%2520rely%2520on%2520convolutional%2520neural%2520networks%2520%2528CNN%2529%2520for%2520their%250Atasks%252C%2520which%252C%2520for%2520image%2520classification%252C%2520experience%2520high%2520latency%2520due%2520to%2520the%250Anumber%2520of%2520operations%2520they%2520perform%252C%2520which%2520can%2520be%2520problematic%2520in%2520real-time%250Aapplications.%2520Additionally%252C%2520many%2520image%2520classification%2520models%2520work%2520on%2520both%2520RGB%250Aand%2520grayscale%2520datasets.%2520Classifiers%2520that%2520operate%2520solely%2520on%2520grayscale%2520images%2520are%250Amuch%2520less%2520common.%2520Grayscale%2520image%2520classification%2520has%2520diverse%2520applications%252C%250Aincluding%2520but%2520not%2520limited%2520to%2520medical%2520image%2520classification%2520and%2520synthetic%250Aaperture%2520radar%2520%2528SAR%2529%2520automatic%2520target%2520recognition%2520%2528ATR%2529.%2520Thus%252C%2520we%2520present%2520a%250Anovel%2520grayscale%2520image%2520classification%2520approach%2520using%2520a%2520vectorized%2520view%2520of%250Aimages.%2520We%2520exploit%2520the%2520lightweightness%2520of%2520MLPs%2520by%2520viewing%2520images%2520as%2520vectors%2520and%250Areducing%2520our%2520problem%2520setting%2520to%2520the%2520grayscale%2520image%2520classification%2520setting.%2520We%250Afind%2520that%2520using%2520a%2520single%2520graph%2520convolutional%2520layer%2520batch-wise%2520increases%250Aaccuracy%2520and%2520reduces%2520variance%2520in%2520the%2520performance%2520of%2520our%2520model.%2520Moreover%252C%2520we%250Adevelop%2520a%2520customized%2520accelerator%2520on%2520FPGA%2520for%2520the%2520proposed%2520model%2520with%2520several%250Aoptimizations%2520to%2520improve%2520its%2520performance.%2520Our%2520experimental%2520results%2520on%2520benchmark%250Agrayscale%2520image%2520datasets%2520demonstrate%2520the%2520effectiveness%2520of%2520the%2520proposed%2520model%252C%250Aachieving%2520vastly%2520lower%2520latency%2520%2528up%2520to%252016%2524%255Ctimes%2524%2520less%2529%2520and%2520competitive%2520or%250Aleading%2520performance%2520compared%2520to%2520other%2520state-of-the-art%2520image%2520classification%250Amodels%2520on%2520various%2520domain-specific%2520grayscale%2520image%2520classification%2520datasets.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2402.00564v5%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=A%20Single%20Graph%20Convolution%20Is%20All%20You%20Need%3A%20Efficient%20Grayscale%20Image%0A%20%20Classification&entry.906535625=Jacob%20Fein-Ashley%20and%20Tian%20Ye%20and%20Sachini%20Wickramasinghe%20and%20Bingyi%20Zhang%20and%20Rajgopal%20Kannan%20and%20Viktor%20Prasanna&entry.1292438233=%20%20Image%20classifiers%20often%20rely%20on%20convolutional%20neural%20networks%20%28CNN%29%20for%20their%0Atasks%2C%20which%2C%20for%20image%20classification%2C%20experience%20high%20latency%20due%20to%20the%0Anumber%20of%20operations%20they%20perform%2C%20which%20can%20be%20problematic%20in%20real-time%0Aapplications.%20Additionally%2C%20many%20image%20classification%20models%20work%20on%20both%20RGB%0Aand%20grayscale%20datasets.%20Classifiers%20that%20operate%20solely%20on%20grayscale%20images%20are%0Amuch%20less%20common.%20Grayscale%20image%20classification%20has%20diverse%20applications%2C%0Aincluding%20but%20not%20limited%20to%20medical%20image%20classification%20and%20synthetic%0Aaperture%20radar%20%28SAR%29%20automatic%20target%20recognition%20%28ATR%29.%20Thus%2C%20we%20present%20a%0Anovel%20grayscale%20image%20classification%20approach%20using%20a%20vectorized%20view%20of%0Aimages.%20We%20exploit%20the%20lightweightness%20of%20MLPs%20by%20viewing%20images%20as%20vectors%20and%0Areducing%20our%20problem%20setting%20to%20the%20grayscale%20image%20classification%20setting.%20We%0Afind%20that%20using%20a%20single%20graph%20convolutional%20layer%20batch-wise%20increases%0Aaccuracy%20and%20reduces%20variance%20in%20the%20performance%20of%20our%20model.%20Moreover%2C%20we%0Adevelop%20a%20customized%20accelerator%20on%20FPGA%20for%20the%20proposed%20model%20with%20several%0Aoptimizations%20to%20improve%20its%20performance.%20Our%20experimental%20results%20on%20benchmark%0Agrayscale%20image%20datasets%20demonstrate%20the%20effectiveness%20of%20the%20proposed%20model%2C%0Aachieving%20vastly%20lower%20latency%20%28up%20to%2016%24%5Ctimes%24%20less%29%20and%20competitive%20or%0Aleading%20performance%20compared%20to%20other%20state-of-the-art%20image%20classification%0Amodels%20on%20various%20domain-specific%20grayscale%20image%20classification%20datasets.%0A&entry.1838667208=http%3A//arxiv.org/abs/2402.00564v5&entry.124074799=Read"},
{"title": "Mask the Unknown: Assessing Different Strategies to Handle Weak\n  Annotations in the MICCAI2023 Mediastinal Lymph Node Quantification Challenge", "author": "Stefan M. Fischer and Johannes Kiechle and Daniel M. Lang and Jan C. Peeken and Julia A. Schnabel", "abstract": "  Pathological lymph node delineation is crucial in cancer diagnosis,\nprogression assessment, and treatment planning. The MICCAI 2023 Lymph Node\nQuantification Challenge published the first public dataset for pathological\nlymph node segmentation in the mediastinum. As lymph node annotations are\nexpensive, the challenge was formed as a weakly supervised learning task, where\nonly a subset of all lymph nodes in the training set have been annotated. For\nthe challenge submission, multiple methods for training on these weakly\nsupervised data were explored, including noisy label training, loss masking of\nunlabeled data, and an approach that integrated the TotalSegmentator toolbox as\na form of pseudo labeling in order to reduce the number of unknown voxels.\nFurthermore, multiple public TCIA datasets were incorporated into the training\nto improve the performance of the deep learning model. Our submitted model\nachieved a Dice score of 0.628 and an average symmetric surface distance of\n5.8~mm on the challenge test set. With our submitted model, we accomplished\nthird rank in the MICCAI2023 LNQ challenge. A finding of our analysis was that\nthe integration of all visible, including non-pathological, lymph nodes\nimproved the overall segmentation performance on pathological lymph nodes of\nthe test set. Furthermore, segmentation models trained only on clinically\nenlarged lymph nodes, as given in the challenge scenario, could not generalize\nto smaller pathological lymph nodes. The code and model for the challenge\nsubmission are available at\n\\url{https://gitlab.lrz.de/compai/MediastinalLymphNodeSegmentation}.\n", "link": "http://arxiv.org/abs/2406.14365v1", "date": "2024-06-20", "relevancy": 2.037, "topK": [{"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.5525}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5098}, {"title": "DDM-NET: End-to-end learning of keypoint feature Detection, Description and Matching for 3D localization", "link": "https://arxiv.org/abs/2212.04575", "similarity": 0.4914}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Mask%20the%20Unknown%3A%20Assessing%20Different%20Strategies%20to%20Handle%20Weak%0A%20%20Annotations%20in%20the%20MICCAI2023%20Mediastinal%20Lymph%20Node%20Quantification%20Challenge&body=Title%3A%20Mask%20the%20Unknown%3A%20Assessing%20Different%20Strategies%20to%20Handle%20Weak%0A%20%20Annotations%20in%20the%20MICCAI2023%20Mediastinal%20Lymph%20Node%20Quantification%20Challenge%0AAuthor%3A%20Stefan%20M.%20Fischer%20and%20Johannes%20Kiechle%20and%20Daniel%20M.%20Lang%20and%20Jan%20C.%20Peeken%20and%20Julia%20A.%20Schnabel%0AAbstract%3A%20%20%20Pathological%20lymph%20node%20delineation%20is%20crucial%20in%20cancer%20diagnosis%2C%0Aprogression%20assessment%2C%20and%20treatment%20planning.%20The%20MICCAI%202023%20Lymph%20Node%0AQuantification%20Challenge%20published%20the%20first%20public%20dataset%20for%20pathological%0Alymph%20node%20segmentation%20in%20the%20mediastinum.%20As%20lymph%20node%20annotations%20are%0Aexpensive%2C%20the%20challenge%20was%20formed%20as%20a%20weakly%20supervised%20learning%20task%2C%20where%0Aonly%20a%20subset%20of%20all%20lymph%20nodes%20in%20the%20training%20set%20have%20been%20annotated.%20For%0Athe%20challenge%20submission%2C%20multiple%20methods%20for%20training%20on%20these%20weakly%0Asupervised%20data%20were%20explored%2C%20including%20noisy%20label%20training%2C%20loss%20masking%20of%0Aunlabeled%20data%2C%20and%20an%20approach%20that%20integrated%20the%20TotalSegmentator%20toolbox%20as%0Aa%20form%20of%20pseudo%20labeling%20in%20order%20to%20reduce%20the%20number%20of%20unknown%20voxels.%0AFurthermore%2C%20multiple%20public%20TCIA%20datasets%20were%20incorporated%20into%20the%20training%0Ato%20improve%20the%20performance%20of%20the%20deep%20learning%20model.%20Our%20submitted%20model%0Aachieved%20a%20Dice%20score%20of%200.628%20and%20an%20average%20symmetric%20surface%20distance%20of%0A5.8~mm%20on%20the%20challenge%20test%20set.%20With%20our%20submitted%20model%2C%20we%20accomplished%0Athird%20rank%20in%20the%20MICCAI2023%20LNQ%20challenge.%20A%20finding%20of%20our%20analysis%20was%20that%0Athe%20integration%20of%20all%20visible%2C%20including%20non-pathological%2C%20lymph%20nodes%0Aimproved%20the%20overall%20segmentation%20performance%20on%20pathological%20lymph%20nodes%20of%0Athe%20test%20set.%20Furthermore%2C%20segmentation%20models%20trained%20only%20on%20clinically%0Aenlarged%20lymph%20nodes%2C%20as%20given%20in%20the%20challenge%20scenario%2C%20could%20not%20generalize%0Ato%20smaller%20pathological%20lymph%20nodes.%20The%20code%20and%20model%20for%20the%20challenge%0Asubmission%20are%20available%20at%0A%5Curl%7Bhttps%3A//gitlab.lrz.de/compai/MediastinalLymphNodeSegmentation%7D.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2406.14365v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DMask%2520the%2520Unknown%253A%2520Assessing%2520Different%2520Strategies%2520to%2520Handle%2520Weak%250A%2520%2520Annotations%2520in%2520the%2520MICCAI2023%2520Mediastinal%2520Lymph%2520Node%2520Quantification%2520Challenge%26entry.906535625%3DStefan%2520M.%2520Fischer%2520and%2520Johannes%2520Kiechle%2520and%2520Daniel%2520M.%2520Lang%2520and%2520Jan%2520C.%2520Peeken%2520and%2520Julia%2520A.%2520Schnabel%26entry.1292438233%3D%2520%2520Pathological%2520lymph%2520node%2520delineation%2520is%2520crucial%2520in%2520cancer%2520diagnosis%252C%250Aprogression%2520assessment%252C%2520and%2520treatment%2520planning.%2520The%2520MICCAI%25202023%2520Lymph%2520Node%250AQuantification%2520Challenge%2520published%2520the%2520first%2520public%2520dataset%2520for%2520pathological%250Alymph%2520node%2520segmentation%2520in%2520the%2520mediastinum.%2520As%2520lymph%2520node%2520annotations%2520are%250Aexpensive%252C%2520the%2520challenge%2520was%2520formed%2520as%2520a%2520weakly%2520supervised%2520learning%2520task%252C%2520where%250Aonly%2520a%2520subset%2520of%2520all%2520lymph%2520nodes%2520in%2520the%2520training%2520set%2520have%2520been%2520annotated.%2520For%250Athe%2520challenge%2520submission%252C%2520multiple%2520methods%2520for%2520training%2520on%2520these%2520weakly%250Asupervised%2520data%2520were%2520explored%252C%2520including%2520noisy%2520label%2520training%252C%2520loss%2520masking%2520of%250Aunlabeled%2520data%252C%2520and%2520an%2520approach%2520that%2520integrated%2520the%2520TotalSegmentator%2520toolbox%2520as%250Aa%2520form%2520of%2520pseudo%2520labeling%2520in%2520order%2520to%2520reduce%2520the%2520number%2520of%2520unknown%2520voxels.%250AFurthermore%252C%2520multiple%2520public%2520TCIA%2520datasets%2520were%2520incorporated%2520into%2520the%2520training%250Ato%2520improve%2520the%2520performance%2520of%2520the%2520deep%2520learning%2520model.%2520Our%2520submitted%2520model%250Aachieved%2520a%2520Dice%2520score%2520of%25200.628%2520and%2520an%2520average%2520symmetric%2520surface%2520distance%2520of%250A5.8~mm%2520on%2520the%2520challenge%2520test%2520set.%2520With%2520our%2520submitted%2520model%252C%2520we%2520accomplished%250Athird%2520rank%2520in%2520the%2520MICCAI2023%2520LNQ%2520challenge.%2520A%2520finding%2520of%2520our%2520analysis%2520was%2520that%250Athe%2520integration%2520of%2520all%2520visible%252C%2520including%2520non-pathological%252C%2520lymph%2520nodes%250Aimproved%2520the%2520overall%2520segmentation%2520performance%2520on%2520pathological%2520lymph%2520nodes%2520of%250Athe%2520test%2520set.%2520Furthermore%252C%2520segmentation%2520models%2520trained%2520only%2520on%2520clinically%250Aenlarged%2520lymph%2520nodes%252C%2520as%2520given%2520in%2520the%2520challenge%2520scenario%252C%2520could%2520not%2520generalize%250Ato%2520smaller%2520pathological%2520lymph%2520nodes.%2520The%2520code%2520and%2520model%2520for%2520the%2520challenge%250Asubmission%2520are%2520available%2520at%250A%255Curl%257Bhttps%253A//gitlab.lrz.de/compai/MediastinalLymphNodeSegmentation%257D.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2406.14365v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Mask%20the%20Unknown%3A%20Assessing%20Different%20Strategies%20to%20Handle%20Weak%0A%20%20Annotations%20in%20the%20MICCAI2023%20Mediastinal%20Lymph%20Node%20Quantification%20Challenge&entry.906535625=Stefan%20M.%20Fischer%20and%20Johannes%20Kiechle%20and%20Daniel%20M.%20Lang%20and%20Jan%20C.%20Peeken%20and%20Julia%20A.%20Schnabel&entry.1292438233=%20%20Pathological%20lymph%20node%20delineation%20is%20crucial%20in%20cancer%20diagnosis%2C%0Aprogression%20assessment%2C%20and%20treatment%20planning.%20The%20MICCAI%202023%20Lymph%20Node%0AQuantification%20Challenge%20published%20the%20first%20public%20dataset%20for%20pathological%0Alymph%20node%20segmentation%20in%20the%20mediastinum.%20As%20lymph%20node%20annotations%20are%0Aexpensive%2C%20the%20challenge%20was%20formed%20as%20a%20weakly%20supervised%20learning%20task%2C%20where%0Aonly%20a%20subset%20of%20all%20lymph%20nodes%20in%20the%20training%20set%20have%20been%20annotated.%20For%0Athe%20challenge%20submission%2C%20multiple%20methods%20for%20training%20on%20these%20weakly%0Asupervised%20data%20were%20explored%2C%20including%20noisy%20label%20training%2C%20loss%20masking%20of%0Aunlabeled%20data%2C%20and%20an%20approach%20that%20integrated%20the%20TotalSegmentator%20toolbox%20as%0Aa%20form%20of%20pseudo%20labeling%20in%20order%20to%20reduce%20the%20number%20of%20unknown%20voxels.%0AFurthermore%2C%20multiple%20public%20TCIA%20datasets%20were%20incorporated%20into%20the%20training%0Ato%20improve%20the%20performance%20of%20the%20deep%20learning%20model.%20Our%20submitted%20model%0Aachieved%20a%20Dice%20score%20of%200.628%20and%20an%20average%20symmetric%20surface%20distance%20of%0A5.8~mm%20on%20the%20challenge%20test%20set.%20With%20our%20submitted%20model%2C%20we%20accomplished%0Athird%20rank%20in%20the%20MICCAI2023%20LNQ%20challenge.%20A%20finding%20of%20our%20analysis%20was%20that%0Athe%20integration%20of%20all%20visible%2C%20including%20non-pathological%2C%20lymph%20nodes%0Aimproved%20the%20overall%20segmentation%20performance%20on%20pathological%20lymph%20nodes%20of%0Athe%20test%20set.%20Furthermore%2C%20segmentation%20models%20trained%20only%20on%20clinically%0Aenlarged%20lymph%20nodes%2C%20as%20given%20in%20the%20challenge%20scenario%2C%20could%20not%20generalize%0Ato%20smaller%20pathological%20lymph%20nodes.%20The%20code%20and%20model%20for%20the%20challenge%0Asubmission%20are%20available%20at%0A%5Curl%7Bhttps%3A//gitlab.lrz.de/compai/MediastinalLymphNodeSegmentation%7D.%0A&entry.1838667208=http%3A//arxiv.org/abs/2406.14365v1&entry.124074799=Read"},
{"title": "Determinantal Point Process as an alternative to NMS", "author": "Samik Some and Mithun Das Gupta and Vinay P. Namboodiri", "abstract": "  We present a determinantal point process (DPP) inspired alternative to\nnon-maximum suppression (NMS) which has become an integral step in all\nstate-of-the-art object detection frameworks. DPPs have been shown to encourage\ndiversity in subset selection problems. We pose NMS as a subset selection\nproblem and posit that directly incorporating DPP like framework can improve\nthe overall performance of the object detection system. We propose an\noptimization problem which takes the same inputs as NMS, but introduces a novel\nsub-modularity based diverse subset selection functional. Our results strongly\nindicate that the modifications proposed in this paper can provide consistent\nimprovements to state-of-the-art object detection pipelines.\n", "link": "http://arxiv.org/abs/2008.11451v2", "date": "2024-06-20", "relevancy": 2.0355, "topK": [{"title": "DDM-NET: End-to-end learning of keypoint feature Detection, Description and Matching for 3D localization", "link": "https://arxiv.org/abs/2212.04575", "similarity": 0.538}, {"title": "SDXS: Real-Time One-Step Latent Diffusion Models with Image Conditions", "link": "http://arxiv.org/abs/2403.16627v1", "similarity": 0.5059}, {"title": "Dynamic Gaussians Mesh: Consistent Mesh Reconstruction from Monocular\n  Videos", "link": "http://arxiv.org/abs/2404.12379v2", "similarity": 0.4809}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Determinantal%20Point%20Process%20as%20an%20alternative%20to%20NMS&body=Title%3A%20Determinantal%20Point%20Process%20as%20an%20alternative%20to%20NMS%0AAuthor%3A%20Samik%20Some%20and%20Mithun%20Das%20Gupta%20and%20Vinay%20P.%20Namboodiri%0AAbstract%3A%20%20%20We%20present%20a%20determinantal%20point%20process%20%28DPP%29%20inspired%20alternative%20to%0Anon-maximum%20suppression%20%28NMS%29%20which%20has%20become%20an%20integral%20step%20in%20all%0Astate-of-the-art%20object%20detection%20frameworks.%20DPPs%20have%20been%20shown%20to%20encourage%0Adiversity%20in%20subset%20selection%20problems.%20We%20pose%20NMS%20as%20a%20subset%20selection%0Aproblem%20and%20posit%20that%20directly%20incorporating%20DPP%20like%20framework%20can%20improve%0Athe%20overall%20performance%20of%20the%20object%20detection%20system.%20We%20propose%20an%0Aoptimization%20problem%20which%20takes%20the%20same%20inputs%20as%20NMS%2C%20but%20introduces%20a%20novel%0Asub-modularity%20based%20diverse%20subset%20selection%20functional.%20Our%20results%20strongly%0Aindicate%20that%20the%20modifications%20proposed%20in%20this%20paper%20can%20provide%20consistent%0Aimprovements%20to%20state-of-the-art%20object%20detection%20pipelines.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2008.11451v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DDeterminantal%2520Point%2520Process%2520as%2520an%2520alternative%2520to%2520NMS%26entry.906535625%3DSamik%2520Some%2520and%2520Mithun%2520Das%2520Gupta%2520and%2520Vinay%2520P.%2520Namboodiri%26entry.1292438233%3D%2520%2520We%2520present%2520a%2520determinantal%2520point%2520process%2520%2528DPP%2529%2520inspired%2520alternative%2520to%250Anon-maximum%2520suppression%2520%2528NMS%2529%2520which%2520has%2520become%2520an%2520integral%2520step%2520in%2520all%250Astate-of-the-art%2520object%2520detection%2520frameworks.%2520DPPs%2520have%2520been%2520shown%2520to%2520encourage%250Adiversity%2520in%2520subset%2520selection%2520problems.%2520We%2520pose%2520NMS%2520as%2520a%2520subset%2520selection%250Aproblem%2520and%2520posit%2520that%2520directly%2520incorporating%2520DPP%2520like%2520framework%2520can%2520improve%250Athe%2520overall%2520performance%2520of%2520the%2520object%2520detection%2520system.%2520We%2520propose%2520an%250Aoptimization%2520problem%2520which%2520takes%2520the%2520same%2520inputs%2520as%2520NMS%252C%2520but%2520introduces%2520a%2520novel%250Asub-modularity%2520based%2520diverse%2520subset%2520selection%2520functional.%2520Our%2520results%2520strongly%250Aindicate%2520that%2520the%2520modifications%2520proposed%2520in%2520this%2520paper%2520can%2520provide%2520consistent%250Aimprovements%2520to%2520state-of-the-art%2520object%2520detection%2520pipelines.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2008.11451v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Determinantal%20Point%20Process%20as%20an%20alternative%20to%20NMS&entry.906535625=Samik%20Some%20and%20Mithun%20Das%20Gupta%20and%20Vinay%20P.%20Namboodiri&entry.1292438233=%20%20We%20present%20a%20determinantal%20point%20process%20%28DPP%29%20inspired%20alternative%20to%0Anon-maximum%20suppression%20%28NMS%29%20which%20has%20become%20an%20integral%20step%20in%20all%0Astate-of-the-art%20object%20detection%20frameworks.%20DPPs%20have%20been%20shown%20to%20encourage%0Adiversity%20in%20subset%20selection%20problems.%20We%20pose%20NMS%20as%20a%20subset%20selection%0Aproblem%20and%20posit%20that%20directly%20incorporating%20DPP%20like%20framework%20can%20improve%0Athe%20overall%20performance%20of%20the%20object%20detection%20system.%20We%20propose%20an%0Aoptimization%20problem%20which%20takes%20the%20same%20inputs%20as%20NMS%2C%20but%20introduces%20a%20novel%0Asub-modularity%20based%20diverse%20subset%20selection%20functional.%20Our%20results%20strongly%0Aindicate%20that%20the%20modifications%20proposed%20in%20this%20paper%20can%20provide%20consistent%0Aimprovements%20to%20state-of-the-art%20object%20detection%20pipelines.%0A&entry.1838667208=http%3A//arxiv.org/abs/2008.11451v2&entry.124074799=Read"},
{"title": "Low-Rank Quantization-Aware Training for LLMs", "author": "Yelysei Bondarenko and Riccardo Del Chiaro and Markus Nagel", "abstract": "  Large language models (LLMs) are omnipresent, however their practical\ndeployment is challenging due to their ever increasing computational and memory\ndemands. Quantization is one of the most effective ways to make them more\ncompute and memory efficient. Quantization-aware training (QAT) methods,\ngenerally produce the best quantized performance, however it comes at the cost\nof potentially long training time and excessive memory usage, making it\nimpractical when applying for LLMs. Inspired by parameter-efficient fine-tuning\n(PEFT) and low-rank adaptation (LoRA) literature, we propose LR-QAT -- a\nlightweight and memory-efficient QAT algorithm for LLMs. LR-QAT employs several\ncomponents to save memory without sacrificing predictive performance: (a)\nlow-rank auxiliary weights that are aware of the quantization grid; (b) a\ndowncasting operator using fixed-point or double-packed integers and (c)\ncheckpointing. Unlike most related work, our method (i) is inference-efficient,\nleading to no additional overhead compared to traditional PTQ; (ii) can be seen\nas a general extended pretraining framework, meaning that the resulting model\ncan still be utilized for any downstream task afterwards; (iii) can be applied\nacross a wide range of quantization settings, such as different choices\nquantization granularity, activation quantization, and seamlessly combined with\nmany PTQ techniques. We apply LR-QAT to LLaMA-2/3 and Mistral model families\nand validate its effectiveness on several downstream tasks. Our method\noutperforms common post-training quantization (PTQ) approaches and reaches the\nsame model performance as full-model QAT at the fraction of its memory usage.\nSpecifically, we can train a 7B LLM on a single consumer grade GPU with 24GB of\nmemory.\n", "link": "http://arxiv.org/abs/2406.06385v2", "date": "2024-06-20", "relevancy": 2.0271, "topK": [{"title": "ToDo: Token Downsampling for Efficient Generation of High-Resolution\n  Images", "link": "http://arxiv.org/abs/2402.13573v2", "similarity": 0.5184}, {"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.5056}, {"title": "Breadcrumbs: Adversarial class-balanced sampling for long-tailed recognition", "link": "https://link.springer.com/chapter/10.1007/978-3-031-20053-3_37", "similarity": 0.4956}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Low-Rank%20Quantization-Aware%20Training%20for%20LLMs&body=Title%3A%20Low-Rank%20Quantization-Aware%20Training%20for%20LLMs%0AAuthor%3A%20Yelysei%20Bondarenko%20and%20Riccardo%20Del%20Chiaro%20and%20Markus%20Nagel%0AAbstract%3A%20%20%20Large%20language%20models%20%28LLMs%29%20are%20omnipresent%2C%20however%20their%20practical%0Adeployment%20is%20challenging%20due%20to%20their%20ever%20increasing%20computational%20and%20memory%0Ademands.%20Quantization%20is%20one%20of%20the%20most%20effective%20ways%20to%20make%20them%20more%0Acompute%20and%20memory%20efficient.%20Quantization-aware%20training%20%28QAT%29%20methods%2C%0Agenerally%20produce%20the%20best%20quantized%20performance%2C%20however%20it%20comes%20at%20the%20cost%0Aof%20potentially%20long%20training%20time%20and%20excessive%20memory%20usage%2C%20making%20it%0Aimpractical%20when%20applying%20for%20LLMs.%20Inspired%20by%20parameter-efficient%20fine-tuning%0A%28PEFT%29%20and%20low-rank%20adaptation%20%28LoRA%29%20literature%2C%20we%20propose%20LR-QAT%20--%20a%0Alightweight%20and%20memory-efficient%20QAT%20algorithm%20for%20LLMs.%20LR-QAT%20employs%20several%0Acomponents%20to%20save%20memory%20without%20sacrificing%20predictive%20performance%3A%20%28a%29%0Alow-rank%20auxiliary%20weights%20that%20are%20aware%20of%20the%20quantization%20grid%3B%20%28b%29%20a%0Adowncasting%20operator%20using%20fixed-point%20or%20double-packed%20integers%20and%20%28c%29%0Acheckpointing.%20Unlike%20most%20related%20work%2C%20our%20method%20%28i%29%20is%20inference-efficient%2C%0Aleading%20to%20no%20additional%20overhead%20compared%20to%20traditional%20PTQ%3B%20%28ii%29%20can%20be%20seen%0Aas%20a%20general%20extended%20pretraining%20framework%2C%20meaning%20that%20the%20resulting%20model%0Acan%20still%20be%20utilized%20for%20any%20downstream%20task%20afterwards%3B%20%28iii%29%20can%20be%20applied%0Aacross%20a%20wide%20range%20of%20quantization%20settings%2C%20such%20as%20different%20choices%0Aquantization%20granularity%2C%20activation%20quantization%2C%20and%20seamlessly%20combined%20with%0Amany%20PTQ%20techniques.%20We%20apply%20LR-QAT%20to%20LLaMA-2/3%20and%20Mistral%20model%20families%0Aand%20validate%20its%20effectiveness%20on%20several%20downstream%20tasks.%20Our%20method%0Aoutperforms%20common%20post-training%20quantization%20%28PTQ%29%20approaches%20and%20reaches%20the%0Asame%20model%20performance%20as%20full-model%20QAT%20at%20the%20fraction%20of%20its%20memory%20usage.%0ASpecifically%2C%20we%20can%20train%20a%207B%20LLM%20on%20a%20single%20consumer%20grade%20GPU%20with%2024GB%20of%0Amemory.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2406.06385v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DLow-Rank%2520Quantization-Aware%2520Training%2520for%2520LLMs%26entry.906535625%3DYelysei%2520Bondarenko%2520and%2520Riccardo%2520Del%2520Chiaro%2520and%2520Markus%2520Nagel%26entry.1292438233%3D%2520%2520Large%2520language%2520models%2520%2528LLMs%2529%2520are%2520omnipresent%252C%2520however%2520their%2520practical%250Adeployment%2520is%2520challenging%2520due%2520to%2520their%2520ever%2520increasing%2520computational%2520and%2520memory%250Ademands.%2520Quantization%2520is%2520one%2520of%2520the%2520most%2520effective%2520ways%2520to%2520make%2520them%2520more%250Acompute%2520and%2520memory%2520efficient.%2520Quantization-aware%2520training%2520%2528QAT%2529%2520methods%252C%250Agenerally%2520produce%2520the%2520best%2520quantized%2520performance%252C%2520however%2520it%2520comes%2520at%2520the%2520cost%250Aof%2520potentially%2520long%2520training%2520time%2520and%2520excessive%2520memory%2520usage%252C%2520making%2520it%250Aimpractical%2520when%2520applying%2520for%2520LLMs.%2520Inspired%2520by%2520parameter-efficient%2520fine-tuning%250A%2528PEFT%2529%2520and%2520low-rank%2520adaptation%2520%2528LoRA%2529%2520literature%252C%2520we%2520propose%2520LR-QAT%2520--%2520a%250Alightweight%2520and%2520memory-efficient%2520QAT%2520algorithm%2520for%2520LLMs.%2520LR-QAT%2520employs%2520several%250Acomponents%2520to%2520save%2520memory%2520without%2520sacrificing%2520predictive%2520performance%253A%2520%2528a%2529%250Alow-rank%2520auxiliary%2520weights%2520that%2520are%2520aware%2520of%2520the%2520quantization%2520grid%253B%2520%2528b%2529%2520a%250Adowncasting%2520operator%2520using%2520fixed-point%2520or%2520double-packed%2520integers%2520and%2520%2528c%2529%250Acheckpointing.%2520Unlike%2520most%2520related%2520work%252C%2520our%2520method%2520%2528i%2529%2520is%2520inference-efficient%252C%250Aleading%2520to%2520no%2520additional%2520overhead%2520compared%2520to%2520traditional%2520PTQ%253B%2520%2528ii%2529%2520can%2520be%2520seen%250Aas%2520a%2520general%2520extended%2520pretraining%2520framework%252C%2520meaning%2520that%2520the%2520resulting%2520model%250Acan%2520still%2520be%2520utilized%2520for%2520any%2520downstream%2520task%2520afterwards%253B%2520%2528iii%2529%2520can%2520be%2520applied%250Aacross%2520a%2520wide%2520range%2520of%2520quantization%2520settings%252C%2520such%2520as%2520different%2520choices%250Aquantization%2520granularity%252C%2520activation%2520quantization%252C%2520and%2520seamlessly%2520combined%2520with%250Amany%2520PTQ%2520techniques.%2520We%2520apply%2520LR-QAT%2520to%2520LLaMA-2/3%2520and%2520Mistral%2520model%2520families%250Aand%2520validate%2520its%2520effectiveness%2520on%2520several%2520downstream%2520tasks.%2520Our%2520method%250Aoutperforms%2520common%2520post-training%2520quantization%2520%2528PTQ%2529%2520approaches%2520and%2520reaches%2520the%250Asame%2520model%2520performance%2520as%2520full-model%2520QAT%2520at%2520the%2520fraction%2520of%2520its%2520memory%2520usage.%250ASpecifically%252C%2520we%2520can%2520train%2520a%25207B%2520LLM%2520on%2520a%2520single%2520consumer%2520grade%2520GPU%2520with%252024GB%2520of%250Amemory.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2406.06385v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Low-Rank%20Quantization-Aware%20Training%20for%20LLMs&entry.906535625=Yelysei%20Bondarenko%20and%20Riccardo%20Del%20Chiaro%20and%20Markus%20Nagel&entry.1292438233=%20%20Large%20language%20models%20%28LLMs%29%20are%20omnipresent%2C%20however%20their%20practical%0Adeployment%20is%20challenging%20due%20to%20their%20ever%20increasing%20computational%20and%20memory%0Ademands.%20Quantization%20is%20one%20of%20the%20most%20effective%20ways%20to%20make%20them%20more%0Acompute%20and%20memory%20efficient.%20Quantization-aware%20training%20%28QAT%29%20methods%2C%0Agenerally%20produce%20the%20best%20quantized%20performance%2C%20however%20it%20comes%20at%20the%20cost%0Aof%20potentially%20long%20training%20time%20and%20excessive%20memory%20usage%2C%20making%20it%0Aimpractical%20when%20applying%20for%20LLMs.%20Inspired%20by%20parameter-efficient%20fine-tuning%0A%28PEFT%29%20and%20low-rank%20adaptation%20%28LoRA%29%20literature%2C%20we%20propose%20LR-QAT%20--%20a%0Alightweight%20and%20memory-efficient%20QAT%20algorithm%20for%20LLMs.%20LR-QAT%20employs%20several%0Acomponents%20to%20save%20memory%20without%20sacrificing%20predictive%20performance%3A%20%28a%29%0Alow-rank%20auxiliary%20weights%20that%20are%20aware%20of%20the%20quantization%20grid%3B%20%28b%29%20a%0Adowncasting%20operator%20using%20fixed-point%20or%20double-packed%20integers%20and%20%28c%29%0Acheckpointing.%20Unlike%20most%20related%20work%2C%20our%20method%20%28i%29%20is%20inference-efficient%2C%0Aleading%20to%20no%20additional%20overhead%20compared%20to%20traditional%20PTQ%3B%20%28ii%29%20can%20be%20seen%0Aas%20a%20general%20extended%20pretraining%20framework%2C%20meaning%20that%20the%20resulting%20model%0Acan%20still%20be%20utilized%20for%20any%20downstream%20task%20afterwards%3B%20%28iii%29%20can%20be%20applied%0Aacross%20a%20wide%20range%20of%20quantization%20settings%2C%20such%20as%20different%20choices%0Aquantization%20granularity%2C%20activation%20quantization%2C%20and%20seamlessly%20combined%20with%0Amany%20PTQ%20techniques.%20We%20apply%20LR-QAT%20to%20LLaMA-2/3%20and%20Mistral%20model%20families%0Aand%20validate%20its%20effectiveness%20on%20several%20downstream%20tasks.%20Our%20method%0Aoutperforms%20common%20post-training%20quantization%20%28PTQ%29%20approaches%20and%20reaches%20the%0Asame%20model%20performance%20as%20full-model%20QAT%20at%20the%20fraction%20of%20its%20memory%20usage.%0ASpecifically%2C%20we%20can%20train%20a%207B%20LLM%20on%20a%20single%20consumer%20grade%20GPU%20with%2024GB%20of%0Amemory.%0A&entry.1838667208=http%3A//arxiv.org/abs/2406.06385v2&entry.124074799=Read"},
{"title": "Timo: Towards Better Temporal Reasoning for Language Models", "author": "Zhaochen Su and Jun Zhang and Tong Zhu and Xiaoye Qu and Juntao Li and Min Zhang and Yu Cheng", "abstract": "  Reasoning about time is essential for Large Language Models (LLMs) to\nunderstand the world. Previous works focus on solving specific tasks, primarily\non time-sensitive question answering. While these methods have proven\neffective, they cannot generalize to a wider spectrum of temporal reasoning\ntasks. Therefore, we propose a crucial question: Can we build a universal\nframework to handle a variety of temporal reasoning tasks? To that end, we\nsystematically study 38 temporal reasoning tasks. Based on the observation that\n19 tasks are directly related to mathematics, we first leverage the available\nmathematical dataset to set a solid foundation for temporal reasoning. However,\nthe in-depth study indicates that focusing solely on mathematical enhancement\nfalls short of addressing pure temporal reasoning tasks. To mitigate this\nlimitation, we propose a simple but effective self-critic temporal optimization\nmethod to enhance the model's temporal reasoning capabilities without\nsacrificing general task abilities. Finally, we develop Timo, a model designed\nto excel in temporal reasoning at the 7B and 13B scales. Notably, Timo\noutperforms the counterpart LLMs by 10.0 and 7.6 in average accuracy scores and\nachieves the new state-of-the-art (SOTA) performance of comparable size.\nExtensive experiments further validate our framework's effectiveness and its\ngeneralization across diverse temporal tasks. The code is available at\nhttps://github.com/zhaochen0110/Timo.\n", "link": "http://arxiv.org/abs/2406.14192v1", "date": "2024-06-20", "relevancy": 2.024, "topK": [{"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5312}, {"title": "Weakly-guided self-supervised pretraining for temporal activity detection", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/25189", "similarity": 0.4994}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.4595}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Timo%3A%20Towards%20Better%20Temporal%20Reasoning%20for%20Language%20Models&body=Title%3A%20Timo%3A%20Towards%20Better%20Temporal%20Reasoning%20for%20Language%20Models%0AAuthor%3A%20Zhaochen%20Su%20and%20Jun%20Zhang%20and%20Tong%20Zhu%20and%20Xiaoye%20Qu%20and%20Juntao%20Li%20and%20Min%20Zhang%20and%20Yu%20Cheng%0AAbstract%3A%20%20%20Reasoning%20about%20time%20is%20essential%20for%20Large%20Language%20Models%20%28LLMs%29%20to%0Aunderstand%20the%20world.%20Previous%20works%20focus%20on%20solving%20specific%20tasks%2C%20primarily%0Aon%20time-sensitive%20question%20answering.%20While%20these%20methods%20have%20proven%0Aeffective%2C%20they%20cannot%20generalize%20to%20a%20wider%20spectrum%20of%20temporal%20reasoning%0Atasks.%20Therefore%2C%20we%20propose%20a%20crucial%20question%3A%20Can%20we%20build%20a%20universal%0Aframework%20to%20handle%20a%20variety%20of%20temporal%20reasoning%20tasks%3F%20To%20that%20end%2C%20we%0Asystematically%20study%2038%20temporal%20reasoning%20tasks.%20Based%20on%20the%20observation%20that%0A19%20tasks%20are%20directly%20related%20to%20mathematics%2C%20we%20first%20leverage%20the%20available%0Amathematical%20dataset%20to%20set%20a%20solid%20foundation%20for%20temporal%20reasoning.%20However%2C%0Athe%20in-depth%20study%20indicates%20that%20focusing%20solely%20on%20mathematical%20enhancement%0Afalls%20short%20of%20addressing%20pure%20temporal%20reasoning%20tasks.%20To%20mitigate%20this%0Alimitation%2C%20we%20propose%20a%20simple%20but%20effective%20self-critic%20temporal%20optimization%0Amethod%20to%20enhance%20the%20model%27s%20temporal%20reasoning%20capabilities%20without%0Asacrificing%20general%20task%20abilities.%20Finally%2C%20we%20develop%20Timo%2C%20a%20model%20designed%0Ato%20excel%20in%20temporal%20reasoning%20at%20the%207B%20and%2013B%20scales.%20Notably%2C%20Timo%0Aoutperforms%20the%20counterpart%20LLMs%20by%2010.0%20and%207.6%20in%20average%20accuracy%20scores%20and%0Aachieves%20the%20new%20state-of-the-art%20%28SOTA%29%20performance%20of%20comparable%20size.%0AExtensive%20experiments%20further%20validate%20our%20framework%27s%20effectiveness%20and%20its%0Ageneralization%20across%20diverse%20temporal%20tasks.%20The%20code%20is%20available%20at%0Ahttps%3A//github.com/zhaochen0110/Timo.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2406.14192v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DTimo%253A%2520Towards%2520Better%2520Temporal%2520Reasoning%2520for%2520Language%2520Models%26entry.906535625%3DZhaochen%2520Su%2520and%2520Jun%2520Zhang%2520and%2520Tong%2520Zhu%2520and%2520Xiaoye%2520Qu%2520and%2520Juntao%2520Li%2520and%2520Min%2520Zhang%2520and%2520Yu%2520Cheng%26entry.1292438233%3D%2520%2520Reasoning%2520about%2520time%2520is%2520essential%2520for%2520Large%2520Language%2520Models%2520%2528LLMs%2529%2520to%250Aunderstand%2520the%2520world.%2520Previous%2520works%2520focus%2520on%2520solving%2520specific%2520tasks%252C%2520primarily%250Aon%2520time-sensitive%2520question%2520answering.%2520While%2520these%2520methods%2520have%2520proven%250Aeffective%252C%2520they%2520cannot%2520generalize%2520to%2520a%2520wider%2520spectrum%2520of%2520temporal%2520reasoning%250Atasks.%2520Therefore%252C%2520we%2520propose%2520a%2520crucial%2520question%253A%2520Can%2520we%2520build%2520a%2520universal%250Aframework%2520to%2520handle%2520a%2520variety%2520of%2520temporal%2520reasoning%2520tasks%253F%2520To%2520that%2520end%252C%2520we%250Asystematically%2520study%252038%2520temporal%2520reasoning%2520tasks.%2520Based%2520on%2520the%2520observation%2520that%250A19%2520tasks%2520are%2520directly%2520related%2520to%2520mathematics%252C%2520we%2520first%2520leverage%2520the%2520available%250Amathematical%2520dataset%2520to%2520set%2520a%2520solid%2520foundation%2520for%2520temporal%2520reasoning.%2520However%252C%250Athe%2520in-depth%2520study%2520indicates%2520that%2520focusing%2520solely%2520on%2520mathematical%2520enhancement%250Afalls%2520short%2520of%2520addressing%2520pure%2520temporal%2520reasoning%2520tasks.%2520To%2520mitigate%2520this%250Alimitation%252C%2520we%2520propose%2520a%2520simple%2520but%2520effective%2520self-critic%2520temporal%2520optimization%250Amethod%2520to%2520enhance%2520the%2520model%2527s%2520temporal%2520reasoning%2520capabilities%2520without%250Asacrificing%2520general%2520task%2520abilities.%2520Finally%252C%2520we%2520develop%2520Timo%252C%2520a%2520model%2520designed%250Ato%2520excel%2520in%2520temporal%2520reasoning%2520at%2520the%25207B%2520and%252013B%2520scales.%2520Notably%252C%2520Timo%250Aoutperforms%2520the%2520counterpart%2520LLMs%2520by%252010.0%2520and%25207.6%2520in%2520average%2520accuracy%2520scores%2520and%250Aachieves%2520the%2520new%2520state-of-the-art%2520%2528SOTA%2529%2520performance%2520of%2520comparable%2520size.%250AExtensive%2520experiments%2520further%2520validate%2520our%2520framework%2527s%2520effectiveness%2520and%2520its%250Ageneralization%2520across%2520diverse%2520temporal%2520tasks.%2520The%2520code%2520is%2520available%2520at%250Ahttps%253A//github.com/zhaochen0110/Timo.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2406.14192v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Timo%3A%20Towards%20Better%20Temporal%20Reasoning%20for%20Language%20Models&entry.906535625=Zhaochen%20Su%20and%20Jun%20Zhang%20and%20Tong%20Zhu%20and%20Xiaoye%20Qu%20and%20Juntao%20Li%20and%20Min%20Zhang%20and%20Yu%20Cheng&entry.1292438233=%20%20Reasoning%20about%20time%20is%20essential%20for%20Large%20Language%20Models%20%28LLMs%29%20to%0Aunderstand%20the%20world.%20Previous%20works%20focus%20on%20solving%20specific%20tasks%2C%20primarily%0Aon%20time-sensitive%20question%20answering.%20While%20these%20methods%20have%20proven%0Aeffective%2C%20they%20cannot%20generalize%20to%20a%20wider%20spectrum%20of%20temporal%20reasoning%0Atasks.%20Therefore%2C%20we%20propose%20a%20crucial%20question%3A%20Can%20we%20build%20a%20universal%0Aframework%20to%20handle%20a%20variety%20of%20temporal%20reasoning%20tasks%3F%20To%20that%20end%2C%20we%0Asystematically%20study%2038%20temporal%20reasoning%20tasks.%20Based%20on%20the%20observation%20that%0A19%20tasks%20are%20directly%20related%20to%20mathematics%2C%20we%20first%20leverage%20the%20available%0Amathematical%20dataset%20to%20set%20a%20solid%20foundation%20for%20temporal%20reasoning.%20However%2C%0Athe%20in-depth%20study%20indicates%20that%20focusing%20solely%20on%20mathematical%20enhancement%0Afalls%20short%20of%20addressing%20pure%20temporal%20reasoning%20tasks.%20To%20mitigate%20this%0Alimitation%2C%20we%20propose%20a%20simple%20but%20effective%20self-critic%20temporal%20optimization%0Amethod%20to%20enhance%20the%20model%27s%20temporal%20reasoning%20capabilities%20without%0Asacrificing%20general%20task%20abilities.%20Finally%2C%20we%20develop%20Timo%2C%20a%20model%20designed%0Ato%20excel%20in%20temporal%20reasoning%20at%20the%207B%20and%2013B%20scales.%20Notably%2C%20Timo%0Aoutperforms%20the%20counterpart%20LLMs%20by%2010.0%20and%207.6%20in%20average%20accuracy%20scores%20and%0Aachieves%20the%20new%20state-of-the-art%20%28SOTA%29%20performance%20of%20comparable%20size.%0AExtensive%20experiments%20further%20validate%20our%20framework%27s%20effectiveness%20and%20its%0Ageneralization%20across%20diverse%20temporal%20tasks.%20The%20code%20is%20available%20at%0Ahttps%3A//github.com/zhaochen0110/Timo.%0A&entry.1838667208=http%3A//arxiv.org/abs/2406.14192v1&entry.124074799=Read"},
{"title": "SeCoKD: Aligning Large Language Models for In-Context Learning with\n  Fewer Shots", "author": "Weixing Wang and Haojin Yang and Christoph Meinel", "abstract": "  Previous studies have shown that demonstrations can significantly help Large\nLanguage Models (LLMs ) perform better on the given tasks. However, this\nso-called In-Context Learning ( ICL ) ability is very sensitive to the\npresenting context, and often dozens of demonstrations are needed. In this\nwork, we investigate if we can reduce the shot number while still maintaining a\ncompetitive performance. We present SeCoKD, a self-Knowledge Distillation ( KD\n) training framework that aligns the student model with a heavily prompted\nvariation, thereby increasing the utilization of a single demonstration. We\nexperiment with the SeCoKD across three LLMs and six benchmarks focusing mainly\non reasoning tasks. Results show that our method outperforms the base model and\nSupervised Fine-tuning ( SFT ), especially in zero-shot and one-shot settings\nby 30% and 10%, respectively. Moreover, SeCoKD brings little negative artifacts\nwhen evaluated on new tasks, which is more robust than Supervised Fine-tuning.\n", "link": "http://arxiv.org/abs/2406.14208v1", "date": "2024-06-20", "relevancy": 2.015, "topK": [{"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5075}, {"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.5062}, {"title": "Self-supervised Photographic Image Layout Representation Learning", "link": "http://arxiv.org/abs/2403.03740v1", "similarity": 0.499}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20SeCoKD%3A%20Aligning%20Large%20Language%20Models%20for%20In-Context%20Learning%20with%0A%20%20Fewer%20Shots&body=Title%3A%20SeCoKD%3A%20Aligning%20Large%20Language%20Models%20for%20In-Context%20Learning%20with%0A%20%20Fewer%20Shots%0AAuthor%3A%20Weixing%20Wang%20and%20Haojin%20Yang%20and%20Christoph%20Meinel%0AAbstract%3A%20%20%20Previous%20studies%20have%20shown%20that%20demonstrations%20can%20significantly%20help%20Large%0ALanguage%20Models%20%28LLMs%20%29%20perform%20better%20on%20the%20given%20tasks.%20However%2C%20this%0Aso-called%20In-Context%20Learning%20%28%20ICL%20%29%20ability%20is%20very%20sensitive%20to%20the%0Apresenting%20context%2C%20and%20often%20dozens%20of%20demonstrations%20are%20needed.%20In%20this%0Awork%2C%20we%20investigate%20if%20we%20can%20reduce%20the%20shot%20number%20while%20still%20maintaining%20a%0Acompetitive%20performance.%20We%20present%20SeCoKD%2C%20a%20self-Knowledge%20Distillation%20%28%20KD%0A%29%20training%20framework%20that%20aligns%20the%20student%20model%20with%20a%20heavily%20prompted%0Avariation%2C%20thereby%20increasing%20the%20utilization%20of%20a%20single%20demonstration.%20We%0Aexperiment%20with%20the%20SeCoKD%20across%20three%20LLMs%20and%20six%20benchmarks%20focusing%20mainly%0Aon%20reasoning%20tasks.%20Results%20show%20that%20our%20method%20outperforms%20the%20base%20model%20and%0ASupervised%20Fine-tuning%20%28%20SFT%20%29%2C%20especially%20in%20zero-shot%20and%20one-shot%20settings%0Aby%2030%25%20and%2010%25%2C%20respectively.%20Moreover%2C%20SeCoKD%20brings%20little%20negative%20artifacts%0Awhen%20evaluated%20on%20new%20tasks%2C%20which%20is%20more%20robust%20than%20Supervised%20Fine-tuning.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2406.14208v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DSeCoKD%253A%2520Aligning%2520Large%2520Language%2520Models%2520for%2520In-Context%2520Learning%2520with%250A%2520%2520Fewer%2520Shots%26entry.906535625%3DWeixing%2520Wang%2520and%2520Haojin%2520Yang%2520and%2520Christoph%2520Meinel%26entry.1292438233%3D%2520%2520Previous%2520studies%2520have%2520shown%2520that%2520demonstrations%2520can%2520significantly%2520help%2520Large%250ALanguage%2520Models%2520%2528LLMs%2520%2529%2520perform%2520better%2520on%2520the%2520given%2520tasks.%2520However%252C%2520this%250Aso-called%2520In-Context%2520Learning%2520%2528%2520ICL%2520%2529%2520ability%2520is%2520very%2520sensitive%2520to%2520the%250Apresenting%2520context%252C%2520and%2520often%2520dozens%2520of%2520demonstrations%2520are%2520needed.%2520In%2520this%250Awork%252C%2520we%2520investigate%2520if%2520we%2520can%2520reduce%2520the%2520shot%2520number%2520while%2520still%2520maintaining%2520a%250Acompetitive%2520performance.%2520We%2520present%2520SeCoKD%252C%2520a%2520self-Knowledge%2520Distillation%2520%2528%2520KD%250A%2529%2520training%2520framework%2520that%2520aligns%2520the%2520student%2520model%2520with%2520a%2520heavily%2520prompted%250Avariation%252C%2520thereby%2520increasing%2520the%2520utilization%2520of%2520a%2520single%2520demonstration.%2520We%250Aexperiment%2520with%2520the%2520SeCoKD%2520across%2520three%2520LLMs%2520and%2520six%2520benchmarks%2520focusing%2520mainly%250Aon%2520reasoning%2520tasks.%2520Results%2520show%2520that%2520our%2520method%2520outperforms%2520the%2520base%2520model%2520and%250ASupervised%2520Fine-tuning%2520%2528%2520SFT%2520%2529%252C%2520especially%2520in%2520zero-shot%2520and%2520one-shot%2520settings%250Aby%252030%2525%2520and%252010%2525%252C%2520respectively.%2520Moreover%252C%2520SeCoKD%2520brings%2520little%2520negative%2520artifacts%250Awhen%2520evaluated%2520on%2520new%2520tasks%252C%2520which%2520is%2520more%2520robust%2520than%2520Supervised%2520Fine-tuning.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2406.14208v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=SeCoKD%3A%20Aligning%20Large%20Language%20Models%20for%20In-Context%20Learning%20with%0A%20%20Fewer%20Shots&entry.906535625=Weixing%20Wang%20and%20Haojin%20Yang%20and%20Christoph%20Meinel&entry.1292438233=%20%20Previous%20studies%20have%20shown%20that%20demonstrations%20can%20significantly%20help%20Large%0ALanguage%20Models%20%28LLMs%20%29%20perform%20better%20on%20the%20given%20tasks.%20However%2C%20this%0Aso-called%20In-Context%20Learning%20%28%20ICL%20%29%20ability%20is%20very%20sensitive%20to%20the%0Apresenting%20context%2C%20and%20often%20dozens%20of%20demonstrations%20are%20needed.%20In%20this%0Awork%2C%20we%20investigate%20if%20we%20can%20reduce%20the%20shot%20number%20while%20still%20maintaining%20a%0Acompetitive%20performance.%20We%20present%20SeCoKD%2C%20a%20self-Knowledge%20Distillation%20%28%20KD%0A%29%20training%20framework%20that%20aligns%20the%20student%20model%20with%20a%20heavily%20prompted%0Avariation%2C%20thereby%20increasing%20the%20utilization%20of%20a%20single%20demonstration.%20We%0Aexperiment%20with%20the%20SeCoKD%20across%20three%20LLMs%20and%20six%20benchmarks%20focusing%20mainly%0Aon%20reasoning%20tasks.%20Results%20show%20that%20our%20method%20outperforms%20the%20base%20model%20and%0ASupervised%20Fine-tuning%20%28%20SFT%20%29%2C%20especially%20in%20zero-shot%20and%20one-shot%20settings%0Aby%2030%25%20and%2010%25%2C%20respectively.%20Moreover%2C%20SeCoKD%20brings%20little%20negative%20artifacts%0Awhen%20evaluated%20on%20new%20tasks%2C%20which%20is%20more%20robust%20than%20Supervised%20Fine-tuning.%0A&entry.1838667208=http%3A//arxiv.org/abs/2406.14208v1&entry.124074799=Read"},
{"title": "WEATHER-5K: A Large-scale Global Station Weather Dataset Towards\n  Comprehensive Time-series Forecasting Benchmark", "author": "Tao Han and Song Guo and Zhenghao Chen and Wanghan Xu and Lei Bai", "abstract": "  Global Station Weather Forecasting (GSWF) is crucial for various sectors,\nincluding aviation, agriculture, energy, and disaster preparedness. Recent\nadvancements in deep learning have significantly improved the accuracy of\nweather predictions by optimizing models based on public meteorological data.\nHowever, existing public datasets for GSWF optimization and benchmarking still\nsuffer from significant limitations, such as small sizes, limited temporal\ncoverage, and a lack of comprehensive variables. These shortcomings prevent\nthem from effectively reflecting the benchmarks of current forecasting methods\nand fail to support the real needs of operational weather forecasting. To\naddress these challenges, we present the WEATHER-5K dataset. This dataset\ncomprises a comprehensive collection of data from 5,672 weather stations\nworldwide, spanning a 10-year period with one-hour intervals. It includes\nmultiple crucial weather elements, providing a more reliable and interpretable\nresource for forecasting. Furthermore, our WEATHER-5K dataset can serve as a\nbenchmark for comprehensively evaluating existing well-known forecasting\nmodels, extending beyond GSWF methods to support future time-series research\nchallenges and opportunities. The dataset and benchmark implementation are\npublicly available at: https://github.com/taohan10200/WEATHER-5K.\n", "link": "http://arxiv.org/abs/2406.14399v1", "date": "2024-06-20", "relevancy": 2.007, "topK": [{"title": "RTG-SLAM: Real-time 3D Reconstruction at Scale using Gaussian Splatting", "link": "http://arxiv.org/abs/2404.19706v1", "similarity": 0.4025}, {"title": "Weakly-guided self-supervised pretraining for temporal activity detection", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/25189", "similarity": 0.4014}, {"title": "Self-supervised Photographic Image Layout Representation Learning", "link": "http://arxiv.org/abs/2403.03740v1", "similarity": 0.4003}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20WEATHER-5K%3A%20A%20Large-scale%20Global%20Station%20Weather%20Dataset%20Towards%0A%20%20Comprehensive%20Time-series%20Forecasting%20Benchmark&body=Title%3A%20WEATHER-5K%3A%20A%20Large-scale%20Global%20Station%20Weather%20Dataset%20Towards%0A%20%20Comprehensive%20Time-series%20Forecasting%20Benchmark%0AAuthor%3A%20Tao%20Han%20and%20Song%20Guo%20and%20Zhenghao%20Chen%20and%20Wanghan%20Xu%20and%20Lei%20Bai%0AAbstract%3A%20%20%20Global%20Station%20Weather%20Forecasting%20%28GSWF%29%20is%20crucial%20for%20various%20sectors%2C%0Aincluding%20aviation%2C%20agriculture%2C%20energy%2C%20and%20disaster%20preparedness.%20Recent%0Aadvancements%20in%20deep%20learning%20have%20significantly%20improved%20the%20accuracy%20of%0Aweather%20predictions%20by%20optimizing%20models%20based%20on%20public%20meteorological%20data.%0AHowever%2C%20existing%20public%20datasets%20for%20GSWF%20optimization%20and%20benchmarking%20still%0Asuffer%20from%20significant%20limitations%2C%20such%20as%20small%20sizes%2C%20limited%20temporal%0Acoverage%2C%20and%20a%20lack%20of%20comprehensive%20variables.%20These%20shortcomings%20prevent%0Athem%20from%20effectively%20reflecting%20the%20benchmarks%20of%20current%20forecasting%20methods%0Aand%20fail%20to%20support%20the%20real%20needs%20of%20operational%20weather%20forecasting.%20To%0Aaddress%20these%20challenges%2C%20we%20present%20the%20WEATHER-5K%20dataset.%20This%20dataset%0Acomprises%20a%20comprehensive%20collection%20of%20data%20from%205%2C672%20weather%20stations%0Aworldwide%2C%20spanning%20a%2010-year%20period%20with%20one-hour%20intervals.%20It%20includes%0Amultiple%20crucial%20weather%20elements%2C%20providing%20a%20more%20reliable%20and%20interpretable%0Aresource%20for%20forecasting.%20Furthermore%2C%20our%20WEATHER-5K%20dataset%20can%20serve%20as%20a%0Abenchmark%20for%20comprehensively%20evaluating%20existing%20well-known%20forecasting%0Amodels%2C%20extending%20beyond%20GSWF%20methods%20to%20support%20future%20time-series%20research%0Achallenges%20and%20opportunities.%20The%20dataset%20and%20benchmark%20implementation%20are%0Apublicly%20available%20at%3A%20https%3A//github.com/taohan10200/WEATHER-5K.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2406.14399v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DWEATHER-5K%253A%2520A%2520Large-scale%2520Global%2520Station%2520Weather%2520Dataset%2520Towards%250A%2520%2520Comprehensive%2520Time-series%2520Forecasting%2520Benchmark%26entry.906535625%3DTao%2520Han%2520and%2520Song%2520Guo%2520and%2520Zhenghao%2520Chen%2520and%2520Wanghan%2520Xu%2520and%2520Lei%2520Bai%26entry.1292438233%3D%2520%2520Global%2520Station%2520Weather%2520Forecasting%2520%2528GSWF%2529%2520is%2520crucial%2520for%2520various%2520sectors%252C%250Aincluding%2520aviation%252C%2520agriculture%252C%2520energy%252C%2520and%2520disaster%2520preparedness.%2520Recent%250Aadvancements%2520in%2520deep%2520learning%2520have%2520significantly%2520improved%2520the%2520accuracy%2520of%250Aweather%2520predictions%2520by%2520optimizing%2520models%2520based%2520on%2520public%2520meteorological%2520data.%250AHowever%252C%2520existing%2520public%2520datasets%2520for%2520GSWF%2520optimization%2520and%2520benchmarking%2520still%250Asuffer%2520from%2520significant%2520limitations%252C%2520such%2520as%2520small%2520sizes%252C%2520limited%2520temporal%250Acoverage%252C%2520and%2520a%2520lack%2520of%2520comprehensive%2520variables.%2520These%2520shortcomings%2520prevent%250Athem%2520from%2520effectively%2520reflecting%2520the%2520benchmarks%2520of%2520current%2520forecasting%2520methods%250Aand%2520fail%2520to%2520support%2520the%2520real%2520needs%2520of%2520operational%2520weather%2520forecasting.%2520To%250Aaddress%2520these%2520challenges%252C%2520we%2520present%2520the%2520WEATHER-5K%2520dataset.%2520This%2520dataset%250Acomprises%2520a%2520comprehensive%2520collection%2520of%2520data%2520from%25205%252C672%2520weather%2520stations%250Aworldwide%252C%2520spanning%2520a%252010-year%2520period%2520with%2520one-hour%2520intervals.%2520It%2520includes%250Amultiple%2520crucial%2520weather%2520elements%252C%2520providing%2520a%2520more%2520reliable%2520and%2520interpretable%250Aresource%2520for%2520forecasting.%2520Furthermore%252C%2520our%2520WEATHER-5K%2520dataset%2520can%2520serve%2520as%2520a%250Abenchmark%2520for%2520comprehensively%2520evaluating%2520existing%2520well-known%2520forecasting%250Amodels%252C%2520extending%2520beyond%2520GSWF%2520methods%2520to%2520support%2520future%2520time-series%2520research%250Achallenges%2520and%2520opportunities.%2520The%2520dataset%2520and%2520benchmark%2520implementation%2520are%250Apublicly%2520available%2520at%253A%2520https%253A//github.com/taohan10200/WEATHER-5K.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2406.14399v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=WEATHER-5K%3A%20A%20Large-scale%20Global%20Station%20Weather%20Dataset%20Towards%0A%20%20Comprehensive%20Time-series%20Forecasting%20Benchmark&entry.906535625=Tao%20Han%20and%20Song%20Guo%20and%20Zhenghao%20Chen%20and%20Wanghan%20Xu%20and%20Lei%20Bai&entry.1292438233=%20%20Global%20Station%20Weather%20Forecasting%20%28GSWF%29%20is%20crucial%20for%20various%20sectors%2C%0Aincluding%20aviation%2C%20agriculture%2C%20energy%2C%20and%20disaster%20preparedness.%20Recent%0Aadvancements%20in%20deep%20learning%20have%20significantly%20improved%20the%20accuracy%20of%0Aweather%20predictions%20by%20optimizing%20models%20based%20on%20public%20meteorological%20data.%0AHowever%2C%20existing%20public%20datasets%20for%20GSWF%20optimization%20and%20benchmarking%20still%0Asuffer%20from%20significant%20limitations%2C%20such%20as%20small%20sizes%2C%20limited%20temporal%0Acoverage%2C%20and%20a%20lack%20of%20comprehensive%20variables.%20These%20shortcomings%20prevent%0Athem%20from%20effectively%20reflecting%20the%20benchmarks%20of%20current%20forecasting%20methods%0Aand%20fail%20to%20support%20the%20real%20needs%20of%20operational%20weather%20forecasting.%20To%0Aaddress%20these%20challenges%2C%20we%20present%20the%20WEATHER-5K%20dataset.%20This%20dataset%0Acomprises%20a%20comprehensive%20collection%20of%20data%20from%205%2C672%20weather%20stations%0Aworldwide%2C%20spanning%20a%2010-year%20period%20with%20one-hour%20intervals.%20It%20includes%0Amultiple%20crucial%20weather%20elements%2C%20providing%20a%20more%20reliable%20and%20interpretable%0Aresource%20for%20forecasting.%20Furthermore%2C%20our%20WEATHER-5K%20dataset%20can%20serve%20as%20a%0Abenchmark%20for%20comprehensively%20evaluating%20existing%20well-known%20forecasting%0Amodels%2C%20extending%20beyond%20GSWF%20methods%20to%20support%20future%20time-series%20research%0Achallenges%20and%20opportunities.%20The%20dataset%20and%20benchmark%20implementation%20are%0Apublicly%20available%20at%3A%20https%3A//github.com/taohan10200/WEATHER-5K.%0A&entry.1838667208=http%3A//arxiv.org/abs/2406.14399v1&entry.124074799=Read"},
{"title": "Certified Robust Accuracy of Neural Networks Are Bounded due to Bayes\n  Errors", "author": "Ruihan Zhang and Jun Sun", "abstract": "  Adversarial examples pose a security threat to many critical systems built on\nneural networks. While certified training improves robustness, it also\ndecreases accuracy noticeably. Despite various proposals for addressing this\nissue, the significant accuracy drop remains. More importantly, it is not clear\nwhether there is a certain fundamental limit on achieving robustness whilst\nmaintaining accuracy. In this work, we offer a novel perspective based on Bayes\nerrors. By adopting Bayes error to robustness analysis, we investigate the\nlimit of certified robust accuracy, taking into account data distribution\nuncertainties. We first show that the accuracy inevitably decreases in the\npursuit of robustness due to changed Bayes error in the altered data\ndistribution. Subsequently, we establish an upper bound for certified robust\naccuracy, considering the distribution of individual classes and their\nboundaries. Our theoretical results are empirically evaluated on real-world\ndatasets and are shown to be consistent with the limited success of existing\ncertified training results, e.g., for CIFAR10, our analysis results in an upper\nbound (of certified robust accuracy) of 67.49\\%, meanwhile existing approaches\nare only able to increase it from 53.89\\% in 2017 to 62.84\\% in 2023.\n", "link": "http://arxiv.org/abs/2405.11547v2", "date": "2024-06-20", "relevancy": 2.0064, "topK": [{"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.5053}, {"title": "Boosted dynamic neural networks", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/26302", "similarity": 0.499}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.499}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Certified%20Robust%20Accuracy%20of%20Neural%20Networks%20Are%20Bounded%20due%20to%20Bayes%0A%20%20Errors&body=Title%3A%20Certified%20Robust%20Accuracy%20of%20Neural%20Networks%20Are%20Bounded%20due%20to%20Bayes%0A%20%20Errors%0AAuthor%3A%20Ruihan%20Zhang%20and%20Jun%20Sun%0AAbstract%3A%20%20%20Adversarial%20examples%20pose%20a%20security%20threat%20to%20many%20critical%20systems%20built%20on%0Aneural%20networks.%20While%20certified%20training%20improves%20robustness%2C%20it%20also%0Adecreases%20accuracy%20noticeably.%20Despite%20various%20proposals%20for%20addressing%20this%0Aissue%2C%20the%20significant%20accuracy%20drop%20remains.%20More%20importantly%2C%20it%20is%20not%20clear%0Awhether%20there%20is%20a%20certain%20fundamental%20limit%20on%20achieving%20robustness%20whilst%0Amaintaining%20accuracy.%20In%20this%20work%2C%20we%20offer%20a%20novel%20perspective%20based%20on%20Bayes%0Aerrors.%20By%20adopting%20Bayes%20error%20to%20robustness%20analysis%2C%20we%20investigate%20the%0Alimit%20of%20certified%20robust%20accuracy%2C%20taking%20into%20account%20data%20distribution%0Auncertainties.%20We%20first%20show%20that%20the%20accuracy%20inevitably%20decreases%20in%20the%0Apursuit%20of%20robustness%20due%20to%20changed%20Bayes%20error%20in%20the%20altered%20data%0Adistribution.%20Subsequently%2C%20we%20establish%20an%20upper%20bound%20for%20certified%20robust%0Aaccuracy%2C%20considering%20the%20distribution%20of%20individual%20classes%20and%20their%0Aboundaries.%20Our%20theoretical%20results%20are%20empirically%20evaluated%20on%20real-world%0Adatasets%20and%20are%20shown%20to%20be%20consistent%20with%20the%20limited%20success%20of%20existing%0Acertified%20training%20results%2C%20e.g.%2C%20for%20CIFAR10%2C%20our%20analysis%20results%20in%20an%20upper%0Abound%20%28of%20certified%20robust%20accuracy%29%20of%2067.49%5C%25%2C%20meanwhile%20existing%20approaches%0Aare%20only%20able%20to%20increase%20it%20from%2053.89%5C%25%20in%202017%20to%2062.84%5C%25%20in%202023.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2405.11547v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DCertified%2520Robust%2520Accuracy%2520of%2520Neural%2520Networks%2520Are%2520Bounded%2520due%2520to%2520Bayes%250A%2520%2520Errors%26entry.906535625%3DRuihan%2520Zhang%2520and%2520Jun%2520Sun%26entry.1292438233%3D%2520%2520Adversarial%2520examples%2520pose%2520a%2520security%2520threat%2520to%2520many%2520critical%2520systems%2520built%2520on%250Aneural%2520networks.%2520While%2520certified%2520training%2520improves%2520robustness%252C%2520it%2520also%250Adecreases%2520accuracy%2520noticeably.%2520Despite%2520various%2520proposals%2520for%2520addressing%2520this%250Aissue%252C%2520the%2520significant%2520accuracy%2520drop%2520remains.%2520More%2520importantly%252C%2520it%2520is%2520not%2520clear%250Awhether%2520there%2520is%2520a%2520certain%2520fundamental%2520limit%2520on%2520achieving%2520robustness%2520whilst%250Amaintaining%2520accuracy.%2520In%2520this%2520work%252C%2520we%2520offer%2520a%2520novel%2520perspective%2520based%2520on%2520Bayes%250Aerrors.%2520By%2520adopting%2520Bayes%2520error%2520to%2520robustness%2520analysis%252C%2520we%2520investigate%2520the%250Alimit%2520of%2520certified%2520robust%2520accuracy%252C%2520taking%2520into%2520account%2520data%2520distribution%250Auncertainties.%2520We%2520first%2520show%2520that%2520the%2520accuracy%2520inevitably%2520decreases%2520in%2520the%250Apursuit%2520of%2520robustness%2520due%2520to%2520changed%2520Bayes%2520error%2520in%2520the%2520altered%2520data%250Adistribution.%2520Subsequently%252C%2520we%2520establish%2520an%2520upper%2520bound%2520for%2520certified%2520robust%250Aaccuracy%252C%2520considering%2520the%2520distribution%2520of%2520individual%2520classes%2520and%2520their%250Aboundaries.%2520Our%2520theoretical%2520results%2520are%2520empirically%2520evaluated%2520on%2520real-world%250Adatasets%2520and%2520are%2520shown%2520to%2520be%2520consistent%2520with%2520the%2520limited%2520success%2520of%2520existing%250Acertified%2520training%2520results%252C%2520e.g.%252C%2520for%2520CIFAR10%252C%2520our%2520analysis%2520results%2520in%2520an%2520upper%250Abound%2520%2528of%2520certified%2520robust%2520accuracy%2529%2520of%252067.49%255C%2525%252C%2520meanwhile%2520existing%2520approaches%250Aare%2520only%2520able%2520to%2520increase%2520it%2520from%252053.89%255C%2525%2520in%25202017%2520to%252062.84%255C%2525%2520in%25202023.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2405.11547v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Certified%20Robust%20Accuracy%20of%20Neural%20Networks%20Are%20Bounded%20due%20to%20Bayes%0A%20%20Errors&entry.906535625=Ruihan%20Zhang%20and%20Jun%20Sun&entry.1292438233=%20%20Adversarial%20examples%20pose%20a%20security%20threat%20to%20many%20critical%20systems%20built%20on%0Aneural%20networks.%20While%20certified%20training%20improves%20robustness%2C%20it%20also%0Adecreases%20accuracy%20noticeably.%20Despite%20various%20proposals%20for%20addressing%20this%0Aissue%2C%20the%20significant%20accuracy%20drop%20remains.%20More%20importantly%2C%20it%20is%20not%20clear%0Awhether%20there%20is%20a%20certain%20fundamental%20limit%20on%20achieving%20robustness%20whilst%0Amaintaining%20accuracy.%20In%20this%20work%2C%20we%20offer%20a%20novel%20perspective%20based%20on%20Bayes%0Aerrors.%20By%20adopting%20Bayes%20error%20to%20robustness%20analysis%2C%20we%20investigate%20the%0Alimit%20of%20certified%20robust%20accuracy%2C%20taking%20into%20account%20data%20distribution%0Auncertainties.%20We%20first%20show%20that%20the%20accuracy%20inevitably%20decreases%20in%20the%0Apursuit%20of%20robustness%20due%20to%20changed%20Bayes%20error%20in%20the%20altered%20data%0Adistribution.%20Subsequently%2C%20we%20establish%20an%20upper%20bound%20for%20certified%20robust%0Aaccuracy%2C%20considering%20the%20distribution%20of%20individual%20classes%20and%20their%0Aboundaries.%20Our%20theoretical%20results%20are%20empirically%20evaluated%20on%20real-world%0Adatasets%20and%20are%20shown%20to%20be%20consistent%20with%20the%20limited%20success%20of%20existing%0Acertified%20training%20results%2C%20e.g.%2C%20for%20CIFAR10%2C%20our%20analysis%20results%20in%20an%20upper%0Abound%20%28of%20certified%20robust%20accuracy%29%20of%2067.49%5C%25%2C%20meanwhile%20existing%20approaches%0Aare%20only%20able%20to%20increase%20it%20from%2053.89%5C%25%20in%202017%20to%2062.84%5C%25%20in%202023.%0A&entry.1838667208=http%3A//arxiv.org/abs/2405.11547v2&entry.124074799=Read"},
{"title": "LeYOLO, New Scalable and Efficient CNN Architecture for Object Detection", "author": "Lilian Hollard and Lucas Mohimont and Nathalie Gaveau and Luiz-Angelo Steffenel", "abstract": "  Computational efficiency in deep neural networks is critical for object\ndetection, especially as newer models prioritize speed over efficient\ncomputation (FLOP). This evolution has somewhat left behind embedded and\nmobile-oriented AI object detection applications. In this paper, we focus on\ndesign choices of neural network architectures for efficient object detection\ncomputation based on FLOP and propose several optimizations to enhance the\nefficiency of YOLO-based models.\n  Firstly, we introduce an efficient backbone scaling inspired by inverted\nbottlenecks and theoretical insights from the Information Bottleneck principle.\nSecondly, we present the Fast Pyramidal Architecture Network (FPAN), designed\nto facilitate fast multiscale feature sharing while reducing computational\nresources. Lastly, we propose a Decoupled Network-in-Network (DNiN) detection\nhead engineered to deliver rapid yet lightweight computations for\nclassification and regression tasks.\n  Building upon these optimizations and leveraging more efficient backbones,\nthis paper contributes to a new scaling paradigm for object detection and\nYOLO-centric models called LeYOLO. Our contribution consistently outperforms\nexisting models in various resource constraints, achieving unprecedented\naccuracy and flop ratio. Notably, LeYOLO-Small achieves a competitive mAP score\nof 38.2% on the COCOval with just 4.5 FLOP(G), representing a 42% reduction in\ncomputational load compared to the latest state-of-the-art YOLOv9-Tiny model\nwhile achieving similar accuracy. Our novel model family achieves a\nFLOP-to-accuracy ratio previously unattained, offering scalability that spans\nfrom ultra-low neural network configurations (< 1 GFLOP) to efficient yet\ndemanding object detection setups (> 4 GFLOPs) with 25.2, 31.3, 35.2, 38.2,\n39.3 and 41 mAP for 0.66, 1.47, 2.53, 4.51, 5.8 and 8.4 FLOP(G).\n", "link": "http://arxiv.org/abs/2406.14239v1", "date": "2024-06-20", "relevancy": 1.9966, "topK": [{"title": "Self-supervised Photographic Image Layout Representation Learning", "link": "http://arxiv.org/abs/2403.03740v1", "similarity": 0.5057}, {"title": "SDXS: Real-Time One-Step Latent Diffusion Models with Image Conditions", "link": "http://arxiv.org/abs/2403.16627v1", "similarity": 0.4997}, {"title": "ToDo: Token Downsampling for Efficient Generation of High-Resolution\n  Images", "link": "http://arxiv.org/abs/2402.13573v2", "similarity": 0.4924}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20LeYOLO%2C%20New%20Scalable%20and%20Efficient%20CNN%20Architecture%20for%20Object%20Detection&body=Title%3A%20LeYOLO%2C%20New%20Scalable%20and%20Efficient%20CNN%20Architecture%20for%20Object%20Detection%0AAuthor%3A%20Lilian%20Hollard%20and%20Lucas%20Mohimont%20and%20Nathalie%20Gaveau%20and%20Luiz-Angelo%20Steffenel%0AAbstract%3A%20%20%20Computational%20efficiency%20in%20deep%20neural%20networks%20is%20critical%20for%20object%0Adetection%2C%20especially%20as%20newer%20models%20prioritize%20speed%20over%20efficient%0Acomputation%20%28FLOP%29.%20This%20evolution%20has%20somewhat%20left%20behind%20embedded%20and%0Amobile-oriented%20AI%20object%20detection%20applications.%20In%20this%20paper%2C%20we%20focus%20on%0Adesign%20choices%20of%20neural%20network%20architectures%20for%20efficient%20object%20detection%0Acomputation%20based%20on%20FLOP%20and%20propose%20several%20optimizations%20to%20enhance%20the%0Aefficiency%20of%20YOLO-based%20models.%0A%20%20Firstly%2C%20we%20introduce%20an%20efficient%20backbone%20scaling%20inspired%20by%20inverted%0Abottlenecks%20and%20theoretical%20insights%20from%20the%20Information%20Bottleneck%20principle.%0ASecondly%2C%20we%20present%20the%20Fast%20Pyramidal%20Architecture%20Network%20%28FPAN%29%2C%20designed%0Ato%20facilitate%20fast%20multiscale%20feature%20sharing%20while%20reducing%20computational%0Aresources.%20Lastly%2C%20we%20propose%20a%20Decoupled%20Network-in-Network%20%28DNiN%29%20detection%0Ahead%20engineered%20to%20deliver%20rapid%20yet%20lightweight%20computations%20for%0Aclassification%20and%20regression%20tasks.%0A%20%20Building%20upon%20these%20optimizations%20and%20leveraging%20more%20efficient%20backbones%2C%0Athis%20paper%20contributes%20to%20a%20new%20scaling%20paradigm%20for%20object%20detection%20and%0AYOLO-centric%20models%20called%20LeYOLO.%20Our%20contribution%20consistently%20outperforms%0Aexisting%20models%20in%20various%20resource%20constraints%2C%20achieving%20unprecedented%0Aaccuracy%20and%20flop%20ratio.%20Notably%2C%20LeYOLO-Small%20achieves%20a%20competitive%20mAP%20score%0Aof%2038.2%25%20on%20the%20COCOval%20with%20just%204.5%20FLOP%28G%29%2C%20representing%20a%2042%25%20reduction%20in%0Acomputational%20load%20compared%20to%20the%20latest%20state-of-the-art%20YOLOv9-Tiny%20model%0Awhile%20achieving%20similar%20accuracy.%20Our%20novel%20model%20family%20achieves%20a%0AFLOP-to-accuracy%20ratio%20previously%20unattained%2C%20offering%20scalability%20that%20spans%0Afrom%20ultra-low%20neural%20network%20configurations%20%28%3C%201%20GFLOP%29%20to%20efficient%20yet%0Ademanding%20object%20detection%20setups%20%28%3E%204%20GFLOPs%29%20with%2025.2%2C%2031.3%2C%2035.2%2C%2038.2%2C%0A39.3%20and%2041%20mAP%20for%200.66%2C%201.47%2C%202.53%2C%204.51%2C%205.8%20and%208.4%20FLOP%28G%29.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2406.14239v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DLeYOLO%252C%2520New%2520Scalable%2520and%2520Efficient%2520CNN%2520Architecture%2520for%2520Object%2520Detection%26entry.906535625%3DLilian%2520Hollard%2520and%2520Lucas%2520Mohimont%2520and%2520Nathalie%2520Gaveau%2520and%2520Luiz-Angelo%2520Steffenel%26entry.1292438233%3D%2520%2520Computational%2520efficiency%2520in%2520deep%2520neural%2520networks%2520is%2520critical%2520for%2520object%250Adetection%252C%2520especially%2520as%2520newer%2520models%2520prioritize%2520speed%2520over%2520efficient%250Acomputation%2520%2528FLOP%2529.%2520This%2520evolution%2520has%2520somewhat%2520left%2520behind%2520embedded%2520and%250Amobile-oriented%2520AI%2520object%2520detection%2520applications.%2520In%2520this%2520paper%252C%2520we%2520focus%2520on%250Adesign%2520choices%2520of%2520neural%2520network%2520architectures%2520for%2520efficient%2520object%2520detection%250Acomputation%2520based%2520on%2520FLOP%2520and%2520propose%2520several%2520optimizations%2520to%2520enhance%2520the%250Aefficiency%2520of%2520YOLO-based%2520models.%250A%2520%2520Firstly%252C%2520we%2520introduce%2520an%2520efficient%2520backbone%2520scaling%2520inspired%2520by%2520inverted%250Abottlenecks%2520and%2520theoretical%2520insights%2520from%2520the%2520Information%2520Bottleneck%2520principle.%250ASecondly%252C%2520we%2520present%2520the%2520Fast%2520Pyramidal%2520Architecture%2520Network%2520%2528FPAN%2529%252C%2520designed%250Ato%2520facilitate%2520fast%2520multiscale%2520feature%2520sharing%2520while%2520reducing%2520computational%250Aresources.%2520Lastly%252C%2520we%2520propose%2520a%2520Decoupled%2520Network-in-Network%2520%2528DNiN%2529%2520detection%250Ahead%2520engineered%2520to%2520deliver%2520rapid%2520yet%2520lightweight%2520computations%2520for%250Aclassification%2520and%2520regression%2520tasks.%250A%2520%2520Building%2520upon%2520these%2520optimizations%2520and%2520leveraging%2520more%2520efficient%2520backbones%252C%250Athis%2520paper%2520contributes%2520to%2520a%2520new%2520scaling%2520paradigm%2520for%2520object%2520detection%2520and%250AYOLO-centric%2520models%2520called%2520LeYOLO.%2520Our%2520contribution%2520consistently%2520outperforms%250Aexisting%2520models%2520in%2520various%2520resource%2520constraints%252C%2520achieving%2520unprecedented%250Aaccuracy%2520and%2520flop%2520ratio.%2520Notably%252C%2520LeYOLO-Small%2520achieves%2520a%2520competitive%2520mAP%2520score%250Aof%252038.2%2525%2520on%2520the%2520COCOval%2520with%2520just%25204.5%2520FLOP%2528G%2529%252C%2520representing%2520a%252042%2525%2520reduction%2520in%250Acomputational%2520load%2520compared%2520to%2520the%2520latest%2520state-of-the-art%2520YOLOv9-Tiny%2520model%250Awhile%2520achieving%2520similar%2520accuracy.%2520Our%2520novel%2520model%2520family%2520achieves%2520a%250AFLOP-to-accuracy%2520ratio%2520previously%2520unattained%252C%2520offering%2520scalability%2520that%2520spans%250Afrom%2520ultra-low%2520neural%2520network%2520configurations%2520%2528%253C%25201%2520GFLOP%2529%2520to%2520efficient%2520yet%250Ademanding%2520object%2520detection%2520setups%2520%2528%253E%25204%2520GFLOPs%2529%2520with%252025.2%252C%252031.3%252C%252035.2%252C%252038.2%252C%250A39.3%2520and%252041%2520mAP%2520for%25200.66%252C%25201.47%252C%25202.53%252C%25204.51%252C%25205.8%2520and%25208.4%2520FLOP%2528G%2529.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2406.14239v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=LeYOLO%2C%20New%20Scalable%20and%20Efficient%20CNN%20Architecture%20for%20Object%20Detection&entry.906535625=Lilian%20Hollard%20and%20Lucas%20Mohimont%20and%20Nathalie%20Gaveau%20and%20Luiz-Angelo%20Steffenel&entry.1292438233=%20%20Computational%20efficiency%20in%20deep%20neural%20networks%20is%20critical%20for%20object%0Adetection%2C%20especially%20as%20newer%20models%20prioritize%20speed%20over%20efficient%0Acomputation%20%28FLOP%29.%20This%20evolution%20has%20somewhat%20left%20behind%20embedded%20and%0Amobile-oriented%20AI%20object%20detection%20applications.%20In%20this%20paper%2C%20we%20focus%20on%0Adesign%20choices%20of%20neural%20network%20architectures%20for%20efficient%20object%20detection%0Acomputation%20based%20on%20FLOP%20and%20propose%20several%20optimizations%20to%20enhance%20the%0Aefficiency%20of%20YOLO-based%20models.%0A%20%20Firstly%2C%20we%20introduce%20an%20efficient%20backbone%20scaling%20inspired%20by%20inverted%0Abottlenecks%20and%20theoretical%20insights%20from%20the%20Information%20Bottleneck%20principle.%0ASecondly%2C%20we%20present%20the%20Fast%20Pyramidal%20Architecture%20Network%20%28FPAN%29%2C%20designed%0Ato%20facilitate%20fast%20multiscale%20feature%20sharing%20while%20reducing%20computational%0Aresources.%20Lastly%2C%20we%20propose%20a%20Decoupled%20Network-in-Network%20%28DNiN%29%20detection%0Ahead%20engineered%20to%20deliver%20rapid%20yet%20lightweight%20computations%20for%0Aclassification%20and%20regression%20tasks.%0A%20%20Building%20upon%20these%20optimizations%20and%20leveraging%20more%20efficient%20backbones%2C%0Athis%20paper%20contributes%20to%20a%20new%20scaling%20paradigm%20for%20object%20detection%20and%0AYOLO-centric%20models%20called%20LeYOLO.%20Our%20contribution%20consistently%20outperforms%0Aexisting%20models%20in%20various%20resource%20constraints%2C%20achieving%20unprecedented%0Aaccuracy%20and%20flop%20ratio.%20Notably%2C%20LeYOLO-Small%20achieves%20a%20competitive%20mAP%20score%0Aof%2038.2%25%20on%20the%20COCOval%20with%20just%204.5%20FLOP%28G%29%2C%20representing%20a%2042%25%20reduction%20in%0Acomputational%20load%20compared%20to%20the%20latest%20state-of-the-art%20YOLOv9-Tiny%20model%0Awhile%20achieving%20similar%20accuracy.%20Our%20novel%20model%20family%20achieves%20a%0AFLOP-to-accuracy%20ratio%20previously%20unattained%2C%20offering%20scalability%20that%20spans%0Afrom%20ultra-low%20neural%20network%20configurations%20%28%3C%201%20GFLOP%29%20to%20efficient%20yet%0Ademanding%20object%20detection%20setups%20%28%3E%204%20GFLOPs%29%20with%2025.2%2C%2031.3%2C%2035.2%2C%2038.2%2C%0A39.3%20and%2041%20mAP%20for%200.66%2C%201.47%2C%202.53%2C%204.51%2C%205.8%20and%208.4%20FLOP%28G%29.%0A&entry.1838667208=http%3A//arxiv.org/abs/2406.14239v1&entry.124074799=Read"},
{"title": "Transferable Boltzmann Generators", "author": "Leon Klein and Frank No\u00e9", "abstract": "  The generation of equilibrium samples of molecular systems has been a\nlong-standing problem in statistical physics. Boltzmann Generators are a\ngenerative machine learning method that addresses this issue by learning a\ntransformation via a normalizing flow from a simple prior distribution to the\ntarget Boltzmann distribution of interest. Recently, flow matching has been\nemployed to train Boltzmann Generators for small molecular systems in Cartesian\ncoordinates. We extend this work and propose a first framework for Boltzmann\nGenerators that are transferable across chemical space, such that they predict\nzero-shot Boltzmann distributions for test molecules without being retrained\nfor these systems. These transferable Boltzmann Generators allow approximate\nsampling from the target distribution of unseen systems, as well as efficient\nreweighting to the target Boltzmann distribution. The transferability of the\nproposed framework is evaluated on dipeptides, where we show that it\ngeneralizes efficiently to unseen systems. Furthermore, we demonstrate that our\nproposed architecture enhances the efficiency of Boltzmann Generators trained\non single molecular systems.\n", "link": "http://arxiv.org/abs/2406.14426v1", "date": "2024-06-20", "relevancy": 1.9952, "topK": [{"title": "Scaling Rectified Flow Transformers for High-Resolution Image Synthesis", "link": "http://arxiv.org/abs/2403.03206v1", "similarity": 0.5225}, {"title": "Genie: Generative Interactive Environments", "link": "http://arxiv.org/abs/2402.15391v1", "similarity": 0.5044}, {"title": "UGG: Unified Generative Grasping", "link": "https://arxiv.org/abs/2311.16917", "similarity": 0.4838}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Transferable%20Boltzmann%20Generators&body=Title%3A%20Transferable%20Boltzmann%20Generators%0AAuthor%3A%20Leon%20Klein%20and%20Frank%20No%C3%A9%0AAbstract%3A%20%20%20The%20generation%20of%20equilibrium%20samples%20of%20molecular%20systems%20has%20been%20a%0Along-standing%20problem%20in%20statistical%20physics.%20Boltzmann%20Generators%20are%20a%0Agenerative%20machine%20learning%20method%20that%20addresses%20this%20issue%20by%20learning%20a%0Atransformation%20via%20a%20normalizing%20flow%20from%20a%20simple%20prior%20distribution%20to%20the%0Atarget%20Boltzmann%20distribution%20of%20interest.%20Recently%2C%20flow%20matching%20has%20been%0Aemployed%20to%20train%20Boltzmann%20Generators%20for%20small%20molecular%20systems%20in%20Cartesian%0Acoordinates.%20We%20extend%20this%20work%20and%20propose%20a%20first%20framework%20for%20Boltzmann%0AGenerators%20that%20are%20transferable%20across%20chemical%20space%2C%20such%20that%20they%20predict%0Azero-shot%20Boltzmann%20distributions%20for%20test%20molecules%20without%20being%20retrained%0Afor%20these%20systems.%20These%20transferable%20Boltzmann%20Generators%20allow%20approximate%0Asampling%20from%20the%20target%20distribution%20of%20unseen%20systems%2C%20as%20well%20as%20efficient%0Areweighting%20to%20the%20target%20Boltzmann%20distribution.%20The%20transferability%20of%20the%0Aproposed%20framework%20is%20evaluated%20on%20dipeptides%2C%20where%20we%20show%20that%20it%0Ageneralizes%20efficiently%20to%20unseen%20systems.%20Furthermore%2C%20we%20demonstrate%20that%20our%0Aproposed%20architecture%20enhances%20the%20efficiency%20of%20Boltzmann%20Generators%20trained%0Aon%20single%20molecular%20systems.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2406.14426v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DTransferable%2520Boltzmann%2520Generators%26entry.906535625%3DLeon%2520Klein%2520and%2520Frank%2520No%25C3%25A9%26entry.1292438233%3D%2520%2520The%2520generation%2520of%2520equilibrium%2520samples%2520of%2520molecular%2520systems%2520has%2520been%2520a%250Along-standing%2520problem%2520in%2520statistical%2520physics.%2520Boltzmann%2520Generators%2520are%2520a%250Agenerative%2520machine%2520learning%2520method%2520that%2520addresses%2520this%2520issue%2520by%2520learning%2520a%250Atransformation%2520via%2520a%2520normalizing%2520flow%2520from%2520a%2520simple%2520prior%2520distribution%2520to%2520the%250Atarget%2520Boltzmann%2520distribution%2520of%2520interest.%2520Recently%252C%2520flow%2520matching%2520has%2520been%250Aemployed%2520to%2520train%2520Boltzmann%2520Generators%2520for%2520small%2520molecular%2520systems%2520in%2520Cartesian%250Acoordinates.%2520We%2520extend%2520this%2520work%2520and%2520propose%2520a%2520first%2520framework%2520for%2520Boltzmann%250AGenerators%2520that%2520are%2520transferable%2520across%2520chemical%2520space%252C%2520such%2520that%2520they%2520predict%250Azero-shot%2520Boltzmann%2520distributions%2520for%2520test%2520molecules%2520without%2520being%2520retrained%250Afor%2520these%2520systems.%2520These%2520transferable%2520Boltzmann%2520Generators%2520allow%2520approximate%250Asampling%2520from%2520the%2520target%2520distribution%2520of%2520unseen%2520systems%252C%2520as%2520well%2520as%2520efficient%250Areweighting%2520to%2520the%2520target%2520Boltzmann%2520distribution.%2520The%2520transferability%2520of%2520the%250Aproposed%2520framework%2520is%2520evaluated%2520on%2520dipeptides%252C%2520where%2520we%2520show%2520that%2520it%250Ageneralizes%2520efficiently%2520to%2520unseen%2520systems.%2520Furthermore%252C%2520we%2520demonstrate%2520that%2520our%250Aproposed%2520architecture%2520enhances%2520the%2520efficiency%2520of%2520Boltzmann%2520Generators%2520trained%250Aon%2520single%2520molecular%2520systems.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2406.14426v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Transferable%20Boltzmann%20Generators&entry.906535625=Leon%20Klein%20and%20Frank%20No%C3%A9&entry.1292438233=%20%20The%20generation%20of%20equilibrium%20samples%20of%20molecular%20systems%20has%20been%20a%0Along-standing%20problem%20in%20statistical%20physics.%20Boltzmann%20Generators%20are%20a%0Agenerative%20machine%20learning%20method%20that%20addresses%20this%20issue%20by%20learning%20a%0Atransformation%20via%20a%20normalizing%20flow%20from%20a%20simple%20prior%20distribution%20to%20the%0Atarget%20Boltzmann%20distribution%20of%20interest.%20Recently%2C%20flow%20matching%20has%20been%0Aemployed%20to%20train%20Boltzmann%20Generators%20for%20small%20molecular%20systems%20in%20Cartesian%0Acoordinates.%20We%20extend%20this%20work%20and%20propose%20a%20first%20framework%20for%20Boltzmann%0AGenerators%20that%20are%20transferable%20across%20chemical%20space%2C%20such%20that%20they%20predict%0Azero-shot%20Boltzmann%20distributions%20for%20test%20molecules%20without%20being%20retrained%0Afor%20these%20systems.%20These%20transferable%20Boltzmann%20Generators%20allow%20approximate%0Asampling%20from%20the%20target%20distribution%20of%20unseen%20systems%2C%20as%20well%20as%20efficient%0Areweighting%20to%20the%20target%20Boltzmann%20distribution.%20The%20transferability%20of%20the%0Aproposed%20framework%20is%20evaluated%20on%20dipeptides%2C%20where%20we%20show%20that%20it%0Ageneralizes%20efficiently%20to%20unseen%20systems.%20Furthermore%2C%20we%20demonstrate%20that%20our%0Aproposed%20architecture%20enhances%20the%20efficiency%20of%20Boltzmann%20Generators%20trained%0Aon%20single%20molecular%20systems.%0A&entry.1838667208=http%3A//arxiv.org/abs/2406.14426v1&entry.124074799=Read"},
{"title": "Revealing the learning process in reinforcement learning agents through\n  attention-oriented metrics", "author": "Charlotte Beylier and Simon M. Hofmann and Nico Scherf", "abstract": "  The learning process of a reinforcement learning (RL) agent remains poorly\nunderstood beyond the mathematical formulation of its learning algorithm. To\naddress this gap, we introduce attention-oriented metrics (ATOMs) to\ninvestigate the development of an RL agent's attention during training. We\ntested ATOMs on three variations of a Pong game, each designed to teach the\nagent distinct behaviours, complemented by a behavioural assessment. Our\nfindings reveal that ATOMs successfully delineate the attention patterns of an\nagent trained on each game variation, and that these differences in attention\npatterns translate into differences in the agent's behaviour. Through\ncontinuous monitoring of ATOMs during training, we observed that the agent's\nattention developed in phases, and that these phases were consistent across\ngames. Finally, we noted that the agent's attention to its paddle emerged\nrelatively late in the training and coincided with a marked increase in its\nperformance score. Overall, we believe that ATOMs could significantly enhance\nour understanding of RL agents' learning processes, which is essential for\nimproving their reliability and efficiency.\n", "link": "http://arxiv.org/abs/2406.14324v1", "date": "2024-06-20", "relevancy": 1.994, "topK": [{"title": "Safe and efficient training of a control agent", "link": "https://patents.google.com/patent/US11709462B2/en", "similarity": 0.5045}, {"title": "ToDo: Token Downsampling for Efficient Generation of High-Resolution\n  Images", "link": "http://arxiv.org/abs/2402.13573v2", "similarity": 0.4969}, {"title": "RoboEXP: Action-Conditioned Scene Graph via Interactive Exploration for\n  Robotic Manipulation", "link": "http://arxiv.org/abs/2402.15487v1", "similarity": 0.4876}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Revealing%20the%20learning%20process%20in%20reinforcement%20learning%20agents%20through%0A%20%20attention-oriented%20metrics&body=Title%3A%20Revealing%20the%20learning%20process%20in%20reinforcement%20learning%20agents%20through%0A%20%20attention-oriented%20metrics%0AAuthor%3A%20Charlotte%20Beylier%20and%20Simon%20M.%20Hofmann%20and%20Nico%20Scherf%0AAbstract%3A%20%20%20The%20learning%20process%20of%20a%20reinforcement%20learning%20%28RL%29%20agent%20remains%20poorly%0Aunderstood%20beyond%20the%20mathematical%20formulation%20of%20its%20learning%20algorithm.%20To%0Aaddress%20this%20gap%2C%20we%20introduce%20attention-oriented%20metrics%20%28ATOMs%29%20to%0Ainvestigate%20the%20development%20of%20an%20RL%20agent%27s%20attention%20during%20training.%20We%0Atested%20ATOMs%20on%20three%20variations%20of%20a%20Pong%20game%2C%20each%20designed%20to%20teach%20the%0Aagent%20distinct%20behaviours%2C%20complemented%20by%20a%20behavioural%20assessment.%20Our%0Afindings%20reveal%20that%20ATOMs%20successfully%20delineate%20the%20attention%20patterns%20of%20an%0Aagent%20trained%20on%20each%20game%20variation%2C%20and%20that%20these%20differences%20in%20attention%0Apatterns%20translate%20into%20differences%20in%20the%20agent%27s%20behaviour.%20Through%0Acontinuous%20monitoring%20of%20ATOMs%20during%20training%2C%20we%20observed%20that%20the%20agent%27s%0Aattention%20developed%20in%20phases%2C%20and%20that%20these%20phases%20were%20consistent%20across%0Agames.%20Finally%2C%20we%20noted%20that%20the%20agent%27s%20attention%20to%20its%20paddle%20emerged%0Arelatively%20late%20in%20the%20training%20and%20coincided%20with%20a%20marked%20increase%20in%20its%0Aperformance%20score.%20Overall%2C%20we%20believe%20that%20ATOMs%20could%20significantly%20enhance%0Aour%20understanding%20of%20RL%20agents%27%20learning%20processes%2C%20which%20is%20essential%20for%0Aimproving%20their%20reliability%20and%20efficiency.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2406.14324v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DRevealing%2520the%2520learning%2520process%2520in%2520reinforcement%2520learning%2520agents%2520through%250A%2520%2520attention-oriented%2520metrics%26entry.906535625%3DCharlotte%2520Beylier%2520and%2520Simon%2520M.%2520Hofmann%2520and%2520Nico%2520Scherf%26entry.1292438233%3D%2520%2520The%2520learning%2520process%2520of%2520a%2520reinforcement%2520learning%2520%2528RL%2529%2520agent%2520remains%2520poorly%250Aunderstood%2520beyond%2520the%2520mathematical%2520formulation%2520of%2520its%2520learning%2520algorithm.%2520To%250Aaddress%2520this%2520gap%252C%2520we%2520introduce%2520attention-oriented%2520metrics%2520%2528ATOMs%2529%2520to%250Ainvestigate%2520the%2520development%2520of%2520an%2520RL%2520agent%2527s%2520attention%2520during%2520training.%2520We%250Atested%2520ATOMs%2520on%2520three%2520variations%2520of%2520a%2520Pong%2520game%252C%2520each%2520designed%2520to%2520teach%2520the%250Aagent%2520distinct%2520behaviours%252C%2520complemented%2520by%2520a%2520behavioural%2520assessment.%2520Our%250Afindings%2520reveal%2520that%2520ATOMs%2520successfully%2520delineate%2520the%2520attention%2520patterns%2520of%2520an%250Aagent%2520trained%2520on%2520each%2520game%2520variation%252C%2520and%2520that%2520these%2520differences%2520in%2520attention%250Apatterns%2520translate%2520into%2520differences%2520in%2520the%2520agent%2527s%2520behaviour.%2520Through%250Acontinuous%2520monitoring%2520of%2520ATOMs%2520during%2520training%252C%2520we%2520observed%2520that%2520the%2520agent%2527s%250Aattention%2520developed%2520in%2520phases%252C%2520and%2520that%2520these%2520phases%2520were%2520consistent%2520across%250Agames.%2520Finally%252C%2520we%2520noted%2520that%2520the%2520agent%2527s%2520attention%2520to%2520its%2520paddle%2520emerged%250Arelatively%2520late%2520in%2520the%2520training%2520and%2520coincided%2520with%2520a%2520marked%2520increase%2520in%2520its%250Aperformance%2520score.%2520Overall%252C%2520we%2520believe%2520that%2520ATOMs%2520could%2520significantly%2520enhance%250Aour%2520understanding%2520of%2520RL%2520agents%2527%2520learning%2520processes%252C%2520which%2520is%2520essential%2520for%250Aimproving%2520their%2520reliability%2520and%2520efficiency.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2406.14324v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Revealing%20the%20learning%20process%20in%20reinforcement%20learning%20agents%20through%0A%20%20attention-oriented%20metrics&entry.906535625=Charlotte%20Beylier%20and%20Simon%20M.%20Hofmann%20and%20Nico%20Scherf&entry.1292438233=%20%20The%20learning%20process%20of%20a%20reinforcement%20learning%20%28RL%29%20agent%20remains%20poorly%0Aunderstood%20beyond%20the%20mathematical%20formulation%20of%20its%20learning%20algorithm.%20To%0Aaddress%20this%20gap%2C%20we%20introduce%20attention-oriented%20metrics%20%28ATOMs%29%20to%0Ainvestigate%20the%20development%20of%20an%20RL%20agent%27s%20attention%20during%20training.%20We%0Atested%20ATOMs%20on%20three%20variations%20of%20a%20Pong%20game%2C%20each%20designed%20to%20teach%20the%0Aagent%20distinct%20behaviours%2C%20complemented%20by%20a%20behavioural%20assessment.%20Our%0Afindings%20reveal%20that%20ATOMs%20successfully%20delineate%20the%20attention%20patterns%20of%20an%0Aagent%20trained%20on%20each%20game%20variation%2C%20and%20that%20these%20differences%20in%20attention%0Apatterns%20translate%20into%20differences%20in%20the%20agent%27s%20behaviour.%20Through%0Acontinuous%20monitoring%20of%20ATOMs%20during%20training%2C%20we%20observed%20that%20the%20agent%27s%0Aattention%20developed%20in%20phases%2C%20and%20that%20these%20phases%20were%20consistent%20across%0Agames.%20Finally%2C%20we%20noted%20that%20the%20agent%27s%20attention%20to%20its%20paddle%20emerged%0Arelatively%20late%20in%20the%20training%20and%20coincided%20with%20a%20marked%20increase%20in%20its%0Aperformance%20score.%20Overall%2C%20we%20believe%20that%20ATOMs%20could%20significantly%20enhance%0Aour%20understanding%20of%20RL%20agents%27%20learning%20processes%2C%20which%20is%20essential%20for%0Aimproving%20their%20reliability%20and%20efficiency.%0A&entry.1838667208=http%3A//arxiv.org/abs/2406.14324v1&entry.124074799=Read"},
{"title": "Prism: A Framework for Decoupling and Assessing the Capabilities of VLMs", "author": "Yuxuan Qiao and Haodong Duan and Xinyu Fang and Junming Yang and Lin Chen and Songyang Zhang and Jiaqi Wang and Dahua Lin and Kai Chen", "abstract": "  Vision Language Models (VLMs) demonstrate remarkable proficiency in\naddressing a wide array of visual questions, which requires strong perception\nand reasoning faculties. Assessing these two competencies independently is\ncrucial for model refinement, despite the inherent difficulty due to the\nintertwined nature of seeing and reasoning in existing VLMs. To tackle this\nissue, we present Prism, an innovative framework designed to disentangle the\nperception and reasoning processes involved in visual question solving. Prism\ncomprises two distinct stages: a perception stage that utilizes a VLM to\nextract and articulate visual information in textual form, and a reasoning\nstage that formulates responses based on the extracted visual information using\na Large Language Model (LLM). This modular design enables the systematic\ncomparison and assessment of both proprietary and open-source VLM for their\nperception and reasoning strengths. Our analytical framework provides several\nvaluable insights, underscoring Prism's potential as a cost-effective solution\nfor vision-language tasks. By combining a streamlined VLM focused on perception\nwith a powerful LLM tailored for reasoning, Prism achieves superior results in\ngeneral vision-language tasks while substantially cutting down on training and\noperational expenses. Quantitative evaluations show that Prism, when configured\nwith a vanilla 2B LLaVA and freely accessible GPT-3.5, delivers performance on\npar with VLMs $10 \\times$ larger on the rigorous multimodal benchmark MMStar.\nThe project is released at: https://github.com/SparksJoe/Prism.\n", "link": "http://arxiv.org/abs/2406.14544v1", "date": "2024-06-20", "relevancy": 1.9935, "topK": [{"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5165}, {"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.4859}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.4843}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Prism%3A%20A%20Framework%20for%20Decoupling%20and%20Assessing%20the%20Capabilities%20of%20VLMs&body=Title%3A%20Prism%3A%20A%20Framework%20for%20Decoupling%20and%20Assessing%20the%20Capabilities%20of%20VLMs%0AAuthor%3A%20Yuxuan%20Qiao%20and%20Haodong%20Duan%20and%20Xinyu%20Fang%20and%20Junming%20Yang%20and%20Lin%20Chen%20and%20Songyang%20Zhang%20and%20Jiaqi%20Wang%20and%20Dahua%20Lin%20and%20Kai%20Chen%0AAbstract%3A%20%20%20Vision%20Language%20Models%20%28VLMs%29%20demonstrate%20remarkable%20proficiency%20in%0Aaddressing%20a%20wide%20array%20of%20visual%20questions%2C%20which%20requires%20strong%20perception%0Aand%20reasoning%20faculties.%20Assessing%20these%20two%20competencies%20independently%20is%0Acrucial%20for%20model%20refinement%2C%20despite%20the%20inherent%20difficulty%20due%20to%20the%0Aintertwined%20nature%20of%20seeing%20and%20reasoning%20in%20existing%20VLMs.%20To%20tackle%20this%0Aissue%2C%20we%20present%20Prism%2C%20an%20innovative%20framework%20designed%20to%20disentangle%20the%0Aperception%20and%20reasoning%20processes%20involved%20in%20visual%20question%20solving.%20Prism%0Acomprises%20two%20distinct%20stages%3A%20a%20perception%20stage%20that%20utilizes%20a%20VLM%20to%0Aextract%20and%20articulate%20visual%20information%20in%20textual%20form%2C%20and%20a%20reasoning%0Astage%20that%20formulates%20responses%20based%20on%20the%20extracted%20visual%20information%20using%0Aa%20Large%20Language%20Model%20%28LLM%29.%20This%20modular%20design%20enables%20the%20systematic%0Acomparison%20and%20assessment%20of%20both%20proprietary%20and%20open-source%20VLM%20for%20their%0Aperception%20and%20reasoning%20strengths.%20Our%20analytical%20framework%20provides%20several%0Avaluable%20insights%2C%20underscoring%20Prism%27s%20potential%20as%20a%20cost-effective%20solution%0Afor%20vision-language%20tasks.%20By%20combining%20a%20streamlined%20VLM%20focused%20on%20perception%0Awith%20a%20powerful%20LLM%20tailored%20for%20reasoning%2C%20Prism%20achieves%20superior%20results%20in%0Ageneral%20vision-language%20tasks%20while%20substantially%20cutting%20down%20on%20training%20and%0Aoperational%20expenses.%20Quantitative%20evaluations%20show%20that%20Prism%2C%20when%20configured%0Awith%20a%20vanilla%202B%20LLaVA%20and%20freely%20accessible%20GPT-3.5%2C%20delivers%20performance%20on%0Apar%20with%20VLMs%20%2410%20%5Ctimes%24%20larger%20on%20the%20rigorous%20multimodal%20benchmark%20MMStar.%0AThe%20project%20is%20released%20at%3A%20https%3A//github.com/SparksJoe/Prism.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2406.14544v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DPrism%253A%2520A%2520Framework%2520for%2520Decoupling%2520and%2520Assessing%2520the%2520Capabilities%2520of%2520VLMs%26entry.906535625%3DYuxuan%2520Qiao%2520and%2520Haodong%2520Duan%2520and%2520Xinyu%2520Fang%2520and%2520Junming%2520Yang%2520and%2520Lin%2520Chen%2520and%2520Songyang%2520Zhang%2520and%2520Jiaqi%2520Wang%2520and%2520Dahua%2520Lin%2520and%2520Kai%2520Chen%26entry.1292438233%3D%2520%2520Vision%2520Language%2520Models%2520%2528VLMs%2529%2520demonstrate%2520remarkable%2520proficiency%2520in%250Aaddressing%2520a%2520wide%2520array%2520of%2520visual%2520questions%252C%2520which%2520requires%2520strong%2520perception%250Aand%2520reasoning%2520faculties.%2520Assessing%2520these%2520two%2520competencies%2520independently%2520is%250Acrucial%2520for%2520model%2520refinement%252C%2520despite%2520the%2520inherent%2520difficulty%2520due%2520to%2520the%250Aintertwined%2520nature%2520of%2520seeing%2520and%2520reasoning%2520in%2520existing%2520VLMs.%2520To%2520tackle%2520this%250Aissue%252C%2520we%2520present%2520Prism%252C%2520an%2520innovative%2520framework%2520designed%2520to%2520disentangle%2520the%250Aperception%2520and%2520reasoning%2520processes%2520involved%2520in%2520visual%2520question%2520solving.%2520Prism%250Acomprises%2520two%2520distinct%2520stages%253A%2520a%2520perception%2520stage%2520that%2520utilizes%2520a%2520VLM%2520to%250Aextract%2520and%2520articulate%2520visual%2520information%2520in%2520textual%2520form%252C%2520and%2520a%2520reasoning%250Astage%2520that%2520formulates%2520responses%2520based%2520on%2520the%2520extracted%2520visual%2520information%2520using%250Aa%2520Large%2520Language%2520Model%2520%2528LLM%2529.%2520This%2520modular%2520design%2520enables%2520the%2520systematic%250Acomparison%2520and%2520assessment%2520of%2520both%2520proprietary%2520and%2520open-source%2520VLM%2520for%2520their%250Aperception%2520and%2520reasoning%2520strengths.%2520Our%2520analytical%2520framework%2520provides%2520several%250Avaluable%2520insights%252C%2520underscoring%2520Prism%2527s%2520potential%2520as%2520a%2520cost-effective%2520solution%250Afor%2520vision-language%2520tasks.%2520By%2520combining%2520a%2520streamlined%2520VLM%2520focused%2520on%2520perception%250Awith%2520a%2520powerful%2520LLM%2520tailored%2520for%2520reasoning%252C%2520Prism%2520achieves%2520superior%2520results%2520in%250Ageneral%2520vision-language%2520tasks%2520while%2520substantially%2520cutting%2520down%2520on%2520training%2520and%250Aoperational%2520expenses.%2520Quantitative%2520evaluations%2520show%2520that%2520Prism%252C%2520when%2520configured%250Awith%2520a%2520vanilla%25202B%2520LLaVA%2520and%2520freely%2520accessible%2520GPT-3.5%252C%2520delivers%2520performance%2520on%250Apar%2520with%2520VLMs%2520%252410%2520%255Ctimes%2524%2520larger%2520on%2520the%2520rigorous%2520multimodal%2520benchmark%2520MMStar.%250AThe%2520project%2520is%2520released%2520at%253A%2520https%253A//github.com/SparksJoe/Prism.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2406.14544v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Prism%3A%20A%20Framework%20for%20Decoupling%20and%20Assessing%20the%20Capabilities%20of%20VLMs&entry.906535625=Yuxuan%20Qiao%20and%20Haodong%20Duan%20and%20Xinyu%20Fang%20and%20Junming%20Yang%20and%20Lin%20Chen%20and%20Songyang%20Zhang%20and%20Jiaqi%20Wang%20and%20Dahua%20Lin%20and%20Kai%20Chen&entry.1292438233=%20%20Vision%20Language%20Models%20%28VLMs%29%20demonstrate%20remarkable%20proficiency%20in%0Aaddressing%20a%20wide%20array%20of%20visual%20questions%2C%20which%20requires%20strong%20perception%0Aand%20reasoning%20faculties.%20Assessing%20these%20two%20competencies%20independently%20is%0Acrucial%20for%20model%20refinement%2C%20despite%20the%20inherent%20difficulty%20due%20to%20the%0Aintertwined%20nature%20of%20seeing%20and%20reasoning%20in%20existing%20VLMs.%20To%20tackle%20this%0Aissue%2C%20we%20present%20Prism%2C%20an%20innovative%20framework%20designed%20to%20disentangle%20the%0Aperception%20and%20reasoning%20processes%20involved%20in%20visual%20question%20solving.%20Prism%0Acomprises%20two%20distinct%20stages%3A%20a%20perception%20stage%20that%20utilizes%20a%20VLM%20to%0Aextract%20and%20articulate%20visual%20information%20in%20textual%20form%2C%20and%20a%20reasoning%0Astage%20that%20formulates%20responses%20based%20on%20the%20extracted%20visual%20information%20using%0Aa%20Large%20Language%20Model%20%28LLM%29.%20This%20modular%20design%20enables%20the%20systematic%0Acomparison%20and%20assessment%20of%20both%20proprietary%20and%20open-source%20VLM%20for%20their%0Aperception%20and%20reasoning%20strengths.%20Our%20analytical%20framework%20provides%20several%0Avaluable%20insights%2C%20underscoring%20Prism%27s%20potential%20as%20a%20cost-effective%20solution%0Afor%20vision-language%20tasks.%20By%20combining%20a%20streamlined%20VLM%20focused%20on%20perception%0Awith%20a%20powerful%20LLM%20tailored%20for%20reasoning%2C%20Prism%20achieves%20superior%20results%20in%0Ageneral%20vision-language%20tasks%20while%20substantially%20cutting%20down%20on%20training%20and%0Aoperational%20expenses.%20Quantitative%20evaluations%20show%20that%20Prism%2C%20when%20configured%0Awith%20a%20vanilla%202B%20LLaVA%20and%20freely%20accessible%20GPT-3.5%2C%20delivers%20performance%20on%0Apar%20with%20VLMs%20%2410%20%5Ctimes%24%20larger%20on%20the%20rigorous%20multimodal%20benchmark%20MMStar.%0AThe%20project%20is%20released%20at%3A%20https%3A//github.com/SparksJoe/Prism.%0A&entry.1838667208=http%3A//arxiv.org/abs/2406.14544v1&entry.124074799=Read"},
{"title": "[Experiments & Analysis] Evaluating the Feasibility of Sampling-Based\n  Techniques for Training Multilayer Perceptrons", "author": "Sana Ebrahimi and Rishi Advani and Abolfazl Asudeh", "abstract": "  The training process of neural networks is known to be time-consuming, and\nhaving a deep architecture only aggravates the issue. This process consists\nmostly of matrix operations, among which matrix multiplication is the\nbottleneck. Several sampling-based techniques have been proposed for speeding\nup the training time of deep neural networks by approximating the matrix\nproducts. These techniques fall under two categories: (i) sampling a subset of\nnodes in every hidden layer as active at every iteration and (ii) sampling a\nsubset of nodes from the previous layer to approximate the current layer's\nactivations using the edges from the sampled nodes. In both cases, the matrix\nproducts are computed using only the selected samples. In this paper, we\nevaluate the feasibility of these approaches on CPU machines with limited\ncomputational resources. Making a connection between the two research\ndirections as special cases of approximating matrix multiplications in the\ncontext of neural networks, we provide a negative theoretical analysis that\nshows feedforward approximation is an obstacle against scalability. We conduct\ncomprehensive experimental evaluations that demonstrate the most pressing\nchallenges and limitations associated with the studied approaches. We observe\nthat the hashing-based node selection method is not scalable to a large number\nof layers, confirming our theoretical analysis. Finally, we identify directions\nfor future research.\n", "link": "http://arxiv.org/abs/2306.09293v2", "date": "2024-06-20", "relevancy": 1.9911, "topK": [{"title": "Breadcrumbs: Adversarial class-balanced sampling for long-tailed recognition", "link": "https://link.springer.com/chapter/10.1007/978-3-031-20053-3_37", "similarity": 0.4983}, {"title": "ToDo: Token Downsampling for Efficient Generation of High-Resolution\n  Images", "link": "http://arxiv.org/abs/2402.13573v2", "similarity": 0.4979}, {"title": "SDXS: Real-Time One-Step Latent Diffusion Models with Image Conditions", "link": "http://arxiv.org/abs/2403.16627v1", "similarity": 0.4962}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20%5BExperiments%20%26%20Analysis%5D%20Evaluating%20the%20Feasibility%20of%20Sampling-Based%0A%20%20Techniques%20for%20Training%20Multilayer%20Perceptrons&body=Title%3A%20%5BExperiments%20%26%20Analysis%5D%20Evaluating%20the%20Feasibility%20of%20Sampling-Based%0A%20%20Techniques%20for%20Training%20Multilayer%20Perceptrons%0AAuthor%3A%20Sana%20Ebrahimi%20and%20Rishi%20Advani%20and%20Abolfazl%20Asudeh%0AAbstract%3A%20%20%20The%20training%20process%20of%20neural%20networks%20is%20known%20to%20be%20time-consuming%2C%20and%0Ahaving%20a%20deep%20architecture%20only%20aggravates%20the%20issue.%20This%20process%20consists%0Amostly%20of%20matrix%20operations%2C%20among%20which%20matrix%20multiplication%20is%20the%0Abottleneck.%20Several%20sampling-based%20techniques%20have%20been%20proposed%20for%20speeding%0Aup%20the%20training%20time%20of%20deep%20neural%20networks%20by%20approximating%20the%20matrix%0Aproducts.%20These%20techniques%20fall%20under%20two%20categories%3A%20%28i%29%20sampling%20a%20subset%20of%0Anodes%20in%20every%20hidden%20layer%20as%20active%20at%20every%20iteration%20and%20%28ii%29%20sampling%20a%0Asubset%20of%20nodes%20from%20the%20previous%20layer%20to%20approximate%20the%20current%20layer%27s%0Aactivations%20using%20the%20edges%20from%20the%20sampled%20nodes.%20In%20both%20cases%2C%20the%20matrix%0Aproducts%20are%20computed%20using%20only%20the%20selected%20samples.%20In%20this%20paper%2C%20we%0Aevaluate%20the%20feasibility%20of%20these%20approaches%20on%20CPU%20machines%20with%20limited%0Acomputational%20resources.%20Making%20a%20connection%20between%20the%20two%20research%0Adirections%20as%20special%20cases%20of%20approximating%20matrix%20multiplications%20in%20the%0Acontext%20of%20neural%20networks%2C%20we%20provide%20a%20negative%20theoretical%20analysis%20that%0Ashows%20feedforward%20approximation%20is%20an%20obstacle%20against%20scalability.%20We%20conduct%0Acomprehensive%20experimental%20evaluations%20that%20demonstrate%20the%20most%20pressing%0Achallenges%20and%20limitations%20associated%20with%20the%20studied%20approaches.%20We%20observe%0Athat%20the%20hashing-based%20node%20selection%20method%20is%20not%20scalable%20to%20a%20large%20number%0Aof%20layers%2C%20confirming%20our%20theoretical%20analysis.%20Finally%2C%20we%20identify%20directions%0Afor%20future%20research.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2306.09293v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3D%255BExperiments%2520%2526%2520Analysis%255D%2520Evaluating%2520the%2520Feasibility%2520of%2520Sampling-Based%250A%2520%2520Techniques%2520for%2520Training%2520Multilayer%2520Perceptrons%26entry.906535625%3DSana%2520Ebrahimi%2520and%2520Rishi%2520Advani%2520and%2520Abolfazl%2520Asudeh%26entry.1292438233%3D%2520%2520The%2520training%2520process%2520of%2520neural%2520networks%2520is%2520known%2520to%2520be%2520time-consuming%252C%2520and%250Ahaving%2520a%2520deep%2520architecture%2520only%2520aggravates%2520the%2520issue.%2520This%2520process%2520consists%250Amostly%2520of%2520matrix%2520operations%252C%2520among%2520which%2520matrix%2520multiplication%2520is%2520the%250Abottleneck.%2520Several%2520sampling-based%2520techniques%2520have%2520been%2520proposed%2520for%2520speeding%250Aup%2520the%2520training%2520time%2520of%2520deep%2520neural%2520networks%2520by%2520approximating%2520the%2520matrix%250Aproducts.%2520These%2520techniques%2520fall%2520under%2520two%2520categories%253A%2520%2528i%2529%2520sampling%2520a%2520subset%2520of%250Anodes%2520in%2520every%2520hidden%2520layer%2520as%2520active%2520at%2520every%2520iteration%2520and%2520%2528ii%2529%2520sampling%2520a%250Asubset%2520of%2520nodes%2520from%2520the%2520previous%2520layer%2520to%2520approximate%2520the%2520current%2520layer%2527s%250Aactivations%2520using%2520the%2520edges%2520from%2520the%2520sampled%2520nodes.%2520In%2520both%2520cases%252C%2520the%2520matrix%250Aproducts%2520are%2520computed%2520using%2520only%2520the%2520selected%2520samples.%2520In%2520this%2520paper%252C%2520we%250Aevaluate%2520the%2520feasibility%2520of%2520these%2520approaches%2520on%2520CPU%2520machines%2520with%2520limited%250Acomputational%2520resources.%2520Making%2520a%2520connection%2520between%2520the%2520two%2520research%250Adirections%2520as%2520special%2520cases%2520of%2520approximating%2520matrix%2520multiplications%2520in%2520the%250Acontext%2520of%2520neural%2520networks%252C%2520we%2520provide%2520a%2520negative%2520theoretical%2520analysis%2520that%250Ashows%2520feedforward%2520approximation%2520is%2520an%2520obstacle%2520against%2520scalability.%2520We%2520conduct%250Acomprehensive%2520experimental%2520evaluations%2520that%2520demonstrate%2520the%2520most%2520pressing%250Achallenges%2520and%2520limitations%2520associated%2520with%2520the%2520studied%2520approaches.%2520We%2520observe%250Athat%2520the%2520hashing-based%2520node%2520selection%2520method%2520is%2520not%2520scalable%2520to%2520a%2520large%2520number%250Aof%2520layers%252C%2520confirming%2520our%2520theoretical%2520analysis.%2520Finally%252C%2520we%2520identify%2520directions%250Afor%2520future%2520research.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2306.09293v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=%5BExperiments%20%26%20Analysis%5D%20Evaluating%20the%20Feasibility%20of%20Sampling-Based%0A%20%20Techniques%20for%20Training%20Multilayer%20Perceptrons&entry.906535625=Sana%20Ebrahimi%20and%20Rishi%20Advani%20and%20Abolfazl%20Asudeh&entry.1292438233=%20%20The%20training%20process%20of%20neural%20networks%20is%20known%20to%20be%20time-consuming%2C%20and%0Ahaving%20a%20deep%20architecture%20only%20aggravates%20the%20issue.%20This%20process%20consists%0Amostly%20of%20matrix%20operations%2C%20among%20which%20matrix%20multiplication%20is%20the%0Abottleneck.%20Several%20sampling-based%20techniques%20have%20been%20proposed%20for%20speeding%0Aup%20the%20training%20time%20of%20deep%20neural%20networks%20by%20approximating%20the%20matrix%0Aproducts.%20These%20techniques%20fall%20under%20two%20categories%3A%20%28i%29%20sampling%20a%20subset%20of%0Anodes%20in%20every%20hidden%20layer%20as%20active%20at%20every%20iteration%20and%20%28ii%29%20sampling%20a%0Asubset%20of%20nodes%20from%20the%20previous%20layer%20to%20approximate%20the%20current%20layer%27s%0Aactivations%20using%20the%20edges%20from%20the%20sampled%20nodes.%20In%20both%20cases%2C%20the%20matrix%0Aproducts%20are%20computed%20using%20only%20the%20selected%20samples.%20In%20this%20paper%2C%20we%0Aevaluate%20the%20feasibility%20of%20these%20approaches%20on%20CPU%20machines%20with%20limited%0Acomputational%20resources.%20Making%20a%20connection%20between%20the%20two%20research%0Adirections%20as%20special%20cases%20of%20approximating%20matrix%20multiplications%20in%20the%0Acontext%20of%20neural%20networks%2C%20we%20provide%20a%20negative%20theoretical%20analysis%20that%0Ashows%20feedforward%20approximation%20is%20an%20obstacle%20against%20scalability.%20We%20conduct%0Acomprehensive%20experimental%20evaluations%20that%20demonstrate%20the%20most%20pressing%0Achallenges%20and%20limitations%20associated%20with%20the%20studied%20approaches.%20We%20observe%0Athat%20the%20hashing-based%20node%20selection%20method%20is%20not%20scalable%20to%20a%20large%20number%0Aof%20layers%2C%20confirming%20our%20theoretical%20analysis.%20Finally%2C%20we%20identify%20directions%0Afor%20future%20research.%0A&entry.1838667208=http%3A//arxiv.org/abs/2306.09293v2&entry.124074799=Read"},
{"title": "High-Resolution Building and Road Detection from Sentinel-2", "author": "Wojciech Sirko and Emmanuel Asiedu Brempong and Juliana T. C. Marcos and Abigail Annkah and Abel Korme and Mohammed Alewi Hassen and Krishna Sapkota and Tomer Shekel and Abdoulaye Diack and Sella Nevo and Jason Hickey and John Quinn", "abstract": "  Mapping buildings and roads automatically with remote sensing typically\nrequires high-resolution imagery, which is expensive to obtain and often\nsparsely available. In this work we demonstrate how multiple 10 m resolution\nSentinel-2 images can be used to generate 50 cm resolution building and road\nsegmentation masks. This is done by training a `student' model with access to\nSentinel-2 images to reproduce the predictions of a `teacher' model which has\naccess to corresponding high-resolution imagery. While the predictions do not\nhave all the fine detail of the teacher model, we find that we are able to\nretain much of the performance: for building segmentation we achieve 78.3%\nmIoU, compared to the high-resolution teacher model accuracy of 85.3% mIoU. We\nalso describe a related method for counting individual buildings in a\nSentinel-2 patch which achieves R^2 = 0.91 against true counts. This work opens\nup new possibilities for using freely available Sentinel-2 imagery for a range\nof tasks that previously could only be done with high-resolution satellite\nimagery.\n", "link": "http://arxiv.org/abs/2310.11622v2", "date": "2024-06-20", "relevancy": 1.9885, "topK": [{"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.5018}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.4964}, {"title": "ToDo: Token Downsampling for Efficient Generation of High-Resolution\n  Images", "link": "http://arxiv.org/abs/2402.13573v2", "similarity": 0.4928}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20High-Resolution%20Building%20and%20Road%20Detection%20from%20Sentinel-2&body=Title%3A%20High-Resolution%20Building%20and%20Road%20Detection%20from%20Sentinel-2%0AAuthor%3A%20Wojciech%20Sirko%20and%20Emmanuel%20Asiedu%20Brempong%20and%20Juliana%20T.%20C.%20Marcos%20and%20Abigail%20Annkah%20and%20Abel%20Korme%20and%20Mohammed%20Alewi%20Hassen%20and%20Krishna%20Sapkota%20and%20Tomer%20Shekel%20and%20Abdoulaye%20Diack%20and%20Sella%20Nevo%20and%20Jason%20Hickey%20and%20John%20Quinn%0AAbstract%3A%20%20%20Mapping%20buildings%20and%20roads%20automatically%20with%20remote%20sensing%20typically%0Arequires%20high-resolution%20imagery%2C%20which%20is%20expensive%20to%20obtain%20and%20often%0Asparsely%20available.%20In%20this%20work%20we%20demonstrate%20how%20multiple%2010%20m%20resolution%0ASentinel-2%20images%20can%20be%20used%20to%20generate%2050%20cm%20resolution%20building%20and%20road%0Asegmentation%20masks.%20This%20is%20done%20by%20training%20a%20%60student%27%20model%20with%20access%20to%0ASentinel-2%20images%20to%20reproduce%20the%20predictions%20of%20a%20%60teacher%27%20model%20which%20has%0Aaccess%20to%20corresponding%20high-resolution%20imagery.%20While%20the%20predictions%20do%20not%0Ahave%20all%20the%20fine%20detail%20of%20the%20teacher%20model%2C%20we%20find%20that%20we%20are%20able%20to%0Aretain%20much%20of%20the%20performance%3A%20for%20building%20segmentation%20we%20achieve%2078.3%25%0AmIoU%2C%20compared%20to%20the%20high-resolution%20teacher%20model%20accuracy%20of%2085.3%25%20mIoU.%20We%0Aalso%20describe%20a%20related%20method%20for%20counting%20individual%20buildings%20in%20a%0ASentinel-2%20patch%20which%20achieves%20R%5E2%20%3D%200.91%20against%20true%20counts.%20This%20work%20opens%0Aup%20new%20possibilities%20for%20using%20freely%20available%20Sentinel-2%20imagery%20for%20a%20range%0Aof%20tasks%20that%20previously%20could%20only%20be%20done%20with%20high-resolution%20satellite%0Aimagery.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2310.11622v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DHigh-Resolution%2520Building%2520and%2520Road%2520Detection%2520from%2520Sentinel-2%26entry.906535625%3DWojciech%2520Sirko%2520and%2520Emmanuel%2520Asiedu%2520Brempong%2520and%2520Juliana%2520T.%2520C.%2520Marcos%2520and%2520Abigail%2520Annkah%2520and%2520Abel%2520Korme%2520and%2520Mohammed%2520Alewi%2520Hassen%2520and%2520Krishna%2520Sapkota%2520and%2520Tomer%2520Shekel%2520and%2520Abdoulaye%2520Diack%2520and%2520Sella%2520Nevo%2520and%2520Jason%2520Hickey%2520and%2520John%2520Quinn%26entry.1292438233%3D%2520%2520Mapping%2520buildings%2520and%2520roads%2520automatically%2520with%2520remote%2520sensing%2520typically%250Arequires%2520high-resolution%2520imagery%252C%2520which%2520is%2520expensive%2520to%2520obtain%2520and%2520often%250Asparsely%2520available.%2520In%2520this%2520work%2520we%2520demonstrate%2520how%2520multiple%252010%2520m%2520resolution%250ASentinel-2%2520images%2520can%2520be%2520used%2520to%2520generate%252050%2520cm%2520resolution%2520building%2520and%2520road%250Asegmentation%2520masks.%2520This%2520is%2520done%2520by%2520training%2520a%2520%2560student%2527%2520model%2520with%2520access%2520to%250ASentinel-2%2520images%2520to%2520reproduce%2520the%2520predictions%2520of%2520a%2520%2560teacher%2527%2520model%2520which%2520has%250Aaccess%2520to%2520corresponding%2520high-resolution%2520imagery.%2520While%2520the%2520predictions%2520do%2520not%250Ahave%2520all%2520the%2520fine%2520detail%2520of%2520the%2520teacher%2520model%252C%2520we%2520find%2520that%2520we%2520are%2520able%2520to%250Aretain%2520much%2520of%2520the%2520performance%253A%2520for%2520building%2520segmentation%2520we%2520achieve%252078.3%2525%250AmIoU%252C%2520compared%2520to%2520the%2520high-resolution%2520teacher%2520model%2520accuracy%2520of%252085.3%2525%2520mIoU.%2520We%250Aalso%2520describe%2520a%2520related%2520method%2520for%2520counting%2520individual%2520buildings%2520in%2520a%250ASentinel-2%2520patch%2520which%2520achieves%2520R%255E2%2520%253D%25200.91%2520against%2520true%2520counts.%2520This%2520work%2520opens%250Aup%2520new%2520possibilities%2520for%2520using%2520freely%2520available%2520Sentinel-2%2520imagery%2520for%2520a%2520range%250Aof%2520tasks%2520that%2520previously%2520could%2520only%2520be%2520done%2520with%2520high-resolution%2520satellite%250Aimagery.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2310.11622v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=High-Resolution%20Building%20and%20Road%20Detection%20from%20Sentinel-2&entry.906535625=Wojciech%20Sirko%20and%20Emmanuel%20Asiedu%20Brempong%20and%20Juliana%20T.%20C.%20Marcos%20and%20Abigail%20Annkah%20and%20Abel%20Korme%20and%20Mohammed%20Alewi%20Hassen%20and%20Krishna%20Sapkota%20and%20Tomer%20Shekel%20and%20Abdoulaye%20Diack%20and%20Sella%20Nevo%20and%20Jason%20Hickey%20and%20John%20Quinn&entry.1292438233=%20%20Mapping%20buildings%20and%20roads%20automatically%20with%20remote%20sensing%20typically%0Arequires%20high-resolution%20imagery%2C%20which%20is%20expensive%20to%20obtain%20and%20often%0Asparsely%20available.%20In%20this%20work%20we%20demonstrate%20how%20multiple%2010%20m%20resolution%0ASentinel-2%20images%20can%20be%20used%20to%20generate%2050%20cm%20resolution%20building%20and%20road%0Asegmentation%20masks.%20This%20is%20done%20by%20training%20a%20%60student%27%20model%20with%20access%20to%0ASentinel-2%20images%20to%20reproduce%20the%20predictions%20of%20a%20%60teacher%27%20model%20which%20has%0Aaccess%20to%20corresponding%20high-resolution%20imagery.%20While%20the%20predictions%20do%20not%0Ahave%20all%20the%20fine%20detail%20of%20the%20teacher%20model%2C%20we%20find%20that%20we%20are%20able%20to%0Aretain%20much%20of%20the%20performance%3A%20for%20building%20segmentation%20we%20achieve%2078.3%25%0AmIoU%2C%20compared%20to%20the%20high-resolution%20teacher%20model%20accuracy%20of%2085.3%25%20mIoU.%20We%0Aalso%20describe%20a%20related%20method%20for%20counting%20individual%20buildings%20in%20a%0ASentinel-2%20patch%20which%20achieves%20R%5E2%20%3D%200.91%20against%20true%20counts.%20This%20work%20opens%0Aup%20new%20possibilities%20for%20using%20freely%20available%20Sentinel-2%20imagery%20for%20a%20range%0Aof%20tasks%20that%20previously%20could%20only%20be%20done%20with%20high-resolution%20satellite%0Aimagery.%0A&entry.1838667208=http%3A//arxiv.org/abs/2310.11622v2&entry.124074799=Read"},
{"title": "Control when confidence is costly", "author": "Itzel Olivos-Castillo and Paul Schrater and Xaq Pitkow", "abstract": "  We develop a version of stochastic control that accounts for computational\ncosts of inference. Past studies identified efficient coding without control,\nor efficient control that neglects the cost of synthesizing information. Here\nwe combine these concepts into a framework where agents rationally approximate\ninference for efficient control. Specifically, we study Linear Quadratic\nGaussian (LQG) control with an added internal cost on the relative precision of\nthe posterior probability over the world state. This creates a trade-off: an\nagent can obtain more utility overall by sacrificing some task performance, if\ndoing so saves enough bits during inference. We discover that the rational\nstrategy that solves the joint inference and control problem goes through phase\ntransitions depending on the task demands, switching from a costly but optimal\ninference to a family of suboptimal inferences related by rotation\ntransformations, each misestimate the stability of the world. In all cases, the\nagent moves more to think less. This work provides a foundation for a new type\nof rational computations that could be used by both brains and machines for\nefficient but computationally constrained control.\n", "link": "http://arxiv.org/abs/2406.14427v1", "date": "2024-06-20", "relevancy": 1.9702, "topK": [{"title": "Safe and efficient training of a control agent", "link": "https://patents.google.com/patent/US11709462B2/en", "similarity": 0.5391}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.4934}, {"title": "Flexible Visual Recognition by Evidential Modeling of Confusion and Ignorance", "link": "http://openaccess.thecvf.com/content/ICCV2023/html/Fan_Flexible_Visual_Recognition_by_Evidential_Modeling_of_Confusion_and_Ignorance_ICCV_2023_paper.html", "similarity": 0.4457}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Control%20when%20confidence%20is%20costly&body=Title%3A%20Control%20when%20confidence%20is%20costly%0AAuthor%3A%20Itzel%20Olivos-Castillo%20and%20Paul%20Schrater%20and%20Xaq%20Pitkow%0AAbstract%3A%20%20%20We%20develop%20a%20version%20of%20stochastic%20control%20that%20accounts%20for%20computational%0Acosts%20of%20inference.%20Past%20studies%20identified%20efficient%20coding%20without%20control%2C%0Aor%20efficient%20control%20that%20neglects%20the%20cost%20of%20synthesizing%20information.%20Here%0Awe%20combine%20these%20concepts%20into%20a%20framework%20where%20agents%20rationally%20approximate%0Ainference%20for%20efficient%20control.%20Specifically%2C%20we%20study%20Linear%20Quadratic%0AGaussian%20%28LQG%29%20control%20with%20an%20added%20internal%20cost%20on%20the%20relative%20precision%20of%0Athe%20posterior%20probability%20over%20the%20world%20state.%20This%20creates%20a%20trade-off%3A%20an%0Aagent%20can%20obtain%20more%20utility%20overall%20by%20sacrificing%20some%20task%20performance%2C%20if%0Adoing%20so%20saves%20enough%20bits%20during%20inference.%20We%20discover%20that%20the%20rational%0Astrategy%20that%20solves%20the%20joint%20inference%20and%20control%20problem%20goes%20through%20phase%0Atransitions%20depending%20on%20the%20task%20demands%2C%20switching%20from%20a%20costly%20but%20optimal%0Ainference%20to%20a%20family%20of%20suboptimal%20inferences%20related%20by%20rotation%0Atransformations%2C%20each%20misestimate%20the%20stability%20of%20the%20world.%20In%20all%20cases%2C%20the%0Aagent%20moves%20more%20to%20think%20less.%20This%20work%20provides%20a%20foundation%20for%20a%20new%20type%0Aof%20rational%20computations%20that%20could%20be%20used%20by%20both%20brains%20and%20machines%20for%0Aefficient%20but%20computationally%20constrained%20control.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2406.14427v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DControl%2520when%2520confidence%2520is%2520costly%26entry.906535625%3DItzel%2520Olivos-Castillo%2520and%2520Paul%2520Schrater%2520and%2520Xaq%2520Pitkow%26entry.1292438233%3D%2520%2520We%2520develop%2520a%2520version%2520of%2520stochastic%2520control%2520that%2520accounts%2520for%2520computational%250Acosts%2520of%2520inference.%2520Past%2520studies%2520identified%2520efficient%2520coding%2520without%2520control%252C%250Aor%2520efficient%2520control%2520that%2520neglects%2520the%2520cost%2520of%2520synthesizing%2520information.%2520Here%250Awe%2520combine%2520these%2520concepts%2520into%2520a%2520framework%2520where%2520agents%2520rationally%2520approximate%250Ainference%2520for%2520efficient%2520control.%2520Specifically%252C%2520we%2520study%2520Linear%2520Quadratic%250AGaussian%2520%2528LQG%2529%2520control%2520with%2520an%2520added%2520internal%2520cost%2520on%2520the%2520relative%2520precision%2520of%250Athe%2520posterior%2520probability%2520over%2520the%2520world%2520state.%2520This%2520creates%2520a%2520trade-off%253A%2520an%250Aagent%2520can%2520obtain%2520more%2520utility%2520overall%2520by%2520sacrificing%2520some%2520task%2520performance%252C%2520if%250Adoing%2520so%2520saves%2520enough%2520bits%2520during%2520inference.%2520We%2520discover%2520that%2520the%2520rational%250Astrategy%2520that%2520solves%2520the%2520joint%2520inference%2520and%2520control%2520problem%2520goes%2520through%2520phase%250Atransitions%2520depending%2520on%2520the%2520task%2520demands%252C%2520switching%2520from%2520a%2520costly%2520but%2520optimal%250Ainference%2520to%2520a%2520family%2520of%2520suboptimal%2520inferences%2520related%2520by%2520rotation%250Atransformations%252C%2520each%2520misestimate%2520the%2520stability%2520of%2520the%2520world.%2520In%2520all%2520cases%252C%2520the%250Aagent%2520moves%2520more%2520to%2520think%2520less.%2520This%2520work%2520provides%2520a%2520foundation%2520for%2520a%2520new%2520type%250Aof%2520rational%2520computations%2520that%2520could%2520be%2520used%2520by%2520both%2520brains%2520and%2520machines%2520for%250Aefficient%2520but%2520computationally%2520constrained%2520control.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2406.14427v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Control%20when%20confidence%20is%20costly&entry.906535625=Itzel%20Olivos-Castillo%20and%20Paul%20Schrater%20and%20Xaq%20Pitkow&entry.1292438233=%20%20We%20develop%20a%20version%20of%20stochastic%20control%20that%20accounts%20for%20computational%0Acosts%20of%20inference.%20Past%20studies%20identified%20efficient%20coding%20without%20control%2C%0Aor%20efficient%20control%20that%20neglects%20the%20cost%20of%20synthesizing%20information.%20Here%0Awe%20combine%20these%20concepts%20into%20a%20framework%20where%20agents%20rationally%20approximate%0Ainference%20for%20efficient%20control.%20Specifically%2C%20we%20study%20Linear%20Quadratic%0AGaussian%20%28LQG%29%20control%20with%20an%20added%20internal%20cost%20on%20the%20relative%20precision%20of%0Athe%20posterior%20probability%20over%20the%20world%20state.%20This%20creates%20a%20trade-off%3A%20an%0Aagent%20can%20obtain%20more%20utility%20overall%20by%20sacrificing%20some%20task%20performance%2C%20if%0Adoing%20so%20saves%20enough%20bits%20during%20inference.%20We%20discover%20that%20the%20rational%0Astrategy%20that%20solves%20the%20joint%20inference%20and%20control%20problem%20goes%20through%20phase%0Atransitions%20depending%20on%20the%20task%20demands%2C%20switching%20from%20a%20costly%20but%20optimal%0Ainference%20to%20a%20family%20of%20suboptimal%20inferences%20related%20by%20rotation%0Atransformations%2C%20each%20misestimate%20the%20stability%20of%20the%20world.%20In%20all%20cases%2C%20the%0Aagent%20moves%20more%20to%20think%20less.%20This%20work%20provides%20a%20foundation%20for%20a%20new%20type%0Aof%20rational%20computations%20that%20could%20be%20used%20by%20both%20brains%20and%20machines%20for%0Aefficient%20but%20computationally%20constrained%20control.%0A&entry.1838667208=http%3A//arxiv.org/abs/2406.14427v1&entry.124074799=Read"},
{"title": "Rewarding What Matters: Step-by-Step Reinforcement Learning for\n  Task-Oriented Dialogue", "author": "Huifang Du and Shuqin Li and Minghao Wu and Xuejing Feng and Yuan-Fang Li and Haofen Wang", "abstract": "  Reinforcement learning (RL) is a powerful approach to enhance task-oriented\ndialogue (TOD) systems. However, existing RL methods tend to mainly focus on\ngeneration tasks, such as dialogue policy learning (DPL) or response generation\n(RG), while neglecting dialogue state tracking (DST) for understanding. This\nnarrow focus limits the systems to achieve globally optimal performance by\noverlooking the interdependence between understanding and generation.\nAdditionally, RL methods face challenges with sparse and delayed rewards, which\ncomplicates training and optimization. To address these issues, we extend RL\ninto both understanding and generation tasks by introducing step-by-step\nrewards throughout the token generation. The understanding reward increases as\nmore slots are correctly filled in DST, while the generation reward grows with\nthe accurate inclusion of user requests. Our approach provides a balanced\noptimization aligned with task completion. Experimental results demonstrate\nthat our approach effectively enhances the performance of TOD systems and\nachieves new state-of-the-art results on three widely used datasets, including\nMultiWOZ2.0, MultiWOZ2.1, and In-Car. Our approach also shows superior few-shot\nability in low-resource settings compared to current models.\n", "link": "http://arxiv.org/abs/2406.14457v1", "date": "2024-06-20", "relevancy": 1.9691, "topK": [{"title": "ToDo: Token Downsampling for Efficient Generation of High-Resolution\n  Images", "link": "http://arxiv.org/abs/2402.13573v2", "similarity": 0.5036}, {"title": "RoboEXP: Action-Conditioned Scene Graph via Interactive Exploration for\n  Robotic Manipulation", "link": "http://arxiv.org/abs/2402.15487v1", "similarity": 0.4922}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.481}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Rewarding%20What%20Matters%3A%20Step-by-Step%20Reinforcement%20Learning%20for%0A%20%20Task-Oriented%20Dialogue&body=Title%3A%20Rewarding%20What%20Matters%3A%20Step-by-Step%20Reinforcement%20Learning%20for%0A%20%20Task-Oriented%20Dialogue%0AAuthor%3A%20Huifang%20Du%20and%20Shuqin%20Li%20and%20Minghao%20Wu%20and%20Xuejing%20Feng%20and%20Yuan-Fang%20Li%20and%20Haofen%20Wang%0AAbstract%3A%20%20%20Reinforcement%20learning%20%28RL%29%20is%20a%20powerful%20approach%20to%20enhance%20task-oriented%0Adialogue%20%28TOD%29%20systems.%20However%2C%20existing%20RL%20methods%20tend%20to%20mainly%20focus%20on%0Ageneration%20tasks%2C%20such%20as%20dialogue%20policy%20learning%20%28DPL%29%20or%20response%20generation%0A%28RG%29%2C%20while%20neglecting%20dialogue%20state%20tracking%20%28DST%29%20for%20understanding.%20This%0Anarrow%20focus%20limits%20the%20systems%20to%20achieve%20globally%20optimal%20performance%20by%0Aoverlooking%20the%20interdependence%20between%20understanding%20and%20generation.%0AAdditionally%2C%20RL%20methods%20face%20challenges%20with%20sparse%20and%20delayed%20rewards%2C%20which%0Acomplicates%20training%20and%20optimization.%20To%20address%20these%20issues%2C%20we%20extend%20RL%0Ainto%20both%20understanding%20and%20generation%20tasks%20by%20introducing%20step-by-step%0Arewards%20throughout%20the%20token%20generation.%20The%20understanding%20reward%20increases%20as%0Amore%20slots%20are%20correctly%20filled%20in%20DST%2C%20while%20the%20generation%20reward%20grows%20with%0Athe%20accurate%20inclusion%20of%20user%20requests.%20Our%20approach%20provides%20a%20balanced%0Aoptimization%20aligned%20with%20task%20completion.%20Experimental%20results%20demonstrate%0Athat%20our%20approach%20effectively%20enhances%20the%20performance%20of%20TOD%20systems%20and%0Aachieves%20new%20state-of-the-art%20results%20on%20three%20widely%20used%20datasets%2C%20including%0AMultiWOZ2.0%2C%20MultiWOZ2.1%2C%20and%20In-Car.%20Our%20approach%20also%20shows%20superior%20few-shot%0Aability%20in%20low-resource%20settings%20compared%20to%20current%20models.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2406.14457v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DRewarding%2520What%2520Matters%253A%2520Step-by-Step%2520Reinforcement%2520Learning%2520for%250A%2520%2520Task-Oriented%2520Dialogue%26entry.906535625%3DHuifang%2520Du%2520and%2520Shuqin%2520Li%2520and%2520Minghao%2520Wu%2520and%2520Xuejing%2520Feng%2520and%2520Yuan-Fang%2520Li%2520and%2520Haofen%2520Wang%26entry.1292438233%3D%2520%2520Reinforcement%2520learning%2520%2528RL%2529%2520is%2520a%2520powerful%2520approach%2520to%2520enhance%2520task-oriented%250Adialogue%2520%2528TOD%2529%2520systems.%2520However%252C%2520existing%2520RL%2520methods%2520tend%2520to%2520mainly%2520focus%2520on%250Ageneration%2520tasks%252C%2520such%2520as%2520dialogue%2520policy%2520learning%2520%2528DPL%2529%2520or%2520response%2520generation%250A%2528RG%2529%252C%2520while%2520neglecting%2520dialogue%2520state%2520tracking%2520%2528DST%2529%2520for%2520understanding.%2520This%250Anarrow%2520focus%2520limits%2520the%2520systems%2520to%2520achieve%2520globally%2520optimal%2520performance%2520by%250Aoverlooking%2520the%2520interdependence%2520between%2520understanding%2520and%2520generation.%250AAdditionally%252C%2520RL%2520methods%2520face%2520challenges%2520with%2520sparse%2520and%2520delayed%2520rewards%252C%2520which%250Acomplicates%2520training%2520and%2520optimization.%2520To%2520address%2520these%2520issues%252C%2520we%2520extend%2520RL%250Ainto%2520both%2520understanding%2520and%2520generation%2520tasks%2520by%2520introducing%2520step-by-step%250Arewards%2520throughout%2520the%2520token%2520generation.%2520The%2520understanding%2520reward%2520increases%2520as%250Amore%2520slots%2520are%2520correctly%2520filled%2520in%2520DST%252C%2520while%2520the%2520generation%2520reward%2520grows%2520with%250Athe%2520accurate%2520inclusion%2520of%2520user%2520requests.%2520Our%2520approach%2520provides%2520a%2520balanced%250Aoptimization%2520aligned%2520with%2520task%2520completion.%2520Experimental%2520results%2520demonstrate%250Athat%2520our%2520approach%2520effectively%2520enhances%2520the%2520performance%2520of%2520TOD%2520systems%2520and%250Aachieves%2520new%2520state-of-the-art%2520results%2520on%2520three%2520widely%2520used%2520datasets%252C%2520including%250AMultiWOZ2.0%252C%2520MultiWOZ2.1%252C%2520and%2520In-Car.%2520Our%2520approach%2520also%2520shows%2520superior%2520few-shot%250Aability%2520in%2520low-resource%2520settings%2520compared%2520to%2520current%2520models.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2406.14457v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Rewarding%20What%20Matters%3A%20Step-by-Step%20Reinforcement%20Learning%20for%0A%20%20Task-Oriented%20Dialogue&entry.906535625=Huifang%20Du%20and%20Shuqin%20Li%20and%20Minghao%20Wu%20and%20Xuejing%20Feng%20and%20Yuan-Fang%20Li%20and%20Haofen%20Wang&entry.1292438233=%20%20Reinforcement%20learning%20%28RL%29%20is%20a%20powerful%20approach%20to%20enhance%20task-oriented%0Adialogue%20%28TOD%29%20systems.%20However%2C%20existing%20RL%20methods%20tend%20to%20mainly%20focus%20on%0Ageneration%20tasks%2C%20such%20as%20dialogue%20policy%20learning%20%28DPL%29%20or%20response%20generation%0A%28RG%29%2C%20while%20neglecting%20dialogue%20state%20tracking%20%28DST%29%20for%20understanding.%20This%0Anarrow%20focus%20limits%20the%20systems%20to%20achieve%20globally%20optimal%20performance%20by%0Aoverlooking%20the%20interdependence%20between%20understanding%20and%20generation.%0AAdditionally%2C%20RL%20methods%20face%20challenges%20with%20sparse%20and%20delayed%20rewards%2C%20which%0Acomplicates%20training%20and%20optimization.%20To%20address%20these%20issues%2C%20we%20extend%20RL%0Ainto%20both%20understanding%20and%20generation%20tasks%20by%20introducing%20step-by-step%0Arewards%20throughout%20the%20token%20generation.%20The%20understanding%20reward%20increases%20as%0Amore%20slots%20are%20correctly%20filled%20in%20DST%2C%20while%20the%20generation%20reward%20grows%20with%0Athe%20accurate%20inclusion%20of%20user%20requests.%20Our%20approach%20provides%20a%20balanced%0Aoptimization%20aligned%20with%20task%20completion.%20Experimental%20results%20demonstrate%0Athat%20our%20approach%20effectively%20enhances%20the%20performance%20of%20TOD%20systems%20and%0Aachieves%20new%20state-of-the-art%20results%20on%20three%20widely%20used%20datasets%2C%20including%0AMultiWOZ2.0%2C%20MultiWOZ2.1%2C%20and%20In-Car.%20Our%20approach%20also%20shows%20superior%20few-shot%0Aability%20in%20low-resource%20settings%20compared%20to%20current%20models.%0A&entry.1838667208=http%3A//arxiv.org/abs/2406.14457v1&entry.124074799=Read"},
{"title": "Imputation of missing values in multi-view data", "author": "Wouter van Loon and Marjolein Fokkema and Frank de Vos and Marisa Koini and Reinhold Schmidt and Mark de Rooij", "abstract": "  Data for which a set of objects is described by multiple distinct feature\nsets (called views) is known as multi-view data. When missing values occur in\nmulti-view data, all features in a view are likely to be missing\nsimultaneously. This may lead to very large quantities of missing data which,\nespecially when combined with high-dimensionality, can make the application of\nconditional imputation methods computationally infeasible. However, the\nmulti-view structure could be leveraged to reduce the complexity and\ncomputational load of imputation. We introduce a new imputation method based on\nthe existing stacked penalized logistic regression (StaPLR) algorithm for\nmulti-view learning. It performs imputation in a dimension-reduced space to\naddress computational challenges inherent to the multi-view context. We compare\nthe performance of the new imputation method with several existing imputation\nalgorithms in simulated data sets and a real data application. The results show\nthat the new imputation method leads to competitive results at a much lower\ncomputational cost, and makes the use of advanced imputation algorithms such as\nmissForest and predictive mean matching possible in settings where they would\notherwise be computationally infeasible.\n", "link": "http://arxiv.org/abs/2210.14484v4", "date": "2024-06-20", "relevancy": 1.9619, "topK": [{"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.5239}, {"title": "CAT3D: Create Anything in 3D with Multi-View Diffusion Models", "link": "http://arxiv.org/abs/2405.10314v1", "similarity": 0.4838}, {"title": "CAT3D: Create Anything in 3D with Multi-View Diffusion Models", "link": "http://arxiv.org/abs/2405.10314v1", "similarity": 0.4838}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Imputation%20of%20missing%20values%20in%20multi-view%20data&body=Title%3A%20Imputation%20of%20missing%20values%20in%20multi-view%20data%0AAuthor%3A%20Wouter%20van%20Loon%20and%20Marjolein%20Fokkema%20and%20Frank%20de%20Vos%20and%20Marisa%20Koini%20and%20Reinhold%20Schmidt%20and%20Mark%20de%20Rooij%0AAbstract%3A%20%20%20Data%20for%20which%20a%20set%20of%20objects%20is%20described%20by%20multiple%20distinct%20feature%0Asets%20%28called%20views%29%20is%20known%20as%20multi-view%20data.%20When%20missing%20values%20occur%20in%0Amulti-view%20data%2C%20all%20features%20in%20a%20view%20are%20likely%20to%20be%20missing%0Asimultaneously.%20This%20may%20lead%20to%20very%20large%20quantities%20of%20missing%20data%20which%2C%0Aespecially%20when%20combined%20with%20high-dimensionality%2C%20can%20make%20the%20application%20of%0Aconditional%20imputation%20methods%20computationally%20infeasible.%20However%2C%20the%0Amulti-view%20structure%20could%20be%20leveraged%20to%20reduce%20the%20complexity%20and%0Acomputational%20load%20of%20imputation.%20We%20introduce%20a%20new%20imputation%20method%20based%20on%0Athe%20existing%20stacked%20penalized%20logistic%20regression%20%28StaPLR%29%20algorithm%20for%0Amulti-view%20learning.%20It%20performs%20imputation%20in%20a%20dimension-reduced%20space%20to%0Aaddress%20computational%20challenges%20inherent%20to%20the%20multi-view%20context.%20We%20compare%0Athe%20performance%20of%20the%20new%20imputation%20method%20with%20several%20existing%20imputation%0Aalgorithms%20in%20simulated%20data%20sets%20and%20a%20real%20data%20application.%20The%20results%20show%0Athat%20the%20new%20imputation%20method%20leads%20to%20competitive%20results%20at%20a%20much%20lower%0Acomputational%20cost%2C%20and%20makes%20the%20use%20of%20advanced%20imputation%20algorithms%20such%20as%0AmissForest%20and%20predictive%20mean%20matching%20possible%20in%20settings%20where%20they%20would%0Aotherwise%20be%20computationally%20infeasible.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2210.14484v4%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DImputation%2520of%2520missing%2520values%2520in%2520multi-view%2520data%26entry.906535625%3DWouter%2520van%2520Loon%2520and%2520Marjolein%2520Fokkema%2520and%2520Frank%2520de%2520Vos%2520and%2520Marisa%2520Koini%2520and%2520Reinhold%2520Schmidt%2520and%2520Mark%2520de%2520Rooij%26entry.1292438233%3D%2520%2520Data%2520for%2520which%2520a%2520set%2520of%2520objects%2520is%2520described%2520by%2520multiple%2520distinct%2520feature%250Asets%2520%2528called%2520views%2529%2520is%2520known%2520as%2520multi-view%2520data.%2520When%2520missing%2520values%2520occur%2520in%250Amulti-view%2520data%252C%2520all%2520features%2520in%2520a%2520view%2520are%2520likely%2520to%2520be%2520missing%250Asimultaneously.%2520This%2520may%2520lead%2520to%2520very%2520large%2520quantities%2520of%2520missing%2520data%2520which%252C%250Aespecially%2520when%2520combined%2520with%2520high-dimensionality%252C%2520can%2520make%2520the%2520application%2520of%250Aconditional%2520imputation%2520methods%2520computationally%2520infeasible.%2520However%252C%2520the%250Amulti-view%2520structure%2520could%2520be%2520leveraged%2520to%2520reduce%2520the%2520complexity%2520and%250Acomputational%2520load%2520of%2520imputation.%2520We%2520introduce%2520a%2520new%2520imputation%2520method%2520based%2520on%250Athe%2520existing%2520stacked%2520penalized%2520logistic%2520regression%2520%2528StaPLR%2529%2520algorithm%2520for%250Amulti-view%2520learning.%2520It%2520performs%2520imputation%2520in%2520a%2520dimension-reduced%2520space%2520to%250Aaddress%2520computational%2520challenges%2520inherent%2520to%2520the%2520multi-view%2520context.%2520We%2520compare%250Athe%2520performance%2520of%2520the%2520new%2520imputation%2520method%2520with%2520several%2520existing%2520imputation%250Aalgorithms%2520in%2520simulated%2520data%2520sets%2520and%2520a%2520real%2520data%2520application.%2520The%2520results%2520show%250Athat%2520the%2520new%2520imputation%2520method%2520leads%2520to%2520competitive%2520results%2520at%2520a%2520much%2520lower%250Acomputational%2520cost%252C%2520and%2520makes%2520the%2520use%2520of%2520advanced%2520imputation%2520algorithms%2520such%2520as%250AmissForest%2520and%2520predictive%2520mean%2520matching%2520possible%2520in%2520settings%2520where%2520they%2520would%250Aotherwise%2520be%2520computationally%2520infeasible.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2210.14484v4%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Imputation%20of%20missing%20values%20in%20multi-view%20data&entry.906535625=Wouter%20van%20Loon%20and%20Marjolein%20Fokkema%20and%20Frank%20de%20Vos%20and%20Marisa%20Koini%20and%20Reinhold%20Schmidt%20and%20Mark%20de%20Rooij&entry.1292438233=%20%20Data%20for%20which%20a%20set%20of%20objects%20is%20described%20by%20multiple%20distinct%20feature%0Asets%20%28called%20views%29%20is%20known%20as%20multi-view%20data.%20When%20missing%20values%20occur%20in%0Amulti-view%20data%2C%20all%20features%20in%20a%20view%20are%20likely%20to%20be%20missing%0Asimultaneously.%20This%20may%20lead%20to%20very%20large%20quantities%20of%20missing%20data%20which%2C%0Aespecially%20when%20combined%20with%20high-dimensionality%2C%20can%20make%20the%20application%20of%0Aconditional%20imputation%20methods%20computationally%20infeasible.%20However%2C%20the%0Amulti-view%20structure%20could%20be%20leveraged%20to%20reduce%20the%20complexity%20and%0Acomputational%20load%20of%20imputation.%20We%20introduce%20a%20new%20imputation%20method%20based%20on%0Athe%20existing%20stacked%20penalized%20logistic%20regression%20%28StaPLR%29%20algorithm%20for%0Amulti-view%20learning.%20It%20performs%20imputation%20in%20a%20dimension-reduced%20space%20to%0Aaddress%20computational%20challenges%20inherent%20to%20the%20multi-view%20context.%20We%20compare%0Athe%20performance%20of%20the%20new%20imputation%20method%20with%20several%20existing%20imputation%0Aalgorithms%20in%20simulated%20data%20sets%20and%20a%20real%20data%20application.%20The%20results%20show%0Athat%20the%20new%20imputation%20method%20leads%20to%20competitive%20results%20at%20a%20much%20lower%0Acomputational%20cost%2C%20and%20makes%20the%20use%20of%20advanced%20imputation%20algorithms%20such%20as%0AmissForest%20and%20predictive%20mean%20matching%20possible%20in%20settings%20where%20they%20would%0Aotherwise%20be%20computationally%20infeasible.%0A&entry.1838667208=http%3A//arxiv.org/abs/2210.14484v4&entry.124074799=Read"},
{"title": "Keep Moving: identifying task-relevant subspaces to maximise plasticity\n  for newly learned tasks", "author": "Daniel Anthes and Sushrut Thorat and Peter K\u00f6nig and Tim C. Kietzmann", "abstract": "  Continual learning algorithms strive to acquire new knowledge while\npreserving prior information. Often, these algorithms emphasise stability and\nrestrict network updates upon learning new tasks. In many cases, such\nrestrictions come at a cost to the model's plasticity, i.e. the model's ability\nto adapt to the requirements of a new task. But is all change detrimental?\nHere, we approach this question by proposing that activation spaces in neural\nnetworks can be decomposed into two subspaces: a readout range in which change\naffects prior tasks and a null space in which change does not alter prior\nperformance. Based on experiments with this novel technique, we show that,\nindeed, not all activation change is associated with forgetting. Instead, only\nchange in the subspace visible to the readout of a task can lead to decreased\nstability, while restricting change outside of this subspace is associated only\nwith a loss of plasticity. Analysing various commonly used algorithms, we show\nthat regularisation-based techniques do not fully disentangle the two spaces\nand, as a result, restrict plasticity more than need be. We expand our results\nby investigating a linear model in which we can manipulate learning in the two\nsubspaces directly and thus causally link activation changes to stability and\nplasticity. For hierarchical, nonlinear cases, we present an approximation that\nenables us to estimate functionally relevant subspaces at every layer of a deep\nnonlinear network, corroborating our previous insights. Together, this work\nprovides novel means to derive insights into the mechanisms behind stability\nand plasticity in continual learning and may serve as a diagnostic tool to\nguide developments of future continual learning algorithms that stabilise\ninference while allowing maximal space for learning.\n", "link": "http://arxiv.org/abs/2310.04741v6", "date": "2024-06-20", "relevancy": 1.9585, "topK": [{"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.5145}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.4878}, {"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.4815}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Keep%20Moving%3A%20identifying%20task-relevant%20subspaces%20to%20maximise%20plasticity%0A%20%20for%20newly%20learned%20tasks&body=Title%3A%20Keep%20Moving%3A%20identifying%20task-relevant%20subspaces%20to%20maximise%20plasticity%0A%20%20for%20newly%20learned%20tasks%0AAuthor%3A%20Daniel%20Anthes%20and%20Sushrut%20Thorat%20and%20Peter%20K%C3%B6nig%20and%20Tim%20C.%20Kietzmann%0AAbstract%3A%20%20%20Continual%20learning%20algorithms%20strive%20to%20acquire%20new%20knowledge%20while%0Apreserving%20prior%20information.%20Often%2C%20these%20algorithms%20emphasise%20stability%20and%0Arestrict%20network%20updates%20upon%20learning%20new%20tasks.%20In%20many%20cases%2C%20such%0Arestrictions%20come%20at%20a%20cost%20to%20the%20model%27s%20plasticity%2C%20i.e.%20the%20model%27s%20ability%0Ato%20adapt%20to%20the%20requirements%20of%20a%20new%20task.%20But%20is%20all%20change%20detrimental%3F%0AHere%2C%20we%20approach%20this%20question%20by%20proposing%20that%20activation%20spaces%20in%20neural%0Anetworks%20can%20be%20decomposed%20into%20two%20subspaces%3A%20a%20readout%20range%20in%20which%20change%0Aaffects%20prior%20tasks%20and%20a%20null%20space%20in%20which%20change%20does%20not%20alter%20prior%0Aperformance.%20Based%20on%20experiments%20with%20this%20novel%20technique%2C%20we%20show%20that%2C%0Aindeed%2C%20not%20all%20activation%20change%20is%20associated%20with%20forgetting.%20Instead%2C%20only%0Achange%20in%20the%20subspace%20visible%20to%20the%20readout%20of%20a%20task%20can%20lead%20to%20decreased%0Astability%2C%20while%20restricting%20change%20outside%20of%20this%20subspace%20is%20associated%20only%0Awith%20a%20loss%20of%20plasticity.%20Analysing%20various%20commonly%20used%20algorithms%2C%20we%20show%0Athat%20regularisation-based%20techniques%20do%20not%20fully%20disentangle%20the%20two%20spaces%0Aand%2C%20as%20a%20result%2C%20restrict%20plasticity%20more%20than%20need%20be.%20We%20expand%20our%20results%0Aby%20investigating%20a%20linear%20model%20in%20which%20we%20can%20manipulate%20learning%20in%20the%20two%0Asubspaces%20directly%20and%20thus%20causally%20link%20activation%20changes%20to%20stability%20and%0Aplasticity.%20For%20hierarchical%2C%20nonlinear%20cases%2C%20we%20present%20an%20approximation%20that%0Aenables%20us%20to%20estimate%20functionally%20relevant%20subspaces%20at%20every%20layer%20of%20a%20deep%0Anonlinear%20network%2C%20corroborating%20our%20previous%20insights.%20Together%2C%20this%20work%0Aprovides%20novel%20means%20to%20derive%20insights%20into%20the%20mechanisms%20behind%20stability%0Aand%20plasticity%20in%20continual%20learning%20and%20may%20serve%20as%20a%20diagnostic%20tool%20to%0Aguide%20developments%20of%20future%20continual%20learning%20algorithms%20that%20stabilise%0Ainference%20while%20allowing%20maximal%20space%20for%20learning.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2310.04741v6%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DKeep%2520Moving%253A%2520identifying%2520task-relevant%2520subspaces%2520to%2520maximise%2520plasticity%250A%2520%2520for%2520newly%2520learned%2520tasks%26entry.906535625%3DDaniel%2520Anthes%2520and%2520Sushrut%2520Thorat%2520and%2520Peter%2520K%25C3%25B6nig%2520and%2520Tim%2520C.%2520Kietzmann%26entry.1292438233%3D%2520%2520Continual%2520learning%2520algorithms%2520strive%2520to%2520acquire%2520new%2520knowledge%2520while%250Apreserving%2520prior%2520information.%2520Often%252C%2520these%2520algorithms%2520emphasise%2520stability%2520and%250Arestrict%2520network%2520updates%2520upon%2520learning%2520new%2520tasks.%2520In%2520many%2520cases%252C%2520such%250Arestrictions%2520come%2520at%2520a%2520cost%2520to%2520the%2520model%2527s%2520plasticity%252C%2520i.e.%2520the%2520model%2527s%2520ability%250Ato%2520adapt%2520to%2520the%2520requirements%2520of%2520a%2520new%2520task.%2520But%2520is%2520all%2520change%2520detrimental%253F%250AHere%252C%2520we%2520approach%2520this%2520question%2520by%2520proposing%2520that%2520activation%2520spaces%2520in%2520neural%250Anetworks%2520can%2520be%2520decomposed%2520into%2520two%2520subspaces%253A%2520a%2520readout%2520range%2520in%2520which%2520change%250Aaffects%2520prior%2520tasks%2520and%2520a%2520null%2520space%2520in%2520which%2520change%2520does%2520not%2520alter%2520prior%250Aperformance.%2520Based%2520on%2520experiments%2520with%2520this%2520novel%2520technique%252C%2520we%2520show%2520that%252C%250Aindeed%252C%2520not%2520all%2520activation%2520change%2520is%2520associated%2520with%2520forgetting.%2520Instead%252C%2520only%250Achange%2520in%2520the%2520subspace%2520visible%2520to%2520the%2520readout%2520of%2520a%2520task%2520can%2520lead%2520to%2520decreased%250Astability%252C%2520while%2520restricting%2520change%2520outside%2520of%2520this%2520subspace%2520is%2520associated%2520only%250Awith%2520a%2520loss%2520of%2520plasticity.%2520Analysing%2520various%2520commonly%2520used%2520algorithms%252C%2520we%2520show%250Athat%2520regularisation-based%2520techniques%2520do%2520not%2520fully%2520disentangle%2520the%2520two%2520spaces%250Aand%252C%2520as%2520a%2520result%252C%2520restrict%2520plasticity%2520more%2520than%2520need%2520be.%2520We%2520expand%2520our%2520results%250Aby%2520investigating%2520a%2520linear%2520model%2520in%2520which%2520we%2520can%2520manipulate%2520learning%2520in%2520the%2520two%250Asubspaces%2520directly%2520and%2520thus%2520causally%2520link%2520activation%2520changes%2520to%2520stability%2520and%250Aplasticity.%2520For%2520hierarchical%252C%2520nonlinear%2520cases%252C%2520we%2520present%2520an%2520approximation%2520that%250Aenables%2520us%2520to%2520estimate%2520functionally%2520relevant%2520subspaces%2520at%2520every%2520layer%2520of%2520a%2520deep%250Anonlinear%2520network%252C%2520corroborating%2520our%2520previous%2520insights.%2520Together%252C%2520this%2520work%250Aprovides%2520novel%2520means%2520to%2520derive%2520insights%2520into%2520the%2520mechanisms%2520behind%2520stability%250Aand%2520plasticity%2520in%2520continual%2520learning%2520and%2520may%2520serve%2520as%2520a%2520diagnostic%2520tool%2520to%250Aguide%2520developments%2520of%2520future%2520continual%2520learning%2520algorithms%2520that%2520stabilise%250Ainference%2520while%2520allowing%2520maximal%2520space%2520for%2520learning.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2310.04741v6%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Keep%20Moving%3A%20identifying%20task-relevant%20subspaces%20to%20maximise%20plasticity%0A%20%20for%20newly%20learned%20tasks&entry.906535625=Daniel%20Anthes%20and%20Sushrut%20Thorat%20and%20Peter%20K%C3%B6nig%20and%20Tim%20C.%20Kietzmann&entry.1292438233=%20%20Continual%20learning%20algorithms%20strive%20to%20acquire%20new%20knowledge%20while%0Apreserving%20prior%20information.%20Often%2C%20these%20algorithms%20emphasise%20stability%20and%0Arestrict%20network%20updates%20upon%20learning%20new%20tasks.%20In%20many%20cases%2C%20such%0Arestrictions%20come%20at%20a%20cost%20to%20the%20model%27s%20plasticity%2C%20i.e.%20the%20model%27s%20ability%0Ato%20adapt%20to%20the%20requirements%20of%20a%20new%20task.%20But%20is%20all%20change%20detrimental%3F%0AHere%2C%20we%20approach%20this%20question%20by%20proposing%20that%20activation%20spaces%20in%20neural%0Anetworks%20can%20be%20decomposed%20into%20two%20subspaces%3A%20a%20readout%20range%20in%20which%20change%0Aaffects%20prior%20tasks%20and%20a%20null%20space%20in%20which%20change%20does%20not%20alter%20prior%0Aperformance.%20Based%20on%20experiments%20with%20this%20novel%20technique%2C%20we%20show%20that%2C%0Aindeed%2C%20not%20all%20activation%20change%20is%20associated%20with%20forgetting.%20Instead%2C%20only%0Achange%20in%20the%20subspace%20visible%20to%20the%20readout%20of%20a%20task%20can%20lead%20to%20decreased%0Astability%2C%20while%20restricting%20change%20outside%20of%20this%20subspace%20is%20associated%20only%0Awith%20a%20loss%20of%20plasticity.%20Analysing%20various%20commonly%20used%20algorithms%2C%20we%20show%0Athat%20regularisation-based%20techniques%20do%20not%20fully%20disentangle%20the%20two%20spaces%0Aand%2C%20as%20a%20result%2C%20restrict%20plasticity%20more%20than%20need%20be.%20We%20expand%20our%20results%0Aby%20investigating%20a%20linear%20model%20in%20which%20we%20can%20manipulate%20learning%20in%20the%20two%0Asubspaces%20directly%20and%20thus%20causally%20link%20activation%20changes%20to%20stability%20and%0Aplasticity.%20For%20hierarchical%2C%20nonlinear%20cases%2C%20we%20present%20an%20approximation%20that%0Aenables%20us%20to%20estimate%20functionally%20relevant%20subspaces%20at%20every%20layer%20of%20a%20deep%0Anonlinear%20network%2C%20corroborating%20our%20previous%20insights.%20Together%2C%20this%20work%0Aprovides%20novel%20means%20to%20derive%20insights%20into%20the%20mechanisms%20behind%20stability%0Aand%20plasticity%20in%20continual%20learning%20and%20may%20serve%20as%20a%20diagnostic%20tool%20to%0Aguide%20developments%20of%20future%20continual%20learning%20algorithms%20that%20stabilise%0Ainference%20while%20allowing%20maximal%20space%20for%20learning.%0A&entry.1838667208=http%3A//arxiv.org/abs/2310.04741v6&entry.124074799=Read"},
{"title": "DASB -- Discrete Audio and Speech Benchmark", "author": "Pooneh Mousavi and Luca Della Libera and Jarod Duret and Artem Ploujnikov and Cem Subakan and Mirco Ravanelli", "abstract": "  Discrete audio tokens have recently gained considerable attention for their\npotential to connect audio and language processing, enabling the creation of\nmodern multimodal large language models. Ideal audio tokens must effectively\npreserve phonetic and semantic content along with paralinguistic information,\nspeaker identity, and other details. While several types of audio tokens have\nbeen recently proposed, identifying the optimal tokenizer for various tasks is\nchallenging due to the inconsistent evaluation settings in existing studies. To\naddress this gap, we release the Discrete Audio and Speech Benchmark (DASB), a\ncomprehensive leaderboard for benchmarking discrete audio tokens across a wide\nrange of discriminative tasks, including speech recognition, speaker\nidentification and verification, emotion recognition, keyword spotting, and\nintent classification, as well as generative tasks such as speech enhancement,\nseparation, and text-to-speech. Our results show that, on average, semantic\ntokens outperform compression tokens across most discriminative and generative\ntasks. However, the performance gap between semantic tokens and standard\ncontinuous representations remains substantial, highlighting the need for\nfurther research in this field.\n", "link": "http://arxiv.org/abs/2406.14294v1", "date": "2024-06-20", "relevancy": 1.9555, "topK": [{"title": "ToDo: Token Downsampling for Efficient Generation of High-Resolution\n  Images", "link": "http://arxiv.org/abs/2402.13573v2", "similarity": 0.5179}, {"title": "Breadcrumbs: Adversarial class-balanced sampling for long-tailed recognition", "link": "https://link.springer.com/chapter/10.1007/978-3-031-20053-3_37", "similarity": 0.4703}, {"title": "SDXS: Real-Time One-Step Latent Diffusion Models with Image Conditions", "link": "http://arxiv.org/abs/2403.16627v1", "similarity": 0.4626}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20DASB%20--%20Discrete%20Audio%20and%20Speech%20Benchmark&body=Title%3A%20DASB%20--%20Discrete%20Audio%20and%20Speech%20Benchmark%0AAuthor%3A%20Pooneh%20Mousavi%20and%20Luca%20Della%20Libera%20and%20Jarod%20Duret%20and%20Artem%20Ploujnikov%20and%20Cem%20Subakan%20and%20Mirco%20Ravanelli%0AAbstract%3A%20%20%20Discrete%20audio%20tokens%20have%20recently%20gained%20considerable%20attention%20for%20their%0Apotential%20to%20connect%20audio%20and%20language%20processing%2C%20enabling%20the%20creation%20of%0Amodern%20multimodal%20large%20language%20models.%20Ideal%20audio%20tokens%20must%20effectively%0Apreserve%20phonetic%20and%20semantic%20content%20along%20with%20paralinguistic%20information%2C%0Aspeaker%20identity%2C%20and%20other%20details.%20While%20several%20types%20of%20audio%20tokens%20have%0Abeen%20recently%20proposed%2C%20identifying%20the%20optimal%20tokenizer%20for%20various%20tasks%20is%0Achallenging%20due%20to%20the%20inconsistent%20evaluation%20settings%20in%20existing%20studies.%20To%0Aaddress%20this%20gap%2C%20we%20release%20the%20Discrete%20Audio%20and%20Speech%20Benchmark%20%28DASB%29%2C%20a%0Acomprehensive%20leaderboard%20for%20benchmarking%20discrete%20audio%20tokens%20across%20a%20wide%0Arange%20of%20discriminative%20tasks%2C%20including%20speech%20recognition%2C%20speaker%0Aidentification%20and%20verification%2C%20emotion%20recognition%2C%20keyword%20spotting%2C%20and%0Aintent%20classification%2C%20as%20well%20as%20generative%20tasks%20such%20as%20speech%20enhancement%2C%0Aseparation%2C%20and%20text-to-speech.%20Our%20results%20show%20that%2C%20on%20average%2C%20semantic%0Atokens%20outperform%20compression%20tokens%20across%20most%20discriminative%20and%20generative%0Atasks.%20However%2C%20the%20performance%20gap%20between%20semantic%20tokens%20and%20standard%0Acontinuous%20representations%20remains%20substantial%2C%20highlighting%20the%20need%20for%0Afurther%20research%20in%20this%20field.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2406.14294v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DDASB%2520--%2520Discrete%2520Audio%2520and%2520Speech%2520Benchmark%26entry.906535625%3DPooneh%2520Mousavi%2520and%2520Luca%2520Della%2520Libera%2520and%2520Jarod%2520Duret%2520and%2520Artem%2520Ploujnikov%2520and%2520Cem%2520Subakan%2520and%2520Mirco%2520Ravanelli%26entry.1292438233%3D%2520%2520Discrete%2520audio%2520tokens%2520have%2520recently%2520gained%2520considerable%2520attention%2520for%2520their%250Apotential%2520to%2520connect%2520audio%2520and%2520language%2520processing%252C%2520enabling%2520the%2520creation%2520of%250Amodern%2520multimodal%2520large%2520language%2520models.%2520Ideal%2520audio%2520tokens%2520must%2520effectively%250Apreserve%2520phonetic%2520and%2520semantic%2520content%2520along%2520with%2520paralinguistic%2520information%252C%250Aspeaker%2520identity%252C%2520and%2520other%2520details.%2520While%2520several%2520types%2520of%2520audio%2520tokens%2520have%250Abeen%2520recently%2520proposed%252C%2520identifying%2520the%2520optimal%2520tokenizer%2520for%2520various%2520tasks%2520is%250Achallenging%2520due%2520to%2520the%2520inconsistent%2520evaluation%2520settings%2520in%2520existing%2520studies.%2520To%250Aaddress%2520this%2520gap%252C%2520we%2520release%2520the%2520Discrete%2520Audio%2520and%2520Speech%2520Benchmark%2520%2528DASB%2529%252C%2520a%250Acomprehensive%2520leaderboard%2520for%2520benchmarking%2520discrete%2520audio%2520tokens%2520across%2520a%2520wide%250Arange%2520of%2520discriminative%2520tasks%252C%2520including%2520speech%2520recognition%252C%2520speaker%250Aidentification%2520and%2520verification%252C%2520emotion%2520recognition%252C%2520keyword%2520spotting%252C%2520and%250Aintent%2520classification%252C%2520as%2520well%2520as%2520generative%2520tasks%2520such%2520as%2520speech%2520enhancement%252C%250Aseparation%252C%2520and%2520text-to-speech.%2520Our%2520results%2520show%2520that%252C%2520on%2520average%252C%2520semantic%250Atokens%2520outperform%2520compression%2520tokens%2520across%2520most%2520discriminative%2520and%2520generative%250Atasks.%2520However%252C%2520the%2520performance%2520gap%2520between%2520semantic%2520tokens%2520and%2520standard%250Acontinuous%2520representations%2520remains%2520substantial%252C%2520highlighting%2520the%2520need%2520for%250Afurther%2520research%2520in%2520this%2520field.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2406.14294v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=DASB%20--%20Discrete%20Audio%20and%20Speech%20Benchmark&entry.906535625=Pooneh%20Mousavi%20and%20Luca%20Della%20Libera%20and%20Jarod%20Duret%20and%20Artem%20Ploujnikov%20and%20Cem%20Subakan%20and%20Mirco%20Ravanelli&entry.1292438233=%20%20Discrete%20audio%20tokens%20have%20recently%20gained%20considerable%20attention%20for%20their%0Apotential%20to%20connect%20audio%20and%20language%20processing%2C%20enabling%20the%20creation%20of%0Amodern%20multimodal%20large%20language%20models.%20Ideal%20audio%20tokens%20must%20effectively%0Apreserve%20phonetic%20and%20semantic%20content%20along%20with%20paralinguistic%20information%2C%0Aspeaker%20identity%2C%20and%20other%20details.%20While%20several%20types%20of%20audio%20tokens%20have%0Abeen%20recently%20proposed%2C%20identifying%20the%20optimal%20tokenizer%20for%20various%20tasks%20is%0Achallenging%20due%20to%20the%20inconsistent%20evaluation%20settings%20in%20existing%20studies.%20To%0Aaddress%20this%20gap%2C%20we%20release%20the%20Discrete%20Audio%20and%20Speech%20Benchmark%20%28DASB%29%2C%20a%0Acomprehensive%20leaderboard%20for%20benchmarking%20discrete%20audio%20tokens%20across%20a%20wide%0Arange%20of%20discriminative%20tasks%2C%20including%20speech%20recognition%2C%20speaker%0Aidentification%20and%20verification%2C%20emotion%20recognition%2C%20keyword%20spotting%2C%20and%0Aintent%20classification%2C%20as%20well%20as%20generative%20tasks%20such%20as%20speech%20enhancement%2C%0Aseparation%2C%20and%20text-to-speech.%20Our%20results%20show%20that%2C%20on%20average%2C%20semantic%0Atokens%20outperform%20compression%20tokens%20across%20most%20discriminative%20and%20generative%0Atasks.%20However%2C%20the%20performance%20gap%20between%20semantic%20tokens%20and%20standard%0Acontinuous%20representations%20remains%20substantial%2C%20highlighting%20the%20need%20for%0Afurther%20research%20in%20this%20field.%0A&entry.1838667208=http%3A//arxiv.org/abs/2406.14294v1&entry.124074799=Read"},
{"title": "Identifying User Goals from UI Trajectories", "author": "Omri Berkovitch and Sapir Caduri and Noam Kahlon and Anatoly Efros and Avi Caciularu and Ido Dagan", "abstract": "  Autonomous agents that interact with graphical user interfaces (GUIs) hold\nsignificant potential for enhancing user experiences. To further improve these\nexperiences, agents need to be personalized and proactive. By effectively\ncomprehending user intentions through their actions and interactions with GUIs,\nagents will be better positioned to achieve these goals. This paper introduces\nthe task of goal identification from observed UI trajectories, aiming to infer\nthe user's intended task based on their GUI interactions. We propose a novel\nevaluation metric to assess whether two task descriptions are paraphrases\nwithin a specific UI environment. By Leveraging the inverse relation with the\nUI automation task, we utilized the Android-In-The-Wild and Mind2Web datasets\nfor our experiments. Using our metric and these datasets, we conducted several\nexperiments comparing the performance of humans and state-of-the-art models,\nspecifically GPT-4 and Gemini-1.5 Pro. Our results show that Gemini performs\nbetter than GPT but still underperforms compared to humans, indicating\nsignificant room for improvement.\n", "link": "http://arxiv.org/abs/2406.14314v1", "date": "2024-06-20", "relevancy": 1.9554, "topK": [{"title": "Egocentric Computer Vision for Hands-Free Robotic Wheelchair Navigation", "link": "https://link.springer.com/article/10.1007/s10846-023-01807-4", "similarity": 0.4897}, {"title": "UGG: Unified Generative Grasping", "link": "https://arxiv.org/abs/2311.16917", "similarity": 0.4885}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.4874}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Identifying%20User%20Goals%20from%20UI%20Trajectories&body=Title%3A%20Identifying%20User%20Goals%20from%20UI%20Trajectories%0AAuthor%3A%20Omri%20Berkovitch%20and%20Sapir%20Caduri%20and%20Noam%20Kahlon%20and%20Anatoly%20Efros%20and%20Avi%20Caciularu%20and%20Ido%20Dagan%0AAbstract%3A%20%20%20Autonomous%20agents%20that%20interact%20with%20graphical%20user%20interfaces%20%28GUIs%29%20hold%0Asignificant%20potential%20for%20enhancing%20user%20experiences.%20To%20further%20improve%20these%0Aexperiences%2C%20agents%20need%20to%20be%20personalized%20and%20proactive.%20By%20effectively%0Acomprehending%20user%20intentions%20through%20their%20actions%20and%20interactions%20with%20GUIs%2C%0Aagents%20will%20be%20better%20positioned%20to%20achieve%20these%20goals.%20This%20paper%20introduces%0Athe%20task%20of%20goal%20identification%20from%20observed%20UI%20trajectories%2C%20aiming%20to%20infer%0Athe%20user%27s%20intended%20task%20based%20on%20their%20GUI%20interactions.%20We%20propose%20a%20novel%0Aevaluation%20metric%20to%20assess%20whether%20two%20task%20descriptions%20are%20paraphrases%0Awithin%20a%20specific%20UI%20environment.%20By%20Leveraging%20the%20inverse%20relation%20with%20the%0AUI%20automation%20task%2C%20we%20utilized%20the%20Android-In-The-Wild%20and%20Mind2Web%20datasets%0Afor%20our%20experiments.%20Using%20our%20metric%20and%20these%20datasets%2C%20we%20conducted%20several%0Aexperiments%20comparing%20the%20performance%20of%20humans%20and%20state-of-the-art%20models%2C%0Aspecifically%20GPT-4%20and%20Gemini-1.5%20Pro.%20Our%20results%20show%20that%20Gemini%20performs%0Abetter%20than%20GPT%20but%20still%20underperforms%20compared%20to%20humans%2C%20indicating%0Asignificant%20room%20for%20improvement.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2406.14314v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DIdentifying%2520User%2520Goals%2520from%2520UI%2520Trajectories%26entry.906535625%3DOmri%2520Berkovitch%2520and%2520Sapir%2520Caduri%2520and%2520Noam%2520Kahlon%2520and%2520Anatoly%2520Efros%2520and%2520Avi%2520Caciularu%2520and%2520Ido%2520Dagan%26entry.1292438233%3D%2520%2520Autonomous%2520agents%2520that%2520interact%2520with%2520graphical%2520user%2520interfaces%2520%2528GUIs%2529%2520hold%250Asignificant%2520potential%2520for%2520enhancing%2520user%2520experiences.%2520To%2520further%2520improve%2520these%250Aexperiences%252C%2520agents%2520need%2520to%2520be%2520personalized%2520and%2520proactive.%2520By%2520effectively%250Acomprehending%2520user%2520intentions%2520through%2520their%2520actions%2520and%2520interactions%2520with%2520GUIs%252C%250Aagents%2520will%2520be%2520better%2520positioned%2520to%2520achieve%2520these%2520goals.%2520This%2520paper%2520introduces%250Athe%2520task%2520of%2520goal%2520identification%2520from%2520observed%2520UI%2520trajectories%252C%2520aiming%2520to%2520infer%250Athe%2520user%2527s%2520intended%2520task%2520based%2520on%2520their%2520GUI%2520interactions.%2520We%2520propose%2520a%2520novel%250Aevaluation%2520metric%2520to%2520assess%2520whether%2520two%2520task%2520descriptions%2520are%2520paraphrases%250Awithin%2520a%2520specific%2520UI%2520environment.%2520By%2520Leveraging%2520the%2520inverse%2520relation%2520with%2520the%250AUI%2520automation%2520task%252C%2520we%2520utilized%2520the%2520Android-In-The-Wild%2520and%2520Mind2Web%2520datasets%250Afor%2520our%2520experiments.%2520Using%2520our%2520metric%2520and%2520these%2520datasets%252C%2520we%2520conducted%2520several%250Aexperiments%2520comparing%2520the%2520performance%2520of%2520humans%2520and%2520state-of-the-art%2520models%252C%250Aspecifically%2520GPT-4%2520and%2520Gemini-1.5%2520Pro.%2520Our%2520results%2520show%2520that%2520Gemini%2520performs%250Abetter%2520than%2520GPT%2520but%2520still%2520underperforms%2520compared%2520to%2520humans%252C%2520indicating%250Asignificant%2520room%2520for%2520improvement.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2406.14314v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Identifying%20User%20Goals%20from%20UI%20Trajectories&entry.906535625=Omri%20Berkovitch%20and%20Sapir%20Caduri%20and%20Noam%20Kahlon%20and%20Anatoly%20Efros%20and%20Avi%20Caciularu%20and%20Ido%20Dagan&entry.1292438233=%20%20Autonomous%20agents%20that%20interact%20with%20graphical%20user%20interfaces%20%28GUIs%29%20hold%0Asignificant%20potential%20for%20enhancing%20user%20experiences.%20To%20further%20improve%20these%0Aexperiences%2C%20agents%20need%20to%20be%20personalized%20and%20proactive.%20By%20effectively%0Acomprehending%20user%20intentions%20through%20their%20actions%20and%20interactions%20with%20GUIs%2C%0Aagents%20will%20be%20better%20positioned%20to%20achieve%20these%20goals.%20This%20paper%20introduces%0Athe%20task%20of%20goal%20identification%20from%20observed%20UI%20trajectories%2C%20aiming%20to%20infer%0Athe%20user%27s%20intended%20task%20based%20on%20their%20GUI%20interactions.%20We%20propose%20a%20novel%0Aevaluation%20metric%20to%20assess%20whether%20two%20task%20descriptions%20are%20paraphrases%0Awithin%20a%20specific%20UI%20environment.%20By%20Leveraging%20the%20inverse%20relation%20with%20the%0AUI%20automation%20task%2C%20we%20utilized%20the%20Android-In-The-Wild%20and%20Mind2Web%20datasets%0Afor%20our%20experiments.%20Using%20our%20metric%20and%20these%20datasets%2C%20we%20conducted%20several%0Aexperiments%20comparing%20the%20performance%20of%20humans%20and%20state-of-the-art%20models%2C%0Aspecifically%20GPT-4%20and%20Gemini-1.5%20Pro.%20Our%20results%20show%20that%20Gemini%20performs%0Abetter%20than%20GPT%20but%20still%20underperforms%20compared%20to%20humans%2C%20indicating%0Asignificant%20room%20for%20improvement.%0A&entry.1838667208=http%3A//arxiv.org/abs/2406.14314v1&entry.124074799=Read"},
{"title": "Enhancing robustness of data-driven SHM models: adversarial training\n  with circle loss", "author": "Xiangli Yang and Xijie Deng and Hanwei Zhang and Yang Zou and Jianxi Yang", "abstract": "  Structural health monitoring (SHM) is critical to safeguarding the safety and\nreliability of aerospace, civil, and mechanical infrastructure. Machine\nlearning-based data-driven approaches have gained popularity in SHM due to\nadvancements in sensors and computational power. However, machine learning\nmodels used in SHM are vulnerable to adversarial examples -- even small changes\nin input can lead to different model outputs. This paper aims to address this\nproblem by discussing adversarial defenses in SHM. In this paper, we propose an\nadversarial training method for defense, which uses circle loss to optimize the\ndistance between features in training to keep examples away from the decision\nboundary. Through this simple yet effective constraint, our method demonstrates\nsubstantial improvements in model robustness, surpassing existing defense\nmechanisms.\n", "link": "http://arxiv.org/abs/2406.14232v1", "date": "2024-06-20", "relevancy": 1.9464, "topK": [{"title": "Breadcrumbs: Adversarial class-balanced sampling for long-tailed recognition", "link": "https://link.springer.com/chapter/10.1007/978-3-031-20053-3_37", "similarity": 0.4992}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.4875}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.4527}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Enhancing%20robustness%20of%20data-driven%20SHM%20models%3A%20adversarial%20training%0A%20%20with%20circle%20loss&body=Title%3A%20Enhancing%20robustness%20of%20data-driven%20SHM%20models%3A%20adversarial%20training%0A%20%20with%20circle%20loss%0AAuthor%3A%20Xiangli%20Yang%20and%20Xijie%20Deng%20and%20Hanwei%20Zhang%20and%20Yang%20Zou%20and%20Jianxi%20Yang%0AAbstract%3A%20%20%20Structural%20health%20monitoring%20%28SHM%29%20is%20critical%20to%20safeguarding%20the%20safety%20and%0Areliability%20of%20aerospace%2C%20civil%2C%20and%20mechanical%20infrastructure.%20Machine%0Alearning-based%20data-driven%20approaches%20have%20gained%20popularity%20in%20SHM%20due%20to%0Aadvancements%20in%20sensors%20and%20computational%20power.%20However%2C%20machine%20learning%0Amodels%20used%20in%20SHM%20are%20vulnerable%20to%20adversarial%20examples%20--%20even%20small%20changes%0Ain%20input%20can%20lead%20to%20different%20model%20outputs.%20This%20paper%20aims%20to%20address%20this%0Aproblem%20by%20discussing%20adversarial%20defenses%20in%20SHM.%20In%20this%20paper%2C%20we%20propose%20an%0Aadversarial%20training%20method%20for%20defense%2C%20which%20uses%20circle%20loss%20to%20optimize%20the%0Adistance%20between%20features%20in%20training%20to%20keep%20examples%20away%20from%20the%20decision%0Aboundary.%20Through%20this%20simple%20yet%20effective%20constraint%2C%20our%20method%20demonstrates%0Asubstantial%20improvements%20in%20model%20robustness%2C%20surpassing%20existing%20defense%0Amechanisms.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2406.14232v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DEnhancing%2520robustness%2520of%2520data-driven%2520SHM%2520models%253A%2520adversarial%2520training%250A%2520%2520with%2520circle%2520loss%26entry.906535625%3DXiangli%2520Yang%2520and%2520Xijie%2520Deng%2520and%2520Hanwei%2520Zhang%2520and%2520Yang%2520Zou%2520and%2520Jianxi%2520Yang%26entry.1292438233%3D%2520%2520Structural%2520health%2520monitoring%2520%2528SHM%2529%2520is%2520critical%2520to%2520safeguarding%2520the%2520safety%2520and%250Areliability%2520of%2520aerospace%252C%2520civil%252C%2520and%2520mechanical%2520infrastructure.%2520Machine%250Alearning-based%2520data-driven%2520approaches%2520have%2520gained%2520popularity%2520in%2520SHM%2520due%2520to%250Aadvancements%2520in%2520sensors%2520and%2520computational%2520power.%2520However%252C%2520machine%2520learning%250Amodels%2520used%2520in%2520SHM%2520are%2520vulnerable%2520to%2520adversarial%2520examples%2520--%2520even%2520small%2520changes%250Ain%2520input%2520can%2520lead%2520to%2520different%2520model%2520outputs.%2520This%2520paper%2520aims%2520to%2520address%2520this%250Aproblem%2520by%2520discussing%2520adversarial%2520defenses%2520in%2520SHM.%2520In%2520this%2520paper%252C%2520we%2520propose%2520an%250Aadversarial%2520training%2520method%2520for%2520defense%252C%2520which%2520uses%2520circle%2520loss%2520to%2520optimize%2520the%250Adistance%2520between%2520features%2520in%2520training%2520to%2520keep%2520examples%2520away%2520from%2520the%2520decision%250Aboundary.%2520Through%2520this%2520simple%2520yet%2520effective%2520constraint%252C%2520our%2520method%2520demonstrates%250Asubstantial%2520improvements%2520in%2520model%2520robustness%252C%2520surpassing%2520existing%2520defense%250Amechanisms.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2406.14232v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Enhancing%20robustness%20of%20data-driven%20SHM%20models%3A%20adversarial%20training%0A%20%20with%20circle%20loss&entry.906535625=Xiangli%20Yang%20and%20Xijie%20Deng%20and%20Hanwei%20Zhang%20and%20Yang%20Zou%20and%20Jianxi%20Yang&entry.1292438233=%20%20Structural%20health%20monitoring%20%28SHM%29%20is%20critical%20to%20safeguarding%20the%20safety%20and%0Areliability%20of%20aerospace%2C%20civil%2C%20and%20mechanical%20infrastructure.%20Machine%0Alearning-based%20data-driven%20approaches%20have%20gained%20popularity%20in%20SHM%20due%20to%0Aadvancements%20in%20sensors%20and%20computational%20power.%20However%2C%20machine%20learning%0Amodels%20used%20in%20SHM%20are%20vulnerable%20to%20adversarial%20examples%20--%20even%20small%20changes%0Ain%20input%20can%20lead%20to%20different%20model%20outputs.%20This%20paper%20aims%20to%20address%20this%0Aproblem%20by%20discussing%20adversarial%20defenses%20in%20SHM.%20In%20this%20paper%2C%20we%20propose%20an%0Aadversarial%20training%20method%20for%20defense%2C%20which%20uses%20circle%20loss%20to%20optimize%20the%0Adistance%20between%20features%20in%20training%20to%20keep%20examples%20away%20from%20the%20decision%0Aboundary.%20Through%20this%20simple%20yet%20effective%20constraint%2C%20our%20method%20demonstrates%0Asubstantial%20improvements%20in%20model%20robustness%2C%20surpassing%20existing%20defense%0Amechanisms.%0A&entry.1838667208=http%3A//arxiv.org/abs/2406.14232v1&entry.124074799=Read"},
{"title": "Model Merging and Safety Alignment: One Bad Model Spoils the Bunch", "author": "Hasan Abed Al Kader Hammoud and Umberto Michieli and Fabio Pizzati and Philip Torr and Adel Bibi and Bernard Ghanem and Mete Ozay", "abstract": "  Merging Large Language Models (LLMs) is a cost-effective technique for\ncombining multiple expert LLMs into a single versatile model, retaining the\nexpertise of the original ones. However, current approaches often overlook the\nimportance of safety alignment during merging, leading to highly misaligned\nmodels. This work investigates the effects of model merging on alignment. We\nevaluate several popular model merging techniques, demonstrating that existing\nmethods do not only transfer domain expertise but also propagate misalignment.\nWe propose a simple two-step approach to address this problem: (i) generating\nsynthetic safety and domain-specific data, and (ii) incorporating these\ngenerated data into the optimization process of existing data-aware model\nmerging techniques. This allows us to treat alignment as a skill that can be\nmaximized in the resulting merged LLM. Our experiments illustrate the\neffectiveness of integrating alignment-related data during merging, resulting\nin models that excel in both domain expertise and alignment.\n", "link": "http://arxiv.org/abs/2406.14563v1", "date": "2024-06-20", "relevancy": 1.9461, "topK": [{"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.5096}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5065}, {"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.4574}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Model%20Merging%20and%20Safety%20Alignment%3A%20One%20Bad%20Model%20Spoils%20the%20Bunch&body=Title%3A%20Model%20Merging%20and%20Safety%20Alignment%3A%20One%20Bad%20Model%20Spoils%20the%20Bunch%0AAuthor%3A%20Hasan%20Abed%20Al%20Kader%20Hammoud%20and%20Umberto%20Michieli%20and%20Fabio%20Pizzati%20and%20Philip%20Torr%20and%20Adel%20Bibi%20and%20Bernard%20Ghanem%20and%20Mete%20Ozay%0AAbstract%3A%20%20%20Merging%20Large%20Language%20Models%20%28LLMs%29%20is%20a%20cost-effective%20technique%20for%0Acombining%20multiple%20expert%20LLMs%20into%20a%20single%20versatile%20model%2C%20retaining%20the%0Aexpertise%20of%20the%20original%20ones.%20However%2C%20current%20approaches%20often%20overlook%20the%0Aimportance%20of%20safety%20alignment%20during%20merging%2C%20leading%20to%20highly%20misaligned%0Amodels.%20This%20work%20investigates%20the%20effects%20of%20model%20merging%20on%20alignment.%20We%0Aevaluate%20several%20popular%20model%20merging%20techniques%2C%20demonstrating%20that%20existing%0Amethods%20do%20not%20only%20transfer%20domain%20expertise%20but%20also%20propagate%20misalignment.%0AWe%20propose%20a%20simple%20two-step%20approach%20to%20address%20this%20problem%3A%20%28i%29%20generating%0Asynthetic%20safety%20and%20domain-specific%20data%2C%20and%20%28ii%29%20incorporating%20these%0Agenerated%20data%20into%20the%20optimization%20process%20of%20existing%20data-aware%20model%0Amerging%20techniques.%20This%20allows%20us%20to%20treat%20alignment%20as%20a%20skill%20that%20can%20be%0Amaximized%20in%20the%20resulting%20merged%20LLM.%20Our%20experiments%20illustrate%20the%0Aeffectiveness%20of%20integrating%20alignment-related%20data%20during%20merging%2C%20resulting%0Ain%20models%20that%20excel%20in%20both%20domain%20expertise%20and%20alignment.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2406.14563v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DModel%2520Merging%2520and%2520Safety%2520Alignment%253A%2520One%2520Bad%2520Model%2520Spoils%2520the%2520Bunch%26entry.906535625%3DHasan%2520Abed%2520Al%2520Kader%2520Hammoud%2520and%2520Umberto%2520Michieli%2520and%2520Fabio%2520Pizzati%2520and%2520Philip%2520Torr%2520and%2520Adel%2520Bibi%2520and%2520Bernard%2520Ghanem%2520and%2520Mete%2520Ozay%26entry.1292438233%3D%2520%2520Merging%2520Large%2520Language%2520Models%2520%2528LLMs%2529%2520is%2520a%2520cost-effective%2520technique%2520for%250Acombining%2520multiple%2520expert%2520LLMs%2520into%2520a%2520single%2520versatile%2520model%252C%2520retaining%2520the%250Aexpertise%2520of%2520the%2520original%2520ones.%2520However%252C%2520current%2520approaches%2520often%2520overlook%2520the%250Aimportance%2520of%2520safety%2520alignment%2520during%2520merging%252C%2520leading%2520to%2520highly%2520misaligned%250Amodels.%2520This%2520work%2520investigates%2520the%2520effects%2520of%2520model%2520merging%2520on%2520alignment.%2520We%250Aevaluate%2520several%2520popular%2520model%2520merging%2520techniques%252C%2520demonstrating%2520that%2520existing%250Amethods%2520do%2520not%2520only%2520transfer%2520domain%2520expertise%2520but%2520also%2520propagate%2520misalignment.%250AWe%2520propose%2520a%2520simple%2520two-step%2520approach%2520to%2520address%2520this%2520problem%253A%2520%2528i%2529%2520generating%250Asynthetic%2520safety%2520and%2520domain-specific%2520data%252C%2520and%2520%2528ii%2529%2520incorporating%2520these%250Agenerated%2520data%2520into%2520the%2520optimization%2520process%2520of%2520existing%2520data-aware%2520model%250Amerging%2520techniques.%2520This%2520allows%2520us%2520to%2520treat%2520alignment%2520as%2520a%2520skill%2520that%2520can%2520be%250Amaximized%2520in%2520the%2520resulting%2520merged%2520LLM.%2520Our%2520experiments%2520illustrate%2520the%250Aeffectiveness%2520of%2520integrating%2520alignment-related%2520data%2520during%2520merging%252C%2520resulting%250Ain%2520models%2520that%2520excel%2520in%2520both%2520domain%2520expertise%2520and%2520alignment.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2406.14563v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Model%20Merging%20and%20Safety%20Alignment%3A%20One%20Bad%20Model%20Spoils%20the%20Bunch&entry.906535625=Hasan%20Abed%20Al%20Kader%20Hammoud%20and%20Umberto%20Michieli%20and%20Fabio%20Pizzati%20and%20Philip%20Torr%20and%20Adel%20Bibi%20and%20Bernard%20Ghanem%20and%20Mete%20Ozay&entry.1292438233=%20%20Merging%20Large%20Language%20Models%20%28LLMs%29%20is%20a%20cost-effective%20technique%20for%0Acombining%20multiple%20expert%20LLMs%20into%20a%20single%20versatile%20model%2C%20retaining%20the%0Aexpertise%20of%20the%20original%20ones.%20However%2C%20current%20approaches%20often%20overlook%20the%0Aimportance%20of%20safety%20alignment%20during%20merging%2C%20leading%20to%20highly%20misaligned%0Amodels.%20This%20work%20investigates%20the%20effects%20of%20model%20merging%20on%20alignment.%20We%0Aevaluate%20several%20popular%20model%20merging%20techniques%2C%20demonstrating%20that%20existing%0Amethods%20do%20not%20only%20transfer%20domain%20expertise%20but%20also%20propagate%20misalignment.%0AWe%20propose%20a%20simple%20two-step%20approach%20to%20address%20this%20problem%3A%20%28i%29%20generating%0Asynthetic%20safety%20and%20domain-specific%20data%2C%20and%20%28ii%29%20incorporating%20these%0Agenerated%20data%20into%20the%20optimization%20process%20of%20existing%20data-aware%20model%0Amerging%20techniques.%20This%20allows%20us%20to%20treat%20alignment%20as%20a%20skill%20that%20can%20be%0Amaximized%20in%20the%20resulting%20merged%20LLM.%20Our%20experiments%20illustrate%20the%0Aeffectiveness%20of%20integrating%20alignment-related%20data%20during%20merging%2C%20resulting%0Ain%20models%20that%20excel%20in%20both%20domain%20expertise%20and%20alignment.%0A&entry.1838667208=http%3A//arxiv.org/abs/2406.14563v1&entry.124074799=Read"},
{"title": "Raising the Bar: Investigating the Values of Large Language Models via\n  Generative Evolving Testing", "author": "Han Jiang and Xiaoyuan Yi and Zhihua Wei and Shu Wang and Xing Xie", "abstract": "  Warning: this paper contains model outputs exhibiting unethical information.\nLarge Language Models (LLMs) have achieved significant breakthroughs, but their\ngenerated unethical content poses potential risks. Measuring value alignment of\nLLMs becomes crucial for their regulation and responsible deployment. Numerous\ndatasets have been constructed to assess social bias, toxicity, and ethics in\nLLMs, but they suffer from evaluation chronoeffect, that is, as models rapidly\nevolve, existing data becomes leaked or undemanding, overestimating\never-developing LLMs. To tackle this problem, we propose GETA, a novel\ngenerative evolving testing approach that dynamically probes the underlying\nmoral baselines of LLMs. Distinct from previous adaptive testing methods that\nrely on static datasets with limited difficulty, GETA incorporates an\niteratively-updated item generator which infers each LLM's moral boundaries and\ngenerates difficulty-tailored testing items, accurately reflecting the true\nalignment extent. This process theoretically learns a joint distribution of\nitem and model response, with item difficulty and value conformity as latent\nvariables, where the generator co-evolves with the LLM, addressing\nchronoeffect. We evaluate various popular LLMs with diverse capabilities and\ndemonstrate that GETA can create difficulty-matching testing items and more\naccurately assess LLMs' values, better consistent with their performance on\nunseen OOD and i.i.d. items, laying the groundwork for future evaluation\nparadigms.\n", "link": "http://arxiv.org/abs/2406.14230v1", "date": "2024-06-20", "relevancy": 1.946, "topK": [{"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.5214}, {"title": "Genie: Generative Interactive Environments", "link": "http://arxiv.org/abs/2402.15391v1", "similarity": 0.482}, {"title": "UGG: Unified Generative Grasping", "link": "https://arxiv.org/abs/2311.16917", "similarity": 0.477}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Raising%20the%20Bar%3A%20Investigating%20the%20Values%20of%20Large%20Language%20Models%20via%0A%20%20Generative%20Evolving%20Testing&body=Title%3A%20Raising%20the%20Bar%3A%20Investigating%20the%20Values%20of%20Large%20Language%20Models%20via%0A%20%20Generative%20Evolving%20Testing%0AAuthor%3A%20Han%20Jiang%20and%20Xiaoyuan%20Yi%20and%20Zhihua%20Wei%20and%20Shu%20Wang%20and%20Xing%20Xie%0AAbstract%3A%20%20%20Warning%3A%20this%20paper%20contains%20model%20outputs%20exhibiting%20unethical%20information.%0ALarge%20Language%20Models%20%28LLMs%29%20have%20achieved%20significant%20breakthroughs%2C%20but%20their%0Agenerated%20unethical%20content%20poses%20potential%20risks.%20Measuring%20value%20alignment%20of%0ALLMs%20becomes%20crucial%20for%20their%20regulation%20and%20responsible%20deployment.%20Numerous%0Adatasets%20have%20been%20constructed%20to%20assess%20social%20bias%2C%20toxicity%2C%20and%20ethics%20in%0ALLMs%2C%20but%20they%20suffer%20from%20evaluation%20chronoeffect%2C%20that%20is%2C%20as%20models%20rapidly%0Aevolve%2C%20existing%20data%20becomes%20leaked%20or%20undemanding%2C%20overestimating%0Aever-developing%20LLMs.%20To%20tackle%20this%20problem%2C%20we%20propose%20GETA%2C%20a%20novel%0Agenerative%20evolving%20testing%20approach%20that%20dynamically%20probes%20the%20underlying%0Amoral%20baselines%20of%20LLMs.%20Distinct%20from%20previous%20adaptive%20testing%20methods%20that%0Arely%20on%20static%20datasets%20with%20limited%20difficulty%2C%20GETA%20incorporates%20an%0Aiteratively-updated%20item%20generator%20which%20infers%20each%20LLM%27s%20moral%20boundaries%20and%0Agenerates%20difficulty-tailored%20testing%20items%2C%20accurately%20reflecting%20the%20true%0Aalignment%20extent.%20This%20process%20theoretically%20learns%20a%20joint%20distribution%20of%0Aitem%20and%20model%20response%2C%20with%20item%20difficulty%20and%20value%20conformity%20as%20latent%0Avariables%2C%20where%20the%20generator%20co-evolves%20with%20the%20LLM%2C%20addressing%0Achronoeffect.%20We%20evaluate%20various%20popular%20LLMs%20with%20diverse%20capabilities%20and%0Ademonstrate%20that%20GETA%20can%20create%20difficulty-matching%20testing%20items%20and%20more%0Aaccurately%20assess%20LLMs%27%20values%2C%20better%20consistent%20with%20their%20performance%20on%0Aunseen%20OOD%20and%20i.i.d.%20items%2C%20laying%20the%20groundwork%20for%20future%20evaluation%0Aparadigms.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2406.14230v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DRaising%2520the%2520Bar%253A%2520Investigating%2520the%2520Values%2520of%2520Large%2520Language%2520Models%2520via%250A%2520%2520Generative%2520Evolving%2520Testing%26entry.906535625%3DHan%2520Jiang%2520and%2520Xiaoyuan%2520Yi%2520and%2520Zhihua%2520Wei%2520and%2520Shu%2520Wang%2520and%2520Xing%2520Xie%26entry.1292438233%3D%2520%2520Warning%253A%2520this%2520paper%2520contains%2520model%2520outputs%2520exhibiting%2520unethical%2520information.%250ALarge%2520Language%2520Models%2520%2528LLMs%2529%2520have%2520achieved%2520significant%2520breakthroughs%252C%2520but%2520their%250Agenerated%2520unethical%2520content%2520poses%2520potential%2520risks.%2520Measuring%2520value%2520alignment%2520of%250ALLMs%2520becomes%2520crucial%2520for%2520their%2520regulation%2520and%2520responsible%2520deployment.%2520Numerous%250Adatasets%2520have%2520been%2520constructed%2520to%2520assess%2520social%2520bias%252C%2520toxicity%252C%2520and%2520ethics%2520in%250ALLMs%252C%2520but%2520they%2520suffer%2520from%2520evaluation%2520chronoeffect%252C%2520that%2520is%252C%2520as%2520models%2520rapidly%250Aevolve%252C%2520existing%2520data%2520becomes%2520leaked%2520or%2520undemanding%252C%2520overestimating%250Aever-developing%2520LLMs.%2520To%2520tackle%2520this%2520problem%252C%2520we%2520propose%2520GETA%252C%2520a%2520novel%250Agenerative%2520evolving%2520testing%2520approach%2520that%2520dynamically%2520probes%2520the%2520underlying%250Amoral%2520baselines%2520of%2520LLMs.%2520Distinct%2520from%2520previous%2520adaptive%2520testing%2520methods%2520that%250Arely%2520on%2520static%2520datasets%2520with%2520limited%2520difficulty%252C%2520GETA%2520incorporates%2520an%250Aiteratively-updated%2520item%2520generator%2520which%2520infers%2520each%2520LLM%2527s%2520moral%2520boundaries%2520and%250Agenerates%2520difficulty-tailored%2520testing%2520items%252C%2520accurately%2520reflecting%2520the%2520true%250Aalignment%2520extent.%2520This%2520process%2520theoretically%2520learns%2520a%2520joint%2520distribution%2520of%250Aitem%2520and%2520model%2520response%252C%2520with%2520item%2520difficulty%2520and%2520value%2520conformity%2520as%2520latent%250Avariables%252C%2520where%2520the%2520generator%2520co-evolves%2520with%2520the%2520LLM%252C%2520addressing%250Achronoeffect.%2520We%2520evaluate%2520various%2520popular%2520LLMs%2520with%2520diverse%2520capabilities%2520and%250Ademonstrate%2520that%2520GETA%2520can%2520create%2520difficulty-matching%2520testing%2520items%2520and%2520more%250Aaccurately%2520assess%2520LLMs%2527%2520values%252C%2520better%2520consistent%2520with%2520their%2520performance%2520on%250Aunseen%2520OOD%2520and%2520i.i.d.%2520items%252C%2520laying%2520the%2520groundwork%2520for%2520future%2520evaluation%250Aparadigms.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2406.14230v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Raising%20the%20Bar%3A%20Investigating%20the%20Values%20of%20Large%20Language%20Models%20via%0A%20%20Generative%20Evolving%20Testing&entry.906535625=Han%20Jiang%20and%20Xiaoyuan%20Yi%20and%20Zhihua%20Wei%20and%20Shu%20Wang%20and%20Xing%20Xie&entry.1292438233=%20%20Warning%3A%20this%20paper%20contains%20model%20outputs%20exhibiting%20unethical%20information.%0ALarge%20Language%20Models%20%28LLMs%29%20have%20achieved%20significant%20breakthroughs%2C%20but%20their%0Agenerated%20unethical%20content%20poses%20potential%20risks.%20Measuring%20value%20alignment%20of%0ALLMs%20becomes%20crucial%20for%20their%20regulation%20and%20responsible%20deployment.%20Numerous%0Adatasets%20have%20been%20constructed%20to%20assess%20social%20bias%2C%20toxicity%2C%20and%20ethics%20in%0ALLMs%2C%20but%20they%20suffer%20from%20evaluation%20chronoeffect%2C%20that%20is%2C%20as%20models%20rapidly%0Aevolve%2C%20existing%20data%20becomes%20leaked%20or%20undemanding%2C%20overestimating%0Aever-developing%20LLMs.%20To%20tackle%20this%20problem%2C%20we%20propose%20GETA%2C%20a%20novel%0Agenerative%20evolving%20testing%20approach%20that%20dynamically%20probes%20the%20underlying%0Amoral%20baselines%20of%20LLMs.%20Distinct%20from%20previous%20adaptive%20testing%20methods%20that%0Arely%20on%20static%20datasets%20with%20limited%20difficulty%2C%20GETA%20incorporates%20an%0Aiteratively-updated%20item%20generator%20which%20infers%20each%20LLM%27s%20moral%20boundaries%20and%0Agenerates%20difficulty-tailored%20testing%20items%2C%20accurately%20reflecting%20the%20true%0Aalignment%20extent.%20This%20process%20theoretically%20learns%20a%20joint%20distribution%20of%0Aitem%20and%20model%20response%2C%20with%20item%20difficulty%20and%20value%20conformity%20as%20latent%0Avariables%2C%20where%20the%20generator%20co-evolves%20with%20the%20LLM%2C%20addressing%0Achronoeffect.%20We%20evaluate%20various%20popular%20LLMs%20with%20diverse%20capabilities%20and%0Ademonstrate%20that%20GETA%20can%20create%20difficulty-matching%20testing%20items%20and%20more%0Aaccurately%20assess%20LLMs%27%20values%2C%20better%20consistent%20with%20their%20performance%20on%0Aunseen%20OOD%20and%20i.i.d.%20items%2C%20laying%20the%20groundwork%20for%20future%20evaluation%0Aparadigms.%0A&entry.1838667208=http%3A//arxiv.org/abs/2406.14230v1&entry.124074799=Read"},
{"title": "VAIYAKARANA : A Benchmark for Automatic Grammar Correction in Bangla", "author": "Pramit Bhattacharyya and Arnab Bhattacharya", "abstract": "  Bangla (Bengali) is the fifth most spoken language globally and, yet, the\nproblem of automatic grammar correction in Bangla is still in its nascent\nstage. This is mostly due to the need for a large corpus of grammatically\nincorrect sentences, with their corresponding correct counterparts. The present\nstate-of-the-art techniques to curate a corpus for grammatically wrong\nsentences involve random swapping, insertion and deletion of words.\nHowever,these steps may not always generate grammatically wrong sentences in\nBangla. In this work, we propose a pragmatic approach to generate grammatically\nwrong sentences in Bangla. We first categorize the different kinds of errors in\nBangla into 5 broad classes and 12 finer classes. We then use these to generate\ngrammatically wrong sentences systematically from a correct sentence. This\napproach can generate a large number of wrong sentences and can, thus, mitigate\nthe challenge of lacking a large corpus for neural networks. We provide a\ndataset, Vaiyakarana, consisting of 92,830 grammatically incorrect sentences as\nwell as 18,426 correct sentences. We also collected 619 human-generated\nsentences from essays written by Bangla native speakers. This helped us to\nunderstand errors that are more frequent. We evaluated our corpus against\nneural models and LLMs and also benchmark it against human evaluators who are\nnative speakers of Bangla. Our analysis shows that native speakers are far more\naccurate than state-of-the-art models to detect whether the sentence is\ngrammatically correct. Our methodology of generating erroneous sentences can be\napplied for most other Indian languages as well.\n", "link": "http://arxiv.org/abs/2406.14284v1", "date": "2024-06-20", "relevancy": 1.9459, "topK": [{"title": "UGG: Unified Generative Grasping", "link": "https://arxiv.org/abs/2311.16917", "similarity": 0.4013}, {"title": "DressCode: Autoregressively Sewing and Generating Garments from Text\n  Guidance", "link": "http://arxiv.org/abs/2401.16465v3", "similarity": 0.3961}, {"title": "Genie: Generative Interactive Environments", "link": "http://arxiv.org/abs/2402.15391v1", "similarity": 0.37}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20VAIYAKARANA%20%3A%20A%20Benchmark%20for%20Automatic%20Grammar%20Correction%20in%20Bangla&body=Title%3A%20VAIYAKARANA%20%3A%20A%20Benchmark%20for%20Automatic%20Grammar%20Correction%20in%20Bangla%0AAuthor%3A%20Pramit%20Bhattacharyya%20and%20Arnab%20Bhattacharya%0AAbstract%3A%20%20%20Bangla%20%28Bengali%29%20is%20the%20fifth%20most%20spoken%20language%20globally%20and%2C%20yet%2C%20the%0Aproblem%20of%20automatic%20grammar%20correction%20in%20Bangla%20is%20still%20in%20its%20nascent%0Astage.%20This%20is%20mostly%20due%20to%20the%20need%20for%20a%20large%20corpus%20of%20grammatically%0Aincorrect%20sentences%2C%20with%20their%20corresponding%20correct%20counterparts.%20The%20present%0Astate-of-the-art%20techniques%20to%20curate%20a%20corpus%20for%20grammatically%20wrong%0Asentences%20involve%20random%20swapping%2C%20insertion%20and%20deletion%20of%20words.%0AHowever%2Cthese%20steps%20may%20not%20always%20generate%20grammatically%20wrong%20sentences%20in%0ABangla.%20In%20this%20work%2C%20we%20propose%20a%20pragmatic%20approach%20to%20generate%20grammatically%0Awrong%20sentences%20in%20Bangla.%20We%20first%20categorize%20the%20different%20kinds%20of%20errors%20in%0ABangla%20into%205%20broad%20classes%20and%2012%20finer%20classes.%20We%20then%20use%20these%20to%20generate%0Agrammatically%20wrong%20sentences%20systematically%20from%20a%20correct%20sentence.%20This%0Aapproach%20can%20generate%20a%20large%20number%20of%20wrong%20sentences%20and%20can%2C%20thus%2C%20mitigate%0Athe%20challenge%20of%20lacking%20a%20large%20corpus%20for%20neural%20networks.%20We%20provide%20a%0Adataset%2C%20Vaiyakarana%2C%20consisting%20of%2092%2C830%20grammatically%20incorrect%20sentences%20as%0Awell%20as%2018%2C426%20correct%20sentences.%20We%20also%20collected%20619%20human-generated%0Asentences%20from%20essays%20written%20by%20Bangla%20native%20speakers.%20This%20helped%20us%20to%0Aunderstand%20errors%20that%20are%20more%20frequent.%20We%20evaluated%20our%20corpus%20against%0Aneural%20models%20and%20LLMs%20and%20also%20benchmark%20it%20against%20human%20evaluators%20who%20are%0Anative%20speakers%20of%20Bangla.%20Our%20analysis%20shows%20that%20native%20speakers%20are%20far%20more%0Aaccurate%20than%20state-of-the-art%20models%20to%20detect%20whether%20the%20sentence%20is%0Agrammatically%20correct.%20Our%20methodology%20of%20generating%20erroneous%20sentences%20can%20be%0Aapplied%20for%20most%20other%20Indian%20languages%20as%20well.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2406.14284v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DVAIYAKARANA%2520%253A%2520A%2520Benchmark%2520for%2520Automatic%2520Grammar%2520Correction%2520in%2520Bangla%26entry.906535625%3DPramit%2520Bhattacharyya%2520and%2520Arnab%2520Bhattacharya%26entry.1292438233%3D%2520%2520Bangla%2520%2528Bengali%2529%2520is%2520the%2520fifth%2520most%2520spoken%2520language%2520globally%2520and%252C%2520yet%252C%2520the%250Aproblem%2520of%2520automatic%2520grammar%2520correction%2520in%2520Bangla%2520is%2520still%2520in%2520its%2520nascent%250Astage.%2520This%2520is%2520mostly%2520due%2520to%2520the%2520need%2520for%2520a%2520large%2520corpus%2520of%2520grammatically%250Aincorrect%2520sentences%252C%2520with%2520their%2520corresponding%2520correct%2520counterparts.%2520The%2520present%250Astate-of-the-art%2520techniques%2520to%2520curate%2520a%2520corpus%2520for%2520grammatically%2520wrong%250Asentences%2520involve%2520random%2520swapping%252C%2520insertion%2520and%2520deletion%2520of%2520words.%250AHowever%252Cthese%2520steps%2520may%2520not%2520always%2520generate%2520grammatically%2520wrong%2520sentences%2520in%250ABangla.%2520In%2520this%2520work%252C%2520we%2520propose%2520a%2520pragmatic%2520approach%2520to%2520generate%2520grammatically%250Awrong%2520sentences%2520in%2520Bangla.%2520We%2520first%2520categorize%2520the%2520different%2520kinds%2520of%2520errors%2520in%250ABangla%2520into%25205%2520broad%2520classes%2520and%252012%2520finer%2520classes.%2520We%2520then%2520use%2520these%2520to%2520generate%250Agrammatically%2520wrong%2520sentences%2520systematically%2520from%2520a%2520correct%2520sentence.%2520This%250Aapproach%2520can%2520generate%2520a%2520large%2520number%2520of%2520wrong%2520sentences%2520and%2520can%252C%2520thus%252C%2520mitigate%250Athe%2520challenge%2520of%2520lacking%2520a%2520large%2520corpus%2520for%2520neural%2520networks.%2520We%2520provide%2520a%250Adataset%252C%2520Vaiyakarana%252C%2520consisting%2520of%252092%252C830%2520grammatically%2520incorrect%2520sentences%2520as%250Awell%2520as%252018%252C426%2520correct%2520sentences.%2520We%2520also%2520collected%2520619%2520human-generated%250Asentences%2520from%2520essays%2520written%2520by%2520Bangla%2520native%2520speakers.%2520This%2520helped%2520us%2520to%250Aunderstand%2520errors%2520that%2520are%2520more%2520frequent.%2520We%2520evaluated%2520our%2520corpus%2520against%250Aneural%2520models%2520and%2520LLMs%2520and%2520also%2520benchmark%2520it%2520against%2520human%2520evaluators%2520who%2520are%250Anative%2520speakers%2520of%2520Bangla.%2520Our%2520analysis%2520shows%2520that%2520native%2520speakers%2520are%2520far%2520more%250Aaccurate%2520than%2520state-of-the-art%2520models%2520to%2520detect%2520whether%2520the%2520sentence%2520is%250Agrammatically%2520correct.%2520Our%2520methodology%2520of%2520generating%2520erroneous%2520sentences%2520can%2520be%250Aapplied%2520for%2520most%2520other%2520Indian%2520languages%2520as%2520well.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2406.14284v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=VAIYAKARANA%20%3A%20A%20Benchmark%20for%20Automatic%20Grammar%20Correction%20in%20Bangla&entry.906535625=Pramit%20Bhattacharyya%20and%20Arnab%20Bhattacharya&entry.1292438233=%20%20Bangla%20%28Bengali%29%20is%20the%20fifth%20most%20spoken%20language%20globally%20and%2C%20yet%2C%20the%0Aproblem%20of%20automatic%20grammar%20correction%20in%20Bangla%20is%20still%20in%20its%20nascent%0Astage.%20This%20is%20mostly%20due%20to%20the%20need%20for%20a%20large%20corpus%20of%20grammatically%0Aincorrect%20sentences%2C%20with%20their%20corresponding%20correct%20counterparts.%20The%20present%0Astate-of-the-art%20techniques%20to%20curate%20a%20corpus%20for%20grammatically%20wrong%0Asentences%20involve%20random%20swapping%2C%20insertion%20and%20deletion%20of%20words.%0AHowever%2Cthese%20steps%20may%20not%20always%20generate%20grammatically%20wrong%20sentences%20in%0ABangla.%20In%20this%20work%2C%20we%20propose%20a%20pragmatic%20approach%20to%20generate%20grammatically%0Awrong%20sentences%20in%20Bangla.%20We%20first%20categorize%20the%20different%20kinds%20of%20errors%20in%0ABangla%20into%205%20broad%20classes%20and%2012%20finer%20classes.%20We%20then%20use%20these%20to%20generate%0Agrammatically%20wrong%20sentences%20systematically%20from%20a%20correct%20sentence.%20This%0Aapproach%20can%20generate%20a%20large%20number%20of%20wrong%20sentences%20and%20can%2C%20thus%2C%20mitigate%0Athe%20challenge%20of%20lacking%20a%20large%20corpus%20for%20neural%20networks.%20We%20provide%20a%0Adataset%2C%20Vaiyakarana%2C%20consisting%20of%2092%2C830%20grammatically%20incorrect%20sentences%20as%0Awell%20as%2018%2C426%20correct%20sentences.%20We%20also%20collected%20619%20human-generated%0Asentences%20from%20essays%20written%20by%20Bangla%20native%20speakers.%20This%20helped%20us%20to%0Aunderstand%20errors%20that%20are%20more%20frequent.%20We%20evaluated%20our%20corpus%20against%0Aneural%20models%20and%20LLMs%20and%20also%20benchmark%20it%20against%20human%20evaluators%20who%20are%0Anative%20speakers%20of%20Bangla.%20Our%20analysis%20shows%20that%20native%20speakers%20are%20far%20more%0Aaccurate%20than%20state-of-the-art%20models%20to%20detect%20whether%20the%20sentence%20is%0Agrammatically%20correct.%20Our%20methodology%20of%20generating%20erroneous%20sentences%20can%20be%0Aapplied%20for%20most%20other%20Indian%20languages%20as%20well.%0A&entry.1838667208=http%3A//arxiv.org/abs/2406.14284v1&entry.124074799=Read"},
{"title": "GraphReader: Building Graph-based Agent to Enhance Long-Context\n  Abilities of Large Language Models", "author": "Shilong Li and Yancheng He and Hangyu Guo and Xingyuan Bu and Ge Bai and Jie Liu and Jiaheng Liu and Xingwei Qu and Yangguang Li and Wanli Ouyang and Wenbo Su and Bo Zheng", "abstract": "  Long-context capabilities are essential for large language models (LLMs) to\ntackle complex and long-input tasks. Despite numerous efforts made to optimize\nLLMs for long contexts, challenges persist in robustly processing long inputs.\nIn this paper, we introduce GraphReader, a graph-based agent system designed to\nhandle long texts by structuring them into a graph and employing an agent to\nexplore this graph autonomously. Upon receiving a question, the agent first\nundertakes a step-by-step analysis and devises a rational plan. It then invokes\na set of predefined functions to read node content and neighbors, facilitating\na coarse-to-fine exploration of the graph. Throughout the exploration, the\nagent continuously records new insights and reflects on current circumstances\nto optimize the process until it has gathered sufficient information to\ngenerate an answer. Experimental results on the LV-Eval dataset reveal that\nGraphReader, using a 4k context window, consistently outperforms GPT-4-128k\nacross context lengths from 16k to 256k by a large margin. Additionally, our\napproach demonstrates superior performance on four challenging single-hop and\nmulti-hop benchmarks.\n", "link": "http://arxiv.org/abs/2406.14550v1", "date": "2024-06-20", "relevancy": 1.9414, "topK": [{"title": "RoboEXP: Action-Conditioned Scene Graph via Interactive Exploration for\n  Robotic Manipulation", "link": "http://arxiv.org/abs/2402.15487v1", "similarity": 0.5454}, {"title": "Genie: Generative Interactive Environments", "link": "http://arxiv.org/abs/2402.15391v1", "similarity": 0.4801}, {"title": "Self-supervised Photographic Image Layout Representation Learning", "link": "http://arxiv.org/abs/2403.03740v1", "similarity": 0.4665}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20GraphReader%3A%20Building%20Graph-based%20Agent%20to%20Enhance%20Long-Context%0A%20%20Abilities%20of%20Large%20Language%20Models&body=Title%3A%20GraphReader%3A%20Building%20Graph-based%20Agent%20to%20Enhance%20Long-Context%0A%20%20Abilities%20of%20Large%20Language%20Models%0AAuthor%3A%20Shilong%20Li%20and%20Yancheng%20He%20and%20Hangyu%20Guo%20and%20Xingyuan%20Bu%20and%20Ge%20Bai%20and%20Jie%20Liu%20and%20Jiaheng%20Liu%20and%20Xingwei%20Qu%20and%20Yangguang%20Li%20and%20Wanli%20Ouyang%20and%20Wenbo%20Su%20and%20Bo%20Zheng%0AAbstract%3A%20%20%20Long-context%20capabilities%20are%20essential%20for%20large%20language%20models%20%28LLMs%29%20to%0Atackle%20complex%20and%20long-input%20tasks.%20Despite%20numerous%20efforts%20made%20to%20optimize%0ALLMs%20for%20long%20contexts%2C%20challenges%20persist%20in%20robustly%20processing%20long%20inputs.%0AIn%20this%20paper%2C%20we%20introduce%20GraphReader%2C%20a%20graph-based%20agent%20system%20designed%20to%0Ahandle%20long%20texts%20by%20structuring%20them%20into%20a%20graph%20and%20employing%20an%20agent%20to%0Aexplore%20this%20graph%20autonomously.%20Upon%20receiving%20a%20question%2C%20the%20agent%20first%0Aundertakes%20a%20step-by-step%20analysis%20and%20devises%20a%20rational%20plan.%20It%20then%20invokes%0Aa%20set%20of%20predefined%20functions%20to%20read%20node%20content%20and%20neighbors%2C%20facilitating%0Aa%20coarse-to-fine%20exploration%20of%20the%20graph.%20Throughout%20the%20exploration%2C%20the%0Aagent%20continuously%20records%20new%20insights%20and%20reflects%20on%20current%20circumstances%0Ato%20optimize%20the%20process%20until%20it%20has%20gathered%20sufficient%20information%20to%0Agenerate%20an%20answer.%20Experimental%20results%20on%20the%20LV-Eval%20dataset%20reveal%20that%0AGraphReader%2C%20using%20a%204k%20context%20window%2C%20consistently%20outperforms%20GPT-4-128k%0Aacross%20context%20lengths%20from%2016k%20to%20256k%20by%20a%20large%20margin.%20Additionally%2C%20our%0Aapproach%20demonstrates%20superior%20performance%20on%20four%20challenging%20single-hop%20and%0Amulti-hop%20benchmarks.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2406.14550v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DGraphReader%253A%2520Building%2520Graph-based%2520Agent%2520to%2520Enhance%2520Long-Context%250A%2520%2520Abilities%2520of%2520Large%2520Language%2520Models%26entry.906535625%3DShilong%2520Li%2520and%2520Yancheng%2520He%2520and%2520Hangyu%2520Guo%2520and%2520Xingyuan%2520Bu%2520and%2520Ge%2520Bai%2520and%2520Jie%2520Liu%2520and%2520Jiaheng%2520Liu%2520and%2520Xingwei%2520Qu%2520and%2520Yangguang%2520Li%2520and%2520Wanli%2520Ouyang%2520and%2520Wenbo%2520Su%2520and%2520Bo%2520Zheng%26entry.1292438233%3D%2520%2520Long-context%2520capabilities%2520are%2520essential%2520for%2520large%2520language%2520models%2520%2528LLMs%2529%2520to%250Atackle%2520complex%2520and%2520long-input%2520tasks.%2520Despite%2520numerous%2520efforts%2520made%2520to%2520optimize%250ALLMs%2520for%2520long%2520contexts%252C%2520challenges%2520persist%2520in%2520robustly%2520processing%2520long%2520inputs.%250AIn%2520this%2520paper%252C%2520we%2520introduce%2520GraphReader%252C%2520a%2520graph-based%2520agent%2520system%2520designed%2520to%250Ahandle%2520long%2520texts%2520by%2520structuring%2520them%2520into%2520a%2520graph%2520and%2520employing%2520an%2520agent%2520to%250Aexplore%2520this%2520graph%2520autonomously.%2520Upon%2520receiving%2520a%2520question%252C%2520the%2520agent%2520first%250Aundertakes%2520a%2520step-by-step%2520analysis%2520and%2520devises%2520a%2520rational%2520plan.%2520It%2520then%2520invokes%250Aa%2520set%2520of%2520predefined%2520functions%2520to%2520read%2520node%2520content%2520and%2520neighbors%252C%2520facilitating%250Aa%2520coarse-to-fine%2520exploration%2520of%2520the%2520graph.%2520Throughout%2520the%2520exploration%252C%2520the%250Aagent%2520continuously%2520records%2520new%2520insights%2520and%2520reflects%2520on%2520current%2520circumstances%250Ato%2520optimize%2520the%2520process%2520until%2520it%2520has%2520gathered%2520sufficient%2520information%2520to%250Agenerate%2520an%2520answer.%2520Experimental%2520results%2520on%2520the%2520LV-Eval%2520dataset%2520reveal%2520that%250AGraphReader%252C%2520using%2520a%25204k%2520context%2520window%252C%2520consistently%2520outperforms%2520GPT-4-128k%250Aacross%2520context%2520lengths%2520from%252016k%2520to%2520256k%2520by%2520a%2520large%2520margin.%2520Additionally%252C%2520our%250Aapproach%2520demonstrates%2520superior%2520performance%2520on%2520four%2520challenging%2520single-hop%2520and%250Amulti-hop%2520benchmarks.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2406.14550v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=GraphReader%3A%20Building%20Graph-based%20Agent%20to%20Enhance%20Long-Context%0A%20%20Abilities%20of%20Large%20Language%20Models&entry.906535625=Shilong%20Li%20and%20Yancheng%20He%20and%20Hangyu%20Guo%20and%20Xingyuan%20Bu%20and%20Ge%20Bai%20and%20Jie%20Liu%20and%20Jiaheng%20Liu%20and%20Xingwei%20Qu%20and%20Yangguang%20Li%20and%20Wanli%20Ouyang%20and%20Wenbo%20Su%20and%20Bo%20Zheng&entry.1292438233=%20%20Long-context%20capabilities%20are%20essential%20for%20large%20language%20models%20%28LLMs%29%20to%0Atackle%20complex%20and%20long-input%20tasks.%20Despite%20numerous%20efforts%20made%20to%20optimize%0ALLMs%20for%20long%20contexts%2C%20challenges%20persist%20in%20robustly%20processing%20long%20inputs.%0AIn%20this%20paper%2C%20we%20introduce%20GraphReader%2C%20a%20graph-based%20agent%20system%20designed%20to%0Ahandle%20long%20texts%20by%20structuring%20them%20into%20a%20graph%20and%20employing%20an%20agent%20to%0Aexplore%20this%20graph%20autonomously.%20Upon%20receiving%20a%20question%2C%20the%20agent%20first%0Aundertakes%20a%20step-by-step%20analysis%20and%20devises%20a%20rational%20plan.%20It%20then%20invokes%0Aa%20set%20of%20predefined%20functions%20to%20read%20node%20content%20and%20neighbors%2C%20facilitating%0Aa%20coarse-to-fine%20exploration%20of%20the%20graph.%20Throughout%20the%20exploration%2C%20the%0Aagent%20continuously%20records%20new%20insights%20and%20reflects%20on%20current%20circumstances%0Ato%20optimize%20the%20process%20until%20it%20has%20gathered%20sufficient%20information%20to%0Agenerate%20an%20answer.%20Experimental%20results%20on%20the%20LV-Eval%20dataset%20reveal%20that%0AGraphReader%2C%20using%20a%204k%20context%20window%2C%20consistently%20outperforms%20GPT-4-128k%0Aacross%20context%20lengths%20from%2016k%20to%20256k%20by%20a%20large%20margin.%20Additionally%2C%20our%0Aapproach%20demonstrates%20superior%20performance%20on%20four%20challenging%20single-hop%20and%0Amulti-hop%20benchmarks.%0A&entry.1838667208=http%3A//arxiv.org/abs/2406.14550v1&entry.124074799=Read"},
{"title": "Lessons on Datasets and Paradigms in Machine Learning for Symbolic\n  Computation: A Case Study on CAD", "author": "Tereso del R\u00edo and Matthew England", "abstract": "  Symbolic Computation algorithms and their implementation in computer algebra\nsystems often contain choices which do not affect the correctness of the output\nbut can significantly impact the resources required: such choices can benefit\nfrom having them made separately for each problem via a machine learning model.\nThis study reports lessons on such use of machine learning in symbolic\ncomputation, in particular on the importance of analysing datasets prior to\nmachine learning and on the different machine learning paradigms that may be\nutilised. We present results for a particular case study, the selection of\nvariable ordering for cylindrical algebraic decomposition, but expect that the\nlessons learned are applicable to other decisions in symbolic computation.\n  We utilise an existing dataset of examples derived from applications which\nwas found to be imbalanced with respect to the variable ordering decision. We\nintroduce an augmentation technique for polynomial systems problems that allows\nus to balance and further augment the dataset, improving the machine learning\nresults by 28\\% and 38\\% on average, respectively. We then demonstrate how the\nexisting machine learning methodology used for the problem $-$ classification\n$-$ might be recast into the regression paradigm. While this does not have a\nradical change on the performance, it does widen the scope in which the\nmethodology can be applied to make choices.\n", "link": "http://arxiv.org/abs/2401.13343v2", "date": "2024-06-20", "relevancy": 1.9387, "topK": [{"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.4975}, {"title": "Self-supervised Photographic Image Layout Representation Learning", "link": "http://arxiv.org/abs/2403.03740v1", "similarity": 0.4862}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.478}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Lessons%20on%20Datasets%20and%20Paradigms%20in%20Machine%20Learning%20for%20Symbolic%0A%20%20Computation%3A%20A%20Case%20Study%20on%20CAD&body=Title%3A%20Lessons%20on%20Datasets%20and%20Paradigms%20in%20Machine%20Learning%20for%20Symbolic%0A%20%20Computation%3A%20A%20Case%20Study%20on%20CAD%0AAuthor%3A%20Tereso%20del%20R%C3%ADo%20and%20Matthew%20England%0AAbstract%3A%20%20%20Symbolic%20Computation%20algorithms%20and%20their%20implementation%20in%20computer%20algebra%0Asystems%20often%20contain%20choices%20which%20do%20not%20affect%20the%20correctness%20of%20the%20output%0Abut%20can%20significantly%20impact%20the%20resources%20required%3A%20such%20choices%20can%20benefit%0Afrom%20having%20them%20made%20separately%20for%20each%20problem%20via%20a%20machine%20learning%20model.%0AThis%20study%20reports%20lessons%20on%20such%20use%20of%20machine%20learning%20in%20symbolic%0Acomputation%2C%20in%20particular%20on%20the%20importance%20of%20analysing%20datasets%20prior%20to%0Amachine%20learning%20and%20on%20the%20different%20machine%20learning%20paradigms%20that%20may%20be%0Autilised.%20We%20present%20results%20for%20a%20particular%20case%20study%2C%20the%20selection%20of%0Avariable%20ordering%20for%20cylindrical%20algebraic%20decomposition%2C%20but%20expect%20that%20the%0Alessons%20learned%20are%20applicable%20to%20other%20decisions%20in%20symbolic%20computation.%0A%20%20We%20utilise%20an%20existing%20dataset%20of%20examples%20derived%20from%20applications%20which%0Awas%20found%20to%20be%20imbalanced%20with%20respect%20to%20the%20variable%20ordering%20decision.%20We%0Aintroduce%20an%20augmentation%20technique%20for%20polynomial%20systems%20problems%20that%20allows%0Aus%20to%20balance%20and%20further%20augment%20the%20dataset%2C%20improving%20the%20machine%20learning%0Aresults%20by%2028%5C%25%20and%2038%5C%25%20on%20average%2C%20respectively.%20We%20then%20demonstrate%20how%20the%0Aexisting%20machine%20learning%20methodology%20used%20for%20the%20problem%20%24-%24%20classification%0A%24-%24%20might%20be%20recast%20into%20the%20regression%20paradigm.%20While%20this%20does%20not%20have%20a%0Aradical%20change%20on%20the%20performance%2C%20it%20does%20widen%20the%20scope%20in%20which%20the%0Amethodology%20can%20be%20applied%20to%20make%20choices.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2401.13343v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DLessons%2520on%2520Datasets%2520and%2520Paradigms%2520in%2520Machine%2520Learning%2520for%2520Symbolic%250A%2520%2520Computation%253A%2520A%2520Case%2520Study%2520on%2520CAD%26entry.906535625%3DTereso%2520del%2520R%25C3%25ADo%2520and%2520Matthew%2520England%26entry.1292438233%3D%2520%2520Symbolic%2520Computation%2520algorithms%2520and%2520their%2520implementation%2520in%2520computer%2520algebra%250Asystems%2520often%2520contain%2520choices%2520which%2520do%2520not%2520affect%2520the%2520correctness%2520of%2520the%2520output%250Abut%2520can%2520significantly%2520impact%2520the%2520resources%2520required%253A%2520such%2520choices%2520can%2520benefit%250Afrom%2520having%2520them%2520made%2520separately%2520for%2520each%2520problem%2520via%2520a%2520machine%2520learning%2520model.%250AThis%2520study%2520reports%2520lessons%2520on%2520such%2520use%2520of%2520machine%2520learning%2520in%2520symbolic%250Acomputation%252C%2520in%2520particular%2520on%2520the%2520importance%2520of%2520analysing%2520datasets%2520prior%2520to%250Amachine%2520learning%2520and%2520on%2520the%2520different%2520machine%2520learning%2520paradigms%2520that%2520may%2520be%250Autilised.%2520We%2520present%2520results%2520for%2520a%2520particular%2520case%2520study%252C%2520the%2520selection%2520of%250Avariable%2520ordering%2520for%2520cylindrical%2520algebraic%2520decomposition%252C%2520but%2520expect%2520that%2520the%250Alessons%2520learned%2520are%2520applicable%2520to%2520other%2520decisions%2520in%2520symbolic%2520computation.%250A%2520%2520We%2520utilise%2520an%2520existing%2520dataset%2520of%2520examples%2520derived%2520from%2520applications%2520which%250Awas%2520found%2520to%2520be%2520imbalanced%2520with%2520respect%2520to%2520the%2520variable%2520ordering%2520decision.%2520We%250Aintroduce%2520an%2520augmentation%2520technique%2520for%2520polynomial%2520systems%2520problems%2520that%2520allows%250Aus%2520to%2520balance%2520and%2520further%2520augment%2520the%2520dataset%252C%2520improving%2520the%2520machine%2520learning%250Aresults%2520by%252028%255C%2525%2520and%252038%255C%2525%2520on%2520average%252C%2520respectively.%2520We%2520then%2520demonstrate%2520how%2520the%250Aexisting%2520machine%2520learning%2520methodology%2520used%2520for%2520the%2520problem%2520%2524-%2524%2520classification%250A%2524-%2524%2520might%2520be%2520recast%2520into%2520the%2520regression%2520paradigm.%2520While%2520this%2520does%2520not%2520have%2520a%250Aradical%2520change%2520on%2520the%2520performance%252C%2520it%2520does%2520widen%2520the%2520scope%2520in%2520which%2520the%250Amethodology%2520can%2520be%2520applied%2520to%2520make%2520choices.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2401.13343v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Lessons%20on%20Datasets%20and%20Paradigms%20in%20Machine%20Learning%20for%20Symbolic%0A%20%20Computation%3A%20A%20Case%20Study%20on%20CAD&entry.906535625=Tereso%20del%20R%C3%ADo%20and%20Matthew%20England&entry.1292438233=%20%20Symbolic%20Computation%20algorithms%20and%20their%20implementation%20in%20computer%20algebra%0Asystems%20often%20contain%20choices%20which%20do%20not%20affect%20the%20correctness%20of%20the%20output%0Abut%20can%20significantly%20impact%20the%20resources%20required%3A%20such%20choices%20can%20benefit%0Afrom%20having%20them%20made%20separately%20for%20each%20problem%20via%20a%20machine%20learning%20model.%0AThis%20study%20reports%20lessons%20on%20such%20use%20of%20machine%20learning%20in%20symbolic%0Acomputation%2C%20in%20particular%20on%20the%20importance%20of%20analysing%20datasets%20prior%20to%0Amachine%20learning%20and%20on%20the%20different%20machine%20learning%20paradigms%20that%20may%20be%0Autilised.%20We%20present%20results%20for%20a%20particular%20case%20study%2C%20the%20selection%20of%0Avariable%20ordering%20for%20cylindrical%20algebraic%20decomposition%2C%20but%20expect%20that%20the%0Alessons%20learned%20are%20applicable%20to%20other%20decisions%20in%20symbolic%20computation.%0A%20%20We%20utilise%20an%20existing%20dataset%20of%20examples%20derived%20from%20applications%20which%0Awas%20found%20to%20be%20imbalanced%20with%20respect%20to%20the%20variable%20ordering%20decision.%20We%0Aintroduce%20an%20augmentation%20technique%20for%20polynomial%20systems%20problems%20that%20allows%0Aus%20to%20balance%20and%20further%20augment%20the%20dataset%2C%20improving%20the%20machine%20learning%0Aresults%20by%2028%5C%25%20and%2038%5C%25%20on%20average%2C%20respectively.%20We%20then%20demonstrate%20how%20the%0Aexisting%20machine%20learning%20methodology%20used%20for%20the%20problem%20%24-%24%20classification%0A%24-%24%20might%20be%20recast%20into%20the%20regression%20paradigm.%20While%20this%20does%20not%20have%20a%0Aradical%20change%20on%20the%20performance%2C%20it%20does%20widen%20the%20scope%20in%20which%20the%0Amethodology%20can%20be%20applied%20to%20make%20choices.%0A&entry.1838667208=http%3A//arxiv.org/abs/2401.13343v2&entry.124074799=Read"},
{"title": "A Survey of Multimodal-Guided Image Editing with Text-to-Image Diffusion\n  Models", "author": "Xincheng Shuai and Henghui Ding and Xingjun Ma and Rongcheng Tu and Yu-Gang Jiang and Dacheng Tao", "abstract": "  Image editing aims to edit the given synthetic or real image to meet the\nspecific requirements from users. It is widely studied in recent years as a\npromising and challenging field of Artificial Intelligence Generative Content\n(AIGC). Recent significant advancement in this field is based on the\ndevelopment of text-to-image (T2I) diffusion models, which generate images\naccording to text prompts. These models demonstrate remarkable generative\ncapabilities and have become widely used tools for image editing. T2I-based\nimage editing methods significantly enhance editing performance and offer a\nuser-friendly interface for modifying content guided by multimodal inputs. In\nthis survey, we provide a comprehensive review of multimodal-guided image\nediting techniques that leverage T2I diffusion models. First, we define the\nscope of image editing from a holistic perspective and detail various control\nsignals and editing scenarios. We then propose a unified framework to formalize\nthe editing process, categorizing it into two primary algorithm families. This\nframework offers a design space for users to achieve specific goals.\nSubsequently, we present an in-depth analysis of each component within this\nframework, examining the characteristics and applicable scenarios of different\ncombinations. Given that training-based methods learn to directly map the\nsource image to target one under user guidance, we discuss them separately, and\nintroduce injection schemes of source image in different scenarios.\nAdditionally, we review the application of 2D techniques to video editing,\nhighlighting solutions for inter-frame inconsistency. Finally, we discuss open\nchallenges in the field and suggest potential future research directions. We\nkeep tracing related works at\nhttps://github.com/xinchengshuai/Awesome-Image-Editing.\n", "link": "http://arxiv.org/abs/2406.14555v1", "date": "2024-06-20", "relevancy": 1.9325, "topK": [{"title": "Multimodal-Conditioned Latent Diffusion Models for Fashion Image Editing", "link": "http://arxiv.org/abs/2403.14828v2", "similarity": 0.6813}, {"title": "Customize-A-Video: One-Shot Motion Customization of Text-to-Video\n  Diffusion Models", "link": "http://arxiv.org/abs/2402.14780v1", "similarity": 0.6452}, {"title": "Magic-Me: Identity-Specific Video Customized Diffusion", "link": "http://arxiv.org/abs/2402.09368v1", "similarity": 0.6289}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20A%20Survey%20of%20Multimodal-Guided%20Image%20Editing%20with%20Text-to-Image%20Diffusion%0A%20%20Models&body=Title%3A%20A%20Survey%20of%20Multimodal-Guided%20Image%20Editing%20with%20Text-to-Image%20Diffusion%0A%20%20Models%0AAuthor%3A%20Xincheng%20Shuai%20and%20Henghui%20Ding%20and%20Xingjun%20Ma%20and%20Rongcheng%20Tu%20and%20Yu-Gang%20Jiang%20and%20Dacheng%20Tao%0AAbstract%3A%20%20%20Image%20editing%20aims%20to%20edit%20the%20given%20synthetic%20or%20real%20image%20to%20meet%20the%0Aspecific%20requirements%20from%20users.%20It%20is%20widely%20studied%20in%20recent%20years%20as%20a%0Apromising%20and%20challenging%20field%20of%20Artificial%20Intelligence%20Generative%20Content%0A%28AIGC%29.%20Recent%20significant%20advancement%20in%20this%20field%20is%20based%20on%20the%0Adevelopment%20of%20text-to-image%20%28T2I%29%20diffusion%20models%2C%20which%20generate%20images%0Aaccording%20to%20text%20prompts.%20These%20models%20demonstrate%20remarkable%20generative%0Acapabilities%20and%20have%20become%20widely%20used%20tools%20for%20image%20editing.%20T2I-based%0Aimage%20editing%20methods%20significantly%20enhance%20editing%20performance%20and%20offer%20a%0Auser-friendly%20interface%20for%20modifying%20content%20guided%20by%20multimodal%20inputs.%20In%0Athis%20survey%2C%20we%20provide%20a%20comprehensive%20review%20of%20multimodal-guided%20image%0Aediting%20techniques%20that%20leverage%20T2I%20diffusion%20models.%20First%2C%20we%20define%20the%0Ascope%20of%20image%20editing%20from%20a%20holistic%20perspective%20and%20detail%20various%20control%0Asignals%20and%20editing%20scenarios.%20We%20then%20propose%20a%20unified%20framework%20to%20formalize%0Athe%20editing%20process%2C%20categorizing%20it%20into%20two%20primary%20algorithm%20families.%20This%0Aframework%20offers%20a%20design%20space%20for%20users%20to%20achieve%20specific%20goals.%0ASubsequently%2C%20we%20present%20an%20in-depth%20analysis%20of%20each%20component%20within%20this%0Aframework%2C%20examining%20the%20characteristics%20and%20applicable%20scenarios%20of%20different%0Acombinations.%20Given%20that%20training-based%20methods%20learn%20to%20directly%20map%20the%0Asource%20image%20to%20target%20one%20under%20user%20guidance%2C%20we%20discuss%20them%20separately%2C%20and%0Aintroduce%20injection%20schemes%20of%20source%20image%20in%20different%20scenarios.%0AAdditionally%2C%20we%20review%20the%20application%20of%202D%20techniques%20to%20video%20editing%2C%0Ahighlighting%20solutions%20for%20inter-frame%20inconsistency.%20Finally%2C%20we%20discuss%20open%0Achallenges%20in%20the%20field%20and%20suggest%20potential%20future%20research%20directions.%20We%0Akeep%20tracing%20related%20works%20at%0Ahttps%3A//github.com/xinchengshuai/Awesome-Image-Editing.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2406.14555v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DA%2520Survey%2520of%2520Multimodal-Guided%2520Image%2520Editing%2520with%2520Text-to-Image%2520Diffusion%250A%2520%2520Models%26entry.906535625%3DXincheng%2520Shuai%2520and%2520Henghui%2520Ding%2520and%2520Xingjun%2520Ma%2520and%2520Rongcheng%2520Tu%2520and%2520Yu-Gang%2520Jiang%2520and%2520Dacheng%2520Tao%26entry.1292438233%3D%2520%2520Image%2520editing%2520aims%2520to%2520edit%2520the%2520given%2520synthetic%2520or%2520real%2520image%2520to%2520meet%2520the%250Aspecific%2520requirements%2520from%2520users.%2520It%2520is%2520widely%2520studied%2520in%2520recent%2520years%2520as%2520a%250Apromising%2520and%2520challenging%2520field%2520of%2520Artificial%2520Intelligence%2520Generative%2520Content%250A%2528AIGC%2529.%2520Recent%2520significant%2520advancement%2520in%2520this%2520field%2520is%2520based%2520on%2520the%250Adevelopment%2520of%2520text-to-image%2520%2528T2I%2529%2520diffusion%2520models%252C%2520which%2520generate%2520images%250Aaccording%2520to%2520text%2520prompts.%2520These%2520models%2520demonstrate%2520remarkable%2520generative%250Acapabilities%2520and%2520have%2520become%2520widely%2520used%2520tools%2520for%2520image%2520editing.%2520T2I-based%250Aimage%2520editing%2520methods%2520significantly%2520enhance%2520editing%2520performance%2520and%2520offer%2520a%250Auser-friendly%2520interface%2520for%2520modifying%2520content%2520guided%2520by%2520multimodal%2520inputs.%2520In%250Athis%2520survey%252C%2520we%2520provide%2520a%2520comprehensive%2520review%2520of%2520multimodal-guided%2520image%250Aediting%2520techniques%2520that%2520leverage%2520T2I%2520diffusion%2520models.%2520First%252C%2520we%2520define%2520the%250Ascope%2520of%2520image%2520editing%2520from%2520a%2520holistic%2520perspective%2520and%2520detail%2520various%2520control%250Asignals%2520and%2520editing%2520scenarios.%2520We%2520then%2520propose%2520a%2520unified%2520framework%2520to%2520formalize%250Athe%2520editing%2520process%252C%2520categorizing%2520it%2520into%2520two%2520primary%2520algorithm%2520families.%2520This%250Aframework%2520offers%2520a%2520design%2520space%2520for%2520users%2520to%2520achieve%2520specific%2520goals.%250ASubsequently%252C%2520we%2520present%2520an%2520in-depth%2520analysis%2520of%2520each%2520component%2520within%2520this%250Aframework%252C%2520examining%2520the%2520characteristics%2520and%2520applicable%2520scenarios%2520of%2520different%250Acombinations.%2520Given%2520that%2520training-based%2520methods%2520learn%2520to%2520directly%2520map%2520the%250Asource%2520image%2520to%2520target%2520one%2520under%2520user%2520guidance%252C%2520we%2520discuss%2520them%2520separately%252C%2520and%250Aintroduce%2520injection%2520schemes%2520of%2520source%2520image%2520in%2520different%2520scenarios.%250AAdditionally%252C%2520we%2520review%2520the%2520application%2520of%25202D%2520techniques%2520to%2520video%2520editing%252C%250Ahighlighting%2520solutions%2520for%2520inter-frame%2520inconsistency.%2520Finally%252C%2520we%2520discuss%2520open%250Achallenges%2520in%2520the%2520field%2520and%2520suggest%2520potential%2520future%2520research%2520directions.%2520We%250Akeep%2520tracing%2520related%2520works%2520at%250Ahttps%253A//github.com/xinchengshuai/Awesome-Image-Editing.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2406.14555v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=A%20Survey%20of%20Multimodal-Guided%20Image%20Editing%20with%20Text-to-Image%20Diffusion%0A%20%20Models&entry.906535625=Xincheng%20Shuai%20and%20Henghui%20Ding%20and%20Xingjun%20Ma%20and%20Rongcheng%20Tu%20and%20Yu-Gang%20Jiang%20and%20Dacheng%20Tao&entry.1292438233=%20%20Image%20editing%20aims%20to%20edit%20the%20given%20synthetic%20or%20real%20image%20to%20meet%20the%0Aspecific%20requirements%20from%20users.%20It%20is%20widely%20studied%20in%20recent%20years%20as%20a%0Apromising%20and%20challenging%20field%20of%20Artificial%20Intelligence%20Generative%20Content%0A%28AIGC%29.%20Recent%20significant%20advancement%20in%20this%20field%20is%20based%20on%20the%0Adevelopment%20of%20text-to-image%20%28T2I%29%20diffusion%20models%2C%20which%20generate%20images%0Aaccording%20to%20text%20prompts.%20These%20models%20demonstrate%20remarkable%20generative%0Acapabilities%20and%20have%20become%20widely%20used%20tools%20for%20image%20editing.%20T2I-based%0Aimage%20editing%20methods%20significantly%20enhance%20editing%20performance%20and%20offer%20a%0Auser-friendly%20interface%20for%20modifying%20content%20guided%20by%20multimodal%20inputs.%20In%0Athis%20survey%2C%20we%20provide%20a%20comprehensive%20review%20of%20multimodal-guided%20image%0Aediting%20techniques%20that%20leverage%20T2I%20diffusion%20models.%20First%2C%20we%20define%20the%0Ascope%20of%20image%20editing%20from%20a%20holistic%20perspective%20and%20detail%20various%20control%0Asignals%20and%20editing%20scenarios.%20We%20then%20propose%20a%20unified%20framework%20to%20formalize%0Athe%20editing%20process%2C%20categorizing%20it%20into%20two%20primary%20algorithm%20families.%20This%0Aframework%20offers%20a%20design%20space%20for%20users%20to%20achieve%20specific%20goals.%0ASubsequently%2C%20we%20present%20an%20in-depth%20analysis%20of%20each%20component%20within%20this%0Aframework%2C%20examining%20the%20characteristics%20and%20applicable%20scenarios%20of%20different%0Acombinations.%20Given%20that%20training-based%20methods%20learn%20to%20directly%20map%20the%0Asource%20image%20to%20target%20one%20under%20user%20guidance%2C%20we%20discuss%20them%20separately%2C%20and%0Aintroduce%20injection%20schemes%20of%20source%20image%20in%20different%20scenarios.%0AAdditionally%2C%20we%20review%20the%20application%20of%202D%20techniques%20to%20video%20editing%2C%0Ahighlighting%20solutions%20for%20inter-frame%20inconsistency.%20Finally%2C%20we%20discuss%20open%0Achallenges%20in%20the%20field%20and%20suggest%20potential%20future%20research%20directions.%20We%0Akeep%20tracing%20related%20works%20at%0Ahttps%3A//github.com/xinchengshuai/Awesome-Image-Editing.%0A&entry.1838667208=http%3A//arxiv.org/abs/2406.14555v1&entry.124074799=Read"},
{"title": "Biology-inspired joint distribution neurons based on Hierarchical\n  Correlation Reconstruction allowing for multidirectional neural networks", "author": "Jarek Duda", "abstract": "  Popular artificial neural networks (ANN) optimize parameters for\nunidirectional value propagation, assuming some arbitrary parametrization type\nlike Multi-Layer Perceptron (MLP) or Kolmogorov-Arnold Network (KAN). In\ncontrast, for biological neurons e.g. \"it is not uncommon for axonal\npropagation of action potentials to happen in both directions\"~\\cite{axon} -\nsuggesting they are optimized to continuously operate in multidirectional way.\nAdditionally, statistical dependencies a single neuron could model is not just\n(expected) value dependence, but entire joint distributions including also\nhigher moments. Such more agnostic joint distribution neuron would allow for\nmultidirectional propagation (of distributions or values) e.g. $\\rho(x|y,z)$ or\n$\\rho(y,z|x)$ by substituting to $\\rho(x,y,z)$ and normalizing. There will be\ndiscussed Hierarchical Correlation Reconstruction (HCR) for such neuron model:\nassuming $\\rho(x,y,z)=\\sum_{ijk} a_{ijk} f_i(x) f_j(y) f_k(z)$ type\nparametrization of joint distribution in polynomial basis $f_i$, which allows\nfor flexible, inexpensive processing including nonlinearities, direct model\nestimation and update, trained through standard backpropagation or novel ways\nfor such structure up to tensor decomposition or information bottleneck\napproach. Using only pairwise (input-output) dependencies, its expected value\nprediction becomes KAN-like with trained activation functions as polynomials,\ncan be extended by adding higher order dependencies through included products -\nin conscious interpretable way, allowing for multidirectional propagation of\nboth values and probability densities.\n", "link": "http://arxiv.org/abs/2405.05097v2", "date": "2024-06-20", "relevancy": 1.9261, "topK": [{"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.5254}, {"title": "Boosted dynamic neural networks", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/26302", "similarity": 0.5127}, {"title": "Dynamic Gaussians Mesh: Consistent Mesh Reconstruction from Monocular\n  Videos", "link": "http://arxiv.org/abs/2404.12379v2", "similarity": 0.4328}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Biology-inspired%20joint%20distribution%20neurons%20based%20on%20Hierarchical%0A%20%20Correlation%20Reconstruction%20allowing%20for%20multidirectional%20neural%20networks&body=Title%3A%20Biology-inspired%20joint%20distribution%20neurons%20based%20on%20Hierarchical%0A%20%20Correlation%20Reconstruction%20allowing%20for%20multidirectional%20neural%20networks%0AAuthor%3A%20Jarek%20Duda%0AAbstract%3A%20%20%20Popular%20artificial%20neural%20networks%20%28ANN%29%20optimize%20parameters%20for%0Aunidirectional%20value%20propagation%2C%20assuming%20some%20arbitrary%20parametrization%20type%0Alike%20Multi-Layer%20Perceptron%20%28MLP%29%20or%20Kolmogorov-Arnold%20Network%20%28KAN%29.%20In%0Acontrast%2C%20for%20biological%20neurons%20e.g.%20%22it%20is%20not%20uncommon%20for%20axonal%0Apropagation%20of%20action%20potentials%20to%20happen%20in%20both%20directions%22~%5Ccite%7Baxon%7D%20-%0Asuggesting%20they%20are%20optimized%20to%20continuously%20operate%20in%20multidirectional%20way.%0AAdditionally%2C%20statistical%20dependencies%20a%20single%20neuron%20could%20model%20is%20not%20just%0A%28expected%29%20value%20dependence%2C%20but%20entire%20joint%20distributions%20including%20also%0Ahigher%20moments.%20Such%20more%20agnostic%20joint%20distribution%20neuron%20would%20allow%20for%0Amultidirectional%20propagation%20%28of%20distributions%20or%20values%29%20e.g.%20%24%5Crho%28x%7Cy%2Cz%29%24%20or%0A%24%5Crho%28y%2Cz%7Cx%29%24%20by%20substituting%20to%20%24%5Crho%28x%2Cy%2Cz%29%24%20and%20normalizing.%20There%20will%20be%0Adiscussed%20Hierarchical%20Correlation%20Reconstruction%20%28HCR%29%20for%20such%20neuron%20model%3A%0Aassuming%20%24%5Crho%28x%2Cy%2Cz%29%3D%5Csum_%7Bijk%7D%20a_%7Bijk%7D%20f_i%28x%29%20f_j%28y%29%20f_k%28z%29%24%20type%0Aparametrization%20of%20joint%20distribution%20in%20polynomial%20basis%20%24f_i%24%2C%20which%20allows%0Afor%20flexible%2C%20inexpensive%20processing%20including%20nonlinearities%2C%20direct%20model%0Aestimation%20and%20update%2C%20trained%20through%20standard%20backpropagation%20or%20novel%20ways%0Afor%20such%20structure%20up%20to%20tensor%20decomposition%20or%20information%20bottleneck%0Aapproach.%20Using%20only%20pairwise%20%28input-output%29%20dependencies%2C%20its%20expected%20value%0Aprediction%20becomes%20KAN-like%20with%20trained%20activation%20functions%20as%20polynomials%2C%0Acan%20be%20extended%20by%20adding%20higher%20order%20dependencies%20through%20included%20products%20-%0Ain%20conscious%20interpretable%20way%2C%20allowing%20for%20multidirectional%20propagation%20of%0Aboth%20values%20and%20probability%20densities.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2405.05097v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DBiology-inspired%2520joint%2520distribution%2520neurons%2520based%2520on%2520Hierarchical%250A%2520%2520Correlation%2520Reconstruction%2520allowing%2520for%2520multidirectional%2520neural%2520networks%26entry.906535625%3DJarek%2520Duda%26entry.1292438233%3D%2520%2520Popular%2520artificial%2520neural%2520networks%2520%2528ANN%2529%2520optimize%2520parameters%2520for%250Aunidirectional%2520value%2520propagation%252C%2520assuming%2520some%2520arbitrary%2520parametrization%2520type%250Alike%2520Multi-Layer%2520Perceptron%2520%2528MLP%2529%2520or%2520Kolmogorov-Arnold%2520Network%2520%2528KAN%2529.%2520In%250Acontrast%252C%2520for%2520biological%2520neurons%2520e.g.%2520%2522it%2520is%2520not%2520uncommon%2520for%2520axonal%250Apropagation%2520of%2520action%2520potentials%2520to%2520happen%2520in%2520both%2520directions%2522~%255Ccite%257Baxon%257D%2520-%250Asuggesting%2520they%2520are%2520optimized%2520to%2520continuously%2520operate%2520in%2520multidirectional%2520way.%250AAdditionally%252C%2520statistical%2520dependencies%2520a%2520single%2520neuron%2520could%2520model%2520is%2520not%2520just%250A%2528expected%2529%2520value%2520dependence%252C%2520but%2520entire%2520joint%2520distributions%2520including%2520also%250Ahigher%2520moments.%2520Such%2520more%2520agnostic%2520joint%2520distribution%2520neuron%2520would%2520allow%2520for%250Amultidirectional%2520propagation%2520%2528of%2520distributions%2520or%2520values%2529%2520e.g.%2520%2524%255Crho%2528x%257Cy%252Cz%2529%2524%2520or%250A%2524%255Crho%2528y%252Cz%257Cx%2529%2524%2520by%2520substituting%2520to%2520%2524%255Crho%2528x%252Cy%252Cz%2529%2524%2520and%2520normalizing.%2520There%2520will%2520be%250Adiscussed%2520Hierarchical%2520Correlation%2520Reconstruction%2520%2528HCR%2529%2520for%2520such%2520neuron%2520model%253A%250Aassuming%2520%2524%255Crho%2528x%252Cy%252Cz%2529%253D%255Csum_%257Bijk%257D%2520a_%257Bijk%257D%2520f_i%2528x%2529%2520f_j%2528y%2529%2520f_k%2528z%2529%2524%2520type%250Aparametrization%2520of%2520joint%2520distribution%2520in%2520polynomial%2520basis%2520%2524f_i%2524%252C%2520which%2520allows%250Afor%2520flexible%252C%2520inexpensive%2520processing%2520including%2520nonlinearities%252C%2520direct%2520model%250Aestimation%2520and%2520update%252C%2520trained%2520through%2520standard%2520backpropagation%2520or%2520novel%2520ways%250Afor%2520such%2520structure%2520up%2520to%2520tensor%2520decomposition%2520or%2520information%2520bottleneck%250Aapproach.%2520Using%2520only%2520pairwise%2520%2528input-output%2529%2520dependencies%252C%2520its%2520expected%2520value%250Aprediction%2520becomes%2520KAN-like%2520with%2520trained%2520activation%2520functions%2520as%2520polynomials%252C%250Acan%2520be%2520extended%2520by%2520adding%2520higher%2520order%2520dependencies%2520through%2520included%2520products%2520-%250Ain%2520conscious%2520interpretable%2520way%252C%2520allowing%2520for%2520multidirectional%2520propagation%2520of%250Aboth%2520values%2520and%2520probability%2520densities.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2405.05097v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Biology-inspired%20joint%20distribution%20neurons%20based%20on%20Hierarchical%0A%20%20Correlation%20Reconstruction%20allowing%20for%20multidirectional%20neural%20networks&entry.906535625=Jarek%20Duda&entry.1292438233=%20%20Popular%20artificial%20neural%20networks%20%28ANN%29%20optimize%20parameters%20for%0Aunidirectional%20value%20propagation%2C%20assuming%20some%20arbitrary%20parametrization%20type%0Alike%20Multi-Layer%20Perceptron%20%28MLP%29%20or%20Kolmogorov-Arnold%20Network%20%28KAN%29.%20In%0Acontrast%2C%20for%20biological%20neurons%20e.g.%20%22it%20is%20not%20uncommon%20for%20axonal%0Apropagation%20of%20action%20potentials%20to%20happen%20in%20both%20directions%22~%5Ccite%7Baxon%7D%20-%0Asuggesting%20they%20are%20optimized%20to%20continuously%20operate%20in%20multidirectional%20way.%0AAdditionally%2C%20statistical%20dependencies%20a%20single%20neuron%20could%20model%20is%20not%20just%0A%28expected%29%20value%20dependence%2C%20but%20entire%20joint%20distributions%20including%20also%0Ahigher%20moments.%20Such%20more%20agnostic%20joint%20distribution%20neuron%20would%20allow%20for%0Amultidirectional%20propagation%20%28of%20distributions%20or%20values%29%20e.g.%20%24%5Crho%28x%7Cy%2Cz%29%24%20or%0A%24%5Crho%28y%2Cz%7Cx%29%24%20by%20substituting%20to%20%24%5Crho%28x%2Cy%2Cz%29%24%20and%20normalizing.%20There%20will%20be%0Adiscussed%20Hierarchical%20Correlation%20Reconstruction%20%28HCR%29%20for%20such%20neuron%20model%3A%0Aassuming%20%24%5Crho%28x%2Cy%2Cz%29%3D%5Csum_%7Bijk%7D%20a_%7Bijk%7D%20f_i%28x%29%20f_j%28y%29%20f_k%28z%29%24%20type%0Aparametrization%20of%20joint%20distribution%20in%20polynomial%20basis%20%24f_i%24%2C%20which%20allows%0Afor%20flexible%2C%20inexpensive%20processing%20including%20nonlinearities%2C%20direct%20model%0Aestimation%20and%20update%2C%20trained%20through%20standard%20backpropagation%20or%20novel%20ways%0Afor%20such%20structure%20up%20to%20tensor%20decomposition%20or%20information%20bottleneck%0Aapproach.%20Using%20only%20pairwise%20%28input-output%29%20dependencies%2C%20its%20expected%20value%0Aprediction%20becomes%20KAN-like%20with%20trained%20activation%20functions%20as%20polynomials%2C%0Acan%20be%20extended%20by%20adding%20higher%20order%20dependencies%20through%20included%20products%20-%0Ain%20conscious%20interpretable%20way%2C%20allowing%20for%20multidirectional%20propagation%20of%0Aboth%20values%20and%20probability%20densities.%0A&entry.1838667208=http%3A//arxiv.org/abs/2405.05097v2&entry.124074799=Read"},
{"title": "Semi-Autonomous Mobile Search and Rescue Robot for Radiation Disaster\n  Scenarios", "author": "Simon Schwaiger and Lucas Muster and Georg Novotny and Michael Schebek and Wilfried W\u00f6ber and Stefan Thalhammer and Christoph B\u00f6hm", "abstract": "  This paper describes a novel semi-autonomous mobile robot system designed to\nassist search and rescue (SAR) first responders in disaster scenarios. While\nrobots offer significant potential in SAR missions, current solutions are\nlimited in their ability to handle a diverse range of tasks. This gap is\naddressed by presenting a system capable of (1) autonomous navigation and\nmapping, allowing the robot to autonomously explore and map areas affected by\ncatastrophic events, (2) radiation mapping, enabling the system to triangulate\na radiation map from discrete radiation measurements to aid in identifying\nhazardous areas, (3) semi-autonomous substance sampling, allowing the robot to\ncollect samples of suspicious substances and analyze them onboard with\nimmediate classification, and (4) valve manipulation, enabling teleoperated\nclosing of valves that control hazardous material flow. This semi-autonomous\napproach balances human control over critical tasks like substance sampling\nwith efficient robot navigation in low-risk areas. The system is evaluated\nduring three trials that simulate possible disaster scenarios, two of which\nhave been recorded during the European Robotics Hackathon (EnRicH).\nFurthermore, we provide recorded sensor data as well as the implemented\nsoftware system as supplemental material through a GitHub repository:\nhttps://github.com/TW-Robotics/search-and-rescue-robot-IROS2024.\n", "link": "http://arxiv.org/abs/2406.14385v1", "date": "2024-06-20", "relevancy": 1.6494, "topK": [{"title": "RoboEXP: Action-Conditioned Scene Graph via Interactive Exploration for\n  Robotic Manipulation", "link": "http://arxiv.org/abs/2402.15487v1", "similarity": 0.5593}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.5534}, {"title": "Egocentric Computer Vision for Hands-Free Robotic Wheelchair Navigation", "link": "https://link.springer.com/article/10.1007/s10846-023-01807-4", "similarity": 0.5446}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Semi-Autonomous%20Mobile%20Search%20and%20Rescue%20Robot%20for%20Radiation%20Disaster%0A%20%20Scenarios&body=Title%3A%20Semi-Autonomous%20Mobile%20Search%20and%20Rescue%20Robot%20for%20Radiation%20Disaster%0A%20%20Scenarios%0AAuthor%3A%20Simon%20Schwaiger%20and%20Lucas%20Muster%20and%20Georg%20Novotny%20and%20Michael%20Schebek%20and%20Wilfried%20W%C3%B6ber%20and%20Stefan%20Thalhammer%20and%20Christoph%20B%C3%B6hm%0AAbstract%3A%20%20%20This%20paper%20describes%20a%20novel%20semi-autonomous%20mobile%20robot%20system%20designed%20to%0Aassist%20search%20and%20rescue%20%28SAR%29%20first%20responders%20in%20disaster%20scenarios.%20While%0Arobots%20offer%20significant%20potential%20in%20SAR%20missions%2C%20current%20solutions%20are%0Alimited%20in%20their%20ability%20to%20handle%20a%20diverse%20range%20of%20tasks.%20This%20gap%20is%0Aaddressed%20by%20presenting%20a%20system%20capable%20of%20%281%29%20autonomous%20navigation%20and%0Amapping%2C%20allowing%20the%20robot%20to%20autonomously%20explore%20and%20map%20areas%20affected%20by%0Acatastrophic%20events%2C%20%282%29%20radiation%20mapping%2C%20enabling%20the%20system%20to%20triangulate%0Aa%20radiation%20map%20from%20discrete%20radiation%20measurements%20to%20aid%20in%20identifying%0Ahazardous%20areas%2C%20%283%29%20semi-autonomous%20substance%20sampling%2C%20allowing%20the%20robot%20to%0Acollect%20samples%20of%20suspicious%20substances%20and%20analyze%20them%20onboard%20with%0Aimmediate%20classification%2C%20and%20%284%29%20valve%20manipulation%2C%20enabling%20teleoperated%0Aclosing%20of%20valves%20that%20control%20hazardous%20material%20flow.%20This%20semi-autonomous%0Aapproach%20balances%20human%20control%20over%20critical%20tasks%20like%20substance%20sampling%0Awith%20efficient%20robot%20navigation%20in%20low-risk%20areas.%20The%20system%20is%20evaluated%0Aduring%20three%20trials%20that%20simulate%20possible%20disaster%20scenarios%2C%20two%20of%20which%0Ahave%20been%20recorded%20during%20the%20European%20Robotics%20Hackathon%20%28EnRicH%29.%0AFurthermore%2C%20we%20provide%20recorded%20sensor%20data%20as%20well%20as%20the%20implemented%0Asoftware%20system%20as%20supplemental%20material%20through%20a%20GitHub%20repository%3A%0Ahttps%3A//github.com/TW-Robotics/search-and-rescue-robot-IROS2024.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2406.14385v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DSemi-Autonomous%2520Mobile%2520Search%2520and%2520Rescue%2520Robot%2520for%2520Radiation%2520Disaster%250A%2520%2520Scenarios%26entry.906535625%3DSimon%2520Schwaiger%2520and%2520Lucas%2520Muster%2520and%2520Georg%2520Novotny%2520and%2520Michael%2520Schebek%2520and%2520Wilfried%2520W%25C3%25B6ber%2520and%2520Stefan%2520Thalhammer%2520and%2520Christoph%2520B%25C3%25B6hm%26entry.1292438233%3D%2520%2520This%2520paper%2520describes%2520a%2520novel%2520semi-autonomous%2520mobile%2520robot%2520system%2520designed%2520to%250Aassist%2520search%2520and%2520rescue%2520%2528SAR%2529%2520first%2520responders%2520in%2520disaster%2520scenarios.%2520While%250Arobots%2520offer%2520significant%2520potential%2520in%2520SAR%2520missions%252C%2520current%2520solutions%2520are%250Alimited%2520in%2520their%2520ability%2520to%2520handle%2520a%2520diverse%2520range%2520of%2520tasks.%2520This%2520gap%2520is%250Aaddressed%2520by%2520presenting%2520a%2520system%2520capable%2520of%2520%25281%2529%2520autonomous%2520navigation%2520and%250Amapping%252C%2520allowing%2520the%2520robot%2520to%2520autonomously%2520explore%2520and%2520map%2520areas%2520affected%2520by%250Acatastrophic%2520events%252C%2520%25282%2529%2520radiation%2520mapping%252C%2520enabling%2520the%2520system%2520to%2520triangulate%250Aa%2520radiation%2520map%2520from%2520discrete%2520radiation%2520measurements%2520to%2520aid%2520in%2520identifying%250Ahazardous%2520areas%252C%2520%25283%2529%2520semi-autonomous%2520substance%2520sampling%252C%2520allowing%2520the%2520robot%2520to%250Acollect%2520samples%2520of%2520suspicious%2520substances%2520and%2520analyze%2520them%2520onboard%2520with%250Aimmediate%2520classification%252C%2520and%2520%25284%2529%2520valve%2520manipulation%252C%2520enabling%2520teleoperated%250Aclosing%2520of%2520valves%2520that%2520control%2520hazardous%2520material%2520flow.%2520This%2520semi-autonomous%250Aapproach%2520balances%2520human%2520control%2520over%2520critical%2520tasks%2520like%2520substance%2520sampling%250Awith%2520efficient%2520robot%2520navigation%2520in%2520low-risk%2520areas.%2520The%2520system%2520is%2520evaluated%250Aduring%2520three%2520trials%2520that%2520simulate%2520possible%2520disaster%2520scenarios%252C%2520two%2520of%2520which%250Ahave%2520been%2520recorded%2520during%2520the%2520European%2520Robotics%2520Hackathon%2520%2528EnRicH%2529.%250AFurthermore%252C%2520we%2520provide%2520recorded%2520sensor%2520data%2520as%2520well%2520as%2520the%2520implemented%250Asoftware%2520system%2520as%2520supplemental%2520material%2520through%2520a%2520GitHub%2520repository%253A%250Ahttps%253A//github.com/TW-Robotics/search-and-rescue-robot-IROS2024.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2406.14385v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Semi-Autonomous%20Mobile%20Search%20and%20Rescue%20Robot%20for%20Radiation%20Disaster%0A%20%20Scenarios&entry.906535625=Simon%20Schwaiger%20and%20Lucas%20Muster%20and%20Georg%20Novotny%20and%20Michael%20Schebek%20and%20Wilfried%20W%C3%B6ber%20and%20Stefan%20Thalhammer%20and%20Christoph%20B%C3%B6hm&entry.1292438233=%20%20This%20paper%20describes%20a%20novel%20semi-autonomous%20mobile%20robot%20system%20designed%20to%0Aassist%20search%20and%20rescue%20%28SAR%29%20first%20responders%20in%20disaster%20scenarios.%20While%0Arobots%20offer%20significant%20potential%20in%20SAR%20missions%2C%20current%20solutions%20are%0Alimited%20in%20their%20ability%20to%20handle%20a%20diverse%20range%20of%20tasks.%20This%20gap%20is%0Aaddressed%20by%20presenting%20a%20system%20capable%20of%20%281%29%20autonomous%20navigation%20and%0Amapping%2C%20allowing%20the%20robot%20to%20autonomously%20explore%20and%20map%20areas%20affected%20by%0Acatastrophic%20events%2C%20%282%29%20radiation%20mapping%2C%20enabling%20the%20system%20to%20triangulate%0Aa%20radiation%20map%20from%20discrete%20radiation%20measurements%20to%20aid%20in%20identifying%0Ahazardous%20areas%2C%20%283%29%20semi-autonomous%20substance%20sampling%2C%20allowing%20the%20robot%20to%0Acollect%20samples%20of%20suspicious%20substances%20and%20analyze%20them%20onboard%20with%0Aimmediate%20classification%2C%20and%20%284%29%20valve%20manipulation%2C%20enabling%20teleoperated%0Aclosing%20of%20valves%20that%20control%20hazardous%20material%20flow.%20This%20semi-autonomous%0Aapproach%20balances%20human%20control%20over%20critical%20tasks%20like%20substance%20sampling%0Awith%20efficient%20robot%20navigation%20in%20low-risk%20areas.%20The%20system%20is%20evaluated%0Aduring%20three%20trials%20that%20simulate%20possible%20disaster%20scenarios%2C%20two%20of%20which%0Ahave%20been%20recorded%20during%20the%20European%20Robotics%20Hackathon%20%28EnRicH%29.%0AFurthermore%2C%20we%20provide%20recorded%20sensor%20data%20as%20well%20as%20the%20implemented%0Asoftware%20system%20as%20supplemental%20material%20through%20a%20GitHub%20repository%3A%0Ahttps%3A//github.com/TW-Robotics/search-and-rescue-robot-IROS2024.%0A&entry.1838667208=http%3A//arxiv.org/abs/2406.14385v1&entry.124074799=Read"},
{"title": "Augmenting Query and Passage for Retrieval-Augmented Generation using\n  LLMs for Open-Domain Question Answering", "author": "Minsang Kim and Cheoneum Park and Seungjun Baek", "abstract": "  Retrieval-augmented generation (RAG) has received much attention for\nOpen-domain question-answering (ODQA) tasks as a means to compensate for the\nparametric knowledge of large language models (LLMs). While previous approaches\nfocused on processing retrieved passages to remove irrelevant context, they\nstill rely heavily on the quality of retrieved passages which can degrade if\nthe question is ambiguous or complex. In this paper, we propose a simple yet\nefficient method called question and passage augmentation via LLMs for\nopen-domain QA. Our method first decomposes the original questions into\nmultiple-step sub-questions. By augmenting the original question with detailed\nsub-questions and planning, we are able to make the query more specific on what\nneeds to be retrieved, improving the retrieval performance. In addition, to\ncompensate for the case where the retrieved passages contain distracting\ninformation or divided opinions, we augment the retrieved passages with\nself-generated passages by LLMs to guide the answer extraction. Experimental\nresults show that the proposed scheme outperforms the previous state-of-the-art\nand achieves significant performance gain over existing RAG methods.\n", "link": "http://arxiv.org/abs/2406.14277v1", "date": "2024-06-20", "relevancy": 1.3996, "topK": [{"title": "UGG: Unified Generative Grasping", "link": "https://arxiv.org/abs/2311.16917", "similarity": 0.4729}, {"title": "RoboEXP: Action-Conditioned Scene Graph via Interactive Exploration for\n  Robotic Manipulation", "link": "http://arxiv.org/abs/2402.15487v1", "similarity": 0.4722}, {"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.445}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Augmenting%20Query%20and%20Passage%20for%20Retrieval-Augmented%20Generation%20using%0A%20%20LLMs%20for%20Open-Domain%20Question%20Answering&body=Title%3A%20Augmenting%20Query%20and%20Passage%20for%20Retrieval-Augmented%20Generation%20using%0A%20%20LLMs%20for%20Open-Domain%20Question%20Answering%0AAuthor%3A%20Minsang%20Kim%20and%20Cheoneum%20Park%20and%20Seungjun%20Baek%0AAbstract%3A%20%20%20Retrieval-augmented%20generation%20%28RAG%29%20has%20received%20much%20attention%20for%0AOpen-domain%20question-answering%20%28ODQA%29%20tasks%20as%20a%20means%20to%20compensate%20for%20the%0Aparametric%20knowledge%20of%20large%20language%20models%20%28LLMs%29.%20While%20previous%20approaches%0Afocused%20on%20processing%20retrieved%20passages%20to%20remove%20irrelevant%20context%2C%20they%0Astill%20rely%20heavily%20on%20the%20quality%20of%20retrieved%20passages%20which%20can%20degrade%20if%0Athe%20question%20is%20ambiguous%20or%20complex.%20In%20this%20paper%2C%20we%20propose%20a%20simple%20yet%0Aefficient%20method%20called%20question%20and%20passage%20augmentation%20via%20LLMs%20for%0Aopen-domain%20QA.%20Our%20method%20first%20decomposes%20the%20original%20questions%20into%0Amultiple-step%20sub-questions.%20By%20augmenting%20the%20original%20question%20with%20detailed%0Asub-questions%20and%20planning%2C%20we%20are%20able%20to%20make%20the%20query%20more%20specific%20on%20what%0Aneeds%20to%20be%20retrieved%2C%20improving%20the%20retrieval%20performance.%20In%20addition%2C%20to%0Acompensate%20for%20the%20case%20where%20the%20retrieved%20passages%20contain%20distracting%0Ainformation%20or%20divided%20opinions%2C%20we%20augment%20the%20retrieved%20passages%20with%0Aself-generated%20passages%20by%20LLMs%20to%20guide%20the%20answer%20extraction.%20Experimental%0Aresults%20show%20that%20the%20proposed%20scheme%20outperforms%20the%20previous%20state-of-the-art%0Aand%20achieves%20significant%20performance%20gain%20over%20existing%20RAG%20methods.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2406.14277v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DAugmenting%2520Query%2520and%2520Passage%2520for%2520Retrieval-Augmented%2520Generation%2520using%250A%2520%2520LLMs%2520for%2520Open-Domain%2520Question%2520Answering%26entry.906535625%3DMinsang%2520Kim%2520and%2520Cheoneum%2520Park%2520and%2520Seungjun%2520Baek%26entry.1292438233%3D%2520%2520Retrieval-augmented%2520generation%2520%2528RAG%2529%2520has%2520received%2520much%2520attention%2520for%250AOpen-domain%2520question-answering%2520%2528ODQA%2529%2520tasks%2520as%2520a%2520means%2520to%2520compensate%2520for%2520the%250Aparametric%2520knowledge%2520of%2520large%2520language%2520models%2520%2528LLMs%2529.%2520While%2520previous%2520approaches%250Afocused%2520on%2520processing%2520retrieved%2520passages%2520to%2520remove%2520irrelevant%2520context%252C%2520they%250Astill%2520rely%2520heavily%2520on%2520the%2520quality%2520of%2520retrieved%2520passages%2520which%2520can%2520degrade%2520if%250Athe%2520question%2520is%2520ambiguous%2520or%2520complex.%2520In%2520this%2520paper%252C%2520we%2520propose%2520a%2520simple%2520yet%250Aefficient%2520method%2520called%2520question%2520and%2520passage%2520augmentation%2520via%2520LLMs%2520for%250Aopen-domain%2520QA.%2520Our%2520method%2520first%2520decomposes%2520the%2520original%2520questions%2520into%250Amultiple-step%2520sub-questions.%2520By%2520augmenting%2520the%2520original%2520question%2520with%2520detailed%250Asub-questions%2520and%2520planning%252C%2520we%2520are%2520able%2520to%2520make%2520the%2520query%2520more%2520specific%2520on%2520what%250Aneeds%2520to%2520be%2520retrieved%252C%2520improving%2520the%2520retrieval%2520performance.%2520In%2520addition%252C%2520to%250Acompensate%2520for%2520the%2520case%2520where%2520the%2520retrieved%2520passages%2520contain%2520distracting%250Ainformation%2520or%2520divided%2520opinions%252C%2520we%2520augment%2520the%2520retrieved%2520passages%2520with%250Aself-generated%2520passages%2520by%2520LLMs%2520to%2520guide%2520the%2520answer%2520extraction.%2520Experimental%250Aresults%2520show%2520that%2520the%2520proposed%2520scheme%2520outperforms%2520the%2520previous%2520state-of-the-art%250Aand%2520achieves%2520significant%2520performance%2520gain%2520over%2520existing%2520RAG%2520methods.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2406.14277v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Augmenting%20Query%20and%20Passage%20for%20Retrieval-Augmented%20Generation%20using%0A%20%20LLMs%20for%20Open-Domain%20Question%20Answering&entry.906535625=Minsang%20Kim%20and%20Cheoneum%20Park%20and%20Seungjun%20Baek&entry.1292438233=%20%20Retrieval-augmented%20generation%20%28RAG%29%20has%20received%20much%20attention%20for%0AOpen-domain%20question-answering%20%28ODQA%29%20tasks%20as%20a%20means%20to%20compensate%20for%20the%0Aparametric%20knowledge%20of%20large%20language%20models%20%28LLMs%29.%20While%20previous%20approaches%0Afocused%20on%20processing%20retrieved%20passages%20to%20remove%20irrelevant%20context%2C%20they%0Astill%20rely%20heavily%20on%20the%20quality%20of%20retrieved%20passages%20which%20can%20degrade%20if%0Athe%20question%20is%20ambiguous%20or%20complex.%20In%20this%20paper%2C%20we%20propose%20a%20simple%20yet%0Aefficient%20method%20called%20question%20and%20passage%20augmentation%20via%20LLMs%20for%0Aopen-domain%20QA.%20Our%20method%20first%20decomposes%20the%20original%20questions%20into%0Amultiple-step%20sub-questions.%20By%20augmenting%20the%20original%20question%20with%20detailed%0Asub-questions%20and%20planning%2C%20we%20are%20able%20to%20make%20the%20query%20more%20specific%20on%20what%0Aneeds%20to%20be%20retrieved%2C%20improving%20the%20retrieval%20performance.%20In%20addition%2C%20to%0Acompensate%20for%20the%20case%20where%20the%20retrieved%20passages%20contain%20distracting%0Ainformation%20or%20divided%20opinions%2C%20we%20augment%20the%20retrieved%20passages%20with%0Aself-generated%20passages%20by%20LLMs%20to%20guide%20the%20answer%20extraction.%20Experimental%0Aresults%20show%20that%20the%20proposed%20scheme%20outperforms%20the%20previous%20state-of-the-art%0Aand%20achieves%20significant%20performance%20gain%20over%20existing%20RAG%20methods.%0A&entry.1838667208=http%3A//arxiv.org/abs/2406.14277v1&entry.124074799=Read"},
{"title": "Estimating Treatment Effects under Recommender Interference: A\n  Structured Neural Networks Approach", "author": "Ruohan Zhan and Shichao Han and Yuchen Hu and Zhenling Jiang", "abstract": "  Recommender systems are essential for content-sharing platforms by curating\npersonalized content. To evaluate updates of recommender systems targeting\ncontent creators, platforms frequently engage in creator-side randomized\nexperiments to estimate treatment effect, defined as the difference in outcomes\nwhen a new (vs. the status quo) algorithm is deployed on the platform. We show\nthat the standard difference-in-means estimator can lead to a biased treatment\neffect estimate. This bias arises because of recommender interference, which\noccurs when treated and control creators compete for exposure through the\nrecommender system. We propose a \"recommender choice model\" that captures how\nan item is chosen among a pool comprised of both treated and control content\nitems. By combining a structural choice model with neural networks, the\nframework directly models the interference pathway in a microfounded way while\naccounting for rich viewer-content heterogeneity. Using the model, we construct\na double/debiased estimator of the treatment effect that is consistent and\nasymptotically normal. We demonstrate its empirical performance with a field\nexperiment on Weixin short-video platform: besides the standard creator-side\nexperiment, we carry out a costly blocked double-sided randomization design to\nobtain a benchmark estimate without interference bias. We show that the\nproposed estimator significantly reduces the bias in treatment effect estimates\ncompared to the standard difference-in-means estimator.\n", "link": "http://arxiv.org/abs/2406.14380v1", "date": "2024-06-20", "relevancy": 1.9199, "topK": [{"title": "Boosted dynamic neural networks", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/26302", "similarity": 0.5184}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.4619}, {"title": "Magic-Me: Identity-Specific Video Customized Diffusion", "link": "http://arxiv.org/abs/2402.09368v1", "similarity": 0.4488}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Estimating%20Treatment%20Effects%20under%20Recommender%20Interference%3A%20A%0A%20%20Structured%20Neural%20Networks%20Approach&body=Title%3A%20Estimating%20Treatment%20Effects%20under%20Recommender%20Interference%3A%20A%0A%20%20Structured%20Neural%20Networks%20Approach%0AAuthor%3A%20Ruohan%20Zhan%20and%20Shichao%20Han%20and%20Yuchen%20Hu%20and%20Zhenling%20Jiang%0AAbstract%3A%20%20%20Recommender%20systems%20are%20essential%20for%20content-sharing%20platforms%20by%20curating%0Apersonalized%20content.%20To%20evaluate%20updates%20of%20recommender%20systems%20targeting%0Acontent%20creators%2C%20platforms%20frequently%20engage%20in%20creator-side%20randomized%0Aexperiments%20to%20estimate%20treatment%20effect%2C%20defined%20as%20the%20difference%20in%20outcomes%0Awhen%20a%20new%20%28vs.%20the%20status%20quo%29%20algorithm%20is%20deployed%20on%20the%20platform.%20We%20show%0Athat%20the%20standard%20difference-in-means%20estimator%20can%20lead%20to%20a%20biased%20treatment%0Aeffect%20estimate.%20This%20bias%20arises%20because%20of%20recommender%20interference%2C%20which%0Aoccurs%20when%20treated%20and%20control%20creators%20compete%20for%20exposure%20through%20the%0Arecommender%20system.%20We%20propose%20a%20%22recommender%20choice%20model%22%20that%20captures%20how%0Aan%20item%20is%20chosen%20among%20a%20pool%20comprised%20of%20both%20treated%20and%20control%20content%0Aitems.%20By%20combining%20a%20structural%20choice%20model%20with%20neural%20networks%2C%20the%0Aframework%20directly%20models%20the%20interference%20pathway%20in%20a%20microfounded%20way%20while%0Aaccounting%20for%20rich%20viewer-content%20heterogeneity.%20Using%20the%20model%2C%20we%20construct%0Aa%20double/debiased%20estimator%20of%20the%20treatment%20effect%20that%20is%20consistent%20and%0Aasymptotically%20normal.%20We%20demonstrate%20its%20empirical%20performance%20with%20a%20field%0Aexperiment%20on%20Weixin%20short-video%20platform%3A%20besides%20the%20standard%20creator-side%0Aexperiment%2C%20we%20carry%20out%20a%20costly%20blocked%20double-sided%20randomization%20design%20to%0Aobtain%20a%20benchmark%20estimate%20without%20interference%20bias.%20We%20show%20that%20the%0Aproposed%20estimator%20significantly%20reduces%20the%20bias%20in%20treatment%20effect%20estimates%0Acompared%20to%20the%20standard%20difference-in-means%20estimator.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2406.14380v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DEstimating%2520Treatment%2520Effects%2520under%2520Recommender%2520Interference%253A%2520A%250A%2520%2520Structured%2520Neural%2520Networks%2520Approach%26entry.906535625%3DRuohan%2520Zhan%2520and%2520Shichao%2520Han%2520and%2520Yuchen%2520Hu%2520and%2520Zhenling%2520Jiang%26entry.1292438233%3D%2520%2520Recommender%2520systems%2520are%2520essential%2520for%2520content-sharing%2520platforms%2520by%2520curating%250Apersonalized%2520content.%2520To%2520evaluate%2520updates%2520of%2520recommender%2520systems%2520targeting%250Acontent%2520creators%252C%2520platforms%2520frequently%2520engage%2520in%2520creator-side%2520randomized%250Aexperiments%2520to%2520estimate%2520treatment%2520effect%252C%2520defined%2520as%2520the%2520difference%2520in%2520outcomes%250Awhen%2520a%2520new%2520%2528vs.%2520the%2520status%2520quo%2529%2520algorithm%2520is%2520deployed%2520on%2520the%2520platform.%2520We%2520show%250Athat%2520the%2520standard%2520difference-in-means%2520estimator%2520can%2520lead%2520to%2520a%2520biased%2520treatment%250Aeffect%2520estimate.%2520This%2520bias%2520arises%2520because%2520of%2520recommender%2520interference%252C%2520which%250Aoccurs%2520when%2520treated%2520and%2520control%2520creators%2520compete%2520for%2520exposure%2520through%2520the%250Arecommender%2520system.%2520We%2520propose%2520a%2520%2522recommender%2520choice%2520model%2522%2520that%2520captures%2520how%250Aan%2520item%2520is%2520chosen%2520among%2520a%2520pool%2520comprised%2520of%2520both%2520treated%2520and%2520control%2520content%250Aitems.%2520By%2520combining%2520a%2520structural%2520choice%2520model%2520with%2520neural%2520networks%252C%2520the%250Aframework%2520directly%2520models%2520the%2520interference%2520pathway%2520in%2520a%2520microfounded%2520way%2520while%250Aaccounting%2520for%2520rich%2520viewer-content%2520heterogeneity.%2520Using%2520the%2520model%252C%2520we%2520construct%250Aa%2520double/debiased%2520estimator%2520of%2520the%2520treatment%2520effect%2520that%2520is%2520consistent%2520and%250Aasymptotically%2520normal.%2520We%2520demonstrate%2520its%2520empirical%2520performance%2520with%2520a%2520field%250Aexperiment%2520on%2520Weixin%2520short-video%2520platform%253A%2520besides%2520the%2520standard%2520creator-side%250Aexperiment%252C%2520we%2520carry%2520out%2520a%2520costly%2520blocked%2520double-sided%2520randomization%2520design%2520to%250Aobtain%2520a%2520benchmark%2520estimate%2520without%2520interference%2520bias.%2520We%2520show%2520that%2520the%250Aproposed%2520estimator%2520significantly%2520reduces%2520the%2520bias%2520in%2520treatment%2520effect%2520estimates%250Acompared%2520to%2520the%2520standard%2520difference-in-means%2520estimator.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2406.14380v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Estimating%20Treatment%20Effects%20under%20Recommender%20Interference%3A%20A%0A%20%20Structured%20Neural%20Networks%20Approach&entry.906535625=Ruohan%20Zhan%20and%20Shichao%20Han%20and%20Yuchen%20Hu%20and%20Zhenling%20Jiang&entry.1292438233=%20%20Recommender%20systems%20are%20essential%20for%20content-sharing%20platforms%20by%20curating%0Apersonalized%20content.%20To%20evaluate%20updates%20of%20recommender%20systems%20targeting%0Acontent%20creators%2C%20platforms%20frequently%20engage%20in%20creator-side%20randomized%0Aexperiments%20to%20estimate%20treatment%20effect%2C%20defined%20as%20the%20difference%20in%20outcomes%0Awhen%20a%20new%20%28vs.%20the%20status%20quo%29%20algorithm%20is%20deployed%20on%20the%20platform.%20We%20show%0Athat%20the%20standard%20difference-in-means%20estimator%20can%20lead%20to%20a%20biased%20treatment%0Aeffect%20estimate.%20This%20bias%20arises%20because%20of%20recommender%20interference%2C%20which%0Aoccurs%20when%20treated%20and%20control%20creators%20compete%20for%20exposure%20through%20the%0Arecommender%20system.%20We%20propose%20a%20%22recommender%20choice%20model%22%20that%20captures%20how%0Aan%20item%20is%20chosen%20among%20a%20pool%20comprised%20of%20both%20treated%20and%20control%20content%0Aitems.%20By%20combining%20a%20structural%20choice%20model%20with%20neural%20networks%2C%20the%0Aframework%20directly%20models%20the%20interference%20pathway%20in%20a%20microfounded%20way%20while%0Aaccounting%20for%20rich%20viewer-content%20heterogeneity.%20Using%20the%20model%2C%20we%20construct%0Aa%20double/debiased%20estimator%20of%20the%20treatment%20effect%20that%20is%20consistent%20and%0Aasymptotically%20normal.%20We%20demonstrate%20its%20empirical%20performance%20with%20a%20field%0Aexperiment%20on%20Weixin%20short-video%20platform%3A%20besides%20the%20standard%20creator-side%0Aexperiment%2C%20we%20carry%20out%20a%20costly%20blocked%20double-sided%20randomization%20design%20to%0Aobtain%20a%20benchmark%20estimate%20without%20interference%20bias.%20We%20show%20that%20the%0Aproposed%20estimator%20significantly%20reduces%20the%20bias%20in%20treatment%20effect%20estimates%0Acompared%20to%20the%20standard%20difference-in-means%20estimator.%0A&entry.1838667208=http%3A//arxiv.org/abs/2406.14380v1&entry.124074799=Read"},
{"title": "VLBiasBench: A Comprehensive Benchmark for Evaluating Bias in Large\n  Vision-Language Model", "author": "Jie Zhang and Sibo Wang and Xiangkui Cao and Zheng Yuan and Shiguang Shan and Xilin Chen and Wen Gao", "abstract": "  The emergence of Large Vision-Language Models (LVLMs) marks significant\nstrides towards achieving general artificial intelligence. However, these\nadvancements are tempered by the outputs that often reflect biases, a concern\nnot yet extensively investigated. Existing benchmarks are not sufficiently\ncomprehensive in evaluating biases due to their limited data scale, single\nquestioning format and narrow sources of bias. To address this problem, we\nintroduce VLBiasBench, a benchmark aimed at evaluating biases in LVLMs\ncomprehensively. In VLBiasBench, we construct a dataset encompassing nine\ndistinct categories of social biases, including age, disability status, gender,\nnationality, physical appearance, race, religion, profession, social economic\nstatus and two intersectional bias categories (race x gender, and race x social\neconomic status). To create a large-scale dataset, we use Stable Diffusion XL\nmodel to generate 46,848 high-quality images, which are combined with different\nquestions to form 128,342 samples. These questions are categorized into open\nand close ended types, fully considering the sources of bias and\ncomprehensively evaluating the biases of LVLM from multiple perspectives. We\nsubsequently conduct extensive evaluations on 15 open-source models as well as\none advanced closed-source model, providing some new insights into the biases\nrevealing from these models. Our benchmark is available at\nhttps://github.com/Xiangkui-Cao/VLBiasBench.\n", "link": "http://arxiv.org/abs/2406.14194v1", "date": "2024-06-20", "relevancy": 1.4357, "topK": [{"title": "Multimodal-Conditioned Latent Diffusion Models for Fashion Image Editing", "link": "http://arxiv.org/abs/2403.14828v2", "similarity": 0.4819}, {"title": "Flexible Visual Recognition by Evidential Modeling of Confusion and Ignorance", "link": "http://openaccess.thecvf.com/content/ICCV2023/html/Fan_Flexible_Visual_Recognition_by_Evidential_Modeling_of_Confusion_and_Ignorance_ICCV_2023_paper.html", "similarity": 0.4785}, {"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.4755}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20VLBiasBench%3A%20A%20Comprehensive%20Benchmark%20for%20Evaluating%20Bias%20in%20Large%0A%20%20Vision-Language%20Model&body=Title%3A%20VLBiasBench%3A%20A%20Comprehensive%20Benchmark%20for%20Evaluating%20Bias%20in%20Large%0A%20%20Vision-Language%20Model%0AAuthor%3A%20Jie%20Zhang%20and%20Sibo%20Wang%20and%20Xiangkui%20Cao%20and%20Zheng%20Yuan%20and%20Shiguang%20Shan%20and%20Xilin%20Chen%20and%20Wen%20Gao%0AAbstract%3A%20%20%20The%20emergence%20of%20Large%20Vision-Language%20Models%20%28LVLMs%29%20marks%20significant%0Astrides%20towards%20achieving%20general%20artificial%20intelligence.%20However%2C%20these%0Aadvancements%20are%20tempered%20by%20the%20outputs%20that%20often%20reflect%20biases%2C%20a%20concern%0Anot%20yet%20extensively%20investigated.%20Existing%20benchmarks%20are%20not%20sufficiently%0Acomprehensive%20in%20evaluating%20biases%20due%20to%20their%20limited%20data%20scale%2C%20single%0Aquestioning%20format%20and%20narrow%20sources%20of%20bias.%20To%20address%20this%20problem%2C%20we%0Aintroduce%20VLBiasBench%2C%20a%20benchmark%20aimed%20at%20evaluating%20biases%20in%20LVLMs%0Acomprehensively.%20In%20VLBiasBench%2C%20we%20construct%20a%20dataset%20encompassing%20nine%0Adistinct%20categories%20of%20social%20biases%2C%20including%20age%2C%20disability%20status%2C%20gender%2C%0Anationality%2C%20physical%20appearance%2C%20race%2C%20religion%2C%20profession%2C%20social%20economic%0Astatus%20and%20two%20intersectional%20bias%20categories%20%28race%20x%20gender%2C%20and%20race%20x%20social%0Aeconomic%20status%29.%20To%20create%20a%20large-scale%20dataset%2C%20we%20use%20Stable%20Diffusion%20XL%0Amodel%20to%20generate%2046%2C848%20high-quality%20images%2C%20which%20are%20combined%20with%20different%0Aquestions%20to%20form%20128%2C342%20samples.%20These%20questions%20are%20categorized%20into%20open%0Aand%20close%20ended%20types%2C%20fully%20considering%20the%20sources%20of%20bias%20and%0Acomprehensively%20evaluating%20the%20biases%20of%20LVLM%20from%20multiple%20perspectives.%20We%0Asubsequently%20conduct%20extensive%20evaluations%20on%2015%20open-source%20models%20as%20well%20as%0Aone%20advanced%20closed-source%20model%2C%20providing%20some%20new%20insights%20into%20the%20biases%0Arevealing%20from%20these%20models.%20Our%20benchmark%20is%20available%20at%0Ahttps%3A//github.com/Xiangkui-Cao/VLBiasBench.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2406.14194v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DVLBiasBench%253A%2520A%2520Comprehensive%2520Benchmark%2520for%2520Evaluating%2520Bias%2520in%2520Large%250A%2520%2520Vision-Language%2520Model%26entry.906535625%3DJie%2520Zhang%2520and%2520Sibo%2520Wang%2520and%2520Xiangkui%2520Cao%2520and%2520Zheng%2520Yuan%2520and%2520Shiguang%2520Shan%2520and%2520Xilin%2520Chen%2520and%2520Wen%2520Gao%26entry.1292438233%3D%2520%2520The%2520emergence%2520of%2520Large%2520Vision-Language%2520Models%2520%2528LVLMs%2529%2520marks%2520significant%250Astrides%2520towards%2520achieving%2520general%2520artificial%2520intelligence.%2520However%252C%2520these%250Aadvancements%2520are%2520tempered%2520by%2520the%2520outputs%2520that%2520often%2520reflect%2520biases%252C%2520a%2520concern%250Anot%2520yet%2520extensively%2520investigated.%2520Existing%2520benchmarks%2520are%2520not%2520sufficiently%250Acomprehensive%2520in%2520evaluating%2520biases%2520due%2520to%2520their%2520limited%2520data%2520scale%252C%2520single%250Aquestioning%2520format%2520and%2520narrow%2520sources%2520of%2520bias.%2520To%2520address%2520this%2520problem%252C%2520we%250Aintroduce%2520VLBiasBench%252C%2520a%2520benchmark%2520aimed%2520at%2520evaluating%2520biases%2520in%2520LVLMs%250Acomprehensively.%2520In%2520VLBiasBench%252C%2520we%2520construct%2520a%2520dataset%2520encompassing%2520nine%250Adistinct%2520categories%2520of%2520social%2520biases%252C%2520including%2520age%252C%2520disability%2520status%252C%2520gender%252C%250Anationality%252C%2520physical%2520appearance%252C%2520race%252C%2520religion%252C%2520profession%252C%2520social%2520economic%250Astatus%2520and%2520two%2520intersectional%2520bias%2520categories%2520%2528race%2520x%2520gender%252C%2520and%2520race%2520x%2520social%250Aeconomic%2520status%2529.%2520To%2520create%2520a%2520large-scale%2520dataset%252C%2520we%2520use%2520Stable%2520Diffusion%2520XL%250Amodel%2520to%2520generate%252046%252C848%2520high-quality%2520images%252C%2520which%2520are%2520combined%2520with%2520different%250Aquestions%2520to%2520form%2520128%252C342%2520samples.%2520These%2520questions%2520are%2520categorized%2520into%2520open%250Aand%2520close%2520ended%2520types%252C%2520fully%2520considering%2520the%2520sources%2520of%2520bias%2520and%250Acomprehensively%2520evaluating%2520the%2520biases%2520of%2520LVLM%2520from%2520multiple%2520perspectives.%2520We%250Asubsequently%2520conduct%2520extensive%2520evaluations%2520on%252015%2520open-source%2520models%2520as%2520well%2520as%250Aone%2520advanced%2520closed-source%2520model%252C%2520providing%2520some%2520new%2520insights%2520into%2520the%2520biases%250Arevealing%2520from%2520these%2520models.%2520Our%2520benchmark%2520is%2520available%2520at%250Ahttps%253A//github.com/Xiangkui-Cao/VLBiasBench.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2406.14194v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=VLBiasBench%3A%20A%20Comprehensive%20Benchmark%20for%20Evaluating%20Bias%20in%20Large%0A%20%20Vision-Language%20Model&entry.906535625=Jie%20Zhang%20and%20Sibo%20Wang%20and%20Xiangkui%20Cao%20and%20Zheng%20Yuan%20and%20Shiguang%20Shan%20and%20Xilin%20Chen%20and%20Wen%20Gao&entry.1292438233=%20%20The%20emergence%20of%20Large%20Vision-Language%20Models%20%28LVLMs%29%20marks%20significant%0Astrides%20towards%20achieving%20general%20artificial%20intelligence.%20However%2C%20these%0Aadvancements%20are%20tempered%20by%20the%20outputs%20that%20often%20reflect%20biases%2C%20a%20concern%0Anot%20yet%20extensively%20investigated.%20Existing%20benchmarks%20are%20not%20sufficiently%0Acomprehensive%20in%20evaluating%20biases%20due%20to%20their%20limited%20data%20scale%2C%20single%0Aquestioning%20format%20and%20narrow%20sources%20of%20bias.%20To%20address%20this%20problem%2C%20we%0Aintroduce%20VLBiasBench%2C%20a%20benchmark%20aimed%20at%20evaluating%20biases%20in%20LVLMs%0Acomprehensively.%20In%20VLBiasBench%2C%20we%20construct%20a%20dataset%20encompassing%20nine%0Adistinct%20categories%20of%20social%20biases%2C%20including%20age%2C%20disability%20status%2C%20gender%2C%0Anationality%2C%20physical%20appearance%2C%20race%2C%20religion%2C%20profession%2C%20social%20economic%0Astatus%20and%20two%20intersectional%20bias%20categories%20%28race%20x%20gender%2C%20and%20race%20x%20social%0Aeconomic%20status%29.%20To%20create%20a%20large-scale%20dataset%2C%20we%20use%20Stable%20Diffusion%20XL%0Amodel%20to%20generate%2046%2C848%20high-quality%20images%2C%20which%20are%20combined%20with%20different%0Aquestions%20to%20form%20128%2C342%20samples.%20These%20questions%20are%20categorized%20into%20open%0Aand%20close%20ended%20types%2C%20fully%20considering%20the%20sources%20of%20bias%20and%0Acomprehensively%20evaluating%20the%20biases%20of%20LVLM%20from%20multiple%20perspectives.%20We%0Asubsequently%20conduct%20extensive%20evaluations%20on%2015%20open-source%20models%20as%20well%20as%0Aone%20advanced%20closed-source%20model%2C%20providing%20some%20new%20insights%20into%20the%20biases%0Arevealing%20from%20these%20models.%20Our%20benchmark%20is%20available%20at%0Ahttps%3A//github.com/Xiangkui-Cao/VLBiasBench.%0A&entry.1838667208=http%3A//arxiv.org/abs/2406.14194v1&entry.124074799=Read"},
{"title": "Reproducibility in Machine Learning-based Research: Overview, Barriers\n  and Drivers", "author": "Harald Semmelrock and Tony Ross-Hellauer and Simone Kopeinik and Dieter Theiler and Armin Haberl and Stefan Thalmann and Dominik Kowald", "abstract": "  Research in various fields is currently experiencing challenges regarding the\nreproducibility of results. This problem is also prevalent in machine learning\n(ML) research. The issue arises primarily due to unpublished data and/or source\ncode and the sensitivity of ML training conditions. Although different\nsolutions have been proposed to address this issue, such as using ML platforms,\nthe level of reproducibility in ML-driven research remains unsatisfactory.\nTherefore, in this article, we discuss the reproducibility of ML-driven\nresearch with three main aims: (i) identify the barriers to reproducibility\nwhen applying ML in research as well as categorize the barriers to different\ntypes of reproducibility (description, code, data, and experiment\nreproducibility), (ii) identify potential drivers such as tools, practices, and\ninterventions that support ML reproducibility as well as distinguish between\ntechnology-driven drivers, procedural drivers, and drivers related to awareness\nand education, and (iii) map the drivers to the barriers. With this work, we\nhope to provide insights and contribute to the decision-making process\nregarding the adoption of different solutions to support ML reproducibility.\n", "link": "http://arxiv.org/abs/2406.14325v1", "date": "2024-06-20", "relevancy": 1.2049, "topK": [{"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.4182}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.4097}, {"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.3918}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Reproducibility%20in%20Machine%20Learning-based%20Research%3A%20Overview%2C%20Barriers%0A%20%20and%20Drivers&body=Title%3A%20Reproducibility%20in%20Machine%20Learning-based%20Research%3A%20Overview%2C%20Barriers%0A%20%20and%20Drivers%0AAuthor%3A%20Harald%20Semmelrock%20and%20Tony%20Ross-Hellauer%20and%20Simone%20Kopeinik%20and%20Dieter%20Theiler%20and%20Armin%20Haberl%20and%20Stefan%20Thalmann%20and%20Dominik%20Kowald%0AAbstract%3A%20%20%20Research%20in%20various%20fields%20is%20currently%20experiencing%20challenges%20regarding%20the%0Areproducibility%20of%20results.%20This%20problem%20is%20also%20prevalent%20in%20machine%20learning%0A%28ML%29%20research.%20The%20issue%20arises%20primarily%20due%20to%20unpublished%20data%20and/or%20source%0Acode%20and%20the%20sensitivity%20of%20ML%20training%20conditions.%20Although%20different%0Asolutions%20have%20been%20proposed%20to%20address%20this%20issue%2C%20such%20as%20using%20ML%20platforms%2C%0Athe%20level%20of%20reproducibility%20in%20ML-driven%20research%20remains%20unsatisfactory.%0ATherefore%2C%20in%20this%20article%2C%20we%20discuss%20the%20reproducibility%20of%20ML-driven%0Aresearch%20with%20three%20main%20aims%3A%20%28i%29%20identify%20the%20barriers%20to%20reproducibility%0Awhen%20applying%20ML%20in%20research%20as%20well%20as%20categorize%20the%20barriers%20to%20different%0Atypes%20of%20reproducibility%20%28description%2C%20code%2C%20data%2C%20and%20experiment%0Areproducibility%29%2C%20%28ii%29%20identify%20potential%20drivers%20such%20as%20tools%2C%20practices%2C%20and%0Ainterventions%20that%20support%20ML%20reproducibility%20as%20well%20as%20distinguish%20between%0Atechnology-driven%20drivers%2C%20procedural%20drivers%2C%20and%20drivers%20related%20to%20awareness%0Aand%20education%2C%20and%20%28iii%29%20map%20the%20drivers%20to%20the%20barriers.%20With%20this%20work%2C%20we%0Ahope%20to%20provide%20insights%20and%20contribute%20to%20the%20decision-making%20process%0Aregarding%20the%20adoption%20of%20different%20solutions%20to%20support%20ML%20reproducibility.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2406.14325v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DReproducibility%2520in%2520Machine%2520Learning-based%2520Research%253A%2520Overview%252C%2520Barriers%250A%2520%2520and%2520Drivers%26entry.906535625%3DHarald%2520Semmelrock%2520and%2520Tony%2520Ross-Hellauer%2520and%2520Simone%2520Kopeinik%2520and%2520Dieter%2520Theiler%2520and%2520Armin%2520Haberl%2520and%2520Stefan%2520Thalmann%2520and%2520Dominik%2520Kowald%26entry.1292438233%3D%2520%2520Research%2520in%2520various%2520fields%2520is%2520currently%2520experiencing%2520challenges%2520regarding%2520the%250Areproducibility%2520of%2520results.%2520This%2520problem%2520is%2520also%2520prevalent%2520in%2520machine%2520learning%250A%2528ML%2529%2520research.%2520The%2520issue%2520arises%2520primarily%2520due%2520to%2520unpublished%2520data%2520and/or%2520source%250Acode%2520and%2520the%2520sensitivity%2520of%2520ML%2520training%2520conditions.%2520Although%2520different%250Asolutions%2520have%2520been%2520proposed%2520to%2520address%2520this%2520issue%252C%2520such%2520as%2520using%2520ML%2520platforms%252C%250Athe%2520level%2520of%2520reproducibility%2520in%2520ML-driven%2520research%2520remains%2520unsatisfactory.%250ATherefore%252C%2520in%2520this%2520article%252C%2520we%2520discuss%2520the%2520reproducibility%2520of%2520ML-driven%250Aresearch%2520with%2520three%2520main%2520aims%253A%2520%2528i%2529%2520identify%2520the%2520barriers%2520to%2520reproducibility%250Awhen%2520applying%2520ML%2520in%2520research%2520as%2520well%2520as%2520categorize%2520the%2520barriers%2520to%2520different%250Atypes%2520of%2520reproducibility%2520%2528description%252C%2520code%252C%2520data%252C%2520and%2520experiment%250Areproducibility%2529%252C%2520%2528ii%2529%2520identify%2520potential%2520drivers%2520such%2520as%2520tools%252C%2520practices%252C%2520and%250Ainterventions%2520that%2520support%2520ML%2520reproducibility%2520as%2520well%2520as%2520distinguish%2520between%250Atechnology-driven%2520drivers%252C%2520procedural%2520drivers%252C%2520and%2520drivers%2520related%2520to%2520awareness%250Aand%2520education%252C%2520and%2520%2528iii%2529%2520map%2520the%2520drivers%2520to%2520the%2520barriers.%2520With%2520this%2520work%252C%2520we%250Ahope%2520to%2520provide%2520insights%2520and%2520contribute%2520to%2520the%2520decision-making%2520process%250Aregarding%2520the%2520adoption%2520of%2520different%2520solutions%2520to%2520support%2520ML%2520reproducibility.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2406.14325v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Reproducibility%20in%20Machine%20Learning-based%20Research%3A%20Overview%2C%20Barriers%0A%20%20and%20Drivers&entry.906535625=Harald%20Semmelrock%20and%20Tony%20Ross-Hellauer%20and%20Simone%20Kopeinik%20and%20Dieter%20Theiler%20and%20Armin%20Haberl%20and%20Stefan%20Thalmann%20and%20Dominik%20Kowald&entry.1292438233=%20%20Research%20in%20various%20fields%20is%20currently%20experiencing%20challenges%20regarding%20the%0Areproducibility%20of%20results.%20This%20problem%20is%20also%20prevalent%20in%20machine%20learning%0A%28ML%29%20research.%20The%20issue%20arises%20primarily%20due%20to%20unpublished%20data%20and/or%20source%0Acode%20and%20the%20sensitivity%20of%20ML%20training%20conditions.%20Although%20different%0Asolutions%20have%20been%20proposed%20to%20address%20this%20issue%2C%20such%20as%20using%20ML%20platforms%2C%0Athe%20level%20of%20reproducibility%20in%20ML-driven%20research%20remains%20unsatisfactory.%0ATherefore%2C%20in%20this%20article%2C%20we%20discuss%20the%20reproducibility%20of%20ML-driven%0Aresearch%20with%20three%20main%20aims%3A%20%28i%29%20identify%20the%20barriers%20to%20reproducibility%0Awhen%20applying%20ML%20in%20research%20as%20well%20as%20categorize%20the%20barriers%20to%20different%0Atypes%20of%20reproducibility%20%28description%2C%20code%2C%20data%2C%20and%20experiment%0Areproducibility%29%2C%20%28ii%29%20identify%20potential%20drivers%20such%20as%20tools%2C%20practices%2C%20and%0Ainterventions%20that%20support%20ML%20reproducibility%20as%20well%20as%20distinguish%20between%0Atechnology-driven%20drivers%2C%20procedural%20drivers%2C%20and%20drivers%20related%20to%20awareness%0Aand%20education%2C%20and%20%28iii%29%20map%20the%20drivers%20to%20the%20barriers.%20With%20this%20work%2C%20we%0Ahope%20to%20provide%20insights%20and%20contribute%20to%20the%20decision-making%20process%0Aregarding%20the%20adoption%20of%20different%20solutions%20to%20support%20ML%20reproducibility.%0A&entry.1838667208=http%3A//arxiv.org/abs/2406.14325v1&entry.124074799=Read"},
{"title": "$\\nabla^2$DFT: A Universal Quantum Chemistry Dataset of Drug-Like\n  Molecules and a Benchmark for Neural Network Potentials", "author": "Kuzma Khrabrov and Anton Ber and Artem Tsypin and Konstantin Ushenin and Egor Rumiantsev and Alexander Telepov and Dmitry Protasov and Ilya Shenbin and Anton Alekseev and Mikhail Shirokikh and Sergey Nikolenko and Elena Tutubalina and Artur Kadurin", "abstract": "  Methods of computational quantum chemistry provide accurate approximations of\nmolecular properties crucial for computer-aided drug discovery and other areas\nof chemical science. However, high computational complexity limits the\nscalability of their applications. Neural network potentials (NNPs) are a\npromising alternative to quantum chemistry methods, but they require large and\ndiverse datasets for training. This work presents a new dataset and benchmark\ncalled $\\nabla^2$DFT that is based on the nablaDFT. It contains twice as much\nmolecular structures, three times more conformations, new data types and tasks,\nand state-of-the-art models. The dataset includes energies, forces, 17\nmolecular properties, Hamiltonian and overlap matrices, and a wavefunction\nobject. All calculations were performed at the DFT level\n($\\omega$B97X-D/def2-SVP) for each conformation. Moreover, $\\nabla^2$DFT is the\nfirst dataset that contains relaxation trajectories for a substantial number of\ndrug-like molecules. We also introduce a novel benchmark for evaluating NNPs in\nmolecular property prediction, Hamiltonian prediction, and conformational\noptimization tasks. Finally, we propose an extendable framework for training\nNNPs and implement 10 models within it.\n", "link": "http://arxiv.org/abs/2406.14347v1", "date": "2024-06-20", "relevancy": 1.7252, "topK": [{"title": "SDXS: Real-Time One-Step Latent Diffusion Models with Image Conditions", "link": "http://arxiv.org/abs/2403.16627v1", "similarity": 0.4492}, {"title": "Boosted dynamic neural networks", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/26302", "similarity": 0.4314}, {"title": "ToDo: Token Downsampling for Efficient Generation of High-Resolution\n  Images", "link": "http://arxiv.org/abs/2402.13573v2", "similarity": 0.424}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20%24%5Cnabla%5E2%24DFT%3A%20A%20Universal%20Quantum%20Chemistry%20Dataset%20of%20Drug-Like%0A%20%20Molecules%20and%20a%20Benchmark%20for%20Neural%20Network%20Potentials&body=Title%3A%20%24%5Cnabla%5E2%24DFT%3A%20A%20Universal%20Quantum%20Chemistry%20Dataset%20of%20Drug-Like%0A%20%20Molecules%20and%20a%20Benchmark%20for%20Neural%20Network%20Potentials%0AAuthor%3A%20Kuzma%20Khrabrov%20and%20Anton%20Ber%20and%20Artem%20Tsypin%20and%20Konstantin%20Ushenin%20and%20Egor%20Rumiantsev%20and%20Alexander%20Telepov%20and%20Dmitry%20Protasov%20and%20Ilya%20Shenbin%20and%20Anton%20Alekseev%20and%20Mikhail%20Shirokikh%20and%20Sergey%20Nikolenko%20and%20Elena%20Tutubalina%20and%20Artur%20Kadurin%0AAbstract%3A%20%20%20Methods%20of%20computational%20quantum%20chemistry%20provide%20accurate%20approximations%20of%0Amolecular%20properties%20crucial%20for%20computer-aided%20drug%20discovery%20and%20other%20areas%0Aof%20chemical%20science.%20However%2C%20high%20computational%20complexity%20limits%20the%0Ascalability%20of%20their%20applications.%20Neural%20network%20potentials%20%28NNPs%29%20are%20a%0Apromising%20alternative%20to%20quantum%20chemistry%20methods%2C%20but%20they%20require%20large%20and%0Adiverse%20datasets%20for%20training.%20This%20work%20presents%20a%20new%20dataset%20and%20benchmark%0Acalled%20%24%5Cnabla%5E2%24DFT%20that%20is%20based%20on%20the%20nablaDFT.%20It%20contains%20twice%20as%20much%0Amolecular%20structures%2C%20three%20times%20more%20conformations%2C%20new%20data%20types%20and%20tasks%2C%0Aand%20state-of-the-art%20models.%20The%20dataset%20includes%20energies%2C%20forces%2C%2017%0Amolecular%20properties%2C%20Hamiltonian%20and%20overlap%20matrices%2C%20and%20a%20wavefunction%0Aobject.%20All%20calculations%20were%20performed%20at%20the%20DFT%20level%0A%28%24%5Comega%24B97X-D/def2-SVP%29%20for%20each%20conformation.%20Moreover%2C%20%24%5Cnabla%5E2%24DFT%20is%20the%0Afirst%20dataset%20that%20contains%20relaxation%20trajectories%20for%20a%20substantial%20number%20of%0Adrug-like%20molecules.%20We%20also%20introduce%20a%20novel%20benchmark%20for%20evaluating%20NNPs%20in%0Amolecular%20property%20prediction%2C%20Hamiltonian%20prediction%2C%20and%20conformational%0Aoptimization%20tasks.%20Finally%2C%20we%20propose%20an%20extendable%20framework%20for%20training%0ANNPs%20and%20implement%2010%20models%20within%20it.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2406.14347v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3D%2524%255Cnabla%255E2%2524DFT%253A%2520A%2520Universal%2520Quantum%2520Chemistry%2520Dataset%2520of%2520Drug-Like%250A%2520%2520Molecules%2520and%2520a%2520Benchmark%2520for%2520Neural%2520Network%2520Potentials%26entry.906535625%3DKuzma%2520Khrabrov%2520and%2520Anton%2520Ber%2520and%2520Artem%2520Tsypin%2520and%2520Konstantin%2520Ushenin%2520and%2520Egor%2520Rumiantsev%2520and%2520Alexander%2520Telepov%2520and%2520Dmitry%2520Protasov%2520and%2520Ilya%2520Shenbin%2520and%2520Anton%2520Alekseev%2520and%2520Mikhail%2520Shirokikh%2520and%2520Sergey%2520Nikolenko%2520and%2520Elena%2520Tutubalina%2520and%2520Artur%2520Kadurin%26entry.1292438233%3D%2520%2520Methods%2520of%2520computational%2520quantum%2520chemistry%2520provide%2520accurate%2520approximations%2520of%250Amolecular%2520properties%2520crucial%2520for%2520computer-aided%2520drug%2520discovery%2520and%2520other%2520areas%250Aof%2520chemical%2520science.%2520However%252C%2520high%2520computational%2520complexity%2520limits%2520the%250Ascalability%2520of%2520their%2520applications.%2520Neural%2520network%2520potentials%2520%2528NNPs%2529%2520are%2520a%250Apromising%2520alternative%2520to%2520quantum%2520chemistry%2520methods%252C%2520but%2520they%2520require%2520large%2520and%250Adiverse%2520datasets%2520for%2520training.%2520This%2520work%2520presents%2520a%2520new%2520dataset%2520and%2520benchmark%250Acalled%2520%2524%255Cnabla%255E2%2524DFT%2520that%2520is%2520based%2520on%2520the%2520nablaDFT.%2520It%2520contains%2520twice%2520as%2520much%250Amolecular%2520structures%252C%2520three%2520times%2520more%2520conformations%252C%2520new%2520data%2520types%2520and%2520tasks%252C%250Aand%2520state-of-the-art%2520models.%2520The%2520dataset%2520includes%2520energies%252C%2520forces%252C%252017%250Amolecular%2520properties%252C%2520Hamiltonian%2520and%2520overlap%2520matrices%252C%2520and%2520a%2520wavefunction%250Aobject.%2520All%2520calculations%2520were%2520performed%2520at%2520the%2520DFT%2520level%250A%2528%2524%255Comega%2524B97X-D/def2-SVP%2529%2520for%2520each%2520conformation.%2520Moreover%252C%2520%2524%255Cnabla%255E2%2524DFT%2520is%2520the%250Afirst%2520dataset%2520that%2520contains%2520relaxation%2520trajectories%2520for%2520a%2520substantial%2520number%2520of%250Adrug-like%2520molecules.%2520We%2520also%2520introduce%2520a%2520novel%2520benchmark%2520for%2520evaluating%2520NNPs%2520in%250Amolecular%2520property%2520prediction%252C%2520Hamiltonian%2520prediction%252C%2520and%2520conformational%250Aoptimization%2520tasks.%2520Finally%252C%2520we%2520propose%2520an%2520extendable%2520framework%2520for%2520training%250ANNPs%2520and%2520implement%252010%2520models%2520within%2520it.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2406.14347v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=%24%5Cnabla%5E2%24DFT%3A%20A%20Universal%20Quantum%20Chemistry%20Dataset%20of%20Drug-Like%0A%20%20Molecules%20and%20a%20Benchmark%20for%20Neural%20Network%20Potentials&entry.906535625=Kuzma%20Khrabrov%20and%20Anton%20Ber%20and%20Artem%20Tsypin%20and%20Konstantin%20Ushenin%20and%20Egor%20Rumiantsev%20and%20Alexander%20Telepov%20and%20Dmitry%20Protasov%20and%20Ilya%20Shenbin%20and%20Anton%20Alekseev%20and%20Mikhail%20Shirokikh%20and%20Sergey%20Nikolenko%20and%20Elena%20Tutubalina%20and%20Artur%20Kadurin&entry.1292438233=%20%20Methods%20of%20computational%20quantum%20chemistry%20provide%20accurate%20approximations%20of%0Amolecular%20properties%20crucial%20for%20computer-aided%20drug%20discovery%20and%20other%20areas%0Aof%20chemical%20science.%20However%2C%20high%20computational%20complexity%20limits%20the%0Ascalability%20of%20their%20applications.%20Neural%20network%20potentials%20%28NNPs%29%20are%20a%0Apromising%20alternative%20to%20quantum%20chemistry%20methods%2C%20but%20they%20require%20large%20and%0Adiverse%20datasets%20for%20training.%20This%20work%20presents%20a%20new%20dataset%20and%20benchmark%0Acalled%20%24%5Cnabla%5E2%24DFT%20that%20is%20based%20on%20the%20nablaDFT.%20It%20contains%20twice%20as%20much%0Amolecular%20structures%2C%20three%20times%20more%20conformations%2C%20new%20data%20types%20and%20tasks%2C%0Aand%20state-of-the-art%20models.%20The%20dataset%20includes%20energies%2C%20forces%2C%2017%0Amolecular%20properties%2C%20Hamiltonian%20and%20overlap%20matrices%2C%20and%20a%20wavefunction%0Aobject.%20All%20calculations%20were%20performed%20at%20the%20DFT%20level%0A%28%24%5Comega%24B97X-D/def2-SVP%29%20for%20each%20conformation.%20Moreover%2C%20%24%5Cnabla%5E2%24DFT%20is%20the%0Afirst%20dataset%20that%20contains%20relaxation%20trajectories%20for%20a%20substantial%20number%20of%0Adrug-like%20molecules.%20We%20also%20introduce%20a%20novel%20benchmark%20for%20evaluating%20NNPs%20in%0Amolecular%20property%20prediction%2C%20Hamiltonian%20prediction%2C%20and%20conformational%0Aoptimization%20tasks.%20Finally%2C%20we%20propose%20an%20extendable%20framework%20for%20training%0ANNPs%20and%20implement%2010%20models%20within%20it.%0A&entry.1838667208=http%3A//arxiv.org/abs/2406.14347v1&entry.124074799=Read"},
{"title": "Centimeter Positioning Accuracy using AI/ML for 6G Applications", "author": "Sai Prasanth Kotturi and Radha Krishna Ganti", "abstract": "  This research looks at using AI/ML to achieve centimeter-level user\npositioning in 6G applications such as the Industrial Internet of Things\n(IIoT). Initial results show that our AI/ML-based method can estimate user\npositions with an accuracy of 17 cm in an indoor factory environment. In this\nproposal, we highlight our approaches and future directions.\n", "link": "http://arxiv.org/abs/2406.14458v1", "date": "2024-06-20", "relevancy": 1.3397, "topK": [{"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.4702}, {"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.4404}, {"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.4396}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Centimeter%20Positioning%20Accuracy%20using%20AI/ML%20for%206G%20Applications&body=Title%3A%20Centimeter%20Positioning%20Accuracy%20using%20AI/ML%20for%206G%20Applications%0AAuthor%3A%20Sai%20Prasanth%20Kotturi%20and%20Radha%20Krishna%20Ganti%0AAbstract%3A%20%20%20This%20research%20looks%20at%20using%20AI/ML%20to%20achieve%20centimeter-level%20user%0Apositioning%20in%206G%20applications%20such%20as%20the%20Industrial%20Internet%20of%20Things%0A%28IIoT%29.%20Initial%20results%20show%20that%20our%20AI/ML-based%20method%20can%20estimate%20user%0Apositions%20with%20an%20accuracy%20of%2017%20cm%20in%20an%20indoor%20factory%20environment.%20In%20this%0Aproposal%2C%20we%20highlight%20our%20approaches%20and%20future%20directions.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2406.14458v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DCentimeter%2520Positioning%2520Accuracy%2520using%2520AI/ML%2520for%25206G%2520Applications%26entry.906535625%3DSai%2520Prasanth%2520Kotturi%2520and%2520Radha%2520Krishna%2520Ganti%26entry.1292438233%3D%2520%2520This%2520research%2520looks%2520at%2520using%2520AI/ML%2520to%2520achieve%2520centimeter-level%2520user%250Apositioning%2520in%25206G%2520applications%2520such%2520as%2520the%2520Industrial%2520Internet%2520of%2520Things%250A%2528IIoT%2529.%2520Initial%2520results%2520show%2520that%2520our%2520AI/ML-based%2520method%2520can%2520estimate%2520user%250Apositions%2520with%2520an%2520accuracy%2520of%252017%2520cm%2520in%2520an%2520indoor%2520factory%2520environment.%2520In%2520this%250Aproposal%252C%2520we%2520highlight%2520our%2520approaches%2520and%2520future%2520directions.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2406.14458v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Centimeter%20Positioning%20Accuracy%20using%20AI/ML%20for%206G%20Applications&entry.906535625=Sai%20Prasanth%20Kotturi%20and%20Radha%20Krishna%20Ganti&entry.1292438233=%20%20This%20research%20looks%20at%20using%20AI/ML%20to%20achieve%20centimeter-level%20user%0Apositioning%20in%206G%20applications%20such%20as%20the%20Industrial%20Internet%20of%20Things%0A%28IIoT%29.%20Initial%20results%20show%20that%20our%20AI/ML-based%20method%20can%20estimate%20user%0Apositions%20with%20an%20accuracy%20of%2017%20cm%20in%20an%20indoor%20factory%20environment.%20In%20this%0Aproposal%2C%20we%20highlight%20our%20approaches%20and%20future%20directions.%0A&entry.1838667208=http%3A//arxiv.org/abs/2406.14458v1&entry.124074799=Read"},
{"title": "WWW: What, When, Where to Compute-in-Memory", "author": "Tanvi Sharma and Mustafa Ali and Indranil Chakraborty and Kaushik Roy", "abstract": "  Compute-in-memory (CiM) has emerged as a highly energy efficient solution for\nperforming matrix multiplication during Machine Learning (ML) inference.\nHowever, integrating compute in memory poses key questions, such as 1) What\ntype of CiM to use: Given a multitude of CiM design characteristics,\ndetermining their suitability from architecture perspective is needed. 2) When\nto use CiM: ML inference includes workloads with a variety of memory and\ncompute requirements, making it difficult to identify when CiM is more\nbeneficial. 3) Where to integrate CiM: Each memory level has different\nbandwidth and capacity, creating different data reuse opportunities for CiM\nintegration.\n  To answer such questions regarding on-chip CiM integration for accelerating\nML workloads, we use an analytical architecture evaluation methodology where we\ntailor the dataflow mapping. The mapping algorithm aims to achieve highest\nweight reuse and reduced data movements for a given CiM prototype and workload.\nOur experiments show that CiM integrated memory improves energy efficiency by\nup to 3.4x and throughput by up to 15.6x compared to tensor-core-like baseline\narchitecture, with INT-8 precision under iso-area constraints. We believe the\nproposed work provides insights into what type of CiM to use, and when and\nwhere to optimally integrate it in the cache hierarchy for efficient matrix\nmultiplication.\n", "link": "http://arxiv.org/abs/2312.15896v2", "date": "2024-06-20", "relevancy": 1.3138, "topK": [{"title": "SDXS: Real-Time One-Step Latent Diffusion Models with Image Conditions", "link": "http://arxiv.org/abs/2403.16627v1", "similarity": 0.4495}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.4354}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.4343}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20WWW%3A%20What%2C%20When%2C%20Where%20to%20Compute-in-Memory&body=Title%3A%20WWW%3A%20What%2C%20When%2C%20Where%20to%20Compute-in-Memory%0AAuthor%3A%20Tanvi%20Sharma%20and%20Mustafa%20Ali%20and%20Indranil%20Chakraborty%20and%20Kaushik%20Roy%0AAbstract%3A%20%20%20Compute-in-memory%20%28CiM%29%20has%20emerged%20as%20a%20highly%20energy%20efficient%20solution%20for%0Aperforming%20matrix%20multiplication%20during%20Machine%20Learning%20%28ML%29%20inference.%0AHowever%2C%20integrating%20compute%20in%20memory%20poses%20key%20questions%2C%20such%20as%201%29%20What%0Atype%20of%20CiM%20to%20use%3A%20Given%20a%20multitude%20of%20CiM%20design%20characteristics%2C%0Adetermining%20their%20suitability%20from%20architecture%20perspective%20is%20needed.%202%29%20When%0Ato%20use%20CiM%3A%20ML%20inference%20includes%20workloads%20with%20a%20variety%20of%20memory%20and%0Acompute%20requirements%2C%20making%20it%20difficult%20to%20identify%20when%20CiM%20is%20more%0Abeneficial.%203%29%20Where%20to%20integrate%20CiM%3A%20Each%20memory%20level%20has%20different%0Abandwidth%20and%20capacity%2C%20creating%20different%20data%20reuse%20opportunities%20for%20CiM%0Aintegration.%0A%20%20To%20answer%20such%20questions%20regarding%20on-chip%20CiM%20integration%20for%20accelerating%0AML%20workloads%2C%20we%20use%20an%20analytical%20architecture%20evaluation%20methodology%20where%20we%0Atailor%20the%20dataflow%20mapping.%20The%20mapping%20algorithm%20aims%20to%20achieve%20highest%0Aweight%20reuse%20and%20reduced%20data%20movements%20for%20a%20given%20CiM%20prototype%20and%20workload.%0AOur%20experiments%20show%20that%20CiM%20integrated%20memory%20improves%20energy%20efficiency%20by%0Aup%20to%203.4x%20and%20throughput%20by%20up%20to%2015.6x%20compared%20to%20tensor-core-like%20baseline%0Aarchitecture%2C%20with%20INT-8%20precision%20under%20iso-area%20constraints.%20We%20believe%20the%0Aproposed%20work%20provides%20insights%20into%20what%20type%20of%20CiM%20to%20use%2C%20and%20when%20and%0Awhere%20to%20optimally%20integrate%20it%20in%20the%20cache%20hierarchy%20for%20efficient%20matrix%0Amultiplication.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2312.15896v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DWWW%253A%2520What%252C%2520When%252C%2520Where%2520to%2520Compute-in-Memory%26entry.906535625%3DTanvi%2520Sharma%2520and%2520Mustafa%2520Ali%2520and%2520Indranil%2520Chakraborty%2520and%2520Kaushik%2520Roy%26entry.1292438233%3D%2520%2520Compute-in-memory%2520%2528CiM%2529%2520has%2520emerged%2520as%2520a%2520highly%2520energy%2520efficient%2520solution%2520for%250Aperforming%2520matrix%2520multiplication%2520during%2520Machine%2520Learning%2520%2528ML%2529%2520inference.%250AHowever%252C%2520integrating%2520compute%2520in%2520memory%2520poses%2520key%2520questions%252C%2520such%2520as%25201%2529%2520What%250Atype%2520of%2520CiM%2520to%2520use%253A%2520Given%2520a%2520multitude%2520of%2520CiM%2520design%2520characteristics%252C%250Adetermining%2520their%2520suitability%2520from%2520architecture%2520perspective%2520is%2520needed.%25202%2529%2520When%250Ato%2520use%2520CiM%253A%2520ML%2520inference%2520includes%2520workloads%2520with%2520a%2520variety%2520of%2520memory%2520and%250Acompute%2520requirements%252C%2520making%2520it%2520difficult%2520to%2520identify%2520when%2520CiM%2520is%2520more%250Abeneficial.%25203%2529%2520Where%2520to%2520integrate%2520CiM%253A%2520Each%2520memory%2520level%2520has%2520different%250Abandwidth%2520and%2520capacity%252C%2520creating%2520different%2520data%2520reuse%2520opportunities%2520for%2520CiM%250Aintegration.%250A%2520%2520To%2520answer%2520such%2520questions%2520regarding%2520on-chip%2520CiM%2520integration%2520for%2520accelerating%250AML%2520workloads%252C%2520we%2520use%2520an%2520analytical%2520architecture%2520evaluation%2520methodology%2520where%2520we%250Atailor%2520the%2520dataflow%2520mapping.%2520The%2520mapping%2520algorithm%2520aims%2520to%2520achieve%2520highest%250Aweight%2520reuse%2520and%2520reduced%2520data%2520movements%2520for%2520a%2520given%2520CiM%2520prototype%2520and%2520workload.%250AOur%2520experiments%2520show%2520that%2520CiM%2520integrated%2520memory%2520improves%2520energy%2520efficiency%2520by%250Aup%2520to%25203.4x%2520and%2520throughput%2520by%2520up%2520to%252015.6x%2520compared%2520to%2520tensor-core-like%2520baseline%250Aarchitecture%252C%2520with%2520INT-8%2520precision%2520under%2520iso-area%2520constraints.%2520We%2520believe%2520the%250Aproposed%2520work%2520provides%2520insights%2520into%2520what%2520type%2520of%2520CiM%2520to%2520use%252C%2520and%2520when%2520and%250Awhere%2520to%2520optimally%2520integrate%2520it%2520in%2520the%2520cache%2520hierarchy%2520for%2520efficient%2520matrix%250Amultiplication.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2312.15896v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=WWW%3A%20What%2C%20When%2C%20Where%20to%20Compute-in-Memory&entry.906535625=Tanvi%20Sharma%20and%20Mustafa%20Ali%20and%20Indranil%20Chakraborty%20and%20Kaushik%20Roy&entry.1292438233=%20%20Compute-in-memory%20%28CiM%29%20has%20emerged%20as%20a%20highly%20energy%20efficient%20solution%20for%0Aperforming%20matrix%20multiplication%20during%20Machine%20Learning%20%28ML%29%20inference.%0AHowever%2C%20integrating%20compute%20in%20memory%20poses%20key%20questions%2C%20such%20as%201%29%20What%0Atype%20of%20CiM%20to%20use%3A%20Given%20a%20multitude%20of%20CiM%20design%20characteristics%2C%0Adetermining%20their%20suitability%20from%20architecture%20perspective%20is%20needed.%202%29%20When%0Ato%20use%20CiM%3A%20ML%20inference%20includes%20workloads%20with%20a%20variety%20of%20memory%20and%0Acompute%20requirements%2C%20making%20it%20difficult%20to%20identify%20when%20CiM%20is%20more%0Abeneficial.%203%29%20Where%20to%20integrate%20CiM%3A%20Each%20memory%20level%20has%20different%0Abandwidth%20and%20capacity%2C%20creating%20different%20data%20reuse%20opportunities%20for%20CiM%0Aintegration.%0A%20%20To%20answer%20such%20questions%20regarding%20on-chip%20CiM%20integration%20for%20accelerating%0AML%20workloads%2C%20we%20use%20an%20analytical%20architecture%20evaluation%20methodology%20where%20we%0Atailor%20the%20dataflow%20mapping.%20The%20mapping%20algorithm%20aims%20to%20achieve%20highest%0Aweight%20reuse%20and%20reduced%20data%20movements%20for%20a%20given%20CiM%20prototype%20and%20workload.%0AOur%20experiments%20show%20that%20CiM%20integrated%20memory%20improves%20energy%20efficiency%20by%0Aup%20to%203.4x%20and%20throughput%20by%20up%20to%2015.6x%20compared%20to%20tensor-core-like%20baseline%0Aarchitecture%2C%20with%20INT-8%20precision%20under%20iso-area%20constraints.%20We%20believe%20the%0Aproposed%20work%20provides%20insights%20into%20what%20type%20of%20CiM%20to%20use%2C%20and%20when%20and%0Awhere%20to%20optimally%20integrate%20it%20in%20the%20cache%20hierarchy%20for%20efficient%20matrix%0Amultiplication.%0A&entry.1838667208=http%3A//arxiv.org/abs/2312.15896v2&entry.124074799=Read"},
{"title": "APEER: Automatic Prompt Engineering Enhances Large Language Model\n  Reranking", "author": "Can Jin and Hongwu Peng and Shiyu Zhao and Zhenting Wang and Wujiang Xu and Ligong Han and Jiahui Zhao and Kai Zhong and Sanguthevar Rajasekaran and Dimitris N. Metaxas", "abstract": "  Large Language Models (LLMs) have significantly enhanced Information\nRetrieval (IR) across various modules, such as reranking. Despite impressive\nperformance, current zero-shot relevance ranking with LLMs heavily relies on\nhuman prompt engineering. Existing automatic prompt engineering algorithms\nprimarily focus on language modeling and classification tasks, leaving the\ndomain of IR, particularly reranking, underexplored. Directly applying current\nprompt engineering algorithms to relevance ranking is challenging due to the\nintegration of query and long passage pairs in the input, where the ranking\ncomplexity surpasses classification tasks. To reduce human effort and unlock\nthe potential of prompt optimization in reranking, we introduce a novel\nautomatic prompt engineering algorithm named APEER. APEER iteratively generates\nrefined prompts through feedback and preference optimization. Extensive\nexperiments with four LLMs and ten datasets demonstrate the substantial\nperformance improvement of APEER over existing state-of-the-art (SoTA) manual\nprompts. Furthermore, we find that the prompts generated by APEER exhibit\nbetter transferability across diverse tasks and LLMs. Code is available at\nhttps://github.com/jincan333/APEER.\n", "link": "http://arxiv.org/abs/2406.14449v1", "date": "2024-06-20", "relevancy": 1.3991, "topK": [{"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.4768}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.4641}, {"title": "RoboEXP: Action-Conditioned Scene Graph via Interactive Exploration for\n  Robotic Manipulation", "link": "http://arxiv.org/abs/2402.15487v1", "similarity": 0.4617}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20APEER%3A%20Automatic%20Prompt%20Engineering%20Enhances%20Large%20Language%20Model%0A%20%20Reranking&body=Title%3A%20APEER%3A%20Automatic%20Prompt%20Engineering%20Enhances%20Large%20Language%20Model%0A%20%20Reranking%0AAuthor%3A%20Can%20Jin%20and%20Hongwu%20Peng%20and%20Shiyu%20Zhao%20and%20Zhenting%20Wang%20and%20Wujiang%20Xu%20and%20Ligong%20Han%20and%20Jiahui%20Zhao%20and%20Kai%20Zhong%20and%20Sanguthevar%20Rajasekaran%20and%20Dimitris%20N.%20Metaxas%0AAbstract%3A%20%20%20Large%20Language%20Models%20%28LLMs%29%20have%20significantly%20enhanced%20Information%0ARetrieval%20%28IR%29%20across%20various%20modules%2C%20such%20as%20reranking.%20Despite%20impressive%0Aperformance%2C%20current%20zero-shot%20relevance%20ranking%20with%20LLMs%20heavily%20relies%20on%0Ahuman%20prompt%20engineering.%20Existing%20automatic%20prompt%20engineering%20algorithms%0Aprimarily%20focus%20on%20language%20modeling%20and%20classification%20tasks%2C%20leaving%20the%0Adomain%20of%20IR%2C%20particularly%20reranking%2C%20underexplored.%20Directly%20applying%20current%0Aprompt%20engineering%20algorithms%20to%20relevance%20ranking%20is%20challenging%20due%20to%20the%0Aintegration%20of%20query%20and%20long%20passage%20pairs%20in%20the%20input%2C%20where%20the%20ranking%0Acomplexity%20surpasses%20classification%20tasks.%20To%20reduce%20human%20effort%20and%20unlock%0Athe%20potential%20of%20prompt%20optimization%20in%20reranking%2C%20we%20introduce%20a%20novel%0Aautomatic%20prompt%20engineering%20algorithm%20named%20APEER.%20APEER%20iteratively%20generates%0Arefined%20prompts%20through%20feedback%20and%20preference%20optimization.%20Extensive%0Aexperiments%20with%20four%20LLMs%20and%20ten%20datasets%20demonstrate%20the%20substantial%0Aperformance%20improvement%20of%20APEER%20over%20existing%20state-of-the-art%20%28SoTA%29%20manual%0Aprompts.%20Furthermore%2C%20we%20find%20that%20the%20prompts%20generated%20by%20APEER%20exhibit%0Abetter%20transferability%20across%20diverse%20tasks%20and%20LLMs.%20Code%20is%20available%20at%0Ahttps%3A//github.com/jincan333/APEER.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2406.14449v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DAPEER%253A%2520Automatic%2520Prompt%2520Engineering%2520Enhances%2520Large%2520Language%2520Model%250A%2520%2520Reranking%26entry.906535625%3DCan%2520Jin%2520and%2520Hongwu%2520Peng%2520and%2520Shiyu%2520Zhao%2520and%2520Zhenting%2520Wang%2520and%2520Wujiang%2520Xu%2520and%2520Ligong%2520Han%2520and%2520Jiahui%2520Zhao%2520and%2520Kai%2520Zhong%2520and%2520Sanguthevar%2520Rajasekaran%2520and%2520Dimitris%2520N.%2520Metaxas%26entry.1292438233%3D%2520%2520Large%2520Language%2520Models%2520%2528LLMs%2529%2520have%2520significantly%2520enhanced%2520Information%250ARetrieval%2520%2528IR%2529%2520across%2520various%2520modules%252C%2520such%2520as%2520reranking.%2520Despite%2520impressive%250Aperformance%252C%2520current%2520zero-shot%2520relevance%2520ranking%2520with%2520LLMs%2520heavily%2520relies%2520on%250Ahuman%2520prompt%2520engineering.%2520Existing%2520automatic%2520prompt%2520engineering%2520algorithms%250Aprimarily%2520focus%2520on%2520language%2520modeling%2520and%2520classification%2520tasks%252C%2520leaving%2520the%250Adomain%2520of%2520IR%252C%2520particularly%2520reranking%252C%2520underexplored.%2520Directly%2520applying%2520current%250Aprompt%2520engineering%2520algorithms%2520to%2520relevance%2520ranking%2520is%2520challenging%2520due%2520to%2520the%250Aintegration%2520of%2520query%2520and%2520long%2520passage%2520pairs%2520in%2520the%2520input%252C%2520where%2520the%2520ranking%250Acomplexity%2520surpasses%2520classification%2520tasks.%2520To%2520reduce%2520human%2520effort%2520and%2520unlock%250Athe%2520potential%2520of%2520prompt%2520optimization%2520in%2520reranking%252C%2520we%2520introduce%2520a%2520novel%250Aautomatic%2520prompt%2520engineering%2520algorithm%2520named%2520APEER.%2520APEER%2520iteratively%2520generates%250Arefined%2520prompts%2520through%2520feedback%2520and%2520preference%2520optimization.%2520Extensive%250Aexperiments%2520with%2520four%2520LLMs%2520and%2520ten%2520datasets%2520demonstrate%2520the%2520substantial%250Aperformance%2520improvement%2520of%2520APEER%2520over%2520existing%2520state-of-the-art%2520%2528SoTA%2529%2520manual%250Aprompts.%2520Furthermore%252C%2520we%2520find%2520that%2520the%2520prompts%2520generated%2520by%2520APEER%2520exhibit%250Abetter%2520transferability%2520across%2520diverse%2520tasks%2520and%2520LLMs.%2520Code%2520is%2520available%2520at%250Ahttps%253A//github.com/jincan333/APEER.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2406.14449v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=APEER%3A%20Automatic%20Prompt%20Engineering%20Enhances%20Large%20Language%20Model%0A%20%20Reranking&entry.906535625=Can%20Jin%20and%20Hongwu%20Peng%20and%20Shiyu%20Zhao%20and%20Zhenting%20Wang%20and%20Wujiang%20Xu%20and%20Ligong%20Han%20and%20Jiahui%20Zhao%20and%20Kai%20Zhong%20and%20Sanguthevar%20Rajasekaran%20and%20Dimitris%20N.%20Metaxas&entry.1292438233=%20%20Large%20Language%20Models%20%28LLMs%29%20have%20significantly%20enhanced%20Information%0ARetrieval%20%28IR%29%20across%20various%20modules%2C%20such%20as%20reranking.%20Despite%20impressive%0Aperformance%2C%20current%20zero-shot%20relevance%20ranking%20with%20LLMs%20heavily%20relies%20on%0Ahuman%20prompt%20engineering.%20Existing%20automatic%20prompt%20engineering%20algorithms%0Aprimarily%20focus%20on%20language%20modeling%20and%20classification%20tasks%2C%20leaving%20the%0Adomain%20of%20IR%2C%20particularly%20reranking%2C%20underexplored.%20Directly%20applying%20current%0Aprompt%20engineering%20algorithms%20to%20relevance%20ranking%20is%20challenging%20due%20to%20the%0Aintegration%20of%20query%20and%20long%20passage%20pairs%20in%20the%20input%2C%20where%20the%20ranking%0Acomplexity%20surpasses%20classification%20tasks.%20To%20reduce%20human%20effort%20and%20unlock%0Athe%20potential%20of%20prompt%20optimization%20in%20reranking%2C%20we%20introduce%20a%20novel%0Aautomatic%20prompt%20engineering%20algorithm%20named%20APEER.%20APEER%20iteratively%20generates%0Arefined%20prompts%20through%20feedback%20and%20preference%20optimization.%20Extensive%0Aexperiments%20with%20four%20LLMs%20and%20ten%20datasets%20demonstrate%20the%20substantial%0Aperformance%20improvement%20of%20APEER%20over%20existing%20state-of-the-art%20%28SoTA%29%20manual%0Aprompts.%20Furthermore%2C%20we%20find%20that%20the%20prompts%20generated%20by%20APEER%20exhibit%0Abetter%20transferability%20across%20diverse%20tasks%20and%20LLMs.%20Code%20is%20available%20at%0Ahttps%3A//github.com/jincan333/APEER.%0A&entry.1838667208=http%3A//arxiv.org/abs/2406.14449v1&entry.124074799=Read"},
{"title": "MEAT: Median-Ensemble Adversarial Training for Improving Robustness and\n  Generalization", "author": "Zhaozhe Hu and Jia-Li Yin and Bin Chen and Luojun Lin and Bo-Hao Chen and Ximeng Liu", "abstract": "  Self-ensemble adversarial training methods improve model robustness by\nensembling models at different training epochs, such as model weight averaging\n(WA). However, previous research has shown that self-ensemble defense methods\nin adversarial training (AT) still suffer from robust overfitting, which\nseverely affects the generalization performance. Empirically, in the late\nphases of training, the AT becomes more overfitting to the extent that the\nindividuals for weight averaging also suffer from overfitting and produce\nanomalous weight values, which causes the self-ensemble model to continue to\nundergo robust overfitting due to the failure in removing the weight anomalies.\nTo solve this problem, we aim to tackle the influence of outliers in the weight\nspace in this work and propose an easy-to-operate and effective Median-Ensemble\nAdversarial Training (MEAT) method to solve the robust overfitting phenomenon\nexisting in self-ensemble defense from the source by searching for the median\nof the historical model weights. Experimental results show that MEAT achieves\nthe best robustness against the powerful AutoAttack and can effectively\nallievate the robust overfitting. We further demonstrate that most defense\nmethods can improve robust generalization and robustness by combining with\nMEAT.\n", "link": "http://arxiv.org/abs/2406.14259v1", "date": "2024-06-20", "relevancy": 1.8748, "topK": [{"title": "Breadcrumbs: Adversarial class-balanced sampling for long-tailed recognition", "link": "https://link.springer.com/chapter/10.1007/978-3-031-20053-3_37", "similarity": 0.5017}, {"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.4689}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.4356}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20MEAT%3A%20Median-Ensemble%20Adversarial%20Training%20for%20Improving%20Robustness%20and%0A%20%20Generalization&body=Title%3A%20MEAT%3A%20Median-Ensemble%20Adversarial%20Training%20for%20Improving%20Robustness%20and%0A%20%20Generalization%0AAuthor%3A%20Zhaozhe%20Hu%20and%20Jia-Li%20Yin%20and%20Bin%20Chen%20and%20Luojun%20Lin%20and%20Bo-Hao%20Chen%20and%20Ximeng%20Liu%0AAbstract%3A%20%20%20Self-ensemble%20adversarial%20training%20methods%20improve%20model%20robustness%20by%0Aensembling%20models%20at%20different%20training%20epochs%2C%20such%20as%20model%20weight%20averaging%0A%28WA%29.%20However%2C%20previous%20research%20has%20shown%20that%20self-ensemble%20defense%20methods%0Ain%20adversarial%20training%20%28AT%29%20still%20suffer%20from%20robust%20overfitting%2C%20which%0Aseverely%20affects%20the%20generalization%20performance.%20Empirically%2C%20in%20the%20late%0Aphases%20of%20training%2C%20the%20AT%20becomes%20more%20overfitting%20to%20the%20extent%20that%20the%0Aindividuals%20for%20weight%20averaging%20also%20suffer%20from%20overfitting%20and%20produce%0Aanomalous%20weight%20values%2C%20which%20causes%20the%20self-ensemble%20model%20to%20continue%20to%0Aundergo%20robust%20overfitting%20due%20to%20the%20failure%20in%20removing%20the%20weight%20anomalies.%0ATo%20solve%20this%20problem%2C%20we%20aim%20to%20tackle%20the%20influence%20of%20outliers%20in%20the%20weight%0Aspace%20in%20this%20work%20and%20propose%20an%20easy-to-operate%20and%20effective%20Median-Ensemble%0AAdversarial%20Training%20%28MEAT%29%20method%20to%20solve%20the%20robust%20overfitting%20phenomenon%0Aexisting%20in%20self-ensemble%20defense%20from%20the%20source%20by%20searching%20for%20the%20median%0Aof%20the%20historical%20model%20weights.%20Experimental%20results%20show%20that%20MEAT%20achieves%0Athe%20best%20robustness%20against%20the%20powerful%20AutoAttack%20and%20can%20effectively%0Aallievate%20the%20robust%20overfitting.%20We%20further%20demonstrate%20that%20most%20defense%0Amethods%20can%20improve%20robust%20generalization%20and%20robustness%20by%20combining%20with%0AMEAT.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2406.14259v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DMEAT%253A%2520Median-Ensemble%2520Adversarial%2520Training%2520for%2520Improving%2520Robustness%2520and%250A%2520%2520Generalization%26entry.906535625%3DZhaozhe%2520Hu%2520and%2520Jia-Li%2520Yin%2520and%2520Bin%2520Chen%2520and%2520Luojun%2520Lin%2520and%2520Bo-Hao%2520Chen%2520and%2520Ximeng%2520Liu%26entry.1292438233%3D%2520%2520Self-ensemble%2520adversarial%2520training%2520methods%2520improve%2520model%2520robustness%2520by%250Aensembling%2520models%2520at%2520different%2520training%2520epochs%252C%2520such%2520as%2520model%2520weight%2520averaging%250A%2528WA%2529.%2520However%252C%2520previous%2520research%2520has%2520shown%2520that%2520self-ensemble%2520defense%2520methods%250Ain%2520adversarial%2520training%2520%2528AT%2529%2520still%2520suffer%2520from%2520robust%2520overfitting%252C%2520which%250Aseverely%2520affects%2520the%2520generalization%2520performance.%2520Empirically%252C%2520in%2520the%2520late%250Aphases%2520of%2520training%252C%2520the%2520AT%2520becomes%2520more%2520overfitting%2520to%2520the%2520extent%2520that%2520the%250Aindividuals%2520for%2520weight%2520averaging%2520also%2520suffer%2520from%2520overfitting%2520and%2520produce%250Aanomalous%2520weight%2520values%252C%2520which%2520causes%2520the%2520self-ensemble%2520model%2520to%2520continue%2520to%250Aundergo%2520robust%2520overfitting%2520due%2520to%2520the%2520failure%2520in%2520removing%2520the%2520weight%2520anomalies.%250ATo%2520solve%2520this%2520problem%252C%2520we%2520aim%2520to%2520tackle%2520the%2520influence%2520of%2520outliers%2520in%2520the%2520weight%250Aspace%2520in%2520this%2520work%2520and%2520propose%2520an%2520easy-to-operate%2520and%2520effective%2520Median-Ensemble%250AAdversarial%2520Training%2520%2528MEAT%2529%2520method%2520to%2520solve%2520the%2520robust%2520overfitting%2520phenomenon%250Aexisting%2520in%2520self-ensemble%2520defense%2520from%2520the%2520source%2520by%2520searching%2520for%2520the%2520median%250Aof%2520the%2520historical%2520model%2520weights.%2520Experimental%2520results%2520show%2520that%2520MEAT%2520achieves%250Athe%2520best%2520robustness%2520against%2520the%2520powerful%2520AutoAttack%2520and%2520can%2520effectively%250Aallievate%2520the%2520robust%2520overfitting.%2520We%2520further%2520demonstrate%2520that%2520most%2520defense%250Amethods%2520can%2520improve%2520robust%2520generalization%2520and%2520robustness%2520by%2520combining%2520with%250AMEAT.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2406.14259v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=MEAT%3A%20Median-Ensemble%20Adversarial%20Training%20for%20Improving%20Robustness%20and%0A%20%20Generalization&entry.906535625=Zhaozhe%20Hu%20and%20Jia-Li%20Yin%20and%20Bin%20Chen%20and%20Luojun%20Lin%20and%20Bo-Hao%20Chen%20and%20Ximeng%20Liu&entry.1292438233=%20%20Self-ensemble%20adversarial%20training%20methods%20improve%20model%20robustness%20by%0Aensembling%20models%20at%20different%20training%20epochs%2C%20such%20as%20model%20weight%20averaging%0A%28WA%29.%20However%2C%20previous%20research%20has%20shown%20that%20self-ensemble%20defense%20methods%0Ain%20adversarial%20training%20%28AT%29%20still%20suffer%20from%20robust%20overfitting%2C%20which%0Aseverely%20affects%20the%20generalization%20performance.%20Empirically%2C%20in%20the%20late%0Aphases%20of%20training%2C%20the%20AT%20becomes%20more%20overfitting%20to%20the%20extent%20that%20the%0Aindividuals%20for%20weight%20averaging%20also%20suffer%20from%20overfitting%20and%20produce%0Aanomalous%20weight%20values%2C%20which%20causes%20the%20self-ensemble%20model%20to%20continue%20to%0Aundergo%20robust%20overfitting%20due%20to%20the%20failure%20in%20removing%20the%20weight%20anomalies.%0ATo%20solve%20this%20problem%2C%20we%20aim%20to%20tackle%20the%20influence%20of%20outliers%20in%20the%20weight%0Aspace%20in%20this%20work%20and%20propose%20an%20easy-to-operate%20and%20effective%20Median-Ensemble%0AAdversarial%20Training%20%28MEAT%29%20method%20to%20solve%20the%20robust%20overfitting%20phenomenon%0Aexisting%20in%20self-ensemble%20defense%20from%20the%20source%20by%20searching%20for%20the%20median%0Aof%20the%20historical%20model%20weights.%20Experimental%20results%20show%20that%20MEAT%20achieves%0Athe%20best%20robustness%20against%20the%20powerful%20AutoAttack%20and%20can%20effectively%0Aallievate%20the%20robust%20overfitting.%20We%20further%20demonstrate%20that%20most%20defense%0Amethods%20can%20improve%20robust%20generalization%20and%20robustness%20by%20combining%20with%0AMEAT.%0A&entry.1838667208=http%3A//arxiv.org/abs/2406.14259v1&entry.124074799=Read"},
      ];
      const content = document.getElementById('content');
      function createPostElement(post) {
        const postElement = document.createElement('div');
        postElement.className = 'post';
        const dateElem = document.createElement('p');
        dateElem.setAttribute("class", "date");
        dateElem.textContent = post.date;
        postElement.appendChild(dateElem);

        const textElem = document.createElement('p');
        textElem.setAttribute("class", "text");
        const titleElem = document.createElement('p');
        titleElem.setAttribute("class", "title");
        titleElem.textContent = post.title;
        textElem.appendChild(titleElem);
        const authorElem = document.createElement('p');
        authorElem.setAttribute("class", "author");
        authorElem.textContent = post.author;
        textElem.appendChild(authorElem);
        const abstractElem = document.createElement('p');
        abstractElem.setAttribute("class", "abstract");
        abstractElem.textContent = post.abstract;
        textElem.appendChild(abstractElem);

        const linkElement = document.createElement('a');
        linkElement.setAttribute("class", "link");
        linkElement.href = post.link;
        linkElement.target = "_blank";
        linkElement.textContent = post.link.length > 50 ? post.link.substring(0, 50) + '...' : post.link;
        textElem.appendChild(linkElement);
        postElement.appendChild(textElem);

        const linkElementContainer = document.createElement('div');
        linkElementContainer.setAttribute("class", "comment");
        const actionElement = document.createElement('a');
        actionElement.setAttribute("class", "comment");
        actionElement.href = post.form;
        actionElement.textContent = "Action";
        actionElement.target = "_blank";
        linkElementContainer.appendChild(actionElement);
        const emailElement = document.createElement('a');
        emailElement.setAttribute("class", "comment");
        emailElement.href = post.mailto;
        emailElement.textContent = "Email";
        emailElement.target = "_blank";
        linkElementContainer.appendChild(emailElement);
        postElement.appendChild(linkElementContainer);
        const e = document.createElement('div');
        e.setAttribute("class", "clear");
        postElement.appendChild(e);

        const relevancyContainer = document.createElement('div');
        const relevancyValElem = document.createElement('p');
        relevancyValElem.textContent = "Relevancy " + post.relevancy;
        relevancyContainer.appendChild(relevancyValElem);
        post.topK.forEach((sub) => {
          const topKElem = document.createElement('a');
          topKElem.setAttribute("class", "topK");
          topKElem.href = sub.link;
          topKElem.textContent = sub.title + " (" + sub.similarity + ")";
          topKElem.target = "_blank";
          relevancyContainer.appendChild(topKElem);
        });
        postElement.appendChild(relevancyContainer);
        return postElement;
      }
      function loadPosts() {
        // Simulate loading more posts
        posts.forEach((post) => {
          const postElement = createPostElement(post);
          content.appendChild(postElement);
        });
      }
      // Load initial posts
      loadPosts();
    </script>

  </body>
</html>


