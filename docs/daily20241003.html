<!DOCTYPE html>
<html lang="en">
  <head>
    <script async src="https://www.googletagmanager.com/gtag/js?id=G-V34CNNDP8V"></script>
    <script>
      window.dataLayer = window.dataLayer || [];
      function gtag(){dataLayer.push(arguments);}
      gtag('js', new Date());
      gtag('config', 'G-V34CNNDP8V');
    </script>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Arxiv Paper Selection</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.3/css/all.min.css" crossorigin="anonymous" referrerpolicy="no-referrer" />
    <style>
    body {
      font-family: Arial, sans-serif;
      margin: 0;
      padding: 0;
      background-color: #f4f4f4;
    }
    header {
      position: fixed;
      top: 0;
      left: 0;
      width: 100%;
      background-color: #ffffff;
      color: black;
      padding: 10px;
      text-align: center;
      z-index: 1000;
      border-bottom: 1px solid #ddd;
    }
    header div {
      display: block;
      margin: 10px auto;
    }

    #home-icon {
      display: block;
      float: left;
      margin: 5px;
      text-decoration: none;
      color: black;
    }

    main {
      margin-top: 60px; /* Adjusted margin to account for fixed header */
      padding: 20px;
    }

    .post {
      background-color: white;
      border: 1px solid #ddd;
      border-radius: 5px;
      margin-bottom: 10px;
      padding: 10px 20px;
      max-height: 2000px;
      overflow: scroll;
    }
    .post img {
      display: block;
      margin-top: 5px;
      max-width: auto;
      max-height: 100px;
    }
    .post .clear {
      clear: both;
      display: block;
    }
    .post a {
      text-decoration: none;
    }
    .post a:hover {
      color: #0056b3;
    }
    .post a:visited {
      color: #0056b3;
    }
    .post div.comment {
      text-align: right;
    }
    .post div.comment a {
      margin: 1em;
    }
    .post .text {
      margin: 1em 0em;
      padding: 0;
    }
    .post .text .title {
    }
    .post .text .author {
    }
    .post .text .abstract {
    }
    .post .topK {
      display: block;
      margin: 0.5em;
    }
    .post .date {
      margin: 0;
      padding: 0;
      text-size: small; 
      color: gray;
    }
    .post .link {
      margin: 0;
      padding: 0;
    }
    @media screen and (max-width: 600px) {
      body {
        max-width: 100%; 
      }
      #home-icon {
        float: none;
        display: block;
        text-align: center;
        margin-bottom: 10px;
      }
    }
    footer {
      width: 100%;
      background-color: #ddd;
      text-align: center;
      z-index: 1000;
      padding: 20px 0px;
      margin-bottom: 20px;
      left: 0;
    }

    #next-btn,
    #prev-btn {
      background-color: #4CAF50;
      color: white;
      padding: 8px 16px;
      margin: 0 50px;
      border: none;
      border-radius: 4px;
      cursor: pointer;
    }

    .links {
      padding: 20px;
    }
    .links a {
      text-decoration: none;
    }
    .links a:hover {
      color: #0056b3;
    }
    .links a:visited {
      color: #0056b3;
    }

    #page-index {
      font-size: small;
    }
    .ads {
      width: 100%;
    }
    #prev-btn {
      background-color: #4CAF50;
      color: white;
      padding: 8px 16px;
      margin: 0 50px;
      border: none;
      border-radius: 4px;
      cursor: pointer;
    }
    </style>
  </head>
  <body>

    <header>
      <a id="prev-btn" href="daily20240930.html"><i class="fas fa-chevron-left"></i></a>
      <a href="https://haoxiang.org/">About</a>
    </header>

    <main id="content">
      <!-- Posts will be dynamically added here using JavaScript -->
    </main>

    <script>
      // Dummy data for posts
      const posts = [
{"title": "CaRtGS: Computational Alignment for Real-Time Gaussian Splatting SLAM", "author": "Dapeng Feng and Zhiqiang Chen and Yizhen Yin and Shipeng Zhong and Yuhua Qi and Hongbo Chen", "abstract": "  Simultaneous Localization and Mapping (SLAM) is pivotal in robotics, with\nphotorealistic scene reconstruction emerging as a key challenge. To address\nthis, we introduce Computational Alignment for Real-Time Gaussian Splatting\nSLAM (CaRtGS), a novel method enhancing the efficiency and quality of\nphotorealistic scene reconstruction in real-time environments. Leveraging 3D\nGaussian Splatting (3DGS), CaRtGS achieves superior rendering quality and\nprocessing speed, which is crucial for scene photorealistic reconstruction. Our\napproach tackles computational misalignment in Gaussian Splatting SLAM\n(GS-SLAM) through an adaptive strategy that optimizes training, addresses\nlong-tail optimization, and refines densification. Experiments on Replica and\nTUM-RGBD datasets demonstrate CaRtGS's effectiveness in achieving high-fidelity\nrendering with fewer Gaussian primitives. This work propels SLAM towards\nreal-time, photorealistic dense rendering, significantly advancing\nphotorealistic scene representation. For the benefit of the research community,\nwe release the code on our project website:\nhttps://dapengfeng.github.io/cartgs.\n", "link": "http://arxiv.org/abs/2410.00486v2", "date": "2024-10-02", "relevancy": 3.5413, "topK": [{"title": "RTG-SLAM: Real-time 3D Reconstruction at Scale using Gaussian Splatting", "link": "http://arxiv.org/abs/2404.19706v1", "similarity": 0.8184}, {"title": "GST: Precise 3D Human Body from a Single Image with Gaussian Splatting\n  Transformers", "link": "http://arxiv.org/abs/2409.04196v1", "similarity": 0.6719}, {"title": "Dynamic Gaussians Mesh: Consistent Mesh Reconstruction from Monocular\n  Videos", "link": "http://arxiv.org/abs/2404.12379v2", "similarity": 0.6345}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20CaRtGS%3A%20Computational%20Alignment%20for%20Real-Time%20Gaussian%20Splatting%20SLAM&body=Title%3A%20CaRtGS%3A%20Computational%20Alignment%20for%20Real-Time%20Gaussian%20Splatting%20SLAM%0AAuthor%3A%20Dapeng%20Feng%20and%20Zhiqiang%20Chen%20and%20Yizhen%20Yin%20and%20Shipeng%20Zhong%20and%20Yuhua%20Qi%20and%20Hongbo%20Chen%0AAbstract%3A%20%20%20Simultaneous%20Localization%20and%20Mapping%20%28SLAM%29%20is%20pivotal%20in%20robotics%2C%20with%0Aphotorealistic%20scene%20reconstruction%20emerging%20as%20a%20key%20challenge.%20To%20address%0Athis%2C%20we%20introduce%20Computational%20Alignment%20for%20Real-Time%20Gaussian%20Splatting%0ASLAM%20%28CaRtGS%29%2C%20a%20novel%20method%20enhancing%20the%20efficiency%20and%20quality%20of%0Aphotorealistic%20scene%20reconstruction%20in%20real-time%20environments.%20Leveraging%203D%0AGaussian%20Splatting%20%283DGS%29%2C%20CaRtGS%20achieves%20superior%20rendering%20quality%20and%0Aprocessing%20speed%2C%20which%20is%20crucial%20for%20scene%20photorealistic%20reconstruction.%20Our%0Aapproach%20tackles%20computational%20misalignment%20in%20Gaussian%20Splatting%20SLAM%0A%28GS-SLAM%29%20through%20an%20adaptive%20strategy%20that%20optimizes%20training%2C%20addresses%0Along-tail%20optimization%2C%20and%20refines%20densification.%20Experiments%20on%20Replica%20and%0ATUM-RGBD%20datasets%20demonstrate%20CaRtGS%27s%20effectiveness%20in%20achieving%20high-fidelity%0Arendering%20with%20fewer%20Gaussian%20primitives.%20This%20work%20propels%20SLAM%20towards%0Areal-time%2C%20photorealistic%20dense%20rendering%2C%20significantly%20advancing%0Aphotorealistic%20scene%20representation.%20For%20the%20benefit%20of%20the%20research%20community%2C%0Awe%20release%20the%20code%20on%20our%20project%20website%3A%0Ahttps%3A//dapengfeng.github.io/cartgs.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2410.00486v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DCaRtGS%253A%2520Computational%2520Alignment%2520for%2520Real-Time%2520Gaussian%2520Splatting%2520SLAM%26entry.906535625%3DDapeng%2520Feng%2520and%2520Zhiqiang%2520Chen%2520and%2520Yizhen%2520Yin%2520and%2520Shipeng%2520Zhong%2520and%2520Yuhua%2520Qi%2520and%2520Hongbo%2520Chen%26entry.1292438233%3D%2520%2520Simultaneous%2520Localization%2520and%2520Mapping%2520%2528SLAM%2529%2520is%2520pivotal%2520in%2520robotics%252C%2520with%250Aphotorealistic%2520scene%2520reconstruction%2520emerging%2520as%2520a%2520key%2520challenge.%2520To%2520address%250Athis%252C%2520we%2520introduce%2520Computational%2520Alignment%2520for%2520Real-Time%2520Gaussian%2520Splatting%250ASLAM%2520%2528CaRtGS%2529%252C%2520a%2520novel%2520method%2520enhancing%2520the%2520efficiency%2520and%2520quality%2520of%250Aphotorealistic%2520scene%2520reconstruction%2520in%2520real-time%2520environments.%2520Leveraging%25203D%250AGaussian%2520Splatting%2520%25283DGS%2529%252C%2520CaRtGS%2520achieves%2520superior%2520rendering%2520quality%2520and%250Aprocessing%2520speed%252C%2520which%2520is%2520crucial%2520for%2520scene%2520photorealistic%2520reconstruction.%2520Our%250Aapproach%2520tackles%2520computational%2520misalignment%2520in%2520Gaussian%2520Splatting%2520SLAM%250A%2528GS-SLAM%2529%2520through%2520an%2520adaptive%2520strategy%2520that%2520optimizes%2520training%252C%2520addresses%250Along-tail%2520optimization%252C%2520and%2520refines%2520densification.%2520Experiments%2520on%2520Replica%2520and%250ATUM-RGBD%2520datasets%2520demonstrate%2520CaRtGS%2527s%2520effectiveness%2520in%2520achieving%2520high-fidelity%250Arendering%2520with%2520fewer%2520Gaussian%2520primitives.%2520This%2520work%2520propels%2520SLAM%2520towards%250Areal-time%252C%2520photorealistic%2520dense%2520rendering%252C%2520significantly%2520advancing%250Aphotorealistic%2520scene%2520representation.%2520For%2520the%2520benefit%2520of%2520the%2520research%2520community%252C%250Awe%2520release%2520the%2520code%2520on%2520our%2520project%2520website%253A%250Ahttps%253A//dapengfeng.github.io/cartgs.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2410.00486v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=CaRtGS%3A%20Computational%20Alignment%20for%20Real-Time%20Gaussian%20Splatting%20SLAM&entry.906535625=Dapeng%20Feng%20and%20Zhiqiang%20Chen%20and%20Yizhen%20Yin%20and%20Shipeng%20Zhong%20and%20Yuhua%20Qi%20and%20Hongbo%20Chen&entry.1292438233=%20%20Simultaneous%20Localization%20and%20Mapping%20%28SLAM%29%20is%20pivotal%20in%20robotics%2C%20with%0Aphotorealistic%20scene%20reconstruction%20emerging%20as%20a%20key%20challenge.%20To%20address%0Athis%2C%20we%20introduce%20Computational%20Alignment%20for%20Real-Time%20Gaussian%20Splatting%0ASLAM%20%28CaRtGS%29%2C%20a%20novel%20method%20enhancing%20the%20efficiency%20and%20quality%20of%0Aphotorealistic%20scene%20reconstruction%20in%20real-time%20environments.%20Leveraging%203D%0AGaussian%20Splatting%20%283DGS%29%2C%20CaRtGS%20achieves%20superior%20rendering%20quality%20and%0Aprocessing%20speed%2C%20which%20is%20crucial%20for%20scene%20photorealistic%20reconstruction.%20Our%0Aapproach%20tackles%20computational%20misalignment%20in%20Gaussian%20Splatting%20SLAM%0A%28GS-SLAM%29%20through%20an%20adaptive%20strategy%20that%20optimizes%20training%2C%20addresses%0Along-tail%20optimization%2C%20and%20refines%20densification.%20Experiments%20on%20Replica%20and%0ATUM-RGBD%20datasets%20demonstrate%20CaRtGS%27s%20effectiveness%20in%20achieving%20high-fidelity%0Arendering%20with%20fewer%20Gaussian%20primitives.%20This%20work%20propels%20SLAM%20towards%0Areal-time%2C%20photorealistic%20dense%20rendering%2C%20significantly%20advancing%0Aphotorealistic%20scene%20representation.%20For%20the%20benefit%20of%20the%20research%20community%2C%0Awe%20release%20the%20code%20on%20our%20project%20website%3A%0Ahttps%3A//dapengfeng.github.io/cartgs.%0A&entry.1838667208=http%3A//arxiv.org/abs/2410.00486v2&entry.124074799=Read"},
{"title": "GSLoc: Efficient Camera Pose Refinement via 3D Gaussian Splatting", "author": "Changkun Liu and Shuai Chen and Yash Bhalgat and Siyan Hu and Ming Cheng and Zirui Wang and Victor Adrian Prisacariu and Tristan Braud", "abstract": "  We leverage 3D Gaussian Splatting (3DGS) as a scene representation and\npropose a novel test-time camera pose refinement framework, GSLoc. This\nframework enhances the localization accuracy of state-of-the-art absolute pose\nregression and scene coordinate regression methods. The 3DGS model renders\nhigh-quality synthetic images and depth maps to facilitate the establishment of\n2D-3D correspondences. GSLoc obviates the need for training feature extractors\nor descriptors by operating directly on RGB images, utilizing the 3D foundation\nmodel, MASt3R, for precise 2D matching. To improve the robustness of our model\nin challenging outdoor environments, we incorporate an exposure-adaptive module\nwithin the 3DGS framework. Consequently, GSLoc enables efficient one-shot pose\nrefinement given a single RGB query and a coarse initial pose estimation. Our\nproposed approach surpasses leading NeRF-based optimization methods in both\naccuracy and runtime across indoor and outdoor visual localization benchmarks,\nachieving new state-of-the-art accuracy on two indoor datasets.\n", "link": "http://arxiv.org/abs/2408.11085v2", "date": "2024-10-02", "relevancy": 3.5068, "topK": [{"title": "RTG-SLAM: Real-time 3D Reconstruction at Scale using Gaussian Splatting", "link": "http://arxiv.org/abs/2404.19706v1", "similarity": 0.7651}, {"title": "GST: Precise 3D Human Body from a Single Image with Gaussian Splatting\n  Transformers", "link": "http://arxiv.org/abs/2409.04196v1", "similarity": 0.7137}, {"title": "Dynamic Gaussians Mesh: Consistent Mesh Reconstruction from Monocular\n  Videos", "link": "http://arxiv.org/abs/2404.12379v2", "similarity": 0.6253}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20GSLoc%3A%20Efficient%20Camera%20Pose%20Refinement%20via%203D%20Gaussian%20Splatting&body=Title%3A%20GSLoc%3A%20Efficient%20Camera%20Pose%20Refinement%20via%203D%20Gaussian%20Splatting%0AAuthor%3A%20Changkun%20Liu%20and%20Shuai%20Chen%20and%20Yash%20Bhalgat%20and%20Siyan%20Hu%20and%20Ming%20Cheng%20and%20Zirui%20Wang%20and%20Victor%20Adrian%20Prisacariu%20and%20Tristan%20Braud%0AAbstract%3A%20%20%20We%20leverage%203D%20Gaussian%20Splatting%20%283DGS%29%20as%20a%20scene%20representation%20and%0Apropose%20a%20novel%20test-time%20camera%20pose%20refinement%20framework%2C%20GSLoc.%20This%0Aframework%20enhances%20the%20localization%20accuracy%20of%20state-of-the-art%20absolute%20pose%0Aregression%20and%20scene%20coordinate%20regression%20methods.%20The%203DGS%20model%20renders%0Ahigh-quality%20synthetic%20images%20and%20depth%20maps%20to%20facilitate%20the%20establishment%20of%0A2D-3D%20correspondences.%20GSLoc%20obviates%20the%20need%20for%20training%20feature%20extractors%0Aor%20descriptors%20by%20operating%20directly%20on%20RGB%20images%2C%20utilizing%20the%203D%20foundation%0Amodel%2C%20MASt3R%2C%20for%20precise%202D%20matching.%20To%20improve%20the%20robustness%20of%20our%20model%0Ain%20challenging%20outdoor%20environments%2C%20we%20incorporate%20an%20exposure-adaptive%20module%0Awithin%20the%203DGS%20framework.%20Consequently%2C%20GSLoc%20enables%20efficient%20one-shot%20pose%0Arefinement%20given%20a%20single%20RGB%20query%20and%20a%20coarse%20initial%20pose%20estimation.%20Our%0Aproposed%20approach%20surpasses%20leading%20NeRF-based%20optimization%20methods%20in%20both%0Aaccuracy%20and%20runtime%20across%20indoor%20and%20outdoor%20visual%20localization%20benchmarks%2C%0Aachieving%20new%20state-of-the-art%20accuracy%20on%20two%20indoor%20datasets.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2408.11085v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DGSLoc%253A%2520Efficient%2520Camera%2520Pose%2520Refinement%2520via%25203D%2520Gaussian%2520Splatting%26entry.906535625%3DChangkun%2520Liu%2520and%2520Shuai%2520Chen%2520and%2520Yash%2520Bhalgat%2520and%2520Siyan%2520Hu%2520and%2520Ming%2520Cheng%2520and%2520Zirui%2520Wang%2520and%2520Victor%2520Adrian%2520Prisacariu%2520and%2520Tristan%2520Braud%26entry.1292438233%3D%2520%2520We%2520leverage%25203D%2520Gaussian%2520Splatting%2520%25283DGS%2529%2520as%2520a%2520scene%2520representation%2520and%250Apropose%2520a%2520novel%2520test-time%2520camera%2520pose%2520refinement%2520framework%252C%2520GSLoc.%2520This%250Aframework%2520enhances%2520the%2520localization%2520accuracy%2520of%2520state-of-the-art%2520absolute%2520pose%250Aregression%2520and%2520scene%2520coordinate%2520regression%2520methods.%2520The%25203DGS%2520model%2520renders%250Ahigh-quality%2520synthetic%2520images%2520and%2520depth%2520maps%2520to%2520facilitate%2520the%2520establishment%2520of%250A2D-3D%2520correspondences.%2520GSLoc%2520obviates%2520the%2520need%2520for%2520training%2520feature%2520extractors%250Aor%2520descriptors%2520by%2520operating%2520directly%2520on%2520RGB%2520images%252C%2520utilizing%2520the%25203D%2520foundation%250Amodel%252C%2520MASt3R%252C%2520for%2520precise%25202D%2520matching.%2520To%2520improve%2520the%2520robustness%2520of%2520our%2520model%250Ain%2520challenging%2520outdoor%2520environments%252C%2520we%2520incorporate%2520an%2520exposure-adaptive%2520module%250Awithin%2520the%25203DGS%2520framework.%2520Consequently%252C%2520GSLoc%2520enables%2520efficient%2520one-shot%2520pose%250Arefinement%2520given%2520a%2520single%2520RGB%2520query%2520and%2520a%2520coarse%2520initial%2520pose%2520estimation.%2520Our%250Aproposed%2520approach%2520surpasses%2520leading%2520NeRF-based%2520optimization%2520methods%2520in%2520both%250Aaccuracy%2520and%2520runtime%2520across%2520indoor%2520and%2520outdoor%2520visual%2520localization%2520benchmarks%252C%250Aachieving%2520new%2520state-of-the-art%2520accuracy%2520on%2520two%2520indoor%2520datasets.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2408.11085v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=GSLoc%3A%20Efficient%20Camera%20Pose%20Refinement%20via%203D%20Gaussian%20Splatting&entry.906535625=Changkun%20Liu%20and%20Shuai%20Chen%20and%20Yash%20Bhalgat%20and%20Siyan%20Hu%20and%20Ming%20Cheng%20and%20Zirui%20Wang%20and%20Victor%20Adrian%20Prisacariu%20and%20Tristan%20Braud&entry.1292438233=%20%20We%20leverage%203D%20Gaussian%20Splatting%20%283DGS%29%20as%20a%20scene%20representation%20and%0Apropose%20a%20novel%20test-time%20camera%20pose%20refinement%20framework%2C%20GSLoc.%20This%0Aframework%20enhances%20the%20localization%20accuracy%20of%20state-of-the-art%20absolute%20pose%0Aregression%20and%20scene%20coordinate%20regression%20methods.%20The%203DGS%20model%20renders%0Ahigh-quality%20synthetic%20images%20and%20depth%20maps%20to%20facilitate%20the%20establishment%20of%0A2D-3D%20correspondences.%20GSLoc%20obviates%20the%20need%20for%20training%20feature%20extractors%0Aor%20descriptors%20by%20operating%20directly%20on%20RGB%20images%2C%20utilizing%20the%203D%20foundation%0Amodel%2C%20MASt3R%2C%20for%20precise%202D%20matching.%20To%20improve%20the%20robustness%20of%20our%20model%0Ain%20challenging%20outdoor%20environments%2C%20we%20incorporate%20an%20exposure-adaptive%20module%0Awithin%20the%203DGS%20framework.%20Consequently%2C%20GSLoc%20enables%20efficient%20one-shot%20pose%0Arefinement%20given%20a%20single%20RGB%20query%20and%20a%20coarse%20initial%20pose%20estimation.%20Our%0Aproposed%20approach%20surpasses%20leading%20NeRF-based%20optimization%20methods%20in%20both%0Aaccuracy%20and%20runtime%20across%20indoor%20and%20outdoor%20visual%20localization%20benchmarks%2C%0Aachieving%20new%20state-of-the-art%20accuracy%20on%20two%20indoor%20datasets.%0A&entry.1838667208=http%3A//arxiv.org/abs/2408.11085v2&entry.124074799=Read"},
{"title": "Diffusion$^2$: Dynamic 3D Content Generation via Score Composition of\n  Video and Multi-view Diffusion Models", "author": "Zeyu Yang and Zijie Pan and Chun Gu and Li Zhang", "abstract": "  Recent advancements in 3D generation are predominantly propelled by\nimprovements in 3D-aware image diffusion models. These models are pretrained on\nInternet-scale image data and fine-tuned on massive 3D data, offering the\ncapability of producing highly consistent multi-view images. However, due to\nthe scarcity of synchronized multi-view video data, it remains challenging to\nadapt this paradigm to 4D generation directly. Despite that, the available\nvideo and 3D data are adequate for training video and multi-view diffusion\nmodels separately that can provide satisfactory dynamic and geometric priors\nrespectively. To take advantage of both, this paper presents Diffusion$^2$, a\nnovel framework for dynamic 3D content creation that reconciles the knowledge\nabout geometric consistency and temporal smoothness from these models to\ndirectly sample dense multi-view multi-frame images which can be employed to\noptimize continuous 4D representation. Specifically, we design a simple yet\neffective denoising strategy via score composition of pretrained video and\nmulti-view diffusion models based on the probability structure of the target\nimage array. To alleviate the potential conflicts between two heterogeneous\nscores, we further introduce variance-reducing sampling via interpolated steps,\nfacilitating smooth and stable generation. Owing to the high parallelism of the\nproposed image generation process and the efficiency of the modern 4D\nreconstruction pipeline, our framework can generate 4D content within few\nminutes. Notably, our method circumvents the reliance on expensive and\nhard-to-scale 4D data, thereby having the potential to benefit from the scaling\nof the foundation video and multi-view diffusion models. Extensive experiments\ndemonstrate the efficacy of our proposed framework in generating highly\nseamless and consistent 4D assets under various types of conditions.\n", "link": "http://arxiv.org/abs/2404.02148v4", "date": "2024-10-02", "relevancy": 3.4502, "topK": [{"title": "CAT3D: Create Anything in 3D with Multi-View Diffusion Models", "link": "http://arxiv.org/abs/2405.10314v1", "similarity": 0.6996}, {"title": "CAT3D: Create Anything in 3D with Multi-View Diffusion Models", "link": "http://arxiv.org/abs/2405.10314v1", "similarity": 0.6996}, {"title": "Magic-Me: Identity-Specific Video Customized Diffusion", "link": "http://arxiv.org/abs/2402.09368v1", "similarity": 0.6709}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Diffusion%24%5E2%24%3A%20Dynamic%203D%20Content%20Generation%20via%20Score%20Composition%20of%0A%20%20Video%20and%20Multi-view%20Diffusion%20Models&body=Title%3A%20Diffusion%24%5E2%24%3A%20Dynamic%203D%20Content%20Generation%20via%20Score%20Composition%20of%0A%20%20Video%20and%20Multi-view%20Diffusion%20Models%0AAuthor%3A%20Zeyu%20Yang%20and%20Zijie%20Pan%20and%20Chun%20Gu%20and%20Li%20Zhang%0AAbstract%3A%20%20%20Recent%20advancements%20in%203D%20generation%20are%20predominantly%20propelled%20by%0Aimprovements%20in%203D-aware%20image%20diffusion%20models.%20These%20models%20are%20pretrained%20on%0AInternet-scale%20image%20data%20and%20fine-tuned%20on%20massive%203D%20data%2C%20offering%20the%0Acapability%20of%20producing%20highly%20consistent%20multi-view%20images.%20However%2C%20due%20to%0Athe%20scarcity%20of%20synchronized%20multi-view%20video%20data%2C%20it%20remains%20challenging%20to%0Aadapt%20this%20paradigm%20to%204D%20generation%20directly.%20Despite%20that%2C%20the%20available%0Avideo%20and%203D%20data%20are%20adequate%20for%20training%20video%20and%20multi-view%20diffusion%0Amodels%20separately%20that%20can%20provide%20satisfactory%20dynamic%20and%20geometric%20priors%0Arespectively.%20To%20take%20advantage%20of%20both%2C%20this%20paper%20presents%20Diffusion%24%5E2%24%2C%20a%0Anovel%20framework%20for%20dynamic%203D%20content%20creation%20that%20reconciles%20the%20knowledge%0Aabout%20geometric%20consistency%20and%20temporal%20smoothness%20from%20these%20models%20to%0Adirectly%20sample%20dense%20multi-view%20multi-frame%20images%20which%20can%20be%20employed%20to%0Aoptimize%20continuous%204D%20representation.%20Specifically%2C%20we%20design%20a%20simple%20yet%0Aeffective%20denoising%20strategy%20via%20score%20composition%20of%20pretrained%20video%20and%0Amulti-view%20diffusion%20models%20based%20on%20the%20probability%20structure%20of%20the%20target%0Aimage%20array.%20To%20alleviate%20the%20potential%20conflicts%20between%20two%20heterogeneous%0Ascores%2C%20we%20further%20introduce%20variance-reducing%20sampling%20via%20interpolated%20steps%2C%0Afacilitating%20smooth%20and%20stable%20generation.%20Owing%20to%20the%20high%20parallelism%20of%20the%0Aproposed%20image%20generation%20process%20and%20the%20efficiency%20of%20the%20modern%204D%0Areconstruction%20pipeline%2C%20our%20framework%20can%20generate%204D%20content%20within%20few%0Aminutes.%20Notably%2C%20our%20method%20circumvents%20the%20reliance%20on%20expensive%20and%0Ahard-to-scale%204D%20data%2C%20thereby%20having%20the%20potential%20to%20benefit%20from%20the%20scaling%0Aof%20the%20foundation%20video%20and%20multi-view%20diffusion%20models.%20Extensive%20experiments%0Ademonstrate%20the%20efficacy%20of%20our%20proposed%20framework%20in%20generating%20highly%0Aseamless%20and%20consistent%204D%20assets%20under%20various%20types%20of%20conditions.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2404.02148v4%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DDiffusion%2524%255E2%2524%253A%2520Dynamic%25203D%2520Content%2520Generation%2520via%2520Score%2520Composition%2520of%250A%2520%2520Video%2520and%2520Multi-view%2520Diffusion%2520Models%26entry.906535625%3DZeyu%2520Yang%2520and%2520Zijie%2520Pan%2520and%2520Chun%2520Gu%2520and%2520Li%2520Zhang%26entry.1292438233%3D%2520%2520Recent%2520advancements%2520in%25203D%2520generation%2520are%2520predominantly%2520propelled%2520by%250Aimprovements%2520in%25203D-aware%2520image%2520diffusion%2520models.%2520These%2520models%2520are%2520pretrained%2520on%250AInternet-scale%2520image%2520data%2520and%2520fine-tuned%2520on%2520massive%25203D%2520data%252C%2520offering%2520the%250Acapability%2520of%2520producing%2520highly%2520consistent%2520multi-view%2520images.%2520However%252C%2520due%2520to%250Athe%2520scarcity%2520of%2520synchronized%2520multi-view%2520video%2520data%252C%2520it%2520remains%2520challenging%2520to%250Aadapt%2520this%2520paradigm%2520to%25204D%2520generation%2520directly.%2520Despite%2520that%252C%2520the%2520available%250Avideo%2520and%25203D%2520data%2520are%2520adequate%2520for%2520training%2520video%2520and%2520multi-view%2520diffusion%250Amodels%2520separately%2520that%2520can%2520provide%2520satisfactory%2520dynamic%2520and%2520geometric%2520priors%250Arespectively.%2520To%2520take%2520advantage%2520of%2520both%252C%2520this%2520paper%2520presents%2520Diffusion%2524%255E2%2524%252C%2520a%250Anovel%2520framework%2520for%2520dynamic%25203D%2520content%2520creation%2520that%2520reconciles%2520the%2520knowledge%250Aabout%2520geometric%2520consistency%2520and%2520temporal%2520smoothness%2520from%2520these%2520models%2520to%250Adirectly%2520sample%2520dense%2520multi-view%2520multi-frame%2520images%2520which%2520can%2520be%2520employed%2520to%250Aoptimize%2520continuous%25204D%2520representation.%2520Specifically%252C%2520we%2520design%2520a%2520simple%2520yet%250Aeffective%2520denoising%2520strategy%2520via%2520score%2520composition%2520of%2520pretrained%2520video%2520and%250Amulti-view%2520diffusion%2520models%2520based%2520on%2520the%2520probability%2520structure%2520of%2520the%2520target%250Aimage%2520array.%2520To%2520alleviate%2520the%2520potential%2520conflicts%2520between%2520two%2520heterogeneous%250Ascores%252C%2520we%2520further%2520introduce%2520variance-reducing%2520sampling%2520via%2520interpolated%2520steps%252C%250Afacilitating%2520smooth%2520and%2520stable%2520generation.%2520Owing%2520to%2520the%2520high%2520parallelism%2520of%2520the%250Aproposed%2520image%2520generation%2520process%2520and%2520the%2520efficiency%2520of%2520the%2520modern%25204D%250Areconstruction%2520pipeline%252C%2520our%2520framework%2520can%2520generate%25204D%2520content%2520within%2520few%250Aminutes.%2520Notably%252C%2520our%2520method%2520circumvents%2520the%2520reliance%2520on%2520expensive%2520and%250Ahard-to-scale%25204D%2520data%252C%2520thereby%2520having%2520the%2520potential%2520to%2520benefit%2520from%2520the%2520scaling%250Aof%2520the%2520foundation%2520video%2520and%2520multi-view%2520diffusion%2520models.%2520Extensive%2520experiments%250Ademonstrate%2520the%2520efficacy%2520of%2520our%2520proposed%2520framework%2520in%2520generating%2520highly%250Aseamless%2520and%2520consistent%25204D%2520assets%2520under%2520various%2520types%2520of%2520conditions.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2404.02148v4%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Diffusion%24%5E2%24%3A%20Dynamic%203D%20Content%20Generation%20via%20Score%20Composition%20of%0A%20%20Video%20and%20Multi-view%20Diffusion%20Models&entry.906535625=Zeyu%20Yang%20and%20Zijie%20Pan%20and%20Chun%20Gu%20and%20Li%20Zhang&entry.1292438233=%20%20Recent%20advancements%20in%203D%20generation%20are%20predominantly%20propelled%20by%0Aimprovements%20in%203D-aware%20image%20diffusion%20models.%20These%20models%20are%20pretrained%20on%0AInternet-scale%20image%20data%20and%20fine-tuned%20on%20massive%203D%20data%2C%20offering%20the%0Acapability%20of%20producing%20highly%20consistent%20multi-view%20images.%20However%2C%20due%20to%0Athe%20scarcity%20of%20synchronized%20multi-view%20video%20data%2C%20it%20remains%20challenging%20to%0Aadapt%20this%20paradigm%20to%204D%20generation%20directly.%20Despite%20that%2C%20the%20available%0Avideo%20and%203D%20data%20are%20adequate%20for%20training%20video%20and%20multi-view%20diffusion%0Amodels%20separately%20that%20can%20provide%20satisfactory%20dynamic%20and%20geometric%20priors%0Arespectively.%20To%20take%20advantage%20of%20both%2C%20this%20paper%20presents%20Diffusion%24%5E2%24%2C%20a%0Anovel%20framework%20for%20dynamic%203D%20content%20creation%20that%20reconciles%20the%20knowledge%0Aabout%20geometric%20consistency%20and%20temporal%20smoothness%20from%20these%20models%20to%0Adirectly%20sample%20dense%20multi-view%20multi-frame%20images%20which%20can%20be%20employed%20to%0Aoptimize%20continuous%204D%20representation.%20Specifically%2C%20we%20design%20a%20simple%20yet%0Aeffective%20denoising%20strategy%20via%20score%20composition%20of%20pretrained%20video%20and%0Amulti-view%20diffusion%20models%20based%20on%20the%20probability%20structure%20of%20the%20target%0Aimage%20array.%20To%20alleviate%20the%20potential%20conflicts%20between%20two%20heterogeneous%0Ascores%2C%20we%20further%20introduce%20variance-reducing%20sampling%20via%20interpolated%20steps%2C%0Afacilitating%20smooth%20and%20stable%20generation.%20Owing%20to%20the%20high%20parallelism%20of%20the%0Aproposed%20image%20generation%20process%20and%20the%20efficiency%20of%20the%20modern%204D%0Areconstruction%20pipeline%2C%20our%20framework%20can%20generate%204D%20content%20within%20few%0Aminutes.%20Notably%2C%20our%20method%20circumvents%20the%20reliance%20on%20expensive%20and%0Ahard-to-scale%204D%20data%2C%20thereby%20having%20the%20potential%20to%20benefit%20from%20the%20scaling%0Aof%20the%20foundation%20video%20and%20multi-view%20diffusion%20models.%20Extensive%20experiments%0Ademonstrate%20the%20efficacy%20of%20our%20proposed%20framework%20in%20generating%20highly%0Aseamless%20and%20consistent%204D%20assets%20under%20various%20types%20of%20conditions.%0A&entry.1838667208=http%3A//arxiv.org/abs/2404.02148v4&entry.124074799=Read"},
{"title": "Spec-Gaussian: Anisotropic View-Dependent Appearance for 3D Gaussian\n  Splatting", "author": "Ziyi Yang and Xinyu Gao and Yangtian Sun and Yihua Huang and Xiaoyang Lyu and Wen Zhou and Shaohui Jiao and Xiaojuan Qi and Xiaogang Jin", "abstract": "  The recent advancements in 3D Gaussian splatting (3D-GS) have not only\nfacilitated real-time rendering through modern GPU rasterization pipelines but\nhave also attained state-of-the-art rendering quality. Nevertheless, despite\nits exceptional rendering quality and performance on standard datasets, 3D-GS\nfrequently encounters difficulties in accurately modeling specular and\nanisotropic components. This issue stems from the limited ability of spherical\nharmonics (SH) to represent high-frequency information. To overcome this\nchallenge, we introduce Spec-Gaussian, an approach that utilizes an anisotropic\nspherical Gaussian (ASG) appearance field instead of SH for modeling the\nview-dependent appearance of each 3D Gaussian. Additionally, we have developed\na coarse-to-fine training strategy to improve learning efficiency and eliminate\nfloaters caused by overfitting in real-world scenes. Our experimental results\ndemonstrate that our method surpasses existing approaches in terms of rendering\nquality. Thanks to ASG, we have significantly improved the ability of 3D-GS to\nmodel scenes with specular and anisotropic components without increasing the\nnumber of 3D Gaussians. This improvement extends the applicability of 3D GS to\nhandle intricate scenarios with specular and anisotropic surfaces. Project page\nis https://ingra14m.github.io/Spec-Gaussian-website/.\n", "link": "http://arxiv.org/abs/2402.15870v2", "date": "2024-10-02", "relevancy": 3.4416, "topK": [{"title": "GST: Precise 3D Human Body from a Single Image with Gaussian Splatting\n  Transformers", "link": "http://arxiv.org/abs/2409.04196v1", "similarity": 0.706}, {"title": "RTG-SLAM: Real-time 3D Reconstruction at Scale using Gaussian Splatting", "link": "http://arxiv.org/abs/2404.19706v1", "similarity": 0.7009}, {"title": "Dynamic Gaussians Mesh: Consistent Mesh Reconstruction from Monocular\n  Videos", "link": "http://arxiv.org/abs/2404.12379v2", "similarity": 0.6581}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Spec-Gaussian%3A%20Anisotropic%20View-Dependent%20Appearance%20for%203D%20Gaussian%0A%20%20Splatting&body=Title%3A%20Spec-Gaussian%3A%20Anisotropic%20View-Dependent%20Appearance%20for%203D%20Gaussian%0A%20%20Splatting%0AAuthor%3A%20Ziyi%20Yang%20and%20Xinyu%20Gao%20and%20Yangtian%20Sun%20and%20Yihua%20Huang%20and%20Xiaoyang%20Lyu%20and%20Wen%20Zhou%20and%20Shaohui%20Jiao%20and%20Xiaojuan%20Qi%20and%20Xiaogang%20Jin%0AAbstract%3A%20%20%20The%20recent%20advancements%20in%203D%20Gaussian%20splatting%20%283D-GS%29%20have%20not%20only%0Afacilitated%20real-time%20rendering%20through%20modern%20GPU%20rasterization%20pipelines%20but%0Ahave%20also%20attained%20state-of-the-art%20rendering%20quality.%20Nevertheless%2C%20despite%0Aits%20exceptional%20rendering%20quality%20and%20performance%20on%20standard%20datasets%2C%203D-GS%0Afrequently%20encounters%20difficulties%20in%20accurately%20modeling%20specular%20and%0Aanisotropic%20components.%20This%20issue%20stems%20from%20the%20limited%20ability%20of%20spherical%0Aharmonics%20%28SH%29%20to%20represent%20high-frequency%20information.%20To%20overcome%20this%0Achallenge%2C%20we%20introduce%20Spec-Gaussian%2C%20an%20approach%20that%20utilizes%20an%20anisotropic%0Aspherical%20Gaussian%20%28ASG%29%20appearance%20field%20instead%20of%20SH%20for%20modeling%20the%0Aview-dependent%20appearance%20of%20each%203D%20Gaussian.%20Additionally%2C%20we%20have%20developed%0Aa%20coarse-to-fine%20training%20strategy%20to%20improve%20learning%20efficiency%20and%20eliminate%0Afloaters%20caused%20by%20overfitting%20in%20real-world%20scenes.%20Our%20experimental%20results%0Ademonstrate%20that%20our%20method%20surpasses%20existing%20approaches%20in%20terms%20of%20rendering%0Aquality.%20Thanks%20to%20ASG%2C%20we%20have%20significantly%20improved%20the%20ability%20of%203D-GS%20to%0Amodel%20scenes%20with%20specular%20and%20anisotropic%20components%20without%20increasing%20the%0Anumber%20of%203D%20Gaussians.%20This%20improvement%20extends%20the%20applicability%20of%203D%20GS%20to%0Ahandle%20intricate%20scenarios%20with%20specular%20and%20anisotropic%20surfaces.%20Project%20page%0Ais%20https%3A//ingra14m.github.io/Spec-Gaussian-website/.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2402.15870v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DSpec-Gaussian%253A%2520Anisotropic%2520View-Dependent%2520Appearance%2520for%25203D%2520Gaussian%250A%2520%2520Splatting%26entry.906535625%3DZiyi%2520Yang%2520and%2520Xinyu%2520Gao%2520and%2520Yangtian%2520Sun%2520and%2520Yihua%2520Huang%2520and%2520Xiaoyang%2520Lyu%2520and%2520Wen%2520Zhou%2520and%2520Shaohui%2520Jiao%2520and%2520Xiaojuan%2520Qi%2520and%2520Xiaogang%2520Jin%26entry.1292438233%3D%2520%2520The%2520recent%2520advancements%2520in%25203D%2520Gaussian%2520splatting%2520%25283D-GS%2529%2520have%2520not%2520only%250Afacilitated%2520real-time%2520rendering%2520through%2520modern%2520GPU%2520rasterization%2520pipelines%2520but%250Ahave%2520also%2520attained%2520state-of-the-art%2520rendering%2520quality.%2520Nevertheless%252C%2520despite%250Aits%2520exceptional%2520rendering%2520quality%2520and%2520performance%2520on%2520standard%2520datasets%252C%25203D-GS%250Afrequently%2520encounters%2520difficulties%2520in%2520accurately%2520modeling%2520specular%2520and%250Aanisotropic%2520components.%2520This%2520issue%2520stems%2520from%2520the%2520limited%2520ability%2520of%2520spherical%250Aharmonics%2520%2528SH%2529%2520to%2520represent%2520high-frequency%2520information.%2520To%2520overcome%2520this%250Achallenge%252C%2520we%2520introduce%2520Spec-Gaussian%252C%2520an%2520approach%2520that%2520utilizes%2520an%2520anisotropic%250Aspherical%2520Gaussian%2520%2528ASG%2529%2520appearance%2520field%2520instead%2520of%2520SH%2520for%2520modeling%2520the%250Aview-dependent%2520appearance%2520of%2520each%25203D%2520Gaussian.%2520Additionally%252C%2520we%2520have%2520developed%250Aa%2520coarse-to-fine%2520training%2520strategy%2520to%2520improve%2520learning%2520efficiency%2520and%2520eliminate%250Afloaters%2520caused%2520by%2520overfitting%2520in%2520real-world%2520scenes.%2520Our%2520experimental%2520results%250Ademonstrate%2520that%2520our%2520method%2520surpasses%2520existing%2520approaches%2520in%2520terms%2520of%2520rendering%250Aquality.%2520Thanks%2520to%2520ASG%252C%2520we%2520have%2520significantly%2520improved%2520the%2520ability%2520of%25203D-GS%2520to%250Amodel%2520scenes%2520with%2520specular%2520and%2520anisotropic%2520components%2520without%2520increasing%2520the%250Anumber%2520of%25203D%2520Gaussians.%2520This%2520improvement%2520extends%2520the%2520applicability%2520of%25203D%2520GS%2520to%250Ahandle%2520intricate%2520scenarios%2520with%2520specular%2520and%2520anisotropic%2520surfaces.%2520Project%2520page%250Ais%2520https%253A//ingra14m.github.io/Spec-Gaussian-website/.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2402.15870v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Spec-Gaussian%3A%20Anisotropic%20View-Dependent%20Appearance%20for%203D%20Gaussian%0A%20%20Splatting&entry.906535625=Ziyi%20Yang%20and%20Xinyu%20Gao%20and%20Yangtian%20Sun%20and%20Yihua%20Huang%20and%20Xiaoyang%20Lyu%20and%20Wen%20Zhou%20and%20Shaohui%20Jiao%20and%20Xiaojuan%20Qi%20and%20Xiaogang%20Jin&entry.1292438233=%20%20The%20recent%20advancements%20in%203D%20Gaussian%20splatting%20%283D-GS%29%20have%20not%20only%0Afacilitated%20real-time%20rendering%20through%20modern%20GPU%20rasterization%20pipelines%20but%0Ahave%20also%20attained%20state-of-the-art%20rendering%20quality.%20Nevertheless%2C%20despite%0Aits%20exceptional%20rendering%20quality%20and%20performance%20on%20standard%20datasets%2C%203D-GS%0Afrequently%20encounters%20difficulties%20in%20accurately%20modeling%20specular%20and%0Aanisotropic%20components.%20This%20issue%20stems%20from%20the%20limited%20ability%20of%20spherical%0Aharmonics%20%28SH%29%20to%20represent%20high-frequency%20information.%20To%20overcome%20this%0Achallenge%2C%20we%20introduce%20Spec-Gaussian%2C%20an%20approach%20that%20utilizes%20an%20anisotropic%0Aspherical%20Gaussian%20%28ASG%29%20appearance%20field%20instead%20of%20SH%20for%20modeling%20the%0Aview-dependent%20appearance%20of%20each%203D%20Gaussian.%20Additionally%2C%20we%20have%20developed%0Aa%20coarse-to-fine%20training%20strategy%20to%20improve%20learning%20efficiency%20and%20eliminate%0Afloaters%20caused%20by%20overfitting%20in%20real-world%20scenes.%20Our%20experimental%20results%0Ademonstrate%20that%20our%20method%20surpasses%20existing%20approaches%20in%20terms%20of%20rendering%0Aquality.%20Thanks%20to%20ASG%2C%20we%20have%20significantly%20improved%20the%20ability%20of%203D-GS%20to%0Amodel%20scenes%20with%20specular%20and%20anisotropic%20components%20without%20increasing%20the%0Anumber%20of%203D%20Gaussians.%20This%20improvement%20extends%20the%20applicability%20of%203D%20GS%20to%0Ahandle%20intricate%20scenarios%20with%20specular%20and%20anisotropic%20surfaces.%20Project%20page%0Ais%20https%3A//ingra14m.github.io/Spec-Gaussian-website/.%0A&entry.1838667208=http%3A//arxiv.org/abs/2402.15870v2&entry.124074799=Read"},
{"title": "Flex3D: Feed-Forward 3D Generation With Flexible Reconstruction Model\n  And Input View Curation", "author": "Junlin Han and Jianyuan Wang and Andrea Vedaldi and Philip Torr and Filippos Kokkinos", "abstract": "  Generating high-quality 3D content from text, single images, or sparse view\nimages remains a challenging task with broad applications. Existing methods\ntypically employ multi-view diffusion models to synthesize multi-view images,\nfollowed by a feed-forward process for 3D reconstruction. However, these\napproaches are often constrained by a small and fixed number of input views,\nlimiting their ability to capture diverse viewpoints and, even worse, leading\nto suboptimal generation results if the synthesized views are of poor quality.\nTo address these limitations, we propose Flex3D, a novel two-stage framework\ncapable of leveraging an arbitrary number of high-quality input views. The\nfirst stage consists of a candidate view generation and curation pipeline. We\nemploy a fine-tuned multi-view image diffusion model and a video diffusion\nmodel to generate a pool of candidate views, enabling a rich representation of\nthe target 3D object. Subsequently, a view selection pipeline filters these\nviews based on quality and consistency, ensuring that only the high-quality and\nreliable views are used for reconstruction. In the second stage, the curated\nviews are fed into a Flexible Reconstruction Model (FlexRM), built upon a\ntransformer architecture that can effectively process an arbitrary number of\ninputs. FlemRM directly outputs 3D Gaussian points leveraging a tri-plane\nrepresentation, enabling efficient and detailed 3D generation. Through\nextensive exploration of design and training strategies, we optimize FlexRM to\nachieve superior performance in both reconstruction and generation tasks. Our\nresults demonstrate that Flex3D achieves state-of-the-art performance, with a\nuser study winning rate of over 92% in 3D generation tasks when compared to\nseveral of the latest feed-forward 3D generative models.\n", "link": "http://arxiv.org/abs/2410.00890v2", "date": "2024-10-02", "relevancy": 3.3796, "topK": [{"title": "CAT3D: Create Anything in 3D with Multi-View Diffusion Models", "link": "http://arxiv.org/abs/2405.10314v1", "similarity": 0.7098}, {"title": "CAT3D: Create Anything in 3D with Multi-View Diffusion Models", "link": "http://arxiv.org/abs/2405.10314v1", "similarity": 0.7098}, {"title": "GST: Precise 3D Human Body from a Single Image with Gaussian Splatting\n  Transformers", "link": "http://arxiv.org/abs/2409.04196v1", "similarity": 0.6081}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Flex3D%3A%20Feed-Forward%203D%20Generation%20With%20Flexible%20Reconstruction%20Model%0A%20%20And%20Input%20View%20Curation&body=Title%3A%20Flex3D%3A%20Feed-Forward%203D%20Generation%20With%20Flexible%20Reconstruction%20Model%0A%20%20And%20Input%20View%20Curation%0AAuthor%3A%20Junlin%20Han%20and%20Jianyuan%20Wang%20and%20Andrea%20Vedaldi%20and%20Philip%20Torr%20and%20Filippos%20Kokkinos%0AAbstract%3A%20%20%20Generating%20high-quality%203D%20content%20from%20text%2C%20single%20images%2C%20or%20sparse%20view%0Aimages%20remains%20a%20challenging%20task%20with%20broad%20applications.%20Existing%20methods%0Atypically%20employ%20multi-view%20diffusion%20models%20to%20synthesize%20multi-view%20images%2C%0Afollowed%20by%20a%20feed-forward%20process%20for%203D%20reconstruction.%20However%2C%20these%0Aapproaches%20are%20often%20constrained%20by%20a%20small%20and%20fixed%20number%20of%20input%20views%2C%0Alimiting%20their%20ability%20to%20capture%20diverse%20viewpoints%20and%2C%20even%20worse%2C%20leading%0Ato%20suboptimal%20generation%20results%20if%20the%20synthesized%20views%20are%20of%20poor%20quality.%0ATo%20address%20these%20limitations%2C%20we%20propose%20Flex3D%2C%20a%20novel%20two-stage%20framework%0Acapable%20of%20leveraging%20an%20arbitrary%20number%20of%20high-quality%20input%20views.%20The%0Afirst%20stage%20consists%20of%20a%20candidate%20view%20generation%20and%20curation%20pipeline.%20We%0Aemploy%20a%20fine-tuned%20multi-view%20image%20diffusion%20model%20and%20a%20video%20diffusion%0Amodel%20to%20generate%20a%20pool%20of%20candidate%20views%2C%20enabling%20a%20rich%20representation%20of%0Athe%20target%203D%20object.%20Subsequently%2C%20a%20view%20selection%20pipeline%20filters%20these%0Aviews%20based%20on%20quality%20and%20consistency%2C%20ensuring%20that%20only%20the%20high-quality%20and%0Areliable%20views%20are%20used%20for%20reconstruction.%20In%20the%20second%20stage%2C%20the%20curated%0Aviews%20are%20fed%20into%20a%20Flexible%20Reconstruction%20Model%20%28FlexRM%29%2C%20built%20upon%20a%0Atransformer%20architecture%20that%20can%20effectively%20process%20an%20arbitrary%20number%20of%0Ainputs.%20FlemRM%20directly%20outputs%203D%20Gaussian%20points%20leveraging%20a%20tri-plane%0Arepresentation%2C%20enabling%20efficient%20and%20detailed%203D%20generation.%20Through%0Aextensive%20exploration%20of%20design%20and%20training%20strategies%2C%20we%20optimize%20FlexRM%20to%0Aachieve%20superior%20performance%20in%20both%20reconstruction%20and%20generation%20tasks.%20Our%0Aresults%20demonstrate%20that%20Flex3D%20achieves%20state-of-the-art%20performance%2C%20with%20a%0Auser%20study%20winning%20rate%20of%20over%2092%25%20in%203D%20generation%20tasks%20when%20compared%20to%0Aseveral%20of%20the%20latest%20feed-forward%203D%20generative%20models.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2410.00890v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DFlex3D%253A%2520Feed-Forward%25203D%2520Generation%2520With%2520Flexible%2520Reconstruction%2520Model%250A%2520%2520And%2520Input%2520View%2520Curation%26entry.906535625%3DJunlin%2520Han%2520and%2520Jianyuan%2520Wang%2520and%2520Andrea%2520Vedaldi%2520and%2520Philip%2520Torr%2520and%2520Filippos%2520Kokkinos%26entry.1292438233%3D%2520%2520Generating%2520high-quality%25203D%2520content%2520from%2520text%252C%2520single%2520images%252C%2520or%2520sparse%2520view%250Aimages%2520remains%2520a%2520challenging%2520task%2520with%2520broad%2520applications.%2520Existing%2520methods%250Atypically%2520employ%2520multi-view%2520diffusion%2520models%2520to%2520synthesize%2520multi-view%2520images%252C%250Afollowed%2520by%2520a%2520feed-forward%2520process%2520for%25203D%2520reconstruction.%2520However%252C%2520these%250Aapproaches%2520are%2520often%2520constrained%2520by%2520a%2520small%2520and%2520fixed%2520number%2520of%2520input%2520views%252C%250Alimiting%2520their%2520ability%2520to%2520capture%2520diverse%2520viewpoints%2520and%252C%2520even%2520worse%252C%2520leading%250Ato%2520suboptimal%2520generation%2520results%2520if%2520the%2520synthesized%2520views%2520are%2520of%2520poor%2520quality.%250ATo%2520address%2520these%2520limitations%252C%2520we%2520propose%2520Flex3D%252C%2520a%2520novel%2520two-stage%2520framework%250Acapable%2520of%2520leveraging%2520an%2520arbitrary%2520number%2520of%2520high-quality%2520input%2520views.%2520The%250Afirst%2520stage%2520consists%2520of%2520a%2520candidate%2520view%2520generation%2520and%2520curation%2520pipeline.%2520We%250Aemploy%2520a%2520fine-tuned%2520multi-view%2520image%2520diffusion%2520model%2520and%2520a%2520video%2520diffusion%250Amodel%2520to%2520generate%2520a%2520pool%2520of%2520candidate%2520views%252C%2520enabling%2520a%2520rich%2520representation%2520of%250Athe%2520target%25203D%2520object.%2520Subsequently%252C%2520a%2520view%2520selection%2520pipeline%2520filters%2520these%250Aviews%2520based%2520on%2520quality%2520and%2520consistency%252C%2520ensuring%2520that%2520only%2520the%2520high-quality%2520and%250Areliable%2520views%2520are%2520used%2520for%2520reconstruction.%2520In%2520the%2520second%2520stage%252C%2520the%2520curated%250Aviews%2520are%2520fed%2520into%2520a%2520Flexible%2520Reconstruction%2520Model%2520%2528FlexRM%2529%252C%2520built%2520upon%2520a%250Atransformer%2520architecture%2520that%2520can%2520effectively%2520process%2520an%2520arbitrary%2520number%2520of%250Ainputs.%2520FlemRM%2520directly%2520outputs%25203D%2520Gaussian%2520points%2520leveraging%2520a%2520tri-plane%250Arepresentation%252C%2520enabling%2520efficient%2520and%2520detailed%25203D%2520generation.%2520Through%250Aextensive%2520exploration%2520of%2520design%2520and%2520training%2520strategies%252C%2520we%2520optimize%2520FlexRM%2520to%250Aachieve%2520superior%2520performance%2520in%2520both%2520reconstruction%2520and%2520generation%2520tasks.%2520Our%250Aresults%2520demonstrate%2520that%2520Flex3D%2520achieves%2520state-of-the-art%2520performance%252C%2520with%2520a%250Auser%2520study%2520winning%2520rate%2520of%2520over%252092%2525%2520in%25203D%2520generation%2520tasks%2520when%2520compared%2520to%250Aseveral%2520of%2520the%2520latest%2520feed-forward%25203D%2520generative%2520models.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2410.00890v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Flex3D%3A%20Feed-Forward%203D%20Generation%20With%20Flexible%20Reconstruction%20Model%0A%20%20And%20Input%20View%20Curation&entry.906535625=Junlin%20Han%20and%20Jianyuan%20Wang%20and%20Andrea%20Vedaldi%20and%20Philip%20Torr%20and%20Filippos%20Kokkinos&entry.1292438233=%20%20Generating%20high-quality%203D%20content%20from%20text%2C%20single%20images%2C%20or%20sparse%20view%0Aimages%20remains%20a%20challenging%20task%20with%20broad%20applications.%20Existing%20methods%0Atypically%20employ%20multi-view%20diffusion%20models%20to%20synthesize%20multi-view%20images%2C%0Afollowed%20by%20a%20feed-forward%20process%20for%203D%20reconstruction.%20However%2C%20these%0Aapproaches%20are%20often%20constrained%20by%20a%20small%20and%20fixed%20number%20of%20input%20views%2C%0Alimiting%20their%20ability%20to%20capture%20diverse%20viewpoints%20and%2C%20even%20worse%2C%20leading%0Ato%20suboptimal%20generation%20results%20if%20the%20synthesized%20views%20are%20of%20poor%20quality.%0ATo%20address%20these%20limitations%2C%20we%20propose%20Flex3D%2C%20a%20novel%20two-stage%20framework%0Acapable%20of%20leveraging%20an%20arbitrary%20number%20of%20high-quality%20input%20views.%20The%0Afirst%20stage%20consists%20of%20a%20candidate%20view%20generation%20and%20curation%20pipeline.%20We%0Aemploy%20a%20fine-tuned%20multi-view%20image%20diffusion%20model%20and%20a%20video%20diffusion%0Amodel%20to%20generate%20a%20pool%20of%20candidate%20views%2C%20enabling%20a%20rich%20representation%20of%0Athe%20target%203D%20object.%20Subsequently%2C%20a%20view%20selection%20pipeline%20filters%20these%0Aviews%20based%20on%20quality%20and%20consistency%2C%20ensuring%20that%20only%20the%20high-quality%20and%0Areliable%20views%20are%20used%20for%20reconstruction.%20In%20the%20second%20stage%2C%20the%20curated%0Aviews%20are%20fed%20into%20a%20Flexible%20Reconstruction%20Model%20%28FlexRM%29%2C%20built%20upon%20a%0Atransformer%20architecture%20that%20can%20effectively%20process%20an%20arbitrary%20number%20of%0Ainputs.%20FlemRM%20directly%20outputs%203D%20Gaussian%20points%20leveraging%20a%20tri-plane%0Arepresentation%2C%20enabling%20efficient%20and%20detailed%203D%20generation.%20Through%0Aextensive%20exploration%20of%20design%20and%20training%20strategies%2C%20we%20optimize%20FlexRM%20to%0Aachieve%20superior%20performance%20in%20both%20reconstruction%20and%20generation%20tasks.%20Our%0Aresults%20demonstrate%20that%20Flex3D%20achieves%20state-of-the-art%20performance%2C%20with%20a%0Auser%20study%20winning%20rate%20of%20over%2092%25%20in%203D%20generation%20tasks%20when%20compared%20to%0Aseveral%20of%20the%20latest%20feed-forward%203D%20generative%20models.%0A&entry.1838667208=http%3A//arxiv.org/abs/2410.00890v2&entry.124074799=Read"},
{"title": "EVA-Gaussian: 3D Gaussian-based Real-time Human Novel View Synthesis\n  under Diverse Camera Settings", "author": "Yingdong Hu and Zhening Liu and Jiawei Shao and Zehong Lin and Jun Zhang", "abstract": "  The feed-forward based 3D Gaussian Splatting method has demonstrated\nexceptional capability in real-time human novel view synthesis. However,\nexisting approaches are restricted to dense viewpoint settings, which limits\ntheir flexibility in free-viewpoint rendering across a wide range of camera\nview angle discrepancies. To address this limitation, we propose a real-time\npipeline named EVA-Gaussian for 3D human novel view synthesis across diverse\ncamera settings. Specifically, we first introduce an Efficient cross-View\nAttention (EVA) module to accurately estimate the position of each 3D Gaussian\nfrom the source images. Then, we integrate the source images with the estimated\nGaussian position map to predict the attributes and feature embeddings of the\n3D Gaussians. Moreover, we employ a recurrent feature refiner to correct\nartifacts caused by geometric errors in position estimation and enhance visual\nfidelity.To further improve synthesis quality, we incorporate a powerful anchor\nloss function for both 3D Gaussian attributes and human face landmarks.\nExperimental results on the THuman2.0 and THumansit datasets showcase the\nsuperiority of our EVA-Gaussian approach in rendering quality across diverse\ncamera settings. Project page:\nhttps://zhenliuzju.github.io/huyingdong/EVA-Gaussian.\n", "link": "http://arxiv.org/abs/2410.01425v1", "date": "2024-10-02", "relevancy": 3.3649, "topK": [{"title": "GST: Precise 3D Human Body from a Single Image with Gaussian Splatting\n  Transformers", "link": "http://arxiv.org/abs/2409.04196v1", "similarity": 0.6965}, {"title": "RTG-SLAM: Real-time 3D Reconstruction at Scale using Gaussian Splatting", "link": "http://arxiv.org/abs/2404.19706v1", "similarity": 0.6699}, {"title": "3D Gaussian Blendshapes for Head Avatar Animation", "link": "http://arxiv.org/abs/2404.19398v2", "similarity": 0.6526}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20EVA-Gaussian%3A%203D%20Gaussian-based%20Real-time%20Human%20Novel%20View%20Synthesis%0A%20%20under%20Diverse%20Camera%20Settings&body=Title%3A%20EVA-Gaussian%3A%203D%20Gaussian-based%20Real-time%20Human%20Novel%20View%20Synthesis%0A%20%20under%20Diverse%20Camera%20Settings%0AAuthor%3A%20Yingdong%20Hu%20and%20Zhening%20Liu%20and%20Jiawei%20Shao%20and%20Zehong%20Lin%20and%20Jun%20Zhang%0AAbstract%3A%20%20%20The%20feed-forward%20based%203D%20Gaussian%20Splatting%20method%20has%20demonstrated%0Aexceptional%20capability%20in%20real-time%20human%20novel%20view%20synthesis.%20However%2C%0Aexisting%20approaches%20are%20restricted%20to%20dense%20viewpoint%20settings%2C%20which%20limits%0Atheir%20flexibility%20in%20free-viewpoint%20rendering%20across%20a%20wide%20range%20of%20camera%0Aview%20angle%20discrepancies.%20To%20address%20this%20limitation%2C%20we%20propose%20a%20real-time%0Apipeline%20named%20EVA-Gaussian%20for%203D%20human%20novel%20view%20synthesis%20across%20diverse%0Acamera%20settings.%20Specifically%2C%20we%20first%20introduce%20an%20Efficient%20cross-View%0AAttention%20%28EVA%29%20module%20to%20accurately%20estimate%20the%20position%20of%20each%203D%20Gaussian%0Afrom%20the%20source%20images.%20Then%2C%20we%20integrate%20the%20source%20images%20with%20the%20estimated%0AGaussian%20position%20map%20to%20predict%20the%20attributes%20and%20feature%20embeddings%20of%20the%0A3D%20Gaussians.%20Moreover%2C%20we%20employ%20a%20recurrent%20feature%20refiner%20to%20correct%0Aartifacts%20caused%20by%20geometric%20errors%20in%20position%20estimation%20and%20enhance%20visual%0Afidelity.To%20further%20improve%20synthesis%20quality%2C%20we%20incorporate%20a%20powerful%20anchor%0Aloss%20function%20for%20both%203D%20Gaussian%20attributes%20and%20human%20face%20landmarks.%0AExperimental%20results%20on%20the%20THuman2.0%20and%20THumansit%20datasets%20showcase%20the%0Asuperiority%20of%20our%20EVA-Gaussian%20approach%20in%20rendering%20quality%20across%20diverse%0Acamera%20settings.%20Project%20page%3A%0Ahttps%3A//zhenliuzju.github.io/huyingdong/EVA-Gaussian.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2410.01425v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DEVA-Gaussian%253A%25203D%2520Gaussian-based%2520Real-time%2520Human%2520Novel%2520View%2520Synthesis%250A%2520%2520under%2520Diverse%2520Camera%2520Settings%26entry.906535625%3DYingdong%2520Hu%2520and%2520Zhening%2520Liu%2520and%2520Jiawei%2520Shao%2520and%2520Zehong%2520Lin%2520and%2520Jun%2520Zhang%26entry.1292438233%3D%2520%2520The%2520feed-forward%2520based%25203D%2520Gaussian%2520Splatting%2520method%2520has%2520demonstrated%250Aexceptional%2520capability%2520in%2520real-time%2520human%2520novel%2520view%2520synthesis.%2520However%252C%250Aexisting%2520approaches%2520are%2520restricted%2520to%2520dense%2520viewpoint%2520settings%252C%2520which%2520limits%250Atheir%2520flexibility%2520in%2520free-viewpoint%2520rendering%2520across%2520a%2520wide%2520range%2520of%2520camera%250Aview%2520angle%2520discrepancies.%2520To%2520address%2520this%2520limitation%252C%2520we%2520propose%2520a%2520real-time%250Apipeline%2520named%2520EVA-Gaussian%2520for%25203D%2520human%2520novel%2520view%2520synthesis%2520across%2520diverse%250Acamera%2520settings.%2520Specifically%252C%2520we%2520first%2520introduce%2520an%2520Efficient%2520cross-View%250AAttention%2520%2528EVA%2529%2520module%2520to%2520accurately%2520estimate%2520the%2520position%2520of%2520each%25203D%2520Gaussian%250Afrom%2520the%2520source%2520images.%2520Then%252C%2520we%2520integrate%2520the%2520source%2520images%2520with%2520the%2520estimated%250AGaussian%2520position%2520map%2520to%2520predict%2520the%2520attributes%2520and%2520feature%2520embeddings%2520of%2520the%250A3D%2520Gaussians.%2520Moreover%252C%2520we%2520employ%2520a%2520recurrent%2520feature%2520refiner%2520to%2520correct%250Aartifacts%2520caused%2520by%2520geometric%2520errors%2520in%2520position%2520estimation%2520and%2520enhance%2520visual%250Afidelity.To%2520further%2520improve%2520synthesis%2520quality%252C%2520we%2520incorporate%2520a%2520powerful%2520anchor%250Aloss%2520function%2520for%2520both%25203D%2520Gaussian%2520attributes%2520and%2520human%2520face%2520landmarks.%250AExperimental%2520results%2520on%2520the%2520THuman2.0%2520and%2520THumansit%2520datasets%2520showcase%2520the%250Asuperiority%2520of%2520our%2520EVA-Gaussian%2520approach%2520in%2520rendering%2520quality%2520across%2520diverse%250Acamera%2520settings.%2520Project%2520page%253A%250Ahttps%253A//zhenliuzju.github.io/huyingdong/EVA-Gaussian.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2410.01425v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=EVA-Gaussian%3A%203D%20Gaussian-based%20Real-time%20Human%20Novel%20View%20Synthesis%0A%20%20under%20Diverse%20Camera%20Settings&entry.906535625=Yingdong%20Hu%20and%20Zhening%20Liu%20and%20Jiawei%20Shao%20and%20Zehong%20Lin%20and%20Jun%20Zhang&entry.1292438233=%20%20The%20feed-forward%20based%203D%20Gaussian%20Splatting%20method%20has%20demonstrated%0Aexceptional%20capability%20in%20real-time%20human%20novel%20view%20synthesis.%20However%2C%0Aexisting%20approaches%20are%20restricted%20to%20dense%20viewpoint%20settings%2C%20which%20limits%0Atheir%20flexibility%20in%20free-viewpoint%20rendering%20across%20a%20wide%20range%20of%20camera%0Aview%20angle%20discrepancies.%20To%20address%20this%20limitation%2C%20we%20propose%20a%20real-time%0Apipeline%20named%20EVA-Gaussian%20for%203D%20human%20novel%20view%20synthesis%20across%20diverse%0Acamera%20settings.%20Specifically%2C%20we%20first%20introduce%20an%20Efficient%20cross-View%0AAttention%20%28EVA%29%20module%20to%20accurately%20estimate%20the%20position%20of%20each%203D%20Gaussian%0Afrom%20the%20source%20images.%20Then%2C%20we%20integrate%20the%20source%20images%20with%20the%20estimated%0AGaussian%20position%20map%20to%20predict%20the%20attributes%20and%20feature%20embeddings%20of%20the%0A3D%20Gaussians.%20Moreover%2C%20we%20employ%20a%20recurrent%20feature%20refiner%20to%20correct%0Aartifacts%20caused%20by%20geometric%20errors%20in%20position%20estimation%20and%20enhance%20visual%0Afidelity.To%20further%20improve%20synthesis%20quality%2C%20we%20incorporate%20a%20powerful%20anchor%0Aloss%20function%20for%20both%203D%20Gaussian%20attributes%20and%20human%20face%20landmarks.%0AExperimental%20results%20on%20the%20THuman2.0%20and%20THumansit%20datasets%20showcase%20the%0Asuperiority%20of%20our%20EVA-Gaussian%20approach%20in%20rendering%20quality%20across%20diverse%0Acamera%20settings.%20Project%20page%3A%0Ahttps%3A//zhenliuzju.github.io/huyingdong/EVA-Gaussian.%0A&entry.1838667208=http%3A//arxiv.org/abs/2410.01425v1&entry.124074799=Read"},
{"title": "UW-GS: Distractor-Aware 3D Gaussian Splatting for Enhanced Underwater\n  Scene Reconstruction", "author": "Haoran Wang and Nantheera Anantrasirichai and Fan Zhang and David Bull", "abstract": "  3D Gaussian splatting (3DGS) offers the capability to achieve real-time high\nquality 3D scene rendering. However, 3DGS assumes that the scene is in a clear\nmedium environment and struggles to generate satisfactory representations in\nunderwater scenes, where light absorption and scattering are prevalent and\nmoving objects are involved. To overcome these, we introduce a novel Gaussian\nSplatting-based method, UW-GS, designed specifically for underwater\napplications. It introduces a color appearance that models distance-dependent\ncolor variation, employs a new physics-based density control strategy to\nenhance clarity for distant objects, and uses a binary motion mask to handle\ndynamic content. Optimized with a well-designed loss function supporting for\nscattering media and strengthened by pseudo-depth maps, UW-GS outperforms\nexisting methods with PSNR gains up to 1.26dB. To fully verify the\neffectiveness of the model, we also developed a new underwater dataset, S-UW,\nwith dynamic object masks.\n", "link": "http://arxiv.org/abs/2410.01517v1", "date": "2024-10-02", "relevancy": 3.274, "topK": [{"title": "RTG-SLAM: Real-time 3D Reconstruction at Scale using Gaussian Splatting", "link": "http://arxiv.org/abs/2404.19706v1", "similarity": 0.6859}, {"title": "GST: Precise 3D Human Body from a Single Image with Gaussian Splatting\n  Transformers", "link": "http://arxiv.org/abs/2409.04196v1", "similarity": 0.6554}, {"title": "Dynamic Gaussians Mesh: Consistent Mesh Reconstruction from Monocular\n  Videos", "link": "http://arxiv.org/abs/2404.12379v2", "similarity": 0.6231}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20UW-GS%3A%20Distractor-Aware%203D%20Gaussian%20Splatting%20for%20Enhanced%20Underwater%0A%20%20Scene%20Reconstruction&body=Title%3A%20UW-GS%3A%20Distractor-Aware%203D%20Gaussian%20Splatting%20for%20Enhanced%20Underwater%0A%20%20Scene%20Reconstruction%0AAuthor%3A%20Haoran%20Wang%20and%20Nantheera%20Anantrasirichai%20and%20Fan%20Zhang%20and%20David%20Bull%0AAbstract%3A%20%20%203D%20Gaussian%20splatting%20%283DGS%29%20offers%20the%20capability%20to%20achieve%20real-time%20high%0Aquality%203D%20scene%20rendering.%20However%2C%203DGS%20assumes%20that%20the%20scene%20is%20in%20a%20clear%0Amedium%20environment%20and%20struggles%20to%20generate%20satisfactory%20representations%20in%0Aunderwater%20scenes%2C%20where%20light%20absorption%20and%20scattering%20are%20prevalent%20and%0Amoving%20objects%20are%20involved.%20To%20overcome%20these%2C%20we%20introduce%20a%20novel%20Gaussian%0ASplatting-based%20method%2C%20UW-GS%2C%20designed%20specifically%20for%20underwater%0Aapplications.%20It%20introduces%20a%20color%20appearance%20that%20models%20distance-dependent%0Acolor%20variation%2C%20employs%20a%20new%20physics-based%20density%20control%20strategy%20to%0Aenhance%20clarity%20for%20distant%20objects%2C%20and%20uses%20a%20binary%20motion%20mask%20to%20handle%0Adynamic%20content.%20Optimized%20with%20a%20well-designed%20loss%20function%20supporting%20for%0Ascattering%20media%20and%20strengthened%20by%20pseudo-depth%20maps%2C%20UW-GS%20outperforms%0Aexisting%20methods%20with%20PSNR%20gains%20up%20to%201.26dB.%20To%20fully%20verify%20the%0Aeffectiveness%20of%20the%20model%2C%20we%20also%20developed%20a%20new%20underwater%20dataset%2C%20S-UW%2C%0Awith%20dynamic%20object%20masks.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2410.01517v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DUW-GS%253A%2520Distractor-Aware%25203D%2520Gaussian%2520Splatting%2520for%2520Enhanced%2520Underwater%250A%2520%2520Scene%2520Reconstruction%26entry.906535625%3DHaoran%2520Wang%2520and%2520Nantheera%2520Anantrasirichai%2520and%2520Fan%2520Zhang%2520and%2520David%2520Bull%26entry.1292438233%3D%2520%25203D%2520Gaussian%2520splatting%2520%25283DGS%2529%2520offers%2520the%2520capability%2520to%2520achieve%2520real-time%2520high%250Aquality%25203D%2520scene%2520rendering.%2520However%252C%25203DGS%2520assumes%2520that%2520the%2520scene%2520is%2520in%2520a%2520clear%250Amedium%2520environment%2520and%2520struggles%2520to%2520generate%2520satisfactory%2520representations%2520in%250Aunderwater%2520scenes%252C%2520where%2520light%2520absorption%2520and%2520scattering%2520are%2520prevalent%2520and%250Amoving%2520objects%2520are%2520involved.%2520To%2520overcome%2520these%252C%2520we%2520introduce%2520a%2520novel%2520Gaussian%250ASplatting-based%2520method%252C%2520UW-GS%252C%2520designed%2520specifically%2520for%2520underwater%250Aapplications.%2520It%2520introduces%2520a%2520color%2520appearance%2520that%2520models%2520distance-dependent%250Acolor%2520variation%252C%2520employs%2520a%2520new%2520physics-based%2520density%2520control%2520strategy%2520to%250Aenhance%2520clarity%2520for%2520distant%2520objects%252C%2520and%2520uses%2520a%2520binary%2520motion%2520mask%2520to%2520handle%250Adynamic%2520content.%2520Optimized%2520with%2520a%2520well-designed%2520loss%2520function%2520supporting%2520for%250Ascattering%2520media%2520and%2520strengthened%2520by%2520pseudo-depth%2520maps%252C%2520UW-GS%2520outperforms%250Aexisting%2520methods%2520with%2520PSNR%2520gains%2520up%2520to%25201.26dB.%2520To%2520fully%2520verify%2520the%250Aeffectiveness%2520of%2520the%2520model%252C%2520we%2520also%2520developed%2520a%2520new%2520underwater%2520dataset%252C%2520S-UW%252C%250Awith%2520dynamic%2520object%2520masks.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2410.01517v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=UW-GS%3A%20Distractor-Aware%203D%20Gaussian%20Splatting%20for%20Enhanced%20Underwater%0A%20%20Scene%20Reconstruction&entry.906535625=Haoran%20Wang%20and%20Nantheera%20Anantrasirichai%20and%20Fan%20Zhang%20and%20David%20Bull&entry.1292438233=%20%203D%20Gaussian%20splatting%20%283DGS%29%20offers%20the%20capability%20to%20achieve%20real-time%20high%0Aquality%203D%20scene%20rendering.%20However%2C%203DGS%20assumes%20that%20the%20scene%20is%20in%20a%20clear%0Amedium%20environment%20and%20struggles%20to%20generate%20satisfactory%20representations%20in%0Aunderwater%20scenes%2C%20where%20light%20absorption%20and%20scattering%20are%20prevalent%20and%0Amoving%20objects%20are%20involved.%20To%20overcome%20these%2C%20we%20introduce%20a%20novel%20Gaussian%0ASplatting-based%20method%2C%20UW-GS%2C%20designed%20specifically%20for%20underwater%0Aapplications.%20It%20introduces%20a%20color%20appearance%20that%20models%20distance-dependent%0Acolor%20variation%2C%20employs%20a%20new%20physics-based%20density%20control%20strategy%20to%0Aenhance%20clarity%20for%20distant%20objects%2C%20and%20uses%20a%20binary%20motion%20mask%20to%20handle%0Adynamic%20content.%20Optimized%20with%20a%20well-designed%20loss%20function%20supporting%20for%0Ascattering%20media%20and%20strengthened%20by%20pseudo-depth%20maps%2C%20UW-GS%20outperforms%0Aexisting%20methods%20with%20PSNR%20gains%20up%20to%201.26dB.%20To%20fully%20verify%20the%0Aeffectiveness%20of%20the%20model%2C%20we%20also%20developed%20a%20new%20underwater%20dataset%2C%20S-UW%2C%0Awith%20dynamic%20object%20masks.%0A&entry.1838667208=http%3A//arxiv.org/abs/2410.01517v1&entry.124074799=Read"},
{"title": "3DGS-DET: Empower 3D Gaussian Splatting with Boundary Guidance and\n  Box-Focused Sampling for 3D Object Detection", "author": "Yang Cao and Yuanliang Jv and Dan Xu", "abstract": "  Neural Radiance Fields (NeRF) are widely used for novel-view synthesis and\nhave been adapted for 3D Object Detection (3DOD), offering a promising approach\nto 3DOD through view-synthesis representation. However, NeRF faces inherent\nlimitations: (i) limited representational capacity for 3DOD due to its implicit\nnature, and (ii) slow rendering speeds. Recently, 3D Gaussian Splatting (3DGS)\nhas emerged as an explicit 3D representation that addresses these limitations.\nInspired by these advantages, this paper introduces 3DGS into 3DOD for the\nfirst time, identifying two main challenges: (i) Ambiguous spatial distribution\nof Gaussian blobs: 3DGS primarily relies on 2D pixel-level supervision,\nresulting in unclear 3D spatial distribution of Gaussian blobs and poor\ndifferentiation between objects and background, which hinders 3DOD; (ii)\nExcessive background blobs: 2D images often include numerous background pixels,\nleading to densely reconstructed 3DGS with many noisy Gaussian blobs\nrepresenting the background, negatively affecting detection. To tackle the\nchallenge (i), we leverage the fact that 3DGS reconstruction is derived from 2D\nimages, and propose an elegant and efficient solution by incorporating 2D\nBoundary Guidance to significantly enhance the spatial distribution of Gaussian\nblobs, resulting in clearer differentiation between objects and their\nbackground. To address the challenge (ii), we propose a Box-Focused Sampling\nstrategy using 2D boxes to generate object probability distribution in 3D\nspaces, allowing effective probabilistic sampling in 3D to retain more object\nblobs and reduce noisy background blobs. Benefiting from our designs, our\n3DGS-DET significantly outperforms the SOTA NeRF-based method, NeRF-Det,\nachieving improvements of +6.6 on mAP@0.25 and +8.1 on mAP@0.5 for the ScanNet\ndataset, and impressive +31.5 on mAP@0.25 for the ARKITScenes dataset.\n", "link": "http://arxiv.org/abs/2410.01647v1", "date": "2024-10-02", "relevancy": 3.2715, "topK": [{"title": "RTG-SLAM: Real-time 3D Reconstruction at Scale using Gaussian Splatting", "link": "http://arxiv.org/abs/2404.19706v1", "similarity": 0.6958}, {"title": "GST: Precise 3D Human Body from a Single Image with Gaussian Splatting\n  Transformers", "link": "http://arxiv.org/abs/2409.04196v1", "similarity": 0.6519}, {"title": "Dynamic Gaussians Mesh: Consistent Mesh Reconstruction from Monocular\n  Videos", "link": "http://arxiv.org/abs/2404.12379v2", "similarity": 0.6152}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%203DGS-DET%3A%20Empower%203D%20Gaussian%20Splatting%20with%20Boundary%20Guidance%20and%0A%20%20Box-Focused%20Sampling%20for%203D%20Object%20Detection&body=Title%3A%203DGS-DET%3A%20Empower%203D%20Gaussian%20Splatting%20with%20Boundary%20Guidance%20and%0A%20%20Box-Focused%20Sampling%20for%203D%20Object%20Detection%0AAuthor%3A%20Yang%20Cao%20and%20Yuanliang%20Jv%20and%20Dan%20Xu%0AAbstract%3A%20%20%20Neural%20Radiance%20Fields%20%28NeRF%29%20are%20widely%20used%20for%20novel-view%20synthesis%20and%0Ahave%20been%20adapted%20for%203D%20Object%20Detection%20%283DOD%29%2C%20offering%20a%20promising%20approach%0Ato%203DOD%20through%20view-synthesis%20representation.%20However%2C%20NeRF%20faces%20inherent%0Alimitations%3A%20%28i%29%20limited%20representational%20capacity%20for%203DOD%20due%20to%20its%20implicit%0Anature%2C%20and%20%28ii%29%20slow%20rendering%20speeds.%20Recently%2C%203D%20Gaussian%20Splatting%20%283DGS%29%0Ahas%20emerged%20as%20an%20explicit%203D%20representation%20that%20addresses%20these%20limitations.%0AInspired%20by%20these%20advantages%2C%20this%20paper%20introduces%203DGS%20into%203DOD%20for%20the%0Afirst%20time%2C%20identifying%20two%20main%20challenges%3A%20%28i%29%20Ambiguous%20spatial%20distribution%0Aof%20Gaussian%20blobs%3A%203DGS%20primarily%20relies%20on%202D%20pixel-level%20supervision%2C%0Aresulting%20in%20unclear%203D%20spatial%20distribution%20of%20Gaussian%20blobs%20and%20poor%0Adifferentiation%20between%20objects%20and%20background%2C%20which%20hinders%203DOD%3B%20%28ii%29%0AExcessive%20background%20blobs%3A%202D%20images%20often%20include%20numerous%20background%20pixels%2C%0Aleading%20to%20densely%20reconstructed%203DGS%20with%20many%20noisy%20Gaussian%20blobs%0Arepresenting%20the%20background%2C%20negatively%20affecting%20detection.%20To%20tackle%20the%0Achallenge%20%28i%29%2C%20we%20leverage%20the%20fact%20that%203DGS%20reconstruction%20is%20derived%20from%202D%0Aimages%2C%20and%20propose%20an%20elegant%20and%20efficient%20solution%20by%20incorporating%202D%0ABoundary%20Guidance%20to%20significantly%20enhance%20the%20spatial%20distribution%20of%20Gaussian%0Ablobs%2C%20resulting%20in%20clearer%20differentiation%20between%20objects%20and%20their%0Abackground.%20To%20address%20the%20challenge%20%28ii%29%2C%20we%20propose%20a%20Box-Focused%20Sampling%0Astrategy%20using%202D%20boxes%20to%20generate%20object%20probability%20distribution%20in%203D%0Aspaces%2C%20allowing%20effective%20probabilistic%20sampling%20in%203D%20to%20retain%20more%20object%0Ablobs%20and%20reduce%20noisy%20background%20blobs.%20Benefiting%20from%20our%20designs%2C%20our%0A3DGS-DET%20significantly%20outperforms%20the%20SOTA%20NeRF-based%20method%2C%20NeRF-Det%2C%0Aachieving%20improvements%20of%20%2B6.6%20on%20mAP%400.25%20and%20%2B8.1%20on%20mAP%400.5%20for%20the%20ScanNet%0Adataset%2C%20and%20impressive%20%2B31.5%20on%20mAP%400.25%20for%20the%20ARKITScenes%20dataset.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2410.01647v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3D3DGS-DET%253A%2520Empower%25203D%2520Gaussian%2520Splatting%2520with%2520Boundary%2520Guidance%2520and%250A%2520%2520Box-Focused%2520Sampling%2520for%25203D%2520Object%2520Detection%26entry.906535625%3DYang%2520Cao%2520and%2520Yuanliang%2520Jv%2520and%2520Dan%2520Xu%26entry.1292438233%3D%2520%2520Neural%2520Radiance%2520Fields%2520%2528NeRF%2529%2520are%2520widely%2520used%2520for%2520novel-view%2520synthesis%2520and%250Ahave%2520been%2520adapted%2520for%25203D%2520Object%2520Detection%2520%25283DOD%2529%252C%2520offering%2520a%2520promising%2520approach%250Ato%25203DOD%2520through%2520view-synthesis%2520representation.%2520However%252C%2520NeRF%2520faces%2520inherent%250Alimitations%253A%2520%2528i%2529%2520limited%2520representational%2520capacity%2520for%25203DOD%2520due%2520to%2520its%2520implicit%250Anature%252C%2520and%2520%2528ii%2529%2520slow%2520rendering%2520speeds.%2520Recently%252C%25203D%2520Gaussian%2520Splatting%2520%25283DGS%2529%250Ahas%2520emerged%2520as%2520an%2520explicit%25203D%2520representation%2520that%2520addresses%2520these%2520limitations.%250AInspired%2520by%2520these%2520advantages%252C%2520this%2520paper%2520introduces%25203DGS%2520into%25203DOD%2520for%2520the%250Afirst%2520time%252C%2520identifying%2520two%2520main%2520challenges%253A%2520%2528i%2529%2520Ambiguous%2520spatial%2520distribution%250Aof%2520Gaussian%2520blobs%253A%25203DGS%2520primarily%2520relies%2520on%25202D%2520pixel-level%2520supervision%252C%250Aresulting%2520in%2520unclear%25203D%2520spatial%2520distribution%2520of%2520Gaussian%2520blobs%2520and%2520poor%250Adifferentiation%2520between%2520objects%2520and%2520background%252C%2520which%2520hinders%25203DOD%253B%2520%2528ii%2529%250AExcessive%2520background%2520blobs%253A%25202D%2520images%2520often%2520include%2520numerous%2520background%2520pixels%252C%250Aleading%2520to%2520densely%2520reconstructed%25203DGS%2520with%2520many%2520noisy%2520Gaussian%2520blobs%250Arepresenting%2520the%2520background%252C%2520negatively%2520affecting%2520detection.%2520To%2520tackle%2520the%250Achallenge%2520%2528i%2529%252C%2520we%2520leverage%2520the%2520fact%2520that%25203DGS%2520reconstruction%2520is%2520derived%2520from%25202D%250Aimages%252C%2520and%2520propose%2520an%2520elegant%2520and%2520efficient%2520solution%2520by%2520incorporating%25202D%250ABoundary%2520Guidance%2520to%2520significantly%2520enhance%2520the%2520spatial%2520distribution%2520of%2520Gaussian%250Ablobs%252C%2520resulting%2520in%2520clearer%2520differentiation%2520between%2520objects%2520and%2520their%250Abackground.%2520To%2520address%2520the%2520challenge%2520%2528ii%2529%252C%2520we%2520propose%2520a%2520Box-Focused%2520Sampling%250Astrategy%2520using%25202D%2520boxes%2520to%2520generate%2520object%2520probability%2520distribution%2520in%25203D%250Aspaces%252C%2520allowing%2520effective%2520probabilistic%2520sampling%2520in%25203D%2520to%2520retain%2520more%2520object%250Ablobs%2520and%2520reduce%2520noisy%2520background%2520blobs.%2520Benefiting%2520from%2520our%2520designs%252C%2520our%250A3DGS-DET%2520significantly%2520outperforms%2520the%2520SOTA%2520NeRF-based%2520method%252C%2520NeRF-Det%252C%250Aachieving%2520improvements%2520of%2520%252B6.6%2520on%2520mAP%25400.25%2520and%2520%252B8.1%2520on%2520mAP%25400.5%2520for%2520the%2520ScanNet%250Adataset%252C%2520and%2520impressive%2520%252B31.5%2520on%2520mAP%25400.25%2520for%2520the%2520ARKITScenes%2520dataset.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2410.01647v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=3DGS-DET%3A%20Empower%203D%20Gaussian%20Splatting%20with%20Boundary%20Guidance%20and%0A%20%20Box-Focused%20Sampling%20for%203D%20Object%20Detection&entry.906535625=Yang%20Cao%20and%20Yuanliang%20Jv%20and%20Dan%20Xu&entry.1292438233=%20%20Neural%20Radiance%20Fields%20%28NeRF%29%20are%20widely%20used%20for%20novel-view%20synthesis%20and%0Ahave%20been%20adapted%20for%203D%20Object%20Detection%20%283DOD%29%2C%20offering%20a%20promising%20approach%0Ato%203DOD%20through%20view-synthesis%20representation.%20However%2C%20NeRF%20faces%20inherent%0Alimitations%3A%20%28i%29%20limited%20representational%20capacity%20for%203DOD%20due%20to%20its%20implicit%0Anature%2C%20and%20%28ii%29%20slow%20rendering%20speeds.%20Recently%2C%203D%20Gaussian%20Splatting%20%283DGS%29%0Ahas%20emerged%20as%20an%20explicit%203D%20representation%20that%20addresses%20these%20limitations.%0AInspired%20by%20these%20advantages%2C%20this%20paper%20introduces%203DGS%20into%203DOD%20for%20the%0Afirst%20time%2C%20identifying%20two%20main%20challenges%3A%20%28i%29%20Ambiguous%20spatial%20distribution%0Aof%20Gaussian%20blobs%3A%203DGS%20primarily%20relies%20on%202D%20pixel-level%20supervision%2C%0Aresulting%20in%20unclear%203D%20spatial%20distribution%20of%20Gaussian%20blobs%20and%20poor%0Adifferentiation%20between%20objects%20and%20background%2C%20which%20hinders%203DOD%3B%20%28ii%29%0AExcessive%20background%20blobs%3A%202D%20images%20often%20include%20numerous%20background%20pixels%2C%0Aleading%20to%20densely%20reconstructed%203DGS%20with%20many%20noisy%20Gaussian%20blobs%0Arepresenting%20the%20background%2C%20negatively%20affecting%20detection.%20To%20tackle%20the%0Achallenge%20%28i%29%2C%20we%20leverage%20the%20fact%20that%203DGS%20reconstruction%20is%20derived%20from%202D%0Aimages%2C%20and%20propose%20an%20elegant%20and%20efficient%20solution%20by%20incorporating%202D%0ABoundary%20Guidance%20to%20significantly%20enhance%20the%20spatial%20distribution%20of%20Gaussian%0Ablobs%2C%20resulting%20in%20clearer%20differentiation%20between%20objects%20and%20their%0Abackground.%20To%20address%20the%20challenge%20%28ii%29%2C%20we%20propose%20a%20Box-Focused%20Sampling%0Astrategy%20using%202D%20boxes%20to%20generate%20object%20probability%20distribution%20in%203D%0Aspaces%2C%20allowing%20effective%20probabilistic%20sampling%20in%203D%20to%20retain%20more%20object%0Ablobs%20and%20reduce%20noisy%20background%20blobs.%20Benefiting%20from%20our%20designs%2C%20our%0A3DGS-DET%20significantly%20outperforms%20the%20SOTA%20NeRF-based%20method%2C%20NeRF-Det%2C%0Aachieving%20improvements%20of%20%2B6.6%20on%20mAP%400.25%20and%20%2B8.1%20on%20mAP%400.5%20for%20the%20ScanNet%0Adataset%2C%20and%20impressive%20%2B31.5%20on%20mAP%400.25%20for%20the%20ARKITScenes%20dataset.%0A&entry.1838667208=http%3A//arxiv.org/abs/2410.01647v1&entry.124074799=Read"},
{"title": "Gaussian-Det: Learning Closed-Surface Gaussians for 3D Object Detection", "author": "Hongru Yan and Yu Zheng and Yueqi Duan", "abstract": "  Skins wrapping around our bodies, leathers covering over the sofa, sheet\nmetal coating the car - it suggests that objects are enclosed by a series of\ncontinuous surfaces, which provides us with informative geometry prior for\nobjectness deduction. In this paper, we propose Gaussian-Det which leverages\nGaussian Splatting as surface representation for multi-view based 3D object\ndetection. Unlike existing monocular or NeRF-based methods which depict the\nobjects via discrete positional data, Gaussian-Det models the objects in a\ncontinuous manner by formulating the input Gaussians as feature descriptors on\na mass of partial surfaces. Furthermore, to address the numerous outliers\ninherently introduced by Gaussian splatting, we accordingly devise a Closure\nInferring Module (CIM) for the comprehensive surface-based objectness\ndeduction. CIM firstly estimates the probabilistic feature residuals for\npartial surfaces given the underdetermined nature of Gaussian Splatting, which\nare then coalesced into a holistic representation on the overall surface\nclosure of the object proposal. In this way, the surface information\nGaussian-Det exploits serves as the prior on the quality and reliability of\nobjectness and the information basis of proposal refinement. Experiments on\nboth synthetic and real-world datasets demonstrate that Gaussian-Det\noutperforms various existing approaches, in terms of both average precision and\nrecall.\n", "link": "http://arxiv.org/abs/2410.01404v1", "date": "2024-10-02", "relevancy": 3.2071, "topK": [{"title": "GST: Precise 3D Human Body from a Single Image with Gaussian Splatting\n  Transformers", "link": "http://arxiv.org/abs/2409.04196v1", "similarity": 0.664}, {"title": "Dynamic Gaussians Mesh: Consistent Mesh Reconstruction from Monocular\n  Videos", "link": "http://arxiv.org/abs/2404.12379v2", "similarity": 0.6354}, {"title": "RTG-SLAM: Real-time 3D Reconstruction at Scale using Gaussian Splatting", "link": "http://arxiv.org/abs/2404.19706v1", "similarity": 0.6248}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Gaussian-Det%3A%20Learning%20Closed-Surface%20Gaussians%20for%203D%20Object%20Detection&body=Title%3A%20Gaussian-Det%3A%20Learning%20Closed-Surface%20Gaussians%20for%203D%20Object%20Detection%0AAuthor%3A%20Hongru%20Yan%20and%20Yu%20Zheng%20and%20Yueqi%20Duan%0AAbstract%3A%20%20%20Skins%20wrapping%20around%20our%20bodies%2C%20leathers%20covering%20over%20the%20sofa%2C%20sheet%0Ametal%20coating%20the%20car%20-%20it%20suggests%20that%20objects%20are%20enclosed%20by%20a%20series%20of%0Acontinuous%20surfaces%2C%20which%20provides%20us%20with%20informative%20geometry%20prior%20for%0Aobjectness%20deduction.%20In%20this%20paper%2C%20we%20propose%20Gaussian-Det%20which%20leverages%0AGaussian%20Splatting%20as%20surface%20representation%20for%20multi-view%20based%203D%20object%0Adetection.%20Unlike%20existing%20monocular%20or%20NeRF-based%20methods%20which%20depict%20the%0Aobjects%20via%20discrete%20positional%20data%2C%20Gaussian-Det%20models%20the%20objects%20in%20a%0Acontinuous%20manner%20by%20formulating%20the%20input%20Gaussians%20as%20feature%20descriptors%20on%0Aa%20mass%20of%20partial%20surfaces.%20Furthermore%2C%20to%20address%20the%20numerous%20outliers%0Ainherently%20introduced%20by%20Gaussian%20splatting%2C%20we%20accordingly%20devise%20a%20Closure%0AInferring%20Module%20%28CIM%29%20for%20the%20comprehensive%20surface-based%20objectness%0Adeduction.%20CIM%20firstly%20estimates%20the%20probabilistic%20feature%20residuals%20for%0Apartial%20surfaces%20given%20the%20underdetermined%20nature%20of%20Gaussian%20Splatting%2C%20which%0Aare%20then%20coalesced%20into%20a%20holistic%20representation%20on%20the%20overall%20surface%0Aclosure%20of%20the%20object%20proposal.%20In%20this%20way%2C%20the%20surface%20information%0AGaussian-Det%20exploits%20serves%20as%20the%20prior%20on%20the%20quality%20and%20reliability%20of%0Aobjectness%20and%20the%20information%20basis%20of%20proposal%20refinement.%20Experiments%20on%0Aboth%20synthetic%20and%20real-world%20datasets%20demonstrate%20that%20Gaussian-Det%0Aoutperforms%20various%20existing%20approaches%2C%20in%20terms%20of%20both%20average%20precision%20and%0Arecall.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2410.01404v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DGaussian-Det%253A%2520Learning%2520Closed-Surface%2520Gaussians%2520for%25203D%2520Object%2520Detection%26entry.906535625%3DHongru%2520Yan%2520and%2520Yu%2520Zheng%2520and%2520Yueqi%2520Duan%26entry.1292438233%3D%2520%2520Skins%2520wrapping%2520around%2520our%2520bodies%252C%2520leathers%2520covering%2520over%2520the%2520sofa%252C%2520sheet%250Ametal%2520coating%2520the%2520car%2520-%2520it%2520suggests%2520that%2520objects%2520are%2520enclosed%2520by%2520a%2520series%2520of%250Acontinuous%2520surfaces%252C%2520which%2520provides%2520us%2520with%2520informative%2520geometry%2520prior%2520for%250Aobjectness%2520deduction.%2520In%2520this%2520paper%252C%2520we%2520propose%2520Gaussian-Det%2520which%2520leverages%250AGaussian%2520Splatting%2520as%2520surface%2520representation%2520for%2520multi-view%2520based%25203D%2520object%250Adetection.%2520Unlike%2520existing%2520monocular%2520or%2520NeRF-based%2520methods%2520which%2520depict%2520the%250Aobjects%2520via%2520discrete%2520positional%2520data%252C%2520Gaussian-Det%2520models%2520the%2520objects%2520in%2520a%250Acontinuous%2520manner%2520by%2520formulating%2520the%2520input%2520Gaussians%2520as%2520feature%2520descriptors%2520on%250Aa%2520mass%2520of%2520partial%2520surfaces.%2520Furthermore%252C%2520to%2520address%2520the%2520numerous%2520outliers%250Ainherently%2520introduced%2520by%2520Gaussian%2520splatting%252C%2520we%2520accordingly%2520devise%2520a%2520Closure%250AInferring%2520Module%2520%2528CIM%2529%2520for%2520the%2520comprehensive%2520surface-based%2520objectness%250Adeduction.%2520CIM%2520firstly%2520estimates%2520the%2520probabilistic%2520feature%2520residuals%2520for%250Apartial%2520surfaces%2520given%2520the%2520underdetermined%2520nature%2520of%2520Gaussian%2520Splatting%252C%2520which%250Aare%2520then%2520coalesced%2520into%2520a%2520holistic%2520representation%2520on%2520the%2520overall%2520surface%250Aclosure%2520of%2520the%2520object%2520proposal.%2520In%2520this%2520way%252C%2520the%2520surface%2520information%250AGaussian-Det%2520exploits%2520serves%2520as%2520the%2520prior%2520on%2520the%2520quality%2520and%2520reliability%2520of%250Aobjectness%2520and%2520the%2520information%2520basis%2520of%2520proposal%2520refinement.%2520Experiments%2520on%250Aboth%2520synthetic%2520and%2520real-world%2520datasets%2520demonstrate%2520that%2520Gaussian-Det%250Aoutperforms%2520various%2520existing%2520approaches%252C%2520in%2520terms%2520of%2520both%2520average%2520precision%2520and%250Arecall.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2410.01404v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Gaussian-Det%3A%20Learning%20Closed-Surface%20Gaussians%20for%203D%20Object%20Detection&entry.906535625=Hongru%20Yan%20and%20Yu%20Zheng%20and%20Yueqi%20Duan&entry.1292438233=%20%20Skins%20wrapping%20around%20our%20bodies%2C%20leathers%20covering%20over%20the%20sofa%2C%20sheet%0Ametal%20coating%20the%20car%20-%20it%20suggests%20that%20objects%20are%20enclosed%20by%20a%20series%20of%0Acontinuous%20surfaces%2C%20which%20provides%20us%20with%20informative%20geometry%20prior%20for%0Aobjectness%20deduction.%20In%20this%20paper%2C%20we%20propose%20Gaussian-Det%20which%20leverages%0AGaussian%20Splatting%20as%20surface%20representation%20for%20multi-view%20based%203D%20object%0Adetection.%20Unlike%20existing%20monocular%20or%20NeRF-based%20methods%20which%20depict%20the%0Aobjects%20via%20discrete%20positional%20data%2C%20Gaussian-Det%20models%20the%20objects%20in%20a%0Acontinuous%20manner%20by%20formulating%20the%20input%20Gaussians%20as%20feature%20descriptors%20on%0Aa%20mass%20of%20partial%20surfaces.%20Furthermore%2C%20to%20address%20the%20numerous%20outliers%0Ainherently%20introduced%20by%20Gaussian%20splatting%2C%20we%20accordingly%20devise%20a%20Closure%0AInferring%20Module%20%28CIM%29%20for%20the%20comprehensive%20surface-based%20objectness%0Adeduction.%20CIM%20firstly%20estimates%20the%20probabilistic%20feature%20residuals%20for%0Apartial%20surfaces%20given%20the%20underdetermined%20nature%20of%20Gaussian%20Splatting%2C%20which%0Aare%20then%20coalesced%20into%20a%20holistic%20representation%20on%20the%20overall%20surface%0Aclosure%20of%20the%20object%20proposal.%20In%20this%20way%2C%20the%20surface%20information%0AGaussian-Det%20exploits%20serves%20as%20the%20prior%20on%20the%20quality%20and%20reliability%20of%0Aobjectness%20and%20the%20information%20basis%20of%20proposal%20refinement.%20Experiments%20on%0Aboth%20synthetic%20and%20real-world%20datasets%20demonstrate%20that%20Gaussian-Det%0Aoutperforms%20various%20existing%20approaches%2C%20in%20terms%20of%20both%20average%20precision%20and%0Arecall.%0A&entry.1838667208=http%3A//arxiv.org/abs/2410.01404v1&entry.124074799=Read"},
{"title": "GaussianBlock: Building Part-Aware Compositional and Editable 3D Scene\n  by Primitives and Gaussians", "author": "Shuyi Jiang and Qihao Zhao and Hossein Rahmani and De Wen Soh and Jun Liu and Na Zhao", "abstract": "  Recently, with the development of Neural Radiance Fields and Gaussian\nSplatting, 3D reconstruction techniques have achieved remarkably high fidelity.\nHowever, the latent representations learnt by these methods are highly\nentangled and lack interpretability. In this paper, we propose a novel\npart-aware compositional reconstruction method, called GaussianBlock, that\nenables semantically coherent and disentangled representations, allowing for\nprecise and physical editing akin to building blocks, while simultaneously\nmaintaining high fidelity. Our GaussianBlock introduces a hybrid representation\nthat leverages the advantages of both primitives, known for their flexible\nactionability and editability, and 3D Gaussians, which excel in reconstruction\nquality. Specifically, we achieve semantically coherent primitives through a\nnovel attention-guided centering loss derived from 2D semantic priors,\ncomplemented by a dynamic splitting and fusion strategy. Furthermore, we\nutilize 3D Gaussians that hybridize with primitives to refine structural\ndetails and enhance fidelity. Additionally, a binding inheritance strategy is\nemployed to strengthen and maintain the connection between the two. Our\nreconstructed scenes are evidenced to be disentangled, compositional, and\ncompact across diverse benchmarks, enabling seamless, direct and precise\nediting while maintaining high quality.\n", "link": "http://arxiv.org/abs/2410.01535v1", "date": "2024-10-02", "relevancy": 3.1221, "topK": [{"title": "Dynamic Gaussians Mesh: Consistent Mesh Reconstruction from Monocular\n  Videos", "link": "http://arxiv.org/abs/2404.12379v2", "similarity": 0.6398}, {"title": "RTG-SLAM: Real-time 3D Reconstruction at Scale using Gaussian Splatting", "link": "http://arxiv.org/abs/2404.19706v1", "similarity": 0.636}, {"title": "GST: Precise 3D Human Body from a Single Image with Gaussian Splatting\n  Transformers", "link": "http://arxiv.org/abs/2409.04196v1", "similarity": 0.5975}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20GaussianBlock%3A%20Building%20Part-Aware%20Compositional%20and%20Editable%203D%20Scene%0A%20%20by%20Primitives%20and%20Gaussians&body=Title%3A%20GaussianBlock%3A%20Building%20Part-Aware%20Compositional%20and%20Editable%203D%20Scene%0A%20%20by%20Primitives%20and%20Gaussians%0AAuthor%3A%20Shuyi%20Jiang%20and%20Qihao%20Zhao%20and%20Hossein%20Rahmani%20and%20De%20Wen%20Soh%20and%20Jun%20Liu%20and%20Na%20Zhao%0AAbstract%3A%20%20%20Recently%2C%20with%20the%20development%20of%20Neural%20Radiance%20Fields%20and%20Gaussian%0ASplatting%2C%203D%20reconstruction%20techniques%20have%20achieved%20remarkably%20high%20fidelity.%0AHowever%2C%20the%20latent%20representations%20learnt%20by%20these%20methods%20are%20highly%0Aentangled%20and%20lack%20interpretability.%20In%20this%20paper%2C%20we%20propose%20a%20novel%0Apart-aware%20compositional%20reconstruction%20method%2C%20called%20GaussianBlock%2C%20that%0Aenables%20semantically%20coherent%20and%20disentangled%20representations%2C%20allowing%20for%0Aprecise%20and%20physical%20editing%20akin%20to%20building%20blocks%2C%20while%20simultaneously%0Amaintaining%20high%20fidelity.%20Our%20GaussianBlock%20introduces%20a%20hybrid%20representation%0Athat%20leverages%20the%20advantages%20of%20both%20primitives%2C%20known%20for%20their%20flexible%0Aactionability%20and%20editability%2C%20and%203D%20Gaussians%2C%20which%20excel%20in%20reconstruction%0Aquality.%20Specifically%2C%20we%20achieve%20semantically%20coherent%20primitives%20through%20a%0Anovel%20attention-guided%20centering%20loss%20derived%20from%202D%20semantic%20priors%2C%0Acomplemented%20by%20a%20dynamic%20splitting%20and%20fusion%20strategy.%20Furthermore%2C%20we%0Autilize%203D%20Gaussians%20that%20hybridize%20with%20primitives%20to%20refine%20structural%0Adetails%20and%20enhance%20fidelity.%20Additionally%2C%20a%20binding%20inheritance%20strategy%20is%0Aemployed%20to%20strengthen%20and%20maintain%20the%20connection%20between%20the%20two.%20Our%0Areconstructed%20scenes%20are%20evidenced%20to%20be%20disentangled%2C%20compositional%2C%20and%0Acompact%20across%20diverse%20benchmarks%2C%20enabling%20seamless%2C%20direct%20and%20precise%0Aediting%20while%20maintaining%20high%20quality.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2410.01535v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DGaussianBlock%253A%2520Building%2520Part-Aware%2520Compositional%2520and%2520Editable%25203D%2520Scene%250A%2520%2520by%2520Primitives%2520and%2520Gaussians%26entry.906535625%3DShuyi%2520Jiang%2520and%2520Qihao%2520Zhao%2520and%2520Hossein%2520Rahmani%2520and%2520De%2520Wen%2520Soh%2520and%2520Jun%2520Liu%2520and%2520Na%2520Zhao%26entry.1292438233%3D%2520%2520Recently%252C%2520with%2520the%2520development%2520of%2520Neural%2520Radiance%2520Fields%2520and%2520Gaussian%250ASplatting%252C%25203D%2520reconstruction%2520techniques%2520have%2520achieved%2520remarkably%2520high%2520fidelity.%250AHowever%252C%2520the%2520latent%2520representations%2520learnt%2520by%2520these%2520methods%2520are%2520highly%250Aentangled%2520and%2520lack%2520interpretability.%2520In%2520this%2520paper%252C%2520we%2520propose%2520a%2520novel%250Apart-aware%2520compositional%2520reconstruction%2520method%252C%2520called%2520GaussianBlock%252C%2520that%250Aenables%2520semantically%2520coherent%2520and%2520disentangled%2520representations%252C%2520allowing%2520for%250Aprecise%2520and%2520physical%2520editing%2520akin%2520to%2520building%2520blocks%252C%2520while%2520simultaneously%250Amaintaining%2520high%2520fidelity.%2520Our%2520GaussianBlock%2520introduces%2520a%2520hybrid%2520representation%250Athat%2520leverages%2520the%2520advantages%2520of%2520both%2520primitives%252C%2520known%2520for%2520their%2520flexible%250Aactionability%2520and%2520editability%252C%2520and%25203D%2520Gaussians%252C%2520which%2520excel%2520in%2520reconstruction%250Aquality.%2520Specifically%252C%2520we%2520achieve%2520semantically%2520coherent%2520primitives%2520through%2520a%250Anovel%2520attention-guided%2520centering%2520loss%2520derived%2520from%25202D%2520semantic%2520priors%252C%250Acomplemented%2520by%2520a%2520dynamic%2520splitting%2520and%2520fusion%2520strategy.%2520Furthermore%252C%2520we%250Autilize%25203D%2520Gaussians%2520that%2520hybridize%2520with%2520primitives%2520to%2520refine%2520structural%250Adetails%2520and%2520enhance%2520fidelity.%2520Additionally%252C%2520a%2520binding%2520inheritance%2520strategy%2520is%250Aemployed%2520to%2520strengthen%2520and%2520maintain%2520the%2520connection%2520between%2520the%2520two.%2520Our%250Areconstructed%2520scenes%2520are%2520evidenced%2520to%2520be%2520disentangled%252C%2520compositional%252C%2520and%250Acompact%2520across%2520diverse%2520benchmarks%252C%2520enabling%2520seamless%252C%2520direct%2520and%2520precise%250Aediting%2520while%2520maintaining%2520high%2520quality.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2410.01535v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=GaussianBlock%3A%20Building%20Part-Aware%20Compositional%20and%20Editable%203D%20Scene%0A%20%20by%20Primitives%20and%20Gaussians&entry.906535625=Shuyi%20Jiang%20and%20Qihao%20Zhao%20and%20Hossein%20Rahmani%20and%20De%20Wen%20Soh%20and%20Jun%20Liu%20and%20Na%20Zhao&entry.1292438233=%20%20Recently%2C%20with%20the%20development%20of%20Neural%20Radiance%20Fields%20and%20Gaussian%0ASplatting%2C%203D%20reconstruction%20techniques%20have%20achieved%20remarkably%20high%20fidelity.%0AHowever%2C%20the%20latent%20representations%20learnt%20by%20these%20methods%20are%20highly%0Aentangled%20and%20lack%20interpretability.%20In%20this%20paper%2C%20we%20propose%20a%20novel%0Apart-aware%20compositional%20reconstruction%20method%2C%20called%20GaussianBlock%2C%20that%0Aenables%20semantically%20coherent%20and%20disentangled%20representations%2C%20allowing%20for%0Aprecise%20and%20physical%20editing%20akin%20to%20building%20blocks%2C%20while%20simultaneously%0Amaintaining%20high%20fidelity.%20Our%20GaussianBlock%20introduces%20a%20hybrid%20representation%0Athat%20leverages%20the%20advantages%20of%20both%20primitives%2C%20known%20for%20their%20flexible%0Aactionability%20and%20editability%2C%20and%203D%20Gaussians%2C%20which%20excel%20in%20reconstruction%0Aquality.%20Specifically%2C%20we%20achieve%20semantically%20coherent%20primitives%20through%20a%0Anovel%20attention-guided%20centering%20loss%20derived%20from%202D%20semantic%20priors%2C%0Acomplemented%20by%20a%20dynamic%20splitting%20and%20fusion%20strategy.%20Furthermore%2C%20we%0Autilize%203D%20Gaussians%20that%20hybridize%20with%20primitives%20to%20refine%20structural%0Adetails%20and%20enhance%20fidelity.%20Additionally%2C%20a%20binding%20inheritance%20strategy%20is%0Aemployed%20to%20strengthen%20and%20maintain%20the%20connection%20between%20the%20two.%20Our%0Areconstructed%20scenes%20are%20evidenced%20to%20be%20disentangled%2C%20compositional%2C%20and%0Acompact%20across%20diverse%20benchmarks%2C%20enabling%20seamless%2C%20direct%20and%20precise%0Aediting%20while%20maintaining%20high%20quality.%0A&entry.1838667208=http%3A//arxiv.org/abs/2410.01535v1&entry.124074799=Read"},
{"title": "LEOPARD : A Vision Language Model For Text-Rich Multi-Image Tasks", "author": "Mengzhao Jia and Wenhao Yu and Kaixin Ma and Tianqing Fang and Zhihan Zhang and Siru Ouyang and Hongming Zhang and Meng Jiang and Dong Yu", "abstract": "  Text-rich images, where text serves as the central visual element guiding the\noverall understanding, are prevalent in real-world applications, such as\npresentation slides, scanned documents, and webpage snapshots. Tasks involving\nmultiple text-rich images are especially challenging, as they require not only\nunderstanding the content of individual images but reasoning about\ninter-relationships and logical flows across multiple visual inputs. Despite\nthe importance of these scenarios, current multimodal large language models\n(MLLMs) struggle to handle such tasks due to two key challenges: (1) the\nscarcity of high-quality instruction tuning datasets for text-rich multi-image\nscenarios, and (2) the difficulty in balancing image resolution with visual\nfeature sequence length. To address these challenges, we propose \\OurMethod, a\nMLLM designed specifically for handling vision-language tasks involving\nmultiple text-rich images. First, we curated about one million high-quality\nmultimodal instruction-tuning data, tailored to text-rich, multi-image\nscenarios. Second, we developed an adaptive high-resolution multi-image\nencoding module to dynamically optimize the allocation of visual sequence\nlength based on the original aspect ratios and resolutions of the input images.\nExperiments across a wide range of benchmarks demonstrate our model's superior\ncapabilities in text-rich, multi-image evaluations and competitive performance\nin general domain evaluations.\n", "link": "http://arxiv.org/abs/2410.01744v1", "date": "2024-10-02", "relevancy": 3.1021, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.6242}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.6242}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.6129}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20LEOPARD%20%3A%20A%20Vision%20Language%20Model%20For%20Text-Rich%20Multi-Image%20Tasks&body=Title%3A%20LEOPARD%20%3A%20A%20Vision%20Language%20Model%20For%20Text-Rich%20Multi-Image%20Tasks%0AAuthor%3A%20Mengzhao%20Jia%20and%20Wenhao%20Yu%20and%20Kaixin%20Ma%20and%20Tianqing%20Fang%20and%20Zhihan%20Zhang%20and%20Siru%20Ouyang%20and%20Hongming%20Zhang%20and%20Meng%20Jiang%20and%20Dong%20Yu%0AAbstract%3A%20%20%20Text-rich%20images%2C%20where%20text%20serves%20as%20the%20central%20visual%20element%20guiding%20the%0Aoverall%20understanding%2C%20are%20prevalent%20in%20real-world%20applications%2C%20such%20as%0Apresentation%20slides%2C%20scanned%20documents%2C%20and%20webpage%20snapshots.%20Tasks%20involving%0Amultiple%20text-rich%20images%20are%20especially%20challenging%2C%20as%20they%20require%20not%20only%0Aunderstanding%20the%20content%20of%20individual%20images%20but%20reasoning%20about%0Ainter-relationships%20and%20logical%20flows%20across%20multiple%20visual%20inputs.%20Despite%0Athe%20importance%20of%20these%20scenarios%2C%20current%20multimodal%20large%20language%20models%0A%28MLLMs%29%20struggle%20to%20handle%20such%20tasks%20due%20to%20two%20key%20challenges%3A%20%281%29%20the%0Ascarcity%20of%20high-quality%20instruction%20tuning%20datasets%20for%20text-rich%20multi-image%0Ascenarios%2C%20and%20%282%29%20the%20difficulty%20in%20balancing%20image%20resolution%20with%20visual%0Afeature%20sequence%20length.%20To%20address%20these%20challenges%2C%20we%20propose%20%5COurMethod%2C%20a%0AMLLM%20designed%20specifically%20for%20handling%20vision-language%20tasks%20involving%0Amultiple%20text-rich%20images.%20First%2C%20we%20curated%20about%20one%20million%20high-quality%0Amultimodal%20instruction-tuning%20data%2C%20tailored%20to%20text-rich%2C%20multi-image%0Ascenarios.%20Second%2C%20we%20developed%20an%20adaptive%20high-resolution%20multi-image%0Aencoding%20module%20to%20dynamically%20optimize%20the%20allocation%20of%20visual%20sequence%0Alength%20based%20on%20the%20original%20aspect%20ratios%20and%20resolutions%20of%20the%20input%20images.%0AExperiments%20across%20a%20wide%20range%20of%20benchmarks%20demonstrate%20our%20model%27s%20superior%0Acapabilities%20in%20text-rich%2C%20multi-image%20evaluations%20and%20competitive%20performance%0Ain%20general%20domain%20evaluations.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2410.01744v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DLEOPARD%2520%253A%2520A%2520Vision%2520Language%2520Model%2520For%2520Text-Rich%2520Multi-Image%2520Tasks%26entry.906535625%3DMengzhao%2520Jia%2520and%2520Wenhao%2520Yu%2520and%2520Kaixin%2520Ma%2520and%2520Tianqing%2520Fang%2520and%2520Zhihan%2520Zhang%2520and%2520Siru%2520Ouyang%2520and%2520Hongming%2520Zhang%2520and%2520Meng%2520Jiang%2520and%2520Dong%2520Yu%26entry.1292438233%3D%2520%2520Text-rich%2520images%252C%2520where%2520text%2520serves%2520as%2520the%2520central%2520visual%2520element%2520guiding%2520the%250Aoverall%2520understanding%252C%2520are%2520prevalent%2520in%2520real-world%2520applications%252C%2520such%2520as%250Apresentation%2520slides%252C%2520scanned%2520documents%252C%2520and%2520webpage%2520snapshots.%2520Tasks%2520involving%250Amultiple%2520text-rich%2520images%2520are%2520especially%2520challenging%252C%2520as%2520they%2520require%2520not%2520only%250Aunderstanding%2520the%2520content%2520of%2520individual%2520images%2520but%2520reasoning%2520about%250Ainter-relationships%2520and%2520logical%2520flows%2520across%2520multiple%2520visual%2520inputs.%2520Despite%250Athe%2520importance%2520of%2520these%2520scenarios%252C%2520current%2520multimodal%2520large%2520language%2520models%250A%2528MLLMs%2529%2520struggle%2520to%2520handle%2520such%2520tasks%2520due%2520to%2520two%2520key%2520challenges%253A%2520%25281%2529%2520the%250Ascarcity%2520of%2520high-quality%2520instruction%2520tuning%2520datasets%2520for%2520text-rich%2520multi-image%250Ascenarios%252C%2520and%2520%25282%2529%2520the%2520difficulty%2520in%2520balancing%2520image%2520resolution%2520with%2520visual%250Afeature%2520sequence%2520length.%2520To%2520address%2520these%2520challenges%252C%2520we%2520propose%2520%255COurMethod%252C%2520a%250AMLLM%2520designed%2520specifically%2520for%2520handling%2520vision-language%2520tasks%2520involving%250Amultiple%2520text-rich%2520images.%2520First%252C%2520we%2520curated%2520about%2520one%2520million%2520high-quality%250Amultimodal%2520instruction-tuning%2520data%252C%2520tailored%2520to%2520text-rich%252C%2520multi-image%250Ascenarios.%2520Second%252C%2520we%2520developed%2520an%2520adaptive%2520high-resolution%2520multi-image%250Aencoding%2520module%2520to%2520dynamically%2520optimize%2520the%2520allocation%2520of%2520visual%2520sequence%250Alength%2520based%2520on%2520the%2520original%2520aspect%2520ratios%2520and%2520resolutions%2520of%2520the%2520input%2520images.%250AExperiments%2520across%2520a%2520wide%2520range%2520of%2520benchmarks%2520demonstrate%2520our%2520model%2527s%2520superior%250Acapabilities%2520in%2520text-rich%252C%2520multi-image%2520evaluations%2520and%2520competitive%2520performance%250Ain%2520general%2520domain%2520evaluations.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2410.01744v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=LEOPARD%20%3A%20A%20Vision%20Language%20Model%20For%20Text-Rich%20Multi-Image%20Tasks&entry.906535625=Mengzhao%20Jia%20and%20Wenhao%20Yu%20and%20Kaixin%20Ma%20and%20Tianqing%20Fang%20and%20Zhihan%20Zhang%20and%20Siru%20Ouyang%20and%20Hongming%20Zhang%20and%20Meng%20Jiang%20and%20Dong%20Yu&entry.1292438233=%20%20Text-rich%20images%2C%20where%20text%20serves%20as%20the%20central%20visual%20element%20guiding%20the%0Aoverall%20understanding%2C%20are%20prevalent%20in%20real-world%20applications%2C%20such%20as%0Apresentation%20slides%2C%20scanned%20documents%2C%20and%20webpage%20snapshots.%20Tasks%20involving%0Amultiple%20text-rich%20images%20are%20especially%20challenging%2C%20as%20they%20require%20not%20only%0Aunderstanding%20the%20content%20of%20individual%20images%20but%20reasoning%20about%0Ainter-relationships%20and%20logical%20flows%20across%20multiple%20visual%20inputs.%20Despite%0Athe%20importance%20of%20these%20scenarios%2C%20current%20multimodal%20large%20language%20models%0A%28MLLMs%29%20struggle%20to%20handle%20such%20tasks%20due%20to%20two%20key%20challenges%3A%20%281%29%20the%0Ascarcity%20of%20high-quality%20instruction%20tuning%20datasets%20for%20text-rich%20multi-image%0Ascenarios%2C%20and%20%282%29%20the%20difficulty%20in%20balancing%20image%20resolution%20with%20visual%0Afeature%20sequence%20length.%20To%20address%20these%20challenges%2C%20we%20propose%20%5COurMethod%2C%20a%0AMLLM%20designed%20specifically%20for%20handling%20vision-language%20tasks%20involving%0Amultiple%20text-rich%20images.%20First%2C%20we%20curated%20about%20one%20million%20high-quality%0Amultimodal%20instruction-tuning%20data%2C%20tailored%20to%20text-rich%2C%20multi-image%0Ascenarios.%20Second%2C%20we%20developed%20an%20adaptive%20high-resolution%20multi-image%0Aencoding%20module%20to%20dynamically%20optimize%20the%20allocation%20of%20visual%20sequence%0Alength%20based%20on%20the%20original%20aspect%20ratios%20and%20resolutions%20of%20the%20input%20images.%0AExperiments%20across%20a%20wide%20range%20of%20benchmarks%20demonstrate%20our%20model%27s%20superior%0Acapabilities%20in%20text-rich%2C%20multi-image%20evaluations%20and%20competitive%20performance%0Ain%20general%20domain%20evaluations.%0A&entry.1838667208=http%3A//arxiv.org/abs/2410.01744v1&entry.124074799=Read"},
{"title": "Gaussian Splatting in Mirrors: Reflection-Aware Rendering via Virtual\n  Camera Optimization", "author": "Zihan Wang and Shuzhe Wang and Matias Turkulainen and Junyuan Fang and Juho Kannala", "abstract": "  Recent advancements in 3D Gaussian Splatting (3D-GS) have revolutionized\nnovel view synthesis, facilitating real-time, high-quality image rendering.\nHowever, in scenarios involving reflective surfaces, particularly mirrors,\n3D-GS often misinterprets reflections as virtual spaces, resulting in blurred\nand inconsistent multi-view rendering within mirrors. Our paper presents a\nnovel method aimed at obtaining high-quality multi-view consistent reflection\nrendering by modelling reflections as physically-based virtual cameras. We\nestimate mirror planes with depth and normal estimates from 3D-GS and define\nvirtual cameras that are placed symmetrically about the mirror plane. These\nvirtual cameras are then used to explain mirror reflections in the scene. To\naddress imperfections in mirror plane estimates, we propose a straightforward\nyet effective virtual camera optimization method to enhance reflection quality.\nWe collect a new mirror dataset including three real-world scenarios for more\ndiverse evaluation. Experimental validation on both Mirror-Nerf and our\nreal-world dataset demonstrate the efficacy of our approach. We achieve\ncomparable or superior results while significantly reducing training time\ncompared to previous state-of-the-art.\n", "link": "http://arxiv.org/abs/2410.01614v1", "date": "2024-10-02", "relevancy": 3.0766, "topK": [{"title": "RTG-SLAM: Real-time 3D Reconstruction at Scale using Gaussian Splatting", "link": "http://arxiv.org/abs/2404.19706v1", "similarity": 0.6494}, {"title": "Dynamic Gaussians Mesh: Consistent Mesh Reconstruction from Monocular\n  Videos", "link": "http://arxiv.org/abs/2404.12379v2", "similarity": 0.6066}, {"title": "GST: Precise 3D Human Body from a Single Image with Gaussian Splatting\n  Transformers", "link": "http://arxiv.org/abs/2409.04196v1", "similarity": 0.5899}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Gaussian%20Splatting%20in%20Mirrors%3A%20Reflection-Aware%20Rendering%20via%20Virtual%0A%20%20Camera%20Optimization&body=Title%3A%20Gaussian%20Splatting%20in%20Mirrors%3A%20Reflection-Aware%20Rendering%20via%20Virtual%0A%20%20Camera%20Optimization%0AAuthor%3A%20Zihan%20Wang%20and%20Shuzhe%20Wang%20and%20Matias%20Turkulainen%20and%20Junyuan%20Fang%20and%20Juho%20Kannala%0AAbstract%3A%20%20%20Recent%20advancements%20in%203D%20Gaussian%20Splatting%20%283D-GS%29%20have%20revolutionized%0Anovel%20view%20synthesis%2C%20facilitating%20real-time%2C%20high-quality%20image%20rendering.%0AHowever%2C%20in%20scenarios%20involving%20reflective%20surfaces%2C%20particularly%20mirrors%2C%0A3D-GS%20often%20misinterprets%20reflections%20as%20virtual%20spaces%2C%20resulting%20in%20blurred%0Aand%20inconsistent%20multi-view%20rendering%20within%20mirrors.%20Our%20paper%20presents%20a%0Anovel%20method%20aimed%20at%20obtaining%20high-quality%20multi-view%20consistent%20reflection%0Arendering%20by%20modelling%20reflections%20as%20physically-based%20virtual%20cameras.%20We%0Aestimate%20mirror%20planes%20with%20depth%20and%20normal%20estimates%20from%203D-GS%20and%20define%0Avirtual%20cameras%20that%20are%20placed%20symmetrically%20about%20the%20mirror%20plane.%20These%0Avirtual%20cameras%20are%20then%20used%20to%20explain%20mirror%20reflections%20in%20the%20scene.%20To%0Aaddress%20imperfections%20in%20mirror%20plane%20estimates%2C%20we%20propose%20a%20straightforward%0Ayet%20effective%20virtual%20camera%20optimization%20method%20to%20enhance%20reflection%20quality.%0AWe%20collect%20a%20new%20mirror%20dataset%20including%20three%20real-world%20scenarios%20for%20more%0Adiverse%20evaluation.%20Experimental%20validation%20on%20both%20Mirror-Nerf%20and%20our%0Areal-world%20dataset%20demonstrate%20the%20efficacy%20of%20our%20approach.%20We%20achieve%0Acomparable%20or%20superior%20results%20while%20significantly%20reducing%20training%20time%0Acompared%20to%20previous%20state-of-the-art.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2410.01614v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DGaussian%2520Splatting%2520in%2520Mirrors%253A%2520Reflection-Aware%2520Rendering%2520via%2520Virtual%250A%2520%2520Camera%2520Optimization%26entry.906535625%3DZihan%2520Wang%2520and%2520Shuzhe%2520Wang%2520and%2520Matias%2520Turkulainen%2520and%2520Junyuan%2520Fang%2520and%2520Juho%2520Kannala%26entry.1292438233%3D%2520%2520Recent%2520advancements%2520in%25203D%2520Gaussian%2520Splatting%2520%25283D-GS%2529%2520have%2520revolutionized%250Anovel%2520view%2520synthesis%252C%2520facilitating%2520real-time%252C%2520high-quality%2520image%2520rendering.%250AHowever%252C%2520in%2520scenarios%2520involving%2520reflective%2520surfaces%252C%2520particularly%2520mirrors%252C%250A3D-GS%2520often%2520misinterprets%2520reflections%2520as%2520virtual%2520spaces%252C%2520resulting%2520in%2520blurred%250Aand%2520inconsistent%2520multi-view%2520rendering%2520within%2520mirrors.%2520Our%2520paper%2520presents%2520a%250Anovel%2520method%2520aimed%2520at%2520obtaining%2520high-quality%2520multi-view%2520consistent%2520reflection%250Arendering%2520by%2520modelling%2520reflections%2520as%2520physically-based%2520virtual%2520cameras.%2520We%250Aestimate%2520mirror%2520planes%2520with%2520depth%2520and%2520normal%2520estimates%2520from%25203D-GS%2520and%2520define%250Avirtual%2520cameras%2520that%2520are%2520placed%2520symmetrically%2520about%2520the%2520mirror%2520plane.%2520These%250Avirtual%2520cameras%2520are%2520then%2520used%2520to%2520explain%2520mirror%2520reflections%2520in%2520the%2520scene.%2520To%250Aaddress%2520imperfections%2520in%2520mirror%2520plane%2520estimates%252C%2520we%2520propose%2520a%2520straightforward%250Ayet%2520effective%2520virtual%2520camera%2520optimization%2520method%2520to%2520enhance%2520reflection%2520quality.%250AWe%2520collect%2520a%2520new%2520mirror%2520dataset%2520including%2520three%2520real-world%2520scenarios%2520for%2520more%250Adiverse%2520evaluation.%2520Experimental%2520validation%2520on%2520both%2520Mirror-Nerf%2520and%2520our%250Areal-world%2520dataset%2520demonstrate%2520the%2520efficacy%2520of%2520our%2520approach.%2520We%2520achieve%250Acomparable%2520or%2520superior%2520results%2520while%2520significantly%2520reducing%2520training%2520time%250Acompared%2520to%2520previous%2520state-of-the-art.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2410.01614v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Gaussian%20Splatting%20in%20Mirrors%3A%20Reflection-Aware%20Rendering%20via%20Virtual%0A%20%20Camera%20Optimization&entry.906535625=Zihan%20Wang%20and%20Shuzhe%20Wang%20and%20Matias%20Turkulainen%20and%20Junyuan%20Fang%20and%20Juho%20Kannala&entry.1292438233=%20%20Recent%20advancements%20in%203D%20Gaussian%20Splatting%20%283D-GS%29%20have%20revolutionized%0Anovel%20view%20synthesis%2C%20facilitating%20real-time%2C%20high-quality%20image%20rendering.%0AHowever%2C%20in%20scenarios%20involving%20reflective%20surfaces%2C%20particularly%20mirrors%2C%0A3D-GS%20often%20misinterprets%20reflections%20as%20virtual%20spaces%2C%20resulting%20in%20blurred%0Aand%20inconsistent%20multi-view%20rendering%20within%20mirrors.%20Our%20paper%20presents%20a%0Anovel%20method%20aimed%20at%20obtaining%20high-quality%20multi-view%20consistent%20reflection%0Arendering%20by%20modelling%20reflections%20as%20physically-based%20virtual%20cameras.%20We%0Aestimate%20mirror%20planes%20with%20depth%20and%20normal%20estimates%20from%203D-GS%20and%20define%0Avirtual%20cameras%20that%20are%20placed%20symmetrically%20about%20the%20mirror%20plane.%20These%0Avirtual%20cameras%20are%20then%20used%20to%20explain%20mirror%20reflections%20in%20the%20scene.%20To%0Aaddress%20imperfections%20in%20mirror%20plane%20estimates%2C%20we%20propose%20a%20straightforward%0Ayet%20effective%20virtual%20camera%20optimization%20method%20to%20enhance%20reflection%20quality.%0AWe%20collect%20a%20new%20mirror%20dataset%20including%20three%20real-world%20scenarios%20for%20more%0Adiverse%20evaluation.%20Experimental%20validation%20on%20both%20Mirror-Nerf%20and%20our%0Areal-world%20dataset%20demonstrate%20the%20efficacy%20of%20our%20approach.%20We%20achieve%0Acomparable%20or%20superior%20results%20while%20significantly%20reducing%20training%20time%0Acompared%20to%20previous%20state-of-the-art.%0A&entry.1838667208=http%3A//arxiv.org/abs/2410.01614v1&entry.124074799=Read"},
{"title": "MiraGe: Editable 2D Images using Gaussian Splatting", "author": "Joanna Waczy\u0144ska and Tomasz Szczepanik and Piotr Borycki and S\u0142awomir Tadeja and Thomas Bohn\u00e9 and Przemys\u0142aw Spurek", "abstract": "  Implicit Neural Representations (INRs) approximate discrete data through\ncontinuous functions and are commonly used for encoding 2D images. Traditional\nimage-based INRs employ neural networks to map pixel coordinates to RGB values,\ncapturing shapes, colors, and textures within the network's weights. Recently,\nGaussianImage has been proposed as an alternative, using Gaussian functions\ninstead of neural networks to achieve comparable quality and compression. Such\na solution obtains a quality and compression ratio similar to classical INR\nmodels but does not allow image modification. In contrast, our work introduces\na novel method, MiraGe, which uses mirror reflections to perceive 2D images in\n3D space and employs flat-controlled Gaussians for precise 2D image editing.\nOur approach improves the rendering quality and allows realistic image\nmodifications, including human-inspired perception of photos in the 3D world.\nThanks to modeling images in 3D space, we obtain the illusion of 3D-based\nmodification in 2D images. We also show that our Gaussian representation can be\neasily combined with a physics engine to produce physics-based modification of\n2D images. Consequently, MiraGe allows for better quality than the standard\napproach and natural modification of 2D images.\n", "link": "http://arxiv.org/abs/2410.01521v1", "date": "2024-10-02", "relevancy": 3.065, "topK": [{"title": "RTG-SLAM: Real-time 3D Reconstruction at Scale using Gaussian Splatting", "link": "http://arxiv.org/abs/2404.19706v1", "similarity": 0.6201}, {"title": "GST: Precise 3D Human Body from a Single Image with Gaussian Splatting\n  Transformers", "link": "http://arxiv.org/abs/2409.04196v1", "similarity": 0.6154}, {"title": "Dynamic Gaussians Mesh: Consistent Mesh Reconstruction from Monocular\n  Videos", "link": "http://arxiv.org/abs/2404.12379v2", "similarity": 0.6036}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20MiraGe%3A%20Editable%202D%20Images%20using%20Gaussian%20Splatting&body=Title%3A%20MiraGe%3A%20Editable%202D%20Images%20using%20Gaussian%20Splatting%0AAuthor%3A%20Joanna%20Waczy%C5%84ska%20and%20Tomasz%20Szczepanik%20and%20Piotr%20Borycki%20and%20S%C5%82awomir%20Tadeja%20and%20Thomas%20Bohn%C3%A9%20and%20Przemys%C5%82aw%20Spurek%0AAbstract%3A%20%20%20Implicit%20Neural%20Representations%20%28INRs%29%20approximate%20discrete%20data%20through%0Acontinuous%20functions%20and%20are%20commonly%20used%20for%20encoding%202D%20images.%20Traditional%0Aimage-based%20INRs%20employ%20neural%20networks%20to%20map%20pixel%20coordinates%20to%20RGB%20values%2C%0Acapturing%20shapes%2C%20colors%2C%20and%20textures%20within%20the%20network%27s%20weights.%20Recently%2C%0AGaussianImage%20has%20been%20proposed%20as%20an%20alternative%2C%20using%20Gaussian%20functions%0Ainstead%20of%20neural%20networks%20to%20achieve%20comparable%20quality%20and%20compression.%20Such%0Aa%20solution%20obtains%20a%20quality%20and%20compression%20ratio%20similar%20to%20classical%20INR%0Amodels%20but%20does%20not%20allow%20image%20modification.%20In%20contrast%2C%20our%20work%20introduces%0Aa%20novel%20method%2C%20MiraGe%2C%20which%20uses%20mirror%20reflections%20to%20perceive%202D%20images%20in%0A3D%20space%20and%20employs%20flat-controlled%20Gaussians%20for%20precise%202D%20image%20editing.%0AOur%20approach%20improves%20the%20rendering%20quality%20and%20allows%20realistic%20image%0Amodifications%2C%20including%20human-inspired%20perception%20of%20photos%20in%20the%203D%20world.%0AThanks%20to%20modeling%20images%20in%203D%20space%2C%20we%20obtain%20the%20illusion%20of%203D-based%0Amodification%20in%202D%20images.%20We%20also%20show%20that%20our%20Gaussian%20representation%20can%20be%0Aeasily%20combined%20with%20a%20physics%20engine%20to%20produce%20physics-based%20modification%20of%0A2D%20images.%20Consequently%2C%20MiraGe%20allows%20for%20better%20quality%20than%20the%20standard%0Aapproach%20and%20natural%20modification%20of%202D%20images.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2410.01521v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DMiraGe%253A%2520Editable%25202D%2520Images%2520using%2520Gaussian%2520Splatting%26entry.906535625%3DJoanna%2520Waczy%25C5%2584ska%2520and%2520Tomasz%2520Szczepanik%2520and%2520Piotr%2520Borycki%2520and%2520S%25C5%2582awomir%2520Tadeja%2520and%2520Thomas%2520Bohn%25C3%25A9%2520and%2520Przemys%25C5%2582aw%2520Spurek%26entry.1292438233%3D%2520%2520Implicit%2520Neural%2520Representations%2520%2528INRs%2529%2520approximate%2520discrete%2520data%2520through%250Acontinuous%2520functions%2520and%2520are%2520commonly%2520used%2520for%2520encoding%25202D%2520images.%2520Traditional%250Aimage-based%2520INRs%2520employ%2520neural%2520networks%2520to%2520map%2520pixel%2520coordinates%2520to%2520RGB%2520values%252C%250Acapturing%2520shapes%252C%2520colors%252C%2520and%2520textures%2520within%2520the%2520network%2527s%2520weights.%2520Recently%252C%250AGaussianImage%2520has%2520been%2520proposed%2520as%2520an%2520alternative%252C%2520using%2520Gaussian%2520functions%250Ainstead%2520of%2520neural%2520networks%2520to%2520achieve%2520comparable%2520quality%2520and%2520compression.%2520Such%250Aa%2520solution%2520obtains%2520a%2520quality%2520and%2520compression%2520ratio%2520similar%2520to%2520classical%2520INR%250Amodels%2520but%2520does%2520not%2520allow%2520image%2520modification.%2520In%2520contrast%252C%2520our%2520work%2520introduces%250Aa%2520novel%2520method%252C%2520MiraGe%252C%2520which%2520uses%2520mirror%2520reflections%2520to%2520perceive%25202D%2520images%2520in%250A3D%2520space%2520and%2520employs%2520flat-controlled%2520Gaussians%2520for%2520precise%25202D%2520image%2520editing.%250AOur%2520approach%2520improves%2520the%2520rendering%2520quality%2520and%2520allows%2520realistic%2520image%250Amodifications%252C%2520including%2520human-inspired%2520perception%2520of%2520photos%2520in%2520the%25203D%2520world.%250AThanks%2520to%2520modeling%2520images%2520in%25203D%2520space%252C%2520we%2520obtain%2520the%2520illusion%2520of%25203D-based%250Amodification%2520in%25202D%2520images.%2520We%2520also%2520show%2520that%2520our%2520Gaussian%2520representation%2520can%2520be%250Aeasily%2520combined%2520with%2520a%2520physics%2520engine%2520to%2520produce%2520physics-based%2520modification%2520of%250A2D%2520images.%2520Consequently%252C%2520MiraGe%2520allows%2520for%2520better%2520quality%2520than%2520the%2520standard%250Aapproach%2520and%2520natural%2520modification%2520of%25202D%2520images.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2410.01521v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=MiraGe%3A%20Editable%202D%20Images%20using%20Gaussian%20Splatting&entry.906535625=Joanna%20Waczy%C5%84ska%20and%20Tomasz%20Szczepanik%20and%20Piotr%20Borycki%20and%20S%C5%82awomir%20Tadeja%20and%20Thomas%20Bohn%C3%A9%20and%20Przemys%C5%82aw%20Spurek&entry.1292438233=%20%20Implicit%20Neural%20Representations%20%28INRs%29%20approximate%20discrete%20data%20through%0Acontinuous%20functions%20and%20are%20commonly%20used%20for%20encoding%202D%20images.%20Traditional%0Aimage-based%20INRs%20employ%20neural%20networks%20to%20map%20pixel%20coordinates%20to%20RGB%20values%2C%0Acapturing%20shapes%2C%20colors%2C%20and%20textures%20within%20the%20network%27s%20weights.%20Recently%2C%0AGaussianImage%20has%20been%20proposed%20as%20an%20alternative%2C%20using%20Gaussian%20functions%0Ainstead%20of%20neural%20networks%20to%20achieve%20comparable%20quality%20and%20compression.%20Such%0Aa%20solution%20obtains%20a%20quality%20and%20compression%20ratio%20similar%20to%20classical%20INR%0Amodels%20but%20does%20not%20allow%20image%20modification.%20In%20contrast%2C%20our%20work%20introduces%0Aa%20novel%20method%2C%20MiraGe%2C%20which%20uses%20mirror%20reflections%20to%20perceive%202D%20images%20in%0A3D%20space%20and%20employs%20flat-controlled%20Gaussians%20for%20precise%202D%20image%20editing.%0AOur%20approach%20improves%20the%20rendering%20quality%20and%20allows%20realistic%20image%0Amodifications%2C%20including%20human-inspired%20perception%20of%20photos%20in%20the%203D%20world.%0AThanks%20to%20modeling%20images%20in%203D%20space%2C%20we%20obtain%20the%20illusion%20of%203D-based%0Amodification%20in%202D%20images.%20We%20also%20show%20that%20our%20Gaussian%20representation%20can%20be%0Aeasily%20combined%20with%20a%20physics%20engine%20to%20produce%20physics-based%20modification%20of%0A2D%20images.%20Consequently%2C%20MiraGe%20allows%20for%20better%20quality%20than%20the%20standard%0Aapproach%20and%20natural%20modification%20of%202D%20images.%0A&entry.1838667208=http%3A//arxiv.org/abs/2410.01521v1&entry.124074799=Read"},
{"title": "ArtVLM: Attribute Recognition Through Vision-Based Prefix Language\n  Modeling", "author": "William Yicheng Zhu and Keren Ye and Junjie Ke and Jiahui Yu and Leonidas Guibas and Peyman Milanfar and Feng Yang", "abstract": "  Recognizing and disentangling visual attributes from objects is a foundation\nto many computer vision applications. While large vision language\nrepresentations like CLIP had largely resolved the task of zero-shot object\nrecognition, zero-shot visual attribute recognition remains a challenge because\nCLIP's contrastively-learned vision-language representation cannot effectively\ncapture object-attribute dependencies. In this paper, we target this weakness\nand propose a sentence generation-based retrieval formulation for attribute\nrecognition that is novel in 1) explicitly modeling a to-be-measured and\nretrieved object-attribute relation as a conditional probability graph, which\nconverts the recognition problem into a dependency-sensitive language-modeling\nproblem, and 2) applying a large pretrained Vision-Language Model (VLM) on this\nreformulation and naturally distilling its knowledge of image-object-attribute\nrelations to use towards attribute recognition. Specifically, for each\nattribute to be recognized on an image, we measure the visual-conditioned\nprobability of generating a short sentence encoding the attribute's relation to\nobjects on the image. Unlike contrastive retrieval, which measures likelihood\nby globally aligning elements of the sentence to the image, generative\nretrieval is sensitive to the order and dependency of objects and attributes in\nthe sentence. We demonstrate through experiments that generative retrieval\nconsistently outperforms contrastive retrieval on two visual reasoning\ndatasets, Visual Attribute in the Wild (VAW), and our newly-proposed Visual\nGenome Attribute Ranking (VGARank).\n", "link": "http://arxiv.org/abs/2408.04102v3", "date": "2024-10-02", "relevancy": 2.9954, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.6069}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.6069}, {"title": "Flexible Visual Recognition by Evidential Modeling of Confusion and Ignorance", "link": "http://openaccess.thecvf.com/content/ICCV2023/html/Fan_Flexible_Visual_Recognition_by_Evidential_Modeling_of_Confusion_and_Ignorance_ICCV_2023_paper.html", "similarity": 0.5835}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20ArtVLM%3A%20Attribute%20Recognition%20Through%20Vision-Based%20Prefix%20Language%0A%20%20Modeling&body=Title%3A%20ArtVLM%3A%20Attribute%20Recognition%20Through%20Vision-Based%20Prefix%20Language%0A%20%20Modeling%0AAuthor%3A%20William%20Yicheng%20Zhu%20and%20Keren%20Ye%20and%20Junjie%20Ke%20and%20Jiahui%20Yu%20and%20Leonidas%20Guibas%20and%20Peyman%20Milanfar%20and%20Feng%20Yang%0AAbstract%3A%20%20%20Recognizing%20and%20disentangling%20visual%20attributes%20from%20objects%20is%20a%20foundation%0Ato%20many%20computer%20vision%20applications.%20While%20large%20vision%20language%0Arepresentations%20like%20CLIP%20had%20largely%20resolved%20the%20task%20of%20zero-shot%20object%0Arecognition%2C%20zero-shot%20visual%20attribute%20recognition%20remains%20a%20challenge%20because%0ACLIP%27s%20contrastively-learned%20vision-language%20representation%20cannot%20effectively%0Acapture%20object-attribute%20dependencies.%20In%20this%20paper%2C%20we%20target%20this%20weakness%0Aand%20propose%20a%20sentence%20generation-based%20retrieval%20formulation%20for%20attribute%0Arecognition%20that%20is%20novel%20in%201%29%20explicitly%20modeling%20a%20to-be-measured%20and%0Aretrieved%20object-attribute%20relation%20as%20a%20conditional%20probability%20graph%2C%20which%0Aconverts%20the%20recognition%20problem%20into%20a%20dependency-sensitive%20language-modeling%0Aproblem%2C%20and%202%29%20applying%20a%20large%20pretrained%20Vision-Language%20Model%20%28VLM%29%20on%20this%0Areformulation%20and%20naturally%20distilling%20its%20knowledge%20of%20image-object-attribute%0Arelations%20to%20use%20towards%20attribute%20recognition.%20Specifically%2C%20for%20each%0Aattribute%20to%20be%20recognized%20on%20an%20image%2C%20we%20measure%20the%20visual-conditioned%0Aprobability%20of%20generating%20a%20short%20sentence%20encoding%20the%20attribute%27s%20relation%20to%0Aobjects%20on%20the%20image.%20Unlike%20contrastive%20retrieval%2C%20which%20measures%20likelihood%0Aby%20globally%20aligning%20elements%20of%20the%20sentence%20to%20the%20image%2C%20generative%0Aretrieval%20is%20sensitive%20to%20the%20order%20and%20dependency%20of%20objects%20and%20attributes%20in%0Athe%20sentence.%20We%20demonstrate%20through%20experiments%20that%20generative%20retrieval%0Aconsistently%20outperforms%20contrastive%20retrieval%20on%20two%20visual%20reasoning%0Adatasets%2C%20Visual%20Attribute%20in%20the%20Wild%20%28VAW%29%2C%20and%20our%20newly-proposed%20Visual%0AGenome%20Attribute%20Ranking%20%28VGARank%29.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2408.04102v3%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DArtVLM%253A%2520Attribute%2520Recognition%2520Through%2520Vision-Based%2520Prefix%2520Language%250A%2520%2520Modeling%26entry.906535625%3DWilliam%2520Yicheng%2520Zhu%2520and%2520Keren%2520Ye%2520and%2520Junjie%2520Ke%2520and%2520Jiahui%2520Yu%2520and%2520Leonidas%2520Guibas%2520and%2520Peyman%2520Milanfar%2520and%2520Feng%2520Yang%26entry.1292438233%3D%2520%2520Recognizing%2520and%2520disentangling%2520visual%2520attributes%2520from%2520objects%2520is%2520a%2520foundation%250Ato%2520many%2520computer%2520vision%2520applications.%2520While%2520large%2520vision%2520language%250Arepresentations%2520like%2520CLIP%2520had%2520largely%2520resolved%2520the%2520task%2520of%2520zero-shot%2520object%250Arecognition%252C%2520zero-shot%2520visual%2520attribute%2520recognition%2520remains%2520a%2520challenge%2520because%250ACLIP%2527s%2520contrastively-learned%2520vision-language%2520representation%2520cannot%2520effectively%250Acapture%2520object-attribute%2520dependencies.%2520In%2520this%2520paper%252C%2520we%2520target%2520this%2520weakness%250Aand%2520propose%2520a%2520sentence%2520generation-based%2520retrieval%2520formulation%2520for%2520attribute%250Arecognition%2520that%2520is%2520novel%2520in%25201%2529%2520explicitly%2520modeling%2520a%2520to-be-measured%2520and%250Aretrieved%2520object-attribute%2520relation%2520as%2520a%2520conditional%2520probability%2520graph%252C%2520which%250Aconverts%2520the%2520recognition%2520problem%2520into%2520a%2520dependency-sensitive%2520language-modeling%250Aproblem%252C%2520and%25202%2529%2520applying%2520a%2520large%2520pretrained%2520Vision-Language%2520Model%2520%2528VLM%2529%2520on%2520this%250Areformulation%2520and%2520naturally%2520distilling%2520its%2520knowledge%2520of%2520image-object-attribute%250Arelations%2520to%2520use%2520towards%2520attribute%2520recognition.%2520Specifically%252C%2520for%2520each%250Aattribute%2520to%2520be%2520recognized%2520on%2520an%2520image%252C%2520we%2520measure%2520the%2520visual-conditioned%250Aprobability%2520of%2520generating%2520a%2520short%2520sentence%2520encoding%2520the%2520attribute%2527s%2520relation%2520to%250Aobjects%2520on%2520the%2520image.%2520Unlike%2520contrastive%2520retrieval%252C%2520which%2520measures%2520likelihood%250Aby%2520globally%2520aligning%2520elements%2520of%2520the%2520sentence%2520to%2520the%2520image%252C%2520generative%250Aretrieval%2520is%2520sensitive%2520to%2520the%2520order%2520and%2520dependency%2520of%2520objects%2520and%2520attributes%2520in%250Athe%2520sentence.%2520We%2520demonstrate%2520through%2520experiments%2520that%2520generative%2520retrieval%250Aconsistently%2520outperforms%2520contrastive%2520retrieval%2520on%2520two%2520visual%2520reasoning%250Adatasets%252C%2520Visual%2520Attribute%2520in%2520the%2520Wild%2520%2528VAW%2529%252C%2520and%2520our%2520newly-proposed%2520Visual%250AGenome%2520Attribute%2520Ranking%2520%2528VGARank%2529.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2408.04102v3%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=ArtVLM%3A%20Attribute%20Recognition%20Through%20Vision-Based%20Prefix%20Language%0A%20%20Modeling&entry.906535625=William%20Yicheng%20Zhu%20and%20Keren%20Ye%20and%20Junjie%20Ke%20and%20Jiahui%20Yu%20and%20Leonidas%20Guibas%20and%20Peyman%20Milanfar%20and%20Feng%20Yang&entry.1292438233=%20%20Recognizing%20and%20disentangling%20visual%20attributes%20from%20objects%20is%20a%20foundation%0Ato%20many%20computer%20vision%20applications.%20While%20large%20vision%20language%0Arepresentations%20like%20CLIP%20had%20largely%20resolved%20the%20task%20of%20zero-shot%20object%0Arecognition%2C%20zero-shot%20visual%20attribute%20recognition%20remains%20a%20challenge%20because%0ACLIP%27s%20contrastively-learned%20vision-language%20representation%20cannot%20effectively%0Acapture%20object-attribute%20dependencies.%20In%20this%20paper%2C%20we%20target%20this%20weakness%0Aand%20propose%20a%20sentence%20generation-based%20retrieval%20formulation%20for%20attribute%0Arecognition%20that%20is%20novel%20in%201%29%20explicitly%20modeling%20a%20to-be-measured%20and%0Aretrieved%20object-attribute%20relation%20as%20a%20conditional%20probability%20graph%2C%20which%0Aconverts%20the%20recognition%20problem%20into%20a%20dependency-sensitive%20language-modeling%0Aproblem%2C%20and%202%29%20applying%20a%20large%20pretrained%20Vision-Language%20Model%20%28VLM%29%20on%20this%0Areformulation%20and%20naturally%20distilling%20its%20knowledge%20of%20image-object-attribute%0Arelations%20to%20use%20towards%20attribute%20recognition.%20Specifically%2C%20for%20each%0Aattribute%20to%20be%20recognized%20on%20an%20image%2C%20we%20measure%20the%20visual-conditioned%0Aprobability%20of%20generating%20a%20short%20sentence%20encoding%20the%20attribute%27s%20relation%20to%0Aobjects%20on%20the%20image.%20Unlike%20contrastive%20retrieval%2C%20which%20measures%20likelihood%0Aby%20globally%20aligning%20elements%20of%20the%20sentence%20to%20the%20image%2C%20generative%0Aretrieval%20is%20sensitive%20to%20the%20order%20and%20dependency%20of%20objects%20and%20attributes%20in%0Athe%20sentence.%20We%20demonstrate%20through%20experiments%20that%20generative%20retrieval%0Aconsistently%20outperforms%20contrastive%20retrieval%20on%20two%20visual%20reasoning%0Adatasets%2C%20Visual%20Attribute%20in%20the%20Wild%20%28VAW%29%2C%20and%20our%20newly-proposed%20Visual%0AGenome%20Attribute%20Ranking%20%28VGARank%29.%0A&entry.1838667208=http%3A//arxiv.org/abs/2408.04102v3&entry.124074799=Read"},
{"title": "3D Uncertain Implicit Surface Mapping using GMM and GP", "author": "Qianqian Zou and Monika Sester", "abstract": "  In this study, we address the challenge of constructing continuous\nthree-dimensional (3D) models that accurately represent uncertain surfaces,\nderived from noisy and incomplete LiDAR scanning data. Building upon our prior\nwork, which utilized the Gaussian Process (GP) and Gaussian Mixture Model (GMM)\nfor structured building models, we introduce a more generalized approach\ntailored for complex surfaces in urban scenes, where GMM Regression and GP with\nderivative observations are applied. A Hierarchical GMM (HGMM) is employed to\noptimize the number of GMM components and speed up the GMM training. With the\nprior map obtained from HGMM, GP inference is followed for the refinement of\nthe final map. Our approach models the implicit surface of the geo-object and\nenables the inference of the regions that are not completely covered by\nmeasurements. The integration of GMM and GP yields well-calibrated\nuncertainties alongside the surface model, enhancing both accuracy and\nreliability. The proposed method is evaluated on real data collected by a\nmobile mapping system. Compared to the performance in mapping accuracy and\nuncertainty quantification of other state-of-the-art methods, the proposed\nmethod achieves lower RMSEs, higher log-likelihood values and lower\ncomputational costs for the evaluated datasets.\n", "link": "http://arxiv.org/abs/2403.07223v4", "date": "2024-10-02", "relevancy": 2.9786, "topK": [{"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.6157}, {"title": "RTG-SLAM: Real-time 3D Reconstruction at Scale using Gaussian Splatting", "link": "http://arxiv.org/abs/2404.19706v1", "similarity": 0.5911}, {"title": "GST: Precise 3D Human Body from a Single Image with Gaussian Splatting\n  Transformers", "link": "http://arxiv.org/abs/2409.04196v1", "similarity": 0.5804}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%203D%20Uncertain%20Implicit%20Surface%20Mapping%20using%20GMM%20and%20GP&body=Title%3A%203D%20Uncertain%20Implicit%20Surface%20Mapping%20using%20GMM%20and%20GP%0AAuthor%3A%20Qianqian%20Zou%20and%20Monika%20Sester%0AAbstract%3A%20%20%20In%20this%20study%2C%20we%20address%20the%20challenge%20of%20constructing%20continuous%0Athree-dimensional%20%283D%29%20models%20that%20accurately%20represent%20uncertain%20surfaces%2C%0Aderived%20from%20noisy%20and%20incomplete%20LiDAR%20scanning%20data.%20Building%20upon%20our%20prior%0Awork%2C%20which%20utilized%20the%20Gaussian%20Process%20%28GP%29%20and%20Gaussian%20Mixture%20Model%20%28GMM%29%0Afor%20structured%20building%20models%2C%20we%20introduce%20a%20more%20generalized%20approach%0Atailored%20for%20complex%20surfaces%20in%20urban%20scenes%2C%20where%20GMM%20Regression%20and%20GP%20with%0Aderivative%20observations%20are%20applied.%20A%20Hierarchical%20GMM%20%28HGMM%29%20is%20employed%20to%0Aoptimize%20the%20number%20of%20GMM%20components%20and%20speed%20up%20the%20GMM%20training.%20With%20the%0Aprior%20map%20obtained%20from%20HGMM%2C%20GP%20inference%20is%20followed%20for%20the%20refinement%20of%0Athe%20final%20map.%20Our%20approach%20models%20the%20implicit%20surface%20of%20the%20geo-object%20and%0Aenables%20the%20inference%20of%20the%20regions%20that%20are%20not%20completely%20covered%20by%0Ameasurements.%20The%20integration%20of%20GMM%20and%20GP%20yields%20well-calibrated%0Auncertainties%20alongside%20the%20surface%20model%2C%20enhancing%20both%20accuracy%20and%0Areliability.%20The%20proposed%20method%20is%20evaluated%20on%20real%20data%20collected%20by%20a%0Amobile%20mapping%20system.%20Compared%20to%20the%20performance%20in%20mapping%20accuracy%20and%0Auncertainty%20quantification%20of%20other%20state-of-the-art%20methods%2C%20the%20proposed%0Amethod%20achieves%20lower%20RMSEs%2C%20higher%20log-likelihood%20values%20and%20lower%0Acomputational%20costs%20for%20the%20evaluated%20datasets.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2403.07223v4%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3D3D%2520Uncertain%2520Implicit%2520Surface%2520Mapping%2520using%2520GMM%2520and%2520GP%26entry.906535625%3DQianqian%2520Zou%2520and%2520Monika%2520Sester%26entry.1292438233%3D%2520%2520In%2520this%2520study%252C%2520we%2520address%2520the%2520challenge%2520of%2520constructing%2520continuous%250Athree-dimensional%2520%25283D%2529%2520models%2520that%2520accurately%2520represent%2520uncertain%2520surfaces%252C%250Aderived%2520from%2520noisy%2520and%2520incomplete%2520LiDAR%2520scanning%2520data.%2520Building%2520upon%2520our%2520prior%250Awork%252C%2520which%2520utilized%2520the%2520Gaussian%2520Process%2520%2528GP%2529%2520and%2520Gaussian%2520Mixture%2520Model%2520%2528GMM%2529%250Afor%2520structured%2520building%2520models%252C%2520we%2520introduce%2520a%2520more%2520generalized%2520approach%250Atailored%2520for%2520complex%2520surfaces%2520in%2520urban%2520scenes%252C%2520where%2520GMM%2520Regression%2520and%2520GP%2520with%250Aderivative%2520observations%2520are%2520applied.%2520A%2520Hierarchical%2520GMM%2520%2528HGMM%2529%2520is%2520employed%2520to%250Aoptimize%2520the%2520number%2520of%2520GMM%2520components%2520and%2520speed%2520up%2520the%2520GMM%2520training.%2520With%2520the%250Aprior%2520map%2520obtained%2520from%2520HGMM%252C%2520GP%2520inference%2520is%2520followed%2520for%2520the%2520refinement%2520of%250Athe%2520final%2520map.%2520Our%2520approach%2520models%2520the%2520implicit%2520surface%2520of%2520the%2520geo-object%2520and%250Aenables%2520the%2520inference%2520of%2520the%2520regions%2520that%2520are%2520not%2520completely%2520covered%2520by%250Ameasurements.%2520The%2520integration%2520of%2520GMM%2520and%2520GP%2520yields%2520well-calibrated%250Auncertainties%2520alongside%2520the%2520surface%2520model%252C%2520enhancing%2520both%2520accuracy%2520and%250Areliability.%2520The%2520proposed%2520method%2520is%2520evaluated%2520on%2520real%2520data%2520collected%2520by%2520a%250Amobile%2520mapping%2520system.%2520Compared%2520to%2520the%2520performance%2520in%2520mapping%2520accuracy%2520and%250Auncertainty%2520quantification%2520of%2520other%2520state-of-the-art%2520methods%252C%2520the%2520proposed%250Amethod%2520achieves%2520lower%2520RMSEs%252C%2520higher%2520log-likelihood%2520values%2520and%2520lower%250Acomputational%2520costs%2520for%2520the%2520evaluated%2520datasets.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2403.07223v4%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=3D%20Uncertain%20Implicit%20Surface%20Mapping%20using%20GMM%20and%20GP&entry.906535625=Qianqian%20Zou%20and%20Monika%20Sester&entry.1292438233=%20%20In%20this%20study%2C%20we%20address%20the%20challenge%20of%20constructing%20continuous%0Athree-dimensional%20%283D%29%20models%20that%20accurately%20represent%20uncertain%20surfaces%2C%0Aderived%20from%20noisy%20and%20incomplete%20LiDAR%20scanning%20data.%20Building%20upon%20our%20prior%0Awork%2C%20which%20utilized%20the%20Gaussian%20Process%20%28GP%29%20and%20Gaussian%20Mixture%20Model%20%28GMM%29%0Afor%20structured%20building%20models%2C%20we%20introduce%20a%20more%20generalized%20approach%0Atailored%20for%20complex%20surfaces%20in%20urban%20scenes%2C%20where%20GMM%20Regression%20and%20GP%20with%0Aderivative%20observations%20are%20applied.%20A%20Hierarchical%20GMM%20%28HGMM%29%20is%20employed%20to%0Aoptimize%20the%20number%20of%20GMM%20components%20and%20speed%20up%20the%20GMM%20training.%20With%20the%0Aprior%20map%20obtained%20from%20HGMM%2C%20GP%20inference%20is%20followed%20for%20the%20refinement%20of%0Athe%20final%20map.%20Our%20approach%20models%20the%20implicit%20surface%20of%20the%20geo-object%20and%0Aenables%20the%20inference%20of%20the%20regions%20that%20are%20not%20completely%20covered%20by%0Ameasurements.%20The%20integration%20of%20GMM%20and%20GP%20yields%20well-calibrated%0Auncertainties%20alongside%20the%20surface%20model%2C%20enhancing%20both%20accuracy%20and%0Areliability.%20The%20proposed%20method%20is%20evaluated%20on%20real%20data%20collected%20by%20a%0Amobile%20mapping%20system.%20Compared%20to%20the%20performance%20in%20mapping%20accuracy%20and%0Auncertainty%20quantification%20of%20other%20state-of-the-art%20methods%2C%20the%20proposed%0Amethod%20achieves%20lower%20RMSEs%2C%20higher%20log-likelihood%20values%20and%20lower%0Acomputational%20costs%20for%20the%20evaluated%20datasets.%0A&entry.1838667208=http%3A//arxiv.org/abs/2403.07223v4&entry.124074799=Read"},
{"title": "Toward a Holistic Evaluation of Robustness in CLIP Models", "author": "Weijie Tu and Weijian Deng and Tom Gedeon", "abstract": "  Contrastive Language-Image Pre-training (CLIP) models have shown significant\npotential, particularly in zero-shot classification across diverse distribution\nshifts. Building on existing evaluations of overall classification robustness,\nthis work aims to provide a more comprehensive assessment of CLIP by\nintroducing several new perspectives. First, we investigate their robustness to\nvariations in specific visual factors. Second, we assess two critical safety\nobjectives--confidence uncertainty and out-of-distribution detection--beyond\nmere classification accuracy. Third, we evaluate the finesse with which CLIP\nmodels bridge the image and text modalities. Fourth, we extend our examination\nto 3D awareness in CLIP models, moving beyond traditional 2D image\nunderstanding. Finally, we explore the interaction between vision and language\nencoders within modern large multimodal models (LMMs) that utilize CLIP as the\nvisual backbone, focusing on how this interaction impacts classification\nrobustness. In each aspect, we consider the impact of six factors on CLIP\nmodels: model architecture, training distribution, training set size,\nfine-tuning, contrastive loss, and test-time prompts. Our study uncovers\nseveral previously unknown insights into CLIP. For instance, the architecture\nof the visual encoder in CLIP plays a significant role in their robustness\nagainst 3D corruption. CLIP models tend to exhibit a bias towards shape when\nmaking predictions. Moreover, this bias tends to diminish after fine-tuning on\nImageNet. Vision-language models like LLaVA, leveraging the CLIP vision\nencoder, could exhibit benefits in classification performance for challenging\ncategories over CLIP alone. Our findings are poised to offer valuable guidance\nfor enhancing the robustness and reliability of CLIP models.\n", "link": "http://arxiv.org/abs/2410.01534v1", "date": "2024-10-02", "relevancy": 2.9737, "topK": [{"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5969}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5937}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5937}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Toward%20a%20Holistic%20Evaluation%20of%20Robustness%20in%20CLIP%20Models&body=Title%3A%20Toward%20a%20Holistic%20Evaluation%20of%20Robustness%20in%20CLIP%20Models%0AAuthor%3A%20Weijie%20Tu%20and%20Weijian%20Deng%20and%20Tom%20Gedeon%0AAbstract%3A%20%20%20Contrastive%20Language-Image%20Pre-training%20%28CLIP%29%20models%20have%20shown%20significant%0Apotential%2C%20particularly%20in%20zero-shot%20classification%20across%20diverse%20distribution%0Ashifts.%20Building%20on%20existing%20evaluations%20of%20overall%20classification%20robustness%2C%0Athis%20work%20aims%20to%20provide%20a%20more%20comprehensive%20assessment%20of%20CLIP%20by%0Aintroducing%20several%20new%20perspectives.%20First%2C%20we%20investigate%20their%20robustness%20to%0Avariations%20in%20specific%20visual%20factors.%20Second%2C%20we%20assess%20two%20critical%20safety%0Aobjectives--confidence%20uncertainty%20and%20out-of-distribution%20detection--beyond%0Amere%20classification%20accuracy.%20Third%2C%20we%20evaluate%20the%20finesse%20with%20which%20CLIP%0Amodels%20bridge%20the%20image%20and%20text%20modalities.%20Fourth%2C%20we%20extend%20our%20examination%0Ato%203D%20awareness%20in%20CLIP%20models%2C%20moving%20beyond%20traditional%202D%20image%0Aunderstanding.%20Finally%2C%20we%20explore%20the%20interaction%20between%20vision%20and%20language%0Aencoders%20within%20modern%20large%20multimodal%20models%20%28LMMs%29%20that%20utilize%20CLIP%20as%20the%0Avisual%20backbone%2C%20focusing%20on%20how%20this%20interaction%20impacts%20classification%0Arobustness.%20In%20each%20aspect%2C%20we%20consider%20the%20impact%20of%20six%20factors%20on%20CLIP%0Amodels%3A%20model%20architecture%2C%20training%20distribution%2C%20training%20set%20size%2C%0Afine-tuning%2C%20contrastive%20loss%2C%20and%20test-time%20prompts.%20Our%20study%20uncovers%0Aseveral%20previously%20unknown%20insights%20into%20CLIP.%20For%20instance%2C%20the%20architecture%0Aof%20the%20visual%20encoder%20in%20CLIP%20plays%20a%20significant%20role%20in%20their%20robustness%0Aagainst%203D%20corruption.%20CLIP%20models%20tend%20to%20exhibit%20a%20bias%20towards%20shape%20when%0Amaking%20predictions.%20Moreover%2C%20this%20bias%20tends%20to%20diminish%20after%20fine-tuning%20on%0AImageNet.%20Vision-language%20models%20like%20LLaVA%2C%20leveraging%20the%20CLIP%20vision%0Aencoder%2C%20could%20exhibit%20benefits%20in%20classification%20performance%20for%20challenging%0Acategories%20over%20CLIP%20alone.%20Our%20findings%20are%20poised%20to%20offer%20valuable%20guidance%0Afor%20enhancing%20the%20robustness%20and%20reliability%20of%20CLIP%20models.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2410.01534v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DToward%2520a%2520Holistic%2520Evaluation%2520of%2520Robustness%2520in%2520CLIP%2520Models%26entry.906535625%3DWeijie%2520Tu%2520and%2520Weijian%2520Deng%2520and%2520Tom%2520Gedeon%26entry.1292438233%3D%2520%2520Contrastive%2520Language-Image%2520Pre-training%2520%2528CLIP%2529%2520models%2520have%2520shown%2520significant%250Apotential%252C%2520particularly%2520in%2520zero-shot%2520classification%2520across%2520diverse%2520distribution%250Ashifts.%2520Building%2520on%2520existing%2520evaluations%2520of%2520overall%2520classification%2520robustness%252C%250Athis%2520work%2520aims%2520to%2520provide%2520a%2520more%2520comprehensive%2520assessment%2520of%2520CLIP%2520by%250Aintroducing%2520several%2520new%2520perspectives.%2520First%252C%2520we%2520investigate%2520their%2520robustness%2520to%250Avariations%2520in%2520specific%2520visual%2520factors.%2520Second%252C%2520we%2520assess%2520two%2520critical%2520safety%250Aobjectives--confidence%2520uncertainty%2520and%2520out-of-distribution%2520detection--beyond%250Amere%2520classification%2520accuracy.%2520Third%252C%2520we%2520evaluate%2520the%2520finesse%2520with%2520which%2520CLIP%250Amodels%2520bridge%2520the%2520image%2520and%2520text%2520modalities.%2520Fourth%252C%2520we%2520extend%2520our%2520examination%250Ato%25203D%2520awareness%2520in%2520CLIP%2520models%252C%2520moving%2520beyond%2520traditional%25202D%2520image%250Aunderstanding.%2520Finally%252C%2520we%2520explore%2520the%2520interaction%2520between%2520vision%2520and%2520language%250Aencoders%2520within%2520modern%2520large%2520multimodal%2520models%2520%2528LMMs%2529%2520that%2520utilize%2520CLIP%2520as%2520the%250Avisual%2520backbone%252C%2520focusing%2520on%2520how%2520this%2520interaction%2520impacts%2520classification%250Arobustness.%2520In%2520each%2520aspect%252C%2520we%2520consider%2520the%2520impact%2520of%2520six%2520factors%2520on%2520CLIP%250Amodels%253A%2520model%2520architecture%252C%2520training%2520distribution%252C%2520training%2520set%2520size%252C%250Afine-tuning%252C%2520contrastive%2520loss%252C%2520and%2520test-time%2520prompts.%2520Our%2520study%2520uncovers%250Aseveral%2520previously%2520unknown%2520insights%2520into%2520CLIP.%2520For%2520instance%252C%2520the%2520architecture%250Aof%2520the%2520visual%2520encoder%2520in%2520CLIP%2520plays%2520a%2520significant%2520role%2520in%2520their%2520robustness%250Aagainst%25203D%2520corruption.%2520CLIP%2520models%2520tend%2520to%2520exhibit%2520a%2520bias%2520towards%2520shape%2520when%250Amaking%2520predictions.%2520Moreover%252C%2520this%2520bias%2520tends%2520to%2520diminish%2520after%2520fine-tuning%2520on%250AImageNet.%2520Vision-language%2520models%2520like%2520LLaVA%252C%2520leveraging%2520the%2520CLIP%2520vision%250Aencoder%252C%2520could%2520exhibit%2520benefits%2520in%2520classification%2520performance%2520for%2520challenging%250Acategories%2520over%2520CLIP%2520alone.%2520Our%2520findings%2520are%2520poised%2520to%2520offer%2520valuable%2520guidance%250Afor%2520enhancing%2520the%2520robustness%2520and%2520reliability%2520of%2520CLIP%2520models.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2410.01534v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Toward%20a%20Holistic%20Evaluation%20of%20Robustness%20in%20CLIP%20Models&entry.906535625=Weijie%20Tu%20and%20Weijian%20Deng%20and%20Tom%20Gedeon&entry.1292438233=%20%20Contrastive%20Language-Image%20Pre-training%20%28CLIP%29%20models%20have%20shown%20significant%0Apotential%2C%20particularly%20in%20zero-shot%20classification%20across%20diverse%20distribution%0Ashifts.%20Building%20on%20existing%20evaluations%20of%20overall%20classification%20robustness%2C%0Athis%20work%20aims%20to%20provide%20a%20more%20comprehensive%20assessment%20of%20CLIP%20by%0Aintroducing%20several%20new%20perspectives.%20First%2C%20we%20investigate%20their%20robustness%20to%0Avariations%20in%20specific%20visual%20factors.%20Second%2C%20we%20assess%20two%20critical%20safety%0Aobjectives--confidence%20uncertainty%20and%20out-of-distribution%20detection--beyond%0Amere%20classification%20accuracy.%20Third%2C%20we%20evaluate%20the%20finesse%20with%20which%20CLIP%0Amodels%20bridge%20the%20image%20and%20text%20modalities.%20Fourth%2C%20we%20extend%20our%20examination%0Ato%203D%20awareness%20in%20CLIP%20models%2C%20moving%20beyond%20traditional%202D%20image%0Aunderstanding.%20Finally%2C%20we%20explore%20the%20interaction%20between%20vision%20and%20language%0Aencoders%20within%20modern%20large%20multimodal%20models%20%28LMMs%29%20that%20utilize%20CLIP%20as%20the%0Avisual%20backbone%2C%20focusing%20on%20how%20this%20interaction%20impacts%20classification%0Arobustness.%20In%20each%20aspect%2C%20we%20consider%20the%20impact%20of%20six%20factors%20on%20CLIP%0Amodels%3A%20model%20architecture%2C%20training%20distribution%2C%20training%20set%20size%2C%0Afine-tuning%2C%20contrastive%20loss%2C%20and%20test-time%20prompts.%20Our%20study%20uncovers%0Aseveral%20previously%20unknown%20insights%20into%20CLIP.%20For%20instance%2C%20the%20architecture%0Aof%20the%20visual%20encoder%20in%20CLIP%20plays%20a%20significant%20role%20in%20their%20robustness%0Aagainst%203D%20corruption.%20CLIP%20models%20tend%20to%20exhibit%20a%20bias%20towards%20shape%20when%0Amaking%20predictions.%20Moreover%2C%20this%20bias%20tends%20to%20diminish%20after%20fine-tuning%20on%0AImageNet.%20Vision-language%20models%20like%20LLaVA%2C%20leveraging%20the%20CLIP%20vision%0Aencoder%2C%20could%20exhibit%20benefits%20in%20classification%20performance%20for%20challenging%0Acategories%20over%20CLIP%20alone.%20Our%20findings%20are%20poised%20to%20offer%20valuable%20guidance%0Afor%20enhancing%20the%20robustness%20and%20reliability%20of%20CLIP%20models.%0A&entry.1838667208=http%3A//arxiv.org/abs/2410.01534v1&entry.124074799=Read"},
{"title": "3DSAM-adapter: Holistic adaptation of SAM from 2D to 3D for promptable\n  tumor segmentation", "author": "Shizhan Gong and Yuan Zhong and Wenao Ma and Jinpeng Li and Zhao Wang and Jingyang Zhang and Pheng-Ann Heng and Qi Dou", "abstract": "  Despite that the segment anything model (SAM) achieved impressive results on\ngeneral-purpose semantic segmentation with strong generalization ability on\ndaily images, its demonstrated performance on medical image segmentation is\nless precise and not stable, especially when dealing with tumor segmentation\ntasks that involve objects of small sizes, irregular shapes, and low contrast.\nNotably, the original SAM architecture is designed for 2D natural images,\ntherefore would not be able to extract the 3D spatial information from\nvolumetric medical data effectively. In this paper, we propose a novel\nadaptation method for transferring SAM from 2D to 3D for promptable medical\nimage segmentation. Through a holistically designed scheme for architecture\nmodification, we transfer the SAM to support volumetric inputs while retaining\nthe majority of its pre-trained parameters for reuse. The fine-tuning process\nis conducted in a parameter-efficient manner, wherein most of the pre-trained\nparameters remain frozen, and only a few lightweight spatial adapters are\nintroduced and tuned. Regardless of the domain gap between natural and medical\ndata and the disparity in the spatial arrangement between 2D and 3D, the\ntransformer trained on natural images can effectively capture the spatial\npatterns present in volumetric medical images with only lightweight\nadaptations. We conduct experiments on four open-source tumor segmentation\ndatasets, and with a single click prompt, our model can outperform domain\nstate-of-the-art medical image segmentation models on 3 out of 4 tasks,\nspecifically by 8.25%, 29.87%, and 10.11% for kidney tumor, pancreas tumor,\ncolon cancer segmentation, and achieve similar performance for liver tumor\nsegmentation. We also compare our adaptation method with existing popular\nadapters, and observed significant performance improvement on most datasets.\n", "link": "http://arxiv.org/abs/2306.13465v2", "date": "2024-10-02", "relevancy": 2.9561, "topK": [{"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.651}, {"title": "GST: Precise 3D Human Body from a Single Image with Gaussian Splatting\n  Transformers", "link": "http://arxiv.org/abs/2409.04196v1", "similarity": 0.5742}, {"title": "CAT3D: Create Anything in 3D with Multi-View Diffusion Models", "link": "http://arxiv.org/abs/2405.10314v1", "similarity": 0.5485}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%203DSAM-adapter%3A%20Holistic%20adaptation%20of%20SAM%20from%202D%20to%203D%20for%20promptable%0A%20%20tumor%20segmentation&body=Title%3A%203DSAM-adapter%3A%20Holistic%20adaptation%20of%20SAM%20from%202D%20to%203D%20for%20promptable%0A%20%20tumor%20segmentation%0AAuthor%3A%20Shizhan%20Gong%20and%20Yuan%20Zhong%20and%20Wenao%20Ma%20and%20Jinpeng%20Li%20and%20Zhao%20Wang%20and%20Jingyang%20Zhang%20and%20Pheng-Ann%20Heng%20and%20Qi%20Dou%0AAbstract%3A%20%20%20Despite%20that%20the%20segment%20anything%20model%20%28SAM%29%20achieved%20impressive%20results%20on%0Ageneral-purpose%20semantic%20segmentation%20with%20strong%20generalization%20ability%20on%0Adaily%20images%2C%20its%20demonstrated%20performance%20on%20medical%20image%20segmentation%20is%0Aless%20precise%20and%20not%20stable%2C%20especially%20when%20dealing%20with%20tumor%20segmentation%0Atasks%20that%20involve%20objects%20of%20small%20sizes%2C%20irregular%20shapes%2C%20and%20low%20contrast.%0ANotably%2C%20the%20original%20SAM%20architecture%20is%20designed%20for%202D%20natural%20images%2C%0Atherefore%20would%20not%20be%20able%20to%20extract%20the%203D%20spatial%20information%20from%0Avolumetric%20medical%20data%20effectively.%20In%20this%20paper%2C%20we%20propose%20a%20novel%0Aadaptation%20method%20for%20transferring%20SAM%20from%202D%20to%203D%20for%20promptable%20medical%0Aimage%20segmentation.%20Through%20a%20holistically%20designed%20scheme%20for%20architecture%0Amodification%2C%20we%20transfer%20the%20SAM%20to%20support%20volumetric%20inputs%20while%20retaining%0Athe%20majority%20of%20its%20pre-trained%20parameters%20for%20reuse.%20The%20fine-tuning%20process%0Ais%20conducted%20in%20a%20parameter-efficient%20manner%2C%20wherein%20most%20of%20the%20pre-trained%0Aparameters%20remain%20frozen%2C%20and%20only%20a%20few%20lightweight%20spatial%20adapters%20are%0Aintroduced%20and%20tuned.%20Regardless%20of%20the%20domain%20gap%20between%20natural%20and%20medical%0Adata%20and%20the%20disparity%20in%20the%20spatial%20arrangement%20between%202D%20and%203D%2C%20the%0Atransformer%20trained%20on%20natural%20images%20can%20effectively%20capture%20the%20spatial%0Apatterns%20present%20in%20volumetric%20medical%20images%20with%20only%20lightweight%0Aadaptations.%20We%20conduct%20experiments%20on%20four%20open-source%20tumor%20segmentation%0Adatasets%2C%20and%20with%20a%20single%20click%20prompt%2C%20our%20model%20can%20outperform%20domain%0Astate-of-the-art%20medical%20image%20segmentation%20models%20on%203%20out%20of%204%20tasks%2C%0Aspecifically%20by%208.25%25%2C%2029.87%25%2C%20and%2010.11%25%20for%20kidney%20tumor%2C%20pancreas%20tumor%2C%0Acolon%20cancer%20segmentation%2C%20and%20achieve%20similar%20performance%20for%20liver%20tumor%0Asegmentation.%20We%20also%20compare%20our%20adaptation%20method%20with%20existing%20popular%0Aadapters%2C%20and%20observed%20significant%20performance%20improvement%20on%20most%20datasets.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2306.13465v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3D3DSAM-adapter%253A%2520Holistic%2520adaptation%2520of%2520SAM%2520from%25202D%2520to%25203D%2520for%2520promptable%250A%2520%2520tumor%2520segmentation%26entry.906535625%3DShizhan%2520Gong%2520and%2520Yuan%2520Zhong%2520and%2520Wenao%2520Ma%2520and%2520Jinpeng%2520Li%2520and%2520Zhao%2520Wang%2520and%2520Jingyang%2520Zhang%2520and%2520Pheng-Ann%2520Heng%2520and%2520Qi%2520Dou%26entry.1292438233%3D%2520%2520Despite%2520that%2520the%2520segment%2520anything%2520model%2520%2528SAM%2529%2520achieved%2520impressive%2520results%2520on%250Ageneral-purpose%2520semantic%2520segmentation%2520with%2520strong%2520generalization%2520ability%2520on%250Adaily%2520images%252C%2520its%2520demonstrated%2520performance%2520on%2520medical%2520image%2520segmentation%2520is%250Aless%2520precise%2520and%2520not%2520stable%252C%2520especially%2520when%2520dealing%2520with%2520tumor%2520segmentation%250Atasks%2520that%2520involve%2520objects%2520of%2520small%2520sizes%252C%2520irregular%2520shapes%252C%2520and%2520low%2520contrast.%250ANotably%252C%2520the%2520original%2520SAM%2520architecture%2520is%2520designed%2520for%25202D%2520natural%2520images%252C%250Atherefore%2520would%2520not%2520be%2520able%2520to%2520extract%2520the%25203D%2520spatial%2520information%2520from%250Avolumetric%2520medical%2520data%2520effectively.%2520In%2520this%2520paper%252C%2520we%2520propose%2520a%2520novel%250Aadaptation%2520method%2520for%2520transferring%2520SAM%2520from%25202D%2520to%25203D%2520for%2520promptable%2520medical%250Aimage%2520segmentation.%2520Through%2520a%2520holistically%2520designed%2520scheme%2520for%2520architecture%250Amodification%252C%2520we%2520transfer%2520the%2520SAM%2520to%2520support%2520volumetric%2520inputs%2520while%2520retaining%250Athe%2520majority%2520of%2520its%2520pre-trained%2520parameters%2520for%2520reuse.%2520The%2520fine-tuning%2520process%250Ais%2520conducted%2520in%2520a%2520parameter-efficient%2520manner%252C%2520wherein%2520most%2520of%2520the%2520pre-trained%250Aparameters%2520remain%2520frozen%252C%2520and%2520only%2520a%2520few%2520lightweight%2520spatial%2520adapters%2520are%250Aintroduced%2520and%2520tuned.%2520Regardless%2520of%2520the%2520domain%2520gap%2520between%2520natural%2520and%2520medical%250Adata%2520and%2520the%2520disparity%2520in%2520the%2520spatial%2520arrangement%2520between%25202D%2520and%25203D%252C%2520the%250Atransformer%2520trained%2520on%2520natural%2520images%2520can%2520effectively%2520capture%2520the%2520spatial%250Apatterns%2520present%2520in%2520volumetric%2520medical%2520images%2520with%2520only%2520lightweight%250Aadaptations.%2520We%2520conduct%2520experiments%2520on%2520four%2520open-source%2520tumor%2520segmentation%250Adatasets%252C%2520and%2520with%2520a%2520single%2520click%2520prompt%252C%2520our%2520model%2520can%2520outperform%2520domain%250Astate-of-the-art%2520medical%2520image%2520segmentation%2520models%2520on%25203%2520out%2520of%25204%2520tasks%252C%250Aspecifically%2520by%25208.25%2525%252C%252029.87%2525%252C%2520and%252010.11%2525%2520for%2520kidney%2520tumor%252C%2520pancreas%2520tumor%252C%250Acolon%2520cancer%2520segmentation%252C%2520and%2520achieve%2520similar%2520performance%2520for%2520liver%2520tumor%250Asegmentation.%2520We%2520also%2520compare%2520our%2520adaptation%2520method%2520with%2520existing%2520popular%250Aadapters%252C%2520and%2520observed%2520significant%2520performance%2520improvement%2520on%2520most%2520datasets.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2306.13465v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=3DSAM-adapter%3A%20Holistic%20adaptation%20of%20SAM%20from%202D%20to%203D%20for%20promptable%0A%20%20tumor%20segmentation&entry.906535625=Shizhan%20Gong%20and%20Yuan%20Zhong%20and%20Wenao%20Ma%20and%20Jinpeng%20Li%20and%20Zhao%20Wang%20and%20Jingyang%20Zhang%20and%20Pheng-Ann%20Heng%20and%20Qi%20Dou&entry.1292438233=%20%20Despite%20that%20the%20segment%20anything%20model%20%28SAM%29%20achieved%20impressive%20results%20on%0Ageneral-purpose%20semantic%20segmentation%20with%20strong%20generalization%20ability%20on%0Adaily%20images%2C%20its%20demonstrated%20performance%20on%20medical%20image%20segmentation%20is%0Aless%20precise%20and%20not%20stable%2C%20especially%20when%20dealing%20with%20tumor%20segmentation%0Atasks%20that%20involve%20objects%20of%20small%20sizes%2C%20irregular%20shapes%2C%20and%20low%20contrast.%0ANotably%2C%20the%20original%20SAM%20architecture%20is%20designed%20for%202D%20natural%20images%2C%0Atherefore%20would%20not%20be%20able%20to%20extract%20the%203D%20spatial%20information%20from%0Avolumetric%20medical%20data%20effectively.%20In%20this%20paper%2C%20we%20propose%20a%20novel%0Aadaptation%20method%20for%20transferring%20SAM%20from%202D%20to%203D%20for%20promptable%20medical%0Aimage%20segmentation.%20Through%20a%20holistically%20designed%20scheme%20for%20architecture%0Amodification%2C%20we%20transfer%20the%20SAM%20to%20support%20volumetric%20inputs%20while%20retaining%0Athe%20majority%20of%20its%20pre-trained%20parameters%20for%20reuse.%20The%20fine-tuning%20process%0Ais%20conducted%20in%20a%20parameter-efficient%20manner%2C%20wherein%20most%20of%20the%20pre-trained%0Aparameters%20remain%20frozen%2C%20and%20only%20a%20few%20lightweight%20spatial%20adapters%20are%0Aintroduced%20and%20tuned.%20Regardless%20of%20the%20domain%20gap%20between%20natural%20and%20medical%0Adata%20and%20the%20disparity%20in%20the%20spatial%20arrangement%20between%202D%20and%203D%2C%20the%0Atransformer%20trained%20on%20natural%20images%20can%20effectively%20capture%20the%20spatial%0Apatterns%20present%20in%20volumetric%20medical%20images%20with%20only%20lightweight%0Aadaptations.%20We%20conduct%20experiments%20on%20four%20open-source%20tumor%20segmentation%0Adatasets%2C%20and%20with%20a%20single%20click%20prompt%2C%20our%20model%20can%20outperform%20domain%0Astate-of-the-art%20medical%20image%20segmentation%20models%20on%203%20out%20of%204%20tasks%2C%0Aspecifically%20by%208.25%25%2C%2029.87%25%2C%20and%2010.11%25%20for%20kidney%20tumor%2C%20pancreas%20tumor%2C%0Acolon%20cancer%20segmentation%2C%20and%20achieve%20similar%20performance%20for%20liver%20tumor%0Asegmentation.%20We%20also%20compare%20our%20adaptation%20method%20with%20existing%20popular%0Aadapters%2C%20and%20observed%20significant%20performance%20improvement%20on%20most%20datasets.%0A&entry.1838667208=http%3A//arxiv.org/abs/2306.13465v2&entry.124074799=Read"},
{"title": "SurgPointTransformer: Vertebrae Shape Completion with RGB-D Data", "author": "Aidana Massalimova and Florentin Liebmann and Sascha Jecklin and Fabio Carrillo and Farshad Mazda and Philipp F\u00fcrnstahl", "abstract": "  State-of-the-art computer- and robot-assisted surgery systems heavily depend\non intraoperative imaging technologies such as CT and fluoroscopy to generate\ndetailed 3D visualization of the patient's anatomy. While imaging techniques\nare highly accurate, they are based on ionizing radiation and expose patients\nand clinicians. This study introduces an alternative, radiation-free approach\nfor reconstructing the 3D spine anatomy using RGB-D data. Drawing inspiration\nfrom the 3D \"mental map\" that surgeons form during surgeries, we introduce\nSurgPointTransformer, a shape completion approach for surgical applications\nthat can accurately reconstruct the unexposed spine regions from sparse\nobservations of the exposed surface.\n  Our method involves two main steps: segmentation and shape completion. The\nsegmentation step includes spinal column localization and segmentation,\nfollowed by vertebra-wise segmentation. The segmented vertebra point clouds are\nthen subjected to SurgPointTransformer, which leverages an attention mechanism\nto learn patterns between visible surface features and the underlying anatomy.\nFor evaluation, we utilize an ex-vivo dataset of nine specimens. Their CT data\nis used to establish ground truth data that were used to compare to the outputs\nof our methods. Our method significantly outperforms the state-of-the-art\nbaselines, achieving an average Chamfer Distance of 5.39, an F-Score of 0.85,\nan Earth Mover's Distance of 0.011, and a Signal-to-Noise Ratio of 22.90 dB.\n  This study demonstrates the potential of our reconstruction method for 3D\nvertebral shape completion. It enables 3D reconstruction of the entire lumbar\nspine and surgical guidance without ionizing radiation or invasive imaging. Our\nwork contributes to computer-aided and robot-assisted surgery, advancing the\nperception and intelligence of these systems.\n", "link": "http://arxiv.org/abs/2410.01443v1", "date": "2024-10-02", "relevancy": 2.9484, "topK": [{"title": "GST: Precise 3D Human Body from a Single Image with Gaussian Splatting\n  Transformers", "link": "http://arxiv.org/abs/2409.04196v1", "similarity": 0.6267}, {"title": "RTG-SLAM: Real-time 3D Reconstruction at Scale using Gaussian Splatting", "link": "http://arxiv.org/abs/2404.19706v1", "similarity": 0.5902}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5521}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20SurgPointTransformer%3A%20Vertebrae%20Shape%20Completion%20with%20RGB-D%20Data&body=Title%3A%20SurgPointTransformer%3A%20Vertebrae%20Shape%20Completion%20with%20RGB-D%20Data%0AAuthor%3A%20Aidana%20Massalimova%20and%20Florentin%20Liebmann%20and%20Sascha%20Jecklin%20and%20Fabio%20Carrillo%20and%20Farshad%20Mazda%20and%20Philipp%20F%C3%BCrnstahl%0AAbstract%3A%20%20%20State-of-the-art%20computer-%20and%20robot-assisted%20surgery%20systems%20heavily%20depend%0Aon%20intraoperative%20imaging%20technologies%20such%20as%20CT%20and%20fluoroscopy%20to%20generate%0Adetailed%203D%20visualization%20of%20the%20patient%27s%20anatomy.%20While%20imaging%20techniques%0Aare%20highly%20accurate%2C%20they%20are%20based%20on%20ionizing%20radiation%20and%20expose%20patients%0Aand%20clinicians.%20This%20study%20introduces%20an%20alternative%2C%20radiation-free%20approach%0Afor%20reconstructing%20the%203D%20spine%20anatomy%20using%20RGB-D%20data.%20Drawing%20inspiration%0Afrom%20the%203D%20%22mental%20map%22%20that%20surgeons%20form%20during%20surgeries%2C%20we%20introduce%0ASurgPointTransformer%2C%20a%20shape%20completion%20approach%20for%20surgical%20applications%0Athat%20can%20accurately%20reconstruct%20the%20unexposed%20spine%20regions%20from%20sparse%0Aobservations%20of%20the%20exposed%20surface.%0A%20%20Our%20method%20involves%20two%20main%20steps%3A%20segmentation%20and%20shape%20completion.%20The%0Asegmentation%20step%20includes%20spinal%20column%20localization%20and%20segmentation%2C%0Afollowed%20by%20vertebra-wise%20segmentation.%20The%20segmented%20vertebra%20point%20clouds%20are%0Athen%20subjected%20to%20SurgPointTransformer%2C%20which%20leverages%20an%20attention%20mechanism%0Ato%20learn%20patterns%20between%20visible%20surface%20features%20and%20the%20underlying%20anatomy.%0AFor%20evaluation%2C%20we%20utilize%20an%20ex-vivo%20dataset%20of%20nine%20specimens.%20Their%20CT%20data%0Ais%20used%20to%20establish%20ground%20truth%20data%20that%20were%20used%20to%20compare%20to%20the%20outputs%0Aof%20our%20methods.%20Our%20method%20significantly%20outperforms%20the%20state-of-the-art%0Abaselines%2C%20achieving%20an%20average%20Chamfer%20Distance%20of%205.39%2C%20an%20F-Score%20of%200.85%2C%0Aan%20Earth%20Mover%27s%20Distance%20of%200.011%2C%20and%20a%20Signal-to-Noise%20Ratio%20of%2022.90%20dB.%0A%20%20This%20study%20demonstrates%20the%20potential%20of%20our%20reconstruction%20method%20for%203D%0Avertebral%20shape%20completion.%20It%20enables%203D%20reconstruction%20of%20the%20entire%20lumbar%0Aspine%20and%20surgical%20guidance%20without%20ionizing%20radiation%20or%20invasive%20imaging.%20Our%0Awork%20contributes%20to%20computer-aided%20and%20robot-assisted%20surgery%2C%20advancing%20the%0Aperception%20and%20intelligence%20of%20these%20systems.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2410.01443v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DSurgPointTransformer%253A%2520Vertebrae%2520Shape%2520Completion%2520with%2520RGB-D%2520Data%26entry.906535625%3DAidana%2520Massalimova%2520and%2520Florentin%2520Liebmann%2520and%2520Sascha%2520Jecklin%2520and%2520Fabio%2520Carrillo%2520and%2520Farshad%2520Mazda%2520and%2520Philipp%2520F%25C3%25BCrnstahl%26entry.1292438233%3D%2520%2520State-of-the-art%2520computer-%2520and%2520robot-assisted%2520surgery%2520systems%2520heavily%2520depend%250Aon%2520intraoperative%2520imaging%2520technologies%2520such%2520as%2520CT%2520and%2520fluoroscopy%2520to%2520generate%250Adetailed%25203D%2520visualization%2520of%2520the%2520patient%2527s%2520anatomy.%2520While%2520imaging%2520techniques%250Aare%2520highly%2520accurate%252C%2520they%2520are%2520based%2520on%2520ionizing%2520radiation%2520and%2520expose%2520patients%250Aand%2520clinicians.%2520This%2520study%2520introduces%2520an%2520alternative%252C%2520radiation-free%2520approach%250Afor%2520reconstructing%2520the%25203D%2520spine%2520anatomy%2520using%2520RGB-D%2520data.%2520Drawing%2520inspiration%250Afrom%2520the%25203D%2520%2522mental%2520map%2522%2520that%2520surgeons%2520form%2520during%2520surgeries%252C%2520we%2520introduce%250ASurgPointTransformer%252C%2520a%2520shape%2520completion%2520approach%2520for%2520surgical%2520applications%250Athat%2520can%2520accurately%2520reconstruct%2520the%2520unexposed%2520spine%2520regions%2520from%2520sparse%250Aobservations%2520of%2520the%2520exposed%2520surface.%250A%2520%2520Our%2520method%2520involves%2520two%2520main%2520steps%253A%2520segmentation%2520and%2520shape%2520completion.%2520The%250Asegmentation%2520step%2520includes%2520spinal%2520column%2520localization%2520and%2520segmentation%252C%250Afollowed%2520by%2520vertebra-wise%2520segmentation.%2520The%2520segmented%2520vertebra%2520point%2520clouds%2520are%250Athen%2520subjected%2520to%2520SurgPointTransformer%252C%2520which%2520leverages%2520an%2520attention%2520mechanism%250Ato%2520learn%2520patterns%2520between%2520visible%2520surface%2520features%2520and%2520the%2520underlying%2520anatomy.%250AFor%2520evaluation%252C%2520we%2520utilize%2520an%2520ex-vivo%2520dataset%2520of%2520nine%2520specimens.%2520Their%2520CT%2520data%250Ais%2520used%2520to%2520establish%2520ground%2520truth%2520data%2520that%2520were%2520used%2520to%2520compare%2520to%2520the%2520outputs%250Aof%2520our%2520methods.%2520Our%2520method%2520significantly%2520outperforms%2520the%2520state-of-the-art%250Abaselines%252C%2520achieving%2520an%2520average%2520Chamfer%2520Distance%2520of%25205.39%252C%2520an%2520F-Score%2520of%25200.85%252C%250Aan%2520Earth%2520Mover%2527s%2520Distance%2520of%25200.011%252C%2520and%2520a%2520Signal-to-Noise%2520Ratio%2520of%252022.90%2520dB.%250A%2520%2520This%2520study%2520demonstrates%2520the%2520potential%2520of%2520our%2520reconstruction%2520method%2520for%25203D%250Avertebral%2520shape%2520completion.%2520It%2520enables%25203D%2520reconstruction%2520of%2520the%2520entire%2520lumbar%250Aspine%2520and%2520surgical%2520guidance%2520without%2520ionizing%2520radiation%2520or%2520invasive%2520imaging.%2520Our%250Awork%2520contributes%2520to%2520computer-aided%2520and%2520robot-assisted%2520surgery%252C%2520advancing%2520the%250Aperception%2520and%2520intelligence%2520of%2520these%2520systems.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2410.01443v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=SurgPointTransformer%3A%20Vertebrae%20Shape%20Completion%20with%20RGB-D%20Data&entry.906535625=Aidana%20Massalimova%20and%20Florentin%20Liebmann%20and%20Sascha%20Jecklin%20and%20Fabio%20Carrillo%20and%20Farshad%20Mazda%20and%20Philipp%20F%C3%BCrnstahl&entry.1292438233=%20%20State-of-the-art%20computer-%20and%20robot-assisted%20surgery%20systems%20heavily%20depend%0Aon%20intraoperative%20imaging%20technologies%20such%20as%20CT%20and%20fluoroscopy%20to%20generate%0Adetailed%203D%20visualization%20of%20the%20patient%27s%20anatomy.%20While%20imaging%20techniques%0Aare%20highly%20accurate%2C%20they%20are%20based%20on%20ionizing%20radiation%20and%20expose%20patients%0Aand%20clinicians.%20This%20study%20introduces%20an%20alternative%2C%20radiation-free%20approach%0Afor%20reconstructing%20the%203D%20spine%20anatomy%20using%20RGB-D%20data.%20Drawing%20inspiration%0Afrom%20the%203D%20%22mental%20map%22%20that%20surgeons%20form%20during%20surgeries%2C%20we%20introduce%0ASurgPointTransformer%2C%20a%20shape%20completion%20approach%20for%20surgical%20applications%0Athat%20can%20accurately%20reconstruct%20the%20unexposed%20spine%20regions%20from%20sparse%0Aobservations%20of%20the%20exposed%20surface.%0A%20%20Our%20method%20involves%20two%20main%20steps%3A%20segmentation%20and%20shape%20completion.%20The%0Asegmentation%20step%20includes%20spinal%20column%20localization%20and%20segmentation%2C%0Afollowed%20by%20vertebra-wise%20segmentation.%20The%20segmented%20vertebra%20point%20clouds%20are%0Athen%20subjected%20to%20SurgPointTransformer%2C%20which%20leverages%20an%20attention%20mechanism%0Ato%20learn%20patterns%20between%20visible%20surface%20features%20and%20the%20underlying%20anatomy.%0AFor%20evaluation%2C%20we%20utilize%20an%20ex-vivo%20dataset%20of%20nine%20specimens.%20Their%20CT%20data%0Ais%20used%20to%20establish%20ground%20truth%20data%20that%20were%20used%20to%20compare%20to%20the%20outputs%0Aof%20our%20methods.%20Our%20method%20significantly%20outperforms%20the%20state-of-the-art%0Abaselines%2C%20achieving%20an%20average%20Chamfer%20Distance%20of%205.39%2C%20an%20F-Score%20of%200.85%2C%0Aan%20Earth%20Mover%27s%20Distance%20of%200.011%2C%20and%20a%20Signal-to-Noise%20Ratio%20of%2022.90%20dB.%0A%20%20This%20study%20demonstrates%20the%20potential%20of%20our%20reconstruction%20method%20for%203D%0Avertebral%20shape%20completion.%20It%20enables%203D%20reconstruction%20of%20the%20entire%20lumbar%0Aspine%20and%20surgical%20guidance%20without%20ionizing%20radiation%20or%20invasive%20imaging.%20Our%0Awork%20contributes%20to%20computer-aided%20and%20robot-assisted%20surgery%2C%20advancing%20the%0Aperception%20and%20intelligence%20of%20these%20systems.%0A&entry.1838667208=http%3A//arxiv.org/abs/2410.01443v1&entry.124074799=Read"},
{"title": "Social Conjuring: Multi-User Runtime Collaboration with AI in Building\n  Virtual 3D Worlds", "author": "Amina Kobenova and Cyan DeVeaux and Samyak Parajuli and Andrzej Banburski-Fahey and Judith Amores Fernandez and Jaron Lanier", "abstract": "  Generative artificial intelligence has shown promise in prompting virtual\nworlds into existence, yet little attention has been given to understanding how\nthis process unfolds as social interaction. We present Social Conjurer, a\nframework for AI-augmented dynamic 3D scene co-creation, where multiple users\ncollaboratively build and modify virtual worlds in real-time. Through an\nexpanded set of interactions, including social and tool-based engagements as\nwell as spatial reasoning, our framework facilitates the creation of rich,\ndiverse virtual environments. Findings from a preliminary user study (N=12)\nprovide insight into the user experience of this approach, how social contexts\nshape the prompting of spatial environments, and perspective on social\napplications of prompt-based 3D co-creation. In addition to highlighting the\npotential of AI-supported multi-user world creation and offering new pathways\nfor AI-augmented creative processes in VR, this article presents a set of\nimplications for designing human-centered interfaces that incorporate AI models\ninto 3D content generation.\n", "link": "http://arxiv.org/abs/2410.00274v2", "date": "2024-10-02", "relevancy": 2.9313, "topK": [{"title": "Genie: Generative Interactive Environments", "link": "http://arxiv.org/abs/2402.15391v1", "similarity": 0.5885}, {"title": "CAT3D: Create Anything in 3D with Multi-View Diffusion Models", "link": "http://arxiv.org/abs/2405.10314v1", "similarity": 0.5852}, {"title": "CAT3D: Create Anything in 3D with Multi-View Diffusion Models", "link": "http://arxiv.org/abs/2405.10314v1", "similarity": 0.5852}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Social%20Conjuring%3A%20Multi-User%20Runtime%20Collaboration%20with%20AI%20in%20Building%0A%20%20Virtual%203D%20Worlds&body=Title%3A%20Social%20Conjuring%3A%20Multi-User%20Runtime%20Collaboration%20with%20AI%20in%20Building%0A%20%20Virtual%203D%20Worlds%0AAuthor%3A%20Amina%20Kobenova%20and%20Cyan%20DeVeaux%20and%20Samyak%20Parajuli%20and%20Andrzej%20Banburski-Fahey%20and%20Judith%20Amores%20Fernandez%20and%20Jaron%20Lanier%0AAbstract%3A%20%20%20Generative%20artificial%20intelligence%20has%20shown%20promise%20in%20prompting%20virtual%0Aworlds%20into%20existence%2C%20yet%20little%20attention%20has%20been%20given%20to%20understanding%20how%0Athis%20process%20unfolds%20as%20social%20interaction.%20We%20present%20Social%20Conjurer%2C%20a%0Aframework%20for%20AI-augmented%20dynamic%203D%20scene%20co-creation%2C%20where%20multiple%20users%0Acollaboratively%20build%20and%20modify%20virtual%20worlds%20in%20real-time.%20Through%20an%0Aexpanded%20set%20of%20interactions%2C%20including%20social%20and%20tool-based%20engagements%20as%0Awell%20as%20spatial%20reasoning%2C%20our%20framework%20facilitates%20the%20creation%20of%20rich%2C%0Adiverse%20virtual%20environments.%20Findings%20from%20a%20preliminary%20user%20study%20%28N%3D12%29%0Aprovide%20insight%20into%20the%20user%20experience%20of%20this%20approach%2C%20how%20social%20contexts%0Ashape%20the%20prompting%20of%20spatial%20environments%2C%20and%20perspective%20on%20social%0Aapplications%20of%20prompt-based%203D%20co-creation.%20In%20addition%20to%20highlighting%20the%0Apotential%20of%20AI-supported%20multi-user%20world%20creation%20and%20offering%20new%20pathways%0Afor%20AI-augmented%20creative%20processes%20in%20VR%2C%20this%20article%20presents%20a%20set%20of%0Aimplications%20for%20designing%20human-centered%20interfaces%20that%20incorporate%20AI%20models%0Ainto%203D%20content%20generation.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2410.00274v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DSocial%2520Conjuring%253A%2520Multi-User%2520Runtime%2520Collaboration%2520with%2520AI%2520in%2520Building%250A%2520%2520Virtual%25203D%2520Worlds%26entry.906535625%3DAmina%2520Kobenova%2520and%2520Cyan%2520DeVeaux%2520and%2520Samyak%2520Parajuli%2520and%2520Andrzej%2520Banburski-Fahey%2520and%2520Judith%2520Amores%2520Fernandez%2520and%2520Jaron%2520Lanier%26entry.1292438233%3D%2520%2520Generative%2520artificial%2520intelligence%2520has%2520shown%2520promise%2520in%2520prompting%2520virtual%250Aworlds%2520into%2520existence%252C%2520yet%2520little%2520attention%2520has%2520been%2520given%2520to%2520understanding%2520how%250Athis%2520process%2520unfolds%2520as%2520social%2520interaction.%2520We%2520present%2520Social%2520Conjurer%252C%2520a%250Aframework%2520for%2520AI-augmented%2520dynamic%25203D%2520scene%2520co-creation%252C%2520where%2520multiple%2520users%250Acollaboratively%2520build%2520and%2520modify%2520virtual%2520worlds%2520in%2520real-time.%2520Through%2520an%250Aexpanded%2520set%2520of%2520interactions%252C%2520including%2520social%2520and%2520tool-based%2520engagements%2520as%250Awell%2520as%2520spatial%2520reasoning%252C%2520our%2520framework%2520facilitates%2520the%2520creation%2520of%2520rich%252C%250Adiverse%2520virtual%2520environments.%2520Findings%2520from%2520a%2520preliminary%2520user%2520study%2520%2528N%253D12%2529%250Aprovide%2520insight%2520into%2520the%2520user%2520experience%2520of%2520this%2520approach%252C%2520how%2520social%2520contexts%250Ashape%2520the%2520prompting%2520of%2520spatial%2520environments%252C%2520and%2520perspective%2520on%2520social%250Aapplications%2520of%2520prompt-based%25203D%2520co-creation.%2520In%2520addition%2520to%2520highlighting%2520the%250Apotential%2520of%2520AI-supported%2520multi-user%2520world%2520creation%2520and%2520offering%2520new%2520pathways%250Afor%2520AI-augmented%2520creative%2520processes%2520in%2520VR%252C%2520this%2520article%2520presents%2520a%2520set%2520of%250Aimplications%2520for%2520designing%2520human-centered%2520interfaces%2520that%2520incorporate%2520AI%2520models%250Ainto%25203D%2520content%2520generation.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2410.00274v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Social%20Conjuring%3A%20Multi-User%20Runtime%20Collaboration%20with%20AI%20in%20Building%0A%20%20Virtual%203D%20Worlds&entry.906535625=Amina%20Kobenova%20and%20Cyan%20DeVeaux%20and%20Samyak%20Parajuli%20and%20Andrzej%20Banburski-Fahey%20and%20Judith%20Amores%20Fernandez%20and%20Jaron%20Lanier&entry.1292438233=%20%20Generative%20artificial%20intelligence%20has%20shown%20promise%20in%20prompting%20virtual%0Aworlds%20into%20existence%2C%20yet%20little%20attention%20has%20been%20given%20to%20understanding%20how%0Athis%20process%20unfolds%20as%20social%20interaction.%20We%20present%20Social%20Conjurer%2C%20a%0Aframework%20for%20AI-augmented%20dynamic%203D%20scene%20co-creation%2C%20where%20multiple%20users%0Acollaboratively%20build%20and%20modify%20virtual%20worlds%20in%20real-time.%20Through%20an%0Aexpanded%20set%20of%20interactions%2C%20including%20social%20and%20tool-based%20engagements%20as%0Awell%20as%20spatial%20reasoning%2C%20our%20framework%20facilitates%20the%20creation%20of%20rich%2C%0Adiverse%20virtual%20environments.%20Findings%20from%20a%20preliminary%20user%20study%20%28N%3D12%29%0Aprovide%20insight%20into%20the%20user%20experience%20of%20this%20approach%2C%20how%20social%20contexts%0Ashape%20the%20prompting%20of%20spatial%20environments%2C%20and%20perspective%20on%20social%0Aapplications%20of%20prompt-based%203D%20co-creation.%20In%20addition%20to%20highlighting%20the%0Apotential%20of%20AI-supported%20multi-user%20world%20creation%20and%20offering%20new%20pathways%0Afor%20AI-augmented%20creative%20processes%20in%20VR%2C%20this%20article%20presents%20a%20set%20of%0Aimplications%20for%20designing%20human-centered%20interfaces%20that%20incorporate%20AI%20models%0Ainto%203D%20content%20generation.%0A&entry.1838667208=http%3A//arxiv.org/abs/2410.00274v2&entry.124074799=Read"},
{"title": "LMOD: A Large Multimodal Ophthalmology Dataset and Benchmark for Large\n  Vision-Language Models", "author": "Zhenyue Qin and Yu Yin and Dylan Campbell and Xuansheng Wu and Ke Zou and Yih-Chung Tham and Ninghao Liu and Xiuzhen Zhang and Qingyu Chen", "abstract": "  Ophthalmology relies heavily on detailed image analysis for diagnosis and\ntreatment planning. While large vision-language models (LVLMs) have shown\npromise in understanding complex visual information, their performance on\nophthalmology images remains underexplored. We introduce LMOD, a dataset and\nbenchmark for evaluating LVLMs on ophthalmology images, covering anatomical\nunderstanding, diagnostic analysis, and demographic extraction. LMODincludes\n21,993 images spanning optical coherence tomography, scanning laser\nophthalmoscopy, eye photos, surgical scenes, and color fundus photographs. We\nbenchmark 13 state-of-the-art LVLMs and find that they are far from perfect for\ncomprehending ophthalmology images. Models struggle with diagnostic analysis\nand demographic extraction, reveal weaknesses in spatial reasoning, diagnostic\nanalysis, handling out-of-domain queries, and safeguards for handling\nbiomarkers of ophthalmology images.\n", "link": "http://arxiv.org/abs/2410.01620v1", "date": "2024-10-02", "relevancy": 2.921, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5986}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5986}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5555}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20LMOD%3A%20A%20Large%20Multimodal%20Ophthalmology%20Dataset%20and%20Benchmark%20for%20Large%0A%20%20Vision-Language%20Models&body=Title%3A%20LMOD%3A%20A%20Large%20Multimodal%20Ophthalmology%20Dataset%20and%20Benchmark%20for%20Large%0A%20%20Vision-Language%20Models%0AAuthor%3A%20Zhenyue%20Qin%20and%20Yu%20Yin%20and%20Dylan%20Campbell%20and%20Xuansheng%20Wu%20and%20Ke%20Zou%20and%20Yih-Chung%20Tham%20and%20Ninghao%20Liu%20and%20Xiuzhen%20Zhang%20and%20Qingyu%20Chen%0AAbstract%3A%20%20%20Ophthalmology%20relies%20heavily%20on%20detailed%20image%20analysis%20for%20diagnosis%20and%0Atreatment%20planning.%20While%20large%20vision-language%20models%20%28LVLMs%29%20have%20shown%0Apromise%20in%20understanding%20complex%20visual%20information%2C%20their%20performance%20on%0Aophthalmology%20images%20remains%20underexplored.%20We%20introduce%20LMOD%2C%20a%20dataset%20and%0Abenchmark%20for%20evaluating%20LVLMs%20on%20ophthalmology%20images%2C%20covering%20anatomical%0Aunderstanding%2C%20diagnostic%20analysis%2C%20and%20demographic%20extraction.%20LMODincludes%0A21%2C993%20images%20spanning%20optical%20coherence%20tomography%2C%20scanning%20laser%0Aophthalmoscopy%2C%20eye%20photos%2C%20surgical%20scenes%2C%20and%20color%20fundus%20photographs.%20We%0Abenchmark%2013%20state-of-the-art%20LVLMs%20and%20find%20that%20they%20are%20far%20from%20perfect%20for%0Acomprehending%20ophthalmology%20images.%20Models%20struggle%20with%20diagnostic%20analysis%0Aand%20demographic%20extraction%2C%20reveal%20weaknesses%20in%20spatial%20reasoning%2C%20diagnostic%0Aanalysis%2C%20handling%20out-of-domain%20queries%2C%20and%20safeguards%20for%20handling%0Abiomarkers%20of%20ophthalmology%20images.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2410.01620v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DLMOD%253A%2520A%2520Large%2520Multimodal%2520Ophthalmology%2520Dataset%2520and%2520Benchmark%2520for%2520Large%250A%2520%2520Vision-Language%2520Models%26entry.906535625%3DZhenyue%2520Qin%2520and%2520Yu%2520Yin%2520and%2520Dylan%2520Campbell%2520and%2520Xuansheng%2520Wu%2520and%2520Ke%2520Zou%2520and%2520Yih-Chung%2520Tham%2520and%2520Ninghao%2520Liu%2520and%2520Xiuzhen%2520Zhang%2520and%2520Qingyu%2520Chen%26entry.1292438233%3D%2520%2520Ophthalmology%2520relies%2520heavily%2520on%2520detailed%2520image%2520analysis%2520for%2520diagnosis%2520and%250Atreatment%2520planning.%2520While%2520large%2520vision-language%2520models%2520%2528LVLMs%2529%2520have%2520shown%250Apromise%2520in%2520understanding%2520complex%2520visual%2520information%252C%2520their%2520performance%2520on%250Aophthalmology%2520images%2520remains%2520underexplored.%2520We%2520introduce%2520LMOD%252C%2520a%2520dataset%2520and%250Abenchmark%2520for%2520evaluating%2520LVLMs%2520on%2520ophthalmology%2520images%252C%2520covering%2520anatomical%250Aunderstanding%252C%2520diagnostic%2520analysis%252C%2520and%2520demographic%2520extraction.%2520LMODincludes%250A21%252C993%2520images%2520spanning%2520optical%2520coherence%2520tomography%252C%2520scanning%2520laser%250Aophthalmoscopy%252C%2520eye%2520photos%252C%2520surgical%2520scenes%252C%2520and%2520color%2520fundus%2520photographs.%2520We%250Abenchmark%252013%2520state-of-the-art%2520LVLMs%2520and%2520find%2520that%2520they%2520are%2520far%2520from%2520perfect%2520for%250Acomprehending%2520ophthalmology%2520images.%2520Models%2520struggle%2520with%2520diagnostic%2520analysis%250Aand%2520demographic%2520extraction%252C%2520reveal%2520weaknesses%2520in%2520spatial%2520reasoning%252C%2520diagnostic%250Aanalysis%252C%2520handling%2520out-of-domain%2520queries%252C%2520and%2520safeguards%2520for%2520handling%250Abiomarkers%2520of%2520ophthalmology%2520images.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2410.01620v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=LMOD%3A%20A%20Large%20Multimodal%20Ophthalmology%20Dataset%20and%20Benchmark%20for%20Large%0A%20%20Vision-Language%20Models&entry.906535625=Zhenyue%20Qin%20and%20Yu%20Yin%20and%20Dylan%20Campbell%20and%20Xuansheng%20Wu%20and%20Ke%20Zou%20and%20Yih-Chung%20Tham%20and%20Ninghao%20Liu%20and%20Xiuzhen%20Zhang%20and%20Qingyu%20Chen&entry.1292438233=%20%20Ophthalmology%20relies%20heavily%20on%20detailed%20image%20analysis%20for%20diagnosis%20and%0Atreatment%20planning.%20While%20large%20vision-language%20models%20%28LVLMs%29%20have%20shown%0Apromise%20in%20understanding%20complex%20visual%20information%2C%20their%20performance%20on%0Aophthalmology%20images%20remains%20underexplored.%20We%20introduce%20LMOD%2C%20a%20dataset%20and%0Abenchmark%20for%20evaluating%20LVLMs%20on%20ophthalmology%20images%2C%20covering%20anatomical%0Aunderstanding%2C%20diagnostic%20analysis%2C%20and%20demographic%20extraction.%20LMODincludes%0A21%2C993%20images%20spanning%20optical%20coherence%20tomography%2C%20scanning%20laser%0Aophthalmoscopy%2C%20eye%20photos%2C%20surgical%20scenes%2C%20and%20color%20fundus%20photographs.%20We%0Abenchmark%2013%20state-of-the-art%20LVLMs%20and%20find%20that%20they%20are%20far%20from%20perfect%20for%0Acomprehending%20ophthalmology%20images.%20Models%20struggle%20with%20diagnostic%20analysis%0Aand%20demographic%20extraction%2C%20reveal%20weaknesses%20in%20spatial%20reasoning%2C%20diagnostic%0Aanalysis%2C%20handling%20out-of-domain%20queries%2C%20and%20safeguards%20for%20handling%0Abiomarkers%20of%20ophthalmology%20images.%0A&entry.1838667208=http%3A//arxiv.org/abs/2410.01620v1&entry.124074799=Read"},
{"title": "Why context matters in VQA and Reasoning: Semantic interventions for VLM\n  input modalities", "author": "Kenza Amara and Lukas Klein and Carsten L\u00fcth and Paul J\u00e4ger and Hendrik Strobelt and Mennatallah El-Assady", "abstract": "  The various limitations of Generative AI, such as hallucinations and model\nfailures, have made it crucial to understand the role of different modalities\nin Visual Language Model (VLM) predictions. Our work investigates how the\nintegration of information from image and text modalities influences the\nperformance and behavior of VLMs in visual question answering (VQA) and\nreasoning tasks. We measure this effect through answer accuracy, reasoning\nquality, model uncertainty, and modality relevance. We study the interplay\nbetween text and image modalities in different configurations where visual\ncontent is essential for solving the VQA task. Our contributions include (1)\nthe Semantic Interventions (SI)-VQA dataset, (2) a benchmark study of various\nVLM architectures under different modality configurations, and (3) the\nInteractive Semantic Interventions (ISI) tool. The SI-VQA dataset serves as the\nfoundation for the benchmark, while the ISI tool provides an interface to test\nand apply semantic interventions in image and text inputs, enabling more\nfine-grained analysis. Our results show that complementary information between\nmodalities improves answer and reasoning quality, while contradictory\ninformation harms model performance and confidence. Image text annotations have\nminimal impact on accuracy and uncertainty, slightly increasing image\nrelevance. Attention analysis confirms the dominant role of image inputs over\ntext in VQA tasks. In this study, we evaluate state-of-the-art VLMs that allow\nus to extract attention coefficients for each modality. A key finding is\nPaliGemma's harmful overconfidence, which poses a higher risk of silent\nfailures compared to the LLaVA models. This work sets the foundation for\nrigorous analysis of modality integration, supported by datasets specifically\ndesigned for this purpose.\n", "link": "http://arxiv.org/abs/2410.01690v1", "date": "2024-10-02", "relevancy": 2.92, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.6035}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.6035}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5449}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Why%20context%20matters%20in%20VQA%20and%20Reasoning%3A%20Semantic%20interventions%20for%20VLM%0A%20%20input%20modalities&body=Title%3A%20Why%20context%20matters%20in%20VQA%20and%20Reasoning%3A%20Semantic%20interventions%20for%20VLM%0A%20%20input%20modalities%0AAuthor%3A%20Kenza%20Amara%20and%20Lukas%20Klein%20and%20Carsten%20L%C3%BCth%20and%20Paul%20J%C3%A4ger%20and%20Hendrik%20Strobelt%20and%20Mennatallah%20El-Assady%0AAbstract%3A%20%20%20The%20various%20limitations%20of%20Generative%20AI%2C%20such%20as%20hallucinations%20and%20model%0Afailures%2C%20have%20made%20it%20crucial%20to%20understand%20the%20role%20of%20different%20modalities%0Ain%20Visual%20Language%20Model%20%28VLM%29%20predictions.%20Our%20work%20investigates%20how%20the%0Aintegration%20of%20information%20from%20image%20and%20text%20modalities%20influences%20the%0Aperformance%20and%20behavior%20of%20VLMs%20in%20visual%20question%20answering%20%28VQA%29%20and%0Areasoning%20tasks.%20We%20measure%20this%20effect%20through%20answer%20accuracy%2C%20reasoning%0Aquality%2C%20model%20uncertainty%2C%20and%20modality%20relevance.%20We%20study%20the%20interplay%0Abetween%20text%20and%20image%20modalities%20in%20different%20configurations%20where%20visual%0Acontent%20is%20essential%20for%20solving%20the%20VQA%20task.%20Our%20contributions%20include%20%281%29%0Athe%20Semantic%20Interventions%20%28SI%29-VQA%20dataset%2C%20%282%29%20a%20benchmark%20study%20of%20various%0AVLM%20architectures%20under%20different%20modality%20configurations%2C%20and%20%283%29%20the%0AInteractive%20Semantic%20Interventions%20%28ISI%29%20tool.%20The%20SI-VQA%20dataset%20serves%20as%20the%0Afoundation%20for%20the%20benchmark%2C%20while%20the%20ISI%20tool%20provides%20an%20interface%20to%20test%0Aand%20apply%20semantic%20interventions%20in%20image%20and%20text%20inputs%2C%20enabling%20more%0Afine-grained%20analysis.%20Our%20results%20show%20that%20complementary%20information%20between%0Amodalities%20improves%20answer%20and%20reasoning%20quality%2C%20while%20contradictory%0Ainformation%20harms%20model%20performance%20and%20confidence.%20Image%20text%20annotations%20have%0Aminimal%20impact%20on%20accuracy%20and%20uncertainty%2C%20slightly%20increasing%20image%0Arelevance.%20Attention%20analysis%20confirms%20the%20dominant%20role%20of%20image%20inputs%20over%0Atext%20in%20VQA%20tasks.%20In%20this%20study%2C%20we%20evaluate%20state-of-the-art%20VLMs%20that%20allow%0Aus%20to%20extract%20attention%20coefficients%20for%20each%20modality.%20A%20key%20finding%20is%0APaliGemma%27s%20harmful%20overconfidence%2C%20which%20poses%20a%20higher%20risk%20of%20silent%0Afailures%20compared%20to%20the%20LLaVA%20models.%20This%20work%20sets%20the%20foundation%20for%0Arigorous%20analysis%20of%20modality%20integration%2C%20supported%20by%20datasets%20specifically%0Adesigned%20for%20this%20purpose.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2410.01690v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DWhy%2520context%2520matters%2520in%2520VQA%2520and%2520Reasoning%253A%2520Semantic%2520interventions%2520for%2520VLM%250A%2520%2520input%2520modalities%26entry.906535625%3DKenza%2520Amara%2520and%2520Lukas%2520Klein%2520and%2520Carsten%2520L%25C3%25BCth%2520and%2520Paul%2520J%25C3%25A4ger%2520and%2520Hendrik%2520Strobelt%2520and%2520Mennatallah%2520El-Assady%26entry.1292438233%3D%2520%2520The%2520various%2520limitations%2520of%2520Generative%2520AI%252C%2520such%2520as%2520hallucinations%2520and%2520model%250Afailures%252C%2520have%2520made%2520it%2520crucial%2520to%2520understand%2520the%2520role%2520of%2520different%2520modalities%250Ain%2520Visual%2520Language%2520Model%2520%2528VLM%2529%2520predictions.%2520Our%2520work%2520investigates%2520how%2520the%250Aintegration%2520of%2520information%2520from%2520image%2520and%2520text%2520modalities%2520influences%2520the%250Aperformance%2520and%2520behavior%2520of%2520VLMs%2520in%2520visual%2520question%2520answering%2520%2528VQA%2529%2520and%250Areasoning%2520tasks.%2520We%2520measure%2520this%2520effect%2520through%2520answer%2520accuracy%252C%2520reasoning%250Aquality%252C%2520model%2520uncertainty%252C%2520and%2520modality%2520relevance.%2520We%2520study%2520the%2520interplay%250Abetween%2520text%2520and%2520image%2520modalities%2520in%2520different%2520configurations%2520where%2520visual%250Acontent%2520is%2520essential%2520for%2520solving%2520the%2520VQA%2520task.%2520Our%2520contributions%2520include%2520%25281%2529%250Athe%2520Semantic%2520Interventions%2520%2528SI%2529-VQA%2520dataset%252C%2520%25282%2529%2520a%2520benchmark%2520study%2520of%2520various%250AVLM%2520architectures%2520under%2520different%2520modality%2520configurations%252C%2520and%2520%25283%2529%2520the%250AInteractive%2520Semantic%2520Interventions%2520%2528ISI%2529%2520tool.%2520The%2520SI-VQA%2520dataset%2520serves%2520as%2520the%250Afoundation%2520for%2520the%2520benchmark%252C%2520while%2520the%2520ISI%2520tool%2520provides%2520an%2520interface%2520to%2520test%250Aand%2520apply%2520semantic%2520interventions%2520in%2520image%2520and%2520text%2520inputs%252C%2520enabling%2520more%250Afine-grained%2520analysis.%2520Our%2520results%2520show%2520that%2520complementary%2520information%2520between%250Amodalities%2520improves%2520answer%2520and%2520reasoning%2520quality%252C%2520while%2520contradictory%250Ainformation%2520harms%2520model%2520performance%2520and%2520confidence.%2520Image%2520text%2520annotations%2520have%250Aminimal%2520impact%2520on%2520accuracy%2520and%2520uncertainty%252C%2520slightly%2520increasing%2520image%250Arelevance.%2520Attention%2520analysis%2520confirms%2520the%2520dominant%2520role%2520of%2520image%2520inputs%2520over%250Atext%2520in%2520VQA%2520tasks.%2520In%2520this%2520study%252C%2520we%2520evaluate%2520state-of-the-art%2520VLMs%2520that%2520allow%250Aus%2520to%2520extract%2520attention%2520coefficients%2520for%2520each%2520modality.%2520A%2520key%2520finding%2520is%250APaliGemma%2527s%2520harmful%2520overconfidence%252C%2520which%2520poses%2520a%2520higher%2520risk%2520of%2520silent%250Afailures%2520compared%2520to%2520the%2520LLaVA%2520models.%2520This%2520work%2520sets%2520the%2520foundation%2520for%250Arigorous%2520analysis%2520of%2520modality%2520integration%252C%2520supported%2520by%2520datasets%2520specifically%250Adesigned%2520for%2520this%2520purpose.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2410.01690v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Why%20context%20matters%20in%20VQA%20and%20Reasoning%3A%20Semantic%20interventions%20for%20VLM%0A%20%20input%20modalities&entry.906535625=Kenza%20Amara%20and%20Lukas%20Klein%20and%20Carsten%20L%C3%BCth%20and%20Paul%20J%C3%A4ger%20and%20Hendrik%20Strobelt%20and%20Mennatallah%20El-Assady&entry.1292438233=%20%20The%20various%20limitations%20of%20Generative%20AI%2C%20such%20as%20hallucinations%20and%20model%0Afailures%2C%20have%20made%20it%20crucial%20to%20understand%20the%20role%20of%20different%20modalities%0Ain%20Visual%20Language%20Model%20%28VLM%29%20predictions.%20Our%20work%20investigates%20how%20the%0Aintegration%20of%20information%20from%20image%20and%20text%20modalities%20influences%20the%0Aperformance%20and%20behavior%20of%20VLMs%20in%20visual%20question%20answering%20%28VQA%29%20and%0Areasoning%20tasks.%20We%20measure%20this%20effect%20through%20answer%20accuracy%2C%20reasoning%0Aquality%2C%20model%20uncertainty%2C%20and%20modality%20relevance.%20We%20study%20the%20interplay%0Abetween%20text%20and%20image%20modalities%20in%20different%20configurations%20where%20visual%0Acontent%20is%20essential%20for%20solving%20the%20VQA%20task.%20Our%20contributions%20include%20%281%29%0Athe%20Semantic%20Interventions%20%28SI%29-VQA%20dataset%2C%20%282%29%20a%20benchmark%20study%20of%20various%0AVLM%20architectures%20under%20different%20modality%20configurations%2C%20and%20%283%29%20the%0AInteractive%20Semantic%20Interventions%20%28ISI%29%20tool.%20The%20SI-VQA%20dataset%20serves%20as%20the%0Afoundation%20for%20the%20benchmark%2C%20while%20the%20ISI%20tool%20provides%20an%20interface%20to%20test%0Aand%20apply%20semantic%20interventions%20in%20image%20and%20text%20inputs%2C%20enabling%20more%0Afine-grained%20analysis.%20Our%20results%20show%20that%20complementary%20information%20between%0Amodalities%20improves%20answer%20and%20reasoning%20quality%2C%20while%20contradictory%0Ainformation%20harms%20model%20performance%20and%20confidence.%20Image%20text%20annotations%20have%0Aminimal%20impact%20on%20accuracy%20and%20uncertainty%2C%20slightly%20increasing%20image%0Arelevance.%20Attention%20analysis%20confirms%20the%20dominant%20role%20of%20image%20inputs%20over%0Atext%20in%20VQA%20tasks.%20In%20this%20study%2C%20we%20evaluate%20state-of-the-art%20VLMs%20that%20allow%0Aus%20to%20extract%20attention%20coefficients%20for%20each%20modality.%20A%20key%20finding%20is%0APaliGemma%27s%20harmful%20overconfidence%2C%20which%20poses%20a%20higher%20risk%20of%20silent%0Afailures%20compared%20to%20the%20LLaVA%20models.%20This%20work%20sets%20the%20foundation%20for%0Arigorous%20analysis%20of%20modality%20integration%2C%20supported%20by%20datasets%20specifically%0Adesigned%20for%20this%20purpose.%0A&entry.1838667208=http%3A//arxiv.org/abs/2410.01690v1&entry.124074799=Read"},
{"title": "Towards a vision foundation model for comprehensive assessment of\n  Cardiac MRI", "author": "Athira J Jacob and Indraneel Borgohain and Teodora Chitiboi and Puneet Sharma and Dorin Comaniciu and Daniel Rueckert", "abstract": "  Cardiac magnetic resonance imaging (CMR), considered the gold standard for\nnoninvasive cardiac assessment, is a diverse and complex modality requiring a\nwide variety of image processing tasks for comprehensive assessment of cardiac\nmorphology and function. Advances in deep learning have enabled the development\nof state-of-the-art (SoTA) models for these tasks. However, model training is\nchallenging due to data and label scarcity, especially in the less common\nimaging sequences. Moreover, each model is often trained for a specific task,\nwith no connection between related tasks. In this work, we introduce a vision\nfoundation model trained for CMR assessment, that is trained in a\nself-supervised fashion on 36 million CMR images. We then finetune the model in\nsupervised way for 9 clinical tasks typical to a CMR workflow, across\nclassification, segmentation, landmark localization, and pathology detection.\nWe demonstrate improved accuracy and robustness across all tasks, over a range\nof available labeled dataset sizes. We also demonstrate improved few-shot\nlearning with fewer labeled samples, a common challenge in medical image\nanalyses. We achieve an out-of-box performance comparable to SoTA for most\nclinical tasks. The proposed method thus presents a resource-efficient, unified\nframework for CMR assessment, with the potential to accelerate the development\nof deep learning-based solutions for image analysis tasks, even with few\nannotated data available.\n", "link": "http://arxiv.org/abs/2410.01665v1", "date": "2024-10-02", "relevancy": 2.9132, "topK": [{"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.583}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5825}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5825}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Towards%20a%20vision%20foundation%20model%20for%20comprehensive%20assessment%20of%0A%20%20Cardiac%20MRI&body=Title%3A%20Towards%20a%20vision%20foundation%20model%20for%20comprehensive%20assessment%20of%0A%20%20Cardiac%20MRI%0AAuthor%3A%20Athira%20J%20Jacob%20and%20Indraneel%20Borgohain%20and%20Teodora%20Chitiboi%20and%20Puneet%20Sharma%20and%20Dorin%20Comaniciu%20and%20Daniel%20Rueckert%0AAbstract%3A%20%20%20Cardiac%20magnetic%20resonance%20imaging%20%28CMR%29%2C%20considered%20the%20gold%20standard%20for%0Anoninvasive%20cardiac%20assessment%2C%20is%20a%20diverse%20and%20complex%20modality%20requiring%20a%0Awide%20variety%20of%20image%20processing%20tasks%20for%20comprehensive%20assessment%20of%20cardiac%0Amorphology%20and%20function.%20Advances%20in%20deep%20learning%20have%20enabled%20the%20development%0Aof%20state-of-the-art%20%28SoTA%29%20models%20for%20these%20tasks.%20However%2C%20model%20training%20is%0Achallenging%20due%20to%20data%20and%20label%20scarcity%2C%20especially%20in%20the%20less%20common%0Aimaging%20sequences.%20Moreover%2C%20each%20model%20is%20often%20trained%20for%20a%20specific%20task%2C%0Awith%20no%20connection%20between%20related%20tasks.%20In%20this%20work%2C%20we%20introduce%20a%20vision%0Afoundation%20model%20trained%20for%20CMR%20assessment%2C%20that%20is%20trained%20in%20a%0Aself-supervised%20fashion%20on%2036%20million%20CMR%20images.%20We%20then%20finetune%20the%20model%20in%0Asupervised%20way%20for%209%20clinical%20tasks%20typical%20to%20a%20CMR%20workflow%2C%20across%0Aclassification%2C%20segmentation%2C%20landmark%20localization%2C%20and%20pathology%20detection.%0AWe%20demonstrate%20improved%20accuracy%20and%20robustness%20across%20all%20tasks%2C%20over%20a%20range%0Aof%20available%20labeled%20dataset%20sizes.%20We%20also%20demonstrate%20improved%20few-shot%0Alearning%20with%20fewer%20labeled%20samples%2C%20a%20common%20challenge%20in%20medical%20image%0Aanalyses.%20We%20achieve%20an%20out-of-box%20performance%20comparable%20to%20SoTA%20for%20most%0Aclinical%20tasks.%20The%20proposed%20method%20thus%20presents%20a%20resource-efficient%2C%20unified%0Aframework%20for%20CMR%20assessment%2C%20with%20the%20potential%20to%20accelerate%20the%20development%0Aof%20deep%20learning-based%20solutions%20for%20image%20analysis%20tasks%2C%20even%20with%20few%0Aannotated%20data%20available.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2410.01665v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DTowards%2520a%2520vision%2520foundation%2520model%2520for%2520comprehensive%2520assessment%2520of%250A%2520%2520Cardiac%2520MRI%26entry.906535625%3DAthira%2520J%2520Jacob%2520and%2520Indraneel%2520Borgohain%2520and%2520Teodora%2520Chitiboi%2520and%2520Puneet%2520Sharma%2520and%2520Dorin%2520Comaniciu%2520and%2520Daniel%2520Rueckert%26entry.1292438233%3D%2520%2520Cardiac%2520magnetic%2520resonance%2520imaging%2520%2528CMR%2529%252C%2520considered%2520the%2520gold%2520standard%2520for%250Anoninvasive%2520cardiac%2520assessment%252C%2520is%2520a%2520diverse%2520and%2520complex%2520modality%2520requiring%2520a%250Awide%2520variety%2520of%2520image%2520processing%2520tasks%2520for%2520comprehensive%2520assessment%2520of%2520cardiac%250Amorphology%2520and%2520function.%2520Advances%2520in%2520deep%2520learning%2520have%2520enabled%2520the%2520development%250Aof%2520state-of-the-art%2520%2528SoTA%2529%2520models%2520for%2520these%2520tasks.%2520However%252C%2520model%2520training%2520is%250Achallenging%2520due%2520to%2520data%2520and%2520label%2520scarcity%252C%2520especially%2520in%2520the%2520less%2520common%250Aimaging%2520sequences.%2520Moreover%252C%2520each%2520model%2520is%2520often%2520trained%2520for%2520a%2520specific%2520task%252C%250Awith%2520no%2520connection%2520between%2520related%2520tasks.%2520In%2520this%2520work%252C%2520we%2520introduce%2520a%2520vision%250Afoundation%2520model%2520trained%2520for%2520CMR%2520assessment%252C%2520that%2520is%2520trained%2520in%2520a%250Aself-supervised%2520fashion%2520on%252036%2520million%2520CMR%2520images.%2520We%2520then%2520finetune%2520the%2520model%2520in%250Asupervised%2520way%2520for%25209%2520clinical%2520tasks%2520typical%2520to%2520a%2520CMR%2520workflow%252C%2520across%250Aclassification%252C%2520segmentation%252C%2520landmark%2520localization%252C%2520and%2520pathology%2520detection.%250AWe%2520demonstrate%2520improved%2520accuracy%2520and%2520robustness%2520across%2520all%2520tasks%252C%2520over%2520a%2520range%250Aof%2520available%2520labeled%2520dataset%2520sizes.%2520We%2520also%2520demonstrate%2520improved%2520few-shot%250Alearning%2520with%2520fewer%2520labeled%2520samples%252C%2520a%2520common%2520challenge%2520in%2520medical%2520image%250Aanalyses.%2520We%2520achieve%2520an%2520out-of-box%2520performance%2520comparable%2520to%2520SoTA%2520for%2520most%250Aclinical%2520tasks.%2520The%2520proposed%2520method%2520thus%2520presents%2520a%2520resource-efficient%252C%2520unified%250Aframework%2520for%2520CMR%2520assessment%252C%2520with%2520the%2520potential%2520to%2520accelerate%2520the%2520development%250Aof%2520deep%2520learning-based%2520solutions%2520for%2520image%2520analysis%2520tasks%252C%2520even%2520with%2520few%250Aannotated%2520data%2520available.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2410.01665v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Towards%20a%20vision%20foundation%20model%20for%20comprehensive%20assessment%20of%0A%20%20Cardiac%20MRI&entry.906535625=Athira%20J%20Jacob%20and%20Indraneel%20Borgohain%20and%20Teodora%20Chitiboi%20and%20Puneet%20Sharma%20and%20Dorin%20Comaniciu%20and%20Daniel%20Rueckert&entry.1292438233=%20%20Cardiac%20magnetic%20resonance%20imaging%20%28CMR%29%2C%20considered%20the%20gold%20standard%20for%0Anoninvasive%20cardiac%20assessment%2C%20is%20a%20diverse%20and%20complex%20modality%20requiring%20a%0Awide%20variety%20of%20image%20processing%20tasks%20for%20comprehensive%20assessment%20of%20cardiac%0Amorphology%20and%20function.%20Advances%20in%20deep%20learning%20have%20enabled%20the%20development%0Aof%20state-of-the-art%20%28SoTA%29%20models%20for%20these%20tasks.%20However%2C%20model%20training%20is%0Achallenging%20due%20to%20data%20and%20label%20scarcity%2C%20especially%20in%20the%20less%20common%0Aimaging%20sequences.%20Moreover%2C%20each%20model%20is%20often%20trained%20for%20a%20specific%20task%2C%0Awith%20no%20connection%20between%20related%20tasks.%20In%20this%20work%2C%20we%20introduce%20a%20vision%0Afoundation%20model%20trained%20for%20CMR%20assessment%2C%20that%20is%20trained%20in%20a%0Aself-supervised%20fashion%20on%2036%20million%20CMR%20images.%20We%20then%20finetune%20the%20model%20in%0Asupervised%20way%20for%209%20clinical%20tasks%20typical%20to%20a%20CMR%20workflow%2C%20across%0Aclassification%2C%20segmentation%2C%20landmark%20localization%2C%20and%20pathology%20detection.%0AWe%20demonstrate%20improved%20accuracy%20and%20robustness%20across%20all%20tasks%2C%20over%20a%20range%0Aof%20available%20labeled%20dataset%20sizes.%20We%20also%20demonstrate%20improved%20few-shot%0Alearning%20with%20fewer%20labeled%20samples%2C%20a%20common%20challenge%20in%20medical%20image%0Aanalyses.%20We%20achieve%20an%20out-of-box%20performance%20comparable%20to%20SoTA%20for%20most%0Aclinical%20tasks.%20The%20proposed%20method%20thus%20presents%20a%20resource-efficient%2C%20unified%0Aframework%20for%20CMR%20assessment%2C%20with%20the%20potential%20to%20accelerate%20the%20development%0Aof%20deep%20learning-based%20solutions%20for%20image%20analysis%20tasks%2C%20even%20with%20few%0Aannotated%20data%20available.%0A&entry.1838667208=http%3A//arxiv.org/abs/2410.01665v1&entry.124074799=Read"},
{"title": "Open3DTrack: Towards Open-Vocabulary 3D Multi-Object Tracking", "author": "Ayesha Ishaq and Mohamed El Amine Boudjoghra and Jean Lahoud and Fahad Shahbaz Khan and Salman Khan and Hisham Cholakkal and Rao Muhammad Anwer", "abstract": "  3D multi-object tracking plays a critical role in autonomous driving by\nenabling the real-time monitoring and prediction of multiple objects'\nmovements. Traditional 3D tracking systems are typically constrained by\npredefined object categories, limiting their adaptability to novel, unseen\nobjects in dynamic environments. To address this limitation, we introduce\nopen-vocabulary 3D tracking, which extends the scope of 3D tracking to include\nobjects beyond predefined categories. We formulate the problem of\nopen-vocabulary 3D tracking and introduce dataset splits designed to represent\nvarious open-vocabulary scenarios. We propose a novel approach that integrates\nopen-vocabulary capabilities into a 3D tracking framework, allowing for\ngeneralization to unseen object classes. Our method effectively reduces the\nperformance gap between tracking known and novel objects through strategic\nadaptation. Experimental results demonstrate the robustness and adaptability of\nour method in diverse outdoor driving scenarios. To the best of our knowledge,\nthis work is the first to address open-vocabulary 3D tracking, presenting a\nsignificant advancement for autonomous systems in real-world settings. Code,\ntrained models, and dataset splits are available publicly.\n", "link": "http://arxiv.org/abs/2410.01678v1", "date": "2024-10-02", "relevancy": 2.9072, "topK": [{"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.59}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5772}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5772}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Open3DTrack%3A%20Towards%20Open-Vocabulary%203D%20Multi-Object%20Tracking&body=Title%3A%20Open3DTrack%3A%20Towards%20Open-Vocabulary%203D%20Multi-Object%20Tracking%0AAuthor%3A%20Ayesha%20Ishaq%20and%20Mohamed%20El%20Amine%20Boudjoghra%20and%20Jean%20Lahoud%20and%20Fahad%20Shahbaz%20Khan%20and%20Salman%20Khan%20and%20Hisham%20Cholakkal%20and%20Rao%20Muhammad%20Anwer%0AAbstract%3A%20%20%203D%20multi-object%20tracking%20plays%20a%20critical%20role%20in%20autonomous%20driving%20by%0Aenabling%20the%20real-time%20monitoring%20and%20prediction%20of%20multiple%20objects%27%0Amovements.%20Traditional%203D%20tracking%20systems%20are%20typically%20constrained%20by%0Apredefined%20object%20categories%2C%20limiting%20their%20adaptability%20to%20novel%2C%20unseen%0Aobjects%20in%20dynamic%20environments.%20To%20address%20this%20limitation%2C%20we%20introduce%0Aopen-vocabulary%203D%20tracking%2C%20which%20extends%20the%20scope%20of%203D%20tracking%20to%20include%0Aobjects%20beyond%20predefined%20categories.%20We%20formulate%20the%20problem%20of%0Aopen-vocabulary%203D%20tracking%20and%20introduce%20dataset%20splits%20designed%20to%20represent%0Avarious%20open-vocabulary%20scenarios.%20We%20propose%20a%20novel%20approach%20that%20integrates%0Aopen-vocabulary%20capabilities%20into%20a%203D%20tracking%20framework%2C%20allowing%20for%0Ageneralization%20to%20unseen%20object%20classes.%20Our%20method%20effectively%20reduces%20the%0Aperformance%20gap%20between%20tracking%20known%20and%20novel%20objects%20through%20strategic%0Aadaptation.%20Experimental%20results%20demonstrate%20the%20robustness%20and%20adaptability%20of%0Aour%20method%20in%20diverse%20outdoor%20driving%20scenarios.%20To%20the%20best%20of%20our%20knowledge%2C%0Athis%20work%20is%20the%20first%20to%20address%20open-vocabulary%203D%20tracking%2C%20presenting%20a%0Asignificant%20advancement%20for%20autonomous%20systems%20in%20real-world%20settings.%20Code%2C%0Atrained%20models%2C%20and%20dataset%20splits%20are%20available%20publicly.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2410.01678v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DOpen3DTrack%253A%2520Towards%2520Open-Vocabulary%25203D%2520Multi-Object%2520Tracking%26entry.906535625%3DAyesha%2520Ishaq%2520and%2520Mohamed%2520El%2520Amine%2520Boudjoghra%2520and%2520Jean%2520Lahoud%2520and%2520Fahad%2520Shahbaz%2520Khan%2520and%2520Salman%2520Khan%2520and%2520Hisham%2520Cholakkal%2520and%2520Rao%2520Muhammad%2520Anwer%26entry.1292438233%3D%2520%25203D%2520multi-object%2520tracking%2520plays%2520a%2520critical%2520role%2520in%2520autonomous%2520driving%2520by%250Aenabling%2520the%2520real-time%2520monitoring%2520and%2520prediction%2520of%2520multiple%2520objects%2527%250Amovements.%2520Traditional%25203D%2520tracking%2520systems%2520are%2520typically%2520constrained%2520by%250Apredefined%2520object%2520categories%252C%2520limiting%2520their%2520adaptability%2520to%2520novel%252C%2520unseen%250Aobjects%2520in%2520dynamic%2520environments.%2520To%2520address%2520this%2520limitation%252C%2520we%2520introduce%250Aopen-vocabulary%25203D%2520tracking%252C%2520which%2520extends%2520the%2520scope%2520of%25203D%2520tracking%2520to%2520include%250Aobjects%2520beyond%2520predefined%2520categories.%2520We%2520formulate%2520the%2520problem%2520of%250Aopen-vocabulary%25203D%2520tracking%2520and%2520introduce%2520dataset%2520splits%2520designed%2520to%2520represent%250Avarious%2520open-vocabulary%2520scenarios.%2520We%2520propose%2520a%2520novel%2520approach%2520that%2520integrates%250Aopen-vocabulary%2520capabilities%2520into%2520a%25203D%2520tracking%2520framework%252C%2520allowing%2520for%250Ageneralization%2520to%2520unseen%2520object%2520classes.%2520Our%2520method%2520effectively%2520reduces%2520the%250Aperformance%2520gap%2520between%2520tracking%2520known%2520and%2520novel%2520objects%2520through%2520strategic%250Aadaptation.%2520Experimental%2520results%2520demonstrate%2520the%2520robustness%2520and%2520adaptability%2520of%250Aour%2520method%2520in%2520diverse%2520outdoor%2520driving%2520scenarios.%2520To%2520the%2520best%2520of%2520our%2520knowledge%252C%250Athis%2520work%2520is%2520the%2520first%2520to%2520address%2520open-vocabulary%25203D%2520tracking%252C%2520presenting%2520a%250Asignificant%2520advancement%2520for%2520autonomous%2520systems%2520in%2520real-world%2520settings.%2520Code%252C%250Atrained%2520models%252C%2520and%2520dataset%2520splits%2520are%2520available%2520publicly.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2410.01678v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Open3DTrack%3A%20Towards%20Open-Vocabulary%203D%20Multi-Object%20Tracking&entry.906535625=Ayesha%20Ishaq%20and%20Mohamed%20El%20Amine%20Boudjoghra%20and%20Jean%20Lahoud%20and%20Fahad%20Shahbaz%20Khan%20and%20Salman%20Khan%20and%20Hisham%20Cholakkal%20and%20Rao%20Muhammad%20Anwer&entry.1292438233=%20%203D%20multi-object%20tracking%20plays%20a%20critical%20role%20in%20autonomous%20driving%20by%0Aenabling%20the%20real-time%20monitoring%20and%20prediction%20of%20multiple%20objects%27%0Amovements.%20Traditional%203D%20tracking%20systems%20are%20typically%20constrained%20by%0Apredefined%20object%20categories%2C%20limiting%20their%20adaptability%20to%20novel%2C%20unseen%0Aobjects%20in%20dynamic%20environments.%20To%20address%20this%20limitation%2C%20we%20introduce%0Aopen-vocabulary%203D%20tracking%2C%20which%20extends%20the%20scope%20of%203D%20tracking%20to%20include%0Aobjects%20beyond%20predefined%20categories.%20We%20formulate%20the%20problem%20of%0Aopen-vocabulary%203D%20tracking%20and%20introduce%20dataset%20splits%20designed%20to%20represent%0Avarious%20open-vocabulary%20scenarios.%20We%20propose%20a%20novel%20approach%20that%20integrates%0Aopen-vocabulary%20capabilities%20into%20a%203D%20tracking%20framework%2C%20allowing%20for%0Ageneralization%20to%20unseen%20object%20classes.%20Our%20method%20effectively%20reduces%20the%0Aperformance%20gap%20between%20tracking%20known%20and%20novel%20objects%20through%20strategic%0Aadaptation.%20Experimental%20results%20demonstrate%20the%20robustness%20and%20adaptability%20of%0Aour%20method%20in%20diverse%20outdoor%20driving%20scenarios.%20To%20the%20best%20of%20our%20knowledge%2C%0Athis%20work%20is%20the%20first%20to%20address%20open-vocabulary%203D%20tracking%2C%20presenting%20a%0Asignificant%20advancement%20for%20autonomous%20systems%20in%20real-world%20settings.%20Code%2C%0Atrained%20models%2C%20and%20dataset%20splits%20are%20available%20publicly.%0A&entry.1838667208=http%3A//arxiv.org/abs/2410.01678v1&entry.124074799=Read"},
{"title": "DAViD: Domain Adaptive Visually-Rich Document Understanding with\n  Synthetic Insights", "author": "Yihao Ding and Soyeon Caren Han and Zechuan Li and Hyunsuk Chung", "abstract": "  Visually-Rich Documents (VRDs), encompassing elements like charts, tables,\nand references, convey complex information across various fields. However,\nextracting information from these rich documents is labor-intensive, especially\ngiven their inconsistent formats and domain-specific requirements. While\npretrained models for VRD Understanding have progressed, their reliance on\nlarge, annotated datasets limits scalability. This paper introduces the Domain\nAdaptive Visually-rich Document Understanding (DAViD) framework, which utilises\nmachine-generated synthetic data for domain adaptation. DAViD integrates\nfine-grained and coarse-grained document representation learning and employs\nsynthetic annotations to reduce the need for costly manual labelling. By\nleveraging pretrained models and synthetic data, DAViD achieves competitive\nperformance with minimal annotated datasets. Extensive experiments validate\nDAViD's effectiveness, demonstrating its ability to efficiently adapt to\ndomain-specific VRDU tasks.\n", "link": "http://arxiv.org/abs/2410.01609v1", "date": "2024-10-02", "relevancy": 2.8979, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5967}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5967}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5454}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20DAViD%3A%20Domain%20Adaptive%20Visually-Rich%20Document%20Understanding%20with%0A%20%20Synthetic%20Insights&body=Title%3A%20DAViD%3A%20Domain%20Adaptive%20Visually-Rich%20Document%20Understanding%20with%0A%20%20Synthetic%20Insights%0AAuthor%3A%20Yihao%20Ding%20and%20Soyeon%20Caren%20Han%20and%20Zechuan%20Li%20and%20Hyunsuk%20Chung%0AAbstract%3A%20%20%20Visually-Rich%20Documents%20%28VRDs%29%2C%20encompassing%20elements%20like%20charts%2C%20tables%2C%0Aand%20references%2C%20convey%20complex%20information%20across%20various%20fields.%20However%2C%0Aextracting%20information%20from%20these%20rich%20documents%20is%20labor-intensive%2C%20especially%0Agiven%20their%20inconsistent%20formats%20and%20domain-specific%20requirements.%20While%0Apretrained%20models%20for%20VRD%20Understanding%20have%20progressed%2C%20their%20reliance%20on%0Alarge%2C%20annotated%20datasets%20limits%20scalability.%20This%20paper%20introduces%20the%20Domain%0AAdaptive%20Visually-rich%20Document%20Understanding%20%28DAViD%29%20framework%2C%20which%20utilises%0Amachine-generated%20synthetic%20data%20for%20domain%20adaptation.%20DAViD%20integrates%0Afine-grained%20and%20coarse-grained%20document%20representation%20learning%20and%20employs%0Asynthetic%20annotations%20to%20reduce%20the%20need%20for%20costly%20manual%20labelling.%20By%0Aleveraging%20pretrained%20models%20and%20synthetic%20data%2C%20DAViD%20achieves%20competitive%0Aperformance%20with%20minimal%20annotated%20datasets.%20Extensive%20experiments%20validate%0ADAViD%27s%20effectiveness%2C%20demonstrating%20its%20ability%20to%20efficiently%20adapt%20to%0Adomain-specific%20VRDU%20tasks.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2410.01609v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DDAViD%253A%2520Domain%2520Adaptive%2520Visually-Rich%2520Document%2520Understanding%2520with%250A%2520%2520Synthetic%2520Insights%26entry.906535625%3DYihao%2520Ding%2520and%2520Soyeon%2520Caren%2520Han%2520and%2520Zechuan%2520Li%2520and%2520Hyunsuk%2520Chung%26entry.1292438233%3D%2520%2520Visually-Rich%2520Documents%2520%2528VRDs%2529%252C%2520encompassing%2520elements%2520like%2520charts%252C%2520tables%252C%250Aand%2520references%252C%2520convey%2520complex%2520information%2520across%2520various%2520fields.%2520However%252C%250Aextracting%2520information%2520from%2520these%2520rich%2520documents%2520is%2520labor-intensive%252C%2520especially%250Agiven%2520their%2520inconsistent%2520formats%2520and%2520domain-specific%2520requirements.%2520While%250Apretrained%2520models%2520for%2520VRD%2520Understanding%2520have%2520progressed%252C%2520their%2520reliance%2520on%250Alarge%252C%2520annotated%2520datasets%2520limits%2520scalability.%2520This%2520paper%2520introduces%2520the%2520Domain%250AAdaptive%2520Visually-rich%2520Document%2520Understanding%2520%2528DAViD%2529%2520framework%252C%2520which%2520utilises%250Amachine-generated%2520synthetic%2520data%2520for%2520domain%2520adaptation.%2520DAViD%2520integrates%250Afine-grained%2520and%2520coarse-grained%2520document%2520representation%2520learning%2520and%2520employs%250Asynthetic%2520annotations%2520to%2520reduce%2520the%2520need%2520for%2520costly%2520manual%2520labelling.%2520By%250Aleveraging%2520pretrained%2520models%2520and%2520synthetic%2520data%252C%2520DAViD%2520achieves%2520competitive%250Aperformance%2520with%2520minimal%2520annotated%2520datasets.%2520Extensive%2520experiments%2520validate%250ADAViD%2527s%2520effectiveness%252C%2520demonstrating%2520its%2520ability%2520to%2520efficiently%2520adapt%2520to%250Adomain-specific%2520VRDU%2520tasks.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2410.01609v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=DAViD%3A%20Domain%20Adaptive%20Visually-Rich%20Document%20Understanding%20with%0A%20%20Synthetic%20Insights&entry.906535625=Yihao%20Ding%20and%20Soyeon%20Caren%20Han%20and%20Zechuan%20Li%20and%20Hyunsuk%20Chung&entry.1292438233=%20%20Visually-Rich%20Documents%20%28VRDs%29%2C%20encompassing%20elements%20like%20charts%2C%20tables%2C%0Aand%20references%2C%20convey%20complex%20information%20across%20various%20fields.%20However%2C%0Aextracting%20information%20from%20these%20rich%20documents%20is%20labor-intensive%2C%20especially%0Agiven%20their%20inconsistent%20formats%20and%20domain-specific%20requirements.%20While%0Apretrained%20models%20for%20VRD%20Understanding%20have%20progressed%2C%20their%20reliance%20on%0Alarge%2C%20annotated%20datasets%20limits%20scalability.%20This%20paper%20introduces%20the%20Domain%0AAdaptive%20Visually-rich%20Document%20Understanding%20%28DAViD%29%20framework%2C%20which%20utilises%0Amachine-generated%20synthetic%20data%20for%20domain%20adaptation.%20DAViD%20integrates%0Afine-grained%20and%20coarse-grained%20document%20representation%20learning%20and%20employs%0Asynthetic%20annotations%20to%20reduce%20the%20need%20for%20costly%20manual%20labelling.%20By%0Aleveraging%20pretrained%20models%20and%20synthetic%20data%2C%20DAViD%20achieves%20competitive%0Aperformance%20with%20minimal%20annotated%20datasets.%20Extensive%20experiments%20validate%0ADAViD%27s%20effectiveness%2C%20demonstrating%20its%20ability%20to%20efficiently%20adapt%20to%0Adomain-specific%20VRDU%20tasks.%0A&entry.1838667208=http%3A//arxiv.org/abs/2410.01609v1&entry.124074799=Read"},
{"title": "Geometry-Aware Attenuation Learning for Sparse-View CBCT Reconstruction", "author": "Zhentao Liu and Yu Fang and Changjian Li and Han Wu and Yuan Liu and Dinggang Shen and Zhiming Cui", "abstract": "  Cone Beam Computed Tomography (CBCT) plays a vital role in clinical imaging.\nTraditional methods typically require hundreds of 2D X-ray projections to\nreconstruct a high-quality 3D CBCT image, leading to considerable radiation\nexposure. This has led to a growing interest in sparse-view CBCT reconstruction\nto reduce radiation doses. While recent advances, including deep learning and\nneural rendering algorithms, have made strides in this area, these methods\neither produce unsatisfactory results or suffer from time inefficiency of\nindividual optimization. In this paper, we introduce a novel geometry-aware\nencoder-decoder framework to solve this problem. Our framework starts by\nencoding multi-view 2D features from various 2D X-ray projections with a 2D CNN\nencoder. Leveraging the geometry of CBCT scanning, it then back-projects the\nmulti-view 2D features into the 3D space to formulate a comprehensive\nvolumetric feature map, followed by a 3D CNN decoder to recover 3D CBCT image.\nImportantly, our approach respects the geometric relationship between 3D CBCT\nimage and its 2D X-ray projections during feature back projection stage, and\nenjoys the prior knowledge learned from the data population. This ensures its\nadaptability in dealing with extremly sparse view inputs without individual\ntraining, such as scenarios with only 5 or 10 X-ray projections. Extensive\nevaluations on two simulated datasets and one real-world dataset demonstrate\nexceptional reconstruction quality and time efficiency of our method.\n", "link": "http://arxiv.org/abs/2303.14739v2", "date": "2024-10-02", "relevancy": 2.793, "topK": [{"title": "CAT3D: Create Anything in 3D with Multi-View Diffusion Models", "link": "http://arxiv.org/abs/2405.10314v1", "similarity": 0.5623}, {"title": "CAT3D: Create Anything in 3D with Multi-View Diffusion Models", "link": "http://arxiv.org/abs/2405.10314v1", "similarity": 0.5623}, {"title": "RTG-SLAM: Real-time 3D Reconstruction at Scale using Gaussian Splatting", "link": "http://arxiv.org/abs/2404.19706v1", "similarity": 0.5512}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Geometry-Aware%20Attenuation%20Learning%20for%20Sparse-View%20CBCT%20Reconstruction&body=Title%3A%20Geometry-Aware%20Attenuation%20Learning%20for%20Sparse-View%20CBCT%20Reconstruction%0AAuthor%3A%20Zhentao%20Liu%20and%20Yu%20Fang%20and%20Changjian%20Li%20and%20Han%20Wu%20and%20Yuan%20Liu%20and%20Dinggang%20Shen%20and%20Zhiming%20Cui%0AAbstract%3A%20%20%20Cone%20Beam%20Computed%20Tomography%20%28CBCT%29%20plays%20a%20vital%20role%20in%20clinical%20imaging.%0ATraditional%20methods%20typically%20require%20hundreds%20of%202D%20X-ray%20projections%20to%0Areconstruct%20a%20high-quality%203D%20CBCT%20image%2C%20leading%20to%20considerable%20radiation%0Aexposure.%20This%20has%20led%20to%20a%20growing%20interest%20in%20sparse-view%20CBCT%20reconstruction%0Ato%20reduce%20radiation%20doses.%20While%20recent%20advances%2C%20including%20deep%20learning%20and%0Aneural%20rendering%20algorithms%2C%20have%20made%20strides%20in%20this%20area%2C%20these%20methods%0Aeither%20produce%20unsatisfactory%20results%20or%20suffer%20from%20time%20inefficiency%20of%0Aindividual%20optimization.%20In%20this%20paper%2C%20we%20introduce%20a%20novel%20geometry-aware%0Aencoder-decoder%20framework%20to%20solve%20this%20problem.%20Our%20framework%20starts%20by%0Aencoding%20multi-view%202D%20features%20from%20various%202D%20X-ray%20projections%20with%20a%202D%20CNN%0Aencoder.%20Leveraging%20the%20geometry%20of%20CBCT%20scanning%2C%20it%20then%20back-projects%20the%0Amulti-view%202D%20features%20into%20the%203D%20space%20to%20formulate%20a%20comprehensive%0Avolumetric%20feature%20map%2C%20followed%20by%20a%203D%20CNN%20decoder%20to%20recover%203D%20CBCT%20image.%0AImportantly%2C%20our%20approach%20respects%20the%20geometric%20relationship%20between%203D%20CBCT%0Aimage%20and%20its%202D%20X-ray%20projections%20during%20feature%20back%20projection%20stage%2C%20and%0Aenjoys%20the%20prior%20knowledge%20learned%20from%20the%20data%20population.%20This%20ensures%20its%0Aadaptability%20in%20dealing%20with%20extremly%20sparse%20view%20inputs%20without%20individual%0Atraining%2C%20such%20as%20scenarios%20with%20only%205%20or%2010%20X-ray%20projections.%20Extensive%0Aevaluations%20on%20two%20simulated%20datasets%20and%20one%20real-world%20dataset%20demonstrate%0Aexceptional%20reconstruction%20quality%20and%20time%20efficiency%20of%20our%20method.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2303.14739v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DGeometry-Aware%2520Attenuation%2520Learning%2520for%2520Sparse-View%2520CBCT%2520Reconstruction%26entry.906535625%3DZhentao%2520Liu%2520and%2520Yu%2520Fang%2520and%2520Changjian%2520Li%2520and%2520Han%2520Wu%2520and%2520Yuan%2520Liu%2520and%2520Dinggang%2520Shen%2520and%2520Zhiming%2520Cui%26entry.1292438233%3D%2520%2520Cone%2520Beam%2520Computed%2520Tomography%2520%2528CBCT%2529%2520plays%2520a%2520vital%2520role%2520in%2520clinical%2520imaging.%250ATraditional%2520methods%2520typically%2520require%2520hundreds%2520of%25202D%2520X-ray%2520projections%2520to%250Areconstruct%2520a%2520high-quality%25203D%2520CBCT%2520image%252C%2520leading%2520to%2520considerable%2520radiation%250Aexposure.%2520This%2520has%2520led%2520to%2520a%2520growing%2520interest%2520in%2520sparse-view%2520CBCT%2520reconstruction%250Ato%2520reduce%2520radiation%2520doses.%2520While%2520recent%2520advances%252C%2520including%2520deep%2520learning%2520and%250Aneural%2520rendering%2520algorithms%252C%2520have%2520made%2520strides%2520in%2520this%2520area%252C%2520these%2520methods%250Aeither%2520produce%2520unsatisfactory%2520results%2520or%2520suffer%2520from%2520time%2520inefficiency%2520of%250Aindividual%2520optimization.%2520In%2520this%2520paper%252C%2520we%2520introduce%2520a%2520novel%2520geometry-aware%250Aencoder-decoder%2520framework%2520to%2520solve%2520this%2520problem.%2520Our%2520framework%2520starts%2520by%250Aencoding%2520multi-view%25202D%2520features%2520from%2520various%25202D%2520X-ray%2520projections%2520with%2520a%25202D%2520CNN%250Aencoder.%2520Leveraging%2520the%2520geometry%2520of%2520CBCT%2520scanning%252C%2520it%2520then%2520back-projects%2520the%250Amulti-view%25202D%2520features%2520into%2520the%25203D%2520space%2520to%2520formulate%2520a%2520comprehensive%250Avolumetric%2520feature%2520map%252C%2520followed%2520by%2520a%25203D%2520CNN%2520decoder%2520to%2520recover%25203D%2520CBCT%2520image.%250AImportantly%252C%2520our%2520approach%2520respects%2520the%2520geometric%2520relationship%2520between%25203D%2520CBCT%250Aimage%2520and%2520its%25202D%2520X-ray%2520projections%2520during%2520feature%2520back%2520projection%2520stage%252C%2520and%250Aenjoys%2520the%2520prior%2520knowledge%2520learned%2520from%2520the%2520data%2520population.%2520This%2520ensures%2520its%250Aadaptability%2520in%2520dealing%2520with%2520extremly%2520sparse%2520view%2520inputs%2520without%2520individual%250Atraining%252C%2520such%2520as%2520scenarios%2520with%2520only%25205%2520or%252010%2520X-ray%2520projections.%2520Extensive%250Aevaluations%2520on%2520two%2520simulated%2520datasets%2520and%2520one%2520real-world%2520dataset%2520demonstrate%250Aexceptional%2520reconstruction%2520quality%2520and%2520time%2520efficiency%2520of%2520our%2520method.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2303.14739v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Geometry-Aware%20Attenuation%20Learning%20for%20Sparse-View%20CBCT%20Reconstruction&entry.906535625=Zhentao%20Liu%20and%20Yu%20Fang%20and%20Changjian%20Li%20and%20Han%20Wu%20and%20Yuan%20Liu%20and%20Dinggang%20Shen%20and%20Zhiming%20Cui&entry.1292438233=%20%20Cone%20Beam%20Computed%20Tomography%20%28CBCT%29%20plays%20a%20vital%20role%20in%20clinical%20imaging.%0ATraditional%20methods%20typically%20require%20hundreds%20of%202D%20X-ray%20projections%20to%0Areconstruct%20a%20high-quality%203D%20CBCT%20image%2C%20leading%20to%20considerable%20radiation%0Aexposure.%20This%20has%20led%20to%20a%20growing%20interest%20in%20sparse-view%20CBCT%20reconstruction%0Ato%20reduce%20radiation%20doses.%20While%20recent%20advances%2C%20including%20deep%20learning%20and%0Aneural%20rendering%20algorithms%2C%20have%20made%20strides%20in%20this%20area%2C%20these%20methods%0Aeither%20produce%20unsatisfactory%20results%20or%20suffer%20from%20time%20inefficiency%20of%0Aindividual%20optimization.%20In%20this%20paper%2C%20we%20introduce%20a%20novel%20geometry-aware%0Aencoder-decoder%20framework%20to%20solve%20this%20problem.%20Our%20framework%20starts%20by%0Aencoding%20multi-view%202D%20features%20from%20various%202D%20X-ray%20projections%20with%20a%202D%20CNN%0Aencoder.%20Leveraging%20the%20geometry%20of%20CBCT%20scanning%2C%20it%20then%20back-projects%20the%0Amulti-view%202D%20features%20into%20the%203D%20space%20to%20formulate%20a%20comprehensive%0Avolumetric%20feature%20map%2C%20followed%20by%20a%203D%20CNN%20decoder%20to%20recover%203D%20CBCT%20image.%0AImportantly%2C%20our%20approach%20respects%20the%20geometric%20relationship%20between%203D%20CBCT%0Aimage%20and%20its%202D%20X-ray%20projections%20during%20feature%20back%20projection%20stage%2C%20and%0Aenjoys%20the%20prior%20knowledge%20learned%20from%20the%20data%20population.%20This%20ensures%20its%0Aadaptability%20in%20dealing%20with%20extremly%20sparse%20view%20inputs%20without%20individual%0Atraining%2C%20such%20as%20scenarios%20with%20only%205%20or%2010%20X-ray%20projections.%20Extensive%0Aevaluations%20on%20two%20simulated%20datasets%20and%20one%20real-world%20dataset%20demonstrate%0Aexceptional%20reconstruction%20quality%20and%20time%20efficiency%20of%20our%20method.%0A&entry.1838667208=http%3A//arxiv.org/abs/2303.14739v2&entry.124074799=Read"},
{"title": "EVER: Exact Volumetric Ellipsoid Rendering for Real-time View Synthesis", "author": "Alexander Mai and Peter Hedman and George Kopanas and Dor Verbin and David Futschik and Qiangeng Xu and Falko Kuester and Jon Barron and Yinda Zhang", "abstract": "  We present Exact Volumetric Ellipsoid Rendering (EVER), a method for\nreal-time differentiable emission-only volume rendering. Unlike recent\nrasterization based approach by 3D Gaussian Splatting (3DGS), our primitive\nbased representation allows for exact volume rendering, rather than alpha\ncompositing 3D Gaussian billboards. As such, unlike 3DGS our formulation does\nnot suffer from popping artifacts and view dependent density, but still\nachieves frame rates of $\\sim\\!30$ FPS at 720p on an NVIDIA RTX4090. Since our\napproach is built upon ray tracing it enables effects such as defocus blur and\ncamera distortion (e.g. such as from fisheye cameras), which are difficult to\nachieve by rasterization. We show that our method is more accurate with fewer\nblending issues than 3DGS and follow-up work on view-consistent rendering,\nespecially on the challenging large-scale scenes from the Zip-NeRF dataset\nwhere it achieves sharpest results among real-time techniques.\n", "link": "http://arxiv.org/abs/2410.01804v1", "date": "2024-10-02", "relevancy": 2.7838, "topK": [{"title": "RTG-SLAM: Real-time 3D Reconstruction at Scale using Gaussian Splatting", "link": "http://arxiv.org/abs/2404.19706v1", "similarity": 0.6072}, {"title": "CAT3D: Create Anything in 3D with Multi-View Diffusion Models", "link": "http://arxiv.org/abs/2405.10314v1", "similarity": 0.5315}, {"title": "CAT3D: Create Anything in 3D with Multi-View Diffusion Models", "link": "http://arxiv.org/abs/2405.10314v1", "similarity": 0.5315}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20EVER%3A%20Exact%20Volumetric%20Ellipsoid%20Rendering%20for%20Real-time%20View%20Synthesis&body=Title%3A%20EVER%3A%20Exact%20Volumetric%20Ellipsoid%20Rendering%20for%20Real-time%20View%20Synthesis%0AAuthor%3A%20Alexander%20Mai%20and%20Peter%20Hedman%20and%20George%20Kopanas%20and%20Dor%20Verbin%20and%20David%20Futschik%20and%20Qiangeng%20Xu%20and%20Falko%20Kuester%20and%20Jon%20Barron%20and%20Yinda%20Zhang%0AAbstract%3A%20%20%20We%20present%20Exact%20Volumetric%20Ellipsoid%20Rendering%20%28EVER%29%2C%20a%20method%20for%0Areal-time%20differentiable%20emission-only%20volume%20rendering.%20Unlike%20recent%0Arasterization%20based%20approach%20by%203D%20Gaussian%20Splatting%20%283DGS%29%2C%20our%20primitive%0Abased%20representation%20allows%20for%20exact%20volume%20rendering%2C%20rather%20than%20alpha%0Acompositing%203D%20Gaussian%20billboards.%20As%20such%2C%20unlike%203DGS%20our%20formulation%20does%0Anot%20suffer%20from%20popping%20artifacts%20and%20view%20dependent%20density%2C%20but%20still%0Aachieves%20frame%20rates%20of%20%24%5Csim%5C%2130%24%20FPS%20at%20720p%20on%20an%20NVIDIA%20RTX4090.%20Since%20our%0Aapproach%20is%20built%20upon%20ray%20tracing%20it%20enables%20effects%20such%20as%20defocus%20blur%20and%0Acamera%20distortion%20%28e.g.%20such%20as%20from%20fisheye%20cameras%29%2C%20which%20are%20difficult%20to%0Aachieve%20by%20rasterization.%20We%20show%20that%20our%20method%20is%20more%20accurate%20with%20fewer%0Ablending%20issues%20than%203DGS%20and%20follow-up%20work%20on%20view-consistent%20rendering%2C%0Aespecially%20on%20the%20challenging%20large-scale%20scenes%20from%20the%20Zip-NeRF%20dataset%0Awhere%20it%20achieves%20sharpest%20results%20among%20real-time%20techniques.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2410.01804v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DEVER%253A%2520Exact%2520Volumetric%2520Ellipsoid%2520Rendering%2520for%2520Real-time%2520View%2520Synthesis%26entry.906535625%3DAlexander%2520Mai%2520and%2520Peter%2520Hedman%2520and%2520George%2520Kopanas%2520and%2520Dor%2520Verbin%2520and%2520David%2520Futschik%2520and%2520Qiangeng%2520Xu%2520and%2520Falko%2520Kuester%2520and%2520Jon%2520Barron%2520and%2520Yinda%2520Zhang%26entry.1292438233%3D%2520%2520We%2520present%2520Exact%2520Volumetric%2520Ellipsoid%2520Rendering%2520%2528EVER%2529%252C%2520a%2520method%2520for%250Areal-time%2520differentiable%2520emission-only%2520volume%2520rendering.%2520Unlike%2520recent%250Arasterization%2520based%2520approach%2520by%25203D%2520Gaussian%2520Splatting%2520%25283DGS%2529%252C%2520our%2520primitive%250Abased%2520representation%2520allows%2520for%2520exact%2520volume%2520rendering%252C%2520rather%2520than%2520alpha%250Acompositing%25203D%2520Gaussian%2520billboards.%2520As%2520such%252C%2520unlike%25203DGS%2520our%2520formulation%2520does%250Anot%2520suffer%2520from%2520popping%2520artifacts%2520and%2520view%2520dependent%2520density%252C%2520but%2520still%250Aachieves%2520frame%2520rates%2520of%2520%2524%255Csim%255C%252130%2524%2520FPS%2520at%2520720p%2520on%2520an%2520NVIDIA%2520RTX4090.%2520Since%2520our%250Aapproach%2520is%2520built%2520upon%2520ray%2520tracing%2520it%2520enables%2520effects%2520such%2520as%2520defocus%2520blur%2520and%250Acamera%2520distortion%2520%2528e.g.%2520such%2520as%2520from%2520fisheye%2520cameras%2529%252C%2520which%2520are%2520difficult%2520to%250Aachieve%2520by%2520rasterization.%2520We%2520show%2520that%2520our%2520method%2520is%2520more%2520accurate%2520with%2520fewer%250Ablending%2520issues%2520than%25203DGS%2520and%2520follow-up%2520work%2520on%2520view-consistent%2520rendering%252C%250Aespecially%2520on%2520the%2520challenging%2520large-scale%2520scenes%2520from%2520the%2520Zip-NeRF%2520dataset%250Awhere%2520it%2520achieves%2520sharpest%2520results%2520among%2520real-time%2520techniques.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2410.01804v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=EVER%3A%20Exact%20Volumetric%20Ellipsoid%20Rendering%20for%20Real-time%20View%20Synthesis&entry.906535625=Alexander%20Mai%20and%20Peter%20Hedman%20and%20George%20Kopanas%20and%20Dor%20Verbin%20and%20David%20Futschik%20and%20Qiangeng%20Xu%20and%20Falko%20Kuester%20and%20Jon%20Barron%20and%20Yinda%20Zhang&entry.1292438233=%20%20We%20present%20Exact%20Volumetric%20Ellipsoid%20Rendering%20%28EVER%29%2C%20a%20method%20for%0Areal-time%20differentiable%20emission-only%20volume%20rendering.%20Unlike%20recent%0Arasterization%20based%20approach%20by%203D%20Gaussian%20Splatting%20%283DGS%29%2C%20our%20primitive%0Abased%20representation%20allows%20for%20exact%20volume%20rendering%2C%20rather%20than%20alpha%0Acompositing%203D%20Gaussian%20billboards.%20As%20such%2C%20unlike%203DGS%20our%20formulation%20does%0Anot%20suffer%20from%20popping%20artifacts%20and%20view%20dependent%20density%2C%20but%20still%0Aachieves%20frame%20rates%20of%20%24%5Csim%5C%2130%24%20FPS%20at%20720p%20on%20an%20NVIDIA%20RTX4090.%20Since%20our%0Aapproach%20is%20built%20upon%20ray%20tracing%20it%20enables%20effects%20such%20as%20defocus%20blur%20and%0Acamera%20distortion%20%28e.g.%20such%20as%20from%20fisheye%20cameras%29%2C%20which%20are%20difficult%20to%0Aachieve%20by%20rasterization.%20We%20show%20that%20our%20method%20is%20more%20accurate%20with%20fewer%0Ablending%20issues%20than%203DGS%20and%20follow-up%20work%20on%20view-consistent%20rendering%2C%0Aespecially%20on%20the%20challenging%20large-scale%20scenes%20from%20the%20Zip-NeRF%20dataset%0Awhere%20it%20achieves%20sharpest%20results%20among%20real-time%20techniques.%0A&entry.1838667208=http%3A//arxiv.org/abs/2410.01804v1&entry.124074799=Read"},
{"title": "PerSense: Personalized Instance Segmentation in Dense Images", "author": "Muhammad Ibraheem Siddiqui and Muhammad Umer Sheikh and Hassan Abid and Muhammad Haris Khan", "abstract": "  Leveraging large-scale pre-training, vision foundational models showcase\nnotable performance benefits. Recent segmentation algorithms for natural scenes\nhave advanced significantly. However, existing models still struggle to\nautomatically segment personalized instances in dense and crowded scenarios,\nwhere severe occlusions, scale variations, and background clutter pose a\nchallenge to accurately delineate densely packed instances of the target\nobject. To address this, we propose PerSense, an end-to-end, training-free, and\nmodel-agnostic one-shot framework for Personalized instance Segmentation in\ndense images. We develop a new baseline capable of automatically generating\ninstance-level point prompts via proposing a novel Instance Detection Module\n(IDM) that leverages density maps, encapsulating spatial distribution of\nobjects in an image. To mitigate false positives within generated point\nprompts, we design Point Prompt Selection Module (PPSM). Both IDM and PPSM\ntransform density maps into personalized precise point prompts for\ninstance-level segmentation and offer a seamless integration in our\nmodel-agnostic framework. We also introduce a feedback mechanism which enables\nPerSense to improve the accuracy of density maps by automating the exemplar\nselection process for density map generation. To promote algorithmic advances\nand effective tools for this relatively underexplored task, we introduce\nPerSense-D, a diverse dataset exclusive to personalized instance segmentation\nin dense images. Our extensive experiments establish PerSense superiority in\ndense scenarios by achieving an mIoU of 71.61% on PerSense-D, outperforming\nrecent SOTA models by significant margins of +47.16%, +42.27%, +8.83%, and\n+5.69%. Additionally, our qualitative findings demonstrate the adaptability of\nour framework to images captured in-the-wild.\n", "link": "http://arxiv.org/abs/2405.13518v2", "date": "2024-10-02", "relevancy": 2.7821, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5568}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5568}, {"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.5557}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20PerSense%3A%20Personalized%20Instance%20Segmentation%20in%20Dense%20Images&body=Title%3A%20PerSense%3A%20Personalized%20Instance%20Segmentation%20in%20Dense%20Images%0AAuthor%3A%20Muhammad%20Ibraheem%20Siddiqui%20and%20Muhammad%20Umer%20Sheikh%20and%20Hassan%20Abid%20and%20Muhammad%20Haris%20Khan%0AAbstract%3A%20%20%20Leveraging%20large-scale%20pre-training%2C%20vision%20foundational%20models%20showcase%0Anotable%20performance%20benefits.%20Recent%20segmentation%20algorithms%20for%20natural%20scenes%0Ahave%20advanced%20significantly.%20However%2C%20existing%20models%20still%20struggle%20to%0Aautomatically%20segment%20personalized%20instances%20in%20dense%20and%20crowded%20scenarios%2C%0Awhere%20severe%20occlusions%2C%20scale%20variations%2C%20and%20background%20clutter%20pose%20a%0Achallenge%20to%20accurately%20delineate%20densely%20packed%20instances%20of%20the%20target%0Aobject.%20To%20address%20this%2C%20we%20propose%20PerSense%2C%20an%20end-to-end%2C%20training-free%2C%20and%0Amodel-agnostic%20one-shot%20framework%20for%20Personalized%20instance%20Segmentation%20in%0Adense%20images.%20We%20develop%20a%20new%20baseline%20capable%20of%20automatically%20generating%0Ainstance-level%20point%20prompts%20via%20proposing%20a%20novel%20Instance%20Detection%20Module%0A%28IDM%29%20that%20leverages%20density%20maps%2C%20encapsulating%20spatial%20distribution%20of%0Aobjects%20in%20an%20image.%20To%20mitigate%20false%20positives%20within%20generated%20point%0Aprompts%2C%20we%20design%20Point%20Prompt%20Selection%20Module%20%28PPSM%29.%20Both%20IDM%20and%20PPSM%0Atransform%20density%20maps%20into%20personalized%20precise%20point%20prompts%20for%0Ainstance-level%20segmentation%20and%20offer%20a%20seamless%20integration%20in%20our%0Amodel-agnostic%20framework.%20We%20also%20introduce%20a%20feedback%20mechanism%20which%20enables%0APerSense%20to%20improve%20the%20accuracy%20of%20density%20maps%20by%20automating%20the%20exemplar%0Aselection%20process%20for%20density%20map%20generation.%20To%20promote%20algorithmic%20advances%0Aand%20effective%20tools%20for%20this%20relatively%20underexplored%20task%2C%20we%20introduce%0APerSense-D%2C%20a%20diverse%20dataset%20exclusive%20to%20personalized%20instance%20segmentation%0Ain%20dense%20images.%20Our%20extensive%20experiments%20establish%20PerSense%20superiority%20in%0Adense%20scenarios%20by%20achieving%20an%20mIoU%20of%2071.61%25%20on%20PerSense-D%2C%20outperforming%0Arecent%20SOTA%20models%20by%20significant%20margins%20of%20%2B47.16%25%2C%20%2B42.27%25%2C%20%2B8.83%25%2C%20and%0A%2B5.69%25.%20Additionally%2C%20our%20qualitative%20findings%20demonstrate%20the%20adaptability%20of%0Aour%20framework%20to%20images%20captured%20in-the-wild.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2405.13518v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DPerSense%253A%2520Personalized%2520Instance%2520Segmentation%2520in%2520Dense%2520Images%26entry.906535625%3DMuhammad%2520Ibraheem%2520Siddiqui%2520and%2520Muhammad%2520Umer%2520Sheikh%2520and%2520Hassan%2520Abid%2520and%2520Muhammad%2520Haris%2520Khan%26entry.1292438233%3D%2520%2520Leveraging%2520large-scale%2520pre-training%252C%2520vision%2520foundational%2520models%2520showcase%250Anotable%2520performance%2520benefits.%2520Recent%2520segmentation%2520algorithms%2520for%2520natural%2520scenes%250Ahave%2520advanced%2520significantly.%2520However%252C%2520existing%2520models%2520still%2520struggle%2520to%250Aautomatically%2520segment%2520personalized%2520instances%2520in%2520dense%2520and%2520crowded%2520scenarios%252C%250Awhere%2520severe%2520occlusions%252C%2520scale%2520variations%252C%2520and%2520background%2520clutter%2520pose%2520a%250Achallenge%2520to%2520accurately%2520delineate%2520densely%2520packed%2520instances%2520of%2520the%2520target%250Aobject.%2520To%2520address%2520this%252C%2520we%2520propose%2520PerSense%252C%2520an%2520end-to-end%252C%2520training-free%252C%2520and%250Amodel-agnostic%2520one-shot%2520framework%2520for%2520Personalized%2520instance%2520Segmentation%2520in%250Adense%2520images.%2520We%2520develop%2520a%2520new%2520baseline%2520capable%2520of%2520automatically%2520generating%250Ainstance-level%2520point%2520prompts%2520via%2520proposing%2520a%2520novel%2520Instance%2520Detection%2520Module%250A%2528IDM%2529%2520that%2520leverages%2520density%2520maps%252C%2520encapsulating%2520spatial%2520distribution%2520of%250Aobjects%2520in%2520an%2520image.%2520To%2520mitigate%2520false%2520positives%2520within%2520generated%2520point%250Aprompts%252C%2520we%2520design%2520Point%2520Prompt%2520Selection%2520Module%2520%2528PPSM%2529.%2520Both%2520IDM%2520and%2520PPSM%250Atransform%2520density%2520maps%2520into%2520personalized%2520precise%2520point%2520prompts%2520for%250Ainstance-level%2520segmentation%2520and%2520offer%2520a%2520seamless%2520integration%2520in%2520our%250Amodel-agnostic%2520framework.%2520We%2520also%2520introduce%2520a%2520feedback%2520mechanism%2520which%2520enables%250APerSense%2520to%2520improve%2520the%2520accuracy%2520of%2520density%2520maps%2520by%2520automating%2520the%2520exemplar%250Aselection%2520process%2520for%2520density%2520map%2520generation.%2520To%2520promote%2520algorithmic%2520advances%250Aand%2520effective%2520tools%2520for%2520this%2520relatively%2520underexplored%2520task%252C%2520we%2520introduce%250APerSense-D%252C%2520a%2520diverse%2520dataset%2520exclusive%2520to%2520personalized%2520instance%2520segmentation%250Ain%2520dense%2520images.%2520Our%2520extensive%2520experiments%2520establish%2520PerSense%2520superiority%2520in%250Adense%2520scenarios%2520by%2520achieving%2520an%2520mIoU%2520of%252071.61%2525%2520on%2520PerSense-D%252C%2520outperforming%250Arecent%2520SOTA%2520models%2520by%2520significant%2520margins%2520of%2520%252B47.16%2525%252C%2520%252B42.27%2525%252C%2520%252B8.83%2525%252C%2520and%250A%252B5.69%2525.%2520Additionally%252C%2520our%2520qualitative%2520findings%2520demonstrate%2520the%2520adaptability%2520of%250Aour%2520framework%2520to%2520images%2520captured%2520in-the-wild.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2405.13518v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=PerSense%3A%20Personalized%20Instance%20Segmentation%20in%20Dense%20Images&entry.906535625=Muhammad%20Ibraheem%20Siddiqui%20and%20Muhammad%20Umer%20Sheikh%20and%20Hassan%20Abid%20and%20Muhammad%20Haris%20Khan&entry.1292438233=%20%20Leveraging%20large-scale%20pre-training%2C%20vision%20foundational%20models%20showcase%0Anotable%20performance%20benefits.%20Recent%20segmentation%20algorithms%20for%20natural%20scenes%0Ahave%20advanced%20significantly.%20However%2C%20existing%20models%20still%20struggle%20to%0Aautomatically%20segment%20personalized%20instances%20in%20dense%20and%20crowded%20scenarios%2C%0Awhere%20severe%20occlusions%2C%20scale%20variations%2C%20and%20background%20clutter%20pose%20a%0Achallenge%20to%20accurately%20delineate%20densely%20packed%20instances%20of%20the%20target%0Aobject.%20To%20address%20this%2C%20we%20propose%20PerSense%2C%20an%20end-to-end%2C%20training-free%2C%20and%0Amodel-agnostic%20one-shot%20framework%20for%20Personalized%20instance%20Segmentation%20in%0Adense%20images.%20We%20develop%20a%20new%20baseline%20capable%20of%20automatically%20generating%0Ainstance-level%20point%20prompts%20via%20proposing%20a%20novel%20Instance%20Detection%20Module%0A%28IDM%29%20that%20leverages%20density%20maps%2C%20encapsulating%20spatial%20distribution%20of%0Aobjects%20in%20an%20image.%20To%20mitigate%20false%20positives%20within%20generated%20point%0Aprompts%2C%20we%20design%20Point%20Prompt%20Selection%20Module%20%28PPSM%29.%20Both%20IDM%20and%20PPSM%0Atransform%20density%20maps%20into%20personalized%20precise%20point%20prompts%20for%0Ainstance-level%20segmentation%20and%20offer%20a%20seamless%20integration%20in%20our%0Amodel-agnostic%20framework.%20We%20also%20introduce%20a%20feedback%20mechanism%20which%20enables%0APerSense%20to%20improve%20the%20accuracy%20of%20density%20maps%20by%20automating%20the%20exemplar%0Aselection%20process%20for%20density%20map%20generation.%20To%20promote%20algorithmic%20advances%0Aand%20effective%20tools%20for%20this%20relatively%20underexplored%20task%2C%20we%20introduce%0APerSense-D%2C%20a%20diverse%20dataset%20exclusive%20to%20personalized%20instance%20segmentation%0Ain%20dense%20images.%20Our%20extensive%20experiments%20establish%20PerSense%20superiority%20in%0Adense%20scenarios%20by%20achieving%20an%20mIoU%20of%2071.61%25%20on%20PerSense-D%2C%20outperforming%0Arecent%20SOTA%20models%20by%20significant%20margins%20of%20%2B47.16%25%2C%20%2B42.27%25%2C%20%2B8.83%25%2C%20and%0A%2B5.69%25.%20Additionally%2C%20our%20qualitative%20findings%20demonstrate%20the%20adaptability%20of%0Aour%20framework%20to%20images%20captured%20in-the-wild.%0A&entry.1838667208=http%3A//arxiv.org/abs/2405.13518v2&entry.124074799=Read"},
{"title": "Unveiling the Invisible: Captioning Videos with Metaphors", "author": "Abisek Rajakumar Kalarani and Pushpak Bhattacharyya and Sumit Shekhar", "abstract": "  Metaphors are a common communication tool used in our day-to-day life. The\ndetection and generation of metaphors in textual form have been studied\nextensively but metaphors in other forms have been under-explored. Recent\nstudies have shown that Vision-Language (VL) models cannot understand visual\nmetaphors in memes and adverts. As of now, no probing studies have been done\nthat involve complex language phenomena like metaphors with videos. Hence, we\nintroduce a new VL task of describing the metaphors present in the videos in\nour work. To facilitate this novel task, we construct and release a manually\ncreated dataset with 705 videos and 2115 human-written captions, along with a\nnew metric called Average Concept Distance (ACD), to automatically evaluate the\ncreativity of the metaphors generated. We also propose a novel low-resource\nvideo metaphor captioning system: GIT-LLaVA, which obtains comparable\nperformance to SoTA video language models on the proposed task. We perform a\ncomprehensive analysis of existing video language models on this task and\npublish our dataset, models, and benchmark results to enable further research.\n", "link": "http://arxiv.org/abs/2406.04886v2", "date": "2024-10-02", "relevancy": 2.7704, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5628}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5628}, {"title": "Magic-Me: Identity-Specific Video Customized Diffusion", "link": "http://arxiv.org/abs/2402.09368v1", "similarity": 0.5367}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Unveiling%20the%20Invisible%3A%20Captioning%20Videos%20with%20Metaphors&body=Title%3A%20Unveiling%20the%20Invisible%3A%20Captioning%20Videos%20with%20Metaphors%0AAuthor%3A%20Abisek%20Rajakumar%20Kalarani%20and%20Pushpak%20Bhattacharyya%20and%20Sumit%20Shekhar%0AAbstract%3A%20%20%20Metaphors%20are%20a%20common%20communication%20tool%20used%20in%20our%20day-to-day%20life.%20The%0Adetection%20and%20generation%20of%20metaphors%20in%20textual%20form%20have%20been%20studied%0Aextensively%20but%20metaphors%20in%20other%20forms%20have%20been%20under-explored.%20Recent%0Astudies%20have%20shown%20that%20Vision-Language%20%28VL%29%20models%20cannot%20understand%20visual%0Ametaphors%20in%20memes%20and%20adverts.%20As%20of%20now%2C%20no%20probing%20studies%20have%20been%20done%0Athat%20involve%20complex%20language%20phenomena%20like%20metaphors%20with%20videos.%20Hence%2C%20we%0Aintroduce%20a%20new%20VL%20task%20of%20describing%20the%20metaphors%20present%20in%20the%20videos%20in%0Aour%20work.%20To%20facilitate%20this%20novel%20task%2C%20we%20construct%20and%20release%20a%20manually%0Acreated%20dataset%20with%20705%20videos%20and%202115%20human-written%20captions%2C%20along%20with%20a%0Anew%20metric%20called%20Average%20Concept%20Distance%20%28ACD%29%2C%20to%20automatically%20evaluate%20the%0Acreativity%20of%20the%20metaphors%20generated.%20We%20also%20propose%20a%20novel%20low-resource%0Avideo%20metaphor%20captioning%20system%3A%20GIT-LLaVA%2C%20which%20obtains%20comparable%0Aperformance%20to%20SoTA%20video%20language%20models%20on%20the%20proposed%20task.%20We%20perform%20a%0Acomprehensive%20analysis%20of%20existing%20video%20language%20models%20on%20this%20task%20and%0Apublish%20our%20dataset%2C%20models%2C%20and%20benchmark%20results%20to%20enable%20further%20research.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2406.04886v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DUnveiling%2520the%2520Invisible%253A%2520Captioning%2520Videos%2520with%2520Metaphors%26entry.906535625%3DAbisek%2520Rajakumar%2520Kalarani%2520and%2520Pushpak%2520Bhattacharyya%2520and%2520Sumit%2520Shekhar%26entry.1292438233%3D%2520%2520Metaphors%2520are%2520a%2520common%2520communication%2520tool%2520used%2520in%2520our%2520day-to-day%2520life.%2520The%250Adetection%2520and%2520generation%2520of%2520metaphors%2520in%2520textual%2520form%2520have%2520been%2520studied%250Aextensively%2520but%2520metaphors%2520in%2520other%2520forms%2520have%2520been%2520under-explored.%2520Recent%250Astudies%2520have%2520shown%2520that%2520Vision-Language%2520%2528VL%2529%2520models%2520cannot%2520understand%2520visual%250Ametaphors%2520in%2520memes%2520and%2520adverts.%2520As%2520of%2520now%252C%2520no%2520probing%2520studies%2520have%2520been%2520done%250Athat%2520involve%2520complex%2520language%2520phenomena%2520like%2520metaphors%2520with%2520videos.%2520Hence%252C%2520we%250Aintroduce%2520a%2520new%2520VL%2520task%2520of%2520describing%2520the%2520metaphors%2520present%2520in%2520the%2520videos%2520in%250Aour%2520work.%2520To%2520facilitate%2520this%2520novel%2520task%252C%2520we%2520construct%2520and%2520release%2520a%2520manually%250Acreated%2520dataset%2520with%2520705%2520videos%2520and%25202115%2520human-written%2520captions%252C%2520along%2520with%2520a%250Anew%2520metric%2520called%2520Average%2520Concept%2520Distance%2520%2528ACD%2529%252C%2520to%2520automatically%2520evaluate%2520the%250Acreativity%2520of%2520the%2520metaphors%2520generated.%2520We%2520also%2520propose%2520a%2520novel%2520low-resource%250Avideo%2520metaphor%2520captioning%2520system%253A%2520GIT-LLaVA%252C%2520which%2520obtains%2520comparable%250Aperformance%2520to%2520SoTA%2520video%2520language%2520models%2520on%2520the%2520proposed%2520task.%2520We%2520perform%2520a%250Acomprehensive%2520analysis%2520of%2520existing%2520video%2520language%2520models%2520on%2520this%2520task%2520and%250Apublish%2520our%2520dataset%252C%2520models%252C%2520and%2520benchmark%2520results%2520to%2520enable%2520further%2520research.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2406.04886v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Unveiling%20the%20Invisible%3A%20Captioning%20Videos%20with%20Metaphors&entry.906535625=Abisek%20Rajakumar%20Kalarani%20and%20Pushpak%20Bhattacharyya%20and%20Sumit%20Shekhar&entry.1292438233=%20%20Metaphors%20are%20a%20common%20communication%20tool%20used%20in%20our%20day-to-day%20life.%20The%0Adetection%20and%20generation%20of%20metaphors%20in%20textual%20form%20have%20been%20studied%0Aextensively%20but%20metaphors%20in%20other%20forms%20have%20been%20under-explored.%20Recent%0Astudies%20have%20shown%20that%20Vision-Language%20%28VL%29%20models%20cannot%20understand%20visual%0Ametaphors%20in%20memes%20and%20adverts.%20As%20of%20now%2C%20no%20probing%20studies%20have%20been%20done%0Athat%20involve%20complex%20language%20phenomena%20like%20metaphors%20with%20videos.%20Hence%2C%20we%0Aintroduce%20a%20new%20VL%20task%20of%20describing%20the%20metaphors%20present%20in%20the%20videos%20in%0Aour%20work.%20To%20facilitate%20this%20novel%20task%2C%20we%20construct%20and%20release%20a%20manually%0Acreated%20dataset%20with%20705%20videos%20and%202115%20human-written%20captions%2C%20along%20with%20a%0Anew%20metric%20called%20Average%20Concept%20Distance%20%28ACD%29%2C%20to%20automatically%20evaluate%20the%0Acreativity%20of%20the%20metaphors%20generated.%20We%20also%20propose%20a%20novel%20low-resource%0Avideo%20metaphor%20captioning%20system%3A%20GIT-LLaVA%2C%20which%20obtains%20comparable%0Aperformance%20to%20SoTA%20video%20language%20models%20on%20the%20proposed%20task.%20We%20perform%20a%0Acomprehensive%20analysis%20of%20existing%20video%20language%20models%20on%20this%20task%20and%0Apublish%20our%20dataset%2C%20models%2C%20and%20benchmark%20results%20to%20enable%20further%20research.%0A&entry.1838667208=http%3A//arxiv.org/abs/2406.04886v2&entry.124074799=Read"},
{"title": "SegEarth-OV: Towards Traning-Free Open-Vocabulary Segmentation for\n  Remote Sensing Images", "author": "Kaiyu Li and Ruixun Liu and Xiangyong Cao and Deyu Meng and Zhi Wang", "abstract": "  Remote sensing image plays an irreplaceable role in fields such as\nagriculture, water resources, military, and disaster relief. Pixel-level\ninterpretation is a critical aspect of remote sensing image applications;\nhowever, a prevalent limitation remains the need for extensive manual\nannotation. For this, we try to introduce open-vocabulary semantic segmentation\n(OVSS) into the remote sensing context. However, due to the sensitivity of\nremote sensing images to low-resolution features, distorted target shapes and\nill-fitting boundaries are exhibited in the prediction mask. To tackle this\nissue, we propose a simple and general upsampler, SimFeatUp, to restore lost\nspatial information in deep features in a training-free style. Further, based\non the observation of the abnormal response of local patch tokens to [CLS]\ntoken in CLIP, we propose to execute a straightforward subtraction operation to\nalleviate the global bias in patch tokens. Extensive experiments are conducted\non 17 remote sensing datasets spanning semantic segmentation, building\nextraction, road detection, and flood detection tasks. Our method achieves an\naverage of 5.8%, 8.2%, 4%, and 15.3% improvement over state-of-the-art methods\non 4 tasks. All codes are released.\n\\url{https://earth-insights.github.io/SegEarth-OV}\n", "link": "http://arxiv.org/abs/2410.01768v1", "date": "2024-10-02", "relevancy": 2.7622, "topK": [{"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.5604}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5485}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5485}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20SegEarth-OV%3A%20Towards%20Traning-Free%20Open-Vocabulary%20Segmentation%20for%0A%20%20Remote%20Sensing%20Images&body=Title%3A%20SegEarth-OV%3A%20Towards%20Traning-Free%20Open-Vocabulary%20Segmentation%20for%0A%20%20Remote%20Sensing%20Images%0AAuthor%3A%20Kaiyu%20Li%20and%20Ruixun%20Liu%20and%20Xiangyong%20Cao%20and%20Deyu%20Meng%20and%20Zhi%20Wang%0AAbstract%3A%20%20%20Remote%20sensing%20image%20plays%20an%20irreplaceable%20role%20in%20fields%20such%20as%0Aagriculture%2C%20water%20resources%2C%20military%2C%20and%20disaster%20relief.%20Pixel-level%0Ainterpretation%20is%20a%20critical%20aspect%20of%20remote%20sensing%20image%20applications%3B%0Ahowever%2C%20a%20prevalent%20limitation%20remains%20the%20need%20for%20extensive%20manual%0Aannotation.%20For%20this%2C%20we%20try%20to%20introduce%20open-vocabulary%20semantic%20segmentation%0A%28OVSS%29%20into%20the%20remote%20sensing%20context.%20However%2C%20due%20to%20the%20sensitivity%20of%0Aremote%20sensing%20images%20to%20low-resolution%20features%2C%20distorted%20target%20shapes%20and%0Aill-fitting%20boundaries%20are%20exhibited%20in%20the%20prediction%20mask.%20To%20tackle%20this%0Aissue%2C%20we%20propose%20a%20simple%20and%20general%20upsampler%2C%20SimFeatUp%2C%20to%20restore%20lost%0Aspatial%20information%20in%20deep%20features%20in%20a%20training-free%20style.%20Further%2C%20based%0Aon%20the%20observation%20of%20the%20abnormal%20response%20of%20local%20patch%20tokens%20to%20%5BCLS%5D%0Atoken%20in%20CLIP%2C%20we%20propose%20to%20execute%20a%20straightforward%20subtraction%20operation%20to%0Aalleviate%20the%20global%20bias%20in%20patch%20tokens.%20Extensive%20experiments%20are%20conducted%0Aon%2017%20remote%20sensing%20datasets%20spanning%20semantic%20segmentation%2C%20building%0Aextraction%2C%20road%20detection%2C%20and%20flood%20detection%20tasks.%20Our%20method%20achieves%20an%0Aaverage%20of%205.8%25%2C%208.2%25%2C%204%25%2C%20and%2015.3%25%20improvement%20over%20state-of-the-art%20methods%0Aon%204%20tasks.%20All%20codes%20are%20released.%0A%5Curl%7Bhttps%3A//earth-insights.github.io/SegEarth-OV%7D%0A%0ALink%3A%20http%3A//arxiv.org/abs/2410.01768v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DSegEarth-OV%253A%2520Towards%2520Traning-Free%2520Open-Vocabulary%2520Segmentation%2520for%250A%2520%2520Remote%2520Sensing%2520Images%26entry.906535625%3DKaiyu%2520Li%2520and%2520Ruixun%2520Liu%2520and%2520Xiangyong%2520Cao%2520and%2520Deyu%2520Meng%2520and%2520Zhi%2520Wang%26entry.1292438233%3D%2520%2520Remote%2520sensing%2520image%2520plays%2520an%2520irreplaceable%2520role%2520in%2520fields%2520such%2520as%250Aagriculture%252C%2520water%2520resources%252C%2520military%252C%2520and%2520disaster%2520relief.%2520Pixel-level%250Ainterpretation%2520is%2520a%2520critical%2520aspect%2520of%2520remote%2520sensing%2520image%2520applications%253B%250Ahowever%252C%2520a%2520prevalent%2520limitation%2520remains%2520the%2520need%2520for%2520extensive%2520manual%250Aannotation.%2520For%2520this%252C%2520we%2520try%2520to%2520introduce%2520open-vocabulary%2520semantic%2520segmentation%250A%2528OVSS%2529%2520into%2520the%2520remote%2520sensing%2520context.%2520However%252C%2520due%2520to%2520the%2520sensitivity%2520of%250Aremote%2520sensing%2520images%2520to%2520low-resolution%2520features%252C%2520distorted%2520target%2520shapes%2520and%250Aill-fitting%2520boundaries%2520are%2520exhibited%2520in%2520the%2520prediction%2520mask.%2520To%2520tackle%2520this%250Aissue%252C%2520we%2520propose%2520a%2520simple%2520and%2520general%2520upsampler%252C%2520SimFeatUp%252C%2520to%2520restore%2520lost%250Aspatial%2520information%2520in%2520deep%2520features%2520in%2520a%2520training-free%2520style.%2520Further%252C%2520based%250Aon%2520the%2520observation%2520of%2520the%2520abnormal%2520response%2520of%2520local%2520patch%2520tokens%2520to%2520%255BCLS%255D%250Atoken%2520in%2520CLIP%252C%2520we%2520propose%2520to%2520execute%2520a%2520straightforward%2520subtraction%2520operation%2520to%250Aalleviate%2520the%2520global%2520bias%2520in%2520patch%2520tokens.%2520Extensive%2520experiments%2520are%2520conducted%250Aon%252017%2520remote%2520sensing%2520datasets%2520spanning%2520semantic%2520segmentation%252C%2520building%250Aextraction%252C%2520road%2520detection%252C%2520and%2520flood%2520detection%2520tasks.%2520Our%2520method%2520achieves%2520an%250Aaverage%2520of%25205.8%2525%252C%25208.2%2525%252C%25204%2525%252C%2520and%252015.3%2525%2520improvement%2520over%2520state-of-the-art%2520methods%250Aon%25204%2520tasks.%2520All%2520codes%2520are%2520released.%250A%255Curl%257Bhttps%253A//earth-insights.github.io/SegEarth-OV%257D%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2410.01768v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=SegEarth-OV%3A%20Towards%20Traning-Free%20Open-Vocabulary%20Segmentation%20for%0A%20%20Remote%20Sensing%20Images&entry.906535625=Kaiyu%20Li%20and%20Ruixun%20Liu%20and%20Xiangyong%20Cao%20and%20Deyu%20Meng%20and%20Zhi%20Wang&entry.1292438233=%20%20Remote%20sensing%20image%20plays%20an%20irreplaceable%20role%20in%20fields%20such%20as%0Aagriculture%2C%20water%20resources%2C%20military%2C%20and%20disaster%20relief.%20Pixel-level%0Ainterpretation%20is%20a%20critical%20aspect%20of%20remote%20sensing%20image%20applications%3B%0Ahowever%2C%20a%20prevalent%20limitation%20remains%20the%20need%20for%20extensive%20manual%0Aannotation.%20For%20this%2C%20we%20try%20to%20introduce%20open-vocabulary%20semantic%20segmentation%0A%28OVSS%29%20into%20the%20remote%20sensing%20context.%20However%2C%20due%20to%20the%20sensitivity%20of%0Aremote%20sensing%20images%20to%20low-resolution%20features%2C%20distorted%20target%20shapes%20and%0Aill-fitting%20boundaries%20are%20exhibited%20in%20the%20prediction%20mask.%20To%20tackle%20this%0Aissue%2C%20we%20propose%20a%20simple%20and%20general%20upsampler%2C%20SimFeatUp%2C%20to%20restore%20lost%0Aspatial%20information%20in%20deep%20features%20in%20a%20training-free%20style.%20Further%2C%20based%0Aon%20the%20observation%20of%20the%20abnormal%20response%20of%20local%20patch%20tokens%20to%20%5BCLS%5D%0Atoken%20in%20CLIP%2C%20we%20propose%20to%20execute%20a%20straightforward%20subtraction%20operation%20to%0Aalleviate%20the%20global%20bias%20in%20patch%20tokens.%20Extensive%20experiments%20are%20conducted%0Aon%2017%20remote%20sensing%20datasets%20spanning%20semantic%20segmentation%2C%20building%0Aextraction%2C%20road%20detection%2C%20and%20flood%20detection%20tasks.%20Our%20method%20achieves%20an%0Aaverage%20of%205.8%25%2C%208.2%25%2C%204%25%2C%20and%2015.3%25%20improvement%20over%20state-of-the-art%20methods%0Aon%204%20tasks.%20All%20codes%20are%20released.%0A%5Curl%7Bhttps%3A//earth-insights.github.io/SegEarth-OV%7D%0A&entry.1838667208=http%3A//arxiv.org/abs/2410.01768v1&entry.124074799=Read"},
{"title": "Document-Level In-Context Few-Shot Relation Extraction via Pre-Trained\n  Language Models", "author": "Yilmazcan Ozyurt and Stefan Feuerriegel and Ce Zhang", "abstract": "  Document-level relation extraction aims at inferring structured human\nknowledge from textual documents. State-of-the-art methods for this task use\npre-trained language models (LMs) via fine-tuning, yet fine-tuning is\ncomputationally expensive and cannot adapt to new relation types or new LMs. As\na remedy, we leverage the generalization capabilities of pre-trained LMs and\npresent a novel framework for document-level in-context few-shot relation\nextraction. Our framework has three strengths: it eliminates the need (1) for\nnamed entity recognition and (2) for human annotations of documents, and (3) it\ncan be updated to new LMs without re-training. We evaluate our framework using\nDocRED, the largest publicly available dataset for document-level relation\nextraction, and demonstrate that our framework achieves state-of-the-art\nperformance. We further show that our framework actually performs much better\nthan the original labels from the development set of DocRED. Finally, we\nconduct an extensive benchmark demonstrating the effectiveness of our\nframework, achieving state-of-the-art results across six relation extraction\ndatasets and outperforming more than 30 baseline methods. Unlike our framework,\nthe baseline methods have large computational overhead (e.g., from\nfine-tuning). To the best of our knowledge, we are the first to reformulate the\ndocument-level relation extraction task as a tailored in-context few-shot\nlearning paradigm.\n", "link": "http://arxiv.org/abs/2310.11085v4", "date": "2024-10-02", "relevancy": 2.76, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5658}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5658}, {"title": "Self-supervised Photographic Image Layout Representation Learning", "link": "http://arxiv.org/abs/2403.03740v1", "similarity": 0.5245}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Document-Level%20In-Context%20Few-Shot%20Relation%20Extraction%20via%20Pre-Trained%0A%20%20Language%20Models&body=Title%3A%20Document-Level%20In-Context%20Few-Shot%20Relation%20Extraction%20via%20Pre-Trained%0A%20%20Language%20Models%0AAuthor%3A%20Yilmazcan%20Ozyurt%20and%20Stefan%20Feuerriegel%20and%20Ce%20Zhang%0AAbstract%3A%20%20%20Document-level%20relation%20extraction%20aims%20at%20inferring%20structured%20human%0Aknowledge%20from%20textual%20documents.%20State-of-the-art%20methods%20for%20this%20task%20use%0Apre-trained%20language%20models%20%28LMs%29%20via%20fine-tuning%2C%20yet%20fine-tuning%20is%0Acomputationally%20expensive%20and%20cannot%20adapt%20to%20new%20relation%20types%20or%20new%20LMs.%20As%0Aa%20remedy%2C%20we%20leverage%20the%20generalization%20capabilities%20of%20pre-trained%20LMs%20and%0Apresent%20a%20novel%20framework%20for%20document-level%20in-context%20few-shot%20relation%0Aextraction.%20Our%20framework%20has%20three%20strengths%3A%20it%20eliminates%20the%20need%20%281%29%20for%0Anamed%20entity%20recognition%20and%20%282%29%20for%20human%20annotations%20of%20documents%2C%20and%20%283%29%20it%0Acan%20be%20updated%20to%20new%20LMs%20without%20re-training.%20We%20evaluate%20our%20framework%20using%0ADocRED%2C%20the%20largest%20publicly%20available%20dataset%20for%20document-level%20relation%0Aextraction%2C%20and%20demonstrate%20that%20our%20framework%20achieves%20state-of-the-art%0Aperformance.%20We%20further%20show%20that%20our%20framework%20actually%20performs%20much%20better%0Athan%20the%20original%20labels%20from%20the%20development%20set%20of%20DocRED.%20Finally%2C%20we%0Aconduct%20an%20extensive%20benchmark%20demonstrating%20the%20effectiveness%20of%20our%0Aframework%2C%20achieving%20state-of-the-art%20results%20across%20six%20relation%20extraction%0Adatasets%20and%20outperforming%20more%20than%2030%20baseline%20methods.%20Unlike%20our%20framework%2C%0Athe%20baseline%20methods%20have%20large%20computational%20overhead%20%28e.g.%2C%20from%0Afine-tuning%29.%20To%20the%20best%20of%20our%20knowledge%2C%20we%20are%20the%20first%20to%20reformulate%20the%0Adocument-level%20relation%20extraction%20task%20as%20a%20tailored%20in-context%20few-shot%0Alearning%20paradigm.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2310.11085v4%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DDocument-Level%2520In-Context%2520Few-Shot%2520Relation%2520Extraction%2520via%2520Pre-Trained%250A%2520%2520Language%2520Models%26entry.906535625%3DYilmazcan%2520Ozyurt%2520and%2520Stefan%2520Feuerriegel%2520and%2520Ce%2520Zhang%26entry.1292438233%3D%2520%2520Document-level%2520relation%2520extraction%2520aims%2520at%2520inferring%2520structured%2520human%250Aknowledge%2520from%2520textual%2520documents.%2520State-of-the-art%2520methods%2520for%2520this%2520task%2520use%250Apre-trained%2520language%2520models%2520%2528LMs%2529%2520via%2520fine-tuning%252C%2520yet%2520fine-tuning%2520is%250Acomputationally%2520expensive%2520and%2520cannot%2520adapt%2520to%2520new%2520relation%2520types%2520or%2520new%2520LMs.%2520As%250Aa%2520remedy%252C%2520we%2520leverage%2520the%2520generalization%2520capabilities%2520of%2520pre-trained%2520LMs%2520and%250Apresent%2520a%2520novel%2520framework%2520for%2520document-level%2520in-context%2520few-shot%2520relation%250Aextraction.%2520Our%2520framework%2520has%2520three%2520strengths%253A%2520it%2520eliminates%2520the%2520need%2520%25281%2529%2520for%250Anamed%2520entity%2520recognition%2520and%2520%25282%2529%2520for%2520human%2520annotations%2520of%2520documents%252C%2520and%2520%25283%2529%2520it%250Acan%2520be%2520updated%2520to%2520new%2520LMs%2520without%2520re-training.%2520We%2520evaluate%2520our%2520framework%2520using%250ADocRED%252C%2520the%2520largest%2520publicly%2520available%2520dataset%2520for%2520document-level%2520relation%250Aextraction%252C%2520and%2520demonstrate%2520that%2520our%2520framework%2520achieves%2520state-of-the-art%250Aperformance.%2520We%2520further%2520show%2520that%2520our%2520framework%2520actually%2520performs%2520much%2520better%250Athan%2520the%2520original%2520labels%2520from%2520the%2520development%2520set%2520of%2520DocRED.%2520Finally%252C%2520we%250Aconduct%2520an%2520extensive%2520benchmark%2520demonstrating%2520the%2520effectiveness%2520of%2520our%250Aframework%252C%2520achieving%2520state-of-the-art%2520results%2520across%2520six%2520relation%2520extraction%250Adatasets%2520and%2520outperforming%2520more%2520than%252030%2520baseline%2520methods.%2520Unlike%2520our%2520framework%252C%250Athe%2520baseline%2520methods%2520have%2520large%2520computational%2520overhead%2520%2528e.g.%252C%2520from%250Afine-tuning%2529.%2520To%2520the%2520best%2520of%2520our%2520knowledge%252C%2520we%2520are%2520the%2520first%2520to%2520reformulate%2520the%250Adocument-level%2520relation%2520extraction%2520task%2520as%2520a%2520tailored%2520in-context%2520few-shot%250Alearning%2520paradigm.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2310.11085v4%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Document-Level%20In-Context%20Few-Shot%20Relation%20Extraction%20via%20Pre-Trained%0A%20%20Language%20Models&entry.906535625=Yilmazcan%20Ozyurt%20and%20Stefan%20Feuerriegel%20and%20Ce%20Zhang&entry.1292438233=%20%20Document-level%20relation%20extraction%20aims%20at%20inferring%20structured%20human%0Aknowledge%20from%20textual%20documents.%20State-of-the-art%20methods%20for%20this%20task%20use%0Apre-trained%20language%20models%20%28LMs%29%20via%20fine-tuning%2C%20yet%20fine-tuning%20is%0Acomputationally%20expensive%20and%20cannot%20adapt%20to%20new%20relation%20types%20or%20new%20LMs.%20As%0Aa%20remedy%2C%20we%20leverage%20the%20generalization%20capabilities%20of%20pre-trained%20LMs%20and%0Apresent%20a%20novel%20framework%20for%20document-level%20in-context%20few-shot%20relation%0Aextraction.%20Our%20framework%20has%20three%20strengths%3A%20it%20eliminates%20the%20need%20%281%29%20for%0Anamed%20entity%20recognition%20and%20%282%29%20for%20human%20annotations%20of%20documents%2C%20and%20%283%29%20it%0Acan%20be%20updated%20to%20new%20LMs%20without%20re-training.%20We%20evaluate%20our%20framework%20using%0ADocRED%2C%20the%20largest%20publicly%20available%20dataset%20for%20document-level%20relation%0Aextraction%2C%20and%20demonstrate%20that%20our%20framework%20achieves%20state-of-the-art%0Aperformance.%20We%20further%20show%20that%20our%20framework%20actually%20performs%20much%20better%0Athan%20the%20original%20labels%20from%20the%20development%20set%20of%20DocRED.%20Finally%2C%20we%0Aconduct%20an%20extensive%20benchmark%20demonstrating%20the%20effectiveness%20of%20our%0Aframework%2C%20achieving%20state-of-the-art%20results%20across%20six%20relation%20extraction%0Adatasets%20and%20outperforming%20more%20than%2030%20baseline%20methods.%20Unlike%20our%20framework%2C%0Athe%20baseline%20methods%20have%20large%20computational%20overhead%20%28e.g.%2C%20from%0Afine-tuning%29.%20To%20the%20best%20of%20our%20knowledge%2C%20we%20are%20the%20first%20to%20reformulate%20the%0Adocument-level%20relation%20extraction%20task%20as%20a%20tailored%20in-context%20few-shot%0Alearning%20paradigm.%0A&entry.1838667208=http%3A//arxiv.org/abs/2310.11085v4&entry.124074799=Read"},
{"title": "AgriCLIP: Adapting CLIP for Agriculture and Livestock via\n  Domain-Specialized Cross-Model Alignment", "author": "Umair Nawaz and Muhammad Awais and Hanan Gani and Muzammal Naseer and Fahad Khan and Salman Khan and Rao Muhammad Anwer", "abstract": "  Capitalizing on vast amount of image-text data, large-scale vision-language\npre-training has demonstrated remarkable zero-shot capabilities and has been\nutilized in several applications. However, models trained on general everyday\nweb-crawled data often exhibit sub-optimal performance for specialized domains,\nlikely due to domain shift. Recent works have tackled this problem for some\ndomains (e.g., healthcare) by constructing domain-specialized image-text data.\nHowever, constructing a dedicated large-scale image-text dataset for\nsustainable area of agriculture and livestock is still open to research.\nFurther, this domain desires fine-grained feature learning due to the subtle\nnature of the downstream tasks (e.g, nutrient deficiency detection, livestock\nbreed classification). To address this we present AgriCLIP, a vision-language\nfoundational model dedicated to the domain of agriculture and livestock. First,\nwe propose a large-scale dataset, named ALive, that leverages customized prompt\ngeneration strategy to overcome the scarcity of expert annotations. Our ALive\ndataset covers crops, livestock, and fishery, with around 600,000 image-text\npairs. Second, we propose a training pipeline that integrates both contrastive\nand self-supervised learning to learn both global semantic and local\nfine-grained domain-specialized features. Experiments on diverse set of 20\ndownstream tasks demonstrate the effectiveness of AgriCLIP framework, achieving\nan absolute gain of 7.8\\% in terms of average zero-shot classification\naccuracy, over the standard CLIP adaptation via domain-specialized ALive\ndataset. Our ALive dataset and code can be accessible at\n\\href{https://github.com/umair1221/AgriCLIP/tree/main}{Github}.\n", "link": "http://arxiv.org/abs/2410.01407v1", "date": "2024-10-02", "relevancy": 2.7417, "topK": [{"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.6105}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5173}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5173}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20AgriCLIP%3A%20Adapting%20CLIP%20for%20Agriculture%20and%20Livestock%20via%0A%20%20Domain-Specialized%20Cross-Model%20Alignment&body=Title%3A%20AgriCLIP%3A%20Adapting%20CLIP%20for%20Agriculture%20and%20Livestock%20via%0A%20%20Domain-Specialized%20Cross-Model%20Alignment%0AAuthor%3A%20Umair%20Nawaz%20and%20Muhammad%20Awais%20and%20Hanan%20Gani%20and%20Muzammal%20Naseer%20and%20Fahad%20Khan%20and%20Salman%20Khan%20and%20Rao%20Muhammad%20Anwer%0AAbstract%3A%20%20%20Capitalizing%20on%20vast%20amount%20of%20image-text%20data%2C%20large-scale%20vision-language%0Apre-training%20has%20demonstrated%20remarkable%20zero-shot%20capabilities%20and%20has%20been%0Autilized%20in%20several%20applications.%20However%2C%20models%20trained%20on%20general%20everyday%0Aweb-crawled%20data%20often%20exhibit%20sub-optimal%20performance%20for%20specialized%20domains%2C%0Alikely%20due%20to%20domain%20shift.%20Recent%20works%20have%20tackled%20this%20problem%20for%20some%0Adomains%20%28e.g.%2C%20healthcare%29%20by%20constructing%20domain-specialized%20image-text%20data.%0AHowever%2C%20constructing%20a%20dedicated%20large-scale%20image-text%20dataset%20for%0Asustainable%20area%20of%20agriculture%20and%20livestock%20is%20still%20open%20to%20research.%0AFurther%2C%20this%20domain%20desires%20fine-grained%20feature%20learning%20due%20to%20the%20subtle%0Anature%20of%20the%20downstream%20tasks%20%28e.g%2C%20nutrient%20deficiency%20detection%2C%20livestock%0Abreed%20classification%29.%20To%20address%20this%20we%20present%20AgriCLIP%2C%20a%20vision-language%0Afoundational%20model%20dedicated%20to%20the%20domain%20of%20agriculture%20and%20livestock.%20First%2C%0Awe%20propose%20a%20large-scale%20dataset%2C%20named%20ALive%2C%20that%20leverages%20customized%20prompt%0Ageneration%20strategy%20to%20overcome%20the%20scarcity%20of%20expert%20annotations.%20Our%20ALive%0Adataset%20covers%20crops%2C%20livestock%2C%20and%20fishery%2C%20with%20around%20600%2C000%20image-text%0Apairs.%20Second%2C%20we%20propose%20a%20training%20pipeline%20that%20integrates%20both%20contrastive%0Aand%20self-supervised%20learning%20to%20learn%20both%20global%20semantic%20and%20local%0Afine-grained%20domain-specialized%20features.%20Experiments%20on%20diverse%20set%20of%2020%0Adownstream%20tasks%20demonstrate%20the%20effectiveness%20of%20AgriCLIP%20framework%2C%20achieving%0Aan%20absolute%20gain%20of%207.8%5C%25%20in%20terms%20of%20average%20zero-shot%20classification%0Aaccuracy%2C%20over%20the%20standard%20CLIP%20adaptation%20via%20domain-specialized%20ALive%0Adataset.%20Our%20ALive%20dataset%20and%20code%20can%20be%20accessible%20at%0A%5Chref%7Bhttps%3A//github.com/umair1221/AgriCLIP/tree/main%7D%7BGithub%7D.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2410.01407v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DAgriCLIP%253A%2520Adapting%2520CLIP%2520for%2520Agriculture%2520and%2520Livestock%2520via%250A%2520%2520Domain-Specialized%2520Cross-Model%2520Alignment%26entry.906535625%3DUmair%2520Nawaz%2520and%2520Muhammad%2520Awais%2520and%2520Hanan%2520Gani%2520and%2520Muzammal%2520Naseer%2520and%2520Fahad%2520Khan%2520and%2520Salman%2520Khan%2520and%2520Rao%2520Muhammad%2520Anwer%26entry.1292438233%3D%2520%2520Capitalizing%2520on%2520vast%2520amount%2520of%2520image-text%2520data%252C%2520large-scale%2520vision-language%250Apre-training%2520has%2520demonstrated%2520remarkable%2520zero-shot%2520capabilities%2520and%2520has%2520been%250Autilized%2520in%2520several%2520applications.%2520However%252C%2520models%2520trained%2520on%2520general%2520everyday%250Aweb-crawled%2520data%2520often%2520exhibit%2520sub-optimal%2520performance%2520for%2520specialized%2520domains%252C%250Alikely%2520due%2520to%2520domain%2520shift.%2520Recent%2520works%2520have%2520tackled%2520this%2520problem%2520for%2520some%250Adomains%2520%2528e.g.%252C%2520healthcare%2529%2520by%2520constructing%2520domain-specialized%2520image-text%2520data.%250AHowever%252C%2520constructing%2520a%2520dedicated%2520large-scale%2520image-text%2520dataset%2520for%250Asustainable%2520area%2520of%2520agriculture%2520and%2520livestock%2520is%2520still%2520open%2520to%2520research.%250AFurther%252C%2520this%2520domain%2520desires%2520fine-grained%2520feature%2520learning%2520due%2520to%2520the%2520subtle%250Anature%2520of%2520the%2520downstream%2520tasks%2520%2528e.g%252C%2520nutrient%2520deficiency%2520detection%252C%2520livestock%250Abreed%2520classification%2529.%2520To%2520address%2520this%2520we%2520present%2520AgriCLIP%252C%2520a%2520vision-language%250Afoundational%2520model%2520dedicated%2520to%2520the%2520domain%2520of%2520agriculture%2520and%2520livestock.%2520First%252C%250Awe%2520propose%2520a%2520large-scale%2520dataset%252C%2520named%2520ALive%252C%2520that%2520leverages%2520customized%2520prompt%250Ageneration%2520strategy%2520to%2520overcome%2520the%2520scarcity%2520of%2520expert%2520annotations.%2520Our%2520ALive%250Adataset%2520covers%2520crops%252C%2520livestock%252C%2520and%2520fishery%252C%2520with%2520around%2520600%252C000%2520image-text%250Apairs.%2520Second%252C%2520we%2520propose%2520a%2520training%2520pipeline%2520that%2520integrates%2520both%2520contrastive%250Aand%2520self-supervised%2520learning%2520to%2520learn%2520both%2520global%2520semantic%2520and%2520local%250Afine-grained%2520domain-specialized%2520features.%2520Experiments%2520on%2520diverse%2520set%2520of%252020%250Adownstream%2520tasks%2520demonstrate%2520the%2520effectiveness%2520of%2520AgriCLIP%2520framework%252C%2520achieving%250Aan%2520absolute%2520gain%2520of%25207.8%255C%2525%2520in%2520terms%2520of%2520average%2520zero-shot%2520classification%250Aaccuracy%252C%2520over%2520the%2520standard%2520CLIP%2520adaptation%2520via%2520domain-specialized%2520ALive%250Adataset.%2520Our%2520ALive%2520dataset%2520and%2520code%2520can%2520be%2520accessible%2520at%250A%255Chref%257Bhttps%253A//github.com/umair1221/AgriCLIP/tree/main%257D%257BGithub%257D.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2410.01407v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=AgriCLIP%3A%20Adapting%20CLIP%20for%20Agriculture%20and%20Livestock%20via%0A%20%20Domain-Specialized%20Cross-Model%20Alignment&entry.906535625=Umair%20Nawaz%20and%20Muhammad%20Awais%20and%20Hanan%20Gani%20and%20Muzammal%20Naseer%20and%20Fahad%20Khan%20and%20Salman%20Khan%20and%20Rao%20Muhammad%20Anwer&entry.1292438233=%20%20Capitalizing%20on%20vast%20amount%20of%20image-text%20data%2C%20large-scale%20vision-language%0Apre-training%20has%20demonstrated%20remarkable%20zero-shot%20capabilities%20and%20has%20been%0Autilized%20in%20several%20applications.%20However%2C%20models%20trained%20on%20general%20everyday%0Aweb-crawled%20data%20often%20exhibit%20sub-optimal%20performance%20for%20specialized%20domains%2C%0Alikely%20due%20to%20domain%20shift.%20Recent%20works%20have%20tackled%20this%20problem%20for%20some%0Adomains%20%28e.g.%2C%20healthcare%29%20by%20constructing%20domain-specialized%20image-text%20data.%0AHowever%2C%20constructing%20a%20dedicated%20large-scale%20image-text%20dataset%20for%0Asustainable%20area%20of%20agriculture%20and%20livestock%20is%20still%20open%20to%20research.%0AFurther%2C%20this%20domain%20desires%20fine-grained%20feature%20learning%20due%20to%20the%20subtle%0Anature%20of%20the%20downstream%20tasks%20%28e.g%2C%20nutrient%20deficiency%20detection%2C%20livestock%0Abreed%20classification%29.%20To%20address%20this%20we%20present%20AgriCLIP%2C%20a%20vision-language%0Afoundational%20model%20dedicated%20to%20the%20domain%20of%20agriculture%20and%20livestock.%20First%2C%0Awe%20propose%20a%20large-scale%20dataset%2C%20named%20ALive%2C%20that%20leverages%20customized%20prompt%0Ageneration%20strategy%20to%20overcome%20the%20scarcity%20of%20expert%20annotations.%20Our%20ALive%0Adataset%20covers%20crops%2C%20livestock%2C%20and%20fishery%2C%20with%20around%20600%2C000%20image-text%0Apairs.%20Second%2C%20we%20propose%20a%20training%20pipeline%20that%20integrates%20both%20contrastive%0Aand%20self-supervised%20learning%20to%20learn%20both%20global%20semantic%20and%20local%0Afine-grained%20domain-specialized%20features.%20Experiments%20on%20diverse%20set%20of%2020%0Adownstream%20tasks%20demonstrate%20the%20effectiveness%20of%20AgriCLIP%20framework%2C%20achieving%0Aan%20absolute%20gain%20of%207.8%5C%25%20in%20terms%20of%20average%20zero-shot%20classification%0Aaccuracy%2C%20over%20the%20standard%20CLIP%20adaptation%20via%20domain-specialized%20ALive%0Adataset.%20Our%20ALive%20dataset%20and%20code%20can%20be%20accessible%20at%0A%5Chref%7Bhttps%3A//github.com/umair1221/AgriCLIP/tree/main%7D%7BGithub%7D.%0A&entry.1838667208=http%3A//arxiv.org/abs/2410.01407v1&entry.124074799=Read"},
{"title": "Multi-Scale Fusion for Object Representation", "author": "Rongzhen Zhao and Vivienne Wang and Juho Kannala and Joni Pajarinen", "abstract": "  Representing images or videos as object-level feature vectors, rather than\npixel-level feature maps, facilitates advanced visual tasks. Object-Centric\nLearning (OCL) primarily achieves this by reconstructing the input under the\nguidance of Variational Autoencoder (VAE) intermediate representation to drive\nso-called \\textit{slots} to aggregate as much object information as possible.\nHowever, existing VAE guidance does not explicitly address that objects can\nvary in pixel sizes while models typically excel at specific pattern scales. We\npropose \\textit{Multi-Scale Fusion} (MSF) to enhance VAE guidance for OCL\ntraining. To ensure objects of all sizes fall within VAE's comfort zone, we\nadopt the \\textit{image pyramid}, which produces intermediate representations\nat multiple scales; To foster scale-invariance/variance in object super-pixels,\nwe devise \\textit{inter}/\\textit{intra-scale fusion}, which augments\nlow-quality object super-pixels of one scale with corresponding high-quality\nsuper-pixels from another scale. On standard OCL benchmarks, our technique\nimproves mainstream methods, including state-of-the-art diffusion-based ones.\nThe source code is available in the supplemental material.\n", "link": "http://arxiv.org/abs/2410.01539v1", "date": "2024-10-02", "relevancy": 2.7311, "topK": [{"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.57}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5343}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5343}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Multi-Scale%20Fusion%20for%20Object%20Representation&body=Title%3A%20Multi-Scale%20Fusion%20for%20Object%20Representation%0AAuthor%3A%20Rongzhen%20Zhao%20and%20Vivienne%20Wang%20and%20Juho%20Kannala%20and%20Joni%20Pajarinen%0AAbstract%3A%20%20%20Representing%20images%20or%20videos%20as%20object-level%20feature%20vectors%2C%20rather%20than%0Apixel-level%20feature%20maps%2C%20facilitates%20advanced%20visual%20tasks.%20Object-Centric%0ALearning%20%28OCL%29%20primarily%20achieves%20this%20by%20reconstructing%20the%20input%20under%20the%0Aguidance%20of%20Variational%20Autoencoder%20%28VAE%29%20intermediate%20representation%20to%20drive%0Aso-called%20%5Ctextit%7Bslots%7D%20to%20aggregate%20as%20much%20object%20information%20as%20possible.%0AHowever%2C%20existing%20VAE%20guidance%20does%20not%20explicitly%20address%20that%20objects%20can%0Avary%20in%20pixel%20sizes%20while%20models%20typically%20excel%20at%20specific%20pattern%20scales.%20We%0Apropose%20%5Ctextit%7BMulti-Scale%20Fusion%7D%20%28MSF%29%20to%20enhance%20VAE%20guidance%20for%20OCL%0Atraining.%20To%20ensure%20objects%20of%20all%20sizes%20fall%20within%20VAE%27s%20comfort%20zone%2C%20we%0Aadopt%20the%20%5Ctextit%7Bimage%20pyramid%7D%2C%20which%20produces%20intermediate%20representations%0Aat%20multiple%20scales%3B%20To%20foster%20scale-invariance/variance%20in%20object%20super-pixels%2C%0Awe%20devise%20%5Ctextit%7Binter%7D/%5Ctextit%7Bintra-scale%20fusion%7D%2C%20which%20augments%0Alow-quality%20object%20super-pixels%20of%20one%20scale%20with%20corresponding%20high-quality%0Asuper-pixels%20from%20another%20scale.%20On%20standard%20OCL%20benchmarks%2C%20our%20technique%0Aimproves%20mainstream%20methods%2C%20including%20state-of-the-art%20diffusion-based%20ones.%0AThe%20source%20code%20is%20available%20in%20the%20supplemental%20material.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2410.01539v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DMulti-Scale%2520Fusion%2520for%2520Object%2520Representation%26entry.906535625%3DRongzhen%2520Zhao%2520and%2520Vivienne%2520Wang%2520and%2520Juho%2520Kannala%2520and%2520Joni%2520Pajarinen%26entry.1292438233%3D%2520%2520Representing%2520images%2520or%2520videos%2520as%2520object-level%2520feature%2520vectors%252C%2520rather%2520than%250Apixel-level%2520feature%2520maps%252C%2520facilitates%2520advanced%2520visual%2520tasks.%2520Object-Centric%250ALearning%2520%2528OCL%2529%2520primarily%2520achieves%2520this%2520by%2520reconstructing%2520the%2520input%2520under%2520the%250Aguidance%2520of%2520Variational%2520Autoencoder%2520%2528VAE%2529%2520intermediate%2520representation%2520to%2520drive%250Aso-called%2520%255Ctextit%257Bslots%257D%2520to%2520aggregate%2520as%2520much%2520object%2520information%2520as%2520possible.%250AHowever%252C%2520existing%2520VAE%2520guidance%2520does%2520not%2520explicitly%2520address%2520that%2520objects%2520can%250Avary%2520in%2520pixel%2520sizes%2520while%2520models%2520typically%2520excel%2520at%2520specific%2520pattern%2520scales.%2520We%250Apropose%2520%255Ctextit%257BMulti-Scale%2520Fusion%257D%2520%2528MSF%2529%2520to%2520enhance%2520VAE%2520guidance%2520for%2520OCL%250Atraining.%2520To%2520ensure%2520objects%2520of%2520all%2520sizes%2520fall%2520within%2520VAE%2527s%2520comfort%2520zone%252C%2520we%250Aadopt%2520the%2520%255Ctextit%257Bimage%2520pyramid%257D%252C%2520which%2520produces%2520intermediate%2520representations%250Aat%2520multiple%2520scales%253B%2520To%2520foster%2520scale-invariance/variance%2520in%2520object%2520super-pixels%252C%250Awe%2520devise%2520%255Ctextit%257Binter%257D/%255Ctextit%257Bintra-scale%2520fusion%257D%252C%2520which%2520augments%250Alow-quality%2520object%2520super-pixels%2520of%2520one%2520scale%2520with%2520corresponding%2520high-quality%250Asuper-pixels%2520from%2520another%2520scale.%2520On%2520standard%2520OCL%2520benchmarks%252C%2520our%2520technique%250Aimproves%2520mainstream%2520methods%252C%2520including%2520state-of-the-art%2520diffusion-based%2520ones.%250AThe%2520source%2520code%2520is%2520available%2520in%2520the%2520supplemental%2520material.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2410.01539v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Multi-Scale%20Fusion%20for%20Object%20Representation&entry.906535625=Rongzhen%20Zhao%20and%20Vivienne%20Wang%20and%20Juho%20Kannala%20and%20Joni%20Pajarinen&entry.1292438233=%20%20Representing%20images%20or%20videos%20as%20object-level%20feature%20vectors%2C%20rather%20than%0Apixel-level%20feature%20maps%2C%20facilitates%20advanced%20visual%20tasks.%20Object-Centric%0ALearning%20%28OCL%29%20primarily%20achieves%20this%20by%20reconstructing%20the%20input%20under%20the%0Aguidance%20of%20Variational%20Autoencoder%20%28VAE%29%20intermediate%20representation%20to%20drive%0Aso-called%20%5Ctextit%7Bslots%7D%20to%20aggregate%20as%20much%20object%20information%20as%20possible.%0AHowever%2C%20existing%20VAE%20guidance%20does%20not%20explicitly%20address%20that%20objects%20can%0Avary%20in%20pixel%20sizes%20while%20models%20typically%20excel%20at%20specific%20pattern%20scales.%20We%0Apropose%20%5Ctextit%7BMulti-Scale%20Fusion%7D%20%28MSF%29%20to%20enhance%20VAE%20guidance%20for%20OCL%0Atraining.%20To%20ensure%20objects%20of%20all%20sizes%20fall%20within%20VAE%27s%20comfort%20zone%2C%20we%0Aadopt%20the%20%5Ctextit%7Bimage%20pyramid%7D%2C%20which%20produces%20intermediate%20representations%0Aat%20multiple%20scales%3B%20To%20foster%20scale-invariance/variance%20in%20object%20super-pixels%2C%0Awe%20devise%20%5Ctextit%7Binter%7D/%5Ctextit%7Bintra-scale%20fusion%7D%2C%20which%20augments%0Alow-quality%20object%20super-pixels%20of%20one%20scale%20with%20corresponding%20high-quality%0Asuper-pixels%20from%20another%20scale.%20On%20standard%20OCL%20benchmarks%2C%20our%20technique%0Aimproves%20mainstream%20methods%2C%20including%20state-of-the-art%20diffusion-based%20ones.%0AThe%20source%20code%20is%20available%20in%20the%20supplemental%20material.%0A&entry.1838667208=http%3A//arxiv.org/abs/2410.01539v1&entry.124074799=Read"},
{"title": "LEGO: Learnable Expansion of Graph Operators for Multi-Modal Feature\n  Fusion", "author": "Dexuan Ding and Lei Wang and Liyun Zhu and Tom Gedeon and Piotr Koniusz", "abstract": "  In computer vision tasks, features often come from diverse representations,\ndomains, and modalities, such as text, images, and videos. Effectively fusing\nthese features is essential for robust performance, especially with the\navailability of powerful pre-trained models like vision-language models.\nHowever, common fusion methods, such as concatenation, element-wise operations,\nand non-linear techniques, often fail to capture structural relationships, deep\nfeature interactions, and suffer from inefficiency or misalignment of features\nacross domains. In this paper, we shift from high-dimensional feature space to\na lower-dimensional, interpretable graph space by constructing similarity\ngraphs that encode feature relationships at different levels, e.g., clip,\nframe, patch, token, etc. To capture deeper interactions, we use graph power\nexpansions and introduce a learnable graph fusion operator to combine these\ngraph powers for more effective fusion. Our approach is relationship-centric,\noperates in a homogeneous space, and is mathematically principled, resembling\nelement-wise similarity score aggregation via multilinear polynomials. We\ndemonstrate the effectiveness of our graph-based fusion method on video anomaly\ndetection, showing strong performance across multi-representational,\nmulti-modal, and multi-domain feature fusion tasks.\n", "link": "http://arxiv.org/abs/2410.01506v1", "date": "2024-10-02", "relevancy": 2.721, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5449}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5449}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5427}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20LEGO%3A%20Learnable%20Expansion%20of%20Graph%20Operators%20for%20Multi-Modal%20Feature%0A%20%20Fusion&body=Title%3A%20LEGO%3A%20Learnable%20Expansion%20of%20Graph%20Operators%20for%20Multi-Modal%20Feature%0A%20%20Fusion%0AAuthor%3A%20Dexuan%20Ding%20and%20Lei%20Wang%20and%20Liyun%20Zhu%20and%20Tom%20Gedeon%20and%20Piotr%20Koniusz%0AAbstract%3A%20%20%20In%20computer%20vision%20tasks%2C%20features%20often%20come%20from%20diverse%20representations%2C%0Adomains%2C%20and%20modalities%2C%20such%20as%20text%2C%20images%2C%20and%20videos.%20Effectively%20fusing%0Athese%20features%20is%20essential%20for%20robust%20performance%2C%20especially%20with%20the%0Aavailability%20of%20powerful%20pre-trained%20models%20like%20vision-language%20models.%0AHowever%2C%20common%20fusion%20methods%2C%20such%20as%20concatenation%2C%20element-wise%20operations%2C%0Aand%20non-linear%20techniques%2C%20often%20fail%20to%20capture%20structural%20relationships%2C%20deep%0Afeature%20interactions%2C%20and%20suffer%20from%20inefficiency%20or%20misalignment%20of%20features%0Aacross%20domains.%20In%20this%20paper%2C%20we%20shift%20from%20high-dimensional%20feature%20space%20to%0Aa%20lower-dimensional%2C%20interpretable%20graph%20space%20by%20constructing%20similarity%0Agraphs%20that%20encode%20feature%20relationships%20at%20different%20levels%2C%20e.g.%2C%20clip%2C%0Aframe%2C%20patch%2C%20token%2C%20etc.%20To%20capture%20deeper%20interactions%2C%20we%20use%20graph%20power%0Aexpansions%20and%20introduce%20a%20learnable%20graph%20fusion%20operator%20to%20combine%20these%0Agraph%20powers%20for%20more%20effective%20fusion.%20Our%20approach%20is%20relationship-centric%2C%0Aoperates%20in%20a%20homogeneous%20space%2C%20and%20is%20mathematically%20principled%2C%20resembling%0Aelement-wise%20similarity%20score%20aggregation%20via%20multilinear%20polynomials.%20We%0Ademonstrate%20the%20effectiveness%20of%20our%20graph-based%20fusion%20method%20on%20video%20anomaly%0Adetection%2C%20showing%20strong%20performance%20across%20multi-representational%2C%0Amulti-modal%2C%20and%20multi-domain%20feature%20fusion%20tasks.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2410.01506v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DLEGO%253A%2520Learnable%2520Expansion%2520of%2520Graph%2520Operators%2520for%2520Multi-Modal%2520Feature%250A%2520%2520Fusion%26entry.906535625%3DDexuan%2520Ding%2520and%2520Lei%2520Wang%2520and%2520Liyun%2520Zhu%2520and%2520Tom%2520Gedeon%2520and%2520Piotr%2520Koniusz%26entry.1292438233%3D%2520%2520In%2520computer%2520vision%2520tasks%252C%2520features%2520often%2520come%2520from%2520diverse%2520representations%252C%250Adomains%252C%2520and%2520modalities%252C%2520such%2520as%2520text%252C%2520images%252C%2520and%2520videos.%2520Effectively%2520fusing%250Athese%2520features%2520is%2520essential%2520for%2520robust%2520performance%252C%2520especially%2520with%2520the%250Aavailability%2520of%2520powerful%2520pre-trained%2520models%2520like%2520vision-language%2520models.%250AHowever%252C%2520common%2520fusion%2520methods%252C%2520such%2520as%2520concatenation%252C%2520element-wise%2520operations%252C%250Aand%2520non-linear%2520techniques%252C%2520often%2520fail%2520to%2520capture%2520structural%2520relationships%252C%2520deep%250Afeature%2520interactions%252C%2520and%2520suffer%2520from%2520inefficiency%2520or%2520misalignment%2520of%2520features%250Aacross%2520domains.%2520In%2520this%2520paper%252C%2520we%2520shift%2520from%2520high-dimensional%2520feature%2520space%2520to%250Aa%2520lower-dimensional%252C%2520interpretable%2520graph%2520space%2520by%2520constructing%2520similarity%250Agraphs%2520that%2520encode%2520feature%2520relationships%2520at%2520different%2520levels%252C%2520e.g.%252C%2520clip%252C%250Aframe%252C%2520patch%252C%2520token%252C%2520etc.%2520To%2520capture%2520deeper%2520interactions%252C%2520we%2520use%2520graph%2520power%250Aexpansions%2520and%2520introduce%2520a%2520learnable%2520graph%2520fusion%2520operator%2520to%2520combine%2520these%250Agraph%2520powers%2520for%2520more%2520effective%2520fusion.%2520Our%2520approach%2520is%2520relationship-centric%252C%250Aoperates%2520in%2520a%2520homogeneous%2520space%252C%2520and%2520is%2520mathematically%2520principled%252C%2520resembling%250Aelement-wise%2520similarity%2520score%2520aggregation%2520via%2520multilinear%2520polynomials.%2520We%250Ademonstrate%2520the%2520effectiveness%2520of%2520our%2520graph-based%2520fusion%2520method%2520on%2520video%2520anomaly%250Adetection%252C%2520showing%2520strong%2520performance%2520across%2520multi-representational%252C%250Amulti-modal%252C%2520and%2520multi-domain%2520feature%2520fusion%2520tasks.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2410.01506v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=LEGO%3A%20Learnable%20Expansion%20of%20Graph%20Operators%20for%20Multi-Modal%20Feature%0A%20%20Fusion&entry.906535625=Dexuan%20Ding%20and%20Lei%20Wang%20and%20Liyun%20Zhu%20and%20Tom%20Gedeon%20and%20Piotr%20Koniusz&entry.1292438233=%20%20In%20computer%20vision%20tasks%2C%20features%20often%20come%20from%20diverse%20representations%2C%0Adomains%2C%20and%20modalities%2C%20such%20as%20text%2C%20images%2C%20and%20videos.%20Effectively%20fusing%0Athese%20features%20is%20essential%20for%20robust%20performance%2C%20especially%20with%20the%0Aavailability%20of%20powerful%20pre-trained%20models%20like%20vision-language%20models.%0AHowever%2C%20common%20fusion%20methods%2C%20such%20as%20concatenation%2C%20element-wise%20operations%2C%0Aand%20non-linear%20techniques%2C%20often%20fail%20to%20capture%20structural%20relationships%2C%20deep%0Afeature%20interactions%2C%20and%20suffer%20from%20inefficiency%20or%20misalignment%20of%20features%0Aacross%20domains.%20In%20this%20paper%2C%20we%20shift%20from%20high-dimensional%20feature%20space%20to%0Aa%20lower-dimensional%2C%20interpretable%20graph%20space%20by%20constructing%20similarity%0Agraphs%20that%20encode%20feature%20relationships%20at%20different%20levels%2C%20e.g.%2C%20clip%2C%0Aframe%2C%20patch%2C%20token%2C%20etc.%20To%20capture%20deeper%20interactions%2C%20we%20use%20graph%20power%0Aexpansions%20and%20introduce%20a%20learnable%20graph%20fusion%20operator%20to%20combine%20these%0Agraph%20powers%20for%20more%20effective%20fusion.%20Our%20approach%20is%20relationship-centric%2C%0Aoperates%20in%20a%20homogeneous%20space%2C%20and%20is%20mathematically%20principled%2C%20resembling%0Aelement-wise%20similarity%20score%20aggregation%20via%20multilinear%20polynomials.%20We%0Ademonstrate%20the%20effectiveness%20of%20our%20graph-based%20fusion%20method%20on%20video%20anomaly%0Adetection%2C%20showing%20strong%20performance%20across%20multi-representational%2C%0Amulti-modal%2C%20and%20multi-domain%20feature%20fusion%20tasks.%0A&entry.1838667208=http%3A//arxiv.org/abs/2410.01506v1&entry.124074799=Read"},
{"title": "Peeling Back the Layers: An In-Depth Evaluation of Encoder Architectures\n  in Neural News Recommenders", "author": "Andreea Iana and Goran Glava\u0161 and Heiko Paulheim", "abstract": "  Encoder architectures play a pivotal role in neural news recommenders by\nembedding the semantic and contextual information of news and users. Thus,\nresearch has heavily focused on enhancing the representational capabilities of\nnews and user encoders to improve recommender performance. Despite the\nsignificant impact of encoder architectures on the quality of news and user\nrepresentations, existing analyses of encoder designs focus only on the overall\ndownstream recommendation performance. This offers a one-sided assessment of\nthe encoders' similarity, ignoring more nuanced differences in their behavior,\nand potentially resulting in sub-optimal model selection. In this work, we\nperform a comprehensive analysis of encoder architectures in neural news\nrecommender systems. We systematically evaluate the most prominent news and\nuser encoder architectures, focusing on their (i) representational similarity,\nmeasured with the Central Kernel Alignment, (ii) overlap of generated\nrecommendation lists, quantified with the Jaccard similarity, and (iii) the\noverall recommendation performance. Our analysis reveals that the complexity of\ncertain encoding techniques is often empirically unjustified, highlighting the\npotential for simpler, more efficient architectures. By isolating the effects\nof individual components, we provide valuable insights for researchers and\npractitioners to make better informed decisions about encoder selection and\navoid unnecessary complexity in the design of news recommenders.\n", "link": "http://arxiv.org/abs/2410.01470v1", "date": "2024-10-02", "relevancy": 2.7186, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5563}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5563}, {"title": "IAE: Implicit Autoencoder for Point Cloud Self-supervised Representation Learning", "link": "https://arxiv.org/abs/2201.00785", "similarity": 0.5186}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Peeling%20Back%20the%20Layers%3A%20An%20In-Depth%20Evaluation%20of%20Encoder%20Architectures%0A%20%20in%20Neural%20News%20Recommenders&body=Title%3A%20Peeling%20Back%20the%20Layers%3A%20An%20In-Depth%20Evaluation%20of%20Encoder%20Architectures%0A%20%20in%20Neural%20News%20Recommenders%0AAuthor%3A%20Andreea%20Iana%20and%20Goran%20Glava%C5%A1%20and%20Heiko%20Paulheim%0AAbstract%3A%20%20%20Encoder%20architectures%20play%20a%20pivotal%20role%20in%20neural%20news%20recommenders%20by%0Aembedding%20the%20semantic%20and%20contextual%20information%20of%20news%20and%20users.%20Thus%2C%0Aresearch%20has%20heavily%20focused%20on%20enhancing%20the%20representational%20capabilities%20of%0Anews%20and%20user%20encoders%20to%20improve%20recommender%20performance.%20Despite%20the%0Asignificant%20impact%20of%20encoder%20architectures%20on%20the%20quality%20of%20news%20and%20user%0Arepresentations%2C%20existing%20analyses%20of%20encoder%20designs%20focus%20only%20on%20the%20overall%0Adownstream%20recommendation%20performance.%20This%20offers%20a%20one-sided%20assessment%20of%0Athe%20encoders%27%20similarity%2C%20ignoring%20more%20nuanced%20differences%20in%20their%20behavior%2C%0Aand%20potentially%20resulting%20in%20sub-optimal%20model%20selection.%20In%20this%20work%2C%20we%0Aperform%20a%20comprehensive%20analysis%20of%20encoder%20architectures%20in%20neural%20news%0Arecommender%20systems.%20We%20systematically%20evaluate%20the%20most%20prominent%20news%20and%0Auser%20encoder%20architectures%2C%20focusing%20on%20their%20%28i%29%20representational%20similarity%2C%0Ameasured%20with%20the%20Central%20Kernel%20Alignment%2C%20%28ii%29%20overlap%20of%20generated%0Arecommendation%20lists%2C%20quantified%20with%20the%20Jaccard%20similarity%2C%20and%20%28iii%29%20the%0Aoverall%20recommendation%20performance.%20Our%20analysis%20reveals%20that%20the%20complexity%20of%0Acertain%20encoding%20techniques%20is%20often%20empirically%20unjustified%2C%20highlighting%20the%0Apotential%20for%20simpler%2C%20more%20efficient%20architectures.%20By%20isolating%20the%20effects%0Aof%20individual%20components%2C%20we%20provide%20valuable%20insights%20for%20researchers%20and%0Apractitioners%20to%20make%20better%20informed%20decisions%20about%20encoder%20selection%20and%0Aavoid%20unnecessary%20complexity%20in%20the%20design%20of%20news%20recommenders.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2410.01470v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DPeeling%2520Back%2520the%2520Layers%253A%2520An%2520In-Depth%2520Evaluation%2520of%2520Encoder%2520Architectures%250A%2520%2520in%2520Neural%2520News%2520Recommenders%26entry.906535625%3DAndreea%2520Iana%2520and%2520Goran%2520Glava%25C5%25A1%2520and%2520Heiko%2520Paulheim%26entry.1292438233%3D%2520%2520Encoder%2520architectures%2520play%2520a%2520pivotal%2520role%2520in%2520neural%2520news%2520recommenders%2520by%250Aembedding%2520the%2520semantic%2520and%2520contextual%2520information%2520of%2520news%2520and%2520users.%2520Thus%252C%250Aresearch%2520has%2520heavily%2520focused%2520on%2520enhancing%2520the%2520representational%2520capabilities%2520of%250Anews%2520and%2520user%2520encoders%2520to%2520improve%2520recommender%2520performance.%2520Despite%2520the%250Asignificant%2520impact%2520of%2520encoder%2520architectures%2520on%2520the%2520quality%2520of%2520news%2520and%2520user%250Arepresentations%252C%2520existing%2520analyses%2520of%2520encoder%2520designs%2520focus%2520only%2520on%2520the%2520overall%250Adownstream%2520recommendation%2520performance.%2520This%2520offers%2520a%2520one-sided%2520assessment%2520of%250Athe%2520encoders%2527%2520similarity%252C%2520ignoring%2520more%2520nuanced%2520differences%2520in%2520their%2520behavior%252C%250Aand%2520potentially%2520resulting%2520in%2520sub-optimal%2520model%2520selection.%2520In%2520this%2520work%252C%2520we%250Aperform%2520a%2520comprehensive%2520analysis%2520of%2520encoder%2520architectures%2520in%2520neural%2520news%250Arecommender%2520systems.%2520We%2520systematically%2520evaluate%2520the%2520most%2520prominent%2520news%2520and%250Auser%2520encoder%2520architectures%252C%2520focusing%2520on%2520their%2520%2528i%2529%2520representational%2520similarity%252C%250Ameasured%2520with%2520the%2520Central%2520Kernel%2520Alignment%252C%2520%2528ii%2529%2520overlap%2520of%2520generated%250Arecommendation%2520lists%252C%2520quantified%2520with%2520the%2520Jaccard%2520similarity%252C%2520and%2520%2528iii%2529%2520the%250Aoverall%2520recommendation%2520performance.%2520Our%2520analysis%2520reveals%2520that%2520the%2520complexity%2520of%250Acertain%2520encoding%2520techniques%2520is%2520often%2520empirically%2520unjustified%252C%2520highlighting%2520the%250Apotential%2520for%2520simpler%252C%2520more%2520efficient%2520architectures.%2520By%2520isolating%2520the%2520effects%250Aof%2520individual%2520components%252C%2520we%2520provide%2520valuable%2520insights%2520for%2520researchers%2520and%250Apractitioners%2520to%2520make%2520better%2520informed%2520decisions%2520about%2520encoder%2520selection%2520and%250Aavoid%2520unnecessary%2520complexity%2520in%2520the%2520design%2520of%2520news%2520recommenders.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2410.01470v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Peeling%20Back%20the%20Layers%3A%20An%20In-Depth%20Evaluation%20of%20Encoder%20Architectures%0A%20%20in%20Neural%20News%20Recommenders&entry.906535625=Andreea%20Iana%20and%20Goran%20Glava%C5%A1%20and%20Heiko%20Paulheim&entry.1292438233=%20%20Encoder%20architectures%20play%20a%20pivotal%20role%20in%20neural%20news%20recommenders%20by%0Aembedding%20the%20semantic%20and%20contextual%20information%20of%20news%20and%20users.%20Thus%2C%0Aresearch%20has%20heavily%20focused%20on%20enhancing%20the%20representational%20capabilities%20of%0Anews%20and%20user%20encoders%20to%20improve%20recommender%20performance.%20Despite%20the%0Asignificant%20impact%20of%20encoder%20architectures%20on%20the%20quality%20of%20news%20and%20user%0Arepresentations%2C%20existing%20analyses%20of%20encoder%20designs%20focus%20only%20on%20the%20overall%0Adownstream%20recommendation%20performance.%20This%20offers%20a%20one-sided%20assessment%20of%0Athe%20encoders%27%20similarity%2C%20ignoring%20more%20nuanced%20differences%20in%20their%20behavior%2C%0Aand%20potentially%20resulting%20in%20sub-optimal%20model%20selection.%20In%20this%20work%2C%20we%0Aperform%20a%20comprehensive%20analysis%20of%20encoder%20architectures%20in%20neural%20news%0Arecommender%20systems.%20We%20systematically%20evaluate%20the%20most%20prominent%20news%20and%0Auser%20encoder%20architectures%2C%20focusing%20on%20their%20%28i%29%20representational%20similarity%2C%0Ameasured%20with%20the%20Central%20Kernel%20Alignment%2C%20%28ii%29%20overlap%20of%20generated%0Arecommendation%20lists%2C%20quantified%20with%20the%20Jaccard%20similarity%2C%20and%20%28iii%29%20the%0Aoverall%20recommendation%20performance.%20Our%20analysis%20reveals%20that%20the%20complexity%20of%0Acertain%20encoding%20techniques%20is%20often%20empirically%20unjustified%2C%20highlighting%20the%0Apotential%20for%20simpler%2C%20more%20efficient%20architectures.%20By%20isolating%20the%20effects%0Aof%20individual%20components%2C%20we%20provide%20valuable%20insights%20for%20researchers%20and%0Apractitioners%20to%20make%20better%20informed%20decisions%20about%20encoder%20selection%20and%0Aavoid%20unnecessary%20complexity%20in%20the%20design%20of%20news%20recommenders.%0A&entry.1838667208=http%3A//arxiv.org/abs/2410.01470v1&entry.124074799=Read"},
{"title": "Bridging Context Gaps: Leveraging Coreference Resolution for Long\n  Contextual Understanding", "author": "Yanming Liu and Xinyue Peng and Jiannan Cao and Shi Bo and Yanxin Shen and Xuhong Zhang and Sheng Cheng and Xun Wang and Jianwei Yin and Tianyu Du", "abstract": "  Large language models (LLMs) have shown remarkable capabilities in natural\nlanguage processing; however, they still face difficulties when tasked with\nunderstanding lengthy contexts and executing effective question answering.\nThese challenges often arise due to the complexity and ambiguity present in\nlonger texts. To enhance the performance of LLMs in such scenarios, we\nintroduce the Long Question Coreference Adaptation (LQCA) method. This\ninnovative framework focuses on coreference resolution tailored to long\ncontexts, allowing the model to identify and manage references effectively. The\nLQCA method encompasses four key steps: resolving coreferences within\nsub-documents, computing the distances between mentions, defining a\nrepresentative mention for coreference, and answering questions through mention\nreplacement. By processing information systematically, the framework provides\neasier-to-handle partitions for LLMs, promoting better understanding.\nExperimental evaluations on a range of LLMs and datasets have yielded positive\nresults, with a notable improvements on OpenAI-o1-mini and GPT-4o models,\nhighlighting the effectiveness of leveraging coreference resolution to bridge\ncontext gaps in question answering.\n", "link": "http://arxiv.org/abs/2410.01671v1", "date": "2024-10-02", "relevancy": 2.7162, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.58}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.58}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.4696}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Bridging%20Context%20Gaps%3A%20Leveraging%20Coreference%20Resolution%20for%20Long%0A%20%20Contextual%20Understanding&body=Title%3A%20Bridging%20Context%20Gaps%3A%20Leveraging%20Coreference%20Resolution%20for%20Long%0A%20%20Contextual%20Understanding%0AAuthor%3A%20Yanming%20Liu%20and%20Xinyue%20Peng%20and%20Jiannan%20Cao%20and%20Shi%20Bo%20and%20Yanxin%20Shen%20and%20Xuhong%20Zhang%20and%20Sheng%20Cheng%20and%20Xun%20Wang%20and%20Jianwei%20Yin%20and%20Tianyu%20Du%0AAbstract%3A%20%20%20Large%20language%20models%20%28LLMs%29%20have%20shown%20remarkable%20capabilities%20in%20natural%0Alanguage%20processing%3B%20however%2C%20they%20still%20face%20difficulties%20when%20tasked%20with%0Aunderstanding%20lengthy%20contexts%20and%20executing%20effective%20question%20answering.%0AThese%20challenges%20often%20arise%20due%20to%20the%20complexity%20and%20ambiguity%20present%20in%0Alonger%20texts.%20To%20enhance%20the%20performance%20of%20LLMs%20in%20such%20scenarios%2C%20we%0Aintroduce%20the%20Long%20Question%20Coreference%20Adaptation%20%28LQCA%29%20method.%20This%0Ainnovative%20framework%20focuses%20on%20coreference%20resolution%20tailored%20to%20long%0Acontexts%2C%20allowing%20the%20model%20to%20identify%20and%20manage%20references%20effectively.%20The%0ALQCA%20method%20encompasses%20four%20key%20steps%3A%20resolving%20coreferences%20within%0Asub-documents%2C%20computing%20the%20distances%20between%20mentions%2C%20defining%20a%0Arepresentative%20mention%20for%20coreference%2C%20and%20answering%20questions%20through%20mention%0Areplacement.%20By%20processing%20information%20systematically%2C%20the%20framework%20provides%0Aeasier-to-handle%20partitions%20for%20LLMs%2C%20promoting%20better%20understanding.%0AExperimental%20evaluations%20on%20a%20range%20of%20LLMs%20and%20datasets%20have%20yielded%20positive%0Aresults%2C%20with%20a%20notable%20improvements%20on%20OpenAI-o1-mini%20and%20GPT-4o%20models%2C%0Ahighlighting%20the%20effectiveness%20of%20leveraging%20coreference%20resolution%20to%20bridge%0Acontext%20gaps%20in%20question%20answering.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2410.01671v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DBridging%2520Context%2520Gaps%253A%2520Leveraging%2520Coreference%2520Resolution%2520for%2520Long%250A%2520%2520Contextual%2520Understanding%26entry.906535625%3DYanming%2520Liu%2520and%2520Xinyue%2520Peng%2520and%2520Jiannan%2520Cao%2520and%2520Shi%2520Bo%2520and%2520Yanxin%2520Shen%2520and%2520Xuhong%2520Zhang%2520and%2520Sheng%2520Cheng%2520and%2520Xun%2520Wang%2520and%2520Jianwei%2520Yin%2520and%2520Tianyu%2520Du%26entry.1292438233%3D%2520%2520Large%2520language%2520models%2520%2528LLMs%2529%2520have%2520shown%2520remarkable%2520capabilities%2520in%2520natural%250Alanguage%2520processing%253B%2520however%252C%2520they%2520still%2520face%2520difficulties%2520when%2520tasked%2520with%250Aunderstanding%2520lengthy%2520contexts%2520and%2520executing%2520effective%2520question%2520answering.%250AThese%2520challenges%2520often%2520arise%2520due%2520to%2520the%2520complexity%2520and%2520ambiguity%2520present%2520in%250Alonger%2520texts.%2520To%2520enhance%2520the%2520performance%2520of%2520LLMs%2520in%2520such%2520scenarios%252C%2520we%250Aintroduce%2520the%2520Long%2520Question%2520Coreference%2520Adaptation%2520%2528LQCA%2529%2520method.%2520This%250Ainnovative%2520framework%2520focuses%2520on%2520coreference%2520resolution%2520tailored%2520to%2520long%250Acontexts%252C%2520allowing%2520the%2520model%2520to%2520identify%2520and%2520manage%2520references%2520effectively.%2520The%250ALQCA%2520method%2520encompasses%2520four%2520key%2520steps%253A%2520resolving%2520coreferences%2520within%250Asub-documents%252C%2520computing%2520the%2520distances%2520between%2520mentions%252C%2520defining%2520a%250Arepresentative%2520mention%2520for%2520coreference%252C%2520and%2520answering%2520questions%2520through%2520mention%250Areplacement.%2520By%2520processing%2520information%2520systematically%252C%2520the%2520framework%2520provides%250Aeasier-to-handle%2520partitions%2520for%2520LLMs%252C%2520promoting%2520better%2520understanding.%250AExperimental%2520evaluations%2520on%2520a%2520range%2520of%2520LLMs%2520and%2520datasets%2520have%2520yielded%2520positive%250Aresults%252C%2520with%2520a%2520notable%2520improvements%2520on%2520OpenAI-o1-mini%2520and%2520GPT-4o%2520models%252C%250Ahighlighting%2520the%2520effectiveness%2520of%2520leveraging%2520coreference%2520resolution%2520to%2520bridge%250Acontext%2520gaps%2520in%2520question%2520answering.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2410.01671v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Bridging%20Context%20Gaps%3A%20Leveraging%20Coreference%20Resolution%20for%20Long%0A%20%20Contextual%20Understanding&entry.906535625=Yanming%20Liu%20and%20Xinyue%20Peng%20and%20Jiannan%20Cao%20and%20Shi%20Bo%20and%20Yanxin%20Shen%20and%20Xuhong%20Zhang%20and%20Sheng%20Cheng%20and%20Xun%20Wang%20and%20Jianwei%20Yin%20and%20Tianyu%20Du&entry.1292438233=%20%20Large%20language%20models%20%28LLMs%29%20have%20shown%20remarkable%20capabilities%20in%20natural%0Alanguage%20processing%3B%20however%2C%20they%20still%20face%20difficulties%20when%20tasked%20with%0Aunderstanding%20lengthy%20contexts%20and%20executing%20effective%20question%20answering.%0AThese%20challenges%20often%20arise%20due%20to%20the%20complexity%20and%20ambiguity%20present%20in%0Alonger%20texts.%20To%20enhance%20the%20performance%20of%20LLMs%20in%20such%20scenarios%2C%20we%0Aintroduce%20the%20Long%20Question%20Coreference%20Adaptation%20%28LQCA%29%20method.%20This%0Ainnovative%20framework%20focuses%20on%20coreference%20resolution%20tailored%20to%20long%0Acontexts%2C%20allowing%20the%20model%20to%20identify%20and%20manage%20references%20effectively.%20The%0ALQCA%20method%20encompasses%20four%20key%20steps%3A%20resolving%20coreferences%20within%0Asub-documents%2C%20computing%20the%20distances%20between%20mentions%2C%20defining%20a%0Arepresentative%20mention%20for%20coreference%2C%20and%20answering%20questions%20through%20mention%0Areplacement.%20By%20processing%20information%20systematically%2C%20the%20framework%20provides%0Aeasier-to-handle%20partitions%20for%20LLMs%2C%20promoting%20better%20understanding.%0AExperimental%20evaluations%20on%20a%20range%20of%20LLMs%20and%20datasets%20have%20yielded%20positive%0Aresults%2C%20with%20a%20notable%20improvements%20on%20OpenAI-o1-mini%20and%20GPT-4o%20models%2C%0Ahighlighting%20the%20effectiveness%20of%20leveraging%20coreference%20resolution%20to%20bridge%0Acontext%20gaps%20in%20question%20answering.%0A&entry.1838667208=http%3A//arxiv.org/abs/2410.01671v1&entry.124074799=Read"},
{"title": "Integrative Decoding: Improve Factuality via Implicit Self-consistency", "author": "Yi Cheng and Xiao Liang and Yeyun Gong and Wen Xiao and Song Wang and Yuji Zhang and Wenjun Hou and Kaishuai Xu and Wenge Liu and Wenjie Li and Jian Jiao and Qi Chen and Peng Cheng and Wayne Xiong", "abstract": "  Self-consistency-based approaches, which involve repeatedly sampling multiple\noutputs and selecting the most consistent one as the final response, prove to\nbe remarkably effective in improving the factual accuracy of large language\nmodels. Nonetheless, existing methods usually have strict constraints on the\ntask format, largely limiting their applicability. In this paper, we present\nIntegrative Decoding (ID), to unlock the potential of self-consistency in\nopen-ended generation tasks. ID operates by constructing a set of inputs, each\nprepended with a previously sampled response, and then processes them\nconcurrently, with the next token being selected by aggregating of all their\ncorresponding predictions at each decoding step. In essence, this simple\napproach implicitly incorporates self-consistency in the decoding objective.\nExtensive evaluation shows that ID consistently enhances factuality over a wide\nrange of language models, with substantial improvements on the TruthfulQA\n(+11.2%), Biographies (+15.4%) and LongFact (+8.5%) benchmarks. The performance\ngains amplify progressively as the number of sampled responses increases,\nindicating the potential of ID to scale up with repeated sampling.\n", "link": "http://arxiv.org/abs/2410.01556v1", "date": "2024-10-02", "relevancy": 2.7035, "topK": [{"title": "Magic-Me: Identity-Specific Video Customized Diffusion", "link": "http://arxiv.org/abs/2402.09368v1", "similarity": 0.5414}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5403}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5403}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Integrative%20Decoding%3A%20Improve%20Factuality%20via%20Implicit%20Self-consistency&body=Title%3A%20Integrative%20Decoding%3A%20Improve%20Factuality%20via%20Implicit%20Self-consistency%0AAuthor%3A%20Yi%20Cheng%20and%20Xiao%20Liang%20and%20Yeyun%20Gong%20and%20Wen%20Xiao%20and%20Song%20Wang%20and%20Yuji%20Zhang%20and%20Wenjun%20Hou%20and%20Kaishuai%20Xu%20and%20Wenge%20Liu%20and%20Wenjie%20Li%20and%20Jian%20Jiao%20and%20Qi%20Chen%20and%20Peng%20Cheng%20and%20Wayne%20Xiong%0AAbstract%3A%20%20%20Self-consistency-based%20approaches%2C%20which%20involve%20repeatedly%20sampling%20multiple%0Aoutputs%20and%20selecting%20the%20most%20consistent%20one%20as%20the%20final%20response%2C%20prove%20to%0Abe%20remarkably%20effective%20in%20improving%20the%20factual%20accuracy%20of%20large%20language%0Amodels.%20Nonetheless%2C%20existing%20methods%20usually%20have%20strict%20constraints%20on%20the%0Atask%20format%2C%20largely%20limiting%20their%20applicability.%20In%20this%20paper%2C%20we%20present%0AIntegrative%20Decoding%20%28ID%29%2C%20to%20unlock%20the%20potential%20of%20self-consistency%20in%0Aopen-ended%20generation%20tasks.%20ID%20operates%20by%20constructing%20a%20set%20of%20inputs%2C%20each%0Aprepended%20with%20a%20previously%20sampled%20response%2C%20and%20then%20processes%20them%0Aconcurrently%2C%20with%20the%20next%20token%20being%20selected%20by%20aggregating%20of%20all%20their%0Acorresponding%20predictions%20at%20each%20decoding%20step.%20In%20essence%2C%20this%20simple%0Aapproach%20implicitly%20incorporates%20self-consistency%20in%20the%20decoding%20objective.%0AExtensive%20evaluation%20shows%20that%20ID%20consistently%20enhances%20factuality%20over%20a%20wide%0Arange%20of%20language%20models%2C%20with%20substantial%20improvements%20on%20the%20TruthfulQA%0A%28%2B11.2%25%29%2C%20Biographies%20%28%2B15.4%25%29%20and%20LongFact%20%28%2B8.5%25%29%20benchmarks.%20The%20performance%0Agains%20amplify%20progressively%20as%20the%20number%20of%20sampled%20responses%20increases%2C%0Aindicating%20the%20potential%20of%20ID%20to%20scale%20up%20with%20repeated%20sampling.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2410.01556v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DIntegrative%2520Decoding%253A%2520Improve%2520Factuality%2520via%2520Implicit%2520Self-consistency%26entry.906535625%3DYi%2520Cheng%2520and%2520Xiao%2520Liang%2520and%2520Yeyun%2520Gong%2520and%2520Wen%2520Xiao%2520and%2520Song%2520Wang%2520and%2520Yuji%2520Zhang%2520and%2520Wenjun%2520Hou%2520and%2520Kaishuai%2520Xu%2520and%2520Wenge%2520Liu%2520and%2520Wenjie%2520Li%2520and%2520Jian%2520Jiao%2520and%2520Qi%2520Chen%2520and%2520Peng%2520Cheng%2520and%2520Wayne%2520Xiong%26entry.1292438233%3D%2520%2520Self-consistency-based%2520approaches%252C%2520which%2520involve%2520repeatedly%2520sampling%2520multiple%250Aoutputs%2520and%2520selecting%2520the%2520most%2520consistent%2520one%2520as%2520the%2520final%2520response%252C%2520prove%2520to%250Abe%2520remarkably%2520effective%2520in%2520improving%2520the%2520factual%2520accuracy%2520of%2520large%2520language%250Amodels.%2520Nonetheless%252C%2520existing%2520methods%2520usually%2520have%2520strict%2520constraints%2520on%2520the%250Atask%2520format%252C%2520largely%2520limiting%2520their%2520applicability.%2520In%2520this%2520paper%252C%2520we%2520present%250AIntegrative%2520Decoding%2520%2528ID%2529%252C%2520to%2520unlock%2520the%2520potential%2520of%2520self-consistency%2520in%250Aopen-ended%2520generation%2520tasks.%2520ID%2520operates%2520by%2520constructing%2520a%2520set%2520of%2520inputs%252C%2520each%250Aprepended%2520with%2520a%2520previously%2520sampled%2520response%252C%2520and%2520then%2520processes%2520them%250Aconcurrently%252C%2520with%2520the%2520next%2520token%2520being%2520selected%2520by%2520aggregating%2520of%2520all%2520their%250Acorresponding%2520predictions%2520at%2520each%2520decoding%2520step.%2520In%2520essence%252C%2520this%2520simple%250Aapproach%2520implicitly%2520incorporates%2520self-consistency%2520in%2520the%2520decoding%2520objective.%250AExtensive%2520evaluation%2520shows%2520that%2520ID%2520consistently%2520enhances%2520factuality%2520over%2520a%2520wide%250Arange%2520of%2520language%2520models%252C%2520with%2520substantial%2520improvements%2520on%2520the%2520TruthfulQA%250A%2528%252B11.2%2525%2529%252C%2520Biographies%2520%2528%252B15.4%2525%2529%2520and%2520LongFact%2520%2528%252B8.5%2525%2529%2520benchmarks.%2520The%2520performance%250Agains%2520amplify%2520progressively%2520as%2520the%2520number%2520of%2520sampled%2520responses%2520increases%252C%250Aindicating%2520the%2520potential%2520of%2520ID%2520to%2520scale%2520up%2520with%2520repeated%2520sampling.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2410.01556v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Integrative%20Decoding%3A%20Improve%20Factuality%20via%20Implicit%20Self-consistency&entry.906535625=Yi%20Cheng%20and%20Xiao%20Liang%20and%20Yeyun%20Gong%20and%20Wen%20Xiao%20and%20Song%20Wang%20and%20Yuji%20Zhang%20and%20Wenjun%20Hou%20and%20Kaishuai%20Xu%20and%20Wenge%20Liu%20and%20Wenjie%20Li%20and%20Jian%20Jiao%20and%20Qi%20Chen%20and%20Peng%20Cheng%20and%20Wayne%20Xiong&entry.1292438233=%20%20Self-consistency-based%20approaches%2C%20which%20involve%20repeatedly%20sampling%20multiple%0Aoutputs%20and%20selecting%20the%20most%20consistent%20one%20as%20the%20final%20response%2C%20prove%20to%0Abe%20remarkably%20effective%20in%20improving%20the%20factual%20accuracy%20of%20large%20language%0Amodels.%20Nonetheless%2C%20existing%20methods%20usually%20have%20strict%20constraints%20on%20the%0Atask%20format%2C%20largely%20limiting%20their%20applicability.%20In%20this%20paper%2C%20we%20present%0AIntegrative%20Decoding%20%28ID%29%2C%20to%20unlock%20the%20potential%20of%20self-consistency%20in%0Aopen-ended%20generation%20tasks.%20ID%20operates%20by%20constructing%20a%20set%20of%20inputs%2C%20each%0Aprepended%20with%20a%20previously%20sampled%20response%2C%20and%20then%20processes%20them%0Aconcurrently%2C%20with%20the%20next%20token%20being%20selected%20by%20aggregating%20of%20all%20their%0Acorresponding%20predictions%20at%20each%20decoding%20step.%20In%20essence%2C%20this%20simple%0Aapproach%20implicitly%20incorporates%20self-consistency%20in%20the%20decoding%20objective.%0AExtensive%20evaluation%20shows%20that%20ID%20consistently%20enhances%20factuality%20over%20a%20wide%0Arange%20of%20language%20models%2C%20with%20substantial%20improvements%20on%20the%20TruthfulQA%0A%28%2B11.2%25%29%2C%20Biographies%20%28%2B15.4%25%29%20and%20LongFact%20%28%2B8.5%25%29%20benchmarks.%20The%20performance%0Agains%20amplify%20progressively%20as%20the%20number%20of%20sampled%20responses%20increases%2C%0Aindicating%20the%20potential%20of%20ID%20to%20scale%20up%20with%20repeated%20sampling.%0A&entry.1838667208=http%3A//arxiv.org/abs/2410.01556v1&entry.124074799=Read"},
{"title": "Boosting Weakly-Supervised Referring Image Segmentation via Progressive\n  Comprehension", "author": "Zaiquan Yang and Yuhao Liu and Jiaying Lin and Gerhard Hancke and Rynson W. H. Lau", "abstract": "  This paper explores the weakly-supervised referring image segmentation (WRIS)\nproblem, and focuses on a challenging setup where target localization is\nlearned directly from image-text pairs. We note that the input text description\ntypically already contains detailed information on how to localize the target\nobject, and we also observe that humans often follow a step-by-step\ncomprehension process (\\ie, progressively utilizing target-related attributes\nand relations as cues) to identify the target object. Hence, we propose a novel\nProgressive Comprehension Network (PCNet) to leverage target-related textual\ncues from the input description for progressively localizing the target object.\nSpecifically, we first use a Large Language Model (LLM) to decompose the input\ntext description into short phrases. These short phrases are taken as\ntarget-related cues and fed into a Conditional Referring Module (CRM) in\nmultiple stages, to allow updating the referring text embedding and enhance the\nresponse map for target localization in a multi-stage manner. Based on the CRM,\nwe then propose a Region-aware Shrinking (RaS) loss to constrain the visual\nlocalization to be conducted progressively in a coarse-to-fine manner across\ndifferent stages. Finally, we introduce an Instance-aware Disambiguation (IaD)\nloss to suppress instance localization ambiguity by differentiating overlapping\nresponse maps generated by different referring texts on the same image.\nExtensive experiments show that our method outperforms SOTA methods on three\ncommon benchmarks.\n", "link": "http://arxiv.org/abs/2410.01544v1", "date": "2024-10-02", "relevancy": 2.6983, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5398}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5398}, {"title": "DDM-NET: End-to-end learning of keypoint feature Detection, Description and Matching for 3D localization", "link": "https://arxiv.org/abs/2212.04575", "similarity": 0.5393}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Boosting%20Weakly-Supervised%20Referring%20Image%20Segmentation%20via%20Progressive%0A%20%20Comprehension&body=Title%3A%20Boosting%20Weakly-Supervised%20Referring%20Image%20Segmentation%20via%20Progressive%0A%20%20Comprehension%0AAuthor%3A%20Zaiquan%20Yang%20and%20Yuhao%20Liu%20and%20Jiaying%20Lin%20and%20Gerhard%20Hancke%20and%20Rynson%20W.%20H.%20Lau%0AAbstract%3A%20%20%20This%20paper%20explores%20the%20weakly-supervised%20referring%20image%20segmentation%20%28WRIS%29%0Aproblem%2C%20and%20focuses%20on%20a%20challenging%20setup%20where%20target%20localization%20is%0Alearned%20directly%20from%20image-text%20pairs.%20We%20note%20that%20the%20input%20text%20description%0Atypically%20already%20contains%20detailed%20information%20on%20how%20to%20localize%20the%20target%0Aobject%2C%20and%20we%20also%20observe%20that%20humans%20often%20follow%20a%20step-by-step%0Acomprehension%20process%20%28%5Cie%2C%20progressively%20utilizing%20target-related%20attributes%0Aand%20relations%20as%20cues%29%20to%20identify%20the%20target%20object.%20Hence%2C%20we%20propose%20a%20novel%0AProgressive%20Comprehension%20Network%20%28PCNet%29%20to%20leverage%20target-related%20textual%0Acues%20from%20the%20input%20description%20for%20progressively%20localizing%20the%20target%20object.%0ASpecifically%2C%20we%20first%20use%20a%20Large%20Language%20Model%20%28LLM%29%20to%20decompose%20the%20input%0Atext%20description%20into%20short%20phrases.%20These%20short%20phrases%20are%20taken%20as%0Atarget-related%20cues%20and%20fed%20into%20a%20Conditional%20Referring%20Module%20%28CRM%29%20in%0Amultiple%20stages%2C%20to%20allow%20updating%20the%20referring%20text%20embedding%20and%20enhance%20the%0Aresponse%20map%20for%20target%20localization%20in%20a%20multi-stage%20manner.%20Based%20on%20the%20CRM%2C%0Awe%20then%20propose%20a%20Region-aware%20Shrinking%20%28RaS%29%20loss%20to%20constrain%20the%20visual%0Alocalization%20to%20be%20conducted%20progressively%20in%20a%20coarse-to-fine%20manner%20across%0Adifferent%20stages.%20Finally%2C%20we%20introduce%20an%20Instance-aware%20Disambiguation%20%28IaD%29%0Aloss%20to%20suppress%20instance%20localization%20ambiguity%20by%20differentiating%20overlapping%0Aresponse%20maps%20generated%20by%20different%20referring%20texts%20on%20the%20same%20image.%0AExtensive%20experiments%20show%20that%20our%20method%20outperforms%20SOTA%20methods%20on%20three%0Acommon%20benchmarks.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2410.01544v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DBoosting%2520Weakly-Supervised%2520Referring%2520Image%2520Segmentation%2520via%2520Progressive%250A%2520%2520Comprehension%26entry.906535625%3DZaiquan%2520Yang%2520and%2520Yuhao%2520Liu%2520and%2520Jiaying%2520Lin%2520and%2520Gerhard%2520Hancke%2520and%2520Rynson%2520W.%2520H.%2520Lau%26entry.1292438233%3D%2520%2520This%2520paper%2520explores%2520the%2520weakly-supervised%2520referring%2520image%2520segmentation%2520%2528WRIS%2529%250Aproblem%252C%2520and%2520focuses%2520on%2520a%2520challenging%2520setup%2520where%2520target%2520localization%2520is%250Alearned%2520directly%2520from%2520image-text%2520pairs.%2520We%2520note%2520that%2520the%2520input%2520text%2520description%250Atypically%2520already%2520contains%2520detailed%2520information%2520on%2520how%2520to%2520localize%2520the%2520target%250Aobject%252C%2520and%2520we%2520also%2520observe%2520that%2520humans%2520often%2520follow%2520a%2520step-by-step%250Acomprehension%2520process%2520%2528%255Cie%252C%2520progressively%2520utilizing%2520target-related%2520attributes%250Aand%2520relations%2520as%2520cues%2529%2520to%2520identify%2520the%2520target%2520object.%2520Hence%252C%2520we%2520propose%2520a%2520novel%250AProgressive%2520Comprehension%2520Network%2520%2528PCNet%2529%2520to%2520leverage%2520target-related%2520textual%250Acues%2520from%2520the%2520input%2520description%2520for%2520progressively%2520localizing%2520the%2520target%2520object.%250ASpecifically%252C%2520we%2520first%2520use%2520a%2520Large%2520Language%2520Model%2520%2528LLM%2529%2520to%2520decompose%2520the%2520input%250Atext%2520description%2520into%2520short%2520phrases.%2520These%2520short%2520phrases%2520are%2520taken%2520as%250Atarget-related%2520cues%2520and%2520fed%2520into%2520a%2520Conditional%2520Referring%2520Module%2520%2528CRM%2529%2520in%250Amultiple%2520stages%252C%2520to%2520allow%2520updating%2520the%2520referring%2520text%2520embedding%2520and%2520enhance%2520the%250Aresponse%2520map%2520for%2520target%2520localization%2520in%2520a%2520multi-stage%2520manner.%2520Based%2520on%2520the%2520CRM%252C%250Awe%2520then%2520propose%2520a%2520Region-aware%2520Shrinking%2520%2528RaS%2529%2520loss%2520to%2520constrain%2520the%2520visual%250Alocalization%2520to%2520be%2520conducted%2520progressively%2520in%2520a%2520coarse-to-fine%2520manner%2520across%250Adifferent%2520stages.%2520Finally%252C%2520we%2520introduce%2520an%2520Instance-aware%2520Disambiguation%2520%2528IaD%2529%250Aloss%2520to%2520suppress%2520instance%2520localization%2520ambiguity%2520by%2520differentiating%2520overlapping%250Aresponse%2520maps%2520generated%2520by%2520different%2520referring%2520texts%2520on%2520the%2520same%2520image.%250AExtensive%2520experiments%2520show%2520that%2520our%2520method%2520outperforms%2520SOTA%2520methods%2520on%2520three%250Acommon%2520benchmarks.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2410.01544v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Boosting%20Weakly-Supervised%20Referring%20Image%20Segmentation%20via%20Progressive%0A%20%20Comprehension&entry.906535625=Zaiquan%20Yang%20and%20Yuhao%20Liu%20and%20Jiaying%20Lin%20and%20Gerhard%20Hancke%20and%20Rynson%20W.%20H.%20Lau&entry.1292438233=%20%20This%20paper%20explores%20the%20weakly-supervised%20referring%20image%20segmentation%20%28WRIS%29%0Aproblem%2C%20and%20focuses%20on%20a%20challenging%20setup%20where%20target%20localization%20is%0Alearned%20directly%20from%20image-text%20pairs.%20We%20note%20that%20the%20input%20text%20description%0Atypically%20already%20contains%20detailed%20information%20on%20how%20to%20localize%20the%20target%0Aobject%2C%20and%20we%20also%20observe%20that%20humans%20often%20follow%20a%20step-by-step%0Acomprehension%20process%20%28%5Cie%2C%20progressively%20utilizing%20target-related%20attributes%0Aand%20relations%20as%20cues%29%20to%20identify%20the%20target%20object.%20Hence%2C%20we%20propose%20a%20novel%0AProgressive%20Comprehension%20Network%20%28PCNet%29%20to%20leverage%20target-related%20textual%0Acues%20from%20the%20input%20description%20for%20progressively%20localizing%20the%20target%20object.%0ASpecifically%2C%20we%20first%20use%20a%20Large%20Language%20Model%20%28LLM%29%20to%20decompose%20the%20input%0Atext%20description%20into%20short%20phrases.%20These%20short%20phrases%20are%20taken%20as%0Atarget-related%20cues%20and%20fed%20into%20a%20Conditional%20Referring%20Module%20%28CRM%29%20in%0Amultiple%20stages%2C%20to%20allow%20updating%20the%20referring%20text%20embedding%20and%20enhance%20the%0Aresponse%20map%20for%20target%20localization%20in%20a%20multi-stage%20manner.%20Based%20on%20the%20CRM%2C%0Awe%20then%20propose%20a%20Region-aware%20Shrinking%20%28RaS%29%20loss%20to%20constrain%20the%20visual%0Alocalization%20to%20be%20conducted%20progressively%20in%20a%20coarse-to-fine%20manner%20across%0Adifferent%20stages.%20Finally%2C%20we%20introduce%20an%20Instance-aware%20Disambiguation%20%28IaD%29%0Aloss%20to%20suppress%20instance%20localization%20ambiguity%20by%20differentiating%20overlapping%0Aresponse%20maps%20generated%20by%20different%20referring%20texts%20on%20the%20same%20image.%0AExtensive%20experiments%20show%20that%20our%20method%20outperforms%20SOTA%20methods%20on%20three%0Acommon%20benchmarks.%0A&entry.1838667208=http%3A//arxiv.org/abs/2410.01544v1&entry.124074799=Read"},
{"title": "Decorrelation-based Self-Supervised Visual Representation Learning for\n  Writer Identification", "author": "Arkadip Maitra and Shree Mitra and Siladittya Manna and Saumik Bhattacharya and Umapada Pal", "abstract": "  Self-supervised learning has developed rapidly over the last decade and has\nbeen applied in many areas of computer vision. Decorrelation-based\nself-supervised pretraining has shown great promise among non-contrastive\nalgorithms, yielding performance at par with supervised and contrastive\nself-supervised baselines. In this work, we explore the decorrelation-based\nparadigm of self-supervised learning and apply the same to learning\ndisentangled stroke features for writer identification. Here we propose a\nmodified formulation of the decorrelation-based framework named SWIS which was\nproposed for signature verification by standardizing the features along each\ndimension on top of the existing framework. We show that the proposed framework\noutperforms the contemporary self-supervised learning framework on the writer\nidentification benchmark and also outperforms several supervised methods as\nwell. To the best of our knowledge, this work is the first of its kind to apply\nself-supervised learning for learning representations for writer verification\ntasks.\n", "link": "http://arxiv.org/abs/2410.01441v1", "date": "2024-10-02", "relevancy": 2.6851, "topK": [{"title": "Self-supervised Photographic Image Layout Representation Learning", "link": "http://arxiv.org/abs/2403.03740v1", "similarity": 0.6003}, {"title": "Weakly-guided self-supervised pretraining for temporal activity detection", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/25189", "similarity": 0.5118}, {"title": "IAE: Implicit Autoencoder for Point Cloud Self-supervised Representation Learning", "link": "https://arxiv.org/abs/2201.00785", "similarity": 0.4989}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Decorrelation-based%20Self-Supervised%20Visual%20Representation%20Learning%20for%0A%20%20Writer%20Identification&body=Title%3A%20Decorrelation-based%20Self-Supervised%20Visual%20Representation%20Learning%20for%0A%20%20Writer%20Identification%0AAuthor%3A%20Arkadip%20Maitra%20and%20Shree%20Mitra%20and%20Siladittya%20Manna%20and%20Saumik%20Bhattacharya%20and%20Umapada%20Pal%0AAbstract%3A%20%20%20Self-supervised%20learning%20has%20developed%20rapidly%20over%20the%20last%20decade%20and%20has%0Abeen%20applied%20in%20many%20areas%20of%20computer%20vision.%20Decorrelation-based%0Aself-supervised%20pretraining%20has%20shown%20great%20promise%20among%20non-contrastive%0Aalgorithms%2C%20yielding%20performance%20at%20par%20with%20supervised%20and%20contrastive%0Aself-supervised%20baselines.%20In%20this%20work%2C%20we%20explore%20the%20decorrelation-based%0Aparadigm%20of%20self-supervised%20learning%20and%20apply%20the%20same%20to%20learning%0Adisentangled%20stroke%20features%20for%20writer%20identification.%20Here%20we%20propose%20a%0Amodified%20formulation%20of%20the%20decorrelation-based%20framework%20named%20SWIS%20which%20was%0Aproposed%20for%20signature%20verification%20by%20standardizing%20the%20features%20along%20each%0Adimension%20on%20top%20of%20the%20existing%20framework.%20We%20show%20that%20the%20proposed%20framework%0Aoutperforms%20the%20contemporary%20self-supervised%20learning%20framework%20on%20the%20writer%0Aidentification%20benchmark%20and%20also%20outperforms%20several%20supervised%20methods%20as%0Awell.%20To%20the%20best%20of%20our%20knowledge%2C%20this%20work%20is%20the%20first%20of%20its%20kind%20to%20apply%0Aself-supervised%20learning%20for%20learning%20representations%20for%20writer%20verification%0Atasks.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2410.01441v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DDecorrelation-based%2520Self-Supervised%2520Visual%2520Representation%2520Learning%2520for%250A%2520%2520Writer%2520Identification%26entry.906535625%3DArkadip%2520Maitra%2520and%2520Shree%2520Mitra%2520and%2520Siladittya%2520Manna%2520and%2520Saumik%2520Bhattacharya%2520and%2520Umapada%2520Pal%26entry.1292438233%3D%2520%2520Self-supervised%2520learning%2520has%2520developed%2520rapidly%2520over%2520the%2520last%2520decade%2520and%2520has%250Abeen%2520applied%2520in%2520many%2520areas%2520of%2520computer%2520vision.%2520Decorrelation-based%250Aself-supervised%2520pretraining%2520has%2520shown%2520great%2520promise%2520among%2520non-contrastive%250Aalgorithms%252C%2520yielding%2520performance%2520at%2520par%2520with%2520supervised%2520and%2520contrastive%250Aself-supervised%2520baselines.%2520In%2520this%2520work%252C%2520we%2520explore%2520the%2520decorrelation-based%250Aparadigm%2520of%2520self-supervised%2520learning%2520and%2520apply%2520the%2520same%2520to%2520learning%250Adisentangled%2520stroke%2520features%2520for%2520writer%2520identification.%2520Here%2520we%2520propose%2520a%250Amodified%2520formulation%2520of%2520the%2520decorrelation-based%2520framework%2520named%2520SWIS%2520which%2520was%250Aproposed%2520for%2520signature%2520verification%2520by%2520standardizing%2520the%2520features%2520along%2520each%250Adimension%2520on%2520top%2520of%2520the%2520existing%2520framework.%2520We%2520show%2520that%2520the%2520proposed%2520framework%250Aoutperforms%2520the%2520contemporary%2520self-supervised%2520learning%2520framework%2520on%2520the%2520writer%250Aidentification%2520benchmark%2520and%2520also%2520outperforms%2520several%2520supervised%2520methods%2520as%250Awell.%2520To%2520the%2520best%2520of%2520our%2520knowledge%252C%2520this%2520work%2520is%2520the%2520first%2520of%2520its%2520kind%2520to%2520apply%250Aself-supervised%2520learning%2520for%2520learning%2520representations%2520for%2520writer%2520verification%250Atasks.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2410.01441v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Decorrelation-based%20Self-Supervised%20Visual%20Representation%20Learning%20for%0A%20%20Writer%20Identification&entry.906535625=Arkadip%20Maitra%20and%20Shree%20Mitra%20and%20Siladittya%20Manna%20and%20Saumik%20Bhattacharya%20and%20Umapada%20Pal&entry.1292438233=%20%20Self-supervised%20learning%20has%20developed%20rapidly%20over%20the%20last%20decade%20and%20has%0Abeen%20applied%20in%20many%20areas%20of%20computer%20vision.%20Decorrelation-based%0Aself-supervised%20pretraining%20has%20shown%20great%20promise%20among%20non-contrastive%0Aalgorithms%2C%20yielding%20performance%20at%20par%20with%20supervised%20and%20contrastive%0Aself-supervised%20baselines.%20In%20this%20work%2C%20we%20explore%20the%20decorrelation-based%0Aparadigm%20of%20self-supervised%20learning%20and%20apply%20the%20same%20to%20learning%0Adisentangled%20stroke%20features%20for%20writer%20identification.%20Here%20we%20propose%20a%0Amodified%20formulation%20of%20the%20decorrelation-based%20framework%20named%20SWIS%20which%20was%0Aproposed%20for%20signature%20verification%20by%20standardizing%20the%20features%20along%20each%0Adimension%20on%20top%20of%20the%20existing%20framework.%20We%20show%20that%20the%20proposed%20framework%0Aoutperforms%20the%20contemporary%20self-supervised%20learning%20framework%20on%20the%20writer%0Aidentification%20benchmark%20and%20also%20outperforms%20several%20supervised%20methods%20as%0Awell.%20To%20the%20best%20of%20our%20knowledge%2C%20this%20work%20is%20the%20first%20of%20its%20kind%20to%20apply%0Aself-supervised%20learning%20for%20learning%20representations%20for%20writer%20verification%0Atasks.%0A&entry.1838667208=http%3A//arxiv.org/abs/2410.01441v1&entry.124074799=Read"},
{"title": "SinkSAM: A Monocular Depth-Guided SAM Framework for Automatic Sinkhole\n  Segmentation", "author": "Osher Rafaeli and Tal Svoray and Ariel Nahlieli", "abstract": "  Soil sinkholes significantly influence soil degradation, but their irregular\nshapes, along with interference from shadow and vegetation, make it challenging\nto accurately quantify their properties using remotely sensed data. We present\na novel framework for sinkhole segmentation that combines traditional\ntopographic computations of closed depressions with the newly developed\nprompt-based Segment Anything Model (SAM). Within this framework, termed\nSinkSAM, we highlight four key improvements: (1) The integration of topographic\ncomputations with SAM enables pixel-level refinement of sinkhole boundaries\nsegmentation; (2) A coherent mathematical prompting strategy, based on closed\ndepressions, addresses the limitations of purely learning-based models (CNNs)\nin detecting and segmenting undefined sinkhole features, while improving\ngeneralization to new, unseen regions; (3) Using Depth Anything V2 monocular\ndepth for automatic prompts eliminates photogrammetric biases, enabling\nsinkhole mapping without the dependence on LiDAR data; and (4) An established\nsinkhole database facilitates fine-tuning of SAM, improving its zero-shot\nperformance in sinkhole segmentation. These advancements allow the deployment\nof SinkSAM, in an unseen test area, in the highly variable semiarid region,\nachieving an intersection-over-union (IoU) of 40.27\\% and surpassing previous\nresults. This paper also presents the first SAM implementation for sinkhole\nsegmentation and demonstrates the robustness of SinkSAM in extracting sinkhole\nmaps using a single RGB image.\n", "link": "http://arxiv.org/abs/2410.01473v1", "date": "2024-10-02", "relevancy": 2.6614, "topK": [{"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.5398}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5357}, {"title": "RTG-SLAM: Real-time 3D Reconstruction at Scale using Gaussian Splatting", "link": "http://arxiv.org/abs/2404.19706v1", "similarity": 0.5214}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20SinkSAM%3A%20A%20Monocular%20Depth-Guided%20SAM%20Framework%20for%20Automatic%20Sinkhole%0A%20%20Segmentation&body=Title%3A%20SinkSAM%3A%20A%20Monocular%20Depth-Guided%20SAM%20Framework%20for%20Automatic%20Sinkhole%0A%20%20Segmentation%0AAuthor%3A%20Osher%20Rafaeli%20and%20Tal%20Svoray%20and%20Ariel%20Nahlieli%0AAbstract%3A%20%20%20Soil%20sinkholes%20significantly%20influence%20soil%20degradation%2C%20but%20their%20irregular%0Ashapes%2C%20along%20with%20interference%20from%20shadow%20and%20vegetation%2C%20make%20it%20challenging%0Ato%20accurately%20quantify%20their%20properties%20using%20remotely%20sensed%20data.%20We%20present%0Aa%20novel%20framework%20for%20sinkhole%20segmentation%20that%20combines%20traditional%0Atopographic%20computations%20of%20closed%20depressions%20with%20the%20newly%20developed%0Aprompt-based%20Segment%20Anything%20Model%20%28SAM%29.%20Within%20this%20framework%2C%20termed%0ASinkSAM%2C%20we%20highlight%20four%20key%20improvements%3A%20%281%29%20The%20integration%20of%20topographic%0Acomputations%20with%20SAM%20enables%20pixel-level%20refinement%20of%20sinkhole%20boundaries%0Asegmentation%3B%20%282%29%20A%20coherent%20mathematical%20prompting%20strategy%2C%20based%20on%20closed%0Adepressions%2C%20addresses%20the%20limitations%20of%20purely%20learning-based%20models%20%28CNNs%29%0Ain%20detecting%20and%20segmenting%20undefined%20sinkhole%20features%2C%20while%20improving%0Ageneralization%20to%20new%2C%20unseen%20regions%3B%20%283%29%20Using%20Depth%20Anything%20V2%20monocular%0Adepth%20for%20automatic%20prompts%20eliminates%20photogrammetric%20biases%2C%20enabling%0Asinkhole%20mapping%20without%20the%20dependence%20on%20LiDAR%20data%3B%20and%20%284%29%20An%20established%0Asinkhole%20database%20facilitates%20fine-tuning%20of%20SAM%2C%20improving%20its%20zero-shot%0Aperformance%20in%20sinkhole%20segmentation.%20These%20advancements%20allow%20the%20deployment%0Aof%20SinkSAM%2C%20in%20an%20unseen%20test%20area%2C%20in%20the%20highly%20variable%20semiarid%20region%2C%0Aachieving%20an%20intersection-over-union%20%28IoU%29%20of%2040.27%5C%25%20and%20surpassing%20previous%0Aresults.%20This%20paper%20also%20presents%20the%20first%20SAM%20implementation%20for%20sinkhole%0Asegmentation%20and%20demonstrates%20the%20robustness%20of%20SinkSAM%20in%20extracting%20sinkhole%0Amaps%20using%20a%20single%20RGB%20image.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2410.01473v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DSinkSAM%253A%2520A%2520Monocular%2520Depth-Guided%2520SAM%2520Framework%2520for%2520Automatic%2520Sinkhole%250A%2520%2520Segmentation%26entry.906535625%3DOsher%2520Rafaeli%2520and%2520Tal%2520Svoray%2520and%2520Ariel%2520Nahlieli%26entry.1292438233%3D%2520%2520Soil%2520sinkholes%2520significantly%2520influence%2520soil%2520degradation%252C%2520but%2520their%2520irregular%250Ashapes%252C%2520along%2520with%2520interference%2520from%2520shadow%2520and%2520vegetation%252C%2520make%2520it%2520challenging%250Ato%2520accurately%2520quantify%2520their%2520properties%2520using%2520remotely%2520sensed%2520data.%2520We%2520present%250Aa%2520novel%2520framework%2520for%2520sinkhole%2520segmentation%2520that%2520combines%2520traditional%250Atopographic%2520computations%2520of%2520closed%2520depressions%2520with%2520the%2520newly%2520developed%250Aprompt-based%2520Segment%2520Anything%2520Model%2520%2528SAM%2529.%2520Within%2520this%2520framework%252C%2520termed%250ASinkSAM%252C%2520we%2520highlight%2520four%2520key%2520improvements%253A%2520%25281%2529%2520The%2520integration%2520of%2520topographic%250Acomputations%2520with%2520SAM%2520enables%2520pixel-level%2520refinement%2520of%2520sinkhole%2520boundaries%250Asegmentation%253B%2520%25282%2529%2520A%2520coherent%2520mathematical%2520prompting%2520strategy%252C%2520based%2520on%2520closed%250Adepressions%252C%2520addresses%2520the%2520limitations%2520of%2520purely%2520learning-based%2520models%2520%2528CNNs%2529%250Ain%2520detecting%2520and%2520segmenting%2520undefined%2520sinkhole%2520features%252C%2520while%2520improving%250Ageneralization%2520to%2520new%252C%2520unseen%2520regions%253B%2520%25283%2529%2520Using%2520Depth%2520Anything%2520V2%2520monocular%250Adepth%2520for%2520automatic%2520prompts%2520eliminates%2520photogrammetric%2520biases%252C%2520enabling%250Asinkhole%2520mapping%2520without%2520the%2520dependence%2520on%2520LiDAR%2520data%253B%2520and%2520%25284%2529%2520An%2520established%250Asinkhole%2520database%2520facilitates%2520fine-tuning%2520of%2520SAM%252C%2520improving%2520its%2520zero-shot%250Aperformance%2520in%2520sinkhole%2520segmentation.%2520These%2520advancements%2520allow%2520the%2520deployment%250Aof%2520SinkSAM%252C%2520in%2520an%2520unseen%2520test%2520area%252C%2520in%2520the%2520highly%2520variable%2520semiarid%2520region%252C%250Aachieving%2520an%2520intersection-over-union%2520%2528IoU%2529%2520of%252040.27%255C%2525%2520and%2520surpassing%2520previous%250Aresults.%2520This%2520paper%2520also%2520presents%2520the%2520first%2520SAM%2520implementation%2520for%2520sinkhole%250Asegmentation%2520and%2520demonstrates%2520the%2520robustness%2520of%2520SinkSAM%2520in%2520extracting%2520sinkhole%250Amaps%2520using%2520a%2520single%2520RGB%2520image.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2410.01473v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=SinkSAM%3A%20A%20Monocular%20Depth-Guided%20SAM%20Framework%20for%20Automatic%20Sinkhole%0A%20%20Segmentation&entry.906535625=Osher%20Rafaeli%20and%20Tal%20Svoray%20and%20Ariel%20Nahlieli&entry.1292438233=%20%20Soil%20sinkholes%20significantly%20influence%20soil%20degradation%2C%20but%20their%20irregular%0Ashapes%2C%20along%20with%20interference%20from%20shadow%20and%20vegetation%2C%20make%20it%20challenging%0Ato%20accurately%20quantify%20their%20properties%20using%20remotely%20sensed%20data.%20We%20present%0Aa%20novel%20framework%20for%20sinkhole%20segmentation%20that%20combines%20traditional%0Atopographic%20computations%20of%20closed%20depressions%20with%20the%20newly%20developed%0Aprompt-based%20Segment%20Anything%20Model%20%28SAM%29.%20Within%20this%20framework%2C%20termed%0ASinkSAM%2C%20we%20highlight%20four%20key%20improvements%3A%20%281%29%20The%20integration%20of%20topographic%0Acomputations%20with%20SAM%20enables%20pixel-level%20refinement%20of%20sinkhole%20boundaries%0Asegmentation%3B%20%282%29%20A%20coherent%20mathematical%20prompting%20strategy%2C%20based%20on%20closed%0Adepressions%2C%20addresses%20the%20limitations%20of%20purely%20learning-based%20models%20%28CNNs%29%0Ain%20detecting%20and%20segmenting%20undefined%20sinkhole%20features%2C%20while%20improving%0Ageneralization%20to%20new%2C%20unseen%20regions%3B%20%283%29%20Using%20Depth%20Anything%20V2%20monocular%0Adepth%20for%20automatic%20prompts%20eliminates%20photogrammetric%20biases%2C%20enabling%0Asinkhole%20mapping%20without%20the%20dependence%20on%20LiDAR%20data%3B%20and%20%284%29%20An%20established%0Asinkhole%20database%20facilitates%20fine-tuning%20of%20SAM%2C%20improving%20its%20zero-shot%0Aperformance%20in%20sinkhole%20segmentation.%20These%20advancements%20allow%20the%20deployment%0Aof%20SinkSAM%2C%20in%20an%20unseen%20test%20area%2C%20in%20the%20highly%20variable%20semiarid%20region%2C%0Aachieving%20an%20intersection-over-union%20%28IoU%29%20of%2040.27%5C%25%20and%20surpassing%20previous%0Aresults.%20This%20paper%20also%20presents%20the%20first%20SAM%20implementation%20for%20sinkhole%0Asegmentation%20and%20demonstrates%20the%20robustness%20of%20SinkSAM%20in%20extracting%20sinkhole%0Amaps%20using%20a%20single%20RGB%20image.%0A&entry.1838667208=http%3A//arxiv.org/abs/2410.01473v1&entry.124074799=Read"},
{"title": "Coordinate-Based Neural Representation Enabling Zero-Shot Learning for\n  3D Multiparametric Quantitative MRI", "author": "Guoyan Lao and Ruimin Feng and Haikun Qi and Zhenfeng Lv and Qiangqiang Liu and Chunlei Liu and Yuyao Zhang and Hongjiang Wei", "abstract": "  Quantitative magnetic resonance imaging (qMRI) offers tissue-specific\nphysical parameters with significant potential for neuroscience research and\nclinical practice. However, lengthy scan times for 3D multiparametric qMRI\nacquisition limit its clinical utility. Here, we propose SUMMIT, an innovative\nimaging methodology that includes data acquisition and an unsupervised\nreconstruction for simultaneous multiparametric qMRI. SUMMIT first encodes\nmultiple important quantitative properties into highly undersampled k-space. It\nfurther leverages implicit neural representation incorporated with a dedicated\nphysics model to reconstruct the desired multiparametric maps without needing\nexternal training datasets. SUMMIT delivers co-registered T1, T2, T2*, and\nquantitative susceptibility mapping. Extensive simulations and phantom imaging\ndemonstrate SUMMIT's high accuracy. Additionally, the proposed unsupervised\napproach for qMRI reconstruction also introduces a novel zero-shot learning\nparadigm for multiparametric imaging applicable to various medical imaging\nmodalities.\n", "link": "http://arxiv.org/abs/2410.01577v1", "date": "2024-10-02", "relevancy": 2.6601, "topK": [{"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5325}, {"title": "CAT3D: Create Anything in 3D with Multi-View Diffusion Models", "link": "http://arxiv.org/abs/2405.10314v1", "similarity": 0.5318}, {"title": "CAT3D: Create Anything in 3D with Multi-View Diffusion Models", "link": "http://arxiv.org/abs/2405.10314v1", "similarity": 0.5318}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Coordinate-Based%20Neural%20Representation%20Enabling%20Zero-Shot%20Learning%20for%0A%20%203D%20Multiparametric%20Quantitative%20MRI&body=Title%3A%20Coordinate-Based%20Neural%20Representation%20Enabling%20Zero-Shot%20Learning%20for%0A%20%203D%20Multiparametric%20Quantitative%20MRI%0AAuthor%3A%20Guoyan%20Lao%20and%20Ruimin%20Feng%20and%20Haikun%20Qi%20and%20Zhenfeng%20Lv%20and%20Qiangqiang%20Liu%20and%20Chunlei%20Liu%20and%20Yuyao%20Zhang%20and%20Hongjiang%20Wei%0AAbstract%3A%20%20%20Quantitative%20magnetic%20resonance%20imaging%20%28qMRI%29%20offers%20tissue-specific%0Aphysical%20parameters%20with%20significant%20potential%20for%20neuroscience%20research%20and%0Aclinical%20practice.%20However%2C%20lengthy%20scan%20times%20for%203D%20multiparametric%20qMRI%0Aacquisition%20limit%20its%20clinical%20utility.%20Here%2C%20we%20propose%20SUMMIT%2C%20an%20innovative%0Aimaging%20methodology%20that%20includes%20data%20acquisition%20and%20an%20unsupervised%0Areconstruction%20for%20simultaneous%20multiparametric%20qMRI.%20SUMMIT%20first%20encodes%0Amultiple%20important%20quantitative%20properties%20into%20highly%20undersampled%20k-space.%20It%0Afurther%20leverages%20implicit%20neural%20representation%20incorporated%20with%20a%20dedicated%0Aphysics%20model%20to%20reconstruct%20the%20desired%20multiparametric%20maps%20without%20needing%0Aexternal%20training%20datasets.%20SUMMIT%20delivers%20co-registered%20T1%2C%20T2%2C%20T2%2A%2C%20and%0Aquantitative%20susceptibility%20mapping.%20Extensive%20simulations%20and%20phantom%20imaging%0Ademonstrate%20SUMMIT%27s%20high%20accuracy.%20Additionally%2C%20the%20proposed%20unsupervised%0Aapproach%20for%20qMRI%20reconstruction%20also%20introduces%20a%20novel%20zero-shot%20learning%0Aparadigm%20for%20multiparametric%20imaging%20applicable%20to%20various%20medical%20imaging%0Amodalities.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2410.01577v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DCoordinate-Based%2520Neural%2520Representation%2520Enabling%2520Zero-Shot%2520Learning%2520for%250A%2520%25203D%2520Multiparametric%2520Quantitative%2520MRI%26entry.906535625%3DGuoyan%2520Lao%2520and%2520Ruimin%2520Feng%2520and%2520Haikun%2520Qi%2520and%2520Zhenfeng%2520Lv%2520and%2520Qiangqiang%2520Liu%2520and%2520Chunlei%2520Liu%2520and%2520Yuyao%2520Zhang%2520and%2520Hongjiang%2520Wei%26entry.1292438233%3D%2520%2520Quantitative%2520magnetic%2520resonance%2520imaging%2520%2528qMRI%2529%2520offers%2520tissue-specific%250Aphysical%2520parameters%2520with%2520significant%2520potential%2520for%2520neuroscience%2520research%2520and%250Aclinical%2520practice.%2520However%252C%2520lengthy%2520scan%2520times%2520for%25203D%2520multiparametric%2520qMRI%250Aacquisition%2520limit%2520its%2520clinical%2520utility.%2520Here%252C%2520we%2520propose%2520SUMMIT%252C%2520an%2520innovative%250Aimaging%2520methodology%2520that%2520includes%2520data%2520acquisition%2520and%2520an%2520unsupervised%250Areconstruction%2520for%2520simultaneous%2520multiparametric%2520qMRI.%2520SUMMIT%2520first%2520encodes%250Amultiple%2520important%2520quantitative%2520properties%2520into%2520highly%2520undersampled%2520k-space.%2520It%250Afurther%2520leverages%2520implicit%2520neural%2520representation%2520incorporated%2520with%2520a%2520dedicated%250Aphysics%2520model%2520to%2520reconstruct%2520the%2520desired%2520multiparametric%2520maps%2520without%2520needing%250Aexternal%2520training%2520datasets.%2520SUMMIT%2520delivers%2520co-registered%2520T1%252C%2520T2%252C%2520T2%252A%252C%2520and%250Aquantitative%2520susceptibility%2520mapping.%2520Extensive%2520simulations%2520and%2520phantom%2520imaging%250Ademonstrate%2520SUMMIT%2527s%2520high%2520accuracy.%2520Additionally%252C%2520the%2520proposed%2520unsupervised%250Aapproach%2520for%2520qMRI%2520reconstruction%2520also%2520introduces%2520a%2520novel%2520zero-shot%2520learning%250Aparadigm%2520for%2520multiparametric%2520imaging%2520applicable%2520to%2520various%2520medical%2520imaging%250Amodalities.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2410.01577v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Coordinate-Based%20Neural%20Representation%20Enabling%20Zero-Shot%20Learning%20for%0A%20%203D%20Multiparametric%20Quantitative%20MRI&entry.906535625=Guoyan%20Lao%20and%20Ruimin%20Feng%20and%20Haikun%20Qi%20and%20Zhenfeng%20Lv%20and%20Qiangqiang%20Liu%20and%20Chunlei%20Liu%20and%20Yuyao%20Zhang%20and%20Hongjiang%20Wei&entry.1292438233=%20%20Quantitative%20magnetic%20resonance%20imaging%20%28qMRI%29%20offers%20tissue-specific%0Aphysical%20parameters%20with%20significant%20potential%20for%20neuroscience%20research%20and%0Aclinical%20practice.%20However%2C%20lengthy%20scan%20times%20for%203D%20multiparametric%20qMRI%0Aacquisition%20limit%20its%20clinical%20utility.%20Here%2C%20we%20propose%20SUMMIT%2C%20an%20innovative%0Aimaging%20methodology%20that%20includes%20data%20acquisition%20and%20an%20unsupervised%0Areconstruction%20for%20simultaneous%20multiparametric%20qMRI.%20SUMMIT%20first%20encodes%0Amultiple%20important%20quantitative%20properties%20into%20highly%20undersampled%20k-space.%20It%0Afurther%20leverages%20implicit%20neural%20representation%20incorporated%20with%20a%20dedicated%0Aphysics%20model%20to%20reconstruct%20the%20desired%20multiparametric%20maps%20without%20needing%0Aexternal%20training%20datasets.%20SUMMIT%20delivers%20co-registered%20T1%2C%20T2%2C%20T2%2A%2C%20and%0Aquantitative%20susceptibility%20mapping.%20Extensive%20simulations%20and%20phantom%20imaging%0Ademonstrate%20SUMMIT%27s%20high%20accuracy.%20Additionally%2C%20the%20proposed%20unsupervised%0Aapproach%20for%20qMRI%20reconstruction%20also%20introduces%20a%20novel%20zero-shot%20learning%0Aparadigm%20for%20multiparametric%20imaging%20applicable%20to%20various%20medical%20imaging%0Amodalities.%0A&entry.1838667208=http%3A//arxiv.org/abs/2410.01577v1&entry.124074799=Read"},
{"title": "Unleashing Parameter Potential of Neural Representation for Efficient\n  Video Compression", "author": "Gai Zhang and Xinfeng Zhang and Lv Tang and Yue Li and Kai Zhang and Li Zhang", "abstract": "  For decades, video compression technology has been a prominent research area.\nTraditional hybrid video compression framework and end-to-end frameworks\ncontinue to explore various intra- and inter-frame reference and prediction\nstrategies based on discrete transforms and deep learning techniques. However,\nthe emerging implicit neural representation (INR) technique models entire\nvideos as basic units, automatically capturing intra-frame and inter-frame\ncorrelations and obtaining promising performance. INR uses a compact neural\nnetwork to store video information in network parameters, effectively\neliminating spatial and temporal redundancy in the original video. However, in\nthis paper, our exploration and verification reveal that current INR video\ncompression methods do not fully exploit their potential to preserve\ninformation. We investigate the potential of enhancing network parameter\nstorage through parameter reuse. By deepening the network, we designed a\nfeasible INR parameter reuse scheme to further improve compression performance.\nExtensive experimental results show that our method significantly enhances the\nrate-distortion performance of INR video compression.\n", "link": "http://arxiv.org/abs/2410.01654v1", "date": "2024-10-02", "relevancy": 2.6506, "topK": [{"title": "Tuning-Free Noise Rectification for High Fidelity Image-to-Video\n  Generation", "link": "http://arxiv.org/abs/2403.02827v1", "similarity": 0.5605}, {"title": "IAE: Implicit Autoencoder for Point Cloud Self-supervised Representation Learning", "link": "https://arxiv.org/abs/2201.00785", "similarity": 0.5186}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5113}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Unleashing%20Parameter%20Potential%20of%20Neural%20Representation%20for%20Efficient%0A%20%20Video%20Compression&body=Title%3A%20Unleashing%20Parameter%20Potential%20of%20Neural%20Representation%20for%20Efficient%0A%20%20Video%20Compression%0AAuthor%3A%20Gai%20Zhang%20and%20Xinfeng%20Zhang%20and%20Lv%20Tang%20and%20Yue%20Li%20and%20Kai%20Zhang%20and%20Li%20Zhang%0AAbstract%3A%20%20%20For%20decades%2C%20video%20compression%20technology%20has%20been%20a%20prominent%20research%20area.%0ATraditional%20hybrid%20video%20compression%20framework%20and%20end-to-end%20frameworks%0Acontinue%20to%20explore%20various%20intra-%20and%20inter-frame%20reference%20and%20prediction%0Astrategies%20based%20on%20discrete%20transforms%20and%20deep%20learning%20techniques.%20However%2C%0Athe%20emerging%20implicit%20neural%20representation%20%28INR%29%20technique%20models%20entire%0Avideos%20as%20basic%20units%2C%20automatically%20capturing%20intra-frame%20and%20inter-frame%0Acorrelations%20and%20obtaining%20promising%20performance.%20INR%20uses%20a%20compact%20neural%0Anetwork%20to%20store%20video%20information%20in%20network%20parameters%2C%20effectively%0Aeliminating%20spatial%20and%20temporal%20redundancy%20in%20the%20original%20video.%20However%2C%20in%0Athis%20paper%2C%20our%20exploration%20and%20verification%20reveal%20that%20current%20INR%20video%0Acompression%20methods%20do%20not%20fully%20exploit%20their%20potential%20to%20preserve%0Ainformation.%20We%20investigate%20the%20potential%20of%20enhancing%20network%20parameter%0Astorage%20through%20parameter%20reuse.%20By%20deepening%20the%20network%2C%20we%20designed%20a%0Afeasible%20INR%20parameter%20reuse%20scheme%20to%20further%20improve%20compression%20performance.%0AExtensive%20experimental%20results%20show%20that%20our%20method%20significantly%20enhances%20the%0Arate-distortion%20performance%20of%20INR%20video%20compression.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2410.01654v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DUnleashing%2520Parameter%2520Potential%2520of%2520Neural%2520Representation%2520for%2520Efficient%250A%2520%2520Video%2520Compression%26entry.906535625%3DGai%2520Zhang%2520and%2520Xinfeng%2520Zhang%2520and%2520Lv%2520Tang%2520and%2520Yue%2520Li%2520and%2520Kai%2520Zhang%2520and%2520Li%2520Zhang%26entry.1292438233%3D%2520%2520For%2520decades%252C%2520video%2520compression%2520technology%2520has%2520been%2520a%2520prominent%2520research%2520area.%250ATraditional%2520hybrid%2520video%2520compression%2520framework%2520and%2520end-to-end%2520frameworks%250Acontinue%2520to%2520explore%2520various%2520intra-%2520and%2520inter-frame%2520reference%2520and%2520prediction%250Astrategies%2520based%2520on%2520discrete%2520transforms%2520and%2520deep%2520learning%2520techniques.%2520However%252C%250Athe%2520emerging%2520implicit%2520neural%2520representation%2520%2528INR%2529%2520technique%2520models%2520entire%250Avideos%2520as%2520basic%2520units%252C%2520automatically%2520capturing%2520intra-frame%2520and%2520inter-frame%250Acorrelations%2520and%2520obtaining%2520promising%2520performance.%2520INR%2520uses%2520a%2520compact%2520neural%250Anetwork%2520to%2520store%2520video%2520information%2520in%2520network%2520parameters%252C%2520effectively%250Aeliminating%2520spatial%2520and%2520temporal%2520redundancy%2520in%2520the%2520original%2520video.%2520However%252C%2520in%250Athis%2520paper%252C%2520our%2520exploration%2520and%2520verification%2520reveal%2520that%2520current%2520INR%2520video%250Acompression%2520methods%2520do%2520not%2520fully%2520exploit%2520their%2520potential%2520to%2520preserve%250Ainformation.%2520We%2520investigate%2520the%2520potential%2520of%2520enhancing%2520network%2520parameter%250Astorage%2520through%2520parameter%2520reuse.%2520By%2520deepening%2520the%2520network%252C%2520we%2520designed%2520a%250Afeasible%2520INR%2520parameter%2520reuse%2520scheme%2520to%2520further%2520improve%2520compression%2520performance.%250AExtensive%2520experimental%2520results%2520show%2520that%2520our%2520method%2520significantly%2520enhances%2520the%250Arate-distortion%2520performance%2520of%2520INR%2520video%2520compression.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2410.01654v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Unleashing%20Parameter%20Potential%20of%20Neural%20Representation%20for%20Efficient%0A%20%20Video%20Compression&entry.906535625=Gai%20Zhang%20and%20Xinfeng%20Zhang%20and%20Lv%20Tang%20and%20Yue%20Li%20and%20Kai%20Zhang%20and%20Li%20Zhang&entry.1292438233=%20%20For%20decades%2C%20video%20compression%20technology%20has%20been%20a%20prominent%20research%20area.%0ATraditional%20hybrid%20video%20compression%20framework%20and%20end-to-end%20frameworks%0Acontinue%20to%20explore%20various%20intra-%20and%20inter-frame%20reference%20and%20prediction%0Astrategies%20based%20on%20discrete%20transforms%20and%20deep%20learning%20techniques.%20However%2C%0Athe%20emerging%20implicit%20neural%20representation%20%28INR%29%20technique%20models%20entire%0Avideos%20as%20basic%20units%2C%20automatically%20capturing%20intra-frame%20and%20inter-frame%0Acorrelations%20and%20obtaining%20promising%20performance.%20INR%20uses%20a%20compact%20neural%0Anetwork%20to%20store%20video%20information%20in%20network%20parameters%2C%20effectively%0Aeliminating%20spatial%20and%20temporal%20redundancy%20in%20the%20original%20video.%20However%2C%20in%0Athis%20paper%2C%20our%20exploration%20and%20verification%20reveal%20that%20current%20INR%20video%0Acompression%20methods%20do%20not%20fully%20exploit%20their%20potential%20to%20preserve%0Ainformation.%20We%20investigate%20the%20potential%20of%20enhancing%20network%20parameter%0Astorage%20through%20parameter%20reuse.%20By%20deepening%20the%20network%2C%20we%20designed%20a%0Afeasible%20INR%20parameter%20reuse%20scheme%20to%20further%20improve%20compression%20performance.%0AExtensive%20experimental%20results%20show%20that%20our%20method%20significantly%20enhances%20the%0Arate-distortion%20performance%20of%20INR%20video%20compression.%0A&entry.1838667208=http%3A//arxiv.org/abs/2410.01654v1&entry.124074799=Read"},
{"title": "CodeGRAG: Bridging the Gap between Natural Language and Programming\n  Language via Graphical Retrieval Augmented Generation", "author": "Kounianhua Du and Jizheng Chen and Renting Rui and Huacan Chai and Lingyue Fu and Wei Xia and Yasheng Wang and Ruiming Tang and Yong Yu and Weinan Zhang", "abstract": "  Utilizing large language models to generate codes has shown promising meaning\nin software development revolution. Despite the intelligence shown by the\ngeneral large language models, their specificity in code generation can still\nbe improved due to the syntactic gap and mismatched vocabulary existing among\nnatural language and different programming languages. In this paper, we propose\nCodeGRAG, a Graphical Retrieval Augmented Code Generation framework to enhance\nthe performance of LLMs. CodeGRAG builds the graphical view of code blocks\nbased on the control flow and data flow of them to fill the gap between\nprogramming languages and natural language, which can facilitate natural\nlanguage based LLMs for better understanding of code syntax and serve as a\nbridge among different programming languages. To take the extracted structural\nknowledge into the foundation models, we propose 1) a hard meta-graph prompt\ntemplate to transform the challenging graphical representation into informative\nknowledge for tuning-free models and 2) a soft prompting technique that injects\nthe domain knowledge of programming languages into the model parameters via\nfinetuning the models with the help of a pretrained GNN expert model. Various\nexperiments and ablations are done on four datasets including both the C++ and\npython languages to validate the hard meta-graph prompt, the soft prompting\ntechnique, and the effectiveness of the objectives for pretrained GNN expert.\nCodeGRAG improves the code generation ability of LLMs and can even offer\nperformance gain for cross-lingual code generation. The implementation is\navailable at https://anonymous.4open.science/r/Code-5970/.\n", "link": "http://arxiv.org/abs/2405.02355v2", "date": "2024-10-02", "relevancy": 2.642, "topK": [{"title": "DressCode: Autoregressively Sewing and Generating Garments from Text\n  Guidance", "link": "http://arxiv.org/abs/2401.16465v3", "similarity": 0.5462}, {"title": "UGG: Unified Generative Grasping", "link": "https://arxiv.org/abs/2311.16917", "similarity": 0.5301}, {"title": "Genie: Generative Interactive Environments", "link": "http://arxiv.org/abs/2402.15391v1", "similarity": 0.5089}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20CodeGRAG%3A%20Bridging%20the%20Gap%20between%20Natural%20Language%20and%20Programming%0A%20%20Language%20via%20Graphical%20Retrieval%20Augmented%20Generation&body=Title%3A%20CodeGRAG%3A%20Bridging%20the%20Gap%20between%20Natural%20Language%20and%20Programming%0A%20%20Language%20via%20Graphical%20Retrieval%20Augmented%20Generation%0AAuthor%3A%20Kounianhua%20Du%20and%20Jizheng%20Chen%20and%20Renting%20Rui%20and%20Huacan%20Chai%20and%20Lingyue%20Fu%20and%20Wei%20Xia%20and%20Yasheng%20Wang%20and%20Ruiming%20Tang%20and%20Yong%20Yu%20and%20Weinan%20Zhang%0AAbstract%3A%20%20%20Utilizing%20large%20language%20models%20to%20generate%20codes%20has%20shown%20promising%20meaning%0Ain%20software%20development%20revolution.%20Despite%20the%20intelligence%20shown%20by%20the%0Ageneral%20large%20language%20models%2C%20their%20specificity%20in%20code%20generation%20can%20still%0Abe%20improved%20due%20to%20the%20syntactic%20gap%20and%20mismatched%20vocabulary%20existing%20among%0Anatural%20language%20and%20different%20programming%20languages.%20In%20this%20paper%2C%20we%20propose%0ACodeGRAG%2C%20a%20Graphical%20Retrieval%20Augmented%20Code%20Generation%20framework%20to%20enhance%0Athe%20performance%20of%20LLMs.%20CodeGRAG%20builds%20the%20graphical%20view%20of%20code%20blocks%0Abased%20on%20the%20control%20flow%20and%20data%20flow%20of%20them%20to%20fill%20the%20gap%20between%0Aprogramming%20languages%20and%20natural%20language%2C%20which%20can%20facilitate%20natural%0Alanguage%20based%20LLMs%20for%20better%20understanding%20of%20code%20syntax%20and%20serve%20as%20a%0Abridge%20among%20different%20programming%20languages.%20To%20take%20the%20extracted%20structural%0Aknowledge%20into%20the%20foundation%20models%2C%20we%20propose%201%29%20a%20hard%20meta-graph%20prompt%0Atemplate%20to%20transform%20the%20challenging%20graphical%20representation%20into%20informative%0Aknowledge%20for%20tuning-free%20models%20and%202%29%20a%20soft%20prompting%20technique%20that%20injects%0Athe%20domain%20knowledge%20of%20programming%20languages%20into%20the%20model%20parameters%20via%0Afinetuning%20the%20models%20with%20the%20help%20of%20a%20pretrained%20GNN%20expert%20model.%20Various%0Aexperiments%20and%20ablations%20are%20done%20on%20four%20datasets%20including%20both%20the%20C%2B%2B%20and%0Apython%20languages%20to%20validate%20the%20hard%20meta-graph%20prompt%2C%20the%20soft%20prompting%0Atechnique%2C%20and%20the%20effectiveness%20of%20the%20objectives%20for%20pretrained%20GNN%20expert.%0ACodeGRAG%20improves%20the%20code%20generation%20ability%20of%20LLMs%20and%20can%20even%20offer%0Aperformance%20gain%20for%20cross-lingual%20code%20generation.%20The%20implementation%20is%0Aavailable%20at%20https%3A//anonymous.4open.science/r/Code-5970/.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2405.02355v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DCodeGRAG%253A%2520Bridging%2520the%2520Gap%2520between%2520Natural%2520Language%2520and%2520Programming%250A%2520%2520Language%2520via%2520Graphical%2520Retrieval%2520Augmented%2520Generation%26entry.906535625%3DKounianhua%2520Du%2520and%2520Jizheng%2520Chen%2520and%2520Renting%2520Rui%2520and%2520Huacan%2520Chai%2520and%2520Lingyue%2520Fu%2520and%2520Wei%2520Xia%2520and%2520Yasheng%2520Wang%2520and%2520Ruiming%2520Tang%2520and%2520Yong%2520Yu%2520and%2520Weinan%2520Zhang%26entry.1292438233%3D%2520%2520Utilizing%2520large%2520language%2520models%2520to%2520generate%2520codes%2520has%2520shown%2520promising%2520meaning%250Ain%2520software%2520development%2520revolution.%2520Despite%2520the%2520intelligence%2520shown%2520by%2520the%250Ageneral%2520large%2520language%2520models%252C%2520their%2520specificity%2520in%2520code%2520generation%2520can%2520still%250Abe%2520improved%2520due%2520to%2520the%2520syntactic%2520gap%2520and%2520mismatched%2520vocabulary%2520existing%2520among%250Anatural%2520language%2520and%2520different%2520programming%2520languages.%2520In%2520this%2520paper%252C%2520we%2520propose%250ACodeGRAG%252C%2520a%2520Graphical%2520Retrieval%2520Augmented%2520Code%2520Generation%2520framework%2520to%2520enhance%250Athe%2520performance%2520of%2520LLMs.%2520CodeGRAG%2520builds%2520the%2520graphical%2520view%2520of%2520code%2520blocks%250Abased%2520on%2520the%2520control%2520flow%2520and%2520data%2520flow%2520of%2520them%2520to%2520fill%2520the%2520gap%2520between%250Aprogramming%2520languages%2520and%2520natural%2520language%252C%2520which%2520can%2520facilitate%2520natural%250Alanguage%2520based%2520LLMs%2520for%2520better%2520understanding%2520of%2520code%2520syntax%2520and%2520serve%2520as%2520a%250Abridge%2520among%2520different%2520programming%2520languages.%2520To%2520take%2520the%2520extracted%2520structural%250Aknowledge%2520into%2520the%2520foundation%2520models%252C%2520we%2520propose%25201%2529%2520a%2520hard%2520meta-graph%2520prompt%250Atemplate%2520to%2520transform%2520the%2520challenging%2520graphical%2520representation%2520into%2520informative%250Aknowledge%2520for%2520tuning-free%2520models%2520and%25202%2529%2520a%2520soft%2520prompting%2520technique%2520that%2520injects%250Athe%2520domain%2520knowledge%2520of%2520programming%2520languages%2520into%2520the%2520model%2520parameters%2520via%250Afinetuning%2520the%2520models%2520with%2520the%2520help%2520of%2520a%2520pretrained%2520GNN%2520expert%2520model.%2520Various%250Aexperiments%2520and%2520ablations%2520are%2520done%2520on%2520four%2520datasets%2520including%2520both%2520the%2520C%252B%252B%2520and%250Apython%2520languages%2520to%2520validate%2520the%2520hard%2520meta-graph%2520prompt%252C%2520the%2520soft%2520prompting%250Atechnique%252C%2520and%2520the%2520effectiveness%2520of%2520the%2520objectives%2520for%2520pretrained%2520GNN%2520expert.%250ACodeGRAG%2520improves%2520the%2520code%2520generation%2520ability%2520of%2520LLMs%2520and%2520can%2520even%2520offer%250Aperformance%2520gain%2520for%2520cross-lingual%2520code%2520generation.%2520The%2520implementation%2520is%250Aavailable%2520at%2520https%253A//anonymous.4open.science/r/Code-5970/.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2405.02355v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=CodeGRAG%3A%20Bridging%20the%20Gap%20between%20Natural%20Language%20and%20Programming%0A%20%20Language%20via%20Graphical%20Retrieval%20Augmented%20Generation&entry.906535625=Kounianhua%20Du%20and%20Jizheng%20Chen%20and%20Renting%20Rui%20and%20Huacan%20Chai%20and%20Lingyue%20Fu%20and%20Wei%20Xia%20and%20Yasheng%20Wang%20and%20Ruiming%20Tang%20and%20Yong%20Yu%20and%20Weinan%20Zhang&entry.1292438233=%20%20Utilizing%20large%20language%20models%20to%20generate%20codes%20has%20shown%20promising%20meaning%0Ain%20software%20development%20revolution.%20Despite%20the%20intelligence%20shown%20by%20the%0Ageneral%20large%20language%20models%2C%20their%20specificity%20in%20code%20generation%20can%20still%0Abe%20improved%20due%20to%20the%20syntactic%20gap%20and%20mismatched%20vocabulary%20existing%20among%0Anatural%20language%20and%20different%20programming%20languages.%20In%20this%20paper%2C%20we%20propose%0ACodeGRAG%2C%20a%20Graphical%20Retrieval%20Augmented%20Code%20Generation%20framework%20to%20enhance%0Athe%20performance%20of%20LLMs.%20CodeGRAG%20builds%20the%20graphical%20view%20of%20code%20blocks%0Abased%20on%20the%20control%20flow%20and%20data%20flow%20of%20them%20to%20fill%20the%20gap%20between%0Aprogramming%20languages%20and%20natural%20language%2C%20which%20can%20facilitate%20natural%0Alanguage%20based%20LLMs%20for%20better%20understanding%20of%20code%20syntax%20and%20serve%20as%20a%0Abridge%20among%20different%20programming%20languages.%20To%20take%20the%20extracted%20structural%0Aknowledge%20into%20the%20foundation%20models%2C%20we%20propose%201%29%20a%20hard%20meta-graph%20prompt%0Atemplate%20to%20transform%20the%20challenging%20graphical%20representation%20into%20informative%0Aknowledge%20for%20tuning-free%20models%20and%202%29%20a%20soft%20prompting%20technique%20that%20injects%0Athe%20domain%20knowledge%20of%20programming%20languages%20into%20the%20model%20parameters%20via%0Afinetuning%20the%20models%20with%20the%20help%20of%20a%20pretrained%20GNN%20expert%20model.%20Various%0Aexperiments%20and%20ablations%20are%20done%20on%20four%20datasets%20including%20both%20the%20C%2B%2B%20and%0Apython%20languages%20to%20validate%20the%20hard%20meta-graph%20prompt%2C%20the%20soft%20prompting%0Atechnique%2C%20and%20the%20effectiveness%20of%20the%20objectives%20for%20pretrained%20GNN%20expert.%0ACodeGRAG%20improves%20the%20code%20generation%20ability%20of%20LLMs%20and%20can%20even%20offer%0Aperformance%20gain%20for%20cross-lingual%20code%20generation.%20The%20implementation%20is%0Aavailable%20at%20https%3A//anonymous.4open.science/r/Code-5970/.%0A&entry.1838667208=http%3A//arxiv.org/abs/2405.02355v2&entry.124074799=Read"},
{"title": "EEG-Language Modeling for Pathology Detection", "author": "Sam Gijsen and Kerstin Ritter", "abstract": "  Multimodal language modeling constitutes a recent breakthrough which\nleverages advances in large language models to pretrain capable multimodal\nmodels. The integration of natural language during pretraining has been shown\nto significantly improve learned representations, particularly in computer\nvision. However, the efficacy of multimodal language modeling in the realm of\nfunctional brain data, specifically for advancing pathology detection, remains\nunexplored. This study pioneers EEG-language models trained on clinical reports\nand 15000 EEGs. We extend methods for multimodal alignment to this novel domain\nand investigate which textual information in reports is useful for training\nEEG-language models. Our results indicate that models learn richer\nrepresentations from being exposed to a variety of report segments, including\nthe patient's clinical history, description of the EEG, and the physician's\ninterpretation. Compared to models exposed to narrower clinical text\ninformation, we find such models to retrieve EEGs based on clinical reports\n(and vice versa) with substantially higher accuracy. Yet, this is only observed\nwhen using a contrastive learning approach. Particularly in regimes with few\nannotations, we observe that representations of EEG-language models can\nsignificantly improve pathology detection compared to those of EEG-only models,\nas demonstrated by both zero-shot classification and linear probes. In sum,\nthese results highlight the potential of integrating brain activity data with\nclinical text, suggesting that EEG-language models represent significant\nprogress for clinical applications.\n", "link": "http://arxiv.org/abs/2409.07480v2", "date": "2024-10-02", "relevancy": 2.6191, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5387}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5387}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.494}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20EEG-Language%20Modeling%20for%20Pathology%20Detection&body=Title%3A%20EEG-Language%20Modeling%20for%20Pathology%20Detection%0AAuthor%3A%20Sam%20Gijsen%20and%20Kerstin%20Ritter%0AAbstract%3A%20%20%20Multimodal%20language%20modeling%20constitutes%20a%20recent%20breakthrough%20which%0Aleverages%20advances%20in%20large%20language%20models%20to%20pretrain%20capable%20multimodal%0Amodels.%20The%20integration%20of%20natural%20language%20during%20pretraining%20has%20been%20shown%0Ato%20significantly%20improve%20learned%20representations%2C%20particularly%20in%20computer%0Avision.%20However%2C%20the%20efficacy%20of%20multimodal%20language%20modeling%20in%20the%20realm%20of%0Afunctional%20brain%20data%2C%20specifically%20for%20advancing%20pathology%20detection%2C%20remains%0Aunexplored.%20This%20study%20pioneers%20EEG-language%20models%20trained%20on%20clinical%20reports%0Aand%2015000%20EEGs.%20We%20extend%20methods%20for%20multimodal%20alignment%20to%20this%20novel%20domain%0Aand%20investigate%20which%20textual%20information%20in%20reports%20is%20useful%20for%20training%0AEEG-language%20models.%20Our%20results%20indicate%20that%20models%20learn%20richer%0Arepresentations%20from%20being%20exposed%20to%20a%20variety%20of%20report%20segments%2C%20including%0Athe%20patient%27s%20clinical%20history%2C%20description%20of%20the%20EEG%2C%20and%20the%20physician%27s%0Ainterpretation.%20Compared%20to%20models%20exposed%20to%20narrower%20clinical%20text%0Ainformation%2C%20we%20find%20such%20models%20to%20retrieve%20EEGs%20based%20on%20clinical%20reports%0A%28and%20vice%20versa%29%20with%20substantially%20higher%20accuracy.%20Yet%2C%20this%20is%20only%20observed%0Awhen%20using%20a%20contrastive%20learning%20approach.%20Particularly%20in%20regimes%20with%20few%0Aannotations%2C%20we%20observe%20that%20representations%20of%20EEG-language%20models%20can%0Asignificantly%20improve%20pathology%20detection%20compared%20to%20those%20of%20EEG-only%20models%2C%0Aas%20demonstrated%20by%20both%20zero-shot%20classification%20and%20linear%20probes.%20In%20sum%2C%0Athese%20results%20highlight%20the%20potential%20of%20integrating%20brain%20activity%20data%20with%0Aclinical%20text%2C%20suggesting%20that%20EEG-language%20models%20represent%20significant%0Aprogress%20for%20clinical%20applications.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2409.07480v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DEEG-Language%2520Modeling%2520for%2520Pathology%2520Detection%26entry.906535625%3DSam%2520Gijsen%2520and%2520Kerstin%2520Ritter%26entry.1292438233%3D%2520%2520Multimodal%2520language%2520modeling%2520constitutes%2520a%2520recent%2520breakthrough%2520which%250Aleverages%2520advances%2520in%2520large%2520language%2520models%2520to%2520pretrain%2520capable%2520multimodal%250Amodels.%2520The%2520integration%2520of%2520natural%2520language%2520during%2520pretraining%2520has%2520been%2520shown%250Ato%2520significantly%2520improve%2520learned%2520representations%252C%2520particularly%2520in%2520computer%250Avision.%2520However%252C%2520the%2520efficacy%2520of%2520multimodal%2520language%2520modeling%2520in%2520the%2520realm%2520of%250Afunctional%2520brain%2520data%252C%2520specifically%2520for%2520advancing%2520pathology%2520detection%252C%2520remains%250Aunexplored.%2520This%2520study%2520pioneers%2520EEG-language%2520models%2520trained%2520on%2520clinical%2520reports%250Aand%252015000%2520EEGs.%2520We%2520extend%2520methods%2520for%2520multimodal%2520alignment%2520to%2520this%2520novel%2520domain%250Aand%2520investigate%2520which%2520textual%2520information%2520in%2520reports%2520is%2520useful%2520for%2520training%250AEEG-language%2520models.%2520Our%2520results%2520indicate%2520that%2520models%2520learn%2520richer%250Arepresentations%2520from%2520being%2520exposed%2520to%2520a%2520variety%2520of%2520report%2520segments%252C%2520including%250Athe%2520patient%2527s%2520clinical%2520history%252C%2520description%2520of%2520the%2520EEG%252C%2520and%2520the%2520physician%2527s%250Ainterpretation.%2520Compared%2520to%2520models%2520exposed%2520to%2520narrower%2520clinical%2520text%250Ainformation%252C%2520we%2520find%2520such%2520models%2520to%2520retrieve%2520EEGs%2520based%2520on%2520clinical%2520reports%250A%2528and%2520vice%2520versa%2529%2520with%2520substantially%2520higher%2520accuracy.%2520Yet%252C%2520this%2520is%2520only%2520observed%250Awhen%2520using%2520a%2520contrastive%2520learning%2520approach.%2520Particularly%2520in%2520regimes%2520with%2520few%250Aannotations%252C%2520we%2520observe%2520that%2520representations%2520of%2520EEG-language%2520models%2520can%250Asignificantly%2520improve%2520pathology%2520detection%2520compared%2520to%2520those%2520of%2520EEG-only%2520models%252C%250Aas%2520demonstrated%2520by%2520both%2520zero-shot%2520classification%2520and%2520linear%2520probes.%2520In%2520sum%252C%250Athese%2520results%2520highlight%2520the%2520potential%2520of%2520integrating%2520brain%2520activity%2520data%2520with%250Aclinical%2520text%252C%2520suggesting%2520that%2520EEG-language%2520models%2520represent%2520significant%250Aprogress%2520for%2520clinical%2520applications.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2409.07480v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=EEG-Language%20Modeling%20for%20Pathology%20Detection&entry.906535625=Sam%20Gijsen%20and%20Kerstin%20Ritter&entry.1292438233=%20%20Multimodal%20language%20modeling%20constitutes%20a%20recent%20breakthrough%20which%0Aleverages%20advances%20in%20large%20language%20models%20to%20pretrain%20capable%20multimodal%0Amodels.%20The%20integration%20of%20natural%20language%20during%20pretraining%20has%20been%20shown%0Ato%20significantly%20improve%20learned%20representations%2C%20particularly%20in%20computer%0Avision.%20However%2C%20the%20efficacy%20of%20multimodal%20language%20modeling%20in%20the%20realm%20of%0Afunctional%20brain%20data%2C%20specifically%20for%20advancing%20pathology%20detection%2C%20remains%0Aunexplored.%20This%20study%20pioneers%20EEG-language%20models%20trained%20on%20clinical%20reports%0Aand%2015000%20EEGs.%20We%20extend%20methods%20for%20multimodal%20alignment%20to%20this%20novel%20domain%0Aand%20investigate%20which%20textual%20information%20in%20reports%20is%20useful%20for%20training%0AEEG-language%20models.%20Our%20results%20indicate%20that%20models%20learn%20richer%0Arepresentations%20from%20being%20exposed%20to%20a%20variety%20of%20report%20segments%2C%20including%0Athe%20patient%27s%20clinical%20history%2C%20description%20of%20the%20EEG%2C%20and%20the%20physician%27s%0Ainterpretation.%20Compared%20to%20models%20exposed%20to%20narrower%20clinical%20text%0Ainformation%2C%20we%20find%20such%20models%20to%20retrieve%20EEGs%20based%20on%20clinical%20reports%0A%28and%20vice%20versa%29%20with%20substantially%20higher%20accuracy.%20Yet%2C%20this%20is%20only%20observed%0Awhen%20using%20a%20contrastive%20learning%20approach.%20Particularly%20in%20regimes%20with%20few%0Aannotations%2C%20we%20observe%20that%20representations%20of%20EEG-language%20models%20can%0Asignificantly%20improve%20pathology%20detection%20compared%20to%20those%20of%20EEG-only%20models%2C%0Aas%20demonstrated%20by%20both%20zero-shot%20classification%20and%20linear%20probes.%20In%20sum%2C%0Athese%20results%20highlight%20the%20potential%20of%20integrating%20brain%20activity%20data%20with%0Aclinical%20text%2C%20suggesting%20that%20EEG-language%20models%20represent%20significant%0Aprogress%20for%20clinical%20applications.%0A&entry.1838667208=http%3A//arxiv.org/abs/2409.07480v2&entry.124074799=Read"},
{"title": "Morphology-based non-rigid registration of coronary computed tomography\n  and intravascular images through virtual catheter path optimization", "author": "Karim Kadry and Abhishek Karmakar and Andreas Schuh and Kersten Peterson and Michiel Schaap and David Marlevi and Charles Taylor and Elazer Edelman and Farhad Nezami", "abstract": "  Coronary computed tomography angiography (CCTA) provides 3D information on\nobstructive coronary artery disease, but cannot fully visualize high-resolution\nfeatures within the vessel wall. Intravascular imaging, in contrast, can\nspatially resolve atherosclerotic in cross sectional slices, but is limited in\ncapturing 3D relationships between each slice. Co-registering CCTA and\nintravascular images enables a variety of clinical research applications but is\ntime consuming and user-dependent. This is due to intravascular images\nsuffering from non-rigid distortions arising from irregularities in the imaging\ncatheter path. To address these issues, we present a morphology-based framework\nfor the rigid and non-rigid matching of intravascular images to CCTA images. To\ndo this, we find the optimal virtual catheter path that samples the coronary\nartery in CCTA image space to recapitulate the coronary artery morphology\nobserved in the intravascular image. We validate our framework on a\nmulti-center cohort of 40 patients using bifurcation landmarks as ground truth\nfor longitudinal and rotational registration. Our registration approach\nsignificantly outperforms other approaches for bifurcation alignment. By\nproviding a differentiable framework for multi-modal vascular co-registration,\nour framework reduces the manual effort required to conduct large-scale\nmulti-modal clinical studies and enables the development of machine\nlearning-based co-registration approaches.\n", "link": "http://arxiv.org/abs/2301.00060v2", "date": "2024-10-02", "relevancy": 2.6119, "topK": [{"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5348}, {"title": "DDM-NET: End-to-end learning of keypoint feature Detection, Description and Matching for 3D localization", "link": "https://arxiv.org/abs/2212.04575", "similarity": 0.5173}, {"title": "CAT3D: Create Anything in 3D with Multi-View Diffusion Models", "link": "http://arxiv.org/abs/2405.10314v1", "similarity": 0.515}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Morphology-based%20non-rigid%20registration%20of%20coronary%20computed%20tomography%0A%20%20and%20intravascular%20images%20through%20virtual%20catheter%20path%20optimization&body=Title%3A%20Morphology-based%20non-rigid%20registration%20of%20coronary%20computed%20tomography%0A%20%20and%20intravascular%20images%20through%20virtual%20catheter%20path%20optimization%0AAuthor%3A%20Karim%20Kadry%20and%20Abhishek%20Karmakar%20and%20Andreas%20Schuh%20and%20Kersten%20Peterson%20and%20Michiel%20Schaap%20and%20David%20Marlevi%20and%20Charles%20Taylor%20and%20Elazer%20Edelman%20and%20Farhad%20Nezami%0AAbstract%3A%20%20%20Coronary%20computed%20tomography%20angiography%20%28CCTA%29%20provides%203D%20information%20on%0Aobstructive%20coronary%20artery%20disease%2C%20but%20cannot%20fully%20visualize%20high-resolution%0Afeatures%20within%20the%20vessel%20wall.%20Intravascular%20imaging%2C%20in%20contrast%2C%20can%0Aspatially%20resolve%20atherosclerotic%20in%20cross%20sectional%20slices%2C%20but%20is%20limited%20in%0Acapturing%203D%20relationships%20between%20each%20slice.%20Co-registering%20CCTA%20and%0Aintravascular%20images%20enables%20a%20variety%20of%20clinical%20research%20applications%20but%20is%0Atime%20consuming%20and%20user-dependent.%20This%20is%20due%20to%20intravascular%20images%0Asuffering%20from%20non-rigid%20distortions%20arising%20from%20irregularities%20in%20the%20imaging%0Acatheter%20path.%20To%20address%20these%20issues%2C%20we%20present%20a%20morphology-based%20framework%0Afor%20the%20rigid%20and%20non-rigid%20matching%20of%20intravascular%20images%20to%20CCTA%20images.%20To%0Ado%20this%2C%20we%20find%20the%20optimal%20virtual%20catheter%20path%20that%20samples%20the%20coronary%0Aartery%20in%20CCTA%20image%20space%20to%20recapitulate%20the%20coronary%20artery%20morphology%0Aobserved%20in%20the%20intravascular%20image.%20We%20validate%20our%20framework%20on%20a%0Amulti-center%20cohort%20of%2040%20patients%20using%20bifurcation%20landmarks%20as%20ground%20truth%0Afor%20longitudinal%20and%20rotational%20registration.%20Our%20registration%20approach%0Asignificantly%20outperforms%20other%20approaches%20for%20bifurcation%20alignment.%20By%0Aproviding%20a%20differentiable%20framework%20for%20multi-modal%20vascular%20co-registration%2C%0Aour%20framework%20reduces%20the%20manual%20effort%20required%20to%20conduct%20large-scale%0Amulti-modal%20clinical%20studies%20and%20enables%20the%20development%20of%20machine%0Alearning-based%20co-registration%20approaches.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2301.00060v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DMorphology-based%2520non-rigid%2520registration%2520of%2520coronary%2520computed%2520tomography%250A%2520%2520and%2520intravascular%2520images%2520through%2520virtual%2520catheter%2520path%2520optimization%26entry.906535625%3DKarim%2520Kadry%2520and%2520Abhishek%2520Karmakar%2520and%2520Andreas%2520Schuh%2520and%2520Kersten%2520Peterson%2520and%2520Michiel%2520Schaap%2520and%2520David%2520Marlevi%2520and%2520Charles%2520Taylor%2520and%2520Elazer%2520Edelman%2520and%2520Farhad%2520Nezami%26entry.1292438233%3D%2520%2520Coronary%2520computed%2520tomography%2520angiography%2520%2528CCTA%2529%2520provides%25203D%2520information%2520on%250Aobstructive%2520coronary%2520artery%2520disease%252C%2520but%2520cannot%2520fully%2520visualize%2520high-resolution%250Afeatures%2520within%2520the%2520vessel%2520wall.%2520Intravascular%2520imaging%252C%2520in%2520contrast%252C%2520can%250Aspatially%2520resolve%2520atherosclerotic%2520in%2520cross%2520sectional%2520slices%252C%2520but%2520is%2520limited%2520in%250Acapturing%25203D%2520relationships%2520between%2520each%2520slice.%2520Co-registering%2520CCTA%2520and%250Aintravascular%2520images%2520enables%2520a%2520variety%2520of%2520clinical%2520research%2520applications%2520but%2520is%250Atime%2520consuming%2520and%2520user-dependent.%2520This%2520is%2520due%2520to%2520intravascular%2520images%250Asuffering%2520from%2520non-rigid%2520distortions%2520arising%2520from%2520irregularities%2520in%2520the%2520imaging%250Acatheter%2520path.%2520To%2520address%2520these%2520issues%252C%2520we%2520present%2520a%2520morphology-based%2520framework%250Afor%2520the%2520rigid%2520and%2520non-rigid%2520matching%2520of%2520intravascular%2520images%2520to%2520CCTA%2520images.%2520To%250Ado%2520this%252C%2520we%2520find%2520the%2520optimal%2520virtual%2520catheter%2520path%2520that%2520samples%2520the%2520coronary%250Aartery%2520in%2520CCTA%2520image%2520space%2520to%2520recapitulate%2520the%2520coronary%2520artery%2520morphology%250Aobserved%2520in%2520the%2520intravascular%2520image.%2520We%2520validate%2520our%2520framework%2520on%2520a%250Amulti-center%2520cohort%2520of%252040%2520patients%2520using%2520bifurcation%2520landmarks%2520as%2520ground%2520truth%250Afor%2520longitudinal%2520and%2520rotational%2520registration.%2520Our%2520registration%2520approach%250Asignificantly%2520outperforms%2520other%2520approaches%2520for%2520bifurcation%2520alignment.%2520By%250Aproviding%2520a%2520differentiable%2520framework%2520for%2520multi-modal%2520vascular%2520co-registration%252C%250Aour%2520framework%2520reduces%2520the%2520manual%2520effort%2520required%2520to%2520conduct%2520large-scale%250Amulti-modal%2520clinical%2520studies%2520and%2520enables%2520the%2520development%2520of%2520machine%250Alearning-based%2520co-registration%2520approaches.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2301.00060v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Morphology-based%20non-rigid%20registration%20of%20coronary%20computed%20tomography%0A%20%20and%20intravascular%20images%20through%20virtual%20catheter%20path%20optimization&entry.906535625=Karim%20Kadry%20and%20Abhishek%20Karmakar%20and%20Andreas%20Schuh%20and%20Kersten%20Peterson%20and%20Michiel%20Schaap%20and%20David%20Marlevi%20and%20Charles%20Taylor%20and%20Elazer%20Edelman%20and%20Farhad%20Nezami&entry.1292438233=%20%20Coronary%20computed%20tomography%20angiography%20%28CCTA%29%20provides%203D%20information%20on%0Aobstructive%20coronary%20artery%20disease%2C%20but%20cannot%20fully%20visualize%20high-resolution%0Afeatures%20within%20the%20vessel%20wall.%20Intravascular%20imaging%2C%20in%20contrast%2C%20can%0Aspatially%20resolve%20atherosclerotic%20in%20cross%20sectional%20slices%2C%20but%20is%20limited%20in%0Acapturing%203D%20relationships%20between%20each%20slice.%20Co-registering%20CCTA%20and%0Aintravascular%20images%20enables%20a%20variety%20of%20clinical%20research%20applications%20but%20is%0Atime%20consuming%20and%20user-dependent.%20This%20is%20due%20to%20intravascular%20images%0Asuffering%20from%20non-rigid%20distortions%20arising%20from%20irregularities%20in%20the%20imaging%0Acatheter%20path.%20To%20address%20these%20issues%2C%20we%20present%20a%20morphology-based%20framework%0Afor%20the%20rigid%20and%20non-rigid%20matching%20of%20intravascular%20images%20to%20CCTA%20images.%20To%0Ado%20this%2C%20we%20find%20the%20optimal%20virtual%20catheter%20path%20that%20samples%20the%20coronary%0Aartery%20in%20CCTA%20image%20space%20to%20recapitulate%20the%20coronary%20artery%20morphology%0Aobserved%20in%20the%20intravascular%20image.%20We%20validate%20our%20framework%20on%20a%0Amulti-center%20cohort%20of%2040%20patients%20using%20bifurcation%20landmarks%20as%20ground%20truth%0Afor%20longitudinal%20and%20rotational%20registration.%20Our%20registration%20approach%0Asignificantly%20outperforms%20other%20approaches%20for%20bifurcation%20alignment.%20By%0Aproviding%20a%20differentiable%20framework%20for%20multi-modal%20vascular%20co-registration%2C%0Aour%20framework%20reduces%20the%20manual%20effort%20required%20to%20conduct%20large-scale%0Amulti-modal%20clinical%20studies%20and%20enables%20the%20development%20of%20machine%0Alearning-based%20co-registration%20approaches.%0A&entry.1838667208=http%3A//arxiv.org/abs/2301.00060v2&entry.124074799=Read"},
{"title": "Learning Dynamics of LLM Finetuning", "author": "Yi Ren and Danica J. Sutherland", "abstract": "  Learning dynamics, which describes how the learning of specific training\nexamples influences the model's predictions on other examples, gives us a\npowerful tool for understanding the behavior of deep learning systems. We study\nthe learning dynamics of large language models during different types of\nfinetuning, by analyzing the step-wise decomposition of how influence\naccumulates among different potential responses. Our framework allows a uniform\ninterpretation of many interesting observations about the training of popular\nalgorithms for both instruction tuning and preference tuning. In particular, we\npropose a hypothetical explanation of why specific types of hallucination are\nstrengthened after finetuning, e.g., the model might use phrases or facts in\nthe response for question B to answer question A, or the model might keep\nrepeating similar simple phrases when generating responses. We also extend our\nframework and highlight a unique \"squeezing effect\" to explain a previously\nobserved phenomenon in off-policy direct preference optimization (DPO), where\nrunning DPO for too long makes even the desired outputs less likely. This\nframework also provides insights into where the benefits of on-policy DPO and\nother variants come from. The analysis not only provides a novel perspective of\nunderstanding LLM's finetuning but also inspires a simple, effective method to\nimprove alignment performance.\n", "link": "http://arxiv.org/abs/2407.10490v2", "date": "2024-10-02", "relevancy": 2.6113, "topK": [{"title": "Boosted dynamic neural networks", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/26302", "similarity": 0.5311}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5178}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5178}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Learning%20Dynamics%20of%20LLM%20Finetuning&body=Title%3A%20Learning%20Dynamics%20of%20LLM%20Finetuning%0AAuthor%3A%20Yi%20Ren%20and%20Danica%20J.%20Sutherland%0AAbstract%3A%20%20%20Learning%20dynamics%2C%20which%20describes%20how%20the%20learning%20of%20specific%20training%0Aexamples%20influences%20the%20model%27s%20predictions%20on%20other%20examples%2C%20gives%20us%20a%0Apowerful%20tool%20for%20understanding%20the%20behavior%20of%20deep%20learning%20systems.%20We%20study%0Athe%20learning%20dynamics%20of%20large%20language%20models%20during%20different%20types%20of%0Afinetuning%2C%20by%20analyzing%20the%20step-wise%20decomposition%20of%20how%20influence%0Aaccumulates%20among%20different%20potential%20responses.%20Our%20framework%20allows%20a%20uniform%0Ainterpretation%20of%20many%20interesting%20observations%20about%20the%20training%20of%20popular%0Aalgorithms%20for%20both%20instruction%20tuning%20and%20preference%20tuning.%20In%20particular%2C%20we%0Apropose%20a%20hypothetical%20explanation%20of%20why%20specific%20types%20of%20hallucination%20are%0Astrengthened%20after%20finetuning%2C%20e.g.%2C%20the%20model%20might%20use%20phrases%20or%20facts%20in%0Athe%20response%20for%20question%20B%20to%20answer%20question%20A%2C%20or%20the%20model%20might%20keep%0Arepeating%20similar%20simple%20phrases%20when%20generating%20responses.%20We%20also%20extend%20our%0Aframework%20and%20highlight%20a%20unique%20%22squeezing%20effect%22%20to%20explain%20a%20previously%0Aobserved%20phenomenon%20in%20off-policy%20direct%20preference%20optimization%20%28DPO%29%2C%20where%0Arunning%20DPO%20for%20too%20long%20makes%20even%20the%20desired%20outputs%20less%20likely.%20This%0Aframework%20also%20provides%20insights%20into%20where%20the%20benefits%20of%20on-policy%20DPO%20and%0Aother%20variants%20come%20from.%20The%20analysis%20not%20only%20provides%20a%20novel%20perspective%20of%0Aunderstanding%20LLM%27s%20finetuning%20but%20also%20inspires%20a%20simple%2C%20effective%20method%20to%0Aimprove%20alignment%20performance.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2407.10490v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DLearning%2520Dynamics%2520of%2520LLM%2520Finetuning%26entry.906535625%3DYi%2520Ren%2520and%2520Danica%2520J.%2520Sutherland%26entry.1292438233%3D%2520%2520Learning%2520dynamics%252C%2520which%2520describes%2520how%2520the%2520learning%2520of%2520specific%2520training%250Aexamples%2520influences%2520the%2520model%2527s%2520predictions%2520on%2520other%2520examples%252C%2520gives%2520us%2520a%250Apowerful%2520tool%2520for%2520understanding%2520the%2520behavior%2520of%2520deep%2520learning%2520systems.%2520We%2520study%250Athe%2520learning%2520dynamics%2520of%2520large%2520language%2520models%2520during%2520different%2520types%2520of%250Afinetuning%252C%2520by%2520analyzing%2520the%2520step-wise%2520decomposition%2520of%2520how%2520influence%250Aaccumulates%2520among%2520different%2520potential%2520responses.%2520Our%2520framework%2520allows%2520a%2520uniform%250Ainterpretation%2520of%2520many%2520interesting%2520observations%2520about%2520the%2520training%2520of%2520popular%250Aalgorithms%2520for%2520both%2520instruction%2520tuning%2520and%2520preference%2520tuning.%2520In%2520particular%252C%2520we%250Apropose%2520a%2520hypothetical%2520explanation%2520of%2520why%2520specific%2520types%2520of%2520hallucination%2520are%250Astrengthened%2520after%2520finetuning%252C%2520e.g.%252C%2520the%2520model%2520might%2520use%2520phrases%2520or%2520facts%2520in%250Athe%2520response%2520for%2520question%2520B%2520to%2520answer%2520question%2520A%252C%2520or%2520the%2520model%2520might%2520keep%250Arepeating%2520similar%2520simple%2520phrases%2520when%2520generating%2520responses.%2520We%2520also%2520extend%2520our%250Aframework%2520and%2520highlight%2520a%2520unique%2520%2522squeezing%2520effect%2522%2520to%2520explain%2520a%2520previously%250Aobserved%2520phenomenon%2520in%2520off-policy%2520direct%2520preference%2520optimization%2520%2528DPO%2529%252C%2520where%250Arunning%2520DPO%2520for%2520too%2520long%2520makes%2520even%2520the%2520desired%2520outputs%2520less%2520likely.%2520This%250Aframework%2520also%2520provides%2520insights%2520into%2520where%2520the%2520benefits%2520of%2520on-policy%2520DPO%2520and%250Aother%2520variants%2520come%2520from.%2520The%2520analysis%2520not%2520only%2520provides%2520a%2520novel%2520perspective%2520of%250Aunderstanding%2520LLM%2527s%2520finetuning%2520but%2520also%2520inspires%2520a%2520simple%252C%2520effective%2520method%2520to%250Aimprove%2520alignment%2520performance.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2407.10490v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Learning%20Dynamics%20of%20LLM%20Finetuning&entry.906535625=Yi%20Ren%20and%20Danica%20J.%20Sutherland&entry.1292438233=%20%20Learning%20dynamics%2C%20which%20describes%20how%20the%20learning%20of%20specific%20training%0Aexamples%20influences%20the%20model%27s%20predictions%20on%20other%20examples%2C%20gives%20us%20a%0Apowerful%20tool%20for%20understanding%20the%20behavior%20of%20deep%20learning%20systems.%20We%20study%0Athe%20learning%20dynamics%20of%20large%20language%20models%20during%20different%20types%20of%0Afinetuning%2C%20by%20analyzing%20the%20step-wise%20decomposition%20of%20how%20influence%0Aaccumulates%20among%20different%20potential%20responses.%20Our%20framework%20allows%20a%20uniform%0Ainterpretation%20of%20many%20interesting%20observations%20about%20the%20training%20of%20popular%0Aalgorithms%20for%20both%20instruction%20tuning%20and%20preference%20tuning.%20In%20particular%2C%20we%0Apropose%20a%20hypothetical%20explanation%20of%20why%20specific%20types%20of%20hallucination%20are%0Astrengthened%20after%20finetuning%2C%20e.g.%2C%20the%20model%20might%20use%20phrases%20or%20facts%20in%0Athe%20response%20for%20question%20B%20to%20answer%20question%20A%2C%20or%20the%20model%20might%20keep%0Arepeating%20similar%20simple%20phrases%20when%20generating%20responses.%20We%20also%20extend%20our%0Aframework%20and%20highlight%20a%20unique%20%22squeezing%20effect%22%20to%20explain%20a%20previously%0Aobserved%20phenomenon%20in%20off-policy%20direct%20preference%20optimization%20%28DPO%29%2C%20where%0Arunning%20DPO%20for%20too%20long%20makes%20even%20the%20desired%20outputs%20less%20likely.%20This%0Aframework%20also%20provides%20insights%20into%20where%20the%20benefits%20of%20on-policy%20DPO%20and%0Aother%20variants%20come%20from.%20The%20analysis%20not%20only%20provides%20a%20novel%20perspective%20of%0Aunderstanding%20LLM%27s%20finetuning%20but%20also%20inspires%20a%20simple%2C%20effective%20method%20to%0Aimprove%20alignment%20performance.%0A&entry.1838667208=http%3A//arxiv.org/abs/2407.10490v2&entry.124074799=Read"},
{"title": "Adaptive teachers for amortized samplers", "author": "Minsu Kim and Sanghyeok Choi and Taeyoung Yun and Emmanuel Bengio and Leo Feng and Jarrid Rector-Brooks and Sungsoo Ahn and Jinkyoo Park and Nikolay Malkin and Yoshua Bengio", "abstract": "  Amortized inference is the task of training a parametric model, such as a\nneural network, to approximate a distribution with a given unnormalized density\nwhere exact sampling is intractable. When sampling is implemented as a\nsequential decision-making process, reinforcement learning (RL) methods, such\nas generative flow networks, can be used to train the sampling policy.\nOff-policy RL training facilitates the discovery of diverse, high-reward\ncandidates, but existing methods still face challenges in efficient\nexploration. We propose to use an adaptive training distribution (the Teacher)\nto guide the training of the primary amortized sampler (the Student) by\nprioritizing high-loss regions. The Teacher, an auxiliary behavior model, is\ntrained to sample high-error regions of the Student and can generalize across\nunexplored modes, thereby enhancing mode coverage by providing an efficient\ntraining curriculum. We validate the effectiveness of this approach in a\nsynthetic environment designed to present an exploration challenge, two\ndiffusion-based sampling tasks, and four biochemical discovery tasks\ndemonstrating its ability to improve sample efficiency and mode coverage.\n", "link": "http://arxiv.org/abs/2410.01432v1", "date": "2024-10-02", "relevancy": 2.5884, "topK": [{"title": "Breadcrumbs: Adversarial class-balanced sampling for long-tailed recognition", "link": "https://link.springer.com/chapter/10.1007/978-3-031-20053-3_37", "similarity": 0.5473}, {"title": "ToDo: Token Downsampling for Efficient Generation of High-Resolution\n  Images", "link": "http://arxiv.org/abs/2402.13573v2", "similarity": 0.5071}, {"title": "Safe and efficient training of a control agent", "link": "https://patents.google.com/patent/US11709462B2/en", "similarity": 0.4986}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Adaptive%20teachers%20for%20amortized%20samplers&body=Title%3A%20Adaptive%20teachers%20for%20amortized%20samplers%0AAuthor%3A%20Minsu%20Kim%20and%20Sanghyeok%20Choi%20and%20Taeyoung%20Yun%20and%20Emmanuel%20Bengio%20and%20Leo%20Feng%20and%20Jarrid%20Rector-Brooks%20and%20Sungsoo%20Ahn%20and%20Jinkyoo%20Park%20and%20Nikolay%20Malkin%20and%20Yoshua%20Bengio%0AAbstract%3A%20%20%20Amortized%20inference%20is%20the%20task%20of%20training%20a%20parametric%20model%2C%20such%20as%20a%0Aneural%20network%2C%20to%20approximate%20a%20distribution%20with%20a%20given%20unnormalized%20density%0Awhere%20exact%20sampling%20is%20intractable.%20When%20sampling%20is%20implemented%20as%20a%0Asequential%20decision-making%20process%2C%20reinforcement%20learning%20%28RL%29%20methods%2C%20such%0Aas%20generative%20flow%20networks%2C%20can%20be%20used%20to%20train%20the%20sampling%20policy.%0AOff-policy%20RL%20training%20facilitates%20the%20discovery%20of%20diverse%2C%20high-reward%0Acandidates%2C%20but%20existing%20methods%20still%20face%20challenges%20in%20efficient%0Aexploration.%20We%20propose%20to%20use%20an%20adaptive%20training%20distribution%20%28the%20Teacher%29%0Ato%20guide%20the%20training%20of%20the%20primary%20amortized%20sampler%20%28the%20Student%29%20by%0Aprioritizing%20high-loss%20regions.%20The%20Teacher%2C%20an%20auxiliary%20behavior%20model%2C%20is%0Atrained%20to%20sample%20high-error%20regions%20of%20the%20Student%20and%20can%20generalize%20across%0Aunexplored%20modes%2C%20thereby%20enhancing%20mode%20coverage%20by%20providing%20an%20efficient%0Atraining%20curriculum.%20We%20validate%20the%20effectiveness%20of%20this%20approach%20in%20a%0Asynthetic%20environment%20designed%20to%20present%20an%20exploration%20challenge%2C%20two%0Adiffusion-based%20sampling%20tasks%2C%20and%20four%20biochemical%20discovery%20tasks%0Ademonstrating%20its%20ability%20to%20improve%20sample%20efficiency%20and%20mode%20coverage.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2410.01432v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DAdaptive%2520teachers%2520for%2520amortized%2520samplers%26entry.906535625%3DMinsu%2520Kim%2520and%2520Sanghyeok%2520Choi%2520and%2520Taeyoung%2520Yun%2520and%2520Emmanuel%2520Bengio%2520and%2520Leo%2520Feng%2520and%2520Jarrid%2520Rector-Brooks%2520and%2520Sungsoo%2520Ahn%2520and%2520Jinkyoo%2520Park%2520and%2520Nikolay%2520Malkin%2520and%2520Yoshua%2520Bengio%26entry.1292438233%3D%2520%2520Amortized%2520inference%2520is%2520the%2520task%2520of%2520training%2520a%2520parametric%2520model%252C%2520such%2520as%2520a%250Aneural%2520network%252C%2520to%2520approximate%2520a%2520distribution%2520with%2520a%2520given%2520unnormalized%2520density%250Awhere%2520exact%2520sampling%2520is%2520intractable.%2520When%2520sampling%2520is%2520implemented%2520as%2520a%250Asequential%2520decision-making%2520process%252C%2520reinforcement%2520learning%2520%2528RL%2529%2520methods%252C%2520such%250Aas%2520generative%2520flow%2520networks%252C%2520can%2520be%2520used%2520to%2520train%2520the%2520sampling%2520policy.%250AOff-policy%2520RL%2520training%2520facilitates%2520the%2520discovery%2520of%2520diverse%252C%2520high-reward%250Acandidates%252C%2520but%2520existing%2520methods%2520still%2520face%2520challenges%2520in%2520efficient%250Aexploration.%2520We%2520propose%2520to%2520use%2520an%2520adaptive%2520training%2520distribution%2520%2528the%2520Teacher%2529%250Ato%2520guide%2520the%2520training%2520of%2520the%2520primary%2520amortized%2520sampler%2520%2528the%2520Student%2529%2520by%250Aprioritizing%2520high-loss%2520regions.%2520The%2520Teacher%252C%2520an%2520auxiliary%2520behavior%2520model%252C%2520is%250Atrained%2520to%2520sample%2520high-error%2520regions%2520of%2520the%2520Student%2520and%2520can%2520generalize%2520across%250Aunexplored%2520modes%252C%2520thereby%2520enhancing%2520mode%2520coverage%2520by%2520providing%2520an%2520efficient%250Atraining%2520curriculum.%2520We%2520validate%2520the%2520effectiveness%2520of%2520this%2520approach%2520in%2520a%250Asynthetic%2520environment%2520designed%2520to%2520present%2520an%2520exploration%2520challenge%252C%2520two%250Adiffusion-based%2520sampling%2520tasks%252C%2520and%2520four%2520biochemical%2520discovery%2520tasks%250Ademonstrating%2520its%2520ability%2520to%2520improve%2520sample%2520efficiency%2520and%2520mode%2520coverage.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2410.01432v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Adaptive%20teachers%20for%20amortized%20samplers&entry.906535625=Minsu%20Kim%20and%20Sanghyeok%20Choi%20and%20Taeyoung%20Yun%20and%20Emmanuel%20Bengio%20and%20Leo%20Feng%20and%20Jarrid%20Rector-Brooks%20and%20Sungsoo%20Ahn%20and%20Jinkyoo%20Park%20and%20Nikolay%20Malkin%20and%20Yoshua%20Bengio&entry.1292438233=%20%20Amortized%20inference%20is%20the%20task%20of%20training%20a%20parametric%20model%2C%20such%20as%20a%0Aneural%20network%2C%20to%20approximate%20a%20distribution%20with%20a%20given%20unnormalized%20density%0Awhere%20exact%20sampling%20is%20intractable.%20When%20sampling%20is%20implemented%20as%20a%0Asequential%20decision-making%20process%2C%20reinforcement%20learning%20%28RL%29%20methods%2C%20such%0Aas%20generative%20flow%20networks%2C%20can%20be%20used%20to%20train%20the%20sampling%20policy.%0AOff-policy%20RL%20training%20facilitates%20the%20discovery%20of%20diverse%2C%20high-reward%0Acandidates%2C%20but%20existing%20methods%20still%20face%20challenges%20in%20efficient%0Aexploration.%20We%20propose%20to%20use%20an%20adaptive%20training%20distribution%20%28the%20Teacher%29%0Ato%20guide%20the%20training%20of%20the%20primary%20amortized%20sampler%20%28the%20Student%29%20by%0Aprioritizing%20high-loss%20regions.%20The%20Teacher%2C%20an%20auxiliary%20behavior%20model%2C%20is%0Atrained%20to%20sample%20high-error%20regions%20of%20the%20Student%20and%20can%20generalize%20across%0Aunexplored%20modes%2C%20thereby%20enhancing%20mode%20coverage%20by%20providing%20an%20efficient%0Atraining%20curriculum.%20We%20validate%20the%20effectiveness%20of%20this%20approach%20in%20a%0Asynthetic%20environment%20designed%20to%20present%20an%20exploration%20challenge%2C%20two%0Adiffusion-based%20sampling%20tasks%2C%20and%20four%20biochemical%20discovery%20tasks%0Ademonstrating%20its%20ability%20to%20improve%20sample%20efficiency%20and%20mode%20coverage.%0A&entry.1838667208=http%3A//arxiv.org/abs/2410.01432v1&entry.124074799=Read"},
{"title": "Concept-skill Transferability-based Data Selection for Large\n  Vision-Language Models", "author": "Jaewoo Lee and Boyang Li and Sung Ju Hwang", "abstract": "  Instruction tuning, or supervised finetuning on extensive task-specific data,\nis necessary for Large Vision-Language Models (LVLMs) to generalize well across\na broad range of vision-language (VL) tasks. However, training on large VL\ndatasets can become prohibitively expensive. In this work, we introduce\nCOINCIDE, an effective and scalable data selection technique that uses a small\nmodel as a reference model to select visual instruction tuning data for\nefficient finetuning of a target LVLM, focusing on diversity and\ntransferability. Specifically, we cluster the training data using internal\nactivations from a small model, which identifies VL concept-skill compositions\nneeded by a target LVLM. We then sample data from these diverse clusters by\nconsidering their density and transferability, or the ability to transfer well\nto other concept-skill compositions. This approach ensures the diversity of\nthese compositions, which is vital for LVLM generalization. Extensive\nexperiments demonstrate that COINCIDE achieves superior performance and data\nselection efficiency against 8 strong baselines on two distinct datasets:\nLLaVA-1.5 and Vision-Flan. Using only 20% of the LLaVA-1.5 dataset, COINCIDE\nachieves performance comparable to the LVLM finetuned on the whole dataset,\nwith 70% reduction of the wall-clock running time. On the Vision-Flan dataset,\nour method achieves superior results with only 16.7% of the training data.\n", "link": "http://arxiv.org/abs/2406.10995v2", "date": "2024-10-02", "relevancy": 2.577, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5248}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5248}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.4966}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Concept-skill%20Transferability-based%20Data%20Selection%20for%20Large%0A%20%20Vision-Language%20Models&body=Title%3A%20Concept-skill%20Transferability-based%20Data%20Selection%20for%20Large%0A%20%20Vision-Language%20Models%0AAuthor%3A%20Jaewoo%20Lee%20and%20Boyang%20Li%20and%20Sung%20Ju%20Hwang%0AAbstract%3A%20%20%20Instruction%20tuning%2C%20or%20supervised%20finetuning%20on%20extensive%20task-specific%20data%2C%0Ais%20necessary%20for%20Large%20Vision-Language%20Models%20%28LVLMs%29%20to%20generalize%20well%20across%0Aa%20broad%20range%20of%20vision-language%20%28VL%29%20tasks.%20However%2C%20training%20on%20large%20VL%0Adatasets%20can%20become%20prohibitively%20expensive.%20In%20this%20work%2C%20we%20introduce%0ACOINCIDE%2C%20an%20effective%20and%20scalable%20data%20selection%20technique%20that%20uses%20a%20small%0Amodel%20as%20a%20reference%20model%20to%20select%20visual%20instruction%20tuning%20data%20for%0Aefficient%20finetuning%20of%20a%20target%20LVLM%2C%20focusing%20on%20diversity%20and%0Atransferability.%20Specifically%2C%20we%20cluster%20the%20training%20data%20using%20internal%0Aactivations%20from%20a%20small%20model%2C%20which%20identifies%20VL%20concept-skill%20compositions%0Aneeded%20by%20a%20target%20LVLM.%20We%20then%20sample%20data%20from%20these%20diverse%20clusters%20by%0Aconsidering%20their%20density%20and%20transferability%2C%20or%20the%20ability%20to%20transfer%20well%0Ato%20other%20concept-skill%20compositions.%20This%20approach%20ensures%20the%20diversity%20of%0Athese%20compositions%2C%20which%20is%20vital%20for%20LVLM%20generalization.%20Extensive%0Aexperiments%20demonstrate%20that%20COINCIDE%20achieves%20superior%20performance%20and%20data%0Aselection%20efficiency%20against%208%20strong%20baselines%20on%20two%20distinct%20datasets%3A%0ALLaVA-1.5%20and%20Vision-Flan.%20Using%20only%2020%25%20of%20the%20LLaVA-1.5%20dataset%2C%20COINCIDE%0Aachieves%20performance%20comparable%20to%20the%20LVLM%20finetuned%20on%20the%20whole%20dataset%2C%0Awith%2070%25%20reduction%20of%20the%20wall-clock%20running%20time.%20On%20the%20Vision-Flan%20dataset%2C%0Aour%20method%20achieves%20superior%20results%20with%20only%2016.7%25%20of%20the%20training%20data.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2406.10995v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DConcept-skill%2520Transferability-based%2520Data%2520Selection%2520for%2520Large%250A%2520%2520Vision-Language%2520Models%26entry.906535625%3DJaewoo%2520Lee%2520and%2520Boyang%2520Li%2520and%2520Sung%2520Ju%2520Hwang%26entry.1292438233%3D%2520%2520Instruction%2520tuning%252C%2520or%2520supervised%2520finetuning%2520on%2520extensive%2520task-specific%2520data%252C%250Ais%2520necessary%2520for%2520Large%2520Vision-Language%2520Models%2520%2528LVLMs%2529%2520to%2520generalize%2520well%2520across%250Aa%2520broad%2520range%2520of%2520vision-language%2520%2528VL%2529%2520tasks.%2520However%252C%2520training%2520on%2520large%2520VL%250Adatasets%2520can%2520become%2520prohibitively%2520expensive.%2520In%2520this%2520work%252C%2520we%2520introduce%250ACOINCIDE%252C%2520an%2520effective%2520and%2520scalable%2520data%2520selection%2520technique%2520that%2520uses%2520a%2520small%250Amodel%2520as%2520a%2520reference%2520model%2520to%2520select%2520visual%2520instruction%2520tuning%2520data%2520for%250Aefficient%2520finetuning%2520of%2520a%2520target%2520LVLM%252C%2520focusing%2520on%2520diversity%2520and%250Atransferability.%2520Specifically%252C%2520we%2520cluster%2520the%2520training%2520data%2520using%2520internal%250Aactivations%2520from%2520a%2520small%2520model%252C%2520which%2520identifies%2520VL%2520concept-skill%2520compositions%250Aneeded%2520by%2520a%2520target%2520LVLM.%2520We%2520then%2520sample%2520data%2520from%2520these%2520diverse%2520clusters%2520by%250Aconsidering%2520their%2520density%2520and%2520transferability%252C%2520or%2520the%2520ability%2520to%2520transfer%2520well%250Ato%2520other%2520concept-skill%2520compositions.%2520This%2520approach%2520ensures%2520the%2520diversity%2520of%250Athese%2520compositions%252C%2520which%2520is%2520vital%2520for%2520LVLM%2520generalization.%2520Extensive%250Aexperiments%2520demonstrate%2520that%2520COINCIDE%2520achieves%2520superior%2520performance%2520and%2520data%250Aselection%2520efficiency%2520against%25208%2520strong%2520baselines%2520on%2520two%2520distinct%2520datasets%253A%250ALLaVA-1.5%2520and%2520Vision-Flan.%2520Using%2520only%252020%2525%2520of%2520the%2520LLaVA-1.5%2520dataset%252C%2520COINCIDE%250Aachieves%2520performance%2520comparable%2520to%2520the%2520LVLM%2520finetuned%2520on%2520the%2520whole%2520dataset%252C%250Awith%252070%2525%2520reduction%2520of%2520the%2520wall-clock%2520running%2520time.%2520On%2520the%2520Vision-Flan%2520dataset%252C%250Aour%2520method%2520achieves%2520superior%2520results%2520with%2520only%252016.7%2525%2520of%2520the%2520training%2520data.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2406.10995v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Concept-skill%20Transferability-based%20Data%20Selection%20for%20Large%0A%20%20Vision-Language%20Models&entry.906535625=Jaewoo%20Lee%20and%20Boyang%20Li%20and%20Sung%20Ju%20Hwang&entry.1292438233=%20%20Instruction%20tuning%2C%20or%20supervised%20finetuning%20on%20extensive%20task-specific%20data%2C%0Ais%20necessary%20for%20Large%20Vision-Language%20Models%20%28LVLMs%29%20to%20generalize%20well%20across%0Aa%20broad%20range%20of%20vision-language%20%28VL%29%20tasks.%20However%2C%20training%20on%20large%20VL%0Adatasets%20can%20become%20prohibitively%20expensive.%20In%20this%20work%2C%20we%20introduce%0ACOINCIDE%2C%20an%20effective%20and%20scalable%20data%20selection%20technique%20that%20uses%20a%20small%0Amodel%20as%20a%20reference%20model%20to%20select%20visual%20instruction%20tuning%20data%20for%0Aefficient%20finetuning%20of%20a%20target%20LVLM%2C%20focusing%20on%20diversity%20and%0Atransferability.%20Specifically%2C%20we%20cluster%20the%20training%20data%20using%20internal%0Aactivations%20from%20a%20small%20model%2C%20which%20identifies%20VL%20concept-skill%20compositions%0Aneeded%20by%20a%20target%20LVLM.%20We%20then%20sample%20data%20from%20these%20diverse%20clusters%20by%0Aconsidering%20their%20density%20and%20transferability%2C%20or%20the%20ability%20to%20transfer%20well%0Ato%20other%20concept-skill%20compositions.%20This%20approach%20ensures%20the%20diversity%20of%0Athese%20compositions%2C%20which%20is%20vital%20for%20LVLM%20generalization.%20Extensive%0Aexperiments%20demonstrate%20that%20COINCIDE%20achieves%20superior%20performance%20and%20data%0Aselection%20efficiency%20against%208%20strong%20baselines%20on%20two%20distinct%20datasets%3A%0ALLaVA-1.5%20and%20Vision-Flan.%20Using%20only%2020%25%20of%20the%20LLaVA-1.5%20dataset%2C%20COINCIDE%0Aachieves%20performance%20comparable%20to%20the%20LVLM%20finetuned%20on%20the%20whole%20dataset%2C%0Awith%2070%25%20reduction%20of%20the%20wall-clock%20running%20time.%20On%20the%20Vision-Flan%20dataset%2C%0Aour%20method%20achieves%20superior%20results%20with%20only%2016.7%25%20of%20the%20training%20data.%0A&entry.1838667208=http%3A//arxiv.org/abs/2406.10995v2&entry.124074799=Read"},
{"title": "Verbalized Graph Representation Learning: A Fully Interpretable Graph\n  Model Based on Large Language Models Throughout the Entire Process", "author": "Xingyu Ji and Jiale Liu and Lu Li and Maojun Wang and Zeyu Zhang", "abstract": "  Representation learning on text-attributed graphs (TAGs) has attracted\nsignificant interest due to its wide-ranging real-world applications,\nparticularly through Graph Neural Networks (GNNs). Traditional GNN methods\nfocus on encoding the structural information of graphs, often using shallow\ntext embeddings for node or edge attributes. This limits the model to\nunderstand the rich semantic information in the data and its reasoning ability\nfor complex downstream tasks, while also lacking interpretability. With the\nrise of large language models (LLMs), an increasing number of studies are\ncombining them with GNNs for graph representation learning and downstream\ntasks. While these approaches effectively leverage the rich semantic\ninformation in TAGs datasets, their main drawback is that they are only\npartially interpretable, which limits their application in critical fields. In\nthis paper, we propose a verbalized graph representation learning (VGRL) method\nwhich is fully interpretable. In contrast to traditional graph machine learning\nmodels, which are usually optimized within a continuous parameter space, VGRL\nconstrains this parameter space to be text description which ensures complete\ninterpretability throughout the entire process, making it easier for users to\nunderstand and trust the decisions of the model. We conduct several studies to\nempirically evaluate the effectiveness of VGRL and we believe these method can\nserve as a stepping stone in graph representation learning.\n", "link": "http://arxiv.org/abs/2410.01457v1", "date": "2024-10-02", "relevancy": 2.5691, "topK": [{"title": "Self-supervised Photographic Image Layout Representation Learning", "link": "http://arxiv.org/abs/2403.03740v1", "similarity": 0.5239}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5088}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5088}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Verbalized%20Graph%20Representation%20Learning%3A%20A%20Fully%20Interpretable%20Graph%0A%20%20Model%20Based%20on%20Large%20Language%20Models%20Throughout%20the%20Entire%20Process&body=Title%3A%20Verbalized%20Graph%20Representation%20Learning%3A%20A%20Fully%20Interpretable%20Graph%0A%20%20Model%20Based%20on%20Large%20Language%20Models%20Throughout%20the%20Entire%20Process%0AAuthor%3A%20Xingyu%20Ji%20and%20Jiale%20Liu%20and%20Lu%20Li%20and%20Maojun%20Wang%20and%20Zeyu%20Zhang%0AAbstract%3A%20%20%20Representation%20learning%20on%20text-attributed%20graphs%20%28TAGs%29%20has%20attracted%0Asignificant%20interest%20due%20to%20its%20wide-ranging%20real-world%20applications%2C%0Aparticularly%20through%20Graph%20Neural%20Networks%20%28GNNs%29.%20Traditional%20GNN%20methods%0Afocus%20on%20encoding%20the%20structural%20information%20of%20graphs%2C%20often%20using%20shallow%0Atext%20embeddings%20for%20node%20or%20edge%20attributes.%20This%20limits%20the%20model%20to%0Aunderstand%20the%20rich%20semantic%20information%20in%20the%20data%20and%20its%20reasoning%20ability%0Afor%20complex%20downstream%20tasks%2C%20while%20also%20lacking%20interpretability.%20With%20the%0Arise%20of%20large%20language%20models%20%28LLMs%29%2C%20an%20increasing%20number%20of%20studies%20are%0Acombining%20them%20with%20GNNs%20for%20graph%20representation%20learning%20and%20downstream%0Atasks.%20While%20these%20approaches%20effectively%20leverage%20the%20rich%20semantic%0Ainformation%20in%20TAGs%20datasets%2C%20their%20main%20drawback%20is%20that%20they%20are%20only%0Apartially%20interpretable%2C%20which%20limits%20their%20application%20in%20critical%20fields.%20In%0Athis%20paper%2C%20we%20propose%20a%20verbalized%20graph%20representation%20learning%20%28VGRL%29%20method%0Awhich%20is%20fully%20interpretable.%20In%20contrast%20to%20traditional%20graph%20machine%20learning%0Amodels%2C%20which%20are%20usually%20optimized%20within%20a%20continuous%20parameter%20space%2C%20VGRL%0Aconstrains%20this%20parameter%20space%20to%20be%20text%20description%20which%20ensures%20complete%0Ainterpretability%20throughout%20the%20entire%20process%2C%20making%20it%20easier%20for%20users%20to%0Aunderstand%20and%20trust%20the%20decisions%20of%20the%20model.%20We%20conduct%20several%20studies%20to%0Aempirically%20evaluate%20the%20effectiveness%20of%20VGRL%20and%20we%20believe%20these%20method%20can%0Aserve%20as%20a%20stepping%20stone%20in%20graph%20representation%20learning.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2410.01457v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DVerbalized%2520Graph%2520Representation%2520Learning%253A%2520A%2520Fully%2520Interpretable%2520Graph%250A%2520%2520Model%2520Based%2520on%2520Large%2520Language%2520Models%2520Throughout%2520the%2520Entire%2520Process%26entry.906535625%3DXingyu%2520Ji%2520and%2520Jiale%2520Liu%2520and%2520Lu%2520Li%2520and%2520Maojun%2520Wang%2520and%2520Zeyu%2520Zhang%26entry.1292438233%3D%2520%2520Representation%2520learning%2520on%2520text-attributed%2520graphs%2520%2528TAGs%2529%2520has%2520attracted%250Asignificant%2520interest%2520due%2520to%2520its%2520wide-ranging%2520real-world%2520applications%252C%250Aparticularly%2520through%2520Graph%2520Neural%2520Networks%2520%2528GNNs%2529.%2520Traditional%2520GNN%2520methods%250Afocus%2520on%2520encoding%2520the%2520structural%2520information%2520of%2520graphs%252C%2520often%2520using%2520shallow%250Atext%2520embeddings%2520for%2520node%2520or%2520edge%2520attributes.%2520This%2520limits%2520the%2520model%2520to%250Aunderstand%2520the%2520rich%2520semantic%2520information%2520in%2520the%2520data%2520and%2520its%2520reasoning%2520ability%250Afor%2520complex%2520downstream%2520tasks%252C%2520while%2520also%2520lacking%2520interpretability.%2520With%2520the%250Arise%2520of%2520large%2520language%2520models%2520%2528LLMs%2529%252C%2520an%2520increasing%2520number%2520of%2520studies%2520are%250Acombining%2520them%2520with%2520GNNs%2520for%2520graph%2520representation%2520learning%2520and%2520downstream%250Atasks.%2520While%2520these%2520approaches%2520effectively%2520leverage%2520the%2520rich%2520semantic%250Ainformation%2520in%2520TAGs%2520datasets%252C%2520their%2520main%2520drawback%2520is%2520that%2520they%2520are%2520only%250Apartially%2520interpretable%252C%2520which%2520limits%2520their%2520application%2520in%2520critical%2520fields.%2520In%250Athis%2520paper%252C%2520we%2520propose%2520a%2520verbalized%2520graph%2520representation%2520learning%2520%2528VGRL%2529%2520method%250Awhich%2520is%2520fully%2520interpretable.%2520In%2520contrast%2520to%2520traditional%2520graph%2520machine%2520learning%250Amodels%252C%2520which%2520are%2520usually%2520optimized%2520within%2520a%2520continuous%2520parameter%2520space%252C%2520VGRL%250Aconstrains%2520this%2520parameter%2520space%2520to%2520be%2520text%2520description%2520which%2520ensures%2520complete%250Ainterpretability%2520throughout%2520the%2520entire%2520process%252C%2520making%2520it%2520easier%2520for%2520users%2520to%250Aunderstand%2520and%2520trust%2520the%2520decisions%2520of%2520the%2520model.%2520We%2520conduct%2520several%2520studies%2520to%250Aempirically%2520evaluate%2520the%2520effectiveness%2520of%2520VGRL%2520and%2520we%2520believe%2520these%2520method%2520can%250Aserve%2520as%2520a%2520stepping%2520stone%2520in%2520graph%2520representation%2520learning.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2410.01457v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Verbalized%20Graph%20Representation%20Learning%3A%20A%20Fully%20Interpretable%20Graph%0A%20%20Model%20Based%20on%20Large%20Language%20Models%20Throughout%20the%20Entire%20Process&entry.906535625=Xingyu%20Ji%20and%20Jiale%20Liu%20and%20Lu%20Li%20and%20Maojun%20Wang%20and%20Zeyu%20Zhang&entry.1292438233=%20%20Representation%20learning%20on%20text-attributed%20graphs%20%28TAGs%29%20has%20attracted%0Asignificant%20interest%20due%20to%20its%20wide-ranging%20real-world%20applications%2C%0Aparticularly%20through%20Graph%20Neural%20Networks%20%28GNNs%29.%20Traditional%20GNN%20methods%0Afocus%20on%20encoding%20the%20structural%20information%20of%20graphs%2C%20often%20using%20shallow%0Atext%20embeddings%20for%20node%20or%20edge%20attributes.%20This%20limits%20the%20model%20to%0Aunderstand%20the%20rich%20semantic%20information%20in%20the%20data%20and%20its%20reasoning%20ability%0Afor%20complex%20downstream%20tasks%2C%20while%20also%20lacking%20interpretability.%20With%20the%0Arise%20of%20large%20language%20models%20%28LLMs%29%2C%20an%20increasing%20number%20of%20studies%20are%0Acombining%20them%20with%20GNNs%20for%20graph%20representation%20learning%20and%20downstream%0Atasks.%20While%20these%20approaches%20effectively%20leverage%20the%20rich%20semantic%0Ainformation%20in%20TAGs%20datasets%2C%20their%20main%20drawback%20is%20that%20they%20are%20only%0Apartially%20interpretable%2C%20which%20limits%20their%20application%20in%20critical%20fields.%20In%0Athis%20paper%2C%20we%20propose%20a%20verbalized%20graph%20representation%20learning%20%28VGRL%29%20method%0Awhich%20is%20fully%20interpretable.%20In%20contrast%20to%20traditional%20graph%20machine%20learning%0Amodels%2C%20which%20are%20usually%20optimized%20within%20a%20continuous%20parameter%20space%2C%20VGRL%0Aconstrains%20this%20parameter%20space%20to%20be%20text%20description%20which%20ensures%20complete%0Ainterpretability%20throughout%20the%20entire%20process%2C%20making%20it%20easier%20for%20users%20to%0Aunderstand%20and%20trust%20the%20decisions%20of%20the%20model.%20We%20conduct%20several%20studies%20to%0Aempirically%20evaluate%20the%20effectiveness%20of%20VGRL%20and%20we%20believe%20these%20method%20can%0Aserve%20as%20a%20stepping%20stone%20in%20graph%20representation%20learning.%0A&entry.1838667208=http%3A//arxiv.org/abs/2410.01457v1&entry.124074799=Read"},
{"title": "Mind Scramble: Unveiling Large Language Model Psychology Via\n  Typoglycemia", "author": "Miao Yu and Junyuan Mao and Guibin Zhang and Jingheng Ye and Junfeng Fang and Aoxiao Zhong and Yang Liu and Yuxuan Liang and Kun Wang and Qingsong Wen", "abstract": "  Research into the external behaviors and internal mechanisms of large\nlanguage models (LLMs) has shown promise in addressing complex tasks in the\nphysical world. Studies suggest that powerful LLMs, like GPT-4, are beginning\nto exhibit human-like cognitive abilities, including planning, reasoning, and\nreflection. In this paper, we introduce a research line and methodology called\nLLM Psychology, leveraging human psychology experiments to investigate the\ncognitive behaviors and mechanisms of LLMs. We migrate the Typoglycemia\nphenomenon from psychology to explore the \"mind\" of LLMs. Unlike human brains,\nwhich rely on context and word patterns to comprehend scrambled text, LLMs use\ndistinct encoding and decoding processes. Through Typoglycemia experiments at\nthe character, word, and sentence levels, we observe: (I) LLMs demonstrate\nhuman-like behaviors on a macro scale, such as lower task accuracy and higher\ntoken/time consumption; (II) LLMs exhibit varying robustness to scrambled\ninput, making Typoglycemia a benchmark for model evaluation without new\ndatasets; (III) Different task types have varying impacts, with complex logical\ntasks (e.g., math) being more challenging in scrambled form; (IV) Each LLM has\na unique and consistent \"cognitive pattern\" across tasks, revealing general\nmechanisms in its psychology process. We provide an in-depth analysis of hidden\nlayers to explain these phenomena, paving the way for future research in LLM\nPsychology and deeper interpretability.\n", "link": "http://arxiv.org/abs/2410.01677v1", "date": "2024-10-02", "relevancy": 2.5639, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5311}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5311}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.4761}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Mind%20Scramble%3A%20Unveiling%20Large%20Language%20Model%20Psychology%20Via%0A%20%20Typoglycemia&body=Title%3A%20Mind%20Scramble%3A%20Unveiling%20Large%20Language%20Model%20Psychology%20Via%0A%20%20Typoglycemia%0AAuthor%3A%20Miao%20Yu%20and%20Junyuan%20Mao%20and%20Guibin%20Zhang%20and%20Jingheng%20Ye%20and%20Junfeng%20Fang%20and%20Aoxiao%20Zhong%20and%20Yang%20Liu%20and%20Yuxuan%20Liang%20and%20Kun%20Wang%20and%20Qingsong%20Wen%0AAbstract%3A%20%20%20Research%20into%20the%20external%20behaviors%20and%20internal%20mechanisms%20of%20large%0Alanguage%20models%20%28LLMs%29%20has%20shown%20promise%20in%20addressing%20complex%20tasks%20in%20the%0Aphysical%20world.%20Studies%20suggest%20that%20powerful%20LLMs%2C%20like%20GPT-4%2C%20are%20beginning%0Ato%20exhibit%20human-like%20cognitive%20abilities%2C%20including%20planning%2C%20reasoning%2C%20and%0Areflection.%20In%20this%20paper%2C%20we%20introduce%20a%20research%20line%20and%20methodology%20called%0ALLM%20Psychology%2C%20leveraging%20human%20psychology%20experiments%20to%20investigate%20the%0Acognitive%20behaviors%20and%20mechanisms%20of%20LLMs.%20We%20migrate%20the%20Typoglycemia%0Aphenomenon%20from%20psychology%20to%20explore%20the%20%22mind%22%20of%20LLMs.%20Unlike%20human%20brains%2C%0Awhich%20rely%20on%20context%20and%20word%20patterns%20to%20comprehend%20scrambled%20text%2C%20LLMs%20use%0Adistinct%20encoding%20and%20decoding%20processes.%20Through%20Typoglycemia%20experiments%20at%0Athe%20character%2C%20word%2C%20and%20sentence%20levels%2C%20we%20observe%3A%20%28I%29%20LLMs%20demonstrate%0Ahuman-like%20behaviors%20on%20a%20macro%20scale%2C%20such%20as%20lower%20task%20accuracy%20and%20higher%0Atoken/time%20consumption%3B%20%28II%29%20LLMs%20exhibit%20varying%20robustness%20to%20scrambled%0Ainput%2C%20making%20Typoglycemia%20a%20benchmark%20for%20model%20evaluation%20without%20new%0Adatasets%3B%20%28III%29%20Different%20task%20types%20have%20varying%20impacts%2C%20with%20complex%20logical%0Atasks%20%28e.g.%2C%20math%29%20being%20more%20challenging%20in%20scrambled%20form%3B%20%28IV%29%20Each%20LLM%20has%0Aa%20unique%20and%20consistent%20%22cognitive%20pattern%22%20across%20tasks%2C%20revealing%20general%0Amechanisms%20in%20its%20psychology%20process.%20We%20provide%20an%20in-depth%20analysis%20of%20hidden%0Alayers%20to%20explain%20these%20phenomena%2C%20paving%20the%20way%20for%20future%20research%20in%20LLM%0APsychology%20and%20deeper%20interpretability.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2410.01677v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DMind%2520Scramble%253A%2520Unveiling%2520Large%2520Language%2520Model%2520Psychology%2520Via%250A%2520%2520Typoglycemia%26entry.906535625%3DMiao%2520Yu%2520and%2520Junyuan%2520Mao%2520and%2520Guibin%2520Zhang%2520and%2520Jingheng%2520Ye%2520and%2520Junfeng%2520Fang%2520and%2520Aoxiao%2520Zhong%2520and%2520Yang%2520Liu%2520and%2520Yuxuan%2520Liang%2520and%2520Kun%2520Wang%2520and%2520Qingsong%2520Wen%26entry.1292438233%3D%2520%2520Research%2520into%2520the%2520external%2520behaviors%2520and%2520internal%2520mechanisms%2520of%2520large%250Alanguage%2520models%2520%2528LLMs%2529%2520has%2520shown%2520promise%2520in%2520addressing%2520complex%2520tasks%2520in%2520the%250Aphysical%2520world.%2520Studies%2520suggest%2520that%2520powerful%2520LLMs%252C%2520like%2520GPT-4%252C%2520are%2520beginning%250Ato%2520exhibit%2520human-like%2520cognitive%2520abilities%252C%2520including%2520planning%252C%2520reasoning%252C%2520and%250Areflection.%2520In%2520this%2520paper%252C%2520we%2520introduce%2520a%2520research%2520line%2520and%2520methodology%2520called%250ALLM%2520Psychology%252C%2520leveraging%2520human%2520psychology%2520experiments%2520to%2520investigate%2520the%250Acognitive%2520behaviors%2520and%2520mechanisms%2520of%2520LLMs.%2520We%2520migrate%2520the%2520Typoglycemia%250Aphenomenon%2520from%2520psychology%2520to%2520explore%2520the%2520%2522mind%2522%2520of%2520LLMs.%2520Unlike%2520human%2520brains%252C%250Awhich%2520rely%2520on%2520context%2520and%2520word%2520patterns%2520to%2520comprehend%2520scrambled%2520text%252C%2520LLMs%2520use%250Adistinct%2520encoding%2520and%2520decoding%2520processes.%2520Through%2520Typoglycemia%2520experiments%2520at%250Athe%2520character%252C%2520word%252C%2520and%2520sentence%2520levels%252C%2520we%2520observe%253A%2520%2528I%2529%2520LLMs%2520demonstrate%250Ahuman-like%2520behaviors%2520on%2520a%2520macro%2520scale%252C%2520such%2520as%2520lower%2520task%2520accuracy%2520and%2520higher%250Atoken/time%2520consumption%253B%2520%2528II%2529%2520LLMs%2520exhibit%2520varying%2520robustness%2520to%2520scrambled%250Ainput%252C%2520making%2520Typoglycemia%2520a%2520benchmark%2520for%2520model%2520evaluation%2520without%2520new%250Adatasets%253B%2520%2528III%2529%2520Different%2520task%2520types%2520have%2520varying%2520impacts%252C%2520with%2520complex%2520logical%250Atasks%2520%2528e.g.%252C%2520math%2529%2520being%2520more%2520challenging%2520in%2520scrambled%2520form%253B%2520%2528IV%2529%2520Each%2520LLM%2520has%250Aa%2520unique%2520and%2520consistent%2520%2522cognitive%2520pattern%2522%2520across%2520tasks%252C%2520revealing%2520general%250Amechanisms%2520in%2520its%2520psychology%2520process.%2520We%2520provide%2520an%2520in-depth%2520analysis%2520of%2520hidden%250Alayers%2520to%2520explain%2520these%2520phenomena%252C%2520paving%2520the%2520way%2520for%2520future%2520research%2520in%2520LLM%250APsychology%2520and%2520deeper%2520interpretability.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2410.01677v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Mind%20Scramble%3A%20Unveiling%20Large%20Language%20Model%20Psychology%20Via%0A%20%20Typoglycemia&entry.906535625=Miao%20Yu%20and%20Junyuan%20Mao%20and%20Guibin%20Zhang%20and%20Jingheng%20Ye%20and%20Junfeng%20Fang%20and%20Aoxiao%20Zhong%20and%20Yang%20Liu%20and%20Yuxuan%20Liang%20and%20Kun%20Wang%20and%20Qingsong%20Wen&entry.1292438233=%20%20Research%20into%20the%20external%20behaviors%20and%20internal%20mechanisms%20of%20large%0Alanguage%20models%20%28LLMs%29%20has%20shown%20promise%20in%20addressing%20complex%20tasks%20in%20the%0Aphysical%20world.%20Studies%20suggest%20that%20powerful%20LLMs%2C%20like%20GPT-4%2C%20are%20beginning%0Ato%20exhibit%20human-like%20cognitive%20abilities%2C%20including%20planning%2C%20reasoning%2C%20and%0Areflection.%20In%20this%20paper%2C%20we%20introduce%20a%20research%20line%20and%20methodology%20called%0ALLM%20Psychology%2C%20leveraging%20human%20psychology%20experiments%20to%20investigate%20the%0Acognitive%20behaviors%20and%20mechanisms%20of%20LLMs.%20We%20migrate%20the%20Typoglycemia%0Aphenomenon%20from%20psychology%20to%20explore%20the%20%22mind%22%20of%20LLMs.%20Unlike%20human%20brains%2C%0Awhich%20rely%20on%20context%20and%20word%20patterns%20to%20comprehend%20scrambled%20text%2C%20LLMs%20use%0Adistinct%20encoding%20and%20decoding%20processes.%20Through%20Typoglycemia%20experiments%20at%0Athe%20character%2C%20word%2C%20and%20sentence%20levels%2C%20we%20observe%3A%20%28I%29%20LLMs%20demonstrate%0Ahuman-like%20behaviors%20on%20a%20macro%20scale%2C%20such%20as%20lower%20task%20accuracy%20and%20higher%0Atoken/time%20consumption%3B%20%28II%29%20LLMs%20exhibit%20varying%20robustness%20to%20scrambled%0Ainput%2C%20making%20Typoglycemia%20a%20benchmark%20for%20model%20evaluation%20without%20new%0Adatasets%3B%20%28III%29%20Different%20task%20types%20have%20varying%20impacts%2C%20with%20complex%20logical%0Atasks%20%28e.g.%2C%20math%29%20being%20more%20challenging%20in%20scrambled%20form%3B%20%28IV%29%20Each%20LLM%20has%0Aa%20unique%20and%20consistent%20%22cognitive%20pattern%22%20across%20tasks%2C%20revealing%20general%0Amechanisms%20in%20its%20psychology%20process.%20We%20provide%20an%20in-depth%20analysis%20of%20hidden%0Alayers%20to%20explain%20these%20phenomena%2C%20paving%20the%20way%20for%20future%20research%20in%20LLM%0APsychology%20and%20deeper%20interpretability.%0A&entry.1838667208=http%3A//arxiv.org/abs/2410.01677v1&entry.124074799=Read"},
{"title": "Information-Theoretical Principled Trade-off between Jailbreakability\n  and Stealthiness on Vision Language Models", "author": "Ching-Chia Kao and Chia-Mu Yu and Chun-Shien Lu and Chu-Song Chen", "abstract": "  In recent years, Vision-Language Models (VLMs) have demonstrated significant\nadvancements in artificial intelligence, transforming tasks across various\ndomains. Despite their capabilities, these models are susceptible to jailbreak\nattacks, which can compromise their safety and reliability. This paper explores\nthe trade-off between jailbreakability and stealthiness in VLMs, presenting a\nnovel algorithm to detect non-stealthy jailbreak attacks and enhance model\nrobustness. We introduce a stealthiness-aware jailbreak attack using diffusion\nmodels, highlighting the challenge of detecting AI-generated content. Our\napproach leverages Fano's inequality to elucidate the relationship between\nattack success rates and stealthiness scores, providing an explainable\nframework for evaluating these threats. Our contributions aim to fortify AI\nsystems against sophisticated attacks, ensuring their outputs remain aligned\nwith ethical standards and user expectations.\n", "link": "http://arxiv.org/abs/2410.01438v1", "date": "2024-10-02", "relevancy": 2.5593, "topK": [{"title": "Flexible Visual Recognition by Evidential Modeling of Confusion and Ignorance", "link": "http://openaccess.thecvf.com/content/ICCV2023/html/Fan_Flexible_Visual_Recognition_by_Evidential_Modeling_of_Confusion_and_Ignorance_ICCV_2023_paper.html", "similarity": 0.5171}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5092}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5092}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Information-Theoretical%20Principled%20Trade-off%20between%20Jailbreakability%0A%20%20and%20Stealthiness%20on%20Vision%20Language%20Models&body=Title%3A%20Information-Theoretical%20Principled%20Trade-off%20between%20Jailbreakability%0A%20%20and%20Stealthiness%20on%20Vision%20Language%20Models%0AAuthor%3A%20Ching-Chia%20Kao%20and%20Chia-Mu%20Yu%20and%20Chun-Shien%20Lu%20and%20Chu-Song%20Chen%0AAbstract%3A%20%20%20In%20recent%20years%2C%20Vision-Language%20Models%20%28VLMs%29%20have%20demonstrated%20significant%0Aadvancements%20in%20artificial%20intelligence%2C%20transforming%20tasks%20across%20various%0Adomains.%20Despite%20their%20capabilities%2C%20these%20models%20are%20susceptible%20to%20jailbreak%0Aattacks%2C%20which%20can%20compromise%20their%20safety%20and%20reliability.%20This%20paper%20explores%0Athe%20trade-off%20between%20jailbreakability%20and%20stealthiness%20in%20VLMs%2C%20presenting%20a%0Anovel%20algorithm%20to%20detect%20non-stealthy%20jailbreak%20attacks%20and%20enhance%20model%0Arobustness.%20We%20introduce%20a%20stealthiness-aware%20jailbreak%20attack%20using%20diffusion%0Amodels%2C%20highlighting%20the%20challenge%20of%20detecting%20AI-generated%20content.%20Our%0Aapproach%20leverages%20Fano%27s%20inequality%20to%20elucidate%20the%20relationship%20between%0Aattack%20success%20rates%20and%20stealthiness%20scores%2C%20providing%20an%20explainable%0Aframework%20for%20evaluating%20these%20threats.%20Our%20contributions%20aim%20to%20fortify%20AI%0Asystems%20against%20sophisticated%20attacks%2C%20ensuring%20their%20outputs%20remain%20aligned%0Awith%20ethical%20standards%20and%20user%20expectations.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2410.01438v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DInformation-Theoretical%2520Principled%2520Trade-off%2520between%2520Jailbreakability%250A%2520%2520and%2520Stealthiness%2520on%2520Vision%2520Language%2520Models%26entry.906535625%3DChing-Chia%2520Kao%2520and%2520Chia-Mu%2520Yu%2520and%2520Chun-Shien%2520Lu%2520and%2520Chu-Song%2520Chen%26entry.1292438233%3D%2520%2520In%2520recent%2520years%252C%2520Vision-Language%2520Models%2520%2528VLMs%2529%2520have%2520demonstrated%2520significant%250Aadvancements%2520in%2520artificial%2520intelligence%252C%2520transforming%2520tasks%2520across%2520various%250Adomains.%2520Despite%2520their%2520capabilities%252C%2520these%2520models%2520are%2520susceptible%2520to%2520jailbreak%250Aattacks%252C%2520which%2520can%2520compromise%2520their%2520safety%2520and%2520reliability.%2520This%2520paper%2520explores%250Athe%2520trade-off%2520between%2520jailbreakability%2520and%2520stealthiness%2520in%2520VLMs%252C%2520presenting%2520a%250Anovel%2520algorithm%2520to%2520detect%2520non-stealthy%2520jailbreak%2520attacks%2520and%2520enhance%2520model%250Arobustness.%2520We%2520introduce%2520a%2520stealthiness-aware%2520jailbreak%2520attack%2520using%2520diffusion%250Amodels%252C%2520highlighting%2520the%2520challenge%2520of%2520detecting%2520AI-generated%2520content.%2520Our%250Aapproach%2520leverages%2520Fano%2527s%2520inequality%2520to%2520elucidate%2520the%2520relationship%2520between%250Aattack%2520success%2520rates%2520and%2520stealthiness%2520scores%252C%2520providing%2520an%2520explainable%250Aframework%2520for%2520evaluating%2520these%2520threats.%2520Our%2520contributions%2520aim%2520to%2520fortify%2520AI%250Asystems%2520against%2520sophisticated%2520attacks%252C%2520ensuring%2520their%2520outputs%2520remain%2520aligned%250Awith%2520ethical%2520standards%2520and%2520user%2520expectations.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2410.01438v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Information-Theoretical%20Principled%20Trade-off%20between%20Jailbreakability%0A%20%20and%20Stealthiness%20on%20Vision%20Language%20Models&entry.906535625=Ching-Chia%20Kao%20and%20Chia-Mu%20Yu%20and%20Chun-Shien%20Lu%20and%20Chu-Song%20Chen&entry.1292438233=%20%20In%20recent%20years%2C%20Vision-Language%20Models%20%28VLMs%29%20have%20demonstrated%20significant%0Aadvancements%20in%20artificial%20intelligence%2C%20transforming%20tasks%20across%20various%0Adomains.%20Despite%20their%20capabilities%2C%20these%20models%20are%20susceptible%20to%20jailbreak%0Aattacks%2C%20which%20can%20compromise%20their%20safety%20and%20reliability.%20This%20paper%20explores%0Athe%20trade-off%20between%20jailbreakability%20and%20stealthiness%20in%20VLMs%2C%20presenting%20a%0Anovel%20algorithm%20to%20detect%20non-stealthy%20jailbreak%20attacks%20and%20enhance%20model%0Arobustness.%20We%20introduce%20a%20stealthiness-aware%20jailbreak%20attack%20using%20diffusion%0Amodels%2C%20highlighting%20the%20challenge%20of%20detecting%20AI-generated%20content.%20Our%0Aapproach%20leverages%20Fano%27s%20inequality%20to%20elucidate%20the%20relationship%20between%0Aattack%20success%20rates%20and%20stealthiness%20scores%2C%20providing%20an%20explainable%0Aframework%20for%20evaluating%20these%20threats.%20Our%20contributions%20aim%20to%20fortify%20AI%0Asystems%20against%20sophisticated%20attacks%2C%20ensuring%20their%20outputs%20remain%20aligned%0Awith%20ethical%20standards%20and%20user%20expectations.%0A&entry.1838667208=http%3A//arxiv.org/abs/2410.01438v1&entry.124074799=Read"},
{"title": "PreND: Enhancing Intrinsic Motivation in Reinforcement Learning through\n  Pre-trained Network Distillation", "author": "Mohammadamin Davoodabadi and Negin Hashemi Dijujin and Mahdieh Soleymani Baghshah", "abstract": "  Intrinsic motivation, inspired by the psychology of developmental learning in\ninfants, stimulates exploration in agents without relying solely on sparse\nexternal rewards. Existing methods in reinforcement learning like Random\nNetwork Distillation (RND) face significant limitations, including (1) relying\non raw visual inputs, leading to a lack of meaningful representations, (2) the\ninability to build a robust latent space, (3) poor target network\ninitialization and (4) rapid degradation of intrinsic rewards. In this paper,\nwe introduce Pre-trained Network Distillation (PreND), a novel approach to\nenhance intrinsic motivation in reinforcement learning (RL) by improving upon\nthe widely used prediction-based method, RND. PreND addresses these challenges\nby incorporating pre-trained representation models into both the target and\npredictor networks, resulting in more meaningful and stable intrinsic rewards,\nwhile enhancing the representation learned by the model. We also tried simple\nbut effective variants of the predictor network optimization by controlling the\nlearning rate. Through experiments on the Atari domain, we demonstrate that\nPreND significantly outperforms RND, offering a more robust intrinsic\nmotivation signal that leads to better exploration, improving overall\nperformance and sample efficiency. This research highlights the importance of\ntarget and predictor networks representation in prediction-based intrinsic\nmotivation, setting a new direction for improving RL agents' learning\nefficiency in sparse reward environments.\n", "link": "http://arxiv.org/abs/2410.01745v1", "date": "2024-10-02", "relevancy": 2.5421, "topK": [{"title": "Boosted dynamic neural networks", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/26302", "similarity": 0.5252}, {"title": "Safe and efficient training of a control agent", "link": "https://patents.google.com/patent/US11709462B2/en", "similarity": 0.5}, {"title": "ToDo: Token Downsampling for Efficient Generation of High-Resolution\n  Images", "link": "http://arxiv.org/abs/2402.13573v2", "similarity": 0.5}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20PreND%3A%20Enhancing%20Intrinsic%20Motivation%20in%20Reinforcement%20Learning%20through%0A%20%20Pre-trained%20Network%20Distillation&body=Title%3A%20PreND%3A%20Enhancing%20Intrinsic%20Motivation%20in%20Reinforcement%20Learning%20through%0A%20%20Pre-trained%20Network%20Distillation%0AAuthor%3A%20Mohammadamin%20Davoodabadi%20and%20Negin%20Hashemi%20Dijujin%20and%20Mahdieh%20Soleymani%20Baghshah%0AAbstract%3A%20%20%20Intrinsic%20motivation%2C%20inspired%20by%20the%20psychology%20of%20developmental%20learning%20in%0Ainfants%2C%20stimulates%20exploration%20in%20agents%20without%20relying%20solely%20on%20sparse%0Aexternal%20rewards.%20Existing%20methods%20in%20reinforcement%20learning%20like%20Random%0ANetwork%20Distillation%20%28RND%29%20face%20significant%20limitations%2C%20including%20%281%29%20relying%0Aon%20raw%20visual%20inputs%2C%20leading%20to%20a%20lack%20of%20meaningful%20representations%2C%20%282%29%20the%0Ainability%20to%20build%20a%20robust%20latent%20space%2C%20%283%29%20poor%20target%20network%0Ainitialization%20and%20%284%29%20rapid%20degradation%20of%20intrinsic%20rewards.%20In%20this%20paper%2C%0Awe%20introduce%20Pre-trained%20Network%20Distillation%20%28PreND%29%2C%20a%20novel%20approach%20to%0Aenhance%20intrinsic%20motivation%20in%20reinforcement%20learning%20%28RL%29%20by%20improving%20upon%0Athe%20widely%20used%20prediction-based%20method%2C%20RND.%20PreND%20addresses%20these%20challenges%0Aby%20incorporating%20pre-trained%20representation%20models%20into%20both%20the%20target%20and%0Apredictor%20networks%2C%20resulting%20in%20more%20meaningful%20and%20stable%20intrinsic%20rewards%2C%0Awhile%20enhancing%20the%20representation%20learned%20by%20the%20model.%20We%20also%20tried%20simple%0Abut%20effective%20variants%20of%20the%20predictor%20network%20optimization%20by%20controlling%20the%0Alearning%20rate.%20Through%20experiments%20on%20the%20Atari%20domain%2C%20we%20demonstrate%20that%0APreND%20significantly%20outperforms%20RND%2C%20offering%20a%20more%20robust%20intrinsic%0Amotivation%20signal%20that%20leads%20to%20better%20exploration%2C%20improving%20overall%0Aperformance%20and%20sample%20efficiency.%20This%20research%20highlights%20the%20importance%20of%0Atarget%20and%20predictor%20networks%20representation%20in%20prediction-based%20intrinsic%0Amotivation%2C%20setting%20a%20new%20direction%20for%20improving%20RL%20agents%27%20learning%0Aefficiency%20in%20sparse%20reward%20environments.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2410.01745v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DPreND%253A%2520Enhancing%2520Intrinsic%2520Motivation%2520in%2520Reinforcement%2520Learning%2520through%250A%2520%2520Pre-trained%2520Network%2520Distillation%26entry.906535625%3DMohammadamin%2520Davoodabadi%2520and%2520Negin%2520Hashemi%2520Dijujin%2520and%2520Mahdieh%2520Soleymani%2520Baghshah%26entry.1292438233%3D%2520%2520Intrinsic%2520motivation%252C%2520inspired%2520by%2520the%2520psychology%2520of%2520developmental%2520learning%2520in%250Ainfants%252C%2520stimulates%2520exploration%2520in%2520agents%2520without%2520relying%2520solely%2520on%2520sparse%250Aexternal%2520rewards.%2520Existing%2520methods%2520in%2520reinforcement%2520learning%2520like%2520Random%250ANetwork%2520Distillation%2520%2528RND%2529%2520face%2520significant%2520limitations%252C%2520including%2520%25281%2529%2520relying%250Aon%2520raw%2520visual%2520inputs%252C%2520leading%2520to%2520a%2520lack%2520of%2520meaningful%2520representations%252C%2520%25282%2529%2520the%250Ainability%2520to%2520build%2520a%2520robust%2520latent%2520space%252C%2520%25283%2529%2520poor%2520target%2520network%250Ainitialization%2520and%2520%25284%2529%2520rapid%2520degradation%2520of%2520intrinsic%2520rewards.%2520In%2520this%2520paper%252C%250Awe%2520introduce%2520Pre-trained%2520Network%2520Distillation%2520%2528PreND%2529%252C%2520a%2520novel%2520approach%2520to%250Aenhance%2520intrinsic%2520motivation%2520in%2520reinforcement%2520learning%2520%2528RL%2529%2520by%2520improving%2520upon%250Athe%2520widely%2520used%2520prediction-based%2520method%252C%2520RND.%2520PreND%2520addresses%2520these%2520challenges%250Aby%2520incorporating%2520pre-trained%2520representation%2520models%2520into%2520both%2520the%2520target%2520and%250Apredictor%2520networks%252C%2520resulting%2520in%2520more%2520meaningful%2520and%2520stable%2520intrinsic%2520rewards%252C%250Awhile%2520enhancing%2520the%2520representation%2520learned%2520by%2520the%2520model.%2520We%2520also%2520tried%2520simple%250Abut%2520effective%2520variants%2520of%2520the%2520predictor%2520network%2520optimization%2520by%2520controlling%2520the%250Alearning%2520rate.%2520Through%2520experiments%2520on%2520the%2520Atari%2520domain%252C%2520we%2520demonstrate%2520that%250APreND%2520significantly%2520outperforms%2520RND%252C%2520offering%2520a%2520more%2520robust%2520intrinsic%250Amotivation%2520signal%2520that%2520leads%2520to%2520better%2520exploration%252C%2520improving%2520overall%250Aperformance%2520and%2520sample%2520efficiency.%2520This%2520research%2520highlights%2520the%2520importance%2520of%250Atarget%2520and%2520predictor%2520networks%2520representation%2520in%2520prediction-based%2520intrinsic%250Amotivation%252C%2520setting%2520a%2520new%2520direction%2520for%2520improving%2520RL%2520agents%2527%2520learning%250Aefficiency%2520in%2520sparse%2520reward%2520environments.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2410.01745v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=PreND%3A%20Enhancing%20Intrinsic%20Motivation%20in%20Reinforcement%20Learning%20through%0A%20%20Pre-trained%20Network%20Distillation&entry.906535625=Mohammadamin%20Davoodabadi%20and%20Negin%20Hashemi%20Dijujin%20and%20Mahdieh%20Soleymani%20Baghshah&entry.1292438233=%20%20Intrinsic%20motivation%2C%20inspired%20by%20the%20psychology%20of%20developmental%20learning%20in%0Ainfants%2C%20stimulates%20exploration%20in%20agents%20without%20relying%20solely%20on%20sparse%0Aexternal%20rewards.%20Existing%20methods%20in%20reinforcement%20learning%20like%20Random%0ANetwork%20Distillation%20%28RND%29%20face%20significant%20limitations%2C%20including%20%281%29%20relying%0Aon%20raw%20visual%20inputs%2C%20leading%20to%20a%20lack%20of%20meaningful%20representations%2C%20%282%29%20the%0Ainability%20to%20build%20a%20robust%20latent%20space%2C%20%283%29%20poor%20target%20network%0Ainitialization%20and%20%284%29%20rapid%20degradation%20of%20intrinsic%20rewards.%20In%20this%20paper%2C%0Awe%20introduce%20Pre-trained%20Network%20Distillation%20%28PreND%29%2C%20a%20novel%20approach%20to%0Aenhance%20intrinsic%20motivation%20in%20reinforcement%20learning%20%28RL%29%20by%20improving%20upon%0Athe%20widely%20used%20prediction-based%20method%2C%20RND.%20PreND%20addresses%20these%20challenges%0Aby%20incorporating%20pre-trained%20representation%20models%20into%20both%20the%20target%20and%0Apredictor%20networks%2C%20resulting%20in%20more%20meaningful%20and%20stable%20intrinsic%20rewards%2C%0Awhile%20enhancing%20the%20representation%20learned%20by%20the%20model.%20We%20also%20tried%20simple%0Abut%20effective%20variants%20of%20the%20predictor%20network%20optimization%20by%20controlling%20the%0Alearning%20rate.%20Through%20experiments%20on%20the%20Atari%20domain%2C%20we%20demonstrate%20that%0APreND%20significantly%20outperforms%20RND%2C%20offering%20a%20more%20robust%20intrinsic%0Amotivation%20signal%20that%20leads%20to%20better%20exploration%2C%20improving%20overall%0Aperformance%20and%20sample%20efficiency.%20This%20research%20highlights%20the%20importance%20of%0Atarget%20and%20predictor%20networks%20representation%20in%20prediction-based%20intrinsic%0Amotivation%2C%20setting%20a%20new%20direction%20for%20improving%20RL%20agents%27%20learning%0Aefficiency%20in%20sparse%20reward%20environments.%0A&entry.1838667208=http%3A//arxiv.org/abs/2410.01745v1&entry.124074799=Read"},
{"title": "FabricDiffusion: High-Fidelity Texture Transfer for 3D Garments\n  Generation from In-The-Wild Clothing Images", "author": "Cheng Zhang and Yuanhao Wang and Francisco Vicente Carrasco and Chenglei Wu and Jinlong Yang and Thabo Beeler and Fernando De la Torre", "abstract": "  We introduce FabricDiffusion, a method for transferring fabric textures from\na single clothing image to 3D garments of arbitrary shapes. Existing approaches\ntypically synthesize textures on the garment surface through 2D-to-3D texture\nmapping or depth-aware inpainting via generative models. Unfortunately, these\nmethods often struggle to capture and preserve texture details, particularly\ndue to challenging occlusions, distortions, or poses in the input image.\nInspired by the observation that in the fashion industry, most garments are\nconstructed by stitching sewing patterns with flat, repeatable textures, we\ncast the task of clothing texture transfer as extracting distortion-free,\ntileable texture materials that are subsequently mapped onto the UV space of\nthe garment. Building upon this insight, we train a denoising diffusion model\nwith a large-scale synthetic dataset to rectify distortions in the input\ntexture image. This process yields a flat texture map that enables a tight\ncoupling with existing Physically-Based Rendering (PBR) material generation\npipelines, allowing for realistic relighting of the garment under various\nlighting conditions. We show that FabricDiffusion can transfer various features\nfrom a single clothing image including texture patterns, material properties,\nand detailed prints and logos. Extensive experiments demonstrate that our model\nsignificantly outperforms state-to-the-art methods on both synthetic data and\nreal-world, in-the-wild clothing images while generalizing to unseen textures\nand garment shapes.\n", "link": "http://arxiv.org/abs/2410.01801v1", "date": "2024-10-02", "relevancy": 2.5341, "topK": [{"title": "Multimodal-Conditioned Latent Diffusion Models for Fashion Image Editing", "link": "http://arxiv.org/abs/2403.14828v2", "similarity": 0.6975}, {"title": "DressCode: Autoregressively Sewing and Generating Garments from Text\n  Guidance", "link": "http://arxiv.org/abs/2401.16465v3", "similarity": 0.6722}, {"title": "CAT3D: Create Anything in 3D with Multi-View Diffusion Models", "link": "http://arxiv.org/abs/2405.10314v1", "similarity": 0.5693}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20FabricDiffusion%3A%20High-Fidelity%20Texture%20Transfer%20for%203D%20Garments%0A%20%20Generation%20from%20In-The-Wild%20Clothing%20Images&body=Title%3A%20FabricDiffusion%3A%20High-Fidelity%20Texture%20Transfer%20for%203D%20Garments%0A%20%20Generation%20from%20In-The-Wild%20Clothing%20Images%0AAuthor%3A%20Cheng%20Zhang%20and%20Yuanhao%20Wang%20and%20Francisco%20Vicente%20Carrasco%20and%20Chenglei%20Wu%20and%20Jinlong%20Yang%20and%20Thabo%20Beeler%20and%20Fernando%20De%20la%20Torre%0AAbstract%3A%20%20%20We%20introduce%20FabricDiffusion%2C%20a%20method%20for%20transferring%20fabric%20textures%20from%0Aa%20single%20clothing%20image%20to%203D%20garments%20of%20arbitrary%20shapes.%20Existing%20approaches%0Atypically%20synthesize%20textures%20on%20the%20garment%20surface%20through%202D-to-3D%20texture%0Amapping%20or%20depth-aware%20inpainting%20via%20generative%20models.%20Unfortunately%2C%20these%0Amethods%20often%20struggle%20to%20capture%20and%20preserve%20texture%20details%2C%20particularly%0Adue%20to%20challenging%20occlusions%2C%20distortions%2C%20or%20poses%20in%20the%20input%20image.%0AInspired%20by%20the%20observation%20that%20in%20the%20fashion%20industry%2C%20most%20garments%20are%0Aconstructed%20by%20stitching%20sewing%20patterns%20with%20flat%2C%20repeatable%20textures%2C%20we%0Acast%20the%20task%20of%20clothing%20texture%20transfer%20as%20extracting%20distortion-free%2C%0Atileable%20texture%20materials%20that%20are%20subsequently%20mapped%20onto%20the%20UV%20space%20of%0Athe%20garment.%20Building%20upon%20this%20insight%2C%20we%20train%20a%20denoising%20diffusion%20model%0Awith%20a%20large-scale%20synthetic%20dataset%20to%20rectify%20distortions%20in%20the%20input%0Atexture%20image.%20This%20process%20yields%20a%20flat%20texture%20map%20that%20enables%20a%20tight%0Acoupling%20with%20existing%20Physically-Based%20Rendering%20%28PBR%29%20material%20generation%0Apipelines%2C%20allowing%20for%20realistic%20relighting%20of%20the%20garment%20under%20various%0Alighting%20conditions.%20We%20show%20that%20FabricDiffusion%20can%20transfer%20various%20features%0Afrom%20a%20single%20clothing%20image%20including%20texture%20patterns%2C%20material%20properties%2C%0Aand%20detailed%20prints%20and%20logos.%20Extensive%20experiments%20demonstrate%20that%20our%20model%0Asignificantly%20outperforms%20state-to-the-art%20methods%20on%20both%20synthetic%20data%20and%0Areal-world%2C%20in-the-wild%20clothing%20images%20while%20generalizing%20to%20unseen%20textures%0Aand%20garment%20shapes.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2410.01801v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DFabricDiffusion%253A%2520High-Fidelity%2520Texture%2520Transfer%2520for%25203D%2520Garments%250A%2520%2520Generation%2520from%2520In-The-Wild%2520Clothing%2520Images%26entry.906535625%3DCheng%2520Zhang%2520and%2520Yuanhao%2520Wang%2520and%2520Francisco%2520Vicente%2520Carrasco%2520and%2520Chenglei%2520Wu%2520and%2520Jinlong%2520Yang%2520and%2520Thabo%2520Beeler%2520and%2520Fernando%2520De%2520la%2520Torre%26entry.1292438233%3D%2520%2520We%2520introduce%2520FabricDiffusion%252C%2520a%2520method%2520for%2520transferring%2520fabric%2520textures%2520from%250Aa%2520single%2520clothing%2520image%2520to%25203D%2520garments%2520of%2520arbitrary%2520shapes.%2520Existing%2520approaches%250Atypically%2520synthesize%2520textures%2520on%2520the%2520garment%2520surface%2520through%25202D-to-3D%2520texture%250Amapping%2520or%2520depth-aware%2520inpainting%2520via%2520generative%2520models.%2520Unfortunately%252C%2520these%250Amethods%2520often%2520struggle%2520to%2520capture%2520and%2520preserve%2520texture%2520details%252C%2520particularly%250Adue%2520to%2520challenging%2520occlusions%252C%2520distortions%252C%2520or%2520poses%2520in%2520the%2520input%2520image.%250AInspired%2520by%2520the%2520observation%2520that%2520in%2520the%2520fashion%2520industry%252C%2520most%2520garments%2520are%250Aconstructed%2520by%2520stitching%2520sewing%2520patterns%2520with%2520flat%252C%2520repeatable%2520textures%252C%2520we%250Acast%2520the%2520task%2520of%2520clothing%2520texture%2520transfer%2520as%2520extracting%2520distortion-free%252C%250Atileable%2520texture%2520materials%2520that%2520are%2520subsequently%2520mapped%2520onto%2520the%2520UV%2520space%2520of%250Athe%2520garment.%2520Building%2520upon%2520this%2520insight%252C%2520we%2520train%2520a%2520denoising%2520diffusion%2520model%250Awith%2520a%2520large-scale%2520synthetic%2520dataset%2520to%2520rectify%2520distortions%2520in%2520the%2520input%250Atexture%2520image.%2520This%2520process%2520yields%2520a%2520flat%2520texture%2520map%2520that%2520enables%2520a%2520tight%250Acoupling%2520with%2520existing%2520Physically-Based%2520Rendering%2520%2528PBR%2529%2520material%2520generation%250Apipelines%252C%2520allowing%2520for%2520realistic%2520relighting%2520of%2520the%2520garment%2520under%2520various%250Alighting%2520conditions.%2520We%2520show%2520that%2520FabricDiffusion%2520can%2520transfer%2520various%2520features%250Afrom%2520a%2520single%2520clothing%2520image%2520including%2520texture%2520patterns%252C%2520material%2520properties%252C%250Aand%2520detailed%2520prints%2520and%2520logos.%2520Extensive%2520experiments%2520demonstrate%2520that%2520our%2520model%250Asignificantly%2520outperforms%2520state-to-the-art%2520methods%2520on%2520both%2520synthetic%2520data%2520and%250Areal-world%252C%2520in-the-wild%2520clothing%2520images%2520while%2520generalizing%2520to%2520unseen%2520textures%250Aand%2520garment%2520shapes.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2410.01801v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=FabricDiffusion%3A%20High-Fidelity%20Texture%20Transfer%20for%203D%20Garments%0A%20%20Generation%20from%20In-The-Wild%20Clothing%20Images&entry.906535625=Cheng%20Zhang%20and%20Yuanhao%20Wang%20and%20Francisco%20Vicente%20Carrasco%20and%20Chenglei%20Wu%20and%20Jinlong%20Yang%20and%20Thabo%20Beeler%20and%20Fernando%20De%20la%20Torre&entry.1292438233=%20%20We%20introduce%20FabricDiffusion%2C%20a%20method%20for%20transferring%20fabric%20textures%20from%0Aa%20single%20clothing%20image%20to%203D%20garments%20of%20arbitrary%20shapes.%20Existing%20approaches%0Atypically%20synthesize%20textures%20on%20the%20garment%20surface%20through%202D-to-3D%20texture%0Amapping%20or%20depth-aware%20inpainting%20via%20generative%20models.%20Unfortunately%2C%20these%0Amethods%20often%20struggle%20to%20capture%20and%20preserve%20texture%20details%2C%20particularly%0Adue%20to%20challenging%20occlusions%2C%20distortions%2C%20or%20poses%20in%20the%20input%20image.%0AInspired%20by%20the%20observation%20that%20in%20the%20fashion%20industry%2C%20most%20garments%20are%0Aconstructed%20by%20stitching%20sewing%20patterns%20with%20flat%2C%20repeatable%20textures%2C%20we%0Acast%20the%20task%20of%20clothing%20texture%20transfer%20as%20extracting%20distortion-free%2C%0Atileable%20texture%20materials%20that%20are%20subsequently%20mapped%20onto%20the%20UV%20space%20of%0Athe%20garment.%20Building%20upon%20this%20insight%2C%20we%20train%20a%20denoising%20diffusion%20model%0Awith%20a%20large-scale%20synthetic%20dataset%20to%20rectify%20distortions%20in%20the%20input%0Atexture%20image.%20This%20process%20yields%20a%20flat%20texture%20map%20that%20enables%20a%20tight%0Acoupling%20with%20existing%20Physically-Based%20Rendering%20%28PBR%29%20material%20generation%0Apipelines%2C%20allowing%20for%20realistic%20relighting%20of%20the%20garment%20under%20various%0Alighting%20conditions.%20We%20show%20that%20FabricDiffusion%20can%20transfer%20various%20features%0Afrom%20a%20single%20clothing%20image%20including%20texture%20patterns%2C%20material%20properties%2C%0Aand%20detailed%20prints%20and%20logos.%20Extensive%20experiments%20demonstrate%20that%20our%20model%0Asignificantly%20outperforms%20state-to-the-art%20methods%20on%20both%20synthetic%20data%20and%0Areal-world%2C%20in-the-wild%20clothing%20images%20while%20generalizing%20to%20unseen%20textures%0Aand%20garment%20shapes.%0A&entry.1838667208=http%3A//arxiv.org/abs/2410.01801v1&entry.124074799=Read"},
{"title": "Recursive Abstractive Processing for Retrieval in Dynamic Datasets", "author": "Charbel Chucri and Rami Azouz and Joachim Ott", "abstract": "  Recent retrieval-augmented models enhance basic methods by building a\nhierarchical structure over retrieved text chunks through recursive embedding,\nclustering, and summarization. The most relevant information is then retrieved\nfrom both the original text and generated summaries. However, such approaches\nface limitations with dynamic datasets, where adding or removing documents over\ntime complicates the updating of hierarchical representations formed through\nclustering. We propose a new algorithm to efficiently maintain the\nrecursive-abstractive tree structure in dynamic datasets, without compromising\nperformance. Additionally, we introduce a novel post-retrieval method that\napplies query-focused recursive abstractive processing to substantially improve\ncontext quality. Our method overcomes the limitations of other approaches by\nfunctioning as a black-box post-retrieval layer compatible with any retrieval\nalgorithm. Both algorithms are validated through extensive experiments on\nreal-world datasets, demonstrating their effectiveness in handling dynamic data\nand improving retrieval performance.\n", "link": "http://arxiv.org/abs/2410.01736v1", "date": "2024-10-02", "relevancy": 2.5137, "topK": [{"title": "ToDo: Token Downsampling for Efficient Generation of High-Resolution\n  Images", "link": "http://arxiv.org/abs/2402.13573v2", "similarity": 0.5032}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5025}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5025}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Recursive%20Abstractive%20Processing%20for%20Retrieval%20in%20Dynamic%20Datasets&body=Title%3A%20Recursive%20Abstractive%20Processing%20for%20Retrieval%20in%20Dynamic%20Datasets%0AAuthor%3A%20Charbel%20Chucri%20and%20Rami%20Azouz%20and%20Joachim%20Ott%0AAbstract%3A%20%20%20Recent%20retrieval-augmented%20models%20enhance%20basic%20methods%20by%20building%20a%0Ahierarchical%20structure%20over%20retrieved%20text%20chunks%20through%20recursive%20embedding%2C%0Aclustering%2C%20and%20summarization.%20The%20most%20relevant%20information%20is%20then%20retrieved%0Afrom%20both%20the%20original%20text%20and%20generated%20summaries.%20However%2C%20such%20approaches%0Aface%20limitations%20with%20dynamic%20datasets%2C%20where%20adding%20or%20removing%20documents%20over%0Atime%20complicates%20the%20updating%20of%20hierarchical%20representations%20formed%20through%0Aclustering.%20We%20propose%20a%20new%20algorithm%20to%20efficiently%20maintain%20the%0Arecursive-abstractive%20tree%20structure%20in%20dynamic%20datasets%2C%20without%20compromising%0Aperformance.%20Additionally%2C%20we%20introduce%20a%20novel%20post-retrieval%20method%20that%0Aapplies%20query-focused%20recursive%20abstractive%20processing%20to%20substantially%20improve%0Acontext%20quality.%20Our%20method%20overcomes%20the%20limitations%20of%20other%20approaches%20by%0Afunctioning%20as%20a%20black-box%20post-retrieval%20layer%20compatible%20with%20any%20retrieval%0Aalgorithm.%20Both%20algorithms%20are%20validated%20through%20extensive%20experiments%20on%0Areal-world%20datasets%2C%20demonstrating%20their%20effectiveness%20in%20handling%20dynamic%20data%0Aand%20improving%20retrieval%20performance.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2410.01736v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DRecursive%2520Abstractive%2520Processing%2520for%2520Retrieval%2520in%2520Dynamic%2520Datasets%26entry.906535625%3DCharbel%2520Chucri%2520and%2520Rami%2520Azouz%2520and%2520Joachim%2520Ott%26entry.1292438233%3D%2520%2520Recent%2520retrieval-augmented%2520models%2520enhance%2520basic%2520methods%2520by%2520building%2520a%250Ahierarchical%2520structure%2520over%2520retrieved%2520text%2520chunks%2520through%2520recursive%2520embedding%252C%250Aclustering%252C%2520and%2520summarization.%2520The%2520most%2520relevant%2520information%2520is%2520then%2520retrieved%250Afrom%2520both%2520the%2520original%2520text%2520and%2520generated%2520summaries.%2520However%252C%2520such%2520approaches%250Aface%2520limitations%2520with%2520dynamic%2520datasets%252C%2520where%2520adding%2520or%2520removing%2520documents%2520over%250Atime%2520complicates%2520the%2520updating%2520of%2520hierarchical%2520representations%2520formed%2520through%250Aclustering.%2520We%2520propose%2520a%2520new%2520algorithm%2520to%2520efficiently%2520maintain%2520the%250Arecursive-abstractive%2520tree%2520structure%2520in%2520dynamic%2520datasets%252C%2520without%2520compromising%250Aperformance.%2520Additionally%252C%2520we%2520introduce%2520a%2520novel%2520post-retrieval%2520method%2520that%250Aapplies%2520query-focused%2520recursive%2520abstractive%2520processing%2520to%2520substantially%2520improve%250Acontext%2520quality.%2520Our%2520method%2520overcomes%2520the%2520limitations%2520of%2520other%2520approaches%2520by%250Afunctioning%2520as%2520a%2520black-box%2520post-retrieval%2520layer%2520compatible%2520with%2520any%2520retrieval%250Aalgorithm.%2520Both%2520algorithms%2520are%2520validated%2520through%2520extensive%2520experiments%2520on%250Areal-world%2520datasets%252C%2520demonstrating%2520their%2520effectiveness%2520in%2520handling%2520dynamic%2520data%250Aand%2520improving%2520retrieval%2520performance.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2410.01736v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Recursive%20Abstractive%20Processing%20for%20Retrieval%20in%20Dynamic%20Datasets&entry.906535625=Charbel%20Chucri%20and%20Rami%20Azouz%20and%20Joachim%20Ott&entry.1292438233=%20%20Recent%20retrieval-augmented%20models%20enhance%20basic%20methods%20by%20building%20a%0Ahierarchical%20structure%20over%20retrieved%20text%20chunks%20through%20recursive%20embedding%2C%0Aclustering%2C%20and%20summarization.%20The%20most%20relevant%20information%20is%20then%20retrieved%0Afrom%20both%20the%20original%20text%20and%20generated%20summaries.%20However%2C%20such%20approaches%0Aface%20limitations%20with%20dynamic%20datasets%2C%20where%20adding%20or%20removing%20documents%20over%0Atime%20complicates%20the%20updating%20of%20hierarchical%20representations%20formed%20through%0Aclustering.%20We%20propose%20a%20new%20algorithm%20to%20efficiently%20maintain%20the%0Arecursive-abstractive%20tree%20structure%20in%20dynamic%20datasets%2C%20without%20compromising%0Aperformance.%20Additionally%2C%20we%20introduce%20a%20novel%20post-retrieval%20method%20that%0Aapplies%20query-focused%20recursive%20abstractive%20processing%20to%20substantially%20improve%0Acontext%20quality.%20Our%20method%20overcomes%20the%20limitations%20of%20other%20approaches%20by%0Afunctioning%20as%20a%20black-box%20post-retrieval%20layer%20compatible%20with%20any%20retrieval%0Aalgorithm.%20Both%20algorithms%20are%20validated%20through%20extensive%20experiments%20on%0Areal-world%20datasets%2C%20demonstrating%20their%20effectiveness%20in%20handling%20dynamic%20data%0Aand%20improving%20retrieval%20performance.%0A&entry.1838667208=http%3A//arxiv.org/abs/2410.01736v1&entry.124074799=Read"},
{"title": "SAAS: Solving Ability Amplification Strategy for Enhanced Mathematical\n  Reasoning in Large Language Models", "author": "Hyeonwoo Kim and Gyoungjin Gim and Yungi Kim and Jihoo Kim and Byungju Kim and Wonseok Lee and Chanjun Park", "abstract": "  This study presents a novel learning approach designed to enhance both\nmathematical reasoning and problem-solving abilities of Large Language Models\n(LLMs). We focus on integrating the Chain-of-Thought (CoT) and the\nProgram-of-Thought (PoT) learning, hypothesizing that prioritizing the learning\nof mathematical reasoning ability is helpful for the amplification of\nproblem-solving ability. Thus, the initial learning with CoT is essential for\nsolving challenging mathematical problems. To this end, we propose a sequential\nlearning approach, named SAAS (Solving Ability Amplification Strategy), which\nstrategically transitions from CoT learning to PoT learning. Our empirical\nstudy, involving an extensive performance comparison using several benchmarks,\ndemonstrates that our SAAS achieves state-of-the-art (SOTA) performance. The\nresults underscore the effectiveness of our sequential learning approach,\nmarking a significant advancement in the field of mathematical reasoning in\nLLMs.\n", "link": "http://arxiv.org/abs/2404.03887v4", "date": "2024-10-02", "relevancy": 2.507, "topK": [{"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5075}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.4983}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.4983}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20SAAS%3A%20Solving%20Ability%20Amplification%20Strategy%20for%20Enhanced%20Mathematical%0A%20%20Reasoning%20in%20Large%20Language%20Models&body=Title%3A%20SAAS%3A%20Solving%20Ability%20Amplification%20Strategy%20for%20Enhanced%20Mathematical%0A%20%20Reasoning%20in%20Large%20Language%20Models%0AAuthor%3A%20Hyeonwoo%20Kim%20and%20Gyoungjin%20Gim%20and%20Yungi%20Kim%20and%20Jihoo%20Kim%20and%20Byungju%20Kim%20and%20Wonseok%20Lee%20and%20Chanjun%20Park%0AAbstract%3A%20%20%20This%20study%20presents%20a%20novel%20learning%20approach%20designed%20to%20enhance%20both%0Amathematical%20reasoning%20and%20problem-solving%20abilities%20of%20Large%20Language%20Models%0A%28LLMs%29.%20We%20focus%20on%20integrating%20the%20Chain-of-Thought%20%28CoT%29%20and%20the%0AProgram-of-Thought%20%28PoT%29%20learning%2C%20hypothesizing%20that%20prioritizing%20the%20learning%0Aof%20mathematical%20reasoning%20ability%20is%20helpful%20for%20the%20amplification%20of%0Aproblem-solving%20ability.%20Thus%2C%20the%20initial%20learning%20with%20CoT%20is%20essential%20for%0Asolving%20challenging%20mathematical%20problems.%20To%20this%20end%2C%20we%20propose%20a%20sequential%0Alearning%20approach%2C%20named%20SAAS%20%28Solving%20Ability%20Amplification%20Strategy%29%2C%20which%0Astrategically%20transitions%20from%20CoT%20learning%20to%20PoT%20learning.%20Our%20empirical%0Astudy%2C%20involving%20an%20extensive%20performance%20comparison%20using%20several%20benchmarks%2C%0Ademonstrates%20that%20our%20SAAS%20achieves%20state-of-the-art%20%28SOTA%29%20performance.%20The%0Aresults%20underscore%20the%20effectiveness%20of%20our%20sequential%20learning%20approach%2C%0Amarking%20a%20significant%20advancement%20in%20the%20field%20of%20mathematical%20reasoning%20in%0ALLMs.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2404.03887v4%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DSAAS%253A%2520Solving%2520Ability%2520Amplification%2520Strategy%2520for%2520Enhanced%2520Mathematical%250A%2520%2520Reasoning%2520in%2520Large%2520Language%2520Models%26entry.906535625%3DHyeonwoo%2520Kim%2520and%2520Gyoungjin%2520Gim%2520and%2520Yungi%2520Kim%2520and%2520Jihoo%2520Kim%2520and%2520Byungju%2520Kim%2520and%2520Wonseok%2520Lee%2520and%2520Chanjun%2520Park%26entry.1292438233%3D%2520%2520This%2520study%2520presents%2520a%2520novel%2520learning%2520approach%2520designed%2520to%2520enhance%2520both%250Amathematical%2520reasoning%2520and%2520problem-solving%2520abilities%2520of%2520Large%2520Language%2520Models%250A%2528LLMs%2529.%2520We%2520focus%2520on%2520integrating%2520the%2520Chain-of-Thought%2520%2528CoT%2529%2520and%2520the%250AProgram-of-Thought%2520%2528PoT%2529%2520learning%252C%2520hypothesizing%2520that%2520prioritizing%2520the%2520learning%250Aof%2520mathematical%2520reasoning%2520ability%2520is%2520helpful%2520for%2520the%2520amplification%2520of%250Aproblem-solving%2520ability.%2520Thus%252C%2520the%2520initial%2520learning%2520with%2520CoT%2520is%2520essential%2520for%250Asolving%2520challenging%2520mathematical%2520problems.%2520To%2520this%2520end%252C%2520we%2520propose%2520a%2520sequential%250Alearning%2520approach%252C%2520named%2520SAAS%2520%2528Solving%2520Ability%2520Amplification%2520Strategy%2529%252C%2520which%250Astrategically%2520transitions%2520from%2520CoT%2520learning%2520to%2520PoT%2520learning.%2520Our%2520empirical%250Astudy%252C%2520involving%2520an%2520extensive%2520performance%2520comparison%2520using%2520several%2520benchmarks%252C%250Ademonstrates%2520that%2520our%2520SAAS%2520achieves%2520state-of-the-art%2520%2528SOTA%2529%2520performance.%2520The%250Aresults%2520underscore%2520the%2520effectiveness%2520of%2520our%2520sequential%2520learning%2520approach%252C%250Amarking%2520a%2520significant%2520advancement%2520in%2520the%2520field%2520of%2520mathematical%2520reasoning%2520in%250ALLMs.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2404.03887v4%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=SAAS%3A%20Solving%20Ability%20Amplification%20Strategy%20for%20Enhanced%20Mathematical%0A%20%20Reasoning%20in%20Large%20Language%20Models&entry.906535625=Hyeonwoo%20Kim%20and%20Gyoungjin%20Gim%20and%20Yungi%20Kim%20and%20Jihoo%20Kim%20and%20Byungju%20Kim%20and%20Wonseok%20Lee%20and%20Chanjun%20Park&entry.1292438233=%20%20This%20study%20presents%20a%20novel%20learning%20approach%20designed%20to%20enhance%20both%0Amathematical%20reasoning%20and%20problem-solving%20abilities%20of%20Large%20Language%20Models%0A%28LLMs%29.%20We%20focus%20on%20integrating%20the%20Chain-of-Thought%20%28CoT%29%20and%20the%0AProgram-of-Thought%20%28PoT%29%20learning%2C%20hypothesizing%20that%20prioritizing%20the%20learning%0Aof%20mathematical%20reasoning%20ability%20is%20helpful%20for%20the%20amplification%20of%0Aproblem-solving%20ability.%20Thus%2C%20the%20initial%20learning%20with%20CoT%20is%20essential%20for%0Asolving%20challenging%20mathematical%20problems.%20To%20this%20end%2C%20we%20propose%20a%20sequential%0Alearning%20approach%2C%20named%20SAAS%20%28Solving%20Ability%20Amplification%20Strategy%29%2C%20which%0Astrategically%20transitions%20from%20CoT%20learning%20to%20PoT%20learning.%20Our%20empirical%0Astudy%2C%20involving%20an%20extensive%20performance%20comparison%20using%20several%20benchmarks%2C%0Ademonstrates%20that%20our%20SAAS%20achieves%20state-of-the-art%20%28SOTA%29%20performance.%20The%0Aresults%20underscore%20the%20effectiveness%20of%20our%20sequential%20learning%20approach%2C%0Amarking%20a%20significant%20advancement%20in%20the%20field%20of%20mathematical%20reasoning%20in%0ALLMs.%0A&entry.1838667208=http%3A//arxiv.org/abs/2404.03887v4&entry.124074799=Read"},
{"title": "On exploring the potential of quantum auto-encoder for learning quantum\n  systems", "author": "Yuxuan Du and Dacheng Tao", "abstract": "  The frequent interactions between quantum computing and machine learning\nrevolutionize both fields. One prototypical achievement is the quantum\nauto-encoder (QAE), as the leading strategy to relieve the curse of\ndimensionality ubiquitous in the quantum world. Despite its attractive\ncapabilities, practical applications of QAE have yet largely unexplored. To\nnarrow this knowledge gap, here we devise three effective QAE-based learning\nprotocols to address three classically computational hard learning problems\nwhen learning quantum systems, which are low-rank state fidelity estimation,\nquantum Fisher information estimation, and Gibbs state preparation. Attributed\nto the versatility of QAE, our proposals can be readily executed on near-term\nquantum machines. Besides, we analyze the error bounds of the trained protocols\nand showcase the necessary conditions to provide practical utility from the\nperspective of complexity theory. We conduct numerical simulations to confirm\nthe effectiveness of the proposed three protocols. Our work sheds new light on\ndeveloping advanced quantum learning algorithms to accomplish hard quantum\nphysics and quantum information processing tasks.\n", "link": "http://arxiv.org/abs/2106.15432v2", "date": "2024-10-02", "relevancy": 2.5046, "topK": [{"title": "IAE: Implicit Autoencoder for Point Cloud Self-supervised Representation Learning", "link": "https://arxiv.org/abs/2201.00785", "similarity": 0.5395}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.4817}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.4817}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20On%20exploring%20the%20potential%20of%20quantum%20auto-encoder%20for%20learning%20quantum%0A%20%20systems&body=Title%3A%20On%20exploring%20the%20potential%20of%20quantum%20auto-encoder%20for%20learning%20quantum%0A%20%20systems%0AAuthor%3A%20Yuxuan%20Du%20and%20Dacheng%20Tao%0AAbstract%3A%20%20%20The%20frequent%20interactions%20between%20quantum%20computing%20and%20machine%20learning%0Arevolutionize%20both%20fields.%20One%20prototypical%20achievement%20is%20the%20quantum%0Aauto-encoder%20%28QAE%29%2C%20as%20the%20leading%20strategy%20to%20relieve%20the%20curse%20of%0Adimensionality%20ubiquitous%20in%20the%20quantum%20world.%20Despite%20its%20attractive%0Acapabilities%2C%20practical%20applications%20of%20QAE%20have%20yet%20largely%20unexplored.%20To%0Anarrow%20this%20knowledge%20gap%2C%20here%20we%20devise%20three%20effective%20QAE-based%20learning%0Aprotocols%20to%20address%20three%20classically%20computational%20hard%20learning%20problems%0Awhen%20learning%20quantum%20systems%2C%20which%20are%20low-rank%20state%20fidelity%20estimation%2C%0Aquantum%20Fisher%20information%20estimation%2C%20and%20Gibbs%20state%20preparation.%20Attributed%0Ato%20the%20versatility%20of%20QAE%2C%20our%20proposals%20can%20be%20readily%20executed%20on%20near-term%0Aquantum%20machines.%20Besides%2C%20we%20analyze%20the%20error%20bounds%20of%20the%20trained%20protocols%0Aand%20showcase%20the%20necessary%20conditions%20to%20provide%20practical%20utility%20from%20the%0Aperspective%20of%20complexity%20theory.%20We%20conduct%20numerical%20simulations%20to%20confirm%0Athe%20effectiveness%20of%20the%20proposed%20three%20protocols.%20Our%20work%20sheds%20new%20light%20on%0Adeveloping%20advanced%20quantum%20learning%20algorithms%20to%20accomplish%20hard%20quantum%0Aphysics%20and%20quantum%20information%20processing%20tasks.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2106.15432v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DOn%2520exploring%2520the%2520potential%2520of%2520quantum%2520auto-encoder%2520for%2520learning%2520quantum%250A%2520%2520systems%26entry.906535625%3DYuxuan%2520Du%2520and%2520Dacheng%2520Tao%26entry.1292438233%3D%2520%2520The%2520frequent%2520interactions%2520between%2520quantum%2520computing%2520and%2520machine%2520learning%250Arevolutionize%2520both%2520fields.%2520One%2520prototypical%2520achievement%2520is%2520the%2520quantum%250Aauto-encoder%2520%2528QAE%2529%252C%2520as%2520the%2520leading%2520strategy%2520to%2520relieve%2520the%2520curse%2520of%250Adimensionality%2520ubiquitous%2520in%2520the%2520quantum%2520world.%2520Despite%2520its%2520attractive%250Acapabilities%252C%2520practical%2520applications%2520of%2520QAE%2520have%2520yet%2520largely%2520unexplored.%2520To%250Anarrow%2520this%2520knowledge%2520gap%252C%2520here%2520we%2520devise%2520three%2520effective%2520QAE-based%2520learning%250Aprotocols%2520to%2520address%2520three%2520classically%2520computational%2520hard%2520learning%2520problems%250Awhen%2520learning%2520quantum%2520systems%252C%2520which%2520are%2520low-rank%2520state%2520fidelity%2520estimation%252C%250Aquantum%2520Fisher%2520information%2520estimation%252C%2520and%2520Gibbs%2520state%2520preparation.%2520Attributed%250Ato%2520the%2520versatility%2520of%2520QAE%252C%2520our%2520proposals%2520can%2520be%2520readily%2520executed%2520on%2520near-term%250Aquantum%2520machines.%2520Besides%252C%2520we%2520analyze%2520the%2520error%2520bounds%2520of%2520the%2520trained%2520protocols%250Aand%2520showcase%2520the%2520necessary%2520conditions%2520to%2520provide%2520practical%2520utility%2520from%2520the%250Aperspective%2520of%2520complexity%2520theory.%2520We%2520conduct%2520numerical%2520simulations%2520to%2520confirm%250Athe%2520effectiveness%2520of%2520the%2520proposed%2520three%2520protocols.%2520Our%2520work%2520sheds%2520new%2520light%2520on%250Adeveloping%2520advanced%2520quantum%2520learning%2520algorithms%2520to%2520accomplish%2520hard%2520quantum%250Aphysics%2520and%2520quantum%2520information%2520processing%2520tasks.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2106.15432v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=On%20exploring%20the%20potential%20of%20quantum%20auto-encoder%20for%20learning%20quantum%0A%20%20systems&entry.906535625=Yuxuan%20Du%20and%20Dacheng%20Tao&entry.1292438233=%20%20The%20frequent%20interactions%20between%20quantum%20computing%20and%20machine%20learning%0Arevolutionize%20both%20fields.%20One%20prototypical%20achievement%20is%20the%20quantum%0Aauto-encoder%20%28QAE%29%2C%20as%20the%20leading%20strategy%20to%20relieve%20the%20curse%20of%0Adimensionality%20ubiquitous%20in%20the%20quantum%20world.%20Despite%20its%20attractive%0Acapabilities%2C%20practical%20applications%20of%20QAE%20have%20yet%20largely%20unexplored.%20To%0Anarrow%20this%20knowledge%20gap%2C%20here%20we%20devise%20three%20effective%20QAE-based%20learning%0Aprotocols%20to%20address%20three%20classically%20computational%20hard%20learning%20problems%0Awhen%20learning%20quantum%20systems%2C%20which%20are%20low-rank%20state%20fidelity%20estimation%2C%0Aquantum%20Fisher%20information%20estimation%2C%20and%20Gibbs%20state%20preparation.%20Attributed%0Ato%20the%20versatility%20of%20QAE%2C%20our%20proposals%20can%20be%20readily%20executed%20on%20near-term%0Aquantum%20machines.%20Besides%2C%20we%20analyze%20the%20error%20bounds%20of%20the%20trained%20protocols%0Aand%20showcase%20the%20necessary%20conditions%20to%20provide%20practical%20utility%20from%20the%0Aperspective%20of%20complexity%20theory.%20We%20conduct%20numerical%20simulations%20to%20confirm%0Athe%20effectiveness%20of%20the%20proposed%20three%20protocols.%20Our%20work%20sheds%20new%20light%20on%0Adeveloping%20advanced%20quantum%20learning%20algorithms%20to%20accomplish%20hard%20quantum%0Aphysics%20and%20quantum%20information%20processing%20tasks.%0A&entry.1838667208=http%3A//arxiv.org/abs/2106.15432v2&entry.124074799=Read"},
{"title": "Meta-TTT: A Meta-learning Minimax Framework For Test-Time Training", "author": "Chen Tao and Li Shen and Soumik Mondal", "abstract": "  Test-time domain adaptation is a challenging task that aims to adapt a\npre-trained model to limited, unlabeled target data during inference. Current\nmethods that rely on self-supervision and entropy minimization underperform\nwhen the self-supervised learning (SSL) task does not align well with the\nprimary objective. Additionally, minimizing entropy can lead to suboptimal\nsolutions when there is limited diversity within minibatches. This paper\nintroduces a meta-learning minimax framework for test-time training on batch\nnormalization (BN) layers, ensuring that the SSL task aligns with the primary\ntask while addressing minibatch overfitting. We adopt a mixed-BN approach that\ninterpolates current test batch statistics with the statistics from source\ndomains and propose a stochastic domain synthesizing method to improve model\ngeneralization and robustness to domain shifts. Extensive experiments\ndemonstrate that our method surpasses state-of-the-art techniques across\nvarious domain adaptation and generalization benchmarks, significantly\nenhancing the pre-trained model's robustness on unseen domains.\n", "link": "http://arxiv.org/abs/2410.01709v1", "date": "2024-10-02", "relevancy": 2.5044, "topK": [{"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5337}, {"title": "Boosted dynamic neural networks", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/26302", "similarity": 0.4853}, {"title": "Breadcrumbs: Adversarial class-balanced sampling for long-tailed recognition", "link": "https://link.springer.com/chapter/10.1007/978-3-031-20053-3_37", "similarity": 0.4836}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Meta-TTT%3A%20A%20Meta-learning%20Minimax%20Framework%20For%20Test-Time%20Training&body=Title%3A%20Meta-TTT%3A%20A%20Meta-learning%20Minimax%20Framework%20For%20Test-Time%20Training%0AAuthor%3A%20Chen%20Tao%20and%20Li%20Shen%20and%20Soumik%20Mondal%0AAbstract%3A%20%20%20Test-time%20domain%20adaptation%20is%20a%20challenging%20task%20that%20aims%20to%20adapt%20a%0Apre-trained%20model%20to%20limited%2C%20unlabeled%20target%20data%20during%20inference.%20Current%0Amethods%20that%20rely%20on%20self-supervision%20and%20entropy%20minimization%20underperform%0Awhen%20the%20self-supervised%20learning%20%28SSL%29%20task%20does%20not%20align%20well%20with%20the%0Aprimary%20objective.%20Additionally%2C%20minimizing%20entropy%20can%20lead%20to%20suboptimal%0Asolutions%20when%20there%20is%20limited%20diversity%20within%20minibatches.%20This%20paper%0Aintroduces%20a%20meta-learning%20minimax%20framework%20for%20test-time%20training%20on%20batch%0Anormalization%20%28BN%29%20layers%2C%20ensuring%20that%20the%20SSL%20task%20aligns%20with%20the%20primary%0Atask%20while%20addressing%20minibatch%20overfitting.%20We%20adopt%20a%20mixed-BN%20approach%20that%0Ainterpolates%20current%20test%20batch%20statistics%20with%20the%20statistics%20from%20source%0Adomains%20and%20propose%20a%20stochastic%20domain%20synthesizing%20method%20to%20improve%20model%0Ageneralization%20and%20robustness%20to%20domain%20shifts.%20Extensive%20experiments%0Ademonstrate%20that%20our%20method%20surpasses%20state-of-the-art%20techniques%20across%0Avarious%20domain%20adaptation%20and%20generalization%20benchmarks%2C%20significantly%0Aenhancing%20the%20pre-trained%20model%27s%20robustness%20on%20unseen%20domains.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2410.01709v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DMeta-TTT%253A%2520A%2520Meta-learning%2520Minimax%2520Framework%2520For%2520Test-Time%2520Training%26entry.906535625%3DChen%2520Tao%2520and%2520Li%2520Shen%2520and%2520Soumik%2520Mondal%26entry.1292438233%3D%2520%2520Test-time%2520domain%2520adaptation%2520is%2520a%2520challenging%2520task%2520that%2520aims%2520to%2520adapt%2520a%250Apre-trained%2520model%2520to%2520limited%252C%2520unlabeled%2520target%2520data%2520during%2520inference.%2520Current%250Amethods%2520that%2520rely%2520on%2520self-supervision%2520and%2520entropy%2520minimization%2520underperform%250Awhen%2520the%2520self-supervised%2520learning%2520%2528SSL%2529%2520task%2520does%2520not%2520align%2520well%2520with%2520the%250Aprimary%2520objective.%2520Additionally%252C%2520minimizing%2520entropy%2520can%2520lead%2520to%2520suboptimal%250Asolutions%2520when%2520there%2520is%2520limited%2520diversity%2520within%2520minibatches.%2520This%2520paper%250Aintroduces%2520a%2520meta-learning%2520minimax%2520framework%2520for%2520test-time%2520training%2520on%2520batch%250Anormalization%2520%2528BN%2529%2520layers%252C%2520ensuring%2520that%2520the%2520SSL%2520task%2520aligns%2520with%2520the%2520primary%250Atask%2520while%2520addressing%2520minibatch%2520overfitting.%2520We%2520adopt%2520a%2520mixed-BN%2520approach%2520that%250Ainterpolates%2520current%2520test%2520batch%2520statistics%2520with%2520the%2520statistics%2520from%2520source%250Adomains%2520and%2520propose%2520a%2520stochastic%2520domain%2520synthesizing%2520method%2520to%2520improve%2520model%250Ageneralization%2520and%2520robustness%2520to%2520domain%2520shifts.%2520Extensive%2520experiments%250Ademonstrate%2520that%2520our%2520method%2520surpasses%2520state-of-the-art%2520techniques%2520across%250Avarious%2520domain%2520adaptation%2520and%2520generalization%2520benchmarks%252C%2520significantly%250Aenhancing%2520the%2520pre-trained%2520model%2527s%2520robustness%2520on%2520unseen%2520domains.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2410.01709v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Meta-TTT%3A%20A%20Meta-learning%20Minimax%20Framework%20For%20Test-Time%20Training&entry.906535625=Chen%20Tao%20and%20Li%20Shen%20and%20Soumik%20Mondal&entry.1292438233=%20%20Test-time%20domain%20adaptation%20is%20a%20challenging%20task%20that%20aims%20to%20adapt%20a%0Apre-trained%20model%20to%20limited%2C%20unlabeled%20target%20data%20during%20inference.%20Current%0Amethods%20that%20rely%20on%20self-supervision%20and%20entropy%20minimization%20underperform%0Awhen%20the%20self-supervised%20learning%20%28SSL%29%20task%20does%20not%20align%20well%20with%20the%0Aprimary%20objective.%20Additionally%2C%20minimizing%20entropy%20can%20lead%20to%20suboptimal%0Asolutions%20when%20there%20is%20limited%20diversity%20within%20minibatches.%20This%20paper%0Aintroduces%20a%20meta-learning%20minimax%20framework%20for%20test-time%20training%20on%20batch%0Anormalization%20%28BN%29%20layers%2C%20ensuring%20that%20the%20SSL%20task%20aligns%20with%20the%20primary%0Atask%20while%20addressing%20minibatch%20overfitting.%20We%20adopt%20a%20mixed-BN%20approach%20that%0Ainterpolates%20current%20test%20batch%20statistics%20with%20the%20statistics%20from%20source%0Adomains%20and%20propose%20a%20stochastic%20domain%20synthesizing%20method%20to%20improve%20model%0Ageneralization%20and%20robustness%20to%20domain%20shifts.%20Extensive%20experiments%0Ademonstrate%20that%20our%20method%20surpasses%20state-of-the-art%20techniques%20across%0Avarious%20domain%20adaptation%20and%20generalization%20benchmarks%2C%20significantly%0Aenhancing%20the%20pre-trained%20model%27s%20robustness%20on%20unseen%20domains.%0A&entry.1838667208=http%3A//arxiv.org/abs/2410.01709v1&entry.124074799=Read"},
{"title": "Efficient Long-range Language Modeling with Self-supervised Causal\n  Retrieval", "author": "Xiang Hu and Zhihao Teng and Wei Wu and Kewei Tu", "abstract": "  Recently, retrieval-based language models (RLMs) have received much\nattention. However, most of them leverage a pre-trained retriever with fixed\nparameters, which may not adapt well to causal language models. In this work,\nwe propose Grouped Cross-Attention, a novel module enabling joint pre-training\nof the retriever and causal LM, and apply it to long-context modeling. For a\ngiven input sequence, we split it into chunks and use the current chunk to\nretrieve past chunks for subsequent text generation. Our innovation allows the\nretriever to learn how to retrieve past chunks that better minimize the\nauto-regressive loss of subsequent tokens in an end-to-end manner. By\nintegrating top-$k$ retrieval, our model can be pre-trained efficiently from\nscratch with context lengths up to 64K tokens. Our experiments show our model,\ncompared with long-range LM baselines, can achieve lower perplexity with\ncomparable or lower pre-training and inference costs.\n", "link": "http://arxiv.org/abs/2410.01651v1", "date": "2024-10-02", "relevancy": 2.4879, "topK": [{"title": "Self-supervised Photographic Image Layout Representation Learning", "link": "http://arxiv.org/abs/2403.03740v1", "similarity": 0.5096}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.4915}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.4915}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Efficient%20Long-range%20Language%20Modeling%20with%20Self-supervised%20Causal%0A%20%20Retrieval&body=Title%3A%20Efficient%20Long-range%20Language%20Modeling%20with%20Self-supervised%20Causal%0A%20%20Retrieval%0AAuthor%3A%20Xiang%20Hu%20and%20Zhihao%20Teng%20and%20Wei%20Wu%20and%20Kewei%20Tu%0AAbstract%3A%20%20%20Recently%2C%20retrieval-based%20language%20models%20%28RLMs%29%20have%20received%20much%0Aattention.%20However%2C%20most%20of%20them%20leverage%20a%20pre-trained%20retriever%20with%20fixed%0Aparameters%2C%20which%20may%20not%20adapt%20well%20to%20causal%20language%20models.%20In%20this%20work%2C%0Awe%20propose%20Grouped%20Cross-Attention%2C%20a%20novel%20module%20enabling%20joint%20pre-training%0Aof%20the%20retriever%20and%20causal%20LM%2C%20and%20apply%20it%20to%20long-context%20modeling.%20For%20a%0Agiven%20input%20sequence%2C%20we%20split%20it%20into%20chunks%20and%20use%20the%20current%20chunk%20to%0Aretrieve%20past%20chunks%20for%20subsequent%20text%20generation.%20Our%20innovation%20allows%20the%0Aretriever%20to%20learn%20how%20to%20retrieve%20past%20chunks%20that%20better%20minimize%20the%0Aauto-regressive%20loss%20of%20subsequent%20tokens%20in%20an%20end-to-end%20manner.%20By%0Aintegrating%20top-%24k%24%20retrieval%2C%20our%20model%20can%20be%20pre-trained%20efficiently%20from%0Ascratch%20with%20context%20lengths%20up%20to%2064K%20tokens.%20Our%20experiments%20show%20our%20model%2C%0Acompared%20with%20long-range%20LM%20baselines%2C%20can%20achieve%20lower%20perplexity%20with%0Acomparable%20or%20lower%20pre-training%20and%20inference%20costs.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2410.01651v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DEfficient%2520Long-range%2520Language%2520Modeling%2520with%2520Self-supervised%2520Causal%250A%2520%2520Retrieval%26entry.906535625%3DXiang%2520Hu%2520and%2520Zhihao%2520Teng%2520and%2520Wei%2520Wu%2520and%2520Kewei%2520Tu%26entry.1292438233%3D%2520%2520Recently%252C%2520retrieval-based%2520language%2520models%2520%2528RLMs%2529%2520have%2520received%2520much%250Aattention.%2520However%252C%2520most%2520of%2520them%2520leverage%2520a%2520pre-trained%2520retriever%2520with%2520fixed%250Aparameters%252C%2520which%2520may%2520not%2520adapt%2520well%2520to%2520causal%2520language%2520models.%2520In%2520this%2520work%252C%250Awe%2520propose%2520Grouped%2520Cross-Attention%252C%2520a%2520novel%2520module%2520enabling%2520joint%2520pre-training%250Aof%2520the%2520retriever%2520and%2520causal%2520LM%252C%2520and%2520apply%2520it%2520to%2520long-context%2520modeling.%2520For%2520a%250Agiven%2520input%2520sequence%252C%2520we%2520split%2520it%2520into%2520chunks%2520and%2520use%2520the%2520current%2520chunk%2520to%250Aretrieve%2520past%2520chunks%2520for%2520subsequent%2520text%2520generation.%2520Our%2520innovation%2520allows%2520the%250Aretriever%2520to%2520learn%2520how%2520to%2520retrieve%2520past%2520chunks%2520that%2520better%2520minimize%2520the%250Aauto-regressive%2520loss%2520of%2520subsequent%2520tokens%2520in%2520an%2520end-to-end%2520manner.%2520By%250Aintegrating%2520top-%2524k%2524%2520retrieval%252C%2520our%2520model%2520can%2520be%2520pre-trained%2520efficiently%2520from%250Ascratch%2520with%2520context%2520lengths%2520up%2520to%252064K%2520tokens.%2520Our%2520experiments%2520show%2520our%2520model%252C%250Acompared%2520with%2520long-range%2520LM%2520baselines%252C%2520can%2520achieve%2520lower%2520perplexity%2520with%250Acomparable%2520or%2520lower%2520pre-training%2520and%2520inference%2520costs.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2410.01651v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Efficient%20Long-range%20Language%20Modeling%20with%20Self-supervised%20Causal%0A%20%20Retrieval&entry.906535625=Xiang%20Hu%20and%20Zhihao%20Teng%20and%20Wei%20Wu%20and%20Kewei%20Tu&entry.1292438233=%20%20Recently%2C%20retrieval-based%20language%20models%20%28RLMs%29%20have%20received%20much%0Aattention.%20However%2C%20most%20of%20them%20leverage%20a%20pre-trained%20retriever%20with%20fixed%0Aparameters%2C%20which%20may%20not%20adapt%20well%20to%20causal%20language%20models.%20In%20this%20work%2C%0Awe%20propose%20Grouped%20Cross-Attention%2C%20a%20novel%20module%20enabling%20joint%20pre-training%0Aof%20the%20retriever%20and%20causal%20LM%2C%20and%20apply%20it%20to%20long-context%20modeling.%20For%20a%0Agiven%20input%20sequence%2C%20we%20split%20it%20into%20chunks%20and%20use%20the%20current%20chunk%20to%0Aretrieve%20past%20chunks%20for%20subsequent%20text%20generation.%20Our%20innovation%20allows%20the%0Aretriever%20to%20learn%20how%20to%20retrieve%20past%20chunks%20that%20better%20minimize%20the%0Aauto-regressive%20loss%20of%20subsequent%20tokens%20in%20an%20end-to-end%20manner.%20By%0Aintegrating%20top-%24k%24%20retrieval%2C%20our%20model%20can%20be%20pre-trained%20efficiently%20from%0Ascratch%20with%20context%20lengths%20up%20to%2064K%20tokens.%20Our%20experiments%20show%20our%20model%2C%0Acompared%20with%20long-range%20LM%20baselines%2C%20can%20achieve%20lower%20perplexity%20with%0Acomparable%20or%20lower%20pre-training%20and%20inference%20costs.%0A&entry.1838667208=http%3A//arxiv.org/abs/2410.01651v1&entry.124074799=Read"},
{"title": "Knowledge-Driven Feature Selection and Engineering for Genotype Data\n  with Large Language Models", "author": "Joseph Lee and Shu Yang and Jae Young Baik and Xiaoxi Liu and Zhen Tan and Dawei Li and Zixuan Wen and Bojian Hou and Duy Duong-Tran and Tianlong Chen and Li Shen", "abstract": "  Predicting phenotypes with complex genetic bases based on a small,\ninterpretable set of variant features remains a challenging task.\nConventionally, data-driven approaches are utilized for this task, yet the high\ndimensional nature of genotype data makes the analysis and prediction\ndifficult. Motivated by the extensive knowledge encoded in pre-trained LLMs and\ntheir success in processing complex biomedical concepts, we set to examine the\nability of LLMs in feature selection and engineering for tabular genotype data,\nwith a novel knowledge-driven framework. We develop FREEFORM, Free-flow\nReasoning and Ensembling for Enhanced Feature Output and Robust Modeling,\ndesigned with chain-of-thought and ensembling principles, to select and\nengineer features with the intrinsic knowledge of LLMs. Evaluated on two\ndistinct genotype-phenotype datasets, genetic ancestry and hereditary hearing\nloss, we find this framework outperforms several data-driven methods,\nparticularly on low-shot regimes. FREEFORM is available as open-source\nframework at GitHub: https://github.com/PennShenLab/FREEFORM.\n", "link": "http://arxiv.org/abs/2410.01795v1", "date": "2024-10-02", "relevancy": 2.4543, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5054}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5054}, {"title": "Breadcrumbs: Adversarial class-balanced sampling for long-tailed recognition", "link": "https://link.springer.com/chapter/10.1007/978-3-031-20053-3_37", "similarity": 0.4619}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Knowledge-Driven%20Feature%20Selection%20and%20Engineering%20for%20Genotype%20Data%0A%20%20with%20Large%20Language%20Models&body=Title%3A%20Knowledge-Driven%20Feature%20Selection%20and%20Engineering%20for%20Genotype%20Data%0A%20%20with%20Large%20Language%20Models%0AAuthor%3A%20Joseph%20Lee%20and%20Shu%20Yang%20and%20Jae%20Young%20Baik%20and%20Xiaoxi%20Liu%20and%20Zhen%20Tan%20and%20Dawei%20Li%20and%20Zixuan%20Wen%20and%20Bojian%20Hou%20and%20Duy%20Duong-Tran%20and%20Tianlong%20Chen%20and%20Li%20Shen%0AAbstract%3A%20%20%20Predicting%20phenotypes%20with%20complex%20genetic%20bases%20based%20on%20a%20small%2C%0Ainterpretable%20set%20of%20variant%20features%20remains%20a%20challenging%20task.%0AConventionally%2C%20data-driven%20approaches%20are%20utilized%20for%20this%20task%2C%20yet%20the%20high%0Adimensional%20nature%20of%20genotype%20data%20makes%20the%20analysis%20and%20prediction%0Adifficult.%20Motivated%20by%20the%20extensive%20knowledge%20encoded%20in%20pre-trained%20LLMs%20and%0Atheir%20success%20in%20processing%20complex%20biomedical%20concepts%2C%20we%20set%20to%20examine%20the%0Aability%20of%20LLMs%20in%20feature%20selection%20and%20engineering%20for%20tabular%20genotype%20data%2C%0Awith%20a%20novel%20knowledge-driven%20framework.%20We%20develop%20FREEFORM%2C%20Free-flow%0AReasoning%20and%20Ensembling%20for%20Enhanced%20Feature%20Output%20and%20Robust%20Modeling%2C%0Adesigned%20with%20chain-of-thought%20and%20ensembling%20principles%2C%20to%20select%20and%0Aengineer%20features%20with%20the%20intrinsic%20knowledge%20of%20LLMs.%20Evaluated%20on%20two%0Adistinct%20genotype-phenotype%20datasets%2C%20genetic%20ancestry%20and%20hereditary%20hearing%0Aloss%2C%20we%20find%20this%20framework%20outperforms%20several%20data-driven%20methods%2C%0Aparticularly%20on%20low-shot%20regimes.%20FREEFORM%20is%20available%20as%20open-source%0Aframework%20at%20GitHub%3A%20https%3A//github.com/PennShenLab/FREEFORM.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2410.01795v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DKnowledge-Driven%2520Feature%2520Selection%2520and%2520Engineering%2520for%2520Genotype%2520Data%250A%2520%2520with%2520Large%2520Language%2520Models%26entry.906535625%3DJoseph%2520Lee%2520and%2520Shu%2520Yang%2520and%2520Jae%2520Young%2520Baik%2520and%2520Xiaoxi%2520Liu%2520and%2520Zhen%2520Tan%2520and%2520Dawei%2520Li%2520and%2520Zixuan%2520Wen%2520and%2520Bojian%2520Hou%2520and%2520Duy%2520Duong-Tran%2520and%2520Tianlong%2520Chen%2520and%2520Li%2520Shen%26entry.1292438233%3D%2520%2520Predicting%2520phenotypes%2520with%2520complex%2520genetic%2520bases%2520based%2520on%2520a%2520small%252C%250Ainterpretable%2520set%2520of%2520variant%2520features%2520remains%2520a%2520challenging%2520task.%250AConventionally%252C%2520data-driven%2520approaches%2520are%2520utilized%2520for%2520this%2520task%252C%2520yet%2520the%2520high%250Adimensional%2520nature%2520of%2520genotype%2520data%2520makes%2520the%2520analysis%2520and%2520prediction%250Adifficult.%2520Motivated%2520by%2520the%2520extensive%2520knowledge%2520encoded%2520in%2520pre-trained%2520LLMs%2520and%250Atheir%2520success%2520in%2520processing%2520complex%2520biomedical%2520concepts%252C%2520we%2520set%2520to%2520examine%2520the%250Aability%2520of%2520LLMs%2520in%2520feature%2520selection%2520and%2520engineering%2520for%2520tabular%2520genotype%2520data%252C%250Awith%2520a%2520novel%2520knowledge-driven%2520framework.%2520We%2520develop%2520FREEFORM%252C%2520Free-flow%250AReasoning%2520and%2520Ensembling%2520for%2520Enhanced%2520Feature%2520Output%2520and%2520Robust%2520Modeling%252C%250Adesigned%2520with%2520chain-of-thought%2520and%2520ensembling%2520principles%252C%2520to%2520select%2520and%250Aengineer%2520features%2520with%2520the%2520intrinsic%2520knowledge%2520of%2520LLMs.%2520Evaluated%2520on%2520two%250Adistinct%2520genotype-phenotype%2520datasets%252C%2520genetic%2520ancestry%2520and%2520hereditary%2520hearing%250Aloss%252C%2520we%2520find%2520this%2520framework%2520outperforms%2520several%2520data-driven%2520methods%252C%250Aparticularly%2520on%2520low-shot%2520regimes.%2520FREEFORM%2520is%2520available%2520as%2520open-source%250Aframework%2520at%2520GitHub%253A%2520https%253A//github.com/PennShenLab/FREEFORM.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2410.01795v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Knowledge-Driven%20Feature%20Selection%20and%20Engineering%20for%20Genotype%20Data%0A%20%20with%20Large%20Language%20Models&entry.906535625=Joseph%20Lee%20and%20Shu%20Yang%20and%20Jae%20Young%20Baik%20and%20Xiaoxi%20Liu%20and%20Zhen%20Tan%20and%20Dawei%20Li%20and%20Zixuan%20Wen%20and%20Bojian%20Hou%20and%20Duy%20Duong-Tran%20and%20Tianlong%20Chen%20and%20Li%20Shen&entry.1292438233=%20%20Predicting%20phenotypes%20with%20complex%20genetic%20bases%20based%20on%20a%20small%2C%0Ainterpretable%20set%20of%20variant%20features%20remains%20a%20challenging%20task.%0AConventionally%2C%20data-driven%20approaches%20are%20utilized%20for%20this%20task%2C%20yet%20the%20high%0Adimensional%20nature%20of%20genotype%20data%20makes%20the%20analysis%20and%20prediction%0Adifficult.%20Motivated%20by%20the%20extensive%20knowledge%20encoded%20in%20pre-trained%20LLMs%20and%0Atheir%20success%20in%20processing%20complex%20biomedical%20concepts%2C%20we%20set%20to%20examine%20the%0Aability%20of%20LLMs%20in%20feature%20selection%20and%20engineering%20for%20tabular%20genotype%20data%2C%0Awith%20a%20novel%20knowledge-driven%20framework.%20We%20develop%20FREEFORM%2C%20Free-flow%0AReasoning%20and%20Ensembling%20for%20Enhanced%20Feature%20Output%20and%20Robust%20Modeling%2C%0Adesigned%20with%20chain-of-thought%20and%20ensembling%20principles%2C%20to%20select%20and%0Aengineer%20features%20with%20the%20intrinsic%20knowledge%20of%20LLMs.%20Evaluated%20on%20two%0Adistinct%20genotype-phenotype%20datasets%2C%20genetic%20ancestry%20and%20hereditary%20hearing%0Aloss%2C%20we%20find%20this%20framework%20outperforms%20several%20data-driven%20methods%2C%0Aparticularly%20on%20low-shot%20regimes.%20FREEFORM%20is%20available%20as%20open-source%0Aframework%20at%20GitHub%3A%20https%3A//github.com/PennShenLab/FREEFORM.%0A&entry.1838667208=http%3A//arxiv.org/abs/2410.01795v1&entry.124074799=Read"},
{"title": "Geometric Signatures of Compositionality Across a Language Model's\n  Lifetime", "author": "Jin Hwa Lee and Thomas Jiralerspong and Lei Yu and Yoshua Bengio and Emily Cheng", "abstract": "  Compositionality, the notion that the meaning of an expression is constructed\nfrom the meaning of its parts and syntactic rules, permits the infinite\nproductivity of human language. For the first time, artificial language models\n(LMs) are able to match human performance in a number of compositional\ngeneralization tasks. However, much remains to be understood about the\nrepresentational mechanisms underlying these abilities. We take a high-level\ngeometric approach to this problem by relating the degree of compositionality\nin a dataset to the intrinsic dimensionality of its representations under an\nLM, a measure of feature complexity. We find not only that the degree of\ndataset compositionality is reflected in representations' intrinsic\ndimensionality, but that the relationship between compositionality and\ngeometric complexity arises due to learned linguistic features over training.\nFinally, our analyses reveal a striking contrast between linear and nonlinear\ndimensionality, showing that they respectively encode formal and semantic\naspects of linguistic composition.\n", "link": "http://arxiv.org/abs/2410.01444v1", "date": "2024-10-02", "relevancy": 2.4461, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5003}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5003}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.4672}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Geometric%20Signatures%20of%20Compositionality%20Across%20a%20Language%20Model%27s%0A%20%20Lifetime&body=Title%3A%20Geometric%20Signatures%20of%20Compositionality%20Across%20a%20Language%20Model%27s%0A%20%20Lifetime%0AAuthor%3A%20Jin%20Hwa%20Lee%20and%20Thomas%20Jiralerspong%20and%20Lei%20Yu%20and%20Yoshua%20Bengio%20and%20Emily%20Cheng%0AAbstract%3A%20%20%20Compositionality%2C%20the%20notion%20that%20the%20meaning%20of%20an%20expression%20is%20constructed%0Afrom%20the%20meaning%20of%20its%20parts%20and%20syntactic%20rules%2C%20permits%20the%20infinite%0Aproductivity%20of%20human%20language.%20For%20the%20first%20time%2C%20artificial%20language%20models%0A%28LMs%29%20are%20able%20to%20match%20human%20performance%20in%20a%20number%20of%20compositional%0Ageneralization%20tasks.%20However%2C%20much%20remains%20to%20be%20understood%20about%20the%0Arepresentational%20mechanisms%20underlying%20these%20abilities.%20We%20take%20a%20high-level%0Ageometric%20approach%20to%20this%20problem%20by%20relating%20the%20degree%20of%20compositionality%0Ain%20a%20dataset%20to%20the%20intrinsic%20dimensionality%20of%20its%20representations%20under%20an%0ALM%2C%20a%20measure%20of%20feature%20complexity.%20We%20find%20not%20only%20that%20the%20degree%20of%0Adataset%20compositionality%20is%20reflected%20in%20representations%27%20intrinsic%0Adimensionality%2C%20but%20that%20the%20relationship%20between%20compositionality%20and%0Ageometric%20complexity%20arises%20due%20to%20learned%20linguistic%20features%20over%20training.%0AFinally%2C%20our%20analyses%20reveal%20a%20striking%20contrast%20between%20linear%20and%20nonlinear%0Adimensionality%2C%20showing%20that%20they%20respectively%20encode%20formal%20and%20semantic%0Aaspects%20of%20linguistic%20composition.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2410.01444v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DGeometric%2520Signatures%2520of%2520Compositionality%2520Across%2520a%2520Language%2520Model%2527s%250A%2520%2520Lifetime%26entry.906535625%3DJin%2520Hwa%2520Lee%2520and%2520Thomas%2520Jiralerspong%2520and%2520Lei%2520Yu%2520and%2520Yoshua%2520Bengio%2520and%2520Emily%2520Cheng%26entry.1292438233%3D%2520%2520Compositionality%252C%2520the%2520notion%2520that%2520the%2520meaning%2520of%2520an%2520expression%2520is%2520constructed%250Afrom%2520the%2520meaning%2520of%2520its%2520parts%2520and%2520syntactic%2520rules%252C%2520permits%2520the%2520infinite%250Aproductivity%2520of%2520human%2520language.%2520For%2520the%2520first%2520time%252C%2520artificial%2520language%2520models%250A%2528LMs%2529%2520are%2520able%2520to%2520match%2520human%2520performance%2520in%2520a%2520number%2520of%2520compositional%250Ageneralization%2520tasks.%2520However%252C%2520much%2520remains%2520to%2520be%2520understood%2520about%2520the%250Arepresentational%2520mechanisms%2520underlying%2520these%2520abilities.%2520We%2520take%2520a%2520high-level%250Ageometric%2520approach%2520to%2520this%2520problem%2520by%2520relating%2520the%2520degree%2520of%2520compositionality%250Ain%2520a%2520dataset%2520to%2520the%2520intrinsic%2520dimensionality%2520of%2520its%2520representations%2520under%2520an%250ALM%252C%2520a%2520measure%2520of%2520feature%2520complexity.%2520We%2520find%2520not%2520only%2520that%2520the%2520degree%2520of%250Adataset%2520compositionality%2520is%2520reflected%2520in%2520representations%2527%2520intrinsic%250Adimensionality%252C%2520but%2520that%2520the%2520relationship%2520between%2520compositionality%2520and%250Ageometric%2520complexity%2520arises%2520due%2520to%2520learned%2520linguistic%2520features%2520over%2520training.%250AFinally%252C%2520our%2520analyses%2520reveal%2520a%2520striking%2520contrast%2520between%2520linear%2520and%2520nonlinear%250Adimensionality%252C%2520showing%2520that%2520they%2520respectively%2520encode%2520formal%2520and%2520semantic%250Aaspects%2520of%2520linguistic%2520composition.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2410.01444v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Geometric%20Signatures%20of%20Compositionality%20Across%20a%20Language%20Model%27s%0A%20%20Lifetime&entry.906535625=Jin%20Hwa%20Lee%20and%20Thomas%20Jiralerspong%20and%20Lei%20Yu%20and%20Yoshua%20Bengio%20and%20Emily%20Cheng&entry.1292438233=%20%20Compositionality%2C%20the%20notion%20that%20the%20meaning%20of%20an%20expression%20is%20constructed%0Afrom%20the%20meaning%20of%20its%20parts%20and%20syntactic%20rules%2C%20permits%20the%20infinite%0Aproductivity%20of%20human%20language.%20For%20the%20first%20time%2C%20artificial%20language%20models%0A%28LMs%29%20are%20able%20to%20match%20human%20performance%20in%20a%20number%20of%20compositional%0Ageneralization%20tasks.%20However%2C%20much%20remains%20to%20be%20understood%20about%20the%0Arepresentational%20mechanisms%20underlying%20these%20abilities.%20We%20take%20a%20high-level%0Ageometric%20approach%20to%20this%20problem%20by%20relating%20the%20degree%20of%20compositionality%0Ain%20a%20dataset%20to%20the%20intrinsic%20dimensionality%20of%20its%20representations%20under%20an%0ALM%2C%20a%20measure%20of%20feature%20complexity.%20We%20find%20not%20only%20that%20the%20degree%20of%0Adataset%20compositionality%20is%20reflected%20in%20representations%27%20intrinsic%0Adimensionality%2C%20but%20that%20the%20relationship%20between%20compositionality%20and%0Ageometric%20complexity%20arises%20due%20to%20learned%20linguistic%20features%20over%20training.%0AFinally%2C%20our%20analyses%20reveal%20a%20striking%20contrast%20between%20linear%20and%20nonlinear%0Adimensionality%2C%20showing%20that%20they%20respectively%20encode%20formal%20and%20semantic%0Aaspects%20of%20linguistic%20composition.%0A&entry.1838667208=http%3A//arxiv.org/abs/2410.01444v1&entry.124074799=Read"},
{"title": "On Using Certified Training towards Empirical Robustness", "author": "Alessandro De Palma and Serge Durand and Zakaria Chihani and Fran\u00e7ois Terrier and Caterina Urban", "abstract": "  Adversarial training is arguably the most popular way to provide empirical\nrobustness against specific adversarial examples. While variants based on\nmulti-step attacks incur significant computational overhead, single-step\nvariants are vulnerable to a failure mode known as catastrophic overfitting,\nwhich hinders their practical utility for large perturbations. A parallel line\nof work, certified training, has focused on producing networks amenable to\nformal guarantees of robustness against any possible attack. However, the wide\ngap between the best-performing empirical and certified defenses has severely\nlimited the applicability of the latter. Inspired by recent developments in\ncertified training, which rely on a combination of adversarial attacks with\nnetwork over-approximations, and by the connections between local linearity and\ncatastrophic overfitting, we present experimental evidence on the practical\nutility and limitations of using certified training towards empirical\nrobustness. We show that, when tuned for the purpose, a recent certified\ntraining algorithm can prevent catastrophic overfitting on single-step attacks,\nand that it can bridge the gap to multi-step baselines under appropriate\nexperimental settings. Finally, we present a novel regularizer for network\nover-approximations that can achieve similar effects while markedly reducing\nruntime.\n", "link": "http://arxiv.org/abs/2410.01617v1", "date": "2024-10-02", "relevancy": 2.4393, "topK": [{"title": "Safe and efficient training of a control agent", "link": "https://patents.google.com/patent/US11709462B2/en", "similarity": 0.5072}, {"title": "Breadcrumbs: Adversarial class-balanced sampling for long-tailed recognition", "link": "https://link.springer.com/chapter/10.1007/978-3-031-20053-3_37", "similarity": 0.4826}, {"title": "Boosted dynamic neural networks", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/26302", "similarity": 0.4737}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20On%20Using%20Certified%20Training%20towards%20Empirical%20Robustness&body=Title%3A%20On%20Using%20Certified%20Training%20towards%20Empirical%20Robustness%0AAuthor%3A%20Alessandro%20De%20Palma%20and%20Serge%20Durand%20and%20Zakaria%20Chihani%20and%20Fran%C3%A7ois%20Terrier%20and%20Caterina%20Urban%0AAbstract%3A%20%20%20Adversarial%20training%20is%20arguably%20the%20most%20popular%20way%20to%20provide%20empirical%0Arobustness%20against%20specific%20adversarial%20examples.%20While%20variants%20based%20on%0Amulti-step%20attacks%20incur%20significant%20computational%20overhead%2C%20single-step%0Avariants%20are%20vulnerable%20to%20a%20failure%20mode%20known%20as%20catastrophic%20overfitting%2C%0Awhich%20hinders%20their%20practical%20utility%20for%20large%20perturbations.%20A%20parallel%20line%0Aof%20work%2C%20certified%20training%2C%20has%20focused%20on%20producing%20networks%20amenable%20to%0Aformal%20guarantees%20of%20robustness%20against%20any%20possible%20attack.%20However%2C%20the%20wide%0Agap%20between%20the%20best-performing%20empirical%20and%20certified%20defenses%20has%20severely%0Alimited%20the%20applicability%20of%20the%20latter.%20Inspired%20by%20recent%20developments%20in%0Acertified%20training%2C%20which%20rely%20on%20a%20combination%20of%20adversarial%20attacks%20with%0Anetwork%20over-approximations%2C%20and%20by%20the%20connections%20between%20local%20linearity%20and%0Acatastrophic%20overfitting%2C%20we%20present%20experimental%20evidence%20on%20the%20practical%0Autility%20and%20limitations%20of%20using%20certified%20training%20towards%20empirical%0Arobustness.%20We%20show%20that%2C%20when%20tuned%20for%20the%20purpose%2C%20a%20recent%20certified%0Atraining%20algorithm%20can%20prevent%20catastrophic%20overfitting%20on%20single-step%20attacks%2C%0Aand%20that%20it%20can%20bridge%20the%20gap%20to%20multi-step%20baselines%20under%20appropriate%0Aexperimental%20settings.%20Finally%2C%20we%20present%20a%20novel%20regularizer%20for%20network%0Aover-approximations%20that%20can%20achieve%20similar%20effects%20while%20markedly%20reducing%0Aruntime.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2410.01617v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DOn%2520Using%2520Certified%2520Training%2520towards%2520Empirical%2520Robustness%26entry.906535625%3DAlessandro%2520De%2520Palma%2520and%2520Serge%2520Durand%2520and%2520Zakaria%2520Chihani%2520and%2520Fran%25C3%25A7ois%2520Terrier%2520and%2520Caterina%2520Urban%26entry.1292438233%3D%2520%2520Adversarial%2520training%2520is%2520arguably%2520the%2520most%2520popular%2520way%2520to%2520provide%2520empirical%250Arobustness%2520against%2520specific%2520adversarial%2520examples.%2520While%2520variants%2520based%2520on%250Amulti-step%2520attacks%2520incur%2520significant%2520computational%2520overhead%252C%2520single-step%250Avariants%2520are%2520vulnerable%2520to%2520a%2520failure%2520mode%2520known%2520as%2520catastrophic%2520overfitting%252C%250Awhich%2520hinders%2520their%2520practical%2520utility%2520for%2520large%2520perturbations.%2520A%2520parallel%2520line%250Aof%2520work%252C%2520certified%2520training%252C%2520has%2520focused%2520on%2520producing%2520networks%2520amenable%2520to%250Aformal%2520guarantees%2520of%2520robustness%2520against%2520any%2520possible%2520attack.%2520However%252C%2520the%2520wide%250Agap%2520between%2520the%2520best-performing%2520empirical%2520and%2520certified%2520defenses%2520has%2520severely%250Alimited%2520the%2520applicability%2520of%2520the%2520latter.%2520Inspired%2520by%2520recent%2520developments%2520in%250Acertified%2520training%252C%2520which%2520rely%2520on%2520a%2520combination%2520of%2520adversarial%2520attacks%2520with%250Anetwork%2520over-approximations%252C%2520and%2520by%2520the%2520connections%2520between%2520local%2520linearity%2520and%250Acatastrophic%2520overfitting%252C%2520we%2520present%2520experimental%2520evidence%2520on%2520the%2520practical%250Autility%2520and%2520limitations%2520of%2520using%2520certified%2520training%2520towards%2520empirical%250Arobustness.%2520We%2520show%2520that%252C%2520when%2520tuned%2520for%2520the%2520purpose%252C%2520a%2520recent%2520certified%250Atraining%2520algorithm%2520can%2520prevent%2520catastrophic%2520overfitting%2520on%2520single-step%2520attacks%252C%250Aand%2520that%2520it%2520can%2520bridge%2520the%2520gap%2520to%2520multi-step%2520baselines%2520under%2520appropriate%250Aexperimental%2520settings.%2520Finally%252C%2520we%2520present%2520a%2520novel%2520regularizer%2520for%2520network%250Aover-approximations%2520that%2520can%2520achieve%2520similar%2520effects%2520while%2520markedly%2520reducing%250Aruntime.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2410.01617v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=On%20Using%20Certified%20Training%20towards%20Empirical%20Robustness&entry.906535625=Alessandro%20De%20Palma%20and%20Serge%20Durand%20and%20Zakaria%20Chihani%20and%20Fran%C3%A7ois%20Terrier%20and%20Caterina%20Urban&entry.1292438233=%20%20Adversarial%20training%20is%20arguably%20the%20most%20popular%20way%20to%20provide%20empirical%0Arobustness%20against%20specific%20adversarial%20examples.%20While%20variants%20based%20on%0Amulti-step%20attacks%20incur%20significant%20computational%20overhead%2C%20single-step%0Avariants%20are%20vulnerable%20to%20a%20failure%20mode%20known%20as%20catastrophic%20overfitting%2C%0Awhich%20hinders%20their%20practical%20utility%20for%20large%20perturbations.%20A%20parallel%20line%0Aof%20work%2C%20certified%20training%2C%20has%20focused%20on%20producing%20networks%20amenable%20to%0Aformal%20guarantees%20of%20robustness%20against%20any%20possible%20attack.%20However%2C%20the%20wide%0Agap%20between%20the%20best-performing%20empirical%20and%20certified%20defenses%20has%20severely%0Alimited%20the%20applicability%20of%20the%20latter.%20Inspired%20by%20recent%20developments%20in%0Acertified%20training%2C%20which%20rely%20on%20a%20combination%20of%20adversarial%20attacks%20with%0Anetwork%20over-approximations%2C%20and%20by%20the%20connections%20between%20local%20linearity%20and%0Acatastrophic%20overfitting%2C%20we%20present%20experimental%20evidence%20on%20the%20practical%0Autility%20and%20limitations%20of%20using%20certified%20training%20towards%20empirical%0Arobustness.%20We%20show%20that%2C%20when%20tuned%20for%20the%20purpose%2C%20a%20recent%20certified%0Atraining%20algorithm%20can%20prevent%20catastrophic%20overfitting%20on%20single-step%20attacks%2C%0Aand%20that%20it%20can%20bridge%20the%20gap%20to%20multi-step%20baselines%20under%20appropriate%0Aexperimental%20settings.%20Finally%2C%20we%20present%20a%20novel%20regularizer%20for%20network%0Aover-approximations%20that%20can%20achieve%20similar%20effects%20while%20markedly%20reducing%0Aruntime.%0A&entry.1838667208=http%3A//arxiv.org/abs/2410.01617v1&entry.124074799=Read"},
{"title": "PROXI: Challenging the GNNs for Link Prediction", "author": "Astrit Tola and Jack Myrick and Baris Coskunuzer", "abstract": "  Over the past decade, Graph Neural Networks (GNNs) have transformed graph\nrepresentation learning. In the widely adopted message-passing GNN framework,\nnodes refine their representations by aggregating information from neighboring\nnodes iteratively. While GNNs excel in various domains, recent theoretical\nstudies have raised concerns about their capabilities. GNNs aim to address\nvarious graph-related tasks by utilizing such node representations, however,\nthis one-size-fits-all approach proves suboptimal for diverse tasks.\n  Motivated by these observations, we conduct empirical tests to compare the\nperformance of current GNN models with more conventional and direct methods in\nlink prediction tasks. Introducing our model, PROXI, which leverages proximity\ninformation of node pairs in both graph and attribute spaces, we find that\nstandard machine learning (ML) models perform competitively, even outperforming\ncutting-edge GNN models when applied to these proximity metrics derived from\nnode neighborhoods and attributes. This holds true across both homophilic and\nheterophilic networks, as well as small and large benchmark datasets, including\nthose from the Open Graph Benchmark (OGB). Moreover, we show that augmenting\ntraditional GNNs with PROXI significantly boosts their link prediction\nperformance. Our empirical findings corroborate the previously mentioned\ntheoretical observations and imply that there exists ample room for enhancement\nin current GNN models to reach their potential.\n", "link": "http://arxiv.org/abs/2410.01802v1", "date": "2024-10-02", "relevancy": 2.4345, "topK": [{"title": "Boosted dynamic neural networks", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/26302", "similarity": 0.5059}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.4774}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.4774}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20PROXI%3A%20Challenging%20the%20GNNs%20for%20Link%20Prediction&body=Title%3A%20PROXI%3A%20Challenging%20the%20GNNs%20for%20Link%20Prediction%0AAuthor%3A%20Astrit%20Tola%20and%20Jack%20Myrick%20and%20Baris%20Coskunuzer%0AAbstract%3A%20%20%20Over%20the%20past%20decade%2C%20Graph%20Neural%20Networks%20%28GNNs%29%20have%20transformed%20graph%0Arepresentation%20learning.%20In%20the%20widely%20adopted%20message-passing%20GNN%20framework%2C%0Anodes%20refine%20their%20representations%20by%20aggregating%20information%20from%20neighboring%0Anodes%20iteratively.%20While%20GNNs%20excel%20in%20various%20domains%2C%20recent%20theoretical%0Astudies%20have%20raised%20concerns%20about%20their%20capabilities.%20GNNs%20aim%20to%20address%0Avarious%20graph-related%20tasks%20by%20utilizing%20such%20node%20representations%2C%20however%2C%0Athis%20one-size-fits-all%20approach%20proves%20suboptimal%20for%20diverse%20tasks.%0A%20%20Motivated%20by%20these%20observations%2C%20we%20conduct%20empirical%20tests%20to%20compare%20the%0Aperformance%20of%20current%20GNN%20models%20with%20more%20conventional%20and%20direct%20methods%20in%0Alink%20prediction%20tasks.%20Introducing%20our%20model%2C%20PROXI%2C%20which%20leverages%20proximity%0Ainformation%20of%20node%20pairs%20in%20both%20graph%20and%20attribute%20spaces%2C%20we%20find%20that%0Astandard%20machine%20learning%20%28ML%29%20models%20perform%20competitively%2C%20even%20outperforming%0Acutting-edge%20GNN%20models%20when%20applied%20to%20these%20proximity%20metrics%20derived%20from%0Anode%20neighborhoods%20and%20attributes.%20This%20holds%20true%20across%20both%20homophilic%20and%0Aheterophilic%20networks%2C%20as%20well%20as%20small%20and%20large%20benchmark%20datasets%2C%20including%0Athose%20from%20the%20Open%20Graph%20Benchmark%20%28OGB%29.%20Moreover%2C%20we%20show%20that%20augmenting%0Atraditional%20GNNs%20with%20PROXI%20significantly%20boosts%20their%20link%20prediction%0Aperformance.%20Our%20empirical%20findings%20corroborate%20the%20previously%20mentioned%0Atheoretical%20observations%20and%20imply%20that%20there%20exists%20ample%20room%20for%20enhancement%0Ain%20current%20GNN%20models%20to%20reach%20their%20potential.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2410.01802v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DPROXI%253A%2520Challenging%2520the%2520GNNs%2520for%2520Link%2520Prediction%26entry.906535625%3DAstrit%2520Tola%2520and%2520Jack%2520Myrick%2520and%2520Baris%2520Coskunuzer%26entry.1292438233%3D%2520%2520Over%2520the%2520past%2520decade%252C%2520Graph%2520Neural%2520Networks%2520%2528GNNs%2529%2520have%2520transformed%2520graph%250Arepresentation%2520learning.%2520In%2520the%2520widely%2520adopted%2520message-passing%2520GNN%2520framework%252C%250Anodes%2520refine%2520their%2520representations%2520by%2520aggregating%2520information%2520from%2520neighboring%250Anodes%2520iteratively.%2520While%2520GNNs%2520excel%2520in%2520various%2520domains%252C%2520recent%2520theoretical%250Astudies%2520have%2520raised%2520concerns%2520about%2520their%2520capabilities.%2520GNNs%2520aim%2520to%2520address%250Avarious%2520graph-related%2520tasks%2520by%2520utilizing%2520such%2520node%2520representations%252C%2520however%252C%250Athis%2520one-size-fits-all%2520approach%2520proves%2520suboptimal%2520for%2520diverse%2520tasks.%250A%2520%2520Motivated%2520by%2520these%2520observations%252C%2520we%2520conduct%2520empirical%2520tests%2520to%2520compare%2520the%250Aperformance%2520of%2520current%2520GNN%2520models%2520with%2520more%2520conventional%2520and%2520direct%2520methods%2520in%250Alink%2520prediction%2520tasks.%2520Introducing%2520our%2520model%252C%2520PROXI%252C%2520which%2520leverages%2520proximity%250Ainformation%2520of%2520node%2520pairs%2520in%2520both%2520graph%2520and%2520attribute%2520spaces%252C%2520we%2520find%2520that%250Astandard%2520machine%2520learning%2520%2528ML%2529%2520models%2520perform%2520competitively%252C%2520even%2520outperforming%250Acutting-edge%2520GNN%2520models%2520when%2520applied%2520to%2520these%2520proximity%2520metrics%2520derived%2520from%250Anode%2520neighborhoods%2520and%2520attributes.%2520This%2520holds%2520true%2520across%2520both%2520homophilic%2520and%250Aheterophilic%2520networks%252C%2520as%2520well%2520as%2520small%2520and%2520large%2520benchmark%2520datasets%252C%2520including%250Athose%2520from%2520the%2520Open%2520Graph%2520Benchmark%2520%2528OGB%2529.%2520Moreover%252C%2520we%2520show%2520that%2520augmenting%250Atraditional%2520GNNs%2520with%2520PROXI%2520significantly%2520boosts%2520their%2520link%2520prediction%250Aperformance.%2520Our%2520empirical%2520findings%2520corroborate%2520the%2520previously%2520mentioned%250Atheoretical%2520observations%2520and%2520imply%2520that%2520there%2520exists%2520ample%2520room%2520for%2520enhancement%250Ain%2520current%2520GNN%2520models%2520to%2520reach%2520their%2520potential.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2410.01802v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=PROXI%3A%20Challenging%20the%20GNNs%20for%20Link%20Prediction&entry.906535625=Astrit%20Tola%20and%20Jack%20Myrick%20and%20Baris%20Coskunuzer&entry.1292438233=%20%20Over%20the%20past%20decade%2C%20Graph%20Neural%20Networks%20%28GNNs%29%20have%20transformed%20graph%0Arepresentation%20learning.%20In%20the%20widely%20adopted%20message-passing%20GNN%20framework%2C%0Anodes%20refine%20their%20representations%20by%20aggregating%20information%20from%20neighboring%0Anodes%20iteratively.%20While%20GNNs%20excel%20in%20various%20domains%2C%20recent%20theoretical%0Astudies%20have%20raised%20concerns%20about%20their%20capabilities.%20GNNs%20aim%20to%20address%0Avarious%20graph-related%20tasks%20by%20utilizing%20such%20node%20representations%2C%20however%2C%0Athis%20one-size-fits-all%20approach%20proves%20suboptimal%20for%20diverse%20tasks.%0A%20%20Motivated%20by%20these%20observations%2C%20we%20conduct%20empirical%20tests%20to%20compare%20the%0Aperformance%20of%20current%20GNN%20models%20with%20more%20conventional%20and%20direct%20methods%20in%0Alink%20prediction%20tasks.%20Introducing%20our%20model%2C%20PROXI%2C%20which%20leverages%20proximity%0Ainformation%20of%20node%20pairs%20in%20both%20graph%20and%20attribute%20spaces%2C%20we%20find%20that%0Astandard%20machine%20learning%20%28ML%29%20models%20perform%20competitively%2C%20even%20outperforming%0Acutting-edge%20GNN%20models%20when%20applied%20to%20these%20proximity%20metrics%20derived%20from%0Anode%20neighborhoods%20and%20attributes.%20This%20holds%20true%20across%20both%20homophilic%20and%0Aheterophilic%20networks%2C%20as%20well%20as%20small%20and%20large%20benchmark%20datasets%2C%20including%0Athose%20from%20the%20Open%20Graph%20Benchmark%20%28OGB%29.%20Moreover%2C%20we%20show%20that%20augmenting%0Atraditional%20GNNs%20with%20PROXI%20significantly%20boosts%20their%20link%20prediction%0Aperformance.%20Our%20empirical%20findings%20corroborate%20the%20previously%20mentioned%0Atheoretical%20observations%20and%20imply%20that%20there%20exists%20ample%20room%20for%20enhancement%0Ain%20current%20GNN%20models%20to%20reach%20their%20potential.%0A&entry.1838667208=http%3A//arxiv.org/abs/2410.01802v1&entry.124074799=Read"},
{"title": "OpenMathInstruct-2: Accelerating AI for Math with Massive Open-Source\n  Instruction Data", "author": "Shubham Toshniwal and Wei Du and Ivan Moshkov and Branislav Kisacanin and Alexan Ayrapetyan and Igor Gitman", "abstract": "  Mathematical reasoning continues to be a critical challenge in large language\nmodel (LLM) development with significant interest. However, most of the\ncutting-edge progress in mathematical reasoning with LLMs has become\n\\emph{closed-source} due to lack of access to training data. This lack of data\naccess limits researchers from understanding the impact of different choices\nfor synthesizing and utilizing the data. With the goal of creating a\nhigh-quality finetuning (SFT) dataset for math reasoning, we conduct careful\nablation experiments on data synthesis using the recently released\n\\texttt{Llama3.1} family of models. Our experiments show that: (a) solution\nformat matters, with excessively verbose solutions proving detrimental to SFT\nperformance, (b) data generated by a strong teacher outperforms\n\\emph{on-policy} data generated by a weak student model, (c) SFT is robust to\nlow-quality solutions, allowing for imprecise data filtering, and (d) question\ndiversity is crucial for achieving data scaling gains. Based on these insights,\nwe create the OpenMathInstruct-2 dataset, which consists of 14M\nquestion-solution pairs ($\\approx$ 600K unique questions), making it nearly\neight times larger than the previous largest open-source math reasoning\ndataset. Finetuning the \\texttt{Llama-3.1-8B-Base} using OpenMathInstruct-2\noutperforms \\texttt{Llama3.1-8B-Instruct} on MATH by an absolute 15.9\\% (51.9\\%\n$\\rightarrow$ 67.8\\%). Finally, to accelerate the open-source efforts, we\nrelease the code, the finetuned models, and the OpenMathInstruct-2 dataset\nunder a commercially permissive license.\n", "link": "http://arxiv.org/abs/2410.01560v1", "date": "2024-10-02", "relevancy": 2.4275, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.4864}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.4864}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.4838}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20OpenMathInstruct-2%3A%20Accelerating%20AI%20for%20Math%20with%20Massive%20Open-Source%0A%20%20Instruction%20Data&body=Title%3A%20OpenMathInstruct-2%3A%20Accelerating%20AI%20for%20Math%20with%20Massive%20Open-Source%0A%20%20Instruction%20Data%0AAuthor%3A%20Shubham%20Toshniwal%20and%20Wei%20Du%20and%20Ivan%20Moshkov%20and%20Branislav%20Kisacanin%20and%20Alexan%20Ayrapetyan%20and%20Igor%20Gitman%0AAbstract%3A%20%20%20Mathematical%20reasoning%20continues%20to%20be%20a%20critical%20challenge%20in%20large%20language%0Amodel%20%28LLM%29%20development%20with%20significant%20interest.%20However%2C%20most%20of%20the%0Acutting-edge%20progress%20in%20mathematical%20reasoning%20with%20LLMs%20has%20become%0A%5Cemph%7Bclosed-source%7D%20due%20to%20lack%20of%20access%20to%20training%20data.%20This%20lack%20of%20data%0Aaccess%20limits%20researchers%20from%20understanding%20the%20impact%20of%20different%20choices%0Afor%20synthesizing%20and%20utilizing%20the%20data.%20With%20the%20goal%20of%20creating%20a%0Ahigh-quality%20finetuning%20%28SFT%29%20dataset%20for%20math%20reasoning%2C%20we%20conduct%20careful%0Aablation%20experiments%20on%20data%20synthesis%20using%20the%20recently%20released%0A%5Ctexttt%7BLlama3.1%7D%20family%20of%20models.%20Our%20experiments%20show%20that%3A%20%28a%29%20solution%0Aformat%20matters%2C%20with%20excessively%20verbose%20solutions%20proving%20detrimental%20to%20SFT%0Aperformance%2C%20%28b%29%20data%20generated%20by%20a%20strong%20teacher%20outperforms%0A%5Cemph%7Bon-policy%7D%20data%20generated%20by%20a%20weak%20student%20model%2C%20%28c%29%20SFT%20is%20robust%20to%0Alow-quality%20solutions%2C%20allowing%20for%20imprecise%20data%20filtering%2C%20and%20%28d%29%20question%0Adiversity%20is%20crucial%20for%20achieving%20data%20scaling%20gains.%20Based%20on%20these%20insights%2C%0Awe%20create%20the%20OpenMathInstruct-2%20dataset%2C%20which%20consists%20of%2014M%0Aquestion-solution%20pairs%20%28%24%5Capprox%24%20600K%20unique%20questions%29%2C%20making%20it%20nearly%0Aeight%20times%20larger%20than%20the%20previous%20largest%20open-source%20math%20reasoning%0Adataset.%20Finetuning%20the%20%5Ctexttt%7BLlama-3.1-8B-Base%7D%20using%20OpenMathInstruct-2%0Aoutperforms%20%5Ctexttt%7BLlama3.1-8B-Instruct%7D%20on%20MATH%20by%20an%20absolute%2015.9%5C%25%20%2851.9%5C%25%0A%24%5Crightarrow%24%2067.8%5C%25%29.%20Finally%2C%20to%20accelerate%20the%20open-source%20efforts%2C%20we%0Arelease%20the%20code%2C%20the%20finetuned%20models%2C%20and%20the%20OpenMathInstruct-2%20dataset%0Aunder%20a%20commercially%20permissive%20license.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2410.01560v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DOpenMathInstruct-2%253A%2520Accelerating%2520AI%2520for%2520Math%2520with%2520Massive%2520Open-Source%250A%2520%2520Instruction%2520Data%26entry.906535625%3DShubham%2520Toshniwal%2520and%2520Wei%2520Du%2520and%2520Ivan%2520Moshkov%2520and%2520Branislav%2520Kisacanin%2520and%2520Alexan%2520Ayrapetyan%2520and%2520Igor%2520Gitman%26entry.1292438233%3D%2520%2520Mathematical%2520reasoning%2520continues%2520to%2520be%2520a%2520critical%2520challenge%2520in%2520large%2520language%250Amodel%2520%2528LLM%2529%2520development%2520with%2520significant%2520interest.%2520However%252C%2520most%2520of%2520the%250Acutting-edge%2520progress%2520in%2520mathematical%2520reasoning%2520with%2520LLMs%2520has%2520become%250A%255Cemph%257Bclosed-source%257D%2520due%2520to%2520lack%2520of%2520access%2520to%2520training%2520data.%2520This%2520lack%2520of%2520data%250Aaccess%2520limits%2520researchers%2520from%2520understanding%2520the%2520impact%2520of%2520different%2520choices%250Afor%2520synthesizing%2520and%2520utilizing%2520the%2520data.%2520With%2520the%2520goal%2520of%2520creating%2520a%250Ahigh-quality%2520finetuning%2520%2528SFT%2529%2520dataset%2520for%2520math%2520reasoning%252C%2520we%2520conduct%2520careful%250Aablation%2520experiments%2520on%2520data%2520synthesis%2520using%2520the%2520recently%2520released%250A%255Ctexttt%257BLlama3.1%257D%2520family%2520of%2520models.%2520Our%2520experiments%2520show%2520that%253A%2520%2528a%2529%2520solution%250Aformat%2520matters%252C%2520with%2520excessively%2520verbose%2520solutions%2520proving%2520detrimental%2520to%2520SFT%250Aperformance%252C%2520%2528b%2529%2520data%2520generated%2520by%2520a%2520strong%2520teacher%2520outperforms%250A%255Cemph%257Bon-policy%257D%2520data%2520generated%2520by%2520a%2520weak%2520student%2520model%252C%2520%2528c%2529%2520SFT%2520is%2520robust%2520to%250Alow-quality%2520solutions%252C%2520allowing%2520for%2520imprecise%2520data%2520filtering%252C%2520and%2520%2528d%2529%2520question%250Adiversity%2520is%2520crucial%2520for%2520achieving%2520data%2520scaling%2520gains.%2520Based%2520on%2520these%2520insights%252C%250Awe%2520create%2520the%2520OpenMathInstruct-2%2520dataset%252C%2520which%2520consists%2520of%252014M%250Aquestion-solution%2520pairs%2520%2528%2524%255Capprox%2524%2520600K%2520unique%2520questions%2529%252C%2520making%2520it%2520nearly%250Aeight%2520times%2520larger%2520than%2520the%2520previous%2520largest%2520open-source%2520math%2520reasoning%250Adataset.%2520Finetuning%2520the%2520%255Ctexttt%257BLlama-3.1-8B-Base%257D%2520using%2520OpenMathInstruct-2%250Aoutperforms%2520%255Ctexttt%257BLlama3.1-8B-Instruct%257D%2520on%2520MATH%2520by%2520an%2520absolute%252015.9%255C%2525%2520%252851.9%255C%2525%250A%2524%255Crightarrow%2524%252067.8%255C%2525%2529.%2520Finally%252C%2520to%2520accelerate%2520the%2520open-source%2520efforts%252C%2520we%250Arelease%2520the%2520code%252C%2520the%2520finetuned%2520models%252C%2520and%2520the%2520OpenMathInstruct-2%2520dataset%250Aunder%2520a%2520commercially%2520permissive%2520license.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2410.01560v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=OpenMathInstruct-2%3A%20Accelerating%20AI%20for%20Math%20with%20Massive%20Open-Source%0A%20%20Instruction%20Data&entry.906535625=Shubham%20Toshniwal%20and%20Wei%20Du%20and%20Ivan%20Moshkov%20and%20Branislav%20Kisacanin%20and%20Alexan%20Ayrapetyan%20and%20Igor%20Gitman&entry.1292438233=%20%20Mathematical%20reasoning%20continues%20to%20be%20a%20critical%20challenge%20in%20large%20language%0Amodel%20%28LLM%29%20development%20with%20significant%20interest.%20However%2C%20most%20of%20the%0Acutting-edge%20progress%20in%20mathematical%20reasoning%20with%20LLMs%20has%20become%0A%5Cemph%7Bclosed-source%7D%20due%20to%20lack%20of%20access%20to%20training%20data.%20This%20lack%20of%20data%0Aaccess%20limits%20researchers%20from%20understanding%20the%20impact%20of%20different%20choices%0Afor%20synthesizing%20and%20utilizing%20the%20data.%20With%20the%20goal%20of%20creating%20a%0Ahigh-quality%20finetuning%20%28SFT%29%20dataset%20for%20math%20reasoning%2C%20we%20conduct%20careful%0Aablation%20experiments%20on%20data%20synthesis%20using%20the%20recently%20released%0A%5Ctexttt%7BLlama3.1%7D%20family%20of%20models.%20Our%20experiments%20show%20that%3A%20%28a%29%20solution%0Aformat%20matters%2C%20with%20excessively%20verbose%20solutions%20proving%20detrimental%20to%20SFT%0Aperformance%2C%20%28b%29%20data%20generated%20by%20a%20strong%20teacher%20outperforms%0A%5Cemph%7Bon-policy%7D%20data%20generated%20by%20a%20weak%20student%20model%2C%20%28c%29%20SFT%20is%20robust%20to%0Alow-quality%20solutions%2C%20allowing%20for%20imprecise%20data%20filtering%2C%20and%20%28d%29%20question%0Adiversity%20is%20crucial%20for%20achieving%20data%20scaling%20gains.%20Based%20on%20these%20insights%2C%0Awe%20create%20the%20OpenMathInstruct-2%20dataset%2C%20which%20consists%20of%2014M%0Aquestion-solution%20pairs%20%28%24%5Capprox%24%20600K%20unique%20questions%29%2C%20making%20it%20nearly%0Aeight%20times%20larger%20than%20the%20previous%20largest%20open-source%20math%20reasoning%0Adataset.%20Finetuning%20the%20%5Ctexttt%7BLlama-3.1-8B-Base%7D%20using%20OpenMathInstruct-2%0Aoutperforms%20%5Ctexttt%7BLlama3.1-8B-Instruct%7D%20on%20MATH%20by%20an%20absolute%2015.9%5C%25%20%2851.9%5C%25%0A%24%5Crightarrow%24%2067.8%5C%25%29.%20Finally%2C%20to%20accelerate%20the%20open-source%20efforts%2C%20we%0Arelease%20the%20code%2C%20the%20finetuned%20models%2C%20and%20the%20OpenMathInstruct-2%20dataset%0Aunder%20a%20commercially%20permissive%20license.%0A&entry.1838667208=http%3A//arxiv.org/abs/2410.01560v1&entry.124074799=Read"},
{"title": "DreamCatalyst: Fast and High-Quality 3D Editing via Controlling\n  Editability and Identity Preservation", "author": "Jiwook Kim and Seonho Lee and Jaeyo Shin and Jiho Choi and Hyunjung Shim", "abstract": "  Score distillation sampling (SDS) has emerged as an effective framework in\ntext-driven 3D editing tasks, leveraging diffusion models for 3D consistent\nediting. However, existing SDS-based 3D editing methods suffer from long\ntraining times and produce low-quality results. We identify that the root cause\nof this performance degradation is their conflict with the sampling dynamics of\ndiffusion models. Addressing this conflict allows us to treat SDS as a\ndiffusion reverse process for 3D editing via sampling from data space. In\ncontrast, existing methods naively distill the score function using diffusion\nmodels. From these insights, we propose DreamCatalyst, a novel framework that\nconsiders these sampling dynamics in the SDS framework. Specifically, we devise\nthe optimization process of our DreamCatalyst to approximate the diffusion\nreverse process in editing tasks, thereby aligning with diffusion sampling\ndynamics. As a result, DreamCatalyst successfully reduces training time and\nimproves editing quality. Our method offers two modes: (1) a fast mode that\nedits Neural Radiance Fields (NeRF) scenes approximately 23 times faster than\ncurrent state-of-the-art NeRF editing methods, and (2) a high-quality mode that\nproduces superior results about 8 times faster than these methods. Notably, our\nhigh-quality mode outperforms current state-of-the-art NeRF editing methods in\nterms of both speed and quality. DreamCatalyst also surpasses the\nstate-of-the-art 3D Gaussian Splatting (3DGS) editing methods, establishing\nitself as an effective and model-agnostic 3D editing solution. See more\nextensive results on our project page: https://dream-catalyst.github.io.\n", "link": "http://arxiv.org/abs/2407.11394v2", "date": "2024-10-02", "relevancy": 2.4207, "topK": [{"title": "CAT3D: Create Anything in 3D with Multi-View Diffusion Models", "link": "http://arxiv.org/abs/2405.10314v1", "similarity": 0.608}, {"title": "CAT3D: Create Anything in 3D with Multi-View Diffusion Models", "link": "http://arxiv.org/abs/2405.10314v1", "similarity": 0.608}, {"title": "SDXS: Real-Time One-Step Latent Diffusion Models with Image Conditions", "link": "http://arxiv.org/abs/2403.16627v1", "similarity": 0.591}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20DreamCatalyst%3A%20Fast%20and%20High-Quality%203D%20Editing%20via%20Controlling%0A%20%20Editability%20and%20Identity%20Preservation&body=Title%3A%20DreamCatalyst%3A%20Fast%20and%20High-Quality%203D%20Editing%20via%20Controlling%0A%20%20Editability%20and%20Identity%20Preservation%0AAuthor%3A%20Jiwook%20Kim%20and%20Seonho%20Lee%20and%20Jaeyo%20Shin%20and%20Jiho%20Choi%20and%20Hyunjung%20Shim%0AAbstract%3A%20%20%20Score%20distillation%20sampling%20%28SDS%29%20has%20emerged%20as%20an%20effective%20framework%20in%0Atext-driven%203D%20editing%20tasks%2C%20leveraging%20diffusion%20models%20for%203D%20consistent%0Aediting.%20However%2C%20existing%20SDS-based%203D%20editing%20methods%20suffer%20from%20long%0Atraining%20times%20and%20produce%20low-quality%20results.%20We%20identify%20that%20the%20root%20cause%0Aof%20this%20performance%20degradation%20is%20their%20conflict%20with%20the%20sampling%20dynamics%20of%0Adiffusion%20models.%20Addressing%20this%20conflict%20allows%20us%20to%20treat%20SDS%20as%20a%0Adiffusion%20reverse%20process%20for%203D%20editing%20via%20sampling%20from%20data%20space.%20In%0Acontrast%2C%20existing%20methods%20naively%20distill%20the%20score%20function%20using%20diffusion%0Amodels.%20From%20these%20insights%2C%20we%20propose%20DreamCatalyst%2C%20a%20novel%20framework%20that%0Aconsiders%20these%20sampling%20dynamics%20in%20the%20SDS%20framework.%20Specifically%2C%20we%20devise%0Athe%20optimization%20process%20of%20our%20DreamCatalyst%20to%20approximate%20the%20diffusion%0Areverse%20process%20in%20editing%20tasks%2C%20thereby%20aligning%20with%20diffusion%20sampling%0Adynamics.%20As%20a%20result%2C%20DreamCatalyst%20successfully%20reduces%20training%20time%20and%0Aimproves%20editing%20quality.%20Our%20method%20offers%20two%20modes%3A%20%281%29%20a%20fast%20mode%20that%0Aedits%20Neural%20Radiance%20Fields%20%28NeRF%29%20scenes%20approximately%2023%20times%20faster%20than%0Acurrent%20state-of-the-art%20NeRF%20editing%20methods%2C%20and%20%282%29%20a%20high-quality%20mode%20that%0Aproduces%20superior%20results%20about%208%20times%20faster%20than%20these%20methods.%20Notably%2C%20our%0Ahigh-quality%20mode%20outperforms%20current%20state-of-the-art%20NeRF%20editing%20methods%20in%0Aterms%20of%20both%20speed%20and%20quality.%20DreamCatalyst%20also%20surpasses%20the%0Astate-of-the-art%203D%20Gaussian%20Splatting%20%283DGS%29%20editing%20methods%2C%20establishing%0Aitself%20as%20an%20effective%20and%20model-agnostic%203D%20editing%20solution.%20See%20more%0Aextensive%20results%20on%20our%20project%20page%3A%20https%3A//dream-catalyst.github.io.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2407.11394v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DDreamCatalyst%253A%2520Fast%2520and%2520High-Quality%25203D%2520Editing%2520via%2520Controlling%250A%2520%2520Editability%2520and%2520Identity%2520Preservation%26entry.906535625%3DJiwook%2520Kim%2520and%2520Seonho%2520Lee%2520and%2520Jaeyo%2520Shin%2520and%2520Jiho%2520Choi%2520and%2520Hyunjung%2520Shim%26entry.1292438233%3D%2520%2520Score%2520distillation%2520sampling%2520%2528SDS%2529%2520has%2520emerged%2520as%2520an%2520effective%2520framework%2520in%250Atext-driven%25203D%2520editing%2520tasks%252C%2520leveraging%2520diffusion%2520models%2520for%25203D%2520consistent%250Aediting.%2520However%252C%2520existing%2520SDS-based%25203D%2520editing%2520methods%2520suffer%2520from%2520long%250Atraining%2520times%2520and%2520produce%2520low-quality%2520results.%2520We%2520identify%2520that%2520the%2520root%2520cause%250Aof%2520this%2520performance%2520degradation%2520is%2520their%2520conflict%2520with%2520the%2520sampling%2520dynamics%2520of%250Adiffusion%2520models.%2520Addressing%2520this%2520conflict%2520allows%2520us%2520to%2520treat%2520SDS%2520as%2520a%250Adiffusion%2520reverse%2520process%2520for%25203D%2520editing%2520via%2520sampling%2520from%2520data%2520space.%2520In%250Acontrast%252C%2520existing%2520methods%2520naively%2520distill%2520the%2520score%2520function%2520using%2520diffusion%250Amodels.%2520From%2520these%2520insights%252C%2520we%2520propose%2520DreamCatalyst%252C%2520a%2520novel%2520framework%2520that%250Aconsiders%2520these%2520sampling%2520dynamics%2520in%2520the%2520SDS%2520framework.%2520Specifically%252C%2520we%2520devise%250Athe%2520optimization%2520process%2520of%2520our%2520DreamCatalyst%2520to%2520approximate%2520the%2520diffusion%250Areverse%2520process%2520in%2520editing%2520tasks%252C%2520thereby%2520aligning%2520with%2520diffusion%2520sampling%250Adynamics.%2520As%2520a%2520result%252C%2520DreamCatalyst%2520successfully%2520reduces%2520training%2520time%2520and%250Aimproves%2520editing%2520quality.%2520Our%2520method%2520offers%2520two%2520modes%253A%2520%25281%2529%2520a%2520fast%2520mode%2520that%250Aedits%2520Neural%2520Radiance%2520Fields%2520%2528NeRF%2529%2520scenes%2520approximately%252023%2520times%2520faster%2520than%250Acurrent%2520state-of-the-art%2520NeRF%2520editing%2520methods%252C%2520and%2520%25282%2529%2520a%2520high-quality%2520mode%2520that%250Aproduces%2520superior%2520results%2520about%25208%2520times%2520faster%2520than%2520these%2520methods.%2520Notably%252C%2520our%250Ahigh-quality%2520mode%2520outperforms%2520current%2520state-of-the-art%2520NeRF%2520editing%2520methods%2520in%250Aterms%2520of%2520both%2520speed%2520and%2520quality.%2520DreamCatalyst%2520also%2520surpasses%2520the%250Astate-of-the-art%25203D%2520Gaussian%2520Splatting%2520%25283DGS%2529%2520editing%2520methods%252C%2520establishing%250Aitself%2520as%2520an%2520effective%2520and%2520model-agnostic%25203D%2520editing%2520solution.%2520See%2520more%250Aextensive%2520results%2520on%2520our%2520project%2520page%253A%2520https%253A//dream-catalyst.github.io.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2407.11394v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=DreamCatalyst%3A%20Fast%20and%20High-Quality%203D%20Editing%20via%20Controlling%0A%20%20Editability%20and%20Identity%20Preservation&entry.906535625=Jiwook%20Kim%20and%20Seonho%20Lee%20and%20Jaeyo%20Shin%20and%20Jiho%20Choi%20and%20Hyunjung%20Shim&entry.1292438233=%20%20Score%20distillation%20sampling%20%28SDS%29%20has%20emerged%20as%20an%20effective%20framework%20in%0Atext-driven%203D%20editing%20tasks%2C%20leveraging%20diffusion%20models%20for%203D%20consistent%0Aediting.%20However%2C%20existing%20SDS-based%203D%20editing%20methods%20suffer%20from%20long%0Atraining%20times%20and%20produce%20low-quality%20results.%20We%20identify%20that%20the%20root%20cause%0Aof%20this%20performance%20degradation%20is%20their%20conflict%20with%20the%20sampling%20dynamics%20of%0Adiffusion%20models.%20Addressing%20this%20conflict%20allows%20us%20to%20treat%20SDS%20as%20a%0Adiffusion%20reverse%20process%20for%203D%20editing%20via%20sampling%20from%20data%20space.%20In%0Acontrast%2C%20existing%20methods%20naively%20distill%20the%20score%20function%20using%20diffusion%0Amodels.%20From%20these%20insights%2C%20we%20propose%20DreamCatalyst%2C%20a%20novel%20framework%20that%0Aconsiders%20these%20sampling%20dynamics%20in%20the%20SDS%20framework.%20Specifically%2C%20we%20devise%0Athe%20optimization%20process%20of%20our%20DreamCatalyst%20to%20approximate%20the%20diffusion%0Areverse%20process%20in%20editing%20tasks%2C%20thereby%20aligning%20with%20diffusion%20sampling%0Adynamics.%20As%20a%20result%2C%20DreamCatalyst%20successfully%20reduces%20training%20time%20and%0Aimproves%20editing%20quality.%20Our%20method%20offers%20two%20modes%3A%20%281%29%20a%20fast%20mode%20that%0Aedits%20Neural%20Radiance%20Fields%20%28NeRF%29%20scenes%20approximately%2023%20times%20faster%20than%0Acurrent%20state-of-the-art%20NeRF%20editing%20methods%2C%20and%20%282%29%20a%20high-quality%20mode%20that%0Aproduces%20superior%20results%20about%208%20times%20faster%20than%20these%20methods.%20Notably%2C%20our%0Ahigh-quality%20mode%20outperforms%20current%20state-of-the-art%20NeRF%20editing%20methods%20in%0Aterms%20of%20both%20speed%20and%20quality.%20DreamCatalyst%20also%20surpasses%20the%0Astate-of-the-art%203D%20Gaussian%20Splatting%20%283DGS%29%20editing%20methods%2C%20establishing%0Aitself%20as%20an%20effective%20and%20model-agnostic%203D%20editing%20solution.%20See%20more%0Aextensive%20results%20on%20our%20project%20page%3A%20https%3A//dream-catalyst.github.io.%0A&entry.1838667208=http%3A//arxiv.org/abs/2407.11394v2&entry.124074799=Read"},
{"title": "Trying to be human: Linguistic traces of stochastic empathy in language\n  models", "author": "Bennett Kleinberg and Jari Zegers and Jonas Festor and Stefana Vida and Julian Pr\u00e4sent and Riccardo Loconte and Sanne Peereboom", "abstract": "  Differentiating between generated and human-written content is important for\nnavigating the modern world. Large language models (LLMs) are crucial drivers\nbehind the increased quality of computer-generated content. Reportedly, humans\nfind it increasingly difficult to identify whether an AI model generated a\npiece of text. Our work tests how two important factors contribute to the human\nvs AI race: empathy and an incentive to appear human. We address both aspects\nin two experiments: human participants and a state-of-the-art LLM wrote\nrelationship advice (Study 1, n=530) or mere descriptions (Study 2, n=610),\neither instructed to be as human as possible or not. New samples of humans\n(n=428 and n=408) then judged the texts' source. Our findings show that when\nempathy is required, humans excel. Contrary to expectations, instructions to\nappear human were only effective for the LLM, so the human advantage\ndiminished. Computational text analysis revealed that LLMs become more human\nbecause they may have an implicit representation of what makes a text human and\neffortlessly apply these heuristics. The model resorts to a conversational,\nself-referential, informal tone with a simpler vocabulary to mimic stochastic\nempathy. We discuss these findings in light of recent claims on the on-par\nperformance of LLMs.\n", "link": "http://arxiv.org/abs/2410.01675v1", "date": "2024-10-02", "relevancy": 2.4199, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.4889}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.4889}, {"title": "DressCode: Autoregressively Sewing and Generating Garments from Text\n  Guidance", "link": "http://arxiv.org/abs/2401.16465v3", "similarity": 0.4742}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Trying%20to%20be%20human%3A%20Linguistic%20traces%20of%20stochastic%20empathy%20in%20language%0A%20%20models&body=Title%3A%20Trying%20to%20be%20human%3A%20Linguistic%20traces%20of%20stochastic%20empathy%20in%20language%0A%20%20models%0AAuthor%3A%20Bennett%20Kleinberg%20and%20Jari%20Zegers%20and%20Jonas%20Festor%20and%20Stefana%20Vida%20and%20Julian%20Pr%C3%A4sent%20and%20Riccardo%20Loconte%20and%20Sanne%20Peereboom%0AAbstract%3A%20%20%20Differentiating%20between%20generated%20and%20human-written%20content%20is%20important%20for%0Anavigating%20the%20modern%20world.%20Large%20language%20models%20%28LLMs%29%20are%20crucial%20drivers%0Abehind%20the%20increased%20quality%20of%20computer-generated%20content.%20Reportedly%2C%20humans%0Afind%20it%20increasingly%20difficult%20to%20identify%20whether%20an%20AI%20model%20generated%20a%0Apiece%20of%20text.%20Our%20work%20tests%20how%20two%20important%20factors%20contribute%20to%20the%20human%0Avs%20AI%20race%3A%20empathy%20and%20an%20incentive%20to%20appear%20human.%20We%20address%20both%20aspects%0Ain%20two%20experiments%3A%20human%20participants%20and%20a%20state-of-the-art%20LLM%20wrote%0Arelationship%20advice%20%28Study%201%2C%20n%3D530%29%20or%20mere%20descriptions%20%28Study%202%2C%20n%3D610%29%2C%0Aeither%20instructed%20to%20be%20as%20human%20as%20possible%20or%20not.%20New%20samples%20of%20humans%0A%28n%3D428%20and%20n%3D408%29%20then%20judged%20the%20texts%27%20source.%20Our%20findings%20show%20that%20when%0Aempathy%20is%20required%2C%20humans%20excel.%20Contrary%20to%20expectations%2C%20instructions%20to%0Aappear%20human%20were%20only%20effective%20for%20the%20LLM%2C%20so%20the%20human%20advantage%0Adiminished.%20Computational%20text%20analysis%20revealed%20that%20LLMs%20become%20more%20human%0Abecause%20they%20may%20have%20an%20implicit%20representation%20of%20what%20makes%20a%20text%20human%20and%0Aeffortlessly%20apply%20these%20heuristics.%20The%20model%20resorts%20to%20a%20conversational%2C%0Aself-referential%2C%20informal%20tone%20with%20a%20simpler%20vocabulary%20to%20mimic%20stochastic%0Aempathy.%20We%20discuss%20these%20findings%20in%20light%20of%20recent%20claims%20on%20the%20on-par%0Aperformance%20of%20LLMs.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2410.01675v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DTrying%2520to%2520be%2520human%253A%2520Linguistic%2520traces%2520of%2520stochastic%2520empathy%2520in%2520language%250A%2520%2520models%26entry.906535625%3DBennett%2520Kleinberg%2520and%2520Jari%2520Zegers%2520and%2520Jonas%2520Festor%2520and%2520Stefana%2520Vida%2520and%2520Julian%2520Pr%25C3%25A4sent%2520and%2520Riccardo%2520Loconte%2520and%2520Sanne%2520Peereboom%26entry.1292438233%3D%2520%2520Differentiating%2520between%2520generated%2520and%2520human-written%2520content%2520is%2520important%2520for%250Anavigating%2520the%2520modern%2520world.%2520Large%2520language%2520models%2520%2528LLMs%2529%2520are%2520crucial%2520drivers%250Abehind%2520the%2520increased%2520quality%2520of%2520computer-generated%2520content.%2520Reportedly%252C%2520humans%250Afind%2520it%2520increasingly%2520difficult%2520to%2520identify%2520whether%2520an%2520AI%2520model%2520generated%2520a%250Apiece%2520of%2520text.%2520Our%2520work%2520tests%2520how%2520two%2520important%2520factors%2520contribute%2520to%2520the%2520human%250Avs%2520AI%2520race%253A%2520empathy%2520and%2520an%2520incentive%2520to%2520appear%2520human.%2520We%2520address%2520both%2520aspects%250Ain%2520two%2520experiments%253A%2520human%2520participants%2520and%2520a%2520state-of-the-art%2520LLM%2520wrote%250Arelationship%2520advice%2520%2528Study%25201%252C%2520n%253D530%2529%2520or%2520mere%2520descriptions%2520%2528Study%25202%252C%2520n%253D610%2529%252C%250Aeither%2520instructed%2520to%2520be%2520as%2520human%2520as%2520possible%2520or%2520not.%2520New%2520samples%2520of%2520humans%250A%2528n%253D428%2520and%2520n%253D408%2529%2520then%2520judged%2520the%2520texts%2527%2520source.%2520Our%2520findings%2520show%2520that%2520when%250Aempathy%2520is%2520required%252C%2520humans%2520excel.%2520Contrary%2520to%2520expectations%252C%2520instructions%2520to%250Aappear%2520human%2520were%2520only%2520effective%2520for%2520the%2520LLM%252C%2520so%2520the%2520human%2520advantage%250Adiminished.%2520Computational%2520text%2520analysis%2520revealed%2520that%2520LLMs%2520become%2520more%2520human%250Abecause%2520they%2520may%2520have%2520an%2520implicit%2520representation%2520of%2520what%2520makes%2520a%2520text%2520human%2520and%250Aeffortlessly%2520apply%2520these%2520heuristics.%2520The%2520model%2520resorts%2520to%2520a%2520conversational%252C%250Aself-referential%252C%2520informal%2520tone%2520with%2520a%2520simpler%2520vocabulary%2520to%2520mimic%2520stochastic%250Aempathy.%2520We%2520discuss%2520these%2520findings%2520in%2520light%2520of%2520recent%2520claims%2520on%2520the%2520on-par%250Aperformance%2520of%2520LLMs.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2410.01675v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Trying%20to%20be%20human%3A%20Linguistic%20traces%20of%20stochastic%20empathy%20in%20language%0A%20%20models&entry.906535625=Bennett%20Kleinberg%20and%20Jari%20Zegers%20and%20Jonas%20Festor%20and%20Stefana%20Vida%20and%20Julian%20Pr%C3%A4sent%20and%20Riccardo%20Loconte%20and%20Sanne%20Peereboom&entry.1292438233=%20%20Differentiating%20between%20generated%20and%20human-written%20content%20is%20important%20for%0Anavigating%20the%20modern%20world.%20Large%20language%20models%20%28LLMs%29%20are%20crucial%20drivers%0Abehind%20the%20increased%20quality%20of%20computer-generated%20content.%20Reportedly%2C%20humans%0Afind%20it%20increasingly%20difficult%20to%20identify%20whether%20an%20AI%20model%20generated%20a%0Apiece%20of%20text.%20Our%20work%20tests%20how%20two%20important%20factors%20contribute%20to%20the%20human%0Avs%20AI%20race%3A%20empathy%20and%20an%20incentive%20to%20appear%20human.%20We%20address%20both%20aspects%0Ain%20two%20experiments%3A%20human%20participants%20and%20a%20state-of-the-art%20LLM%20wrote%0Arelationship%20advice%20%28Study%201%2C%20n%3D530%29%20or%20mere%20descriptions%20%28Study%202%2C%20n%3D610%29%2C%0Aeither%20instructed%20to%20be%20as%20human%20as%20possible%20or%20not.%20New%20samples%20of%20humans%0A%28n%3D428%20and%20n%3D408%29%20then%20judged%20the%20texts%27%20source.%20Our%20findings%20show%20that%20when%0Aempathy%20is%20required%2C%20humans%20excel.%20Contrary%20to%20expectations%2C%20instructions%20to%0Aappear%20human%20were%20only%20effective%20for%20the%20LLM%2C%20so%20the%20human%20advantage%0Adiminished.%20Computational%20text%20analysis%20revealed%20that%20LLMs%20become%20more%20human%0Abecause%20they%20may%20have%20an%20implicit%20representation%20of%20what%20makes%20a%20text%20human%20and%0Aeffortlessly%20apply%20these%20heuristics.%20The%20model%20resorts%20to%20a%20conversational%2C%0Aself-referential%2C%20informal%20tone%20with%20a%20simpler%20vocabulary%20to%20mimic%20stochastic%0Aempathy.%20We%20discuss%20these%20findings%20in%20light%20of%20recent%20claims%20on%20the%20on-par%0Aperformance%20of%20LLMs.%0A&entry.1838667208=http%3A//arxiv.org/abs/2410.01675v1&entry.124074799=Read"},
{"title": "ENTP: Encoder-only Next Token Prediction", "author": "Ethan Ewer and Daewon Chae and Thomas Zeng and Jinkyu Kim and Kangwook Lee", "abstract": "  Next-token prediction models have predominantly relied on decoder-only\nTransformers with causal attention, driven by the common belief that causal\nattention is essential to prevent \"cheating\" by masking future tokens. We\nchallenge this widely accepted notion and argue that this design choice is\nabout efficiency rather than necessity. While decoder-only Transformers are\nstill a good choice for practical reasons, they are not the only viable option.\nIn this work, we introduce Encoder-only Next Token Prediction (ENTP). We\nexplore the differences between ENTP and decoder-only Transformers in\nexpressive power and complexity, highlighting potential advantages of ENTP. We\nintroduce the Triplet-Counting task and show, both theoretically and\nexperimentally, that while ENTP can perform this task easily, a decoder-only\nTransformer cannot. Finally, we empirically demonstrate ENTP's superior\nperformance across various realistic tasks, such as length generalization and\nin-context learning.\n", "link": "http://arxiv.org/abs/2410.01600v1", "date": "2024-10-02", "relevancy": 2.4189, "topK": [{"title": "Genie: Generative Interactive Environments", "link": "http://arxiv.org/abs/2402.15391v1", "similarity": 0.4895}, {"title": "Boosted dynamic neural networks", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/26302", "similarity": 0.4866}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.4752}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20ENTP%3A%20Encoder-only%20Next%20Token%20Prediction&body=Title%3A%20ENTP%3A%20Encoder-only%20Next%20Token%20Prediction%0AAuthor%3A%20Ethan%20Ewer%20and%20Daewon%20Chae%20and%20Thomas%20Zeng%20and%20Jinkyu%20Kim%20and%20Kangwook%20Lee%0AAbstract%3A%20%20%20Next-token%20prediction%20models%20have%20predominantly%20relied%20on%20decoder-only%0ATransformers%20with%20causal%20attention%2C%20driven%20by%20the%20common%20belief%20that%20causal%0Aattention%20is%20essential%20to%20prevent%20%22cheating%22%20by%20masking%20future%20tokens.%20We%0Achallenge%20this%20widely%20accepted%20notion%20and%20argue%20that%20this%20design%20choice%20is%0Aabout%20efficiency%20rather%20than%20necessity.%20While%20decoder-only%20Transformers%20are%0Astill%20a%20good%20choice%20for%20practical%20reasons%2C%20they%20are%20not%20the%20only%20viable%20option.%0AIn%20this%20work%2C%20we%20introduce%20Encoder-only%20Next%20Token%20Prediction%20%28ENTP%29.%20We%0Aexplore%20the%20differences%20between%20ENTP%20and%20decoder-only%20Transformers%20in%0Aexpressive%20power%20and%20complexity%2C%20highlighting%20potential%20advantages%20of%20ENTP.%20We%0Aintroduce%20the%20Triplet-Counting%20task%20and%20show%2C%20both%20theoretically%20and%0Aexperimentally%2C%20that%20while%20ENTP%20can%20perform%20this%20task%20easily%2C%20a%20decoder-only%0ATransformer%20cannot.%20Finally%2C%20we%20empirically%20demonstrate%20ENTP%27s%20superior%0Aperformance%20across%20various%20realistic%20tasks%2C%20such%20as%20length%20generalization%20and%0Ain-context%20learning.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2410.01600v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DENTP%253A%2520Encoder-only%2520Next%2520Token%2520Prediction%26entry.906535625%3DEthan%2520Ewer%2520and%2520Daewon%2520Chae%2520and%2520Thomas%2520Zeng%2520and%2520Jinkyu%2520Kim%2520and%2520Kangwook%2520Lee%26entry.1292438233%3D%2520%2520Next-token%2520prediction%2520models%2520have%2520predominantly%2520relied%2520on%2520decoder-only%250ATransformers%2520with%2520causal%2520attention%252C%2520driven%2520by%2520the%2520common%2520belief%2520that%2520causal%250Aattention%2520is%2520essential%2520to%2520prevent%2520%2522cheating%2522%2520by%2520masking%2520future%2520tokens.%2520We%250Achallenge%2520this%2520widely%2520accepted%2520notion%2520and%2520argue%2520that%2520this%2520design%2520choice%2520is%250Aabout%2520efficiency%2520rather%2520than%2520necessity.%2520While%2520decoder-only%2520Transformers%2520are%250Astill%2520a%2520good%2520choice%2520for%2520practical%2520reasons%252C%2520they%2520are%2520not%2520the%2520only%2520viable%2520option.%250AIn%2520this%2520work%252C%2520we%2520introduce%2520Encoder-only%2520Next%2520Token%2520Prediction%2520%2528ENTP%2529.%2520We%250Aexplore%2520the%2520differences%2520between%2520ENTP%2520and%2520decoder-only%2520Transformers%2520in%250Aexpressive%2520power%2520and%2520complexity%252C%2520highlighting%2520potential%2520advantages%2520of%2520ENTP.%2520We%250Aintroduce%2520the%2520Triplet-Counting%2520task%2520and%2520show%252C%2520both%2520theoretically%2520and%250Aexperimentally%252C%2520that%2520while%2520ENTP%2520can%2520perform%2520this%2520task%2520easily%252C%2520a%2520decoder-only%250ATransformer%2520cannot.%2520Finally%252C%2520we%2520empirically%2520demonstrate%2520ENTP%2527s%2520superior%250Aperformance%2520across%2520various%2520realistic%2520tasks%252C%2520such%2520as%2520length%2520generalization%2520and%250Ain-context%2520learning.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2410.01600v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=ENTP%3A%20Encoder-only%20Next%20Token%20Prediction&entry.906535625=Ethan%20Ewer%20and%20Daewon%20Chae%20and%20Thomas%20Zeng%20and%20Jinkyu%20Kim%20and%20Kangwook%20Lee&entry.1292438233=%20%20Next-token%20prediction%20models%20have%20predominantly%20relied%20on%20decoder-only%0ATransformers%20with%20causal%20attention%2C%20driven%20by%20the%20common%20belief%20that%20causal%0Aattention%20is%20essential%20to%20prevent%20%22cheating%22%20by%20masking%20future%20tokens.%20We%0Achallenge%20this%20widely%20accepted%20notion%20and%20argue%20that%20this%20design%20choice%20is%0Aabout%20efficiency%20rather%20than%20necessity.%20While%20decoder-only%20Transformers%20are%0Astill%20a%20good%20choice%20for%20practical%20reasons%2C%20they%20are%20not%20the%20only%20viable%20option.%0AIn%20this%20work%2C%20we%20introduce%20Encoder-only%20Next%20Token%20Prediction%20%28ENTP%29.%20We%0Aexplore%20the%20differences%20between%20ENTP%20and%20decoder-only%20Transformers%20in%0Aexpressive%20power%20and%20complexity%2C%20highlighting%20potential%20advantages%20of%20ENTP.%20We%0Aintroduce%20the%20Triplet-Counting%20task%20and%20show%2C%20both%20theoretically%20and%0Aexperimentally%2C%20that%20while%20ENTP%20can%20perform%20this%20task%20easily%2C%20a%20decoder-only%0ATransformer%20cannot.%20Finally%2C%20we%20empirically%20demonstrate%20ENTP%27s%20superior%0Aperformance%20across%20various%20realistic%20tasks%2C%20such%20as%20length%20generalization%20and%0Ain-context%20learning.%0A&entry.1838667208=http%3A//arxiv.org/abs/2410.01600v1&entry.124074799=Read"},
{"title": "Quo Vadis RankList-based System in Face Recognition?", "author": "Xinyi Zhang and Manuel G\u00fcnther", "abstract": "  Face recognition in the wild has gained a lot of focus in the last few years,\nand many face recognition models are designed to verify faces in medium-quality\nimages. Especially due to the availability of large training datasets with\nsimilar conditions, deep face recognition models perform exceptionally well in\nsuch tasks. However, in other tasks where substantially less training data is\navailable, such methods struggle, especially when required to compare\nhigh-quality enrollment images with low-quality probes. On the other hand,\ntraditional RankList-based methods have been developed that compare faces\nindirectly by comparing to cohort faces with similar conditions. In this paper,\nwe revisit these RankList methods and extend them to use the logits of the\nstate-of-the-art DaliFace network, instead of an external cohort. We show that\nthrough a reasonable Logit-Cohort Selection (LoCoS) the performance of\nRankList-based functions can be improved drastically. Experiments on two\nchallenging face recognition datasets not only demonstrate the enhanced\nperformance of our proposed method but also set the stage for future\nadvancements in handling diverse image qualities.\n", "link": "http://arxiv.org/abs/2410.01498v1", "date": "2024-10-02", "relevancy": 2.4099, "topK": [{"title": "Flexible Visual Recognition by Evidential Modeling of Confusion and Ignorance", "link": "http://openaccess.thecvf.com/content/ICCV2023/html/Fan_Flexible_Visual_Recognition_by_Evidential_Modeling_of_Confusion_and_Ignorance_ICCV_2023_paper.html", "similarity": 0.499}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.4735}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.4735}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Quo%20Vadis%20RankList-based%20System%20in%20Face%20Recognition%3F&body=Title%3A%20Quo%20Vadis%20RankList-based%20System%20in%20Face%20Recognition%3F%0AAuthor%3A%20Xinyi%20Zhang%20and%20Manuel%20G%C3%BCnther%0AAbstract%3A%20%20%20Face%20recognition%20in%20the%20wild%20has%20gained%20a%20lot%20of%20focus%20in%20the%20last%20few%20years%2C%0Aand%20many%20face%20recognition%20models%20are%20designed%20to%20verify%20faces%20in%20medium-quality%0Aimages.%20Especially%20due%20to%20the%20availability%20of%20large%20training%20datasets%20with%0Asimilar%20conditions%2C%20deep%20face%20recognition%20models%20perform%20exceptionally%20well%20in%0Asuch%20tasks.%20However%2C%20in%20other%20tasks%20where%20substantially%20less%20training%20data%20is%0Aavailable%2C%20such%20methods%20struggle%2C%20especially%20when%20required%20to%20compare%0Ahigh-quality%20enrollment%20images%20with%20low-quality%20probes.%20On%20the%20other%20hand%2C%0Atraditional%20RankList-based%20methods%20have%20been%20developed%20that%20compare%20faces%0Aindirectly%20by%20comparing%20to%20cohort%20faces%20with%20similar%20conditions.%20In%20this%20paper%2C%0Awe%20revisit%20these%20RankList%20methods%20and%20extend%20them%20to%20use%20the%20logits%20of%20the%0Astate-of-the-art%20DaliFace%20network%2C%20instead%20of%20an%20external%20cohort.%20We%20show%20that%0Athrough%20a%20reasonable%20Logit-Cohort%20Selection%20%28LoCoS%29%20the%20performance%20of%0ARankList-based%20functions%20can%20be%20improved%20drastically.%20Experiments%20on%20two%0Achallenging%20face%20recognition%20datasets%20not%20only%20demonstrate%20the%20enhanced%0Aperformance%20of%20our%20proposed%20method%20but%20also%20set%20the%20stage%20for%20future%0Aadvancements%20in%20handling%20diverse%20image%20qualities.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2410.01498v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DQuo%2520Vadis%2520RankList-based%2520System%2520in%2520Face%2520Recognition%253F%26entry.906535625%3DXinyi%2520Zhang%2520and%2520Manuel%2520G%25C3%25BCnther%26entry.1292438233%3D%2520%2520Face%2520recognition%2520in%2520the%2520wild%2520has%2520gained%2520a%2520lot%2520of%2520focus%2520in%2520the%2520last%2520few%2520years%252C%250Aand%2520many%2520face%2520recognition%2520models%2520are%2520designed%2520to%2520verify%2520faces%2520in%2520medium-quality%250Aimages.%2520Especially%2520due%2520to%2520the%2520availability%2520of%2520large%2520training%2520datasets%2520with%250Asimilar%2520conditions%252C%2520deep%2520face%2520recognition%2520models%2520perform%2520exceptionally%2520well%2520in%250Asuch%2520tasks.%2520However%252C%2520in%2520other%2520tasks%2520where%2520substantially%2520less%2520training%2520data%2520is%250Aavailable%252C%2520such%2520methods%2520struggle%252C%2520especially%2520when%2520required%2520to%2520compare%250Ahigh-quality%2520enrollment%2520images%2520with%2520low-quality%2520probes.%2520On%2520the%2520other%2520hand%252C%250Atraditional%2520RankList-based%2520methods%2520have%2520been%2520developed%2520that%2520compare%2520faces%250Aindirectly%2520by%2520comparing%2520to%2520cohort%2520faces%2520with%2520similar%2520conditions.%2520In%2520this%2520paper%252C%250Awe%2520revisit%2520these%2520RankList%2520methods%2520and%2520extend%2520them%2520to%2520use%2520the%2520logits%2520of%2520the%250Astate-of-the-art%2520DaliFace%2520network%252C%2520instead%2520of%2520an%2520external%2520cohort.%2520We%2520show%2520that%250Athrough%2520a%2520reasonable%2520Logit-Cohort%2520Selection%2520%2528LoCoS%2529%2520the%2520performance%2520of%250ARankList-based%2520functions%2520can%2520be%2520improved%2520drastically.%2520Experiments%2520on%2520two%250Achallenging%2520face%2520recognition%2520datasets%2520not%2520only%2520demonstrate%2520the%2520enhanced%250Aperformance%2520of%2520our%2520proposed%2520method%2520but%2520also%2520set%2520the%2520stage%2520for%2520future%250Aadvancements%2520in%2520handling%2520diverse%2520image%2520qualities.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2410.01498v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Quo%20Vadis%20RankList-based%20System%20in%20Face%20Recognition%3F&entry.906535625=Xinyi%20Zhang%20and%20Manuel%20G%C3%BCnther&entry.1292438233=%20%20Face%20recognition%20in%20the%20wild%20has%20gained%20a%20lot%20of%20focus%20in%20the%20last%20few%20years%2C%0Aand%20many%20face%20recognition%20models%20are%20designed%20to%20verify%20faces%20in%20medium-quality%0Aimages.%20Especially%20due%20to%20the%20availability%20of%20large%20training%20datasets%20with%0Asimilar%20conditions%2C%20deep%20face%20recognition%20models%20perform%20exceptionally%20well%20in%0Asuch%20tasks.%20However%2C%20in%20other%20tasks%20where%20substantially%20less%20training%20data%20is%0Aavailable%2C%20such%20methods%20struggle%2C%20especially%20when%20required%20to%20compare%0Ahigh-quality%20enrollment%20images%20with%20low-quality%20probes.%20On%20the%20other%20hand%2C%0Atraditional%20RankList-based%20methods%20have%20been%20developed%20that%20compare%20faces%0Aindirectly%20by%20comparing%20to%20cohort%20faces%20with%20similar%20conditions.%20In%20this%20paper%2C%0Awe%20revisit%20these%20RankList%20methods%20and%20extend%20them%20to%20use%20the%20logits%20of%20the%0Astate-of-the-art%20DaliFace%20network%2C%20instead%20of%20an%20external%20cohort.%20We%20show%20that%0Athrough%20a%20reasonable%20Logit-Cohort%20Selection%20%28LoCoS%29%20the%20performance%20of%0ARankList-based%20functions%20can%20be%20improved%20drastically.%20Experiments%20on%20two%0Achallenging%20face%20recognition%20datasets%20not%20only%20demonstrate%20the%20enhanced%0Aperformance%20of%20our%20proposed%20method%20but%20also%20set%20the%20stage%20for%20future%0Aadvancements%20in%20handling%20diverse%20image%20qualities.%0A&entry.1838667208=http%3A//arxiv.org/abs/2410.01498v1&entry.124074799=Read"},
{"title": "Efficient Statistics With Unknown Truncation, Polynomial Time\n  Algorithms, Beyond Gaussians", "author": "Jane H. Lee and Anay Mehrotra and Manolis Zampetakis", "abstract": "  We study the estimation of distributional parameters when samples are shown\nonly if they fall in some unknown set $S \\subseteq \\mathbb{R}^d$. Kontonis,\nTzamos, and Zampetakis (FOCS'19) gave a $d^{\\mathrm{poly}(1/\\varepsilon)}$ time\nalgorithm for finding $\\varepsilon$-accurate parameters for the special case of\nGaussian distributions with diagonal covariance matrix. Recently, Diakonikolas,\nKane, Pittas, and Zarifis (COLT'24) showed that this exponential dependence on\n$1/\\varepsilon$ is necessary even when $S$ belongs to some well-behaved\nclasses. These works leave the following open problems which we address in this\nwork: Can we estimate the parameters of any Gaussian or even extend beyond\nGaussians? Can we design $\\mathrm{poly}(d/\\varepsilon)$ time algorithms when\n$S$ is a simple set such as a halfspace?\n  We make progress on both of these questions by providing the following\nresults:\n  1. Toward the first question, we give a $d^{\\mathrm{poly}(\\ell/\\varepsilon)}$\ntime algorithm for any exponential family that satisfies some structural\nassumptions and any unknown set $S$ that is $\\varepsilon$-approximable by\ndegree-$\\ell$ polynomials. This result has two important applications:\n  1a) The first algorithm for estimating arbitrary Gaussian distributions from\nsamples truncated to an unknown $S$; and\n  1b) The first algorithm for linear regression with unknown truncation and\nGaussian features.\n  2. To address the second question, we provide an algorithm with runtime\n$\\mathrm{poly}(d/\\varepsilon)$ that works for a set of exponential families\n(containing all Gaussians) when $S$ is a halfspace or an axis-aligned\nrectangle.\n  Along the way, we develop tools that may be of independent interest,\nincluding, a reduction from PAC learning with positive and unlabeled samples to\nPAC learning with positive and negative samples that is robust to certain\ncovariate shifts.\n", "link": "http://arxiv.org/abs/2410.01656v1", "date": "2024-10-02", "relevancy": 2.4087, "topK": [{"title": "RTG-SLAM: Real-time 3D Reconstruction at Scale using Gaussian Splatting", "link": "http://arxiv.org/abs/2404.19706v1", "similarity": 0.4867}, {"title": "GST: Precise 3D Human Body from a Single Image with Gaussian Splatting\n  Transformers", "link": "http://arxiv.org/abs/2409.04196v1", "similarity": 0.4854}, {"title": "Dynamic Gaussians Mesh: Consistent Mesh Reconstruction from Monocular\n  Videos", "link": "http://arxiv.org/abs/2404.12379v2", "similarity": 0.4732}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Efficient%20Statistics%20With%20Unknown%20Truncation%2C%20Polynomial%20Time%0A%20%20Algorithms%2C%20Beyond%20Gaussians&body=Title%3A%20Efficient%20Statistics%20With%20Unknown%20Truncation%2C%20Polynomial%20Time%0A%20%20Algorithms%2C%20Beyond%20Gaussians%0AAuthor%3A%20Jane%20H.%20Lee%20and%20Anay%20Mehrotra%20and%20Manolis%20Zampetakis%0AAbstract%3A%20%20%20We%20study%20the%20estimation%20of%20distributional%20parameters%20when%20samples%20are%20shown%0Aonly%20if%20they%20fall%20in%20some%20unknown%20set%20%24S%20%5Csubseteq%20%5Cmathbb%7BR%7D%5Ed%24.%20Kontonis%2C%0ATzamos%2C%20and%20Zampetakis%20%28FOCS%2719%29%20gave%20a%20%24d%5E%7B%5Cmathrm%7Bpoly%7D%281/%5Cvarepsilon%29%7D%24%20time%0Aalgorithm%20for%20finding%20%24%5Cvarepsilon%24-accurate%20parameters%20for%20the%20special%20case%20of%0AGaussian%20distributions%20with%20diagonal%20covariance%20matrix.%20Recently%2C%20Diakonikolas%2C%0AKane%2C%20Pittas%2C%20and%20Zarifis%20%28COLT%2724%29%20showed%20that%20this%20exponential%20dependence%20on%0A%241/%5Cvarepsilon%24%20is%20necessary%20even%20when%20%24S%24%20belongs%20to%20some%20well-behaved%0Aclasses.%20These%20works%20leave%20the%20following%20open%20problems%20which%20we%20address%20in%20this%0Awork%3A%20Can%20we%20estimate%20the%20parameters%20of%20any%20Gaussian%20or%20even%20extend%20beyond%0AGaussians%3F%20Can%20we%20design%20%24%5Cmathrm%7Bpoly%7D%28d/%5Cvarepsilon%29%24%20time%20algorithms%20when%0A%24S%24%20is%20a%20simple%20set%20such%20as%20a%20halfspace%3F%0A%20%20We%20make%20progress%20on%20both%20of%20these%20questions%20by%20providing%20the%20following%0Aresults%3A%0A%20%201.%20Toward%20the%20first%20question%2C%20we%20give%20a%20%24d%5E%7B%5Cmathrm%7Bpoly%7D%28%5Cell/%5Cvarepsilon%29%7D%24%0Atime%20algorithm%20for%20any%20exponential%20family%20that%20satisfies%20some%20structural%0Aassumptions%20and%20any%20unknown%20set%20%24S%24%20that%20is%20%24%5Cvarepsilon%24-approximable%20by%0Adegree-%24%5Cell%24%20polynomials.%20This%20result%20has%20two%20important%20applications%3A%0A%20%201a%29%20The%20first%20algorithm%20for%20estimating%20arbitrary%20Gaussian%20distributions%20from%0Asamples%20truncated%20to%20an%20unknown%20%24S%24%3B%20and%0A%20%201b%29%20The%20first%20algorithm%20for%20linear%20regression%20with%20unknown%20truncation%20and%0AGaussian%20features.%0A%20%202.%20To%20address%20the%20second%20question%2C%20we%20provide%20an%20algorithm%20with%20runtime%0A%24%5Cmathrm%7Bpoly%7D%28d/%5Cvarepsilon%29%24%20that%20works%20for%20a%20set%20of%20exponential%20families%0A%28containing%20all%20Gaussians%29%20when%20%24S%24%20is%20a%20halfspace%20or%20an%20axis-aligned%0Arectangle.%0A%20%20Along%20the%20way%2C%20we%20develop%20tools%20that%20may%20be%20of%20independent%20interest%2C%0Aincluding%2C%20a%20reduction%20from%20PAC%20learning%20with%20positive%20and%20unlabeled%20samples%20to%0APAC%20learning%20with%20positive%20and%20negative%20samples%20that%20is%20robust%20to%20certain%0Acovariate%20shifts.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2410.01656v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DEfficient%2520Statistics%2520With%2520Unknown%2520Truncation%252C%2520Polynomial%2520Time%250A%2520%2520Algorithms%252C%2520Beyond%2520Gaussians%26entry.906535625%3DJane%2520H.%2520Lee%2520and%2520Anay%2520Mehrotra%2520and%2520Manolis%2520Zampetakis%26entry.1292438233%3D%2520%2520We%2520study%2520the%2520estimation%2520of%2520distributional%2520parameters%2520when%2520samples%2520are%2520shown%250Aonly%2520if%2520they%2520fall%2520in%2520some%2520unknown%2520set%2520%2524S%2520%255Csubseteq%2520%255Cmathbb%257BR%257D%255Ed%2524.%2520Kontonis%252C%250ATzamos%252C%2520and%2520Zampetakis%2520%2528FOCS%252719%2529%2520gave%2520a%2520%2524d%255E%257B%255Cmathrm%257Bpoly%257D%25281/%255Cvarepsilon%2529%257D%2524%2520time%250Aalgorithm%2520for%2520finding%2520%2524%255Cvarepsilon%2524-accurate%2520parameters%2520for%2520the%2520special%2520case%2520of%250AGaussian%2520distributions%2520with%2520diagonal%2520covariance%2520matrix.%2520Recently%252C%2520Diakonikolas%252C%250AKane%252C%2520Pittas%252C%2520and%2520Zarifis%2520%2528COLT%252724%2529%2520showed%2520that%2520this%2520exponential%2520dependence%2520on%250A%25241/%255Cvarepsilon%2524%2520is%2520necessary%2520even%2520when%2520%2524S%2524%2520belongs%2520to%2520some%2520well-behaved%250Aclasses.%2520These%2520works%2520leave%2520the%2520following%2520open%2520problems%2520which%2520we%2520address%2520in%2520this%250Awork%253A%2520Can%2520we%2520estimate%2520the%2520parameters%2520of%2520any%2520Gaussian%2520or%2520even%2520extend%2520beyond%250AGaussians%253F%2520Can%2520we%2520design%2520%2524%255Cmathrm%257Bpoly%257D%2528d/%255Cvarepsilon%2529%2524%2520time%2520algorithms%2520when%250A%2524S%2524%2520is%2520a%2520simple%2520set%2520such%2520as%2520a%2520halfspace%253F%250A%2520%2520We%2520make%2520progress%2520on%2520both%2520of%2520these%2520questions%2520by%2520providing%2520the%2520following%250Aresults%253A%250A%2520%25201.%2520Toward%2520the%2520first%2520question%252C%2520we%2520give%2520a%2520%2524d%255E%257B%255Cmathrm%257Bpoly%257D%2528%255Cell/%255Cvarepsilon%2529%257D%2524%250Atime%2520algorithm%2520for%2520any%2520exponential%2520family%2520that%2520satisfies%2520some%2520structural%250Aassumptions%2520and%2520any%2520unknown%2520set%2520%2524S%2524%2520that%2520is%2520%2524%255Cvarepsilon%2524-approximable%2520by%250Adegree-%2524%255Cell%2524%2520polynomials.%2520This%2520result%2520has%2520two%2520important%2520applications%253A%250A%2520%25201a%2529%2520The%2520first%2520algorithm%2520for%2520estimating%2520arbitrary%2520Gaussian%2520distributions%2520from%250Asamples%2520truncated%2520to%2520an%2520unknown%2520%2524S%2524%253B%2520and%250A%2520%25201b%2529%2520The%2520first%2520algorithm%2520for%2520linear%2520regression%2520with%2520unknown%2520truncation%2520and%250AGaussian%2520features.%250A%2520%25202.%2520To%2520address%2520the%2520second%2520question%252C%2520we%2520provide%2520an%2520algorithm%2520with%2520runtime%250A%2524%255Cmathrm%257Bpoly%257D%2528d/%255Cvarepsilon%2529%2524%2520that%2520works%2520for%2520a%2520set%2520of%2520exponential%2520families%250A%2528containing%2520all%2520Gaussians%2529%2520when%2520%2524S%2524%2520is%2520a%2520halfspace%2520or%2520an%2520axis-aligned%250Arectangle.%250A%2520%2520Along%2520the%2520way%252C%2520we%2520develop%2520tools%2520that%2520may%2520be%2520of%2520independent%2520interest%252C%250Aincluding%252C%2520a%2520reduction%2520from%2520PAC%2520learning%2520with%2520positive%2520and%2520unlabeled%2520samples%2520to%250APAC%2520learning%2520with%2520positive%2520and%2520negative%2520samples%2520that%2520is%2520robust%2520to%2520certain%250Acovariate%2520shifts.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2410.01656v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Efficient%20Statistics%20With%20Unknown%20Truncation%2C%20Polynomial%20Time%0A%20%20Algorithms%2C%20Beyond%20Gaussians&entry.906535625=Jane%20H.%20Lee%20and%20Anay%20Mehrotra%20and%20Manolis%20Zampetakis&entry.1292438233=%20%20We%20study%20the%20estimation%20of%20distributional%20parameters%20when%20samples%20are%20shown%0Aonly%20if%20they%20fall%20in%20some%20unknown%20set%20%24S%20%5Csubseteq%20%5Cmathbb%7BR%7D%5Ed%24.%20Kontonis%2C%0ATzamos%2C%20and%20Zampetakis%20%28FOCS%2719%29%20gave%20a%20%24d%5E%7B%5Cmathrm%7Bpoly%7D%281/%5Cvarepsilon%29%7D%24%20time%0Aalgorithm%20for%20finding%20%24%5Cvarepsilon%24-accurate%20parameters%20for%20the%20special%20case%20of%0AGaussian%20distributions%20with%20diagonal%20covariance%20matrix.%20Recently%2C%20Diakonikolas%2C%0AKane%2C%20Pittas%2C%20and%20Zarifis%20%28COLT%2724%29%20showed%20that%20this%20exponential%20dependence%20on%0A%241/%5Cvarepsilon%24%20is%20necessary%20even%20when%20%24S%24%20belongs%20to%20some%20well-behaved%0Aclasses.%20These%20works%20leave%20the%20following%20open%20problems%20which%20we%20address%20in%20this%0Awork%3A%20Can%20we%20estimate%20the%20parameters%20of%20any%20Gaussian%20or%20even%20extend%20beyond%0AGaussians%3F%20Can%20we%20design%20%24%5Cmathrm%7Bpoly%7D%28d/%5Cvarepsilon%29%24%20time%20algorithms%20when%0A%24S%24%20is%20a%20simple%20set%20such%20as%20a%20halfspace%3F%0A%20%20We%20make%20progress%20on%20both%20of%20these%20questions%20by%20providing%20the%20following%0Aresults%3A%0A%20%201.%20Toward%20the%20first%20question%2C%20we%20give%20a%20%24d%5E%7B%5Cmathrm%7Bpoly%7D%28%5Cell/%5Cvarepsilon%29%7D%24%0Atime%20algorithm%20for%20any%20exponential%20family%20that%20satisfies%20some%20structural%0Aassumptions%20and%20any%20unknown%20set%20%24S%24%20that%20is%20%24%5Cvarepsilon%24-approximable%20by%0Adegree-%24%5Cell%24%20polynomials.%20This%20result%20has%20two%20important%20applications%3A%0A%20%201a%29%20The%20first%20algorithm%20for%20estimating%20arbitrary%20Gaussian%20distributions%20from%0Asamples%20truncated%20to%20an%20unknown%20%24S%24%3B%20and%0A%20%201b%29%20The%20first%20algorithm%20for%20linear%20regression%20with%20unknown%20truncation%20and%0AGaussian%20features.%0A%20%202.%20To%20address%20the%20second%20question%2C%20we%20provide%20an%20algorithm%20with%20runtime%0A%24%5Cmathrm%7Bpoly%7D%28d/%5Cvarepsilon%29%24%20that%20works%20for%20a%20set%20of%20exponential%20families%0A%28containing%20all%20Gaussians%29%20when%20%24S%24%20is%20a%20halfspace%20or%20an%20axis-aligned%0Arectangle.%0A%20%20Along%20the%20way%2C%20we%20develop%20tools%20that%20may%20be%20of%20independent%20interest%2C%0Aincluding%2C%20a%20reduction%20from%20PAC%20learning%20with%20positive%20and%20unlabeled%20samples%20to%0APAC%20learning%20with%20positive%20and%20negative%20samples%20that%20is%20robust%20to%20certain%0Acovariate%20shifts.%0A&entry.1838667208=http%3A//arxiv.org/abs/2410.01656v1&entry.124074799=Read"},
{"title": "Agnostic Sharpness-Aware Minimization", "author": "Van-Anh Nguyen and Quyen Tran and Tuan Truong and Thanh-Toan Do and Dinh Phung and Trung Le", "abstract": "  Sharpness-aware minimization (SAM) has been instrumental in improving deep\nneural network training by minimizing both the training loss and the sharpness\nof the loss landscape, leading the model into flatter minima that are\nassociated with better generalization properties. In another aspect,\nModel-Agnostic Meta-Learning (MAML) is a framework designed to improve the\nadaptability of models. MAML optimizes a set of meta-models that are\nspecifically tailored for quick adaptation to multiple tasks with minimal\nfine-tuning steps and can generalize well with limited data. In this work, we\nexplore the connection between SAM and MAML in enhancing model generalization.\nWe introduce Agnostic-SAM, a novel approach that combines the principles of\nboth SAM and MAML. Agnostic-SAM adapts the core idea of SAM by optimizing the\nmodel toward wider local minima using training data, while concurrently\nmaintaining low loss values on validation data. By doing so, it seeks flatter\nminima that are not only robust to small perturbations but also less vulnerable\nto data distributional shift problems. Our experimental results demonstrate\nthat Agnostic-SAM significantly improves generalization over baselines across a\nrange of datasets and under challenging conditions such as noisy labels or data\nlimitation.\n", "link": "http://arxiv.org/abs/2406.07107v3", "date": "2024-10-02", "relevancy": 2.3968, "topK": [{"title": "Breadcrumbs: Adversarial class-balanced sampling for long-tailed recognition", "link": "https://link.springer.com/chapter/10.1007/978-3-031-20053-3_37", "similarity": 0.4949}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.4766}, {"title": "Boosted dynamic neural networks", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/26302", "similarity": 0.4666}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Agnostic%20Sharpness-Aware%20Minimization&body=Title%3A%20Agnostic%20Sharpness-Aware%20Minimization%0AAuthor%3A%20Van-Anh%20Nguyen%20and%20Quyen%20Tran%20and%20Tuan%20Truong%20and%20Thanh-Toan%20Do%20and%20Dinh%20Phung%20and%20Trung%20Le%0AAbstract%3A%20%20%20Sharpness-aware%20minimization%20%28SAM%29%20has%20been%20instrumental%20in%20improving%20deep%0Aneural%20network%20training%20by%20minimizing%20both%20the%20training%20loss%20and%20the%20sharpness%0Aof%20the%20loss%20landscape%2C%20leading%20the%20model%20into%20flatter%20minima%20that%20are%0Aassociated%20with%20better%20generalization%20properties.%20In%20another%20aspect%2C%0AModel-Agnostic%20Meta-Learning%20%28MAML%29%20is%20a%20framework%20designed%20to%20improve%20the%0Aadaptability%20of%20models.%20MAML%20optimizes%20a%20set%20of%20meta-models%20that%20are%0Aspecifically%20tailored%20for%20quick%20adaptation%20to%20multiple%20tasks%20with%20minimal%0Afine-tuning%20steps%20and%20can%20generalize%20well%20with%20limited%20data.%20In%20this%20work%2C%20we%0Aexplore%20the%20connection%20between%20SAM%20and%20MAML%20in%20enhancing%20model%20generalization.%0AWe%20introduce%20Agnostic-SAM%2C%20a%20novel%20approach%20that%20combines%20the%20principles%20of%0Aboth%20SAM%20and%20MAML.%20Agnostic-SAM%20adapts%20the%20core%20idea%20of%20SAM%20by%20optimizing%20the%0Amodel%20toward%20wider%20local%20minima%20using%20training%20data%2C%20while%20concurrently%0Amaintaining%20low%20loss%20values%20on%20validation%20data.%20By%20doing%20so%2C%20it%20seeks%20flatter%0Aminima%20that%20are%20not%20only%20robust%20to%20small%20perturbations%20but%20also%20less%20vulnerable%0Ato%20data%20distributional%20shift%20problems.%20Our%20experimental%20results%20demonstrate%0Athat%20Agnostic-SAM%20significantly%20improves%20generalization%20over%20baselines%20across%20a%0Arange%20of%20datasets%20and%20under%20challenging%20conditions%20such%20as%20noisy%20labels%20or%20data%0Alimitation.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2406.07107v3%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DAgnostic%2520Sharpness-Aware%2520Minimization%26entry.906535625%3DVan-Anh%2520Nguyen%2520and%2520Quyen%2520Tran%2520and%2520Tuan%2520Truong%2520and%2520Thanh-Toan%2520Do%2520and%2520Dinh%2520Phung%2520and%2520Trung%2520Le%26entry.1292438233%3D%2520%2520Sharpness-aware%2520minimization%2520%2528SAM%2529%2520has%2520been%2520instrumental%2520in%2520improving%2520deep%250Aneural%2520network%2520training%2520by%2520minimizing%2520both%2520the%2520training%2520loss%2520and%2520the%2520sharpness%250Aof%2520the%2520loss%2520landscape%252C%2520leading%2520the%2520model%2520into%2520flatter%2520minima%2520that%2520are%250Aassociated%2520with%2520better%2520generalization%2520properties.%2520In%2520another%2520aspect%252C%250AModel-Agnostic%2520Meta-Learning%2520%2528MAML%2529%2520is%2520a%2520framework%2520designed%2520to%2520improve%2520the%250Aadaptability%2520of%2520models.%2520MAML%2520optimizes%2520a%2520set%2520of%2520meta-models%2520that%2520are%250Aspecifically%2520tailored%2520for%2520quick%2520adaptation%2520to%2520multiple%2520tasks%2520with%2520minimal%250Afine-tuning%2520steps%2520and%2520can%2520generalize%2520well%2520with%2520limited%2520data.%2520In%2520this%2520work%252C%2520we%250Aexplore%2520the%2520connection%2520between%2520SAM%2520and%2520MAML%2520in%2520enhancing%2520model%2520generalization.%250AWe%2520introduce%2520Agnostic-SAM%252C%2520a%2520novel%2520approach%2520that%2520combines%2520the%2520principles%2520of%250Aboth%2520SAM%2520and%2520MAML.%2520Agnostic-SAM%2520adapts%2520the%2520core%2520idea%2520of%2520SAM%2520by%2520optimizing%2520the%250Amodel%2520toward%2520wider%2520local%2520minima%2520using%2520training%2520data%252C%2520while%2520concurrently%250Amaintaining%2520low%2520loss%2520values%2520on%2520validation%2520data.%2520By%2520doing%2520so%252C%2520it%2520seeks%2520flatter%250Aminima%2520that%2520are%2520not%2520only%2520robust%2520to%2520small%2520perturbations%2520but%2520also%2520less%2520vulnerable%250Ato%2520data%2520distributional%2520shift%2520problems.%2520Our%2520experimental%2520results%2520demonstrate%250Athat%2520Agnostic-SAM%2520significantly%2520improves%2520generalization%2520over%2520baselines%2520across%2520a%250Arange%2520of%2520datasets%2520and%2520under%2520challenging%2520conditions%2520such%2520as%2520noisy%2520labels%2520or%2520data%250Alimitation.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2406.07107v3%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Agnostic%20Sharpness-Aware%20Minimization&entry.906535625=Van-Anh%20Nguyen%20and%20Quyen%20Tran%20and%20Tuan%20Truong%20and%20Thanh-Toan%20Do%20and%20Dinh%20Phung%20and%20Trung%20Le&entry.1292438233=%20%20Sharpness-aware%20minimization%20%28SAM%29%20has%20been%20instrumental%20in%20improving%20deep%0Aneural%20network%20training%20by%20minimizing%20both%20the%20training%20loss%20and%20the%20sharpness%0Aof%20the%20loss%20landscape%2C%20leading%20the%20model%20into%20flatter%20minima%20that%20are%0Aassociated%20with%20better%20generalization%20properties.%20In%20another%20aspect%2C%0AModel-Agnostic%20Meta-Learning%20%28MAML%29%20is%20a%20framework%20designed%20to%20improve%20the%0Aadaptability%20of%20models.%20MAML%20optimizes%20a%20set%20of%20meta-models%20that%20are%0Aspecifically%20tailored%20for%20quick%20adaptation%20to%20multiple%20tasks%20with%20minimal%0Afine-tuning%20steps%20and%20can%20generalize%20well%20with%20limited%20data.%20In%20this%20work%2C%20we%0Aexplore%20the%20connection%20between%20SAM%20and%20MAML%20in%20enhancing%20model%20generalization.%0AWe%20introduce%20Agnostic-SAM%2C%20a%20novel%20approach%20that%20combines%20the%20principles%20of%0Aboth%20SAM%20and%20MAML.%20Agnostic-SAM%20adapts%20the%20core%20idea%20of%20SAM%20by%20optimizing%20the%0Amodel%20toward%20wider%20local%20minima%20using%20training%20data%2C%20while%20concurrently%0Amaintaining%20low%20loss%20values%20on%20validation%20data.%20By%20doing%20so%2C%20it%20seeks%20flatter%0Aminima%20that%20are%20not%20only%20robust%20to%20small%20perturbations%20but%20also%20less%20vulnerable%0Ato%20data%20distributional%20shift%20problems.%20Our%20experimental%20results%20demonstrate%0Athat%20Agnostic-SAM%20significantly%20improves%20generalization%20over%20baselines%20across%20a%0Arange%20of%20datasets%20and%20under%20challenging%20conditions%20such%20as%20noisy%20labels%20or%20data%0Alimitation.%0A&entry.1838667208=http%3A//arxiv.org/abs/2406.07107v3&entry.124074799=Read"},
{"title": "TopER: Topological Embeddings in Graph Representation Learning", "author": "Astrit Tola and Funmilola Mary Taiwom and Cuneyt Gurcan Akcora and Baris Coskunuzer", "abstract": "  Graph embeddings play a critical role in graph representation learning,\nallowing machine learning models to explore and interpret graph-structured\ndata. However, existing methods often rely on opaque, high-dimensional\nembeddings, limiting interpretability and practical visualization.\n  In this work, we introduce Topological Evolution Rate (TopER), a novel,\nlow-dimensional embedding approach grounded in topological data analysis. TopER\nsimplifies a key topological approach, Persistent Homology, by calculating the\nevolution rate of graph substructures, resulting in intuitive and interpretable\nvisualizations of graph data. This approach not only enhances the exploration\nof graph datasets but also delivers competitive performance in graph clustering\nand classification tasks. Our TopER-based models achieve or surpass\nstate-of-the-art results across molecular, biological, and social network\ndatasets in tasks such as classification, clustering, and visualization.\n", "link": "http://arxiv.org/abs/2410.01778v1", "date": "2024-10-02", "relevancy": 2.3779, "topK": [{"title": "Self-supervised Photographic Image Layout Representation Learning", "link": "http://arxiv.org/abs/2403.03740v1", "similarity": 0.5041}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.4613}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.4613}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20TopER%3A%20Topological%20Embeddings%20in%20Graph%20Representation%20Learning&body=Title%3A%20TopER%3A%20Topological%20Embeddings%20in%20Graph%20Representation%20Learning%0AAuthor%3A%20Astrit%20Tola%20and%20Funmilola%20Mary%20Taiwom%20and%20Cuneyt%20Gurcan%20Akcora%20and%20Baris%20Coskunuzer%0AAbstract%3A%20%20%20Graph%20embeddings%20play%20a%20critical%20role%20in%20graph%20representation%20learning%2C%0Aallowing%20machine%20learning%20models%20to%20explore%20and%20interpret%20graph-structured%0Adata.%20However%2C%20existing%20methods%20often%20rely%20on%20opaque%2C%20high-dimensional%0Aembeddings%2C%20limiting%20interpretability%20and%20practical%20visualization.%0A%20%20In%20this%20work%2C%20we%20introduce%20Topological%20Evolution%20Rate%20%28TopER%29%2C%20a%20novel%2C%0Alow-dimensional%20embedding%20approach%20grounded%20in%20topological%20data%20analysis.%20TopER%0Asimplifies%20a%20key%20topological%20approach%2C%20Persistent%20Homology%2C%20by%20calculating%20the%0Aevolution%20rate%20of%20graph%20substructures%2C%20resulting%20in%20intuitive%20and%20interpretable%0Avisualizations%20of%20graph%20data.%20This%20approach%20not%20only%20enhances%20the%20exploration%0Aof%20graph%20datasets%20but%20also%20delivers%20competitive%20performance%20in%20graph%20clustering%0Aand%20classification%20tasks.%20Our%20TopER-based%20models%20achieve%20or%20surpass%0Astate-of-the-art%20results%20across%20molecular%2C%20biological%2C%20and%20social%20network%0Adatasets%20in%20tasks%20such%20as%20classification%2C%20clustering%2C%20and%20visualization.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2410.01778v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DTopER%253A%2520Topological%2520Embeddings%2520in%2520Graph%2520Representation%2520Learning%26entry.906535625%3DAstrit%2520Tola%2520and%2520Funmilola%2520Mary%2520Taiwom%2520and%2520Cuneyt%2520Gurcan%2520Akcora%2520and%2520Baris%2520Coskunuzer%26entry.1292438233%3D%2520%2520Graph%2520embeddings%2520play%2520a%2520critical%2520role%2520in%2520graph%2520representation%2520learning%252C%250Aallowing%2520machine%2520learning%2520models%2520to%2520explore%2520and%2520interpret%2520graph-structured%250Adata.%2520However%252C%2520existing%2520methods%2520often%2520rely%2520on%2520opaque%252C%2520high-dimensional%250Aembeddings%252C%2520limiting%2520interpretability%2520and%2520practical%2520visualization.%250A%2520%2520In%2520this%2520work%252C%2520we%2520introduce%2520Topological%2520Evolution%2520Rate%2520%2528TopER%2529%252C%2520a%2520novel%252C%250Alow-dimensional%2520embedding%2520approach%2520grounded%2520in%2520topological%2520data%2520analysis.%2520TopER%250Asimplifies%2520a%2520key%2520topological%2520approach%252C%2520Persistent%2520Homology%252C%2520by%2520calculating%2520the%250Aevolution%2520rate%2520of%2520graph%2520substructures%252C%2520resulting%2520in%2520intuitive%2520and%2520interpretable%250Avisualizations%2520of%2520graph%2520data.%2520This%2520approach%2520not%2520only%2520enhances%2520the%2520exploration%250Aof%2520graph%2520datasets%2520but%2520also%2520delivers%2520competitive%2520performance%2520in%2520graph%2520clustering%250Aand%2520classification%2520tasks.%2520Our%2520TopER-based%2520models%2520achieve%2520or%2520surpass%250Astate-of-the-art%2520results%2520across%2520molecular%252C%2520biological%252C%2520and%2520social%2520network%250Adatasets%2520in%2520tasks%2520such%2520as%2520classification%252C%2520clustering%252C%2520and%2520visualization.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2410.01778v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=TopER%3A%20Topological%20Embeddings%20in%20Graph%20Representation%20Learning&entry.906535625=Astrit%20Tola%20and%20Funmilola%20Mary%20Taiwom%20and%20Cuneyt%20Gurcan%20Akcora%20and%20Baris%20Coskunuzer&entry.1292438233=%20%20Graph%20embeddings%20play%20a%20critical%20role%20in%20graph%20representation%20learning%2C%0Aallowing%20machine%20learning%20models%20to%20explore%20and%20interpret%20graph-structured%0Adata.%20However%2C%20existing%20methods%20often%20rely%20on%20opaque%2C%20high-dimensional%0Aembeddings%2C%20limiting%20interpretability%20and%20practical%20visualization.%0A%20%20In%20this%20work%2C%20we%20introduce%20Topological%20Evolution%20Rate%20%28TopER%29%2C%20a%20novel%2C%0Alow-dimensional%20embedding%20approach%20grounded%20in%20topological%20data%20analysis.%20TopER%0Asimplifies%20a%20key%20topological%20approach%2C%20Persistent%20Homology%2C%20by%20calculating%20the%0Aevolution%20rate%20of%20graph%20substructures%2C%20resulting%20in%20intuitive%20and%20interpretable%0Avisualizations%20of%20graph%20data.%20This%20approach%20not%20only%20enhances%20the%20exploration%0Aof%20graph%20datasets%20but%20also%20delivers%20competitive%20performance%20in%20graph%20clustering%0Aand%20classification%20tasks.%20Our%20TopER-based%20models%20achieve%20or%20surpass%0Astate-of-the-art%20results%20across%20molecular%2C%20biological%2C%20and%20social%20network%0Adatasets%20in%20tasks%20such%20as%20classification%2C%20clustering%2C%20and%20visualization.%0A&entry.1838667208=http%3A//arxiv.org/abs/2410.01778v1&entry.124074799=Read"},
{"title": "From Reward Shaping to Q-Shaping: Achieving Unbiased Learning with\n  LLM-Guided Knowledge", "author": "Xiefeng Wu", "abstract": "  Q-shaping is an extension of Q-value initialization and serves as an\nalternative to reward shaping for incorporating domain knowledge to accelerate\nagent training, thereby improving sample efficiency by directly shaping\nQ-values. This approach is both general and robust across diverse tasks,\nallowing for immediate impact assessment while guaranteeing optimality. We\nevaluated Q-shaping across 20 different environments using a large language\nmodel (LLM) as the heuristic provider. The results demonstrate that Q-shaping\nsignificantly enhances sample efficiency, achieving a \\textbf{16.87\\%}\nimprovement over the best baseline in each environment and a \\textbf{253.80\\%}\nimprovement compared to LLM-based reward shaping methods. These findings\nestablish Q-shaping as a superior and unbiased alternative to conventional\nreward shaping in reinforcement learning.\n", "link": "http://arxiv.org/abs/2410.01458v1", "date": "2024-10-02", "relevancy": 2.3713, "topK": [{"title": "Safe and efficient training of a control agent", "link": "https://patents.google.com/patent/US11709462B2/en", "similarity": 0.4886}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.4809}, {"title": "Self-supervised Photographic Image Layout Representation Learning", "link": "http://arxiv.org/abs/2403.03740v1", "similarity": 0.4533}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20From%20Reward%20Shaping%20to%20Q-Shaping%3A%20Achieving%20Unbiased%20Learning%20with%0A%20%20LLM-Guided%20Knowledge&body=Title%3A%20From%20Reward%20Shaping%20to%20Q-Shaping%3A%20Achieving%20Unbiased%20Learning%20with%0A%20%20LLM-Guided%20Knowledge%0AAuthor%3A%20Xiefeng%20Wu%0AAbstract%3A%20%20%20Q-shaping%20is%20an%20extension%20of%20Q-value%20initialization%20and%20serves%20as%20an%0Aalternative%20to%20reward%20shaping%20for%20incorporating%20domain%20knowledge%20to%20accelerate%0Aagent%20training%2C%20thereby%20improving%20sample%20efficiency%20by%20directly%20shaping%0AQ-values.%20This%20approach%20is%20both%20general%20and%20robust%20across%20diverse%20tasks%2C%0Aallowing%20for%20immediate%20impact%20assessment%20while%20guaranteeing%20optimality.%20We%0Aevaluated%20Q-shaping%20across%2020%20different%20environments%20using%20a%20large%20language%0Amodel%20%28LLM%29%20as%20the%20heuristic%20provider.%20The%20results%20demonstrate%20that%20Q-shaping%0Asignificantly%20enhances%20sample%20efficiency%2C%20achieving%20a%20%5Ctextbf%7B16.87%5C%25%7D%0Aimprovement%20over%20the%20best%20baseline%20in%20each%20environment%20and%20a%20%5Ctextbf%7B253.80%5C%25%7D%0Aimprovement%20compared%20to%20LLM-based%20reward%20shaping%20methods.%20These%20findings%0Aestablish%20Q-shaping%20as%20a%20superior%20and%20unbiased%20alternative%20to%20conventional%0Areward%20shaping%20in%20reinforcement%20learning.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2410.01458v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DFrom%2520Reward%2520Shaping%2520to%2520Q-Shaping%253A%2520Achieving%2520Unbiased%2520Learning%2520with%250A%2520%2520LLM-Guided%2520Knowledge%26entry.906535625%3DXiefeng%2520Wu%26entry.1292438233%3D%2520%2520Q-shaping%2520is%2520an%2520extension%2520of%2520Q-value%2520initialization%2520and%2520serves%2520as%2520an%250Aalternative%2520to%2520reward%2520shaping%2520for%2520incorporating%2520domain%2520knowledge%2520to%2520accelerate%250Aagent%2520training%252C%2520thereby%2520improving%2520sample%2520efficiency%2520by%2520directly%2520shaping%250AQ-values.%2520This%2520approach%2520is%2520both%2520general%2520and%2520robust%2520across%2520diverse%2520tasks%252C%250Aallowing%2520for%2520immediate%2520impact%2520assessment%2520while%2520guaranteeing%2520optimality.%2520We%250Aevaluated%2520Q-shaping%2520across%252020%2520different%2520environments%2520using%2520a%2520large%2520language%250Amodel%2520%2528LLM%2529%2520as%2520the%2520heuristic%2520provider.%2520The%2520results%2520demonstrate%2520that%2520Q-shaping%250Asignificantly%2520enhances%2520sample%2520efficiency%252C%2520achieving%2520a%2520%255Ctextbf%257B16.87%255C%2525%257D%250Aimprovement%2520over%2520the%2520best%2520baseline%2520in%2520each%2520environment%2520and%2520a%2520%255Ctextbf%257B253.80%255C%2525%257D%250Aimprovement%2520compared%2520to%2520LLM-based%2520reward%2520shaping%2520methods.%2520These%2520findings%250Aestablish%2520Q-shaping%2520as%2520a%2520superior%2520and%2520unbiased%2520alternative%2520to%2520conventional%250Areward%2520shaping%2520in%2520reinforcement%2520learning.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2410.01458v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=From%20Reward%20Shaping%20to%20Q-Shaping%3A%20Achieving%20Unbiased%20Learning%20with%0A%20%20LLM-Guided%20Knowledge&entry.906535625=Xiefeng%20Wu&entry.1292438233=%20%20Q-shaping%20is%20an%20extension%20of%20Q-value%20initialization%20and%20serves%20as%20an%0Aalternative%20to%20reward%20shaping%20for%20incorporating%20domain%20knowledge%20to%20accelerate%0Aagent%20training%2C%20thereby%20improving%20sample%20efficiency%20by%20directly%20shaping%0AQ-values.%20This%20approach%20is%20both%20general%20and%20robust%20across%20diverse%20tasks%2C%0Aallowing%20for%20immediate%20impact%20assessment%20while%20guaranteeing%20optimality.%20We%0Aevaluated%20Q-shaping%20across%2020%20different%20environments%20using%20a%20large%20language%0Amodel%20%28LLM%29%20as%20the%20heuristic%20provider.%20The%20results%20demonstrate%20that%20Q-shaping%0Asignificantly%20enhances%20sample%20efficiency%2C%20achieving%20a%20%5Ctextbf%7B16.87%5C%25%7D%0Aimprovement%20over%20the%20best%20baseline%20in%20each%20environment%20and%20a%20%5Ctextbf%7B253.80%5C%25%7D%0Aimprovement%20compared%20to%20LLM-based%20reward%20shaping%20methods.%20These%20findings%0Aestablish%20Q-shaping%20as%20a%20superior%20and%20unbiased%20alternative%20to%20conventional%0Areward%20shaping%20in%20reinforcement%20learning.%0A&entry.1838667208=http%3A//arxiv.org/abs/2410.01458v1&entry.124074799=Read"},
{"title": "Truncated Kernel Stochastic Gradient Descent on Spheres", "author": "JinHui Bai and Lei Shi", "abstract": "  Inspired by the structure of spherical harmonics, we propose the truncated\nkernel stochastic gradient descent (T-kernel SGD) algorithm with a least-square\nloss function for spherical data fitting. T-kernel SGD employs a \"truncation\"\noperation, enabling the application of a series-based kernel function in\nstochastic gradient descent, thereby avoiding the difficulties of finding\nsuitable closed-form kernel functions in high-dimensional spaces. In contrast\nto traditional kernel SGD, T-kernel SGD is more effective in balancing bias and\nvariance by dynamically adjusting the hypothesis space during iterations. The\nmost significant advantage of the proposed algorithm is that it can achieve\ntheoretically optimal convergence rates using a constant step size (independent\nof the sample size) while overcoming the inherent saturation problem of kernel\nSGD. Additionally, we leverage the structure of spherical polynomials to derive\nan equivalent T-kernel SGD, significantly reducing storage and computational\ncosts compared to kernel SGD. Typically, T-kernel SGD requires only\n$\\mathcal{O}(n^{1+\\frac{d}{d-1}\\epsilon})$ computational complexity and\n$\\mathcal{O}(n^{\\frac{d}{d-1}\\epsilon})$ storage to achieve optimal rates for\nthe d-dimensional sphere, where $0<\\epsilon<\\frac{1}{2}$ can be arbitrarily\nsmall if the optimal fitting or the underlying space possesses sufficient\nregularity. This regularity is determined by the smoothness parameter of the\nobjective function and the decaying rate of the eigenvalues of the integral\noperator associated with the kernel function, both of which reflect the\ndifficulty of the estimation problem. Our main results quantitatively\ncharacterize how this prior information influences the convergence of T-kernel\nSGD. The numerical experiments further validate the theoretical findings\npresented in this paper.\n", "link": "http://arxiv.org/abs/2410.01570v1", "date": "2024-10-02", "relevancy": 2.3672, "topK": [{"title": "GST: Precise 3D Human Body from a Single Image with Gaussian Splatting\n  Transformers", "link": "http://arxiv.org/abs/2409.04196v1", "similarity": 0.4856}, {"title": "RTG-SLAM: Real-time 3D Reconstruction at Scale using Gaussian Splatting", "link": "http://arxiv.org/abs/2404.19706v1", "similarity": 0.4692}, {"title": "Dynamic Gaussians Mesh: Consistent Mesh Reconstruction from Monocular\n  Videos", "link": "http://arxiv.org/abs/2404.12379v2", "similarity": 0.4655}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Truncated%20Kernel%20Stochastic%20Gradient%20Descent%20on%20Spheres&body=Title%3A%20Truncated%20Kernel%20Stochastic%20Gradient%20Descent%20on%20Spheres%0AAuthor%3A%20JinHui%20Bai%20and%20Lei%20Shi%0AAbstract%3A%20%20%20Inspired%20by%20the%20structure%20of%20spherical%20harmonics%2C%20we%20propose%20the%20truncated%0Akernel%20stochastic%20gradient%20descent%20%28T-kernel%20SGD%29%20algorithm%20with%20a%20least-square%0Aloss%20function%20for%20spherical%20data%20fitting.%20T-kernel%20SGD%20employs%20a%20%22truncation%22%0Aoperation%2C%20enabling%20the%20application%20of%20a%20series-based%20kernel%20function%20in%0Astochastic%20gradient%20descent%2C%20thereby%20avoiding%20the%20difficulties%20of%20finding%0Asuitable%20closed-form%20kernel%20functions%20in%20high-dimensional%20spaces.%20In%20contrast%0Ato%20traditional%20kernel%20SGD%2C%20T-kernel%20SGD%20is%20more%20effective%20in%20balancing%20bias%20and%0Avariance%20by%20dynamically%20adjusting%20the%20hypothesis%20space%20during%20iterations.%20The%0Amost%20significant%20advantage%20of%20the%20proposed%20algorithm%20is%20that%20it%20can%20achieve%0Atheoretically%20optimal%20convergence%20rates%20using%20a%20constant%20step%20size%20%28independent%0Aof%20the%20sample%20size%29%20while%20overcoming%20the%20inherent%20saturation%20problem%20of%20kernel%0ASGD.%20Additionally%2C%20we%20leverage%20the%20structure%20of%20spherical%20polynomials%20to%20derive%0Aan%20equivalent%20T-kernel%20SGD%2C%20significantly%20reducing%20storage%20and%20computational%0Acosts%20compared%20to%20kernel%20SGD.%20Typically%2C%20T-kernel%20SGD%20requires%20only%0A%24%5Cmathcal%7BO%7D%28n%5E%7B1%2B%5Cfrac%7Bd%7D%7Bd-1%7D%5Cepsilon%7D%29%24%20computational%20complexity%20and%0A%24%5Cmathcal%7BO%7D%28n%5E%7B%5Cfrac%7Bd%7D%7Bd-1%7D%5Cepsilon%7D%29%24%20storage%20to%20achieve%20optimal%20rates%20for%0Athe%20d-dimensional%20sphere%2C%20where%20%240%3C%5Cepsilon%3C%5Cfrac%7B1%7D%7B2%7D%24%20can%20be%20arbitrarily%0Asmall%20if%20the%20optimal%20fitting%20or%20the%20underlying%20space%20possesses%20sufficient%0Aregularity.%20This%20regularity%20is%20determined%20by%20the%20smoothness%20parameter%20of%20the%0Aobjective%20function%20and%20the%20decaying%20rate%20of%20the%20eigenvalues%20of%20the%20integral%0Aoperator%20associated%20with%20the%20kernel%20function%2C%20both%20of%20which%20reflect%20the%0Adifficulty%20of%20the%20estimation%20problem.%20Our%20main%20results%20quantitatively%0Acharacterize%20how%20this%20prior%20information%20influences%20the%20convergence%20of%20T-kernel%0ASGD.%20The%20numerical%20experiments%20further%20validate%20the%20theoretical%20findings%0Apresented%20in%20this%20paper.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2410.01570v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DTruncated%2520Kernel%2520Stochastic%2520Gradient%2520Descent%2520on%2520Spheres%26entry.906535625%3DJinHui%2520Bai%2520and%2520Lei%2520Shi%26entry.1292438233%3D%2520%2520Inspired%2520by%2520the%2520structure%2520of%2520spherical%2520harmonics%252C%2520we%2520propose%2520the%2520truncated%250Akernel%2520stochastic%2520gradient%2520descent%2520%2528T-kernel%2520SGD%2529%2520algorithm%2520with%2520a%2520least-square%250Aloss%2520function%2520for%2520spherical%2520data%2520fitting.%2520T-kernel%2520SGD%2520employs%2520a%2520%2522truncation%2522%250Aoperation%252C%2520enabling%2520the%2520application%2520of%2520a%2520series-based%2520kernel%2520function%2520in%250Astochastic%2520gradient%2520descent%252C%2520thereby%2520avoiding%2520the%2520difficulties%2520of%2520finding%250Asuitable%2520closed-form%2520kernel%2520functions%2520in%2520high-dimensional%2520spaces.%2520In%2520contrast%250Ato%2520traditional%2520kernel%2520SGD%252C%2520T-kernel%2520SGD%2520is%2520more%2520effective%2520in%2520balancing%2520bias%2520and%250Avariance%2520by%2520dynamically%2520adjusting%2520the%2520hypothesis%2520space%2520during%2520iterations.%2520The%250Amost%2520significant%2520advantage%2520of%2520the%2520proposed%2520algorithm%2520is%2520that%2520it%2520can%2520achieve%250Atheoretically%2520optimal%2520convergence%2520rates%2520using%2520a%2520constant%2520step%2520size%2520%2528independent%250Aof%2520the%2520sample%2520size%2529%2520while%2520overcoming%2520the%2520inherent%2520saturation%2520problem%2520of%2520kernel%250ASGD.%2520Additionally%252C%2520we%2520leverage%2520the%2520structure%2520of%2520spherical%2520polynomials%2520to%2520derive%250Aan%2520equivalent%2520T-kernel%2520SGD%252C%2520significantly%2520reducing%2520storage%2520and%2520computational%250Acosts%2520compared%2520to%2520kernel%2520SGD.%2520Typically%252C%2520T-kernel%2520SGD%2520requires%2520only%250A%2524%255Cmathcal%257BO%257D%2528n%255E%257B1%252B%255Cfrac%257Bd%257D%257Bd-1%257D%255Cepsilon%257D%2529%2524%2520computational%2520complexity%2520and%250A%2524%255Cmathcal%257BO%257D%2528n%255E%257B%255Cfrac%257Bd%257D%257Bd-1%257D%255Cepsilon%257D%2529%2524%2520storage%2520to%2520achieve%2520optimal%2520rates%2520for%250Athe%2520d-dimensional%2520sphere%252C%2520where%2520%25240%253C%255Cepsilon%253C%255Cfrac%257B1%257D%257B2%257D%2524%2520can%2520be%2520arbitrarily%250Asmall%2520if%2520the%2520optimal%2520fitting%2520or%2520the%2520underlying%2520space%2520possesses%2520sufficient%250Aregularity.%2520This%2520regularity%2520is%2520determined%2520by%2520the%2520smoothness%2520parameter%2520of%2520the%250Aobjective%2520function%2520and%2520the%2520decaying%2520rate%2520of%2520the%2520eigenvalues%2520of%2520the%2520integral%250Aoperator%2520associated%2520with%2520the%2520kernel%2520function%252C%2520both%2520of%2520which%2520reflect%2520the%250Adifficulty%2520of%2520the%2520estimation%2520problem.%2520Our%2520main%2520results%2520quantitatively%250Acharacterize%2520how%2520this%2520prior%2520information%2520influences%2520the%2520convergence%2520of%2520T-kernel%250ASGD.%2520The%2520numerical%2520experiments%2520further%2520validate%2520the%2520theoretical%2520findings%250Apresented%2520in%2520this%2520paper.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2410.01570v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Truncated%20Kernel%20Stochastic%20Gradient%20Descent%20on%20Spheres&entry.906535625=JinHui%20Bai%20and%20Lei%20Shi&entry.1292438233=%20%20Inspired%20by%20the%20structure%20of%20spherical%20harmonics%2C%20we%20propose%20the%20truncated%0Akernel%20stochastic%20gradient%20descent%20%28T-kernel%20SGD%29%20algorithm%20with%20a%20least-square%0Aloss%20function%20for%20spherical%20data%20fitting.%20T-kernel%20SGD%20employs%20a%20%22truncation%22%0Aoperation%2C%20enabling%20the%20application%20of%20a%20series-based%20kernel%20function%20in%0Astochastic%20gradient%20descent%2C%20thereby%20avoiding%20the%20difficulties%20of%20finding%0Asuitable%20closed-form%20kernel%20functions%20in%20high-dimensional%20spaces.%20In%20contrast%0Ato%20traditional%20kernel%20SGD%2C%20T-kernel%20SGD%20is%20more%20effective%20in%20balancing%20bias%20and%0Avariance%20by%20dynamically%20adjusting%20the%20hypothesis%20space%20during%20iterations.%20The%0Amost%20significant%20advantage%20of%20the%20proposed%20algorithm%20is%20that%20it%20can%20achieve%0Atheoretically%20optimal%20convergence%20rates%20using%20a%20constant%20step%20size%20%28independent%0Aof%20the%20sample%20size%29%20while%20overcoming%20the%20inherent%20saturation%20problem%20of%20kernel%0ASGD.%20Additionally%2C%20we%20leverage%20the%20structure%20of%20spherical%20polynomials%20to%20derive%0Aan%20equivalent%20T-kernel%20SGD%2C%20significantly%20reducing%20storage%20and%20computational%0Acosts%20compared%20to%20kernel%20SGD.%20Typically%2C%20T-kernel%20SGD%20requires%20only%0A%24%5Cmathcal%7BO%7D%28n%5E%7B1%2B%5Cfrac%7Bd%7D%7Bd-1%7D%5Cepsilon%7D%29%24%20computational%20complexity%20and%0A%24%5Cmathcal%7BO%7D%28n%5E%7B%5Cfrac%7Bd%7D%7Bd-1%7D%5Cepsilon%7D%29%24%20storage%20to%20achieve%20optimal%20rates%20for%0Athe%20d-dimensional%20sphere%2C%20where%20%240%3C%5Cepsilon%3C%5Cfrac%7B1%7D%7B2%7D%24%20can%20be%20arbitrarily%0Asmall%20if%20the%20optimal%20fitting%20or%20the%20underlying%20space%20possesses%20sufficient%0Aregularity.%20This%20regularity%20is%20determined%20by%20the%20smoothness%20parameter%20of%20the%0Aobjective%20function%20and%20the%20decaying%20rate%20of%20the%20eigenvalues%20of%20the%20integral%0Aoperator%20associated%20with%20the%20kernel%20function%2C%20both%20of%20which%20reflect%20the%0Adifficulty%20of%20the%20estimation%20problem.%20Our%20main%20results%20quantitatively%0Acharacterize%20how%20this%20prior%20information%20influences%20the%20convergence%20of%20T-kernel%0ASGD.%20The%20numerical%20experiments%20further%20validate%20the%20theoretical%20findings%0Apresented%20in%20this%20paper.%0A&entry.1838667208=http%3A//arxiv.org/abs/2410.01570v1&entry.124074799=Read"},
{"title": "Training-Free Message Passing for Learning on Hypergraphs", "author": "Bohan Tang and Zexi Liu and Keyue Jiang and Siheng Chen and Xiaowen Dong", "abstract": "  Hypergraphs are crucial for modelling higher-order interactions in real-world\ndata. Hypergraph neural networks (HNNs) effectively utilise these structures by\nmessage passing to generate informative node features for various downstream\ntasks like node classification. However, the message passing module in existing\nHNNs typically requires a computationally intensive training process, which\nlimits their practical use. To tackle this challenge, we propose an alternative\napproach by decoupling the usage of hypergraph structural information from the\nmodel learning stage. This leads to a novel training-free message passing\nmodule, named TF-MP-Module, which can be precomputed in the data preprocessing\nstage, thereby reducing the computational burden. We refer to the hypergraph\nneural network equipped with our TF-MP-Module as TF-HNN. We theoretically\nsupport the efficiency and effectiveness of TF-HNN by showing that: 1) It is\nmore training-efficient compared to existing HNNs; 2) It utilises as much\ninformation as existing HNNs for node feature generation; and 3) It is robust\nagainst the oversmoothing issue while using long-range interactions.\nExperiments based on seven real-world hypergraph benchmarks in node\nclassification and hyperlink prediction show that, compared to state-of-the-art\nHNNs, TF-HNN exhibits both competitive performance and superior training\nefficiency. Specifically, on the large-scale benchmark, Trivago, TF-HNN\noutperforms the node classification accuracy of the best baseline by 10% with\njust 1% of the training time of that baseline.\n", "link": "http://arxiv.org/abs/2402.05569v4", "date": "2024-10-02", "relevancy": 2.3648, "topK": [{"title": "Boosted dynamic neural networks", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/26302", "similarity": 0.4919}, {"title": "Self-supervised Photographic Image Layout Representation Learning", "link": "http://arxiv.org/abs/2403.03740v1", "similarity": 0.4661}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.4609}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Training-Free%20Message%20Passing%20for%20Learning%20on%20Hypergraphs&body=Title%3A%20Training-Free%20Message%20Passing%20for%20Learning%20on%20Hypergraphs%0AAuthor%3A%20Bohan%20Tang%20and%20Zexi%20Liu%20and%20Keyue%20Jiang%20and%20Siheng%20Chen%20and%20Xiaowen%20Dong%0AAbstract%3A%20%20%20Hypergraphs%20are%20crucial%20for%20modelling%20higher-order%20interactions%20in%20real-world%0Adata.%20Hypergraph%20neural%20networks%20%28HNNs%29%20effectively%20utilise%20these%20structures%20by%0Amessage%20passing%20to%20generate%20informative%20node%20features%20for%20various%20downstream%0Atasks%20like%20node%20classification.%20However%2C%20the%20message%20passing%20module%20in%20existing%0AHNNs%20typically%20requires%20a%20computationally%20intensive%20training%20process%2C%20which%0Alimits%20their%20practical%20use.%20To%20tackle%20this%20challenge%2C%20we%20propose%20an%20alternative%0Aapproach%20by%20decoupling%20the%20usage%20of%20hypergraph%20structural%20information%20from%20the%0Amodel%20learning%20stage.%20This%20leads%20to%20a%20novel%20training-free%20message%20passing%0Amodule%2C%20named%20TF-MP-Module%2C%20which%20can%20be%20precomputed%20in%20the%20data%20preprocessing%0Astage%2C%20thereby%20reducing%20the%20computational%20burden.%20We%20refer%20to%20the%20hypergraph%0Aneural%20network%20equipped%20with%20our%20TF-MP-Module%20as%20TF-HNN.%20We%20theoretically%0Asupport%20the%20efficiency%20and%20effectiveness%20of%20TF-HNN%20by%20showing%20that%3A%201%29%20It%20is%0Amore%20training-efficient%20compared%20to%20existing%20HNNs%3B%202%29%20It%20utilises%20as%20much%0Ainformation%20as%20existing%20HNNs%20for%20node%20feature%20generation%3B%20and%203%29%20It%20is%20robust%0Aagainst%20the%20oversmoothing%20issue%20while%20using%20long-range%20interactions.%0AExperiments%20based%20on%20seven%20real-world%20hypergraph%20benchmarks%20in%20node%0Aclassification%20and%20hyperlink%20prediction%20show%20that%2C%20compared%20to%20state-of-the-art%0AHNNs%2C%20TF-HNN%20exhibits%20both%20competitive%20performance%20and%20superior%20training%0Aefficiency.%20Specifically%2C%20on%20the%20large-scale%20benchmark%2C%20Trivago%2C%20TF-HNN%0Aoutperforms%20the%20node%20classification%20accuracy%20of%20the%20best%20baseline%20by%2010%25%20with%0Ajust%201%25%20of%20the%20training%20time%20of%20that%20baseline.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2402.05569v4%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DTraining-Free%2520Message%2520Passing%2520for%2520Learning%2520on%2520Hypergraphs%26entry.906535625%3DBohan%2520Tang%2520and%2520Zexi%2520Liu%2520and%2520Keyue%2520Jiang%2520and%2520Siheng%2520Chen%2520and%2520Xiaowen%2520Dong%26entry.1292438233%3D%2520%2520Hypergraphs%2520are%2520crucial%2520for%2520modelling%2520higher-order%2520interactions%2520in%2520real-world%250Adata.%2520Hypergraph%2520neural%2520networks%2520%2528HNNs%2529%2520effectively%2520utilise%2520these%2520structures%2520by%250Amessage%2520passing%2520to%2520generate%2520informative%2520node%2520features%2520for%2520various%2520downstream%250Atasks%2520like%2520node%2520classification.%2520However%252C%2520the%2520message%2520passing%2520module%2520in%2520existing%250AHNNs%2520typically%2520requires%2520a%2520computationally%2520intensive%2520training%2520process%252C%2520which%250Alimits%2520their%2520practical%2520use.%2520To%2520tackle%2520this%2520challenge%252C%2520we%2520propose%2520an%2520alternative%250Aapproach%2520by%2520decoupling%2520the%2520usage%2520of%2520hypergraph%2520structural%2520information%2520from%2520the%250Amodel%2520learning%2520stage.%2520This%2520leads%2520to%2520a%2520novel%2520training-free%2520message%2520passing%250Amodule%252C%2520named%2520TF-MP-Module%252C%2520which%2520can%2520be%2520precomputed%2520in%2520the%2520data%2520preprocessing%250Astage%252C%2520thereby%2520reducing%2520the%2520computational%2520burden.%2520We%2520refer%2520to%2520the%2520hypergraph%250Aneural%2520network%2520equipped%2520with%2520our%2520TF-MP-Module%2520as%2520TF-HNN.%2520We%2520theoretically%250Asupport%2520the%2520efficiency%2520and%2520effectiveness%2520of%2520TF-HNN%2520by%2520showing%2520that%253A%25201%2529%2520It%2520is%250Amore%2520training-efficient%2520compared%2520to%2520existing%2520HNNs%253B%25202%2529%2520It%2520utilises%2520as%2520much%250Ainformation%2520as%2520existing%2520HNNs%2520for%2520node%2520feature%2520generation%253B%2520and%25203%2529%2520It%2520is%2520robust%250Aagainst%2520the%2520oversmoothing%2520issue%2520while%2520using%2520long-range%2520interactions.%250AExperiments%2520based%2520on%2520seven%2520real-world%2520hypergraph%2520benchmarks%2520in%2520node%250Aclassification%2520and%2520hyperlink%2520prediction%2520show%2520that%252C%2520compared%2520to%2520state-of-the-art%250AHNNs%252C%2520TF-HNN%2520exhibits%2520both%2520competitive%2520performance%2520and%2520superior%2520training%250Aefficiency.%2520Specifically%252C%2520on%2520the%2520large-scale%2520benchmark%252C%2520Trivago%252C%2520TF-HNN%250Aoutperforms%2520the%2520node%2520classification%2520accuracy%2520of%2520the%2520best%2520baseline%2520by%252010%2525%2520with%250Ajust%25201%2525%2520of%2520the%2520training%2520time%2520of%2520that%2520baseline.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2402.05569v4%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Training-Free%20Message%20Passing%20for%20Learning%20on%20Hypergraphs&entry.906535625=Bohan%20Tang%20and%20Zexi%20Liu%20and%20Keyue%20Jiang%20and%20Siheng%20Chen%20and%20Xiaowen%20Dong&entry.1292438233=%20%20Hypergraphs%20are%20crucial%20for%20modelling%20higher-order%20interactions%20in%20real-world%0Adata.%20Hypergraph%20neural%20networks%20%28HNNs%29%20effectively%20utilise%20these%20structures%20by%0Amessage%20passing%20to%20generate%20informative%20node%20features%20for%20various%20downstream%0Atasks%20like%20node%20classification.%20However%2C%20the%20message%20passing%20module%20in%20existing%0AHNNs%20typically%20requires%20a%20computationally%20intensive%20training%20process%2C%20which%0Alimits%20their%20practical%20use.%20To%20tackle%20this%20challenge%2C%20we%20propose%20an%20alternative%0Aapproach%20by%20decoupling%20the%20usage%20of%20hypergraph%20structural%20information%20from%20the%0Amodel%20learning%20stage.%20This%20leads%20to%20a%20novel%20training-free%20message%20passing%0Amodule%2C%20named%20TF-MP-Module%2C%20which%20can%20be%20precomputed%20in%20the%20data%20preprocessing%0Astage%2C%20thereby%20reducing%20the%20computational%20burden.%20We%20refer%20to%20the%20hypergraph%0Aneural%20network%20equipped%20with%20our%20TF-MP-Module%20as%20TF-HNN.%20We%20theoretically%0Asupport%20the%20efficiency%20and%20effectiveness%20of%20TF-HNN%20by%20showing%20that%3A%201%29%20It%20is%0Amore%20training-efficient%20compared%20to%20existing%20HNNs%3B%202%29%20It%20utilises%20as%20much%0Ainformation%20as%20existing%20HNNs%20for%20node%20feature%20generation%3B%20and%203%29%20It%20is%20robust%0Aagainst%20the%20oversmoothing%20issue%20while%20using%20long-range%20interactions.%0AExperiments%20based%20on%20seven%20real-world%20hypergraph%20benchmarks%20in%20node%0Aclassification%20and%20hyperlink%20prediction%20show%20that%2C%20compared%20to%20state-of-the-art%0AHNNs%2C%20TF-HNN%20exhibits%20both%20competitive%20performance%20and%20superior%20training%0Aefficiency.%20Specifically%2C%20on%20the%20large-scale%20benchmark%2C%20Trivago%2C%20TF-HNN%0Aoutperforms%20the%20node%20classification%20accuracy%20of%20the%20best%20baseline%20by%2010%25%20with%0Ajust%201%25%20of%20the%20training%20time%20of%20that%20baseline.%0A&entry.1838667208=http%3A//arxiv.org/abs/2402.05569v4&entry.124074799=Read"},
{"title": "VLM-Auto: VLM-based Autonomous Driving Assistant with Human-like\n  Behavior and Understanding for Complex Road Scenes", "author": "Ziang Guo and Zakhar Yagudin and Artem Lykov and Mikhail Konenkov and Dzmitry Tsetserukou", "abstract": "  Recent research on Large Language Models for autonomous driving shows promise\nin planning and control. However, high computational demands and hallucinations\nstill challenge accurate trajectory prediction and control signal generation.\nDeterministic algorithms offer reliability but lack adaptability to complex\ndriving scenarios and struggle with context and uncertainty. To address this\nproblem, we propose VLM-Auto, a novel autonomous driving assistant system to\nempower the autonomous vehicles with adjustable driving behaviors based on the\nunderstanding of road scenes. A pipeline involving the CARLA simulator and\nRobot Operating System 2 (ROS2) verifying the effectiveness of our system is\npresented, utilizing a single Nvidia 4090 24G GPU while exploiting the capacity\nof textual output of the Visual Language Model (VLM). Besides, we also\ncontribute a dataset containing an image set and a corresponding prompt set for\nfine-tuning the VLM module of our system. In CARLA experiments, our system\nachieved $97.82\\%$ average precision on 5 types of labels in our dataset. In\nthe real-world driving dataset, our system achieved $96.97\\%$ prediction\naccuracy in night scenes and gloomy scenes. Our VLM-Auto dataset will be\nreleased at https://github.com/ZionGo6/VLM-Auto.\n", "link": "http://arxiv.org/abs/2405.05885v3", "date": "2024-10-02", "relevancy": 2.3191, "topK": [{"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.6006}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.5792}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5592}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20VLM-Auto%3A%20VLM-based%20Autonomous%20Driving%20Assistant%20with%20Human-like%0A%20%20Behavior%20and%20Understanding%20for%20Complex%20Road%20Scenes&body=Title%3A%20VLM-Auto%3A%20VLM-based%20Autonomous%20Driving%20Assistant%20with%20Human-like%0A%20%20Behavior%20and%20Understanding%20for%20Complex%20Road%20Scenes%0AAuthor%3A%20Ziang%20Guo%20and%20Zakhar%20Yagudin%20and%20Artem%20Lykov%20and%20Mikhail%20Konenkov%20and%20Dzmitry%20Tsetserukou%0AAbstract%3A%20%20%20Recent%20research%20on%20Large%20Language%20Models%20for%20autonomous%20driving%20shows%20promise%0Ain%20planning%20and%20control.%20However%2C%20high%20computational%20demands%20and%20hallucinations%0Astill%20challenge%20accurate%20trajectory%20prediction%20and%20control%20signal%20generation.%0ADeterministic%20algorithms%20offer%20reliability%20but%20lack%20adaptability%20to%20complex%0Adriving%20scenarios%20and%20struggle%20with%20context%20and%20uncertainty.%20To%20address%20this%0Aproblem%2C%20we%20propose%20VLM-Auto%2C%20a%20novel%20autonomous%20driving%20assistant%20system%20to%0Aempower%20the%20autonomous%20vehicles%20with%20adjustable%20driving%20behaviors%20based%20on%20the%0Aunderstanding%20of%20road%20scenes.%20A%20pipeline%20involving%20the%20CARLA%20simulator%20and%0ARobot%20Operating%20System%202%20%28ROS2%29%20verifying%20the%20effectiveness%20of%20our%20system%20is%0Apresented%2C%20utilizing%20a%20single%20Nvidia%204090%2024G%20GPU%20while%20exploiting%20the%20capacity%0Aof%20textual%20output%20of%20the%20Visual%20Language%20Model%20%28VLM%29.%20Besides%2C%20we%20also%0Acontribute%20a%20dataset%20containing%20an%20image%20set%20and%20a%20corresponding%20prompt%20set%20for%0Afine-tuning%20the%20VLM%20module%20of%20our%20system.%20In%20CARLA%20experiments%2C%20our%20system%0Aachieved%20%2497.82%5C%25%24%20average%20precision%20on%205%20types%20of%20labels%20in%20our%20dataset.%20In%0Athe%20real-world%20driving%20dataset%2C%20our%20system%20achieved%20%2496.97%5C%25%24%20prediction%0Aaccuracy%20in%20night%20scenes%20and%20gloomy%20scenes.%20Our%20VLM-Auto%20dataset%20will%20be%0Areleased%20at%20https%3A//github.com/ZionGo6/VLM-Auto.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2405.05885v3%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DVLM-Auto%253A%2520VLM-based%2520Autonomous%2520Driving%2520Assistant%2520with%2520Human-like%250A%2520%2520Behavior%2520and%2520Understanding%2520for%2520Complex%2520Road%2520Scenes%26entry.906535625%3DZiang%2520Guo%2520and%2520Zakhar%2520Yagudin%2520and%2520Artem%2520Lykov%2520and%2520Mikhail%2520Konenkov%2520and%2520Dzmitry%2520Tsetserukou%26entry.1292438233%3D%2520%2520Recent%2520research%2520on%2520Large%2520Language%2520Models%2520for%2520autonomous%2520driving%2520shows%2520promise%250Ain%2520planning%2520and%2520control.%2520However%252C%2520high%2520computational%2520demands%2520and%2520hallucinations%250Astill%2520challenge%2520accurate%2520trajectory%2520prediction%2520and%2520control%2520signal%2520generation.%250ADeterministic%2520algorithms%2520offer%2520reliability%2520but%2520lack%2520adaptability%2520to%2520complex%250Adriving%2520scenarios%2520and%2520struggle%2520with%2520context%2520and%2520uncertainty.%2520To%2520address%2520this%250Aproblem%252C%2520we%2520propose%2520VLM-Auto%252C%2520a%2520novel%2520autonomous%2520driving%2520assistant%2520system%2520to%250Aempower%2520the%2520autonomous%2520vehicles%2520with%2520adjustable%2520driving%2520behaviors%2520based%2520on%2520the%250Aunderstanding%2520of%2520road%2520scenes.%2520A%2520pipeline%2520involving%2520the%2520CARLA%2520simulator%2520and%250ARobot%2520Operating%2520System%25202%2520%2528ROS2%2529%2520verifying%2520the%2520effectiveness%2520of%2520our%2520system%2520is%250Apresented%252C%2520utilizing%2520a%2520single%2520Nvidia%25204090%252024G%2520GPU%2520while%2520exploiting%2520the%2520capacity%250Aof%2520textual%2520output%2520of%2520the%2520Visual%2520Language%2520Model%2520%2528VLM%2529.%2520Besides%252C%2520we%2520also%250Acontribute%2520a%2520dataset%2520containing%2520an%2520image%2520set%2520and%2520a%2520corresponding%2520prompt%2520set%2520for%250Afine-tuning%2520the%2520VLM%2520module%2520of%2520our%2520system.%2520In%2520CARLA%2520experiments%252C%2520our%2520system%250Aachieved%2520%252497.82%255C%2525%2524%2520average%2520precision%2520on%25205%2520types%2520of%2520labels%2520in%2520our%2520dataset.%2520In%250Athe%2520real-world%2520driving%2520dataset%252C%2520our%2520system%2520achieved%2520%252496.97%255C%2525%2524%2520prediction%250Aaccuracy%2520in%2520night%2520scenes%2520and%2520gloomy%2520scenes.%2520Our%2520VLM-Auto%2520dataset%2520will%2520be%250Areleased%2520at%2520https%253A//github.com/ZionGo6/VLM-Auto.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2405.05885v3%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=VLM-Auto%3A%20VLM-based%20Autonomous%20Driving%20Assistant%20with%20Human-like%0A%20%20Behavior%20and%20Understanding%20for%20Complex%20Road%20Scenes&entry.906535625=Ziang%20Guo%20and%20Zakhar%20Yagudin%20and%20Artem%20Lykov%20and%20Mikhail%20Konenkov%20and%20Dzmitry%20Tsetserukou&entry.1292438233=%20%20Recent%20research%20on%20Large%20Language%20Models%20for%20autonomous%20driving%20shows%20promise%0Ain%20planning%20and%20control.%20However%2C%20high%20computational%20demands%20and%20hallucinations%0Astill%20challenge%20accurate%20trajectory%20prediction%20and%20control%20signal%20generation.%0ADeterministic%20algorithms%20offer%20reliability%20but%20lack%20adaptability%20to%20complex%0Adriving%20scenarios%20and%20struggle%20with%20context%20and%20uncertainty.%20To%20address%20this%0Aproblem%2C%20we%20propose%20VLM-Auto%2C%20a%20novel%20autonomous%20driving%20assistant%20system%20to%0Aempower%20the%20autonomous%20vehicles%20with%20adjustable%20driving%20behaviors%20based%20on%20the%0Aunderstanding%20of%20road%20scenes.%20A%20pipeline%20involving%20the%20CARLA%20simulator%20and%0ARobot%20Operating%20System%202%20%28ROS2%29%20verifying%20the%20effectiveness%20of%20our%20system%20is%0Apresented%2C%20utilizing%20a%20single%20Nvidia%204090%2024G%20GPU%20while%20exploiting%20the%20capacity%0Aof%20textual%20output%20of%20the%20Visual%20Language%20Model%20%28VLM%29.%20Besides%2C%20we%20also%0Acontribute%20a%20dataset%20containing%20an%20image%20set%20and%20a%20corresponding%20prompt%20set%20for%0Afine-tuning%20the%20VLM%20module%20of%20our%20system.%20In%20CARLA%20experiments%2C%20our%20system%0Aachieved%20%2497.82%5C%25%24%20average%20precision%20on%205%20types%20of%20labels%20in%20our%20dataset.%20In%0Athe%20real-world%20driving%20dataset%2C%20our%20system%20achieved%20%2496.97%5C%25%24%20prediction%0Aaccuracy%20in%20night%20scenes%20and%20gloomy%20scenes.%20Our%20VLM-Auto%20dataset%20will%20be%0Areleased%20at%20https%3A//github.com/ZionGo6/VLM-Auto.%0A&entry.1838667208=http%3A//arxiv.org/abs/2405.05885v3&entry.124074799=Read"},
{"title": "AgentStudio: A Toolkit for Building General Virtual Agents", "author": "Longtao Zheng and Zhiyuan Huang and Zhenghai Xue and Xinrun Wang and Bo An and Shuicheng Yan", "abstract": "  General virtual agents need to handle multimodal observations, master complex\naction spaces, and self-improve in dynamic, open-domain environments. However,\nexisting environments are often domain-specific and require complex setups,\nwhich limits agent development and evaluation in real-world settings. As a\nresult, current evaluations lack in-depth analyses that decompose fundamental\nagent capabilities. We introduce AgentStudio, a trinity of environments, tools,\nand benchmarks to address these issues. AgentStudio provides a lightweight,\ninteractive environment with highly generic observation and action spaces,\ne.g., video observations and GUI/API actions. It integrates tools for creating\nonline benchmark tasks, annotating GUI elements, and labeling actions in\nvideos. Based on our environment and tools, we curate an online task suite that\nbenchmarks both GUI interactions and function calling with efficient\nauto-evaluation. We also reorganize existing datasets and collect new ones\nusing our tools to establish three datasets: GroundUI, IDMBench, and\nCriticBench. These datasets evaluate fundamental agent abilities, including GUI\ngrounding, learning from videos, and success detection, pointing to the\ndesiderata for robust, general, and open-ended virtual agents.\n", "link": "http://arxiv.org/abs/2403.17918v2", "date": "2024-10-02", "relevancy": 2.3053, "topK": [{"title": "Genie: Generative Interactive Environments", "link": "http://arxiv.org/abs/2402.15391v1", "similarity": 0.6238}, {"title": "RoboEXP: Action-Conditioned Scene Graph via Interactive Exploration for\n  Robotic Manipulation", "link": "http://arxiv.org/abs/2402.15487v1", "similarity": 0.5947}, {"title": "Safe and efficient training of a control agent", "link": "https://patents.google.com/patent/US11709462B2/en", "similarity": 0.5215}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20AgentStudio%3A%20A%20Toolkit%20for%20Building%20General%20Virtual%20Agents&body=Title%3A%20AgentStudio%3A%20A%20Toolkit%20for%20Building%20General%20Virtual%20Agents%0AAuthor%3A%20Longtao%20Zheng%20and%20Zhiyuan%20Huang%20and%20Zhenghai%20Xue%20and%20Xinrun%20Wang%20and%20Bo%20An%20and%20Shuicheng%20Yan%0AAbstract%3A%20%20%20General%20virtual%20agents%20need%20to%20handle%20multimodal%20observations%2C%20master%20complex%0Aaction%20spaces%2C%20and%20self-improve%20in%20dynamic%2C%20open-domain%20environments.%20However%2C%0Aexisting%20environments%20are%20often%20domain-specific%20and%20require%20complex%20setups%2C%0Awhich%20limits%20agent%20development%20and%20evaluation%20in%20real-world%20settings.%20As%20a%0Aresult%2C%20current%20evaluations%20lack%20in-depth%20analyses%20that%20decompose%20fundamental%0Aagent%20capabilities.%20We%20introduce%20AgentStudio%2C%20a%20trinity%20of%20environments%2C%20tools%2C%0Aand%20benchmarks%20to%20address%20these%20issues.%20AgentStudio%20provides%20a%20lightweight%2C%0Ainteractive%20environment%20with%20highly%20generic%20observation%20and%20action%20spaces%2C%0Ae.g.%2C%20video%20observations%20and%20GUI/API%20actions.%20It%20integrates%20tools%20for%20creating%0Aonline%20benchmark%20tasks%2C%20annotating%20GUI%20elements%2C%20and%20labeling%20actions%20in%0Avideos.%20Based%20on%20our%20environment%20and%20tools%2C%20we%20curate%20an%20online%20task%20suite%20that%0Abenchmarks%20both%20GUI%20interactions%20and%20function%20calling%20with%20efficient%0Aauto-evaluation.%20We%20also%20reorganize%20existing%20datasets%20and%20collect%20new%20ones%0Ausing%20our%20tools%20to%20establish%20three%20datasets%3A%20GroundUI%2C%20IDMBench%2C%20and%0ACriticBench.%20These%20datasets%20evaluate%20fundamental%20agent%20abilities%2C%20including%20GUI%0Agrounding%2C%20learning%20from%20videos%2C%20and%20success%20detection%2C%20pointing%20to%20the%0Adesiderata%20for%20robust%2C%20general%2C%20and%20open-ended%20virtual%20agents.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2403.17918v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DAgentStudio%253A%2520A%2520Toolkit%2520for%2520Building%2520General%2520Virtual%2520Agents%26entry.906535625%3DLongtao%2520Zheng%2520and%2520Zhiyuan%2520Huang%2520and%2520Zhenghai%2520Xue%2520and%2520Xinrun%2520Wang%2520and%2520Bo%2520An%2520and%2520Shuicheng%2520Yan%26entry.1292438233%3D%2520%2520General%2520virtual%2520agents%2520need%2520to%2520handle%2520multimodal%2520observations%252C%2520master%2520complex%250Aaction%2520spaces%252C%2520and%2520self-improve%2520in%2520dynamic%252C%2520open-domain%2520environments.%2520However%252C%250Aexisting%2520environments%2520are%2520often%2520domain-specific%2520and%2520require%2520complex%2520setups%252C%250Awhich%2520limits%2520agent%2520development%2520and%2520evaluation%2520in%2520real-world%2520settings.%2520As%2520a%250Aresult%252C%2520current%2520evaluations%2520lack%2520in-depth%2520analyses%2520that%2520decompose%2520fundamental%250Aagent%2520capabilities.%2520We%2520introduce%2520AgentStudio%252C%2520a%2520trinity%2520of%2520environments%252C%2520tools%252C%250Aand%2520benchmarks%2520to%2520address%2520these%2520issues.%2520AgentStudio%2520provides%2520a%2520lightweight%252C%250Ainteractive%2520environment%2520with%2520highly%2520generic%2520observation%2520and%2520action%2520spaces%252C%250Ae.g.%252C%2520video%2520observations%2520and%2520GUI/API%2520actions.%2520It%2520integrates%2520tools%2520for%2520creating%250Aonline%2520benchmark%2520tasks%252C%2520annotating%2520GUI%2520elements%252C%2520and%2520labeling%2520actions%2520in%250Avideos.%2520Based%2520on%2520our%2520environment%2520and%2520tools%252C%2520we%2520curate%2520an%2520online%2520task%2520suite%2520that%250Abenchmarks%2520both%2520GUI%2520interactions%2520and%2520function%2520calling%2520with%2520efficient%250Aauto-evaluation.%2520We%2520also%2520reorganize%2520existing%2520datasets%2520and%2520collect%2520new%2520ones%250Ausing%2520our%2520tools%2520to%2520establish%2520three%2520datasets%253A%2520GroundUI%252C%2520IDMBench%252C%2520and%250ACriticBench.%2520These%2520datasets%2520evaluate%2520fundamental%2520agent%2520abilities%252C%2520including%2520GUI%250Agrounding%252C%2520learning%2520from%2520videos%252C%2520and%2520success%2520detection%252C%2520pointing%2520to%2520the%250Adesiderata%2520for%2520robust%252C%2520general%252C%2520and%2520open-ended%2520virtual%2520agents.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2403.17918v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=AgentStudio%3A%20A%20Toolkit%20for%20Building%20General%20Virtual%20Agents&entry.906535625=Longtao%20Zheng%20and%20Zhiyuan%20Huang%20and%20Zhenghai%20Xue%20and%20Xinrun%20Wang%20and%20Bo%20An%20and%20Shuicheng%20Yan&entry.1292438233=%20%20General%20virtual%20agents%20need%20to%20handle%20multimodal%20observations%2C%20master%20complex%0Aaction%20spaces%2C%20and%20self-improve%20in%20dynamic%2C%20open-domain%20environments.%20However%2C%0Aexisting%20environments%20are%20often%20domain-specific%20and%20require%20complex%20setups%2C%0Awhich%20limits%20agent%20development%20and%20evaluation%20in%20real-world%20settings.%20As%20a%0Aresult%2C%20current%20evaluations%20lack%20in-depth%20analyses%20that%20decompose%20fundamental%0Aagent%20capabilities.%20We%20introduce%20AgentStudio%2C%20a%20trinity%20of%20environments%2C%20tools%2C%0Aand%20benchmarks%20to%20address%20these%20issues.%20AgentStudio%20provides%20a%20lightweight%2C%0Ainteractive%20environment%20with%20highly%20generic%20observation%20and%20action%20spaces%2C%0Ae.g.%2C%20video%20observations%20and%20GUI/API%20actions.%20It%20integrates%20tools%20for%20creating%0Aonline%20benchmark%20tasks%2C%20annotating%20GUI%20elements%2C%20and%20labeling%20actions%20in%0Avideos.%20Based%20on%20our%20environment%20and%20tools%2C%20we%20curate%20an%20online%20task%20suite%20that%0Abenchmarks%20both%20GUI%20interactions%20and%20function%20calling%20with%20efficient%0Aauto-evaluation.%20We%20also%20reorganize%20existing%20datasets%20and%20collect%20new%20ones%0Ausing%20our%20tools%20to%20establish%20three%20datasets%3A%20GroundUI%2C%20IDMBench%2C%20and%0ACriticBench.%20These%20datasets%20evaluate%20fundamental%20agent%20abilities%2C%20including%20GUI%0Agrounding%2C%20learning%20from%20videos%2C%20and%20success%20detection%2C%20pointing%20to%20the%0Adesiderata%20for%20robust%2C%20general%2C%20and%20open-ended%20virtual%20agents.%0A&entry.1838667208=http%3A//arxiv.org/abs/2403.17918v2&entry.124074799=Read"},
{"title": "Image Copy Detection for Diffusion Models", "author": "Wenhao Wang and Yifan Sun and Zhentao Tan and Yi Yang", "abstract": "  Images produced by diffusion models are increasingly popular in digital\nartwork and visual marketing. However, such generated images might replicate\ncontent from existing ones and pose the challenge of content originality.\nExisting Image Copy Detection (ICD) models, though accurate in detecting\nhand-crafted replicas, overlook the challenge from diffusion models. This\nmotivates us to introduce ICDiff, the first ICD specialized for diffusion\nmodels. To this end, we construct a Diffusion-Replication (D-Rep) dataset and\ncorrespondingly propose a novel deep embedding method. D-Rep uses a\nstate-of-the-art diffusion model (Stable Diffusion V1.5) to generate 40, 000\nimage-replica pairs, which are manually annotated into 6 replication levels\nranging from 0 (no replication) to 5 (total replication). Our method,\nPDF-Embedding, transforms the replication level of each image-replica pair into\na probability density function (PDF) as the supervision signal. The intuition\nis that the probability of neighboring replication levels should be continuous\nand smooth. Experimental results show that PDF-Embedding surpasses\nprotocol-driven methods and non-PDF choices on the D-Rep test set. Moreover, by\nutilizing PDF-Embedding, we find that the replication ratios of well-known\ndiffusion models against an open-source gallery range from 10% to 20%. The\nproject is publicly available at https://icdiff.github.io/.\n", "link": "http://arxiv.org/abs/2409.19952v2", "date": "2024-10-02", "relevancy": 2.3045, "topK": [{"title": "SDXS: Real-Time One-Step Latent Diffusion Models with Image Conditions", "link": "http://arxiv.org/abs/2403.16627v1", "similarity": 0.6104}, {"title": "VirtualModel: Generating Object-ID-retentive Human-object Interaction\n  Image by Diffusion Model for E-commerce Marketing", "link": "http://arxiv.org/abs/2405.09985v1", "similarity": 0.572}, {"title": "Magic-Me: Identity-Specific Video Customized Diffusion", "link": "http://arxiv.org/abs/2402.09368v1", "similarity": 0.5665}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Image%20Copy%20Detection%20for%20Diffusion%20Models&body=Title%3A%20Image%20Copy%20Detection%20for%20Diffusion%20Models%0AAuthor%3A%20Wenhao%20Wang%20and%20Yifan%20Sun%20and%20Zhentao%20Tan%20and%20Yi%20Yang%0AAbstract%3A%20%20%20Images%20produced%20by%20diffusion%20models%20are%20increasingly%20popular%20in%20digital%0Aartwork%20and%20visual%20marketing.%20However%2C%20such%20generated%20images%20might%20replicate%0Acontent%20from%20existing%20ones%20and%20pose%20the%20challenge%20of%20content%20originality.%0AExisting%20Image%20Copy%20Detection%20%28ICD%29%20models%2C%20though%20accurate%20in%20detecting%0Ahand-crafted%20replicas%2C%20overlook%20the%20challenge%20from%20diffusion%20models.%20This%0Amotivates%20us%20to%20introduce%20ICDiff%2C%20the%20first%20ICD%20specialized%20for%20diffusion%0Amodels.%20To%20this%20end%2C%20we%20construct%20a%20Diffusion-Replication%20%28D-Rep%29%20dataset%20and%0Acorrespondingly%20propose%20a%20novel%20deep%20embedding%20method.%20D-Rep%20uses%20a%0Astate-of-the-art%20diffusion%20model%20%28Stable%20Diffusion%20V1.5%29%20to%20generate%2040%2C%20000%0Aimage-replica%20pairs%2C%20which%20are%20manually%20annotated%20into%206%20replication%20levels%0Aranging%20from%200%20%28no%20replication%29%20to%205%20%28total%20replication%29.%20Our%20method%2C%0APDF-Embedding%2C%20transforms%20the%20replication%20level%20of%20each%20image-replica%20pair%20into%0Aa%20probability%20density%20function%20%28PDF%29%20as%20the%20supervision%20signal.%20The%20intuition%0Ais%20that%20the%20probability%20of%20neighboring%20replication%20levels%20should%20be%20continuous%0Aand%20smooth.%20Experimental%20results%20show%20that%20PDF-Embedding%20surpasses%0Aprotocol-driven%20methods%20and%20non-PDF%20choices%20on%20the%20D-Rep%20test%20set.%20Moreover%2C%20by%0Autilizing%20PDF-Embedding%2C%20we%20find%20that%20the%20replication%20ratios%20of%20well-known%0Adiffusion%20models%20against%20an%20open-source%20gallery%20range%20from%2010%25%20to%2020%25.%20The%0Aproject%20is%20publicly%20available%20at%20https%3A//icdiff.github.io/.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2409.19952v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DImage%2520Copy%2520Detection%2520for%2520Diffusion%2520Models%26entry.906535625%3DWenhao%2520Wang%2520and%2520Yifan%2520Sun%2520and%2520Zhentao%2520Tan%2520and%2520Yi%2520Yang%26entry.1292438233%3D%2520%2520Images%2520produced%2520by%2520diffusion%2520models%2520are%2520increasingly%2520popular%2520in%2520digital%250Aartwork%2520and%2520visual%2520marketing.%2520However%252C%2520such%2520generated%2520images%2520might%2520replicate%250Acontent%2520from%2520existing%2520ones%2520and%2520pose%2520the%2520challenge%2520of%2520content%2520originality.%250AExisting%2520Image%2520Copy%2520Detection%2520%2528ICD%2529%2520models%252C%2520though%2520accurate%2520in%2520detecting%250Ahand-crafted%2520replicas%252C%2520overlook%2520the%2520challenge%2520from%2520diffusion%2520models.%2520This%250Amotivates%2520us%2520to%2520introduce%2520ICDiff%252C%2520the%2520first%2520ICD%2520specialized%2520for%2520diffusion%250Amodels.%2520To%2520this%2520end%252C%2520we%2520construct%2520a%2520Diffusion-Replication%2520%2528D-Rep%2529%2520dataset%2520and%250Acorrespondingly%2520propose%2520a%2520novel%2520deep%2520embedding%2520method.%2520D-Rep%2520uses%2520a%250Astate-of-the-art%2520diffusion%2520model%2520%2528Stable%2520Diffusion%2520V1.5%2529%2520to%2520generate%252040%252C%2520000%250Aimage-replica%2520pairs%252C%2520which%2520are%2520manually%2520annotated%2520into%25206%2520replication%2520levels%250Aranging%2520from%25200%2520%2528no%2520replication%2529%2520to%25205%2520%2528total%2520replication%2529.%2520Our%2520method%252C%250APDF-Embedding%252C%2520transforms%2520the%2520replication%2520level%2520of%2520each%2520image-replica%2520pair%2520into%250Aa%2520probability%2520density%2520function%2520%2528PDF%2529%2520as%2520the%2520supervision%2520signal.%2520The%2520intuition%250Ais%2520that%2520the%2520probability%2520of%2520neighboring%2520replication%2520levels%2520should%2520be%2520continuous%250Aand%2520smooth.%2520Experimental%2520results%2520show%2520that%2520PDF-Embedding%2520surpasses%250Aprotocol-driven%2520methods%2520and%2520non-PDF%2520choices%2520on%2520the%2520D-Rep%2520test%2520set.%2520Moreover%252C%2520by%250Autilizing%2520PDF-Embedding%252C%2520we%2520find%2520that%2520the%2520replication%2520ratios%2520of%2520well-known%250Adiffusion%2520models%2520against%2520an%2520open-source%2520gallery%2520range%2520from%252010%2525%2520to%252020%2525.%2520The%250Aproject%2520is%2520publicly%2520available%2520at%2520https%253A//icdiff.github.io/.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2409.19952v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Image%20Copy%20Detection%20for%20Diffusion%20Models&entry.906535625=Wenhao%20Wang%20and%20Yifan%20Sun%20and%20Zhentao%20Tan%20and%20Yi%20Yang&entry.1292438233=%20%20Images%20produced%20by%20diffusion%20models%20are%20increasingly%20popular%20in%20digital%0Aartwork%20and%20visual%20marketing.%20However%2C%20such%20generated%20images%20might%20replicate%0Acontent%20from%20existing%20ones%20and%20pose%20the%20challenge%20of%20content%20originality.%0AExisting%20Image%20Copy%20Detection%20%28ICD%29%20models%2C%20though%20accurate%20in%20detecting%0Ahand-crafted%20replicas%2C%20overlook%20the%20challenge%20from%20diffusion%20models.%20This%0Amotivates%20us%20to%20introduce%20ICDiff%2C%20the%20first%20ICD%20specialized%20for%20diffusion%0Amodels.%20To%20this%20end%2C%20we%20construct%20a%20Diffusion-Replication%20%28D-Rep%29%20dataset%20and%0Acorrespondingly%20propose%20a%20novel%20deep%20embedding%20method.%20D-Rep%20uses%20a%0Astate-of-the-art%20diffusion%20model%20%28Stable%20Diffusion%20V1.5%29%20to%20generate%2040%2C%20000%0Aimage-replica%20pairs%2C%20which%20are%20manually%20annotated%20into%206%20replication%20levels%0Aranging%20from%200%20%28no%20replication%29%20to%205%20%28total%20replication%29.%20Our%20method%2C%0APDF-Embedding%2C%20transforms%20the%20replication%20level%20of%20each%20image-replica%20pair%20into%0Aa%20probability%20density%20function%20%28PDF%29%20as%20the%20supervision%20signal.%20The%20intuition%0Ais%20that%20the%20probability%20of%20neighboring%20replication%20levels%20should%20be%20continuous%0Aand%20smooth.%20Experimental%20results%20show%20that%20PDF-Embedding%20surpasses%0Aprotocol-driven%20methods%20and%20non-PDF%20choices%20on%20the%20D-Rep%20test%20set.%20Moreover%2C%20by%0Autilizing%20PDF-Embedding%2C%20we%20find%20that%20the%20replication%20ratios%20of%20well-known%0Adiffusion%20models%20against%20an%20open-source%20gallery%20range%20from%2010%25%20to%2020%25.%20The%0Aproject%20is%20publicly%20available%20at%20https%3A//icdiff.github.io/.%0A&entry.1838667208=http%3A//arxiv.org/abs/2409.19952v2&entry.124074799=Read"},
{"title": "Data Extrapolation for Text-to-image Generation on Small Datasets", "author": "Senmao Ye and Fei Liu", "abstract": "  Text-to-image generation requires large amount of training data to\nsynthesizing high-quality images. For augmenting training data, previous\nmethods rely on data interpolations like cropping, flipping, and mixing up,\nwhich fail to introduce new information and yield only marginal improvements.\nIn this paper, we propose a new data augmentation method for text-to-image\ngeneration using linear extrapolation. Specifically, we apply linear\nextrapolation only on text feature, and new image data are retrieved from the\ninternet by search engines. For the reliability of new text-image pairs, we\ndesign two outlier detectors to purify retrieved images. Based on\nextrapolation, we construct training samples dozens of times larger than the\noriginal dataset, resulting in a significant improvement in text-to-image\nperformance. Moreover, we propose a NULL-guidance to refine score estimation,\nand apply recurrent affine transformation to fuse text information. Our model\nachieves FID scores of 7.91, 9.52 and 5.00 on the CUB, Oxford and COCO\ndatasets. The code and data will be available on GitHub\n(https://github.com/senmaoy/RAT-Diffusion).\n", "link": "http://arxiv.org/abs/2410.01638v1", "date": "2024-10-02", "relevancy": 2.3016, "topK": [{"title": "Scaling Rectified Flow Transformers for High-Resolution Image Synthesis", "link": "http://arxiv.org/abs/2403.03206v1", "similarity": 0.6128}, {"title": "ToDo: Token Downsampling for Efficient Generation of High-Resolution\n  Images", "link": "http://arxiv.org/abs/2402.13573v2", "similarity": 0.5904}, {"title": "Magic-Me: Identity-Specific Video Customized Diffusion", "link": "http://arxiv.org/abs/2402.09368v1", "similarity": 0.5454}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Data%20Extrapolation%20for%20Text-to-image%20Generation%20on%20Small%20Datasets&body=Title%3A%20Data%20Extrapolation%20for%20Text-to-image%20Generation%20on%20Small%20Datasets%0AAuthor%3A%20Senmao%20Ye%20and%20Fei%20Liu%0AAbstract%3A%20%20%20Text-to-image%20generation%20requires%20large%20amount%20of%20training%20data%20to%0Asynthesizing%20high-quality%20images.%20For%20augmenting%20training%20data%2C%20previous%0Amethods%20rely%20on%20data%20interpolations%20like%20cropping%2C%20flipping%2C%20and%20mixing%20up%2C%0Awhich%20fail%20to%20introduce%20new%20information%20and%20yield%20only%20marginal%20improvements.%0AIn%20this%20paper%2C%20we%20propose%20a%20new%20data%20augmentation%20method%20for%20text-to-image%0Ageneration%20using%20linear%20extrapolation.%20Specifically%2C%20we%20apply%20linear%0Aextrapolation%20only%20on%20text%20feature%2C%20and%20new%20image%20data%20are%20retrieved%20from%20the%0Ainternet%20by%20search%20engines.%20For%20the%20reliability%20of%20new%20text-image%20pairs%2C%20we%0Adesign%20two%20outlier%20detectors%20to%20purify%20retrieved%20images.%20Based%20on%0Aextrapolation%2C%20we%20construct%20training%20samples%20dozens%20of%20times%20larger%20than%20the%0Aoriginal%20dataset%2C%20resulting%20in%20a%20significant%20improvement%20in%20text-to-image%0Aperformance.%20Moreover%2C%20we%20propose%20a%20NULL-guidance%20to%20refine%20score%20estimation%2C%0Aand%20apply%20recurrent%20affine%20transformation%20to%20fuse%20text%20information.%20Our%20model%0Aachieves%20FID%20scores%20of%207.91%2C%209.52%20and%205.00%20on%20the%20CUB%2C%20Oxford%20and%20COCO%0Adatasets.%20The%20code%20and%20data%20will%20be%20available%20on%20GitHub%0A%28https%3A//github.com/senmaoy/RAT-Diffusion%29.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2410.01638v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DData%2520Extrapolation%2520for%2520Text-to-image%2520Generation%2520on%2520Small%2520Datasets%26entry.906535625%3DSenmao%2520Ye%2520and%2520Fei%2520Liu%26entry.1292438233%3D%2520%2520Text-to-image%2520generation%2520requires%2520large%2520amount%2520of%2520training%2520data%2520to%250Asynthesizing%2520high-quality%2520images.%2520For%2520augmenting%2520training%2520data%252C%2520previous%250Amethods%2520rely%2520on%2520data%2520interpolations%2520like%2520cropping%252C%2520flipping%252C%2520and%2520mixing%2520up%252C%250Awhich%2520fail%2520to%2520introduce%2520new%2520information%2520and%2520yield%2520only%2520marginal%2520improvements.%250AIn%2520this%2520paper%252C%2520we%2520propose%2520a%2520new%2520data%2520augmentation%2520method%2520for%2520text-to-image%250Ageneration%2520using%2520linear%2520extrapolation.%2520Specifically%252C%2520we%2520apply%2520linear%250Aextrapolation%2520only%2520on%2520text%2520feature%252C%2520and%2520new%2520image%2520data%2520are%2520retrieved%2520from%2520the%250Ainternet%2520by%2520search%2520engines.%2520For%2520the%2520reliability%2520of%2520new%2520text-image%2520pairs%252C%2520we%250Adesign%2520two%2520outlier%2520detectors%2520to%2520purify%2520retrieved%2520images.%2520Based%2520on%250Aextrapolation%252C%2520we%2520construct%2520training%2520samples%2520dozens%2520of%2520times%2520larger%2520than%2520the%250Aoriginal%2520dataset%252C%2520resulting%2520in%2520a%2520significant%2520improvement%2520in%2520text-to-image%250Aperformance.%2520Moreover%252C%2520we%2520propose%2520a%2520NULL-guidance%2520to%2520refine%2520score%2520estimation%252C%250Aand%2520apply%2520recurrent%2520affine%2520transformation%2520to%2520fuse%2520text%2520information.%2520Our%2520model%250Aachieves%2520FID%2520scores%2520of%25207.91%252C%25209.52%2520and%25205.00%2520on%2520the%2520CUB%252C%2520Oxford%2520and%2520COCO%250Adatasets.%2520The%2520code%2520and%2520data%2520will%2520be%2520available%2520on%2520GitHub%250A%2528https%253A//github.com/senmaoy/RAT-Diffusion%2529.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2410.01638v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Data%20Extrapolation%20for%20Text-to-image%20Generation%20on%20Small%20Datasets&entry.906535625=Senmao%20Ye%20and%20Fei%20Liu&entry.1292438233=%20%20Text-to-image%20generation%20requires%20large%20amount%20of%20training%20data%20to%0Asynthesizing%20high-quality%20images.%20For%20augmenting%20training%20data%2C%20previous%0Amethods%20rely%20on%20data%20interpolations%20like%20cropping%2C%20flipping%2C%20and%20mixing%20up%2C%0Awhich%20fail%20to%20introduce%20new%20information%20and%20yield%20only%20marginal%20improvements.%0AIn%20this%20paper%2C%20we%20propose%20a%20new%20data%20augmentation%20method%20for%20text-to-image%0Ageneration%20using%20linear%20extrapolation.%20Specifically%2C%20we%20apply%20linear%0Aextrapolation%20only%20on%20text%20feature%2C%20and%20new%20image%20data%20are%20retrieved%20from%20the%0Ainternet%20by%20search%20engines.%20For%20the%20reliability%20of%20new%20text-image%20pairs%2C%20we%0Adesign%20two%20outlier%20detectors%20to%20purify%20retrieved%20images.%20Based%20on%0Aextrapolation%2C%20we%20construct%20training%20samples%20dozens%20of%20times%20larger%20than%20the%0Aoriginal%20dataset%2C%20resulting%20in%20a%20significant%20improvement%20in%20text-to-image%0Aperformance.%20Moreover%2C%20we%20propose%20a%20NULL-guidance%20to%20refine%20score%20estimation%2C%0Aand%20apply%20recurrent%20affine%20transformation%20to%20fuse%20text%20information.%20Our%20model%0Aachieves%20FID%20scores%20of%207.91%2C%209.52%20and%205.00%20on%20the%20CUB%2C%20Oxford%20and%20COCO%0Adatasets.%20The%20code%20and%20data%20will%20be%20available%20on%20GitHub%0A%28https%3A//github.com/senmaoy/RAT-Diffusion%29.%0A&entry.1838667208=http%3A//arxiv.org/abs/2410.01638v1&entry.124074799=Read"},
{"title": "VinePPO: Unlocking RL Potential For LLM Reasoning Through Refined Credit\n  Assignment", "author": "Amirhossein Kazemnejad and Milad Aghajohari and Eva Portelance and Alessandro Sordoni and Siva Reddy and Aaron Courville and Nicolas Le Roux", "abstract": "  Large language models (LLMs) are increasingly applied to complex reasoning\ntasks that require executing several complex steps before receiving any reward.\nProperly assigning credit to these steps is essential for enhancing model\nperformance. Proximal Policy Optimization (PPO), a state-of-the-art\nreinforcement learning (RL) algorithm used for LLM finetuning, employs value\nnetworks to tackle credit assignment. However, value networks face challenges\nin predicting the expected cumulative rewards accurately in complex reasoning\ntasks, often leading to high-variance updates and suboptimal performance. In\nthis work, we systematically evaluate the efficacy of value networks and reveal\ntheir significant shortcomings in reasoning-heavy LLM tasks, showing that they\nbarely outperform a random baseline when comparing alternative steps. To\naddress this, we propose VinePPO, a straightforward approach that leverages the\nflexibility of language environments to compute unbiased Monte Carlo-based\nestimates, bypassing the need for large value networks. Our method consistently\noutperforms PPO and other RL-free baselines across MATH and GSM8K datasets with\nfewer gradient updates (up to 9x), less wall-clock time (up to 3.0x). These\nresults emphasize the importance of accurate credit assignment in RL finetuning\nof LLM and demonstrate VinePPO's potential as a superior alternative.\n", "link": "http://arxiv.org/abs/2410.01679v1", "date": "2024-10-02", "relevancy": 2.2965, "topK": [{"title": "Self-supervised Photographic Image Layout Representation Learning", "link": "http://arxiv.org/abs/2403.03740v1", "similarity": 0.4602}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.4588}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.4588}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20VinePPO%3A%20Unlocking%20RL%20Potential%20For%20LLM%20Reasoning%20Through%20Refined%20Credit%0A%20%20Assignment&body=Title%3A%20VinePPO%3A%20Unlocking%20RL%20Potential%20For%20LLM%20Reasoning%20Through%20Refined%20Credit%0A%20%20Assignment%0AAuthor%3A%20Amirhossein%20Kazemnejad%20and%20Milad%20Aghajohari%20and%20Eva%20Portelance%20and%20Alessandro%20Sordoni%20and%20Siva%20Reddy%20and%20Aaron%20Courville%20and%20Nicolas%20Le%20Roux%0AAbstract%3A%20%20%20Large%20language%20models%20%28LLMs%29%20are%20increasingly%20applied%20to%20complex%20reasoning%0Atasks%20that%20require%20executing%20several%20complex%20steps%20before%20receiving%20any%20reward.%0AProperly%20assigning%20credit%20to%20these%20steps%20is%20essential%20for%20enhancing%20model%0Aperformance.%20Proximal%20Policy%20Optimization%20%28PPO%29%2C%20a%20state-of-the-art%0Areinforcement%20learning%20%28RL%29%20algorithm%20used%20for%20LLM%20finetuning%2C%20employs%20value%0Anetworks%20to%20tackle%20credit%20assignment.%20However%2C%20value%20networks%20face%20challenges%0Ain%20predicting%20the%20expected%20cumulative%20rewards%20accurately%20in%20complex%20reasoning%0Atasks%2C%20often%20leading%20to%20high-variance%20updates%20and%20suboptimal%20performance.%20In%0Athis%20work%2C%20we%20systematically%20evaluate%20the%20efficacy%20of%20value%20networks%20and%20reveal%0Atheir%20significant%20shortcomings%20in%20reasoning-heavy%20LLM%20tasks%2C%20showing%20that%20they%0Abarely%20outperform%20a%20random%20baseline%20when%20comparing%20alternative%20steps.%20To%0Aaddress%20this%2C%20we%20propose%20VinePPO%2C%20a%20straightforward%20approach%20that%20leverages%20the%0Aflexibility%20of%20language%20environments%20to%20compute%20unbiased%20Monte%20Carlo-based%0Aestimates%2C%20bypassing%20the%20need%20for%20large%20value%20networks.%20Our%20method%20consistently%0Aoutperforms%20PPO%20and%20other%20RL-free%20baselines%20across%20MATH%20and%20GSM8K%20datasets%20with%0Afewer%20gradient%20updates%20%28up%20to%209x%29%2C%20less%20wall-clock%20time%20%28up%20to%203.0x%29.%20These%0Aresults%20emphasize%20the%20importance%20of%20accurate%20credit%20assignment%20in%20RL%20finetuning%0Aof%20LLM%20and%20demonstrate%20VinePPO%27s%20potential%20as%20a%20superior%20alternative.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2410.01679v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DVinePPO%253A%2520Unlocking%2520RL%2520Potential%2520For%2520LLM%2520Reasoning%2520Through%2520Refined%2520Credit%250A%2520%2520Assignment%26entry.906535625%3DAmirhossein%2520Kazemnejad%2520and%2520Milad%2520Aghajohari%2520and%2520Eva%2520Portelance%2520and%2520Alessandro%2520Sordoni%2520and%2520Siva%2520Reddy%2520and%2520Aaron%2520Courville%2520and%2520Nicolas%2520Le%2520Roux%26entry.1292438233%3D%2520%2520Large%2520language%2520models%2520%2528LLMs%2529%2520are%2520increasingly%2520applied%2520to%2520complex%2520reasoning%250Atasks%2520that%2520require%2520executing%2520several%2520complex%2520steps%2520before%2520receiving%2520any%2520reward.%250AProperly%2520assigning%2520credit%2520to%2520these%2520steps%2520is%2520essential%2520for%2520enhancing%2520model%250Aperformance.%2520Proximal%2520Policy%2520Optimization%2520%2528PPO%2529%252C%2520a%2520state-of-the-art%250Areinforcement%2520learning%2520%2528RL%2529%2520algorithm%2520used%2520for%2520LLM%2520finetuning%252C%2520employs%2520value%250Anetworks%2520to%2520tackle%2520credit%2520assignment.%2520However%252C%2520value%2520networks%2520face%2520challenges%250Ain%2520predicting%2520the%2520expected%2520cumulative%2520rewards%2520accurately%2520in%2520complex%2520reasoning%250Atasks%252C%2520often%2520leading%2520to%2520high-variance%2520updates%2520and%2520suboptimal%2520performance.%2520In%250Athis%2520work%252C%2520we%2520systematically%2520evaluate%2520the%2520efficacy%2520of%2520value%2520networks%2520and%2520reveal%250Atheir%2520significant%2520shortcomings%2520in%2520reasoning-heavy%2520LLM%2520tasks%252C%2520showing%2520that%2520they%250Abarely%2520outperform%2520a%2520random%2520baseline%2520when%2520comparing%2520alternative%2520steps.%2520To%250Aaddress%2520this%252C%2520we%2520propose%2520VinePPO%252C%2520a%2520straightforward%2520approach%2520that%2520leverages%2520the%250Aflexibility%2520of%2520language%2520environments%2520to%2520compute%2520unbiased%2520Monte%2520Carlo-based%250Aestimates%252C%2520bypassing%2520the%2520need%2520for%2520large%2520value%2520networks.%2520Our%2520method%2520consistently%250Aoutperforms%2520PPO%2520and%2520other%2520RL-free%2520baselines%2520across%2520MATH%2520and%2520GSM8K%2520datasets%2520with%250Afewer%2520gradient%2520updates%2520%2528up%2520to%25209x%2529%252C%2520less%2520wall-clock%2520time%2520%2528up%2520to%25203.0x%2529.%2520These%250Aresults%2520emphasize%2520the%2520importance%2520of%2520accurate%2520credit%2520assignment%2520in%2520RL%2520finetuning%250Aof%2520LLM%2520and%2520demonstrate%2520VinePPO%2527s%2520potential%2520as%2520a%2520superior%2520alternative.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2410.01679v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=VinePPO%3A%20Unlocking%20RL%20Potential%20For%20LLM%20Reasoning%20Through%20Refined%20Credit%0A%20%20Assignment&entry.906535625=Amirhossein%20Kazemnejad%20and%20Milad%20Aghajohari%20and%20Eva%20Portelance%20and%20Alessandro%20Sordoni%20and%20Siva%20Reddy%20and%20Aaron%20Courville%20and%20Nicolas%20Le%20Roux&entry.1292438233=%20%20Large%20language%20models%20%28LLMs%29%20are%20increasingly%20applied%20to%20complex%20reasoning%0Atasks%20that%20require%20executing%20several%20complex%20steps%20before%20receiving%20any%20reward.%0AProperly%20assigning%20credit%20to%20these%20steps%20is%20essential%20for%20enhancing%20model%0Aperformance.%20Proximal%20Policy%20Optimization%20%28PPO%29%2C%20a%20state-of-the-art%0Areinforcement%20learning%20%28RL%29%20algorithm%20used%20for%20LLM%20finetuning%2C%20employs%20value%0Anetworks%20to%20tackle%20credit%20assignment.%20However%2C%20value%20networks%20face%20challenges%0Ain%20predicting%20the%20expected%20cumulative%20rewards%20accurately%20in%20complex%20reasoning%0Atasks%2C%20often%20leading%20to%20high-variance%20updates%20and%20suboptimal%20performance.%20In%0Athis%20work%2C%20we%20systematically%20evaluate%20the%20efficacy%20of%20value%20networks%20and%20reveal%0Atheir%20significant%20shortcomings%20in%20reasoning-heavy%20LLM%20tasks%2C%20showing%20that%20they%0Abarely%20outperform%20a%20random%20baseline%20when%20comparing%20alternative%20steps.%20To%0Aaddress%20this%2C%20we%20propose%20VinePPO%2C%20a%20straightforward%20approach%20that%20leverages%20the%0Aflexibility%20of%20language%20environments%20to%20compute%20unbiased%20Monte%20Carlo-based%0Aestimates%2C%20bypassing%20the%20need%20for%20large%20value%20networks.%20Our%20method%20consistently%0Aoutperforms%20PPO%20and%20other%20RL-free%20baselines%20across%20MATH%20and%20GSM8K%20datasets%20with%0Afewer%20gradient%20updates%20%28up%20to%209x%29%2C%20less%20wall-clock%20time%20%28up%20to%203.0x%29.%20These%0Aresults%20emphasize%20the%20importance%20of%20accurate%20credit%20assignment%20in%20RL%20finetuning%0Aof%20LLM%20and%20demonstrate%20VinePPO%27s%20potential%20as%20a%20superior%20alternative.%0A&entry.1838667208=http%3A//arxiv.org/abs/2410.01679v1&entry.124074799=Read"},
{"title": "Exploring Scalability of Self-Training for Open-Vocabulary Temporal\n  Action Localization", "author": "Jeongseok Hyun and Su Ho Han and Hyolim Kang and Joon-Young Lee and Seon Joo Kim", "abstract": "  The vocabulary size in temporal action localization (TAL) is limited by the\nscarcity of large-scale annotated datasets. To overcome this, recent works\nintegrate vision-language models (VLMs), such as CLIP, for open-vocabulary TAL\n(OV-TAL). However, despite the success of VLMs trained on extensive datasets,\nexisting OV-TAL methods still rely on human-labeled TAL datasets of limited\nsize to train action localizers, limiting their generalizability. In this\npaper, we explore the scalability of self-training with unlabeled YouTube\nvideos for OV-TAL. Our approach consists of two stages: (1) a class-agnostic\naction localizer is trained on a human-labeled TAL dataset to generate\npseudo-labels for unlabeled videos, and (2) the large-scale pseudo-labeled\ndataset is then used to train the localizer. Extensive experiments demonstrate\nthat leveraging web-scale videos in self-training significantly enhances the\ngeneralizability of an action localizer. Additionally, we identify limitations\nin existing OV-TAL evaluation schemes and propose a new benchmark for thorough\nassessment. Finally, we showcase the TAL performance of the large multimodal\nmodel Gemini-1.5 on our new benchmark. Code is released at\nhttps://github.com/HYUNJS/STOV-TAL.\n", "link": "http://arxiv.org/abs/2407.07024v2", "date": "2024-10-02", "relevancy": 2.287, "topK": [{"title": "Weakly-guided self-supervised pretraining for temporal activity detection", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/25189", "similarity": 0.5796}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5787}, {"title": "Customize-A-Video: One-Shot Motion Customization of Text-to-Video\n  Diffusion Models", "link": "http://arxiv.org/abs/2402.14780v1", "similarity": 0.5349}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Exploring%20Scalability%20of%20Self-Training%20for%20Open-Vocabulary%20Temporal%0A%20%20Action%20Localization&body=Title%3A%20Exploring%20Scalability%20of%20Self-Training%20for%20Open-Vocabulary%20Temporal%0A%20%20Action%20Localization%0AAuthor%3A%20Jeongseok%20Hyun%20and%20Su%20Ho%20Han%20and%20Hyolim%20Kang%20and%20Joon-Young%20Lee%20and%20Seon%20Joo%20Kim%0AAbstract%3A%20%20%20The%20vocabulary%20size%20in%20temporal%20action%20localization%20%28TAL%29%20is%20limited%20by%20the%0Ascarcity%20of%20large-scale%20annotated%20datasets.%20To%20overcome%20this%2C%20recent%20works%0Aintegrate%20vision-language%20models%20%28VLMs%29%2C%20such%20as%20CLIP%2C%20for%20open-vocabulary%20TAL%0A%28OV-TAL%29.%20However%2C%20despite%20the%20success%20of%20VLMs%20trained%20on%20extensive%20datasets%2C%0Aexisting%20OV-TAL%20methods%20still%20rely%20on%20human-labeled%20TAL%20datasets%20of%20limited%0Asize%20to%20train%20action%20localizers%2C%20limiting%20their%20generalizability.%20In%20this%0Apaper%2C%20we%20explore%20the%20scalability%20of%20self-training%20with%20unlabeled%20YouTube%0Avideos%20for%20OV-TAL.%20Our%20approach%20consists%20of%20two%20stages%3A%20%281%29%20a%20class-agnostic%0Aaction%20localizer%20is%20trained%20on%20a%20human-labeled%20TAL%20dataset%20to%20generate%0Apseudo-labels%20for%20unlabeled%20videos%2C%20and%20%282%29%20the%20large-scale%20pseudo-labeled%0Adataset%20is%20then%20used%20to%20train%20the%20localizer.%20Extensive%20experiments%20demonstrate%0Athat%20leveraging%20web-scale%20videos%20in%20self-training%20significantly%20enhances%20the%0Ageneralizability%20of%20an%20action%20localizer.%20Additionally%2C%20we%20identify%20limitations%0Ain%20existing%20OV-TAL%20evaluation%20schemes%20and%20propose%20a%20new%20benchmark%20for%20thorough%0Aassessment.%20Finally%2C%20we%20showcase%20the%20TAL%20performance%20of%20the%20large%20multimodal%0Amodel%20Gemini-1.5%20on%20our%20new%20benchmark.%20Code%20is%20released%20at%0Ahttps%3A//github.com/HYUNJS/STOV-TAL.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2407.07024v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DExploring%2520Scalability%2520of%2520Self-Training%2520for%2520Open-Vocabulary%2520Temporal%250A%2520%2520Action%2520Localization%26entry.906535625%3DJeongseok%2520Hyun%2520and%2520Su%2520Ho%2520Han%2520and%2520Hyolim%2520Kang%2520and%2520Joon-Young%2520Lee%2520and%2520Seon%2520Joo%2520Kim%26entry.1292438233%3D%2520%2520The%2520vocabulary%2520size%2520in%2520temporal%2520action%2520localization%2520%2528TAL%2529%2520is%2520limited%2520by%2520the%250Ascarcity%2520of%2520large-scale%2520annotated%2520datasets.%2520To%2520overcome%2520this%252C%2520recent%2520works%250Aintegrate%2520vision-language%2520models%2520%2528VLMs%2529%252C%2520such%2520as%2520CLIP%252C%2520for%2520open-vocabulary%2520TAL%250A%2528OV-TAL%2529.%2520However%252C%2520despite%2520the%2520success%2520of%2520VLMs%2520trained%2520on%2520extensive%2520datasets%252C%250Aexisting%2520OV-TAL%2520methods%2520still%2520rely%2520on%2520human-labeled%2520TAL%2520datasets%2520of%2520limited%250Asize%2520to%2520train%2520action%2520localizers%252C%2520limiting%2520their%2520generalizability.%2520In%2520this%250Apaper%252C%2520we%2520explore%2520the%2520scalability%2520of%2520self-training%2520with%2520unlabeled%2520YouTube%250Avideos%2520for%2520OV-TAL.%2520Our%2520approach%2520consists%2520of%2520two%2520stages%253A%2520%25281%2529%2520a%2520class-agnostic%250Aaction%2520localizer%2520is%2520trained%2520on%2520a%2520human-labeled%2520TAL%2520dataset%2520to%2520generate%250Apseudo-labels%2520for%2520unlabeled%2520videos%252C%2520and%2520%25282%2529%2520the%2520large-scale%2520pseudo-labeled%250Adataset%2520is%2520then%2520used%2520to%2520train%2520the%2520localizer.%2520Extensive%2520experiments%2520demonstrate%250Athat%2520leveraging%2520web-scale%2520videos%2520in%2520self-training%2520significantly%2520enhances%2520the%250Ageneralizability%2520of%2520an%2520action%2520localizer.%2520Additionally%252C%2520we%2520identify%2520limitations%250Ain%2520existing%2520OV-TAL%2520evaluation%2520schemes%2520and%2520propose%2520a%2520new%2520benchmark%2520for%2520thorough%250Aassessment.%2520Finally%252C%2520we%2520showcase%2520the%2520TAL%2520performance%2520of%2520the%2520large%2520multimodal%250Amodel%2520Gemini-1.5%2520on%2520our%2520new%2520benchmark.%2520Code%2520is%2520released%2520at%250Ahttps%253A//github.com/HYUNJS/STOV-TAL.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2407.07024v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Exploring%20Scalability%20of%20Self-Training%20for%20Open-Vocabulary%20Temporal%0A%20%20Action%20Localization&entry.906535625=Jeongseok%20Hyun%20and%20Su%20Ho%20Han%20and%20Hyolim%20Kang%20and%20Joon-Young%20Lee%20and%20Seon%20Joo%20Kim&entry.1292438233=%20%20The%20vocabulary%20size%20in%20temporal%20action%20localization%20%28TAL%29%20is%20limited%20by%20the%0Ascarcity%20of%20large-scale%20annotated%20datasets.%20To%20overcome%20this%2C%20recent%20works%0Aintegrate%20vision-language%20models%20%28VLMs%29%2C%20such%20as%20CLIP%2C%20for%20open-vocabulary%20TAL%0A%28OV-TAL%29.%20However%2C%20despite%20the%20success%20of%20VLMs%20trained%20on%20extensive%20datasets%2C%0Aexisting%20OV-TAL%20methods%20still%20rely%20on%20human-labeled%20TAL%20datasets%20of%20limited%0Asize%20to%20train%20action%20localizers%2C%20limiting%20their%20generalizability.%20In%20this%0Apaper%2C%20we%20explore%20the%20scalability%20of%20self-training%20with%20unlabeled%20YouTube%0Avideos%20for%20OV-TAL.%20Our%20approach%20consists%20of%20two%20stages%3A%20%281%29%20a%20class-agnostic%0Aaction%20localizer%20is%20trained%20on%20a%20human-labeled%20TAL%20dataset%20to%20generate%0Apseudo-labels%20for%20unlabeled%20videos%2C%20and%20%282%29%20the%20large-scale%20pseudo-labeled%0Adataset%20is%20then%20used%20to%20train%20the%20localizer.%20Extensive%20experiments%20demonstrate%0Athat%20leveraging%20web-scale%20videos%20in%20self-training%20significantly%20enhances%20the%0Ageneralizability%20of%20an%20action%20localizer.%20Additionally%2C%20we%20identify%20limitations%0Ain%20existing%20OV-TAL%20evaluation%20schemes%20and%20propose%20a%20new%20benchmark%20for%20thorough%0Aassessment.%20Finally%2C%20we%20showcase%20the%20TAL%20performance%20of%20the%20large%20multimodal%0Amodel%20Gemini-1.5%20on%20our%20new%20benchmark.%20Code%20is%20released%20at%0Ahttps%3A//github.com/HYUNJS/STOV-TAL.%0A&entry.1838667208=http%3A//arxiv.org/abs/2407.07024v2&entry.124074799=Read"},
{"title": "Motion meets Attention: Video Motion Prompts", "author": "Qixiang Chen and Lei Wang and Piotr Koniusz and Tom Gedeon", "abstract": "  Videos contain rich spatio-temporal information. Traditional methods for\nextracting motion, used in tasks such as action recognition, often rely on\nvisual contents rather than precise motion features. This phenomenon is\nreferred to as 'blind motion extraction' behavior, which proves inefficient in\ncapturing motions of interest due to a lack of motion-guided cues. Recently,\nattention mechanisms have enhanced many computer vision tasks by effectively\nhighlighting salient visual areas. Inspired by this, we propose a modified\nSigmoid function with learnable slope and shift parameters as an attention\nmechanism to modulate motion signals from frame differencing maps. This\napproach generates a sequence of attention maps that enhance the processing of\nmotion-related video content. To ensure temporal continuity and smoothness of\nthe attention maps, we apply pair-wise temporal attention variation\nregularization to remove unwanted motions (e.g., noise) while preserving\nimportant ones. We then perform Hadamard product between each pair of attention\nmaps and the original video frames to highlight the evolving motions of\ninterest over time. These highlighted motions, termed video motion prompts, are\nsubsequently used as inputs to the model instead of the original video frames.\nWe formalize this process as a motion prompt layer and incorporate the\nregularization term into the loss function to learn better motion prompts. This\nlayer serves as an adapter between the model and the video data, bridging the\ngap between traditional 'blind motion extraction' and the extraction of\nrelevant motions of interest. We show that our lightweight, plug-and-play\nmotion prompt layer seamlessly integrates into models like SlowFast, X3D, and\nTimeSformer, enhancing performance on benchmarks such as FineGym and MPII\nCooking 2.\n", "link": "http://arxiv.org/abs/2407.03179v2", "date": "2024-10-02", "relevancy": 2.2835, "topK": [{"title": "Customize-A-Video: One-Shot Motion Customization of Text-to-Video\n  Diffusion Models", "link": "http://arxiv.org/abs/2402.14780v1", "similarity": 0.6748}, {"title": "Magic-Me: Identity-Specific Video Customized Diffusion", "link": "http://arxiv.org/abs/2402.09368v1", "similarity": 0.5547}, {"title": "Weakly-guided self-supervised pretraining for temporal activity detection", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/25189", "similarity": 0.5455}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Motion%20meets%20Attention%3A%20Video%20Motion%20Prompts&body=Title%3A%20Motion%20meets%20Attention%3A%20Video%20Motion%20Prompts%0AAuthor%3A%20Qixiang%20Chen%20and%20Lei%20Wang%20and%20Piotr%20Koniusz%20and%20Tom%20Gedeon%0AAbstract%3A%20%20%20Videos%20contain%20rich%20spatio-temporal%20information.%20Traditional%20methods%20for%0Aextracting%20motion%2C%20used%20in%20tasks%20such%20as%20action%20recognition%2C%20often%20rely%20on%0Avisual%20contents%20rather%20than%20precise%20motion%20features.%20This%20phenomenon%20is%0Areferred%20to%20as%20%27blind%20motion%20extraction%27%20behavior%2C%20which%20proves%20inefficient%20in%0Acapturing%20motions%20of%20interest%20due%20to%20a%20lack%20of%20motion-guided%20cues.%20Recently%2C%0Aattention%20mechanisms%20have%20enhanced%20many%20computer%20vision%20tasks%20by%20effectively%0Ahighlighting%20salient%20visual%20areas.%20Inspired%20by%20this%2C%20we%20propose%20a%20modified%0ASigmoid%20function%20with%20learnable%20slope%20and%20shift%20parameters%20as%20an%20attention%0Amechanism%20to%20modulate%20motion%20signals%20from%20frame%20differencing%20maps.%20This%0Aapproach%20generates%20a%20sequence%20of%20attention%20maps%20that%20enhance%20the%20processing%20of%0Amotion-related%20video%20content.%20To%20ensure%20temporal%20continuity%20and%20smoothness%20of%0Athe%20attention%20maps%2C%20we%20apply%20pair-wise%20temporal%20attention%20variation%0Aregularization%20to%20remove%20unwanted%20motions%20%28e.g.%2C%20noise%29%20while%20preserving%0Aimportant%20ones.%20We%20then%20perform%20Hadamard%20product%20between%20each%20pair%20of%20attention%0Amaps%20and%20the%20original%20video%20frames%20to%20highlight%20the%20evolving%20motions%20of%0Ainterest%20over%20time.%20These%20highlighted%20motions%2C%20termed%20video%20motion%20prompts%2C%20are%0Asubsequently%20used%20as%20inputs%20to%20the%20model%20instead%20of%20the%20original%20video%20frames.%0AWe%20formalize%20this%20process%20as%20a%20motion%20prompt%20layer%20and%20incorporate%20the%0Aregularization%20term%20into%20the%20loss%20function%20to%20learn%20better%20motion%20prompts.%20This%0Alayer%20serves%20as%20an%20adapter%20between%20the%20model%20and%20the%20video%20data%2C%20bridging%20the%0Agap%20between%20traditional%20%27blind%20motion%20extraction%27%20and%20the%20extraction%20of%0Arelevant%20motions%20of%20interest.%20We%20show%20that%20our%20lightweight%2C%20plug-and-play%0Amotion%20prompt%20layer%20seamlessly%20integrates%20into%20models%20like%20SlowFast%2C%20X3D%2C%20and%0ATimeSformer%2C%20enhancing%20performance%20on%20benchmarks%20such%20as%20FineGym%20and%20MPII%0ACooking%202.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2407.03179v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DMotion%2520meets%2520Attention%253A%2520Video%2520Motion%2520Prompts%26entry.906535625%3DQixiang%2520Chen%2520and%2520Lei%2520Wang%2520and%2520Piotr%2520Koniusz%2520and%2520Tom%2520Gedeon%26entry.1292438233%3D%2520%2520Videos%2520contain%2520rich%2520spatio-temporal%2520information.%2520Traditional%2520methods%2520for%250Aextracting%2520motion%252C%2520used%2520in%2520tasks%2520such%2520as%2520action%2520recognition%252C%2520often%2520rely%2520on%250Avisual%2520contents%2520rather%2520than%2520precise%2520motion%2520features.%2520This%2520phenomenon%2520is%250Areferred%2520to%2520as%2520%2527blind%2520motion%2520extraction%2527%2520behavior%252C%2520which%2520proves%2520inefficient%2520in%250Acapturing%2520motions%2520of%2520interest%2520due%2520to%2520a%2520lack%2520of%2520motion-guided%2520cues.%2520Recently%252C%250Aattention%2520mechanisms%2520have%2520enhanced%2520many%2520computer%2520vision%2520tasks%2520by%2520effectively%250Ahighlighting%2520salient%2520visual%2520areas.%2520Inspired%2520by%2520this%252C%2520we%2520propose%2520a%2520modified%250ASigmoid%2520function%2520with%2520learnable%2520slope%2520and%2520shift%2520parameters%2520as%2520an%2520attention%250Amechanism%2520to%2520modulate%2520motion%2520signals%2520from%2520frame%2520differencing%2520maps.%2520This%250Aapproach%2520generates%2520a%2520sequence%2520of%2520attention%2520maps%2520that%2520enhance%2520the%2520processing%2520of%250Amotion-related%2520video%2520content.%2520To%2520ensure%2520temporal%2520continuity%2520and%2520smoothness%2520of%250Athe%2520attention%2520maps%252C%2520we%2520apply%2520pair-wise%2520temporal%2520attention%2520variation%250Aregularization%2520to%2520remove%2520unwanted%2520motions%2520%2528e.g.%252C%2520noise%2529%2520while%2520preserving%250Aimportant%2520ones.%2520We%2520then%2520perform%2520Hadamard%2520product%2520between%2520each%2520pair%2520of%2520attention%250Amaps%2520and%2520the%2520original%2520video%2520frames%2520to%2520highlight%2520the%2520evolving%2520motions%2520of%250Ainterest%2520over%2520time.%2520These%2520highlighted%2520motions%252C%2520termed%2520video%2520motion%2520prompts%252C%2520are%250Asubsequently%2520used%2520as%2520inputs%2520to%2520the%2520model%2520instead%2520of%2520the%2520original%2520video%2520frames.%250AWe%2520formalize%2520this%2520process%2520as%2520a%2520motion%2520prompt%2520layer%2520and%2520incorporate%2520the%250Aregularization%2520term%2520into%2520the%2520loss%2520function%2520to%2520learn%2520better%2520motion%2520prompts.%2520This%250Alayer%2520serves%2520as%2520an%2520adapter%2520between%2520the%2520model%2520and%2520the%2520video%2520data%252C%2520bridging%2520the%250Agap%2520between%2520traditional%2520%2527blind%2520motion%2520extraction%2527%2520and%2520the%2520extraction%2520of%250Arelevant%2520motions%2520of%2520interest.%2520We%2520show%2520that%2520our%2520lightweight%252C%2520plug-and-play%250Amotion%2520prompt%2520layer%2520seamlessly%2520integrates%2520into%2520models%2520like%2520SlowFast%252C%2520X3D%252C%2520and%250ATimeSformer%252C%2520enhancing%2520performance%2520on%2520benchmarks%2520such%2520as%2520FineGym%2520and%2520MPII%250ACooking%25202.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2407.03179v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Motion%20meets%20Attention%3A%20Video%20Motion%20Prompts&entry.906535625=Qixiang%20Chen%20and%20Lei%20Wang%20and%20Piotr%20Koniusz%20and%20Tom%20Gedeon&entry.1292438233=%20%20Videos%20contain%20rich%20spatio-temporal%20information.%20Traditional%20methods%20for%0Aextracting%20motion%2C%20used%20in%20tasks%20such%20as%20action%20recognition%2C%20often%20rely%20on%0Avisual%20contents%20rather%20than%20precise%20motion%20features.%20This%20phenomenon%20is%0Areferred%20to%20as%20%27blind%20motion%20extraction%27%20behavior%2C%20which%20proves%20inefficient%20in%0Acapturing%20motions%20of%20interest%20due%20to%20a%20lack%20of%20motion-guided%20cues.%20Recently%2C%0Aattention%20mechanisms%20have%20enhanced%20many%20computer%20vision%20tasks%20by%20effectively%0Ahighlighting%20salient%20visual%20areas.%20Inspired%20by%20this%2C%20we%20propose%20a%20modified%0ASigmoid%20function%20with%20learnable%20slope%20and%20shift%20parameters%20as%20an%20attention%0Amechanism%20to%20modulate%20motion%20signals%20from%20frame%20differencing%20maps.%20This%0Aapproach%20generates%20a%20sequence%20of%20attention%20maps%20that%20enhance%20the%20processing%20of%0Amotion-related%20video%20content.%20To%20ensure%20temporal%20continuity%20and%20smoothness%20of%0Athe%20attention%20maps%2C%20we%20apply%20pair-wise%20temporal%20attention%20variation%0Aregularization%20to%20remove%20unwanted%20motions%20%28e.g.%2C%20noise%29%20while%20preserving%0Aimportant%20ones.%20We%20then%20perform%20Hadamard%20product%20between%20each%20pair%20of%20attention%0Amaps%20and%20the%20original%20video%20frames%20to%20highlight%20the%20evolving%20motions%20of%0Ainterest%20over%20time.%20These%20highlighted%20motions%2C%20termed%20video%20motion%20prompts%2C%20are%0Asubsequently%20used%20as%20inputs%20to%20the%20model%20instead%20of%20the%20original%20video%20frames.%0AWe%20formalize%20this%20process%20as%20a%20motion%20prompt%20layer%20and%20incorporate%20the%0Aregularization%20term%20into%20the%20loss%20function%20to%20learn%20better%20motion%20prompts.%20This%0Alayer%20serves%20as%20an%20adapter%20between%20the%20model%20and%20the%20video%20data%2C%20bridging%20the%0Agap%20between%20traditional%20%27blind%20motion%20extraction%27%20and%20the%20extraction%20of%0Arelevant%20motions%20of%20interest.%20We%20show%20that%20our%20lightweight%2C%20plug-and-play%0Amotion%20prompt%20layer%20seamlessly%20integrates%20into%20models%20like%20SlowFast%2C%20X3D%2C%20and%0ATimeSformer%2C%20enhancing%20performance%20on%20benchmarks%20such%20as%20FineGym%20and%20MPII%0ACooking%202.%0A&entry.1838667208=http%3A//arxiv.org/abs/2407.03179v2&entry.124074799=Read"},
{"title": "Selective Aggregation for Low-Rank Adaptation in Federated Learning", "author": "Pengxin Guo and Shuang Zeng and Yanran Wang and Huijie Fan and Feifei Wang and Liangqiong Qu", "abstract": "  We investigate LoRA in federated learning through the lens of the asymmetry\nanalysis of the learned $A$ and $B$ matrices. In doing so, we uncover that $A$\nmatrices are responsible for learning general knowledge, while $B$ matrices\nfocus on capturing client-specific knowledge. Based on this finding, we\nintroduce Federated Share-A Low-Rank Adaptation (FedSA-LoRA), which employs two\nlow-rank trainable matrices $A$ and $B$ to model the weight update, but only\n$A$ matrices are shared with the server for aggregation. Moreover, we delve\ninto the relationship between the learned $A$ and $B$ matrices in other LoRA\nvariants, such as rsLoRA and VeRA, revealing a consistent pattern.\nConsequently, we extend our FedSA-LoRA method to these LoRA variants, resulting\nin FedSA-rsLoRA and FedSA-VeRA. In this way, we establish a general paradigm\nfor integrating LoRA with FL, offering guidance for future work on subsequent\nLoRA variants combined with FL. Extensive experimental results on natural\nlanguage understanding and generation tasks demonstrate the effectiveness of\nthe proposed method.\n", "link": "http://arxiv.org/abs/2410.01463v1", "date": "2024-10-02", "relevancy": 2.2807, "topK": [{"title": "Breadcrumbs: Adversarial class-balanced sampling for long-tailed recognition", "link": "https://link.springer.com/chapter/10.1007/978-3-031-20053-3_37", "similarity": 0.4703}, {"title": "Self-supervised Photographic Image Layout Representation Learning", "link": "http://arxiv.org/abs/2403.03740v1", "similarity": 0.4517}, {"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.4464}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Selective%20Aggregation%20for%20Low-Rank%20Adaptation%20in%20Federated%20Learning&body=Title%3A%20Selective%20Aggregation%20for%20Low-Rank%20Adaptation%20in%20Federated%20Learning%0AAuthor%3A%20Pengxin%20Guo%20and%20Shuang%20Zeng%20and%20Yanran%20Wang%20and%20Huijie%20Fan%20and%20Feifei%20Wang%20and%20Liangqiong%20Qu%0AAbstract%3A%20%20%20We%20investigate%20LoRA%20in%20federated%20learning%20through%20the%20lens%20of%20the%20asymmetry%0Aanalysis%20of%20the%20learned%20%24A%24%20and%20%24B%24%20matrices.%20In%20doing%20so%2C%20we%20uncover%20that%20%24A%24%0Amatrices%20are%20responsible%20for%20learning%20general%20knowledge%2C%20while%20%24B%24%20matrices%0Afocus%20on%20capturing%20client-specific%20knowledge.%20Based%20on%20this%20finding%2C%20we%0Aintroduce%20Federated%20Share-A%20Low-Rank%20Adaptation%20%28FedSA-LoRA%29%2C%20which%20employs%20two%0Alow-rank%20trainable%20matrices%20%24A%24%20and%20%24B%24%20to%20model%20the%20weight%20update%2C%20but%20only%0A%24A%24%20matrices%20are%20shared%20with%20the%20server%20for%20aggregation.%20Moreover%2C%20we%20delve%0Ainto%20the%20relationship%20between%20the%20learned%20%24A%24%20and%20%24B%24%20matrices%20in%20other%20LoRA%0Avariants%2C%20such%20as%20rsLoRA%20and%20VeRA%2C%20revealing%20a%20consistent%20pattern.%0AConsequently%2C%20we%20extend%20our%20FedSA-LoRA%20method%20to%20these%20LoRA%20variants%2C%20resulting%0Ain%20FedSA-rsLoRA%20and%20FedSA-VeRA.%20In%20this%20way%2C%20we%20establish%20a%20general%20paradigm%0Afor%20integrating%20LoRA%20with%20FL%2C%20offering%20guidance%20for%20future%20work%20on%20subsequent%0ALoRA%20variants%20combined%20with%20FL.%20Extensive%20experimental%20results%20on%20natural%0Alanguage%20understanding%20and%20generation%20tasks%20demonstrate%20the%20effectiveness%20of%0Athe%20proposed%20method.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2410.01463v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DSelective%2520Aggregation%2520for%2520Low-Rank%2520Adaptation%2520in%2520Federated%2520Learning%26entry.906535625%3DPengxin%2520Guo%2520and%2520Shuang%2520Zeng%2520and%2520Yanran%2520Wang%2520and%2520Huijie%2520Fan%2520and%2520Feifei%2520Wang%2520and%2520Liangqiong%2520Qu%26entry.1292438233%3D%2520%2520We%2520investigate%2520LoRA%2520in%2520federated%2520learning%2520through%2520the%2520lens%2520of%2520the%2520asymmetry%250Aanalysis%2520of%2520the%2520learned%2520%2524A%2524%2520and%2520%2524B%2524%2520matrices.%2520In%2520doing%2520so%252C%2520we%2520uncover%2520that%2520%2524A%2524%250Amatrices%2520are%2520responsible%2520for%2520learning%2520general%2520knowledge%252C%2520while%2520%2524B%2524%2520matrices%250Afocus%2520on%2520capturing%2520client-specific%2520knowledge.%2520Based%2520on%2520this%2520finding%252C%2520we%250Aintroduce%2520Federated%2520Share-A%2520Low-Rank%2520Adaptation%2520%2528FedSA-LoRA%2529%252C%2520which%2520employs%2520two%250Alow-rank%2520trainable%2520matrices%2520%2524A%2524%2520and%2520%2524B%2524%2520to%2520model%2520the%2520weight%2520update%252C%2520but%2520only%250A%2524A%2524%2520matrices%2520are%2520shared%2520with%2520the%2520server%2520for%2520aggregation.%2520Moreover%252C%2520we%2520delve%250Ainto%2520the%2520relationship%2520between%2520the%2520learned%2520%2524A%2524%2520and%2520%2524B%2524%2520matrices%2520in%2520other%2520LoRA%250Avariants%252C%2520such%2520as%2520rsLoRA%2520and%2520VeRA%252C%2520revealing%2520a%2520consistent%2520pattern.%250AConsequently%252C%2520we%2520extend%2520our%2520FedSA-LoRA%2520method%2520to%2520these%2520LoRA%2520variants%252C%2520resulting%250Ain%2520FedSA-rsLoRA%2520and%2520FedSA-VeRA.%2520In%2520this%2520way%252C%2520we%2520establish%2520a%2520general%2520paradigm%250Afor%2520integrating%2520LoRA%2520with%2520FL%252C%2520offering%2520guidance%2520for%2520future%2520work%2520on%2520subsequent%250ALoRA%2520variants%2520combined%2520with%2520FL.%2520Extensive%2520experimental%2520results%2520on%2520natural%250Alanguage%2520understanding%2520and%2520generation%2520tasks%2520demonstrate%2520the%2520effectiveness%2520of%250Athe%2520proposed%2520method.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2410.01463v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Selective%20Aggregation%20for%20Low-Rank%20Adaptation%20in%20Federated%20Learning&entry.906535625=Pengxin%20Guo%20and%20Shuang%20Zeng%20and%20Yanran%20Wang%20and%20Huijie%20Fan%20and%20Feifei%20Wang%20and%20Liangqiong%20Qu&entry.1292438233=%20%20We%20investigate%20LoRA%20in%20federated%20learning%20through%20the%20lens%20of%20the%20asymmetry%0Aanalysis%20of%20the%20learned%20%24A%24%20and%20%24B%24%20matrices.%20In%20doing%20so%2C%20we%20uncover%20that%20%24A%24%0Amatrices%20are%20responsible%20for%20learning%20general%20knowledge%2C%20while%20%24B%24%20matrices%0Afocus%20on%20capturing%20client-specific%20knowledge.%20Based%20on%20this%20finding%2C%20we%0Aintroduce%20Federated%20Share-A%20Low-Rank%20Adaptation%20%28FedSA-LoRA%29%2C%20which%20employs%20two%0Alow-rank%20trainable%20matrices%20%24A%24%20and%20%24B%24%20to%20model%20the%20weight%20update%2C%20but%20only%0A%24A%24%20matrices%20are%20shared%20with%20the%20server%20for%20aggregation.%20Moreover%2C%20we%20delve%0Ainto%20the%20relationship%20between%20the%20learned%20%24A%24%20and%20%24B%24%20matrices%20in%20other%20LoRA%0Avariants%2C%20such%20as%20rsLoRA%20and%20VeRA%2C%20revealing%20a%20consistent%20pattern.%0AConsequently%2C%20we%20extend%20our%20FedSA-LoRA%20method%20to%20these%20LoRA%20variants%2C%20resulting%0Ain%20FedSA-rsLoRA%20and%20FedSA-VeRA.%20In%20this%20way%2C%20we%20establish%20a%20general%20paradigm%0Afor%20integrating%20LoRA%20with%20FL%2C%20offering%20guidance%20for%20future%20work%20on%20subsequent%0ALoRA%20variants%20combined%20with%20FL.%20Extensive%20experimental%20results%20on%20natural%0Alanguage%20understanding%20and%20generation%20tasks%20demonstrate%20the%20effectiveness%20of%0Athe%20proposed%20method.%0A&entry.1838667208=http%3A//arxiv.org/abs/2410.01463v1&entry.124074799=Read"},
{"title": "SGBA: Semantic Gaussian Mixture Model-Based LiDAR Bundle Adjustment", "author": "Xingyu Ji and Shenghai Yuan and Jianping Li and Pengyu Yin and Haozhi Cao and Lihua Xie", "abstract": "  LiDAR bundle adjustment (BA) is an effective approach to reduce the drifts in\npose estimation from the front-end. Existing works on LiDAR BA usually rely on\npredefined geometric features for landmark representation. This reliance\nrestricts generalizability, as the system will inevitably deteriorate in\nenvironments where these specific features are absent. To address this issue,\nwe propose SGBA, a LiDAR BA scheme that models the environment as a semantic\nGaussian mixture model (GMM) without predefined feature types. This approach\nencodes both geometric and semantic information, offering a comprehensive and\ngeneral representation adaptable to various environments. Additionally, to\nlimit computational complexity while ensuring generalizability, we propose an\nadaptive semantic selection framework that selects the most informative\nsemantic clusters for optimization by evaluating the condition number of the\ncost function. Lastly, we introduce a probabilistic feature association scheme\nthat considers the entire probability density of assignments, which can manage\nuncertainties in measurement and initial pose estimation. We have conducted\nvarious experiments and the results demonstrate that SGBA can achieve accurate\nand robust pose refinement even in challenging scenarios with low-quality\ninitial pose estimation and limited geometric features. We plan to open-source\nthe work for the benefit of the community https://github.com/Ji1Xinyu/SGBA.\n", "link": "http://arxiv.org/abs/2410.01618v1", "date": "2024-10-02", "relevancy": 2.2729, "topK": [{"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.578}, {"title": "RTG-SLAM: Real-time 3D Reconstruction at Scale using Gaussian Splatting", "link": "http://arxiv.org/abs/2404.19706v1", "similarity": 0.5763}, {"title": "GST: Precise 3D Human Body from a Single Image with Gaussian Splatting\n  Transformers", "link": "http://arxiv.org/abs/2409.04196v1", "similarity": 0.5562}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20SGBA%3A%20Semantic%20Gaussian%20Mixture%20Model-Based%20LiDAR%20Bundle%20Adjustment&body=Title%3A%20SGBA%3A%20Semantic%20Gaussian%20Mixture%20Model-Based%20LiDAR%20Bundle%20Adjustment%0AAuthor%3A%20Xingyu%20Ji%20and%20Shenghai%20Yuan%20and%20Jianping%20Li%20and%20Pengyu%20Yin%20and%20Haozhi%20Cao%20and%20Lihua%20Xie%0AAbstract%3A%20%20%20LiDAR%20bundle%20adjustment%20%28BA%29%20is%20an%20effective%20approach%20to%20reduce%20the%20drifts%20in%0Apose%20estimation%20from%20the%20front-end.%20Existing%20works%20on%20LiDAR%20BA%20usually%20rely%20on%0Apredefined%20geometric%20features%20for%20landmark%20representation.%20This%20reliance%0Arestricts%20generalizability%2C%20as%20the%20system%20will%20inevitably%20deteriorate%20in%0Aenvironments%20where%20these%20specific%20features%20are%20absent.%20To%20address%20this%20issue%2C%0Awe%20propose%20SGBA%2C%20a%20LiDAR%20BA%20scheme%20that%20models%20the%20environment%20as%20a%20semantic%0AGaussian%20mixture%20model%20%28GMM%29%20without%20predefined%20feature%20types.%20This%20approach%0Aencodes%20both%20geometric%20and%20semantic%20information%2C%20offering%20a%20comprehensive%20and%0Ageneral%20representation%20adaptable%20to%20various%20environments.%20Additionally%2C%20to%0Alimit%20computational%20complexity%20while%20ensuring%20generalizability%2C%20we%20propose%20an%0Aadaptive%20semantic%20selection%20framework%20that%20selects%20the%20most%20informative%0Asemantic%20clusters%20for%20optimization%20by%20evaluating%20the%20condition%20number%20of%20the%0Acost%20function.%20Lastly%2C%20we%20introduce%20a%20probabilistic%20feature%20association%20scheme%0Athat%20considers%20the%20entire%20probability%20density%20of%20assignments%2C%20which%20can%20manage%0Auncertainties%20in%20measurement%20and%20initial%20pose%20estimation.%20We%20have%20conducted%0Avarious%20experiments%20and%20the%20results%20demonstrate%20that%20SGBA%20can%20achieve%20accurate%0Aand%20robust%20pose%20refinement%20even%20in%20challenging%20scenarios%20with%20low-quality%0Ainitial%20pose%20estimation%20and%20limited%20geometric%20features.%20We%20plan%20to%20open-source%0Athe%20work%20for%20the%20benefit%20of%20the%20community%20https%3A//github.com/Ji1Xinyu/SGBA.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2410.01618v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DSGBA%253A%2520Semantic%2520Gaussian%2520Mixture%2520Model-Based%2520LiDAR%2520Bundle%2520Adjustment%26entry.906535625%3DXingyu%2520Ji%2520and%2520Shenghai%2520Yuan%2520and%2520Jianping%2520Li%2520and%2520Pengyu%2520Yin%2520and%2520Haozhi%2520Cao%2520and%2520Lihua%2520Xie%26entry.1292438233%3D%2520%2520LiDAR%2520bundle%2520adjustment%2520%2528BA%2529%2520is%2520an%2520effective%2520approach%2520to%2520reduce%2520the%2520drifts%2520in%250Apose%2520estimation%2520from%2520the%2520front-end.%2520Existing%2520works%2520on%2520LiDAR%2520BA%2520usually%2520rely%2520on%250Apredefined%2520geometric%2520features%2520for%2520landmark%2520representation.%2520This%2520reliance%250Arestricts%2520generalizability%252C%2520as%2520the%2520system%2520will%2520inevitably%2520deteriorate%2520in%250Aenvironments%2520where%2520these%2520specific%2520features%2520are%2520absent.%2520To%2520address%2520this%2520issue%252C%250Awe%2520propose%2520SGBA%252C%2520a%2520LiDAR%2520BA%2520scheme%2520that%2520models%2520the%2520environment%2520as%2520a%2520semantic%250AGaussian%2520mixture%2520model%2520%2528GMM%2529%2520without%2520predefined%2520feature%2520types.%2520This%2520approach%250Aencodes%2520both%2520geometric%2520and%2520semantic%2520information%252C%2520offering%2520a%2520comprehensive%2520and%250Ageneral%2520representation%2520adaptable%2520to%2520various%2520environments.%2520Additionally%252C%2520to%250Alimit%2520computational%2520complexity%2520while%2520ensuring%2520generalizability%252C%2520we%2520propose%2520an%250Aadaptive%2520semantic%2520selection%2520framework%2520that%2520selects%2520the%2520most%2520informative%250Asemantic%2520clusters%2520for%2520optimization%2520by%2520evaluating%2520the%2520condition%2520number%2520of%2520the%250Acost%2520function.%2520Lastly%252C%2520we%2520introduce%2520a%2520probabilistic%2520feature%2520association%2520scheme%250Athat%2520considers%2520the%2520entire%2520probability%2520density%2520of%2520assignments%252C%2520which%2520can%2520manage%250Auncertainties%2520in%2520measurement%2520and%2520initial%2520pose%2520estimation.%2520We%2520have%2520conducted%250Avarious%2520experiments%2520and%2520the%2520results%2520demonstrate%2520that%2520SGBA%2520can%2520achieve%2520accurate%250Aand%2520robust%2520pose%2520refinement%2520even%2520in%2520challenging%2520scenarios%2520with%2520low-quality%250Ainitial%2520pose%2520estimation%2520and%2520limited%2520geometric%2520features.%2520We%2520plan%2520to%2520open-source%250Athe%2520work%2520for%2520the%2520benefit%2520of%2520the%2520community%2520https%253A//github.com/Ji1Xinyu/SGBA.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2410.01618v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=SGBA%3A%20Semantic%20Gaussian%20Mixture%20Model-Based%20LiDAR%20Bundle%20Adjustment&entry.906535625=Xingyu%20Ji%20and%20Shenghai%20Yuan%20and%20Jianping%20Li%20and%20Pengyu%20Yin%20and%20Haozhi%20Cao%20and%20Lihua%20Xie&entry.1292438233=%20%20LiDAR%20bundle%20adjustment%20%28BA%29%20is%20an%20effective%20approach%20to%20reduce%20the%20drifts%20in%0Apose%20estimation%20from%20the%20front-end.%20Existing%20works%20on%20LiDAR%20BA%20usually%20rely%20on%0Apredefined%20geometric%20features%20for%20landmark%20representation.%20This%20reliance%0Arestricts%20generalizability%2C%20as%20the%20system%20will%20inevitably%20deteriorate%20in%0Aenvironments%20where%20these%20specific%20features%20are%20absent.%20To%20address%20this%20issue%2C%0Awe%20propose%20SGBA%2C%20a%20LiDAR%20BA%20scheme%20that%20models%20the%20environment%20as%20a%20semantic%0AGaussian%20mixture%20model%20%28GMM%29%20without%20predefined%20feature%20types.%20This%20approach%0Aencodes%20both%20geometric%20and%20semantic%20information%2C%20offering%20a%20comprehensive%20and%0Ageneral%20representation%20adaptable%20to%20various%20environments.%20Additionally%2C%20to%0Alimit%20computational%20complexity%20while%20ensuring%20generalizability%2C%20we%20propose%20an%0Aadaptive%20semantic%20selection%20framework%20that%20selects%20the%20most%20informative%0Asemantic%20clusters%20for%20optimization%20by%20evaluating%20the%20condition%20number%20of%20the%0Acost%20function.%20Lastly%2C%20we%20introduce%20a%20probabilistic%20feature%20association%20scheme%0Athat%20considers%20the%20entire%20probability%20density%20of%20assignments%2C%20which%20can%20manage%0Auncertainties%20in%20measurement%20and%20initial%20pose%20estimation.%20We%20have%20conducted%0Avarious%20experiments%20and%20the%20results%20demonstrate%20that%20SGBA%20can%20achieve%20accurate%0Aand%20robust%20pose%20refinement%20even%20in%20challenging%20scenarios%20with%20low-quality%0Ainitial%20pose%20estimation%20and%20limited%20geometric%20features.%20We%20plan%20to%20open-source%0Athe%20work%20for%20the%20benefit%20of%20the%20community%20https%3A//github.com/Ji1Xinyu/SGBA.%0A&entry.1838667208=http%3A//arxiv.org/abs/2410.01618v1&entry.124074799=Read"},
{"title": "Grouped Discrete Representation Guides Object-Centric Learning", "author": "Rongzhen Zhao and Vivienne Wang and Juho Kannala and Joni Pajarinen", "abstract": "  Similar to humans perceiving visual scenes as objects, Object-Centric\nLearning (OCL) can abstract dense images or videos into sparse object-level\nfeatures. Transformer-based OCL handles complex textures well due to the\ndecoding guidance of discrete representation, obtained by discretizing noisy\nfeatures in image or video feature maps using template features from a\ncodebook. However, treating features as minimal units overlooks their composing\nattributes, thus impeding model generalization; indexing features with natural\nnumbers loses attribute-level commonalities and characteristics, thus\ndiminishing heuristics for model convergence. We propose \\textit{Grouped\nDiscrete Representation} (GDR) to address these issues by grouping features\ninto attributes and indexing them with tuple numbers. In extensive experiments\nacross different query initializations, dataset modalities, and model\narchitectures, GDR consistently improves convergence and generalizability.\nVisualizations show that our method effectively captures attribute-level\ninformation in features. The source code will be available upon acceptance.\n", "link": "http://arxiv.org/abs/2407.01726v2", "date": "2024-10-02", "relevancy": 2.2652, "topK": [{"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5733}, {"title": "Thinking Outside the BBox: Unconstrained Generative Object Compositing", "link": "http://arxiv.org/abs/2409.04559v2", "similarity": 0.5624}, {"title": "Self-supervised Photographic Image Layout Representation Learning", "link": "http://arxiv.org/abs/2403.03740v1", "similarity": 0.5608}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Grouped%20Discrete%20Representation%20Guides%20Object-Centric%20Learning&body=Title%3A%20Grouped%20Discrete%20Representation%20Guides%20Object-Centric%20Learning%0AAuthor%3A%20Rongzhen%20Zhao%20and%20Vivienne%20Wang%20and%20Juho%20Kannala%20and%20Joni%20Pajarinen%0AAbstract%3A%20%20%20Similar%20to%20humans%20perceiving%20visual%20scenes%20as%20objects%2C%20Object-Centric%0ALearning%20%28OCL%29%20can%20abstract%20dense%20images%20or%20videos%20into%20sparse%20object-level%0Afeatures.%20Transformer-based%20OCL%20handles%20complex%20textures%20well%20due%20to%20the%0Adecoding%20guidance%20of%20discrete%20representation%2C%20obtained%20by%20discretizing%20noisy%0Afeatures%20in%20image%20or%20video%20feature%20maps%20using%20template%20features%20from%20a%0Acodebook.%20However%2C%20treating%20features%20as%20minimal%20units%20overlooks%20their%20composing%0Aattributes%2C%20thus%20impeding%20model%20generalization%3B%20indexing%20features%20with%20natural%0Anumbers%20loses%20attribute-level%20commonalities%20and%20characteristics%2C%20thus%0Adiminishing%20heuristics%20for%20model%20convergence.%20We%20propose%20%5Ctextit%7BGrouped%0ADiscrete%20Representation%7D%20%28GDR%29%20to%20address%20these%20issues%20by%20grouping%20features%0Ainto%20attributes%20and%20indexing%20them%20with%20tuple%20numbers.%20In%20extensive%20experiments%0Aacross%20different%20query%20initializations%2C%20dataset%20modalities%2C%20and%20model%0Aarchitectures%2C%20GDR%20consistently%20improves%20convergence%20and%20generalizability.%0AVisualizations%20show%20that%20our%20method%20effectively%20captures%20attribute-level%0Ainformation%20in%20features.%20The%20source%20code%20will%20be%20available%20upon%20acceptance.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2407.01726v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DGrouped%2520Discrete%2520Representation%2520Guides%2520Object-Centric%2520Learning%26entry.906535625%3DRongzhen%2520Zhao%2520and%2520Vivienne%2520Wang%2520and%2520Juho%2520Kannala%2520and%2520Joni%2520Pajarinen%26entry.1292438233%3D%2520%2520Similar%2520to%2520humans%2520perceiving%2520visual%2520scenes%2520as%2520objects%252C%2520Object-Centric%250ALearning%2520%2528OCL%2529%2520can%2520abstract%2520dense%2520images%2520or%2520videos%2520into%2520sparse%2520object-level%250Afeatures.%2520Transformer-based%2520OCL%2520handles%2520complex%2520textures%2520well%2520due%2520to%2520the%250Adecoding%2520guidance%2520of%2520discrete%2520representation%252C%2520obtained%2520by%2520discretizing%2520noisy%250Afeatures%2520in%2520image%2520or%2520video%2520feature%2520maps%2520using%2520template%2520features%2520from%2520a%250Acodebook.%2520However%252C%2520treating%2520features%2520as%2520minimal%2520units%2520overlooks%2520their%2520composing%250Aattributes%252C%2520thus%2520impeding%2520model%2520generalization%253B%2520indexing%2520features%2520with%2520natural%250Anumbers%2520loses%2520attribute-level%2520commonalities%2520and%2520characteristics%252C%2520thus%250Adiminishing%2520heuristics%2520for%2520model%2520convergence.%2520We%2520propose%2520%255Ctextit%257BGrouped%250ADiscrete%2520Representation%257D%2520%2528GDR%2529%2520to%2520address%2520these%2520issues%2520by%2520grouping%2520features%250Ainto%2520attributes%2520and%2520indexing%2520them%2520with%2520tuple%2520numbers.%2520In%2520extensive%2520experiments%250Aacross%2520different%2520query%2520initializations%252C%2520dataset%2520modalities%252C%2520and%2520model%250Aarchitectures%252C%2520GDR%2520consistently%2520improves%2520convergence%2520and%2520generalizability.%250AVisualizations%2520show%2520that%2520our%2520method%2520effectively%2520captures%2520attribute-level%250Ainformation%2520in%2520features.%2520The%2520source%2520code%2520will%2520be%2520available%2520upon%2520acceptance.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2407.01726v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Grouped%20Discrete%20Representation%20Guides%20Object-Centric%20Learning&entry.906535625=Rongzhen%20Zhao%20and%20Vivienne%20Wang%20and%20Juho%20Kannala%20and%20Joni%20Pajarinen&entry.1292438233=%20%20Similar%20to%20humans%20perceiving%20visual%20scenes%20as%20objects%2C%20Object-Centric%0ALearning%20%28OCL%29%20can%20abstract%20dense%20images%20or%20videos%20into%20sparse%20object-level%0Afeatures.%20Transformer-based%20OCL%20handles%20complex%20textures%20well%20due%20to%20the%0Adecoding%20guidance%20of%20discrete%20representation%2C%20obtained%20by%20discretizing%20noisy%0Afeatures%20in%20image%20or%20video%20feature%20maps%20using%20template%20features%20from%20a%0Acodebook.%20However%2C%20treating%20features%20as%20minimal%20units%20overlooks%20their%20composing%0Aattributes%2C%20thus%20impeding%20model%20generalization%3B%20indexing%20features%20with%20natural%0Anumbers%20loses%20attribute-level%20commonalities%20and%20characteristics%2C%20thus%0Adiminishing%20heuristics%20for%20model%20convergence.%20We%20propose%20%5Ctextit%7BGrouped%0ADiscrete%20Representation%7D%20%28GDR%29%20to%20address%20these%20issues%20by%20grouping%20features%0Ainto%20attributes%20and%20indexing%20them%20with%20tuple%20numbers.%20In%20extensive%20experiments%0Aacross%20different%20query%20initializations%2C%20dataset%20modalities%2C%20and%20model%0Aarchitectures%2C%20GDR%20consistently%20improves%20convergence%20and%20generalizability.%0AVisualizations%20show%20that%20our%20method%20effectively%20captures%20attribute-level%0Ainformation%20in%20features.%20The%20source%20code%20will%20be%20available%20upon%20acceptance.%0A&entry.1838667208=http%3A//arxiv.org/abs/2407.01726v2&entry.124074799=Read"},
{"title": "Fira: Can We Achieve Full-rank Training of LLMs Under Low-rank\n  Constraint?", "author": "Xi Chen and Kaituo Feng and Changsheng Li and Xunhao Lai and Xiangyu Yue and Ye Yuan and Guoren Wang", "abstract": "  Low-rank training has emerged as a promising approach for reducing memory\nusage in training Large Language Models (LLMs). Previous methods either rely on\ndecomposing weight matrices (e.g., LoRA), or seek to decompose gradient\nmatrices (e.g., GaLore) to ensure reduced memory consumption. However, both of\nthem constrain the training in a low-rank subspace, thus inevitably leading to\nsub-optimal performance. This raises a question: whether it is possible to\nconsistently preserve the low-rank constraint for memory efficiency, while\nachieving full-rank training (i.e., training with full-rank gradients of\nfull-rank weights) to avoid inferior outcomes? In this paper, we propose a new\nplug-and-play training framework for LLMs called Fira, as the first attempt to\nachieve this goal. First, we observe an interesting phenomenon during LLM\ntraining: the scaling impact of adaptive optimizers (e.g., Adam) on the\ngradient norm remains similar from low-rank to full-rank training. Based on\nthis observation, we propose a norm-based scaling method, which utilizes the\nscaling impact of low-rank optimizers as substitutes for that of original\nfull-rank optimizers to enable full-rank training. In this way, we can preserve\nthe low-rank constraint in the optimizer while achieving full-rank training for\nbetter performance. Moreover, we find that there are sudden gradient rises\nduring the optimization process, potentially causing loss spikes. To address\nthis, we further put forward a norm-growth limiter to smooth the gradient via\nregulating the relative increase of gradient norms. Extensive experiments on\nthe pre-training and fine-tuning of LLMs show that Fira outperforms both LoRA\nand GaLore, achieving performance that is comparable to or even better than\nfull-rank training.\n", "link": "http://arxiv.org/abs/2410.01623v1", "date": "2024-10-02", "relevancy": 2.2643, "topK": [{"title": "Self-supervised Photographic Image Layout Representation Learning", "link": "http://arxiv.org/abs/2403.03740v1", "similarity": 0.462}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.4498}, {"title": "Tuning-Free Noise Rectification for High Fidelity Image-to-Video\n  Generation", "link": "http://arxiv.org/abs/2403.02827v1", "similarity": 0.4468}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Fira%3A%20Can%20We%20Achieve%20Full-rank%20Training%20of%20LLMs%20Under%20Low-rank%0A%20%20Constraint%3F&body=Title%3A%20Fira%3A%20Can%20We%20Achieve%20Full-rank%20Training%20of%20LLMs%20Under%20Low-rank%0A%20%20Constraint%3F%0AAuthor%3A%20Xi%20Chen%20and%20Kaituo%20Feng%20and%20Changsheng%20Li%20and%20Xunhao%20Lai%20and%20Xiangyu%20Yue%20and%20Ye%20Yuan%20and%20Guoren%20Wang%0AAbstract%3A%20%20%20Low-rank%20training%20has%20emerged%20as%20a%20promising%20approach%20for%20reducing%20memory%0Ausage%20in%20training%20Large%20Language%20Models%20%28LLMs%29.%20Previous%20methods%20either%20rely%20on%0Adecomposing%20weight%20matrices%20%28e.g.%2C%20LoRA%29%2C%20or%20seek%20to%20decompose%20gradient%0Amatrices%20%28e.g.%2C%20GaLore%29%20to%20ensure%20reduced%20memory%20consumption.%20However%2C%20both%20of%0Athem%20constrain%20the%20training%20in%20a%20low-rank%20subspace%2C%20thus%20inevitably%20leading%20to%0Asub-optimal%20performance.%20This%20raises%20a%20question%3A%20whether%20it%20is%20possible%20to%0Aconsistently%20preserve%20the%20low-rank%20constraint%20for%20memory%20efficiency%2C%20while%0Aachieving%20full-rank%20training%20%28i.e.%2C%20training%20with%20full-rank%20gradients%20of%0Afull-rank%20weights%29%20to%20avoid%20inferior%20outcomes%3F%20In%20this%20paper%2C%20we%20propose%20a%20new%0Aplug-and-play%20training%20framework%20for%20LLMs%20called%20Fira%2C%20as%20the%20first%20attempt%20to%0Aachieve%20this%20goal.%20First%2C%20we%20observe%20an%20interesting%20phenomenon%20during%20LLM%0Atraining%3A%20the%20scaling%20impact%20of%20adaptive%20optimizers%20%28e.g.%2C%20Adam%29%20on%20the%0Agradient%20norm%20remains%20similar%20from%20low-rank%20to%20full-rank%20training.%20Based%20on%0Athis%20observation%2C%20we%20propose%20a%20norm-based%20scaling%20method%2C%20which%20utilizes%20the%0Ascaling%20impact%20of%20low-rank%20optimizers%20as%20substitutes%20for%20that%20of%20original%0Afull-rank%20optimizers%20to%20enable%20full-rank%20training.%20In%20this%20way%2C%20we%20can%20preserve%0Athe%20low-rank%20constraint%20in%20the%20optimizer%20while%20achieving%20full-rank%20training%20for%0Abetter%20performance.%20Moreover%2C%20we%20find%20that%20there%20are%20sudden%20gradient%20rises%0Aduring%20the%20optimization%20process%2C%20potentially%20causing%20loss%20spikes.%20To%20address%0Athis%2C%20we%20further%20put%20forward%20a%20norm-growth%20limiter%20to%20smooth%20the%20gradient%20via%0Aregulating%20the%20relative%20increase%20of%20gradient%20norms.%20Extensive%20experiments%20on%0Athe%20pre-training%20and%20fine-tuning%20of%20LLMs%20show%20that%20Fira%20outperforms%20both%20LoRA%0Aand%20GaLore%2C%20achieving%20performance%20that%20is%20comparable%20to%20or%20even%20better%20than%0Afull-rank%20training.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2410.01623v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DFira%253A%2520Can%2520We%2520Achieve%2520Full-rank%2520Training%2520of%2520LLMs%2520Under%2520Low-rank%250A%2520%2520Constraint%253F%26entry.906535625%3DXi%2520Chen%2520and%2520Kaituo%2520Feng%2520and%2520Changsheng%2520Li%2520and%2520Xunhao%2520Lai%2520and%2520Xiangyu%2520Yue%2520and%2520Ye%2520Yuan%2520and%2520Guoren%2520Wang%26entry.1292438233%3D%2520%2520Low-rank%2520training%2520has%2520emerged%2520as%2520a%2520promising%2520approach%2520for%2520reducing%2520memory%250Ausage%2520in%2520training%2520Large%2520Language%2520Models%2520%2528LLMs%2529.%2520Previous%2520methods%2520either%2520rely%2520on%250Adecomposing%2520weight%2520matrices%2520%2528e.g.%252C%2520LoRA%2529%252C%2520or%2520seek%2520to%2520decompose%2520gradient%250Amatrices%2520%2528e.g.%252C%2520GaLore%2529%2520to%2520ensure%2520reduced%2520memory%2520consumption.%2520However%252C%2520both%2520of%250Athem%2520constrain%2520the%2520training%2520in%2520a%2520low-rank%2520subspace%252C%2520thus%2520inevitably%2520leading%2520to%250Asub-optimal%2520performance.%2520This%2520raises%2520a%2520question%253A%2520whether%2520it%2520is%2520possible%2520to%250Aconsistently%2520preserve%2520the%2520low-rank%2520constraint%2520for%2520memory%2520efficiency%252C%2520while%250Aachieving%2520full-rank%2520training%2520%2528i.e.%252C%2520training%2520with%2520full-rank%2520gradients%2520of%250Afull-rank%2520weights%2529%2520to%2520avoid%2520inferior%2520outcomes%253F%2520In%2520this%2520paper%252C%2520we%2520propose%2520a%2520new%250Aplug-and-play%2520training%2520framework%2520for%2520LLMs%2520called%2520Fira%252C%2520as%2520the%2520first%2520attempt%2520to%250Aachieve%2520this%2520goal.%2520First%252C%2520we%2520observe%2520an%2520interesting%2520phenomenon%2520during%2520LLM%250Atraining%253A%2520the%2520scaling%2520impact%2520of%2520adaptive%2520optimizers%2520%2528e.g.%252C%2520Adam%2529%2520on%2520the%250Agradient%2520norm%2520remains%2520similar%2520from%2520low-rank%2520to%2520full-rank%2520training.%2520Based%2520on%250Athis%2520observation%252C%2520we%2520propose%2520a%2520norm-based%2520scaling%2520method%252C%2520which%2520utilizes%2520the%250Ascaling%2520impact%2520of%2520low-rank%2520optimizers%2520as%2520substitutes%2520for%2520that%2520of%2520original%250Afull-rank%2520optimizers%2520to%2520enable%2520full-rank%2520training.%2520In%2520this%2520way%252C%2520we%2520can%2520preserve%250Athe%2520low-rank%2520constraint%2520in%2520the%2520optimizer%2520while%2520achieving%2520full-rank%2520training%2520for%250Abetter%2520performance.%2520Moreover%252C%2520we%2520find%2520that%2520there%2520are%2520sudden%2520gradient%2520rises%250Aduring%2520the%2520optimization%2520process%252C%2520potentially%2520causing%2520loss%2520spikes.%2520To%2520address%250Athis%252C%2520we%2520further%2520put%2520forward%2520a%2520norm-growth%2520limiter%2520to%2520smooth%2520the%2520gradient%2520via%250Aregulating%2520the%2520relative%2520increase%2520of%2520gradient%2520norms.%2520Extensive%2520experiments%2520on%250Athe%2520pre-training%2520and%2520fine-tuning%2520of%2520LLMs%2520show%2520that%2520Fira%2520outperforms%2520both%2520LoRA%250Aand%2520GaLore%252C%2520achieving%2520performance%2520that%2520is%2520comparable%2520to%2520or%2520even%2520better%2520than%250Afull-rank%2520training.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2410.01623v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Fira%3A%20Can%20We%20Achieve%20Full-rank%20Training%20of%20LLMs%20Under%20Low-rank%0A%20%20Constraint%3F&entry.906535625=Xi%20Chen%20and%20Kaituo%20Feng%20and%20Changsheng%20Li%20and%20Xunhao%20Lai%20and%20Xiangyu%20Yue%20and%20Ye%20Yuan%20and%20Guoren%20Wang&entry.1292438233=%20%20Low-rank%20training%20has%20emerged%20as%20a%20promising%20approach%20for%20reducing%20memory%0Ausage%20in%20training%20Large%20Language%20Models%20%28LLMs%29.%20Previous%20methods%20either%20rely%20on%0Adecomposing%20weight%20matrices%20%28e.g.%2C%20LoRA%29%2C%20or%20seek%20to%20decompose%20gradient%0Amatrices%20%28e.g.%2C%20GaLore%29%20to%20ensure%20reduced%20memory%20consumption.%20However%2C%20both%20of%0Athem%20constrain%20the%20training%20in%20a%20low-rank%20subspace%2C%20thus%20inevitably%20leading%20to%0Asub-optimal%20performance.%20This%20raises%20a%20question%3A%20whether%20it%20is%20possible%20to%0Aconsistently%20preserve%20the%20low-rank%20constraint%20for%20memory%20efficiency%2C%20while%0Aachieving%20full-rank%20training%20%28i.e.%2C%20training%20with%20full-rank%20gradients%20of%0Afull-rank%20weights%29%20to%20avoid%20inferior%20outcomes%3F%20In%20this%20paper%2C%20we%20propose%20a%20new%0Aplug-and-play%20training%20framework%20for%20LLMs%20called%20Fira%2C%20as%20the%20first%20attempt%20to%0Aachieve%20this%20goal.%20First%2C%20we%20observe%20an%20interesting%20phenomenon%20during%20LLM%0Atraining%3A%20the%20scaling%20impact%20of%20adaptive%20optimizers%20%28e.g.%2C%20Adam%29%20on%20the%0Agradient%20norm%20remains%20similar%20from%20low-rank%20to%20full-rank%20training.%20Based%20on%0Athis%20observation%2C%20we%20propose%20a%20norm-based%20scaling%20method%2C%20which%20utilizes%20the%0Ascaling%20impact%20of%20low-rank%20optimizers%20as%20substitutes%20for%20that%20of%20original%0Afull-rank%20optimizers%20to%20enable%20full-rank%20training.%20In%20this%20way%2C%20we%20can%20preserve%0Athe%20low-rank%20constraint%20in%20the%20optimizer%20while%20achieving%20full-rank%20training%20for%0Abetter%20performance.%20Moreover%2C%20we%20find%20that%20there%20are%20sudden%20gradient%20rises%0Aduring%20the%20optimization%20process%2C%20potentially%20causing%20loss%20spikes.%20To%20address%0Athis%2C%20we%20further%20put%20forward%20a%20norm-growth%20limiter%20to%20smooth%20the%20gradient%20via%0Aregulating%20the%20relative%20increase%20of%20gradient%20norms.%20Extensive%20experiments%20on%0Athe%20pre-training%20and%20fine-tuning%20of%20LLMs%20show%20that%20Fira%20outperforms%20both%20LoRA%0Aand%20GaLore%2C%20achieving%20performance%20that%20is%20comparable%20to%20or%20even%20better%20than%0Afull-rank%20training.%0A&entry.1838667208=http%3A//arxiv.org/abs/2410.01623v1&entry.124074799=Read"},
{"title": "ImageFolder: Autoregressive Image Generation with Folded Tokens", "author": "Xiang Li and Hao Chen and Kai Qiu and Jason Kuen and Jiuxiang Gu and Bhiksha Raj and Zhe Lin", "abstract": "  Image tokenizers are crucial for visual generative models, e.g., diffusion\nmodels (DMs) and autoregressive (AR) models, as they construct the latent\nrepresentation for modeling. Increasing token length is a common approach to\nimprove the image reconstruction quality. However, tokenizers with longer token\nlengths are not guaranteed to achieve better generation quality. There exists a\ntrade-off between reconstruction and generation quality regarding token length.\nIn this paper, we investigate the impact of token length on both image\nreconstruction and generation and provide a flexible solution to the tradeoff.\nWe propose ImageFolder, a semantic tokenizer that provides spatially aligned\nimage tokens that can be folded during autoregressive modeling to improve both\ngeneration efficiency and quality. To enhance the representative capability\nwithout increasing token length, we leverage dual-branch product quantization\nto capture different contexts of images. Specifically, semantic regularization\nis introduced in one branch to encourage compacted semantic information while\nanother branch is designed to capture the remaining pixel-level details.\nExtensive experiments demonstrate the superior quality of image generation and\nshorter token length with ImageFolder tokenizer.\n", "link": "http://arxiv.org/abs/2410.01756v1", "date": "2024-10-02", "relevancy": 2.2617, "topK": [{"title": "ToDo: Token Downsampling for Efficient Generation of High-Resolution\n  Images", "link": "http://arxiv.org/abs/2402.13573v2", "similarity": 0.6369}, {"title": "Scaling Rectified Flow Transformers for High-Resolution Image Synthesis", "link": "http://arxiv.org/abs/2403.03206v1", "similarity": 0.5373}, {"title": "Magic-Me: Identity-Specific Video Customized Diffusion", "link": "http://arxiv.org/abs/2402.09368v1", "similarity": 0.5051}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20ImageFolder%3A%20Autoregressive%20Image%20Generation%20with%20Folded%20Tokens&body=Title%3A%20ImageFolder%3A%20Autoregressive%20Image%20Generation%20with%20Folded%20Tokens%0AAuthor%3A%20Xiang%20Li%20and%20Hao%20Chen%20and%20Kai%20Qiu%20and%20Jason%20Kuen%20and%20Jiuxiang%20Gu%20and%20Bhiksha%20Raj%20and%20Zhe%20Lin%0AAbstract%3A%20%20%20Image%20tokenizers%20are%20crucial%20for%20visual%20generative%20models%2C%20e.g.%2C%20diffusion%0Amodels%20%28DMs%29%20and%20autoregressive%20%28AR%29%20models%2C%20as%20they%20construct%20the%20latent%0Arepresentation%20for%20modeling.%20Increasing%20token%20length%20is%20a%20common%20approach%20to%0Aimprove%20the%20image%20reconstruction%20quality.%20However%2C%20tokenizers%20with%20longer%20token%0Alengths%20are%20not%20guaranteed%20to%20achieve%20better%20generation%20quality.%20There%20exists%20a%0Atrade-off%20between%20reconstruction%20and%20generation%20quality%20regarding%20token%20length.%0AIn%20this%20paper%2C%20we%20investigate%20the%20impact%20of%20token%20length%20on%20both%20image%0Areconstruction%20and%20generation%20and%20provide%20a%20flexible%20solution%20to%20the%20tradeoff.%0AWe%20propose%20ImageFolder%2C%20a%20semantic%20tokenizer%20that%20provides%20spatially%20aligned%0Aimage%20tokens%20that%20can%20be%20folded%20during%20autoregressive%20modeling%20to%20improve%20both%0Ageneration%20efficiency%20and%20quality.%20To%20enhance%20the%20representative%20capability%0Awithout%20increasing%20token%20length%2C%20we%20leverage%20dual-branch%20product%20quantization%0Ato%20capture%20different%20contexts%20of%20images.%20Specifically%2C%20semantic%20regularization%0Ais%20introduced%20in%20one%20branch%20to%20encourage%20compacted%20semantic%20information%20while%0Aanother%20branch%20is%20designed%20to%20capture%20the%20remaining%20pixel-level%20details.%0AExtensive%20experiments%20demonstrate%20the%20superior%20quality%20of%20image%20generation%20and%0Ashorter%20token%20length%20with%20ImageFolder%20tokenizer.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2410.01756v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DImageFolder%253A%2520Autoregressive%2520Image%2520Generation%2520with%2520Folded%2520Tokens%26entry.906535625%3DXiang%2520Li%2520and%2520Hao%2520Chen%2520and%2520Kai%2520Qiu%2520and%2520Jason%2520Kuen%2520and%2520Jiuxiang%2520Gu%2520and%2520Bhiksha%2520Raj%2520and%2520Zhe%2520Lin%26entry.1292438233%3D%2520%2520Image%2520tokenizers%2520are%2520crucial%2520for%2520visual%2520generative%2520models%252C%2520e.g.%252C%2520diffusion%250Amodels%2520%2528DMs%2529%2520and%2520autoregressive%2520%2528AR%2529%2520models%252C%2520as%2520they%2520construct%2520the%2520latent%250Arepresentation%2520for%2520modeling.%2520Increasing%2520token%2520length%2520is%2520a%2520common%2520approach%2520to%250Aimprove%2520the%2520image%2520reconstruction%2520quality.%2520However%252C%2520tokenizers%2520with%2520longer%2520token%250Alengths%2520are%2520not%2520guaranteed%2520to%2520achieve%2520better%2520generation%2520quality.%2520There%2520exists%2520a%250Atrade-off%2520between%2520reconstruction%2520and%2520generation%2520quality%2520regarding%2520token%2520length.%250AIn%2520this%2520paper%252C%2520we%2520investigate%2520the%2520impact%2520of%2520token%2520length%2520on%2520both%2520image%250Areconstruction%2520and%2520generation%2520and%2520provide%2520a%2520flexible%2520solution%2520to%2520the%2520tradeoff.%250AWe%2520propose%2520ImageFolder%252C%2520a%2520semantic%2520tokenizer%2520that%2520provides%2520spatially%2520aligned%250Aimage%2520tokens%2520that%2520can%2520be%2520folded%2520during%2520autoregressive%2520modeling%2520to%2520improve%2520both%250Ageneration%2520efficiency%2520and%2520quality.%2520To%2520enhance%2520the%2520representative%2520capability%250Awithout%2520increasing%2520token%2520length%252C%2520we%2520leverage%2520dual-branch%2520product%2520quantization%250Ato%2520capture%2520different%2520contexts%2520of%2520images.%2520Specifically%252C%2520semantic%2520regularization%250Ais%2520introduced%2520in%2520one%2520branch%2520to%2520encourage%2520compacted%2520semantic%2520information%2520while%250Aanother%2520branch%2520is%2520designed%2520to%2520capture%2520the%2520remaining%2520pixel-level%2520details.%250AExtensive%2520experiments%2520demonstrate%2520the%2520superior%2520quality%2520of%2520image%2520generation%2520and%250Ashorter%2520token%2520length%2520with%2520ImageFolder%2520tokenizer.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2410.01756v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=ImageFolder%3A%20Autoregressive%20Image%20Generation%20with%20Folded%20Tokens&entry.906535625=Xiang%20Li%20and%20Hao%20Chen%20and%20Kai%20Qiu%20and%20Jason%20Kuen%20and%20Jiuxiang%20Gu%20and%20Bhiksha%20Raj%20and%20Zhe%20Lin&entry.1292438233=%20%20Image%20tokenizers%20are%20crucial%20for%20visual%20generative%20models%2C%20e.g.%2C%20diffusion%0Amodels%20%28DMs%29%20and%20autoregressive%20%28AR%29%20models%2C%20as%20they%20construct%20the%20latent%0Arepresentation%20for%20modeling.%20Increasing%20token%20length%20is%20a%20common%20approach%20to%0Aimprove%20the%20image%20reconstruction%20quality.%20However%2C%20tokenizers%20with%20longer%20token%0Alengths%20are%20not%20guaranteed%20to%20achieve%20better%20generation%20quality.%20There%20exists%20a%0Atrade-off%20between%20reconstruction%20and%20generation%20quality%20regarding%20token%20length.%0AIn%20this%20paper%2C%20we%20investigate%20the%20impact%20of%20token%20length%20on%20both%20image%0Areconstruction%20and%20generation%20and%20provide%20a%20flexible%20solution%20to%20the%20tradeoff.%0AWe%20propose%20ImageFolder%2C%20a%20semantic%20tokenizer%20that%20provides%20spatially%20aligned%0Aimage%20tokens%20that%20can%20be%20folded%20during%20autoregressive%20modeling%20to%20improve%20both%0Ageneration%20efficiency%20and%20quality.%20To%20enhance%20the%20representative%20capability%0Awithout%20increasing%20token%20length%2C%20we%20leverage%20dual-branch%20product%20quantization%0Ato%20capture%20different%20contexts%20of%20images.%20Specifically%2C%20semantic%20regularization%0Ais%20introduced%20in%20one%20branch%20to%20encourage%20compacted%20semantic%20information%20while%0Aanother%20branch%20is%20designed%20to%20capture%20the%20remaining%20pixel-level%20details.%0AExtensive%20experiments%20demonstrate%20the%20superior%20quality%20of%20image%20generation%20and%0Ashorter%20token%20length%20with%20ImageFolder%20tokenizer.%0A&entry.1838667208=http%3A//arxiv.org/abs/2410.01756v1&entry.124074799=Read"},
{"title": "MallowsPO: Fine-Tune Your LLM with Preference Dispersions", "author": "Haoxian Chen and Hanyang Zhao and Henry Lam and David Yao and Wenpin Tang", "abstract": "  Direct Preference Optimization (DPO) has recently emerged as a popular\napproach to improve reinforcement learning with human feedback (RLHF), leading\nto better techniques to fine-tune large language models (LLM). A weakness of\nDPO, however, lies in its lack of capability to characterize the diversity of\nhuman preferences. Inspired by Mallows' theory of preference ranking, we\ndevelop in this paper a new approach, the MallowsPO. A distinct feature of this\napproach is a dispersion index, which reflects the dispersion of human\npreference to prompts. We show that existing DPO models can be reduced to\nspecial cases of this dispersion index, thus unified with MallowsPO. More\nimportantly, we demonstrate (empirically) how to use this dispersion index to\nenhance the performance of DPO in a broad array of benchmark tasks, from\nsynthetic bandit selection to controllable generations and dialogues, while\nmaintaining great generalization capabilities. MallowsPO is also compatible\nwith other SOTA offline preference optimization methods, boosting nearly 2\\%\nextra LC win rate when used as a plugin for fine-tuning Llama3-Instruct.\n", "link": "http://arxiv.org/abs/2405.14953v3", "date": "2024-10-02", "relevancy": 2.2607, "topK": [{"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.4573}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.4496}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.4496}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20MallowsPO%3A%20Fine-Tune%20Your%20LLM%20with%20Preference%20Dispersions&body=Title%3A%20MallowsPO%3A%20Fine-Tune%20Your%20LLM%20with%20Preference%20Dispersions%0AAuthor%3A%20Haoxian%20Chen%20and%20Hanyang%20Zhao%20and%20Henry%20Lam%20and%20David%20Yao%20and%20Wenpin%20Tang%0AAbstract%3A%20%20%20Direct%20Preference%20Optimization%20%28DPO%29%20has%20recently%20emerged%20as%20a%20popular%0Aapproach%20to%20improve%20reinforcement%20learning%20with%20human%20feedback%20%28RLHF%29%2C%20leading%0Ato%20better%20techniques%20to%20fine-tune%20large%20language%20models%20%28LLM%29.%20A%20weakness%20of%0ADPO%2C%20however%2C%20lies%20in%20its%20lack%20of%20capability%20to%20characterize%20the%20diversity%20of%0Ahuman%20preferences.%20Inspired%20by%20Mallows%27%20theory%20of%20preference%20ranking%2C%20we%0Adevelop%20in%20this%20paper%20a%20new%20approach%2C%20the%20MallowsPO.%20A%20distinct%20feature%20of%20this%0Aapproach%20is%20a%20dispersion%20index%2C%20which%20reflects%20the%20dispersion%20of%20human%0Apreference%20to%20prompts.%20We%20show%20that%20existing%20DPO%20models%20can%20be%20reduced%20to%0Aspecial%20cases%20of%20this%20dispersion%20index%2C%20thus%20unified%20with%20MallowsPO.%20More%0Aimportantly%2C%20we%20demonstrate%20%28empirically%29%20how%20to%20use%20this%20dispersion%20index%20to%0Aenhance%20the%20performance%20of%20DPO%20in%20a%20broad%20array%20of%20benchmark%20tasks%2C%20from%0Asynthetic%20bandit%20selection%20to%20controllable%20generations%20and%20dialogues%2C%20while%0Amaintaining%20great%20generalization%20capabilities.%20MallowsPO%20is%20also%20compatible%0Awith%20other%20SOTA%20offline%20preference%20optimization%20methods%2C%20boosting%20nearly%202%5C%25%0Aextra%20LC%20win%20rate%20when%20used%20as%20a%20plugin%20for%20fine-tuning%20Llama3-Instruct.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2405.14953v3%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DMallowsPO%253A%2520Fine-Tune%2520Your%2520LLM%2520with%2520Preference%2520Dispersions%26entry.906535625%3DHaoxian%2520Chen%2520and%2520Hanyang%2520Zhao%2520and%2520Henry%2520Lam%2520and%2520David%2520Yao%2520and%2520Wenpin%2520Tang%26entry.1292438233%3D%2520%2520Direct%2520Preference%2520Optimization%2520%2528DPO%2529%2520has%2520recently%2520emerged%2520as%2520a%2520popular%250Aapproach%2520to%2520improve%2520reinforcement%2520learning%2520with%2520human%2520feedback%2520%2528RLHF%2529%252C%2520leading%250Ato%2520better%2520techniques%2520to%2520fine-tune%2520large%2520language%2520models%2520%2528LLM%2529.%2520A%2520weakness%2520of%250ADPO%252C%2520however%252C%2520lies%2520in%2520its%2520lack%2520of%2520capability%2520to%2520characterize%2520the%2520diversity%2520of%250Ahuman%2520preferences.%2520Inspired%2520by%2520Mallows%2527%2520theory%2520of%2520preference%2520ranking%252C%2520we%250Adevelop%2520in%2520this%2520paper%2520a%2520new%2520approach%252C%2520the%2520MallowsPO.%2520A%2520distinct%2520feature%2520of%2520this%250Aapproach%2520is%2520a%2520dispersion%2520index%252C%2520which%2520reflects%2520the%2520dispersion%2520of%2520human%250Apreference%2520to%2520prompts.%2520We%2520show%2520that%2520existing%2520DPO%2520models%2520can%2520be%2520reduced%2520to%250Aspecial%2520cases%2520of%2520this%2520dispersion%2520index%252C%2520thus%2520unified%2520with%2520MallowsPO.%2520More%250Aimportantly%252C%2520we%2520demonstrate%2520%2528empirically%2529%2520how%2520to%2520use%2520this%2520dispersion%2520index%2520to%250Aenhance%2520the%2520performance%2520of%2520DPO%2520in%2520a%2520broad%2520array%2520of%2520benchmark%2520tasks%252C%2520from%250Asynthetic%2520bandit%2520selection%2520to%2520controllable%2520generations%2520and%2520dialogues%252C%2520while%250Amaintaining%2520great%2520generalization%2520capabilities.%2520MallowsPO%2520is%2520also%2520compatible%250Awith%2520other%2520SOTA%2520offline%2520preference%2520optimization%2520methods%252C%2520boosting%2520nearly%25202%255C%2525%250Aextra%2520LC%2520win%2520rate%2520when%2520used%2520as%2520a%2520plugin%2520for%2520fine-tuning%2520Llama3-Instruct.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2405.14953v3%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=MallowsPO%3A%20Fine-Tune%20Your%20LLM%20with%20Preference%20Dispersions&entry.906535625=Haoxian%20Chen%20and%20Hanyang%20Zhao%20and%20Henry%20Lam%20and%20David%20Yao%20and%20Wenpin%20Tang&entry.1292438233=%20%20Direct%20Preference%20Optimization%20%28DPO%29%20has%20recently%20emerged%20as%20a%20popular%0Aapproach%20to%20improve%20reinforcement%20learning%20with%20human%20feedback%20%28RLHF%29%2C%20leading%0Ato%20better%20techniques%20to%20fine-tune%20large%20language%20models%20%28LLM%29.%20A%20weakness%20of%0ADPO%2C%20however%2C%20lies%20in%20its%20lack%20of%20capability%20to%20characterize%20the%20diversity%20of%0Ahuman%20preferences.%20Inspired%20by%20Mallows%27%20theory%20of%20preference%20ranking%2C%20we%0Adevelop%20in%20this%20paper%20a%20new%20approach%2C%20the%20MallowsPO.%20A%20distinct%20feature%20of%20this%0Aapproach%20is%20a%20dispersion%20index%2C%20which%20reflects%20the%20dispersion%20of%20human%0Apreference%20to%20prompts.%20We%20show%20that%20existing%20DPO%20models%20can%20be%20reduced%20to%0Aspecial%20cases%20of%20this%20dispersion%20index%2C%20thus%20unified%20with%20MallowsPO.%20More%0Aimportantly%2C%20we%20demonstrate%20%28empirically%29%20how%20to%20use%20this%20dispersion%20index%20to%0Aenhance%20the%20performance%20of%20DPO%20in%20a%20broad%20array%20of%20benchmark%20tasks%2C%20from%0Asynthetic%20bandit%20selection%20to%20controllable%20generations%20and%20dialogues%2C%20while%0Amaintaining%20great%20generalization%20capabilities.%20MallowsPO%20is%20also%20compatible%0Awith%20other%20SOTA%20offline%20preference%20optimization%20methods%2C%20boosting%20nearly%202%5C%25%0Aextra%20LC%20win%20rate%20when%20used%20as%20a%20plugin%20for%20fine-tuning%20Llama3-Instruct.%0A&entry.1838667208=http%3A//arxiv.org/abs/2405.14953v3&entry.124074799=Read"},
{"title": "Open-Set Graph Anomaly Detection via Normal Structure Regularisation", "author": "Qizhou Wang and Guansong Pang and Mahsa Salehi and Xiaokun Xia and Christopher Leckie", "abstract": "  This paper considers an important Graph Anomaly Detection (GAD) task, namely\nopen-set GAD, which aims to train a detection model using a small number of\nnormal and anomaly nodes (referred to as seen anomalies) to detect both seen\nanomalies and unseen anomalies (i.e., anomalies that cannot be illustrated the\ntraining anomalies). Those labelled training data provide crucial prior\nknowledge about abnormalities for GAD models, enabling substantially reduced\ndetection errors. However, current supervised GAD methods tend to\nover-emphasise fitting the seen anomalies, leading to many errors of detecting\nthe unseen anomalies as normal nodes. Further, existing open-set AD models were\nintroduced to handle Euclidean data, failing to effectively capture\ndiscriminative features from graph structure and node attributes for GAD. In\nthis work, we propose a novel open-set GAD approach, namely normal structure\nregularisation (NSReg), to achieve generalised detection ability to unseen\nanomalies, while maintaining its effectiveness on detecting seen anomalies. The\nkey idea in NSReg is to introduce a regularisation term that enforces the\nlearning of compact, semantically-rich representations of normal nodes based on\ntheir structural relations to other nodes. When being optimised with supervised\nanomaly detection losses, the regularisation term helps incorporate strong\nnormality into the modelling, and thus, it effectively avoids over-fitting the\nseen anomalies and learns a better normality decision boundary, largely\nreducing the false negatives of detecting unseen anomalies as normal. Extensive\nempirical results on seven real-world datasets show that NSReg significantly\noutperforms state-of-the-art competing methods by at least 14% AUC-ROC on the\nunseen anomaly classes and by 10% AUC-ROC on all anomaly classes.\n", "link": "http://arxiv.org/abs/2311.06835v4", "date": "2024-10-02", "relevancy": 2.2587, "topK": [{"title": "Self-supervised Photographic Image Layout Representation Learning", "link": "http://arxiv.org/abs/2403.03740v1", "similarity": 0.4607}, {"title": "Breadcrumbs: Adversarial class-balanced sampling for long-tailed recognition", "link": "https://link.springer.com/chapter/10.1007/978-3-031-20053-3_37", "similarity": 0.4477}, {"title": "Weakly-guided self-supervised pretraining for temporal activity detection", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/25189", "similarity": 0.4468}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Open-Set%20Graph%20Anomaly%20Detection%20via%20Normal%20Structure%20Regularisation&body=Title%3A%20Open-Set%20Graph%20Anomaly%20Detection%20via%20Normal%20Structure%20Regularisation%0AAuthor%3A%20Qizhou%20Wang%20and%20Guansong%20Pang%20and%20Mahsa%20Salehi%20and%20Xiaokun%20Xia%20and%20Christopher%20Leckie%0AAbstract%3A%20%20%20This%20paper%20considers%20an%20important%20Graph%20Anomaly%20Detection%20%28GAD%29%20task%2C%20namely%0Aopen-set%20GAD%2C%20which%20aims%20to%20train%20a%20detection%20model%20using%20a%20small%20number%20of%0Anormal%20and%20anomaly%20nodes%20%28referred%20to%20as%20seen%20anomalies%29%20to%20detect%20both%20seen%0Aanomalies%20and%20unseen%20anomalies%20%28i.e.%2C%20anomalies%20that%20cannot%20be%20illustrated%20the%0Atraining%20anomalies%29.%20Those%20labelled%20training%20data%20provide%20crucial%20prior%0Aknowledge%20about%20abnormalities%20for%20GAD%20models%2C%20enabling%20substantially%20reduced%0Adetection%20errors.%20However%2C%20current%20supervised%20GAD%20methods%20tend%20to%0Aover-emphasise%20fitting%20the%20seen%20anomalies%2C%20leading%20to%20many%20errors%20of%20detecting%0Athe%20unseen%20anomalies%20as%20normal%20nodes.%20Further%2C%20existing%20open-set%20AD%20models%20were%0Aintroduced%20to%20handle%20Euclidean%20data%2C%20failing%20to%20effectively%20capture%0Adiscriminative%20features%20from%20graph%20structure%20and%20node%20attributes%20for%20GAD.%20In%0Athis%20work%2C%20we%20propose%20a%20novel%20open-set%20GAD%20approach%2C%20namely%20normal%20structure%0Aregularisation%20%28NSReg%29%2C%20to%20achieve%20generalised%20detection%20ability%20to%20unseen%0Aanomalies%2C%20while%20maintaining%20its%20effectiveness%20on%20detecting%20seen%20anomalies.%20The%0Akey%20idea%20in%20NSReg%20is%20to%20introduce%20a%20regularisation%20term%20that%20enforces%20the%0Alearning%20of%20compact%2C%20semantically-rich%20representations%20of%20normal%20nodes%20based%20on%0Atheir%20structural%20relations%20to%20other%20nodes.%20When%20being%20optimised%20with%20supervised%0Aanomaly%20detection%20losses%2C%20the%20regularisation%20term%20helps%20incorporate%20strong%0Anormality%20into%20the%20modelling%2C%20and%20thus%2C%20it%20effectively%20avoids%20over-fitting%20the%0Aseen%20anomalies%20and%20learns%20a%20better%20normality%20decision%20boundary%2C%20largely%0Areducing%20the%20false%20negatives%20of%20detecting%20unseen%20anomalies%20as%20normal.%20Extensive%0Aempirical%20results%20on%20seven%20real-world%20datasets%20show%20that%20NSReg%20significantly%0Aoutperforms%20state-of-the-art%20competing%20methods%20by%20at%20least%2014%25%20AUC-ROC%20on%20the%0Aunseen%20anomaly%20classes%20and%20by%2010%25%20AUC-ROC%20on%20all%20anomaly%20classes.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2311.06835v4%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DOpen-Set%2520Graph%2520Anomaly%2520Detection%2520via%2520Normal%2520Structure%2520Regularisation%26entry.906535625%3DQizhou%2520Wang%2520and%2520Guansong%2520Pang%2520and%2520Mahsa%2520Salehi%2520and%2520Xiaokun%2520Xia%2520and%2520Christopher%2520Leckie%26entry.1292438233%3D%2520%2520This%2520paper%2520considers%2520an%2520important%2520Graph%2520Anomaly%2520Detection%2520%2528GAD%2529%2520task%252C%2520namely%250Aopen-set%2520GAD%252C%2520which%2520aims%2520to%2520train%2520a%2520detection%2520model%2520using%2520a%2520small%2520number%2520of%250Anormal%2520and%2520anomaly%2520nodes%2520%2528referred%2520to%2520as%2520seen%2520anomalies%2529%2520to%2520detect%2520both%2520seen%250Aanomalies%2520and%2520unseen%2520anomalies%2520%2528i.e.%252C%2520anomalies%2520that%2520cannot%2520be%2520illustrated%2520the%250Atraining%2520anomalies%2529.%2520Those%2520labelled%2520training%2520data%2520provide%2520crucial%2520prior%250Aknowledge%2520about%2520abnormalities%2520for%2520GAD%2520models%252C%2520enabling%2520substantially%2520reduced%250Adetection%2520errors.%2520However%252C%2520current%2520supervised%2520GAD%2520methods%2520tend%2520to%250Aover-emphasise%2520fitting%2520the%2520seen%2520anomalies%252C%2520leading%2520to%2520many%2520errors%2520of%2520detecting%250Athe%2520unseen%2520anomalies%2520as%2520normal%2520nodes.%2520Further%252C%2520existing%2520open-set%2520AD%2520models%2520were%250Aintroduced%2520to%2520handle%2520Euclidean%2520data%252C%2520failing%2520to%2520effectively%2520capture%250Adiscriminative%2520features%2520from%2520graph%2520structure%2520and%2520node%2520attributes%2520for%2520GAD.%2520In%250Athis%2520work%252C%2520we%2520propose%2520a%2520novel%2520open-set%2520GAD%2520approach%252C%2520namely%2520normal%2520structure%250Aregularisation%2520%2528NSReg%2529%252C%2520to%2520achieve%2520generalised%2520detection%2520ability%2520to%2520unseen%250Aanomalies%252C%2520while%2520maintaining%2520its%2520effectiveness%2520on%2520detecting%2520seen%2520anomalies.%2520The%250Akey%2520idea%2520in%2520NSReg%2520is%2520to%2520introduce%2520a%2520regularisation%2520term%2520that%2520enforces%2520the%250Alearning%2520of%2520compact%252C%2520semantically-rich%2520representations%2520of%2520normal%2520nodes%2520based%2520on%250Atheir%2520structural%2520relations%2520to%2520other%2520nodes.%2520When%2520being%2520optimised%2520with%2520supervised%250Aanomaly%2520detection%2520losses%252C%2520the%2520regularisation%2520term%2520helps%2520incorporate%2520strong%250Anormality%2520into%2520the%2520modelling%252C%2520and%2520thus%252C%2520it%2520effectively%2520avoids%2520over-fitting%2520the%250Aseen%2520anomalies%2520and%2520learns%2520a%2520better%2520normality%2520decision%2520boundary%252C%2520largely%250Areducing%2520the%2520false%2520negatives%2520of%2520detecting%2520unseen%2520anomalies%2520as%2520normal.%2520Extensive%250Aempirical%2520results%2520on%2520seven%2520real-world%2520datasets%2520show%2520that%2520NSReg%2520significantly%250Aoutperforms%2520state-of-the-art%2520competing%2520methods%2520by%2520at%2520least%252014%2525%2520AUC-ROC%2520on%2520the%250Aunseen%2520anomaly%2520classes%2520and%2520by%252010%2525%2520AUC-ROC%2520on%2520all%2520anomaly%2520classes.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2311.06835v4%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Open-Set%20Graph%20Anomaly%20Detection%20via%20Normal%20Structure%20Regularisation&entry.906535625=Qizhou%20Wang%20and%20Guansong%20Pang%20and%20Mahsa%20Salehi%20and%20Xiaokun%20Xia%20and%20Christopher%20Leckie&entry.1292438233=%20%20This%20paper%20considers%20an%20important%20Graph%20Anomaly%20Detection%20%28GAD%29%20task%2C%20namely%0Aopen-set%20GAD%2C%20which%20aims%20to%20train%20a%20detection%20model%20using%20a%20small%20number%20of%0Anormal%20and%20anomaly%20nodes%20%28referred%20to%20as%20seen%20anomalies%29%20to%20detect%20both%20seen%0Aanomalies%20and%20unseen%20anomalies%20%28i.e.%2C%20anomalies%20that%20cannot%20be%20illustrated%20the%0Atraining%20anomalies%29.%20Those%20labelled%20training%20data%20provide%20crucial%20prior%0Aknowledge%20about%20abnormalities%20for%20GAD%20models%2C%20enabling%20substantially%20reduced%0Adetection%20errors.%20However%2C%20current%20supervised%20GAD%20methods%20tend%20to%0Aover-emphasise%20fitting%20the%20seen%20anomalies%2C%20leading%20to%20many%20errors%20of%20detecting%0Athe%20unseen%20anomalies%20as%20normal%20nodes.%20Further%2C%20existing%20open-set%20AD%20models%20were%0Aintroduced%20to%20handle%20Euclidean%20data%2C%20failing%20to%20effectively%20capture%0Adiscriminative%20features%20from%20graph%20structure%20and%20node%20attributes%20for%20GAD.%20In%0Athis%20work%2C%20we%20propose%20a%20novel%20open-set%20GAD%20approach%2C%20namely%20normal%20structure%0Aregularisation%20%28NSReg%29%2C%20to%20achieve%20generalised%20detection%20ability%20to%20unseen%0Aanomalies%2C%20while%20maintaining%20its%20effectiveness%20on%20detecting%20seen%20anomalies.%20The%0Akey%20idea%20in%20NSReg%20is%20to%20introduce%20a%20regularisation%20term%20that%20enforces%20the%0Alearning%20of%20compact%2C%20semantically-rich%20representations%20of%20normal%20nodes%20based%20on%0Atheir%20structural%20relations%20to%20other%20nodes.%20When%20being%20optimised%20with%20supervised%0Aanomaly%20detection%20losses%2C%20the%20regularisation%20term%20helps%20incorporate%20strong%0Anormality%20into%20the%20modelling%2C%20and%20thus%2C%20it%20effectively%20avoids%20over-fitting%20the%0Aseen%20anomalies%20and%20learns%20a%20better%20normality%20decision%20boundary%2C%20largely%0Areducing%20the%20false%20negatives%20of%20detecting%20unseen%20anomalies%20as%20normal.%20Extensive%0Aempirical%20results%20on%20seven%20real-world%20datasets%20show%20that%20NSReg%20significantly%0Aoutperforms%20state-of-the-art%20competing%20methods%20by%20at%20least%2014%25%20AUC-ROC%20on%20the%0Aunseen%20anomaly%20classes%20and%20by%2010%25%20AUC-ROC%20on%20all%20anomaly%20classes.%0A&entry.1838667208=http%3A//arxiv.org/abs/2311.06835v4&entry.124074799=Read"},
{"title": "Organized Grouped Discrete Representation for Object-Centric Learning", "author": "Rongzhen Zhao and Vivienne Wang and Juho Kannala and Joni Pajarinen", "abstract": "  Object-Centric Learning (OCL) represents dense image or video pixels as\nsparse object features. Representative methods utilize discrete representation\ncomposed of Variational Autoencoder (VAE) template features to suppress\npixel-level information redundancy and guide object-level feature aggregation.\nThe most recent advancement, Grouped Discrete Representation (GDR), further\ndecomposes these template features into attributes. However, its naive channel\ngrouping as decomposition may erroneously group channels belonging to different\nattributes together and discretize them as sub-optimal template attributes,\nwhich losses information and harms expressivity. We propose Organized GDR\n(OGDR) to organize channels belonging to the same attributes together for\ncorrect decomposition from features into attributes. In unsupervised\nsegmentation experiments, OGDR is fully superior to GDR in augmentating\nclassical transformer-based OCL methods; it even improves state-of-the-art\ndiffusion-based ones. Codebook PCA and representation similarity analyses show\nthat compared with GDR, our OGDR eliminates redundancy and preserves\ninformation better for guiding object representation learning. The source code\nis available in the supplementary material.\n", "link": "http://arxiv.org/abs/2409.03553v3", "date": "2024-10-02", "relevancy": 2.2412, "topK": [{"title": "Self-supervised Photographic Image Layout Representation Learning", "link": "http://arxiv.org/abs/2403.03740v1", "similarity": 0.5754}, {"title": "Thinking Outside the BBox: Unconstrained Generative Object Compositing", "link": "http://arxiv.org/abs/2409.04559v2", "similarity": 0.5524}, {"title": "UGG: Unified Generative Grasping", "link": "https://arxiv.org/abs/2311.16917", "similarity": 0.5483}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Organized%20Grouped%20Discrete%20Representation%20for%20Object-Centric%20Learning&body=Title%3A%20Organized%20Grouped%20Discrete%20Representation%20for%20Object-Centric%20Learning%0AAuthor%3A%20Rongzhen%20Zhao%20and%20Vivienne%20Wang%20and%20Juho%20Kannala%20and%20Joni%20Pajarinen%0AAbstract%3A%20%20%20Object-Centric%20Learning%20%28OCL%29%20represents%20dense%20image%20or%20video%20pixels%20as%0Asparse%20object%20features.%20Representative%20methods%20utilize%20discrete%20representation%0Acomposed%20of%20Variational%20Autoencoder%20%28VAE%29%20template%20features%20to%20suppress%0Apixel-level%20information%20redundancy%20and%20guide%20object-level%20feature%20aggregation.%0AThe%20most%20recent%20advancement%2C%20Grouped%20Discrete%20Representation%20%28GDR%29%2C%20further%0Adecomposes%20these%20template%20features%20into%20attributes.%20However%2C%20its%20naive%20channel%0Agrouping%20as%20decomposition%20may%20erroneously%20group%20channels%20belonging%20to%20different%0Aattributes%20together%20and%20discretize%20them%20as%20sub-optimal%20template%20attributes%2C%0Awhich%20losses%20information%20and%20harms%20expressivity.%20We%20propose%20Organized%20GDR%0A%28OGDR%29%20to%20organize%20channels%20belonging%20to%20the%20same%20attributes%20together%20for%0Acorrect%20decomposition%20from%20features%20into%20attributes.%20In%20unsupervised%0Asegmentation%20experiments%2C%20OGDR%20is%20fully%20superior%20to%20GDR%20in%20augmentating%0Aclassical%20transformer-based%20OCL%20methods%3B%20it%20even%20improves%20state-of-the-art%0Adiffusion-based%20ones.%20Codebook%20PCA%20and%20representation%20similarity%20analyses%20show%0Athat%20compared%20with%20GDR%2C%20our%20OGDR%20eliminates%20redundancy%20and%20preserves%0Ainformation%20better%20for%20guiding%20object%20representation%20learning.%20The%20source%20code%0Ais%20available%20in%20the%20supplementary%20material.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2409.03553v3%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DOrganized%2520Grouped%2520Discrete%2520Representation%2520for%2520Object-Centric%2520Learning%26entry.906535625%3DRongzhen%2520Zhao%2520and%2520Vivienne%2520Wang%2520and%2520Juho%2520Kannala%2520and%2520Joni%2520Pajarinen%26entry.1292438233%3D%2520%2520Object-Centric%2520Learning%2520%2528OCL%2529%2520represents%2520dense%2520image%2520or%2520video%2520pixels%2520as%250Asparse%2520object%2520features.%2520Representative%2520methods%2520utilize%2520discrete%2520representation%250Acomposed%2520of%2520Variational%2520Autoencoder%2520%2528VAE%2529%2520template%2520features%2520to%2520suppress%250Apixel-level%2520information%2520redundancy%2520and%2520guide%2520object-level%2520feature%2520aggregation.%250AThe%2520most%2520recent%2520advancement%252C%2520Grouped%2520Discrete%2520Representation%2520%2528GDR%2529%252C%2520further%250Adecomposes%2520these%2520template%2520features%2520into%2520attributes.%2520However%252C%2520its%2520naive%2520channel%250Agrouping%2520as%2520decomposition%2520may%2520erroneously%2520group%2520channels%2520belonging%2520to%2520different%250Aattributes%2520together%2520and%2520discretize%2520them%2520as%2520sub-optimal%2520template%2520attributes%252C%250Awhich%2520losses%2520information%2520and%2520harms%2520expressivity.%2520We%2520propose%2520Organized%2520GDR%250A%2528OGDR%2529%2520to%2520organize%2520channels%2520belonging%2520to%2520the%2520same%2520attributes%2520together%2520for%250Acorrect%2520decomposition%2520from%2520features%2520into%2520attributes.%2520In%2520unsupervised%250Asegmentation%2520experiments%252C%2520OGDR%2520is%2520fully%2520superior%2520to%2520GDR%2520in%2520augmentating%250Aclassical%2520transformer-based%2520OCL%2520methods%253B%2520it%2520even%2520improves%2520state-of-the-art%250Adiffusion-based%2520ones.%2520Codebook%2520PCA%2520and%2520representation%2520similarity%2520analyses%2520show%250Athat%2520compared%2520with%2520GDR%252C%2520our%2520OGDR%2520eliminates%2520redundancy%2520and%2520preserves%250Ainformation%2520better%2520for%2520guiding%2520object%2520representation%2520learning.%2520The%2520source%2520code%250Ais%2520available%2520in%2520the%2520supplementary%2520material.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2409.03553v3%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Organized%20Grouped%20Discrete%20Representation%20for%20Object-Centric%20Learning&entry.906535625=Rongzhen%20Zhao%20and%20Vivienne%20Wang%20and%20Juho%20Kannala%20and%20Joni%20Pajarinen&entry.1292438233=%20%20Object-Centric%20Learning%20%28OCL%29%20represents%20dense%20image%20or%20video%20pixels%20as%0Asparse%20object%20features.%20Representative%20methods%20utilize%20discrete%20representation%0Acomposed%20of%20Variational%20Autoencoder%20%28VAE%29%20template%20features%20to%20suppress%0Apixel-level%20information%20redundancy%20and%20guide%20object-level%20feature%20aggregation.%0AThe%20most%20recent%20advancement%2C%20Grouped%20Discrete%20Representation%20%28GDR%29%2C%20further%0Adecomposes%20these%20template%20features%20into%20attributes.%20However%2C%20its%20naive%20channel%0Agrouping%20as%20decomposition%20may%20erroneously%20group%20channels%20belonging%20to%20different%0Aattributes%20together%20and%20discretize%20them%20as%20sub-optimal%20template%20attributes%2C%0Awhich%20losses%20information%20and%20harms%20expressivity.%20We%20propose%20Organized%20GDR%0A%28OGDR%29%20to%20organize%20channels%20belonging%20to%20the%20same%20attributes%20together%20for%0Acorrect%20decomposition%20from%20features%20into%20attributes.%20In%20unsupervised%0Asegmentation%20experiments%2C%20OGDR%20is%20fully%20superior%20to%20GDR%20in%20augmentating%0Aclassical%20transformer-based%20OCL%20methods%3B%20it%20even%20improves%20state-of-the-art%0Adiffusion-based%20ones.%20Codebook%20PCA%20and%20representation%20similarity%20analyses%20show%0Athat%20compared%20with%20GDR%2C%20our%20OGDR%20eliminates%20redundancy%20and%20preserves%0Ainformation%20better%20for%20guiding%20object%20representation%20learning.%20The%20source%20code%0Ais%20available%20in%20the%20supplementary%20material.%0A&entry.1838667208=http%3A//arxiv.org/abs/2409.03553v3&entry.124074799=Read"},
{"title": "Scalable and Consistent Graph Neural Networks for Distributed Mesh-based\n  Data-driven Modeling", "author": "Shivam Barwey and Riccardo Balin and Bethany Lusch and Saumil Patel and Ramesh Balakrishnan and Pinaki Pal and Romit Maulik and Venkatram Vishwanath", "abstract": "  This work develops a distributed graph neural network (GNN) methodology for\nmesh-based modeling applications using a consistent neural message passing\nlayer. As the name implies, the focus is on enabling scalable operations that\nsatisfy physical consistency via halo nodes at sub-graph boundaries. Here,\nconsistency refers to the fact that a GNN trained and evaluated on one rank\n(one large graph) is arithmetically equivalent to evaluations on multiple ranks\n(a partitioned graph). This concept is demonstrated by interfacing GNNs with\nNekRS, a GPU-capable exascale CFD solver developed at Argonne National\nLaboratory. It is shown how the NekRS mesh partitioning can be linked to the\ndistributed GNN training and inference routines, resulting in a scalable\nmesh-based data-driven modeling workflow. We study the impact of consistency on\nthe scalability of mesh-based GNNs, demonstrating efficient scaling in\nconsistent GNNs for up to O(1B) graph nodes on the Frontier exascale\nsupercomputer.\n", "link": "http://arxiv.org/abs/2410.01657v1", "date": "2024-10-02", "relevancy": 2.232, "topK": [{"title": "Dynamic Gaussians Mesh: Consistent Mesh Reconstruction from Monocular\n  Videos", "link": "http://arxiv.org/abs/2404.12379v2", "similarity": 0.6271}, {"title": "RTG-SLAM: Real-time 3D Reconstruction at Scale using Gaussian Splatting", "link": "http://arxiv.org/abs/2404.19706v1", "similarity": 0.5174}, {"title": "Scaling Rectified Flow Transformers for High-Resolution Image Synthesis", "link": "http://arxiv.org/abs/2403.03206v1", "similarity": 0.487}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Scalable%20and%20Consistent%20Graph%20Neural%20Networks%20for%20Distributed%20Mesh-based%0A%20%20Data-driven%20Modeling&body=Title%3A%20Scalable%20and%20Consistent%20Graph%20Neural%20Networks%20for%20Distributed%20Mesh-based%0A%20%20Data-driven%20Modeling%0AAuthor%3A%20Shivam%20Barwey%20and%20Riccardo%20Balin%20and%20Bethany%20Lusch%20and%20Saumil%20Patel%20and%20Ramesh%20Balakrishnan%20and%20Pinaki%20Pal%20and%20Romit%20Maulik%20and%20Venkatram%20Vishwanath%0AAbstract%3A%20%20%20This%20work%20develops%20a%20distributed%20graph%20neural%20network%20%28GNN%29%20methodology%20for%0Amesh-based%20modeling%20applications%20using%20a%20consistent%20neural%20message%20passing%0Alayer.%20As%20the%20name%20implies%2C%20the%20focus%20is%20on%20enabling%20scalable%20operations%20that%0Asatisfy%20physical%20consistency%20via%20halo%20nodes%20at%20sub-graph%20boundaries.%20Here%2C%0Aconsistency%20refers%20to%20the%20fact%20that%20a%20GNN%20trained%20and%20evaluated%20on%20one%20rank%0A%28one%20large%20graph%29%20is%20arithmetically%20equivalent%20to%20evaluations%20on%20multiple%20ranks%0A%28a%20partitioned%20graph%29.%20This%20concept%20is%20demonstrated%20by%20interfacing%20GNNs%20with%0ANekRS%2C%20a%20GPU-capable%20exascale%20CFD%20solver%20developed%20at%20Argonne%20National%0ALaboratory.%20It%20is%20shown%20how%20the%20NekRS%20mesh%20partitioning%20can%20be%20linked%20to%20the%0Adistributed%20GNN%20training%20and%20inference%20routines%2C%20resulting%20in%20a%20scalable%0Amesh-based%20data-driven%20modeling%20workflow.%20We%20study%20the%20impact%20of%20consistency%20on%0Athe%20scalability%20of%20mesh-based%20GNNs%2C%20demonstrating%20efficient%20scaling%20in%0Aconsistent%20GNNs%20for%20up%20to%20O%281B%29%20graph%20nodes%20on%20the%20Frontier%20exascale%0Asupercomputer.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2410.01657v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DScalable%2520and%2520Consistent%2520Graph%2520Neural%2520Networks%2520for%2520Distributed%2520Mesh-based%250A%2520%2520Data-driven%2520Modeling%26entry.906535625%3DShivam%2520Barwey%2520and%2520Riccardo%2520Balin%2520and%2520Bethany%2520Lusch%2520and%2520Saumil%2520Patel%2520and%2520Ramesh%2520Balakrishnan%2520and%2520Pinaki%2520Pal%2520and%2520Romit%2520Maulik%2520and%2520Venkatram%2520Vishwanath%26entry.1292438233%3D%2520%2520This%2520work%2520develops%2520a%2520distributed%2520graph%2520neural%2520network%2520%2528GNN%2529%2520methodology%2520for%250Amesh-based%2520modeling%2520applications%2520using%2520a%2520consistent%2520neural%2520message%2520passing%250Alayer.%2520As%2520the%2520name%2520implies%252C%2520the%2520focus%2520is%2520on%2520enabling%2520scalable%2520operations%2520that%250Asatisfy%2520physical%2520consistency%2520via%2520halo%2520nodes%2520at%2520sub-graph%2520boundaries.%2520Here%252C%250Aconsistency%2520refers%2520to%2520the%2520fact%2520that%2520a%2520GNN%2520trained%2520and%2520evaluated%2520on%2520one%2520rank%250A%2528one%2520large%2520graph%2529%2520is%2520arithmetically%2520equivalent%2520to%2520evaluations%2520on%2520multiple%2520ranks%250A%2528a%2520partitioned%2520graph%2529.%2520This%2520concept%2520is%2520demonstrated%2520by%2520interfacing%2520GNNs%2520with%250ANekRS%252C%2520a%2520GPU-capable%2520exascale%2520CFD%2520solver%2520developed%2520at%2520Argonne%2520National%250ALaboratory.%2520It%2520is%2520shown%2520how%2520the%2520NekRS%2520mesh%2520partitioning%2520can%2520be%2520linked%2520to%2520the%250Adistributed%2520GNN%2520training%2520and%2520inference%2520routines%252C%2520resulting%2520in%2520a%2520scalable%250Amesh-based%2520data-driven%2520modeling%2520workflow.%2520We%2520study%2520the%2520impact%2520of%2520consistency%2520on%250Athe%2520scalability%2520of%2520mesh-based%2520GNNs%252C%2520demonstrating%2520efficient%2520scaling%2520in%250Aconsistent%2520GNNs%2520for%2520up%2520to%2520O%25281B%2529%2520graph%2520nodes%2520on%2520the%2520Frontier%2520exascale%250Asupercomputer.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2410.01657v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Scalable%20and%20Consistent%20Graph%20Neural%20Networks%20for%20Distributed%20Mesh-based%0A%20%20Data-driven%20Modeling&entry.906535625=Shivam%20Barwey%20and%20Riccardo%20Balin%20and%20Bethany%20Lusch%20and%20Saumil%20Patel%20and%20Ramesh%20Balakrishnan%20and%20Pinaki%20Pal%20and%20Romit%20Maulik%20and%20Venkatram%20Vishwanath&entry.1292438233=%20%20This%20work%20develops%20a%20distributed%20graph%20neural%20network%20%28GNN%29%20methodology%20for%0Amesh-based%20modeling%20applications%20using%20a%20consistent%20neural%20message%20passing%0Alayer.%20As%20the%20name%20implies%2C%20the%20focus%20is%20on%20enabling%20scalable%20operations%20that%0Asatisfy%20physical%20consistency%20via%20halo%20nodes%20at%20sub-graph%20boundaries.%20Here%2C%0Aconsistency%20refers%20to%20the%20fact%20that%20a%20GNN%20trained%20and%20evaluated%20on%20one%20rank%0A%28one%20large%20graph%29%20is%20arithmetically%20equivalent%20to%20evaluations%20on%20multiple%20ranks%0A%28a%20partitioned%20graph%29.%20This%20concept%20is%20demonstrated%20by%20interfacing%20GNNs%20with%0ANekRS%2C%20a%20GPU-capable%20exascale%20CFD%20solver%20developed%20at%20Argonne%20National%0ALaboratory.%20It%20is%20shown%20how%20the%20NekRS%20mesh%20partitioning%20can%20be%20linked%20to%20the%0Adistributed%20GNN%20training%20and%20inference%20routines%2C%20resulting%20in%20a%20scalable%0Amesh-based%20data-driven%20modeling%20workflow.%20We%20study%20the%20impact%20of%20consistency%20on%0Athe%20scalability%20of%20mesh-based%20GNNs%2C%20demonstrating%20efficient%20scaling%20in%0Aconsistent%20GNNs%20for%20up%20to%20O%281B%29%20graph%20nodes%20on%20the%20Frontier%20exascale%0Asupercomputer.%0A&entry.1838667208=http%3A//arxiv.org/abs/2410.01657v1&entry.124074799=Read"},
{"title": "A method for supervoxel-wise association studies of age and other\n  non-imaging variables from coronary computed tomography angiograms", "author": "Johan \u00d6fverstedt and Elin Lundstr\u00f6m and G\u00f6ran Bergstr\u00f6m and Joel Kullberg and H\u00e5kan Ahlstr\u00f6m", "abstract": "  The study of associations between an individual's age and imaging and\nnon-imaging data is an active research area that attempts to aid understanding\nof the effects and patterns of aging. In this work we have conducted a\nsupervoxel-wise association study between both volumetric and tissue density\nfeatures in coronary computed tomography angiograms and the chronological age\nof a subject, to understand the localized changes in morphology and tissue\ndensity with age. To enable a supervoxel-wise study of volume and tissue\ndensity, we developed a novel method based on image segmentation, inter-subject\nimage registration, and robust supervoxel-based correlation analysis, to\nachieve a statistical association study between the images and age. We evaluate\nthe registration methodology in terms of the Dice coefficient for the heart\nchambers and myocardium, and the inverse consistency of the transformations,\nshowing that the method works well in most cases with high overlap and inverse\nconsistency. In a sex-stratified study conducted on a subset of $n=1388$ images\nfrom the SCAPIS study, the supervoxel-wise analysis was able to find localized\nassociations with age outside of the commonly segmented and analyzed\nsub-regions, and several substantial differences between the sexes in the\nassociation of age and volume.\n", "link": "http://arxiv.org/abs/2405.07762v2", "date": "2024-10-02", "relevancy": 2.2145, "topK": [{"title": "CAT3D: Create Anything in 3D with Multi-View Diffusion Models", "link": "http://arxiv.org/abs/2405.10314v1", "similarity": 0.4437}, {"title": "CAT3D: Create Anything in 3D with Multi-View Diffusion Models", "link": "http://arxiv.org/abs/2405.10314v1", "similarity": 0.4437}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.4413}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20A%20method%20for%20supervoxel-wise%20association%20studies%20of%20age%20and%20other%0A%20%20non-imaging%20variables%20from%20coronary%20computed%20tomography%20angiograms&body=Title%3A%20A%20method%20for%20supervoxel-wise%20association%20studies%20of%20age%20and%20other%0A%20%20non-imaging%20variables%20from%20coronary%20computed%20tomography%20angiograms%0AAuthor%3A%20Johan%20%C3%96fverstedt%20and%20Elin%20Lundstr%C3%B6m%20and%20G%C3%B6ran%20Bergstr%C3%B6m%20and%20Joel%20Kullberg%20and%20H%C3%A5kan%20Ahlstr%C3%B6m%0AAbstract%3A%20%20%20The%20study%20of%20associations%20between%20an%20individual%27s%20age%20and%20imaging%20and%0Anon-imaging%20data%20is%20an%20active%20research%20area%20that%20attempts%20to%20aid%20understanding%0Aof%20the%20effects%20and%20patterns%20of%20aging.%20In%20this%20work%20we%20have%20conducted%20a%0Asupervoxel-wise%20association%20study%20between%20both%20volumetric%20and%20tissue%20density%0Afeatures%20in%20coronary%20computed%20tomography%20angiograms%20and%20the%20chronological%20age%0Aof%20a%20subject%2C%20to%20understand%20the%20localized%20changes%20in%20morphology%20and%20tissue%0Adensity%20with%20age.%20To%20enable%20a%20supervoxel-wise%20study%20of%20volume%20and%20tissue%0Adensity%2C%20we%20developed%20a%20novel%20method%20based%20on%20image%20segmentation%2C%20inter-subject%0Aimage%20registration%2C%20and%20robust%20supervoxel-based%20correlation%20analysis%2C%20to%0Aachieve%20a%20statistical%20association%20study%20between%20the%20images%20and%20age.%20We%20evaluate%0Athe%20registration%20methodology%20in%20terms%20of%20the%20Dice%20coefficient%20for%20the%20heart%0Achambers%20and%20myocardium%2C%20and%20the%20inverse%20consistency%20of%20the%20transformations%2C%0Ashowing%20that%20the%20method%20works%20well%20in%20most%20cases%20with%20high%20overlap%20and%20inverse%0Aconsistency.%20In%20a%20sex-stratified%20study%20conducted%20on%20a%20subset%20of%20%24n%3D1388%24%20images%0Afrom%20the%20SCAPIS%20study%2C%20the%20supervoxel-wise%20analysis%20was%20able%20to%20find%20localized%0Aassociations%20with%20age%20outside%20of%20the%20commonly%20segmented%20and%20analyzed%0Asub-regions%2C%20and%20several%20substantial%20differences%20between%20the%20sexes%20in%20the%0Aassociation%20of%20age%20and%20volume.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2405.07762v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DA%2520method%2520for%2520supervoxel-wise%2520association%2520studies%2520of%2520age%2520and%2520other%250A%2520%2520non-imaging%2520variables%2520from%2520coronary%2520computed%2520tomography%2520angiograms%26entry.906535625%3DJohan%2520%25C3%2596fverstedt%2520and%2520Elin%2520Lundstr%25C3%25B6m%2520and%2520G%25C3%25B6ran%2520Bergstr%25C3%25B6m%2520and%2520Joel%2520Kullberg%2520and%2520H%25C3%25A5kan%2520Ahlstr%25C3%25B6m%26entry.1292438233%3D%2520%2520The%2520study%2520of%2520associations%2520between%2520an%2520individual%2527s%2520age%2520and%2520imaging%2520and%250Anon-imaging%2520data%2520is%2520an%2520active%2520research%2520area%2520that%2520attempts%2520to%2520aid%2520understanding%250Aof%2520the%2520effects%2520and%2520patterns%2520of%2520aging.%2520In%2520this%2520work%2520we%2520have%2520conducted%2520a%250Asupervoxel-wise%2520association%2520study%2520between%2520both%2520volumetric%2520and%2520tissue%2520density%250Afeatures%2520in%2520coronary%2520computed%2520tomography%2520angiograms%2520and%2520the%2520chronological%2520age%250Aof%2520a%2520subject%252C%2520to%2520understand%2520the%2520localized%2520changes%2520in%2520morphology%2520and%2520tissue%250Adensity%2520with%2520age.%2520To%2520enable%2520a%2520supervoxel-wise%2520study%2520of%2520volume%2520and%2520tissue%250Adensity%252C%2520we%2520developed%2520a%2520novel%2520method%2520based%2520on%2520image%2520segmentation%252C%2520inter-subject%250Aimage%2520registration%252C%2520and%2520robust%2520supervoxel-based%2520correlation%2520analysis%252C%2520to%250Aachieve%2520a%2520statistical%2520association%2520study%2520between%2520the%2520images%2520and%2520age.%2520We%2520evaluate%250Athe%2520registration%2520methodology%2520in%2520terms%2520of%2520the%2520Dice%2520coefficient%2520for%2520the%2520heart%250Achambers%2520and%2520myocardium%252C%2520and%2520the%2520inverse%2520consistency%2520of%2520the%2520transformations%252C%250Ashowing%2520that%2520the%2520method%2520works%2520well%2520in%2520most%2520cases%2520with%2520high%2520overlap%2520and%2520inverse%250Aconsistency.%2520In%2520a%2520sex-stratified%2520study%2520conducted%2520on%2520a%2520subset%2520of%2520%2524n%253D1388%2524%2520images%250Afrom%2520the%2520SCAPIS%2520study%252C%2520the%2520supervoxel-wise%2520analysis%2520was%2520able%2520to%2520find%2520localized%250Aassociations%2520with%2520age%2520outside%2520of%2520the%2520commonly%2520segmented%2520and%2520analyzed%250Asub-regions%252C%2520and%2520several%2520substantial%2520differences%2520between%2520the%2520sexes%2520in%2520the%250Aassociation%2520of%2520age%2520and%2520volume.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2405.07762v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=A%20method%20for%20supervoxel-wise%20association%20studies%20of%20age%20and%20other%0A%20%20non-imaging%20variables%20from%20coronary%20computed%20tomography%20angiograms&entry.906535625=Johan%20%C3%96fverstedt%20and%20Elin%20Lundstr%C3%B6m%20and%20G%C3%B6ran%20Bergstr%C3%B6m%20and%20Joel%20Kullberg%20and%20H%C3%A5kan%20Ahlstr%C3%B6m&entry.1292438233=%20%20The%20study%20of%20associations%20between%20an%20individual%27s%20age%20and%20imaging%20and%0Anon-imaging%20data%20is%20an%20active%20research%20area%20that%20attempts%20to%20aid%20understanding%0Aof%20the%20effects%20and%20patterns%20of%20aging.%20In%20this%20work%20we%20have%20conducted%20a%0Asupervoxel-wise%20association%20study%20between%20both%20volumetric%20and%20tissue%20density%0Afeatures%20in%20coronary%20computed%20tomography%20angiograms%20and%20the%20chronological%20age%0Aof%20a%20subject%2C%20to%20understand%20the%20localized%20changes%20in%20morphology%20and%20tissue%0Adensity%20with%20age.%20To%20enable%20a%20supervoxel-wise%20study%20of%20volume%20and%20tissue%0Adensity%2C%20we%20developed%20a%20novel%20method%20based%20on%20image%20segmentation%2C%20inter-subject%0Aimage%20registration%2C%20and%20robust%20supervoxel-based%20correlation%20analysis%2C%20to%0Aachieve%20a%20statistical%20association%20study%20between%20the%20images%20and%20age.%20We%20evaluate%0Athe%20registration%20methodology%20in%20terms%20of%20the%20Dice%20coefficient%20for%20the%20heart%0Achambers%20and%20myocardium%2C%20and%20the%20inverse%20consistency%20of%20the%20transformations%2C%0Ashowing%20that%20the%20method%20works%20well%20in%20most%20cases%20with%20high%20overlap%20and%20inverse%0Aconsistency.%20In%20a%20sex-stratified%20study%20conducted%20on%20a%20subset%20of%20%24n%3D1388%24%20images%0Afrom%20the%20SCAPIS%20study%2C%20the%20supervoxel-wise%20analysis%20was%20able%20to%20find%20localized%0Aassociations%20with%20age%20outside%20of%20the%20commonly%20segmented%20and%20analyzed%0Asub-regions%2C%20and%20several%20substantial%20differences%20between%20the%20sexes%20in%20the%0Aassociation%20of%20age%20and%20volume.%0A&entry.1838667208=http%3A//arxiv.org/abs/2405.07762v2&entry.124074799=Read"},
{"title": "MOREL: Enhancing Adversarial Robustness through Multi-Objective\n  Representation Learning", "author": "Sedjro Salomon Hotegni and Sebastian Peitz", "abstract": "  Extensive research has shown that deep neural networks (DNNs) are vulnerable\nto slight adversarial perturbations$-$small changes to the input data that\nappear insignificant but cause the model to produce drastically different\noutputs. In addition to augmenting training data with adversarial examples\ngenerated from a specific attack method, most of the current defense strategies\nnecessitate modifying the original model architecture components to improve\nrobustness or performing test-time data purification to handle adversarial\nattacks. In this work, we demonstrate that strong feature representation\nlearning during training can significantly enhance the original model's\nrobustness. We propose MOREL, a multi-objective feature representation learning\napproach, encouraging classification models to produce similar features for\ninputs within the same class, despite perturbations. Our training method\ninvolves an embedding space where cosine similarity loss and multi-positive\ncontrastive loss are used to align natural and adversarial features from the\nmodel encoder and ensure tight clustering. Concurrently, the classifier is\nmotivated to achieve accurate predictions. Through extensive experiments, we\ndemonstrate that our approach significantly enhances the robustness of DNNs\nagainst white-box and black-box adversarial attacks, outperforming other\nmethods that similarly require no architectural changes or test-time data\npurification. Our code is available at https://github.com/salomonhotegni/MOREL\n", "link": "http://arxiv.org/abs/2410.01697v1", "date": "2024-10-02", "relevancy": 2.1937, "topK": [{"title": "Breadcrumbs: Adversarial class-balanced sampling for long-tailed recognition", "link": "https://link.springer.com/chapter/10.1007/978-3-031-20053-3_37", "similarity": 0.5652}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.544}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.5177}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20MOREL%3A%20Enhancing%20Adversarial%20Robustness%20through%20Multi-Objective%0A%20%20Representation%20Learning&body=Title%3A%20MOREL%3A%20Enhancing%20Adversarial%20Robustness%20through%20Multi-Objective%0A%20%20Representation%20Learning%0AAuthor%3A%20Sedjro%20Salomon%20Hotegni%20and%20Sebastian%20Peitz%0AAbstract%3A%20%20%20Extensive%20research%20has%20shown%20that%20deep%20neural%20networks%20%28DNNs%29%20are%20vulnerable%0Ato%20slight%20adversarial%20perturbations%24-%24small%20changes%20to%20the%20input%20data%20that%0Aappear%20insignificant%20but%20cause%20the%20model%20to%20produce%20drastically%20different%0Aoutputs.%20In%20addition%20to%20augmenting%20training%20data%20with%20adversarial%20examples%0Agenerated%20from%20a%20specific%20attack%20method%2C%20most%20of%20the%20current%20defense%20strategies%0Anecessitate%20modifying%20the%20original%20model%20architecture%20components%20to%20improve%0Arobustness%20or%20performing%20test-time%20data%20purification%20to%20handle%20adversarial%0Aattacks.%20In%20this%20work%2C%20we%20demonstrate%20that%20strong%20feature%20representation%0Alearning%20during%20training%20can%20significantly%20enhance%20the%20original%20model%27s%0Arobustness.%20We%20propose%20MOREL%2C%20a%20multi-objective%20feature%20representation%20learning%0Aapproach%2C%20encouraging%20classification%20models%20to%20produce%20similar%20features%20for%0Ainputs%20within%20the%20same%20class%2C%20despite%20perturbations.%20Our%20training%20method%0Ainvolves%20an%20embedding%20space%20where%20cosine%20similarity%20loss%20and%20multi-positive%0Acontrastive%20loss%20are%20used%20to%20align%20natural%20and%20adversarial%20features%20from%20the%0Amodel%20encoder%20and%20ensure%20tight%20clustering.%20Concurrently%2C%20the%20classifier%20is%0Amotivated%20to%20achieve%20accurate%20predictions.%20Through%20extensive%20experiments%2C%20we%0Ademonstrate%20that%20our%20approach%20significantly%20enhances%20the%20robustness%20of%20DNNs%0Aagainst%20white-box%20and%20black-box%20adversarial%20attacks%2C%20outperforming%20other%0Amethods%20that%20similarly%20require%20no%20architectural%20changes%20or%20test-time%20data%0Apurification.%20Our%20code%20is%20available%20at%20https%3A//github.com/salomonhotegni/MOREL%0A%0ALink%3A%20http%3A//arxiv.org/abs/2410.01697v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DMOREL%253A%2520Enhancing%2520Adversarial%2520Robustness%2520through%2520Multi-Objective%250A%2520%2520Representation%2520Learning%26entry.906535625%3DSedjro%2520Salomon%2520Hotegni%2520and%2520Sebastian%2520Peitz%26entry.1292438233%3D%2520%2520Extensive%2520research%2520has%2520shown%2520that%2520deep%2520neural%2520networks%2520%2528DNNs%2529%2520are%2520vulnerable%250Ato%2520slight%2520adversarial%2520perturbations%2524-%2524small%2520changes%2520to%2520the%2520input%2520data%2520that%250Aappear%2520insignificant%2520but%2520cause%2520the%2520model%2520to%2520produce%2520drastically%2520different%250Aoutputs.%2520In%2520addition%2520to%2520augmenting%2520training%2520data%2520with%2520adversarial%2520examples%250Agenerated%2520from%2520a%2520specific%2520attack%2520method%252C%2520most%2520of%2520the%2520current%2520defense%2520strategies%250Anecessitate%2520modifying%2520the%2520original%2520model%2520architecture%2520components%2520to%2520improve%250Arobustness%2520or%2520performing%2520test-time%2520data%2520purification%2520to%2520handle%2520adversarial%250Aattacks.%2520In%2520this%2520work%252C%2520we%2520demonstrate%2520that%2520strong%2520feature%2520representation%250Alearning%2520during%2520training%2520can%2520significantly%2520enhance%2520the%2520original%2520model%2527s%250Arobustness.%2520We%2520propose%2520MOREL%252C%2520a%2520multi-objective%2520feature%2520representation%2520learning%250Aapproach%252C%2520encouraging%2520classification%2520models%2520to%2520produce%2520similar%2520features%2520for%250Ainputs%2520within%2520the%2520same%2520class%252C%2520despite%2520perturbations.%2520Our%2520training%2520method%250Ainvolves%2520an%2520embedding%2520space%2520where%2520cosine%2520similarity%2520loss%2520and%2520multi-positive%250Acontrastive%2520loss%2520are%2520used%2520to%2520align%2520natural%2520and%2520adversarial%2520features%2520from%2520the%250Amodel%2520encoder%2520and%2520ensure%2520tight%2520clustering.%2520Concurrently%252C%2520the%2520classifier%2520is%250Amotivated%2520to%2520achieve%2520accurate%2520predictions.%2520Through%2520extensive%2520experiments%252C%2520we%250Ademonstrate%2520that%2520our%2520approach%2520significantly%2520enhances%2520the%2520robustness%2520of%2520DNNs%250Aagainst%2520white-box%2520and%2520black-box%2520adversarial%2520attacks%252C%2520outperforming%2520other%250Amethods%2520that%2520similarly%2520require%2520no%2520architectural%2520changes%2520or%2520test-time%2520data%250Apurification.%2520Our%2520code%2520is%2520available%2520at%2520https%253A//github.com/salomonhotegni/MOREL%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2410.01697v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=MOREL%3A%20Enhancing%20Adversarial%20Robustness%20through%20Multi-Objective%0A%20%20Representation%20Learning&entry.906535625=Sedjro%20Salomon%20Hotegni%20and%20Sebastian%20Peitz&entry.1292438233=%20%20Extensive%20research%20has%20shown%20that%20deep%20neural%20networks%20%28DNNs%29%20are%20vulnerable%0Ato%20slight%20adversarial%20perturbations%24-%24small%20changes%20to%20the%20input%20data%20that%0Aappear%20insignificant%20but%20cause%20the%20model%20to%20produce%20drastically%20different%0Aoutputs.%20In%20addition%20to%20augmenting%20training%20data%20with%20adversarial%20examples%0Agenerated%20from%20a%20specific%20attack%20method%2C%20most%20of%20the%20current%20defense%20strategies%0Anecessitate%20modifying%20the%20original%20model%20architecture%20components%20to%20improve%0Arobustness%20or%20performing%20test-time%20data%20purification%20to%20handle%20adversarial%0Aattacks.%20In%20this%20work%2C%20we%20demonstrate%20that%20strong%20feature%20representation%0Alearning%20during%20training%20can%20significantly%20enhance%20the%20original%20model%27s%0Arobustness.%20We%20propose%20MOREL%2C%20a%20multi-objective%20feature%20representation%20learning%0Aapproach%2C%20encouraging%20classification%20models%20to%20produce%20similar%20features%20for%0Ainputs%20within%20the%20same%20class%2C%20despite%20perturbations.%20Our%20training%20method%0Ainvolves%20an%20embedding%20space%20where%20cosine%20similarity%20loss%20and%20multi-positive%0Acontrastive%20loss%20are%20used%20to%20align%20natural%20and%20adversarial%20features%20from%20the%0Amodel%20encoder%20and%20ensure%20tight%20clustering.%20Concurrently%2C%20the%20classifier%20is%0Amotivated%20to%20achieve%20accurate%20predictions.%20Through%20extensive%20experiments%2C%20we%0Ademonstrate%20that%20our%20approach%20significantly%20enhances%20the%20robustness%20of%20DNNs%0Aagainst%20white-box%20and%20black-box%20adversarial%20attacks%2C%20outperforming%20other%0Amethods%20that%20similarly%20require%20no%20architectural%20changes%20or%20test-time%20data%0Apurification.%20Our%20code%20is%20available%20at%20https%3A//github.com/salomonhotegni/MOREL%0A&entry.1838667208=http%3A//arxiv.org/abs/2410.01697v1&entry.124074799=Read"},
{"title": "Robo-MUTUAL: Robotic Multimodal Task Specification via Unimodal Learning", "author": "Jianxiong Li and Zhihao Wang and Jinliang Zheng and Xiaoai Zhou and Guanming Wang and Guanglu Song and Yu Liu and Jingjing Liu and Ya-Qin Zhang and Junzhi Yu and Xianyuan Zhan", "abstract": "  Multimodal task specification is essential for enhanced robotic performance,\nwhere \\textit{Cross-modality Alignment} enables the robot to holistically\nunderstand complex task instructions. Directly annotating multimodal\ninstructions for model training proves impractical, due to the sparsity of\npaired multimodal data. In this study, we demonstrate that by leveraging\nunimodal instructions abundant in real data, we can effectively teach robots to\nlearn multimodal task specifications. First, we endow the robot with strong\n\\textit{Cross-modality Alignment} capabilities, by pretraining a robotic\nmultimodal encoder using extensive out-of-domain data. Then, we employ two\nCollapse and Corrupt operations to further bridge the remaining modality gap in\nthe learned multimodal representation. This approach projects different\nmodalities of identical task goal as interchangeable representations, thus\nenabling accurate robotic operations within a well-aligned multimodal latent\nspace. Evaluation across more than 130 tasks and 4000 evaluations on both\nsimulated LIBERO benchmark and real robot platforms showcases the superior\ncapabilities of our proposed framework, demonstrating significant advantage in\novercoming data constraints in robotic learning. Website:\nzh1hao.wang/Robo_MUTUAL\n", "link": "http://arxiv.org/abs/2410.01529v1", "date": "2024-10-02", "relevancy": 1.798, "topK": [{"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.6241}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5954}, {"title": "RoboEXP: Action-Conditioned Scene Graph via Interactive Exploration for\n  Robotic Manipulation", "link": "http://arxiv.org/abs/2402.15487v1", "similarity": 0.5846}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Robo-MUTUAL%3A%20Robotic%20Multimodal%20Task%20Specification%20via%20Unimodal%20Learning&body=Title%3A%20Robo-MUTUAL%3A%20Robotic%20Multimodal%20Task%20Specification%20via%20Unimodal%20Learning%0AAuthor%3A%20Jianxiong%20Li%20and%20Zhihao%20Wang%20and%20Jinliang%20Zheng%20and%20Xiaoai%20Zhou%20and%20Guanming%20Wang%20and%20Guanglu%20Song%20and%20Yu%20Liu%20and%20Jingjing%20Liu%20and%20Ya-Qin%20Zhang%20and%20Junzhi%20Yu%20and%20Xianyuan%20Zhan%0AAbstract%3A%20%20%20Multimodal%20task%20specification%20is%20essential%20for%20enhanced%20robotic%20performance%2C%0Awhere%20%5Ctextit%7BCross-modality%20Alignment%7D%20enables%20the%20robot%20to%20holistically%0Aunderstand%20complex%20task%20instructions.%20Directly%20annotating%20multimodal%0Ainstructions%20for%20model%20training%20proves%20impractical%2C%20due%20to%20the%20sparsity%20of%0Apaired%20multimodal%20data.%20In%20this%20study%2C%20we%20demonstrate%20that%20by%20leveraging%0Aunimodal%20instructions%20abundant%20in%20real%20data%2C%20we%20can%20effectively%20teach%20robots%20to%0Alearn%20multimodal%20task%20specifications.%20First%2C%20we%20endow%20the%20robot%20with%20strong%0A%5Ctextit%7BCross-modality%20Alignment%7D%20capabilities%2C%20by%20pretraining%20a%20robotic%0Amultimodal%20encoder%20using%20extensive%20out-of-domain%20data.%20Then%2C%20we%20employ%20two%0ACollapse%20and%20Corrupt%20operations%20to%20further%20bridge%20the%20remaining%20modality%20gap%20in%0Athe%20learned%20multimodal%20representation.%20This%20approach%20projects%20different%0Amodalities%20of%20identical%20task%20goal%20as%20interchangeable%20representations%2C%20thus%0Aenabling%20accurate%20robotic%20operations%20within%20a%20well-aligned%20multimodal%20latent%0Aspace.%20Evaluation%20across%20more%20than%20130%20tasks%20and%204000%20evaluations%20on%20both%0Asimulated%20LIBERO%20benchmark%20and%20real%20robot%20platforms%20showcases%20the%20superior%0Acapabilities%20of%20our%20proposed%20framework%2C%20demonstrating%20significant%20advantage%20in%0Aovercoming%20data%20constraints%20in%20robotic%20learning.%20Website%3A%0Azh1hao.wang/Robo_MUTUAL%0A%0ALink%3A%20http%3A//arxiv.org/abs/2410.01529v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DRobo-MUTUAL%253A%2520Robotic%2520Multimodal%2520Task%2520Specification%2520via%2520Unimodal%2520Learning%26entry.906535625%3DJianxiong%2520Li%2520and%2520Zhihao%2520Wang%2520and%2520Jinliang%2520Zheng%2520and%2520Xiaoai%2520Zhou%2520and%2520Guanming%2520Wang%2520and%2520Guanglu%2520Song%2520and%2520Yu%2520Liu%2520and%2520Jingjing%2520Liu%2520and%2520Ya-Qin%2520Zhang%2520and%2520Junzhi%2520Yu%2520and%2520Xianyuan%2520Zhan%26entry.1292438233%3D%2520%2520Multimodal%2520task%2520specification%2520is%2520essential%2520for%2520enhanced%2520robotic%2520performance%252C%250Awhere%2520%255Ctextit%257BCross-modality%2520Alignment%257D%2520enables%2520the%2520robot%2520to%2520holistically%250Aunderstand%2520complex%2520task%2520instructions.%2520Directly%2520annotating%2520multimodal%250Ainstructions%2520for%2520model%2520training%2520proves%2520impractical%252C%2520due%2520to%2520the%2520sparsity%2520of%250Apaired%2520multimodal%2520data.%2520In%2520this%2520study%252C%2520we%2520demonstrate%2520that%2520by%2520leveraging%250Aunimodal%2520instructions%2520abundant%2520in%2520real%2520data%252C%2520we%2520can%2520effectively%2520teach%2520robots%2520to%250Alearn%2520multimodal%2520task%2520specifications.%2520First%252C%2520we%2520endow%2520the%2520robot%2520with%2520strong%250A%255Ctextit%257BCross-modality%2520Alignment%257D%2520capabilities%252C%2520by%2520pretraining%2520a%2520robotic%250Amultimodal%2520encoder%2520using%2520extensive%2520out-of-domain%2520data.%2520Then%252C%2520we%2520employ%2520two%250ACollapse%2520and%2520Corrupt%2520operations%2520to%2520further%2520bridge%2520the%2520remaining%2520modality%2520gap%2520in%250Athe%2520learned%2520multimodal%2520representation.%2520This%2520approach%2520projects%2520different%250Amodalities%2520of%2520identical%2520task%2520goal%2520as%2520interchangeable%2520representations%252C%2520thus%250Aenabling%2520accurate%2520robotic%2520operations%2520within%2520a%2520well-aligned%2520multimodal%2520latent%250Aspace.%2520Evaluation%2520across%2520more%2520than%2520130%2520tasks%2520and%25204000%2520evaluations%2520on%2520both%250Asimulated%2520LIBERO%2520benchmark%2520and%2520real%2520robot%2520platforms%2520showcases%2520the%2520superior%250Acapabilities%2520of%2520our%2520proposed%2520framework%252C%2520demonstrating%2520significant%2520advantage%2520in%250Aovercoming%2520data%2520constraints%2520in%2520robotic%2520learning.%2520Website%253A%250Azh1hao.wang/Robo_MUTUAL%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2410.01529v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Robo-MUTUAL%3A%20Robotic%20Multimodal%20Task%20Specification%20via%20Unimodal%20Learning&entry.906535625=Jianxiong%20Li%20and%20Zhihao%20Wang%20and%20Jinliang%20Zheng%20and%20Xiaoai%20Zhou%20and%20Guanming%20Wang%20and%20Guanglu%20Song%20and%20Yu%20Liu%20and%20Jingjing%20Liu%20and%20Ya-Qin%20Zhang%20and%20Junzhi%20Yu%20and%20Xianyuan%20Zhan&entry.1292438233=%20%20Multimodal%20task%20specification%20is%20essential%20for%20enhanced%20robotic%20performance%2C%0Awhere%20%5Ctextit%7BCross-modality%20Alignment%7D%20enables%20the%20robot%20to%20holistically%0Aunderstand%20complex%20task%20instructions.%20Directly%20annotating%20multimodal%0Ainstructions%20for%20model%20training%20proves%20impractical%2C%20due%20to%20the%20sparsity%20of%0Apaired%20multimodal%20data.%20In%20this%20study%2C%20we%20demonstrate%20that%20by%20leveraging%0Aunimodal%20instructions%20abundant%20in%20real%20data%2C%20we%20can%20effectively%20teach%20robots%20to%0Alearn%20multimodal%20task%20specifications.%20First%2C%20we%20endow%20the%20robot%20with%20strong%0A%5Ctextit%7BCross-modality%20Alignment%7D%20capabilities%2C%20by%20pretraining%20a%20robotic%0Amultimodal%20encoder%20using%20extensive%20out-of-domain%20data.%20Then%2C%20we%20employ%20two%0ACollapse%20and%20Corrupt%20operations%20to%20further%20bridge%20the%20remaining%20modality%20gap%20in%0Athe%20learned%20multimodal%20representation.%20This%20approach%20projects%20different%0Amodalities%20of%20identical%20task%20goal%20as%20interchangeable%20representations%2C%20thus%0Aenabling%20accurate%20robotic%20operations%20within%20a%20well-aligned%20multimodal%20latent%0Aspace.%20Evaluation%20across%20more%20than%20130%20tasks%20and%204000%20evaluations%20on%20both%0Asimulated%20LIBERO%20benchmark%20and%20real%20robot%20platforms%20showcases%20the%20superior%0Acapabilities%20of%20our%20proposed%20framework%2C%20demonstrating%20significant%20advantage%20in%0Aovercoming%20data%20constraints%20in%20robotic%20learning.%20Website%3A%0Azh1hao.wang/Robo_MUTUAL%0A&entry.1838667208=http%3A//arxiv.org/abs/2410.01529v1&entry.124074799=Read"},
{"title": "Towards Model Discovery Using Domain Decomposition and PINNs", "author": "Tirtho S. Saha and Alexander Heinlein and Cordula Reisch", "abstract": "  We enhance machine learning algorithms for learning model parameters in\ncomplex systems represented by ordinary differential equations (ODEs) with\ndomain decomposition methods. The study evaluates the performance of two\napproaches, namely (vanilla) Physics-Informed Neural Networks (PINNs) and\nFinite Basis Physics-Informed Neural Networks (FBPINNs), in learning the\ndynamics of test models with a quasi-stationary longtime behavior. We test the\napproaches for data sets in different dynamical regions and with varying noise\nlevel. As results, we find a better performance for the FBPINN approach\ncompared to the vanilla PINN approach, even in cases with data from only a\nquasi-stationary time domain with few dynamics.\n", "link": "http://arxiv.org/abs/2410.01599v1", "date": "2024-10-02", "relevancy": 1.9491, "topK": [{"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.5056}, {"title": "Boosted dynamic neural networks", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/26302", "similarity": 0.4872}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.48}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Towards%20Model%20Discovery%20Using%20Domain%20Decomposition%20and%20PINNs&body=Title%3A%20Towards%20Model%20Discovery%20Using%20Domain%20Decomposition%20and%20PINNs%0AAuthor%3A%20Tirtho%20S.%20Saha%20and%20Alexander%20Heinlein%20and%20Cordula%20Reisch%0AAbstract%3A%20%20%20We%20enhance%20machine%20learning%20algorithms%20for%20learning%20model%20parameters%20in%0Acomplex%20systems%20represented%20by%20ordinary%20differential%20equations%20%28ODEs%29%20with%0Adomain%20decomposition%20methods.%20The%20study%20evaluates%20the%20performance%20of%20two%0Aapproaches%2C%20namely%20%28vanilla%29%20Physics-Informed%20Neural%20Networks%20%28PINNs%29%20and%0AFinite%20Basis%20Physics-Informed%20Neural%20Networks%20%28FBPINNs%29%2C%20in%20learning%20the%0Adynamics%20of%20test%20models%20with%20a%20quasi-stationary%20longtime%20behavior.%20We%20test%20the%0Aapproaches%20for%20data%20sets%20in%20different%20dynamical%20regions%20and%20with%20varying%20noise%0Alevel.%20As%20results%2C%20we%20find%20a%20better%20performance%20for%20the%20FBPINN%20approach%0Acompared%20to%20the%20vanilla%20PINN%20approach%2C%20even%20in%20cases%20with%20data%20from%20only%20a%0Aquasi-stationary%20time%20domain%20with%20few%20dynamics.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2410.01599v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DTowards%2520Model%2520Discovery%2520Using%2520Domain%2520Decomposition%2520and%2520PINNs%26entry.906535625%3DTirtho%2520S.%2520Saha%2520and%2520Alexander%2520Heinlein%2520and%2520Cordula%2520Reisch%26entry.1292438233%3D%2520%2520We%2520enhance%2520machine%2520learning%2520algorithms%2520for%2520learning%2520model%2520parameters%2520in%250Acomplex%2520systems%2520represented%2520by%2520ordinary%2520differential%2520equations%2520%2528ODEs%2529%2520with%250Adomain%2520decomposition%2520methods.%2520The%2520study%2520evaluates%2520the%2520performance%2520of%2520two%250Aapproaches%252C%2520namely%2520%2528vanilla%2529%2520Physics-Informed%2520Neural%2520Networks%2520%2528PINNs%2529%2520and%250AFinite%2520Basis%2520Physics-Informed%2520Neural%2520Networks%2520%2528FBPINNs%2529%252C%2520in%2520learning%2520the%250Adynamics%2520of%2520test%2520models%2520with%2520a%2520quasi-stationary%2520longtime%2520behavior.%2520We%2520test%2520the%250Aapproaches%2520for%2520data%2520sets%2520in%2520different%2520dynamical%2520regions%2520and%2520with%2520varying%2520noise%250Alevel.%2520As%2520results%252C%2520we%2520find%2520a%2520better%2520performance%2520for%2520the%2520FBPINN%2520approach%250Acompared%2520to%2520the%2520vanilla%2520PINN%2520approach%252C%2520even%2520in%2520cases%2520with%2520data%2520from%2520only%2520a%250Aquasi-stationary%2520time%2520domain%2520with%2520few%2520dynamics.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2410.01599v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Towards%20Model%20Discovery%20Using%20Domain%20Decomposition%20and%20PINNs&entry.906535625=Tirtho%20S.%20Saha%20and%20Alexander%20Heinlein%20and%20Cordula%20Reisch&entry.1292438233=%20%20We%20enhance%20machine%20learning%20algorithms%20for%20learning%20model%20parameters%20in%0Acomplex%20systems%20represented%20by%20ordinary%20differential%20equations%20%28ODEs%29%20with%0Adomain%20decomposition%20methods.%20The%20study%20evaluates%20the%20performance%20of%20two%0Aapproaches%2C%20namely%20%28vanilla%29%20Physics-Informed%20Neural%20Networks%20%28PINNs%29%20and%0AFinite%20Basis%20Physics-Informed%20Neural%20Networks%20%28FBPINNs%29%2C%20in%20learning%20the%0Adynamics%20of%20test%20models%20with%20a%20quasi-stationary%20longtime%20behavior.%20We%20test%20the%0Aapproaches%20for%20data%20sets%20in%20different%20dynamical%20regions%20and%20with%20varying%20noise%0Alevel.%20As%20results%2C%20we%20find%20a%20better%20performance%20for%20the%20FBPINN%20approach%0Acompared%20to%20the%20vanilla%20PINN%20approach%2C%20even%20in%20cases%20with%20data%20from%20only%20a%0Aquasi-stationary%20time%20domain%20with%20few%20dynamics.%0A&entry.1838667208=http%3A//arxiv.org/abs/2410.01599v1&entry.124074799=Read"},
{"title": "HAMLET: Graph Transformer Neural Operator for Partial Differential\n  Equations", "author": "Andrey Bryutkin and Jiahao Huang and Zhongying Deng and Guang Yang and Carola-Bibiane Sch\u00f6nlieb and Angelica Aviles-Rivero", "abstract": "  We present a novel graph transformer framework, HAMLET, designed to address\nthe challenges in solving partial differential equations (PDEs) using neural\nnetworks. The framework uses graph transformers with modular input encoders to\ndirectly incorporate differential equation information into the solution\nprocess. This modularity enhances parameter correspondence control, making\nHAMLET adaptable to PDEs of arbitrary geometries and varied input formats.\nNotably, HAMLET scales effectively with increasing data complexity and noise,\nshowcasing its robustness. HAMLET is not just tailored to a single type of\nphysical simulation, but can be applied across various domains. Moreover, it\nboosts model resilience and performance, especially in scenarios with limited\ndata. We demonstrate, through extensive experiments, that our framework is\ncapable of outperforming current techniques for PDEs.\n", "link": "http://arxiv.org/abs/2402.03541v2", "date": "2024-10-02", "relevancy": 1.8849, "topK": [{"title": "Scaling Rectified Flow Transformers for High-Resolution Image Synthesis", "link": "http://arxiv.org/abs/2403.03206v1", "similarity": 0.5553}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.4571}, {"title": "Boosted dynamic neural networks", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/26302", "similarity": 0.4517}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20HAMLET%3A%20Graph%20Transformer%20Neural%20Operator%20for%20Partial%20Differential%0A%20%20Equations&body=Title%3A%20HAMLET%3A%20Graph%20Transformer%20Neural%20Operator%20for%20Partial%20Differential%0A%20%20Equations%0AAuthor%3A%20Andrey%20Bryutkin%20and%20Jiahao%20Huang%20and%20Zhongying%20Deng%20and%20Guang%20Yang%20and%20Carola-Bibiane%20Sch%C3%B6nlieb%20and%20Angelica%20Aviles-Rivero%0AAbstract%3A%20%20%20We%20present%20a%20novel%20graph%20transformer%20framework%2C%20HAMLET%2C%20designed%20to%20address%0Athe%20challenges%20in%20solving%20partial%20differential%20equations%20%28PDEs%29%20using%20neural%0Anetworks.%20The%20framework%20uses%20graph%20transformers%20with%20modular%20input%20encoders%20to%0Adirectly%20incorporate%20differential%20equation%20information%20into%20the%20solution%0Aprocess.%20This%20modularity%20enhances%20parameter%20correspondence%20control%2C%20making%0AHAMLET%20adaptable%20to%20PDEs%20of%20arbitrary%20geometries%20and%20varied%20input%20formats.%0ANotably%2C%20HAMLET%20scales%20effectively%20with%20increasing%20data%20complexity%20and%20noise%2C%0Ashowcasing%20its%20robustness.%20HAMLET%20is%20not%20just%20tailored%20to%20a%20single%20type%20of%0Aphysical%20simulation%2C%20but%20can%20be%20applied%20across%20various%20domains.%20Moreover%2C%20it%0Aboosts%20model%20resilience%20and%20performance%2C%20especially%20in%20scenarios%20with%20limited%0Adata.%20We%20demonstrate%2C%20through%20extensive%20experiments%2C%20that%20our%20framework%20is%0Acapable%20of%20outperforming%20current%20techniques%20for%20PDEs.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2402.03541v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DHAMLET%253A%2520Graph%2520Transformer%2520Neural%2520Operator%2520for%2520Partial%2520Differential%250A%2520%2520Equations%26entry.906535625%3DAndrey%2520Bryutkin%2520and%2520Jiahao%2520Huang%2520and%2520Zhongying%2520Deng%2520and%2520Guang%2520Yang%2520and%2520Carola-Bibiane%2520Sch%25C3%25B6nlieb%2520and%2520Angelica%2520Aviles-Rivero%26entry.1292438233%3D%2520%2520We%2520present%2520a%2520novel%2520graph%2520transformer%2520framework%252C%2520HAMLET%252C%2520designed%2520to%2520address%250Athe%2520challenges%2520in%2520solving%2520partial%2520differential%2520equations%2520%2528PDEs%2529%2520using%2520neural%250Anetworks.%2520The%2520framework%2520uses%2520graph%2520transformers%2520with%2520modular%2520input%2520encoders%2520to%250Adirectly%2520incorporate%2520differential%2520equation%2520information%2520into%2520the%2520solution%250Aprocess.%2520This%2520modularity%2520enhances%2520parameter%2520correspondence%2520control%252C%2520making%250AHAMLET%2520adaptable%2520to%2520PDEs%2520of%2520arbitrary%2520geometries%2520and%2520varied%2520input%2520formats.%250ANotably%252C%2520HAMLET%2520scales%2520effectively%2520with%2520increasing%2520data%2520complexity%2520and%2520noise%252C%250Ashowcasing%2520its%2520robustness.%2520HAMLET%2520is%2520not%2520just%2520tailored%2520to%2520a%2520single%2520type%2520of%250Aphysical%2520simulation%252C%2520but%2520can%2520be%2520applied%2520across%2520various%2520domains.%2520Moreover%252C%2520it%250Aboosts%2520model%2520resilience%2520and%2520performance%252C%2520especially%2520in%2520scenarios%2520with%2520limited%250Adata.%2520We%2520demonstrate%252C%2520through%2520extensive%2520experiments%252C%2520that%2520our%2520framework%2520is%250Acapable%2520of%2520outperforming%2520current%2520techniques%2520for%2520PDEs.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2402.03541v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=HAMLET%3A%20Graph%20Transformer%20Neural%20Operator%20for%20Partial%20Differential%0A%20%20Equations&entry.906535625=Andrey%20Bryutkin%20and%20Jiahao%20Huang%20and%20Zhongying%20Deng%20and%20Guang%20Yang%20and%20Carola-Bibiane%20Sch%C3%B6nlieb%20and%20Angelica%20Aviles-Rivero&entry.1292438233=%20%20We%20present%20a%20novel%20graph%20transformer%20framework%2C%20HAMLET%2C%20designed%20to%20address%0Athe%20challenges%20in%20solving%20partial%20differential%20equations%20%28PDEs%29%20using%20neural%0Anetworks.%20The%20framework%20uses%20graph%20transformers%20with%20modular%20input%20encoders%20to%0Adirectly%20incorporate%20differential%20equation%20information%20into%20the%20solution%0Aprocess.%20This%20modularity%20enhances%20parameter%20correspondence%20control%2C%20making%0AHAMLET%20adaptable%20to%20PDEs%20of%20arbitrary%20geometries%20and%20varied%20input%20formats.%0ANotably%2C%20HAMLET%20scales%20effectively%20with%20increasing%20data%20complexity%20and%20noise%2C%0Ashowcasing%20its%20robustness.%20HAMLET%20is%20not%20just%20tailored%20to%20a%20single%20type%20of%0Aphysical%20simulation%2C%20but%20can%20be%20applied%20across%20various%20domains.%20Moreover%2C%20it%0Aboosts%20model%20resilience%20and%20performance%2C%20especially%20in%20scenarios%20with%20limited%0Adata.%20We%20demonstrate%2C%20through%20extensive%20experiments%2C%20that%20our%20framework%20is%0Acapable%20of%20outperforming%20current%20techniques%20for%20PDEs.%0A&entry.1838667208=http%3A//arxiv.org/abs/2402.03541v2&entry.124074799=Read"},
{"title": "Towards Futuristic Autonomous Experimentation--A Surprise-Reacting\n  Sequential Experiment Policy", "author": "Imtiaz Ahmed and Satish Bukkapatnam and Bhaskar Botcha and Yu Ding", "abstract": "  An autonomous experimentation platform in manufacturing is supposedly capable\nof conducting a sequential search for finding suitable manufacturing conditions\nby itself or even for discovering new materials with minimal human\nintervention. The core of the intelligent control of such platforms is a policy\nto decide where to conduct the next experiment based on what has been done thus\nfar. Such policy inevitably trades off between exploitation and exploration.\nCurrently, the prevailing approach is to use various acquisition functions in\nthe Bayesian optimization framework. We discuss whether it is beneficial to\ntrade off exploitation versus exploration by measuring the element and degree\nof surprise associated with the immediate past observation. We devise a\nsurprise-reacting policy using two existing surprise metrics, known as the\nShannon surprise and Bayesian surprise. Our analysis shows that the\nsurprise-reacting policy appears to be better suited for quickly characterizing\nthe overall landscape of a response surface under resource constraints. We do\nnot claim that we have a fully autonomous experimentation system but believe\nthat the surprise-reacting capability benefits the automation of sequential\ndecisions in autonomous experimentation.\n", "link": "http://arxiv.org/abs/2112.00600v3", "date": "2024-10-02", "relevancy": 1.4895, "topK": [{"title": "RoboEXP: Action-Conditioned Scene Graph via Interactive Exploration for\n  Robotic Manipulation", "link": "http://arxiv.org/abs/2402.15487v1", "similarity": 0.5878}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.4931}, {"title": "Safe and efficient training of a control agent", "link": "https://patents.google.com/patent/US11709462B2/en", "similarity": 0.4614}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Towards%20Futuristic%20Autonomous%20Experimentation--A%20Surprise-Reacting%0A%20%20Sequential%20Experiment%20Policy&body=Title%3A%20Towards%20Futuristic%20Autonomous%20Experimentation--A%20Surprise-Reacting%0A%20%20Sequential%20Experiment%20Policy%0AAuthor%3A%20Imtiaz%20Ahmed%20and%20Satish%20Bukkapatnam%20and%20Bhaskar%20Botcha%20and%20Yu%20Ding%0AAbstract%3A%20%20%20An%20autonomous%20experimentation%20platform%20in%20manufacturing%20is%20supposedly%20capable%0Aof%20conducting%20a%20sequential%20search%20for%20finding%20suitable%20manufacturing%20conditions%0Aby%20itself%20or%20even%20for%20discovering%20new%20materials%20with%20minimal%20human%0Aintervention.%20The%20core%20of%20the%20intelligent%20control%20of%20such%20platforms%20is%20a%20policy%0Ato%20decide%20where%20to%20conduct%20the%20next%20experiment%20based%20on%20what%20has%20been%20done%20thus%0Afar.%20Such%20policy%20inevitably%20trades%20off%20between%20exploitation%20and%20exploration.%0ACurrently%2C%20the%20prevailing%20approach%20is%20to%20use%20various%20acquisition%20functions%20in%0Athe%20Bayesian%20optimization%20framework.%20We%20discuss%20whether%20it%20is%20beneficial%20to%0Atrade%20off%20exploitation%20versus%20exploration%20by%20measuring%20the%20element%20and%20degree%0Aof%20surprise%20associated%20with%20the%20immediate%20past%20observation.%20We%20devise%20a%0Asurprise-reacting%20policy%20using%20two%20existing%20surprise%20metrics%2C%20known%20as%20the%0AShannon%20surprise%20and%20Bayesian%20surprise.%20Our%20analysis%20shows%20that%20the%0Asurprise-reacting%20policy%20appears%20to%20be%20better%20suited%20for%20quickly%20characterizing%0Athe%20overall%20landscape%20of%20a%20response%20surface%20under%20resource%20constraints.%20We%20do%0Anot%20claim%20that%20we%20have%20a%20fully%20autonomous%20experimentation%20system%20but%20believe%0Athat%20the%20surprise-reacting%20capability%20benefits%20the%20automation%20of%20sequential%0Adecisions%20in%20autonomous%20experimentation.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2112.00600v3%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DTowards%2520Futuristic%2520Autonomous%2520Experimentation--A%2520Surprise-Reacting%250A%2520%2520Sequential%2520Experiment%2520Policy%26entry.906535625%3DImtiaz%2520Ahmed%2520and%2520Satish%2520Bukkapatnam%2520and%2520Bhaskar%2520Botcha%2520and%2520Yu%2520Ding%26entry.1292438233%3D%2520%2520An%2520autonomous%2520experimentation%2520platform%2520in%2520manufacturing%2520is%2520supposedly%2520capable%250Aof%2520conducting%2520a%2520sequential%2520search%2520for%2520finding%2520suitable%2520manufacturing%2520conditions%250Aby%2520itself%2520or%2520even%2520for%2520discovering%2520new%2520materials%2520with%2520minimal%2520human%250Aintervention.%2520The%2520core%2520of%2520the%2520intelligent%2520control%2520of%2520such%2520platforms%2520is%2520a%2520policy%250Ato%2520decide%2520where%2520to%2520conduct%2520the%2520next%2520experiment%2520based%2520on%2520what%2520has%2520been%2520done%2520thus%250Afar.%2520Such%2520policy%2520inevitably%2520trades%2520off%2520between%2520exploitation%2520and%2520exploration.%250ACurrently%252C%2520the%2520prevailing%2520approach%2520is%2520to%2520use%2520various%2520acquisition%2520functions%2520in%250Athe%2520Bayesian%2520optimization%2520framework.%2520We%2520discuss%2520whether%2520it%2520is%2520beneficial%2520to%250Atrade%2520off%2520exploitation%2520versus%2520exploration%2520by%2520measuring%2520the%2520element%2520and%2520degree%250Aof%2520surprise%2520associated%2520with%2520the%2520immediate%2520past%2520observation.%2520We%2520devise%2520a%250Asurprise-reacting%2520policy%2520using%2520two%2520existing%2520surprise%2520metrics%252C%2520known%2520as%2520the%250AShannon%2520surprise%2520and%2520Bayesian%2520surprise.%2520Our%2520analysis%2520shows%2520that%2520the%250Asurprise-reacting%2520policy%2520appears%2520to%2520be%2520better%2520suited%2520for%2520quickly%2520characterizing%250Athe%2520overall%2520landscape%2520of%2520a%2520response%2520surface%2520under%2520resource%2520constraints.%2520We%2520do%250Anot%2520claim%2520that%2520we%2520have%2520a%2520fully%2520autonomous%2520experimentation%2520system%2520but%2520believe%250Athat%2520the%2520surprise-reacting%2520capability%2520benefits%2520the%2520automation%2520of%2520sequential%250Adecisions%2520in%2520autonomous%2520experimentation.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2112.00600v3%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Towards%20Futuristic%20Autonomous%20Experimentation--A%20Surprise-Reacting%0A%20%20Sequential%20Experiment%20Policy&entry.906535625=Imtiaz%20Ahmed%20and%20Satish%20Bukkapatnam%20and%20Bhaskar%20Botcha%20and%20Yu%20Ding&entry.1292438233=%20%20An%20autonomous%20experimentation%20platform%20in%20manufacturing%20is%20supposedly%20capable%0Aof%20conducting%20a%20sequential%20search%20for%20finding%20suitable%20manufacturing%20conditions%0Aby%20itself%20or%20even%20for%20discovering%20new%20materials%20with%20minimal%20human%0Aintervention.%20The%20core%20of%20the%20intelligent%20control%20of%20such%20platforms%20is%20a%20policy%0Ato%20decide%20where%20to%20conduct%20the%20next%20experiment%20based%20on%20what%20has%20been%20done%20thus%0Afar.%20Such%20policy%20inevitably%20trades%20off%20between%20exploitation%20and%20exploration.%0ACurrently%2C%20the%20prevailing%20approach%20is%20to%20use%20various%20acquisition%20functions%20in%0Athe%20Bayesian%20optimization%20framework.%20We%20discuss%20whether%20it%20is%20beneficial%20to%0Atrade%20off%20exploitation%20versus%20exploration%20by%20measuring%20the%20element%20and%20degree%0Aof%20surprise%20associated%20with%20the%20immediate%20past%20observation.%20We%20devise%20a%0Asurprise-reacting%20policy%20using%20two%20existing%20surprise%20metrics%2C%20known%20as%20the%0AShannon%20surprise%20and%20Bayesian%20surprise.%20Our%20analysis%20shows%20that%20the%0Asurprise-reacting%20policy%20appears%20to%20be%20better%20suited%20for%20quickly%20characterizing%0Athe%20overall%20landscape%20of%20a%20response%20surface%20under%20resource%20constraints.%20We%20do%0Anot%20claim%20that%20we%20have%20a%20fully%20autonomous%20experimentation%20system%20but%20believe%0Athat%20the%20surprise-reacting%20capability%20benefits%20the%20automation%20of%20sequential%0Adecisions%20in%20autonomous%20experimentation.%0A&entry.1838667208=http%3A//arxiv.org/abs/2112.00600v3&entry.124074799=Read"},
{"title": "When a language model is optimized for reasoning, does it still show\n  embers of autoregression? An analysis of OpenAI o1", "author": "R. Thomas McCoy and Shunyu Yao and Dan Friedman and Mathew D. Hardy and Thomas L. Griffiths", "abstract": "  In \"Embers of Autoregression\" (McCoy et al., 2023), we showed that several\nlarge language models (LLMs) have some important limitations that are\nattributable to their origins in next-word prediction. Here we investigate\nwhether these issues persist with o1, a new system from OpenAI that differs\nfrom previous LLMs in that it is optimized for reasoning. We find that o1\nsubstantially outperforms previous LLMs in many cases, with particularly large\nimprovements on rare variants of common tasks (e.g., forming acronyms from the\nsecond letter of each word in a list, rather than the first letter). Despite\nthese quantitative improvements, however, o1 still displays the same\nqualitative trends that we observed in previous systems. Specifically, o1 -\nlike previous LLMs - is sensitive to the probability of examples and tasks,\nperforming better and requiring fewer \"thinking tokens\" in high-probability\nsettings than in low-probability ones. These results show that optimizing a\nlanguage model for reasoning can mitigate but might not fully overcome the\nlanguage model's probability sensitivity.\n", "link": "http://arxiv.org/abs/2410.01792v1", "date": "2024-10-02", "relevancy": 1.9599, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.4977}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.4977}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.4514}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20When%20a%20language%20model%20is%20optimized%20for%20reasoning%2C%20does%20it%20still%20show%0A%20%20embers%20of%20autoregression%3F%20An%20analysis%20of%20OpenAI%20o1&body=Title%3A%20When%20a%20language%20model%20is%20optimized%20for%20reasoning%2C%20does%20it%20still%20show%0A%20%20embers%20of%20autoregression%3F%20An%20analysis%20of%20OpenAI%20o1%0AAuthor%3A%20R.%20Thomas%20McCoy%20and%20Shunyu%20Yao%20and%20Dan%20Friedman%20and%20Mathew%20D.%20Hardy%20and%20Thomas%20L.%20Griffiths%0AAbstract%3A%20%20%20In%20%22Embers%20of%20Autoregression%22%20%28McCoy%20et%20al.%2C%202023%29%2C%20we%20showed%20that%20several%0Alarge%20language%20models%20%28LLMs%29%20have%20some%20important%20limitations%20that%20are%0Aattributable%20to%20their%20origins%20in%20next-word%20prediction.%20Here%20we%20investigate%0Awhether%20these%20issues%20persist%20with%20o1%2C%20a%20new%20system%20from%20OpenAI%20that%20differs%0Afrom%20previous%20LLMs%20in%20that%20it%20is%20optimized%20for%20reasoning.%20We%20find%20that%20o1%0Asubstantially%20outperforms%20previous%20LLMs%20in%20many%20cases%2C%20with%20particularly%20large%0Aimprovements%20on%20rare%20variants%20of%20common%20tasks%20%28e.g.%2C%20forming%20acronyms%20from%20the%0Asecond%20letter%20of%20each%20word%20in%20a%20list%2C%20rather%20than%20the%20first%20letter%29.%20Despite%0Athese%20quantitative%20improvements%2C%20however%2C%20o1%20still%20displays%20the%20same%0Aqualitative%20trends%20that%20we%20observed%20in%20previous%20systems.%20Specifically%2C%20o1%20-%0Alike%20previous%20LLMs%20-%20is%20sensitive%20to%20the%20probability%20of%20examples%20and%20tasks%2C%0Aperforming%20better%20and%20requiring%20fewer%20%22thinking%20tokens%22%20in%20high-probability%0Asettings%20than%20in%20low-probability%20ones.%20These%20results%20show%20that%20optimizing%20a%0Alanguage%20model%20for%20reasoning%20can%20mitigate%20but%20might%20not%20fully%20overcome%20the%0Alanguage%20model%27s%20probability%20sensitivity.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2410.01792v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DWhen%2520a%2520language%2520model%2520is%2520optimized%2520for%2520reasoning%252C%2520does%2520it%2520still%2520show%250A%2520%2520embers%2520of%2520autoregression%253F%2520An%2520analysis%2520of%2520OpenAI%2520o1%26entry.906535625%3DR.%2520Thomas%2520McCoy%2520and%2520Shunyu%2520Yao%2520and%2520Dan%2520Friedman%2520and%2520Mathew%2520D.%2520Hardy%2520and%2520Thomas%2520L.%2520Griffiths%26entry.1292438233%3D%2520%2520In%2520%2522Embers%2520of%2520Autoregression%2522%2520%2528McCoy%2520et%2520al.%252C%25202023%2529%252C%2520we%2520showed%2520that%2520several%250Alarge%2520language%2520models%2520%2528LLMs%2529%2520have%2520some%2520important%2520limitations%2520that%2520are%250Aattributable%2520to%2520their%2520origins%2520in%2520next-word%2520prediction.%2520Here%2520we%2520investigate%250Awhether%2520these%2520issues%2520persist%2520with%2520o1%252C%2520a%2520new%2520system%2520from%2520OpenAI%2520that%2520differs%250Afrom%2520previous%2520LLMs%2520in%2520that%2520it%2520is%2520optimized%2520for%2520reasoning.%2520We%2520find%2520that%2520o1%250Asubstantially%2520outperforms%2520previous%2520LLMs%2520in%2520many%2520cases%252C%2520with%2520particularly%2520large%250Aimprovements%2520on%2520rare%2520variants%2520of%2520common%2520tasks%2520%2528e.g.%252C%2520forming%2520acronyms%2520from%2520the%250Asecond%2520letter%2520of%2520each%2520word%2520in%2520a%2520list%252C%2520rather%2520than%2520the%2520first%2520letter%2529.%2520Despite%250Athese%2520quantitative%2520improvements%252C%2520however%252C%2520o1%2520still%2520displays%2520the%2520same%250Aqualitative%2520trends%2520that%2520we%2520observed%2520in%2520previous%2520systems.%2520Specifically%252C%2520o1%2520-%250Alike%2520previous%2520LLMs%2520-%2520is%2520sensitive%2520to%2520the%2520probability%2520of%2520examples%2520and%2520tasks%252C%250Aperforming%2520better%2520and%2520requiring%2520fewer%2520%2522thinking%2520tokens%2522%2520in%2520high-probability%250Asettings%2520than%2520in%2520low-probability%2520ones.%2520These%2520results%2520show%2520that%2520optimizing%2520a%250Alanguage%2520model%2520for%2520reasoning%2520can%2520mitigate%2520but%2520might%2520not%2520fully%2520overcome%2520the%250Alanguage%2520model%2527s%2520probability%2520sensitivity.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2410.01792v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=When%20a%20language%20model%20is%20optimized%20for%20reasoning%2C%20does%20it%20still%20show%0A%20%20embers%20of%20autoregression%3F%20An%20analysis%20of%20OpenAI%20o1&entry.906535625=R.%20Thomas%20McCoy%20and%20Shunyu%20Yao%20and%20Dan%20Friedman%20and%20Mathew%20D.%20Hardy%20and%20Thomas%20L.%20Griffiths&entry.1292438233=%20%20In%20%22Embers%20of%20Autoregression%22%20%28McCoy%20et%20al.%2C%202023%29%2C%20we%20showed%20that%20several%0Alarge%20language%20models%20%28LLMs%29%20have%20some%20important%20limitations%20that%20are%0Aattributable%20to%20their%20origins%20in%20next-word%20prediction.%20Here%20we%20investigate%0Awhether%20these%20issues%20persist%20with%20o1%2C%20a%20new%20system%20from%20OpenAI%20that%20differs%0Afrom%20previous%20LLMs%20in%20that%20it%20is%20optimized%20for%20reasoning.%20We%20find%20that%20o1%0Asubstantially%20outperforms%20previous%20LLMs%20in%20many%20cases%2C%20with%20particularly%20large%0Aimprovements%20on%20rare%20variants%20of%20common%20tasks%20%28e.g.%2C%20forming%20acronyms%20from%20the%0Asecond%20letter%20of%20each%20word%20in%20a%20list%2C%20rather%20than%20the%20first%20letter%29.%20Despite%0Athese%20quantitative%20improvements%2C%20however%2C%20o1%20still%20displays%20the%20same%0Aqualitative%20trends%20that%20we%20observed%20in%20previous%20systems.%20Specifically%2C%20o1%20-%0Alike%20previous%20LLMs%20-%20is%20sensitive%20to%20the%20probability%20of%20examples%20and%20tasks%2C%0Aperforming%20better%20and%20requiring%20fewer%20%22thinking%20tokens%22%20in%20high-probability%0Asettings%20than%20in%20low-probability%20ones.%20These%20results%20show%20that%20optimizing%20a%0Alanguage%20model%20for%20reasoning%20can%20mitigate%20but%20might%20not%20fully%20overcome%20the%0Alanguage%20model%27s%20probability%20sensitivity.%0A&entry.1838667208=http%3A//arxiv.org/abs/2410.01792v1&entry.124074799=Read"},
{"title": "Cost-Effective Online Multi-LLM Selection with Versatile Reward Models", "author": "Xiangxiang Dai and Jin Li and Xutong Liu and Anqi Yu and John C. S. Lui", "abstract": "  With the rapid advancement of large language models (LLMs), the diversity of\nmulti-LLM tasks and the variability in their pricing structures have become\nincreasingly important, as costs can vary greatly between different LLMs. To\ntackle these challenges, we introduce the \\textit{C2MAB-V}, a\n\\underline{C}ost-effective \\underline{C}ombinatorial \\underline{M}ulti-armed\n\\underline{B}andit with \\underline{V}ersatile reward models for optimal LLM\nselection and usage. This online model differs from traditional static\napproaches or those reliant on a single LLM without cost consideration. With\nmultiple LLMs deployed on a scheduling cloud and a local server dedicated to\nhandling user queries, \\textit{C2MAB-V} facilitates the selection of multiple\nLLMs over a combinatorial search space, specifically tailored for various\ncollaborative task types with different reward models. Based on our designed\nonline feedback mechanism and confidence bound technique, \\textit{C2MAB-V} can\neffectively address the multi-LLM selection challenge by managing the\nexploration-exploitation trade-off across different models, while also\nbalancing cost and reward for diverse tasks. The NP-hard integer linear\nprogramming problem for selecting multiple LLMs with trade-off dilemmas is\naddressed by: i) decomposing the integer problem into a relaxed form by the\nlocal server, ii) utilizing a discretization rounding scheme that provides\noptimal LLM combinations by the scheduling cloud, and iii) continual online\nupdates based on feedback. Theoretically, we prove that \\textit{C2MAB-V} offers\nstrict guarantees over versatile reward models, matching state-of-the-art\nresults for regret and violations in some degenerate cases. Empirically, we\nshow that \\textit{C2MAB-V} effectively balances performance and cost-efficiency\nwith nine LLMs for three application scenarios.\n", "link": "http://arxiv.org/abs/2405.16587v2", "date": "2024-10-02", "relevancy": 1.4531, "topK": [{"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.4919}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.4882}, {"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.4671}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Cost-Effective%20Online%20Multi-LLM%20Selection%20with%20Versatile%20Reward%20Models&body=Title%3A%20Cost-Effective%20Online%20Multi-LLM%20Selection%20with%20Versatile%20Reward%20Models%0AAuthor%3A%20Xiangxiang%20Dai%20and%20Jin%20Li%20and%20Xutong%20Liu%20and%20Anqi%20Yu%20and%20John%20C.%20S.%20Lui%0AAbstract%3A%20%20%20With%20the%20rapid%20advancement%20of%20large%20language%20models%20%28LLMs%29%2C%20the%20diversity%20of%0Amulti-LLM%20tasks%20and%20the%20variability%20in%20their%20pricing%20structures%20have%20become%0Aincreasingly%20important%2C%20as%20costs%20can%20vary%20greatly%20between%20different%20LLMs.%20To%0Atackle%20these%20challenges%2C%20we%20introduce%20the%20%5Ctextit%7BC2MAB-V%7D%2C%20a%0A%5Cunderline%7BC%7Dost-effective%20%5Cunderline%7BC%7Dombinatorial%20%5Cunderline%7BM%7Dulti-armed%0A%5Cunderline%7BB%7Dandit%20with%20%5Cunderline%7BV%7Dersatile%20reward%20models%20for%20optimal%20LLM%0Aselection%20and%20usage.%20This%20online%20model%20differs%20from%20traditional%20static%0Aapproaches%20or%20those%20reliant%20on%20a%20single%20LLM%20without%20cost%20consideration.%20With%0Amultiple%20LLMs%20deployed%20on%20a%20scheduling%20cloud%20and%20a%20local%20server%20dedicated%20to%0Ahandling%20user%20queries%2C%20%5Ctextit%7BC2MAB-V%7D%20facilitates%20the%20selection%20of%20multiple%0ALLMs%20over%20a%20combinatorial%20search%20space%2C%20specifically%20tailored%20for%20various%0Acollaborative%20task%20types%20with%20different%20reward%20models.%20Based%20on%20our%20designed%0Aonline%20feedback%20mechanism%20and%20confidence%20bound%20technique%2C%20%5Ctextit%7BC2MAB-V%7D%20can%0Aeffectively%20address%20the%20multi-LLM%20selection%20challenge%20by%20managing%20the%0Aexploration-exploitation%20trade-off%20across%20different%20models%2C%20while%20also%0Abalancing%20cost%20and%20reward%20for%20diverse%20tasks.%20The%20NP-hard%20integer%20linear%0Aprogramming%20problem%20for%20selecting%20multiple%20LLMs%20with%20trade-off%20dilemmas%20is%0Aaddressed%20by%3A%20i%29%20decomposing%20the%20integer%20problem%20into%20a%20relaxed%20form%20by%20the%0Alocal%20server%2C%20ii%29%20utilizing%20a%20discretization%20rounding%20scheme%20that%20provides%0Aoptimal%20LLM%20combinations%20by%20the%20scheduling%20cloud%2C%20and%20iii%29%20continual%20online%0Aupdates%20based%20on%20feedback.%20Theoretically%2C%20we%20prove%20that%20%5Ctextit%7BC2MAB-V%7D%20offers%0Astrict%20guarantees%20over%20versatile%20reward%20models%2C%20matching%20state-of-the-art%0Aresults%20for%20regret%20and%20violations%20in%20some%20degenerate%20cases.%20Empirically%2C%20we%0Ashow%20that%20%5Ctextit%7BC2MAB-V%7D%20effectively%20balances%20performance%20and%20cost-efficiency%0Awith%20nine%20LLMs%20for%20three%20application%20scenarios.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2405.16587v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DCost-Effective%2520Online%2520Multi-LLM%2520Selection%2520with%2520Versatile%2520Reward%2520Models%26entry.906535625%3DXiangxiang%2520Dai%2520and%2520Jin%2520Li%2520and%2520Xutong%2520Liu%2520and%2520Anqi%2520Yu%2520and%2520John%2520C.%2520S.%2520Lui%26entry.1292438233%3D%2520%2520With%2520the%2520rapid%2520advancement%2520of%2520large%2520language%2520models%2520%2528LLMs%2529%252C%2520the%2520diversity%2520of%250Amulti-LLM%2520tasks%2520and%2520the%2520variability%2520in%2520their%2520pricing%2520structures%2520have%2520become%250Aincreasingly%2520important%252C%2520as%2520costs%2520can%2520vary%2520greatly%2520between%2520different%2520LLMs.%2520To%250Atackle%2520these%2520challenges%252C%2520we%2520introduce%2520the%2520%255Ctextit%257BC2MAB-V%257D%252C%2520a%250A%255Cunderline%257BC%257Dost-effective%2520%255Cunderline%257BC%257Dombinatorial%2520%255Cunderline%257BM%257Dulti-armed%250A%255Cunderline%257BB%257Dandit%2520with%2520%255Cunderline%257BV%257Dersatile%2520reward%2520models%2520for%2520optimal%2520LLM%250Aselection%2520and%2520usage.%2520This%2520online%2520model%2520differs%2520from%2520traditional%2520static%250Aapproaches%2520or%2520those%2520reliant%2520on%2520a%2520single%2520LLM%2520without%2520cost%2520consideration.%2520With%250Amultiple%2520LLMs%2520deployed%2520on%2520a%2520scheduling%2520cloud%2520and%2520a%2520local%2520server%2520dedicated%2520to%250Ahandling%2520user%2520queries%252C%2520%255Ctextit%257BC2MAB-V%257D%2520facilitates%2520the%2520selection%2520of%2520multiple%250ALLMs%2520over%2520a%2520combinatorial%2520search%2520space%252C%2520specifically%2520tailored%2520for%2520various%250Acollaborative%2520task%2520types%2520with%2520different%2520reward%2520models.%2520Based%2520on%2520our%2520designed%250Aonline%2520feedback%2520mechanism%2520and%2520confidence%2520bound%2520technique%252C%2520%255Ctextit%257BC2MAB-V%257D%2520can%250Aeffectively%2520address%2520the%2520multi-LLM%2520selection%2520challenge%2520by%2520managing%2520the%250Aexploration-exploitation%2520trade-off%2520across%2520different%2520models%252C%2520while%2520also%250Abalancing%2520cost%2520and%2520reward%2520for%2520diverse%2520tasks.%2520The%2520NP-hard%2520integer%2520linear%250Aprogramming%2520problem%2520for%2520selecting%2520multiple%2520LLMs%2520with%2520trade-off%2520dilemmas%2520is%250Aaddressed%2520by%253A%2520i%2529%2520decomposing%2520the%2520integer%2520problem%2520into%2520a%2520relaxed%2520form%2520by%2520the%250Alocal%2520server%252C%2520ii%2529%2520utilizing%2520a%2520discretization%2520rounding%2520scheme%2520that%2520provides%250Aoptimal%2520LLM%2520combinations%2520by%2520the%2520scheduling%2520cloud%252C%2520and%2520iii%2529%2520continual%2520online%250Aupdates%2520based%2520on%2520feedback.%2520Theoretically%252C%2520we%2520prove%2520that%2520%255Ctextit%257BC2MAB-V%257D%2520offers%250Astrict%2520guarantees%2520over%2520versatile%2520reward%2520models%252C%2520matching%2520state-of-the-art%250Aresults%2520for%2520regret%2520and%2520violations%2520in%2520some%2520degenerate%2520cases.%2520Empirically%252C%2520we%250Ashow%2520that%2520%255Ctextit%257BC2MAB-V%257D%2520effectively%2520balances%2520performance%2520and%2520cost-efficiency%250Awith%2520nine%2520LLMs%2520for%2520three%2520application%2520scenarios.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2405.16587v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Cost-Effective%20Online%20Multi-LLM%20Selection%20with%20Versatile%20Reward%20Models&entry.906535625=Xiangxiang%20Dai%20and%20Jin%20Li%20and%20Xutong%20Liu%20and%20Anqi%20Yu%20and%20John%20C.%20S.%20Lui&entry.1292438233=%20%20With%20the%20rapid%20advancement%20of%20large%20language%20models%20%28LLMs%29%2C%20the%20diversity%20of%0Amulti-LLM%20tasks%20and%20the%20variability%20in%20their%20pricing%20structures%20have%20become%0Aincreasingly%20important%2C%20as%20costs%20can%20vary%20greatly%20between%20different%20LLMs.%20To%0Atackle%20these%20challenges%2C%20we%20introduce%20the%20%5Ctextit%7BC2MAB-V%7D%2C%20a%0A%5Cunderline%7BC%7Dost-effective%20%5Cunderline%7BC%7Dombinatorial%20%5Cunderline%7BM%7Dulti-armed%0A%5Cunderline%7BB%7Dandit%20with%20%5Cunderline%7BV%7Dersatile%20reward%20models%20for%20optimal%20LLM%0Aselection%20and%20usage.%20This%20online%20model%20differs%20from%20traditional%20static%0Aapproaches%20or%20those%20reliant%20on%20a%20single%20LLM%20without%20cost%20consideration.%20With%0Amultiple%20LLMs%20deployed%20on%20a%20scheduling%20cloud%20and%20a%20local%20server%20dedicated%20to%0Ahandling%20user%20queries%2C%20%5Ctextit%7BC2MAB-V%7D%20facilitates%20the%20selection%20of%20multiple%0ALLMs%20over%20a%20combinatorial%20search%20space%2C%20specifically%20tailored%20for%20various%0Acollaborative%20task%20types%20with%20different%20reward%20models.%20Based%20on%20our%20designed%0Aonline%20feedback%20mechanism%20and%20confidence%20bound%20technique%2C%20%5Ctextit%7BC2MAB-V%7D%20can%0Aeffectively%20address%20the%20multi-LLM%20selection%20challenge%20by%20managing%20the%0Aexploration-exploitation%20trade-off%20across%20different%20models%2C%20while%20also%0Abalancing%20cost%20and%20reward%20for%20diverse%20tasks.%20The%20NP-hard%20integer%20linear%0Aprogramming%20problem%20for%20selecting%20multiple%20LLMs%20with%20trade-off%20dilemmas%20is%0Aaddressed%20by%3A%20i%29%20decomposing%20the%20integer%20problem%20into%20a%20relaxed%20form%20by%20the%0Alocal%20server%2C%20ii%29%20utilizing%20a%20discretization%20rounding%20scheme%20that%20provides%0Aoptimal%20LLM%20combinations%20by%20the%20scheduling%20cloud%2C%20and%20iii%29%20continual%20online%0Aupdates%20based%20on%20feedback.%20Theoretically%2C%20we%20prove%20that%20%5Ctextit%7BC2MAB-V%7D%20offers%0Astrict%20guarantees%20over%20versatile%20reward%20models%2C%20matching%20state-of-the-art%0Aresults%20for%20regret%20and%20violations%20in%20some%20degenerate%20cases.%20Empirically%2C%20we%0Ashow%20that%20%5Ctextit%7BC2MAB-V%7D%20effectively%20balances%20performance%20and%20cost-efficiency%0Awith%20nine%20LLMs%20for%20three%20application%20scenarios.%0A&entry.1838667208=http%3A//arxiv.org/abs/2405.16587v2&entry.124074799=Read"},
{"title": "Entropy-Based Uncertainty Modeling for Trajectory Prediction in\n  Autonomous Driving", "author": "Aron Distelzweig and Andreas Look and Eitan Kosman and Faris Janjo\u0161 and J\u00f6rg Wagner and Abhinav Valadaa", "abstract": "  In autonomous driving, accurate motion prediction is essential for safe and\nefficient motion planning. To ensure safety, planners must rely on reliable\nuncertainty information about the predicted future behavior of surrounding\nagents, yet this aspect has received limited attention. This paper addresses\nthe so-far neglected problem of uncertainty modeling in trajectory prediction.\nWe adopt a holistic approach that focuses on uncertainty quantification,\ndecomposition, and the influence of model composition. Our method is based on a\ntheoretically grounded information-theoretic approach to measure uncertainty,\nallowing us to decompose total uncertainty into its aleatoric and epistemic\ncomponents. We conduct extensive experiments on the nuScenes dataset to assess\nhow different model architectures and configurations affect uncertainty\nquantification and model robustness.\n", "link": "http://arxiv.org/abs/2410.01628v1", "date": "2024-10-02", "relevancy": 1.779, "topK": [{"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.6138}, {"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.6}, {"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.5547}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Entropy-Based%20Uncertainty%20Modeling%20for%20Trajectory%20Prediction%20in%0A%20%20Autonomous%20Driving&body=Title%3A%20Entropy-Based%20Uncertainty%20Modeling%20for%20Trajectory%20Prediction%20in%0A%20%20Autonomous%20Driving%0AAuthor%3A%20Aron%20Distelzweig%20and%20Andreas%20Look%20and%20Eitan%20Kosman%20and%20Faris%20Janjo%C5%A1%20and%20J%C3%B6rg%20Wagner%20and%20Abhinav%20Valadaa%0AAbstract%3A%20%20%20In%20autonomous%20driving%2C%20accurate%20motion%20prediction%20is%20essential%20for%20safe%20and%0Aefficient%20motion%20planning.%20To%20ensure%20safety%2C%20planners%20must%20rely%20on%20reliable%0Auncertainty%20information%20about%20the%20predicted%20future%20behavior%20of%20surrounding%0Aagents%2C%20yet%20this%20aspect%20has%20received%20limited%20attention.%20This%20paper%20addresses%0Athe%20so-far%20neglected%20problem%20of%20uncertainty%20modeling%20in%20trajectory%20prediction.%0AWe%20adopt%20a%20holistic%20approach%20that%20focuses%20on%20uncertainty%20quantification%2C%0Adecomposition%2C%20and%20the%20influence%20of%20model%20composition.%20Our%20method%20is%20based%20on%20a%0Atheoretically%20grounded%20information-theoretic%20approach%20to%20measure%20uncertainty%2C%0Aallowing%20us%20to%20decompose%20total%20uncertainty%20into%20its%20aleatoric%20and%20epistemic%0Acomponents.%20We%20conduct%20extensive%20experiments%20on%20the%20nuScenes%20dataset%20to%20assess%0Ahow%20different%20model%20architectures%20and%20configurations%20affect%20uncertainty%0Aquantification%20and%20model%20robustness.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2410.01628v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DEntropy-Based%2520Uncertainty%2520Modeling%2520for%2520Trajectory%2520Prediction%2520in%250A%2520%2520Autonomous%2520Driving%26entry.906535625%3DAron%2520Distelzweig%2520and%2520Andreas%2520Look%2520and%2520Eitan%2520Kosman%2520and%2520Faris%2520Janjo%25C5%25A1%2520and%2520J%25C3%25B6rg%2520Wagner%2520and%2520Abhinav%2520Valadaa%26entry.1292438233%3D%2520%2520In%2520autonomous%2520driving%252C%2520accurate%2520motion%2520prediction%2520is%2520essential%2520for%2520safe%2520and%250Aefficient%2520motion%2520planning.%2520To%2520ensure%2520safety%252C%2520planners%2520must%2520rely%2520on%2520reliable%250Auncertainty%2520information%2520about%2520the%2520predicted%2520future%2520behavior%2520of%2520surrounding%250Aagents%252C%2520yet%2520this%2520aspect%2520has%2520received%2520limited%2520attention.%2520This%2520paper%2520addresses%250Athe%2520so-far%2520neglected%2520problem%2520of%2520uncertainty%2520modeling%2520in%2520trajectory%2520prediction.%250AWe%2520adopt%2520a%2520holistic%2520approach%2520that%2520focuses%2520on%2520uncertainty%2520quantification%252C%250Adecomposition%252C%2520and%2520the%2520influence%2520of%2520model%2520composition.%2520Our%2520method%2520is%2520based%2520on%2520a%250Atheoretically%2520grounded%2520information-theoretic%2520approach%2520to%2520measure%2520uncertainty%252C%250Aallowing%2520us%2520to%2520decompose%2520total%2520uncertainty%2520into%2520its%2520aleatoric%2520and%2520epistemic%250Acomponents.%2520We%2520conduct%2520extensive%2520experiments%2520on%2520the%2520nuScenes%2520dataset%2520to%2520assess%250Ahow%2520different%2520model%2520architectures%2520and%2520configurations%2520affect%2520uncertainty%250Aquantification%2520and%2520model%2520robustness.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2410.01628v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Entropy-Based%20Uncertainty%20Modeling%20for%20Trajectory%20Prediction%20in%0A%20%20Autonomous%20Driving&entry.906535625=Aron%20Distelzweig%20and%20Andreas%20Look%20and%20Eitan%20Kosman%20and%20Faris%20Janjo%C5%A1%20and%20J%C3%B6rg%20Wagner%20and%20Abhinav%20Valadaa&entry.1292438233=%20%20In%20autonomous%20driving%2C%20accurate%20motion%20prediction%20is%20essential%20for%20safe%20and%0Aefficient%20motion%20planning.%20To%20ensure%20safety%2C%20planners%20must%20rely%20on%20reliable%0Auncertainty%20information%20about%20the%20predicted%20future%20behavior%20of%20surrounding%0Aagents%2C%20yet%20this%20aspect%20has%20received%20limited%20attention.%20This%20paper%20addresses%0Athe%20so-far%20neglected%20problem%20of%20uncertainty%20modeling%20in%20trajectory%20prediction.%0AWe%20adopt%20a%20holistic%20approach%20that%20focuses%20on%20uncertainty%20quantification%2C%0Adecomposition%2C%20and%20the%20influence%20of%20model%20composition.%20Our%20method%20is%20based%20on%20a%0Atheoretically%20grounded%20information-theoretic%20approach%20to%20measure%20uncertainty%2C%0Aallowing%20us%20to%20decompose%20total%20uncertainty%20into%20its%20aleatoric%20and%20epistemic%0Acomponents.%20We%20conduct%20extensive%20experiments%20on%20the%20nuScenes%20dataset%20to%20assess%0Ahow%20different%20model%20architectures%20and%20configurations%20affect%20uncertainty%0Aquantification%20and%20model%20robustness.%0A&entry.1838667208=http%3A//arxiv.org/abs/2410.01628v1&entry.124074799=Read"},
{"title": "The Impact of Scanner Domain Shift on Deep Learning Performance in\n  Medical Imaging: an Experimental Study", "author": "Brian Guo and Darui Lu and Gregory Szumel and Rongze Gui and Tingyu Wang and Nicholas Konz and Maciej A. Mazurowski", "abstract": "  Purpose: Medical images acquired using different scanners and protocols can\ndiffer substantially in their appearance. This phenomenon, scanner domain\nshift, can result in a drop in the performance of deep neural networks which\nare trained on data acquired by one scanner and tested on another. This\nsignificant practical issue is well-acknowledged, however, no systematic study\nof the issue is available across different modalities and diagnostic tasks.\nMaterials and Methods: In this paper, we present a broad experimental study\nevaluating the impact of scanner domain shift on convolutional neural network\nperformance for different automated diagnostic tasks. We evaluate this\nphenomenon in common radiological modalities, including X-ray, CT, and MRI.\nResults: We find that network performance on data from a different scanner is\nalmost always worse than on same-scanner data, and we quantify the degree of\nperformance drop across different datasets. Notably, we find that this drop is\nmost severe for MRI, moderate for X-ray, and quite small for CT, on average,\nwhich we attribute to the standardized nature of CT acquisition systems which\nis not present in MRI or X-ray. We also study how injecting varying amounts of\ntarget domain data into the training set, as well as adding noise to the\ntraining data, helps with generalization. Conclusion: Our results provide\nextensive experimental evidence and quantification of the extent of performance\ndrop caused by scanner domain shift in deep learning across different\nmodalities, with the goal of guiding the future development of robust deep\nlearning models for medical image analysis.\n", "link": "http://arxiv.org/abs/2409.04368v2", "date": "2024-10-02", "relevancy": 0.9904, "topK": [{"title": "SDXS: Real-Time One-Step Latent Diffusion Models with Image Conditions", "link": "http://arxiv.org/abs/2403.16627v1", "similarity": 0.515}, {"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.4863}, {"title": "Scaling Rectified Flow Transformers for High-Resolution Image Synthesis", "link": "http://arxiv.org/abs/2403.03206v1", "similarity": 0.4843}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20The%20Impact%20of%20Scanner%20Domain%20Shift%20on%20Deep%20Learning%20Performance%20in%0A%20%20Medical%20Imaging%3A%20an%20Experimental%20Study&body=Title%3A%20The%20Impact%20of%20Scanner%20Domain%20Shift%20on%20Deep%20Learning%20Performance%20in%0A%20%20Medical%20Imaging%3A%20an%20Experimental%20Study%0AAuthor%3A%20Brian%20Guo%20and%20Darui%20Lu%20and%20Gregory%20Szumel%20and%20Rongze%20Gui%20and%20Tingyu%20Wang%20and%20Nicholas%20Konz%20and%20Maciej%20A.%20Mazurowski%0AAbstract%3A%20%20%20Purpose%3A%20Medical%20images%20acquired%20using%20different%20scanners%20and%20protocols%20can%0Adiffer%20substantially%20in%20their%20appearance.%20This%20phenomenon%2C%20scanner%20domain%0Ashift%2C%20can%20result%20in%20a%20drop%20in%20the%20performance%20of%20deep%20neural%20networks%20which%0Aare%20trained%20on%20data%20acquired%20by%20one%20scanner%20and%20tested%20on%20another.%20This%0Asignificant%20practical%20issue%20is%20well-acknowledged%2C%20however%2C%20no%20systematic%20study%0Aof%20the%20issue%20is%20available%20across%20different%20modalities%20and%20diagnostic%20tasks.%0AMaterials%20and%20Methods%3A%20In%20this%20paper%2C%20we%20present%20a%20broad%20experimental%20study%0Aevaluating%20the%20impact%20of%20scanner%20domain%20shift%20on%20convolutional%20neural%20network%0Aperformance%20for%20different%20automated%20diagnostic%20tasks.%20We%20evaluate%20this%0Aphenomenon%20in%20common%20radiological%20modalities%2C%20including%20X-ray%2C%20CT%2C%20and%20MRI.%0AResults%3A%20We%20find%20that%20network%20performance%20on%20data%20from%20a%20different%20scanner%20is%0Aalmost%20always%20worse%20than%20on%20same-scanner%20data%2C%20and%20we%20quantify%20the%20degree%20of%0Aperformance%20drop%20across%20different%20datasets.%20Notably%2C%20we%20find%20that%20this%20drop%20is%0Amost%20severe%20for%20MRI%2C%20moderate%20for%20X-ray%2C%20and%20quite%20small%20for%20CT%2C%20on%20average%2C%0Awhich%20we%20attribute%20to%20the%20standardized%20nature%20of%20CT%20acquisition%20systems%20which%0Ais%20not%20present%20in%20MRI%20or%20X-ray.%20We%20also%20study%20how%20injecting%20varying%20amounts%20of%0Atarget%20domain%20data%20into%20the%20training%20set%2C%20as%20well%20as%20adding%20noise%20to%20the%0Atraining%20data%2C%20helps%20with%20generalization.%20Conclusion%3A%20Our%20results%20provide%0Aextensive%20experimental%20evidence%20and%20quantification%20of%20the%20extent%20of%20performance%0Adrop%20caused%20by%20scanner%20domain%20shift%20in%20deep%20learning%20across%20different%0Amodalities%2C%20with%20the%20goal%20of%20guiding%20the%20future%20development%20of%20robust%20deep%0Alearning%20models%20for%20medical%20image%20analysis.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2409.04368v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DThe%2520Impact%2520of%2520Scanner%2520Domain%2520Shift%2520on%2520Deep%2520Learning%2520Performance%2520in%250A%2520%2520Medical%2520Imaging%253A%2520an%2520Experimental%2520Study%26entry.906535625%3DBrian%2520Guo%2520and%2520Darui%2520Lu%2520and%2520Gregory%2520Szumel%2520and%2520Rongze%2520Gui%2520and%2520Tingyu%2520Wang%2520and%2520Nicholas%2520Konz%2520and%2520Maciej%2520A.%2520Mazurowski%26entry.1292438233%3D%2520%2520Purpose%253A%2520Medical%2520images%2520acquired%2520using%2520different%2520scanners%2520and%2520protocols%2520can%250Adiffer%2520substantially%2520in%2520their%2520appearance.%2520This%2520phenomenon%252C%2520scanner%2520domain%250Ashift%252C%2520can%2520result%2520in%2520a%2520drop%2520in%2520the%2520performance%2520of%2520deep%2520neural%2520networks%2520which%250Aare%2520trained%2520on%2520data%2520acquired%2520by%2520one%2520scanner%2520and%2520tested%2520on%2520another.%2520This%250Asignificant%2520practical%2520issue%2520is%2520well-acknowledged%252C%2520however%252C%2520no%2520systematic%2520study%250Aof%2520the%2520issue%2520is%2520available%2520across%2520different%2520modalities%2520and%2520diagnostic%2520tasks.%250AMaterials%2520and%2520Methods%253A%2520In%2520this%2520paper%252C%2520we%2520present%2520a%2520broad%2520experimental%2520study%250Aevaluating%2520the%2520impact%2520of%2520scanner%2520domain%2520shift%2520on%2520convolutional%2520neural%2520network%250Aperformance%2520for%2520different%2520automated%2520diagnostic%2520tasks.%2520We%2520evaluate%2520this%250Aphenomenon%2520in%2520common%2520radiological%2520modalities%252C%2520including%2520X-ray%252C%2520CT%252C%2520and%2520MRI.%250AResults%253A%2520We%2520find%2520that%2520network%2520performance%2520on%2520data%2520from%2520a%2520different%2520scanner%2520is%250Aalmost%2520always%2520worse%2520than%2520on%2520same-scanner%2520data%252C%2520and%2520we%2520quantify%2520the%2520degree%2520of%250Aperformance%2520drop%2520across%2520different%2520datasets.%2520Notably%252C%2520we%2520find%2520that%2520this%2520drop%2520is%250Amost%2520severe%2520for%2520MRI%252C%2520moderate%2520for%2520X-ray%252C%2520and%2520quite%2520small%2520for%2520CT%252C%2520on%2520average%252C%250Awhich%2520we%2520attribute%2520to%2520the%2520standardized%2520nature%2520of%2520CT%2520acquisition%2520systems%2520which%250Ais%2520not%2520present%2520in%2520MRI%2520or%2520X-ray.%2520We%2520also%2520study%2520how%2520injecting%2520varying%2520amounts%2520of%250Atarget%2520domain%2520data%2520into%2520the%2520training%2520set%252C%2520as%2520well%2520as%2520adding%2520noise%2520to%2520the%250Atraining%2520data%252C%2520helps%2520with%2520generalization.%2520Conclusion%253A%2520Our%2520results%2520provide%250Aextensive%2520experimental%2520evidence%2520and%2520quantification%2520of%2520the%2520extent%2520of%2520performance%250Adrop%2520caused%2520by%2520scanner%2520domain%2520shift%2520in%2520deep%2520learning%2520across%2520different%250Amodalities%252C%2520with%2520the%2520goal%2520of%2520guiding%2520the%2520future%2520development%2520of%2520robust%2520deep%250Alearning%2520models%2520for%2520medical%2520image%2520analysis.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2409.04368v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=The%20Impact%20of%20Scanner%20Domain%20Shift%20on%20Deep%20Learning%20Performance%20in%0A%20%20Medical%20Imaging%3A%20an%20Experimental%20Study&entry.906535625=Brian%20Guo%20and%20Darui%20Lu%20and%20Gregory%20Szumel%20and%20Rongze%20Gui%20and%20Tingyu%20Wang%20and%20Nicholas%20Konz%20and%20Maciej%20A.%20Mazurowski&entry.1292438233=%20%20Purpose%3A%20Medical%20images%20acquired%20using%20different%20scanners%20and%20protocols%20can%0Adiffer%20substantially%20in%20their%20appearance.%20This%20phenomenon%2C%20scanner%20domain%0Ashift%2C%20can%20result%20in%20a%20drop%20in%20the%20performance%20of%20deep%20neural%20networks%20which%0Aare%20trained%20on%20data%20acquired%20by%20one%20scanner%20and%20tested%20on%20another.%20This%0Asignificant%20practical%20issue%20is%20well-acknowledged%2C%20however%2C%20no%20systematic%20study%0Aof%20the%20issue%20is%20available%20across%20different%20modalities%20and%20diagnostic%20tasks.%0AMaterials%20and%20Methods%3A%20In%20this%20paper%2C%20we%20present%20a%20broad%20experimental%20study%0Aevaluating%20the%20impact%20of%20scanner%20domain%20shift%20on%20convolutional%20neural%20network%0Aperformance%20for%20different%20automated%20diagnostic%20tasks.%20We%20evaluate%20this%0Aphenomenon%20in%20common%20radiological%20modalities%2C%20including%20X-ray%2C%20CT%2C%20and%20MRI.%0AResults%3A%20We%20find%20that%20network%20performance%20on%20data%20from%20a%20different%20scanner%20is%0Aalmost%20always%20worse%20than%20on%20same-scanner%20data%2C%20and%20we%20quantify%20the%20degree%20of%0Aperformance%20drop%20across%20different%20datasets.%20Notably%2C%20we%20find%20that%20this%20drop%20is%0Amost%20severe%20for%20MRI%2C%20moderate%20for%20X-ray%2C%20and%20quite%20small%20for%20CT%2C%20on%20average%2C%0Awhich%20we%20attribute%20to%20the%20standardized%20nature%20of%20CT%20acquisition%20systems%20which%0Ais%20not%20present%20in%20MRI%20or%20X-ray.%20We%20also%20study%20how%20injecting%20varying%20amounts%20of%0Atarget%20domain%20data%20into%20the%20training%20set%2C%20as%20well%20as%20adding%20noise%20to%20the%0Atraining%20data%2C%20helps%20with%20generalization.%20Conclusion%3A%20Our%20results%20provide%0Aextensive%20experimental%20evidence%20and%20quantification%20of%20the%20extent%20of%20performance%0Adrop%20caused%20by%20scanner%20domain%20shift%20in%20deep%20learning%20across%20different%0Amodalities%2C%20with%20the%20goal%20of%20guiding%20the%20future%20development%20of%20robust%20deep%0Alearning%20models%20for%20medical%20image%20analysis.%0A&entry.1838667208=http%3A//arxiv.org/abs/2409.04368v2&entry.124074799=Read"},
{"title": "Imaging foundation model for universal enhancement of non-ideal\n  measurement CT", "author": "Yuxin Liu and Rongjun Ge and Yuting He and Zhan Wu and Chenyu You and Shuo Li and Yang Chen", "abstract": "  Non-ideal measurement computed tomography (NICT), which sacrifices optimal\nimaging standards for new advantages in CT imaging, is expanding the clinical\napplication scope of CT images. However, with the reduction of imaging\nstandards, the image quality has also been reduced, extremely limiting the\nclinical acceptability. Although numerous studies have demonstrated the\nfeasibility of deep learning for the NICT enhancement in specific scenarios,\ntheir high data cost and limited generalizability have become large obstacles.\nThe recent research on the foundation model has brought new opportunities for\nbuilding a universal NICT enhancement model - bridging the image quality\ndegradation with minimal data cost. However, owing to the challenges in the\ncollection of large pre-training datasets and the compatibility of data\nvariation, no success has been reported. In this paper, we propose a\nmulti-scale integrated Transformer AMPlifier (TAMP), the first imaging\nfoundation model for universal NICT enhancement. It has been pre-trained on a\nlarge-scale physical-driven simulation dataset with 3.6 million NICT-ICT image\npairs, and is able to directly generalize to the NICT enhancement tasks with\nvarious non-ideal settings and body regions. Via the adaptation with few data,\nit can further achieve professional performance in real-world specific\nscenarios. Our extensive experiments have demonstrated that the proposed TAMP\nhas significant potential for promoting the exploration and application of NICT\nand serving a wider range of medical scenarios.\n", "link": "http://arxiv.org/abs/2410.01591v1", "date": "2024-10-02", "relevancy": 1.7341, "topK": [{"title": "SDXS: Real-Time One-Step Latent Diffusion Models with Image Conditions", "link": "http://arxiv.org/abs/2403.16627v1", "similarity": 0.5841}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5782}, {"title": "Scaling Rectified Flow Transformers for High-Resolution Image Synthesis", "link": "http://arxiv.org/abs/2403.03206v1", "similarity": 0.5716}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Imaging%20foundation%20model%20for%20universal%20enhancement%20of%20non-ideal%0A%20%20measurement%20CT&body=Title%3A%20Imaging%20foundation%20model%20for%20universal%20enhancement%20of%20non-ideal%0A%20%20measurement%20CT%0AAuthor%3A%20Yuxin%20Liu%20and%20Rongjun%20Ge%20and%20Yuting%20He%20and%20Zhan%20Wu%20and%20Chenyu%20You%20and%20Shuo%20Li%20and%20Yang%20Chen%0AAbstract%3A%20%20%20Non-ideal%20measurement%20computed%20tomography%20%28NICT%29%2C%20which%20sacrifices%20optimal%0Aimaging%20standards%20for%20new%20advantages%20in%20CT%20imaging%2C%20is%20expanding%20the%20clinical%0Aapplication%20scope%20of%20CT%20images.%20However%2C%20with%20the%20reduction%20of%20imaging%0Astandards%2C%20the%20image%20quality%20has%20also%20been%20reduced%2C%20extremely%20limiting%20the%0Aclinical%20acceptability.%20Although%20numerous%20studies%20have%20demonstrated%20the%0Afeasibility%20of%20deep%20learning%20for%20the%20NICT%20enhancement%20in%20specific%20scenarios%2C%0Atheir%20high%20data%20cost%20and%20limited%20generalizability%20have%20become%20large%20obstacles.%0AThe%20recent%20research%20on%20the%20foundation%20model%20has%20brought%20new%20opportunities%20for%0Abuilding%20a%20universal%20NICT%20enhancement%20model%20-%20bridging%20the%20image%20quality%0Adegradation%20with%20minimal%20data%20cost.%20However%2C%20owing%20to%20the%20challenges%20in%20the%0Acollection%20of%20large%20pre-training%20datasets%20and%20the%20compatibility%20of%20data%0Avariation%2C%20no%20success%20has%20been%20reported.%20In%20this%20paper%2C%20we%20propose%20a%0Amulti-scale%20integrated%20Transformer%20AMPlifier%20%28TAMP%29%2C%20the%20first%20imaging%0Afoundation%20model%20for%20universal%20NICT%20enhancement.%20It%20has%20been%20pre-trained%20on%20a%0Alarge-scale%20physical-driven%20simulation%20dataset%20with%203.6%20million%20NICT-ICT%20image%0Apairs%2C%20and%20is%20able%20to%20directly%20generalize%20to%20the%20NICT%20enhancement%20tasks%20with%0Avarious%20non-ideal%20settings%20and%20body%20regions.%20Via%20the%20adaptation%20with%20few%20data%2C%0Ait%20can%20further%20achieve%20professional%20performance%20in%20real-world%20specific%0Ascenarios.%20Our%20extensive%20experiments%20have%20demonstrated%20that%20the%20proposed%20TAMP%0Ahas%20significant%20potential%20for%20promoting%20the%20exploration%20and%20application%20of%20NICT%0Aand%20serving%20a%20wider%20range%20of%20medical%20scenarios.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2410.01591v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DImaging%2520foundation%2520model%2520for%2520universal%2520enhancement%2520of%2520non-ideal%250A%2520%2520measurement%2520CT%26entry.906535625%3DYuxin%2520Liu%2520and%2520Rongjun%2520Ge%2520and%2520Yuting%2520He%2520and%2520Zhan%2520Wu%2520and%2520Chenyu%2520You%2520and%2520Shuo%2520Li%2520and%2520Yang%2520Chen%26entry.1292438233%3D%2520%2520Non-ideal%2520measurement%2520computed%2520tomography%2520%2528NICT%2529%252C%2520which%2520sacrifices%2520optimal%250Aimaging%2520standards%2520for%2520new%2520advantages%2520in%2520CT%2520imaging%252C%2520is%2520expanding%2520the%2520clinical%250Aapplication%2520scope%2520of%2520CT%2520images.%2520However%252C%2520with%2520the%2520reduction%2520of%2520imaging%250Astandards%252C%2520the%2520image%2520quality%2520has%2520also%2520been%2520reduced%252C%2520extremely%2520limiting%2520the%250Aclinical%2520acceptability.%2520Although%2520numerous%2520studies%2520have%2520demonstrated%2520the%250Afeasibility%2520of%2520deep%2520learning%2520for%2520the%2520NICT%2520enhancement%2520in%2520specific%2520scenarios%252C%250Atheir%2520high%2520data%2520cost%2520and%2520limited%2520generalizability%2520have%2520become%2520large%2520obstacles.%250AThe%2520recent%2520research%2520on%2520the%2520foundation%2520model%2520has%2520brought%2520new%2520opportunities%2520for%250Abuilding%2520a%2520universal%2520NICT%2520enhancement%2520model%2520-%2520bridging%2520the%2520image%2520quality%250Adegradation%2520with%2520minimal%2520data%2520cost.%2520However%252C%2520owing%2520to%2520the%2520challenges%2520in%2520the%250Acollection%2520of%2520large%2520pre-training%2520datasets%2520and%2520the%2520compatibility%2520of%2520data%250Avariation%252C%2520no%2520success%2520has%2520been%2520reported.%2520In%2520this%2520paper%252C%2520we%2520propose%2520a%250Amulti-scale%2520integrated%2520Transformer%2520AMPlifier%2520%2528TAMP%2529%252C%2520the%2520first%2520imaging%250Afoundation%2520model%2520for%2520universal%2520NICT%2520enhancement.%2520It%2520has%2520been%2520pre-trained%2520on%2520a%250Alarge-scale%2520physical-driven%2520simulation%2520dataset%2520with%25203.6%2520million%2520NICT-ICT%2520image%250Apairs%252C%2520and%2520is%2520able%2520to%2520directly%2520generalize%2520to%2520the%2520NICT%2520enhancement%2520tasks%2520with%250Avarious%2520non-ideal%2520settings%2520and%2520body%2520regions.%2520Via%2520the%2520adaptation%2520with%2520few%2520data%252C%250Ait%2520can%2520further%2520achieve%2520professional%2520performance%2520in%2520real-world%2520specific%250Ascenarios.%2520Our%2520extensive%2520experiments%2520have%2520demonstrated%2520that%2520the%2520proposed%2520TAMP%250Ahas%2520significant%2520potential%2520for%2520promoting%2520the%2520exploration%2520and%2520application%2520of%2520NICT%250Aand%2520serving%2520a%2520wider%2520range%2520of%2520medical%2520scenarios.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2410.01591v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Imaging%20foundation%20model%20for%20universal%20enhancement%20of%20non-ideal%0A%20%20measurement%20CT&entry.906535625=Yuxin%20Liu%20and%20Rongjun%20Ge%20and%20Yuting%20He%20and%20Zhan%20Wu%20and%20Chenyu%20You%20and%20Shuo%20Li%20and%20Yang%20Chen&entry.1292438233=%20%20Non-ideal%20measurement%20computed%20tomography%20%28NICT%29%2C%20which%20sacrifices%20optimal%0Aimaging%20standards%20for%20new%20advantages%20in%20CT%20imaging%2C%20is%20expanding%20the%20clinical%0Aapplication%20scope%20of%20CT%20images.%20However%2C%20with%20the%20reduction%20of%20imaging%0Astandards%2C%20the%20image%20quality%20has%20also%20been%20reduced%2C%20extremely%20limiting%20the%0Aclinical%20acceptability.%20Although%20numerous%20studies%20have%20demonstrated%20the%0Afeasibility%20of%20deep%20learning%20for%20the%20NICT%20enhancement%20in%20specific%20scenarios%2C%0Atheir%20high%20data%20cost%20and%20limited%20generalizability%20have%20become%20large%20obstacles.%0AThe%20recent%20research%20on%20the%20foundation%20model%20has%20brought%20new%20opportunities%20for%0Abuilding%20a%20universal%20NICT%20enhancement%20model%20-%20bridging%20the%20image%20quality%0Adegradation%20with%20minimal%20data%20cost.%20However%2C%20owing%20to%20the%20challenges%20in%20the%0Acollection%20of%20large%20pre-training%20datasets%20and%20the%20compatibility%20of%20data%0Avariation%2C%20no%20success%20has%20been%20reported.%20In%20this%20paper%2C%20we%20propose%20a%0Amulti-scale%20integrated%20Transformer%20AMPlifier%20%28TAMP%29%2C%20the%20first%20imaging%0Afoundation%20model%20for%20universal%20NICT%20enhancement.%20It%20has%20been%20pre-trained%20on%20a%0Alarge-scale%20physical-driven%20simulation%20dataset%20with%203.6%20million%20NICT-ICT%20image%0Apairs%2C%20and%20is%20able%20to%20directly%20generalize%20to%20the%20NICT%20enhancement%20tasks%20with%0Avarious%20non-ideal%20settings%20and%20body%20regions.%20Via%20the%20adaptation%20with%20few%20data%2C%0Ait%20can%20further%20achieve%20professional%20performance%20in%20real-world%20specific%0Ascenarios.%20Our%20extensive%20experiments%20have%20demonstrated%20that%20the%20proposed%20TAMP%0Ahas%20significant%20potential%20for%20promoting%20the%20exploration%20and%20application%20of%20NICT%0Aand%20serving%20a%20wider%20range%20of%20medical%20scenarios.%0A&entry.1838667208=http%3A//arxiv.org/abs/2410.01591v1&entry.124074799=Read"},
{"title": "$\\mathcal{D(R,O)}$ Grasp: A Unified Representation of Robot and Object\n  Interaction for Cross-Embodiment Dexterous Grasping", "author": "Zhenyu Wei and Zhixuan Xu and Jingxiang Guo and Yiwen Hou and Chongkai Gao and Zhehao Cai and Jiayu Luo and Lin Shao", "abstract": "  Dexterous grasping is a fundamental yet challenging skill in robotic\nmanipulation, requiring precise interaction between robotic hands and objects.\nIn this paper, we present $\\mathcal{D(R,O)}$ Grasp, a novel framework that\nmodels the interaction between the robotic hand in its grasping pose and the\nobject, enabling broad generalization across various robot hands and object\ngeometries. Our model takes the robot hand's description and object point cloud\nas inputs and efficiently predicts kinematically valid and stable grasps,\ndemonstrating strong adaptability to diverse robot embodiments and object\ngeometries. Extensive experiments conducted in both simulated and real-world\nenvironments validate the effectiveness of our approach, with significant\nimprovements in success rate, grasp diversity, and inference speed across\nmultiple robotic hands. Our method achieves an average success rate of 87.53%\nin simulation in less than one second, tested across three different dexterous\nrobotic hands. In real-world experiments using the LeapHand, the method also\ndemonstrates an average success rate of 89%. $\\mathcal{D(R,O)}$ Grasp provides\na robust solution for dexterous grasping in complex and varied environments.\nThe code, appendix, and videos are available on our project website at\nhttps://nus-lins-lab.github.io/drograspweb/.\n", "link": "http://arxiv.org/abs/2410.01702v1", "date": "2024-10-02", "relevancy": 1.9546, "topK": [{"title": "UGG: Unified Generative Grasping", "link": "https://arxiv.org/abs/2311.16917", "similarity": 0.7163}, {"title": "RoboEXP: Action-Conditioned Scene Graph via Interactive Exploration for\n  Robotic Manipulation", "link": "http://arxiv.org/abs/2402.15487v1", "similarity": 0.592}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.5491}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20%24%5Cmathcal%7BD%28R%2CO%29%7D%24%20Grasp%3A%20A%20Unified%20Representation%20of%20Robot%20and%20Object%0A%20%20Interaction%20for%20Cross-Embodiment%20Dexterous%20Grasping&body=Title%3A%20%24%5Cmathcal%7BD%28R%2CO%29%7D%24%20Grasp%3A%20A%20Unified%20Representation%20of%20Robot%20and%20Object%0A%20%20Interaction%20for%20Cross-Embodiment%20Dexterous%20Grasping%0AAuthor%3A%20Zhenyu%20Wei%20and%20Zhixuan%20Xu%20and%20Jingxiang%20Guo%20and%20Yiwen%20Hou%20and%20Chongkai%20Gao%20and%20Zhehao%20Cai%20and%20Jiayu%20Luo%20and%20Lin%20Shao%0AAbstract%3A%20%20%20Dexterous%20grasping%20is%20a%20fundamental%20yet%20challenging%20skill%20in%20robotic%0Amanipulation%2C%20requiring%20precise%20interaction%20between%20robotic%20hands%20and%20objects.%0AIn%20this%20paper%2C%20we%20present%20%24%5Cmathcal%7BD%28R%2CO%29%7D%24%20Grasp%2C%20a%20novel%20framework%20that%0Amodels%20the%20interaction%20between%20the%20robotic%20hand%20in%20its%20grasping%20pose%20and%20the%0Aobject%2C%20enabling%20broad%20generalization%20across%20various%20robot%20hands%20and%20object%0Ageometries.%20Our%20model%20takes%20the%20robot%20hand%27s%20description%20and%20object%20point%20cloud%0Aas%20inputs%20and%20efficiently%20predicts%20kinematically%20valid%20and%20stable%20grasps%2C%0Ademonstrating%20strong%20adaptability%20to%20diverse%20robot%20embodiments%20and%20object%0Ageometries.%20Extensive%20experiments%20conducted%20in%20both%20simulated%20and%20real-world%0Aenvironments%20validate%20the%20effectiveness%20of%20our%20approach%2C%20with%20significant%0Aimprovements%20in%20success%20rate%2C%20grasp%20diversity%2C%20and%20inference%20speed%20across%0Amultiple%20robotic%20hands.%20Our%20method%20achieves%20an%20average%20success%20rate%20of%2087.53%25%0Ain%20simulation%20in%20less%20than%20one%20second%2C%20tested%20across%20three%20different%20dexterous%0Arobotic%20hands.%20In%20real-world%20experiments%20using%20the%20LeapHand%2C%20the%20method%20also%0Ademonstrates%20an%20average%20success%20rate%20of%2089%25.%20%24%5Cmathcal%7BD%28R%2CO%29%7D%24%20Grasp%20provides%0Aa%20robust%20solution%20for%20dexterous%20grasping%20in%20complex%20and%20varied%20environments.%0AThe%20code%2C%20appendix%2C%20and%20videos%20are%20available%20on%20our%20project%20website%20at%0Ahttps%3A//nus-lins-lab.github.io/drograspweb/.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2410.01702v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3D%2524%255Cmathcal%257BD%2528R%252CO%2529%257D%2524%2520Grasp%253A%2520A%2520Unified%2520Representation%2520of%2520Robot%2520and%2520Object%250A%2520%2520Interaction%2520for%2520Cross-Embodiment%2520Dexterous%2520Grasping%26entry.906535625%3DZhenyu%2520Wei%2520and%2520Zhixuan%2520Xu%2520and%2520Jingxiang%2520Guo%2520and%2520Yiwen%2520Hou%2520and%2520Chongkai%2520Gao%2520and%2520Zhehao%2520Cai%2520and%2520Jiayu%2520Luo%2520and%2520Lin%2520Shao%26entry.1292438233%3D%2520%2520Dexterous%2520grasping%2520is%2520a%2520fundamental%2520yet%2520challenging%2520skill%2520in%2520robotic%250Amanipulation%252C%2520requiring%2520precise%2520interaction%2520between%2520robotic%2520hands%2520and%2520objects.%250AIn%2520this%2520paper%252C%2520we%2520present%2520%2524%255Cmathcal%257BD%2528R%252CO%2529%257D%2524%2520Grasp%252C%2520a%2520novel%2520framework%2520that%250Amodels%2520the%2520interaction%2520between%2520the%2520robotic%2520hand%2520in%2520its%2520grasping%2520pose%2520and%2520the%250Aobject%252C%2520enabling%2520broad%2520generalization%2520across%2520various%2520robot%2520hands%2520and%2520object%250Ageometries.%2520Our%2520model%2520takes%2520the%2520robot%2520hand%2527s%2520description%2520and%2520object%2520point%2520cloud%250Aas%2520inputs%2520and%2520efficiently%2520predicts%2520kinematically%2520valid%2520and%2520stable%2520grasps%252C%250Ademonstrating%2520strong%2520adaptability%2520to%2520diverse%2520robot%2520embodiments%2520and%2520object%250Ageometries.%2520Extensive%2520experiments%2520conducted%2520in%2520both%2520simulated%2520and%2520real-world%250Aenvironments%2520validate%2520the%2520effectiveness%2520of%2520our%2520approach%252C%2520with%2520significant%250Aimprovements%2520in%2520success%2520rate%252C%2520grasp%2520diversity%252C%2520and%2520inference%2520speed%2520across%250Amultiple%2520robotic%2520hands.%2520Our%2520method%2520achieves%2520an%2520average%2520success%2520rate%2520of%252087.53%2525%250Ain%2520simulation%2520in%2520less%2520than%2520one%2520second%252C%2520tested%2520across%2520three%2520different%2520dexterous%250Arobotic%2520hands.%2520In%2520real-world%2520experiments%2520using%2520the%2520LeapHand%252C%2520the%2520method%2520also%250Ademonstrates%2520an%2520average%2520success%2520rate%2520of%252089%2525.%2520%2524%255Cmathcal%257BD%2528R%252CO%2529%257D%2524%2520Grasp%2520provides%250Aa%2520robust%2520solution%2520for%2520dexterous%2520grasping%2520in%2520complex%2520and%2520varied%2520environments.%250AThe%2520code%252C%2520appendix%252C%2520and%2520videos%2520are%2520available%2520on%2520our%2520project%2520website%2520at%250Ahttps%253A//nus-lins-lab.github.io/drograspweb/.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2410.01702v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=%24%5Cmathcal%7BD%28R%2CO%29%7D%24%20Grasp%3A%20A%20Unified%20Representation%20of%20Robot%20and%20Object%0A%20%20Interaction%20for%20Cross-Embodiment%20Dexterous%20Grasping&entry.906535625=Zhenyu%20Wei%20and%20Zhixuan%20Xu%20and%20Jingxiang%20Guo%20and%20Yiwen%20Hou%20and%20Chongkai%20Gao%20and%20Zhehao%20Cai%20and%20Jiayu%20Luo%20and%20Lin%20Shao&entry.1292438233=%20%20Dexterous%20grasping%20is%20a%20fundamental%20yet%20challenging%20skill%20in%20robotic%0Amanipulation%2C%20requiring%20precise%20interaction%20between%20robotic%20hands%20and%20objects.%0AIn%20this%20paper%2C%20we%20present%20%24%5Cmathcal%7BD%28R%2CO%29%7D%24%20Grasp%2C%20a%20novel%20framework%20that%0Amodels%20the%20interaction%20between%20the%20robotic%20hand%20in%20its%20grasping%20pose%20and%20the%0Aobject%2C%20enabling%20broad%20generalization%20across%20various%20robot%20hands%20and%20object%0Ageometries.%20Our%20model%20takes%20the%20robot%20hand%27s%20description%20and%20object%20point%20cloud%0Aas%20inputs%20and%20efficiently%20predicts%20kinematically%20valid%20and%20stable%20grasps%2C%0Ademonstrating%20strong%20adaptability%20to%20diverse%20robot%20embodiments%20and%20object%0Ageometries.%20Extensive%20experiments%20conducted%20in%20both%20simulated%20and%20real-world%0Aenvironments%20validate%20the%20effectiveness%20of%20our%20approach%2C%20with%20significant%0Aimprovements%20in%20success%20rate%2C%20grasp%20diversity%2C%20and%20inference%20speed%20across%0Amultiple%20robotic%20hands.%20Our%20method%20achieves%20an%20average%20success%20rate%20of%2087.53%25%0Ain%20simulation%20in%20less%20than%20one%20second%2C%20tested%20across%20three%20different%20dexterous%0Arobotic%20hands.%20In%20real-world%20experiments%20using%20the%20LeapHand%2C%20the%20method%20also%0Ademonstrates%20an%20average%20success%20rate%20of%2089%25.%20%24%5Cmathcal%7BD%28R%2CO%29%7D%24%20Grasp%20provides%0Aa%20robust%20solution%20for%20dexterous%20grasping%20in%20complex%20and%20varied%20environments.%0AThe%20code%2C%20appendix%2C%20and%20videos%20are%20available%20on%20our%20project%20website%20at%0Ahttps%3A//nus-lins-lab.github.io/drograspweb/.%0A&entry.1838667208=http%3A//arxiv.org/abs/2410.01702v1&entry.124074799=Read"},
      ];
      const content = document.getElementById('content');
      function createPostElement(post) {
        const postElement = document.createElement('div');
        postElement.className = 'post';
        const dateElem = document.createElement('p');
        dateElem.setAttribute("class", "date");
        dateElem.textContent = post.date;
        postElement.appendChild(dateElem);

        const textElem = document.createElement('p');
        textElem.setAttribute("class", "text");
        const titleElem = document.createElement('p');
        titleElem.setAttribute("class", "title");
        titleElem.textContent = post.title;
        textElem.appendChild(titleElem);
        const authorElem = document.createElement('p');
        authorElem.setAttribute("class", "author");
        authorElem.textContent = post.author;
        textElem.appendChild(authorElem);
        const abstractElem = document.createElement('p');
        abstractElem.setAttribute("class", "abstract");
        abstractElem.textContent = post.abstract;
        textElem.appendChild(abstractElem);

        const linkElement = document.createElement('a');
        linkElement.setAttribute("class", "link");
        linkElement.href = post.link;
        linkElement.target = "_blank";
        linkElement.textContent = post.link.length > 50 ? post.link.substring(0, 50) + '...' : post.link;
        textElem.appendChild(linkElement);
        postElement.appendChild(textElem);

        const linkElementContainer = document.createElement('div');
        linkElementContainer.setAttribute("class", "comment");
        const actionElement = document.createElement('a');
        actionElement.setAttribute("class", "comment");
        actionElement.href = post.form;
        actionElement.textContent = "Action";
        actionElement.target = "_blank";
        linkElementContainer.appendChild(actionElement);
        const emailElement = document.createElement('a');
        emailElement.setAttribute("class", "comment");
        emailElement.href = post.mailto;
        emailElement.textContent = "Email";
        emailElement.target = "_blank";
        linkElementContainer.appendChild(emailElement);
        postElement.appendChild(linkElementContainer);
        const e = document.createElement('div');
        e.setAttribute("class", "clear");
        postElement.appendChild(e);

        const relevancyContainer = document.createElement('div');
        const relevancyValElem = document.createElement('p');
        relevancyValElem.textContent = "Relevancy " + post.relevancy;
        relevancyContainer.appendChild(relevancyValElem);
        post.topK.forEach((sub) => {
          const topKElem = document.createElement('a');
          topKElem.setAttribute("class", "topK");
          topKElem.href = sub.link;
          topKElem.textContent = sub.title + " (" + sub.similarity + ")";
          topKElem.target = "_blank";
          relevancyContainer.appendChild(topKElem);
        });
        postElement.appendChild(relevancyContainer);
        return postElement;
      }
      function loadPosts() {
        // Simulate loading more posts
        posts.forEach((post) => {
          const postElement = createPostElement(post);
          content.appendChild(postElement);
        });
      }
      // Load initial posts
      loadPosts();
    </script>

  </body>
</html>


