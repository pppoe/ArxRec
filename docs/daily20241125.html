<!DOCTYPE html>
<html lang="en">
  <head>
    <script async src="https://www.googletagmanager.com/gtag/js?id=G-V34CNNDP8V"></script>
    <script>
      window.dataLayer = window.dataLayer || [];
      function gtag(){dataLayer.push(arguments);}
      gtag('js', new Date());
      gtag('config', 'G-V34CNNDP8V');
    </script>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Arxiv Paper Selection</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.3/css/all.min.css" crossorigin="anonymous" referrerpolicy="no-referrer" />
    <style>
    body {
      font-family: Arial, sans-serif;
      margin: 0;
      padding: 0;
      background-color: #f4f4f4;
    }
    header {
      position: fixed;
      top: 0;
      left: 0;
      width: 100%;
      background-color: #ffffff;
      color: black;
      padding: 10px;
      text-align: center;
      z-index: 1000;
      border-bottom: 1px solid #ddd;
    }
    header div {
      display: block;
      margin: 10px auto;
    }

    #home-icon {
      display: block;
      float: left;
      margin: 5px;
      text-decoration: none;
      color: black;
    }

    main {
      margin-top: 60px; /* Adjusted margin to account for fixed header */
      padding: 20px;
    }

    .post {
      background-color: white;
      border: 1px solid #ddd;
      border-radius: 5px;
      margin-bottom: 10px;
      padding: 10px 20px;
      max-height: 2000px;
      overflow: scroll;
    }
    .post img {
      display: block;
      margin-top: 5px;
      max-width: auto;
      max-height: 100px;
    }
    .post .clear {
      clear: both;
      display: block;
    }
    .post a {
      text-decoration: none;
    }
    .post a:hover {
      color: #0056b3;
    }
    .post a:visited {
      color: #0056b3;
    }
    .post div.comment {
      text-align: right;
    }
    .post div.comment a {
      margin: 1em;
    }
    .post .text {
      margin: 1em 0em;
      padding: 0;
    }
    .post .text .title {
    }
    .post .text .author {
    }
    .post .text .abstract {
    }
    .post .topK {
      display: block;
      margin: 0.5em;
    }
    .post .date {
      margin: 0;
      padding: 0;
      text-size: small; 
      color: gray;
    }
    .post .link {
      margin: 0;
      padding: 0;
    }
    @media screen and (max-width: 600px) {
      body {
        max-width: 100%; 
      }
      #home-icon {
        float: none;
        display: block;
        text-align: center;
        margin-bottom: 10px;
      }
    }
    footer {
      width: 100%;
      background-color: #ddd;
      text-align: center;
      z-index: 1000;
      padding: 20px 0px;
      margin-bottom: 20px;
      left: 0;
    }

    #next-btn,
    #prev-btn {
      background-color: #4CAF50;
      color: white;
      padding: 8px 16px;
      margin: 0 50px;
      border: none;
      border-radius: 4px;
      cursor: pointer;
    }

    .links {
      padding: 20px;
    }
    .links a {
      text-decoration: none;
    }
    .links a:hover {
      color: #0056b3;
    }
    .links a:visited {
      color: #0056b3;
    }

    #page-index {
      font-size: small;
    }
    .ads {
      width: 100%;
    }
    #prev-btn {
      background-color: #4CAF50;
      color: white;
      padding: 8px 16px;
      margin: 0 50px;
      border: none;
      border-radius: 4px;
      cursor: pointer;
    }
    </style>
  </head>
  <body>

    <header>
      <a id="prev-btn" href="daily20241124.html"><i class="fas fa-chevron-left"></i></a>
      <a href="https://haoxiang.org/">About</a>
    </header>

    <main id="content">
      <!-- Posts will be dynamically added here using JavaScript -->
    </main>

    <script>
      // Dummy data for posts
      const posts = [
{"title": "MaGS: Reconstructing and Simulating Dynamic 3D Objects with\n  Mesh-adsorbed Gaussian Splatting", "author": "Shaojie Ma and Yawei Luo and Wei Yang and Yi Yang", "abstract": "  3D reconstruction and simulation, although interrelated, have distinct\nobjectives: reconstruction requires a flexible 3D representation that can adapt\nto diverse scenes, while simulation needs a structured representation to model\nmotion principles effectively. This paper introduces the Mesh-adsorbed Gaussian\nSplatting (MaGS) method to address this challenge. MaGS constrains 3D Gaussians\nto roam near the mesh, creating a mutually adsorbed mesh-Gaussian 3D\nrepresentation. Such representation harnesses both the rendering flexibility of\n3D Gaussians and the structured property of meshes. To achieve this, we\nintroduce RMD-Net, a network that learns motion priors from video data to\nrefine mesh deformations, alongside RGD-Net, which models the relative\ndisplacement between the mesh and Gaussians to enhance rendering fidelity under\nmesh constraints. To generalize to novel, user-defined deformations beyond\ninput video without reliance on temporal data, we propose MPE-Net, which\nleverages inherent mesh information to bootstrap RMD-Net and RGD-Net. Due to\nthe universality of meshes, MaGS is compatible with various deformation priors\nsuch as ARAP, SMPL, and soft physics simulation. Extensive experiments on the\nD-NeRF, DG-Mesh, and PeopleSnapshot datasets demonstrate that MaGS achieves\nstate-of-the-art performance in both reconstruction and simulation.\n", "link": "http://arxiv.org/abs/2406.01593v2", "date": "2024-11-22", "relevancy": 3.6699, "topK": [{"title": "Dynamic Gaussians Mesh: Consistent Mesh Reconstruction from Monocular\n  Videos", "link": "http://arxiv.org/abs/2404.12379v2", "similarity": 0.789}, {"title": "GST: Precise 3D Human Body from a Single Image with Gaussian Splatting\n  Transformers", "link": "http://arxiv.org/abs/2409.04196v1", "similarity": 0.7101}, {"title": "RTG-SLAM: Real-time 3D Reconstruction at Scale using Gaussian Splatting", "link": "http://arxiv.org/abs/2404.19706v1", "similarity": 0.7028}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20MaGS%3A%20Reconstructing%20and%20Simulating%20Dynamic%203D%20Objects%20with%0A%20%20Mesh-adsorbed%20Gaussian%20Splatting&body=Title%3A%20MaGS%3A%20Reconstructing%20and%20Simulating%20Dynamic%203D%20Objects%20with%0A%20%20Mesh-adsorbed%20Gaussian%20Splatting%0AAuthor%3A%20Shaojie%20Ma%20and%20Yawei%20Luo%20and%20Wei%20Yang%20and%20Yi%20Yang%0AAbstract%3A%20%20%203D%20reconstruction%20and%20simulation%2C%20although%20interrelated%2C%20have%20distinct%0Aobjectives%3A%20reconstruction%20requires%20a%20flexible%203D%20representation%20that%20can%20adapt%0Ato%20diverse%20scenes%2C%20while%20simulation%20needs%20a%20structured%20representation%20to%20model%0Amotion%20principles%20effectively.%20This%20paper%20introduces%20the%20Mesh-adsorbed%20Gaussian%0ASplatting%20%28MaGS%29%20method%20to%20address%20this%20challenge.%20MaGS%20constrains%203D%20Gaussians%0Ato%20roam%20near%20the%20mesh%2C%20creating%20a%20mutually%20adsorbed%20mesh-Gaussian%203D%0Arepresentation.%20Such%20representation%20harnesses%20both%20the%20rendering%20flexibility%20of%0A3D%20Gaussians%20and%20the%20structured%20property%20of%20meshes.%20To%20achieve%20this%2C%20we%0Aintroduce%20RMD-Net%2C%20a%20network%20that%20learns%20motion%20priors%20from%20video%20data%20to%0Arefine%20mesh%20deformations%2C%20alongside%20RGD-Net%2C%20which%20models%20the%20relative%0Adisplacement%20between%20the%20mesh%20and%20Gaussians%20to%20enhance%20rendering%20fidelity%20under%0Amesh%20constraints.%20To%20generalize%20to%20novel%2C%20user-defined%20deformations%20beyond%0Ainput%20video%20without%20reliance%20on%20temporal%20data%2C%20we%20propose%20MPE-Net%2C%20which%0Aleverages%20inherent%20mesh%20information%20to%20bootstrap%20RMD-Net%20and%20RGD-Net.%20Due%20to%0Athe%20universality%20of%20meshes%2C%20MaGS%20is%20compatible%20with%20various%20deformation%20priors%0Asuch%20as%20ARAP%2C%20SMPL%2C%20and%20soft%20physics%20simulation.%20Extensive%20experiments%20on%20the%0AD-NeRF%2C%20DG-Mesh%2C%20and%20PeopleSnapshot%20datasets%20demonstrate%20that%20MaGS%20achieves%0Astate-of-the-art%20performance%20in%20both%20reconstruction%20and%20simulation.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2406.01593v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DMaGS%253A%2520Reconstructing%2520and%2520Simulating%2520Dynamic%25203D%2520Objects%2520with%250A%2520%2520Mesh-adsorbed%2520Gaussian%2520Splatting%26entry.906535625%3DShaojie%2520Ma%2520and%2520Yawei%2520Luo%2520and%2520Wei%2520Yang%2520and%2520Yi%2520Yang%26entry.1292438233%3D%2520%25203D%2520reconstruction%2520and%2520simulation%252C%2520although%2520interrelated%252C%2520have%2520distinct%250Aobjectives%253A%2520reconstruction%2520requires%2520a%2520flexible%25203D%2520representation%2520that%2520can%2520adapt%250Ato%2520diverse%2520scenes%252C%2520while%2520simulation%2520needs%2520a%2520structured%2520representation%2520to%2520model%250Amotion%2520principles%2520effectively.%2520This%2520paper%2520introduces%2520the%2520Mesh-adsorbed%2520Gaussian%250ASplatting%2520%2528MaGS%2529%2520method%2520to%2520address%2520this%2520challenge.%2520MaGS%2520constrains%25203D%2520Gaussians%250Ato%2520roam%2520near%2520the%2520mesh%252C%2520creating%2520a%2520mutually%2520adsorbed%2520mesh-Gaussian%25203D%250Arepresentation.%2520Such%2520representation%2520harnesses%2520both%2520the%2520rendering%2520flexibility%2520of%250A3D%2520Gaussians%2520and%2520the%2520structured%2520property%2520of%2520meshes.%2520To%2520achieve%2520this%252C%2520we%250Aintroduce%2520RMD-Net%252C%2520a%2520network%2520that%2520learns%2520motion%2520priors%2520from%2520video%2520data%2520to%250Arefine%2520mesh%2520deformations%252C%2520alongside%2520RGD-Net%252C%2520which%2520models%2520the%2520relative%250Adisplacement%2520between%2520the%2520mesh%2520and%2520Gaussians%2520to%2520enhance%2520rendering%2520fidelity%2520under%250Amesh%2520constraints.%2520To%2520generalize%2520to%2520novel%252C%2520user-defined%2520deformations%2520beyond%250Ainput%2520video%2520without%2520reliance%2520on%2520temporal%2520data%252C%2520we%2520propose%2520MPE-Net%252C%2520which%250Aleverages%2520inherent%2520mesh%2520information%2520to%2520bootstrap%2520RMD-Net%2520and%2520RGD-Net.%2520Due%2520to%250Athe%2520universality%2520of%2520meshes%252C%2520MaGS%2520is%2520compatible%2520with%2520various%2520deformation%2520priors%250Asuch%2520as%2520ARAP%252C%2520SMPL%252C%2520and%2520soft%2520physics%2520simulation.%2520Extensive%2520experiments%2520on%2520the%250AD-NeRF%252C%2520DG-Mesh%252C%2520and%2520PeopleSnapshot%2520datasets%2520demonstrate%2520that%2520MaGS%2520achieves%250Astate-of-the-art%2520performance%2520in%2520both%2520reconstruction%2520and%2520simulation.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2406.01593v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=MaGS%3A%20Reconstructing%20and%20Simulating%20Dynamic%203D%20Objects%20with%0A%20%20Mesh-adsorbed%20Gaussian%20Splatting&entry.906535625=Shaojie%20Ma%20and%20Yawei%20Luo%20and%20Wei%20Yang%20and%20Yi%20Yang&entry.1292438233=%20%203D%20reconstruction%20and%20simulation%2C%20although%20interrelated%2C%20have%20distinct%0Aobjectives%3A%20reconstruction%20requires%20a%20flexible%203D%20representation%20that%20can%20adapt%0Ato%20diverse%20scenes%2C%20while%20simulation%20needs%20a%20structured%20representation%20to%20model%0Amotion%20principles%20effectively.%20This%20paper%20introduces%20the%20Mesh-adsorbed%20Gaussian%0ASplatting%20%28MaGS%29%20method%20to%20address%20this%20challenge.%20MaGS%20constrains%203D%20Gaussians%0Ato%20roam%20near%20the%20mesh%2C%20creating%20a%20mutually%20adsorbed%20mesh-Gaussian%203D%0Arepresentation.%20Such%20representation%20harnesses%20both%20the%20rendering%20flexibility%20of%0A3D%20Gaussians%20and%20the%20structured%20property%20of%20meshes.%20To%20achieve%20this%2C%20we%0Aintroduce%20RMD-Net%2C%20a%20network%20that%20learns%20motion%20priors%20from%20video%20data%20to%0Arefine%20mesh%20deformations%2C%20alongside%20RGD-Net%2C%20which%20models%20the%20relative%0Adisplacement%20between%20the%20mesh%20and%20Gaussians%20to%20enhance%20rendering%20fidelity%20under%0Amesh%20constraints.%20To%20generalize%20to%20novel%2C%20user-defined%20deformations%20beyond%0Ainput%20video%20without%20reliance%20on%20temporal%20data%2C%20we%20propose%20MPE-Net%2C%20which%0Aleverages%20inherent%20mesh%20information%20to%20bootstrap%20RMD-Net%20and%20RGD-Net.%20Due%20to%0Athe%20universality%20of%20meshes%2C%20MaGS%20is%20compatible%20with%20various%20deformation%20priors%0Asuch%20as%20ARAP%2C%20SMPL%2C%20and%20soft%20physics%20simulation.%20Extensive%20experiments%20on%20the%0AD-NeRF%2C%20DG-Mesh%2C%20and%20PeopleSnapshot%20datasets%20demonstrate%20that%20MaGS%20achieves%0Astate-of-the-art%20performance%20in%20both%20reconstruction%20and%20simulation.%0A&entry.1838667208=http%3A//arxiv.org/abs/2406.01593v2&entry.124074799=Read"},
{"title": "Dynamics-Aware Gaussian Splatting Streaming Towards Fast On-the-Fly\n  Training for 4D Reconstruction", "author": "Zhening Liu and Yingdong Hu and Xinjie Zhang and Jiawei Shao and Zehong Lin and Jun Zhang", "abstract": "  The recent development of 3D Gaussian Splatting (3DGS) has led to great\ninterest in 4D dynamic spatial reconstruction from multi-view visual inputs.\nWhile existing approaches mainly rely on processing full-length multi-view\nvideos for 4D reconstruction, there has been limited exploration of iterative\nonline reconstruction methods that enable on-the-fly training and per-frame\nstreaming. Current 3DGS-based streaming methods treat the Gaussian primitives\nuniformly and constantly renew the densified Gaussians, thereby overlooking the\ndifference between dynamic and static features and also neglecting the temporal\ncontinuity in the scene. To address these limitations, we propose a novel\nthree-stage pipeline for iterative streamable 4D dynamic spatial\nreconstruction. Our pipeline comprises a selective inheritance stage to\npreserve temporal continuity, a dynamics-aware shift stage for distinguishing\ndynamic and static primitives and optimizing their movements, and an\nerror-guided densification stage to accommodate emerging objects. Our method\nachieves state-of-the-art performance in online 4D reconstruction,\ndemonstrating a 20% improvement in on-the-fly training speed, superior\nrepresentation quality, and real-time rendering capability. Project page:\nhttps://www.liuzhening.top/DASS\n", "link": "http://arxiv.org/abs/2411.14847v1", "date": "2024-11-22", "relevancy": 3.4813, "topK": [{"title": "Dynamic Gaussians Mesh: Consistent Mesh Reconstruction from Monocular\n  Videos", "link": "http://arxiv.org/abs/2404.12379v2", "similarity": 0.7233}, {"title": "RTG-SLAM: Real-time 3D Reconstruction at Scale using Gaussian Splatting", "link": "http://arxiv.org/abs/2404.19706v1", "similarity": 0.7166}, {"title": "GST: Precise 3D Human Body from a Single Image with Gaussian Splatting\n  Transformers", "link": "http://arxiv.org/abs/2409.04196v1", "similarity": 0.6489}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Dynamics-Aware%20Gaussian%20Splatting%20Streaming%20Towards%20Fast%20On-the-Fly%0A%20%20Training%20for%204D%20Reconstruction&body=Title%3A%20Dynamics-Aware%20Gaussian%20Splatting%20Streaming%20Towards%20Fast%20On-the-Fly%0A%20%20Training%20for%204D%20Reconstruction%0AAuthor%3A%20Zhening%20Liu%20and%20Yingdong%20Hu%20and%20Xinjie%20Zhang%20and%20Jiawei%20Shao%20and%20Zehong%20Lin%20and%20Jun%20Zhang%0AAbstract%3A%20%20%20The%20recent%20development%20of%203D%20Gaussian%20Splatting%20%283DGS%29%20has%20led%20to%20great%0Ainterest%20in%204D%20dynamic%20spatial%20reconstruction%20from%20multi-view%20visual%20inputs.%0AWhile%20existing%20approaches%20mainly%20rely%20on%20processing%20full-length%20multi-view%0Avideos%20for%204D%20reconstruction%2C%20there%20has%20been%20limited%20exploration%20of%20iterative%0Aonline%20reconstruction%20methods%20that%20enable%20on-the-fly%20training%20and%20per-frame%0Astreaming.%20Current%203DGS-based%20streaming%20methods%20treat%20the%20Gaussian%20primitives%0Auniformly%20and%20constantly%20renew%20the%20densified%20Gaussians%2C%20thereby%20overlooking%20the%0Adifference%20between%20dynamic%20and%20static%20features%20and%20also%20neglecting%20the%20temporal%0Acontinuity%20in%20the%20scene.%20To%20address%20these%20limitations%2C%20we%20propose%20a%20novel%0Athree-stage%20pipeline%20for%20iterative%20streamable%204D%20dynamic%20spatial%0Areconstruction.%20Our%20pipeline%20comprises%20a%20selective%20inheritance%20stage%20to%0Apreserve%20temporal%20continuity%2C%20a%20dynamics-aware%20shift%20stage%20for%20distinguishing%0Adynamic%20and%20static%20primitives%20and%20optimizing%20their%20movements%2C%20and%20an%0Aerror-guided%20densification%20stage%20to%20accommodate%20emerging%20objects.%20Our%20method%0Aachieves%20state-of-the-art%20performance%20in%20online%204D%20reconstruction%2C%0Ademonstrating%20a%2020%25%20improvement%20in%20on-the-fly%20training%20speed%2C%20superior%0Arepresentation%20quality%2C%20and%20real-time%20rendering%20capability.%20Project%20page%3A%0Ahttps%3A//www.liuzhening.top/DASS%0A%0ALink%3A%20http%3A//arxiv.org/abs/2411.14847v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DDynamics-Aware%2520Gaussian%2520Splatting%2520Streaming%2520Towards%2520Fast%2520On-the-Fly%250A%2520%2520Training%2520for%25204D%2520Reconstruction%26entry.906535625%3DZhening%2520Liu%2520and%2520Yingdong%2520Hu%2520and%2520Xinjie%2520Zhang%2520and%2520Jiawei%2520Shao%2520and%2520Zehong%2520Lin%2520and%2520Jun%2520Zhang%26entry.1292438233%3D%2520%2520The%2520recent%2520development%2520of%25203D%2520Gaussian%2520Splatting%2520%25283DGS%2529%2520has%2520led%2520to%2520great%250Ainterest%2520in%25204D%2520dynamic%2520spatial%2520reconstruction%2520from%2520multi-view%2520visual%2520inputs.%250AWhile%2520existing%2520approaches%2520mainly%2520rely%2520on%2520processing%2520full-length%2520multi-view%250Avideos%2520for%25204D%2520reconstruction%252C%2520there%2520has%2520been%2520limited%2520exploration%2520of%2520iterative%250Aonline%2520reconstruction%2520methods%2520that%2520enable%2520on-the-fly%2520training%2520and%2520per-frame%250Astreaming.%2520Current%25203DGS-based%2520streaming%2520methods%2520treat%2520the%2520Gaussian%2520primitives%250Auniformly%2520and%2520constantly%2520renew%2520the%2520densified%2520Gaussians%252C%2520thereby%2520overlooking%2520the%250Adifference%2520between%2520dynamic%2520and%2520static%2520features%2520and%2520also%2520neglecting%2520the%2520temporal%250Acontinuity%2520in%2520the%2520scene.%2520To%2520address%2520these%2520limitations%252C%2520we%2520propose%2520a%2520novel%250Athree-stage%2520pipeline%2520for%2520iterative%2520streamable%25204D%2520dynamic%2520spatial%250Areconstruction.%2520Our%2520pipeline%2520comprises%2520a%2520selective%2520inheritance%2520stage%2520to%250Apreserve%2520temporal%2520continuity%252C%2520a%2520dynamics-aware%2520shift%2520stage%2520for%2520distinguishing%250Adynamic%2520and%2520static%2520primitives%2520and%2520optimizing%2520their%2520movements%252C%2520and%2520an%250Aerror-guided%2520densification%2520stage%2520to%2520accommodate%2520emerging%2520objects.%2520Our%2520method%250Aachieves%2520state-of-the-art%2520performance%2520in%2520online%25204D%2520reconstruction%252C%250Ademonstrating%2520a%252020%2525%2520improvement%2520in%2520on-the-fly%2520training%2520speed%252C%2520superior%250Arepresentation%2520quality%252C%2520and%2520real-time%2520rendering%2520capability.%2520Project%2520page%253A%250Ahttps%253A//www.liuzhening.top/DASS%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2411.14847v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Dynamics-Aware%20Gaussian%20Splatting%20Streaming%20Towards%20Fast%20On-the-Fly%0A%20%20Training%20for%204D%20Reconstruction&entry.906535625=Zhening%20Liu%20and%20Yingdong%20Hu%20and%20Xinjie%20Zhang%20and%20Jiawei%20Shao%20and%20Zehong%20Lin%20and%20Jun%20Zhang&entry.1292438233=%20%20The%20recent%20development%20of%203D%20Gaussian%20Splatting%20%283DGS%29%20has%20led%20to%20great%0Ainterest%20in%204D%20dynamic%20spatial%20reconstruction%20from%20multi-view%20visual%20inputs.%0AWhile%20existing%20approaches%20mainly%20rely%20on%20processing%20full-length%20multi-view%0Avideos%20for%204D%20reconstruction%2C%20there%20has%20been%20limited%20exploration%20of%20iterative%0Aonline%20reconstruction%20methods%20that%20enable%20on-the-fly%20training%20and%20per-frame%0Astreaming.%20Current%203DGS-based%20streaming%20methods%20treat%20the%20Gaussian%20primitives%0Auniformly%20and%20constantly%20renew%20the%20densified%20Gaussians%2C%20thereby%20overlooking%20the%0Adifference%20between%20dynamic%20and%20static%20features%20and%20also%20neglecting%20the%20temporal%0Acontinuity%20in%20the%20scene.%20To%20address%20these%20limitations%2C%20we%20propose%20a%20novel%0Athree-stage%20pipeline%20for%20iterative%20streamable%204D%20dynamic%20spatial%0Areconstruction.%20Our%20pipeline%20comprises%20a%20selective%20inheritance%20stage%20to%0Apreserve%20temporal%20continuity%2C%20a%20dynamics-aware%20shift%20stage%20for%20distinguishing%0Adynamic%20and%20static%20primitives%20and%20optimizing%20their%20movements%2C%20and%20an%0Aerror-guided%20densification%20stage%20to%20accommodate%20emerging%20objects.%20Our%20method%0Aachieves%20state-of-the-art%20performance%20in%20online%204D%20reconstruction%2C%0Ademonstrating%20a%2020%25%20improvement%20in%20on-the-fly%20training%20speed%2C%20superior%0Arepresentation%20quality%2C%20and%20real-time%20rendering%20capability.%20Project%20page%3A%0Ahttps%3A//www.liuzhening.top/DASS%0A&entry.1838667208=http%3A//arxiv.org/abs/2411.14847v1&entry.124074799=Read"},
{"title": "3D Convex Splatting: Radiance Field Rendering with 3D Smooth Convexes", "author": "Jan Held and Renaud Vandeghen and Abdullah Hamdi and Adrien Deliege and Anthony Cioppa and Silvio Giancola and Andrea Vedaldi and Bernard Ghanem and Marc Van Droogenbroeck", "abstract": "  Recent advances in radiance field reconstruction, such as 3D Gaussian\nSplatting (3DGS), have achieved high-quality novel view synthesis and fast\nrendering by representing scenes with compositions of Gaussian primitives.\nHowever, 3D Gaussians present several limitations for scene reconstruction.\nAccurately capturing hard edges is challenging without significantly increasing\nthe number of Gaussians, creating a large memory footprint. Moreover, they\nstruggle to represent flat surfaces, as they are diffused in space. Without\nhand-crafted regularizers, they tend to disperse irregularly around the actual\nsurface. To circumvent these issues, we introduce a novel method, named 3D\nConvex Splatting (3DCS), which leverages 3D smooth convexes as primitives for\nmodeling geometrically-meaningful radiance fields from multi-view images.\nSmooth convex shapes offer greater flexibility than Gaussians, allowing for a\nbetter representation of 3D scenes with hard edges and dense volumes using\nfewer primitives. Powered by our efficient CUDA-based rasterizer, 3DCS achieves\nsuperior performance over 3DGS on benchmarks such as Mip-NeRF360, Tanks and\nTemples, and Deep Blending. Specifically, our method attains an improvement of\nup to 0.81 in PSNR and 0.026 in LPIPS compared to 3DGS while maintaining high\nrendering speeds and reducing the number of required primitives. Our results\nhighlight the potential of 3D Convex Splatting to become the new standard for\nhigh-quality scene reconstruction and novel view synthesis. Project page:\nwww.convexsplatting.com.\n", "link": "http://arxiv.org/abs/2411.14974v1", "date": "2024-11-22", "relevancy": 3.3502, "topK": [{"title": "RTG-SLAM: Real-time 3D Reconstruction at Scale using Gaussian Splatting", "link": "http://arxiv.org/abs/2404.19706v1", "similarity": 0.7282}, {"title": "Dynamic Gaussians Mesh: Consistent Mesh Reconstruction from Monocular\n  Videos", "link": "http://arxiv.org/abs/2404.12379v2", "similarity": 0.6428}, {"title": "MiraGe: Editable 2D Images using Gaussian Splatting", "link": "http://arxiv.org/abs/2410.01521v1", "similarity": 0.6391}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%203D%20Convex%20Splatting%3A%20Radiance%20Field%20Rendering%20with%203D%20Smooth%20Convexes&body=Title%3A%203D%20Convex%20Splatting%3A%20Radiance%20Field%20Rendering%20with%203D%20Smooth%20Convexes%0AAuthor%3A%20Jan%20Held%20and%20Renaud%20Vandeghen%20and%20Abdullah%20Hamdi%20and%20Adrien%20Deliege%20and%20Anthony%20Cioppa%20and%20Silvio%20Giancola%20and%20Andrea%20Vedaldi%20and%20Bernard%20Ghanem%20and%20Marc%20Van%20Droogenbroeck%0AAbstract%3A%20%20%20Recent%20advances%20in%20radiance%20field%20reconstruction%2C%20such%20as%203D%20Gaussian%0ASplatting%20%283DGS%29%2C%20have%20achieved%20high-quality%20novel%20view%20synthesis%20and%20fast%0Arendering%20by%20representing%20scenes%20with%20compositions%20of%20Gaussian%20primitives.%0AHowever%2C%203D%20Gaussians%20present%20several%20limitations%20for%20scene%20reconstruction.%0AAccurately%20capturing%20hard%20edges%20is%20challenging%20without%20significantly%20increasing%0Athe%20number%20of%20Gaussians%2C%20creating%20a%20large%20memory%20footprint.%20Moreover%2C%20they%0Astruggle%20to%20represent%20flat%20surfaces%2C%20as%20they%20are%20diffused%20in%20space.%20Without%0Ahand-crafted%20regularizers%2C%20they%20tend%20to%20disperse%20irregularly%20around%20the%20actual%0Asurface.%20To%20circumvent%20these%20issues%2C%20we%20introduce%20a%20novel%20method%2C%20named%203D%0AConvex%20Splatting%20%283DCS%29%2C%20which%20leverages%203D%20smooth%20convexes%20as%20primitives%20for%0Amodeling%20geometrically-meaningful%20radiance%20fields%20from%20multi-view%20images.%0ASmooth%20convex%20shapes%20offer%20greater%20flexibility%20than%20Gaussians%2C%20allowing%20for%20a%0Abetter%20representation%20of%203D%20scenes%20with%20hard%20edges%20and%20dense%20volumes%20using%0Afewer%20primitives.%20Powered%20by%20our%20efficient%20CUDA-based%20rasterizer%2C%203DCS%20achieves%0Asuperior%20performance%20over%203DGS%20on%20benchmarks%20such%20as%20Mip-NeRF360%2C%20Tanks%20and%0ATemples%2C%20and%20Deep%20Blending.%20Specifically%2C%20our%20method%20attains%20an%20improvement%20of%0Aup%20to%200.81%20in%20PSNR%20and%200.026%20in%20LPIPS%20compared%20to%203DGS%20while%20maintaining%20high%0Arendering%20speeds%20and%20reducing%20the%20number%20of%20required%20primitives.%20Our%20results%0Ahighlight%20the%20potential%20of%203D%20Convex%20Splatting%20to%20become%20the%20new%20standard%20for%0Ahigh-quality%20scene%20reconstruction%20and%20novel%20view%20synthesis.%20Project%20page%3A%0Awww.convexsplatting.com.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2411.14974v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3D3D%2520Convex%2520Splatting%253A%2520Radiance%2520Field%2520Rendering%2520with%25203D%2520Smooth%2520Convexes%26entry.906535625%3DJan%2520Held%2520and%2520Renaud%2520Vandeghen%2520and%2520Abdullah%2520Hamdi%2520and%2520Adrien%2520Deliege%2520and%2520Anthony%2520Cioppa%2520and%2520Silvio%2520Giancola%2520and%2520Andrea%2520Vedaldi%2520and%2520Bernard%2520Ghanem%2520and%2520Marc%2520Van%2520Droogenbroeck%26entry.1292438233%3D%2520%2520Recent%2520advances%2520in%2520radiance%2520field%2520reconstruction%252C%2520such%2520as%25203D%2520Gaussian%250ASplatting%2520%25283DGS%2529%252C%2520have%2520achieved%2520high-quality%2520novel%2520view%2520synthesis%2520and%2520fast%250Arendering%2520by%2520representing%2520scenes%2520with%2520compositions%2520of%2520Gaussian%2520primitives.%250AHowever%252C%25203D%2520Gaussians%2520present%2520several%2520limitations%2520for%2520scene%2520reconstruction.%250AAccurately%2520capturing%2520hard%2520edges%2520is%2520challenging%2520without%2520significantly%2520increasing%250Athe%2520number%2520of%2520Gaussians%252C%2520creating%2520a%2520large%2520memory%2520footprint.%2520Moreover%252C%2520they%250Astruggle%2520to%2520represent%2520flat%2520surfaces%252C%2520as%2520they%2520are%2520diffused%2520in%2520space.%2520Without%250Ahand-crafted%2520regularizers%252C%2520they%2520tend%2520to%2520disperse%2520irregularly%2520around%2520the%2520actual%250Asurface.%2520To%2520circumvent%2520these%2520issues%252C%2520we%2520introduce%2520a%2520novel%2520method%252C%2520named%25203D%250AConvex%2520Splatting%2520%25283DCS%2529%252C%2520which%2520leverages%25203D%2520smooth%2520convexes%2520as%2520primitives%2520for%250Amodeling%2520geometrically-meaningful%2520radiance%2520fields%2520from%2520multi-view%2520images.%250ASmooth%2520convex%2520shapes%2520offer%2520greater%2520flexibility%2520than%2520Gaussians%252C%2520allowing%2520for%2520a%250Abetter%2520representation%2520of%25203D%2520scenes%2520with%2520hard%2520edges%2520and%2520dense%2520volumes%2520using%250Afewer%2520primitives.%2520Powered%2520by%2520our%2520efficient%2520CUDA-based%2520rasterizer%252C%25203DCS%2520achieves%250Asuperior%2520performance%2520over%25203DGS%2520on%2520benchmarks%2520such%2520as%2520Mip-NeRF360%252C%2520Tanks%2520and%250ATemples%252C%2520and%2520Deep%2520Blending.%2520Specifically%252C%2520our%2520method%2520attains%2520an%2520improvement%2520of%250Aup%2520to%25200.81%2520in%2520PSNR%2520and%25200.026%2520in%2520LPIPS%2520compared%2520to%25203DGS%2520while%2520maintaining%2520high%250Arendering%2520speeds%2520and%2520reducing%2520the%2520number%2520of%2520required%2520primitives.%2520Our%2520results%250Ahighlight%2520the%2520potential%2520of%25203D%2520Convex%2520Splatting%2520to%2520become%2520the%2520new%2520standard%2520for%250Ahigh-quality%2520scene%2520reconstruction%2520and%2520novel%2520view%2520synthesis.%2520Project%2520page%253A%250Awww.convexsplatting.com.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2411.14974v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=3D%20Convex%20Splatting%3A%20Radiance%20Field%20Rendering%20with%203D%20Smooth%20Convexes&entry.906535625=Jan%20Held%20and%20Renaud%20Vandeghen%20and%20Abdullah%20Hamdi%20and%20Adrien%20Deliege%20and%20Anthony%20Cioppa%20and%20Silvio%20Giancola%20and%20Andrea%20Vedaldi%20and%20Bernard%20Ghanem%20and%20Marc%20Van%20Droogenbroeck&entry.1292438233=%20%20Recent%20advances%20in%20radiance%20field%20reconstruction%2C%20such%20as%203D%20Gaussian%0ASplatting%20%283DGS%29%2C%20have%20achieved%20high-quality%20novel%20view%20synthesis%20and%20fast%0Arendering%20by%20representing%20scenes%20with%20compositions%20of%20Gaussian%20primitives.%0AHowever%2C%203D%20Gaussians%20present%20several%20limitations%20for%20scene%20reconstruction.%0AAccurately%20capturing%20hard%20edges%20is%20challenging%20without%20significantly%20increasing%0Athe%20number%20of%20Gaussians%2C%20creating%20a%20large%20memory%20footprint.%20Moreover%2C%20they%0Astruggle%20to%20represent%20flat%20surfaces%2C%20as%20they%20are%20diffused%20in%20space.%20Without%0Ahand-crafted%20regularizers%2C%20they%20tend%20to%20disperse%20irregularly%20around%20the%20actual%0Asurface.%20To%20circumvent%20these%20issues%2C%20we%20introduce%20a%20novel%20method%2C%20named%203D%0AConvex%20Splatting%20%283DCS%29%2C%20which%20leverages%203D%20smooth%20convexes%20as%20primitives%20for%0Amodeling%20geometrically-meaningful%20radiance%20fields%20from%20multi-view%20images.%0ASmooth%20convex%20shapes%20offer%20greater%20flexibility%20than%20Gaussians%2C%20allowing%20for%20a%0Abetter%20representation%20of%203D%20scenes%20with%20hard%20edges%20and%20dense%20volumes%20using%0Afewer%20primitives.%20Powered%20by%20our%20efficient%20CUDA-based%20rasterizer%2C%203DCS%20achieves%0Asuperior%20performance%20over%203DGS%20on%20benchmarks%20such%20as%20Mip-NeRF360%2C%20Tanks%20and%0ATemples%2C%20and%20Deep%20Blending.%20Specifically%2C%20our%20method%20attains%20an%20improvement%20of%0Aup%20to%200.81%20in%20PSNR%20and%200.026%20in%20LPIPS%20compared%20to%203DGS%20while%20maintaining%20high%0Arendering%20speeds%20and%20reducing%20the%20number%20of%20required%20primitives.%20Our%20results%0Ahighlight%20the%20potential%20of%203D%20Convex%20Splatting%20to%20become%20the%20new%20standard%20for%0Ahigh-quality%20scene%20reconstruction%20and%20novel%20view%20synthesis.%20Project%20page%3A%0Awww.convexsplatting.com.%0A&entry.1838667208=http%3A//arxiv.org/abs/2411.14974v1&entry.124074799=Read"},
{"title": "Sketch-guided Cage-based 3D Gaussian Splatting Deformation", "author": "Tianhao Xie and Noam Aigerman and Eugene Belilovsky and Tiberiu Popa", "abstract": "  3D Gaussian Splatting (GS) is one of the most promising novel 3D\nrepresentations that has received great interest in computer graphics and\ncomputer vision. While various systems have introduced editing capabilities for\n3D GS, such as those guided by text prompts, fine-grained control over\ndeformation remains an open challenge. In this work, we present a novel\nsketch-guided 3D GS deformation system that allows users to intuitively modify\nthe geometry of a 3D GS model by drawing a silhouette sketch from a single\nviewpoint. Our approach introduces a new deformation method that combines\ncage-based deformations with a variant of Neural Jacobian Fields, enabling\nprecise, fine-grained control. Additionally, it leverages large-scale 2D\ndiffusion priors and ControlNet to ensure the generated deformations are\nsemantically plausible. Through a series of experiments, we demonstrate the\neffectiveness of our method and showcase its ability to animate static 3D GS\nmodels as one of its key applications.\n", "link": "http://arxiv.org/abs/2411.12168v2", "date": "2024-11-22", "relevancy": 3.3456, "topK": [{"title": "GST: Precise 3D Human Body from a Single Image with Gaussian Splatting\n  Transformers", "link": "http://arxiv.org/abs/2409.04196v1", "similarity": 0.6936}, {"title": "RTG-SLAM: Real-time 3D Reconstruction at Scale using Gaussian Splatting", "link": "http://arxiv.org/abs/2404.19706v1", "similarity": 0.6598}, {"title": "Dynamic Gaussians Mesh: Consistent Mesh Reconstruction from Monocular\n  Videos", "link": "http://arxiv.org/abs/2404.12379v2", "similarity": 0.654}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Sketch-guided%20Cage-based%203D%20Gaussian%20Splatting%20Deformation&body=Title%3A%20Sketch-guided%20Cage-based%203D%20Gaussian%20Splatting%20Deformation%0AAuthor%3A%20Tianhao%20Xie%20and%20Noam%20Aigerman%20and%20Eugene%20Belilovsky%20and%20Tiberiu%20Popa%0AAbstract%3A%20%20%203D%20Gaussian%20Splatting%20%28GS%29%20is%20one%20of%20the%20most%20promising%20novel%203D%0Arepresentations%20that%20has%20received%20great%20interest%20in%20computer%20graphics%20and%0Acomputer%20vision.%20While%20various%20systems%20have%20introduced%20editing%20capabilities%20for%0A3D%20GS%2C%20such%20as%20those%20guided%20by%20text%20prompts%2C%20fine-grained%20control%20over%0Adeformation%20remains%20an%20open%20challenge.%20In%20this%20work%2C%20we%20present%20a%20novel%0Asketch-guided%203D%20GS%20deformation%20system%20that%20allows%20users%20to%20intuitively%20modify%0Athe%20geometry%20of%20a%203D%20GS%20model%20by%20drawing%20a%20silhouette%20sketch%20from%20a%20single%0Aviewpoint.%20Our%20approach%20introduces%20a%20new%20deformation%20method%20that%20combines%0Acage-based%20deformations%20with%20a%20variant%20of%20Neural%20Jacobian%20Fields%2C%20enabling%0Aprecise%2C%20fine-grained%20control.%20Additionally%2C%20it%20leverages%20large-scale%202D%0Adiffusion%20priors%20and%20ControlNet%20to%20ensure%20the%20generated%20deformations%20are%0Asemantically%20plausible.%20Through%20a%20series%20of%20experiments%2C%20we%20demonstrate%20the%0Aeffectiveness%20of%20our%20method%20and%20showcase%20its%20ability%20to%20animate%20static%203D%20GS%0Amodels%20as%20one%20of%20its%20key%20applications.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2411.12168v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DSketch-guided%2520Cage-based%25203D%2520Gaussian%2520Splatting%2520Deformation%26entry.906535625%3DTianhao%2520Xie%2520and%2520Noam%2520Aigerman%2520and%2520Eugene%2520Belilovsky%2520and%2520Tiberiu%2520Popa%26entry.1292438233%3D%2520%25203D%2520Gaussian%2520Splatting%2520%2528GS%2529%2520is%2520one%2520of%2520the%2520most%2520promising%2520novel%25203D%250Arepresentations%2520that%2520has%2520received%2520great%2520interest%2520in%2520computer%2520graphics%2520and%250Acomputer%2520vision.%2520While%2520various%2520systems%2520have%2520introduced%2520editing%2520capabilities%2520for%250A3D%2520GS%252C%2520such%2520as%2520those%2520guided%2520by%2520text%2520prompts%252C%2520fine-grained%2520control%2520over%250Adeformation%2520remains%2520an%2520open%2520challenge.%2520In%2520this%2520work%252C%2520we%2520present%2520a%2520novel%250Asketch-guided%25203D%2520GS%2520deformation%2520system%2520that%2520allows%2520users%2520to%2520intuitively%2520modify%250Athe%2520geometry%2520of%2520a%25203D%2520GS%2520model%2520by%2520drawing%2520a%2520silhouette%2520sketch%2520from%2520a%2520single%250Aviewpoint.%2520Our%2520approach%2520introduces%2520a%2520new%2520deformation%2520method%2520that%2520combines%250Acage-based%2520deformations%2520with%2520a%2520variant%2520of%2520Neural%2520Jacobian%2520Fields%252C%2520enabling%250Aprecise%252C%2520fine-grained%2520control.%2520Additionally%252C%2520it%2520leverages%2520large-scale%25202D%250Adiffusion%2520priors%2520and%2520ControlNet%2520to%2520ensure%2520the%2520generated%2520deformations%2520are%250Asemantically%2520plausible.%2520Through%2520a%2520series%2520of%2520experiments%252C%2520we%2520demonstrate%2520the%250Aeffectiveness%2520of%2520our%2520method%2520and%2520showcase%2520its%2520ability%2520to%2520animate%2520static%25203D%2520GS%250Amodels%2520as%2520one%2520of%2520its%2520key%2520applications.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2411.12168v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Sketch-guided%20Cage-based%203D%20Gaussian%20Splatting%20Deformation&entry.906535625=Tianhao%20Xie%20and%20Noam%20Aigerman%20and%20Eugene%20Belilovsky%20and%20Tiberiu%20Popa&entry.1292438233=%20%203D%20Gaussian%20Splatting%20%28GS%29%20is%20one%20of%20the%20most%20promising%20novel%203D%0Arepresentations%20that%20has%20received%20great%20interest%20in%20computer%20graphics%20and%0Acomputer%20vision.%20While%20various%20systems%20have%20introduced%20editing%20capabilities%20for%0A3D%20GS%2C%20such%20as%20those%20guided%20by%20text%20prompts%2C%20fine-grained%20control%20over%0Adeformation%20remains%20an%20open%20challenge.%20In%20this%20work%2C%20we%20present%20a%20novel%0Asketch-guided%203D%20GS%20deformation%20system%20that%20allows%20users%20to%20intuitively%20modify%0Athe%20geometry%20of%20a%203D%20GS%20model%20by%20drawing%20a%20silhouette%20sketch%20from%20a%20single%0Aviewpoint.%20Our%20approach%20introduces%20a%20new%20deformation%20method%20that%20combines%0Acage-based%20deformations%20with%20a%20variant%20of%20Neural%20Jacobian%20Fields%2C%20enabling%0Aprecise%2C%20fine-grained%20control.%20Additionally%2C%20it%20leverages%20large-scale%202D%0Adiffusion%20priors%20and%20ControlNet%20to%20ensure%20the%20generated%20deformations%20are%0Asemantically%20plausible.%20Through%20a%20series%20of%20experiments%2C%20we%20demonstrate%20the%0Aeffectiveness%20of%20our%20method%20and%20showcase%20its%20ability%20to%20animate%20static%203D%20GS%0Amodels%20as%20one%20of%20its%20key%20applications.%0A&entry.1838667208=http%3A//arxiv.org/abs/2411.12168v2&entry.124074799=Read"},
{"title": "Material Anything: Generating Materials for Any 3D Object via Diffusion", "author": "Xin Huang and Tengfei Wang and Ziwei Liu and Qing Wang", "abstract": "  We present Material Anything, a fully-automated, unified diffusion framework\ndesigned to generate physically-based materials for 3D objects. Unlike existing\nmethods that rely on complex pipelines or case-specific optimizations, Material\nAnything offers a robust, end-to-end solution adaptable to objects under\ndiverse lighting conditions. Our approach leverages a pre-trained image\ndiffusion model, enhanced with a triple-head architecture and rendering loss to\nimprove stability and material quality. Additionally, we introduce confidence\nmasks as a dynamic switcher within the diffusion model, enabling it to\neffectively handle both textured and texture-less objects across varying\nlighting conditions. By employing a progressive material generation strategy\nguided by these confidence masks, along with a UV-space material refiner, our\nmethod ensures consistent, UV-ready material outputs. Extensive experiments\ndemonstrate our approach outperforms existing methods across a wide range of\nobject categories and lighting conditions.\n", "link": "http://arxiv.org/abs/2411.15138v1", "date": "2024-11-22", "relevancy": 3.2722, "topK": [{"title": "FabricDiffusion: High-Fidelity Texture Transfer for 3D Garments\n  Generation from In-The-Wild Clothing Images", "link": "http://arxiv.org/abs/2410.01801v1", "similarity": 0.6856}, {"title": "CAT3D: Create Anything in 3D with Multi-View Diffusion Models", "link": "http://arxiv.org/abs/2405.10314v1", "similarity": 0.6389}, {"title": "CAT3D: Create Anything in 3D with Multi-View Diffusion Models", "link": "http://arxiv.org/abs/2405.10314v1", "similarity": 0.6389}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Material%20Anything%3A%20Generating%20Materials%20for%20Any%203D%20Object%20via%20Diffusion&body=Title%3A%20Material%20Anything%3A%20Generating%20Materials%20for%20Any%203D%20Object%20via%20Diffusion%0AAuthor%3A%20Xin%20Huang%20and%20Tengfei%20Wang%20and%20Ziwei%20Liu%20and%20Qing%20Wang%0AAbstract%3A%20%20%20We%20present%20Material%20Anything%2C%20a%20fully-automated%2C%20unified%20diffusion%20framework%0Adesigned%20to%20generate%20physically-based%20materials%20for%203D%20objects.%20Unlike%20existing%0Amethods%20that%20rely%20on%20complex%20pipelines%20or%20case-specific%20optimizations%2C%20Material%0AAnything%20offers%20a%20robust%2C%20end-to-end%20solution%20adaptable%20to%20objects%20under%0Adiverse%20lighting%20conditions.%20Our%20approach%20leverages%20a%20pre-trained%20image%0Adiffusion%20model%2C%20enhanced%20with%20a%20triple-head%20architecture%20and%20rendering%20loss%20to%0Aimprove%20stability%20and%20material%20quality.%20Additionally%2C%20we%20introduce%20confidence%0Amasks%20as%20a%20dynamic%20switcher%20within%20the%20diffusion%20model%2C%20enabling%20it%20to%0Aeffectively%20handle%20both%20textured%20and%20texture-less%20objects%20across%20varying%0Alighting%20conditions.%20By%20employing%20a%20progressive%20material%20generation%20strategy%0Aguided%20by%20these%20confidence%20masks%2C%20along%20with%20a%20UV-space%20material%20refiner%2C%20our%0Amethod%20ensures%20consistent%2C%20UV-ready%20material%20outputs.%20Extensive%20experiments%0Ademonstrate%20our%20approach%20outperforms%20existing%20methods%20across%20a%20wide%20range%20of%0Aobject%20categories%20and%20lighting%20conditions.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2411.15138v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DMaterial%2520Anything%253A%2520Generating%2520Materials%2520for%2520Any%25203D%2520Object%2520via%2520Diffusion%26entry.906535625%3DXin%2520Huang%2520and%2520Tengfei%2520Wang%2520and%2520Ziwei%2520Liu%2520and%2520Qing%2520Wang%26entry.1292438233%3D%2520%2520We%2520present%2520Material%2520Anything%252C%2520a%2520fully-automated%252C%2520unified%2520diffusion%2520framework%250Adesigned%2520to%2520generate%2520physically-based%2520materials%2520for%25203D%2520objects.%2520Unlike%2520existing%250Amethods%2520that%2520rely%2520on%2520complex%2520pipelines%2520or%2520case-specific%2520optimizations%252C%2520Material%250AAnything%2520offers%2520a%2520robust%252C%2520end-to-end%2520solution%2520adaptable%2520to%2520objects%2520under%250Adiverse%2520lighting%2520conditions.%2520Our%2520approach%2520leverages%2520a%2520pre-trained%2520image%250Adiffusion%2520model%252C%2520enhanced%2520with%2520a%2520triple-head%2520architecture%2520and%2520rendering%2520loss%2520to%250Aimprove%2520stability%2520and%2520material%2520quality.%2520Additionally%252C%2520we%2520introduce%2520confidence%250Amasks%2520as%2520a%2520dynamic%2520switcher%2520within%2520the%2520diffusion%2520model%252C%2520enabling%2520it%2520to%250Aeffectively%2520handle%2520both%2520textured%2520and%2520texture-less%2520objects%2520across%2520varying%250Alighting%2520conditions.%2520By%2520employing%2520a%2520progressive%2520material%2520generation%2520strategy%250Aguided%2520by%2520these%2520confidence%2520masks%252C%2520along%2520with%2520a%2520UV-space%2520material%2520refiner%252C%2520our%250Amethod%2520ensures%2520consistent%252C%2520UV-ready%2520material%2520outputs.%2520Extensive%2520experiments%250Ademonstrate%2520our%2520approach%2520outperforms%2520existing%2520methods%2520across%2520a%2520wide%2520range%2520of%250Aobject%2520categories%2520and%2520lighting%2520conditions.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2411.15138v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Material%20Anything%3A%20Generating%20Materials%20for%20Any%203D%20Object%20via%20Diffusion&entry.906535625=Xin%20Huang%20and%20Tengfei%20Wang%20and%20Ziwei%20Liu%20and%20Qing%20Wang&entry.1292438233=%20%20We%20present%20Material%20Anything%2C%20a%20fully-automated%2C%20unified%20diffusion%20framework%0Adesigned%20to%20generate%20physically-based%20materials%20for%203D%20objects.%20Unlike%20existing%0Amethods%20that%20rely%20on%20complex%20pipelines%20or%20case-specific%20optimizations%2C%20Material%0AAnything%20offers%20a%20robust%2C%20end-to-end%20solution%20adaptable%20to%20objects%20under%0Adiverse%20lighting%20conditions.%20Our%20approach%20leverages%20a%20pre-trained%20image%0Adiffusion%20model%2C%20enhanced%20with%20a%20triple-head%20architecture%20and%20rendering%20loss%20to%0Aimprove%20stability%20and%20material%20quality.%20Additionally%2C%20we%20introduce%20confidence%0Amasks%20as%20a%20dynamic%20switcher%20within%20the%20diffusion%20model%2C%20enabling%20it%20to%0Aeffectively%20handle%20both%20textured%20and%20texture-less%20objects%20across%20varying%0Alighting%20conditions.%20By%20employing%20a%20progressive%20material%20generation%20strategy%0Aguided%20by%20these%20confidence%20masks%2C%20along%20with%20a%20UV-space%20material%20refiner%2C%20our%0Amethod%20ensures%20consistent%2C%20UV-ready%20material%20outputs.%20Extensive%20experiments%0Ademonstrate%20our%20approach%20outperforms%20existing%20methods%20across%20a%20wide%20range%20of%0Aobject%20categories%20and%20lighting%20conditions.%0A&entry.1838667208=http%3A//arxiv.org/abs/2411.15138v1&entry.124074799=Read"},
{"title": "Self-Ensembling Gaussian Splatting for Few-Shot Novel View Synthesis", "author": "Chen Zhao and Xuan Wang and Tong Zhang and Saqib Javed and Mathieu Salzmann", "abstract": "  3D Gaussian Splatting (3DGS) has demonstrated remarkable effectiveness for\nnovel view synthesis (NVS). However, the 3DGS model tends to overfit when\ntrained with sparse posed views, limiting its generalization ability to novel\nviews. In this paper, we alleviate the overfitting problem, presenting a\nSelf-Ensembling Gaussian Splatting (SE-GS) approach. Our method encompasses a\n$\\mathbf{\\Sigma}$-model and a $\\mathbf{\\Delta}$-model. The\n$\\mathbf{\\Sigma}$-model serves as an ensemble of 3DGS models that generates\nnovel-view images during inference. We achieve the self-ensembling by\nintroducing an uncertainty-aware perturbation strategy at the training state.\nWe complement the $\\mathbf{\\Sigma}$-model with the $\\mathbf{\\Delta}$-model,\nwhich is dynamically perturbed based on the uncertainties of novel-view\nrenderings across different training steps. The perturbation yields diverse\ntemporal samples in the Gaussian parameter space without additional training\ncosts. The geometry of the $\\mathbf{\\Sigma}$-model is regularized by penalizing\ndiscrepancies between the $\\mathbf{\\Sigma}$-model and these temporal samples.\nTherefore, our SE-GS conducts an effective and efficient regularization across\na large number of 3DGS models, resulting in a robust ensemble, the\n$\\mathbf{\\Sigma}$-model. Our experimental results on the LLFF, Mip-NeRF360,\nDTU, and MVImgNet datasets show that our approach improves NVS quality with\nfew-shot training views, outperforming existing state-of-the-art methods. The\ncode is released at: https://sailor-z.github.io/projects/SEGS.html.\n", "link": "http://arxiv.org/abs/2411.00144v2", "date": "2024-11-22", "relevancy": 3.1836, "topK": [{"title": "RTG-SLAM: Real-time 3D Reconstruction at Scale using Gaussian Splatting", "link": "http://arxiv.org/abs/2404.19706v1", "similarity": 0.6513}, {"title": "GST: Precise 3D Human Body from a Single Image with Gaussian Splatting\n  Transformers", "link": "http://arxiv.org/abs/2409.04196v1", "similarity": 0.6491}, {"title": "Dynamic Gaussians Mesh: Consistent Mesh Reconstruction from Monocular\n  Videos", "link": "http://arxiv.org/abs/2404.12379v2", "similarity": 0.6097}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Self-Ensembling%20Gaussian%20Splatting%20for%20Few-Shot%20Novel%20View%20Synthesis&body=Title%3A%20Self-Ensembling%20Gaussian%20Splatting%20for%20Few-Shot%20Novel%20View%20Synthesis%0AAuthor%3A%20Chen%20Zhao%20and%20Xuan%20Wang%20and%20Tong%20Zhang%20and%20Saqib%20Javed%20and%20Mathieu%20Salzmann%0AAbstract%3A%20%20%203D%20Gaussian%20Splatting%20%283DGS%29%20has%20demonstrated%20remarkable%20effectiveness%20for%0Anovel%20view%20synthesis%20%28NVS%29.%20However%2C%20the%203DGS%20model%20tends%20to%20overfit%20when%0Atrained%20with%20sparse%20posed%20views%2C%20limiting%20its%20generalization%20ability%20to%20novel%0Aviews.%20In%20this%20paper%2C%20we%20alleviate%20the%20overfitting%20problem%2C%20presenting%20a%0ASelf-Ensembling%20Gaussian%20Splatting%20%28SE-GS%29%20approach.%20Our%20method%20encompasses%20a%0A%24%5Cmathbf%7B%5CSigma%7D%24-model%20and%20a%20%24%5Cmathbf%7B%5CDelta%7D%24-model.%20The%0A%24%5Cmathbf%7B%5CSigma%7D%24-model%20serves%20as%20an%20ensemble%20of%203DGS%20models%20that%20generates%0Anovel-view%20images%20during%20inference.%20We%20achieve%20the%20self-ensembling%20by%0Aintroducing%20an%20uncertainty-aware%20perturbation%20strategy%20at%20the%20training%20state.%0AWe%20complement%20the%20%24%5Cmathbf%7B%5CSigma%7D%24-model%20with%20the%20%24%5Cmathbf%7B%5CDelta%7D%24-model%2C%0Awhich%20is%20dynamically%20perturbed%20based%20on%20the%20uncertainties%20of%20novel-view%0Arenderings%20across%20different%20training%20steps.%20The%20perturbation%20yields%20diverse%0Atemporal%20samples%20in%20the%20Gaussian%20parameter%20space%20without%20additional%20training%0Acosts.%20The%20geometry%20of%20the%20%24%5Cmathbf%7B%5CSigma%7D%24-model%20is%20regularized%20by%20penalizing%0Adiscrepancies%20between%20the%20%24%5Cmathbf%7B%5CSigma%7D%24-model%20and%20these%20temporal%20samples.%0ATherefore%2C%20our%20SE-GS%20conducts%20an%20effective%20and%20efficient%20regularization%20across%0Aa%20large%20number%20of%203DGS%20models%2C%20resulting%20in%20a%20robust%20ensemble%2C%20the%0A%24%5Cmathbf%7B%5CSigma%7D%24-model.%20Our%20experimental%20results%20on%20the%20LLFF%2C%20Mip-NeRF360%2C%0ADTU%2C%20and%20MVImgNet%20datasets%20show%20that%20our%20approach%20improves%20NVS%20quality%20with%0Afew-shot%20training%20views%2C%20outperforming%20existing%20state-of-the-art%20methods.%20The%0Acode%20is%20released%20at%3A%20https%3A//sailor-z.github.io/projects/SEGS.html.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2411.00144v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DSelf-Ensembling%2520Gaussian%2520Splatting%2520for%2520Few-Shot%2520Novel%2520View%2520Synthesis%26entry.906535625%3DChen%2520Zhao%2520and%2520Xuan%2520Wang%2520and%2520Tong%2520Zhang%2520and%2520Saqib%2520Javed%2520and%2520Mathieu%2520Salzmann%26entry.1292438233%3D%2520%25203D%2520Gaussian%2520Splatting%2520%25283DGS%2529%2520has%2520demonstrated%2520remarkable%2520effectiveness%2520for%250Anovel%2520view%2520synthesis%2520%2528NVS%2529.%2520However%252C%2520the%25203DGS%2520model%2520tends%2520to%2520overfit%2520when%250Atrained%2520with%2520sparse%2520posed%2520views%252C%2520limiting%2520its%2520generalization%2520ability%2520to%2520novel%250Aviews.%2520In%2520this%2520paper%252C%2520we%2520alleviate%2520the%2520overfitting%2520problem%252C%2520presenting%2520a%250ASelf-Ensembling%2520Gaussian%2520Splatting%2520%2528SE-GS%2529%2520approach.%2520Our%2520method%2520encompasses%2520a%250A%2524%255Cmathbf%257B%255CSigma%257D%2524-model%2520and%2520a%2520%2524%255Cmathbf%257B%255CDelta%257D%2524-model.%2520The%250A%2524%255Cmathbf%257B%255CSigma%257D%2524-model%2520serves%2520as%2520an%2520ensemble%2520of%25203DGS%2520models%2520that%2520generates%250Anovel-view%2520images%2520during%2520inference.%2520We%2520achieve%2520the%2520self-ensembling%2520by%250Aintroducing%2520an%2520uncertainty-aware%2520perturbation%2520strategy%2520at%2520the%2520training%2520state.%250AWe%2520complement%2520the%2520%2524%255Cmathbf%257B%255CSigma%257D%2524-model%2520with%2520the%2520%2524%255Cmathbf%257B%255CDelta%257D%2524-model%252C%250Awhich%2520is%2520dynamically%2520perturbed%2520based%2520on%2520the%2520uncertainties%2520of%2520novel-view%250Arenderings%2520across%2520different%2520training%2520steps.%2520The%2520perturbation%2520yields%2520diverse%250Atemporal%2520samples%2520in%2520the%2520Gaussian%2520parameter%2520space%2520without%2520additional%2520training%250Acosts.%2520The%2520geometry%2520of%2520the%2520%2524%255Cmathbf%257B%255CSigma%257D%2524-model%2520is%2520regularized%2520by%2520penalizing%250Adiscrepancies%2520between%2520the%2520%2524%255Cmathbf%257B%255CSigma%257D%2524-model%2520and%2520these%2520temporal%2520samples.%250ATherefore%252C%2520our%2520SE-GS%2520conducts%2520an%2520effective%2520and%2520efficient%2520regularization%2520across%250Aa%2520large%2520number%2520of%25203DGS%2520models%252C%2520resulting%2520in%2520a%2520robust%2520ensemble%252C%2520the%250A%2524%255Cmathbf%257B%255CSigma%257D%2524-model.%2520Our%2520experimental%2520results%2520on%2520the%2520LLFF%252C%2520Mip-NeRF360%252C%250ADTU%252C%2520and%2520MVImgNet%2520datasets%2520show%2520that%2520our%2520approach%2520improves%2520NVS%2520quality%2520with%250Afew-shot%2520training%2520views%252C%2520outperforming%2520existing%2520state-of-the-art%2520methods.%2520The%250Acode%2520is%2520released%2520at%253A%2520https%253A//sailor-z.github.io/projects/SEGS.html.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2411.00144v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Self-Ensembling%20Gaussian%20Splatting%20for%20Few-Shot%20Novel%20View%20Synthesis&entry.906535625=Chen%20Zhao%20and%20Xuan%20Wang%20and%20Tong%20Zhang%20and%20Saqib%20Javed%20and%20Mathieu%20Salzmann&entry.1292438233=%20%203D%20Gaussian%20Splatting%20%283DGS%29%20has%20demonstrated%20remarkable%20effectiveness%20for%0Anovel%20view%20synthesis%20%28NVS%29.%20However%2C%20the%203DGS%20model%20tends%20to%20overfit%20when%0Atrained%20with%20sparse%20posed%20views%2C%20limiting%20its%20generalization%20ability%20to%20novel%0Aviews.%20In%20this%20paper%2C%20we%20alleviate%20the%20overfitting%20problem%2C%20presenting%20a%0ASelf-Ensembling%20Gaussian%20Splatting%20%28SE-GS%29%20approach.%20Our%20method%20encompasses%20a%0A%24%5Cmathbf%7B%5CSigma%7D%24-model%20and%20a%20%24%5Cmathbf%7B%5CDelta%7D%24-model.%20The%0A%24%5Cmathbf%7B%5CSigma%7D%24-model%20serves%20as%20an%20ensemble%20of%203DGS%20models%20that%20generates%0Anovel-view%20images%20during%20inference.%20We%20achieve%20the%20self-ensembling%20by%0Aintroducing%20an%20uncertainty-aware%20perturbation%20strategy%20at%20the%20training%20state.%0AWe%20complement%20the%20%24%5Cmathbf%7B%5CSigma%7D%24-model%20with%20the%20%24%5Cmathbf%7B%5CDelta%7D%24-model%2C%0Awhich%20is%20dynamically%20perturbed%20based%20on%20the%20uncertainties%20of%20novel-view%0Arenderings%20across%20different%20training%20steps.%20The%20perturbation%20yields%20diverse%0Atemporal%20samples%20in%20the%20Gaussian%20parameter%20space%20without%20additional%20training%0Acosts.%20The%20geometry%20of%20the%20%24%5Cmathbf%7B%5CSigma%7D%24-model%20is%20regularized%20by%20penalizing%0Adiscrepancies%20between%20the%20%24%5Cmathbf%7B%5CSigma%7D%24-model%20and%20these%20temporal%20samples.%0ATherefore%2C%20our%20SE-GS%20conducts%20an%20effective%20and%20efficient%20regularization%20across%0Aa%20large%20number%20of%203DGS%20models%2C%20resulting%20in%20a%20robust%20ensemble%2C%20the%0A%24%5Cmathbf%7B%5CSigma%7D%24-model.%20Our%20experimental%20results%20on%20the%20LLFF%2C%20Mip-NeRF360%2C%0ADTU%2C%20and%20MVImgNet%20datasets%20show%20that%20our%20approach%20improves%20NVS%20quality%20with%0Afew-shot%20training%20views%2C%20outperforming%20existing%20state-of-the-art%20methods.%20The%0Acode%20is%20released%20at%3A%20https%3A//sailor-z.github.io/projects/SEGS.html.%0A&entry.1838667208=http%3A//arxiv.org/abs/2411.00144v2&entry.124074799=Read"},
{"title": "BillBoard Splatting (BBSplat): Learnable Textured Primitives for Novel\n  View Synthesis", "author": "David Svitov and Pietro Morerio and Lourdes Agapito and Alessio Del Bue", "abstract": "  We present billboard Splatting (BBSplat) - a novel approach for 3D scene\nrepresentation based on textured geometric primitives. BBSplat represents the\nscene as a set of optimizable textured planar primitives with learnable RGB\ntextures and alpha-maps to control their shape. BBSplat primitives can be used\nin any Gaussian Splatting pipeline as drop-in replacements for Gaussians. Our\nmethod's qualitative and quantitative improvements over 3D and 2D Gaussians are\nmost noticeable when fewer primitives are used, when BBSplat achieves over 1200\nFPS. Our novel regularization term encourages textures to have a sparser\nstructure, unlocking an efficient compression that leads to a reduction in\nstorage space of the model. Our experiments show the efficiency of BBSplat on\nstandard datasets of real indoor and outdoor scenes such as Tanks&Temples, DTU,\nand Mip-NeRF-360. We demonstrate improvements on PSNR, SSIM, and LPIPS metrics\ncompared to the state-of-the-art, especially for the case when fewer primitives\nare used, which, on the other hand, leads to up to 2 times inference speed\nimprovement for the same rendering quality.\n", "link": "http://arxiv.org/abs/2411.08508v2", "date": "2024-11-22", "relevancy": 3.1266, "topK": [{"title": "RTG-SLAM: Real-time 3D Reconstruction at Scale using Gaussian Splatting", "link": "http://arxiv.org/abs/2404.19706v1", "similarity": 0.6578}, {"title": "GST: Precise 3D Human Body from a Single Image with Gaussian Splatting\n  Transformers", "link": "http://arxiv.org/abs/2409.04196v1", "similarity": 0.6291}, {"title": "MiraGe: Editable 2D Images using Gaussian Splatting", "link": "http://arxiv.org/abs/2410.01521v1", "similarity": 0.5891}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20BillBoard%20Splatting%20%28BBSplat%29%3A%20Learnable%20Textured%20Primitives%20for%20Novel%0A%20%20View%20Synthesis&body=Title%3A%20BillBoard%20Splatting%20%28BBSplat%29%3A%20Learnable%20Textured%20Primitives%20for%20Novel%0A%20%20View%20Synthesis%0AAuthor%3A%20David%20Svitov%20and%20Pietro%20Morerio%20and%20Lourdes%20Agapito%20and%20Alessio%20Del%20Bue%0AAbstract%3A%20%20%20We%20present%20billboard%20Splatting%20%28BBSplat%29%20-%20a%20novel%20approach%20for%203D%20scene%0Arepresentation%20based%20on%20textured%20geometric%20primitives.%20BBSplat%20represents%20the%0Ascene%20as%20a%20set%20of%20optimizable%20textured%20planar%20primitives%20with%20learnable%20RGB%0Atextures%20and%20alpha-maps%20to%20control%20their%20shape.%20BBSplat%20primitives%20can%20be%20used%0Ain%20any%20Gaussian%20Splatting%20pipeline%20as%20drop-in%20replacements%20for%20Gaussians.%20Our%0Amethod%27s%20qualitative%20and%20quantitative%20improvements%20over%203D%20and%202D%20Gaussians%20are%0Amost%20noticeable%20when%20fewer%20primitives%20are%20used%2C%20when%20BBSplat%20achieves%20over%201200%0AFPS.%20Our%20novel%20regularization%20term%20encourages%20textures%20to%20have%20a%20sparser%0Astructure%2C%20unlocking%20an%20efficient%20compression%20that%20leads%20to%20a%20reduction%20in%0Astorage%20space%20of%20the%20model.%20Our%20experiments%20show%20the%20efficiency%20of%20BBSplat%20on%0Astandard%20datasets%20of%20real%20indoor%20and%20outdoor%20scenes%20such%20as%20Tanks%26Temples%2C%20DTU%2C%0Aand%20Mip-NeRF-360.%20We%20demonstrate%20improvements%20on%20PSNR%2C%20SSIM%2C%20and%20LPIPS%20metrics%0Acompared%20to%20the%20state-of-the-art%2C%20especially%20for%20the%20case%20when%20fewer%20primitives%0Aare%20used%2C%20which%2C%20on%20the%20other%20hand%2C%20leads%20to%20up%20to%202%20times%20inference%20speed%0Aimprovement%20for%20the%20same%20rendering%20quality.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2411.08508v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DBillBoard%2520Splatting%2520%2528BBSplat%2529%253A%2520Learnable%2520Textured%2520Primitives%2520for%2520Novel%250A%2520%2520View%2520Synthesis%26entry.906535625%3DDavid%2520Svitov%2520and%2520Pietro%2520Morerio%2520and%2520Lourdes%2520Agapito%2520and%2520Alessio%2520Del%2520Bue%26entry.1292438233%3D%2520%2520We%2520present%2520billboard%2520Splatting%2520%2528BBSplat%2529%2520-%2520a%2520novel%2520approach%2520for%25203D%2520scene%250Arepresentation%2520based%2520on%2520textured%2520geometric%2520primitives.%2520BBSplat%2520represents%2520the%250Ascene%2520as%2520a%2520set%2520of%2520optimizable%2520textured%2520planar%2520primitives%2520with%2520learnable%2520RGB%250Atextures%2520and%2520alpha-maps%2520to%2520control%2520their%2520shape.%2520BBSplat%2520primitives%2520can%2520be%2520used%250Ain%2520any%2520Gaussian%2520Splatting%2520pipeline%2520as%2520drop-in%2520replacements%2520for%2520Gaussians.%2520Our%250Amethod%2527s%2520qualitative%2520and%2520quantitative%2520improvements%2520over%25203D%2520and%25202D%2520Gaussians%2520are%250Amost%2520noticeable%2520when%2520fewer%2520primitives%2520are%2520used%252C%2520when%2520BBSplat%2520achieves%2520over%25201200%250AFPS.%2520Our%2520novel%2520regularization%2520term%2520encourages%2520textures%2520to%2520have%2520a%2520sparser%250Astructure%252C%2520unlocking%2520an%2520efficient%2520compression%2520that%2520leads%2520to%2520a%2520reduction%2520in%250Astorage%2520space%2520of%2520the%2520model.%2520Our%2520experiments%2520show%2520the%2520efficiency%2520of%2520BBSplat%2520on%250Astandard%2520datasets%2520of%2520real%2520indoor%2520and%2520outdoor%2520scenes%2520such%2520as%2520Tanks%2526Temples%252C%2520DTU%252C%250Aand%2520Mip-NeRF-360.%2520We%2520demonstrate%2520improvements%2520on%2520PSNR%252C%2520SSIM%252C%2520and%2520LPIPS%2520metrics%250Acompared%2520to%2520the%2520state-of-the-art%252C%2520especially%2520for%2520the%2520case%2520when%2520fewer%2520primitives%250Aare%2520used%252C%2520which%252C%2520on%2520the%2520other%2520hand%252C%2520leads%2520to%2520up%2520to%25202%2520times%2520inference%2520speed%250Aimprovement%2520for%2520the%2520same%2520rendering%2520quality.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2411.08508v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=BillBoard%20Splatting%20%28BBSplat%29%3A%20Learnable%20Textured%20Primitives%20for%20Novel%0A%20%20View%20Synthesis&entry.906535625=David%20Svitov%20and%20Pietro%20Morerio%20and%20Lourdes%20Agapito%20and%20Alessio%20Del%20Bue&entry.1292438233=%20%20We%20present%20billboard%20Splatting%20%28BBSplat%29%20-%20a%20novel%20approach%20for%203D%20scene%0Arepresentation%20based%20on%20textured%20geometric%20primitives.%20BBSplat%20represents%20the%0Ascene%20as%20a%20set%20of%20optimizable%20textured%20planar%20primitives%20with%20learnable%20RGB%0Atextures%20and%20alpha-maps%20to%20control%20their%20shape.%20BBSplat%20primitives%20can%20be%20used%0Ain%20any%20Gaussian%20Splatting%20pipeline%20as%20drop-in%20replacements%20for%20Gaussians.%20Our%0Amethod%27s%20qualitative%20and%20quantitative%20improvements%20over%203D%20and%202D%20Gaussians%20are%0Amost%20noticeable%20when%20fewer%20primitives%20are%20used%2C%20when%20BBSplat%20achieves%20over%201200%0AFPS.%20Our%20novel%20regularization%20term%20encourages%20textures%20to%20have%20a%20sparser%0Astructure%2C%20unlocking%20an%20efficient%20compression%20that%20leads%20to%20a%20reduction%20in%0Astorage%20space%20of%20the%20model.%20Our%20experiments%20show%20the%20efficiency%20of%20BBSplat%20on%0Astandard%20datasets%20of%20real%20indoor%20and%20outdoor%20scenes%20such%20as%20Tanks%26Temples%2C%20DTU%2C%0Aand%20Mip-NeRF-360.%20We%20demonstrate%20improvements%20on%20PSNR%2C%20SSIM%2C%20and%20LPIPS%20metrics%0Acompared%20to%20the%20state-of-the-art%2C%20especially%20for%20the%20case%20when%20fewer%20primitives%0Aare%20used%2C%20which%2C%20on%20the%20other%20hand%2C%20leads%20to%20up%20to%202%20times%20inference%20speed%0Aimprovement%20for%20the%20same%20rendering%20quality.%0A&entry.1838667208=http%3A//arxiv.org/abs/2411.08508v2&entry.124074799=Read"},
{"title": "Cinematic Gaussians: Real-Time HDR Radiance Fields with Depth of Field", "author": "Chao Wang and Krzysztof Wolski and Bernhard Kerbl and Ana Serrano and Mojtaba Bemana and Hans-Peter Seidel and Karol Myszkowski and Thomas Leimk\u00fchler", "abstract": "  Radiance field methods represent the state of the art in reconstructing\ncomplex scenes from multi-view photos. However, these reconstructions often\nsuffer from one or both of the following limitations: First, they typically\nrepresent scenes in low dynamic range (LDR), which restricts their use to\nevenly lit environments and hinders immersive viewing experiences. Secondly,\ntheir reliance on a pinhole camera model, assuming all scene elements are in\nfocus in the input images, presents practical challenges and complicates\nrefocusing during novel-view synthesis. Addressing these limitations, we\npresent a lightweight method based on 3D Gaussian Splatting that utilizes\nmulti-view LDR images of a scene with varying exposure times, apertures, and\nfocus distances as input to reconstruct a high-dynamic-range (HDR) radiance\nfield. By incorporating analytical convolutions of Gaussians based on a\nthin-lens camera model as well as a tonemapping module, our reconstructions\nenable the rendering of HDR content with flexible refocusing capabilities. We\ndemonstrate that our combined treatment of HDR and depth of field facilitates\nreal-time cinematic rendering, outperforming the state of the art.\n", "link": "http://arxiv.org/abs/2406.07329v4", "date": "2024-11-22", "relevancy": 3.1109, "topK": [{"title": "RTG-SLAM: Real-time 3D Reconstruction at Scale using Gaussian Splatting", "link": "http://arxiv.org/abs/2404.19706v1", "similarity": 0.6639}, {"title": "Dynamic Gaussians Mesh: Consistent Mesh Reconstruction from Monocular\n  Videos", "link": "http://arxiv.org/abs/2404.12379v2", "similarity": 0.6134}, {"title": "MiraGe: Editable 2D Images using Gaussian Splatting", "link": "http://arxiv.org/abs/2410.01521v1", "similarity": 0.5892}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Cinematic%20Gaussians%3A%20Real-Time%20HDR%20Radiance%20Fields%20with%20Depth%20of%20Field&body=Title%3A%20Cinematic%20Gaussians%3A%20Real-Time%20HDR%20Radiance%20Fields%20with%20Depth%20of%20Field%0AAuthor%3A%20Chao%20Wang%20and%20Krzysztof%20Wolski%20and%20Bernhard%20Kerbl%20and%20Ana%20Serrano%20and%20Mojtaba%20Bemana%20and%20Hans-Peter%20Seidel%20and%20Karol%20Myszkowski%20and%20Thomas%20Leimk%C3%BChler%0AAbstract%3A%20%20%20Radiance%20field%20methods%20represent%20the%20state%20of%20the%20art%20in%20reconstructing%0Acomplex%20scenes%20from%20multi-view%20photos.%20However%2C%20these%20reconstructions%20often%0Asuffer%20from%20one%20or%20both%20of%20the%20following%20limitations%3A%20First%2C%20they%20typically%0Arepresent%20scenes%20in%20low%20dynamic%20range%20%28LDR%29%2C%20which%20restricts%20their%20use%20to%0Aevenly%20lit%20environments%20and%20hinders%20immersive%20viewing%20experiences.%20Secondly%2C%0Atheir%20reliance%20on%20a%20pinhole%20camera%20model%2C%20assuming%20all%20scene%20elements%20are%20in%0Afocus%20in%20the%20input%20images%2C%20presents%20practical%20challenges%20and%20complicates%0Arefocusing%20during%20novel-view%20synthesis.%20Addressing%20these%20limitations%2C%20we%0Apresent%20a%20lightweight%20method%20based%20on%203D%20Gaussian%20Splatting%20that%20utilizes%0Amulti-view%20LDR%20images%20of%20a%20scene%20with%20varying%20exposure%20times%2C%20apertures%2C%20and%0Afocus%20distances%20as%20input%20to%20reconstruct%20a%20high-dynamic-range%20%28HDR%29%20radiance%0Afield.%20By%20incorporating%20analytical%20convolutions%20of%20Gaussians%20based%20on%20a%0Athin-lens%20camera%20model%20as%20well%20as%20a%20tonemapping%20module%2C%20our%20reconstructions%0Aenable%20the%20rendering%20of%20HDR%20content%20with%20flexible%20refocusing%20capabilities.%20We%0Ademonstrate%20that%20our%20combined%20treatment%20of%20HDR%20and%20depth%20of%20field%20facilitates%0Areal-time%20cinematic%20rendering%2C%20outperforming%20the%20state%20of%20the%20art.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2406.07329v4%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DCinematic%2520Gaussians%253A%2520Real-Time%2520HDR%2520Radiance%2520Fields%2520with%2520Depth%2520of%2520Field%26entry.906535625%3DChao%2520Wang%2520and%2520Krzysztof%2520Wolski%2520and%2520Bernhard%2520Kerbl%2520and%2520Ana%2520Serrano%2520and%2520Mojtaba%2520Bemana%2520and%2520Hans-Peter%2520Seidel%2520and%2520Karol%2520Myszkowski%2520and%2520Thomas%2520Leimk%25C3%25BChler%26entry.1292438233%3D%2520%2520Radiance%2520field%2520methods%2520represent%2520the%2520state%2520of%2520the%2520art%2520in%2520reconstructing%250Acomplex%2520scenes%2520from%2520multi-view%2520photos.%2520However%252C%2520these%2520reconstructions%2520often%250Asuffer%2520from%2520one%2520or%2520both%2520of%2520the%2520following%2520limitations%253A%2520First%252C%2520they%2520typically%250Arepresent%2520scenes%2520in%2520low%2520dynamic%2520range%2520%2528LDR%2529%252C%2520which%2520restricts%2520their%2520use%2520to%250Aevenly%2520lit%2520environments%2520and%2520hinders%2520immersive%2520viewing%2520experiences.%2520Secondly%252C%250Atheir%2520reliance%2520on%2520a%2520pinhole%2520camera%2520model%252C%2520assuming%2520all%2520scene%2520elements%2520are%2520in%250Afocus%2520in%2520the%2520input%2520images%252C%2520presents%2520practical%2520challenges%2520and%2520complicates%250Arefocusing%2520during%2520novel-view%2520synthesis.%2520Addressing%2520these%2520limitations%252C%2520we%250Apresent%2520a%2520lightweight%2520method%2520based%2520on%25203D%2520Gaussian%2520Splatting%2520that%2520utilizes%250Amulti-view%2520LDR%2520images%2520of%2520a%2520scene%2520with%2520varying%2520exposure%2520times%252C%2520apertures%252C%2520and%250Afocus%2520distances%2520as%2520input%2520to%2520reconstruct%2520a%2520high-dynamic-range%2520%2528HDR%2529%2520radiance%250Afield.%2520By%2520incorporating%2520analytical%2520convolutions%2520of%2520Gaussians%2520based%2520on%2520a%250Athin-lens%2520camera%2520model%2520as%2520well%2520as%2520a%2520tonemapping%2520module%252C%2520our%2520reconstructions%250Aenable%2520the%2520rendering%2520of%2520HDR%2520content%2520with%2520flexible%2520refocusing%2520capabilities.%2520We%250Ademonstrate%2520that%2520our%2520combined%2520treatment%2520of%2520HDR%2520and%2520depth%2520of%2520field%2520facilitates%250Areal-time%2520cinematic%2520rendering%252C%2520outperforming%2520the%2520state%2520of%2520the%2520art.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2406.07329v4%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Cinematic%20Gaussians%3A%20Real-Time%20HDR%20Radiance%20Fields%20with%20Depth%20of%20Field&entry.906535625=Chao%20Wang%20and%20Krzysztof%20Wolski%20and%20Bernhard%20Kerbl%20and%20Ana%20Serrano%20and%20Mojtaba%20Bemana%20and%20Hans-Peter%20Seidel%20and%20Karol%20Myszkowski%20and%20Thomas%20Leimk%C3%BChler&entry.1292438233=%20%20Radiance%20field%20methods%20represent%20the%20state%20of%20the%20art%20in%20reconstructing%0Acomplex%20scenes%20from%20multi-view%20photos.%20However%2C%20these%20reconstructions%20often%0Asuffer%20from%20one%20or%20both%20of%20the%20following%20limitations%3A%20First%2C%20they%20typically%0Arepresent%20scenes%20in%20low%20dynamic%20range%20%28LDR%29%2C%20which%20restricts%20their%20use%20to%0Aevenly%20lit%20environments%20and%20hinders%20immersive%20viewing%20experiences.%20Secondly%2C%0Atheir%20reliance%20on%20a%20pinhole%20camera%20model%2C%20assuming%20all%20scene%20elements%20are%20in%0Afocus%20in%20the%20input%20images%2C%20presents%20practical%20challenges%20and%20complicates%0Arefocusing%20during%20novel-view%20synthesis.%20Addressing%20these%20limitations%2C%20we%0Apresent%20a%20lightweight%20method%20based%20on%203D%20Gaussian%20Splatting%20that%20utilizes%0Amulti-view%20LDR%20images%20of%20a%20scene%20with%20varying%20exposure%20times%2C%20apertures%2C%20and%0Afocus%20distances%20as%20input%20to%20reconstruct%20a%20high-dynamic-range%20%28HDR%29%20radiance%0Afield.%20By%20incorporating%20analytical%20convolutions%20of%20Gaussians%20based%20on%20a%0Athin-lens%20camera%20model%20as%20well%20as%20a%20tonemapping%20module%2C%20our%20reconstructions%0Aenable%20the%20rendering%20of%20HDR%20content%20with%20flexible%20refocusing%20capabilities.%20We%0Ademonstrate%20that%20our%20combined%20treatment%20of%20HDR%20and%20depth%20of%20field%20facilitates%0Areal-time%20cinematic%20rendering%2C%20outperforming%20the%20state%20of%20the%20art.%0A&entry.1838667208=http%3A//arxiv.org/abs/2406.07329v4&entry.124074799=Read"},
{"title": "OVO-SLAM: Open-Vocabulary Online Simultaneous Localization and Mapping", "author": "Tomas Berriel Martins and Martin R. Oswald and Javier Civera", "abstract": "  This paper presents the first Open-Vocabulary Online 3D semantic SLAM\npipeline, that we denote as OVO-SLAM. Our primary contribution is in the\npipeline itself, particularly in the mapping thread. Given a set of posed RGB-D\nframes, we detect and track 3D segments, which we describe using CLIP vectors,\ncalculated through a novel aggregation from the viewpoints where these 3D\nsegments are observed. Notably, our OVO-SLAM pipeline is not only faster but\nalso achieves better segmentation metrics compared to offline approaches in the\nliterature. Along with superior segmentation performance, we show experimental\nresults of our contributions integrated with Gaussian-SLAM, being the first\nones demonstrating end-to-end open-vocabulary online 3D reconstructions without\nrelying on ground-truth camera poses or scene geometry.\n", "link": "http://arxiv.org/abs/2411.15043v1", "date": "2024-11-22", "relevancy": 3.0914, "topK": [{"title": "RTG-SLAM: Real-time 3D Reconstruction at Scale using Gaussian Splatting", "link": "http://arxiv.org/abs/2404.19706v1", "similarity": 0.6446}, {"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.6236}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5867}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20OVO-SLAM%3A%20Open-Vocabulary%20Online%20Simultaneous%20Localization%20and%20Mapping&body=Title%3A%20OVO-SLAM%3A%20Open-Vocabulary%20Online%20Simultaneous%20Localization%20and%20Mapping%0AAuthor%3A%20Tomas%20Berriel%20Martins%20and%20Martin%20R.%20Oswald%20and%20Javier%20Civera%0AAbstract%3A%20%20%20This%20paper%20presents%20the%20first%20Open-Vocabulary%20Online%203D%20semantic%20SLAM%0Apipeline%2C%20that%20we%20denote%20as%20OVO-SLAM.%20Our%20primary%20contribution%20is%20in%20the%0Apipeline%20itself%2C%20particularly%20in%20the%20mapping%20thread.%20Given%20a%20set%20of%20posed%20RGB-D%0Aframes%2C%20we%20detect%20and%20track%203D%20segments%2C%20which%20we%20describe%20using%20CLIP%20vectors%2C%0Acalculated%20through%20a%20novel%20aggregation%20from%20the%20viewpoints%20where%20these%203D%0Asegments%20are%20observed.%20Notably%2C%20our%20OVO-SLAM%20pipeline%20is%20not%20only%20faster%20but%0Aalso%20achieves%20better%20segmentation%20metrics%20compared%20to%20offline%20approaches%20in%20the%0Aliterature.%20Along%20with%20superior%20segmentation%20performance%2C%20we%20show%20experimental%0Aresults%20of%20our%20contributions%20integrated%20with%20Gaussian-SLAM%2C%20being%20the%20first%0Aones%20demonstrating%20end-to-end%20open-vocabulary%20online%203D%20reconstructions%20without%0Arelying%20on%20ground-truth%20camera%20poses%20or%20scene%20geometry.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2411.15043v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DOVO-SLAM%253A%2520Open-Vocabulary%2520Online%2520Simultaneous%2520Localization%2520and%2520Mapping%26entry.906535625%3DTomas%2520Berriel%2520Martins%2520and%2520Martin%2520R.%2520Oswald%2520and%2520Javier%2520Civera%26entry.1292438233%3D%2520%2520This%2520paper%2520presents%2520the%2520first%2520Open-Vocabulary%2520Online%25203D%2520semantic%2520SLAM%250Apipeline%252C%2520that%2520we%2520denote%2520as%2520OVO-SLAM.%2520Our%2520primary%2520contribution%2520is%2520in%2520the%250Apipeline%2520itself%252C%2520particularly%2520in%2520the%2520mapping%2520thread.%2520Given%2520a%2520set%2520of%2520posed%2520RGB-D%250Aframes%252C%2520we%2520detect%2520and%2520track%25203D%2520segments%252C%2520which%2520we%2520describe%2520using%2520CLIP%2520vectors%252C%250Acalculated%2520through%2520a%2520novel%2520aggregation%2520from%2520the%2520viewpoints%2520where%2520these%25203D%250Asegments%2520are%2520observed.%2520Notably%252C%2520our%2520OVO-SLAM%2520pipeline%2520is%2520not%2520only%2520faster%2520but%250Aalso%2520achieves%2520better%2520segmentation%2520metrics%2520compared%2520to%2520offline%2520approaches%2520in%2520the%250Aliterature.%2520Along%2520with%2520superior%2520segmentation%2520performance%252C%2520we%2520show%2520experimental%250Aresults%2520of%2520our%2520contributions%2520integrated%2520with%2520Gaussian-SLAM%252C%2520being%2520the%2520first%250Aones%2520demonstrating%2520end-to-end%2520open-vocabulary%2520online%25203D%2520reconstructions%2520without%250Arelying%2520on%2520ground-truth%2520camera%2520poses%2520or%2520scene%2520geometry.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2411.15043v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=OVO-SLAM%3A%20Open-Vocabulary%20Online%20Simultaneous%20Localization%20and%20Mapping&entry.906535625=Tomas%20Berriel%20Martins%20and%20Martin%20R.%20Oswald%20and%20Javier%20Civera&entry.1292438233=%20%20This%20paper%20presents%20the%20first%20Open-Vocabulary%20Online%203D%20semantic%20SLAM%0Apipeline%2C%20that%20we%20denote%20as%20OVO-SLAM.%20Our%20primary%20contribution%20is%20in%20the%0Apipeline%20itself%2C%20particularly%20in%20the%20mapping%20thread.%20Given%20a%20set%20of%20posed%20RGB-D%0Aframes%2C%20we%20detect%20and%20track%203D%20segments%2C%20which%20we%20describe%20using%20CLIP%20vectors%2C%0Acalculated%20through%20a%20novel%20aggregation%20from%20the%20viewpoints%20where%20these%203D%0Asegments%20are%20observed.%20Notably%2C%20our%20OVO-SLAM%20pipeline%20is%20not%20only%20faster%20but%0Aalso%20achieves%20better%20segmentation%20metrics%20compared%20to%20offline%20approaches%20in%20the%0Aliterature.%20Along%20with%20superior%20segmentation%20performance%2C%20we%20show%20experimental%0Aresults%20of%20our%20contributions%20integrated%20with%20Gaussian-SLAM%2C%20being%20the%20first%0Aones%20demonstrating%20end-to-end%20open-vocabulary%20online%203D%20reconstructions%20without%0Arelying%20on%20ground-truth%20camera%20poses%20or%20scene%20geometry.%0A&entry.1838667208=http%3A//arxiv.org/abs/2411.15043v1&entry.124074799=Read"},
{"title": "Hierarchical localization with panoramic views and triplet loss\n  functions", "author": "Marcos Alfaro and Juan Jos\u00e9 Cabrera and Mar\u00eda Flores and \u00d3scar Reinoso and Luis Pay\u00e1", "abstract": "  The main objective of this paper is to tackle visual localization, which is\nessential for the safe navigation of mobile robots. The solution we propose\nemploys panoramic images and triplet convolutional neural networks. We seek to\nexploit the properties of such architectures to address both hierarchical and\nglobal localization in indoor environments, which are prone to visual aliasing\nand other phenomena. Considering their importance in these architectures, a\ncomplete comparative evaluation of different triplet loss functions is\nperformed. The experimental section proves that triplet networks can be trained\nwith a relatively low number of images captured under a specific lighting\ncondition and even so, the resulting networks are a robust tool to perform\nvisual localization under dynamic conditions. Our approach has been evaluated\nagainst some of these effects, such as changes in the lighting conditions,\nocclusions, noise and motion blurring. Furthermore, to explore the limits of\nour approach, triplet networks have been tested in different indoor\nenvironments simultaneously. In all the cases, these architectures have\ndemonstrated a great capability to generalize to diverse and challenging\nscenarios. The code used in the experiments is available at\nhttps://github.com/MarcosAlfaro/TripletNetworksIndoorLocalization.git.\n", "link": "http://arxiv.org/abs/2404.14117v2", "date": "2024-11-22", "relevancy": 2.9931, "topK": [{"title": "DDM-NET: End-to-end learning of keypoint feature Detection, Description and Matching for 3D localization", "link": "https://arxiv.org/abs/2212.04575", "similarity": 0.6313}, {"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.6205}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5441}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Hierarchical%20localization%20with%20panoramic%20views%20and%20triplet%20loss%0A%20%20functions&body=Title%3A%20Hierarchical%20localization%20with%20panoramic%20views%20and%20triplet%20loss%0A%20%20functions%0AAuthor%3A%20Marcos%20Alfaro%20and%20Juan%20Jos%C3%A9%20Cabrera%20and%20Mar%C3%ADa%20Flores%20and%20%C3%93scar%20Reinoso%20and%20Luis%20Pay%C3%A1%0AAbstract%3A%20%20%20The%20main%20objective%20of%20this%20paper%20is%20to%20tackle%20visual%20localization%2C%20which%20is%0Aessential%20for%20the%20safe%20navigation%20of%20mobile%20robots.%20The%20solution%20we%20propose%0Aemploys%20panoramic%20images%20and%20triplet%20convolutional%20neural%20networks.%20We%20seek%20to%0Aexploit%20the%20properties%20of%20such%20architectures%20to%20address%20both%20hierarchical%20and%0Aglobal%20localization%20in%20indoor%20environments%2C%20which%20are%20prone%20to%20visual%20aliasing%0Aand%20other%20phenomena.%20Considering%20their%20importance%20in%20these%20architectures%2C%20a%0Acomplete%20comparative%20evaluation%20of%20different%20triplet%20loss%20functions%20is%0Aperformed.%20The%20experimental%20section%20proves%20that%20triplet%20networks%20can%20be%20trained%0Awith%20a%20relatively%20low%20number%20of%20images%20captured%20under%20a%20specific%20lighting%0Acondition%20and%20even%20so%2C%20the%20resulting%20networks%20are%20a%20robust%20tool%20to%20perform%0Avisual%20localization%20under%20dynamic%20conditions.%20Our%20approach%20has%20been%20evaluated%0Aagainst%20some%20of%20these%20effects%2C%20such%20as%20changes%20in%20the%20lighting%20conditions%2C%0Aocclusions%2C%20noise%20and%20motion%20blurring.%20Furthermore%2C%20to%20explore%20the%20limits%20of%0Aour%20approach%2C%20triplet%20networks%20have%20been%20tested%20in%20different%20indoor%0Aenvironments%20simultaneously.%20In%20all%20the%20cases%2C%20these%20architectures%20have%0Ademonstrated%20a%20great%20capability%20to%20generalize%20to%20diverse%20and%20challenging%0Ascenarios.%20The%20code%20used%20in%20the%20experiments%20is%20available%20at%0Ahttps%3A//github.com/MarcosAlfaro/TripletNetworksIndoorLocalization.git.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2404.14117v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DHierarchical%2520localization%2520with%2520panoramic%2520views%2520and%2520triplet%2520loss%250A%2520%2520functions%26entry.906535625%3DMarcos%2520Alfaro%2520and%2520Juan%2520Jos%25C3%25A9%2520Cabrera%2520and%2520Mar%25C3%25ADa%2520Flores%2520and%2520%25C3%2593scar%2520Reinoso%2520and%2520Luis%2520Pay%25C3%25A1%26entry.1292438233%3D%2520%2520The%2520main%2520objective%2520of%2520this%2520paper%2520is%2520to%2520tackle%2520visual%2520localization%252C%2520which%2520is%250Aessential%2520for%2520the%2520safe%2520navigation%2520of%2520mobile%2520robots.%2520The%2520solution%2520we%2520propose%250Aemploys%2520panoramic%2520images%2520and%2520triplet%2520convolutional%2520neural%2520networks.%2520We%2520seek%2520to%250Aexploit%2520the%2520properties%2520of%2520such%2520architectures%2520to%2520address%2520both%2520hierarchical%2520and%250Aglobal%2520localization%2520in%2520indoor%2520environments%252C%2520which%2520are%2520prone%2520to%2520visual%2520aliasing%250Aand%2520other%2520phenomena.%2520Considering%2520their%2520importance%2520in%2520these%2520architectures%252C%2520a%250Acomplete%2520comparative%2520evaluation%2520of%2520different%2520triplet%2520loss%2520functions%2520is%250Aperformed.%2520The%2520experimental%2520section%2520proves%2520that%2520triplet%2520networks%2520can%2520be%2520trained%250Awith%2520a%2520relatively%2520low%2520number%2520of%2520images%2520captured%2520under%2520a%2520specific%2520lighting%250Acondition%2520and%2520even%2520so%252C%2520the%2520resulting%2520networks%2520are%2520a%2520robust%2520tool%2520to%2520perform%250Avisual%2520localization%2520under%2520dynamic%2520conditions.%2520Our%2520approach%2520has%2520been%2520evaluated%250Aagainst%2520some%2520of%2520these%2520effects%252C%2520such%2520as%2520changes%2520in%2520the%2520lighting%2520conditions%252C%250Aocclusions%252C%2520noise%2520and%2520motion%2520blurring.%2520Furthermore%252C%2520to%2520explore%2520the%2520limits%2520of%250Aour%2520approach%252C%2520triplet%2520networks%2520have%2520been%2520tested%2520in%2520different%2520indoor%250Aenvironments%2520simultaneously.%2520In%2520all%2520the%2520cases%252C%2520these%2520architectures%2520have%250Ademonstrated%2520a%2520great%2520capability%2520to%2520generalize%2520to%2520diverse%2520and%2520challenging%250Ascenarios.%2520The%2520code%2520used%2520in%2520the%2520experiments%2520is%2520available%2520at%250Ahttps%253A//github.com/MarcosAlfaro/TripletNetworksIndoorLocalization.git.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2404.14117v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Hierarchical%20localization%20with%20panoramic%20views%20and%20triplet%20loss%0A%20%20functions&entry.906535625=Marcos%20Alfaro%20and%20Juan%20Jos%C3%A9%20Cabrera%20and%20Mar%C3%ADa%20Flores%20and%20%C3%93scar%20Reinoso%20and%20Luis%20Pay%C3%A1&entry.1292438233=%20%20The%20main%20objective%20of%20this%20paper%20is%20to%20tackle%20visual%20localization%2C%20which%20is%0Aessential%20for%20the%20safe%20navigation%20of%20mobile%20robots.%20The%20solution%20we%20propose%0Aemploys%20panoramic%20images%20and%20triplet%20convolutional%20neural%20networks.%20We%20seek%20to%0Aexploit%20the%20properties%20of%20such%20architectures%20to%20address%20both%20hierarchical%20and%0Aglobal%20localization%20in%20indoor%20environments%2C%20which%20are%20prone%20to%20visual%20aliasing%0Aand%20other%20phenomena.%20Considering%20their%20importance%20in%20these%20architectures%2C%20a%0Acomplete%20comparative%20evaluation%20of%20different%20triplet%20loss%20functions%20is%0Aperformed.%20The%20experimental%20section%20proves%20that%20triplet%20networks%20can%20be%20trained%0Awith%20a%20relatively%20low%20number%20of%20images%20captured%20under%20a%20specific%20lighting%0Acondition%20and%20even%20so%2C%20the%20resulting%20networks%20are%20a%20robust%20tool%20to%20perform%0Avisual%20localization%20under%20dynamic%20conditions.%20Our%20approach%20has%20been%20evaluated%0Aagainst%20some%20of%20these%20effects%2C%20such%20as%20changes%20in%20the%20lighting%20conditions%2C%0Aocclusions%2C%20noise%20and%20motion%20blurring.%20Furthermore%2C%20to%20explore%20the%20limits%20of%0Aour%20approach%2C%20triplet%20networks%20have%20been%20tested%20in%20different%20indoor%0Aenvironments%20simultaneously.%20In%20all%20the%20cases%2C%20these%20architectures%20have%0Ademonstrated%20a%20great%20capability%20to%20generalize%20to%20diverse%20and%20challenging%0Ascenarios.%20The%20code%20used%20in%20the%20experiments%20is%20available%20at%0Ahttps%3A//github.com/MarcosAlfaro/TripletNetworksIndoorLocalization.git.%0A&entry.1838667208=http%3A//arxiv.org/abs/2404.14117v2&entry.124074799=Read"},
{"title": "Neural 4D Evolution under Large Topological Changes from 2D Images", "author": "AmirHossein Naghi Razlighi and Tiago Novello and Asen Nachkov and Thomas Probst and Danda Paudel", "abstract": "  In the literature, it has been shown that the evolution of the known explicit\n3D surface to the target one can be learned from 2D images using the\ninstantaneous flow field, where the known and target 3D surfaces may largely\ndiffer in topology. We are interested in capturing 4D shapes whose topology\nchanges largely over time. We encounter that the straightforward extension of\nthe existing 3D-based method to the desired 4D case performs poorly.\n  In this work, we address the challenges in extending 3D neural evolution to\n4D under large topological changes by proposing two novel modifications. More\nprecisely, we introduce (i) a new architecture to discretize and encode the\ndeformation and learn the SDF and (ii) a technique to impose the temporal\nconsistency. (iii) Also, we propose a rendering scheme for color prediction\nbased on Gaussian splatting. Furthermore, to facilitate learning directly from\n2D images, we propose a learning framework that can disentangle the geometry\nand appearance from RGB images. This method of disentanglement, while also\nuseful for the 4D evolution problem that we are concentrating on, is also novel\nand valid for static scenes. Our extensive experiments on various data provide\nawesome results and, most importantly, open a new approach toward\nreconstructing challenging scenes with significant topological changes and\ndeformations. Our source code and the dataset are publicly available at\nhttps://github.com/insait-institute/N4DE.\n", "link": "http://arxiv.org/abs/2411.15018v1", "date": "2024-11-22", "relevancy": 2.9885, "topK": [{"title": "MiraGe: Editable 2D Images using Gaussian Splatting", "link": "http://arxiv.org/abs/2410.01521v1", "similarity": 0.6117}, {"title": "RTG-SLAM: Real-time 3D Reconstruction at Scale using Gaussian Splatting", "link": "http://arxiv.org/abs/2404.19706v1", "similarity": 0.5942}, {"title": "CAT3D: Create Anything in 3D with Multi-View Diffusion Models", "link": "http://arxiv.org/abs/2405.10314v1", "similarity": 0.5872}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Neural%204D%20Evolution%20under%20Large%20Topological%20Changes%20from%202D%20Images&body=Title%3A%20Neural%204D%20Evolution%20under%20Large%20Topological%20Changes%20from%202D%20Images%0AAuthor%3A%20AmirHossein%20Naghi%20Razlighi%20and%20Tiago%20Novello%20and%20Asen%20Nachkov%20and%20Thomas%20Probst%20and%20Danda%20Paudel%0AAbstract%3A%20%20%20In%20the%20literature%2C%20it%20has%20been%20shown%20that%20the%20evolution%20of%20the%20known%20explicit%0A3D%20surface%20to%20the%20target%20one%20can%20be%20learned%20from%202D%20images%20using%20the%0Ainstantaneous%20flow%20field%2C%20where%20the%20known%20and%20target%203D%20surfaces%20may%20largely%0Adiffer%20in%20topology.%20We%20are%20interested%20in%20capturing%204D%20shapes%20whose%20topology%0Achanges%20largely%20over%20time.%20We%20encounter%20that%20the%20straightforward%20extension%20of%0Athe%20existing%203D-based%20method%20to%20the%20desired%204D%20case%20performs%20poorly.%0A%20%20In%20this%20work%2C%20we%20address%20the%20challenges%20in%20extending%203D%20neural%20evolution%20to%0A4D%20under%20large%20topological%20changes%20by%20proposing%20two%20novel%20modifications.%20More%0Aprecisely%2C%20we%20introduce%20%28i%29%20a%20new%20architecture%20to%20discretize%20and%20encode%20the%0Adeformation%20and%20learn%20the%20SDF%20and%20%28ii%29%20a%20technique%20to%20impose%20the%20temporal%0Aconsistency.%20%28iii%29%20Also%2C%20we%20propose%20a%20rendering%20scheme%20for%20color%20prediction%0Abased%20on%20Gaussian%20splatting.%20Furthermore%2C%20to%20facilitate%20learning%20directly%20from%0A2D%20images%2C%20we%20propose%20a%20learning%20framework%20that%20can%20disentangle%20the%20geometry%0Aand%20appearance%20from%20RGB%20images.%20This%20method%20of%20disentanglement%2C%20while%20also%0Auseful%20for%20the%204D%20evolution%20problem%20that%20we%20are%20concentrating%20on%2C%20is%20also%20novel%0Aand%20valid%20for%20static%20scenes.%20Our%20extensive%20experiments%20on%20various%20data%20provide%0Aawesome%20results%20and%2C%20most%20importantly%2C%20open%20a%20new%20approach%20toward%0Areconstructing%20challenging%20scenes%20with%20significant%20topological%20changes%20and%0Adeformations.%20Our%20source%20code%20and%20the%20dataset%20are%20publicly%20available%20at%0Ahttps%3A//github.com/insait-institute/N4DE.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2411.15018v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DNeural%25204D%2520Evolution%2520under%2520Large%2520Topological%2520Changes%2520from%25202D%2520Images%26entry.906535625%3DAmirHossein%2520Naghi%2520Razlighi%2520and%2520Tiago%2520Novello%2520and%2520Asen%2520Nachkov%2520and%2520Thomas%2520Probst%2520and%2520Danda%2520Paudel%26entry.1292438233%3D%2520%2520In%2520the%2520literature%252C%2520it%2520has%2520been%2520shown%2520that%2520the%2520evolution%2520of%2520the%2520known%2520explicit%250A3D%2520surface%2520to%2520the%2520target%2520one%2520can%2520be%2520learned%2520from%25202D%2520images%2520using%2520the%250Ainstantaneous%2520flow%2520field%252C%2520where%2520the%2520known%2520and%2520target%25203D%2520surfaces%2520may%2520largely%250Adiffer%2520in%2520topology.%2520We%2520are%2520interested%2520in%2520capturing%25204D%2520shapes%2520whose%2520topology%250Achanges%2520largely%2520over%2520time.%2520We%2520encounter%2520that%2520the%2520straightforward%2520extension%2520of%250Athe%2520existing%25203D-based%2520method%2520to%2520the%2520desired%25204D%2520case%2520performs%2520poorly.%250A%2520%2520In%2520this%2520work%252C%2520we%2520address%2520the%2520challenges%2520in%2520extending%25203D%2520neural%2520evolution%2520to%250A4D%2520under%2520large%2520topological%2520changes%2520by%2520proposing%2520two%2520novel%2520modifications.%2520More%250Aprecisely%252C%2520we%2520introduce%2520%2528i%2529%2520a%2520new%2520architecture%2520to%2520discretize%2520and%2520encode%2520the%250Adeformation%2520and%2520learn%2520the%2520SDF%2520and%2520%2528ii%2529%2520a%2520technique%2520to%2520impose%2520the%2520temporal%250Aconsistency.%2520%2528iii%2529%2520Also%252C%2520we%2520propose%2520a%2520rendering%2520scheme%2520for%2520color%2520prediction%250Abased%2520on%2520Gaussian%2520splatting.%2520Furthermore%252C%2520to%2520facilitate%2520learning%2520directly%2520from%250A2D%2520images%252C%2520we%2520propose%2520a%2520learning%2520framework%2520that%2520can%2520disentangle%2520the%2520geometry%250Aand%2520appearance%2520from%2520RGB%2520images.%2520This%2520method%2520of%2520disentanglement%252C%2520while%2520also%250Auseful%2520for%2520the%25204D%2520evolution%2520problem%2520that%2520we%2520are%2520concentrating%2520on%252C%2520is%2520also%2520novel%250Aand%2520valid%2520for%2520static%2520scenes.%2520Our%2520extensive%2520experiments%2520on%2520various%2520data%2520provide%250Aawesome%2520results%2520and%252C%2520most%2520importantly%252C%2520open%2520a%2520new%2520approach%2520toward%250Areconstructing%2520challenging%2520scenes%2520with%2520significant%2520topological%2520changes%2520and%250Adeformations.%2520Our%2520source%2520code%2520and%2520the%2520dataset%2520are%2520publicly%2520available%2520at%250Ahttps%253A//github.com/insait-institute/N4DE.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2411.15018v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Neural%204D%20Evolution%20under%20Large%20Topological%20Changes%20from%202D%20Images&entry.906535625=AmirHossein%20Naghi%20Razlighi%20and%20Tiago%20Novello%20and%20Asen%20Nachkov%20and%20Thomas%20Probst%20and%20Danda%20Paudel&entry.1292438233=%20%20In%20the%20literature%2C%20it%20has%20been%20shown%20that%20the%20evolution%20of%20the%20known%20explicit%0A3D%20surface%20to%20the%20target%20one%20can%20be%20learned%20from%202D%20images%20using%20the%0Ainstantaneous%20flow%20field%2C%20where%20the%20known%20and%20target%203D%20surfaces%20may%20largely%0Adiffer%20in%20topology.%20We%20are%20interested%20in%20capturing%204D%20shapes%20whose%20topology%0Achanges%20largely%20over%20time.%20We%20encounter%20that%20the%20straightforward%20extension%20of%0Athe%20existing%203D-based%20method%20to%20the%20desired%204D%20case%20performs%20poorly.%0A%20%20In%20this%20work%2C%20we%20address%20the%20challenges%20in%20extending%203D%20neural%20evolution%20to%0A4D%20under%20large%20topological%20changes%20by%20proposing%20two%20novel%20modifications.%20More%0Aprecisely%2C%20we%20introduce%20%28i%29%20a%20new%20architecture%20to%20discretize%20and%20encode%20the%0Adeformation%20and%20learn%20the%20SDF%20and%20%28ii%29%20a%20technique%20to%20impose%20the%20temporal%0Aconsistency.%20%28iii%29%20Also%2C%20we%20propose%20a%20rendering%20scheme%20for%20color%20prediction%0Abased%20on%20Gaussian%20splatting.%20Furthermore%2C%20to%20facilitate%20learning%20directly%20from%0A2D%20images%2C%20we%20propose%20a%20learning%20framework%20that%20can%20disentangle%20the%20geometry%0Aand%20appearance%20from%20RGB%20images.%20This%20method%20of%20disentanglement%2C%20while%20also%0Auseful%20for%20the%204D%20evolution%20problem%20that%20we%20are%20concentrating%20on%2C%20is%20also%20novel%0Aand%20valid%20for%20static%20scenes.%20Our%20extensive%20experiments%20on%20various%20data%20provide%0Aawesome%20results%20and%2C%20most%20importantly%2C%20open%20a%20new%20approach%20toward%0Areconstructing%20challenging%20scenes%20with%20significant%20topological%20changes%20and%0Adeformations.%20Our%20source%20code%20and%20the%20dataset%20are%20publicly%20available%20at%0Ahttps%3A//github.com/insait-institute/N4DE.%0A&entry.1838667208=http%3A//arxiv.org/abs/2411.15018v1&entry.124074799=Read"},
{"title": "Large Multi-modal Models Can Interpret Features in Large Multi-modal\n  Models", "author": "Kaichen Zhang and Yifei Shen and Bo Li and Ziwei Liu", "abstract": "  Recent advances in Large Multimodal Models (LMMs) lead to significant\nbreakthroughs in both academia and industry. One question that arises is how\nwe, as humans, can understand their internal neural representations. This paper\ntakes an initial step towards addressing this question by presenting a\nversatile framework to identify and interpret the semantics within LMMs.\nSpecifically, 1) we first apply a Sparse Autoencoder(SAE) to disentangle the\nrepresentations into human understandable features. 2) We then present an\nautomatic interpretation framework to interpreted the open-semantic features\nlearned in SAE by the LMMs themselves. We employ this framework to analyze the\nLLaVA-NeXT-8B model using the LLaVA-OV-72B model, demonstrating that these\nfeatures can effectively steer the model's behavior. Our results contribute to\na deeper understanding of why LMMs excel in specific tasks, including EQ tests,\nand illuminate the nature of their mistakes along with potential strategies for\ntheir rectification. These findings offer new insights into the internal\nmechanisms of LMMs and suggest parallels with the cognitive processes of the\nhuman brain.\n", "link": "http://arxiv.org/abs/2411.14982v1", "date": "2024-11-22", "relevancy": 2.9823, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.6072}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.6072}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.575}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Large%20Multi-modal%20Models%20Can%20Interpret%20Features%20in%20Large%20Multi-modal%0A%20%20Models&body=Title%3A%20Large%20Multi-modal%20Models%20Can%20Interpret%20Features%20in%20Large%20Multi-modal%0A%20%20Models%0AAuthor%3A%20Kaichen%20Zhang%20and%20Yifei%20Shen%20and%20Bo%20Li%20and%20Ziwei%20Liu%0AAbstract%3A%20%20%20Recent%20advances%20in%20Large%20Multimodal%20Models%20%28LMMs%29%20lead%20to%20significant%0Abreakthroughs%20in%20both%20academia%20and%20industry.%20One%20question%20that%20arises%20is%20how%0Awe%2C%20as%20humans%2C%20can%20understand%20their%20internal%20neural%20representations.%20This%20paper%0Atakes%20an%20initial%20step%20towards%20addressing%20this%20question%20by%20presenting%20a%0Aversatile%20framework%20to%20identify%20and%20interpret%20the%20semantics%20within%20LMMs.%0ASpecifically%2C%201%29%20we%20first%20apply%20a%20Sparse%20Autoencoder%28SAE%29%20to%20disentangle%20the%0Arepresentations%20into%20human%20understandable%20features.%202%29%20We%20then%20present%20an%0Aautomatic%20interpretation%20framework%20to%20interpreted%20the%20open-semantic%20features%0Alearned%20in%20SAE%20by%20the%20LMMs%20themselves.%20We%20employ%20this%20framework%20to%20analyze%20the%0ALLaVA-NeXT-8B%20model%20using%20the%20LLaVA-OV-72B%20model%2C%20demonstrating%20that%20these%0Afeatures%20can%20effectively%20steer%20the%20model%27s%20behavior.%20Our%20results%20contribute%20to%0Aa%20deeper%20understanding%20of%20why%20LMMs%20excel%20in%20specific%20tasks%2C%20including%20EQ%20tests%2C%0Aand%20illuminate%20the%20nature%20of%20their%20mistakes%20along%20with%20potential%20strategies%20for%0Atheir%20rectification.%20These%20findings%20offer%20new%20insights%20into%20the%20internal%0Amechanisms%20of%20LMMs%20and%20suggest%20parallels%20with%20the%20cognitive%20processes%20of%20the%0Ahuman%20brain.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2411.14982v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DLarge%2520Multi-modal%2520Models%2520Can%2520Interpret%2520Features%2520in%2520Large%2520Multi-modal%250A%2520%2520Models%26entry.906535625%3DKaichen%2520Zhang%2520and%2520Yifei%2520Shen%2520and%2520Bo%2520Li%2520and%2520Ziwei%2520Liu%26entry.1292438233%3D%2520%2520Recent%2520advances%2520in%2520Large%2520Multimodal%2520Models%2520%2528LMMs%2529%2520lead%2520to%2520significant%250Abreakthroughs%2520in%2520both%2520academia%2520and%2520industry.%2520One%2520question%2520that%2520arises%2520is%2520how%250Awe%252C%2520as%2520humans%252C%2520can%2520understand%2520their%2520internal%2520neural%2520representations.%2520This%2520paper%250Atakes%2520an%2520initial%2520step%2520towards%2520addressing%2520this%2520question%2520by%2520presenting%2520a%250Aversatile%2520framework%2520to%2520identify%2520and%2520interpret%2520the%2520semantics%2520within%2520LMMs.%250ASpecifically%252C%25201%2529%2520we%2520first%2520apply%2520a%2520Sparse%2520Autoencoder%2528SAE%2529%2520to%2520disentangle%2520the%250Arepresentations%2520into%2520human%2520understandable%2520features.%25202%2529%2520We%2520then%2520present%2520an%250Aautomatic%2520interpretation%2520framework%2520to%2520interpreted%2520the%2520open-semantic%2520features%250Alearned%2520in%2520SAE%2520by%2520the%2520LMMs%2520themselves.%2520We%2520employ%2520this%2520framework%2520to%2520analyze%2520the%250ALLaVA-NeXT-8B%2520model%2520using%2520the%2520LLaVA-OV-72B%2520model%252C%2520demonstrating%2520that%2520these%250Afeatures%2520can%2520effectively%2520steer%2520the%2520model%2527s%2520behavior.%2520Our%2520results%2520contribute%2520to%250Aa%2520deeper%2520understanding%2520of%2520why%2520LMMs%2520excel%2520in%2520specific%2520tasks%252C%2520including%2520EQ%2520tests%252C%250Aand%2520illuminate%2520the%2520nature%2520of%2520their%2520mistakes%2520along%2520with%2520potential%2520strategies%2520for%250Atheir%2520rectification.%2520These%2520findings%2520offer%2520new%2520insights%2520into%2520the%2520internal%250Amechanisms%2520of%2520LMMs%2520and%2520suggest%2520parallels%2520with%2520the%2520cognitive%2520processes%2520of%2520the%250Ahuman%2520brain.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2411.14982v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Large%20Multi-modal%20Models%20Can%20Interpret%20Features%20in%20Large%20Multi-modal%0A%20%20Models&entry.906535625=Kaichen%20Zhang%20and%20Yifei%20Shen%20and%20Bo%20Li%20and%20Ziwei%20Liu&entry.1292438233=%20%20Recent%20advances%20in%20Large%20Multimodal%20Models%20%28LMMs%29%20lead%20to%20significant%0Abreakthroughs%20in%20both%20academia%20and%20industry.%20One%20question%20that%20arises%20is%20how%0Awe%2C%20as%20humans%2C%20can%20understand%20their%20internal%20neural%20representations.%20This%20paper%0Atakes%20an%20initial%20step%20towards%20addressing%20this%20question%20by%20presenting%20a%0Aversatile%20framework%20to%20identify%20and%20interpret%20the%20semantics%20within%20LMMs.%0ASpecifically%2C%201%29%20we%20first%20apply%20a%20Sparse%20Autoencoder%28SAE%29%20to%20disentangle%20the%0Arepresentations%20into%20human%20understandable%20features.%202%29%20We%20then%20present%20an%0Aautomatic%20interpretation%20framework%20to%20interpreted%20the%20open-semantic%20features%0Alearned%20in%20SAE%20by%20the%20LMMs%20themselves.%20We%20employ%20this%20framework%20to%20analyze%20the%0ALLaVA-NeXT-8B%20model%20using%20the%20LLaVA-OV-72B%20model%2C%20demonstrating%20that%20these%0Afeatures%20can%20effectively%20steer%20the%20model%27s%20behavior.%20Our%20results%20contribute%20to%0Aa%20deeper%20understanding%20of%20why%20LMMs%20excel%20in%20specific%20tasks%2C%20including%20EQ%20tests%2C%0Aand%20illuminate%20the%20nature%20of%20their%20mistakes%20along%20with%20potential%20strategies%20for%0Atheir%20rectification.%20These%20findings%20offer%20new%20insights%20into%20the%20internal%0Amechanisms%20of%20LMMs%20and%20suggest%20parallels%20with%20the%20cognitive%20processes%20of%20the%0Ahuman%20brain.%0A&entry.1838667208=http%3A//arxiv.org/abs/2411.14982v1&entry.124074799=Read"},
{"title": "BIP3D: Bridging 2D Images and 3D Perception for Embodied Intelligence", "author": "Xuewu Lin and Tianwei Lin and Lichao Huang and Hongyu Xie and Zhizhong Su", "abstract": "  In embodied intelligence systems, a key component is 3D perception algorithm,\nwhich enables agents to understand their surrounding environments. Previous\nalgorithms primarily rely on point cloud, which, despite offering precise\ngeometric information, still constrain perception performance due to inherent\nsparsity, noise, and data scarcity. In this work, we introduce a novel\nimage-centric 3D perception model, BIP3D, which leverages expressive image\nfeatures with explicit 3D position encoding to overcome the limitations of\npoint-centric methods. Specifically, we leverage pre-trained 2D vision\nfoundation models to enhance semantic understanding, and introduce a spatial\nenhancer module to improve spatial understanding. Together, these modules\nenable BIP3D to achieve multi-view, multi-modal feature fusion and end-to-end\n3D perception. In our experiments, BIP3D outperforms current state-of-the-art\nresults on the EmbodiedScan benchmark, achieving improvements of 5.69% in the\n3D detection task and 15.25% in the 3D visual grounding task.\n", "link": "http://arxiv.org/abs/2411.14869v1", "date": "2024-11-22", "relevancy": 2.979, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.6041}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.6041}, {"title": "DDM-NET: End-to-end learning of keypoint feature Detection, Description and Matching for 3D localization", "link": "https://arxiv.org/abs/2212.04575", "similarity": 0.5792}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20BIP3D%3A%20Bridging%202D%20Images%20and%203D%20Perception%20for%20Embodied%20Intelligence&body=Title%3A%20BIP3D%3A%20Bridging%202D%20Images%20and%203D%20Perception%20for%20Embodied%20Intelligence%0AAuthor%3A%20Xuewu%20Lin%20and%20Tianwei%20Lin%20and%20Lichao%20Huang%20and%20Hongyu%20Xie%20and%20Zhizhong%20Su%0AAbstract%3A%20%20%20In%20embodied%20intelligence%20systems%2C%20a%20key%20component%20is%203D%20perception%20algorithm%2C%0Awhich%20enables%20agents%20to%20understand%20their%20surrounding%20environments.%20Previous%0Aalgorithms%20primarily%20rely%20on%20point%20cloud%2C%20which%2C%20despite%20offering%20precise%0Ageometric%20information%2C%20still%20constrain%20perception%20performance%20due%20to%20inherent%0Asparsity%2C%20noise%2C%20and%20data%20scarcity.%20In%20this%20work%2C%20we%20introduce%20a%20novel%0Aimage-centric%203D%20perception%20model%2C%20BIP3D%2C%20which%20leverages%20expressive%20image%0Afeatures%20with%20explicit%203D%20position%20encoding%20to%20overcome%20the%20limitations%20of%0Apoint-centric%20methods.%20Specifically%2C%20we%20leverage%20pre-trained%202D%20vision%0Afoundation%20models%20to%20enhance%20semantic%20understanding%2C%20and%20introduce%20a%20spatial%0Aenhancer%20module%20to%20improve%20spatial%20understanding.%20Together%2C%20these%20modules%0Aenable%20BIP3D%20to%20achieve%20multi-view%2C%20multi-modal%20feature%20fusion%20and%20end-to-end%0A3D%20perception.%20In%20our%20experiments%2C%20BIP3D%20outperforms%20current%20state-of-the-art%0Aresults%20on%20the%20EmbodiedScan%20benchmark%2C%20achieving%20improvements%20of%205.69%25%20in%20the%0A3D%20detection%20task%20and%2015.25%25%20in%20the%203D%20visual%20grounding%20task.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2411.14869v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DBIP3D%253A%2520Bridging%25202D%2520Images%2520and%25203D%2520Perception%2520for%2520Embodied%2520Intelligence%26entry.906535625%3DXuewu%2520Lin%2520and%2520Tianwei%2520Lin%2520and%2520Lichao%2520Huang%2520and%2520Hongyu%2520Xie%2520and%2520Zhizhong%2520Su%26entry.1292438233%3D%2520%2520In%2520embodied%2520intelligence%2520systems%252C%2520a%2520key%2520component%2520is%25203D%2520perception%2520algorithm%252C%250Awhich%2520enables%2520agents%2520to%2520understand%2520their%2520surrounding%2520environments.%2520Previous%250Aalgorithms%2520primarily%2520rely%2520on%2520point%2520cloud%252C%2520which%252C%2520despite%2520offering%2520precise%250Ageometric%2520information%252C%2520still%2520constrain%2520perception%2520performance%2520due%2520to%2520inherent%250Asparsity%252C%2520noise%252C%2520and%2520data%2520scarcity.%2520In%2520this%2520work%252C%2520we%2520introduce%2520a%2520novel%250Aimage-centric%25203D%2520perception%2520model%252C%2520BIP3D%252C%2520which%2520leverages%2520expressive%2520image%250Afeatures%2520with%2520explicit%25203D%2520position%2520encoding%2520to%2520overcome%2520the%2520limitations%2520of%250Apoint-centric%2520methods.%2520Specifically%252C%2520we%2520leverage%2520pre-trained%25202D%2520vision%250Afoundation%2520models%2520to%2520enhance%2520semantic%2520understanding%252C%2520and%2520introduce%2520a%2520spatial%250Aenhancer%2520module%2520to%2520improve%2520spatial%2520understanding.%2520Together%252C%2520these%2520modules%250Aenable%2520BIP3D%2520to%2520achieve%2520multi-view%252C%2520multi-modal%2520feature%2520fusion%2520and%2520end-to-end%250A3D%2520perception.%2520In%2520our%2520experiments%252C%2520BIP3D%2520outperforms%2520current%2520state-of-the-art%250Aresults%2520on%2520the%2520EmbodiedScan%2520benchmark%252C%2520achieving%2520improvements%2520of%25205.69%2525%2520in%2520the%250A3D%2520detection%2520task%2520and%252015.25%2525%2520in%2520the%25203D%2520visual%2520grounding%2520task.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2411.14869v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=BIP3D%3A%20Bridging%202D%20Images%20and%203D%20Perception%20for%20Embodied%20Intelligence&entry.906535625=Xuewu%20Lin%20and%20Tianwei%20Lin%20and%20Lichao%20Huang%20and%20Hongyu%20Xie%20and%20Zhizhong%20Su&entry.1292438233=%20%20In%20embodied%20intelligence%20systems%2C%20a%20key%20component%20is%203D%20perception%20algorithm%2C%0Awhich%20enables%20agents%20to%20understand%20their%20surrounding%20environments.%20Previous%0Aalgorithms%20primarily%20rely%20on%20point%20cloud%2C%20which%2C%20despite%20offering%20precise%0Ageometric%20information%2C%20still%20constrain%20perception%20performance%20due%20to%20inherent%0Asparsity%2C%20noise%2C%20and%20data%20scarcity.%20In%20this%20work%2C%20we%20introduce%20a%20novel%0Aimage-centric%203D%20perception%20model%2C%20BIP3D%2C%20which%20leverages%20expressive%20image%0Afeatures%20with%20explicit%203D%20position%20encoding%20to%20overcome%20the%20limitations%20of%0Apoint-centric%20methods.%20Specifically%2C%20we%20leverage%20pre-trained%202D%20vision%0Afoundation%20models%20to%20enhance%20semantic%20understanding%2C%20and%20introduce%20a%20spatial%0Aenhancer%20module%20to%20improve%20spatial%20understanding.%20Together%2C%20these%20modules%0Aenable%20BIP3D%20to%20achieve%20multi-view%2C%20multi-modal%20feature%20fusion%20and%20end-to-end%0A3D%20perception.%20In%20our%20experiments%2C%20BIP3D%20outperforms%20current%20state-of-the-art%0Aresults%20on%20the%20EmbodiedScan%20benchmark%2C%20achieving%20improvements%20of%205.69%25%20in%20the%0A3D%20detection%20task%20and%2015.25%25%20in%20the%203D%20visual%20grounding%20task.%0A&entry.1838667208=http%3A//arxiv.org/abs/2411.14869v1&entry.124074799=Read"},
{"title": "MSSF: A 4D Radar and Camera Fusion Framework With Multi-Stage Sampling\n  for 3D Object Detection in Autonomous Driving", "author": "Hongsi Liu and Jun Liu and Guangfeng Jiang and Xin Jin", "abstract": "  As one of the automotive sensors that have emerged in recent years, 4D\nmillimeter-wave radar has a higher resolution than conventional 3D radar and\nprovides precise elevation measurements. But its point clouds are still sparse\nand noisy, making it challenging to meet the requirements of autonomous\ndriving. Camera, as another commonly used sensor, can capture rich semantic\ninformation. As a result, the fusion of 4D radar and camera can provide an\naffordable and robust perception solution for autonomous driving systems.\nHowever, previous radar-camera fusion methods have not yet been thoroughly\ninvestigated, resulting in a large performance gap compared to LiDAR-based\nmethods. Specifically, they ignore the feature-blurring problem and do not\ndeeply interact with image semantic information. To this end, we present a\nsimple but effective multi-stage sampling fusion (MSSF) network based on 4D\nradar and camera. On the one hand, we design a fusion block that can deeply\ninteract point cloud features with image features, and can be applied to\ncommonly used single-modal backbones in a plug-and-play manner. The fusion\nblock encompasses two types, namely, simple feature fusion (SFF) and multiscale\ndeformable feature fusion (MSDFF). The SFF is easy to implement, while the\nMSDFF has stronger fusion abilities. On the other hand, we propose a\nsemantic-guided head to perform foreground-background segmentation on voxels\nwith voxel feature re-weighting, further alleviating the problem of feature\nblurring. Extensive experiments on the View-of-Delft (VoD) and TJ4DRadset\ndatasets demonstrate the effectiveness of our MSSF. Notably, compared to\nstate-of-the-art methods, MSSF achieves a 7.0% and 4.0% improvement in 3D mean\naverage precision on the VoD and TJ4DRadSet datasets, respectively. It even\nsurpasses classical LiDAR-based methods on the VoD dataset.\n", "link": "http://arxiv.org/abs/2411.15016v1", "date": "2024-11-22", "relevancy": 2.9014, "topK": [{"title": "RTG-SLAM: Real-time 3D Reconstruction at Scale using Gaussian Splatting", "link": "http://arxiv.org/abs/2404.19706v1", "similarity": 0.5972}, {"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.5735}, {"title": "CAT3D: Create Anything in 3D with Multi-View Diffusion Models", "link": "http://arxiv.org/abs/2405.10314v1", "similarity": 0.5701}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20MSSF%3A%20A%204D%20Radar%20and%20Camera%20Fusion%20Framework%20With%20Multi-Stage%20Sampling%0A%20%20for%203D%20Object%20Detection%20in%20Autonomous%20Driving&body=Title%3A%20MSSF%3A%20A%204D%20Radar%20and%20Camera%20Fusion%20Framework%20With%20Multi-Stage%20Sampling%0A%20%20for%203D%20Object%20Detection%20in%20Autonomous%20Driving%0AAuthor%3A%20Hongsi%20Liu%20and%20Jun%20Liu%20and%20Guangfeng%20Jiang%20and%20Xin%20Jin%0AAbstract%3A%20%20%20As%20one%20of%20the%20automotive%20sensors%20that%20have%20emerged%20in%20recent%20years%2C%204D%0Amillimeter-wave%20radar%20has%20a%20higher%20resolution%20than%20conventional%203D%20radar%20and%0Aprovides%20precise%20elevation%20measurements.%20But%20its%20point%20clouds%20are%20still%20sparse%0Aand%20noisy%2C%20making%20it%20challenging%20to%20meet%20the%20requirements%20of%20autonomous%0Adriving.%20Camera%2C%20as%20another%20commonly%20used%20sensor%2C%20can%20capture%20rich%20semantic%0Ainformation.%20As%20a%20result%2C%20the%20fusion%20of%204D%20radar%20and%20camera%20can%20provide%20an%0Aaffordable%20and%20robust%20perception%20solution%20for%20autonomous%20driving%20systems.%0AHowever%2C%20previous%20radar-camera%20fusion%20methods%20have%20not%20yet%20been%20thoroughly%0Ainvestigated%2C%20resulting%20in%20a%20large%20performance%20gap%20compared%20to%20LiDAR-based%0Amethods.%20Specifically%2C%20they%20ignore%20the%20feature-blurring%20problem%20and%20do%20not%0Adeeply%20interact%20with%20image%20semantic%20information.%20To%20this%20end%2C%20we%20present%20a%0Asimple%20but%20effective%20multi-stage%20sampling%20fusion%20%28MSSF%29%20network%20based%20on%204D%0Aradar%20and%20camera.%20On%20the%20one%20hand%2C%20we%20design%20a%20fusion%20block%20that%20can%20deeply%0Ainteract%20point%20cloud%20features%20with%20image%20features%2C%20and%20can%20be%20applied%20to%0Acommonly%20used%20single-modal%20backbones%20in%20a%20plug-and-play%20manner.%20The%20fusion%0Ablock%20encompasses%20two%20types%2C%20namely%2C%20simple%20feature%20fusion%20%28SFF%29%20and%20multiscale%0Adeformable%20feature%20fusion%20%28MSDFF%29.%20The%20SFF%20is%20easy%20to%20implement%2C%20while%20the%0AMSDFF%20has%20stronger%20fusion%20abilities.%20On%20the%20other%20hand%2C%20we%20propose%20a%0Asemantic-guided%20head%20to%20perform%20foreground-background%20segmentation%20on%20voxels%0Awith%20voxel%20feature%20re-weighting%2C%20further%20alleviating%20the%20problem%20of%20feature%0Ablurring.%20Extensive%20experiments%20on%20the%20View-of-Delft%20%28VoD%29%20and%20TJ4DRadset%0Adatasets%20demonstrate%20the%20effectiveness%20of%20our%20MSSF.%20Notably%2C%20compared%20to%0Astate-of-the-art%20methods%2C%20MSSF%20achieves%20a%207.0%25%20and%204.0%25%20improvement%20in%203D%20mean%0Aaverage%20precision%20on%20the%20VoD%20and%20TJ4DRadSet%20datasets%2C%20respectively.%20It%20even%0Asurpasses%20classical%20LiDAR-based%20methods%20on%20the%20VoD%20dataset.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2411.15016v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DMSSF%253A%2520A%25204D%2520Radar%2520and%2520Camera%2520Fusion%2520Framework%2520With%2520Multi-Stage%2520Sampling%250A%2520%2520for%25203D%2520Object%2520Detection%2520in%2520Autonomous%2520Driving%26entry.906535625%3DHongsi%2520Liu%2520and%2520Jun%2520Liu%2520and%2520Guangfeng%2520Jiang%2520and%2520Xin%2520Jin%26entry.1292438233%3D%2520%2520As%2520one%2520of%2520the%2520automotive%2520sensors%2520that%2520have%2520emerged%2520in%2520recent%2520years%252C%25204D%250Amillimeter-wave%2520radar%2520has%2520a%2520higher%2520resolution%2520than%2520conventional%25203D%2520radar%2520and%250Aprovides%2520precise%2520elevation%2520measurements.%2520But%2520its%2520point%2520clouds%2520are%2520still%2520sparse%250Aand%2520noisy%252C%2520making%2520it%2520challenging%2520to%2520meet%2520the%2520requirements%2520of%2520autonomous%250Adriving.%2520Camera%252C%2520as%2520another%2520commonly%2520used%2520sensor%252C%2520can%2520capture%2520rich%2520semantic%250Ainformation.%2520As%2520a%2520result%252C%2520the%2520fusion%2520of%25204D%2520radar%2520and%2520camera%2520can%2520provide%2520an%250Aaffordable%2520and%2520robust%2520perception%2520solution%2520for%2520autonomous%2520driving%2520systems.%250AHowever%252C%2520previous%2520radar-camera%2520fusion%2520methods%2520have%2520not%2520yet%2520been%2520thoroughly%250Ainvestigated%252C%2520resulting%2520in%2520a%2520large%2520performance%2520gap%2520compared%2520to%2520LiDAR-based%250Amethods.%2520Specifically%252C%2520they%2520ignore%2520the%2520feature-blurring%2520problem%2520and%2520do%2520not%250Adeeply%2520interact%2520with%2520image%2520semantic%2520information.%2520To%2520this%2520end%252C%2520we%2520present%2520a%250Asimple%2520but%2520effective%2520multi-stage%2520sampling%2520fusion%2520%2528MSSF%2529%2520network%2520based%2520on%25204D%250Aradar%2520and%2520camera.%2520On%2520the%2520one%2520hand%252C%2520we%2520design%2520a%2520fusion%2520block%2520that%2520can%2520deeply%250Ainteract%2520point%2520cloud%2520features%2520with%2520image%2520features%252C%2520and%2520can%2520be%2520applied%2520to%250Acommonly%2520used%2520single-modal%2520backbones%2520in%2520a%2520plug-and-play%2520manner.%2520The%2520fusion%250Ablock%2520encompasses%2520two%2520types%252C%2520namely%252C%2520simple%2520feature%2520fusion%2520%2528SFF%2529%2520and%2520multiscale%250Adeformable%2520feature%2520fusion%2520%2528MSDFF%2529.%2520The%2520SFF%2520is%2520easy%2520to%2520implement%252C%2520while%2520the%250AMSDFF%2520has%2520stronger%2520fusion%2520abilities.%2520On%2520the%2520other%2520hand%252C%2520we%2520propose%2520a%250Asemantic-guided%2520head%2520to%2520perform%2520foreground-background%2520segmentation%2520on%2520voxels%250Awith%2520voxel%2520feature%2520re-weighting%252C%2520further%2520alleviating%2520the%2520problem%2520of%2520feature%250Ablurring.%2520Extensive%2520experiments%2520on%2520the%2520View-of-Delft%2520%2528VoD%2529%2520and%2520TJ4DRadset%250Adatasets%2520demonstrate%2520the%2520effectiveness%2520of%2520our%2520MSSF.%2520Notably%252C%2520compared%2520to%250Astate-of-the-art%2520methods%252C%2520MSSF%2520achieves%2520a%25207.0%2525%2520and%25204.0%2525%2520improvement%2520in%25203D%2520mean%250Aaverage%2520precision%2520on%2520the%2520VoD%2520and%2520TJ4DRadSet%2520datasets%252C%2520respectively.%2520It%2520even%250Asurpasses%2520classical%2520LiDAR-based%2520methods%2520on%2520the%2520VoD%2520dataset.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2411.15016v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=MSSF%3A%20A%204D%20Radar%20and%20Camera%20Fusion%20Framework%20With%20Multi-Stage%20Sampling%0A%20%20for%203D%20Object%20Detection%20in%20Autonomous%20Driving&entry.906535625=Hongsi%20Liu%20and%20Jun%20Liu%20and%20Guangfeng%20Jiang%20and%20Xin%20Jin&entry.1292438233=%20%20As%20one%20of%20the%20automotive%20sensors%20that%20have%20emerged%20in%20recent%20years%2C%204D%0Amillimeter-wave%20radar%20has%20a%20higher%20resolution%20than%20conventional%203D%20radar%20and%0Aprovides%20precise%20elevation%20measurements.%20But%20its%20point%20clouds%20are%20still%20sparse%0Aand%20noisy%2C%20making%20it%20challenging%20to%20meet%20the%20requirements%20of%20autonomous%0Adriving.%20Camera%2C%20as%20another%20commonly%20used%20sensor%2C%20can%20capture%20rich%20semantic%0Ainformation.%20As%20a%20result%2C%20the%20fusion%20of%204D%20radar%20and%20camera%20can%20provide%20an%0Aaffordable%20and%20robust%20perception%20solution%20for%20autonomous%20driving%20systems.%0AHowever%2C%20previous%20radar-camera%20fusion%20methods%20have%20not%20yet%20been%20thoroughly%0Ainvestigated%2C%20resulting%20in%20a%20large%20performance%20gap%20compared%20to%20LiDAR-based%0Amethods.%20Specifically%2C%20they%20ignore%20the%20feature-blurring%20problem%20and%20do%20not%0Adeeply%20interact%20with%20image%20semantic%20information.%20To%20this%20end%2C%20we%20present%20a%0Asimple%20but%20effective%20multi-stage%20sampling%20fusion%20%28MSSF%29%20network%20based%20on%204D%0Aradar%20and%20camera.%20On%20the%20one%20hand%2C%20we%20design%20a%20fusion%20block%20that%20can%20deeply%0Ainteract%20point%20cloud%20features%20with%20image%20features%2C%20and%20can%20be%20applied%20to%0Acommonly%20used%20single-modal%20backbones%20in%20a%20plug-and-play%20manner.%20The%20fusion%0Ablock%20encompasses%20two%20types%2C%20namely%2C%20simple%20feature%20fusion%20%28SFF%29%20and%20multiscale%0Adeformable%20feature%20fusion%20%28MSDFF%29.%20The%20SFF%20is%20easy%20to%20implement%2C%20while%20the%0AMSDFF%20has%20stronger%20fusion%20abilities.%20On%20the%20other%20hand%2C%20we%20propose%20a%0Asemantic-guided%20head%20to%20perform%20foreground-background%20segmentation%20on%20voxels%0Awith%20voxel%20feature%20re-weighting%2C%20further%20alleviating%20the%20problem%20of%20feature%0Ablurring.%20Extensive%20experiments%20on%20the%20View-of-Delft%20%28VoD%29%20and%20TJ4DRadset%0Adatasets%20demonstrate%20the%20effectiveness%20of%20our%20MSSF.%20Notably%2C%20compared%20to%0Astate-of-the-art%20methods%2C%20MSSF%20achieves%20a%207.0%25%20and%204.0%25%20improvement%20in%203D%20mean%0Aaverage%20precision%20on%20the%20VoD%20and%20TJ4DRadSet%20datasets%2C%20respectively.%20It%20even%0Asurpasses%20classical%20LiDAR-based%20methods%20on%20the%20VoD%20dataset.%0A&entry.1838667208=http%3A//arxiv.org/abs/2411.15016v1&entry.124074799=Read"},
{"title": "STREAM: A Universal State-Space Model for Sparse Geometric Data", "author": "Mark Sch\u00f6ne and Yash Bhisikar and Karan Bania and Khaleelulla Khan Nazeer and Christian Mayr and Anand Subramoney and David Kappel", "abstract": "  Handling sparse and unstructured geometric data, such as point clouds or\nevent-based vision, is a pressing challenge in the field of machine vision.\nRecently, sequence models such as Transformers and state-space models entered\nthe domain of geometric data. These methods require specialized preprocessing\nto create a sequential view of a set of points. Furthermore, prior works\ninvolving sequence models iterate geometric data with either uniform or learned\nstep sizes, implicitly relying on the model to infer the underlying geometric\nstructure. In this work, we propose to encode geometric structure explicitly\ninto the parameterization of a state-space model. State-space models are based\non linear dynamics governed by a one-dimensional variable such as time or a\nspatial coordinate. We exploit this dynamic variable to inject relative\ndifferences of coordinates into the step size of the state-space model. The\nresulting geometric operation computes interactions between all pairs of N\npoints in O(N) steps. Our model deploys the Mamba selective state-space model\nwith a modified CUDA kernel to efficiently map sparse geometric data to modern\nhardware. The resulting sequence model, which we call STREAM, achieves\ncompetitive results on a range of benchmarks from point-cloud classification to\nevent-based vision and audio classification. STREAM demonstrates a powerful\ninductive bias for sparse geometric data by improving the PointMamba baseline\nwhen trained from scratch on the ModelNet40 and ScanObjectNN point cloud\nanalysis datasets. It further achieves, for the first time, 100% test accuracy\non all 11 classes of the DVS128 Gestures dataset.\n", "link": "http://arxiv.org/abs/2411.12603v2", "date": "2024-11-22", "relevancy": 2.8082, "topK": [{"title": "GST: Precise 3D Human Body from a Single Image with Gaussian Splatting\n  Transformers", "link": "http://arxiv.org/abs/2409.04196v1", "similarity": 0.5677}, {"title": "Dynamic Gaussians Mesh: Consistent Mesh Reconstruction from Monocular\n  Videos", "link": "http://arxiv.org/abs/2404.12379v2", "similarity": 0.559}, {"title": "RTG-SLAM: Real-time 3D Reconstruction at Scale using Gaussian Splatting", "link": "http://arxiv.org/abs/2404.19706v1", "similarity": 0.5583}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20STREAM%3A%20A%20Universal%20State-Space%20Model%20for%20Sparse%20Geometric%20Data&body=Title%3A%20STREAM%3A%20A%20Universal%20State-Space%20Model%20for%20Sparse%20Geometric%20Data%0AAuthor%3A%20Mark%20Sch%C3%B6ne%20and%20Yash%20Bhisikar%20and%20Karan%20Bania%20and%20Khaleelulla%20Khan%20Nazeer%20and%20Christian%20Mayr%20and%20Anand%20Subramoney%20and%20David%20Kappel%0AAbstract%3A%20%20%20Handling%20sparse%20and%20unstructured%20geometric%20data%2C%20such%20as%20point%20clouds%20or%0Aevent-based%20vision%2C%20is%20a%20pressing%20challenge%20in%20the%20field%20of%20machine%20vision.%0ARecently%2C%20sequence%20models%20such%20as%20Transformers%20and%20state-space%20models%20entered%0Athe%20domain%20of%20geometric%20data.%20These%20methods%20require%20specialized%20preprocessing%0Ato%20create%20a%20sequential%20view%20of%20a%20set%20of%20points.%20Furthermore%2C%20prior%20works%0Ainvolving%20sequence%20models%20iterate%20geometric%20data%20with%20either%20uniform%20or%20learned%0Astep%20sizes%2C%20implicitly%20relying%20on%20the%20model%20to%20infer%20the%20underlying%20geometric%0Astructure.%20In%20this%20work%2C%20we%20propose%20to%20encode%20geometric%20structure%20explicitly%0Ainto%20the%20parameterization%20of%20a%20state-space%20model.%20State-space%20models%20are%20based%0Aon%20linear%20dynamics%20governed%20by%20a%20one-dimensional%20variable%20such%20as%20time%20or%20a%0Aspatial%20coordinate.%20We%20exploit%20this%20dynamic%20variable%20to%20inject%20relative%0Adifferences%20of%20coordinates%20into%20the%20step%20size%20of%20the%20state-space%20model.%20The%0Aresulting%20geometric%20operation%20computes%20interactions%20between%20all%20pairs%20of%20N%0Apoints%20in%20O%28N%29%20steps.%20Our%20model%20deploys%20the%20Mamba%20selective%20state-space%20model%0Awith%20a%20modified%20CUDA%20kernel%20to%20efficiently%20map%20sparse%20geometric%20data%20to%20modern%0Ahardware.%20The%20resulting%20sequence%20model%2C%20which%20we%20call%20STREAM%2C%20achieves%0Acompetitive%20results%20on%20a%20range%20of%20benchmarks%20from%20point-cloud%20classification%20to%0Aevent-based%20vision%20and%20audio%20classification.%20STREAM%20demonstrates%20a%20powerful%0Ainductive%20bias%20for%20sparse%20geometric%20data%20by%20improving%20the%20PointMamba%20baseline%0Awhen%20trained%20from%20scratch%20on%20the%20ModelNet40%20and%20ScanObjectNN%20point%20cloud%0Aanalysis%20datasets.%20It%20further%20achieves%2C%20for%20the%20first%20time%2C%20100%25%20test%20accuracy%0Aon%20all%2011%20classes%20of%20the%20DVS128%20Gestures%20dataset.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2411.12603v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DSTREAM%253A%2520A%2520Universal%2520State-Space%2520Model%2520for%2520Sparse%2520Geometric%2520Data%26entry.906535625%3DMark%2520Sch%25C3%25B6ne%2520and%2520Yash%2520Bhisikar%2520and%2520Karan%2520Bania%2520and%2520Khaleelulla%2520Khan%2520Nazeer%2520and%2520Christian%2520Mayr%2520and%2520Anand%2520Subramoney%2520and%2520David%2520Kappel%26entry.1292438233%3D%2520%2520Handling%2520sparse%2520and%2520unstructured%2520geometric%2520data%252C%2520such%2520as%2520point%2520clouds%2520or%250Aevent-based%2520vision%252C%2520is%2520a%2520pressing%2520challenge%2520in%2520the%2520field%2520of%2520machine%2520vision.%250ARecently%252C%2520sequence%2520models%2520such%2520as%2520Transformers%2520and%2520state-space%2520models%2520entered%250Athe%2520domain%2520of%2520geometric%2520data.%2520These%2520methods%2520require%2520specialized%2520preprocessing%250Ato%2520create%2520a%2520sequential%2520view%2520of%2520a%2520set%2520of%2520points.%2520Furthermore%252C%2520prior%2520works%250Ainvolving%2520sequence%2520models%2520iterate%2520geometric%2520data%2520with%2520either%2520uniform%2520or%2520learned%250Astep%2520sizes%252C%2520implicitly%2520relying%2520on%2520the%2520model%2520to%2520infer%2520the%2520underlying%2520geometric%250Astructure.%2520In%2520this%2520work%252C%2520we%2520propose%2520to%2520encode%2520geometric%2520structure%2520explicitly%250Ainto%2520the%2520parameterization%2520of%2520a%2520state-space%2520model.%2520State-space%2520models%2520are%2520based%250Aon%2520linear%2520dynamics%2520governed%2520by%2520a%2520one-dimensional%2520variable%2520such%2520as%2520time%2520or%2520a%250Aspatial%2520coordinate.%2520We%2520exploit%2520this%2520dynamic%2520variable%2520to%2520inject%2520relative%250Adifferences%2520of%2520coordinates%2520into%2520the%2520step%2520size%2520of%2520the%2520state-space%2520model.%2520The%250Aresulting%2520geometric%2520operation%2520computes%2520interactions%2520between%2520all%2520pairs%2520of%2520N%250Apoints%2520in%2520O%2528N%2529%2520steps.%2520Our%2520model%2520deploys%2520the%2520Mamba%2520selective%2520state-space%2520model%250Awith%2520a%2520modified%2520CUDA%2520kernel%2520to%2520efficiently%2520map%2520sparse%2520geometric%2520data%2520to%2520modern%250Ahardware.%2520The%2520resulting%2520sequence%2520model%252C%2520which%2520we%2520call%2520STREAM%252C%2520achieves%250Acompetitive%2520results%2520on%2520a%2520range%2520of%2520benchmarks%2520from%2520point-cloud%2520classification%2520to%250Aevent-based%2520vision%2520and%2520audio%2520classification.%2520STREAM%2520demonstrates%2520a%2520powerful%250Ainductive%2520bias%2520for%2520sparse%2520geometric%2520data%2520by%2520improving%2520the%2520PointMamba%2520baseline%250Awhen%2520trained%2520from%2520scratch%2520on%2520the%2520ModelNet40%2520and%2520ScanObjectNN%2520point%2520cloud%250Aanalysis%2520datasets.%2520It%2520further%2520achieves%252C%2520for%2520the%2520first%2520time%252C%2520100%2525%2520test%2520accuracy%250Aon%2520all%252011%2520classes%2520of%2520the%2520DVS128%2520Gestures%2520dataset.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2411.12603v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=STREAM%3A%20A%20Universal%20State-Space%20Model%20for%20Sparse%20Geometric%20Data&entry.906535625=Mark%20Sch%C3%B6ne%20and%20Yash%20Bhisikar%20and%20Karan%20Bania%20and%20Khaleelulla%20Khan%20Nazeer%20and%20Christian%20Mayr%20and%20Anand%20Subramoney%20and%20David%20Kappel&entry.1292438233=%20%20Handling%20sparse%20and%20unstructured%20geometric%20data%2C%20such%20as%20point%20clouds%20or%0Aevent-based%20vision%2C%20is%20a%20pressing%20challenge%20in%20the%20field%20of%20machine%20vision.%0ARecently%2C%20sequence%20models%20such%20as%20Transformers%20and%20state-space%20models%20entered%0Athe%20domain%20of%20geometric%20data.%20These%20methods%20require%20specialized%20preprocessing%0Ato%20create%20a%20sequential%20view%20of%20a%20set%20of%20points.%20Furthermore%2C%20prior%20works%0Ainvolving%20sequence%20models%20iterate%20geometric%20data%20with%20either%20uniform%20or%20learned%0Astep%20sizes%2C%20implicitly%20relying%20on%20the%20model%20to%20infer%20the%20underlying%20geometric%0Astructure.%20In%20this%20work%2C%20we%20propose%20to%20encode%20geometric%20structure%20explicitly%0Ainto%20the%20parameterization%20of%20a%20state-space%20model.%20State-space%20models%20are%20based%0Aon%20linear%20dynamics%20governed%20by%20a%20one-dimensional%20variable%20such%20as%20time%20or%20a%0Aspatial%20coordinate.%20We%20exploit%20this%20dynamic%20variable%20to%20inject%20relative%0Adifferences%20of%20coordinates%20into%20the%20step%20size%20of%20the%20state-space%20model.%20The%0Aresulting%20geometric%20operation%20computes%20interactions%20between%20all%20pairs%20of%20N%0Apoints%20in%20O%28N%29%20steps.%20Our%20model%20deploys%20the%20Mamba%20selective%20state-space%20model%0Awith%20a%20modified%20CUDA%20kernel%20to%20efficiently%20map%20sparse%20geometric%20data%20to%20modern%0Ahardware.%20The%20resulting%20sequence%20model%2C%20which%20we%20call%20STREAM%2C%20achieves%0Acompetitive%20results%20on%20a%20range%20of%20benchmarks%20from%20point-cloud%20classification%20to%0Aevent-based%20vision%20and%20audio%20classification.%20STREAM%20demonstrates%20a%20powerful%0Ainductive%20bias%20for%20sparse%20geometric%20data%20by%20improving%20the%20PointMamba%20baseline%0Awhen%20trained%20from%20scratch%20on%20the%20ModelNet40%20and%20ScanObjectNN%20point%20cloud%0Aanalysis%20datasets.%20It%20further%20achieves%2C%20for%20the%20first%20time%2C%20100%25%20test%20accuracy%0Aon%20all%2011%20classes%20of%20the%20DVS128%20Gestures%20dataset.%0A&entry.1838667208=http%3A//arxiv.org/abs/2411.12603v2&entry.124074799=Read"},
{"title": "Learning to Stabilize Faces", "author": "Jan Bednarik and Erroll Wood and Vasileios Choutas and Timo Bolkart and Daoye Wang and Chenglei Wu and Thabo Beeler", "abstract": "  Nowadays, it is possible to scan faces and automatically register them with\nhigh quality. However, the resulting face meshes often need further processing:\nwe need to stabilize them to remove unwanted head movement. Stabilization is\nimportant for tasks like game development or movie making which require facial\nexpressions to be cleanly separated from rigid head motion. Since manual\nstabilization is labor-intensive, there have been attempts to automate it.\nHowever, previous methods remain impractical: they either still require some\nmanual input, produce imprecise alignments, rely on dubious heuristics and slow\noptimization, or assume a temporally ordered input. Instead, we present a new\nlearning-based approach that is simple and fully automatic. We treat\nstabilization as a regression problem: given two face meshes, our network\ndirectly predicts the rigid transform between them that brings their skulls\ninto alignment. We generate synthetic training data using a 3D Morphable Model\n(3DMM), exploiting the fact that 3DMM parameters separate skull motion from\nfacial skin motion. Through extensive experiments we show that our approach\noutperforms the state-of-the-art both quantitatively and qualitatively on the\ntasks of stabilizing discrete sets of facial expressions as well as dynamic\nfacial performances. Furthermore, we provide an ablation study detailing the\ndesign choices and best practices to help others adopt our approach for their\nown uses. Supplementary videos can be found on the project webpage\nsyntec-research.github.io/FaceStab.\n", "link": "http://arxiv.org/abs/2411.15074v1", "date": "2024-11-22", "relevancy": 2.8061, "topK": [{"title": "Dynamic Gaussians Mesh: Consistent Mesh Reconstruction from Monocular\n  Videos", "link": "http://arxiv.org/abs/2404.12379v2", "similarity": 0.5674}, {"title": "3D Gaussian Blendshapes for Head Avatar Animation", "link": "http://arxiv.org/abs/2404.19398v2", "similarity": 0.5581}, {"title": "3D Gaussian Blendshapes for Head Avatar Animation", "link": "http://arxiv.org/abs/2404.19398v2", "similarity": 0.5581}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Learning%20to%20Stabilize%20Faces&body=Title%3A%20Learning%20to%20Stabilize%20Faces%0AAuthor%3A%20Jan%20Bednarik%20and%20Erroll%20Wood%20and%20Vasileios%20Choutas%20and%20Timo%20Bolkart%20and%20Daoye%20Wang%20and%20Chenglei%20Wu%20and%20Thabo%20Beeler%0AAbstract%3A%20%20%20Nowadays%2C%20it%20is%20possible%20to%20scan%20faces%20and%20automatically%20register%20them%20with%0Ahigh%20quality.%20However%2C%20the%20resulting%20face%20meshes%20often%20need%20further%20processing%3A%0Awe%20need%20to%20stabilize%20them%20to%20remove%20unwanted%20head%20movement.%20Stabilization%20is%0Aimportant%20for%20tasks%20like%20game%20development%20or%20movie%20making%20which%20require%20facial%0Aexpressions%20to%20be%20cleanly%20separated%20from%20rigid%20head%20motion.%20Since%20manual%0Astabilization%20is%20labor-intensive%2C%20there%20have%20been%20attempts%20to%20automate%20it.%0AHowever%2C%20previous%20methods%20remain%20impractical%3A%20they%20either%20still%20require%20some%0Amanual%20input%2C%20produce%20imprecise%20alignments%2C%20rely%20on%20dubious%20heuristics%20and%20slow%0Aoptimization%2C%20or%20assume%20a%20temporally%20ordered%20input.%20Instead%2C%20we%20present%20a%20new%0Alearning-based%20approach%20that%20is%20simple%20and%20fully%20automatic.%20We%20treat%0Astabilization%20as%20a%20regression%20problem%3A%20given%20two%20face%20meshes%2C%20our%20network%0Adirectly%20predicts%20the%20rigid%20transform%20between%20them%20that%20brings%20their%20skulls%0Ainto%20alignment.%20We%20generate%20synthetic%20training%20data%20using%20a%203D%20Morphable%20Model%0A%283DMM%29%2C%20exploiting%20the%20fact%20that%203DMM%20parameters%20separate%20skull%20motion%20from%0Afacial%20skin%20motion.%20Through%20extensive%20experiments%20we%20show%20that%20our%20approach%0Aoutperforms%20the%20state-of-the-art%20both%20quantitatively%20and%20qualitatively%20on%20the%0Atasks%20of%20stabilizing%20discrete%20sets%20of%20facial%20expressions%20as%20well%20as%20dynamic%0Afacial%20performances.%20Furthermore%2C%20we%20provide%20an%20ablation%20study%20detailing%20the%0Adesign%20choices%20and%20best%20practices%20to%20help%20others%20adopt%20our%20approach%20for%20their%0Aown%20uses.%20Supplementary%20videos%20can%20be%20found%20on%20the%20project%20webpage%0Asyntec-research.github.io/FaceStab.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2411.15074v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DLearning%2520to%2520Stabilize%2520Faces%26entry.906535625%3DJan%2520Bednarik%2520and%2520Erroll%2520Wood%2520and%2520Vasileios%2520Choutas%2520and%2520Timo%2520Bolkart%2520and%2520Daoye%2520Wang%2520and%2520Chenglei%2520Wu%2520and%2520Thabo%2520Beeler%26entry.1292438233%3D%2520%2520Nowadays%252C%2520it%2520is%2520possible%2520to%2520scan%2520faces%2520and%2520automatically%2520register%2520them%2520with%250Ahigh%2520quality.%2520However%252C%2520the%2520resulting%2520face%2520meshes%2520often%2520need%2520further%2520processing%253A%250Awe%2520need%2520to%2520stabilize%2520them%2520to%2520remove%2520unwanted%2520head%2520movement.%2520Stabilization%2520is%250Aimportant%2520for%2520tasks%2520like%2520game%2520development%2520or%2520movie%2520making%2520which%2520require%2520facial%250Aexpressions%2520to%2520be%2520cleanly%2520separated%2520from%2520rigid%2520head%2520motion.%2520Since%2520manual%250Astabilization%2520is%2520labor-intensive%252C%2520there%2520have%2520been%2520attempts%2520to%2520automate%2520it.%250AHowever%252C%2520previous%2520methods%2520remain%2520impractical%253A%2520they%2520either%2520still%2520require%2520some%250Amanual%2520input%252C%2520produce%2520imprecise%2520alignments%252C%2520rely%2520on%2520dubious%2520heuristics%2520and%2520slow%250Aoptimization%252C%2520or%2520assume%2520a%2520temporally%2520ordered%2520input.%2520Instead%252C%2520we%2520present%2520a%2520new%250Alearning-based%2520approach%2520that%2520is%2520simple%2520and%2520fully%2520automatic.%2520We%2520treat%250Astabilization%2520as%2520a%2520regression%2520problem%253A%2520given%2520two%2520face%2520meshes%252C%2520our%2520network%250Adirectly%2520predicts%2520the%2520rigid%2520transform%2520between%2520them%2520that%2520brings%2520their%2520skulls%250Ainto%2520alignment.%2520We%2520generate%2520synthetic%2520training%2520data%2520using%2520a%25203D%2520Morphable%2520Model%250A%25283DMM%2529%252C%2520exploiting%2520the%2520fact%2520that%25203DMM%2520parameters%2520separate%2520skull%2520motion%2520from%250Afacial%2520skin%2520motion.%2520Through%2520extensive%2520experiments%2520we%2520show%2520that%2520our%2520approach%250Aoutperforms%2520the%2520state-of-the-art%2520both%2520quantitatively%2520and%2520qualitatively%2520on%2520the%250Atasks%2520of%2520stabilizing%2520discrete%2520sets%2520of%2520facial%2520expressions%2520as%2520well%2520as%2520dynamic%250Afacial%2520performances.%2520Furthermore%252C%2520we%2520provide%2520an%2520ablation%2520study%2520detailing%2520the%250Adesign%2520choices%2520and%2520best%2520practices%2520to%2520help%2520others%2520adopt%2520our%2520approach%2520for%2520their%250Aown%2520uses.%2520Supplementary%2520videos%2520can%2520be%2520found%2520on%2520the%2520project%2520webpage%250Asyntec-research.github.io/FaceStab.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2411.15074v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Learning%20to%20Stabilize%20Faces&entry.906535625=Jan%20Bednarik%20and%20Erroll%20Wood%20and%20Vasileios%20Choutas%20and%20Timo%20Bolkart%20and%20Daoye%20Wang%20and%20Chenglei%20Wu%20and%20Thabo%20Beeler&entry.1292438233=%20%20Nowadays%2C%20it%20is%20possible%20to%20scan%20faces%20and%20automatically%20register%20them%20with%0Ahigh%20quality.%20However%2C%20the%20resulting%20face%20meshes%20often%20need%20further%20processing%3A%0Awe%20need%20to%20stabilize%20them%20to%20remove%20unwanted%20head%20movement.%20Stabilization%20is%0Aimportant%20for%20tasks%20like%20game%20development%20or%20movie%20making%20which%20require%20facial%0Aexpressions%20to%20be%20cleanly%20separated%20from%20rigid%20head%20motion.%20Since%20manual%0Astabilization%20is%20labor-intensive%2C%20there%20have%20been%20attempts%20to%20automate%20it.%0AHowever%2C%20previous%20methods%20remain%20impractical%3A%20they%20either%20still%20require%20some%0Amanual%20input%2C%20produce%20imprecise%20alignments%2C%20rely%20on%20dubious%20heuristics%20and%20slow%0Aoptimization%2C%20or%20assume%20a%20temporally%20ordered%20input.%20Instead%2C%20we%20present%20a%20new%0Alearning-based%20approach%20that%20is%20simple%20and%20fully%20automatic.%20We%20treat%0Astabilization%20as%20a%20regression%20problem%3A%20given%20two%20face%20meshes%2C%20our%20network%0Adirectly%20predicts%20the%20rigid%20transform%20between%20them%20that%20brings%20their%20skulls%0Ainto%20alignment.%20We%20generate%20synthetic%20training%20data%20using%20a%203D%20Morphable%20Model%0A%283DMM%29%2C%20exploiting%20the%20fact%20that%203DMM%20parameters%20separate%20skull%20motion%20from%0Afacial%20skin%20motion.%20Through%20extensive%20experiments%20we%20show%20that%20our%20approach%0Aoutperforms%20the%20state-of-the-art%20both%20quantitatively%20and%20qualitatively%20on%20the%0Atasks%20of%20stabilizing%20discrete%20sets%20of%20facial%20expressions%20as%20well%20as%20dynamic%0Afacial%20performances.%20Furthermore%2C%20we%20provide%20an%20ablation%20study%20detailing%20the%0Adesign%20choices%20and%20best%20practices%20to%20help%20others%20adopt%20our%20approach%20for%20their%0Aown%20uses.%20Supplementary%20videos%20can%20be%20found%20on%20the%20project%20webpage%0Asyntec-research.github.io/FaceStab.%0A&entry.1838667208=http%3A//arxiv.org/abs/2411.15074v1&entry.124074799=Read"},
{"title": "VisGraphVar: A Benchmark Generator for Assessing Variability in Graph\n  Analysis Using Large Vision-Language Models", "author": "Camilo Chac\u00f3n Sartori and Christian Blum and Filippo Bistaffa", "abstract": "  The fast advancement of Large Vision-Language Models (LVLMs) has shown\nimmense potential. These models are increasingly capable of tackling abstract\nvisual tasks. Geometric structures, particularly graphs with their inherent\nflexibility and complexity, serve as an excellent benchmark for evaluating\nthese models' predictive capabilities. While human observers can readily\nidentify subtle visual details and perform accurate analyses, our investigation\nreveals that state-of-the-art LVLMs exhibit consistent limitations in specific\nvisual graph scenarios, especially when confronted with stylistic variations.\nIn response to these challenges, we introduce VisGraphVar (Visual Graph\nVariability), a customizable benchmark generator able to produce graph images\nfor seven distinct task categories (detection, classification, segmentation,\npattern recognition, link prediction, reasoning, matching), designed to\nsystematically evaluate the strengths and limitations of individual LVLMs. We\nuse VisGraphVar to produce 990 graph images and evaluate six LVLMs, employing\ntwo distinct prompting strategies, namely zero-shot and chain-of-thought. The\nfindings demonstrate that variations in visual attributes of images (e.g., node\nlabeling and layout) and the deliberate inclusion of visual imperfections, such\nas overlapping nodes, significantly affect model performance. This research\nemphasizes the importance of a comprehensive evaluation across graph-related\ntasks, extending beyond reasoning alone. VisGraphVar offers valuable insights\nto guide the development of more reliable and robust systems capable of\nperforming advanced visual graph analysis.\n", "link": "http://arxiv.org/abs/2411.14832v1", "date": "2024-11-22", "relevancy": 2.7952, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5718}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5718}, {"title": "Self-supervised Photographic Image Layout Representation Learning", "link": "http://arxiv.org/abs/2403.03740v1", "similarity": 0.5335}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20VisGraphVar%3A%20A%20Benchmark%20Generator%20for%20Assessing%20Variability%20in%20Graph%0A%20%20Analysis%20Using%20Large%20Vision-Language%20Models&body=Title%3A%20VisGraphVar%3A%20A%20Benchmark%20Generator%20for%20Assessing%20Variability%20in%20Graph%0A%20%20Analysis%20Using%20Large%20Vision-Language%20Models%0AAuthor%3A%20Camilo%20Chac%C3%B3n%20Sartori%20and%20Christian%20Blum%20and%20Filippo%20Bistaffa%0AAbstract%3A%20%20%20The%20fast%20advancement%20of%20Large%20Vision-Language%20Models%20%28LVLMs%29%20has%20shown%0Aimmense%20potential.%20These%20models%20are%20increasingly%20capable%20of%20tackling%20abstract%0Avisual%20tasks.%20Geometric%20structures%2C%20particularly%20graphs%20with%20their%20inherent%0Aflexibility%20and%20complexity%2C%20serve%20as%20an%20excellent%20benchmark%20for%20evaluating%0Athese%20models%27%20predictive%20capabilities.%20While%20human%20observers%20can%20readily%0Aidentify%20subtle%20visual%20details%20and%20perform%20accurate%20analyses%2C%20our%20investigation%0Areveals%20that%20state-of-the-art%20LVLMs%20exhibit%20consistent%20limitations%20in%20specific%0Avisual%20graph%20scenarios%2C%20especially%20when%20confronted%20with%20stylistic%20variations.%0AIn%20response%20to%20these%20challenges%2C%20we%20introduce%20VisGraphVar%20%28Visual%20Graph%0AVariability%29%2C%20a%20customizable%20benchmark%20generator%20able%20to%20produce%20graph%20images%0Afor%20seven%20distinct%20task%20categories%20%28detection%2C%20classification%2C%20segmentation%2C%0Apattern%20recognition%2C%20link%20prediction%2C%20reasoning%2C%20matching%29%2C%20designed%20to%0Asystematically%20evaluate%20the%20strengths%20and%20limitations%20of%20individual%20LVLMs.%20We%0Ause%20VisGraphVar%20to%20produce%20990%20graph%20images%20and%20evaluate%20six%20LVLMs%2C%20employing%0Atwo%20distinct%20prompting%20strategies%2C%20namely%20zero-shot%20and%20chain-of-thought.%20The%0Afindings%20demonstrate%20that%20variations%20in%20visual%20attributes%20of%20images%20%28e.g.%2C%20node%0Alabeling%20and%20layout%29%20and%20the%20deliberate%20inclusion%20of%20visual%20imperfections%2C%20such%0Aas%20overlapping%20nodes%2C%20significantly%20affect%20model%20performance.%20This%20research%0Aemphasizes%20the%20importance%20of%20a%20comprehensive%20evaluation%20across%20graph-related%0Atasks%2C%20extending%20beyond%20reasoning%20alone.%20VisGraphVar%20offers%20valuable%20insights%0Ato%20guide%20the%20development%20of%20more%20reliable%20and%20robust%20systems%20capable%20of%0Aperforming%20advanced%20visual%20graph%20analysis.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2411.14832v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DVisGraphVar%253A%2520A%2520Benchmark%2520Generator%2520for%2520Assessing%2520Variability%2520in%2520Graph%250A%2520%2520Analysis%2520Using%2520Large%2520Vision-Language%2520Models%26entry.906535625%3DCamilo%2520Chac%25C3%25B3n%2520Sartori%2520and%2520Christian%2520Blum%2520and%2520Filippo%2520Bistaffa%26entry.1292438233%3D%2520%2520The%2520fast%2520advancement%2520of%2520Large%2520Vision-Language%2520Models%2520%2528LVLMs%2529%2520has%2520shown%250Aimmense%2520potential.%2520These%2520models%2520are%2520increasingly%2520capable%2520of%2520tackling%2520abstract%250Avisual%2520tasks.%2520Geometric%2520structures%252C%2520particularly%2520graphs%2520with%2520their%2520inherent%250Aflexibility%2520and%2520complexity%252C%2520serve%2520as%2520an%2520excellent%2520benchmark%2520for%2520evaluating%250Athese%2520models%2527%2520predictive%2520capabilities.%2520While%2520human%2520observers%2520can%2520readily%250Aidentify%2520subtle%2520visual%2520details%2520and%2520perform%2520accurate%2520analyses%252C%2520our%2520investigation%250Areveals%2520that%2520state-of-the-art%2520LVLMs%2520exhibit%2520consistent%2520limitations%2520in%2520specific%250Avisual%2520graph%2520scenarios%252C%2520especially%2520when%2520confronted%2520with%2520stylistic%2520variations.%250AIn%2520response%2520to%2520these%2520challenges%252C%2520we%2520introduce%2520VisGraphVar%2520%2528Visual%2520Graph%250AVariability%2529%252C%2520a%2520customizable%2520benchmark%2520generator%2520able%2520to%2520produce%2520graph%2520images%250Afor%2520seven%2520distinct%2520task%2520categories%2520%2528detection%252C%2520classification%252C%2520segmentation%252C%250Apattern%2520recognition%252C%2520link%2520prediction%252C%2520reasoning%252C%2520matching%2529%252C%2520designed%2520to%250Asystematically%2520evaluate%2520the%2520strengths%2520and%2520limitations%2520of%2520individual%2520LVLMs.%2520We%250Ause%2520VisGraphVar%2520to%2520produce%2520990%2520graph%2520images%2520and%2520evaluate%2520six%2520LVLMs%252C%2520employing%250Atwo%2520distinct%2520prompting%2520strategies%252C%2520namely%2520zero-shot%2520and%2520chain-of-thought.%2520The%250Afindings%2520demonstrate%2520that%2520variations%2520in%2520visual%2520attributes%2520of%2520images%2520%2528e.g.%252C%2520node%250Alabeling%2520and%2520layout%2529%2520and%2520the%2520deliberate%2520inclusion%2520of%2520visual%2520imperfections%252C%2520such%250Aas%2520overlapping%2520nodes%252C%2520significantly%2520affect%2520model%2520performance.%2520This%2520research%250Aemphasizes%2520the%2520importance%2520of%2520a%2520comprehensive%2520evaluation%2520across%2520graph-related%250Atasks%252C%2520extending%2520beyond%2520reasoning%2520alone.%2520VisGraphVar%2520offers%2520valuable%2520insights%250Ato%2520guide%2520the%2520development%2520of%2520more%2520reliable%2520and%2520robust%2520systems%2520capable%2520of%250Aperforming%2520advanced%2520visual%2520graph%2520analysis.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2411.14832v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=VisGraphVar%3A%20A%20Benchmark%20Generator%20for%20Assessing%20Variability%20in%20Graph%0A%20%20Analysis%20Using%20Large%20Vision-Language%20Models&entry.906535625=Camilo%20Chac%C3%B3n%20Sartori%20and%20Christian%20Blum%20and%20Filippo%20Bistaffa&entry.1292438233=%20%20The%20fast%20advancement%20of%20Large%20Vision-Language%20Models%20%28LVLMs%29%20has%20shown%0Aimmense%20potential.%20These%20models%20are%20increasingly%20capable%20of%20tackling%20abstract%0Avisual%20tasks.%20Geometric%20structures%2C%20particularly%20graphs%20with%20their%20inherent%0Aflexibility%20and%20complexity%2C%20serve%20as%20an%20excellent%20benchmark%20for%20evaluating%0Athese%20models%27%20predictive%20capabilities.%20While%20human%20observers%20can%20readily%0Aidentify%20subtle%20visual%20details%20and%20perform%20accurate%20analyses%2C%20our%20investigation%0Areveals%20that%20state-of-the-art%20LVLMs%20exhibit%20consistent%20limitations%20in%20specific%0Avisual%20graph%20scenarios%2C%20especially%20when%20confronted%20with%20stylistic%20variations.%0AIn%20response%20to%20these%20challenges%2C%20we%20introduce%20VisGraphVar%20%28Visual%20Graph%0AVariability%29%2C%20a%20customizable%20benchmark%20generator%20able%20to%20produce%20graph%20images%0Afor%20seven%20distinct%20task%20categories%20%28detection%2C%20classification%2C%20segmentation%2C%0Apattern%20recognition%2C%20link%20prediction%2C%20reasoning%2C%20matching%29%2C%20designed%20to%0Asystematically%20evaluate%20the%20strengths%20and%20limitations%20of%20individual%20LVLMs.%20We%0Ause%20VisGraphVar%20to%20produce%20990%20graph%20images%20and%20evaluate%20six%20LVLMs%2C%20employing%0Atwo%20distinct%20prompting%20strategies%2C%20namely%20zero-shot%20and%20chain-of-thought.%20The%0Afindings%20demonstrate%20that%20variations%20in%20visual%20attributes%20of%20images%20%28e.g.%2C%20node%0Alabeling%20and%20layout%29%20and%20the%20deliberate%20inclusion%20of%20visual%20imperfections%2C%20such%0Aas%20overlapping%20nodes%2C%20significantly%20affect%20model%20performance.%20This%20research%0Aemphasizes%20the%20importance%20of%20a%20comprehensive%20evaluation%20across%20graph-related%0Atasks%2C%20extending%20beyond%20reasoning%20alone.%20VisGraphVar%20offers%20valuable%20insights%0Ato%20guide%20the%20development%20of%20more%20reliable%20and%20robust%20systems%20capable%20of%0Aperforming%20advanced%20visual%20graph%20analysis.%0A&entry.1838667208=http%3A//arxiv.org/abs/2411.14832v1&entry.124074799=Read"},
{"title": "Context-Aware Multimodal Pretraining", "author": "Karsten Roth and Zeynep Akata and Dima Damen and Ivana Bala\u017eevi\u0107 and Olivier J. H\u00e9naff", "abstract": "  Large-scale multimodal representation learning successfully optimizes for\nzero-shot transfer at test time. Yet the standard pretraining paradigm\n(contrastive learning on large amounts of image-text data) does not explicitly\nencourage representations to support few-shot adaptation. In this work, we\npropose a simple, but carefully designed extension to multimodal pretraining\nwhich enables representations to accommodate additional context. Using this\nobjective, we show that vision-language models can be trained to exhibit\nsignificantly increased few-shot adaptation: across 21 downstream tasks, we\nfind up to four-fold improvements in test-time sample efficiency, and average\nfew-shot adaptation gains of over 5%, while retaining zero-shot generalization\nperformance across model scales and training durations. In particular, equipped\nwith simple, training-free, metric-based adaptation mechanisms, our\nrepresentations easily surpass more complex and expensive optimization-based\nschemes, vastly simplifying generalization to new domains.\n", "link": "http://arxiv.org/abs/2411.15099v1", "date": "2024-11-22", "relevancy": 2.7852, "topK": [{"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.6178}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5266}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5266}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Context-Aware%20Multimodal%20Pretraining&body=Title%3A%20Context-Aware%20Multimodal%20Pretraining%0AAuthor%3A%20Karsten%20Roth%20and%20Zeynep%20Akata%20and%20Dima%20Damen%20and%20Ivana%20Bala%C5%BEevi%C4%87%20and%20Olivier%20J.%20H%C3%A9naff%0AAbstract%3A%20%20%20Large-scale%20multimodal%20representation%20learning%20successfully%20optimizes%20for%0Azero-shot%20transfer%20at%20test%20time.%20Yet%20the%20standard%20pretraining%20paradigm%0A%28contrastive%20learning%20on%20large%20amounts%20of%20image-text%20data%29%20does%20not%20explicitly%0Aencourage%20representations%20to%20support%20few-shot%20adaptation.%20In%20this%20work%2C%20we%0Apropose%20a%20simple%2C%20but%20carefully%20designed%20extension%20to%20multimodal%20pretraining%0Awhich%20enables%20representations%20to%20accommodate%20additional%20context.%20Using%20this%0Aobjective%2C%20we%20show%20that%20vision-language%20models%20can%20be%20trained%20to%20exhibit%0Asignificantly%20increased%20few-shot%20adaptation%3A%20across%2021%20downstream%20tasks%2C%20we%0Afind%20up%20to%20four-fold%20improvements%20in%20test-time%20sample%20efficiency%2C%20and%20average%0Afew-shot%20adaptation%20gains%20of%20over%205%25%2C%20while%20retaining%20zero-shot%20generalization%0Aperformance%20across%20model%20scales%20and%20training%20durations.%20In%20particular%2C%20equipped%0Awith%20simple%2C%20training-free%2C%20metric-based%20adaptation%20mechanisms%2C%20our%0Arepresentations%20easily%20surpass%20more%20complex%20and%20expensive%20optimization-based%0Aschemes%2C%20vastly%20simplifying%20generalization%20to%20new%20domains.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2411.15099v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DContext-Aware%2520Multimodal%2520Pretraining%26entry.906535625%3DKarsten%2520Roth%2520and%2520Zeynep%2520Akata%2520and%2520Dima%2520Damen%2520and%2520Ivana%2520Bala%25C5%25BEevi%25C4%2587%2520and%2520Olivier%2520J.%2520H%25C3%25A9naff%26entry.1292438233%3D%2520%2520Large-scale%2520multimodal%2520representation%2520learning%2520successfully%2520optimizes%2520for%250Azero-shot%2520transfer%2520at%2520test%2520time.%2520Yet%2520the%2520standard%2520pretraining%2520paradigm%250A%2528contrastive%2520learning%2520on%2520large%2520amounts%2520of%2520image-text%2520data%2529%2520does%2520not%2520explicitly%250Aencourage%2520representations%2520to%2520support%2520few-shot%2520adaptation.%2520In%2520this%2520work%252C%2520we%250Apropose%2520a%2520simple%252C%2520but%2520carefully%2520designed%2520extension%2520to%2520multimodal%2520pretraining%250Awhich%2520enables%2520representations%2520to%2520accommodate%2520additional%2520context.%2520Using%2520this%250Aobjective%252C%2520we%2520show%2520that%2520vision-language%2520models%2520can%2520be%2520trained%2520to%2520exhibit%250Asignificantly%2520increased%2520few-shot%2520adaptation%253A%2520across%252021%2520downstream%2520tasks%252C%2520we%250Afind%2520up%2520to%2520four-fold%2520improvements%2520in%2520test-time%2520sample%2520efficiency%252C%2520and%2520average%250Afew-shot%2520adaptation%2520gains%2520of%2520over%25205%2525%252C%2520while%2520retaining%2520zero-shot%2520generalization%250Aperformance%2520across%2520model%2520scales%2520and%2520training%2520durations.%2520In%2520particular%252C%2520equipped%250Awith%2520simple%252C%2520training-free%252C%2520metric-based%2520adaptation%2520mechanisms%252C%2520our%250Arepresentations%2520easily%2520surpass%2520more%2520complex%2520and%2520expensive%2520optimization-based%250Aschemes%252C%2520vastly%2520simplifying%2520generalization%2520to%2520new%2520domains.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2411.15099v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Context-Aware%20Multimodal%20Pretraining&entry.906535625=Karsten%20Roth%20and%20Zeynep%20Akata%20and%20Dima%20Damen%20and%20Ivana%20Bala%C5%BEevi%C4%87%20and%20Olivier%20J.%20H%C3%A9naff&entry.1292438233=%20%20Large-scale%20multimodal%20representation%20learning%20successfully%20optimizes%20for%0Azero-shot%20transfer%20at%20test%20time.%20Yet%20the%20standard%20pretraining%20paradigm%0A%28contrastive%20learning%20on%20large%20amounts%20of%20image-text%20data%29%20does%20not%20explicitly%0Aencourage%20representations%20to%20support%20few-shot%20adaptation.%20In%20this%20work%2C%20we%0Apropose%20a%20simple%2C%20but%20carefully%20designed%20extension%20to%20multimodal%20pretraining%0Awhich%20enables%20representations%20to%20accommodate%20additional%20context.%20Using%20this%0Aobjective%2C%20we%20show%20that%20vision-language%20models%20can%20be%20trained%20to%20exhibit%0Asignificantly%20increased%20few-shot%20adaptation%3A%20across%2021%20downstream%20tasks%2C%20we%0Afind%20up%20to%20four-fold%20improvements%20in%20test-time%20sample%20efficiency%2C%20and%20average%0Afew-shot%20adaptation%20gains%20of%20over%205%25%2C%20while%20retaining%20zero-shot%20generalization%0Aperformance%20across%20model%20scales%20and%20training%20durations.%20In%20particular%2C%20equipped%0Awith%20simple%2C%20training-free%2C%20metric-based%20adaptation%20mechanisms%2C%20our%0Arepresentations%20easily%20surpass%20more%20complex%20and%20expensive%20optimization-based%0Aschemes%2C%20vastly%20simplifying%20generalization%20to%20new%20domains.%0A&entry.1838667208=http%3A//arxiv.org/abs/2411.15099v1&entry.124074799=Read"},
{"title": "SPAC-Net: Rethinking Point Cloud Completion with Structural Prior", "author": "Zizhao Wu and Jian Shi and Xuan Deng and Cheng Zhang and Genfu Yang and Ming Zeng and Yunhai Wang", "abstract": "  Point cloud completion aims to infer a complete shape from its partial\nobservation. Many approaches utilize a pure encoderdecoder paradigm in which\ncomplete shape can be directly predicted by shape priors learned from partial\nscans, however, these methods suffer from the loss of details inevitably due to\nthe feature abstraction issues. In this paper, we propose a novel\nframework,termed SPAC-Net, that aims to rethink the completion task under the\nguidance of a new structural prior, we call it interface. Specifically, our\nmethod first investigates Marginal Detector (MAD) module to localize the\ninterface, defined as the intersection between the known observation and the\nmissing parts. Based on the interface, our method predicts the coarse shape by\nlearning the displacement from the points in interface move to their\ncorresponding position in missing parts. Furthermore, we devise an additional\nStructure Supplement(SSP) module before the upsampling stage to enhance the\nstructural details of the coarse shape, enabling the upsampling module to focus\nmore on the upsampling task. Extensive experiments have been conducted on\nseveral challenging benchmarks, and the results demonstrate that our method\noutperforms existing state-of-the-art approaches.\n", "link": "http://arxiv.org/abs/2411.15066v1", "date": "2024-11-22", "relevancy": 2.7384, "topK": [{"title": "DDM-NET: End-to-end learning of keypoint feature Detection, Description and Matching for 3D localization", "link": "https://arxiv.org/abs/2212.04575", "similarity": 0.5623}, {"title": "IAE: Implicit Autoencoder for Point Cloud Self-supervised Representation Learning", "link": "https://arxiv.org/abs/2201.00785", "similarity": 0.5407}, {"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.54}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20SPAC-Net%3A%20Rethinking%20Point%20Cloud%20Completion%20with%20Structural%20Prior&body=Title%3A%20SPAC-Net%3A%20Rethinking%20Point%20Cloud%20Completion%20with%20Structural%20Prior%0AAuthor%3A%20Zizhao%20Wu%20and%20Jian%20Shi%20and%20Xuan%20Deng%20and%20Cheng%20Zhang%20and%20Genfu%20Yang%20and%20Ming%20Zeng%20and%20Yunhai%20Wang%0AAbstract%3A%20%20%20Point%20cloud%20completion%20aims%20to%20infer%20a%20complete%20shape%20from%20its%20partial%0Aobservation.%20Many%20approaches%20utilize%20a%20pure%20encoderdecoder%20paradigm%20in%20which%0Acomplete%20shape%20can%20be%20directly%20predicted%20by%20shape%20priors%20learned%20from%20partial%0Ascans%2C%20however%2C%20these%20methods%20suffer%20from%20the%20loss%20of%20details%20inevitably%20due%20to%0Athe%20feature%20abstraction%20issues.%20In%20this%20paper%2C%20we%20propose%20a%20novel%0Aframework%2Ctermed%20SPAC-Net%2C%20that%20aims%20to%20rethink%20the%20completion%20task%20under%20the%0Aguidance%20of%20a%20new%20structural%20prior%2C%20we%20call%20it%20interface.%20Specifically%2C%20our%0Amethod%20first%20investigates%20Marginal%20Detector%20%28MAD%29%20module%20to%20localize%20the%0Ainterface%2C%20defined%20as%20the%20intersection%20between%20the%20known%20observation%20and%20the%0Amissing%20parts.%20Based%20on%20the%20interface%2C%20our%20method%20predicts%20the%20coarse%20shape%20by%0Alearning%20the%20displacement%20from%20the%20points%20in%20interface%20move%20to%20their%0Acorresponding%20position%20in%20missing%20parts.%20Furthermore%2C%20we%20devise%20an%20additional%0AStructure%20Supplement%28SSP%29%20module%20before%20the%20upsampling%20stage%20to%20enhance%20the%0Astructural%20details%20of%20the%20coarse%20shape%2C%20enabling%20the%20upsampling%20module%20to%20focus%0Amore%20on%20the%20upsampling%20task.%20Extensive%20experiments%20have%20been%20conducted%20on%0Aseveral%20challenging%20benchmarks%2C%20and%20the%20results%20demonstrate%20that%20our%20method%0Aoutperforms%20existing%20state-of-the-art%20approaches.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2411.15066v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DSPAC-Net%253A%2520Rethinking%2520Point%2520Cloud%2520Completion%2520with%2520Structural%2520Prior%26entry.906535625%3DZizhao%2520Wu%2520and%2520Jian%2520Shi%2520and%2520Xuan%2520Deng%2520and%2520Cheng%2520Zhang%2520and%2520Genfu%2520Yang%2520and%2520Ming%2520Zeng%2520and%2520Yunhai%2520Wang%26entry.1292438233%3D%2520%2520Point%2520cloud%2520completion%2520aims%2520to%2520infer%2520a%2520complete%2520shape%2520from%2520its%2520partial%250Aobservation.%2520Many%2520approaches%2520utilize%2520a%2520pure%2520encoderdecoder%2520paradigm%2520in%2520which%250Acomplete%2520shape%2520can%2520be%2520directly%2520predicted%2520by%2520shape%2520priors%2520learned%2520from%2520partial%250Ascans%252C%2520however%252C%2520these%2520methods%2520suffer%2520from%2520the%2520loss%2520of%2520details%2520inevitably%2520due%2520to%250Athe%2520feature%2520abstraction%2520issues.%2520In%2520this%2520paper%252C%2520we%2520propose%2520a%2520novel%250Aframework%252Ctermed%2520SPAC-Net%252C%2520that%2520aims%2520to%2520rethink%2520the%2520completion%2520task%2520under%2520the%250Aguidance%2520of%2520a%2520new%2520structural%2520prior%252C%2520we%2520call%2520it%2520interface.%2520Specifically%252C%2520our%250Amethod%2520first%2520investigates%2520Marginal%2520Detector%2520%2528MAD%2529%2520module%2520to%2520localize%2520the%250Ainterface%252C%2520defined%2520as%2520the%2520intersection%2520between%2520the%2520known%2520observation%2520and%2520the%250Amissing%2520parts.%2520Based%2520on%2520the%2520interface%252C%2520our%2520method%2520predicts%2520the%2520coarse%2520shape%2520by%250Alearning%2520the%2520displacement%2520from%2520the%2520points%2520in%2520interface%2520move%2520to%2520their%250Acorresponding%2520position%2520in%2520missing%2520parts.%2520Furthermore%252C%2520we%2520devise%2520an%2520additional%250AStructure%2520Supplement%2528SSP%2529%2520module%2520before%2520the%2520upsampling%2520stage%2520to%2520enhance%2520the%250Astructural%2520details%2520of%2520the%2520coarse%2520shape%252C%2520enabling%2520the%2520upsampling%2520module%2520to%2520focus%250Amore%2520on%2520the%2520upsampling%2520task.%2520Extensive%2520experiments%2520have%2520been%2520conducted%2520on%250Aseveral%2520challenging%2520benchmarks%252C%2520and%2520the%2520results%2520demonstrate%2520that%2520our%2520method%250Aoutperforms%2520existing%2520state-of-the-art%2520approaches.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2411.15066v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=SPAC-Net%3A%20Rethinking%20Point%20Cloud%20Completion%20with%20Structural%20Prior&entry.906535625=Zizhao%20Wu%20and%20Jian%20Shi%20and%20Xuan%20Deng%20and%20Cheng%20Zhang%20and%20Genfu%20Yang%20and%20Ming%20Zeng%20and%20Yunhai%20Wang&entry.1292438233=%20%20Point%20cloud%20completion%20aims%20to%20infer%20a%20complete%20shape%20from%20its%20partial%0Aobservation.%20Many%20approaches%20utilize%20a%20pure%20encoderdecoder%20paradigm%20in%20which%0Acomplete%20shape%20can%20be%20directly%20predicted%20by%20shape%20priors%20learned%20from%20partial%0Ascans%2C%20however%2C%20these%20methods%20suffer%20from%20the%20loss%20of%20details%20inevitably%20due%20to%0Athe%20feature%20abstraction%20issues.%20In%20this%20paper%2C%20we%20propose%20a%20novel%0Aframework%2Ctermed%20SPAC-Net%2C%20that%20aims%20to%20rethink%20the%20completion%20task%20under%20the%0Aguidance%20of%20a%20new%20structural%20prior%2C%20we%20call%20it%20interface.%20Specifically%2C%20our%0Amethod%20first%20investigates%20Marginal%20Detector%20%28MAD%29%20module%20to%20localize%20the%0Ainterface%2C%20defined%20as%20the%20intersection%20between%20the%20known%20observation%20and%20the%0Amissing%20parts.%20Based%20on%20the%20interface%2C%20our%20method%20predicts%20the%20coarse%20shape%20by%0Alearning%20the%20displacement%20from%20the%20points%20in%20interface%20move%20to%20their%0Acorresponding%20position%20in%20missing%20parts.%20Furthermore%2C%20we%20devise%20an%20additional%0AStructure%20Supplement%28SSP%29%20module%20before%20the%20upsampling%20stage%20to%20enhance%20the%0Astructural%20details%20of%20the%20coarse%20shape%2C%20enabling%20the%20upsampling%20module%20to%20focus%0Amore%20on%20the%20upsampling%20task.%20Extensive%20experiments%20have%20been%20conducted%20on%0Aseveral%20challenging%20benchmarks%2C%20and%20the%20results%20demonstrate%20that%20our%20method%0Aoutperforms%20existing%20state-of-the-art%20approaches.%0A&entry.1838667208=http%3A//arxiv.org/abs/2411.15066v1&entry.124074799=Read"},
{"title": "Generalized Implicit Neural Representation for Efficient MRI Parallel\n  Imaging Reconstruction", "author": "Hao Li and Yusheng Zhou and Jianan Liu and Xiling Liu and Tao Huang and Zhihan Lyu and Weidong Cai and Wei Chen", "abstract": "  High-resolution magnetic resonance imaging (MRI) is essential in clinical\ndiagnosis. However, its long acquisition time remains a critical issue.\nParallel imaging (PI) is a common approach to reduce acquisition time by\nperiodically skipping specific k-space lines and reconstructing images from\nundersampled data. This study presents a generalized implicit neural\nrepresentation (INR)-based framework for MRI PI reconstruction, addressing\nlimitations commonly encountered in conventional methods, such as\nsubject-specific or undersampling scale-specific requirements and long\nreconstruction time. The proposed method overcomes these limitations by\nleveraging prior knowledge of voxel-specific features and integrating a novel\nscale-embedded encoder module. This encoder generates scale-independent\nvoxel-specific features from undersampled images, enabling robust\nreconstruction across various undersampling scales without requiring retraining\nfor each specific scale or subject. The framework's INR model treats fully\nsampled MR images as a continuous function of spatial coordinates and prior\nvoxel-specific features, efficiently reconstructing high-quality MR images from\nundersampled data. Extensive experiments on publicly available MRI datasets\ndemonstrate the superior performance of the proposed method in reconstructing\nimages at multiple acceleration factors (4x, 5x, and 6x), achieving higher\nevaluation metrics and visual fidelity compared to state-of-the-art methods. In\nterms of efficiency, this INR-based approach exhibits notable advantages,\nincluding reduced floating point operations and GPU usage, allowing for\naccelerated processing times while maintaining high reconstruction quality. The\ngeneralized design of the model significantly reduces computational resources\nand time consumption, making it more suitable for real-time clinical\napplications.\n", "link": "http://arxiv.org/abs/2309.06067v7", "date": "2024-11-22", "relevancy": 2.7232, "topK": [{"title": "MiraGe: Editable 2D Images using Gaussian Splatting", "link": "http://arxiv.org/abs/2410.01521v1", "similarity": 0.5539}, {"title": "RTG-SLAM: Real-time 3D Reconstruction at Scale using Gaussian Splatting", "link": "http://arxiv.org/abs/2404.19706v1", "similarity": 0.5538}, {"title": "IAE: Implicit Autoencoder for Point Cloud Self-supervised Representation Learning", "link": "https://arxiv.org/abs/2201.00785", "similarity": 0.5262}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Generalized%20Implicit%20Neural%20Representation%20for%20Efficient%20MRI%20Parallel%0A%20%20Imaging%20Reconstruction&body=Title%3A%20Generalized%20Implicit%20Neural%20Representation%20for%20Efficient%20MRI%20Parallel%0A%20%20Imaging%20Reconstruction%0AAuthor%3A%20Hao%20Li%20and%20Yusheng%20Zhou%20and%20Jianan%20Liu%20and%20Xiling%20Liu%20and%20Tao%20Huang%20and%20Zhihan%20Lyu%20and%20Weidong%20Cai%20and%20Wei%20Chen%0AAbstract%3A%20%20%20High-resolution%20magnetic%20resonance%20imaging%20%28MRI%29%20is%20essential%20in%20clinical%0Adiagnosis.%20However%2C%20its%20long%20acquisition%20time%20remains%20a%20critical%20issue.%0AParallel%20imaging%20%28PI%29%20is%20a%20common%20approach%20to%20reduce%20acquisition%20time%20by%0Aperiodically%20skipping%20specific%20k-space%20lines%20and%20reconstructing%20images%20from%0Aundersampled%20data.%20This%20study%20presents%20a%20generalized%20implicit%20neural%0Arepresentation%20%28INR%29-based%20framework%20for%20MRI%20PI%20reconstruction%2C%20addressing%0Alimitations%20commonly%20encountered%20in%20conventional%20methods%2C%20such%20as%0Asubject-specific%20or%20undersampling%20scale-specific%20requirements%20and%20long%0Areconstruction%20time.%20The%20proposed%20method%20overcomes%20these%20limitations%20by%0Aleveraging%20prior%20knowledge%20of%20voxel-specific%20features%20and%20integrating%20a%20novel%0Ascale-embedded%20encoder%20module.%20This%20encoder%20generates%20scale-independent%0Avoxel-specific%20features%20from%20undersampled%20images%2C%20enabling%20robust%0Areconstruction%20across%20various%20undersampling%20scales%20without%20requiring%20retraining%0Afor%20each%20specific%20scale%20or%20subject.%20The%20framework%27s%20INR%20model%20treats%20fully%0Asampled%20MR%20images%20as%20a%20continuous%20function%20of%20spatial%20coordinates%20and%20prior%0Avoxel-specific%20features%2C%20efficiently%20reconstructing%20high-quality%20MR%20images%20from%0Aundersampled%20data.%20Extensive%20experiments%20on%20publicly%20available%20MRI%20datasets%0Ademonstrate%20the%20superior%20performance%20of%20the%20proposed%20method%20in%20reconstructing%0Aimages%20at%20multiple%20acceleration%20factors%20%284x%2C%205x%2C%20and%206x%29%2C%20achieving%20higher%0Aevaluation%20metrics%20and%20visual%20fidelity%20compared%20to%20state-of-the-art%20methods.%20In%0Aterms%20of%20efficiency%2C%20this%20INR-based%20approach%20exhibits%20notable%20advantages%2C%0Aincluding%20reduced%20floating%20point%20operations%20and%20GPU%20usage%2C%20allowing%20for%0Aaccelerated%20processing%20times%20while%20maintaining%20high%20reconstruction%20quality.%20The%0Ageneralized%20design%20of%20the%20model%20significantly%20reduces%20computational%20resources%0Aand%20time%20consumption%2C%20making%20it%20more%20suitable%20for%20real-time%20clinical%0Aapplications.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2309.06067v7%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DGeneralized%2520Implicit%2520Neural%2520Representation%2520for%2520Efficient%2520MRI%2520Parallel%250A%2520%2520Imaging%2520Reconstruction%26entry.906535625%3DHao%2520Li%2520and%2520Yusheng%2520Zhou%2520and%2520Jianan%2520Liu%2520and%2520Xiling%2520Liu%2520and%2520Tao%2520Huang%2520and%2520Zhihan%2520Lyu%2520and%2520Weidong%2520Cai%2520and%2520Wei%2520Chen%26entry.1292438233%3D%2520%2520High-resolution%2520magnetic%2520resonance%2520imaging%2520%2528MRI%2529%2520is%2520essential%2520in%2520clinical%250Adiagnosis.%2520However%252C%2520its%2520long%2520acquisition%2520time%2520remains%2520a%2520critical%2520issue.%250AParallel%2520imaging%2520%2528PI%2529%2520is%2520a%2520common%2520approach%2520to%2520reduce%2520acquisition%2520time%2520by%250Aperiodically%2520skipping%2520specific%2520k-space%2520lines%2520and%2520reconstructing%2520images%2520from%250Aundersampled%2520data.%2520This%2520study%2520presents%2520a%2520generalized%2520implicit%2520neural%250Arepresentation%2520%2528INR%2529-based%2520framework%2520for%2520MRI%2520PI%2520reconstruction%252C%2520addressing%250Alimitations%2520commonly%2520encountered%2520in%2520conventional%2520methods%252C%2520such%2520as%250Asubject-specific%2520or%2520undersampling%2520scale-specific%2520requirements%2520and%2520long%250Areconstruction%2520time.%2520The%2520proposed%2520method%2520overcomes%2520these%2520limitations%2520by%250Aleveraging%2520prior%2520knowledge%2520of%2520voxel-specific%2520features%2520and%2520integrating%2520a%2520novel%250Ascale-embedded%2520encoder%2520module.%2520This%2520encoder%2520generates%2520scale-independent%250Avoxel-specific%2520features%2520from%2520undersampled%2520images%252C%2520enabling%2520robust%250Areconstruction%2520across%2520various%2520undersampling%2520scales%2520without%2520requiring%2520retraining%250Afor%2520each%2520specific%2520scale%2520or%2520subject.%2520The%2520framework%2527s%2520INR%2520model%2520treats%2520fully%250Asampled%2520MR%2520images%2520as%2520a%2520continuous%2520function%2520of%2520spatial%2520coordinates%2520and%2520prior%250Avoxel-specific%2520features%252C%2520efficiently%2520reconstructing%2520high-quality%2520MR%2520images%2520from%250Aundersampled%2520data.%2520Extensive%2520experiments%2520on%2520publicly%2520available%2520MRI%2520datasets%250Ademonstrate%2520the%2520superior%2520performance%2520of%2520the%2520proposed%2520method%2520in%2520reconstructing%250Aimages%2520at%2520multiple%2520acceleration%2520factors%2520%25284x%252C%25205x%252C%2520and%25206x%2529%252C%2520achieving%2520higher%250Aevaluation%2520metrics%2520and%2520visual%2520fidelity%2520compared%2520to%2520state-of-the-art%2520methods.%2520In%250Aterms%2520of%2520efficiency%252C%2520this%2520INR-based%2520approach%2520exhibits%2520notable%2520advantages%252C%250Aincluding%2520reduced%2520floating%2520point%2520operations%2520and%2520GPU%2520usage%252C%2520allowing%2520for%250Aaccelerated%2520processing%2520times%2520while%2520maintaining%2520high%2520reconstruction%2520quality.%2520The%250Ageneralized%2520design%2520of%2520the%2520model%2520significantly%2520reduces%2520computational%2520resources%250Aand%2520time%2520consumption%252C%2520making%2520it%2520more%2520suitable%2520for%2520real-time%2520clinical%250Aapplications.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2309.06067v7%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Generalized%20Implicit%20Neural%20Representation%20for%20Efficient%20MRI%20Parallel%0A%20%20Imaging%20Reconstruction&entry.906535625=Hao%20Li%20and%20Yusheng%20Zhou%20and%20Jianan%20Liu%20and%20Xiling%20Liu%20and%20Tao%20Huang%20and%20Zhihan%20Lyu%20and%20Weidong%20Cai%20and%20Wei%20Chen&entry.1292438233=%20%20High-resolution%20magnetic%20resonance%20imaging%20%28MRI%29%20is%20essential%20in%20clinical%0Adiagnosis.%20However%2C%20its%20long%20acquisition%20time%20remains%20a%20critical%20issue.%0AParallel%20imaging%20%28PI%29%20is%20a%20common%20approach%20to%20reduce%20acquisition%20time%20by%0Aperiodically%20skipping%20specific%20k-space%20lines%20and%20reconstructing%20images%20from%0Aundersampled%20data.%20This%20study%20presents%20a%20generalized%20implicit%20neural%0Arepresentation%20%28INR%29-based%20framework%20for%20MRI%20PI%20reconstruction%2C%20addressing%0Alimitations%20commonly%20encountered%20in%20conventional%20methods%2C%20such%20as%0Asubject-specific%20or%20undersampling%20scale-specific%20requirements%20and%20long%0Areconstruction%20time.%20The%20proposed%20method%20overcomes%20these%20limitations%20by%0Aleveraging%20prior%20knowledge%20of%20voxel-specific%20features%20and%20integrating%20a%20novel%0Ascale-embedded%20encoder%20module.%20This%20encoder%20generates%20scale-independent%0Avoxel-specific%20features%20from%20undersampled%20images%2C%20enabling%20robust%0Areconstruction%20across%20various%20undersampling%20scales%20without%20requiring%20retraining%0Afor%20each%20specific%20scale%20or%20subject.%20The%20framework%27s%20INR%20model%20treats%20fully%0Asampled%20MR%20images%20as%20a%20continuous%20function%20of%20spatial%20coordinates%20and%20prior%0Avoxel-specific%20features%2C%20efficiently%20reconstructing%20high-quality%20MR%20images%20from%0Aundersampled%20data.%20Extensive%20experiments%20on%20publicly%20available%20MRI%20datasets%0Ademonstrate%20the%20superior%20performance%20of%20the%20proposed%20method%20in%20reconstructing%0Aimages%20at%20multiple%20acceleration%20factors%20%284x%2C%205x%2C%20and%206x%29%2C%20achieving%20higher%0Aevaluation%20metrics%20and%20visual%20fidelity%20compared%20to%20state-of-the-art%20methods.%20In%0Aterms%20of%20efficiency%2C%20this%20INR-based%20approach%20exhibits%20notable%20advantages%2C%0Aincluding%20reduced%20floating%20point%20operations%20and%20GPU%20usage%2C%20allowing%20for%0Aaccelerated%20processing%20times%20while%20maintaining%20high%20reconstruction%20quality.%20The%0Ageneralized%20design%20of%20the%20model%20significantly%20reduces%20computational%20resources%0Aand%20time%20consumption%2C%20making%20it%20more%20suitable%20for%20real-time%20clinical%0Aapplications.%0A&entry.1838667208=http%3A//arxiv.org/abs/2309.06067v7&entry.124074799=Read"},
{"title": "RED: Effective Trajectory Representation Learning with Comprehensive\n  Information", "author": "Silin Zhou and Shuo Shang and Lisi Chen and Christian S. Jensen and Panos Kalnis", "abstract": "  Trajectory representation learning (TRL) maps trajectories to vectors that\ncan then be used for various downstream tasks, including trajectory similarity\ncomputation, trajectory classification, and travel-time estimation. However,\nexisting TRL methods often produce vectors that, when used in downstream tasks,\nyield insufficiently accurate results. A key reason is that they fail to\nutilize the comprehensive information encompassed by trajectories. We propose a\nself-supervised TRL framework, called RED, which effectively exploits multiple\ntypes of trajectory information. Overall, RED adopts the Transformer as the\nbackbone model and masks the constituting paths in trajectories to train a\nmasked autoencoder (MAE). In particular, RED considers the moving patterns of\ntrajectories by employing a Road-aware masking strategy} that retains key paths\nof trajectories during masking, thereby preserving crucial information of the\ntrajectories. RED also adopts a spatial-temporal-user joint Embedding scheme to\nencode comprehensive information when preparing the trajectories as model\ninputs. To conduct training, RED adopts Dual-objective task learning}: the\nTransformer encoder predicts the next segment in a trajectory, and the\nTransformer decoder reconstructs the entire trajectory. RED also considers the\nspatial-temporal correlations of trajectories by modifying the attention\nmechanism of the Transformer. We compare RED with 9 state-of-the-art TRL\nmethods for 4 downstream tasks on 3 real-world datasets, finding that RED can\nusually improve the accuracy of the best-performing baseline by over 5%.\n", "link": "http://arxiv.org/abs/2411.15096v1", "date": "2024-11-22", "relevancy": 2.7144, "topK": [{"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5636}, {"title": "Self-supervised Photographic Image Layout Representation Learning", "link": "http://arxiv.org/abs/2403.03740v1", "similarity": 0.5329}, {"title": "RTG-SLAM: Real-time 3D Reconstruction at Scale using Gaussian Splatting", "link": "http://arxiv.org/abs/2404.19706v1", "similarity": 0.5321}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20RED%3A%20Effective%20Trajectory%20Representation%20Learning%20with%20Comprehensive%0A%20%20Information&body=Title%3A%20RED%3A%20Effective%20Trajectory%20Representation%20Learning%20with%20Comprehensive%0A%20%20Information%0AAuthor%3A%20Silin%20Zhou%20and%20Shuo%20Shang%20and%20Lisi%20Chen%20and%20Christian%20S.%20Jensen%20and%20Panos%20Kalnis%0AAbstract%3A%20%20%20Trajectory%20representation%20learning%20%28TRL%29%20maps%20trajectories%20to%20vectors%20that%0Acan%20then%20be%20used%20for%20various%20downstream%20tasks%2C%20including%20trajectory%20similarity%0Acomputation%2C%20trajectory%20classification%2C%20and%20travel-time%20estimation.%20However%2C%0Aexisting%20TRL%20methods%20often%20produce%20vectors%20that%2C%20when%20used%20in%20downstream%20tasks%2C%0Ayield%20insufficiently%20accurate%20results.%20A%20key%20reason%20is%20that%20they%20fail%20to%0Autilize%20the%20comprehensive%20information%20encompassed%20by%20trajectories.%20We%20propose%20a%0Aself-supervised%20TRL%20framework%2C%20called%20RED%2C%20which%20effectively%20exploits%20multiple%0Atypes%20of%20trajectory%20information.%20Overall%2C%20RED%20adopts%20the%20Transformer%20as%20the%0Abackbone%20model%20and%20masks%20the%20constituting%20paths%20in%20trajectories%20to%20train%20a%0Amasked%20autoencoder%20%28MAE%29.%20In%20particular%2C%20RED%20considers%20the%20moving%20patterns%20of%0Atrajectories%20by%20employing%20a%20Road-aware%20masking%20strategy%7D%20that%20retains%20key%20paths%0Aof%20trajectories%20during%20masking%2C%20thereby%20preserving%20crucial%20information%20of%20the%0Atrajectories.%20RED%20also%20adopts%20a%20spatial-temporal-user%20joint%20Embedding%20scheme%20to%0Aencode%20comprehensive%20information%20when%20preparing%20the%20trajectories%20as%20model%0Ainputs.%20To%20conduct%20training%2C%20RED%20adopts%20Dual-objective%20task%20learning%7D%3A%20the%0ATransformer%20encoder%20predicts%20the%20next%20segment%20in%20a%20trajectory%2C%20and%20the%0ATransformer%20decoder%20reconstructs%20the%20entire%20trajectory.%20RED%20also%20considers%20the%0Aspatial-temporal%20correlations%20of%20trajectories%20by%20modifying%20the%20attention%0Amechanism%20of%20the%20Transformer.%20We%20compare%20RED%20with%209%20state-of-the-art%20TRL%0Amethods%20for%204%20downstream%20tasks%20on%203%20real-world%20datasets%2C%20finding%20that%20RED%20can%0Ausually%20improve%20the%20accuracy%20of%20the%20best-performing%20baseline%20by%20over%205%25.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2411.15096v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DRED%253A%2520Effective%2520Trajectory%2520Representation%2520Learning%2520with%2520Comprehensive%250A%2520%2520Information%26entry.906535625%3DSilin%2520Zhou%2520and%2520Shuo%2520Shang%2520and%2520Lisi%2520Chen%2520and%2520Christian%2520S.%2520Jensen%2520and%2520Panos%2520Kalnis%26entry.1292438233%3D%2520%2520Trajectory%2520representation%2520learning%2520%2528TRL%2529%2520maps%2520trajectories%2520to%2520vectors%2520that%250Acan%2520then%2520be%2520used%2520for%2520various%2520downstream%2520tasks%252C%2520including%2520trajectory%2520similarity%250Acomputation%252C%2520trajectory%2520classification%252C%2520and%2520travel-time%2520estimation.%2520However%252C%250Aexisting%2520TRL%2520methods%2520often%2520produce%2520vectors%2520that%252C%2520when%2520used%2520in%2520downstream%2520tasks%252C%250Ayield%2520insufficiently%2520accurate%2520results.%2520A%2520key%2520reason%2520is%2520that%2520they%2520fail%2520to%250Autilize%2520the%2520comprehensive%2520information%2520encompassed%2520by%2520trajectories.%2520We%2520propose%2520a%250Aself-supervised%2520TRL%2520framework%252C%2520called%2520RED%252C%2520which%2520effectively%2520exploits%2520multiple%250Atypes%2520of%2520trajectory%2520information.%2520Overall%252C%2520RED%2520adopts%2520the%2520Transformer%2520as%2520the%250Abackbone%2520model%2520and%2520masks%2520the%2520constituting%2520paths%2520in%2520trajectories%2520to%2520train%2520a%250Amasked%2520autoencoder%2520%2528MAE%2529.%2520In%2520particular%252C%2520RED%2520considers%2520the%2520moving%2520patterns%2520of%250Atrajectories%2520by%2520employing%2520a%2520Road-aware%2520masking%2520strategy%257D%2520that%2520retains%2520key%2520paths%250Aof%2520trajectories%2520during%2520masking%252C%2520thereby%2520preserving%2520crucial%2520information%2520of%2520the%250Atrajectories.%2520RED%2520also%2520adopts%2520a%2520spatial-temporal-user%2520joint%2520Embedding%2520scheme%2520to%250Aencode%2520comprehensive%2520information%2520when%2520preparing%2520the%2520trajectories%2520as%2520model%250Ainputs.%2520To%2520conduct%2520training%252C%2520RED%2520adopts%2520Dual-objective%2520task%2520learning%257D%253A%2520the%250ATransformer%2520encoder%2520predicts%2520the%2520next%2520segment%2520in%2520a%2520trajectory%252C%2520and%2520the%250ATransformer%2520decoder%2520reconstructs%2520the%2520entire%2520trajectory.%2520RED%2520also%2520considers%2520the%250Aspatial-temporal%2520correlations%2520of%2520trajectories%2520by%2520modifying%2520the%2520attention%250Amechanism%2520of%2520the%2520Transformer.%2520We%2520compare%2520RED%2520with%25209%2520state-of-the-art%2520TRL%250Amethods%2520for%25204%2520downstream%2520tasks%2520on%25203%2520real-world%2520datasets%252C%2520finding%2520that%2520RED%2520can%250Ausually%2520improve%2520the%2520accuracy%2520of%2520the%2520best-performing%2520baseline%2520by%2520over%25205%2525.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2411.15096v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=RED%3A%20Effective%20Trajectory%20Representation%20Learning%20with%20Comprehensive%0A%20%20Information&entry.906535625=Silin%20Zhou%20and%20Shuo%20Shang%20and%20Lisi%20Chen%20and%20Christian%20S.%20Jensen%20and%20Panos%20Kalnis&entry.1292438233=%20%20Trajectory%20representation%20learning%20%28TRL%29%20maps%20trajectories%20to%20vectors%20that%0Acan%20then%20be%20used%20for%20various%20downstream%20tasks%2C%20including%20trajectory%20similarity%0Acomputation%2C%20trajectory%20classification%2C%20and%20travel-time%20estimation.%20However%2C%0Aexisting%20TRL%20methods%20often%20produce%20vectors%20that%2C%20when%20used%20in%20downstream%20tasks%2C%0Ayield%20insufficiently%20accurate%20results.%20A%20key%20reason%20is%20that%20they%20fail%20to%0Autilize%20the%20comprehensive%20information%20encompassed%20by%20trajectories.%20We%20propose%20a%0Aself-supervised%20TRL%20framework%2C%20called%20RED%2C%20which%20effectively%20exploits%20multiple%0Atypes%20of%20trajectory%20information.%20Overall%2C%20RED%20adopts%20the%20Transformer%20as%20the%0Abackbone%20model%20and%20masks%20the%20constituting%20paths%20in%20trajectories%20to%20train%20a%0Amasked%20autoencoder%20%28MAE%29.%20In%20particular%2C%20RED%20considers%20the%20moving%20patterns%20of%0Atrajectories%20by%20employing%20a%20Road-aware%20masking%20strategy%7D%20that%20retains%20key%20paths%0Aof%20trajectories%20during%20masking%2C%20thereby%20preserving%20crucial%20information%20of%20the%0Atrajectories.%20RED%20also%20adopts%20a%20spatial-temporal-user%20joint%20Embedding%20scheme%20to%0Aencode%20comprehensive%20information%20when%20preparing%20the%20trajectories%20as%20model%0Ainputs.%20To%20conduct%20training%2C%20RED%20adopts%20Dual-objective%20task%20learning%7D%3A%20the%0ATransformer%20encoder%20predicts%20the%20next%20segment%20in%20a%20trajectory%2C%20and%20the%0ATransformer%20decoder%20reconstructs%20the%20entire%20trajectory.%20RED%20also%20considers%20the%0Aspatial-temporal%20correlations%20of%20trajectories%20by%20modifying%20the%20attention%0Amechanism%20of%20the%20Transformer.%20We%20compare%20RED%20with%209%20state-of-the-art%20TRL%0Amethods%20for%204%20downstream%20tasks%20on%203%20real-world%20datasets%2C%20finding%20that%20RED%20can%0Ausually%20improve%20the%20accuracy%20of%20the%20best-performing%20baseline%20by%20over%205%25.%0A&entry.1838667208=http%3A//arxiv.org/abs/2411.15096v1&entry.124074799=Read"},
{"title": "CE-VAE: Capsule Enhanced Variational AutoEncoder for Underwater Image\n  Enhancement", "author": "Rita Pucci and Niki Martinel", "abstract": "  Unmanned underwater image analysis for marine monitoring faces two key\nchallenges: (i) degraded image quality due to light attenuation and (ii)\nhardware storage constraints limiting high-resolution image collection.\nExisting methods primarily address image enhancement with approaches that hinge\non storing the full-size input. In contrast, we introduce the Capsule Enhanced\nVariational AutoEncoder (CE-VAE), a novel architecture designed to efficiently\ncompress and enhance degraded underwater images. Our attention-aware image\nencoder can project the input image onto a latent space representation while\nbeing able to run online on a remote device. The only information that needs to\nbe stored on the device or sent to a beacon is a compressed representation.\nThere is a dual-decoder module that performs offline, full-size enhanced image\ngeneration. One branch reconstructs spatial details from the compressed latent\nspace, while the second branch utilizes a capsule-clustering layer to capture\nentity-level structures and complex spatial relationships. This parallel\ndecoding strategy enables the model to balance fine-detail preservation with\ncontext-aware enhancements. CE-VAE achieves state-of-the-art performance in\nunderwater image enhancement on six benchmark datasets, providing up to 3x\nhigher compression efficiency than existing approaches. Code available at\n\\url{https://github.com/iN1k1/ce-vae-underwater-image-enhancement}.\n", "link": "http://arxiv.org/abs/2406.01294v2", "date": "2024-11-22", "relevancy": 2.6635, "topK": [{"title": "IAE: Implicit Autoencoder for Point Cloud Self-supervised Representation Learning", "link": "https://arxiv.org/abs/2201.00785", "similarity": 0.5499}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5241}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5241}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20CE-VAE%3A%20Capsule%20Enhanced%20Variational%20AutoEncoder%20for%20Underwater%20Image%0A%20%20Enhancement&body=Title%3A%20CE-VAE%3A%20Capsule%20Enhanced%20Variational%20AutoEncoder%20for%20Underwater%20Image%0A%20%20Enhancement%0AAuthor%3A%20Rita%20Pucci%20and%20Niki%20Martinel%0AAbstract%3A%20%20%20Unmanned%20underwater%20image%20analysis%20for%20marine%20monitoring%20faces%20two%20key%0Achallenges%3A%20%28i%29%20degraded%20image%20quality%20due%20to%20light%20attenuation%20and%20%28ii%29%0Ahardware%20storage%20constraints%20limiting%20high-resolution%20image%20collection.%0AExisting%20methods%20primarily%20address%20image%20enhancement%20with%20approaches%20that%20hinge%0Aon%20storing%20the%20full-size%20input.%20In%20contrast%2C%20we%20introduce%20the%20Capsule%20Enhanced%0AVariational%20AutoEncoder%20%28CE-VAE%29%2C%20a%20novel%20architecture%20designed%20to%20efficiently%0Acompress%20and%20enhance%20degraded%20underwater%20images.%20Our%20attention-aware%20image%0Aencoder%20can%20project%20the%20input%20image%20onto%20a%20latent%20space%20representation%20while%0Abeing%20able%20to%20run%20online%20on%20a%20remote%20device.%20The%20only%20information%20that%20needs%20to%0Abe%20stored%20on%20the%20device%20or%20sent%20to%20a%20beacon%20is%20a%20compressed%20representation.%0AThere%20is%20a%20dual-decoder%20module%20that%20performs%20offline%2C%20full-size%20enhanced%20image%0Ageneration.%20One%20branch%20reconstructs%20spatial%20details%20from%20the%20compressed%20latent%0Aspace%2C%20while%20the%20second%20branch%20utilizes%20a%20capsule-clustering%20layer%20to%20capture%0Aentity-level%20structures%20and%20complex%20spatial%20relationships.%20This%20parallel%0Adecoding%20strategy%20enables%20the%20model%20to%20balance%20fine-detail%20preservation%20with%0Acontext-aware%20enhancements.%20CE-VAE%20achieves%20state-of-the-art%20performance%20in%0Aunderwater%20image%20enhancement%20on%20six%20benchmark%20datasets%2C%20providing%20up%20to%203x%0Ahigher%20compression%20efficiency%20than%20existing%20approaches.%20Code%20available%20at%0A%5Curl%7Bhttps%3A//github.com/iN1k1/ce-vae-underwater-image-enhancement%7D.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2406.01294v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DCE-VAE%253A%2520Capsule%2520Enhanced%2520Variational%2520AutoEncoder%2520for%2520Underwater%2520Image%250A%2520%2520Enhancement%26entry.906535625%3DRita%2520Pucci%2520and%2520Niki%2520Martinel%26entry.1292438233%3D%2520%2520Unmanned%2520underwater%2520image%2520analysis%2520for%2520marine%2520monitoring%2520faces%2520two%2520key%250Achallenges%253A%2520%2528i%2529%2520degraded%2520image%2520quality%2520due%2520to%2520light%2520attenuation%2520and%2520%2528ii%2529%250Ahardware%2520storage%2520constraints%2520limiting%2520high-resolution%2520image%2520collection.%250AExisting%2520methods%2520primarily%2520address%2520image%2520enhancement%2520with%2520approaches%2520that%2520hinge%250Aon%2520storing%2520the%2520full-size%2520input.%2520In%2520contrast%252C%2520we%2520introduce%2520the%2520Capsule%2520Enhanced%250AVariational%2520AutoEncoder%2520%2528CE-VAE%2529%252C%2520a%2520novel%2520architecture%2520designed%2520to%2520efficiently%250Acompress%2520and%2520enhance%2520degraded%2520underwater%2520images.%2520Our%2520attention-aware%2520image%250Aencoder%2520can%2520project%2520the%2520input%2520image%2520onto%2520a%2520latent%2520space%2520representation%2520while%250Abeing%2520able%2520to%2520run%2520online%2520on%2520a%2520remote%2520device.%2520The%2520only%2520information%2520that%2520needs%2520to%250Abe%2520stored%2520on%2520the%2520device%2520or%2520sent%2520to%2520a%2520beacon%2520is%2520a%2520compressed%2520representation.%250AThere%2520is%2520a%2520dual-decoder%2520module%2520that%2520performs%2520offline%252C%2520full-size%2520enhanced%2520image%250Ageneration.%2520One%2520branch%2520reconstructs%2520spatial%2520details%2520from%2520the%2520compressed%2520latent%250Aspace%252C%2520while%2520the%2520second%2520branch%2520utilizes%2520a%2520capsule-clustering%2520layer%2520to%2520capture%250Aentity-level%2520structures%2520and%2520complex%2520spatial%2520relationships.%2520This%2520parallel%250Adecoding%2520strategy%2520enables%2520the%2520model%2520to%2520balance%2520fine-detail%2520preservation%2520with%250Acontext-aware%2520enhancements.%2520CE-VAE%2520achieves%2520state-of-the-art%2520performance%2520in%250Aunderwater%2520image%2520enhancement%2520on%2520six%2520benchmark%2520datasets%252C%2520providing%2520up%2520to%25203x%250Ahigher%2520compression%2520efficiency%2520than%2520existing%2520approaches.%2520Code%2520available%2520at%250A%255Curl%257Bhttps%253A//github.com/iN1k1/ce-vae-underwater-image-enhancement%257D.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2406.01294v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=CE-VAE%3A%20Capsule%20Enhanced%20Variational%20AutoEncoder%20for%20Underwater%20Image%0A%20%20Enhancement&entry.906535625=Rita%20Pucci%20and%20Niki%20Martinel&entry.1292438233=%20%20Unmanned%20underwater%20image%20analysis%20for%20marine%20monitoring%20faces%20two%20key%0Achallenges%3A%20%28i%29%20degraded%20image%20quality%20due%20to%20light%20attenuation%20and%20%28ii%29%0Ahardware%20storage%20constraints%20limiting%20high-resolution%20image%20collection.%0AExisting%20methods%20primarily%20address%20image%20enhancement%20with%20approaches%20that%20hinge%0Aon%20storing%20the%20full-size%20input.%20In%20contrast%2C%20we%20introduce%20the%20Capsule%20Enhanced%0AVariational%20AutoEncoder%20%28CE-VAE%29%2C%20a%20novel%20architecture%20designed%20to%20efficiently%0Acompress%20and%20enhance%20degraded%20underwater%20images.%20Our%20attention-aware%20image%0Aencoder%20can%20project%20the%20input%20image%20onto%20a%20latent%20space%20representation%20while%0Abeing%20able%20to%20run%20online%20on%20a%20remote%20device.%20The%20only%20information%20that%20needs%20to%0Abe%20stored%20on%20the%20device%20or%20sent%20to%20a%20beacon%20is%20a%20compressed%20representation.%0AThere%20is%20a%20dual-decoder%20module%20that%20performs%20offline%2C%20full-size%20enhanced%20image%0Ageneration.%20One%20branch%20reconstructs%20spatial%20details%20from%20the%20compressed%20latent%0Aspace%2C%20while%20the%20second%20branch%20utilizes%20a%20capsule-clustering%20layer%20to%20capture%0Aentity-level%20structures%20and%20complex%20spatial%20relationships.%20This%20parallel%0Adecoding%20strategy%20enables%20the%20model%20to%20balance%20fine-detail%20preservation%20with%0Acontext-aware%20enhancements.%20CE-VAE%20achieves%20state-of-the-art%20performance%20in%0Aunderwater%20image%20enhancement%20on%20six%20benchmark%20datasets%2C%20providing%20up%20to%203x%0Ahigher%20compression%20efficiency%20than%20existing%20approaches.%20Code%20available%20at%0A%5Curl%7Bhttps%3A//github.com/iN1k1/ce-vae-underwater-image-enhancement%7D.%0A&entry.1838667208=http%3A//arxiv.org/abs/2406.01294v2&entry.124074799=Read"},
{"title": "HistoEncoder: a digital pathology foundation model for prostate cancer", "author": "Joona Pohjonen and Abderrahim-Oussama Batouche and Antti Rannikko and Kevin Sandeman and Andrew Erickson and Esa Pitkanen and Tuomas Mirtti", "abstract": "  Foundation models are trained on massive amounts of data to distinguish\ncomplex patterns and can be adapted to a wide range of downstream tasks with\nminimal computational resources. Here, we develop a foundation model for\nprostate cancer digital pathology called HistoEncoder by pre-training on 48\nmillion prostate tissue tile images. We demonstrate that HistoEncoder features\nextracted from tile images with similar histological patterns map closely\ntogether in the feature space. HistoEncoder outperforms models pre-trained with\nnatural images, even without fine-tuning or with 1000 times less training data.\nWe describe two use cases that leverage the capabilities of HistoEncoder by\nfine-tuning the model with a limited amount of data and computational\nresources. First, we show how HistoEncoder can be used to automatically\nannotate large-scale datasets with high accuracy. Second, we combine histomics\nwith commonly used clinical nomograms, significantly improving prostate\ncancer-specific death survival models. Foundation models such as HistoEncoder\ncan allow organizations with limited resources to build effective clinical\nsoftware tools without needing extensive datasets or significant amounts of\ncomputing.\n", "link": "http://arxiv.org/abs/2411.11458v2", "date": "2024-11-22", "relevancy": 2.5612, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5447}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5447}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.4474}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20HistoEncoder%3A%20a%20digital%20pathology%20foundation%20model%20for%20prostate%20cancer&body=Title%3A%20HistoEncoder%3A%20a%20digital%20pathology%20foundation%20model%20for%20prostate%20cancer%0AAuthor%3A%20Joona%20Pohjonen%20and%20Abderrahim-Oussama%20Batouche%20and%20Antti%20Rannikko%20and%20Kevin%20Sandeman%20and%20Andrew%20Erickson%20and%20Esa%20Pitkanen%20and%20Tuomas%20Mirtti%0AAbstract%3A%20%20%20Foundation%20models%20are%20trained%20on%20massive%20amounts%20of%20data%20to%20distinguish%0Acomplex%20patterns%20and%20can%20be%20adapted%20to%20a%20wide%20range%20of%20downstream%20tasks%20with%0Aminimal%20computational%20resources.%20Here%2C%20we%20develop%20a%20foundation%20model%20for%0Aprostate%20cancer%20digital%20pathology%20called%20HistoEncoder%20by%20pre-training%20on%2048%0Amillion%20prostate%20tissue%20tile%20images.%20We%20demonstrate%20that%20HistoEncoder%20features%0Aextracted%20from%20tile%20images%20with%20similar%20histological%20patterns%20map%20closely%0Atogether%20in%20the%20feature%20space.%20HistoEncoder%20outperforms%20models%20pre-trained%20with%0Anatural%20images%2C%20even%20without%20fine-tuning%20or%20with%201000%20times%20less%20training%20data.%0AWe%20describe%20two%20use%20cases%20that%20leverage%20the%20capabilities%20of%20HistoEncoder%20by%0Afine-tuning%20the%20model%20with%20a%20limited%20amount%20of%20data%20and%20computational%0Aresources.%20First%2C%20we%20show%20how%20HistoEncoder%20can%20be%20used%20to%20automatically%0Aannotate%20large-scale%20datasets%20with%20high%20accuracy.%20Second%2C%20we%20combine%20histomics%0Awith%20commonly%20used%20clinical%20nomograms%2C%20significantly%20improving%20prostate%0Acancer-specific%20death%20survival%20models.%20Foundation%20models%20such%20as%20HistoEncoder%0Acan%20allow%20organizations%20with%20limited%20resources%20to%20build%20effective%20clinical%0Asoftware%20tools%20without%20needing%20extensive%20datasets%20or%20significant%20amounts%20of%0Acomputing.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2411.11458v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DHistoEncoder%253A%2520a%2520digital%2520pathology%2520foundation%2520model%2520for%2520prostate%2520cancer%26entry.906535625%3DJoona%2520Pohjonen%2520and%2520Abderrahim-Oussama%2520Batouche%2520and%2520Antti%2520Rannikko%2520and%2520Kevin%2520Sandeman%2520and%2520Andrew%2520Erickson%2520and%2520Esa%2520Pitkanen%2520and%2520Tuomas%2520Mirtti%26entry.1292438233%3D%2520%2520Foundation%2520models%2520are%2520trained%2520on%2520massive%2520amounts%2520of%2520data%2520to%2520distinguish%250Acomplex%2520patterns%2520and%2520can%2520be%2520adapted%2520to%2520a%2520wide%2520range%2520of%2520downstream%2520tasks%2520with%250Aminimal%2520computational%2520resources.%2520Here%252C%2520we%2520develop%2520a%2520foundation%2520model%2520for%250Aprostate%2520cancer%2520digital%2520pathology%2520called%2520HistoEncoder%2520by%2520pre-training%2520on%252048%250Amillion%2520prostate%2520tissue%2520tile%2520images.%2520We%2520demonstrate%2520that%2520HistoEncoder%2520features%250Aextracted%2520from%2520tile%2520images%2520with%2520similar%2520histological%2520patterns%2520map%2520closely%250Atogether%2520in%2520the%2520feature%2520space.%2520HistoEncoder%2520outperforms%2520models%2520pre-trained%2520with%250Anatural%2520images%252C%2520even%2520without%2520fine-tuning%2520or%2520with%25201000%2520times%2520less%2520training%2520data.%250AWe%2520describe%2520two%2520use%2520cases%2520that%2520leverage%2520the%2520capabilities%2520of%2520HistoEncoder%2520by%250Afine-tuning%2520the%2520model%2520with%2520a%2520limited%2520amount%2520of%2520data%2520and%2520computational%250Aresources.%2520First%252C%2520we%2520show%2520how%2520HistoEncoder%2520can%2520be%2520used%2520to%2520automatically%250Aannotate%2520large-scale%2520datasets%2520with%2520high%2520accuracy.%2520Second%252C%2520we%2520combine%2520histomics%250Awith%2520commonly%2520used%2520clinical%2520nomograms%252C%2520significantly%2520improving%2520prostate%250Acancer-specific%2520death%2520survival%2520models.%2520Foundation%2520models%2520such%2520as%2520HistoEncoder%250Acan%2520allow%2520organizations%2520with%2520limited%2520resources%2520to%2520build%2520effective%2520clinical%250Asoftware%2520tools%2520without%2520needing%2520extensive%2520datasets%2520or%2520significant%2520amounts%2520of%250Acomputing.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2411.11458v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=HistoEncoder%3A%20a%20digital%20pathology%20foundation%20model%20for%20prostate%20cancer&entry.906535625=Joona%20Pohjonen%20and%20Abderrahim-Oussama%20Batouche%20and%20Antti%20Rannikko%20and%20Kevin%20Sandeman%20and%20Andrew%20Erickson%20and%20Esa%20Pitkanen%20and%20Tuomas%20Mirtti&entry.1292438233=%20%20Foundation%20models%20are%20trained%20on%20massive%20amounts%20of%20data%20to%20distinguish%0Acomplex%20patterns%20and%20can%20be%20adapted%20to%20a%20wide%20range%20of%20downstream%20tasks%20with%0Aminimal%20computational%20resources.%20Here%2C%20we%20develop%20a%20foundation%20model%20for%0Aprostate%20cancer%20digital%20pathology%20called%20HistoEncoder%20by%20pre-training%20on%2048%0Amillion%20prostate%20tissue%20tile%20images.%20We%20demonstrate%20that%20HistoEncoder%20features%0Aextracted%20from%20tile%20images%20with%20similar%20histological%20patterns%20map%20closely%0Atogether%20in%20the%20feature%20space.%20HistoEncoder%20outperforms%20models%20pre-trained%20with%0Anatural%20images%2C%20even%20without%20fine-tuning%20or%20with%201000%20times%20less%20training%20data.%0AWe%20describe%20two%20use%20cases%20that%20leverage%20the%20capabilities%20of%20HistoEncoder%20by%0Afine-tuning%20the%20model%20with%20a%20limited%20amount%20of%20data%20and%20computational%0Aresources.%20First%2C%20we%20show%20how%20HistoEncoder%20can%20be%20used%20to%20automatically%0Aannotate%20large-scale%20datasets%20with%20high%20accuracy.%20Second%2C%20we%20combine%20histomics%0Awith%20commonly%20used%20clinical%20nomograms%2C%20significantly%20improving%20prostate%0Acancer-specific%20death%20survival%20models.%20Foundation%20models%20such%20as%20HistoEncoder%0Acan%20allow%20organizations%20with%20limited%20resources%20to%20build%20effective%20clinical%0Asoftware%20tools%20without%20needing%20extensive%20datasets%20or%20significant%20amounts%20of%0Acomputing.%0A&entry.1838667208=http%3A//arxiv.org/abs/2411.11458v2&entry.124074799=Read"},
{"title": "RankByGene: Gene-Guided Histopathology Representation Learning Through\n  Cross-Modal Ranking Consistency", "author": "Wentao Huang and Meilong Xu and Xiaoling Hu and Shahira Abousamra and Aniruddha Ganguly and Saarthak Kapse and Alisa Yurovsky and Prateek Prasanna and Tahsin Kurc and Joel Saltz and Michael L. Miller and Chao Chen", "abstract": "  Spatial transcriptomics (ST) provides essential spatial context by mapping\ngene expression within tissue, enabling detailed study of cellular\nheterogeneity and tissue organization. However, aligning ST data with histology\nimages poses challenges due to inherent spatial distortions and\nmodality-specific variations. Existing methods largely rely on direct\nalignment, which often fails to capture complex cross-modal relationships. To\naddress these limitations, we propose a novel framework that aligns gene and\nimage features using a ranking-based alignment loss, preserving relative\nsimilarity across modalities and enabling robust multi-scale alignment. To\nfurther enhance the alignment's stability, we employ self-supervised knowledge\ndistillation with a teacher-student network architecture, effectively\nmitigating disruptions from high dimensionality, sparsity, and noise in gene\nexpression data. Extensive experiments on gene expression prediction and\nsurvival analysis demonstrate our framework's effectiveness, showing improved\nalignment and predictive performance over existing methods and establishing a\nrobust tool for gene-guided image representation learning in digital pathology.\n", "link": "http://arxiv.org/abs/2411.15076v1", "date": "2024-11-22", "relevancy": 2.5451, "topK": [{"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5188}, {"title": "Self-supervised Photographic Image Layout Representation Learning", "link": "http://arxiv.org/abs/2403.03740v1", "similarity": 0.5101}, {"title": "DDM-NET: End-to-end learning of keypoint feature Detection, Description and Matching for 3D localization", "link": "https://arxiv.org/abs/2212.04575", "similarity": 0.4982}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20RankByGene%3A%20Gene-Guided%20Histopathology%20Representation%20Learning%20Through%0A%20%20Cross-Modal%20Ranking%20Consistency&body=Title%3A%20RankByGene%3A%20Gene-Guided%20Histopathology%20Representation%20Learning%20Through%0A%20%20Cross-Modal%20Ranking%20Consistency%0AAuthor%3A%20Wentao%20Huang%20and%20Meilong%20Xu%20and%20Xiaoling%20Hu%20and%20Shahira%20Abousamra%20and%20Aniruddha%20Ganguly%20and%20Saarthak%20Kapse%20and%20Alisa%20Yurovsky%20and%20Prateek%20Prasanna%20and%20Tahsin%20Kurc%20and%20Joel%20Saltz%20and%20Michael%20L.%20Miller%20and%20Chao%20Chen%0AAbstract%3A%20%20%20Spatial%20transcriptomics%20%28ST%29%20provides%20essential%20spatial%20context%20by%20mapping%0Agene%20expression%20within%20tissue%2C%20enabling%20detailed%20study%20of%20cellular%0Aheterogeneity%20and%20tissue%20organization.%20However%2C%20aligning%20ST%20data%20with%20histology%0Aimages%20poses%20challenges%20due%20to%20inherent%20spatial%20distortions%20and%0Amodality-specific%20variations.%20Existing%20methods%20largely%20rely%20on%20direct%0Aalignment%2C%20which%20often%20fails%20to%20capture%20complex%20cross-modal%20relationships.%20To%0Aaddress%20these%20limitations%2C%20we%20propose%20a%20novel%20framework%20that%20aligns%20gene%20and%0Aimage%20features%20using%20a%20ranking-based%20alignment%20loss%2C%20preserving%20relative%0Asimilarity%20across%20modalities%20and%20enabling%20robust%20multi-scale%20alignment.%20To%0Afurther%20enhance%20the%20alignment%27s%20stability%2C%20we%20employ%20self-supervised%20knowledge%0Adistillation%20with%20a%20teacher-student%20network%20architecture%2C%20effectively%0Amitigating%20disruptions%20from%20high%20dimensionality%2C%20sparsity%2C%20and%20noise%20in%20gene%0Aexpression%20data.%20Extensive%20experiments%20on%20gene%20expression%20prediction%20and%0Asurvival%20analysis%20demonstrate%20our%20framework%27s%20effectiveness%2C%20showing%20improved%0Aalignment%20and%20predictive%20performance%20over%20existing%20methods%20and%20establishing%20a%0Arobust%20tool%20for%20gene-guided%20image%20representation%20learning%20in%20digital%20pathology.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2411.15076v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DRankByGene%253A%2520Gene-Guided%2520Histopathology%2520Representation%2520Learning%2520Through%250A%2520%2520Cross-Modal%2520Ranking%2520Consistency%26entry.906535625%3DWentao%2520Huang%2520and%2520Meilong%2520Xu%2520and%2520Xiaoling%2520Hu%2520and%2520Shahira%2520Abousamra%2520and%2520Aniruddha%2520Ganguly%2520and%2520Saarthak%2520Kapse%2520and%2520Alisa%2520Yurovsky%2520and%2520Prateek%2520Prasanna%2520and%2520Tahsin%2520Kurc%2520and%2520Joel%2520Saltz%2520and%2520Michael%2520L.%2520Miller%2520and%2520Chao%2520Chen%26entry.1292438233%3D%2520%2520Spatial%2520transcriptomics%2520%2528ST%2529%2520provides%2520essential%2520spatial%2520context%2520by%2520mapping%250Agene%2520expression%2520within%2520tissue%252C%2520enabling%2520detailed%2520study%2520of%2520cellular%250Aheterogeneity%2520and%2520tissue%2520organization.%2520However%252C%2520aligning%2520ST%2520data%2520with%2520histology%250Aimages%2520poses%2520challenges%2520due%2520to%2520inherent%2520spatial%2520distortions%2520and%250Amodality-specific%2520variations.%2520Existing%2520methods%2520largely%2520rely%2520on%2520direct%250Aalignment%252C%2520which%2520often%2520fails%2520to%2520capture%2520complex%2520cross-modal%2520relationships.%2520To%250Aaddress%2520these%2520limitations%252C%2520we%2520propose%2520a%2520novel%2520framework%2520that%2520aligns%2520gene%2520and%250Aimage%2520features%2520using%2520a%2520ranking-based%2520alignment%2520loss%252C%2520preserving%2520relative%250Asimilarity%2520across%2520modalities%2520and%2520enabling%2520robust%2520multi-scale%2520alignment.%2520To%250Afurther%2520enhance%2520the%2520alignment%2527s%2520stability%252C%2520we%2520employ%2520self-supervised%2520knowledge%250Adistillation%2520with%2520a%2520teacher-student%2520network%2520architecture%252C%2520effectively%250Amitigating%2520disruptions%2520from%2520high%2520dimensionality%252C%2520sparsity%252C%2520and%2520noise%2520in%2520gene%250Aexpression%2520data.%2520Extensive%2520experiments%2520on%2520gene%2520expression%2520prediction%2520and%250Asurvival%2520analysis%2520demonstrate%2520our%2520framework%2527s%2520effectiveness%252C%2520showing%2520improved%250Aalignment%2520and%2520predictive%2520performance%2520over%2520existing%2520methods%2520and%2520establishing%2520a%250Arobust%2520tool%2520for%2520gene-guided%2520image%2520representation%2520learning%2520in%2520digital%2520pathology.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2411.15076v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=RankByGene%3A%20Gene-Guided%20Histopathology%20Representation%20Learning%20Through%0A%20%20Cross-Modal%20Ranking%20Consistency&entry.906535625=Wentao%20Huang%20and%20Meilong%20Xu%20and%20Xiaoling%20Hu%20and%20Shahira%20Abousamra%20and%20Aniruddha%20Ganguly%20and%20Saarthak%20Kapse%20and%20Alisa%20Yurovsky%20and%20Prateek%20Prasanna%20and%20Tahsin%20Kurc%20and%20Joel%20Saltz%20and%20Michael%20L.%20Miller%20and%20Chao%20Chen&entry.1292438233=%20%20Spatial%20transcriptomics%20%28ST%29%20provides%20essential%20spatial%20context%20by%20mapping%0Agene%20expression%20within%20tissue%2C%20enabling%20detailed%20study%20of%20cellular%0Aheterogeneity%20and%20tissue%20organization.%20However%2C%20aligning%20ST%20data%20with%20histology%0Aimages%20poses%20challenges%20due%20to%20inherent%20spatial%20distortions%20and%0Amodality-specific%20variations.%20Existing%20methods%20largely%20rely%20on%20direct%0Aalignment%2C%20which%20often%20fails%20to%20capture%20complex%20cross-modal%20relationships.%20To%0Aaddress%20these%20limitations%2C%20we%20propose%20a%20novel%20framework%20that%20aligns%20gene%20and%0Aimage%20features%20using%20a%20ranking-based%20alignment%20loss%2C%20preserving%20relative%0Asimilarity%20across%20modalities%20and%20enabling%20robust%20multi-scale%20alignment.%20To%0Afurther%20enhance%20the%20alignment%27s%20stability%2C%20we%20employ%20self-supervised%20knowledge%0Adistillation%20with%20a%20teacher-student%20network%20architecture%2C%20effectively%0Amitigating%20disruptions%20from%20high%20dimensionality%2C%20sparsity%2C%20and%20noise%20in%20gene%0Aexpression%20data.%20Extensive%20experiments%20on%20gene%20expression%20prediction%20and%0Asurvival%20analysis%20demonstrate%20our%20framework%27s%20effectiveness%2C%20showing%20improved%0Aalignment%20and%20predictive%20performance%20over%20existing%20methods%20and%20establishing%20a%0Arobust%20tool%20for%20gene-guided%20image%20representation%20learning%20in%20digital%20pathology.%0A&entry.1838667208=http%3A//arxiv.org/abs/2411.15076v1&entry.124074799=Read"},
{"title": "Adaptive Communications in Collaborative Perception with Domain\n  Alignment for Autonomous Driving", "author": "Senkang Hu and Zhengru Fang and Haonan An and Guowen Xu and Yuan Zhou and Xianhao Chen and Yuguang Fang", "abstract": "  Collaborative perception among multiple connected and autonomous vehicles can\ngreatly enhance perceptive capabilities by allowing vehicles to exchange\nsupplementary information via communications. Despite advances in previous\napproaches, challenges still remain due to channel variations and data\nheterogeneity among collaborative vehicles. To address these issues, we propose\nACC-DA, a channel-aware collaborative perception framework to dynamically\nadjust the communication graph and minimize the average transmission delay\nwhile mitigating the side effects from the data heterogeneity. Our novelties\nlie in three aspects. We first design a transmission delay minimization method,\nwhich can construct the communication graph and minimize the transmission delay\naccording to different channel information state. We then propose an adaptive\ndata reconstruction mechanism, which can dynamically adjust the rate-distortion\ntrade-off to enhance perception efficiency. Moreover, it minimizes the temporal\nredundancy during data transmissions. Finally, we conceive a domain alignment\nscheme to align the data distribution from different vehicles, which can\nmitigate the domain gap between different vehicles and improve the performance\nof the target task. Comprehensive experiments demonstrate the effectiveness of\nour method in comparison to the existing state-of-the-art works.\n", "link": "http://arxiv.org/abs/2310.00013v4", "date": "2024-11-22", "relevancy": 2.5254, "topK": [{"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.542}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.4931}, {"title": "Egocentric Computer Vision for Hands-Free Robotic Wheelchair Navigation", "link": "https://link.springer.com/article/10.1007/s10846-023-01807-4", "similarity": 0.4802}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Adaptive%20Communications%20in%20Collaborative%20Perception%20with%20Domain%0A%20%20Alignment%20for%20Autonomous%20Driving&body=Title%3A%20Adaptive%20Communications%20in%20Collaborative%20Perception%20with%20Domain%0A%20%20Alignment%20for%20Autonomous%20Driving%0AAuthor%3A%20Senkang%20Hu%20and%20Zhengru%20Fang%20and%20Haonan%20An%20and%20Guowen%20Xu%20and%20Yuan%20Zhou%20and%20Xianhao%20Chen%20and%20Yuguang%20Fang%0AAbstract%3A%20%20%20Collaborative%20perception%20among%20multiple%20connected%20and%20autonomous%20vehicles%20can%0Agreatly%20enhance%20perceptive%20capabilities%20by%20allowing%20vehicles%20to%20exchange%0Asupplementary%20information%20via%20communications.%20Despite%20advances%20in%20previous%0Aapproaches%2C%20challenges%20still%20remain%20due%20to%20channel%20variations%20and%20data%0Aheterogeneity%20among%20collaborative%20vehicles.%20To%20address%20these%20issues%2C%20we%20propose%0AACC-DA%2C%20a%20channel-aware%20collaborative%20perception%20framework%20to%20dynamically%0Aadjust%20the%20communication%20graph%20and%20minimize%20the%20average%20transmission%20delay%0Awhile%20mitigating%20the%20side%20effects%20from%20the%20data%20heterogeneity.%20Our%20novelties%0Alie%20in%20three%20aspects.%20We%20first%20design%20a%20transmission%20delay%20minimization%20method%2C%0Awhich%20can%20construct%20the%20communication%20graph%20and%20minimize%20the%20transmission%20delay%0Aaccording%20to%20different%20channel%20information%20state.%20We%20then%20propose%20an%20adaptive%0Adata%20reconstruction%20mechanism%2C%20which%20can%20dynamically%20adjust%20the%20rate-distortion%0Atrade-off%20to%20enhance%20perception%20efficiency.%20Moreover%2C%20it%20minimizes%20the%20temporal%0Aredundancy%20during%20data%20transmissions.%20Finally%2C%20we%20conceive%20a%20domain%20alignment%0Ascheme%20to%20align%20the%20data%20distribution%20from%20different%20vehicles%2C%20which%20can%0Amitigate%20the%20domain%20gap%20between%20different%20vehicles%20and%20improve%20the%20performance%0Aof%20the%20target%20task.%20Comprehensive%20experiments%20demonstrate%20the%20effectiveness%20of%0Aour%20method%20in%20comparison%20to%20the%20existing%20state-of-the-art%20works.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2310.00013v4%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DAdaptive%2520Communications%2520in%2520Collaborative%2520Perception%2520with%2520Domain%250A%2520%2520Alignment%2520for%2520Autonomous%2520Driving%26entry.906535625%3DSenkang%2520Hu%2520and%2520Zhengru%2520Fang%2520and%2520Haonan%2520An%2520and%2520Guowen%2520Xu%2520and%2520Yuan%2520Zhou%2520and%2520Xianhao%2520Chen%2520and%2520Yuguang%2520Fang%26entry.1292438233%3D%2520%2520Collaborative%2520perception%2520among%2520multiple%2520connected%2520and%2520autonomous%2520vehicles%2520can%250Agreatly%2520enhance%2520perceptive%2520capabilities%2520by%2520allowing%2520vehicles%2520to%2520exchange%250Asupplementary%2520information%2520via%2520communications.%2520Despite%2520advances%2520in%2520previous%250Aapproaches%252C%2520challenges%2520still%2520remain%2520due%2520to%2520channel%2520variations%2520and%2520data%250Aheterogeneity%2520among%2520collaborative%2520vehicles.%2520To%2520address%2520these%2520issues%252C%2520we%2520propose%250AACC-DA%252C%2520a%2520channel-aware%2520collaborative%2520perception%2520framework%2520to%2520dynamically%250Aadjust%2520the%2520communication%2520graph%2520and%2520minimize%2520the%2520average%2520transmission%2520delay%250Awhile%2520mitigating%2520the%2520side%2520effects%2520from%2520the%2520data%2520heterogeneity.%2520Our%2520novelties%250Alie%2520in%2520three%2520aspects.%2520We%2520first%2520design%2520a%2520transmission%2520delay%2520minimization%2520method%252C%250Awhich%2520can%2520construct%2520the%2520communication%2520graph%2520and%2520minimize%2520the%2520transmission%2520delay%250Aaccording%2520to%2520different%2520channel%2520information%2520state.%2520We%2520then%2520propose%2520an%2520adaptive%250Adata%2520reconstruction%2520mechanism%252C%2520which%2520can%2520dynamically%2520adjust%2520the%2520rate-distortion%250Atrade-off%2520to%2520enhance%2520perception%2520efficiency.%2520Moreover%252C%2520it%2520minimizes%2520the%2520temporal%250Aredundancy%2520during%2520data%2520transmissions.%2520Finally%252C%2520we%2520conceive%2520a%2520domain%2520alignment%250Ascheme%2520to%2520align%2520the%2520data%2520distribution%2520from%2520different%2520vehicles%252C%2520which%2520can%250Amitigate%2520the%2520domain%2520gap%2520between%2520different%2520vehicles%2520and%2520improve%2520the%2520performance%250Aof%2520the%2520target%2520task.%2520Comprehensive%2520experiments%2520demonstrate%2520the%2520effectiveness%2520of%250Aour%2520method%2520in%2520comparison%2520to%2520the%2520existing%2520state-of-the-art%2520works.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2310.00013v4%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Adaptive%20Communications%20in%20Collaborative%20Perception%20with%20Domain%0A%20%20Alignment%20for%20Autonomous%20Driving&entry.906535625=Senkang%20Hu%20and%20Zhengru%20Fang%20and%20Haonan%20An%20and%20Guowen%20Xu%20and%20Yuan%20Zhou%20and%20Xianhao%20Chen%20and%20Yuguang%20Fang&entry.1292438233=%20%20Collaborative%20perception%20among%20multiple%20connected%20and%20autonomous%20vehicles%20can%0Agreatly%20enhance%20perceptive%20capabilities%20by%20allowing%20vehicles%20to%20exchange%0Asupplementary%20information%20via%20communications.%20Despite%20advances%20in%20previous%0Aapproaches%2C%20challenges%20still%20remain%20due%20to%20channel%20variations%20and%20data%0Aheterogeneity%20among%20collaborative%20vehicles.%20To%20address%20these%20issues%2C%20we%20propose%0AACC-DA%2C%20a%20channel-aware%20collaborative%20perception%20framework%20to%20dynamically%0Aadjust%20the%20communication%20graph%20and%20minimize%20the%20average%20transmission%20delay%0Awhile%20mitigating%20the%20side%20effects%20from%20the%20data%20heterogeneity.%20Our%20novelties%0Alie%20in%20three%20aspects.%20We%20first%20design%20a%20transmission%20delay%20minimization%20method%2C%0Awhich%20can%20construct%20the%20communication%20graph%20and%20minimize%20the%20transmission%20delay%0Aaccording%20to%20different%20channel%20information%20state.%20We%20then%20propose%20an%20adaptive%0Adata%20reconstruction%20mechanism%2C%20which%20can%20dynamically%20adjust%20the%20rate-distortion%0Atrade-off%20to%20enhance%20perception%20efficiency.%20Moreover%2C%20it%20minimizes%20the%20temporal%0Aredundancy%20during%20data%20transmissions.%20Finally%2C%20we%20conceive%20a%20domain%20alignment%0Ascheme%20to%20align%20the%20data%20distribution%20from%20different%20vehicles%2C%20which%20can%0Amitigate%20the%20domain%20gap%20between%20different%20vehicles%20and%20improve%20the%20performance%0Aof%20the%20target%20task.%20Comprehensive%20experiments%20demonstrate%20the%20effectiveness%20of%0Aour%20method%20in%20comparison%20to%20the%20existing%20state-of-the-art%20works.%0A&entry.1838667208=http%3A//arxiv.org/abs/2310.00013v4&entry.124074799=Read"},
{"title": "VideoRepair: Improving Text-to-Video Generation via Misalignment\n  Evaluation and Localized Refinement", "author": "Daeun Lee and Jaehong Yoon and Jaemin Cho and Mohit Bansal", "abstract": "  Recent text-to-video (T2V) diffusion models have demonstrated impressive\ngeneration capabilities across various domains. However, these models often\ngenerate videos that have misalignments with text prompts, especially when the\nprompts describe complex scenes with multiple objects and attributes. To\naddress this, we introduce VideoRepair, a novel model-agnostic, training-free\nvideo refinement framework that automatically identifies fine-grained\ntext-video misalignments and generates explicit spatial and textual feedback,\nenabling a T2V diffusion model to perform targeted, localized refinements.\nVideoRepair consists of four stages: In (1) video evaluation, we detect\nmisalignments by generating fine-grained evaluation questions and answering\nthose questions with MLLM. In (2) refinement planning, we identify accurately\ngenerated objects and then create localized prompts to refine other areas in\nthe video. Next, in (3) region decomposition, we segment the correctly\ngenerated area using a combined grounding module. We regenerate the video by\nadjusting the misaligned regions while preserving the correct regions in (4)\nlocalized refinement. On two popular video generation benchmarks (EvalCrafter\nand T2V-CompBench), VideoRepair substantially outperforms recent baselines\nacross various text-video alignment metrics. We provide a comprehensive\nanalysis of VideoRepair components and qualitative examples.\n", "link": "http://arxiv.org/abs/2411.15115v1", "date": "2024-11-22", "relevancy": 2.5144, "topK": [{"title": "Customize-A-Video: One-Shot Motion Customization of Text-to-Video\n  Diffusion Models", "link": "http://arxiv.org/abs/2402.14780v1", "similarity": 0.6757}, {"title": "Magic-Me: Identity-Specific Video Customized Diffusion", "link": "http://arxiv.org/abs/2402.09368v1", "similarity": 0.6257}, {"title": "Tuning-Free Noise Rectification for High Fidelity Image-to-Video\n  Generation", "link": "http://arxiv.org/abs/2403.02827v1", "similarity": 0.6127}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20VideoRepair%3A%20Improving%20Text-to-Video%20Generation%20via%20Misalignment%0A%20%20Evaluation%20and%20Localized%20Refinement&body=Title%3A%20VideoRepair%3A%20Improving%20Text-to-Video%20Generation%20via%20Misalignment%0A%20%20Evaluation%20and%20Localized%20Refinement%0AAuthor%3A%20Daeun%20Lee%20and%20Jaehong%20Yoon%20and%20Jaemin%20Cho%20and%20Mohit%20Bansal%0AAbstract%3A%20%20%20Recent%20text-to-video%20%28T2V%29%20diffusion%20models%20have%20demonstrated%20impressive%0Ageneration%20capabilities%20across%20various%20domains.%20However%2C%20these%20models%20often%0Agenerate%20videos%20that%20have%20misalignments%20with%20text%20prompts%2C%20especially%20when%20the%0Aprompts%20describe%20complex%20scenes%20with%20multiple%20objects%20and%20attributes.%20To%0Aaddress%20this%2C%20we%20introduce%20VideoRepair%2C%20a%20novel%20model-agnostic%2C%20training-free%0Avideo%20refinement%20framework%20that%20automatically%20identifies%20fine-grained%0Atext-video%20misalignments%20and%20generates%20explicit%20spatial%20and%20textual%20feedback%2C%0Aenabling%20a%20T2V%20diffusion%20model%20to%20perform%20targeted%2C%20localized%20refinements.%0AVideoRepair%20consists%20of%20four%20stages%3A%20In%20%281%29%20video%20evaluation%2C%20we%20detect%0Amisalignments%20by%20generating%20fine-grained%20evaluation%20questions%20and%20answering%0Athose%20questions%20with%20MLLM.%20In%20%282%29%20refinement%20planning%2C%20we%20identify%20accurately%0Agenerated%20objects%20and%20then%20create%20localized%20prompts%20to%20refine%20other%20areas%20in%0Athe%20video.%20Next%2C%20in%20%283%29%20region%20decomposition%2C%20we%20segment%20the%20correctly%0Agenerated%20area%20using%20a%20combined%20grounding%20module.%20We%20regenerate%20the%20video%20by%0Aadjusting%20the%20misaligned%20regions%20while%20preserving%20the%20correct%20regions%20in%20%284%29%0Alocalized%20refinement.%20On%20two%20popular%20video%20generation%20benchmarks%20%28EvalCrafter%0Aand%20T2V-CompBench%29%2C%20VideoRepair%20substantially%20outperforms%20recent%20baselines%0Aacross%20various%20text-video%20alignment%20metrics.%20We%20provide%20a%20comprehensive%0Aanalysis%20of%20VideoRepair%20components%20and%20qualitative%20examples.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2411.15115v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DVideoRepair%253A%2520Improving%2520Text-to-Video%2520Generation%2520via%2520Misalignment%250A%2520%2520Evaluation%2520and%2520Localized%2520Refinement%26entry.906535625%3DDaeun%2520Lee%2520and%2520Jaehong%2520Yoon%2520and%2520Jaemin%2520Cho%2520and%2520Mohit%2520Bansal%26entry.1292438233%3D%2520%2520Recent%2520text-to-video%2520%2528T2V%2529%2520diffusion%2520models%2520have%2520demonstrated%2520impressive%250Ageneration%2520capabilities%2520across%2520various%2520domains.%2520However%252C%2520these%2520models%2520often%250Agenerate%2520videos%2520that%2520have%2520misalignments%2520with%2520text%2520prompts%252C%2520especially%2520when%2520the%250Aprompts%2520describe%2520complex%2520scenes%2520with%2520multiple%2520objects%2520and%2520attributes.%2520To%250Aaddress%2520this%252C%2520we%2520introduce%2520VideoRepair%252C%2520a%2520novel%2520model-agnostic%252C%2520training-free%250Avideo%2520refinement%2520framework%2520that%2520automatically%2520identifies%2520fine-grained%250Atext-video%2520misalignments%2520and%2520generates%2520explicit%2520spatial%2520and%2520textual%2520feedback%252C%250Aenabling%2520a%2520T2V%2520diffusion%2520model%2520to%2520perform%2520targeted%252C%2520localized%2520refinements.%250AVideoRepair%2520consists%2520of%2520four%2520stages%253A%2520In%2520%25281%2529%2520video%2520evaluation%252C%2520we%2520detect%250Amisalignments%2520by%2520generating%2520fine-grained%2520evaluation%2520questions%2520and%2520answering%250Athose%2520questions%2520with%2520MLLM.%2520In%2520%25282%2529%2520refinement%2520planning%252C%2520we%2520identify%2520accurately%250Agenerated%2520objects%2520and%2520then%2520create%2520localized%2520prompts%2520to%2520refine%2520other%2520areas%2520in%250Athe%2520video.%2520Next%252C%2520in%2520%25283%2529%2520region%2520decomposition%252C%2520we%2520segment%2520the%2520correctly%250Agenerated%2520area%2520using%2520a%2520combined%2520grounding%2520module.%2520We%2520regenerate%2520the%2520video%2520by%250Aadjusting%2520the%2520misaligned%2520regions%2520while%2520preserving%2520the%2520correct%2520regions%2520in%2520%25284%2529%250Alocalized%2520refinement.%2520On%2520two%2520popular%2520video%2520generation%2520benchmarks%2520%2528EvalCrafter%250Aand%2520T2V-CompBench%2529%252C%2520VideoRepair%2520substantially%2520outperforms%2520recent%2520baselines%250Aacross%2520various%2520text-video%2520alignment%2520metrics.%2520We%2520provide%2520a%2520comprehensive%250Aanalysis%2520of%2520VideoRepair%2520components%2520and%2520qualitative%2520examples.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2411.15115v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=VideoRepair%3A%20Improving%20Text-to-Video%20Generation%20via%20Misalignment%0A%20%20Evaluation%20and%20Localized%20Refinement&entry.906535625=Daeun%20Lee%20and%20Jaehong%20Yoon%20and%20Jaemin%20Cho%20and%20Mohit%20Bansal&entry.1292438233=%20%20Recent%20text-to-video%20%28T2V%29%20diffusion%20models%20have%20demonstrated%20impressive%0Ageneration%20capabilities%20across%20various%20domains.%20However%2C%20these%20models%20often%0Agenerate%20videos%20that%20have%20misalignments%20with%20text%20prompts%2C%20especially%20when%20the%0Aprompts%20describe%20complex%20scenes%20with%20multiple%20objects%20and%20attributes.%20To%0Aaddress%20this%2C%20we%20introduce%20VideoRepair%2C%20a%20novel%20model-agnostic%2C%20training-free%0Avideo%20refinement%20framework%20that%20automatically%20identifies%20fine-grained%0Atext-video%20misalignments%20and%20generates%20explicit%20spatial%20and%20textual%20feedback%2C%0Aenabling%20a%20T2V%20diffusion%20model%20to%20perform%20targeted%2C%20localized%20refinements.%0AVideoRepair%20consists%20of%20four%20stages%3A%20In%20%281%29%20video%20evaluation%2C%20we%20detect%0Amisalignments%20by%20generating%20fine-grained%20evaluation%20questions%20and%20answering%0Athose%20questions%20with%20MLLM.%20In%20%282%29%20refinement%20planning%2C%20we%20identify%20accurately%0Agenerated%20objects%20and%20then%20create%20localized%20prompts%20to%20refine%20other%20areas%20in%0Athe%20video.%20Next%2C%20in%20%283%29%20region%20decomposition%2C%20we%20segment%20the%20correctly%0Agenerated%20area%20using%20a%20combined%20grounding%20module.%20We%20regenerate%20the%20video%20by%0Aadjusting%20the%20misaligned%20regions%20while%20preserving%20the%20correct%20regions%20in%20%284%29%0Alocalized%20refinement.%20On%20two%20popular%20video%20generation%20benchmarks%20%28EvalCrafter%0Aand%20T2V-CompBench%29%2C%20VideoRepair%20substantially%20outperforms%20recent%20baselines%0Aacross%20various%20text-video%20alignment%20metrics.%20We%20provide%20a%20comprehensive%0Aanalysis%20of%20VideoRepair%20components%20and%20qualitative%20examples.%0A&entry.1838667208=http%3A//arxiv.org/abs/2411.15115v1&entry.124074799=Read"},
{"title": "Saliency Map-based Image Retrieval using Invariant Krawtchouk Moments", "author": "Ashkan Nejad and Mohammad Reza Faraji and Xiaojun Qi", "abstract": "  With the widespread adoption of digital devices equipped with cameras and the\nrapid development of Internet technology, numerous content-based image\nretrieval systems and novel image feature extraction techniques have emerged in\nrecent years. This paper introduces a saliency map-based image retrieval\napproach using invariant Krawtchouk moments (SM-IKM) to enhance retrieval speed\nand accuracy. The proposed method applies a global contrast-based salient\nregion detection algorithm to create a saliency map that effectively isolates\nthe foreground from the background. It then combines multiple orders of\ninvariant Krawtchouk moments (IKM) with local binary patterns (LBPs) and color\nhistograms to comprehensively represent the foreground and background.\nAdditionally, it incorporates LBPs derived from the saliency map to improve\ndiscriminative power, facilitating more precise image differentiation. A\nbag-of-visual-words (BoVW) model is employed to generate a codebook for\nclassification and discrimination. By using compact IKMs in the BoVW framework\nand integrating a range of region-based feature-including color histograms,\nLBPs, and saliency map-enhanced LBPs, our proposed SM-IKM achieves efficient\nand accurate image retrieval. Extensive experiments on publicly available\ndatasets, such as Caltech 101 and Wang, demonstrate that SM-IKM outperforms\nrecent state-of-the-art retrieval methods. The source code for SM-IKM is\navailable at github.com/arnejad/SMIKM.\n", "link": "http://arxiv.org/abs/2411.08567v2", "date": "2024-11-22", "relevancy": 2.5107, "topK": [{"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.5149}, {"title": "VirtualModel: Generating Object-ID-retentive Human-object Interaction\n  Image by Diffusion Model for E-commerce Marketing", "link": "http://arxiv.org/abs/2405.09985v1", "similarity": 0.4977}, {"title": "Self-supervised Photographic Image Layout Representation Learning", "link": "http://arxiv.org/abs/2403.03740v1", "similarity": 0.4937}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Saliency%20Map-based%20Image%20Retrieval%20using%20Invariant%20Krawtchouk%20Moments&body=Title%3A%20Saliency%20Map-based%20Image%20Retrieval%20using%20Invariant%20Krawtchouk%20Moments%0AAuthor%3A%20Ashkan%20Nejad%20and%20Mohammad%20Reza%20Faraji%20and%20Xiaojun%20Qi%0AAbstract%3A%20%20%20With%20the%20widespread%20adoption%20of%20digital%20devices%20equipped%20with%20cameras%20and%20the%0Arapid%20development%20of%20Internet%20technology%2C%20numerous%20content-based%20image%0Aretrieval%20systems%20and%20novel%20image%20feature%20extraction%20techniques%20have%20emerged%20in%0Arecent%20years.%20This%20paper%20introduces%20a%20saliency%20map-based%20image%20retrieval%0Aapproach%20using%20invariant%20Krawtchouk%20moments%20%28SM-IKM%29%20to%20enhance%20retrieval%20speed%0Aand%20accuracy.%20The%20proposed%20method%20applies%20a%20global%20contrast-based%20salient%0Aregion%20detection%20algorithm%20to%20create%20a%20saliency%20map%20that%20effectively%20isolates%0Athe%20foreground%20from%20the%20background.%20It%20then%20combines%20multiple%20orders%20of%0Ainvariant%20Krawtchouk%20moments%20%28IKM%29%20with%20local%20binary%20patterns%20%28LBPs%29%20and%20color%0Ahistograms%20to%20comprehensively%20represent%20the%20foreground%20and%20background.%0AAdditionally%2C%20it%20incorporates%20LBPs%20derived%20from%20the%20saliency%20map%20to%20improve%0Adiscriminative%20power%2C%20facilitating%20more%20precise%20image%20differentiation.%20A%0Abag-of-visual-words%20%28BoVW%29%20model%20is%20employed%20to%20generate%20a%20codebook%20for%0Aclassification%20and%20discrimination.%20By%20using%20compact%20IKMs%20in%20the%20BoVW%20framework%0Aand%20integrating%20a%20range%20of%20region-based%20feature-including%20color%20histograms%2C%0ALBPs%2C%20and%20saliency%20map-enhanced%20LBPs%2C%20our%20proposed%20SM-IKM%20achieves%20efficient%0Aand%20accurate%20image%20retrieval.%20Extensive%20experiments%20on%20publicly%20available%0Adatasets%2C%20such%20as%20Caltech%20101%20and%20Wang%2C%20demonstrate%20that%20SM-IKM%20outperforms%0Arecent%20state-of-the-art%20retrieval%20methods.%20The%20source%20code%20for%20SM-IKM%20is%0Aavailable%20at%20github.com/arnejad/SMIKM.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2411.08567v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DSaliency%2520Map-based%2520Image%2520Retrieval%2520using%2520Invariant%2520Krawtchouk%2520Moments%26entry.906535625%3DAshkan%2520Nejad%2520and%2520Mohammad%2520Reza%2520Faraji%2520and%2520Xiaojun%2520Qi%26entry.1292438233%3D%2520%2520With%2520the%2520widespread%2520adoption%2520of%2520digital%2520devices%2520equipped%2520with%2520cameras%2520and%2520the%250Arapid%2520development%2520of%2520Internet%2520technology%252C%2520numerous%2520content-based%2520image%250Aretrieval%2520systems%2520and%2520novel%2520image%2520feature%2520extraction%2520techniques%2520have%2520emerged%2520in%250Arecent%2520years.%2520This%2520paper%2520introduces%2520a%2520saliency%2520map-based%2520image%2520retrieval%250Aapproach%2520using%2520invariant%2520Krawtchouk%2520moments%2520%2528SM-IKM%2529%2520to%2520enhance%2520retrieval%2520speed%250Aand%2520accuracy.%2520The%2520proposed%2520method%2520applies%2520a%2520global%2520contrast-based%2520salient%250Aregion%2520detection%2520algorithm%2520to%2520create%2520a%2520saliency%2520map%2520that%2520effectively%2520isolates%250Athe%2520foreground%2520from%2520the%2520background.%2520It%2520then%2520combines%2520multiple%2520orders%2520of%250Ainvariant%2520Krawtchouk%2520moments%2520%2528IKM%2529%2520with%2520local%2520binary%2520patterns%2520%2528LBPs%2529%2520and%2520color%250Ahistograms%2520to%2520comprehensively%2520represent%2520the%2520foreground%2520and%2520background.%250AAdditionally%252C%2520it%2520incorporates%2520LBPs%2520derived%2520from%2520the%2520saliency%2520map%2520to%2520improve%250Adiscriminative%2520power%252C%2520facilitating%2520more%2520precise%2520image%2520differentiation.%2520A%250Abag-of-visual-words%2520%2528BoVW%2529%2520model%2520is%2520employed%2520to%2520generate%2520a%2520codebook%2520for%250Aclassification%2520and%2520discrimination.%2520By%2520using%2520compact%2520IKMs%2520in%2520the%2520BoVW%2520framework%250Aand%2520integrating%2520a%2520range%2520of%2520region-based%2520feature-including%2520color%2520histograms%252C%250ALBPs%252C%2520and%2520saliency%2520map-enhanced%2520LBPs%252C%2520our%2520proposed%2520SM-IKM%2520achieves%2520efficient%250Aand%2520accurate%2520image%2520retrieval.%2520Extensive%2520experiments%2520on%2520publicly%2520available%250Adatasets%252C%2520such%2520as%2520Caltech%2520101%2520and%2520Wang%252C%2520demonstrate%2520that%2520SM-IKM%2520outperforms%250Arecent%2520state-of-the-art%2520retrieval%2520methods.%2520The%2520source%2520code%2520for%2520SM-IKM%2520is%250Aavailable%2520at%2520github.com/arnejad/SMIKM.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2411.08567v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Saliency%20Map-based%20Image%20Retrieval%20using%20Invariant%20Krawtchouk%20Moments&entry.906535625=Ashkan%20Nejad%20and%20Mohammad%20Reza%20Faraji%20and%20Xiaojun%20Qi&entry.1292438233=%20%20With%20the%20widespread%20adoption%20of%20digital%20devices%20equipped%20with%20cameras%20and%20the%0Arapid%20development%20of%20Internet%20technology%2C%20numerous%20content-based%20image%0Aretrieval%20systems%20and%20novel%20image%20feature%20extraction%20techniques%20have%20emerged%20in%0Arecent%20years.%20This%20paper%20introduces%20a%20saliency%20map-based%20image%20retrieval%0Aapproach%20using%20invariant%20Krawtchouk%20moments%20%28SM-IKM%29%20to%20enhance%20retrieval%20speed%0Aand%20accuracy.%20The%20proposed%20method%20applies%20a%20global%20contrast-based%20salient%0Aregion%20detection%20algorithm%20to%20create%20a%20saliency%20map%20that%20effectively%20isolates%0Athe%20foreground%20from%20the%20background.%20It%20then%20combines%20multiple%20orders%20of%0Ainvariant%20Krawtchouk%20moments%20%28IKM%29%20with%20local%20binary%20patterns%20%28LBPs%29%20and%20color%0Ahistograms%20to%20comprehensively%20represent%20the%20foreground%20and%20background.%0AAdditionally%2C%20it%20incorporates%20LBPs%20derived%20from%20the%20saliency%20map%20to%20improve%0Adiscriminative%20power%2C%20facilitating%20more%20precise%20image%20differentiation.%20A%0Abag-of-visual-words%20%28BoVW%29%20model%20is%20employed%20to%20generate%20a%20codebook%20for%0Aclassification%20and%20discrimination.%20By%20using%20compact%20IKMs%20in%20the%20BoVW%20framework%0Aand%20integrating%20a%20range%20of%20region-based%20feature-including%20color%20histograms%2C%0ALBPs%2C%20and%20saliency%20map-enhanced%20LBPs%2C%20our%20proposed%20SM-IKM%20achieves%20efficient%0Aand%20accurate%20image%20retrieval.%20Extensive%20experiments%20on%20publicly%20available%0Adatasets%2C%20such%20as%20Caltech%20101%20and%20Wang%2C%20demonstrate%20that%20SM-IKM%20outperforms%0Arecent%20state-of-the-art%20retrieval%20methods.%20The%20source%20code%20for%20SM-IKM%20is%0Aavailable%20at%20github.com/arnejad/SMIKM.%0A&entry.1838667208=http%3A//arxiv.org/abs/2411.08567v2&entry.124074799=Read"},
{"title": "Exploring Foundation Models Fine-Tuning for Cytology Classification", "author": "Manon Dausort and Tiffanie Godelaine and Maxime Zanella and Karim El Khoury and Isabelle Salmon and Beno\u00eet Macq", "abstract": "  Cytology slides are essential tools in diagnosing and staging cancer, but\ntheir analysis is time-consuming and costly. Foundation models have shown great\npotential to assist in these tasks. In this paper, we explore how existing\nfoundation models can be applied to cytological classification. More\nparticularly, we focus on low-rank adaptation, a parameter-efficient\nfine-tuning method suited to few-shot learning. We evaluated five foundation\nmodels across four cytological classification datasets. Our results demonstrate\nthat fine-tuning the pre-trained backbones with LoRA significantly improves\nmodel performance compared to fine-tuning only the classifier head, achieving\nstate-of-the-art results on both simple and complex classification tasks while\nrequiring fewer data samples.\n", "link": "http://arxiv.org/abs/2411.14975v1", "date": "2024-11-22", "relevancy": 2.5017, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.508}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.508}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.485}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Exploring%20Foundation%20Models%20Fine-Tuning%20for%20Cytology%20Classification&body=Title%3A%20Exploring%20Foundation%20Models%20Fine-Tuning%20for%20Cytology%20Classification%0AAuthor%3A%20Manon%20Dausort%20and%20Tiffanie%20Godelaine%20and%20Maxime%20Zanella%20and%20Karim%20El%20Khoury%20and%20Isabelle%20Salmon%20and%20Beno%C3%AEt%20Macq%0AAbstract%3A%20%20%20Cytology%20slides%20are%20essential%20tools%20in%20diagnosing%20and%20staging%20cancer%2C%20but%0Atheir%20analysis%20is%20time-consuming%20and%20costly.%20Foundation%20models%20have%20shown%20great%0Apotential%20to%20assist%20in%20these%20tasks.%20In%20this%20paper%2C%20we%20explore%20how%20existing%0Afoundation%20models%20can%20be%20applied%20to%20cytological%20classification.%20More%0Aparticularly%2C%20we%20focus%20on%20low-rank%20adaptation%2C%20a%20parameter-efficient%0Afine-tuning%20method%20suited%20to%20few-shot%20learning.%20We%20evaluated%20five%20foundation%0Amodels%20across%20four%20cytological%20classification%20datasets.%20Our%20results%20demonstrate%0Athat%20fine-tuning%20the%20pre-trained%20backbones%20with%20LoRA%20significantly%20improves%0Amodel%20performance%20compared%20to%20fine-tuning%20only%20the%20classifier%20head%2C%20achieving%0Astate-of-the-art%20results%20on%20both%20simple%20and%20complex%20classification%20tasks%20while%0Arequiring%20fewer%20data%20samples.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2411.14975v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DExploring%2520Foundation%2520Models%2520Fine-Tuning%2520for%2520Cytology%2520Classification%26entry.906535625%3DManon%2520Dausort%2520and%2520Tiffanie%2520Godelaine%2520and%2520Maxime%2520Zanella%2520and%2520Karim%2520El%2520Khoury%2520and%2520Isabelle%2520Salmon%2520and%2520Beno%25C3%25AEt%2520Macq%26entry.1292438233%3D%2520%2520Cytology%2520slides%2520are%2520essential%2520tools%2520in%2520diagnosing%2520and%2520staging%2520cancer%252C%2520but%250Atheir%2520analysis%2520is%2520time-consuming%2520and%2520costly.%2520Foundation%2520models%2520have%2520shown%2520great%250Apotential%2520to%2520assist%2520in%2520these%2520tasks.%2520In%2520this%2520paper%252C%2520we%2520explore%2520how%2520existing%250Afoundation%2520models%2520can%2520be%2520applied%2520to%2520cytological%2520classification.%2520More%250Aparticularly%252C%2520we%2520focus%2520on%2520low-rank%2520adaptation%252C%2520a%2520parameter-efficient%250Afine-tuning%2520method%2520suited%2520to%2520few-shot%2520learning.%2520We%2520evaluated%2520five%2520foundation%250Amodels%2520across%2520four%2520cytological%2520classification%2520datasets.%2520Our%2520results%2520demonstrate%250Athat%2520fine-tuning%2520the%2520pre-trained%2520backbones%2520with%2520LoRA%2520significantly%2520improves%250Amodel%2520performance%2520compared%2520to%2520fine-tuning%2520only%2520the%2520classifier%2520head%252C%2520achieving%250Astate-of-the-art%2520results%2520on%2520both%2520simple%2520and%2520complex%2520classification%2520tasks%2520while%250Arequiring%2520fewer%2520data%2520samples.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2411.14975v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Exploring%20Foundation%20Models%20Fine-Tuning%20for%20Cytology%20Classification&entry.906535625=Manon%20Dausort%20and%20Tiffanie%20Godelaine%20and%20Maxime%20Zanella%20and%20Karim%20El%20Khoury%20and%20Isabelle%20Salmon%20and%20Beno%C3%AEt%20Macq&entry.1292438233=%20%20Cytology%20slides%20are%20essential%20tools%20in%20diagnosing%20and%20staging%20cancer%2C%20but%0Atheir%20analysis%20is%20time-consuming%20and%20costly.%20Foundation%20models%20have%20shown%20great%0Apotential%20to%20assist%20in%20these%20tasks.%20In%20this%20paper%2C%20we%20explore%20how%20existing%0Afoundation%20models%20can%20be%20applied%20to%20cytological%20classification.%20More%0Aparticularly%2C%20we%20focus%20on%20low-rank%20adaptation%2C%20a%20parameter-efficient%0Afine-tuning%20method%20suited%20to%20few-shot%20learning.%20We%20evaluated%20five%20foundation%0Amodels%20across%20four%20cytological%20classification%20datasets.%20Our%20results%20demonstrate%0Athat%20fine-tuning%20the%20pre-trained%20backbones%20with%20LoRA%20significantly%20improves%0Amodel%20performance%20compared%20to%20fine-tuning%20only%20the%20classifier%20head%2C%20achieving%0Astate-of-the-art%20results%20on%20both%20simple%20and%20complex%20classification%20tasks%20while%0Arequiring%20fewer%20data%20samples.%0A&entry.1838667208=http%3A//arxiv.org/abs/2411.14975v1&entry.124074799=Read"},
{"title": "NeuroGraph: Benchmarks for Graph Machine Learning in Brain Connectomics", "author": "Anwar Said and Roza G. Bayrak and Tyler Derr and Mudassir Shabbir and Daniel Moyer and Catie Chang and Xenofon Koutsoukos", "abstract": "  Machine learning provides a valuable tool for analyzing high-dimensional\nfunctional neuroimaging data, and is proving effective in predicting various\nneurological conditions, psychiatric disorders, and cognitive patterns. In\nfunctional magnetic resonance imaging (MRI) research, interactions between\nbrain regions are commonly modeled using graph-based representations. The\npotency of graph machine learning methods has been established across myriad\ndomains, marking a transformative step in data interpretation and predictive\nmodeling. Yet, despite their promise, the transposition of these techniques to\nthe neuroimaging domain has been challenging due to the expansive number of\npotential preprocessing pipelines and the large parameter search space for\ngraph-based dataset construction. In this paper, we introduce NeuroGraph, a\ncollection of graph-based neuroimaging datasets, and demonstrated its utility\nfor predicting multiple categories of behavioral and cognitive traits. We delve\ndeeply into the dataset generation search space by crafting 35 datasets that\nencompass static and dynamic brain connectivity, running in excess of 15\nbaseline methods for benchmarking. Additionally, we provide generic frameworks\nfor learning on both static and dynamic graphs. Our extensive experiments lead\nto several key observations. Notably, using correlation vectors as node\nfeatures, incorporating larger number of regions of interest, and employing\nsparser graphs lead to improved performance. To foster further advancements in\ngraph-based data driven neuroimaging analysis, we offer a comprehensive\nopen-source Python package that includes the benchmark datasets, baseline\nimplementations, model training, and standard evaluation.\n", "link": "http://arxiv.org/abs/2306.06202v4", "date": "2024-11-22", "relevancy": 2.4489, "topK": [{"title": "Boosted dynamic neural networks", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/26302", "similarity": 0.5068}, {"title": "Self-supervised Photographic Image Layout Representation Learning", "link": "http://arxiv.org/abs/2403.03740v1", "similarity": 0.4993}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.4632}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20NeuroGraph%3A%20Benchmarks%20for%20Graph%20Machine%20Learning%20in%20Brain%20Connectomics&body=Title%3A%20NeuroGraph%3A%20Benchmarks%20for%20Graph%20Machine%20Learning%20in%20Brain%20Connectomics%0AAuthor%3A%20Anwar%20Said%20and%20Roza%20G.%20Bayrak%20and%20Tyler%20Derr%20and%20Mudassir%20Shabbir%20and%20Daniel%20Moyer%20and%20Catie%20Chang%20and%20Xenofon%20Koutsoukos%0AAbstract%3A%20%20%20Machine%20learning%20provides%20a%20valuable%20tool%20for%20analyzing%20high-dimensional%0Afunctional%20neuroimaging%20data%2C%20and%20is%20proving%20effective%20in%20predicting%20various%0Aneurological%20conditions%2C%20psychiatric%20disorders%2C%20and%20cognitive%20patterns.%20In%0Afunctional%20magnetic%20resonance%20imaging%20%28MRI%29%20research%2C%20interactions%20between%0Abrain%20regions%20are%20commonly%20modeled%20using%20graph-based%20representations.%20The%0Apotency%20of%20graph%20machine%20learning%20methods%20has%20been%20established%20across%20myriad%0Adomains%2C%20marking%20a%20transformative%20step%20in%20data%20interpretation%20and%20predictive%0Amodeling.%20Yet%2C%20despite%20their%20promise%2C%20the%20transposition%20of%20these%20techniques%20to%0Athe%20neuroimaging%20domain%20has%20been%20challenging%20due%20to%20the%20expansive%20number%20of%0Apotential%20preprocessing%20pipelines%20and%20the%20large%20parameter%20search%20space%20for%0Agraph-based%20dataset%20construction.%20In%20this%20paper%2C%20we%20introduce%20NeuroGraph%2C%20a%0Acollection%20of%20graph-based%20neuroimaging%20datasets%2C%20and%20demonstrated%20its%20utility%0Afor%20predicting%20multiple%20categories%20of%20behavioral%20and%20cognitive%20traits.%20We%20delve%0Adeeply%20into%20the%20dataset%20generation%20search%20space%20by%20crafting%2035%20datasets%20that%0Aencompass%20static%20and%20dynamic%20brain%20connectivity%2C%20running%20in%20excess%20of%2015%0Abaseline%20methods%20for%20benchmarking.%20Additionally%2C%20we%20provide%20generic%20frameworks%0Afor%20learning%20on%20both%20static%20and%20dynamic%20graphs.%20Our%20extensive%20experiments%20lead%0Ato%20several%20key%20observations.%20Notably%2C%20using%20correlation%20vectors%20as%20node%0Afeatures%2C%20incorporating%20larger%20number%20of%20regions%20of%20interest%2C%20and%20employing%0Asparser%20graphs%20lead%20to%20improved%20performance.%20To%20foster%20further%20advancements%20in%0Agraph-based%20data%20driven%20neuroimaging%20analysis%2C%20we%20offer%20a%20comprehensive%0Aopen-source%20Python%20package%20that%20includes%20the%20benchmark%20datasets%2C%20baseline%0Aimplementations%2C%20model%20training%2C%20and%20standard%20evaluation.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2306.06202v4%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DNeuroGraph%253A%2520Benchmarks%2520for%2520Graph%2520Machine%2520Learning%2520in%2520Brain%2520Connectomics%26entry.906535625%3DAnwar%2520Said%2520and%2520Roza%2520G.%2520Bayrak%2520and%2520Tyler%2520Derr%2520and%2520Mudassir%2520Shabbir%2520and%2520Daniel%2520Moyer%2520and%2520Catie%2520Chang%2520and%2520Xenofon%2520Koutsoukos%26entry.1292438233%3D%2520%2520Machine%2520learning%2520provides%2520a%2520valuable%2520tool%2520for%2520analyzing%2520high-dimensional%250Afunctional%2520neuroimaging%2520data%252C%2520and%2520is%2520proving%2520effective%2520in%2520predicting%2520various%250Aneurological%2520conditions%252C%2520psychiatric%2520disorders%252C%2520and%2520cognitive%2520patterns.%2520In%250Afunctional%2520magnetic%2520resonance%2520imaging%2520%2528MRI%2529%2520research%252C%2520interactions%2520between%250Abrain%2520regions%2520are%2520commonly%2520modeled%2520using%2520graph-based%2520representations.%2520The%250Apotency%2520of%2520graph%2520machine%2520learning%2520methods%2520has%2520been%2520established%2520across%2520myriad%250Adomains%252C%2520marking%2520a%2520transformative%2520step%2520in%2520data%2520interpretation%2520and%2520predictive%250Amodeling.%2520Yet%252C%2520despite%2520their%2520promise%252C%2520the%2520transposition%2520of%2520these%2520techniques%2520to%250Athe%2520neuroimaging%2520domain%2520has%2520been%2520challenging%2520due%2520to%2520the%2520expansive%2520number%2520of%250Apotential%2520preprocessing%2520pipelines%2520and%2520the%2520large%2520parameter%2520search%2520space%2520for%250Agraph-based%2520dataset%2520construction.%2520In%2520this%2520paper%252C%2520we%2520introduce%2520NeuroGraph%252C%2520a%250Acollection%2520of%2520graph-based%2520neuroimaging%2520datasets%252C%2520and%2520demonstrated%2520its%2520utility%250Afor%2520predicting%2520multiple%2520categories%2520of%2520behavioral%2520and%2520cognitive%2520traits.%2520We%2520delve%250Adeeply%2520into%2520the%2520dataset%2520generation%2520search%2520space%2520by%2520crafting%252035%2520datasets%2520that%250Aencompass%2520static%2520and%2520dynamic%2520brain%2520connectivity%252C%2520running%2520in%2520excess%2520of%252015%250Abaseline%2520methods%2520for%2520benchmarking.%2520Additionally%252C%2520we%2520provide%2520generic%2520frameworks%250Afor%2520learning%2520on%2520both%2520static%2520and%2520dynamic%2520graphs.%2520Our%2520extensive%2520experiments%2520lead%250Ato%2520several%2520key%2520observations.%2520Notably%252C%2520using%2520correlation%2520vectors%2520as%2520node%250Afeatures%252C%2520incorporating%2520larger%2520number%2520of%2520regions%2520of%2520interest%252C%2520and%2520employing%250Asparser%2520graphs%2520lead%2520to%2520improved%2520performance.%2520To%2520foster%2520further%2520advancements%2520in%250Agraph-based%2520data%2520driven%2520neuroimaging%2520analysis%252C%2520we%2520offer%2520a%2520comprehensive%250Aopen-source%2520Python%2520package%2520that%2520includes%2520the%2520benchmark%2520datasets%252C%2520baseline%250Aimplementations%252C%2520model%2520training%252C%2520and%2520standard%2520evaluation.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2306.06202v4%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=NeuroGraph%3A%20Benchmarks%20for%20Graph%20Machine%20Learning%20in%20Brain%20Connectomics&entry.906535625=Anwar%20Said%20and%20Roza%20G.%20Bayrak%20and%20Tyler%20Derr%20and%20Mudassir%20Shabbir%20and%20Daniel%20Moyer%20and%20Catie%20Chang%20and%20Xenofon%20Koutsoukos&entry.1292438233=%20%20Machine%20learning%20provides%20a%20valuable%20tool%20for%20analyzing%20high-dimensional%0Afunctional%20neuroimaging%20data%2C%20and%20is%20proving%20effective%20in%20predicting%20various%0Aneurological%20conditions%2C%20psychiatric%20disorders%2C%20and%20cognitive%20patterns.%20In%0Afunctional%20magnetic%20resonance%20imaging%20%28MRI%29%20research%2C%20interactions%20between%0Abrain%20regions%20are%20commonly%20modeled%20using%20graph-based%20representations.%20The%0Apotency%20of%20graph%20machine%20learning%20methods%20has%20been%20established%20across%20myriad%0Adomains%2C%20marking%20a%20transformative%20step%20in%20data%20interpretation%20and%20predictive%0Amodeling.%20Yet%2C%20despite%20their%20promise%2C%20the%20transposition%20of%20these%20techniques%20to%0Athe%20neuroimaging%20domain%20has%20been%20challenging%20due%20to%20the%20expansive%20number%20of%0Apotential%20preprocessing%20pipelines%20and%20the%20large%20parameter%20search%20space%20for%0Agraph-based%20dataset%20construction.%20In%20this%20paper%2C%20we%20introduce%20NeuroGraph%2C%20a%0Acollection%20of%20graph-based%20neuroimaging%20datasets%2C%20and%20demonstrated%20its%20utility%0Afor%20predicting%20multiple%20categories%20of%20behavioral%20and%20cognitive%20traits.%20We%20delve%0Adeeply%20into%20the%20dataset%20generation%20search%20space%20by%20crafting%2035%20datasets%20that%0Aencompass%20static%20and%20dynamic%20brain%20connectivity%2C%20running%20in%20excess%20of%2015%0Abaseline%20methods%20for%20benchmarking.%20Additionally%2C%20we%20provide%20generic%20frameworks%0Afor%20learning%20on%20both%20static%20and%20dynamic%20graphs.%20Our%20extensive%20experiments%20lead%0Ato%20several%20key%20observations.%20Notably%2C%20using%20correlation%20vectors%20as%20node%0Afeatures%2C%20incorporating%20larger%20number%20of%20regions%20of%20interest%2C%20and%20employing%0Asparser%20graphs%20lead%20to%20improved%20performance.%20To%20foster%20further%20advancements%20in%0Agraph-based%20data%20driven%20neuroimaging%20analysis%2C%20we%20offer%20a%20comprehensive%0Aopen-source%20Python%20package%20that%20includes%20the%20benchmark%20datasets%2C%20baseline%0Aimplementations%2C%20model%20training%2C%20and%20standard%20evaluation.%0A&entry.1838667208=http%3A//arxiv.org/abs/2306.06202v4&entry.124074799=Read"},
{"title": "Cell as Point: One-Stage Framework for Efficient Cell Tracking", "author": "Yaxuan Song and Jianan Fan and Heng Huang and Mei Chen and Weidong Cai", "abstract": "  Cellular activities are dynamic and intricate, playing a crucial role in\nadvancing diagnostic and therapeutic techniques, yet they often require\nsubstantial resources for accurate tracking. Despite recent progress, the\nconventional multi-stage cell tracking approaches not only heavily rely on\ndetection or segmentation results as a prerequisite for the tracking stage,\ndemanding plenty of refined segmentation masks, but are also deteriorated by\nimbalanced and long sequence data, leading to under-learning in training and\nmissing cells in inference procedures. To alleviate the above issues, this\npaper proposes the novel end-to-end CAP framework, which leverages the idea of\nregarding Cell as Point to achieve efficient and stable cell tracking in one\nstage. CAP abandons detection or segmentation stages and simplifies the process\nby exploiting the correlation among the trajectories of cell points to track\ncells jointly, thus reducing the label demand and complexity of the pipeline.\nWith cell point trajectory and visibility to represent cell locations and\nlineage relationships, CAP leverages the key innovations of adaptive\nevent-guided (AEG) sampling for addressing data imbalance in cell division\nevents and the rolling-as-window (RAW) inference method to ensure continuous\ntracking of new cells in the long term. Eliminating the need for a prerequisite\ndetection or segmentation stage, CAP demonstrates strong cell tracking\nperformance while also being 10 to 55 times more efficient than existing\nmethods. The code and models will be released.\n", "link": "http://arxiv.org/abs/2411.14833v1", "date": "2024-11-22", "relevancy": 2.446, "topK": [{"title": "DDM-NET: End-to-end learning of keypoint feature Detection, Description and Matching for 3D localization", "link": "https://arxiv.org/abs/2212.04575", "similarity": 0.5119}, {"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.4883}, {"title": "RTG-SLAM: Real-time 3D Reconstruction at Scale using Gaussian Splatting", "link": "http://arxiv.org/abs/2404.19706v1", "similarity": 0.4673}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Cell%20as%20Point%3A%20One-Stage%20Framework%20for%20Efficient%20Cell%20Tracking&body=Title%3A%20Cell%20as%20Point%3A%20One-Stage%20Framework%20for%20Efficient%20Cell%20Tracking%0AAuthor%3A%20Yaxuan%20Song%20and%20Jianan%20Fan%20and%20Heng%20Huang%20and%20Mei%20Chen%20and%20Weidong%20Cai%0AAbstract%3A%20%20%20Cellular%20activities%20are%20dynamic%20and%20intricate%2C%20playing%20a%20crucial%20role%20in%0Aadvancing%20diagnostic%20and%20therapeutic%20techniques%2C%20yet%20they%20often%20require%0Asubstantial%20resources%20for%20accurate%20tracking.%20Despite%20recent%20progress%2C%20the%0Aconventional%20multi-stage%20cell%20tracking%20approaches%20not%20only%20heavily%20rely%20on%0Adetection%20or%20segmentation%20results%20as%20a%20prerequisite%20for%20the%20tracking%20stage%2C%0Ademanding%20plenty%20of%20refined%20segmentation%20masks%2C%20but%20are%20also%20deteriorated%20by%0Aimbalanced%20and%20long%20sequence%20data%2C%20leading%20to%20under-learning%20in%20training%20and%0Amissing%20cells%20in%20inference%20procedures.%20To%20alleviate%20the%20above%20issues%2C%20this%0Apaper%20proposes%20the%20novel%20end-to-end%20CAP%20framework%2C%20which%20leverages%20the%20idea%20of%0Aregarding%20Cell%20as%20Point%20to%20achieve%20efficient%20and%20stable%20cell%20tracking%20in%20one%0Astage.%20CAP%20abandons%20detection%20or%20segmentation%20stages%20and%20simplifies%20the%20process%0Aby%20exploiting%20the%20correlation%20among%20the%20trajectories%20of%20cell%20points%20to%20track%0Acells%20jointly%2C%20thus%20reducing%20the%20label%20demand%20and%20complexity%20of%20the%20pipeline.%0AWith%20cell%20point%20trajectory%20and%20visibility%20to%20represent%20cell%20locations%20and%0Alineage%20relationships%2C%20CAP%20leverages%20the%20key%20innovations%20of%20adaptive%0Aevent-guided%20%28AEG%29%20sampling%20for%20addressing%20data%20imbalance%20in%20cell%20division%0Aevents%20and%20the%20rolling-as-window%20%28RAW%29%20inference%20method%20to%20ensure%20continuous%0Atracking%20of%20new%20cells%20in%20the%20long%20term.%20Eliminating%20the%20need%20for%20a%20prerequisite%0Adetection%20or%20segmentation%20stage%2C%20CAP%20demonstrates%20strong%20cell%20tracking%0Aperformance%20while%20also%20being%2010%20to%2055%20times%20more%20efficient%20than%20existing%0Amethods.%20The%20code%20and%20models%20will%20be%20released.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2411.14833v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DCell%2520as%2520Point%253A%2520One-Stage%2520Framework%2520for%2520Efficient%2520Cell%2520Tracking%26entry.906535625%3DYaxuan%2520Song%2520and%2520Jianan%2520Fan%2520and%2520Heng%2520Huang%2520and%2520Mei%2520Chen%2520and%2520Weidong%2520Cai%26entry.1292438233%3D%2520%2520Cellular%2520activities%2520are%2520dynamic%2520and%2520intricate%252C%2520playing%2520a%2520crucial%2520role%2520in%250Aadvancing%2520diagnostic%2520and%2520therapeutic%2520techniques%252C%2520yet%2520they%2520often%2520require%250Asubstantial%2520resources%2520for%2520accurate%2520tracking.%2520Despite%2520recent%2520progress%252C%2520the%250Aconventional%2520multi-stage%2520cell%2520tracking%2520approaches%2520not%2520only%2520heavily%2520rely%2520on%250Adetection%2520or%2520segmentation%2520results%2520as%2520a%2520prerequisite%2520for%2520the%2520tracking%2520stage%252C%250Ademanding%2520plenty%2520of%2520refined%2520segmentation%2520masks%252C%2520but%2520are%2520also%2520deteriorated%2520by%250Aimbalanced%2520and%2520long%2520sequence%2520data%252C%2520leading%2520to%2520under-learning%2520in%2520training%2520and%250Amissing%2520cells%2520in%2520inference%2520procedures.%2520To%2520alleviate%2520the%2520above%2520issues%252C%2520this%250Apaper%2520proposes%2520the%2520novel%2520end-to-end%2520CAP%2520framework%252C%2520which%2520leverages%2520the%2520idea%2520of%250Aregarding%2520Cell%2520as%2520Point%2520to%2520achieve%2520efficient%2520and%2520stable%2520cell%2520tracking%2520in%2520one%250Astage.%2520CAP%2520abandons%2520detection%2520or%2520segmentation%2520stages%2520and%2520simplifies%2520the%2520process%250Aby%2520exploiting%2520the%2520correlation%2520among%2520the%2520trajectories%2520of%2520cell%2520points%2520to%2520track%250Acells%2520jointly%252C%2520thus%2520reducing%2520the%2520label%2520demand%2520and%2520complexity%2520of%2520the%2520pipeline.%250AWith%2520cell%2520point%2520trajectory%2520and%2520visibility%2520to%2520represent%2520cell%2520locations%2520and%250Alineage%2520relationships%252C%2520CAP%2520leverages%2520the%2520key%2520innovations%2520of%2520adaptive%250Aevent-guided%2520%2528AEG%2529%2520sampling%2520for%2520addressing%2520data%2520imbalance%2520in%2520cell%2520division%250Aevents%2520and%2520the%2520rolling-as-window%2520%2528RAW%2529%2520inference%2520method%2520to%2520ensure%2520continuous%250Atracking%2520of%2520new%2520cells%2520in%2520the%2520long%2520term.%2520Eliminating%2520the%2520need%2520for%2520a%2520prerequisite%250Adetection%2520or%2520segmentation%2520stage%252C%2520CAP%2520demonstrates%2520strong%2520cell%2520tracking%250Aperformance%2520while%2520also%2520being%252010%2520to%252055%2520times%2520more%2520efficient%2520than%2520existing%250Amethods.%2520The%2520code%2520and%2520models%2520will%2520be%2520released.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2411.14833v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Cell%20as%20Point%3A%20One-Stage%20Framework%20for%20Efficient%20Cell%20Tracking&entry.906535625=Yaxuan%20Song%20and%20Jianan%20Fan%20and%20Heng%20Huang%20and%20Mei%20Chen%20and%20Weidong%20Cai&entry.1292438233=%20%20Cellular%20activities%20are%20dynamic%20and%20intricate%2C%20playing%20a%20crucial%20role%20in%0Aadvancing%20diagnostic%20and%20therapeutic%20techniques%2C%20yet%20they%20often%20require%0Asubstantial%20resources%20for%20accurate%20tracking.%20Despite%20recent%20progress%2C%20the%0Aconventional%20multi-stage%20cell%20tracking%20approaches%20not%20only%20heavily%20rely%20on%0Adetection%20or%20segmentation%20results%20as%20a%20prerequisite%20for%20the%20tracking%20stage%2C%0Ademanding%20plenty%20of%20refined%20segmentation%20masks%2C%20but%20are%20also%20deteriorated%20by%0Aimbalanced%20and%20long%20sequence%20data%2C%20leading%20to%20under-learning%20in%20training%20and%0Amissing%20cells%20in%20inference%20procedures.%20To%20alleviate%20the%20above%20issues%2C%20this%0Apaper%20proposes%20the%20novel%20end-to-end%20CAP%20framework%2C%20which%20leverages%20the%20idea%20of%0Aregarding%20Cell%20as%20Point%20to%20achieve%20efficient%20and%20stable%20cell%20tracking%20in%20one%0Astage.%20CAP%20abandons%20detection%20or%20segmentation%20stages%20and%20simplifies%20the%20process%0Aby%20exploiting%20the%20correlation%20among%20the%20trajectories%20of%20cell%20points%20to%20track%0Acells%20jointly%2C%20thus%20reducing%20the%20label%20demand%20and%20complexity%20of%20the%20pipeline.%0AWith%20cell%20point%20trajectory%20and%20visibility%20to%20represent%20cell%20locations%20and%0Alineage%20relationships%2C%20CAP%20leverages%20the%20key%20innovations%20of%20adaptive%0Aevent-guided%20%28AEG%29%20sampling%20for%20addressing%20data%20imbalance%20in%20cell%20division%0Aevents%20and%20the%20rolling-as-window%20%28RAW%29%20inference%20method%20to%20ensure%20continuous%0Atracking%20of%20new%20cells%20in%20the%20long%20term.%20Eliminating%20the%20need%20for%20a%20prerequisite%0Adetection%20or%20segmentation%20stage%2C%20CAP%20demonstrates%20strong%20cell%20tracking%0Aperformance%20while%20also%20being%2010%20to%2055%20times%20more%20efficient%20than%20existing%0Amethods.%20The%20code%20and%20models%20will%20be%20released.%0A&entry.1838667208=http%3A//arxiv.org/abs/2411.14833v1&entry.124074799=Read"},
{"title": "FedCRL: Personalized Federated Learning with Contrastive Shared\n  Representations for Label Heterogeneity in Non-IID Data", "author": "Chenghao Huang and Xiaolu Chen and Yanru Zhang and Hao Wang", "abstract": "  Heterogeneity resulting from label distribution skew and data scarcity can\nlead to inaccuracy and unfairness in intelligent communication applications\nthat mainly rely on distributed computing. To deal with it, this paper proposes\na novel personalized federated learning algorithm, named Federated Contrastive\nShareable Representations (FedCoSR), to facilitate knowledge sharing among\nclients while maintaining data privacy. Specifically, parameters of local\nmodels' shallow layers and typical local representations are both considered\nshareable information for the server and aggregated globally. To address poor\nperformance caused by label distribution skew among clients, contrastive\nlearning is adopted between local and global representations to enrich local\nknowledge. Additionally, to ensure fairness for clients with scarce data,\nFedCoSR introduces adaptive local aggregation to coordinate the global model\ninvolvement in each client. Our simulations demonstrate FedCoSR's effectiveness\nin mitigating label heterogeneity by achieving accuracy and fairness\nimprovements over existing methods on datasets with varying degrees of label\nheterogeneity.\n", "link": "http://arxiv.org/abs/2404.17916v2", "date": "2024-11-22", "relevancy": 2.4284, "topK": [{"title": "Self-supervised Photographic Image Layout Representation Learning", "link": "http://arxiv.org/abs/2403.03740v1", "similarity": 0.4912}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.484}, {"title": "Flexible Visual Recognition by Evidential Modeling of Confusion and Ignorance", "link": "http://openaccess.thecvf.com/content/ICCV2023/html/Fan_Flexible_Visual_Recognition_by_Evidential_Modeling_of_Confusion_and_Ignorance_ICCV_2023_paper.html", "similarity": 0.4818}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20FedCRL%3A%20Personalized%20Federated%20Learning%20with%20Contrastive%20Shared%0A%20%20Representations%20for%20Label%20Heterogeneity%20in%20Non-IID%20Data&body=Title%3A%20FedCRL%3A%20Personalized%20Federated%20Learning%20with%20Contrastive%20Shared%0A%20%20Representations%20for%20Label%20Heterogeneity%20in%20Non-IID%20Data%0AAuthor%3A%20Chenghao%20Huang%20and%20Xiaolu%20Chen%20and%20Yanru%20Zhang%20and%20Hao%20Wang%0AAbstract%3A%20%20%20Heterogeneity%20resulting%20from%20label%20distribution%20skew%20and%20data%20scarcity%20can%0Alead%20to%20inaccuracy%20and%20unfairness%20in%20intelligent%20communication%20applications%0Athat%20mainly%20rely%20on%20distributed%20computing.%20To%20deal%20with%20it%2C%20this%20paper%20proposes%0Aa%20novel%20personalized%20federated%20learning%20algorithm%2C%20named%20Federated%20Contrastive%0AShareable%20Representations%20%28FedCoSR%29%2C%20to%20facilitate%20knowledge%20sharing%20among%0Aclients%20while%20maintaining%20data%20privacy.%20Specifically%2C%20parameters%20of%20local%0Amodels%27%20shallow%20layers%20and%20typical%20local%20representations%20are%20both%20considered%0Ashareable%20information%20for%20the%20server%20and%20aggregated%20globally.%20To%20address%20poor%0Aperformance%20caused%20by%20label%20distribution%20skew%20among%20clients%2C%20contrastive%0Alearning%20is%20adopted%20between%20local%20and%20global%20representations%20to%20enrich%20local%0Aknowledge.%20Additionally%2C%20to%20ensure%20fairness%20for%20clients%20with%20scarce%20data%2C%0AFedCoSR%20introduces%20adaptive%20local%20aggregation%20to%20coordinate%20the%20global%20model%0Ainvolvement%20in%20each%20client.%20Our%20simulations%20demonstrate%20FedCoSR%27s%20effectiveness%0Ain%20mitigating%20label%20heterogeneity%20by%20achieving%20accuracy%20and%20fairness%0Aimprovements%20over%20existing%20methods%20on%20datasets%20with%20varying%20degrees%20of%20label%0Aheterogeneity.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2404.17916v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DFedCRL%253A%2520Personalized%2520Federated%2520Learning%2520with%2520Contrastive%2520Shared%250A%2520%2520Representations%2520for%2520Label%2520Heterogeneity%2520in%2520Non-IID%2520Data%26entry.906535625%3DChenghao%2520Huang%2520and%2520Xiaolu%2520Chen%2520and%2520Yanru%2520Zhang%2520and%2520Hao%2520Wang%26entry.1292438233%3D%2520%2520Heterogeneity%2520resulting%2520from%2520label%2520distribution%2520skew%2520and%2520data%2520scarcity%2520can%250Alead%2520to%2520inaccuracy%2520and%2520unfairness%2520in%2520intelligent%2520communication%2520applications%250Athat%2520mainly%2520rely%2520on%2520distributed%2520computing.%2520To%2520deal%2520with%2520it%252C%2520this%2520paper%2520proposes%250Aa%2520novel%2520personalized%2520federated%2520learning%2520algorithm%252C%2520named%2520Federated%2520Contrastive%250AShareable%2520Representations%2520%2528FedCoSR%2529%252C%2520to%2520facilitate%2520knowledge%2520sharing%2520among%250Aclients%2520while%2520maintaining%2520data%2520privacy.%2520Specifically%252C%2520parameters%2520of%2520local%250Amodels%2527%2520shallow%2520layers%2520and%2520typical%2520local%2520representations%2520are%2520both%2520considered%250Ashareable%2520information%2520for%2520the%2520server%2520and%2520aggregated%2520globally.%2520To%2520address%2520poor%250Aperformance%2520caused%2520by%2520label%2520distribution%2520skew%2520among%2520clients%252C%2520contrastive%250Alearning%2520is%2520adopted%2520between%2520local%2520and%2520global%2520representations%2520to%2520enrich%2520local%250Aknowledge.%2520Additionally%252C%2520to%2520ensure%2520fairness%2520for%2520clients%2520with%2520scarce%2520data%252C%250AFedCoSR%2520introduces%2520adaptive%2520local%2520aggregation%2520to%2520coordinate%2520the%2520global%2520model%250Ainvolvement%2520in%2520each%2520client.%2520Our%2520simulations%2520demonstrate%2520FedCoSR%2527s%2520effectiveness%250Ain%2520mitigating%2520label%2520heterogeneity%2520by%2520achieving%2520accuracy%2520and%2520fairness%250Aimprovements%2520over%2520existing%2520methods%2520on%2520datasets%2520with%2520varying%2520degrees%2520of%2520label%250Aheterogeneity.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2404.17916v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=FedCRL%3A%20Personalized%20Federated%20Learning%20with%20Contrastive%20Shared%0A%20%20Representations%20for%20Label%20Heterogeneity%20in%20Non-IID%20Data&entry.906535625=Chenghao%20Huang%20and%20Xiaolu%20Chen%20and%20Yanru%20Zhang%20and%20Hao%20Wang&entry.1292438233=%20%20Heterogeneity%20resulting%20from%20label%20distribution%20skew%20and%20data%20scarcity%20can%0Alead%20to%20inaccuracy%20and%20unfairness%20in%20intelligent%20communication%20applications%0Athat%20mainly%20rely%20on%20distributed%20computing.%20To%20deal%20with%20it%2C%20this%20paper%20proposes%0Aa%20novel%20personalized%20federated%20learning%20algorithm%2C%20named%20Federated%20Contrastive%0AShareable%20Representations%20%28FedCoSR%29%2C%20to%20facilitate%20knowledge%20sharing%20among%0Aclients%20while%20maintaining%20data%20privacy.%20Specifically%2C%20parameters%20of%20local%0Amodels%27%20shallow%20layers%20and%20typical%20local%20representations%20are%20both%20considered%0Ashareable%20information%20for%20the%20server%20and%20aggregated%20globally.%20To%20address%20poor%0Aperformance%20caused%20by%20label%20distribution%20skew%20among%20clients%2C%20contrastive%0Alearning%20is%20adopted%20between%20local%20and%20global%20representations%20to%20enrich%20local%0Aknowledge.%20Additionally%2C%20to%20ensure%20fairness%20for%20clients%20with%20scarce%20data%2C%0AFedCoSR%20introduces%20adaptive%20local%20aggregation%20to%20coordinate%20the%20global%20model%0Ainvolvement%20in%20each%20client.%20Our%20simulations%20demonstrate%20FedCoSR%27s%20effectiveness%0Ain%20mitigating%20label%20heterogeneity%20by%20achieving%20accuracy%20and%20fairness%0Aimprovements%20over%20existing%20methods%20on%20datasets%20with%20varying%20degrees%20of%20label%0Aheterogeneity.%0A&entry.1838667208=http%3A//arxiv.org/abs/2404.17916v2&entry.124074799=Read"},
{"title": "Maps from Motion (MfM): Generating 2D Semantic Maps from Sparse\n  Multi-view Images", "author": "Matteo Toso and Stefano Fiorini and Stuart James and Alessio Del Bue", "abstract": "  World-wide detailed 2D maps require enormous collective efforts.\nOpenStreetMap is the result of 11 million registered users manually annotating\nthe GPS location of over 1.75 billion entries, including distinctive landmarks\nand common urban objects. At the same time, manual annotations can include\nerrors and are slow to update, limiting the map's accuracy. Maps from Motion\n(MfM) is a step forward to automatize such time-consuming map making procedure\nby computing 2D maps of semantic objects directly from a collection of\nuncalibrated multi-view images. From each image, we extract a set of object\ndetections, and estimate their spatial arrangement in a top-down local map\ncentered in the reference frame of the camera that captured the image. Aligning\nthese local maps is not a trivial problem, since they provide incomplete, noisy\nfragments of the scene, and matching detections across them is unreliable\nbecause of the presence of repeated pattern and the limited appearance\nvariability of urban objects. We address this with a novel graph-based\nframework, that encodes the spatial and semantic distribution of the objects\ndetected in each image, and learns how to combine them to predict the objects'\nposes in a global reference system, while taking into account all possible\ndetection matches and preserving the topology observed in each image. Despite\nthe complexity of the problem, our best model achieves global 2D registration\nwith an average accuracy within 4 meters (i.e., below GPS accuracy) even on\nsparse sequences with strong viewpoint change, on which COLMAP has an 80%\nfailure rate. We provide extensive evaluation on synthetic and real-world data,\nshowing how the method obtains a solution even in scenarios where standard\noptimization techniques fail.\n", "link": "http://arxiv.org/abs/2411.12620v2", "date": "2024-11-22", "relevancy": 2.4218, "topK": [{"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.6346}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.5907}, {"title": "DDM-NET: End-to-end learning of keypoint feature Detection, Description and Matching for 3D localization", "link": "https://arxiv.org/abs/2212.04575", "similarity": 0.5822}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Maps%20from%20Motion%20%28MfM%29%3A%20Generating%202D%20Semantic%20Maps%20from%20Sparse%0A%20%20Multi-view%20Images&body=Title%3A%20Maps%20from%20Motion%20%28MfM%29%3A%20Generating%202D%20Semantic%20Maps%20from%20Sparse%0A%20%20Multi-view%20Images%0AAuthor%3A%20Matteo%20Toso%20and%20Stefano%20Fiorini%20and%20Stuart%20James%20and%20Alessio%20Del%20Bue%0AAbstract%3A%20%20%20World-wide%20detailed%202D%20maps%20require%20enormous%20collective%20efforts.%0AOpenStreetMap%20is%20the%20result%20of%2011%20million%20registered%20users%20manually%20annotating%0Athe%20GPS%20location%20of%20over%201.75%20billion%20entries%2C%20including%20distinctive%20landmarks%0Aand%20common%20urban%20objects.%20At%20the%20same%20time%2C%20manual%20annotations%20can%20include%0Aerrors%20and%20are%20slow%20to%20update%2C%20limiting%20the%20map%27s%20accuracy.%20Maps%20from%20Motion%0A%28MfM%29%20is%20a%20step%20forward%20to%20automatize%20such%20time-consuming%20map%20making%20procedure%0Aby%20computing%202D%20maps%20of%20semantic%20objects%20directly%20from%20a%20collection%20of%0Auncalibrated%20multi-view%20images.%20From%20each%20image%2C%20we%20extract%20a%20set%20of%20object%0Adetections%2C%20and%20estimate%20their%20spatial%20arrangement%20in%20a%20top-down%20local%20map%0Acentered%20in%20the%20reference%20frame%20of%20the%20camera%20that%20captured%20the%20image.%20Aligning%0Athese%20local%20maps%20is%20not%20a%20trivial%20problem%2C%20since%20they%20provide%20incomplete%2C%20noisy%0Afragments%20of%20the%20scene%2C%20and%20matching%20detections%20across%20them%20is%20unreliable%0Abecause%20of%20the%20presence%20of%20repeated%20pattern%20and%20the%20limited%20appearance%0Avariability%20of%20urban%20objects.%20We%20address%20this%20with%20a%20novel%20graph-based%0Aframework%2C%20that%20encodes%20the%20spatial%20and%20semantic%20distribution%20of%20the%20objects%0Adetected%20in%20each%20image%2C%20and%20learns%20how%20to%20combine%20them%20to%20predict%20the%20objects%27%0Aposes%20in%20a%20global%20reference%20system%2C%20while%20taking%20into%20account%20all%20possible%0Adetection%20matches%20and%20preserving%20the%20topology%20observed%20in%20each%20image.%20Despite%0Athe%20complexity%20of%20the%20problem%2C%20our%20best%20model%20achieves%20global%202D%20registration%0Awith%20an%20average%20accuracy%20within%204%20meters%20%28i.e.%2C%20below%20GPS%20accuracy%29%20even%20on%0Asparse%20sequences%20with%20strong%20viewpoint%20change%2C%20on%20which%20COLMAP%20has%20an%2080%25%0Afailure%20rate.%20We%20provide%20extensive%20evaluation%20on%20synthetic%20and%20real-world%20data%2C%0Ashowing%20how%20the%20method%20obtains%20a%20solution%20even%20in%20scenarios%20where%20standard%0Aoptimization%20techniques%20fail.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2411.12620v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DMaps%2520from%2520Motion%2520%2528MfM%2529%253A%2520Generating%25202D%2520Semantic%2520Maps%2520from%2520Sparse%250A%2520%2520Multi-view%2520Images%26entry.906535625%3DMatteo%2520Toso%2520and%2520Stefano%2520Fiorini%2520and%2520Stuart%2520James%2520and%2520Alessio%2520Del%2520Bue%26entry.1292438233%3D%2520%2520World-wide%2520detailed%25202D%2520maps%2520require%2520enormous%2520collective%2520efforts.%250AOpenStreetMap%2520is%2520the%2520result%2520of%252011%2520million%2520registered%2520users%2520manually%2520annotating%250Athe%2520GPS%2520location%2520of%2520over%25201.75%2520billion%2520entries%252C%2520including%2520distinctive%2520landmarks%250Aand%2520common%2520urban%2520objects.%2520At%2520the%2520same%2520time%252C%2520manual%2520annotations%2520can%2520include%250Aerrors%2520and%2520are%2520slow%2520to%2520update%252C%2520limiting%2520the%2520map%2527s%2520accuracy.%2520Maps%2520from%2520Motion%250A%2528MfM%2529%2520is%2520a%2520step%2520forward%2520to%2520automatize%2520such%2520time-consuming%2520map%2520making%2520procedure%250Aby%2520computing%25202D%2520maps%2520of%2520semantic%2520objects%2520directly%2520from%2520a%2520collection%2520of%250Auncalibrated%2520multi-view%2520images.%2520From%2520each%2520image%252C%2520we%2520extract%2520a%2520set%2520of%2520object%250Adetections%252C%2520and%2520estimate%2520their%2520spatial%2520arrangement%2520in%2520a%2520top-down%2520local%2520map%250Acentered%2520in%2520the%2520reference%2520frame%2520of%2520the%2520camera%2520that%2520captured%2520the%2520image.%2520Aligning%250Athese%2520local%2520maps%2520is%2520not%2520a%2520trivial%2520problem%252C%2520since%2520they%2520provide%2520incomplete%252C%2520noisy%250Afragments%2520of%2520the%2520scene%252C%2520and%2520matching%2520detections%2520across%2520them%2520is%2520unreliable%250Abecause%2520of%2520the%2520presence%2520of%2520repeated%2520pattern%2520and%2520the%2520limited%2520appearance%250Avariability%2520of%2520urban%2520objects.%2520We%2520address%2520this%2520with%2520a%2520novel%2520graph-based%250Aframework%252C%2520that%2520encodes%2520the%2520spatial%2520and%2520semantic%2520distribution%2520of%2520the%2520objects%250Adetected%2520in%2520each%2520image%252C%2520and%2520learns%2520how%2520to%2520combine%2520them%2520to%2520predict%2520the%2520objects%2527%250Aposes%2520in%2520a%2520global%2520reference%2520system%252C%2520while%2520taking%2520into%2520account%2520all%2520possible%250Adetection%2520matches%2520and%2520preserving%2520the%2520topology%2520observed%2520in%2520each%2520image.%2520Despite%250Athe%2520complexity%2520of%2520the%2520problem%252C%2520our%2520best%2520model%2520achieves%2520global%25202D%2520registration%250Awith%2520an%2520average%2520accuracy%2520within%25204%2520meters%2520%2528i.e.%252C%2520below%2520GPS%2520accuracy%2529%2520even%2520on%250Asparse%2520sequences%2520with%2520strong%2520viewpoint%2520change%252C%2520on%2520which%2520COLMAP%2520has%2520an%252080%2525%250Afailure%2520rate.%2520We%2520provide%2520extensive%2520evaluation%2520on%2520synthetic%2520and%2520real-world%2520data%252C%250Ashowing%2520how%2520the%2520method%2520obtains%2520a%2520solution%2520even%2520in%2520scenarios%2520where%2520standard%250Aoptimization%2520techniques%2520fail.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2411.12620v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Maps%20from%20Motion%20%28MfM%29%3A%20Generating%202D%20Semantic%20Maps%20from%20Sparse%0A%20%20Multi-view%20Images&entry.906535625=Matteo%20Toso%20and%20Stefano%20Fiorini%20and%20Stuart%20James%20and%20Alessio%20Del%20Bue&entry.1292438233=%20%20World-wide%20detailed%202D%20maps%20require%20enormous%20collective%20efforts.%0AOpenStreetMap%20is%20the%20result%20of%2011%20million%20registered%20users%20manually%20annotating%0Athe%20GPS%20location%20of%20over%201.75%20billion%20entries%2C%20including%20distinctive%20landmarks%0Aand%20common%20urban%20objects.%20At%20the%20same%20time%2C%20manual%20annotations%20can%20include%0Aerrors%20and%20are%20slow%20to%20update%2C%20limiting%20the%20map%27s%20accuracy.%20Maps%20from%20Motion%0A%28MfM%29%20is%20a%20step%20forward%20to%20automatize%20such%20time-consuming%20map%20making%20procedure%0Aby%20computing%202D%20maps%20of%20semantic%20objects%20directly%20from%20a%20collection%20of%0Auncalibrated%20multi-view%20images.%20From%20each%20image%2C%20we%20extract%20a%20set%20of%20object%0Adetections%2C%20and%20estimate%20their%20spatial%20arrangement%20in%20a%20top-down%20local%20map%0Acentered%20in%20the%20reference%20frame%20of%20the%20camera%20that%20captured%20the%20image.%20Aligning%0Athese%20local%20maps%20is%20not%20a%20trivial%20problem%2C%20since%20they%20provide%20incomplete%2C%20noisy%0Afragments%20of%20the%20scene%2C%20and%20matching%20detections%20across%20them%20is%20unreliable%0Abecause%20of%20the%20presence%20of%20repeated%20pattern%20and%20the%20limited%20appearance%0Avariability%20of%20urban%20objects.%20We%20address%20this%20with%20a%20novel%20graph-based%0Aframework%2C%20that%20encodes%20the%20spatial%20and%20semantic%20distribution%20of%20the%20objects%0Adetected%20in%20each%20image%2C%20and%20learns%20how%20to%20combine%20them%20to%20predict%20the%20objects%27%0Aposes%20in%20a%20global%20reference%20system%2C%20while%20taking%20into%20account%20all%20possible%0Adetection%20matches%20and%20preserving%20the%20topology%20observed%20in%20each%20image.%20Despite%0Athe%20complexity%20of%20the%20problem%2C%20our%20best%20model%20achieves%20global%202D%20registration%0Awith%20an%20average%20accuracy%20within%204%20meters%20%28i.e.%2C%20below%20GPS%20accuracy%29%20even%20on%0Asparse%20sequences%20with%20strong%20viewpoint%20change%2C%20on%20which%20COLMAP%20has%20an%2080%25%0Afailure%20rate.%20We%20provide%20extensive%20evaluation%20on%20synthetic%20and%20real-world%20data%2C%0Ashowing%20how%20the%20method%20obtains%20a%20solution%20even%20in%20scenarios%20where%20standard%0Aoptimization%20techniques%20fail.%0A&entry.1838667208=http%3A//arxiv.org/abs/2411.12620v2&entry.124074799=Read"},
{"title": "Geminio: Language-Guided Gradient Inversion Attacks in Federated\n  Learning", "author": "Junjie Shan and Ziqi Zhao and Jialin Lu and Rui Zhang and Siu Ming Yiu and Ka-Ho Chow", "abstract": "  Foundation models that bridge vision and language have made significant\nprogress, inspiring numerous life-enriching applications. However, their\npotential for misuse to introduce new threats remains largely unexplored. This\npaper reveals that vision-language models (VLMs) can be exploited to overcome\nlongstanding limitations in gradient inversion attacks (GIAs) within federated\nlearning (FL), where an FL server reconstructs private data samples from\ngradients shared by victim clients. Current GIAs face challenges in\nreconstructing high-resolution images, especially when the victim has a large\nlocal data batch. While focusing reconstruction on valuable samples rather than\nthe entire batch is promising, existing methods lack the flexibility to allow\nattackers to specify their target data. In this paper, we introduce Geminio,\nthe first approach to transform GIAs into semantically meaningful, targeted\nattacks. Geminio enables a brand new privacy attack experience: attackers can\ndescribe, in natural language, the types of data they consider valuable, and\nGeminio will prioritize reconstruction to focus on those high-value samples.\nThis is achieved by leveraging a pretrained VLM to guide the optimization of a\nmalicious global model that, when shared with and optimized by a victim,\nretains only gradients of samples that match the attacker-specified query.\nExtensive experiments demonstrate Geminio's effectiveness in pinpointing and\nreconstructing targeted samples, with high success rates across complex\ndatasets under FL and large batch sizes and showing resilience against existing\ndefenses.\n", "link": "http://arxiv.org/abs/2411.14937v1", "date": "2024-11-22", "relevancy": 2.4205, "topK": [{"title": "Dynamic Gaussians Mesh: Consistent Mesh Reconstruction from Monocular\n  Videos", "link": "http://arxiv.org/abs/2404.12379v2", "similarity": 0.4963}, {"title": "MiraGe: Editable 2D Images using Gaussian Splatting", "link": "http://arxiv.org/abs/2410.01521v1", "similarity": 0.4845}, {"title": "UGG: Unified Generative Grasping", "link": "https://arxiv.org/abs/2311.16917", "similarity": 0.4715}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Geminio%3A%20Language-Guided%20Gradient%20Inversion%20Attacks%20in%20Federated%0A%20%20Learning&body=Title%3A%20Geminio%3A%20Language-Guided%20Gradient%20Inversion%20Attacks%20in%20Federated%0A%20%20Learning%0AAuthor%3A%20Junjie%20Shan%20and%20Ziqi%20Zhao%20and%20Jialin%20Lu%20and%20Rui%20Zhang%20and%20Siu%20Ming%20Yiu%20and%20Ka-Ho%20Chow%0AAbstract%3A%20%20%20Foundation%20models%20that%20bridge%20vision%20and%20language%20have%20made%20significant%0Aprogress%2C%20inspiring%20numerous%20life-enriching%20applications.%20However%2C%20their%0Apotential%20for%20misuse%20to%20introduce%20new%20threats%20remains%20largely%20unexplored.%20This%0Apaper%20reveals%20that%20vision-language%20models%20%28VLMs%29%20can%20be%20exploited%20to%20overcome%0Alongstanding%20limitations%20in%20gradient%20inversion%20attacks%20%28GIAs%29%20within%20federated%0Alearning%20%28FL%29%2C%20where%20an%20FL%20server%20reconstructs%20private%20data%20samples%20from%0Agradients%20shared%20by%20victim%20clients.%20Current%20GIAs%20face%20challenges%20in%0Areconstructing%20high-resolution%20images%2C%20especially%20when%20the%20victim%20has%20a%20large%0Alocal%20data%20batch.%20While%20focusing%20reconstruction%20on%20valuable%20samples%20rather%20than%0Athe%20entire%20batch%20is%20promising%2C%20existing%20methods%20lack%20the%20flexibility%20to%20allow%0Aattackers%20to%20specify%20their%20target%20data.%20In%20this%20paper%2C%20we%20introduce%20Geminio%2C%0Athe%20first%20approach%20to%20transform%20GIAs%20into%20semantically%20meaningful%2C%20targeted%0Aattacks.%20Geminio%20enables%20a%20brand%20new%20privacy%20attack%20experience%3A%20attackers%20can%0Adescribe%2C%20in%20natural%20language%2C%20the%20types%20of%20data%20they%20consider%20valuable%2C%20and%0AGeminio%20will%20prioritize%20reconstruction%20to%20focus%20on%20those%20high-value%20samples.%0AThis%20is%20achieved%20by%20leveraging%20a%20pretrained%20VLM%20to%20guide%20the%20optimization%20of%20a%0Amalicious%20global%20model%20that%2C%20when%20shared%20with%20and%20optimized%20by%20a%20victim%2C%0Aretains%20only%20gradients%20of%20samples%20that%20match%20the%20attacker-specified%20query.%0AExtensive%20experiments%20demonstrate%20Geminio%27s%20effectiveness%20in%20pinpointing%20and%0Areconstructing%20targeted%20samples%2C%20with%20high%20success%20rates%20across%20complex%0Adatasets%20under%20FL%20and%20large%20batch%20sizes%20and%20showing%20resilience%20against%20existing%0Adefenses.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2411.14937v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DGeminio%253A%2520Language-Guided%2520Gradient%2520Inversion%2520Attacks%2520in%2520Federated%250A%2520%2520Learning%26entry.906535625%3DJunjie%2520Shan%2520and%2520Ziqi%2520Zhao%2520and%2520Jialin%2520Lu%2520and%2520Rui%2520Zhang%2520and%2520Siu%2520Ming%2520Yiu%2520and%2520Ka-Ho%2520Chow%26entry.1292438233%3D%2520%2520Foundation%2520models%2520that%2520bridge%2520vision%2520and%2520language%2520have%2520made%2520significant%250Aprogress%252C%2520inspiring%2520numerous%2520life-enriching%2520applications.%2520However%252C%2520their%250Apotential%2520for%2520misuse%2520to%2520introduce%2520new%2520threats%2520remains%2520largely%2520unexplored.%2520This%250Apaper%2520reveals%2520that%2520vision-language%2520models%2520%2528VLMs%2529%2520can%2520be%2520exploited%2520to%2520overcome%250Alongstanding%2520limitations%2520in%2520gradient%2520inversion%2520attacks%2520%2528GIAs%2529%2520within%2520federated%250Alearning%2520%2528FL%2529%252C%2520where%2520an%2520FL%2520server%2520reconstructs%2520private%2520data%2520samples%2520from%250Agradients%2520shared%2520by%2520victim%2520clients.%2520Current%2520GIAs%2520face%2520challenges%2520in%250Areconstructing%2520high-resolution%2520images%252C%2520especially%2520when%2520the%2520victim%2520has%2520a%2520large%250Alocal%2520data%2520batch.%2520While%2520focusing%2520reconstruction%2520on%2520valuable%2520samples%2520rather%2520than%250Athe%2520entire%2520batch%2520is%2520promising%252C%2520existing%2520methods%2520lack%2520the%2520flexibility%2520to%2520allow%250Aattackers%2520to%2520specify%2520their%2520target%2520data.%2520In%2520this%2520paper%252C%2520we%2520introduce%2520Geminio%252C%250Athe%2520first%2520approach%2520to%2520transform%2520GIAs%2520into%2520semantically%2520meaningful%252C%2520targeted%250Aattacks.%2520Geminio%2520enables%2520a%2520brand%2520new%2520privacy%2520attack%2520experience%253A%2520attackers%2520can%250Adescribe%252C%2520in%2520natural%2520language%252C%2520the%2520types%2520of%2520data%2520they%2520consider%2520valuable%252C%2520and%250AGeminio%2520will%2520prioritize%2520reconstruction%2520to%2520focus%2520on%2520those%2520high-value%2520samples.%250AThis%2520is%2520achieved%2520by%2520leveraging%2520a%2520pretrained%2520VLM%2520to%2520guide%2520the%2520optimization%2520of%2520a%250Amalicious%2520global%2520model%2520that%252C%2520when%2520shared%2520with%2520and%2520optimized%2520by%2520a%2520victim%252C%250Aretains%2520only%2520gradients%2520of%2520samples%2520that%2520match%2520the%2520attacker-specified%2520query.%250AExtensive%2520experiments%2520demonstrate%2520Geminio%2527s%2520effectiveness%2520in%2520pinpointing%2520and%250Areconstructing%2520targeted%2520samples%252C%2520with%2520high%2520success%2520rates%2520across%2520complex%250Adatasets%2520under%2520FL%2520and%2520large%2520batch%2520sizes%2520and%2520showing%2520resilience%2520against%2520existing%250Adefenses.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2411.14937v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Geminio%3A%20Language-Guided%20Gradient%20Inversion%20Attacks%20in%20Federated%0A%20%20Learning&entry.906535625=Junjie%20Shan%20and%20Ziqi%20Zhao%20and%20Jialin%20Lu%20and%20Rui%20Zhang%20and%20Siu%20Ming%20Yiu%20and%20Ka-Ho%20Chow&entry.1292438233=%20%20Foundation%20models%20that%20bridge%20vision%20and%20language%20have%20made%20significant%0Aprogress%2C%20inspiring%20numerous%20life-enriching%20applications.%20However%2C%20their%0Apotential%20for%20misuse%20to%20introduce%20new%20threats%20remains%20largely%20unexplored.%20This%0Apaper%20reveals%20that%20vision-language%20models%20%28VLMs%29%20can%20be%20exploited%20to%20overcome%0Alongstanding%20limitations%20in%20gradient%20inversion%20attacks%20%28GIAs%29%20within%20federated%0Alearning%20%28FL%29%2C%20where%20an%20FL%20server%20reconstructs%20private%20data%20samples%20from%0Agradients%20shared%20by%20victim%20clients.%20Current%20GIAs%20face%20challenges%20in%0Areconstructing%20high-resolution%20images%2C%20especially%20when%20the%20victim%20has%20a%20large%0Alocal%20data%20batch.%20While%20focusing%20reconstruction%20on%20valuable%20samples%20rather%20than%0Athe%20entire%20batch%20is%20promising%2C%20existing%20methods%20lack%20the%20flexibility%20to%20allow%0Aattackers%20to%20specify%20their%20target%20data.%20In%20this%20paper%2C%20we%20introduce%20Geminio%2C%0Athe%20first%20approach%20to%20transform%20GIAs%20into%20semantically%20meaningful%2C%20targeted%0Aattacks.%20Geminio%20enables%20a%20brand%20new%20privacy%20attack%20experience%3A%20attackers%20can%0Adescribe%2C%20in%20natural%20language%2C%20the%20types%20of%20data%20they%20consider%20valuable%2C%20and%0AGeminio%20will%20prioritize%20reconstruction%20to%20focus%20on%20those%20high-value%20samples.%0AThis%20is%20achieved%20by%20leveraging%20a%20pretrained%20VLM%20to%20guide%20the%20optimization%20of%20a%0Amalicious%20global%20model%20that%2C%20when%20shared%20with%20and%20optimized%20by%20a%20victim%2C%0Aretains%20only%20gradients%20of%20samples%20that%20match%20the%20attacker-specified%20query.%0AExtensive%20experiments%20demonstrate%20Geminio%27s%20effectiveness%20in%20pinpointing%20and%0Areconstructing%20targeted%20samples%2C%20with%20high%20success%20rates%20across%20complex%0Adatasets%20under%20FL%20and%20large%20batch%20sizes%20and%20showing%20resilience%20against%20existing%0Adefenses.%0A&entry.1838667208=http%3A//arxiv.org/abs/2411.14937v1&entry.124074799=Read"},
{"title": "About Time: Advances, Challenges, and Outlooks of Action Understanding", "author": "Alexandros Stergiou and Ronald Poppe", "abstract": "  We have witnessed impressive advances in video action understanding.\nIncreased dataset sizes, variability, and computation availability have enabled\nleaps in performance and task diversification. Current systems can provide\ncoarse- and fine-grained descriptions of video scenes, extract segments\ncorresponding to queries, synthesize unobserved parts of videos, and predict\ncontext. This survey comprehensively reviews advances in uni- and multi-modal\naction understanding across a range of tasks. We focus on prevalent challenges,\noverview widely adopted datasets, and survey seminal works with an emphasis on\nrecent advances. We broadly distinguish between three temporal scopes: (1)\nrecognition tasks of actions observed in full, (2) prediction tasks for ongoing\npartially observed actions, and (3) forecasting tasks for subsequent unobserved\naction. This division allows us to identify specific action modeling and video\nrepresentation challenges. Finally, we outline future directions to address\ncurrent shortcomings.\n", "link": "http://arxiv.org/abs/2411.15106v1", "date": "2024-11-22", "relevancy": 2.403, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.6056}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.6056}, {"title": "RoboEXP: Action-Conditioned Scene Graph via Interactive Exploration for\n  Robotic Manipulation", "link": "http://arxiv.org/abs/2402.15487v1", "similarity": 0.5763}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20About%20Time%3A%20Advances%2C%20Challenges%2C%20and%20Outlooks%20of%20Action%20Understanding&body=Title%3A%20About%20Time%3A%20Advances%2C%20Challenges%2C%20and%20Outlooks%20of%20Action%20Understanding%0AAuthor%3A%20Alexandros%20Stergiou%20and%20Ronald%20Poppe%0AAbstract%3A%20%20%20We%20have%20witnessed%20impressive%20advances%20in%20video%20action%20understanding.%0AIncreased%20dataset%20sizes%2C%20variability%2C%20and%20computation%20availability%20have%20enabled%0Aleaps%20in%20performance%20and%20task%20diversification.%20Current%20systems%20can%20provide%0Acoarse-%20and%20fine-grained%20descriptions%20of%20video%20scenes%2C%20extract%20segments%0Acorresponding%20to%20queries%2C%20synthesize%20unobserved%20parts%20of%20videos%2C%20and%20predict%0Acontext.%20This%20survey%20comprehensively%20reviews%20advances%20in%20uni-%20and%20multi-modal%0Aaction%20understanding%20across%20a%20range%20of%20tasks.%20We%20focus%20on%20prevalent%20challenges%2C%0Aoverview%20widely%20adopted%20datasets%2C%20and%20survey%20seminal%20works%20with%20an%20emphasis%20on%0Arecent%20advances.%20We%20broadly%20distinguish%20between%20three%20temporal%20scopes%3A%20%281%29%0Arecognition%20tasks%20of%20actions%20observed%20in%20full%2C%20%282%29%20prediction%20tasks%20for%20ongoing%0Apartially%20observed%20actions%2C%20and%20%283%29%20forecasting%20tasks%20for%20subsequent%20unobserved%0Aaction.%20This%20division%20allows%20us%20to%20identify%20specific%20action%20modeling%20and%20video%0Arepresentation%20challenges.%20Finally%2C%20we%20outline%20future%20directions%20to%20address%0Acurrent%20shortcomings.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2411.15106v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DAbout%2520Time%253A%2520Advances%252C%2520Challenges%252C%2520and%2520Outlooks%2520of%2520Action%2520Understanding%26entry.906535625%3DAlexandros%2520Stergiou%2520and%2520Ronald%2520Poppe%26entry.1292438233%3D%2520%2520We%2520have%2520witnessed%2520impressive%2520advances%2520in%2520video%2520action%2520understanding.%250AIncreased%2520dataset%2520sizes%252C%2520variability%252C%2520and%2520computation%2520availability%2520have%2520enabled%250Aleaps%2520in%2520performance%2520and%2520task%2520diversification.%2520Current%2520systems%2520can%2520provide%250Acoarse-%2520and%2520fine-grained%2520descriptions%2520of%2520video%2520scenes%252C%2520extract%2520segments%250Acorresponding%2520to%2520queries%252C%2520synthesize%2520unobserved%2520parts%2520of%2520videos%252C%2520and%2520predict%250Acontext.%2520This%2520survey%2520comprehensively%2520reviews%2520advances%2520in%2520uni-%2520and%2520multi-modal%250Aaction%2520understanding%2520across%2520a%2520range%2520of%2520tasks.%2520We%2520focus%2520on%2520prevalent%2520challenges%252C%250Aoverview%2520widely%2520adopted%2520datasets%252C%2520and%2520survey%2520seminal%2520works%2520with%2520an%2520emphasis%2520on%250Arecent%2520advances.%2520We%2520broadly%2520distinguish%2520between%2520three%2520temporal%2520scopes%253A%2520%25281%2529%250Arecognition%2520tasks%2520of%2520actions%2520observed%2520in%2520full%252C%2520%25282%2529%2520prediction%2520tasks%2520for%2520ongoing%250Apartially%2520observed%2520actions%252C%2520and%2520%25283%2529%2520forecasting%2520tasks%2520for%2520subsequent%2520unobserved%250Aaction.%2520This%2520division%2520allows%2520us%2520to%2520identify%2520specific%2520action%2520modeling%2520and%2520video%250Arepresentation%2520challenges.%2520Finally%252C%2520we%2520outline%2520future%2520directions%2520to%2520address%250Acurrent%2520shortcomings.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2411.15106v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=About%20Time%3A%20Advances%2C%20Challenges%2C%20and%20Outlooks%20of%20Action%20Understanding&entry.906535625=Alexandros%20Stergiou%20and%20Ronald%20Poppe&entry.1292438233=%20%20We%20have%20witnessed%20impressive%20advances%20in%20video%20action%20understanding.%0AIncreased%20dataset%20sizes%2C%20variability%2C%20and%20computation%20availability%20have%20enabled%0Aleaps%20in%20performance%20and%20task%20diversification.%20Current%20systems%20can%20provide%0Acoarse-%20and%20fine-grained%20descriptions%20of%20video%20scenes%2C%20extract%20segments%0Acorresponding%20to%20queries%2C%20synthesize%20unobserved%20parts%20of%20videos%2C%20and%20predict%0Acontext.%20This%20survey%20comprehensively%20reviews%20advances%20in%20uni-%20and%20multi-modal%0Aaction%20understanding%20across%20a%20range%20of%20tasks.%20We%20focus%20on%20prevalent%20challenges%2C%0Aoverview%20widely%20adopted%20datasets%2C%20and%20survey%20seminal%20works%20with%20an%20emphasis%20on%0Arecent%20advances.%20We%20broadly%20distinguish%20between%20three%20temporal%20scopes%3A%20%281%29%0Arecognition%20tasks%20of%20actions%20observed%20in%20full%2C%20%282%29%20prediction%20tasks%20for%20ongoing%0Apartially%20observed%20actions%2C%20and%20%283%29%20forecasting%20tasks%20for%20subsequent%20unobserved%0Aaction.%20This%20division%20allows%20us%20to%20identify%20specific%20action%20modeling%20and%20video%0Arepresentation%20challenges.%20Finally%2C%20we%20outline%20future%20directions%20to%20address%0Acurrent%20shortcomings.%0A&entry.1838667208=http%3A//arxiv.org/abs/2411.15106v1&entry.124074799=Read"},
{"title": "AttriBoT: A Bag of Tricks for Efficiently Approximating Leave-One-Out\n  Context Attribution", "author": "Fengyuan Liu and Nikhil Kandpal and Colin Raffel", "abstract": "  The influence of contextual input on the behavior of large language models\n(LLMs) has prompted the development of context attribution methods that aim to\nquantify each context span's effect on an LLM's generations. The leave-one-out\n(LOO) error, which measures the change in the likelihood of the LLM's response\nwhen a given span of the context is removed, provides a principled way to\nperform context attribution, but can be prohibitively expensive to compute for\nlarge models. In this work, we introduce AttriBoT, a series of novel techniques\nfor efficiently computing an approximation of the LOO error for context\nattribution. Specifically, AttriBoT uses cached activations to avoid redundant\noperations, performs hierarchical attribution to reduce computation, and\nemulates the behavior of large target models with smaller proxy models. Taken\ntogether, AttriBoT can provide a >300x speedup while remaining more faithful to\na target model's LOO error than prior context attribution methods. This stark\nincrease in performance makes computing context attributions for a given\nresponse 30x faster than generating the response itself, empowering real-world\napplications that require computing attributions at scale. We release a\nuser-friendly and efficient implementation of AttriBoT to enable efficient LLM\ninterpretability as well as encourage future development of efficient context\nattribution methods.\n", "link": "http://arxiv.org/abs/2411.15102v1", "date": "2024-11-22", "relevancy": 2.3898, "topK": [{"title": "ToDo: Token Downsampling for Efficient Generation of High-Resolution\n  Images", "link": "http://arxiv.org/abs/2402.13573v2", "similarity": 0.4801}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.4796}, {"title": "Breadcrumbs: Adversarial class-balanced sampling for long-tailed recognition", "link": "https://link.springer.com/chapter/10.1007/978-3-031-20053-3_37", "similarity": 0.4743}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20AttriBoT%3A%20A%20Bag%20of%20Tricks%20for%20Efficiently%20Approximating%20Leave-One-Out%0A%20%20Context%20Attribution&body=Title%3A%20AttriBoT%3A%20A%20Bag%20of%20Tricks%20for%20Efficiently%20Approximating%20Leave-One-Out%0A%20%20Context%20Attribution%0AAuthor%3A%20Fengyuan%20Liu%20and%20Nikhil%20Kandpal%20and%20Colin%20Raffel%0AAbstract%3A%20%20%20The%20influence%20of%20contextual%20input%20on%20the%20behavior%20of%20large%20language%20models%0A%28LLMs%29%20has%20prompted%20the%20development%20of%20context%20attribution%20methods%20that%20aim%20to%0Aquantify%20each%20context%20span%27s%20effect%20on%20an%20LLM%27s%20generations.%20The%20leave-one-out%0A%28LOO%29%20error%2C%20which%20measures%20the%20change%20in%20the%20likelihood%20of%20the%20LLM%27s%20response%0Awhen%20a%20given%20span%20of%20the%20context%20is%20removed%2C%20provides%20a%20principled%20way%20to%0Aperform%20context%20attribution%2C%20but%20can%20be%20prohibitively%20expensive%20to%20compute%20for%0Alarge%20models.%20In%20this%20work%2C%20we%20introduce%20AttriBoT%2C%20a%20series%20of%20novel%20techniques%0Afor%20efficiently%20computing%20an%20approximation%20of%20the%20LOO%20error%20for%20context%0Aattribution.%20Specifically%2C%20AttriBoT%20uses%20cached%20activations%20to%20avoid%20redundant%0Aoperations%2C%20performs%20hierarchical%20attribution%20to%20reduce%20computation%2C%20and%0Aemulates%20the%20behavior%20of%20large%20target%20models%20with%20smaller%20proxy%20models.%20Taken%0Atogether%2C%20AttriBoT%20can%20provide%20a%20%3E300x%20speedup%20while%20remaining%20more%20faithful%20to%0Aa%20target%20model%27s%20LOO%20error%20than%20prior%20context%20attribution%20methods.%20This%20stark%0Aincrease%20in%20performance%20makes%20computing%20context%20attributions%20for%20a%20given%0Aresponse%2030x%20faster%20than%20generating%20the%20response%20itself%2C%20empowering%20real-world%0Aapplications%20that%20require%20computing%20attributions%20at%20scale.%20We%20release%20a%0Auser-friendly%20and%20efficient%20implementation%20of%20AttriBoT%20to%20enable%20efficient%20LLM%0Ainterpretability%20as%20well%20as%20encourage%20future%20development%20of%20efficient%20context%0Aattribution%20methods.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2411.15102v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DAttriBoT%253A%2520A%2520Bag%2520of%2520Tricks%2520for%2520Efficiently%2520Approximating%2520Leave-One-Out%250A%2520%2520Context%2520Attribution%26entry.906535625%3DFengyuan%2520Liu%2520and%2520Nikhil%2520Kandpal%2520and%2520Colin%2520Raffel%26entry.1292438233%3D%2520%2520The%2520influence%2520of%2520contextual%2520input%2520on%2520the%2520behavior%2520of%2520large%2520language%2520models%250A%2528LLMs%2529%2520has%2520prompted%2520the%2520development%2520of%2520context%2520attribution%2520methods%2520that%2520aim%2520to%250Aquantify%2520each%2520context%2520span%2527s%2520effect%2520on%2520an%2520LLM%2527s%2520generations.%2520The%2520leave-one-out%250A%2528LOO%2529%2520error%252C%2520which%2520measures%2520the%2520change%2520in%2520the%2520likelihood%2520of%2520the%2520LLM%2527s%2520response%250Awhen%2520a%2520given%2520span%2520of%2520the%2520context%2520is%2520removed%252C%2520provides%2520a%2520principled%2520way%2520to%250Aperform%2520context%2520attribution%252C%2520but%2520can%2520be%2520prohibitively%2520expensive%2520to%2520compute%2520for%250Alarge%2520models.%2520In%2520this%2520work%252C%2520we%2520introduce%2520AttriBoT%252C%2520a%2520series%2520of%2520novel%2520techniques%250Afor%2520efficiently%2520computing%2520an%2520approximation%2520of%2520the%2520LOO%2520error%2520for%2520context%250Aattribution.%2520Specifically%252C%2520AttriBoT%2520uses%2520cached%2520activations%2520to%2520avoid%2520redundant%250Aoperations%252C%2520performs%2520hierarchical%2520attribution%2520to%2520reduce%2520computation%252C%2520and%250Aemulates%2520the%2520behavior%2520of%2520large%2520target%2520models%2520with%2520smaller%2520proxy%2520models.%2520Taken%250Atogether%252C%2520AttriBoT%2520can%2520provide%2520a%2520%253E300x%2520speedup%2520while%2520remaining%2520more%2520faithful%2520to%250Aa%2520target%2520model%2527s%2520LOO%2520error%2520than%2520prior%2520context%2520attribution%2520methods.%2520This%2520stark%250Aincrease%2520in%2520performance%2520makes%2520computing%2520context%2520attributions%2520for%2520a%2520given%250Aresponse%252030x%2520faster%2520than%2520generating%2520the%2520response%2520itself%252C%2520empowering%2520real-world%250Aapplications%2520that%2520require%2520computing%2520attributions%2520at%2520scale.%2520We%2520release%2520a%250Auser-friendly%2520and%2520efficient%2520implementation%2520of%2520AttriBoT%2520to%2520enable%2520efficient%2520LLM%250Ainterpretability%2520as%2520well%2520as%2520encourage%2520future%2520development%2520of%2520efficient%2520context%250Aattribution%2520methods.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2411.15102v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=AttriBoT%3A%20A%20Bag%20of%20Tricks%20for%20Efficiently%20Approximating%20Leave-One-Out%0A%20%20Context%20Attribution&entry.906535625=Fengyuan%20Liu%20and%20Nikhil%20Kandpal%20and%20Colin%20Raffel&entry.1292438233=%20%20The%20influence%20of%20contextual%20input%20on%20the%20behavior%20of%20large%20language%20models%0A%28LLMs%29%20has%20prompted%20the%20development%20of%20context%20attribution%20methods%20that%20aim%20to%0Aquantify%20each%20context%20span%27s%20effect%20on%20an%20LLM%27s%20generations.%20The%20leave-one-out%0A%28LOO%29%20error%2C%20which%20measures%20the%20change%20in%20the%20likelihood%20of%20the%20LLM%27s%20response%0Awhen%20a%20given%20span%20of%20the%20context%20is%20removed%2C%20provides%20a%20principled%20way%20to%0Aperform%20context%20attribution%2C%20but%20can%20be%20prohibitively%20expensive%20to%20compute%20for%0Alarge%20models.%20In%20this%20work%2C%20we%20introduce%20AttriBoT%2C%20a%20series%20of%20novel%20techniques%0Afor%20efficiently%20computing%20an%20approximation%20of%20the%20LOO%20error%20for%20context%0Aattribution.%20Specifically%2C%20AttriBoT%20uses%20cached%20activations%20to%20avoid%20redundant%0Aoperations%2C%20performs%20hierarchical%20attribution%20to%20reduce%20computation%2C%20and%0Aemulates%20the%20behavior%20of%20large%20target%20models%20with%20smaller%20proxy%20models.%20Taken%0Atogether%2C%20AttriBoT%20can%20provide%20a%20%3E300x%20speedup%20while%20remaining%20more%20faithful%20to%0Aa%20target%20model%27s%20LOO%20error%20than%20prior%20context%20attribution%20methods.%20This%20stark%0Aincrease%20in%20performance%20makes%20computing%20context%20attributions%20for%20a%20given%0Aresponse%2030x%20faster%20than%20generating%20the%20response%20itself%2C%20empowering%20real-world%0Aapplications%20that%20require%20computing%20attributions%20at%20scale.%20We%20release%20a%0Auser-friendly%20and%20efficient%20implementation%20of%20AttriBoT%20to%20enable%20efficient%20LLM%0Ainterpretability%20as%20well%20as%20encourage%20future%20development%20of%20efficient%20context%0Aattribution%20methods.%0A&entry.1838667208=http%3A//arxiv.org/abs/2411.15102v1&entry.124074799=Read"},
{"title": "TIMotion: Temporal and Interactive Framework for Efficient Human-Human\n  Motion Generation", "author": "Yabiao Wang and Shuo Wang and Jiangning Zhang and Ke Fan and Jiafu Wu and Zhucun Xue and Yong Liu", "abstract": "  Human-human motion generation is essential for understanding humans as social\nbeings. Current methods fall into two main categories: single-person-based\nmethods and separate modeling-based methods. To delve into this field, we\nabstract the overall generation process into a general framework MetaMotion,\nwhich consists of two phases: temporal modeling and interaction mixing. For\ntemporal modeling, the single-person-based methods concatenate two people into\na single one directly, while the separate modeling-based methods skip the\nmodeling of interaction sequences. The inadequate modeling described above\nresulted in sub-optimal performance and redundant model parameters. In this\npaper, we introduce TIMotion (Temporal and Interactive Modeling), an efficient\nand effective framework for human-human motion generation. Specifically, we\nfirst propose Causal Interactive Injection to model two separate sequences as a\ncausal sequence leveraging the temporal and causal properties. Then we present\nRole-Evolving Scanning to adjust to the change in the active and passive roles\nthroughout the interaction. Finally, to generate smoother and more rational\nmotion, we design Localized Pattern Amplification to capture short-term motion\npatterns. Extensive experiments on InterHuman and InterX demonstrate that our\nmethod achieves superior performance. The project code will be released upon\nacceptance. Project page: https://aigc-explorer.github.io/TIMotion-page/\n", "link": "http://arxiv.org/abs/2408.17135v2", "date": "2024-11-22", "relevancy": 2.361, "topK": [{"title": "Customize-A-Video: One-Shot Motion Customization of Text-to-Video\n  Diffusion Models", "link": "http://arxiv.org/abs/2402.14780v1", "similarity": 0.6004}, {"title": "VirtualModel: Generating Object-ID-retentive Human-object Interaction\n  Image by Diffusion Model for E-commerce Marketing", "link": "http://arxiv.org/abs/2405.09985v1", "similarity": 0.5926}, {"title": "PhysGen: Rigid-Body Physics-Grounded Image-to-Video Generation", "link": "http://arxiv.org/abs/2409.18964v1", "similarity": 0.5838}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20TIMotion%3A%20Temporal%20and%20Interactive%20Framework%20for%20Efficient%20Human-Human%0A%20%20Motion%20Generation&body=Title%3A%20TIMotion%3A%20Temporal%20and%20Interactive%20Framework%20for%20Efficient%20Human-Human%0A%20%20Motion%20Generation%0AAuthor%3A%20Yabiao%20Wang%20and%20Shuo%20Wang%20and%20Jiangning%20Zhang%20and%20Ke%20Fan%20and%20Jiafu%20Wu%20and%20Zhucun%20Xue%20and%20Yong%20Liu%0AAbstract%3A%20%20%20Human-human%20motion%20generation%20is%20essential%20for%20understanding%20humans%20as%20social%0Abeings.%20Current%20methods%20fall%20into%20two%20main%20categories%3A%20single-person-based%0Amethods%20and%20separate%20modeling-based%20methods.%20To%20delve%20into%20this%20field%2C%20we%0Aabstract%20the%20overall%20generation%20process%20into%20a%20general%20framework%20MetaMotion%2C%0Awhich%20consists%20of%20two%20phases%3A%20temporal%20modeling%20and%20interaction%20mixing.%20For%0Atemporal%20modeling%2C%20the%20single-person-based%20methods%20concatenate%20two%20people%20into%0Aa%20single%20one%20directly%2C%20while%20the%20separate%20modeling-based%20methods%20skip%20the%0Amodeling%20of%20interaction%20sequences.%20The%20inadequate%20modeling%20described%20above%0Aresulted%20in%20sub-optimal%20performance%20and%20redundant%20model%20parameters.%20In%20this%0Apaper%2C%20we%20introduce%20TIMotion%20%28Temporal%20and%20Interactive%20Modeling%29%2C%20an%20efficient%0Aand%20effective%20framework%20for%20human-human%20motion%20generation.%20Specifically%2C%20we%0Afirst%20propose%20Causal%20Interactive%20Injection%20to%20model%20two%20separate%20sequences%20as%20a%0Acausal%20sequence%20leveraging%20the%20temporal%20and%20causal%20properties.%20Then%20we%20present%0ARole-Evolving%20Scanning%20to%20adjust%20to%20the%20change%20in%20the%20active%20and%20passive%20roles%0Athroughout%20the%20interaction.%20Finally%2C%20to%20generate%20smoother%20and%20more%20rational%0Amotion%2C%20we%20design%20Localized%20Pattern%20Amplification%20to%20capture%20short-term%20motion%0Apatterns.%20Extensive%20experiments%20on%20InterHuman%20and%20InterX%20demonstrate%20that%20our%0Amethod%20achieves%20superior%20performance.%20The%20project%20code%20will%20be%20released%20upon%0Aacceptance.%20Project%20page%3A%20https%3A//aigc-explorer.github.io/TIMotion-page/%0A%0ALink%3A%20http%3A//arxiv.org/abs/2408.17135v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DTIMotion%253A%2520Temporal%2520and%2520Interactive%2520Framework%2520for%2520Efficient%2520Human-Human%250A%2520%2520Motion%2520Generation%26entry.906535625%3DYabiao%2520Wang%2520and%2520Shuo%2520Wang%2520and%2520Jiangning%2520Zhang%2520and%2520Ke%2520Fan%2520and%2520Jiafu%2520Wu%2520and%2520Zhucun%2520Xue%2520and%2520Yong%2520Liu%26entry.1292438233%3D%2520%2520Human-human%2520motion%2520generation%2520is%2520essential%2520for%2520understanding%2520humans%2520as%2520social%250Abeings.%2520Current%2520methods%2520fall%2520into%2520two%2520main%2520categories%253A%2520single-person-based%250Amethods%2520and%2520separate%2520modeling-based%2520methods.%2520To%2520delve%2520into%2520this%2520field%252C%2520we%250Aabstract%2520the%2520overall%2520generation%2520process%2520into%2520a%2520general%2520framework%2520MetaMotion%252C%250Awhich%2520consists%2520of%2520two%2520phases%253A%2520temporal%2520modeling%2520and%2520interaction%2520mixing.%2520For%250Atemporal%2520modeling%252C%2520the%2520single-person-based%2520methods%2520concatenate%2520two%2520people%2520into%250Aa%2520single%2520one%2520directly%252C%2520while%2520the%2520separate%2520modeling-based%2520methods%2520skip%2520the%250Amodeling%2520of%2520interaction%2520sequences.%2520The%2520inadequate%2520modeling%2520described%2520above%250Aresulted%2520in%2520sub-optimal%2520performance%2520and%2520redundant%2520model%2520parameters.%2520In%2520this%250Apaper%252C%2520we%2520introduce%2520TIMotion%2520%2528Temporal%2520and%2520Interactive%2520Modeling%2529%252C%2520an%2520efficient%250Aand%2520effective%2520framework%2520for%2520human-human%2520motion%2520generation.%2520Specifically%252C%2520we%250Afirst%2520propose%2520Causal%2520Interactive%2520Injection%2520to%2520model%2520two%2520separate%2520sequences%2520as%2520a%250Acausal%2520sequence%2520leveraging%2520the%2520temporal%2520and%2520causal%2520properties.%2520Then%2520we%2520present%250ARole-Evolving%2520Scanning%2520to%2520adjust%2520to%2520the%2520change%2520in%2520the%2520active%2520and%2520passive%2520roles%250Athroughout%2520the%2520interaction.%2520Finally%252C%2520to%2520generate%2520smoother%2520and%2520more%2520rational%250Amotion%252C%2520we%2520design%2520Localized%2520Pattern%2520Amplification%2520to%2520capture%2520short-term%2520motion%250Apatterns.%2520Extensive%2520experiments%2520on%2520InterHuman%2520and%2520InterX%2520demonstrate%2520that%2520our%250Amethod%2520achieves%2520superior%2520performance.%2520The%2520project%2520code%2520will%2520be%2520released%2520upon%250Aacceptance.%2520Project%2520page%253A%2520https%253A//aigc-explorer.github.io/TIMotion-page/%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2408.17135v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=TIMotion%3A%20Temporal%20and%20Interactive%20Framework%20for%20Efficient%20Human-Human%0A%20%20Motion%20Generation&entry.906535625=Yabiao%20Wang%20and%20Shuo%20Wang%20and%20Jiangning%20Zhang%20and%20Ke%20Fan%20and%20Jiafu%20Wu%20and%20Zhucun%20Xue%20and%20Yong%20Liu&entry.1292438233=%20%20Human-human%20motion%20generation%20is%20essential%20for%20understanding%20humans%20as%20social%0Abeings.%20Current%20methods%20fall%20into%20two%20main%20categories%3A%20single-person-based%0Amethods%20and%20separate%20modeling-based%20methods.%20To%20delve%20into%20this%20field%2C%20we%0Aabstract%20the%20overall%20generation%20process%20into%20a%20general%20framework%20MetaMotion%2C%0Awhich%20consists%20of%20two%20phases%3A%20temporal%20modeling%20and%20interaction%20mixing.%20For%0Atemporal%20modeling%2C%20the%20single-person-based%20methods%20concatenate%20two%20people%20into%0Aa%20single%20one%20directly%2C%20while%20the%20separate%20modeling-based%20methods%20skip%20the%0Amodeling%20of%20interaction%20sequences.%20The%20inadequate%20modeling%20described%20above%0Aresulted%20in%20sub-optimal%20performance%20and%20redundant%20model%20parameters.%20In%20this%0Apaper%2C%20we%20introduce%20TIMotion%20%28Temporal%20and%20Interactive%20Modeling%29%2C%20an%20efficient%0Aand%20effective%20framework%20for%20human-human%20motion%20generation.%20Specifically%2C%20we%0Afirst%20propose%20Causal%20Interactive%20Injection%20to%20model%20two%20separate%20sequences%20as%20a%0Acausal%20sequence%20leveraging%20the%20temporal%20and%20causal%20properties.%20Then%20we%20present%0ARole-Evolving%20Scanning%20to%20adjust%20to%20the%20change%20in%20the%20active%20and%20passive%20roles%0Athroughout%20the%20interaction.%20Finally%2C%20to%20generate%20smoother%20and%20more%20rational%0Amotion%2C%20we%20design%20Localized%20Pattern%20Amplification%20to%20capture%20short-term%20motion%0Apatterns.%20Extensive%20experiments%20on%20InterHuman%20and%20InterX%20demonstrate%20that%20our%0Amethod%20achieves%20superior%20performance.%20The%20project%20code%20will%20be%20released%20upon%0Aacceptance.%20Project%20page%3A%20https%3A//aigc-explorer.github.io/TIMotion-page/%0A&entry.1838667208=http%3A//arxiv.org/abs/2408.17135v2&entry.124074799=Read"},
{"title": "Morph: A Motion-free Physics Optimization Framework for Human Motion\n  Generation", "author": "Zhuo Li and Mingshuang Luo and Ruibing Hou and Xin Zhao and Hao Liu and Hong Chang and Zimo Liu and Chen Li", "abstract": "  Human motion generation plays a vital role in applications such as digital\nhumans and humanoid robot control. However, most existing approaches disregard\nphysics constraints, leading to the frequent production of physically\nimplausible motions with pronounced artifacts such as floating and foot\nsliding. In this paper, we propose \\textbf{Morph}, a\n\\textbf{Mo}tion-f\\textbf{r}ee \\textbf{ph}ysics optimization framework,\ncomprising a Motion Generator and a Motion Physics Refinement module, for\nenhancing physical plausibility without relying on costly real-world motion\ndata. Specifically, the Motion Generator is responsible for providing\nlarge-scale synthetic motion data, while the Motion Physics Refinement Module\nutilizes these synthetic data to train a motion imitator within a physics\nsimulator, enforcing physical constraints to project the noisy motions into a\nphysically-plausible space. These physically refined motions, in turn, are used\nto fine-tune the Motion Generator, further enhancing its capability.\nExperiments on both text-to-motion and music-to-dance generation tasks\ndemonstrate that our framework achieves state-of-the-art motion generation\nquality while improving physical plausibility drastically.\n", "link": "http://arxiv.org/abs/2411.14951v1", "date": "2024-11-22", "relevancy": 2.357, "topK": [{"title": "PhysGen: Rigid-Body Physics-Grounded Image-to-Video Generation", "link": "http://arxiv.org/abs/2409.18964v1", "similarity": 0.6584}, {"title": "Thinking Outside the BBox: Unconstrained Generative Object Compositing", "link": "http://arxiv.org/abs/2409.04559v2", "similarity": 0.5416}, {"title": "Tuning-Free Noise Rectification for High Fidelity Image-to-Video\n  Generation", "link": "http://arxiv.org/abs/2403.02827v1", "similarity": 0.5391}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Morph%3A%20A%20Motion-free%20Physics%20Optimization%20Framework%20for%20Human%20Motion%0A%20%20Generation&body=Title%3A%20Morph%3A%20A%20Motion-free%20Physics%20Optimization%20Framework%20for%20Human%20Motion%0A%20%20Generation%0AAuthor%3A%20Zhuo%20Li%20and%20Mingshuang%20Luo%20and%20Ruibing%20Hou%20and%20Xin%20Zhao%20and%20Hao%20Liu%20and%20Hong%20Chang%20and%20Zimo%20Liu%20and%20Chen%20Li%0AAbstract%3A%20%20%20Human%20motion%20generation%20plays%20a%20vital%20role%20in%20applications%20such%20as%20digital%0Ahumans%20and%20humanoid%20robot%20control.%20However%2C%20most%20existing%20approaches%20disregard%0Aphysics%20constraints%2C%20leading%20to%20the%20frequent%20production%20of%20physically%0Aimplausible%20motions%20with%20pronounced%20artifacts%20such%20as%20floating%20and%20foot%0Asliding.%20In%20this%20paper%2C%20we%20propose%20%5Ctextbf%7BMorph%7D%2C%20a%0A%5Ctextbf%7BMo%7Dtion-f%5Ctextbf%7Br%7Dee%20%5Ctextbf%7Bph%7Dysics%20optimization%20framework%2C%0Acomprising%20a%20Motion%20Generator%20and%20a%20Motion%20Physics%20Refinement%20module%2C%20for%0Aenhancing%20physical%20plausibility%20without%20relying%20on%20costly%20real-world%20motion%0Adata.%20Specifically%2C%20the%20Motion%20Generator%20is%20responsible%20for%20providing%0Alarge-scale%20synthetic%20motion%20data%2C%20while%20the%20Motion%20Physics%20Refinement%20Module%0Autilizes%20these%20synthetic%20data%20to%20train%20a%20motion%20imitator%20within%20a%20physics%0Asimulator%2C%20enforcing%20physical%20constraints%20to%20project%20the%20noisy%20motions%20into%20a%0Aphysically-plausible%20space.%20These%20physically%20refined%20motions%2C%20in%20turn%2C%20are%20used%0Ato%20fine-tune%20the%20Motion%20Generator%2C%20further%20enhancing%20its%20capability.%0AExperiments%20on%20both%20text-to-motion%20and%20music-to-dance%20generation%20tasks%0Ademonstrate%20that%20our%20framework%20achieves%20state-of-the-art%20motion%20generation%0Aquality%20while%20improving%20physical%20plausibility%20drastically.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2411.14951v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DMorph%253A%2520A%2520Motion-free%2520Physics%2520Optimization%2520Framework%2520for%2520Human%2520Motion%250A%2520%2520Generation%26entry.906535625%3DZhuo%2520Li%2520and%2520Mingshuang%2520Luo%2520and%2520Ruibing%2520Hou%2520and%2520Xin%2520Zhao%2520and%2520Hao%2520Liu%2520and%2520Hong%2520Chang%2520and%2520Zimo%2520Liu%2520and%2520Chen%2520Li%26entry.1292438233%3D%2520%2520Human%2520motion%2520generation%2520plays%2520a%2520vital%2520role%2520in%2520applications%2520such%2520as%2520digital%250Ahumans%2520and%2520humanoid%2520robot%2520control.%2520However%252C%2520most%2520existing%2520approaches%2520disregard%250Aphysics%2520constraints%252C%2520leading%2520to%2520the%2520frequent%2520production%2520of%2520physically%250Aimplausible%2520motions%2520with%2520pronounced%2520artifacts%2520such%2520as%2520floating%2520and%2520foot%250Asliding.%2520In%2520this%2520paper%252C%2520we%2520propose%2520%255Ctextbf%257BMorph%257D%252C%2520a%250A%255Ctextbf%257BMo%257Dtion-f%255Ctextbf%257Br%257Dee%2520%255Ctextbf%257Bph%257Dysics%2520optimization%2520framework%252C%250Acomprising%2520a%2520Motion%2520Generator%2520and%2520a%2520Motion%2520Physics%2520Refinement%2520module%252C%2520for%250Aenhancing%2520physical%2520plausibility%2520without%2520relying%2520on%2520costly%2520real-world%2520motion%250Adata.%2520Specifically%252C%2520the%2520Motion%2520Generator%2520is%2520responsible%2520for%2520providing%250Alarge-scale%2520synthetic%2520motion%2520data%252C%2520while%2520the%2520Motion%2520Physics%2520Refinement%2520Module%250Autilizes%2520these%2520synthetic%2520data%2520to%2520train%2520a%2520motion%2520imitator%2520within%2520a%2520physics%250Asimulator%252C%2520enforcing%2520physical%2520constraints%2520to%2520project%2520the%2520noisy%2520motions%2520into%2520a%250Aphysically-plausible%2520space.%2520These%2520physically%2520refined%2520motions%252C%2520in%2520turn%252C%2520are%2520used%250Ato%2520fine-tune%2520the%2520Motion%2520Generator%252C%2520further%2520enhancing%2520its%2520capability.%250AExperiments%2520on%2520both%2520text-to-motion%2520and%2520music-to-dance%2520generation%2520tasks%250Ademonstrate%2520that%2520our%2520framework%2520achieves%2520state-of-the-art%2520motion%2520generation%250Aquality%2520while%2520improving%2520physical%2520plausibility%2520drastically.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2411.14951v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Morph%3A%20A%20Motion-free%20Physics%20Optimization%20Framework%20for%20Human%20Motion%0A%20%20Generation&entry.906535625=Zhuo%20Li%20and%20Mingshuang%20Luo%20and%20Ruibing%20Hou%20and%20Xin%20Zhao%20and%20Hao%20Liu%20and%20Hong%20Chang%20and%20Zimo%20Liu%20and%20Chen%20Li&entry.1292438233=%20%20Human%20motion%20generation%20plays%20a%20vital%20role%20in%20applications%20such%20as%20digital%0Ahumans%20and%20humanoid%20robot%20control.%20However%2C%20most%20existing%20approaches%20disregard%0Aphysics%20constraints%2C%20leading%20to%20the%20frequent%20production%20of%20physically%0Aimplausible%20motions%20with%20pronounced%20artifacts%20such%20as%20floating%20and%20foot%0Asliding.%20In%20this%20paper%2C%20we%20propose%20%5Ctextbf%7BMorph%7D%2C%20a%0A%5Ctextbf%7BMo%7Dtion-f%5Ctextbf%7Br%7Dee%20%5Ctextbf%7Bph%7Dysics%20optimization%20framework%2C%0Acomprising%20a%20Motion%20Generator%20and%20a%20Motion%20Physics%20Refinement%20module%2C%20for%0Aenhancing%20physical%20plausibility%20without%20relying%20on%20costly%20real-world%20motion%0Adata.%20Specifically%2C%20the%20Motion%20Generator%20is%20responsible%20for%20providing%0Alarge-scale%20synthetic%20motion%20data%2C%20while%20the%20Motion%20Physics%20Refinement%20Module%0Autilizes%20these%20synthetic%20data%20to%20train%20a%20motion%20imitator%20within%20a%20physics%0Asimulator%2C%20enforcing%20physical%20constraints%20to%20project%20the%20noisy%20motions%20into%20a%0Aphysically-plausible%20space.%20These%20physically%20refined%20motions%2C%20in%20turn%2C%20are%20used%0Ato%20fine-tune%20the%20Motion%20Generator%2C%20further%20enhancing%20its%20capability.%0AExperiments%20on%20both%20text-to-motion%20and%20music-to-dance%20generation%20tasks%0Ademonstrate%20that%20our%20framework%20achieves%20state-of-the-art%20motion%20generation%0Aquality%20while%20improving%20physical%20plausibility%20drastically.%0A&entry.1838667208=http%3A//arxiv.org/abs/2411.14951v1&entry.124074799=Read"},
{"title": "Who Can Withstand Chat-Audio Attacks? An Evaluation Benchmark for Large\n  Language Models", "author": "Wanqi Yang and Yanda Li and Meng Fang and Yunchao Wei and Tianyi Zhou and Ling Chen", "abstract": "  Adversarial audio attacks pose a significant threat to the growing use of\nlarge language models (LLMs) in voice-based human-machine interactions. While\nexisting research has primarily focused on model-specific adversarial methods,\nreal-world applications demand a more generalizable and universal approach to\naudio adversarial attacks. In this paper, we introduce the Chat-Audio Attacks\n(CAA) benchmark including four distinct types of audio attacks, which aims to\nexplore the the vulnerabilities of LLMs to these audio attacks in\nconversational scenarios. To evaluate the robustness of LLMs, we propose three\nevaluation strategies: Standard Evaluation, utilizing traditional metrics to\nquantify model performance under attacks; GPT-4o-Based Evaluation, which\nsimulates real-world conversational complexities; and Human Evaluation,\noffering insights into user perception and trust. We evaluate six\nstate-of-the-art LLMs with voice interaction capabilities, including\nGemini-1.5-Pro, GPT-4o, and others, using three distinct evaluation methods on\nthe CAA benchmark. Our comprehensive analysis reveals the impact of four types\nof audio attacks on the performance of these models, demonstrating that GPT-4o\nexhibits the highest level of resilience.\n", "link": "http://arxiv.org/abs/2411.14842v1", "date": "2024-11-22", "relevancy": 2.3511, "topK": [{"title": "Breadcrumbs: Adversarial class-balanced sampling for long-tailed recognition", "link": "https://link.springer.com/chapter/10.1007/978-3-031-20053-3_37", "similarity": 0.4841}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.4633}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.4633}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Who%20Can%20Withstand%20Chat-Audio%20Attacks%3F%20An%20Evaluation%20Benchmark%20for%20Large%0A%20%20Language%20Models&body=Title%3A%20Who%20Can%20Withstand%20Chat-Audio%20Attacks%3F%20An%20Evaluation%20Benchmark%20for%20Large%0A%20%20Language%20Models%0AAuthor%3A%20Wanqi%20Yang%20and%20Yanda%20Li%20and%20Meng%20Fang%20and%20Yunchao%20Wei%20and%20Tianyi%20Zhou%20and%20Ling%20Chen%0AAbstract%3A%20%20%20Adversarial%20audio%20attacks%20pose%20a%20significant%20threat%20to%20the%20growing%20use%20of%0Alarge%20language%20models%20%28LLMs%29%20in%20voice-based%20human-machine%20interactions.%20While%0Aexisting%20research%20has%20primarily%20focused%20on%20model-specific%20adversarial%20methods%2C%0Areal-world%20applications%20demand%20a%20more%20generalizable%20and%20universal%20approach%20to%0Aaudio%20adversarial%20attacks.%20In%20this%20paper%2C%20we%20introduce%20the%20Chat-Audio%20Attacks%0A%28CAA%29%20benchmark%20including%20four%20distinct%20types%20of%20audio%20attacks%2C%20which%20aims%20to%0Aexplore%20the%20the%20vulnerabilities%20of%20LLMs%20to%20these%20audio%20attacks%20in%0Aconversational%20scenarios.%20To%20evaluate%20the%20robustness%20of%20LLMs%2C%20we%20propose%20three%0Aevaluation%20strategies%3A%20Standard%20Evaluation%2C%20utilizing%20traditional%20metrics%20to%0Aquantify%20model%20performance%20under%20attacks%3B%20GPT-4o-Based%20Evaluation%2C%20which%0Asimulates%20real-world%20conversational%20complexities%3B%20and%20Human%20Evaluation%2C%0Aoffering%20insights%20into%20user%20perception%20and%20trust.%20We%20evaluate%20six%0Astate-of-the-art%20LLMs%20with%20voice%20interaction%20capabilities%2C%20including%0AGemini-1.5-Pro%2C%20GPT-4o%2C%20and%20others%2C%20using%20three%20distinct%20evaluation%20methods%20on%0Athe%20CAA%20benchmark.%20Our%20comprehensive%20analysis%20reveals%20the%20impact%20of%20four%20types%0Aof%20audio%20attacks%20on%20the%20performance%20of%20these%20models%2C%20demonstrating%20that%20GPT-4o%0Aexhibits%20the%20highest%20level%20of%20resilience.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2411.14842v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DWho%2520Can%2520Withstand%2520Chat-Audio%2520Attacks%253F%2520An%2520Evaluation%2520Benchmark%2520for%2520Large%250A%2520%2520Language%2520Models%26entry.906535625%3DWanqi%2520Yang%2520and%2520Yanda%2520Li%2520and%2520Meng%2520Fang%2520and%2520Yunchao%2520Wei%2520and%2520Tianyi%2520Zhou%2520and%2520Ling%2520Chen%26entry.1292438233%3D%2520%2520Adversarial%2520audio%2520attacks%2520pose%2520a%2520significant%2520threat%2520to%2520the%2520growing%2520use%2520of%250Alarge%2520language%2520models%2520%2528LLMs%2529%2520in%2520voice-based%2520human-machine%2520interactions.%2520While%250Aexisting%2520research%2520has%2520primarily%2520focused%2520on%2520model-specific%2520adversarial%2520methods%252C%250Areal-world%2520applications%2520demand%2520a%2520more%2520generalizable%2520and%2520universal%2520approach%2520to%250Aaudio%2520adversarial%2520attacks.%2520In%2520this%2520paper%252C%2520we%2520introduce%2520the%2520Chat-Audio%2520Attacks%250A%2528CAA%2529%2520benchmark%2520including%2520four%2520distinct%2520types%2520of%2520audio%2520attacks%252C%2520which%2520aims%2520to%250Aexplore%2520the%2520the%2520vulnerabilities%2520of%2520LLMs%2520to%2520these%2520audio%2520attacks%2520in%250Aconversational%2520scenarios.%2520To%2520evaluate%2520the%2520robustness%2520of%2520LLMs%252C%2520we%2520propose%2520three%250Aevaluation%2520strategies%253A%2520Standard%2520Evaluation%252C%2520utilizing%2520traditional%2520metrics%2520to%250Aquantify%2520model%2520performance%2520under%2520attacks%253B%2520GPT-4o-Based%2520Evaluation%252C%2520which%250Asimulates%2520real-world%2520conversational%2520complexities%253B%2520and%2520Human%2520Evaluation%252C%250Aoffering%2520insights%2520into%2520user%2520perception%2520and%2520trust.%2520We%2520evaluate%2520six%250Astate-of-the-art%2520LLMs%2520with%2520voice%2520interaction%2520capabilities%252C%2520including%250AGemini-1.5-Pro%252C%2520GPT-4o%252C%2520and%2520others%252C%2520using%2520three%2520distinct%2520evaluation%2520methods%2520on%250Athe%2520CAA%2520benchmark.%2520Our%2520comprehensive%2520analysis%2520reveals%2520the%2520impact%2520of%2520four%2520types%250Aof%2520audio%2520attacks%2520on%2520the%2520performance%2520of%2520these%2520models%252C%2520demonstrating%2520that%2520GPT-4o%250Aexhibits%2520the%2520highest%2520level%2520of%2520resilience.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2411.14842v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Who%20Can%20Withstand%20Chat-Audio%20Attacks%3F%20An%20Evaluation%20Benchmark%20for%20Large%0A%20%20Language%20Models&entry.906535625=Wanqi%20Yang%20and%20Yanda%20Li%20and%20Meng%20Fang%20and%20Yunchao%20Wei%20and%20Tianyi%20Zhou%20and%20Ling%20Chen&entry.1292438233=%20%20Adversarial%20audio%20attacks%20pose%20a%20significant%20threat%20to%20the%20growing%20use%20of%0Alarge%20language%20models%20%28LLMs%29%20in%20voice-based%20human-machine%20interactions.%20While%0Aexisting%20research%20has%20primarily%20focused%20on%20model-specific%20adversarial%20methods%2C%0Areal-world%20applications%20demand%20a%20more%20generalizable%20and%20universal%20approach%20to%0Aaudio%20adversarial%20attacks.%20In%20this%20paper%2C%20we%20introduce%20the%20Chat-Audio%20Attacks%0A%28CAA%29%20benchmark%20including%20four%20distinct%20types%20of%20audio%20attacks%2C%20which%20aims%20to%0Aexplore%20the%20the%20vulnerabilities%20of%20LLMs%20to%20these%20audio%20attacks%20in%0Aconversational%20scenarios.%20To%20evaluate%20the%20robustness%20of%20LLMs%2C%20we%20propose%20three%0Aevaluation%20strategies%3A%20Standard%20Evaluation%2C%20utilizing%20traditional%20metrics%20to%0Aquantify%20model%20performance%20under%20attacks%3B%20GPT-4o-Based%20Evaluation%2C%20which%0Asimulates%20real-world%20conversational%20complexities%3B%20and%20Human%20Evaluation%2C%0Aoffering%20insights%20into%20user%20perception%20and%20trust.%20We%20evaluate%20six%0Astate-of-the-art%20LLMs%20with%20voice%20interaction%20capabilities%2C%20including%0AGemini-1.5-Pro%2C%20GPT-4o%2C%20and%20others%2C%20using%20three%20distinct%20evaluation%20methods%20on%0Athe%20CAA%20benchmark.%20Our%20comprehensive%20analysis%20reveals%20the%20impact%20of%20four%20types%0Aof%20audio%20attacks%20on%20the%20performance%20of%20these%20models%2C%20demonstrating%20that%20GPT-4o%0Aexhibits%20the%20highest%20level%20of%20resilience.%0A&entry.1838667208=http%3A//arxiv.org/abs/2411.14842v1&entry.124074799=Read"},
{"title": "Semantically-Prompted Language Models Improve Visual Descriptions", "author": "Michael Ogezi and Bradley Hauer and Grzegorz Kondrak", "abstract": "  Language-vision models like CLIP have made significant strides in vision\ntasks, such as zero-shot image classification (ZSIC). However, generating\nspecific and expressive visual descriptions remains challenging; descriptions\nproduced by current methods are often ambiguous and lacking in granularity. To\ntackle these issues, we propose V-GLOSS: Visual Glosses, a novel method built\nupon two key ideas. The first is Semantic Prompting, which conditions a\nlanguage model on structured semantic knowledge. The second is a new\ncontrastive algorithm that elicits fine-grained distinctions between similar\nconcepts. With both ideas, we demonstrate that V-GLOSS improves visual\ndescriptions and achieves strong results in the zero-shot setting on general\nand fine-grained image-classification datasets, including ImageNet, STL-10,\nFGVC Aircraft, and Flowers 102. Moreover, these descriptive capabilities\ncontribute to enhancing image-generation performance. Finally, we introduce a\nquality-tested silver dataset with descriptions generated with V-GLOSS for all\nImageNet classes.\n", "link": "http://arxiv.org/abs/2306.06077v4", "date": "2024-11-22", "relevancy": 2.3368, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.589}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.589}, {"title": "Multimodal-Conditioned Latent Diffusion Models for Fashion Image Editing", "link": "http://arxiv.org/abs/2403.14828v2", "similarity": 0.5602}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Semantically-Prompted%20Language%20Models%20Improve%20Visual%20Descriptions&body=Title%3A%20Semantically-Prompted%20Language%20Models%20Improve%20Visual%20Descriptions%0AAuthor%3A%20Michael%20Ogezi%20and%20Bradley%20Hauer%20and%20Grzegorz%20Kondrak%0AAbstract%3A%20%20%20Language-vision%20models%20like%20CLIP%20have%20made%20significant%20strides%20in%20vision%0Atasks%2C%20such%20as%20zero-shot%20image%20classification%20%28ZSIC%29.%20However%2C%20generating%0Aspecific%20and%20expressive%20visual%20descriptions%20remains%20challenging%3B%20descriptions%0Aproduced%20by%20current%20methods%20are%20often%20ambiguous%20and%20lacking%20in%20granularity.%20To%0Atackle%20these%20issues%2C%20we%20propose%20V-GLOSS%3A%20Visual%20Glosses%2C%20a%20novel%20method%20built%0Aupon%20two%20key%20ideas.%20The%20first%20is%20Semantic%20Prompting%2C%20which%20conditions%20a%0Alanguage%20model%20on%20structured%20semantic%20knowledge.%20The%20second%20is%20a%20new%0Acontrastive%20algorithm%20that%20elicits%20fine-grained%20distinctions%20between%20similar%0Aconcepts.%20With%20both%20ideas%2C%20we%20demonstrate%20that%20V-GLOSS%20improves%20visual%0Adescriptions%20and%20achieves%20strong%20results%20in%20the%20zero-shot%20setting%20on%20general%0Aand%20fine-grained%20image-classification%20datasets%2C%20including%20ImageNet%2C%20STL-10%2C%0AFGVC%20Aircraft%2C%20and%20Flowers%20102.%20Moreover%2C%20these%20descriptive%20capabilities%0Acontribute%20to%20enhancing%20image-generation%20performance.%20Finally%2C%20we%20introduce%20a%0Aquality-tested%20silver%20dataset%20with%20descriptions%20generated%20with%20V-GLOSS%20for%20all%0AImageNet%20classes.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2306.06077v4%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DSemantically-Prompted%2520Language%2520Models%2520Improve%2520Visual%2520Descriptions%26entry.906535625%3DMichael%2520Ogezi%2520and%2520Bradley%2520Hauer%2520and%2520Grzegorz%2520Kondrak%26entry.1292438233%3D%2520%2520Language-vision%2520models%2520like%2520CLIP%2520have%2520made%2520significant%2520strides%2520in%2520vision%250Atasks%252C%2520such%2520as%2520zero-shot%2520image%2520classification%2520%2528ZSIC%2529.%2520However%252C%2520generating%250Aspecific%2520and%2520expressive%2520visual%2520descriptions%2520remains%2520challenging%253B%2520descriptions%250Aproduced%2520by%2520current%2520methods%2520are%2520often%2520ambiguous%2520and%2520lacking%2520in%2520granularity.%2520To%250Atackle%2520these%2520issues%252C%2520we%2520propose%2520V-GLOSS%253A%2520Visual%2520Glosses%252C%2520a%2520novel%2520method%2520built%250Aupon%2520two%2520key%2520ideas.%2520The%2520first%2520is%2520Semantic%2520Prompting%252C%2520which%2520conditions%2520a%250Alanguage%2520model%2520on%2520structured%2520semantic%2520knowledge.%2520The%2520second%2520is%2520a%2520new%250Acontrastive%2520algorithm%2520that%2520elicits%2520fine-grained%2520distinctions%2520between%2520similar%250Aconcepts.%2520With%2520both%2520ideas%252C%2520we%2520demonstrate%2520that%2520V-GLOSS%2520improves%2520visual%250Adescriptions%2520and%2520achieves%2520strong%2520results%2520in%2520the%2520zero-shot%2520setting%2520on%2520general%250Aand%2520fine-grained%2520image-classification%2520datasets%252C%2520including%2520ImageNet%252C%2520STL-10%252C%250AFGVC%2520Aircraft%252C%2520and%2520Flowers%2520102.%2520Moreover%252C%2520these%2520descriptive%2520capabilities%250Acontribute%2520to%2520enhancing%2520image-generation%2520performance.%2520Finally%252C%2520we%2520introduce%2520a%250Aquality-tested%2520silver%2520dataset%2520with%2520descriptions%2520generated%2520with%2520V-GLOSS%2520for%2520all%250AImageNet%2520classes.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2306.06077v4%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Semantically-Prompted%20Language%20Models%20Improve%20Visual%20Descriptions&entry.906535625=Michael%20Ogezi%20and%20Bradley%20Hauer%20and%20Grzegorz%20Kondrak&entry.1292438233=%20%20Language-vision%20models%20like%20CLIP%20have%20made%20significant%20strides%20in%20vision%0Atasks%2C%20such%20as%20zero-shot%20image%20classification%20%28ZSIC%29.%20However%2C%20generating%0Aspecific%20and%20expressive%20visual%20descriptions%20remains%20challenging%3B%20descriptions%0Aproduced%20by%20current%20methods%20are%20often%20ambiguous%20and%20lacking%20in%20granularity.%20To%0Atackle%20these%20issues%2C%20we%20propose%20V-GLOSS%3A%20Visual%20Glosses%2C%20a%20novel%20method%20built%0Aupon%20two%20key%20ideas.%20The%20first%20is%20Semantic%20Prompting%2C%20which%20conditions%20a%0Alanguage%20model%20on%20structured%20semantic%20knowledge.%20The%20second%20is%20a%20new%0Acontrastive%20algorithm%20that%20elicits%20fine-grained%20distinctions%20between%20similar%0Aconcepts.%20With%20both%20ideas%2C%20we%20demonstrate%20that%20V-GLOSS%20improves%20visual%0Adescriptions%20and%20achieves%20strong%20results%20in%20the%20zero-shot%20setting%20on%20general%0Aand%20fine-grained%20image-classification%20datasets%2C%20including%20ImageNet%2C%20STL-10%2C%0AFGVC%20Aircraft%2C%20and%20Flowers%20102.%20Moreover%2C%20these%20descriptive%20capabilities%0Acontribute%20to%20enhancing%20image-generation%20performance.%20Finally%2C%20we%20introduce%20a%0Aquality-tested%20silver%20dataset%20with%20descriptions%20generated%20with%20V-GLOSS%20for%20all%0AImageNet%20classes.%0A&entry.1838667208=http%3A//arxiv.org/abs/2306.06077v4&entry.124074799=Read"},
{"title": "SwissADT: An Audio Description Translation System for Swiss Languages", "author": "Lukas Fischer and Yingqiang Gao and Alexa Lintner and Sarah Ebling", "abstract": "  Audio description (AD) is a crucial accessibility service provided to blind\npersons and persons with visual impairment, designed to convey visual\ninformation in acoustic form. Despite recent advancements in multilingual\nmachine translation research, the lack of well-crafted and time-synchronized AD\ndata impedes the development of audio description translation (ADT) systems\nthat address the needs of multilingual countries such as Switzerland.\nFurthermore, since the majority of ADT systems rely solely on text, uncertainty\nexists as to whether incorporating visual information from the corresponding\nvideo clips can enhance the quality of ADT outputs. In this work, we present\nSwissADT, the first ADT system implemented for three main Swiss languages and\nEnglish. By collecting well-crafted AD data augmented with video clips in\nGerman, French, Italian, and English, and leveraging the power of Large\nLanguage Models (LLMs), we aim to enhance information accessibility for diverse\nlanguage populations in Switzerland by automatically translating AD scripts to\nthe desired Swiss language. Our extensive experimental ADT results, composed of\nboth automatic and human evaluations of ADT quality, demonstrate the promising\ncapability of SwissADT for the ADT task. We believe that combining human\nexpertise with the generation power of LLMs can further enhance the performance\nof ADT systems, ultimately benefiting a larger multilingual target population.\n", "link": "http://arxiv.org/abs/2411.14967v1", "date": "2024-11-22", "relevancy": 2.3284, "topK": [{"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.4729}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.4621}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.4621}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20SwissADT%3A%20An%20Audio%20Description%20Translation%20System%20for%20Swiss%20Languages&body=Title%3A%20SwissADT%3A%20An%20Audio%20Description%20Translation%20System%20for%20Swiss%20Languages%0AAuthor%3A%20Lukas%20Fischer%20and%20Yingqiang%20Gao%20and%20Alexa%20Lintner%20and%20Sarah%20Ebling%0AAbstract%3A%20%20%20Audio%20description%20%28AD%29%20is%20a%20crucial%20accessibility%20service%20provided%20to%20blind%0Apersons%20and%20persons%20with%20visual%20impairment%2C%20designed%20to%20convey%20visual%0Ainformation%20in%20acoustic%20form.%20Despite%20recent%20advancements%20in%20multilingual%0Amachine%20translation%20research%2C%20the%20lack%20of%20well-crafted%20and%20time-synchronized%20AD%0Adata%20impedes%20the%20development%20of%20audio%20description%20translation%20%28ADT%29%20systems%0Athat%20address%20the%20needs%20of%20multilingual%20countries%20such%20as%20Switzerland.%0AFurthermore%2C%20since%20the%20majority%20of%20ADT%20systems%20rely%20solely%20on%20text%2C%20uncertainty%0Aexists%20as%20to%20whether%20incorporating%20visual%20information%20from%20the%20corresponding%0Avideo%20clips%20can%20enhance%20the%20quality%20of%20ADT%20outputs.%20In%20this%20work%2C%20we%20present%0ASwissADT%2C%20the%20first%20ADT%20system%20implemented%20for%20three%20main%20Swiss%20languages%20and%0AEnglish.%20By%20collecting%20well-crafted%20AD%20data%20augmented%20with%20video%20clips%20in%0AGerman%2C%20French%2C%20Italian%2C%20and%20English%2C%20and%20leveraging%20the%20power%20of%20Large%0ALanguage%20Models%20%28LLMs%29%2C%20we%20aim%20to%20enhance%20information%20accessibility%20for%20diverse%0Alanguage%20populations%20in%20Switzerland%20by%20automatically%20translating%20AD%20scripts%20to%0Athe%20desired%20Swiss%20language.%20Our%20extensive%20experimental%20ADT%20results%2C%20composed%20of%0Aboth%20automatic%20and%20human%20evaluations%20of%20ADT%20quality%2C%20demonstrate%20the%20promising%0Acapability%20of%20SwissADT%20for%20the%20ADT%20task.%20We%20believe%20that%20combining%20human%0Aexpertise%20with%20the%20generation%20power%20of%20LLMs%20can%20further%20enhance%20the%20performance%0Aof%20ADT%20systems%2C%20ultimately%20benefiting%20a%20larger%20multilingual%20target%20population.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2411.14967v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DSwissADT%253A%2520An%2520Audio%2520Description%2520Translation%2520System%2520for%2520Swiss%2520Languages%26entry.906535625%3DLukas%2520Fischer%2520and%2520Yingqiang%2520Gao%2520and%2520Alexa%2520Lintner%2520and%2520Sarah%2520Ebling%26entry.1292438233%3D%2520%2520Audio%2520description%2520%2528AD%2529%2520is%2520a%2520crucial%2520accessibility%2520service%2520provided%2520to%2520blind%250Apersons%2520and%2520persons%2520with%2520visual%2520impairment%252C%2520designed%2520to%2520convey%2520visual%250Ainformation%2520in%2520acoustic%2520form.%2520Despite%2520recent%2520advancements%2520in%2520multilingual%250Amachine%2520translation%2520research%252C%2520the%2520lack%2520of%2520well-crafted%2520and%2520time-synchronized%2520AD%250Adata%2520impedes%2520the%2520development%2520of%2520audio%2520description%2520translation%2520%2528ADT%2529%2520systems%250Athat%2520address%2520the%2520needs%2520of%2520multilingual%2520countries%2520such%2520as%2520Switzerland.%250AFurthermore%252C%2520since%2520the%2520majority%2520of%2520ADT%2520systems%2520rely%2520solely%2520on%2520text%252C%2520uncertainty%250Aexists%2520as%2520to%2520whether%2520incorporating%2520visual%2520information%2520from%2520the%2520corresponding%250Avideo%2520clips%2520can%2520enhance%2520the%2520quality%2520of%2520ADT%2520outputs.%2520In%2520this%2520work%252C%2520we%2520present%250ASwissADT%252C%2520the%2520first%2520ADT%2520system%2520implemented%2520for%2520three%2520main%2520Swiss%2520languages%2520and%250AEnglish.%2520By%2520collecting%2520well-crafted%2520AD%2520data%2520augmented%2520with%2520video%2520clips%2520in%250AGerman%252C%2520French%252C%2520Italian%252C%2520and%2520English%252C%2520and%2520leveraging%2520the%2520power%2520of%2520Large%250ALanguage%2520Models%2520%2528LLMs%2529%252C%2520we%2520aim%2520to%2520enhance%2520information%2520accessibility%2520for%2520diverse%250Alanguage%2520populations%2520in%2520Switzerland%2520by%2520automatically%2520translating%2520AD%2520scripts%2520to%250Athe%2520desired%2520Swiss%2520language.%2520Our%2520extensive%2520experimental%2520ADT%2520results%252C%2520composed%2520of%250Aboth%2520automatic%2520and%2520human%2520evaluations%2520of%2520ADT%2520quality%252C%2520demonstrate%2520the%2520promising%250Acapability%2520of%2520SwissADT%2520for%2520the%2520ADT%2520task.%2520We%2520believe%2520that%2520combining%2520human%250Aexpertise%2520with%2520the%2520generation%2520power%2520of%2520LLMs%2520can%2520further%2520enhance%2520the%2520performance%250Aof%2520ADT%2520systems%252C%2520ultimately%2520benefiting%2520a%2520larger%2520multilingual%2520target%2520population.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2411.14967v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=SwissADT%3A%20An%20Audio%20Description%20Translation%20System%20for%20Swiss%20Languages&entry.906535625=Lukas%20Fischer%20and%20Yingqiang%20Gao%20and%20Alexa%20Lintner%20and%20Sarah%20Ebling&entry.1292438233=%20%20Audio%20description%20%28AD%29%20is%20a%20crucial%20accessibility%20service%20provided%20to%20blind%0Apersons%20and%20persons%20with%20visual%20impairment%2C%20designed%20to%20convey%20visual%0Ainformation%20in%20acoustic%20form.%20Despite%20recent%20advancements%20in%20multilingual%0Amachine%20translation%20research%2C%20the%20lack%20of%20well-crafted%20and%20time-synchronized%20AD%0Adata%20impedes%20the%20development%20of%20audio%20description%20translation%20%28ADT%29%20systems%0Athat%20address%20the%20needs%20of%20multilingual%20countries%20such%20as%20Switzerland.%0AFurthermore%2C%20since%20the%20majority%20of%20ADT%20systems%20rely%20solely%20on%20text%2C%20uncertainty%0Aexists%20as%20to%20whether%20incorporating%20visual%20information%20from%20the%20corresponding%0Avideo%20clips%20can%20enhance%20the%20quality%20of%20ADT%20outputs.%20In%20this%20work%2C%20we%20present%0ASwissADT%2C%20the%20first%20ADT%20system%20implemented%20for%20three%20main%20Swiss%20languages%20and%0AEnglish.%20By%20collecting%20well-crafted%20AD%20data%20augmented%20with%20video%20clips%20in%0AGerman%2C%20French%2C%20Italian%2C%20and%20English%2C%20and%20leveraging%20the%20power%20of%20Large%0ALanguage%20Models%20%28LLMs%29%2C%20we%20aim%20to%20enhance%20information%20accessibility%20for%20diverse%0Alanguage%20populations%20in%20Switzerland%20by%20automatically%20translating%20AD%20scripts%20to%0Athe%20desired%20Swiss%20language.%20Our%20extensive%20experimental%20ADT%20results%2C%20composed%20of%0Aboth%20automatic%20and%20human%20evaluations%20of%20ADT%20quality%2C%20demonstrate%20the%20promising%0Acapability%20of%20SwissADT%20for%20the%20ADT%20task.%20We%20believe%20that%20combining%20human%0Aexpertise%20with%20the%20generation%20power%20of%20LLMs%20can%20further%20enhance%20the%20performance%0Aof%20ADT%20systems%2C%20ultimately%20benefiting%20a%20larger%20multilingual%20target%20population.%0A&entry.1838667208=http%3A//arxiv.org/abs/2411.14967v1&entry.124074799=Read"},
{"title": "DyCoke: Dynamic Compression of Tokens for Fast Video Large Language\n  Models", "author": "Keda Tao and Can Qin and Haoxuan You and Yang Sui and Huan Wang", "abstract": "  Video large language models (VLLMs) have significantly advanced recently in\nprocessing complex video content, yet their inference efficiency remains\nconstrained because of the high computational cost stemming from the thousands\nof visual tokens generated from the video inputs. We empirically observe that,\nunlike single image inputs, VLLMs typically attend visual tokens from different\nframes at different decoding iterations, making a one-shot pruning strategy\nprone to removing important tokens by mistake. Motivated by this, we present\nDyCoke, a training-free token compression method to optimize token\nrepresentation and accelerate VLLMs. DyCoke incorporates a plug-and-play\ntemporal compression module to minimize temporal redundancy by merging\nredundant tokens across frames, and applies dynamic KV cache reduction to prune\nspatially redundant tokens selectively. It ensures high-quality inference by\ndynamically retaining the critical tokens at each decoding step. Extensive\nexperimental results demonstrate that DyCoke can outperform the prior SoTA\ncounterparts, achieving 1.5X inference speedup, 1.4X memory reduction against\nthe baseline VLLM, while still improving the performance, with no training.\n", "link": "http://arxiv.org/abs/2411.15024v1", "date": "2024-11-22", "relevancy": 2.3276, "topK": [{"title": "ToDo: Token Downsampling for Efficient Generation of High-Resolution\n  Images", "link": "http://arxiv.org/abs/2402.13573v2", "similarity": 0.5869}, {"title": "Magic-Me: Identity-Specific Video Customized Diffusion", "link": "http://arxiv.org/abs/2402.09368v1", "similarity": 0.5799}, {"title": "SDXS: Real-Time One-Step Latent Diffusion Models with Image Conditions", "link": "http://arxiv.org/abs/2403.16627v1", "similarity": 0.5743}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20DyCoke%3A%20Dynamic%20Compression%20of%20Tokens%20for%20Fast%20Video%20Large%20Language%0A%20%20Models&body=Title%3A%20DyCoke%3A%20Dynamic%20Compression%20of%20Tokens%20for%20Fast%20Video%20Large%20Language%0A%20%20Models%0AAuthor%3A%20Keda%20Tao%20and%20Can%20Qin%20and%20Haoxuan%20You%20and%20Yang%20Sui%20and%20Huan%20Wang%0AAbstract%3A%20%20%20Video%20large%20language%20models%20%28VLLMs%29%20have%20significantly%20advanced%20recently%20in%0Aprocessing%20complex%20video%20content%2C%20yet%20their%20inference%20efficiency%20remains%0Aconstrained%20because%20of%20the%20high%20computational%20cost%20stemming%20from%20the%20thousands%0Aof%20visual%20tokens%20generated%20from%20the%20video%20inputs.%20We%20empirically%20observe%20that%2C%0Aunlike%20single%20image%20inputs%2C%20VLLMs%20typically%20attend%20visual%20tokens%20from%20different%0Aframes%20at%20different%20decoding%20iterations%2C%20making%20a%20one-shot%20pruning%20strategy%0Aprone%20to%20removing%20important%20tokens%20by%20mistake.%20Motivated%20by%20this%2C%20we%20present%0ADyCoke%2C%20a%20training-free%20token%20compression%20method%20to%20optimize%20token%0Arepresentation%20and%20accelerate%20VLLMs.%20DyCoke%20incorporates%20a%20plug-and-play%0Atemporal%20compression%20module%20to%20minimize%20temporal%20redundancy%20by%20merging%0Aredundant%20tokens%20across%20frames%2C%20and%20applies%20dynamic%20KV%20cache%20reduction%20to%20prune%0Aspatially%20redundant%20tokens%20selectively.%20It%20ensures%20high-quality%20inference%20by%0Adynamically%20retaining%20the%20critical%20tokens%20at%20each%20decoding%20step.%20Extensive%0Aexperimental%20results%20demonstrate%20that%20DyCoke%20can%20outperform%20the%20prior%20SoTA%0Acounterparts%2C%20achieving%201.5X%20inference%20speedup%2C%201.4X%20memory%20reduction%20against%0Athe%20baseline%20VLLM%2C%20while%20still%20improving%20the%20performance%2C%20with%20no%20training.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2411.15024v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DDyCoke%253A%2520Dynamic%2520Compression%2520of%2520Tokens%2520for%2520Fast%2520Video%2520Large%2520Language%250A%2520%2520Models%26entry.906535625%3DKeda%2520Tao%2520and%2520Can%2520Qin%2520and%2520Haoxuan%2520You%2520and%2520Yang%2520Sui%2520and%2520Huan%2520Wang%26entry.1292438233%3D%2520%2520Video%2520large%2520language%2520models%2520%2528VLLMs%2529%2520have%2520significantly%2520advanced%2520recently%2520in%250Aprocessing%2520complex%2520video%2520content%252C%2520yet%2520their%2520inference%2520efficiency%2520remains%250Aconstrained%2520because%2520of%2520the%2520high%2520computational%2520cost%2520stemming%2520from%2520the%2520thousands%250Aof%2520visual%2520tokens%2520generated%2520from%2520the%2520video%2520inputs.%2520We%2520empirically%2520observe%2520that%252C%250Aunlike%2520single%2520image%2520inputs%252C%2520VLLMs%2520typically%2520attend%2520visual%2520tokens%2520from%2520different%250Aframes%2520at%2520different%2520decoding%2520iterations%252C%2520making%2520a%2520one-shot%2520pruning%2520strategy%250Aprone%2520to%2520removing%2520important%2520tokens%2520by%2520mistake.%2520Motivated%2520by%2520this%252C%2520we%2520present%250ADyCoke%252C%2520a%2520training-free%2520token%2520compression%2520method%2520to%2520optimize%2520token%250Arepresentation%2520and%2520accelerate%2520VLLMs.%2520DyCoke%2520incorporates%2520a%2520plug-and-play%250Atemporal%2520compression%2520module%2520to%2520minimize%2520temporal%2520redundancy%2520by%2520merging%250Aredundant%2520tokens%2520across%2520frames%252C%2520and%2520applies%2520dynamic%2520KV%2520cache%2520reduction%2520to%2520prune%250Aspatially%2520redundant%2520tokens%2520selectively.%2520It%2520ensures%2520high-quality%2520inference%2520by%250Adynamically%2520retaining%2520the%2520critical%2520tokens%2520at%2520each%2520decoding%2520step.%2520Extensive%250Aexperimental%2520results%2520demonstrate%2520that%2520DyCoke%2520can%2520outperform%2520the%2520prior%2520SoTA%250Acounterparts%252C%2520achieving%25201.5X%2520inference%2520speedup%252C%25201.4X%2520memory%2520reduction%2520against%250Athe%2520baseline%2520VLLM%252C%2520while%2520still%2520improving%2520the%2520performance%252C%2520with%2520no%2520training.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2411.15024v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=DyCoke%3A%20Dynamic%20Compression%20of%20Tokens%20for%20Fast%20Video%20Large%20Language%0A%20%20Models&entry.906535625=Keda%20Tao%20and%20Can%20Qin%20and%20Haoxuan%20You%20and%20Yang%20Sui%20and%20Huan%20Wang&entry.1292438233=%20%20Video%20large%20language%20models%20%28VLLMs%29%20have%20significantly%20advanced%20recently%20in%0Aprocessing%20complex%20video%20content%2C%20yet%20their%20inference%20efficiency%20remains%0Aconstrained%20because%20of%20the%20high%20computational%20cost%20stemming%20from%20the%20thousands%0Aof%20visual%20tokens%20generated%20from%20the%20video%20inputs.%20We%20empirically%20observe%20that%2C%0Aunlike%20single%20image%20inputs%2C%20VLLMs%20typically%20attend%20visual%20tokens%20from%20different%0Aframes%20at%20different%20decoding%20iterations%2C%20making%20a%20one-shot%20pruning%20strategy%0Aprone%20to%20removing%20important%20tokens%20by%20mistake.%20Motivated%20by%20this%2C%20we%20present%0ADyCoke%2C%20a%20training-free%20token%20compression%20method%20to%20optimize%20token%0Arepresentation%20and%20accelerate%20VLLMs.%20DyCoke%20incorporates%20a%20plug-and-play%0Atemporal%20compression%20module%20to%20minimize%20temporal%20redundancy%20by%20merging%0Aredundant%20tokens%20across%20frames%2C%20and%20applies%20dynamic%20KV%20cache%20reduction%20to%20prune%0Aspatially%20redundant%20tokens%20selectively.%20It%20ensures%20high-quality%20inference%20by%0Adynamically%20retaining%20the%20critical%20tokens%20at%20each%20decoding%20step.%20Extensive%0Aexperimental%20results%20demonstrate%20that%20DyCoke%20can%20outperform%20the%20prior%20SoTA%0Acounterparts%2C%20achieving%201.5X%20inference%20speedup%2C%201.4X%20memory%20reduction%20against%0Athe%20baseline%20VLLM%2C%20while%20still%20improving%20the%20performance%2C%20with%20no%20training.%0A&entry.1838667208=http%3A//arxiv.org/abs/2411.15024v1&entry.124074799=Read"},
{"title": "Financial Fraud Detection using Jump-Attentive Graph Neural Networks", "author": "Prashank Kadam", "abstract": "  As the availability of financial services online continues to grow, the\nincidence of fraud has surged correspondingly. Fraudsters continually seek new\nand innovative ways to circumvent the detection algorithms in place.\nTraditionally, fraud detection relied on rule-based methods, where rules were\nmanually created based on transaction data features. However, these techniques\nsoon became ineffective due to their reliance on manual rule creation and their\ninability to detect complex data patterns. Today, a significant portion of the\nfinancial services sector employs various machine learning algorithms, such as\nXGBoost, Random Forest, and neural networks, to model transaction data. While\nthese techniques have proven more efficient than rule-based methods, they still\nfail to capture interactions between different transactions and their\ninterrelationships. Recently, graph-based techniques have been adopted for\nfinancial fraud detection, leveraging graph topology to aggregate neighborhood\ninformation of transaction data using Graph Neural Networks (GNNs). Despite\nshowing improvements over previous methods, these techniques still struggle to\nkeep pace with the evolving camouflaging tactics of fraudsters and suffer from\ninformation loss due to over-smoothing. In this paper, we propose a novel\nalgorithm that employs an efficient neighborhood sampling method, effective for\ncamouflage detection and preserving crucial feature information from\nnon-similar nodes. Additionally, we introduce a novel GNN architecture that\nutilizes attention mechanisms and preserves holistic neighborhood information\nto prevent information loss. We test our algorithm on financial data to show\nthat our method outperforms other state-of-the-art graph algorithms.\n", "link": "http://arxiv.org/abs/2411.05857v2", "date": "2024-11-22", "relevancy": 2.3252, "topK": [{"title": "Boosted dynamic neural networks", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/26302", "similarity": 0.4996}, {"title": "Self-supervised Photographic Image Layout Representation Learning", "link": "http://arxiv.org/abs/2403.03740v1", "similarity": 0.4537}, {"title": "Breadcrumbs: Adversarial class-balanced sampling for long-tailed recognition", "link": "https://link.springer.com/chapter/10.1007/978-3-031-20053-3_37", "similarity": 0.4418}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Financial%20Fraud%20Detection%20using%20Jump-Attentive%20Graph%20Neural%20Networks&body=Title%3A%20Financial%20Fraud%20Detection%20using%20Jump-Attentive%20Graph%20Neural%20Networks%0AAuthor%3A%20Prashank%20Kadam%0AAbstract%3A%20%20%20As%20the%20availability%20of%20financial%20services%20online%20continues%20to%20grow%2C%20the%0Aincidence%20of%20fraud%20has%20surged%20correspondingly.%20Fraudsters%20continually%20seek%20new%0Aand%20innovative%20ways%20to%20circumvent%20the%20detection%20algorithms%20in%20place.%0ATraditionally%2C%20fraud%20detection%20relied%20on%20rule-based%20methods%2C%20where%20rules%20were%0Amanually%20created%20based%20on%20transaction%20data%20features.%20However%2C%20these%20techniques%0Asoon%20became%20ineffective%20due%20to%20their%20reliance%20on%20manual%20rule%20creation%20and%20their%0Ainability%20to%20detect%20complex%20data%20patterns.%20Today%2C%20a%20significant%20portion%20of%20the%0Afinancial%20services%20sector%20employs%20various%20machine%20learning%20algorithms%2C%20such%20as%0AXGBoost%2C%20Random%20Forest%2C%20and%20neural%20networks%2C%20to%20model%20transaction%20data.%20While%0Athese%20techniques%20have%20proven%20more%20efficient%20than%20rule-based%20methods%2C%20they%20still%0Afail%20to%20capture%20interactions%20between%20different%20transactions%20and%20their%0Ainterrelationships.%20Recently%2C%20graph-based%20techniques%20have%20been%20adopted%20for%0Afinancial%20fraud%20detection%2C%20leveraging%20graph%20topology%20to%20aggregate%20neighborhood%0Ainformation%20of%20transaction%20data%20using%20Graph%20Neural%20Networks%20%28GNNs%29.%20Despite%0Ashowing%20improvements%20over%20previous%20methods%2C%20these%20techniques%20still%20struggle%20to%0Akeep%20pace%20with%20the%20evolving%20camouflaging%20tactics%20of%20fraudsters%20and%20suffer%20from%0Ainformation%20loss%20due%20to%20over-smoothing.%20In%20this%20paper%2C%20we%20propose%20a%20novel%0Aalgorithm%20that%20employs%20an%20efficient%20neighborhood%20sampling%20method%2C%20effective%20for%0Acamouflage%20detection%20and%20preserving%20crucial%20feature%20information%20from%0Anon-similar%20nodes.%20Additionally%2C%20we%20introduce%20a%20novel%20GNN%20architecture%20that%0Autilizes%20attention%20mechanisms%20and%20preserves%20holistic%20neighborhood%20information%0Ato%20prevent%20information%20loss.%20We%20test%20our%20algorithm%20on%20financial%20data%20to%20show%0Athat%20our%20method%20outperforms%20other%20state-of-the-art%20graph%20algorithms.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2411.05857v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DFinancial%2520Fraud%2520Detection%2520using%2520Jump-Attentive%2520Graph%2520Neural%2520Networks%26entry.906535625%3DPrashank%2520Kadam%26entry.1292438233%3D%2520%2520As%2520the%2520availability%2520of%2520financial%2520services%2520online%2520continues%2520to%2520grow%252C%2520the%250Aincidence%2520of%2520fraud%2520has%2520surged%2520correspondingly.%2520Fraudsters%2520continually%2520seek%2520new%250Aand%2520innovative%2520ways%2520to%2520circumvent%2520the%2520detection%2520algorithms%2520in%2520place.%250ATraditionally%252C%2520fraud%2520detection%2520relied%2520on%2520rule-based%2520methods%252C%2520where%2520rules%2520were%250Amanually%2520created%2520based%2520on%2520transaction%2520data%2520features.%2520However%252C%2520these%2520techniques%250Asoon%2520became%2520ineffective%2520due%2520to%2520their%2520reliance%2520on%2520manual%2520rule%2520creation%2520and%2520their%250Ainability%2520to%2520detect%2520complex%2520data%2520patterns.%2520Today%252C%2520a%2520significant%2520portion%2520of%2520the%250Afinancial%2520services%2520sector%2520employs%2520various%2520machine%2520learning%2520algorithms%252C%2520such%2520as%250AXGBoost%252C%2520Random%2520Forest%252C%2520and%2520neural%2520networks%252C%2520to%2520model%2520transaction%2520data.%2520While%250Athese%2520techniques%2520have%2520proven%2520more%2520efficient%2520than%2520rule-based%2520methods%252C%2520they%2520still%250Afail%2520to%2520capture%2520interactions%2520between%2520different%2520transactions%2520and%2520their%250Ainterrelationships.%2520Recently%252C%2520graph-based%2520techniques%2520have%2520been%2520adopted%2520for%250Afinancial%2520fraud%2520detection%252C%2520leveraging%2520graph%2520topology%2520to%2520aggregate%2520neighborhood%250Ainformation%2520of%2520transaction%2520data%2520using%2520Graph%2520Neural%2520Networks%2520%2528GNNs%2529.%2520Despite%250Ashowing%2520improvements%2520over%2520previous%2520methods%252C%2520these%2520techniques%2520still%2520struggle%2520to%250Akeep%2520pace%2520with%2520the%2520evolving%2520camouflaging%2520tactics%2520of%2520fraudsters%2520and%2520suffer%2520from%250Ainformation%2520loss%2520due%2520to%2520over-smoothing.%2520In%2520this%2520paper%252C%2520we%2520propose%2520a%2520novel%250Aalgorithm%2520that%2520employs%2520an%2520efficient%2520neighborhood%2520sampling%2520method%252C%2520effective%2520for%250Acamouflage%2520detection%2520and%2520preserving%2520crucial%2520feature%2520information%2520from%250Anon-similar%2520nodes.%2520Additionally%252C%2520we%2520introduce%2520a%2520novel%2520GNN%2520architecture%2520that%250Autilizes%2520attention%2520mechanisms%2520and%2520preserves%2520holistic%2520neighborhood%2520information%250Ato%2520prevent%2520information%2520loss.%2520We%2520test%2520our%2520algorithm%2520on%2520financial%2520data%2520to%2520show%250Athat%2520our%2520method%2520outperforms%2520other%2520state-of-the-art%2520graph%2520algorithms.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2411.05857v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Financial%20Fraud%20Detection%20using%20Jump-Attentive%20Graph%20Neural%20Networks&entry.906535625=Prashank%20Kadam&entry.1292438233=%20%20As%20the%20availability%20of%20financial%20services%20online%20continues%20to%20grow%2C%20the%0Aincidence%20of%20fraud%20has%20surged%20correspondingly.%20Fraudsters%20continually%20seek%20new%0Aand%20innovative%20ways%20to%20circumvent%20the%20detection%20algorithms%20in%20place.%0ATraditionally%2C%20fraud%20detection%20relied%20on%20rule-based%20methods%2C%20where%20rules%20were%0Amanually%20created%20based%20on%20transaction%20data%20features.%20However%2C%20these%20techniques%0Asoon%20became%20ineffective%20due%20to%20their%20reliance%20on%20manual%20rule%20creation%20and%20their%0Ainability%20to%20detect%20complex%20data%20patterns.%20Today%2C%20a%20significant%20portion%20of%20the%0Afinancial%20services%20sector%20employs%20various%20machine%20learning%20algorithms%2C%20such%20as%0AXGBoost%2C%20Random%20Forest%2C%20and%20neural%20networks%2C%20to%20model%20transaction%20data.%20While%0Athese%20techniques%20have%20proven%20more%20efficient%20than%20rule-based%20methods%2C%20they%20still%0Afail%20to%20capture%20interactions%20between%20different%20transactions%20and%20their%0Ainterrelationships.%20Recently%2C%20graph-based%20techniques%20have%20been%20adopted%20for%0Afinancial%20fraud%20detection%2C%20leveraging%20graph%20topology%20to%20aggregate%20neighborhood%0Ainformation%20of%20transaction%20data%20using%20Graph%20Neural%20Networks%20%28GNNs%29.%20Despite%0Ashowing%20improvements%20over%20previous%20methods%2C%20these%20techniques%20still%20struggle%20to%0Akeep%20pace%20with%20the%20evolving%20camouflaging%20tactics%20of%20fraudsters%20and%20suffer%20from%0Ainformation%20loss%20due%20to%20over-smoothing.%20In%20this%20paper%2C%20we%20propose%20a%20novel%0Aalgorithm%20that%20employs%20an%20efficient%20neighborhood%20sampling%20method%2C%20effective%20for%0Acamouflage%20detection%20and%20preserving%20crucial%20feature%20information%20from%0Anon-similar%20nodes.%20Additionally%2C%20we%20introduce%20a%20novel%20GNN%20architecture%20that%0Autilizes%20attention%20mechanisms%20and%20preserves%20holistic%20neighborhood%20information%0Ato%20prevent%20information%20loss.%20We%20test%20our%20algorithm%20on%20financial%20data%20to%20show%0Athat%20our%20method%20outperforms%20other%20state-of-the-art%20graph%20algorithms.%0A&entry.1838667208=http%3A//arxiv.org/abs/2411.05857v2&entry.124074799=Read"},
{"title": "What Do GNNs Actually Learn? Towards Understanding their Representations", "author": "Giannis Nikolentzos and Michail Chatzianastasis and Michalis Vazirgiannis", "abstract": "  In recent years, graph neural networks (GNNs) have achieved great success in\nthe field of graph representation learning. Although prior work has shed light\non the expressiveness of those models (\\ie whether they can distinguish pairs\nof non-isomorphic graphs), it is still not clear what structural information is\nencoded into the node representations that are learned by those models. In this\npaper, we address this gap by studying the node representations learned by four\nstandard GNN models. We find that some models produce identical representations\nfor all nodes, while the representations learned by other models are linked to\nsome notion of walks of specific length that start from the nodes. We establish\nLipschitz bounds for these models with respect to the number of (normalized)\nwalks. Additionally, we investigate the influence of node features on the\nlearned representations. We find that if the initial representations of all\nnodes point in the same direction, the representations learned at the $k$-th\nlayer of the models are also related to the initial features of nodes that can\nbe reached in exactly $k$ steps. We also apply our findings to understand the\nphenomenon of oversquashing that occurs in GNNs. Our theoretical analysis is\nvalidated through experiments on synthetic and real-world datasets.\n", "link": "http://arxiv.org/abs/2304.10851v2", "date": "2024-11-22", "relevancy": 2.3215, "topK": [{"title": "Self-supervised Photographic Image Layout Representation Learning", "link": "http://arxiv.org/abs/2403.03740v1", "similarity": 0.5068}, {"title": "UGG: Unified Generative Grasping", "link": "https://arxiv.org/abs/2311.16917", "similarity": 0.4457}, {"title": "DDM-NET: End-to-end learning of keypoint feature Detection, Description and Matching for 3D localization", "link": "https://arxiv.org/abs/2212.04575", "similarity": 0.4403}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20What%20Do%20GNNs%20Actually%20Learn%3F%20Towards%20Understanding%20their%20Representations&body=Title%3A%20What%20Do%20GNNs%20Actually%20Learn%3F%20Towards%20Understanding%20their%20Representations%0AAuthor%3A%20Giannis%20Nikolentzos%20and%20Michail%20Chatzianastasis%20and%20Michalis%20Vazirgiannis%0AAbstract%3A%20%20%20In%20recent%20years%2C%20graph%20neural%20networks%20%28GNNs%29%20have%20achieved%20great%20success%20in%0Athe%20field%20of%20graph%20representation%20learning.%20Although%20prior%20work%20has%20shed%20light%0Aon%20the%20expressiveness%20of%20those%20models%20%28%5Cie%20whether%20they%20can%20distinguish%20pairs%0Aof%20non-isomorphic%20graphs%29%2C%20it%20is%20still%20not%20clear%20what%20structural%20information%20is%0Aencoded%20into%20the%20node%20representations%20that%20are%20learned%20by%20those%20models.%20In%20this%0Apaper%2C%20we%20address%20this%20gap%20by%20studying%20the%20node%20representations%20learned%20by%20four%0Astandard%20GNN%20models.%20We%20find%20that%20some%20models%20produce%20identical%20representations%0Afor%20all%20nodes%2C%20while%20the%20representations%20learned%20by%20other%20models%20are%20linked%20to%0Asome%20notion%20of%20walks%20of%20specific%20length%20that%20start%20from%20the%20nodes.%20We%20establish%0ALipschitz%20bounds%20for%20these%20models%20with%20respect%20to%20the%20number%20of%20%28normalized%29%0Awalks.%20Additionally%2C%20we%20investigate%20the%20influence%20of%20node%20features%20on%20the%0Alearned%20representations.%20We%20find%20that%20if%20the%20initial%20representations%20of%20all%0Anodes%20point%20in%20the%20same%20direction%2C%20the%20representations%20learned%20at%20the%20%24k%24-th%0Alayer%20of%20the%20models%20are%20also%20related%20to%20the%20initial%20features%20of%20nodes%20that%20can%0Abe%20reached%20in%20exactly%20%24k%24%20steps.%20We%20also%20apply%20our%20findings%20to%20understand%20the%0Aphenomenon%20of%20oversquashing%20that%20occurs%20in%20GNNs.%20Our%20theoretical%20analysis%20is%0Avalidated%20through%20experiments%20on%20synthetic%20and%20real-world%20datasets.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2304.10851v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DWhat%2520Do%2520GNNs%2520Actually%2520Learn%253F%2520Towards%2520Understanding%2520their%2520Representations%26entry.906535625%3DGiannis%2520Nikolentzos%2520and%2520Michail%2520Chatzianastasis%2520and%2520Michalis%2520Vazirgiannis%26entry.1292438233%3D%2520%2520In%2520recent%2520years%252C%2520graph%2520neural%2520networks%2520%2528GNNs%2529%2520have%2520achieved%2520great%2520success%2520in%250Athe%2520field%2520of%2520graph%2520representation%2520learning.%2520Although%2520prior%2520work%2520has%2520shed%2520light%250Aon%2520the%2520expressiveness%2520of%2520those%2520models%2520%2528%255Cie%2520whether%2520they%2520can%2520distinguish%2520pairs%250Aof%2520non-isomorphic%2520graphs%2529%252C%2520it%2520is%2520still%2520not%2520clear%2520what%2520structural%2520information%2520is%250Aencoded%2520into%2520the%2520node%2520representations%2520that%2520are%2520learned%2520by%2520those%2520models.%2520In%2520this%250Apaper%252C%2520we%2520address%2520this%2520gap%2520by%2520studying%2520the%2520node%2520representations%2520learned%2520by%2520four%250Astandard%2520GNN%2520models.%2520We%2520find%2520that%2520some%2520models%2520produce%2520identical%2520representations%250Afor%2520all%2520nodes%252C%2520while%2520the%2520representations%2520learned%2520by%2520other%2520models%2520are%2520linked%2520to%250Asome%2520notion%2520of%2520walks%2520of%2520specific%2520length%2520that%2520start%2520from%2520the%2520nodes.%2520We%2520establish%250ALipschitz%2520bounds%2520for%2520these%2520models%2520with%2520respect%2520to%2520the%2520number%2520of%2520%2528normalized%2529%250Awalks.%2520Additionally%252C%2520we%2520investigate%2520the%2520influence%2520of%2520node%2520features%2520on%2520the%250Alearned%2520representations.%2520We%2520find%2520that%2520if%2520the%2520initial%2520representations%2520of%2520all%250Anodes%2520point%2520in%2520the%2520same%2520direction%252C%2520the%2520representations%2520learned%2520at%2520the%2520%2524k%2524-th%250Alayer%2520of%2520the%2520models%2520are%2520also%2520related%2520to%2520the%2520initial%2520features%2520of%2520nodes%2520that%2520can%250Abe%2520reached%2520in%2520exactly%2520%2524k%2524%2520steps.%2520We%2520also%2520apply%2520our%2520findings%2520to%2520understand%2520the%250Aphenomenon%2520of%2520oversquashing%2520that%2520occurs%2520in%2520GNNs.%2520Our%2520theoretical%2520analysis%2520is%250Avalidated%2520through%2520experiments%2520on%2520synthetic%2520and%2520real-world%2520datasets.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2304.10851v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=What%20Do%20GNNs%20Actually%20Learn%3F%20Towards%20Understanding%20their%20Representations&entry.906535625=Giannis%20Nikolentzos%20and%20Michail%20Chatzianastasis%20and%20Michalis%20Vazirgiannis&entry.1292438233=%20%20In%20recent%20years%2C%20graph%20neural%20networks%20%28GNNs%29%20have%20achieved%20great%20success%20in%0Athe%20field%20of%20graph%20representation%20learning.%20Although%20prior%20work%20has%20shed%20light%0Aon%20the%20expressiveness%20of%20those%20models%20%28%5Cie%20whether%20they%20can%20distinguish%20pairs%0Aof%20non-isomorphic%20graphs%29%2C%20it%20is%20still%20not%20clear%20what%20structural%20information%20is%0Aencoded%20into%20the%20node%20representations%20that%20are%20learned%20by%20those%20models.%20In%20this%0Apaper%2C%20we%20address%20this%20gap%20by%20studying%20the%20node%20representations%20learned%20by%20four%0Astandard%20GNN%20models.%20We%20find%20that%20some%20models%20produce%20identical%20representations%0Afor%20all%20nodes%2C%20while%20the%20representations%20learned%20by%20other%20models%20are%20linked%20to%0Asome%20notion%20of%20walks%20of%20specific%20length%20that%20start%20from%20the%20nodes.%20We%20establish%0ALipschitz%20bounds%20for%20these%20models%20with%20respect%20to%20the%20number%20of%20%28normalized%29%0Awalks.%20Additionally%2C%20we%20investigate%20the%20influence%20of%20node%20features%20on%20the%0Alearned%20representations.%20We%20find%20that%20if%20the%20initial%20representations%20of%20all%0Anodes%20point%20in%20the%20same%20direction%2C%20the%20representations%20learned%20at%20the%20%24k%24-th%0Alayer%20of%20the%20models%20are%20also%20related%20to%20the%20initial%20features%20of%20nodes%20that%20can%0Abe%20reached%20in%20exactly%20%24k%24%20steps.%20We%20also%20apply%20our%20findings%20to%20understand%20the%0Aphenomenon%20of%20oversquashing%20that%20occurs%20in%20GNNs.%20Our%20theoretical%20analysis%20is%0Avalidated%20through%20experiments%20on%20synthetic%20and%20real-world%20datasets.%0A&entry.1838667208=http%3A//arxiv.org/abs/2304.10851v2&entry.124074799=Read"},
{"title": "HeadRouter: A Training-free Image Editing Framework for MM-DiTs by\n  Adaptively Routing Attention Heads", "author": "Yu Xu and Fan Tang and Juan Cao and Yuxin Zhang and Xiaoyu Kong and Jintao Li and Oliver Deussen and Tong-Yee Lee", "abstract": "  Diffusion Transformers (DiTs) have exhibited robust capabilities in image\ngeneration tasks. However, accurate text-guided image editing for multimodal\nDiTs (MM-DiTs) still poses a significant challenge. Unlike UNet-based\nstructures that could utilize self/cross-attention maps for semantic editing,\nMM-DiTs inherently lack support for explicit and consistent incorporated text\nguidance, resulting in semantic misalignment between the edited results and\ntexts. In this study, we disclose the sensitivity of different attention heads\nto different image semantics within MM-DiTs and introduce HeadRouter, a\ntraining-free image editing framework that edits the source image by adaptively\nrouting the text guidance to different attention heads in MM-DiTs. Furthermore,\nwe present a dual-token refinement module to refine text/image token\nrepresentations for precise semantic guidance and accurate region expression.\nExperimental results on multiple benchmarks demonstrate HeadRouter's\nperformance in terms of editing fidelity and image quality.\n", "link": "http://arxiv.org/abs/2411.15034v1", "date": "2024-11-22", "relevancy": 2.3131, "topK": [{"title": "Scaling Rectified Flow Transformers for High-Resolution Image Synthesis", "link": "http://arxiv.org/abs/2403.03206v1", "similarity": 0.596}, {"title": "ToDo: Token Downsampling for Efficient Generation of High-Resolution\n  Images", "link": "http://arxiv.org/abs/2402.13573v2", "similarity": 0.5855}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.564}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20HeadRouter%3A%20A%20Training-free%20Image%20Editing%20Framework%20for%20MM-DiTs%20by%0A%20%20Adaptively%20Routing%20Attention%20Heads&body=Title%3A%20HeadRouter%3A%20A%20Training-free%20Image%20Editing%20Framework%20for%20MM-DiTs%20by%0A%20%20Adaptively%20Routing%20Attention%20Heads%0AAuthor%3A%20Yu%20Xu%20and%20Fan%20Tang%20and%20Juan%20Cao%20and%20Yuxin%20Zhang%20and%20Xiaoyu%20Kong%20and%20Jintao%20Li%20and%20Oliver%20Deussen%20and%20Tong-Yee%20Lee%0AAbstract%3A%20%20%20Diffusion%20Transformers%20%28DiTs%29%20have%20exhibited%20robust%20capabilities%20in%20image%0Ageneration%20tasks.%20However%2C%20accurate%20text-guided%20image%20editing%20for%20multimodal%0ADiTs%20%28MM-DiTs%29%20still%20poses%20a%20significant%20challenge.%20Unlike%20UNet-based%0Astructures%20that%20could%20utilize%20self/cross-attention%20maps%20for%20semantic%20editing%2C%0AMM-DiTs%20inherently%20lack%20support%20for%20explicit%20and%20consistent%20incorporated%20text%0Aguidance%2C%20resulting%20in%20semantic%20misalignment%20between%20the%20edited%20results%20and%0Atexts.%20In%20this%20study%2C%20we%20disclose%20the%20sensitivity%20of%20different%20attention%20heads%0Ato%20different%20image%20semantics%20within%20MM-DiTs%20and%20introduce%20HeadRouter%2C%20a%0Atraining-free%20image%20editing%20framework%20that%20edits%20the%20source%20image%20by%20adaptively%0Arouting%20the%20text%20guidance%20to%20different%20attention%20heads%20in%20MM-DiTs.%20Furthermore%2C%0Awe%20present%20a%20dual-token%20refinement%20module%20to%20refine%20text/image%20token%0Arepresentations%20for%20precise%20semantic%20guidance%20and%20accurate%20region%20expression.%0AExperimental%20results%20on%20multiple%20benchmarks%20demonstrate%20HeadRouter%27s%0Aperformance%20in%20terms%20of%20editing%20fidelity%20and%20image%20quality.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2411.15034v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DHeadRouter%253A%2520A%2520Training-free%2520Image%2520Editing%2520Framework%2520for%2520MM-DiTs%2520by%250A%2520%2520Adaptively%2520Routing%2520Attention%2520Heads%26entry.906535625%3DYu%2520Xu%2520and%2520Fan%2520Tang%2520and%2520Juan%2520Cao%2520and%2520Yuxin%2520Zhang%2520and%2520Xiaoyu%2520Kong%2520and%2520Jintao%2520Li%2520and%2520Oliver%2520Deussen%2520and%2520Tong-Yee%2520Lee%26entry.1292438233%3D%2520%2520Diffusion%2520Transformers%2520%2528DiTs%2529%2520have%2520exhibited%2520robust%2520capabilities%2520in%2520image%250Ageneration%2520tasks.%2520However%252C%2520accurate%2520text-guided%2520image%2520editing%2520for%2520multimodal%250ADiTs%2520%2528MM-DiTs%2529%2520still%2520poses%2520a%2520significant%2520challenge.%2520Unlike%2520UNet-based%250Astructures%2520that%2520could%2520utilize%2520self/cross-attention%2520maps%2520for%2520semantic%2520editing%252C%250AMM-DiTs%2520inherently%2520lack%2520support%2520for%2520explicit%2520and%2520consistent%2520incorporated%2520text%250Aguidance%252C%2520resulting%2520in%2520semantic%2520misalignment%2520between%2520the%2520edited%2520results%2520and%250Atexts.%2520In%2520this%2520study%252C%2520we%2520disclose%2520the%2520sensitivity%2520of%2520different%2520attention%2520heads%250Ato%2520different%2520image%2520semantics%2520within%2520MM-DiTs%2520and%2520introduce%2520HeadRouter%252C%2520a%250Atraining-free%2520image%2520editing%2520framework%2520that%2520edits%2520the%2520source%2520image%2520by%2520adaptively%250Arouting%2520the%2520text%2520guidance%2520to%2520different%2520attention%2520heads%2520in%2520MM-DiTs.%2520Furthermore%252C%250Awe%2520present%2520a%2520dual-token%2520refinement%2520module%2520to%2520refine%2520text/image%2520token%250Arepresentations%2520for%2520precise%2520semantic%2520guidance%2520and%2520accurate%2520region%2520expression.%250AExperimental%2520results%2520on%2520multiple%2520benchmarks%2520demonstrate%2520HeadRouter%2527s%250Aperformance%2520in%2520terms%2520of%2520editing%2520fidelity%2520and%2520image%2520quality.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2411.15034v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=HeadRouter%3A%20A%20Training-free%20Image%20Editing%20Framework%20for%20MM-DiTs%20by%0A%20%20Adaptively%20Routing%20Attention%20Heads&entry.906535625=Yu%20Xu%20and%20Fan%20Tang%20and%20Juan%20Cao%20and%20Yuxin%20Zhang%20and%20Xiaoyu%20Kong%20and%20Jintao%20Li%20and%20Oliver%20Deussen%20and%20Tong-Yee%20Lee&entry.1292438233=%20%20Diffusion%20Transformers%20%28DiTs%29%20have%20exhibited%20robust%20capabilities%20in%20image%0Ageneration%20tasks.%20However%2C%20accurate%20text-guided%20image%20editing%20for%20multimodal%0ADiTs%20%28MM-DiTs%29%20still%20poses%20a%20significant%20challenge.%20Unlike%20UNet-based%0Astructures%20that%20could%20utilize%20self/cross-attention%20maps%20for%20semantic%20editing%2C%0AMM-DiTs%20inherently%20lack%20support%20for%20explicit%20and%20consistent%20incorporated%20text%0Aguidance%2C%20resulting%20in%20semantic%20misalignment%20between%20the%20edited%20results%20and%0Atexts.%20In%20this%20study%2C%20we%20disclose%20the%20sensitivity%20of%20different%20attention%20heads%0Ato%20different%20image%20semantics%20within%20MM-DiTs%20and%20introduce%20HeadRouter%2C%20a%0Atraining-free%20image%20editing%20framework%20that%20edits%20the%20source%20image%20by%20adaptively%0Arouting%20the%20text%20guidance%20to%20different%20attention%20heads%20in%20MM-DiTs.%20Furthermore%2C%0Awe%20present%20a%20dual-token%20refinement%20module%20to%20refine%20text/image%20token%0Arepresentations%20for%20precise%20semantic%20guidance%20and%20accurate%20region%20expression.%0AExperimental%20results%20on%20multiple%20benchmarks%20demonstrate%20HeadRouter%27s%0Aperformance%20in%20terms%20of%20editing%20fidelity%20and%20image%20quality.%0A&entry.1838667208=http%3A//arxiv.org/abs/2411.15034v1&entry.124074799=Read"},
{"title": "ArcSin: Adaptive ranged cosine Similarity injected noise for\n  Language-Driven Visual Tasks", "author": "Yang Liu and Xiaomin Yu and Gongyu Zhang and Zhen Zhu and Christos Bergeles and Prokar Dasgupta and Alejandro Granados and Sebastien Ourselin", "abstract": "  \"A data scientist is tasked with developing a low-cost surgical VQA system\nfor a 2-month workshop. Due to data sensitivity, she collects 50 hours of\nsurgical video from a hospital, requiring two months for privacy approvals.\nPrivacy restrictions prevent uploading data to platforms like ChatGPT, so she\nassembles one annotator and a medical expert to manually create QA pairs. This\nprocess takes three weeks and costs over $10,000. The trained model provides\naccurate responses within the limited data scope but lacks broader\ngeneralizability, completing the project in 3 months.\"\n  To simplify the challenges presented in the scenario above. In this paper, we\nreplace the image input with text for Vision-language training. Inspired by\nprior noise injection methods to reduce modality gaps, we introduce Adaptive\nranged cosine Similarity injected noise (ArcSin). First, we introduce an\ninnovative adaptive noise scale that effectively generates the textual elements\nwith more variability while preserving the original text feature's integrity.\nSecond, a similarity pool strategy is employed, expanding the domain\ngeneralization potential by broadening the overall noise scale. This dual\nstrategy effectively broadens the scope of the original domain while\nsafeguarding content integrity. Our empirical results demonstrate that these\nmodels closely rival those trained on images in terms of performance.\nSpecifically, our method exhibits substantial improvements over the previous\nstate-of-the-art, achieving gains of 1.9 and 1.1 CIDEr points in S-Cap and\nM-Cap, respectively. Additionally, we observe increases of 0.5 percentage\npoints (pp), 1.4 pp, and 1.4 pp in accuracy for VQA, VQA-E, and VE,\nrespectively, pushing the boundaries of what is achievable within the\nconstraints of image-trained model benchmarks.\n", "link": "http://arxiv.org/abs/2402.17298v2", "date": "2024-11-22", "relevancy": 2.2877, "topK": [{"title": "Tuning-Free Noise Rectification for High Fidelity Image-to-Video\n  Generation", "link": "http://arxiv.org/abs/2403.02827v1", "similarity": 0.5759}, {"title": "Magic-Me: Identity-Specific Video Customized Diffusion", "link": "http://arxiv.org/abs/2402.09368v1", "similarity": 0.5715}, {"title": "Scaling Rectified Flow Transformers for High-Resolution Image Synthesis", "link": "http://arxiv.org/abs/2403.03206v1", "similarity": 0.563}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20ArcSin%3A%20Adaptive%20ranged%20cosine%20Similarity%20injected%20noise%20for%0A%20%20Language-Driven%20Visual%20Tasks&body=Title%3A%20ArcSin%3A%20Adaptive%20ranged%20cosine%20Similarity%20injected%20noise%20for%0A%20%20Language-Driven%20Visual%20Tasks%0AAuthor%3A%20Yang%20Liu%20and%20Xiaomin%20Yu%20and%20Gongyu%20Zhang%20and%20Zhen%20Zhu%20and%20Christos%20Bergeles%20and%20Prokar%20Dasgupta%20and%20Alejandro%20Granados%20and%20Sebastien%20Ourselin%0AAbstract%3A%20%20%20%22A%20data%20scientist%20is%20tasked%20with%20developing%20a%20low-cost%20surgical%20VQA%20system%0Afor%20a%202-month%20workshop.%20Due%20to%20data%20sensitivity%2C%20she%20collects%2050%20hours%20of%0Asurgical%20video%20from%20a%20hospital%2C%20requiring%20two%20months%20for%20privacy%20approvals.%0APrivacy%20restrictions%20prevent%20uploading%20data%20to%20platforms%20like%20ChatGPT%2C%20so%20she%0Aassembles%20one%20annotator%20and%20a%20medical%20expert%20to%20manually%20create%20QA%20pairs.%20This%0Aprocess%20takes%20three%20weeks%20and%20costs%20over%20%2410%2C000.%20The%20trained%20model%20provides%0Aaccurate%20responses%20within%20the%20limited%20data%20scope%20but%20lacks%20broader%0Ageneralizability%2C%20completing%20the%20project%20in%203%20months.%22%0A%20%20To%20simplify%20the%20challenges%20presented%20in%20the%20scenario%20above.%20In%20this%20paper%2C%20we%0Areplace%20the%20image%20input%20with%20text%20for%20Vision-language%20training.%20Inspired%20by%0Aprior%20noise%20injection%20methods%20to%20reduce%20modality%20gaps%2C%20we%20introduce%20Adaptive%0Aranged%20cosine%20Similarity%20injected%20noise%20%28ArcSin%29.%20First%2C%20we%20introduce%20an%0Ainnovative%20adaptive%20noise%20scale%20that%20effectively%20generates%20the%20textual%20elements%0Awith%20more%20variability%20while%20preserving%20the%20original%20text%20feature%27s%20integrity.%0ASecond%2C%20a%20similarity%20pool%20strategy%20is%20employed%2C%20expanding%20the%20domain%0Ageneralization%20potential%20by%20broadening%20the%20overall%20noise%20scale.%20This%20dual%0Astrategy%20effectively%20broadens%20the%20scope%20of%20the%20original%20domain%20while%0Asafeguarding%20content%20integrity.%20Our%20empirical%20results%20demonstrate%20that%20these%0Amodels%20closely%20rival%20those%20trained%20on%20images%20in%20terms%20of%20performance.%0ASpecifically%2C%20our%20method%20exhibits%20substantial%20improvements%20over%20the%20previous%0Astate-of-the-art%2C%20achieving%20gains%20of%201.9%20and%201.1%20CIDEr%20points%20in%20S-Cap%20and%0AM-Cap%2C%20respectively.%20Additionally%2C%20we%20observe%20increases%20of%200.5%20percentage%0Apoints%20%28pp%29%2C%201.4%20pp%2C%20and%201.4%20pp%20in%20accuracy%20for%20VQA%2C%20VQA-E%2C%20and%20VE%2C%0Arespectively%2C%20pushing%20the%20boundaries%20of%20what%20is%20achievable%20within%20the%0Aconstraints%20of%20image-trained%20model%20benchmarks.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2402.17298v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DArcSin%253A%2520Adaptive%2520ranged%2520cosine%2520Similarity%2520injected%2520noise%2520for%250A%2520%2520Language-Driven%2520Visual%2520Tasks%26entry.906535625%3DYang%2520Liu%2520and%2520Xiaomin%2520Yu%2520and%2520Gongyu%2520Zhang%2520and%2520Zhen%2520Zhu%2520and%2520Christos%2520Bergeles%2520and%2520Prokar%2520Dasgupta%2520and%2520Alejandro%2520Granados%2520and%2520Sebastien%2520Ourselin%26entry.1292438233%3D%2520%2520%2522A%2520data%2520scientist%2520is%2520tasked%2520with%2520developing%2520a%2520low-cost%2520surgical%2520VQA%2520system%250Afor%2520a%25202-month%2520workshop.%2520Due%2520to%2520data%2520sensitivity%252C%2520she%2520collects%252050%2520hours%2520of%250Asurgical%2520video%2520from%2520a%2520hospital%252C%2520requiring%2520two%2520months%2520for%2520privacy%2520approvals.%250APrivacy%2520restrictions%2520prevent%2520uploading%2520data%2520to%2520platforms%2520like%2520ChatGPT%252C%2520so%2520she%250Aassembles%2520one%2520annotator%2520and%2520a%2520medical%2520expert%2520to%2520manually%2520create%2520QA%2520pairs.%2520This%250Aprocess%2520takes%2520three%2520weeks%2520and%2520costs%2520over%2520%252410%252C000.%2520The%2520trained%2520model%2520provides%250Aaccurate%2520responses%2520within%2520the%2520limited%2520data%2520scope%2520but%2520lacks%2520broader%250Ageneralizability%252C%2520completing%2520the%2520project%2520in%25203%2520months.%2522%250A%2520%2520To%2520simplify%2520the%2520challenges%2520presented%2520in%2520the%2520scenario%2520above.%2520In%2520this%2520paper%252C%2520we%250Areplace%2520the%2520image%2520input%2520with%2520text%2520for%2520Vision-language%2520training.%2520Inspired%2520by%250Aprior%2520noise%2520injection%2520methods%2520to%2520reduce%2520modality%2520gaps%252C%2520we%2520introduce%2520Adaptive%250Aranged%2520cosine%2520Similarity%2520injected%2520noise%2520%2528ArcSin%2529.%2520First%252C%2520we%2520introduce%2520an%250Ainnovative%2520adaptive%2520noise%2520scale%2520that%2520effectively%2520generates%2520the%2520textual%2520elements%250Awith%2520more%2520variability%2520while%2520preserving%2520the%2520original%2520text%2520feature%2527s%2520integrity.%250ASecond%252C%2520a%2520similarity%2520pool%2520strategy%2520is%2520employed%252C%2520expanding%2520the%2520domain%250Ageneralization%2520potential%2520by%2520broadening%2520the%2520overall%2520noise%2520scale.%2520This%2520dual%250Astrategy%2520effectively%2520broadens%2520the%2520scope%2520of%2520the%2520original%2520domain%2520while%250Asafeguarding%2520content%2520integrity.%2520Our%2520empirical%2520results%2520demonstrate%2520that%2520these%250Amodels%2520closely%2520rival%2520those%2520trained%2520on%2520images%2520in%2520terms%2520of%2520performance.%250ASpecifically%252C%2520our%2520method%2520exhibits%2520substantial%2520improvements%2520over%2520the%2520previous%250Astate-of-the-art%252C%2520achieving%2520gains%2520of%25201.9%2520and%25201.1%2520CIDEr%2520points%2520in%2520S-Cap%2520and%250AM-Cap%252C%2520respectively.%2520Additionally%252C%2520we%2520observe%2520increases%2520of%25200.5%2520percentage%250Apoints%2520%2528pp%2529%252C%25201.4%2520pp%252C%2520and%25201.4%2520pp%2520in%2520accuracy%2520for%2520VQA%252C%2520VQA-E%252C%2520and%2520VE%252C%250Arespectively%252C%2520pushing%2520the%2520boundaries%2520of%2520what%2520is%2520achievable%2520within%2520the%250Aconstraints%2520of%2520image-trained%2520model%2520benchmarks.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2402.17298v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=ArcSin%3A%20Adaptive%20ranged%20cosine%20Similarity%20injected%20noise%20for%0A%20%20Language-Driven%20Visual%20Tasks&entry.906535625=Yang%20Liu%20and%20Xiaomin%20Yu%20and%20Gongyu%20Zhang%20and%20Zhen%20Zhu%20and%20Christos%20Bergeles%20and%20Prokar%20Dasgupta%20and%20Alejandro%20Granados%20and%20Sebastien%20Ourselin&entry.1292438233=%20%20%22A%20data%20scientist%20is%20tasked%20with%20developing%20a%20low-cost%20surgical%20VQA%20system%0Afor%20a%202-month%20workshop.%20Due%20to%20data%20sensitivity%2C%20she%20collects%2050%20hours%20of%0Asurgical%20video%20from%20a%20hospital%2C%20requiring%20two%20months%20for%20privacy%20approvals.%0APrivacy%20restrictions%20prevent%20uploading%20data%20to%20platforms%20like%20ChatGPT%2C%20so%20she%0Aassembles%20one%20annotator%20and%20a%20medical%20expert%20to%20manually%20create%20QA%20pairs.%20This%0Aprocess%20takes%20three%20weeks%20and%20costs%20over%20%2410%2C000.%20The%20trained%20model%20provides%0Aaccurate%20responses%20within%20the%20limited%20data%20scope%20but%20lacks%20broader%0Ageneralizability%2C%20completing%20the%20project%20in%203%20months.%22%0A%20%20To%20simplify%20the%20challenges%20presented%20in%20the%20scenario%20above.%20In%20this%20paper%2C%20we%0Areplace%20the%20image%20input%20with%20text%20for%20Vision-language%20training.%20Inspired%20by%0Aprior%20noise%20injection%20methods%20to%20reduce%20modality%20gaps%2C%20we%20introduce%20Adaptive%0Aranged%20cosine%20Similarity%20injected%20noise%20%28ArcSin%29.%20First%2C%20we%20introduce%20an%0Ainnovative%20adaptive%20noise%20scale%20that%20effectively%20generates%20the%20textual%20elements%0Awith%20more%20variability%20while%20preserving%20the%20original%20text%20feature%27s%20integrity.%0ASecond%2C%20a%20similarity%20pool%20strategy%20is%20employed%2C%20expanding%20the%20domain%0Ageneralization%20potential%20by%20broadening%20the%20overall%20noise%20scale.%20This%20dual%0Astrategy%20effectively%20broadens%20the%20scope%20of%20the%20original%20domain%20while%0Asafeguarding%20content%20integrity.%20Our%20empirical%20results%20demonstrate%20that%20these%0Amodels%20closely%20rival%20those%20trained%20on%20images%20in%20terms%20of%20performance.%0ASpecifically%2C%20our%20method%20exhibits%20substantial%20improvements%20over%20the%20previous%0Astate-of-the-art%2C%20achieving%20gains%20of%201.9%20and%201.1%20CIDEr%20points%20in%20S-Cap%20and%0AM-Cap%2C%20respectively.%20Additionally%2C%20we%20observe%20increases%20of%200.5%20percentage%0Apoints%20%28pp%29%2C%201.4%20pp%2C%20and%201.4%20pp%20in%20accuracy%20for%20VQA%2C%20VQA-E%2C%20and%20VE%2C%0Arespectively%2C%20pushing%20the%20boundaries%20of%20what%20is%20achievable%20within%20the%0Aconstraints%20of%20image-trained%20model%20benchmarks.%0A&entry.1838667208=http%3A//arxiv.org/abs/2402.17298v2&entry.124074799=Read"},
{"title": "How Sparse Can We Prune A Deep Network: A Fundamental Limit Viewpoint", "author": "Qiaozhe Zhang and Ruijie Zhang and Jun Sun and Yingzhuang Liu", "abstract": "  Network pruning is a commonly used measure to alleviate the storage and\ncomputational burden of deep neural networks. However, the fundamental limit of\nnetwork pruning is still lacking. To close the gap, in this work we'll take a\nfirst-principles approach, i.e. we'll directly impose the sparsity constraint\non the loss function and leverage the framework of statistical dimension in\nconvex geometry, thus we're able to characterize the sharp phase transition\npoint, i.e. the fundamental limit of the pruning ratio. Through this limit,\nwe're able to identify two key factors that determine the pruning ratio limit,\nnamely, weight magnitude and network sharpness. Generally speaking, the flatter\nthe loss landscape or the smaller the weight magnitude, the smaller pruning\nratio. Moreover, we provide efficient countermeasures to address the challenges\nin the computation of the pruning limit, which involves accurate spectrum\nestimation of a large-scale and non-positive Hessian matrix. Moreover, through\nthe lens of the pruning ratio threshold, we can provide rigorous\ninterpretations on several heuristics in existing pruning algorithms. Extensive\nexperiments are performed that demonstrate that our theoretical pruning ratio\nthreshold coincides very well with the experiments. All codes are available at:\nhttps://github.com/QiaozheZhang/Global-One-shot-Pruning\n", "link": "http://arxiv.org/abs/2306.05857v3", "date": "2024-11-22", "relevancy": 2.259, "topK": [{"title": "ToDo: Token Downsampling for Efficient Generation of High-Resolution\n  Images", "link": "http://arxiv.org/abs/2402.13573v2", "similarity": 0.4683}, {"title": "Self-supervised Photographic Image Layout Representation Learning", "link": "http://arxiv.org/abs/2403.03740v1", "similarity": 0.4443}, {"title": "Boosted dynamic neural networks", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/26302", "similarity": 0.4429}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20How%20Sparse%20Can%20We%20Prune%20A%20Deep%20Network%3A%20A%20Fundamental%20Limit%20Viewpoint&body=Title%3A%20How%20Sparse%20Can%20We%20Prune%20A%20Deep%20Network%3A%20A%20Fundamental%20Limit%20Viewpoint%0AAuthor%3A%20Qiaozhe%20Zhang%20and%20Ruijie%20Zhang%20and%20Jun%20Sun%20and%20Yingzhuang%20Liu%0AAbstract%3A%20%20%20Network%20pruning%20is%20a%20commonly%20used%20measure%20to%20alleviate%20the%20storage%20and%0Acomputational%20burden%20of%20deep%20neural%20networks.%20However%2C%20the%20fundamental%20limit%20of%0Anetwork%20pruning%20is%20still%20lacking.%20To%20close%20the%20gap%2C%20in%20this%20work%20we%27ll%20take%20a%0Afirst-principles%20approach%2C%20i.e.%20we%27ll%20directly%20impose%20the%20sparsity%20constraint%0Aon%20the%20loss%20function%20and%20leverage%20the%20framework%20of%20statistical%20dimension%20in%0Aconvex%20geometry%2C%20thus%20we%27re%20able%20to%20characterize%20the%20sharp%20phase%20transition%0Apoint%2C%20i.e.%20the%20fundamental%20limit%20of%20the%20pruning%20ratio.%20Through%20this%20limit%2C%0Awe%27re%20able%20to%20identify%20two%20key%20factors%20that%20determine%20the%20pruning%20ratio%20limit%2C%0Anamely%2C%20weight%20magnitude%20and%20network%20sharpness.%20Generally%20speaking%2C%20the%20flatter%0Athe%20loss%20landscape%20or%20the%20smaller%20the%20weight%20magnitude%2C%20the%20smaller%20pruning%0Aratio.%20Moreover%2C%20we%20provide%20efficient%20countermeasures%20to%20address%20the%20challenges%0Ain%20the%20computation%20of%20the%20pruning%20limit%2C%20which%20involves%20accurate%20spectrum%0Aestimation%20of%20a%20large-scale%20and%20non-positive%20Hessian%20matrix.%20Moreover%2C%20through%0Athe%20lens%20of%20the%20pruning%20ratio%20threshold%2C%20we%20can%20provide%20rigorous%0Ainterpretations%20on%20several%20heuristics%20in%20existing%20pruning%20algorithms.%20Extensive%0Aexperiments%20are%20performed%20that%20demonstrate%20that%20our%20theoretical%20pruning%20ratio%0Athreshold%20coincides%20very%20well%20with%20the%20experiments.%20All%20codes%20are%20available%20at%3A%0Ahttps%3A//github.com/QiaozheZhang/Global-One-shot-Pruning%0A%0ALink%3A%20http%3A//arxiv.org/abs/2306.05857v3%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DHow%2520Sparse%2520Can%2520We%2520Prune%2520A%2520Deep%2520Network%253A%2520A%2520Fundamental%2520Limit%2520Viewpoint%26entry.906535625%3DQiaozhe%2520Zhang%2520and%2520Ruijie%2520Zhang%2520and%2520Jun%2520Sun%2520and%2520Yingzhuang%2520Liu%26entry.1292438233%3D%2520%2520Network%2520pruning%2520is%2520a%2520commonly%2520used%2520measure%2520to%2520alleviate%2520the%2520storage%2520and%250Acomputational%2520burden%2520of%2520deep%2520neural%2520networks.%2520However%252C%2520the%2520fundamental%2520limit%2520of%250Anetwork%2520pruning%2520is%2520still%2520lacking.%2520To%2520close%2520the%2520gap%252C%2520in%2520this%2520work%2520we%2527ll%2520take%2520a%250Afirst-principles%2520approach%252C%2520i.e.%2520we%2527ll%2520directly%2520impose%2520the%2520sparsity%2520constraint%250Aon%2520the%2520loss%2520function%2520and%2520leverage%2520the%2520framework%2520of%2520statistical%2520dimension%2520in%250Aconvex%2520geometry%252C%2520thus%2520we%2527re%2520able%2520to%2520characterize%2520the%2520sharp%2520phase%2520transition%250Apoint%252C%2520i.e.%2520the%2520fundamental%2520limit%2520of%2520the%2520pruning%2520ratio.%2520Through%2520this%2520limit%252C%250Awe%2527re%2520able%2520to%2520identify%2520two%2520key%2520factors%2520that%2520determine%2520the%2520pruning%2520ratio%2520limit%252C%250Anamely%252C%2520weight%2520magnitude%2520and%2520network%2520sharpness.%2520Generally%2520speaking%252C%2520the%2520flatter%250Athe%2520loss%2520landscape%2520or%2520the%2520smaller%2520the%2520weight%2520magnitude%252C%2520the%2520smaller%2520pruning%250Aratio.%2520Moreover%252C%2520we%2520provide%2520efficient%2520countermeasures%2520to%2520address%2520the%2520challenges%250Ain%2520the%2520computation%2520of%2520the%2520pruning%2520limit%252C%2520which%2520involves%2520accurate%2520spectrum%250Aestimation%2520of%2520a%2520large-scale%2520and%2520non-positive%2520Hessian%2520matrix.%2520Moreover%252C%2520through%250Athe%2520lens%2520of%2520the%2520pruning%2520ratio%2520threshold%252C%2520we%2520can%2520provide%2520rigorous%250Ainterpretations%2520on%2520several%2520heuristics%2520in%2520existing%2520pruning%2520algorithms.%2520Extensive%250Aexperiments%2520are%2520performed%2520that%2520demonstrate%2520that%2520our%2520theoretical%2520pruning%2520ratio%250Athreshold%2520coincides%2520very%2520well%2520with%2520the%2520experiments.%2520All%2520codes%2520are%2520available%2520at%253A%250Ahttps%253A//github.com/QiaozheZhang/Global-One-shot-Pruning%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2306.05857v3%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=How%20Sparse%20Can%20We%20Prune%20A%20Deep%20Network%3A%20A%20Fundamental%20Limit%20Viewpoint&entry.906535625=Qiaozhe%20Zhang%20and%20Ruijie%20Zhang%20and%20Jun%20Sun%20and%20Yingzhuang%20Liu&entry.1292438233=%20%20Network%20pruning%20is%20a%20commonly%20used%20measure%20to%20alleviate%20the%20storage%20and%0Acomputational%20burden%20of%20deep%20neural%20networks.%20However%2C%20the%20fundamental%20limit%20of%0Anetwork%20pruning%20is%20still%20lacking.%20To%20close%20the%20gap%2C%20in%20this%20work%20we%27ll%20take%20a%0Afirst-principles%20approach%2C%20i.e.%20we%27ll%20directly%20impose%20the%20sparsity%20constraint%0Aon%20the%20loss%20function%20and%20leverage%20the%20framework%20of%20statistical%20dimension%20in%0Aconvex%20geometry%2C%20thus%20we%27re%20able%20to%20characterize%20the%20sharp%20phase%20transition%0Apoint%2C%20i.e.%20the%20fundamental%20limit%20of%20the%20pruning%20ratio.%20Through%20this%20limit%2C%0Awe%27re%20able%20to%20identify%20two%20key%20factors%20that%20determine%20the%20pruning%20ratio%20limit%2C%0Anamely%2C%20weight%20magnitude%20and%20network%20sharpness.%20Generally%20speaking%2C%20the%20flatter%0Athe%20loss%20landscape%20or%20the%20smaller%20the%20weight%20magnitude%2C%20the%20smaller%20pruning%0Aratio.%20Moreover%2C%20we%20provide%20efficient%20countermeasures%20to%20address%20the%20challenges%0Ain%20the%20computation%20of%20the%20pruning%20limit%2C%20which%20involves%20accurate%20spectrum%0Aestimation%20of%20a%20large-scale%20and%20non-positive%20Hessian%20matrix.%20Moreover%2C%20through%0Athe%20lens%20of%20the%20pruning%20ratio%20threshold%2C%20we%20can%20provide%20rigorous%0Ainterpretations%20on%20several%20heuristics%20in%20existing%20pruning%20algorithms.%20Extensive%0Aexperiments%20are%20performed%20that%20demonstrate%20that%20our%20theoretical%20pruning%20ratio%0Athreshold%20coincides%20very%20well%20with%20the%20experiments.%20All%20codes%20are%20available%20at%3A%0Ahttps%3A//github.com/QiaozheZhang/Global-One-shot-Pruning%0A&entry.1838667208=http%3A//arxiv.org/abs/2306.05857v3&entry.124074799=Read"},
{"title": "LoRA-FAIR: Federated LoRA Fine-Tuning with Aggregation and\n  Initialization Refinement", "author": "Jieming Bian and Lei Wang and Letian Zhang and Jie Xu", "abstract": "  Foundation models (FMs) achieve strong performance across diverse tasks with\ntask-specific fine-tuning, yet full parameter fine-tuning is often\ncomputationally prohibitive for large models. Parameter-efficient fine-tuning\n(PEFT) methods like Low-Rank Adaptation (LoRA) reduce this cost by introducing\nlow-rank matrices for tuning fewer parameters. While LoRA allows for efficient\nfine-tuning, it requires significant data for adaptation, making Federated\nLearning (FL) an appealing solution due to its privacy-preserving collaborative\nframework. However, combining LoRA with FL introduces two key challenges: the\n\\textbf{Server-Side LoRA Aggregation Bias}, where server-side averaging of LoRA\nmatrices diverges from the ideal global update, and the \\textbf{Client-Side\nLoRA Initialization Drift}, emphasizing the need for consistent initialization\nacross rounds. Existing approaches address these challenges individually,\nlimiting their effectiveness. We propose LoRA-FAIR, a novel method that tackles\nboth issues by introducing a correction term on the server while keeping the\noriginal LoRA modules, enhancing aggregation efficiency and accuracy. LoRA-FAIR\nmaintains computational and communication efficiency, yielding superior\nperformance over state-of-the-art methods. Experimental results on ViT and\nMLP-Mixer models across large-scale datasets demonstrate that LoRA-FAIR\nconsistently achieves performance improvements in FL settings.\n", "link": "http://arxiv.org/abs/2411.14961v1", "date": "2024-11-22", "relevancy": 2.2548, "topK": [{"title": "Tuning-Free Noise Rectification for High Fidelity Image-to-Video\n  Generation", "link": "http://arxiv.org/abs/2403.02827v1", "similarity": 0.4553}, {"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.4543}, {"title": "ToDo: Token Downsampling for Efficient Generation of High-Resolution\n  Images", "link": "http://arxiv.org/abs/2402.13573v2", "similarity": 0.4433}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20LoRA-FAIR%3A%20Federated%20LoRA%20Fine-Tuning%20with%20Aggregation%20and%0A%20%20Initialization%20Refinement&body=Title%3A%20LoRA-FAIR%3A%20Federated%20LoRA%20Fine-Tuning%20with%20Aggregation%20and%0A%20%20Initialization%20Refinement%0AAuthor%3A%20Jieming%20Bian%20and%20Lei%20Wang%20and%20Letian%20Zhang%20and%20Jie%20Xu%0AAbstract%3A%20%20%20Foundation%20models%20%28FMs%29%20achieve%20strong%20performance%20across%20diverse%20tasks%20with%0Atask-specific%20fine-tuning%2C%20yet%20full%20parameter%20fine-tuning%20is%20often%0Acomputationally%20prohibitive%20for%20large%20models.%20Parameter-efficient%20fine-tuning%0A%28PEFT%29%20methods%20like%20Low-Rank%20Adaptation%20%28LoRA%29%20reduce%20this%20cost%20by%20introducing%0Alow-rank%20matrices%20for%20tuning%20fewer%20parameters.%20While%20LoRA%20allows%20for%20efficient%0Afine-tuning%2C%20it%20requires%20significant%20data%20for%20adaptation%2C%20making%20Federated%0ALearning%20%28FL%29%20an%20appealing%20solution%20due%20to%20its%20privacy-preserving%20collaborative%0Aframework.%20However%2C%20combining%20LoRA%20with%20FL%20introduces%20two%20key%20challenges%3A%20the%0A%5Ctextbf%7BServer-Side%20LoRA%20Aggregation%20Bias%7D%2C%20where%20server-side%20averaging%20of%20LoRA%0Amatrices%20diverges%20from%20the%20ideal%20global%20update%2C%20and%20the%20%5Ctextbf%7BClient-Side%0ALoRA%20Initialization%20Drift%7D%2C%20emphasizing%20the%20need%20for%20consistent%20initialization%0Aacross%20rounds.%20Existing%20approaches%20address%20these%20challenges%20individually%2C%0Alimiting%20their%20effectiveness.%20We%20propose%20LoRA-FAIR%2C%20a%20novel%20method%20that%20tackles%0Aboth%20issues%20by%20introducing%20a%20correction%20term%20on%20the%20server%20while%20keeping%20the%0Aoriginal%20LoRA%20modules%2C%20enhancing%20aggregation%20efficiency%20and%20accuracy.%20LoRA-FAIR%0Amaintains%20computational%20and%20communication%20efficiency%2C%20yielding%20superior%0Aperformance%20over%20state-of-the-art%20methods.%20Experimental%20results%20on%20ViT%20and%0AMLP-Mixer%20models%20across%20large-scale%20datasets%20demonstrate%20that%20LoRA-FAIR%0Aconsistently%20achieves%20performance%20improvements%20in%20FL%20settings.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2411.14961v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DLoRA-FAIR%253A%2520Federated%2520LoRA%2520Fine-Tuning%2520with%2520Aggregation%2520and%250A%2520%2520Initialization%2520Refinement%26entry.906535625%3DJieming%2520Bian%2520and%2520Lei%2520Wang%2520and%2520Letian%2520Zhang%2520and%2520Jie%2520Xu%26entry.1292438233%3D%2520%2520Foundation%2520models%2520%2528FMs%2529%2520achieve%2520strong%2520performance%2520across%2520diverse%2520tasks%2520with%250Atask-specific%2520fine-tuning%252C%2520yet%2520full%2520parameter%2520fine-tuning%2520is%2520often%250Acomputationally%2520prohibitive%2520for%2520large%2520models.%2520Parameter-efficient%2520fine-tuning%250A%2528PEFT%2529%2520methods%2520like%2520Low-Rank%2520Adaptation%2520%2528LoRA%2529%2520reduce%2520this%2520cost%2520by%2520introducing%250Alow-rank%2520matrices%2520for%2520tuning%2520fewer%2520parameters.%2520While%2520LoRA%2520allows%2520for%2520efficient%250Afine-tuning%252C%2520it%2520requires%2520significant%2520data%2520for%2520adaptation%252C%2520making%2520Federated%250ALearning%2520%2528FL%2529%2520an%2520appealing%2520solution%2520due%2520to%2520its%2520privacy-preserving%2520collaborative%250Aframework.%2520However%252C%2520combining%2520LoRA%2520with%2520FL%2520introduces%2520two%2520key%2520challenges%253A%2520the%250A%255Ctextbf%257BServer-Side%2520LoRA%2520Aggregation%2520Bias%257D%252C%2520where%2520server-side%2520averaging%2520of%2520LoRA%250Amatrices%2520diverges%2520from%2520the%2520ideal%2520global%2520update%252C%2520and%2520the%2520%255Ctextbf%257BClient-Side%250ALoRA%2520Initialization%2520Drift%257D%252C%2520emphasizing%2520the%2520need%2520for%2520consistent%2520initialization%250Aacross%2520rounds.%2520Existing%2520approaches%2520address%2520these%2520challenges%2520individually%252C%250Alimiting%2520their%2520effectiveness.%2520We%2520propose%2520LoRA-FAIR%252C%2520a%2520novel%2520method%2520that%2520tackles%250Aboth%2520issues%2520by%2520introducing%2520a%2520correction%2520term%2520on%2520the%2520server%2520while%2520keeping%2520the%250Aoriginal%2520LoRA%2520modules%252C%2520enhancing%2520aggregation%2520efficiency%2520and%2520accuracy.%2520LoRA-FAIR%250Amaintains%2520computational%2520and%2520communication%2520efficiency%252C%2520yielding%2520superior%250Aperformance%2520over%2520state-of-the-art%2520methods.%2520Experimental%2520results%2520on%2520ViT%2520and%250AMLP-Mixer%2520models%2520across%2520large-scale%2520datasets%2520demonstrate%2520that%2520LoRA-FAIR%250Aconsistently%2520achieves%2520performance%2520improvements%2520in%2520FL%2520settings.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2411.14961v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=LoRA-FAIR%3A%20Federated%20LoRA%20Fine-Tuning%20with%20Aggregation%20and%0A%20%20Initialization%20Refinement&entry.906535625=Jieming%20Bian%20and%20Lei%20Wang%20and%20Letian%20Zhang%20and%20Jie%20Xu&entry.1292438233=%20%20Foundation%20models%20%28FMs%29%20achieve%20strong%20performance%20across%20diverse%20tasks%20with%0Atask-specific%20fine-tuning%2C%20yet%20full%20parameter%20fine-tuning%20is%20often%0Acomputationally%20prohibitive%20for%20large%20models.%20Parameter-efficient%20fine-tuning%0A%28PEFT%29%20methods%20like%20Low-Rank%20Adaptation%20%28LoRA%29%20reduce%20this%20cost%20by%20introducing%0Alow-rank%20matrices%20for%20tuning%20fewer%20parameters.%20While%20LoRA%20allows%20for%20efficient%0Afine-tuning%2C%20it%20requires%20significant%20data%20for%20adaptation%2C%20making%20Federated%0ALearning%20%28FL%29%20an%20appealing%20solution%20due%20to%20its%20privacy-preserving%20collaborative%0Aframework.%20However%2C%20combining%20LoRA%20with%20FL%20introduces%20two%20key%20challenges%3A%20the%0A%5Ctextbf%7BServer-Side%20LoRA%20Aggregation%20Bias%7D%2C%20where%20server-side%20averaging%20of%20LoRA%0Amatrices%20diverges%20from%20the%20ideal%20global%20update%2C%20and%20the%20%5Ctextbf%7BClient-Side%0ALoRA%20Initialization%20Drift%7D%2C%20emphasizing%20the%20need%20for%20consistent%20initialization%0Aacross%20rounds.%20Existing%20approaches%20address%20these%20challenges%20individually%2C%0Alimiting%20their%20effectiveness.%20We%20propose%20LoRA-FAIR%2C%20a%20novel%20method%20that%20tackles%0Aboth%20issues%20by%20introducing%20a%20correction%20term%20on%20the%20server%20while%20keeping%20the%0Aoriginal%20LoRA%20modules%2C%20enhancing%20aggregation%20efficiency%20and%20accuracy.%20LoRA-FAIR%0Amaintains%20computational%20and%20communication%20efficiency%2C%20yielding%20superior%0Aperformance%20over%20state-of-the-art%20methods.%20Experimental%20results%20on%20ViT%20and%0AMLP-Mixer%20models%20across%20large-scale%20datasets%20demonstrate%20that%20LoRA-FAIR%0Aconsistently%20achieves%20performance%20improvements%20in%20FL%20settings.%0A&entry.1838667208=http%3A//arxiv.org/abs/2411.14961v1&entry.124074799=Read"},
{"title": "ReVisionLLM: Recursive Vision-Language Model for Temporal Grounding in\n  Hour-Long Videos", "author": "Tanveer Hannan and Md Mohaiminul Islam and Jindong Gu and Thomas Seidl and Gedas Bertasius", "abstract": "  Large language models (LLMs) excel at retrieving information from lengthy\ntext, but their vision-language counterparts (VLMs) face difficulties with\nhour-long videos, especially for temporal grounding. Specifically, these VLMs\nare constrained by frame limitations, often losing essential temporal details\nneeded for accurate event localization in extended video content. We propose\nReVisionLLM, a recursive vision-language model designed to locate events in\nhour-long videos. Inspired by human search strategies, our model initially\ntargets broad segments of interest, progressively revising its focus to\npinpoint exact temporal boundaries. Our model can seamlessly handle videos of\nvastly different lengths, from minutes to hours. We also introduce a\nhierarchical training strategy that starts with short clips to capture distinct\nevents and progressively extends to longer videos. To our knowledge,\nReVisionLLM is the first VLM capable of temporal grounding in hour-long videos,\noutperforming previous state-of-the-art methods across multiple datasets by a\nsignificant margin (+2.6% R1@0.1 on MAD). The code is available at\nhttps://github.com/Tanveer81/ReVisionLLM.\n", "link": "http://arxiv.org/abs/2411.14901v1", "date": "2024-11-22", "relevancy": 2.2531, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.569}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.569}, {"title": "Customize-A-Video: One-Shot Motion Customization of Text-to-Video\n  Diffusion Models", "link": "http://arxiv.org/abs/2402.14780v1", "similarity": 0.5348}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20ReVisionLLM%3A%20Recursive%20Vision-Language%20Model%20for%20Temporal%20Grounding%20in%0A%20%20Hour-Long%20Videos&body=Title%3A%20ReVisionLLM%3A%20Recursive%20Vision-Language%20Model%20for%20Temporal%20Grounding%20in%0A%20%20Hour-Long%20Videos%0AAuthor%3A%20Tanveer%20Hannan%20and%20Md%20Mohaiminul%20Islam%20and%20Jindong%20Gu%20and%20Thomas%20Seidl%20and%20Gedas%20Bertasius%0AAbstract%3A%20%20%20Large%20language%20models%20%28LLMs%29%20excel%20at%20retrieving%20information%20from%20lengthy%0Atext%2C%20but%20their%20vision-language%20counterparts%20%28VLMs%29%20face%20difficulties%20with%0Ahour-long%20videos%2C%20especially%20for%20temporal%20grounding.%20Specifically%2C%20these%20VLMs%0Aare%20constrained%20by%20frame%20limitations%2C%20often%20losing%20essential%20temporal%20details%0Aneeded%20for%20accurate%20event%20localization%20in%20extended%20video%20content.%20We%20propose%0AReVisionLLM%2C%20a%20recursive%20vision-language%20model%20designed%20to%20locate%20events%20in%0Ahour-long%20videos.%20Inspired%20by%20human%20search%20strategies%2C%20our%20model%20initially%0Atargets%20broad%20segments%20of%20interest%2C%20progressively%20revising%20its%20focus%20to%0Apinpoint%20exact%20temporal%20boundaries.%20Our%20model%20can%20seamlessly%20handle%20videos%20of%0Avastly%20different%20lengths%2C%20from%20minutes%20to%20hours.%20We%20also%20introduce%20a%0Ahierarchical%20training%20strategy%20that%20starts%20with%20short%20clips%20to%20capture%20distinct%0Aevents%20and%20progressively%20extends%20to%20longer%20videos.%20To%20our%20knowledge%2C%0AReVisionLLM%20is%20the%20first%20VLM%20capable%20of%20temporal%20grounding%20in%20hour-long%20videos%2C%0Aoutperforming%20previous%20state-of-the-art%20methods%20across%20multiple%20datasets%20by%20a%0Asignificant%20margin%20%28%2B2.6%25%20R1%400.1%20on%20MAD%29.%20The%20code%20is%20available%20at%0Ahttps%3A//github.com/Tanveer81/ReVisionLLM.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2411.14901v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DReVisionLLM%253A%2520Recursive%2520Vision-Language%2520Model%2520for%2520Temporal%2520Grounding%2520in%250A%2520%2520Hour-Long%2520Videos%26entry.906535625%3DTanveer%2520Hannan%2520and%2520Md%2520Mohaiminul%2520Islam%2520and%2520Jindong%2520Gu%2520and%2520Thomas%2520Seidl%2520and%2520Gedas%2520Bertasius%26entry.1292438233%3D%2520%2520Large%2520language%2520models%2520%2528LLMs%2529%2520excel%2520at%2520retrieving%2520information%2520from%2520lengthy%250Atext%252C%2520but%2520their%2520vision-language%2520counterparts%2520%2528VLMs%2529%2520face%2520difficulties%2520with%250Ahour-long%2520videos%252C%2520especially%2520for%2520temporal%2520grounding.%2520Specifically%252C%2520these%2520VLMs%250Aare%2520constrained%2520by%2520frame%2520limitations%252C%2520often%2520losing%2520essential%2520temporal%2520details%250Aneeded%2520for%2520accurate%2520event%2520localization%2520in%2520extended%2520video%2520content.%2520We%2520propose%250AReVisionLLM%252C%2520a%2520recursive%2520vision-language%2520model%2520designed%2520to%2520locate%2520events%2520in%250Ahour-long%2520videos.%2520Inspired%2520by%2520human%2520search%2520strategies%252C%2520our%2520model%2520initially%250Atargets%2520broad%2520segments%2520of%2520interest%252C%2520progressively%2520revising%2520its%2520focus%2520to%250Apinpoint%2520exact%2520temporal%2520boundaries.%2520Our%2520model%2520can%2520seamlessly%2520handle%2520videos%2520of%250Avastly%2520different%2520lengths%252C%2520from%2520minutes%2520to%2520hours.%2520We%2520also%2520introduce%2520a%250Ahierarchical%2520training%2520strategy%2520that%2520starts%2520with%2520short%2520clips%2520to%2520capture%2520distinct%250Aevents%2520and%2520progressively%2520extends%2520to%2520longer%2520videos.%2520To%2520our%2520knowledge%252C%250AReVisionLLM%2520is%2520the%2520first%2520VLM%2520capable%2520of%2520temporal%2520grounding%2520in%2520hour-long%2520videos%252C%250Aoutperforming%2520previous%2520state-of-the-art%2520methods%2520across%2520multiple%2520datasets%2520by%2520a%250Asignificant%2520margin%2520%2528%252B2.6%2525%2520R1%25400.1%2520on%2520MAD%2529.%2520The%2520code%2520is%2520available%2520at%250Ahttps%253A//github.com/Tanveer81/ReVisionLLM.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2411.14901v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=ReVisionLLM%3A%20Recursive%20Vision-Language%20Model%20for%20Temporal%20Grounding%20in%0A%20%20Hour-Long%20Videos&entry.906535625=Tanveer%20Hannan%20and%20Md%20Mohaiminul%20Islam%20and%20Jindong%20Gu%20and%20Thomas%20Seidl%20and%20Gedas%20Bertasius&entry.1292438233=%20%20Large%20language%20models%20%28LLMs%29%20excel%20at%20retrieving%20information%20from%20lengthy%0Atext%2C%20but%20their%20vision-language%20counterparts%20%28VLMs%29%20face%20difficulties%20with%0Ahour-long%20videos%2C%20especially%20for%20temporal%20grounding.%20Specifically%2C%20these%20VLMs%0Aare%20constrained%20by%20frame%20limitations%2C%20often%20losing%20essential%20temporal%20details%0Aneeded%20for%20accurate%20event%20localization%20in%20extended%20video%20content.%20We%20propose%0AReVisionLLM%2C%20a%20recursive%20vision-language%20model%20designed%20to%20locate%20events%20in%0Ahour-long%20videos.%20Inspired%20by%20human%20search%20strategies%2C%20our%20model%20initially%0Atargets%20broad%20segments%20of%20interest%2C%20progressively%20revising%20its%20focus%20to%0Apinpoint%20exact%20temporal%20boundaries.%20Our%20model%20can%20seamlessly%20handle%20videos%20of%0Avastly%20different%20lengths%2C%20from%20minutes%20to%20hours.%20We%20also%20introduce%20a%0Ahierarchical%20training%20strategy%20that%20starts%20with%20short%20clips%20to%20capture%20distinct%0Aevents%20and%20progressively%20extends%20to%20longer%20videos.%20To%20our%20knowledge%2C%0AReVisionLLM%20is%20the%20first%20VLM%20capable%20of%20temporal%20grounding%20in%20hour-long%20videos%2C%0Aoutperforming%20previous%20state-of-the-art%20methods%20across%20multiple%20datasets%20by%20a%0Asignificant%20margin%20%28%2B2.6%25%20R1%400.1%20on%20MAD%29.%20The%20code%20is%20available%20at%0Ahttps%3A//github.com/Tanveer81/ReVisionLLM.%0A&entry.1838667208=http%3A//arxiv.org/abs/2411.14901v1&entry.124074799=Read"},
{"title": "PRIMUS: Pretraining IMU Encoders with Multimodal Self-Supervision", "author": "Arnav M. Das and Chi Ian Tang and Fahim Kawsar and Mohammad Malekzadeh", "abstract": "  Sensing human motions through Inertial Measurement Units (IMUs) embedded in\npersonal devices has enabled significant applications in health and wellness.\nWhile labeled IMU data is scarce, we can collect unlabeled or weakly labeled\nIMU data to model human motions. For video or text modalities, the \"pretrain\nand adapt\" approach utilizes large volumes of unlabeled or weakly labeled data\nfor pretraining, building a strong feature extractor, followed by adaptation to\nspecific tasks using limited labeled data. This approach has not been widely\nadopted in the IMU domain for two reasons: (1) pretraining methods are poorly\nunderstood in the context of IMU, and (2) open-source pretrained models that\ngeneralize across datasets are rarely publicly available. In this paper, we aim\nto address the first issue by proposing PRIMUS, a method for PRetraining IMU\nencoderS. We conduct a systematic and unified evaluation of various\nself-supervised and multimodal learning pretraining objectives. Our findings\nindicate that using PRIMUS, which combines self-supervision, multimodal\nsupervision, and nearest-neighbor supervision, can significantly enhance\ndownstream performance. With fewer than 500 labeled samples per class, PRIMUS\neffectively enhances downstream performance by up to 15% in held-out test data,\ncompared to the state-of-the-art multimodal training method. To benefit the\nbroader community, our code and pre-trained IMU encoders will be made publicly\navailable at github.com/nokia-bell-labs upon publication.\n", "link": "http://arxiv.org/abs/2411.15127v1", "date": "2024-11-22", "relevancy": 2.2461, "topK": [{"title": "Weakly-guided self-supervised pretraining for temporal activity detection", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/25189", "similarity": 0.5737}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.5537}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5525}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20PRIMUS%3A%20Pretraining%20IMU%20Encoders%20with%20Multimodal%20Self-Supervision&body=Title%3A%20PRIMUS%3A%20Pretraining%20IMU%20Encoders%20with%20Multimodal%20Self-Supervision%0AAuthor%3A%20Arnav%20M.%20Das%20and%20Chi%20Ian%20Tang%20and%20Fahim%20Kawsar%20and%20Mohammad%20Malekzadeh%0AAbstract%3A%20%20%20Sensing%20human%20motions%20through%20Inertial%20Measurement%20Units%20%28IMUs%29%20embedded%20in%0Apersonal%20devices%20has%20enabled%20significant%20applications%20in%20health%20and%20wellness.%0AWhile%20labeled%20IMU%20data%20is%20scarce%2C%20we%20can%20collect%20unlabeled%20or%20weakly%20labeled%0AIMU%20data%20to%20model%20human%20motions.%20For%20video%20or%20text%20modalities%2C%20the%20%22pretrain%0Aand%20adapt%22%20approach%20utilizes%20large%20volumes%20of%20unlabeled%20or%20weakly%20labeled%20data%0Afor%20pretraining%2C%20building%20a%20strong%20feature%20extractor%2C%20followed%20by%20adaptation%20to%0Aspecific%20tasks%20using%20limited%20labeled%20data.%20This%20approach%20has%20not%20been%20widely%0Aadopted%20in%20the%20IMU%20domain%20for%20two%20reasons%3A%20%281%29%20pretraining%20methods%20are%20poorly%0Aunderstood%20in%20the%20context%20of%20IMU%2C%20and%20%282%29%20open-source%20pretrained%20models%20that%0Ageneralize%20across%20datasets%20are%20rarely%20publicly%20available.%20In%20this%20paper%2C%20we%20aim%0Ato%20address%20the%20first%20issue%20by%20proposing%20PRIMUS%2C%20a%20method%20for%20PRetraining%20IMU%0AencoderS.%20We%20conduct%20a%20systematic%20and%20unified%20evaluation%20of%20various%0Aself-supervised%20and%20multimodal%20learning%20pretraining%20objectives.%20Our%20findings%0Aindicate%20that%20using%20PRIMUS%2C%20which%20combines%20self-supervision%2C%20multimodal%0Asupervision%2C%20and%20nearest-neighbor%20supervision%2C%20can%20significantly%20enhance%0Adownstream%20performance.%20With%20fewer%20than%20500%20labeled%20samples%20per%20class%2C%20PRIMUS%0Aeffectively%20enhances%20downstream%20performance%20by%20up%20to%2015%25%20in%20held-out%20test%20data%2C%0Acompared%20to%20the%20state-of-the-art%20multimodal%20training%20method.%20To%20benefit%20the%0Abroader%20community%2C%20our%20code%20and%20pre-trained%20IMU%20encoders%20will%20be%20made%20publicly%0Aavailable%20at%20github.com/nokia-bell-labs%20upon%20publication.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2411.15127v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DPRIMUS%253A%2520Pretraining%2520IMU%2520Encoders%2520with%2520Multimodal%2520Self-Supervision%26entry.906535625%3DArnav%2520M.%2520Das%2520and%2520Chi%2520Ian%2520Tang%2520and%2520Fahim%2520Kawsar%2520and%2520Mohammad%2520Malekzadeh%26entry.1292438233%3D%2520%2520Sensing%2520human%2520motions%2520through%2520Inertial%2520Measurement%2520Units%2520%2528IMUs%2529%2520embedded%2520in%250Apersonal%2520devices%2520has%2520enabled%2520significant%2520applications%2520in%2520health%2520and%2520wellness.%250AWhile%2520labeled%2520IMU%2520data%2520is%2520scarce%252C%2520we%2520can%2520collect%2520unlabeled%2520or%2520weakly%2520labeled%250AIMU%2520data%2520to%2520model%2520human%2520motions.%2520For%2520video%2520or%2520text%2520modalities%252C%2520the%2520%2522pretrain%250Aand%2520adapt%2522%2520approach%2520utilizes%2520large%2520volumes%2520of%2520unlabeled%2520or%2520weakly%2520labeled%2520data%250Afor%2520pretraining%252C%2520building%2520a%2520strong%2520feature%2520extractor%252C%2520followed%2520by%2520adaptation%2520to%250Aspecific%2520tasks%2520using%2520limited%2520labeled%2520data.%2520This%2520approach%2520has%2520not%2520been%2520widely%250Aadopted%2520in%2520the%2520IMU%2520domain%2520for%2520two%2520reasons%253A%2520%25281%2529%2520pretraining%2520methods%2520are%2520poorly%250Aunderstood%2520in%2520the%2520context%2520of%2520IMU%252C%2520and%2520%25282%2529%2520open-source%2520pretrained%2520models%2520that%250Ageneralize%2520across%2520datasets%2520are%2520rarely%2520publicly%2520available.%2520In%2520this%2520paper%252C%2520we%2520aim%250Ato%2520address%2520the%2520first%2520issue%2520by%2520proposing%2520PRIMUS%252C%2520a%2520method%2520for%2520PRetraining%2520IMU%250AencoderS.%2520We%2520conduct%2520a%2520systematic%2520and%2520unified%2520evaluation%2520of%2520various%250Aself-supervised%2520and%2520multimodal%2520learning%2520pretraining%2520objectives.%2520Our%2520findings%250Aindicate%2520that%2520using%2520PRIMUS%252C%2520which%2520combines%2520self-supervision%252C%2520multimodal%250Asupervision%252C%2520and%2520nearest-neighbor%2520supervision%252C%2520can%2520significantly%2520enhance%250Adownstream%2520performance.%2520With%2520fewer%2520than%2520500%2520labeled%2520samples%2520per%2520class%252C%2520PRIMUS%250Aeffectively%2520enhances%2520downstream%2520performance%2520by%2520up%2520to%252015%2525%2520in%2520held-out%2520test%2520data%252C%250Acompared%2520to%2520the%2520state-of-the-art%2520multimodal%2520training%2520method.%2520To%2520benefit%2520the%250Abroader%2520community%252C%2520our%2520code%2520and%2520pre-trained%2520IMU%2520encoders%2520will%2520be%2520made%2520publicly%250Aavailable%2520at%2520github.com/nokia-bell-labs%2520upon%2520publication.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2411.15127v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=PRIMUS%3A%20Pretraining%20IMU%20Encoders%20with%20Multimodal%20Self-Supervision&entry.906535625=Arnav%20M.%20Das%20and%20Chi%20Ian%20Tang%20and%20Fahim%20Kawsar%20and%20Mohammad%20Malekzadeh&entry.1292438233=%20%20Sensing%20human%20motions%20through%20Inertial%20Measurement%20Units%20%28IMUs%29%20embedded%20in%0Apersonal%20devices%20has%20enabled%20significant%20applications%20in%20health%20and%20wellness.%0AWhile%20labeled%20IMU%20data%20is%20scarce%2C%20we%20can%20collect%20unlabeled%20or%20weakly%20labeled%0AIMU%20data%20to%20model%20human%20motions.%20For%20video%20or%20text%20modalities%2C%20the%20%22pretrain%0Aand%20adapt%22%20approach%20utilizes%20large%20volumes%20of%20unlabeled%20or%20weakly%20labeled%20data%0Afor%20pretraining%2C%20building%20a%20strong%20feature%20extractor%2C%20followed%20by%20adaptation%20to%0Aspecific%20tasks%20using%20limited%20labeled%20data.%20This%20approach%20has%20not%20been%20widely%0Aadopted%20in%20the%20IMU%20domain%20for%20two%20reasons%3A%20%281%29%20pretraining%20methods%20are%20poorly%0Aunderstood%20in%20the%20context%20of%20IMU%2C%20and%20%282%29%20open-source%20pretrained%20models%20that%0Ageneralize%20across%20datasets%20are%20rarely%20publicly%20available.%20In%20this%20paper%2C%20we%20aim%0Ato%20address%20the%20first%20issue%20by%20proposing%20PRIMUS%2C%20a%20method%20for%20PRetraining%20IMU%0AencoderS.%20We%20conduct%20a%20systematic%20and%20unified%20evaluation%20of%20various%0Aself-supervised%20and%20multimodal%20learning%20pretraining%20objectives.%20Our%20findings%0Aindicate%20that%20using%20PRIMUS%2C%20which%20combines%20self-supervision%2C%20multimodal%0Asupervision%2C%20and%20nearest-neighbor%20supervision%2C%20can%20significantly%20enhance%0Adownstream%20performance.%20With%20fewer%20than%20500%20labeled%20samples%20per%20class%2C%20PRIMUS%0Aeffectively%20enhances%20downstream%20performance%20by%20up%20to%2015%25%20in%20held-out%20test%20data%2C%0Acompared%20to%20the%20state-of-the-art%20multimodal%20training%20method.%20To%20benefit%20the%0Abroader%20community%2C%20our%20code%20and%20pre-trained%20IMU%20encoders%20will%20be%20made%20publicly%0Aavailable%20at%20github.com/nokia-bell-labs%20upon%20publication.%0A&entry.1838667208=http%3A//arxiv.org/abs/2411.15127v1&entry.124074799=Read"},
{"title": "Autonomous Tail-Sitter Flights in Unknown Environments", "author": "Guozheng Lu and Yunfan Ren and Fangcheng Zhu and Haotian Li and Ruize Xue and Yixi Cai and Ximin Lyu and Fu Zhang", "abstract": "  Trajectory generation for fully autonomous flights of tail-sitter unmanned\naerial vehicles (UAVs) presents substantial challenges due to their highly\nnonlinear aerodynamics. In this paper, we introduce, to the best of our\nknowledge, the world's first fully autonomous tail-sitter UAV capable of\nhigh-speed navigation in unknown, cluttered environments. The UAV autonomy is\nenabled by cutting-edge technologies including LiDAR-based sensing,\ndifferential-flatness-based trajectory planning and control with purely onboard\ncomputation. In particular, we propose an optimization-based tail-sitter\ntrajectory planning framework that generates high-speed, collision-free, and\ndynamically-feasible trajectories. To efficiently and reliably solve this\nnonlinear, constrained \\textcolor{black}{problem}, we develop an efficient\nfeasibility-assured solver, EFOPT, tailored for the online planning of\ntail-sitter UAVs. We conduct extensive simulation studies to benchmark EFOPT's\nsuperiority in planning tasks against conventional NLP solvers. We also\ndemonstrate exhaustive experiments of aggressive autonomous flights with speeds\nup to 15m/s in various real-world environments, including indoor laboratories,\nunderground parking lots, and outdoor parks. A video demonstration is available\nat https://youtu.be/OvqhlB2h3k8, and the EFOPT solver is open-sourced at\nhttps://github.com/hku-mars/EFOPT.\n", "link": "http://arxiv.org/abs/2411.15003v1", "date": "2024-11-22", "relevancy": 2.2413, "topK": [{"title": "Egocentric Computer Vision for Hands-Free Robotic Wheelchair Navigation", "link": "https://link.springer.com/article/10.1007/s10846-023-01807-4", "similarity": 0.5716}, {"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.5592}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.5349}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Autonomous%20Tail-Sitter%20Flights%20in%20Unknown%20Environments&body=Title%3A%20Autonomous%20Tail-Sitter%20Flights%20in%20Unknown%20Environments%0AAuthor%3A%20Guozheng%20Lu%20and%20Yunfan%20Ren%20and%20Fangcheng%20Zhu%20and%20Haotian%20Li%20and%20Ruize%20Xue%20and%20Yixi%20Cai%20and%20Ximin%20Lyu%20and%20Fu%20Zhang%0AAbstract%3A%20%20%20Trajectory%20generation%20for%20fully%20autonomous%20flights%20of%20tail-sitter%20unmanned%0Aaerial%20vehicles%20%28UAVs%29%20presents%20substantial%20challenges%20due%20to%20their%20highly%0Anonlinear%20aerodynamics.%20In%20this%20paper%2C%20we%20introduce%2C%20to%20the%20best%20of%20our%0Aknowledge%2C%20the%20world%27s%20first%20fully%20autonomous%20tail-sitter%20UAV%20capable%20of%0Ahigh-speed%20navigation%20in%20unknown%2C%20cluttered%20environments.%20The%20UAV%20autonomy%20is%0Aenabled%20by%20cutting-edge%20technologies%20including%20LiDAR-based%20sensing%2C%0Adifferential-flatness-based%20trajectory%20planning%20and%20control%20with%20purely%20onboard%0Acomputation.%20In%20particular%2C%20we%20propose%20an%20optimization-based%20tail-sitter%0Atrajectory%20planning%20framework%20that%20generates%20high-speed%2C%20collision-free%2C%20and%0Adynamically-feasible%20trajectories.%20To%20efficiently%20and%20reliably%20solve%20this%0Anonlinear%2C%20constrained%20%5Ctextcolor%7Bblack%7D%7Bproblem%7D%2C%20we%20develop%20an%20efficient%0Afeasibility-assured%20solver%2C%20EFOPT%2C%20tailored%20for%20the%20online%20planning%20of%0Atail-sitter%20UAVs.%20We%20conduct%20extensive%20simulation%20studies%20to%20benchmark%20EFOPT%27s%0Asuperiority%20in%20planning%20tasks%20against%20conventional%20NLP%20solvers.%20We%20also%0Ademonstrate%20exhaustive%20experiments%20of%20aggressive%20autonomous%20flights%20with%20speeds%0Aup%20to%2015m/s%20in%20various%20real-world%20environments%2C%20including%20indoor%20laboratories%2C%0Aunderground%20parking%20lots%2C%20and%20outdoor%20parks.%20A%20video%20demonstration%20is%20available%0Aat%20https%3A//youtu.be/OvqhlB2h3k8%2C%20and%20the%20EFOPT%20solver%20is%20open-sourced%20at%0Ahttps%3A//github.com/hku-mars/EFOPT.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2411.15003v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DAutonomous%2520Tail-Sitter%2520Flights%2520in%2520Unknown%2520Environments%26entry.906535625%3DGuozheng%2520Lu%2520and%2520Yunfan%2520Ren%2520and%2520Fangcheng%2520Zhu%2520and%2520Haotian%2520Li%2520and%2520Ruize%2520Xue%2520and%2520Yixi%2520Cai%2520and%2520Ximin%2520Lyu%2520and%2520Fu%2520Zhang%26entry.1292438233%3D%2520%2520Trajectory%2520generation%2520for%2520fully%2520autonomous%2520flights%2520of%2520tail-sitter%2520unmanned%250Aaerial%2520vehicles%2520%2528UAVs%2529%2520presents%2520substantial%2520challenges%2520due%2520to%2520their%2520highly%250Anonlinear%2520aerodynamics.%2520In%2520this%2520paper%252C%2520we%2520introduce%252C%2520to%2520the%2520best%2520of%2520our%250Aknowledge%252C%2520the%2520world%2527s%2520first%2520fully%2520autonomous%2520tail-sitter%2520UAV%2520capable%2520of%250Ahigh-speed%2520navigation%2520in%2520unknown%252C%2520cluttered%2520environments.%2520The%2520UAV%2520autonomy%2520is%250Aenabled%2520by%2520cutting-edge%2520technologies%2520including%2520LiDAR-based%2520sensing%252C%250Adifferential-flatness-based%2520trajectory%2520planning%2520and%2520control%2520with%2520purely%2520onboard%250Acomputation.%2520In%2520particular%252C%2520we%2520propose%2520an%2520optimization-based%2520tail-sitter%250Atrajectory%2520planning%2520framework%2520that%2520generates%2520high-speed%252C%2520collision-free%252C%2520and%250Adynamically-feasible%2520trajectories.%2520To%2520efficiently%2520and%2520reliably%2520solve%2520this%250Anonlinear%252C%2520constrained%2520%255Ctextcolor%257Bblack%257D%257Bproblem%257D%252C%2520we%2520develop%2520an%2520efficient%250Afeasibility-assured%2520solver%252C%2520EFOPT%252C%2520tailored%2520for%2520the%2520online%2520planning%2520of%250Atail-sitter%2520UAVs.%2520We%2520conduct%2520extensive%2520simulation%2520studies%2520to%2520benchmark%2520EFOPT%2527s%250Asuperiority%2520in%2520planning%2520tasks%2520against%2520conventional%2520NLP%2520solvers.%2520We%2520also%250Ademonstrate%2520exhaustive%2520experiments%2520of%2520aggressive%2520autonomous%2520flights%2520with%2520speeds%250Aup%2520to%252015m/s%2520in%2520various%2520real-world%2520environments%252C%2520including%2520indoor%2520laboratories%252C%250Aunderground%2520parking%2520lots%252C%2520and%2520outdoor%2520parks.%2520A%2520video%2520demonstration%2520is%2520available%250Aat%2520https%253A//youtu.be/OvqhlB2h3k8%252C%2520and%2520the%2520EFOPT%2520solver%2520is%2520open-sourced%2520at%250Ahttps%253A//github.com/hku-mars/EFOPT.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2411.15003v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Autonomous%20Tail-Sitter%20Flights%20in%20Unknown%20Environments&entry.906535625=Guozheng%20Lu%20and%20Yunfan%20Ren%20and%20Fangcheng%20Zhu%20and%20Haotian%20Li%20and%20Ruize%20Xue%20and%20Yixi%20Cai%20and%20Ximin%20Lyu%20and%20Fu%20Zhang&entry.1292438233=%20%20Trajectory%20generation%20for%20fully%20autonomous%20flights%20of%20tail-sitter%20unmanned%0Aaerial%20vehicles%20%28UAVs%29%20presents%20substantial%20challenges%20due%20to%20their%20highly%0Anonlinear%20aerodynamics.%20In%20this%20paper%2C%20we%20introduce%2C%20to%20the%20best%20of%20our%0Aknowledge%2C%20the%20world%27s%20first%20fully%20autonomous%20tail-sitter%20UAV%20capable%20of%0Ahigh-speed%20navigation%20in%20unknown%2C%20cluttered%20environments.%20The%20UAV%20autonomy%20is%0Aenabled%20by%20cutting-edge%20technologies%20including%20LiDAR-based%20sensing%2C%0Adifferential-flatness-based%20trajectory%20planning%20and%20control%20with%20purely%20onboard%0Acomputation.%20In%20particular%2C%20we%20propose%20an%20optimization-based%20tail-sitter%0Atrajectory%20planning%20framework%20that%20generates%20high-speed%2C%20collision-free%2C%20and%0Adynamically-feasible%20trajectories.%20To%20efficiently%20and%20reliably%20solve%20this%0Anonlinear%2C%20constrained%20%5Ctextcolor%7Bblack%7D%7Bproblem%7D%2C%20we%20develop%20an%20efficient%0Afeasibility-assured%20solver%2C%20EFOPT%2C%20tailored%20for%20the%20online%20planning%20of%0Atail-sitter%20UAVs.%20We%20conduct%20extensive%20simulation%20studies%20to%20benchmark%20EFOPT%27s%0Asuperiority%20in%20planning%20tasks%20against%20conventional%20NLP%20solvers.%20We%20also%0Ademonstrate%20exhaustive%20experiments%20of%20aggressive%20autonomous%20flights%20with%20speeds%0Aup%20to%2015m/s%20in%20various%20real-world%20environments%2C%20including%20indoor%20laboratories%2C%0Aunderground%20parking%20lots%2C%20and%20outdoor%20parks.%20A%20video%20demonstration%20is%20available%0Aat%20https%3A//youtu.be/OvqhlB2h3k8%2C%20and%20the%20EFOPT%20solver%20is%20open-sourced%20at%0Ahttps%3A//github.com/hku-mars/EFOPT.%0A&entry.1838667208=http%3A//arxiv.org/abs/2411.15003v1&entry.124074799=Read"},
{"title": "ReXrank: A Public Leaderboard for AI-Powered Radiology Report Generation", "author": "Xiaoman Zhang and Hong-Yu Zhou and Xiaoli Yang and Oishi Banerjee and Juli\u00e1n N. Acosta and Josh Miller and Ouwen Huang and Pranav Rajpurkar", "abstract": "  AI-driven models have demonstrated significant potential in automating\nradiology report generation for chest X-rays. However, there is no standardized\nbenchmark for objectively evaluating their performance. To address this, we\npresent ReXrank, https://rexrank.ai, a public leaderboard and challenge for\nassessing AI-powered radiology report generation. Our framework incorporates\nReXGradient, the largest test dataset consisting of 10,000 studies, and three\npublic datasets (MIMIC-CXR, IU-Xray, CheXpert Plus) for report generation\nassessment. ReXrank employs 8 evaluation metrics and separately assesses models\ncapable of generating only findings sections and those providing both findings\nand impressions sections. By providing this standardized evaluation framework,\nReXrank enables meaningful comparisons of model performance and offers crucial\ninsights into their robustness across diverse clinical settings. Beyond its\ncurrent focus on chest X-rays, ReXrank's framework sets the stage for\ncomprehensive evaluation of automated reporting across the full spectrum of\nmedical imaging.\n", "link": "http://arxiv.org/abs/2411.15122v1", "date": "2024-11-22", "relevancy": 2.2271, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.4509}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.4509}, {"title": "RTG-SLAM: Real-time 3D Reconstruction at Scale using Gaussian Splatting", "link": "http://arxiv.org/abs/2404.19706v1", "similarity": 0.4344}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20ReXrank%3A%20A%20Public%20Leaderboard%20for%20AI-Powered%20Radiology%20Report%20Generation&body=Title%3A%20ReXrank%3A%20A%20Public%20Leaderboard%20for%20AI-Powered%20Radiology%20Report%20Generation%0AAuthor%3A%20Xiaoman%20Zhang%20and%20Hong-Yu%20Zhou%20and%20Xiaoli%20Yang%20and%20Oishi%20Banerjee%20and%20Juli%C3%A1n%20N.%20Acosta%20and%20Josh%20Miller%20and%20Ouwen%20Huang%20and%20Pranav%20Rajpurkar%0AAbstract%3A%20%20%20AI-driven%20models%20have%20demonstrated%20significant%20potential%20in%20automating%0Aradiology%20report%20generation%20for%20chest%20X-rays.%20However%2C%20there%20is%20no%20standardized%0Abenchmark%20for%20objectively%20evaluating%20their%20performance.%20To%20address%20this%2C%20we%0Apresent%20ReXrank%2C%20https%3A//rexrank.ai%2C%20a%20public%20leaderboard%20and%20challenge%20for%0Aassessing%20AI-powered%20radiology%20report%20generation.%20Our%20framework%20incorporates%0AReXGradient%2C%20the%20largest%20test%20dataset%20consisting%20of%2010%2C000%20studies%2C%20and%20three%0Apublic%20datasets%20%28MIMIC-CXR%2C%20IU-Xray%2C%20CheXpert%20Plus%29%20for%20report%20generation%0Aassessment.%20ReXrank%20employs%208%20evaluation%20metrics%20and%20separately%20assesses%20models%0Acapable%20of%20generating%20only%20findings%20sections%20and%20those%20providing%20both%20findings%0Aand%20impressions%20sections.%20By%20providing%20this%20standardized%20evaluation%20framework%2C%0AReXrank%20enables%20meaningful%20comparisons%20of%20model%20performance%20and%20offers%20crucial%0Ainsights%20into%20their%20robustness%20across%20diverse%20clinical%20settings.%20Beyond%20its%0Acurrent%20focus%20on%20chest%20X-rays%2C%20ReXrank%27s%20framework%20sets%20the%20stage%20for%0Acomprehensive%20evaluation%20of%20automated%20reporting%20across%20the%20full%20spectrum%20of%0Amedical%20imaging.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2411.15122v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DReXrank%253A%2520A%2520Public%2520Leaderboard%2520for%2520AI-Powered%2520Radiology%2520Report%2520Generation%26entry.906535625%3DXiaoman%2520Zhang%2520and%2520Hong-Yu%2520Zhou%2520and%2520Xiaoli%2520Yang%2520and%2520Oishi%2520Banerjee%2520and%2520Juli%25C3%25A1n%2520N.%2520Acosta%2520and%2520Josh%2520Miller%2520and%2520Ouwen%2520Huang%2520and%2520Pranav%2520Rajpurkar%26entry.1292438233%3D%2520%2520AI-driven%2520models%2520have%2520demonstrated%2520significant%2520potential%2520in%2520automating%250Aradiology%2520report%2520generation%2520for%2520chest%2520X-rays.%2520However%252C%2520there%2520is%2520no%2520standardized%250Abenchmark%2520for%2520objectively%2520evaluating%2520their%2520performance.%2520To%2520address%2520this%252C%2520we%250Apresent%2520ReXrank%252C%2520https%253A//rexrank.ai%252C%2520a%2520public%2520leaderboard%2520and%2520challenge%2520for%250Aassessing%2520AI-powered%2520radiology%2520report%2520generation.%2520Our%2520framework%2520incorporates%250AReXGradient%252C%2520the%2520largest%2520test%2520dataset%2520consisting%2520of%252010%252C000%2520studies%252C%2520and%2520three%250Apublic%2520datasets%2520%2528MIMIC-CXR%252C%2520IU-Xray%252C%2520CheXpert%2520Plus%2529%2520for%2520report%2520generation%250Aassessment.%2520ReXrank%2520employs%25208%2520evaluation%2520metrics%2520and%2520separately%2520assesses%2520models%250Acapable%2520of%2520generating%2520only%2520findings%2520sections%2520and%2520those%2520providing%2520both%2520findings%250Aand%2520impressions%2520sections.%2520By%2520providing%2520this%2520standardized%2520evaluation%2520framework%252C%250AReXrank%2520enables%2520meaningful%2520comparisons%2520of%2520model%2520performance%2520and%2520offers%2520crucial%250Ainsights%2520into%2520their%2520robustness%2520across%2520diverse%2520clinical%2520settings.%2520Beyond%2520its%250Acurrent%2520focus%2520on%2520chest%2520X-rays%252C%2520ReXrank%2527s%2520framework%2520sets%2520the%2520stage%2520for%250Acomprehensive%2520evaluation%2520of%2520automated%2520reporting%2520across%2520the%2520full%2520spectrum%2520of%250Amedical%2520imaging.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2411.15122v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=ReXrank%3A%20A%20Public%20Leaderboard%20for%20AI-Powered%20Radiology%20Report%20Generation&entry.906535625=Xiaoman%20Zhang%20and%20Hong-Yu%20Zhou%20and%20Xiaoli%20Yang%20and%20Oishi%20Banerjee%20and%20Juli%C3%A1n%20N.%20Acosta%20and%20Josh%20Miller%20and%20Ouwen%20Huang%20and%20Pranav%20Rajpurkar&entry.1292438233=%20%20AI-driven%20models%20have%20demonstrated%20significant%20potential%20in%20automating%0Aradiology%20report%20generation%20for%20chest%20X-rays.%20However%2C%20there%20is%20no%20standardized%0Abenchmark%20for%20objectively%20evaluating%20their%20performance.%20To%20address%20this%2C%20we%0Apresent%20ReXrank%2C%20https%3A//rexrank.ai%2C%20a%20public%20leaderboard%20and%20challenge%20for%0Aassessing%20AI-powered%20radiology%20report%20generation.%20Our%20framework%20incorporates%0AReXGradient%2C%20the%20largest%20test%20dataset%20consisting%20of%2010%2C000%20studies%2C%20and%20three%0Apublic%20datasets%20%28MIMIC-CXR%2C%20IU-Xray%2C%20CheXpert%20Plus%29%20for%20report%20generation%0Aassessment.%20ReXrank%20employs%208%20evaluation%20metrics%20and%20separately%20assesses%20models%0Acapable%20of%20generating%20only%20findings%20sections%20and%20those%20providing%20both%20findings%0Aand%20impressions%20sections.%20By%20providing%20this%20standardized%20evaluation%20framework%2C%0AReXrank%20enables%20meaningful%20comparisons%20of%20model%20performance%20and%20offers%20crucial%0Ainsights%20into%20their%20robustness%20across%20diverse%20clinical%20settings.%20Beyond%20its%0Acurrent%20focus%20on%20chest%20X-rays%2C%20ReXrank%27s%20framework%20sets%20the%20stage%20for%0Acomprehensive%20evaluation%20of%20automated%20reporting%20across%20the%20full%20spectrum%20of%0Amedical%20imaging.%0A&entry.1838667208=http%3A//arxiv.org/abs/2411.15122v1&entry.124074799=Read"},
{"title": "Towards Speaker Identification with Minimal Dataset and Constrained\n  Resources using 1D-Convolution Neural Network", "author": "Irfan Nafiz Shahan and Pulok Ahmed Auvi", "abstract": "  Voice recognition and speaker identification are vital for applications in\nsecurity and personal assistants. This paper presents a lightweight\n1D-Convolutional Neural Network (1D-CNN) designed to perform speaker\nidentification on minimal datasets. Our approach achieves a validation accuracy\nof 97.87%, leveraging data augmentation techniques to handle background noise\nand limited training samples. Future improvements include testing on larger\ndatasets and integrating transfer learning methods to enhance generalizability.\nWe provide all code, the custom dataset, and the trained models to facilitate\nreproducibility. These resources are available on our GitHub repository:\nhttps://github.com/IrfanNafiz/RecMe.\n", "link": "http://arxiv.org/abs/2411.15082v1", "date": "2024-11-22", "relevancy": 2.2256, "topK": [{"title": "Magic-Me: Identity-Specific Video Customized Diffusion", "link": "http://arxiv.org/abs/2402.09368v1", "similarity": 0.4594}, {"title": "Weakly-guided self-supervised pretraining for temporal activity detection", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/25189", "similarity": 0.4437}, {"title": "IAE: Implicit Autoencoder for Point Cloud Self-supervised Representation Learning", "link": "https://arxiv.org/abs/2201.00785", "similarity": 0.4323}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Towards%20Speaker%20Identification%20with%20Minimal%20Dataset%20and%20Constrained%0A%20%20Resources%20using%201D-Convolution%20Neural%20Network&body=Title%3A%20Towards%20Speaker%20Identification%20with%20Minimal%20Dataset%20and%20Constrained%0A%20%20Resources%20using%201D-Convolution%20Neural%20Network%0AAuthor%3A%20Irfan%20Nafiz%20Shahan%20and%20Pulok%20Ahmed%20Auvi%0AAbstract%3A%20%20%20Voice%20recognition%20and%20speaker%20identification%20are%20vital%20for%20applications%20in%0Asecurity%20and%20personal%20assistants.%20This%20paper%20presents%20a%20lightweight%0A1D-Convolutional%20Neural%20Network%20%281D-CNN%29%20designed%20to%20perform%20speaker%0Aidentification%20on%20minimal%20datasets.%20Our%20approach%20achieves%20a%20validation%20accuracy%0Aof%2097.87%25%2C%20leveraging%20data%20augmentation%20techniques%20to%20handle%20background%20noise%0Aand%20limited%20training%20samples.%20Future%20improvements%20include%20testing%20on%20larger%0Adatasets%20and%20integrating%20transfer%20learning%20methods%20to%20enhance%20generalizability.%0AWe%20provide%20all%20code%2C%20the%20custom%20dataset%2C%20and%20the%20trained%20models%20to%20facilitate%0Areproducibility.%20These%20resources%20are%20available%20on%20our%20GitHub%20repository%3A%0Ahttps%3A//github.com/IrfanNafiz/RecMe.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2411.15082v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DTowards%2520Speaker%2520Identification%2520with%2520Minimal%2520Dataset%2520and%2520Constrained%250A%2520%2520Resources%2520using%25201D-Convolution%2520Neural%2520Network%26entry.906535625%3DIrfan%2520Nafiz%2520Shahan%2520and%2520Pulok%2520Ahmed%2520Auvi%26entry.1292438233%3D%2520%2520Voice%2520recognition%2520and%2520speaker%2520identification%2520are%2520vital%2520for%2520applications%2520in%250Asecurity%2520and%2520personal%2520assistants.%2520This%2520paper%2520presents%2520a%2520lightweight%250A1D-Convolutional%2520Neural%2520Network%2520%25281D-CNN%2529%2520designed%2520to%2520perform%2520speaker%250Aidentification%2520on%2520minimal%2520datasets.%2520Our%2520approach%2520achieves%2520a%2520validation%2520accuracy%250Aof%252097.87%2525%252C%2520leveraging%2520data%2520augmentation%2520techniques%2520to%2520handle%2520background%2520noise%250Aand%2520limited%2520training%2520samples.%2520Future%2520improvements%2520include%2520testing%2520on%2520larger%250Adatasets%2520and%2520integrating%2520transfer%2520learning%2520methods%2520to%2520enhance%2520generalizability.%250AWe%2520provide%2520all%2520code%252C%2520the%2520custom%2520dataset%252C%2520and%2520the%2520trained%2520models%2520to%2520facilitate%250Areproducibility.%2520These%2520resources%2520are%2520available%2520on%2520our%2520GitHub%2520repository%253A%250Ahttps%253A//github.com/IrfanNafiz/RecMe.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2411.15082v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Towards%20Speaker%20Identification%20with%20Minimal%20Dataset%20and%20Constrained%0A%20%20Resources%20using%201D-Convolution%20Neural%20Network&entry.906535625=Irfan%20Nafiz%20Shahan%20and%20Pulok%20Ahmed%20Auvi&entry.1292438233=%20%20Voice%20recognition%20and%20speaker%20identification%20are%20vital%20for%20applications%20in%0Asecurity%20and%20personal%20assistants.%20This%20paper%20presents%20a%20lightweight%0A1D-Convolutional%20Neural%20Network%20%281D-CNN%29%20designed%20to%20perform%20speaker%0Aidentification%20on%20minimal%20datasets.%20Our%20approach%20achieves%20a%20validation%20accuracy%0Aof%2097.87%25%2C%20leveraging%20data%20augmentation%20techniques%20to%20handle%20background%20noise%0Aand%20limited%20training%20samples.%20Future%20improvements%20include%20testing%20on%20larger%0Adatasets%20and%20integrating%20transfer%20learning%20methods%20to%20enhance%20generalizability.%0AWe%20provide%20all%20code%2C%20the%20custom%20dataset%2C%20and%20the%20trained%20models%20to%20facilitate%0Areproducibility.%20These%20resources%20are%20available%20on%20our%20GitHub%20repository%3A%0Ahttps%3A//github.com/IrfanNafiz/RecMe.%0A&entry.1838667208=http%3A//arxiv.org/abs/2411.15082v1&entry.124074799=Read"},
{"title": "Multi-Branch Generative Models for Multichannel Imaging with an\n  Application to PET/CT Synergistic Reconstruction", "author": "Noel Jeffrey Pinton and Alexandre Bousse and Catherine Cheze-Le-Rest and Dimitris Visvikis", "abstract": "  This paper presents a novel approach for learned synergistic reconstruction\nof medical images using multi-branch generative models. Leveraging variational\nautoencoders (VAEs), our model learns from pairs of images simultaneously,\nenabling effective denoising and reconstruction. Synergistic image\nreconstruction is achieved by incorporating the trained models in a regularizer\nthat evaluates the distance between the images and the model. We demonstrate\nthe efficacy of our approach on both Modified National Institute of Standards\nand Technology (MNIST) and positron emission tomography (PET)/computed\ntomography (CT) datasets, showcasing improved image quality for low-dose\nimaging. Despite challenges such as patch decomposition and model limitations,\nour results underscore the potential of generative models for enhancing medical\nimaging reconstruction.\n", "link": "http://arxiv.org/abs/2404.08748v3", "date": "2024-11-22", "relevancy": 2.2242, "topK": [{"title": "Thinking Outside the BBox: Unconstrained Generative Object Compositing", "link": "http://arxiv.org/abs/2409.04559v2", "similarity": 0.5651}, {"title": "CAT3D: Create Anything in 3D with Multi-View Diffusion Models", "link": "http://arxiv.org/abs/2405.10314v1", "similarity": 0.5543}, {"title": "CAT3D: Create Anything in 3D with Multi-View Diffusion Models", "link": "http://arxiv.org/abs/2405.10314v1", "similarity": 0.5543}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Multi-Branch%20Generative%20Models%20for%20Multichannel%20Imaging%20with%20an%0A%20%20Application%20to%20PET/CT%20Synergistic%20Reconstruction&body=Title%3A%20Multi-Branch%20Generative%20Models%20for%20Multichannel%20Imaging%20with%20an%0A%20%20Application%20to%20PET/CT%20Synergistic%20Reconstruction%0AAuthor%3A%20Noel%20Jeffrey%20Pinton%20and%20Alexandre%20Bousse%20and%20Catherine%20Cheze-Le-Rest%20and%20Dimitris%20Visvikis%0AAbstract%3A%20%20%20This%20paper%20presents%20a%20novel%20approach%20for%20learned%20synergistic%20reconstruction%0Aof%20medical%20images%20using%20multi-branch%20generative%20models.%20Leveraging%20variational%0Aautoencoders%20%28VAEs%29%2C%20our%20model%20learns%20from%20pairs%20of%20images%20simultaneously%2C%0Aenabling%20effective%20denoising%20and%20reconstruction.%20Synergistic%20image%0Areconstruction%20is%20achieved%20by%20incorporating%20the%20trained%20models%20in%20a%20regularizer%0Athat%20evaluates%20the%20distance%20between%20the%20images%20and%20the%20model.%20We%20demonstrate%0Athe%20efficacy%20of%20our%20approach%20on%20both%20Modified%20National%20Institute%20of%20Standards%0Aand%20Technology%20%28MNIST%29%20and%20positron%20emission%20tomography%20%28PET%29/computed%0Atomography%20%28CT%29%20datasets%2C%20showcasing%20improved%20image%20quality%20for%20low-dose%0Aimaging.%20Despite%20challenges%20such%20as%20patch%20decomposition%20and%20model%20limitations%2C%0Aour%20results%20underscore%20the%20potential%20of%20generative%20models%20for%20enhancing%20medical%0Aimaging%20reconstruction.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2404.08748v3%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DMulti-Branch%2520Generative%2520Models%2520for%2520Multichannel%2520Imaging%2520with%2520an%250A%2520%2520Application%2520to%2520PET/CT%2520Synergistic%2520Reconstruction%26entry.906535625%3DNoel%2520Jeffrey%2520Pinton%2520and%2520Alexandre%2520Bousse%2520and%2520Catherine%2520Cheze-Le-Rest%2520and%2520Dimitris%2520Visvikis%26entry.1292438233%3D%2520%2520This%2520paper%2520presents%2520a%2520novel%2520approach%2520for%2520learned%2520synergistic%2520reconstruction%250Aof%2520medical%2520images%2520using%2520multi-branch%2520generative%2520models.%2520Leveraging%2520variational%250Aautoencoders%2520%2528VAEs%2529%252C%2520our%2520model%2520learns%2520from%2520pairs%2520of%2520images%2520simultaneously%252C%250Aenabling%2520effective%2520denoising%2520and%2520reconstruction.%2520Synergistic%2520image%250Areconstruction%2520is%2520achieved%2520by%2520incorporating%2520the%2520trained%2520models%2520in%2520a%2520regularizer%250Athat%2520evaluates%2520the%2520distance%2520between%2520the%2520images%2520and%2520the%2520model.%2520We%2520demonstrate%250Athe%2520efficacy%2520of%2520our%2520approach%2520on%2520both%2520Modified%2520National%2520Institute%2520of%2520Standards%250Aand%2520Technology%2520%2528MNIST%2529%2520and%2520positron%2520emission%2520tomography%2520%2528PET%2529/computed%250Atomography%2520%2528CT%2529%2520datasets%252C%2520showcasing%2520improved%2520image%2520quality%2520for%2520low-dose%250Aimaging.%2520Despite%2520challenges%2520such%2520as%2520patch%2520decomposition%2520and%2520model%2520limitations%252C%250Aour%2520results%2520underscore%2520the%2520potential%2520of%2520generative%2520models%2520for%2520enhancing%2520medical%250Aimaging%2520reconstruction.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2404.08748v3%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Multi-Branch%20Generative%20Models%20for%20Multichannel%20Imaging%20with%20an%0A%20%20Application%20to%20PET/CT%20Synergistic%20Reconstruction&entry.906535625=Noel%20Jeffrey%20Pinton%20and%20Alexandre%20Bousse%20and%20Catherine%20Cheze-Le-Rest%20and%20Dimitris%20Visvikis&entry.1292438233=%20%20This%20paper%20presents%20a%20novel%20approach%20for%20learned%20synergistic%20reconstruction%0Aof%20medical%20images%20using%20multi-branch%20generative%20models.%20Leveraging%20variational%0Aautoencoders%20%28VAEs%29%2C%20our%20model%20learns%20from%20pairs%20of%20images%20simultaneously%2C%0Aenabling%20effective%20denoising%20and%20reconstruction.%20Synergistic%20image%0Areconstruction%20is%20achieved%20by%20incorporating%20the%20trained%20models%20in%20a%20regularizer%0Athat%20evaluates%20the%20distance%20between%20the%20images%20and%20the%20model.%20We%20demonstrate%0Athe%20efficacy%20of%20our%20approach%20on%20both%20Modified%20National%20Institute%20of%20Standards%0Aand%20Technology%20%28MNIST%29%20and%20positron%20emission%20tomography%20%28PET%29/computed%0Atomography%20%28CT%29%20datasets%2C%20showcasing%20improved%20image%20quality%20for%20low-dose%0Aimaging.%20Despite%20challenges%20such%20as%20patch%20decomposition%20and%20model%20limitations%2C%0Aour%20results%20underscore%20the%20potential%20of%20generative%20models%20for%20enhancing%20medical%0Aimaging%20reconstruction.%0A&entry.1838667208=http%3A//arxiv.org/abs/2404.08748v3&entry.124074799=Read"},
{"title": "OCD-FL: A Novel Communication-Efficient Peer Selection-based\n  Decentralized Federated Learning", "author": "Nizar Masmoudi and Wael Jaafar", "abstract": "  The conjunction of edge intelligence and the ever-growing Internet-of-Things\n(IoT) network heralds a new era of collaborative machine learning, with\nfederated learning (FL) emerging as the most prominent paradigm. With the\ngrowing interest in these learning schemes, researchers started addressing some\nof their most fundamental limitations. Indeed, conventional FL with a central\naggregator presents a single point of failure and a network bottleneck. To\nbypass this issue, decentralized FL where nodes collaborate in a peer-to-peer\nnetwork has been proposed. Despite the latter's efficiency, communication costs\nand data heterogeneity remain key challenges in decentralized FL. In this\ncontext, we propose a novel scheme, called opportunistic\ncommunication-efficient decentralized federated learning, a.k.a., OCD-FL,\nconsisting of a systematic FL peer selection for collaboration, aiming to\nachieve maximum FL knowledge gain while reducing energy consumption.\nExperimental results demonstrate the capability of OCD-FL to achieve similar or\nbetter performances than the fully collaborative FL, while significantly\nreducing consumed energy by at least 30% and up to 80%.\n", "link": "http://arxiv.org/abs/2403.04037v2", "date": "2024-11-22", "relevancy": 2.2218, "topK": [{"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.4485}, {"title": "Self-supervised Photographic Image Layout Representation Learning", "link": "http://arxiv.org/abs/2403.03740v1", "similarity": 0.4454}, {"title": "Flexible Visual Recognition by Evidential Modeling of Confusion and Ignorance", "link": "http://openaccess.thecvf.com/content/ICCV2023/html/Fan_Flexible_Visual_Recognition_by_Evidential_Modeling_of_Confusion_and_Ignorance_ICCV_2023_paper.html", "similarity": 0.4391}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20OCD-FL%3A%20A%20Novel%20Communication-Efficient%20Peer%20Selection-based%0A%20%20Decentralized%20Federated%20Learning&body=Title%3A%20OCD-FL%3A%20A%20Novel%20Communication-Efficient%20Peer%20Selection-based%0A%20%20Decentralized%20Federated%20Learning%0AAuthor%3A%20Nizar%20Masmoudi%20and%20Wael%20Jaafar%0AAbstract%3A%20%20%20The%20conjunction%20of%20edge%20intelligence%20and%20the%20ever-growing%20Internet-of-Things%0A%28IoT%29%20network%20heralds%20a%20new%20era%20of%20collaborative%20machine%20learning%2C%20with%0Afederated%20learning%20%28FL%29%20emerging%20as%20the%20most%20prominent%20paradigm.%20With%20the%0Agrowing%20interest%20in%20these%20learning%20schemes%2C%20researchers%20started%20addressing%20some%0Aof%20their%20most%20fundamental%20limitations.%20Indeed%2C%20conventional%20FL%20with%20a%20central%0Aaggregator%20presents%20a%20single%20point%20of%20failure%20and%20a%20network%20bottleneck.%20To%0Abypass%20this%20issue%2C%20decentralized%20FL%20where%20nodes%20collaborate%20in%20a%20peer-to-peer%0Anetwork%20has%20been%20proposed.%20Despite%20the%20latter%27s%20efficiency%2C%20communication%20costs%0Aand%20data%20heterogeneity%20remain%20key%20challenges%20in%20decentralized%20FL.%20In%20this%0Acontext%2C%20we%20propose%20a%20novel%20scheme%2C%20called%20opportunistic%0Acommunication-efficient%20decentralized%20federated%20learning%2C%20a.k.a.%2C%20OCD-FL%2C%0Aconsisting%20of%20a%20systematic%20FL%20peer%20selection%20for%20collaboration%2C%20aiming%20to%0Aachieve%20maximum%20FL%20knowledge%20gain%20while%20reducing%20energy%20consumption.%0AExperimental%20results%20demonstrate%20the%20capability%20of%20OCD-FL%20to%20achieve%20similar%20or%0Abetter%20performances%20than%20the%20fully%20collaborative%20FL%2C%20while%20significantly%0Areducing%20consumed%20energy%20by%20at%20least%2030%25%20and%20up%20to%2080%25.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2403.04037v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DOCD-FL%253A%2520A%2520Novel%2520Communication-Efficient%2520Peer%2520Selection-based%250A%2520%2520Decentralized%2520Federated%2520Learning%26entry.906535625%3DNizar%2520Masmoudi%2520and%2520Wael%2520Jaafar%26entry.1292438233%3D%2520%2520The%2520conjunction%2520of%2520edge%2520intelligence%2520and%2520the%2520ever-growing%2520Internet-of-Things%250A%2528IoT%2529%2520network%2520heralds%2520a%2520new%2520era%2520of%2520collaborative%2520machine%2520learning%252C%2520with%250Afederated%2520learning%2520%2528FL%2529%2520emerging%2520as%2520the%2520most%2520prominent%2520paradigm.%2520With%2520the%250Agrowing%2520interest%2520in%2520these%2520learning%2520schemes%252C%2520researchers%2520started%2520addressing%2520some%250Aof%2520their%2520most%2520fundamental%2520limitations.%2520Indeed%252C%2520conventional%2520FL%2520with%2520a%2520central%250Aaggregator%2520presents%2520a%2520single%2520point%2520of%2520failure%2520and%2520a%2520network%2520bottleneck.%2520To%250Abypass%2520this%2520issue%252C%2520decentralized%2520FL%2520where%2520nodes%2520collaborate%2520in%2520a%2520peer-to-peer%250Anetwork%2520has%2520been%2520proposed.%2520Despite%2520the%2520latter%2527s%2520efficiency%252C%2520communication%2520costs%250Aand%2520data%2520heterogeneity%2520remain%2520key%2520challenges%2520in%2520decentralized%2520FL.%2520In%2520this%250Acontext%252C%2520we%2520propose%2520a%2520novel%2520scheme%252C%2520called%2520opportunistic%250Acommunication-efficient%2520decentralized%2520federated%2520learning%252C%2520a.k.a.%252C%2520OCD-FL%252C%250Aconsisting%2520of%2520a%2520systematic%2520FL%2520peer%2520selection%2520for%2520collaboration%252C%2520aiming%2520to%250Aachieve%2520maximum%2520FL%2520knowledge%2520gain%2520while%2520reducing%2520energy%2520consumption.%250AExperimental%2520results%2520demonstrate%2520the%2520capability%2520of%2520OCD-FL%2520to%2520achieve%2520similar%2520or%250Abetter%2520performances%2520than%2520the%2520fully%2520collaborative%2520FL%252C%2520while%2520significantly%250Areducing%2520consumed%2520energy%2520by%2520at%2520least%252030%2525%2520and%2520up%2520to%252080%2525.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2403.04037v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=OCD-FL%3A%20A%20Novel%20Communication-Efficient%20Peer%20Selection-based%0A%20%20Decentralized%20Federated%20Learning&entry.906535625=Nizar%20Masmoudi%20and%20Wael%20Jaafar&entry.1292438233=%20%20The%20conjunction%20of%20edge%20intelligence%20and%20the%20ever-growing%20Internet-of-Things%0A%28IoT%29%20network%20heralds%20a%20new%20era%20of%20collaborative%20machine%20learning%2C%20with%0Afederated%20learning%20%28FL%29%20emerging%20as%20the%20most%20prominent%20paradigm.%20With%20the%0Agrowing%20interest%20in%20these%20learning%20schemes%2C%20researchers%20started%20addressing%20some%0Aof%20their%20most%20fundamental%20limitations.%20Indeed%2C%20conventional%20FL%20with%20a%20central%0Aaggregator%20presents%20a%20single%20point%20of%20failure%20and%20a%20network%20bottleneck.%20To%0Abypass%20this%20issue%2C%20decentralized%20FL%20where%20nodes%20collaborate%20in%20a%20peer-to-peer%0Anetwork%20has%20been%20proposed.%20Despite%20the%20latter%27s%20efficiency%2C%20communication%20costs%0Aand%20data%20heterogeneity%20remain%20key%20challenges%20in%20decentralized%20FL.%20In%20this%0Acontext%2C%20we%20propose%20a%20novel%20scheme%2C%20called%20opportunistic%0Acommunication-efficient%20decentralized%20federated%20learning%2C%20a.k.a.%2C%20OCD-FL%2C%0Aconsisting%20of%20a%20systematic%20FL%20peer%20selection%20for%20collaboration%2C%20aiming%20to%0Aachieve%20maximum%20FL%20knowledge%20gain%20while%20reducing%20energy%20consumption.%0AExperimental%20results%20demonstrate%20the%20capability%20of%20OCD-FL%20to%20achieve%20similar%20or%0Abetter%20performances%20than%20the%20fully%20collaborative%20FL%2C%20while%20significantly%0Areducing%20consumed%20energy%20by%20at%20least%2030%25%20and%20up%20to%2080%25.%0A&entry.1838667208=http%3A//arxiv.org/abs/2403.04037v2&entry.124074799=Read"},
{"title": "LLaNA: Large Language and NeRF Assistant", "author": "Andrea Amaduzzi and Pierluigi Zama Ramirez and Giuseppe Lisanti and Samuele Salti and Luigi Di Stefano", "abstract": "  Multimodal Large Language Models (MLLMs) have demonstrated an excellent\nunderstanding of images and 3D data. However, both modalities have shortcomings\nin holistically capturing the appearance and geometry of objects. Meanwhile,\nNeural Radiance Fields (NeRFs), which encode information within the weights of\na simple Multi-Layer Perceptron (MLP), have emerged as an increasingly\nwidespread modality that simultaneously encodes the geometry and photorealistic\nappearance of objects. This paper investigates the feasibility and\neffectiveness of ingesting NeRF into MLLM. We create LLaNA, the first\ngeneral-purpose NeRF-language assistant capable of performing new tasks such as\nNeRF captioning and Q\\&A. Notably, our method directly processes the weights of\nthe NeRF's MLP to extract information about the represented objects without the\nneed to render images or materialize 3D data structures. Moreover, we build a\ndataset of NeRFs with text annotations for various NeRF-language tasks with no\nhuman intervention. Based on this dataset, we develop a benchmark to evaluate\nthe NeRF understanding capability of our method. Results show that processing\nNeRF weights performs favourably against extracting 2D or 3D representations\nfrom NeRFs.\n", "link": "http://arxiv.org/abs/2406.11840v2", "date": "2024-11-22", "relevancy": 2.2172, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5597}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5597}, {"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.5274}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20LLaNA%3A%20Large%20Language%20and%20NeRF%20Assistant&body=Title%3A%20LLaNA%3A%20Large%20Language%20and%20NeRF%20Assistant%0AAuthor%3A%20Andrea%20Amaduzzi%20and%20Pierluigi%20Zama%20Ramirez%20and%20Giuseppe%20Lisanti%20and%20Samuele%20Salti%20and%20Luigi%20Di%20Stefano%0AAbstract%3A%20%20%20Multimodal%20Large%20Language%20Models%20%28MLLMs%29%20have%20demonstrated%20an%20excellent%0Aunderstanding%20of%20images%20and%203D%20data.%20However%2C%20both%20modalities%20have%20shortcomings%0Ain%20holistically%20capturing%20the%20appearance%20and%20geometry%20of%20objects.%20Meanwhile%2C%0ANeural%20Radiance%20Fields%20%28NeRFs%29%2C%20which%20encode%20information%20within%20the%20weights%20of%0Aa%20simple%20Multi-Layer%20Perceptron%20%28MLP%29%2C%20have%20emerged%20as%20an%20increasingly%0Awidespread%20modality%20that%20simultaneously%20encodes%20the%20geometry%20and%20photorealistic%0Aappearance%20of%20objects.%20This%20paper%20investigates%20the%20feasibility%20and%0Aeffectiveness%20of%20ingesting%20NeRF%20into%20MLLM.%20We%20create%20LLaNA%2C%20the%20first%0Ageneral-purpose%20NeRF-language%20assistant%20capable%20of%20performing%20new%20tasks%20such%20as%0ANeRF%20captioning%20and%20Q%5C%26A.%20Notably%2C%20our%20method%20directly%20processes%20the%20weights%20of%0Athe%20NeRF%27s%20MLP%20to%20extract%20information%20about%20the%20represented%20objects%20without%20the%0Aneed%20to%20render%20images%20or%20materialize%203D%20data%20structures.%20Moreover%2C%20we%20build%20a%0Adataset%20of%20NeRFs%20with%20text%20annotations%20for%20various%20NeRF-language%20tasks%20with%20no%0Ahuman%20intervention.%20Based%20on%20this%20dataset%2C%20we%20develop%20a%20benchmark%20to%20evaluate%0Athe%20NeRF%20understanding%20capability%20of%20our%20method.%20Results%20show%20that%20processing%0ANeRF%20weights%20performs%20favourably%20against%20extracting%202D%20or%203D%20representations%0Afrom%20NeRFs.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2406.11840v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DLLaNA%253A%2520Large%2520Language%2520and%2520NeRF%2520Assistant%26entry.906535625%3DAndrea%2520Amaduzzi%2520and%2520Pierluigi%2520Zama%2520Ramirez%2520and%2520Giuseppe%2520Lisanti%2520and%2520Samuele%2520Salti%2520and%2520Luigi%2520Di%2520Stefano%26entry.1292438233%3D%2520%2520Multimodal%2520Large%2520Language%2520Models%2520%2528MLLMs%2529%2520have%2520demonstrated%2520an%2520excellent%250Aunderstanding%2520of%2520images%2520and%25203D%2520data.%2520However%252C%2520both%2520modalities%2520have%2520shortcomings%250Ain%2520holistically%2520capturing%2520the%2520appearance%2520and%2520geometry%2520of%2520objects.%2520Meanwhile%252C%250ANeural%2520Radiance%2520Fields%2520%2528NeRFs%2529%252C%2520which%2520encode%2520information%2520within%2520the%2520weights%2520of%250Aa%2520simple%2520Multi-Layer%2520Perceptron%2520%2528MLP%2529%252C%2520have%2520emerged%2520as%2520an%2520increasingly%250Awidespread%2520modality%2520that%2520simultaneously%2520encodes%2520the%2520geometry%2520and%2520photorealistic%250Aappearance%2520of%2520objects.%2520This%2520paper%2520investigates%2520the%2520feasibility%2520and%250Aeffectiveness%2520of%2520ingesting%2520NeRF%2520into%2520MLLM.%2520We%2520create%2520LLaNA%252C%2520the%2520first%250Ageneral-purpose%2520NeRF-language%2520assistant%2520capable%2520of%2520performing%2520new%2520tasks%2520such%2520as%250ANeRF%2520captioning%2520and%2520Q%255C%2526A.%2520Notably%252C%2520our%2520method%2520directly%2520processes%2520the%2520weights%2520of%250Athe%2520NeRF%2527s%2520MLP%2520to%2520extract%2520information%2520about%2520the%2520represented%2520objects%2520without%2520the%250Aneed%2520to%2520render%2520images%2520or%2520materialize%25203D%2520data%2520structures.%2520Moreover%252C%2520we%2520build%2520a%250Adataset%2520of%2520NeRFs%2520with%2520text%2520annotations%2520for%2520various%2520NeRF-language%2520tasks%2520with%2520no%250Ahuman%2520intervention.%2520Based%2520on%2520this%2520dataset%252C%2520we%2520develop%2520a%2520benchmark%2520to%2520evaluate%250Athe%2520NeRF%2520understanding%2520capability%2520of%2520our%2520method.%2520Results%2520show%2520that%2520processing%250ANeRF%2520weights%2520performs%2520favourably%2520against%2520extracting%25202D%2520or%25203D%2520representations%250Afrom%2520NeRFs.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2406.11840v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=LLaNA%3A%20Large%20Language%20and%20NeRF%20Assistant&entry.906535625=Andrea%20Amaduzzi%20and%20Pierluigi%20Zama%20Ramirez%20and%20Giuseppe%20Lisanti%20and%20Samuele%20Salti%20and%20Luigi%20Di%20Stefano&entry.1292438233=%20%20Multimodal%20Large%20Language%20Models%20%28MLLMs%29%20have%20demonstrated%20an%20excellent%0Aunderstanding%20of%20images%20and%203D%20data.%20However%2C%20both%20modalities%20have%20shortcomings%0Ain%20holistically%20capturing%20the%20appearance%20and%20geometry%20of%20objects.%20Meanwhile%2C%0ANeural%20Radiance%20Fields%20%28NeRFs%29%2C%20which%20encode%20information%20within%20the%20weights%20of%0Aa%20simple%20Multi-Layer%20Perceptron%20%28MLP%29%2C%20have%20emerged%20as%20an%20increasingly%0Awidespread%20modality%20that%20simultaneously%20encodes%20the%20geometry%20and%20photorealistic%0Aappearance%20of%20objects.%20This%20paper%20investigates%20the%20feasibility%20and%0Aeffectiveness%20of%20ingesting%20NeRF%20into%20MLLM.%20We%20create%20LLaNA%2C%20the%20first%0Ageneral-purpose%20NeRF-language%20assistant%20capable%20of%20performing%20new%20tasks%20such%20as%0ANeRF%20captioning%20and%20Q%5C%26A.%20Notably%2C%20our%20method%20directly%20processes%20the%20weights%20of%0Athe%20NeRF%27s%20MLP%20to%20extract%20information%20about%20the%20represented%20objects%20without%20the%0Aneed%20to%20render%20images%20or%20materialize%203D%20data%20structures.%20Moreover%2C%20we%20build%20a%0Adataset%20of%20NeRFs%20with%20text%20annotations%20for%20various%20NeRF-language%20tasks%20with%20no%0Ahuman%20intervention.%20Based%20on%20this%20dataset%2C%20we%20develop%20a%20benchmark%20to%20evaluate%0Athe%20NeRF%20understanding%20capability%20of%20our%20method.%20Results%20show%20that%20processing%0ANeRF%20weights%20performs%20favourably%20against%20extracting%202D%20or%203D%20representations%0Afrom%20NeRFs.%0A&entry.1838667208=http%3A//arxiv.org/abs/2406.11840v2&entry.124074799=Read"},
{"title": "Boundless Across Domains: A New Paradigm of Adaptive Feature and\n  Cross-Attention for Domain Generalization in Medical Image Segmentation", "author": "Yuheng Xu and Taiping Zhang", "abstract": "  Domain-invariant representation learning is a powerful method for domain\ngeneralization. Previous approaches face challenges such as high computational\ndemands, training instability, and limited effectiveness with high-dimensional\ndata, potentially leading to the loss of valuable features. To address these\nissues, we hypothesize that an ideal generalized representation should exhibit\nsimilar pattern responses within the same channel across cross-domain images.\nBased on this hypothesis, we use deep features from the source domain as\nqueries, and deep features from the generated domain as keys and values.\nThrough a cross-channel attention mechanism, the original deep features are\nreconstructed into robust regularization representations, forming an explicit\nconstraint that guides the model to learn domain-invariant representations.\nAdditionally, style augmentation is another common method. However, existing\nmethods typically generate new styles through convex combinations of source\ndomains, which limits the diversity of training samples by confining the\ngenerated styles to the original distribution. To overcome this limitation, we\npropose an Adaptive Feature Blending (AFB) method that generates\nout-of-distribution samples while exploring the in-distribution space,\nsignificantly expanding the domain range. Extensive experimental results\ndemonstrate that our proposed methods achieve superior performance on two\nstandard domain generalization benchmarks for medical image segmentation.\n", "link": "http://arxiv.org/abs/2411.14883v1", "date": "2024-11-22", "relevancy": 2.2118, "topK": [{"title": "Thinking Outside the BBox: Unconstrained Generative Object Compositing", "link": "http://arxiv.org/abs/2409.04559v2", "similarity": 0.5733}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5506}, {"title": "Self-supervised Photographic Image Layout Representation Learning", "link": "http://arxiv.org/abs/2403.03740v1", "similarity": 0.5471}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Boundless%20Across%20Domains%3A%20A%20New%20Paradigm%20of%20Adaptive%20Feature%20and%0A%20%20Cross-Attention%20for%20Domain%20Generalization%20in%20Medical%20Image%20Segmentation&body=Title%3A%20Boundless%20Across%20Domains%3A%20A%20New%20Paradigm%20of%20Adaptive%20Feature%20and%0A%20%20Cross-Attention%20for%20Domain%20Generalization%20in%20Medical%20Image%20Segmentation%0AAuthor%3A%20Yuheng%20Xu%20and%20Taiping%20Zhang%0AAbstract%3A%20%20%20Domain-invariant%20representation%20learning%20is%20a%20powerful%20method%20for%20domain%0Ageneralization.%20Previous%20approaches%20face%20challenges%20such%20as%20high%20computational%0Ademands%2C%20training%20instability%2C%20and%20limited%20effectiveness%20with%20high-dimensional%0Adata%2C%20potentially%20leading%20to%20the%20loss%20of%20valuable%20features.%20To%20address%20these%0Aissues%2C%20we%20hypothesize%20that%20an%20ideal%20generalized%20representation%20should%20exhibit%0Asimilar%20pattern%20responses%20within%20the%20same%20channel%20across%20cross-domain%20images.%0ABased%20on%20this%20hypothesis%2C%20we%20use%20deep%20features%20from%20the%20source%20domain%20as%0Aqueries%2C%20and%20deep%20features%20from%20the%20generated%20domain%20as%20keys%20and%20values.%0AThrough%20a%20cross-channel%20attention%20mechanism%2C%20the%20original%20deep%20features%20are%0Areconstructed%20into%20robust%20regularization%20representations%2C%20forming%20an%20explicit%0Aconstraint%20that%20guides%20the%20model%20to%20learn%20domain-invariant%20representations.%0AAdditionally%2C%20style%20augmentation%20is%20another%20common%20method.%20However%2C%20existing%0Amethods%20typically%20generate%20new%20styles%20through%20convex%20combinations%20of%20source%0Adomains%2C%20which%20limits%20the%20diversity%20of%20training%20samples%20by%20confining%20the%0Agenerated%20styles%20to%20the%20original%20distribution.%20To%20overcome%20this%20limitation%2C%20we%0Apropose%20an%20Adaptive%20Feature%20Blending%20%28AFB%29%20method%20that%20generates%0Aout-of-distribution%20samples%20while%20exploring%20the%20in-distribution%20space%2C%0Asignificantly%20expanding%20the%20domain%20range.%20Extensive%20experimental%20results%0Ademonstrate%20that%20our%20proposed%20methods%20achieve%20superior%20performance%20on%20two%0Astandard%20domain%20generalization%20benchmarks%20for%20medical%20image%20segmentation.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2411.14883v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DBoundless%2520Across%2520Domains%253A%2520A%2520New%2520Paradigm%2520of%2520Adaptive%2520Feature%2520and%250A%2520%2520Cross-Attention%2520for%2520Domain%2520Generalization%2520in%2520Medical%2520Image%2520Segmentation%26entry.906535625%3DYuheng%2520Xu%2520and%2520Taiping%2520Zhang%26entry.1292438233%3D%2520%2520Domain-invariant%2520representation%2520learning%2520is%2520a%2520powerful%2520method%2520for%2520domain%250Ageneralization.%2520Previous%2520approaches%2520face%2520challenges%2520such%2520as%2520high%2520computational%250Ademands%252C%2520training%2520instability%252C%2520and%2520limited%2520effectiveness%2520with%2520high-dimensional%250Adata%252C%2520potentially%2520leading%2520to%2520the%2520loss%2520of%2520valuable%2520features.%2520To%2520address%2520these%250Aissues%252C%2520we%2520hypothesize%2520that%2520an%2520ideal%2520generalized%2520representation%2520should%2520exhibit%250Asimilar%2520pattern%2520responses%2520within%2520the%2520same%2520channel%2520across%2520cross-domain%2520images.%250ABased%2520on%2520this%2520hypothesis%252C%2520we%2520use%2520deep%2520features%2520from%2520the%2520source%2520domain%2520as%250Aqueries%252C%2520and%2520deep%2520features%2520from%2520the%2520generated%2520domain%2520as%2520keys%2520and%2520values.%250AThrough%2520a%2520cross-channel%2520attention%2520mechanism%252C%2520the%2520original%2520deep%2520features%2520are%250Areconstructed%2520into%2520robust%2520regularization%2520representations%252C%2520forming%2520an%2520explicit%250Aconstraint%2520that%2520guides%2520the%2520model%2520to%2520learn%2520domain-invariant%2520representations.%250AAdditionally%252C%2520style%2520augmentation%2520is%2520another%2520common%2520method.%2520However%252C%2520existing%250Amethods%2520typically%2520generate%2520new%2520styles%2520through%2520convex%2520combinations%2520of%2520source%250Adomains%252C%2520which%2520limits%2520the%2520diversity%2520of%2520training%2520samples%2520by%2520confining%2520the%250Agenerated%2520styles%2520to%2520the%2520original%2520distribution.%2520To%2520overcome%2520this%2520limitation%252C%2520we%250Apropose%2520an%2520Adaptive%2520Feature%2520Blending%2520%2528AFB%2529%2520method%2520that%2520generates%250Aout-of-distribution%2520samples%2520while%2520exploring%2520the%2520in-distribution%2520space%252C%250Asignificantly%2520expanding%2520the%2520domain%2520range.%2520Extensive%2520experimental%2520results%250Ademonstrate%2520that%2520our%2520proposed%2520methods%2520achieve%2520superior%2520performance%2520on%2520two%250Astandard%2520domain%2520generalization%2520benchmarks%2520for%2520medical%2520image%2520segmentation.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2411.14883v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Boundless%20Across%20Domains%3A%20A%20New%20Paradigm%20of%20Adaptive%20Feature%20and%0A%20%20Cross-Attention%20for%20Domain%20Generalization%20in%20Medical%20Image%20Segmentation&entry.906535625=Yuheng%20Xu%20and%20Taiping%20Zhang&entry.1292438233=%20%20Domain-invariant%20representation%20learning%20is%20a%20powerful%20method%20for%20domain%0Ageneralization.%20Previous%20approaches%20face%20challenges%20such%20as%20high%20computational%0Ademands%2C%20training%20instability%2C%20and%20limited%20effectiveness%20with%20high-dimensional%0Adata%2C%20potentially%20leading%20to%20the%20loss%20of%20valuable%20features.%20To%20address%20these%0Aissues%2C%20we%20hypothesize%20that%20an%20ideal%20generalized%20representation%20should%20exhibit%0Asimilar%20pattern%20responses%20within%20the%20same%20channel%20across%20cross-domain%20images.%0ABased%20on%20this%20hypothesis%2C%20we%20use%20deep%20features%20from%20the%20source%20domain%20as%0Aqueries%2C%20and%20deep%20features%20from%20the%20generated%20domain%20as%20keys%20and%20values.%0AThrough%20a%20cross-channel%20attention%20mechanism%2C%20the%20original%20deep%20features%20are%0Areconstructed%20into%20robust%20regularization%20representations%2C%20forming%20an%20explicit%0Aconstraint%20that%20guides%20the%20model%20to%20learn%20domain-invariant%20representations.%0AAdditionally%2C%20style%20augmentation%20is%20another%20common%20method.%20However%2C%20existing%0Amethods%20typically%20generate%20new%20styles%20through%20convex%20combinations%20of%20source%0Adomains%2C%20which%20limits%20the%20diversity%20of%20training%20samples%20by%20confining%20the%0Agenerated%20styles%20to%20the%20original%20distribution.%20To%20overcome%20this%20limitation%2C%20we%0Apropose%20an%20Adaptive%20Feature%20Blending%20%28AFB%29%20method%20that%20generates%0Aout-of-distribution%20samples%20while%20exploring%20the%20in-distribution%20space%2C%0Asignificantly%20expanding%20the%20domain%20range.%20Extensive%20experimental%20results%0Ademonstrate%20that%20our%20proposed%20methods%20achieve%20superior%20performance%20on%20two%0Astandard%20domain%20generalization%20benchmarks%20for%20medical%20image%20segmentation.%0A&entry.1838667208=http%3A//arxiv.org/abs/2411.14883v1&entry.124074799=Read"},
{"title": "Sketched Equivariant Imaging Regularization and Deep Internal Learning\n  for Inverse Problems", "author": "Guixian Xu and Jinglai Li and Junqi Tang", "abstract": "  Equivariant Imaging (EI) regularization has become the de-facto technique for\nunsupervised training of deep imaging networks, without any need of\nground-truth data. Observing that the EI-based unsupervised training paradigm\ncurrently has significant computational redundancy leading to inefficiency in\nhigh-dimensional applications, we propose a sketched EI regularization which\nleverages the randomized sketching techniques for acceleration. We then extend\nour sketched EI regularization to develop an accelerated deep internal learning\nframework -- Sketched Equivariant Deep Image Prior (Sk-EI-DIP), which can be\nefficiently applied for single-image and task-adapted reconstruction.\nAdditionally, for network adaptation tasks, we propose a parameter-efficient\napproach for accelerating both EI-DIP and Sk-EI-DIP via optimizing only the\nnormalization layers. Our numerical study on X-ray CT image reconstruction\ntasks demonstrate that our approach can achieve order-of-magnitude\ncomputational acceleration over standard EI-based counterpart in single-input\nsetting, and network adaptation at test time.\n", "link": "http://arxiv.org/abs/2411.05771v2", "date": "2024-11-22", "relevancy": 2.2038, "topK": [{"title": "SDXS: Real-Time One-Step Latent Diffusion Models with Image Conditions", "link": "http://arxiv.org/abs/2403.16627v1", "similarity": 0.5624}, {"title": "DDM-NET: End-to-end learning of keypoint feature Detection, Description and Matching for 3D localization", "link": "https://arxiv.org/abs/2212.04575", "similarity": 0.5545}, {"title": "Self-supervised Photographic Image Layout Representation Learning", "link": "http://arxiv.org/abs/2403.03740v1", "similarity": 0.5429}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Sketched%20Equivariant%20Imaging%20Regularization%20and%20Deep%20Internal%20Learning%0A%20%20for%20Inverse%20Problems&body=Title%3A%20Sketched%20Equivariant%20Imaging%20Regularization%20and%20Deep%20Internal%20Learning%0A%20%20for%20Inverse%20Problems%0AAuthor%3A%20Guixian%20Xu%20and%20Jinglai%20Li%20and%20Junqi%20Tang%0AAbstract%3A%20%20%20Equivariant%20Imaging%20%28EI%29%20regularization%20has%20become%20the%20de-facto%20technique%20for%0Aunsupervised%20training%20of%20deep%20imaging%20networks%2C%20without%20any%20need%20of%0Aground-truth%20data.%20Observing%20that%20the%20EI-based%20unsupervised%20training%20paradigm%0Acurrently%20has%20significant%20computational%20redundancy%20leading%20to%20inefficiency%20in%0Ahigh-dimensional%20applications%2C%20we%20propose%20a%20sketched%20EI%20regularization%20which%0Aleverages%20the%20randomized%20sketching%20techniques%20for%20acceleration.%20We%20then%20extend%0Aour%20sketched%20EI%20regularization%20to%20develop%20an%20accelerated%20deep%20internal%20learning%0Aframework%20--%20Sketched%20Equivariant%20Deep%20Image%20Prior%20%28Sk-EI-DIP%29%2C%20which%20can%20be%0Aefficiently%20applied%20for%20single-image%20and%20task-adapted%20reconstruction.%0AAdditionally%2C%20for%20network%20adaptation%20tasks%2C%20we%20propose%20a%20parameter-efficient%0Aapproach%20for%20accelerating%20both%20EI-DIP%20and%20Sk-EI-DIP%20via%20optimizing%20only%20the%0Anormalization%20layers.%20Our%20numerical%20study%20on%20X-ray%20CT%20image%20reconstruction%0Atasks%20demonstrate%20that%20our%20approach%20can%20achieve%20order-of-magnitude%0Acomputational%20acceleration%20over%20standard%20EI-based%20counterpart%20in%20single-input%0Asetting%2C%20and%20network%20adaptation%20at%20test%20time.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2411.05771v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DSketched%2520Equivariant%2520Imaging%2520Regularization%2520and%2520Deep%2520Internal%2520Learning%250A%2520%2520for%2520Inverse%2520Problems%26entry.906535625%3DGuixian%2520Xu%2520and%2520Jinglai%2520Li%2520and%2520Junqi%2520Tang%26entry.1292438233%3D%2520%2520Equivariant%2520Imaging%2520%2528EI%2529%2520regularization%2520has%2520become%2520the%2520de-facto%2520technique%2520for%250Aunsupervised%2520training%2520of%2520deep%2520imaging%2520networks%252C%2520without%2520any%2520need%2520of%250Aground-truth%2520data.%2520Observing%2520that%2520the%2520EI-based%2520unsupervised%2520training%2520paradigm%250Acurrently%2520has%2520significant%2520computational%2520redundancy%2520leading%2520to%2520inefficiency%2520in%250Ahigh-dimensional%2520applications%252C%2520we%2520propose%2520a%2520sketched%2520EI%2520regularization%2520which%250Aleverages%2520the%2520randomized%2520sketching%2520techniques%2520for%2520acceleration.%2520We%2520then%2520extend%250Aour%2520sketched%2520EI%2520regularization%2520to%2520develop%2520an%2520accelerated%2520deep%2520internal%2520learning%250Aframework%2520--%2520Sketched%2520Equivariant%2520Deep%2520Image%2520Prior%2520%2528Sk-EI-DIP%2529%252C%2520which%2520can%2520be%250Aefficiently%2520applied%2520for%2520single-image%2520and%2520task-adapted%2520reconstruction.%250AAdditionally%252C%2520for%2520network%2520adaptation%2520tasks%252C%2520we%2520propose%2520a%2520parameter-efficient%250Aapproach%2520for%2520accelerating%2520both%2520EI-DIP%2520and%2520Sk-EI-DIP%2520via%2520optimizing%2520only%2520the%250Anormalization%2520layers.%2520Our%2520numerical%2520study%2520on%2520X-ray%2520CT%2520image%2520reconstruction%250Atasks%2520demonstrate%2520that%2520our%2520approach%2520can%2520achieve%2520order-of-magnitude%250Acomputational%2520acceleration%2520over%2520standard%2520EI-based%2520counterpart%2520in%2520single-input%250Asetting%252C%2520and%2520network%2520adaptation%2520at%2520test%2520time.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2411.05771v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Sketched%20Equivariant%20Imaging%20Regularization%20and%20Deep%20Internal%20Learning%0A%20%20for%20Inverse%20Problems&entry.906535625=Guixian%20Xu%20and%20Jinglai%20Li%20and%20Junqi%20Tang&entry.1292438233=%20%20Equivariant%20Imaging%20%28EI%29%20regularization%20has%20become%20the%20de-facto%20technique%20for%0Aunsupervised%20training%20of%20deep%20imaging%20networks%2C%20without%20any%20need%20of%0Aground-truth%20data.%20Observing%20that%20the%20EI-based%20unsupervised%20training%20paradigm%0Acurrently%20has%20significant%20computational%20redundancy%20leading%20to%20inefficiency%20in%0Ahigh-dimensional%20applications%2C%20we%20propose%20a%20sketched%20EI%20regularization%20which%0Aleverages%20the%20randomized%20sketching%20techniques%20for%20acceleration.%20We%20then%20extend%0Aour%20sketched%20EI%20regularization%20to%20develop%20an%20accelerated%20deep%20internal%20learning%0Aframework%20--%20Sketched%20Equivariant%20Deep%20Image%20Prior%20%28Sk-EI-DIP%29%2C%20which%20can%20be%0Aefficiently%20applied%20for%20single-image%20and%20task-adapted%20reconstruction.%0AAdditionally%2C%20for%20network%20adaptation%20tasks%2C%20we%20propose%20a%20parameter-efficient%0Aapproach%20for%20accelerating%20both%20EI-DIP%20and%20Sk-EI-DIP%20via%20optimizing%20only%20the%0Anormalization%20layers.%20Our%20numerical%20study%20on%20X-ray%20CT%20image%20reconstruction%0Atasks%20demonstrate%20that%20our%20approach%20can%20achieve%20order-of-magnitude%0Acomputational%20acceleration%20over%20standard%20EI-based%20counterpart%20in%20single-input%0Asetting%2C%20and%20network%20adaptation%20at%20test%20time.%0A&entry.1838667208=http%3A//arxiv.org/abs/2411.05771v2&entry.124074799=Read"},
{"title": "Random Fourier Signature Features", "author": "Csaba Toth and Harald Oberhauser and Zoltan Szabo", "abstract": "  Tensor algebras give rise to one of the most powerful measures of similarity\nfor sequences of arbitrary length called the signature kernel accompanied with\nattractive theoretical guarantees from stochastic analysis. Previous algorithms\nto compute the signature kernel scale quadratically in terms of the length and\nthe number of the sequences. To mitigate this severe computational bottleneck,\nwe develop a random Fourier feature-based acceleration of the signature kernel\nacting on the inherently non-Euclidean domain of sequences. We show uniform\napproximation guarantees for the proposed unbiased estimator of the signature\nkernel, while keeping its computation linear in the sequence length and number.\nIn addition, combined with recent advances on tensor projections, we derive two\neven more scalable time series features with favourable concentration\nproperties and computational complexity both in time and memory. Our empirical\nresults show that the reduction in computational cost comes at a negligible\nprice in terms of accuracy on moderate-sized datasets, and it enables one to\nscale to large datasets up to a million time series.\n", "link": "http://arxiv.org/abs/2311.12214v2", "date": "2024-11-22", "relevancy": 2.1736, "topK": [{"title": "Breadcrumbs: Adversarial class-balanced sampling for long-tailed recognition", "link": "https://link.springer.com/chapter/10.1007/978-3-031-20053-3_37", "similarity": 0.4629}, {"title": "ToDo: Token Downsampling for Efficient Generation of High-Resolution\n  Images", "link": "http://arxiv.org/abs/2402.13573v2", "similarity": 0.4329}, {"title": "RTG-SLAM: Real-time 3D Reconstruction at Scale using Gaussian Splatting", "link": "http://arxiv.org/abs/2404.19706v1", "similarity": 0.4084}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Random%20Fourier%20Signature%20Features&body=Title%3A%20Random%20Fourier%20Signature%20Features%0AAuthor%3A%20Csaba%20Toth%20and%20Harald%20Oberhauser%20and%20Zoltan%20Szabo%0AAbstract%3A%20%20%20Tensor%20algebras%20give%20rise%20to%20one%20of%20the%20most%20powerful%20measures%20of%20similarity%0Afor%20sequences%20of%20arbitrary%20length%20called%20the%20signature%20kernel%20accompanied%20with%0Aattractive%20theoretical%20guarantees%20from%20stochastic%20analysis.%20Previous%20algorithms%0Ato%20compute%20the%20signature%20kernel%20scale%20quadratically%20in%20terms%20of%20the%20length%20and%0Athe%20number%20of%20the%20sequences.%20To%20mitigate%20this%20severe%20computational%20bottleneck%2C%0Awe%20develop%20a%20random%20Fourier%20feature-based%20acceleration%20of%20the%20signature%20kernel%0Aacting%20on%20the%20inherently%20non-Euclidean%20domain%20of%20sequences.%20We%20show%20uniform%0Aapproximation%20guarantees%20for%20the%20proposed%20unbiased%20estimator%20of%20the%20signature%0Akernel%2C%20while%20keeping%20its%20computation%20linear%20in%20the%20sequence%20length%20and%20number.%0AIn%20addition%2C%20combined%20with%20recent%20advances%20on%20tensor%20projections%2C%20we%20derive%20two%0Aeven%20more%20scalable%20time%20series%20features%20with%20favourable%20concentration%0Aproperties%20and%20computational%20complexity%20both%20in%20time%20and%20memory.%20Our%20empirical%0Aresults%20show%20that%20the%20reduction%20in%20computational%20cost%20comes%20at%20a%20negligible%0Aprice%20in%20terms%20of%20accuracy%20on%20moderate-sized%20datasets%2C%20and%20it%20enables%20one%20to%0Ascale%20to%20large%20datasets%20up%20to%20a%20million%20time%20series.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2311.12214v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DRandom%2520Fourier%2520Signature%2520Features%26entry.906535625%3DCsaba%2520Toth%2520and%2520Harald%2520Oberhauser%2520and%2520Zoltan%2520Szabo%26entry.1292438233%3D%2520%2520Tensor%2520algebras%2520give%2520rise%2520to%2520one%2520of%2520the%2520most%2520powerful%2520measures%2520of%2520similarity%250Afor%2520sequences%2520of%2520arbitrary%2520length%2520called%2520the%2520signature%2520kernel%2520accompanied%2520with%250Aattractive%2520theoretical%2520guarantees%2520from%2520stochastic%2520analysis.%2520Previous%2520algorithms%250Ato%2520compute%2520the%2520signature%2520kernel%2520scale%2520quadratically%2520in%2520terms%2520of%2520the%2520length%2520and%250Athe%2520number%2520of%2520the%2520sequences.%2520To%2520mitigate%2520this%2520severe%2520computational%2520bottleneck%252C%250Awe%2520develop%2520a%2520random%2520Fourier%2520feature-based%2520acceleration%2520of%2520the%2520signature%2520kernel%250Aacting%2520on%2520the%2520inherently%2520non-Euclidean%2520domain%2520of%2520sequences.%2520We%2520show%2520uniform%250Aapproximation%2520guarantees%2520for%2520the%2520proposed%2520unbiased%2520estimator%2520of%2520the%2520signature%250Akernel%252C%2520while%2520keeping%2520its%2520computation%2520linear%2520in%2520the%2520sequence%2520length%2520and%2520number.%250AIn%2520addition%252C%2520combined%2520with%2520recent%2520advances%2520on%2520tensor%2520projections%252C%2520we%2520derive%2520two%250Aeven%2520more%2520scalable%2520time%2520series%2520features%2520with%2520favourable%2520concentration%250Aproperties%2520and%2520computational%2520complexity%2520both%2520in%2520time%2520and%2520memory.%2520Our%2520empirical%250Aresults%2520show%2520that%2520the%2520reduction%2520in%2520computational%2520cost%2520comes%2520at%2520a%2520negligible%250Aprice%2520in%2520terms%2520of%2520accuracy%2520on%2520moderate-sized%2520datasets%252C%2520and%2520it%2520enables%2520one%2520to%250Ascale%2520to%2520large%2520datasets%2520up%2520to%2520a%2520million%2520time%2520series.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2311.12214v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Random%20Fourier%20Signature%20Features&entry.906535625=Csaba%20Toth%20and%20Harald%20Oberhauser%20and%20Zoltan%20Szabo&entry.1292438233=%20%20Tensor%20algebras%20give%20rise%20to%20one%20of%20the%20most%20powerful%20measures%20of%20similarity%0Afor%20sequences%20of%20arbitrary%20length%20called%20the%20signature%20kernel%20accompanied%20with%0Aattractive%20theoretical%20guarantees%20from%20stochastic%20analysis.%20Previous%20algorithms%0Ato%20compute%20the%20signature%20kernel%20scale%20quadratically%20in%20terms%20of%20the%20length%20and%0Athe%20number%20of%20the%20sequences.%20To%20mitigate%20this%20severe%20computational%20bottleneck%2C%0Awe%20develop%20a%20random%20Fourier%20feature-based%20acceleration%20of%20the%20signature%20kernel%0Aacting%20on%20the%20inherently%20non-Euclidean%20domain%20of%20sequences.%20We%20show%20uniform%0Aapproximation%20guarantees%20for%20the%20proposed%20unbiased%20estimator%20of%20the%20signature%0Akernel%2C%20while%20keeping%20its%20computation%20linear%20in%20the%20sequence%20length%20and%20number.%0AIn%20addition%2C%20combined%20with%20recent%20advances%20on%20tensor%20projections%2C%20we%20derive%20two%0Aeven%20more%20scalable%20time%20series%20features%20with%20favourable%20concentration%0Aproperties%20and%20computational%20complexity%20both%20in%20time%20and%20memory.%20Our%20empirical%0Aresults%20show%20that%20the%20reduction%20in%20computational%20cost%20comes%20at%20a%20negligible%0Aprice%20in%20terms%20of%20accuracy%20on%20moderate-sized%20datasets%2C%20and%20it%20enables%20one%20to%0Ascale%20to%20large%20datasets%20up%20to%20a%20million%20time%20series.%0A&entry.1838667208=http%3A//arxiv.org/abs/2311.12214v2&entry.124074799=Read"},
{"title": "LiDAR-based End-to-end Temporal Perception for Vehicle-Infrastructure\n  Cooperation", "author": "Zhenwei Yang and Jilei Mao and Wenxian Yang and Yibo Ai and Yu Kong and Haibao Yu and Weidong Zhang", "abstract": "  Temporal perception, the ability to detect and track objects over time, is\ncritical in autonomous driving for maintaining a comprehensive understanding of\ndynamic environments. However, this task is hindered by significant challenges,\nincluding incomplete perception caused by occluded objects and observational\nblind spots, which are common in single-vehicle perception systems. To address\nthese issues, we introduce LET-VIC, a LiDAR-based End-to-End Tracking framework\nfor Vehicle-Infrastructure Cooperation (VIC). LET-VIC leverages\nVehicle-to-Everything (V2X) communication to enhance temporal perception by\nfusing spatial and temporal data from both vehicle and infrastructure sensors.\nFirst, it spatially integrates Bird's Eye View (BEV) features from vehicle-side\nand infrastructure-side LiDAR data, creating a comprehensive view that\nmitigates occlusions and compensates for blind spots. Second, LET-VIC\nincorporates temporal context across frames, allowing the model to leverage\nhistorical data for enhanced tracking stability and accuracy. To further\nimprove robustness, LET-VIC includes a Calibration Error Compensation (CEC)\nmodule to address sensor misalignments and ensure precise feature alignment.\nExperiments on the V2X-Seq-SPD dataset demonstrate that LET-VIC significantly\noutperforms baseline models, achieving at least a 13.7% improvement in mAP and\na 13.1% improvement in AMOTA without considering communication delays. This\nwork offers a practical solution and a new research direction for advancing\ntemporal perception in autonomous driving through vehicle-infrastructure\ncooperation.\n", "link": "http://arxiv.org/abs/2411.14927v1", "date": "2024-11-22", "relevancy": 2.1597, "topK": [{"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.5681}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.5201}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5198}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20LiDAR-based%20End-to-end%20Temporal%20Perception%20for%20Vehicle-Infrastructure%0A%20%20Cooperation&body=Title%3A%20LiDAR-based%20End-to-end%20Temporal%20Perception%20for%20Vehicle-Infrastructure%0A%20%20Cooperation%0AAuthor%3A%20Zhenwei%20Yang%20and%20Jilei%20Mao%20and%20Wenxian%20Yang%20and%20Yibo%20Ai%20and%20Yu%20Kong%20and%20Haibao%20Yu%20and%20Weidong%20Zhang%0AAbstract%3A%20%20%20Temporal%20perception%2C%20the%20ability%20to%20detect%20and%20track%20objects%20over%20time%2C%20is%0Acritical%20in%20autonomous%20driving%20for%20maintaining%20a%20comprehensive%20understanding%20of%0Adynamic%20environments.%20However%2C%20this%20task%20is%20hindered%20by%20significant%20challenges%2C%0Aincluding%20incomplete%20perception%20caused%20by%20occluded%20objects%20and%20observational%0Ablind%20spots%2C%20which%20are%20common%20in%20single-vehicle%20perception%20systems.%20To%20address%0Athese%20issues%2C%20we%20introduce%20LET-VIC%2C%20a%20LiDAR-based%20End-to-End%20Tracking%20framework%0Afor%20Vehicle-Infrastructure%20Cooperation%20%28VIC%29.%20LET-VIC%20leverages%0AVehicle-to-Everything%20%28V2X%29%20communication%20to%20enhance%20temporal%20perception%20by%0Afusing%20spatial%20and%20temporal%20data%20from%20both%20vehicle%20and%20infrastructure%20sensors.%0AFirst%2C%20it%20spatially%20integrates%20Bird%27s%20Eye%20View%20%28BEV%29%20features%20from%20vehicle-side%0Aand%20infrastructure-side%20LiDAR%20data%2C%20creating%20a%20comprehensive%20view%20that%0Amitigates%20occlusions%20and%20compensates%20for%20blind%20spots.%20Second%2C%20LET-VIC%0Aincorporates%20temporal%20context%20across%20frames%2C%20allowing%20the%20model%20to%20leverage%0Ahistorical%20data%20for%20enhanced%20tracking%20stability%20and%20accuracy.%20To%20further%0Aimprove%20robustness%2C%20LET-VIC%20includes%20a%20Calibration%20Error%20Compensation%20%28CEC%29%0Amodule%20to%20address%20sensor%20misalignments%20and%20ensure%20precise%20feature%20alignment.%0AExperiments%20on%20the%20V2X-Seq-SPD%20dataset%20demonstrate%20that%20LET-VIC%20significantly%0Aoutperforms%20baseline%20models%2C%20achieving%20at%20least%20a%2013.7%25%20improvement%20in%20mAP%20and%0Aa%2013.1%25%20improvement%20in%20AMOTA%20without%20considering%20communication%20delays.%20This%0Awork%20offers%20a%20practical%20solution%20and%20a%20new%20research%20direction%20for%20advancing%0Atemporal%20perception%20in%20autonomous%20driving%20through%20vehicle-infrastructure%0Acooperation.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2411.14927v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DLiDAR-based%2520End-to-end%2520Temporal%2520Perception%2520for%2520Vehicle-Infrastructure%250A%2520%2520Cooperation%26entry.906535625%3DZhenwei%2520Yang%2520and%2520Jilei%2520Mao%2520and%2520Wenxian%2520Yang%2520and%2520Yibo%2520Ai%2520and%2520Yu%2520Kong%2520and%2520Haibao%2520Yu%2520and%2520Weidong%2520Zhang%26entry.1292438233%3D%2520%2520Temporal%2520perception%252C%2520the%2520ability%2520to%2520detect%2520and%2520track%2520objects%2520over%2520time%252C%2520is%250Acritical%2520in%2520autonomous%2520driving%2520for%2520maintaining%2520a%2520comprehensive%2520understanding%2520of%250Adynamic%2520environments.%2520However%252C%2520this%2520task%2520is%2520hindered%2520by%2520significant%2520challenges%252C%250Aincluding%2520incomplete%2520perception%2520caused%2520by%2520occluded%2520objects%2520and%2520observational%250Ablind%2520spots%252C%2520which%2520are%2520common%2520in%2520single-vehicle%2520perception%2520systems.%2520To%2520address%250Athese%2520issues%252C%2520we%2520introduce%2520LET-VIC%252C%2520a%2520LiDAR-based%2520End-to-End%2520Tracking%2520framework%250Afor%2520Vehicle-Infrastructure%2520Cooperation%2520%2528VIC%2529.%2520LET-VIC%2520leverages%250AVehicle-to-Everything%2520%2528V2X%2529%2520communication%2520to%2520enhance%2520temporal%2520perception%2520by%250Afusing%2520spatial%2520and%2520temporal%2520data%2520from%2520both%2520vehicle%2520and%2520infrastructure%2520sensors.%250AFirst%252C%2520it%2520spatially%2520integrates%2520Bird%2527s%2520Eye%2520View%2520%2528BEV%2529%2520features%2520from%2520vehicle-side%250Aand%2520infrastructure-side%2520LiDAR%2520data%252C%2520creating%2520a%2520comprehensive%2520view%2520that%250Amitigates%2520occlusions%2520and%2520compensates%2520for%2520blind%2520spots.%2520Second%252C%2520LET-VIC%250Aincorporates%2520temporal%2520context%2520across%2520frames%252C%2520allowing%2520the%2520model%2520to%2520leverage%250Ahistorical%2520data%2520for%2520enhanced%2520tracking%2520stability%2520and%2520accuracy.%2520To%2520further%250Aimprove%2520robustness%252C%2520LET-VIC%2520includes%2520a%2520Calibration%2520Error%2520Compensation%2520%2528CEC%2529%250Amodule%2520to%2520address%2520sensor%2520misalignments%2520and%2520ensure%2520precise%2520feature%2520alignment.%250AExperiments%2520on%2520the%2520V2X-Seq-SPD%2520dataset%2520demonstrate%2520that%2520LET-VIC%2520significantly%250Aoutperforms%2520baseline%2520models%252C%2520achieving%2520at%2520least%2520a%252013.7%2525%2520improvement%2520in%2520mAP%2520and%250Aa%252013.1%2525%2520improvement%2520in%2520AMOTA%2520without%2520considering%2520communication%2520delays.%2520This%250Awork%2520offers%2520a%2520practical%2520solution%2520and%2520a%2520new%2520research%2520direction%2520for%2520advancing%250Atemporal%2520perception%2520in%2520autonomous%2520driving%2520through%2520vehicle-infrastructure%250Acooperation.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2411.14927v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=LiDAR-based%20End-to-end%20Temporal%20Perception%20for%20Vehicle-Infrastructure%0A%20%20Cooperation&entry.906535625=Zhenwei%20Yang%20and%20Jilei%20Mao%20and%20Wenxian%20Yang%20and%20Yibo%20Ai%20and%20Yu%20Kong%20and%20Haibao%20Yu%20and%20Weidong%20Zhang&entry.1292438233=%20%20Temporal%20perception%2C%20the%20ability%20to%20detect%20and%20track%20objects%20over%20time%2C%20is%0Acritical%20in%20autonomous%20driving%20for%20maintaining%20a%20comprehensive%20understanding%20of%0Adynamic%20environments.%20However%2C%20this%20task%20is%20hindered%20by%20significant%20challenges%2C%0Aincluding%20incomplete%20perception%20caused%20by%20occluded%20objects%20and%20observational%0Ablind%20spots%2C%20which%20are%20common%20in%20single-vehicle%20perception%20systems.%20To%20address%0Athese%20issues%2C%20we%20introduce%20LET-VIC%2C%20a%20LiDAR-based%20End-to-End%20Tracking%20framework%0Afor%20Vehicle-Infrastructure%20Cooperation%20%28VIC%29.%20LET-VIC%20leverages%0AVehicle-to-Everything%20%28V2X%29%20communication%20to%20enhance%20temporal%20perception%20by%0Afusing%20spatial%20and%20temporal%20data%20from%20both%20vehicle%20and%20infrastructure%20sensors.%0AFirst%2C%20it%20spatially%20integrates%20Bird%27s%20Eye%20View%20%28BEV%29%20features%20from%20vehicle-side%0Aand%20infrastructure-side%20LiDAR%20data%2C%20creating%20a%20comprehensive%20view%20that%0Amitigates%20occlusions%20and%20compensates%20for%20blind%20spots.%20Second%2C%20LET-VIC%0Aincorporates%20temporal%20context%20across%20frames%2C%20allowing%20the%20model%20to%20leverage%0Ahistorical%20data%20for%20enhanced%20tracking%20stability%20and%20accuracy.%20To%20further%0Aimprove%20robustness%2C%20LET-VIC%20includes%20a%20Calibration%20Error%20Compensation%20%28CEC%29%0Amodule%20to%20address%20sensor%20misalignments%20and%20ensure%20precise%20feature%20alignment.%0AExperiments%20on%20the%20V2X-Seq-SPD%20dataset%20demonstrate%20that%20LET-VIC%20significantly%0Aoutperforms%20baseline%20models%2C%20achieving%20at%20least%20a%2013.7%25%20improvement%20in%20mAP%20and%0Aa%2013.1%25%20improvement%20in%20AMOTA%20without%20considering%20communication%20delays.%20This%0Awork%20offers%20a%20practical%20solution%20and%20a%20new%20research%20direction%20for%20advancing%0Atemporal%20perception%20in%20autonomous%20driving%20through%20vehicle-infrastructure%0Acooperation.%0A&entry.1838667208=http%3A//arxiv.org/abs/2411.14927v1&entry.124074799=Read"},
{"title": "Empowering Clients: Transformation of Design Processes Due to Generative\n  AI", "author": "Johannes Schneider and Kilic Sinem and Daniel Stockhammer", "abstract": "  The domain of computational design, driven by advancements in Generative AI,\nis transforming creative fields. We explore the transformative effects of\nGenerative AI on the architectural design process and discuss the role of the\narchitect. The case of architecture is interesting as designing houses is\ncomplex, involving extensive customer interaction. We employ a within-subject\nexperiment using a popular general-purpose text-to-image tool for generating\ndesigns and providing feedback on existing designs, followed by expert\ninterviews. The study reveals that AI can disrupt the ideation phase by\nenabling clients to engage in the design process through rapid visualization of\ntheir own ideas. In turn, the architect's role shifts more towards assessing\nthe feasibility of designs generated conjointly by clients and AI. Our study\nalso shows that while AI can provide valuable feedback on designs, it might\nfail to generate such designs, allowing for interesting connections to\nfoundations in computer science, i.e., NP-completeness. AI's feedback also\ntends to hamper creativity and innovation by suggesting altering novel,\ninnovative approaches toward more standardized designs. Our study also reveals\nthat there is uncertainty among architects about the interpretative sovereignty\nof architecture and loss of meaning and identity when AI increasingly takes\nover authorship in the design process.\n", "link": "http://arxiv.org/abs/2411.15061v1", "date": "2024-11-22", "relevancy": 2.1417, "topK": [{"title": "DressCode: Autoregressively Sewing and Generating Garments from Text\n  Guidance", "link": "http://arxiv.org/abs/2401.16465v3", "similarity": 0.5399}, {"title": "Genie: Generative Interactive Environments", "link": "http://arxiv.org/abs/2402.15391v1", "similarity": 0.5352}, {"title": "Thinking Outside the BBox: Unconstrained Generative Object Compositing", "link": "http://arxiv.org/abs/2409.04559v2", "similarity": 0.5248}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Empowering%20Clients%3A%20Transformation%20of%20Design%20Processes%20Due%20to%20Generative%0A%20%20AI&body=Title%3A%20Empowering%20Clients%3A%20Transformation%20of%20Design%20Processes%20Due%20to%20Generative%0A%20%20AI%0AAuthor%3A%20Johannes%20Schneider%20and%20Kilic%20Sinem%20and%20Daniel%20Stockhammer%0AAbstract%3A%20%20%20The%20domain%20of%20computational%20design%2C%20driven%20by%20advancements%20in%20Generative%20AI%2C%0Ais%20transforming%20creative%20fields.%20We%20explore%20the%20transformative%20effects%20of%0AGenerative%20AI%20on%20the%20architectural%20design%20process%20and%20discuss%20the%20role%20of%20the%0Aarchitect.%20The%20case%20of%20architecture%20is%20interesting%20as%20designing%20houses%20is%0Acomplex%2C%20involving%20extensive%20customer%20interaction.%20We%20employ%20a%20within-subject%0Aexperiment%20using%20a%20popular%20general-purpose%20text-to-image%20tool%20for%20generating%0Adesigns%20and%20providing%20feedback%20on%20existing%20designs%2C%20followed%20by%20expert%0Ainterviews.%20The%20study%20reveals%20that%20AI%20can%20disrupt%20the%20ideation%20phase%20by%0Aenabling%20clients%20to%20engage%20in%20the%20design%20process%20through%20rapid%20visualization%20of%0Atheir%20own%20ideas.%20In%20turn%2C%20the%20architect%27s%20role%20shifts%20more%20towards%20assessing%0Athe%20feasibility%20of%20designs%20generated%20conjointly%20by%20clients%20and%20AI.%20Our%20study%0Aalso%20shows%20that%20while%20AI%20can%20provide%20valuable%20feedback%20on%20designs%2C%20it%20might%0Afail%20to%20generate%20such%20designs%2C%20allowing%20for%20interesting%20connections%20to%0Afoundations%20in%20computer%20science%2C%20i.e.%2C%20NP-completeness.%20AI%27s%20feedback%20also%0Atends%20to%20hamper%20creativity%20and%20innovation%20by%20suggesting%20altering%20novel%2C%0Ainnovative%20approaches%20toward%20more%20standardized%20designs.%20Our%20study%20also%20reveals%0Athat%20there%20is%20uncertainty%20among%20architects%20about%20the%20interpretative%20sovereignty%0Aof%20architecture%20and%20loss%20of%20meaning%20and%20identity%20when%20AI%20increasingly%20takes%0Aover%20authorship%20in%20the%20design%20process.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2411.15061v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DEmpowering%2520Clients%253A%2520Transformation%2520of%2520Design%2520Processes%2520Due%2520to%2520Generative%250A%2520%2520AI%26entry.906535625%3DJohannes%2520Schneider%2520and%2520Kilic%2520Sinem%2520and%2520Daniel%2520Stockhammer%26entry.1292438233%3D%2520%2520The%2520domain%2520of%2520computational%2520design%252C%2520driven%2520by%2520advancements%2520in%2520Generative%2520AI%252C%250Ais%2520transforming%2520creative%2520fields.%2520We%2520explore%2520the%2520transformative%2520effects%2520of%250AGenerative%2520AI%2520on%2520the%2520architectural%2520design%2520process%2520and%2520discuss%2520the%2520role%2520of%2520the%250Aarchitect.%2520The%2520case%2520of%2520architecture%2520is%2520interesting%2520as%2520designing%2520houses%2520is%250Acomplex%252C%2520involving%2520extensive%2520customer%2520interaction.%2520We%2520employ%2520a%2520within-subject%250Aexperiment%2520using%2520a%2520popular%2520general-purpose%2520text-to-image%2520tool%2520for%2520generating%250Adesigns%2520and%2520providing%2520feedback%2520on%2520existing%2520designs%252C%2520followed%2520by%2520expert%250Ainterviews.%2520The%2520study%2520reveals%2520that%2520AI%2520can%2520disrupt%2520the%2520ideation%2520phase%2520by%250Aenabling%2520clients%2520to%2520engage%2520in%2520the%2520design%2520process%2520through%2520rapid%2520visualization%2520of%250Atheir%2520own%2520ideas.%2520In%2520turn%252C%2520the%2520architect%2527s%2520role%2520shifts%2520more%2520towards%2520assessing%250Athe%2520feasibility%2520of%2520designs%2520generated%2520conjointly%2520by%2520clients%2520and%2520AI.%2520Our%2520study%250Aalso%2520shows%2520that%2520while%2520AI%2520can%2520provide%2520valuable%2520feedback%2520on%2520designs%252C%2520it%2520might%250Afail%2520to%2520generate%2520such%2520designs%252C%2520allowing%2520for%2520interesting%2520connections%2520to%250Afoundations%2520in%2520computer%2520science%252C%2520i.e.%252C%2520NP-completeness.%2520AI%2527s%2520feedback%2520also%250Atends%2520to%2520hamper%2520creativity%2520and%2520innovation%2520by%2520suggesting%2520altering%2520novel%252C%250Ainnovative%2520approaches%2520toward%2520more%2520standardized%2520designs.%2520Our%2520study%2520also%2520reveals%250Athat%2520there%2520is%2520uncertainty%2520among%2520architects%2520about%2520the%2520interpretative%2520sovereignty%250Aof%2520architecture%2520and%2520loss%2520of%2520meaning%2520and%2520identity%2520when%2520AI%2520increasingly%2520takes%250Aover%2520authorship%2520in%2520the%2520design%2520process.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2411.15061v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Empowering%20Clients%3A%20Transformation%20of%20Design%20Processes%20Due%20to%20Generative%0A%20%20AI&entry.906535625=Johannes%20Schneider%20and%20Kilic%20Sinem%20and%20Daniel%20Stockhammer&entry.1292438233=%20%20The%20domain%20of%20computational%20design%2C%20driven%20by%20advancements%20in%20Generative%20AI%2C%0Ais%20transforming%20creative%20fields.%20We%20explore%20the%20transformative%20effects%20of%0AGenerative%20AI%20on%20the%20architectural%20design%20process%20and%20discuss%20the%20role%20of%20the%0Aarchitect.%20The%20case%20of%20architecture%20is%20interesting%20as%20designing%20houses%20is%0Acomplex%2C%20involving%20extensive%20customer%20interaction.%20We%20employ%20a%20within-subject%0Aexperiment%20using%20a%20popular%20general-purpose%20text-to-image%20tool%20for%20generating%0Adesigns%20and%20providing%20feedback%20on%20existing%20designs%2C%20followed%20by%20expert%0Ainterviews.%20The%20study%20reveals%20that%20AI%20can%20disrupt%20the%20ideation%20phase%20by%0Aenabling%20clients%20to%20engage%20in%20the%20design%20process%20through%20rapid%20visualization%20of%0Atheir%20own%20ideas.%20In%20turn%2C%20the%20architect%27s%20role%20shifts%20more%20towards%20assessing%0Athe%20feasibility%20of%20designs%20generated%20conjointly%20by%20clients%20and%20AI.%20Our%20study%0Aalso%20shows%20that%20while%20AI%20can%20provide%20valuable%20feedback%20on%20designs%2C%20it%20might%0Afail%20to%20generate%20such%20designs%2C%20allowing%20for%20interesting%20connections%20to%0Afoundations%20in%20computer%20science%2C%20i.e.%2C%20NP-completeness.%20AI%27s%20feedback%20also%0Atends%20to%20hamper%20creativity%20and%20innovation%20by%20suggesting%20altering%20novel%2C%0Ainnovative%20approaches%20toward%20more%20standardized%20designs.%20Our%20study%20also%20reveals%0Athat%20there%20is%20uncertainty%20among%20architects%20about%20the%20interpretative%20sovereignty%0Aof%20architecture%20and%20loss%20of%20meaning%20and%20identity%20when%20AI%20increasingly%20takes%0Aover%20authorship%20in%20the%20design%20process.%0A&entry.1838667208=http%3A//arxiv.org/abs/2411.15061v1&entry.124074799=Read"},
{"title": "Bi-level Trajectory Optimization on Uneven Terrains with Differentiable\n  Wheel-Terrain Interaction Model", "author": "Amith Manoharan and Aditya Sharma and Himani Belsare and Kaustab Pal and K. Madhava Krishna and Arun Kumar Singh", "abstract": "  Navigation of wheeled vehicles on uneven terrain necessitates going beyond\nthe 2D approaches for trajectory planning. Specifically, it is essential to\nincorporate the full 6dof variation of vehicle pose and its associated\nstability cost in the planning process. To this end, most recent works aim to\nlearn a neural network model to predict the vehicle evolution. However, such\napproaches are data-intensive and fraught with generalization issues. In this\npaper, we present a purely model-based approach that just requires the digital\nelevation information of the terrain. Specifically, we express the\nwheel-terrain interaction and 6dof pose prediction as a non-linear least\nsquares (NLS) problem. As a result, trajectory planning can be viewed as a\nbi-level optimization. The inner optimization layer predicts the pose on the\nterrain along a given trajectory, while the outer layer deforms the trajectory\nitself to reduce the stability and kinematic costs of the pose. We improve the\nstate-of-the-art in the following respects. First, we show that our NLS based\npose prediction closely matches the output from a high-fidelity physics engine.\nThis result coupled with the fact that we can query gradients of the NLS\nsolver, makes our pose predictor, a differentiable wheel-terrain interaction\nmodel. We further leverage this differentiability to efficiently solve the\nproposed bi-level trajectory optimization problem. Finally, we perform\nextensive experiments, and comparison with a baseline to showcase the\neffectiveness of our approach in obtaining smooth, stable trajectories.\n", "link": "http://arxiv.org/abs/2404.03307v3", "date": "2024-11-22", "relevancy": 2.1401, "topK": [{"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.5798}, {"title": "Egocentric Computer Vision for Hands-Free Robotic Wheelchair Navigation", "link": "https://link.springer.com/article/10.1007/s10846-023-01807-4", "similarity": 0.561}, {"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.4912}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Bi-level%20Trajectory%20Optimization%20on%20Uneven%20Terrains%20with%20Differentiable%0A%20%20Wheel-Terrain%20Interaction%20Model&body=Title%3A%20Bi-level%20Trajectory%20Optimization%20on%20Uneven%20Terrains%20with%20Differentiable%0A%20%20Wheel-Terrain%20Interaction%20Model%0AAuthor%3A%20Amith%20Manoharan%20and%20Aditya%20Sharma%20and%20Himani%20Belsare%20and%20Kaustab%20Pal%20and%20K.%20Madhava%20Krishna%20and%20Arun%20Kumar%20Singh%0AAbstract%3A%20%20%20Navigation%20of%20wheeled%20vehicles%20on%20uneven%20terrain%20necessitates%20going%20beyond%0Athe%202D%20approaches%20for%20trajectory%20planning.%20Specifically%2C%20it%20is%20essential%20to%0Aincorporate%20the%20full%206dof%20variation%20of%20vehicle%20pose%20and%20its%20associated%0Astability%20cost%20in%20the%20planning%20process.%20To%20this%20end%2C%20most%20recent%20works%20aim%20to%0Alearn%20a%20neural%20network%20model%20to%20predict%20the%20vehicle%20evolution.%20However%2C%20such%0Aapproaches%20are%20data-intensive%20and%20fraught%20with%20generalization%20issues.%20In%20this%0Apaper%2C%20we%20present%20a%20purely%20model-based%20approach%20that%20just%20requires%20the%20digital%0Aelevation%20information%20of%20the%20terrain.%20Specifically%2C%20we%20express%20the%0Awheel-terrain%20interaction%20and%206dof%20pose%20prediction%20as%20a%20non-linear%20least%0Asquares%20%28NLS%29%20problem.%20As%20a%20result%2C%20trajectory%20planning%20can%20be%20viewed%20as%20a%0Abi-level%20optimization.%20The%20inner%20optimization%20layer%20predicts%20the%20pose%20on%20the%0Aterrain%20along%20a%20given%20trajectory%2C%20while%20the%20outer%20layer%20deforms%20the%20trajectory%0Aitself%20to%20reduce%20the%20stability%20and%20kinematic%20costs%20of%20the%20pose.%20We%20improve%20the%0Astate-of-the-art%20in%20the%20following%20respects.%20First%2C%20we%20show%20that%20our%20NLS%20based%0Apose%20prediction%20closely%20matches%20the%20output%20from%20a%20high-fidelity%20physics%20engine.%0AThis%20result%20coupled%20with%20the%20fact%20that%20we%20can%20query%20gradients%20of%20the%20NLS%0Asolver%2C%20makes%20our%20pose%20predictor%2C%20a%20differentiable%20wheel-terrain%20interaction%0Amodel.%20We%20further%20leverage%20this%20differentiability%20to%20efficiently%20solve%20the%0Aproposed%20bi-level%20trajectory%20optimization%20problem.%20Finally%2C%20we%20perform%0Aextensive%20experiments%2C%20and%20comparison%20with%20a%20baseline%20to%20showcase%20the%0Aeffectiveness%20of%20our%20approach%20in%20obtaining%20smooth%2C%20stable%20trajectories.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2404.03307v3%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DBi-level%2520Trajectory%2520Optimization%2520on%2520Uneven%2520Terrains%2520with%2520Differentiable%250A%2520%2520Wheel-Terrain%2520Interaction%2520Model%26entry.906535625%3DAmith%2520Manoharan%2520and%2520Aditya%2520Sharma%2520and%2520Himani%2520Belsare%2520and%2520Kaustab%2520Pal%2520and%2520K.%2520Madhava%2520Krishna%2520and%2520Arun%2520Kumar%2520Singh%26entry.1292438233%3D%2520%2520Navigation%2520of%2520wheeled%2520vehicles%2520on%2520uneven%2520terrain%2520necessitates%2520going%2520beyond%250Athe%25202D%2520approaches%2520for%2520trajectory%2520planning.%2520Specifically%252C%2520it%2520is%2520essential%2520to%250Aincorporate%2520the%2520full%25206dof%2520variation%2520of%2520vehicle%2520pose%2520and%2520its%2520associated%250Astability%2520cost%2520in%2520the%2520planning%2520process.%2520To%2520this%2520end%252C%2520most%2520recent%2520works%2520aim%2520to%250Alearn%2520a%2520neural%2520network%2520model%2520to%2520predict%2520the%2520vehicle%2520evolution.%2520However%252C%2520such%250Aapproaches%2520are%2520data-intensive%2520and%2520fraught%2520with%2520generalization%2520issues.%2520In%2520this%250Apaper%252C%2520we%2520present%2520a%2520purely%2520model-based%2520approach%2520that%2520just%2520requires%2520the%2520digital%250Aelevation%2520information%2520of%2520the%2520terrain.%2520Specifically%252C%2520we%2520express%2520the%250Awheel-terrain%2520interaction%2520and%25206dof%2520pose%2520prediction%2520as%2520a%2520non-linear%2520least%250Asquares%2520%2528NLS%2529%2520problem.%2520As%2520a%2520result%252C%2520trajectory%2520planning%2520can%2520be%2520viewed%2520as%2520a%250Abi-level%2520optimization.%2520The%2520inner%2520optimization%2520layer%2520predicts%2520the%2520pose%2520on%2520the%250Aterrain%2520along%2520a%2520given%2520trajectory%252C%2520while%2520the%2520outer%2520layer%2520deforms%2520the%2520trajectory%250Aitself%2520to%2520reduce%2520the%2520stability%2520and%2520kinematic%2520costs%2520of%2520the%2520pose.%2520We%2520improve%2520the%250Astate-of-the-art%2520in%2520the%2520following%2520respects.%2520First%252C%2520we%2520show%2520that%2520our%2520NLS%2520based%250Apose%2520prediction%2520closely%2520matches%2520the%2520output%2520from%2520a%2520high-fidelity%2520physics%2520engine.%250AThis%2520result%2520coupled%2520with%2520the%2520fact%2520that%2520we%2520can%2520query%2520gradients%2520of%2520the%2520NLS%250Asolver%252C%2520makes%2520our%2520pose%2520predictor%252C%2520a%2520differentiable%2520wheel-terrain%2520interaction%250Amodel.%2520We%2520further%2520leverage%2520this%2520differentiability%2520to%2520efficiently%2520solve%2520the%250Aproposed%2520bi-level%2520trajectory%2520optimization%2520problem.%2520Finally%252C%2520we%2520perform%250Aextensive%2520experiments%252C%2520and%2520comparison%2520with%2520a%2520baseline%2520to%2520showcase%2520the%250Aeffectiveness%2520of%2520our%2520approach%2520in%2520obtaining%2520smooth%252C%2520stable%2520trajectories.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2404.03307v3%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Bi-level%20Trajectory%20Optimization%20on%20Uneven%20Terrains%20with%20Differentiable%0A%20%20Wheel-Terrain%20Interaction%20Model&entry.906535625=Amith%20Manoharan%20and%20Aditya%20Sharma%20and%20Himani%20Belsare%20and%20Kaustab%20Pal%20and%20K.%20Madhava%20Krishna%20and%20Arun%20Kumar%20Singh&entry.1292438233=%20%20Navigation%20of%20wheeled%20vehicles%20on%20uneven%20terrain%20necessitates%20going%20beyond%0Athe%202D%20approaches%20for%20trajectory%20planning.%20Specifically%2C%20it%20is%20essential%20to%0Aincorporate%20the%20full%206dof%20variation%20of%20vehicle%20pose%20and%20its%20associated%0Astability%20cost%20in%20the%20planning%20process.%20To%20this%20end%2C%20most%20recent%20works%20aim%20to%0Alearn%20a%20neural%20network%20model%20to%20predict%20the%20vehicle%20evolution.%20However%2C%20such%0Aapproaches%20are%20data-intensive%20and%20fraught%20with%20generalization%20issues.%20In%20this%0Apaper%2C%20we%20present%20a%20purely%20model-based%20approach%20that%20just%20requires%20the%20digital%0Aelevation%20information%20of%20the%20terrain.%20Specifically%2C%20we%20express%20the%0Awheel-terrain%20interaction%20and%206dof%20pose%20prediction%20as%20a%20non-linear%20least%0Asquares%20%28NLS%29%20problem.%20As%20a%20result%2C%20trajectory%20planning%20can%20be%20viewed%20as%20a%0Abi-level%20optimization.%20The%20inner%20optimization%20layer%20predicts%20the%20pose%20on%20the%0Aterrain%20along%20a%20given%20trajectory%2C%20while%20the%20outer%20layer%20deforms%20the%20trajectory%0Aitself%20to%20reduce%20the%20stability%20and%20kinematic%20costs%20of%20the%20pose.%20We%20improve%20the%0Astate-of-the-art%20in%20the%20following%20respects.%20First%2C%20we%20show%20that%20our%20NLS%20based%0Apose%20prediction%20closely%20matches%20the%20output%20from%20a%20high-fidelity%20physics%20engine.%0AThis%20result%20coupled%20with%20the%20fact%20that%20we%20can%20query%20gradients%20of%20the%20NLS%0Asolver%2C%20makes%20our%20pose%20predictor%2C%20a%20differentiable%20wheel-terrain%20interaction%0Amodel.%20We%20further%20leverage%20this%20differentiability%20to%20efficiently%20solve%20the%0Aproposed%20bi-level%20trajectory%20optimization%20problem.%20Finally%2C%20we%20perform%0Aextensive%20experiments%2C%20and%20comparison%20with%20a%20baseline%20to%20showcase%20the%0Aeffectiveness%20of%20our%20approach%20in%20obtaining%20smooth%2C%20stable%20trajectories.%0A&entry.1838667208=http%3A//arxiv.org/abs/2404.03307v3&entry.124074799=Read"},
{"title": "Physically Interpretable Probabilistic Domain Characterization", "author": "Ana\u00efs Halin and S\u00e9bastien Pi\u00e9rard and Renaud Vandeghen and Beno\u00eet G\u00e9rin and Maxime Zanella and Martin Colot and Jan Held and Anthony Cioppa and Emmanuel Jean and Gianluca Bontempi and Sa\u00efd Mahmoudi and Beno\u00eet Macq and Marc Van Droogenbroeck", "abstract": "  Characterizing domains is essential for models analyzing dynamic\nenvironments, as it allows them to adapt to evolving conditions or to hand the\ntask over to backup systems when facing conditions outside their operational\ndomain. Existing solutions typically characterize a domain by solving a\nregression or classification problem, which limits their applicability as they\nonly provide a limited summarized description of the domain. In this paper, we\npresent a novel approach to domain characterization by characterizing domains\nas probability distributions. Particularly, we develop a method to predict the\nlikelihood of different weather conditions from images captured by\nvehicle-mounted cameras by estimating distributions of physical parameters\nusing normalizing flows. To validate our proposed approach, we conduct\nexperiments within the context of autonomous vehicles, focusing on predicting\nthe distribution of weather parameters to characterize the operational domain.\nThis domain is characterized by physical parameters (absolute characterization)\nand arbitrarily predefined domains (relative characterization). Finally, we\nevaluate whether a system can safely operate in a target domain by comparing it\nto multiple source domains where safety has already been established. This\napproach holds significant potential, as accurate weather prediction and\neffective domain adaptation are crucial for autonomous systems to adjust to\ndynamic environmental conditions.\n", "link": "http://arxiv.org/abs/2411.14827v1", "date": "2024-11-22", "relevancy": 2.1231, "topK": [{"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.5883}, {"title": "PhysGen: Rigid-Body Physics-Grounded Image-to-Video Generation", "link": "http://arxiv.org/abs/2409.18964v1", "similarity": 0.5334}, {"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.5052}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Physically%20Interpretable%20Probabilistic%20Domain%20Characterization&body=Title%3A%20Physically%20Interpretable%20Probabilistic%20Domain%20Characterization%0AAuthor%3A%20Ana%C3%AFs%20Halin%20and%20S%C3%A9bastien%20Pi%C3%A9rard%20and%20Renaud%20Vandeghen%20and%20Beno%C3%AEt%20G%C3%A9rin%20and%20Maxime%20Zanella%20and%20Martin%20Colot%20and%20Jan%20Held%20and%20Anthony%20Cioppa%20and%20Emmanuel%20Jean%20and%20Gianluca%20Bontempi%20and%20Sa%C3%AFd%20Mahmoudi%20and%20Beno%C3%AEt%20Macq%20and%20Marc%20Van%20Droogenbroeck%0AAbstract%3A%20%20%20Characterizing%20domains%20is%20essential%20for%20models%20analyzing%20dynamic%0Aenvironments%2C%20as%20it%20allows%20them%20to%20adapt%20to%20evolving%20conditions%20or%20to%20hand%20the%0Atask%20over%20to%20backup%20systems%20when%20facing%20conditions%20outside%20their%20operational%0Adomain.%20Existing%20solutions%20typically%20characterize%20a%20domain%20by%20solving%20a%0Aregression%20or%20classification%20problem%2C%20which%20limits%20their%20applicability%20as%20they%0Aonly%20provide%20a%20limited%20summarized%20description%20of%20the%20domain.%20In%20this%20paper%2C%20we%0Apresent%20a%20novel%20approach%20to%20domain%20characterization%20by%20characterizing%20domains%0Aas%20probability%20distributions.%20Particularly%2C%20we%20develop%20a%20method%20to%20predict%20the%0Alikelihood%20of%20different%20weather%20conditions%20from%20images%20captured%20by%0Avehicle-mounted%20cameras%20by%20estimating%20distributions%20of%20physical%20parameters%0Ausing%20normalizing%20flows.%20To%20validate%20our%20proposed%20approach%2C%20we%20conduct%0Aexperiments%20within%20the%20context%20of%20autonomous%20vehicles%2C%20focusing%20on%20predicting%0Athe%20distribution%20of%20weather%20parameters%20to%20characterize%20the%20operational%20domain.%0AThis%20domain%20is%20characterized%20by%20physical%20parameters%20%28absolute%20characterization%29%0Aand%20arbitrarily%20predefined%20domains%20%28relative%20characterization%29.%20Finally%2C%20we%0Aevaluate%20whether%20a%20system%20can%20safely%20operate%20in%20a%20target%20domain%20by%20comparing%20it%0Ato%20multiple%20source%20domains%20where%20safety%20has%20already%20been%20established.%20This%0Aapproach%20holds%20significant%20potential%2C%20as%20accurate%20weather%20prediction%20and%0Aeffective%20domain%20adaptation%20are%20crucial%20for%20autonomous%20systems%20to%20adjust%20to%0Adynamic%20environmental%20conditions.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2411.14827v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DPhysically%2520Interpretable%2520Probabilistic%2520Domain%2520Characterization%26entry.906535625%3DAna%25C3%25AFs%2520Halin%2520and%2520S%25C3%25A9bastien%2520Pi%25C3%25A9rard%2520and%2520Renaud%2520Vandeghen%2520and%2520Beno%25C3%25AEt%2520G%25C3%25A9rin%2520and%2520Maxime%2520Zanella%2520and%2520Martin%2520Colot%2520and%2520Jan%2520Held%2520and%2520Anthony%2520Cioppa%2520and%2520Emmanuel%2520Jean%2520and%2520Gianluca%2520Bontempi%2520and%2520Sa%25C3%25AFd%2520Mahmoudi%2520and%2520Beno%25C3%25AEt%2520Macq%2520and%2520Marc%2520Van%2520Droogenbroeck%26entry.1292438233%3D%2520%2520Characterizing%2520domains%2520is%2520essential%2520for%2520models%2520analyzing%2520dynamic%250Aenvironments%252C%2520as%2520it%2520allows%2520them%2520to%2520adapt%2520to%2520evolving%2520conditions%2520or%2520to%2520hand%2520the%250Atask%2520over%2520to%2520backup%2520systems%2520when%2520facing%2520conditions%2520outside%2520their%2520operational%250Adomain.%2520Existing%2520solutions%2520typically%2520characterize%2520a%2520domain%2520by%2520solving%2520a%250Aregression%2520or%2520classification%2520problem%252C%2520which%2520limits%2520their%2520applicability%2520as%2520they%250Aonly%2520provide%2520a%2520limited%2520summarized%2520description%2520of%2520the%2520domain.%2520In%2520this%2520paper%252C%2520we%250Apresent%2520a%2520novel%2520approach%2520to%2520domain%2520characterization%2520by%2520characterizing%2520domains%250Aas%2520probability%2520distributions.%2520Particularly%252C%2520we%2520develop%2520a%2520method%2520to%2520predict%2520the%250Alikelihood%2520of%2520different%2520weather%2520conditions%2520from%2520images%2520captured%2520by%250Avehicle-mounted%2520cameras%2520by%2520estimating%2520distributions%2520of%2520physical%2520parameters%250Ausing%2520normalizing%2520flows.%2520To%2520validate%2520our%2520proposed%2520approach%252C%2520we%2520conduct%250Aexperiments%2520within%2520the%2520context%2520of%2520autonomous%2520vehicles%252C%2520focusing%2520on%2520predicting%250Athe%2520distribution%2520of%2520weather%2520parameters%2520to%2520characterize%2520the%2520operational%2520domain.%250AThis%2520domain%2520is%2520characterized%2520by%2520physical%2520parameters%2520%2528absolute%2520characterization%2529%250Aand%2520arbitrarily%2520predefined%2520domains%2520%2528relative%2520characterization%2529.%2520Finally%252C%2520we%250Aevaluate%2520whether%2520a%2520system%2520can%2520safely%2520operate%2520in%2520a%2520target%2520domain%2520by%2520comparing%2520it%250Ato%2520multiple%2520source%2520domains%2520where%2520safety%2520has%2520already%2520been%2520established.%2520This%250Aapproach%2520holds%2520significant%2520potential%252C%2520as%2520accurate%2520weather%2520prediction%2520and%250Aeffective%2520domain%2520adaptation%2520are%2520crucial%2520for%2520autonomous%2520systems%2520to%2520adjust%2520to%250Adynamic%2520environmental%2520conditions.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2411.14827v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Physically%20Interpretable%20Probabilistic%20Domain%20Characterization&entry.906535625=Ana%C3%AFs%20Halin%20and%20S%C3%A9bastien%20Pi%C3%A9rard%20and%20Renaud%20Vandeghen%20and%20Beno%C3%AEt%20G%C3%A9rin%20and%20Maxime%20Zanella%20and%20Martin%20Colot%20and%20Jan%20Held%20and%20Anthony%20Cioppa%20and%20Emmanuel%20Jean%20and%20Gianluca%20Bontempi%20and%20Sa%C3%AFd%20Mahmoudi%20and%20Beno%C3%AEt%20Macq%20and%20Marc%20Van%20Droogenbroeck&entry.1292438233=%20%20Characterizing%20domains%20is%20essential%20for%20models%20analyzing%20dynamic%0Aenvironments%2C%20as%20it%20allows%20them%20to%20adapt%20to%20evolving%20conditions%20or%20to%20hand%20the%0Atask%20over%20to%20backup%20systems%20when%20facing%20conditions%20outside%20their%20operational%0Adomain.%20Existing%20solutions%20typically%20characterize%20a%20domain%20by%20solving%20a%0Aregression%20or%20classification%20problem%2C%20which%20limits%20their%20applicability%20as%20they%0Aonly%20provide%20a%20limited%20summarized%20description%20of%20the%20domain.%20In%20this%20paper%2C%20we%0Apresent%20a%20novel%20approach%20to%20domain%20characterization%20by%20characterizing%20domains%0Aas%20probability%20distributions.%20Particularly%2C%20we%20develop%20a%20method%20to%20predict%20the%0Alikelihood%20of%20different%20weather%20conditions%20from%20images%20captured%20by%0Avehicle-mounted%20cameras%20by%20estimating%20distributions%20of%20physical%20parameters%0Ausing%20normalizing%20flows.%20To%20validate%20our%20proposed%20approach%2C%20we%20conduct%0Aexperiments%20within%20the%20context%20of%20autonomous%20vehicles%2C%20focusing%20on%20predicting%0Athe%20distribution%20of%20weather%20parameters%20to%20characterize%20the%20operational%20domain.%0AThis%20domain%20is%20characterized%20by%20physical%20parameters%20%28absolute%20characterization%29%0Aand%20arbitrarily%20predefined%20domains%20%28relative%20characterization%29.%20Finally%2C%20we%0Aevaluate%20whether%20a%20system%20can%20safely%20operate%20in%20a%20target%20domain%20by%20comparing%20it%0Ato%20multiple%20source%20domains%20where%20safety%20has%20already%20been%20established.%20This%0Aapproach%20holds%20significant%20potential%2C%20as%20accurate%20weather%20prediction%20and%0Aeffective%20domain%20adaptation%20are%20crucial%20for%20autonomous%20systems%20to%20adjust%20to%0Adynamic%20environmental%20conditions.%0A&entry.1838667208=http%3A//arxiv.org/abs/2411.14827v1&entry.124074799=Read"},
{"title": "Characterizing User Archetypes and Discussions on Scored.co", "author": "Andrea Failla and Salvatore Citraro and Giulio Rossetti and Francesco Cauteruccio", "abstract": "  In recent years, the proliferation of social platforms has drastically\ntransformed the way individuals interact, organize, and share information. In\nthis scenario, we experience an unprecedented increase in the scale and\ncomplexity of interactions and, at the same time, little to no research about\nsome fringe social platforms. In this paper, we present a multi-dimensional\nframework for characterizing nodes and hyperedges in social hypernetworks, with\na focus on the understudied alt-right platform Scored.co. Our approach\nintegrates the possibility of studying higher-order interactions, thanks to the\nhypernetwork representation, and various node features such as user activity,\nsentiment, and toxicity, with the aim to define distinct user archetypes and\nunderstand their roles within the network. Utilizing a comprehensive dataset\nfrom Scored.co, we analyze the dynamics of these archetypes over time and\nexplore their interactions and influence within the community. The framework's\nversatility allows for detailed analysis of both individual user behaviors and\nbroader social structures. Our findings highlight the importance of\nhigher-order interactions in understanding social dynamics, offering new\ninsights into the roles and behaviors that emerge in complex online\nenvironments.\n", "link": "http://arxiv.org/abs/2407.21753v2", "date": "2024-11-22", "relevancy": 2.1208, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.4321}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.4321}, {"title": "DressCode: Autoregressively Sewing and Generating Garments from Text\n  Guidance", "link": "http://arxiv.org/abs/2401.16465v3", "similarity": 0.4084}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Characterizing%20User%20Archetypes%20and%20Discussions%20on%20Scored.co&body=Title%3A%20Characterizing%20User%20Archetypes%20and%20Discussions%20on%20Scored.co%0AAuthor%3A%20Andrea%20Failla%20and%20Salvatore%20Citraro%20and%20Giulio%20Rossetti%20and%20Francesco%20Cauteruccio%0AAbstract%3A%20%20%20In%20recent%20years%2C%20the%20proliferation%20of%20social%20platforms%20has%20drastically%0Atransformed%20the%20way%20individuals%20interact%2C%20organize%2C%20and%20share%20information.%20In%0Athis%20scenario%2C%20we%20experience%20an%20unprecedented%20increase%20in%20the%20scale%20and%0Acomplexity%20of%20interactions%20and%2C%20at%20the%20same%20time%2C%20little%20to%20no%20research%20about%0Asome%20fringe%20social%20platforms.%20In%20this%20paper%2C%20we%20present%20a%20multi-dimensional%0Aframework%20for%20characterizing%20nodes%20and%20hyperedges%20in%20social%20hypernetworks%2C%20with%0Aa%20focus%20on%20the%20understudied%20alt-right%20platform%20Scored.co.%20Our%20approach%0Aintegrates%20the%20possibility%20of%20studying%20higher-order%20interactions%2C%20thanks%20to%20the%0Ahypernetwork%20representation%2C%20and%20various%20node%20features%20such%20as%20user%20activity%2C%0Asentiment%2C%20and%20toxicity%2C%20with%20the%20aim%20to%20define%20distinct%20user%20archetypes%20and%0Aunderstand%20their%20roles%20within%20the%20network.%20Utilizing%20a%20comprehensive%20dataset%0Afrom%20Scored.co%2C%20we%20analyze%20the%20dynamics%20of%20these%20archetypes%20over%20time%20and%0Aexplore%20their%20interactions%20and%20influence%20within%20the%20community.%20The%20framework%27s%0Aversatility%20allows%20for%20detailed%20analysis%20of%20both%20individual%20user%20behaviors%20and%0Abroader%20social%20structures.%20Our%20findings%20highlight%20the%20importance%20of%0Ahigher-order%20interactions%20in%20understanding%20social%20dynamics%2C%20offering%20new%0Ainsights%20into%20the%20roles%20and%20behaviors%20that%20emerge%20in%20complex%20online%0Aenvironments.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2407.21753v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DCharacterizing%2520User%2520Archetypes%2520and%2520Discussions%2520on%2520Scored.co%26entry.906535625%3DAndrea%2520Failla%2520and%2520Salvatore%2520Citraro%2520and%2520Giulio%2520Rossetti%2520and%2520Francesco%2520Cauteruccio%26entry.1292438233%3D%2520%2520In%2520recent%2520years%252C%2520the%2520proliferation%2520of%2520social%2520platforms%2520has%2520drastically%250Atransformed%2520the%2520way%2520individuals%2520interact%252C%2520organize%252C%2520and%2520share%2520information.%2520In%250Athis%2520scenario%252C%2520we%2520experience%2520an%2520unprecedented%2520increase%2520in%2520the%2520scale%2520and%250Acomplexity%2520of%2520interactions%2520and%252C%2520at%2520the%2520same%2520time%252C%2520little%2520to%2520no%2520research%2520about%250Asome%2520fringe%2520social%2520platforms.%2520In%2520this%2520paper%252C%2520we%2520present%2520a%2520multi-dimensional%250Aframework%2520for%2520characterizing%2520nodes%2520and%2520hyperedges%2520in%2520social%2520hypernetworks%252C%2520with%250Aa%2520focus%2520on%2520the%2520understudied%2520alt-right%2520platform%2520Scored.co.%2520Our%2520approach%250Aintegrates%2520the%2520possibility%2520of%2520studying%2520higher-order%2520interactions%252C%2520thanks%2520to%2520the%250Ahypernetwork%2520representation%252C%2520and%2520various%2520node%2520features%2520such%2520as%2520user%2520activity%252C%250Asentiment%252C%2520and%2520toxicity%252C%2520with%2520the%2520aim%2520to%2520define%2520distinct%2520user%2520archetypes%2520and%250Aunderstand%2520their%2520roles%2520within%2520the%2520network.%2520Utilizing%2520a%2520comprehensive%2520dataset%250Afrom%2520Scored.co%252C%2520we%2520analyze%2520the%2520dynamics%2520of%2520these%2520archetypes%2520over%2520time%2520and%250Aexplore%2520their%2520interactions%2520and%2520influence%2520within%2520the%2520community.%2520The%2520framework%2527s%250Aversatility%2520allows%2520for%2520detailed%2520analysis%2520of%2520both%2520individual%2520user%2520behaviors%2520and%250Abroader%2520social%2520structures.%2520Our%2520findings%2520highlight%2520the%2520importance%2520of%250Ahigher-order%2520interactions%2520in%2520understanding%2520social%2520dynamics%252C%2520offering%2520new%250Ainsights%2520into%2520the%2520roles%2520and%2520behaviors%2520that%2520emerge%2520in%2520complex%2520online%250Aenvironments.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2407.21753v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Characterizing%20User%20Archetypes%20and%20Discussions%20on%20Scored.co&entry.906535625=Andrea%20Failla%20and%20Salvatore%20Citraro%20and%20Giulio%20Rossetti%20and%20Francesco%20Cauteruccio&entry.1292438233=%20%20In%20recent%20years%2C%20the%20proliferation%20of%20social%20platforms%20has%20drastically%0Atransformed%20the%20way%20individuals%20interact%2C%20organize%2C%20and%20share%20information.%20In%0Athis%20scenario%2C%20we%20experience%20an%20unprecedented%20increase%20in%20the%20scale%20and%0Acomplexity%20of%20interactions%20and%2C%20at%20the%20same%20time%2C%20little%20to%20no%20research%20about%0Asome%20fringe%20social%20platforms.%20In%20this%20paper%2C%20we%20present%20a%20multi-dimensional%0Aframework%20for%20characterizing%20nodes%20and%20hyperedges%20in%20social%20hypernetworks%2C%20with%0Aa%20focus%20on%20the%20understudied%20alt-right%20platform%20Scored.co.%20Our%20approach%0Aintegrates%20the%20possibility%20of%20studying%20higher-order%20interactions%2C%20thanks%20to%20the%0Ahypernetwork%20representation%2C%20and%20various%20node%20features%20such%20as%20user%20activity%2C%0Asentiment%2C%20and%20toxicity%2C%20with%20the%20aim%20to%20define%20distinct%20user%20archetypes%20and%0Aunderstand%20their%20roles%20within%20the%20network.%20Utilizing%20a%20comprehensive%20dataset%0Afrom%20Scored.co%2C%20we%20analyze%20the%20dynamics%20of%20these%20archetypes%20over%20time%20and%0Aexplore%20their%20interactions%20and%20influence%20within%20the%20community.%20The%20framework%27s%0Aversatility%20allows%20for%20detailed%20analysis%20of%20both%20individual%20user%20behaviors%20and%0Abroader%20social%20structures.%20Our%20findings%20highlight%20the%20importance%20of%0Ahigher-order%20interactions%20in%20understanding%20social%20dynamics%2C%20offering%20new%0Ainsights%20into%20the%20roles%20and%20behaviors%20that%20emerge%20in%20complex%20online%0Aenvironments.%0A&entry.1838667208=http%3A//arxiv.org/abs/2407.21753v2&entry.124074799=Read"},
{"title": "Enhancing Autonomous Driving Safety through World Model-Based Predictive\n  Navigation and Adaptive Learning Algorithms for 5G Wireless Applications", "author": "Hong Ding and Ziming Wang and Yi Ding and Hongjie Lin and SuYang Xi and Chia Chao Kang", "abstract": "  Addressing the challenge of ensuring safety in ever-changing and\nunpredictable environments, particularly in the swiftly advancing realm of\nautonomous driving in today's 5G wireless communication world, we present\nNavigation Secure (NavSecure). This vision-based navigation framework merges\nthe strengths of world models with crucial safety-focused decision-making\ncapabilities, enabling autonomous vehicles to navigate real-world complexities\nsecurely. Our approach anticipates potential threats and formulates safer\nroutes by harnessing the predictive capabilities of world models, thus\nsignificantly reducing the need for extensive real-world trial-and-error\nlearning. Additionally, our method empowers vehicles to autonomously learn and\ndevelop through continuous practice, ensuring the system evolves and adapts to\nnew challenges. Incorporating radio frequency technology, NavSecure leverages\n5G networks to enhance real-time data exchange, improving communication and\nresponsiveness. Validated through rigorous experiments under simulation-to-real\ndriving conditions, NavSecure has shown exceptional performance in\nsafety-critical scenarios, such as sudden obstacle avoidance. Results indicate\nthat NavSecure excels in key safety metrics, including collision prevention and\nrisk reduction, surpassing other end-to-end methodologies. This framework not\nonly advances autonomous driving safety but also demonstrates how world models\ncan enhance decision-making in critical applications. NavSecure sets a new\nstandard for developing more robust and trustworthy autonomous driving systems,\ncapable of handling the inherent dynamics and uncertainties of real-world\nenvironments.\n", "link": "http://arxiv.org/abs/2411.15042v1", "date": "2024-11-22", "relevancy": 2.1135, "topK": [{"title": "Egocentric Computer Vision for Hands-Free Robotic Wheelchair Navigation", "link": "https://link.springer.com/article/10.1007/s10846-023-01807-4", "similarity": 0.5367}, {"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.5268}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.5115}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Enhancing%20Autonomous%20Driving%20Safety%20through%20World%20Model-Based%20Predictive%0A%20%20Navigation%20and%20Adaptive%20Learning%20Algorithms%20for%205G%20Wireless%20Applications&body=Title%3A%20Enhancing%20Autonomous%20Driving%20Safety%20through%20World%20Model-Based%20Predictive%0A%20%20Navigation%20and%20Adaptive%20Learning%20Algorithms%20for%205G%20Wireless%20Applications%0AAuthor%3A%20Hong%20Ding%20and%20Ziming%20Wang%20and%20Yi%20Ding%20and%20Hongjie%20Lin%20and%20SuYang%20Xi%20and%20Chia%20Chao%20Kang%0AAbstract%3A%20%20%20Addressing%20the%20challenge%20of%20ensuring%20safety%20in%20ever-changing%20and%0Aunpredictable%20environments%2C%20particularly%20in%20the%20swiftly%20advancing%20realm%20of%0Aautonomous%20driving%20in%20today%27s%205G%20wireless%20communication%20world%2C%20we%20present%0ANavigation%20Secure%20%28NavSecure%29.%20This%20vision-based%20navigation%20framework%20merges%0Athe%20strengths%20of%20world%20models%20with%20crucial%20safety-focused%20decision-making%0Acapabilities%2C%20enabling%20autonomous%20vehicles%20to%20navigate%20real-world%20complexities%0Asecurely.%20Our%20approach%20anticipates%20potential%20threats%20and%20formulates%20safer%0Aroutes%20by%20harnessing%20the%20predictive%20capabilities%20of%20world%20models%2C%20thus%0Asignificantly%20reducing%20the%20need%20for%20extensive%20real-world%20trial-and-error%0Alearning.%20Additionally%2C%20our%20method%20empowers%20vehicles%20to%20autonomously%20learn%20and%0Adevelop%20through%20continuous%20practice%2C%20ensuring%20the%20system%20evolves%20and%20adapts%20to%0Anew%20challenges.%20Incorporating%20radio%20frequency%20technology%2C%20NavSecure%20leverages%0A5G%20networks%20to%20enhance%20real-time%20data%20exchange%2C%20improving%20communication%20and%0Aresponsiveness.%20Validated%20through%20rigorous%20experiments%20under%20simulation-to-real%0Adriving%20conditions%2C%20NavSecure%20has%20shown%20exceptional%20performance%20in%0Asafety-critical%20scenarios%2C%20such%20as%20sudden%20obstacle%20avoidance.%20Results%20indicate%0Athat%20NavSecure%20excels%20in%20key%20safety%20metrics%2C%20including%20collision%20prevention%20and%0Arisk%20reduction%2C%20surpassing%20other%20end-to-end%20methodologies.%20This%20framework%20not%0Aonly%20advances%20autonomous%20driving%20safety%20but%20also%20demonstrates%20how%20world%20models%0Acan%20enhance%20decision-making%20in%20critical%20applications.%20NavSecure%20sets%20a%20new%0Astandard%20for%20developing%20more%20robust%20and%20trustworthy%20autonomous%20driving%20systems%2C%0Acapable%20of%20handling%20the%20inherent%20dynamics%20and%20uncertainties%20of%20real-world%0Aenvironments.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2411.15042v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DEnhancing%2520Autonomous%2520Driving%2520Safety%2520through%2520World%2520Model-Based%2520Predictive%250A%2520%2520Navigation%2520and%2520Adaptive%2520Learning%2520Algorithms%2520for%25205G%2520Wireless%2520Applications%26entry.906535625%3DHong%2520Ding%2520and%2520Ziming%2520Wang%2520and%2520Yi%2520Ding%2520and%2520Hongjie%2520Lin%2520and%2520SuYang%2520Xi%2520and%2520Chia%2520Chao%2520Kang%26entry.1292438233%3D%2520%2520Addressing%2520the%2520challenge%2520of%2520ensuring%2520safety%2520in%2520ever-changing%2520and%250Aunpredictable%2520environments%252C%2520particularly%2520in%2520the%2520swiftly%2520advancing%2520realm%2520of%250Aautonomous%2520driving%2520in%2520today%2527s%25205G%2520wireless%2520communication%2520world%252C%2520we%2520present%250ANavigation%2520Secure%2520%2528NavSecure%2529.%2520This%2520vision-based%2520navigation%2520framework%2520merges%250Athe%2520strengths%2520of%2520world%2520models%2520with%2520crucial%2520safety-focused%2520decision-making%250Acapabilities%252C%2520enabling%2520autonomous%2520vehicles%2520to%2520navigate%2520real-world%2520complexities%250Asecurely.%2520Our%2520approach%2520anticipates%2520potential%2520threats%2520and%2520formulates%2520safer%250Aroutes%2520by%2520harnessing%2520the%2520predictive%2520capabilities%2520of%2520world%2520models%252C%2520thus%250Asignificantly%2520reducing%2520the%2520need%2520for%2520extensive%2520real-world%2520trial-and-error%250Alearning.%2520Additionally%252C%2520our%2520method%2520empowers%2520vehicles%2520to%2520autonomously%2520learn%2520and%250Adevelop%2520through%2520continuous%2520practice%252C%2520ensuring%2520the%2520system%2520evolves%2520and%2520adapts%2520to%250Anew%2520challenges.%2520Incorporating%2520radio%2520frequency%2520technology%252C%2520NavSecure%2520leverages%250A5G%2520networks%2520to%2520enhance%2520real-time%2520data%2520exchange%252C%2520improving%2520communication%2520and%250Aresponsiveness.%2520Validated%2520through%2520rigorous%2520experiments%2520under%2520simulation-to-real%250Adriving%2520conditions%252C%2520NavSecure%2520has%2520shown%2520exceptional%2520performance%2520in%250Asafety-critical%2520scenarios%252C%2520such%2520as%2520sudden%2520obstacle%2520avoidance.%2520Results%2520indicate%250Athat%2520NavSecure%2520excels%2520in%2520key%2520safety%2520metrics%252C%2520including%2520collision%2520prevention%2520and%250Arisk%2520reduction%252C%2520surpassing%2520other%2520end-to-end%2520methodologies.%2520This%2520framework%2520not%250Aonly%2520advances%2520autonomous%2520driving%2520safety%2520but%2520also%2520demonstrates%2520how%2520world%2520models%250Acan%2520enhance%2520decision-making%2520in%2520critical%2520applications.%2520NavSecure%2520sets%2520a%2520new%250Astandard%2520for%2520developing%2520more%2520robust%2520and%2520trustworthy%2520autonomous%2520driving%2520systems%252C%250Acapable%2520of%2520handling%2520the%2520inherent%2520dynamics%2520and%2520uncertainties%2520of%2520real-world%250Aenvironments.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2411.15042v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Enhancing%20Autonomous%20Driving%20Safety%20through%20World%20Model-Based%20Predictive%0A%20%20Navigation%20and%20Adaptive%20Learning%20Algorithms%20for%205G%20Wireless%20Applications&entry.906535625=Hong%20Ding%20and%20Ziming%20Wang%20and%20Yi%20Ding%20and%20Hongjie%20Lin%20and%20SuYang%20Xi%20and%20Chia%20Chao%20Kang&entry.1292438233=%20%20Addressing%20the%20challenge%20of%20ensuring%20safety%20in%20ever-changing%20and%0Aunpredictable%20environments%2C%20particularly%20in%20the%20swiftly%20advancing%20realm%20of%0Aautonomous%20driving%20in%20today%27s%205G%20wireless%20communication%20world%2C%20we%20present%0ANavigation%20Secure%20%28NavSecure%29.%20This%20vision-based%20navigation%20framework%20merges%0Athe%20strengths%20of%20world%20models%20with%20crucial%20safety-focused%20decision-making%0Acapabilities%2C%20enabling%20autonomous%20vehicles%20to%20navigate%20real-world%20complexities%0Asecurely.%20Our%20approach%20anticipates%20potential%20threats%20and%20formulates%20safer%0Aroutes%20by%20harnessing%20the%20predictive%20capabilities%20of%20world%20models%2C%20thus%0Asignificantly%20reducing%20the%20need%20for%20extensive%20real-world%20trial-and-error%0Alearning.%20Additionally%2C%20our%20method%20empowers%20vehicles%20to%20autonomously%20learn%20and%0Adevelop%20through%20continuous%20practice%2C%20ensuring%20the%20system%20evolves%20and%20adapts%20to%0Anew%20challenges.%20Incorporating%20radio%20frequency%20technology%2C%20NavSecure%20leverages%0A5G%20networks%20to%20enhance%20real-time%20data%20exchange%2C%20improving%20communication%20and%0Aresponsiveness.%20Validated%20through%20rigorous%20experiments%20under%20simulation-to-real%0Adriving%20conditions%2C%20NavSecure%20has%20shown%20exceptional%20performance%20in%0Asafety-critical%20scenarios%2C%20such%20as%20sudden%20obstacle%20avoidance.%20Results%20indicate%0Athat%20NavSecure%20excels%20in%20key%20safety%20metrics%2C%20including%20collision%20prevention%20and%0Arisk%20reduction%2C%20surpassing%20other%20end-to-end%20methodologies.%20This%20framework%20not%0Aonly%20advances%20autonomous%20driving%20safety%20but%20also%20demonstrates%20how%20world%20models%0Acan%20enhance%20decision-making%20in%20critical%20applications.%20NavSecure%20sets%20a%20new%0Astandard%20for%20developing%20more%20robust%20and%20trustworthy%20autonomous%20driving%20systems%2C%0Acapable%20of%20handling%20the%20inherent%20dynamics%20and%20uncertainties%20of%20real-world%0Aenvironments.%0A&entry.1838667208=http%3A//arxiv.org/abs/2411.15042v1&entry.124074799=Read"},
{"title": "Evaluating Vision Transformer Models for Visual Quality Control in\n  Industrial Manufacturing", "author": "Miriam Alber and Christoph H\u00f6nes and Patrick Baier", "abstract": "  One of the most promising use-cases for machine learning in industrial\nmanufacturing is the early detection of defective products using a quality\ncontrol system. Such a system can save costs and reduces human errors due to\nthe monotonous nature of visual inspections. Today, a rich body of research\nexists which employs machine learning methods to identify rare defective\nproducts in unbalanced visual quality control datasets. These methods typically\nrely on two components: A visual backbone to capture the features of the input\nimage and an anomaly detection algorithm that decides if these features are\nwithin an expected distribution. With the rise of transformer architecture as\nvisual backbones of choice, there exists now a great variety of different\ncombinations of these two components, ranging all along the trade-off between\ndetection quality and inference time. Facing this variety, practitioners in the\nfield often have to spend a considerable amount of time on researching the\nright combination for their use-case at hand. Our contribution is to help\npractitioners with this choice by reviewing and evaluating current vision\ntransformer models together with anomaly detection methods. For this, we chose\nSotA models of both disciplines, combined them and evaluated them towards the\ngoal of having small, fast and efficient anomaly detection models suitable for\nindustrial manufacturing. We evaluated the results of our experiments on the\nwell-known MVTecAD and BTAD datasets. Moreover, we give guidelines for choosing\na suitable model architecture for a quality control system in practice,\nconsidering given use-case and hardware constraints.\n", "link": "http://arxiv.org/abs/2411.14953v1", "date": "2024-11-22", "relevancy": 2.1037, "topK": [{"title": "Scaling Rectified Flow Transformers for High-Resolution Image Synthesis", "link": "http://arxiv.org/abs/2403.03206v1", "similarity": 0.5483}, {"title": "GST: Precise 3D Human Body from a Single Image with Gaussian Splatting\n  Transformers", "link": "http://arxiv.org/abs/2409.04196v1", "similarity": 0.5249}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5179}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Evaluating%20Vision%20Transformer%20Models%20for%20Visual%20Quality%20Control%20in%0A%20%20Industrial%20Manufacturing&body=Title%3A%20Evaluating%20Vision%20Transformer%20Models%20for%20Visual%20Quality%20Control%20in%0A%20%20Industrial%20Manufacturing%0AAuthor%3A%20Miriam%20Alber%20and%20Christoph%20H%C3%B6nes%20and%20Patrick%20Baier%0AAbstract%3A%20%20%20One%20of%20the%20most%20promising%20use-cases%20for%20machine%20learning%20in%20industrial%0Amanufacturing%20is%20the%20early%20detection%20of%20defective%20products%20using%20a%20quality%0Acontrol%20system.%20Such%20a%20system%20can%20save%20costs%20and%20reduces%20human%20errors%20due%20to%0Athe%20monotonous%20nature%20of%20visual%20inspections.%20Today%2C%20a%20rich%20body%20of%20research%0Aexists%20which%20employs%20machine%20learning%20methods%20to%20identify%20rare%20defective%0Aproducts%20in%20unbalanced%20visual%20quality%20control%20datasets.%20These%20methods%20typically%0Arely%20on%20two%20components%3A%20A%20visual%20backbone%20to%20capture%20the%20features%20of%20the%20input%0Aimage%20and%20an%20anomaly%20detection%20algorithm%20that%20decides%20if%20these%20features%20are%0Awithin%20an%20expected%20distribution.%20With%20the%20rise%20of%20transformer%20architecture%20as%0Avisual%20backbones%20of%20choice%2C%20there%20exists%20now%20a%20great%20variety%20of%20different%0Acombinations%20of%20these%20two%20components%2C%20ranging%20all%20along%20the%20trade-off%20between%0Adetection%20quality%20and%20inference%20time.%20Facing%20this%20variety%2C%20practitioners%20in%20the%0Afield%20often%20have%20to%20spend%20a%20considerable%20amount%20of%20time%20on%20researching%20the%0Aright%20combination%20for%20their%20use-case%20at%20hand.%20Our%20contribution%20is%20to%20help%0Apractitioners%20with%20this%20choice%20by%20reviewing%20and%20evaluating%20current%20vision%0Atransformer%20models%20together%20with%20anomaly%20detection%20methods.%20For%20this%2C%20we%20chose%0ASotA%20models%20of%20both%20disciplines%2C%20combined%20them%20and%20evaluated%20them%20towards%20the%0Agoal%20of%20having%20small%2C%20fast%20and%20efficient%20anomaly%20detection%20models%20suitable%20for%0Aindustrial%20manufacturing.%20We%20evaluated%20the%20results%20of%20our%20experiments%20on%20the%0Awell-known%20MVTecAD%20and%20BTAD%20datasets.%20Moreover%2C%20we%20give%20guidelines%20for%20choosing%0Aa%20suitable%20model%20architecture%20for%20a%20quality%20control%20system%20in%20practice%2C%0Aconsidering%20given%20use-case%20and%20hardware%20constraints.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2411.14953v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DEvaluating%2520Vision%2520Transformer%2520Models%2520for%2520Visual%2520Quality%2520Control%2520in%250A%2520%2520Industrial%2520Manufacturing%26entry.906535625%3DMiriam%2520Alber%2520and%2520Christoph%2520H%25C3%25B6nes%2520and%2520Patrick%2520Baier%26entry.1292438233%3D%2520%2520One%2520of%2520the%2520most%2520promising%2520use-cases%2520for%2520machine%2520learning%2520in%2520industrial%250Amanufacturing%2520is%2520the%2520early%2520detection%2520of%2520defective%2520products%2520using%2520a%2520quality%250Acontrol%2520system.%2520Such%2520a%2520system%2520can%2520save%2520costs%2520and%2520reduces%2520human%2520errors%2520due%2520to%250Athe%2520monotonous%2520nature%2520of%2520visual%2520inspections.%2520Today%252C%2520a%2520rich%2520body%2520of%2520research%250Aexists%2520which%2520employs%2520machine%2520learning%2520methods%2520to%2520identify%2520rare%2520defective%250Aproducts%2520in%2520unbalanced%2520visual%2520quality%2520control%2520datasets.%2520These%2520methods%2520typically%250Arely%2520on%2520two%2520components%253A%2520A%2520visual%2520backbone%2520to%2520capture%2520the%2520features%2520of%2520the%2520input%250Aimage%2520and%2520an%2520anomaly%2520detection%2520algorithm%2520that%2520decides%2520if%2520these%2520features%2520are%250Awithin%2520an%2520expected%2520distribution.%2520With%2520the%2520rise%2520of%2520transformer%2520architecture%2520as%250Avisual%2520backbones%2520of%2520choice%252C%2520there%2520exists%2520now%2520a%2520great%2520variety%2520of%2520different%250Acombinations%2520of%2520these%2520two%2520components%252C%2520ranging%2520all%2520along%2520the%2520trade-off%2520between%250Adetection%2520quality%2520and%2520inference%2520time.%2520Facing%2520this%2520variety%252C%2520practitioners%2520in%2520the%250Afield%2520often%2520have%2520to%2520spend%2520a%2520considerable%2520amount%2520of%2520time%2520on%2520researching%2520the%250Aright%2520combination%2520for%2520their%2520use-case%2520at%2520hand.%2520Our%2520contribution%2520is%2520to%2520help%250Apractitioners%2520with%2520this%2520choice%2520by%2520reviewing%2520and%2520evaluating%2520current%2520vision%250Atransformer%2520models%2520together%2520with%2520anomaly%2520detection%2520methods.%2520For%2520this%252C%2520we%2520chose%250ASotA%2520models%2520of%2520both%2520disciplines%252C%2520combined%2520them%2520and%2520evaluated%2520them%2520towards%2520the%250Agoal%2520of%2520having%2520small%252C%2520fast%2520and%2520efficient%2520anomaly%2520detection%2520models%2520suitable%2520for%250Aindustrial%2520manufacturing.%2520We%2520evaluated%2520the%2520results%2520of%2520our%2520experiments%2520on%2520the%250Awell-known%2520MVTecAD%2520and%2520BTAD%2520datasets.%2520Moreover%252C%2520we%2520give%2520guidelines%2520for%2520choosing%250Aa%2520suitable%2520model%2520architecture%2520for%2520a%2520quality%2520control%2520system%2520in%2520practice%252C%250Aconsidering%2520given%2520use-case%2520and%2520hardware%2520constraints.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2411.14953v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Evaluating%20Vision%20Transformer%20Models%20for%20Visual%20Quality%20Control%20in%0A%20%20Industrial%20Manufacturing&entry.906535625=Miriam%20Alber%20and%20Christoph%20H%C3%B6nes%20and%20Patrick%20Baier&entry.1292438233=%20%20One%20of%20the%20most%20promising%20use-cases%20for%20machine%20learning%20in%20industrial%0Amanufacturing%20is%20the%20early%20detection%20of%20defective%20products%20using%20a%20quality%0Acontrol%20system.%20Such%20a%20system%20can%20save%20costs%20and%20reduces%20human%20errors%20due%20to%0Athe%20monotonous%20nature%20of%20visual%20inspections.%20Today%2C%20a%20rich%20body%20of%20research%0Aexists%20which%20employs%20machine%20learning%20methods%20to%20identify%20rare%20defective%0Aproducts%20in%20unbalanced%20visual%20quality%20control%20datasets.%20These%20methods%20typically%0Arely%20on%20two%20components%3A%20A%20visual%20backbone%20to%20capture%20the%20features%20of%20the%20input%0Aimage%20and%20an%20anomaly%20detection%20algorithm%20that%20decides%20if%20these%20features%20are%0Awithin%20an%20expected%20distribution.%20With%20the%20rise%20of%20transformer%20architecture%20as%0Avisual%20backbones%20of%20choice%2C%20there%20exists%20now%20a%20great%20variety%20of%20different%0Acombinations%20of%20these%20two%20components%2C%20ranging%20all%20along%20the%20trade-off%20between%0Adetection%20quality%20and%20inference%20time.%20Facing%20this%20variety%2C%20practitioners%20in%20the%0Afield%20often%20have%20to%20spend%20a%20considerable%20amount%20of%20time%20on%20researching%20the%0Aright%20combination%20for%20their%20use-case%20at%20hand.%20Our%20contribution%20is%20to%20help%0Apractitioners%20with%20this%20choice%20by%20reviewing%20and%20evaluating%20current%20vision%0Atransformer%20models%20together%20with%20anomaly%20detection%20methods.%20For%20this%2C%20we%20chose%0ASotA%20models%20of%20both%20disciplines%2C%20combined%20them%20and%20evaluated%20them%20towards%20the%0Agoal%20of%20having%20small%2C%20fast%20and%20efficient%20anomaly%20detection%20models%20suitable%20for%0Aindustrial%20manufacturing.%20We%20evaluated%20the%20results%20of%20our%20experiments%20on%20the%0Awell-known%20MVTecAD%20and%20BTAD%20datasets.%20Moreover%2C%20we%20give%20guidelines%20for%20choosing%0Aa%20suitable%20model%20architecture%20for%20a%20quality%20control%20system%20in%20practice%2C%0Aconsidering%20given%20use-case%20and%20hardware%20constraints.%0A&entry.1838667208=http%3A//arxiv.org/abs/2411.14953v1&entry.124074799=Read"},
{"title": "LLM for Barcodes: Generating Diverse Synthetic Data for Identity\n  Documents", "author": "Hitesh Laxmichand Patel and Amit Agarwal and Bhargava Kumar and Karan Gupta and Priyaranjan Pattnayak", "abstract": "  Accurate barcode detection and decoding in Identity documents is crucial for\napplications like security, healthcare, and education, where reliable data\nextraction and verification are essential. However, building robust detection\nmodels is challenging due to the lack of diverse, realistic datasets an issue\noften tied to privacy concerns and the wide variety of document formats.\nTraditional tools like Faker rely on predefined templates, making them less\neffective for capturing the complexity of real-world identity documents. In\nthis paper, we introduce a new approach to synthetic data generation that uses\nLLMs to create contextually rich and realistic data without relying on\npredefined field. Using the vast knowledge LLMs have about different documents\nand content, our method creates data that reflects the variety found in real\nidentity documents. This data is then encoded into barcode and overlayed on\ntemplates for documents such as Driver's licenses, Insurance cards, Student\nIDs. Our approach simplifies the process of dataset creation, eliminating the\nneed for extensive domain knowledge or predefined fields. Compared to\ntraditional methods like Faker, data generated by LLM demonstrates greater\ndiversity and contextual relevance, leading to improved performance in barcode\ndetection models. This scalable, privacy-first solution is a big step forward\nin advancing machine learning for automated document processing and identity\nverification.\n", "link": "http://arxiv.org/abs/2411.14962v1", "date": "2024-11-22", "relevancy": 2.0989, "topK": [{"title": "Magic-Me: Identity-Specific Video Customized Diffusion", "link": "http://arxiv.org/abs/2402.09368v1", "similarity": 0.5568}, {"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.5209}, {"title": "VirtualModel: Generating Object-ID-retentive Human-object Interaction\n  Image by Diffusion Model for E-commerce Marketing", "link": "http://arxiv.org/abs/2405.09985v1", "similarity": 0.4942}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20LLM%20for%20Barcodes%3A%20Generating%20Diverse%20Synthetic%20Data%20for%20Identity%0A%20%20Documents&body=Title%3A%20LLM%20for%20Barcodes%3A%20Generating%20Diverse%20Synthetic%20Data%20for%20Identity%0A%20%20Documents%0AAuthor%3A%20Hitesh%20Laxmichand%20Patel%20and%20Amit%20Agarwal%20and%20Bhargava%20Kumar%20and%20Karan%20Gupta%20and%20Priyaranjan%20Pattnayak%0AAbstract%3A%20%20%20Accurate%20barcode%20detection%20and%20decoding%20in%20Identity%20documents%20is%20crucial%20for%0Aapplications%20like%20security%2C%20healthcare%2C%20and%20education%2C%20where%20reliable%20data%0Aextraction%20and%20verification%20are%20essential.%20However%2C%20building%20robust%20detection%0Amodels%20is%20challenging%20due%20to%20the%20lack%20of%20diverse%2C%20realistic%20datasets%20an%20issue%0Aoften%20tied%20to%20privacy%20concerns%20and%20the%20wide%20variety%20of%20document%20formats.%0ATraditional%20tools%20like%20Faker%20rely%20on%20predefined%20templates%2C%20making%20them%20less%0Aeffective%20for%20capturing%20the%20complexity%20of%20real-world%20identity%20documents.%20In%0Athis%20paper%2C%20we%20introduce%20a%20new%20approach%20to%20synthetic%20data%20generation%20that%20uses%0ALLMs%20to%20create%20contextually%20rich%20and%20realistic%20data%20without%20relying%20on%0Apredefined%20field.%20Using%20the%20vast%20knowledge%20LLMs%20have%20about%20different%20documents%0Aand%20content%2C%20our%20method%20creates%20data%20that%20reflects%20the%20variety%20found%20in%20real%0Aidentity%20documents.%20This%20data%20is%20then%20encoded%20into%20barcode%20and%20overlayed%20on%0Atemplates%20for%20documents%20such%20as%20Driver%27s%20licenses%2C%20Insurance%20cards%2C%20Student%0AIDs.%20Our%20approach%20simplifies%20the%20process%20of%20dataset%20creation%2C%20eliminating%20the%0Aneed%20for%20extensive%20domain%20knowledge%20or%20predefined%20fields.%20Compared%20to%0Atraditional%20methods%20like%20Faker%2C%20data%20generated%20by%20LLM%20demonstrates%20greater%0Adiversity%20and%20contextual%20relevance%2C%20leading%20to%20improved%20performance%20in%20barcode%0Adetection%20models.%20This%20scalable%2C%20privacy-first%20solution%20is%20a%20big%20step%20forward%0Ain%20advancing%20machine%20learning%20for%20automated%20document%20processing%20and%20identity%0Averification.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2411.14962v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DLLM%2520for%2520Barcodes%253A%2520Generating%2520Diverse%2520Synthetic%2520Data%2520for%2520Identity%250A%2520%2520Documents%26entry.906535625%3DHitesh%2520Laxmichand%2520Patel%2520and%2520Amit%2520Agarwal%2520and%2520Bhargava%2520Kumar%2520and%2520Karan%2520Gupta%2520and%2520Priyaranjan%2520Pattnayak%26entry.1292438233%3D%2520%2520Accurate%2520barcode%2520detection%2520and%2520decoding%2520in%2520Identity%2520documents%2520is%2520crucial%2520for%250Aapplications%2520like%2520security%252C%2520healthcare%252C%2520and%2520education%252C%2520where%2520reliable%2520data%250Aextraction%2520and%2520verification%2520are%2520essential.%2520However%252C%2520building%2520robust%2520detection%250Amodels%2520is%2520challenging%2520due%2520to%2520the%2520lack%2520of%2520diverse%252C%2520realistic%2520datasets%2520an%2520issue%250Aoften%2520tied%2520to%2520privacy%2520concerns%2520and%2520the%2520wide%2520variety%2520of%2520document%2520formats.%250ATraditional%2520tools%2520like%2520Faker%2520rely%2520on%2520predefined%2520templates%252C%2520making%2520them%2520less%250Aeffective%2520for%2520capturing%2520the%2520complexity%2520of%2520real-world%2520identity%2520documents.%2520In%250Athis%2520paper%252C%2520we%2520introduce%2520a%2520new%2520approach%2520to%2520synthetic%2520data%2520generation%2520that%2520uses%250ALLMs%2520to%2520create%2520contextually%2520rich%2520and%2520realistic%2520data%2520without%2520relying%2520on%250Apredefined%2520field.%2520Using%2520the%2520vast%2520knowledge%2520LLMs%2520have%2520about%2520different%2520documents%250Aand%2520content%252C%2520our%2520method%2520creates%2520data%2520that%2520reflects%2520the%2520variety%2520found%2520in%2520real%250Aidentity%2520documents.%2520This%2520data%2520is%2520then%2520encoded%2520into%2520barcode%2520and%2520overlayed%2520on%250Atemplates%2520for%2520documents%2520such%2520as%2520Driver%2527s%2520licenses%252C%2520Insurance%2520cards%252C%2520Student%250AIDs.%2520Our%2520approach%2520simplifies%2520the%2520process%2520of%2520dataset%2520creation%252C%2520eliminating%2520the%250Aneed%2520for%2520extensive%2520domain%2520knowledge%2520or%2520predefined%2520fields.%2520Compared%2520to%250Atraditional%2520methods%2520like%2520Faker%252C%2520data%2520generated%2520by%2520LLM%2520demonstrates%2520greater%250Adiversity%2520and%2520contextual%2520relevance%252C%2520leading%2520to%2520improved%2520performance%2520in%2520barcode%250Adetection%2520models.%2520This%2520scalable%252C%2520privacy-first%2520solution%2520is%2520a%2520big%2520step%2520forward%250Ain%2520advancing%2520machine%2520learning%2520for%2520automated%2520document%2520processing%2520and%2520identity%250Averification.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2411.14962v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=LLM%20for%20Barcodes%3A%20Generating%20Diverse%20Synthetic%20Data%20for%20Identity%0A%20%20Documents&entry.906535625=Hitesh%20Laxmichand%20Patel%20and%20Amit%20Agarwal%20and%20Bhargava%20Kumar%20and%20Karan%20Gupta%20and%20Priyaranjan%20Pattnayak&entry.1292438233=%20%20Accurate%20barcode%20detection%20and%20decoding%20in%20Identity%20documents%20is%20crucial%20for%0Aapplications%20like%20security%2C%20healthcare%2C%20and%20education%2C%20where%20reliable%20data%0Aextraction%20and%20verification%20are%20essential.%20However%2C%20building%20robust%20detection%0Amodels%20is%20challenging%20due%20to%20the%20lack%20of%20diverse%2C%20realistic%20datasets%20an%20issue%0Aoften%20tied%20to%20privacy%20concerns%20and%20the%20wide%20variety%20of%20document%20formats.%0ATraditional%20tools%20like%20Faker%20rely%20on%20predefined%20templates%2C%20making%20them%20less%0Aeffective%20for%20capturing%20the%20complexity%20of%20real-world%20identity%20documents.%20In%0Athis%20paper%2C%20we%20introduce%20a%20new%20approach%20to%20synthetic%20data%20generation%20that%20uses%0ALLMs%20to%20create%20contextually%20rich%20and%20realistic%20data%20without%20relying%20on%0Apredefined%20field.%20Using%20the%20vast%20knowledge%20LLMs%20have%20about%20different%20documents%0Aand%20content%2C%20our%20method%20creates%20data%20that%20reflects%20the%20variety%20found%20in%20real%0Aidentity%20documents.%20This%20data%20is%20then%20encoded%20into%20barcode%20and%20overlayed%20on%0Atemplates%20for%20documents%20such%20as%20Driver%27s%20licenses%2C%20Insurance%20cards%2C%20Student%0AIDs.%20Our%20approach%20simplifies%20the%20process%20of%20dataset%20creation%2C%20eliminating%20the%0Aneed%20for%20extensive%20domain%20knowledge%20or%20predefined%20fields.%20Compared%20to%0Atraditional%20methods%20like%20Faker%2C%20data%20generated%20by%20LLM%20demonstrates%20greater%0Adiversity%20and%20contextual%20relevance%2C%20leading%20to%20improved%20performance%20in%20barcode%0Adetection%20models.%20This%20scalable%2C%20privacy-first%20solution%20is%20a%20big%20step%20forward%0Ain%20advancing%20machine%20learning%20for%20automated%20document%20processing%20and%20identity%0Averification.%0A&entry.1838667208=http%3A//arxiv.org/abs/2411.14962v1&entry.124074799=Read"},
{"title": "Discrete Latent Structure in Neural Networks", "author": "Vlad Niculae and Caio F. Corro and Nikita Nangia and Tsvetomila Mihaylova and Andr\u00e9 F. T. Martins", "abstract": "  Many types of data from fields including natural language processing,\ncomputer vision, and bioinformatics, are well represented by discrete,\ncompositional structures such as trees, sequences, or matchings. Latent\nstructure models are a powerful tool for learning to extract such\nrepresentations, offering a way to incorporate structural bias, discover\ninsight about the data, and interpret decisions. However, effective training is\nchallenging, as neural networks are typically designed for continuous\ncomputation.\n  This text explores three broad strategies for learning with discrete latent\nstructure: continuous relaxation, surrogate gradients, and probabilistic\nestimation. Our presentation relies on consistent notations for a wide range of\nmodels. As such, we reveal many new connections between latent structure\nlearning strategies, showing how most consist of the same small set of\nfundamental building blocks, but use them differently, leading to substantially\ndifferent applicability and properties.\n", "link": "http://arxiv.org/abs/2301.07473v2", "date": "2024-11-22", "relevancy": 2.0957, "topK": [{"title": "SDXS: Real-Time One-Step Latent Diffusion Models with Image Conditions", "link": "http://arxiv.org/abs/2403.16627v1", "similarity": 0.5285}, {"title": "Self-supervised Photographic Image Layout Representation Learning", "link": "http://arxiv.org/abs/2403.03740v1", "similarity": 0.5252}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5208}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Discrete%20Latent%20Structure%20in%20Neural%20Networks&body=Title%3A%20Discrete%20Latent%20Structure%20in%20Neural%20Networks%0AAuthor%3A%20Vlad%20Niculae%20and%20Caio%20F.%20Corro%20and%20Nikita%20Nangia%20and%20Tsvetomila%20Mihaylova%20and%20Andr%C3%A9%20F.%20T.%20Martins%0AAbstract%3A%20%20%20Many%20types%20of%20data%20from%20fields%20including%20natural%20language%20processing%2C%0Acomputer%20vision%2C%20and%20bioinformatics%2C%20are%20well%20represented%20by%20discrete%2C%0Acompositional%20structures%20such%20as%20trees%2C%20sequences%2C%20or%20matchings.%20Latent%0Astructure%20models%20are%20a%20powerful%20tool%20for%20learning%20to%20extract%20such%0Arepresentations%2C%20offering%20a%20way%20to%20incorporate%20structural%20bias%2C%20discover%0Ainsight%20about%20the%20data%2C%20and%20interpret%20decisions.%20However%2C%20effective%20training%20is%0Achallenging%2C%20as%20neural%20networks%20are%20typically%20designed%20for%20continuous%0Acomputation.%0A%20%20This%20text%20explores%20three%20broad%20strategies%20for%20learning%20with%20discrete%20latent%0Astructure%3A%20continuous%20relaxation%2C%20surrogate%20gradients%2C%20and%20probabilistic%0Aestimation.%20Our%20presentation%20relies%20on%20consistent%20notations%20for%20a%20wide%20range%20of%0Amodels.%20As%20such%2C%20we%20reveal%20many%20new%20connections%20between%20latent%20structure%0Alearning%20strategies%2C%20showing%20how%20most%20consist%20of%20the%20same%20small%20set%20of%0Afundamental%20building%20blocks%2C%20but%20use%20them%20differently%2C%20leading%20to%20substantially%0Adifferent%20applicability%20and%20properties.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2301.07473v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DDiscrete%2520Latent%2520Structure%2520in%2520Neural%2520Networks%26entry.906535625%3DVlad%2520Niculae%2520and%2520Caio%2520F.%2520Corro%2520and%2520Nikita%2520Nangia%2520and%2520Tsvetomila%2520Mihaylova%2520and%2520Andr%25C3%25A9%2520F.%2520T.%2520Martins%26entry.1292438233%3D%2520%2520Many%2520types%2520of%2520data%2520from%2520fields%2520including%2520natural%2520language%2520processing%252C%250Acomputer%2520vision%252C%2520and%2520bioinformatics%252C%2520are%2520well%2520represented%2520by%2520discrete%252C%250Acompositional%2520structures%2520such%2520as%2520trees%252C%2520sequences%252C%2520or%2520matchings.%2520Latent%250Astructure%2520models%2520are%2520a%2520powerful%2520tool%2520for%2520learning%2520to%2520extract%2520such%250Arepresentations%252C%2520offering%2520a%2520way%2520to%2520incorporate%2520structural%2520bias%252C%2520discover%250Ainsight%2520about%2520the%2520data%252C%2520and%2520interpret%2520decisions.%2520However%252C%2520effective%2520training%2520is%250Achallenging%252C%2520as%2520neural%2520networks%2520are%2520typically%2520designed%2520for%2520continuous%250Acomputation.%250A%2520%2520This%2520text%2520explores%2520three%2520broad%2520strategies%2520for%2520learning%2520with%2520discrete%2520latent%250Astructure%253A%2520continuous%2520relaxation%252C%2520surrogate%2520gradients%252C%2520and%2520probabilistic%250Aestimation.%2520Our%2520presentation%2520relies%2520on%2520consistent%2520notations%2520for%2520a%2520wide%2520range%2520of%250Amodels.%2520As%2520such%252C%2520we%2520reveal%2520many%2520new%2520connections%2520between%2520latent%2520structure%250Alearning%2520strategies%252C%2520showing%2520how%2520most%2520consist%2520of%2520the%2520same%2520small%2520set%2520of%250Afundamental%2520building%2520blocks%252C%2520but%2520use%2520them%2520differently%252C%2520leading%2520to%2520substantially%250Adifferent%2520applicability%2520and%2520properties.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2301.07473v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Discrete%20Latent%20Structure%20in%20Neural%20Networks&entry.906535625=Vlad%20Niculae%20and%20Caio%20F.%20Corro%20and%20Nikita%20Nangia%20and%20Tsvetomila%20Mihaylova%20and%20Andr%C3%A9%20F.%20T.%20Martins&entry.1292438233=%20%20Many%20types%20of%20data%20from%20fields%20including%20natural%20language%20processing%2C%0Acomputer%20vision%2C%20and%20bioinformatics%2C%20are%20well%20represented%20by%20discrete%2C%0Acompositional%20structures%20such%20as%20trees%2C%20sequences%2C%20or%20matchings.%20Latent%0Astructure%20models%20are%20a%20powerful%20tool%20for%20learning%20to%20extract%20such%0Arepresentations%2C%20offering%20a%20way%20to%20incorporate%20structural%20bias%2C%20discover%0Ainsight%20about%20the%20data%2C%20and%20interpret%20decisions.%20However%2C%20effective%20training%20is%0Achallenging%2C%20as%20neural%20networks%20are%20typically%20designed%20for%20continuous%0Acomputation.%0A%20%20This%20text%20explores%20three%20broad%20strategies%20for%20learning%20with%20discrete%20latent%0Astructure%3A%20continuous%20relaxation%2C%20surrogate%20gradients%2C%20and%20probabilistic%0Aestimation.%20Our%20presentation%20relies%20on%20consistent%20notations%20for%20a%20wide%20range%20of%0Amodels.%20As%20such%2C%20we%20reveal%20many%20new%20connections%20between%20latent%20structure%0Alearning%20strategies%2C%20showing%20how%20most%20consist%20of%20the%20same%20small%20set%20of%0Afundamental%20building%20blocks%2C%20but%20use%20them%20differently%2C%20leading%20to%20substantially%0Adifferent%20applicability%20and%20properties.%0A&entry.1838667208=http%3A//arxiv.org/abs/2301.07473v2&entry.124074799=Read"},
{"title": "Unveil Benign Overfitting for Transformer in Vision: Training Dynamics,\n  Convergence, and Generalization", "author": "Jiarui Jiang and Wei Huang and Miao Zhang and Taiji Suzuki and Liqiang Nie", "abstract": "  Transformers have demonstrated great power in the recent development of large\nfoundational models. In particular, the Vision Transformer (ViT) has brought\nrevolutionary changes to the field of vision, achieving significant\naccomplishments on the experimental side. However, their theoretical\ncapabilities, particularly in terms of generalization when trained to overfit\ntraining data, are still not fully understood. To address this gap, this work\ndelves deeply into the benign overfitting perspective of transformers in\nvision. To this end, we study the optimization of a Transformer composed of a\nself-attention layer with softmax followed by a fully connected layer under\ngradient descent on a certain data distribution model. By developing techniques\nthat address the challenges posed by softmax and the interdependent nature of\nmultiple weights in transformer optimization, we successfully characterized the\ntraining dynamics and achieved generalization in post-training. Our results\nestablish a sharp condition that can distinguish between the small test error\nphase and the large test error regime, based on the signal-to-noise ratio in\nthe data model. The theoretical results are further verified by experimental\nsimulation. To the best of our knowledge, this is the first work to\ncharacterize benign overfitting for Transformers.\n", "link": "http://arxiv.org/abs/2409.19345v2", "date": "2024-11-22", "relevancy": 2.0817, "topK": [{"title": "Scaling Rectified Flow Transformers for High-Resolution Image Synthesis", "link": "http://arxiv.org/abs/2403.03206v1", "similarity": 0.5892}, {"title": "Boosted dynamic neural networks", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/26302", "similarity": 0.5179}, {"title": "Tuning-Free Noise Rectification for High Fidelity Image-to-Video\n  Generation", "link": "http://arxiv.org/abs/2403.02827v1", "similarity": 0.4955}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Unveil%20Benign%20Overfitting%20for%20Transformer%20in%20Vision%3A%20Training%20Dynamics%2C%0A%20%20Convergence%2C%20and%20Generalization&body=Title%3A%20Unveil%20Benign%20Overfitting%20for%20Transformer%20in%20Vision%3A%20Training%20Dynamics%2C%0A%20%20Convergence%2C%20and%20Generalization%0AAuthor%3A%20Jiarui%20Jiang%20and%20Wei%20Huang%20and%20Miao%20Zhang%20and%20Taiji%20Suzuki%20and%20Liqiang%20Nie%0AAbstract%3A%20%20%20Transformers%20have%20demonstrated%20great%20power%20in%20the%20recent%20development%20of%20large%0Afoundational%20models.%20In%20particular%2C%20the%20Vision%20Transformer%20%28ViT%29%20has%20brought%0Arevolutionary%20changes%20to%20the%20field%20of%20vision%2C%20achieving%20significant%0Aaccomplishments%20on%20the%20experimental%20side.%20However%2C%20their%20theoretical%0Acapabilities%2C%20particularly%20in%20terms%20of%20generalization%20when%20trained%20to%20overfit%0Atraining%20data%2C%20are%20still%20not%20fully%20understood.%20To%20address%20this%20gap%2C%20this%20work%0Adelves%20deeply%20into%20the%20benign%20overfitting%20perspective%20of%20transformers%20in%0Avision.%20To%20this%20end%2C%20we%20study%20the%20optimization%20of%20a%20Transformer%20composed%20of%20a%0Aself-attention%20layer%20with%20softmax%20followed%20by%20a%20fully%20connected%20layer%20under%0Agradient%20descent%20on%20a%20certain%20data%20distribution%20model.%20By%20developing%20techniques%0Athat%20address%20the%20challenges%20posed%20by%20softmax%20and%20the%20interdependent%20nature%20of%0Amultiple%20weights%20in%20transformer%20optimization%2C%20we%20successfully%20characterized%20the%0Atraining%20dynamics%20and%20achieved%20generalization%20in%20post-training.%20Our%20results%0Aestablish%20a%20sharp%20condition%20that%20can%20distinguish%20between%20the%20small%20test%20error%0Aphase%20and%20the%20large%20test%20error%20regime%2C%20based%20on%20the%20signal-to-noise%20ratio%20in%0Athe%20data%20model.%20The%20theoretical%20results%20are%20further%20verified%20by%20experimental%0Asimulation.%20To%20the%20best%20of%20our%20knowledge%2C%20this%20is%20the%20first%20work%20to%0Acharacterize%20benign%20overfitting%20for%20Transformers.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2409.19345v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DUnveil%2520Benign%2520Overfitting%2520for%2520Transformer%2520in%2520Vision%253A%2520Training%2520Dynamics%252C%250A%2520%2520Convergence%252C%2520and%2520Generalization%26entry.906535625%3DJiarui%2520Jiang%2520and%2520Wei%2520Huang%2520and%2520Miao%2520Zhang%2520and%2520Taiji%2520Suzuki%2520and%2520Liqiang%2520Nie%26entry.1292438233%3D%2520%2520Transformers%2520have%2520demonstrated%2520great%2520power%2520in%2520the%2520recent%2520development%2520of%2520large%250Afoundational%2520models.%2520In%2520particular%252C%2520the%2520Vision%2520Transformer%2520%2528ViT%2529%2520has%2520brought%250Arevolutionary%2520changes%2520to%2520the%2520field%2520of%2520vision%252C%2520achieving%2520significant%250Aaccomplishments%2520on%2520the%2520experimental%2520side.%2520However%252C%2520their%2520theoretical%250Acapabilities%252C%2520particularly%2520in%2520terms%2520of%2520generalization%2520when%2520trained%2520to%2520overfit%250Atraining%2520data%252C%2520are%2520still%2520not%2520fully%2520understood.%2520To%2520address%2520this%2520gap%252C%2520this%2520work%250Adelves%2520deeply%2520into%2520the%2520benign%2520overfitting%2520perspective%2520of%2520transformers%2520in%250Avision.%2520To%2520this%2520end%252C%2520we%2520study%2520the%2520optimization%2520of%2520a%2520Transformer%2520composed%2520of%2520a%250Aself-attention%2520layer%2520with%2520softmax%2520followed%2520by%2520a%2520fully%2520connected%2520layer%2520under%250Agradient%2520descent%2520on%2520a%2520certain%2520data%2520distribution%2520model.%2520By%2520developing%2520techniques%250Athat%2520address%2520the%2520challenges%2520posed%2520by%2520softmax%2520and%2520the%2520interdependent%2520nature%2520of%250Amultiple%2520weights%2520in%2520transformer%2520optimization%252C%2520we%2520successfully%2520characterized%2520the%250Atraining%2520dynamics%2520and%2520achieved%2520generalization%2520in%2520post-training.%2520Our%2520results%250Aestablish%2520a%2520sharp%2520condition%2520that%2520can%2520distinguish%2520between%2520the%2520small%2520test%2520error%250Aphase%2520and%2520the%2520large%2520test%2520error%2520regime%252C%2520based%2520on%2520the%2520signal-to-noise%2520ratio%2520in%250Athe%2520data%2520model.%2520The%2520theoretical%2520results%2520are%2520further%2520verified%2520by%2520experimental%250Asimulation.%2520To%2520the%2520best%2520of%2520our%2520knowledge%252C%2520this%2520is%2520the%2520first%2520work%2520to%250Acharacterize%2520benign%2520overfitting%2520for%2520Transformers.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2409.19345v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Unveil%20Benign%20Overfitting%20for%20Transformer%20in%20Vision%3A%20Training%20Dynamics%2C%0A%20%20Convergence%2C%20and%20Generalization&entry.906535625=Jiarui%20Jiang%20and%20Wei%20Huang%20and%20Miao%20Zhang%20and%20Taiji%20Suzuki%20and%20Liqiang%20Nie&entry.1292438233=%20%20Transformers%20have%20demonstrated%20great%20power%20in%20the%20recent%20development%20of%20large%0Afoundational%20models.%20In%20particular%2C%20the%20Vision%20Transformer%20%28ViT%29%20has%20brought%0Arevolutionary%20changes%20to%20the%20field%20of%20vision%2C%20achieving%20significant%0Aaccomplishments%20on%20the%20experimental%20side.%20However%2C%20their%20theoretical%0Acapabilities%2C%20particularly%20in%20terms%20of%20generalization%20when%20trained%20to%20overfit%0Atraining%20data%2C%20are%20still%20not%20fully%20understood.%20To%20address%20this%20gap%2C%20this%20work%0Adelves%20deeply%20into%20the%20benign%20overfitting%20perspective%20of%20transformers%20in%0Avision.%20To%20this%20end%2C%20we%20study%20the%20optimization%20of%20a%20Transformer%20composed%20of%20a%0Aself-attention%20layer%20with%20softmax%20followed%20by%20a%20fully%20connected%20layer%20under%0Agradient%20descent%20on%20a%20certain%20data%20distribution%20model.%20By%20developing%20techniques%0Athat%20address%20the%20challenges%20posed%20by%20softmax%20and%20the%20interdependent%20nature%20of%0Amultiple%20weights%20in%20transformer%20optimization%2C%20we%20successfully%20characterized%20the%0Atraining%20dynamics%20and%20achieved%20generalization%20in%20post-training.%20Our%20results%0Aestablish%20a%20sharp%20condition%20that%20can%20distinguish%20between%20the%20small%20test%20error%0Aphase%20and%20the%20large%20test%20error%20regime%2C%20based%20on%20the%20signal-to-noise%20ratio%20in%0Athe%20data%20model.%20The%20theoretical%20results%20are%20further%20verified%20by%20experimental%0Asimulation.%20To%20the%20best%20of%20our%20knowledge%2C%20this%20is%20the%20first%20work%20to%0Acharacterize%20benign%20overfitting%20for%20Transformers.%0A&entry.1838667208=http%3A//arxiv.org/abs/2409.19345v2&entry.124074799=Read"},
{"title": "Implementation of Real-Time Lane Detection on Autonomous Mobile Robot", "author": "Midriem Mirdanies and Roni Permana Saputra and Edwar Yazid and Rozeha A. Rashid", "abstract": "  This paper describes the implementation of a learning-based lane detection\nalgorithm on an Autonomous Mobile Robot. It aims to implement the Ultra Fast\nLane Detection algorithm for real-time application on the SEATER P2MC-BRIN\nprototype using a camera and optimize its performance on the Jetson Nano\nplatform. Preliminary experiments were conducted to evaluate the algorithm's\nperformance in terms of data processing speed and accuracy using two types of\ndatasets: outdoor using a public dataset and indoor using an internal dataset\nfrom the indoor area of the BRIN Workshop Building in Bandung. The experiments\nrevealed that the algorithm runs more optimally on the Jetson Nano platform\nafter conversion to TensorRT compared to the ONNX model, achieving processing\nspeeds of approximately 101 ms using CULane and 105 ms using TuSimple, which is\nabout 22 times faster than the previous model. While the algorithm demonstrates\ngood accuracy on the outdoor public dataset, its performance falls short on the\nindoor dataset. Future work should focus on transfer learning and fine-tuning\nto enhance indoor lane detection accuracy.\n", "link": "http://arxiv.org/abs/2411.14873v1", "date": "2024-11-22", "relevancy": 2.0762, "topK": [{"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.5672}, {"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.529}, {"title": "RTG-SLAM: Real-time 3D Reconstruction at Scale using Gaussian Splatting", "link": "http://arxiv.org/abs/2404.19706v1", "similarity": 0.4898}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Implementation%20of%20Real-Time%20Lane%20Detection%20on%20Autonomous%20Mobile%20Robot&body=Title%3A%20Implementation%20of%20Real-Time%20Lane%20Detection%20on%20Autonomous%20Mobile%20Robot%0AAuthor%3A%20Midriem%20Mirdanies%20and%20Roni%20Permana%20Saputra%20and%20Edwar%20Yazid%20and%20Rozeha%20A.%20Rashid%0AAbstract%3A%20%20%20This%20paper%20describes%20the%20implementation%20of%20a%20learning-based%20lane%20detection%0Aalgorithm%20on%20an%20Autonomous%20Mobile%20Robot.%20It%20aims%20to%20implement%20the%20Ultra%20Fast%0ALane%20Detection%20algorithm%20for%20real-time%20application%20on%20the%20SEATER%20P2MC-BRIN%0Aprototype%20using%20a%20camera%20and%20optimize%20its%20performance%20on%20the%20Jetson%20Nano%0Aplatform.%20Preliminary%20experiments%20were%20conducted%20to%20evaluate%20the%20algorithm%27s%0Aperformance%20in%20terms%20of%20data%20processing%20speed%20and%20accuracy%20using%20two%20types%20of%0Adatasets%3A%20outdoor%20using%20a%20public%20dataset%20and%20indoor%20using%20an%20internal%20dataset%0Afrom%20the%20indoor%20area%20of%20the%20BRIN%20Workshop%20Building%20in%20Bandung.%20The%20experiments%0Arevealed%20that%20the%20algorithm%20runs%20more%20optimally%20on%20the%20Jetson%20Nano%20platform%0Aafter%20conversion%20to%20TensorRT%20compared%20to%20the%20ONNX%20model%2C%20achieving%20processing%0Aspeeds%20of%20approximately%20101%20ms%20using%20CULane%20and%20105%20ms%20using%20TuSimple%2C%20which%20is%0Aabout%2022%20times%20faster%20than%20the%20previous%20model.%20While%20the%20algorithm%20demonstrates%0Agood%20accuracy%20on%20the%20outdoor%20public%20dataset%2C%20its%20performance%20falls%20short%20on%20the%0Aindoor%20dataset.%20Future%20work%20should%20focus%20on%20transfer%20learning%20and%20fine-tuning%0Ato%20enhance%20indoor%20lane%20detection%20accuracy.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2411.14873v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DImplementation%2520of%2520Real-Time%2520Lane%2520Detection%2520on%2520Autonomous%2520Mobile%2520Robot%26entry.906535625%3DMidriem%2520Mirdanies%2520and%2520Roni%2520Permana%2520Saputra%2520and%2520Edwar%2520Yazid%2520and%2520Rozeha%2520A.%2520Rashid%26entry.1292438233%3D%2520%2520This%2520paper%2520describes%2520the%2520implementation%2520of%2520a%2520learning-based%2520lane%2520detection%250Aalgorithm%2520on%2520an%2520Autonomous%2520Mobile%2520Robot.%2520It%2520aims%2520to%2520implement%2520the%2520Ultra%2520Fast%250ALane%2520Detection%2520algorithm%2520for%2520real-time%2520application%2520on%2520the%2520SEATER%2520P2MC-BRIN%250Aprototype%2520using%2520a%2520camera%2520and%2520optimize%2520its%2520performance%2520on%2520the%2520Jetson%2520Nano%250Aplatform.%2520Preliminary%2520experiments%2520were%2520conducted%2520to%2520evaluate%2520the%2520algorithm%2527s%250Aperformance%2520in%2520terms%2520of%2520data%2520processing%2520speed%2520and%2520accuracy%2520using%2520two%2520types%2520of%250Adatasets%253A%2520outdoor%2520using%2520a%2520public%2520dataset%2520and%2520indoor%2520using%2520an%2520internal%2520dataset%250Afrom%2520the%2520indoor%2520area%2520of%2520the%2520BRIN%2520Workshop%2520Building%2520in%2520Bandung.%2520The%2520experiments%250Arevealed%2520that%2520the%2520algorithm%2520runs%2520more%2520optimally%2520on%2520the%2520Jetson%2520Nano%2520platform%250Aafter%2520conversion%2520to%2520TensorRT%2520compared%2520to%2520the%2520ONNX%2520model%252C%2520achieving%2520processing%250Aspeeds%2520of%2520approximately%2520101%2520ms%2520using%2520CULane%2520and%2520105%2520ms%2520using%2520TuSimple%252C%2520which%2520is%250Aabout%252022%2520times%2520faster%2520than%2520the%2520previous%2520model.%2520While%2520the%2520algorithm%2520demonstrates%250Agood%2520accuracy%2520on%2520the%2520outdoor%2520public%2520dataset%252C%2520its%2520performance%2520falls%2520short%2520on%2520the%250Aindoor%2520dataset.%2520Future%2520work%2520should%2520focus%2520on%2520transfer%2520learning%2520and%2520fine-tuning%250Ato%2520enhance%2520indoor%2520lane%2520detection%2520accuracy.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2411.14873v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Implementation%20of%20Real-Time%20Lane%20Detection%20on%20Autonomous%20Mobile%20Robot&entry.906535625=Midriem%20Mirdanies%20and%20Roni%20Permana%20Saputra%20and%20Edwar%20Yazid%20and%20Rozeha%20A.%20Rashid&entry.1292438233=%20%20This%20paper%20describes%20the%20implementation%20of%20a%20learning-based%20lane%20detection%0Aalgorithm%20on%20an%20Autonomous%20Mobile%20Robot.%20It%20aims%20to%20implement%20the%20Ultra%20Fast%0ALane%20Detection%20algorithm%20for%20real-time%20application%20on%20the%20SEATER%20P2MC-BRIN%0Aprototype%20using%20a%20camera%20and%20optimize%20its%20performance%20on%20the%20Jetson%20Nano%0Aplatform.%20Preliminary%20experiments%20were%20conducted%20to%20evaluate%20the%20algorithm%27s%0Aperformance%20in%20terms%20of%20data%20processing%20speed%20and%20accuracy%20using%20two%20types%20of%0Adatasets%3A%20outdoor%20using%20a%20public%20dataset%20and%20indoor%20using%20an%20internal%20dataset%0Afrom%20the%20indoor%20area%20of%20the%20BRIN%20Workshop%20Building%20in%20Bandung.%20The%20experiments%0Arevealed%20that%20the%20algorithm%20runs%20more%20optimally%20on%20the%20Jetson%20Nano%20platform%0Aafter%20conversion%20to%20TensorRT%20compared%20to%20the%20ONNX%20model%2C%20achieving%20processing%0Aspeeds%20of%20approximately%20101%20ms%20using%20CULane%20and%20105%20ms%20using%20TuSimple%2C%20which%20is%0Aabout%2022%20times%20faster%20than%20the%20previous%20model.%20While%20the%20algorithm%20demonstrates%0Agood%20accuracy%20on%20the%20outdoor%20public%20dataset%2C%20its%20performance%20falls%20short%20on%20the%0Aindoor%20dataset.%20Future%20work%20should%20focus%20on%20transfer%20learning%20and%20fine-tuning%0Ato%20enhance%20indoor%20lane%20detection%20accuracy.%0A&entry.1838667208=http%3A//arxiv.org/abs/2411.14873v1&entry.124074799=Read"},
{"title": "Improving the accuracy of automated labeling of specimen images datasets\n  via a confidence-based process", "author": "Quentin Bateux and Jonathan Koss and Patrick W. Sweeney and Erika Edwards and Nelson Rios and Aaron M. Dollar", "abstract": "  The digitization of natural history collections over the past three decades\nhas unlocked a treasure trove of specimen imagery and metadata. There is great\ninterest in making this data more useful by further labeling it with additional\ntrait data, and modern deep learning machine learning techniques utilizing\nconvolutional neural nets (CNNs) and similar networks show particular promise\nto reduce the amount of required manual labeling by human experts, making the\nprocess much faster and less expensive. However, in most cases, the accuracy of\nthese approaches is too low for reliable utilization of the automatic labeling,\ntypically in the range of 80-85% accuracy. In this paper, we present and\nvalidate an approach that can greatly improve this accuracy, essentially by\nexamining the confidence that the network has in the generated label as well as\nutilizing a user-defined threshold to reject labels that fall below a chosen\nlevel. We demonstrate that a naive model that produced 86% initial accuracy can\nachieve improved performance - over 95% accuracy (rejecting about 40% of the\nlabels) or over 99% accuracy (rejecting about 65%) by selecting higher\nconfidence thresholds. This gives flexibility to adapt existing models to the\nstatistical requirements of various types of research and has the potential to\nmove these automatic labeling approaches from being unusably inaccurate to\nbeing an invaluable new tool. After validating the approach in a number of\nways, we annotate the reproductive state of a large dataset of over 600,000\nherbarium specimens. The analysis of the results points at under-investigated\ncorrelations as well as general alignment with known trends. By sharing this\nnew dataset alongside this work, we want to allow ecologists to gather insights\nfor their own research questions, at their chosen point of accuracy/coverage\ntrade-off.\n", "link": "http://arxiv.org/abs/2411.10074v2", "date": "2024-11-22", "relevancy": 2.0559, "topK": [{"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.576}, {"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.5166}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.4866}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Improving%20the%20accuracy%20of%20automated%20labeling%20of%20specimen%20images%20datasets%0A%20%20via%20a%20confidence-based%20process&body=Title%3A%20Improving%20the%20accuracy%20of%20automated%20labeling%20of%20specimen%20images%20datasets%0A%20%20via%20a%20confidence-based%20process%0AAuthor%3A%20Quentin%20Bateux%20and%20Jonathan%20Koss%20and%20Patrick%20W.%20Sweeney%20and%20Erika%20Edwards%20and%20Nelson%20Rios%20and%20Aaron%20M.%20Dollar%0AAbstract%3A%20%20%20The%20digitization%20of%20natural%20history%20collections%20over%20the%20past%20three%20decades%0Ahas%20unlocked%20a%20treasure%20trove%20of%20specimen%20imagery%20and%20metadata.%20There%20is%20great%0Ainterest%20in%20making%20this%20data%20more%20useful%20by%20further%20labeling%20it%20with%20additional%0Atrait%20data%2C%20and%20modern%20deep%20learning%20machine%20learning%20techniques%20utilizing%0Aconvolutional%20neural%20nets%20%28CNNs%29%20and%20similar%20networks%20show%20particular%20promise%0Ato%20reduce%20the%20amount%20of%20required%20manual%20labeling%20by%20human%20experts%2C%20making%20the%0Aprocess%20much%20faster%20and%20less%20expensive.%20However%2C%20in%20most%20cases%2C%20the%20accuracy%20of%0Athese%20approaches%20is%20too%20low%20for%20reliable%20utilization%20of%20the%20automatic%20labeling%2C%0Atypically%20in%20the%20range%20of%2080-85%25%20accuracy.%20In%20this%20paper%2C%20we%20present%20and%0Avalidate%20an%20approach%20that%20can%20greatly%20improve%20this%20accuracy%2C%20essentially%20by%0Aexamining%20the%20confidence%20that%20the%20network%20has%20in%20the%20generated%20label%20as%20well%20as%0Autilizing%20a%20user-defined%20threshold%20to%20reject%20labels%20that%20fall%20below%20a%20chosen%0Alevel.%20We%20demonstrate%20that%20a%20naive%20model%20that%20produced%2086%25%20initial%20accuracy%20can%0Aachieve%20improved%20performance%20-%20over%2095%25%20accuracy%20%28rejecting%20about%2040%25%20of%20the%0Alabels%29%20or%20over%2099%25%20accuracy%20%28rejecting%20about%2065%25%29%20by%20selecting%20higher%0Aconfidence%20thresholds.%20This%20gives%20flexibility%20to%20adapt%20existing%20models%20to%20the%0Astatistical%20requirements%20of%20various%20types%20of%20research%20and%20has%20the%20potential%20to%0Amove%20these%20automatic%20labeling%20approaches%20from%20being%20unusably%20inaccurate%20to%0Abeing%20an%20invaluable%20new%20tool.%20After%20validating%20the%20approach%20in%20a%20number%20of%0Aways%2C%20we%20annotate%20the%20reproductive%20state%20of%20a%20large%20dataset%20of%20over%20600%2C000%0Aherbarium%20specimens.%20The%20analysis%20of%20the%20results%20points%20at%20under-investigated%0Acorrelations%20as%20well%20as%20general%20alignment%20with%20known%20trends.%20By%20sharing%20this%0Anew%20dataset%20alongside%20this%20work%2C%20we%20want%20to%20allow%20ecologists%20to%20gather%20insights%0Afor%20their%20own%20research%20questions%2C%20at%20their%20chosen%20point%20of%20accuracy/coverage%0Atrade-off.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2411.10074v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DImproving%2520the%2520accuracy%2520of%2520automated%2520labeling%2520of%2520specimen%2520images%2520datasets%250A%2520%2520via%2520a%2520confidence-based%2520process%26entry.906535625%3DQuentin%2520Bateux%2520and%2520Jonathan%2520Koss%2520and%2520Patrick%2520W.%2520Sweeney%2520and%2520Erika%2520Edwards%2520and%2520Nelson%2520Rios%2520and%2520Aaron%2520M.%2520Dollar%26entry.1292438233%3D%2520%2520The%2520digitization%2520of%2520natural%2520history%2520collections%2520over%2520the%2520past%2520three%2520decades%250Ahas%2520unlocked%2520a%2520treasure%2520trove%2520of%2520specimen%2520imagery%2520and%2520metadata.%2520There%2520is%2520great%250Ainterest%2520in%2520making%2520this%2520data%2520more%2520useful%2520by%2520further%2520labeling%2520it%2520with%2520additional%250Atrait%2520data%252C%2520and%2520modern%2520deep%2520learning%2520machine%2520learning%2520techniques%2520utilizing%250Aconvolutional%2520neural%2520nets%2520%2528CNNs%2529%2520and%2520similar%2520networks%2520show%2520particular%2520promise%250Ato%2520reduce%2520the%2520amount%2520of%2520required%2520manual%2520labeling%2520by%2520human%2520experts%252C%2520making%2520the%250Aprocess%2520much%2520faster%2520and%2520less%2520expensive.%2520However%252C%2520in%2520most%2520cases%252C%2520the%2520accuracy%2520of%250Athese%2520approaches%2520is%2520too%2520low%2520for%2520reliable%2520utilization%2520of%2520the%2520automatic%2520labeling%252C%250Atypically%2520in%2520the%2520range%2520of%252080-85%2525%2520accuracy.%2520In%2520this%2520paper%252C%2520we%2520present%2520and%250Avalidate%2520an%2520approach%2520that%2520can%2520greatly%2520improve%2520this%2520accuracy%252C%2520essentially%2520by%250Aexamining%2520the%2520confidence%2520that%2520the%2520network%2520has%2520in%2520the%2520generated%2520label%2520as%2520well%2520as%250Autilizing%2520a%2520user-defined%2520threshold%2520to%2520reject%2520labels%2520that%2520fall%2520below%2520a%2520chosen%250Alevel.%2520We%2520demonstrate%2520that%2520a%2520naive%2520model%2520that%2520produced%252086%2525%2520initial%2520accuracy%2520can%250Aachieve%2520improved%2520performance%2520-%2520over%252095%2525%2520accuracy%2520%2528rejecting%2520about%252040%2525%2520of%2520the%250Alabels%2529%2520or%2520over%252099%2525%2520accuracy%2520%2528rejecting%2520about%252065%2525%2529%2520by%2520selecting%2520higher%250Aconfidence%2520thresholds.%2520This%2520gives%2520flexibility%2520to%2520adapt%2520existing%2520models%2520to%2520the%250Astatistical%2520requirements%2520of%2520various%2520types%2520of%2520research%2520and%2520has%2520the%2520potential%2520to%250Amove%2520these%2520automatic%2520labeling%2520approaches%2520from%2520being%2520unusably%2520inaccurate%2520to%250Abeing%2520an%2520invaluable%2520new%2520tool.%2520After%2520validating%2520the%2520approach%2520in%2520a%2520number%2520of%250Aways%252C%2520we%2520annotate%2520the%2520reproductive%2520state%2520of%2520a%2520large%2520dataset%2520of%2520over%2520600%252C000%250Aherbarium%2520specimens.%2520The%2520analysis%2520of%2520the%2520results%2520points%2520at%2520under-investigated%250Acorrelations%2520as%2520well%2520as%2520general%2520alignment%2520with%2520known%2520trends.%2520By%2520sharing%2520this%250Anew%2520dataset%2520alongside%2520this%2520work%252C%2520we%2520want%2520to%2520allow%2520ecologists%2520to%2520gather%2520insights%250Afor%2520their%2520own%2520research%2520questions%252C%2520at%2520their%2520chosen%2520point%2520of%2520accuracy/coverage%250Atrade-off.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2411.10074v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Improving%20the%20accuracy%20of%20automated%20labeling%20of%20specimen%20images%20datasets%0A%20%20via%20a%20confidence-based%20process&entry.906535625=Quentin%20Bateux%20and%20Jonathan%20Koss%20and%20Patrick%20W.%20Sweeney%20and%20Erika%20Edwards%20and%20Nelson%20Rios%20and%20Aaron%20M.%20Dollar&entry.1292438233=%20%20The%20digitization%20of%20natural%20history%20collections%20over%20the%20past%20three%20decades%0Ahas%20unlocked%20a%20treasure%20trove%20of%20specimen%20imagery%20and%20metadata.%20There%20is%20great%0Ainterest%20in%20making%20this%20data%20more%20useful%20by%20further%20labeling%20it%20with%20additional%0Atrait%20data%2C%20and%20modern%20deep%20learning%20machine%20learning%20techniques%20utilizing%0Aconvolutional%20neural%20nets%20%28CNNs%29%20and%20similar%20networks%20show%20particular%20promise%0Ato%20reduce%20the%20amount%20of%20required%20manual%20labeling%20by%20human%20experts%2C%20making%20the%0Aprocess%20much%20faster%20and%20less%20expensive.%20However%2C%20in%20most%20cases%2C%20the%20accuracy%20of%0Athese%20approaches%20is%20too%20low%20for%20reliable%20utilization%20of%20the%20automatic%20labeling%2C%0Atypically%20in%20the%20range%20of%2080-85%25%20accuracy.%20In%20this%20paper%2C%20we%20present%20and%0Avalidate%20an%20approach%20that%20can%20greatly%20improve%20this%20accuracy%2C%20essentially%20by%0Aexamining%20the%20confidence%20that%20the%20network%20has%20in%20the%20generated%20label%20as%20well%20as%0Autilizing%20a%20user-defined%20threshold%20to%20reject%20labels%20that%20fall%20below%20a%20chosen%0Alevel.%20We%20demonstrate%20that%20a%20naive%20model%20that%20produced%2086%25%20initial%20accuracy%20can%0Aachieve%20improved%20performance%20-%20over%2095%25%20accuracy%20%28rejecting%20about%2040%25%20of%20the%0Alabels%29%20or%20over%2099%25%20accuracy%20%28rejecting%20about%2065%25%29%20by%20selecting%20higher%0Aconfidence%20thresholds.%20This%20gives%20flexibility%20to%20adapt%20existing%20models%20to%20the%0Astatistical%20requirements%20of%20various%20types%20of%20research%20and%20has%20the%20potential%20to%0Amove%20these%20automatic%20labeling%20approaches%20from%20being%20unusably%20inaccurate%20to%0Abeing%20an%20invaluable%20new%20tool.%20After%20validating%20the%20approach%20in%20a%20number%20of%0Aways%2C%20we%20annotate%20the%20reproductive%20state%20of%20a%20large%20dataset%20of%20over%20600%2C000%0Aherbarium%20specimens.%20The%20analysis%20of%20the%20results%20points%20at%20under-investigated%0Acorrelations%20as%20well%20as%20general%20alignment%20with%20known%20trends.%20By%20sharing%20this%0Anew%20dataset%20alongside%20this%20work%2C%20we%20want%20to%20allow%20ecologists%20to%20gather%20insights%0Afor%20their%20own%20research%20questions%2C%20at%20their%20chosen%20point%20of%20accuracy/coverage%0Atrade-off.%0A&entry.1838667208=http%3A//arxiv.org/abs/2411.10074v2&entry.124074799=Read"},
{"title": "ScribeAgent: Towards Specialized Web Agents Using Production-Scale\n  Workflow Data", "author": "Junhong Shen and Atishay Jain and Zedian Xiao and Ishan Amlekar and Mouad Hadji and Aaron Podolny and Ameet Talwalkar", "abstract": "  Large Language Model (LLM) agents are rapidly improving to handle\nincreasingly complex web-based tasks. Most of these agents rely on\ngeneral-purpose, proprietary models like GPT-4 and focus on designing better\nprompts to improve their planning abilities. However, general-purpose LLMs are\nnot specifically trained to understand specialized web contexts such as HTML,\nand they often struggle with long-horizon planning. We explore an alternative\napproach that fine-tunes open-source LLMs using production-scale workflow data\ncollected from over 250 domains corresponding to 6 billion tokens. This simple\nyet effective approach shows substantial gains over prompting-based agents on\nexisting benchmarks -- ScribeAgent achieves state-of-the-art direct generation\nperformance on Mind2Web and improves the task success rate by 14.1% over the\nprevious best text-only web agents on WebArena. We further perform detailed\nablation studies on various fine-tuning design choices and provide insights\ninto LLM selection, training recipes, context window optimization, and effect\nof dataset sizes.\n", "link": "http://arxiv.org/abs/2411.15004v1", "date": "2024-11-22", "relevancy": 2.0462, "topK": [{"title": "RoboEXP: Action-Conditioned Scene Graph via Interactive Exploration for\n  Robotic Manipulation", "link": "http://arxiv.org/abs/2402.15487v1", "similarity": 0.5354}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5068}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5068}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20ScribeAgent%3A%20Towards%20Specialized%20Web%20Agents%20Using%20Production-Scale%0A%20%20Workflow%20Data&body=Title%3A%20ScribeAgent%3A%20Towards%20Specialized%20Web%20Agents%20Using%20Production-Scale%0A%20%20Workflow%20Data%0AAuthor%3A%20Junhong%20Shen%20and%20Atishay%20Jain%20and%20Zedian%20Xiao%20and%20Ishan%20Amlekar%20and%20Mouad%20Hadji%20and%20Aaron%20Podolny%20and%20Ameet%20Talwalkar%0AAbstract%3A%20%20%20Large%20Language%20Model%20%28LLM%29%20agents%20are%20rapidly%20improving%20to%20handle%0Aincreasingly%20complex%20web-based%20tasks.%20Most%20of%20these%20agents%20rely%20on%0Ageneral-purpose%2C%20proprietary%20models%20like%20GPT-4%20and%20focus%20on%20designing%20better%0Aprompts%20to%20improve%20their%20planning%20abilities.%20However%2C%20general-purpose%20LLMs%20are%0Anot%20specifically%20trained%20to%20understand%20specialized%20web%20contexts%20such%20as%20HTML%2C%0Aand%20they%20often%20struggle%20with%20long-horizon%20planning.%20We%20explore%20an%20alternative%0Aapproach%20that%20fine-tunes%20open-source%20LLMs%20using%20production-scale%20workflow%20data%0Acollected%20from%20over%20250%20domains%20corresponding%20to%206%20billion%20tokens.%20This%20simple%0Ayet%20effective%20approach%20shows%20substantial%20gains%20over%20prompting-based%20agents%20on%0Aexisting%20benchmarks%20--%20ScribeAgent%20achieves%20state-of-the-art%20direct%20generation%0Aperformance%20on%20Mind2Web%20and%20improves%20the%20task%20success%20rate%20by%2014.1%25%20over%20the%0Aprevious%20best%20text-only%20web%20agents%20on%20WebArena.%20We%20further%20perform%20detailed%0Aablation%20studies%20on%20various%20fine-tuning%20design%20choices%20and%20provide%20insights%0Ainto%20LLM%20selection%2C%20training%20recipes%2C%20context%20window%20optimization%2C%20and%20effect%0Aof%20dataset%20sizes.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2411.15004v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DScribeAgent%253A%2520Towards%2520Specialized%2520Web%2520Agents%2520Using%2520Production-Scale%250A%2520%2520Workflow%2520Data%26entry.906535625%3DJunhong%2520Shen%2520and%2520Atishay%2520Jain%2520and%2520Zedian%2520Xiao%2520and%2520Ishan%2520Amlekar%2520and%2520Mouad%2520Hadji%2520and%2520Aaron%2520Podolny%2520and%2520Ameet%2520Talwalkar%26entry.1292438233%3D%2520%2520Large%2520Language%2520Model%2520%2528LLM%2529%2520agents%2520are%2520rapidly%2520improving%2520to%2520handle%250Aincreasingly%2520complex%2520web-based%2520tasks.%2520Most%2520of%2520these%2520agents%2520rely%2520on%250Ageneral-purpose%252C%2520proprietary%2520models%2520like%2520GPT-4%2520and%2520focus%2520on%2520designing%2520better%250Aprompts%2520to%2520improve%2520their%2520planning%2520abilities.%2520However%252C%2520general-purpose%2520LLMs%2520are%250Anot%2520specifically%2520trained%2520to%2520understand%2520specialized%2520web%2520contexts%2520such%2520as%2520HTML%252C%250Aand%2520they%2520often%2520struggle%2520with%2520long-horizon%2520planning.%2520We%2520explore%2520an%2520alternative%250Aapproach%2520that%2520fine-tunes%2520open-source%2520LLMs%2520using%2520production-scale%2520workflow%2520data%250Acollected%2520from%2520over%2520250%2520domains%2520corresponding%2520to%25206%2520billion%2520tokens.%2520This%2520simple%250Ayet%2520effective%2520approach%2520shows%2520substantial%2520gains%2520over%2520prompting-based%2520agents%2520on%250Aexisting%2520benchmarks%2520--%2520ScribeAgent%2520achieves%2520state-of-the-art%2520direct%2520generation%250Aperformance%2520on%2520Mind2Web%2520and%2520improves%2520the%2520task%2520success%2520rate%2520by%252014.1%2525%2520over%2520the%250Aprevious%2520best%2520text-only%2520web%2520agents%2520on%2520WebArena.%2520We%2520further%2520perform%2520detailed%250Aablation%2520studies%2520on%2520various%2520fine-tuning%2520design%2520choices%2520and%2520provide%2520insights%250Ainto%2520LLM%2520selection%252C%2520training%2520recipes%252C%2520context%2520window%2520optimization%252C%2520and%2520effect%250Aof%2520dataset%2520sizes.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2411.15004v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=ScribeAgent%3A%20Towards%20Specialized%20Web%20Agents%20Using%20Production-Scale%0A%20%20Workflow%20Data&entry.906535625=Junhong%20Shen%20and%20Atishay%20Jain%20and%20Zedian%20Xiao%20and%20Ishan%20Amlekar%20and%20Mouad%20Hadji%20and%20Aaron%20Podolny%20and%20Ameet%20Talwalkar&entry.1292438233=%20%20Large%20Language%20Model%20%28LLM%29%20agents%20are%20rapidly%20improving%20to%20handle%0Aincreasingly%20complex%20web-based%20tasks.%20Most%20of%20these%20agents%20rely%20on%0Ageneral-purpose%2C%20proprietary%20models%20like%20GPT-4%20and%20focus%20on%20designing%20better%0Aprompts%20to%20improve%20their%20planning%20abilities.%20However%2C%20general-purpose%20LLMs%20are%0Anot%20specifically%20trained%20to%20understand%20specialized%20web%20contexts%20such%20as%20HTML%2C%0Aand%20they%20often%20struggle%20with%20long-horizon%20planning.%20We%20explore%20an%20alternative%0Aapproach%20that%20fine-tunes%20open-source%20LLMs%20using%20production-scale%20workflow%20data%0Acollected%20from%20over%20250%20domains%20corresponding%20to%206%20billion%20tokens.%20This%20simple%0Ayet%20effective%20approach%20shows%20substantial%20gains%20over%20prompting-based%20agents%20on%0Aexisting%20benchmarks%20--%20ScribeAgent%20achieves%20state-of-the-art%20direct%20generation%0Aperformance%20on%20Mind2Web%20and%20improves%20the%20task%20success%20rate%20by%2014.1%25%20over%20the%0Aprevious%20best%20text-only%20web%20agents%20on%20WebArena.%20We%20further%20perform%20detailed%0Aablation%20studies%20on%20various%20fine-tuning%20design%20choices%20and%20provide%20insights%0Ainto%20LLM%20selection%2C%20training%20recipes%2C%20context%20window%20optimization%2C%20and%20effect%0Aof%20dataset%20sizes.%0A&entry.1838667208=http%3A//arxiv.org/abs/2411.15004v1&entry.124074799=Read"},
{"title": "Single color digital H&E staining with In-and-Out Net", "author": "Mengkun Chen and Yen-Tung Liu and Fadeel Sher Khan and Matthew C. Fox and Jason S. Reichenberg and Fabiana C. P. S. Lopes and Katherine R. Sebastian and Mia K. Markey and James W. Tunnell", "abstract": "  Virtual staining streamlines traditional staining procedures by digitally\ngenerating stained images from unstained or differently stained images. While\nconventional staining methods involve time-consuming chemical processes,\nvirtual staining offers an efficient and low infrastructure alternative.\nLeveraging microscopy-based techniques, such as confocal microscopy,\nresearchers can expedite tissue analysis without the need for physical\nsectioning. However, interpreting grayscale or pseudo-color microscopic images\nremains a challenge for pathologists and surgeons accustomed to traditional\nhistologically stained images. To fill this gap, various studies explore\ndigitally simulating staining to mimic targeted histological stains. This paper\nintroduces a novel network, In-and-Out Net, specifically designed for virtual\nstaining tasks. Based on Generative Adversarial Networks (GAN), our model\nefficiently transforms Reflectance Confocal Microscopy (RCM) images into\nHematoxylin and Eosin (H&E) stained images. We enhance nuclei contrast in RCM\nimages using aluminum chloride preprocessing for skin tissues. Training the\nmodel with virtual H\\&E labels featuring two fluorescence channels eliminates\nthe need for image registration and provides pixel-level ground truth. Our\ncontributions include proposing an optimal training strategy, conducting a\ncomparative analysis demonstrating state-of-the-art performance, validating the\nmodel through an ablation study, and collecting perfectly matched input and\nground truth images without registration. In-and-Out Net showcases promising\nresults, offering a valuable tool for virtual staining tasks and advancing the\nfield of histological image analysis.\n", "link": "http://arxiv.org/abs/2405.13278v2", "date": "2024-11-22", "relevancy": 2.0388, "topK": [{"title": "VirtualModel: Generating Object-ID-retentive Human-object Interaction\n  Image by Diffusion Model for E-commerce Marketing", "link": "http://arxiv.org/abs/2405.09985v1", "similarity": 0.5151}, {"title": "Tuning-Free Noise Rectification for High Fidelity Image-to-Video\n  Generation", "link": "http://arxiv.org/abs/2403.02827v1", "similarity": 0.5061}, {"title": "SDXS: Real-Time One-Step Latent Diffusion Models with Image Conditions", "link": "http://arxiv.org/abs/2403.16627v1", "similarity": 0.5052}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Single%20color%20digital%20H%26E%20staining%20with%20In-and-Out%20Net&body=Title%3A%20Single%20color%20digital%20H%26E%20staining%20with%20In-and-Out%20Net%0AAuthor%3A%20Mengkun%20Chen%20and%20Yen-Tung%20Liu%20and%20Fadeel%20Sher%20Khan%20and%20Matthew%20C.%20Fox%20and%20Jason%20S.%20Reichenberg%20and%20Fabiana%20C.%20P.%20S.%20Lopes%20and%20Katherine%20R.%20Sebastian%20and%20Mia%20K.%20Markey%20and%20James%20W.%20Tunnell%0AAbstract%3A%20%20%20Virtual%20staining%20streamlines%20traditional%20staining%20procedures%20by%20digitally%0Agenerating%20stained%20images%20from%20unstained%20or%20differently%20stained%20images.%20While%0Aconventional%20staining%20methods%20involve%20time-consuming%20chemical%20processes%2C%0Avirtual%20staining%20offers%20an%20efficient%20and%20low%20infrastructure%20alternative.%0ALeveraging%20microscopy-based%20techniques%2C%20such%20as%20confocal%20microscopy%2C%0Aresearchers%20can%20expedite%20tissue%20analysis%20without%20the%20need%20for%20physical%0Asectioning.%20However%2C%20interpreting%20grayscale%20or%20pseudo-color%20microscopic%20images%0Aremains%20a%20challenge%20for%20pathologists%20and%20surgeons%20accustomed%20to%20traditional%0Ahistologically%20stained%20images.%20To%20fill%20this%20gap%2C%20various%20studies%20explore%0Adigitally%20simulating%20staining%20to%20mimic%20targeted%20histological%20stains.%20This%20paper%0Aintroduces%20a%20novel%20network%2C%20In-and-Out%20Net%2C%20specifically%20designed%20for%20virtual%0Astaining%20tasks.%20Based%20on%20Generative%20Adversarial%20Networks%20%28GAN%29%2C%20our%20model%0Aefficiently%20transforms%20Reflectance%20Confocal%20Microscopy%20%28RCM%29%20images%20into%0AHematoxylin%20and%20Eosin%20%28H%26E%29%20stained%20images.%20We%20enhance%20nuclei%20contrast%20in%20RCM%0Aimages%20using%20aluminum%20chloride%20preprocessing%20for%20skin%20tissues.%20Training%20the%0Amodel%20with%20virtual%20H%5C%26E%20labels%20featuring%20two%20fluorescence%20channels%20eliminates%0Athe%20need%20for%20image%20registration%20and%20provides%20pixel-level%20ground%20truth.%20Our%0Acontributions%20include%20proposing%20an%20optimal%20training%20strategy%2C%20conducting%20a%0Acomparative%20analysis%20demonstrating%20state-of-the-art%20performance%2C%20validating%20the%0Amodel%20through%20an%20ablation%20study%2C%20and%20collecting%20perfectly%20matched%20input%20and%0Aground%20truth%20images%20without%20registration.%20In-and-Out%20Net%20showcases%20promising%0Aresults%2C%20offering%20a%20valuable%20tool%20for%20virtual%20staining%20tasks%20and%20advancing%20the%0Afield%20of%20histological%20image%20analysis.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2405.13278v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DSingle%2520color%2520digital%2520H%2526E%2520staining%2520with%2520In-and-Out%2520Net%26entry.906535625%3DMengkun%2520Chen%2520and%2520Yen-Tung%2520Liu%2520and%2520Fadeel%2520Sher%2520Khan%2520and%2520Matthew%2520C.%2520Fox%2520and%2520Jason%2520S.%2520Reichenberg%2520and%2520Fabiana%2520C.%2520P.%2520S.%2520Lopes%2520and%2520Katherine%2520R.%2520Sebastian%2520and%2520Mia%2520K.%2520Markey%2520and%2520James%2520W.%2520Tunnell%26entry.1292438233%3D%2520%2520Virtual%2520staining%2520streamlines%2520traditional%2520staining%2520procedures%2520by%2520digitally%250Agenerating%2520stained%2520images%2520from%2520unstained%2520or%2520differently%2520stained%2520images.%2520While%250Aconventional%2520staining%2520methods%2520involve%2520time-consuming%2520chemical%2520processes%252C%250Avirtual%2520staining%2520offers%2520an%2520efficient%2520and%2520low%2520infrastructure%2520alternative.%250ALeveraging%2520microscopy-based%2520techniques%252C%2520such%2520as%2520confocal%2520microscopy%252C%250Aresearchers%2520can%2520expedite%2520tissue%2520analysis%2520without%2520the%2520need%2520for%2520physical%250Asectioning.%2520However%252C%2520interpreting%2520grayscale%2520or%2520pseudo-color%2520microscopic%2520images%250Aremains%2520a%2520challenge%2520for%2520pathologists%2520and%2520surgeons%2520accustomed%2520to%2520traditional%250Ahistologically%2520stained%2520images.%2520To%2520fill%2520this%2520gap%252C%2520various%2520studies%2520explore%250Adigitally%2520simulating%2520staining%2520to%2520mimic%2520targeted%2520histological%2520stains.%2520This%2520paper%250Aintroduces%2520a%2520novel%2520network%252C%2520In-and-Out%2520Net%252C%2520specifically%2520designed%2520for%2520virtual%250Astaining%2520tasks.%2520Based%2520on%2520Generative%2520Adversarial%2520Networks%2520%2528GAN%2529%252C%2520our%2520model%250Aefficiently%2520transforms%2520Reflectance%2520Confocal%2520Microscopy%2520%2528RCM%2529%2520images%2520into%250AHematoxylin%2520and%2520Eosin%2520%2528H%2526E%2529%2520stained%2520images.%2520We%2520enhance%2520nuclei%2520contrast%2520in%2520RCM%250Aimages%2520using%2520aluminum%2520chloride%2520preprocessing%2520for%2520skin%2520tissues.%2520Training%2520the%250Amodel%2520with%2520virtual%2520H%255C%2526E%2520labels%2520featuring%2520two%2520fluorescence%2520channels%2520eliminates%250Athe%2520need%2520for%2520image%2520registration%2520and%2520provides%2520pixel-level%2520ground%2520truth.%2520Our%250Acontributions%2520include%2520proposing%2520an%2520optimal%2520training%2520strategy%252C%2520conducting%2520a%250Acomparative%2520analysis%2520demonstrating%2520state-of-the-art%2520performance%252C%2520validating%2520the%250Amodel%2520through%2520an%2520ablation%2520study%252C%2520and%2520collecting%2520perfectly%2520matched%2520input%2520and%250Aground%2520truth%2520images%2520without%2520registration.%2520In-and-Out%2520Net%2520showcases%2520promising%250Aresults%252C%2520offering%2520a%2520valuable%2520tool%2520for%2520virtual%2520staining%2520tasks%2520and%2520advancing%2520the%250Afield%2520of%2520histological%2520image%2520analysis.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2405.13278v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Single%20color%20digital%20H%26E%20staining%20with%20In-and-Out%20Net&entry.906535625=Mengkun%20Chen%20and%20Yen-Tung%20Liu%20and%20Fadeel%20Sher%20Khan%20and%20Matthew%20C.%20Fox%20and%20Jason%20S.%20Reichenberg%20and%20Fabiana%20C.%20P.%20S.%20Lopes%20and%20Katherine%20R.%20Sebastian%20and%20Mia%20K.%20Markey%20and%20James%20W.%20Tunnell&entry.1292438233=%20%20Virtual%20staining%20streamlines%20traditional%20staining%20procedures%20by%20digitally%0Agenerating%20stained%20images%20from%20unstained%20or%20differently%20stained%20images.%20While%0Aconventional%20staining%20methods%20involve%20time-consuming%20chemical%20processes%2C%0Avirtual%20staining%20offers%20an%20efficient%20and%20low%20infrastructure%20alternative.%0ALeveraging%20microscopy-based%20techniques%2C%20such%20as%20confocal%20microscopy%2C%0Aresearchers%20can%20expedite%20tissue%20analysis%20without%20the%20need%20for%20physical%0Asectioning.%20However%2C%20interpreting%20grayscale%20or%20pseudo-color%20microscopic%20images%0Aremains%20a%20challenge%20for%20pathologists%20and%20surgeons%20accustomed%20to%20traditional%0Ahistologically%20stained%20images.%20To%20fill%20this%20gap%2C%20various%20studies%20explore%0Adigitally%20simulating%20staining%20to%20mimic%20targeted%20histological%20stains.%20This%20paper%0Aintroduces%20a%20novel%20network%2C%20In-and-Out%20Net%2C%20specifically%20designed%20for%20virtual%0Astaining%20tasks.%20Based%20on%20Generative%20Adversarial%20Networks%20%28GAN%29%2C%20our%20model%0Aefficiently%20transforms%20Reflectance%20Confocal%20Microscopy%20%28RCM%29%20images%20into%0AHematoxylin%20and%20Eosin%20%28H%26E%29%20stained%20images.%20We%20enhance%20nuclei%20contrast%20in%20RCM%0Aimages%20using%20aluminum%20chloride%20preprocessing%20for%20skin%20tissues.%20Training%20the%0Amodel%20with%20virtual%20H%5C%26E%20labels%20featuring%20two%20fluorescence%20channels%20eliminates%0Athe%20need%20for%20image%20registration%20and%20provides%20pixel-level%20ground%20truth.%20Our%0Acontributions%20include%20proposing%20an%20optimal%20training%20strategy%2C%20conducting%20a%0Acomparative%20analysis%20demonstrating%20state-of-the-art%20performance%2C%20validating%20the%0Amodel%20through%20an%20ablation%20study%2C%20and%20collecting%20perfectly%20matched%20input%20and%0Aground%20truth%20images%20without%20registration.%20In-and-Out%20Net%20showcases%20promising%0Aresults%2C%20offering%20a%20valuable%20tool%20for%20virtual%20staining%20tasks%20and%20advancing%20the%0Afield%20of%20histological%20image%20analysis.%0A&entry.1838667208=http%3A//arxiv.org/abs/2405.13278v2&entry.124074799=Read"},
{"title": "Design-o-meter: Towards Evaluating and Refining Graphic Designs", "author": "Sahil Goyal and Abhinav Mahajan and Swasti Mishra and Prateksha Udhayanan and Tripti Shukla and K J Joseph and Balaji Vasan Srinivasan", "abstract": "  Graphic designs are an effective medium for visual communication. They range\nfrom greeting cards to corporate flyers and beyond. Off-late, machine learning\ntechniques are able to generate such designs, which accelerates the rate of\ncontent production. An automated way of evaluating their quality becomes\ncritical. Towards this end, we introduce Design-o-meter, a data-driven\nmethodology to quantify the goodness of graphic designs. Further, our approach\ncan suggest modifications to these designs to improve its visual appeal. To the\nbest of our knowledge, Design-o-meter is the first approach that scores and\nrefines designs in a unified framework despite the inherent subjectivity and\nambiguity of the setting. Our exhaustive quantitative and qualitative analysis\nof our approach against baselines adapted for the task (including recent\nMultimodal LLM-based approaches) brings out the efficacy of our methodology. We\nhope our work will usher more interest in this important and pragmatic problem\nsetting.\n", "link": "http://arxiv.org/abs/2411.14959v1", "date": "2024-11-22", "relevancy": 2.0366, "topK": [{"title": "Multimodal-Conditioned Latent Diffusion Models for Fashion Image Editing", "link": "http://arxiv.org/abs/2403.14828v2", "similarity": 0.5304}, {"title": "Self-supervised Photographic Image Layout Representation Learning", "link": "http://arxiv.org/abs/2403.03740v1", "similarity": 0.5244}, {"title": "VirtualModel: Generating Object-ID-retentive Human-object Interaction\n  Image by Diffusion Model for E-commerce Marketing", "link": "http://arxiv.org/abs/2405.09985v1", "similarity": 0.4853}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Design-o-meter%3A%20Towards%20Evaluating%20and%20Refining%20Graphic%20Designs&body=Title%3A%20Design-o-meter%3A%20Towards%20Evaluating%20and%20Refining%20Graphic%20Designs%0AAuthor%3A%20Sahil%20Goyal%20and%20Abhinav%20Mahajan%20and%20Swasti%20Mishra%20and%20Prateksha%20Udhayanan%20and%20Tripti%20Shukla%20and%20K%20J%20Joseph%20and%20Balaji%20Vasan%20Srinivasan%0AAbstract%3A%20%20%20Graphic%20designs%20are%20an%20effective%20medium%20for%20visual%20communication.%20They%20range%0Afrom%20greeting%20cards%20to%20corporate%20flyers%20and%20beyond.%20Off-late%2C%20machine%20learning%0Atechniques%20are%20able%20to%20generate%20such%20designs%2C%20which%20accelerates%20the%20rate%20of%0Acontent%20production.%20An%20automated%20way%20of%20evaluating%20their%20quality%20becomes%0Acritical.%20Towards%20this%20end%2C%20we%20introduce%20Design-o-meter%2C%20a%20data-driven%0Amethodology%20to%20quantify%20the%20goodness%20of%20graphic%20designs.%20Further%2C%20our%20approach%0Acan%20suggest%20modifications%20to%20these%20designs%20to%20improve%20its%20visual%20appeal.%20To%20the%0Abest%20of%20our%20knowledge%2C%20Design-o-meter%20is%20the%20first%20approach%20that%20scores%20and%0Arefines%20designs%20in%20a%20unified%20framework%20despite%20the%20inherent%20subjectivity%20and%0Aambiguity%20of%20the%20setting.%20Our%20exhaustive%20quantitative%20and%20qualitative%20analysis%0Aof%20our%20approach%20against%20baselines%20adapted%20for%20the%20task%20%28including%20recent%0AMultimodal%20LLM-based%20approaches%29%20brings%20out%20the%20efficacy%20of%20our%20methodology.%20We%0Ahope%20our%20work%20will%20usher%20more%20interest%20in%20this%20important%20and%20pragmatic%20problem%0Asetting.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2411.14959v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DDesign-o-meter%253A%2520Towards%2520Evaluating%2520and%2520Refining%2520Graphic%2520Designs%26entry.906535625%3DSahil%2520Goyal%2520and%2520Abhinav%2520Mahajan%2520and%2520Swasti%2520Mishra%2520and%2520Prateksha%2520Udhayanan%2520and%2520Tripti%2520Shukla%2520and%2520K%2520J%2520Joseph%2520and%2520Balaji%2520Vasan%2520Srinivasan%26entry.1292438233%3D%2520%2520Graphic%2520designs%2520are%2520an%2520effective%2520medium%2520for%2520visual%2520communication.%2520They%2520range%250Afrom%2520greeting%2520cards%2520to%2520corporate%2520flyers%2520and%2520beyond.%2520Off-late%252C%2520machine%2520learning%250Atechniques%2520are%2520able%2520to%2520generate%2520such%2520designs%252C%2520which%2520accelerates%2520the%2520rate%2520of%250Acontent%2520production.%2520An%2520automated%2520way%2520of%2520evaluating%2520their%2520quality%2520becomes%250Acritical.%2520Towards%2520this%2520end%252C%2520we%2520introduce%2520Design-o-meter%252C%2520a%2520data-driven%250Amethodology%2520to%2520quantify%2520the%2520goodness%2520of%2520graphic%2520designs.%2520Further%252C%2520our%2520approach%250Acan%2520suggest%2520modifications%2520to%2520these%2520designs%2520to%2520improve%2520its%2520visual%2520appeal.%2520To%2520the%250Abest%2520of%2520our%2520knowledge%252C%2520Design-o-meter%2520is%2520the%2520first%2520approach%2520that%2520scores%2520and%250Arefines%2520designs%2520in%2520a%2520unified%2520framework%2520despite%2520the%2520inherent%2520subjectivity%2520and%250Aambiguity%2520of%2520the%2520setting.%2520Our%2520exhaustive%2520quantitative%2520and%2520qualitative%2520analysis%250Aof%2520our%2520approach%2520against%2520baselines%2520adapted%2520for%2520the%2520task%2520%2528including%2520recent%250AMultimodal%2520LLM-based%2520approaches%2529%2520brings%2520out%2520the%2520efficacy%2520of%2520our%2520methodology.%2520We%250Ahope%2520our%2520work%2520will%2520usher%2520more%2520interest%2520in%2520this%2520important%2520and%2520pragmatic%2520problem%250Asetting.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2411.14959v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Design-o-meter%3A%20Towards%20Evaluating%20and%20Refining%20Graphic%20Designs&entry.906535625=Sahil%20Goyal%20and%20Abhinav%20Mahajan%20and%20Swasti%20Mishra%20and%20Prateksha%20Udhayanan%20and%20Tripti%20Shukla%20and%20K%20J%20Joseph%20and%20Balaji%20Vasan%20Srinivasan&entry.1292438233=%20%20Graphic%20designs%20are%20an%20effective%20medium%20for%20visual%20communication.%20They%20range%0Afrom%20greeting%20cards%20to%20corporate%20flyers%20and%20beyond.%20Off-late%2C%20machine%20learning%0Atechniques%20are%20able%20to%20generate%20such%20designs%2C%20which%20accelerates%20the%20rate%20of%0Acontent%20production.%20An%20automated%20way%20of%20evaluating%20their%20quality%20becomes%0Acritical.%20Towards%20this%20end%2C%20we%20introduce%20Design-o-meter%2C%20a%20data-driven%0Amethodology%20to%20quantify%20the%20goodness%20of%20graphic%20designs.%20Further%2C%20our%20approach%0Acan%20suggest%20modifications%20to%20these%20designs%20to%20improve%20its%20visual%20appeal.%20To%20the%0Abest%20of%20our%20knowledge%2C%20Design-o-meter%20is%20the%20first%20approach%20that%20scores%20and%0Arefines%20designs%20in%20a%20unified%20framework%20despite%20the%20inherent%20subjectivity%20and%0Aambiguity%20of%20the%20setting.%20Our%20exhaustive%20quantitative%20and%20qualitative%20analysis%0Aof%20our%20approach%20against%20baselines%20adapted%20for%20the%20task%20%28including%20recent%0AMultimodal%20LLM-based%20approaches%29%20brings%20out%20the%20efficacy%20of%20our%20methodology.%20We%0Ahope%20our%20work%20will%20usher%20more%20interest%20in%20this%20important%20and%20pragmatic%20problem%0Asetting.%0A&entry.1838667208=http%3A//arxiv.org/abs/2411.14959v1&entry.124074799=Read"},
{"title": "Differentiable Biomechanics for Markerless Motion Capture in Upper Limb\n  Stroke Rehabilitation: A Comparison with Optical Motion Capture", "author": "Tim Unger and Arash Sal Moslehian and J. D. Peiffer and Johann Ullrich and Roger Gassert and Olivier Lambercy and R. James Cotton and Chris Awai Easthope", "abstract": "  Marker-based Optical Motion Capture (OMC) paired with biomechanical modeling\nis currently considered the most precise and accurate method for measuring\nhuman movement kinematics. However, combining differentiable biomechanical\nmodeling with Markerless Motion Capture (MMC) offers a promising approach to\nmotion capture in clinical settings, requiring only minimal equipment, such as\nsynchronized webcams, and minimal effort for data collection. This study\ncompares key kinematic outcomes from biomechanically modeled MMC and OMC data\nin 15 stroke patients performing the drinking task, a functional task\nrecommended for assessing upper limb movement quality. We observed a high level\nof agreement in kinematic trajectories between MMC and OMC, as indicated by\nhigh correlations (median r above 0.95 for the majority of kinematic\ntrajectories) and median RMSE values ranging from 2-5 degrees for joint angles,\n0.04 m/s for end-effector velocity, and 6 mm for trunk displacement.\nTrial-to-trial biases between OMC and MMC were consistent within participant\nsessions, with interquartile ranges of bias around 1-3 degrees for joint\nangles, 0.01 m/s in end-effector velocity, and approximately 3mm for trunk\ndisplacement. Our findings indicate that our MMC for arm tracking is\napproaching the accuracy of marker-based methods, supporting its potential for\nuse in clinical settings. MMC could provide valuable insights into movement\nrehabilitation in stroke patients, potentially enhancing the effectiveness of\nrehabilitation strategies.\n", "link": "http://arxiv.org/abs/2411.14992v1", "date": "2024-11-22", "relevancy": 2.0317, "topK": [{"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.5334}, {"title": "Egocentric Computer Vision for Hands-Free Robotic Wheelchair Navigation", "link": "https://link.springer.com/article/10.1007/s10846-023-01807-4", "similarity": 0.5239}, {"title": "Dynamic Gaussians Mesh: Consistent Mesh Reconstruction from Monocular\n  Videos", "link": "http://arxiv.org/abs/2404.12379v2", "similarity": 0.4818}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Differentiable%20Biomechanics%20for%20Markerless%20Motion%20Capture%20in%20Upper%20Limb%0A%20%20Stroke%20Rehabilitation%3A%20A%20Comparison%20with%20Optical%20Motion%20Capture&body=Title%3A%20Differentiable%20Biomechanics%20for%20Markerless%20Motion%20Capture%20in%20Upper%20Limb%0A%20%20Stroke%20Rehabilitation%3A%20A%20Comparison%20with%20Optical%20Motion%20Capture%0AAuthor%3A%20Tim%20Unger%20and%20Arash%20Sal%20Moslehian%20and%20J.%20D.%20Peiffer%20and%20Johann%20Ullrich%20and%20Roger%20Gassert%20and%20Olivier%20Lambercy%20and%20R.%20James%20Cotton%20and%20Chris%20Awai%20Easthope%0AAbstract%3A%20%20%20Marker-based%20Optical%20Motion%20Capture%20%28OMC%29%20paired%20with%20biomechanical%20modeling%0Ais%20currently%20considered%20the%20most%20precise%20and%20accurate%20method%20for%20measuring%0Ahuman%20movement%20kinematics.%20However%2C%20combining%20differentiable%20biomechanical%0Amodeling%20with%20Markerless%20Motion%20Capture%20%28MMC%29%20offers%20a%20promising%20approach%20to%0Amotion%20capture%20in%20clinical%20settings%2C%20requiring%20only%20minimal%20equipment%2C%20such%20as%0Asynchronized%20webcams%2C%20and%20minimal%20effort%20for%20data%20collection.%20This%20study%0Acompares%20key%20kinematic%20outcomes%20from%20biomechanically%20modeled%20MMC%20and%20OMC%20data%0Ain%2015%20stroke%20patients%20performing%20the%20drinking%20task%2C%20a%20functional%20task%0Arecommended%20for%20assessing%20upper%20limb%20movement%20quality.%20We%20observed%20a%20high%20level%0Aof%20agreement%20in%20kinematic%20trajectories%20between%20MMC%20and%20OMC%2C%20as%20indicated%20by%0Ahigh%20correlations%20%28median%20r%20above%200.95%20for%20the%20majority%20of%20kinematic%0Atrajectories%29%20and%20median%20RMSE%20values%20ranging%20from%202-5%20degrees%20for%20joint%20angles%2C%0A0.04%20m/s%20for%20end-effector%20velocity%2C%20and%206%20mm%20for%20trunk%20displacement.%0ATrial-to-trial%20biases%20between%20OMC%20and%20MMC%20were%20consistent%20within%20participant%0Asessions%2C%20with%20interquartile%20ranges%20of%20bias%20around%201-3%20degrees%20for%20joint%0Aangles%2C%200.01%20m/s%20in%20end-effector%20velocity%2C%20and%20approximately%203mm%20for%20trunk%0Adisplacement.%20Our%20findings%20indicate%20that%20our%20MMC%20for%20arm%20tracking%20is%0Aapproaching%20the%20accuracy%20of%20marker-based%20methods%2C%20supporting%20its%20potential%20for%0Ause%20in%20clinical%20settings.%20MMC%20could%20provide%20valuable%20insights%20into%20movement%0Arehabilitation%20in%20stroke%20patients%2C%20potentially%20enhancing%20the%20effectiveness%20of%0Arehabilitation%20strategies.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2411.14992v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DDifferentiable%2520Biomechanics%2520for%2520Markerless%2520Motion%2520Capture%2520in%2520Upper%2520Limb%250A%2520%2520Stroke%2520Rehabilitation%253A%2520A%2520Comparison%2520with%2520Optical%2520Motion%2520Capture%26entry.906535625%3DTim%2520Unger%2520and%2520Arash%2520Sal%2520Moslehian%2520and%2520J.%2520D.%2520Peiffer%2520and%2520Johann%2520Ullrich%2520and%2520Roger%2520Gassert%2520and%2520Olivier%2520Lambercy%2520and%2520R.%2520James%2520Cotton%2520and%2520Chris%2520Awai%2520Easthope%26entry.1292438233%3D%2520%2520Marker-based%2520Optical%2520Motion%2520Capture%2520%2528OMC%2529%2520paired%2520with%2520biomechanical%2520modeling%250Ais%2520currently%2520considered%2520the%2520most%2520precise%2520and%2520accurate%2520method%2520for%2520measuring%250Ahuman%2520movement%2520kinematics.%2520However%252C%2520combining%2520differentiable%2520biomechanical%250Amodeling%2520with%2520Markerless%2520Motion%2520Capture%2520%2528MMC%2529%2520offers%2520a%2520promising%2520approach%2520to%250Amotion%2520capture%2520in%2520clinical%2520settings%252C%2520requiring%2520only%2520minimal%2520equipment%252C%2520such%2520as%250Asynchronized%2520webcams%252C%2520and%2520minimal%2520effort%2520for%2520data%2520collection.%2520This%2520study%250Acompares%2520key%2520kinematic%2520outcomes%2520from%2520biomechanically%2520modeled%2520MMC%2520and%2520OMC%2520data%250Ain%252015%2520stroke%2520patients%2520performing%2520the%2520drinking%2520task%252C%2520a%2520functional%2520task%250Arecommended%2520for%2520assessing%2520upper%2520limb%2520movement%2520quality.%2520We%2520observed%2520a%2520high%2520level%250Aof%2520agreement%2520in%2520kinematic%2520trajectories%2520between%2520MMC%2520and%2520OMC%252C%2520as%2520indicated%2520by%250Ahigh%2520correlations%2520%2528median%2520r%2520above%25200.95%2520for%2520the%2520majority%2520of%2520kinematic%250Atrajectories%2529%2520and%2520median%2520RMSE%2520values%2520ranging%2520from%25202-5%2520degrees%2520for%2520joint%2520angles%252C%250A0.04%2520m/s%2520for%2520end-effector%2520velocity%252C%2520and%25206%2520mm%2520for%2520trunk%2520displacement.%250ATrial-to-trial%2520biases%2520between%2520OMC%2520and%2520MMC%2520were%2520consistent%2520within%2520participant%250Asessions%252C%2520with%2520interquartile%2520ranges%2520of%2520bias%2520around%25201-3%2520degrees%2520for%2520joint%250Aangles%252C%25200.01%2520m/s%2520in%2520end-effector%2520velocity%252C%2520and%2520approximately%25203mm%2520for%2520trunk%250Adisplacement.%2520Our%2520findings%2520indicate%2520that%2520our%2520MMC%2520for%2520arm%2520tracking%2520is%250Aapproaching%2520the%2520accuracy%2520of%2520marker-based%2520methods%252C%2520supporting%2520its%2520potential%2520for%250Ause%2520in%2520clinical%2520settings.%2520MMC%2520could%2520provide%2520valuable%2520insights%2520into%2520movement%250Arehabilitation%2520in%2520stroke%2520patients%252C%2520potentially%2520enhancing%2520the%2520effectiveness%2520of%250Arehabilitation%2520strategies.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2411.14992v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Differentiable%20Biomechanics%20for%20Markerless%20Motion%20Capture%20in%20Upper%20Limb%0A%20%20Stroke%20Rehabilitation%3A%20A%20Comparison%20with%20Optical%20Motion%20Capture&entry.906535625=Tim%20Unger%20and%20Arash%20Sal%20Moslehian%20and%20J.%20D.%20Peiffer%20and%20Johann%20Ullrich%20and%20Roger%20Gassert%20and%20Olivier%20Lambercy%20and%20R.%20James%20Cotton%20and%20Chris%20Awai%20Easthope&entry.1292438233=%20%20Marker-based%20Optical%20Motion%20Capture%20%28OMC%29%20paired%20with%20biomechanical%20modeling%0Ais%20currently%20considered%20the%20most%20precise%20and%20accurate%20method%20for%20measuring%0Ahuman%20movement%20kinematics.%20However%2C%20combining%20differentiable%20biomechanical%0Amodeling%20with%20Markerless%20Motion%20Capture%20%28MMC%29%20offers%20a%20promising%20approach%20to%0Amotion%20capture%20in%20clinical%20settings%2C%20requiring%20only%20minimal%20equipment%2C%20such%20as%0Asynchronized%20webcams%2C%20and%20minimal%20effort%20for%20data%20collection.%20This%20study%0Acompares%20key%20kinematic%20outcomes%20from%20biomechanically%20modeled%20MMC%20and%20OMC%20data%0Ain%2015%20stroke%20patients%20performing%20the%20drinking%20task%2C%20a%20functional%20task%0Arecommended%20for%20assessing%20upper%20limb%20movement%20quality.%20We%20observed%20a%20high%20level%0Aof%20agreement%20in%20kinematic%20trajectories%20between%20MMC%20and%20OMC%2C%20as%20indicated%20by%0Ahigh%20correlations%20%28median%20r%20above%200.95%20for%20the%20majority%20of%20kinematic%0Atrajectories%29%20and%20median%20RMSE%20values%20ranging%20from%202-5%20degrees%20for%20joint%20angles%2C%0A0.04%20m/s%20for%20end-effector%20velocity%2C%20and%206%20mm%20for%20trunk%20displacement.%0ATrial-to-trial%20biases%20between%20OMC%20and%20MMC%20were%20consistent%20within%20participant%0Asessions%2C%20with%20interquartile%20ranges%20of%20bias%20around%201-3%20degrees%20for%20joint%0Aangles%2C%200.01%20m/s%20in%20end-effector%20velocity%2C%20and%20approximately%203mm%20for%20trunk%0Adisplacement.%20Our%20findings%20indicate%20that%20our%20MMC%20for%20arm%20tracking%20is%0Aapproaching%20the%20accuracy%20of%20marker-based%20methods%2C%20supporting%20its%20potential%20for%0Ause%20in%20clinical%20settings.%20MMC%20could%20provide%20valuable%20insights%20into%20movement%0Arehabilitation%20in%20stroke%20patients%2C%20potentially%20enhancing%20the%20effectiveness%20of%0Arehabilitation%20strategies.%0A&entry.1838667208=http%3A//arxiv.org/abs/2411.14992v1&entry.124074799=Read"},
{"title": "Instance-Aware Generalized Referring Expression Segmentation", "author": "E-Ro Nguyen and Hieu Le and Dimitris Samaras and Michael Ryoo", "abstract": "  Recent works on Generalized Referring Expression Segmentation (GRES) struggle\nwith handling complex expressions referring to multiple distinct objects. This\nis because these methods typically employ an end-to-end foreground-background\nsegmentation and lack a mechanism to explicitly differentiate and associate\ndifferent object instances to the text query. To this end, we propose\nInstAlign, a method that incorporates object-level reasoning into the\nsegmentation process. Our model leverages both text and image inputs to extract\na set of object-level tokens that capture both the semantic information in the\ninput prompt and the objects within the image. By modeling the text-object\nalignment via instance-level supervision, each token uniquely represents an\nobject segment in the image, while also aligning with relevant semantic\ninformation from the text. Extensive experiments on the gRefCOCO and Ref-ZOM\nbenchmarks demonstrate that our method significantly advances state-of-the-art\nperformance, setting a new standard for precise and flexible GRES.\n", "link": "http://arxiv.org/abs/2411.15087v1", "date": "2024-11-22", "relevancy": 2.028, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5083}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5083}, {"title": "Thinking Outside the BBox: Unconstrained Generative Object Compositing", "link": "http://arxiv.org/abs/2409.04559v2", "similarity": 0.5005}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Instance-Aware%20Generalized%20Referring%20Expression%20Segmentation&body=Title%3A%20Instance-Aware%20Generalized%20Referring%20Expression%20Segmentation%0AAuthor%3A%20E-Ro%20Nguyen%20and%20Hieu%20Le%20and%20Dimitris%20Samaras%20and%20Michael%20Ryoo%0AAbstract%3A%20%20%20Recent%20works%20on%20Generalized%20Referring%20Expression%20Segmentation%20%28GRES%29%20struggle%0Awith%20handling%20complex%20expressions%20referring%20to%20multiple%20distinct%20objects.%20This%0Ais%20because%20these%20methods%20typically%20employ%20an%20end-to-end%20foreground-background%0Asegmentation%20and%20lack%20a%20mechanism%20to%20explicitly%20differentiate%20and%20associate%0Adifferent%20object%20instances%20to%20the%20text%20query.%20To%20this%20end%2C%20we%20propose%0AInstAlign%2C%20a%20method%20that%20incorporates%20object-level%20reasoning%20into%20the%0Asegmentation%20process.%20Our%20model%20leverages%20both%20text%20and%20image%20inputs%20to%20extract%0Aa%20set%20of%20object-level%20tokens%20that%20capture%20both%20the%20semantic%20information%20in%20the%0Ainput%20prompt%20and%20the%20objects%20within%20the%20image.%20By%20modeling%20the%20text-object%0Aalignment%20via%20instance-level%20supervision%2C%20each%20token%20uniquely%20represents%20an%0Aobject%20segment%20in%20the%20image%2C%20while%20also%20aligning%20with%20relevant%20semantic%0Ainformation%20from%20the%20text.%20Extensive%20experiments%20on%20the%20gRefCOCO%20and%20Ref-ZOM%0Abenchmarks%20demonstrate%20that%20our%20method%20significantly%20advances%20state-of-the-art%0Aperformance%2C%20setting%20a%20new%20standard%20for%20precise%20and%20flexible%20GRES.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2411.15087v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DInstance-Aware%2520Generalized%2520Referring%2520Expression%2520Segmentation%26entry.906535625%3DE-Ro%2520Nguyen%2520and%2520Hieu%2520Le%2520and%2520Dimitris%2520Samaras%2520and%2520Michael%2520Ryoo%26entry.1292438233%3D%2520%2520Recent%2520works%2520on%2520Generalized%2520Referring%2520Expression%2520Segmentation%2520%2528GRES%2529%2520struggle%250Awith%2520handling%2520complex%2520expressions%2520referring%2520to%2520multiple%2520distinct%2520objects.%2520This%250Ais%2520because%2520these%2520methods%2520typically%2520employ%2520an%2520end-to-end%2520foreground-background%250Asegmentation%2520and%2520lack%2520a%2520mechanism%2520to%2520explicitly%2520differentiate%2520and%2520associate%250Adifferent%2520object%2520instances%2520to%2520the%2520text%2520query.%2520To%2520this%2520end%252C%2520we%2520propose%250AInstAlign%252C%2520a%2520method%2520that%2520incorporates%2520object-level%2520reasoning%2520into%2520the%250Asegmentation%2520process.%2520Our%2520model%2520leverages%2520both%2520text%2520and%2520image%2520inputs%2520to%2520extract%250Aa%2520set%2520of%2520object-level%2520tokens%2520that%2520capture%2520both%2520the%2520semantic%2520information%2520in%2520the%250Ainput%2520prompt%2520and%2520the%2520objects%2520within%2520the%2520image.%2520By%2520modeling%2520the%2520text-object%250Aalignment%2520via%2520instance-level%2520supervision%252C%2520each%2520token%2520uniquely%2520represents%2520an%250Aobject%2520segment%2520in%2520the%2520image%252C%2520while%2520also%2520aligning%2520with%2520relevant%2520semantic%250Ainformation%2520from%2520the%2520text.%2520Extensive%2520experiments%2520on%2520the%2520gRefCOCO%2520and%2520Ref-ZOM%250Abenchmarks%2520demonstrate%2520that%2520our%2520method%2520significantly%2520advances%2520state-of-the-art%250Aperformance%252C%2520setting%2520a%2520new%2520standard%2520for%2520precise%2520and%2520flexible%2520GRES.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2411.15087v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Instance-Aware%20Generalized%20Referring%20Expression%20Segmentation&entry.906535625=E-Ro%20Nguyen%20and%20Hieu%20Le%20and%20Dimitris%20Samaras%20and%20Michael%20Ryoo&entry.1292438233=%20%20Recent%20works%20on%20Generalized%20Referring%20Expression%20Segmentation%20%28GRES%29%20struggle%0Awith%20handling%20complex%20expressions%20referring%20to%20multiple%20distinct%20objects.%20This%0Ais%20because%20these%20methods%20typically%20employ%20an%20end-to-end%20foreground-background%0Asegmentation%20and%20lack%20a%20mechanism%20to%20explicitly%20differentiate%20and%20associate%0Adifferent%20object%20instances%20to%20the%20text%20query.%20To%20this%20end%2C%20we%20propose%0AInstAlign%2C%20a%20method%20that%20incorporates%20object-level%20reasoning%20into%20the%0Asegmentation%20process.%20Our%20model%20leverages%20both%20text%20and%20image%20inputs%20to%20extract%0Aa%20set%20of%20object-level%20tokens%20that%20capture%20both%20the%20semantic%20information%20in%20the%0Ainput%20prompt%20and%20the%20objects%20within%20the%20image.%20By%20modeling%20the%20text-object%0Aalignment%20via%20instance-level%20supervision%2C%20each%20token%20uniquely%20represents%20an%0Aobject%20segment%20in%20the%20image%2C%20while%20also%20aligning%20with%20relevant%20semantic%0Ainformation%20from%20the%20text.%20Extensive%20experiments%20on%20the%20gRefCOCO%20and%20Ref-ZOM%0Abenchmarks%20demonstrate%20that%20our%20method%20significantly%20advances%20state-of-the-art%0Aperformance%2C%20setting%20a%20new%20standard%20for%20precise%20and%20flexible%20GRES.%0A&entry.1838667208=http%3A//arxiv.org/abs/2411.15087v1&entry.124074799=Read"},
{"title": "Leveraging Hallucinations to Reduce Manual Prompt Dependency in\n  Promptable Segmentation", "author": "Jian Hu and Jiayi Lin and Junchi Yan and Shaogang Gong", "abstract": "  Promptable segmentation typically requires instance-specific manual prompts\nto guide the segmentation of each desired object. To minimize such a need,\ntask-generic promptable segmentation has been introduced, which employs a\nsingle task-generic prompt to segment various images of different objects in\nthe same task. Current methods use Multimodal Large Language Models (MLLMs) to\nreason detailed instance-specific prompts from a task-generic prompt for\nimproving segmentation accuracy. The effectiveness of this segmentation heavily\ndepends on the precision of these derived prompts. However, MLLMs often suffer\nhallucinations during reasoning, resulting in inaccurate prompting. While\nexisting methods focus on eliminating hallucinations to improve a model, we\nargue that MLLM hallucinations can reveal valuable contextual insights when\nleveraged correctly, as they represent pre-trained large-scale knowledge beyond\nindividual images. In this paper, we utilize hallucinations to mine\ntask-related information from images and verify its accuracy for enhancing\nprecision of the generated prompts. Specifically, we introduce an iterative\nPrompt-Mask Cycle generation framework (ProMaC) with a prompt generator and a\nmask generator.The prompt generator uses a multi-scale chain of thought\nprompting, initially exploring hallucinations for extracting extended\ncontextual knowledge on a test image.These hallucinations are then reduced to\nformulate precise instance-specific prompts, directing the mask generator to\nproduce masks that are consistent with task semantics by mask semantic\nalignment. The generated masks iteratively induce the prompt generator to focus\nmore on task-relevant image areas and reduce irrelevant hallucinations,\nresulting jointly in better prompts and masks. Experiments on 5 benchmarks\ndemonstrate the effectiveness of ProMaC. Code given in\nhttps://lwpyh.github.io/ProMaC/.\n", "link": "http://arxiv.org/abs/2408.15205v3", "date": "2024-11-22", "relevancy": 2.0225, "topK": [{"title": "Thinking Outside the BBox: Unconstrained Generative Object Compositing", "link": "http://arxiv.org/abs/2409.04559v2", "similarity": 0.5288}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.501}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.501}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Leveraging%20Hallucinations%20to%20Reduce%20Manual%20Prompt%20Dependency%20in%0A%20%20Promptable%20Segmentation&body=Title%3A%20Leveraging%20Hallucinations%20to%20Reduce%20Manual%20Prompt%20Dependency%20in%0A%20%20Promptable%20Segmentation%0AAuthor%3A%20Jian%20Hu%20and%20Jiayi%20Lin%20and%20Junchi%20Yan%20and%20Shaogang%20Gong%0AAbstract%3A%20%20%20Promptable%20segmentation%20typically%20requires%20instance-specific%20manual%20prompts%0Ato%20guide%20the%20segmentation%20of%20each%20desired%20object.%20To%20minimize%20such%20a%20need%2C%0Atask-generic%20promptable%20segmentation%20has%20been%20introduced%2C%20which%20employs%20a%0Asingle%20task-generic%20prompt%20to%20segment%20various%20images%20of%20different%20objects%20in%0Athe%20same%20task.%20Current%20methods%20use%20Multimodal%20Large%20Language%20Models%20%28MLLMs%29%20to%0Areason%20detailed%20instance-specific%20prompts%20from%20a%20task-generic%20prompt%20for%0Aimproving%20segmentation%20accuracy.%20The%20effectiveness%20of%20this%20segmentation%20heavily%0Adepends%20on%20the%20precision%20of%20these%20derived%20prompts.%20However%2C%20MLLMs%20often%20suffer%0Ahallucinations%20during%20reasoning%2C%20resulting%20in%20inaccurate%20prompting.%20While%0Aexisting%20methods%20focus%20on%20eliminating%20hallucinations%20to%20improve%20a%20model%2C%20we%0Aargue%20that%20MLLM%20hallucinations%20can%20reveal%20valuable%20contextual%20insights%20when%0Aleveraged%20correctly%2C%20as%20they%20represent%20pre-trained%20large-scale%20knowledge%20beyond%0Aindividual%20images.%20In%20this%20paper%2C%20we%20utilize%20hallucinations%20to%20mine%0Atask-related%20information%20from%20images%20and%20verify%20its%20accuracy%20for%20enhancing%0Aprecision%20of%20the%20generated%20prompts.%20Specifically%2C%20we%20introduce%20an%20iterative%0APrompt-Mask%20Cycle%20generation%20framework%20%28ProMaC%29%20with%20a%20prompt%20generator%20and%20a%0Amask%20generator.The%20prompt%20generator%20uses%20a%20multi-scale%20chain%20of%20thought%0Aprompting%2C%20initially%20exploring%20hallucinations%20for%20extracting%20extended%0Acontextual%20knowledge%20on%20a%20test%20image.These%20hallucinations%20are%20then%20reduced%20to%0Aformulate%20precise%20instance-specific%20prompts%2C%20directing%20the%20mask%20generator%20to%0Aproduce%20masks%20that%20are%20consistent%20with%20task%20semantics%20by%20mask%20semantic%0Aalignment.%20The%20generated%20masks%20iteratively%20induce%20the%20prompt%20generator%20to%20focus%0Amore%20on%20task-relevant%20image%20areas%20and%20reduce%20irrelevant%20hallucinations%2C%0Aresulting%20jointly%20in%20better%20prompts%20and%20masks.%20Experiments%20on%205%20benchmarks%0Ademonstrate%20the%20effectiveness%20of%20ProMaC.%20Code%20given%20in%0Ahttps%3A//lwpyh.github.io/ProMaC/.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2408.15205v3%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DLeveraging%2520Hallucinations%2520to%2520Reduce%2520Manual%2520Prompt%2520Dependency%2520in%250A%2520%2520Promptable%2520Segmentation%26entry.906535625%3DJian%2520Hu%2520and%2520Jiayi%2520Lin%2520and%2520Junchi%2520Yan%2520and%2520Shaogang%2520Gong%26entry.1292438233%3D%2520%2520Promptable%2520segmentation%2520typically%2520requires%2520instance-specific%2520manual%2520prompts%250Ato%2520guide%2520the%2520segmentation%2520of%2520each%2520desired%2520object.%2520To%2520minimize%2520such%2520a%2520need%252C%250Atask-generic%2520promptable%2520segmentation%2520has%2520been%2520introduced%252C%2520which%2520employs%2520a%250Asingle%2520task-generic%2520prompt%2520to%2520segment%2520various%2520images%2520of%2520different%2520objects%2520in%250Athe%2520same%2520task.%2520Current%2520methods%2520use%2520Multimodal%2520Large%2520Language%2520Models%2520%2528MLLMs%2529%2520to%250Areason%2520detailed%2520instance-specific%2520prompts%2520from%2520a%2520task-generic%2520prompt%2520for%250Aimproving%2520segmentation%2520accuracy.%2520The%2520effectiveness%2520of%2520this%2520segmentation%2520heavily%250Adepends%2520on%2520the%2520precision%2520of%2520these%2520derived%2520prompts.%2520However%252C%2520MLLMs%2520often%2520suffer%250Ahallucinations%2520during%2520reasoning%252C%2520resulting%2520in%2520inaccurate%2520prompting.%2520While%250Aexisting%2520methods%2520focus%2520on%2520eliminating%2520hallucinations%2520to%2520improve%2520a%2520model%252C%2520we%250Aargue%2520that%2520MLLM%2520hallucinations%2520can%2520reveal%2520valuable%2520contextual%2520insights%2520when%250Aleveraged%2520correctly%252C%2520as%2520they%2520represent%2520pre-trained%2520large-scale%2520knowledge%2520beyond%250Aindividual%2520images.%2520In%2520this%2520paper%252C%2520we%2520utilize%2520hallucinations%2520to%2520mine%250Atask-related%2520information%2520from%2520images%2520and%2520verify%2520its%2520accuracy%2520for%2520enhancing%250Aprecision%2520of%2520the%2520generated%2520prompts.%2520Specifically%252C%2520we%2520introduce%2520an%2520iterative%250APrompt-Mask%2520Cycle%2520generation%2520framework%2520%2528ProMaC%2529%2520with%2520a%2520prompt%2520generator%2520and%2520a%250Amask%2520generator.The%2520prompt%2520generator%2520uses%2520a%2520multi-scale%2520chain%2520of%2520thought%250Aprompting%252C%2520initially%2520exploring%2520hallucinations%2520for%2520extracting%2520extended%250Acontextual%2520knowledge%2520on%2520a%2520test%2520image.These%2520hallucinations%2520are%2520then%2520reduced%2520to%250Aformulate%2520precise%2520instance-specific%2520prompts%252C%2520directing%2520the%2520mask%2520generator%2520to%250Aproduce%2520masks%2520that%2520are%2520consistent%2520with%2520task%2520semantics%2520by%2520mask%2520semantic%250Aalignment.%2520The%2520generated%2520masks%2520iteratively%2520induce%2520the%2520prompt%2520generator%2520to%2520focus%250Amore%2520on%2520task-relevant%2520image%2520areas%2520and%2520reduce%2520irrelevant%2520hallucinations%252C%250Aresulting%2520jointly%2520in%2520better%2520prompts%2520and%2520masks.%2520Experiments%2520on%25205%2520benchmarks%250Ademonstrate%2520the%2520effectiveness%2520of%2520ProMaC.%2520Code%2520given%2520in%250Ahttps%253A//lwpyh.github.io/ProMaC/.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2408.15205v3%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Leveraging%20Hallucinations%20to%20Reduce%20Manual%20Prompt%20Dependency%20in%0A%20%20Promptable%20Segmentation&entry.906535625=Jian%20Hu%20and%20Jiayi%20Lin%20and%20Junchi%20Yan%20and%20Shaogang%20Gong&entry.1292438233=%20%20Promptable%20segmentation%20typically%20requires%20instance-specific%20manual%20prompts%0Ato%20guide%20the%20segmentation%20of%20each%20desired%20object.%20To%20minimize%20such%20a%20need%2C%0Atask-generic%20promptable%20segmentation%20has%20been%20introduced%2C%20which%20employs%20a%0Asingle%20task-generic%20prompt%20to%20segment%20various%20images%20of%20different%20objects%20in%0Athe%20same%20task.%20Current%20methods%20use%20Multimodal%20Large%20Language%20Models%20%28MLLMs%29%20to%0Areason%20detailed%20instance-specific%20prompts%20from%20a%20task-generic%20prompt%20for%0Aimproving%20segmentation%20accuracy.%20The%20effectiveness%20of%20this%20segmentation%20heavily%0Adepends%20on%20the%20precision%20of%20these%20derived%20prompts.%20However%2C%20MLLMs%20often%20suffer%0Ahallucinations%20during%20reasoning%2C%20resulting%20in%20inaccurate%20prompting.%20While%0Aexisting%20methods%20focus%20on%20eliminating%20hallucinations%20to%20improve%20a%20model%2C%20we%0Aargue%20that%20MLLM%20hallucinations%20can%20reveal%20valuable%20contextual%20insights%20when%0Aleveraged%20correctly%2C%20as%20they%20represent%20pre-trained%20large-scale%20knowledge%20beyond%0Aindividual%20images.%20In%20this%20paper%2C%20we%20utilize%20hallucinations%20to%20mine%0Atask-related%20information%20from%20images%20and%20verify%20its%20accuracy%20for%20enhancing%0Aprecision%20of%20the%20generated%20prompts.%20Specifically%2C%20we%20introduce%20an%20iterative%0APrompt-Mask%20Cycle%20generation%20framework%20%28ProMaC%29%20with%20a%20prompt%20generator%20and%20a%0Amask%20generator.The%20prompt%20generator%20uses%20a%20multi-scale%20chain%20of%20thought%0Aprompting%2C%20initially%20exploring%20hallucinations%20for%20extracting%20extended%0Acontextual%20knowledge%20on%20a%20test%20image.These%20hallucinations%20are%20then%20reduced%20to%0Aformulate%20precise%20instance-specific%20prompts%2C%20directing%20the%20mask%20generator%20to%0Aproduce%20masks%20that%20are%20consistent%20with%20task%20semantics%20by%20mask%20semantic%0Aalignment.%20The%20generated%20masks%20iteratively%20induce%20the%20prompt%20generator%20to%20focus%0Amore%20on%20task-relevant%20image%20areas%20and%20reduce%20irrelevant%20hallucinations%2C%0Aresulting%20jointly%20in%20better%20prompts%20and%20masks.%20Experiments%20on%205%20benchmarks%0Ademonstrate%20the%20effectiveness%20of%20ProMaC.%20Code%20given%20in%0Ahttps%3A//lwpyh.github.io/ProMaC/.%0A&entry.1838667208=http%3A//arxiv.org/abs/2408.15205v3&entry.124074799=Read"},
{"title": "Gradient Masking All-at-Once: Ensemble Everything Everywhere Is Not\n  Robust", "author": "Jie Zhang and Kristina Nikoli\u0107 and Nicholas Carlini and Florian Tram\u00e8r", "abstract": "  Ensemble everything everywhere is a defense to adversarial examples that was\nrecently proposed to make image classifiers robust. This defense works by\nensembling a model's intermediate representations at multiple noisy image\nresolutions, producing a single robust classification. This defense was shown\nto be effective against multiple state-of-the-art attacks. Perhaps even more\nconvincingly, it was shown that the model's gradients are perceptually aligned:\nattacks against the model produce noise that perceptually resembles the\ntargeted class.\n  In this short note, we show that this defense is not robust to adversarial\nattack. We first show that the defense's randomness and ensembling method cause\nsevere gradient masking. We then use standard adaptive attack techniques to\nreduce the defense's robust accuracy from 48% to 1% on CIFAR-100 and from 62%\nto 4% on CIFAR-10, under the $\\ell_\\infty$-norm threat model with\n$\\varepsilon=8/255$.\n", "link": "http://arxiv.org/abs/2411.14834v1", "date": "2024-11-22", "relevancy": 2.011, "topK": [{"title": "Thinking Outside the BBox: Unconstrained Generative Object Compositing", "link": "http://arxiv.org/abs/2409.04559v2", "similarity": 0.5184}, {"title": "Boosted dynamic neural networks", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/26302", "similarity": 0.5045}, {"title": "ToDo: Token Downsampling for Efficient Generation of High-Resolution\n  Images", "link": "http://arxiv.org/abs/2402.13573v2", "similarity": 0.4948}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Gradient%20Masking%20All-at-Once%3A%20Ensemble%20Everything%20Everywhere%20Is%20Not%0A%20%20Robust&body=Title%3A%20Gradient%20Masking%20All-at-Once%3A%20Ensemble%20Everything%20Everywhere%20Is%20Not%0A%20%20Robust%0AAuthor%3A%20Jie%20Zhang%20and%20Kristina%20Nikoli%C4%87%20and%20Nicholas%20Carlini%20and%20Florian%20Tram%C3%A8r%0AAbstract%3A%20%20%20Ensemble%20everything%20everywhere%20is%20a%20defense%20to%20adversarial%20examples%20that%20was%0Arecently%20proposed%20to%20make%20image%20classifiers%20robust.%20This%20defense%20works%20by%0Aensembling%20a%20model%27s%20intermediate%20representations%20at%20multiple%20noisy%20image%0Aresolutions%2C%20producing%20a%20single%20robust%20classification.%20This%20defense%20was%20shown%0Ato%20be%20effective%20against%20multiple%20state-of-the-art%20attacks.%20Perhaps%20even%20more%0Aconvincingly%2C%20it%20was%20shown%20that%20the%20model%27s%20gradients%20are%20perceptually%20aligned%3A%0Aattacks%20against%20the%20model%20produce%20noise%20that%20perceptually%20resembles%20the%0Atargeted%20class.%0A%20%20In%20this%20short%20note%2C%20we%20show%20that%20this%20defense%20is%20not%20robust%20to%20adversarial%0Aattack.%20We%20first%20show%20that%20the%20defense%27s%20randomness%20and%20ensembling%20method%20cause%0Asevere%20gradient%20masking.%20We%20then%20use%20standard%20adaptive%20attack%20techniques%20to%0Areduce%20the%20defense%27s%20robust%20accuracy%20from%2048%25%20to%201%25%20on%20CIFAR-100%20and%20from%2062%25%0Ato%204%25%20on%20CIFAR-10%2C%20under%20the%20%24%5Cell_%5Cinfty%24-norm%20threat%20model%20with%0A%24%5Cvarepsilon%3D8/255%24.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2411.14834v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DGradient%2520Masking%2520All-at-Once%253A%2520Ensemble%2520Everything%2520Everywhere%2520Is%2520Not%250A%2520%2520Robust%26entry.906535625%3DJie%2520Zhang%2520and%2520Kristina%2520Nikoli%25C4%2587%2520and%2520Nicholas%2520Carlini%2520and%2520Florian%2520Tram%25C3%25A8r%26entry.1292438233%3D%2520%2520Ensemble%2520everything%2520everywhere%2520is%2520a%2520defense%2520to%2520adversarial%2520examples%2520that%2520was%250Arecently%2520proposed%2520to%2520make%2520image%2520classifiers%2520robust.%2520This%2520defense%2520works%2520by%250Aensembling%2520a%2520model%2527s%2520intermediate%2520representations%2520at%2520multiple%2520noisy%2520image%250Aresolutions%252C%2520producing%2520a%2520single%2520robust%2520classification.%2520This%2520defense%2520was%2520shown%250Ato%2520be%2520effective%2520against%2520multiple%2520state-of-the-art%2520attacks.%2520Perhaps%2520even%2520more%250Aconvincingly%252C%2520it%2520was%2520shown%2520that%2520the%2520model%2527s%2520gradients%2520are%2520perceptually%2520aligned%253A%250Aattacks%2520against%2520the%2520model%2520produce%2520noise%2520that%2520perceptually%2520resembles%2520the%250Atargeted%2520class.%250A%2520%2520In%2520this%2520short%2520note%252C%2520we%2520show%2520that%2520this%2520defense%2520is%2520not%2520robust%2520to%2520adversarial%250Aattack.%2520We%2520first%2520show%2520that%2520the%2520defense%2527s%2520randomness%2520and%2520ensembling%2520method%2520cause%250Asevere%2520gradient%2520masking.%2520We%2520then%2520use%2520standard%2520adaptive%2520attack%2520techniques%2520to%250Areduce%2520the%2520defense%2527s%2520robust%2520accuracy%2520from%252048%2525%2520to%25201%2525%2520on%2520CIFAR-100%2520and%2520from%252062%2525%250Ato%25204%2525%2520on%2520CIFAR-10%252C%2520under%2520the%2520%2524%255Cell_%255Cinfty%2524-norm%2520threat%2520model%2520with%250A%2524%255Cvarepsilon%253D8/255%2524.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2411.14834v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Gradient%20Masking%20All-at-Once%3A%20Ensemble%20Everything%20Everywhere%20Is%20Not%0A%20%20Robust&entry.906535625=Jie%20Zhang%20and%20Kristina%20Nikoli%C4%87%20and%20Nicholas%20Carlini%20and%20Florian%20Tram%C3%A8r&entry.1292438233=%20%20Ensemble%20everything%20everywhere%20is%20a%20defense%20to%20adversarial%20examples%20that%20was%0Arecently%20proposed%20to%20make%20image%20classifiers%20robust.%20This%20defense%20works%20by%0Aensembling%20a%20model%27s%20intermediate%20representations%20at%20multiple%20noisy%20image%0Aresolutions%2C%20producing%20a%20single%20robust%20classification.%20This%20defense%20was%20shown%0Ato%20be%20effective%20against%20multiple%20state-of-the-art%20attacks.%20Perhaps%20even%20more%0Aconvincingly%2C%20it%20was%20shown%20that%20the%20model%27s%20gradients%20are%20perceptually%20aligned%3A%0Aattacks%20against%20the%20model%20produce%20noise%20that%20perceptually%20resembles%20the%0Atargeted%20class.%0A%20%20In%20this%20short%20note%2C%20we%20show%20that%20this%20defense%20is%20not%20robust%20to%20adversarial%0Aattack.%20We%20first%20show%20that%20the%20defense%27s%20randomness%20and%20ensembling%20method%20cause%0Asevere%20gradient%20masking.%20We%20then%20use%20standard%20adaptive%20attack%20techniques%20to%0Areduce%20the%20defense%27s%20robust%20accuracy%20from%2048%25%20to%201%25%20on%20CIFAR-100%20and%20from%2062%25%0Ato%204%25%20on%20CIFAR-10%2C%20under%20the%20%24%5Cell_%5Cinfty%24-norm%20threat%20model%20with%0A%24%5Cvarepsilon%3D8/255%24.%0A&entry.1838667208=http%3A//arxiv.org/abs/2411.14834v1&entry.124074799=Read"},
{"title": "GTA: A Benchmark for General Tool Agents", "author": "Jize Wang and Zerun Ma and Yining Li and Songyang Zhang and Cailian Chen and Kai Chen and Xinyi Le", "abstract": "  Significant focus has been placed on integrating large language models (LLMs)\nwith various tools in developing general-purpose agents. This poses a challenge\nto LLMs' tool-use capabilities. However, there are evident gaps between\nexisting tool-use evaluations and real-world scenarios. Current evaluations\noften use AI-generated queries, single-step tasks, dummy tools, and text-only\ninteractions, failing to reveal the agents' real-world problem-solving\nabilities effectively. To address this, we propose GTA, a benchmark for General\nTool Agents, featuring three main aspects: (i) Real user queries: human-written\nqueries with simple real-world objectives but implicit tool-use, requiring the\nLLM to reason the suitable tools and plan the solution steps. (ii) Real\ndeployed tools: an evaluation platform equipped with tools across perception,\noperation, logic, and creativity categories to evaluate the agents' actual task\nexecution performance. (iii) Real multimodal inputs: authentic image files,\nsuch as spatial scenes, web page screenshots, tables, code snippets, and\nprinted/handwritten materials, used as the query contexts to align with\nreal-world scenarios closely. We design 229 real-world tasks and executable\ntool chains to evaluate mainstream LLMs. Our findings show that real-world user\nqueries are challenging for existing LLMs, with GPT-4 completing less than 50%\nof the tasks and most LLMs achieving below 25%. This evaluation reveals the\nbottlenecks in the tool-use capabilities of current LLMs in real-world\nscenarios, which provides future direction for advancing general-purpose tool\nagents. The code and dataset are available at\nhttps://github.com/open-compass/GTA.\n", "link": "http://arxiv.org/abs/2407.08713v2", "date": "2024-11-22", "relevancy": 2.0007, "topK": [{"title": "RoboEXP: Action-Conditioned Scene Graph via Interactive Exploration for\n  Robotic Manipulation", "link": "http://arxiv.org/abs/2402.15487v1", "similarity": 0.53}, {"title": "Genie: Generative Interactive Environments", "link": "http://arxiv.org/abs/2402.15391v1", "similarity": 0.5099}, {"title": "UGG: Unified Generative Grasping", "link": "https://arxiv.org/abs/2311.16917", "similarity": 0.4785}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20GTA%3A%20A%20Benchmark%20for%20General%20Tool%20Agents&body=Title%3A%20GTA%3A%20A%20Benchmark%20for%20General%20Tool%20Agents%0AAuthor%3A%20Jize%20Wang%20and%20Zerun%20Ma%20and%20Yining%20Li%20and%20Songyang%20Zhang%20and%20Cailian%20Chen%20and%20Kai%20Chen%20and%20Xinyi%20Le%0AAbstract%3A%20%20%20Significant%20focus%20has%20been%20placed%20on%20integrating%20large%20language%20models%20%28LLMs%29%0Awith%20various%20tools%20in%20developing%20general-purpose%20agents.%20This%20poses%20a%20challenge%0Ato%20LLMs%27%20tool-use%20capabilities.%20However%2C%20there%20are%20evident%20gaps%20between%0Aexisting%20tool-use%20evaluations%20and%20real-world%20scenarios.%20Current%20evaluations%0Aoften%20use%20AI-generated%20queries%2C%20single-step%20tasks%2C%20dummy%20tools%2C%20and%20text-only%0Ainteractions%2C%20failing%20to%20reveal%20the%20agents%27%20real-world%20problem-solving%0Aabilities%20effectively.%20To%20address%20this%2C%20we%20propose%20GTA%2C%20a%20benchmark%20for%20General%0ATool%20Agents%2C%20featuring%20three%20main%20aspects%3A%20%28i%29%20Real%20user%20queries%3A%20human-written%0Aqueries%20with%20simple%20real-world%20objectives%20but%20implicit%20tool-use%2C%20requiring%20the%0ALLM%20to%20reason%20the%20suitable%20tools%20and%20plan%20the%20solution%20steps.%20%28ii%29%20Real%0Adeployed%20tools%3A%20an%20evaluation%20platform%20equipped%20with%20tools%20across%20perception%2C%0Aoperation%2C%20logic%2C%20and%20creativity%20categories%20to%20evaluate%20the%20agents%27%20actual%20task%0Aexecution%20performance.%20%28iii%29%20Real%20multimodal%20inputs%3A%20authentic%20image%20files%2C%0Asuch%20as%20spatial%20scenes%2C%20web%20page%20screenshots%2C%20tables%2C%20code%20snippets%2C%20and%0Aprinted/handwritten%20materials%2C%20used%20as%20the%20query%20contexts%20to%20align%20with%0Areal-world%20scenarios%20closely.%20We%20design%20229%20real-world%20tasks%20and%20executable%0Atool%20chains%20to%20evaluate%20mainstream%20LLMs.%20Our%20findings%20show%20that%20real-world%20user%0Aqueries%20are%20challenging%20for%20existing%20LLMs%2C%20with%20GPT-4%20completing%20less%20than%2050%25%0Aof%20the%20tasks%20and%20most%20LLMs%20achieving%20below%2025%25.%20This%20evaluation%20reveals%20the%0Abottlenecks%20in%20the%20tool-use%20capabilities%20of%20current%20LLMs%20in%20real-world%0Ascenarios%2C%20which%20provides%20future%20direction%20for%20advancing%20general-purpose%20tool%0Aagents.%20The%20code%20and%20dataset%20are%20available%20at%0Ahttps%3A//github.com/open-compass/GTA.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2407.08713v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DGTA%253A%2520A%2520Benchmark%2520for%2520General%2520Tool%2520Agents%26entry.906535625%3DJize%2520Wang%2520and%2520Zerun%2520Ma%2520and%2520Yining%2520Li%2520and%2520Songyang%2520Zhang%2520and%2520Cailian%2520Chen%2520and%2520Kai%2520Chen%2520and%2520Xinyi%2520Le%26entry.1292438233%3D%2520%2520Significant%2520focus%2520has%2520been%2520placed%2520on%2520integrating%2520large%2520language%2520models%2520%2528LLMs%2529%250Awith%2520various%2520tools%2520in%2520developing%2520general-purpose%2520agents.%2520This%2520poses%2520a%2520challenge%250Ato%2520LLMs%2527%2520tool-use%2520capabilities.%2520However%252C%2520there%2520are%2520evident%2520gaps%2520between%250Aexisting%2520tool-use%2520evaluations%2520and%2520real-world%2520scenarios.%2520Current%2520evaluations%250Aoften%2520use%2520AI-generated%2520queries%252C%2520single-step%2520tasks%252C%2520dummy%2520tools%252C%2520and%2520text-only%250Ainteractions%252C%2520failing%2520to%2520reveal%2520the%2520agents%2527%2520real-world%2520problem-solving%250Aabilities%2520effectively.%2520To%2520address%2520this%252C%2520we%2520propose%2520GTA%252C%2520a%2520benchmark%2520for%2520General%250ATool%2520Agents%252C%2520featuring%2520three%2520main%2520aspects%253A%2520%2528i%2529%2520Real%2520user%2520queries%253A%2520human-written%250Aqueries%2520with%2520simple%2520real-world%2520objectives%2520but%2520implicit%2520tool-use%252C%2520requiring%2520the%250ALLM%2520to%2520reason%2520the%2520suitable%2520tools%2520and%2520plan%2520the%2520solution%2520steps.%2520%2528ii%2529%2520Real%250Adeployed%2520tools%253A%2520an%2520evaluation%2520platform%2520equipped%2520with%2520tools%2520across%2520perception%252C%250Aoperation%252C%2520logic%252C%2520and%2520creativity%2520categories%2520to%2520evaluate%2520the%2520agents%2527%2520actual%2520task%250Aexecution%2520performance.%2520%2528iii%2529%2520Real%2520multimodal%2520inputs%253A%2520authentic%2520image%2520files%252C%250Asuch%2520as%2520spatial%2520scenes%252C%2520web%2520page%2520screenshots%252C%2520tables%252C%2520code%2520snippets%252C%2520and%250Aprinted/handwritten%2520materials%252C%2520used%2520as%2520the%2520query%2520contexts%2520to%2520align%2520with%250Areal-world%2520scenarios%2520closely.%2520We%2520design%2520229%2520real-world%2520tasks%2520and%2520executable%250Atool%2520chains%2520to%2520evaluate%2520mainstream%2520LLMs.%2520Our%2520findings%2520show%2520that%2520real-world%2520user%250Aqueries%2520are%2520challenging%2520for%2520existing%2520LLMs%252C%2520with%2520GPT-4%2520completing%2520less%2520than%252050%2525%250Aof%2520the%2520tasks%2520and%2520most%2520LLMs%2520achieving%2520below%252025%2525.%2520This%2520evaluation%2520reveals%2520the%250Abottlenecks%2520in%2520the%2520tool-use%2520capabilities%2520of%2520current%2520LLMs%2520in%2520real-world%250Ascenarios%252C%2520which%2520provides%2520future%2520direction%2520for%2520advancing%2520general-purpose%2520tool%250Aagents.%2520The%2520code%2520and%2520dataset%2520are%2520available%2520at%250Ahttps%253A//github.com/open-compass/GTA.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2407.08713v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=GTA%3A%20A%20Benchmark%20for%20General%20Tool%20Agents&entry.906535625=Jize%20Wang%20and%20Zerun%20Ma%20and%20Yining%20Li%20and%20Songyang%20Zhang%20and%20Cailian%20Chen%20and%20Kai%20Chen%20and%20Xinyi%20Le&entry.1292438233=%20%20Significant%20focus%20has%20been%20placed%20on%20integrating%20large%20language%20models%20%28LLMs%29%0Awith%20various%20tools%20in%20developing%20general-purpose%20agents.%20This%20poses%20a%20challenge%0Ato%20LLMs%27%20tool-use%20capabilities.%20However%2C%20there%20are%20evident%20gaps%20between%0Aexisting%20tool-use%20evaluations%20and%20real-world%20scenarios.%20Current%20evaluations%0Aoften%20use%20AI-generated%20queries%2C%20single-step%20tasks%2C%20dummy%20tools%2C%20and%20text-only%0Ainteractions%2C%20failing%20to%20reveal%20the%20agents%27%20real-world%20problem-solving%0Aabilities%20effectively.%20To%20address%20this%2C%20we%20propose%20GTA%2C%20a%20benchmark%20for%20General%0ATool%20Agents%2C%20featuring%20three%20main%20aspects%3A%20%28i%29%20Real%20user%20queries%3A%20human-written%0Aqueries%20with%20simple%20real-world%20objectives%20but%20implicit%20tool-use%2C%20requiring%20the%0ALLM%20to%20reason%20the%20suitable%20tools%20and%20plan%20the%20solution%20steps.%20%28ii%29%20Real%0Adeployed%20tools%3A%20an%20evaluation%20platform%20equipped%20with%20tools%20across%20perception%2C%0Aoperation%2C%20logic%2C%20and%20creativity%20categories%20to%20evaluate%20the%20agents%27%20actual%20task%0Aexecution%20performance.%20%28iii%29%20Real%20multimodal%20inputs%3A%20authentic%20image%20files%2C%0Asuch%20as%20spatial%20scenes%2C%20web%20page%20screenshots%2C%20tables%2C%20code%20snippets%2C%20and%0Aprinted/handwritten%20materials%2C%20used%20as%20the%20query%20contexts%20to%20align%20with%0Areal-world%20scenarios%20closely.%20We%20design%20229%20real-world%20tasks%20and%20executable%0Atool%20chains%20to%20evaluate%20mainstream%20LLMs.%20Our%20findings%20show%20that%20real-world%20user%0Aqueries%20are%20challenging%20for%20existing%20LLMs%2C%20with%20GPT-4%20completing%20less%20than%2050%25%0Aof%20the%20tasks%20and%20most%20LLMs%20achieving%20below%2025%25.%20This%20evaluation%20reveals%20the%0Abottlenecks%20in%20the%20tool-use%20capabilities%20of%20current%20LLMs%20in%20real-world%0Ascenarios%2C%20which%20provides%20future%20direction%20for%20advancing%20general-purpose%20tool%0Aagents.%20The%20code%20and%20dataset%20are%20available%20at%0Ahttps%3A//github.com/open-compass/GTA.%0A&entry.1838667208=http%3A//arxiv.org/abs/2407.08713v2&entry.124074799=Read"},
{"title": "TIPO: Text to Image with Text Presampling for Prompt Optimization", "author": "Shih-Ying Yeh and Sang-Hyun Park and Giyeong Oh and Min Song and Youngjae Yu", "abstract": "  TIPO (Text to Image with text pre-sampling for Prompt Optimization) is an\ninnovative framework designed to enhance text-to-image (T2I) generation by\nlanguage model (LM) for automatic prompt engineering. By refining and extending\nuser-provided prompts, TIPO bridges the gap between simple inputs and the\ndetailed prompts required for high-quality image generation. Unlike previous\napproaches that rely on Large Language Models (LLMs) or reinforcement learning\n(RL), TIPO adjusts user input prompts with the distribution of a trained prompt\ndataset, eliminating the need for complex runtime cost via lightweight model.\nThis pre-sampling approach enables efficient and scalable prompt optimization,\ngrounded in the model's training distribution. Experimental results demonstrate\nTIPO's effectiveness in improving aesthetic scores, reducing image corruption,\nand better aligning generated images with dataset distributions. These findings\nhighlight the critical role of prompt engineering in T2I systems and open\navenues for broader applications of automatic prompt refinement.\n", "link": "http://arxiv.org/abs/2411.08127v2", "date": "2024-11-22", "relevancy": 1.9992, "topK": [{"title": "Customize-A-Video: One-Shot Motion Customization of Text-to-Video\n  Diffusion Models", "link": "http://arxiv.org/abs/2402.14780v1", "similarity": 0.5026}, {"title": "ToDo: Token Downsampling for Efficient Generation of High-Resolution\n  Images", "link": "http://arxiv.org/abs/2402.13573v2", "similarity": 0.4999}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.4986}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20TIPO%3A%20Text%20to%20Image%20with%20Text%20Presampling%20for%20Prompt%20Optimization&body=Title%3A%20TIPO%3A%20Text%20to%20Image%20with%20Text%20Presampling%20for%20Prompt%20Optimization%0AAuthor%3A%20Shih-Ying%20Yeh%20and%20Sang-Hyun%20Park%20and%20Giyeong%20Oh%20and%20Min%20Song%20and%20Youngjae%20Yu%0AAbstract%3A%20%20%20TIPO%20%28Text%20to%20Image%20with%20text%20pre-sampling%20for%20Prompt%20Optimization%29%20is%20an%0Ainnovative%20framework%20designed%20to%20enhance%20text-to-image%20%28T2I%29%20generation%20by%0Alanguage%20model%20%28LM%29%20for%20automatic%20prompt%20engineering.%20By%20refining%20and%20extending%0Auser-provided%20prompts%2C%20TIPO%20bridges%20the%20gap%20between%20simple%20inputs%20and%20the%0Adetailed%20prompts%20required%20for%20high-quality%20image%20generation.%20Unlike%20previous%0Aapproaches%20that%20rely%20on%20Large%20Language%20Models%20%28LLMs%29%20or%20reinforcement%20learning%0A%28RL%29%2C%20TIPO%20adjusts%20user%20input%20prompts%20with%20the%20distribution%20of%20a%20trained%20prompt%0Adataset%2C%20eliminating%20the%20need%20for%20complex%20runtime%20cost%20via%20lightweight%20model.%0AThis%20pre-sampling%20approach%20enables%20efficient%20and%20scalable%20prompt%20optimization%2C%0Agrounded%20in%20the%20model%27s%20training%20distribution.%20Experimental%20results%20demonstrate%0ATIPO%27s%20effectiveness%20in%20improving%20aesthetic%20scores%2C%20reducing%20image%20corruption%2C%0Aand%20better%20aligning%20generated%20images%20with%20dataset%20distributions.%20These%20findings%0Ahighlight%20the%20critical%20role%20of%20prompt%20engineering%20in%20T2I%20systems%20and%20open%0Aavenues%20for%20broader%20applications%20of%20automatic%20prompt%20refinement.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2411.08127v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DTIPO%253A%2520Text%2520to%2520Image%2520with%2520Text%2520Presampling%2520for%2520Prompt%2520Optimization%26entry.906535625%3DShih-Ying%2520Yeh%2520and%2520Sang-Hyun%2520Park%2520and%2520Giyeong%2520Oh%2520and%2520Min%2520Song%2520and%2520Youngjae%2520Yu%26entry.1292438233%3D%2520%2520TIPO%2520%2528Text%2520to%2520Image%2520with%2520text%2520pre-sampling%2520for%2520Prompt%2520Optimization%2529%2520is%2520an%250Ainnovative%2520framework%2520designed%2520to%2520enhance%2520text-to-image%2520%2528T2I%2529%2520generation%2520by%250Alanguage%2520model%2520%2528LM%2529%2520for%2520automatic%2520prompt%2520engineering.%2520By%2520refining%2520and%2520extending%250Auser-provided%2520prompts%252C%2520TIPO%2520bridges%2520the%2520gap%2520between%2520simple%2520inputs%2520and%2520the%250Adetailed%2520prompts%2520required%2520for%2520high-quality%2520image%2520generation.%2520Unlike%2520previous%250Aapproaches%2520that%2520rely%2520on%2520Large%2520Language%2520Models%2520%2528LLMs%2529%2520or%2520reinforcement%2520learning%250A%2528RL%2529%252C%2520TIPO%2520adjusts%2520user%2520input%2520prompts%2520with%2520the%2520distribution%2520of%2520a%2520trained%2520prompt%250Adataset%252C%2520eliminating%2520the%2520need%2520for%2520complex%2520runtime%2520cost%2520via%2520lightweight%2520model.%250AThis%2520pre-sampling%2520approach%2520enables%2520efficient%2520and%2520scalable%2520prompt%2520optimization%252C%250Agrounded%2520in%2520the%2520model%2527s%2520training%2520distribution.%2520Experimental%2520results%2520demonstrate%250ATIPO%2527s%2520effectiveness%2520in%2520improving%2520aesthetic%2520scores%252C%2520reducing%2520image%2520corruption%252C%250Aand%2520better%2520aligning%2520generated%2520images%2520with%2520dataset%2520distributions.%2520These%2520findings%250Ahighlight%2520the%2520critical%2520role%2520of%2520prompt%2520engineering%2520in%2520T2I%2520systems%2520and%2520open%250Aavenues%2520for%2520broader%2520applications%2520of%2520automatic%2520prompt%2520refinement.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2411.08127v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=TIPO%3A%20Text%20to%20Image%20with%20Text%20Presampling%20for%20Prompt%20Optimization&entry.906535625=Shih-Ying%20Yeh%20and%20Sang-Hyun%20Park%20and%20Giyeong%20Oh%20and%20Min%20Song%20and%20Youngjae%20Yu&entry.1292438233=%20%20TIPO%20%28Text%20to%20Image%20with%20text%20pre-sampling%20for%20Prompt%20Optimization%29%20is%20an%0Ainnovative%20framework%20designed%20to%20enhance%20text-to-image%20%28T2I%29%20generation%20by%0Alanguage%20model%20%28LM%29%20for%20automatic%20prompt%20engineering.%20By%20refining%20and%20extending%0Auser-provided%20prompts%2C%20TIPO%20bridges%20the%20gap%20between%20simple%20inputs%20and%20the%0Adetailed%20prompts%20required%20for%20high-quality%20image%20generation.%20Unlike%20previous%0Aapproaches%20that%20rely%20on%20Large%20Language%20Models%20%28LLMs%29%20or%20reinforcement%20learning%0A%28RL%29%2C%20TIPO%20adjusts%20user%20input%20prompts%20with%20the%20distribution%20of%20a%20trained%20prompt%0Adataset%2C%20eliminating%20the%20need%20for%20complex%20runtime%20cost%20via%20lightweight%20model.%0AThis%20pre-sampling%20approach%20enables%20efficient%20and%20scalable%20prompt%20optimization%2C%0Agrounded%20in%20the%20model%27s%20training%20distribution.%20Experimental%20results%20demonstrate%0ATIPO%27s%20effectiveness%20in%20improving%20aesthetic%20scores%2C%20reducing%20image%20corruption%2C%0Aand%20better%20aligning%20generated%20images%20with%20dataset%20distributions.%20These%20findings%0Ahighlight%20the%20critical%20role%20of%20prompt%20engineering%20in%20T2I%20systems%20and%20open%0Aavenues%20for%20broader%20applications%20of%20automatic%20prompt%20refinement.%0A&entry.1838667208=http%3A//arxiv.org/abs/2411.08127v2&entry.124074799=Read"},
{"title": "Dimension-independent rates for structured neural density estimation", "author": "Robert A. Vandermeulen and Wai Ming Tai and Bryon Aragam", "abstract": "  We show that deep neural networks achieve dimension-independent rates of\nconvergence for learning structured densities such as those arising in image,\naudio, video, and text applications. More precisely, we demonstrate that neural\nnetworks with a simple $L^2$-minimizing loss achieve a rate of $n^{-1/(4+r)}$\nin nonparametric density estimation when the underlying density is Markov to a\ngraph whose maximum clique size is at most $r$, and we provide evidence that in\nthe aforementioned applications, this size is typically constant, i.e.,\n$r=O(1)$. We then establish that the optimal rate in $L^1$ is $n^{-1/(2+r)}$\nwhich, compared to the standard nonparametric rate of $n^{-1/(2+d)}$, reveals\nthat the effective dimension of such problems is the size of the largest clique\nin the Markov random field. These rates are independent of the data's ambient\ndimension, making them applicable to realistic models of image, sound, video,\nand text data. Our results provide a novel justification for deep learning's\nability to circumvent the curse of dimensionality, demonstrating\ndimension-independent convergence rates in these contexts.\n", "link": "http://arxiv.org/abs/2411.15095v1", "date": "2024-11-22", "relevancy": 1.9885, "topK": [{"title": "Boosted dynamic neural networks", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/26302", "similarity": 0.5142}, {"title": "ToDo: Token Downsampling for Efficient Generation of High-Resolution\n  Images", "link": "http://arxiv.org/abs/2402.13573v2", "similarity": 0.4861}, {"title": "SDXS: Real-Time One-Step Latent Diffusion Models with Image Conditions", "link": "http://arxiv.org/abs/2403.16627v1", "similarity": 0.4818}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Dimension-independent%20rates%20for%20structured%20neural%20density%20estimation&body=Title%3A%20Dimension-independent%20rates%20for%20structured%20neural%20density%20estimation%0AAuthor%3A%20Robert%20A.%20Vandermeulen%20and%20Wai%20Ming%20Tai%20and%20Bryon%20Aragam%0AAbstract%3A%20%20%20We%20show%20that%20deep%20neural%20networks%20achieve%20dimension-independent%20rates%20of%0Aconvergence%20for%20learning%20structured%20densities%20such%20as%20those%20arising%20in%20image%2C%0Aaudio%2C%20video%2C%20and%20text%20applications.%20More%20precisely%2C%20we%20demonstrate%20that%20neural%0Anetworks%20with%20a%20simple%20%24L%5E2%24-minimizing%20loss%20achieve%20a%20rate%20of%20%24n%5E%7B-1/%284%2Br%29%7D%24%0Ain%20nonparametric%20density%20estimation%20when%20the%20underlying%20density%20is%20Markov%20to%20a%0Agraph%20whose%20maximum%20clique%20size%20is%20at%20most%20%24r%24%2C%20and%20we%20provide%20evidence%20that%20in%0Athe%20aforementioned%20applications%2C%20this%20size%20is%20typically%20constant%2C%20i.e.%2C%0A%24r%3DO%281%29%24.%20We%20then%20establish%20that%20the%20optimal%20rate%20in%20%24L%5E1%24%20is%20%24n%5E%7B-1/%282%2Br%29%7D%24%0Awhich%2C%20compared%20to%20the%20standard%20nonparametric%20rate%20of%20%24n%5E%7B-1/%282%2Bd%29%7D%24%2C%20reveals%0Athat%20the%20effective%20dimension%20of%20such%20problems%20is%20the%20size%20of%20the%20largest%20clique%0Ain%20the%20Markov%20random%20field.%20These%20rates%20are%20independent%20of%20the%20data%27s%20ambient%0Adimension%2C%20making%20them%20applicable%20to%20realistic%20models%20of%20image%2C%20sound%2C%20video%2C%0Aand%20text%20data.%20Our%20results%20provide%20a%20novel%20justification%20for%20deep%20learning%27s%0Aability%20to%20circumvent%20the%20curse%20of%20dimensionality%2C%20demonstrating%0Adimension-independent%20convergence%20rates%20in%20these%20contexts.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2411.15095v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DDimension-independent%2520rates%2520for%2520structured%2520neural%2520density%2520estimation%26entry.906535625%3DRobert%2520A.%2520Vandermeulen%2520and%2520Wai%2520Ming%2520Tai%2520and%2520Bryon%2520Aragam%26entry.1292438233%3D%2520%2520We%2520show%2520that%2520deep%2520neural%2520networks%2520achieve%2520dimension-independent%2520rates%2520of%250Aconvergence%2520for%2520learning%2520structured%2520densities%2520such%2520as%2520those%2520arising%2520in%2520image%252C%250Aaudio%252C%2520video%252C%2520and%2520text%2520applications.%2520More%2520precisely%252C%2520we%2520demonstrate%2520that%2520neural%250Anetworks%2520with%2520a%2520simple%2520%2524L%255E2%2524-minimizing%2520loss%2520achieve%2520a%2520rate%2520of%2520%2524n%255E%257B-1/%25284%252Br%2529%257D%2524%250Ain%2520nonparametric%2520density%2520estimation%2520when%2520the%2520underlying%2520density%2520is%2520Markov%2520to%2520a%250Agraph%2520whose%2520maximum%2520clique%2520size%2520is%2520at%2520most%2520%2524r%2524%252C%2520and%2520we%2520provide%2520evidence%2520that%2520in%250Athe%2520aforementioned%2520applications%252C%2520this%2520size%2520is%2520typically%2520constant%252C%2520i.e.%252C%250A%2524r%253DO%25281%2529%2524.%2520We%2520then%2520establish%2520that%2520the%2520optimal%2520rate%2520in%2520%2524L%255E1%2524%2520is%2520%2524n%255E%257B-1/%25282%252Br%2529%257D%2524%250Awhich%252C%2520compared%2520to%2520the%2520standard%2520nonparametric%2520rate%2520of%2520%2524n%255E%257B-1/%25282%252Bd%2529%257D%2524%252C%2520reveals%250Athat%2520the%2520effective%2520dimension%2520of%2520such%2520problems%2520is%2520the%2520size%2520of%2520the%2520largest%2520clique%250Ain%2520the%2520Markov%2520random%2520field.%2520These%2520rates%2520are%2520independent%2520of%2520the%2520data%2527s%2520ambient%250Adimension%252C%2520making%2520them%2520applicable%2520to%2520realistic%2520models%2520of%2520image%252C%2520sound%252C%2520video%252C%250Aand%2520text%2520data.%2520Our%2520results%2520provide%2520a%2520novel%2520justification%2520for%2520deep%2520learning%2527s%250Aability%2520to%2520circumvent%2520the%2520curse%2520of%2520dimensionality%252C%2520demonstrating%250Adimension-independent%2520convergence%2520rates%2520in%2520these%2520contexts.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2411.15095v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Dimension-independent%20rates%20for%20structured%20neural%20density%20estimation&entry.906535625=Robert%20A.%20Vandermeulen%20and%20Wai%20Ming%20Tai%20and%20Bryon%20Aragam&entry.1292438233=%20%20We%20show%20that%20deep%20neural%20networks%20achieve%20dimension-independent%20rates%20of%0Aconvergence%20for%20learning%20structured%20densities%20such%20as%20those%20arising%20in%20image%2C%0Aaudio%2C%20video%2C%20and%20text%20applications.%20More%20precisely%2C%20we%20demonstrate%20that%20neural%0Anetworks%20with%20a%20simple%20%24L%5E2%24-minimizing%20loss%20achieve%20a%20rate%20of%20%24n%5E%7B-1/%284%2Br%29%7D%24%0Ain%20nonparametric%20density%20estimation%20when%20the%20underlying%20density%20is%20Markov%20to%20a%0Agraph%20whose%20maximum%20clique%20size%20is%20at%20most%20%24r%24%2C%20and%20we%20provide%20evidence%20that%20in%0Athe%20aforementioned%20applications%2C%20this%20size%20is%20typically%20constant%2C%20i.e.%2C%0A%24r%3DO%281%29%24.%20We%20then%20establish%20that%20the%20optimal%20rate%20in%20%24L%5E1%24%20is%20%24n%5E%7B-1/%282%2Br%29%7D%24%0Awhich%2C%20compared%20to%20the%20standard%20nonparametric%20rate%20of%20%24n%5E%7B-1/%282%2Bd%29%7D%24%2C%20reveals%0Athat%20the%20effective%20dimension%20of%20such%20problems%20is%20the%20size%20of%20the%20largest%20clique%0Ain%20the%20Markov%20random%20field.%20These%20rates%20are%20independent%20of%20the%20data%27s%20ambient%0Adimension%2C%20making%20them%20applicable%20to%20realistic%20models%20of%20image%2C%20sound%2C%20video%2C%0Aand%20text%20data.%20Our%20results%20provide%20a%20novel%20justification%20for%20deep%20learning%27s%0Aability%20to%20circumvent%20the%20curse%20of%20dimensionality%2C%20demonstrating%0Adimension-independent%20convergence%20rates%20in%20these%20contexts.%0A&entry.1838667208=http%3A//arxiv.org/abs/2411.15095v1&entry.124074799=Read"},
{"title": "Engagement-Driven Content Generation with Large Language Models", "author": "Erica Coppolillo and Federico Cinus and Marco Minici and Francesco Bonchi and Giuseppe Manco", "abstract": "  Large Language Models (LLMs) exhibit significant persuasion capabilities in\none-on-one interactions, but their influence within social networks remains\nunderexplored. This study investigates the potential social impact of LLMs in\nthese environments, where interconnected users and complex opinion dynamics\npose unique challenges. In particular, we address the following research\nquestion: can LLMs learn to generate meaningful content that maximizes user\nengagement on social networks?\n  To answer this question, we define a pipeline to guide the LLM-based content\ngeneration which employs reinforcement learning with simulated feedback. In our\nframework, the reward is based on an engagement model borrowed from the\nliterature on opinion dynamics and information propagation. Moreover, we force\nthe text generated by the LLM to be aligned with a given topic and to satisfy a\nminimum fluency requirement.\n  Using our framework, we analyze the capabilities and limitations of LLMs in\ntackling the given task, specifically considering the relative positions of the\nLLM as an agent within the social network and the distribution of opinions in\nthe network on the given topic. Our findings show the full potential of LLMs in\ncreating social engagement. Notable properties of our approach are that the\nlearning procedure is adaptive to the opinion distribution of the underlying\nnetwork and agnostic to the specifics of the engagement model, which is\nembedded as a plug-and-play component. In this regard, our approach can be\neasily refined for more complex engagement tasks and interventions in\ncomputational social science.\n  The code used for the experiments is publicly available at\nhttps://anonymous.4open.science/r/EDCG/.\n", "link": "http://arxiv.org/abs/2411.13187v3", "date": "2024-11-22", "relevancy": 1.9843, "topK": [{"title": "Genie: Generative Interactive Environments", "link": "http://arxiv.org/abs/2402.15391v1", "similarity": 0.5041}, {"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.4913}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.49}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Engagement-Driven%20Content%20Generation%20with%20Large%20Language%20Models&body=Title%3A%20Engagement-Driven%20Content%20Generation%20with%20Large%20Language%20Models%0AAuthor%3A%20Erica%20Coppolillo%20and%20Federico%20Cinus%20and%20Marco%20Minici%20and%20Francesco%20Bonchi%20and%20Giuseppe%20Manco%0AAbstract%3A%20%20%20Large%20Language%20Models%20%28LLMs%29%20exhibit%20significant%20persuasion%20capabilities%20in%0Aone-on-one%20interactions%2C%20but%20their%20influence%20within%20social%20networks%20remains%0Aunderexplored.%20This%20study%20investigates%20the%20potential%20social%20impact%20of%20LLMs%20in%0Athese%20environments%2C%20where%20interconnected%20users%20and%20complex%20opinion%20dynamics%0Apose%20unique%20challenges.%20In%20particular%2C%20we%20address%20the%20following%20research%0Aquestion%3A%20can%20LLMs%20learn%20to%20generate%20meaningful%20content%20that%20maximizes%20user%0Aengagement%20on%20social%20networks%3F%0A%20%20To%20answer%20this%20question%2C%20we%20define%20a%20pipeline%20to%20guide%20the%20LLM-based%20content%0Ageneration%20which%20employs%20reinforcement%20learning%20with%20simulated%20feedback.%20In%20our%0Aframework%2C%20the%20reward%20is%20based%20on%20an%20engagement%20model%20borrowed%20from%20the%0Aliterature%20on%20opinion%20dynamics%20and%20information%20propagation.%20Moreover%2C%20we%20force%0Athe%20text%20generated%20by%20the%20LLM%20to%20be%20aligned%20with%20a%20given%20topic%20and%20to%20satisfy%20a%0Aminimum%20fluency%20requirement.%0A%20%20Using%20our%20framework%2C%20we%20analyze%20the%20capabilities%20and%20limitations%20of%20LLMs%20in%0Atackling%20the%20given%20task%2C%20specifically%20considering%20the%20relative%20positions%20of%20the%0ALLM%20as%20an%20agent%20within%20the%20social%20network%20and%20the%20distribution%20of%20opinions%20in%0Athe%20network%20on%20the%20given%20topic.%20Our%20findings%20show%20the%20full%20potential%20of%20LLMs%20in%0Acreating%20social%20engagement.%20Notable%20properties%20of%20our%20approach%20are%20that%20the%0Alearning%20procedure%20is%20adaptive%20to%20the%20opinion%20distribution%20of%20the%20underlying%0Anetwork%20and%20agnostic%20to%20the%20specifics%20of%20the%20engagement%20model%2C%20which%20is%0Aembedded%20as%20a%20plug-and-play%20component.%20In%20this%20regard%2C%20our%20approach%20can%20be%0Aeasily%20refined%20for%20more%20complex%20engagement%20tasks%20and%20interventions%20in%0Acomputational%20social%20science.%0A%20%20The%20code%20used%20for%20the%20experiments%20is%20publicly%20available%20at%0Ahttps%3A//anonymous.4open.science/r/EDCG/.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2411.13187v3%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DEngagement-Driven%2520Content%2520Generation%2520with%2520Large%2520Language%2520Models%26entry.906535625%3DErica%2520Coppolillo%2520and%2520Federico%2520Cinus%2520and%2520Marco%2520Minici%2520and%2520Francesco%2520Bonchi%2520and%2520Giuseppe%2520Manco%26entry.1292438233%3D%2520%2520Large%2520Language%2520Models%2520%2528LLMs%2529%2520exhibit%2520significant%2520persuasion%2520capabilities%2520in%250Aone-on-one%2520interactions%252C%2520but%2520their%2520influence%2520within%2520social%2520networks%2520remains%250Aunderexplored.%2520This%2520study%2520investigates%2520the%2520potential%2520social%2520impact%2520of%2520LLMs%2520in%250Athese%2520environments%252C%2520where%2520interconnected%2520users%2520and%2520complex%2520opinion%2520dynamics%250Apose%2520unique%2520challenges.%2520In%2520particular%252C%2520we%2520address%2520the%2520following%2520research%250Aquestion%253A%2520can%2520LLMs%2520learn%2520to%2520generate%2520meaningful%2520content%2520that%2520maximizes%2520user%250Aengagement%2520on%2520social%2520networks%253F%250A%2520%2520To%2520answer%2520this%2520question%252C%2520we%2520define%2520a%2520pipeline%2520to%2520guide%2520the%2520LLM-based%2520content%250Ageneration%2520which%2520employs%2520reinforcement%2520learning%2520with%2520simulated%2520feedback.%2520In%2520our%250Aframework%252C%2520the%2520reward%2520is%2520based%2520on%2520an%2520engagement%2520model%2520borrowed%2520from%2520the%250Aliterature%2520on%2520opinion%2520dynamics%2520and%2520information%2520propagation.%2520Moreover%252C%2520we%2520force%250Athe%2520text%2520generated%2520by%2520the%2520LLM%2520to%2520be%2520aligned%2520with%2520a%2520given%2520topic%2520and%2520to%2520satisfy%2520a%250Aminimum%2520fluency%2520requirement.%250A%2520%2520Using%2520our%2520framework%252C%2520we%2520analyze%2520the%2520capabilities%2520and%2520limitations%2520of%2520LLMs%2520in%250Atackling%2520the%2520given%2520task%252C%2520specifically%2520considering%2520the%2520relative%2520positions%2520of%2520the%250ALLM%2520as%2520an%2520agent%2520within%2520the%2520social%2520network%2520and%2520the%2520distribution%2520of%2520opinions%2520in%250Athe%2520network%2520on%2520the%2520given%2520topic.%2520Our%2520findings%2520show%2520the%2520full%2520potential%2520of%2520LLMs%2520in%250Acreating%2520social%2520engagement.%2520Notable%2520properties%2520of%2520our%2520approach%2520are%2520that%2520the%250Alearning%2520procedure%2520is%2520adaptive%2520to%2520the%2520opinion%2520distribution%2520of%2520the%2520underlying%250Anetwork%2520and%2520agnostic%2520to%2520the%2520specifics%2520of%2520the%2520engagement%2520model%252C%2520which%2520is%250Aembedded%2520as%2520a%2520plug-and-play%2520component.%2520In%2520this%2520regard%252C%2520our%2520approach%2520can%2520be%250Aeasily%2520refined%2520for%2520more%2520complex%2520engagement%2520tasks%2520and%2520interventions%2520in%250Acomputational%2520social%2520science.%250A%2520%2520The%2520code%2520used%2520for%2520the%2520experiments%2520is%2520publicly%2520available%2520at%250Ahttps%253A//anonymous.4open.science/r/EDCG/.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2411.13187v3%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Engagement-Driven%20Content%20Generation%20with%20Large%20Language%20Models&entry.906535625=Erica%20Coppolillo%20and%20Federico%20Cinus%20and%20Marco%20Minici%20and%20Francesco%20Bonchi%20and%20Giuseppe%20Manco&entry.1292438233=%20%20Large%20Language%20Models%20%28LLMs%29%20exhibit%20significant%20persuasion%20capabilities%20in%0Aone-on-one%20interactions%2C%20but%20their%20influence%20within%20social%20networks%20remains%0Aunderexplored.%20This%20study%20investigates%20the%20potential%20social%20impact%20of%20LLMs%20in%0Athese%20environments%2C%20where%20interconnected%20users%20and%20complex%20opinion%20dynamics%0Apose%20unique%20challenges.%20In%20particular%2C%20we%20address%20the%20following%20research%0Aquestion%3A%20can%20LLMs%20learn%20to%20generate%20meaningful%20content%20that%20maximizes%20user%0Aengagement%20on%20social%20networks%3F%0A%20%20To%20answer%20this%20question%2C%20we%20define%20a%20pipeline%20to%20guide%20the%20LLM-based%20content%0Ageneration%20which%20employs%20reinforcement%20learning%20with%20simulated%20feedback.%20In%20our%0Aframework%2C%20the%20reward%20is%20based%20on%20an%20engagement%20model%20borrowed%20from%20the%0Aliterature%20on%20opinion%20dynamics%20and%20information%20propagation.%20Moreover%2C%20we%20force%0Athe%20text%20generated%20by%20the%20LLM%20to%20be%20aligned%20with%20a%20given%20topic%20and%20to%20satisfy%20a%0Aminimum%20fluency%20requirement.%0A%20%20Using%20our%20framework%2C%20we%20analyze%20the%20capabilities%20and%20limitations%20of%20LLMs%20in%0Atackling%20the%20given%20task%2C%20specifically%20considering%20the%20relative%20positions%20of%20the%0ALLM%20as%20an%20agent%20within%20the%20social%20network%20and%20the%20distribution%20of%20opinions%20in%0Athe%20network%20on%20the%20given%20topic.%20Our%20findings%20show%20the%20full%20potential%20of%20LLMs%20in%0Acreating%20social%20engagement.%20Notable%20properties%20of%20our%20approach%20are%20that%20the%0Alearning%20procedure%20is%20adaptive%20to%20the%20opinion%20distribution%20of%20the%20underlying%0Anetwork%20and%20agnostic%20to%20the%20specifics%20of%20the%20engagement%20model%2C%20which%20is%0Aembedded%20as%20a%20plug-and-play%20component.%20In%20this%20regard%2C%20our%20approach%20can%20be%0Aeasily%20refined%20for%20more%20complex%20engagement%20tasks%20and%20interventions%20in%0Acomputational%20social%20science.%0A%20%20The%20code%20used%20for%20the%20experiments%20is%20publicly%20available%20at%0Ahttps%3A//anonymous.4open.science/r/EDCG/.%0A&entry.1838667208=http%3A//arxiv.org/abs/2411.13187v3&entry.124074799=Read"},
{"title": "XGrammar: Flexible and Efficient Structured Generation Engine for Large\n  Language Models", "author": "Yixin Dong and Charlie F. Ruan and Yaxing Cai and Ruihang Lai and Ziyi Xu and Yilong Zhao and Tianqi Chen", "abstract": "  The applications of LLM Agents are becoming increasingly complex and diverse,\nleading to a high demand for structured outputs that can be parsed into code,\nstructured function calls, and embodied agent commands. These developments\nbring significant demands for structured generation in LLM inference.\nContext-free grammar is a flexible approach to enable structured generation via\nconstrained decoding. However, executing context-free grammar requires going\nthrough several stack states over all tokens in vocabulary during runtime,\nbringing non-negligible overhead for structured generation. In this paper, we\npropose XGrammar, a flexible and efficient structure generation engine for\nlarge language models. XGrammar accelerates context-free grammar execution by\ndividing the vocabulary into context-independent tokens that can be prechecked\nand context-dependent tokens that need to be interpreted during runtime. We\nfurther build transformations to expand the grammar context and reduce the\nnumber of context-independent tokens. Additionally, we build an efficient\npersistent stack to accelerate the context-dependent token checks. Finally, we\nco-design the grammar engine with LLM inference engine to overlap grammar\ncomputation with GPU executions. Evaluation results show that XGrammar can\nachieve up to 100x speedup over existing solutions. Combined with an LLM\ninference engine, it can generate near-zero overhead structure generation in\nend-to-end low-LLM serving.\n", "link": "http://arxiv.org/abs/2411.15100v1", "date": "2024-11-22", "relevancy": 1.9818, "topK": [{"title": "Genie: Generative Interactive Environments", "link": "http://arxiv.org/abs/2402.15391v1", "similarity": 0.497}, {"title": "UGG: Unified Generative Grasping", "link": "https://arxiv.org/abs/2311.16917", "similarity": 0.497}, {"title": "RoboEXP: Action-Conditioned Scene Graph via Interactive Exploration for\n  Robotic Manipulation", "link": "http://arxiv.org/abs/2402.15487v1", "similarity": 0.4878}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20XGrammar%3A%20Flexible%20and%20Efficient%20Structured%20Generation%20Engine%20for%20Large%0A%20%20Language%20Models&body=Title%3A%20XGrammar%3A%20Flexible%20and%20Efficient%20Structured%20Generation%20Engine%20for%20Large%0A%20%20Language%20Models%0AAuthor%3A%20Yixin%20Dong%20and%20Charlie%20F.%20Ruan%20and%20Yaxing%20Cai%20and%20Ruihang%20Lai%20and%20Ziyi%20Xu%20and%20Yilong%20Zhao%20and%20Tianqi%20Chen%0AAbstract%3A%20%20%20The%20applications%20of%20LLM%20Agents%20are%20becoming%20increasingly%20complex%20and%20diverse%2C%0Aleading%20to%20a%20high%20demand%20for%20structured%20outputs%20that%20can%20be%20parsed%20into%20code%2C%0Astructured%20function%20calls%2C%20and%20embodied%20agent%20commands.%20These%20developments%0Abring%20significant%20demands%20for%20structured%20generation%20in%20LLM%20inference.%0AContext-free%20grammar%20is%20a%20flexible%20approach%20to%20enable%20structured%20generation%20via%0Aconstrained%20decoding.%20However%2C%20executing%20context-free%20grammar%20requires%20going%0Athrough%20several%20stack%20states%20over%20all%20tokens%20in%20vocabulary%20during%20runtime%2C%0Abringing%20non-negligible%20overhead%20for%20structured%20generation.%20In%20this%20paper%2C%20we%0Apropose%20XGrammar%2C%20a%20flexible%20and%20efficient%20structure%20generation%20engine%20for%0Alarge%20language%20models.%20XGrammar%20accelerates%20context-free%20grammar%20execution%20by%0Adividing%20the%20vocabulary%20into%20context-independent%20tokens%20that%20can%20be%20prechecked%0Aand%20context-dependent%20tokens%20that%20need%20to%20be%20interpreted%20during%20runtime.%20We%0Afurther%20build%20transformations%20to%20expand%20the%20grammar%20context%20and%20reduce%20the%0Anumber%20of%20context-independent%20tokens.%20Additionally%2C%20we%20build%20an%20efficient%0Apersistent%20stack%20to%20accelerate%20the%20context-dependent%20token%20checks.%20Finally%2C%20we%0Aco-design%20the%20grammar%20engine%20with%20LLM%20inference%20engine%20to%20overlap%20grammar%0Acomputation%20with%20GPU%20executions.%20Evaluation%20results%20show%20that%20XGrammar%20can%0Aachieve%20up%20to%20100x%20speedup%20over%20existing%20solutions.%20Combined%20with%20an%20LLM%0Ainference%20engine%2C%20it%20can%20generate%20near-zero%20overhead%20structure%20generation%20in%0Aend-to-end%20low-LLM%20serving.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2411.15100v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DXGrammar%253A%2520Flexible%2520and%2520Efficient%2520Structured%2520Generation%2520Engine%2520for%2520Large%250A%2520%2520Language%2520Models%26entry.906535625%3DYixin%2520Dong%2520and%2520Charlie%2520F.%2520Ruan%2520and%2520Yaxing%2520Cai%2520and%2520Ruihang%2520Lai%2520and%2520Ziyi%2520Xu%2520and%2520Yilong%2520Zhao%2520and%2520Tianqi%2520Chen%26entry.1292438233%3D%2520%2520The%2520applications%2520of%2520LLM%2520Agents%2520are%2520becoming%2520increasingly%2520complex%2520and%2520diverse%252C%250Aleading%2520to%2520a%2520high%2520demand%2520for%2520structured%2520outputs%2520that%2520can%2520be%2520parsed%2520into%2520code%252C%250Astructured%2520function%2520calls%252C%2520and%2520embodied%2520agent%2520commands.%2520These%2520developments%250Abring%2520significant%2520demands%2520for%2520structured%2520generation%2520in%2520LLM%2520inference.%250AContext-free%2520grammar%2520is%2520a%2520flexible%2520approach%2520to%2520enable%2520structured%2520generation%2520via%250Aconstrained%2520decoding.%2520However%252C%2520executing%2520context-free%2520grammar%2520requires%2520going%250Athrough%2520several%2520stack%2520states%2520over%2520all%2520tokens%2520in%2520vocabulary%2520during%2520runtime%252C%250Abringing%2520non-negligible%2520overhead%2520for%2520structured%2520generation.%2520In%2520this%2520paper%252C%2520we%250Apropose%2520XGrammar%252C%2520a%2520flexible%2520and%2520efficient%2520structure%2520generation%2520engine%2520for%250Alarge%2520language%2520models.%2520XGrammar%2520accelerates%2520context-free%2520grammar%2520execution%2520by%250Adividing%2520the%2520vocabulary%2520into%2520context-independent%2520tokens%2520that%2520can%2520be%2520prechecked%250Aand%2520context-dependent%2520tokens%2520that%2520need%2520to%2520be%2520interpreted%2520during%2520runtime.%2520We%250Afurther%2520build%2520transformations%2520to%2520expand%2520the%2520grammar%2520context%2520and%2520reduce%2520the%250Anumber%2520of%2520context-independent%2520tokens.%2520Additionally%252C%2520we%2520build%2520an%2520efficient%250Apersistent%2520stack%2520to%2520accelerate%2520the%2520context-dependent%2520token%2520checks.%2520Finally%252C%2520we%250Aco-design%2520the%2520grammar%2520engine%2520with%2520LLM%2520inference%2520engine%2520to%2520overlap%2520grammar%250Acomputation%2520with%2520GPU%2520executions.%2520Evaluation%2520results%2520show%2520that%2520XGrammar%2520can%250Aachieve%2520up%2520to%2520100x%2520speedup%2520over%2520existing%2520solutions.%2520Combined%2520with%2520an%2520LLM%250Ainference%2520engine%252C%2520it%2520can%2520generate%2520near-zero%2520overhead%2520structure%2520generation%2520in%250Aend-to-end%2520low-LLM%2520serving.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2411.15100v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=XGrammar%3A%20Flexible%20and%20Efficient%20Structured%20Generation%20Engine%20for%20Large%0A%20%20Language%20Models&entry.906535625=Yixin%20Dong%20and%20Charlie%20F.%20Ruan%20and%20Yaxing%20Cai%20and%20Ruihang%20Lai%20and%20Ziyi%20Xu%20and%20Yilong%20Zhao%20and%20Tianqi%20Chen&entry.1292438233=%20%20The%20applications%20of%20LLM%20Agents%20are%20becoming%20increasingly%20complex%20and%20diverse%2C%0Aleading%20to%20a%20high%20demand%20for%20structured%20outputs%20that%20can%20be%20parsed%20into%20code%2C%0Astructured%20function%20calls%2C%20and%20embodied%20agent%20commands.%20These%20developments%0Abring%20significant%20demands%20for%20structured%20generation%20in%20LLM%20inference.%0AContext-free%20grammar%20is%20a%20flexible%20approach%20to%20enable%20structured%20generation%20via%0Aconstrained%20decoding.%20However%2C%20executing%20context-free%20grammar%20requires%20going%0Athrough%20several%20stack%20states%20over%20all%20tokens%20in%20vocabulary%20during%20runtime%2C%0Abringing%20non-negligible%20overhead%20for%20structured%20generation.%20In%20this%20paper%2C%20we%0Apropose%20XGrammar%2C%20a%20flexible%20and%20efficient%20structure%20generation%20engine%20for%0Alarge%20language%20models.%20XGrammar%20accelerates%20context-free%20grammar%20execution%20by%0Adividing%20the%20vocabulary%20into%20context-independent%20tokens%20that%20can%20be%20prechecked%0Aand%20context-dependent%20tokens%20that%20need%20to%20be%20interpreted%20during%20runtime.%20We%0Afurther%20build%20transformations%20to%20expand%20the%20grammar%20context%20and%20reduce%20the%0Anumber%20of%20context-independent%20tokens.%20Additionally%2C%20we%20build%20an%20efficient%0Apersistent%20stack%20to%20accelerate%20the%20context-dependent%20token%20checks.%20Finally%2C%20we%0Aco-design%20the%20grammar%20engine%20with%20LLM%20inference%20engine%20to%20overlap%20grammar%0Acomputation%20with%20GPU%20executions.%20Evaluation%20results%20show%20that%20XGrammar%20can%0Aachieve%20up%20to%20100x%20speedup%20over%20existing%20solutions.%20Combined%20with%20an%20LLM%0Ainference%20engine%2C%20it%20can%20generate%20near-zero%20overhead%20structure%20generation%20in%0Aend-to-end%20low-LLM%20serving.%0A&entry.1838667208=http%3A//arxiv.org/abs/2411.15100v1&entry.124074799=Read"},
{"title": "RE-Bench: Evaluating frontier AI R&D capabilities of language model\n  agents against human experts", "author": "Hjalmar Wijk and Tao Lin and Joel Becker and Sami Jawhar and Neev Parikh and Thomas Broadley and Lawrence Chan and Michael Chen and Josh Clymer and Jai Dhyani and Elena Ericheva and Katharyn Garcia and Brian Goodrich and Nikola Jurkovic and Megan Kinniment and Aron Lajko and Seraphina Nix and Lucas Sato and William Saunders and Maksym Taran and Ben West and Elizabeth Barnes", "abstract": "  Frontier AI safety policies highlight automation of AI research and\ndevelopment (R&D) by AI agents as an important capability to anticipate.\nHowever, there exist few evaluations for AI R&D capabilities, and none that are\nhighly realistic and have a direct comparison to human performance. We\nintroduce RE-Bench (Research Engineering Benchmark, v1), which consists of 7\nchallenging, open-ended ML research engineering environments and data from 71\n8-hour attempts by 61 distinct human experts. We confirm that our experts make\nprogress in the environments given 8 hours, with 82% of expert attempts\nachieving a non-zero score and 24% matching or exceeding our strong reference\nsolutions. We compare humans to several public frontier models through\nbest-of-k with varying time budgets and agent designs, and find that the best\nAI agents achieve a score 4x higher than human experts when both are given a\ntotal time budget of 2 hours per environment. However, humans currently display\nbetter returns to increasing time budgets, narrowly exceeding the top AI agent\nscores given an 8-hour budget, and achieving 2x the score of the top AI agent\nwhen both are given 32 total hours (across different attempts). Qualitatively,\nwe find that modern AI agents possess significant expertise in many ML topics\n-- e.g. an agent wrote a faster custom Triton kernel than any of our human\nexperts' -- and can generate and test solutions over ten times faster than\nhumans, at much lower cost. We open-source the evaluation environments, human\nexpert data, analysis code and agent trajectories to facilitate future\nresearch.\n", "link": "http://arxiv.org/abs/2411.15114v1", "date": "2024-11-22", "relevancy": 1.9816, "topK": [{"title": "RoboEXP: Action-Conditioned Scene Graph via Interactive Exploration for\n  Robotic Manipulation", "link": "http://arxiv.org/abs/2402.15487v1", "similarity": 0.5143}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.4916}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.4916}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20RE-Bench%3A%20Evaluating%20frontier%20AI%20R%26D%20capabilities%20of%20language%20model%0A%20%20agents%20against%20human%20experts&body=Title%3A%20RE-Bench%3A%20Evaluating%20frontier%20AI%20R%26D%20capabilities%20of%20language%20model%0A%20%20agents%20against%20human%20experts%0AAuthor%3A%20Hjalmar%20Wijk%20and%20Tao%20Lin%20and%20Joel%20Becker%20and%20Sami%20Jawhar%20and%20Neev%20Parikh%20and%20Thomas%20Broadley%20and%20Lawrence%20Chan%20and%20Michael%20Chen%20and%20Josh%20Clymer%20and%20Jai%20Dhyani%20and%20Elena%20Ericheva%20and%20Katharyn%20Garcia%20and%20Brian%20Goodrich%20and%20Nikola%20Jurkovic%20and%20Megan%20Kinniment%20and%20Aron%20Lajko%20and%20Seraphina%20Nix%20and%20Lucas%20Sato%20and%20William%20Saunders%20and%20Maksym%20Taran%20and%20Ben%20West%20and%20Elizabeth%20Barnes%0AAbstract%3A%20%20%20Frontier%20AI%20safety%20policies%20highlight%20automation%20of%20AI%20research%20and%0Adevelopment%20%28R%26D%29%20by%20AI%20agents%20as%20an%20important%20capability%20to%20anticipate.%0AHowever%2C%20there%20exist%20few%20evaluations%20for%20AI%20R%26D%20capabilities%2C%20and%20none%20that%20are%0Ahighly%20realistic%20and%20have%20a%20direct%20comparison%20to%20human%20performance.%20We%0Aintroduce%20RE-Bench%20%28Research%20Engineering%20Benchmark%2C%20v1%29%2C%20which%20consists%20of%207%0Achallenging%2C%20open-ended%20ML%20research%20engineering%20environments%20and%20data%20from%2071%0A8-hour%20attempts%20by%2061%20distinct%20human%20experts.%20We%20confirm%20that%20our%20experts%20make%0Aprogress%20in%20the%20environments%20given%208%20hours%2C%20with%2082%25%20of%20expert%20attempts%0Aachieving%20a%20non-zero%20score%20and%2024%25%20matching%20or%20exceeding%20our%20strong%20reference%0Asolutions.%20We%20compare%20humans%20to%20several%20public%20frontier%20models%20through%0Abest-of-k%20with%20varying%20time%20budgets%20and%20agent%20designs%2C%20and%20find%20that%20the%20best%0AAI%20agents%20achieve%20a%20score%204x%20higher%20than%20human%20experts%20when%20both%20are%20given%20a%0Atotal%20time%20budget%20of%202%20hours%20per%20environment.%20However%2C%20humans%20currently%20display%0Abetter%20returns%20to%20increasing%20time%20budgets%2C%20narrowly%20exceeding%20the%20top%20AI%20agent%0Ascores%20given%20an%208-hour%20budget%2C%20and%20achieving%202x%20the%20score%20of%20the%20top%20AI%20agent%0Awhen%20both%20are%20given%2032%20total%20hours%20%28across%20different%20attempts%29.%20Qualitatively%2C%0Awe%20find%20that%20modern%20AI%20agents%20possess%20significant%20expertise%20in%20many%20ML%20topics%0A--%20e.g.%20an%20agent%20wrote%20a%20faster%20custom%20Triton%20kernel%20than%20any%20of%20our%20human%0Aexperts%27%20--%20and%20can%20generate%20and%20test%20solutions%20over%20ten%20times%20faster%20than%0Ahumans%2C%20at%20much%20lower%20cost.%20We%20open-source%20the%20evaluation%20environments%2C%20human%0Aexpert%20data%2C%20analysis%20code%20and%20agent%20trajectories%20to%20facilitate%20future%0Aresearch.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2411.15114v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DRE-Bench%253A%2520Evaluating%2520frontier%2520AI%2520R%2526D%2520capabilities%2520of%2520language%2520model%250A%2520%2520agents%2520against%2520human%2520experts%26entry.906535625%3DHjalmar%2520Wijk%2520and%2520Tao%2520Lin%2520and%2520Joel%2520Becker%2520and%2520Sami%2520Jawhar%2520and%2520Neev%2520Parikh%2520and%2520Thomas%2520Broadley%2520and%2520Lawrence%2520Chan%2520and%2520Michael%2520Chen%2520and%2520Josh%2520Clymer%2520and%2520Jai%2520Dhyani%2520and%2520Elena%2520Ericheva%2520and%2520Katharyn%2520Garcia%2520and%2520Brian%2520Goodrich%2520and%2520Nikola%2520Jurkovic%2520and%2520Megan%2520Kinniment%2520and%2520Aron%2520Lajko%2520and%2520Seraphina%2520Nix%2520and%2520Lucas%2520Sato%2520and%2520William%2520Saunders%2520and%2520Maksym%2520Taran%2520and%2520Ben%2520West%2520and%2520Elizabeth%2520Barnes%26entry.1292438233%3D%2520%2520Frontier%2520AI%2520safety%2520policies%2520highlight%2520automation%2520of%2520AI%2520research%2520and%250Adevelopment%2520%2528R%2526D%2529%2520by%2520AI%2520agents%2520as%2520an%2520important%2520capability%2520to%2520anticipate.%250AHowever%252C%2520there%2520exist%2520few%2520evaluations%2520for%2520AI%2520R%2526D%2520capabilities%252C%2520and%2520none%2520that%2520are%250Ahighly%2520realistic%2520and%2520have%2520a%2520direct%2520comparison%2520to%2520human%2520performance.%2520We%250Aintroduce%2520RE-Bench%2520%2528Research%2520Engineering%2520Benchmark%252C%2520v1%2529%252C%2520which%2520consists%2520of%25207%250Achallenging%252C%2520open-ended%2520ML%2520research%2520engineering%2520environments%2520and%2520data%2520from%252071%250A8-hour%2520attempts%2520by%252061%2520distinct%2520human%2520experts.%2520We%2520confirm%2520that%2520our%2520experts%2520make%250Aprogress%2520in%2520the%2520environments%2520given%25208%2520hours%252C%2520with%252082%2525%2520of%2520expert%2520attempts%250Aachieving%2520a%2520non-zero%2520score%2520and%252024%2525%2520matching%2520or%2520exceeding%2520our%2520strong%2520reference%250Asolutions.%2520We%2520compare%2520humans%2520to%2520several%2520public%2520frontier%2520models%2520through%250Abest-of-k%2520with%2520varying%2520time%2520budgets%2520and%2520agent%2520designs%252C%2520and%2520find%2520that%2520the%2520best%250AAI%2520agents%2520achieve%2520a%2520score%25204x%2520higher%2520than%2520human%2520experts%2520when%2520both%2520are%2520given%2520a%250Atotal%2520time%2520budget%2520of%25202%2520hours%2520per%2520environment.%2520However%252C%2520humans%2520currently%2520display%250Abetter%2520returns%2520to%2520increasing%2520time%2520budgets%252C%2520narrowly%2520exceeding%2520the%2520top%2520AI%2520agent%250Ascores%2520given%2520an%25208-hour%2520budget%252C%2520and%2520achieving%25202x%2520the%2520score%2520of%2520the%2520top%2520AI%2520agent%250Awhen%2520both%2520are%2520given%252032%2520total%2520hours%2520%2528across%2520different%2520attempts%2529.%2520Qualitatively%252C%250Awe%2520find%2520that%2520modern%2520AI%2520agents%2520possess%2520significant%2520expertise%2520in%2520many%2520ML%2520topics%250A--%2520e.g.%2520an%2520agent%2520wrote%2520a%2520faster%2520custom%2520Triton%2520kernel%2520than%2520any%2520of%2520our%2520human%250Aexperts%2527%2520--%2520and%2520can%2520generate%2520and%2520test%2520solutions%2520over%2520ten%2520times%2520faster%2520than%250Ahumans%252C%2520at%2520much%2520lower%2520cost.%2520We%2520open-source%2520the%2520evaluation%2520environments%252C%2520human%250Aexpert%2520data%252C%2520analysis%2520code%2520and%2520agent%2520trajectories%2520to%2520facilitate%2520future%250Aresearch.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2411.15114v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=RE-Bench%3A%20Evaluating%20frontier%20AI%20R%26D%20capabilities%20of%20language%20model%0A%20%20agents%20against%20human%20experts&entry.906535625=Hjalmar%20Wijk%20and%20Tao%20Lin%20and%20Joel%20Becker%20and%20Sami%20Jawhar%20and%20Neev%20Parikh%20and%20Thomas%20Broadley%20and%20Lawrence%20Chan%20and%20Michael%20Chen%20and%20Josh%20Clymer%20and%20Jai%20Dhyani%20and%20Elena%20Ericheva%20and%20Katharyn%20Garcia%20and%20Brian%20Goodrich%20and%20Nikola%20Jurkovic%20and%20Megan%20Kinniment%20and%20Aron%20Lajko%20and%20Seraphina%20Nix%20and%20Lucas%20Sato%20and%20William%20Saunders%20and%20Maksym%20Taran%20and%20Ben%20West%20and%20Elizabeth%20Barnes&entry.1292438233=%20%20Frontier%20AI%20safety%20policies%20highlight%20automation%20of%20AI%20research%20and%0Adevelopment%20%28R%26D%29%20by%20AI%20agents%20as%20an%20important%20capability%20to%20anticipate.%0AHowever%2C%20there%20exist%20few%20evaluations%20for%20AI%20R%26D%20capabilities%2C%20and%20none%20that%20are%0Ahighly%20realistic%20and%20have%20a%20direct%20comparison%20to%20human%20performance.%20We%0Aintroduce%20RE-Bench%20%28Research%20Engineering%20Benchmark%2C%20v1%29%2C%20which%20consists%20of%207%0Achallenging%2C%20open-ended%20ML%20research%20engineering%20environments%20and%20data%20from%2071%0A8-hour%20attempts%20by%2061%20distinct%20human%20experts.%20We%20confirm%20that%20our%20experts%20make%0Aprogress%20in%20the%20environments%20given%208%20hours%2C%20with%2082%25%20of%20expert%20attempts%0Aachieving%20a%20non-zero%20score%20and%2024%25%20matching%20or%20exceeding%20our%20strong%20reference%0Asolutions.%20We%20compare%20humans%20to%20several%20public%20frontier%20models%20through%0Abest-of-k%20with%20varying%20time%20budgets%20and%20agent%20designs%2C%20and%20find%20that%20the%20best%0AAI%20agents%20achieve%20a%20score%204x%20higher%20than%20human%20experts%20when%20both%20are%20given%20a%0Atotal%20time%20budget%20of%202%20hours%20per%20environment.%20However%2C%20humans%20currently%20display%0Abetter%20returns%20to%20increasing%20time%20budgets%2C%20narrowly%20exceeding%20the%20top%20AI%20agent%0Ascores%20given%20an%208-hour%20budget%2C%20and%20achieving%202x%20the%20score%20of%20the%20top%20AI%20agent%0Awhen%20both%20are%20given%2032%20total%20hours%20%28across%20different%20attempts%29.%20Qualitatively%2C%0Awe%20find%20that%20modern%20AI%20agents%20possess%20significant%20expertise%20in%20many%20ML%20topics%0A--%20e.g.%20an%20agent%20wrote%20a%20faster%20custom%20Triton%20kernel%20than%20any%20of%20our%20human%0Aexperts%27%20--%20and%20can%20generate%20and%20test%20solutions%20over%20ten%20times%20faster%20than%0Ahumans%2C%20at%20much%20lower%20cost.%20We%20open-source%20the%20evaluation%20environments%2C%20human%0Aexpert%20data%2C%20analysis%20code%20and%20agent%20trajectories%20to%20facilitate%20future%0Aresearch.%0A&entry.1838667208=http%3A//arxiv.org/abs/2411.15114v1&entry.124074799=Read"},
{"title": "FTA generation using GenAI with an Autonomy sensor Usecase", "author": "Sneha Sudhir Shetiya and Divya Garikapati and Veeraja Sohoni", "abstract": "  Functional safety forms an important aspect in the design of systems. Its\nemphasis on the automotive industry has evolved significantly over the years.\nTill date many methods have been developed to get appropriate FTA(Fault Tree\nanalysis) for various scenarios and features pertaining to Autonomous Driving.\nThis paper is an attempt to explore the scope of using Generative Artificial\nIntelligence(GenAI) in order to develop Fault Tree Analysis(FTA) with the use\ncase of malfunction for the Lidar sensor in mind. We explore various available\nopen source Large Language Models(LLM) models and then dive deep into one of\nthem to study its responses and provide our analysis. This paper successfully\nshows the possibility to train existing Large Language models through Prompt\nEngineering for fault tree analysis for any Autonomy usecase aided with\nPlantUML tool.\n", "link": "http://arxiv.org/abs/2411.15007v1", "date": "2024-11-22", "relevancy": 1.9699, "topK": [{"title": "Genie: Generative Interactive Environments", "link": "http://arxiv.org/abs/2402.15391v1", "similarity": 0.5153}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.4795}, {"title": "UGG: Unified Generative Grasping", "link": "https://arxiv.org/abs/2311.16917", "similarity": 0.4748}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20FTA%20generation%20using%20GenAI%20with%20an%20Autonomy%20sensor%20Usecase&body=Title%3A%20FTA%20generation%20using%20GenAI%20with%20an%20Autonomy%20sensor%20Usecase%0AAuthor%3A%20Sneha%20Sudhir%20Shetiya%20and%20Divya%20Garikapati%20and%20Veeraja%20Sohoni%0AAbstract%3A%20%20%20Functional%20safety%20forms%20an%20important%20aspect%20in%20the%20design%20of%20systems.%20Its%0Aemphasis%20on%20the%20automotive%20industry%20has%20evolved%20significantly%20over%20the%20years.%0ATill%20date%20many%20methods%20have%20been%20developed%20to%20get%20appropriate%20FTA%28Fault%20Tree%0Aanalysis%29%20for%20various%20scenarios%20and%20features%20pertaining%20to%20Autonomous%20Driving.%0AThis%20paper%20is%20an%20attempt%20to%20explore%20the%20scope%20of%20using%20Generative%20Artificial%0AIntelligence%28GenAI%29%20in%20order%20to%20develop%20Fault%20Tree%20Analysis%28FTA%29%20with%20the%20use%0Acase%20of%20malfunction%20for%20the%20Lidar%20sensor%20in%20mind.%20We%20explore%20various%20available%0Aopen%20source%20Large%20Language%20Models%28LLM%29%20models%20and%20then%20dive%20deep%20into%20one%20of%0Athem%20to%20study%20its%20responses%20and%20provide%20our%20analysis.%20This%20paper%20successfully%0Ashows%20the%20possibility%20to%20train%20existing%20Large%20Language%20models%20through%20Prompt%0AEngineering%20for%20fault%20tree%20analysis%20for%20any%20Autonomy%20usecase%20aided%20with%0APlantUML%20tool.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2411.15007v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DFTA%2520generation%2520using%2520GenAI%2520with%2520an%2520Autonomy%2520sensor%2520Usecase%26entry.906535625%3DSneha%2520Sudhir%2520Shetiya%2520and%2520Divya%2520Garikapati%2520and%2520Veeraja%2520Sohoni%26entry.1292438233%3D%2520%2520Functional%2520safety%2520forms%2520an%2520important%2520aspect%2520in%2520the%2520design%2520of%2520systems.%2520Its%250Aemphasis%2520on%2520the%2520automotive%2520industry%2520has%2520evolved%2520significantly%2520over%2520the%2520years.%250ATill%2520date%2520many%2520methods%2520have%2520been%2520developed%2520to%2520get%2520appropriate%2520FTA%2528Fault%2520Tree%250Aanalysis%2529%2520for%2520various%2520scenarios%2520and%2520features%2520pertaining%2520to%2520Autonomous%2520Driving.%250AThis%2520paper%2520is%2520an%2520attempt%2520to%2520explore%2520the%2520scope%2520of%2520using%2520Generative%2520Artificial%250AIntelligence%2528GenAI%2529%2520in%2520order%2520to%2520develop%2520Fault%2520Tree%2520Analysis%2528FTA%2529%2520with%2520the%2520use%250Acase%2520of%2520malfunction%2520for%2520the%2520Lidar%2520sensor%2520in%2520mind.%2520We%2520explore%2520various%2520available%250Aopen%2520source%2520Large%2520Language%2520Models%2528LLM%2529%2520models%2520and%2520then%2520dive%2520deep%2520into%2520one%2520of%250Athem%2520to%2520study%2520its%2520responses%2520and%2520provide%2520our%2520analysis.%2520This%2520paper%2520successfully%250Ashows%2520the%2520possibility%2520to%2520train%2520existing%2520Large%2520Language%2520models%2520through%2520Prompt%250AEngineering%2520for%2520fault%2520tree%2520analysis%2520for%2520any%2520Autonomy%2520usecase%2520aided%2520with%250APlantUML%2520tool.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2411.15007v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=FTA%20generation%20using%20GenAI%20with%20an%20Autonomy%20sensor%20Usecase&entry.906535625=Sneha%20Sudhir%20Shetiya%20and%20Divya%20Garikapati%20and%20Veeraja%20Sohoni&entry.1292438233=%20%20Functional%20safety%20forms%20an%20important%20aspect%20in%20the%20design%20of%20systems.%20Its%0Aemphasis%20on%20the%20automotive%20industry%20has%20evolved%20significantly%20over%20the%20years.%0ATill%20date%20many%20methods%20have%20been%20developed%20to%20get%20appropriate%20FTA%28Fault%20Tree%0Aanalysis%29%20for%20various%20scenarios%20and%20features%20pertaining%20to%20Autonomous%20Driving.%0AThis%20paper%20is%20an%20attempt%20to%20explore%20the%20scope%20of%20using%20Generative%20Artificial%0AIntelligence%28GenAI%29%20in%20order%20to%20develop%20Fault%20Tree%20Analysis%28FTA%29%20with%20the%20use%0Acase%20of%20malfunction%20for%20the%20Lidar%20sensor%20in%20mind.%20We%20explore%20various%20available%0Aopen%20source%20Large%20Language%20Models%28LLM%29%20models%20and%20then%20dive%20deep%20into%20one%20of%0Athem%20to%20study%20its%20responses%20and%20provide%20our%20analysis.%20This%20paper%20successfully%0Ashows%20the%20possibility%20to%20train%20existing%20Large%20Language%20models%20through%20Prompt%0AEngineering%20for%20fault%20tree%20analysis%20for%20any%20Autonomy%20usecase%20aided%20with%0APlantUML%20tool.%0A&entry.1838667208=http%3A//arxiv.org/abs/2411.15007v1&entry.124074799=Read"},
{"title": "Health AI Developer Foundations", "author": "Atilla P. Kiraly and Sebastien Baur and Kenneth Philbrick and Fereshteh Mahvar and Liron Yatziv and Tiffany Chen and Bram Sterling and Nick George and Fayaz Jamil and Jing Tang and Kai Bailey and Faruk Ahmed and Akshay Goel and Abbi Ward and Lin Yang and Andrew Sellergren and Yossi Matias and Avinatan Hassidim and Shravya Shetty and Daniel Golden and Shekoofeh Azizi and David F. Steiner and Yun Liu and Tim Thelin and Rory Pilgrim and Can Kirmizibayrak", "abstract": "  Robust medical Machine Learning (ML) models have the potential to\nrevolutionize healthcare by accelerating clinical research, improving workflows\nand outcomes, and producing novel insights or capabilities. Developing such ML\nmodels from scratch is cost prohibitive and requires substantial compute, data,\nand time (e.g., expert labeling). To address these challenges, we introduce\nHealth AI Developer Foundations (HAI-DEF), a suite of pre-trained,\ndomain-specific foundation models, tools, and recipes to accelerate building ML\nfor health applications. The models cover various modalities and domains,\nincluding radiology (X-rays and computed tomography), histopathology,\ndermatological imaging, and audio. These models provide domain specific\nembeddings that facilitate AI development with less labeled data, shorter\ntraining times, and reduced computational costs compared to traditional\napproaches. In addition, we utilize a common interface and style across these\nmodels, and prioritize usability to enable developers to integrate HAI-DEF\nefficiently. We present model evaluations across various tasks and conclude\nwith a discussion of their application and evaluation, covering the importance\nof ensuring efficacy, fairness, and equity. Finally, while HAI-DEF and\nspecifically the foundation models lower the barrier to entry for ML in\nhealthcare, we emphasize the importance of validation with problem- and\npopulation-specific data for each desired usage setting. This technical report\nwill be updated over time as more modalities and features are added.\n", "link": "http://arxiv.org/abs/2411.15128v1", "date": "2024-11-22", "relevancy": 1.9588, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.4926}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.4926}, {"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.4752}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Health%20AI%20Developer%20Foundations&body=Title%3A%20Health%20AI%20Developer%20Foundations%0AAuthor%3A%20Atilla%20P.%20Kiraly%20and%20Sebastien%20Baur%20and%20Kenneth%20Philbrick%20and%20Fereshteh%20Mahvar%20and%20Liron%20Yatziv%20and%20Tiffany%20Chen%20and%20Bram%20Sterling%20and%20Nick%20George%20and%20Fayaz%20Jamil%20and%20Jing%20Tang%20and%20Kai%20Bailey%20and%20Faruk%20Ahmed%20and%20Akshay%20Goel%20and%20Abbi%20Ward%20and%20Lin%20Yang%20and%20Andrew%20Sellergren%20and%20Yossi%20Matias%20and%20Avinatan%20Hassidim%20and%20Shravya%20Shetty%20and%20Daniel%20Golden%20and%20Shekoofeh%20Azizi%20and%20David%20F.%20Steiner%20and%20Yun%20Liu%20and%20Tim%20Thelin%20and%20Rory%20Pilgrim%20and%20Can%20Kirmizibayrak%0AAbstract%3A%20%20%20Robust%20medical%20Machine%20Learning%20%28ML%29%20models%20have%20the%20potential%20to%0Arevolutionize%20healthcare%20by%20accelerating%20clinical%20research%2C%20improving%20workflows%0Aand%20outcomes%2C%20and%20producing%20novel%20insights%20or%20capabilities.%20Developing%20such%20ML%0Amodels%20from%20scratch%20is%20cost%20prohibitive%20and%20requires%20substantial%20compute%2C%20data%2C%0Aand%20time%20%28e.g.%2C%20expert%20labeling%29.%20To%20address%20these%20challenges%2C%20we%20introduce%0AHealth%20AI%20Developer%20Foundations%20%28HAI-DEF%29%2C%20a%20suite%20of%20pre-trained%2C%0Adomain-specific%20foundation%20models%2C%20tools%2C%20and%20recipes%20to%20accelerate%20building%20ML%0Afor%20health%20applications.%20The%20models%20cover%20various%20modalities%20and%20domains%2C%0Aincluding%20radiology%20%28X-rays%20and%20computed%20tomography%29%2C%20histopathology%2C%0Adermatological%20imaging%2C%20and%20audio.%20These%20models%20provide%20domain%20specific%0Aembeddings%20that%20facilitate%20AI%20development%20with%20less%20labeled%20data%2C%20shorter%0Atraining%20times%2C%20and%20reduced%20computational%20costs%20compared%20to%20traditional%0Aapproaches.%20In%20addition%2C%20we%20utilize%20a%20common%20interface%20and%20style%20across%20these%0Amodels%2C%20and%20prioritize%20usability%20to%20enable%20developers%20to%20integrate%20HAI-DEF%0Aefficiently.%20We%20present%20model%20evaluations%20across%20various%20tasks%20and%20conclude%0Awith%20a%20discussion%20of%20their%20application%20and%20evaluation%2C%20covering%20the%20importance%0Aof%20ensuring%20efficacy%2C%20fairness%2C%20and%20equity.%20Finally%2C%20while%20HAI-DEF%20and%0Aspecifically%20the%20foundation%20models%20lower%20the%20barrier%20to%20entry%20for%20ML%20in%0Ahealthcare%2C%20we%20emphasize%20the%20importance%20of%20validation%20with%20problem-%20and%0Apopulation-specific%20data%20for%20each%20desired%20usage%20setting.%20This%20technical%20report%0Awill%20be%20updated%20over%20time%20as%20more%20modalities%20and%20features%20are%20added.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2411.15128v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DHealth%2520AI%2520Developer%2520Foundations%26entry.906535625%3DAtilla%2520P.%2520Kiraly%2520and%2520Sebastien%2520Baur%2520and%2520Kenneth%2520Philbrick%2520and%2520Fereshteh%2520Mahvar%2520and%2520Liron%2520Yatziv%2520and%2520Tiffany%2520Chen%2520and%2520Bram%2520Sterling%2520and%2520Nick%2520George%2520and%2520Fayaz%2520Jamil%2520and%2520Jing%2520Tang%2520and%2520Kai%2520Bailey%2520and%2520Faruk%2520Ahmed%2520and%2520Akshay%2520Goel%2520and%2520Abbi%2520Ward%2520and%2520Lin%2520Yang%2520and%2520Andrew%2520Sellergren%2520and%2520Yossi%2520Matias%2520and%2520Avinatan%2520Hassidim%2520and%2520Shravya%2520Shetty%2520and%2520Daniel%2520Golden%2520and%2520Shekoofeh%2520Azizi%2520and%2520David%2520F.%2520Steiner%2520and%2520Yun%2520Liu%2520and%2520Tim%2520Thelin%2520and%2520Rory%2520Pilgrim%2520and%2520Can%2520Kirmizibayrak%26entry.1292438233%3D%2520%2520Robust%2520medical%2520Machine%2520Learning%2520%2528ML%2529%2520models%2520have%2520the%2520potential%2520to%250Arevolutionize%2520healthcare%2520by%2520accelerating%2520clinical%2520research%252C%2520improving%2520workflows%250Aand%2520outcomes%252C%2520and%2520producing%2520novel%2520insights%2520or%2520capabilities.%2520Developing%2520such%2520ML%250Amodels%2520from%2520scratch%2520is%2520cost%2520prohibitive%2520and%2520requires%2520substantial%2520compute%252C%2520data%252C%250Aand%2520time%2520%2528e.g.%252C%2520expert%2520labeling%2529.%2520To%2520address%2520these%2520challenges%252C%2520we%2520introduce%250AHealth%2520AI%2520Developer%2520Foundations%2520%2528HAI-DEF%2529%252C%2520a%2520suite%2520of%2520pre-trained%252C%250Adomain-specific%2520foundation%2520models%252C%2520tools%252C%2520and%2520recipes%2520to%2520accelerate%2520building%2520ML%250Afor%2520health%2520applications.%2520The%2520models%2520cover%2520various%2520modalities%2520and%2520domains%252C%250Aincluding%2520radiology%2520%2528X-rays%2520and%2520computed%2520tomography%2529%252C%2520histopathology%252C%250Adermatological%2520imaging%252C%2520and%2520audio.%2520These%2520models%2520provide%2520domain%2520specific%250Aembeddings%2520that%2520facilitate%2520AI%2520development%2520with%2520less%2520labeled%2520data%252C%2520shorter%250Atraining%2520times%252C%2520and%2520reduced%2520computational%2520costs%2520compared%2520to%2520traditional%250Aapproaches.%2520In%2520addition%252C%2520we%2520utilize%2520a%2520common%2520interface%2520and%2520style%2520across%2520these%250Amodels%252C%2520and%2520prioritize%2520usability%2520to%2520enable%2520developers%2520to%2520integrate%2520HAI-DEF%250Aefficiently.%2520We%2520present%2520model%2520evaluations%2520across%2520various%2520tasks%2520and%2520conclude%250Awith%2520a%2520discussion%2520of%2520their%2520application%2520and%2520evaluation%252C%2520covering%2520the%2520importance%250Aof%2520ensuring%2520efficacy%252C%2520fairness%252C%2520and%2520equity.%2520Finally%252C%2520while%2520HAI-DEF%2520and%250Aspecifically%2520the%2520foundation%2520models%2520lower%2520the%2520barrier%2520to%2520entry%2520for%2520ML%2520in%250Ahealthcare%252C%2520we%2520emphasize%2520the%2520importance%2520of%2520validation%2520with%2520problem-%2520and%250Apopulation-specific%2520data%2520for%2520each%2520desired%2520usage%2520setting.%2520This%2520technical%2520report%250Awill%2520be%2520updated%2520over%2520time%2520as%2520more%2520modalities%2520and%2520features%2520are%2520added.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2411.15128v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Health%20AI%20Developer%20Foundations&entry.906535625=Atilla%20P.%20Kiraly%20and%20Sebastien%20Baur%20and%20Kenneth%20Philbrick%20and%20Fereshteh%20Mahvar%20and%20Liron%20Yatziv%20and%20Tiffany%20Chen%20and%20Bram%20Sterling%20and%20Nick%20George%20and%20Fayaz%20Jamil%20and%20Jing%20Tang%20and%20Kai%20Bailey%20and%20Faruk%20Ahmed%20and%20Akshay%20Goel%20and%20Abbi%20Ward%20and%20Lin%20Yang%20and%20Andrew%20Sellergren%20and%20Yossi%20Matias%20and%20Avinatan%20Hassidim%20and%20Shravya%20Shetty%20and%20Daniel%20Golden%20and%20Shekoofeh%20Azizi%20and%20David%20F.%20Steiner%20and%20Yun%20Liu%20and%20Tim%20Thelin%20and%20Rory%20Pilgrim%20and%20Can%20Kirmizibayrak&entry.1292438233=%20%20Robust%20medical%20Machine%20Learning%20%28ML%29%20models%20have%20the%20potential%20to%0Arevolutionize%20healthcare%20by%20accelerating%20clinical%20research%2C%20improving%20workflows%0Aand%20outcomes%2C%20and%20producing%20novel%20insights%20or%20capabilities.%20Developing%20such%20ML%0Amodels%20from%20scratch%20is%20cost%20prohibitive%20and%20requires%20substantial%20compute%2C%20data%2C%0Aand%20time%20%28e.g.%2C%20expert%20labeling%29.%20To%20address%20these%20challenges%2C%20we%20introduce%0AHealth%20AI%20Developer%20Foundations%20%28HAI-DEF%29%2C%20a%20suite%20of%20pre-trained%2C%0Adomain-specific%20foundation%20models%2C%20tools%2C%20and%20recipes%20to%20accelerate%20building%20ML%0Afor%20health%20applications.%20The%20models%20cover%20various%20modalities%20and%20domains%2C%0Aincluding%20radiology%20%28X-rays%20and%20computed%20tomography%29%2C%20histopathology%2C%0Adermatological%20imaging%2C%20and%20audio.%20These%20models%20provide%20domain%20specific%0Aembeddings%20that%20facilitate%20AI%20development%20with%20less%20labeled%20data%2C%20shorter%0Atraining%20times%2C%20and%20reduced%20computational%20costs%20compared%20to%20traditional%0Aapproaches.%20In%20addition%2C%20we%20utilize%20a%20common%20interface%20and%20style%20across%20these%0Amodels%2C%20and%20prioritize%20usability%20to%20enable%20developers%20to%20integrate%20HAI-DEF%0Aefficiently.%20We%20present%20model%20evaluations%20across%20various%20tasks%20and%20conclude%0Awith%20a%20discussion%20of%20their%20application%20and%20evaluation%2C%20covering%20the%20importance%0Aof%20ensuring%20efficacy%2C%20fairness%2C%20and%20equity.%20Finally%2C%20while%20HAI-DEF%20and%0Aspecifically%20the%20foundation%20models%20lower%20the%20barrier%20to%20entry%20for%20ML%20in%0Ahealthcare%2C%20we%20emphasize%20the%20importance%20of%20validation%20with%20problem-%20and%0Apopulation-specific%20data%20for%20each%20desired%20usage%20setting.%20This%20technical%20report%0Awill%20be%20updated%20over%20time%20as%20more%20modalities%20and%20features%20are%20added.%0A&entry.1838667208=http%3A//arxiv.org/abs/2411.15128v1&entry.124074799=Read"},
{"title": "Natural Language Processing RELIES on Linguistics", "author": "Juri Opitz and Shira Wein and Nathan Schneider", "abstract": "  Large Language Models (LLMs) have become capable of generating highly fluent\ntext in certain languages, without modules specially designed to capture\ngrammar or semantic coherence. What does this mean for the future of linguistic\nexpertise in NLP? We highlight several aspects in which NLP (still) relies on\nlinguistics, or where linguistic thinking can illuminate new directions. We\nargue our case around the acronym RELIES that encapsulates six major facets\nwhere linguistics contributes to NLP: Resources, Evaluation, Low-resource\nsettings, Interpretability, Explanation, and the Study of language. This list\nis not exhaustive, nor is linguistics the main point of reference for every\neffort under these themes; but at a macro level, these facets highlight the\nenduring importance of studying machine systems vis-\\`a-vis systems of human\nlanguage.\n", "link": "http://arxiv.org/abs/2405.05966v3", "date": "2024-11-22", "relevancy": 1.9585, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5086}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5086}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.3948}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Natural%20Language%20Processing%20RELIES%20on%20Linguistics&body=Title%3A%20Natural%20Language%20Processing%20RELIES%20on%20Linguistics%0AAuthor%3A%20Juri%20Opitz%20and%20Shira%20Wein%20and%20Nathan%20Schneider%0AAbstract%3A%20%20%20Large%20Language%20Models%20%28LLMs%29%20have%20become%20capable%20of%20generating%20highly%20fluent%0Atext%20in%20certain%20languages%2C%20without%20modules%20specially%20designed%20to%20capture%0Agrammar%20or%20semantic%20coherence.%20What%20does%20this%20mean%20for%20the%20future%20of%20linguistic%0Aexpertise%20in%20NLP%3F%20We%20highlight%20several%20aspects%20in%20which%20NLP%20%28still%29%20relies%20on%0Alinguistics%2C%20or%20where%20linguistic%20thinking%20can%20illuminate%20new%20directions.%20We%0Aargue%20our%20case%20around%20the%20acronym%20RELIES%20that%20encapsulates%20six%20major%20facets%0Awhere%20linguistics%20contributes%20to%20NLP%3A%20Resources%2C%20Evaluation%2C%20Low-resource%0Asettings%2C%20Interpretability%2C%20Explanation%2C%20and%20the%20Study%20of%20language.%20This%20list%0Ais%20not%20exhaustive%2C%20nor%20is%20linguistics%20the%20main%20point%20of%20reference%20for%20every%0Aeffort%20under%20these%20themes%3B%20but%20at%20a%20macro%20level%2C%20these%20facets%20highlight%20the%0Aenduring%20importance%20of%20studying%20machine%20systems%20vis-%5C%60a-vis%20systems%20of%20human%0Alanguage.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2405.05966v3%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DNatural%2520Language%2520Processing%2520RELIES%2520on%2520Linguistics%26entry.906535625%3DJuri%2520Opitz%2520and%2520Shira%2520Wein%2520and%2520Nathan%2520Schneider%26entry.1292438233%3D%2520%2520Large%2520Language%2520Models%2520%2528LLMs%2529%2520have%2520become%2520capable%2520of%2520generating%2520highly%2520fluent%250Atext%2520in%2520certain%2520languages%252C%2520without%2520modules%2520specially%2520designed%2520to%2520capture%250Agrammar%2520or%2520semantic%2520coherence.%2520What%2520does%2520this%2520mean%2520for%2520the%2520future%2520of%2520linguistic%250Aexpertise%2520in%2520NLP%253F%2520We%2520highlight%2520several%2520aspects%2520in%2520which%2520NLP%2520%2528still%2529%2520relies%2520on%250Alinguistics%252C%2520or%2520where%2520linguistic%2520thinking%2520can%2520illuminate%2520new%2520directions.%2520We%250Aargue%2520our%2520case%2520around%2520the%2520acronym%2520RELIES%2520that%2520encapsulates%2520six%2520major%2520facets%250Awhere%2520linguistics%2520contributes%2520to%2520NLP%253A%2520Resources%252C%2520Evaluation%252C%2520Low-resource%250Asettings%252C%2520Interpretability%252C%2520Explanation%252C%2520and%2520the%2520Study%2520of%2520language.%2520This%2520list%250Ais%2520not%2520exhaustive%252C%2520nor%2520is%2520linguistics%2520the%2520main%2520point%2520of%2520reference%2520for%2520every%250Aeffort%2520under%2520these%2520themes%253B%2520but%2520at%2520a%2520macro%2520level%252C%2520these%2520facets%2520highlight%2520the%250Aenduring%2520importance%2520of%2520studying%2520machine%2520systems%2520vis-%255C%2560a-vis%2520systems%2520of%2520human%250Alanguage.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2405.05966v3%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Natural%20Language%20Processing%20RELIES%20on%20Linguistics&entry.906535625=Juri%20Opitz%20and%20Shira%20Wein%20and%20Nathan%20Schneider&entry.1292438233=%20%20Large%20Language%20Models%20%28LLMs%29%20have%20become%20capable%20of%20generating%20highly%20fluent%0Atext%20in%20certain%20languages%2C%20without%20modules%20specially%20designed%20to%20capture%0Agrammar%20or%20semantic%20coherence.%20What%20does%20this%20mean%20for%20the%20future%20of%20linguistic%0Aexpertise%20in%20NLP%3F%20We%20highlight%20several%20aspects%20in%20which%20NLP%20%28still%29%20relies%20on%0Alinguistics%2C%20or%20where%20linguistic%20thinking%20can%20illuminate%20new%20directions.%20We%0Aargue%20our%20case%20around%20the%20acronym%20RELIES%20that%20encapsulates%20six%20major%20facets%0Awhere%20linguistics%20contributes%20to%20NLP%3A%20Resources%2C%20Evaluation%2C%20Low-resource%0Asettings%2C%20Interpretability%2C%20Explanation%2C%20and%20the%20Study%20of%20language.%20This%20list%0Ais%20not%20exhaustive%2C%20nor%20is%20linguistics%20the%20main%20point%20of%20reference%20for%20every%0Aeffort%20under%20these%20themes%3B%20but%20at%20a%20macro%20level%2C%20these%20facets%20highlight%20the%0Aenduring%20importance%20of%20studying%20machine%20systems%20vis-%5C%60a-vis%20systems%20of%20human%0Alanguage.%0A&entry.1838667208=http%3A//arxiv.org/abs/2405.05966v3&entry.124074799=Read"},
{"title": "FloAt: Flow Warping of Self-Attention for Clothing Animation Generation", "author": "Swasti Shreya Mishra and Kuldeep Kulkarni and Duygu Ceylan and Balaji Vasan Srinivasan", "abstract": "  We propose a diffusion model-based approach, FloAtControlNet to generate\ncinemagraphs composed of animations of human clothing. We focus on human\nclothing like dresses, skirts and pants. The input to our model is a text\nprompt depicting the type of clothing and the texture of clothing like leopard,\nstriped, or plain, and a sequence of normal maps that capture the underlying\nanimation that we desire in the output. The backbone of our method is a\nnormal-map conditioned ControlNet which is operated in a training-free regime.\nThe key observation is that the underlying animation is embedded in the flow of\nthe normal maps. We utilize the flow thus obtained to manipulate the\nself-attention maps of appropriate layers. Specifically, the self-attention\nmaps of a particular layer and frame are recomputed as a linear combination of\nitself and the self-attention maps of the same layer and the previous frame,\nwarped by the flow on the normal maps of the two frames. We show that\nmanipulating the self-attention maps greatly enhances the quality of the\nclothing animation, making it look more natural as well as suppressing the\nbackground artifacts. Through extensive experiments, we show that the method\nproposed beats all baselines both qualitatively in terms of visual results and\nuser study. Specifically, our method is able to alleviate the background\nflickering that exists in other diffusion model-based baselines that we\nconsider. In addition, we show that our method beats all baselines in terms of\nRMSE and PSNR computed using the input normal map sequences and the normal map\nsequences obtained from the output RGB frames. Further, we show that\nwell-established evaluation metrics like LPIPS, SSIM, and CLIP scores that are\ngenerally for visual quality are not necessarily suitable for capturing the\nsubtle motions in human clothing animations.\n", "link": "http://arxiv.org/abs/2411.15028v1", "date": "2024-11-22", "relevancy": 1.9549, "topK": [{"title": "FabricDiffusion: High-Fidelity Texture Transfer for 3D Garments\n  Generation from In-The-Wild Clothing Images", "link": "http://arxiv.org/abs/2410.01801v1", "similarity": 0.6674}, {"title": "Multimodal-Conditioned Latent Diffusion Models for Fashion Image Editing", "link": "http://arxiv.org/abs/2403.14828v2", "similarity": 0.6628}, {"title": "Scaling Rectified Flow Transformers for High-Resolution Image Synthesis", "link": "http://arxiv.org/abs/2403.03206v1", "similarity": 0.601}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20FloAt%3A%20Flow%20Warping%20of%20Self-Attention%20for%20Clothing%20Animation%20Generation&body=Title%3A%20FloAt%3A%20Flow%20Warping%20of%20Self-Attention%20for%20Clothing%20Animation%20Generation%0AAuthor%3A%20Swasti%20Shreya%20Mishra%20and%20Kuldeep%20Kulkarni%20and%20Duygu%20Ceylan%20and%20Balaji%20Vasan%20Srinivasan%0AAbstract%3A%20%20%20We%20propose%20a%20diffusion%20model-based%20approach%2C%20FloAtControlNet%20to%20generate%0Acinemagraphs%20composed%20of%20animations%20of%20human%20clothing.%20We%20focus%20on%20human%0Aclothing%20like%20dresses%2C%20skirts%20and%20pants.%20The%20input%20to%20our%20model%20is%20a%20text%0Aprompt%20depicting%20the%20type%20of%20clothing%20and%20the%20texture%20of%20clothing%20like%20leopard%2C%0Astriped%2C%20or%20plain%2C%20and%20a%20sequence%20of%20normal%20maps%20that%20capture%20the%20underlying%0Aanimation%20that%20we%20desire%20in%20the%20output.%20The%20backbone%20of%20our%20method%20is%20a%0Anormal-map%20conditioned%20ControlNet%20which%20is%20operated%20in%20a%20training-free%20regime.%0AThe%20key%20observation%20is%20that%20the%20underlying%20animation%20is%20embedded%20in%20the%20flow%20of%0Athe%20normal%20maps.%20We%20utilize%20the%20flow%20thus%20obtained%20to%20manipulate%20the%0Aself-attention%20maps%20of%20appropriate%20layers.%20Specifically%2C%20the%20self-attention%0Amaps%20of%20a%20particular%20layer%20and%20frame%20are%20recomputed%20as%20a%20linear%20combination%20of%0Aitself%20and%20the%20self-attention%20maps%20of%20the%20same%20layer%20and%20the%20previous%20frame%2C%0Awarped%20by%20the%20flow%20on%20the%20normal%20maps%20of%20the%20two%20frames.%20We%20show%20that%0Amanipulating%20the%20self-attention%20maps%20greatly%20enhances%20the%20quality%20of%20the%0Aclothing%20animation%2C%20making%20it%20look%20more%20natural%20as%20well%20as%20suppressing%20the%0Abackground%20artifacts.%20Through%20extensive%20experiments%2C%20we%20show%20that%20the%20method%0Aproposed%20beats%20all%20baselines%20both%20qualitatively%20in%20terms%20of%20visual%20results%20and%0Auser%20study.%20Specifically%2C%20our%20method%20is%20able%20to%20alleviate%20the%20background%0Aflickering%20that%20exists%20in%20other%20diffusion%20model-based%20baselines%20that%20we%0Aconsider.%20In%20addition%2C%20we%20show%20that%20our%20method%20beats%20all%20baselines%20in%20terms%20of%0ARMSE%20and%20PSNR%20computed%20using%20the%20input%20normal%20map%20sequences%20and%20the%20normal%20map%0Asequences%20obtained%20from%20the%20output%20RGB%20frames.%20Further%2C%20we%20show%20that%0Awell-established%20evaluation%20metrics%20like%20LPIPS%2C%20SSIM%2C%20and%20CLIP%20scores%20that%20are%0Agenerally%20for%20visual%20quality%20are%20not%20necessarily%20suitable%20for%20capturing%20the%0Asubtle%20motions%20in%20human%20clothing%20animations.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2411.15028v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DFloAt%253A%2520Flow%2520Warping%2520of%2520Self-Attention%2520for%2520Clothing%2520Animation%2520Generation%26entry.906535625%3DSwasti%2520Shreya%2520Mishra%2520and%2520Kuldeep%2520Kulkarni%2520and%2520Duygu%2520Ceylan%2520and%2520Balaji%2520Vasan%2520Srinivasan%26entry.1292438233%3D%2520%2520We%2520propose%2520a%2520diffusion%2520model-based%2520approach%252C%2520FloAtControlNet%2520to%2520generate%250Acinemagraphs%2520composed%2520of%2520animations%2520of%2520human%2520clothing.%2520We%2520focus%2520on%2520human%250Aclothing%2520like%2520dresses%252C%2520skirts%2520and%2520pants.%2520The%2520input%2520to%2520our%2520model%2520is%2520a%2520text%250Aprompt%2520depicting%2520the%2520type%2520of%2520clothing%2520and%2520the%2520texture%2520of%2520clothing%2520like%2520leopard%252C%250Astriped%252C%2520or%2520plain%252C%2520and%2520a%2520sequence%2520of%2520normal%2520maps%2520that%2520capture%2520the%2520underlying%250Aanimation%2520that%2520we%2520desire%2520in%2520the%2520output.%2520The%2520backbone%2520of%2520our%2520method%2520is%2520a%250Anormal-map%2520conditioned%2520ControlNet%2520which%2520is%2520operated%2520in%2520a%2520training-free%2520regime.%250AThe%2520key%2520observation%2520is%2520that%2520the%2520underlying%2520animation%2520is%2520embedded%2520in%2520the%2520flow%2520of%250Athe%2520normal%2520maps.%2520We%2520utilize%2520the%2520flow%2520thus%2520obtained%2520to%2520manipulate%2520the%250Aself-attention%2520maps%2520of%2520appropriate%2520layers.%2520Specifically%252C%2520the%2520self-attention%250Amaps%2520of%2520a%2520particular%2520layer%2520and%2520frame%2520are%2520recomputed%2520as%2520a%2520linear%2520combination%2520of%250Aitself%2520and%2520the%2520self-attention%2520maps%2520of%2520the%2520same%2520layer%2520and%2520the%2520previous%2520frame%252C%250Awarped%2520by%2520the%2520flow%2520on%2520the%2520normal%2520maps%2520of%2520the%2520two%2520frames.%2520We%2520show%2520that%250Amanipulating%2520the%2520self-attention%2520maps%2520greatly%2520enhances%2520the%2520quality%2520of%2520the%250Aclothing%2520animation%252C%2520making%2520it%2520look%2520more%2520natural%2520as%2520well%2520as%2520suppressing%2520the%250Abackground%2520artifacts.%2520Through%2520extensive%2520experiments%252C%2520we%2520show%2520that%2520the%2520method%250Aproposed%2520beats%2520all%2520baselines%2520both%2520qualitatively%2520in%2520terms%2520of%2520visual%2520results%2520and%250Auser%2520study.%2520Specifically%252C%2520our%2520method%2520is%2520able%2520to%2520alleviate%2520the%2520background%250Aflickering%2520that%2520exists%2520in%2520other%2520diffusion%2520model-based%2520baselines%2520that%2520we%250Aconsider.%2520In%2520addition%252C%2520we%2520show%2520that%2520our%2520method%2520beats%2520all%2520baselines%2520in%2520terms%2520of%250ARMSE%2520and%2520PSNR%2520computed%2520using%2520the%2520input%2520normal%2520map%2520sequences%2520and%2520the%2520normal%2520map%250Asequences%2520obtained%2520from%2520the%2520output%2520RGB%2520frames.%2520Further%252C%2520we%2520show%2520that%250Awell-established%2520evaluation%2520metrics%2520like%2520LPIPS%252C%2520SSIM%252C%2520and%2520CLIP%2520scores%2520that%2520are%250Agenerally%2520for%2520visual%2520quality%2520are%2520not%2520necessarily%2520suitable%2520for%2520capturing%2520the%250Asubtle%2520motions%2520in%2520human%2520clothing%2520animations.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2411.15028v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=FloAt%3A%20Flow%20Warping%20of%20Self-Attention%20for%20Clothing%20Animation%20Generation&entry.906535625=Swasti%20Shreya%20Mishra%20and%20Kuldeep%20Kulkarni%20and%20Duygu%20Ceylan%20and%20Balaji%20Vasan%20Srinivasan&entry.1292438233=%20%20We%20propose%20a%20diffusion%20model-based%20approach%2C%20FloAtControlNet%20to%20generate%0Acinemagraphs%20composed%20of%20animations%20of%20human%20clothing.%20We%20focus%20on%20human%0Aclothing%20like%20dresses%2C%20skirts%20and%20pants.%20The%20input%20to%20our%20model%20is%20a%20text%0Aprompt%20depicting%20the%20type%20of%20clothing%20and%20the%20texture%20of%20clothing%20like%20leopard%2C%0Astriped%2C%20or%20plain%2C%20and%20a%20sequence%20of%20normal%20maps%20that%20capture%20the%20underlying%0Aanimation%20that%20we%20desire%20in%20the%20output.%20The%20backbone%20of%20our%20method%20is%20a%0Anormal-map%20conditioned%20ControlNet%20which%20is%20operated%20in%20a%20training-free%20regime.%0AThe%20key%20observation%20is%20that%20the%20underlying%20animation%20is%20embedded%20in%20the%20flow%20of%0Athe%20normal%20maps.%20We%20utilize%20the%20flow%20thus%20obtained%20to%20manipulate%20the%0Aself-attention%20maps%20of%20appropriate%20layers.%20Specifically%2C%20the%20self-attention%0Amaps%20of%20a%20particular%20layer%20and%20frame%20are%20recomputed%20as%20a%20linear%20combination%20of%0Aitself%20and%20the%20self-attention%20maps%20of%20the%20same%20layer%20and%20the%20previous%20frame%2C%0Awarped%20by%20the%20flow%20on%20the%20normal%20maps%20of%20the%20two%20frames.%20We%20show%20that%0Amanipulating%20the%20self-attention%20maps%20greatly%20enhances%20the%20quality%20of%20the%0Aclothing%20animation%2C%20making%20it%20look%20more%20natural%20as%20well%20as%20suppressing%20the%0Abackground%20artifacts.%20Through%20extensive%20experiments%2C%20we%20show%20that%20the%20method%0Aproposed%20beats%20all%20baselines%20both%20qualitatively%20in%20terms%20of%20visual%20results%20and%0Auser%20study.%20Specifically%2C%20our%20method%20is%20able%20to%20alleviate%20the%20background%0Aflickering%20that%20exists%20in%20other%20diffusion%20model-based%20baselines%20that%20we%0Aconsider.%20In%20addition%2C%20we%20show%20that%20our%20method%20beats%20all%20baselines%20in%20terms%20of%0ARMSE%20and%20PSNR%20computed%20using%20the%20input%20normal%20map%20sequences%20and%20the%20normal%20map%0Asequences%20obtained%20from%20the%20output%20RGB%20frames.%20Further%2C%20we%20show%20that%0Awell-established%20evaluation%20metrics%20like%20LPIPS%2C%20SSIM%2C%20and%20CLIP%20scores%20that%20are%0Agenerally%20for%20visual%20quality%20are%20not%20necessarily%20suitable%20for%20capturing%20the%0Asubtle%20motions%20in%20human%20clothing%20animations.%0A&entry.1838667208=http%3A//arxiv.org/abs/2411.15028v1&entry.124074799=Read"},
{"title": "Utilizing Large Language Models to Synthesize Product Desirability\n  Datasets", "author": "John D. Hastings and Sherri Weitl-Harms and Joseph Doty and Zachary J. Myers and Warren Thompson", "abstract": "  This research explores the application of large language models (LLMs) to\ngenerate synthetic datasets for Product Desirability Toolkit (PDT) testing, a\nkey component in evaluating user sentiment and product experience. Utilizing\ngpt-4o-mini, a cost-effective alternative to larger commercial LLMs, three\nmethods, Word+Review, Review+Word, and Supply-Word, were each used to\nsynthesize 1000 product reviews. The generated datasets were assessed for\nsentiment alignment, textual diversity, and data generation cost. Results\ndemonstrated high sentiment alignment across all methods, with Pearson\ncorrelations ranging from 0.93 to 0.97. Supply-Word exhibited the highest\ndiversity and coverage of PDT terms, although with increased generation costs.\nDespite minor biases toward positive sentiments, in situations with limited\ntest data, LLM-generated synthetic data offers significant advantages,\nincluding scalability, cost savings, and flexibility in dataset production.\n", "link": "http://arxiv.org/abs/2411.13485v2", "date": "2024-11-22", "relevancy": 1.9543, "topK": [{"title": "VirtualModel: Generating Object-ID-retentive Human-object Interaction\n  Image by Diffusion Model for E-commerce Marketing", "link": "http://arxiv.org/abs/2405.09985v1", "similarity": 0.5001}, {"title": "Multimodal-Conditioned Latent Diffusion Models for Fashion Image Editing", "link": "http://arxiv.org/abs/2403.14828v2", "similarity": 0.4947}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.4746}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Utilizing%20Large%20Language%20Models%20to%20Synthesize%20Product%20Desirability%0A%20%20Datasets&body=Title%3A%20Utilizing%20Large%20Language%20Models%20to%20Synthesize%20Product%20Desirability%0A%20%20Datasets%0AAuthor%3A%20John%20D.%20Hastings%20and%20Sherri%20Weitl-Harms%20and%20Joseph%20Doty%20and%20Zachary%20J.%20Myers%20and%20Warren%20Thompson%0AAbstract%3A%20%20%20This%20research%20explores%20the%20application%20of%20large%20language%20models%20%28LLMs%29%20to%0Agenerate%20synthetic%20datasets%20for%20Product%20Desirability%20Toolkit%20%28PDT%29%20testing%2C%20a%0Akey%20component%20in%20evaluating%20user%20sentiment%20and%20product%20experience.%20Utilizing%0Agpt-4o-mini%2C%20a%20cost-effective%20alternative%20to%20larger%20commercial%20LLMs%2C%20three%0Amethods%2C%20Word%2BReview%2C%20Review%2BWord%2C%20and%20Supply-Word%2C%20were%20each%20used%20to%0Asynthesize%201000%20product%20reviews.%20The%20generated%20datasets%20were%20assessed%20for%0Asentiment%20alignment%2C%20textual%20diversity%2C%20and%20data%20generation%20cost.%20Results%0Ademonstrated%20high%20sentiment%20alignment%20across%20all%20methods%2C%20with%20Pearson%0Acorrelations%20ranging%20from%200.93%20to%200.97.%20Supply-Word%20exhibited%20the%20highest%0Adiversity%20and%20coverage%20of%20PDT%20terms%2C%20although%20with%20increased%20generation%20costs.%0ADespite%20minor%20biases%20toward%20positive%20sentiments%2C%20in%20situations%20with%20limited%0Atest%20data%2C%20LLM-generated%20synthetic%20data%20offers%20significant%20advantages%2C%0Aincluding%20scalability%2C%20cost%20savings%2C%20and%20flexibility%20in%20dataset%20production.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2411.13485v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DUtilizing%2520Large%2520Language%2520Models%2520to%2520Synthesize%2520Product%2520Desirability%250A%2520%2520Datasets%26entry.906535625%3DJohn%2520D.%2520Hastings%2520and%2520Sherri%2520Weitl-Harms%2520and%2520Joseph%2520Doty%2520and%2520Zachary%2520J.%2520Myers%2520and%2520Warren%2520Thompson%26entry.1292438233%3D%2520%2520This%2520research%2520explores%2520the%2520application%2520of%2520large%2520language%2520models%2520%2528LLMs%2529%2520to%250Agenerate%2520synthetic%2520datasets%2520for%2520Product%2520Desirability%2520Toolkit%2520%2528PDT%2529%2520testing%252C%2520a%250Akey%2520component%2520in%2520evaluating%2520user%2520sentiment%2520and%2520product%2520experience.%2520Utilizing%250Agpt-4o-mini%252C%2520a%2520cost-effective%2520alternative%2520to%2520larger%2520commercial%2520LLMs%252C%2520three%250Amethods%252C%2520Word%252BReview%252C%2520Review%252BWord%252C%2520and%2520Supply-Word%252C%2520were%2520each%2520used%2520to%250Asynthesize%25201000%2520product%2520reviews.%2520The%2520generated%2520datasets%2520were%2520assessed%2520for%250Asentiment%2520alignment%252C%2520textual%2520diversity%252C%2520and%2520data%2520generation%2520cost.%2520Results%250Ademonstrated%2520high%2520sentiment%2520alignment%2520across%2520all%2520methods%252C%2520with%2520Pearson%250Acorrelations%2520ranging%2520from%25200.93%2520to%25200.97.%2520Supply-Word%2520exhibited%2520the%2520highest%250Adiversity%2520and%2520coverage%2520of%2520PDT%2520terms%252C%2520although%2520with%2520increased%2520generation%2520costs.%250ADespite%2520minor%2520biases%2520toward%2520positive%2520sentiments%252C%2520in%2520situations%2520with%2520limited%250Atest%2520data%252C%2520LLM-generated%2520synthetic%2520data%2520offers%2520significant%2520advantages%252C%250Aincluding%2520scalability%252C%2520cost%2520savings%252C%2520and%2520flexibility%2520in%2520dataset%2520production.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2411.13485v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Utilizing%20Large%20Language%20Models%20to%20Synthesize%20Product%20Desirability%0A%20%20Datasets&entry.906535625=John%20D.%20Hastings%20and%20Sherri%20Weitl-Harms%20and%20Joseph%20Doty%20and%20Zachary%20J.%20Myers%20and%20Warren%20Thompson&entry.1292438233=%20%20This%20research%20explores%20the%20application%20of%20large%20language%20models%20%28LLMs%29%20to%0Agenerate%20synthetic%20datasets%20for%20Product%20Desirability%20Toolkit%20%28PDT%29%20testing%2C%20a%0Akey%20component%20in%20evaluating%20user%20sentiment%20and%20product%20experience.%20Utilizing%0Agpt-4o-mini%2C%20a%20cost-effective%20alternative%20to%20larger%20commercial%20LLMs%2C%20three%0Amethods%2C%20Word%2BReview%2C%20Review%2BWord%2C%20and%20Supply-Word%2C%20were%20each%20used%20to%0Asynthesize%201000%20product%20reviews.%20The%20generated%20datasets%20were%20assessed%20for%0Asentiment%20alignment%2C%20textual%20diversity%2C%20and%20data%20generation%20cost.%20Results%0Ademonstrated%20high%20sentiment%20alignment%20across%20all%20methods%2C%20with%20Pearson%0Acorrelations%20ranging%20from%200.93%20to%200.97.%20Supply-Word%20exhibited%20the%20highest%0Adiversity%20and%20coverage%20of%20PDT%20terms%2C%20although%20with%20increased%20generation%20costs.%0ADespite%20minor%20biases%20toward%20positive%20sentiments%2C%20in%20situations%20with%20limited%0Atest%20data%2C%20LLM-generated%20synthetic%20data%20offers%20significant%20advantages%2C%0Aincluding%20scalability%2C%20cost%20savings%2C%20and%20flexibility%20in%20dataset%20production.%0A&entry.1838667208=http%3A//arxiv.org/abs/2411.13485v2&entry.124074799=Read"},
{"title": "Data Diet: Can Trimming PET/CT Datasets Enhance Lesion Segmentation?", "author": "Alexander Jaus and Simon Rei\u00df and Jens Kleesiek and Rainer Stiefelhagen", "abstract": "  In this work, we describe our approach to compete in the autoPET3 datacentric\ntrack. While conventional wisdom suggests that larger datasets lead to better\nmodel performance, recent studies indicate that excluding certain training\nsamples can enhance model accuracy. We find that in the autoPETIII dataset, a\nmodel that is trained on the entire dataset exhibits undesirable\ncharacteristics by producing a large number of false positives particularly for\nPSMA-PETs. We counteract this by removing the easiest samples from the training\ndataset as measured by the model loss before retraining from scratch. Using the\nproposed approach we manage to drive down the false negative volume and improve\nupon the baseline model in both false negative volume and dice score on the\npreliminary test set. Code and pre-trained models are available at\ngithub.com/alexanderjaus/autopet3_datadiet.\n", "link": "http://arxiv.org/abs/2409.13548v4", "date": "2024-11-22", "relevancy": 1.9391, "topK": [{"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.4937}, {"title": "GST: Precise 3D Human Body from a Single Image with Gaussian Splatting\n  Transformers", "link": "http://arxiv.org/abs/2409.04196v1", "similarity": 0.4882}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.4778}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Data%20Diet%3A%20Can%20Trimming%20PET/CT%20Datasets%20Enhance%20Lesion%20Segmentation%3F&body=Title%3A%20Data%20Diet%3A%20Can%20Trimming%20PET/CT%20Datasets%20Enhance%20Lesion%20Segmentation%3F%0AAuthor%3A%20Alexander%20Jaus%20and%20Simon%20Rei%C3%9F%20and%20Jens%20Kleesiek%20and%20Rainer%20Stiefelhagen%0AAbstract%3A%20%20%20In%20this%20work%2C%20we%20describe%20our%20approach%20to%20compete%20in%20the%20autoPET3%20datacentric%0Atrack.%20While%20conventional%20wisdom%20suggests%20that%20larger%20datasets%20lead%20to%20better%0Amodel%20performance%2C%20recent%20studies%20indicate%20that%20excluding%20certain%20training%0Asamples%20can%20enhance%20model%20accuracy.%20We%20find%20that%20in%20the%20autoPETIII%20dataset%2C%20a%0Amodel%20that%20is%20trained%20on%20the%20entire%20dataset%20exhibits%20undesirable%0Acharacteristics%20by%20producing%20a%20large%20number%20of%20false%20positives%20particularly%20for%0APSMA-PETs.%20We%20counteract%20this%20by%20removing%20the%20easiest%20samples%20from%20the%20training%0Adataset%20as%20measured%20by%20the%20model%20loss%20before%20retraining%20from%20scratch.%20Using%20the%0Aproposed%20approach%20we%20manage%20to%20drive%20down%20the%20false%20negative%20volume%20and%20improve%0Aupon%20the%20baseline%20model%20in%20both%20false%20negative%20volume%20and%20dice%20score%20on%20the%0Apreliminary%20test%20set.%20Code%20and%20pre-trained%20models%20are%20available%20at%0Agithub.com/alexanderjaus/autopet3_datadiet.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2409.13548v4%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DData%2520Diet%253A%2520Can%2520Trimming%2520PET/CT%2520Datasets%2520Enhance%2520Lesion%2520Segmentation%253F%26entry.906535625%3DAlexander%2520Jaus%2520and%2520Simon%2520Rei%25C3%259F%2520and%2520Jens%2520Kleesiek%2520and%2520Rainer%2520Stiefelhagen%26entry.1292438233%3D%2520%2520In%2520this%2520work%252C%2520we%2520describe%2520our%2520approach%2520to%2520compete%2520in%2520the%2520autoPET3%2520datacentric%250Atrack.%2520While%2520conventional%2520wisdom%2520suggests%2520that%2520larger%2520datasets%2520lead%2520to%2520better%250Amodel%2520performance%252C%2520recent%2520studies%2520indicate%2520that%2520excluding%2520certain%2520training%250Asamples%2520can%2520enhance%2520model%2520accuracy.%2520We%2520find%2520that%2520in%2520the%2520autoPETIII%2520dataset%252C%2520a%250Amodel%2520that%2520is%2520trained%2520on%2520the%2520entire%2520dataset%2520exhibits%2520undesirable%250Acharacteristics%2520by%2520producing%2520a%2520large%2520number%2520of%2520false%2520positives%2520particularly%2520for%250APSMA-PETs.%2520We%2520counteract%2520this%2520by%2520removing%2520the%2520easiest%2520samples%2520from%2520the%2520training%250Adataset%2520as%2520measured%2520by%2520the%2520model%2520loss%2520before%2520retraining%2520from%2520scratch.%2520Using%2520the%250Aproposed%2520approach%2520we%2520manage%2520to%2520drive%2520down%2520the%2520false%2520negative%2520volume%2520and%2520improve%250Aupon%2520the%2520baseline%2520model%2520in%2520both%2520false%2520negative%2520volume%2520and%2520dice%2520score%2520on%2520the%250Apreliminary%2520test%2520set.%2520Code%2520and%2520pre-trained%2520models%2520are%2520available%2520at%250Agithub.com/alexanderjaus/autopet3_datadiet.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2409.13548v4%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Data%20Diet%3A%20Can%20Trimming%20PET/CT%20Datasets%20Enhance%20Lesion%20Segmentation%3F&entry.906535625=Alexander%20Jaus%20and%20Simon%20Rei%C3%9F%20and%20Jens%20Kleesiek%20and%20Rainer%20Stiefelhagen&entry.1292438233=%20%20In%20this%20work%2C%20we%20describe%20our%20approach%20to%20compete%20in%20the%20autoPET3%20datacentric%0Atrack.%20While%20conventional%20wisdom%20suggests%20that%20larger%20datasets%20lead%20to%20better%0Amodel%20performance%2C%20recent%20studies%20indicate%20that%20excluding%20certain%20training%0Asamples%20can%20enhance%20model%20accuracy.%20We%20find%20that%20in%20the%20autoPETIII%20dataset%2C%20a%0Amodel%20that%20is%20trained%20on%20the%20entire%20dataset%20exhibits%20undesirable%0Acharacteristics%20by%20producing%20a%20large%20number%20of%20false%20positives%20particularly%20for%0APSMA-PETs.%20We%20counteract%20this%20by%20removing%20the%20easiest%20samples%20from%20the%20training%0Adataset%20as%20measured%20by%20the%20model%20loss%20before%20retraining%20from%20scratch.%20Using%20the%0Aproposed%20approach%20we%20manage%20to%20drive%20down%20the%20false%20negative%20volume%20and%20improve%0Aupon%20the%20baseline%20model%20in%20both%20false%20negative%20volume%20and%20dice%20score%20on%20the%0Apreliminary%20test%20set.%20Code%20and%20pre-trained%20models%20are%20available%20at%0Agithub.com/alexanderjaus/autopet3_datadiet.%0A&entry.1838667208=http%3A//arxiv.org/abs/2409.13548v4&entry.124074799=Read"},
{"title": "Efficient Pruning of Text-to-Image Models: Insights from Pruning Stable\n  Diffusion", "author": "Samarth N Ramesh and Zhixue Zhao", "abstract": "  As text-to-image models grow increasingly powerful and complex, their\nburgeoning size presents a significant obstacle to widespread adoption,\nespecially on resource-constrained devices. This paper presents a pioneering\nstudy on post-training pruning of Stable Diffusion 2, addressing the critical\nneed for model compression in text-to-image domain. Our study tackles the\npruning techniques for the previously unexplored multi-modal generation models,\nand particularly examines the pruning impact on the textual component and the\nimage generation component separately. We conduct a comprehensive comparison on\npruning the model or the single component of the model in various sparsities.\nOur results yield previously undocumented findings. For example, contrary to\nestablished trends in language model pruning, we discover that simple magnitude\npruning outperforms more advanced techniques in text-to-image context.\nFurthermore, our results show that Stable Diffusion 2 can be pruned to 38.5%\nsparsity with minimal quality loss, achieving a significant reduction in model\nsize. We propose an optimal pruning configuration that prunes the text encoder\nto 47.5% and the diffusion generator to 35%. This configuration maintains image\ngeneration quality while substantially reducing computational requirements. In\naddition, our work uncovers intriguing questions about information encoding in\ntext-to-image models: we observe that pruning beyond certain thresholds leads\nto sudden performance drops (unreadable images), suggesting that specific\nweights encode critical semantics information. This finding opens new avenues\nfor future research in model compression, interoperability, and bias\nidentification in text-to-image models. By providing crucial insights into the\npruning behavior of text-to-image models, our study lays the groundwork for\ndeveloping more efficient and accessible AI-driven image generation systems\n", "link": "http://arxiv.org/abs/2411.15113v1", "date": "2024-11-22", "relevancy": 1.9264, "topK": [{"title": "ToDo: Token Downsampling for Efficient Generation of High-Resolution\n  Images", "link": "http://arxiv.org/abs/2402.13573v2", "similarity": 0.6504}, {"title": "Scaling Rectified Flow Transformers for High-Resolution Image Synthesis", "link": "http://arxiv.org/abs/2403.03206v1", "similarity": 0.6418}, {"title": "SDXS: Real-Time One-Step Latent Diffusion Models with Image Conditions", "link": "http://arxiv.org/abs/2403.16627v1", "similarity": 0.6219}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Efficient%20Pruning%20of%20Text-to-Image%20Models%3A%20Insights%20from%20Pruning%20Stable%0A%20%20Diffusion&body=Title%3A%20Efficient%20Pruning%20of%20Text-to-Image%20Models%3A%20Insights%20from%20Pruning%20Stable%0A%20%20Diffusion%0AAuthor%3A%20Samarth%20N%20Ramesh%20and%20Zhixue%20Zhao%0AAbstract%3A%20%20%20As%20text-to-image%20models%20grow%20increasingly%20powerful%20and%20complex%2C%20their%0Aburgeoning%20size%20presents%20a%20significant%20obstacle%20to%20widespread%20adoption%2C%0Aespecially%20on%20resource-constrained%20devices.%20This%20paper%20presents%20a%20pioneering%0Astudy%20on%20post-training%20pruning%20of%20Stable%20Diffusion%202%2C%20addressing%20the%20critical%0Aneed%20for%20model%20compression%20in%20text-to-image%20domain.%20Our%20study%20tackles%20the%0Apruning%20techniques%20for%20the%20previously%20unexplored%20multi-modal%20generation%20models%2C%0Aand%20particularly%20examines%20the%20pruning%20impact%20on%20the%20textual%20component%20and%20the%0Aimage%20generation%20component%20separately.%20We%20conduct%20a%20comprehensive%20comparison%20on%0Apruning%20the%20model%20or%20the%20single%20component%20of%20the%20model%20in%20various%20sparsities.%0AOur%20results%20yield%20previously%20undocumented%20findings.%20For%20example%2C%20contrary%20to%0Aestablished%20trends%20in%20language%20model%20pruning%2C%20we%20discover%20that%20simple%20magnitude%0Apruning%20outperforms%20more%20advanced%20techniques%20in%20text-to-image%20context.%0AFurthermore%2C%20our%20results%20show%20that%20Stable%20Diffusion%202%20can%20be%20pruned%20to%2038.5%25%0Asparsity%20with%20minimal%20quality%20loss%2C%20achieving%20a%20significant%20reduction%20in%20model%0Asize.%20We%20propose%20an%20optimal%20pruning%20configuration%20that%20prunes%20the%20text%20encoder%0Ato%2047.5%25%20and%20the%20diffusion%20generator%20to%2035%25.%20This%20configuration%20maintains%20image%0Ageneration%20quality%20while%20substantially%20reducing%20computational%20requirements.%20In%0Aaddition%2C%20our%20work%20uncovers%20intriguing%20questions%20about%20information%20encoding%20in%0Atext-to-image%20models%3A%20we%20observe%20that%20pruning%20beyond%20certain%20thresholds%20leads%0Ato%20sudden%20performance%20drops%20%28unreadable%20images%29%2C%20suggesting%20that%20specific%0Aweights%20encode%20critical%20semantics%20information.%20This%20finding%20opens%20new%20avenues%0Afor%20future%20research%20in%20model%20compression%2C%20interoperability%2C%20and%20bias%0Aidentification%20in%20text-to-image%20models.%20By%20providing%20crucial%20insights%20into%20the%0Apruning%20behavior%20of%20text-to-image%20models%2C%20our%20study%20lays%20the%20groundwork%20for%0Adeveloping%20more%20efficient%20and%20accessible%20AI-driven%20image%20generation%20systems%0A%0ALink%3A%20http%3A//arxiv.org/abs/2411.15113v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DEfficient%2520Pruning%2520of%2520Text-to-Image%2520Models%253A%2520Insights%2520from%2520Pruning%2520Stable%250A%2520%2520Diffusion%26entry.906535625%3DSamarth%2520N%2520Ramesh%2520and%2520Zhixue%2520Zhao%26entry.1292438233%3D%2520%2520As%2520text-to-image%2520models%2520grow%2520increasingly%2520powerful%2520and%2520complex%252C%2520their%250Aburgeoning%2520size%2520presents%2520a%2520significant%2520obstacle%2520to%2520widespread%2520adoption%252C%250Aespecially%2520on%2520resource-constrained%2520devices.%2520This%2520paper%2520presents%2520a%2520pioneering%250Astudy%2520on%2520post-training%2520pruning%2520of%2520Stable%2520Diffusion%25202%252C%2520addressing%2520the%2520critical%250Aneed%2520for%2520model%2520compression%2520in%2520text-to-image%2520domain.%2520Our%2520study%2520tackles%2520the%250Apruning%2520techniques%2520for%2520the%2520previously%2520unexplored%2520multi-modal%2520generation%2520models%252C%250Aand%2520particularly%2520examines%2520the%2520pruning%2520impact%2520on%2520the%2520textual%2520component%2520and%2520the%250Aimage%2520generation%2520component%2520separately.%2520We%2520conduct%2520a%2520comprehensive%2520comparison%2520on%250Apruning%2520the%2520model%2520or%2520the%2520single%2520component%2520of%2520the%2520model%2520in%2520various%2520sparsities.%250AOur%2520results%2520yield%2520previously%2520undocumented%2520findings.%2520For%2520example%252C%2520contrary%2520to%250Aestablished%2520trends%2520in%2520language%2520model%2520pruning%252C%2520we%2520discover%2520that%2520simple%2520magnitude%250Apruning%2520outperforms%2520more%2520advanced%2520techniques%2520in%2520text-to-image%2520context.%250AFurthermore%252C%2520our%2520results%2520show%2520that%2520Stable%2520Diffusion%25202%2520can%2520be%2520pruned%2520to%252038.5%2525%250Asparsity%2520with%2520minimal%2520quality%2520loss%252C%2520achieving%2520a%2520significant%2520reduction%2520in%2520model%250Asize.%2520We%2520propose%2520an%2520optimal%2520pruning%2520configuration%2520that%2520prunes%2520the%2520text%2520encoder%250Ato%252047.5%2525%2520and%2520the%2520diffusion%2520generator%2520to%252035%2525.%2520This%2520configuration%2520maintains%2520image%250Ageneration%2520quality%2520while%2520substantially%2520reducing%2520computational%2520requirements.%2520In%250Aaddition%252C%2520our%2520work%2520uncovers%2520intriguing%2520questions%2520about%2520information%2520encoding%2520in%250Atext-to-image%2520models%253A%2520we%2520observe%2520that%2520pruning%2520beyond%2520certain%2520thresholds%2520leads%250Ato%2520sudden%2520performance%2520drops%2520%2528unreadable%2520images%2529%252C%2520suggesting%2520that%2520specific%250Aweights%2520encode%2520critical%2520semantics%2520information.%2520This%2520finding%2520opens%2520new%2520avenues%250Afor%2520future%2520research%2520in%2520model%2520compression%252C%2520interoperability%252C%2520and%2520bias%250Aidentification%2520in%2520text-to-image%2520models.%2520By%2520providing%2520crucial%2520insights%2520into%2520the%250Apruning%2520behavior%2520of%2520text-to-image%2520models%252C%2520our%2520study%2520lays%2520the%2520groundwork%2520for%250Adeveloping%2520more%2520efficient%2520and%2520accessible%2520AI-driven%2520image%2520generation%2520systems%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2411.15113v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Efficient%20Pruning%20of%20Text-to-Image%20Models%3A%20Insights%20from%20Pruning%20Stable%0A%20%20Diffusion&entry.906535625=Samarth%20N%20Ramesh%20and%20Zhixue%20Zhao&entry.1292438233=%20%20As%20text-to-image%20models%20grow%20increasingly%20powerful%20and%20complex%2C%20their%0Aburgeoning%20size%20presents%20a%20significant%20obstacle%20to%20widespread%20adoption%2C%0Aespecially%20on%20resource-constrained%20devices.%20This%20paper%20presents%20a%20pioneering%0Astudy%20on%20post-training%20pruning%20of%20Stable%20Diffusion%202%2C%20addressing%20the%20critical%0Aneed%20for%20model%20compression%20in%20text-to-image%20domain.%20Our%20study%20tackles%20the%0Apruning%20techniques%20for%20the%20previously%20unexplored%20multi-modal%20generation%20models%2C%0Aand%20particularly%20examines%20the%20pruning%20impact%20on%20the%20textual%20component%20and%20the%0Aimage%20generation%20component%20separately.%20We%20conduct%20a%20comprehensive%20comparison%20on%0Apruning%20the%20model%20or%20the%20single%20component%20of%20the%20model%20in%20various%20sparsities.%0AOur%20results%20yield%20previously%20undocumented%20findings.%20For%20example%2C%20contrary%20to%0Aestablished%20trends%20in%20language%20model%20pruning%2C%20we%20discover%20that%20simple%20magnitude%0Apruning%20outperforms%20more%20advanced%20techniques%20in%20text-to-image%20context.%0AFurthermore%2C%20our%20results%20show%20that%20Stable%20Diffusion%202%20can%20be%20pruned%20to%2038.5%25%0Asparsity%20with%20minimal%20quality%20loss%2C%20achieving%20a%20significant%20reduction%20in%20model%0Asize.%20We%20propose%20an%20optimal%20pruning%20configuration%20that%20prunes%20the%20text%20encoder%0Ato%2047.5%25%20and%20the%20diffusion%20generator%20to%2035%25.%20This%20configuration%20maintains%20image%0Ageneration%20quality%20while%20substantially%20reducing%20computational%20requirements.%20In%0Aaddition%2C%20our%20work%20uncovers%20intriguing%20questions%20about%20information%20encoding%20in%0Atext-to-image%20models%3A%20we%20observe%20that%20pruning%20beyond%20certain%20thresholds%20leads%0Ato%20sudden%20performance%20drops%20%28unreadable%20images%29%2C%20suggesting%20that%20specific%0Aweights%20encode%20critical%20semantics%20information.%20This%20finding%20opens%20new%20avenues%0Afor%20future%20research%20in%20model%20compression%2C%20interoperability%2C%20and%20bias%0Aidentification%20in%20text-to-image%20models.%20By%20providing%20crucial%20insights%20into%20the%0Apruning%20behavior%20of%20text-to-image%20models%2C%20our%20study%20lays%20the%20groundwork%20for%0Adeveloping%20more%20efficient%20and%20accessible%20AI-driven%20image%20generation%20systems%0A&entry.1838667208=http%3A//arxiv.org/abs/2411.15113v1&entry.124074799=Read"},
{"title": "Learning Lifted STRIPS Models from Action Traces Alone: A Simple,\n  General, and Scalable Solution", "author": "Jonas G\u00f6sgens and Niklas Jansen and Hector Geffner", "abstract": "  Learning STRIPS action models from action traces alone is a challenging\nproblem as it involves learning the domain predicates as well. In this work, a\nnovel approach is introduced which, like the well-known LOCM systems, is\nscalable, but like SAT approaches, is sound and complete. Furthermore, the\napproach is general and imposes no restrictions on the hidden domain or the\nnumber or arity of the predicates. The new learning method is based on an\n\\emph{efficient, novel test} that checks whether the assumption that a\npredicate is affected by a set of action patterns, namely, actions with\nspecific argument positions, is consistent with the traces. The predicates and\naction patterns that pass the test provide the basis for the learned domain\nthat is then easily completed with preconditions and static predicates. The new\nmethod is studied theoretically and experimentally. For the latter, the method\nis evaluated on traces and graphs obtained from standard classical domains like\nthe 8-puzzle, which involve hundreds of thousands of states and transitions.\nThe learned representations are then verified on larger instances.\n", "link": "http://arxiv.org/abs/2411.14995v1", "date": "2024-11-22", "relevancy": 1.9175, "topK": [{"title": "RoboEXP: Action-Conditioned Scene Graph via Interactive Exploration for\n  Robotic Manipulation", "link": "http://arxiv.org/abs/2402.15487v1", "similarity": 0.5195}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.4737}, {"title": "Self-supervised Photographic Image Layout Representation Learning", "link": "http://arxiv.org/abs/2403.03740v1", "similarity": 0.469}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Learning%20Lifted%20STRIPS%20Models%20from%20Action%20Traces%20Alone%3A%20A%20Simple%2C%0A%20%20General%2C%20and%20Scalable%20Solution&body=Title%3A%20Learning%20Lifted%20STRIPS%20Models%20from%20Action%20Traces%20Alone%3A%20A%20Simple%2C%0A%20%20General%2C%20and%20Scalable%20Solution%0AAuthor%3A%20Jonas%20G%C3%B6sgens%20and%20Niklas%20Jansen%20and%20Hector%20Geffner%0AAbstract%3A%20%20%20Learning%20STRIPS%20action%20models%20from%20action%20traces%20alone%20is%20a%20challenging%0Aproblem%20as%20it%20involves%20learning%20the%20domain%20predicates%20as%20well.%20In%20this%20work%2C%20a%0Anovel%20approach%20is%20introduced%20which%2C%20like%20the%20well-known%20LOCM%20systems%2C%20is%0Ascalable%2C%20but%20like%20SAT%20approaches%2C%20is%20sound%20and%20complete.%20Furthermore%2C%20the%0Aapproach%20is%20general%20and%20imposes%20no%20restrictions%20on%20the%20hidden%20domain%20or%20the%0Anumber%20or%20arity%20of%20the%20predicates.%20The%20new%20learning%20method%20is%20based%20on%20an%0A%5Cemph%7Befficient%2C%20novel%20test%7D%20that%20checks%20whether%20the%20assumption%20that%20a%0Apredicate%20is%20affected%20by%20a%20set%20of%20action%20patterns%2C%20namely%2C%20actions%20with%0Aspecific%20argument%20positions%2C%20is%20consistent%20with%20the%20traces.%20The%20predicates%20and%0Aaction%20patterns%20that%20pass%20the%20test%20provide%20the%20basis%20for%20the%20learned%20domain%0Athat%20is%20then%20easily%20completed%20with%20preconditions%20and%20static%20predicates.%20The%20new%0Amethod%20is%20studied%20theoretically%20and%20experimentally.%20For%20the%20latter%2C%20the%20method%0Ais%20evaluated%20on%20traces%20and%20graphs%20obtained%20from%20standard%20classical%20domains%20like%0Athe%208-puzzle%2C%20which%20involve%20hundreds%20of%20thousands%20of%20states%20and%20transitions.%0AThe%20learned%20representations%20are%20then%20verified%20on%20larger%20instances.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2411.14995v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DLearning%2520Lifted%2520STRIPS%2520Models%2520from%2520Action%2520Traces%2520Alone%253A%2520A%2520Simple%252C%250A%2520%2520General%252C%2520and%2520Scalable%2520Solution%26entry.906535625%3DJonas%2520G%25C3%25B6sgens%2520and%2520Niklas%2520Jansen%2520and%2520Hector%2520Geffner%26entry.1292438233%3D%2520%2520Learning%2520STRIPS%2520action%2520models%2520from%2520action%2520traces%2520alone%2520is%2520a%2520challenging%250Aproblem%2520as%2520it%2520involves%2520learning%2520the%2520domain%2520predicates%2520as%2520well.%2520In%2520this%2520work%252C%2520a%250Anovel%2520approach%2520is%2520introduced%2520which%252C%2520like%2520the%2520well-known%2520LOCM%2520systems%252C%2520is%250Ascalable%252C%2520but%2520like%2520SAT%2520approaches%252C%2520is%2520sound%2520and%2520complete.%2520Furthermore%252C%2520the%250Aapproach%2520is%2520general%2520and%2520imposes%2520no%2520restrictions%2520on%2520the%2520hidden%2520domain%2520or%2520the%250Anumber%2520or%2520arity%2520of%2520the%2520predicates.%2520The%2520new%2520learning%2520method%2520is%2520based%2520on%2520an%250A%255Cemph%257Befficient%252C%2520novel%2520test%257D%2520that%2520checks%2520whether%2520the%2520assumption%2520that%2520a%250Apredicate%2520is%2520affected%2520by%2520a%2520set%2520of%2520action%2520patterns%252C%2520namely%252C%2520actions%2520with%250Aspecific%2520argument%2520positions%252C%2520is%2520consistent%2520with%2520the%2520traces.%2520The%2520predicates%2520and%250Aaction%2520patterns%2520that%2520pass%2520the%2520test%2520provide%2520the%2520basis%2520for%2520the%2520learned%2520domain%250Athat%2520is%2520then%2520easily%2520completed%2520with%2520preconditions%2520and%2520static%2520predicates.%2520The%2520new%250Amethod%2520is%2520studied%2520theoretically%2520and%2520experimentally.%2520For%2520the%2520latter%252C%2520the%2520method%250Ais%2520evaluated%2520on%2520traces%2520and%2520graphs%2520obtained%2520from%2520standard%2520classical%2520domains%2520like%250Athe%25208-puzzle%252C%2520which%2520involve%2520hundreds%2520of%2520thousands%2520of%2520states%2520and%2520transitions.%250AThe%2520learned%2520representations%2520are%2520then%2520verified%2520on%2520larger%2520instances.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2411.14995v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Learning%20Lifted%20STRIPS%20Models%20from%20Action%20Traces%20Alone%3A%20A%20Simple%2C%0A%20%20General%2C%20and%20Scalable%20Solution&entry.906535625=Jonas%20G%C3%B6sgens%20and%20Niklas%20Jansen%20and%20Hector%20Geffner&entry.1292438233=%20%20Learning%20STRIPS%20action%20models%20from%20action%20traces%20alone%20is%20a%20challenging%0Aproblem%20as%20it%20involves%20learning%20the%20domain%20predicates%20as%20well.%20In%20this%20work%2C%20a%0Anovel%20approach%20is%20introduced%20which%2C%20like%20the%20well-known%20LOCM%20systems%2C%20is%0Ascalable%2C%20but%20like%20SAT%20approaches%2C%20is%20sound%20and%20complete.%20Furthermore%2C%20the%0Aapproach%20is%20general%20and%20imposes%20no%20restrictions%20on%20the%20hidden%20domain%20or%20the%0Anumber%20or%20arity%20of%20the%20predicates.%20The%20new%20learning%20method%20is%20based%20on%20an%0A%5Cemph%7Befficient%2C%20novel%20test%7D%20that%20checks%20whether%20the%20assumption%20that%20a%0Apredicate%20is%20affected%20by%20a%20set%20of%20action%20patterns%2C%20namely%2C%20actions%20with%0Aspecific%20argument%20positions%2C%20is%20consistent%20with%20the%20traces.%20The%20predicates%20and%0Aaction%20patterns%20that%20pass%20the%20test%20provide%20the%20basis%20for%20the%20learned%20domain%0Athat%20is%20then%20easily%20completed%20with%20preconditions%20and%20static%20predicates.%20The%20new%0Amethod%20is%20studied%20theoretically%20and%20experimentally.%20For%20the%20latter%2C%20the%20method%0Ais%20evaluated%20on%20traces%20and%20graphs%20obtained%20from%20standard%20classical%20domains%20like%0Athe%208-puzzle%2C%20which%20involve%20hundreds%20of%20thousands%20of%20states%20and%20transitions.%0AThe%20learned%20representations%20are%20then%20verified%20on%20larger%20instances.%0A&entry.1838667208=http%3A//arxiv.org/abs/2411.14995v1&entry.124074799=Read"},
{"title": "Enhancing Exploration with Diffusion Policies in Hybrid Off-Policy RL:\n  Application to Non-Prehensile Manipulation", "author": "Huy Le and Miroslav Gabriel and Tai Hoang and Gerhard Neumann and Ngo Anh Vien", "abstract": "  Learning diverse policies for non-prehensile manipulation is essential for\nimproving skill transfer and generalization to out-of-distribution scenarios.\nIn this work, we enhance exploration through a two-fold approach within a\nhybrid framework that tackles both discrete and continuous action spaces.\nFirst, we model the continuous motion parameter policy as a diffusion model,\nand second, we incorporate this into a maximum entropy reinforcement learning\nframework that unifies both the discrete and continuous components. The\ndiscrete action space, such as contact point selection, is optimized through\nQ-value function maximization, while the continuous part is guided by a\ndiffusion-based policy. This hybrid approach leads to a principled objective,\nwhere the maximum entropy term is derived as a lower bound using structured\nvariational inference. We propose the Hybrid Diffusion Policy algorithm (HyDo)\nand evaluate its performance on both simulation and zero-shot sim2real tasks.\nOur results show that HyDo encourages more diverse behavior policies, leading\nto significantly improved success rates across tasks - for example, increasing\nfrom 53% to 72% on a real-world 6D pose alignment task. Project page:\nhttps://leh2rng.github.io/hydo\n", "link": "http://arxiv.org/abs/2411.14913v1", "date": "2024-11-22", "relevancy": 1.7481, "topK": [{"title": "RoboEXP: Action-Conditioned Scene Graph via Interactive Exploration for\n  Robotic Manipulation", "link": "http://arxiv.org/abs/2402.15487v1", "similarity": 0.5985}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.596}, {"title": "UGG: Unified Generative Grasping", "link": "https://arxiv.org/abs/2311.16917", "similarity": 0.5711}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Enhancing%20Exploration%20with%20Diffusion%20Policies%20in%20Hybrid%20Off-Policy%20RL%3A%0A%20%20Application%20to%20Non-Prehensile%20Manipulation&body=Title%3A%20Enhancing%20Exploration%20with%20Diffusion%20Policies%20in%20Hybrid%20Off-Policy%20RL%3A%0A%20%20Application%20to%20Non-Prehensile%20Manipulation%0AAuthor%3A%20Huy%20Le%20and%20Miroslav%20Gabriel%20and%20Tai%20Hoang%20and%20Gerhard%20Neumann%20and%20Ngo%20Anh%20Vien%0AAbstract%3A%20%20%20Learning%20diverse%20policies%20for%20non-prehensile%20manipulation%20is%20essential%20for%0Aimproving%20skill%20transfer%20and%20generalization%20to%20out-of-distribution%20scenarios.%0AIn%20this%20work%2C%20we%20enhance%20exploration%20through%20a%20two-fold%20approach%20within%20a%0Ahybrid%20framework%20that%20tackles%20both%20discrete%20and%20continuous%20action%20spaces.%0AFirst%2C%20we%20model%20the%20continuous%20motion%20parameter%20policy%20as%20a%20diffusion%20model%2C%0Aand%20second%2C%20we%20incorporate%20this%20into%20a%20maximum%20entropy%20reinforcement%20learning%0Aframework%20that%20unifies%20both%20the%20discrete%20and%20continuous%20components.%20The%0Adiscrete%20action%20space%2C%20such%20as%20contact%20point%20selection%2C%20is%20optimized%20through%0AQ-value%20function%20maximization%2C%20while%20the%20continuous%20part%20is%20guided%20by%20a%0Adiffusion-based%20policy.%20This%20hybrid%20approach%20leads%20to%20a%20principled%20objective%2C%0Awhere%20the%20maximum%20entropy%20term%20is%20derived%20as%20a%20lower%20bound%20using%20structured%0Avariational%20inference.%20We%20propose%20the%20Hybrid%20Diffusion%20Policy%20algorithm%20%28HyDo%29%0Aand%20evaluate%20its%20performance%20on%20both%20simulation%20and%20zero-shot%20sim2real%20tasks.%0AOur%20results%20show%20that%20HyDo%20encourages%20more%20diverse%20behavior%20policies%2C%20leading%0Ato%20significantly%20improved%20success%20rates%20across%20tasks%20-%20for%20example%2C%20increasing%0Afrom%2053%25%20to%2072%25%20on%20a%20real-world%206D%20pose%20alignment%20task.%20Project%20page%3A%0Ahttps%3A//leh2rng.github.io/hydo%0A%0ALink%3A%20http%3A//arxiv.org/abs/2411.14913v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DEnhancing%2520Exploration%2520with%2520Diffusion%2520Policies%2520in%2520Hybrid%2520Off-Policy%2520RL%253A%250A%2520%2520Application%2520to%2520Non-Prehensile%2520Manipulation%26entry.906535625%3DHuy%2520Le%2520and%2520Miroslav%2520Gabriel%2520and%2520Tai%2520Hoang%2520and%2520Gerhard%2520Neumann%2520and%2520Ngo%2520Anh%2520Vien%26entry.1292438233%3D%2520%2520Learning%2520diverse%2520policies%2520for%2520non-prehensile%2520manipulation%2520is%2520essential%2520for%250Aimproving%2520skill%2520transfer%2520and%2520generalization%2520to%2520out-of-distribution%2520scenarios.%250AIn%2520this%2520work%252C%2520we%2520enhance%2520exploration%2520through%2520a%2520two-fold%2520approach%2520within%2520a%250Ahybrid%2520framework%2520that%2520tackles%2520both%2520discrete%2520and%2520continuous%2520action%2520spaces.%250AFirst%252C%2520we%2520model%2520the%2520continuous%2520motion%2520parameter%2520policy%2520as%2520a%2520diffusion%2520model%252C%250Aand%2520second%252C%2520we%2520incorporate%2520this%2520into%2520a%2520maximum%2520entropy%2520reinforcement%2520learning%250Aframework%2520that%2520unifies%2520both%2520the%2520discrete%2520and%2520continuous%2520components.%2520The%250Adiscrete%2520action%2520space%252C%2520such%2520as%2520contact%2520point%2520selection%252C%2520is%2520optimized%2520through%250AQ-value%2520function%2520maximization%252C%2520while%2520the%2520continuous%2520part%2520is%2520guided%2520by%2520a%250Adiffusion-based%2520policy.%2520This%2520hybrid%2520approach%2520leads%2520to%2520a%2520principled%2520objective%252C%250Awhere%2520the%2520maximum%2520entropy%2520term%2520is%2520derived%2520as%2520a%2520lower%2520bound%2520using%2520structured%250Avariational%2520inference.%2520We%2520propose%2520the%2520Hybrid%2520Diffusion%2520Policy%2520algorithm%2520%2528HyDo%2529%250Aand%2520evaluate%2520its%2520performance%2520on%2520both%2520simulation%2520and%2520zero-shot%2520sim2real%2520tasks.%250AOur%2520results%2520show%2520that%2520HyDo%2520encourages%2520more%2520diverse%2520behavior%2520policies%252C%2520leading%250Ato%2520significantly%2520improved%2520success%2520rates%2520across%2520tasks%2520-%2520for%2520example%252C%2520increasing%250Afrom%252053%2525%2520to%252072%2525%2520on%2520a%2520real-world%25206D%2520pose%2520alignment%2520task.%2520Project%2520page%253A%250Ahttps%253A//leh2rng.github.io/hydo%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2411.14913v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Enhancing%20Exploration%20with%20Diffusion%20Policies%20in%20Hybrid%20Off-Policy%20RL%3A%0A%20%20Application%20to%20Non-Prehensile%20Manipulation&entry.906535625=Huy%20Le%20and%20Miroslav%20Gabriel%20and%20Tai%20Hoang%20and%20Gerhard%20Neumann%20and%20Ngo%20Anh%20Vien&entry.1292438233=%20%20Learning%20diverse%20policies%20for%20non-prehensile%20manipulation%20is%20essential%20for%0Aimproving%20skill%20transfer%20and%20generalization%20to%20out-of-distribution%20scenarios.%0AIn%20this%20work%2C%20we%20enhance%20exploration%20through%20a%20two-fold%20approach%20within%20a%0Ahybrid%20framework%20that%20tackles%20both%20discrete%20and%20continuous%20action%20spaces.%0AFirst%2C%20we%20model%20the%20continuous%20motion%20parameter%20policy%20as%20a%20diffusion%20model%2C%0Aand%20second%2C%20we%20incorporate%20this%20into%20a%20maximum%20entropy%20reinforcement%20learning%0Aframework%20that%20unifies%20both%20the%20discrete%20and%20continuous%20components.%20The%0Adiscrete%20action%20space%2C%20such%20as%20contact%20point%20selection%2C%20is%20optimized%20through%0AQ-value%20function%20maximization%2C%20while%20the%20continuous%20part%20is%20guided%20by%20a%0Adiffusion-based%20policy.%20This%20hybrid%20approach%20leads%20to%20a%20principled%20objective%2C%0Awhere%20the%20maximum%20entropy%20term%20is%20derived%20as%20a%20lower%20bound%20using%20structured%0Avariational%20inference.%20We%20propose%20the%20Hybrid%20Diffusion%20Policy%20algorithm%20%28HyDo%29%0Aand%20evaluate%20its%20performance%20on%20both%20simulation%20and%20zero-shot%20sim2real%20tasks.%0AOur%20results%20show%20that%20HyDo%20encourages%20more%20diverse%20behavior%20policies%2C%20leading%0Ato%20significantly%20improved%20success%20rates%20across%20tasks%20-%20for%20example%2C%20increasing%0Afrom%2053%25%20to%2072%25%20on%20a%20real-world%206D%20pose%20alignment%20task.%20Project%20page%3A%0Ahttps%3A//leh2rng.github.io/hydo%0A&entry.1838667208=http%3A//arxiv.org/abs/2411.14913v1&entry.124074799=Read"},
{"title": "Rapid Integration of LLMs in Healthcare Raises Ethical Concerns: An\n  Investigation into Deceptive Patterns in Social Robots", "author": "Robert Ranisch and Joschka Haltaufderheide", "abstract": "  Conversational agents are increasingly used in healthcare, and the\nintegration of Large Language Models (LLMs) has significantly enhanced their\ncapabilities. When integrated into social robots, LLMs offer the potential for\nmore natural interactions. However, while LLMs promise numerous benefits, they\nalso raise critical ethical concerns, particularly around the issue of\nhallucinations and deceptive patterns. In this case study, we observed a\ncritical pattern of deceptive behavior in commercially available LLM-based care\nsoftware integrated into robots. The LLM-equipped robot falsely claimed to have\nmedication reminder functionalities. Not only did these systems assure users of\ntheir ability to manage medication schedules, but they also proactively\nsuggested this capability, despite lacking it. This deceptive behavior poses\nsignificant risks in healthcare environments, where reliability is paramount.\nOur findings highlights the ethical and safety concerns surrounding the\ndeployment of LLM-integrated robots in healthcare, emphasizing the need for\noversight to prevent potentially harmful consequences for vulnerable\npopulations.\n", "link": "http://arxiv.org/abs/2410.00434v2", "date": "2024-11-22", "relevancy": 0.964, "topK": [{"title": "RoboEXP: Action-Conditioned Scene Graph via Interactive Exploration for\n  Robotic Manipulation", "link": "http://arxiv.org/abs/2402.15487v1", "similarity": 0.5004}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.473}, {"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.4726}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Rapid%20Integration%20of%20LLMs%20in%20Healthcare%20Raises%20Ethical%20Concerns%3A%20An%0A%20%20Investigation%20into%20Deceptive%20Patterns%20in%20Social%20Robots&body=Title%3A%20Rapid%20Integration%20of%20LLMs%20in%20Healthcare%20Raises%20Ethical%20Concerns%3A%20An%0A%20%20Investigation%20into%20Deceptive%20Patterns%20in%20Social%20Robots%0AAuthor%3A%20Robert%20Ranisch%20and%20Joschka%20Haltaufderheide%0AAbstract%3A%20%20%20Conversational%20agents%20are%20increasingly%20used%20in%20healthcare%2C%20and%20the%0Aintegration%20of%20Large%20Language%20Models%20%28LLMs%29%20has%20significantly%20enhanced%20their%0Acapabilities.%20When%20integrated%20into%20social%20robots%2C%20LLMs%20offer%20the%20potential%20for%0Amore%20natural%20interactions.%20However%2C%20while%20LLMs%20promise%20numerous%20benefits%2C%20they%0Aalso%20raise%20critical%20ethical%20concerns%2C%20particularly%20around%20the%20issue%20of%0Ahallucinations%20and%20deceptive%20patterns.%20In%20this%20case%20study%2C%20we%20observed%20a%0Acritical%20pattern%20of%20deceptive%20behavior%20in%20commercially%20available%20LLM-based%20care%0Asoftware%20integrated%20into%20robots.%20The%20LLM-equipped%20robot%20falsely%20claimed%20to%20have%0Amedication%20reminder%20functionalities.%20Not%20only%20did%20these%20systems%20assure%20users%20of%0Atheir%20ability%20to%20manage%20medication%20schedules%2C%20but%20they%20also%20proactively%0Asuggested%20this%20capability%2C%20despite%20lacking%20it.%20This%20deceptive%20behavior%20poses%0Asignificant%20risks%20in%20healthcare%20environments%2C%20where%20reliability%20is%20paramount.%0AOur%20findings%20highlights%20the%20ethical%20and%20safety%20concerns%20surrounding%20the%0Adeployment%20of%20LLM-integrated%20robots%20in%20healthcare%2C%20emphasizing%20the%20need%20for%0Aoversight%20to%20prevent%20potentially%20harmful%20consequences%20for%20vulnerable%0Apopulations.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2410.00434v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DRapid%2520Integration%2520of%2520LLMs%2520in%2520Healthcare%2520Raises%2520Ethical%2520Concerns%253A%2520An%250A%2520%2520Investigation%2520into%2520Deceptive%2520Patterns%2520in%2520Social%2520Robots%26entry.906535625%3DRobert%2520Ranisch%2520and%2520Joschka%2520Haltaufderheide%26entry.1292438233%3D%2520%2520Conversational%2520agents%2520are%2520increasingly%2520used%2520in%2520healthcare%252C%2520and%2520the%250Aintegration%2520of%2520Large%2520Language%2520Models%2520%2528LLMs%2529%2520has%2520significantly%2520enhanced%2520their%250Acapabilities.%2520When%2520integrated%2520into%2520social%2520robots%252C%2520LLMs%2520offer%2520the%2520potential%2520for%250Amore%2520natural%2520interactions.%2520However%252C%2520while%2520LLMs%2520promise%2520numerous%2520benefits%252C%2520they%250Aalso%2520raise%2520critical%2520ethical%2520concerns%252C%2520particularly%2520around%2520the%2520issue%2520of%250Ahallucinations%2520and%2520deceptive%2520patterns.%2520In%2520this%2520case%2520study%252C%2520we%2520observed%2520a%250Acritical%2520pattern%2520of%2520deceptive%2520behavior%2520in%2520commercially%2520available%2520LLM-based%2520care%250Asoftware%2520integrated%2520into%2520robots.%2520The%2520LLM-equipped%2520robot%2520falsely%2520claimed%2520to%2520have%250Amedication%2520reminder%2520functionalities.%2520Not%2520only%2520did%2520these%2520systems%2520assure%2520users%2520of%250Atheir%2520ability%2520to%2520manage%2520medication%2520schedules%252C%2520but%2520they%2520also%2520proactively%250Asuggested%2520this%2520capability%252C%2520despite%2520lacking%2520it.%2520This%2520deceptive%2520behavior%2520poses%250Asignificant%2520risks%2520in%2520healthcare%2520environments%252C%2520where%2520reliability%2520is%2520paramount.%250AOur%2520findings%2520highlights%2520the%2520ethical%2520and%2520safety%2520concerns%2520surrounding%2520the%250Adeployment%2520of%2520LLM-integrated%2520robots%2520in%2520healthcare%252C%2520emphasizing%2520the%2520need%2520for%250Aoversight%2520to%2520prevent%2520potentially%2520harmful%2520consequences%2520for%2520vulnerable%250Apopulations.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2410.00434v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Rapid%20Integration%20of%20LLMs%20in%20Healthcare%20Raises%20Ethical%20Concerns%3A%20An%0A%20%20Investigation%20into%20Deceptive%20Patterns%20in%20Social%20Robots&entry.906535625=Robert%20Ranisch%20and%20Joschka%20Haltaufderheide&entry.1292438233=%20%20Conversational%20agents%20are%20increasingly%20used%20in%20healthcare%2C%20and%20the%0Aintegration%20of%20Large%20Language%20Models%20%28LLMs%29%20has%20significantly%20enhanced%20their%0Acapabilities.%20When%20integrated%20into%20social%20robots%2C%20LLMs%20offer%20the%20potential%20for%0Amore%20natural%20interactions.%20However%2C%20while%20LLMs%20promise%20numerous%20benefits%2C%20they%0Aalso%20raise%20critical%20ethical%20concerns%2C%20particularly%20around%20the%20issue%20of%0Ahallucinations%20and%20deceptive%20patterns.%20In%20this%20case%20study%2C%20we%20observed%20a%0Acritical%20pattern%20of%20deceptive%20behavior%20in%20commercially%20available%20LLM-based%20care%0Asoftware%20integrated%20into%20robots.%20The%20LLM-equipped%20robot%20falsely%20claimed%20to%20have%0Amedication%20reminder%20functionalities.%20Not%20only%20did%20these%20systems%20assure%20users%20of%0Atheir%20ability%20to%20manage%20medication%20schedules%2C%20but%20they%20also%20proactively%0Asuggested%20this%20capability%2C%20despite%20lacking%20it.%20This%20deceptive%20behavior%20poses%0Asignificant%20risks%20in%20healthcare%20environments%2C%20where%20reliability%20is%20paramount.%0AOur%20findings%20highlights%20the%20ethical%20and%20safety%20concerns%20surrounding%20the%0Adeployment%20of%20LLM-integrated%20robots%20in%20healthcare%2C%20emphasizing%20the%20need%20for%0Aoversight%20to%20prevent%20potentially%20harmful%20consequences%20for%20vulnerable%0Apopulations.%0A&entry.1838667208=http%3A//arxiv.org/abs/2410.00434v2&entry.124074799=Read"},
{"title": "Reliable Evaluation of Attribution Maps in CNNs: A Perturbation-Based\n  Approach", "author": "Lars Nieradzik and Henrike Stephani and Janis Keuper", "abstract": "  In this paper, we present an approach for evaluating attribution maps, which\nplay a central role in interpreting the predictions of convolutional neural\nnetworks (CNNs). We show that the widely used insertion/deletion metrics are\nsusceptible to distribution shifts that affect the reliability of the ranking.\nOur method proposes to replace pixel modifications with adversarial\nperturbations, which provides a more robust evaluation framework. By using\nsmoothness and monotonicity measures, we illustrate the effectiveness of our\napproach in correcting distribution shifts. In addition, we conduct the most\ncomprehensive quantitative and qualitative assessment of attribution maps to\ndate. Introducing baseline attribution maps as sanity checks, we find that our\nmetric is the only contender to pass all checks. Using Kendall's $\\tau$ rank\ncorrelation coefficient, we show the increased consistency of our metric across\n15 dataset-architecture combinations. Of the 16 attribution maps tested, our\nresults clearly show SmoothGrad to be the best map currently available. This\nresearch makes an important contribution to the development of attribution maps\nby providing a reliable and consistent evaluation framework. To ensure\nreproducibility, we will provide the code along with our results.\n", "link": "http://arxiv.org/abs/2411.14946v1", "date": "2024-11-22", "relevancy": 1.6658, "topK": [{"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.5956}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.5084}, {"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.5013}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Reliable%20Evaluation%20of%20Attribution%20Maps%20in%20CNNs%3A%20A%20Perturbation-Based%0A%20%20Approach&body=Title%3A%20Reliable%20Evaluation%20of%20Attribution%20Maps%20in%20CNNs%3A%20A%20Perturbation-Based%0A%20%20Approach%0AAuthor%3A%20Lars%20Nieradzik%20and%20Henrike%20Stephani%20and%20Janis%20Keuper%0AAbstract%3A%20%20%20In%20this%20paper%2C%20we%20present%20an%20approach%20for%20evaluating%20attribution%20maps%2C%20which%0Aplay%20a%20central%20role%20in%20interpreting%20the%20predictions%20of%20convolutional%20neural%0Anetworks%20%28CNNs%29.%20We%20show%20that%20the%20widely%20used%20insertion/deletion%20metrics%20are%0Asusceptible%20to%20distribution%20shifts%20that%20affect%20the%20reliability%20of%20the%20ranking.%0AOur%20method%20proposes%20to%20replace%20pixel%20modifications%20with%20adversarial%0Aperturbations%2C%20which%20provides%20a%20more%20robust%20evaluation%20framework.%20By%20using%0Asmoothness%20and%20monotonicity%20measures%2C%20we%20illustrate%20the%20effectiveness%20of%20our%0Aapproach%20in%20correcting%20distribution%20shifts.%20In%20addition%2C%20we%20conduct%20the%20most%0Acomprehensive%20quantitative%20and%20qualitative%20assessment%20of%20attribution%20maps%20to%0Adate.%20Introducing%20baseline%20attribution%20maps%20as%20sanity%20checks%2C%20we%20find%20that%20our%0Ametric%20is%20the%20only%20contender%20to%20pass%20all%20checks.%20Using%20Kendall%27s%20%24%5Ctau%24%20rank%0Acorrelation%20coefficient%2C%20we%20show%20the%20increased%20consistency%20of%20our%20metric%20across%0A15%20dataset-architecture%20combinations.%20Of%20the%2016%20attribution%20maps%20tested%2C%20our%0Aresults%20clearly%20show%20SmoothGrad%20to%20be%20the%20best%20map%20currently%20available.%20This%0Aresearch%20makes%20an%20important%20contribution%20to%20the%20development%20of%20attribution%20maps%0Aby%20providing%20a%20reliable%20and%20consistent%20evaluation%20framework.%20To%20ensure%0Areproducibility%2C%20we%20will%20provide%20the%20code%20along%20with%20our%20results.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2411.14946v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DReliable%2520Evaluation%2520of%2520Attribution%2520Maps%2520in%2520CNNs%253A%2520A%2520Perturbation-Based%250A%2520%2520Approach%26entry.906535625%3DLars%2520Nieradzik%2520and%2520Henrike%2520Stephani%2520and%2520Janis%2520Keuper%26entry.1292438233%3D%2520%2520In%2520this%2520paper%252C%2520we%2520present%2520an%2520approach%2520for%2520evaluating%2520attribution%2520maps%252C%2520which%250Aplay%2520a%2520central%2520role%2520in%2520interpreting%2520the%2520predictions%2520of%2520convolutional%2520neural%250Anetworks%2520%2528CNNs%2529.%2520We%2520show%2520that%2520the%2520widely%2520used%2520insertion/deletion%2520metrics%2520are%250Asusceptible%2520to%2520distribution%2520shifts%2520that%2520affect%2520the%2520reliability%2520of%2520the%2520ranking.%250AOur%2520method%2520proposes%2520to%2520replace%2520pixel%2520modifications%2520with%2520adversarial%250Aperturbations%252C%2520which%2520provides%2520a%2520more%2520robust%2520evaluation%2520framework.%2520By%2520using%250Asmoothness%2520and%2520monotonicity%2520measures%252C%2520we%2520illustrate%2520the%2520effectiveness%2520of%2520our%250Aapproach%2520in%2520correcting%2520distribution%2520shifts.%2520In%2520addition%252C%2520we%2520conduct%2520the%2520most%250Acomprehensive%2520quantitative%2520and%2520qualitative%2520assessment%2520of%2520attribution%2520maps%2520to%250Adate.%2520Introducing%2520baseline%2520attribution%2520maps%2520as%2520sanity%2520checks%252C%2520we%2520find%2520that%2520our%250Ametric%2520is%2520the%2520only%2520contender%2520to%2520pass%2520all%2520checks.%2520Using%2520Kendall%2527s%2520%2524%255Ctau%2524%2520rank%250Acorrelation%2520coefficient%252C%2520we%2520show%2520the%2520increased%2520consistency%2520of%2520our%2520metric%2520across%250A15%2520dataset-architecture%2520combinations.%2520Of%2520the%252016%2520attribution%2520maps%2520tested%252C%2520our%250Aresults%2520clearly%2520show%2520SmoothGrad%2520to%2520be%2520the%2520best%2520map%2520currently%2520available.%2520This%250Aresearch%2520makes%2520an%2520important%2520contribution%2520to%2520the%2520development%2520of%2520attribution%2520maps%250Aby%2520providing%2520a%2520reliable%2520and%2520consistent%2520evaluation%2520framework.%2520To%2520ensure%250Areproducibility%252C%2520we%2520will%2520provide%2520the%2520code%2520along%2520with%2520our%2520results.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2411.14946v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Reliable%20Evaluation%20of%20Attribution%20Maps%20in%20CNNs%3A%20A%20Perturbation-Based%0A%20%20Approach&entry.906535625=Lars%20Nieradzik%20and%20Henrike%20Stephani%20and%20Janis%20Keuper&entry.1292438233=%20%20In%20this%20paper%2C%20we%20present%20an%20approach%20for%20evaluating%20attribution%20maps%2C%20which%0Aplay%20a%20central%20role%20in%20interpreting%20the%20predictions%20of%20convolutional%20neural%0Anetworks%20%28CNNs%29.%20We%20show%20that%20the%20widely%20used%20insertion/deletion%20metrics%20are%0Asusceptible%20to%20distribution%20shifts%20that%20affect%20the%20reliability%20of%20the%20ranking.%0AOur%20method%20proposes%20to%20replace%20pixel%20modifications%20with%20adversarial%0Aperturbations%2C%20which%20provides%20a%20more%20robust%20evaluation%20framework.%20By%20using%0Asmoothness%20and%20monotonicity%20measures%2C%20we%20illustrate%20the%20effectiveness%20of%20our%0Aapproach%20in%20correcting%20distribution%20shifts.%20In%20addition%2C%20we%20conduct%20the%20most%0Acomprehensive%20quantitative%20and%20qualitative%20assessment%20of%20attribution%20maps%20to%0Adate.%20Introducing%20baseline%20attribution%20maps%20as%20sanity%20checks%2C%20we%20find%20that%20our%0Ametric%20is%20the%20only%20contender%20to%20pass%20all%20checks.%20Using%20Kendall%27s%20%24%5Ctau%24%20rank%0Acorrelation%20coefficient%2C%20we%20show%20the%20increased%20consistency%20of%20our%20metric%20across%0A15%20dataset-architecture%20combinations.%20Of%20the%2016%20attribution%20maps%20tested%2C%20our%0Aresults%20clearly%20show%20SmoothGrad%20to%20be%20the%20best%20map%20currently%20available.%20This%0Aresearch%20makes%20an%20important%20contribution%20to%20the%20development%20of%20attribution%20maps%0Aby%20providing%20a%20reliable%20and%20consistent%20evaluation%20framework.%20To%20ensure%0Areproducibility%2C%20we%20will%20provide%20the%20code%20along%20with%20our%20results.%0A&entry.1838667208=http%3A//arxiv.org/abs/2411.14946v1&entry.124074799=Read"},
{"title": "One to rule them all: natural language to bind communication, perception\n  and action", "author": "Simone Colombani and Dimitri Ognibene and Giuseppe Boccignone", "abstract": "  In recent years, research in the area of human-robot interaction has focused\non developing robots capable of understanding complex human instructions and\nperforming tasks in dynamic and diverse environments. These systems have a wide\nrange of applications, from personal assistance to industrial robotics,\nemphasizing the importance of robots interacting flexibly, naturally and safely\nwith humans. This paper presents an advanced architecture for robotic action\nplanning that integrates communication, perception, and planning with Large\nLanguage Models (LLMs). Our system is designed to translate commands expressed\nin natural language into executable robot actions, incorporating environmental\ninformation and dynamically updating plans based on real-time feedback. The\nPlanner Module is the core of the system where LLMs embedded in a modified\nReAct framework are employed to interpret and carry out user commands. By\nleveraging their extensive pre-trained knowledge, LLMs can effectively process\nuser requests without the need to introduce new knowledge on the changing\nenvironment. The modified ReAct framework further enhances the execution space\nby providing real-time environmental perception and the outcomes of physical\nactions. By combining robust and dynamic semantic map representations as graphs\nwith control components and failure explanations, this architecture enhances a\nrobot adaptability, task execution, and seamless collaboration with human users\nin shared and dynamic environments. Through the integration of continuous\nfeedback loops with the environment the system can dynamically adjusts the plan\nto accommodate unexpected changes, optimizing the robot ability to perform\ntasks. Using a dataset of previous experience is possible to provide detailed\nfeedback about the failure. Updating the LLMs context of the next iteration\nwith suggestion on how to overcame the issue.\n", "link": "http://arxiv.org/abs/2411.15033v1", "date": "2024-11-22", "relevancy": 1.6971, "topK": [{"title": "RoboEXP: Action-Conditioned Scene Graph via Interactive Exploration for\n  Robotic Manipulation", "link": "http://arxiv.org/abs/2402.15487v1", "similarity": 0.6462}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.5691}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5322}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20One%20to%20rule%20them%20all%3A%20natural%20language%20to%20bind%20communication%2C%20perception%0A%20%20and%20action&body=Title%3A%20One%20to%20rule%20them%20all%3A%20natural%20language%20to%20bind%20communication%2C%20perception%0A%20%20and%20action%0AAuthor%3A%20Simone%20Colombani%20and%20Dimitri%20Ognibene%20and%20Giuseppe%20Boccignone%0AAbstract%3A%20%20%20In%20recent%20years%2C%20research%20in%20the%20area%20of%20human-robot%20interaction%20has%20focused%0Aon%20developing%20robots%20capable%20of%20understanding%20complex%20human%20instructions%20and%0Aperforming%20tasks%20in%20dynamic%20and%20diverse%20environments.%20These%20systems%20have%20a%20wide%0Arange%20of%20applications%2C%20from%20personal%20assistance%20to%20industrial%20robotics%2C%0Aemphasizing%20the%20importance%20of%20robots%20interacting%20flexibly%2C%20naturally%20and%20safely%0Awith%20humans.%20This%20paper%20presents%20an%20advanced%20architecture%20for%20robotic%20action%0Aplanning%20that%20integrates%20communication%2C%20perception%2C%20and%20planning%20with%20Large%0ALanguage%20Models%20%28LLMs%29.%20Our%20system%20is%20designed%20to%20translate%20commands%20expressed%0Ain%20natural%20language%20into%20executable%20robot%20actions%2C%20incorporating%20environmental%0Ainformation%20and%20dynamically%20updating%20plans%20based%20on%20real-time%20feedback.%20The%0APlanner%20Module%20is%20the%20core%20of%20the%20system%20where%20LLMs%20embedded%20in%20a%20modified%0AReAct%20framework%20are%20employed%20to%20interpret%20and%20carry%20out%20user%20commands.%20By%0Aleveraging%20their%20extensive%20pre-trained%20knowledge%2C%20LLMs%20can%20effectively%20process%0Auser%20requests%20without%20the%20need%20to%20introduce%20new%20knowledge%20on%20the%20changing%0Aenvironment.%20The%20modified%20ReAct%20framework%20further%20enhances%20the%20execution%20space%0Aby%20providing%20real-time%20environmental%20perception%20and%20the%20outcomes%20of%20physical%0Aactions.%20By%20combining%20robust%20and%20dynamic%20semantic%20map%20representations%20as%20graphs%0Awith%20control%20components%20and%20failure%20explanations%2C%20this%20architecture%20enhances%20a%0Arobot%20adaptability%2C%20task%20execution%2C%20and%20seamless%20collaboration%20with%20human%20users%0Ain%20shared%20and%20dynamic%20environments.%20Through%20the%20integration%20of%20continuous%0Afeedback%20loops%20with%20the%20environment%20the%20system%20can%20dynamically%20adjusts%20the%20plan%0Ato%20accommodate%20unexpected%20changes%2C%20optimizing%20the%20robot%20ability%20to%20perform%0Atasks.%20Using%20a%20dataset%20of%20previous%20experience%20is%20possible%20to%20provide%20detailed%0Afeedback%20about%20the%20failure.%20Updating%20the%20LLMs%20context%20of%20the%20next%20iteration%0Awith%20suggestion%20on%20how%20to%20overcame%20the%20issue.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2411.15033v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DOne%2520to%2520rule%2520them%2520all%253A%2520natural%2520language%2520to%2520bind%2520communication%252C%2520perception%250A%2520%2520and%2520action%26entry.906535625%3DSimone%2520Colombani%2520and%2520Dimitri%2520Ognibene%2520and%2520Giuseppe%2520Boccignone%26entry.1292438233%3D%2520%2520In%2520recent%2520years%252C%2520research%2520in%2520the%2520area%2520of%2520human-robot%2520interaction%2520has%2520focused%250Aon%2520developing%2520robots%2520capable%2520of%2520understanding%2520complex%2520human%2520instructions%2520and%250Aperforming%2520tasks%2520in%2520dynamic%2520and%2520diverse%2520environments.%2520These%2520systems%2520have%2520a%2520wide%250Arange%2520of%2520applications%252C%2520from%2520personal%2520assistance%2520to%2520industrial%2520robotics%252C%250Aemphasizing%2520the%2520importance%2520of%2520robots%2520interacting%2520flexibly%252C%2520naturally%2520and%2520safely%250Awith%2520humans.%2520This%2520paper%2520presents%2520an%2520advanced%2520architecture%2520for%2520robotic%2520action%250Aplanning%2520that%2520integrates%2520communication%252C%2520perception%252C%2520and%2520planning%2520with%2520Large%250ALanguage%2520Models%2520%2528LLMs%2529.%2520Our%2520system%2520is%2520designed%2520to%2520translate%2520commands%2520expressed%250Ain%2520natural%2520language%2520into%2520executable%2520robot%2520actions%252C%2520incorporating%2520environmental%250Ainformation%2520and%2520dynamically%2520updating%2520plans%2520based%2520on%2520real-time%2520feedback.%2520The%250APlanner%2520Module%2520is%2520the%2520core%2520of%2520the%2520system%2520where%2520LLMs%2520embedded%2520in%2520a%2520modified%250AReAct%2520framework%2520are%2520employed%2520to%2520interpret%2520and%2520carry%2520out%2520user%2520commands.%2520By%250Aleveraging%2520their%2520extensive%2520pre-trained%2520knowledge%252C%2520LLMs%2520can%2520effectively%2520process%250Auser%2520requests%2520without%2520the%2520need%2520to%2520introduce%2520new%2520knowledge%2520on%2520the%2520changing%250Aenvironment.%2520The%2520modified%2520ReAct%2520framework%2520further%2520enhances%2520the%2520execution%2520space%250Aby%2520providing%2520real-time%2520environmental%2520perception%2520and%2520the%2520outcomes%2520of%2520physical%250Aactions.%2520By%2520combining%2520robust%2520and%2520dynamic%2520semantic%2520map%2520representations%2520as%2520graphs%250Awith%2520control%2520components%2520and%2520failure%2520explanations%252C%2520this%2520architecture%2520enhances%2520a%250Arobot%2520adaptability%252C%2520task%2520execution%252C%2520and%2520seamless%2520collaboration%2520with%2520human%2520users%250Ain%2520shared%2520and%2520dynamic%2520environments.%2520Through%2520the%2520integration%2520of%2520continuous%250Afeedback%2520loops%2520with%2520the%2520environment%2520the%2520system%2520can%2520dynamically%2520adjusts%2520the%2520plan%250Ato%2520accommodate%2520unexpected%2520changes%252C%2520optimizing%2520the%2520robot%2520ability%2520to%2520perform%250Atasks.%2520Using%2520a%2520dataset%2520of%2520previous%2520experience%2520is%2520possible%2520to%2520provide%2520detailed%250Afeedback%2520about%2520the%2520failure.%2520Updating%2520the%2520LLMs%2520context%2520of%2520the%2520next%2520iteration%250Awith%2520suggestion%2520on%2520how%2520to%2520overcame%2520the%2520issue.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2411.15033v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=One%20to%20rule%20them%20all%3A%20natural%20language%20to%20bind%20communication%2C%20perception%0A%20%20and%20action&entry.906535625=Simone%20Colombani%20and%20Dimitri%20Ognibene%20and%20Giuseppe%20Boccignone&entry.1292438233=%20%20In%20recent%20years%2C%20research%20in%20the%20area%20of%20human-robot%20interaction%20has%20focused%0Aon%20developing%20robots%20capable%20of%20understanding%20complex%20human%20instructions%20and%0Aperforming%20tasks%20in%20dynamic%20and%20diverse%20environments.%20These%20systems%20have%20a%20wide%0Arange%20of%20applications%2C%20from%20personal%20assistance%20to%20industrial%20robotics%2C%0Aemphasizing%20the%20importance%20of%20robots%20interacting%20flexibly%2C%20naturally%20and%20safely%0Awith%20humans.%20This%20paper%20presents%20an%20advanced%20architecture%20for%20robotic%20action%0Aplanning%20that%20integrates%20communication%2C%20perception%2C%20and%20planning%20with%20Large%0ALanguage%20Models%20%28LLMs%29.%20Our%20system%20is%20designed%20to%20translate%20commands%20expressed%0Ain%20natural%20language%20into%20executable%20robot%20actions%2C%20incorporating%20environmental%0Ainformation%20and%20dynamically%20updating%20plans%20based%20on%20real-time%20feedback.%20The%0APlanner%20Module%20is%20the%20core%20of%20the%20system%20where%20LLMs%20embedded%20in%20a%20modified%0AReAct%20framework%20are%20employed%20to%20interpret%20and%20carry%20out%20user%20commands.%20By%0Aleveraging%20their%20extensive%20pre-trained%20knowledge%2C%20LLMs%20can%20effectively%20process%0Auser%20requests%20without%20the%20need%20to%20introduce%20new%20knowledge%20on%20the%20changing%0Aenvironment.%20The%20modified%20ReAct%20framework%20further%20enhances%20the%20execution%20space%0Aby%20providing%20real-time%20environmental%20perception%20and%20the%20outcomes%20of%20physical%0Aactions.%20By%20combining%20robust%20and%20dynamic%20semantic%20map%20representations%20as%20graphs%0Awith%20control%20components%20and%20failure%20explanations%2C%20this%20architecture%20enhances%20a%0Arobot%20adaptability%2C%20task%20execution%2C%20and%20seamless%20collaboration%20with%20human%20users%0Ain%20shared%20and%20dynamic%20environments.%20Through%20the%20integration%20of%20continuous%0Afeedback%20loops%20with%20the%20environment%20the%20system%20can%20dynamically%20adjusts%20the%20plan%0Ato%20accommodate%20unexpected%20changes%2C%20optimizing%20the%20robot%20ability%20to%20perform%0Atasks.%20Using%20a%20dataset%20of%20previous%20experience%20is%20possible%20to%20provide%20detailed%0Afeedback%20about%20the%20failure.%20Updating%20the%20LLMs%20context%20of%20the%20next%20iteration%0Awith%20suggestion%20on%20how%20to%20overcame%20the%20issue.%0A&entry.1838667208=http%3A//arxiv.org/abs/2411.15033v1&entry.124074799=Read"},
{"title": "Quantum-enhanced unsupervised image segmentation for medical images\n  analysis", "author": "Laia Domingo and Mahdi Chehimi", "abstract": "  Breast cancer remains the leading cause of cancer-related mortality among\nwomen worldwide, necessitating the meticulous examination of mammograms by\nradiologists to characterize abnormal lesions. This manual process demands high\naccuracy and is often time-consuming, costly, and error-prone. Automated image\nsegmentation using artificial intelligence offers a promising alternative to\nstreamline this workflow. However, most existing methods are supervised,\nrequiring large, expertly annotated datasets that are not always available, and\nthey experience significant generalization issues. Thus, unsupervised learning\nmodels can be leveraged for image segmentation, but they come at a cost of\nreduced accuracy, or require extensive computational resourcess. In this paper,\nwe propose the first end-to-end quantum-enhanced framework for unsupervised\nmammography medical images segmentation that balances between performance\naccuracy and computational requirements. We first introduce a quantum-inspired\nimage representation that serves as an initial approximation of the\nsegmentation mask. The segmentation task is then formulated as a QUBO problem,\naiming to maximize the contrast between the background and the tumor region\nwhile ensuring a cohesive segmentation mask with minimal connected components.\nWe conduct an extensive evaluation of quantum and quantum-inspired methods for\nimage segmentation, demonstrating that quantum annealing and variational\nquantum circuits achieve performance comparable to classical optimization\ntechniques. Notably, quantum annealing is shown to be an order of magnitude\nfaster than the classical optimization method in our experiments. Our findings\ndemonstrate that this framework achieves performance comparable to\nstate-of-the-art supervised methods, including UNet-based architectures,\noffering a viable unsupervised alternative for breast cancer image\nsegmentation.\n", "link": "http://arxiv.org/abs/2411.15086v1", "date": "2024-11-22", "relevancy": 1.8802, "topK": [{"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.4883}, {"title": "Self-supervised Photographic Image Layout Representation Learning", "link": "http://arxiv.org/abs/2403.03740v1", "similarity": 0.4677}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.465}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Quantum-enhanced%20unsupervised%20image%20segmentation%20for%20medical%20images%0A%20%20analysis&body=Title%3A%20Quantum-enhanced%20unsupervised%20image%20segmentation%20for%20medical%20images%0A%20%20analysis%0AAuthor%3A%20Laia%20Domingo%20and%20Mahdi%20Chehimi%0AAbstract%3A%20%20%20Breast%20cancer%20remains%20the%20leading%20cause%20of%20cancer-related%20mortality%20among%0Awomen%20worldwide%2C%20necessitating%20the%20meticulous%20examination%20of%20mammograms%20by%0Aradiologists%20to%20characterize%20abnormal%20lesions.%20This%20manual%20process%20demands%20high%0Aaccuracy%20and%20is%20often%20time-consuming%2C%20costly%2C%20and%20error-prone.%20Automated%20image%0Asegmentation%20using%20artificial%20intelligence%20offers%20a%20promising%20alternative%20to%0Astreamline%20this%20workflow.%20However%2C%20most%20existing%20methods%20are%20supervised%2C%0Arequiring%20large%2C%20expertly%20annotated%20datasets%20that%20are%20not%20always%20available%2C%20and%0Athey%20experience%20significant%20generalization%20issues.%20Thus%2C%20unsupervised%20learning%0Amodels%20can%20be%20leveraged%20for%20image%20segmentation%2C%20but%20they%20come%20at%20a%20cost%20of%0Areduced%20accuracy%2C%20or%20require%20extensive%20computational%20resourcess.%20In%20this%20paper%2C%0Awe%20propose%20the%20first%20end-to-end%20quantum-enhanced%20framework%20for%20unsupervised%0Amammography%20medical%20images%20segmentation%20that%20balances%20between%20performance%0Aaccuracy%20and%20computational%20requirements.%20We%20first%20introduce%20a%20quantum-inspired%0Aimage%20representation%20that%20serves%20as%20an%20initial%20approximation%20of%20the%0Asegmentation%20mask.%20The%20segmentation%20task%20is%20then%20formulated%20as%20a%20QUBO%20problem%2C%0Aaiming%20to%20maximize%20the%20contrast%20between%20the%20background%20and%20the%20tumor%20region%0Awhile%20ensuring%20a%20cohesive%20segmentation%20mask%20with%20minimal%20connected%20components.%0AWe%20conduct%20an%20extensive%20evaluation%20of%20quantum%20and%20quantum-inspired%20methods%20for%0Aimage%20segmentation%2C%20demonstrating%20that%20quantum%20annealing%20and%20variational%0Aquantum%20circuits%20achieve%20performance%20comparable%20to%20classical%20optimization%0Atechniques.%20Notably%2C%20quantum%20annealing%20is%20shown%20to%20be%20an%20order%20of%20magnitude%0Afaster%20than%20the%20classical%20optimization%20method%20in%20our%20experiments.%20Our%20findings%0Ademonstrate%20that%20this%20framework%20achieves%20performance%20comparable%20to%0Astate-of-the-art%20supervised%20methods%2C%20including%20UNet-based%20architectures%2C%0Aoffering%20a%20viable%20unsupervised%20alternative%20for%20breast%20cancer%20image%0Asegmentation.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2411.15086v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DQuantum-enhanced%2520unsupervised%2520image%2520segmentation%2520for%2520medical%2520images%250A%2520%2520analysis%26entry.906535625%3DLaia%2520Domingo%2520and%2520Mahdi%2520Chehimi%26entry.1292438233%3D%2520%2520Breast%2520cancer%2520remains%2520the%2520leading%2520cause%2520of%2520cancer-related%2520mortality%2520among%250Awomen%2520worldwide%252C%2520necessitating%2520the%2520meticulous%2520examination%2520of%2520mammograms%2520by%250Aradiologists%2520to%2520characterize%2520abnormal%2520lesions.%2520This%2520manual%2520process%2520demands%2520high%250Aaccuracy%2520and%2520is%2520often%2520time-consuming%252C%2520costly%252C%2520and%2520error-prone.%2520Automated%2520image%250Asegmentation%2520using%2520artificial%2520intelligence%2520offers%2520a%2520promising%2520alternative%2520to%250Astreamline%2520this%2520workflow.%2520However%252C%2520most%2520existing%2520methods%2520are%2520supervised%252C%250Arequiring%2520large%252C%2520expertly%2520annotated%2520datasets%2520that%2520are%2520not%2520always%2520available%252C%2520and%250Athey%2520experience%2520significant%2520generalization%2520issues.%2520Thus%252C%2520unsupervised%2520learning%250Amodels%2520can%2520be%2520leveraged%2520for%2520image%2520segmentation%252C%2520but%2520they%2520come%2520at%2520a%2520cost%2520of%250Areduced%2520accuracy%252C%2520or%2520require%2520extensive%2520computational%2520resourcess.%2520In%2520this%2520paper%252C%250Awe%2520propose%2520the%2520first%2520end-to-end%2520quantum-enhanced%2520framework%2520for%2520unsupervised%250Amammography%2520medical%2520images%2520segmentation%2520that%2520balances%2520between%2520performance%250Aaccuracy%2520and%2520computational%2520requirements.%2520We%2520first%2520introduce%2520a%2520quantum-inspired%250Aimage%2520representation%2520that%2520serves%2520as%2520an%2520initial%2520approximation%2520of%2520the%250Asegmentation%2520mask.%2520The%2520segmentation%2520task%2520is%2520then%2520formulated%2520as%2520a%2520QUBO%2520problem%252C%250Aaiming%2520to%2520maximize%2520the%2520contrast%2520between%2520the%2520background%2520and%2520the%2520tumor%2520region%250Awhile%2520ensuring%2520a%2520cohesive%2520segmentation%2520mask%2520with%2520minimal%2520connected%2520components.%250AWe%2520conduct%2520an%2520extensive%2520evaluation%2520of%2520quantum%2520and%2520quantum-inspired%2520methods%2520for%250Aimage%2520segmentation%252C%2520demonstrating%2520that%2520quantum%2520annealing%2520and%2520variational%250Aquantum%2520circuits%2520achieve%2520performance%2520comparable%2520to%2520classical%2520optimization%250Atechniques.%2520Notably%252C%2520quantum%2520annealing%2520is%2520shown%2520to%2520be%2520an%2520order%2520of%2520magnitude%250Afaster%2520than%2520the%2520classical%2520optimization%2520method%2520in%2520our%2520experiments.%2520Our%2520findings%250Ademonstrate%2520that%2520this%2520framework%2520achieves%2520performance%2520comparable%2520to%250Astate-of-the-art%2520supervised%2520methods%252C%2520including%2520UNet-based%2520architectures%252C%250Aoffering%2520a%2520viable%2520unsupervised%2520alternative%2520for%2520breast%2520cancer%2520image%250Asegmentation.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2411.15086v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Quantum-enhanced%20unsupervised%20image%20segmentation%20for%20medical%20images%0A%20%20analysis&entry.906535625=Laia%20Domingo%20and%20Mahdi%20Chehimi&entry.1292438233=%20%20Breast%20cancer%20remains%20the%20leading%20cause%20of%20cancer-related%20mortality%20among%0Awomen%20worldwide%2C%20necessitating%20the%20meticulous%20examination%20of%20mammograms%20by%0Aradiologists%20to%20characterize%20abnormal%20lesions.%20This%20manual%20process%20demands%20high%0Aaccuracy%20and%20is%20often%20time-consuming%2C%20costly%2C%20and%20error-prone.%20Automated%20image%0Asegmentation%20using%20artificial%20intelligence%20offers%20a%20promising%20alternative%20to%0Astreamline%20this%20workflow.%20However%2C%20most%20existing%20methods%20are%20supervised%2C%0Arequiring%20large%2C%20expertly%20annotated%20datasets%20that%20are%20not%20always%20available%2C%20and%0Athey%20experience%20significant%20generalization%20issues.%20Thus%2C%20unsupervised%20learning%0Amodels%20can%20be%20leveraged%20for%20image%20segmentation%2C%20but%20they%20come%20at%20a%20cost%20of%0Areduced%20accuracy%2C%20or%20require%20extensive%20computational%20resourcess.%20In%20this%20paper%2C%0Awe%20propose%20the%20first%20end-to-end%20quantum-enhanced%20framework%20for%20unsupervised%0Amammography%20medical%20images%20segmentation%20that%20balances%20between%20performance%0Aaccuracy%20and%20computational%20requirements.%20We%20first%20introduce%20a%20quantum-inspired%0Aimage%20representation%20that%20serves%20as%20an%20initial%20approximation%20of%20the%0Asegmentation%20mask.%20The%20segmentation%20task%20is%20then%20formulated%20as%20a%20QUBO%20problem%2C%0Aaiming%20to%20maximize%20the%20contrast%20between%20the%20background%20and%20the%20tumor%20region%0Awhile%20ensuring%20a%20cohesive%20segmentation%20mask%20with%20minimal%20connected%20components.%0AWe%20conduct%20an%20extensive%20evaluation%20of%20quantum%20and%20quantum-inspired%20methods%20for%0Aimage%20segmentation%2C%20demonstrating%20that%20quantum%20annealing%20and%20variational%0Aquantum%20circuits%20achieve%20performance%20comparable%20to%20classical%20optimization%0Atechniques.%20Notably%2C%20quantum%20annealing%20is%20shown%20to%20be%20an%20order%20of%20magnitude%0Afaster%20than%20the%20classical%20optimization%20method%20in%20our%20experiments.%20Our%20findings%0Ademonstrate%20that%20this%20framework%20achieves%20performance%20comparable%20to%0Astate-of-the-art%20supervised%20methods%2C%20including%20UNet-based%20architectures%2C%0Aoffering%20a%20viable%20unsupervised%20alternative%20for%20breast%20cancer%20image%0Asegmentation.%0A&entry.1838667208=http%3A//arxiv.org/abs/2411.15086v1&entry.124074799=Read"},
{"title": "Interpretable Water Level Forecaster with Spatiotemporal Causal\n  Attention Mechanisms", "author": "Sunghcul Hong and Yunjin Choi and Jong-June Jeon", "abstract": "  Accurate forecasting of river water levels is vital for effectively managing\ntraffic flow and mitigating the risks associated with natural disasters. This\ntask presents challenges due to the intricate factors influencing the flow of a\nriver. Recent advances in machine learning have introduced numerous effective\nforecasting methods. However, these methods lack interpretability due to their\ncomplex structure, resulting in limited reliability. Addressing this issue,\nthis study proposes a deep learning model that quantifies interpretability,\nwith an emphasis on water level forecasting. This model focuses on generating\nquantitative interpretability measurements, which align with the common\nknowledge embedded in the input data. This is facilitated by the utilization of\na transformer architecture that is purposefully designed with masking,\nincorporating a multi-layer network that captures spatiotemporal causation. We\nperform a comparative analysis on the Han River dataset obtained from Seoul,\nSouth Korea, from 2016 to 2021. The results illustrate that our approach offers\nenhanced interpretability consistent with common knowledge, outperforming\ncompeting methods and also enhances robustness against distribution shift.\n", "link": "http://arxiv.org/abs/2303.00515v7", "date": "2024-11-22", "relevancy": 1.4814, "topK": [{"title": "Scaling Rectified Flow Transformers for High-Resolution Image Synthesis", "link": "http://arxiv.org/abs/2403.03206v1", "similarity": 0.5312}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.5055}, {"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.4742}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Interpretable%20Water%20Level%20Forecaster%20with%20Spatiotemporal%20Causal%0A%20%20Attention%20Mechanisms&body=Title%3A%20Interpretable%20Water%20Level%20Forecaster%20with%20Spatiotemporal%20Causal%0A%20%20Attention%20Mechanisms%0AAuthor%3A%20Sunghcul%20Hong%20and%20Yunjin%20Choi%20and%20Jong-June%20Jeon%0AAbstract%3A%20%20%20Accurate%20forecasting%20of%20river%20water%20levels%20is%20vital%20for%20effectively%20managing%0Atraffic%20flow%20and%20mitigating%20the%20risks%20associated%20with%20natural%20disasters.%20This%0Atask%20presents%20challenges%20due%20to%20the%20intricate%20factors%20influencing%20the%20flow%20of%20a%0Ariver.%20Recent%20advances%20in%20machine%20learning%20have%20introduced%20numerous%20effective%0Aforecasting%20methods.%20However%2C%20these%20methods%20lack%20interpretability%20due%20to%20their%0Acomplex%20structure%2C%20resulting%20in%20limited%20reliability.%20Addressing%20this%20issue%2C%0Athis%20study%20proposes%20a%20deep%20learning%20model%20that%20quantifies%20interpretability%2C%0Awith%20an%20emphasis%20on%20water%20level%20forecasting.%20This%20model%20focuses%20on%20generating%0Aquantitative%20interpretability%20measurements%2C%20which%20align%20with%20the%20common%0Aknowledge%20embedded%20in%20the%20input%20data.%20This%20is%20facilitated%20by%20the%20utilization%20of%0Aa%20transformer%20architecture%20that%20is%20purposefully%20designed%20with%20masking%2C%0Aincorporating%20a%20multi-layer%20network%20that%20captures%20spatiotemporal%20causation.%20We%0Aperform%20a%20comparative%20analysis%20on%20the%20Han%20River%20dataset%20obtained%20from%20Seoul%2C%0ASouth%20Korea%2C%20from%202016%20to%202021.%20The%20results%20illustrate%20that%20our%20approach%20offers%0Aenhanced%20interpretability%20consistent%20with%20common%20knowledge%2C%20outperforming%0Acompeting%20methods%20and%20also%20enhances%20robustness%20against%20distribution%20shift.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2303.00515v7%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DInterpretable%2520Water%2520Level%2520Forecaster%2520with%2520Spatiotemporal%2520Causal%250A%2520%2520Attention%2520Mechanisms%26entry.906535625%3DSunghcul%2520Hong%2520and%2520Yunjin%2520Choi%2520and%2520Jong-June%2520Jeon%26entry.1292438233%3D%2520%2520Accurate%2520forecasting%2520of%2520river%2520water%2520levels%2520is%2520vital%2520for%2520effectively%2520managing%250Atraffic%2520flow%2520and%2520mitigating%2520the%2520risks%2520associated%2520with%2520natural%2520disasters.%2520This%250Atask%2520presents%2520challenges%2520due%2520to%2520the%2520intricate%2520factors%2520influencing%2520the%2520flow%2520of%2520a%250Ariver.%2520Recent%2520advances%2520in%2520machine%2520learning%2520have%2520introduced%2520numerous%2520effective%250Aforecasting%2520methods.%2520However%252C%2520these%2520methods%2520lack%2520interpretability%2520due%2520to%2520their%250Acomplex%2520structure%252C%2520resulting%2520in%2520limited%2520reliability.%2520Addressing%2520this%2520issue%252C%250Athis%2520study%2520proposes%2520a%2520deep%2520learning%2520model%2520that%2520quantifies%2520interpretability%252C%250Awith%2520an%2520emphasis%2520on%2520water%2520level%2520forecasting.%2520This%2520model%2520focuses%2520on%2520generating%250Aquantitative%2520interpretability%2520measurements%252C%2520which%2520align%2520with%2520the%2520common%250Aknowledge%2520embedded%2520in%2520the%2520input%2520data.%2520This%2520is%2520facilitated%2520by%2520the%2520utilization%2520of%250Aa%2520transformer%2520architecture%2520that%2520is%2520purposefully%2520designed%2520with%2520masking%252C%250Aincorporating%2520a%2520multi-layer%2520network%2520that%2520captures%2520spatiotemporal%2520causation.%2520We%250Aperform%2520a%2520comparative%2520analysis%2520on%2520the%2520Han%2520River%2520dataset%2520obtained%2520from%2520Seoul%252C%250ASouth%2520Korea%252C%2520from%25202016%2520to%25202021.%2520The%2520results%2520illustrate%2520that%2520our%2520approach%2520offers%250Aenhanced%2520interpretability%2520consistent%2520with%2520common%2520knowledge%252C%2520outperforming%250Acompeting%2520methods%2520and%2520also%2520enhances%2520robustness%2520against%2520distribution%2520shift.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2303.00515v7%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Interpretable%20Water%20Level%20Forecaster%20with%20Spatiotemporal%20Causal%0A%20%20Attention%20Mechanisms&entry.906535625=Sunghcul%20Hong%20and%20Yunjin%20Choi%20and%20Jong-June%20Jeon&entry.1292438233=%20%20Accurate%20forecasting%20of%20river%20water%20levels%20is%20vital%20for%20effectively%20managing%0Atraffic%20flow%20and%20mitigating%20the%20risks%20associated%20with%20natural%20disasters.%20This%0Atask%20presents%20challenges%20due%20to%20the%20intricate%20factors%20influencing%20the%20flow%20of%20a%0Ariver.%20Recent%20advances%20in%20machine%20learning%20have%20introduced%20numerous%20effective%0Aforecasting%20methods.%20However%2C%20these%20methods%20lack%20interpretability%20due%20to%20their%0Acomplex%20structure%2C%20resulting%20in%20limited%20reliability.%20Addressing%20this%20issue%2C%0Athis%20study%20proposes%20a%20deep%20learning%20model%20that%20quantifies%20interpretability%2C%0Awith%20an%20emphasis%20on%20water%20level%20forecasting.%20This%20model%20focuses%20on%20generating%0Aquantitative%20interpretability%20measurements%2C%20which%20align%20with%20the%20common%0Aknowledge%20embedded%20in%20the%20input%20data.%20This%20is%20facilitated%20by%20the%20utilization%20of%0Aa%20transformer%20architecture%20that%20is%20purposefully%20designed%20with%20masking%2C%0Aincorporating%20a%20multi-layer%20network%20that%20captures%20spatiotemporal%20causation.%20We%0Aperform%20a%20comparative%20analysis%20on%20the%20Han%20River%20dataset%20obtained%20from%20Seoul%2C%0ASouth%20Korea%2C%20from%202016%20to%202021.%20The%20results%20illustrate%20that%20our%20approach%20offers%0Aenhanced%20interpretability%20consistent%20with%20common%20knowledge%2C%20outperforming%0Acompeting%20methods%20and%20also%20enhances%20robustness%20against%20distribution%20shift.%0A&entry.1838667208=http%3A//arxiv.org/abs/2303.00515v7&entry.124074799=Read"},
{"title": "Learning-based Trajectory Tracking for Bird-inspired Flapping-Wing\n  Robots", "author": "Jiaze Cai and Vishnu Sangli and Mintae Kim and Koushil Sreenath", "abstract": "  Bird-sized flapping-wing robots offer significant potential for agile flight\nin complex environments, but achieving agile and robust trajectory tracking\nremains a challenge due to the complex aerodynamics and highly nonlinear\ndynamics inherent in flapping-wing flight. In this work, a learning-based\ncontrol approach is introduced to unlock the versatility and adaptiveness of\nflapping-wing flight. We propose a model-free reinforcement learning (RL)-based\nframework for a high degree-of-freedom (DoF) bird-inspired flapping-wing robot\nthat allows for multimodal flight and agile trajectory tracking. Stability\nanalysis was performed on the closed-loop system comprising of the\nflapping-wing system and the RL policy. Additionally, simulation results\ndemonstrate that the RL-based controller can successfully learn complex wing\ntrajectory patterns, achieve stable flight, switch between flight modes\nspontaneously, and track different trajectories under various aerodynamic\nconditions.\n", "link": "http://arxiv.org/abs/2411.15130v1", "date": "2024-11-22", "relevancy": 1.4933, "topK": [{"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.554}, {"title": "Egocentric Computer Vision for Hands-Free Robotic Wheelchair Navigation", "link": "https://link.springer.com/article/10.1007/s10846-023-01807-4", "similarity": 0.482}, {"title": "RoboEXP: Action-Conditioned Scene Graph via Interactive Exploration for\n  Robotic Manipulation", "link": "http://arxiv.org/abs/2402.15487v1", "similarity": 0.481}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Learning-based%20Trajectory%20Tracking%20for%20Bird-inspired%20Flapping-Wing%0A%20%20Robots&body=Title%3A%20Learning-based%20Trajectory%20Tracking%20for%20Bird-inspired%20Flapping-Wing%0A%20%20Robots%0AAuthor%3A%20Jiaze%20Cai%20and%20Vishnu%20Sangli%20and%20Mintae%20Kim%20and%20Koushil%20Sreenath%0AAbstract%3A%20%20%20Bird-sized%20flapping-wing%20robots%20offer%20significant%20potential%20for%20agile%20flight%0Ain%20complex%20environments%2C%20but%20achieving%20agile%20and%20robust%20trajectory%20tracking%0Aremains%20a%20challenge%20due%20to%20the%20complex%20aerodynamics%20and%20highly%20nonlinear%0Adynamics%20inherent%20in%20flapping-wing%20flight.%20In%20this%20work%2C%20a%20learning-based%0Acontrol%20approach%20is%20introduced%20to%20unlock%20the%20versatility%20and%20adaptiveness%20of%0Aflapping-wing%20flight.%20We%20propose%20a%20model-free%20reinforcement%20learning%20%28RL%29-based%0Aframework%20for%20a%20high%20degree-of-freedom%20%28DoF%29%20bird-inspired%20flapping-wing%20robot%0Athat%20allows%20for%20multimodal%20flight%20and%20agile%20trajectory%20tracking.%20Stability%0Aanalysis%20was%20performed%20on%20the%20closed-loop%20system%20comprising%20of%20the%0Aflapping-wing%20system%20and%20the%20RL%20policy.%20Additionally%2C%20simulation%20results%0Ademonstrate%20that%20the%20RL-based%20controller%20can%20successfully%20learn%20complex%20wing%0Atrajectory%20patterns%2C%20achieve%20stable%20flight%2C%20switch%20between%20flight%20modes%0Aspontaneously%2C%20and%20track%20different%20trajectories%20under%20various%20aerodynamic%0Aconditions.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2411.15130v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DLearning-based%2520Trajectory%2520Tracking%2520for%2520Bird-inspired%2520Flapping-Wing%250A%2520%2520Robots%26entry.906535625%3DJiaze%2520Cai%2520and%2520Vishnu%2520Sangli%2520and%2520Mintae%2520Kim%2520and%2520Koushil%2520Sreenath%26entry.1292438233%3D%2520%2520Bird-sized%2520flapping-wing%2520robots%2520offer%2520significant%2520potential%2520for%2520agile%2520flight%250Ain%2520complex%2520environments%252C%2520but%2520achieving%2520agile%2520and%2520robust%2520trajectory%2520tracking%250Aremains%2520a%2520challenge%2520due%2520to%2520the%2520complex%2520aerodynamics%2520and%2520highly%2520nonlinear%250Adynamics%2520inherent%2520in%2520flapping-wing%2520flight.%2520In%2520this%2520work%252C%2520a%2520learning-based%250Acontrol%2520approach%2520is%2520introduced%2520to%2520unlock%2520the%2520versatility%2520and%2520adaptiveness%2520of%250Aflapping-wing%2520flight.%2520We%2520propose%2520a%2520model-free%2520reinforcement%2520learning%2520%2528RL%2529-based%250Aframework%2520for%2520a%2520high%2520degree-of-freedom%2520%2528DoF%2529%2520bird-inspired%2520flapping-wing%2520robot%250Athat%2520allows%2520for%2520multimodal%2520flight%2520and%2520agile%2520trajectory%2520tracking.%2520Stability%250Aanalysis%2520was%2520performed%2520on%2520the%2520closed-loop%2520system%2520comprising%2520of%2520the%250Aflapping-wing%2520system%2520and%2520the%2520RL%2520policy.%2520Additionally%252C%2520simulation%2520results%250Ademonstrate%2520that%2520the%2520RL-based%2520controller%2520can%2520successfully%2520learn%2520complex%2520wing%250Atrajectory%2520patterns%252C%2520achieve%2520stable%2520flight%252C%2520switch%2520between%2520flight%2520modes%250Aspontaneously%252C%2520and%2520track%2520different%2520trajectories%2520under%2520various%2520aerodynamic%250Aconditions.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2411.15130v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Learning-based%20Trajectory%20Tracking%20for%20Bird-inspired%20Flapping-Wing%0A%20%20Robots&entry.906535625=Jiaze%20Cai%20and%20Vishnu%20Sangli%20and%20Mintae%20Kim%20and%20Koushil%20Sreenath&entry.1292438233=%20%20Bird-sized%20flapping-wing%20robots%20offer%20significant%20potential%20for%20agile%20flight%0Ain%20complex%20environments%2C%20but%20achieving%20agile%20and%20robust%20trajectory%20tracking%0Aremains%20a%20challenge%20due%20to%20the%20complex%20aerodynamics%20and%20highly%20nonlinear%0Adynamics%20inherent%20in%20flapping-wing%20flight.%20In%20this%20work%2C%20a%20learning-based%0Acontrol%20approach%20is%20introduced%20to%20unlock%20the%20versatility%20and%20adaptiveness%20of%0Aflapping-wing%20flight.%20We%20propose%20a%20model-free%20reinforcement%20learning%20%28RL%29-based%0Aframework%20for%20a%20high%20degree-of-freedom%20%28DoF%29%20bird-inspired%20flapping-wing%20robot%0Athat%20allows%20for%20multimodal%20flight%20and%20agile%20trajectory%20tracking.%20Stability%0Aanalysis%20was%20performed%20on%20the%20closed-loop%20system%20comprising%20of%20the%0Aflapping-wing%20system%20and%20the%20RL%20policy.%20Additionally%2C%20simulation%20results%0Ademonstrate%20that%20the%20RL-based%20controller%20can%20successfully%20learn%20complex%20wing%0Atrajectory%20patterns%2C%20achieve%20stable%20flight%2C%20switch%20between%20flight%20modes%0Aspontaneously%2C%20and%20track%20different%20trajectories%20under%20various%20aerodynamic%0Aconditions.%0A&entry.1838667208=http%3A//arxiv.org/abs/2411.15130v1&entry.124074799=Read"},
{"title": "Adaptive Group Robust Ensemble Knowledge Distillation", "author": "Patrik Kenfack and Ulrich A\u00efvodji and Samira Ebrahimi Kahou", "abstract": "  Neural networks can learn spurious correlations in the data, often leading to\nperformance disparity for underrepresented subgroups. Studies have demonstrated\nthat the disparity is amplified when knowledge is distilled from a complex\nteacher model to a relatively \"simple\" student model. Prior work has shown that\nensemble deep learning methods can improve the performance of the worst-case\nsubgroups; however, it is unclear if this advantage carries over when\ndistilling knowledge from an ensemble of teachers, especially when the teacher\nmodels are debiased. This study demonstrates that traditional ensemble\nknowledge distillation can significantly drop the performance of the worst-case\nsubgroups in the distilled student model even when the teacher models are\ndebiased. To overcome this, we propose Adaptive Group Robust Ensemble Knowledge\nDistillation (AGRE-KD), a simple ensembling strategy to ensure that the student\nmodel receives knowledge beneficial for unknown underrepresented subgroups.\nLeveraging an additional biased model, our method selectively chooses teachers\nwhose knowledge would better improve the worst-performing subgroups by\nupweighting the teachers with gradient directions deviating from the biased\nmodel. Our experiments on several datasets demonstrate the superiority of the\nproposed ensemble distillation technique and show that it can even outperform\nclassic model ensembles based on majority voting.\n", "link": "http://arxiv.org/abs/2411.14984v1", "date": "2024-11-22", "relevancy": 1.4421, "topK": [{"title": "Boosted dynamic neural networks", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/26302", "similarity": 0.502}, {"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.471}, {"title": "Thinking Outside the BBox: Unconstrained Generative Object Compositing", "link": "http://arxiv.org/abs/2409.04559v2", "similarity": 0.4372}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Adaptive%20Group%20Robust%20Ensemble%20Knowledge%20Distillation&body=Title%3A%20Adaptive%20Group%20Robust%20Ensemble%20Knowledge%20Distillation%0AAuthor%3A%20Patrik%20Kenfack%20and%20Ulrich%20A%C3%AFvodji%20and%20Samira%20Ebrahimi%20Kahou%0AAbstract%3A%20%20%20Neural%20networks%20can%20learn%20spurious%20correlations%20in%20the%20data%2C%20often%20leading%20to%0Aperformance%20disparity%20for%20underrepresented%20subgroups.%20Studies%20have%20demonstrated%0Athat%20the%20disparity%20is%20amplified%20when%20knowledge%20is%20distilled%20from%20a%20complex%0Ateacher%20model%20to%20a%20relatively%20%22simple%22%20student%20model.%20Prior%20work%20has%20shown%20that%0Aensemble%20deep%20learning%20methods%20can%20improve%20the%20performance%20of%20the%20worst-case%0Asubgroups%3B%20however%2C%20it%20is%20unclear%20if%20this%20advantage%20carries%20over%20when%0Adistilling%20knowledge%20from%20an%20ensemble%20of%20teachers%2C%20especially%20when%20the%20teacher%0Amodels%20are%20debiased.%20This%20study%20demonstrates%20that%20traditional%20ensemble%0Aknowledge%20distillation%20can%20significantly%20drop%20the%20performance%20of%20the%20worst-case%0Asubgroups%20in%20the%20distilled%20student%20model%20even%20when%20the%20teacher%20models%20are%0Adebiased.%20To%20overcome%20this%2C%20we%20propose%20Adaptive%20Group%20Robust%20Ensemble%20Knowledge%0ADistillation%20%28AGRE-KD%29%2C%20a%20simple%20ensembling%20strategy%20to%20ensure%20that%20the%20student%0Amodel%20receives%20knowledge%20beneficial%20for%20unknown%20underrepresented%20subgroups.%0ALeveraging%20an%20additional%20biased%20model%2C%20our%20method%20selectively%20chooses%20teachers%0Awhose%20knowledge%20would%20better%20improve%20the%20worst-performing%20subgroups%20by%0Aupweighting%20the%20teachers%20with%20gradient%20directions%20deviating%20from%20the%20biased%0Amodel.%20Our%20experiments%20on%20several%20datasets%20demonstrate%20the%20superiority%20of%20the%0Aproposed%20ensemble%20distillation%20technique%20and%20show%20that%20it%20can%20even%20outperform%0Aclassic%20model%20ensembles%20based%20on%20majority%20voting.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2411.14984v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DAdaptive%2520Group%2520Robust%2520Ensemble%2520Knowledge%2520Distillation%26entry.906535625%3DPatrik%2520Kenfack%2520and%2520Ulrich%2520A%25C3%25AFvodji%2520and%2520Samira%2520Ebrahimi%2520Kahou%26entry.1292438233%3D%2520%2520Neural%2520networks%2520can%2520learn%2520spurious%2520correlations%2520in%2520the%2520data%252C%2520often%2520leading%2520to%250Aperformance%2520disparity%2520for%2520underrepresented%2520subgroups.%2520Studies%2520have%2520demonstrated%250Athat%2520the%2520disparity%2520is%2520amplified%2520when%2520knowledge%2520is%2520distilled%2520from%2520a%2520complex%250Ateacher%2520model%2520to%2520a%2520relatively%2520%2522simple%2522%2520student%2520model.%2520Prior%2520work%2520has%2520shown%2520that%250Aensemble%2520deep%2520learning%2520methods%2520can%2520improve%2520the%2520performance%2520of%2520the%2520worst-case%250Asubgroups%253B%2520however%252C%2520it%2520is%2520unclear%2520if%2520this%2520advantage%2520carries%2520over%2520when%250Adistilling%2520knowledge%2520from%2520an%2520ensemble%2520of%2520teachers%252C%2520especially%2520when%2520the%2520teacher%250Amodels%2520are%2520debiased.%2520This%2520study%2520demonstrates%2520that%2520traditional%2520ensemble%250Aknowledge%2520distillation%2520can%2520significantly%2520drop%2520the%2520performance%2520of%2520the%2520worst-case%250Asubgroups%2520in%2520the%2520distilled%2520student%2520model%2520even%2520when%2520the%2520teacher%2520models%2520are%250Adebiased.%2520To%2520overcome%2520this%252C%2520we%2520propose%2520Adaptive%2520Group%2520Robust%2520Ensemble%2520Knowledge%250ADistillation%2520%2528AGRE-KD%2529%252C%2520a%2520simple%2520ensembling%2520strategy%2520to%2520ensure%2520that%2520the%2520student%250Amodel%2520receives%2520knowledge%2520beneficial%2520for%2520unknown%2520underrepresented%2520subgroups.%250ALeveraging%2520an%2520additional%2520biased%2520model%252C%2520our%2520method%2520selectively%2520chooses%2520teachers%250Awhose%2520knowledge%2520would%2520better%2520improve%2520the%2520worst-performing%2520subgroups%2520by%250Aupweighting%2520the%2520teachers%2520with%2520gradient%2520directions%2520deviating%2520from%2520the%2520biased%250Amodel.%2520Our%2520experiments%2520on%2520several%2520datasets%2520demonstrate%2520the%2520superiority%2520of%2520the%250Aproposed%2520ensemble%2520distillation%2520technique%2520and%2520show%2520that%2520it%2520can%2520even%2520outperform%250Aclassic%2520model%2520ensembles%2520based%2520on%2520majority%2520voting.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2411.14984v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Adaptive%20Group%20Robust%20Ensemble%20Knowledge%20Distillation&entry.906535625=Patrik%20Kenfack%20and%20Ulrich%20A%C3%AFvodji%20and%20Samira%20Ebrahimi%20Kahou&entry.1292438233=%20%20Neural%20networks%20can%20learn%20spurious%20correlations%20in%20the%20data%2C%20often%20leading%20to%0Aperformance%20disparity%20for%20underrepresented%20subgroups.%20Studies%20have%20demonstrated%0Athat%20the%20disparity%20is%20amplified%20when%20knowledge%20is%20distilled%20from%20a%20complex%0Ateacher%20model%20to%20a%20relatively%20%22simple%22%20student%20model.%20Prior%20work%20has%20shown%20that%0Aensemble%20deep%20learning%20methods%20can%20improve%20the%20performance%20of%20the%20worst-case%0Asubgroups%3B%20however%2C%20it%20is%20unclear%20if%20this%20advantage%20carries%20over%20when%0Adistilling%20knowledge%20from%20an%20ensemble%20of%20teachers%2C%20especially%20when%20the%20teacher%0Amodels%20are%20debiased.%20This%20study%20demonstrates%20that%20traditional%20ensemble%0Aknowledge%20distillation%20can%20significantly%20drop%20the%20performance%20of%20the%20worst-case%0Asubgroups%20in%20the%20distilled%20student%20model%20even%20when%20the%20teacher%20models%20are%0Adebiased.%20To%20overcome%20this%2C%20we%20propose%20Adaptive%20Group%20Robust%20Ensemble%20Knowledge%0ADistillation%20%28AGRE-KD%29%2C%20a%20simple%20ensembling%20strategy%20to%20ensure%20that%20the%20student%0Amodel%20receives%20knowledge%20beneficial%20for%20unknown%20underrepresented%20subgroups.%0ALeveraging%20an%20additional%20biased%20model%2C%20our%20method%20selectively%20chooses%20teachers%0Awhose%20knowledge%20would%20better%20improve%20the%20worst-performing%20subgroups%20by%0Aupweighting%20the%20teachers%20with%20gradient%20directions%20deviating%20from%20the%20biased%0Amodel.%20Our%20experiments%20on%20several%20datasets%20demonstrate%20the%20superiority%20of%20the%0Aproposed%20ensemble%20distillation%20technique%20and%20show%20that%20it%20can%20even%20outperform%0Aclassic%20model%20ensembles%20based%20on%20majority%20voting.%0A&entry.1838667208=http%3A//arxiv.org/abs/2411.14984v1&entry.124074799=Read"},
{"title": "From Novice to Expert: LLM Agent Policy Optimization via Step-wise\n  Reinforcement Learning", "author": "Zhirui Deng and Zhicheng Dou and Yutao Zhu and Ji-Rong Wen and Ruibin Xiong and Mang Wang and Weipeng Chen", "abstract": "  The outstanding capabilities of large language models (LLMs) render them a\ncrucial component in various autonomous agent systems. While traditional\nmethods depend on the inherent knowledge of LLMs without fine-tuning, more\nrecent approaches have shifted toward the reinforcement learning strategy to\nfurther enhance agents' ability to solve complex interactive tasks with\nenvironments and tools. However, previous approaches are constrained by the\nsparse reward issue, where existing datasets solely provide a final scalar\nreward for each multi-step reasoning chain, potentially leading to\nineffectiveness and inefficiency in policy learning. In this paper, we\nintroduce StepAgent, which utilizes step-wise reward to optimize the agent's\nreinforcement learning process. Inheriting the spirit of novice-to-expert\ntheory, we first compare the actions of the expert and the agent to\nautomatically generate intermediate rewards for fine-grained optimization.\nAdditionally, we propose implicit-reward and inverse reinforcement learning\ntechniques to facilitate agent reflection and policy adjustment. Further\ntheoretical analysis demonstrates that the action distribution of the agent can\nconverge toward the expert action distribution over multiple training cycles.\nExperimental results across various datasets indicate that StepAgent\noutperforms existing baseline methods.\n", "link": "http://arxiv.org/abs/2411.03817v2", "date": "2024-11-22", "relevancy": 1.6314, "topK": [{"title": "Safe and efficient training of a control agent", "link": "https://patents.google.com/patent/US11709462B2/en", "similarity": 0.5656}, {"title": "RoboEXP: Action-Conditioned Scene Graph via Interactive Exploration for\n  Robotic Manipulation", "link": "http://arxiv.org/abs/2402.15487v1", "similarity": 0.5299}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.5033}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20From%20Novice%20to%20Expert%3A%20LLM%20Agent%20Policy%20Optimization%20via%20Step-wise%0A%20%20Reinforcement%20Learning&body=Title%3A%20From%20Novice%20to%20Expert%3A%20LLM%20Agent%20Policy%20Optimization%20via%20Step-wise%0A%20%20Reinforcement%20Learning%0AAuthor%3A%20Zhirui%20Deng%20and%20Zhicheng%20Dou%20and%20Yutao%20Zhu%20and%20Ji-Rong%20Wen%20and%20Ruibin%20Xiong%20and%20Mang%20Wang%20and%20Weipeng%20Chen%0AAbstract%3A%20%20%20The%20outstanding%20capabilities%20of%20large%20language%20models%20%28LLMs%29%20render%20them%20a%0Acrucial%20component%20in%20various%20autonomous%20agent%20systems.%20While%20traditional%0Amethods%20depend%20on%20the%20inherent%20knowledge%20of%20LLMs%20without%20fine-tuning%2C%20more%0Arecent%20approaches%20have%20shifted%20toward%20the%20reinforcement%20learning%20strategy%20to%0Afurther%20enhance%20agents%27%20ability%20to%20solve%20complex%20interactive%20tasks%20with%0Aenvironments%20and%20tools.%20However%2C%20previous%20approaches%20are%20constrained%20by%20the%0Asparse%20reward%20issue%2C%20where%20existing%20datasets%20solely%20provide%20a%20final%20scalar%0Areward%20for%20each%20multi-step%20reasoning%20chain%2C%20potentially%20leading%20to%0Aineffectiveness%20and%20inefficiency%20in%20policy%20learning.%20In%20this%20paper%2C%20we%0Aintroduce%20StepAgent%2C%20which%20utilizes%20step-wise%20reward%20to%20optimize%20the%20agent%27s%0Areinforcement%20learning%20process.%20Inheriting%20the%20spirit%20of%20novice-to-expert%0Atheory%2C%20we%20first%20compare%20the%20actions%20of%20the%20expert%20and%20the%20agent%20to%0Aautomatically%20generate%20intermediate%20rewards%20for%20fine-grained%20optimization.%0AAdditionally%2C%20we%20propose%20implicit-reward%20and%20inverse%20reinforcement%20learning%0Atechniques%20to%20facilitate%20agent%20reflection%20and%20policy%20adjustment.%20Further%0Atheoretical%20analysis%20demonstrates%20that%20the%20action%20distribution%20of%20the%20agent%20can%0Aconverge%20toward%20the%20expert%20action%20distribution%20over%20multiple%20training%20cycles.%0AExperimental%20results%20across%20various%20datasets%20indicate%20that%20StepAgent%0Aoutperforms%20existing%20baseline%20methods.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2411.03817v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DFrom%2520Novice%2520to%2520Expert%253A%2520LLM%2520Agent%2520Policy%2520Optimization%2520via%2520Step-wise%250A%2520%2520Reinforcement%2520Learning%26entry.906535625%3DZhirui%2520Deng%2520and%2520Zhicheng%2520Dou%2520and%2520Yutao%2520Zhu%2520and%2520Ji-Rong%2520Wen%2520and%2520Ruibin%2520Xiong%2520and%2520Mang%2520Wang%2520and%2520Weipeng%2520Chen%26entry.1292438233%3D%2520%2520The%2520outstanding%2520capabilities%2520of%2520large%2520language%2520models%2520%2528LLMs%2529%2520render%2520them%2520a%250Acrucial%2520component%2520in%2520various%2520autonomous%2520agent%2520systems.%2520While%2520traditional%250Amethods%2520depend%2520on%2520the%2520inherent%2520knowledge%2520of%2520LLMs%2520without%2520fine-tuning%252C%2520more%250Arecent%2520approaches%2520have%2520shifted%2520toward%2520the%2520reinforcement%2520learning%2520strategy%2520to%250Afurther%2520enhance%2520agents%2527%2520ability%2520to%2520solve%2520complex%2520interactive%2520tasks%2520with%250Aenvironments%2520and%2520tools.%2520However%252C%2520previous%2520approaches%2520are%2520constrained%2520by%2520the%250Asparse%2520reward%2520issue%252C%2520where%2520existing%2520datasets%2520solely%2520provide%2520a%2520final%2520scalar%250Areward%2520for%2520each%2520multi-step%2520reasoning%2520chain%252C%2520potentially%2520leading%2520to%250Aineffectiveness%2520and%2520inefficiency%2520in%2520policy%2520learning.%2520In%2520this%2520paper%252C%2520we%250Aintroduce%2520StepAgent%252C%2520which%2520utilizes%2520step-wise%2520reward%2520to%2520optimize%2520the%2520agent%2527s%250Areinforcement%2520learning%2520process.%2520Inheriting%2520the%2520spirit%2520of%2520novice-to-expert%250Atheory%252C%2520we%2520first%2520compare%2520the%2520actions%2520of%2520the%2520expert%2520and%2520the%2520agent%2520to%250Aautomatically%2520generate%2520intermediate%2520rewards%2520for%2520fine-grained%2520optimization.%250AAdditionally%252C%2520we%2520propose%2520implicit-reward%2520and%2520inverse%2520reinforcement%2520learning%250Atechniques%2520to%2520facilitate%2520agent%2520reflection%2520and%2520policy%2520adjustment.%2520Further%250Atheoretical%2520analysis%2520demonstrates%2520that%2520the%2520action%2520distribution%2520of%2520the%2520agent%2520can%250Aconverge%2520toward%2520the%2520expert%2520action%2520distribution%2520over%2520multiple%2520training%2520cycles.%250AExperimental%2520results%2520across%2520various%2520datasets%2520indicate%2520that%2520StepAgent%250Aoutperforms%2520existing%2520baseline%2520methods.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2411.03817v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=From%20Novice%20to%20Expert%3A%20LLM%20Agent%20Policy%20Optimization%20via%20Step-wise%0A%20%20Reinforcement%20Learning&entry.906535625=Zhirui%20Deng%20and%20Zhicheng%20Dou%20and%20Yutao%20Zhu%20and%20Ji-Rong%20Wen%20and%20Ruibin%20Xiong%20and%20Mang%20Wang%20and%20Weipeng%20Chen&entry.1292438233=%20%20The%20outstanding%20capabilities%20of%20large%20language%20models%20%28LLMs%29%20render%20them%20a%0Acrucial%20component%20in%20various%20autonomous%20agent%20systems.%20While%20traditional%0Amethods%20depend%20on%20the%20inherent%20knowledge%20of%20LLMs%20without%20fine-tuning%2C%20more%0Arecent%20approaches%20have%20shifted%20toward%20the%20reinforcement%20learning%20strategy%20to%0Afurther%20enhance%20agents%27%20ability%20to%20solve%20complex%20interactive%20tasks%20with%0Aenvironments%20and%20tools.%20However%2C%20previous%20approaches%20are%20constrained%20by%20the%0Asparse%20reward%20issue%2C%20where%20existing%20datasets%20solely%20provide%20a%20final%20scalar%0Areward%20for%20each%20multi-step%20reasoning%20chain%2C%20potentially%20leading%20to%0Aineffectiveness%20and%20inefficiency%20in%20policy%20learning.%20In%20this%20paper%2C%20we%0Aintroduce%20StepAgent%2C%20which%20utilizes%20step-wise%20reward%20to%20optimize%20the%20agent%27s%0Areinforcement%20learning%20process.%20Inheriting%20the%20spirit%20of%20novice-to-expert%0Atheory%2C%20we%20first%20compare%20the%20actions%20of%20the%20expert%20and%20the%20agent%20to%0Aautomatically%20generate%20intermediate%20rewards%20for%20fine-grained%20optimization.%0AAdditionally%2C%20we%20propose%20implicit-reward%20and%20inverse%20reinforcement%20learning%0Atechniques%20to%20facilitate%20agent%20reflection%20and%20policy%20adjustment.%20Further%0Atheoretical%20analysis%20demonstrates%20that%20the%20action%20distribution%20of%20the%20agent%20can%0Aconverge%20toward%20the%20expert%20action%20distribution%20over%20multiple%20training%20cycles.%0AExperimental%20results%20across%20various%20datasets%20indicate%20that%20StepAgent%0Aoutperforms%20existing%20baseline%20methods.%0A&entry.1838667208=http%3A//arxiv.org/abs/2411.03817v2&entry.124074799=Read"},
      ];
      const content = document.getElementById('content');
      function createPostElement(post) {
        const postElement = document.createElement('div');
        postElement.className = 'post';
        const dateElem = document.createElement('p');
        dateElem.setAttribute("class", "date");
        dateElem.textContent = post.date;
        postElement.appendChild(dateElem);

        const textElem = document.createElement('p');
        textElem.setAttribute("class", "text");
        const titleElem = document.createElement('p');
        titleElem.setAttribute("class", "title");
        titleElem.textContent = post.title;
        textElem.appendChild(titleElem);
        const authorElem = document.createElement('p');
        authorElem.setAttribute("class", "author");
        authorElem.textContent = post.author;
        textElem.appendChild(authorElem);
        const abstractElem = document.createElement('p');
        abstractElem.setAttribute("class", "abstract");
        abstractElem.textContent = post.abstract;
        textElem.appendChild(abstractElem);

        const linkElement = document.createElement('a');
        linkElement.setAttribute("class", "link");
        linkElement.href = post.link;
        linkElement.target = "_blank";
        linkElement.textContent = post.link.length > 50 ? post.link.substring(0, 50) + '...' : post.link;
        textElem.appendChild(linkElement);
        postElement.appendChild(textElem);

        const linkElementContainer = document.createElement('div');
        linkElementContainer.setAttribute("class", "comment");
        const actionElement = document.createElement('a');
        actionElement.setAttribute("class", "comment");
        actionElement.href = post.form;
        actionElement.textContent = "Action";
        actionElement.target = "_blank";
        linkElementContainer.appendChild(actionElement);
        const emailElement = document.createElement('a');
        emailElement.setAttribute("class", "comment");
        emailElement.href = post.mailto;
        emailElement.textContent = "Email";
        emailElement.target = "_blank";
        linkElementContainer.appendChild(emailElement);
        postElement.appendChild(linkElementContainer);
        const e = document.createElement('div');
        e.setAttribute("class", "clear");
        postElement.appendChild(e);

        const relevancyContainer = document.createElement('div');
        const relevancyValElem = document.createElement('p');
        relevancyValElem.textContent = "Relevancy " + post.relevancy;
        relevancyContainer.appendChild(relevancyValElem);
        post.topK.forEach((sub) => {
          const topKElem = document.createElement('a');
          topKElem.setAttribute("class", "topK");
          topKElem.href = sub.link;
          topKElem.textContent = sub.title + " (" + sub.similarity + ")";
          topKElem.target = "_blank";
          relevancyContainer.appendChild(topKElem);
        });
        postElement.appendChild(relevancyContainer);
        return postElement;
      }
      function loadPosts() {
        // Simulate loading more posts
        posts.forEach((post) => {
          const postElement = createPostElement(post);
          content.appendChild(postElement);
        });
      }
      // Load initial posts
      loadPosts();
    </script>

  </body>
</html>


