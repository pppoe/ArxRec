<!DOCTYPE html>
<html lang="en">
  <head>
    <script async src="https://www.googletagmanager.com/gtag/js?id=G-V34CNNDP8V"></script>
    <script>
      window.dataLayer = window.dataLayer || [];
      function gtag(){dataLayer.push(arguments);}
      gtag('js', new Date());
      gtag('config', 'G-V34CNNDP8V');
    </script>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Arxiv Paper Selection</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.3/css/all.min.css" crossorigin="anonymous" referrerpolicy="no-referrer" />
    <style>
    body {
      font-family: Arial, sans-serif;
      margin: 0;
      padding: 0;
      background-color: #f4f4f4;
    }
    header {
      position: fixed;
      top: 0;
      left: 0;
      width: 100%;
      background-color: #ffffff;
      color: black;
      padding: 10px;
      text-align: center;
      z-index: 1000;
      border-bottom: 1px solid #ddd;
    }
    header div {
      display: block;
      margin: 10px auto;
    }

    #home-icon {
      display: block;
      float: left;
      margin: 5px;
      text-decoration: none;
      color: black;
    }

    main {
      margin-top: 60px; /* Adjusted margin to account for fixed header */
      padding: 20px;
    }

    .post {
      background-color: white;
      border: 1px solid #ddd;
      border-radius: 5px;
      margin-bottom: 10px;
      padding: 10px 20px;
      max-height: 2000px;
      overflow: scroll;
    }
    .post img {
      display: block;
      margin-top: 5px;
      max-width: auto;
      max-height: 100px;
    }
    .post .clear {
      clear: both;
      display: block;
    }
    .post a {
      text-decoration: none;
    }
    .post a:hover {
      color: #0056b3;
    }
    .post a:visited {
      color: #0056b3;
    }
    .post div.comment {
      text-align: right;
    }
    .post div.comment a {
      margin: 1em;
    }
    .post .text {
      margin: 1em 0em;
      padding: 0;
    }
    .post .text .title {
    }
    .post .text .author {
    }
    .post .text .abstract {
    }
    .post .topK {
      display: block;
      margin: 0.5em;
    }
    .post .date {
      margin: 0;
      padding: 0;
      text-size: small; 
      color: gray;
    }
    .post .link {
      margin: 0;
      padding: 0;
    }
    @media screen and (max-width: 600px) {
      body {
        max-width: 100%; 
      }
      #home-icon {
        float: none;
        display: block;
        text-align: center;
        margin-bottom: 10px;
      }
    }
    footer {
      width: 100%;
      background-color: #ddd;
      text-align: center;
      z-index: 1000;
      padding: 20px 0px;
      margin-bottom: 20px;
      left: 0;
    }

    #next-btn,
    #prev-btn {
      background-color: #4CAF50;
      color: white;
      padding: 8px 16px;
      margin: 0 50px;
      border: none;
      border-radius: 4px;
      cursor: pointer;
    }

    .links {
      padding: 20px;
    }
    .links a {
      text-decoration: none;
    }
    .links a:hover {
      color: #0056b3;
    }
    .links a:visited {
      color: #0056b3;
    }

    #page-index {
      font-size: small;
    }
    .ads {
      width: 100%;
    }
    #prev-btn {
      background-color: #4CAF50;
      color: white;
      padding: 8px 16px;
      margin: 0 50px;
      border: none;
      border-radius: 4px;
      cursor: pointer;
    }
    </style>
  </head>
  <body>

    <header>
      <a id="prev-btn" href="daily20250107.html"><i class="fas fa-chevron-left"></i></a>
      <a href="https://haoxiang.org/">About</a>
    </header>

    <main id="content">
      <!-- Posts will be dynamically added here using JavaScript -->
    </main>

    <script>
      // Dummy data for posts
      const posts = [
{"title": "MoDec-GS: Global-to-Local Motion Decomposition and Temporal Interval\n  Adjustment for Compact Dynamic 3D Gaussian Splatting", "author": "Sangwoon Kwak and Joonsoo Kim and Jun Young Jeong and Won-Sik Cheong and Jihyong Oh and Munchurl Kim", "abstract": "  3D Gaussian Splatting (3DGS) has made significant strides in scene\nrepresentation and neural rendering, with intense efforts focused on adapting\nit for dynamic scenes. Despite delivering remarkable rendering quality and\nspeed, existing methods struggle with storage demands and representing complex\nreal-world motions. To tackle these issues, we propose MoDecGS, a\nmemory-efficient Gaussian splatting framework designed for reconstructing novel\nviews in challenging scenarios with complex motions. We introduce GlobaltoLocal\nMotion Decomposition (GLMD) to effectively capture dynamic motions in a\ncoarsetofine manner. This approach leverages Global Canonical Scaffolds (Global\nCS) and Local Canonical Scaffolds (Local CS), extending static Scaffold\nrepresentation to dynamic video reconstruction. For Global CS, we propose\nGlobal Anchor Deformation (GAD) to efficiently represent global dynamics along\ncomplex motions, by directly deforming the implicit Scaffold attributes which\nare anchor position, offset, and local context features. Next, we finely adjust\nlocal motions via the Local Gaussian Deformation (LGD) of Local CS explicitly.\nAdditionally, we introduce Temporal Interval Adjustment (TIA) to automatically\ncontrol the temporal coverage of each Local CS during training, allowing\nMoDecGS to find optimal interval assignments based on the specified number of\ntemporal segments. Extensive evaluations demonstrate that MoDecGS achieves an\naverage 70% reduction in model size over stateoftheart methods for dynamic 3D\nGaussians from realworld dynamic videos while maintaining or even improving\nrendering quality.\n", "link": "http://arxiv.org/abs/2501.03714v1", "date": "2025-01-07", "relevancy": 3.4578, "topK": [{"title": "Dynamic Gaussians Mesh: Consistent Mesh Reconstruction from Monocular\n  Videos", "link": "http://arxiv.org/abs/2404.12379v2", "similarity": 0.729}, {"title": "RTG-SLAM: Real-time 3D Reconstruction at Scale using Gaussian Splatting", "link": "http://arxiv.org/abs/2404.19706v1", "similarity": 0.6844}, {"title": "GST: Precise 3D Human Body from a Single Image with Gaussian Splatting\n  Transformers", "link": "http://arxiv.org/abs/2409.04196v1", "similarity": 0.6613}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20MoDec-GS%3A%20Global-to-Local%20Motion%20Decomposition%20and%20Temporal%20Interval%0A%20%20Adjustment%20for%20Compact%20Dynamic%203D%20Gaussian%20Splatting&body=Title%3A%20MoDec-GS%3A%20Global-to-Local%20Motion%20Decomposition%20and%20Temporal%20Interval%0A%20%20Adjustment%20for%20Compact%20Dynamic%203D%20Gaussian%20Splatting%0AAuthor%3A%20Sangwoon%20Kwak%20and%20Joonsoo%20Kim%20and%20Jun%20Young%20Jeong%20and%20Won-Sik%20Cheong%20and%20Jihyong%20Oh%20and%20Munchurl%20Kim%0AAbstract%3A%20%20%203D%20Gaussian%20Splatting%20%283DGS%29%20has%20made%20significant%20strides%20in%20scene%0Arepresentation%20and%20neural%20rendering%2C%20with%20intense%20efforts%20focused%20on%20adapting%0Ait%20for%20dynamic%20scenes.%20Despite%20delivering%20remarkable%20rendering%20quality%20and%0Aspeed%2C%20existing%20methods%20struggle%20with%20storage%20demands%20and%20representing%20complex%0Areal-world%20motions.%20To%20tackle%20these%20issues%2C%20we%20propose%20MoDecGS%2C%20a%0Amemory-efficient%20Gaussian%20splatting%20framework%20designed%20for%20reconstructing%20novel%0Aviews%20in%20challenging%20scenarios%20with%20complex%20motions.%20We%20introduce%20GlobaltoLocal%0AMotion%20Decomposition%20%28GLMD%29%20to%20effectively%20capture%20dynamic%20motions%20in%20a%0Acoarsetofine%20manner.%20This%20approach%20leverages%20Global%20Canonical%20Scaffolds%20%28Global%0ACS%29%20and%20Local%20Canonical%20Scaffolds%20%28Local%20CS%29%2C%20extending%20static%20Scaffold%0Arepresentation%20to%20dynamic%20video%20reconstruction.%20For%20Global%20CS%2C%20we%20propose%0AGlobal%20Anchor%20Deformation%20%28GAD%29%20to%20efficiently%20represent%20global%20dynamics%20along%0Acomplex%20motions%2C%20by%20directly%20deforming%20the%20implicit%20Scaffold%20attributes%20which%0Aare%20anchor%20position%2C%20offset%2C%20and%20local%20context%20features.%20Next%2C%20we%20finely%20adjust%0Alocal%20motions%20via%20the%20Local%20Gaussian%20Deformation%20%28LGD%29%20of%20Local%20CS%20explicitly.%0AAdditionally%2C%20we%20introduce%20Temporal%20Interval%20Adjustment%20%28TIA%29%20to%20automatically%0Acontrol%20the%20temporal%20coverage%20of%20each%20Local%20CS%20during%20training%2C%20allowing%0AMoDecGS%20to%20find%20optimal%20interval%20assignments%20based%20on%20the%20specified%20number%20of%0Atemporal%20segments.%20Extensive%20evaluations%20demonstrate%20that%20MoDecGS%20achieves%20an%0Aaverage%2070%25%20reduction%20in%20model%20size%20over%20stateoftheart%20methods%20for%20dynamic%203D%0AGaussians%20from%20realworld%20dynamic%20videos%20while%20maintaining%20or%20even%20improving%0Arendering%20quality.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2501.03714v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DMoDec-GS%253A%2520Global-to-Local%2520Motion%2520Decomposition%2520and%2520Temporal%2520Interval%250A%2520%2520Adjustment%2520for%2520Compact%2520Dynamic%25203D%2520Gaussian%2520Splatting%26entry.906535625%3DSangwoon%2520Kwak%2520and%2520Joonsoo%2520Kim%2520and%2520Jun%2520Young%2520Jeong%2520and%2520Won-Sik%2520Cheong%2520and%2520Jihyong%2520Oh%2520and%2520Munchurl%2520Kim%26entry.1292438233%3D%2520%25203D%2520Gaussian%2520Splatting%2520%25283DGS%2529%2520has%2520made%2520significant%2520strides%2520in%2520scene%250Arepresentation%2520and%2520neural%2520rendering%252C%2520with%2520intense%2520efforts%2520focused%2520on%2520adapting%250Ait%2520for%2520dynamic%2520scenes.%2520Despite%2520delivering%2520remarkable%2520rendering%2520quality%2520and%250Aspeed%252C%2520existing%2520methods%2520struggle%2520with%2520storage%2520demands%2520and%2520representing%2520complex%250Areal-world%2520motions.%2520To%2520tackle%2520these%2520issues%252C%2520we%2520propose%2520MoDecGS%252C%2520a%250Amemory-efficient%2520Gaussian%2520splatting%2520framework%2520designed%2520for%2520reconstructing%2520novel%250Aviews%2520in%2520challenging%2520scenarios%2520with%2520complex%2520motions.%2520We%2520introduce%2520GlobaltoLocal%250AMotion%2520Decomposition%2520%2528GLMD%2529%2520to%2520effectively%2520capture%2520dynamic%2520motions%2520in%2520a%250Acoarsetofine%2520manner.%2520This%2520approach%2520leverages%2520Global%2520Canonical%2520Scaffolds%2520%2528Global%250ACS%2529%2520and%2520Local%2520Canonical%2520Scaffolds%2520%2528Local%2520CS%2529%252C%2520extending%2520static%2520Scaffold%250Arepresentation%2520to%2520dynamic%2520video%2520reconstruction.%2520For%2520Global%2520CS%252C%2520we%2520propose%250AGlobal%2520Anchor%2520Deformation%2520%2528GAD%2529%2520to%2520efficiently%2520represent%2520global%2520dynamics%2520along%250Acomplex%2520motions%252C%2520by%2520directly%2520deforming%2520the%2520implicit%2520Scaffold%2520attributes%2520which%250Aare%2520anchor%2520position%252C%2520offset%252C%2520and%2520local%2520context%2520features.%2520Next%252C%2520we%2520finely%2520adjust%250Alocal%2520motions%2520via%2520the%2520Local%2520Gaussian%2520Deformation%2520%2528LGD%2529%2520of%2520Local%2520CS%2520explicitly.%250AAdditionally%252C%2520we%2520introduce%2520Temporal%2520Interval%2520Adjustment%2520%2528TIA%2529%2520to%2520automatically%250Acontrol%2520the%2520temporal%2520coverage%2520of%2520each%2520Local%2520CS%2520during%2520training%252C%2520allowing%250AMoDecGS%2520to%2520find%2520optimal%2520interval%2520assignments%2520based%2520on%2520the%2520specified%2520number%2520of%250Atemporal%2520segments.%2520Extensive%2520evaluations%2520demonstrate%2520that%2520MoDecGS%2520achieves%2520an%250Aaverage%252070%2525%2520reduction%2520in%2520model%2520size%2520over%2520stateoftheart%2520methods%2520for%2520dynamic%25203D%250AGaussians%2520from%2520realworld%2520dynamic%2520videos%2520while%2520maintaining%2520or%2520even%2520improving%250Arendering%2520quality.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2501.03714v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=MoDec-GS%3A%20Global-to-Local%20Motion%20Decomposition%20and%20Temporal%20Interval%0A%20%20Adjustment%20for%20Compact%20Dynamic%203D%20Gaussian%20Splatting&entry.906535625=Sangwoon%20Kwak%20and%20Joonsoo%20Kim%20and%20Jun%20Young%20Jeong%20and%20Won-Sik%20Cheong%20and%20Jihyong%20Oh%20and%20Munchurl%20Kim&entry.1292438233=%20%203D%20Gaussian%20Splatting%20%283DGS%29%20has%20made%20significant%20strides%20in%20scene%0Arepresentation%20and%20neural%20rendering%2C%20with%20intense%20efforts%20focused%20on%20adapting%0Ait%20for%20dynamic%20scenes.%20Despite%20delivering%20remarkable%20rendering%20quality%20and%0Aspeed%2C%20existing%20methods%20struggle%20with%20storage%20demands%20and%20representing%20complex%0Areal-world%20motions.%20To%20tackle%20these%20issues%2C%20we%20propose%20MoDecGS%2C%20a%0Amemory-efficient%20Gaussian%20splatting%20framework%20designed%20for%20reconstructing%20novel%0Aviews%20in%20challenging%20scenarios%20with%20complex%20motions.%20We%20introduce%20GlobaltoLocal%0AMotion%20Decomposition%20%28GLMD%29%20to%20effectively%20capture%20dynamic%20motions%20in%20a%0Acoarsetofine%20manner.%20This%20approach%20leverages%20Global%20Canonical%20Scaffolds%20%28Global%0ACS%29%20and%20Local%20Canonical%20Scaffolds%20%28Local%20CS%29%2C%20extending%20static%20Scaffold%0Arepresentation%20to%20dynamic%20video%20reconstruction.%20For%20Global%20CS%2C%20we%20propose%0AGlobal%20Anchor%20Deformation%20%28GAD%29%20to%20efficiently%20represent%20global%20dynamics%20along%0Acomplex%20motions%2C%20by%20directly%20deforming%20the%20implicit%20Scaffold%20attributes%20which%0Aare%20anchor%20position%2C%20offset%2C%20and%20local%20context%20features.%20Next%2C%20we%20finely%20adjust%0Alocal%20motions%20via%20the%20Local%20Gaussian%20Deformation%20%28LGD%29%20of%20Local%20CS%20explicitly.%0AAdditionally%2C%20we%20introduce%20Temporal%20Interval%20Adjustment%20%28TIA%29%20to%20automatically%0Acontrol%20the%20temporal%20coverage%20of%20each%20Local%20CS%20during%20training%2C%20allowing%0AMoDecGS%20to%20find%20optimal%20interval%20assignments%20based%20on%20the%20specified%20number%20of%0Atemporal%20segments.%20Extensive%20evaluations%20demonstrate%20that%20MoDecGS%20achieves%20an%0Aaverage%2070%25%20reduction%20in%20model%20size%20over%20stateoftheart%20methods%20for%20dynamic%203D%0AGaussians%20from%20realworld%20dynamic%20videos%20while%20maintaining%20or%20even%20improving%0Arendering%20quality.%0A&entry.1838667208=http%3A//arxiv.org/abs/2501.03714v1&entry.124074799=Read"},
{"title": "CL3DOR: Contrastive Learning for 3D Large Multimodal Models via Odds\n  Ratio on High-Resolution Point Clouds", "author": "Keonwoo Kim and Yeongjae Cho and Taebaek Hwang and Minsoo Jo and Sangdo Han", "abstract": "  Recent research has demonstrated that Large Language Models (LLMs) are not\nlimited to text-only tasks but can also function as multimodal models across\nvarious modalities, including audio, images, and videos. In particular,\nresearch on 3D Large Multimodal Models (3D LMMs) is making notable strides,\ndriven by the potential of processing higher-dimensional data like point\nclouds. However, upon closer examination, we find that the visual and textual\ncontent within each sample of existing training datasets lacks both high\ninformational granularity and clarity, which serve as a bottleneck for precise\ncross-modal understanding. To address these issues, we propose CL3DOR,\nContrastive Learning for 3D large multimodal models via Odds ratio on\nhigh-Resolution point clouds, designed to ensure greater specificity and\nclarity in both visual and textual content. Specifically, we increase the\ndensity of point clouds per object and construct informative hard negative\nresponses in the training dataset to penalize unwanted responses. To leverage\nhard negative responses, we incorporate the odds ratio as an auxiliary term for\ncontrastive learning into the conventional language modeling loss. CL3DOR\nachieves state-of-the-art performance in 3D scene understanding and reasoning\nbenchmarks. Additionally, we demonstrate the effectiveness of CL3DOR's key\ncomponents through extensive experiments.\n", "link": "http://arxiv.org/abs/2501.03879v1", "date": "2025-01-07", "relevancy": 3.1749, "topK": [{"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.6434}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.6308}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.6308}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20CL3DOR%3A%20Contrastive%20Learning%20for%203D%20Large%20Multimodal%20Models%20via%20Odds%0A%20%20Ratio%20on%20High-Resolution%20Point%20Clouds&body=Title%3A%20CL3DOR%3A%20Contrastive%20Learning%20for%203D%20Large%20Multimodal%20Models%20via%20Odds%0A%20%20Ratio%20on%20High-Resolution%20Point%20Clouds%0AAuthor%3A%20Keonwoo%20Kim%20and%20Yeongjae%20Cho%20and%20Taebaek%20Hwang%20and%20Minsoo%20Jo%20and%20Sangdo%20Han%0AAbstract%3A%20%20%20Recent%20research%20has%20demonstrated%20that%20Large%20Language%20Models%20%28LLMs%29%20are%20not%0Alimited%20to%20text-only%20tasks%20but%20can%20also%20function%20as%20multimodal%20models%20across%0Avarious%20modalities%2C%20including%20audio%2C%20images%2C%20and%20videos.%20In%20particular%2C%0Aresearch%20on%203D%20Large%20Multimodal%20Models%20%283D%20LMMs%29%20is%20making%20notable%20strides%2C%0Adriven%20by%20the%20potential%20of%20processing%20higher-dimensional%20data%20like%20point%0Aclouds.%20However%2C%20upon%20closer%20examination%2C%20we%20find%20that%20the%20visual%20and%20textual%0Acontent%20within%20each%20sample%20of%20existing%20training%20datasets%20lacks%20both%20high%0Ainformational%20granularity%20and%20clarity%2C%20which%20serve%20as%20a%20bottleneck%20for%20precise%0Across-modal%20understanding.%20To%20address%20these%20issues%2C%20we%20propose%20CL3DOR%2C%0AContrastive%20Learning%20for%203D%20large%20multimodal%20models%20via%20Odds%20ratio%20on%0Ahigh-Resolution%20point%20clouds%2C%20designed%20to%20ensure%20greater%20specificity%20and%0Aclarity%20in%20both%20visual%20and%20textual%20content.%20Specifically%2C%20we%20increase%20the%0Adensity%20of%20point%20clouds%20per%20object%20and%20construct%20informative%20hard%20negative%0Aresponses%20in%20the%20training%20dataset%20to%20penalize%20unwanted%20responses.%20To%20leverage%0Ahard%20negative%20responses%2C%20we%20incorporate%20the%20odds%20ratio%20as%20an%20auxiliary%20term%20for%0Acontrastive%20learning%20into%20the%20conventional%20language%20modeling%20loss.%20CL3DOR%0Aachieves%20state-of-the-art%20performance%20in%203D%20scene%20understanding%20and%20reasoning%0Abenchmarks.%20Additionally%2C%20we%20demonstrate%20the%20effectiveness%20of%20CL3DOR%27s%20key%0Acomponents%20through%20extensive%20experiments.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2501.03879v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DCL3DOR%253A%2520Contrastive%2520Learning%2520for%25203D%2520Large%2520Multimodal%2520Models%2520via%2520Odds%250A%2520%2520Ratio%2520on%2520High-Resolution%2520Point%2520Clouds%26entry.906535625%3DKeonwoo%2520Kim%2520and%2520Yeongjae%2520Cho%2520and%2520Taebaek%2520Hwang%2520and%2520Minsoo%2520Jo%2520and%2520Sangdo%2520Han%26entry.1292438233%3D%2520%2520Recent%2520research%2520has%2520demonstrated%2520that%2520Large%2520Language%2520Models%2520%2528LLMs%2529%2520are%2520not%250Alimited%2520to%2520text-only%2520tasks%2520but%2520can%2520also%2520function%2520as%2520multimodal%2520models%2520across%250Avarious%2520modalities%252C%2520including%2520audio%252C%2520images%252C%2520and%2520videos.%2520In%2520particular%252C%250Aresearch%2520on%25203D%2520Large%2520Multimodal%2520Models%2520%25283D%2520LMMs%2529%2520is%2520making%2520notable%2520strides%252C%250Adriven%2520by%2520the%2520potential%2520of%2520processing%2520higher-dimensional%2520data%2520like%2520point%250Aclouds.%2520However%252C%2520upon%2520closer%2520examination%252C%2520we%2520find%2520that%2520the%2520visual%2520and%2520textual%250Acontent%2520within%2520each%2520sample%2520of%2520existing%2520training%2520datasets%2520lacks%2520both%2520high%250Ainformational%2520granularity%2520and%2520clarity%252C%2520which%2520serve%2520as%2520a%2520bottleneck%2520for%2520precise%250Across-modal%2520understanding.%2520To%2520address%2520these%2520issues%252C%2520we%2520propose%2520CL3DOR%252C%250AContrastive%2520Learning%2520for%25203D%2520large%2520multimodal%2520models%2520via%2520Odds%2520ratio%2520on%250Ahigh-Resolution%2520point%2520clouds%252C%2520designed%2520to%2520ensure%2520greater%2520specificity%2520and%250Aclarity%2520in%2520both%2520visual%2520and%2520textual%2520content.%2520Specifically%252C%2520we%2520increase%2520the%250Adensity%2520of%2520point%2520clouds%2520per%2520object%2520and%2520construct%2520informative%2520hard%2520negative%250Aresponses%2520in%2520the%2520training%2520dataset%2520to%2520penalize%2520unwanted%2520responses.%2520To%2520leverage%250Ahard%2520negative%2520responses%252C%2520we%2520incorporate%2520the%2520odds%2520ratio%2520as%2520an%2520auxiliary%2520term%2520for%250Acontrastive%2520learning%2520into%2520the%2520conventional%2520language%2520modeling%2520loss.%2520CL3DOR%250Aachieves%2520state-of-the-art%2520performance%2520in%25203D%2520scene%2520understanding%2520and%2520reasoning%250Abenchmarks.%2520Additionally%252C%2520we%2520demonstrate%2520the%2520effectiveness%2520of%2520CL3DOR%2527s%2520key%250Acomponents%2520through%2520extensive%2520experiments.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2501.03879v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=CL3DOR%3A%20Contrastive%20Learning%20for%203D%20Large%20Multimodal%20Models%20via%20Odds%0A%20%20Ratio%20on%20High-Resolution%20Point%20Clouds&entry.906535625=Keonwoo%20Kim%20and%20Yeongjae%20Cho%20and%20Taebaek%20Hwang%20and%20Minsoo%20Jo%20and%20Sangdo%20Han&entry.1292438233=%20%20Recent%20research%20has%20demonstrated%20that%20Large%20Language%20Models%20%28LLMs%29%20are%20not%0Alimited%20to%20text-only%20tasks%20but%20can%20also%20function%20as%20multimodal%20models%20across%0Avarious%20modalities%2C%20including%20audio%2C%20images%2C%20and%20videos.%20In%20particular%2C%0Aresearch%20on%203D%20Large%20Multimodal%20Models%20%283D%20LMMs%29%20is%20making%20notable%20strides%2C%0Adriven%20by%20the%20potential%20of%20processing%20higher-dimensional%20data%20like%20point%0Aclouds.%20However%2C%20upon%20closer%20examination%2C%20we%20find%20that%20the%20visual%20and%20textual%0Acontent%20within%20each%20sample%20of%20existing%20training%20datasets%20lacks%20both%20high%0Ainformational%20granularity%20and%20clarity%2C%20which%20serve%20as%20a%20bottleneck%20for%20precise%0Across-modal%20understanding.%20To%20address%20these%20issues%2C%20we%20propose%20CL3DOR%2C%0AContrastive%20Learning%20for%203D%20large%20multimodal%20models%20via%20Odds%20ratio%20on%0Ahigh-Resolution%20point%20clouds%2C%20designed%20to%20ensure%20greater%20specificity%20and%0Aclarity%20in%20both%20visual%20and%20textual%20content.%20Specifically%2C%20we%20increase%20the%0Adensity%20of%20point%20clouds%20per%20object%20and%20construct%20informative%20hard%20negative%0Aresponses%20in%20the%20training%20dataset%20to%20penalize%20unwanted%20responses.%20To%20leverage%0Ahard%20negative%20responses%2C%20we%20incorporate%20the%20odds%20ratio%20as%20an%20auxiliary%20term%20for%0Acontrastive%20learning%20into%20the%20conventional%20language%20modeling%20loss.%20CL3DOR%0Aachieves%20state-of-the-art%20performance%20in%203D%20scene%20understanding%20and%20reasoning%0Abenchmarks.%20Additionally%2C%20we%20demonstrate%20the%20effectiveness%20of%20CL3DOR%27s%20key%0Acomponents%20through%20extensive%20experiments.%0A&entry.1838667208=http%3A//arxiv.org/abs/2501.03879v1&entry.124074799=Read"},
{"title": "Gaussian Building Mesh (GBM): Extract a Building's 3D Mesh with Google\n  Earth and Gaussian Splatting", "author": "Kyle Gao and Liangzhi Li and Hongjie He and Dening Lu and Linlin Xu and Jonathan Li", "abstract": "  Recently released open-source pre-trained foundational image segmentation and\nobject detection models (SAM2+GroundingDINO) allow for geometrically consistent\nsegmentation of objects of interest in multi-view 2D images. Users can use\ntext-based or click-based prompts to segment objects of interest without\nrequiring labeled training datasets. Gaussian Splatting allows for the learning\nof the 3D representation of a scene's geometry and radiance based on 2D images.\nCombining Google Earth Studio, SAM2+GroundingDINO, 2D Gaussian Splatting, and\nour improvements in mask refinement based on morphological operations and\ncontour simplification, we created a pipeline to extract the 3D mesh of any\nbuilding based on its name, address, or geographic coordinates.\n", "link": "http://arxiv.org/abs/2501.00625v2", "date": "2025-01-07", "relevancy": 3.164, "topK": [{"title": "GST: Precise 3D Human Body from a Single Image with Gaussian Splatting\n  Transformers", "link": "http://arxiv.org/abs/2409.04196v1", "similarity": 0.6701}, {"title": "Dynamic Gaussians Mesh: Consistent Mesh Reconstruction from Monocular\n  Videos", "link": "http://arxiv.org/abs/2404.12379v2", "similarity": 0.632}, {"title": "RTG-SLAM: Real-time 3D Reconstruction at Scale using Gaussian Splatting", "link": "http://arxiv.org/abs/2404.19706v1", "similarity": 0.5963}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Gaussian%20Building%20Mesh%20%28GBM%29%3A%20Extract%20a%20Building%27s%203D%20Mesh%20with%20Google%0A%20%20Earth%20and%20Gaussian%20Splatting&body=Title%3A%20Gaussian%20Building%20Mesh%20%28GBM%29%3A%20Extract%20a%20Building%27s%203D%20Mesh%20with%20Google%0A%20%20Earth%20and%20Gaussian%20Splatting%0AAuthor%3A%20Kyle%20Gao%20and%20Liangzhi%20Li%20and%20Hongjie%20He%20and%20Dening%20Lu%20and%20Linlin%20Xu%20and%20Jonathan%20Li%0AAbstract%3A%20%20%20Recently%20released%20open-source%20pre-trained%20foundational%20image%20segmentation%20and%0Aobject%20detection%20models%20%28SAM2%2BGroundingDINO%29%20allow%20for%20geometrically%20consistent%0Asegmentation%20of%20objects%20of%20interest%20in%20multi-view%202D%20images.%20Users%20can%20use%0Atext-based%20or%20click-based%20prompts%20to%20segment%20objects%20of%20interest%20without%0Arequiring%20labeled%20training%20datasets.%20Gaussian%20Splatting%20allows%20for%20the%20learning%0Aof%20the%203D%20representation%20of%20a%20scene%27s%20geometry%20and%20radiance%20based%20on%202D%20images.%0ACombining%20Google%20Earth%20Studio%2C%20SAM2%2BGroundingDINO%2C%202D%20Gaussian%20Splatting%2C%20and%0Aour%20improvements%20in%20mask%20refinement%20based%20on%20morphological%20operations%20and%0Acontour%20simplification%2C%20we%20created%20a%20pipeline%20to%20extract%20the%203D%20mesh%20of%20any%0Abuilding%20based%20on%20its%20name%2C%20address%2C%20or%20geographic%20coordinates.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2501.00625v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DGaussian%2520Building%2520Mesh%2520%2528GBM%2529%253A%2520Extract%2520a%2520Building%2527s%25203D%2520Mesh%2520with%2520Google%250A%2520%2520Earth%2520and%2520Gaussian%2520Splatting%26entry.906535625%3DKyle%2520Gao%2520and%2520Liangzhi%2520Li%2520and%2520Hongjie%2520He%2520and%2520Dening%2520Lu%2520and%2520Linlin%2520Xu%2520and%2520Jonathan%2520Li%26entry.1292438233%3D%2520%2520Recently%2520released%2520open-source%2520pre-trained%2520foundational%2520image%2520segmentation%2520and%250Aobject%2520detection%2520models%2520%2528SAM2%252BGroundingDINO%2529%2520allow%2520for%2520geometrically%2520consistent%250Asegmentation%2520of%2520objects%2520of%2520interest%2520in%2520multi-view%25202D%2520images.%2520Users%2520can%2520use%250Atext-based%2520or%2520click-based%2520prompts%2520to%2520segment%2520objects%2520of%2520interest%2520without%250Arequiring%2520labeled%2520training%2520datasets.%2520Gaussian%2520Splatting%2520allows%2520for%2520the%2520learning%250Aof%2520the%25203D%2520representation%2520of%2520a%2520scene%2527s%2520geometry%2520and%2520radiance%2520based%2520on%25202D%2520images.%250ACombining%2520Google%2520Earth%2520Studio%252C%2520SAM2%252BGroundingDINO%252C%25202D%2520Gaussian%2520Splatting%252C%2520and%250Aour%2520improvements%2520in%2520mask%2520refinement%2520based%2520on%2520morphological%2520operations%2520and%250Acontour%2520simplification%252C%2520we%2520created%2520a%2520pipeline%2520to%2520extract%2520the%25203D%2520mesh%2520of%2520any%250Abuilding%2520based%2520on%2520its%2520name%252C%2520address%252C%2520or%2520geographic%2520coordinates.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2501.00625v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Gaussian%20Building%20Mesh%20%28GBM%29%3A%20Extract%20a%20Building%27s%203D%20Mesh%20with%20Google%0A%20%20Earth%20and%20Gaussian%20Splatting&entry.906535625=Kyle%20Gao%20and%20Liangzhi%20Li%20and%20Hongjie%20He%20and%20Dening%20Lu%20and%20Linlin%20Xu%20and%20Jonathan%20Li&entry.1292438233=%20%20Recently%20released%20open-source%20pre-trained%20foundational%20image%20segmentation%20and%0Aobject%20detection%20models%20%28SAM2%2BGroundingDINO%29%20allow%20for%20geometrically%20consistent%0Asegmentation%20of%20objects%20of%20interest%20in%20multi-view%202D%20images.%20Users%20can%20use%0Atext-based%20or%20click-based%20prompts%20to%20segment%20objects%20of%20interest%20without%0Arequiring%20labeled%20training%20datasets.%20Gaussian%20Splatting%20allows%20for%20the%20learning%0Aof%20the%203D%20representation%20of%20a%20scene%27s%20geometry%20and%20radiance%20based%20on%202D%20images.%0ACombining%20Google%20Earth%20Studio%2C%20SAM2%2BGroundingDINO%2C%202D%20Gaussian%20Splatting%2C%20and%0Aour%20improvements%20in%20mask%20refinement%20based%20on%20morphological%20operations%20and%0Acontour%20simplification%2C%20we%20created%20a%20pipeline%20to%20extract%20the%203D%20mesh%20of%20any%0Abuilding%20based%20on%20its%20name%2C%20address%2C%20or%20geographic%20coordinates.%0A&entry.1838667208=http%3A//arxiv.org/abs/2501.00625v2&entry.124074799=Read"},
{"title": "ZDySS -- Zero-Shot Dynamic Scene Stylization using Gaussian Splatting", "author": "Abhishek Saroha and Florian Hofherr and Mariia Gladkova and Cecilia Curreli and Or Litany and Daniel Cremers", "abstract": "  Stylizing a dynamic scene based on an exemplar image is critical for various\nreal-world applications, including gaming, filmmaking, and augmented and\nvirtual reality. However, achieving consistent stylization across both spatial\nand temporal dimensions remains a significant challenge. Most existing methods\nare designed for static scenes and often require an optimization process for\neach style image, limiting their adaptability. We introduce ZDySS, a zero-shot\nstylization framework for dynamic scenes, allowing our model to generalize to\npreviously unseen style images at inference. Our approach employs Gaussian\nsplatting for scene representation, linking each Gaussian to a learned feature\nvector that renders a feature map for any given view and timestamp. By applying\nstyle transfer on the learned feature vectors instead of the rendered feature\nmap, we enhance spatio-temporal consistency across frames. Our method\ndemonstrates superior performance and coherence over state-of-the-art baselines\nin tests on real-world dynamic scenes, making it a robust solution for\npractical applications.\n", "link": "http://arxiv.org/abs/2501.03875v1", "date": "2025-01-07", "relevancy": 3.0476, "topK": [{"title": "GST: Precise 3D Human Body from a Single Image with Gaussian Splatting\n  Transformers", "link": "http://arxiv.org/abs/2409.04196v1", "similarity": 0.6122}, {"title": "Dynamic Gaussians Mesh: Consistent Mesh Reconstruction from Monocular\n  Videos", "link": "http://arxiv.org/abs/2404.12379v2", "similarity": 0.611}, {"title": "RTG-SLAM: Real-time 3D Reconstruction at Scale using Gaussian Splatting", "link": "http://arxiv.org/abs/2404.19706v1", "similarity": 0.6054}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20ZDySS%20--%20Zero-Shot%20Dynamic%20Scene%20Stylization%20using%20Gaussian%20Splatting&body=Title%3A%20ZDySS%20--%20Zero-Shot%20Dynamic%20Scene%20Stylization%20using%20Gaussian%20Splatting%0AAuthor%3A%20Abhishek%20Saroha%20and%20Florian%20Hofherr%20and%20Mariia%20Gladkova%20and%20Cecilia%20Curreli%20and%20Or%20Litany%20and%20Daniel%20Cremers%0AAbstract%3A%20%20%20Stylizing%20a%20dynamic%20scene%20based%20on%20an%20exemplar%20image%20is%20critical%20for%20various%0Areal-world%20applications%2C%20including%20gaming%2C%20filmmaking%2C%20and%20augmented%20and%0Avirtual%20reality.%20However%2C%20achieving%20consistent%20stylization%20across%20both%20spatial%0Aand%20temporal%20dimensions%20remains%20a%20significant%20challenge.%20Most%20existing%20methods%0Aare%20designed%20for%20static%20scenes%20and%20often%20require%20an%20optimization%20process%20for%0Aeach%20style%20image%2C%20limiting%20their%20adaptability.%20We%20introduce%20ZDySS%2C%20a%20zero-shot%0Astylization%20framework%20for%20dynamic%20scenes%2C%20allowing%20our%20model%20to%20generalize%20to%0Apreviously%20unseen%20style%20images%20at%20inference.%20Our%20approach%20employs%20Gaussian%0Asplatting%20for%20scene%20representation%2C%20linking%20each%20Gaussian%20to%20a%20learned%20feature%0Avector%20that%20renders%20a%20feature%20map%20for%20any%20given%20view%20and%20timestamp.%20By%20applying%0Astyle%20transfer%20on%20the%20learned%20feature%20vectors%20instead%20of%20the%20rendered%20feature%0Amap%2C%20we%20enhance%20spatio-temporal%20consistency%20across%20frames.%20Our%20method%0Ademonstrates%20superior%20performance%20and%20coherence%20over%20state-of-the-art%20baselines%0Ain%20tests%20on%20real-world%20dynamic%20scenes%2C%20making%20it%20a%20robust%20solution%20for%0Apractical%20applications.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2501.03875v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DZDySS%2520--%2520Zero-Shot%2520Dynamic%2520Scene%2520Stylization%2520using%2520Gaussian%2520Splatting%26entry.906535625%3DAbhishek%2520Saroha%2520and%2520Florian%2520Hofherr%2520and%2520Mariia%2520Gladkova%2520and%2520Cecilia%2520Curreli%2520and%2520Or%2520Litany%2520and%2520Daniel%2520Cremers%26entry.1292438233%3D%2520%2520Stylizing%2520a%2520dynamic%2520scene%2520based%2520on%2520an%2520exemplar%2520image%2520is%2520critical%2520for%2520various%250Areal-world%2520applications%252C%2520including%2520gaming%252C%2520filmmaking%252C%2520and%2520augmented%2520and%250Avirtual%2520reality.%2520However%252C%2520achieving%2520consistent%2520stylization%2520across%2520both%2520spatial%250Aand%2520temporal%2520dimensions%2520remains%2520a%2520significant%2520challenge.%2520Most%2520existing%2520methods%250Aare%2520designed%2520for%2520static%2520scenes%2520and%2520often%2520require%2520an%2520optimization%2520process%2520for%250Aeach%2520style%2520image%252C%2520limiting%2520their%2520adaptability.%2520We%2520introduce%2520ZDySS%252C%2520a%2520zero-shot%250Astylization%2520framework%2520for%2520dynamic%2520scenes%252C%2520allowing%2520our%2520model%2520to%2520generalize%2520to%250Apreviously%2520unseen%2520style%2520images%2520at%2520inference.%2520Our%2520approach%2520employs%2520Gaussian%250Asplatting%2520for%2520scene%2520representation%252C%2520linking%2520each%2520Gaussian%2520to%2520a%2520learned%2520feature%250Avector%2520that%2520renders%2520a%2520feature%2520map%2520for%2520any%2520given%2520view%2520and%2520timestamp.%2520By%2520applying%250Astyle%2520transfer%2520on%2520the%2520learned%2520feature%2520vectors%2520instead%2520of%2520the%2520rendered%2520feature%250Amap%252C%2520we%2520enhance%2520spatio-temporal%2520consistency%2520across%2520frames.%2520Our%2520method%250Ademonstrates%2520superior%2520performance%2520and%2520coherence%2520over%2520state-of-the-art%2520baselines%250Ain%2520tests%2520on%2520real-world%2520dynamic%2520scenes%252C%2520making%2520it%2520a%2520robust%2520solution%2520for%250Apractical%2520applications.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2501.03875v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=ZDySS%20--%20Zero-Shot%20Dynamic%20Scene%20Stylization%20using%20Gaussian%20Splatting&entry.906535625=Abhishek%20Saroha%20and%20Florian%20Hofherr%20and%20Mariia%20Gladkova%20and%20Cecilia%20Curreli%20and%20Or%20Litany%20and%20Daniel%20Cremers&entry.1292438233=%20%20Stylizing%20a%20dynamic%20scene%20based%20on%20an%20exemplar%20image%20is%20critical%20for%20various%0Areal-world%20applications%2C%20including%20gaming%2C%20filmmaking%2C%20and%20augmented%20and%0Avirtual%20reality.%20However%2C%20achieving%20consistent%20stylization%20across%20both%20spatial%0Aand%20temporal%20dimensions%20remains%20a%20significant%20challenge.%20Most%20existing%20methods%0Aare%20designed%20for%20static%20scenes%20and%20often%20require%20an%20optimization%20process%20for%0Aeach%20style%20image%2C%20limiting%20their%20adaptability.%20We%20introduce%20ZDySS%2C%20a%20zero-shot%0Astylization%20framework%20for%20dynamic%20scenes%2C%20allowing%20our%20model%20to%20generalize%20to%0Apreviously%20unseen%20style%20images%20at%20inference.%20Our%20approach%20employs%20Gaussian%0Asplatting%20for%20scene%20representation%2C%20linking%20each%20Gaussian%20to%20a%20learned%20feature%0Avector%20that%20renders%20a%20feature%20map%20for%20any%20given%20view%20and%20timestamp.%20By%20applying%0Astyle%20transfer%20on%20the%20learned%20feature%20vectors%20instead%20of%20the%20rendered%20feature%0Amap%2C%20we%20enhance%20spatio-temporal%20consistency%20across%20frames.%20Our%20method%0Ademonstrates%20superior%20performance%20and%20coherence%20over%20state-of-the-art%20baselines%0Ain%20tests%20on%20real-world%20dynamic%20scenes%2C%20making%20it%20a%20robust%20solution%20for%0Apractical%20applications.%0A&entry.1838667208=http%3A//arxiv.org/abs/2501.03875v1&entry.124074799=Read"},
{"title": "Self-adaptive vision-language model for 3D segmentation of pulmonary\n  artery and vein", "author": "Xiaotong Guo and Deqian Yang and Dan Wang and Haochen Zhao and Yuan Li and Zhilin Sui and Tao Zhou and Lijun Zhang and Yanda Meng", "abstract": "  Accurate segmentation of pulmonary structures iscrucial in clinical\ndiagnosis, disease study, and treatment planning. Significant progress has been\nmade in deep learning-based segmentation techniques, but most require much\nlabeled data for training. Consequently, developing precise segmentation\nmethods that demand fewer labeled datasets is paramount in medical image\nanalysis. The emergence of pre-trained vision-language foundation models, such\nas CLIP, recently opened the door for universal computer vision tasks.\nExploiting the generalization ability of these pre-trained foundation models on\ndownstream tasks, such as segmentation, leads to unexpected performance with a\nrelatively small amount of labeled data. However, exploring these models for\npulmonary artery-vein segmentation is still limited. This paper proposes a\nnovel framework called Language-guided self-adaptive Cross-Attention Fusion\nFramework. Our method adopts pre-trained CLIP as a strong feature extractor for\ngenerating the segmentation of 3D CT scans, while adaptively aggregating the\ncross-modality of text and image representations. We propose a s pecially\ndesigned adapter module to fine-tune pre-trained CLIP with a self-adaptive\nlearning strategy to effectively fuse the two modalities of embeddings. We\nextensively validate our method on a local dataset, which is the largest\npulmonary artery-vein CT dataset to date and consists of 718 labeled data in\ntotal. The experiments show that our method outperformed other state-of-the-art\nmethods by a large margin. Our data and code will be made publicly available\nupon acceptance.\n", "link": "http://arxiv.org/abs/2501.03722v1", "date": "2025-01-07", "relevancy": 3.0227, "topK": [{"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.6429}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5853}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5853}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Self-adaptive%20vision-language%20model%20for%203D%20segmentation%20of%20pulmonary%0A%20%20artery%20and%20vein&body=Title%3A%20Self-adaptive%20vision-language%20model%20for%203D%20segmentation%20of%20pulmonary%0A%20%20artery%20and%20vein%0AAuthor%3A%20Xiaotong%20Guo%20and%20Deqian%20Yang%20and%20Dan%20Wang%20and%20Haochen%20Zhao%20and%20Yuan%20Li%20and%20Zhilin%20Sui%20and%20Tao%20Zhou%20and%20Lijun%20Zhang%20and%20Yanda%20Meng%0AAbstract%3A%20%20%20Accurate%20segmentation%20of%20pulmonary%20structures%20iscrucial%20in%20clinical%0Adiagnosis%2C%20disease%20study%2C%20and%20treatment%20planning.%20Significant%20progress%20has%20been%0Amade%20in%20deep%20learning-based%20segmentation%20techniques%2C%20but%20most%20require%20much%0Alabeled%20data%20for%20training.%20Consequently%2C%20developing%20precise%20segmentation%0Amethods%20that%20demand%20fewer%20labeled%20datasets%20is%20paramount%20in%20medical%20image%0Aanalysis.%20The%20emergence%20of%20pre-trained%20vision-language%20foundation%20models%2C%20such%0Aas%20CLIP%2C%20recently%20opened%20the%20door%20for%20universal%20computer%20vision%20tasks.%0AExploiting%20the%20generalization%20ability%20of%20these%20pre-trained%20foundation%20models%20on%0Adownstream%20tasks%2C%20such%20as%20segmentation%2C%20leads%20to%20unexpected%20performance%20with%20a%0Arelatively%20small%20amount%20of%20labeled%20data.%20However%2C%20exploring%20these%20models%20for%0Apulmonary%20artery-vein%20segmentation%20is%20still%20limited.%20This%20paper%20proposes%20a%0Anovel%20framework%20called%20Language-guided%20self-adaptive%20Cross-Attention%20Fusion%0AFramework.%20Our%20method%20adopts%20pre-trained%20CLIP%20as%20a%20strong%20feature%20extractor%20for%0Agenerating%20the%20segmentation%20of%203D%20CT%20scans%2C%20while%20adaptively%20aggregating%20the%0Across-modality%20of%20text%20and%20image%20representations.%20We%20propose%20a%20s%20pecially%0Adesigned%20adapter%20module%20to%20fine-tune%20pre-trained%20CLIP%20with%20a%20self-adaptive%0Alearning%20strategy%20to%20effectively%20fuse%20the%20two%20modalities%20of%20embeddings.%20We%0Aextensively%20validate%20our%20method%20on%20a%20local%20dataset%2C%20which%20is%20the%20largest%0Apulmonary%20artery-vein%20CT%20dataset%20to%20date%20and%20consists%20of%20718%20labeled%20data%20in%0Atotal.%20The%20experiments%20show%20that%20our%20method%20outperformed%20other%20state-of-the-art%0Amethods%20by%20a%20large%20margin.%20Our%20data%20and%20code%20will%20be%20made%20publicly%20available%0Aupon%20acceptance.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2501.03722v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DSelf-adaptive%2520vision-language%2520model%2520for%25203D%2520segmentation%2520of%2520pulmonary%250A%2520%2520artery%2520and%2520vein%26entry.906535625%3DXiaotong%2520Guo%2520and%2520Deqian%2520Yang%2520and%2520Dan%2520Wang%2520and%2520Haochen%2520Zhao%2520and%2520Yuan%2520Li%2520and%2520Zhilin%2520Sui%2520and%2520Tao%2520Zhou%2520and%2520Lijun%2520Zhang%2520and%2520Yanda%2520Meng%26entry.1292438233%3D%2520%2520Accurate%2520segmentation%2520of%2520pulmonary%2520structures%2520iscrucial%2520in%2520clinical%250Adiagnosis%252C%2520disease%2520study%252C%2520and%2520treatment%2520planning.%2520Significant%2520progress%2520has%2520been%250Amade%2520in%2520deep%2520learning-based%2520segmentation%2520techniques%252C%2520but%2520most%2520require%2520much%250Alabeled%2520data%2520for%2520training.%2520Consequently%252C%2520developing%2520precise%2520segmentation%250Amethods%2520that%2520demand%2520fewer%2520labeled%2520datasets%2520is%2520paramount%2520in%2520medical%2520image%250Aanalysis.%2520The%2520emergence%2520of%2520pre-trained%2520vision-language%2520foundation%2520models%252C%2520such%250Aas%2520CLIP%252C%2520recently%2520opened%2520the%2520door%2520for%2520universal%2520computer%2520vision%2520tasks.%250AExploiting%2520the%2520generalization%2520ability%2520of%2520these%2520pre-trained%2520foundation%2520models%2520on%250Adownstream%2520tasks%252C%2520such%2520as%2520segmentation%252C%2520leads%2520to%2520unexpected%2520performance%2520with%2520a%250Arelatively%2520small%2520amount%2520of%2520labeled%2520data.%2520However%252C%2520exploring%2520these%2520models%2520for%250Apulmonary%2520artery-vein%2520segmentation%2520is%2520still%2520limited.%2520This%2520paper%2520proposes%2520a%250Anovel%2520framework%2520called%2520Language-guided%2520self-adaptive%2520Cross-Attention%2520Fusion%250AFramework.%2520Our%2520method%2520adopts%2520pre-trained%2520CLIP%2520as%2520a%2520strong%2520feature%2520extractor%2520for%250Agenerating%2520the%2520segmentation%2520of%25203D%2520CT%2520scans%252C%2520while%2520adaptively%2520aggregating%2520the%250Across-modality%2520of%2520text%2520and%2520image%2520representations.%2520We%2520propose%2520a%2520s%2520pecially%250Adesigned%2520adapter%2520module%2520to%2520fine-tune%2520pre-trained%2520CLIP%2520with%2520a%2520self-adaptive%250Alearning%2520strategy%2520to%2520effectively%2520fuse%2520the%2520two%2520modalities%2520of%2520embeddings.%2520We%250Aextensively%2520validate%2520our%2520method%2520on%2520a%2520local%2520dataset%252C%2520which%2520is%2520the%2520largest%250Apulmonary%2520artery-vein%2520CT%2520dataset%2520to%2520date%2520and%2520consists%2520of%2520718%2520labeled%2520data%2520in%250Atotal.%2520The%2520experiments%2520show%2520that%2520our%2520method%2520outperformed%2520other%2520state-of-the-art%250Amethods%2520by%2520a%2520large%2520margin.%2520Our%2520data%2520and%2520code%2520will%2520be%2520made%2520publicly%2520available%250Aupon%2520acceptance.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2501.03722v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Self-adaptive%20vision-language%20model%20for%203D%20segmentation%20of%20pulmonary%0A%20%20artery%20and%20vein&entry.906535625=Xiaotong%20Guo%20and%20Deqian%20Yang%20and%20Dan%20Wang%20and%20Haochen%20Zhao%20and%20Yuan%20Li%20and%20Zhilin%20Sui%20and%20Tao%20Zhou%20and%20Lijun%20Zhang%20and%20Yanda%20Meng&entry.1292438233=%20%20Accurate%20segmentation%20of%20pulmonary%20structures%20iscrucial%20in%20clinical%0Adiagnosis%2C%20disease%20study%2C%20and%20treatment%20planning.%20Significant%20progress%20has%20been%0Amade%20in%20deep%20learning-based%20segmentation%20techniques%2C%20but%20most%20require%20much%0Alabeled%20data%20for%20training.%20Consequently%2C%20developing%20precise%20segmentation%0Amethods%20that%20demand%20fewer%20labeled%20datasets%20is%20paramount%20in%20medical%20image%0Aanalysis.%20The%20emergence%20of%20pre-trained%20vision-language%20foundation%20models%2C%20such%0Aas%20CLIP%2C%20recently%20opened%20the%20door%20for%20universal%20computer%20vision%20tasks.%0AExploiting%20the%20generalization%20ability%20of%20these%20pre-trained%20foundation%20models%20on%0Adownstream%20tasks%2C%20such%20as%20segmentation%2C%20leads%20to%20unexpected%20performance%20with%20a%0Arelatively%20small%20amount%20of%20labeled%20data.%20However%2C%20exploring%20these%20models%20for%0Apulmonary%20artery-vein%20segmentation%20is%20still%20limited.%20This%20paper%20proposes%20a%0Anovel%20framework%20called%20Language-guided%20self-adaptive%20Cross-Attention%20Fusion%0AFramework.%20Our%20method%20adopts%20pre-trained%20CLIP%20as%20a%20strong%20feature%20extractor%20for%0Agenerating%20the%20segmentation%20of%203D%20CT%20scans%2C%20while%20adaptively%20aggregating%20the%0Across-modality%20of%20text%20and%20image%20representations.%20We%20propose%20a%20s%20pecially%0Adesigned%20adapter%20module%20to%20fine-tune%20pre-trained%20CLIP%20with%20a%20self-adaptive%0Alearning%20strategy%20to%20effectively%20fuse%20the%20two%20modalities%20of%20embeddings.%20We%0Aextensively%20validate%20our%20method%20on%20a%20local%20dataset%2C%20which%20is%20the%20largest%0Apulmonary%20artery-vein%20CT%20dataset%20to%20date%20and%20consists%20of%20718%20labeled%20data%20in%0Atotal.%20The%20experiments%20show%20that%20our%20method%20outperformed%20other%20state-of-the-art%0Amethods%20by%20a%20large%20margin.%20Our%20data%20and%20code%20will%20be%20made%20publicly%20available%0Aupon%20acceptance.%0A&entry.1838667208=http%3A//arxiv.org/abs/2501.03722v1&entry.124074799=Read"},
{"title": "LargeAD: Large-Scale Cross-Sensor Data Pretraining for Autonomous\n  Driving", "author": "Lingdong Kong and Xiang Xu and Youquan Liu and Jun Cen and Runnan Chen and Wenwei Zhang and Liang Pan and Kai Chen and Ziwei Liu", "abstract": "  Recent advancements in vision foundation models (VFMs) have revolutionized\nvisual perception in 2D, yet their potential for 3D scene understanding,\nparticularly in autonomous driving applications, remains underexplored. In this\npaper, we introduce LargeAD, a versatile and scalable framework designed for\nlarge-scale 3D pretraining across diverse real-world driving datasets. Our\nframework leverages VFMs to extract semantically rich superpixels from 2D\nimages, which are aligned with LiDAR point clouds to generate high-quality\ncontrastive samples. This alignment facilitates cross-modal representation\nlearning, enhancing the semantic consistency between 2D and 3D data. We\nintroduce several key innovations: i) VFM-driven superpixel generation for\ndetailed semantic representation, ii) a VFM-assisted contrastive learning\nstrategy to align multimodal features, iii) superpoint temporal consistency to\nmaintain stable representations across time, and iv) multi-source data\npretraining to generalize across various LiDAR configurations. Our approach\ndelivers significant performance improvements over state-of-the-art methods in\nboth linear probing and fine-tuning tasks for both LiDAR-based segmentation and\nobject detection. Extensive experiments on eleven large-scale multi-modal\ndatasets highlight our superior performance, demonstrating the adaptability,\nefficiency, and robustness in real-world autonomous driving scenarios.\n", "link": "http://arxiv.org/abs/2501.04005v1", "date": "2025-01-07", "relevancy": 3.0206, "topK": [{"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.6187}, {"title": "DDM-NET: End-to-end learning of keypoint feature Detection, Description and Matching for 3D localization", "link": "https://arxiv.org/abs/2212.04575", "similarity": 0.5986}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5951}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20LargeAD%3A%20Large-Scale%20Cross-Sensor%20Data%20Pretraining%20for%20Autonomous%0A%20%20Driving&body=Title%3A%20LargeAD%3A%20Large-Scale%20Cross-Sensor%20Data%20Pretraining%20for%20Autonomous%0A%20%20Driving%0AAuthor%3A%20Lingdong%20Kong%20and%20Xiang%20Xu%20and%20Youquan%20Liu%20and%20Jun%20Cen%20and%20Runnan%20Chen%20and%20Wenwei%20Zhang%20and%20Liang%20Pan%20and%20Kai%20Chen%20and%20Ziwei%20Liu%0AAbstract%3A%20%20%20Recent%20advancements%20in%20vision%20foundation%20models%20%28VFMs%29%20have%20revolutionized%0Avisual%20perception%20in%202D%2C%20yet%20their%20potential%20for%203D%20scene%20understanding%2C%0Aparticularly%20in%20autonomous%20driving%20applications%2C%20remains%20underexplored.%20In%20this%0Apaper%2C%20we%20introduce%20LargeAD%2C%20a%20versatile%20and%20scalable%20framework%20designed%20for%0Alarge-scale%203D%20pretraining%20across%20diverse%20real-world%20driving%20datasets.%20Our%0Aframework%20leverages%20VFMs%20to%20extract%20semantically%20rich%20superpixels%20from%202D%0Aimages%2C%20which%20are%20aligned%20with%20LiDAR%20point%20clouds%20to%20generate%20high-quality%0Acontrastive%20samples.%20This%20alignment%20facilitates%20cross-modal%20representation%0Alearning%2C%20enhancing%20the%20semantic%20consistency%20between%202D%20and%203D%20data.%20We%0Aintroduce%20several%20key%20innovations%3A%20i%29%20VFM-driven%20superpixel%20generation%20for%0Adetailed%20semantic%20representation%2C%20ii%29%20a%20VFM-assisted%20contrastive%20learning%0Astrategy%20to%20align%20multimodal%20features%2C%20iii%29%20superpoint%20temporal%20consistency%20to%0Amaintain%20stable%20representations%20across%20time%2C%20and%20iv%29%20multi-source%20data%0Apretraining%20to%20generalize%20across%20various%20LiDAR%20configurations.%20Our%20approach%0Adelivers%20significant%20performance%20improvements%20over%20state-of-the-art%20methods%20in%0Aboth%20linear%20probing%20and%20fine-tuning%20tasks%20for%20both%20LiDAR-based%20segmentation%20and%0Aobject%20detection.%20Extensive%20experiments%20on%20eleven%20large-scale%20multi-modal%0Adatasets%20highlight%20our%20superior%20performance%2C%20demonstrating%20the%20adaptability%2C%0Aefficiency%2C%20and%20robustness%20in%20real-world%20autonomous%20driving%20scenarios.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2501.04005v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DLargeAD%253A%2520Large-Scale%2520Cross-Sensor%2520Data%2520Pretraining%2520for%2520Autonomous%250A%2520%2520Driving%26entry.906535625%3DLingdong%2520Kong%2520and%2520Xiang%2520Xu%2520and%2520Youquan%2520Liu%2520and%2520Jun%2520Cen%2520and%2520Runnan%2520Chen%2520and%2520Wenwei%2520Zhang%2520and%2520Liang%2520Pan%2520and%2520Kai%2520Chen%2520and%2520Ziwei%2520Liu%26entry.1292438233%3D%2520%2520Recent%2520advancements%2520in%2520vision%2520foundation%2520models%2520%2528VFMs%2529%2520have%2520revolutionized%250Avisual%2520perception%2520in%25202D%252C%2520yet%2520their%2520potential%2520for%25203D%2520scene%2520understanding%252C%250Aparticularly%2520in%2520autonomous%2520driving%2520applications%252C%2520remains%2520underexplored.%2520In%2520this%250Apaper%252C%2520we%2520introduce%2520LargeAD%252C%2520a%2520versatile%2520and%2520scalable%2520framework%2520designed%2520for%250Alarge-scale%25203D%2520pretraining%2520across%2520diverse%2520real-world%2520driving%2520datasets.%2520Our%250Aframework%2520leverages%2520VFMs%2520to%2520extract%2520semantically%2520rich%2520superpixels%2520from%25202D%250Aimages%252C%2520which%2520are%2520aligned%2520with%2520LiDAR%2520point%2520clouds%2520to%2520generate%2520high-quality%250Acontrastive%2520samples.%2520This%2520alignment%2520facilitates%2520cross-modal%2520representation%250Alearning%252C%2520enhancing%2520the%2520semantic%2520consistency%2520between%25202D%2520and%25203D%2520data.%2520We%250Aintroduce%2520several%2520key%2520innovations%253A%2520i%2529%2520VFM-driven%2520superpixel%2520generation%2520for%250Adetailed%2520semantic%2520representation%252C%2520ii%2529%2520a%2520VFM-assisted%2520contrastive%2520learning%250Astrategy%2520to%2520align%2520multimodal%2520features%252C%2520iii%2529%2520superpoint%2520temporal%2520consistency%2520to%250Amaintain%2520stable%2520representations%2520across%2520time%252C%2520and%2520iv%2529%2520multi-source%2520data%250Apretraining%2520to%2520generalize%2520across%2520various%2520LiDAR%2520configurations.%2520Our%2520approach%250Adelivers%2520significant%2520performance%2520improvements%2520over%2520state-of-the-art%2520methods%2520in%250Aboth%2520linear%2520probing%2520and%2520fine-tuning%2520tasks%2520for%2520both%2520LiDAR-based%2520segmentation%2520and%250Aobject%2520detection.%2520Extensive%2520experiments%2520on%2520eleven%2520large-scale%2520multi-modal%250Adatasets%2520highlight%2520our%2520superior%2520performance%252C%2520demonstrating%2520the%2520adaptability%252C%250Aefficiency%252C%2520and%2520robustness%2520in%2520real-world%2520autonomous%2520driving%2520scenarios.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2501.04005v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=LargeAD%3A%20Large-Scale%20Cross-Sensor%20Data%20Pretraining%20for%20Autonomous%0A%20%20Driving&entry.906535625=Lingdong%20Kong%20and%20Xiang%20Xu%20and%20Youquan%20Liu%20and%20Jun%20Cen%20and%20Runnan%20Chen%20and%20Wenwei%20Zhang%20and%20Liang%20Pan%20and%20Kai%20Chen%20and%20Ziwei%20Liu&entry.1292438233=%20%20Recent%20advancements%20in%20vision%20foundation%20models%20%28VFMs%29%20have%20revolutionized%0Avisual%20perception%20in%202D%2C%20yet%20their%20potential%20for%203D%20scene%20understanding%2C%0Aparticularly%20in%20autonomous%20driving%20applications%2C%20remains%20underexplored.%20In%20this%0Apaper%2C%20we%20introduce%20LargeAD%2C%20a%20versatile%20and%20scalable%20framework%20designed%20for%0Alarge-scale%203D%20pretraining%20across%20diverse%20real-world%20driving%20datasets.%20Our%0Aframework%20leverages%20VFMs%20to%20extract%20semantically%20rich%20superpixels%20from%202D%0Aimages%2C%20which%20are%20aligned%20with%20LiDAR%20point%20clouds%20to%20generate%20high-quality%0Acontrastive%20samples.%20This%20alignment%20facilitates%20cross-modal%20representation%0Alearning%2C%20enhancing%20the%20semantic%20consistency%20between%202D%20and%203D%20data.%20We%0Aintroduce%20several%20key%20innovations%3A%20i%29%20VFM-driven%20superpixel%20generation%20for%0Adetailed%20semantic%20representation%2C%20ii%29%20a%20VFM-assisted%20contrastive%20learning%0Astrategy%20to%20align%20multimodal%20features%2C%20iii%29%20superpoint%20temporal%20consistency%20to%0Amaintain%20stable%20representations%20across%20time%2C%20and%20iv%29%20multi-source%20data%0Apretraining%20to%20generalize%20across%20various%20LiDAR%20configurations.%20Our%20approach%0Adelivers%20significant%20performance%20improvements%20over%20state-of-the-art%20methods%20in%0Aboth%20linear%20probing%20and%20fine-tuning%20tasks%20for%20both%20LiDAR-based%20segmentation%20and%0Aobject%20detection.%20Extensive%20experiments%20on%20eleven%20large-scale%20multi-modal%0Adatasets%20highlight%20our%20superior%20performance%2C%20demonstrating%20the%20adaptability%2C%0Aefficiency%2C%20and%20robustness%20in%20real-world%20autonomous%20driving%20scenarios.%0A&entry.1838667208=http%3A//arxiv.org/abs/2501.04005v1&entry.124074799=Read"},
{"title": "Finer: Investigating and Enhancing Fine-Grained Visual Concept\n  Recognition in Large Vision Language Models", "author": "Jeonghwan Kim and Heng Ji", "abstract": "  Recent advances in instruction-tuned Large Vision-Language Models (LVLMs)\nhave imbued the models with the ability to generate high-level, image-grounded\nexplanations with ease. While such capability is largely attributed to the rich\nworld knowledge contained within the Large Language Models (LLMs), our work\nreveals their shortcomings in fine-grained visual categorization (FGVC) across\nsix different benchmark settings. Most recent state-of-the-art LVLMs like\nLLaVa-1.5, InstructBLIP and GPT-4V not only severely deteriorate in terms of\nclassification performance, e.g., average drop of 65.58 in EM for Stanford Dogs\nfor LLaVA-1.5, but also struggle to generate an accurate explanation with\ndetailed attributes based on the concept that appears within an input image\ndespite their capability to generate holistic image-level descriptions.\nIn-depth analyses show that instruction-tuned LVLMs exhibit modality gap,\nshowing discrepancy when given textual and visual inputs that correspond to the\nsame concept, preventing the image modality from leveraging the rich parametric\nknowledge within the LLMs. In an effort to further the community's endeavor in\nthis direction, we propose a multiple granularity attribute-centric evaluation\nbenchmark, Finer, which aims to establish a ground to evaluate LVLMs'\nfine-grained visual comprehension ability and provide significantly improved\nexplainability.\n", "link": "http://arxiv.org/abs/2402.16315v4", "date": "2025-01-07", "relevancy": 2.9905, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.6309}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.6309}, {"title": "Self-supervised Photographic Image Layout Representation Learning", "link": "http://arxiv.org/abs/2403.03740v1", "similarity": 0.5324}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Finer%3A%20Investigating%20and%20Enhancing%20Fine-Grained%20Visual%20Concept%0A%20%20Recognition%20in%20Large%20Vision%20Language%20Models&body=Title%3A%20Finer%3A%20Investigating%20and%20Enhancing%20Fine-Grained%20Visual%20Concept%0A%20%20Recognition%20in%20Large%20Vision%20Language%20Models%0AAuthor%3A%20Jeonghwan%20Kim%20and%20Heng%20Ji%0AAbstract%3A%20%20%20Recent%20advances%20in%20instruction-tuned%20Large%20Vision-Language%20Models%20%28LVLMs%29%0Ahave%20imbued%20the%20models%20with%20the%20ability%20to%20generate%20high-level%2C%20image-grounded%0Aexplanations%20with%20ease.%20While%20such%20capability%20is%20largely%20attributed%20to%20the%20rich%0Aworld%20knowledge%20contained%20within%20the%20Large%20Language%20Models%20%28LLMs%29%2C%20our%20work%0Areveals%20their%20shortcomings%20in%20fine-grained%20visual%20categorization%20%28FGVC%29%20across%0Asix%20different%20benchmark%20settings.%20Most%20recent%20state-of-the-art%20LVLMs%20like%0ALLaVa-1.5%2C%20InstructBLIP%20and%20GPT-4V%20not%20only%20severely%20deteriorate%20in%20terms%20of%0Aclassification%20performance%2C%20e.g.%2C%20average%20drop%20of%2065.58%20in%20EM%20for%20Stanford%20Dogs%0Afor%20LLaVA-1.5%2C%20but%20also%20struggle%20to%20generate%20an%20accurate%20explanation%20with%0Adetailed%20attributes%20based%20on%20the%20concept%20that%20appears%20within%20an%20input%20image%0Adespite%20their%20capability%20to%20generate%20holistic%20image-level%20descriptions.%0AIn-depth%20analyses%20show%20that%20instruction-tuned%20LVLMs%20exhibit%20modality%20gap%2C%0Ashowing%20discrepancy%20when%20given%20textual%20and%20visual%20inputs%20that%20correspond%20to%20the%0Asame%20concept%2C%20preventing%20the%20image%20modality%20from%20leveraging%20the%20rich%20parametric%0Aknowledge%20within%20the%20LLMs.%20In%20an%20effort%20to%20further%20the%20community%27s%20endeavor%20in%0Athis%20direction%2C%20we%20propose%20a%20multiple%20granularity%20attribute-centric%20evaluation%0Abenchmark%2C%20Finer%2C%20which%20aims%20to%20establish%20a%20ground%20to%20evaluate%20LVLMs%27%0Afine-grained%20visual%20comprehension%20ability%20and%20provide%20significantly%20improved%0Aexplainability.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2402.16315v4%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DFiner%253A%2520Investigating%2520and%2520Enhancing%2520Fine-Grained%2520Visual%2520Concept%250A%2520%2520Recognition%2520in%2520Large%2520Vision%2520Language%2520Models%26entry.906535625%3DJeonghwan%2520Kim%2520and%2520Heng%2520Ji%26entry.1292438233%3D%2520%2520Recent%2520advances%2520in%2520instruction-tuned%2520Large%2520Vision-Language%2520Models%2520%2528LVLMs%2529%250Ahave%2520imbued%2520the%2520models%2520with%2520the%2520ability%2520to%2520generate%2520high-level%252C%2520image-grounded%250Aexplanations%2520with%2520ease.%2520While%2520such%2520capability%2520is%2520largely%2520attributed%2520to%2520the%2520rich%250Aworld%2520knowledge%2520contained%2520within%2520the%2520Large%2520Language%2520Models%2520%2528LLMs%2529%252C%2520our%2520work%250Areveals%2520their%2520shortcomings%2520in%2520fine-grained%2520visual%2520categorization%2520%2528FGVC%2529%2520across%250Asix%2520different%2520benchmark%2520settings.%2520Most%2520recent%2520state-of-the-art%2520LVLMs%2520like%250ALLaVa-1.5%252C%2520InstructBLIP%2520and%2520GPT-4V%2520not%2520only%2520severely%2520deteriorate%2520in%2520terms%2520of%250Aclassification%2520performance%252C%2520e.g.%252C%2520average%2520drop%2520of%252065.58%2520in%2520EM%2520for%2520Stanford%2520Dogs%250Afor%2520LLaVA-1.5%252C%2520but%2520also%2520struggle%2520to%2520generate%2520an%2520accurate%2520explanation%2520with%250Adetailed%2520attributes%2520based%2520on%2520the%2520concept%2520that%2520appears%2520within%2520an%2520input%2520image%250Adespite%2520their%2520capability%2520to%2520generate%2520holistic%2520image-level%2520descriptions.%250AIn-depth%2520analyses%2520show%2520that%2520instruction-tuned%2520LVLMs%2520exhibit%2520modality%2520gap%252C%250Ashowing%2520discrepancy%2520when%2520given%2520textual%2520and%2520visual%2520inputs%2520that%2520correspond%2520to%2520the%250Asame%2520concept%252C%2520preventing%2520the%2520image%2520modality%2520from%2520leveraging%2520the%2520rich%2520parametric%250Aknowledge%2520within%2520the%2520LLMs.%2520In%2520an%2520effort%2520to%2520further%2520the%2520community%2527s%2520endeavor%2520in%250Athis%2520direction%252C%2520we%2520propose%2520a%2520multiple%2520granularity%2520attribute-centric%2520evaluation%250Abenchmark%252C%2520Finer%252C%2520which%2520aims%2520to%2520establish%2520a%2520ground%2520to%2520evaluate%2520LVLMs%2527%250Afine-grained%2520visual%2520comprehension%2520ability%2520and%2520provide%2520significantly%2520improved%250Aexplainability.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2402.16315v4%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Finer%3A%20Investigating%20and%20Enhancing%20Fine-Grained%20Visual%20Concept%0A%20%20Recognition%20in%20Large%20Vision%20Language%20Models&entry.906535625=Jeonghwan%20Kim%20and%20Heng%20Ji&entry.1292438233=%20%20Recent%20advances%20in%20instruction-tuned%20Large%20Vision-Language%20Models%20%28LVLMs%29%0Ahave%20imbued%20the%20models%20with%20the%20ability%20to%20generate%20high-level%2C%20image-grounded%0Aexplanations%20with%20ease.%20While%20such%20capability%20is%20largely%20attributed%20to%20the%20rich%0Aworld%20knowledge%20contained%20within%20the%20Large%20Language%20Models%20%28LLMs%29%2C%20our%20work%0Areveals%20their%20shortcomings%20in%20fine-grained%20visual%20categorization%20%28FGVC%29%20across%0Asix%20different%20benchmark%20settings.%20Most%20recent%20state-of-the-art%20LVLMs%20like%0ALLaVa-1.5%2C%20InstructBLIP%20and%20GPT-4V%20not%20only%20severely%20deteriorate%20in%20terms%20of%0Aclassification%20performance%2C%20e.g.%2C%20average%20drop%20of%2065.58%20in%20EM%20for%20Stanford%20Dogs%0Afor%20LLaVA-1.5%2C%20but%20also%20struggle%20to%20generate%20an%20accurate%20explanation%20with%0Adetailed%20attributes%20based%20on%20the%20concept%20that%20appears%20within%20an%20input%20image%0Adespite%20their%20capability%20to%20generate%20holistic%20image-level%20descriptions.%0AIn-depth%20analyses%20show%20that%20instruction-tuned%20LVLMs%20exhibit%20modality%20gap%2C%0Ashowing%20discrepancy%20when%20given%20textual%20and%20visual%20inputs%20that%20correspond%20to%20the%0Asame%20concept%2C%20preventing%20the%20image%20modality%20from%20leveraging%20the%20rich%20parametric%0Aknowledge%20within%20the%20LLMs.%20In%20an%20effort%20to%20further%20the%20community%27s%20endeavor%20in%0Athis%20direction%2C%20we%20propose%20a%20multiple%20granularity%20attribute-centric%20evaluation%0Abenchmark%2C%20Finer%2C%20which%20aims%20to%20establish%20a%20ground%20to%20evaluate%20LVLMs%27%0Afine-grained%20visual%20comprehension%20ability%20and%20provide%20significantly%20improved%0Aexplainability.%0A&entry.1838667208=http%3A//arxiv.org/abs/2402.16315v4&entry.124074799=Read"},
{"title": "Gaze-guided Hand-Object Interaction Synthesis: Dataset and Method", "author": "Jie Tian and Ran Ji and Lingxiao Yang and Suting Ni and Yuexin Ma and Lan Xu and Jingyi Yu and Ye Shi and Jingya Wang", "abstract": "  Gaze plays a crucial role in revealing human attention and intention,\nparticularly in hand-object interaction scenarios, where it guides and\nsynchronizes complex tasks that require precise coordination between the brain,\nhand, and object. Motivated by this, we introduce a novel task: Gaze-Guided\nHand-Object Interaction Synthesis, with potential applications in augmented\nreality, virtual reality, and assistive technologies. To support this task, we\npresent GazeHOI, the first dataset to capture simultaneous 3D modeling of gaze,\nhand, and object interactions. This task poses significant challenges due to\nthe inherent sparsity and noise in gaze data, as well as the need for high\nconsistency and physical plausibility in generating hand and object motions. To\ntackle these issues, we propose a stacked gaze-guided hand-object interaction\ndiffusion model, named GHO-Diffusion. The stacked design effectively reduces\nthe complexity of motion generation. We also introduce HOI-Manifold Guidance\nduring the sampling stage of GHO-Diffusion, enabling fine-grained control over\ngenerated motions while maintaining the data manifold. Additionally, we propose\na spatial-temporal gaze feature encoding for the diffusion condition and select\ndiffusion results based on consistency scores between gaze-contact maps and\ngaze-interaction trajectories. Extensive experiments highlight the\neffectiveness of our method and the unique contributions of our dataset. More\ndetails in https://takiee.github.io/gaze-hoi/.\n", "link": "http://arxiv.org/abs/2403.16169v5", "date": "2025-01-07", "relevancy": 2.9665, "topK": [{"title": "Synthesizing Moving People with 3D Control", "link": "http://arxiv.org/abs/2401.10889v2", "similarity": 0.6175}, {"title": "VirtualModel: Generating Object-ID-retentive Human-object Interaction\n  Image by Diffusion Model for E-commerce Marketing", "link": "http://arxiv.org/abs/2405.09985v1", "similarity": 0.5877}, {"title": "UGG: Unified Generative Grasping", "link": "https://arxiv.org/abs/2311.16917", "similarity": 0.5747}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Gaze-guided%20Hand-Object%20Interaction%20Synthesis%3A%20Dataset%20and%20Method&body=Title%3A%20Gaze-guided%20Hand-Object%20Interaction%20Synthesis%3A%20Dataset%20and%20Method%0AAuthor%3A%20Jie%20Tian%20and%20Ran%20Ji%20and%20Lingxiao%20Yang%20and%20Suting%20Ni%20and%20Yuexin%20Ma%20and%20Lan%20Xu%20and%20Jingyi%20Yu%20and%20Ye%20Shi%20and%20Jingya%20Wang%0AAbstract%3A%20%20%20Gaze%20plays%20a%20crucial%20role%20in%20revealing%20human%20attention%20and%20intention%2C%0Aparticularly%20in%20hand-object%20interaction%20scenarios%2C%20where%20it%20guides%20and%0Asynchronizes%20complex%20tasks%20that%20require%20precise%20coordination%20between%20the%20brain%2C%0Ahand%2C%20and%20object.%20Motivated%20by%20this%2C%20we%20introduce%20a%20novel%20task%3A%20Gaze-Guided%0AHand-Object%20Interaction%20Synthesis%2C%20with%20potential%20applications%20in%20augmented%0Areality%2C%20virtual%20reality%2C%20and%20assistive%20technologies.%20To%20support%20this%20task%2C%20we%0Apresent%20GazeHOI%2C%20the%20first%20dataset%20to%20capture%20simultaneous%203D%20modeling%20of%20gaze%2C%0Ahand%2C%20and%20object%20interactions.%20This%20task%20poses%20significant%20challenges%20due%20to%0Athe%20inherent%20sparsity%20and%20noise%20in%20gaze%20data%2C%20as%20well%20as%20the%20need%20for%20high%0Aconsistency%20and%20physical%20plausibility%20in%20generating%20hand%20and%20object%20motions.%20To%0Atackle%20these%20issues%2C%20we%20propose%20a%20stacked%20gaze-guided%20hand-object%20interaction%0Adiffusion%20model%2C%20named%20GHO-Diffusion.%20The%20stacked%20design%20effectively%20reduces%0Athe%20complexity%20of%20motion%20generation.%20We%20also%20introduce%20HOI-Manifold%20Guidance%0Aduring%20the%20sampling%20stage%20of%20GHO-Diffusion%2C%20enabling%20fine-grained%20control%20over%0Agenerated%20motions%20while%20maintaining%20the%20data%20manifold.%20Additionally%2C%20we%20propose%0Aa%20spatial-temporal%20gaze%20feature%20encoding%20for%20the%20diffusion%20condition%20and%20select%0Adiffusion%20results%20based%20on%20consistency%20scores%20between%20gaze-contact%20maps%20and%0Agaze-interaction%20trajectories.%20Extensive%20experiments%20highlight%20the%0Aeffectiveness%20of%20our%20method%20and%20the%20unique%20contributions%20of%20our%20dataset.%20More%0Adetails%20in%20https%3A//takiee.github.io/gaze-hoi/.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2403.16169v5%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DGaze-guided%2520Hand-Object%2520Interaction%2520Synthesis%253A%2520Dataset%2520and%2520Method%26entry.906535625%3DJie%2520Tian%2520and%2520Ran%2520Ji%2520and%2520Lingxiao%2520Yang%2520and%2520Suting%2520Ni%2520and%2520Yuexin%2520Ma%2520and%2520Lan%2520Xu%2520and%2520Jingyi%2520Yu%2520and%2520Ye%2520Shi%2520and%2520Jingya%2520Wang%26entry.1292438233%3D%2520%2520Gaze%2520plays%2520a%2520crucial%2520role%2520in%2520revealing%2520human%2520attention%2520and%2520intention%252C%250Aparticularly%2520in%2520hand-object%2520interaction%2520scenarios%252C%2520where%2520it%2520guides%2520and%250Asynchronizes%2520complex%2520tasks%2520that%2520require%2520precise%2520coordination%2520between%2520the%2520brain%252C%250Ahand%252C%2520and%2520object.%2520Motivated%2520by%2520this%252C%2520we%2520introduce%2520a%2520novel%2520task%253A%2520Gaze-Guided%250AHand-Object%2520Interaction%2520Synthesis%252C%2520with%2520potential%2520applications%2520in%2520augmented%250Areality%252C%2520virtual%2520reality%252C%2520and%2520assistive%2520technologies.%2520To%2520support%2520this%2520task%252C%2520we%250Apresent%2520GazeHOI%252C%2520the%2520first%2520dataset%2520to%2520capture%2520simultaneous%25203D%2520modeling%2520of%2520gaze%252C%250Ahand%252C%2520and%2520object%2520interactions.%2520This%2520task%2520poses%2520significant%2520challenges%2520due%2520to%250Athe%2520inherent%2520sparsity%2520and%2520noise%2520in%2520gaze%2520data%252C%2520as%2520well%2520as%2520the%2520need%2520for%2520high%250Aconsistency%2520and%2520physical%2520plausibility%2520in%2520generating%2520hand%2520and%2520object%2520motions.%2520To%250Atackle%2520these%2520issues%252C%2520we%2520propose%2520a%2520stacked%2520gaze-guided%2520hand-object%2520interaction%250Adiffusion%2520model%252C%2520named%2520GHO-Diffusion.%2520The%2520stacked%2520design%2520effectively%2520reduces%250Athe%2520complexity%2520of%2520motion%2520generation.%2520We%2520also%2520introduce%2520HOI-Manifold%2520Guidance%250Aduring%2520the%2520sampling%2520stage%2520of%2520GHO-Diffusion%252C%2520enabling%2520fine-grained%2520control%2520over%250Agenerated%2520motions%2520while%2520maintaining%2520the%2520data%2520manifold.%2520Additionally%252C%2520we%2520propose%250Aa%2520spatial-temporal%2520gaze%2520feature%2520encoding%2520for%2520the%2520diffusion%2520condition%2520and%2520select%250Adiffusion%2520results%2520based%2520on%2520consistency%2520scores%2520between%2520gaze-contact%2520maps%2520and%250Agaze-interaction%2520trajectories.%2520Extensive%2520experiments%2520highlight%2520the%250Aeffectiveness%2520of%2520our%2520method%2520and%2520the%2520unique%2520contributions%2520of%2520our%2520dataset.%2520More%250Adetails%2520in%2520https%253A//takiee.github.io/gaze-hoi/.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2403.16169v5%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Gaze-guided%20Hand-Object%20Interaction%20Synthesis%3A%20Dataset%20and%20Method&entry.906535625=Jie%20Tian%20and%20Ran%20Ji%20and%20Lingxiao%20Yang%20and%20Suting%20Ni%20and%20Yuexin%20Ma%20and%20Lan%20Xu%20and%20Jingyi%20Yu%20and%20Ye%20Shi%20and%20Jingya%20Wang&entry.1292438233=%20%20Gaze%20plays%20a%20crucial%20role%20in%20revealing%20human%20attention%20and%20intention%2C%0Aparticularly%20in%20hand-object%20interaction%20scenarios%2C%20where%20it%20guides%20and%0Asynchronizes%20complex%20tasks%20that%20require%20precise%20coordination%20between%20the%20brain%2C%0Ahand%2C%20and%20object.%20Motivated%20by%20this%2C%20we%20introduce%20a%20novel%20task%3A%20Gaze-Guided%0AHand-Object%20Interaction%20Synthesis%2C%20with%20potential%20applications%20in%20augmented%0Areality%2C%20virtual%20reality%2C%20and%20assistive%20technologies.%20To%20support%20this%20task%2C%20we%0Apresent%20GazeHOI%2C%20the%20first%20dataset%20to%20capture%20simultaneous%203D%20modeling%20of%20gaze%2C%0Ahand%2C%20and%20object%20interactions.%20This%20task%20poses%20significant%20challenges%20due%20to%0Athe%20inherent%20sparsity%20and%20noise%20in%20gaze%20data%2C%20as%20well%20as%20the%20need%20for%20high%0Aconsistency%20and%20physical%20plausibility%20in%20generating%20hand%20and%20object%20motions.%20To%0Atackle%20these%20issues%2C%20we%20propose%20a%20stacked%20gaze-guided%20hand-object%20interaction%0Adiffusion%20model%2C%20named%20GHO-Diffusion.%20The%20stacked%20design%20effectively%20reduces%0Athe%20complexity%20of%20motion%20generation.%20We%20also%20introduce%20HOI-Manifold%20Guidance%0Aduring%20the%20sampling%20stage%20of%20GHO-Diffusion%2C%20enabling%20fine-grained%20control%20over%0Agenerated%20motions%20while%20maintaining%20the%20data%20manifold.%20Additionally%2C%20we%20propose%0Aa%20spatial-temporal%20gaze%20feature%20encoding%20for%20the%20diffusion%20condition%20and%20select%0Adiffusion%20results%20based%20on%20consistency%20scores%20between%20gaze-contact%20maps%20and%0Agaze-interaction%20trajectories.%20Extensive%20experiments%20highlight%20the%0Aeffectiveness%20of%20our%20method%20and%20the%20unique%20contributions%20of%20our%20dataset.%20More%0Adetails%20in%20https%3A//takiee.github.io/gaze-hoi/.%0A&entry.1838667208=http%3A//arxiv.org/abs/2403.16169v5&entry.124074799=Read"},
{"title": "Sa2VA: Marrying SAM2 with LLaVA for Dense Grounded Understanding of\n  Images and Videos", "author": "Haobo Yuan and Xiangtai Li and Tao Zhang and Zilong Huang and Shilin Xu and Shunping Ji and Yunhai Tong and Lu Qi and Jiashi Feng and Ming-Hsuan Yang", "abstract": "  This work presents Sa2VA, the first unified model for dense grounded\nunderstanding of both images and videos. Unlike existing multi-modal large\nlanguage models, which are often limited to specific modalities and tasks,\nSa2VA supports a wide range of image and video tasks, including referring\nsegmentation and conversation, with minimal one-shot instruction tuning. Sa2VA\ncombines SAM-2, a foundation video segmentation model, with LLaVA, an advanced\nvision-language model, and unifies text, image, and video into a shared LLM\ntoken space. Using the LLM, Sa2VA generates instruction tokens that guide SAM-2\nin producing precise masks, enabling a grounded, multi-modal understanding of\nboth static and dynamic visual content. Additionally, we introduce Ref-SAV, an\nauto-labeled dataset containing over 72k object expressions in complex video\nscenes, designed to boost model performance. We also manually validate 2k video\nobjects in the Ref-SAV datasets to benchmark referring video object\nsegmentation in complex environments. Experiments show that Sa2VA achieves\nstate-of-the-art across multiple tasks, particularly in referring video object\nsegmentation, highlighting its potential for complex real-world applications.\n", "link": "http://arxiv.org/abs/2501.04001v1", "date": "2025-01-07", "relevancy": 2.9548, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5953}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5953}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5823}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Sa2VA%3A%20Marrying%20SAM2%20with%20LLaVA%20for%20Dense%20Grounded%20Understanding%20of%0A%20%20Images%20and%20Videos&body=Title%3A%20Sa2VA%3A%20Marrying%20SAM2%20with%20LLaVA%20for%20Dense%20Grounded%20Understanding%20of%0A%20%20Images%20and%20Videos%0AAuthor%3A%20Haobo%20Yuan%20and%20Xiangtai%20Li%20and%20Tao%20Zhang%20and%20Zilong%20Huang%20and%20Shilin%20Xu%20and%20Shunping%20Ji%20and%20Yunhai%20Tong%20and%20Lu%20Qi%20and%20Jiashi%20Feng%20and%20Ming-Hsuan%20Yang%0AAbstract%3A%20%20%20This%20work%20presents%20Sa2VA%2C%20the%20first%20unified%20model%20for%20dense%20grounded%0Aunderstanding%20of%20both%20images%20and%20videos.%20Unlike%20existing%20multi-modal%20large%0Alanguage%20models%2C%20which%20are%20often%20limited%20to%20specific%20modalities%20and%20tasks%2C%0ASa2VA%20supports%20a%20wide%20range%20of%20image%20and%20video%20tasks%2C%20including%20referring%0Asegmentation%20and%20conversation%2C%20with%20minimal%20one-shot%20instruction%20tuning.%20Sa2VA%0Acombines%20SAM-2%2C%20a%20foundation%20video%20segmentation%20model%2C%20with%20LLaVA%2C%20an%20advanced%0Avision-language%20model%2C%20and%20unifies%20text%2C%20image%2C%20and%20video%20into%20a%20shared%20LLM%0Atoken%20space.%20Using%20the%20LLM%2C%20Sa2VA%20generates%20instruction%20tokens%20that%20guide%20SAM-2%0Ain%20producing%20precise%20masks%2C%20enabling%20a%20grounded%2C%20multi-modal%20understanding%20of%0Aboth%20static%20and%20dynamic%20visual%20content.%20Additionally%2C%20we%20introduce%20Ref-SAV%2C%20an%0Aauto-labeled%20dataset%20containing%20over%2072k%20object%20expressions%20in%20complex%20video%0Ascenes%2C%20designed%20to%20boost%20model%20performance.%20We%20also%20manually%20validate%202k%20video%0Aobjects%20in%20the%20Ref-SAV%20datasets%20to%20benchmark%20referring%20video%20object%0Asegmentation%20in%20complex%20environments.%20Experiments%20show%20that%20Sa2VA%20achieves%0Astate-of-the-art%20across%20multiple%20tasks%2C%20particularly%20in%20referring%20video%20object%0Asegmentation%2C%20highlighting%20its%20potential%20for%20complex%20real-world%20applications.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2501.04001v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DSa2VA%253A%2520Marrying%2520SAM2%2520with%2520LLaVA%2520for%2520Dense%2520Grounded%2520Understanding%2520of%250A%2520%2520Images%2520and%2520Videos%26entry.906535625%3DHaobo%2520Yuan%2520and%2520Xiangtai%2520Li%2520and%2520Tao%2520Zhang%2520and%2520Zilong%2520Huang%2520and%2520Shilin%2520Xu%2520and%2520Shunping%2520Ji%2520and%2520Yunhai%2520Tong%2520and%2520Lu%2520Qi%2520and%2520Jiashi%2520Feng%2520and%2520Ming-Hsuan%2520Yang%26entry.1292438233%3D%2520%2520This%2520work%2520presents%2520Sa2VA%252C%2520the%2520first%2520unified%2520model%2520for%2520dense%2520grounded%250Aunderstanding%2520of%2520both%2520images%2520and%2520videos.%2520Unlike%2520existing%2520multi-modal%2520large%250Alanguage%2520models%252C%2520which%2520are%2520often%2520limited%2520to%2520specific%2520modalities%2520and%2520tasks%252C%250ASa2VA%2520supports%2520a%2520wide%2520range%2520of%2520image%2520and%2520video%2520tasks%252C%2520including%2520referring%250Asegmentation%2520and%2520conversation%252C%2520with%2520minimal%2520one-shot%2520instruction%2520tuning.%2520Sa2VA%250Acombines%2520SAM-2%252C%2520a%2520foundation%2520video%2520segmentation%2520model%252C%2520with%2520LLaVA%252C%2520an%2520advanced%250Avision-language%2520model%252C%2520and%2520unifies%2520text%252C%2520image%252C%2520and%2520video%2520into%2520a%2520shared%2520LLM%250Atoken%2520space.%2520Using%2520the%2520LLM%252C%2520Sa2VA%2520generates%2520instruction%2520tokens%2520that%2520guide%2520SAM-2%250Ain%2520producing%2520precise%2520masks%252C%2520enabling%2520a%2520grounded%252C%2520multi-modal%2520understanding%2520of%250Aboth%2520static%2520and%2520dynamic%2520visual%2520content.%2520Additionally%252C%2520we%2520introduce%2520Ref-SAV%252C%2520an%250Aauto-labeled%2520dataset%2520containing%2520over%252072k%2520object%2520expressions%2520in%2520complex%2520video%250Ascenes%252C%2520designed%2520to%2520boost%2520model%2520performance.%2520We%2520also%2520manually%2520validate%25202k%2520video%250Aobjects%2520in%2520the%2520Ref-SAV%2520datasets%2520to%2520benchmark%2520referring%2520video%2520object%250Asegmentation%2520in%2520complex%2520environments.%2520Experiments%2520show%2520that%2520Sa2VA%2520achieves%250Astate-of-the-art%2520across%2520multiple%2520tasks%252C%2520particularly%2520in%2520referring%2520video%2520object%250Asegmentation%252C%2520highlighting%2520its%2520potential%2520for%2520complex%2520real-world%2520applications.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2501.04001v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Sa2VA%3A%20Marrying%20SAM2%20with%20LLaVA%20for%20Dense%20Grounded%20Understanding%20of%0A%20%20Images%20and%20Videos&entry.906535625=Haobo%20Yuan%20and%20Xiangtai%20Li%20and%20Tao%20Zhang%20and%20Zilong%20Huang%20and%20Shilin%20Xu%20and%20Shunping%20Ji%20and%20Yunhai%20Tong%20and%20Lu%20Qi%20and%20Jiashi%20Feng%20and%20Ming-Hsuan%20Yang&entry.1292438233=%20%20This%20work%20presents%20Sa2VA%2C%20the%20first%20unified%20model%20for%20dense%20grounded%0Aunderstanding%20of%20both%20images%20and%20videos.%20Unlike%20existing%20multi-modal%20large%0Alanguage%20models%2C%20which%20are%20often%20limited%20to%20specific%20modalities%20and%20tasks%2C%0ASa2VA%20supports%20a%20wide%20range%20of%20image%20and%20video%20tasks%2C%20including%20referring%0Asegmentation%20and%20conversation%2C%20with%20minimal%20one-shot%20instruction%20tuning.%20Sa2VA%0Acombines%20SAM-2%2C%20a%20foundation%20video%20segmentation%20model%2C%20with%20LLaVA%2C%20an%20advanced%0Avision-language%20model%2C%20and%20unifies%20text%2C%20image%2C%20and%20video%20into%20a%20shared%20LLM%0Atoken%20space.%20Using%20the%20LLM%2C%20Sa2VA%20generates%20instruction%20tokens%20that%20guide%20SAM-2%0Ain%20producing%20precise%20masks%2C%20enabling%20a%20grounded%2C%20multi-modal%20understanding%20of%0Aboth%20static%20and%20dynamic%20visual%20content.%20Additionally%2C%20we%20introduce%20Ref-SAV%2C%20an%0Aauto-labeled%20dataset%20containing%20over%2072k%20object%20expressions%20in%20complex%20video%0Ascenes%2C%20designed%20to%20boost%20model%20performance.%20We%20also%20manually%20validate%202k%20video%0Aobjects%20in%20the%20Ref-SAV%20datasets%20to%20benchmark%20referring%20video%20object%0Asegmentation%20in%20complex%20environments.%20Experiments%20show%20that%20Sa2VA%20achieves%0Astate-of-the-art%20across%20multiple%20tasks%2C%20particularly%20in%20referring%20video%20object%0Asegmentation%2C%20highlighting%20its%20potential%20for%20complex%20real-world%20applications.%0A&entry.1838667208=http%3A//arxiv.org/abs/2501.04001v1&entry.124074799=Read"},
{"title": "Hyperbolic Contrastive Learning for Hierarchical 3D Point Cloud\n  Embedding", "author": "Yingjie Liu and Pengyu Zhang and Ziyao He and Mingsong Chen and Xuan Tang and Xian Wei", "abstract": "  Hyperbolic spaces allow for more efficient modeling of complex, hierarchical\nstructures, which is particularly beneficial in tasks involving multi-modal\ndata. Although hyperbolic geometries have been proven effective for\nlanguage-image pre-training, their capabilities to unify language, image, and\n3D Point Cloud modalities are under-explored. We extend the 3D Point Cloud\nmodality in hyperbolic multi-modal contrastive pre-training. Additionally, we\nexplore the entailment, modality gap, and alignment regularizers for learning\nhierarchical 3D embeddings and facilitating the transfer of knowledge from both\nText and Image modalities. These regularizers enable the learning of\nintra-modal hierarchy within each modality and inter-modal hierarchy across\ntext, 2D images, and 3D Point Clouds. Experimental results demonstrate that our\nproposed training strategy yields an outstanding 3D Point Cloud encoder, and\nthe obtained 3D Point Cloud hierarchical embeddings significantly improve\nperformance on various downstream tasks.\n", "link": "http://arxiv.org/abs/2501.02285v2", "date": "2025-01-07", "relevancy": 2.9467, "topK": [{"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.6066}, {"title": "DDM-NET: End-to-end learning of keypoint feature Detection, Description and Matching for 3D localization", "link": "https://arxiv.org/abs/2212.04575", "similarity": 0.5809}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5805}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Hyperbolic%20Contrastive%20Learning%20for%20Hierarchical%203D%20Point%20Cloud%0A%20%20Embedding&body=Title%3A%20Hyperbolic%20Contrastive%20Learning%20for%20Hierarchical%203D%20Point%20Cloud%0A%20%20Embedding%0AAuthor%3A%20Yingjie%20Liu%20and%20Pengyu%20Zhang%20and%20Ziyao%20He%20and%20Mingsong%20Chen%20and%20Xuan%20Tang%20and%20Xian%20Wei%0AAbstract%3A%20%20%20Hyperbolic%20spaces%20allow%20for%20more%20efficient%20modeling%20of%20complex%2C%20hierarchical%0Astructures%2C%20which%20is%20particularly%20beneficial%20in%20tasks%20involving%20multi-modal%0Adata.%20Although%20hyperbolic%20geometries%20have%20been%20proven%20effective%20for%0Alanguage-image%20pre-training%2C%20their%20capabilities%20to%20unify%20language%2C%20image%2C%20and%0A3D%20Point%20Cloud%20modalities%20are%20under-explored.%20We%20extend%20the%203D%20Point%20Cloud%0Amodality%20in%20hyperbolic%20multi-modal%20contrastive%20pre-training.%20Additionally%2C%20we%0Aexplore%20the%20entailment%2C%20modality%20gap%2C%20and%20alignment%20regularizers%20for%20learning%0Ahierarchical%203D%20embeddings%20and%20facilitating%20the%20transfer%20of%20knowledge%20from%20both%0AText%20and%20Image%20modalities.%20These%20regularizers%20enable%20the%20learning%20of%0Aintra-modal%20hierarchy%20within%20each%20modality%20and%20inter-modal%20hierarchy%20across%0Atext%2C%202D%20images%2C%20and%203D%20Point%20Clouds.%20Experimental%20results%20demonstrate%20that%20our%0Aproposed%20training%20strategy%20yields%20an%20outstanding%203D%20Point%20Cloud%20encoder%2C%20and%0Athe%20obtained%203D%20Point%20Cloud%20hierarchical%20embeddings%20significantly%20improve%0Aperformance%20on%20various%20downstream%20tasks.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2501.02285v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DHyperbolic%2520Contrastive%2520Learning%2520for%2520Hierarchical%25203D%2520Point%2520Cloud%250A%2520%2520Embedding%26entry.906535625%3DYingjie%2520Liu%2520and%2520Pengyu%2520Zhang%2520and%2520Ziyao%2520He%2520and%2520Mingsong%2520Chen%2520and%2520Xuan%2520Tang%2520and%2520Xian%2520Wei%26entry.1292438233%3D%2520%2520Hyperbolic%2520spaces%2520allow%2520for%2520more%2520efficient%2520modeling%2520of%2520complex%252C%2520hierarchical%250Astructures%252C%2520which%2520is%2520particularly%2520beneficial%2520in%2520tasks%2520involving%2520multi-modal%250Adata.%2520Although%2520hyperbolic%2520geometries%2520have%2520been%2520proven%2520effective%2520for%250Alanguage-image%2520pre-training%252C%2520their%2520capabilities%2520to%2520unify%2520language%252C%2520image%252C%2520and%250A3D%2520Point%2520Cloud%2520modalities%2520are%2520under-explored.%2520We%2520extend%2520the%25203D%2520Point%2520Cloud%250Amodality%2520in%2520hyperbolic%2520multi-modal%2520contrastive%2520pre-training.%2520Additionally%252C%2520we%250Aexplore%2520the%2520entailment%252C%2520modality%2520gap%252C%2520and%2520alignment%2520regularizers%2520for%2520learning%250Ahierarchical%25203D%2520embeddings%2520and%2520facilitating%2520the%2520transfer%2520of%2520knowledge%2520from%2520both%250AText%2520and%2520Image%2520modalities.%2520These%2520regularizers%2520enable%2520the%2520learning%2520of%250Aintra-modal%2520hierarchy%2520within%2520each%2520modality%2520and%2520inter-modal%2520hierarchy%2520across%250Atext%252C%25202D%2520images%252C%2520and%25203D%2520Point%2520Clouds.%2520Experimental%2520results%2520demonstrate%2520that%2520our%250Aproposed%2520training%2520strategy%2520yields%2520an%2520outstanding%25203D%2520Point%2520Cloud%2520encoder%252C%2520and%250Athe%2520obtained%25203D%2520Point%2520Cloud%2520hierarchical%2520embeddings%2520significantly%2520improve%250Aperformance%2520on%2520various%2520downstream%2520tasks.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2501.02285v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Hyperbolic%20Contrastive%20Learning%20for%20Hierarchical%203D%20Point%20Cloud%0A%20%20Embedding&entry.906535625=Yingjie%20Liu%20and%20Pengyu%20Zhang%20and%20Ziyao%20He%20and%20Mingsong%20Chen%20and%20Xuan%20Tang%20and%20Xian%20Wei&entry.1292438233=%20%20Hyperbolic%20spaces%20allow%20for%20more%20efficient%20modeling%20of%20complex%2C%20hierarchical%0Astructures%2C%20which%20is%20particularly%20beneficial%20in%20tasks%20involving%20multi-modal%0Adata.%20Although%20hyperbolic%20geometries%20have%20been%20proven%20effective%20for%0Alanguage-image%20pre-training%2C%20their%20capabilities%20to%20unify%20language%2C%20image%2C%20and%0A3D%20Point%20Cloud%20modalities%20are%20under-explored.%20We%20extend%20the%203D%20Point%20Cloud%0Amodality%20in%20hyperbolic%20multi-modal%20contrastive%20pre-training.%20Additionally%2C%20we%0Aexplore%20the%20entailment%2C%20modality%20gap%2C%20and%20alignment%20regularizers%20for%20learning%0Ahierarchical%203D%20embeddings%20and%20facilitating%20the%20transfer%20of%20knowledge%20from%20both%0AText%20and%20Image%20modalities.%20These%20regularizers%20enable%20the%20learning%20of%0Aintra-modal%20hierarchy%20within%20each%20modality%20and%20inter-modal%20hierarchy%20across%0Atext%2C%202D%20images%2C%20and%203D%20Point%20Clouds.%20Experimental%20results%20demonstrate%20that%20our%0Aproposed%20training%20strategy%20yields%20an%20outstanding%203D%20Point%20Cloud%20encoder%2C%20and%0Athe%20obtained%203D%20Point%20Cloud%20hierarchical%20embeddings%20significantly%20improve%0Aperformance%20on%20various%20downstream%20tasks.%0A&entry.1838667208=http%3A//arxiv.org/abs/2501.02285v2&entry.124074799=Read"},
{"title": "LiMoE: Mixture of LiDAR Representation Learners from Automotive Scenes", "author": "Xiang Xu and Lingdong Kong and Hui Shuai and Liang Pan and Ziwei Liu and Qingshan Liu", "abstract": "  LiDAR data pretraining offers a promising approach to leveraging large-scale,\nreadily available datasets for enhanced data utilization. However, existing\nmethods predominantly focus on sparse voxel representation, overlooking the\ncomplementary attributes provided by other LiDAR representations. In this work,\nwe propose LiMoE, a framework that integrates the Mixture of Experts (MoE)\nparadigm into LiDAR data representation learning to synergistically combine\nmultiple representations, such as range images, sparse voxels, and raw points.\nOur approach consists of three stages: i) Image-to-LiDAR Pretraining, which\ntransfers prior knowledge from images to point clouds across different\nrepresentations; ii) Contrastive Mixture Learning (CML), which uses MoE to\nadaptively activate relevant attributes from each representation and distills\nthese mixed features into a unified 3D network; iii) Semantic Mixture\nSupervision (SMS), which combines semantic logits from multiple representations\nto boost downstream segmentation performance. Extensive experiments across 11\nlarge-scale LiDAR datasets demonstrate our effectiveness and superiority. The\ncode and model checkpoints have been made publicly accessible.\n", "link": "http://arxiv.org/abs/2501.04004v1", "date": "2025-01-07", "relevancy": 2.9225, "topK": [{"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5976}, {"title": "Self-supervised Photographic Image Layout Representation Learning", "link": "http://arxiv.org/abs/2403.03740v1", "similarity": 0.5786}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5773}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20LiMoE%3A%20Mixture%20of%20LiDAR%20Representation%20Learners%20from%20Automotive%20Scenes&body=Title%3A%20LiMoE%3A%20Mixture%20of%20LiDAR%20Representation%20Learners%20from%20Automotive%20Scenes%0AAuthor%3A%20Xiang%20Xu%20and%20Lingdong%20Kong%20and%20Hui%20Shuai%20and%20Liang%20Pan%20and%20Ziwei%20Liu%20and%20Qingshan%20Liu%0AAbstract%3A%20%20%20LiDAR%20data%20pretraining%20offers%20a%20promising%20approach%20to%20leveraging%20large-scale%2C%0Areadily%20available%20datasets%20for%20enhanced%20data%20utilization.%20However%2C%20existing%0Amethods%20predominantly%20focus%20on%20sparse%20voxel%20representation%2C%20overlooking%20the%0Acomplementary%20attributes%20provided%20by%20other%20LiDAR%20representations.%20In%20this%20work%2C%0Awe%20propose%20LiMoE%2C%20a%20framework%20that%20integrates%20the%20Mixture%20of%20Experts%20%28MoE%29%0Aparadigm%20into%20LiDAR%20data%20representation%20learning%20to%20synergistically%20combine%0Amultiple%20representations%2C%20such%20as%20range%20images%2C%20sparse%20voxels%2C%20and%20raw%20points.%0AOur%20approach%20consists%20of%20three%20stages%3A%20i%29%20Image-to-LiDAR%20Pretraining%2C%20which%0Atransfers%20prior%20knowledge%20from%20images%20to%20point%20clouds%20across%20different%0Arepresentations%3B%20ii%29%20Contrastive%20Mixture%20Learning%20%28CML%29%2C%20which%20uses%20MoE%20to%0Aadaptively%20activate%20relevant%20attributes%20from%20each%20representation%20and%20distills%0Athese%20mixed%20features%20into%20a%20unified%203D%20network%3B%20iii%29%20Semantic%20Mixture%0ASupervision%20%28SMS%29%2C%20which%20combines%20semantic%20logits%20from%20multiple%20representations%0Ato%20boost%20downstream%20segmentation%20performance.%20Extensive%20experiments%20across%2011%0Alarge-scale%20LiDAR%20datasets%20demonstrate%20our%20effectiveness%20and%20superiority.%20The%0Acode%20and%20model%20checkpoints%20have%20been%20made%20publicly%20accessible.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2501.04004v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DLiMoE%253A%2520Mixture%2520of%2520LiDAR%2520Representation%2520Learners%2520from%2520Automotive%2520Scenes%26entry.906535625%3DXiang%2520Xu%2520and%2520Lingdong%2520Kong%2520and%2520Hui%2520Shuai%2520and%2520Liang%2520Pan%2520and%2520Ziwei%2520Liu%2520and%2520Qingshan%2520Liu%26entry.1292438233%3D%2520%2520LiDAR%2520data%2520pretraining%2520offers%2520a%2520promising%2520approach%2520to%2520leveraging%2520large-scale%252C%250Areadily%2520available%2520datasets%2520for%2520enhanced%2520data%2520utilization.%2520However%252C%2520existing%250Amethods%2520predominantly%2520focus%2520on%2520sparse%2520voxel%2520representation%252C%2520overlooking%2520the%250Acomplementary%2520attributes%2520provided%2520by%2520other%2520LiDAR%2520representations.%2520In%2520this%2520work%252C%250Awe%2520propose%2520LiMoE%252C%2520a%2520framework%2520that%2520integrates%2520the%2520Mixture%2520of%2520Experts%2520%2528MoE%2529%250Aparadigm%2520into%2520LiDAR%2520data%2520representation%2520learning%2520to%2520synergistically%2520combine%250Amultiple%2520representations%252C%2520such%2520as%2520range%2520images%252C%2520sparse%2520voxels%252C%2520and%2520raw%2520points.%250AOur%2520approach%2520consists%2520of%2520three%2520stages%253A%2520i%2529%2520Image-to-LiDAR%2520Pretraining%252C%2520which%250Atransfers%2520prior%2520knowledge%2520from%2520images%2520to%2520point%2520clouds%2520across%2520different%250Arepresentations%253B%2520ii%2529%2520Contrastive%2520Mixture%2520Learning%2520%2528CML%2529%252C%2520which%2520uses%2520MoE%2520to%250Aadaptively%2520activate%2520relevant%2520attributes%2520from%2520each%2520representation%2520and%2520distills%250Athese%2520mixed%2520features%2520into%2520a%2520unified%25203D%2520network%253B%2520iii%2529%2520Semantic%2520Mixture%250ASupervision%2520%2528SMS%2529%252C%2520which%2520combines%2520semantic%2520logits%2520from%2520multiple%2520representations%250Ato%2520boost%2520downstream%2520segmentation%2520performance.%2520Extensive%2520experiments%2520across%252011%250Alarge-scale%2520LiDAR%2520datasets%2520demonstrate%2520our%2520effectiveness%2520and%2520superiority.%2520The%250Acode%2520and%2520model%2520checkpoints%2520have%2520been%2520made%2520publicly%2520accessible.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2501.04004v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=LiMoE%3A%20Mixture%20of%20LiDAR%20Representation%20Learners%20from%20Automotive%20Scenes&entry.906535625=Xiang%20Xu%20and%20Lingdong%20Kong%20and%20Hui%20Shuai%20and%20Liang%20Pan%20and%20Ziwei%20Liu%20and%20Qingshan%20Liu&entry.1292438233=%20%20LiDAR%20data%20pretraining%20offers%20a%20promising%20approach%20to%20leveraging%20large-scale%2C%0Areadily%20available%20datasets%20for%20enhanced%20data%20utilization.%20However%2C%20existing%0Amethods%20predominantly%20focus%20on%20sparse%20voxel%20representation%2C%20overlooking%20the%0Acomplementary%20attributes%20provided%20by%20other%20LiDAR%20representations.%20In%20this%20work%2C%0Awe%20propose%20LiMoE%2C%20a%20framework%20that%20integrates%20the%20Mixture%20of%20Experts%20%28MoE%29%0Aparadigm%20into%20LiDAR%20data%20representation%20learning%20to%20synergistically%20combine%0Amultiple%20representations%2C%20such%20as%20range%20images%2C%20sparse%20voxels%2C%20and%20raw%20points.%0AOur%20approach%20consists%20of%20three%20stages%3A%20i%29%20Image-to-LiDAR%20Pretraining%2C%20which%0Atransfers%20prior%20knowledge%20from%20images%20to%20point%20clouds%20across%20different%0Arepresentations%3B%20ii%29%20Contrastive%20Mixture%20Learning%20%28CML%29%2C%20which%20uses%20MoE%20to%0Aadaptively%20activate%20relevant%20attributes%20from%20each%20representation%20and%20distills%0Athese%20mixed%20features%20into%20a%20unified%203D%20network%3B%20iii%29%20Semantic%20Mixture%0ASupervision%20%28SMS%29%2C%20which%20combines%20semantic%20logits%20from%20multiple%20representations%0Ato%20boost%20downstream%20segmentation%20performance.%20Extensive%20experiments%20across%2011%0Alarge-scale%20LiDAR%20datasets%20demonstrate%20our%20effectiveness%20and%20superiority.%20The%0Acode%20and%20model%20checkpoints%20have%20been%20made%20publicly%20accessible.%0A&entry.1838667208=http%3A//arxiv.org/abs/2501.04004v1&entry.124074799=Read"},
{"title": "Vision Language Models as Values Detectors", "author": "Giulio Antonio Abbo and Tony Belpaeme", "abstract": "  Large Language Models integrating textual and visual inputs have introduced\nnew possibilities for interpreting complex data. Despite their remarkable\nability to generate coherent and contextually relevant text based on visual\nstimuli, the alignment of these models with human perception in identifying\nrelevant elements in images requires further exploration. This paper\ninvestigates the alignment between state-of-the-art LLMs and human annotators\nin detecting elements of relevance within home environment scenarios. We\ncreated a set of twelve images depicting various domestic scenarios and\nenlisted fourteen annotators to identify the key element in each image. We then\ncompared these human responses with outputs from five different LLMs, including\nGPT-4o and four LLaVA variants. Our findings reveal a varied degree of\nalignment, with LLaVA 34B showing the highest performance but still scoring\nlow. However, an analysis of the results highlights the models' potential to\ndetect value-laden elements in images, suggesting that with improved training\nand refined prompts, LLMs could enhance applications in social robotics,\nassistive technologies, and human-computer interaction by providing deeper\ninsights and more contextually relevant responses.\n", "link": "http://arxiv.org/abs/2501.03957v1", "date": "2025-01-07", "relevancy": 2.9045, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.613}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.613}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5166}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Vision%20Language%20Models%20as%20Values%20Detectors&body=Title%3A%20Vision%20Language%20Models%20as%20Values%20Detectors%0AAuthor%3A%20Giulio%20Antonio%20Abbo%20and%20Tony%20Belpaeme%0AAbstract%3A%20%20%20Large%20Language%20Models%20integrating%20textual%20and%20visual%20inputs%20have%20introduced%0Anew%20possibilities%20for%20interpreting%20complex%20data.%20Despite%20their%20remarkable%0Aability%20to%20generate%20coherent%20and%20contextually%20relevant%20text%20based%20on%20visual%0Astimuli%2C%20the%20alignment%20of%20these%20models%20with%20human%20perception%20in%20identifying%0Arelevant%20elements%20in%20images%20requires%20further%20exploration.%20This%20paper%0Ainvestigates%20the%20alignment%20between%20state-of-the-art%20LLMs%20and%20human%20annotators%0Ain%20detecting%20elements%20of%20relevance%20within%20home%20environment%20scenarios.%20We%0Acreated%20a%20set%20of%20twelve%20images%20depicting%20various%20domestic%20scenarios%20and%0Aenlisted%20fourteen%20annotators%20to%20identify%20the%20key%20element%20in%20each%20image.%20We%20then%0Acompared%20these%20human%20responses%20with%20outputs%20from%20five%20different%20LLMs%2C%20including%0AGPT-4o%20and%20four%20LLaVA%20variants.%20Our%20findings%20reveal%20a%20varied%20degree%20of%0Aalignment%2C%20with%20LLaVA%2034B%20showing%20the%20highest%20performance%20but%20still%20scoring%0Alow.%20However%2C%20an%20analysis%20of%20the%20results%20highlights%20the%20models%27%20potential%20to%0Adetect%20value-laden%20elements%20in%20images%2C%20suggesting%20that%20with%20improved%20training%0Aand%20refined%20prompts%2C%20LLMs%20could%20enhance%20applications%20in%20social%20robotics%2C%0Aassistive%20technologies%2C%20and%20human-computer%20interaction%20by%20providing%20deeper%0Ainsights%20and%20more%20contextually%20relevant%20responses.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2501.03957v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DVision%2520Language%2520Models%2520as%2520Values%2520Detectors%26entry.906535625%3DGiulio%2520Antonio%2520Abbo%2520and%2520Tony%2520Belpaeme%26entry.1292438233%3D%2520%2520Large%2520Language%2520Models%2520integrating%2520textual%2520and%2520visual%2520inputs%2520have%2520introduced%250Anew%2520possibilities%2520for%2520interpreting%2520complex%2520data.%2520Despite%2520their%2520remarkable%250Aability%2520to%2520generate%2520coherent%2520and%2520contextually%2520relevant%2520text%2520based%2520on%2520visual%250Astimuli%252C%2520the%2520alignment%2520of%2520these%2520models%2520with%2520human%2520perception%2520in%2520identifying%250Arelevant%2520elements%2520in%2520images%2520requires%2520further%2520exploration.%2520This%2520paper%250Ainvestigates%2520the%2520alignment%2520between%2520state-of-the-art%2520LLMs%2520and%2520human%2520annotators%250Ain%2520detecting%2520elements%2520of%2520relevance%2520within%2520home%2520environment%2520scenarios.%2520We%250Acreated%2520a%2520set%2520of%2520twelve%2520images%2520depicting%2520various%2520domestic%2520scenarios%2520and%250Aenlisted%2520fourteen%2520annotators%2520to%2520identify%2520the%2520key%2520element%2520in%2520each%2520image.%2520We%2520then%250Acompared%2520these%2520human%2520responses%2520with%2520outputs%2520from%2520five%2520different%2520LLMs%252C%2520including%250AGPT-4o%2520and%2520four%2520LLaVA%2520variants.%2520Our%2520findings%2520reveal%2520a%2520varied%2520degree%2520of%250Aalignment%252C%2520with%2520LLaVA%252034B%2520showing%2520the%2520highest%2520performance%2520but%2520still%2520scoring%250Alow.%2520However%252C%2520an%2520analysis%2520of%2520the%2520results%2520highlights%2520the%2520models%2527%2520potential%2520to%250Adetect%2520value-laden%2520elements%2520in%2520images%252C%2520suggesting%2520that%2520with%2520improved%2520training%250Aand%2520refined%2520prompts%252C%2520LLMs%2520could%2520enhance%2520applications%2520in%2520social%2520robotics%252C%250Aassistive%2520technologies%252C%2520and%2520human-computer%2520interaction%2520by%2520providing%2520deeper%250Ainsights%2520and%2520more%2520contextually%2520relevant%2520responses.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2501.03957v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Vision%20Language%20Models%20as%20Values%20Detectors&entry.906535625=Giulio%20Antonio%20Abbo%20and%20Tony%20Belpaeme&entry.1292438233=%20%20Large%20Language%20Models%20integrating%20textual%20and%20visual%20inputs%20have%20introduced%0Anew%20possibilities%20for%20interpreting%20complex%20data.%20Despite%20their%20remarkable%0Aability%20to%20generate%20coherent%20and%20contextually%20relevant%20text%20based%20on%20visual%0Astimuli%2C%20the%20alignment%20of%20these%20models%20with%20human%20perception%20in%20identifying%0Arelevant%20elements%20in%20images%20requires%20further%20exploration.%20This%20paper%0Ainvestigates%20the%20alignment%20between%20state-of-the-art%20LLMs%20and%20human%20annotators%0Ain%20detecting%20elements%20of%20relevance%20within%20home%20environment%20scenarios.%20We%0Acreated%20a%20set%20of%20twelve%20images%20depicting%20various%20domestic%20scenarios%20and%0Aenlisted%20fourteen%20annotators%20to%20identify%20the%20key%20element%20in%20each%20image.%20We%20then%0Acompared%20these%20human%20responses%20with%20outputs%20from%20five%20different%20LLMs%2C%20including%0AGPT-4o%20and%20four%20LLaVA%20variants.%20Our%20findings%20reveal%20a%20varied%20degree%20of%0Aalignment%2C%20with%20LLaVA%2034B%20showing%20the%20highest%20performance%20but%20still%20scoring%0Alow.%20However%2C%20an%20analysis%20of%20the%20results%20highlights%20the%20models%27%20potential%20to%0Adetect%20value-laden%20elements%20in%20images%2C%20suggesting%20that%20with%20improved%20training%0Aand%20refined%20prompts%2C%20LLMs%20could%20enhance%20applications%20in%20social%20robotics%2C%0Aassistive%20technologies%2C%20and%20human-computer%20interaction%20by%20providing%20deeper%0Ainsights%20and%20more%20contextually%20relevant%20responses.%0A&entry.1838667208=http%3A//arxiv.org/abs/2501.03957v1&entry.124074799=Read"},
{"title": "Multi-source Domain Adaptation for Panoramic Semantic Segmentation", "author": "Jing Jiang and Sicheng Zhao and Jiankun Zhu and Wenbo Tang and Zhaopan Xu and Jidong Yang and Guoping Liu and Tengfei Xing and Pengfei Xu and Hongxun Yao", "abstract": "  Unsupervised domain adaptation methods for panoramic semantic segmentation\nutilize real pinhole images or low-cost synthetic panoramic images to transfer\nsegmentation models to real panoramic images. However, these methods struggle\nto understand the panoramic structure using only real pinhole images and lack\nreal-world scene perception with only synthetic panoramic images. Therefore, in\nthis paper, we propose a new task, Multi-source Domain Adaptation for Panoramic\nSemantic Segmentation (MSDA4PASS), which leverages both real pinhole and\nsynthetic panoramic images to improve segmentation on unlabeled real panoramic\nimages. There are two key issues in the MSDA4PASS task: (1) distortion gaps\nbetween the pinhole and panoramic domains -- panoramic images exhibit global\nand local distortions absent in pinhole images; (2) texture gaps between the\nsource and target domains -- scenes and styles differ across domains. To\naddress these two issues, we propose a novel framework, Deformation Transform\nAligner for Panoramic Semantic Segmentation (DTA4PASS), which converts all\npinhole images in the source domains into distorted images and aligns the\nsource distorted and panoramic images with the target panoramic images.\nSpecifically, DTA4PASS consists of two main components: Unpaired Semantic\nMorphing (USM) and Distortion Gating Alignment (DGA). First, in USM, the\nDual-view Discriminator (DvD) assists in training the diffeomorphic deformation\nnetwork at the image and pixel level, enabling the effective deformation\ntransformation of pinhole images without paired panoramic views, alleviating\ndistortion gaps. Second, DGA assigns pinhole-like (pin-like) and panoramic-like\n(pan-like) features to each image by gating, and aligns these two features\nthrough uncertainty estimation, reducing texture gaps.\n", "link": "http://arxiv.org/abs/2408.16469v2", "date": "2025-01-07", "relevancy": 2.8714, "topK": [{"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.607}, {"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.5615}, {"title": "DDM-NET: End-to-end learning of keypoint feature Detection, Description and Matching for 3D localization", "link": "https://arxiv.org/abs/2212.04575", "similarity": 0.5544}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Multi-source%20Domain%20Adaptation%20for%20Panoramic%20Semantic%20Segmentation&body=Title%3A%20Multi-source%20Domain%20Adaptation%20for%20Panoramic%20Semantic%20Segmentation%0AAuthor%3A%20Jing%20Jiang%20and%20Sicheng%20Zhao%20and%20Jiankun%20Zhu%20and%20Wenbo%20Tang%20and%20Zhaopan%20Xu%20and%20Jidong%20Yang%20and%20Guoping%20Liu%20and%20Tengfei%20Xing%20and%20Pengfei%20Xu%20and%20Hongxun%20Yao%0AAbstract%3A%20%20%20Unsupervised%20domain%20adaptation%20methods%20for%20panoramic%20semantic%20segmentation%0Autilize%20real%20pinhole%20images%20or%20low-cost%20synthetic%20panoramic%20images%20to%20transfer%0Asegmentation%20models%20to%20real%20panoramic%20images.%20However%2C%20these%20methods%20struggle%0Ato%20understand%20the%20panoramic%20structure%20using%20only%20real%20pinhole%20images%20and%20lack%0Areal-world%20scene%20perception%20with%20only%20synthetic%20panoramic%20images.%20Therefore%2C%20in%0Athis%20paper%2C%20we%20propose%20a%20new%20task%2C%20Multi-source%20Domain%20Adaptation%20for%20Panoramic%0ASemantic%20Segmentation%20%28MSDA4PASS%29%2C%20which%20leverages%20both%20real%20pinhole%20and%0Asynthetic%20panoramic%20images%20to%20improve%20segmentation%20on%20unlabeled%20real%20panoramic%0Aimages.%20There%20are%20two%20key%20issues%20in%20the%20MSDA4PASS%20task%3A%20%281%29%20distortion%20gaps%0Abetween%20the%20pinhole%20and%20panoramic%20domains%20--%20panoramic%20images%20exhibit%20global%0Aand%20local%20distortions%20absent%20in%20pinhole%20images%3B%20%282%29%20texture%20gaps%20between%20the%0Asource%20and%20target%20domains%20--%20scenes%20and%20styles%20differ%20across%20domains.%20To%0Aaddress%20these%20two%20issues%2C%20we%20propose%20a%20novel%20framework%2C%20Deformation%20Transform%0AAligner%20for%20Panoramic%20Semantic%20Segmentation%20%28DTA4PASS%29%2C%20which%20converts%20all%0Apinhole%20images%20in%20the%20source%20domains%20into%20distorted%20images%20and%20aligns%20the%0Asource%20distorted%20and%20panoramic%20images%20with%20the%20target%20panoramic%20images.%0ASpecifically%2C%20DTA4PASS%20consists%20of%20two%20main%20components%3A%20Unpaired%20Semantic%0AMorphing%20%28USM%29%20and%20Distortion%20Gating%20Alignment%20%28DGA%29.%20First%2C%20in%20USM%2C%20the%0ADual-view%20Discriminator%20%28DvD%29%20assists%20in%20training%20the%20diffeomorphic%20deformation%0Anetwork%20at%20the%20image%20and%20pixel%20level%2C%20enabling%20the%20effective%20deformation%0Atransformation%20of%20pinhole%20images%20without%20paired%20panoramic%20views%2C%20alleviating%0Adistortion%20gaps.%20Second%2C%20DGA%20assigns%20pinhole-like%20%28pin-like%29%20and%20panoramic-like%0A%28pan-like%29%20features%20to%20each%20image%20by%20gating%2C%20and%20aligns%20these%20two%20features%0Athrough%20uncertainty%20estimation%2C%20reducing%20texture%20gaps.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2408.16469v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DMulti-source%2520Domain%2520Adaptation%2520for%2520Panoramic%2520Semantic%2520Segmentation%26entry.906535625%3DJing%2520Jiang%2520and%2520Sicheng%2520Zhao%2520and%2520Jiankun%2520Zhu%2520and%2520Wenbo%2520Tang%2520and%2520Zhaopan%2520Xu%2520and%2520Jidong%2520Yang%2520and%2520Guoping%2520Liu%2520and%2520Tengfei%2520Xing%2520and%2520Pengfei%2520Xu%2520and%2520Hongxun%2520Yao%26entry.1292438233%3D%2520%2520Unsupervised%2520domain%2520adaptation%2520methods%2520for%2520panoramic%2520semantic%2520segmentation%250Autilize%2520real%2520pinhole%2520images%2520or%2520low-cost%2520synthetic%2520panoramic%2520images%2520to%2520transfer%250Asegmentation%2520models%2520to%2520real%2520panoramic%2520images.%2520However%252C%2520these%2520methods%2520struggle%250Ato%2520understand%2520the%2520panoramic%2520structure%2520using%2520only%2520real%2520pinhole%2520images%2520and%2520lack%250Areal-world%2520scene%2520perception%2520with%2520only%2520synthetic%2520panoramic%2520images.%2520Therefore%252C%2520in%250Athis%2520paper%252C%2520we%2520propose%2520a%2520new%2520task%252C%2520Multi-source%2520Domain%2520Adaptation%2520for%2520Panoramic%250ASemantic%2520Segmentation%2520%2528MSDA4PASS%2529%252C%2520which%2520leverages%2520both%2520real%2520pinhole%2520and%250Asynthetic%2520panoramic%2520images%2520to%2520improve%2520segmentation%2520on%2520unlabeled%2520real%2520panoramic%250Aimages.%2520There%2520are%2520two%2520key%2520issues%2520in%2520the%2520MSDA4PASS%2520task%253A%2520%25281%2529%2520distortion%2520gaps%250Abetween%2520the%2520pinhole%2520and%2520panoramic%2520domains%2520--%2520panoramic%2520images%2520exhibit%2520global%250Aand%2520local%2520distortions%2520absent%2520in%2520pinhole%2520images%253B%2520%25282%2529%2520texture%2520gaps%2520between%2520the%250Asource%2520and%2520target%2520domains%2520--%2520scenes%2520and%2520styles%2520differ%2520across%2520domains.%2520To%250Aaddress%2520these%2520two%2520issues%252C%2520we%2520propose%2520a%2520novel%2520framework%252C%2520Deformation%2520Transform%250AAligner%2520for%2520Panoramic%2520Semantic%2520Segmentation%2520%2528DTA4PASS%2529%252C%2520which%2520converts%2520all%250Apinhole%2520images%2520in%2520the%2520source%2520domains%2520into%2520distorted%2520images%2520and%2520aligns%2520the%250Asource%2520distorted%2520and%2520panoramic%2520images%2520with%2520the%2520target%2520panoramic%2520images.%250ASpecifically%252C%2520DTA4PASS%2520consists%2520of%2520two%2520main%2520components%253A%2520Unpaired%2520Semantic%250AMorphing%2520%2528USM%2529%2520and%2520Distortion%2520Gating%2520Alignment%2520%2528DGA%2529.%2520First%252C%2520in%2520USM%252C%2520the%250ADual-view%2520Discriminator%2520%2528DvD%2529%2520assists%2520in%2520training%2520the%2520diffeomorphic%2520deformation%250Anetwork%2520at%2520the%2520image%2520and%2520pixel%2520level%252C%2520enabling%2520the%2520effective%2520deformation%250Atransformation%2520of%2520pinhole%2520images%2520without%2520paired%2520panoramic%2520views%252C%2520alleviating%250Adistortion%2520gaps.%2520Second%252C%2520DGA%2520assigns%2520pinhole-like%2520%2528pin-like%2529%2520and%2520panoramic-like%250A%2528pan-like%2529%2520features%2520to%2520each%2520image%2520by%2520gating%252C%2520and%2520aligns%2520these%2520two%2520features%250Athrough%2520uncertainty%2520estimation%252C%2520reducing%2520texture%2520gaps.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2408.16469v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Multi-source%20Domain%20Adaptation%20for%20Panoramic%20Semantic%20Segmentation&entry.906535625=Jing%20Jiang%20and%20Sicheng%20Zhao%20and%20Jiankun%20Zhu%20and%20Wenbo%20Tang%20and%20Zhaopan%20Xu%20and%20Jidong%20Yang%20and%20Guoping%20Liu%20and%20Tengfei%20Xing%20and%20Pengfei%20Xu%20and%20Hongxun%20Yao&entry.1292438233=%20%20Unsupervised%20domain%20adaptation%20methods%20for%20panoramic%20semantic%20segmentation%0Autilize%20real%20pinhole%20images%20or%20low-cost%20synthetic%20panoramic%20images%20to%20transfer%0Asegmentation%20models%20to%20real%20panoramic%20images.%20However%2C%20these%20methods%20struggle%0Ato%20understand%20the%20panoramic%20structure%20using%20only%20real%20pinhole%20images%20and%20lack%0Areal-world%20scene%20perception%20with%20only%20synthetic%20panoramic%20images.%20Therefore%2C%20in%0Athis%20paper%2C%20we%20propose%20a%20new%20task%2C%20Multi-source%20Domain%20Adaptation%20for%20Panoramic%0ASemantic%20Segmentation%20%28MSDA4PASS%29%2C%20which%20leverages%20both%20real%20pinhole%20and%0Asynthetic%20panoramic%20images%20to%20improve%20segmentation%20on%20unlabeled%20real%20panoramic%0Aimages.%20There%20are%20two%20key%20issues%20in%20the%20MSDA4PASS%20task%3A%20%281%29%20distortion%20gaps%0Abetween%20the%20pinhole%20and%20panoramic%20domains%20--%20panoramic%20images%20exhibit%20global%0Aand%20local%20distortions%20absent%20in%20pinhole%20images%3B%20%282%29%20texture%20gaps%20between%20the%0Asource%20and%20target%20domains%20--%20scenes%20and%20styles%20differ%20across%20domains.%20To%0Aaddress%20these%20two%20issues%2C%20we%20propose%20a%20novel%20framework%2C%20Deformation%20Transform%0AAligner%20for%20Panoramic%20Semantic%20Segmentation%20%28DTA4PASS%29%2C%20which%20converts%20all%0Apinhole%20images%20in%20the%20source%20domains%20into%20distorted%20images%20and%20aligns%20the%0Asource%20distorted%20and%20panoramic%20images%20with%20the%20target%20panoramic%20images.%0ASpecifically%2C%20DTA4PASS%20consists%20of%20two%20main%20components%3A%20Unpaired%20Semantic%0AMorphing%20%28USM%29%20and%20Distortion%20Gating%20Alignment%20%28DGA%29.%20First%2C%20in%20USM%2C%20the%0ADual-view%20Discriminator%20%28DvD%29%20assists%20in%20training%20the%20diffeomorphic%20deformation%0Anetwork%20at%20the%20image%20and%20pixel%20level%2C%20enabling%20the%20effective%20deformation%0Atransformation%20of%20pinhole%20images%20without%20paired%20panoramic%20views%2C%20alleviating%0Adistortion%20gaps.%20Second%2C%20DGA%20assigns%20pinhole-like%20%28pin-like%29%20and%20panoramic-like%0A%28pan-like%29%20features%20to%20each%20image%20by%20gating%2C%20and%20aligns%20these%20two%20features%0Athrough%20uncertainty%20estimation%2C%20reducing%20texture%20gaps.%0A&entry.1838667208=http%3A//arxiv.org/abs/2408.16469v2&entry.124074799=Read"},
{"title": "PSA-VLM: Enhancing Vision-Language Model Safety through Progressive\n  Concept-Bottleneck-Driven Alignment", "author": "Zhendong Liu and Yuanbi Nie and Yingshui Tan and Jiaheng Liu and Xiangyu Yue and Qiushi Cui and Chongjun Wang and Xiaoyong Zhu and Bo Zheng", "abstract": "  Benefiting from the powerful capabilities of Large Language Models (LLMs),\npre-trained visual encoder models connected to LLMs form Vision Language Models\n(VLMs). However, recent research shows that the visual modality in VLMs is\nhighly vulnerable, allowing attackers to bypass safety alignment in LLMs\nthrough visually transmitted content, launching harmful attacks. To address\nthis challenge, we propose a progressive concept-based alignment strategy,\nPSA-VLM, which incorporates safety modules as concept bottlenecks to enhance\nvisual modality safety alignment. By aligning model predictions with specific\nsafety concepts, we improve defenses against risky images, enhancing\nexplainability and controllability while minimally impacting general\nperformance. Our method is obtained through two-stage training. The low\ncomputational cost of the first stage brings very effective performance\nimprovement, and the fine-tuning of the language model in the second stage\nfurther improves the safety performance. Our method achieves state-of-the-art\nresults on popular VLM safety benchmark.\n", "link": "http://arxiv.org/abs/2411.11543v3", "date": "2025-01-07", "relevancy": 2.8421, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5804}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5804}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5445}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20PSA-VLM%3A%20Enhancing%20Vision-Language%20Model%20Safety%20through%20Progressive%0A%20%20Concept-Bottleneck-Driven%20Alignment&body=Title%3A%20PSA-VLM%3A%20Enhancing%20Vision-Language%20Model%20Safety%20through%20Progressive%0A%20%20Concept-Bottleneck-Driven%20Alignment%0AAuthor%3A%20Zhendong%20Liu%20and%20Yuanbi%20Nie%20and%20Yingshui%20Tan%20and%20Jiaheng%20Liu%20and%20Xiangyu%20Yue%20and%20Qiushi%20Cui%20and%20Chongjun%20Wang%20and%20Xiaoyong%20Zhu%20and%20Bo%20Zheng%0AAbstract%3A%20%20%20Benefiting%20from%20the%20powerful%20capabilities%20of%20Large%20Language%20Models%20%28LLMs%29%2C%0Apre-trained%20visual%20encoder%20models%20connected%20to%20LLMs%20form%20Vision%20Language%20Models%0A%28VLMs%29.%20However%2C%20recent%20research%20shows%20that%20the%20visual%20modality%20in%20VLMs%20is%0Ahighly%20vulnerable%2C%20allowing%20attackers%20to%20bypass%20safety%20alignment%20in%20LLMs%0Athrough%20visually%20transmitted%20content%2C%20launching%20harmful%20attacks.%20To%20address%0Athis%20challenge%2C%20we%20propose%20a%20progressive%20concept-based%20alignment%20strategy%2C%0APSA-VLM%2C%20which%20incorporates%20safety%20modules%20as%20concept%20bottlenecks%20to%20enhance%0Avisual%20modality%20safety%20alignment.%20By%20aligning%20model%20predictions%20with%20specific%0Asafety%20concepts%2C%20we%20improve%20defenses%20against%20risky%20images%2C%20enhancing%0Aexplainability%20and%20controllability%20while%20minimally%20impacting%20general%0Aperformance.%20Our%20method%20is%20obtained%20through%20two-stage%20training.%20The%20low%0Acomputational%20cost%20of%20the%20first%20stage%20brings%20very%20effective%20performance%0Aimprovement%2C%20and%20the%20fine-tuning%20of%20the%20language%20model%20in%20the%20second%20stage%0Afurther%20improves%20the%20safety%20performance.%20Our%20method%20achieves%20state-of-the-art%0Aresults%20on%20popular%20VLM%20safety%20benchmark.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2411.11543v3%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DPSA-VLM%253A%2520Enhancing%2520Vision-Language%2520Model%2520Safety%2520through%2520Progressive%250A%2520%2520Concept-Bottleneck-Driven%2520Alignment%26entry.906535625%3DZhendong%2520Liu%2520and%2520Yuanbi%2520Nie%2520and%2520Yingshui%2520Tan%2520and%2520Jiaheng%2520Liu%2520and%2520Xiangyu%2520Yue%2520and%2520Qiushi%2520Cui%2520and%2520Chongjun%2520Wang%2520and%2520Xiaoyong%2520Zhu%2520and%2520Bo%2520Zheng%26entry.1292438233%3D%2520%2520Benefiting%2520from%2520the%2520powerful%2520capabilities%2520of%2520Large%2520Language%2520Models%2520%2528LLMs%2529%252C%250Apre-trained%2520visual%2520encoder%2520models%2520connected%2520to%2520LLMs%2520form%2520Vision%2520Language%2520Models%250A%2528VLMs%2529.%2520However%252C%2520recent%2520research%2520shows%2520that%2520the%2520visual%2520modality%2520in%2520VLMs%2520is%250Ahighly%2520vulnerable%252C%2520allowing%2520attackers%2520to%2520bypass%2520safety%2520alignment%2520in%2520LLMs%250Athrough%2520visually%2520transmitted%2520content%252C%2520launching%2520harmful%2520attacks.%2520To%2520address%250Athis%2520challenge%252C%2520we%2520propose%2520a%2520progressive%2520concept-based%2520alignment%2520strategy%252C%250APSA-VLM%252C%2520which%2520incorporates%2520safety%2520modules%2520as%2520concept%2520bottlenecks%2520to%2520enhance%250Avisual%2520modality%2520safety%2520alignment.%2520By%2520aligning%2520model%2520predictions%2520with%2520specific%250Asafety%2520concepts%252C%2520we%2520improve%2520defenses%2520against%2520risky%2520images%252C%2520enhancing%250Aexplainability%2520and%2520controllability%2520while%2520minimally%2520impacting%2520general%250Aperformance.%2520Our%2520method%2520is%2520obtained%2520through%2520two-stage%2520training.%2520The%2520low%250Acomputational%2520cost%2520of%2520the%2520first%2520stage%2520brings%2520very%2520effective%2520performance%250Aimprovement%252C%2520and%2520the%2520fine-tuning%2520of%2520the%2520language%2520model%2520in%2520the%2520second%2520stage%250Afurther%2520improves%2520the%2520safety%2520performance.%2520Our%2520method%2520achieves%2520state-of-the-art%250Aresults%2520on%2520popular%2520VLM%2520safety%2520benchmark.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2411.11543v3%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=PSA-VLM%3A%20Enhancing%20Vision-Language%20Model%20Safety%20through%20Progressive%0A%20%20Concept-Bottleneck-Driven%20Alignment&entry.906535625=Zhendong%20Liu%20and%20Yuanbi%20Nie%20and%20Yingshui%20Tan%20and%20Jiaheng%20Liu%20and%20Xiangyu%20Yue%20and%20Qiushi%20Cui%20and%20Chongjun%20Wang%20and%20Xiaoyong%20Zhu%20and%20Bo%20Zheng&entry.1292438233=%20%20Benefiting%20from%20the%20powerful%20capabilities%20of%20Large%20Language%20Models%20%28LLMs%29%2C%0Apre-trained%20visual%20encoder%20models%20connected%20to%20LLMs%20form%20Vision%20Language%20Models%0A%28VLMs%29.%20However%2C%20recent%20research%20shows%20that%20the%20visual%20modality%20in%20VLMs%20is%0Ahighly%20vulnerable%2C%20allowing%20attackers%20to%20bypass%20safety%20alignment%20in%20LLMs%0Athrough%20visually%20transmitted%20content%2C%20launching%20harmful%20attacks.%20To%20address%0Athis%20challenge%2C%20we%20propose%20a%20progressive%20concept-based%20alignment%20strategy%2C%0APSA-VLM%2C%20which%20incorporates%20safety%20modules%20as%20concept%20bottlenecks%20to%20enhance%0Avisual%20modality%20safety%20alignment.%20By%20aligning%20model%20predictions%20with%20specific%0Asafety%20concepts%2C%20we%20improve%20defenses%20against%20risky%20images%2C%20enhancing%0Aexplainability%20and%20controllability%20while%20minimally%20impacting%20general%0Aperformance.%20Our%20method%20is%20obtained%20through%20two-stage%20training.%20The%20low%0Acomputational%20cost%20of%20the%20first%20stage%20brings%20very%20effective%20performance%0Aimprovement%2C%20and%20the%20fine-tuning%20of%20the%20language%20model%20in%20the%20second%20stage%0Afurther%20improves%20the%20safety%20performance.%20Our%20method%20achieves%20state-of-the-art%0Aresults%20on%20popular%20VLM%20safety%20benchmark.%0A&entry.1838667208=http%3A//arxiv.org/abs/2411.11543v3&entry.124074799=Read"},
{"title": "Magic Mirror: ID-Preserved Video Generation in Video Diffusion\n  Transformers", "author": "Yuechen Zhang and Yaoyang Liu and Bin Xia and Bohao Peng and Zexin Yan and Eric Lo and Jiaya Jia", "abstract": "  We present Magic Mirror, a framework for generating identity-preserved videos\nwith cinematic-level quality and dynamic motion. While recent advances in video\ndiffusion models have shown impressive capabilities in text-to-video\ngeneration, maintaining consistent identity while producing natural motion\nremains challenging. Previous methods either require person-specific\nfine-tuning or struggle to balance identity preservation with motion diversity.\nBuilt upon Video Diffusion Transformers, our method introduces three key\ncomponents: (1) a dual-branch facial feature extractor that captures both\nidentity and structural features, (2) a lightweight cross-modal adapter with\nConditioned Adaptive Normalization for efficient identity integration, and (3)\na two-stage training strategy combining synthetic identity pairs with video\ndata. Extensive experiments demonstrate that Magic Mirror effectively balances\nidentity consistency with natural motion, outperforming existing methods across\nmultiple metrics while requiring minimal parameters added. The code and model\nwill be made publicly available at:\nhttps://github.com/dvlab-research/MagicMirror/\n", "link": "http://arxiv.org/abs/2501.03931v1", "date": "2025-01-07", "relevancy": 2.8297, "topK": [{"title": "Magic-Me: Identity-Specific Video Customized Diffusion", "link": "http://arxiv.org/abs/2402.09368v1", "similarity": 0.8092}, {"title": "Customize-A-Video: One-Shot Motion Customization of Text-to-Video\n  Diffusion Models", "link": "http://arxiv.org/abs/2402.14780v1", "similarity": 0.6658}, {"title": "Tuning-Free Noise Rectification for High Fidelity Image-to-Video\n  Generation", "link": "http://arxiv.org/abs/2403.02827v1", "similarity": 0.6223}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Magic%20Mirror%3A%20ID-Preserved%20Video%20Generation%20in%20Video%20Diffusion%0A%20%20Transformers&body=Title%3A%20Magic%20Mirror%3A%20ID-Preserved%20Video%20Generation%20in%20Video%20Diffusion%0A%20%20Transformers%0AAuthor%3A%20Yuechen%20Zhang%20and%20Yaoyang%20Liu%20and%20Bin%20Xia%20and%20Bohao%20Peng%20and%20Zexin%20Yan%20and%20Eric%20Lo%20and%20Jiaya%20Jia%0AAbstract%3A%20%20%20We%20present%20Magic%20Mirror%2C%20a%20framework%20for%20generating%20identity-preserved%20videos%0Awith%20cinematic-level%20quality%20and%20dynamic%20motion.%20While%20recent%20advances%20in%20video%0Adiffusion%20models%20have%20shown%20impressive%20capabilities%20in%20text-to-video%0Ageneration%2C%20maintaining%20consistent%20identity%20while%20producing%20natural%20motion%0Aremains%20challenging.%20Previous%20methods%20either%20require%20person-specific%0Afine-tuning%20or%20struggle%20to%20balance%20identity%20preservation%20with%20motion%20diversity.%0ABuilt%20upon%20Video%20Diffusion%20Transformers%2C%20our%20method%20introduces%20three%20key%0Acomponents%3A%20%281%29%20a%20dual-branch%20facial%20feature%20extractor%20that%20captures%20both%0Aidentity%20and%20structural%20features%2C%20%282%29%20a%20lightweight%20cross-modal%20adapter%20with%0AConditioned%20Adaptive%20Normalization%20for%20efficient%20identity%20integration%2C%20and%20%283%29%0Aa%20two-stage%20training%20strategy%20combining%20synthetic%20identity%20pairs%20with%20video%0Adata.%20Extensive%20experiments%20demonstrate%20that%20Magic%20Mirror%20effectively%20balances%0Aidentity%20consistency%20with%20natural%20motion%2C%20outperforming%20existing%20methods%20across%0Amultiple%20metrics%20while%20requiring%20minimal%20parameters%20added.%20The%20code%20and%20model%0Awill%20be%20made%20publicly%20available%20at%3A%0Ahttps%3A//github.com/dvlab-research/MagicMirror/%0A%0ALink%3A%20http%3A//arxiv.org/abs/2501.03931v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DMagic%2520Mirror%253A%2520ID-Preserved%2520Video%2520Generation%2520in%2520Video%2520Diffusion%250A%2520%2520Transformers%26entry.906535625%3DYuechen%2520Zhang%2520and%2520Yaoyang%2520Liu%2520and%2520Bin%2520Xia%2520and%2520Bohao%2520Peng%2520and%2520Zexin%2520Yan%2520and%2520Eric%2520Lo%2520and%2520Jiaya%2520Jia%26entry.1292438233%3D%2520%2520We%2520present%2520Magic%2520Mirror%252C%2520a%2520framework%2520for%2520generating%2520identity-preserved%2520videos%250Awith%2520cinematic-level%2520quality%2520and%2520dynamic%2520motion.%2520While%2520recent%2520advances%2520in%2520video%250Adiffusion%2520models%2520have%2520shown%2520impressive%2520capabilities%2520in%2520text-to-video%250Ageneration%252C%2520maintaining%2520consistent%2520identity%2520while%2520producing%2520natural%2520motion%250Aremains%2520challenging.%2520Previous%2520methods%2520either%2520require%2520person-specific%250Afine-tuning%2520or%2520struggle%2520to%2520balance%2520identity%2520preservation%2520with%2520motion%2520diversity.%250ABuilt%2520upon%2520Video%2520Diffusion%2520Transformers%252C%2520our%2520method%2520introduces%2520three%2520key%250Acomponents%253A%2520%25281%2529%2520a%2520dual-branch%2520facial%2520feature%2520extractor%2520that%2520captures%2520both%250Aidentity%2520and%2520structural%2520features%252C%2520%25282%2529%2520a%2520lightweight%2520cross-modal%2520adapter%2520with%250AConditioned%2520Adaptive%2520Normalization%2520for%2520efficient%2520identity%2520integration%252C%2520and%2520%25283%2529%250Aa%2520two-stage%2520training%2520strategy%2520combining%2520synthetic%2520identity%2520pairs%2520with%2520video%250Adata.%2520Extensive%2520experiments%2520demonstrate%2520that%2520Magic%2520Mirror%2520effectively%2520balances%250Aidentity%2520consistency%2520with%2520natural%2520motion%252C%2520outperforming%2520existing%2520methods%2520across%250Amultiple%2520metrics%2520while%2520requiring%2520minimal%2520parameters%2520added.%2520The%2520code%2520and%2520model%250Awill%2520be%2520made%2520publicly%2520available%2520at%253A%250Ahttps%253A//github.com/dvlab-research/MagicMirror/%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2501.03931v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Magic%20Mirror%3A%20ID-Preserved%20Video%20Generation%20in%20Video%20Diffusion%0A%20%20Transformers&entry.906535625=Yuechen%20Zhang%20and%20Yaoyang%20Liu%20and%20Bin%20Xia%20and%20Bohao%20Peng%20and%20Zexin%20Yan%20and%20Eric%20Lo%20and%20Jiaya%20Jia&entry.1292438233=%20%20We%20present%20Magic%20Mirror%2C%20a%20framework%20for%20generating%20identity-preserved%20videos%0Awith%20cinematic-level%20quality%20and%20dynamic%20motion.%20While%20recent%20advances%20in%20video%0Adiffusion%20models%20have%20shown%20impressive%20capabilities%20in%20text-to-video%0Ageneration%2C%20maintaining%20consistent%20identity%20while%20producing%20natural%20motion%0Aremains%20challenging.%20Previous%20methods%20either%20require%20person-specific%0Afine-tuning%20or%20struggle%20to%20balance%20identity%20preservation%20with%20motion%20diversity.%0ABuilt%20upon%20Video%20Diffusion%20Transformers%2C%20our%20method%20introduces%20three%20key%0Acomponents%3A%20%281%29%20a%20dual-branch%20facial%20feature%20extractor%20that%20captures%20both%0Aidentity%20and%20structural%20features%2C%20%282%29%20a%20lightweight%20cross-modal%20adapter%20with%0AConditioned%20Adaptive%20Normalization%20for%20efficient%20identity%20integration%2C%20and%20%283%29%0Aa%20two-stage%20training%20strategy%20combining%20synthetic%20identity%20pairs%20with%20video%0Adata.%20Extensive%20experiments%20demonstrate%20that%20Magic%20Mirror%20effectively%20balances%0Aidentity%20consistency%20with%20natural%20motion%2C%20outperforming%20existing%20methods%20across%0Amultiple%20metrics%20while%20requiring%20minimal%20parameters%20added.%20The%20code%20and%20model%0Awill%20be%20made%20publicly%20available%20at%3A%0Ahttps%3A//github.com/dvlab-research/MagicMirror/%0A&entry.1838667208=http%3A//arxiv.org/abs/2501.03931v1&entry.124074799=Read"},
{"title": "KAnoCLIP: Zero-Shot Anomaly Detection through Knowledge-Driven Prompt\n  Learning and Enhanced Cross-Modal Integration", "author": "Chengyuan Li and Suyang Zhou and Jieping Kong and Lei Qi and Hui Xue", "abstract": "  Zero-shot anomaly detection (ZSAD) identifies anomalies without needing\ntraining samples from the target dataset, essential for scenarios with privacy\nconcerns or limited data. Vision-language models like CLIP show potential in\nZSAD but have limitations: relying on manually crafted fixed textual\ndescriptions or anomaly prompts is time-consuming and prone to semantic\nambiguity, and CLIP struggles with pixel-level anomaly segmentation, focusing\nmore on global semantics than local details. To address these limitations, We\nintroduce KAnoCLIP, a novel ZSAD framework that leverages vision-language\nmodels. KAnoCLIP combines general knowledge from a Large Language Model\n(GPT-3.5) and fine-grained, image-specific knowledge from a Visual Question\nAnswering system (Llama3) via Knowledge-Driven Prompt Learning (KnPL). KnPL\nuses a knowledge-driven (KD) loss function to create learnable anomaly prompts,\nremoving the need for fixed text prompts and enhancing generalization. KAnoCLIP\nincludes the CLIP visual encoder with V-V attention (CLIP-VV), Bi-Directional\nCross-Attention for Multi-Level Cross-Modal Interaction (Bi-CMCI), and\nConv-Adapter. These components preserve local visual semantics, improve local\ncross-modal fusion, and align global visual features with textual information,\nenhancing pixel-level anomaly detection. KAnoCLIP achieves state-of-the-art\nperformance in ZSAD across 12 industrial and medical datasets, demonstrating\nsuperior generalization compared to existing methods.\n", "link": "http://arxiv.org/abs/2501.03786v1", "date": "2025-01-07", "relevancy": 2.8123, "topK": [{"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5876}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5499}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5499}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20KAnoCLIP%3A%20Zero-Shot%20Anomaly%20Detection%20through%20Knowledge-Driven%20Prompt%0A%20%20Learning%20and%20Enhanced%20Cross-Modal%20Integration&body=Title%3A%20KAnoCLIP%3A%20Zero-Shot%20Anomaly%20Detection%20through%20Knowledge-Driven%20Prompt%0A%20%20Learning%20and%20Enhanced%20Cross-Modal%20Integration%0AAuthor%3A%20Chengyuan%20Li%20and%20Suyang%20Zhou%20and%20Jieping%20Kong%20and%20Lei%20Qi%20and%20Hui%20Xue%0AAbstract%3A%20%20%20Zero-shot%20anomaly%20detection%20%28ZSAD%29%20identifies%20anomalies%20without%20needing%0Atraining%20samples%20from%20the%20target%20dataset%2C%20essential%20for%20scenarios%20with%20privacy%0Aconcerns%20or%20limited%20data.%20Vision-language%20models%20like%20CLIP%20show%20potential%20in%0AZSAD%20but%20have%20limitations%3A%20relying%20on%20manually%20crafted%20fixed%20textual%0Adescriptions%20or%20anomaly%20prompts%20is%20time-consuming%20and%20prone%20to%20semantic%0Aambiguity%2C%20and%20CLIP%20struggles%20with%20pixel-level%20anomaly%20segmentation%2C%20focusing%0Amore%20on%20global%20semantics%20than%20local%20details.%20To%20address%20these%20limitations%2C%20We%0Aintroduce%20KAnoCLIP%2C%20a%20novel%20ZSAD%20framework%20that%20leverages%20vision-language%0Amodels.%20KAnoCLIP%20combines%20general%20knowledge%20from%20a%20Large%20Language%20Model%0A%28GPT-3.5%29%20and%20fine-grained%2C%20image-specific%20knowledge%20from%20a%20Visual%20Question%0AAnswering%20system%20%28Llama3%29%20via%20Knowledge-Driven%20Prompt%20Learning%20%28KnPL%29.%20KnPL%0Auses%20a%20knowledge-driven%20%28KD%29%20loss%20function%20to%20create%20learnable%20anomaly%20prompts%2C%0Aremoving%20the%20need%20for%20fixed%20text%20prompts%20and%20enhancing%20generalization.%20KAnoCLIP%0Aincludes%20the%20CLIP%20visual%20encoder%20with%20V-V%20attention%20%28CLIP-VV%29%2C%20Bi-Directional%0ACross-Attention%20for%20Multi-Level%20Cross-Modal%20Interaction%20%28Bi-CMCI%29%2C%20and%0AConv-Adapter.%20These%20components%20preserve%20local%20visual%20semantics%2C%20improve%20local%0Across-modal%20fusion%2C%20and%20align%20global%20visual%20features%20with%20textual%20information%2C%0Aenhancing%20pixel-level%20anomaly%20detection.%20KAnoCLIP%20achieves%20state-of-the-art%0Aperformance%20in%20ZSAD%20across%2012%20industrial%20and%20medical%20datasets%2C%20demonstrating%0Asuperior%20generalization%20compared%20to%20existing%20methods.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2501.03786v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DKAnoCLIP%253A%2520Zero-Shot%2520Anomaly%2520Detection%2520through%2520Knowledge-Driven%2520Prompt%250A%2520%2520Learning%2520and%2520Enhanced%2520Cross-Modal%2520Integration%26entry.906535625%3DChengyuan%2520Li%2520and%2520Suyang%2520Zhou%2520and%2520Jieping%2520Kong%2520and%2520Lei%2520Qi%2520and%2520Hui%2520Xue%26entry.1292438233%3D%2520%2520Zero-shot%2520anomaly%2520detection%2520%2528ZSAD%2529%2520identifies%2520anomalies%2520without%2520needing%250Atraining%2520samples%2520from%2520the%2520target%2520dataset%252C%2520essential%2520for%2520scenarios%2520with%2520privacy%250Aconcerns%2520or%2520limited%2520data.%2520Vision-language%2520models%2520like%2520CLIP%2520show%2520potential%2520in%250AZSAD%2520but%2520have%2520limitations%253A%2520relying%2520on%2520manually%2520crafted%2520fixed%2520textual%250Adescriptions%2520or%2520anomaly%2520prompts%2520is%2520time-consuming%2520and%2520prone%2520to%2520semantic%250Aambiguity%252C%2520and%2520CLIP%2520struggles%2520with%2520pixel-level%2520anomaly%2520segmentation%252C%2520focusing%250Amore%2520on%2520global%2520semantics%2520than%2520local%2520details.%2520To%2520address%2520these%2520limitations%252C%2520We%250Aintroduce%2520KAnoCLIP%252C%2520a%2520novel%2520ZSAD%2520framework%2520that%2520leverages%2520vision-language%250Amodels.%2520KAnoCLIP%2520combines%2520general%2520knowledge%2520from%2520a%2520Large%2520Language%2520Model%250A%2528GPT-3.5%2529%2520and%2520fine-grained%252C%2520image-specific%2520knowledge%2520from%2520a%2520Visual%2520Question%250AAnswering%2520system%2520%2528Llama3%2529%2520via%2520Knowledge-Driven%2520Prompt%2520Learning%2520%2528KnPL%2529.%2520KnPL%250Auses%2520a%2520knowledge-driven%2520%2528KD%2529%2520loss%2520function%2520to%2520create%2520learnable%2520anomaly%2520prompts%252C%250Aremoving%2520the%2520need%2520for%2520fixed%2520text%2520prompts%2520and%2520enhancing%2520generalization.%2520KAnoCLIP%250Aincludes%2520the%2520CLIP%2520visual%2520encoder%2520with%2520V-V%2520attention%2520%2528CLIP-VV%2529%252C%2520Bi-Directional%250ACross-Attention%2520for%2520Multi-Level%2520Cross-Modal%2520Interaction%2520%2528Bi-CMCI%2529%252C%2520and%250AConv-Adapter.%2520These%2520components%2520preserve%2520local%2520visual%2520semantics%252C%2520improve%2520local%250Across-modal%2520fusion%252C%2520and%2520align%2520global%2520visual%2520features%2520with%2520textual%2520information%252C%250Aenhancing%2520pixel-level%2520anomaly%2520detection.%2520KAnoCLIP%2520achieves%2520state-of-the-art%250Aperformance%2520in%2520ZSAD%2520across%252012%2520industrial%2520and%2520medical%2520datasets%252C%2520demonstrating%250Asuperior%2520generalization%2520compared%2520to%2520existing%2520methods.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2501.03786v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=KAnoCLIP%3A%20Zero-Shot%20Anomaly%20Detection%20through%20Knowledge-Driven%20Prompt%0A%20%20Learning%20and%20Enhanced%20Cross-Modal%20Integration&entry.906535625=Chengyuan%20Li%20and%20Suyang%20Zhou%20and%20Jieping%20Kong%20and%20Lei%20Qi%20and%20Hui%20Xue&entry.1292438233=%20%20Zero-shot%20anomaly%20detection%20%28ZSAD%29%20identifies%20anomalies%20without%20needing%0Atraining%20samples%20from%20the%20target%20dataset%2C%20essential%20for%20scenarios%20with%20privacy%0Aconcerns%20or%20limited%20data.%20Vision-language%20models%20like%20CLIP%20show%20potential%20in%0AZSAD%20but%20have%20limitations%3A%20relying%20on%20manually%20crafted%20fixed%20textual%0Adescriptions%20or%20anomaly%20prompts%20is%20time-consuming%20and%20prone%20to%20semantic%0Aambiguity%2C%20and%20CLIP%20struggles%20with%20pixel-level%20anomaly%20segmentation%2C%20focusing%0Amore%20on%20global%20semantics%20than%20local%20details.%20To%20address%20these%20limitations%2C%20We%0Aintroduce%20KAnoCLIP%2C%20a%20novel%20ZSAD%20framework%20that%20leverages%20vision-language%0Amodels.%20KAnoCLIP%20combines%20general%20knowledge%20from%20a%20Large%20Language%20Model%0A%28GPT-3.5%29%20and%20fine-grained%2C%20image-specific%20knowledge%20from%20a%20Visual%20Question%0AAnswering%20system%20%28Llama3%29%20via%20Knowledge-Driven%20Prompt%20Learning%20%28KnPL%29.%20KnPL%0Auses%20a%20knowledge-driven%20%28KD%29%20loss%20function%20to%20create%20learnable%20anomaly%20prompts%2C%0Aremoving%20the%20need%20for%20fixed%20text%20prompts%20and%20enhancing%20generalization.%20KAnoCLIP%0Aincludes%20the%20CLIP%20visual%20encoder%20with%20V-V%20attention%20%28CLIP-VV%29%2C%20Bi-Directional%0ACross-Attention%20for%20Multi-Level%20Cross-Modal%20Interaction%20%28Bi-CMCI%29%2C%20and%0AConv-Adapter.%20These%20components%20preserve%20local%20visual%20semantics%2C%20improve%20local%0Across-modal%20fusion%2C%20and%20align%20global%20visual%20features%20with%20textual%20information%2C%0Aenhancing%20pixel-level%20anomaly%20detection.%20KAnoCLIP%20achieves%20state-of-the-art%0Aperformance%20in%20ZSAD%20across%2012%20industrial%20and%20medical%20datasets%2C%20demonstrating%0Asuperior%20generalization%20compared%20to%20existing%20methods.%0A&entry.1838667208=http%3A//arxiv.org/abs/2501.03786v1&entry.124074799=Read"},
{"title": "Visual question answering: from early developments to recent advances --\n  a survey", "author": "Ngoc Dung Huynh and Mohamed Reda Bouadjenek and Sunil Aryal and Imran Razzak and Hakim Hacid", "abstract": "  Visual Question Answering (VQA) is an evolving research field aimed at\nenabling machines to answer questions about visual content by integrating image\nand language processing techniques such as feature extraction, object\ndetection, text embedding, natural language understanding, and language\ngeneration. With the growth of multimodal data research, VQA has gained\nsignificant attention due to its broad applications, including interactive\neducational tools, medical image diagnosis, customer service, entertainment,\nand social media captioning. Additionally, VQA plays a vital role in assisting\nvisually impaired individuals by generating descriptive content from images.\nThis survey introduces a taxonomy of VQA architectures, categorizing them based\non design choices and key components to facilitate comparative analysis and\nevaluation. We review major VQA approaches, focusing on deep learning-based\nmethods, and explore the emerging field of Large Visual Language Models (LVLMs)\nthat have demonstrated success in multimodal tasks like VQA. The paper further\nexamines available datasets and evaluation metrics essential for measuring VQA\nsystem performance, followed by an exploration of real-world VQA applications.\nFinally, we highlight ongoing challenges and future directions in VQA research,\npresenting open questions and potential areas for further development. This\nsurvey serves as a comprehensive resource for researchers and practitioners\ninterested in the latest advancements and future\n", "link": "http://arxiv.org/abs/2501.03939v1", "date": "2025-01-07", "relevancy": 2.7873, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5877}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5877}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.497}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Visual%20question%20answering%3A%20from%20early%20developments%20to%20recent%20advances%20--%0A%20%20a%20survey&body=Title%3A%20Visual%20question%20answering%3A%20from%20early%20developments%20to%20recent%20advances%20--%0A%20%20a%20survey%0AAuthor%3A%20Ngoc%20Dung%20Huynh%20and%20Mohamed%20Reda%20Bouadjenek%20and%20Sunil%20Aryal%20and%20Imran%20Razzak%20and%20Hakim%20Hacid%0AAbstract%3A%20%20%20Visual%20Question%20Answering%20%28VQA%29%20is%20an%20evolving%20research%20field%20aimed%20at%0Aenabling%20machines%20to%20answer%20questions%20about%20visual%20content%20by%20integrating%20image%0Aand%20language%20processing%20techniques%20such%20as%20feature%20extraction%2C%20object%0Adetection%2C%20text%20embedding%2C%20natural%20language%20understanding%2C%20and%20language%0Ageneration.%20With%20the%20growth%20of%20multimodal%20data%20research%2C%20VQA%20has%20gained%0Asignificant%20attention%20due%20to%20its%20broad%20applications%2C%20including%20interactive%0Aeducational%20tools%2C%20medical%20image%20diagnosis%2C%20customer%20service%2C%20entertainment%2C%0Aand%20social%20media%20captioning.%20Additionally%2C%20VQA%20plays%20a%20vital%20role%20in%20assisting%0Avisually%20impaired%20individuals%20by%20generating%20descriptive%20content%20from%20images.%0AThis%20survey%20introduces%20a%20taxonomy%20of%20VQA%20architectures%2C%20categorizing%20them%20based%0Aon%20design%20choices%20and%20key%20components%20to%20facilitate%20comparative%20analysis%20and%0Aevaluation.%20We%20review%20major%20VQA%20approaches%2C%20focusing%20on%20deep%20learning-based%0Amethods%2C%20and%20explore%20the%20emerging%20field%20of%20Large%20Visual%20Language%20Models%20%28LVLMs%29%0Athat%20have%20demonstrated%20success%20in%20multimodal%20tasks%20like%20VQA.%20The%20paper%20further%0Aexamines%20available%20datasets%20and%20evaluation%20metrics%20essential%20for%20measuring%20VQA%0Asystem%20performance%2C%20followed%20by%20an%20exploration%20of%20real-world%20VQA%20applications.%0AFinally%2C%20we%20highlight%20ongoing%20challenges%20and%20future%20directions%20in%20VQA%20research%2C%0Apresenting%20open%20questions%20and%20potential%20areas%20for%20further%20development.%20This%0Asurvey%20serves%20as%20a%20comprehensive%20resource%20for%20researchers%20and%20practitioners%0Ainterested%20in%20the%20latest%20advancements%20and%20future%0A%0ALink%3A%20http%3A//arxiv.org/abs/2501.03939v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DVisual%2520question%2520answering%253A%2520from%2520early%2520developments%2520to%2520recent%2520advances%2520--%250A%2520%2520a%2520survey%26entry.906535625%3DNgoc%2520Dung%2520Huynh%2520and%2520Mohamed%2520Reda%2520Bouadjenek%2520and%2520Sunil%2520Aryal%2520and%2520Imran%2520Razzak%2520and%2520Hakim%2520Hacid%26entry.1292438233%3D%2520%2520Visual%2520Question%2520Answering%2520%2528VQA%2529%2520is%2520an%2520evolving%2520research%2520field%2520aimed%2520at%250Aenabling%2520machines%2520to%2520answer%2520questions%2520about%2520visual%2520content%2520by%2520integrating%2520image%250Aand%2520language%2520processing%2520techniques%2520such%2520as%2520feature%2520extraction%252C%2520object%250Adetection%252C%2520text%2520embedding%252C%2520natural%2520language%2520understanding%252C%2520and%2520language%250Ageneration.%2520With%2520the%2520growth%2520of%2520multimodal%2520data%2520research%252C%2520VQA%2520has%2520gained%250Asignificant%2520attention%2520due%2520to%2520its%2520broad%2520applications%252C%2520including%2520interactive%250Aeducational%2520tools%252C%2520medical%2520image%2520diagnosis%252C%2520customer%2520service%252C%2520entertainment%252C%250Aand%2520social%2520media%2520captioning.%2520Additionally%252C%2520VQA%2520plays%2520a%2520vital%2520role%2520in%2520assisting%250Avisually%2520impaired%2520individuals%2520by%2520generating%2520descriptive%2520content%2520from%2520images.%250AThis%2520survey%2520introduces%2520a%2520taxonomy%2520of%2520VQA%2520architectures%252C%2520categorizing%2520them%2520based%250Aon%2520design%2520choices%2520and%2520key%2520components%2520to%2520facilitate%2520comparative%2520analysis%2520and%250Aevaluation.%2520We%2520review%2520major%2520VQA%2520approaches%252C%2520focusing%2520on%2520deep%2520learning-based%250Amethods%252C%2520and%2520explore%2520the%2520emerging%2520field%2520of%2520Large%2520Visual%2520Language%2520Models%2520%2528LVLMs%2529%250Athat%2520have%2520demonstrated%2520success%2520in%2520multimodal%2520tasks%2520like%2520VQA.%2520The%2520paper%2520further%250Aexamines%2520available%2520datasets%2520and%2520evaluation%2520metrics%2520essential%2520for%2520measuring%2520VQA%250Asystem%2520performance%252C%2520followed%2520by%2520an%2520exploration%2520of%2520real-world%2520VQA%2520applications.%250AFinally%252C%2520we%2520highlight%2520ongoing%2520challenges%2520and%2520future%2520directions%2520in%2520VQA%2520research%252C%250Apresenting%2520open%2520questions%2520and%2520potential%2520areas%2520for%2520further%2520development.%2520This%250Asurvey%2520serves%2520as%2520a%2520comprehensive%2520resource%2520for%2520researchers%2520and%2520practitioners%250Ainterested%2520in%2520the%2520latest%2520advancements%2520and%2520future%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2501.03939v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Visual%20question%20answering%3A%20from%20early%20developments%20to%20recent%20advances%20--%0A%20%20a%20survey&entry.906535625=Ngoc%20Dung%20Huynh%20and%20Mohamed%20Reda%20Bouadjenek%20and%20Sunil%20Aryal%20and%20Imran%20Razzak%20and%20Hakim%20Hacid&entry.1292438233=%20%20Visual%20Question%20Answering%20%28VQA%29%20is%20an%20evolving%20research%20field%20aimed%20at%0Aenabling%20machines%20to%20answer%20questions%20about%20visual%20content%20by%20integrating%20image%0Aand%20language%20processing%20techniques%20such%20as%20feature%20extraction%2C%20object%0Adetection%2C%20text%20embedding%2C%20natural%20language%20understanding%2C%20and%20language%0Ageneration.%20With%20the%20growth%20of%20multimodal%20data%20research%2C%20VQA%20has%20gained%0Asignificant%20attention%20due%20to%20its%20broad%20applications%2C%20including%20interactive%0Aeducational%20tools%2C%20medical%20image%20diagnosis%2C%20customer%20service%2C%20entertainment%2C%0Aand%20social%20media%20captioning.%20Additionally%2C%20VQA%20plays%20a%20vital%20role%20in%20assisting%0Avisually%20impaired%20individuals%20by%20generating%20descriptive%20content%20from%20images.%0AThis%20survey%20introduces%20a%20taxonomy%20of%20VQA%20architectures%2C%20categorizing%20them%20based%0Aon%20design%20choices%20and%20key%20components%20to%20facilitate%20comparative%20analysis%20and%0Aevaluation.%20We%20review%20major%20VQA%20approaches%2C%20focusing%20on%20deep%20learning-based%0Amethods%2C%20and%20explore%20the%20emerging%20field%20of%20Large%20Visual%20Language%20Models%20%28LVLMs%29%0Athat%20have%20demonstrated%20success%20in%20multimodal%20tasks%20like%20VQA.%20The%20paper%20further%0Aexamines%20available%20datasets%20and%20evaluation%20metrics%20essential%20for%20measuring%20VQA%0Asystem%20performance%2C%20followed%20by%20an%20exploration%20of%20real-world%20VQA%20applications.%0AFinally%2C%20we%20highlight%20ongoing%20challenges%20and%20future%20directions%20in%20VQA%20research%2C%0Apresenting%20open%20questions%20and%20potential%20areas%20for%20further%20development.%20This%0Asurvey%20serves%20as%20a%20comprehensive%20resource%20for%20researchers%20and%20practitioners%0Ainterested%20in%20the%20latest%20advancements%20and%20future%0A&entry.1838667208=http%3A//arxiv.org/abs/2501.03939v1&entry.124074799=Read"},
{"title": "Are VLMs Ready for Autonomous Driving? An Empirical Study from the\n  Reliability, Data, and Metric Perspectives", "author": "Shaoyuan Xie and Lingdong Kong and Yuhao Dong and Chonghao Sima and Wenwei Zhang and Qi Alfred Chen and Ziwei Liu and Liang Pan", "abstract": "  Recent advancements in Vision-Language Models (VLMs) have sparked interest in\ntheir use for autonomous driving, particularly in generating interpretable\ndriving decisions through natural language. However, the assumption that VLMs\ninherently provide visually grounded, reliable, and interpretable explanations\nfor driving remains largely unexamined. To address this gap, we introduce\nDriveBench, a benchmark dataset designed to evaluate VLM reliability across 17\nsettings (clean, corrupted, and text-only inputs), encompassing 19,200 frames,\n20,498 question-answer pairs, three question types, four mainstream driving\ntasks, and a total of 12 popular VLMs. Our findings reveal that VLMs often\ngenerate plausible responses derived from general knowledge or textual cues\nrather than true visual grounding, especially under degraded or missing visual\ninputs. This behavior, concealed by dataset imbalances and insufficient\nevaluation metrics, poses significant risks in safety-critical scenarios like\nautonomous driving. We further observe that VLMs struggle with multi-modal\nreasoning and display heightened sensitivity to input corruptions, leading to\ninconsistencies in performance. To address these challenges, we propose refined\nevaluation metrics that prioritize robust visual grounding and multi-modal\nunderstanding. Additionally, we highlight the potential of leveraging VLMs'\nawareness of corruptions to enhance their reliability, offering a roadmap for\ndeveloping more trustworthy and interpretable decision-making systems in\nreal-world autonomous driving contexts. The benchmark toolkit is publicly\naccessible.\n", "link": "http://arxiv.org/abs/2501.04003v1", "date": "2025-01-07", "relevancy": 2.782, "topK": [{"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.5599}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5546}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5546}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Are%20VLMs%20Ready%20for%20Autonomous%20Driving%3F%20An%20Empirical%20Study%20from%20the%0A%20%20Reliability%2C%20Data%2C%20and%20Metric%20Perspectives&body=Title%3A%20Are%20VLMs%20Ready%20for%20Autonomous%20Driving%3F%20An%20Empirical%20Study%20from%20the%0A%20%20Reliability%2C%20Data%2C%20and%20Metric%20Perspectives%0AAuthor%3A%20Shaoyuan%20Xie%20and%20Lingdong%20Kong%20and%20Yuhao%20Dong%20and%20Chonghao%20Sima%20and%20Wenwei%20Zhang%20and%20Qi%20Alfred%20Chen%20and%20Ziwei%20Liu%20and%20Liang%20Pan%0AAbstract%3A%20%20%20Recent%20advancements%20in%20Vision-Language%20Models%20%28VLMs%29%20have%20sparked%20interest%20in%0Atheir%20use%20for%20autonomous%20driving%2C%20particularly%20in%20generating%20interpretable%0Adriving%20decisions%20through%20natural%20language.%20However%2C%20the%20assumption%20that%20VLMs%0Ainherently%20provide%20visually%20grounded%2C%20reliable%2C%20and%20interpretable%20explanations%0Afor%20driving%20remains%20largely%20unexamined.%20To%20address%20this%20gap%2C%20we%20introduce%0ADriveBench%2C%20a%20benchmark%20dataset%20designed%20to%20evaluate%20VLM%20reliability%20across%2017%0Asettings%20%28clean%2C%20corrupted%2C%20and%20text-only%20inputs%29%2C%20encompassing%2019%2C200%20frames%2C%0A20%2C498%20question-answer%20pairs%2C%20three%20question%20types%2C%20four%20mainstream%20driving%0Atasks%2C%20and%20a%20total%20of%2012%20popular%20VLMs.%20Our%20findings%20reveal%20that%20VLMs%20often%0Agenerate%20plausible%20responses%20derived%20from%20general%20knowledge%20or%20textual%20cues%0Arather%20than%20true%20visual%20grounding%2C%20especially%20under%20degraded%20or%20missing%20visual%0Ainputs.%20This%20behavior%2C%20concealed%20by%20dataset%20imbalances%20and%20insufficient%0Aevaluation%20metrics%2C%20poses%20significant%20risks%20in%20safety-critical%20scenarios%20like%0Aautonomous%20driving.%20We%20further%20observe%20that%20VLMs%20struggle%20with%20multi-modal%0Areasoning%20and%20display%20heightened%20sensitivity%20to%20input%20corruptions%2C%20leading%20to%0Ainconsistencies%20in%20performance.%20To%20address%20these%20challenges%2C%20we%20propose%20refined%0Aevaluation%20metrics%20that%20prioritize%20robust%20visual%20grounding%20and%20multi-modal%0Aunderstanding.%20Additionally%2C%20we%20highlight%20the%20potential%20of%20leveraging%20VLMs%27%0Aawareness%20of%20corruptions%20to%20enhance%20their%20reliability%2C%20offering%20a%20roadmap%20for%0Adeveloping%20more%20trustworthy%20and%20interpretable%20decision-making%20systems%20in%0Areal-world%20autonomous%20driving%20contexts.%20The%20benchmark%20toolkit%20is%20publicly%0Aaccessible.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2501.04003v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DAre%2520VLMs%2520Ready%2520for%2520Autonomous%2520Driving%253F%2520An%2520Empirical%2520Study%2520from%2520the%250A%2520%2520Reliability%252C%2520Data%252C%2520and%2520Metric%2520Perspectives%26entry.906535625%3DShaoyuan%2520Xie%2520and%2520Lingdong%2520Kong%2520and%2520Yuhao%2520Dong%2520and%2520Chonghao%2520Sima%2520and%2520Wenwei%2520Zhang%2520and%2520Qi%2520Alfred%2520Chen%2520and%2520Ziwei%2520Liu%2520and%2520Liang%2520Pan%26entry.1292438233%3D%2520%2520Recent%2520advancements%2520in%2520Vision-Language%2520Models%2520%2528VLMs%2529%2520have%2520sparked%2520interest%2520in%250Atheir%2520use%2520for%2520autonomous%2520driving%252C%2520particularly%2520in%2520generating%2520interpretable%250Adriving%2520decisions%2520through%2520natural%2520language.%2520However%252C%2520the%2520assumption%2520that%2520VLMs%250Ainherently%2520provide%2520visually%2520grounded%252C%2520reliable%252C%2520and%2520interpretable%2520explanations%250Afor%2520driving%2520remains%2520largely%2520unexamined.%2520To%2520address%2520this%2520gap%252C%2520we%2520introduce%250ADriveBench%252C%2520a%2520benchmark%2520dataset%2520designed%2520to%2520evaluate%2520VLM%2520reliability%2520across%252017%250Asettings%2520%2528clean%252C%2520corrupted%252C%2520and%2520text-only%2520inputs%2529%252C%2520encompassing%252019%252C200%2520frames%252C%250A20%252C498%2520question-answer%2520pairs%252C%2520three%2520question%2520types%252C%2520four%2520mainstream%2520driving%250Atasks%252C%2520and%2520a%2520total%2520of%252012%2520popular%2520VLMs.%2520Our%2520findings%2520reveal%2520that%2520VLMs%2520often%250Agenerate%2520plausible%2520responses%2520derived%2520from%2520general%2520knowledge%2520or%2520textual%2520cues%250Arather%2520than%2520true%2520visual%2520grounding%252C%2520especially%2520under%2520degraded%2520or%2520missing%2520visual%250Ainputs.%2520This%2520behavior%252C%2520concealed%2520by%2520dataset%2520imbalances%2520and%2520insufficient%250Aevaluation%2520metrics%252C%2520poses%2520significant%2520risks%2520in%2520safety-critical%2520scenarios%2520like%250Aautonomous%2520driving.%2520We%2520further%2520observe%2520that%2520VLMs%2520struggle%2520with%2520multi-modal%250Areasoning%2520and%2520display%2520heightened%2520sensitivity%2520to%2520input%2520corruptions%252C%2520leading%2520to%250Ainconsistencies%2520in%2520performance.%2520To%2520address%2520these%2520challenges%252C%2520we%2520propose%2520refined%250Aevaluation%2520metrics%2520that%2520prioritize%2520robust%2520visual%2520grounding%2520and%2520multi-modal%250Aunderstanding.%2520Additionally%252C%2520we%2520highlight%2520the%2520potential%2520of%2520leveraging%2520VLMs%2527%250Aawareness%2520of%2520corruptions%2520to%2520enhance%2520their%2520reliability%252C%2520offering%2520a%2520roadmap%2520for%250Adeveloping%2520more%2520trustworthy%2520and%2520interpretable%2520decision-making%2520systems%2520in%250Areal-world%2520autonomous%2520driving%2520contexts.%2520The%2520benchmark%2520toolkit%2520is%2520publicly%250Aaccessible.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2501.04003v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Are%20VLMs%20Ready%20for%20Autonomous%20Driving%3F%20An%20Empirical%20Study%20from%20the%0A%20%20Reliability%2C%20Data%2C%20and%20Metric%20Perspectives&entry.906535625=Shaoyuan%20Xie%20and%20Lingdong%20Kong%20and%20Yuhao%20Dong%20and%20Chonghao%20Sima%20and%20Wenwei%20Zhang%20and%20Qi%20Alfred%20Chen%20and%20Ziwei%20Liu%20and%20Liang%20Pan&entry.1292438233=%20%20Recent%20advancements%20in%20Vision-Language%20Models%20%28VLMs%29%20have%20sparked%20interest%20in%0Atheir%20use%20for%20autonomous%20driving%2C%20particularly%20in%20generating%20interpretable%0Adriving%20decisions%20through%20natural%20language.%20However%2C%20the%20assumption%20that%20VLMs%0Ainherently%20provide%20visually%20grounded%2C%20reliable%2C%20and%20interpretable%20explanations%0Afor%20driving%20remains%20largely%20unexamined.%20To%20address%20this%20gap%2C%20we%20introduce%0ADriveBench%2C%20a%20benchmark%20dataset%20designed%20to%20evaluate%20VLM%20reliability%20across%2017%0Asettings%20%28clean%2C%20corrupted%2C%20and%20text-only%20inputs%29%2C%20encompassing%2019%2C200%20frames%2C%0A20%2C498%20question-answer%20pairs%2C%20three%20question%20types%2C%20four%20mainstream%20driving%0Atasks%2C%20and%20a%20total%20of%2012%20popular%20VLMs.%20Our%20findings%20reveal%20that%20VLMs%20often%0Agenerate%20plausible%20responses%20derived%20from%20general%20knowledge%20or%20textual%20cues%0Arather%20than%20true%20visual%20grounding%2C%20especially%20under%20degraded%20or%20missing%20visual%0Ainputs.%20This%20behavior%2C%20concealed%20by%20dataset%20imbalances%20and%20insufficient%0Aevaluation%20metrics%2C%20poses%20significant%20risks%20in%20safety-critical%20scenarios%20like%0Aautonomous%20driving.%20We%20further%20observe%20that%20VLMs%20struggle%20with%20multi-modal%0Areasoning%20and%20display%20heightened%20sensitivity%20to%20input%20corruptions%2C%20leading%20to%0Ainconsistencies%20in%20performance.%20To%20address%20these%20challenges%2C%20we%20propose%20refined%0Aevaluation%20metrics%20that%20prioritize%20robust%20visual%20grounding%20and%20multi-modal%0Aunderstanding.%20Additionally%2C%20we%20highlight%20the%20potential%20of%20leveraging%20VLMs%27%0Aawareness%20of%20corruptions%20to%20enhance%20their%20reliability%2C%20offering%20a%20roadmap%20for%0Adeveloping%20more%20trustworthy%20and%20interpretable%20decision-making%20systems%20in%0Areal-world%20autonomous%20driving%20contexts.%20The%20benchmark%20toolkit%20is%20publicly%0Aaccessible.%0A&entry.1838667208=http%3A//arxiv.org/abs/2501.04003v1&entry.124074799=Read"},
{"title": "MedFocusCLIP : Improving few shot classification in medical datasets\n  using pixel wise attention", "author": "Aadya Arora and Vinay Namboodiri", "abstract": "  With the popularity of foundational models, parameter efficient fine tuning\nhas become the defacto approach to leverage pretrained models to perform\ndownstream tasks. Taking inspiration from recent advances in large language\nmodels, Visual Prompt Tuning, and similar techniques, learn an additional\nprompt to efficiently finetune a pretrained vision foundational model. However,\nwe observe that such prompting is insufficient for fine-grained visual\nclassification tasks such as medical image classification, where there is large\ninter-class variance, and small intra-class variance. Hence, in this paper we\npropose to leverage advanced segmentation capabilities of Segment Anything\nModel 2 (SAM2) as a visual prompting cue to help visual encoder in the CLIP\n(Contrastive Language-Image Pretraining) by guiding the attention in CLIP\nvisual encoder to relevant regions in the image. This helps the model to focus\non highly discriminative regions, without getting distracted from visually\nsimilar background features, an essential requirement in a fewshot, finegrained\nclassification setting. We evaluate our method on diverse medical datasets\nincluding X-rays, CT scans, and MRI images, and report an accuracy of (71%,\n81%, 86%, 58%) from the proposed approach on (COVID, lung-disease, brain-tumor,\nbreast-cancer) datasets against (66%, 70%, 68%, 29%) from a pretrained CLIP\nmodel after fewshot training. The proposed approach also allows to obtain\ninterpretable explanation for the classification performance through the\nlocalization obtained using segmentation.\n", "link": "http://arxiv.org/abs/2501.03839v1", "date": "2025-01-07", "relevancy": 2.7641, "topK": [{"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5855}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5365}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5365}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20MedFocusCLIP%20%3A%20Improving%20few%20shot%20classification%20in%20medical%20datasets%0A%20%20using%20pixel%20wise%20attention&body=Title%3A%20MedFocusCLIP%20%3A%20Improving%20few%20shot%20classification%20in%20medical%20datasets%0A%20%20using%20pixel%20wise%20attention%0AAuthor%3A%20Aadya%20Arora%20and%20Vinay%20Namboodiri%0AAbstract%3A%20%20%20With%20the%20popularity%20of%20foundational%20models%2C%20parameter%20efficient%20fine%20tuning%0Ahas%20become%20the%20defacto%20approach%20to%20leverage%20pretrained%20models%20to%20perform%0Adownstream%20tasks.%20Taking%20inspiration%20from%20recent%20advances%20in%20large%20language%0Amodels%2C%20Visual%20Prompt%20Tuning%2C%20and%20similar%20techniques%2C%20learn%20an%20additional%0Aprompt%20to%20efficiently%20finetune%20a%20pretrained%20vision%20foundational%20model.%20However%2C%0Awe%20observe%20that%20such%20prompting%20is%20insufficient%20for%20fine-grained%20visual%0Aclassification%20tasks%20such%20as%20medical%20image%20classification%2C%20where%20there%20is%20large%0Ainter-class%20variance%2C%20and%20small%20intra-class%20variance.%20Hence%2C%20in%20this%20paper%20we%0Apropose%20to%20leverage%20advanced%20segmentation%20capabilities%20of%20Segment%20Anything%0AModel%202%20%28SAM2%29%20as%20a%20visual%20prompting%20cue%20to%20help%20visual%20encoder%20in%20the%20CLIP%0A%28Contrastive%20Language-Image%20Pretraining%29%20by%20guiding%20the%20attention%20in%20CLIP%0Avisual%20encoder%20to%20relevant%20regions%20in%20the%20image.%20This%20helps%20the%20model%20to%20focus%0Aon%20highly%20discriminative%20regions%2C%20without%20getting%20distracted%20from%20visually%0Asimilar%20background%20features%2C%20an%20essential%20requirement%20in%20a%20fewshot%2C%20finegrained%0Aclassification%20setting.%20We%20evaluate%20our%20method%20on%20diverse%20medical%20datasets%0Aincluding%20X-rays%2C%20CT%20scans%2C%20and%20MRI%20images%2C%20and%20report%20an%20accuracy%20of%20%2871%25%2C%0A81%25%2C%2086%25%2C%2058%25%29%20from%20the%20proposed%20approach%20on%20%28COVID%2C%20lung-disease%2C%20brain-tumor%2C%0Abreast-cancer%29%20datasets%20against%20%2866%25%2C%2070%25%2C%2068%25%2C%2029%25%29%20from%20a%20pretrained%20CLIP%0Amodel%20after%20fewshot%20training.%20The%20proposed%20approach%20also%20allows%20to%20obtain%0Ainterpretable%20explanation%20for%20the%20classification%20performance%20through%20the%0Alocalization%20obtained%20using%20segmentation.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2501.03839v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DMedFocusCLIP%2520%253A%2520Improving%2520few%2520shot%2520classification%2520in%2520medical%2520datasets%250A%2520%2520using%2520pixel%2520wise%2520attention%26entry.906535625%3DAadya%2520Arora%2520and%2520Vinay%2520Namboodiri%26entry.1292438233%3D%2520%2520With%2520the%2520popularity%2520of%2520foundational%2520models%252C%2520parameter%2520efficient%2520fine%2520tuning%250Ahas%2520become%2520the%2520defacto%2520approach%2520to%2520leverage%2520pretrained%2520models%2520to%2520perform%250Adownstream%2520tasks.%2520Taking%2520inspiration%2520from%2520recent%2520advances%2520in%2520large%2520language%250Amodels%252C%2520Visual%2520Prompt%2520Tuning%252C%2520and%2520similar%2520techniques%252C%2520learn%2520an%2520additional%250Aprompt%2520to%2520efficiently%2520finetune%2520a%2520pretrained%2520vision%2520foundational%2520model.%2520However%252C%250Awe%2520observe%2520that%2520such%2520prompting%2520is%2520insufficient%2520for%2520fine-grained%2520visual%250Aclassification%2520tasks%2520such%2520as%2520medical%2520image%2520classification%252C%2520where%2520there%2520is%2520large%250Ainter-class%2520variance%252C%2520and%2520small%2520intra-class%2520variance.%2520Hence%252C%2520in%2520this%2520paper%2520we%250Apropose%2520to%2520leverage%2520advanced%2520segmentation%2520capabilities%2520of%2520Segment%2520Anything%250AModel%25202%2520%2528SAM2%2529%2520as%2520a%2520visual%2520prompting%2520cue%2520to%2520help%2520visual%2520encoder%2520in%2520the%2520CLIP%250A%2528Contrastive%2520Language-Image%2520Pretraining%2529%2520by%2520guiding%2520the%2520attention%2520in%2520CLIP%250Avisual%2520encoder%2520to%2520relevant%2520regions%2520in%2520the%2520image.%2520This%2520helps%2520the%2520model%2520to%2520focus%250Aon%2520highly%2520discriminative%2520regions%252C%2520without%2520getting%2520distracted%2520from%2520visually%250Asimilar%2520background%2520features%252C%2520an%2520essential%2520requirement%2520in%2520a%2520fewshot%252C%2520finegrained%250Aclassification%2520setting.%2520We%2520evaluate%2520our%2520method%2520on%2520diverse%2520medical%2520datasets%250Aincluding%2520X-rays%252C%2520CT%2520scans%252C%2520and%2520MRI%2520images%252C%2520and%2520report%2520an%2520accuracy%2520of%2520%252871%2525%252C%250A81%2525%252C%252086%2525%252C%252058%2525%2529%2520from%2520the%2520proposed%2520approach%2520on%2520%2528COVID%252C%2520lung-disease%252C%2520brain-tumor%252C%250Abreast-cancer%2529%2520datasets%2520against%2520%252866%2525%252C%252070%2525%252C%252068%2525%252C%252029%2525%2529%2520from%2520a%2520pretrained%2520CLIP%250Amodel%2520after%2520fewshot%2520training.%2520The%2520proposed%2520approach%2520also%2520allows%2520to%2520obtain%250Ainterpretable%2520explanation%2520for%2520the%2520classification%2520performance%2520through%2520the%250Alocalization%2520obtained%2520using%2520segmentation.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2501.03839v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=MedFocusCLIP%20%3A%20Improving%20few%20shot%20classification%20in%20medical%20datasets%0A%20%20using%20pixel%20wise%20attention&entry.906535625=Aadya%20Arora%20and%20Vinay%20Namboodiri&entry.1292438233=%20%20With%20the%20popularity%20of%20foundational%20models%2C%20parameter%20efficient%20fine%20tuning%0Ahas%20become%20the%20defacto%20approach%20to%20leverage%20pretrained%20models%20to%20perform%0Adownstream%20tasks.%20Taking%20inspiration%20from%20recent%20advances%20in%20large%20language%0Amodels%2C%20Visual%20Prompt%20Tuning%2C%20and%20similar%20techniques%2C%20learn%20an%20additional%0Aprompt%20to%20efficiently%20finetune%20a%20pretrained%20vision%20foundational%20model.%20However%2C%0Awe%20observe%20that%20such%20prompting%20is%20insufficient%20for%20fine-grained%20visual%0Aclassification%20tasks%20such%20as%20medical%20image%20classification%2C%20where%20there%20is%20large%0Ainter-class%20variance%2C%20and%20small%20intra-class%20variance.%20Hence%2C%20in%20this%20paper%20we%0Apropose%20to%20leverage%20advanced%20segmentation%20capabilities%20of%20Segment%20Anything%0AModel%202%20%28SAM2%29%20as%20a%20visual%20prompting%20cue%20to%20help%20visual%20encoder%20in%20the%20CLIP%0A%28Contrastive%20Language-Image%20Pretraining%29%20by%20guiding%20the%20attention%20in%20CLIP%0Avisual%20encoder%20to%20relevant%20regions%20in%20the%20image.%20This%20helps%20the%20model%20to%20focus%0Aon%20highly%20discriminative%20regions%2C%20without%20getting%20distracted%20from%20visually%0Asimilar%20background%20features%2C%20an%20essential%20requirement%20in%20a%20fewshot%2C%20finegrained%0Aclassification%20setting.%20We%20evaluate%20our%20method%20on%20diverse%20medical%20datasets%0Aincluding%20X-rays%2C%20CT%20scans%2C%20and%20MRI%20images%2C%20and%20report%20an%20accuracy%20of%20%2871%25%2C%0A81%25%2C%2086%25%2C%2058%25%29%20from%20the%20proposed%20approach%20on%20%28COVID%2C%20lung-disease%2C%20brain-tumor%2C%0Abreast-cancer%29%20datasets%20against%20%2866%25%2C%2070%25%2C%2068%25%2C%2029%25%29%20from%20a%20pretrained%20CLIP%0Amodel%20after%20fewshot%20training.%20The%20proposed%20approach%20also%20allows%20to%20obtain%0Ainterpretable%20explanation%20for%20the%20classification%20performance%20through%20the%0Alocalization%20obtained%20using%20segmentation.%0A&entry.1838667208=http%3A//arxiv.org/abs/2501.03839v1&entry.124074799=Read"},
{"title": "LLaVA Steering: Visual Instruction Tuning with 500x Fewer Parameters\n  through Modality Linear Representation-Steering", "author": "Jinhe Bi and Yujun Wang and Haokun Chen and Xun Xiao and Artur Hecker and Volker Tresp and Yunpu Ma", "abstract": "  Multimodal Large Language Models (MLLMs) have significantly advanced visual\ntasks by integrating visual representations into large language models (LLMs).\nThe textual modality, inherited from LLMs, equips MLLMs with abilities like\ninstruction following and in-context learning. In contrast, the visual modality\nenhances performance in downstream tasks by leveraging rich semantic content,\nspatial information, and grounding capabilities. These intrinsic modalities\nwork synergistically across various visual tasks. Our research initially\nreveals a persistent imbalance between these modalities, with text often\ndominating output generation during visual instruction tuning. This imbalance\noccurs when using both full fine-tuning and parameter-efficient fine-tuning\n(PEFT) methods. We then found that re-balancing these modalities can\nsignificantly reduce the number of trainable parameters required, inspiring a\ndirection for further optimizing visual instruction tuning. We introduce\nModality Linear Representation-Steering (MoReS) to achieve the goal. MoReS\neffectively re-balances the intrinsic modalities throughout the model, where\nthe key idea is to steer visual representations through linear transformations\nin the visual subspace across each model layer. To validate our solution, we\ncomposed LLaVA Steering, a suite of models integrated with the proposed MoReS\nmethod. Evaluation results show that the composed LLaVA Steering models\nrequire, on average, 500 times fewer trainable parameters than LoRA needs while\nstill achieving comparable performance across three visual benchmarks and eight\nvisual question-answering tasks. Last, we present the LLaVA Steering Factory,\nan in-house developed platform that enables researchers to quickly customize\nvarious MLLMs with component-based architecture for seamlessly integrating\nstate-of-the-art models, and evaluate their intrinsic modality imbalance.\n", "link": "http://arxiv.org/abs/2412.12359v2", "date": "2025-01-07", "relevancy": 2.7592, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5567}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5567}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5421}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20LLaVA%20Steering%3A%20Visual%20Instruction%20Tuning%20with%20500x%20Fewer%20Parameters%0A%20%20through%20Modality%20Linear%20Representation-Steering&body=Title%3A%20LLaVA%20Steering%3A%20Visual%20Instruction%20Tuning%20with%20500x%20Fewer%20Parameters%0A%20%20through%20Modality%20Linear%20Representation-Steering%0AAuthor%3A%20Jinhe%20Bi%20and%20Yujun%20Wang%20and%20Haokun%20Chen%20and%20Xun%20Xiao%20and%20Artur%20Hecker%20and%20Volker%20Tresp%20and%20Yunpu%20Ma%0AAbstract%3A%20%20%20Multimodal%20Large%20Language%20Models%20%28MLLMs%29%20have%20significantly%20advanced%20visual%0Atasks%20by%20integrating%20visual%20representations%20into%20large%20language%20models%20%28LLMs%29.%0AThe%20textual%20modality%2C%20inherited%20from%20LLMs%2C%20equips%20MLLMs%20with%20abilities%20like%0Ainstruction%20following%20and%20in-context%20learning.%20In%20contrast%2C%20the%20visual%20modality%0Aenhances%20performance%20in%20downstream%20tasks%20by%20leveraging%20rich%20semantic%20content%2C%0Aspatial%20information%2C%20and%20grounding%20capabilities.%20These%20intrinsic%20modalities%0Awork%20synergistically%20across%20various%20visual%20tasks.%20Our%20research%20initially%0Areveals%20a%20persistent%20imbalance%20between%20these%20modalities%2C%20with%20text%20often%0Adominating%20output%20generation%20during%20visual%20instruction%20tuning.%20This%20imbalance%0Aoccurs%20when%20using%20both%20full%20fine-tuning%20and%20parameter-efficient%20fine-tuning%0A%28PEFT%29%20methods.%20We%20then%20found%20that%20re-balancing%20these%20modalities%20can%0Asignificantly%20reduce%20the%20number%20of%20trainable%20parameters%20required%2C%20inspiring%20a%0Adirection%20for%20further%20optimizing%20visual%20instruction%20tuning.%20We%20introduce%0AModality%20Linear%20Representation-Steering%20%28MoReS%29%20to%20achieve%20the%20goal.%20MoReS%0Aeffectively%20re-balances%20the%20intrinsic%20modalities%20throughout%20the%20model%2C%20where%0Athe%20key%20idea%20is%20to%20steer%20visual%20representations%20through%20linear%20transformations%0Ain%20the%20visual%20subspace%20across%20each%20model%20layer.%20To%20validate%20our%20solution%2C%20we%0Acomposed%20LLaVA%20Steering%2C%20a%20suite%20of%20models%20integrated%20with%20the%20proposed%20MoReS%0Amethod.%20Evaluation%20results%20show%20that%20the%20composed%20LLaVA%20Steering%20models%0Arequire%2C%20on%20average%2C%20500%20times%20fewer%20trainable%20parameters%20than%20LoRA%20needs%20while%0Astill%20achieving%20comparable%20performance%20across%20three%20visual%20benchmarks%20and%20eight%0Avisual%20question-answering%20tasks.%20Last%2C%20we%20present%20the%20LLaVA%20Steering%20Factory%2C%0Aan%20in-house%20developed%20platform%20that%20enables%20researchers%20to%20quickly%20customize%0Avarious%20MLLMs%20with%20component-based%20architecture%20for%20seamlessly%20integrating%0Astate-of-the-art%20models%2C%20and%20evaluate%20their%20intrinsic%20modality%20imbalance.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2412.12359v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DLLaVA%2520Steering%253A%2520Visual%2520Instruction%2520Tuning%2520with%2520500x%2520Fewer%2520Parameters%250A%2520%2520through%2520Modality%2520Linear%2520Representation-Steering%26entry.906535625%3DJinhe%2520Bi%2520and%2520Yujun%2520Wang%2520and%2520Haokun%2520Chen%2520and%2520Xun%2520Xiao%2520and%2520Artur%2520Hecker%2520and%2520Volker%2520Tresp%2520and%2520Yunpu%2520Ma%26entry.1292438233%3D%2520%2520Multimodal%2520Large%2520Language%2520Models%2520%2528MLLMs%2529%2520have%2520significantly%2520advanced%2520visual%250Atasks%2520by%2520integrating%2520visual%2520representations%2520into%2520large%2520language%2520models%2520%2528LLMs%2529.%250AThe%2520textual%2520modality%252C%2520inherited%2520from%2520LLMs%252C%2520equips%2520MLLMs%2520with%2520abilities%2520like%250Ainstruction%2520following%2520and%2520in-context%2520learning.%2520In%2520contrast%252C%2520the%2520visual%2520modality%250Aenhances%2520performance%2520in%2520downstream%2520tasks%2520by%2520leveraging%2520rich%2520semantic%2520content%252C%250Aspatial%2520information%252C%2520and%2520grounding%2520capabilities.%2520These%2520intrinsic%2520modalities%250Awork%2520synergistically%2520across%2520various%2520visual%2520tasks.%2520Our%2520research%2520initially%250Areveals%2520a%2520persistent%2520imbalance%2520between%2520these%2520modalities%252C%2520with%2520text%2520often%250Adominating%2520output%2520generation%2520during%2520visual%2520instruction%2520tuning.%2520This%2520imbalance%250Aoccurs%2520when%2520using%2520both%2520full%2520fine-tuning%2520and%2520parameter-efficient%2520fine-tuning%250A%2528PEFT%2529%2520methods.%2520We%2520then%2520found%2520that%2520re-balancing%2520these%2520modalities%2520can%250Asignificantly%2520reduce%2520the%2520number%2520of%2520trainable%2520parameters%2520required%252C%2520inspiring%2520a%250Adirection%2520for%2520further%2520optimizing%2520visual%2520instruction%2520tuning.%2520We%2520introduce%250AModality%2520Linear%2520Representation-Steering%2520%2528MoReS%2529%2520to%2520achieve%2520the%2520goal.%2520MoReS%250Aeffectively%2520re-balances%2520the%2520intrinsic%2520modalities%2520throughout%2520the%2520model%252C%2520where%250Athe%2520key%2520idea%2520is%2520to%2520steer%2520visual%2520representations%2520through%2520linear%2520transformations%250Ain%2520the%2520visual%2520subspace%2520across%2520each%2520model%2520layer.%2520To%2520validate%2520our%2520solution%252C%2520we%250Acomposed%2520LLaVA%2520Steering%252C%2520a%2520suite%2520of%2520models%2520integrated%2520with%2520the%2520proposed%2520MoReS%250Amethod.%2520Evaluation%2520results%2520show%2520that%2520the%2520composed%2520LLaVA%2520Steering%2520models%250Arequire%252C%2520on%2520average%252C%2520500%2520times%2520fewer%2520trainable%2520parameters%2520than%2520LoRA%2520needs%2520while%250Astill%2520achieving%2520comparable%2520performance%2520across%2520three%2520visual%2520benchmarks%2520and%2520eight%250Avisual%2520question-answering%2520tasks.%2520Last%252C%2520we%2520present%2520the%2520LLaVA%2520Steering%2520Factory%252C%250Aan%2520in-house%2520developed%2520platform%2520that%2520enables%2520researchers%2520to%2520quickly%2520customize%250Avarious%2520MLLMs%2520with%2520component-based%2520architecture%2520for%2520seamlessly%2520integrating%250Astate-of-the-art%2520models%252C%2520and%2520evaluate%2520their%2520intrinsic%2520modality%2520imbalance.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2412.12359v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=LLaVA%20Steering%3A%20Visual%20Instruction%20Tuning%20with%20500x%20Fewer%20Parameters%0A%20%20through%20Modality%20Linear%20Representation-Steering&entry.906535625=Jinhe%20Bi%20and%20Yujun%20Wang%20and%20Haokun%20Chen%20and%20Xun%20Xiao%20and%20Artur%20Hecker%20and%20Volker%20Tresp%20and%20Yunpu%20Ma&entry.1292438233=%20%20Multimodal%20Large%20Language%20Models%20%28MLLMs%29%20have%20significantly%20advanced%20visual%0Atasks%20by%20integrating%20visual%20representations%20into%20large%20language%20models%20%28LLMs%29.%0AThe%20textual%20modality%2C%20inherited%20from%20LLMs%2C%20equips%20MLLMs%20with%20abilities%20like%0Ainstruction%20following%20and%20in-context%20learning.%20In%20contrast%2C%20the%20visual%20modality%0Aenhances%20performance%20in%20downstream%20tasks%20by%20leveraging%20rich%20semantic%20content%2C%0Aspatial%20information%2C%20and%20grounding%20capabilities.%20These%20intrinsic%20modalities%0Awork%20synergistically%20across%20various%20visual%20tasks.%20Our%20research%20initially%0Areveals%20a%20persistent%20imbalance%20between%20these%20modalities%2C%20with%20text%20often%0Adominating%20output%20generation%20during%20visual%20instruction%20tuning.%20This%20imbalance%0Aoccurs%20when%20using%20both%20full%20fine-tuning%20and%20parameter-efficient%20fine-tuning%0A%28PEFT%29%20methods.%20We%20then%20found%20that%20re-balancing%20these%20modalities%20can%0Asignificantly%20reduce%20the%20number%20of%20trainable%20parameters%20required%2C%20inspiring%20a%0Adirection%20for%20further%20optimizing%20visual%20instruction%20tuning.%20We%20introduce%0AModality%20Linear%20Representation-Steering%20%28MoReS%29%20to%20achieve%20the%20goal.%20MoReS%0Aeffectively%20re-balances%20the%20intrinsic%20modalities%20throughout%20the%20model%2C%20where%0Athe%20key%20idea%20is%20to%20steer%20visual%20representations%20through%20linear%20transformations%0Ain%20the%20visual%20subspace%20across%20each%20model%20layer.%20To%20validate%20our%20solution%2C%20we%0Acomposed%20LLaVA%20Steering%2C%20a%20suite%20of%20models%20integrated%20with%20the%20proposed%20MoReS%0Amethod.%20Evaluation%20results%20show%20that%20the%20composed%20LLaVA%20Steering%20models%0Arequire%2C%20on%20average%2C%20500%20times%20fewer%20trainable%20parameters%20than%20LoRA%20needs%20while%0Astill%20achieving%20comparable%20performance%20across%20three%20visual%20benchmarks%20and%20eight%0Avisual%20question-answering%20tasks.%20Last%2C%20we%20present%20the%20LLaVA%20Steering%20Factory%2C%0Aan%20in-house%20developed%20platform%20that%20enables%20researchers%20to%20quickly%20customize%0Avarious%20MLLMs%20with%20component-based%20architecture%20for%20seamlessly%20integrating%0Astate-of-the-art%20models%2C%20and%20evaluate%20their%20intrinsic%20modality%20imbalance.%0A&entry.1838667208=http%3A//arxiv.org/abs/2412.12359v2&entry.124074799=Read"},
{"title": "MeshConv3D: Efficient convolution and pooling operators for triangular\n  3D meshes", "author": "Germain Bregeon and Marius Preda and Radu Ispas and Titus Zaharia", "abstract": "  Convolutional neural networks (CNNs) have been pivotal in various 2D image\nanalysis tasks, including computer vision, image indexing and retrieval or\nsemantic classification. Extending CNNs to 3D data such as point clouds and 3D\nmeshes raises significant challenges since the very basic convolution and\npooling operators need to be completely re-visited and re-defined in an\nappropriate manner to tackle irregular connectivity issues. In this paper, we\nintroduce MeshConv3D, a 3D mesh-dedicated methodology integrating specialized\nconvolution and face collapse-based pooling operators. MeshConv3D operates\ndirectly on meshes of arbitrary topology, without any need of prior\nre-meshing/conversion techniques. In order to validate our approach, we have\nconsidered a semantic classification task. The experimental results obtained on\nthree distinct benchmark datasets show that the proposed approach makes it\npossible to achieve equivalent or superior classification results, while\nminimizing the related memory footprint and computational load.\n", "link": "http://arxiv.org/abs/2501.03830v1", "date": "2025-01-07", "relevancy": 2.7455, "topK": [{"title": "Dynamic Gaussians Mesh: Consistent Mesh Reconstruction from Monocular\n  Videos", "link": "http://arxiv.org/abs/2404.12379v2", "similarity": 0.5785}, {"title": "CAT3D: Create Anything in 3D with Multi-View Diffusion Models", "link": "http://arxiv.org/abs/2405.10314v1", "similarity": 0.5344}, {"title": "CAT3D: Create Anything in 3D with Multi-View Diffusion Models", "link": "http://arxiv.org/abs/2405.10314v1", "similarity": 0.5344}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20MeshConv3D%3A%20Efficient%20convolution%20and%20pooling%20operators%20for%20triangular%0A%20%203D%20meshes&body=Title%3A%20MeshConv3D%3A%20Efficient%20convolution%20and%20pooling%20operators%20for%20triangular%0A%20%203D%20meshes%0AAuthor%3A%20Germain%20Bregeon%20and%20Marius%20Preda%20and%20Radu%20Ispas%20and%20Titus%20Zaharia%0AAbstract%3A%20%20%20Convolutional%20neural%20networks%20%28CNNs%29%20have%20been%20pivotal%20in%20various%202D%20image%0Aanalysis%20tasks%2C%20including%20computer%20vision%2C%20image%20indexing%20and%20retrieval%20or%0Asemantic%20classification.%20Extending%20CNNs%20to%203D%20data%20such%20as%20point%20clouds%20and%203D%0Ameshes%20raises%20significant%20challenges%20since%20the%20very%20basic%20convolution%20and%0Apooling%20operators%20need%20to%20be%20completely%20re-visited%20and%20re-defined%20in%20an%0Aappropriate%20manner%20to%20tackle%20irregular%20connectivity%20issues.%20In%20this%20paper%2C%20we%0Aintroduce%20MeshConv3D%2C%20a%203D%20mesh-dedicated%20methodology%20integrating%20specialized%0Aconvolution%20and%20face%20collapse-based%20pooling%20operators.%20MeshConv3D%20operates%0Adirectly%20on%20meshes%20of%20arbitrary%20topology%2C%20without%20any%20need%20of%20prior%0Are-meshing/conversion%20techniques.%20In%20order%20to%20validate%20our%20approach%2C%20we%20have%0Aconsidered%20a%20semantic%20classification%20task.%20The%20experimental%20results%20obtained%20on%0Athree%20distinct%20benchmark%20datasets%20show%20that%20the%20proposed%20approach%20makes%20it%0Apossible%20to%20achieve%20equivalent%20or%20superior%20classification%20results%2C%20while%0Aminimizing%20the%20related%20memory%20footprint%20and%20computational%20load.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2501.03830v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DMeshConv3D%253A%2520Efficient%2520convolution%2520and%2520pooling%2520operators%2520for%2520triangular%250A%2520%25203D%2520meshes%26entry.906535625%3DGermain%2520Bregeon%2520and%2520Marius%2520Preda%2520and%2520Radu%2520Ispas%2520and%2520Titus%2520Zaharia%26entry.1292438233%3D%2520%2520Convolutional%2520neural%2520networks%2520%2528CNNs%2529%2520have%2520been%2520pivotal%2520in%2520various%25202D%2520image%250Aanalysis%2520tasks%252C%2520including%2520computer%2520vision%252C%2520image%2520indexing%2520and%2520retrieval%2520or%250Asemantic%2520classification.%2520Extending%2520CNNs%2520to%25203D%2520data%2520such%2520as%2520point%2520clouds%2520and%25203D%250Ameshes%2520raises%2520significant%2520challenges%2520since%2520the%2520very%2520basic%2520convolution%2520and%250Apooling%2520operators%2520need%2520to%2520be%2520completely%2520re-visited%2520and%2520re-defined%2520in%2520an%250Aappropriate%2520manner%2520to%2520tackle%2520irregular%2520connectivity%2520issues.%2520In%2520this%2520paper%252C%2520we%250Aintroduce%2520MeshConv3D%252C%2520a%25203D%2520mesh-dedicated%2520methodology%2520integrating%2520specialized%250Aconvolution%2520and%2520face%2520collapse-based%2520pooling%2520operators.%2520MeshConv3D%2520operates%250Adirectly%2520on%2520meshes%2520of%2520arbitrary%2520topology%252C%2520without%2520any%2520need%2520of%2520prior%250Are-meshing/conversion%2520techniques.%2520In%2520order%2520to%2520validate%2520our%2520approach%252C%2520we%2520have%250Aconsidered%2520a%2520semantic%2520classification%2520task.%2520The%2520experimental%2520results%2520obtained%2520on%250Athree%2520distinct%2520benchmark%2520datasets%2520show%2520that%2520the%2520proposed%2520approach%2520makes%2520it%250Apossible%2520to%2520achieve%2520equivalent%2520or%2520superior%2520classification%2520results%252C%2520while%250Aminimizing%2520the%2520related%2520memory%2520footprint%2520and%2520computational%2520load.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2501.03830v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=MeshConv3D%3A%20Efficient%20convolution%20and%20pooling%20operators%20for%20triangular%0A%20%203D%20meshes&entry.906535625=Germain%20Bregeon%20and%20Marius%20Preda%20and%20Radu%20Ispas%20and%20Titus%20Zaharia&entry.1292438233=%20%20Convolutional%20neural%20networks%20%28CNNs%29%20have%20been%20pivotal%20in%20various%202D%20image%0Aanalysis%20tasks%2C%20including%20computer%20vision%2C%20image%20indexing%20and%20retrieval%20or%0Asemantic%20classification.%20Extending%20CNNs%20to%203D%20data%20such%20as%20point%20clouds%20and%203D%0Ameshes%20raises%20significant%20challenges%20since%20the%20very%20basic%20convolution%20and%0Apooling%20operators%20need%20to%20be%20completely%20re-visited%20and%20re-defined%20in%20an%0Aappropriate%20manner%20to%20tackle%20irregular%20connectivity%20issues.%20In%20this%20paper%2C%20we%0Aintroduce%20MeshConv3D%2C%20a%203D%20mesh-dedicated%20methodology%20integrating%20specialized%0Aconvolution%20and%20face%20collapse-based%20pooling%20operators.%20MeshConv3D%20operates%0Adirectly%20on%20meshes%20of%20arbitrary%20topology%2C%20without%20any%20need%20of%20prior%0Are-meshing/conversion%20techniques.%20In%20order%20to%20validate%20our%20approach%2C%20we%20have%0Aconsidered%20a%20semantic%20classification%20task.%20The%20experimental%20results%20obtained%20on%0Athree%20distinct%20benchmark%20datasets%20show%20that%20the%20proposed%20approach%20makes%20it%0Apossible%20to%20achieve%20equivalent%20or%20superior%20classification%20results%2C%20while%0Aminimizing%20the%20related%20memory%20footprint%20and%20computational%20load.%0A&entry.1838667208=http%3A//arxiv.org/abs/2501.03830v1&entry.124074799=Read"},
{"title": "SLAM: Towards Efficient Multilingual Reasoning via Selective Language\n  Alignment", "author": "Yuchun Fan and Yongyu Mu and Yilin Wang and Lei Huang and Junhao Ruan and Bei Li and Tong Xiao and Shujian Huang and Xiaocheng Feng and Jingbo Zhu", "abstract": "  Despite the significant improvements achieved by large language models (LLMs)\nin English reasoning tasks, these models continue to struggle with multilingual\nreasoning. Recent studies leverage a full-parameter and two-stage training\nparadigm to teach models to first understand non-English questions and then\nreason. However, this method suffers from both substantial computational\nresource computing and catastrophic forgetting. The fundamental cause is that,\nwith the primary goal of enhancing multilingual comprehension, an excessive\nnumber of irrelevant layers and parameters are tuned during the first stage.\nGiven our findings that the representation learning of languages is merely\nconducted in lower-level layers, we propose an efficient multilingual reasoning\nalignment approach that precisely identifies and fine-tunes the layers\nresponsible for handling multilingualism. Experimental results show that our\nmethod, SLAM, only tunes 6 layers' feed-forward sub-layers including 6.5-8% of\nall parameters within 7B and 13B LLMs, achieving superior average performance\nthan all strong baselines across 10 languages. Meanwhile, SLAM only involves\none training stage, reducing training time by 4.1-11.9 compared to the\ntwo-stage method.\n", "link": "http://arxiv.org/abs/2501.03681v1", "date": "2025-01-07", "relevancy": 2.7194, "topK": [{"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5793}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5262}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5262}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20SLAM%3A%20Towards%20Efficient%20Multilingual%20Reasoning%20via%20Selective%20Language%0A%20%20Alignment&body=Title%3A%20SLAM%3A%20Towards%20Efficient%20Multilingual%20Reasoning%20via%20Selective%20Language%0A%20%20Alignment%0AAuthor%3A%20Yuchun%20Fan%20and%20Yongyu%20Mu%20and%20Yilin%20Wang%20and%20Lei%20Huang%20and%20Junhao%20Ruan%20and%20Bei%20Li%20and%20Tong%20Xiao%20and%20Shujian%20Huang%20and%20Xiaocheng%20Feng%20and%20Jingbo%20Zhu%0AAbstract%3A%20%20%20Despite%20the%20significant%20improvements%20achieved%20by%20large%20language%20models%20%28LLMs%29%0Ain%20English%20reasoning%20tasks%2C%20these%20models%20continue%20to%20struggle%20with%20multilingual%0Areasoning.%20Recent%20studies%20leverage%20a%20full-parameter%20and%20two-stage%20training%0Aparadigm%20to%20teach%20models%20to%20first%20understand%20non-English%20questions%20and%20then%0Areason.%20However%2C%20this%20method%20suffers%20from%20both%20substantial%20computational%0Aresource%20computing%20and%20catastrophic%20forgetting.%20The%20fundamental%20cause%20is%20that%2C%0Awith%20the%20primary%20goal%20of%20enhancing%20multilingual%20comprehension%2C%20an%20excessive%0Anumber%20of%20irrelevant%20layers%20and%20parameters%20are%20tuned%20during%20the%20first%20stage.%0AGiven%20our%20findings%20that%20the%20representation%20learning%20of%20languages%20is%20merely%0Aconducted%20in%20lower-level%20layers%2C%20we%20propose%20an%20efficient%20multilingual%20reasoning%0Aalignment%20approach%20that%20precisely%20identifies%20and%20fine-tunes%20the%20layers%0Aresponsible%20for%20handling%20multilingualism.%20Experimental%20results%20show%20that%20our%0Amethod%2C%20SLAM%2C%20only%20tunes%206%20layers%27%20feed-forward%20sub-layers%20including%206.5-8%25%20of%0Aall%20parameters%20within%207B%20and%2013B%20LLMs%2C%20achieving%20superior%20average%20performance%0Athan%20all%20strong%20baselines%20across%2010%20languages.%20Meanwhile%2C%20SLAM%20only%20involves%0Aone%20training%20stage%2C%20reducing%20training%20time%20by%204.1-11.9%20compared%20to%20the%0Atwo-stage%20method.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2501.03681v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DSLAM%253A%2520Towards%2520Efficient%2520Multilingual%2520Reasoning%2520via%2520Selective%2520Language%250A%2520%2520Alignment%26entry.906535625%3DYuchun%2520Fan%2520and%2520Yongyu%2520Mu%2520and%2520Yilin%2520Wang%2520and%2520Lei%2520Huang%2520and%2520Junhao%2520Ruan%2520and%2520Bei%2520Li%2520and%2520Tong%2520Xiao%2520and%2520Shujian%2520Huang%2520and%2520Xiaocheng%2520Feng%2520and%2520Jingbo%2520Zhu%26entry.1292438233%3D%2520%2520Despite%2520the%2520significant%2520improvements%2520achieved%2520by%2520large%2520language%2520models%2520%2528LLMs%2529%250Ain%2520English%2520reasoning%2520tasks%252C%2520these%2520models%2520continue%2520to%2520struggle%2520with%2520multilingual%250Areasoning.%2520Recent%2520studies%2520leverage%2520a%2520full-parameter%2520and%2520two-stage%2520training%250Aparadigm%2520to%2520teach%2520models%2520to%2520first%2520understand%2520non-English%2520questions%2520and%2520then%250Areason.%2520However%252C%2520this%2520method%2520suffers%2520from%2520both%2520substantial%2520computational%250Aresource%2520computing%2520and%2520catastrophic%2520forgetting.%2520The%2520fundamental%2520cause%2520is%2520that%252C%250Awith%2520the%2520primary%2520goal%2520of%2520enhancing%2520multilingual%2520comprehension%252C%2520an%2520excessive%250Anumber%2520of%2520irrelevant%2520layers%2520and%2520parameters%2520are%2520tuned%2520during%2520the%2520first%2520stage.%250AGiven%2520our%2520findings%2520that%2520the%2520representation%2520learning%2520of%2520languages%2520is%2520merely%250Aconducted%2520in%2520lower-level%2520layers%252C%2520we%2520propose%2520an%2520efficient%2520multilingual%2520reasoning%250Aalignment%2520approach%2520that%2520precisely%2520identifies%2520and%2520fine-tunes%2520the%2520layers%250Aresponsible%2520for%2520handling%2520multilingualism.%2520Experimental%2520results%2520show%2520that%2520our%250Amethod%252C%2520SLAM%252C%2520only%2520tunes%25206%2520layers%2527%2520feed-forward%2520sub-layers%2520including%25206.5-8%2525%2520of%250Aall%2520parameters%2520within%25207B%2520and%252013B%2520LLMs%252C%2520achieving%2520superior%2520average%2520performance%250Athan%2520all%2520strong%2520baselines%2520across%252010%2520languages.%2520Meanwhile%252C%2520SLAM%2520only%2520involves%250Aone%2520training%2520stage%252C%2520reducing%2520training%2520time%2520by%25204.1-11.9%2520compared%2520to%2520the%250Atwo-stage%2520method.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2501.03681v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=SLAM%3A%20Towards%20Efficient%20Multilingual%20Reasoning%20via%20Selective%20Language%0A%20%20Alignment&entry.906535625=Yuchun%20Fan%20and%20Yongyu%20Mu%20and%20Yilin%20Wang%20and%20Lei%20Huang%20and%20Junhao%20Ruan%20and%20Bei%20Li%20and%20Tong%20Xiao%20and%20Shujian%20Huang%20and%20Xiaocheng%20Feng%20and%20Jingbo%20Zhu&entry.1292438233=%20%20Despite%20the%20significant%20improvements%20achieved%20by%20large%20language%20models%20%28LLMs%29%0Ain%20English%20reasoning%20tasks%2C%20these%20models%20continue%20to%20struggle%20with%20multilingual%0Areasoning.%20Recent%20studies%20leverage%20a%20full-parameter%20and%20two-stage%20training%0Aparadigm%20to%20teach%20models%20to%20first%20understand%20non-English%20questions%20and%20then%0Areason.%20However%2C%20this%20method%20suffers%20from%20both%20substantial%20computational%0Aresource%20computing%20and%20catastrophic%20forgetting.%20The%20fundamental%20cause%20is%20that%2C%0Awith%20the%20primary%20goal%20of%20enhancing%20multilingual%20comprehension%2C%20an%20excessive%0Anumber%20of%20irrelevant%20layers%20and%20parameters%20are%20tuned%20during%20the%20first%20stage.%0AGiven%20our%20findings%20that%20the%20representation%20learning%20of%20languages%20is%20merely%0Aconducted%20in%20lower-level%20layers%2C%20we%20propose%20an%20efficient%20multilingual%20reasoning%0Aalignment%20approach%20that%20precisely%20identifies%20and%20fine-tunes%20the%20layers%0Aresponsible%20for%20handling%20multilingualism.%20Experimental%20results%20show%20that%20our%0Amethod%2C%20SLAM%2C%20only%20tunes%206%20layers%27%20feed-forward%20sub-layers%20including%206.5-8%25%20of%0Aall%20parameters%20within%207B%20and%2013B%20LLMs%2C%20achieving%20superior%20average%20performance%0Athan%20all%20strong%20baselines%20across%2010%20languages.%20Meanwhile%2C%20SLAM%20only%20involves%0Aone%20training%20stage%2C%20reducing%20training%20time%20by%204.1-11.9%20compared%20to%20the%0Atwo-stage%20method.%0A&entry.1838667208=http%3A//arxiv.org/abs/2501.03681v1&entry.124074799=Read"},
{"title": "Strip R-CNN: Large Strip Convolution for Remote Sensing Object Detection", "author": "Xinbin Yuan and ZhaoHui Zheng and Yuxuan Li and Xialei Liu and Li Liu and Xiang Li and Qibin Hou and Ming-Ming Cheng", "abstract": "  While witnessed with rapid development, remote sensing object detection\nremains challenging for detecting high aspect ratio objects. This paper shows\nthat large strip convolutions are good feature representation learners for\nremote sensing object detection and can detect objects of various aspect ratios\nwell. Based on large strip convolutions, we build a new network architecture\ncalled Strip R-CNN, which is simple, efficient, and powerful. Unlike recent\nremote sensing object detectors that leverage large-kernel convolutions with\nsquare shapes, our Strip R-CNN takes advantage of sequential orthogonal large\nstrip convolutions to capture spatial information. In addition, we enhance the\nlocalization capability of remote-sensing object detectors by decoupling the\ndetection heads and equipping the localization head with strip convolutions to\nbetter localize the target objects. Extensive experiments on several\nbenchmarks, e.g., DOTA, FAIR1M, HRSC2016, and DIOR, show that our Strip R-CNN\ncan largely improve previous works. Notably, our 30M model achieves 82.75% mAP\non DOTA-v1.0, setting a new state-of-the-art record.Code is available at\nhttps://github.com/YXB-NKU/Strip-R-CNN.\n", "link": "http://arxiv.org/abs/2501.03775v1", "date": "2025-01-07", "relevancy": 2.6833, "topK": [{"title": "RTG-SLAM: Real-time 3D Reconstruction at Scale using Gaussian Splatting", "link": "http://arxiv.org/abs/2404.19706v1", "similarity": 0.5566}, {"title": "Self-supervised Photographic Image Layout Representation Learning", "link": "http://arxiv.org/abs/2403.03740v1", "similarity": 0.5303}, {"title": "DDM-NET: End-to-end learning of keypoint feature Detection, Description and Matching for 3D localization", "link": "https://arxiv.org/abs/2212.04575", "similarity": 0.5231}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Strip%20R-CNN%3A%20Large%20Strip%20Convolution%20for%20Remote%20Sensing%20Object%20Detection&body=Title%3A%20Strip%20R-CNN%3A%20Large%20Strip%20Convolution%20for%20Remote%20Sensing%20Object%20Detection%0AAuthor%3A%20Xinbin%20Yuan%20and%20ZhaoHui%20Zheng%20and%20Yuxuan%20Li%20and%20Xialei%20Liu%20and%20Li%20Liu%20and%20Xiang%20Li%20and%20Qibin%20Hou%20and%20Ming-Ming%20Cheng%0AAbstract%3A%20%20%20While%20witnessed%20with%20rapid%20development%2C%20remote%20sensing%20object%20detection%0Aremains%20challenging%20for%20detecting%20high%20aspect%20ratio%20objects.%20This%20paper%20shows%0Athat%20large%20strip%20convolutions%20are%20good%20feature%20representation%20learners%20for%0Aremote%20sensing%20object%20detection%20and%20can%20detect%20objects%20of%20various%20aspect%20ratios%0Awell.%20Based%20on%20large%20strip%20convolutions%2C%20we%20build%20a%20new%20network%20architecture%0Acalled%20Strip%20R-CNN%2C%20which%20is%20simple%2C%20efficient%2C%20and%20powerful.%20Unlike%20recent%0Aremote%20sensing%20object%20detectors%20that%20leverage%20large-kernel%20convolutions%20with%0Asquare%20shapes%2C%20our%20Strip%20R-CNN%20takes%20advantage%20of%20sequential%20orthogonal%20large%0Astrip%20convolutions%20to%20capture%20spatial%20information.%20In%20addition%2C%20we%20enhance%20the%0Alocalization%20capability%20of%20remote-sensing%20object%20detectors%20by%20decoupling%20the%0Adetection%20heads%20and%20equipping%20the%20localization%20head%20with%20strip%20convolutions%20to%0Abetter%20localize%20the%20target%20objects.%20Extensive%20experiments%20on%20several%0Abenchmarks%2C%20e.g.%2C%20DOTA%2C%20FAIR1M%2C%20HRSC2016%2C%20and%20DIOR%2C%20show%20that%20our%20Strip%20R-CNN%0Acan%20largely%20improve%20previous%20works.%20Notably%2C%20our%2030M%20model%20achieves%2082.75%25%20mAP%0Aon%20DOTA-v1.0%2C%20setting%20a%20new%20state-of-the-art%20record.Code%20is%20available%20at%0Ahttps%3A//github.com/YXB-NKU/Strip-R-CNN.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2501.03775v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DStrip%2520R-CNN%253A%2520Large%2520Strip%2520Convolution%2520for%2520Remote%2520Sensing%2520Object%2520Detection%26entry.906535625%3DXinbin%2520Yuan%2520and%2520ZhaoHui%2520Zheng%2520and%2520Yuxuan%2520Li%2520and%2520Xialei%2520Liu%2520and%2520Li%2520Liu%2520and%2520Xiang%2520Li%2520and%2520Qibin%2520Hou%2520and%2520Ming-Ming%2520Cheng%26entry.1292438233%3D%2520%2520While%2520witnessed%2520with%2520rapid%2520development%252C%2520remote%2520sensing%2520object%2520detection%250Aremains%2520challenging%2520for%2520detecting%2520high%2520aspect%2520ratio%2520objects.%2520This%2520paper%2520shows%250Athat%2520large%2520strip%2520convolutions%2520are%2520good%2520feature%2520representation%2520learners%2520for%250Aremote%2520sensing%2520object%2520detection%2520and%2520can%2520detect%2520objects%2520of%2520various%2520aspect%2520ratios%250Awell.%2520Based%2520on%2520large%2520strip%2520convolutions%252C%2520we%2520build%2520a%2520new%2520network%2520architecture%250Acalled%2520Strip%2520R-CNN%252C%2520which%2520is%2520simple%252C%2520efficient%252C%2520and%2520powerful.%2520Unlike%2520recent%250Aremote%2520sensing%2520object%2520detectors%2520that%2520leverage%2520large-kernel%2520convolutions%2520with%250Asquare%2520shapes%252C%2520our%2520Strip%2520R-CNN%2520takes%2520advantage%2520of%2520sequential%2520orthogonal%2520large%250Astrip%2520convolutions%2520to%2520capture%2520spatial%2520information.%2520In%2520addition%252C%2520we%2520enhance%2520the%250Alocalization%2520capability%2520of%2520remote-sensing%2520object%2520detectors%2520by%2520decoupling%2520the%250Adetection%2520heads%2520and%2520equipping%2520the%2520localization%2520head%2520with%2520strip%2520convolutions%2520to%250Abetter%2520localize%2520the%2520target%2520objects.%2520Extensive%2520experiments%2520on%2520several%250Abenchmarks%252C%2520e.g.%252C%2520DOTA%252C%2520FAIR1M%252C%2520HRSC2016%252C%2520and%2520DIOR%252C%2520show%2520that%2520our%2520Strip%2520R-CNN%250Acan%2520largely%2520improve%2520previous%2520works.%2520Notably%252C%2520our%252030M%2520model%2520achieves%252082.75%2525%2520mAP%250Aon%2520DOTA-v1.0%252C%2520setting%2520a%2520new%2520state-of-the-art%2520record.Code%2520is%2520available%2520at%250Ahttps%253A//github.com/YXB-NKU/Strip-R-CNN.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2501.03775v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Strip%20R-CNN%3A%20Large%20Strip%20Convolution%20for%20Remote%20Sensing%20Object%20Detection&entry.906535625=Xinbin%20Yuan%20and%20ZhaoHui%20Zheng%20and%20Yuxuan%20Li%20and%20Xialei%20Liu%20and%20Li%20Liu%20and%20Xiang%20Li%20and%20Qibin%20Hou%20and%20Ming-Ming%20Cheng&entry.1292438233=%20%20While%20witnessed%20with%20rapid%20development%2C%20remote%20sensing%20object%20detection%0Aremains%20challenging%20for%20detecting%20high%20aspect%20ratio%20objects.%20This%20paper%20shows%0Athat%20large%20strip%20convolutions%20are%20good%20feature%20representation%20learners%20for%0Aremote%20sensing%20object%20detection%20and%20can%20detect%20objects%20of%20various%20aspect%20ratios%0Awell.%20Based%20on%20large%20strip%20convolutions%2C%20we%20build%20a%20new%20network%20architecture%0Acalled%20Strip%20R-CNN%2C%20which%20is%20simple%2C%20efficient%2C%20and%20powerful.%20Unlike%20recent%0Aremote%20sensing%20object%20detectors%20that%20leverage%20large-kernel%20convolutions%20with%0Asquare%20shapes%2C%20our%20Strip%20R-CNN%20takes%20advantage%20of%20sequential%20orthogonal%20large%0Astrip%20convolutions%20to%20capture%20spatial%20information.%20In%20addition%2C%20we%20enhance%20the%0Alocalization%20capability%20of%20remote-sensing%20object%20detectors%20by%20decoupling%20the%0Adetection%20heads%20and%20equipping%20the%20localization%20head%20with%20strip%20convolutions%20to%0Abetter%20localize%20the%20target%20objects.%20Extensive%20experiments%20on%20several%0Abenchmarks%2C%20e.g.%2C%20DOTA%2C%20FAIR1M%2C%20HRSC2016%2C%20and%20DIOR%2C%20show%20that%20our%20Strip%20R-CNN%0Acan%20largely%20improve%20previous%20works.%20Notably%2C%20our%2030M%20model%20achieves%2082.75%25%20mAP%0Aon%20DOTA-v1.0%2C%20setting%20a%20new%20state-of-the-art%20record.Code%20is%20available%20at%0Ahttps%3A//github.com/YXB-NKU/Strip-R-CNN.%0A&entry.1838667208=http%3A//arxiv.org/abs/2501.03775v1&entry.124074799=Read"},
{"title": "Learning the Language of Protein Structure", "author": "Benoit Gaujac and J\u00e9r\u00e9mie Don\u00e0 and Liviu Copoiu and Timothy Atkinson and Thomas Pierrot and Thomas D. Barrett", "abstract": "  Representation learning and \\emph{de novo} generation of proteins are pivotal\ncomputational biology tasks. Whilst natural language processing (NLP)\ntechniques have proven highly effective for protein sequence modelling,\nstructure modelling presents a complex challenge, primarily due to its\ncontinuous and three-dimensional nature. Motivated by this discrepancy, we\nintroduce an approach using a vector-quantized autoencoder that effectively\ntokenizes protein structures into discrete representations. This method\ntransforms the continuous, complex space of protein structures into a\nmanageable, discrete format with a codebook ranging from 4096 to 64000 tokens,\nachieving high-fidelity reconstructions with backbone root mean square\ndeviations (RMSD) of approximately 1-5 \\AA. To demonstrate the efficacy of our\nlearned representations, we show that a simple GPT model trained on our\ncodebooks can generate novel, diverse, and designable protein structures. Our\napproach not only provides representations of protein structure, but also\nmitigates the challenges of disparate modal representations and sets a\nfoundation for seamless, multi-modal integration, enhancing the capabilities of\ncomputational methods in protein design.\n", "link": "http://arxiv.org/abs/2405.15840v2", "date": "2025-01-07", "relevancy": 2.6787, "topK": [{"title": "Self-supervised Photographic Image Layout Representation Learning", "link": "http://arxiv.org/abs/2403.03740v1", "similarity": 0.5419}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5326}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5326}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Learning%20the%20Language%20of%20Protein%20Structure&body=Title%3A%20Learning%20the%20Language%20of%20Protein%20Structure%0AAuthor%3A%20Benoit%20Gaujac%20and%20J%C3%A9r%C3%A9mie%20Don%C3%A0%20and%20Liviu%20Copoiu%20and%20Timothy%20Atkinson%20and%20Thomas%20Pierrot%20and%20Thomas%20D.%20Barrett%0AAbstract%3A%20%20%20Representation%20learning%20and%20%5Cemph%7Bde%20novo%7D%20generation%20of%20proteins%20are%20pivotal%0Acomputational%20biology%20tasks.%20Whilst%20natural%20language%20processing%20%28NLP%29%0Atechniques%20have%20proven%20highly%20effective%20for%20protein%20sequence%20modelling%2C%0Astructure%20modelling%20presents%20a%20complex%20challenge%2C%20primarily%20due%20to%20its%0Acontinuous%20and%20three-dimensional%20nature.%20Motivated%20by%20this%20discrepancy%2C%20we%0Aintroduce%20an%20approach%20using%20a%20vector-quantized%20autoencoder%20that%20effectively%0Atokenizes%20protein%20structures%20into%20discrete%20representations.%20This%20method%0Atransforms%20the%20continuous%2C%20complex%20space%20of%20protein%20structures%20into%20a%0Amanageable%2C%20discrete%20format%20with%20a%20codebook%20ranging%20from%204096%20to%2064000%20tokens%2C%0Aachieving%20high-fidelity%20reconstructions%20with%20backbone%20root%20mean%20square%0Adeviations%20%28RMSD%29%20of%20approximately%201-5%20%5CAA.%20To%20demonstrate%20the%20efficacy%20of%20our%0Alearned%20representations%2C%20we%20show%20that%20a%20simple%20GPT%20model%20trained%20on%20our%0Acodebooks%20can%20generate%20novel%2C%20diverse%2C%20and%20designable%20protein%20structures.%20Our%0Aapproach%20not%20only%20provides%20representations%20of%20protein%20structure%2C%20but%20also%0Amitigates%20the%20challenges%20of%20disparate%20modal%20representations%20and%20sets%20a%0Afoundation%20for%20seamless%2C%20multi-modal%20integration%2C%20enhancing%20the%20capabilities%20of%0Acomputational%20methods%20in%20protein%20design.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2405.15840v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DLearning%2520the%2520Language%2520of%2520Protein%2520Structure%26entry.906535625%3DBenoit%2520Gaujac%2520and%2520J%25C3%25A9r%25C3%25A9mie%2520Don%25C3%25A0%2520and%2520Liviu%2520Copoiu%2520and%2520Timothy%2520Atkinson%2520and%2520Thomas%2520Pierrot%2520and%2520Thomas%2520D.%2520Barrett%26entry.1292438233%3D%2520%2520Representation%2520learning%2520and%2520%255Cemph%257Bde%2520novo%257D%2520generation%2520of%2520proteins%2520are%2520pivotal%250Acomputational%2520biology%2520tasks.%2520Whilst%2520natural%2520language%2520processing%2520%2528NLP%2529%250Atechniques%2520have%2520proven%2520highly%2520effective%2520for%2520protein%2520sequence%2520modelling%252C%250Astructure%2520modelling%2520presents%2520a%2520complex%2520challenge%252C%2520primarily%2520due%2520to%2520its%250Acontinuous%2520and%2520three-dimensional%2520nature.%2520Motivated%2520by%2520this%2520discrepancy%252C%2520we%250Aintroduce%2520an%2520approach%2520using%2520a%2520vector-quantized%2520autoencoder%2520that%2520effectively%250Atokenizes%2520protein%2520structures%2520into%2520discrete%2520representations.%2520This%2520method%250Atransforms%2520the%2520continuous%252C%2520complex%2520space%2520of%2520protein%2520structures%2520into%2520a%250Amanageable%252C%2520discrete%2520format%2520with%2520a%2520codebook%2520ranging%2520from%25204096%2520to%252064000%2520tokens%252C%250Aachieving%2520high-fidelity%2520reconstructions%2520with%2520backbone%2520root%2520mean%2520square%250Adeviations%2520%2528RMSD%2529%2520of%2520approximately%25201-5%2520%255CAA.%2520To%2520demonstrate%2520the%2520efficacy%2520of%2520our%250Alearned%2520representations%252C%2520we%2520show%2520that%2520a%2520simple%2520GPT%2520model%2520trained%2520on%2520our%250Acodebooks%2520can%2520generate%2520novel%252C%2520diverse%252C%2520and%2520designable%2520protein%2520structures.%2520Our%250Aapproach%2520not%2520only%2520provides%2520representations%2520of%2520protein%2520structure%252C%2520but%2520also%250Amitigates%2520the%2520challenges%2520of%2520disparate%2520modal%2520representations%2520and%2520sets%2520a%250Afoundation%2520for%2520seamless%252C%2520multi-modal%2520integration%252C%2520enhancing%2520the%2520capabilities%2520of%250Acomputational%2520methods%2520in%2520protein%2520design.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2405.15840v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Learning%20the%20Language%20of%20Protein%20Structure&entry.906535625=Benoit%20Gaujac%20and%20J%C3%A9r%C3%A9mie%20Don%C3%A0%20and%20Liviu%20Copoiu%20and%20Timothy%20Atkinson%20and%20Thomas%20Pierrot%20and%20Thomas%20D.%20Barrett&entry.1292438233=%20%20Representation%20learning%20and%20%5Cemph%7Bde%20novo%7D%20generation%20of%20proteins%20are%20pivotal%0Acomputational%20biology%20tasks.%20Whilst%20natural%20language%20processing%20%28NLP%29%0Atechniques%20have%20proven%20highly%20effective%20for%20protein%20sequence%20modelling%2C%0Astructure%20modelling%20presents%20a%20complex%20challenge%2C%20primarily%20due%20to%20its%0Acontinuous%20and%20three-dimensional%20nature.%20Motivated%20by%20this%20discrepancy%2C%20we%0Aintroduce%20an%20approach%20using%20a%20vector-quantized%20autoencoder%20that%20effectively%0Atokenizes%20protein%20structures%20into%20discrete%20representations.%20This%20method%0Atransforms%20the%20continuous%2C%20complex%20space%20of%20protein%20structures%20into%20a%0Amanageable%2C%20discrete%20format%20with%20a%20codebook%20ranging%20from%204096%20to%2064000%20tokens%2C%0Aachieving%20high-fidelity%20reconstructions%20with%20backbone%20root%20mean%20square%0Adeviations%20%28RMSD%29%20of%20approximately%201-5%20%5CAA.%20To%20demonstrate%20the%20efficacy%20of%20our%0Alearned%20representations%2C%20we%20show%20that%20a%20simple%20GPT%20model%20trained%20on%20our%0Acodebooks%20can%20generate%20novel%2C%20diverse%2C%20and%20designable%20protein%20structures.%20Our%0Aapproach%20not%20only%20provides%20representations%20of%20protein%20structure%2C%20but%20also%0Amitigates%20the%20challenges%20of%20disparate%20modal%20representations%20and%20sets%20a%0Afoundation%20for%20seamless%2C%20multi-modal%20integration%2C%20enhancing%20the%20capabilities%20of%0Acomputational%20methods%20in%20protein%20design.%0A&entry.1838667208=http%3A//arxiv.org/abs/2405.15840v2&entry.124074799=Read"},
{"title": "Vim-F: Visual State Space Model Benefiting from Learning in the\n  Frequency Domain", "author": "Juntao Zhang and Shaogeng Liu and Kun Bian and You Zhou and Pei Zhang and Wenbo An and Jun Zhou and Kun Shao", "abstract": "  In recent years, State Space Models (SSMs) with efficient hardware-aware\ndesigns, known as the Mamba deep learning models, have made significant\nprogress in modeling long sequences such as language understanding. Therefore,\nbuilding efficient and general-purpose visual backbones based on SSMs is a\npromising direction. Compared to traditional convolutional neural networks\n(CNNs) and Vision Transformers (ViTs), the performance of Vision Mamba (ViM)\nmethods is not yet fully competitive. To enable SSMs to process image data,\nViMs typically flatten 2D images into 1D sequences, inevitably ignoring some 2D\nlocal dependencies, thereby weakening the model's ability to interpret spatial\nrelationships from a global perspective. We use Fast Fourier Transform (FFT) to\nobtain the spectrum of the feature map and add it to the original feature map,\nenabling ViM to model a unified visual representation in both frequency and\nspatial domains. The introduction of frequency domain information enables ViM\nto have a global receptive field during scanning. We propose a novel model\ncalled Vim-F, which employs pure Mamba encoders and scans in both the frequency\nand spatial domains. Moreover, we question the necessity of position embedding\nin ViM and remove it accordingly in Vim-F, which helps to fully utilize the\nefficient long-sequence modeling capability of ViM. Finally, we redesign a\npatch embedding for Vim-F, leveraging a convolutional stem to capture more\nlocal correlations, further improving the performance of Vim-F. Code is\navailable at: \\url{https://github.com/yws-wxs/Vim-F}.\n", "link": "http://arxiv.org/abs/2405.18679v2", "date": "2025-01-07", "relevancy": 2.673, "topK": [{"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5431}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5304}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5304}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Vim-F%3A%20Visual%20State%20Space%20Model%20Benefiting%20from%20Learning%20in%20the%0A%20%20Frequency%20Domain&body=Title%3A%20Vim-F%3A%20Visual%20State%20Space%20Model%20Benefiting%20from%20Learning%20in%20the%0A%20%20Frequency%20Domain%0AAuthor%3A%20Juntao%20Zhang%20and%20Shaogeng%20Liu%20and%20Kun%20Bian%20and%20You%20Zhou%20and%20Pei%20Zhang%20and%20Wenbo%20An%20and%20Jun%20Zhou%20and%20Kun%20Shao%0AAbstract%3A%20%20%20In%20recent%20years%2C%20State%20Space%20Models%20%28SSMs%29%20with%20efficient%20hardware-aware%0Adesigns%2C%20known%20as%20the%20Mamba%20deep%20learning%20models%2C%20have%20made%20significant%0Aprogress%20in%20modeling%20long%20sequences%20such%20as%20language%20understanding.%20Therefore%2C%0Abuilding%20efficient%20and%20general-purpose%20visual%20backbones%20based%20on%20SSMs%20is%20a%0Apromising%20direction.%20Compared%20to%20traditional%20convolutional%20neural%20networks%0A%28CNNs%29%20and%20Vision%20Transformers%20%28ViTs%29%2C%20the%20performance%20of%20Vision%20Mamba%20%28ViM%29%0Amethods%20is%20not%20yet%20fully%20competitive.%20To%20enable%20SSMs%20to%20process%20image%20data%2C%0AViMs%20typically%20flatten%202D%20images%20into%201D%20sequences%2C%20inevitably%20ignoring%20some%202D%0Alocal%20dependencies%2C%20thereby%20weakening%20the%20model%27s%20ability%20to%20interpret%20spatial%0Arelationships%20from%20a%20global%20perspective.%20We%20use%20Fast%20Fourier%20Transform%20%28FFT%29%20to%0Aobtain%20the%20spectrum%20of%20the%20feature%20map%20and%20add%20it%20to%20the%20original%20feature%20map%2C%0Aenabling%20ViM%20to%20model%20a%20unified%20visual%20representation%20in%20both%20frequency%20and%0Aspatial%20domains.%20The%20introduction%20of%20frequency%20domain%20information%20enables%20ViM%0Ato%20have%20a%20global%20receptive%20field%20during%20scanning.%20We%20propose%20a%20novel%20model%0Acalled%20Vim-F%2C%20which%20employs%20pure%20Mamba%20encoders%20and%20scans%20in%20both%20the%20frequency%0Aand%20spatial%20domains.%20Moreover%2C%20we%20question%20the%20necessity%20of%20position%20embedding%0Ain%20ViM%20and%20remove%20it%20accordingly%20in%20Vim-F%2C%20which%20helps%20to%20fully%20utilize%20the%0Aefficient%20long-sequence%20modeling%20capability%20of%20ViM.%20Finally%2C%20we%20redesign%20a%0Apatch%20embedding%20for%20Vim-F%2C%20leveraging%20a%20convolutional%20stem%20to%20capture%20more%0Alocal%20correlations%2C%20further%20improving%20the%20performance%20of%20Vim-F.%20Code%20is%0Aavailable%20at%3A%20%5Curl%7Bhttps%3A//github.com/yws-wxs/Vim-F%7D.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2405.18679v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DVim-F%253A%2520Visual%2520State%2520Space%2520Model%2520Benefiting%2520from%2520Learning%2520in%2520the%250A%2520%2520Frequency%2520Domain%26entry.906535625%3DJuntao%2520Zhang%2520and%2520Shaogeng%2520Liu%2520and%2520Kun%2520Bian%2520and%2520You%2520Zhou%2520and%2520Pei%2520Zhang%2520and%2520Wenbo%2520An%2520and%2520Jun%2520Zhou%2520and%2520Kun%2520Shao%26entry.1292438233%3D%2520%2520In%2520recent%2520years%252C%2520State%2520Space%2520Models%2520%2528SSMs%2529%2520with%2520efficient%2520hardware-aware%250Adesigns%252C%2520known%2520as%2520the%2520Mamba%2520deep%2520learning%2520models%252C%2520have%2520made%2520significant%250Aprogress%2520in%2520modeling%2520long%2520sequences%2520such%2520as%2520language%2520understanding.%2520Therefore%252C%250Abuilding%2520efficient%2520and%2520general-purpose%2520visual%2520backbones%2520based%2520on%2520SSMs%2520is%2520a%250Apromising%2520direction.%2520Compared%2520to%2520traditional%2520convolutional%2520neural%2520networks%250A%2528CNNs%2529%2520and%2520Vision%2520Transformers%2520%2528ViTs%2529%252C%2520the%2520performance%2520of%2520Vision%2520Mamba%2520%2528ViM%2529%250Amethods%2520is%2520not%2520yet%2520fully%2520competitive.%2520To%2520enable%2520SSMs%2520to%2520process%2520image%2520data%252C%250AViMs%2520typically%2520flatten%25202D%2520images%2520into%25201D%2520sequences%252C%2520inevitably%2520ignoring%2520some%25202D%250Alocal%2520dependencies%252C%2520thereby%2520weakening%2520the%2520model%2527s%2520ability%2520to%2520interpret%2520spatial%250Arelationships%2520from%2520a%2520global%2520perspective.%2520We%2520use%2520Fast%2520Fourier%2520Transform%2520%2528FFT%2529%2520to%250Aobtain%2520the%2520spectrum%2520of%2520the%2520feature%2520map%2520and%2520add%2520it%2520to%2520the%2520original%2520feature%2520map%252C%250Aenabling%2520ViM%2520to%2520model%2520a%2520unified%2520visual%2520representation%2520in%2520both%2520frequency%2520and%250Aspatial%2520domains.%2520The%2520introduction%2520of%2520frequency%2520domain%2520information%2520enables%2520ViM%250Ato%2520have%2520a%2520global%2520receptive%2520field%2520during%2520scanning.%2520We%2520propose%2520a%2520novel%2520model%250Acalled%2520Vim-F%252C%2520which%2520employs%2520pure%2520Mamba%2520encoders%2520and%2520scans%2520in%2520both%2520the%2520frequency%250Aand%2520spatial%2520domains.%2520Moreover%252C%2520we%2520question%2520the%2520necessity%2520of%2520position%2520embedding%250Ain%2520ViM%2520and%2520remove%2520it%2520accordingly%2520in%2520Vim-F%252C%2520which%2520helps%2520to%2520fully%2520utilize%2520the%250Aefficient%2520long-sequence%2520modeling%2520capability%2520of%2520ViM.%2520Finally%252C%2520we%2520redesign%2520a%250Apatch%2520embedding%2520for%2520Vim-F%252C%2520leveraging%2520a%2520convolutional%2520stem%2520to%2520capture%2520more%250Alocal%2520correlations%252C%2520further%2520improving%2520the%2520performance%2520of%2520Vim-F.%2520Code%2520is%250Aavailable%2520at%253A%2520%255Curl%257Bhttps%253A//github.com/yws-wxs/Vim-F%257D.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2405.18679v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Vim-F%3A%20Visual%20State%20Space%20Model%20Benefiting%20from%20Learning%20in%20the%0A%20%20Frequency%20Domain&entry.906535625=Juntao%20Zhang%20and%20Shaogeng%20Liu%20and%20Kun%20Bian%20and%20You%20Zhou%20and%20Pei%20Zhang%20and%20Wenbo%20An%20and%20Jun%20Zhou%20and%20Kun%20Shao&entry.1292438233=%20%20In%20recent%20years%2C%20State%20Space%20Models%20%28SSMs%29%20with%20efficient%20hardware-aware%0Adesigns%2C%20known%20as%20the%20Mamba%20deep%20learning%20models%2C%20have%20made%20significant%0Aprogress%20in%20modeling%20long%20sequences%20such%20as%20language%20understanding.%20Therefore%2C%0Abuilding%20efficient%20and%20general-purpose%20visual%20backbones%20based%20on%20SSMs%20is%20a%0Apromising%20direction.%20Compared%20to%20traditional%20convolutional%20neural%20networks%0A%28CNNs%29%20and%20Vision%20Transformers%20%28ViTs%29%2C%20the%20performance%20of%20Vision%20Mamba%20%28ViM%29%0Amethods%20is%20not%20yet%20fully%20competitive.%20To%20enable%20SSMs%20to%20process%20image%20data%2C%0AViMs%20typically%20flatten%202D%20images%20into%201D%20sequences%2C%20inevitably%20ignoring%20some%202D%0Alocal%20dependencies%2C%20thereby%20weakening%20the%20model%27s%20ability%20to%20interpret%20spatial%0Arelationships%20from%20a%20global%20perspective.%20We%20use%20Fast%20Fourier%20Transform%20%28FFT%29%20to%0Aobtain%20the%20spectrum%20of%20the%20feature%20map%20and%20add%20it%20to%20the%20original%20feature%20map%2C%0Aenabling%20ViM%20to%20model%20a%20unified%20visual%20representation%20in%20both%20frequency%20and%0Aspatial%20domains.%20The%20introduction%20of%20frequency%20domain%20information%20enables%20ViM%0Ato%20have%20a%20global%20receptive%20field%20during%20scanning.%20We%20propose%20a%20novel%20model%0Acalled%20Vim-F%2C%20which%20employs%20pure%20Mamba%20encoders%20and%20scans%20in%20both%20the%20frequency%0Aand%20spatial%20domains.%20Moreover%2C%20we%20question%20the%20necessity%20of%20position%20embedding%0Ain%20ViM%20and%20remove%20it%20accordingly%20in%20Vim-F%2C%20which%20helps%20to%20fully%20utilize%20the%0Aefficient%20long-sequence%20modeling%20capability%20of%20ViM.%20Finally%2C%20we%20redesign%20a%0Apatch%20embedding%20for%20Vim-F%2C%20leveraging%20a%20convolutional%20stem%20to%20capture%20more%0Alocal%20correlations%2C%20further%20improving%20the%20performance%20of%20Vim-F.%20Code%20is%0Aavailable%20at%3A%20%5Curl%7Bhttps%3A//github.com/yws-wxs/Vim-F%7D.%0A&entry.1838667208=http%3A//arxiv.org/abs/2405.18679v2&entry.124074799=Read"},
{"title": "Context-Alignment: Activating and Enhancing LLM Capabilities in Time\n  Series", "author": "Yuxiao Hu and Qian Li and Dongxiao Zhang and Jinyue Yan and Yuntian Chen", "abstract": "  Recently, leveraging pre-trained Large Language Models (LLMs) for time series\n(TS) tasks has gained increasing attention, which involves activating and\nenhancing LLMs' capabilities. Many methods aim to activate LLMs' capabilities\nbased on token-level alignment but overlook LLMs' inherent strength on natural\nlanguage processing -- their deep understanding of linguistic logic and\nstructure rather than superficial embedding processing. We propose\nContext-Alignment, a new paradigm that aligns TS with a linguistic component in\nthe language environments familiar to LLMs to enable LLMs to contextualize and\ncomprehend TS data, thereby activating their capabilities. Specifically, such\ncontext-level alignment comprises structural alignment and logical alignment,\nwhich is achieved by a Dual-Scale Context-Alignment GNNs (DSCA-GNNs) applied to\nTS-language multimodal inputs. Structural alignment utilizes dual-scale nodes\nto describe hierarchical structure in TS-language, enabling LLMs treat long TS\ndata as a whole linguistic component while preserving intrinsic token features.\nLogical alignment uses directed edges to guide logical relationships, ensuring\ncoherence in the contextual semantics. Demonstration examples prompt are\nemployed to construct Demonstration Examples based Context-Alignment (DECA)\nfollowing DSCA-GNNs framework. DECA can be flexibly and repeatedly integrated\ninto various layers of pre-trained LLMs to improve awareness of logic and\nstructure, thereby enhancing performance. Extensive experiments show the\neffectiveness of DECA and the importance of Context-Alignment across tasks,\nparticularly in few-shot and zero-shot forecasting, confirming that\nContext-Alignment provide powerful prior knowledge on context.\n", "link": "http://arxiv.org/abs/2501.03747v1", "date": "2025-01-07", "relevancy": 2.6408, "topK": [{"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5379}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5233}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5233}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Context-Alignment%3A%20Activating%20and%20Enhancing%20LLM%20Capabilities%20in%20Time%0A%20%20Series&body=Title%3A%20Context-Alignment%3A%20Activating%20and%20Enhancing%20LLM%20Capabilities%20in%20Time%0A%20%20Series%0AAuthor%3A%20Yuxiao%20Hu%20and%20Qian%20Li%20and%20Dongxiao%20Zhang%20and%20Jinyue%20Yan%20and%20Yuntian%20Chen%0AAbstract%3A%20%20%20Recently%2C%20leveraging%20pre-trained%20Large%20Language%20Models%20%28LLMs%29%20for%20time%20series%0A%28TS%29%20tasks%20has%20gained%20increasing%20attention%2C%20which%20involves%20activating%20and%0Aenhancing%20LLMs%27%20capabilities.%20Many%20methods%20aim%20to%20activate%20LLMs%27%20capabilities%0Abased%20on%20token-level%20alignment%20but%20overlook%20LLMs%27%20inherent%20strength%20on%20natural%0Alanguage%20processing%20--%20their%20deep%20understanding%20of%20linguistic%20logic%20and%0Astructure%20rather%20than%20superficial%20embedding%20processing.%20We%20propose%0AContext-Alignment%2C%20a%20new%20paradigm%20that%20aligns%20TS%20with%20a%20linguistic%20component%20in%0Athe%20language%20environments%20familiar%20to%20LLMs%20to%20enable%20LLMs%20to%20contextualize%20and%0Acomprehend%20TS%20data%2C%20thereby%20activating%20their%20capabilities.%20Specifically%2C%20such%0Acontext-level%20alignment%20comprises%20structural%20alignment%20and%20logical%20alignment%2C%0Awhich%20is%20achieved%20by%20a%20Dual-Scale%20Context-Alignment%20GNNs%20%28DSCA-GNNs%29%20applied%20to%0ATS-language%20multimodal%20inputs.%20Structural%20alignment%20utilizes%20dual-scale%20nodes%0Ato%20describe%20hierarchical%20structure%20in%20TS-language%2C%20enabling%20LLMs%20treat%20long%20TS%0Adata%20as%20a%20whole%20linguistic%20component%20while%20preserving%20intrinsic%20token%20features.%0ALogical%20alignment%20uses%20directed%20edges%20to%20guide%20logical%20relationships%2C%20ensuring%0Acoherence%20in%20the%20contextual%20semantics.%20Demonstration%20examples%20prompt%20are%0Aemployed%20to%20construct%20Demonstration%20Examples%20based%20Context-Alignment%20%28DECA%29%0Afollowing%20DSCA-GNNs%20framework.%20DECA%20can%20be%20flexibly%20and%20repeatedly%20integrated%0Ainto%20various%20layers%20of%20pre-trained%20LLMs%20to%20improve%20awareness%20of%20logic%20and%0Astructure%2C%20thereby%20enhancing%20performance.%20Extensive%20experiments%20show%20the%0Aeffectiveness%20of%20DECA%20and%20the%20importance%20of%20Context-Alignment%20across%20tasks%2C%0Aparticularly%20in%20few-shot%20and%20zero-shot%20forecasting%2C%20confirming%20that%0AContext-Alignment%20provide%20powerful%20prior%20knowledge%20on%20context.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2501.03747v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DContext-Alignment%253A%2520Activating%2520and%2520Enhancing%2520LLM%2520Capabilities%2520in%2520Time%250A%2520%2520Series%26entry.906535625%3DYuxiao%2520Hu%2520and%2520Qian%2520Li%2520and%2520Dongxiao%2520Zhang%2520and%2520Jinyue%2520Yan%2520and%2520Yuntian%2520Chen%26entry.1292438233%3D%2520%2520Recently%252C%2520leveraging%2520pre-trained%2520Large%2520Language%2520Models%2520%2528LLMs%2529%2520for%2520time%2520series%250A%2528TS%2529%2520tasks%2520has%2520gained%2520increasing%2520attention%252C%2520which%2520involves%2520activating%2520and%250Aenhancing%2520LLMs%2527%2520capabilities.%2520Many%2520methods%2520aim%2520to%2520activate%2520LLMs%2527%2520capabilities%250Abased%2520on%2520token-level%2520alignment%2520but%2520overlook%2520LLMs%2527%2520inherent%2520strength%2520on%2520natural%250Alanguage%2520processing%2520--%2520their%2520deep%2520understanding%2520of%2520linguistic%2520logic%2520and%250Astructure%2520rather%2520than%2520superficial%2520embedding%2520processing.%2520We%2520propose%250AContext-Alignment%252C%2520a%2520new%2520paradigm%2520that%2520aligns%2520TS%2520with%2520a%2520linguistic%2520component%2520in%250Athe%2520language%2520environments%2520familiar%2520to%2520LLMs%2520to%2520enable%2520LLMs%2520to%2520contextualize%2520and%250Acomprehend%2520TS%2520data%252C%2520thereby%2520activating%2520their%2520capabilities.%2520Specifically%252C%2520such%250Acontext-level%2520alignment%2520comprises%2520structural%2520alignment%2520and%2520logical%2520alignment%252C%250Awhich%2520is%2520achieved%2520by%2520a%2520Dual-Scale%2520Context-Alignment%2520GNNs%2520%2528DSCA-GNNs%2529%2520applied%2520to%250ATS-language%2520multimodal%2520inputs.%2520Structural%2520alignment%2520utilizes%2520dual-scale%2520nodes%250Ato%2520describe%2520hierarchical%2520structure%2520in%2520TS-language%252C%2520enabling%2520LLMs%2520treat%2520long%2520TS%250Adata%2520as%2520a%2520whole%2520linguistic%2520component%2520while%2520preserving%2520intrinsic%2520token%2520features.%250ALogical%2520alignment%2520uses%2520directed%2520edges%2520to%2520guide%2520logical%2520relationships%252C%2520ensuring%250Acoherence%2520in%2520the%2520contextual%2520semantics.%2520Demonstration%2520examples%2520prompt%2520are%250Aemployed%2520to%2520construct%2520Demonstration%2520Examples%2520based%2520Context-Alignment%2520%2528DECA%2529%250Afollowing%2520DSCA-GNNs%2520framework.%2520DECA%2520can%2520be%2520flexibly%2520and%2520repeatedly%2520integrated%250Ainto%2520various%2520layers%2520of%2520pre-trained%2520LLMs%2520to%2520improve%2520awareness%2520of%2520logic%2520and%250Astructure%252C%2520thereby%2520enhancing%2520performance.%2520Extensive%2520experiments%2520show%2520the%250Aeffectiveness%2520of%2520DECA%2520and%2520the%2520importance%2520of%2520Context-Alignment%2520across%2520tasks%252C%250Aparticularly%2520in%2520few-shot%2520and%2520zero-shot%2520forecasting%252C%2520confirming%2520that%250AContext-Alignment%2520provide%2520powerful%2520prior%2520knowledge%2520on%2520context.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2501.03747v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Context-Alignment%3A%20Activating%20and%20Enhancing%20LLM%20Capabilities%20in%20Time%0A%20%20Series&entry.906535625=Yuxiao%20Hu%20and%20Qian%20Li%20and%20Dongxiao%20Zhang%20and%20Jinyue%20Yan%20and%20Yuntian%20Chen&entry.1292438233=%20%20Recently%2C%20leveraging%20pre-trained%20Large%20Language%20Models%20%28LLMs%29%20for%20time%20series%0A%28TS%29%20tasks%20has%20gained%20increasing%20attention%2C%20which%20involves%20activating%20and%0Aenhancing%20LLMs%27%20capabilities.%20Many%20methods%20aim%20to%20activate%20LLMs%27%20capabilities%0Abased%20on%20token-level%20alignment%20but%20overlook%20LLMs%27%20inherent%20strength%20on%20natural%0Alanguage%20processing%20--%20their%20deep%20understanding%20of%20linguistic%20logic%20and%0Astructure%20rather%20than%20superficial%20embedding%20processing.%20We%20propose%0AContext-Alignment%2C%20a%20new%20paradigm%20that%20aligns%20TS%20with%20a%20linguistic%20component%20in%0Athe%20language%20environments%20familiar%20to%20LLMs%20to%20enable%20LLMs%20to%20contextualize%20and%0Acomprehend%20TS%20data%2C%20thereby%20activating%20their%20capabilities.%20Specifically%2C%20such%0Acontext-level%20alignment%20comprises%20structural%20alignment%20and%20logical%20alignment%2C%0Awhich%20is%20achieved%20by%20a%20Dual-Scale%20Context-Alignment%20GNNs%20%28DSCA-GNNs%29%20applied%20to%0ATS-language%20multimodal%20inputs.%20Structural%20alignment%20utilizes%20dual-scale%20nodes%0Ato%20describe%20hierarchical%20structure%20in%20TS-language%2C%20enabling%20LLMs%20treat%20long%20TS%0Adata%20as%20a%20whole%20linguistic%20component%20while%20preserving%20intrinsic%20token%20features.%0ALogical%20alignment%20uses%20directed%20edges%20to%20guide%20logical%20relationships%2C%20ensuring%0Acoherence%20in%20the%20contextual%20semantics.%20Demonstration%20examples%20prompt%20are%0Aemployed%20to%20construct%20Demonstration%20Examples%20based%20Context-Alignment%20%28DECA%29%0Afollowing%20DSCA-GNNs%20framework.%20DECA%20can%20be%20flexibly%20and%20repeatedly%20integrated%0Ainto%20various%20layers%20of%20pre-trained%20LLMs%20to%20improve%20awareness%20of%20logic%20and%0Astructure%2C%20thereby%20enhancing%20performance.%20Extensive%20experiments%20show%20the%0Aeffectiveness%20of%20DECA%20and%20the%20importance%20of%20Context-Alignment%20across%20tasks%2C%0Aparticularly%20in%20few-shot%20and%20zero-shot%20forecasting%2C%20confirming%20that%0AContext-Alignment%20provide%20powerful%20prior%20knowledge%20on%20context.%0A&entry.1838667208=http%3A//arxiv.org/abs/2501.03747v1&entry.124074799=Read"},
{"title": "SMIR: Efficient Synthetic Data Pipeline To Improve Multi-Image Reasoning", "author": "Andrew Li and Rahul Thapa and Rahul Chalamala and Qingyang Wu and Kezhen Chen and James Zou", "abstract": "  Vision-Language Models (VLMs) have shown strong performance in understanding\nsingle images, aided by numerous high-quality instruction datasets. However,\nmulti-image reasoning tasks are still under-explored in the open-source\ncommunity due to two main challenges: (1) scaling datasets with multiple\ncorrelated images and complex reasoning instructions is resource-intensive and\nmaintaining quality is difficult, and (2) there is a lack of robust evaluation\nbenchmarks for multi-image tasks. To address these issues, we introduce SMIR,\nan efficient synthetic data-generation pipeline for multi-image reasoning, and\na high-quality dataset generated using this pipeline. Our pipeline efficiently\nextracts highly correlated images using multimodal embeddings, combining visual\nand descriptive information and leverages open-source LLMs to generate quality\ninstructions. Using this pipeline, we generated 160K synthetic training\nsamples, offering a cost-effective alternative to expensive closed-source\nsolutions. Additionally, we present SMIR-BENCH, a novel multi-image reasoning\nevaluation benchmark comprising 200 diverse examples across 7 complex\nmulti-image reasoning tasks. SMIR-BENCH is multi-turn and utilizes a VLM judge\nto evaluate free-form responses, providing a comprehensive assessment of model\nexpressiveness and reasoning capability across modalities. We demonstrate the\neffectiveness of SMIR dataset by fine-tuning several open-source VLMs and\nevaluating their performance on SMIR-BENCH. Our results show that models\ntrained on our dataset outperform baseline models in multi-image reasoning\ntasks up to 8% with a much more scalable data pipeline.\n", "link": "http://arxiv.org/abs/2501.03675v1", "date": "2025-01-07", "relevancy": 2.6378, "topK": [{"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5368}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5229}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5229}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20SMIR%3A%20Efficient%20Synthetic%20Data%20Pipeline%20To%20Improve%20Multi-Image%20Reasoning&body=Title%3A%20SMIR%3A%20Efficient%20Synthetic%20Data%20Pipeline%20To%20Improve%20Multi-Image%20Reasoning%0AAuthor%3A%20Andrew%20Li%20and%20Rahul%20Thapa%20and%20Rahul%20Chalamala%20and%20Qingyang%20Wu%20and%20Kezhen%20Chen%20and%20James%20Zou%0AAbstract%3A%20%20%20Vision-Language%20Models%20%28VLMs%29%20have%20shown%20strong%20performance%20in%20understanding%0Asingle%20images%2C%20aided%20by%20numerous%20high-quality%20instruction%20datasets.%20However%2C%0Amulti-image%20reasoning%20tasks%20are%20still%20under-explored%20in%20the%20open-source%0Acommunity%20due%20to%20two%20main%20challenges%3A%20%281%29%20scaling%20datasets%20with%20multiple%0Acorrelated%20images%20and%20complex%20reasoning%20instructions%20is%20resource-intensive%20and%0Amaintaining%20quality%20is%20difficult%2C%20and%20%282%29%20there%20is%20a%20lack%20of%20robust%20evaluation%0Abenchmarks%20for%20multi-image%20tasks.%20To%20address%20these%20issues%2C%20we%20introduce%20SMIR%2C%0Aan%20efficient%20synthetic%20data-generation%20pipeline%20for%20multi-image%20reasoning%2C%20and%0Aa%20high-quality%20dataset%20generated%20using%20this%20pipeline.%20Our%20pipeline%20efficiently%0Aextracts%20highly%20correlated%20images%20using%20multimodal%20embeddings%2C%20combining%20visual%0Aand%20descriptive%20information%20and%20leverages%20open-source%20LLMs%20to%20generate%20quality%0Ainstructions.%20Using%20this%20pipeline%2C%20we%20generated%20160K%20synthetic%20training%0Asamples%2C%20offering%20a%20cost-effective%20alternative%20to%20expensive%20closed-source%0Asolutions.%20Additionally%2C%20we%20present%20SMIR-BENCH%2C%20a%20novel%20multi-image%20reasoning%0Aevaluation%20benchmark%20comprising%20200%20diverse%20examples%20across%207%20complex%0Amulti-image%20reasoning%20tasks.%20SMIR-BENCH%20is%20multi-turn%20and%20utilizes%20a%20VLM%20judge%0Ato%20evaluate%20free-form%20responses%2C%20providing%20a%20comprehensive%20assessment%20of%20model%0Aexpressiveness%20and%20reasoning%20capability%20across%20modalities.%20We%20demonstrate%20the%0Aeffectiveness%20of%20SMIR%20dataset%20by%20fine-tuning%20several%20open-source%20VLMs%20and%0Aevaluating%20their%20performance%20on%20SMIR-BENCH.%20Our%20results%20show%20that%20models%0Atrained%20on%20our%20dataset%20outperform%20baseline%20models%20in%20multi-image%20reasoning%0Atasks%20up%20to%208%25%20with%20a%20much%20more%20scalable%20data%20pipeline.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2501.03675v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DSMIR%253A%2520Efficient%2520Synthetic%2520Data%2520Pipeline%2520To%2520Improve%2520Multi-Image%2520Reasoning%26entry.906535625%3DAndrew%2520Li%2520and%2520Rahul%2520Thapa%2520and%2520Rahul%2520Chalamala%2520and%2520Qingyang%2520Wu%2520and%2520Kezhen%2520Chen%2520and%2520James%2520Zou%26entry.1292438233%3D%2520%2520Vision-Language%2520Models%2520%2528VLMs%2529%2520have%2520shown%2520strong%2520performance%2520in%2520understanding%250Asingle%2520images%252C%2520aided%2520by%2520numerous%2520high-quality%2520instruction%2520datasets.%2520However%252C%250Amulti-image%2520reasoning%2520tasks%2520are%2520still%2520under-explored%2520in%2520the%2520open-source%250Acommunity%2520due%2520to%2520two%2520main%2520challenges%253A%2520%25281%2529%2520scaling%2520datasets%2520with%2520multiple%250Acorrelated%2520images%2520and%2520complex%2520reasoning%2520instructions%2520is%2520resource-intensive%2520and%250Amaintaining%2520quality%2520is%2520difficult%252C%2520and%2520%25282%2529%2520there%2520is%2520a%2520lack%2520of%2520robust%2520evaluation%250Abenchmarks%2520for%2520multi-image%2520tasks.%2520To%2520address%2520these%2520issues%252C%2520we%2520introduce%2520SMIR%252C%250Aan%2520efficient%2520synthetic%2520data-generation%2520pipeline%2520for%2520multi-image%2520reasoning%252C%2520and%250Aa%2520high-quality%2520dataset%2520generated%2520using%2520this%2520pipeline.%2520Our%2520pipeline%2520efficiently%250Aextracts%2520highly%2520correlated%2520images%2520using%2520multimodal%2520embeddings%252C%2520combining%2520visual%250Aand%2520descriptive%2520information%2520and%2520leverages%2520open-source%2520LLMs%2520to%2520generate%2520quality%250Ainstructions.%2520Using%2520this%2520pipeline%252C%2520we%2520generated%2520160K%2520synthetic%2520training%250Asamples%252C%2520offering%2520a%2520cost-effective%2520alternative%2520to%2520expensive%2520closed-source%250Asolutions.%2520Additionally%252C%2520we%2520present%2520SMIR-BENCH%252C%2520a%2520novel%2520multi-image%2520reasoning%250Aevaluation%2520benchmark%2520comprising%2520200%2520diverse%2520examples%2520across%25207%2520complex%250Amulti-image%2520reasoning%2520tasks.%2520SMIR-BENCH%2520is%2520multi-turn%2520and%2520utilizes%2520a%2520VLM%2520judge%250Ato%2520evaluate%2520free-form%2520responses%252C%2520providing%2520a%2520comprehensive%2520assessment%2520of%2520model%250Aexpressiveness%2520and%2520reasoning%2520capability%2520across%2520modalities.%2520We%2520demonstrate%2520the%250Aeffectiveness%2520of%2520SMIR%2520dataset%2520by%2520fine-tuning%2520several%2520open-source%2520VLMs%2520and%250Aevaluating%2520their%2520performance%2520on%2520SMIR-BENCH.%2520Our%2520results%2520show%2520that%2520models%250Atrained%2520on%2520our%2520dataset%2520outperform%2520baseline%2520models%2520in%2520multi-image%2520reasoning%250Atasks%2520up%2520to%25208%2525%2520with%2520a%2520much%2520more%2520scalable%2520data%2520pipeline.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2501.03675v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=SMIR%3A%20Efficient%20Synthetic%20Data%20Pipeline%20To%20Improve%20Multi-Image%20Reasoning&entry.906535625=Andrew%20Li%20and%20Rahul%20Thapa%20and%20Rahul%20Chalamala%20and%20Qingyang%20Wu%20and%20Kezhen%20Chen%20and%20James%20Zou&entry.1292438233=%20%20Vision-Language%20Models%20%28VLMs%29%20have%20shown%20strong%20performance%20in%20understanding%0Asingle%20images%2C%20aided%20by%20numerous%20high-quality%20instruction%20datasets.%20However%2C%0Amulti-image%20reasoning%20tasks%20are%20still%20under-explored%20in%20the%20open-source%0Acommunity%20due%20to%20two%20main%20challenges%3A%20%281%29%20scaling%20datasets%20with%20multiple%0Acorrelated%20images%20and%20complex%20reasoning%20instructions%20is%20resource-intensive%20and%0Amaintaining%20quality%20is%20difficult%2C%20and%20%282%29%20there%20is%20a%20lack%20of%20robust%20evaluation%0Abenchmarks%20for%20multi-image%20tasks.%20To%20address%20these%20issues%2C%20we%20introduce%20SMIR%2C%0Aan%20efficient%20synthetic%20data-generation%20pipeline%20for%20multi-image%20reasoning%2C%20and%0Aa%20high-quality%20dataset%20generated%20using%20this%20pipeline.%20Our%20pipeline%20efficiently%0Aextracts%20highly%20correlated%20images%20using%20multimodal%20embeddings%2C%20combining%20visual%0Aand%20descriptive%20information%20and%20leverages%20open-source%20LLMs%20to%20generate%20quality%0Ainstructions.%20Using%20this%20pipeline%2C%20we%20generated%20160K%20synthetic%20training%0Asamples%2C%20offering%20a%20cost-effective%20alternative%20to%20expensive%20closed-source%0Asolutions.%20Additionally%2C%20we%20present%20SMIR-BENCH%2C%20a%20novel%20multi-image%20reasoning%0Aevaluation%20benchmark%20comprising%20200%20diverse%20examples%20across%207%20complex%0Amulti-image%20reasoning%20tasks.%20SMIR-BENCH%20is%20multi-turn%20and%20utilizes%20a%20VLM%20judge%0Ato%20evaluate%20free-form%20responses%2C%20providing%20a%20comprehensive%20assessment%20of%20model%0Aexpressiveness%20and%20reasoning%20capability%20across%20modalities.%20We%20demonstrate%20the%0Aeffectiveness%20of%20SMIR%20dataset%20by%20fine-tuning%20several%20open-source%20VLMs%20and%0Aevaluating%20their%20performance%20on%20SMIR-BENCH.%20Our%20results%20show%20that%20models%0Atrained%20on%20our%20dataset%20outperform%20baseline%20models%20in%20multi-image%20reasoning%0Atasks%20up%20to%208%25%20with%20a%20much%20more%20scalable%20data%20pipeline.%0A&entry.1838667208=http%3A//arxiv.org/abs/2501.03675v1&entry.124074799=Read"},
{"title": "Diffusion as Shader: 3D-aware Video Diffusion for Versatile Video\n  Generation Control", "author": "Zekai Gu and Rui Yan and Jiahao Lu and Peng Li and Zhiyang Dou and Chenyang Si and Zhen Dong and Qifeng Liu and Cheng Lin and Ziwei Liu and Wenping Wang and Yuan Liu", "abstract": "  Diffusion models have demonstrated impressive performance in generating\nhigh-quality videos from text prompts or images. However, precise control over\nthe video generation process, such as camera manipulation or content editing,\nremains a significant challenge. Existing methods for controlled video\ngeneration are typically limited to a single control type, lacking the\nflexibility to handle diverse control demands. In this paper, we introduce\nDiffusion as Shader (DaS), a novel approach that supports multiple video\ncontrol tasks within a unified architecture. Our key insight is that achieving\nversatile video control necessitates leveraging 3D control signals, as videos\nare fundamentally 2D renderings of dynamic 3D content. Unlike prior methods\nlimited to 2D control signals, DaS leverages 3D tracking videos as control\ninputs, making the video diffusion process inherently 3D-aware. This innovation\nallows DaS to achieve a wide range of video controls by simply manipulating the\n3D tracking videos. A further advantage of using 3D tracking videos is their\nability to effectively link frames, significantly enhancing the temporal\nconsistency of the generated videos. With just 3 days of fine-tuning on 8 H800\nGPUs using less than 10k videos, DaS demonstrates strong control capabilities\nacross diverse tasks, including mesh-to-video generation, camera control,\nmotion transfer, and object manipulation.\n", "link": "http://arxiv.org/abs/2501.03847v1", "date": "2025-01-07", "relevancy": 2.6373, "topK": [{"title": "Synthesizing Moving People with 3D Control", "link": "http://arxiv.org/abs/2401.10889v2", "similarity": 0.6751}, {"title": "Magic-Me: Identity-Specific Video Customized Diffusion", "link": "http://arxiv.org/abs/2402.09368v1", "similarity": 0.6496}, {"title": "Customize-A-Video: One-Shot Motion Customization of Text-to-Video\n  Diffusion Models", "link": "http://arxiv.org/abs/2402.14780v1", "similarity": 0.6442}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Diffusion%20as%20Shader%3A%203D-aware%20Video%20Diffusion%20for%20Versatile%20Video%0A%20%20Generation%20Control&body=Title%3A%20Diffusion%20as%20Shader%3A%203D-aware%20Video%20Diffusion%20for%20Versatile%20Video%0A%20%20Generation%20Control%0AAuthor%3A%20Zekai%20Gu%20and%20Rui%20Yan%20and%20Jiahao%20Lu%20and%20Peng%20Li%20and%20Zhiyang%20Dou%20and%20Chenyang%20Si%20and%20Zhen%20Dong%20and%20Qifeng%20Liu%20and%20Cheng%20Lin%20and%20Ziwei%20Liu%20and%20Wenping%20Wang%20and%20Yuan%20Liu%0AAbstract%3A%20%20%20Diffusion%20models%20have%20demonstrated%20impressive%20performance%20in%20generating%0Ahigh-quality%20videos%20from%20text%20prompts%20or%20images.%20However%2C%20precise%20control%20over%0Athe%20video%20generation%20process%2C%20such%20as%20camera%20manipulation%20or%20content%20editing%2C%0Aremains%20a%20significant%20challenge.%20Existing%20methods%20for%20controlled%20video%0Ageneration%20are%20typically%20limited%20to%20a%20single%20control%20type%2C%20lacking%20the%0Aflexibility%20to%20handle%20diverse%20control%20demands.%20In%20this%20paper%2C%20we%20introduce%0ADiffusion%20as%20Shader%20%28DaS%29%2C%20a%20novel%20approach%20that%20supports%20multiple%20video%0Acontrol%20tasks%20within%20a%20unified%20architecture.%20Our%20key%20insight%20is%20that%20achieving%0Aversatile%20video%20control%20necessitates%20leveraging%203D%20control%20signals%2C%20as%20videos%0Aare%20fundamentally%202D%20renderings%20of%20dynamic%203D%20content.%20Unlike%20prior%20methods%0Alimited%20to%202D%20control%20signals%2C%20DaS%20leverages%203D%20tracking%20videos%20as%20control%0Ainputs%2C%20making%20the%20video%20diffusion%20process%20inherently%203D-aware.%20This%20innovation%0Aallows%20DaS%20to%20achieve%20a%20wide%20range%20of%20video%20controls%20by%20simply%20manipulating%20the%0A3D%20tracking%20videos.%20A%20further%20advantage%20of%20using%203D%20tracking%20videos%20is%20their%0Aability%20to%20effectively%20link%20frames%2C%20significantly%20enhancing%20the%20temporal%0Aconsistency%20of%20the%20generated%20videos.%20With%20just%203%20days%20of%20fine-tuning%20on%208%20H800%0AGPUs%20using%20less%20than%2010k%20videos%2C%20DaS%20demonstrates%20strong%20control%20capabilities%0Aacross%20diverse%20tasks%2C%20including%20mesh-to-video%20generation%2C%20camera%20control%2C%0Amotion%20transfer%2C%20and%20object%20manipulation.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2501.03847v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DDiffusion%2520as%2520Shader%253A%25203D-aware%2520Video%2520Diffusion%2520for%2520Versatile%2520Video%250A%2520%2520Generation%2520Control%26entry.906535625%3DZekai%2520Gu%2520and%2520Rui%2520Yan%2520and%2520Jiahao%2520Lu%2520and%2520Peng%2520Li%2520and%2520Zhiyang%2520Dou%2520and%2520Chenyang%2520Si%2520and%2520Zhen%2520Dong%2520and%2520Qifeng%2520Liu%2520and%2520Cheng%2520Lin%2520and%2520Ziwei%2520Liu%2520and%2520Wenping%2520Wang%2520and%2520Yuan%2520Liu%26entry.1292438233%3D%2520%2520Diffusion%2520models%2520have%2520demonstrated%2520impressive%2520performance%2520in%2520generating%250Ahigh-quality%2520videos%2520from%2520text%2520prompts%2520or%2520images.%2520However%252C%2520precise%2520control%2520over%250Athe%2520video%2520generation%2520process%252C%2520such%2520as%2520camera%2520manipulation%2520or%2520content%2520editing%252C%250Aremains%2520a%2520significant%2520challenge.%2520Existing%2520methods%2520for%2520controlled%2520video%250Ageneration%2520are%2520typically%2520limited%2520to%2520a%2520single%2520control%2520type%252C%2520lacking%2520the%250Aflexibility%2520to%2520handle%2520diverse%2520control%2520demands.%2520In%2520this%2520paper%252C%2520we%2520introduce%250ADiffusion%2520as%2520Shader%2520%2528DaS%2529%252C%2520a%2520novel%2520approach%2520that%2520supports%2520multiple%2520video%250Acontrol%2520tasks%2520within%2520a%2520unified%2520architecture.%2520Our%2520key%2520insight%2520is%2520that%2520achieving%250Aversatile%2520video%2520control%2520necessitates%2520leveraging%25203D%2520control%2520signals%252C%2520as%2520videos%250Aare%2520fundamentally%25202D%2520renderings%2520of%2520dynamic%25203D%2520content.%2520Unlike%2520prior%2520methods%250Alimited%2520to%25202D%2520control%2520signals%252C%2520DaS%2520leverages%25203D%2520tracking%2520videos%2520as%2520control%250Ainputs%252C%2520making%2520the%2520video%2520diffusion%2520process%2520inherently%25203D-aware.%2520This%2520innovation%250Aallows%2520DaS%2520to%2520achieve%2520a%2520wide%2520range%2520of%2520video%2520controls%2520by%2520simply%2520manipulating%2520the%250A3D%2520tracking%2520videos.%2520A%2520further%2520advantage%2520of%2520using%25203D%2520tracking%2520videos%2520is%2520their%250Aability%2520to%2520effectively%2520link%2520frames%252C%2520significantly%2520enhancing%2520the%2520temporal%250Aconsistency%2520of%2520the%2520generated%2520videos.%2520With%2520just%25203%2520days%2520of%2520fine-tuning%2520on%25208%2520H800%250AGPUs%2520using%2520less%2520than%252010k%2520videos%252C%2520DaS%2520demonstrates%2520strong%2520control%2520capabilities%250Aacross%2520diverse%2520tasks%252C%2520including%2520mesh-to-video%2520generation%252C%2520camera%2520control%252C%250Amotion%2520transfer%252C%2520and%2520object%2520manipulation.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2501.03847v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Diffusion%20as%20Shader%3A%203D-aware%20Video%20Diffusion%20for%20Versatile%20Video%0A%20%20Generation%20Control&entry.906535625=Zekai%20Gu%20and%20Rui%20Yan%20and%20Jiahao%20Lu%20and%20Peng%20Li%20and%20Zhiyang%20Dou%20and%20Chenyang%20Si%20and%20Zhen%20Dong%20and%20Qifeng%20Liu%20and%20Cheng%20Lin%20and%20Ziwei%20Liu%20and%20Wenping%20Wang%20and%20Yuan%20Liu&entry.1292438233=%20%20Diffusion%20models%20have%20demonstrated%20impressive%20performance%20in%20generating%0Ahigh-quality%20videos%20from%20text%20prompts%20or%20images.%20However%2C%20precise%20control%20over%0Athe%20video%20generation%20process%2C%20such%20as%20camera%20manipulation%20or%20content%20editing%2C%0Aremains%20a%20significant%20challenge.%20Existing%20methods%20for%20controlled%20video%0Ageneration%20are%20typically%20limited%20to%20a%20single%20control%20type%2C%20lacking%20the%0Aflexibility%20to%20handle%20diverse%20control%20demands.%20In%20this%20paper%2C%20we%20introduce%0ADiffusion%20as%20Shader%20%28DaS%29%2C%20a%20novel%20approach%20that%20supports%20multiple%20video%0Acontrol%20tasks%20within%20a%20unified%20architecture.%20Our%20key%20insight%20is%20that%20achieving%0Aversatile%20video%20control%20necessitates%20leveraging%203D%20control%20signals%2C%20as%20videos%0Aare%20fundamentally%202D%20renderings%20of%20dynamic%203D%20content.%20Unlike%20prior%20methods%0Alimited%20to%202D%20control%20signals%2C%20DaS%20leverages%203D%20tracking%20videos%20as%20control%0Ainputs%2C%20making%20the%20video%20diffusion%20process%20inherently%203D-aware.%20This%20innovation%0Aallows%20DaS%20to%20achieve%20a%20wide%20range%20of%20video%20controls%20by%20simply%20manipulating%20the%0A3D%20tracking%20videos.%20A%20further%20advantage%20of%20using%203D%20tracking%20videos%20is%20their%0Aability%20to%20effectively%20link%20frames%2C%20significantly%20enhancing%20the%20temporal%0Aconsistency%20of%20the%20generated%20videos.%20With%20just%203%20days%20of%20fine-tuning%20on%208%20H800%0AGPUs%20using%20less%20than%2010k%20videos%2C%20DaS%20demonstrates%20strong%20control%20capabilities%0Aacross%20diverse%20tasks%2C%20including%20mesh-to-video%20generation%2C%20camera%20control%2C%0Amotion%20transfer%2C%20and%20object%20manipulation.%0A&entry.1838667208=http%3A//arxiv.org/abs/2501.03847v1&entry.124074799=Read"},
{"title": "Image Segmentation: Inducing graph-based learning", "author": "Aryan Singh and Pepijn Van de Ven and Ciar\u00e1n Eising and Patrick Denny", "abstract": "  This study explores the potential of graph neural networks (GNNs) to enhance\nsemantic segmentation across diverse image modalities. We evaluate the\neffectiveness of a novel GNN-based U-Net architecture on three distinct\ndatasets: PascalVOC, a standard benchmark for natural image segmentation,\nWoodScape, a challenging dataset of fisheye images commonly used in autonomous\ndriving, introducing significant geometric distortions; and ISIC2016, a dataset\nof dermoscopic images for skin lesion segmentation. We compare our proposed\nUNet-GNN model against established convolutional neural networks (CNNs) based\nsegmentation models, including U-Net and U-Net++, as well as the\ntransformer-based SwinUNet. Unlike these methods, which primarily rely on local\nconvolutional operations or global self-attention, GNNs explicitly model\nrelationships between image regions by constructing and operating on a graph\nrepresentation of the image features. This approach allows the model to capture\nlong-range dependencies and complex spatial relationships, which we hypothesize\nwill be particularly beneficial for handling geometric distortions present in\nfisheye imagery and capturing intricate boundaries in medical images. Our\nanalysis demonstrates the versatility of GNNs in addressing diverse\nsegmentation challenges and highlights their potential to improve segmentation\naccuracy in various applications, including autonomous driving and medical\nimage analysis.\n", "link": "http://arxiv.org/abs/2501.03765v1", "date": "2025-01-07", "relevancy": 2.6338, "topK": [{"title": "Self-supervised Photographic Image Layout Representation Learning", "link": "http://arxiv.org/abs/2403.03740v1", "similarity": 0.5462}, {"title": "UGG: Unified Generative Grasping", "link": "https://arxiv.org/abs/2311.16917", "similarity": 0.521}, {"title": "DDM-NET: End-to-end learning of keypoint feature Detection, Description and Matching for 3D localization", "link": "https://arxiv.org/abs/2212.04575", "similarity": 0.5131}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Image%20Segmentation%3A%20Inducing%20graph-based%20learning&body=Title%3A%20Image%20Segmentation%3A%20Inducing%20graph-based%20learning%0AAuthor%3A%20Aryan%20Singh%20and%20Pepijn%20Van%20de%20Ven%20and%20Ciar%C3%A1n%20Eising%20and%20Patrick%20Denny%0AAbstract%3A%20%20%20This%20study%20explores%20the%20potential%20of%20graph%20neural%20networks%20%28GNNs%29%20to%20enhance%0Asemantic%20segmentation%20across%20diverse%20image%20modalities.%20We%20evaluate%20the%0Aeffectiveness%20of%20a%20novel%20GNN-based%20U-Net%20architecture%20on%20three%20distinct%0Adatasets%3A%20PascalVOC%2C%20a%20standard%20benchmark%20for%20natural%20image%20segmentation%2C%0AWoodScape%2C%20a%20challenging%20dataset%20of%20fisheye%20images%20commonly%20used%20in%20autonomous%0Adriving%2C%20introducing%20significant%20geometric%20distortions%3B%20and%20ISIC2016%2C%20a%20dataset%0Aof%20dermoscopic%20images%20for%20skin%20lesion%20segmentation.%20We%20compare%20our%20proposed%0AUNet-GNN%20model%20against%20established%20convolutional%20neural%20networks%20%28CNNs%29%20based%0Asegmentation%20models%2C%20including%20U-Net%20and%20U-Net%2B%2B%2C%20as%20well%20as%20the%0Atransformer-based%20SwinUNet.%20Unlike%20these%20methods%2C%20which%20primarily%20rely%20on%20local%0Aconvolutional%20operations%20or%20global%20self-attention%2C%20GNNs%20explicitly%20model%0Arelationships%20between%20image%20regions%20by%20constructing%20and%20operating%20on%20a%20graph%0Arepresentation%20of%20the%20image%20features.%20This%20approach%20allows%20the%20model%20to%20capture%0Along-range%20dependencies%20and%20complex%20spatial%20relationships%2C%20which%20we%20hypothesize%0Awill%20be%20particularly%20beneficial%20for%20handling%20geometric%20distortions%20present%20in%0Afisheye%20imagery%20and%20capturing%20intricate%20boundaries%20in%20medical%20images.%20Our%0Aanalysis%20demonstrates%20the%20versatility%20of%20GNNs%20in%20addressing%20diverse%0Asegmentation%20challenges%20and%20highlights%20their%20potential%20to%20improve%20segmentation%0Aaccuracy%20in%20various%20applications%2C%20including%20autonomous%20driving%20and%20medical%0Aimage%20analysis.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2501.03765v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DImage%2520Segmentation%253A%2520Inducing%2520graph-based%2520learning%26entry.906535625%3DAryan%2520Singh%2520and%2520Pepijn%2520Van%2520de%2520Ven%2520and%2520Ciar%25C3%25A1n%2520Eising%2520and%2520Patrick%2520Denny%26entry.1292438233%3D%2520%2520This%2520study%2520explores%2520the%2520potential%2520of%2520graph%2520neural%2520networks%2520%2528GNNs%2529%2520to%2520enhance%250Asemantic%2520segmentation%2520across%2520diverse%2520image%2520modalities.%2520We%2520evaluate%2520the%250Aeffectiveness%2520of%2520a%2520novel%2520GNN-based%2520U-Net%2520architecture%2520on%2520three%2520distinct%250Adatasets%253A%2520PascalVOC%252C%2520a%2520standard%2520benchmark%2520for%2520natural%2520image%2520segmentation%252C%250AWoodScape%252C%2520a%2520challenging%2520dataset%2520of%2520fisheye%2520images%2520commonly%2520used%2520in%2520autonomous%250Adriving%252C%2520introducing%2520significant%2520geometric%2520distortions%253B%2520and%2520ISIC2016%252C%2520a%2520dataset%250Aof%2520dermoscopic%2520images%2520for%2520skin%2520lesion%2520segmentation.%2520We%2520compare%2520our%2520proposed%250AUNet-GNN%2520model%2520against%2520established%2520convolutional%2520neural%2520networks%2520%2528CNNs%2529%2520based%250Asegmentation%2520models%252C%2520including%2520U-Net%2520and%2520U-Net%252B%252B%252C%2520as%2520well%2520as%2520the%250Atransformer-based%2520SwinUNet.%2520Unlike%2520these%2520methods%252C%2520which%2520primarily%2520rely%2520on%2520local%250Aconvolutional%2520operations%2520or%2520global%2520self-attention%252C%2520GNNs%2520explicitly%2520model%250Arelationships%2520between%2520image%2520regions%2520by%2520constructing%2520and%2520operating%2520on%2520a%2520graph%250Arepresentation%2520of%2520the%2520image%2520features.%2520This%2520approach%2520allows%2520the%2520model%2520to%2520capture%250Along-range%2520dependencies%2520and%2520complex%2520spatial%2520relationships%252C%2520which%2520we%2520hypothesize%250Awill%2520be%2520particularly%2520beneficial%2520for%2520handling%2520geometric%2520distortions%2520present%2520in%250Afisheye%2520imagery%2520and%2520capturing%2520intricate%2520boundaries%2520in%2520medical%2520images.%2520Our%250Aanalysis%2520demonstrates%2520the%2520versatility%2520of%2520GNNs%2520in%2520addressing%2520diverse%250Asegmentation%2520challenges%2520and%2520highlights%2520their%2520potential%2520to%2520improve%2520segmentation%250Aaccuracy%2520in%2520various%2520applications%252C%2520including%2520autonomous%2520driving%2520and%2520medical%250Aimage%2520analysis.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2501.03765v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Image%20Segmentation%3A%20Inducing%20graph-based%20learning&entry.906535625=Aryan%20Singh%20and%20Pepijn%20Van%20de%20Ven%20and%20Ciar%C3%A1n%20Eising%20and%20Patrick%20Denny&entry.1292438233=%20%20This%20study%20explores%20the%20potential%20of%20graph%20neural%20networks%20%28GNNs%29%20to%20enhance%0Asemantic%20segmentation%20across%20diverse%20image%20modalities.%20We%20evaluate%20the%0Aeffectiveness%20of%20a%20novel%20GNN-based%20U-Net%20architecture%20on%20three%20distinct%0Adatasets%3A%20PascalVOC%2C%20a%20standard%20benchmark%20for%20natural%20image%20segmentation%2C%0AWoodScape%2C%20a%20challenging%20dataset%20of%20fisheye%20images%20commonly%20used%20in%20autonomous%0Adriving%2C%20introducing%20significant%20geometric%20distortions%3B%20and%20ISIC2016%2C%20a%20dataset%0Aof%20dermoscopic%20images%20for%20skin%20lesion%20segmentation.%20We%20compare%20our%20proposed%0AUNet-GNN%20model%20against%20established%20convolutional%20neural%20networks%20%28CNNs%29%20based%0Asegmentation%20models%2C%20including%20U-Net%20and%20U-Net%2B%2B%2C%20as%20well%20as%20the%0Atransformer-based%20SwinUNet.%20Unlike%20these%20methods%2C%20which%20primarily%20rely%20on%20local%0Aconvolutional%20operations%20or%20global%20self-attention%2C%20GNNs%20explicitly%20model%0Arelationships%20between%20image%20regions%20by%20constructing%20and%20operating%20on%20a%20graph%0Arepresentation%20of%20the%20image%20features.%20This%20approach%20allows%20the%20model%20to%20capture%0Along-range%20dependencies%20and%20complex%20spatial%20relationships%2C%20which%20we%20hypothesize%0Awill%20be%20particularly%20beneficial%20for%20handling%20geometric%20distortions%20present%20in%0Afisheye%20imagery%20and%20capturing%20intricate%20boundaries%20in%20medical%20images.%20Our%0Aanalysis%20demonstrates%20the%20versatility%20of%20GNNs%20in%20addressing%20diverse%0Asegmentation%20challenges%20and%20highlights%20their%20potential%20to%20improve%20segmentation%0Aaccuracy%20in%20various%20applications%2C%20including%20autonomous%20driving%20and%20medical%0Aimage%20analysis.%0A&entry.1838667208=http%3A//arxiv.org/abs/2501.03765v1&entry.124074799=Read"},
{"title": "T-FREE: Subword Tokenizer-Free Generative LLMs via Sparse\n  Representations for Memory-Efficient Embeddings", "author": "Bj\u00f6rn Deiseroth and Manuel Brack and Patrick Schramowski and Kristian Kersting and Samuel Weinbach", "abstract": "  Tokenizers are crucial for encoding information in Large Language Models, but\ntheir development has recently stagnated, and they contain inherent weaknesses.\nMajor limitations include computational overhead, ineffective vocabulary use,\nand unnecessarily large embedding and head layers. Additionally, their\nperformance is biased towards a reference corpus, leading to reduced\neffectiveness for underrepresented languages.\n  To remedy these issues, we propose T-FREE, which directly embeds words\nthrough sparse activation patterns over character triplets, and does not\nrequire a reference corpus. T-FREE inherently exploits morphological\nsimilarities and allows for strong compression of embedding layers. In our\nexhaustive experimental evaluation, we achieve competitive downstream\nperformance with a parameter reduction of more than 85% on these layers.\nFurther, T-FREE shows significant improvements in cross-lingual transfer\nlearning.\n", "link": "http://arxiv.org/abs/2406.19223v2", "date": "2025-01-07", "relevancy": 2.606, "topK": [{"title": "ToDo: Token Downsampling for Efficient Generation of High-Resolution\n  Images", "link": "http://arxiv.org/abs/2402.13573v2", "similarity": 0.5562}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5176}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.4898}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20T-FREE%3A%20Subword%20Tokenizer-Free%20Generative%20LLMs%20via%20Sparse%0A%20%20Representations%20for%20Memory-Efficient%20Embeddings&body=Title%3A%20T-FREE%3A%20Subword%20Tokenizer-Free%20Generative%20LLMs%20via%20Sparse%0A%20%20Representations%20for%20Memory-Efficient%20Embeddings%0AAuthor%3A%20Bj%C3%B6rn%20Deiseroth%20and%20Manuel%20Brack%20and%20Patrick%20Schramowski%20and%20Kristian%20Kersting%20and%20Samuel%20Weinbach%0AAbstract%3A%20%20%20Tokenizers%20are%20crucial%20for%20encoding%20information%20in%20Large%20Language%20Models%2C%20but%0Atheir%20development%20has%20recently%20stagnated%2C%20and%20they%20contain%20inherent%20weaknesses.%0AMajor%20limitations%20include%20computational%20overhead%2C%20ineffective%20vocabulary%20use%2C%0Aand%20unnecessarily%20large%20embedding%20and%20head%20layers.%20Additionally%2C%20their%0Aperformance%20is%20biased%20towards%20a%20reference%20corpus%2C%20leading%20to%20reduced%0Aeffectiveness%20for%20underrepresented%20languages.%0A%20%20To%20remedy%20these%20issues%2C%20we%20propose%20T-FREE%2C%20which%20directly%20embeds%20words%0Athrough%20sparse%20activation%20patterns%20over%20character%20triplets%2C%20and%20does%20not%0Arequire%20a%20reference%20corpus.%20T-FREE%20inherently%20exploits%20morphological%0Asimilarities%20and%20allows%20for%20strong%20compression%20of%20embedding%20layers.%20In%20our%0Aexhaustive%20experimental%20evaluation%2C%20we%20achieve%20competitive%20downstream%0Aperformance%20with%20a%20parameter%20reduction%20of%20more%20than%2085%25%20on%20these%20layers.%0AFurther%2C%20T-FREE%20shows%20significant%20improvements%20in%20cross-lingual%20transfer%0Alearning.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2406.19223v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DT-FREE%253A%2520Subword%2520Tokenizer-Free%2520Generative%2520LLMs%2520via%2520Sparse%250A%2520%2520Representations%2520for%2520Memory-Efficient%2520Embeddings%26entry.906535625%3DBj%25C3%25B6rn%2520Deiseroth%2520and%2520Manuel%2520Brack%2520and%2520Patrick%2520Schramowski%2520and%2520Kristian%2520Kersting%2520and%2520Samuel%2520Weinbach%26entry.1292438233%3D%2520%2520Tokenizers%2520are%2520crucial%2520for%2520encoding%2520information%2520in%2520Large%2520Language%2520Models%252C%2520but%250Atheir%2520development%2520has%2520recently%2520stagnated%252C%2520and%2520they%2520contain%2520inherent%2520weaknesses.%250AMajor%2520limitations%2520include%2520computational%2520overhead%252C%2520ineffective%2520vocabulary%2520use%252C%250Aand%2520unnecessarily%2520large%2520embedding%2520and%2520head%2520layers.%2520Additionally%252C%2520their%250Aperformance%2520is%2520biased%2520towards%2520a%2520reference%2520corpus%252C%2520leading%2520to%2520reduced%250Aeffectiveness%2520for%2520underrepresented%2520languages.%250A%2520%2520To%2520remedy%2520these%2520issues%252C%2520we%2520propose%2520T-FREE%252C%2520which%2520directly%2520embeds%2520words%250Athrough%2520sparse%2520activation%2520patterns%2520over%2520character%2520triplets%252C%2520and%2520does%2520not%250Arequire%2520a%2520reference%2520corpus.%2520T-FREE%2520inherently%2520exploits%2520morphological%250Asimilarities%2520and%2520allows%2520for%2520strong%2520compression%2520of%2520embedding%2520layers.%2520In%2520our%250Aexhaustive%2520experimental%2520evaluation%252C%2520we%2520achieve%2520competitive%2520downstream%250Aperformance%2520with%2520a%2520parameter%2520reduction%2520of%2520more%2520than%252085%2525%2520on%2520these%2520layers.%250AFurther%252C%2520T-FREE%2520shows%2520significant%2520improvements%2520in%2520cross-lingual%2520transfer%250Alearning.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2406.19223v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=T-FREE%3A%20Subword%20Tokenizer-Free%20Generative%20LLMs%20via%20Sparse%0A%20%20Representations%20for%20Memory-Efficient%20Embeddings&entry.906535625=Bj%C3%B6rn%20Deiseroth%20and%20Manuel%20Brack%20and%20Patrick%20Schramowski%20and%20Kristian%20Kersting%20and%20Samuel%20Weinbach&entry.1292438233=%20%20Tokenizers%20are%20crucial%20for%20encoding%20information%20in%20Large%20Language%20Models%2C%20but%0Atheir%20development%20has%20recently%20stagnated%2C%20and%20they%20contain%20inherent%20weaknesses.%0AMajor%20limitations%20include%20computational%20overhead%2C%20ineffective%20vocabulary%20use%2C%0Aand%20unnecessarily%20large%20embedding%20and%20head%20layers.%20Additionally%2C%20their%0Aperformance%20is%20biased%20towards%20a%20reference%20corpus%2C%20leading%20to%20reduced%0Aeffectiveness%20for%20underrepresented%20languages.%0A%20%20To%20remedy%20these%20issues%2C%20we%20propose%20T-FREE%2C%20which%20directly%20embeds%20words%0Athrough%20sparse%20activation%20patterns%20over%20character%20triplets%2C%20and%20does%20not%0Arequire%20a%20reference%20corpus.%20T-FREE%20inherently%20exploits%20morphological%0Asimilarities%20and%20allows%20for%20strong%20compression%20of%20embedding%20layers.%20In%20our%0Aexhaustive%20experimental%20evaluation%2C%20we%20achieve%20competitive%20downstream%0Aperformance%20with%20a%20parameter%20reduction%20of%20more%20than%2085%25%20on%20these%20layers.%0AFurther%2C%20T-FREE%20shows%20significant%20improvements%20in%20cross-lingual%20transfer%0Alearning.%0A&entry.1838667208=http%3A//arxiv.org/abs/2406.19223v2&entry.124074799=Read"},
{"title": "Disentangling, Amplifying, and Debiasing: Learning Disentangled\n  Representations for Fair Graph Neural Networks", "author": "Yeon-Chang Lee and Hojung Shin and Sang-Wook Kim", "abstract": "  Graph Neural Networks (GNNs) have become essential tools for graph\nrepresentation learning in various domains, such as social media and\nhealthcare. However, they often suffer from fairness issues due to inherent\nbiases in node attributes and graph structure, leading to unfair predictions.\nTo address these challenges, we propose a novel GNN framework, DAB-GNN, that\nDisentangles, Amplifies, and deBiases attribute, structure, and potential\nbiases in the GNN mechanism. DAB-GNN employs a disentanglement and\namplification module that isolates and amplifies each type of bias through\nspecialized disentanglers, followed by a debiasing module that minimizes the\ndistance between subgroup distributions. Extensive experiments on five datasets\ndemonstrate that DAB-GNN significantly outperforms ten state-of-the-art\ncompetitors in terms of achieving an optimal balance between accuracy and\nfairness. The codebase of DAB-GNN is available at\nhttps://github.com/Bigdasgit/DAB-GNN\n", "link": "http://arxiv.org/abs/2408.12875v2", "date": "2025-01-07", "relevancy": 2.6049, "topK": [{"title": "Boosted dynamic neural networks", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/26302", "similarity": 0.5459}, {"title": "Dynamic Gaussians Mesh: Consistent Mesh Reconstruction from Monocular\n  Videos", "link": "http://arxiv.org/abs/2404.12379v2", "similarity": 0.5227}, {"title": "UGG: Unified Generative Grasping", "link": "https://arxiv.org/abs/2311.16917", "similarity": 0.4943}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Disentangling%2C%20Amplifying%2C%20and%20Debiasing%3A%20Learning%20Disentangled%0A%20%20Representations%20for%20Fair%20Graph%20Neural%20Networks&body=Title%3A%20Disentangling%2C%20Amplifying%2C%20and%20Debiasing%3A%20Learning%20Disentangled%0A%20%20Representations%20for%20Fair%20Graph%20Neural%20Networks%0AAuthor%3A%20Yeon-Chang%20Lee%20and%20Hojung%20Shin%20and%20Sang-Wook%20Kim%0AAbstract%3A%20%20%20Graph%20Neural%20Networks%20%28GNNs%29%20have%20become%20essential%20tools%20for%20graph%0Arepresentation%20learning%20in%20various%20domains%2C%20such%20as%20social%20media%20and%0Ahealthcare.%20However%2C%20they%20often%20suffer%20from%20fairness%20issues%20due%20to%20inherent%0Abiases%20in%20node%20attributes%20and%20graph%20structure%2C%20leading%20to%20unfair%20predictions.%0ATo%20address%20these%20challenges%2C%20we%20propose%20a%20novel%20GNN%20framework%2C%20DAB-GNN%2C%20that%0ADisentangles%2C%20Amplifies%2C%20and%20deBiases%20attribute%2C%20structure%2C%20and%20potential%0Abiases%20in%20the%20GNN%20mechanism.%20DAB-GNN%20employs%20a%20disentanglement%20and%0Aamplification%20module%20that%20isolates%20and%20amplifies%20each%20type%20of%20bias%20through%0Aspecialized%20disentanglers%2C%20followed%20by%20a%20debiasing%20module%20that%20minimizes%20the%0Adistance%20between%20subgroup%20distributions.%20Extensive%20experiments%20on%20five%20datasets%0Ademonstrate%20that%20DAB-GNN%20significantly%20outperforms%20ten%20state-of-the-art%0Acompetitors%20in%20terms%20of%20achieving%20an%20optimal%20balance%20between%20accuracy%20and%0Afairness.%20The%20codebase%20of%20DAB-GNN%20is%20available%20at%0Ahttps%3A//github.com/Bigdasgit/DAB-GNN%0A%0ALink%3A%20http%3A//arxiv.org/abs/2408.12875v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DDisentangling%252C%2520Amplifying%252C%2520and%2520Debiasing%253A%2520Learning%2520Disentangled%250A%2520%2520Representations%2520for%2520Fair%2520Graph%2520Neural%2520Networks%26entry.906535625%3DYeon-Chang%2520Lee%2520and%2520Hojung%2520Shin%2520and%2520Sang-Wook%2520Kim%26entry.1292438233%3D%2520%2520Graph%2520Neural%2520Networks%2520%2528GNNs%2529%2520have%2520become%2520essential%2520tools%2520for%2520graph%250Arepresentation%2520learning%2520in%2520various%2520domains%252C%2520such%2520as%2520social%2520media%2520and%250Ahealthcare.%2520However%252C%2520they%2520often%2520suffer%2520from%2520fairness%2520issues%2520due%2520to%2520inherent%250Abiases%2520in%2520node%2520attributes%2520and%2520graph%2520structure%252C%2520leading%2520to%2520unfair%2520predictions.%250ATo%2520address%2520these%2520challenges%252C%2520we%2520propose%2520a%2520novel%2520GNN%2520framework%252C%2520DAB-GNN%252C%2520that%250ADisentangles%252C%2520Amplifies%252C%2520and%2520deBiases%2520attribute%252C%2520structure%252C%2520and%2520potential%250Abiases%2520in%2520the%2520GNN%2520mechanism.%2520DAB-GNN%2520employs%2520a%2520disentanglement%2520and%250Aamplification%2520module%2520that%2520isolates%2520and%2520amplifies%2520each%2520type%2520of%2520bias%2520through%250Aspecialized%2520disentanglers%252C%2520followed%2520by%2520a%2520debiasing%2520module%2520that%2520minimizes%2520the%250Adistance%2520between%2520subgroup%2520distributions.%2520Extensive%2520experiments%2520on%2520five%2520datasets%250Ademonstrate%2520that%2520DAB-GNN%2520significantly%2520outperforms%2520ten%2520state-of-the-art%250Acompetitors%2520in%2520terms%2520of%2520achieving%2520an%2520optimal%2520balance%2520between%2520accuracy%2520and%250Afairness.%2520The%2520codebase%2520of%2520DAB-GNN%2520is%2520available%2520at%250Ahttps%253A//github.com/Bigdasgit/DAB-GNN%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2408.12875v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Disentangling%2C%20Amplifying%2C%20and%20Debiasing%3A%20Learning%20Disentangled%0A%20%20Representations%20for%20Fair%20Graph%20Neural%20Networks&entry.906535625=Yeon-Chang%20Lee%20and%20Hojung%20Shin%20and%20Sang-Wook%20Kim&entry.1292438233=%20%20Graph%20Neural%20Networks%20%28GNNs%29%20have%20become%20essential%20tools%20for%20graph%0Arepresentation%20learning%20in%20various%20domains%2C%20such%20as%20social%20media%20and%0Ahealthcare.%20However%2C%20they%20often%20suffer%20from%20fairness%20issues%20due%20to%20inherent%0Abiases%20in%20node%20attributes%20and%20graph%20structure%2C%20leading%20to%20unfair%20predictions.%0ATo%20address%20these%20challenges%2C%20we%20propose%20a%20novel%20GNN%20framework%2C%20DAB-GNN%2C%20that%0ADisentangles%2C%20Amplifies%2C%20and%20deBiases%20attribute%2C%20structure%2C%20and%20potential%0Abiases%20in%20the%20GNN%20mechanism.%20DAB-GNN%20employs%20a%20disentanglement%20and%0Aamplification%20module%20that%20isolates%20and%20amplifies%20each%20type%20of%20bias%20through%0Aspecialized%20disentanglers%2C%20followed%20by%20a%20debiasing%20module%20that%20minimizes%20the%0Adistance%20between%20subgroup%20distributions.%20Extensive%20experiments%20on%20five%20datasets%0Ademonstrate%20that%20DAB-GNN%20significantly%20outperforms%20ten%20state-of-the-art%0Acompetitors%20in%20terms%20of%20achieving%20an%20optimal%20balance%20between%20accuracy%20and%0Afairness.%20The%20codebase%20of%20DAB-GNN%20is%20available%20at%0Ahttps%3A//github.com/Bigdasgit/DAB-GNN%0A&entry.1838667208=http%3A//arxiv.org/abs/2408.12875v2&entry.124074799=Read"},
{"title": "Clinical Insights: A Comprehensive Review of Language Models in Medicine", "author": "Nikita Neveditsin and Pawan Lingras and Vijay Mago", "abstract": "  This paper explores the advancements and applications of language models in\nhealthcare, focusing on their clinical use cases. It examines the evolution\nfrom early encoder-based systems requiring extensive fine-tuning to\nstate-of-the-art large language and multimodal models capable of integrating\ntext and visual data through in-context learning. The analysis emphasizes\nlocally deployable models, which enhance data privacy and operational autonomy,\nand their applications in tasks such as text generation, classification,\ninformation extraction, and conversational systems. The paper also highlights a\nstructured organization of tasks and a tiered ethical approach, providing a\nvaluable resource for researchers and practitioners, while discussing key\nchallenges related to ethics, evaluation, and implementation.\n", "link": "http://arxiv.org/abs/2408.11735v3", "date": "2025-01-07", "relevancy": 2.5768, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5381}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5381}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.4699}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Clinical%20Insights%3A%20A%20Comprehensive%20Review%20of%20Language%20Models%20in%20Medicine&body=Title%3A%20Clinical%20Insights%3A%20A%20Comprehensive%20Review%20of%20Language%20Models%20in%20Medicine%0AAuthor%3A%20Nikita%20Neveditsin%20and%20Pawan%20Lingras%20and%20Vijay%20Mago%0AAbstract%3A%20%20%20This%20paper%20explores%20the%20advancements%20and%20applications%20of%20language%20models%20in%0Ahealthcare%2C%20focusing%20on%20their%20clinical%20use%20cases.%20It%20examines%20the%20evolution%0Afrom%20early%20encoder-based%20systems%20requiring%20extensive%20fine-tuning%20to%0Astate-of-the-art%20large%20language%20and%20multimodal%20models%20capable%20of%20integrating%0Atext%20and%20visual%20data%20through%20in-context%20learning.%20The%20analysis%20emphasizes%0Alocally%20deployable%20models%2C%20which%20enhance%20data%20privacy%20and%20operational%20autonomy%2C%0Aand%20their%20applications%20in%20tasks%20such%20as%20text%20generation%2C%20classification%2C%0Ainformation%20extraction%2C%20and%20conversational%20systems.%20The%20paper%20also%20highlights%20a%0Astructured%20organization%20of%20tasks%20and%20a%20tiered%20ethical%20approach%2C%20providing%20a%0Avaluable%20resource%20for%20researchers%20and%20practitioners%2C%20while%20discussing%20key%0Achallenges%20related%20to%20ethics%2C%20evaluation%2C%20and%20implementation.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2408.11735v3%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DClinical%2520Insights%253A%2520A%2520Comprehensive%2520Review%2520of%2520Language%2520Models%2520in%2520Medicine%26entry.906535625%3DNikita%2520Neveditsin%2520and%2520Pawan%2520Lingras%2520and%2520Vijay%2520Mago%26entry.1292438233%3D%2520%2520This%2520paper%2520explores%2520the%2520advancements%2520and%2520applications%2520of%2520language%2520models%2520in%250Ahealthcare%252C%2520focusing%2520on%2520their%2520clinical%2520use%2520cases.%2520It%2520examines%2520the%2520evolution%250Afrom%2520early%2520encoder-based%2520systems%2520requiring%2520extensive%2520fine-tuning%2520to%250Astate-of-the-art%2520large%2520language%2520and%2520multimodal%2520models%2520capable%2520of%2520integrating%250Atext%2520and%2520visual%2520data%2520through%2520in-context%2520learning.%2520The%2520analysis%2520emphasizes%250Alocally%2520deployable%2520models%252C%2520which%2520enhance%2520data%2520privacy%2520and%2520operational%2520autonomy%252C%250Aand%2520their%2520applications%2520in%2520tasks%2520such%2520as%2520text%2520generation%252C%2520classification%252C%250Ainformation%2520extraction%252C%2520and%2520conversational%2520systems.%2520The%2520paper%2520also%2520highlights%2520a%250Astructured%2520organization%2520of%2520tasks%2520and%2520a%2520tiered%2520ethical%2520approach%252C%2520providing%2520a%250Avaluable%2520resource%2520for%2520researchers%2520and%2520practitioners%252C%2520while%2520discussing%2520key%250Achallenges%2520related%2520to%2520ethics%252C%2520evaluation%252C%2520and%2520implementation.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2408.11735v3%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Clinical%20Insights%3A%20A%20Comprehensive%20Review%20of%20Language%20Models%20in%20Medicine&entry.906535625=Nikita%20Neveditsin%20and%20Pawan%20Lingras%20and%20Vijay%20Mago&entry.1292438233=%20%20This%20paper%20explores%20the%20advancements%20and%20applications%20of%20language%20models%20in%0Ahealthcare%2C%20focusing%20on%20their%20clinical%20use%20cases.%20It%20examines%20the%20evolution%0Afrom%20early%20encoder-based%20systems%20requiring%20extensive%20fine-tuning%20to%0Astate-of-the-art%20large%20language%20and%20multimodal%20models%20capable%20of%20integrating%0Atext%20and%20visual%20data%20through%20in-context%20learning.%20The%20analysis%20emphasizes%0Alocally%20deployable%20models%2C%20which%20enhance%20data%20privacy%20and%20operational%20autonomy%2C%0Aand%20their%20applications%20in%20tasks%20such%20as%20text%20generation%2C%20classification%2C%0Ainformation%20extraction%2C%20and%20conversational%20systems.%20The%20paper%20also%20highlights%20a%0Astructured%20organization%20of%20tasks%20and%20a%20tiered%20ethical%20approach%2C%20providing%20a%0Avaluable%20resource%20for%20researchers%20and%20practitioners%2C%20while%20discussing%20key%0Achallenges%20related%20to%20ethics%2C%20evaluation%2C%20and%20implementation.%0A&entry.1838667208=http%3A//arxiv.org/abs/2408.11735v3&entry.124074799=Read"},
{"title": "IDEAL: Leveraging Infinite and Dynamic Characterizations of Large\n  Language Models for Query-focused Summarization", "author": "Jie Cao and Dian Jiao and Qiang Yan and Wenqiao Zhang and Siliang Tang and Yueting Zhuang", "abstract": "  Query-focused summarization (QFS) aims to produce summaries that answer\nparticular questions of interest, enabling greater user control and\npersonalization. With the advent of large language models (LLMs), shows their\nimpressive capability of textual understanding through large-scale pretraining,\nwhich implies the great potential of extractive snippet generation. In this\npaper, we systematically investigated two indispensable characteristics that\nthe LLMs-based QFS models should be harnessed, Lengthy Document Summarization\nand Efficiently Fine-grained Query-LLM Alignment, respectively.\nCorrespondingly, we propose two modules called Query-aware HyperExpert and\nQuery-focused Infini-attention to access the aforementioned characteristics.\nThese innovations pave the way for broader application and accessibility in the\nfield of QFS technology. Extensive experiments conducted on existing QFS\nbenchmarks indicate the effectiveness and generalizability of the proposed\napproach. Our code is publicly available at\nhttps://github.com/DCDmllm/IDEAL_Summary.\n", "link": "http://arxiv.org/abs/2407.10486v2", "date": "2025-01-07", "relevancy": 2.5446, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5357}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5357}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.4554}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20IDEAL%3A%20Leveraging%20Infinite%20and%20Dynamic%20Characterizations%20of%20Large%0A%20%20Language%20Models%20for%20Query-focused%20Summarization&body=Title%3A%20IDEAL%3A%20Leveraging%20Infinite%20and%20Dynamic%20Characterizations%20of%20Large%0A%20%20Language%20Models%20for%20Query-focused%20Summarization%0AAuthor%3A%20Jie%20Cao%20and%20Dian%20Jiao%20and%20Qiang%20Yan%20and%20Wenqiao%20Zhang%20and%20Siliang%20Tang%20and%20Yueting%20Zhuang%0AAbstract%3A%20%20%20Query-focused%20summarization%20%28QFS%29%20aims%20to%20produce%20summaries%20that%20answer%0Aparticular%20questions%20of%20interest%2C%20enabling%20greater%20user%20control%20and%0Apersonalization.%20With%20the%20advent%20of%20large%20language%20models%20%28LLMs%29%2C%20shows%20their%0Aimpressive%20capability%20of%20textual%20understanding%20through%20large-scale%20pretraining%2C%0Awhich%20implies%20the%20great%20potential%20of%20extractive%20snippet%20generation.%20In%20this%0Apaper%2C%20we%20systematically%20investigated%20two%20indispensable%20characteristics%20that%0Athe%20LLMs-based%20QFS%20models%20should%20be%20harnessed%2C%20Lengthy%20Document%20Summarization%0Aand%20Efficiently%20Fine-grained%20Query-LLM%20Alignment%2C%20respectively.%0ACorrespondingly%2C%20we%20propose%20two%20modules%20called%20Query-aware%20HyperExpert%20and%0AQuery-focused%20Infini-attention%20to%20access%20the%20aforementioned%20characteristics.%0AThese%20innovations%20pave%20the%20way%20for%20broader%20application%20and%20accessibility%20in%20the%0Afield%20of%20QFS%20technology.%20Extensive%20experiments%20conducted%20on%20existing%20QFS%0Abenchmarks%20indicate%20the%20effectiveness%20and%20generalizability%20of%20the%20proposed%0Aapproach.%20Our%20code%20is%20publicly%20available%20at%0Ahttps%3A//github.com/DCDmllm/IDEAL_Summary.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2407.10486v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DIDEAL%253A%2520Leveraging%2520Infinite%2520and%2520Dynamic%2520Characterizations%2520of%2520Large%250A%2520%2520Language%2520Models%2520for%2520Query-focused%2520Summarization%26entry.906535625%3DJie%2520Cao%2520and%2520Dian%2520Jiao%2520and%2520Qiang%2520Yan%2520and%2520Wenqiao%2520Zhang%2520and%2520Siliang%2520Tang%2520and%2520Yueting%2520Zhuang%26entry.1292438233%3D%2520%2520Query-focused%2520summarization%2520%2528QFS%2529%2520aims%2520to%2520produce%2520summaries%2520that%2520answer%250Aparticular%2520questions%2520of%2520interest%252C%2520enabling%2520greater%2520user%2520control%2520and%250Apersonalization.%2520With%2520the%2520advent%2520of%2520large%2520language%2520models%2520%2528LLMs%2529%252C%2520shows%2520their%250Aimpressive%2520capability%2520of%2520textual%2520understanding%2520through%2520large-scale%2520pretraining%252C%250Awhich%2520implies%2520the%2520great%2520potential%2520of%2520extractive%2520snippet%2520generation.%2520In%2520this%250Apaper%252C%2520we%2520systematically%2520investigated%2520two%2520indispensable%2520characteristics%2520that%250Athe%2520LLMs-based%2520QFS%2520models%2520should%2520be%2520harnessed%252C%2520Lengthy%2520Document%2520Summarization%250Aand%2520Efficiently%2520Fine-grained%2520Query-LLM%2520Alignment%252C%2520respectively.%250ACorrespondingly%252C%2520we%2520propose%2520two%2520modules%2520called%2520Query-aware%2520HyperExpert%2520and%250AQuery-focused%2520Infini-attention%2520to%2520access%2520the%2520aforementioned%2520characteristics.%250AThese%2520innovations%2520pave%2520the%2520way%2520for%2520broader%2520application%2520and%2520accessibility%2520in%2520the%250Afield%2520of%2520QFS%2520technology.%2520Extensive%2520experiments%2520conducted%2520on%2520existing%2520QFS%250Abenchmarks%2520indicate%2520the%2520effectiveness%2520and%2520generalizability%2520of%2520the%2520proposed%250Aapproach.%2520Our%2520code%2520is%2520publicly%2520available%2520at%250Ahttps%253A//github.com/DCDmllm/IDEAL_Summary.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2407.10486v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=IDEAL%3A%20Leveraging%20Infinite%20and%20Dynamic%20Characterizations%20of%20Large%0A%20%20Language%20Models%20for%20Query-focused%20Summarization&entry.906535625=Jie%20Cao%20and%20Dian%20Jiao%20and%20Qiang%20Yan%20and%20Wenqiao%20Zhang%20and%20Siliang%20Tang%20and%20Yueting%20Zhuang&entry.1292438233=%20%20Query-focused%20summarization%20%28QFS%29%20aims%20to%20produce%20summaries%20that%20answer%0Aparticular%20questions%20of%20interest%2C%20enabling%20greater%20user%20control%20and%0Apersonalization.%20With%20the%20advent%20of%20large%20language%20models%20%28LLMs%29%2C%20shows%20their%0Aimpressive%20capability%20of%20textual%20understanding%20through%20large-scale%20pretraining%2C%0Awhich%20implies%20the%20great%20potential%20of%20extractive%20snippet%20generation.%20In%20this%0Apaper%2C%20we%20systematically%20investigated%20two%20indispensable%20characteristics%20that%0Athe%20LLMs-based%20QFS%20models%20should%20be%20harnessed%2C%20Lengthy%20Document%20Summarization%0Aand%20Efficiently%20Fine-grained%20Query-LLM%20Alignment%2C%20respectively.%0ACorrespondingly%2C%20we%20propose%20two%20modules%20called%20Query-aware%20HyperExpert%20and%0AQuery-focused%20Infini-attention%20to%20access%20the%20aforementioned%20characteristics.%0AThese%20innovations%20pave%20the%20way%20for%20broader%20application%20and%20accessibility%20in%20the%0Afield%20of%20QFS%20technology.%20Extensive%20experiments%20conducted%20on%20existing%20QFS%0Abenchmarks%20indicate%20the%20effectiveness%20and%20generalizability%20of%20the%20proposed%0Aapproach.%20Our%20code%20is%20publicly%20available%20at%0Ahttps%3A//github.com/DCDmllm/IDEAL_Summary.%0A&entry.1838667208=http%3A//arxiv.org/abs/2407.10486v2&entry.124074799=Read"},
{"title": "Deep Learning-based Accelerated MR Cholangiopancreatography without\n  Fully-sampled Data", "author": "Jinho Kim and Marcel Dominik Nickel and Florian Knoll", "abstract": "  The purpose of this study was to accelerate MR cholangiopancreatography\n(MRCP) acquisitions using deep learning-based (DL) reconstruction at 3T and\n0.55T. A total of 35 healthy volunteers underwent conventional two-fold\naccelerated MRCP scans at field strengths of 3T and 0.55T. We trained DL\nreconstructions using two different training strategies, supervised (SV) and\nself-supervised (SSV), with retrospectively six-fold undersampled data obtained\nat 3T. We then evaluated the DL reconstructions against standard techniques,\nparallel imaging (PI) and compressed sensing (CS), focusing on peak\nsignal-to-noise ratio (PSNR) and structural similarity (SSIM) as metrics. We\nalso tested DL reconstructions with prospectively accelerated acquisitions and\nevaluated their robustness when changing fields strengths from 3T to 0.55T. DL\nreconstructions demonstrated a reduction in average acquisition time from\n599/542 to 255/180 seconds for MRCP at 3T/0.55T. In both retrospective and\nprospective undersampling, PSNR and SSIM of DL reconstructions were higher than\nthose of PI and CS. At the same time, DL reconstructions preserved the image\nquality of undersampled data, including sharpness and the visibility of\nhepatobiliary ducts. In addition, both DL approaches produced high-quality\nreconstructions at 0.55T. In summary, DL reconstructions trained for highly\naccelerated MRCP enabled a reduction in acquisition time by a factor of 2.4/3.0\nat 3T/0.55T while maintaining the image quality of conventional acquisitions.\n", "link": "http://arxiv.org/abs/2405.03732v3", "date": "2025-01-07", "relevancy": 2.5379, "topK": [{"title": "CAT3D: Create Anything in 3D with Multi-View Diffusion Models", "link": "http://arxiv.org/abs/2405.10314v1", "similarity": 0.5088}, {"title": "CAT3D: Create Anything in 3D with Multi-View Diffusion Models", "link": "http://arxiv.org/abs/2405.10314v1", "similarity": 0.5088}, {"title": "RTG-SLAM: Real-time 3D Reconstruction at Scale using Gaussian Splatting", "link": "http://arxiv.org/abs/2404.19706v1", "similarity": 0.505}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Deep%20Learning-based%20Accelerated%20MR%20Cholangiopancreatography%20without%0A%20%20Fully-sampled%20Data&body=Title%3A%20Deep%20Learning-based%20Accelerated%20MR%20Cholangiopancreatography%20without%0A%20%20Fully-sampled%20Data%0AAuthor%3A%20Jinho%20Kim%20and%20Marcel%20Dominik%20Nickel%20and%20Florian%20Knoll%0AAbstract%3A%20%20%20The%20purpose%20of%20this%20study%20was%20to%20accelerate%20MR%20cholangiopancreatography%0A%28MRCP%29%20acquisitions%20using%20deep%20learning-based%20%28DL%29%20reconstruction%20at%203T%20and%0A0.55T.%20A%20total%20of%2035%20healthy%20volunteers%20underwent%20conventional%20two-fold%0Aaccelerated%20MRCP%20scans%20at%20field%20strengths%20of%203T%20and%200.55T.%20We%20trained%20DL%0Areconstructions%20using%20two%20different%20training%20strategies%2C%20supervised%20%28SV%29%20and%0Aself-supervised%20%28SSV%29%2C%20with%20retrospectively%20six-fold%20undersampled%20data%20obtained%0Aat%203T.%20We%20then%20evaluated%20the%20DL%20reconstructions%20against%20standard%20techniques%2C%0Aparallel%20imaging%20%28PI%29%20and%20compressed%20sensing%20%28CS%29%2C%20focusing%20on%20peak%0Asignal-to-noise%20ratio%20%28PSNR%29%20and%20structural%20similarity%20%28SSIM%29%20as%20metrics.%20We%0Aalso%20tested%20DL%20reconstructions%20with%20prospectively%20accelerated%20acquisitions%20and%0Aevaluated%20their%20robustness%20when%20changing%20fields%20strengths%20from%203T%20to%200.55T.%20DL%0Areconstructions%20demonstrated%20a%20reduction%20in%20average%20acquisition%20time%20from%0A599/542%20to%20255/180%20seconds%20for%20MRCP%20at%203T/0.55T.%20In%20both%20retrospective%20and%0Aprospective%20undersampling%2C%20PSNR%20and%20SSIM%20of%20DL%20reconstructions%20were%20higher%20than%0Athose%20of%20PI%20and%20CS.%20At%20the%20same%20time%2C%20DL%20reconstructions%20preserved%20the%20image%0Aquality%20of%20undersampled%20data%2C%20including%20sharpness%20and%20the%20visibility%20of%0Ahepatobiliary%20ducts.%20In%20addition%2C%20both%20DL%20approaches%20produced%20high-quality%0Areconstructions%20at%200.55T.%20In%20summary%2C%20DL%20reconstructions%20trained%20for%20highly%0Aaccelerated%20MRCP%20enabled%20a%20reduction%20in%20acquisition%20time%20by%20a%20factor%20of%202.4/3.0%0Aat%203T/0.55T%20while%20maintaining%20the%20image%20quality%20of%20conventional%20acquisitions.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2405.03732v3%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DDeep%2520Learning-based%2520Accelerated%2520MR%2520Cholangiopancreatography%2520without%250A%2520%2520Fully-sampled%2520Data%26entry.906535625%3DJinho%2520Kim%2520and%2520Marcel%2520Dominik%2520Nickel%2520and%2520Florian%2520Knoll%26entry.1292438233%3D%2520%2520The%2520purpose%2520of%2520this%2520study%2520was%2520to%2520accelerate%2520MR%2520cholangiopancreatography%250A%2528MRCP%2529%2520acquisitions%2520using%2520deep%2520learning-based%2520%2528DL%2529%2520reconstruction%2520at%25203T%2520and%250A0.55T.%2520A%2520total%2520of%252035%2520healthy%2520volunteers%2520underwent%2520conventional%2520two-fold%250Aaccelerated%2520MRCP%2520scans%2520at%2520field%2520strengths%2520of%25203T%2520and%25200.55T.%2520We%2520trained%2520DL%250Areconstructions%2520using%2520two%2520different%2520training%2520strategies%252C%2520supervised%2520%2528SV%2529%2520and%250Aself-supervised%2520%2528SSV%2529%252C%2520with%2520retrospectively%2520six-fold%2520undersampled%2520data%2520obtained%250Aat%25203T.%2520We%2520then%2520evaluated%2520the%2520DL%2520reconstructions%2520against%2520standard%2520techniques%252C%250Aparallel%2520imaging%2520%2528PI%2529%2520and%2520compressed%2520sensing%2520%2528CS%2529%252C%2520focusing%2520on%2520peak%250Asignal-to-noise%2520ratio%2520%2528PSNR%2529%2520and%2520structural%2520similarity%2520%2528SSIM%2529%2520as%2520metrics.%2520We%250Aalso%2520tested%2520DL%2520reconstructions%2520with%2520prospectively%2520accelerated%2520acquisitions%2520and%250Aevaluated%2520their%2520robustness%2520when%2520changing%2520fields%2520strengths%2520from%25203T%2520to%25200.55T.%2520DL%250Areconstructions%2520demonstrated%2520a%2520reduction%2520in%2520average%2520acquisition%2520time%2520from%250A599/542%2520to%2520255/180%2520seconds%2520for%2520MRCP%2520at%25203T/0.55T.%2520In%2520both%2520retrospective%2520and%250Aprospective%2520undersampling%252C%2520PSNR%2520and%2520SSIM%2520of%2520DL%2520reconstructions%2520were%2520higher%2520than%250Athose%2520of%2520PI%2520and%2520CS.%2520At%2520the%2520same%2520time%252C%2520DL%2520reconstructions%2520preserved%2520the%2520image%250Aquality%2520of%2520undersampled%2520data%252C%2520including%2520sharpness%2520and%2520the%2520visibility%2520of%250Ahepatobiliary%2520ducts.%2520In%2520addition%252C%2520both%2520DL%2520approaches%2520produced%2520high-quality%250Areconstructions%2520at%25200.55T.%2520In%2520summary%252C%2520DL%2520reconstructions%2520trained%2520for%2520highly%250Aaccelerated%2520MRCP%2520enabled%2520a%2520reduction%2520in%2520acquisition%2520time%2520by%2520a%2520factor%2520of%25202.4/3.0%250Aat%25203T/0.55T%2520while%2520maintaining%2520the%2520image%2520quality%2520of%2520conventional%2520acquisitions.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2405.03732v3%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Deep%20Learning-based%20Accelerated%20MR%20Cholangiopancreatography%20without%0A%20%20Fully-sampled%20Data&entry.906535625=Jinho%20Kim%20and%20Marcel%20Dominik%20Nickel%20and%20Florian%20Knoll&entry.1292438233=%20%20The%20purpose%20of%20this%20study%20was%20to%20accelerate%20MR%20cholangiopancreatography%0A%28MRCP%29%20acquisitions%20using%20deep%20learning-based%20%28DL%29%20reconstruction%20at%203T%20and%0A0.55T.%20A%20total%20of%2035%20healthy%20volunteers%20underwent%20conventional%20two-fold%0Aaccelerated%20MRCP%20scans%20at%20field%20strengths%20of%203T%20and%200.55T.%20We%20trained%20DL%0Areconstructions%20using%20two%20different%20training%20strategies%2C%20supervised%20%28SV%29%20and%0Aself-supervised%20%28SSV%29%2C%20with%20retrospectively%20six-fold%20undersampled%20data%20obtained%0Aat%203T.%20We%20then%20evaluated%20the%20DL%20reconstructions%20against%20standard%20techniques%2C%0Aparallel%20imaging%20%28PI%29%20and%20compressed%20sensing%20%28CS%29%2C%20focusing%20on%20peak%0Asignal-to-noise%20ratio%20%28PSNR%29%20and%20structural%20similarity%20%28SSIM%29%20as%20metrics.%20We%0Aalso%20tested%20DL%20reconstructions%20with%20prospectively%20accelerated%20acquisitions%20and%0Aevaluated%20their%20robustness%20when%20changing%20fields%20strengths%20from%203T%20to%200.55T.%20DL%0Areconstructions%20demonstrated%20a%20reduction%20in%20average%20acquisition%20time%20from%0A599/542%20to%20255/180%20seconds%20for%20MRCP%20at%203T/0.55T.%20In%20both%20retrospective%20and%0Aprospective%20undersampling%2C%20PSNR%20and%20SSIM%20of%20DL%20reconstructions%20were%20higher%20than%0Athose%20of%20PI%20and%20CS.%20At%20the%20same%20time%2C%20DL%20reconstructions%20preserved%20the%20image%0Aquality%20of%20undersampled%20data%2C%20including%20sharpness%20and%20the%20visibility%20of%0Ahepatobiliary%20ducts.%20In%20addition%2C%20both%20DL%20approaches%20produced%20high-quality%0Areconstructions%20at%200.55T.%20In%20summary%2C%20DL%20reconstructions%20trained%20for%20highly%0Aaccelerated%20MRCP%20enabled%20a%20reduction%20in%20acquisition%20time%20by%20a%20factor%20of%202.4/3.0%0Aat%203T/0.55T%20while%20maintaining%20the%20image%20quality%20of%20conventional%20acquisitions.%0A&entry.1838667208=http%3A//arxiv.org/abs/2405.03732v3&entry.124074799=Read"},
{"title": "SelectiveFinetuning: Enhancing Transfer Learning in Sleep Staging\n  through Selective Domain Alignment", "author": "Siyuan Zhao and Chenyu Liu and Yi Ding and Xinliang Zhou", "abstract": "  In practical sleep stage classification, a key challenge is the variability\nof EEG data across different subjects and environments. Differences in\nphysiology, age, health status, and recording conditions can lead to domain\nshifts between data. These domain shifts often result in decreased model\naccuracy and reliability, particularly when the model is applied to new data\nwith characteristics different from those it was originally trained on, which\nis a typical manifestation of negative transfer. To address this, we propose\nSelectiveFinetuning in this paper. Our method utilizes a pretrained Multi\nResolution Convolutional Neural Network (MRCNN) to extract EEG features,\ncapturing the distinctive characteristics of different sleep stages. To\nmitigate the effect of domain shifts, we introduce a domain aligning mechanism\nthat employs Earth Mover Distance (EMD) to evaluate and select source domain\ndata closely matching the target domain. By finetuning the model with selective\nsource data, our SelectiveFinetuning enhances the model's performance on target\ndomain that exhibits domain shifts compared to the data used for training.\nExperimental results show that our method outperforms existing baselines,\noffering greater robustness and adaptability in practical scenarios where data\ndistributions are often unpredictable.\n", "link": "http://arxiv.org/abs/2501.03764v1", "date": "2025-01-07", "relevancy": 2.5305, "topK": [{"title": "Tuning-Free Noise Rectification for High Fidelity Image-to-Video\n  Generation", "link": "http://arxiv.org/abs/2403.02827v1", "similarity": 0.5093}, {"title": "Weakly-guided self-supervised pretraining for temporal activity detection", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/25189", "similarity": 0.5046}, {"title": "Boosted dynamic neural networks", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/26302", "similarity": 0.5044}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20SelectiveFinetuning%3A%20Enhancing%20Transfer%20Learning%20in%20Sleep%20Staging%0A%20%20through%20Selective%20Domain%20Alignment&body=Title%3A%20SelectiveFinetuning%3A%20Enhancing%20Transfer%20Learning%20in%20Sleep%20Staging%0A%20%20through%20Selective%20Domain%20Alignment%0AAuthor%3A%20Siyuan%20Zhao%20and%20Chenyu%20Liu%20and%20Yi%20Ding%20and%20Xinliang%20Zhou%0AAbstract%3A%20%20%20In%20practical%20sleep%20stage%20classification%2C%20a%20key%20challenge%20is%20the%20variability%0Aof%20EEG%20data%20across%20different%20subjects%20and%20environments.%20Differences%20in%0Aphysiology%2C%20age%2C%20health%20status%2C%20and%20recording%20conditions%20can%20lead%20to%20domain%0Ashifts%20between%20data.%20These%20domain%20shifts%20often%20result%20in%20decreased%20model%0Aaccuracy%20and%20reliability%2C%20particularly%20when%20the%20model%20is%20applied%20to%20new%20data%0Awith%20characteristics%20different%20from%20those%20it%20was%20originally%20trained%20on%2C%20which%0Ais%20a%20typical%20manifestation%20of%20negative%20transfer.%20To%20address%20this%2C%20we%20propose%0ASelectiveFinetuning%20in%20this%20paper.%20Our%20method%20utilizes%20a%20pretrained%20Multi%0AResolution%20Convolutional%20Neural%20Network%20%28MRCNN%29%20to%20extract%20EEG%20features%2C%0Acapturing%20the%20distinctive%20characteristics%20of%20different%20sleep%20stages.%20To%0Amitigate%20the%20effect%20of%20domain%20shifts%2C%20we%20introduce%20a%20domain%20aligning%20mechanism%0Athat%20employs%20Earth%20Mover%20Distance%20%28EMD%29%20to%20evaluate%20and%20select%20source%20domain%0Adata%20closely%20matching%20the%20target%20domain.%20By%20finetuning%20the%20model%20with%20selective%0Asource%20data%2C%20our%20SelectiveFinetuning%20enhances%20the%20model%27s%20performance%20on%20target%0Adomain%20that%20exhibits%20domain%20shifts%20compared%20to%20the%20data%20used%20for%20training.%0AExperimental%20results%20show%20that%20our%20method%20outperforms%20existing%20baselines%2C%0Aoffering%20greater%20robustness%20and%20adaptability%20in%20practical%20scenarios%20where%20data%0Adistributions%20are%20often%20unpredictable.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2501.03764v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DSelectiveFinetuning%253A%2520Enhancing%2520Transfer%2520Learning%2520in%2520Sleep%2520Staging%250A%2520%2520through%2520Selective%2520Domain%2520Alignment%26entry.906535625%3DSiyuan%2520Zhao%2520and%2520Chenyu%2520Liu%2520and%2520Yi%2520Ding%2520and%2520Xinliang%2520Zhou%26entry.1292438233%3D%2520%2520In%2520practical%2520sleep%2520stage%2520classification%252C%2520a%2520key%2520challenge%2520is%2520the%2520variability%250Aof%2520EEG%2520data%2520across%2520different%2520subjects%2520and%2520environments.%2520Differences%2520in%250Aphysiology%252C%2520age%252C%2520health%2520status%252C%2520and%2520recording%2520conditions%2520can%2520lead%2520to%2520domain%250Ashifts%2520between%2520data.%2520These%2520domain%2520shifts%2520often%2520result%2520in%2520decreased%2520model%250Aaccuracy%2520and%2520reliability%252C%2520particularly%2520when%2520the%2520model%2520is%2520applied%2520to%2520new%2520data%250Awith%2520characteristics%2520different%2520from%2520those%2520it%2520was%2520originally%2520trained%2520on%252C%2520which%250Ais%2520a%2520typical%2520manifestation%2520of%2520negative%2520transfer.%2520To%2520address%2520this%252C%2520we%2520propose%250ASelectiveFinetuning%2520in%2520this%2520paper.%2520Our%2520method%2520utilizes%2520a%2520pretrained%2520Multi%250AResolution%2520Convolutional%2520Neural%2520Network%2520%2528MRCNN%2529%2520to%2520extract%2520EEG%2520features%252C%250Acapturing%2520the%2520distinctive%2520characteristics%2520of%2520different%2520sleep%2520stages.%2520To%250Amitigate%2520the%2520effect%2520of%2520domain%2520shifts%252C%2520we%2520introduce%2520a%2520domain%2520aligning%2520mechanism%250Athat%2520employs%2520Earth%2520Mover%2520Distance%2520%2528EMD%2529%2520to%2520evaluate%2520and%2520select%2520source%2520domain%250Adata%2520closely%2520matching%2520the%2520target%2520domain.%2520By%2520finetuning%2520the%2520model%2520with%2520selective%250Asource%2520data%252C%2520our%2520SelectiveFinetuning%2520enhances%2520the%2520model%2527s%2520performance%2520on%2520target%250Adomain%2520that%2520exhibits%2520domain%2520shifts%2520compared%2520to%2520the%2520data%2520used%2520for%2520training.%250AExperimental%2520results%2520show%2520that%2520our%2520method%2520outperforms%2520existing%2520baselines%252C%250Aoffering%2520greater%2520robustness%2520and%2520adaptability%2520in%2520practical%2520scenarios%2520where%2520data%250Adistributions%2520are%2520often%2520unpredictable.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2501.03764v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=SelectiveFinetuning%3A%20Enhancing%20Transfer%20Learning%20in%20Sleep%20Staging%0A%20%20through%20Selective%20Domain%20Alignment&entry.906535625=Siyuan%20Zhao%20and%20Chenyu%20Liu%20and%20Yi%20Ding%20and%20Xinliang%20Zhou&entry.1292438233=%20%20In%20practical%20sleep%20stage%20classification%2C%20a%20key%20challenge%20is%20the%20variability%0Aof%20EEG%20data%20across%20different%20subjects%20and%20environments.%20Differences%20in%0Aphysiology%2C%20age%2C%20health%20status%2C%20and%20recording%20conditions%20can%20lead%20to%20domain%0Ashifts%20between%20data.%20These%20domain%20shifts%20often%20result%20in%20decreased%20model%0Aaccuracy%20and%20reliability%2C%20particularly%20when%20the%20model%20is%20applied%20to%20new%20data%0Awith%20characteristics%20different%20from%20those%20it%20was%20originally%20trained%20on%2C%20which%0Ais%20a%20typical%20manifestation%20of%20negative%20transfer.%20To%20address%20this%2C%20we%20propose%0ASelectiveFinetuning%20in%20this%20paper.%20Our%20method%20utilizes%20a%20pretrained%20Multi%0AResolution%20Convolutional%20Neural%20Network%20%28MRCNN%29%20to%20extract%20EEG%20features%2C%0Acapturing%20the%20distinctive%20characteristics%20of%20different%20sleep%20stages.%20To%0Amitigate%20the%20effect%20of%20domain%20shifts%2C%20we%20introduce%20a%20domain%20aligning%20mechanism%0Athat%20employs%20Earth%20Mover%20Distance%20%28EMD%29%20to%20evaluate%20and%20select%20source%20domain%0Adata%20closely%20matching%20the%20target%20domain.%20By%20finetuning%20the%20model%20with%20selective%0Asource%20data%2C%20our%20SelectiveFinetuning%20enhances%20the%20model%27s%20performance%20on%20target%0Adomain%20that%20exhibits%20domain%20shifts%20compared%20to%20the%20data%20used%20for%20training.%0AExperimental%20results%20show%20that%20our%20method%20outperforms%20existing%20baselines%2C%0Aoffering%20greater%20robustness%20and%20adaptability%20in%20practical%20scenarios%20where%20data%0Adistributions%20are%20often%20unpredictable.%0A&entry.1838667208=http%3A//arxiv.org/abs/2501.03764v1&entry.124074799=Read"},
{"title": "A Survey on Large Language Models with Multilingualism: Recent Advances\n  and New Frontiers", "author": "Kaiyu Huang and Fengran Mo and Xinyu Zhang and Hongliang Li and You Li and Yuanchi Zhang and Weijian Yi and Yulong Mao and Jinchen Liu and Yuzhuang Xu and Jinan Xu and Jian-Yun Nie and Yang Liu", "abstract": "  The rapid development of Large Language Models (LLMs) demonstrates remarkable\nmultilingual capabilities in natural language processing, attracting global\nattention in both academia and industry. To mitigate potential discrimination\nand enhance the overall usability and accessibility for diverse language user\ngroups, it is important for the development of language-fair technology.\nDespite the breakthroughs of LLMs, the investigation into the multilingual\nscenario remains insufficient, where a comprehensive survey to summarize recent\napproaches, developments, limitations, and potential solutions is desirable. To\nthis end, we provide a survey with multiple perspectives on the utilization of\nLLMs in the multilingual scenario. We first rethink the transitions between\nprevious and current research on pre-trained language models. Then we introduce\nseveral perspectives on the multilingualism of LLMs, including training and\ninference methods, information retrieval, model security, multi-domain with\nlanguage culture, and usage of datasets. We also discuss the major challenges\nthat arise in these aspects, along with possible solutions. Besides, we\nhighlight future research directions that aim at further enhancing LLMs with\nmultilingualism. The survey aims to help the research community address\nmultilingual problems and provide a comprehensive understanding of the core\nconcepts, key techniques, and latest developments in multilingual natural\nlanguage processing based on LLMs.\n", "link": "http://arxiv.org/abs/2405.10936v2", "date": "2025-01-07", "relevancy": 2.5284, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5219}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5219}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.4732}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20A%20Survey%20on%20Large%20Language%20Models%20with%20Multilingualism%3A%20Recent%20Advances%0A%20%20and%20New%20Frontiers&body=Title%3A%20A%20Survey%20on%20Large%20Language%20Models%20with%20Multilingualism%3A%20Recent%20Advances%0A%20%20and%20New%20Frontiers%0AAuthor%3A%20Kaiyu%20Huang%20and%20Fengran%20Mo%20and%20Xinyu%20Zhang%20and%20Hongliang%20Li%20and%20You%20Li%20and%20Yuanchi%20Zhang%20and%20Weijian%20Yi%20and%20Yulong%20Mao%20and%20Jinchen%20Liu%20and%20Yuzhuang%20Xu%20and%20Jinan%20Xu%20and%20Jian-Yun%20Nie%20and%20Yang%20Liu%0AAbstract%3A%20%20%20The%20rapid%20development%20of%20Large%20Language%20Models%20%28LLMs%29%20demonstrates%20remarkable%0Amultilingual%20capabilities%20in%20natural%20language%20processing%2C%20attracting%20global%0Aattention%20in%20both%20academia%20and%20industry.%20To%20mitigate%20potential%20discrimination%0Aand%20enhance%20the%20overall%20usability%20and%20accessibility%20for%20diverse%20language%20user%0Agroups%2C%20it%20is%20important%20for%20the%20development%20of%20language-fair%20technology.%0ADespite%20the%20breakthroughs%20of%20LLMs%2C%20the%20investigation%20into%20the%20multilingual%0Ascenario%20remains%20insufficient%2C%20where%20a%20comprehensive%20survey%20to%20summarize%20recent%0Aapproaches%2C%20developments%2C%20limitations%2C%20and%20potential%20solutions%20is%20desirable.%20To%0Athis%20end%2C%20we%20provide%20a%20survey%20with%20multiple%20perspectives%20on%20the%20utilization%20of%0ALLMs%20in%20the%20multilingual%20scenario.%20We%20first%20rethink%20the%20transitions%20between%0Aprevious%20and%20current%20research%20on%20pre-trained%20language%20models.%20Then%20we%20introduce%0Aseveral%20perspectives%20on%20the%20multilingualism%20of%20LLMs%2C%20including%20training%20and%0Ainference%20methods%2C%20information%20retrieval%2C%20model%20security%2C%20multi-domain%20with%0Alanguage%20culture%2C%20and%20usage%20of%20datasets.%20We%20also%20discuss%20the%20major%20challenges%0Athat%20arise%20in%20these%20aspects%2C%20along%20with%20possible%20solutions.%20Besides%2C%20we%0Ahighlight%20future%20research%20directions%20that%20aim%20at%20further%20enhancing%20LLMs%20with%0Amultilingualism.%20The%20survey%20aims%20to%20help%20the%20research%20community%20address%0Amultilingual%20problems%20and%20provide%20a%20comprehensive%20understanding%20of%20the%20core%0Aconcepts%2C%20key%20techniques%2C%20and%20latest%20developments%20in%20multilingual%20natural%0Alanguage%20processing%20based%20on%20LLMs.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2405.10936v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DA%2520Survey%2520on%2520Large%2520Language%2520Models%2520with%2520Multilingualism%253A%2520Recent%2520Advances%250A%2520%2520and%2520New%2520Frontiers%26entry.906535625%3DKaiyu%2520Huang%2520and%2520Fengran%2520Mo%2520and%2520Xinyu%2520Zhang%2520and%2520Hongliang%2520Li%2520and%2520You%2520Li%2520and%2520Yuanchi%2520Zhang%2520and%2520Weijian%2520Yi%2520and%2520Yulong%2520Mao%2520and%2520Jinchen%2520Liu%2520and%2520Yuzhuang%2520Xu%2520and%2520Jinan%2520Xu%2520and%2520Jian-Yun%2520Nie%2520and%2520Yang%2520Liu%26entry.1292438233%3D%2520%2520The%2520rapid%2520development%2520of%2520Large%2520Language%2520Models%2520%2528LLMs%2529%2520demonstrates%2520remarkable%250Amultilingual%2520capabilities%2520in%2520natural%2520language%2520processing%252C%2520attracting%2520global%250Aattention%2520in%2520both%2520academia%2520and%2520industry.%2520To%2520mitigate%2520potential%2520discrimination%250Aand%2520enhance%2520the%2520overall%2520usability%2520and%2520accessibility%2520for%2520diverse%2520language%2520user%250Agroups%252C%2520it%2520is%2520important%2520for%2520the%2520development%2520of%2520language-fair%2520technology.%250ADespite%2520the%2520breakthroughs%2520of%2520LLMs%252C%2520the%2520investigation%2520into%2520the%2520multilingual%250Ascenario%2520remains%2520insufficient%252C%2520where%2520a%2520comprehensive%2520survey%2520to%2520summarize%2520recent%250Aapproaches%252C%2520developments%252C%2520limitations%252C%2520and%2520potential%2520solutions%2520is%2520desirable.%2520To%250Athis%2520end%252C%2520we%2520provide%2520a%2520survey%2520with%2520multiple%2520perspectives%2520on%2520the%2520utilization%2520of%250ALLMs%2520in%2520the%2520multilingual%2520scenario.%2520We%2520first%2520rethink%2520the%2520transitions%2520between%250Aprevious%2520and%2520current%2520research%2520on%2520pre-trained%2520language%2520models.%2520Then%2520we%2520introduce%250Aseveral%2520perspectives%2520on%2520the%2520multilingualism%2520of%2520LLMs%252C%2520including%2520training%2520and%250Ainference%2520methods%252C%2520information%2520retrieval%252C%2520model%2520security%252C%2520multi-domain%2520with%250Alanguage%2520culture%252C%2520and%2520usage%2520of%2520datasets.%2520We%2520also%2520discuss%2520the%2520major%2520challenges%250Athat%2520arise%2520in%2520these%2520aspects%252C%2520along%2520with%2520possible%2520solutions.%2520Besides%252C%2520we%250Ahighlight%2520future%2520research%2520directions%2520that%2520aim%2520at%2520further%2520enhancing%2520LLMs%2520with%250Amultilingualism.%2520The%2520survey%2520aims%2520to%2520help%2520the%2520research%2520community%2520address%250Amultilingual%2520problems%2520and%2520provide%2520a%2520comprehensive%2520understanding%2520of%2520the%2520core%250Aconcepts%252C%2520key%2520techniques%252C%2520and%2520latest%2520developments%2520in%2520multilingual%2520natural%250Alanguage%2520processing%2520based%2520on%2520LLMs.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2405.10936v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=A%20Survey%20on%20Large%20Language%20Models%20with%20Multilingualism%3A%20Recent%20Advances%0A%20%20and%20New%20Frontiers&entry.906535625=Kaiyu%20Huang%20and%20Fengran%20Mo%20and%20Xinyu%20Zhang%20and%20Hongliang%20Li%20and%20You%20Li%20and%20Yuanchi%20Zhang%20and%20Weijian%20Yi%20and%20Yulong%20Mao%20and%20Jinchen%20Liu%20and%20Yuzhuang%20Xu%20and%20Jinan%20Xu%20and%20Jian-Yun%20Nie%20and%20Yang%20Liu&entry.1292438233=%20%20The%20rapid%20development%20of%20Large%20Language%20Models%20%28LLMs%29%20demonstrates%20remarkable%0Amultilingual%20capabilities%20in%20natural%20language%20processing%2C%20attracting%20global%0Aattention%20in%20both%20academia%20and%20industry.%20To%20mitigate%20potential%20discrimination%0Aand%20enhance%20the%20overall%20usability%20and%20accessibility%20for%20diverse%20language%20user%0Agroups%2C%20it%20is%20important%20for%20the%20development%20of%20language-fair%20technology.%0ADespite%20the%20breakthroughs%20of%20LLMs%2C%20the%20investigation%20into%20the%20multilingual%0Ascenario%20remains%20insufficient%2C%20where%20a%20comprehensive%20survey%20to%20summarize%20recent%0Aapproaches%2C%20developments%2C%20limitations%2C%20and%20potential%20solutions%20is%20desirable.%20To%0Athis%20end%2C%20we%20provide%20a%20survey%20with%20multiple%20perspectives%20on%20the%20utilization%20of%0ALLMs%20in%20the%20multilingual%20scenario.%20We%20first%20rethink%20the%20transitions%20between%0Aprevious%20and%20current%20research%20on%20pre-trained%20language%20models.%20Then%20we%20introduce%0Aseveral%20perspectives%20on%20the%20multilingualism%20of%20LLMs%2C%20including%20training%20and%0Ainference%20methods%2C%20information%20retrieval%2C%20model%20security%2C%20multi-domain%20with%0Alanguage%20culture%2C%20and%20usage%20of%20datasets.%20We%20also%20discuss%20the%20major%20challenges%0Athat%20arise%20in%20these%20aspects%2C%20along%20with%20possible%20solutions.%20Besides%2C%20we%0Ahighlight%20future%20research%20directions%20that%20aim%20at%20further%20enhancing%20LLMs%20with%0Amultilingualism.%20The%20survey%20aims%20to%20help%20the%20research%20community%20address%0Amultilingual%20problems%20and%20provide%20a%20comprehensive%20understanding%20of%20the%20core%0Aconcepts%2C%20key%20techniques%2C%20and%20latest%20developments%20in%20multilingual%20natural%0Alanguage%20processing%20based%20on%20LLMs.%0A&entry.1838667208=http%3A//arxiv.org/abs/2405.10936v2&entry.124074799=Read"},
{"title": "Unsupervised Speech Segmentation: A General Approach Using Speech\n  Language Models", "author": "Avishai Elmakies and Omri Abend and Yossi Adi", "abstract": "  In this paper, we introduce an unsupervised approach for Speech Segmentation,\nwhich builds on previously researched approaches, e.g., Speaker Diarization,\nwhile being applicable to an inclusive set of acoustic-semantic distinctions,\npaving a path towards a general Unsupervised Speech Segmentation approach.\nUnlike traditional speech and audio segmentation, which mainly focuses on\nspectral changes in the input signal, e.g., phone segmentation, our approach\ntries to segment the spoken utterance into chunks with differing\nacoustic-semantic styles, focusing on acoustic-semantic information that does\nnot translate well into text, e.g., emotion or speaker. While most Speech\nSegmentation tasks only handle one style change, e.g., emotion diarization, our\napproach tries to handle multiple acoustic-semantic style changes. Leveraging\nrecent advances in Speech Language Models (SLMs), we propose a simple\nunsupervised method to segment a given speech utterance. We empirically\ndemonstrate the effectiveness of the proposed approach by considering several\nsetups. Results suggest that the proposed method is superior to the evaluated\nbaselines on boundary detection, segment purity, and over-segmentation. Code is\navailable at\nhttps://github.com/avishaiElmakies/unsupervised_speech_segmentation_using_slm.\n", "link": "http://arxiv.org/abs/2501.03711v1", "date": "2025-01-07", "relevancy": 2.5244, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5121}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5121}, {"title": "Breadcrumbs: Adversarial class-balanced sampling for long-tailed recognition", "link": "https://link.springer.com/chapter/10.1007/978-3-031-20053-3_37", "similarity": 0.4904}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Unsupervised%20Speech%20Segmentation%3A%20A%20General%20Approach%20Using%20Speech%0A%20%20Language%20Models&body=Title%3A%20Unsupervised%20Speech%20Segmentation%3A%20A%20General%20Approach%20Using%20Speech%0A%20%20Language%20Models%0AAuthor%3A%20Avishai%20Elmakies%20and%20Omri%20Abend%20and%20Yossi%20Adi%0AAbstract%3A%20%20%20In%20this%20paper%2C%20we%20introduce%20an%20unsupervised%20approach%20for%20Speech%20Segmentation%2C%0Awhich%20builds%20on%20previously%20researched%20approaches%2C%20e.g.%2C%20Speaker%20Diarization%2C%0Awhile%20being%20applicable%20to%20an%20inclusive%20set%20of%20acoustic-semantic%20distinctions%2C%0Apaving%20a%20path%20towards%20a%20general%20Unsupervised%20Speech%20Segmentation%20approach.%0AUnlike%20traditional%20speech%20and%20audio%20segmentation%2C%20which%20mainly%20focuses%20on%0Aspectral%20changes%20in%20the%20input%20signal%2C%20e.g.%2C%20phone%20segmentation%2C%20our%20approach%0Atries%20to%20segment%20the%20spoken%20utterance%20into%20chunks%20with%20differing%0Aacoustic-semantic%20styles%2C%20focusing%20on%20acoustic-semantic%20information%20that%20does%0Anot%20translate%20well%20into%20text%2C%20e.g.%2C%20emotion%20or%20speaker.%20While%20most%20Speech%0ASegmentation%20tasks%20only%20handle%20one%20style%20change%2C%20e.g.%2C%20emotion%20diarization%2C%20our%0Aapproach%20tries%20to%20handle%20multiple%20acoustic-semantic%20style%20changes.%20Leveraging%0Arecent%20advances%20in%20Speech%20Language%20Models%20%28SLMs%29%2C%20we%20propose%20a%20simple%0Aunsupervised%20method%20to%20segment%20a%20given%20speech%20utterance.%20We%20empirically%0Ademonstrate%20the%20effectiveness%20of%20the%20proposed%20approach%20by%20considering%20several%0Asetups.%20Results%20suggest%20that%20the%20proposed%20method%20is%20superior%20to%20the%20evaluated%0Abaselines%20on%20boundary%20detection%2C%20segment%20purity%2C%20and%20over-segmentation.%20Code%20is%0Aavailable%20at%0Ahttps%3A//github.com/avishaiElmakies/unsupervised_speech_segmentation_using_slm.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2501.03711v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DUnsupervised%2520Speech%2520Segmentation%253A%2520A%2520General%2520Approach%2520Using%2520Speech%250A%2520%2520Language%2520Models%26entry.906535625%3DAvishai%2520Elmakies%2520and%2520Omri%2520Abend%2520and%2520Yossi%2520Adi%26entry.1292438233%3D%2520%2520In%2520this%2520paper%252C%2520we%2520introduce%2520an%2520unsupervised%2520approach%2520for%2520Speech%2520Segmentation%252C%250Awhich%2520builds%2520on%2520previously%2520researched%2520approaches%252C%2520e.g.%252C%2520Speaker%2520Diarization%252C%250Awhile%2520being%2520applicable%2520to%2520an%2520inclusive%2520set%2520of%2520acoustic-semantic%2520distinctions%252C%250Apaving%2520a%2520path%2520towards%2520a%2520general%2520Unsupervised%2520Speech%2520Segmentation%2520approach.%250AUnlike%2520traditional%2520speech%2520and%2520audio%2520segmentation%252C%2520which%2520mainly%2520focuses%2520on%250Aspectral%2520changes%2520in%2520the%2520input%2520signal%252C%2520e.g.%252C%2520phone%2520segmentation%252C%2520our%2520approach%250Atries%2520to%2520segment%2520the%2520spoken%2520utterance%2520into%2520chunks%2520with%2520differing%250Aacoustic-semantic%2520styles%252C%2520focusing%2520on%2520acoustic-semantic%2520information%2520that%2520does%250Anot%2520translate%2520well%2520into%2520text%252C%2520e.g.%252C%2520emotion%2520or%2520speaker.%2520While%2520most%2520Speech%250ASegmentation%2520tasks%2520only%2520handle%2520one%2520style%2520change%252C%2520e.g.%252C%2520emotion%2520diarization%252C%2520our%250Aapproach%2520tries%2520to%2520handle%2520multiple%2520acoustic-semantic%2520style%2520changes.%2520Leveraging%250Arecent%2520advances%2520in%2520Speech%2520Language%2520Models%2520%2528SLMs%2529%252C%2520we%2520propose%2520a%2520simple%250Aunsupervised%2520method%2520to%2520segment%2520a%2520given%2520speech%2520utterance.%2520We%2520empirically%250Ademonstrate%2520the%2520effectiveness%2520of%2520the%2520proposed%2520approach%2520by%2520considering%2520several%250Asetups.%2520Results%2520suggest%2520that%2520the%2520proposed%2520method%2520is%2520superior%2520to%2520the%2520evaluated%250Abaselines%2520on%2520boundary%2520detection%252C%2520segment%2520purity%252C%2520and%2520over-segmentation.%2520Code%2520is%250Aavailable%2520at%250Ahttps%253A//github.com/avishaiElmakies/unsupervised_speech_segmentation_using_slm.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2501.03711v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Unsupervised%20Speech%20Segmentation%3A%20A%20General%20Approach%20Using%20Speech%0A%20%20Language%20Models&entry.906535625=Avishai%20Elmakies%20and%20Omri%20Abend%20and%20Yossi%20Adi&entry.1292438233=%20%20In%20this%20paper%2C%20we%20introduce%20an%20unsupervised%20approach%20for%20Speech%20Segmentation%2C%0Awhich%20builds%20on%20previously%20researched%20approaches%2C%20e.g.%2C%20Speaker%20Diarization%2C%0Awhile%20being%20applicable%20to%20an%20inclusive%20set%20of%20acoustic-semantic%20distinctions%2C%0Apaving%20a%20path%20towards%20a%20general%20Unsupervised%20Speech%20Segmentation%20approach.%0AUnlike%20traditional%20speech%20and%20audio%20segmentation%2C%20which%20mainly%20focuses%20on%0Aspectral%20changes%20in%20the%20input%20signal%2C%20e.g.%2C%20phone%20segmentation%2C%20our%20approach%0Atries%20to%20segment%20the%20spoken%20utterance%20into%20chunks%20with%20differing%0Aacoustic-semantic%20styles%2C%20focusing%20on%20acoustic-semantic%20information%20that%20does%0Anot%20translate%20well%20into%20text%2C%20e.g.%2C%20emotion%20or%20speaker.%20While%20most%20Speech%0ASegmentation%20tasks%20only%20handle%20one%20style%20change%2C%20e.g.%2C%20emotion%20diarization%2C%20our%0Aapproach%20tries%20to%20handle%20multiple%20acoustic-semantic%20style%20changes.%20Leveraging%0Arecent%20advances%20in%20Speech%20Language%20Models%20%28SLMs%29%2C%20we%20propose%20a%20simple%0Aunsupervised%20method%20to%20segment%20a%20given%20speech%20utterance.%20We%20empirically%0Ademonstrate%20the%20effectiveness%20of%20the%20proposed%20approach%20by%20considering%20several%0Asetups.%20Results%20suggest%20that%20the%20proposed%20method%20is%20superior%20to%20the%20evaluated%0Abaselines%20on%20boundary%20detection%2C%20segment%20purity%2C%20and%20over-segmentation.%20Code%20is%0Aavailable%20at%0Ahttps%3A//github.com/avishaiElmakies/unsupervised_speech_segmentation_using_slm.%0A&entry.1838667208=http%3A//arxiv.org/abs/2501.03711v1&entry.124074799=Read"},
{"title": "Extraction Of Cumulative Blobs From Dynamic Gestures", "author": "Rishabh Naulakha and Shubham Gaur and Dhairya Lodha and Mehek Tulsyan and Utsav Kotecha", "abstract": "  Gesture recognition is a perceptual user interface, which is based on CV\ntechnology that allows the computer to interpret human motions as commands,\nallowing users to communicate with a computer without the use of hands, thus\nmaking the mouse and keyboard superfluous. Gesture recognition's main weakness\nis a light condition because gesture control is based on computer vision, which\nheavily relies on cameras. These cameras are used to interpret gestures in 2D\nand 3D, so the extracted information can vary depending on the source of light.\nThe limitation of the system cannot work in a dark environment. A simple night\nvision camera can be used as our camera for motion capture as they also blast\nout infrared light which is not visible to humans but can be clearly seen with\na camera that has no infrared filter this majorly overcomes the limitation of\nsystems which cannot work in a dark environment. So, the video stream from the\ncamera is fed into a Raspberry Pi which has a Python program running OpenCV\nmodule which is used for detecting, isolating and tracking the path of dynamic\ngesture, then we use an algorithm of machine learning to recognize the pattern\ndrawn and accordingly control the GPIOs of the raspberry pi to perform some\nactivities.\n", "link": "http://arxiv.org/abs/2501.04002v1", "date": "2025-01-07", "relevancy": 2.4895, "topK": [{"title": "UGG: Unified Generative Grasping", "link": "https://arxiv.org/abs/2311.16917", "similarity": 0.5132}, {"title": "PhysGen: Rigid-Body Physics-Grounded Image-to-Video Generation", "link": "http://arxiv.org/abs/2409.18964v1", "similarity": 0.4924}, {"title": "Synthesizing Moving People with 3D Control", "link": "http://arxiv.org/abs/2401.10889v2", "similarity": 0.4881}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Extraction%20Of%20Cumulative%20Blobs%20From%20Dynamic%20Gestures&body=Title%3A%20Extraction%20Of%20Cumulative%20Blobs%20From%20Dynamic%20Gestures%0AAuthor%3A%20Rishabh%20Naulakha%20and%20Shubham%20Gaur%20and%20Dhairya%20Lodha%20and%20Mehek%20Tulsyan%20and%20Utsav%20Kotecha%0AAbstract%3A%20%20%20Gesture%20recognition%20is%20a%20perceptual%20user%20interface%2C%20which%20is%20based%20on%20CV%0Atechnology%20that%20allows%20the%20computer%20to%20interpret%20human%20motions%20as%20commands%2C%0Aallowing%20users%20to%20communicate%20with%20a%20computer%20without%20the%20use%20of%20hands%2C%20thus%0Amaking%20the%20mouse%20and%20keyboard%20superfluous.%20Gesture%20recognition%27s%20main%20weakness%0Ais%20a%20light%20condition%20because%20gesture%20control%20is%20based%20on%20computer%20vision%2C%20which%0Aheavily%20relies%20on%20cameras.%20These%20cameras%20are%20used%20to%20interpret%20gestures%20in%202D%0Aand%203D%2C%20so%20the%20extracted%20information%20can%20vary%20depending%20on%20the%20source%20of%20light.%0AThe%20limitation%20of%20the%20system%20cannot%20work%20in%20a%20dark%20environment.%20A%20simple%20night%0Avision%20camera%20can%20be%20used%20as%20our%20camera%20for%20motion%20capture%20as%20they%20also%20blast%0Aout%20infrared%20light%20which%20is%20not%20visible%20to%20humans%20but%20can%20be%20clearly%20seen%20with%0Aa%20camera%20that%20has%20no%20infrared%20filter%20this%20majorly%20overcomes%20the%20limitation%20of%0Asystems%20which%20cannot%20work%20in%20a%20dark%20environment.%20So%2C%20the%20video%20stream%20from%20the%0Acamera%20is%20fed%20into%20a%20Raspberry%20Pi%20which%20has%20a%20Python%20program%20running%20OpenCV%0Amodule%20which%20is%20used%20for%20detecting%2C%20isolating%20and%20tracking%20the%20path%20of%20dynamic%0Agesture%2C%20then%20we%20use%20an%20algorithm%20of%20machine%20learning%20to%20recognize%20the%20pattern%0Adrawn%20and%20accordingly%20control%20the%20GPIOs%20of%20the%20raspberry%20pi%20to%20perform%20some%0Aactivities.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2501.04002v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DExtraction%2520Of%2520Cumulative%2520Blobs%2520From%2520Dynamic%2520Gestures%26entry.906535625%3DRishabh%2520Naulakha%2520and%2520Shubham%2520Gaur%2520and%2520Dhairya%2520Lodha%2520and%2520Mehek%2520Tulsyan%2520and%2520Utsav%2520Kotecha%26entry.1292438233%3D%2520%2520Gesture%2520recognition%2520is%2520a%2520perceptual%2520user%2520interface%252C%2520which%2520is%2520based%2520on%2520CV%250Atechnology%2520that%2520allows%2520the%2520computer%2520to%2520interpret%2520human%2520motions%2520as%2520commands%252C%250Aallowing%2520users%2520to%2520communicate%2520with%2520a%2520computer%2520without%2520the%2520use%2520of%2520hands%252C%2520thus%250Amaking%2520the%2520mouse%2520and%2520keyboard%2520superfluous.%2520Gesture%2520recognition%2527s%2520main%2520weakness%250Ais%2520a%2520light%2520condition%2520because%2520gesture%2520control%2520is%2520based%2520on%2520computer%2520vision%252C%2520which%250Aheavily%2520relies%2520on%2520cameras.%2520These%2520cameras%2520are%2520used%2520to%2520interpret%2520gestures%2520in%25202D%250Aand%25203D%252C%2520so%2520the%2520extracted%2520information%2520can%2520vary%2520depending%2520on%2520the%2520source%2520of%2520light.%250AThe%2520limitation%2520of%2520the%2520system%2520cannot%2520work%2520in%2520a%2520dark%2520environment.%2520A%2520simple%2520night%250Avision%2520camera%2520can%2520be%2520used%2520as%2520our%2520camera%2520for%2520motion%2520capture%2520as%2520they%2520also%2520blast%250Aout%2520infrared%2520light%2520which%2520is%2520not%2520visible%2520to%2520humans%2520but%2520can%2520be%2520clearly%2520seen%2520with%250Aa%2520camera%2520that%2520has%2520no%2520infrared%2520filter%2520this%2520majorly%2520overcomes%2520the%2520limitation%2520of%250Asystems%2520which%2520cannot%2520work%2520in%2520a%2520dark%2520environment.%2520So%252C%2520the%2520video%2520stream%2520from%2520the%250Acamera%2520is%2520fed%2520into%2520a%2520Raspberry%2520Pi%2520which%2520has%2520a%2520Python%2520program%2520running%2520OpenCV%250Amodule%2520which%2520is%2520used%2520for%2520detecting%252C%2520isolating%2520and%2520tracking%2520the%2520path%2520of%2520dynamic%250Agesture%252C%2520then%2520we%2520use%2520an%2520algorithm%2520of%2520machine%2520learning%2520to%2520recognize%2520the%2520pattern%250Adrawn%2520and%2520accordingly%2520control%2520the%2520GPIOs%2520of%2520the%2520raspberry%2520pi%2520to%2520perform%2520some%250Aactivities.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2501.04002v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Extraction%20Of%20Cumulative%20Blobs%20From%20Dynamic%20Gestures&entry.906535625=Rishabh%20Naulakha%20and%20Shubham%20Gaur%20and%20Dhairya%20Lodha%20and%20Mehek%20Tulsyan%20and%20Utsav%20Kotecha&entry.1292438233=%20%20Gesture%20recognition%20is%20a%20perceptual%20user%20interface%2C%20which%20is%20based%20on%20CV%0Atechnology%20that%20allows%20the%20computer%20to%20interpret%20human%20motions%20as%20commands%2C%0Aallowing%20users%20to%20communicate%20with%20a%20computer%20without%20the%20use%20of%20hands%2C%20thus%0Amaking%20the%20mouse%20and%20keyboard%20superfluous.%20Gesture%20recognition%27s%20main%20weakness%0Ais%20a%20light%20condition%20because%20gesture%20control%20is%20based%20on%20computer%20vision%2C%20which%0Aheavily%20relies%20on%20cameras.%20These%20cameras%20are%20used%20to%20interpret%20gestures%20in%202D%0Aand%203D%2C%20so%20the%20extracted%20information%20can%20vary%20depending%20on%20the%20source%20of%20light.%0AThe%20limitation%20of%20the%20system%20cannot%20work%20in%20a%20dark%20environment.%20A%20simple%20night%0Avision%20camera%20can%20be%20used%20as%20our%20camera%20for%20motion%20capture%20as%20they%20also%20blast%0Aout%20infrared%20light%20which%20is%20not%20visible%20to%20humans%20but%20can%20be%20clearly%20seen%20with%0Aa%20camera%20that%20has%20no%20infrared%20filter%20this%20majorly%20overcomes%20the%20limitation%20of%0Asystems%20which%20cannot%20work%20in%20a%20dark%20environment.%20So%2C%20the%20video%20stream%20from%20the%0Acamera%20is%20fed%20into%20a%20Raspberry%20Pi%20which%20has%20a%20Python%20program%20running%20OpenCV%0Amodule%20which%20is%20used%20for%20detecting%2C%20isolating%20and%20tracking%20the%20path%20of%20dynamic%0Agesture%2C%20then%20we%20use%20an%20algorithm%20of%20machine%20learning%20to%20recognize%20the%20pattern%0Adrawn%20and%20accordingly%20control%20the%20GPIOs%20of%20the%20raspberry%20pi%20to%20perform%20some%0Aactivities.%0A&entry.1838667208=http%3A//arxiv.org/abs/2501.04002v1&entry.124074799=Read"},
{"title": "On Local Overfitting and Forgetting in Deep Neural Networks", "author": "Uri Stern and Tomer Yaacoby and Daphna Weinshall", "abstract": "  The infrequent occurrence of overfitting in deep neural networks is\nperplexing: contrary to theoretical expectations, increasing model size often\nenhances performance in practice. But what if overfitting does occur, though\nrestricted to specific sub-regions of the data space? In this work, we propose\na novel score that captures the forgetting rate of deep models on validation\ndata. We posit that this score quantifies local overfitting: a decline in\nperformance confined to certain regions of the data space. We then show\nempirically that local overfitting occurs regardless of the presence of\ntraditional overfitting. Using the framework of deep over-parametrized linear\nmodels, we offer a certain theoretical characterization of forgotten knowledge,\nand show that it correlates with knowledge forgotten by real deep models.\nFinally, we devise a new ensemble method that aims to recover forgotten\nknowledge, relying solely on the training history of a single network. When\ncombined with self-distillation, this method enhances the performance of any\ntrained model without adding inference costs. Extensive empirical evaluations\ndemonstrate the efficacy of our method across multiple datasets, contemporary\nneural network architectures, and training protocols.\n", "link": "http://arxiv.org/abs/2412.12968v2", "date": "2025-01-07", "relevancy": 2.4691, "topK": [{"title": "Boosted dynamic neural networks", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/26302", "similarity": 0.5017}, {"title": "DDM-NET: End-to-end learning of keypoint feature Detection, Description and Matching for 3D localization", "link": "https://arxiv.org/abs/2212.04575", "similarity": 0.4974}, {"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.4824}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20On%20Local%20Overfitting%20and%20Forgetting%20in%20Deep%20Neural%20Networks&body=Title%3A%20On%20Local%20Overfitting%20and%20Forgetting%20in%20Deep%20Neural%20Networks%0AAuthor%3A%20Uri%20Stern%20and%20Tomer%20Yaacoby%20and%20Daphna%20Weinshall%0AAbstract%3A%20%20%20The%20infrequent%20occurrence%20of%20overfitting%20in%20deep%20neural%20networks%20is%0Aperplexing%3A%20contrary%20to%20theoretical%20expectations%2C%20increasing%20model%20size%20often%0Aenhances%20performance%20in%20practice.%20But%20what%20if%20overfitting%20does%20occur%2C%20though%0Arestricted%20to%20specific%20sub-regions%20of%20the%20data%20space%3F%20In%20this%20work%2C%20we%20propose%0Aa%20novel%20score%20that%20captures%20the%20forgetting%20rate%20of%20deep%20models%20on%20validation%0Adata.%20We%20posit%20that%20this%20score%20quantifies%20local%20overfitting%3A%20a%20decline%20in%0Aperformance%20confined%20to%20certain%20regions%20of%20the%20data%20space.%20We%20then%20show%0Aempirically%20that%20local%20overfitting%20occurs%20regardless%20of%20the%20presence%20of%0Atraditional%20overfitting.%20Using%20the%20framework%20of%20deep%20over-parametrized%20linear%0Amodels%2C%20we%20offer%20a%20certain%20theoretical%20characterization%20of%20forgotten%20knowledge%2C%0Aand%20show%20that%20it%20correlates%20with%20knowledge%20forgotten%20by%20real%20deep%20models.%0AFinally%2C%20we%20devise%20a%20new%20ensemble%20method%20that%20aims%20to%20recover%20forgotten%0Aknowledge%2C%20relying%20solely%20on%20the%20training%20history%20of%20a%20single%20network.%20When%0Acombined%20with%20self-distillation%2C%20this%20method%20enhances%20the%20performance%20of%20any%0Atrained%20model%20without%20adding%20inference%20costs.%20Extensive%20empirical%20evaluations%0Ademonstrate%20the%20efficacy%20of%20our%20method%20across%20multiple%20datasets%2C%20contemporary%0Aneural%20network%20architectures%2C%20and%20training%20protocols.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2412.12968v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DOn%2520Local%2520Overfitting%2520and%2520Forgetting%2520in%2520Deep%2520Neural%2520Networks%26entry.906535625%3DUri%2520Stern%2520and%2520Tomer%2520Yaacoby%2520and%2520Daphna%2520Weinshall%26entry.1292438233%3D%2520%2520The%2520infrequent%2520occurrence%2520of%2520overfitting%2520in%2520deep%2520neural%2520networks%2520is%250Aperplexing%253A%2520contrary%2520to%2520theoretical%2520expectations%252C%2520increasing%2520model%2520size%2520often%250Aenhances%2520performance%2520in%2520practice.%2520But%2520what%2520if%2520overfitting%2520does%2520occur%252C%2520though%250Arestricted%2520to%2520specific%2520sub-regions%2520of%2520the%2520data%2520space%253F%2520In%2520this%2520work%252C%2520we%2520propose%250Aa%2520novel%2520score%2520that%2520captures%2520the%2520forgetting%2520rate%2520of%2520deep%2520models%2520on%2520validation%250Adata.%2520We%2520posit%2520that%2520this%2520score%2520quantifies%2520local%2520overfitting%253A%2520a%2520decline%2520in%250Aperformance%2520confined%2520to%2520certain%2520regions%2520of%2520the%2520data%2520space.%2520We%2520then%2520show%250Aempirically%2520that%2520local%2520overfitting%2520occurs%2520regardless%2520of%2520the%2520presence%2520of%250Atraditional%2520overfitting.%2520Using%2520the%2520framework%2520of%2520deep%2520over-parametrized%2520linear%250Amodels%252C%2520we%2520offer%2520a%2520certain%2520theoretical%2520characterization%2520of%2520forgotten%2520knowledge%252C%250Aand%2520show%2520that%2520it%2520correlates%2520with%2520knowledge%2520forgotten%2520by%2520real%2520deep%2520models.%250AFinally%252C%2520we%2520devise%2520a%2520new%2520ensemble%2520method%2520that%2520aims%2520to%2520recover%2520forgotten%250Aknowledge%252C%2520relying%2520solely%2520on%2520the%2520training%2520history%2520of%2520a%2520single%2520network.%2520When%250Acombined%2520with%2520self-distillation%252C%2520this%2520method%2520enhances%2520the%2520performance%2520of%2520any%250Atrained%2520model%2520without%2520adding%2520inference%2520costs.%2520Extensive%2520empirical%2520evaluations%250Ademonstrate%2520the%2520efficacy%2520of%2520our%2520method%2520across%2520multiple%2520datasets%252C%2520contemporary%250Aneural%2520network%2520architectures%252C%2520and%2520training%2520protocols.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2412.12968v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=On%20Local%20Overfitting%20and%20Forgetting%20in%20Deep%20Neural%20Networks&entry.906535625=Uri%20Stern%20and%20Tomer%20Yaacoby%20and%20Daphna%20Weinshall&entry.1292438233=%20%20The%20infrequent%20occurrence%20of%20overfitting%20in%20deep%20neural%20networks%20is%0Aperplexing%3A%20contrary%20to%20theoretical%20expectations%2C%20increasing%20model%20size%20often%0Aenhances%20performance%20in%20practice.%20But%20what%20if%20overfitting%20does%20occur%2C%20though%0Arestricted%20to%20specific%20sub-regions%20of%20the%20data%20space%3F%20In%20this%20work%2C%20we%20propose%0Aa%20novel%20score%20that%20captures%20the%20forgetting%20rate%20of%20deep%20models%20on%20validation%0Adata.%20We%20posit%20that%20this%20score%20quantifies%20local%20overfitting%3A%20a%20decline%20in%0Aperformance%20confined%20to%20certain%20regions%20of%20the%20data%20space.%20We%20then%20show%0Aempirically%20that%20local%20overfitting%20occurs%20regardless%20of%20the%20presence%20of%0Atraditional%20overfitting.%20Using%20the%20framework%20of%20deep%20over-parametrized%20linear%0Amodels%2C%20we%20offer%20a%20certain%20theoretical%20characterization%20of%20forgotten%20knowledge%2C%0Aand%20show%20that%20it%20correlates%20with%20knowledge%20forgotten%20by%20real%20deep%20models.%0AFinally%2C%20we%20devise%20a%20new%20ensemble%20method%20that%20aims%20to%20recover%20forgotten%0Aknowledge%2C%20relying%20solely%20on%20the%20training%20history%20of%20a%20single%20network.%20When%0Acombined%20with%20self-distillation%2C%20this%20method%20enhances%20the%20performance%20of%20any%0Atrained%20model%20without%20adding%20inference%20costs.%20Extensive%20empirical%20evaluations%0Ademonstrate%20the%20efficacy%20of%20our%20method%20across%20multiple%20datasets%2C%20contemporary%0Aneural%20network%20architectures%2C%20and%20training%20protocols.%0A&entry.1838667208=http%3A//arxiv.org/abs/2412.12968v2&entry.124074799=Read"},
{"title": "Robust Gaussian Processes via Relevance Pursuit", "author": "Sebastian Ament and Elizabeth Santorella and David Eriksson and Ben Letham and Maximilian Balandat and Eytan Bakshy", "abstract": "  Gaussian processes (GPs) are non-parametric probabilistic regression models\nthat are popular due to their flexibility, data efficiency, and well-calibrated\nuncertainty estimates. However, standard GP models assume homoskedastic\nGaussian noise, while many real-world applications are subject to non-Gaussian\ncorruptions. Variants of GPs that are more robust to alternative noise models\nhave been proposed, and entail significant trade-offs between accuracy and\nrobustness, and between computational requirements and theoretical guarantees.\nIn this work, we propose and study a GP model that achieves robustness against\nsparse outliers by inferring data-point-specific noise levels with a sequential\nselection procedure maximizing the log marginal likelihood that we refer to as\nrelevance pursuit. We show, surprisingly, that the model can be parameterized\nsuch that the associated log marginal likelihood is strongly concave in the\ndata-point-specific noise variances, a property rarely found in either robust\nregression objectives or GP marginal likelihoods. This in turn implies the weak\nsubmodularity of the corresponding subset selection problem, and thereby proves\napproximation guarantees for the proposed algorithm. We compare the model's\nperformance relative to other approaches on diverse regression and Bayesian\noptimization tasks, including the challenging but common setting of sparse\ncorruptions of the labels within or close to the function range.\n", "link": "http://arxiv.org/abs/2410.24222v2", "date": "2025-01-07", "relevancy": 2.4628, "topK": [{"title": "RTG-SLAM: Real-time 3D Reconstruction at Scale using Gaussian Splatting", "link": "http://arxiv.org/abs/2404.19706v1", "similarity": 0.5021}, {"title": "Dynamic Gaussians Mesh: Consistent Mesh Reconstruction from Monocular\n  Videos", "link": "http://arxiv.org/abs/2404.12379v2", "similarity": 0.4931}, {"title": "MiraGe: Editable 2D Images using Gaussian Splatting", "link": "http://arxiv.org/abs/2410.01521v1", "similarity": 0.4825}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Robust%20Gaussian%20Processes%20via%20Relevance%20Pursuit&body=Title%3A%20Robust%20Gaussian%20Processes%20via%20Relevance%20Pursuit%0AAuthor%3A%20Sebastian%20Ament%20and%20Elizabeth%20Santorella%20and%20David%20Eriksson%20and%20Ben%20Letham%20and%20Maximilian%20Balandat%20and%20Eytan%20Bakshy%0AAbstract%3A%20%20%20Gaussian%20processes%20%28GPs%29%20are%20non-parametric%20probabilistic%20regression%20models%0Athat%20are%20popular%20due%20to%20their%20flexibility%2C%20data%20efficiency%2C%20and%20well-calibrated%0Auncertainty%20estimates.%20However%2C%20standard%20GP%20models%20assume%20homoskedastic%0AGaussian%20noise%2C%20while%20many%20real-world%20applications%20are%20subject%20to%20non-Gaussian%0Acorruptions.%20Variants%20of%20GPs%20that%20are%20more%20robust%20to%20alternative%20noise%20models%0Ahave%20been%20proposed%2C%20and%20entail%20significant%20trade-offs%20between%20accuracy%20and%0Arobustness%2C%20and%20between%20computational%20requirements%20and%20theoretical%20guarantees.%0AIn%20this%20work%2C%20we%20propose%20and%20study%20a%20GP%20model%20that%20achieves%20robustness%20against%0Asparse%20outliers%20by%20inferring%20data-point-specific%20noise%20levels%20with%20a%20sequential%0Aselection%20procedure%20maximizing%20the%20log%20marginal%20likelihood%20that%20we%20refer%20to%20as%0Arelevance%20pursuit.%20We%20show%2C%20surprisingly%2C%20that%20the%20model%20can%20be%20parameterized%0Asuch%20that%20the%20associated%20log%20marginal%20likelihood%20is%20strongly%20concave%20in%20the%0Adata-point-specific%20noise%20variances%2C%20a%20property%20rarely%20found%20in%20either%20robust%0Aregression%20objectives%20or%20GP%20marginal%20likelihoods.%20This%20in%20turn%20implies%20the%20weak%0Asubmodularity%20of%20the%20corresponding%20subset%20selection%20problem%2C%20and%20thereby%20proves%0Aapproximation%20guarantees%20for%20the%20proposed%20algorithm.%20We%20compare%20the%20model%27s%0Aperformance%20relative%20to%20other%20approaches%20on%20diverse%20regression%20and%20Bayesian%0Aoptimization%20tasks%2C%20including%20the%20challenging%20but%20common%20setting%20of%20sparse%0Acorruptions%20of%20the%20labels%20within%20or%20close%20to%20the%20function%20range.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2410.24222v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DRobust%2520Gaussian%2520Processes%2520via%2520Relevance%2520Pursuit%26entry.906535625%3DSebastian%2520Ament%2520and%2520Elizabeth%2520Santorella%2520and%2520David%2520Eriksson%2520and%2520Ben%2520Letham%2520and%2520Maximilian%2520Balandat%2520and%2520Eytan%2520Bakshy%26entry.1292438233%3D%2520%2520Gaussian%2520processes%2520%2528GPs%2529%2520are%2520non-parametric%2520probabilistic%2520regression%2520models%250Athat%2520are%2520popular%2520due%2520to%2520their%2520flexibility%252C%2520data%2520efficiency%252C%2520and%2520well-calibrated%250Auncertainty%2520estimates.%2520However%252C%2520standard%2520GP%2520models%2520assume%2520homoskedastic%250AGaussian%2520noise%252C%2520while%2520many%2520real-world%2520applications%2520are%2520subject%2520to%2520non-Gaussian%250Acorruptions.%2520Variants%2520of%2520GPs%2520that%2520are%2520more%2520robust%2520to%2520alternative%2520noise%2520models%250Ahave%2520been%2520proposed%252C%2520and%2520entail%2520significant%2520trade-offs%2520between%2520accuracy%2520and%250Arobustness%252C%2520and%2520between%2520computational%2520requirements%2520and%2520theoretical%2520guarantees.%250AIn%2520this%2520work%252C%2520we%2520propose%2520and%2520study%2520a%2520GP%2520model%2520that%2520achieves%2520robustness%2520against%250Asparse%2520outliers%2520by%2520inferring%2520data-point-specific%2520noise%2520levels%2520with%2520a%2520sequential%250Aselection%2520procedure%2520maximizing%2520the%2520log%2520marginal%2520likelihood%2520that%2520we%2520refer%2520to%2520as%250Arelevance%2520pursuit.%2520We%2520show%252C%2520surprisingly%252C%2520that%2520the%2520model%2520can%2520be%2520parameterized%250Asuch%2520that%2520the%2520associated%2520log%2520marginal%2520likelihood%2520is%2520strongly%2520concave%2520in%2520the%250Adata-point-specific%2520noise%2520variances%252C%2520a%2520property%2520rarely%2520found%2520in%2520either%2520robust%250Aregression%2520objectives%2520or%2520GP%2520marginal%2520likelihoods.%2520This%2520in%2520turn%2520implies%2520the%2520weak%250Asubmodularity%2520of%2520the%2520corresponding%2520subset%2520selection%2520problem%252C%2520and%2520thereby%2520proves%250Aapproximation%2520guarantees%2520for%2520the%2520proposed%2520algorithm.%2520We%2520compare%2520the%2520model%2527s%250Aperformance%2520relative%2520to%2520other%2520approaches%2520on%2520diverse%2520regression%2520and%2520Bayesian%250Aoptimization%2520tasks%252C%2520including%2520the%2520challenging%2520but%2520common%2520setting%2520of%2520sparse%250Acorruptions%2520of%2520the%2520labels%2520within%2520or%2520close%2520to%2520the%2520function%2520range.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2410.24222v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Robust%20Gaussian%20Processes%20via%20Relevance%20Pursuit&entry.906535625=Sebastian%20Ament%20and%20Elizabeth%20Santorella%20and%20David%20Eriksson%20and%20Ben%20Letham%20and%20Maximilian%20Balandat%20and%20Eytan%20Bakshy&entry.1292438233=%20%20Gaussian%20processes%20%28GPs%29%20are%20non-parametric%20probabilistic%20regression%20models%0Athat%20are%20popular%20due%20to%20their%20flexibility%2C%20data%20efficiency%2C%20and%20well-calibrated%0Auncertainty%20estimates.%20However%2C%20standard%20GP%20models%20assume%20homoskedastic%0AGaussian%20noise%2C%20while%20many%20real-world%20applications%20are%20subject%20to%20non-Gaussian%0Acorruptions.%20Variants%20of%20GPs%20that%20are%20more%20robust%20to%20alternative%20noise%20models%0Ahave%20been%20proposed%2C%20and%20entail%20significant%20trade-offs%20between%20accuracy%20and%0Arobustness%2C%20and%20between%20computational%20requirements%20and%20theoretical%20guarantees.%0AIn%20this%20work%2C%20we%20propose%20and%20study%20a%20GP%20model%20that%20achieves%20robustness%20against%0Asparse%20outliers%20by%20inferring%20data-point-specific%20noise%20levels%20with%20a%20sequential%0Aselection%20procedure%20maximizing%20the%20log%20marginal%20likelihood%20that%20we%20refer%20to%20as%0Arelevance%20pursuit.%20We%20show%2C%20surprisingly%2C%20that%20the%20model%20can%20be%20parameterized%0Asuch%20that%20the%20associated%20log%20marginal%20likelihood%20is%20strongly%20concave%20in%20the%0Adata-point-specific%20noise%20variances%2C%20a%20property%20rarely%20found%20in%20either%20robust%0Aregression%20objectives%20or%20GP%20marginal%20likelihoods.%20This%20in%20turn%20implies%20the%20weak%0Asubmodularity%20of%20the%20corresponding%20subset%20selection%20problem%2C%20and%20thereby%20proves%0Aapproximation%20guarantees%20for%20the%20proposed%20algorithm.%20We%20compare%20the%20model%27s%0Aperformance%20relative%20to%20other%20approaches%20on%20diverse%20regression%20and%20Bayesian%0Aoptimization%20tasks%2C%20including%20the%20challenging%20but%20common%20setting%20of%20sparse%0Acorruptions%20of%20the%20labels%20within%20or%20close%20to%20the%20function%20range.%0A&entry.1838667208=http%3A//arxiv.org/abs/2410.24222v2&entry.124074799=Read"},
{"title": "Continuously Learning New Words in Automatic Speech Recognition", "author": "Christian Huber and Alexander Waibel", "abstract": "  Despite recent advances, Automatic Speech Recognition (ASR) systems are still\nfar from perfect. Typical errors include acronyms, named entities, and\ndomain-specific special words for which little or no labeled data is available.\nTo address the problem of recognizing these words, we propose a self-supervised\ncontinual learning approach: Given the audio of a lecture talk with the\ncorresponding slides, we bias the model towards decoding new words from the\nslides by using a memory-enhanced ASR model from the literature. Then, we\nperform inference on the talk, collecting utterances that contain detected new\nwords into an adaptation data set. Continual learning is then performed by\ntraining adaptation weights added to the model on this data set. The whole\nprocedure is iterated for many talks. We show that with this approach, we\nobtain increasing performance on the new words when they occur more frequently\n(more than 80% recall) while preserving the general performance of the model.\n", "link": "http://arxiv.org/abs/2401.04482v3", "date": "2025-01-07", "relevancy": 2.4574, "topK": [{"title": "Breadcrumbs: Adversarial class-balanced sampling for long-tailed recognition", "link": "https://link.springer.com/chapter/10.1007/978-3-031-20053-3_37", "similarity": 0.4976}, {"title": "Self-supervised Photographic Image Layout Representation Learning", "link": "http://arxiv.org/abs/2403.03740v1", "similarity": 0.4886}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.4882}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Continuously%20Learning%20New%20Words%20in%20Automatic%20Speech%20Recognition&body=Title%3A%20Continuously%20Learning%20New%20Words%20in%20Automatic%20Speech%20Recognition%0AAuthor%3A%20Christian%20Huber%20and%20Alexander%20Waibel%0AAbstract%3A%20%20%20Despite%20recent%20advances%2C%20Automatic%20Speech%20Recognition%20%28ASR%29%20systems%20are%20still%0Afar%20from%20perfect.%20Typical%20errors%20include%20acronyms%2C%20named%20entities%2C%20and%0Adomain-specific%20special%20words%20for%20which%20little%20or%20no%20labeled%20data%20is%20available.%0ATo%20address%20the%20problem%20of%20recognizing%20these%20words%2C%20we%20propose%20a%20self-supervised%0Acontinual%20learning%20approach%3A%20Given%20the%20audio%20of%20a%20lecture%20talk%20with%20the%0Acorresponding%20slides%2C%20we%20bias%20the%20model%20towards%20decoding%20new%20words%20from%20the%0Aslides%20by%20using%20a%20memory-enhanced%20ASR%20model%20from%20the%20literature.%20Then%2C%20we%0Aperform%20inference%20on%20the%20talk%2C%20collecting%20utterances%20that%20contain%20detected%20new%0Awords%20into%20an%20adaptation%20data%20set.%20Continual%20learning%20is%20then%20performed%20by%0Atraining%20adaptation%20weights%20added%20to%20the%20model%20on%20this%20data%20set.%20The%20whole%0Aprocedure%20is%20iterated%20for%20many%20talks.%20We%20show%20that%20with%20this%20approach%2C%20we%0Aobtain%20increasing%20performance%20on%20the%20new%20words%20when%20they%20occur%20more%20frequently%0A%28more%20than%2080%25%20recall%29%20while%20preserving%20the%20general%20performance%20of%20the%20model.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2401.04482v3%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DContinuously%2520Learning%2520New%2520Words%2520in%2520Automatic%2520Speech%2520Recognition%26entry.906535625%3DChristian%2520Huber%2520and%2520Alexander%2520Waibel%26entry.1292438233%3D%2520%2520Despite%2520recent%2520advances%252C%2520Automatic%2520Speech%2520Recognition%2520%2528ASR%2529%2520systems%2520are%2520still%250Afar%2520from%2520perfect.%2520Typical%2520errors%2520include%2520acronyms%252C%2520named%2520entities%252C%2520and%250Adomain-specific%2520special%2520words%2520for%2520which%2520little%2520or%2520no%2520labeled%2520data%2520is%2520available.%250ATo%2520address%2520the%2520problem%2520of%2520recognizing%2520these%2520words%252C%2520we%2520propose%2520a%2520self-supervised%250Acontinual%2520learning%2520approach%253A%2520Given%2520the%2520audio%2520of%2520a%2520lecture%2520talk%2520with%2520the%250Acorresponding%2520slides%252C%2520we%2520bias%2520the%2520model%2520towards%2520decoding%2520new%2520words%2520from%2520the%250Aslides%2520by%2520using%2520a%2520memory-enhanced%2520ASR%2520model%2520from%2520the%2520literature.%2520Then%252C%2520we%250Aperform%2520inference%2520on%2520the%2520talk%252C%2520collecting%2520utterances%2520that%2520contain%2520detected%2520new%250Awords%2520into%2520an%2520adaptation%2520data%2520set.%2520Continual%2520learning%2520is%2520then%2520performed%2520by%250Atraining%2520adaptation%2520weights%2520added%2520to%2520the%2520model%2520on%2520this%2520data%2520set.%2520The%2520whole%250Aprocedure%2520is%2520iterated%2520for%2520many%2520talks.%2520We%2520show%2520that%2520with%2520this%2520approach%252C%2520we%250Aobtain%2520increasing%2520performance%2520on%2520the%2520new%2520words%2520when%2520they%2520occur%2520more%2520frequently%250A%2528more%2520than%252080%2525%2520recall%2529%2520while%2520preserving%2520the%2520general%2520performance%2520of%2520the%2520model.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2401.04482v3%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Continuously%20Learning%20New%20Words%20in%20Automatic%20Speech%20Recognition&entry.906535625=Christian%20Huber%20and%20Alexander%20Waibel&entry.1292438233=%20%20Despite%20recent%20advances%2C%20Automatic%20Speech%20Recognition%20%28ASR%29%20systems%20are%20still%0Afar%20from%20perfect.%20Typical%20errors%20include%20acronyms%2C%20named%20entities%2C%20and%0Adomain-specific%20special%20words%20for%20which%20little%20or%20no%20labeled%20data%20is%20available.%0ATo%20address%20the%20problem%20of%20recognizing%20these%20words%2C%20we%20propose%20a%20self-supervised%0Acontinual%20learning%20approach%3A%20Given%20the%20audio%20of%20a%20lecture%20talk%20with%20the%0Acorresponding%20slides%2C%20we%20bias%20the%20model%20towards%20decoding%20new%20words%20from%20the%0Aslides%20by%20using%20a%20memory-enhanced%20ASR%20model%20from%20the%20literature.%20Then%2C%20we%0Aperform%20inference%20on%20the%20talk%2C%20collecting%20utterances%20that%20contain%20detected%20new%0Awords%20into%20an%20adaptation%20data%20set.%20Continual%20learning%20is%20then%20performed%20by%0Atraining%20adaptation%20weights%20added%20to%20the%20model%20on%20this%20data%20set.%20The%20whole%0Aprocedure%20is%20iterated%20for%20many%20talks.%20We%20show%20that%20with%20this%20approach%2C%20we%0Aobtain%20increasing%20performance%20on%20the%20new%20words%20when%20they%20occur%20more%20frequently%0A%28more%20than%2080%25%20recall%29%20while%20preserving%20the%20general%20performance%20of%20the%20model.%0A&entry.1838667208=http%3A//arxiv.org/abs/2401.04482v3&entry.124074799=Read"},
{"title": "HYB-VITON: A Hybrid Approach to Virtual Try-On Combining Explicit and\n  Implicit Warping", "author": "Kosuke Takemoto and Takafumi Koshinaka", "abstract": "  Virtual try-on systems have significant potential in e-commerce, allowing\ncustomers to visualize garments on themselves. Existing image-based methods\nfall into two categories: those that directly warp garment-images onto\nperson-images (explicit warping), and those using cross-attention to\nreconstruct given garments (implicit warping). Explicit warping preserves\ngarment details but often produces unrealistic output, while implicit warping\nachieves natural reconstruction but struggles with fine details. We propose\nHYB-VITON, a novel approach that combines the advantages of each method and\nincludes both a preprocessing pipeline for warped garments and a novel training\noption. These components allow us to utilize beneficial regions of explicitly\nwarped garments while leveraging the natural reconstruction of implicit\nwarping. A series of experiments demonstrates that HYB-VITON preserves garment\ndetails more faithfully than recent diffusion-based methods, while producing\nmore realistic results than a state-of-the-art explicit warping method.\n", "link": "http://arxiv.org/abs/2501.03910v1", "date": "2025-01-07", "relevancy": 2.4545, "topK": [{"title": "FabricDiffusion: High-Fidelity Texture Transfer for 3D Garments\n  Generation from In-The-Wild Clothing Images", "link": "http://arxiv.org/abs/2410.01801v1", "similarity": 0.6188}, {"title": "VirtualModel: Generating Object-ID-retentive Human-object Interaction\n  Image by Diffusion Model for E-commerce Marketing", "link": "http://arxiv.org/abs/2405.09985v1", "similarity": 0.6158}, {"title": "Multimodal-Conditioned Latent Diffusion Models for Fashion Image Editing", "link": "http://arxiv.org/abs/2403.14828v2", "similarity": 0.5954}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20HYB-VITON%3A%20A%20Hybrid%20Approach%20to%20Virtual%20Try-On%20Combining%20Explicit%20and%0A%20%20Implicit%20Warping&body=Title%3A%20HYB-VITON%3A%20A%20Hybrid%20Approach%20to%20Virtual%20Try-On%20Combining%20Explicit%20and%0A%20%20Implicit%20Warping%0AAuthor%3A%20Kosuke%20Takemoto%20and%20Takafumi%20Koshinaka%0AAbstract%3A%20%20%20Virtual%20try-on%20systems%20have%20significant%20potential%20in%20e-commerce%2C%20allowing%0Acustomers%20to%20visualize%20garments%20on%20themselves.%20Existing%20image-based%20methods%0Afall%20into%20two%20categories%3A%20those%20that%20directly%20warp%20garment-images%20onto%0Aperson-images%20%28explicit%20warping%29%2C%20and%20those%20using%20cross-attention%20to%0Areconstruct%20given%20garments%20%28implicit%20warping%29.%20Explicit%20warping%20preserves%0Agarment%20details%20but%20often%20produces%20unrealistic%20output%2C%20while%20implicit%20warping%0Aachieves%20natural%20reconstruction%20but%20struggles%20with%20fine%20details.%20We%20propose%0AHYB-VITON%2C%20a%20novel%20approach%20that%20combines%20the%20advantages%20of%20each%20method%20and%0Aincludes%20both%20a%20preprocessing%20pipeline%20for%20warped%20garments%20and%20a%20novel%20training%0Aoption.%20These%20components%20allow%20us%20to%20utilize%20beneficial%20regions%20of%20explicitly%0Awarped%20garments%20while%20leveraging%20the%20natural%20reconstruction%20of%20implicit%0Awarping.%20A%20series%20of%20experiments%20demonstrates%20that%20HYB-VITON%20preserves%20garment%0Adetails%20more%20faithfully%20than%20recent%20diffusion-based%20methods%2C%20while%20producing%0Amore%20realistic%20results%20than%20a%20state-of-the-art%20explicit%20warping%20method.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2501.03910v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DHYB-VITON%253A%2520A%2520Hybrid%2520Approach%2520to%2520Virtual%2520Try-On%2520Combining%2520Explicit%2520and%250A%2520%2520Implicit%2520Warping%26entry.906535625%3DKosuke%2520Takemoto%2520and%2520Takafumi%2520Koshinaka%26entry.1292438233%3D%2520%2520Virtual%2520try-on%2520systems%2520have%2520significant%2520potential%2520in%2520e-commerce%252C%2520allowing%250Acustomers%2520to%2520visualize%2520garments%2520on%2520themselves.%2520Existing%2520image-based%2520methods%250Afall%2520into%2520two%2520categories%253A%2520those%2520that%2520directly%2520warp%2520garment-images%2520onto%250Aperson-images%2520%2528explicit%2520warping%2529%252C%2520and%2520those%2520using%2520cross-attention%2520to%250Areconstruct%2520given%2520garments%2520%2528implicit%2520warping%2529.%2520Explicit%2520warping%2520preserves%250Agarment%2520details%2520but%2520often%2520produces%2520unrealistic%2520output%252C%2520while%2520implicit%2520warping%250Aachieves%2520natural%2520reconstruction%2520but%2520struggles%2520with%2520fine%2520details.%2520We%2520propose%250AHYB-VITON%252C%2520a%2520novel%2520approach%2520that%2520combines%2520the%2520advantages%2520of%2520each%2520method%2520and%250Aincludes%2520both%2520a%2520preprocessing%2520pipeline%2520for%2520warped%2520garments%2520and%2520a%2520novel%2520training%250Aoption.%2520These%2520components%2520allow%2520us%2520to%2520utilize%2520beneficial%2520regions%2520of%2520explicitly%250Awarped%2520garments%2520while%2520leveraging%2520the%2520natural%2520reconstruction%2520of%2520implicit%250Awarping.%2520A%2520series%2520of%2520experiments%2520demonstrates%2520that%2520HYB-VITON%2520preserves%2520garment%250Adetails%2520more%2520faithfully%2520than%2520recent%2520diffusion-based%2520methods%252C%2520while%2520producing%250Amore%2520realistic%2520results%2520than%2520a%2520state-of-the-art%2520explicit%2520warping%2520method.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2501.03910v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=HYB-VITON%3A%20A%20Hybrid%20Approach%20to%20Virtual%20Try-On%20Combining%20Explicit%20and%0A%20%20Implicit%20Warping&entry.906535625=Kosuke%20Takemoto%20and%20Takafumi%20Koshinaka&entry.1292438233=%20%20Virtual%20try-on%20systems%20have%20significant%20potential%20in%20e-commerce%2C%20allowing%0Acustomers%20to%20visualize%20garments%20on%20themselves.%20Existing%20image-based%20methods%0Afall%20into%20two%20categories%3A%20those%20that%20directly%20warp%20garment-images%20onto%0Aperson-images%20%28explicit%20warping%29%2C%20and%20those%20using%20cross-attention%20to%0Areconstruct%20given%20garments%20%28implicit%20warping%29.%20Explicit%20warping%20preserves%0Agarment%20details%20but%20often%20produces%20unrealistic%20output%2C%20while%20implicit%20warping%0Aachieves%20natural%20reconstruction%20but%20struggles%20with%20fine%20details.%20We%20propose%0AHYB-VITON%2C%20a%20novel%20approach%20that%20combines%20the%20advantages%20of%20each%20method%20and%0Aincludes%20both%20a%20preprocessing%20pipeline%20for%20warped%20garments%20and%20a%20novel%20training%0Aoption.%20These%20components%20allow%20us%20to%20utilize%20beneficial%20regions%20of%20explicitly%0Awarped%20garments%20while%20leveraging%20the%20natural%20reconstruction%20of%20implicit%0Awarping.%20A%20series%20of%20experiments%20demonstrates%20that%20HYB-VITON%20preserves%20garment%0Adetails%20more%20faithfully%20than%20recent%20diffusion-based%20methods%2C%20while%20producing%0Amore%20realistic%20results%20than%20a%20state-of-the-art%20explicit%20warping%20method.%0A&entry.1838667208=http%3A//arxiv.org/abs/2501.03910v1&entry.124074799=Read"},
{"title": "GraphLoRA: Structure-Aware Contrastive Low-Rank Adaptation for\n  Cross-Graph Transfer Learning", "author": "Zhe-Rui Yang and Jindong Han and Chang-Dong Wang and Hao Liu", "abstract": "  Graph Neural Networks (GNNs) have demonstrated remarkable proficiency in\nhandling a range of graph analytical tasks across various domains, such as\ne-commerce and social networks. Despite their versatility, GNNs face\nsignificant challenges in transferability, limiting their utility in real-world\napplications. Existing research in GNN transfer learning overlooks\ndiscrepancies in distribution among various graph datasets, facing challenges\nwhen transferring across different distributions. How to effectively adopt a\nwell-trained GNN to new graphs with varying feature and structural\ndistributions remains an under-explored problem. Taking inspiration from the\nsuccess of Low-Rank Adaptation (LoRA) in adapting large language models to\nvarious domains, we propose GraphLoRA, an effective and parameter-efficient\nmethod for transferring well-trained GNNs to diverse graph domains.\nSpecifically, we first propose a Structure-aware Maximum Mean Discrepancy\n(SMMD) to align divergent node feature distributions across source and target\ngraphs. Moreover, we introduce low-rank adaptation by injecting a small\ntrainable GNN alongside the pre-trained one, effectively bridging structural\ndistribution gaps while mitigating the catastrophic forgetting. Additionally, a\nstructure-aware regularization objective is proposed to enhance the\nadaptability of the pre-trained GNN to target graph with scarce supervision\nlabels. Extensive experiments on eight real-world datasets demonstrate the\neffectiveness of GraphLoRA against fourteen baselines by tuning only 20% of\nparameters, even across disparate graph domains. The code is available at\nhttps://github.com/AllminerLab/GraphLoRA.\n", "link": "http://arxiv.org/abs/2409.16670v2", "date": "2025-01-07", "relevancy": 2.4294, "topK": [{"title": "Self-supervised Photographic Image Layout Representation Learning", "link": "http://arxiv.org/abs/2403.03740v1", "similarity": 0.5284}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.4733}, {"title": "RTG-SLAM: Real-time 3D Reconstruction at Scale using Gaussian Splatting", "link": "http://arxiv.org/abs/2404.19706v1", "similarity": 0.456}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20GraphLoRA%3A%20Structure-Aware%20Contrastive%20Low-Rank%20Adaptation%20for%0A%20%20Cross-Graph%20Transfer%20Learning&body=Title%3A%20GraphLoRA%3A%20Structure-Aware%20Contrastive%20Low-Rank%20Adaptation%20for%0A%20%20Cross-Graph%20Transfer%20Learning%0AAuthor%3A%20Zhe-Rui%20Yang%20and%20Jindong%20Han%20and%20Chang-Dong%20Wang%20and%20Hao%20Liu%0AAbstract%3A%20%20%20Graph%20Neural%20Networks%20%28GNNs%29%20have%20demonstrated%20remarkable%20proficiency%20in%0Ahandling%20a%20range%20of%20graph%20analytical%20tasks%20across%20various%20domains%2C%20such%20as%0Ae-commerce%20and%20social%20networks.%20Despite%20their%20versatility%2C%20GNNs%20face%0Asignificant%20challenges%20in%20transferability%2C%20limiting%20their%20utility%20in%20real-world%0Aapplications.%20Existing%20research%20in%20GNN%20transfer%20learning%20overlooks%0Adiscrepancies%20in%20distribution%20among%20various%20graph%20datasets%2C%20facing%20challenges%0Awhen%20transferring%20across%20different%20distributions.%20How%20to%20effectively%20adopt%20a%0Awell-trained%20GNN%20to%20new%20graphs%20with%20varying%20feature%20and%20structural%0Adistributions%20remains%20an%20under-explored%20problem.%20Taking%20inspiration%20from%20the%0Asuccess%20of%20Low-Rank%20Adaptation%20%28LoRA%29%20in%20adapting%20large%20language%20models%20to%0Avarious%20domains%2C%20we%20propose%20GraphLoRA%2C%20an%20effective%20and%20parameter-efficient%0Amethod%20for%20transferring%20well-trained%20GNNs%20to%20diverse%20graph%20domains.%0ASpecifically%2C%20we%20first%20propose%20a%20Structure-aware%20Maximum%20Mean%20Discrepancy%0A%28SMMD%29%20to%20align%20divergent%20node%20feature%20distributions%20across%20source%20and%20target%0Agraphs.%20Moreover%2C%20we%20introduce%20low-rank%20adaptation%20by%20injecting%20a%20small%0Atrainable%20GNN%20alongside%20the%20pre-trained%20one%2C%20effectively%20bridging%20structural%0Adistribution%20gaps%20while%20mitigating%20the%20catastrophic%20forgetting.%20Additionally%2C%20a%0Astructure-aware%20regularization%20objective%20is%20proposed%20to%20enhance%20the%0Aadaptability%20of%20the%20pre-trained%20GNN%20to%20target%20graph%20with%20scarce%20supervision%0Alabels.%20Extensive%20experiments%20on%20eight%20real-world%20datasets%20demonstrate%20the%0Aeffectiveness%20of%20GraphLoRA%20against%20fourteen%20baselines%20by%20tuning%20only%2020%25%20of%0Aparameters%2C%20even%20across%20disparate%20graph%20domains.%20The%20code%20is%20available%20at%0Ahttps%3A//github.com/AllminerLab/GraphLoRA.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2409.16670v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DGraphLoRA%253A%2520Structure-Aware%2520Contrastive%2520Low-Rank%2520Adaptation%2520for%250A%2520%2520Cross-Graph%2520Transfer%2520Learning%26entry.906535625%3DZhe-Rui%2520Yang%2520and%2520Jindong%2520Han%2520and%2520Chang-Dong%2520Wang%2520and%2520Hao%2520Liu%26entry.1292438233%3D%2520%2520Graph%2520Neural%2520Networks%2520%2528GNNs%2529%2520have%2520demonstrated%2520remarkable%2520proficiency%2520in%250Ahandling%2520a%2520range%2520of%2520graph%2520analytical%2520tasks%2520across%2520various%2520domains%252C%2520such%2520as%250Ae-commerce%2520and%2520social%2520networks.%2520Despite%2520their%2520versatility%252C%2520GNNs%2520face%250Asignificant%2520challenges%2520in%2520transferability%252C%2520limiting%2520their%2520utility%2520in%2520real-world%250Aapplications.%2520Existing%2520research%2520in%2520GNN%2520transfer%2520learning%2520overlooks%250Adiscrepancies%2520in%2520distribution%2520among%2520various%2520graph%2520datasets%252C%2520facing%2520challenges%250Awhen%2520transferring%2520across%2520different%2520distributions.%2520How%2520to%2520effectively%2520adopt%2520a%250Awell-trained%2520GNN%2520to%2520new%2520graphs%2520with%2520varying%2520feature%2520and%2520structural%250Adistributions%2520remains%2520an%2520under-explored%2520problem.%2520Taking%2520inspiration%2520from%2520the%250Asuccess%2520of%2520Low-Rank%2520Adaptation%2520%2528LoRA%2529%2520in%2520adapting%2520large%2520language%2520models%2520to%250Avarious%2520domains%252C%2520we%2520propose%2520GraphLoRA%252C%2520an%2520effective%2520and%2520parameter-efficient%250Amethod%2520for%2520transferring%2520well-trained%2520GNNs%2520to%2520diverse%2520graph%2520domains.%250ASpecifically%252C%2520we%2520first%2520propose%2520a%2520Structure-aware%2520Maximum%2520Mean%2520Discrepancy%250A%2528SMMD%2529%2520to%2520align%2520divergent%2520node%2520feature%2520distributions%2520across%2520source%2520and%2520target%250Agraphs.%2520Moreover%252C%2520we%2520introduce%2520low-rank%2520adaptation%2520by%2520injecting%2520a%2520small%250Atrainable%2520GNN%2520alongside%2520the%2520pre-trained%2520one%252C%2520effectively%2520bridging%2520structural%250Adistribution%2520gaps%2520while%2520mitigating%2520the%2520catastrophic%2520forgetting.%2520Additionally%252C%2520a%250Astructure-aware%2520regularization%2520objective%2520is%2520proposed%2520to%2520enhance%2520the%250Aadaptability%2520of%2520the%2520pre-trained%2520GNN%2520to%2520target%2520graph%2520with%2520scarce%2520supervision%250Alabels.%2520Extensive%2520experiments%2520on%2520eight%2520real-world%2520datasets%2520demonstrate%2520the%250Aeffectiveness%2520of%2520GraphLoRA%2520against%2520fourteen%2520baselines%2520by%2520tuning%2520only%252020%2525%2520of%250Aparameters%252C%2520even%2520across%2520disparate%2520graph%2520domains.%2520The%2520code%2520is%2520available%2520at%250Ahttps%253A//github.com/AllminerLab/GraphLoRA.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2409.16670v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=GraphLoRA%3A%20Structure-Aware%20Contrastive%20Low-Rank%20Adaptation%20for%0A%20%20Cross-Graph%20Transfer%20Learning&entry.906535625=Zhe-Rui%20Yang%20and%20Jindong%20Han%20and%20Chang-Dong%20Wang%20and%20Hao%20Liu&entry.1292438233=%20%20Graph%20Neural%20Networks%20%28GNNs%29%20have%20demonstrated%20remarkable%20proficiency%20in%0Ahandling%20a%20range%20of%20graph%20analytical%20tasks%20across%20various%20domains%2C%20such%20as%0Ae-commerce%20and%20social%20networks.%20Despite%20their%20versatility%2C%20GNNs%20face%0Asignificant%20challenges%20in%20transferability%2C%20limiting%20their%20utility%20in%20real-world%0Aapplications.%20Existing%20research%20in%20GNN%20transfer%20learning%20overlooks%0Adiscrepancies%20in%20distribution%20among%20various%20graph%20datasets%2C%20facing%20challenges%0Awhen%20transferring%20across%20different%20distributions.%20How%20to%20effectively%20adopt%20a%0Awell-trained%20GNN%20to%20new%20graphs%20with%20varying%20feature%20and%20structural%0Adistributions%20remains%20an%20under-explored%20problem.%20Taking%20inspiration%20from%20the%0Asuccess%20of%20Low-Rank%20Adaptation%20%28LoRA%29%20in%20adapting%20large%20language%20models%20to%0Avarious%20domains%2C%20we%20propose%20GraphLoRA%2C%20an%20effective%20and%20parameter-efficient%0Amethod%20for%20transferring%20well-trained%20GNNs%20to%20diverse%20graph%20domains.%0ASpecifically%2C%20we%20first%20propose%20a%20Structure-aware%20Maximum%20Mean%20Discrepancy%0A%28SMMD%29%20to%20align%20divergent%20node%20feature%20distributions%20across%20source%20and%20target%0Agraphs.%20Moreover%2C%20we%20introduce%20low-rank%20adaptation%20by%20injecting%20a%20small%0Atrainable%20GNN%20alongside%20the%20pre-trained%20one%2C%20effectively%20bridging%20structural%0Adistribution%20gaps%20while%20mitigating%20the%20catastrophic%20forgetting.%20Additionally%2C%20a%0Astructure-aware%20regularization%20objective%20is%20proposed%20to%20enhance%20the%0Aadaptability%20of%20the%20pre-trained%20GNN%20to%20target%20graph%20with%20scarce%20supervision%0Alabels.%20Extensive%20experiments%20on%20eight%20real-world%20datasets%20demonstrate%20the%0Aeffectiveness%20of%20GraphLoRA%20against%20fourteen%20baselines%20by%20tuning%20only%2020%25%20of%0Aparameters%2C%20even%20across%20disparate%20graph%20domains.%20The%20code%20is%20available%20at%0Ahttps%3A//github.com/AllminerLab/GraphLoRA.%0A&entry.1838667208=http%3A//arxiv.org/abs/2409.16670v2&entry.124074799=Read"},
{"title": "3D Annotation-Free Learning by Distilling 2D Open-Vocabulary\n  Segmentation Models for Autonomous Driving", "author": "Boyi Sun and Yuhang Liu and Xingxia Wang and Bin Tian and Long Chen and Fei-Yue Wang", "abstract": "  Point cloud data labeling is considered a time-consuming and expensive task\nin autonomous driving, whereas annotation-free learning training can avoid it\nby learning point cloud representations from unannotated data. In this paper,\nwe propose AFOV, a novel 3D \\textbf{A}nnotation-\\textbf{F}ree framework\nassisted by 2D \\textbf{O}pen-\\textbf{V}ocabulary segmentation models. It\nconsists of two stages: In the first stage, we innovatively integrate\nhigh-quality textual and image features of 2D open-vocabulary models and\npropose the Tri-Modal contrastive Pre-training (TMP). In the second stage,\nspatial mapping between point clouds and images is utilized to generate\npseudo-labels, enabling cross-modal knowledge distillation. Besides, we\nintroduce the Approximate Flat Interaction (AFI) to address the noise during\nalignment and label confusion. To validate the superiority of AFOV, extensive\nexperiments are conducted on multiple related datasets. We achieved a\nrecord-breaking 47.73\\% mIoU on the annotation-free 3D segmentation task in\nnuScenes, surpassing the previous best model by 3.13\\% mIoU. Meanwhile, the\nperformance of fine-tuning with 1\\% data on nuScenes and SemanticKITTI reached\na remarkable 51.75\\% mIoU and 48.14\\% mIoU, outperforming all previous\npre-trained models\n", "link": "http://arxiv.org/abs/2405.15286v3", "date": "2025-01-07", "relevancy": 2.4275, "topK": [{"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.6165}, {"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.6074}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.597}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%203D%20Annotation-Free%20Learning%20by%20Distilling%202D%20Open-Vocabulary%0A%20%20Segmentation%20Models%20for%20Autonomous%20Driving&body=Title%3A%203D%20Annotation-Free%20Learning%20by%20Distilling%202D%20Open-Vocabulary%0A%20%20Segmentation%20Models%20for%20Autonomous%20Driving%0AAuthor%3A%20Boyi%20Sun%20and%20Yuhang%20Liu%20and%20Xingxia%20Wang%20and%20Bin%20Tian%20and%20Long%20Chen%20and%20Fei-Yue%20Wang%0AAbstract%3A%20%20%20Point%20cloud%20data%20labeling%20is%20considered%20a%20time-consuming%20and%20expensive%20task%0Ain%20autonomous%20driving%2C%20whereas%20annotation-free%20learning%20training%20can%20avoid%20it%0Aby%20learning%20point%20cloud%20representations%20from%20unannotated%20data.%20In%20this%20paper%2C%0Awe%20propose%20AFOV%2C%20a%20novel%203D%20%5Ctextbf%7BA%7Dnnotation-%5Ctextbf%7BF%7Dree%20framework%0Aassisted%20by%202D%20%5Ctextbf%7BO%7Dpen-%5Ctextbf%7BV%7Docabulary%20segmentation%20models.%20It%0Aconsists%20of%20two%20stages%3A%20In%20the%20first%20stage%2C%20we%20innovatively%20integrate%0Ahigh-quality%20textual%20and%20image%20features%20of%202D%20open-vocabulary%20models%20and%0Apropose%20the%20Tri-Modal%20contrastive%20Pre-training%20%28TMP%29.%20In%20the%20second%20stage%2C%0Aspatial%20mapping%20between%20point%20clouds%20and%20images%20is%20utilized%20to%20generate%0Apseudo-labels%2C%20enabling%20cross-modal%20knowledge%20distillation.%20Besides%2C%20we%0Aintroduce%20the%20Approximate%20Flat%20Interaction%20%28AFI%29%20to%20address%20the%20noise%20during%0Aalignment%20and%20label%20confusion.%20To%20validate%20the%20superiority%20of%20AFOV%2C%20extensive%0Aexperiments%20are%20conducted%20on%20multiple%20related%20datasets.%20We%20achieved%20a%0Arecord-breaking%2047.73%5C%25%20mIoU%20on%20the%20annotation-free%203D%20segmentation%20task%20in%0AnuScenes%2C%20surpassing%20the%20previous%20best%20model%20by%203.13%5C%25%20mIoU.%20Meanwhile%2C%20the%0Aperformance%20of%20fine-tuning%20with%201%5C%25%20data%20on%20nuScenes%20and%20SemanticKITTI%20reached%0Aa%20remarkable%2051.75%5C%25%20mIoU%20and%2048.14%5C%25%20mIoU%2C%20outperforming%20all%20previous%0Apre-trained%20models%0A%0ALink%3A%20http%3A//arxiv.org/abs/2405.15286v3%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3D3D%2520Annotation-Free%2520Learning%2520by%2520Distilling%25202D%2520Open-Vocabulary%250A%2520%2520Segmentation%2520Models%2520for%2520Autonomous%2520Driving%26entry.906535625%3DBoyi%2520Sun%2520and%2520Yuhang%2520Liu%2520and%2520Xingxia%2520Wang%2520and%2520Bin%2520Tian%2520and%2520Long%2520Chen%2520and%2520Fei-Yue%2520Wang%26entry.1292438233%3D%2520%2520Point%2520cloud%2520data%2520labeling%2520is%2520considered%2520a%2520time-consuming%2520and%2520expensive%2520task%250Ain%2520autonomous%2520driving%252C%2520whereas%2520annotation-free%2520learning%2520training%2520can%2520avoid%2520it%250Aby%2520learning%2520point%2520cloud%2520representations%2520from%2520unannotated%2520data.%2520In%2520this%2520paper%252C%250Awe%2520propose%2520AFOV%252C%2520a%2520novel%25203D%2520%255Ctextbf%257BA%257Dnnotation-%255Ctextbf%257BF%257Dree%2520framework%250Aassisted%2520by%25202D%2520%255Ctextbf%257BO%257Dpen-%255Ctextbf%257BV%257Docabulary%2520segmentation%2520models.%2520It%250Aconsists%2520of%2520two%2520stages%253A%2520In%2520the%2520first%2520stage%252C%2520we%2520innovatively%2520integrate%250Ahigh-quality%2520textual%2520and%2520image%2520features%2520of%25202D%2520open-vocabulary%2520models%2520and%250Apropose%2520the%2520Tri-Modal%2520contrastive%2520Pre-training%2520%2528TMP%2529.%2520In%2520the%2520second%2520stage%252C%250Aspatial%2520mapping%2520between%2520point%2520clouds%2520and%2520images%2520is%2520utilized%2520to%2520generate%250Apseudo-labels%252C%2520enabling%2520cross-modal%2520knowledge%2520distillation.%2520Besides%252C%2520we%250Aintroduce%2520the%2520Approximate%2520Flat%2520Interaction%2520%2528AFI%2529%2520to%2520address%2520the%2520noise%2520during%250Aalignment%2520and%2520label%2520confusion.%2520To%2520validate%2520the%2520superiority%2520of%2520AFOV%252C%2520extensive%250Aexperiments%2520are%2520conducted%2520on%2520multiple%2520related%2520datasets.%2520We%2520achieved%2520a%250Arecord-breaking%252047.73%255C%2525%2520mIoU%2520on%2520the%2520annotation-free%25203D%2520segmentation%2520task%2520in%250AnuScenes%252C%2520surpassing%2520the%2520previous%2520best%2520model%2520by%25203.13%255C%2525%2520mIoU.%2520Meanwhile%252C%2520the%250Aperformance%2520of%2520fine-tuning%2520with%25201%255C%2525%2520data%2520on%2520nuScenes%2520and%2520SemanticKITTI%2520reached%250Aa%2520remarkable%252051.75%255C%2525%2520mIoU%2520and%252048.14%255C%2525%2520mIoU%252C%2520outperforming%2520all%2520previous%250Apre-trained%2520models%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2405.15286v3%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=3D%20Annotation-Free%20Learning%20by%20Distilling%202D%20Open-Vocabulary%0A%20%20Segmentation%20Models%20for%20Autonomous%20Driving&entry.906535625=Boyi%20Sun%20and%20Yuhang%20Liu%20and%20Xingxia%20Wang%20and%20Bin%20Tian%20and%20Long%20Chen%20and%20Fei-Yue%20Wang&entry.1292438233=%20%20Point%20cloud%20data%20labeling%20is%20considered%20a%20time-consuming%20and%20expensive%20task%0Ain%20autonomous%20driving%2C%20whereas%20annotation-free%20learning%20training%20can%20avoid%20it%0Aby%20learning%20point%20cloud%20representations%20from%20unannotated%20data.%20In%20this%20paper%2C%0Awe%20propose%20AFOV%2C%20a%20novel%203D%20%5Ctextbf%7BA%7Dnnotation-%5Ctextbf%7BF%7Dree%20framework%0Aassisted%20by%202D%20%5Ctextbf%7BO%7Dpen-%5Ctextbf%7BV%7Docabulary%20segmentation%20models.%20It%0Aconsists%20of%20two%20stages%3A%20In%20the%20first%20stage%2C%20we%20innovatively%20integrate%0Ahigh-quality%20textual%20and%20image%20features%20of%202D%20open-vocabulary%20models%20and%0Apropose%20the%20Tri-Modal%20contrastive%20Pre-training%20%28TMP%29.%20In%20the%20second%20stage%2C%0Aspatial%20mapping%20between%20point%20clouds%20and%20images%20is%20utilized%20to%20generate%0Apseudo-labels%2C%20enabling%20cross-modal%20knowledge%20distillation.%20Besides%2C%20we%0Aintroduce%20the%20Approximate%20Flat%20Interaction%20%28AFI%29%20to%20address%20the%20noise%20during%0Aalignment%20and%20label%20confusion.%20To%20validate%20the%20superiority%20of%20AFOV%2C%20extensive%0Aexperiments%20are%20conducted%20on%20multiple%20related%20datasets.%20We%20achieved%20a%0Arecord-breaking%2047.73%5C%25%20mIoU%20on%20the%20annotation-free%203D%20segmentation%20task%20in%0AnuScenes%2C%20surpassing%20the%20previous%20best%20model%20by%203.13%5C%25%20mIoU.%20Meanwhile%2C%20the%0Aperformance%20of%20fine-tuning%20with%201%5C%25%20data%20on%20nuScenes%20and%20SemanticKITTI%20reached%0Aa%20remarkable%2051.75%5C%25%20mIoU%20and%2048.14%5C%25%20mIoU%2C%20outperforming%20all%20previous%0Apre-trained%20models%0A&entry.1838667208=http%3A//arxiv.org/abs/2405.15286v3&entry.124074799=Read"},
{"title": "TACLR: A Scalable and Efficient Retrieval-based Method for Industrial\n  Product Attribute Value Identification", "author": "Yindu Su and Huike Zou and Lin Sun and Ting Zhang and Haiyang Yang and Liyu Chen and David Lo and Qingheng Zhang and Shuguang Han and Jufeng Chen", "abstract": "  Product Attribute Value Identification (PAVI) involves identifying attribute\nvalues from product profiles, a key task for improving product search,\nrecommendations, and business analytics on e-commerce platforms. However,\nexisting PAVI methods face critical challenges, such as inferring implicit\nvalues, handling out-of-distribution (OOD) values, and producing normalized\noutputs. To address these limitations, we introduce Taxonomy-Aware Contrastive\nLearning Retrieval (TACLR), the first retrieval-based method for PAVI. TACLR\nformulates PAVI as an information retrieval task by encoding product profiles\nand candidate values into embeddings and retrieving values based on their\nsimilarity to the item embedding. It leverages contrastive training with\ntaxonomy-aware hard negative sampling and employs adaptive inference with\ndynamic thresholds. TACLR offers three key advantages: (1) it effectively\nhandles implicit and OOD values while producing normalized outputs; (2) it\nscales to thousands of categories, tens of thousands of attributes, and\nmillions of values; and (3) it supports efficient inference for high-load\nindustrial scenarios. Extensive experiments on proprietary and public datasets\nvalidate the effectiveness and efficiency of TACLR. Moreover, it has been\nsuccessfully deployed in a real-world e-commerce platform, processing millions\nof product listings daily while supporting dynamic, large-scale attribute\ntaxonomies.\n", "link": "http://arxiv.org/abs/2501.03835v1", "date": "2025-01-07", "relevancy": 2.4155, "topK": [{"title": "VirtualModel: Generating Object-ID-retentive Human-object Interaction\n  Image by Diffusion Model for E-commerce Marketing", "link": "http://arxiv.org/abs/2405.09985v1", "similarity": 0.5044}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.4824}, {"title": "FabricDiffusion: High-Fidelity Texture Transfer for 3D Garments\n  Generation from In-The-Wild Clothing Images", "link": "http://arxiv.org/abs/2410.01801v1", "similarity": 0.4625}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20TACLR%3A%20A%20Scalable%20and%20Efficient%20Retrieval-based%20Method%20for%20Industrial%0A%20%20Product%20Attribute%20Value%20Identification&body=Title%3A%20TACLR%3A%20A%20Scalable%20and%20Efficient%20Retrieval-based%20Method%20for%20Industrial%0A%20%20Product%20Attribute%20Value%20Identification%0AAuthor%3A%20Yindu%20Su%20and%20Huike%20Zou%20and%20Lin%20Sun%20and%20Ting%20Zhang%20and%20Haiyang%20Yang%20and%20Liyu%20Chen%20and%20David%20Lo%20and%20Qingheng%20Zhang%20and%20Shuguang%20Han%20and%20Jufeng%20Chen%0AAbstract%3A%20%20%20Product%20Attribute%20Value%20Identification%20%28PAVI%29%20involves%20identifying%20attribute%0Avalues%20from%20product%20profiles%2C%20a%20key%20task%20for%20improving%20product%20search%2C%0Arecommendations%2C%20and%20business%20analytics%20on%20e-commerce%20platforms.%20However%2C%0Aexisting%20PAVI%20methods%20face%20critical%20challenges%2C%20such%20as%20inferring%20implicit%0Avalues%2C%20handling%20out-of-distribution%20%28OOD%29%20values%2C%20and%20producing%20normalized%0Aoutputs.%20To%20address%20these%20limitations%2C%20we%20introduce%20Taxonomy-Aware%20Contrastive%0ALearning%20Retrieval%20%28TACLR%29%2C%20the%20first%20retrieval-based%20method%20for%20PAVI.%20TACLR%0Aformulates%20PAVI%20as%20an%20information%20retrieval%20task%20by%20encoding%20product%20profiles%0Aand%20candidate%20values%20into%20embeddings%20and%20retrieving%20values%20based%20on%20their%0Asimilarity%20to%20the%20item%20embedding.%20It%20leverages%20contrastive%20training%20with%0Ataxonomy-aware%20hard%20negative%20sampling%20and%20employs%20adaptive%20inference%20with%0Adynamic%20thresholds.%20TACLR%20offers%20three%20key%20advantages%3A%20%281%29%20it%20effectively%0Ahandles%20implicit%20and%20OOD%20values%20while%20producing%20normalized%20outputs%3B%20%282%29%20it%0Ascales%20to%20thousands%20of%20categories%2C%20tens%20of%20thousands%20of%20attributes%2C%20and%0Amillions%20of%20values%3B%20and%20%283%29%20it%20supports%20efficient%20inference%20for%20high-load%0Aindustrial%20scenarios.%20Extensive%20experiments%20on%20proprietary%20and%20public%20datasets%0Avalidate%20the%20effectiveness%20and%20efficiency%20of%20TACLR.%20Moreover%2C%20it%20has%20been%0Asuccessfully%20deployed%20in%20a%20real-world%20e-commerce%20platform%2C%20processing%20millions%0Aof%20product%20listings%20daily%20while%20supporting%20dynamic%2C%20large-scale%20attribute%0Ataxonomies.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2501.03835v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DTACLR%253A%2520A%2520Scalable%2520and%2520Efficient%2520Retrieval-based%2520Method%2520for%2520Industrial%250A%2520%2520Product%2520Attribute%2520Value%2520Identification%26entry.906535625%3DYindu%2520Su%2520and%2520Huike%2520Zou%2520and%2520Lin%2520Sun%2520and%2520Ting%2520Zhang%2520and%2520Haiyang%2520Yang%2520and%2520Liyu%2520Chen%2520and%2520David%2520Lo%2520and%2520Qingheng%2520Zhang%2520and%2520Shuguang%2520Han%2520and%2520Jufeng%2520Chen%26entry.1292438233%3D%2520%2520Product%2520Attribute%2520Value%2520Identification%2520%2528PAVI%2529%2520involves%2520identifying%2520attribute%250Avalues%2520from%2520product%2520profiles%252C%2520a%2520key%2520task%2520for%2520improving%2520product%2520search%252C%250Arecommendations%252C%2520and%2520business%2520analytics%2520on%2520e-commerce%2520platforms.%2520However%252C%250Aexisting%2520PAVI%2520methods%2520face%2520critical%2520challenges%252C%2520such%2520as%2520inferring%2520implicit%250Avalues%252C%2520handling%2520out-of-distribution%2520%2528OOD%2529%2520values%252C%2520and%2520producing%2520normalized%250Aoutputs.%2520To%2520address%2520these%2520limitations%252C%2520we%2520introduce%2520Taxonomy-Aware%2520Contrastive%250ALearning%2520Retrieval%2520%2528TACLR%2529%252C%2520the%2520first%2520retrieval-based%2520method%2520for%2520PAVI.%2520TACLR%250Aformulates%2520PAVI%2520as%2520an%2520information%2520retrieval%2520task%2520by%2520encoding%2520product%2520profiles%250Aand%2520candidate%2520values%2520into%2520embeddings%2520and%2520retrieving%2520values%2520based%2520on%2520their%250Asimilarity%2520to%2520the%2520item%2520embedding.%2520It%2520leverages%2520contrastive%2520training%2520with%250Ataxonomy-aware%2520hard%2520negative%2520sampling%2520and%2520employs%2520adaptive%2520inference%2520with%250Adynamic%2520thresholds.%2520TACLR%2520offers%2520three%2520key%2520advantages%253A%2520%25281%2529%2520it%2520effectively%250Ahandles%2520implicit%2520and%2520OOD%2520values%2520while%2520producing%2520normalized%2520outputs%253B%2520%25282%2529%2520it%250Ascales%2520to%2520thousands%2520of%2520categories%252C%2520tens%2520of%2520thousands%2520of%2520attributes%252C%2520and%250Amillions%2520of%2520values%253B%2520and%2520%25283%2529%2520it%2520supports%2520efficient%2520inference%2520for%2520high-load%250Aindustrial%2520scenarios.%2520Extensive%2520experiments%2520on%2520proprietary%2520and%2520public%2520datasets%250Avalidate%2520the%2520effectiveness%2520and%2520efficiency%2520of%2520TACLR.%2520Moreover%252C%2520it%2520has%2520been%250Asuccessfully%2520deployed%2520in%2520a%2520real-world%2520e-commerce%2520platform%252C%2520processing%2520millions%250Aof%2520product%2520listings%2520daily%2520while%2520supporting%2520dynamic%252C%2520large-scale%2520attribute%250Ataxonomies.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2501.03835v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=TACLR%3A%20A%20Scalable%20and%20Efficient%20Retrieval-based%20Method%20for%20Industrial%0A%20%20Product%20Attribute%20Value%20Identification&entry.906535625=Yindu%20Su%20and%20Huike%20Zou%20and%20Lin%20Sun%20and%20Ting%20Zhang%20and%20Haiyang%20Yang%20and%20Liyu%20Chen%20and%20David%20Lo%20and%20Qingheng%20Zhang%20and%20Shuguang%20Han%20and%20Jufeng%20Chen&entry.1292438233=%20%20Product%20Attribute%20Value%20Identification%20%28PAVI%29%20involves%20identifying%20attribute%0Avalues%20from%20product%20profiles%2C%20a%20key%20task%20for%20improving%20product%20search%2C%0Arecommendations%2C%20and%20business%20analytics%20on%20e-commerce%20platforms.%20However%2C%0Aexisting%20PAVI%20methods%20face%20critical%20challenges%2C%20such%20as%20inferring%20implicit%0Avalues%2C%20handling%20out-of-distribution%20%28OOD%29%20values%2C%20and%20producing%20normalized%0Aoutputs.%20To%20address%20these%20limitations%2C%20we%20introduce%20Taxonomy-Aware%20Contrastive%0ALearning%20Retrieval%20%28TACLR%29%2C%20the%20first%20retrieval-based%20method%20for%20PAVI.%20TACLR%0Aformulates%20PAVI%20as%20an%20information%20retrieval%20task%20by%20encoding%20product%20profiles%0Aand%20candidate%20values%20into%20embeddings%20and%20retrieving%20values%20based%20on%20their%0Asimilarity%20to%20the%20item%20embedding.%20It%20leverages%20contrastive%20training%20with%0Ataxonomy-aware%20hard%20negative%20sampling%20and%20employs%20adaptive%20inference%20with%0Adynamic%20thresholds.%20TACLR%20offers%20three%20key%20advantages%3A%20%281%29%20it%20effectively%0Ahandles%20implicit%20and%20OOD%20values%20while%20producing%20normalized%20outputs%3B%20%282%29%20it%0Ascales%20to%20thousands%20of%20categories%2C%20tens%20of%20thousands%20of%20attributes%2C%20and%0Amillions%20of%20values%3B%20and%20%283%29%20it%20supports%20efficient%20inference%20for%20high-load%0Aindustrial%20scenarios.%20Extensive%20experiments%20on%20proprietary%20and%20public%20datasets%0Avalidate%20the%20effectiveness%20and%20efficiency%20of%20TACLR.%20Moreover%2C%20it%20has%20been%0Asuccessfully%20deployed%20in%20a%20real-world%20e-commerce%20platform%2C%20processing%20millions%0Aof%20product%20listings%20daily%20while%20supporting%20dynamic%2C%20large-scale%20attribute%0Ataxonomies.%0A&entry.1838667208=http%3A//arxiv.org/abs/2501.03835v1&entry.124074799=Read"},
{"title": "Investigating the Impact of Data Selection Strategies on Language Model\n  Performance", "author": "Jiayao Gu and Liting Chen and Yihong Li", "abstract": "  Data selection is critical for enhancing the performance of language models,\nparticularly when aligning training datasets with a desired target\ndistribution. This study explores the effects of different data selection\nmethods and feature types on model performance. We evaluate whether selecting\ndata subsets can influence downstream tasks, whether n-gram features improve\nalignment with target distributions, and whether embedding-based neural\nfeatures provide complementary benefits. Through comparative experiments using\nbaseline random selection methods and distribution aligned approaches, we\nprovide insights into the interplay between data selection strategies and model\ntraining efficacy. All code for this study can be found on\n\\href{https://github.com/jgu13/HIR-Hybrid-Importance-Resampling-for-Language-Models}{github\nrepository}.\n", "link": "http://arxiv.org/abs/2501.03826v1", "date": "2025-01-07", "relevancy": 2.4081, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.4818}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.4818}, {"title": "Breadcrumbs: Adversarial class-balanced sampling for long-tailed recognition", "link": "https://link.springer.com/chapter/10.1007/978-3-031-20053-3_37", "similarity": 0.4813}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Investigating%20the%20Impact%20of%20Data%20Selection%20Strategies%20on%20Language%20Model%0A%20%20Performance&body=Title%3A%20Investigating%20the%20Impact%20of%20Data%20Selection%20Strategies%20on%20Language%20Model%0A%20%20Performance%0AAuthor%3A%20Jiayao%20Gu%20and%20Liting%20Chen%20and%20Yihong%20Li%0AAbstract%3A%20%20%20Data%20selection%20is%20critical%20for%20enhancing%20the%20performance%20of%20language%20models%2C%0Aparticularly%20when%20aligning%20training%20datasets%20with%20a%20desired%20target%0Adistribution.%20This%20study%20explores%20the%20effects%20of%20different%20data%20selection%0Amethods%20and%20feature%20types%20on%20model%20performance.%20We%20evaluate%20whether%20selecting%0Adata%20subsets%20can%20influence%20downstream%20tasks%2C%20whether%20n-gram%20features%20improve%0Aalignment%20with%20target%20distributions%2C%20and%20whether%20embedding-based%20neural%0Afeatures%20provide%20complementary%20benefits.%20Through%20comparative%20experiments%20using%0Abaseline%20random%20selection%20methods%20and%20distribution%20aligned%20approaches%2C%20we%0Aprovide%20insights%20into%20the%20interplay%20between%20data%20selection%20strategies%20and%20model%0Atraining%20efficacy.%20All%20code%20for%20this%20study%20can%20be%20found%20on%0A%5Chref%7Bhttps%3A//github.com/jgu13/HIR-Hybrid-Importance-Resampling-for-Language-Models%7D%7Bgithub%0Arepository%7D.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2501.03826v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DInvestigating%2520the%2520Impact%2520of%2520Data%2520Selection%2520Strategies%2520on%2520Language%2520Model%250A%2520%2520Performance%26entry.906535625%3DJiayao%2520Gu%2520and%2520Liting%2520Chen%2520and%2520Yihong%2520Li%26entry.1292438233%3D%2520%2520Data%2520selection%2520is%2520critical%2520for%2520enhancing%2520the%2520performance%2520of%2520language%2520models%252C%250Aparticularly%2520when%2520aligning%2520training%2520datasets%2520with%2520a%2520desired%2520target%250Adistribution.%2520This%2520study%2520explores%2520the%2520effects%2520of%2520different%2520data%2520selection%250Amethods%2520and%2520feature%2520types%2520on%2520model%2520performance.%2520We%2520evaluate%2520whether%2520selecting%250Adata%2520subsets%2520can%2520influence%2520downstream%2520tasks%252C%2520whether%2520n-gram%2520features%2520improve%250Aalignment%2520with%2520target%2520distributions%252C%2520and%2520whether%2520embedding-based%2520neural%250Afeatures%2520provide%2520complementary%2520benefits.%2520Through%2520comparative%2520experiments%2520using%250Abaseline%2520random%2520selection%2520methods%2520and%2520distribution%2520aligned%2520approaches%252C%2520we%250Aprovide%2520insights%2520into%2520the%2520interplay%2520between%2520data%2520selection%2520strategies%2520and%2520model%250Atraining%2520efficacy.%2520All%2520code%2520for%2520this%2520study%2520can%2520be%2520found%2520on%250A%255Chref%257Bhttps%253A//github.com/jgu13/HIR-Hybrid-Importance-Resampling-for-Language-Models%257D%257Bgithub%250Arepository%257D.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2501.03826v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Investigating%20the%20Impact%20of%20Data%20Selection%20Strategies%20on%20Language%20Model%0A%20%20Performance&entry.906535625=Jiayao%20Gu%20and%20Liting%20Chen%20and%20Yihong%20Li&entry.1292438233=%20%20Data%20selection%20is%20critical%20for%20enhancing%20the%20performance%20of%20language%20models%2C%0Aparticularly%20when%20aligning%20training%20datasets%20with%20a%20desired%20target%0Adistribution.%20This%20study%20explores%20the%20effects%20of%20different%20data%20selection%0Amethods%20and%20feature%20types%20on%20model%20performance.%20We%20evaluate%20whether%20selecting%0Adata%20subsets%20can%20influence%20downstream%20tasks%2C%20whether%20n-gram%20features%20improve%0Aalignment%20with%20target%20distributions%2C%20and%20whether%20embedding-based%20neural%0Afeatures%20provide%20complementary%20benefits.%20Through%20comparative%20experiments%20using%0Abaseline%20random%20selection%20methods%20and%20distribution%20aligned%20approaches%2C%20we%0Aprovide%20insights%20into%20the%20interplay%20between%20data%20selection%20strategies%20and%20model%0Atraining%20efficacy.%20All%20code%20for%20this%20study%20can%20be%20found%20on%0A%5Chref%7Bhttps%3A//github.com/jgu13/HIR-Hybrid-Importance-Resampling-for-Language-Models%7D%7Bgithub%0Arepository%7D.%0A&entry.1838667208=http%3A//arxiv.org/abs/2501.03826v1&entry.124074799=Read"},
{"title": "AuxDepthNet: Real-Time Monocular 3D Object Detection with\n  Depth-Sensitive Features", "author": "Ruochen Zhang and Hyeung-Sik Choi and Dongwook Jung and Phan Huy Nam Anh and Sang-Ki Jeong and Zihao Zhu", "abstract": "  Monocular 3D object detection is a challenging task in autonomous systems due\nto the lack of explicit depth information in single-view images. Existing\nmethods often depend on external depth estimators or expensive sensors, which\nincrease computational complexity and hinder real-time performance. To overcome\nthese limitations, we propose AuxDepthNet, an efficient framework for real-time\nmonocular 3D object detection that eliminates the reliance on external depth\nmaps or pre-trained depth models. AuxDepthNet introduces two key components:\nthe Auxiliary Depth Feature (ADF) module, which implicitly learns\ndepth-sensitive features to improve spatial reasoning and computational\nefficiency, and the Depth Position Mapping (DPM) module, which embeds depth\npositional information directly into the detection process to enable accurate\nobject localization and 3D bounding box regression. Leveraging the DepthFusion\nTransformer architecture, AuxDepthNet globally integrates visual and\ndepth-sensitive features through depth-guided interactions, ensuring robust and\nefficient detection. Extensive experiments on the KITTI dataset show that\nAuxDepthNet achieves state-of-the-art performance, with $\\text{AP}_{3D}$ scores\nof 24.72\\% (Easy), 18.63\\% (Moderate), and 15.31\\% (Hard), and\n$\\text{AP}_{\\text{BEV}}$ scores of 34.11\\% (Easy), 25.18\\% (Moderate), and\n21.90\\% (Hard) at an IoU threshold of 0.7.\n", "link": "http://arxiv.org/abs/2501.03700v1", "date": "2025-01-07", "relevancy": 2.4, "topK": [{"title": "DDM-NET: End-to-end learning of keypoint feature Detection, Description and Matching for 3D localization", "link": "https://arxiv.org/abs/2212.04575", "similarity": 0.6323}, {"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.6146}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5618}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20AuxDepthNet%3A%20Real-Time%20Monocular%203D%20Object%20Detection%20with%0A%20%20Depth-Sensitive%20Features&body=Title%3A%20AuxDepthNet%3A%20Real-Time%20Monocular%203D%20Object%20Detection%20with%0A%20%20Depth-Sensitive%20Features%0AAuthor%3A%20Ruochen%20Zhang%20and%20Hyeung-Sik%20Choi%20and%20Dongwook%20Jung%20and%20Phan%20Huy%20Nam%20Anh%20and%20Sang-Ki%20Jeong%20and%20Zihao%20Zhu%0AAbstract%3A%20%20%20Monocular%203D%20object%20detection%20is%20a%20challenging%20task%20in%20autonomous%20systems%20due%0Ato%20the%20lack%20of%20explicit%20depth%20information%20in%20single-view%20images.%20Existing%0Amethods%20often%20depend%20on%20external%20depth%20estimators%20or%20expensive%20sensors%2C%20which%0Aincrease%20computational%20complexity%20and%20hinder%20real-time%20performance.%20To%20overcome%0Athese%20limitations%2C%20we%20propose%20AuxDepthNet%2C%20an%20efficient%20framework%20for%20real-time%0Amonocular%203D%20object%20detection%20that%20eliminates%20the%20reliance%20on%20external%20depth%0Amaps%20or%20pre-trained%20depth%20models.%20AuxDepthNet%20introduces%20two%20key%20components%3A%0Athe%20Auxiliary%20Depth%20Feature%20%28ADF%29%20module%2C%20which%20implicitly%20learns%0Adepth-sensitive%20features%20to%20improve%20spatial%20reasoning%20and%20computational%0Aefficiency%2C%20and%20the%20Depth%20Position%20Mapping%20%28DPM%29%20module%2C%20which%20embeds%20depth%0Apositional%20information%20directly%20into%20the%20detection%20process%20to%20enable%20accurate%0Aobject%20localization%20and%203D%20bounding%20box%20regression.%20Leveraging%20the%20DepthFusion%0ATransformer%20architecture%2C%20AuxDepthNet%20globally%20integrates%20visual%20and%0Adepth-sensitive%20features%20through%20depth-guided%20interactions%2C%20ensuring%20robust%20and%0Aefficient%20detection.%20Extensive%20experiments%20on%20the%20KITTI%20dataset%20show%20that%0AAuxDepthNet%20achieves%20state-of-the-art%20performance%2C%20with%20%24%5Ctext%7BAP%7D_%7B3D%7D%24%20scores%0Aof%2024.72%5C%25%20%28Easy%29%2C%2018.63%5C%25%20%28Moderate%29%2C%20and%2015.31%5C%25%20%28Hard%29%2C%20and%0A%24%5Ctext%7BAP%7D_%7B%5Ctext%7BBEV%7D%7D%24%20scores%20of%2034.11%5C%25%20%28Easy%29%2C%2025.18%5C%25%20%28Moderate%29%2C%20and%0A21.90%5C%25%20%28Hard%29%20at%20an%20IoU%20threshold%20of%200.7.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2501.03700v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DAuxDepthNet%253A%2520Real-Time%2520Monocular%25203D%2520Object%2520Detection%2520with%250A%2520%2520Depth-Sensitive%2520Features%26entry.906535625%3DRuochen%2520Zhang%2520and%2520Hyeung-Sik%2520Choi%2520and%2520Dongwook%2520Jung%2520and%2520Phan%2520Huy%2520Nam%2520Anh%2520and%2520Sang-Ki%2520Jeong%2520and%2520Zihao%2520Zhu%26entry.1292438233%3D%2520%2520Monocular%25203D%2520object%2520detection%2520is%2520a%2520challenging%2520task%2520in%2520autonomous%2520systems%2520due%250Ato%2520the%2520lack%2520of%2520explicit%2520depth%2520information%2520in%2520single-view%2520images.%2520Existing%250Amethods%2520often%2520depend%2520on%2520external%2520depth%2520estimators%2520or%2520expensive%2520sensors%252C%2520which%250Aincrease%2520computational%2520complexity%2520and%2520hinder%2520real-time%2520performance.%2520To%2520overcome%250Athese%2520limitations%252C%2520we%2520propose%2520AuxDepthNet%252C%2520an%2520efficient%2520framework%2520for%2520real-time%250Amonocular%25203D%2520object%2520detection%2520that%2520eliminates%2520the%2520reliance%2520on%2520external%2520depth%250Amaps%2520or%2520pre-trained%2520depth%2520models.%2520AuxDepthNet%2520introduces%2520two%2520key%2520components%253A%250Athe%2520Auxiliary%2520Depth%2520Feature%2520%2528ADF%2529%2520module%252C%2520which%2520implicitly%2520learns%250Adepth-sensitive%2520features%2520to%2520improve%2520spatial%2520reasoning%2520and%2520computational%250Aefficiency%252C%2520and%2520the%2520Depth%2520Position%2520Mapping%2520%2528DPM%2529%2520module%252C%2520which%2520embeds%2520depth%250Apositional%2520information%2520directly%2520into%2520the%2520detection%2520process%2520to%2520enable%2520accurate%250Aobject%2520localization%2520and%25203D%2520bounding%2520box%2520regression.%2520Leveraging%2520the%2520DepthFusion%250ATransformer%2520architecture%252C%2520AuxDepthNet%2520globally%2520integrates%2520visual%2520and%250Adepth-sensitive%2520features%2520through%2520depth-guided%2520interactions%252C%2520ensuring%2520robust%2520and%250Aefficient%2520detection.%2520Extensive%2520experiments%2520on%2520the%2520KITTI%2520dataset%2520show%2520that%250AAuxDepthNet%2520achieves%2520state-of-the-art%2520performance%252C%2520with%2520%2524%255Ctext%257BAP%257D_%257B3D%257D%2524%2520scores%250Aof%252024.72%255C%2525%2520%2528Easy%2529%252C%252018.63%255C%2525%2520%2528Moderate%2529%252C%2520and%252015.31%255C%2525%2520%2528Hard%2529%252C%2520and%250A%2524%255Ctext%257BAP%257D_%257B%255Ctext%257BBEV%257D%257D%2524%2520scores%2520of%252034.11%255C%2525%2520%2528Easy%2529%252C%252025.18%255C%2525%2520%2528Moderate%2529%252C%2520and%250A21.90%255C%2525%2520%2528Hard%2529%2520at%2520an%2520IoU%2520threshold%2520of%25200.7.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2501.03700v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=AuxDepthNet%3A%20Real-Time%20Monocular%203D%20Object%20Detection%20with%0A%20%20Depth-Sensitive%20Features&entry.906535625=Ruochen%20Zhang%20and%20Hyeung-Sik%20Choi%20and%20Dongwook%20Jung%20and%20Phan%20Huy%20Nam%20Anh%20and%20Sang-Ki%20Jeong%20and%20Zihao%20Zhu&entry.1292438233=%20%20Monocular%203D%20object%20detection%20is%20a%20challenging%20task%20in%20autonomous%20systems%20due%0Ato%20the%20lack%20of%20explicit%20depth%20information%20in%20single-view%20images.%20Existing%0Amethods%20often%20depend%20on%20external%20depth%20estimators%20or%20expensive%20sensors%2C%20which%0Aincrease%20computational%20complexity%20and%20hinder%20real-time%20performance.%20To%20overcome%0Athese%20limitations%2C%20we%20propose%20AuxDepthNet%2C%20an%20efficient%20framework%20for%20real-time%0Amonocular%203D%20object%20detection%20that%20eliminates%20the%20reliance%20on%20external%20depth%0Amaps%20or%20pre-trained%20depth%20models.%20AuxDepthNet%20introduces%20two%20key%20components%3A%0Athe%20Auxiliary%20Depth%20Feature%20%28ADF%29%20module%2C%20which%20implicitly%20learns%0Adepth-sensitive%20features%20to%20improve%20spatial%20reasoning%20and%20computational%0Aefficiency%2C%20and%20the%20Depth%20Position%20Mapping%20%28DPM%29%20module%2C%20which%20embeds%20depth%0Apositional%20information%20directly%20into%20the%20detection%20process%20to%20enable%20accurate%0Aobject%20localization%20and%203D%20bounding%20box%20regression.%20Leveraging%20the%20DepthFusion%0ATransformer%20architecture%2C%20AuxDepthNet%20globally%20integrates%20visual%20and%0Adepth-sensitive%20features%20through%20depth-guided%20interactions%2C%20ensuring%20robust%20and%0Aefficient%20detection.%20Extensive%20experiments%20on%20the%20KITTI%20dataset%20show%20that%0AAuxDepthNet%20achieves%20state-of-the-art%20performance%2C%20with%20%24%5Ctext%7BAP%7D_%7B3D%7D%24%20scores%0Aof%2024.72%5C%25%20%28Easy%29%2C%2018.63%5C%25%20%28Moderate%29%2C%20and%2015.31%5C%25%20%28Hard%29%2C%20and%0A%24%5Ctext%7BAP%7D_%7B%5Ctext%7BBEV%7D%7D%24%20scores%20of%2034.11%5C%25%20%28Easy%29%2C%2025.18%5C%25%20%28Moderate%29%2C%20and%0A21.90%5C%25%20%28Hard%29%20at%20an%20IoU%20threshold%20of%200.7.%0A&entry.1838667208=http%3A//arxiv.org/abs/2501.03700v1&entry.124074799=Read"},
{"title": "Localizing AI: Evaluating Open-Weight Language Models for Languages of\n  Baltic States", "author": "Jurgita Kapo\u010di\u016bt\u0117-Dzikien\u0117 and Toms Bergmanis and M\u0101rcis Pinnis", "abstract": "  Although large language models (LLMs) have transformed our expectations of\nmodern language technologies, concerns over data privacy often restrict the use\nof commercially available LLMs hosted outside of EU jurisdictions. This limits\ntheir application in governmental, defence, and other data-sensitive sectors.\nIn this work, we evaluate the extent to which locally deployable open-weight\nLLMs support lesser-spoken languages such as Lithuanian, Latvian, and Estonian.\nWe examine various size and precision variants of the top-performing\nmultilingual open-weight models, Llama~3, Gemma~2, Phi, and NeMo, on machine\ntranslation, multiple-choice question answering, and free-form text generation.\nThe results indicate that while certain models like Gemma~2 perform close to\nthe top commercially available models, many LLMs struggle with these languages.\nMost surprisingly, however, we find that these models, while showing close to\nstate-of-the-art translation performance, are still prone to lexical\nhallucinations with errors in at least 1 in 20 words for all open-weight\nmultilingual LLMs.\n", "link": "http://arxiv.org/abs/2501.03952v1", "date": "2025-01-07", "relevancy": 2.3742, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.4754}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.4754}, {"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.4737}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Localizing%20AI%3A%20Evaluating%20Open-Weight%20Language%20Models%20for%20Languages%20of%0A%20%20Baltic%20States&body=Title%3A%20Localizing%20AI%3A%20Evaluating%20Open-Weight%20Language%20Models%20for%20Languages%20of%0A%20%20Baltic%20States%0AAuthor%3A%20Jurgita%20Kapo%C4%8Di%C5%ABt%C4%97-Dzikien%C4%97%20and%20Toms%20Bergmanis%20and%20M%C4%81rcis%20Pinnis%0AAbstract%3A%20%20%20Although%20large%20language%20models%20%28LLMs%29%20have%20transformed%20our%20expectations%20of%0Amodern%20language%20technologies%2C%20concerns%20over%20data%20privacy%20often%20restrict%20the%20use%0Aof%20commercially%20available%20LLMs%20hosted%20outside%20of%20EU%20jurisdictions.%20This%20limits%0Atheir%20application%20in%20governmental%2C%20defence%2C%20and%20other%20data-sensitive%20sectors.%0AIn%20this%20work%2C%20we%20evaluate%20the%20extent%20to%20which%20locally%20deployable%20open-weight%0ALLMs%20support%20lesser-spoken%20languages%20such%20as%20Lithuanian%2C%20Latvian%2C%20and%20Estonian.%0AWe%20examine%20various%20size%20and%20precision%20variants%20of%20the%20top-performing%0Amultilingual%20open-weight%20models%2C%20Llama~3%2C%20Gemma~2%2C%20Phi%2C%20and%20NeMo%2C%20on%20machine%0Atranslation%2C%20multiple-choice%20question%20answering%2C%20and%20free-form%20text%20generation.%0AThe%20results%20indicate%20that%20while%20certain%20models%20like%20Gemma~2%20perform%20close%20to%0Athe%20top%20commercially%20available%20models%2C%20many%20LLMs%20struggle%20with%20these%20languages.%0AMost%20surprisingly%2C%20however%2C%20we%20find%20that%20these%20models%2C%20while%20showing%20close%20to%0Astate-of-the-art%20translation%20performance%2C%20are%20still%20prone%20to%20lexical%0Ahallucinations%20with%20errors%20in%20at%20least%201%20in%2020%20words%20for%20all%20open-weight%0Amultilingual%20LLMs.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2501.03952v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DLocalizing%2520AI%253A%2520Evaluating%2520Open-Weight%2520Language%2520Models%2520for%2520Languages%2520of%250A%2520%2520Baltic%2520States%26entry.906535625%3DJurgita%2520Kapo%25C4%258Di%25C5%25ABt%25C4%2597-Dzikien%25C4%2597%2520and%2520Toms%2520Bergmanis%2520and%2520M%25C4%2581rcis%2520Pinnis%26entry.1292438233%3D%2520%2520Although%2520large%2520language%2520models%2520%2528LLMs%2529%2520have%2520transformed%2520our%2520expectations%2520of%250Amodern%2520language%2520technologies%252C%2520concerns%2520over%2520data%2520privacy%2520often%2520restrict%2520the%2520use%250Aof%2520commercially%2520available%2520LLMs%2520hosted%2520outside%2520of%2520EU%2520jurisdictions.%2520This%2520limits%250Atheir%2520application%2520in%2520governmental%252C%2520defence%252C%2520and%2520other%2520data-sensitive%2520sectors.%250AIn%2520this%2520work%252C%2520we%2520evaluate%2520the%2520extent%2520to%2520which%2520locally%2520deployable%2520open-weight%250ALLMs%2520support%2520lesser-spoken%2520languages%2520such%2520as%2520Lithuanian%252C%2520Latvian%252C%2520and%2520Estonian.%250AWe%2520examine%2520various%2520size%2520and%2520precision%2520variants%2520of%2520the%2520top-performing%250Amultilingual%2520open-weight%2520models%252C%2520Llama~3%252C%2520Gemma~2%252C%2520Phi%252C%2520and%2520NeMo%252C%2520on%2520machine%250Atranslation%252C%2520multiple-choice%2520question%2520answering%252C%2520and%2520free-form%2520text%2520generation.%250AThe%2520results%2520indicate%2520that%2520while%2520certain%2520models%2520like%2520Gemma~2%2520perform%2520close%2520to%250Athe%2520top%2520commercially%2520available%2520models%252C%2520many%2520LLMs%2520struggle%2520with%2520these%2520languages.%250AMost%2520surprisingly%252C%2520however%252C%2520we%2520find%2520that%2520these%2520models%252C%2520while%2520showing%2520close%2520to%250Astate-of-the-art%2520translation%2520performance%252C%2520are%2520still%2520prone%2520to%2520lexical%250Ahallucinations%2520with%2520errors%2520in%2520at%2520least%25201%2520in%252020%2520words%2520for%2520all%2520open-weight%250Amultilingual%2520LLMs.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2501.03952v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Localizing%20AI%3A%20Evaluating%20Open-Weight%20Language%20Models%20for%20Languages%20of%0A%20%20Baltic%20States&entry.906535625=Jurgita%20Kapo%C4%8Di%C5%ABt%C4%97-Dzikien%C4%97%20and%20Toms%20Bergmanis%20and%20M%C4%81rcis%20Pinnis&entry.1292438233=%20%20Although%20large%20language%20models%20%28LLMs%29%20have%20transformed%20our%20expectations%20of%0Amodern%20language%20technologies%2C%20concerns%20over%20data%20privacy%20often%20restrict%20the%20use%0Aof%20commercially%20available%20LLMs%20hosted%20outside%20of%20EU%20jurisdictions.%20This%20limits%0Atheir%20application%20in%20governmental%2C%20defence%2C%20and%20other%20data-sensitive%20sectors.%0AIn%20this%20work%2C%20we%20evaluate%20the%20extent%20to%20which%20locally%20deployable%20open-weight%0ALLMs%20support%20lesser-spoken%20languages%20such%20as%20Lithuanian%2C%20Latvian%2C%20and%20Estonian.%0AWe%20examine%20various%20size%20and%20precision%20variants%20of%20the%20top-performing%0Amultilingual%20open-weight%20models%2C%20Llama~3%2C%20Gemma~2%2C%20Phi%2C%20and%20NeMo%2C%20on%20machine%0Atranslation%2C%20multiple-choice%20question%20answering%2C%20and%20free-form%20text%20generation.%0AThe%20results%20indicate%20that%20while%20certain%20models%20like%20Gemma~2%20perform%20close%20to%0Athe%20top%20commercially%20available%20models%2C%20many%20LLMs%20struggle%20with%20these%20languages.%0AMost%20surprisingly%2C%20however%2C%20we%20find%20that%20these%20models%2C%20while%20showing%20close%20to%0Astate-of-the-art%20translation%20performance%2C%20are%20still%20prone%20to%20lexical%0Ahallucinations%20with%20errors%20in%20at%20least%201%20in%2020%20words%20for%20all%20open-weight%0Amultilingual%20LLMs.%0A&entry.1838667208=http%3A//arxiv.org/abs/2501.03952v1&entry.124074799=Read"},
{"title": "AllSpark: A Multimodal Spatio-Temporal General Intelligence Model with\n  Ten Modalities via Language as a Reference Framework", "author": "Run Shao and Cheng Yang and Qiujun Li and Qing Zhu and Yongjun Zhang and YanSheng Li and Yu Liu and Yong Tang and Dapeng Liu and Shizhong Yang and Haifeng Li", "abstract": "  Leveraging multimodal data is an inherent requirement for comprehending\ngeographic objects. However, due to the high heterogeneity in structure and\nsemantics among various spatio-temporal modalities, the joint interpretation of\nmultimodal spatio-temporal data has long been an extremely challenging problem.\nThe primary challenge resides in striking a trade-off between the cohesion and\nautonomy of diverse modalities. This trade-off becomes progressively nonlinear\nas the number of modalities expands. Inspired by the human cognitive system and\nlinguistic philosophy, where perceptual signals from the five senses converge\ninto language, we introduce the Language as Reference Framework (LaRF), a\nfundamental principle for constructing a multimodal unified model. Building\nupon this, we propose AllSpark, a multimodal spatio-temporal general artificial\nintelligence model. Our model integrates ten different modalities into a\nunified framework. To achieve modal cohesion, AllSpark introduces a modal\nbridge and multimodal large language model (LLM) to map diverse modal features\ninto the language feature space. To maintain modality autonomy, AllSpark uses\nmodality-specific encoders to extract the tokens of various spatio-temporal\nmodalities. Finally, observing a gap between the model's interpretability and\ndownstream tasks, we designed modality-specific prompts and task heads,\nenhancing the model's generalization capability across specific tasks.\nExperiments indicate that the incorporation of language enables AllSpark to\nexcel in few-shot classification tasks for RGB and point cloud modalities\nwithout additional training, surpassing baseline performance by up to 41.82\\%.\nThe source code is available at https://github.com/GeoX-Lab/AllSpark.\n", "link": "http://arxiv.org/abs/2401.00546v3", "date": "2025-01-07", "relevancy": 2.3559, "topK": [{"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5948}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.5908}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5825}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20AllSpark%3A%20A%20Multimodal%20Spatio-Temporal%20General%20Intelligence%20Model%20with%0A%20%20Ten%20Modalities%20via%20Language%20as%20a%20Reference%20Framework&body=Title%3A%20AllSpark%3A%20A%20Multimodal%20Spatio-Temporal%20General%20Intelligence%20Model%20with%0A%20%20Ten%20Modalities%20via%20Language%20as%20a%20Reference%20Framework%0AAuthor%3A%20Run%20Shao%20and%20Cheng%20Yang%20and%20Qiujun%20Li%20and%20Qing%20Zhu%20and%20Yongjun%20Zhang%20and%20YanSheng%20Li%20and%20Yu%20Liu%20and%20Yong%20Tang%20and%20Dapeng%20Liu%20and%20Shizhong%20Yang%20and%20Haifeng%20Li%0AAbstract%3A%20%20%20Leveraging%20multimodal%20data%20is%20an%20inherent%20requirement%20for%20comprehending%0Ageographic%20objects.%20However%2C%20due%20to%20the%20high%20heterogeneity%20in%20structure%20and%0Asemantics%20among%20various%20spatio-temporal%20modalities%2C%20the%20joint%20interpretation%20of%0Amultimodal%20spatio-temporal%20data%20has%20long%20been%20an%20extremely%20challenging%20problem.%0AThe%20primary%20challenge%20resides%20in%20striking%20a%20trade-off%20between%20the%20cohesion%20and%0Aautonomy%20of%20diverse%20modalities.%20This%20trade-off%20becomes%20progressively%20nonlinear%0Aas%20the%20number%20of%20modalities%20expands.%20Inspired%20by%20the%20human%20cognitive%20system%20and%0Alinguistic%20philosophy%2C%20where%20perceptual%20signals%20from%20the%20five%20senses%20converge%0Ainto%20language%2C%20we%20introduce%20the%20Language%20as%20Reference%20Framework%20%28LaRF%29%2C%20a%0Afundamental%20principle%20for%20constructing%20a%20multimodal%20unified%20model.%20Building%0Aupon%20this%2C%20we%20propose%20AllSpark%2C%20a%20multimodal%20spatio-temporal%20general%20artificial%0Aintelligence%20model.%20Our%20model%20integrates%20ten%20different%20modalities%20into%20a%0Aunified%20framework.%20To%20achieve%20modal%20cohesion%2C%20AllSpark%20introduces%20a%20modal%0Abridge%20and%20multimodal%20large%20language%20model%20%28LLM%29%20to%20map%20diverse%20modal%20features%0Ainto%20the%20language%20feature%20space.%20To%20maintain%20modality%20autonomy%2C%20AllSpark%20uses%0Amodality-specific%20encoders%20to%20extract%20the%20tokens%20of%20various%20spatio-temporal%0Amodalities.%20Finally%2C%20observing%20a%20gap%20between%20the%20model%27s%20interpretability%20and%0Adownstream%20tasks%2C%20we%20designed%20modality-specific%20prompts%20and%20task%20heads%2C%0Aenhancing%20the%20model%27s%20generalization%20capability%20across%20specific%20tasks.%0AExperiments%20indicate%20that%20the%20incorporation%20of%20language%20enables%20AllSpark%20to%0Aexcel%20in%20few-shot%20classification%20tasks%20for%20RGB%20and%20point%20cloud%20modalities%0Awithout%20additional%20training%2C%20surpassing%20baseline%20performance%20by%20up%20to%2041.82%5C%25.%0AThe%20source%20code%20is%20available%20at%20https%3A//github.com/GeoX-Lab/AllSpark.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2401.00546v3%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DAllSpark%253A%2520A%2520Multimodal%2520Spatio-Temporal%2520General%2520Intelligence%2520Model%2520with%250A%2520%2520Ten%2520Modalities%2520via%2520Language%2520as%2520a%2520Reference%2520Framework%26entry.906535625%3DRun%2520Shao%2520and%2520Cheng%2520Yang%2520and%2520Qiujun%2520Li%2520and%2520Qing%2520Zhu%2520and%2520Yongjun%2520Zhang%2520and%2520YanSheng%2520Li%2520and%2520Yu%2520Liu%2520and%2520Yong%2520Tang%2520and%2520Dapeng%2520Liu%2520and%2520Shizhong%2520Yang%2520and%2520Haifeng%2520Li%26entry.1292438233%3D%2520%2520Leveraging%2520multimodal%2520data%2520is%2520an%2520inherent%2520requirement%2520for%2520comprehending%250Ageographic%2520objects.%2520However%252C%2520due%2520to%2520the%2520high%2520heterogeneity%2520in%2520structure%2520and%250Asemantics%2520among%2520various%2520spatio-temporal%2520modalities%252C%2520the%2520joint%2520interpretation%2520of%250Amultimodal%2520spatio-temporal%2520data%2520has%2520long%2520been%2520an%2520extremely%2520challenging%2520problem.%250AThe%2520primary%2520challenge%2520resides%2520in%2520striking%2520a%2520trade-off%2520between%2520the%2520cohesion%2520and%250Aautonomy%2520of%2520diverse%2520modalities.%2520This%2520trade-off%2520becomes%2520progressively%2520nonlinear%250Aas%2520the%2520number%2520of%2520modalities%2520expands.%2520Inspired%2520by%2520the%2520human%2520cognitive%2520system%2520and%250Alinguistic%2520philosophy%252C%2520where%2520perceptual%2520signals%2520from%2520the%2520five%2520senses%2520converge%250Ainto%2520language%252C%2520we%2520introduce%2520the%2520Language%2520as%2520Reference%2520Framework%2520%2528LaRF%2529%252C%2520a%250Afundamental%2520principle%2520for%2520constructing%2520a%2520multimodal%2520unified%2520model.%2520Building%250Aupon%2520this%252C%2520we%2520propose%2520AllSpark%252C%2520a%2520multimodal%2520spatio-temporal%2520general%2520artificial%250Aintelligence%2520model.%2520Our%2520model%2520integrates%2520ten%2520different%2520modalities%2520into%2520a%250Aunified%2520framework.%2520To%2520achieve%2520modal%2520cohesion%252C%2520AllSpark%2520introduces%2520a%2520modal%250Abridge%2520and%2520multimodal%2520large%2520language%2520model%2520%2528LLM%2529%2520to%2520map%2520diverse%2520modal%2520features%250Ainto%2520the%2520language%2520feature%2520space.%2520To%2520maintain%2520modality%2520autonomy%252C%2520AllSpark%2520uses%250Amodality-specific%2520encoders%2520to%2520extract%2520the%2520tokens%2520of%2520various%2520spatio-temporal%250Amodalities.%2520Finally%252C%2520observing%2520a%2520gap%2520between%2520the%2520model%2527s%2520interpretability%2520and%250Adownstream%2520tasks%252C%2520we%2520designed%2520modality-specific%2520prompts%2520and%2520task%2520heads%252C%250Aenhancing%2520the%2520model%2527s%2520generalization%2520capability%2520across%2520specific%2520tasks.%250AExperiments%2520indicate%2520that%2520the%2520incorporation%2520of%2520language%2520enables%2520AllSpark%2520to%250Aexcel%2520in%2520few-shot%2520classification%2520tasks%2520for%2520RGB%2520and%2520point%2520cloud%2520modalities%250Awithout%2520additional%2520training%252C%2520surpassing%2520baseline%2520performance%2520by%2520up%2520to%252041.82%255C%2525.%250AThe%2520source%2520code%2520is%2520available%2520at%2520https%253A//github.com/GeoX-Lab/AllSpark.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2401.00546v3%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=AllSpark%3A%20A%20Multimodal%20Spatio-Temporal%20General%20Intelligence%20Model%20with%0A%20%20Ten%20Modalities%20via%20Language%20as%20a%20Reference%20Framework&entry.906535625=Run%20Shao%20and%20Cheng%20Yang%20and%20Qiujun%20Li%20and%20Qing%20Zhu%20and%20Yongjun%20Zhang%20and%20YanSheng%20Li%20and%20Yu%20Liu%20and%20Yong%20Tang%20and%20Dapeng%20Liu%20and%20Shizhong%20Yang%20and%20Haifeng%20Li&entry.1292438233=%20%20Leveraging%20multimodal%20data%20is%20an%20inherent%20requirement%20for%20comprehending%0Ageographic%20objects.%20However%2C%20due%20to%20the%20high%20heterogeneity%20in%20structure%20and%0Asemantics%20among%20various%20spatio-temporal%20modalities%2C%20the%20joint%20interpretation%20of%0Amultimodal%20spatio-temporal%20data%20has%20long%20been%20an%20extremely%20challenging%20problem.%0AThe%20primary%20challenge%20resides%20in%20striking%20a%20trade-off%20between%20the%20cohesion%20and%0Aautonomy%20of%20diverse%20modalities.%20This%20trade-off%20becomes%20progressively%20nonlinear%0Aas%20the%20number%20of%20modalities%20expands.%20Inspired%20by%20the%20human%20cognitive%20system%20and%0Alinguistic%20philosophy%2C%20where%20perceptual%20signals%20from%20the%20five%20senses%20converge%0Ainto%20language%2C%20we%20introduce%20the%20Language%20as%20Reference%20Framework%20%28LaRF%29%2C%20a%0Afundamental%20principle%20for%20constructing%20a%20multimodal%20unified%20model.%20Building%0Aupon%20this%2C%20we%20propose%20AllSpark%2C%20a%20multimodal%20spatio-temporal%20general%20artificial%0Aintelligence%20model.%20Our%20model%20integrates%20ten%20different%20modalities%20into%20a%0Aunified%20framework.%20To%20achieve%20modal%20cohesion%2C%20AllSpark%20introduces%20a%20modal%0Abridge%20and%20multimodal%20large%20language%20model%20%28LLM%29%20to%20map%20diverse%20modal%20features%0Ainto%20the%20language%20feature%20space.%20To%20maintain%20modality%20autonomy%2C%20AllSpark%20uses%0Amodality-specific%20encoders%20to%20extract%20the%20tokens%20of%20various%20spatio-temporal%0Amodalities.%20Finally%2C%20observing%20a%20gap%20between%20the%20model%27s%20interpretability%20and%0Adownstream%20tasks%2C%20we%20designed%20modality-specific%20prompts%20and%20task%20heads%2C%0Aenhancing%20the%20model%27s%20generalization%20capability%20across%20specific%20tasks.%0AExperiments%20indicate%20that%20the%20incorporation%20of%20language%20enables%20AllSpark%20to%0Aexcel%20in%20few-shot%20classification%20tasks%20for%20RGB%20and%20point%20cloud%20modalities%0Awithout%20additional%20training%2C%20surpassing%20baseline%20performance%20by%20up%20to%2041.82%5C%25.%0AThe%20source%20code%20is%20available%20at%20https%3A//github.com/GeoX-Lab/AllSpark.%0A&entry.1838667208=http%3A//arxiv.org/abs/2401.00546v3&entry.124074799=Read"},
{"title": "Action Quality Assessment via Hierarchical Pose-guided Multi-stage\n  Contrastive Regression", "author": "Mengshi Qi and Hao Ye and Jiaxuan Peng and Huadong Ma", "abstract": "  Action Quality Assessment (AQA), which aims at automatic and fair evaluation\nof athletic performance, has gained increasing attention in recent years.\nHowever, athletes are often in rapid movement and the corresponding visual\nappearance variances are subtle, making it challenging to capture fine-grained\npose differences and leading to poor estimation performance. Furthermore, most\ncommon AQA tasks, such as diving in sports, are usually divided into multiple\nsub-actions, each of which contains different durations. However, existing\nmethods focus on segmenting the video into fixed frames, which disrupts the\ntemporal continuity of sub-actions resulting in unavoidable prediction errors.\nTo address these challenges, we propose a novel action quality assessment\nmethod through hierarchically pose-guided multi-stage contrastive regression.\nFirstly, we introduce a multi-scale dynamic visual-skeleton encoder to capture\nfine-grained spatio-temporal visual and skeletal features. Then, a procedure\nsegmentation network is introduced to separate different sub-actions and obtain\nsegmented features. Afterwards, the segmented visual and skeletal features are\nboth fed into a multi-modal fusion module as physics structural priors, to\nguide the model in learning refined activity similarities and variances.\nFinally, a multi-stage contrastive learning regression approach is employed to\nlearn discriminative representations and output prediction results. In\naddition, we introduce a newly-annotated FineDiving-Pose Dataset to improve the\ncurrent low-quality human pose labels. In experiments, the results on\nFineDiving and MTL-AQA datasets demonstrate the effectiveness and superiority\nof our proposed approach. Our source code and dataset are available at\nhttps://github.com/Lumos0507/HP-MCoRe.\n", "link": "http://arxiv.org/abs/2501.03674v1", "date": "2025-01-07", "relevancy": 2.293, "topK": [{"title": "PoseAnimate: Zero-shot high fidelity pose controllable character\n  animation", "link": "http://arxiv.org/abs/2404.13680v2", "similarity": 0.591}, {"title": "Synthesizing Moving People with 3D Control", "link": "http://arxiv.org/abs/2401.10889v2", "similarity": 0.5648}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.55}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Action%20Quality%20Assessment%20via%20Hierarchical%20Pose-guided%20Multi-stage%0A%20%20Contrastive%20Regression&body=Title%3A%20Action%20Quality%20Assessment%20via%20Hierarchical%20Pose-guided%20Multi-stage%0A%20%20Contrastive%20Regression%0AAuthor%3A%20Mengshi%20Qi%20and%20Hao%20Ye%20and%20Jiaxuan%20Peng%20and%20Huadong%20Ma%0AAbstract%3A%20%20%20Action%20Quality%20Assessment%20%28AQA%29%2C%20which%20aims%20at%20automatic%20and%20fair%20evaluation%0Aof%20athletic%20performance%2C%20has%20gained%20increasing%20attention%20in%20recent%20years.%0AHowever%2C%20athletes%20are%20often%20in%20rapid%20movement%20and%20the%20corresponding%20visual%0Aappearance%20variances%20are%20subtle%2C%20making%20it%20challenging%20to%20capture%20fine-grained%0Apose%20differences%20and%20leading%20to%20poor%20estimation%20performance.%20Furthermore%2C%20most%0Acommon%20AQA%20tasks%2C%20such%20as%20diving%20in%20sports%2C%20are%20usually%20divided%20into%20multiple%0Asub-actions%2C%20each%20of%20which%20contains%20different%20durations.%20However%2C%20existing%0Amethods%20focus%20on%20segmenting%20the%20video%20into%20fixed%20frames%2C%20which%20disrupts%20the%0Atemporal%20continuity%20of%20sub-actions%20resulting%20in%20unavoidable%20prediction%20errors.%0ATo%20address%20these%20challenges%2C%20we%20propose%20a%20novel%20action%20quality%20assessment%0Amethod%20through%20hierarchically%20pose-guided%20multi-stage%20contrastive%20regression.%0AFirstly%2C%20we%20introduce%20a%20multi-scale%20dynamic%20visual-skeleton%20encoder%20to%20capture%0Afine-grained%20spatio-temporal%20visual%20and%20skeletal%20features.%20Then%2C%20a%20procedure%0Asegmentation%20network%20is%20introduced%20to%20separate%20different%20sub-actions%20and%20obtain%0Asegmented%20features.%20Afterwards%2C%20the%20segmented%20visual%20and%20skeletal%20features%20are%0Aboth%20fed%20into%20a%20multi-modal%20fusion%20module%20as%20physics%20structural%20priors%2C%20to%0Aguide%20the%20model%20in%20learning%20refined%20activity%20similarities%20and%20variances.%0AFinally%2C%20a%20multi-stage%20contrastive%20learning%20regression%20approach%20is%20employed%20to%0Alearn%20discriminative%20representations%20and%20output%20prediction%20results.%20In%0Aaddition%2C%20we%20introduce%20a%20newly-annotated%20FineDiving-Pose%20Dataset%20to%20improve%20the%0Acurrent%20low-quality%20human%20pose%20labels.%20In%20experiments%2C%20the%20results%20on%0AFineDiving%20and%20MTL-AQA%20datasets%20demonstrate%20the%20effectiveness%20and%20superiority%0Aof%20our%20proposed%20approach.%20Our%20source%20code%20and%20dataset%20are%20available%20at%0Ahttps%3A//github.com/Lumos0507/HP-MCoRe.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2501.03674v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DAction%2520Quality%2520Assessment%2520via%2520Hierarchical%2520Pose-guided%2520Multi-stage%250A%2520%2520Contrastive%2520Regression%26entry.906535625%3DMengshi%2520Qi%2520and%2520Hao%2520Ye%2520and%2520Jiaxuan%2520Peng%2520and%2520Huadong%2520Ma%26entry.1292438233%3D%2520%2520Action%2520Quality%2520Assessment%2520%2528AQA%2529%252C%2520which%2520aims%2520at%2520automatic%2520and%2520fair%2520evaluation%250Aof%2520athletic%2520performance%252C%2520has%2520gained%2520increasing%2520attention%2520in%2520recent%2520years.%250AHowever%252C%2520athletes%2520are%2520often%2520in%2520rapid%2520movement%2520and%2520the%2520corresponding%2520visual%250Aappearance%2520variances%2520are%2520subtle%252C%2520making%2520it%2520challenging%2520to%2520capture%2520fine-grained%250Apose%2520differences%2520and%2520leading%2520to%2520poor%2520estimation%2520performance.%2520Furthermore%252C%2520most%250Acommon%2520AQA%2520tasks%252C%2520such%2520as%2520diving%2520in%2520sports%252C%2520are%2520usually%2520divided%2520into%2520multiple%250Asub-actions%252C%2520each%2520of%2520which%2520contains%2520different%2520durations.%2520However%252C%2520existing%250Amethods%2520focus%2520on%2520segmenting%2520the%2520video%2520into%2520fixed%2520frames%252C%2520which%2520disrupts%2520the%250Atemporal%2520continuity%2520of%2520sub-actions%2520resulting%2520in%2520unavoidable%2520prediction%2520errors.%250ATo%2520address%2520these%2520challenges%252C%2520we%2520propose%2520a%2520novel%2520action%2520quality%2520assessment%250Amethod%2520through%2520hierarchically%2520pose-guided%2520multi-stage%2520contrastive%2520regression.%250AFirstly%252C%2520we%2520introduce%2520a%2520multi-scale%2520dynamic%2520visual-skeleton%2520encoder%2520to%2520capture%250Afine-grained%2520spatio-temporal%2520visual%2520and%2520skeletal%2520features.%2520Then%252C%2520a%2520procedure%250Asegmentation%2520network%2520is%2520introduced%2520to%2520separate%2520different%2520sub-actions%2520and%2520obtain%250Asegmented%2520features.%2520Afterwards%252C%2520the%2520segmented%2520visual%2520and%2520skeletal%2520features%2520are%250Aboth%2520fed%2520into%2520a%2520multi-modal%2520fusion%2520module%2520as%2520physics%2520structural%2520priors%252C%2520to%250Aguide%2520the%2520model%2520in%2520learning%2520refined%2520activity%2520similarities%2520and%2520variances.%250AFinally%252C%2520a%2520multi-stage%2520contrastive%2520learning%2520regression%2520approach%2520is%2520employed%2520to%250Alearn%2520discriminative%2520representations%2520and%2520output%2520prediction%2520results.%2520In%250Aaddition%252C%2520we%2520introduce%2520a%2520newly-annotated%2520FineDiving-Pose%2520Dataset%2520to%2520improve%2520the%250Acurrent%2520low-quality%2520human%2520pose%2520labels.%2520In%2520experiments%252C%2520the%2520results%2520on%250AFineDiving%2520and%2520MTL-AQA%2520datasets%2520demonstrate%2520the%2520effectiveness%2520and%2520superiority%250Aof%2520our%2520proposed%2520approach.%2520Our%2520source%2520code%2520and%2520dataset%2520are%2520available%2520at%250Ahttps%253A//github.com/Lumos0507/HP-MCoRe.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2501.03674v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Action%20Quality%20Assessment%20via%20Hierarchical%20Pose-guided%20Multi-stage%0A%20%20Contrastive%20Regression&entry.906535625=Mengshi%20Qi%20and%20Hao%20Ye%20and%20Jiaxuan%20Peng%20and%20Huadong%20Ma&entry.1292438233=%20%20Action%20Quality%20Assessment%20%28AQA%29%2C%20which%20aims%20at%20automatic%20and%20fair%20evaluation%0Aof%20athletic%20performance%2C%20has%20gained%20increasing%20attention%20in%20recent%20years.%0AHowever%2C%20athletes%20are%20often%20in%20rapid%20movement%20and%20the%20corresponding%20visual%0Aappearance%20variances%20are%20subtle%2C%20making%20it%20challenging%20to%20capture%20fine-grained%0Apose%20differences%20and%20leading%20to%20poor%20estimation%20performance.%20Furthermore%2C%20most%0Acommon%20AQA%20tasks%2C%20such%20as%20diving%20in%20sports%2C%20are%20usually%20divided%20into%20multiple%0Asub-actions%2C%20each%20of%20which%20contains%20different%20durations.%20However%2C%20existing%0Amethods%20focus%20on%20segmenting%20the%20video%20into%20fixed%20frames%2C%20which%20disrupts%20the%0Atemporal%20continuity%20of%20sub-actions%20resulting%20in%20unavoidable%20prediction%20errors.%0ATo%20address%20these%20challenges%2C%20we%20propose%20a%20novel%20action%20quality%20assessment%0Amethod%20through%20hierarchically%20pose-guided%20multi-stage%20contrastive%20regression.%0AFirstly%2C%20we%20introduce%20a%20multi-scale%20dynamic%20visual-skeleton%20encoder%20to%20capture%0Afine-grained%20spatio-temporal%20visual%20and%20skeletal%20features.%20Then%2C%20a%20procedure%0Asegmentation%20network%20is%20introduced%20to%20separate%20different%20sub-actions%20and%20obtain%0Asegmented%20features.%20Afterwards%2C%20the%20segmented%20visual%20and%20skeletal%20features%20are%0Aboth%20fed%20into%20a%20multi-modal%20fusion%20module%20as%20physics%20structural%20priors%2C%20to%0Aguide%20the%20model%20in%20learning%20refined%20activity%20similarities%20and%20variances.%0AFinally%2C%20a%20multi-stage%20contrastive%20learning%20regression%20approach%20is%20employed%20to%0Alearn%20discriminative%20representations%20and%20output%20prediction%20results.%20In%0Aaddition%2C%20we%20introduce%20a%20newly-annotated%20FineDiving-Pose%20Dataset%20to%20improve%20the%0Acurrent%20low-quality%20human%20pose%20labels.%20In%20experiments%2C%20the%20results%20on%0AFineDiving%20and%20MTL-AQA%20datasets%20demonstrate%20the%20effectiveness%20and%20superiority%0Aof%20our%20proposed%20approach.%20Our%20source%20code%20and%20dataset%20are%20available%20at%0Ahttps%3A//github.com/Lumos0507/HP-MCoRe.%0A&entry.1838667208=http%3A//arxiv.org/abs/2501.03674v1&entry.124074799=Read"},
{"title": "Fully automated workflow for designing patient-specific orthopaedic\n  implants: application to total knee arthroplasty", "author": "Aziliz Guezou-Philippe and Arnaud Clav\u00e9 and Ehouarn Maguet and Ludivine Maintier and Charles Garraud and Jean-Rassaire Fouefack and Val\u00e9rie Burdin and Eric Stindel and Guillaume Dardenne", "abstract": "  Background. Osteoarthritis affects about 528 million people worldwide,\ncausing pain and stiffness in the joints. Arthroplasty is commonly performed to\ntreat joint osteoarthritis, reducing pain and improving mobility. Nevertheless,\na significant share of patients remain unsatisfied with their surgery.\nPersonalised arthroplasty was introduced to improve surgical outcomes however\ncurrent solutions require delays, making it difficult to integrate in clinical\nroutine. We propose a fully automated workflow to design patient-specific\nimplants for total knee arthroplasty.\n  Methods. The proposed pipeline first uses artificial neural networks to\nsegment the femur and tibia proximal and distal extremities. Then the full\nbones are reconstructed using augmented statistical shape models, combining\nshape and landmarks information. Finally, 77 morphological parameters are\ncomputed to design patient-specific implants. The developed workflow has been\ntrained on 91 CT scans and evaluated on 41 CT scans, in terms of accuracy and\nexecution time.\n  Results. The workflow accuracy was $0.4\\pm0.2mm$ for segmentation,\n$1.0\\pm0.3mm$ for full bone reconstruction, and $2.2\\pm1.5mm$ for anatomical\nlandmarks determination. The custom implants fitted the patients' anatomy with\n$0.9\\pm0.5mm$ accuracy. The whole process from segmentation to implants' design\nlasted about 15 minutes.\n  Conclusion. The proposed workflow performs a fast and reliable\npersonalisation of knee implants, directly from a CT image without requiring\nany manual intervention. It allows the establishment of a patient-specific\npre-operative planning in a very short time, making it easily available for all\npatients. Combined with efficient implant manufacturing techniques, this\nsolution could help answer the growing number of arthroplasties while reducing\ncomplications and improving patients' satisfaction.\n", "link": "http://arxiv.org/abs/2403.15353v3", "date": "2025-01-07", "relevancy": 2.2871, "topK": [{"title": "GST: Precise 3D Human Body from a Single Image with Gaussian Splatting\n  Transformers", "link": "http://arxiv.org/abs/2409.04196v1", "similarity": 0.4657}, {"title": "Synthesizing Moving People with 3D Control", "link": "http://arxiv.org/abs/2401.10889v2", "similarity": 0.4536}, {"title": "Egocentric Computer Vision for Hands-Free Robotic Wheelchair Navigation", "link": "https://link.springer.com/article/10.1007/s10846-023-01807-4", "similarity": 0.4529}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Fully%20automated%20workflow%20for%20designing%20patient-specific%20orthopaedic%0A%20%20implants%3A%20application%20to%20total%20knee%20arthroplasty&body=Title%3A%20Fully%20automated%20workflow%20for%20designing%20patient-specific%20orthopaedic%0A%20%20implants%3A%20application%20to%20total%20knee%20arthroplasty%0AAuthor%3A%20Aziliz%20Guezou-Philippe%20and%20Arnaud%20Clav%C3%A9%20and%20Ehouarn%20Maguet%20and%20Ludivine%20Maintier%20and%20Charles%20Garraud%20and%20Jean-Rassaire%20Fouefack%20and%20Val%C3%A9rie%20Burdin%20and%20Eric%20Stindel%20and%20Guillaume%20Dardenne%0AAbstract%3A%20%20%20Background.%20Osteoarthritis%20affects%20about%20528%20million%20people%20worldwide%2C%0Acausing%20pain%20and%20stiffness%20in%20the%20joints.%20Arthroplasty%20is%20commonly%20performed%20to%0Atreat%20joint%20osteoarthritis%2C%20reducing%20pain%20and%20improving%20mobility.%20Nevertheless%2C%0Aa%20significant%20share%20of%20patients%20remain%20unsatisfied%20with%20their%20surgery.%0APersonalised%20arthroplasty%20was%20introduced%20to%20improve%20surgical%20outcomes%20however%0Acurrent%20solutions%20require%20delays%2C%20making%20it%20difficult%20to%20integrate%20in%20clinical%0Aroutine.%20We%20propose%20a%20fully%20automated%20workflow%20to%20design%20patient-specific%0Aimplants%20for%20total%20knee%20arthroplasty.%0A%20%20Methods.%20The%20proposed%20pipeline%20first%20uses%20artificial%20neural%20networks%20to%0Asegment%20the%20femur%20and%20tibia%20proximal%20and%20distal%20extremities.%20Then%20the%20full%0Abones%20are%20reconstructed%20using%20augmented%20statistical%20shape%20models%2C%20combining%0Ashape%20and%20landmarks%20information.%20Finally%2C%2077%20morphological%20parameters%20are%0Acomputed%20to%20design%20patient-specific%20implants.%20The%20developed%20workflow%20has%20been%0Atrained%20on%2091%20CT%20scans%20and%20evaluated%20on%2041%20CT%20scans%2C%20in%20terms%20of%20accuracy%20and%0Aexecution%20time.%0A%20%20Results.%20The%20workflow%20accuracy%20was%20%240.4%5Cpm0.2mm%24%20for%20segmentation%2C%0A%241.0%5Cpm0.3mm%24%20for%20full%20bone%20reconstruction%2C%20and%20%242.2%5Cpm1.5mm%24%20for%20anatomical%0Alandmarks%20determination.%20The%20custom%20implants%20fitted%20the%20patients%27%20anatomy%20with%0A%240.9%5Cpm0.5mm%24%20accuracy.%20The%20whole%20process%20from%20segmentation%20to%20implants%27%20design%0Alasted%20about%2015%20minutes.%0A%20%20Conclusion.%20The%20proposed%20workflow%20performs%20a%20fast%20and%20reliable%0Apersonalisation%20of%20knee%20implants%2C%20directly%20from%20a%20CT%20image%20without%20requiring%0Aany%20manual%20intervention.%20It%20allows%20the%20establishment%20of%20a%20patient-specific%0Apre-operative%20planning%20in%20a%20very%20short%20time%2C%20making%20it%20easily%20available%20for%20all%0Apatients.%20Combined%20with%20efficient%20implant%20manufacturing%20techniques%2C%20this%0Asolution%20could%20help%20answer%20the%20growing%20number%20of%20arthroplasties%20while%20reducing%0Acomplications%20and%20improving%20patients%27%20satisfaction.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2403.15353v3%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DFully%2520automated%2520workflow%2520for%2520designing%2520patient-specific%2520orthopaedic%250A%2520%2520implants%253A%2520application%2520to%2520total%2520knee%2520arthroplasty%26entry.906535625%3DAziliz%2520Guezou-Philippe%2520and%2520Arnaud%2520Clav%25C3%25A9%2520and%2520Ehouarn%2520Maguet%2520and%2520Ludivine%2520Maintier%2520and%2520Charles%2520Garraud%2520and%2520Jean-Rassaire%2520Fouefack%2520and%2520Val%25C3%25A9rie%2520Burdin%2520and%2520Eric%2520Stindel%2520and%2520Guillaume%2520Dardenne%26entry.1292438233%3D%2520%2520Background.%2520Osteoarthritis%2520affects%2520about%2520528%2520million%2520people%2520worldwide%252C%250Acausing%2520pain%2520and%2520stiffness%2520in%2520the%2520joints.%2520Arthroplasty%2520is%2520commonly%2520performed%2520to%250Atreat%2520joint%2520osteoarthritis%252C%2520reducing%2520pain%2520and%2520improving%2520mobility.%2520Nevertheless%252C%250Aa%2520significant%2520share%2520of%2520patients%2520remain%2520unsatisfied%2520with%2520their%2520surgery.%250APersonalised%2520arthroplasty%2520was%2520introduced%2520to%2520improve%2520surgical%2520outcomes%2520however%250Acurrent%2520solutions%2520require%2520delays%252C%2520making%2520it%2520difficult%2520to%2520integrate%2520in%2520clinical%250Aroutine.%2520We%2520propose%2520a%2520fully%2520automated%2520workflow%2520to%2520design%2520patient-specific%250Aimplants%2520for%2520total%2520knee%2520arthroplasty.%250A%2520%2520Methods.%2520The%2520proposed%2520pipeline%2520first%2520uses%2520artificial%2520neural%2520networks%2520to%250Asegment%2520the%2520femur%2520and%2520tibia%2520proximal%2520and%2520distal%2520extremities.%2520Then%2520the%2520full%250Abones%2520are%2520reconstructed%2520using%2520augmented%2520statistical%2520shape%2520models%252C%2520combining%250Ashape%2520and%2520landmarks%2520information.%2520Finally%252C%252077%2520morphological%2520parameters%2520are%250Acomputed%2520to%2520design%2520patient-specific%2520implants.%2520The%2520developed%2520workflow%2520has%2520been%250Atrained%2520on%252091%2520CT%2520scans%2520and%2520evaluated%2520on%252041%2520CT%2520scans%252C%2520in%2520terms%2520of%2520accuracy%2520and%250Aexecution%2520time.%250A%2520%2520Results.%2520The%2520workflow%2520accuracy%2520was%2520%25240.4%255Cpm0.2mm%2524%2520for%2520segmentation%252C%250A%25241.0%255Cpm0.3mm%2524%2520for%2520full%2520bone%2520reconstruction%252C%2520and%2520%25242.2%255Cpm1.5mm%2524%2520for%2520anatomical%250Alandmarks%2520determination.%2520The%2520custom%2520implants%2520fitted%2520the%2520patients%2527%2520anatomy%2520with%250A%25240.9%255Cpm0.5mm%2524%2520accuracy.%2520The%2520whole%2520process%2520from%2520segmentation%2520to%2520implants%2527%2520design%250Alasted%2520about%252015%2520minutes.%250A%2520%2520Conclusion.%2520The%2520proposed%2520workflow%2520performs%2520a%2520fast%2520and%2520reliable%250Apersonalisation%2520of%2520knee%2520implants%252C%2520directly%2520from%2520a%2520CT%2520image%2520without%2520requiring%250Aany%2520manual%2520intervention.%2520It%2520allows%2520the%2520establishment%2520of%2520a%2520patient-specific%250Apre-operative%2520planning%2520in%2520a%2520very%2520short%2520time%252C%2520making%2520it%2520easily%2520available%2520for%2520all%250Apatients.%2520Combined%2520with%2520efficient%2520implant%2520manufacturing%2520techniques%252C%2520this%250Asolution%2520could%2520help%2520answer%2520the%2520growing%2520number%2520of%2520arthroplasties%2520while%2520reducing%250Acomplications%2520and%2520improving%2520patients%2527%2520satisfaction.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2403.15353v3%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Fully%20automated%20workflow%20for%20designing%20patient-specific%20orthopaedic%0A%20%20implants%3A%20application%20to%20total%20knee%20arthroplasty&entry.906535625=Aziliz%20Guezou-Philippe%20and%20Arnaud%20Clav%C3%A9%20and%20Ehouarn%20Maguet%20and%20Ludivine%20Maintier%20and%20Charles%20Garraud%20and%20Jean-Rassaire%20Fouefack%20and%20Val%C3%A9rie%20Burdin%20and%20Eric%20Stindel%20and%20Guillaume%20Dardenne&entry.1292438233=%20%20Background.%20Osteoarthritis%20affects%20about%20528%20million%20people%20worldwide%2C%0Acausing%20pain%20and%20stiffness%20in%20the%20joints.%20Arthroplasty%20is%20commonly%20performed%20to%0Atreat%20joint%20osteoarthritis%2C%20reducing%20pain%20and%20improving%20mobility.%20Nevertheless%2C%0Aa%20significant%20share%20of%20patients%20remain%20unsatisfied%20with%20their%20surgery.%0APersonalised%20arthroplasty%20was%20introduced%20to%20improve%20surgical%20outcomes%20however%0Acurrent%20solutions%20require%20delays%2C%20making%20it%20difficult%20to%20integrate%20in%20clinical%0Aroutine.%20We%20propose%20a%20fully%20automated%20workflow%20to%20design%20patient-specific%0Aimplants%20for%20total%20knee%20arthroplasty.%0A%20%20Methods.%20The%20proposed%20pipeline%20first%20uses%20artificial%20neural%20networks%20to%0Asegment%20the%20femur%20and%20tibia%20proximal%20and%20distal%20extremities.%20Then%20the%20full%0Abones%20are%20reconstructed%20using%20augmented%20statistical%20shape%20models%2C%20combining%0Ashape%20and%20landmarks%20information.%20Finally%2C%2077%20morphological%20parameters%20are%0Acomputed%20to%20design%20patient-specific%20implants.%20The%20developed%20workflow%20has%20been%0Atrained%20on%2091%20CT%20scans%20and%20evaluated%20on%2041%20CT%20scans%2C%20in%20terms%20of%20accuracy%20and%0Aexecution%20time.%0A%20%20Results.%20The%20workflow%20accuracy%20was%20%240.4%5Cpm0.2mm%24%20for%20segmentation%2C%0A%241.0%5Cpm0.3mm%24%20for%20full%20bone%20reconstruction%2C%20and%20%242.2%5Cpm1.5mm%24%20for%20anatomical%0Alandmarks%20determination.%20The%20custom%20implants%20fitted%20the%20patients%27%20anatomy%20with%0A%240.9%5Cpm0.5mm%24%20accuracy.%20The%20whole%20process%20from%20segmentation%20to%20implants%27%20design%0Alasted%20about%2015%20minutes.%0A%20%20Conclusion.%20The%20proposed%20workflow%20performs%20a%20fast%20and%20reliable%0Apersonalisation%20of%20knee%20implants%2C%20directly%20from%20a%20CT%20image%20without%20requiring%0Aany%20manual%20intervention.%20It%20allows%20the%20establishment%20of%20a%20patient-specific%0Apre-operative%20planning%20in%20a%20very%20short%20time%2C%20making%20it%20easily%20available%20for%20all%0Apatients.%20Combined%20with%20efficient%20implant%20manufacturing%20techniques%2C%20this%0Asolution%20could%20help%20answer%20the%20growing%20number%20of%20arthroplasties%20while%20reducing%0Acomplications%20and%20improving%20patients%27%20satisfaction.%0A&entry.1838667208=http%3A//arxiv.org/abs/2403.15353v3&entry.124074799=Read"},
{"title": "AtMan: Understanding Transformer Predictions Through Memory Efficient\n  Attention Manipulation", "author": "Bj\u00f6rn Deiseroth and Mayukh Deb and Samuel Weinbach and Manuel Brack and Patrick Schramowski and Kristian Kersting", "abstract": "  Generative transformer models have become increasingly complex, with large\nnumbers of parameters and the ability to process multiple input modalities.\nCurrent methods for explaining their predictions are resource-intensive. Most\ncrucially, they require prohibitively large amounts of extra memory, since they\nrely on backpropagation which allocates almost twice as much GPU memory as the\nforward pass. This makes it difficult, if not impossible, to use them in\nproduction. We present AtMan that provides explanations of generative\ntransformer models at almost no extra cost. Specifically, AtMan is a\nmodality-agnostic perturbation method that manipulates the attention mechanisms\nof transformers to produce relevance maps for the input with respect to the\noutput prediction. Instead of using backpropagation, AtMan applies a\nparallelizable token-based search method based on cosine similarity\nneighborhood in the embedding space. Our exhaustive experiments on text and\nimage-text benchmarks demonstrate that AtMan outperforms current\nstate-of-the-art gradient-based methods on several metrics while being\ncomputationally efficient. As such, AtMan is suitable for use in large model\ninference deployments.\n", "link": "http://arxiv.org/abs/2301.08110v6", "date": "2025-01-07", "relevancy": 2.2854, "topK": [{"title": "Scaling Rectified Flow Transformers for High-Resolution Image Synthesis", "link": "http://arxiv.org/abs/2403.03206v1", "similarity": 0.6021}, {"title": "ToDo: Token Downsampling for Efficient Generation of High-Resolution\n  Images", "link": "http://arxiv.org/abs/2402.13573v2", "similarity": 0.5924}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.538}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20AtMan%3A%20Understanding%20Transformer%20Predictions%20Through%20Memory%20Efficient%0A%20%20Attention%20Manipulation&body=Title%3A%20AtMan%3A%20Understanding%20Transformer%20Predictions%20Through%20Memory%20Efficient%0A%20%20Attention%20Manipulation%0AAuthor%3A%20Bj%C3%B6rn%20Deiseroth%20and%20Mayukh%20Deb%20and%20Samuel%20Weinbach%20and%20Manuel%20Brack%20and%20Patrick%20Schramowski%20and%20Kristian%20Kersting%0AAbstract%3A%20%20%20Generative%20transformer%20models%20have%20become%20increasingly%20complex%2C%20with%20large%0Anumbers%20of%20parameters%20and%20the%20ability%20to%20process%20multiple%20input%20modalities.%0ACurrent%20methods%20for%20explaining%20their%20predictions%20are%20resource-intensive.%20Most%0Acrucially%2C%20they%20require%20prohibitively%20large%20amounts%20of%20extra%20memory%2C%20since%20they%0Arely%20on%20backpropagation%20which%20allocates%20almost%20twice%20as%20much%20GPU%20memory%20as%20the%0Aforward%20pass.%20This%20makes%20it%20difficult%2C%20if%20not%20impossible%2C%20to%20use%20them%20in%0Aproduction.%20We%20present%20AtMan%20that%20provides%20explanations%20of%20generative%0Atransformer%20models%20at%20almost%20no%20extra%20cost.%20Specifically%2C%20AtMan%20is%20a%0Amodality-agnostic%20perturbation%20method%20that%20manipulates%20the%20attention%20mechanisms%0Aof%20transformers%20to%20produce%20relevance%20maps%20for%20the%20input%20with%20respect%20to%20the%0Aoutput%20prediction.%20Instead%20of%20using%20backpropagation%2C%20AtMan%20applies%20a%0Aparallelizable%20token-based%20search%20method%20based%20on%20cosine%20similarity%0Aneighborhood%20in%20the%20embedding%20space.%20Our%20exhaustive%20experiments%20on%20text%20and%0Aimage-text%20benchmarks%20demonstrate%20that%20AtMan%20outperforms%20current%0Astate-of-the-art%20gradient-based%20methods%20on%20several%20metrics%20while%20being%0Acomputationally%20efficient.%20As%20such%2C%20AtMan%20is%20suitable%20for%20use%20in%20large%20model%0Ainference%20deployments.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2301.08110v6%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DAtMan%253A%2520Understanding%2520Transformer%2520Predictions%2520Through%2520Memory%2520Efficient%250A%2520%2520Attention%2520Manipulation%26entry.906535625%3DBj%25C3%25B6rn%2520Deiseroth%2520and%2520Mayukh%2520Deb%2520and%2520Samuel%2520Weinbach%2520and%2520Manuel%2520Brack%2520and%2520Patrick%2520Schramowski%2520and%2520Kristian%2520Kersting%26entry.1292438233%3D%2520%2520Generative%2520transformer%2520models%2520have%2520become%2520increasingly%2520complex%252C%2520with%2520large%250Anumbers%2520of%2520parameters%2520and%2520the%2520ability%2520to%2520process%2520multiple%2520input%2520modalities.%250ACurrent%2520methods%2520for%2520explaining%2520their%2520predictions%2520are%2520resource-intensive.%2520Most%250Acrucially%252C%2520they%2520require%2520prohibitively%2520large%2520amounts%2520of%2520extra%2520memory%252C%2520since%2520they%250Arely%2520on%2520backpropagation%2520which%2520allocates%2520almost%2520twice%2520as%2520much%2520GPU%2520memory%2520as%2520the%250Aforward%2520pass.%2520This%2520makes%2520it%2520difficult%252C%2520if%2520not%2520impossible%252C%2520to%2520use%2520them%2520in%250Aproduction.%2520We%2520present%2520AtMan%2520that%2520provides%2520explanations%2520of%2520generative%250Atransformer%2520models%2520at%2520almost%2520no%2520extra%2520cost.%2520Specifically%252C%2520AtMan%2520is%2520a%250Amodality-agnostic%2520perturbation%2520method%2520that%2520manipulates%2520the%2520attention%2520mechanisms%250Aof%2520transformers%2520to%2520produce%2520relevance%2520maps%2520for%2520the%2520input%2520with%2520respect%2520to%2520the%250Aoutput%2520prediction.%2520Instead%2520of%2520using%2520backpropagation%252C%2520AtMan%2520applies%2520a%250Aparallelizable%2520token-based%2520search%2520method%2520based%2520on%2520cosine%2520similarity%250Aneighborhood%2520in%2520the%2520embedding%2520space.%2520Our%2520exhaustive%2520experiments%2520on%2520text%2520and%250Aimage-text%2520benchmarks%2520demonstrate%2520that%2520AtMan%2520outperforms%2520current%250Astate-of-the-art%2520gradient-based%2520methods%2520on%2520several%2520metrics%2520while%2520being%250Acomputationally%2520efficient.%2520As%2520such%252C%2520AtMan%2520is%2520suitable%2520for%2520use%2520in%2520large%2520model%250Ainference%2520deployments.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2301.08110v6%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=AtMan%3A%20Understanding%20Transformer%20Predictions%20Through%20Memory%20Efficient%0A%20%20Attention%20Manipulation&entry.906535625=Bj%C3%B6rn%20Deiseroth%20and%20Mayukh%20Deb%20and%20Samuel%20Weinbach%20and%20Manuel%20Brack%20and%20Patrick%20Schramowski%20and%20Kristian%20Kersting&entry.1292438233=%20%20Generative%20transformer%20models%20have%20become%20increasingly%20complex%2C%20with%20large%0Anumbers%20of%20parameters%20and%20the%20ability%20to%20process%20multiple%20input%20modalities.%0ACurrent%20methods%20for%20explaining%20their%20predictions%20are%20resource-intensive.%20Most%0Acrucially%2C%20they%20require%20prohibitively%20large%20amounts%20of%20extra%20memory%2C%20since%20they%0Arely%20on%20backpropagation%20which%20allocates%20almost%20twice%20as%20much%20GPU%20memory%20as%20the%0Aforward%20pass.%20This%20makes%20it%20difficult%2C%20if%20not%20impossible%2C%20to%20use%20them%20in%0Aproduction.%20We%20present%20AtMan%20that%20provides%20explanations%20of%20generative%0Atransformer%20models%20at%20almost%20no%20extra%20cost.%20Specifically%2C%20AtMan%20is%20a%0Amodality-agnostic%20perturbation%20method%20that%20manipulates%20the%20attention%20mechanisms%0Aof%20transformers%20to%20produce%20relevance%20maps%20for%20the%20input%20with%20respect%20to%20the%0Aoutput%20prediction.%20Instead%20of%20using%20backpropagation%2C%20AtMan%20applies%20a%0Aparallelizable%20token-based%20search%20method%20based%20on%20cosine%20similarity%0Aneighborhood%20in%20the%20embedding%20space.%20Our%20exhaustive%20experiments%20on%20text%20and%0Aimage-text%20benchmarks%20demonstrate%20that%20AtMan%20outperforms%20current%0Astate-of-the-art%20gradient-based%20methods%20on%20several%20metrics%20while%20being%0Acomputationally%20efficient.%20As%20such%2C%20AtMan%20is%20suitable%20for%20use%20in%20large%20model%0Ainference%20deployments.%0A&entry.1838667208=http%3A//arxiv.org/abs/2301.08110v6&entry.124074799=Read"},
{"title": "Materialist: Physically Based Editing Using Single-Image Inverse\n  Rendering", "author": "Lezhong Wang and Duc Minh Tran and Ruiqi Cui and Thomson TG and Manmohan Chandraker and Jeppe Revall Frisvad", "abstract": "  To perform image editing based on single-view, inverse physically based\nrendering, we present a method combining a learning-based approach with\nprogressive differentiable rendering. Given an image, our method leverages\nneural networks to predict initial material properties. Progressive\ndifferentiable rendering is then used to optimize the environment map and\nrefine the material properties with the goal of closely matching the rendered\nresult to the input image. We require only a single image while other inverse\nrendering methods based on the rendering equation require multiple views. In\ncomparison to single-view methods that rely on neural renderers, our approach\nachieves more realistic light material interactions, accurate shadows, and\nglobal illumination. Furthermore, with optimized material properties and\nillumination, our method enables a variety of tasks, including physically based\nmaterial editing, object insertion, and relighting. We also propose a method\nfor material transparency editing that operates effectively without requiring\nfull scene geometry. Compared with methods based on Stable Diffusion, our\napproach offers stronger interpretability and more realistic light refraction\nbased on empirical results.\n", "link": "http://arxiv.org/abs/2501.03717v1", "date": "2025-01-07", "relevancy": 2.28, "topK": [{"title": "MiraGe: Editable 2D Images using Gaussian Splatting", "link": "http://arxiv.org/abs/2410.01521v1", "similarity": 0.58}, {"title": "Synthesizing Moving People with 3D Control", "link": "http://arxiv.org/abs/2401.10889v2", "similarity": 0.5638}, {"title": "Thinking Outside the BBox: Unconstrained Generative Object Compositing", "link": "http://arxiv.org/abs/2409.04559v2", "similarity": 0.5606}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Materialist%3A%20Physically%20Based%20Editing%20Using%20Single-Image%20Inverse%0A%20%20Rendering&body=Title%3A%20Materialist%3A%20Physically%20Based%20Editing%20Using%20Single-Image%20Inverse%0A%20%20Rendering%0AAuthor%3A%20Lezhong%20Wang%20and%20Duc%20Minh%20Tran%20and%20Ruiqi%20Cui%20and%20Thomson%20TG%20and%20Manmohan%20Chandraker%20and%20Jeppe%20Revall%20Frisvad%0AAbstract%3A%20%20%20To%20perform%20image%20editing%20based%20on%20single-view%2C%20inverse%20physically%20based%0Arendering%2C%20we%20present%20a%20method%20combining%20a%20learning-based%20approach%20with%0Aprogressive%20differentiable%20rendering.%20Given%20an%20image%2C%20our%20method%20leverages%0Aneural%20networks%20to%20predict%20initial%20material%20properties.%20Progressive%0Adifferentiable%20rendering%20is%20then%20used%20to%20optimize%20the%20environment%20map%20and%0Arefine%20the%20material%20properties%20with%20the%20goal%20of%20closely%20matching%20the%20rendered%0Aresult%20to%20the%20input%20image.%20We%20require%20only%20a%20single%20image%20while%20other%20inverse%0Arendering%20methods%20based%20on%20the%20rendering%20equation%20require%20multiple%20views.%20In%0Acomparison%20to%20single-view%20methods%20that%20rely%20on%20neural%20renderers%2C%20our%20approach%0Aachieves%20more%20realistic%20light%20material%20interactions%2C%20accurate%20shadows%2C%20and%0Aglobal%20illumination.%20Furthermore%2C%20with%20optimized%20material%20properties%20and%0Aillumination%2C%20our%20method%20enables%20a%20variety%20of%20tasks%2C%20including%20physically%20based%0Amaterial%20editing%2C%20object%20insertion%2C%20and%20relighting.%20We%20also%20propose%20a%20method%0Afor%20material%20transparency%20editing%20that%20operates%20effectively%20without%20requiring%0Afull%20scene%20geometry.%20Compared%20with%20methods%20based%20on%20Stable%20Diffusion%2C%20our%0Aapproach%20offers%20stronger%20interpretability%20and%20more%20realistic%20light%20refraction%0Abased%20on%20empirical%20results.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2501.03717v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DMaterialist%253A%2520Physically%2520Based%2520Editing%2520Using%2520Single-Image%2520Inverse%250A%2520%2520Rendering%26entry.906535625%3DLezhong%2520Wang%2520and%2520Duc%2520Minh%2520Tran%2520and%2520Ruiqi%2520Cui%2520and%2520Thomson%2520TG%2520and%2520Manmohan%2520Chandraker%2520and%2520Jeppe%2520Revall%2520Frisvad%26entry.1292438233%3D%2520%2520To%2520perform%2520image%2520editing%2520based%2520on%2520single-view%252C%2520inverse%2520physically%2520based%250Arendering%252C%2520we%2520present%2520a%2520method%2520combining%2520a%2520learning-based%2520approach%2520with%250Aprogressive%2520differentiable%2520rendering.%2520Given%2520an%2520image%252C%2520our%2520method%2520leverages%250Aneural%2520networks%2520to%2520predict%2520initial%2520material%2520properties.%2520Progressive%250Adifferentiable%2520rendering%2520is%2520then%2520used%2520to%2520optimize%2520the%2520environment%2520map%2520and%250Arefine%2520the%2520material%2520properties%2520with%2520the%2520goal%2520of%2520closely%2520matching%2520the%2520rendered%250Aresult%2520to%2520the%2520input%2520image.%2520We%2520require%2520only%2520a%2520single%2520image%2520while%2520other%2520inverse%250Arendering%2520methods%2520based%2520on%2520the%2520rendering%2520equation%2520require%2520multiple%2520views.%2520In%250Acomparison%2520to%2520single-view%2520methods%2520that%2520rely%2520on%2520neural%2520renderers%252C%2520our%2520approach%250Aachieves%2520more%2520realistic%2520light%2520material%2520interactions%252C%2520accurate%2520shadows%252C%2520and%250Aglobal%2520illumination.%2520Furthermore%252C%2520with%2520optimized%2520material%2520properties%2520and%250Aillumination%252C%2520our%2520method%2520enables%2520a%2520variety%2520of%2520tasks%252C%2520including%2520physically%2520based%250Amaterial%2520editing%252C%2520object%2520insertion%252C%2520and%2520relighting.%2520We%2520also%2520propose%2520a%2520method%250Afor%2520material%2520transparency%2520editing%2520that%2520operates%2520effectively%2520without%2520requiring%250Afull%2520scene%2520geometry.%2520Compared%2520with%2520methods%2520based%2520on%2520Stable%2520Diffusion%252C%2520our%250Aapproach%2520offers%2520stronger%2520interpretability%2520and%2520more%2520realistic%2520light%2520refraction%250Abased%2520on%2520empirical%2520results.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2501.03717v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Materialist%3A%20Physically%20Based%20Editing%20Using%20Single-Image%20Inverse%0A%20%20Rendering&entry.906535625=Lezhong%20Wang%20and%20Duc%20Minh%20Tran%20and%20Ruiqi%20Cui%20and%20Thomson%20TG%20and%20Manmohan%20Chandraker%20and%20Jeppe%20Revall%20Frisvad&entry.1292438233=%20%20To%20perform%20image%20editing%20based%20on%20single-view%2C%20inverse%20physically%20based%0Arendering%2C%20we%20present%20a%20method%20combining%20a%20learning-based%20approach%20with%0Aprogressive%20differentiable%20rendering.%20Given%20an%20image%2C%20our%20method%20leverages%0Aneural%20networks%20to%20predict%20initial%20material%20properties.%20Progressive%0Adifferentiable%20rendering%20is%20then%20used%20to%20optimize%20the%20environment%20map%20and%0Arefine%20the%20material%20properties%20with%20the%20goal%20of%20closely%20matching%20the%20rendered%0Aresult%20to%20the%20input%20image.%20We%20require%20only%20a%20single%20image%20while%20other%20inverse%0Arendering%20methods%20based%20on%20the%20rendering%20equation%20require%20multiple%20views.%20In%0Acomparison%20to%20single-view%20methods%20that%20rely%20on%20neural%20renderers%2C%20our%20approach%0Aachieves%20more%20realistic%20light%20material%20interactions%2C%20accurate%20shadows%2C%20and%0Aglobal%20illumination.%20Furthermore%2C%20with%20optimized%20material%20properties%20and%0Aillumination%2C%20our%20method%20enables%20a%20variety%20of%20tasks%2C%20including%20physically%20based%0Amaterial%20editing%2C%20object%20insertion%2C%20and%20relighting.%20We%20also%20propose%20a%20method%0Afor%20material%20transparency%20editing%20that%20operates%20effectively%20without%20requiring%0Afull%20scene%20geometry.%20Compared%20with%20methods%20based%20on%20Stable%20Diffusion%2C%20our%0Aapproach%20offers%20stronger%20interpretability%20and%20more%20realistic%20light%20refraction%0Abased%20on%20empirical%20results.%0A&entry.1838667208=http%3A//arxiv.org/abs/2501.03717v1&entry.124074799=Read"},
{"title": "Diverse Rare Sample Generation with Pretrained GANs", "author": "Subeen Lee and Jiyeon Han and Soyeon Kim and Jaesik Choi", "abstract": "  Deep generative models are proficient in generating realistic data but\nstruggle with producing rare samples in low density regions due to their\nscarcity of training datasets and the mode collapse problem. While recent\nmethods aim to improve the fidelity of generated samples, they often reduce\ndiversity and coverage by ignoring rare and novel samples. This study proposes\na novel approach for generating diverse rare samples from high-resolution image\ndatasets with pretrained GANs. Our method employs gradient-based optimization\nof latent vectors within a multi-objective framework and utilizes normalizing\nflows for density estimation on the feature space. This enables the generation\nof diverse rare images, with controllable parameters for rarity, diversity, and\nsimilarity to a reference image. We demonstrate the effectiveness of our\napproach both qualitatively and quantitatively across various datasets and GANs\nwithout retraining or fine-tuning the pretrained GANs.\n", "link": "http://arxiv.org/abs/2412.19543v2", "date": "2025-01-07", "relevancy": 2.2766, "topK": [{"title": "Thinking Outside the BBox: Unconstrained Generative Object Compositing", "link": "http://arxiv.org/abs/2409.04559v2", "similarity": 0.5796}, {"title": "ToDo: Token Downsampling for Efficient Generation of High-Resolution\n  Images", "link": "http://arxiv.org/abs/2402.13573v2", "similarity": 0.5771}, {"title": "UGG: Unified Generative Grasping", "link": "https://arxiv.org/abs/2311.16917", "similarity": 0.557}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Diverse%20Rare%20Sample%20Generation%20with%20Pretrained%20GANs&body=Title%3A%20Diverse%20Rare%20Sample%20Generation%20with%20Pretrained%20GANs%0AAuthor%3A%20Subeen%20Lee%20and%20Jiyeon%20Han%20and%20Soyeon%20Kim%20and%20Jaesik%20Choi%0AAbstract%3A%20%20%20Deep%20generative%20models%20are%20proficient%20in%20generating%20realistic%20data%20but%0Astruggle%20with%20producing%20rare%20samples%20in%20low%20density%20regions%20due%20to%20their%0Ascarcity%20of%20training%20datasets%20and%20the%20mode%20collapse%20problem.%20While%20recent%0Amethods%20aim%20to%20improve%20the%20fidelity%20of%20generated%20samples%2C%20they%20often%20reduce%0Adiversity%20and%20coverage%20by%20ignoring%20rare%20and%20novel%20samples.%20This%20study%20proposes%0Aa%20novel%20approach%20for%20generating%20diverse%20rare%20samples%20from%20high-resolution%20image%0Adatasets%20with%20pretrained%20GANs.%20Our%20method%20employs%20gradient-based%20optimization%0Aof%20latent%20vectors%20within%20a%20multi-objective%20framework%20and%20utilizes%20normalizing%0Aflows%20for%20density%20estimation%20on%20the%20feature%20space.%20This%20enables%20the%20generation%0Aof%20diverse%20rare%20images%2C%20with%20controllable%20parameters%20for%20rarity%2C%20diversity%2C%20and%0Asimilarity%20to%20a%20reference%20image.%20We%20demonstrate%20the%20effectiveness%20of%20our%0Aapproach%20both%20qualitatively%20and%20quantitatively%20across%20various%20datasets%20and%20GANs%0Awithout%20retraining%20or%20fine-tuning%20the%20pretrained%20GANs.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2412.19543v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DDiverse%2520Rare%2520Sample%2520Generation%2520with%2520Pretrained%2520GANs%26entry.906535625%3DSubeen%2520Lee%2520and%2520Jiyeon%2520Han%2520and%2520Soyeon%2520Kim%2520and%2520Jaesik%2520Choi%26entry.1292438233%3D%2520%2520Deep%2520generative%2520models%2520are%2520proficient%2520in%2520generating%2520realistic%2520data%2520but%250Astruggle%2520with%2520producing%2520rare%2520samples%2520in%2520low%2520density%2520regions%2520due%2520to%2520their%250Ascarcity%2520of%2520training%2520datasets%2520and%2520the%2520mode%2520collapse%2520problem.%2520While%2520recent%250Amethods%2520aim%2520to%2520improve%2520the%2520fidelity%2520of%2520generated%2520samples%252C%2520they%2520often%2520reduce%250Adiversity%2520and%2520coverage%2520by%2520ignoring%2520rare%2520and%2520novel%2520samples.%2520This%2520study%2520proposes%250Aa%2520novel%2520approach%2520for%2520generating%2520diverse%2520rare%2520samples%2520from%2520high-resolution%2520image%250Adatasets%2520with%2520pretrained%2520GANs.%2520Our%2520method%2520employs%2520gradient-based%2520optimization%250Aof%2520latent%2520vectors%2520within%2520a%2520multi-objective%2520framework%2520and%2520utilizes%2520normalizing%250Aflows%2520for%2520density%2520estimation%2520on%2520the%2520feature%2520space.%2520This%2520enables%2520the%2520generation%250Aof%2520diverse%2520rare%2520images%252C%2520with%2520controllable%2520parameters%2520for%2520rarity%252C%2520diversity%252C%2520and%250Asimilarity%2520to%2520a%2520reference%2520image.%2520We%2520demonstrate%2520the%2520effectiveness%2520of%2520our%250Aapproach%2520both%2520qualitatively%2520and%2520quantitatively%2520across%2520various%2520datasets%2520and%2520GANs%250Awithout%2520retraining%2520or%2520fine-tuning%2520the%2520pretrained%2520GANs.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2412.19543v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Diverse%20Rare%20Sample%20Generation%20with%20Pretrained%20GANs&entry.906535625=Subeen%20Lee%20and%20Jiyeon%20Han%20and%20Soyeon%20Kim%20and%20Jaesik%20Choi&entry.1292438233=%20%20Deep%20generative%20models%20are%20proficient%20in%20generating%20realistic%20data%20but%0Astruggle%20with%20producing%20rare%20samples%20in%20low%20density%20regions%20due%20to%20their%0Ascarcity%20of%20training%20datasets%20and%20the%20mode%20collapse%20problem.%20While%20recent%0Amethods%20aim%20to%20improve%20the%20fidelity%20of%20generated%20samples%2C%20they%20often%20reduce%0Adiversity%20and%20coverage%20by%20ignoring%20rare%20and%20novel%20samples.%20This%20study%20proposes%0Aa%20novel%20approach%20for%20generating%20diverse%20rare%20samples%20from%20high-resolution%20image%0Adatasets%20with%20pretrained%20GANs.%20Our%20method%20employs%20gradient-based%20optimization%0Aof%20latent%20vectors%20within%20a%20multi-objective%20framework%20and%20utilizes%20normalizing%0Aflows%20for%20density%20estimation%20on%20the%20feature%20space.%20This%20enables%20the%20generation%0Aof%20diverse%20rare%20images%2C%20with%20controllable%20parameters%20for%20rarity%2C%20diversity%2C%20and%0Asimilarity%20to%20a%20reference%20image.%20We%20demonstrate%20the%20effectiveness%20of%20our%0Aapproach%20both%20qualitatively%20and%20quantitatively%20across%20various%20datasets%20and%20GANs%0Awithout%20retraining%20or%20fine-tuning%20the%20pretrained%20GANs.%0A&entry.1838667208=http%3A//arxiv.org/abs/2412.19543v2&entry.124074799=Read"},
{"title": "Exploring the Potential of Large Language Models in Public\n  Transportation: San Antonio Case Study", "author": "Ramya Jonnala and Gongbo Liang and Jeong Yang and Izzat Alsmadi", "abstract": "  The integration of large language models (LLMs) into public transit systems\npresents a transformative opportunity to enhance urban mobility. This study\nexplores the potential of LLMs to revolutionize public transportation\nmanagement within the context of San Antonio's transit system. Leveraging the\ncapabilities of LLMs in natural language processing and data analysis, we\ninvestigate their capabilities to optimize route planning, reduce wait times,\nand provide personalized travel assistance. By utilizing the General Transit\nFeed Specification (GTFS) and other relevant data, this research aims to\ndemonstrate how LLMs can potentially improve resource allocation, elevate\npassenger satisfaction, and inform data-driven decision-making in transit\noperations. A comparative analysis of different ChatGPT models was conducted to\nassess their ability to understand transportation information, retrieve\nrelevant data, and provide comprehensive responses. Findings from this study\nsuggest that while LLMs hold immense promise for public transit, careful\nengineering and fine-tuning are essential to realizing their full potential.\nSan Antonio serves as a case study to inform the development of LLM-powered\ntransit systems in other urban environments.\n", "link": "http://arxiv.org/abs/2501.03904v1", "date": "2025-01-07", "relevancy": 2.2654, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.4536}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.4536}, {"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.4521}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Exploring%20the%20Potential%20of%20Large%20Language%20Models%20in%20Public%0A%20%20Transportation%3A%20San%20Antonio%20Case%20Study&body=Title%3A%20Exploring%20the%20Potential%20of%20Large%20Language%20Models%20in%20Public%0A%20%20Transportation%3A%20San%20Antonio%20Case%20Study%0AAuthor%3A%20Ramya%20Jonnala%20and%20Gongbo%20Liang%20and%20Jeong%20Yang%20and%20Izzat%20Alsmadi%0AAbstract%3A%20%20%20The%20integration%20of%20large%20language%20models%20%28LLMs%29%20into%20public%20transit%20systems%0Apresents%20a%20transformative%20opportunity%20to%20enhance%20urban%20mobility.%20This%20study%0Aexplores%20the%20potential%20of%20LLMs%20to%20revolutionize%20public%20transportation%0Amanagement%20within%20the%20context%20of%20San%20Antonio%27s%20transit%20system.%20Leveraging%20the%0Acapabilities%20of%20LLMs%20in%20natural%20language%20processing%20and%20data%20analysis%2C%20we%0Ainvestigate%20their%20capabilities%20to%20optimize%20route%20planning%2C%20reduce%20wait%20times%2C%0Aand%20provide%20personalized%20travel%20assistance.%20By%20utilizing%20the%20General%20Transit%0AFeed%20Specification%20%28GTFS%29%20and%20other%20relevant%20data%2C%20this%20research%20aims%20to%0Ademonstrate%20how%20LLMs%20can%20potentially%20improve%20resource%20allocation%2C%20elevate%0Apassenger%20satisfaction%2C%20and%20inform%20data-driven%20decision-making%20in%20transit%0Aoperations.%20A%20comparative%20analysis%20of%20different%20ChatGPT%20models%20was%20conducted%20to%0Aassess%20their%20ability%20to%20understand%20transportation%20information%2C%20retrieve%0Arelevant%20data%2C%20and%20provide%20comprehensive%20responses.%20Findings%20from%20this%20study%0Asuggest%20that%20while%20LLMs%20hold%20immense%20promise%20for%20public%20transit%2C%20careful%0Aengineering%20and%20fine-tuning%20are%20essential%20to%20realizing%20their%20full%20potential.%0ASan%20Antonio%20serves%20as%20a%20case%20study%20to%20inform%20the%20development%20of%20LLM-powered%0Atransit%20systems%20in%20other%20urban%20environments.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2501.03904v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DExploring%2520the%2520Potential%2520of%2520Large%2520Language%2520Models%2520in%2520Public%250A%2520%2520Transportation%253A%2520San%2520Antonio%2520Case%2520Study%26entry.906535625%3DRamya%2520Jonnala%2520and%2520Gongbo%2520Liang%2520and%2520Jeong%2520Yang%2520and%2520Izzat%2520Alsmadi%26entry.1292438233%3D%2520%2520The%2520integration%2520of%2520large%2520language%2520models%2520%2528LLMs%2529%2520into%2520public%2520transit%2520systems%250Apresents%2520a%2520transformative%2520opportunity%2520to%2520enhance%2520urban%2520mobility.%2520This%2520study%250Aexplores%2520the%2520potential%2520of%2520LLMs%2520to%2520revolutionize%2520public%2520transportation%250Amanagement%2520within%2520the%2520context%2520of%2520San%2520Antonio%2527s%2520transit%2520system.%2520Leveraging%2520the%250Acapabilities%2520of%2520LLMs%2520in%2520natural%2520language%2520processing%2520and%2520data%2520analysis%252C%2520we%250Ainvestigate%2520their%2520capabilities%2520to%2520optimize%2520route%2520planning%252C%2520reduce%2520wait%2520times%252C%250Aand%2520provide%2520personalized%2520travel%2520assistance.%2520By%2520utilizing%2520the%2520General%2520Transit%250AFeed%2520Specification%2520%2528GTFS%2529%2520and%2520other%2520relevant%2520data%252C%2520this%2520research%2520aims%2520to%250Ademonstrate%2520how%2520LLMs%2520can%2520potentially%2520improve%2520resource%2520allocation%252C%2520elevate%250Apassenger%2520satisfaction%252C%2520and%2520inform%2520data-driven%2520decision-making%2520in%2520transit%250Aoperations.%2520A%2520comparative%2520analysis%2520of%2520different%2520ChatGPT%2520models%2520was%2520conducted%2520to%250Aassess%2520their%2520ability%2520to%2520understand%2520transportation%2520information%252C%2520retrieve%250Arelevant%2520data%252C%2520and%2520provide%2520comprehensive%2520responses.%2520Findings%2520from%2520this%2520study%250Asuggest%2520that%2520while%2520LLMs%2520hold%2520immense%2520promise%2520for%2520public%2520transit%252C%2520careful%250Aengineering%2520and%2520fine-tuning%2520are%2520essential%2520to%2520realizing%2520their%2520full%2520potential.%250ASan%2520Antonio%2520serves%2520as%2520a%2520case%2520study%2520to%2520inform%2520the%2520development%2520of%2520LLM-powered%250Atransit%2520systems%2520in%2520other%2520urban%2520environments.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2501.03904v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Exploring%20the%20Potential%20of%20Large%20Language%20Models%20in%20Public%0A%20%20Transportation%3A%20San%20Antonio%20Case%20Study&entry.906535625=Ramya%20Jonnala%20and%20Gongbo%20Liang%20and%20Jeong%20Yang%20and%20Izzat%20Alsmadi&entry.1292438233=%20%20The%20integration%20of%20large%20language%20models%20%28LLMs%29%20into%20public%20transit%20systems%0Apresents%20a%20transformative%20opportunity%20to%20enhance%20urban%20mobility.%20This%20study%0Aexplores%20the%20potential%20of%20LLMs%20to%20revolutionize%20public%20transportation%0Amanagement%20within%20the%20context%20of%20San%20Antonio%27s%20transit%20system.%20Leveraging%20the%0Acapabilities%20of%20LLMs%20in%20natural%20language%20processing%20and%20data%20analysis%2C%20we%0Ainvestigate%20their%20capabilities%20to%20optimize%20route%20planning%2C%20reduce%20wait%20times%2C%0Aand%20provide%20personalized%20travel%20assistance.%20By%20utilizing%20the%20General%20Transit%0AFeed%20Specification%20%28GTFS%29%20and%20other%20relevant%20data%2C%20this%20research%20aims%20to%0Ademonstrate%20how%20LLMs%20can%20potentially%20improve%20resource%20allocation%2C%20elevate%0Apassenger%20satisfaction%2C%20and%20inform%20data-driven%20decision-making%20in%20transit%0Aoperations.%20A%20comparative%20analysis%20of%20different%20ChatGPT%20models%20was%20conducted%20to%0Aassess%20their%20ability%20to%20understand%20transportation%20information%2C%20retrieve%0Arelevant%20data%2C%20and%20provide%20comprehensive%20responses.%20Findings%20from%20this%20study%0Asuggest%20that%20while%20LLMs%20hold%20immense%20promise%20for%20public%20transit%2C%20careful%0Aengineering%20and%20fine-tuning%20are%20essential%20to%20realizing%20their%20full%20potential.%0ASan%20Antonio%20serves%20as%20a%20case%20study%20to%20inform%20the%20development%20of%20LLM-powered%0Atransit%20systems%20in%20other%20urban%20environments.%0A&entry.1838667208=http%3A//arxiv.org/abs/2501.03904v1&entry.124074799=Read"},
{"title": "Learning Lipschitz Operators with respect to Gaussian Measures with\n  Near-Optimal Sample Complexity", "author": "Ben Adcock and Michael Griebel and Gregor Maier", "abstract": "  Operator learning, the approximation of mappings between infinite-dimensional\nfunction spaces using ideas from machine learning, has gained increasing\nresearch attention in recent years. Approximate operators, learned from data,\nhold promise to serve as efficient surrogate models for problems in\ncomputational science and engineering, complementing traditional numerical\nmethods. However, despite their empirical success, our understanding of the\nunderpinning mathematical theory is in large part still incomplete. In this\npaper, we study the approximation of Lipschitz operators in expectation with\nrespect to Gaussian measures. We prove higher Gaussian Sobolev regularity of\nLipschitz operators and establish lower and upper bounds on the Hermite\npolynomial approximation error. We further consider the reconstruction of\nLipschitz operators from $m$ arbitrary (adaptive) linear samples. A key finding\nis the tight characterization of the smallest achievable error for all possible\n(adaptive) sampling and reconstruction maps in terms of $m$. It is shown that\nHermite polynomial approximation is an optimal recovery strategy, but we have\nthe following curse of sample complexity: No method to approximate Lipschitz\noperators based on $m$ samples can achieve algebraic convergence rates in $m$.\nOn the positive side, we prove that a sufficiently fast spectral decay of the\ncovariance operator of the Gaussian measure guarantees convergence rates which\nare arbitrarily close to any algebraic rate in the large data limit $m \\to\n\\infty$. A main focus of this work is on the recovery of Lipschitz operators\nfrom finitely many point samples. We use Christoffel sampling and weighted\nleast-squares approximation to propose an algorithm which provably achieves\nnear-optimal sample complexity in high probability.\n", "link": "http://arxiv.org/abs/2410.23440v2", "date": "2025-01-07", "relevancy": 2.2545, "topK": [{"title": "Dynamic Gaussians Mesh: Consistent Mesh Reconstruction from Monocular\n  Videos", "link": "http://arxiv.org/abs/2404.12379v2", "similarity": 0.4581}, {"title": "RTG-SLAM: Real-time 3D Reconstruction at Scale using Gaussian Splatting", "link": "http://arxiv.org/abs/2404.19706v1", "similarity": 0.4536}, {"title": "Breadcrumbs: Adversarial class-balanced sampling for long-tailed recognition", "link": "https://link.springer.com/chapter/10.1007/978-3-031-20053-3_37", "similarity": 0.441}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Learning%20Lipschitz%20Operators%20with%20respect%20to%20Gaussian%20Measures%20with%0A%20%20Near-Optimal%20Sample%20Complexity&body=Title%3A%20Learning%20Lipschitz%20Operators%20with%20respect%20to%20Gaussian%20Measures%20with%0A%20%20Near-Optimal%20Sample%20Complexity%0AAuthor%3A%20Ben%20Adcock%20and%20Michael%20Griebel%20and%20Gregor%20Maier%0AAbstract%3A%20%20%20Operator%20learning%2C%20the%20approximation%20of%20mappings%20between%20infinite-dimensional%0Afunction%20spaces%20using%20ideas%20from%20machine%20learning%2C%20has%20gained%20increasing%0Aresearch%20attention%20in%20recent%20years.%20Approximate%20operators%2C%20learned%20from%20data%2C%0Ahold%20promise%20to%20serve%20as%20efficient%20surrogate%20models%20for%20problems%20in%0Acomputational%20science%20and%20engineering%2C%20complementing%20traditional%20numerical%0Amethods.%20However%2C%20despite%20their%20empirical%20success%2C%20our%20understanding%20of%20the%0Aunderpinning%20mathematical%20theory%20is%20in%20large%20part%20still%20incomplete.%20In%20this%0Apaper%2C%20we%20study%20the%20approximation%20of%20Lipschitz%20operators%20in%20expectation%20with%0Arespect%20to%20Gaussian%20measures.%20We%20prove%20higher%20Gaussian%20Sobolev%20regularity%20of%0ALipschitz%20operators%20and%20establish%20lower%20and%20upper%20bounds%20on%20the%20Hermite%0Apolynomial%20approximation%20error.%20We%20further%20consider%20the%20reconstruction%20of%0ALipschitz%20operators%20from%20%24m%24%20arbitrary%20%28adaptive%29%20linear%20samples.%20A%20key%20finding%0Ais%20the%20tight%20characterization%20of%20the%20smallest%20achievable%20error%20for%20all%20possible%0A%28adaptive%29%20sampling%20and%20reconstruction%20maps%20in%20terms%20of%20%24m%24.%20It%20is%20shown%20that%0AHermite%20polynomial%20approximation%20is%20an%20optimal%20recovery%20strategy%2C%20but%20we%20have%0Athe%20following%20curse%20of%20sample%20complexity%3A%20No%20method%20to%20approximate%20Lipschitz%0Aoperators%20based%20on%20%24m%24%20samples%20can%20achieve%20algebraic%20convergence%20rates%20in%20%24m%24.%0AOn%20the%20positive%20side%2C%20we%20prove%20that%20a%20sufficiently%20fast%20spectral%20decay%20of%20the%0Acovariance%20operator%20of%20the%20Gaussian%20measure%20guarantees%20convergence%20rates%20which%0Aare%20arbitrarily%20close%20to%20any%20algebraic%20rate%20in%20the%20large%20data%20limit%20%24m%20%5Cto%0A%5Cinfty%24.%20A%20main%20focus%20of%20this%20work%20is%20on%20the%20recovery%20of%20Lipschitz%20operators%0Afrom%20finitely%20many%20point%20samples.%20We%20use%20Christoffel%20sampling%20and%20weighted%0Aleast-squares%20approximation%20to%20propose%20an%20algorithm%20which%20provably%20achieves%0Anear-optimal%20sample%20complexity%20in%20high%20probability.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2410.23440v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DLearning%2520Lipschitz%2520Operators%2520with%2520respect%2520to%2520Gaussian%2520Measures%2520with%250A%2520%2520Near-Optimal%2520Sample%2520Complexity%26entry.906535625%3DBen%2520Adcock%2520and%2520Michael%2520Griebel%2520and%2520Gregor%2520Maier%26entry.1292438233%3D%2520%2520Operator%2520learning%252C%2520the%2520approximation%2520of%2520mappings%2520between%2520infinite-dimensional%250Afunction%2520spaces%2520using%2520ideas%2520from%2520machine%2520learning%252C%2520has%2520gained%2520increasing%250Aresearch%2520attention%2520in%2520recent%2520years.%2520Approximate%2520operators%252C%2520learned%2520from%2520data%252C%250Ahold%2520promise%2520to%2520serve%2520as%2520efficient%2520surrogate%2520models%2520for%2520problems%2520in%250Acomputational%2520science%2520and%2520engineering%252C%2520complementing%2520traditional%2520numerical%250Amethods.%2520However%252C%2520despite%2520their%2520empirical%2520success%252C%2520our%2520understanding%2520of%2520the%250Aunderpinning%2520mathematical%2520theory%2520is%2520in%2520large%2520part%2520still%2520incomplete.%2520In%2520this%250Apaper%252C%2520we%2520study%2520the%2520approximation%2520of%2520Lipschitz%2520operators%2520in%2520expectation%2520with%250Arespect%2520to%2520Gaussian%2520measures.%2520We%2520prove%2520higher%2520Gaussian%2520Sobolev%2520regularity%2520of%250ALipschitz%2520operators%2520and%2520establish%2520lower%2520and%2520upper%2520bounds%2520on%2520the%2520Hermite%250Apolynomial%2520approximation%2520error.%2520We%2520further%2520consider%2520the%2520reconstruction%2520of%250ALipschitz%2520operators%2520from%2520%2524m%2524%2520arbitrary%2520%2528adaptive%2529%2520linear%2520samples.%2520A%2520key%2520finding%250Ais%2520the%2520tight%2520characterization%2520of%2520the%2520smallest%2520achievable%2520error%2520for%2520all%2520possible%250A%2528adaptive%2529%2520sampling%2520and%2520reconstruction%2520maps%2520in%2520terms%2520of%2520%2524m%2524.%2520It%2520is%2520shown%2520that%250AHermite%2520polynomial%2520approximation%2520is%2520an%2520optimal%2520recovery%2520strategy%252C%2520but%2520we%2520have%250Athe%2520following%2520curse%2520of%2520sample%2520complexity%253A%2520No%2520method%2520to%2520approximate%2520Lipschitz%250Aoperators%2520based%2520on%2520%2524m%2524%2520samples%2520can%2520achieve%2520algebraic%2520convergence%2520rates%2520in%2520%2524m%2524.%250AOn%2520the%2520positive%2520side%252C%2520we%2520prove%2520that%2520a%2520sufficiently%2520fast%2520spectral%2520decay%2520of%2520the%250Acovariance%2520operator%2520of%2520the%2520Gaussian%2520measure%2520guarantees%2520convergence%2520rates%2520which%250Aare%2520arbitrarily%2520close%2520to%2520any%2520algebraic%2520rate%2520in%2520the%2520large%2520data%2520limit%2520%2524m%2520%255Cto%250A%255Cinfty%2524.%2520A%2520main%2520focus%2520of%2520this%2520work%2520is%2520on%2520the%2520recovery%2520of%2520Lipschitz%2520operators%250Afrom%2520finitely%2520many%2520point%2520samples.%2520We%2520use%2520Christoffel%2520sampling%2520and%2520weighted%250Aleast-squares%2520approximation%2520to%2520propose%2520an%2520algorithm%2520which%2520provably%2520achieves%250Anear-optimal%2520sample%2520complexity%2520in%2520high%2520probability.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2410.23440v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Learning%20Lipschitz%20Operators%20with%20respect%20to%20Gaussian%20Measures%20with%0A%20%20Near-Optimal%20Sample%20Complexity&entry.906535625=Ben%20Adcock%20and%20Michael%20Griebel%20and%20Gregor%20Maier&entry.1292438233=%20%20Operator%20learning%2C%20the%20approximation%20of%20mappings%20between%20infinite-dimensional%0Afunction%20spaces%20using%20ideas%20from%20machine%20learning%2C%20has%20gained%20increasing%0Aresearch%20attention%20in%20recent%20years.%20Approximate%20operators%2C%20learned%20from%20data%2C%0Ahold%20promise%20to%20serve%20as%20efficient%20surrogate%20models%20for%20problems%20in%0Acomputational%20science%20and%20engineering%2C%20complementing%20traditional%20numerical%0Amethods.%20However%2C%20despite%20their%20empirical%20success%2C%20our%20understanding%20of%20the%0Aunderpinning%20mathematical%20theory%20is%20in%20large%20part%20still%20incomplete.%20In%20this%0Apaper%2C%20we%20study%20the%20approximation%20of%20Lipschitz%20operators%20in%20expectation%20with%0Arespect%20to%20Gaussian%20measures.%20We%20prove%20higher%20Gaussian%20Sobolev%20regularity%20of%0ALipschitz%20operators%20and%20establish%20lower%20and%20upper%20bounds%20on%20the%20Hermite%0Apolynomial%20approximation%20error.%20We%20further%20consider%20the%20reconstruction%20of%0ALipschitz%20operators%20from%20%24m%24%20arbitrary%20%28adaptive%29%20linear%20samples.%20A%20key%20finding%0Ais%20the%20tight%20characterization%20of%20the%20smallest%20achievable%20error%20for%20all%20possible%0A%28adaptive%29%20sampling%20and%20reconstruction%20maps%20in%20terms%20of%20%24m%24.%20It%20is%20shown%20that%0AHermite%20polynomial%20approximation%20is%20an%20optimal%20recovery%20strategy%2C%20but%20we%20have%0Athe%20following%20curse%20of%20sample%20complexity%3A%20No%20method%20to%20approximate%20Lipschitz%0Aoperators%20based%20on%20%24m%24%20samples%20can%20achieve%20algebraic%20convergence%20rates%20in%20%24m%24.%0AOn%20the%20positive%20side%2C%20we%20prove%20that%20a%20sufficiently%20fast%20spectral%20decay%20of%20the%0Acovariance%20operator%20of%20the%20Gaussian%20measure%20guarantees%20convergence%20rates%20which%0Aare%20arbitrarily%20close%20to%20any%20algebraic%20rate%20in%20the%20large%20data%20limit%20%24m%20%5Cto%0A%5Cinfty%24.%20A%20main%20focus%20of%20this%20work%20is%20on%20the%20recovery%20of%20Lipschitz%20operators%0Afrom%20finitely%20many%20point%20samples.%20We%20use%20Christoffel%20sampling%20and%20weighted%0Aleast-squares%20approximation%20to%20propose%20an%20algorithm%20which%20provably%20achieves%0Anear-optimal%20sample%20complexity%20in%20high%20probability.%0A&entry.1838667208=http%3A//arxiv.org/abs/2410.23440v2&entry.124074799=Read"},
{"title": "Catch Causal Signals from Edges for Label Imbalance in Graph\n  Classification", "author": "Fengrui Zhang and Yujia Yin and Hongzong Li and Yifan Chen and Tianyi Qu", "abstract": "  Despite significant advancements in causal research on graphs and its\napplication to cracking label imbalance, the role of edge features in detecting\nthe causal effects within graphs has been largely overlooked, leaving existing\nmethods with untapped potential for further performance gains. In this paper,\nwe enhance the causal attention mechanism through effectively leveraging edge\ninformation to disentangle the causal subgraph from the original graph, as well\nas further utilizing edge features to reshape graph representations. Capturing\nmore comprehensive causal signals, our design leads to improved performance on\ngraph classification tasks with label imbalance issues. We evaluate our\napproach on real-word datasets PTC, Tox21, and ogbg-molhiv, observing\nimprovements over baselines. Overall, we highlight the importance of edge\nfeatures in graph causal detection and provide a promising direction for\naddressing label imbalance challenges in graph-level tasks. The model\nimplementation details and the codes are available on\nhttps://github.com/fengrui-z/ECAL\n", "link": "http://arxiv.org/abs/2501.01707v2", "date": "2025-01-07", "relevancy": 2.2497, "topK": [{"title": "Breadcrumbs: Adversarial class-balanced sampling for long-tailed recognition", "link": "https://link.springer.com/chapter/10.1007/978-3-031-20053-3_37", "similarity": 0.4575}, {"title": "Self-supervised Photographic Image Layout Representation Learning", "link": "http://arxiv.org/abs/2403.03740v1", "similarity": 0.4497}, {"title": "Boosted dynamic neural networks", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/26302", "similarity": 0.4425}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Catch%20Causal%20Signals%20from%20Edges%20for%20Label%20Imbalance%20in%20Graph%0A%20%20Classification&body=Title%3A%20Catch%20Causal%20Signals%20from%20Edges%20for%20Label%20Imbalance%20in%20Graph%0A%20%20Classification%0AAuthor%3A%20Fengrui%20Zhang%20and%20Yujia%20Yin%20and%20Hongzong%20Li%20and%20Yifan%20Chen%20and%20Tianyi%20Qu%0AAbstract%3A%20%20%20Despite%20significant%20advancements%20in%20causal%20research%20on%20graphs%20and%20its%0Aapplication%20to%20cracking%20label%20imbalance%2C%20the%20role%20of%20edge%20features%20in%20detecting%0Athe%20causal%20effects%20within%20graphs%20has%20been%20largely%20overlooked%2C%20leaving%20existing%0Amethods%20with%20untapped%20potential%20for%20further%20performance%20gains.%20In%20this%20paper%2C%0Awe%20enhance%20the%20causal%20attention%20mechanism%20through%20effectively%20leveraging%20edge%0Ainformation%20to%20disentangle%20the%20causal%20subgraph%20from%20the%20original%20graph%2C%20as%20well%0Aas%20further%20utilizing%20edge%20features%20to%20reshape%20graph%20representations.%20Capturing%0Amore%20comprehensive%20causal%20signals%2C%20our%20design%20leads%20to%20improved%20performance%20on%0Agraph%20classification%20tasks%20with%20label%20imbalance%20issues.%20We%20evaluate%20our%0Aapproach%20on%20real-word%20datasets%20PTC%2C%20Tox21%2C%20and%20ogbg-molhiv%2C%20observing%0Aimprovements%20over%20baselines.%20Overall%2C%20we%20highlight%20the%20importance%20of%20edge%0Afeatures%20in%20graph%20causal%20detection%20and%20provide%20a%20promising%20direction%20for%0Aaddressing%20label%20imbalance%20challenges%20in%20graph-level%20tasks.%20The%20model%0Aimplementation%20details%20and%20the%20codes%20are%20available%20on%0Ahttps%3A//github.com/fengrui-z/ECAL%0A%0ALink%3A%20http%3A//arxiv.org/abs/2501.01707v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DCatch%2520Causal%2520Signals%2520from%2520Edges%2520for%2520Label%2520Imbalance%2520in%2520Graph%250A%2520%2520Classification%26entry.906535625%3DFengrui%2520Zhang%2520and%2520Yujia%2520Yin%2520and%2520Hongzong%2520Li%2520and%2520Yifan%2520Chen%2520and%2520Tianyi%2520Qu%26entry.1292438233%3D%2520%2520Despite%2520significant%2520advancements%2520in%2520causal%2520research%2520on%2520graphs%2520and%2520its%250Aapplication%2520to%2520cracking%2520label%2520imbalance%252C%2520the%2520role%2520of%2520edge%2520features%2520in%2520detecting%250Athe%2520causal%2520effects%2520within%2520graphs%2520has%2520been%2520largely%2520overlooked%252C%2520leaving%2520existing%250Amethods%2520with%2520untapped%2520potential%2520for%2520further%2520performance%2520gains.%2520In%2520this%2520paper%252C%250Awe%2520enhance%2520the%2520causal%2520attention%2520mechanism%2520through%2520effectively%2520leveraging%2520edge%250Ainformation%2520to%2520disentangle%2520the%2520causal%2520subgraph%2520from%2520the%2520original%2520graph%252C%2520as%2520well%250Aas%2520further%2520utilizing%2520edge%2520features%2520to%2520reshape%2520graph%2520representations.%2520Capturing%250Amore%2520comprehensive%2520causal%2520signals%252C%2520our%2520design%2520leads%2520to%2520improved%2520performance%2520on%250Agraph%2520classification%2520tasks%2520with%2520label%2520imbalance%2520issues.%2520We%2520evaluate%2520our%250Aapproach%2520on%2520real-word%2520datasets%2520PTC%252C%2520Tox21%252C%2520and%2520ogbg-molhiv%252C%2520observing%250Aimprovements%2520over%2520baselines.%2520Overall%252C%2520we%2520highlight%2520the%2520importance%2520of%2520edge%250Afeatures%2520in%2520graph%2520causal%2520detection%2520and%2520provide%2520a%2520promising%2520direction%2520for%250Aaddressing%2520label%2520imbalance%2520challenges%2520in%2520graph-level%2520tasks.%2520The%2520model%250Aimplementation%2520details%2520and%2520the%2520codes%2520are%2520available%2520on%250Ahttps%253A//github.com/fengrui-z/ECAL%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2501.01707v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Catch%20Causal%20Signals%20from%20Edges%20for%20Label%20Imbalance%20in%20Graph%0A%20%20Classification&entry.906535625=Fengrui%20Zhang%20and%20Yujia%20Yin%20and%20Hongzong%20Li%20and%20Yifan%20Chen%20and%20Tianyi%20Qu&entry.1292438233=%20%20Despite%20significant%20advancements%20in%20causal%20research%20on%20graphs%20and%20its%0Aapplication%20to%20cracking%20label%20imbalance%2C%20the%20role%20of%20edge%20features%20in%20detecting%0Athe%20causal%20effects%20within%20graphs%20has%20been%20largely%20overlooked%2C%20leaving%20existing%0Amethods%20with%20untapped%20potential%20for%20further%20performance%20gains.%20In%20this%20paper%2C%0Awe%20enhance%20the%20causal%20attention%20mechanism%20through%20effectively%20leveraging%20edge%0Ainformation%20to%20disentangle%20the%20causal%20subgraph%20from%20the%20original%20graph%2C%20as%20well%0Aas%20further%20utilizing%20edge%20features%20to%20reshape%20graph%20representations.%20Capturing%0Amore%20comprehensive%20causal%20signals%2C%20our%20design%20leads%20to%20improved%20performance%20on%0Agraph%20classification%20tasks%20with%20label%20imbalance%20issues.%20We%20evaluate%20our%0Aapproach%20on%20real-word%20datasets%20PTC%2C%20Tox21%2C%20and%20ogbg-molhiv%2C%20observing%0Aimprovements%20over%20baselines.%20Overall%2C%20we%20highlight%20the%20importance%20of%20edge%0Afeatures%20in%20graph%20causal%20detection%20and%20provide%20a%20promising%20direction%20for%0Aaddressing%20label%20imbalance%20challenges%20in%20graph-level%20tasks.%20The%20model%0Aimplementation%20details%20and%20the%20codes%20are%20available%20on%0Ahttps%3A//github.com/fengrui-z/ECAL%0A&entry.1838667208=http%3A//arxiv.org/abs/2501.01707v2&entry.124074799=Read"},
{"title": "Hybrid Machine Learning Model with a Constrained Action Space for\n  Trajectory Prediction", "author": "Alexander Fertig and Lakshman Balasubramanian and Michael Botsch", "abstract": "  Trajectory prediction is crucial to advance autonomous driving, improving\nsafety, and efficiency. Although end-to-end models based on deep learning have\ngreat potential, they often do not consider vehicle dynamic limitations,\nleading to unrealistic predictions. To address this problem, this work\nintroduces a novel hybrid model that combines deep learning with a kinematic\nmotion model. It is able to predict object attributes such as acceleration and\nyaw rate and generate trajectories based on them. A key contribution is the\nincorporation of expert knowledge into the learning objective of the deep\nlearning model. This results in the constraint of the available action space,\nthus enabling the prediction of physically feasible object attributes and\ntrajectories, thereby increasing safety and robustness. The proposed hybrid\nmodel facilitates enhanced interpretability, thereby reinforcing the\ntrustworthiness of deep learning methods and promoting the development of safe\nplanning solutions. Experiments conducted on the publicly available real-world\nArgoverse dataset demonstrate realistic driving behaviour, with benchmark\ncomparisons and ablation studies showing promising results.\n", "link": "http://arxiv.org/abs/2501.03666v1", "date": "2025-01-07", "relevancy": 2.2418, "topK": [{"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.6204}, {"title": "Synthesizing Moving People with 3D Control", "link": "http://arxiv.org/abs/2401.10889v2", "similarity": 0.5571}, {"title": "Egocentric Computer Vision for Hands-Free Robotic Wheelchair Navigation", "link": "https://link.springer.com/article/10.1007/s10846-023-01807-4", "similarity": 0.5399}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Hybrid%20Machine%20Learning%20Model%20with%20a%20Constrained%20Action%20Space%20for%0A%20%20Trajectory%20Prediction&body=Title%3A%20Hybrid%20Machine%20Learning%20Model%20with%20a%20Constrained%20Action%20Space%20for%0A%20%20Trajectory%20Prediction%0AAuthor%3A%20Alexander%20Fertig%20and%20Lakshman%20Balasubramanian%20and%20Michael%20Botsch%0AAbstract%3A%20%20%20Trajectory%20prediction%20is%20crucial%20to%20advance%20autonomous%20driving%2C%20improving%0Asafety%2C%20and%20efficiency.%20Although%20end-to-end%20models%20based%20on%20deep%20learning%20have%0Agreat%20potential%2C%20they%20often%20do%20not%20consider%20vehicle%20dynamic%20limitations%2C%0Aleading%20to%20unrealistic%20predictions.%20To%20address%20this%20problem%2C%20this%20work%0Aintroduces%20a%20novel%20hybrid%20model%20that%20combines%20deep%20learning%20with%20a%20kinematic%0Amotion%20model.%20It%20is%20able%20to%20predict%20object%20attributes%20such%20as%20acceleration%20and%0Ayaw%20rate%20and%20generate%20trajectories%20based%20on%20them.%20A%20key%20contribution%20is%20the%0Aincorporation%20of%20expert%20knowledge%20into%20the%20learning%20objective%20of%20the%20deep%0Alearning%20model.%20This%20results%20in%20the%20constraint%20of%20the%20available%20action%20space%2C%0Athus%20enabling%20the%20prediction%20of%20physically%20feasible%20object%20attributes%20and%0Atrajectories%2C%20thereby%20increasing%20safety%20and%20robustness.%20The%20proposed%20hybrid%0Amodel%20facilitates%20enhanced%20interpretability%2C%20thereby%20reinforcing%20the%0Atrustworthiness%20of%20deep%20learning%20methods%20and%20promoting%20the%20development%20of%20safe%0Aplanning%20solutions.%20Experiments%20conducted%20on%20the%20publicly%20available%20real-world%0AArgoverse%20dataset%20demonstrate%20realistic%20driving%20behaviour%2C%20with%20benchmark%0Acomparisons%20and%20ablation%20studies%20showing%20promising%20results.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2501.03666v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DHybrid%2520Machine%2520Learning%2520Model%2520with%2520a%2520Constrained%2520Action%2520Space%2520for%250A%2520%2520Trajectory%2520Prediction%26entry.906535625%3DAlexander%2520Fertig%2520and%2520Lakshman%2520Balasubramanian%2520and%2520Michael%2520Botsch%26entry.1292438233%3D%2520%2520Trajectory%2520prediction%2520is%2520crucial%2520to%2520advance%2520autonomous%2520driving%252C%2520improving%250Asafety%252C%2520and%2520efficiency.%2520Although%2520end-to-end%2520models%2520based%2520on%2520deep%2520learning%2520have%250Agreat%2520potential%252C%2520they%2520often%2520do%2520not%2520consider%2520vehicle%2520dynamic%2520limitations%252C%250Aleading%2520to%2520unrealistic%2520predictions.%2520To%2520address%2520this%2520problem%252C%2520this%2520work%250Aintroduces%2520a%2520novel%2520hybrid%2520model%2520that%2520combines%2520deep%2520learning%2520with%2520a%2520kinematic%250Amotion%2520model.%2520It%2520is%2520able%2520to%2520predict%2520object%2520attributes%2520such%2520as%2520acceleration%2520and%250Ayaw%2520rate%2520and%2520generate%2520trajectories%2520based%2520on%2520them.%2520A%2520key%2520contribution%2520is%2520the%250Aincorporation%2520of%2520expert%2520knowledge%2520into%2520the%2520learning%2520objective%2520of%2520the%2520deep%250Alearning%2520model.%2520This%2520results%2520in%2520the%2520constraint%2520of%2520the%2520available%2520action%2520space%252C%250Athus%2520enabling%2520the%2520prediction%2520of%2520physically%2520feasible%2520object%2520attributes%2520and%250Atrajectories%252C%2520thereby%2520increasing%2520safety%2520and%2520robustness.%2520The%2520proposed%2520hybrid%250Amodel%2520facilitates%2520enhanced%2520interpretability%252C%2520thereby%2520reinforcing%2520the%250Atrustworthiness%2520of%2520deep%2520learning%2520methods%2520and%2520promoting%2520the%2520development%2520of%2520safe%250Aplanning%2520solutions.%2520Experiments%2520conducted%2520on%2520the%2520publicly%2520available%2520real-world%250AArgoverse%2520dataset%2520demonstrate%2520realistic%2520driving%2520behaviour%252C%2520with%2520benchmark%250Acomparisons%2520and%2520ablation%2520studies%2520showing%2520promising%2520results.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2501.03666v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Hybrid%20Machine%20Learning%20Model%20with%20a%20Constrained%20Action%20Space%20for%0A%20%20Trajectory%20Prediction&entry.906535625=Alexander%20Fertig%20and%20Lakshman%20Balasubramanian%20and%20Michael%20Botsch&entry.1292438233=%20%20Trajectory%20prediction%20is%20crucial%20to%20advance%20autonomous%20driving%2C%20improving%0Asafety%2C%20and%20efficiency.%20Although%20end-to-end%20models%20based%20on%20deep%20learning%20have%0Agreat%20potential%2C%20they%20often%20do%20not%20consider%20vehicle%20dynamic%20limitations%2C%0Aleading%20to%20unrealistic%20predictions.%20To%20address%20this%20problem%2C%20this%20work%0Aintroduces%20a%20novel%20hybrid%20model%20that%20combines%20deep%20learning%20with%20a%20kinematic%0Amotion%20model.%20It%20is%20able%20to%20predict%20object%20attributes%20such%20as%20acceleration%20and%0Ayaw%20rate%20and%20generate%20trajectories%20based%20on%20them.%20A%20key%20contribution%20is%20the%0Aincorporation%20of%20expert%20knowledge%20into%20the%20learning%20objective%20of%20the%20deep%0Alearning%20model.%20This%20results%20in%20the%20constraint%20of%20the%20available%20action%20space%2C%0Athus%20enabling%20the%20prediction%20of%20physically%20feasible%20object%20attributes%20and%0Atrajectories%2C%20thereby%20increasing%20safety%20and%20robustness.%20The%20proposed%20hybrid%0Amodel%20facilitates%20enhanced%20interpretability%2C%20thereby%20reinforcing%20the%0Atrustworthiness%20of%20deep%20learning%20methods%20and%20promoting%20the%20development%20of%20safe%0Aplanning%20solutions.%20Experiments%20conducted%20on%20the%20publicly%20available%20real-world%0AArgoverse%20dataset%20demonstrate%20realistic%20driving%20behaviour%2C%20with%20benchmark%0Acomparisons%20and%20ablation%20studies%20showing%20promising%20results.%0A&entry.1838667208=http%3A//arxiv.org/abs/2501.03666v1&entry.124074799=Read"},
{"title": "Adjoint Matching: Fine-tuning Flow and Diffusion Generative Models with\n  Memoryless Stochastic Optimal Control", "author": "Carles Domingo-Enrich and Michal Drozdzal and Brian Karrer and Ricky T. Q. Chen", "abstract": "  Dynamical generative models that produce samples through an iterative\nprocess, such as Flow Matching and denoising diffusion models, have seen\nwidespread use, but there have not been many theoretically-sound methods for\nimproving these models with reward fine-tuning. In this work, we cast reward\nfine-tuning as stochastic optimal control (SOC). Critically, we prove that a\nvery specific memoryless noise schedule must be enforced during fine-tuning, in\norder to account for the dependency between the noise variable and the\ngenerated samples. We also propose a new algorithm named Adjoint Matching which\noutperforms existing SOC algorithms, by casting SOC problems as a regression\nproblem. We find that our approach significantly improves over existing methods\nfor reward fine-tuning, achieving better consistency, realism, and\ngeneralization to unseen human preference reward models, while retaining sample\ndiversity.\n", "link": "http://arxiv.org/abs/2409.08861v5", "date": "2025-01-07", "relevancy": 2.2231, "topK": [{"title": "Scaling Rectified Flow Transformers for High-Resolution Image Synthesis", "link": "http://arxiv.org/abs/2403.03206v1", "similarity": 0.5628}, {"title": "ToDo: Token Downsampling for Efficient Generation of High-Resolution\n  Images", "link": "http://arxiv.org/abs/2402.13573v2", "similarity": 0.5555}, {"title": "Tuning-Free Noise Rectification for High Fidelity Image-to-Video\n  Generation", "link": "http://arxiv.org/abs/2403.02827v1", "similarity": 0.5533}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Adjoint%20Matching%3A%20Fine-tuning%20Flow%20and%20Diffusion%20Generative%20Models%20with%0A%20%20Memoryless%20Stochastic%20Optimal%20Control&body=Title%3A%20Adjoint%20Matching%3A%20Fine-tuning%20Flow%20and%20Diffusion%20Generative%20Models%20with%0A%20%20Memoryless%20Stochastic%20Optimal%20Control%0AAuthor%3A%20Carles%20Domingo-Enrich%20and%20Michal%20Drozdzal%20and%20Brian%20Karrer%20and%20Ricky%20T.%20Q.%20Chen%0AAbstract%3A%20%20%20Dynamical%20generative%20models%20that%20produce%20samples%20through%20an%20iterative%0Aprocess%2C%20such%20as%20Flow%20Matching%20and%20denoising%20diffusion%20models%2C%20have%20seen%0Awidespread%20use%2C%20but%20there%20have%20not%20been%20many%20theoretically-sound%20methods%20for%0Aimproving%20these%20models%20with%20reward%20fine-tuning.%20In%20this%20work%2C%20we%20cast%20reward%0Afine-tuning%20as%20stochastic%20optimal%20control%20%28SOC%29.%20Critically%2C%20we%20prove%20that%20a%0Avery%20specific%20memoryless%20noise%20schedule%20must%20be%20enforced%20during%20fine-tuning%2C%20in%0Aorder%20to%20account%20for%20the%20dependency%20between%20the%20noise%20variable%20and%20the%0Agenerated%20samples.%20We%20also%20propose%20a%20new%20algorithm%20named%20Adjoint%20Matching%20which%0Aoutperforms%20existing%20SOC%20algorithms%2C%20by%20casting%20SOC%20problems%20as%20a%20regression%0Aproblem.%20We%20find%20that%20our%20approach%20significantly%20improves%20over%20existing%20methods%0Afor%20reward%20fine-tuning%2C%20achieving%20better%20consistency%2C%20realism%2C%20and%0Ageneralization%20to%20unseen%20human%20preference%20reward%20models%2C%20while%20retaining%20sample%0Adiversity.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2409.08861v5%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DAdjoint%2520Matching%253A%2520Fine-tuning%2520Flow%2520and%2520Diffusion%2520Generative%2520Models%2520with%250A%2520%2520Memoryless%2520Stochastic%2520Optimal%2520Control%26entry.906535625%3DCarles%2520Domingo-Enrich%2520and%2520Michal%2520Drozdzal%2520and%2520Brian%2520Karrer%2520and%2520Ricky%2520T.%2520Q.%2520Chen%26entry.1292438233%3D%2520%2520Dynamical%2520generative%2520models%2520that%2520produce%2520samples%2520through%2520an%2520iterative%250Aprocess%252C%2520such%2520as%2520Flow%2520Matching%2520and%2520denoising%2520diffusion%2520models%252C%2520have%2520seen%250Awidespread%2520use%252C%2520but%2520there%2520have%2520not%2520been%2520many%2520theoretically-sound%2520methods%2520for%250Aimproving%2520these%2520models%2520with%2520reward%2520fine-tuning.%2520In%2520this%2520work%252C%2520we%2520cast%2520reward%250Afine-tuning%2520as%2520stochastic%2520optimal%2520control%2520%2528SOC%2529.%2520Critically%252C%2520we%2520prove%2520that%2520a%250Avery%2520specific%2520memoryless%2520noise%2520schedule%2520must%2520be%2520enforced%2520during%2520fine-tuning%252C%2520in%250Aorder%2520to%2520account%2520for%2520the%2520dependency%2520between%2520the%2520noise%2520variable%2520and%2520the%250Agenerated%2520samples.%2520We%2520also%2520propose%2520a%2520new%2520algorithm%2520named%2520Adjoint%2520Matching%2520which%250Aoutperforms%2520existing%2520SOC%2520algorithms%252C%2520by%2520casting%2520SOC%2520problems%2520as%2520a%2520regression%250Aproblem.%2520We%2520find%2520that%2520our%2520approach%2520significantly%2520improves%2520over%2520existing%2520methods%250Afor%2520reward%2520fine-tuning%252C%2520achieving%2520better%2520consistency%252C%2520realism%252C%2520and%250Ageneralization%2520to%2520unseen%2520human%2520preference%2520reward%2520models%252C%2520while%2520retaining%2520sample%250Adiversity.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2409.08861v5%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Adjoint%20Matching%3A%20Fine-tuning%20Flow%20and%20Diffusion%20Generative%20Models%20with%0A%20%20Memoryless%20Stochastic%20Optimal%20Control&entry.906535625=Carles%20Domingo-Enrich%20and%20Michal%20Drozdzal%20and%20Brian%20Karrer%20and%20Ricky%20T.%20Q.%20Chen&entry.1292438233=%20%20Dynamical%20generative%20models%20that%20produce%20samples%20through%20an%20iterative%0Aprocess%2C%20such%20as%20Flow%20Matching%20and%20denoising%20diffusion%20models%2C%20have%20seen%0Awidespread%20use%2C%20but%20there%20have%20not%20been%20many%20theoretically-sound%20methods%20for%0Aimproving%20these%20models%20with%20reward%20fine-tuning.%20In%20this%20work%2C%20we%20cast%20reward%0Afine-tuning%20as%20stochastic%20optimal%20control%20%28SOC%29.%20Critically%2C%20we%20prove%20that%20a%0Avery%20specific%20memoryless%20noise%20schedule%20must%20be%20enforced%20during%20fine-tuning%2C%20in%0Aorder%20to%20account%20for%20the%20dependency%20between%20the%20noise%20variable%20and%20the%0Agenerated%20samples.%20We%20also%20propose%20a%20new%20algorithm%20named%20Adjoint%20Matching%20which%0Aoutperforms%20existing%20SOC%20algorithms%2C%20by%20casting%20SOC%20problems%20as%20a%20regression%0Aproblem.%20We%20find%20that%20our%20approach%20significantly%20improves%20over%20existing%20methods%0Afor%20reward%20fine-tuning%2C%20achieving%20better%20consistency%2C%20realism%2C%20and%0Ageneralization%20to%20unseen%20human%20preference%20reward%20models%2C%20while%20retaining%20sample%0Adiversity.%0A&entry.1838667208=http%3A//arxiv.org/abs/2409.08861v5&entry.124074799=Read"},
{"title": "Neuromorphic Optical Tracking and Imaging of Randomly Moving Targets\n  through Strongly Scattering Media", "author": "Ning Zhang and Timothy Shea and Arto Nurmikko", "abstract": "  Tracking and acquiring simultaneous optical images of randomly moving targets\nobscured by scattering media remains a challenging problem of importance to\nmany applications that require precise object localization and identification.\nIn this work we develop an end-to-end neuromorphic optical engineering and\ncomputational approach to demonstrate how to track and image normally invisible\nobjects by combining an event detecting camera with a multistage neuromorphic\ndeep learning strategy. Photons emerging from dense scattering media are\ndetected by the event camera and converted to pixel-wise asynchronized spike\ntrains - a first step in isolating object-specific information from the\ndominant uninformative background. Spiking data is fed into a deep spiking\nneural network (SNN) engine where object tracking and image reconstruction are\nperformed by two separate yet interconnected modules running in parallel in\ndiscrete time steps over the event duration. Through benchtop experiments we\ndemonstrate tracking and imaging randomly moving objects in dense turbid media\nas well as image reconstruction of spatially stationary but optically dynamic\nobjects. Standardized character sets serve as representative proxies for\ngeometrically complex objects, underscoring the method's generality. The\nresults highlight the advantages of a fully neuromorphic approach in meeting a\nmajor imaging technology with high computational efficiency and low power\nconsumption.\n", "link": "http://arxiv.org/abs/2501.03874v1", "date": "2025-01-07", "relevancy": 2.2228, "topK": [{"title": "RTG-SLAM: Real-time 3D Reconstruction at Scale using Gaussian Splatting", "link": "http://arxiv.org/abs/2404.19706v1", "similarity": 0.5665}, {"title": "SDXS: Real-Time One-Step Latent Diffusion Models with Image Conditions", "link": "http://arxiv.org/abs/2403.16627v1", "similarity": 0.551}, {"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.5468}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Neuromorphic%20Optical%20Tracking%20and%20Imaging%20of%20Randomly%20Moving%20Targets%0A%20%20through%20Strongly%20Scattering%20Media&body=Title%3A%20Neuromorphic%20Optical%20Tracking%20and%20Imaging%20of%20Randomly%20Moving%20Targets%0A%20%20through%20Strongly%20Scattering%20Media%0AAuthor%3A%20Ning%20Zhang%20and%20Timothy%20Shea%20and%20Arto%20Nurmikko%0AAbstract%3A%20%20%20Tracking%20and%20acquiring%20simultaneous%20optical%20images%20of%20randomly%20moving%20targets%0Aobscured%20by%20scattering%20media%20remains%20a%20challenging%20problem%20of%20importance%20to%0Amany%20applications%20that%20require%20precise%20object%20localization%20and%20identification.%0AIn%20this%20work%20we%20develop%20an%20end-to-end%20neuromorphic%20optical%20engineering%20and%0Acomputational%20approach%20to%20demonstrate%20how%20to%20track%20and%20image%20normally%20invisible%0Aobjects%20by%20combining%20an%20event%20detecting%20camera%20with%20a%20multistage%20neuromorphic%0Adeep%20learning%20strategy.%20Photons%20emerging%20from%20dense%20scattering%20media%20are%0Adetected%20by%20the%20event%20camera%20and%20converted%20to%20pixel-wise%20asynchronized%20spike%0Atrains%20-%20a%20first%20step%20in%20isolating%20object-specific%20information%20from%20the%0Adominant%20uninformative%20background.%20Spiking%20data%20is%20fed%20into%20a%20deep%20spiking%0Aneural%20network%20%28SNN%29%20engine%20where%20object%20tracking%20and%20image%20reconstruction%20are%0Aperformed%20by%20two%20separate%20yet%20interconnected%20modules%20running%20in%20parallel%20in%0Adiscrete%20time%20steps%20over%20the%20event%20duration.%20Through%20benchtop%20experiments%20we%0Ademonstrate%20tracking%20and%20imaging%20randomly%20moving%20objects%20in%20dense%20turbid%20media%0Aas%20well%20as%20image%20reconstruction%20of%20spatially%20stationary%20but%20optically%20dynamic%0Aobjects.%20Standardized%20character%20sets%20serve%20as%20representative%20proxies%20for%0Ageometrically%20complex%20objects%2C%20underscoring%20the%20method%27s%20generality.%20The%0Aresults%20highlight%20the%20advantages%20of%20a%20fully%20neuromorphic%20approach%20in%20meeting%20a%0Amajor%20imaging%20technology%20with%20high%20computational%20efficiency%20and%20low%20power%0Aconsumption.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2501.03874v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DNeuromorphic%2520Optical%2520Tracking%2520and%2520Imaging%2520of%2520Randomly%2520Moving%2520Targets%250A%2520%2520through%2520Strongly%2520Scattering%2520Media%26entry.906535625%3DNing%2520Zhang%2520and%2520Timothy%2520Shea%2520and%2520Arto%2520Nurmikko%26entry.1292438233%3D%2520%2520Tracking%2520and%2520acquiring%2520simultaneous%2520optical%2520images%2520of%2520randomly%2520moving%2520targets%250Aobscured%2520by%2520scattering%2520media%2520remains%2520a%2520challenging%2520problem%2520of%2520importance%2520to%250Amany%2520applications%2520that%2520require%2520precise%2520object%2520localization%2520and%2520identification.%250AIn%2520this%2520work%2520we%2520develop%2520an%2520end-to-end%2520neuromorphic%2520optical%2520engineering%2520and%250Acomputational%2520approach%2520to%2520demonstrate%2520how%2520to%2520track%2520and%2520image%2520normally%2520invisible%250Aobjects%2520by%2520combining%2520an%2520event%2520detecting%2520camera%2520with%2520a%2520multistage%2520neuromorphic%250Adeep%2520learning%2520strategy.%2520Photons%2520emerging%2520from%2520dense%2520scattering%2520media%2520are%250Adetected%2520by%2520the%2520event%2520camera%2520and%2520converted%2520to%2520pixel-wise%2520asynchronized%2520spike%250Atrains%2520-%2520a%2520first%2520step%2520in%2520isolating%2520object-specific%2520information%2520from%2520the%250Adominant%2520uninformative%2520background.%2520Spiking%2520data%2520is%2520fed%2520into%2520a%2520deep%2520spiking%250Aneural%2520network%2520%2528SNN%2529%2520engine%2520where%2520object%2520tracking%2520and%2520image%2520reconstruction%2520are%250Aperformed%2520by%2520two%2520separate%2520yet%2520interconnected%2520modules%2520running%2520in%2520parallel%2520in%250Adiscrete%2520time%2520steps%2520over%2520the%2520event%2520duration.%2520Through%2520benchtop%2520experiments%2520we%250Ademonstrate%2520tracking%2520and%2520imaging%2520randomly%2520moving%2520objects%2520in%2520dense%2520turbid%2520media%250Aas%2520well%2520as%2520image%2520reconstruction%2520of%2520spatially%2520stationary%2520but%2520optically%2520dynamic%250Aobjects.%2520Standardized%2520character%2520sets%2520serve%2520as%2520representative%2520proxies%2520for%250Ageometrically%2520complex%2520objects%252C%2520underscoring%2520the%2520method%2527s%2520generality.%2520The%250Aresults%2520highlight%2520the%2520advantages%2520of%2520a%2520fully%2520neuromorphic%2520approach%2520in%2520meeting%2520a%250Amajor%2520imaging%2520technology%2520with%2520high%2520computational%2520efficiency%2520and%2520low%2520power%250Aconsumption.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2501.03874v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Neuromorphic%20Optical%20Tracking%20and%20Imaging%20of%20Randomly%20Moving%20Targets%0A%20%20through%20Strongly%20Scattering%20Media&entry.906535625=Ning%20Zhang%20and%20Timothy%20Shea%20and%20Arto%20Nurmikko&entry.1292438233=%20%20Tracking%20and%20acquiring%20simultaneous%20optical%20images%20of%20randomly%20moving%20targets%0Aobscured%20by%20scattering%20media%20remains%20a%20challenging%20problem%20of%20importance%20to%0Amany%20applications%20that%20require%20precise%20object%20localization%20and%20identification.%0AIn%20this%20work%20we%20develop%20an%20end-to-end%20neuromorphic%20optical%20engineering%20and%0Acomputational%20approach%20to%20demonstrate%20how%20to%20track%20and%20image%20normally%20invisible%0Aobjects%20by%20combining%20an%20event%20detecting%20camera%20with%20a%20multistage%20neuromorphic%0Adeep%20learning%20strategy.%20Photons%20emerging%20from%20dense%20scattering%20media%20are%0Adetected%20by%20the%20event%20camera%20and%20converted%20to%20pixel-wise%20asynchronized%20spike%0Atrains%20-%20a%20first%20step%20in%20isolating%20object-specific%20information%20from%20the%0Adominant%20uninformative%20background.%20Spiking%20data%20is%20fed%20into%20a%20deep%20spiking%0Aneural%20network%20%28SNN%29%20engine%20where%20object%20tracking%20and%20image%20reconstruction%20are%0Aperformed%20by%20two%20separate%20yet%20interconnected%20modules%20running%20in%20parallel%20in%0Adiscrete%20time%20steps%20over%20the%20event%20duration.%20Through%20benchtop%20experiments%20we%0Ademonstrate%20tracking%20and%20imaging%20randomly%20moving%20objects%20in%20dense%20turbid%20media%0Aas%20well%20as%20image%20reconstruction%20of%20spatially%20stationary%20but%20optically%20dynamic%0Aobjects.%20Standardized%20character%20sets%20serve%20as%20representative%20proxies%20for%0Ageometrically%20complex%20objects%2C%20underscoring%20the%20method%27s%20generality.%20The%0Aresults%20highlight%20the%20advantages%20of%20a%20fully%20neuromorphic%20approach%20in%20meeting%20a%0Amajor%20imaging%20technology%20with%20high%20computational%20efficiency%20and%20low%20power%0Aconsumption.%0A&entry.1838667208=http%3A//arxiv.org/abs/2501.03874v1&entry.124074799=Read"},
{"title": "SELMA3D challenge: Self-supervised learning for 3D light-sheet\n  microscopy image segmentation", "author": "Ying Chen and Rami Al-Maskari and Izabela Horvath and Mayar Ali and Luciano H\u00f6her and Kaiyuan Yang and Zengming Lin and Zhiwei Zhai and Mengzhe Shen and Dejin Xun and Yi Wang and Tony Xu and Maged Goubran and Yunheng Wu and Ali Erturk and Johannes C. Paetzold", "abstract": "  Recent innovations in light sheet microscopy, paired with developments in\ntissue clearing techniques, enable the 3D imaging of large mammalian tissues\nwith cellular resolution. Combined with the progress in large-scale data\nanalysis, driven by deep learning, these innovations empower researchers to\nrapidly investigate the morphological and functional properties of diverse\nbiological samples. Segmentation, a crucial preliminary step in the analysis\nprocess, can be automated using domain-specific deep learning models with\nexpert-level performance. However, these models exhibit high sensitivity to\ndomain shifts, leading to a significant drop in accuracy when applied to data\noutside their training distribution. To address this limitation, and inspired\nby the recent success of self-supervised learning in training generalizable\nmodels, we organized the SELMA3D Challenge during the MICCAI 2024 conference.\nSELMA3D provides a vast collection of light-sheet images from cleared mice and\nhuman brains, comprising 35 large 3D images-each with over 1000^3 voxels-and\n315 annotated small patches for finetuning, preliminary testing and final\ntesting. The dataset encompasses diverse biological structures, including\nvessel-like and spot-like structures. Five teams participated in all phases of\nthe challenge, and their proposed methods are reviewed in this paper.\nQuantitative and qualitative results from most participating teams demonstrate\nthat self-supervised learning on large datasets improves segmentation model\nperformance and generalization. We will continue to support and extend SELMA3D\nas an inaugural MICCAI challenge focused on self-supervised learning for 3D\nmicroscopy image segmentation.\n", "link": "http://arxiv.org/abs/2501.03880v1", "date": "2025-01-07", "relevancy": 2.22, "topK": [{"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5663}, {"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.5571}, {"title": "Self-supervised Photographic Image Layout Representation Learning", "link": "http://arxiv.org/abs/2403.03740v1", "similarity": 0.5428}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20SELMA3D%20challenge%3A%20Self-supervised%20learning%20for%203D%20light-sheet%0A%20%20microscopy%20image%20segmentation&body=Title%3A%20SELMA3D%20challenge%3A%20Self-supervised%20learning%20for%203D%20light-sheet%0A%20%20microscopy%20image%20segmentation%0AAuthor%3A%20Ying%20Chen%20and%20Rami%20Al-Maskari%20and%20Izabela%20Horvath%20and%20Mayar%20Ali%20and%20Luciano%20H%C3%B6her%20and%20Kaiyuan%20Yang%20and%20Zengming%20Lin%20and%20Zhiwei%20Zhai%20and%20Mengzhe%20Shen%20and%20Dejin%20Xun%20and%20Yi%20Wang%20and%20Tony%20Xu%20and%20Maged%20Goubran%20and%20Yunheng%20Wu%20and%20Ali%20Erturk%20and%20Johannes%20C.%20Paetzold%0AAbstract%3A%20%20%20Recent%20innovations%20in%20light%20sheet%20microscopy%2C%20paired%20with%20developments%20in%0Atissue%20clearing%20techniques%2C%20enable%20the%203D%20imaging%20of%20large%20mammalian%20tissues%0Awith%20cellular%20resolution.%20Combined%20with%20the%20progress%20in%20large-scale%20data%0Aanalysis%2C%20driven%20by%20deep%20learning%2C%20these%20innovations%20empower%20researchers%20to%0Arapidly%20investigate%20the%20morphological%20and%20functional%20properties%20of%20diverse%0Abiological%20samples.%20Segmentation%2C%20a%20crucial%20preliminary%20step%20in%20the%20analysis%0Aprocess%2C%20can%20be%20automated%20using%20domain-specific%20deep%20learning%20models%20with%0Aexpert-level%20performance.%20However%2C%20these%20models%20exhibit%20high%20sensitivity%20to%0Adomain%20shifts%2C%20leading%20to%20a%20significant%20drop%20in%20accuracy%20when%20applied%20to%20data%0Aoutside%20their%20training%20distribution.%20To%20address%20this%20limitation%2C%20and%20inspired%0Aby%20the%20recent%20success%20of%20self-supervised%20learning%20in%20training%20generalizable%0Amodels%2C%20we%20organized%20the%20SELMA3D%20Challenge%20during%20the%20MICCAI%202024%20conference.%0ASELMA3D%20provides%20a%20vast%20collection%20of%20light-sheet%20images%20from%20cleared%20mice%20and%0Ahuman%20brains%2C%20comprising%2035%20large%203D%20images-each%20with%20over%201000%5E3%20voxels-and%0A315%20annotated%20small%20patches%20for%20finetuning%2C%20preliminary%20testing%20and%20final%0Atesting.%20The%20dataset%20encompasses%20diverse%20biological%20structures%2C%20including%0Avessel-like%20and%20spot-like%20structures.%20Five%20teams%20participated%20in%20all%20phases%20of%0Athe%20challenge%2C%20and%20their%20proposed%20methods%20are%20reviewed%20in%20this%20paper.%0AQuantitative%20and%20qualitative%20results%20from%20most%20participating%20teams%20demonstrate%0Athat%20self-supervised%20learning%20on%20large%20datasets%20improves%20segmentation%20model%0Aperformance%20and%20generalization.%20We%20will%20continue%20to%20support%20and%20extend%20SELMA3D%0Aas%20an%20inaugural%20MICCAI%20challenge%20focused%20on%20self-supervised%20learning%20for%203D%0Amicroscopy%20image%20segmentation.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2501.03880v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DSELMA3D%2520challenge%253A%2520Self-supervised%2520learning%2520for%25203D%2520light-sheet%250A%2520%2520microscopy%2520image%2520segmentation%26entry.906535625%3DYing%2520Chen%2520and%2520Rami%2520Al-Maskari%2520and%2520Izabela%2520Horvath%2520and%2520Mayar%2520Ali%2520and%2520Luciano%2520H%25C3%25B6her%2520and%2520Kaiyuan%2520Yang%2520and%2520Zengming%2520Lin%2520and%2520Zhiwei%2520Zhai%2520and%2520Mengzhe%2520Shen%2520and%2520Dejin%2520Xun%2520and%2520Yi%2520Wang%2520and%2520Tony%2520Xu%2520and%2520Maged%2520Goubran%2520and%2520Yunheng%2520Wu%2520and%2520Ali%2520Erturk%2520and%2520Johannes%2520C.%2520Paetzold%26entry.1292438233%3D%2520%2520Recent%2520innovations%2520in%2520light%2520sheet%2520microscopy%252C%2520paired%2520with%2520developments%2520in%250Atissue%2520clearing%2520techniques%252C%2520enable%2520the%25203D%2520imaging%2520of%2520large%2520mammalian%2520tissues%250Awith%2520cellular%2520resolution.%2520Combined%2520with%2520the%2520progress%2520in%2520large-scale%2520data%250Aanalysis%252C%2520driven%2520by%2520deep%2520learning%252C%2520these%2520innovations%2520empower%2520researchers%2520to%250Arapidly%2520investigate%2520the%2520morphological%2520and%2520functional%2520properties%2520of%2520diverse%250Abiological%2520samples.%2520Segmentation%252C%2520a%2520crucial%2520preliminary%2520step%2520in%2520the%2520analysis%250Aprocess%252C%2520can%2520be%2520automated%2520using%2520domain-specific%2520deep%2520learning%2520models%2520with%250Aexpert-level%2520performance.%2520However%252C%2520these%2520models%2520exhibit%2520high%2520sensitivity%2520to%250Adomain%2520shifts%252C%2520leading%2520to%2520a%2520significant%2520drop%2520in%2520accuracy%2520when%2520applied%2520to%2520data%250Aoutside%2520their%2520training%2520distribution.%2520To%2520address%2520this%2520limitation%252C%2520and%2520inspired%250Aby%2520the%2520recent%2520success%2520of%2520self-supervised%2520learning%2520in%2520training%2520generalizable%250Amodels%252C%2520we%2520organized%2520the%2520SELMA3D%2520Challenge%2520during%2520the%2520MICCAI%25202024%2520conference.%250ASELMA3D%2520provides%2520a%2520vast%2520collection%2520of%2520light-sheet%2520images%2520from%2520cleared%2520mice%2520and%250Ahuman%2520brains%252C%2520comprising%252035%2520large%25203D%2520images-each%2520with%2520over%25201000%255E3%2520voxels-and%250A315%2520annotated%2520small%2520patches%2520for%2520finetuning%252C%2520preliminary%2520testing%2520and%2520final%250Atesting.%2520The%2520dataset%2520encompasses%2520diverse%2520biological%2520structures%252C%2520including%250Avessel-like%2520and%2520spot-like%2520structures.%2520Five%2520teams%2520participated%2520in%2520all%2520phases%2520of%250Athe%2520challenge%252C%2520and%2520their%2520proposed%2520methods%2520are%2520reviewed%2520in%2520this%2520paper.%250AQuantitative%2520and%2520qualitative%2520results%2520from%2520most%2520participating%2520teams%2520demonstrate%250Athat%2520self-supervised%2520learning%2520on%2520large%2520datasets%2520improves%2520segmentation%2520model%250Aperformance%2520and%2520generalization.%2520We%2520will%2520continue%2520to%2520support%2520and%2520extend%2520SELMA3D%250Aas%2520an%2520inaugural%2520MICCAI%2520challenge%2520focused%2520on%2520self-supervised%2520learning%2520for%25203D%250Amicroscopy%2520image%2520segmentation.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2501.03880v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=SELMA3D%20challenge%3A%20Self-supervised%20learning%20for%203D%20light-sheet%0A%20%20microscopy%20image%20segmentation&entry.906535625=Ying%20Chen%20and%20Rami%20Al-Maskari%20and%20Izabela%20Horvath%20and%20Mayar%20Ali%20and%20Luciano%20H%C3%B6her%20and%20Kaiyuan%20Yang%20and%20Zengming%20Lin%20and%20Zhiwei%20Zhai%20and%20Mengzhe%20Shen%20and%20Dejin%20Xun%20and%20Yi%20Wang%20and%20Tony%20Xu%20and%20Maged%20Goubran%20and%20Yunheng%20Wu%20and%20Ali%20Erturk%20and%20Johannes%20C.%20Paetzold&entry.1292438233=%20%20Recent%20innovations%20in%20light%20sheet%20microscopy%2C%20paired%20with%20developments%20in%0Atissue%20clearing%20techniques%2C%20enable%20the%203D%20imaging%20of%20large%20mammalian%20tissues%0Awith%20cellular%20resolution.%20Combined%20with%20the%20progress%20in%20large-scale%20data%0Aanalysis%2C%20driven%20by%20deep%20learning%2C%20these%20innovations%20empower%20researchers%20to%0Arapidly%20investigate%20the%20morphological%20and%20functional%20properties%20of%20diverse%0Abiological%20samples.%20Segmentation%2C%20a%20crucial%20preliminary%20step%20in%20the%20analysis%0Aprocess%2C%20can%20be%20automated%20using%20domain-specific%20deep%20learning%20models%20with%0Aexpert-level%20performance.%20However%2C%20these%20models%20exhibit%20high%20sensitivity%20to%0Adomain%20shifts%2C%20leading%20to%20a%20significant%20drop%20in%20accuracy%20when%20applied%20to%20data%0Aoutside%20their%20training%20distribution.%20To%20address%20this%20limitation%2C%20and%20inspired%0Aby%20the%20recent%20success%20of%20self-supervised%20learning%20in%20training%20generalizable%0Amodels%2C%20we%20organized%20the%20SELMA3D%20Challenge%20during%20the%20MICCAI%202024%20conference.%0ASELMA3D%20provides%20a%20vast%20collection%20of%20light-sheet%20images%20from%20cleared%20mice%20and%0Ahuman%20brains%2C%20comprising%2035%20large%203D%20images-each%20with%20over%201000%5E3%20voxels-and%0A315%20annotated%20small%20patches%20for%20finetuning%2C%20preliminary%20testing%20and%20final%0Atesting.%20The%20dataset%20encompasses%20diverse%20biological%20structures%2C%20including%0Avessel-like%20and%20spot-like%20structures.%20Five%20teams%20participated%20in%20all%20phases%20of%0Athe%20challenge%2C%20and%20their%20proposed%20methods%20are%20reviewed%20in%20this%20paper.%0AQuantitative%20and%20qualitative%20results%20from%20most%20participating%20teams%20demonstrate%0Athat%20self-supervised%20learning%20on%20large%20datasets%20improves%20segmentation%20model%0Aperformance%20and%20generalization.%20We%20will%20continue%20to%20support%20and%20extend%20SELMA3D%0Aas%20an%20inaugural%20MICCAI%20challenge%20focused%20on%20self-supervised%20learning%20for%203D%0Amicroscopy%20image%20segmentation.%0A&entry.1838667208=http%3A//arxiv.org/abs/2501.03880v1&entry.124074799=Read"},
{"title": "Realistic Test-Time Adaptation of Vision-Language Models", "author": "Maxime Zanella and Cl\u00e9ment Fuchs and Christophe De Vleeschouwer and Ismail Ben Ayed", "abstract": "  The zero-shot capabilities of Vision-Language Models (VLMs) have been widely\nleveraged to improve predictive performance. However, previous works on\ntransductive or test-time adaptation (TTA) often make strong assumptions about\nthe data distribution, such as the presence of all classes. Our work challenges\nthese favorable deployment scenarios, and introduces a more realistic\nevaluation framework, including: (i) a variable number of effective classes for\nadaptation within a single batch, and (ii) non-i.i.d. batches of test samples\nin online adaptation settings. We provide comprehensive evaluations,\ncomparisons, and ablation studies that demonstrate how current transductive or\nTTA methods for VLMs systematically compromise the models' initial zero-shot\nrobustness across various realistic scenarios, favoring performance gains under\nadvantageous assumptions about the test samples' distributions. Furthermore, we\nintroduce StatA, a versatile method that could handle a wide range of\ndeployment scenarios, including those with a variable number of effective\nclasses at test time. Our approach incorporates a novel regularization term\ndesigned specifically for VLMs, which acts as a statistical anchor preserving\nthe initial text-encoder knowledge, particularly in low-data regimes. Code\navailable at https://github.com/MaxZanella/StatA.\n", "link": "http://arxiv.org/abs/2501.03729v1", "date": "2025-01-07", "relevancy": 2.2194, "topK": [{"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5863}, {"title": "Customize-A-Video: One-Shot Motion Customization of Text-to-Video\n  Diffusion Models", "link": "http://arxiv.org/abs/2402.14780v1", "similarity": 0.5709}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.517}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Realistic%20Test-Time%20Adaptation%20of%20Vision-Language%20Models&body=Title%3A%20Realistic%20Test-Time%20Adaptation%20of%20Vision-Language%20Models%0AAuthor%3A%20Maxime%20Zanella%20and%20Cl%C3%A9ment%20Fuchs%20and%20Christophe%20De%20Vleeschouwer%20and%20Ismail%20Ben%20Ayed%0AAbstract%3A%20%20%20The%20zero-shot%20capabilities%20of%20Vision-Language%20Models%20%28VLMs%29%20have%20been%20widely%0Aleveraged%20to%20improve%20predictive%20performance.%20However%2C%20previous%20works%20on%0Atransductive%20or%20test-time%20adaptation%20%28TTA%29%20often%20make%20strong%20assumptions%20about%0Athe%20data%20distribution%2C%20such%20as%20the%20presence%20of%20all%20classes.%20Our%20work%20challenges%0Athese%20favorable%20deployment%20scenarios%2C%20and%20introduces%20a%20more%20realistic%0Aevaluation%20framework%2C%20including%3A%20%28i%29%20a%20variable%20number%20of%20effective%20classes%20for%0Aadaptation%20within%20a%20single%20batch%2C%20and%20%28ii%29%20non-i.i.d.%20batches%20of%20test%20samples%0Ain%20online%20adaptation%20settings.%20We%20provide%20comprehensive%20evaluations%2C%0Acomparisons%2C%20and%20ablation%20studies%20that%20demonstrate%20how%20current%20transductive%20or%0ATTA%20methods%20for%20VLMs%20systematically%20compromise%20the%20models%27%20initial%20zero-shot%0Arobustness%20across%20various%20realistic%20scenarios%2C%20favoring%20performance%20gains%20under%0Aadvantageous%20assumptions%20about%20the%20test%20samples%27%20distributions.%20Furthermore%2C%20we%0Aintroduce%20StatA%2C%20a%20versatile%20method%20that%20could%20handle%20a%20wide%20range%20of%0Adeployment%20scenarios%2C%20including%20those%20with%20a%20variable%20number%20of%20effective%0Aclasses%20at%20test%20time.%20Our%20approach%20incorporates%20a%20novel%20regularization%20term%0Adesigned%20specifically%20for%20VLMs%2C%20which%20acts%20as%20a%20statistical%20anchor%20preserving%0Athe%20initial%20text-encoder%20knowledge%2C%20particularly%20in%20low-data%20regimes.%20Code%0Aavailable%20at%20https%3A//github.com/MaxZanella/StatA.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2501.03729v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DRealistic%2520Test-Time%2520Adaptation%2520of%2520Vision-Language%2520Models%26entry.906535625%3DMaxime%2520Zanella%2520and%2520Cl%25C3%25A9ment%2520Fuchs%2520and%2520Christophe%2520De%2520Vleeschouwer%2520and%2520Ismail%2520Ben%2520Ayed%26entry.1292438233%3D%2520%2520The%2520zero-shot%2520capabilities%2520of%2520Vision-Language%2520Models%2520%2528VLMs%2529%2520have%2520been%2520widely%250Aleveraged%2520to%2520improve%2520predictive%2520performance.%2520However%252C%2520previous%2520works%2520on%250Atransductive%2520or%2520test-time%2520adaptation%2520%2528TTA%2529%2520often%2520make%2520strong%2520assumptions%2520about%250Athe%2520data%2520distribution%252C%2520such%2520as%2520the%2520presence%2520of%2520all%2520classes.%2520Our%2520work%2520challenges%250Athese%2520favorable%2520deployment%2520scenarios%252C%2520and%2520introduces%2520a%2520more%2520realistic%250Aevaluation%2520framework%252C%2520including%253A%2520%2528i%2529%2520a%2520variable%2520number%2520of%2520effective%2520classes%2520for%250Aadaptation%2520within%2520a%2520single%2520batch%252C%2520and%2520%2528ii%2529%2520non-i.i.d.%2520batches%2520of%2520test%2520samples%250Ain%2520online%2520adaptation%2520settings.%2520We%2520provide%2520comprehensive%2520evaluations%252C%250Acomparisons%252C%2520and%2520ablation%2520studies%2520that%2520demonstrate%2520how%2520current%2520transductive%2520or%250ATTA%2520methods%2520for%2520VLMs%2520systematically%2520compromise%2520the%2520models%2527%2520initial%2520zero-shot%250Arobustness%2520across%2520various%2520realistic%2520scenarios%252C%2520favoring%2520performance%2520gains%2520under%250Aadvantageous%2520assumptions%2520about%2520the%2520test%2520samples%2527%2520distributions.%2520Furthermore%252C%2520we%250Aintroduce%2520StatA%252C%2520a%2520versatile%2520method%2520that%2520could%2520handle%2520a%2520wide%2520range%2520of%250Adeployment%2520scenarios%252C%2520including%2520those%2520with%2520a%2520variable%2520number%2520of%2520effective%250Aclasses%2520at%2520test%2520time.%2520Our%2520approach%2520incorporates%2520a%2520novel%2520regularization%2520term%250Adesigned%2520specifically%2520for%2520VLMs%252C%2520which%2520acts%2520as%2520a%2520statistical%2520anchor%2520preserving%250Athe%2520initial%2520text-encoder%2520knowledge%252C%2520particularly%2520in%2520low-data%2520regimes.%2520Code%250Aavailable%2520at%2520https%253A//github.com/MaxZanella/StatA.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2501.03729v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Realistic%20Test-Time%20Adaptation%20of%20Vision-Language%20Models&entry.906535625=Maxime%20Zanella%20and%20Cl%C3%A9ment%20Fuchs%20and%20Christophe%20De%20Vleeschouwer%20and%20Ismail%20Ben%20Ayed&entry.1292438233=%20%20The%20zero-shot%20capabilities%20of%20Vision-Language%20Models%20%28VLMs%29%20have%20been%20widely%0Aleveraged%20to%20improve%20predictive%20performance.%20However%2C%20previous%20works%20on%0Atransductive%20or%20test-time%20adaptation%20%28TTA%29%20often%20make%20strong%20assumptions%20about%0Athe%20data%20distribution%2C%20such%20as%20the%20presence%20of%20all%20classes.%20Our%20work%20challenges%0Athese%20favorable%20deployment%20scenarios%2C%20and%20introduces%20a%20more%20realistic%0Aevaluation%20framework%2C%20including%3A%20%28i%29%20a%20variable%20number%20of%20effective%20classes%20for%0Aadaptation%20within%20a%20single%20batch%2C%20and%20%28ii%29%20non-i.i.d.%20batches%20of%20test%20samples%0Ain%20online%20adaptation%20settings.%20We%20provide%20comprehensive%20evaluations%2C%0Acomparisons%2C%20and%20ablation%20studies%20that%20demonstrate%20how%20current%20transductive%20or%0ATTA%20methods%20for%20VLMs%20systematically%20compromise%20the%20models%27%20initial%20zero-shot%0Arobustness%20across%20various%20realistic%20scenarios%2C%20favoring%20performance%20gains%20under%0Aadvantageous%20assumptions%20about%20the%20test%20samples%27%20distributions.%20Furthermore%2C%20we%0Aintroduce%20StatA%2C%20a%20versatile%20method%20that%20could%20handle%20a%20wide%20range%20of%0Adeployment%20scenarios%2C%20including%20those%20with%20a%20variable%20number%20of%20effective%0Aclasses%20at%20test%20time.%20Our%20approach%20incorporates%20a%20novel%20regularization%20term%0Adesigned%20specifically%20for%20VLMs%2C%20which%20acts%20as%20a%20statistical%20anchor%20preserving%0Athe%20initial%20text-encoder%20knowledge%2C%20particularly%20in%20low-data%20regimes.%20Code%0Aavailable%20at%20https%3A//github.com/MaxZanella/StatA.%0A&entry.1838667208=http%3A//arxiv.org/abs/2501.03729v1&entry.124074799=Read"},
{"title": "LLaVA-Mini: Efficient Image and Video Large Multimodal Models with One\n  Vision Token", "author": "Shaolei Zhang and Qingkai Fang and Zhe Yang and Yang Feng", "abstract": "  The advent of real-time large multimodal models (LMMs) like GPT-4o has\nsparked considerable interest in efficient LMMs. LMM frameworks typically\nencode visual inputs into vision tokens (continuous representations) and\nintegrate them and textual instructions into the context of large language\nmodels (LLMs), where large-scale parameters and numerous context tokens\n(predominantly vision tokens) result in substantial computational overhead.\nPrevious efforts towards efficient LMMs always focus on replacing the LLM\nbackbone with smaller models, while neglecting the crucial issue of token\nquantity. In this paper, we introduce LLaVA-Mini, an efficient LMM with minimal\nvision tokens. To achieve a high compression ratio of vision tokens while\npreserving visual information, we first analyze how LMMs understand vision\ntokens and find that most vision tokens only play a crucial role in the early\nlayers of LLM backbone, where they mainly fuse visual information into text\ntokens. Building on this finding, LLaVA-Mini introduces modality pre-fusion to\nfuse visual information into text tokens in advance, thereby facilitating the\nextreme compression of vision tokens fed to LLM backbone into one token.\nLLaVA-Mini is a unified large multimodal model that can support the\nunderstanding of images, high-resolution images, and videos in an efficient\nmanner. Experiments across 11 image-based and 7 video-based benchmarks\ndemonstrate that LLaVA-Mini outperforms LLaVA-v1.5 with just 1 vision token\ninstead of 576. Efficiency analyses reveal that LLaVA-Mini can reduce FLOPs by\n77%, deliver low-latency responses within 40 milliseconds, and process over\n10,000 frames of video on the GPU hardware with 24GB of memory.\n", "link": "http://arxiv.org/abs/2501.03895v1", "date": "2025-01-07", "relevancy": 2.2153, "topK": [{"title": "SDXS: Real-Time One-Step Latent Diffusion Models with Image Conditions", "link": "http://arxiv.org/abs/2403.16627v1", "similarity": 0.563}, {"title": "ToDo: Token Downsampling for Efficient Generation of High-Resolution\n  Images", "link": "http://arxiv.org/abs/2402.13573v2", "similarity": 0.5591}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5449}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20LLaVA-Mini%3A%20Efficient%20Image%20and%20Video%20Large%20Multimodal%20Models%20with%20One%0A%20%20Vision%20Token&body=Title%3A%20LLaVA-Mini%3A%20Efficient%20Image%20and%20Video%20Large%20Multimodal%20Models%20with%20One%0A%20%20Vision%20Token%0AAuthor%3A%20Shaolei%20Zhang%20and%20Qingkai%20Fang%20and%20Zhe%20Yang%20and%20Yang%20Feng%0AAbstract%3A%20%20%20The%20advent%20of%20real-time%20large%20multimodal%20models%20%28LMMs%29%20like%20GPT-4o%20has%0Asparked%20considerable%20interest%20in%20efficient%20LMMs.%20LMM%20frameworks%20typically%0Aencode%20visual%20inputs%20into%20vision%20tokens%20%28continuous%20representations%29%20and%0Aintegrate%20them%20and%20textual%20instructions%20into%20the%20context%20of%20large%20language%0Amodels%20%28LLMs%29%2C%20where%20large-scale%20parameters%20and%20numerous%20context%20tokens%0A%28predominantly%20vision%20tokens%29%20result%20in%20substantial%20computational%20overhead.%0APrevious%20efforts%20towards%20efficient%20LMMs%20always%20focus%20on%20replacing%20the%20LLM%0Abackbone%20with%20smaller%20models%2C%20while%20neglecting%20the%20crucial%20issue%20of%20token%0Aquantity.%20In%20this%20paper%2C%20we%20introduce%20LLaVA-Mini%2C%20an%20efficient%20LMM%20with%20minimal%0Avision%20tokens.%20To%20achieve%20a%20high%20compression%20ratio%20of%20vision%20tokens%20while%0Apreserving%20visual%20information%2C%20we%20first%20analyze%20how%20LMMs%20understand%20vision%0Atokens%20and%20find%20that%20most%20vision%20tokens%20only%20play%20a%20crucial%20role%20in%20the%20early%0Alayers%20of%20LLM%20backbone%2C%20where%20they%20mainly%20fuse%20visual%20information%20into%20text%0Atokens.%20Building%20on%20this%20finding%2C%20LLaVA-Mini%20introduces%20modality%20pre-fusion%20to%0Afuse%20visual%20information%20into%20text%20tokens%20in%20advance%2C%20thereby%20facilitating%20the%0Aextreme%20compression%20of%20vision%20tokens%20fed%20to%20LLM%20backbone%20into%20one%20token.%0ALLaVA-Mini%20is%20a%20unified%20large%20multimodal%20model%20that%20can%20support%20the%0Aunderstanding%20of%20images%2C%20high-resolution%20images%2C%20and%20videos%20in%20an%20efficient%0Amanner.%20Experiments%20across%2011%20image-based%20and%207%20video-based%20benchmarks%0Ademonstrate%20that%20LLaVA-Mini%20outperforms%20LLaVA-v1.5%20with%20just%201%20vision%20token%0Ainstead%20of%20576.%20Efficiency%20analyses%20reveal%20that%20LLaVA-Mini%20can%20reduce%20FLOPs%20by%0A77%25%2C%20deliver%20low-latency%20responses%20within%2040%20milliseconds%2C%20and%20process%20over%0A10%2C000%20frames%20of%20video%20on%20the%20GPU%20hardware%20with%2024GB%20of%20memory.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2501.03895v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DLLaVA-Mini%253A%2520Efficient%2520Image%2520and%2520Video%2520Large%2520Multimodal%2520Models%2520with%2520One%250A%2520%2520Vision%2520Token%26entry.906535625%3DShaolei%2520Zhang%2520and%2520Qingkai%2520Fang%2520and%2520Zhe%2520Yang%2520and%2520Yang%2520Feng%26entry.1292438233%3D%2520%2520The%2520advent%2520of%2520real-time%2520large%2520multimodal%2520models%2520%2528LMMs%2529%2520like%2520GPT-4o%2520has%250Asparked%2520considerable%2520interest%2520in%2520efficient%2520LMMs.%2520LMM%2520frameworks%2520typically%250Aencode%2520visual%2520inputs%2520into%2520vision%2520tokens%2520%2528continuous%2520representations%2529%2520and%250Aintegrate%2520them%2520and%2520textual%2520instructions%2520into%2520the%2520context%2520of%2520large%2520language%250Amodels%2520%2528LLMs%2529%252C%2520where%2520large-scale%2520parameters%2520and%2520numerous%2520context%2520tokens%250A%2528predominantly%2520vision%2520tokens%2529%2520result%2520in%2520substantial%2520computational%2520overhead.%250APrevious%2520efforts%2520towards%2520efficient%2520LMMs%2520always%2520focus%2520on%2520replacing%2520the%2520LLM%250Abackbone%2520with%2520smaller%2520models%252C%2520while%2520neglecting%2520the%2520crucial%2520issue%2520of%2520token%250Aquantity.%2520In%2520this%2520paper%252C%2520we%2520introduce%2520LLaVA-Mini%252C%2520an%2520efficient%2520LMM%2520with%2520minimal%250Avision%2520tokens.%2520To%2520achieve%2520a%2520high%2520compression%2520ratio%2520of%2520vision%2520tokens%2520while%250Apreserving%2520visual%2520information%252C%2520we%2520first%2520analyze%2520how%2520LMMs%2520understand%2520vision%250Atokens%2520and%2520find%2520that%2520most%2520vision%2520tokens%2520only%2520play%2520a%2520crucial%2520role%2520in%2520the%2520early%250Alayers%2520of%2520LLM%2520backbone%252C%2520where%2520they%2520mainly%2520fuse%2520visual%2520information%2520into%2520text%250Atokens.%2520Building%2520on%2520this%2520finding%252C%2520LLaVA-Mini%2520introduces%2520modality%2520pre-fusion%2520to%250Afuse%2520visual%2520information%2520into%2520text%2520tokens%2520in%2520advance%252C%2520thereby%2520facilitating%2520the%250Aextreme%2520compression%2520of%2520vision%2520tokens%2520fed%2520to%2520LLM%2520backbone%2520into%2520one%2520token.%250ALLaVA-Mini%2520is%2520a%2520unified%2520large%2520multimodal%2520model%2520that%2520can%2520support%2520the%250Aunderstanding%2520of%2520images%252C%2520high-resolution%2520images%252C%2520and%2520videos%2520in%2520an%2520efficient%250Amanner.%2520Experiments%2520across%252011%2520image-based%2520and%25207%2520video-based%2520benchmarks%250Ademonstrate%2520that%2520LLaVA-Mini%2520outperforms%2520LLaVA-v1.5%2520with%2520just%25201%2520vision%2520token%250Ainstead%2520of%2520576.%2520Efficiency%2520analyses%2520reveal%2520that%2520LLaVA-Mini%2520can%2520reduce%2520FLOPs%2520by%250A77%2525%252C%2520deliver%2520low-latency%2520responses%2520within%252040%2520milliseconds%252C%2520and%2520process%2520over%250A10%252C000%2520frames%2520of%2520video%2520on%2520the%2520GPU%2520hardware%2520with%252024GB%2520of%2520memory.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2501.03895v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=LLaVA-Mini%3A%20Efficient%20Image%20and%20Video%20Large%20Multimodal%20Models%20with%20One%0A%20%20Vision%20Token&entry.906535625=Shaolei%20Zhang%20and%20Qingkai%20Fang%20and%20Zhe%20Yang%20and%20Yang%20Feng&entry.1292438233=%20%20The%20advent%20of%20real-time%20large%20multimodal%20models%20%28LMMs%29%20like%20GPT-4o%20has%0Asparked%20considerable%20interest%20in%20efficient%20LMMs.%20LMM%20frameworks%20typically%0Aencode%20visual%20inputs%20into%20vision%20tokens%20%28continuous%20representations%29%20and%0Aintegrate%20them%20and%20textual%20instructions%20into%20the%20context%20of%20large%20language%0Amodels%20%28LLMs%29%2C%20where%20large-scale%20parameters%20and%20numerous%20context%20tokens%0A%28predominantly%20vision%20tokens%29%20result%20in%20substantial%20computational%20overhead.%0APrevious%20efforts%20towards%20efficient%20LMMs%20always%20focus%20on%20replacing%20the%20LLM%0Abackbone%20with%20smaller%20models%2C%20while%20neglecting%20the%20crucial%20issue%20of%20token%0Aquantity.%20In%20this%20paper%2C%20we%20introduce%20LLaVA-Mini%2C%20an%20efficient%20LMM%20with%20minimal%0Avision%20tokens.%20To%20achieve%20a%20high%20compression%20ratio%20of%20vision%20tokens%20while%0Apreserving%20visual%20information%2C%20we%20first%20analyze%20how%20LMMs%20understand%20vision%0Atokens%20and%20find%20that%20most%20vision%20tokens%20only%20play%20a%20crucial%20role%20in%20the%20early%0Alayers%20of%20LLM%20backbone%2C%20where%20they%20mainly%20fuse%20visual%20information%20into%20text%0Atokens.%20Building%20on%20this%20finding%2C%20LLaVA-Mini%20introduces%20modality%20pre-fusion%20to%0Afuse%20visual%20information%20into%20text%20tokens%20in%20advance%2C%20thereby%20facilitating%20the%0Aextreme%20compression%20of%20vision%20tokens%20fed%20to%20LLM%20backbone%20into%20one%20token.%0ALLaVA-Mini%20is%20a%20unified%20large%20multimodal%20model%20that%20can%20support%20the%0Aunderstanding%20of%20images%2C%20high-resolution%20images%2C%20and%20videos%20in%20an%20efficient%0Amanner.%20Experiments%20across%2011%20image-based%20and%207%20video-based%20benchmarks%0Ademonstrate%20that%20LLaVA-Mini%20outperforms%20LLaVA-v1.5%20with%20just%201%20vision%20token%0Ainstead%20of%20576.%20Efficiency%20analyses%20reveal%20that%20LLaVA-Mini%20can%20reduce%20FLOPs%20by%0A77%25%2C%20deliver%20low-latency%20responses%20within%2040%20milliseconds%2C%20and%20process%20over%0A10%2C000%20frames%20of%20video%20on%20the%20GPU%20hardware%20with%2024GB%20of%20memory.%0A&entry.1838667208=http%3A//arxiv.org/abs/2501.03895v1&entry.124074799=Read"},
{"title": "BERTopic for Topic Modeling of Hindi Short Texts: A Comparative Study", "author": "Atharva Mutsaddi and Anvi Jamkhande and Aryan Thakre and Yashodhara Haribhakta", "abstract": "  As short text data in native languages like Hindi increasingly appear in\nmodern media, robust methods for topic modeling on such data have gained\nimportance. This study investigates the performance of BERTopic in modeling\nHindi short texts, an area that has been under-explored in existing research.\nUsing contextual embeddings, BERTopic can capture semantic relationships in\ndata, making it potentially more effective than traditional models, especially\nfor short and diverse texts. We evaluate BERTopic using 6 different document\nembedding models and compare its performance against 8 established topic\nmodeling techniques, such as Latent Dirichlet Allocation (LDA), Non-negative\nMatrix Factorization (NMF), Latent Semantic Indexing (LSI), Additive\nRegularization of Topic Models (ARTM), Probabilistic Latent Semantic Analysis\n(PLSA), Embedded Topic Model (ETM), Combined Topic Model (CTM), and Top2Vec.\nThe models are assessed using coherence scores across a range of topic counts.\nOur results reveal that BERTopic consistently outperforms other models in\ncapturing coherent topics from short Hindi texts.\n", "link": "http://arxiv.org/abs/2501.03843v1", "date": "2025-01-07", "relevancy": 2.2121, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.4481}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.4481}, {"title": "Breadcrumbs: Adversarial class-balanced sampling for long-tailed recognition", "link": "https://link.springer.com/chapter/10.1007/978-3-031-20053-3_37", "similarity": 0.4311}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20BERTopic%20for%20Topic%20Modeling%20of%20Hindi%20Short%20Texts%3A%20A%20Comparative%20Study&body=Title%3A%20BERTopic%20for%20Topic%20Modeling%20of%20Hindi%20Short%20Texts%3A%20A%20Comparative%20Study%0AAuthor%3A%20Atharva%20Mutsaddi%20and%20Anvi%20Jamkhande%20and%20Aryan%20Thakre%20and%20Yashodhara%20Haribhakta%0AAbstract%3A%20%20%20As%20short%20text%20data%20in%20native%20languages%20like%20Hindi%20increasingly%20appear%20in%0Amodern%20media%2C%20robust%20methods%20for%20topic%20modeling%20on%20such%20data%20have%20gained%0Aimportance.%20This%20study%20investigates%20the%20performance%20of%20BERTopic%20in%20modeling%0AHindi%20short%20texts%2C%20an%20area%20that%20has%20been%20under-explored%20in%20existing%20research.%0AUsing%20contextual%20embeddings%2C%20BERTopic%20can%20capture%20semantic%20relationships%20in%0Adata%2C%20making%20it%20potentially%20more%20effective%20than%20traditional%20models%2C%20especially%0Afor%20short%20and%20diverse%20texts.%20We%20evaluate%20BERTopic%20using%206%20different%20document%0Aembedding%20models%20and%20compare%20its%20performance%20against%208%20established%20topic%0Amodeling%20techniques%2C%20such%20as%20Latent%20Dirichlet%20Allocation%20%28LDA%29%2C%20Non-negative%0AMatrix%20Factorization%20%28NMF%29%2C%20Latent%20Semantic%20Indexing%20%28LSI%29%2C%20Additive%0ARegularization%20of%20Topic%20Models%20%28ARTM%29%2C%20Probabilistic%20Latent%20Semantic%20Analysis%0A%28PLSA%29%2C%20Embedded%20Topic%20Model%20%28ETM%29%2C%20Combined%20Topic%20Model%20%28CTM%29%2C%20and%20Top2Vec.%0AThe%20models%20are%20assessed%20using%20coherence%20scores%20across%20a%20range%20of%20topic%20counts.%0AOur%20results%20reveal%20that%20BERTopic%20consistently%20outperforms%20other%20models%20in%0Acapturing%20coherent%20topics%20from%20short%20Hindi%20texts.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2501.03843v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DBERTopic%2520for%2520Topic%2520Modeling%2520of%2520Hindi%2520Short%2520Texts%253A%2520A%2520Comparative%2520Study%26entry.906535625%3DAtharva%2520Mutsaddi%2520and%2520Anvi%2520Jamkhande%2520and%2520Aryan%2520Thakre%2520and%2520Yashodhara%2520Haribhakta%26entry.1292438233%3D%2520%2520As%2520short%2520text%2520data%2520in%2520native%2520languages%2520like%2520Hindi%2520increasingly%2520appear%2520in%250Amodern%2520media%252C%2520robust%2520methods%2520for%2520topic%2520modeling%2520on%2520such%2520data%2520have%2520gained%250Aimportance.%2520This%2520study%2520investigates%2520the%2520performance%2520of%2520BERTopic%2520in%2520modeling%250AHindi%2520short%2520texts%252C%2520an%2520area%2520that%2520has%2520been%2520under-explored%2520in%2520existing%2520research.%250AUsing%2520contextual%2520embeddings%252C%2520BERTopic%2520can%2520capture%2520semantic%2520relationships%2520in%250Adata%252C%2520making%2520it%2520potentially%2520more%2520effective%2520than%2520traditional%2520models%252C%2520especially%250Afor%2520short%2520and%2520diverse%2520texts.%2520We%2520evaluate%2520BERTopic%2520using%25206%2520different%2520document%250Aembedding%2520models%2520and%2520compare%2520its%2520performance%2520against%25208%2520established%2520topic%250Amodeling%2520techniques%252C%2520such%2520as%2520Latent%2520Dirichlet%2520Allocation%2520%2528LDA%2529%252C%2520Non-negative%250AMatrix%2520Factorization%2520%2528NMF%2529%252C%2520Latent%2520Semantic%2520Indexing%2520%2528LSI%2529%252C%2520Additive%250ARegularization%2520of%2520Topic%2520Models%2520%2528ARTM%2529%252C%2520Probabilistic%2520Latent%2520Semantic%2520Analysis%250A%2528PLSA%2529%252C%2520Embedded%2520Topic%2520Model%2520%2528ETM%2529%252C%2520Combined%2520Topic%2520Model%2520%2528CTM%2529%252C%2520and%2520Top2Vec.%250AThe%2520models%2520are%2520assessed%2520using%2520coherence%2520scores%2520across%2520a%2520range%2520of%2520topic%2520counts.%250AOur%2520results%2520reveal%2520that%2520BERTopic%2520consistently%2520outperforms%2520other%2520models%2520in%250Acapturing%2520coherent%2520topics%2520from%2520short%2520Hindi%2520texts.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2501.03843v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=BERTopic%20for%20Topic%20Modeling%20of%20Hindi%20Short%20Texts%3A%20A%20Comparative%20Study&entry.906535625=Atharva%20Mutsaddi%20and%20Anvi%20Jamkhande%20and%20Aryan%20Thakre%20and%20Yashodhara%20Haribhakta&entry.1292438233=%20%20As%20short%20text%20data%20in%20native%20languages%20like%20Hindi%20increasingly%20appear%20in%0Amodern%20media%2C%20robust%20methods%20for%20topic%20modeling%20on%20such%20data%20have%20gained%0Aimportance.%20This%20study%20investigates%20the%20performance%20of%20BERTopic%20in%20modeling%0AHindi%20short%20texts%2C%20an%20area%20that%20has%20been%20under-explored%20in%20existing%20research.%0AUsing%20contextual%20embeddings%2C%20BERTopic%20can%20capture%20semantic%20relationships%20in%0Adata%2C%20making%20it%20potentially%20more%20effective%20than%20traditional%20models%2C%20especially%0Afor%20short%20and%20diverse%20texts.%20We%20evaluate%20BERTopic%20using%206%20different%20document%0Aembedding%20models%20and%20compare%20its%20performance%20against%208%20established%20topic%0Amodeling%20techniques%2C%20such%20as%20Latent%20Dirichlet%20Allocation%20%28LDA%29%2C%20Non-negative%0AMatrix%20Factorization%20%28NMF%29%2C%20Latent%20Semantic%20Indexing%20%28LSI%29%2C%20Additive%0ARegularization%20of%20Topic%20Models%20%28ARTM%29%2C%20Probabilistic%20Latent%20Semantic%20Analysis%0A%28PLSA%29%2C%20Embedded%20Topic%20Model%20%28ETM%29%2C%20Combined%20Topic%20Model%20%28CTM%29%2C%20and%20Top2Vec.%0AThe%20models%20are%20assessed%20using%20coherence%20scores%20across%20a%20range%20of%20topic%20counts.%0AOur%20results%20reveal%20that%20BERTopic%20consistently%20outperforms%20other%20models%20in%0Acapturing%20coherent%20topics%20from%20short%20Hindi%20texts.%0A&entry.1838667208=http%3A//arxiv.org/abs/2501.03843v1&entry.124074799=Read"},
{"title": "Deep Sylvester Posterior Inference for Adaptive Compressed Sensing in\n  Ultrasound Imaging", "author": "Simon W. Penninga and Hans van Gorp and Ruud J. G. van Sloun", "abstract": "  Ultrasound images are commonly formed by sequential acquisition of\nbeam-steered scan-lines. Minimizing the number of required scan-lines can\nsignificantly enhance frame rate, field of view, energy efficiency, and data\ntransfer speeds. Existing approaches typically use static subsampling schemes\nin combination with sparsity-based or, more recently, deep-learning-based\nrecovery. In this work, we introduce an adaptive subsampling method that\nmaximizes intrinsic information gain in-situ, employing a Sylvester Normalizing\nFlow encoder to infer an approximate Bayesian posterior under partial\nobservation in real-time. Using the Bayesian posterior and a deep generative\nmodel for future observations, we determine the subsampling scheme that\nmaximizes the mutual information between the subsampled observations, and the\nnext frame of the video. We evaluate our approach using the EchoNet cardiac\nultrasound video dataset and demonstrate that our active sampling method\noutperforms competitive baselines, including uniform and variable-density\nrandom sampling, as well as equidistantly spaced scan-lines, improving mean\nabsolute reconstruction error by 15%. Moreover, posterior inference and the\nsampling scheme generation are performed in just 0.015 seconds (66Hz), making\nit fast enough for real-time 2D ultrasound imaging applications.\n", "link": "http://arxiv.org/abs/2501.03825v1", "date": "2025-01-07", "relevancy": 2.1914, "topK": [{"title": "SDXS: Real-Time One-Step Latent Diffusion Models with Image Conditions", "link": "http://arxiv.org/abs/2403.16627v1", "similarity": 0.5726}, {"title": "RTG-SLAM: Real-time 3D Reconstruction at Scale using Gaussian Splatting", "link": "http://arxiv.org/abs/2404.19706v1", "similarity": 0.5478}, {"title": "Tuning-Free Noise Rectification for High Fidelity Image-to-Video\n  Generation", "link": "http://arxiv.org/abs/2403.02827v1", "similarity": 0.538}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Deep%20Sylvester%20Posterior%20Inference%20for%20Adaptive%20Compressed%20Sensing%20in%0A%20%20Ultrasound%20Imaging&body=Title%3A%20Deep%20Sylvester%20Posterior%20Inference%20for%20Adaptive%20Compressed%20Sensing%20in%0A%20%20Ultrasound%20Imaging%0AAuthor%3A%20Simon%20W.%20Penninga%20and%20Hans%20van%20Gorp%20and%20Ruud%20J.%20G.%20van%20Sloun%0AAbstract%3A%20%20%20Ultrasound%20images%20are%20commonly%20formed%20by%20sequential%20acquisition%20of%0Abeam-steered%20scan-lines.%20Minimizing%20the%20number%20of%20required%20scan-lines%20can%0Asignificantly%20enhance%20frame%20rate%2C%20field%20of%20view%2C%20energy%20efficiency%2C%20and%20data%0Atransfer%20speeds.%20Existing%20approaches%20typically%20use%20static%20subsampling%20schemes%0Ain%20combination%20with%20sparsity-based%20or%2C%20more%20recently%2C%20deep-learning-based%0Arecovery.%20In%20this%20work%2C%20we%20introduce%20an%20adaptive%20subsampling%20method%20that%0Amaximizes%20intrinsic%20information%20gain%20in-situ%2C%20employing%20a%20Sylvester%20Normalizing%0AFlow%20encoder%20to%20infer%20an%20approximate%20Bayesian%20posterior%20under%20partial%0Aobservation%20in%20real-time.%20Using%20the%20Bayesian%20posterior%20and%20a%20deep%20generative%0Amodel%20for%20future%20observations%2C%20we%20determine%20the%20subsampling%20scheme%20that%0Amaximizes%20the%20mutual%20information%20between%20the%20subsampled%20observations%2C%20and%20the%0Anext%20frame%20of%20the%20video.%20We%20evaluate%20our%20approach%20using%20the%20EchoNet%20cardiac%0Aultrasound%20video%20dataset%20and%20demonstrate%20that%20our%20active%20sampling%20method%0Aoutperforms%20competitive%20baselines%2C%20including%20uniform%20and%20variable-density%0Arandom%20sampling%2C%20as%20well%20as%20equidistantly%20spaced%20scan-lines%2C%20improving%20mean%0Aabsolute%20reconstruction%20error%20by%2015%25.%20Moreover%2C%20posterior%20inference%20and%20the%0Asampling%20scheme%20generation%20are%20performed%20in%20just%200.015%20seconds%20%2866Hz%29%2C%20making%0Ait%20fast%20enough%20for%20real-time%202D%20ultrasound%20imaging%20applications.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2501.03825v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DDeep%2520Sylvester%2520Posterior%2520Inference%2520for%2520Adaptive%2520Compressed%2520Sensing%2520in%250A%2520%2520Ultrasound%2520Imaging%26entry.906535625%3DSimon%2520W.%2520Penninga%2520and%2520Hans%2520van%2520Gorp%2520and%2520Ruud%2520J.%2520G.%2520van%2520Sloun%26entry.1292438233%3D%2520%2520Ultrasound%2520images%2520are%2520commonly%2520formed%2520by%2520sequential%2520acquisition%2520of%250Abeam-steered%2520scan-lines.%2520Minimizing%2520the%2520number%2520of%2520required%2520scan-lines%2520can%250Asignificantly%2520enhance%2520frame%2520rate%252C%2520field%2520of%2520view%252C%2520energy%2520efficiency%252C%2520and%2520data%250Atransfer%2520speeds.%2520Existing%2520approaches%2520typically%2520use%2520static%2520subsampling%2520schemes%250Ain%2520combination%2520with%2520sparsity-based%2520or%252C%2520more%2520recently%252C%2520deep-learning-based%250Arecovery.%2520In%2520this%2520work%252C%2520we%2520introduce%2520an%2520adaptive%2520subsampling%2520method%2520that%250Amaximizes%2520intrinsic%2520information%2520gain%2520in-situ%252C%2520employing%2520a%2520Sylvester%2520Normalizing%250AFlow%2520encoder%2520to%2520infer%2520an%2520approximate%2520Bayesian%2520posterior%2520under%2520partial%250Aobservation%2520in%2520real-time.%2520Using%2520the%2520Bayesian%2520posterior%2520and%2520a%2520deep%2520generative%250Amodel%2520for%2520future%2520observations%252C%2520we%2520determine%2520the%2520subsampling%2520scheme%2520that%250Amaximizes%2520the%2520mutual%2520information%2520between%2520the%2520subsampled%2520observations%252C%2520and%2520the%250Anext%2520frame%2520of%2520the%2520video.%2520We%2520evaluate%2520our%2520approach%2520using%2520the%2520EchoNet%2520cardiac%250Aultrasound%2520video%2520dataset%2520and%2520demonstrate%2520that%2520our%2520active%2520sampling%2520method%250Aoutperforms%2520competitive%2520baselines%252C%2520including%2520uniform%2520and%2520variable-density%250Arandom%2520sampling%252C%2520as%2520well%2520as%2520equidistantly%2520spaced%2520scan-lines%252C%2520improving%2520mean%250Aabsolute%2520reconstruction%2520error%2520by%252015%2525.%2520Moreover%252C%2520posterior%2520inference%2520and%2520the%250Asampling%2520scheme%2520generation%2520are%2520performed%2520in%2520just%25200.015%2520seconds%2520%252866Hz%2529%252C%2520making%250Ait%2520fast%2520enough%2520for%2520real-time%25202D%2520ultrasound%2520imaging%2520applications.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2501.03825v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Deep%20Sylvester%20Posterior%20Inference%20for%20Adaptive%20Compressed%20Sensing%20in%0A%20%20Ultrasound%20Imaging&entry.906535625=Simon%20W.%20Penninga%20and%20Hans%20van%20Gorp%20and%20Ruud%20J.%20G.%20van%20Sloun&entry.1292438233=%20%20Ultrasound%20images%20are%20commonly%20formed%20by%20sequential%20acquisition%20of%0Abeam-steered%20scan-lines.%20Minimizing%20the%20number%20of%20required%20scan-lines%20can%0Asignificantly%20enhance%20frame%20rate%2C%20field%20of%20view%2C%20energy%20efficiency%2C%20and%20data%0Atransfer%20speeds.%20Existing%20approaches%20typically%20use%20static%20subsampling%20schemes%0Ain%20combination%20with%20sparsity-based%20or%2C%20more%20recently%2C%20deep-learning-based%0Arecovery.%20In%20this%20work%2C%20we%20introduce%20an%20adaptive%20subsampling%20method%20that%0Amaximizes%20intrinsic%20information%20gain%20in-situ%2C%20employing%20a%20Sylvester%20Normalizing%0AFlow%20encoder%20to%20infer%20an%20approximate%20Bayesian%20posterior%20under%20partial%0Aobservation%20in%20real-time.%20Using%20the%20Bayesian%20posterior%20and%20a%20deep%20generative%0Amodel%20for%20future%20observations%2C%20we%20determine%20the%20subsampling%20scheme%20that%0Amaximizes%20the%20mutual%20information%20between%20the%20subsampled%20observations%2C%20and%20the%0Anext%20frame%20of%20the%20video.%20We%20evaluate%20our%20approach%20using%20the%20EchoNet%20cardiac%0Aultrasound%20video%20dataset%20and%20demonstrate%20that%20our%20active%20sampling%20method%0Aoutperforms%20competitive%20baselines%2C%20including%20uniform%20and%20variable-density%0Arandom%20sampling%2C%20as%20well%20as%20equidistantly%20spaced%20scan-lines%2C%20improving%20mean%0Aabsolute%20reconstruction%20error%20by%2015%25.%20Moreover%2C%20posterior%20inference%20and%20the%0Asampling%20scheme%20generation%20are%20performed%20in%20just%200.015%20seconds%20%2866Hz%29%2C%20making%0Ait%20fast%20enough%20for%20real-time%202D%20ultrasound%20imaging%20applications.%0A&entry.1838667208=http%3A//arxiv.org/abs/2501.03825v1&entry.124074799=Read"},
{"title": "LM-Net: A Light-weight and Multi-scale Network for Medical Image\n  Segmentation", "author": "Zhenkun Lu and Chaoyin She and Wei Wang and Qinghua Huang", "abstract": "  Current medical image segmentation approaches have limitations in deeply\nexploring multi-scale information and effectively combining local detail\ntextures with global contextual semantic information. This results in\nover-segmentation, under-segmentation, and blurred segmentation boundaries. To\ntackle these challenges, we explore multi-scale feature representations from\ndifferent perspectives, proposing a novel, lightweight, and multi-scale\narchitecture (LM-Net) that integrates advantages of both Convolutional Neural\nNetworks (CNNs) and Vision Transformers (ViTs) to enhance segmentation\naccuracy. LM-Net employs a lightweight multi-branch module to capture\nmulti-scale features at the same level. Furthermore, we introduce two modules\nto concurrently capture local detail textures and global semantics with\nmulti-scale features at different levels: the Local Feature Transformer (LFT)\nand Global Feature Transformer (GFT). The LFT integrates local window\nself-attention to capture local detail textures, while the GFT leverages global\nself-attention to capture global contextual semantics. By combining these\nmodules, our model achieves complementarity between local and global\nrepresentations, alleviating the problem of blurred segmentation boundaries in\nmedical image segmentation. To evaluate the feasibility of LM-Net, extensive\nexperiments have been conducted on three publicly available datasets with\ndifferent modalities. Our proposed model achieves state-of-the-art results,\nsurpassing previous methods, while only requiring 4.66G FLOPs and 5.4M\nparameters. These state-of-the-art results on three datasets with different\nmodalities demonstrate the effectiveness and adaptability of our proposed\nLM-Net for various medical image segmentation tasks.\n", "link": "http://arxiv.org/abs/2501.03838v1", "date": "2025-01-07", "relevancy": 2.1891, "topK": [{"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5666}, {"title": "DDM-NET: End-to-end learning of keypoint feature Detection, Description and Matching for 3D localization", "link": "https://arxiv.org/abs/2212.04575", "similarity": 0.5351}, {"title": "SDXS: Real-Time One-Step Latent Diffusion Models with Image Conditions", "link": "http://arxiv.org/abs/2403.16627v1", "similarity": 0.5294}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20LM-Net%3A%20A%20Light-weight%20and%20Multi-scale%20Network%20for%20Medical%20Image%0A%20%20Segmentation&body=Title%3A%20LM-Net%3A%20A%20Light-weight%20and%20Multi-scale%20Network%20for%20Medical%20Image%0A%20%20Segmentation%0AAuthor%3A%20Zhenkun%20Lu%20and%20Chaoyin%20She%20and%20Wei%20Wang%20and%20Qinghua%20Huang%0AAbstract%3A%20%20%20Current%20medical%20image%20segmentation%20approaches%20have%20limitations%20in%20deeply%0Aexploring%20multi-scale%20information%20and%20effectively%20combining%20local%20detail%0Atextures%20with%20global%20contextual%20semantic%20information.%20This%20results%20in%0Aover-segmentation%2C%20under-segmentation%2C%20and%20blurred%20segmentation%20boundaries.%20To%0Atackle%20these%20challenges%2C%20we%20explore%20multi-scale%20feature%20representations%20from%0Adifferent%20perspectives%2C%20proposing%20a%20novel%2C%20lightweight%2C%20and%20multi-scale%0Aarchitecture%20%28LM-Net%29%20that%20integrates%20advantages%20of%20both%20Convolutional%20Neural%0ANetworks%20%28CNNs%29%20and%20Vision%20Transformers%20%28ViTs%29%20to%20enhance%20segmentation%0Aaccuracy.%20LM-Net%20employs%20a%20lightweight%20multi-branch%20module%20to%20capture%0Amulti-scale%20features%20at%20the%20same%20level.%20Furthermore%2C%20we%20introduce%20two%20modules%0Ato%20concurrently%20capture%20local%20detail%20textures%20and%20global%20semantics%20with%0Amulti-scale%20features%20at%20different%20levels%3A%20the%20Local%20Feature%20Transformer%20%28LFT%29%0Aand%20Global%20Feature%20Transformer%20%28GFT%29.%20The%20LFT%20integrates%20local%20window%0Aself-attention%20to%20capture%20local%20detail%20textures%2C%20while%20the%20GFT%20leverages%20global%0Aself-attention%20to%20capture%20global%20contextual%20semantics.%20By%20combining%20these%0Amodules%2C%20our%20model%20achieves%20complementarity%20between%20local%20and%20global%0Arepresentations%2C%20alleviating%20the%20problem%20of%20blurred%20segmentation%20boundaries%20in%0Amedical%20image%20segmentation.%20To%20evaluate%20the%20feasibility%20of%20LM-Net%2C%20extensive%0Aexperiments%20have%20been%20conducted%20on%20three%20publicly%20available%20datasets%20with%0Adifferent%20modalities.%20Our%20proposed%20model%20achieves%20state-of-the-art%20results%2C%0Asurpassing%20previous%20methods%2C%20while%20only%20requiring%204.66G%20FLOPs%20and%205.4M%0Aparameters.%20These%20state-of-the-art%20results%20on%20three%20datasets%20with%20different%0Amodalities%20demonstrate%20the%20effectiveness%20and%20adaptability%20of%20our%20proposed%0ALM-Net%20for%20various%20medical%20image%20segmentation%20tasks.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2501.03838v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DLM-Net%253A%2520A%2520Light-weight%2520and%2520Multi-scale%2520Network%2520for%2520Medical%2520Image%250A%2520%2520Segmentation%26entry.906535625%3DZhenkun%2520Lu%2520and%2520Chaoyin%2520She%2520and%2520Wei%2520Wang%2520and%2520Qinghua%2520Huang%26entry.1292438233%3D%2520%2520Current%2520medical%2520image%2520segmentation%2520approaches%2520have%2520limitations%2520in%2520deeply%250Aexploring%2520multi-scale%2520information%2520and%2520effectively%2520combining%2520local%2520detail%250Atextures%2520with%2520global%2520contextual%2520semantic%2520information.%2520This%2520results%2520in%250Aover-segmentation%252C%2520under-segmentation%252C%2520and%2520blurred%2520segmentation%2520boundaries.%2520To%250Atackle%2520these%2520challenges%252C%2520we%2520explore%2520multi-scale%2520feature%2520representations%2520from%250Adifferent%2520perspectives%252C%2520proposing%2520a%2520novel%252C%2520lightweight%252C%2520and%2520multi-scale%250Aarchitecture%2520%2528LM-Net%2529%2520that%2520integrates%2520advantages%2520of%2520both%2520Convolutional%2520Neural%250ANetworks%2520%2528CNNs%2529%2520and%2520Vision%2520Transformers%2520%2528ViTs%2529%2520to%2520enhance%2520segmentation%250Aaccuracy.%2520LM-Net%2520employs%2520a%2520lightweight%2520multi-branch%2520module%2520to%2520capture%250Amulti-scale%2520features%2520at%2520the%2520same%2520level.%2520Furthermore%252C%2520we%2520introduce%2520two%2520modules%250Ato%2520concurrently%2520capture%2520local%2520detail%2520textures%2520and%2520global%2520semantics%2520with%250Amulti-scale%2520features%2520at%2520different%2520levels%253A%2520the%2520Local%2520Feature%2520Transformer%2520%2528LFT%2529%250Aand%2520Global%2520Feature%2520Transformer%2520%2528GFT%2529.%2520The%2520LFT%2520integrates%2520local%2520window%250Aself-attention%2520to%2520capture%2520local%2520detail%2520textures%252C%2520while%2520the%2520GFT%2520leverages%2520global%250Aself-attention%2520to%2520capture%2520global%2520contextual%2520semantics.%2520By%2520combining%2520these%250Amodules%252C%2520our%2520model%2520achieves%2520complementarity%2520between%2520local%2520and%2520global%250Arepresentations%252C%2520alleviating%2520the%2520problem%2520of%2520blurred%2520segmentation%2520boundaries%2520in%250Amedical%2520image%2520segmentation.%2520To%2520evaluate%2520the%2520feasibility%2520of%2520LM-Net%252C%2520extensive%250Aexperiments%2520have%2520been%2520conducted%2520on%2520three%2520publicly%2520available%2520datasets%2520with%250Adifferent%2520modalities.%2520Our%2520proposed%2520model%2520achieves%2520state-of-the-art%2520results%252C%250Asurpassing%2520previous%2520methods%252C%2520while%2520only%2520requiring%25204.66G%2520FLOPs%2520and%25205.4M%250Aparameters.%2520These%2520state-of-the-art%2520results%2520on%2520three%2520datasets%2520with%2520different%250Amodalities%2520demonstrate%2520the%2520effectiveness%2520and%2520adaptability%2520of%2520our%2520proposed%250ALM-Net%2520for%2520various%2520medical%2520image%2520segmentation%2520tasks.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2501.03838v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=LM-Net%3A%20A%20Light-weight%20and%20Multi-scale%20Network%20for%20Medical%20Image%0A%20%20Segmentation&entry.906535625=Zhenkun%20Lu%20and%20Chaoyin%20She%20and%20Wei%20Wang%20and%20Qinghua%20Huang&entry.1292438233=%20%20Current%20medical%20image%20segmentation%20approaches%20have%20limitations%20in%20deeply%0Aexploring%20multi-scale%20information%20and%20effectively%20combining%20local%20detail%0Atextures%20with%20global%20contextual%20semantic%20information.%20This%20results%20in%0Aover-segmentation%2C%20under-segmentation%2C%20and%20blurred%20segmentation%20boundaries.%20To%0Atackle%20these%20challenges%2C%20we%20explore%20multi-scale%20feature%20representations%20from%0Adifferent%20perspectives%2C%20proposing%20a%20novel%2C%20lightweight%2C%20and%20multi-scale%0Aarchitecture%20%28LM-Net%29%20that%20integrates%20advantages%20of%20both%20Convolutional%20Neural%0ANetworks%20%28CNNs%29%20and%20Vision%20Transformers%20%28ViTs%29%20to%20enhance%20segmentation%0Aaccuracy.%20LM-Net%20employs%20a%20lightweight%20multi-branch%20module%20to%20capture%0Amulti-scale%20features%20at%20the%20same%20level.%20Furthermore%2C%20we%20introduce%20two%20modules%0Ato%20concurrently%20capture%20local%20detail%20textures%20and%20global%20semantics%20with%0Amulti-scale%20features%20at%20different%20levels%3A%20the%20Local%20Feature%20Transformer%20%28LFT%29%0Aand%20Global%20Feature%20Transformer%20%28GFT%29.%20The%20LFT%20integrates%20local%20window%0Aself-attention%20to%20capture%20local%20detail%20textures%2C%20while%20the%20GFT%20leverages%20global%0Aself-attention%20to%20capture%20global%20contextual%20semantics.%20By%20combining%20these%0Amodules%2C%20our%20model%20achieves%20complementarity%20between%20local%20and%20global%0Arepresentations%2C%20alleviating%20the%20problem%20of%20blurred%20segmentation%20boundaries%20in%0Amedical%20image%20segmentation.%20To%20evaluate%20the%20feasibility%20of%20LM-Net%2C%20extensive%0Aexperiments%20have%20been%20conducted%20on%20three%20publicly%20available%20datasets%20with%0Adifferent%20modalities.%20Our%20proposed%20model%20achieves%20state-of-the-art%20results%2C%0Asurpassing%20previous%20methods%2C%20while%20only%20requiring%204.66G%20FLOPs%20and%205.4M%0Aparameters.%20These%20state-of-the-art%20results%20on%20three%20datasets%20with%20different%0Amodalities%20demonstrate%20the%20effectiveness%20and%20adaptability%20of%20our%20proposed%0ALM-Net%20for%20various%20medical%20image%20segmentation%20tasks.%0A&entry.1838667208=http%3A//arxiv.org/abs/2501.03838v1&entry.124074799=Read"},
{"title": "Re-Visible Dual-Domain Self-Supervised Deep Unfolding Network for MRI\n  Reconstruction", "author": "Hao Zhang and Qi Wang and Jian Sun and Zhijie Wen and Jun Shi and Shihui Ying", "abstract": "  Magnetic Resonance Imaging (MRI) is widely used in clinical practice, but\nsuffered from prolonged acquisition time. Although deep learning methods have\nbeen proposed to accelerate acquisition and demonstrate promising performance,\nthey rely on high-quality fully-sampled datasets for training in a supervised\nmanner. However, such datasets are time-consuming and expensive-to-collect,\nwhich constrains their broader applications. On the other hand, self-supervised\nmethods offer an alternative by enabling learning from under-sampled data\nalone, but most existing methods rely on further partitioned under-sampled\nk-space data as model's input for training, resulting in a loss of valuable\ninformation. Additionally, their models have not fully incorporated image\npriors, leading to degraded reconstruction performance. In this paper, we\npropose a novel re-visible dual-domain self-supervised deep unfolding network\nto address these issues when only under-sampled datasets are available.\nSpecifically, by incorporating re-visible dual-domain loss, all under-sampled\nk-space data are utilized during training to mitigate information loss caused\nby further partitioning. This design enables the model to implicitly adapt to\nall under-sampled k-space data as input. Additionally, we design a deep\nunfolding network based on Chambolle and Pock Proximal Point Algorithm\n(DUN-CP-PPA) to achieve end-to-end reconstruction, incorporating imaging\nphysics and image priors to guide the reconstruction process. By employing a\nSpatial-Frequency Feature Extraction (SFFE) block to capture global and local\nfeature representation, we enhance the model's efficiency to learn\ncomprehensive image priors. Experiments conducted on the fastMRI and IXI\ndatasets demonstrate that our method significantly outperforms state-of-the-art\napproaches in terms of reconstruction performance.\n", "link": "http://arxiv.org/abs/2501.03737v1", "date": "2025-01-07", "relevancy": 2.1878, "topK": [{"title": "DDM-NET: End-to-end learning of keypoint feature Detection, Description and Matching for 3D localization", "link": "https://arxiv.org/abs/2212.04575", "similarity": 0.5514}, {"title": "Self-supervised Photographic Image Layout Representation Learning", "link": "http://arxiv.org/abs/2403.03740v1", "similarity": 0.5463}, {"title": "SDXS: Real-Time One-Step Latent Diffusion Models with Image Conditions", "link": "http://arxiv.org/abs/2403.16627v1", "similarity": 0.5374}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Re-Visible%20Dual-Domain%20Self-Supervised%20Deep%20Unfolding%20Network%20for%20MRI%0A%20%20Reconstruction&body=Title%3A%20Re-Visible%20Dual-Domain%20Self-Supervised%20Deep%20Unfolding%20Network%20for%20MRI%0A%20%20Reconstruction%0AAuthor%3A%20Hao%20Zhang%20and%20Qi%20Wang%20and%20Jian%20Sun%20and%20Zhijie%20Wen%20and%20Jun%20Shi%20and%20Shihui%20Ying%0AAbstract%3A%20%20%20Magnetic%20Resonance%20Imaging%20%28MRI%29%20is%20widely%20used%20in%20clinical%20practice%2C%20but%0Asuffered%20from%20prolonged%20acquisition%20time.%20Although%20deep%20learning%20methods%20have%0Abeen%20proposed%20to%20accelerate%20acquisition%20and%20demonstrate%20promising%20performance%2C%0Athey%20rely%20on%20high-quality%20fully-sampled%20datasets%20for%20training%20in%20a%20supervised%0Amanner.%20However%2C%20such%20datasets%20are%20time-consuming%20and%20expensive-to-collect%2C%0Awhich%20constrains%20their%20broader%20applications.%20On%20the%20other%20hand%2C%20self-supervised%0Amethods%20offer%20an%20alternative%20by%20enabling%20learning%20from%20under-sampled%20data%0Aalone%2C%20but%20most%20existing%20methods%20rely%20on%20further%20partitioned%20under-sampled%0Ak-space%20data%20as%20model%27s%20input%20for%20training%2C%20resulting%20in%20a%20loss%20of%20valuable%0Ainformation.%20Additionally%2C%20their%20models%20have%20not%20fully%20incorporated%20image%0Apriors%2C%20leading%20to%20degraded%20reconstruction%20performance.%20In%20this%20paper%2C%20we%0Apropose%20a%20novel%20re-visible%20dual-domain%20self-supervised%20deep%20unfolding%20network%0Ato%20address%20these%20issues%20when%20only%20under-sampled%20datasets%20are%20available.%0ASpecifically%2C%20by%20incorporating%20re-visible%20dual-domain%20loss%2C%20all%20under-sampled%0Ak-space%20data%20are%20utilized%20during%20training%20to%20mitigate%20information%20loss%20caused%0Aby%20further%20partitioning.%20This%20design%20enables%20the%20model%20to%20implicitly%20adapt%20to%0Aall%20under-sampled%20k-space%20data%20as%20input.%20Additionally%2C%20we%20design%20a%20deep%0Aunfolding%20network%20based%20on%20Chambolle%20and%20Pock%20Proximal%20Point%20Algorithm%0A%28DUN-CP-PPA%29%20to%20achieve%20end-to-end%20reconstruction%2C%20incorporating%20imaging%0Aphysics%20and%20image%20priors%20to%20guide%20the%20reconstruction%20process.%20By%20employing%20a%0ASpatial-Frequency%20Feature%20Extraction%20%28SFFE%29%20block%20to%20capture%20global%20and%20local%0Afeature%20representation%2C%20we%20enhance%20the%20model%27s%20efficiency%20to%20learn%0Acomprehensive%20image%20priors.%20Experiments%20conducted%20on%20the%20fastMRI%20and%20IXI%0Adatasets%20demonstrate%20that%20our%20method%20significantly%20outperforms%20state-of-the-art%0Aapproaches%20in%20terms%20of%20reconstruction%20performance.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2501.03737v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DRe-Visible%2520Dual-Domain%2520Self-Supervised%2520Deep%2520Unfolding%2520Network%2520for%2520MRI%250A%2520%2520Reconstruction%26entry.906535625%3DHao%2520Zhang%2520and%2520Qi%2520Wang%2520and%2520Jian%2520Sun%2520and%2520Zhijie%2520Wen%2520and%2520Jun%2520Shi%2520and%2520Shihui%2520Ying%26entry.1292438233%3D%2520%2520Magnetic%2520Resonance%2520Imaging%2520%2528MRI%2529%2520is%2520widely%2520used%2520in%2520clinical%2520practice%252C%2520but%250Asuffered%2520from%2520prolonged%2520acquisition%2520time.%2520Although%2520deep%2520learning%2520methods%2520have%250Abeen%2520proposed%2520to%2520accelerate%2520acquisition%2520and%2520demonstrate%2520promising%2520performance%252C%250Athey%2520rely%2520on%2520high-quality%2520fully-sampled%2520datasets%2520for%2520training%2520in%2520a%2520supervised%250Amanner.%2520However%252C%2520such%2520datasets%2520are%2520time-consuming%2520and%2520expensive-to-collect%252C%250Awhich%2520constrains%2520their%2520broader%2520applications.%2520On%2520the%2520other%2520hand%252C%2520self-supervised%250Amethods%2520offer%2520an%2520alternative%2520by%2520enabling%2520learning%2520from%2520under-sampled%2520data%250Aalone%252C%2520but%2520most%2520existing%2520methods%2520rely%2520on%2520further%2520partitioned%2520under-sampled%250Ak-space%2520data%2520as%2520model%2527s%2520input%2520for%2520training%252C%2520resulting%2520in%2520a%2520loss%2520of%2520valuable%250Ainformation.%2520Additionally%252C%2520their%2520models%2520have%2520not%2520fully%2520incorporated%2520image%250Apriors%252C%2520leading%2520to%2520degraded%2520reconstruction%2520performance.%2520In%2520this%2520paper%252C%2520we%250Apropose%2520a%2520novel%2520re-visible%2520dual-domain%2520self-supervised%2520deep%2520unfolding%2520network%250Ato%2520address%2520these%2520issues%2520when%2520only%2520under-sampled%2520datasets%2520are%2520available.%250ASpecifically%252C%2520by%2520incorporating%2520re-visible%2520dual-domain%2520loss%252C%2520all%2520under-sampled%250Ak-space%2520data%2520are%2520utilized%2520during%2520training%2520to%2520mitigate%2520information%2520loss%2520caused%250Aby%2520further%2520partitioning.%2520This%2520design%2520enables%2520the%2520model%2520to%2520implicitly%2520adapt%2520to%250Aall%2520under-sampled%2520k-space%2520data%2520as%2520input.%2520Additionally%252C%2520we%2520design%2520a%2520deep%250Aunfolding%2520network%2520based%2520on%2520Chambolle%2520and%2520Pock%2520Proximal%2520Point%2520Algorithm%250A%2528DUN-CP-PPA%2529%2520to%2520achieve%2520end-to-end%2520reconstruction%252C%2520incorporating%2520imaging%250Aphysics%2520and%2520image%2520priors%2520to%2520guide%2520the%2520reconstruction%2520process.%2520By%2520employing%2520a%250ASpatial-Frequency%2520Feature%2520Extraction%2520%2528SFFE%2529%2520block%2520to%2520capture%2520global%2520and%2520local%250Afeature%2520representation%252C%2520we%2520enhance%2520the%2520model%2527s%2520efficiency%2520to%2520learn%250Acomprehensive%2520image%2520priors.%2520Experiments%2520conducted%2520on%2520the%2520fastMRI%2520and%2520IXI%250Adatasets%2520demonstrate%2520that%2520our%2520method%2520significantly%2520outperforms%2520state-of-the-art%250Aapproaches%2520in%2520terms%2520of%2520reconstruction%2520performance.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2501.03737v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Re-Visible%20Dual-Domain%20Self-Supervised%20Deep%20Unfolding%20Network%20for%20MRI%0A%20%20Reconstruction&entry.906535625=Hao%20Zhang%20and%20Qi%20Wang%20and%20Jian%20Sun%20and%20Zhijie%20Wen%20and%20Jun%20Shi%20and%20Shihui%20Ying&entry.1292438233=%20%20Magnetic%20Resonance%20Imaging%20%28MRI%29%20is%20widely%20used%20in%20clinical%20practice%2C%20but%0Asuffered%20from%20prolonged%20acquisition%20time.%20Although%20deep%20learning%20methods%20have%0Abeen%20proposed%20to%20accelerate%20acquisition%20and%20demonstrate%20promising%20performance%2C%0Athey%20rely%20on%20high-quality%20fully-sampled%20datasets%20for%20training%20in%20a%20supervised%0Amanner.%20However%2C%20such%20datasets%20are%20time-consuming%20and%20expensive-to-collect%2C%0Awhich%20constrains%20their%20broader%20applications.%20On%20the%20other%20hand%2C%20self-supervised%0Amethods%20offer%20an%20alternative%20by%20enabling%20learning%20from%20under-sampled%20data%0Aalone%2C%20but%20most%20existing%20methods%20rely%20on%20further%20partitioned%20under-sampled%0Ak-space%20data%20as%20model%27s%20input%20for%20training%2C%20resulting%20in%20a%20loss%20of%20valuable%0Ainformation.%20Additionally%2C%20their%20models%20have%20not%20fully%20incorporated%20image%0Apriors%2C%20leading%20to%20degraded%20reconstruction%20performance.%20In%20this%20paper%2C%20we%0Apropose%20a%20novel%20re-visible%20dual-domain%20self-supervised%20deep%20unfolding%20network%0Ato%20address%20these%20issues%20when%20only%20under-sampled%20datasets%20are%20available.%0ASpecifically%2C%20by%20incorporating%20re-visible%20dual-domain%20loss%2C%20all%20under-sampled%0Ak-space%20data%20are%20utilized%20during%20training%20to%20mitigate%20information%20loss%20caused%0Aby%20further%20partitioning.%20This%20design%20enables%20the%20model%20to%20implicitly%20adapt%20to%0Aall%20under-sampled%20k-space%20data%20as%20input.%20Additionally%2C%20we%20design%20a%20deep%0Aunfolding%20network%20based%20on%20Chambolle%20and%20Pock%20Proximal%20Point%20Algorithm%0A%28DUN-CP-PPA%29%20to%20achieve%20end-to-end%20reconstruction%2C%20incorporating%20imaging%0Aphysics%20and%20image%20priors%20to%20guide%20the%20reconstruction%20process.%20By%20employing%20a%0ASpatial-Frequency%20Feature%20Extraction%20%28SFFE%29%20block%20to%20capture%20global%20and%20local%0Afeature%20representation%2C%20we%20enhance%20the%20model%27s%20efficiency%20to%20learn%0Acomprehensive%20image%20priors.%20Experiments%20conducted%20on%20the%20fastMRI%20and%20IXI%0Adatasets%20demonstrate%20that%20our%20method%20significantly%20outperforms%20state-of-the-art%0Aapproaches%20in%20terms%20of%20reconstruction%20performance.%0A&entry.1838667208=http%3A//arxiv.org/abs/2501.03737v1&entry.124074799=Read"},
{"title": "Scaling Efficient LLMs", "author": "B. N. Kausik", "abstract": "  Trained LLMs are typically sparse in that most of the parameters are zero,\nraising questions on efficiency. In response, we inquire into efficient LLMs,\ni.e. those with the fewest parameters that achieve the desired accuracy on a\ntraining corpus. Specifically, we compare theoretical and empirical estimates\nfor training loss to obtain upper and lower bounds on the number of unique\nsequences in a natural training corpus as a function of its size. Our result\nimplies (1) to double the number of skills represented in a training corpus,\nthe corpus must scale more than four fold (2) for efficient LLMs, the number of\nparameters N and the size D of a natural training corpus scale as $N \\propto\nD^{0.44}$; (3) if the number of parameters of an LLM is smaller than the number\nof unique sequences in the training corpus, scaling up can uncover emergent\nskills.\n", "link": "http://arxiv.org/abs/2402.14746v3", "date": "2025-01-07", "relevancy": 2.1847, "topK": [{"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.4376}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.4366}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.4366}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Scaling%20Efficient%20LLMs&body=Title%3A%20Scaling%20Efficient%20LLMs%0AAuthor%3A%20B.%20N.%20Kausik%0AAbstract%3A%20%20%20Trained%20LLMs%20are%20typically%20sparse%20in%20that%20most%20of%20the%20parameters%20are%20zero%2C%0Araising%20questions%20on%20efficiency.%20In%20response%2C%20we%20inquire%20into%20efficient%20LLMs%2C%0Ai.e.%20those%20with%20the%20fewest%20parameters%20that%20achieve%20the%20desired%20accuracy%20on%20a%0Atraining%20corpus.%20Specifically%2C%20we%20compare%20theoretical%20and%20empirical%20estimates%0Afor%20training%20loss%20to%20obtain%20upper%20and%20lower%20bounds%20on%20the%20number%20of%20unique%0Asequences%20in%20a%20natural%20training%20corpus%20as%20a%20function%20of%20its%20size.%20Our%20result%0Aimplies%20%281%29%20to%20double%20the%20number%20of%20skills%20represented%20in%20a%20training%20corpus%2C%0Athe%20corpus%20must%20scale%20more%20than%20four%20fold%20%282%29%20for%20efficient%20LLMs%2C%20the%20number%20of%0Aparameters%20N%20and%20the%20size%20D%20of%20a%20natural%20training%20corpus%20scale%20as%20%24N%20%5Cpropto%0AD%5E%7B0.44%7D%24%3B%20%283%29%20if%20the%20number%20of%20parameters%20of%20an%20LLM%20is%20smaller%20than%20the%20number%0Aof%20unique%20sequences%20in%20the%20training%20corpus%2C%20scaling%20up%20can%20uncover%20emergent%0Askills.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2402.14746v3%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DScaling%2520Efficient%2520LLMs%26entry.906535625%3DB.%2520N.%2520Kausik%26entry.1292438233%3D%2520%2520Trained%2520LLMs%2520are%2520typically%2520sparse%2520in%2520that%2520most%2520of%2520the%2520parameters%2520are%2520zero%252C%250Araising%2520questions%2520on%2520efficiency.%2520In%2520response%252C%2520we%2520inquire%2520into%2520efficient%2520LLMs%252C%250Ai.e.%2520those%2520with%2520the%2520fewest%2520parameters%2520that%2520achieve%2520the%2520desired%2520accuracy%2520on%2520a%250Atraining%2520corpus.%2520Specifically%252C%2520we%2520compare%2520theoretical%2520and%2520empirical%2520estimates%250Afor%2520training%2520loss%2520to%2520obtain%2520upper%2520and%2520lower%2520bounds%2520on%2520the%2520number%2520of%2520unique%250Asequences%2520in%2520a%2520natural%2520training%2520corpus%2520as%2520a%2520function%2520of%2520its%2520size.%2520Our%2520result%250Aimplies%2520%25281%2529%2520to%2520double%2520the%2520number%2520of%2520skills%2520represented%2520in%2520a%2520training%2520corpus%252C%250Athe%2520corpus%2520must%2520scale%2520more%2520than%2520four%2520fold%2520%25282%2529%2520for%2520efficient%2520LLMs%252C%2520the%2520number%2520of%250Aparameters%2520N%2520and%2520the%2520size%2520D%2520of%2520a%2520natural%2520training%2520corpus%2520scale%2520as%2520%2524N%2520%255Cpropto%250AD%255E%257B0.44%257D%2524%253B%2520%25283%2529%2520if%2520the%2520number%2520of%2520parameters%2520of%2520an%2520LLM%2520is%2520smaller%2520than%2520the%2520number%250Aof%2520unique%2520sequences%2520in%2520the%2520training%2520corpus%252C%2520scaling%2520up%2520can%2520uncover%2520emergent%250Askills.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2402.14746v3%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Scaling%20Efficient%20LLMs&entry.906535625=B.%20N.%20Kausik&entry.1292438233=%20%20Trained%20LLMs%20are%20typically%20sparse%20in%20that%20most%20of%20the%20parameters%20are%20zero%2C%0Araising%20questions%20on%20efficiency.%20In%20response%2C%20we%20inquire%20into%20efficient%20LLMs%2C%0Ai.e.%20those%20with%20the%20fewest%20parameters%20that%20achieve%20the%20desired%20accuracy%20on%20a%0Atraining%20corpus.%20Specifically%2C%20we%20compare%20theoretical%20and%20empirical%20estimates%0Afor%20training%20loss%20to%20obtain%20upper%20and%20lower%20bounds%20on%20the%20number%20of%20unique%0Asequences%20in%20a%20natural%20training%20corpus%20as%20a%20function%20of%20its%20size.%20Our%20result%0Aimplies%20%281%29%20to%20double%20the%20number%20of%20skills%20represented%20in%20a%20training%20corpus%2C%0Athe%20corpus%20must%20scale%20more%20than%20four%20fold%20%282%29%20for%20efficient%20LLMs%2C%20the%20number%20of%0Aparameters%20N%20and%20the%20size%20D%20of%20a%20natural%20training%20corpus%20scale%20as%20%24N%20%5Cpropto%0AD%5E%7B0.44%7D%24%3B%20%283%29%20if%20the%20number%20of%20parameters%20of%20an%20LLM%20is%20smaller%20than%20the%20number%0Aof%20unique%20sequences%20in%20the%20training%20corpus%2C%20scaling%20up%20can%20uncover%20emergent%0Askills.%0A&entry.1838667208=http%3A//arxiv.org/abs/2402.14746v3&entry.124074799=Read"},
{"title": "CoStruction: Conjoint radiance field optimization for urban scene\n  reconStruction with limited image overlap", "author": "Fusang Wang and Hala Djeghim and Nathan Piasco and Moussab Bennehar and Luis Rold\u00e3o and Dzmitry Tsishkou", "abstract": "  Reconstructing the surrounding surface geometry from recorded driving\nsequences poses a significant challenge due to the limited image overlap and\ncomplex topology of urban environments. SoTA neural implicit surface\nreconstruction methods often struggle in such setting, either failing due to\nsmall vision overlap or exhibiting suboptimal performance in accurately\nreconstructing both the surface and fine structures. To address these\nlimitations, we introduce CoStruction, a novel hybrid implicit surface\nreconstruction method tailored for large driving sequences with limited camera\noverlap. CoStruction leverages cross-representation uncertainty estimation to\nfilter out ambiguous geometry caused by limited observations. Our method\nperforms joint optimization of both radiance fields in addition to guided\nsampling achieving accurate reconstruction of large areas along with fine\nstructures in complex urban scenarios. Extensive evaluation on major driving\ndatasets demonstrates the superiority of our approach in reconstructing large\ndriving sequences with limited image overlap, outperforming concurrent SoTA\nmethods.\n", "link": "http://arxiv.org/abs/2501.03932v1", "date": "2025-01-07", "relevancy": 2.1515, "topK": [{"title": "RTG-SLAM: Real-time 3D Reconstruction at Scale using Gaussian Splatting", "link": "http://arxiv.org/abs/2404.19706v1", "similarity": 0.564}, {"title": "Thinking Outside the BBox: Unconstrained Generative Object Compositing", "link": "http://arxiv.org/abs/2409.04559v2", "similarity": 0.5231}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5177}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20CoStruction%3A%20Conjoint%20radiance%20field%20optimization%20for%20urban%20scene%0A%20%20reconStruction%20with%20limited%20image%20overlap&body=Title%3A%20CoStruction%3A%20Conjoint%20radiance%20field%20optimization%20for%20urban%20scene%0A%20%20reconStruction%20with%20limited%20image%20overlap%0AAuthor%3A%20Fusang%20Wang%20and%20Hala%20Djeghim%20and%20Nathan%20Piasco%20and%20Moussab%20Bennehar%20and%20Luis%20Rold%C3%A3o%20and%20Dzmitry%20Tsishkou%0AAbstract%3A%20%20%20Reconstructing%20the%20surrounding%20surface%20geometry%20from%20recorded%20driving%0Asequences%20poses%20a%20significant%20challenge%20due%20to%20the%20limited%20image%20overlap%20and%0Acomplex%20topology%20of%20urban%20environments.%20SoTA%20neural%20implicit%20surface%0Areconstruction%20methods%20often%20struggle%20in%20such%20setting%2C%20either%20failing%20due%20to%0Asmall%20vision%20overlap%20or%20exhibiting%20suboptimal%20performance%20in%20accurately%0Areconstructing%20both%20the%20surface%20and%20fine%20structures.%20To%20address%20these%0Alimitations%2C%20we%20introduce%20CoStruction%2C%20a%20novel%20hybrid%20implicit%20surface%0Areconstruction%20method%20tailored%20for%20large%20driving%20sequences%20with%20limited%20camera%0Aoverlap.%20CoStruction%20leverages%20cross-representation%20uncertainty%20estimation%20to%0Afilter%20out%20ambiguous%20geometry%20caused%20by%20limited%20observations.%20Our%20method%0Aperforms%20joint%20optimization%20of%20both%20radiance%20fields%20in%20addition%20to%20guided%0Asampling%20achieving%20accurate%20reconstruction%20of%20large%20areas%20along%20with%20fine%0Astructures%20in%20complex%20urban%20scenarios.%20Extensive%20evaluation%20on%20major%20driving%0Adatasets%20demonstrates%20the%20superiority%20of%20our%20approach%20in%20reconstructing%20large%0Adriving%20sequences%20with%20limited%20image%20overlap%2C%20outperforming%20concurrent%20SoTA%0Amethods.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2501.03932v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DCoStruction%253A%2520Conjoint%2520radiance%2520field%2520optimization%2520for%2520urban%2520scene%250A%2520%2520reconStruction%2520with%2520limited%2520image%2520overlap%26entry.906535625%3DFusang%2520Wang%2520and%2520Hala%2520Djeghim%2520and%2520Nathan%2520Piasco%2520and%2520Moussab%2520Bennehar%2520and%2520Luis%2520Rold%25C3%25A3o%2520and%2520Dzmitry%2520Tsishkou%26entry.1292438233%3D%2520%2520Reconstructing%2520the%2520surrounding%2520surface%2520geometry%2520from%2520recorded%2520driving%250Asequences%2520poses%2520a%2520significant%2520challenge%2520due%2520to%2520the%2520limited%2520image%2520overlap%2520and%250Acomplex%2520topology%2520of%2520urban%2520environments.%2520SoTA%2520neural%2520implicit%2520surface%250Areconstruction%2520methods%2520often%2520struggle%2520in%2520such%2520setting%252C%2520either%2520failing%2520due%2520to%250Asmall%2520vision%2520overlap%2520or%2520exhibiting%2520suboptimal%2520performance%2520in%2520accurately%250Areconstructing%2520both%2520the%2520surface%2520and%2520fine%2520structures.%2520To%2520address%2520these%250Alimitations%252C%2520we%2520introduce%2520CoStruction%252C%2520a%2520novel%2520hybrid%2520implicit%2520surface%250Areconstruction%2520method%2520tailored%2520for%2520large%2520driving%2520sequences%2520with%2520limited%2520camera%250Aoverlap.%2520CoStruction%2520leverages%2520cross-representation%2520uncertainty%2520estimation%2520to%250Afilter%2520out%2520ambiguous%2520geometry%2520caused%2520by%2520limited%2520observations.%2520Our%2520method%250Aperforms%2520joint%2520optimization%2520of%2520both%2520radiance%2520fields%2520in%2520addition%2520to%2520guided%250Asampling%2520achieving%2520accurate%2520reconstruction%2520of%2520large%2520areas%2520along%2520with%2520fine%250Astructures%2520in%2520complex%2520urban%2520scenarios.%2520Extensive%2520evaluation%2520on%2520major%2520driving%250Adatasets%2520demonstrates%2520the%2520superiority%2520of%2520our%2520approach%2520in%2520reconstructing%2520large%250Adriving%2520sequences%2520with%2520limited%2520image%2520overlap%252C%2520outperforming%2520concurrent%2520SoTA%250Amethods.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2501.03932v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=CoStruction%3A%20Conjoint%20radiance%20field%20optimization%20for%20urban%20scene%0A%20%20reconStruction%20with%20limited%20image%20overlap&entry.906535625=Fusang%20Wang%20and%20Hala%20Djeghim%20and%20Nathan%20Piasco%20and%20Moussab%20Bennehar%20and%20Luis%20Rold%C3%A3o%20and%20Dzmitry%20Tsishkou&entry.1292438233=%20%20Reconstructing%20the%20surrounding%20surface%20geometry%20from%20recorded%20driving%0Asequences%20poses%20a%20significant%20challenge%20due%20to%20the%20limited%20image%20overlap%20and%0Acomplex%20topology%20of%20urban%20environments.%20SoTA%20neural%20implicit%20surface%0Areconstruction%20methods%20often%20struggle%20in%20such%20setting%2C%20either%20failing%20due%20to%0Asmall%20vision%20overlap%20or%20exhibiting%20suboptimal%20performance%20in%20accurately%0Areconstructing%20both%20the%20surface%20and%20fine%20structures.%20To%20address%20these%0Alimitations%2C%20we%20introduce%20CoStruction%2C%20a%20novel%20hybrid%20implicit%20surface%0Areconstruction%20method%20tailored%20for%20large%20driving%20sequences%20with%20limited%20camera%0Aoverlap.%20CoStruction%20leverages%20cross-representation%20uncertainty%20estimation%20to%0Afilter%20out%20ambiguous%20geometry%20caused%20by%20limited%20observations.%20Our%20method%0Aperforms%20joint%20optimization%20of%20both%20radiance%20fields%20in%20addition%20to%20guided%0Asampling%20achieving%20accurate%20reconstruction%20of%20large%20areas%20along%20with%20fine%0Astructures%20in%20complex%20urban%20scenarios.%20Extensive%20evaluation%20on%20major%20driving%0Adatasets%20demonstrates%20the%20superiority%20of%20our%20approach%20in%20reconstructing%20large%0Adriving%20sequences%20with%20limited%20image%20overlap%2C%20outperforming%20concurrent%20SoTA%0Amethods.%0A&entry.1838667208=http%3A//arxiv.org/abs/2501.03932v1&entry.124074799=Read"},
{"title": "Wavelet-Driven Generalizable Framework for Deepfake Face Forgery\n  Detection", "author": "Lalith Bharadwaj Baru and Rohit Boddeda and Shilhora Akshay Patel and Sai Mohan Gajapaka", "abstract": "  The evolution of digital image manipulation, particularly with the\nadvancement of deep generative models, significantly challenges existing\ndeepfake detection methods, especially when the origin of the deepfake is\nobscure. To tackle the increasing complexity of these forgeries, we propose\n\\textbf{Wavelet-CLIP}, a deepfake detection framework that integrates wavelet\ntransforms with features derived from the ViT-L/14 architecture, pre-trained in\nthe CLIP fashion. Wavelet-CLIP utilizes Wavelet Transforms to deeply analyze\nboth spatial and frequency features from images, thus enhancing the model's\ncapability to detect sophisticated deepfakes. To verify the effectiveness of\nour approach, we conducted extensive evaluations against existing\nstate-of-the-art methods for cross-dataset generalization and detection of\nunseen images generated by standard diffusion models. Our method showcases\noutstanding performance, achieving an average AUC of 0.749 for cross-data\ngeneralization and 0.893 for robustness against unseen deepfakes, outperforming\nall compared methods. The code can be reproduced from the repo:\n\\url{https://github.com/lalithbharadwajbaru/Wavelet-CLIP}\n", "link": "http://arxiv.org/abs/2409.18301v3", "date": "2025-01-07", "relevancy": 2.1513, "topK": [{"title": "Magic-Me: Identity-Specific Video Customized Diffusion", "link": "http://arxiv.org/abs/2402.09368v1", "similarity": 0.5504}, {"title": "Tuning-Free Noise Rectification for High Fidelity Image-to-Video\n  Generation", "link": "http://arxiv.org/abs/2403.02827v1", "similarity": 0.534}, {"title": "Customize-A-Video: One-Shot Motion Customization of Text-to-Video\n  Diffusion Models", "link": "http://arxiv.org/abs/2402.14780v1", "similarity": 0.5158}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Wavelet-Driven%20Generalizable%20Framework%20for%20Deepfake%20Face%20Forgery%0A%20%20Detection&body=Title%3A%20Wavelet-Driven%20Generalizable%20Framework%20for%20Deepfake%20Face%20Forgery%0A%20%20Detection%0AAuthor%3A%20Lalith%20Bharadwaj%20Baru%20and%20Rohit%20Boddeda%20and%20Shilhora%20Akshay%20Patel%20and%20Sai%20Mohan%20Gajapaka%0AAbstract%3A%20%20%20The%20evolution%20of%20digital%20image%20manipulation%2C%20particularly%20with%20the%0Aadvancement%20of%20deep%20generative%20models%2C%20significantly%20challenges%20existing%0Adeepfake%20detection%20methods%2C%20especially%20when%20the%20origin%20of%20the%20deepfake%20is%0Aobscure.%20To%20tackle%20the%20increasing%20complexity%20of%20these%20forgeries%2C%20we%20propose%0A%5Ctextbf%7BWavelet-CLIP%7D%2C%20a%20deepfake%20detection%20framework%20that%20integrates%20wavelet%0Atransforms%20with%20features%20derived%20from%20the%20ViT-L/14%20architecture%2C%20pre-trained%20in%0Athe%20CLIP%20fashion.%20Wavelet-CLIP%20utilizes%20Wavelet%20Transforms%20to%20deeply%20analyze%0Aboth%20spatial%20and%20frequency%20features%20from%20images%2C%20thus%20enhancing%20the%20model%27s%0Acapability%20to%20detect%20sophisticated%20deepfakes.%20To%20verify%20the%20effectiveness%20of%0Aour%20approach%2C%20we%20conducted%20extensive%20evaluations%20against%20existing%0Astate-of-the-art%20methods%20for%20cross-dataset%20generalization%20and%20detection%20of%0Aunseen%20images%20generated%20by%20standard%20diffusion%20models.%20Our%20method%20showcases%0Aoutstanding%20performance%2C%20achieving%20an%20average%20AUC%20of%200.749%20for%20cross-data%0Ageneralization%20and%200.893%20for%20robustness%20against%20unseen%20deepfakes%2C%20outperforming%0Aall%20compared%20methods.%20The%20code%20can%20be%20reproduced%20from%20the%20repo%3A%0A%5Curl%7Bhttps%3A//github.com/lalithbharadwajbaru/Wavelet-CLIP%7D%0A%0ALink%3A%20http%3A//arxiv.org/abs/2409.18301v3%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DWavelet-Driven%2520Generalizable%2520Framework%2520for%2520Deepfake%2520Face%2520Forgery%250A%2520%2520Detection%26entry.906535625%3DLalith%2520Bharadwaj%2520Baru%2520and%2520Rohit%2520Boddeda%2520and%2520Shilhora%2520Akshay%2520Patel%2520and%2520Sai%2520Mohan%2520Gajapaka%26entry.1292438233%3D%2520%2520The%2520evolution%2520of%2520digital%2520image%2520manipulation%252C%2520particularly%2520with%2520the%250Aadvancement%2520of%2520deep%2520generative%2520models%252C%2520significantly%2520challenges%2520existing%250Adeepfake%2520detection%2520methods%252C%2520especially%2520when%2520the%2520origin%2520of%2520the%2520deepfake%2520is%250Aobscure.%2520To%2520tackle%2520the%2520increasing%2520complexity%2520of%2520these%2520forgeries%252C%2520we%2520propose%250A%255Ctextbf%257BWavelet-CLIP%257D%252C%2520a%2520deepfake%2520detection%2520framework%2520that%2520integrates%2520wavelet%250Atransforms%2520with%2520features%2520derived%2520from%2520the%2520ViT-L/14%2520architecture%252C%2520pre-trained%2520in%250Athe%2520CLIP%2520fashion.%2520Wavelet-CLIP%2520utilizes%2520Wavelet%2520Transforms%2520to%2520deeply%2520analyze%250Aboth%2520spatial%2520and%2520frequency%2520features%2520from%2520images%252C%2520thus%2520enhancing%2520the%2520model%2527s%250Acapability%2520to%2520detect%2520sophisticated%2520deepfakes.%2520To%2520verify%2520the%2520effectiveness%2520of%250Aour%2520approach%252C%2520we%2520conducted%2520extensive%2520evaluations%2520against%2520existing%250Astate-of-the-art%2520methods%2520for%2520cross-dataset%2520generalization%2520and%2520detection%2520of%250Aunseen%2520images%2520generated%2520by%2520standard%2520diffusion%2520models.%2520Our%2520method%2520showcases%250Aoutstanding%2520performance%252C%2520achieving%2520an%2520average%2520AUC%2520of%25200.749%2520for%2520cross-data%250Ageneralization%2520and%25200.893%2520for%2520robustness%2520against%2520unseen%2520deepfakes%252C%2520outperforming%250Aall%2520compared%2520methods.%2520The%2520code%2520can%2520be%2520reproduced%2520from%2520the%2520repo%253A%250A%255Curl%257Bhttps%253A//github.com/lalithbharadwajbaru/Wavelet-CLIP%257D%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2409.18301v3%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Wavelet-Driven%20Generalizable%20Framework%20for%20Deepfake%20Face%20Forgery%0A%20%20Detection&entry.906535625=Lalith%20Bharadwaj%20Baru%20and%20Rohit%20Boddeda%20and%20Shilhora%20Akshay%20Patel%20and%20Sai%20Mohan%20Gajapaka&entry.1292438233=%20%20The%20evolution%20of%20digital%20image%20manipulation%2C%20particularly%20with%20the%0Aadvancement%20of%20deep%20generative%20models%2C%20significantly%20challenges%20existing%0Adeepfake%20detection%20methods%2C%20especially%20when%20the%20origin%20of%20the%20deepfake%20is%0Aobscure.%20To%20tackle%20the%20increasing%20complexity%20of%20these%20forgeries%2C%20we%20propose%0A%5Ctextbf%7BWavelet-CLIP%7D%2C%20a%20deepfake%20detection%20framework%20that%20integrates%20wavelet%0Atransforms%20with%20features%20derived%20from%20the%20ViT-L/14%20architecture%2C%20pre-trained%20in%0Athe%20CLIP%20fashion.%20Wavelet-CLIP%20utilizes%20Wavelet%20Transforms%20to%20deeply%20analyze%0Aboth%20spatial%20and%20frequency%20features%20from%20images%2C%20thus%20enhancing%20the%20model%27s%0Acapability%20to%20detect%20sophisticated%20deepfakes.%20To%20verify%20the%20effectiveness%20of%0Aour%20approach%2C%20we%20conducted%20extensive%20evaluations%20against%20existing%0Astate-of-the-art%20methods%20for%20cross-dataset%20generalization%20and%20detection%20of%0Aunseen%20images%20generated%20by%20standard%20diffusion%20models.%20Our%20method%20showcases%0Aoutstanding%20performance%2C%20achieving%20an%20average%20AUC%20of%200.749%20for%20cross-data%0Ageneralization%20and%200.893%20for%20robustness%20against%20unseen%20deepfakes%2C%20outperforming%0Aall%20compared%20methods.%20The%20code%20can%20be%20reproduced%20from%20the%20repo%3A%0A%5Curl%7Bhttps%3A//github.com/lalithbharadwajbaru/Wavelet-CLIP%7D%0A&entry.1838667208=http%3A//arxiv.org/abs/2409.18301v3&entry.124074799=Read"},
{"title": "NeuralSVG: An Implicit Representation for Text-to-Vector Generation", "author": "Sagi Polaczek and Yuval Alaluf and Elad Richardson and Yael Vinker and Daniel Cohen-Or", "abstract": "  Vector graphics are essential in design, providing artists with a versatile\nmedium for creating resolution-independent and highly editable visual content.\nRecent advancements in vision-language and diffusion models have fueled\ninterest in text-to-vector graphics generation. However, existing approaches\noften suffer from over-parameterized outputs or treat the layered structure - a\ncore feature of vector graphics - as a secondary goal, diminishing their\npractical use. Recognizing the importance of layered SVG representations, we\npropose NeuralSVG, an implicit neural representation for generating vector\ngraphics from text prompts. Inspired by Neural Radiance Fields (NeRFs),\nNeuralSVG encodes the entire scene into the weights of a small MLP network,\noptimized using Score Distillation Sampling (SDS). To encourage a layered\nstructure in the generated SVG, we introduce a dropout-based regularization\ntechnique that strengthens the standalone meaning of each shape. We\nadditionally demonstrate that utilizing a neural representation provides an\nadded benefit of inference-time control, enabling users to dynamically adapt\nthe generated SVG based on user-provided inputs, all with a single learned\nrepresentation. Through extensive qualitative and quantitative evaluations, we\ndemonstrate that NeuralSVG outperforms existing methods in generating\nstructured and flexible SVG.\n", "link": "http://arxiv.org/abs/2501.03992v1", "date": "2025-01-07", "relevancy": 2.1338, "topK": [{"title": "Scaling Rectified Flow Transformers for High-Resolution Image Synthesis", "link": "http://arxiv.org/abs/2403.03206v1", "similarity": 0.5688}, {"title": "MiraGe: Editable 2D Images using Gaussian Splatting", "link": "http://arxiv.org/abs/2410.01521v1", "similarity": 0.5299}, {"title": "Magic-Me: Identity-Specific Video Customized Diffusion", "link": "http://arxiv.org/abs/2402.09368v1", "similarity": 0.5228}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20NeuralSVG%3A%20An%20Implicit%20Representation%20for%20Text-to-Vector%20Generation&body=Title%3A%20NeuralSVG%3A%20An%20Implicit%20Representation%20for%20Text-to-Vector%20Generation%0AAuthor%3A%20Sagi%20Polaczek%20and%20Yuval%20Alaluf%20and%20Elad%20Richardson%20and%20Yael%20Vinker%20and%20Daniel%20Cohen-Or%0AAbstract%3A%20%20%20Vector%20graphics%20are%20essential%20in%20design%2C%20providing%20artists%20with%20a%20versatile%0Amedium%20for%20creating%20resolution-independent%20and%20highly%20editable%20visual%20content.%0ARecent%20advancements%20in%20vision-language%20and%20diffusion%20models%20have%20fueled%0Ainterest%20in%20text-to-vector%20graphics%20generation.%20However%2C%20existing%20approaches%0Aoften%20suffer%20from%20over-parameterized%20outputs%20or%20treat%20the%20layered%20structure%20-%20a%0Acore%20feature%20of%20vector%20graphics%20-%20as%20a%20secondary%20goal%2C%20diminishing%20their%0Apractical%20use.%20Recognizing%20the%20importance%20of%20layered%20SVG%20representations%2C%20we%0Apropose%20NeuralSVG%2C%20an%20implicit%20neural%20representation%20for%20generating%20vector%0Agraphics%20from%20text%20prompts.%20Inspired%20by%20Neural%20Radiance%20Fields%20%28NeRFs%29%2C%0ANeuralSVG%20encodes%20the%20entire%20scene%20into%20the%20weights%20of%20a%20small%20MLP%20network%2C%0Aoptimized%20using%20Score%20Distillation%20Sampling%20%28SDS%29.%20To%20encourage%20a%20layered%0Astructure%20in%20the%20generated%20SVG%2C%20we%20introduce%20a%20dropout-based%20regularization%0Atechnique%20that%20strengthens%20the%20standalone%20meaning%20of%20each%20shape.%20We%0Aadditionally%20demonstrate%20that%20utilizing%20a%20neural%20representation%20provides%20an%0Aadded%20benefit%20of%20inference-time%20control%2C%20enabling%20users%20to%20dynamically%20adapt%0Athe%20generated%20SVG%20based%20on%20user-provided%20inputs%2C%20all%20with%20a%20single%20learned%0Arepresentation.%20Through%20extensive%20qualitative%20and%20quantitative%20evaluations%2C%20we%0Ademonstrate%20that%20NeuralSVG%20outperforms%20existing%20methods%20in%20generating%0Astructured%20and%20flexible%20SVG.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2501.03992v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DNeuralSVG%253A%2520An%2520Implicit%2520Representation%2520for%2520Text-to-Vector%2520Generation%26entry.906535625%3DSagi%2520Polaczek%2520and%2520Yuval%2520Alaluf%2520and%2520Elad%2520Richardson%2520and%2520Yael%2520Vinker%2520and%2520Daniel%2520Cohen-Or%26entry.1292438233%3D%2520%2520Vector%2520graphics%2520are%2520essential%2520in%2520design%252C%2520providing%2520artists%2520with%2520a%2520versatile%250Amedium%2520for%2520creating%2520resolution-independent%2520and%2520highly%2520editable%2520visual%2520content.%250ARecent%2520advancements%2520in%2520vision-language%2520and%2520diffusion%2520models%2520have%2520fueled%250Ainterest%2520in%2520text-to-vector%2520graphics%2520generation.%2520However%252C%2520existing%2520approaches%250Aoften%2520suffer%2520from%2520over-parameterized%2520outputs%2520or%2520treat%2520the%2520layered%2520structure%2520-%2520a%250Acore%2520feature%2520of%2520vector%2520graphics%2520-%2520as%2520a%2520secondary%2520goal%252C%2520diminishing%2520their%250Apractical%2520use.%2520Recognizing%2520the%2520importance%2520of%2520layered%2520SVG%2520representations%252C%2520we%250Apropose%2520NeuralSVG%252C%2520an%2520implicit%2520neural%2520representation%2520for%2520generating%2520vector%250Agraphics%2520from%2520text%2520prompts.%2520Inspired%2520by%2520Neural%2520Radiance%2520Fields%2520%2528NeRFs%2529%252C%250ANeuralSVG%2520encodes%2520the%2520entire%2520scene%2520into%2520the%2520weights%2520of%2520a%2520small%2520MLP%2520network%252C%250Aoptimized%2520using%2520Score%2520Distillation%2520Sampling%2520%2528SDS%2529.%2520To%2520encourage%2520a%2520layered%250Astructure%2520in%2520the%2520generated%2520SVG%252C%2520we%2520introduce%2520a%2520dropout-based%2520regularization%250Atechnique%2520that%2520strengthens%2520the%2520standalone%2520meaning%2520of%2520each%2520shape.%2520We%250Aadditionally%2520demonstrate%2520that%2520utilizing%2520a%2520neural%2520representation%2520provides%2520an%250Aadded%2520benefit%2520of%2520inference-time%2520control%252C%2520enabling%2520users%2520to%2520dynamically%2520adapt%250Athe%2520generated%2520SVG%2520based%2520on%2520user-provided%2520inputs%252C%2520all%2520with%2520a%2520single%2520learned%250Arepresentation.%2520Through%2520extensive%2520qualitative%2520and%2520quantitative%2520evaluations%252C%2520we%250Ademonstrate%2520that%2520NeuralSVG%2520outperforms%2520existing%2520methods%2520in%2520generating%250Astructured%2520and%2520flexible%2520SVG.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2501.03992v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=NeuralSVG%3A%20An%20Implicit%20Representation%20for%20Text-to-Vector%20Generation&entry.906535625=Sagi%20Polaczek%20and%20Yuval%20Alaluf%20and%20Elad%20Richardson%20and%20Yael%20Vinker%20and%20Daniel%20Cohen-Or&entry.1292438233=%20%20Vector%20graphics%20are%20essential%20in%20design%2C%20providing%20artists%20with%20a%20versatile%0Amedium%20for%20creating%20resolution-independent%20and%20highly%20editable%20visual%20content.%0ARecent%20advancements%20in%20vision-language%20and%20diffusion%20models%20have%20fueled%0Ainterest%20in%20text-to-vector%20graphics%20generation.%20However%2C%20existing%20approaches%0Aoften%20suffer%20from%20over-parameterized%20outputs%20or%20treat%20the%20layered%20structure%20-%20a%0Acore%20feature%20of%20vector%20graphics%20-%20as%20a%20secondary%20goal%2C%20diminishing%20their%0Apractical%20use.%20Recognizing%20the%20importance%20of%20layered%20SVG%20representations%2C%20we%0Apropose%20NeuralSVG%2C%20an%20implicit%20neural%20representation%20for%20generating%20vector%0Agraphics%20from%20text%20prompts.%20Inspired%20by%20Neural%20Radiance%20Fields%20%28NeRFs%29%2C%0ANeuralSVG%20encodes%20the%20entire%20scene%20into%20the%20weights%20of%20a%20small%20MLP%20network%2C%0Aoptimized%20using%20Score%20Distillation%20Sampling%20%28SDS%29.%20To%20encourage%20a%20layered%0Astructure%20in%20the%20generated%20SVG%2C%20we%20introduce%20a%20dropout-based%20regularization%0Atechnique%20that%20strengthens%20the%20standalone%20meaning%20of%20each%20shape.%20We%0Aadditionally%20demonstrate%20that%20utilizing%20a%20neural%20representation%20provides%20an%0Aadded%20benefit%20of%20inference-time%20control%2C%20enabling%20users%20to%20dynamically%20adapt%0Athe%20generated%20SVG%20based%20on%20user-provided%20inputs%2C%20all%20with%20a%20single%20learned%0Arepresentation.%20Through%20extensive%20qualitative%20and%20quantitative%20evaluations%2C%20we%0Ademonstrate%20that%20NeuralSVG%20outperforms%20existing%20methods%20in%20generating%0Astructured%20and%20flexible%20SVG.%0A&entry.1838667208=http%3A//arxiv.org/abs/2501.03992v1&entry.124074799=Read"},
{"title": "Local Compositional Complexity: How to Detect a Human-readable Messsage", "author": "Louis Mahon", "abstract": "  Data complexity is an important concept in the natural sciences and related\nareas, but lacks a rigorous and computable definition. In this paper, we focus\non a particular sense of complexity that is high if the data is structured in a\nway that could serve to communicate a message. In this sense, human speech,\nwritten language, drawings, diagrams and photographs are high complexity,\nwhereas data that is close to uniform throughout or populated by random values\nis low complexity. We describe a general framework for measuring data\ncomplexity based on dividing the shortest description of the data into a\nstructured and an unstructured portion, and taking the size of the former as\nthe complexity score. We outline an application of this framework in\nstatistical mechanics that may allow a more objective characterisation of the\nmacrostate and entropy of a physical system. Then, we derive a more precise and\ncomputable definition geared towards human communication, by proposing local\ncompositionality as an appropriate specific structure. We demonstrate\nexperimentally that this method can distinguish meaningful signals from noise\nor repetitive signals in auditory, visual and text domains, and could\npotentially help determine whether an extra-terrestrial signal contained a\nmessage.\n", "link": "http://arxiv.org/abs/2501.03664v1", "date": "2025-01-07", "relevancy": 2.1295, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.4338}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.4338}, {"title": "Self-supervised Photographic Image Layout Representation Learning", "link": "http://arxiv.org/abs/2403.03740v1", "similarity": 0.4101}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Local%20Compositional%20Complexity%3A%20How%20to%20Detect%20a%20Human-readable%20Messsage&body=Title%3A%20Local%20Compositional%20Complexity%3A%20How%20to%20Detect%20a%20Human-readable%20Messsage%0AAuthor%3A%20Louis%20Mahon%0AAbstract%3A%20%20%20Data%20complexity%20is%20an%20important%20concept%20in%20the%20natural%20sciences%20and%20related%0Aareas%2C%20but%20lacks%20a%20rigorous%20and%20computable%20definition.%20In%20this%20paper%2C%20we%20focus%0Aon%20a%20particular%20sense%20of%20complexity%20that%20is%20high%20if%20the%20data%20is%20structured%20in%20a%0Away%20that%20could%20serve%20to%20communicate%20a%20message.%20In%20this%20sense%2C%20human%20speech%2C%0Awritten%20language%2C%20drawings%2C%20diagrams%20and%20photographs%20are%20high%20complexity%2C%0Awhereas%20data%20that%20is%20close%20to%20uniform%20throughout%20or%20populated%20by%20random%20values%0Ais%20low%20complexity.%20We%20describe%20a%20general%20framework%20for%20measuring%20data%0Acomplexity%20based%20on%20dividing%20the%20shortest%20description%20of%20the%20data%20into%20a%0Astructured%20and%20an%20unstructured%20portion%2C%20and%20taking%20the%20size%20of%20the%20former%20as%0Athe%20complexity%20score.%20We%20outline%20an%20application%20of%20this%20framework%20in%0Astatistical%20mechanics%20that%20may%20allow%20a%20more%20objective%20characterisation%20of%20the%0Amacrostate%20and%20entropy%20of%20a%20physical%20system.%20Then%2C%20we%20derive%20a%20more%20precise%20and%0Acomputable%20definition%20geared%20towards%20human%20communication%2C%20by%20proposing%20local%0Acompositionality%20as%20an%20appropriate%20specific%20structure.%20We%20demonstrate%0Aexperimentally%20that%20this%20method%20can%20distinguish%20meaningful%20signals%20from%20noise%0Aor%20repetitive%20signals%20in%20auditory%2C%20visual%20and%20text%20domains%2C%20and%20could%0Apotentially%20help%20determine%20whether%20an%20extra-terrestrial%20signal%20contained%20a%0Amessage.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2501.03664v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DLocal%2520Compositional%2520Complexity%253A%2520How%2520to%2520Detect%2520a%2520Human-readable%2520Messsage%26entry.906535625%3DLouis%2520Mahon%26entry.1292438233%3D%2520%2520Data%2520complexity%2520is%2520an%2520important%2520concept%2520in%2520the%2520natural%2520sciences%2520and%2520related%250Aareas%252C%2520but%2520lacks%2520a%2520rigorous%2520and%2520computable%2520definition.%2520In%2520this%2520paper%252C%2520we%2520focus%250Aon%2520a%2520particular%2520sense%2520of%2520complexity%2520that%2520is%2520high%2520if%2520the%2520data%2520is%2520structured%2520in%2520a%250Away%2520that%2520could%2520serve%2520to%2520communicate%2520a%2520message.%2520In%2520this%2520sense%252C%2520human%2520speech%252C%250Awritten%2520language%252C%2520drawings%252C%2520diagrams%2520and%2520photographs%2520are%2520high%2520complexity%252C%250Awhereas%2520data%2520that%2520is%2520close%2520to%2520uniform%2520throughout%2520or%2520populated%2520by%2520random%2520values%250Ais%2520low%2520complexity.%2520We%2520describe%2520a%2520general%2520framework%2520for%2520measuring%2520data%250Acomplexity%2520based%2520on%2520dividing%2520the%2520shortest%2520description%2520of%2520the%2520data%2520into%2520a%250Astructured%2520and%2520an%2520unstructured%2520portion%252C%2520and%2520taking%2520the%2520size%2520of%2520the%2520former%2520as%250Athe%2520complexity%2520score.%2520We%2520outline%2520an%2520application%2520of%2520this%2520framework%2520in%250Astatistical%2520mechanics%2520that%2520may%2520allow%2520a%2520more%2520objective%2520characterisation%2520of%2520the%250Amacrostate%2520and%2520entropy%2520of%2520a%2520physical%2520system.%2520Then%252C%2520we%2520derive%2520a%2520more%2520precise%2520and%250Acomputable%2520definition%2520geared%2520towards%2520human%2520communication%252C%2520by%2520proposing%2520local%250Acompositionality%2520as%2520an%2520appropriate%2520specific%2520structure.%2520We%2520demonstrate%250Aexperimentally%2520that%2520this%2520method%2520can%2520distinguish%2520meaningful%2520signals%2520from%2520noise%250Aor%2520repetitive%2520signals%2520in%2520auditory%252C%2520visual%2520and%2520text%2520domains%252C%2520and%2520could%250Apotentially%2520help%2520determine%2520whether%2520an%2520extra-terrestrial%2520signal%2520contained%2520a%250Amessage.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2501.03664v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Local%20Compositional%20Complexity%3A%20How%20to%20Detect%20a%20Human-readable%20Messsage&entry.906535625=Louis%20Mahon&entry.1292438233=%20%20Data%20complexity%20is%20an%20important%20concept%20in%20the%20natural%20sciences%20and%20related%0Aareas%2C%20but%20lacks%20a%20rigorous%20and%20computable%20definition.%20In%20this%20paper%2C%20we%20focus%0Aon%20a%20particular%20sense%20of%20complexity%20that%20is%20high%20if%20the%20data%20is%20structured%20in%20a%0Away%20that%20could%20serve%20to%20communicate%20a%20message.%20In%20this%20sense%2C%20human%20speech%2C%0Awritten%20language%2C%20drawings%2C%20diagrams%20and%20photographs%20are%20high%20complexity%2C%0Awhereas%20data%20that%20is%20close%20to%20uniform%20throughout%20or%20populated%20by%20random%20values%0Ais%20low%20complexity.%20We%20describe%20a%20general%20framework%20for%20measuring%20data%0Acomplexity%20based%20on%20dividing%20the%20shortest%20description%20of%20the%20data%20into%20a%0Astructured%20and%20an%20unstructured%20portion%2C%20and%20taking%20the%20size%20of%20the%20former%20as%0Athe%20complexity%20score.%20We%20outline%20an%20application%20of%20this%20framework%20in%0Astatistical%20mechanics%20that%20may%20allow%20a%20more%20objective%20characterisation%20of%20the%0Amacrostate%20and%20entropy%20of%20a%20physical%20system.%20Then%2C%20we%20derive%20a%20more%20precise%20and%0Acomputable%20definition%20geared%20towards%20human%20communication%2C%20by%20proposing%20local%0Acompositionality%20as%20an%20appropriate%20specific%20structure.%20We%20demonstrate%0Aexperimentally%20that%20this%20method%20can%20distinguish%20meaningful%20signals%20from%20noise%0Aor%20repetitive%20signals%20in%20auditory%2C%20visual%20and%20text%20domains%2C%20and%20could%0Apotentially%20help%20determine%20whether%20an%20extra-terrestrial%20signal%20contained%20a%0Amessage.%0A&entry.1838667208=http%3A//arxiv.org/abs/2501.03664v1&entry.124074799=Read"},
{"title": "Semise: Semi-supervised learning for severity representation in medical\n  image", "author": "Dung T. Tran and Hung Vu and Anh Tran and Hieu Pham and Hong Nguyen and Phong Nguyen", "abstract": "  This paper introduces SEMISE, a novel method for representation learning in\nmedical imaging that combines self-supervised and supervised learning. By\nleveraging both labeled and augmented data, SEMISE addresses the challenge of\ndata scarcity and enhances the encoder's ability to extract meaningful\nfeatures. This integrated approach leads to more informative representations,\nimproving performance on downstream tasks. As result, our approach achieved a\n12% improvement in classification and a 3% improvement in segmentation,\noutperforming existing methods. These results demonstrate the potential of\nSIMESE to advance medical image analysis and offer more accurate solutions for\nhealthcare applications, particularly in contexts where labeled data is\nlimited.\n", "link": "http://arxiv.org/abs/2501.03848v1", "date": "2025-01-07", "relevancy": 2.1288, "topK": [{"title": "Self-supervised Photographic Image Layout Representation Learning", "link": "http://arxiv.org/abs/2403.03740v1", "similarity": 0.5482}, {"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.5339}, {"title": "IAE: Implicit Autoencoder for Point Cloud Self-supervised Representation Learning", "link": "https://arxiv.org/abs/2201.00785", "similarity": 0.5155}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Semise%3A%20Semi-supervised%20learning%20for%20severity%20representation%20in%20medical%0A%20%20image&body=Title%3A%20Semise%3A%20Semi-supervised%20learning%20for%20severity%20representation%20in%20medical%0A%20%20image%0AAuthor%3A%20Dung%20T.%20Tran%20and%20Hung%20Vu%20and%20Anh%20Tran%20and%20Hieu%20Pham%20and%20Hong%20Nguyen%20and%20Phong%20Nguyen%0AAbstract%3A%20%20%20This%20paper%20introduces%20SEMISE%2C%20a%20novel%20method%20for%20representation%20learning%20in%0Amedical%20imaging%20that%20combines%20self-supervised%20and%20supervised%20learning.%20By%0Aleveraging%20both%20labeled%20and%20augmented%20data%2C%20SEMISE%20addresses%20the%20challenge%20of%0Adata%20scarcity%20and%20enhances%20the%20encoder%27s%20ability%20to%20extract%20meaningful%0Afeatures.%20This%20integrated%20approach%20leads%20to%20more%20informative%20representations%2C%0Aimproving%20performance%20on%20downstream%20tasks.%20As%20result%2C%20our%20approach%20achieved%20a%0A12%25%20improvement%20in%20classification%20and%20a%203%25%20improvement%20in%20segmentation%2C%0Aoutperforming%20existing%20methods.%20These%20results%20demonstrate%20the%20potential%20of%0ASIMESE%20to%20advance%20medical%20image%20analysis%20and%20offer%20more%20accurate%20solutions%20for%0Ahealthcare%20applications%2C%20particularly%20in%20contexts%20where%20labeled%20data%20is%0Alimited.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2501.03848v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DSemise%253A%2520Semi-supervised%2520learning%2520for%2520severity%2520representation%2520in%2520medical%250A%2520%2520image%26entry.906535625%3DDung%2520T.%2520Tran%2520and%2520Hung%2520Vu%2520and%2520Anh%2520Tran%2520and%2520Hieu%2520Pham%2520and%2520Hong%2520Nguyen%2520and%2520Phong%2520Nguyen%26entry.1292438233%3D%2520%2520This%2520paper%2520introduces%2520SEMISE%252C%2520a%2520novel%2520method%2520for%2520representation%2520learning%2520in%250Amedical%2520imaging%2520that%2520combines%2520self-supervised%2520and%2520supervised%2520learning.%2520By%250Aleveraging%2520both%2520labeled%2520and%2520augmented%2520data%252C%2520SEMISE%2520addresses%2520the%2520challenge%2520of%250Adata%2520scarcity%2520and%2520enhances%2520the%2520encoder%2527s%2520ability%2520to%2520extract%2520meaningful%250Afeatures.%2520This%2520integrated%2520approach%2520leads%2520to%2520more%2520informative%2520representations%252C%250Aimproving%2520performance%2520on%2520downstream%2520tasks.%2520As%2520result%252C%2520our%2520approach%2520achieved%2520a%250A12%2525%2520improvement%2520in%2520classification%2520and%2520a%25203%2525%2520improvement%2520in%2520segmentation%252C%250Aoutperforming%2520existing%2520methods.%2520These%2520results%2520demonstrate%2520the%2520potential%2520of%250ASIMESE%2520to%2520advance%2520medical%2520image%2520analysis%2520and%2520offer%2520more%2520accurate%2520solutions%2520for%250Ahealthcare%2520applications%252C%2520particularly%2520in%2520contexts%2520where%2520labeled%2520data%2520is%250Alimited.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2501.03848v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Semise%3A%20Semi-supervised%20learning%20for%20severity%20representation%20in%20medical%0A%20%20image&entry.906535625=Dung%20T.%20Tran%20and%20Hung%20Vu%20and%20Anh%20Tran%20and%20Hieu%20Pham%20and%20Hong%20Nguyen%20and%20Phong%20Nguyen&entry.1292438233=%20%20This%20paper%20introduces%20SEMISE%2C%20a%20novel%20method%20for%20representation%20learning%20in%0Amedical%20imaging%20that%20combines%20self-supervised%20and%20supervised%20learning.%20By%0Aleveraging%20both%20labeled%20and%20augmented%20data%2C%20SEMISE%20addresses%20the%20challenge%20of%0Adata%20scarcity%20and%20enhances%20the%20encoder%27s%20ability%20to%20extract%20meaningful%0Afeatures.%20This%20integrated%20approach%20leads%20to%20more%20informative%20representations%2C%0Aimproving%20performance%20on%20downstream%20tasks.%20As%20result%2C%20our%20approach%20achieved%20a%0A12%25%20improvement%20in%20classification%20and%20a%203%25%20improvement%20in%20segmentation%2C%0Aoutperforming%20existing%20methods.%20These%20results%20demonstrate%20the%20potential%20of%0ASIMESE%20to%20advance%20medical%20image%20analysis%20and%20offer%20more%20accurate%20solutions%20for%0Ahealthcare%20applications%2C%20particularly%20in%20contexts%20where%20labeled%20data%20is%0Alimited.%0A&entry.1838667208=http%3A//arxiv.org/abs/2501.03848v1&entry.124074799=Read"},
{"title": "Detecting Neurocognitive Disorders through Analyses of Topic Evolution\n  and Cross-modal Consistency in Visual-Stimulated Narratives", "author": "Jinchao Li and Yuejiao Wang and Junan Li and Jiawen Kang and Bo Zheng and Simon Wong and Brian Mak and Helene Fung and Jean Woo and Man-Wai Mak and Timothy Kwok and Vincent Mok and Xianmin Gong and Xixin Wu and Xunying Liu and Patrick Wong and Helen Meng", "abstract": "  Early detection of neurocognitive disorders (NCDs) is crucial for timely\nintervention and disease management. Speech analysis offers a non-intrusive and\nscalable screening method, particularly through narrative tasks in\nneuropsychological assessment tools. Traditional narrative analysis often\nfocuses on local indicators in microstructure, such as word usage and syntax.\nWhile these features provide insights into language production abilities, they\noften fail to capture global narrative patterns, or microstructures.\nMacrostructures include coherence, thematic organization, and logical\nprogressions, reflecting essential cognitive skills potentially critical for\nrecognizing NCDs. Addressing this gap, we propose to investigate specific\ncognitive and linguistic challenges by analyzing topical shifts, temporal\ndynamics, and the coherence of narratives over time, aiming to reveal cognitive\ndeficits by identifying narrative impairments, and exploring their impact on\ncommunication and cognition. The investigation is based on the CU-MARVEL Rabbit\nStory corpus, which comprises recordings of a story-telling task from 758 older\nadults. We developed two approaches: the Dynamic Topic Models (DTM)-based\ntemporal analysis to examine the evolution of topics over time, and the\nText-Image Temporal Alignment Network (TITAN) to evaluate the coherence between\nspoken narratives and visual stimuli. DTM-based approach validated the\neffectiveness of dynamic topic consistency as a macrostructural metric\n(F1=0.61, AUC=0.78). The TITAN approach achieved the highest performance\n(F1=0.72, AUC=0.81), surpassing established microstructural and macrostructural\nfeature sets. Cross-comparison and regression tasks further demonstrated the\neffectiveness of proposed dynamic macrostructural modeling approaches for NCD\ndetection.\n", "link": "http://arxiv.org/abs/2501.03727v1", "date": "2025-01-07", "relevancy": 2.1085, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5326}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5326}, {"title": "Multimodal-Conditioned Latent Diffusion Models for Fashion Image Editing", "link": "http://arxiv.org/abs/2403.14828v2", "similarity": 0.4996}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Detecting%20Neurocognitive%20Disorders%20through%20Analyses%20of%20Topic%20Evolution%0A%20%20and%20Cross-modal%20Consistency%20in%20Visual-Stimulated%20Narratives&body=Title%3A%20Detecting%20Neurocognitive%20Disorders%20through%20Analyses%20of%20Topic%20Evolution%0A%20%20and%20Cross-modal%20Consistency%20in%20Visual-Stimulated%20Narratives%0AAuthor%3A%20Jinchao%20Li%20and%20Yuejiao%20Wang%20and%20Junan%20Li%20and%20Jiawen%20Kang%20and%20Bo%20Zheng%20and%20Simon%20Wong%20and%20Brian%20Mak%20and%20Helene%20Fung%20and%20Jean%20Woo%20and%20Man-Wai%20Mak%20and%20Timothy%20Kwok%20and%20Vincent%20Mok%20and%20Xianmin%20Gong%20and%20Xixin%20Wu%20and%20Xunying%20Liu%20and%20Patrick%20Wong%20and%20Helen%20Meng%0AAbstract%3A%20%20%20Early%20detection%20of%20neurocognitive%20disorders%20%28NCDs%29%20is%20crucial%20for%20timely%0Aintervention%20and%20disease%20management.%20Speech%20analysis%20offers%20a%20non-intrusive%20and%0Ascalable%20screening%20method%2C%20particularly%20through%20narrative%20tasks%20in%0Aneuropsychological%20assessment%20tools.%20Traditional%20narrative%20analysis%20often%0Afocuses%20on%20local%20indicators%20in%20microstructure%2C%20such%20as%20word%20usage%20and%20syntax.%0AWhile%20these%20features%20provide%20insights%20into%20language%20production%20abilities%2C%20they%0Aoften%20fail%20to%20capture%20global%20narrative%20patterns%2C%20or%20microstructures.%0AMacrostructures%20include%20coherence%2C%20thematic%20organization%2C%20and%20logical%0Aprogressions%2C%20reflecting%20essential%20cognitive%20skills%20potentially%20critical%20for%0Arecognizing%20NCDs.%20Addressing%20this%20gap%2C%20we%20propose%20to%20investigate%20specific%0Acognitive%20and%20linguistic%20challenges%20by%20analyzing%20topical%20shifts%2C%20temporal%0Adynamics%2C%20and%20the%20coherence%20of%20narratives%20over%20time%2C%20aiming%20to%20reveal%20cognitive%0Adeficits%20by%20identifying%20narrative%20impairments%2C%20and%20exploring%20their%20impact%20on%0Acommunication%20and%20cognition.%20The%20investigation%20is%20based%20on%20the%20CU-MARVEL%20Rabbit%0AStory%20corpus%2C%20which%20comprises%20recordings%20of%20a%20story-telling%20task%20from%20758%20older%0Aadults.%20We%20developed%20two%20approaches%3A%20the%20Dynamic%20Topic%20Models%20%28DTM%29-based%0Atemporal%20analysis%20to%20examine%20the%20evolution%20of%20topics%20over%20time%2C%20and%20the%0AText-Image%20Temporal%20Alignment%20Network%20%28TITAN%29%20to%20evaluate%20the%20coherence%20between%0Aspoken%20narratives%20and%20visual%20stimuli.%20DTM-based%20approach%20validated%20the%0Aeffectiveness%20of%20dynamic%20topic%20consistency%20as%20a%20macrostructural%20metric%0A%28F1%3D0.61%2C%20AUC%3D0.78%29.%20The%20TITAN%20approach%20achieved%20the%20highest%20performance%0A%28F1%3D0.72%2C%20AUC%3D0.81%29%2C%20surpassing%20established%20microstructural%20and%20macrostructural%0Afeature%20sets.%20Cross-comparison%20and%20regression%20tasks%20further%20demonstrated%20the%0Aeffectiveness%20of%20proposed%20dynamic%20macrostructural%20modeling%20approaches%20for%20NCD%0Adetection.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2501.03727v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DDetecting%2520Neurocognitive%2520Disorders%2520through%2520Analyses%2520of%2520Topic%2520Evolution%250A%2520%2520and%2520Cross-modal%2520Consistency%2520in%2520Visual-Stimulated%2520Narratives%26entry.906535625%3DJinchao%2520Li%2520and%2520Yuejiao%2520Wang%2520and%2520Junan%2520Li%2520and%2520Jiawen%2520Kang%2520and%2520Bo%2520Zheng%2520and%2520Simon%2520Wong%2520and%2520Brian%2520Mak%2520and%2520Helene%2520Fung%2520and%2520Jean%2520Woo%2520and%2520Man-Wai%2520Mak%2520and%2520Timothy%2520Kwok%2520and%2520Vincent%2520Mok%2520and%2520Xianmin%2520Gong%2520and%2520Xixin%2520Wu%2520and%2520Xunying%2520Liu%2520and%2520Patrick%2520Wong%2520and%2520Helen%2520Meng%26entry.1292438233%3D%2520%2520Early%2520detection%2520of%2520neurocognitive%2520disorders%2520%2528NCDs%2529%2520is%2520crucial%2520for%2520timely%250Aintervention%2520and%2520disease%2520management.%2520Speech%2520analysis%2520offers%2520a%2520non-intrusive%2520and%250Ascalable%2520screening%2520method%252C%2520particularly%2520through%2520narrative%2520tasks%2520in%250Aneuropsychological%2520assessment%2520tools.%2520Traditional%2520narrative%2520analysis%2520often%250Afocuses%2520on%2520local%2520indicators%2520in%2520microstructure%252C%2520such%2520as%2520word%2520usage%2520and%2520syntax.%250AWhile%2520these%2520features%2520provide%2520insights%2520into%2520language%2520production%2520abilities%252C%2520they%250Aoften%2520fail%2520to%2520capture%2520global%2520narrative%2520patterns%252C%2520or%2520microstructures.%250AMacrostructures%2520include%2520coherence%252C%2520thematic%2520organization%252C%2520and%2520logical%250Aprogressions%252C%2520reflecting%2520essential%2520cognitive%2520skills%2520potentially%2520critical%2520for%250Arecognizing%2520NCDs.%2520Addressing%2520this%2520gap%252C%2520we%2520propose%2520to%2520investigate%2520specific%250Acognitive%2520and%2520linguistic%2520challenges%2520by%2520analyzing%2520topical%2520shifts%252C%2520temporal%250Adynamics%252C%2520and%2520the%2520coherence%2520of%2520narratives%2520over%2520time%252C%2520aiming%2520to%2520reveal%2520cognitive%250Adeficits%2520by%2520identifying%2520narrative%2520impairments%252C%2520and%2520exploring%2520their%2520impact%2520on%250Acommunication%2520and%2520cognition.%2520The%2520investigation%2520is%2520based%2520on%2520the%2520CU-MARVEL%2520Rabbit%250AStory%2520corpus%252C%2520which%2520comprises%2520recordings%2520of%2520a%2520story-telling%2520task%2520from%2520758%2520older%250Aadults.%2520We%2520developed%2520two%2520approaches%253A%2520the%2520Dynamic%2520Topic%2520Models%2520%2528DTM%2529-based%250Atemporal%2520analysis%2520to%2520examine%2520the%2520evolution%2520of%2520topics%2520over%2520time%252C%2520and%2520the%250AText-Image%2520Temporal%2520Alignment%2520Network%2520%2528TITAN%2529%2520to%2520evaluate%2520the%2520coherence%2520between%250Aspoken%2520narratives%2520and%2520visual%2520stimuli.%2520DTM-based%2520approach%2520validated%2520the%250Aeffectiveness%2520of%2520dynamic%2520topic%2520consistency%2520as%2520a%2520macrostructural%2520metric%250A%2528F1%253D0.61%252C%2520AUC%253D0.78%2529.%2520The%2520TITAN%2520approach%2520achieved%2520the%2520highest%2520performance%250A%2528F1%253D0.72%252C%2520AUC%253D0.81%2529%252C%2520surpassing%2520established%2520microstructural%2520and%2520macrostructural%250Afeature%2520sets.%2520Cross-comparison%2520and%2520regression%2520tasks%2520further%2520demonstrated%2520the%250Aeffectiveness%2520of%2520proposed%2520dynamic%2520macrostructural%2520modeling%2520approaches%2520for%2520NCD%250Adetection.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2501.03727v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Detecting%20Neurocognitive%20Disorders%20through%20Analyses%20of%20Topic%20Evolution%0A%20%20and%20Cross-modal%20Consistency%20in%20Visual-Stimulated%20Narratives&entry.906535625=Jinchao%20Li%20and%20Yuejiao%20Wang%20and%20Junan%20Li%20and%20Jiawen%20Kang%20and%20Bo%20Zheng%20and%20Simon%20Wong%20and%20Brian%20Mak%20and%20Helene%20Fung%20and%20Jean%20Woo%20and%20Man-Wai%20Mak%20and%20Timothy%20Kwok%20and%20Vincent%20Mok%20and%20Xianmin%20Gong%20and%20Xixin%20Wu%20and%20Xunying%20Liu%20and%20Patrick%20Wong%20and%20Helen%20Meng&entry.1292438233=%20%20Early%20detection%20of%20neurocognitive%20disorders%20%28NCDs%29%20is%20crucial%20for%20timely%0Aintervention%20and%20disease%20management.%20Speech%20analysis%20offers%20a%20non-intrusive%20and%0Ascalable%20screening%20method%2C%20particularly%20through%20narrative%20tasks%20in%0Aneuropsychological%20assessment%20tools.%20Traditional%20narrative%20analysis%20often%0Afocuses%20on%20local%20indicators%20in%20microstructure%2C%20such%20as%20word%20usage%20and%20syntax.%0AWhile%20these%20features%20provide%20insights%20into%20language%20production%20abilities%2C%20they%0Aoften%20fail%20to%20capture%20global%20narrative%20patterns%2C%20or%20microstructures.%0AMacrostructures%20include%20coherence%2C%20thematic%20organization%2C%20and%20logical%0Aprogressions%2C%20reflecting%20essential%20cognitive%20skills%20potentially%20critical%20for%0Arecognizing%20NCDs.%20Addressing%20this%20gap%2C%20we%20propose%20to%20investigate%20specific%0Acognitive%20and%20linguistic%20challenges%20by%20analyzing%20topical%20shifts%2C%20temporal%0Adynamics%2C%20and%20the%20coherence%20of%20narratives%20over%20time%2C%20aiming%20to%20reveal%20cognitive%0Adeficits%20by%20identifying%20narrative%20impairments%2C%20and%20exploring%20their%20impact%20on%0Acommunication%20and%20cognition.%20The%20investigation%20is%20based%20on%20the%20CU-MARVEL%20Rabbit%0AStory%20corpus%2C%20which%20comprises%20recordings%20of%20a%20story-telling%20task%20from%20758%20older%0Aadults.%20We%20developed%20two%20approaches%3A%20the%20Dynamic%20Topic%20Models%20%28DTM%29-based%0Atemporal%20analysis%20to%20examine%20the%20evolution%20of%20topics%20over%20time%2C%20and%20the%0AText-Image%20Temporal%20Alignment%20Network%20%28TITAN%29%20to%20evaluate%20the%20coherence%20between%0Aspoken%20narratives%20and%20visual%20stimuli.%20DTM-based%20approach%20validated%20the%0Aeffectiveness%20of%20dynamic%20topic%20consistency%20as%20a%20macrostructural%20metric%0A%28F1%3D0.61%2C%20AUC%3D0.78%29.%20The%20TITAN%20approach%20achieved%20the%20highest%20performance%0A%28F1%3D0.72%2C%20AUC%3D0.81%29%2C%20surpassing%20established%20microstructural%20and%20macrostructural%0Afeature%20sets.%20Cross-comparison%20and%20regression%20tasks%20further%20demonstrated%20the%0Aeffectiveness%20of%20proposed%20dynamic%20macrostructural%20modeling%20approaches%20for%20NCD%0Adetection.%0A&entry.1838667208=http%3A//arxiv.org/abs/2501.03727v1&entry.124074799=Read"},
{"title": "Three-dimensional attention Transformer for state evaluation in\n  real-time strategy games", "author": "Yanqing Ye and Weilong Yang and Kai Qiu and Jie Zhang", "abstract": "  Situation assessment in Real-Time Strategy (RTS) games is crucial for\nunderstanding decision-making in complex adversarial environments. However,\nexisting methods remain limited in processing multi-dimensional feature\ninformation and temporal dependencies. Here we propose a tri-dimensional\nSpace-Time-Feature Transformer (TSTF Transformer) architecture, which\nefficiently models battlefield situations through three independent but\ncascaded modules: spatial attention, temporal attention, and feature attention.\nOn a dataset comprising 3,150 adversarial experiments, the 8-layer TSTF\nTransformer demonstrates superior performance: achieving 58.7% accuracy in the\nearly game (~4% progress), significantly outperforming the conventional\nTimesformer's 41.8%; reaching 97.6% accuracy in the mid-game (~40% progress)\nwhile maintaining low performance variation (standard deviation 0.114).\nMeanwhile, this architecture requires fewer parameters (4.75M) compared to the\nbaseline model (5.54M). Our study not only provides new insights into situation\nassessment in RTS games but also presents an innovative paradigm for\nTransformer-based multi-dimensional temporal modeling.\n", "link": "http://arxiv.org/abs/2501.03832v1", "date": "2025-01-07", "relevancy": 2.0981, "topK": [{"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5488}, {"title": "Scaling Rectified Flow Transformers for High-Resolution Image Synthesis", "link": "http://arxiv.org/abs/2403.03206v1", "similarity": 0.524}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5005}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Three-dimensional%20attention%20Transformer%20for%20state%20evaluation%20in%0A%20%20real-time%20strategy%20games&body=Title%3A%20Three-dimensional%20attention%20Transformer%20for%20state%20evaluation%20in%0A%20%20real-time%20strategy%20games%0AAuthor%3A%20Yanqing%20Ye%20and%20Weilong%20Yang%20and%20Kai%20Qiu%20and%20Jie%20Zhang%0AAbstract%3A%20%20%20Situation%20assessment%20in%20Real-Time%20Strategy%20%28RTS%29%20games%20is%20crucial%20for%0Aunderstanding%20decision-making%20in%20complex%20adversarial%20environments.%20However%2C%0Aexisting%20methods%20remain%20limited%20in%20processing%20multi-dimensional%20feature%0Ainformation%20and%20temporal%20dependencies.%20Here%20we%20propose%20a%20tri-dimensional%0ASpace-Time-Feature%20Transformer%20%28TSTF%20Transformer%29%20architecture%2C%20which%0Aefficiently%20models%20battlefield%20situations%20through%20three%20independent%20but%0Acascaded%20modules%3A%20spatial%20attention%2C%20temporal%20attention%2C%20and%20feature%20attention.%0AOn%20a%20dataset%20comprising%203%2C150%20adversarial%20experiments%2C%20the%208-layer%20TSTF%0ATransformer%20demonstrates%20superior%20performance%3A%20achieving%2058.7%25%20accuracy%20in%20the%0Aearly%20game%20%28~4%25%20progress%29%2C%20significantly%20outperforming%20the%20conventional%0ATimesformer%27s%2041.8%25%3B%20reaching%2097.6%25%20accuracy%20in%20the%20mid-game%20%28~40%25%20progress%29%0Awhile%20maintaining%20low%20performance%20variation%20%28standard%20deviation%200.114%29.%0AMeanwhile%2C%20this%20architecture%20requires%20fewer%20parameters%20%284.75M%29%20compared%20to%20the%0Abaseline%20model%20%285.54M%29.%20Our%20study%20not%20only%20provides%20new%20insights%20into%20situation%0Aassessment%20in%20RTS%20games%20but%20also%20presents%20an%20innovative%20paradigm%20for%0ATransformer-based%20multi-dimensional%20temporal%20modeling.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2501.03832v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DThree-dimensional%2520attention%2520Transformer%2520for%2520state%2520evaluation%2520in%250A%2520%2520real-time%2520strategy%2520games%26entry.906535625%3DYanqing%2520Ye%2520and%2520Weilong%2520Yang%2520and%2520Kai%2520Qiu%2520and%2520Jie%2520Zhang%26entry.1292438233%3D%2520%2520Situation%2520assessment%2520in%2520Real-Time%2520Strategy%2520%2528RTS%2529%2520games%2520is%2520crucial%2520for%250Aunderstanding%2520decision-making%2520in%2520complex%2520adversarial%2520environments.%2520However%252C%250Aexisting%2520methods%2520remain%2520limited%2520in%2520processing%2520multi-dimensional%2520feature%250Ainformation%2520and%2520temporal%2520dependencies.%2520Here%2520we%2520propose%2520a%2520tri-dimensional%250ASpace-Time-Feature%2520Transformer%2520%2528TSTF%2520Transformer%2529%2520architecture%252C%2520which%250Aefficiently%2520models%2520battlefield%2520situations%2520through%2520three%2520independent%2520but%250Acascaded%2520modules%253A%2520spatial%2520attention%252C%2520temporal%2520attention%252C%2520and%2520feature%2520attention.%250AOn%2520a%2520dataset%2520comprising%25203%252C150%2520adversarial%2520experiments%252C%2520the%25208-layer%2520TSTF%250ATransformer%2520demonstrates%2520superior%2520performance%253A%2520achieving%252058.7%2525%2520accuracy%2520in%2520the%250Aearly%2520game%2520%2528~4%2525%2520progress%2529%252C%2520significantly%2520outperforming%2520the%2520conventional%250ATimesformer%2527s%252041.8%2525%253B%2520reaching%252097.6%2525%2520accuracy%2520in%2520the%2520mid-game%2520%2528~40%2525%2520progress%2529%250Awhile%2520maintaining%2520low%2520performance%2520variation%2520%2528standard%2520deviation%25200.114%2529.%250AMeanwhile%252C%2520this%2520architecture%2520requires%2520fewer%2520parameters%2520%25284.75M%2529%2520compared%2520to%2520the%250Abaseline%2520model%2520%25285.54M%2529.%2520Our%2520study%2520not%2520only%2520provides%2520new%2520insights%2520into%2520situation%250Aassessment%2520in%2520RTS%2520games%2520but%2520also%2520presents%2520an%2520innovative%2520paradigm%2520for%250ATransformer-based%2520multi-dimensional%2520temporal%2520modeling.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2501.03832v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Three-dimensional%20attention%20Transformer%20for%20state%20evaluation%20in%0A%20%20real-time%20strategy%20games&entry.906535625=Yanqing%20Ye%20and%20Weilong%20Yang%20and%20Kai%20Qiu%20and%20Jie%20Zhang&entry.1292438233=%20%20Situation%20assessment%20in%20Real-Time%20Strategy%20%28RTS%29%20games%20is%20crucial%20for%0Aunderstanding%20decision-making%20in%20complex%20adversarial%20environments.%20However%2C%0Aexisting%20methods%20remain%20limited%20in%20processing%20multi-dimensional%20feature%0Ainformation%20and%20temporal%20dependencies.%20Here%20we%20propose%20a%20tri-dimensional%0ASpace-Time-Feature%20Transformer%20%28TSTF%20Transformer%29%20architecture%2C%20which%0Aefficiently%20models%20battlefield%20situations%20through%20three%20independent%20but%0Acascaded%20modules%3A%20spatial%20attention%2C%20temporal%20attention%2C%20and%20feature%20attention.%0AOn%20a%20dataset%20comprising%203%2C150%20adversarial%20experiments%2C%20the%208-layer%20TSTF%0ATransformer%20demonstrates%20superior%20performance%3A%20achieving%2058.7%25%20accuracy%20in%20the%0Aearly%20game%20%28~4%25%20progress%29%2C%20significantly%20outperforming%20the%20conventional%0ATimesformer%27s%2041.8%25%3B%20reaching%2097.6%25%20accuracy%20in%20the%20mid-game%20%28~40%25%20progress%29%0Awhile%20maintaining%20low%20performance%20variation%20%28standard%20deviation%200.114%29.%0AMeanwhile%2C%20this%20architecture%20requires%20fewer%20parameters%20%284.75M%29%20compared%20to%20the%0Abaseline%20model%20%285.54M%29.%20Our%20study%20not%20only%20provides%20new%20insights%20into%20situation%0Aassessment%20in%20RTS%20games%20but%20also%20presents%20an%20innovative%20paradigm%20for%0ATransformer-based%20multi-dimensional%20temporal%20modeling.%0A&entry.1838667208=http%3A//arxiv.org/abs/2501.03832v1&entry.124074799=Read"},
{"title": "Abstracted Shapes as Tokens -- A Generalizable and Interpretable Model\n  for Time-series Classification", "author": "Yunshi Wen and Tengfei Ma and Tsui-Wei Weng and Lam M. Nguyen and Anak Agung Julius", "abstract": "  In time-series analysis, many recent works seek to provide a unified view and\nrepresentation for time-series across multiple domains, leading to the\ndevelopment of foundation models for time-series data. Despite diverse modeling\ntechniques, existing models are black boxes and fail to provide insights and\nexplanations about their representations. In this paper, we present VQShape, a\npre-trained, generalizable, and interpretable model for time-series\nrepresentation learning and classification. By introducing a novel\nrepresentation for time-series data, we forge a connection between the latent\nspace of VQShape and shape-level features. Using vector quantization, we show\nthat time-series from different domains can be described using a unified set of\nlow-dimensional codes, where each code can be represented as an abstracted\nshape in the time domain. On classification tasks, we show that the\nrepresentations of VQShape can be utilized to build interpretable classifiers,\nachieving comparable performance to specialist models. Additionally, in\nzero-shot learning, VQShape and its codebook can generalize to previously\nunseen datasets and domains that are not included in the pre-training process.\nThe code and pre-trained weights are available at\nhttps://github.com/YunshiWen/VQShape.\n", "link": "http://arxiv.org/abs/2411.01006v3", "date": "2025-01-07", "relevancy": 2.0929, "topK": [{"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.558}, {"title": "Magic-Me: Identity-Specific Video Customized Diffusion", "link": "http://arxiv.org/abs/2402.09368v1", "similarity": 0.4997}, {"title": "Scaling Rectified Flow Transformers for High-Resolution Image Synthesis", "link": "http://arxiv.org/abs/2403.03206v1", "similarity": 0.4951}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Abstracted%20Shapes%20as%20Tokens%20--%20A%20Generalizable%20and%20Interpretable%20Model%0A%20%20for%20Time-series%20Classification&body=Title%3A%20Abstracted%20Shapes%20as%20Tokens%20--%20A%20Generalizable%20and%20Interpretable%20Model%0A%20%20for%20Time-series%20Classification%0AAuthor%3A%20Yunshi%20Wen%20and%20Tengfei%20Ma%20and%20Tsui-Wei%20Weng%20and%20Lam%20M.%20Nguyen%20and%20Anak%20Agung%20Julius%0AAbstract%3A%20%20%20In%20time-series%20analysis%2C%20many%20recent%20works%20seek%20to%20provide%20a%20unified%20view%20and%0Arepresentation%20for%20time-series%20across%20multiple%20domains%2C%20leading%20to%20the%0Adevelopment%20of%20foundation%20models%20for%20time-series%20data.%20Despite%20diverse%20modeling%0Atechniques%2C%20existing%20models%20are%20black%20boxes%20and%20fail%20to%20provide%20insights%20and%0Aexplanations%20about%20their%20representations.%20In%20this%20paper%2C%20we%20present%20VQShape%2C%20a%0Apre-trained%2C%20generalizable%2C%20and%20interpretable%20model%20for%20time-series%0Arepresentation%20learning%20and%20classification.%20By%20introducing%20a%20novel%0Arepresentation%20for%20time-series%20data%2C%20we%20forge%20a%20connection%20between%20the%20latent%0Aspace%20of%20VQShape%20and%20shape-level%20features.%20Using%20vector%20quantization%2C%20we%20show%0Athat%20time-series%20from%20different%20domains%20can%20be%20described%20using%20a%20unified%20set%20of%0Alow-dimensional%20codes%2C%20where%20each%20code%20can%20be%20represented%20as%20an%20abstracted%0Ashape%20in%20the%20time%20domain.%20On%20classification%20tasks%2C%20we%20show%20that%20the%0Arepresentations%20of%20VQShape%20can%20be%20utilized%20to%20build%20interpretable%20classifiers%2C%0Aachieving%20comparable%20performance%20to%20specialist%20models.%20Additionally%2C%20in%0Azero-shot%20learning%2C%20VQShape%20and%20its%20codebook%20can%20generalize%20to%20previously%0Aunseen%20datasets%20and%20domains%20that%20are%20not%20included%20in%20the%20pre-training%20process.%0AThe%20code%20and%20pre-trained%20weights%20are%20available%20at%0Ahttps%3A//github.com/YunshiWen/VQShape.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2411.01006v3%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DAbstracted%2520Shapes%2520as%2520Tokens%2520--%2520A%2520Generalizable%2520and%2520Interpretable%2520Model%250A%2520%2520for%2520Time-series%2520Classification%26entry.906535625%3DYunshi%2520Wen%2520and%2520Tengfei%2520Ma%2520and%2520Tsui-Wei%2520Weng%2520and%2520Lam%2520M.%2520Nguyen%2520and%2520Anak%2520Agung%2520Julius%26entry.1292438233%3D%2520%2520In%2520time-series%2520analysis%252C%2520many%2520recent%2520works%2520seek%2520to%2520provide%2520a%2520unified%2520view%2520and%250Arepresentation%2520for%2520time-series%2520across%2520multiple%2520domains%252C%2520leading%2520to%2520the%250Adevelopment%2520of%2520foundation%2520models%2520for%2520time-series%2520data.%2520Despite%2520diverse%2520modeling%250Atechniques%252C%2520existing%2520models%2520are%2520black%2520boxes%2520and%2520fail%2520to%2520provide%2520insights%2520and%250Aexplanations%2520about%2520their%2520representations.%2520In%2520this%2520paper%252C%2520we%2520present%2520VQShape%252C%2520a%250Apre-trained%252C%2520generalizable%252C%2520and%2520interpretable%2520model%2520for%2520time-series%250Arepresentation%2520learning%2520and%2520classification.%2520By%2520introducing%2520a%2520novel%250Arepresentation%2520for%2520time-series%2520data%252C%2520we%2520forge%2520a%2520connection%2520between%2520the%2520latent%250Aspace%2520of%2520VQShape%2520and%2520shape-level%2520features.%2520Using%2520vector%2520quantization%252C%2520we%2520show%250Athat%2520time-series%2520from%2520different%2520domains%2520can%2520be%2520described%2520using%2520a%2520unified%2520set%2520of%250Alow-dimensional%2520codes%252C%2520where%2520each%2520code%2520can%2520be%2520represented%2520as%2520an%2520abstracted%250Ashape%2520in%2520the%2520time%2520domain.%2520On%2520classification%2520tasks%252C%2520we%2520show%2520that%2520the%250Arepresentations%2520of%2520VQShape%2520can%2520be%2520utilized%2520to%2520build%2520interpretable%2520classifiers%252C%250Aachieving%2520comparable%2520performance%2520to%2520specialist%2520models.%2520Additionally%252C%2520in%250Azero-shot%2520learning%252C%2520VQShape%2520and%2520its%2520codebook%2520can%2520generalize%2520to%2520previously%250Aunseen%2520datasets%2520and%2520domains%2520that%2520are%2520not%2520included%2520in%2520the%2520pre-training%2520process.%250AThe%2520code%2520and%2520pre-trained%2520weights%2520are%2520available%2520at%250Ahttps%253A//github.com/YunshiWen/VQShape.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2411.01006v3%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Abstracted%20Shapes%20as%20Tokens%20--%20A%20Generalizable%20and%20Interpretable%20Model%0A%20%20for%20Time-series%20Classification&entry.906535625=Yunshi%20Wen%20and%20Tengfei%20Ma%20and%20Tsui-Wei%20Weng%20and%20Lam%20M.%20Nguyen%20and%20Anak%20Agung%20Julius&entry.1292438233=%20%20In%20time-series%20analysis%2C%20many%20recent%20works%20seek%20to%20provide%20a%20unified%20view%20and%0Arepresentation%20for%20time-series%20across%20multiple%20domains%2C%20leading%20to%20the%0Adevelopment%20of%20foundation%20models%20for%20time-series%20data.%20Despite%20diverse%20modeling%0Atechniques%2C%20existing%20models%20are%20black%20boxes%20and%20fail%20to%20provide%20insights%20and%0Aexplanations%20about%20their%20representations.%20In%20this%20paper%2C%20we%20present%20VQShape%2C%20a%0Apre-trained%2C%20generalizable%2C%20and%20interpretable%20model%20for%20time-series%0Arepresentation%20learning%20and%20classification.%20By%20introducing%20a%20novel%0Arepresentation%20for%20time-series%20data%2C%20we%20forge%20a%20connection%20between%20the%20latent%0Aspace%20of%20VQShape%20and%20shape-level%20features.%20Using%20vector%20quantization%2C%20we%20show%0Athat%20time-series%20from%20different%20domains%20can%20be%20described%20using%20a%20unified%20set%20of%0Alow-dimensional%20codes%2C%20where%20each%20code%20can%20be%20represented%20as%20an%20abstracted%0Ashape%20in%20the%20time%20domain.%20On%20classification%20tasks%2C%20we%20show%20that%20the%0Arepresentations%20of%20VQShape%20can%20be%20utilized%20to%20build%20interpretable%20classifiers%2C%0Aachieving%20comparable%20performance%20to%20specialist%20models.%20Additionally%2C%20in%0Azero-shot%20learning%2C%20VQShape%20and%20its%20codebook%20can%20generalize%20to%20previously%0Aunseen%20datasets%20and%20domains%20that%20are%20not%20included%20in%20the%20pre-training%20process.%0AThe%20code%20and%20pre-trained%20weights%20are%20available%20at%0Ahttps%3A//github.com/YunshiWen/VQShape.%0A&entry.1838667208=http%3A//arxiv.org/abs/2411.01006v3&entry.124074799=Read"},
{"title": "Learning Informative Latent Representation for Quantum State Tomography", "author": "Hailan Ma and Zhenhong Sun and Daoyi Dong and Dong Gong", "abstract": "  Quantum state tomography (QST) is the process of reconstructing the complete\nstate of a quantum system (mathematically described as a density matrix)\nthrough a series of different measurements. These measurements are performed on\na number of identical copies of the quantum system, with outcomes gathered as\nfrequencies. QST aims to recover the density matrix or the properties of the\nquantum state from the measured frequencies. Although an informationally\ncomplete set of measurements can specify the quantum state accurately in an\nideal scenario with a large number of identical copies, both the measurements\nand identical copies are restricted and imperfect in practical scenarios,\nmaking QST highly ill-posed. The conventional QST methods usually assume\naccurate measured frequencies or rely on manually designed regularizers to\nhandle the ill-posed reconstruction problem, suffering from limited\napplications in realistic scenarios. Recent advances in deep neural networks\n(DNN) led to the emergence of deep learning in QST. However, existing DL-based\nQST approaches often employ generic DNN models that are not optimized for\nimperfect conditions of QST. In this paper, we propose a transformer-based\nautoencoder architecture tailored for QST with imperfect measurement data. Our\nmethod leverages a transformer-based encoder to extract an informative latent\nrepresentation (ILR) from imperfect measurement data and employs a decoder to\npredict the quantum states based on the ILR. We anticipate that the\nhigh-dimensional ILR will capture more comprehensive information about the\nquantum states. To achieve this, we conduct pre-training of the encoder using a\npretext task that involves reconstructing high-quality frequencies from\nmeasured frequencies. Extensive simulations and experiments demonstrate the\nremarkable ability of the informative latent representation to deal with\nimperfect measurement data in QST.\n", "link": "http://arxiv.org/abs/2310.00518v2", "date": "2025-01-07", "relevancy": 2.084, "topK": [{"title": "Scaling Rectified Flow Transformers for High-Resolution Image Synthesis", "link": "http://arxiv.org/abs/2403.03206v1", "similarity": 0.5404}, {"title": "IAE: Implicit Autoencoder for Point Cloud Self-supervised Representation Learning", "link": "https://arxiv.org/abs/2201.00785", "similarity": 0.5205}, {"title": "Self-supervised Photographic Image Layout Representation Learning", "link": "http://arxiv.org/abs/2403.03740v1", "similarity": 0.5138}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Learning%20Informative%20Latent%20Representation%20for%20Quantum%20State%20Tomography&body=Title%3A%20Learning%20Informative%20Latent%20Representation%20for%20Quantum%20State%20Tomography%0AAuthor%3A%20Hailan%20Ma%20and%20Zhenhong%20Sun%20and%20Daoyi%20Dong%20and%20Dong%20Gong%0AAbstract%3A%20%20%20Quantum%20state%20tomography%20%28QST%29%20is%20the%20process%20of%20reconstructing%20the%20complete%0Astate%20of%20a%20quantum%20system%20%28mathematically%20described%20as%20a%20density%20matrix%29%0Athrough%20a%20series%20of%20different%20measurements.%20These%20measurements%20are%20performed%20on%0Aa%20number%20of%20identical%20copies%20of%20the%20quantum%20system%2C%20with%20outcomes%20gathered%20as%0Afrequencies.%20QST%20aims%20to%20recover%20the%20density%20matrix%20or%20the%20properties%20of%20the%0Aquantum%20state%20from%20the%20measured%20frequencies.%20Although%20an%20informationally%0Acomplete%20set%20of%20measurements%20can%20specify%20the%20quantum%20state%20accurately%20in%20an%0Aideal%20scenario%20with%20a%20large%20number%20of%20identical%20copies%2C%20both%20the%20measurements%0Aand%20identical%20copies%20are%20restricted%20and%20imperfect%20in%20practical%20scenarios%2C%0Amaking%20QST%20highly%20ill-posed.%20The%20conventional%20QST%20methods%20usually%20assume%0Aaccurate%20measured%20frequencies%20or%20rely%20on%20manually%20designed%20regularizers%20to%0Ahandle%20the%20ill-posed%20reconstruction%20problem%2C%20suffering%20from%20limited%0Aapplications%20in%20realistic%20scenarios.%20Recent%20advances%20in%20deep%20neural%20networks%0A%28DNN%29%20led%20to%20the%20emergence%20of%20deep%20learning%20in%20QST.%20However%2C%20existing%20DL-based%0AQST%20approaches%20often%20employ%20generic%20DNN%20models%20that%20are%20not%20optimized%20for%0Aimperfect%20conditions%20of%20QST.%20In%20this%20paper%2C%20we%20propose%20a%20transformer-based%0Aautoencoder%20architecture%20tailored%20for%20QST%20with%20imperfect%20measurement%20data.%20Our%0Amethod%20leverages%20a%20transformer-based%20encoder%20to%20extract%20an%20informative%20latent%0Arepresentation%20%28ILR%29%20from%20imperfect%20measurement%20data%20and%20employs%20a%20decoder%20to%0Apredict%20the%20quantum%20states%20based%20on%20the%20ILR.%20We%20anticipate%20that%20the%0Ahigh-dimensional%20ILR%20will%20capture%20more%20comprehensive%20information%20about%20the%0Aquantum%20states.%20To%20achieve%20this%2C%20we%20conduct%20pre-training%20of%20the%20encoder%20using%20a%0Apretext%20task%20that%20involves%20reconstructing%20high-quality%20frequencies%20from%0Ameasured%20frequencies.%20Extensive%20simulations%20and%20experiments%20demonstrate%20the%0Aremarkable%20ability%20of%20the%20informative%20latent%20representation%20to%20deal%20with%0Aimperfect%20measurement%20data%20in%20QST.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2310.00518v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DLearning%2520Informative%2520Latent%2520Representation%2520for%2520Quantum%2520State%2520Tomography%26entry.906535625%3DHailan%2520Ma%2520and%2520Zhenhong%2520Sun%2520and%2520Daoyi%2520Dong%2520and%2520Dong%2520Gong%26entry.1292438233%3D%2520%2520Quantum%2520state%2520tomography%2520%2528QST%2529%2520is%2520the%2520process%2520of%2520reconstructing%2520the%2520complete%250Astate%2520of%2520a%2520quantum%2520system%2520%2528mathematically%2520described%2520as%2520a%2520density%2520matrix%2529%250Athrough%2520a%2520series%2520of%2520different%2520measurements.%2520These%2520measurements%2520are%2520performed%2520on%250Aa%2520number%2520of%2520identical%2520copies%2520of%2520the%2520quantum%2520system%252C%2520with%2520outcomes%2520gathered%2520as%250Afrequencies.%2520QST%2520aims%2520to%2520recover%2520the%2520density%2520matrix%2520or%2520the%2520properties%2520of%2520the%250Aquantum%2520state%2520from%2520the%2520measured%2520frequencies.%2520Although%2520an%2520informationally%250Acomplete%2520set%2520of%2520measurements%2520can%2520specify%2520the%2520quantum%2520state%2520accurately%2520in%2520an%250Aideal%2520scenario%2520with%2520a%2520large%2520number%2520of%2520identical%2520copies%252C%2520both%2520the%2520measurements%250Aand%2520identical%2520copies%2520are%2520restricted%2520and%2520imperfect%2520in%2520practical%2520scenarios%252C%250Amaking%2520QST%2520highly%2520ill-posed.%2520The%2520conventional%2520QST%2520methods%2520usually%2520assume%250Aaccurate%2520measured%2520frequencies%2520or%2520rely%2520on%2520manually%2520designed%2520regularizers%2520to%250Ahandle%2520the%2520ill-posed%2520reconstruction%2520problem%252C%2520suffering%2520from%2520limited%250Aapplications%2520in%2520realistic%2520scenarios.%2520Recent%2520advances%2520in%2520deep%2520neural%2520networks%250A%2528DNN%2529%2520led%2520to%2520the%2520emergence%2520of%2520deep%2520learning%2520in%2520QST.%2520However%252C%2520existing%2520DL-based%250AQST%2520approaches%2520often%2520employ%2520generic%2520DNN%2520models%2520that%2520are%2520not%2520optimized%2520for%250Aimperfect%2520conditions%2520of%2520QST.%2520In%2520this%2520paper%252C%2520we%2520propose%2520a%2520transformer-based%250Aautoencoder%2520architecture%2520tailored%2520for%2520QST%2520with%2520imperfect%2520measurement%2520data.%2520Our%250Amethod%2520leverages%2520a%2520transformer-based%2520encoder%2520to%2520extract%2520an%2520informative%2520latent%250Arepresentation%2520%2528ILR%2529%2520from%2520imperfect%2520measurement%2520data%2520and%2520employs%2520a%2520decoder%2520to%250Apredict%2520the%2520quantum%2520states%2520based%2520on%2520the%2520ILR.%2520We%2520anticipate%2520that%2520the%250Ahigh-dimensional%2520ILR%2520will%2520capture%2520more%2520comprehensive%2520information%2520about%2520the%250Aquantum%2520states.%2520To%2520achieve%2520this%252C%2520we%2520conduct%2520pre-training%2520of%2520the%2520encoder%2520using%2520a%250Apretext%2520task%2520that%2520involves%2520reconstructing%2520high-quality%2520frequencies%2520from%250Ameasured%2520frequencies.%2520Extensive%2520simulations%2520and%2520experiments%2520demonstrate%2520the%250Aremarkable%2520ability%2520of%2520the%2520informative%2520latent%2520representation%2520to%2520deal%2520with%250Aimperfect%2520measurement%2520data%2520in%2520QST.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2310.00518v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Learning%20Informative%20Latent%20Representation%20for%20Quantum%20State%20Tomography&entry.906535625=Hailan%20Ma%20and%20Zhenhong%20Sun%20and%20Daoyi%20Dong%20and%20Dong%20Gong&entry.1292438233=%20%20Quantum%20state%20tomography%20%28QST%29%20is%20the%20process%20of%20reconstructing%20the%20complete%0Astate%20of%20a%20quantum%20system%20%28mathematically%20described%20as%20a%20density%20matrix%29%0Athrough%20a%20series%20of%20different%20measurements.%20These%20measurements%20are%20performed%20on%0Aa%20number%20of%20identical%20copies%20of%20the%20quantum%20system%2C%20with%20outcomes%20gathered%20as%0Afrequencies.%20QST%20aims%20to%20recover%20the%20density%20matrix%20or%20the%20properties%20of%20the%0Aquantum%20state%20from%20the%20measured%20frequencies.%20Although%20an%20informationally%0Acomplete%20set%20of%20measurements%20can%20specify%20the%20quantum%20state%20accurately%20in%20an%0Aideal%20scenario%20with%20a%20large%20number%20of%20identical%20copies%2C%20both%20the%20measurements%0Aand%20identical%20copies%20are%20restricted%20and%20imperfect%20in%20practical%20scenarios%2C%0Amaking%20QST%20highly%20ill-posed.%20The%20conventional%20QST%20methods%20usually%20assume%0Aaccurate%20measured%20frequencies%20or%20rely%20on%20manually%20designed%20regularizers%20to%0Ahandle%20the%20ill-posed%20reconstruction%20problem%2C%20suffering%20from%20limited%0Aapplications%20in%20realistic%20scenarios.%20Recent%20advances%20in%20deep%20neural%20networks%0A%28DNN%29%20led%20to%20the%20emergence%20of%20deep%20learning%20in%20QST.%20However%2C%20existing%20DL-based%0AQST%20approaches%20often%20employ%20generic%20DNN%20models%20that%20are%20not%20optimized%20for%0Aimperfect%20conditions%20of%20QST.%20In%20this%20paper%2C%20we%20propose%20a%20transformer-based%0Aautoencoder%20architecture%20tailored%20for%20QST%20with%20imperfect%20measurement%20data.%20Our%0Amethod%20leverages%20a%20transformer-based%20encoder%20to%20extract%20an%20informative%20latent%0Arepresentation%20%28ILR%29%20from%20imperfect%20measurement%20data%20and%20employs%20a%20decoder%20to%0Apredict%20the%20quantum%20states%20based%20on%20the%20ILR.%20We%20anticipate%20that%20the%0Ahigh-dimensional%20ILR%20will%20capture%20more%20comprehensive%20information%20about%20the%0Aquantum%20states.%20To%20achieve%20this%2C%20we%20conduct%20pre-training%20of%20the%20encoder%20using%20a%0Apretext%20task%20that%20involves%20reconstructing%20high-quality%20frequencies%20from%0Ameasured%20frequencies.%20Extensive%20simulations%20and%20experiments%20demonstrate%20the%0Aremarkable%20ability%20of%20the%20informative%20latent%20representation%20to%20deal%20with%0Aimperfect%20measurement%20data%20in%20QST.%0A&entry.1838667208=http%3A//arxiv.org/abs/2310.00518v2&entry.124074799=Read"},
{"title": "Vision Transformer Neural Architecture Search for Out-of-Distribution\n  Generalization: Benchmark and Insights", "author": "Sy-Tuyen Ho and Tuan Van Vo and Somayeh Ebrahimkhani and Ngai-Man Cheung", "abstract": "  While ViTs have achieved across machine learning tasks, deploying them in\nreal-world scenarios faces a critical challenge: generalizing under OoD shifts.\nA crucial research gap exists in understanding how to design ViT architectures,\nboth manually and automatically, for better OoD generalization. To this end, we\nintroduce OoD-ViT-NAS, the first systematic benchmark for ViTs NAS focused on\nOoD generalization. This benchmark includes 3000 ViT architectures of varying\ncomputational budgets evaluated on 8 common OoD datasets. Using this benchmark,\nwe analyze factors contributing to OoD generalization. Our findings reveal key\ninsights. First, ViT architecture designs significantly affect OoD\ngeneralization. Second, ID accuracy is often a poor indicator of OoD accuracy,\nhighlighting the risk of optimizing ViT architectures solely for ID\nperformance. Third, we perform the first study of NAS for ViTs OoD robustness,\nanalyzing 9 Training-free NAS methods. We find that existing Training-free NAS\nmethods are largely ineffective in predicting OoD accuracy despite excelling at\nID accuracy. Simple proxies like Param or Flop surprisingly outperform complex\nTraining-free NAS methods in predicting OoD accuracy. Finally, we study how ViT\narchitectural attributes impact OoD generalization and discover that increasing\nembedding dimensions generally enhances performance. Our benchmark shows that\nViT architectures exhibit a wide range of OoD accuracy, with up to 11.85%\nimprovement for some OoD shifts. This underscores the importance of studying\nViT architecture design for OoD. We believe OoD-ViT-NAS can catalyze further\nresearch into how ViT designs influence OoD generalization.\n", "link": "http://arxiv.org/abs/2501.03782v1", "date": "2025-01-07", "relevancy": 2.0592, "topK": [{"title": "Scaling Rectified Flow Transformers for High-Resolution Image Synthesis", "link": "http://arxiv.org/abs/2403.03206v1", "similarity": 0.5226}, {"title": "Tuning-Free Noise Rectification for High Fidelity Image-to-Video\n  Generation", "link": "http://arxiv.org/abs/2403.02827v1", "similarity": 0.514}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5124}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Vision%20Transformer%20Neural%20Architecture%20Search%20for%20Out-of-Distribution%0A%20%20Generalization%3A%20Benchmark%20and%20Insights&body=Title%3A%20Vision%20Transformer%20Neural%20Architecture%20Search%20for%20Out-of-Distribution%0A%20%20Generalization%3A%20Benchmark%20and%20Insights%0AAuthor%3A%20Sy-Tuyen%20Ho%20and%20Tuan%20Van%20Vo%20and%20Somayeh%20Ebrahimkhani%20and%20Ngai-Man%20Cheung%0AAbstract%3A%20%20%20While%20ViTs%20have%20achieved%20across%20machine%20learning%20tasks%2C%20deploying%20them%20in%0Areal-world%20scenarios%20faces%20a%20critical%20challenge%3A%20generalizing%20under%20OoD%20shifts.%0AA%20crucial%20research%20gap%20exists%20in%20understanding%20how%20to%20design%20ViT%20architectures%2C%0Aboth%20manually%20and%20automatically%2C%20for%20better%20OoD%20generalization.%20To%20this%20end%2C%20we%0Aintroduce%20OoD-ViT-NAS%2C%20the%20first%20systematic%20benchmark%20for%20ViTs%20NAS%20focused%20on%0AOoD%20generalization.%20This%20benchmark%20includes%203000%20ViT%20architectures%20of%20varying%0Acomputational%20budgets%20evaluated%20on%208%20common%20OoD%20datasets.%20Using%20this%20benchmark%2C%0Awe%20analyze%20factors%20contributing%20to%20OoD%20generalization.%20Our%20findings%20reveal%20key%0Ainsights.%20First%2C%20ViT%20architecture%20designs%20significantly%20affect%20OoD%0Ageneralization.%20Second%2C%20ID%20accuracy%20is%20often%20a%20poor%20indicator%20of%20OoD%20accuracy%2C%0Ahighlighting%20the%20risk%20of%20optimizing%20ViT%20architectures%20solely%20for%20ID%0Aperformance.%20Third%2C%20we%20perform%20the%20first%20study%20of%20NAS%20for%20ViTs%20OoD%20robustness%2C%0Aanalyzing%209%20Training-free%20NAS%20methods.%20We%20find%20that%20existing%20Training-free%20NAS%0Amethods%20are%20largely%20ineffective%20in%20predicting%20OoD%20accuracy%20despite%20excelling%20at%0AID%20accuracy.%20Simple%20proxies%20like%20Param%20or%20Flop%20surprisingly%20outperform%20complex%0ATraining-free%20NAS%20methods%20in%20predicting%20OoD%20accuracy.%20Finally%2C%20we%20study%20how%20ViT%0Aarchitectural%20attributes%20impact%20OoD%20generalization%20and%20discover%20that%20increasing%0Aembedding%20dimensions%20generally%20enhances%20performance.%20Our%20benchmark%20shows%20that%0AViT%20architectures%20exhibit%20a%20wide%20range%20of%20OoD%20accuracy%2C%20with%20up%20to%2011.85%25%0Aimprovement%20for%20some%20OoD%20shifts.%20This%20underscores%20the%20importance%20of%20studying%0AViT%20architecture%20design%20for%20OoD.%20We%20believe%20OoD-ViT-NAS%20can%20catalyze%20further%0Aresearch%20into%20how%20ViT%20designs%20influence%20OoD%20generalization.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2501.03782v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DVision%2520Transformer%2520Neural%2520Architecture%2520Search%2520for%2520Out-of-Distribution%250A%2520%2520Generalization%253A%2520Benchmark%2520and%2520Insights%26entry.906535625%3DSy-Tuyen%2520Ho%2520and%2520Tuan%2520Van%2520Vo%2520and%2520Somayeh%2520Ebrahimkhani%2520and%2520Ngai-Man%2520Cheung%26entry.1292438233%3D%2520%2520While%2520ViTs%2520have%2520achieved%2520across%2520machine%2520learning%2520tasks%252C%2520deploying%2520them%2520in%250Areal-world%2520scenarios%2520faces%2520a%2520critical%2520challenge%253A%2520generalizing%2520under%2520OoD%2520shifts.%250AA%2520crucial%2520research%2520gap%2520exists%2520in%2520understanding%2520how%2520to%2520design%2520ViT%2520architectures%252C%250Aboth%2520manually%2520and%2520automatically%252C%2520for%2520better%2520OoD%2520generalization.%2520To%2520this%2520end%252C%2520we%250Aintroduce%2520OoD-ViT-NAS%252C%2520the%2520first%2520systematic%2520benchmark%2520for%2520ViTs%2520NAS%2520focused%2520on%250AOoD%2520generalization.%2520This%2520benchmark%2520includes%25203000%2520ViT%2520architectures%2520of%2520varying%250Acomputational%2520budgets%2520evaluated%2520on%25208%2520common%2520OoD%2520datasets.%2520Using%2520this%2520benchmark%252C%250Awe%2520analyze%2520factors%2520contributing%2520to%2520OoD%2520generalization.%2520Our%2520findings%2520reveal%2520key%250Ainsights.%2520First%252C%2520ViT%2520architecture%2520designs%2520significantly%2520affect%2520OoD%250Ageneralization.%2520Second%252C%2520ID%2520accuracy%2520is%2520often%2520a%2520poor%2520indicator%2520of%2520OoD%2520accuracy%252C%250Ahighlighting%2520the%2520risk%2520of%2520optimizing%2520ViT%2520architectures%2520solely%2520for%2520ID%250Aperformance.%2520Third%252C%2520we%2520perform%2520the%2520first%2520study%2520of%2520NAS%2520for%2520ViTs%2520OoD%2520robustness%252C%250Aanalyzing%25209%2520Training-free%2520NAS%2520methods.%2520We%2520find%2520that%2520existing%2520Training-free%2520NAS%250Amethods%2520are%2520largely%2520ineffective%2520in%2520predicting%2520OoD%2520accuracy%2520despite%2520excelling%2520at%250AID%2520accuracy.%2520Simple%2520proxies%2520like%2520Param%2520or%2520Flop%2520surprisingly%2520outperform%2520complex%250ATraining-free%2520NAS%2520methods%2520in%2520predicting%2520OoD%2520accuracy.%2520Finally%252C%2520we%2520study%2520how%2520ViT%250Aarchitectural%2520attributes%2520impact%2520OoD%2520generalization%2520and%2520discover%2520that%2520increasing%250Aembedding%2520dimensions%2520generally%2520enhances%2520performance.%2520Our%2520benchmark%2520shows%2520that%250AViT%2520architectures%2520exhibit%2520a%2520wide%2520range%2520of%2520OoD%2520accuracy%252C%2520with%2520up%2520to%252011.85%2525%250Aimprovement%2520for%2520some%2520OoD%2520shifts.%2520This%2520underscores%2520the%2520importance%2520of%2520studying%250AViT%2520architecture%2520design%2520for%2520OoD.%2520We%2520believe%2520OoD-ViT-NAS%2520can%2520catalyze%2520further%250Aresearch%2520into%2520how%2520ViT%2520designs%2520influence%2520OoD%2520generalization.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2501.03782v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Vision%20Transformer%20Neural%20Architecture%20Search%20for%20Out-of-Distribution%0A%20%20Generalization%3A%20Benchmark%20and%20Insights&entry.906535625=Sy-Tuyen%20Ho%20and%20Tuan%20Van%20Vo%20and%20Somayeh%20Ebrahimkhani%20and%20Ngai-Man%20Cheung&entry.1292438233=%20%20While%20ViTs%20have%20achieved%20across%20machine%20learning%20tasks%2C%20deploying%20them%20in%0Areal-world%20scenarios%20faces%20a%20critical%20challenge%3A%20generalizing%20under%20OoD%20shifts.%0AA%20crucial%20research%20gap%20exists%20in%20understanding%20how%20to%20design%20ViT%20architectures%2C%0Aboth%20manually%20and%20automatically%2C%20for%20better%20OoD%20generalization.%20To%20this%20end%2C%20we%0Aintroduce%20OoD-ViT-NAS%2C%20the%20first%20systematic%20benchmark%20for%20ViTs%20NAS%20focused%20on%0AOoD%20generalization.%20This%20benchmark%20includes%203000%20ViT%20architectures%20of%20varying%0Acomputational%20budgets%20evaluated%20on%208%20common%20OoD%20datasets.%20Using%20this%20benchmark%2C%0Awe%20analyze%20factors%20contributing%20to%20OoD%20generalization.%20Our%20findings%20reveal%20key%0Ainsights.%20First%2C%20ViT%20architecture%20designs%20significantly%20affect%20OoD%0Ageneralization.%20Second%2C%20ID%20accuracy%20is%20often%20a%20poor%20indicator%20of%20OoD%20accuracy%2C%0Ahighlighting%20the%20risk%20of%20optimizing%20ViT%20architectures%20solely%20for%20ID%0Aperformance.%20Third%2C%20we%20perform%20the%20first%20study%20of%20NAS%20for%20ViTs%20OoD%20robustness%2C%0Aanalyzing%209%20Training-free%20NAS%20methods.%20We%20find%20that%20existing%20Training-free%20NAS%0Amethods%20are%20largely%20ineffective%20in%20predicting%20OoD%20accuracy%20despite%20excelling%20at%0AID%20accuracy.%20Simple%20proxies%20like%20Param%20or%20Flop%20surprisingly%20outperform%20complex%0ATraining-free%20NAS%20methods%20in%20predicting%20OoD%20accuracy.%20Finally%2C%20we%20study%20how%20ViT%0Aarchitectural%20attributes%20impact%20OoD%20generalization%20and%20discover%20that%20increasing%0Aembedding%20dimensions%20generally%20enhances%20performance.%20Our%20benchmark%20shows%20that%0AViT%20architectures%20exhibit%20a%20wide%20range%20of%20OoD%20accuracy%2C%20with%20up%20to%2011.85%25%0Aimprovement%20for%20some%20OoD%20shifts.%20This%20underscores%20the%20importance%20of%20studying%0AViT%20architecture%20design%20for%20OoD.%20We%20believe%20OoD-ViT-NAS%20can%20catalyze%20further%0Aresearch%20into%20how%20ViT%20designs%20influence%20OoD%20generalization.%0A&entry.1838667208=http%3A//arxiv.org/abs/2501.03782v1&entry.124074799=Read"},
{"title": "PPTAgent: Generating and Evaluating Presentations Beyond Text-to-Slides", "author": "Hao Zheng and Xinyan Guan and Hao Kong and Jia Zheng and Hongyu Lin and Yaojie Lu and Ben He and Xianpei Han and Le Sun", "abstract": "  Automatically generating presentations from documents is a challenging task\nthat requires balancing content quality, visual design, and structural\ncoherence. Existing methods primarily focus on improving and evaluating the\ncontent quality in isolation, often overlooking visual design and structural\ncoherence, which limits their practical applicability. To address these\nlimitations, we propose PPTAgent, which comprehensively improves presentation\ngeneration through a two-stage, edit-based approach inspired by human\nworkflows. PPTAgent first analyzes reference presentations to understand their\nstructural patterns and content schemas, then drafts outlines and generates\nslides through code actions to ensure consistency and alignment. To\ncomprehensively evaluate the quality of generated presentations, we further\nintroduce PPTEval, an evaluation framework that assesses presentations across\nthree dimensions: Content, Design, and Coherence. Experiments show that\nPPTAgent significantly outperforms traditional automatic presentation\ngeneration methods across all three dimensions. The code and data are available\nat https://github.com/icip-cas/PPTAgent.\n", "link": "http://arxiv.org/abs/2501.03936v1", "date": "2025-01-07", "relevancy": 2.0539, "topK": [{"title": "DressCode: Autoregressively Sewing and Generating Garments from Text\n  Guidance", "link": "http://arxiv.org/abs/2401.16465v3", "similarity": 0.5278}, {"title": "PhysGen: Rigid-Body Physics-Grounded Image-to-Video Generation", "link": "http://arxiv.org/abs/2409.18964v1", "similarity": 0.5055}, {"title": "Thinking Outside the BBox: Unconstrained Generative Object Compositing", "link": "http://arxiv.org/abs/2409.04559v2", "similarity": 0.4976}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20PPTAgent%3A%20Generating%20and%20Evaluating%20Presentations%20Beyond%20Text-to-Slides&body=Title%3A%20PPTAgent%3A%20Generating%20and%20Evaluating%20Presentations%20Beyond%20Text-to-Slides%0AAuthor%3A%20Hao%20Zheng%20and%20Xinyan%20Guan%20and%20Hao%20Kong%20and%20Jia%20Zheng%20and%20Hongyu%20Lin%20and%20Yaojie%20Lu%20and%20Ben%20He%20and%20Xianpei%20Han%20and%20Le%20Sun%0AAbstract%3A%20%20%20Automatically%20generating%20presentations%20from%20documents%20is%20a%20challenging%20task%0Athat%20requires%20balancing%20content%20quality%2C%20visual%20design%2C%20and%20structural%0Acoherence.%20Existing%20methods%20primarily%20focus%20on%20improving%20and%20evaluating%20the%0Acontent%20quality%20in%20isolation%2C%20often%20overlooking%20visual%20design%20and%20structural%0Acoherence%2C%20which%20limits%20their%20practical%20applicability.%20To%20address%20these%0Alimitations%2C%20we%20propose%20PPTAgent%2C%20which%20comprehensively%20improves%20presentation%0Ageneration%20through%20a%20two-stage%2C%20edit-based%20approach%20inspired%20by%20human%0Aworkflows.%20PPTAgent%20first%20analyzes%20reference%20presentations%20to%20understand%20their%0Astructural%20patterns%20and%20content%20schemas%2C%20then%20drafts%20outlines%20and%20generates%0Aslides%20through%20code%20actions%20to%20ensure%20consistency%20and%20alignment.%20To%0Acomprehensively%20evaluate%20the%20quality%20of%20generated%20presentations%2C%20we%20further%0Aintroduce%20PPTEval%2C%20an%20evaluation%20framework%20that%20assesses%20presentations%20across%0Athree%20dimensions%3A%20Content%2C%20Design%2C%20and%20Coherence.%20Experiments%20show%20that%0APPTAgent%20significantly%20outperforms%20traditional%20automatic%20presentation%0Ageneration%20methods%20across%20all%20three%20dimensions.%20The%20code%20and%20data%20are%20available%0Aat%20https%3A//github.com/icip-cas/PPTAgent.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2501.03936v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DPPTAgent%253A%2520Generating%2520and%2520Evaluating%2520Presentations%2520Beyond%2520Text-to-Slides%26entry.906535625%3DHao%2520Zheng%2520and%2520Xinyan%2520Guan%2520and%2520Hao%2520Kong%2520and%2520Jia%2520Zheng%2520and%2520Hongyu%2520Lin%2520and%2520Yaojie%2520Lu%2520and%2520Ben%2520He%2520and%2520Xianpei%2520Han%2520and%2520Le%2520Sun%26entry.1292438233%3D%2520%2520Automatically%2520generating%2520presentations%2520from%2520documents%2520is%2520a%2520challenging%2520task%250Athat%2520requires%2520balancing%2520content%2520quality%252C%2520visual%2520design%252C%2520and%2520structural%250Acoherence.%2520Existing%2520methods%2520primarily%2520focus%2520on%2520improving%2520and%2520evaluating%2520the%250Acontent%2520quality%2520in%2520isolation%252C%2520often%2520overlooking%2520visual%2520design%2520and%2520structural%250Acoherence%252C%2520which%2520limits%2520their%2520practical%2520applicability.%2520To%2520address%2520these%250Alimitations%252C%2520we%2520propose%2520PPTAgent%252C%2520which%2520comprehensively%2520improves%2520presentation%250Ageneration%2520through%2520a%2520two-stage%252C%2520edit-based%2520approach%2520inspired%2520by%2520human%250Aworkflows.%2520PPTAgent%2520first%2520analyzes%2520reference%2520presentations%2520to%2520understand%2520their%250Astructural%2520patterns%2520and%2520content%2520schemas%252C%2520then%2520drafts%2520outlines%2520and%2520generates%250Aslides%2520through%2520code%2520actions%2520to%2520ensure%2520consistency%2520and%2520alignment.%2520To%250Acomprehensively%2520evaluate%2520the%2520quality%2520of%2520generated%2520presentations%252C%2520we%2520further%250Aintroduce%2520PPTEval%252C%2520an%2520evaluation%2520framework%2520that%2520assesses%2520presentations%2520across%250Athree%2520dimensions%253A%2520Content%252C%2520Design%252C%2520and%2520Coherence.%2520Experiments%2520show%2520that%250APPTAgent%2520significantly%2520outperforms%2520traditional%2520automatic%2520presentation%250Ageneration%2520methods%2520across%2520all%2520three%2520dimensions.%2520The%2520code%2520and%2520data%2520are%2520available%250Aat%2520https%253A//github.com/icip-cas/PPTAgent.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2501.03936v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=PPTAgent%3A%20Generating%20and%20Evaluating%20Presentations%20Beyond%20Text-to-Slides&entry.906535625=Hao%20Zheng%20and%20Xinyan%20Guan%20and%20Hao%20Kong%20and%20Jia%20Zheng%20and%20Hongyu%20Lin%20and%20Yaojie%20Lu%20and%20Ben%20He%20and%20Xianpei%20Han%20and%20Le%20Sun&entry.1292438233=%20%20Automatically%20generating%20presentations%20from%20documents%20is%20a%20challenging%20task%0Athat%20requires%20balancing%20content%20quality%2C%20visual%20design%2C%20and%20structural%0Acoherence.%20Existing%20methods%20primarily%20focus%20on%20improving%20and%20evaluating%20the%0Acontent%20quality%20in%20isolation%2C%20often%20overlooking%20visual%20design%20and%20structural%0Acoherence%2C%20which%20limits%20their%20practical%20applicability.%20To%20address%20these%0Alimitations%2C%20we%20propose%20PPTAgent%2C%20which%20comprehensively%20improves%20presentation%0Ageneration%20through%20a%20two-stage%2C%20edit-based%20approach%20inspired%20by%20human%0Aworkflows.%20PPTAgent%20first%20analyzes%20reference%20presentations%20to%20understand%20their%0Astructural%20patterns%20and%20content%20schemas%2C%20then%20drafts%20outlines%20and%20generates%0Aslides%20through%20code%20actions%20to%20ensure%20consistency%20and%20alignment.%20To%0Acomprehensively%20evaluate%20the%20quality%20of%20generated%20presentations%2C%20we%20further%0Aintroduce%20PPTEval%2C%20an%20evaluation%20framework%20that%20assesses%20presentations%20across%0Athree%20dimensions%3A%20Content%2C%20Design%2C%20and%20Coherence.%20Experiments%20show%20that%0APPTAgent%20significantly%20outperforms%20traditional%20automatic%20presentation%0Ageneration%20methods%20across%20all%20three%20dimensions.%20The%20code%20and%20data%20are%20available%0Aat%20https%3A//github.com/icip-cas/PPTAgent.%0A&entry.1838667208=http%3A//arxiv.org/abs/2501.03936v1&entry.124074799=Read"},
{"title": "A Survey on Federated Learning in Human Sensing", "author": "Mohan Li and Martin Gjoreski and Pietro Barbiero and Ga\u0161per Slapni\u010dar and Mitja Lu\u0161trek and Nicholas D. Lane and Marc Langheinrich", "abstract": "  Human Sensing, a field that leverages technology to monitor human activities,\npsycho-physiological states, and interactions with the environment, enhances\nour understanding of human behavior and drives the development of advanced\nservices that improve overall quality of life. However, its reliance on\ndetailed and often privacy-sensitive data as the basis for its machine learning\n(ML) models raises significant legal and ethical concerns. The recently\nproposed ML approach of Federated Learning (FL) promises to alleviate many of\nthese concerns, as it is able to create accurate ML models without sending raw\nuser data to a central server. While FL has demonstrated its usefulness across\na variety of areas, such as text prediction and cyber security, its benefits in\nHuman Sensing are under-explored, given the particular challenges in this\ndomain. This survey conducts a comprehensive analysis of the current\nstate-of-the-art studies on FL in Human Sensing, and proposes a taxonomy and an\neight-dimensional assessment for FL approaches. Through the eight-dimensional\nassessment, we then evaluate whether the surveyed studies consider a specific\nFL-in-Human-Sensing challenge or not. Finally, based on the overall analysis,\nwe discuss open challenges and highlight five research aspects related to FL in\nHuman Sensing that require urgent research attention. Our work provides a\ncomprehensive corpus of FL studies and aims to assist FL practitioners in\ndeveloping and evaluating solutions that effectively address the real-world\ncomplexities of Human Sensing.\n", "link": "http://arxiv.org/abs/2501.04000v1", "date": "2025-01-07", "relevancy": 2.0382, "topK": [{"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.5307}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.5041}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.4906}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20A%20Survey%20on%20Federated%20Learning%20in%20Human%20Sensing&body=Title%3A%20A%20Survey%20on%20Federated%20Learning%20in%20Human%20Sensing%0AAuthor%3A%20Mohan%20Li%20and%20Martin%20Gjoreski%20and%20Pietro%20Barbiero%20and%20Ga%C5%A1per%20Slapni%C4%8Dar%20and%20Mitja%20Lu%C5%A1trek%20and%20Nicholas%20D.%20Lane%20and%20Marc%20Langheinrich%0AAbstract%3A%20%20%20Human%20Sensing%2C%20a%20field%20that%20leverages%20technology%20to%20monitor%20human%20activities%2C%0Apsycho-physiological%20states%2C%20and%20interactions%20with%20the%20environment%2C%20enhances%0Aour%20understanding%20of%20human%20behavior%20and%20drives%20the%20development%20of%20advanced%0Aservices%20that%20improve%20overall%20quality%20of%20life.%20However%2C%20its%20reliance%20on%0Adetailed%20and%20often%20privacy-sensitive%20data%20as%20the%20basis%20for%20its%20machine%20learning%0A%28ML%29%20models%20raises%20significant%20legal%20and%20ethical%20concerns.%20The%20recently%0Aproposed%20ML%20approach%20of%20Federated%20Learning%20%28FL%29%20promises%20to%20alleviate%20many%20of%0Athese%20concerns%2C%20as%20it%20is%20able%20to%20create%20accurate%20ML%20models%20without%20sending%20raw%0Auser%20data%20to%20a%20central%20server.%20While%20FL%20has%20demonstrated%20its%20usefulness%20across%0Aa%20variety%20of%20areas%2C%20such%20as%20text%20prediction%20and%20cyber%20security%2C%20its%20benefits%20in%0AHuman%20Sensing%20are%20under-explored%2C%20given%20the%20particular%20challenges%20in%20this%0Adomain.%20This%20survey%20conducts%20a%20comprehensive%20analysis%20of%20the%20current%0Astate-of-the-art%20studies%20on%20FL%20in%20Human%20Sensing%2C%20and%20proposes%20a%20taxonomy%20and%20an%0Aeight-dimensional%20assessment%20for%20FL%20approaches.%20Through%20the%20eight-dimensional%0Aassessment%2C%20we%20then%20evaluate%20whether%20the%20surveyed%20studies%20consider%20a%20specific%0AFL-in-Human-Sensing%20challenge%20or%20not.%20Finally%2C%20based%20on%20the%20overall%20analysis%2C%0Awe%20discuss%20open%20challenges%20and%20highlight%20five%20research%20aspects%20related%20to%20FL%20in%0AHuman%20Sensing%20that%20require%20urgent%20research%20attention.%20Our%20work%20provides%20a%0Acomprehensive%20corpus%20of%20FL%20studies%20and%20aims%20to%20assist%20FL%20practitioners%20in%0Adeveloping%20and%20evaluating%20solutions%20that%20effectively%20address%20the%20real-world%0Acomplexities%20of%20Human%20Sensing.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2501.04000v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DA%2520Survey%2520on%2520Federated%2520Learning%2520in%2520Human%2520Sensing%26entry.906535625%3DMohan%2520Li%2520and%2520Martin%2520Gjoreski%2520and%2520Pietro%2520Barbiero%2520and%2520Ga%25C5%25A1per%2520Slapni%25C4%258Dar%2520and%2520Mitja%2520Lu%25C5%25A1trek%2520and%2520Nicholas%2520D.%2520Lane%2520and%2520Marc%2520Langheinrich%26entry.1292438233%3D%2520%2520Human%2520Sensing%252C%2520a%2520field%2520that%2520leverages%2520technology%2520to%2520monitor%2520human%2520activities%252C%250Apsycho-physiological%2520states%252C%2520and%2520interactions%2520with%2520the%2520environment%252C%2520enhances%250Aour%2520understanding%2520of%2520human%2520behavior%2520and%2520drives%2520the%2520development%2520of%2520advanced%250Aservices%2520that%2520improve%2520overall%2520quality%2520of%2520life.%2520However%252C%2520its%2520reliance%2520on%250Adetailed%2520and%2520often%2520privacy-sensitive%2520data%2520as%2520the%2520basis%2520for%2520its%2520machine%2520learning%250A%2528ML%2529%2520models%2520raises%2520significant%2520legal%2520and%2520ethical%2520concerns.%2520The%2520recently%250Aproposed%2520ML%2520approach%2520of%2520Federated%2520Learning%2520%2528FL%2529%2520promises%2520to%2520alleviate%2520many%2520of%250Athese%2520concerns%252C%2520as%2520it%2520is%2520able%2520to%2520create%2520accurate%2520ML%2520models%2520without%2520sending%2520raw%250Auser%2520data%2520to%2520a%2520central%2520server.%2520While%2520FL%2520has%2520demonstrated%2520its%2520usefulness%2520across%250Aa%2520variety%2520of%2520areas%252C%2520such%2520as%2520text%2520prediction%2520and%2520cyber%2520security%252C%2520its%2520benefits%2520in%250AHuman%2520Sensing%2520are%2520under-explored%252C%2520given%2520the%2520particular%2520challenges%2520in%2520this%250Adomain.%2520This%2520survey%2520conducts%2520a%2520comprehensive%2520analysis%2520of%2520the%2520current%250Astate-of-the-art%2520studies%2520on%2520FL%2520in%2520Human%2520Sensing%252C%2520and%2520proposes%2520a%2520taxonomy%2520and%2520an%250Aeight-dimensional%2520assessment%2520for%2520FL%2520approaches.%2520Through%2520the%2520eight-dimensional%250Aassessment%252C%2520we%2520then%2520evaluate%2520whether%2520the%2520surveyed%2520studies%2520consider%2520a%2520specific%250AFL-in-Human-Sensing%2520challenge%2520or%2520not.%2520Finally%252C%2520based%2520on%2520the%2520overall%2520analysis%252C%250Awe%2520discuss%2520open%2520challenges%2520and%2520highlight%2520five%2520research%2520aspects%2520related%2520to%2520FL%2520in%250AHuman%2520Sensing%2520that%2520require%2520urgent%2520research%2520attention.%2520Our%2520work%2520provides%2520a%250Acomprehensive%2520corpus%2520of%2520FL%2520studies%2520and%2520aims%2520to%2520assist%2520FL%2520practitioners%2520in%250Adeveloping%2520and%2520evaluating%2520solutions%2520that%2520effectively%2520address%2520the%2520real-world%250Acomplexities%2520of%2520Human%2520Sensing.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2501.04000v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=A%20Survey%20on%20Federated%20Learning%20in%20Human%20Sensing&entry.906535625=Mohan%20Li%20and%20Martin%20Gjoreski%20and%20Pietro%20Barbiero%20and%20Ga%C5%A1per%20Slapni%C4%8Dar%20and%20Mitja%20Lu%C5%A1trek%20and%20Nicholas%20D.%20Lane%20and%20Marc%20Langheinrich&entry.1292438233=%20%20Human%20Sensing%2C%20a%20field%20that%20leverages%20technology%20to%20monitor%20human%20activities%2C%0Apsycho-physiological%20states%2C%20and%20interactions%20with%20the%20environment%2C%20enhances%0Aour%20understanding%20of%20human%20behavior%20and%20drives%20the%20development%20of%20advanced%0Aservices%20that%20improve%20overall%20quality%20of%20life.%20However%2C%20its%20reliance%20on%0Adetailed%20and%20often%20privacy-sensitive%20data%20as%20the%20basis%20for%20its%20machine%20learning%0A%28ML%29%20models%20raises%20significant%20legal%20and%20ethical%20concerns.%20The%20recently%0Aproposed%20ML%20approach%20of%20Federated%20Learning%20%28FL%29%20promises%20to%20alleviate%20many%20of%0Athese%20concerns%2C%20as%20it%20is%20able%20to%20create%20accurate%20ML%20models%20without%20sending%20raw%0Auser%20data%20to%20a%20central%20server.%20While%20FL%20has%20demonstrated%20its%20usefulness%20across%0Aa%20variety%20of%20areas%2C%20such%20as%20text%20prediction%20and%20cyber%20security%2C%20its%20benefits%20in%0AHuman%20Sensing%20are%20under-explored%2C%20given%20the%20particular%20challenges%20in%20this%0Adomain.%20This%20survey%20conducts%20a%20comprehensive%20analysis%20of%20the%20current%0Astate-of-the-art%20studies%20on%20FL%20in%20Human%20Sensing%2C%20and%20proposes%20a%20taxonomy%20and%20an%0Aeight-dimensional%20assessment%20for%20FL%20approaches.%20Through%20the%20eight-dimensional%0Aassessment%2C%20we%20then%20evaluate%20whether%20the%20surveyed%20studies%20consider%20a%20specific%0AFL-in-Human-Sensing%20challenge%20or%20not.%20Finally%2C%20based%20on%20the%20overall%20analysis%2C%0Awe%20discuss%20open%20challenges%20and%20highlight%20five%20research%20aspects%20related%20to%20FL%20in%0AHuman%20Sensing%20that%20require%20urgent%20research%20attention.%20Our%20work%20provides%20a%0Acomprehensive%20corpus%20of%20FL%20studies%20and%20aims%20to%20assist%20FL%20practitioners%20in%0Adeveloping%20and%20evaluating%20solutions%20that%20effectively%20address%20the%20real-world%0Acomplexities%20of%20Human%20Sensing.%0A&entry.1838667208=http%3A//arxiv.org/abs/2501.04000v1&entry.124074799=Read"},
{"title": "Hierarchical Light Transformer Ensembles for Multimodal Trajectory\n  Forecasting", "author": "Adrien Lafage and Mathieu Barbier and Gianni Franchi and David Filliat", "abstract": "  Accurate trajectory forecasting is crucial for the performance of various\nsystems, such as advanced driver-assistance systems and self-driving vehicles.\nThese forecasts allow us to anticipate events that lead to collisions and,\ntherefore, to mitigate them. Deep Neural Networks have excelled in motion\nforecasting, but overconfidence and weak uncertainty quantification persist.\nDeep Ensembles address these concerns, yet applying them to multimodal\ndistributions remains challenging. In this paper, we propose a novel approach\nnamed Hierarchical Light Transformer Ensembles (HLT-Ens) aimed at efficiently\ntraining an ensemble of Transformer architectures using a novel hierarchical\nloss function. HLT-Ens leverages grouped fully connected layers, inspired by\ngrouped convolution techniques, to capture multimodal distributions\neffectively. We demonstrate that HLT-Ens achieves state-of-the-art performance\nlevels through extensive experimentation, offering a promising avenue for\nimproving trajectory forecasting techniques.\n", "link": "http://arxiv.org/abs/2403.17678v3", "date": "2025-01-07", "relevancy": 2.0242, "topK": [{"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.5129}, {"title": "Synthesizing Moving People with 3D Control", "link": "http://arxiv.org/abs/2401.10889v2", "similarity": 0.5055}, {"title": "Boosted dynamic neural networks", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/26302", "similarity": 0.5038}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Hierarchical%20Light%20Transformer%20Ensembles%20for%20Multimodal%20Trajectory%0A%20%20Forecasting&body=Title%3A%20Hierarchical%20Light%20Transformer%20Ensembles%20for%20Multimodal%20Trajectory%0A%20%20Forecasting%0AAuthor%3A%20Adrien%20Lafage%20and%20Mathieu%20Barbier%20and%20Gianni%20Franchi%20and%20David%20Filliat%0AAbstract%3A%20%20%20Accurate%20trajectory%20forecasting%20is%20crucial%20for%20the%20performance%20of%20various%0Asystems%2C%20such%20as%20advanced%20driver-assistance%20systems%20and%20self-driving%20vehicles.%0AThese%20forecasts%20allow%20us%20to%20anticipate%20events%20that%20lead%20to%20collisions%20and%2C%0Atherefore%2C%20to%20mitigate%20them.%20Deep%20Neural%20Networks%20have%20excelled%20in%20motion%0Aforecasting%2C%20but%20overconfidence%20and%20weak%20uncertainty%20quantification%20persist.%0ADeep%20Ensembles%20address%20these%20concerns%2C%20yet%20applying%20them%20to%20multimodal%0Adistributions%20remains%20challenging.%20In%20this%20paper%2C%20we%20propose%20a%20novel%20approach%0Anamed%20Hierarchical%20Light%20Transformer%20Ensembles%20%28HLT-Ens%29%20aimed%20at%20efficiently%0Atraining%20an%20ensemble%20of%20Transformer%20architectures%20using%20a%20novel%20hierarchical%0Aloss%20function.%20HLT-Ens%20leverages%20grouped%20fully%20connected%20layers%2C%20inspired%20by%0Agrouped%20convolution%20techniques%2C%20to%20capture%20multimodal%20distributions%0Aeffectively.%20We%20demonstrate%20that%20HLT-Ens%20achieves%20state-of-the-art%20performance%0Alevels%20through%20extensive%20experimentation%2C%20offering%20a%20promising%20avenue%20for%0Aimproving%20trajectory%20forecasting%20techniques.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2403.17678v3%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DHierarchical%2520Light%2520Transformer%2520Ensembles%2520for%2520Multimodal%2520Trajectory%250A%2520%2520Forecasting%26entry.906535625%3DAdrien%2520Lafage%2520and%2520Mathieu%2520Barbier%2520and%2520Gianni%2520Franchi%2520and%2520David%2520Filliat%26entry.1292438233%3D%2520%2520Accurate%2520trajectory%2520forecasting%2520is%2520crucial%2520for%2520the%2520performance%2520of%2520various%250Asystems%252C%2520such%2520as%2520advanced%2520driver-assistance%2520systems%2520and%2520self-driving%2520vehicles.%250AThese%2520forecasts%2520allow%2520us%2520to%2520anticipate%2520events%2520that%2520lead%2520to%2520collisions%2520and%252C%250Atherefore%252C%2520to%2520mitigate%2520them.%2520Deep%2520Neural%2520Networks%2520have%2520excelled%2520in%2520motion%250Aforecasting%252C%2520but%2520overconfidence%2520and%2520weak%2520uncertainty%2520quantification%2520persist.%250ADeep%2520Ensembles%2520address%2520these%2520concerns%252C%2520yet%2520applying%2520them%2520to%2520multimodal%250Adistributions%2520remains%2520challenging.%2520In%2520this%2520paper%252C%2520we%2520propose%2520a%2520novel%2520approach%250Anamed%2520Hierarchical%2520Light%2520Transformer%2520Ensembles%2520%2528HLT-Ens%2529%2520aimed%2520at%2520efficiently%250Atraining%2520an%2520ensemble%2520of%2520Transformer%2520architectures%2520using%2520a%2520novel%2520hierarchical%250Aloss%2520function.%2520HLT-Ens%2520leverages%2520grouped%2520fully%2520connected%2520layers%252C%2520inspired%2520by%250Agrouped%2520convolution%2520techniques%252C%2520to%2520capture%2520multimodal%2520distributions%250Aeffectively.%2520We%2520demonstrate%2520that%2520HLT-Ens%2520achieves%2520state-of-the-art%2520performance%250Alevels%2520through%2520extensive%2520experimentation%252C%2520offering%2520a%2520promising%2520avenue%2520for%250Aimproving%2520trajectory%2520forecasting%2520techniques.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2403.17678v3%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Hierarchical%20Light%20Transformer%20Ensembles%20for%20Multimodal%20Trajectory%0A%20%20Forecasting&entry.906535625=Adrien%20Lafage%20and%20Mathieu%20Barbier%20and%20Gianni%20Franchi%20and%20David%20Filliat&entry.1292438233=%20%20Accurate%20trajectory%20forecasting%20is%20crucial%20for%20the%20performance%20of%20various%0Asystems%2C%20such%20as%20advanced%20driver-assistance%20systems%20and%20self-driving%20vehicles.%0AThese%20forecasts%20allow%20us%20to%20anticipate%20events%20that%20lead%20to%20collisions%20and%2C%0Atherefore%2C%20to%20mitigate%20them.%20Deep%20Neural%20Networks%20have%20excelled%20in%20motion%0Aforecasting%2C%20but%20overconfidence%20and%20weak%20uncertainty%20quantification%20persist.%0ADeep%20Ensembles%20address%20these%20concerns%2C%20yet%20applying%20them%20to%20multimodal%0Adistributions%20remains%20challenging.%20In%20this%20paper%2C%20we%20propose%20a%20novel%20approach%0Anamed%20Hierarchical%20Light%20Transformer%20Ensembles%20%28HLT-Ens%29%20aimed%20at%20efficiently%0Atraining%20an%20ensemble%20of%20Transformer%20architectures%20using%20a%20novel%20hierarchical%0Aloss%20function.%20HLT-Ens%20leverages%20grouped%20fully%20connected%20layers%2C%20inspired%20by%0Agrouped%20convolution%20techniques%2C%20to%20capture%20multimodal%20distributions%0Aeffectively.%20We%20demonstrate%20that%20HLT-Ens%20achieves%20state-of-the-art%20performance%0Alevels%20through%20extensive%20experimentation%2C%20offering%20a%20promising%20avenue%20for%0Aimproving%20trajectory%20forecasting%20techniques.%0A&entry.1838667208=http%3A//arxiv.org/abs/2403.17678v3&entry.124074799=Read"},
{"title": "Temporal Feature Weaving for Neonatal Echocardiographic Viewpoint Video\n  Classification", "author": "Satchel French and Faith Zhu and Amish Jain and Naimul Khan", "abstract": "  Automated viewpoint classification in echocardiograms can help\nunder-resourced clinics and hospitals in providing faster diagnosis and\nscreening when expert technicians may not be available. We propose a novel\napproach towards echocardiographic viewpoint classification. We show that\ntreating viewpoint classification as video classification rather than image\nclassification yields advantage. We propose a CNN-GRU architecture with a novel\ntemporal feature weaving method, which leverages both spatial and temporal\ninformation to yield a 4.33\\% increase in accuracy over baseline image\nclassification while using only four consecutive frames. The proposed approach\nincurs minimal computational overhead. Additionally, we publish the Neonatal\nEchocardiogram Dataset (NED), a professionally-annotated dataset providing\nsixteen viewpoints and associated echocardipgraphy videos to encourage future\nwork and development in this field. Code available at:\nhttps://github.com/satchelfrench/NED\n", "link": "http://arxiv.org/abs/2501.03967v1", "date": "2025-01-07", "relevancy": 2.0209, "topK": [{"title": "Weakly-guided self-supervised pretraining for temporal activity detection", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/25189", "similarity": 0.5139}, {"title": "Tuning-Free Noise Rectification for High Fidelity Image-to-Video\n  Generation", "link": "http://arxiv.org/abs/2403.02827v1", "similarity": 0.5067}, {"title": "Customize-A-Video: One-Shot Motion Customization of Text-to-Video\n  Diffusion Models", "link": "http://arxiv.org/abs/2402.14780v1", "similarity": 0.4799}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Temporal%20Feature%20Weaving%20for%20Neonatal%20Echocardiographic%20Viewpoint%20Video%0A%20%20Classification&body=Title%3A%20Temporal%20Feature%20Weaving%20for%20Neonatal%20Echocardiographic%20Viewpoint%20Video%0A%20%20Classification%0AAuthor%3A%20Satchel%20French%20and%20Faith%20Zhu%20and%20Amish%20Jain%20and%20Naimul%20Khan%0AAbstract%3A%20%20%20Automated%20viewpoint%20classification%20in%20echocardiograms%20can%20help%0Aunder-resourced%20clinics%20and%20hospitals%20in%20providing%20faster%20diagnosis%20and%0Ascreening%20when%20expert%20technicians%20may%20not%20be%20available.%20We%20propose%20a%20novel%0Aapproach%20towards%20echocardiographic%20viewpoint%20classification.%20We%20show%20that%0Atreating%20viewpoint%20classification%20as%20video%20classification%20rather%20than%20image%0Aclassification%20yields%20advantage.%20We%20propose%20a%20CNN-GRU%20architecture%20with%20a%20novel%0Atemporal%20feature%20weaving%20method%2C%20which%20leverages%20both%20spatial%20and%20temporal%0Ainformation%20to%20yield%20a%204.33%5C%25%20increase%20in%20accuracy%20over%20baseline%20image%0Aclassification%20while%20using%20only%20four%20consecutive%20frames.%20The%20proposed%20approach%0Aincurs%20minimal%20computational%20overhead.%20Additionally%2C%20we%20publish%20the%20Neonatal%0AEchocardiogram%20Dataset%20%28NED%29%2C%20a%20professionally-annotated%20dataset%20providing%0Asixteen%20viewpoints%20and%20associated%20echocardipgraphy%20videos%20to%20encourage%20future%0Awork%20and%20development%20in%20this%20field.%20Code%20available%20at%3A%0Ahttps%3A//github.com/satchelfrench/NED%0A%0ALink%3A%20http%3A//arxiv.org/abs/2501.03967v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DTemporal%2520Feature%2520Weaving%2520for%2520Neonatal%2520Echocardiographic%2520Viewpoint%2520Video%250A%2520%2520Classification%26entry.906535625%3DSatchel%2520French%2520and%2520Faith%2520Zhu%2520and%2520Amish%2520Jain%2520and%2520Naimul%2520Khan%26entry.1292438233%3D%2520%2520Automated%2520viewpoint%2520classification%2520in%2520echocardiograms%2520can%2520help%250Aunder-resourced%2520clinics%2520and%2520hospitals%2520in%2520providing%2520faster%2520diagnosis%2520and%250Ascreening%2520when%2520expert%2520technicians%2520may%2520not%2520be%2520available.%2520We%2520propose%2520a%2520novel%250Aapproach%2520towards%2520echocardiographic%2520viewpoint%2520classification.%2520We%2520show%2520that%250Atreating%2520viewpoint%2520classification%2520as%2520video%2520classification%2520rather%2520than%2520image%250Aclassification%2520yields%2520advantage.%2520We%2520propose%2520a%2520CNN-GRU%2520architecture%2520with%2520a%2520novel%250Atemporal%2520feature%2520weaving%2520method%252C%2520which%2520leverages%2520both%2520spatial%2520and%2520temporal%250Ainformation%2520to%2520yield%2520a%25204.33%255C%2525%2520increase%2520in%2520accuracy%2520over%2520baseline%2520image%250Aclassification%2520while%2520using%2520only%2520four%2520consecutive%2520frames.%2520The%2520proposed%2520approach%250Aincurs%2520minimal%2520computational%2520overhead.%2520Additionally%252C%2520we%2520publish%2520the%2520Neonatal%250AEchocardiogram%2520Dataset%2520%2528NED%2529%252C%2520a%2520professionally-annotated%2520dataset%2520providing%250Asixteen%2520viewpoints%2520and%2520associated%2520echocardipgraphy%2520videos%2520to%2520encourage%2520future%250Awork%2520and%2520development%2520in%2520this%2520field.%2520Code%2520available%2520at%253A%250Ahttps%253A//github.com/satchelfrench/NED%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2501.03967v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Temporal%20Feature%20Weaving%20for%20Neonatal%20Echocardiographic%20Viewpoint%20Video%0A%20%20Classification&entry.906535625=Satchel%20French%20and%20Faith%20Zhu%20and%20Amish%20Jain%20and%20Naimul%20Khan&entry.1292438233=%20%20Automated%20viewpoint%20classification%20in%20echocardiograms%20can%20help%0Aunder-resourced%20clinics%20and%20hospitals%20in%20providing%20faster%20diagnosis%20and%0Ascreening%20when%20expert%20technicians%20may%20not%20be%20available.%20We%20propose%20a%20novel%0Aapproach%20towards%20echocardiographic%20viewpoint%20classification.%20We%20show%20that%0Atreating%20viewpoint%20classification%20as%20video%20classification%20rather%20than%20image%0Aclassification%20yields%20advantage.%20We%20propose%20a%20CNN-GRU%20architecture%20with%20a%20novel%0Atemporal%20feature%20weaving%20method%2C%20which%20leverages%20both%20spatial%20and%20temporal%0Ainformation%20to%20yield%20a%204.33%5C%25%20increase%20in%20accuracy%20over%20baseline%20image%0Aclassification%20while%20using%20only%20four%20consecutive%20frames.%20The%20proposed%20approach%0Aincurs%20minimal%20computational%20overhead.%20Additionally%2C%20we%20publish%20the%20Neonatal%0AEchocardiogram%20Dataset%20%28NED%29%2C%20a%20professionally-annotated%20dataset%20providing%0Asixteen%20viewpoints%20and%20associated%20echocardipgraphy%20videos%20to%20encourage%20future%0Awork%20and%20development%20in%20this%20field.%20Code%20available%20at%3A%0Ahttps%3A//github.com/satchelfrench/NED%0A&entry.1838667208=http%3A//arxiv.org/abs/2501.03967v1&entry.124074799=Read"},
{"title": "Not all tokens are created equal: Perplexity Attention Weighted Networks\n  for AI generated text detection", "author": "Pablo Miralles-Gonz\u00e1lez and Javier Huertas-Tato and Alejandro Mart\u00edn and David Camacho", "abstract": "  The rapid advancement in large language models (LLMs) has significantly\nenhanced their ability to generate coherent and contextually relevant text,\nraising concerns about the misuse of AI-generated content and making it\ncritical to detect it. However, the task remains challenging, particularly in\nunseen domains or with unfamiliar LLMs. Leveraging LLM next-token distribution\noutputs offers a theoretically appealing approach for detection, as they\nencapsulate insights from the models' extensive pre-training on diverse\ncorpora. Despite its promise, zero-shot methods that attempt to operationalize\nthese outputs have met with limited success. We hypothesize that one of the\nproblems is that they use the mean to aggregate next-token distribution metrics\nacross tokens, when some tokens are naturally easier or harder to predict and\nshould be weighted differently. Based on this idea, we propose the Perplexity\nAttention Weighted Network (PAWN), which uses the last hidden states of the LLM\nand positions to weight the sum of a series of features based on metrics from\nthe next-token distribution across the sequence length. Although not zero-shot,\nour method allows us to cache the last hidden states and next-token\ndistribution metrics on disk, greatly reducing the training resource\nrequirements. PAWN shows competitive and even better performance\nin-distribution than the strongest baselines (fine-tuned LMs) with a fraction\nof their trainable parameters. Our model also generalizes better to unseen\ndomains and source models, with smaller variability in the decision boundary\nacross distribution shifts. It is also more robust to adversarial attacks, and\nif the backbone has multilingual capabilities, it presents decent\ngeneralization to languages not seen during supervised training, with LLaMA3-1B\nreaching a mean macro-averaged F1 score of 81.46% in cross-validation with nine\nlanguages.\n", "link": "http://arxiv.org/abs/2501.03940v1", "date": "2025-01-07", "relevancy": 2.0167, "topK": [{"title": "ToDo: Token Downsampling for Efficient Generation of High-Resolution\n  Images", "link": "http://arxiv.org/abs/2402.13573v2", "similarity": 0.5292}, {"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.496}, {"title": "Boosted dynamic neural networks", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/26302", "similarity": 0.4824}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Not%20all%20tokens%20are%20created%20equal%3A%20Perplexity%20Attention%20Weighted%20Networks%0A%20%20for%20AI%20generated%20text%20detection&body=Title%3A%20Not%20all%20tokens%20are%20created%20equal%3A%20Perplexity%20Attention%20Weighted%20Networks%0A%20%20for%20AI%20generated%20text%20detection%0AAuthor%3A%20Pablo%20Miralles-Gonz%C3%A1lez%20and%20Javier%20Huertas-Tato%20and%20Alejandro%20Mart%C3%ADn%20and%20David%20Camacho%0AAbstract%3A%20%20%20The%20rapid%20advancement%20in%20large%20language%20models%20%28LLMs%29%20has%20significantly%0Aenhanced%20their%20ability%20to%20generate%20coherent%20and%20contextually%20relevant%20text%2C%0Araising%20concerns%20about%20the%20misuse%20of%20AI-generated%20content%20and%20making%20it%0Acritical%20to%20detect%20it.%20However%2C%20the%20task%20remains%20challenging%2C%20particularly%20in%0Aunseen%20domains%20or%20with%20unfamiliar%20LLMs.%20Leveraging%20LLM%20next-token%20distribution%0Aoutputs%20offers%20a%20theoretically%20appealing%20approach%20for%20detection%2C%20as%20they%0Aencapsulate%20insights%20from%20the%20models%27%20extensive%20pre-training%20on%20diverse%0Acorpora.%20Despite%20its%20promise%2C%20zero-shot%20methods%20that%20attempt%20to%20operationalize%0Athese%20outputs%20have%20met%20with%20limited%20success.%20We%20hypothesize%20that%20one%20of%20the%0Aproblems%20is%20that%20they%20use%20the%20mean%20to%20aggregate%20next-token%20distribution%20metrics%0Aacross%20tokens%2C%20when%20some%20tokens%20are%20naturally%20easier%20or%20harder%20to%20predict%20and%0Ashould%20be%20weighted%20differently.%20Based%20on%20this%20idea%2C%20we%20propose%20the%20Perplexity%0AAttention%20Weighted%20Network%20%28PAWN%29%2C%20which%20uses%20the%20last%20hidden%20states%20of%20the%20LLM%0Aand%20positions%20to%20weight%20the%20sum%20of%20a%20series%20of%20features%20based%20on%20metrics%20from%0Athe%20next-token%20distribution%20across%20the%20sequence%20length.%20Although%20not%20zero-shot%2C%0Aour%20method%20allows%20us%20to%20cache%20the%20last%20hidden%20states%20and%20next-token%0Adistribution%20metrics%20on%20disk%2C%20greatly%20reducing%20the%20training%20resource%0Arequirements.%20PAWN%20shows%20competitive%20and%20even%20better%20performance%0Ain-distribution%20than%20the%20strongest%20baselines%20%28fine-tuned%20LMs%29%20with%20a%20fraction%0Aof%20their%20trainable%20parameters.%20Our%20model%20also%20generalizes%20better%20to%20unseen%0Adomains%20and%20source%20models%2C%20with%20smaller%20variability%20in%20the%20decision%20boundary%0Aacross%20distribution%20shifts.%20It%20is%20also%20more%20robust%20to%20adversarial%20attacks%2C%20and%0Aif%20the%20backbone%20has%20multilingual%20capabilities%2C%20it%20presents%20decent%0Ageneralization%20to%20languages%20not%20seen%20during%20supervised%20training%2C%20with%20LLaMA3-1B%0Areaching%20a%20mean%20macro-averaged%20F1%20score%20of%2081.46%25%20in%20cross-validation%20with%20nine%0Alanguages.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2501.03940v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DNot%2520all%2520tokens%2520are%2520created%2520equal%253A%2520Perplexity%2520Attention%2520Weighted%2520Networks%250A%2520%2520for%2520AI%2520generated%2520text%2520detection%26entry.906535625%3DPablo%2520Miralles-Gonz%25C3%25A1lez%2520and%2520Javier%2520Huertas-Tato%2520and%2520Alejandro%2520Mart%25C3%25ADn%2520and%2520David%2520Camacho%26entry.1292438233%3D%2520%2520The%2520rapid%2520advancement%2520in%2520large%2520language%2520models%2520%2528LLMs%2529%2520has%2520significantly%250Aenhanced%2520their%2520ability%2520to%2520generate%2520coherent%2520and%2520contextually%2520relevant%2520text%252C%250Araising%2520concerns%2520about%2520the%2520misuse%2520of%2520AI-generated%2520content%2520and%2520making%2520it%250Acritical%2520to%2520detect%2520it.%2520However%252C%2520the%2520task%2520remains%2520challenging%252C%2520particularly%2520in%250Aunseen%2520domains%2520or%2520with%2520unfamiliar%2520LLMs.%2520Leveraging%2520LLM%2520next-token%2520distribution%250Aoutputs%2520offers%2520a%2520theoretically%2520appealing%2520approach%2520for%2520detection%252C%2520as%2520they%250Aencapsulate%2520insights%2520from%2520the%2520models%2527%2520extensive%2520pre-training%2520on%2520diverse%250Acorpora.%2520Despite%2520its%2520promise%252C%2520zero-shot%2520methods%2520that%2520attempt%2520to%2520operationalize%250Athese%2520outputs%2520have%2520met%2520with%2520limited%2520success.%2520We%2520hypothesize%2520that%2520one%2520of%2520the%250Aproblems%2520is%2520that%2520they%2520use%2520the%2520mean%2520to%2520aggregate%2520next-token%2520distribution%2520metrics%250Aacross%2520tokens%252C%2520when%2520some%2520tokens%2520are%2520naturally%2520easier%2520or%2520harder%2520to%2520predict%2520and%250Ashould%2520be%2520weighted%2520differently.%2520Based%2520on%2520this%2520idea%252C%2520we%2520propose%2520the%2520Perplexity%250AAttention%2520Weighted%2520Network%2520%2528PAWN%2529%252C%2520which%2520uses%2520the%2520last%2520hidden%2520states%2520of%2520the%2520LLM%250Aand%2520positions%2520to%2520weight%2520the%2520sum%2520of%2520a%2520series%2520of%2520features%2520based%2520on%2520metrics%2520from%250Athe%2520next-token%2520distribution%2520across%2520the%2520sequence%2520length.%2520Although%2520not%2520zero-shot%252C%250Aour%2520method%2520allows%2520us%2520to%2520cache%2520the%2520last%2520hidden%2520states%2520and%2520next-token%250Adistribution%2520metrics%2520on%2520disk%252C%2520greatly%2520reducing%2520the%2520training%2520resource%250Arequirements.%2520PAWN%2520shows%2520competitive%2520and%2520even%2520better%2520performance%250Ain-distribution%2520than%2520the%2520strongest%2520baselines%2520%2528fine-tuned%2520LMs%2529%2520with%2520a%2520fraction%250Aof%2520their%2520trainable%2520parameters.%2520Our%2520model%2520also%2520generalizes%2520better%2520to%2520unseen%250Adomains%2520and%2520source%2520models%252C%2520with%2520smaller%2520variability%2520in%2520the%2520decision%2520boundary%250Aacross%2520distribution%2520shifts.%2520It%2520is%2520also%2520more%2520robust%2520to%2520adversarial%2520attacks%252C%2520and%250Aif%2520the%2520backbone%2520has%2520multilingual%2520capabilities%252C%2520it%2520presents%2520decent%250Ageneralization%2520to%2520languages%2520not%2520seen%2520during%2520supervised%2520training%252C%2520with%2520LLaMA3-1B%250Areaching%2520a%2520mean%2520macro-averaged%2520F1%2520score%2520of%252081.46%2525%2520in%2520cross-validation%2520with%2520nine%250Alanguages.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2501.03940v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Not%20all%20tokens%20are%20created%20equal%3A%20Perplexity%20Attention%20Weighted%20Networks%0A%20%20for%20AI%20generated%20text%20detection&entry.906535625=Pablo%20Miralles-Gonz%C3%A1lez%20and%20Javier%20Huertas-Tato%20and%20Alejandro%20Mart%C3%ADn%20and%20David%20Camacho&entry.1292438233=%20%20The%20rapid%20advancement%20in%20large%20language%20models%20%28LLMs%29%20has%20significantly%0Aenhanced%20their%20ability%20to%20generate%20coherent%20and%20contextually%20relevant%20text%2C%0Araising%20concerns%20about%20the%20misuse%20of%20AI-generated%20content%20and%20making%20it%0Acritical%20to%20detect%20it.%20However%2C%20the%20task%20remains%20challenging%2C%20particularly%20in%0Aunseen%20domains%20or%20with%20unfamiliar%20LLMs.%20Leveraging%20LLM%20next-token%20distribution%0Aoutputs%20offers%20a%20theoretically%20appealing%20approach%20for%20detection%2C%20as%20they%0Aencapsulate%20insights%20from%20the%20models%27%20extensive%20pre-training%20on%20diverse%0Acorpora.%20Despite%20its%20promise%2C%20zero-shot%20methods%20that%20attempt%20to%20operationalize%0Athese%20outputs%20have%20met%20with%20limited%20success.%20We%20hypothesize%20that%20one%20of%20the%0Aproblems%20is%20that%20they%20use%20the%20mean%20to%20aggregate%20next-token%20distribution%20metrics%0Aacross%20tokens%2C%20when%20some%20tokens%20are%20naturally%20easier%20or%20harder%20to%20predict%20and%0Ashould%20be%20weighted%20differently.%20Based%20on%20this%20idea%2C%20we%20propose%20the%20Perplexity%0AAttention%20Weighted%20Network%20%28PAWN%29%2C%20which%20uses%20the%20last%20hidden%20states%20of%20the%20LLM%0Aand%20positions%20to%20weight%20the%20sum%20of%20a%20series%20of%20features%20based%20on%20metrics%20from%0Athe%20next-token%20distribution%20across%20the%20sequence%20length.%20Although%20not%20zero-shot%2C%0Aour%20method%20allows%20us%20to%20cache%20the%20last%20hidden%20states%20and%20next-token%0Adistribution%20metrics%20on%20disk%2C%20greatly%20reducing%20the%20training%20resource%0Arequirements.%20PAWN%20shows%20competitive%20and%20even%20better%20performance%0Ain-distribution%20than%20the%20strongest%20baselines%20%28fine-tuned%20LMs%29%20with%20a%20fraction%0Aof%20their%20trainable%20parameters.%20Our%20model%20also%20generalizes%20better%20to%20unseen%0Adomains%20and%20source%20models%2C%20with%20smaller%20variability%20in%20the%20decision%20boundary%0Aacross%20distribution%20shifts.%20It%20is%20also%20more%20robust%20to%20adversarial%20attacks%2C%20and%0Aif%20the%20backbone%20has%20multilingual%20capabilities%2C%20it%20presents%20decent%0Ageneralization%20to%20languages%20not%20seen%20during%20supervised%20training%2C%20with%20LLaMA3-1B%0Areaching%20a%20mean%20macro-averaged%20F1%20score%20of%2081.46%25%20in%20cross-validation%20with%20nine%0Alanguages.%0A&entry.1838667208=http%3A//arxiv.org/abs/2501.03940v1&entry.124074799=Read"},
{"title": "Constrained Sampling with Primal-Dual Langevin Monte Carlo", "author": "Luiz F. O. Chamon and Mohammad Reza Karimi and Anna Korba", "abstract": "  This work considers the problem of sampling from a probability distribution\nknown up to a normalization constant while satisfying a set of statistical\nconstraints specified by the expected values of general nonlinear functions.\nThis problem finds applications in, e.g., Bayesian inference, where it can\nconstrain moments to evaluate counterfactual scenarios or enforce desiderata\nsuch as prediction fairness. Methods developed to handle support constraints,\nsuch as those based on mirror maps, barriers, and penalties, are not suited for\nthis task. This work therefore relies on gradient descent-ascent dynamics in\nWasserstein space to put forward a discrete-time primal-dual Langevin Monte\nCarlo algorithm (PD-LMC) that simultaneously constrains the target distribution\nand samples from it. We analyze the convergence of PD-LMC under standard\nassumptions on the target distribution and constraints, namely (strong)\nconvexity and log-Sobolev inequalities. To do so, we bring classical\noptimization arguments for saddle-point algorithms to the geometry of\nWasserstein space. We illustrate the relevance and effectiveness of PD-LMC in\nseveral applications.\n", "link": "http://arxiv.org/abs/2411.00568v2", "date": "2025-01-07", "relevancy": 1.9982, "topK": [{"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.5095}, {"title": "Dynamic Gaussians Mesh: Consistent Mesh Reconstruction from Monocular\n  Videos", "link": "http://arxiv.org/abs/2404.12379v2", "similarity": 0.496}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.4837}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Constrained%20Sampling%20with%20Primal-Dual%20Langevin%20Monte%20Carlo&body=Title%3A%20Constrained%20Sampling%20with%20Primal-Dual%20Langevin%20Monte%20Carlo%0AAuthor%3A%20Luiz%20F.%20O.%20Chamon%20and%20Mohammad%20Reza%20Karimi%20and%20Anna%20Korba%0AAbstract%3A%20%20%20This%20work%20considers%20the%20problem%20of%20sampling%20from%20a%20probability%20distribution%0Aknown%20up%20to%20a%20normalization%20constant%20while%20satisfying%20a%20set%20of%20statistical%0Aconstraints%20specified%20by%20the%20expected%20values%20of%20general%20nonlinear%20functions.%0AThis%20problem%20finds%20applications%20in%2C%20e.g.%2C%20Bayesian%20inference%2C%20where%20it%20can%0Aconstrain%20moments%20to%20evaluate%20counterfactual%20scenarios%20or%20enforce%20desiderata%0Asuch%20as%20prediction%20fairness.%20Methods%20developed%20to%20handle%20support%20constraints%2C%0Asuch%20as%20those%20based%20on%20mirror%20maps%2C%20barriers%2C%20and%20penalties%2C%20are%20not%20suited%20for%0Athis%20task.%20This%20work%20therefore%20relies%20on%20gradient%20descent-ascent%20dynamics%20in%0AWasserstein%20space%20to%20put%20forward%20a%20discrete-time%20primal-dual%20Langevin%20Monte%0ACarlo%20algorithm%20%28PD-LMC%29%20that%20simultaneously%20constrains%20the%20target%20distribution%0Aand%20samples%20from%20it.%20We%20analyze%20the%20convergence%20of%20PD-LMC%20under%20standard%0Aassumptions%20on%20the%20target%20distribution%20and%20constraints%2C%20namely%20%28strong%29%0Aconvexity%20and%20log-Sobolev%20inequalities.%20To%20do%20so%2C%20we%20bring%20classical%0Aoptimization%20arguments%20for%20saddle-point%20algorithms%20to%20the%20geometry%20of%0AWasserstein%20space.%20We%20illustrate%20the%20relevance%20and%20effectiveness%20of%20PD-LMC%20in%0Aseveral%20applications.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2411.00568v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DConstrained%2520Sampling%2520with%2520Primal-Dual%2520Langevin%2520Monte%2520Carlo%26entry.906535625%3DLuiz%2520F.%2520O.%2520Chamon%2520and%2520Mohammad%2520Reza%2520Karimi%2520and%2520Anna%2520Korba%26entry.1292438233%3D%2520%2520This%2520work%2520considers%2520the%2520problem%2520of%2520sampling%2520from%2520a%2520probability%2520distribution%250Aknown%2520up%2520to%2520a%2520normalization%2520constant%2520while%2520satisfying%2520a%2520set%2520of%2520statistical%250Aconstraints%2520specified%2520by%2520the%2520expected%2520values%2520of%2520general%2520nonlinear%2520functions.%250AThis%2520problem%2520finds%2520applications%2520in%252C%2520e.g.%252C%2520Bayesian%2520inference%252C%2520where%2520it%2520can%250Aconstrain%2520moments%2520to%2520evaluate%2520counterfactual%2520scenarios%2520or%2520enforce%2520desiderata%250Asuch%2520as%2520prediction%2520fairness.%2520Methods%2520developed%2520to%2520handle%2520support%2520constraints%252C%250Asuch%2520as%2520those%2520based%2520on%2520mirror%2520maps%252C%2520barriers%252C%2520and%2520penalties%252C%2520are%2520not%2520suited%2520for%250Athis%2520task.%2520This%2520work%2520therefore%2520relies%2520on%2520gradient%2520descent-ascent%2520dynamics%2520in%250AWasserstein%2520space%2520to%2520put%2520forward%2520a%2520discrete-time%2520primal-dual%2520Langevin%2520Monte%250ACarlo%2520algorithm%2520%2528PD-LMC%2529%2520that%2520simultaneously%2520constrains%2520the%2520target%2520distribution%250Aand%2520samples%2520from%2520it.%2520We%2520analyze%2520the%2520convergence%2520of%2520PD-LMC%2520under%2520standard%250Aassumptions%2520on%2520the%2520target%2520distribution%2520and%2520constraints%252C%2520namely%2520%2528strong%2529%250Aconvexity%2520and%2520log-Sobolev%2520inequalities.%2520To%2520do%2520so%252C%2520we%2520bring%2520classical%250Aoptimization%2520arguments%2520for%2520saddle-point%2520algorithms%2520to%2520the%2520geometry%2520of%250AWasserstein%2520space.%2520We%2520illustrate%2520the%2520relevance%2520and%2520effectiveness%2520of%2520PD-LMC%2520in%250Aseveral%2520applications.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2411.00568v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Constrained%20Sampling%20with%20Primal-Dual%20Langevin%20Monte%20Carlo&entry.906535625=Luiz%20F.%20O.%20Chamon%20and%20Mohammad%20Reza%20Karimi%20and%20Anna%20Korba&entry.1292438233=%20%20This%20work%20considers%20the%20problem%20of%20sampling%20from%20a%20probability%20distribution%0Aknown%20up%20to%20a%20normalization%20constant%20while%20satisfying%20a%20set%20of%20statistical%0Aconstraints%20specified%20by%20the%20expected%20values%20of%20general%20nonlinear%20functions.%0AThis%20problem%20finds%20applications%20in%2C%20e.g.%2C%20Bayesian%20inference%2C%20where%20it%20can%0Aconstrain%20moments%20to%20evaluate%20counterfactual%20scenarios%20or%20enforce%20desiderata%0Asuch%20as%20prediction%20fairness.%20Methods%20developed%20to%20handle%20support%20constraints%2C%0Asuch%20as%20those%20based%20on%20mirror%20maps%2C%20barriers%2C%20and%20penalties%2C%20are%20not%20suited%20for%0Athis%20task.%20This%20work%20therefore%20relies%20on%20gradient%20descent-ascent%20dynamics%20in%0AWasserstein%20space%20to%20put%20forward%20a%20discrete-time%20primal-dual%20Langevin%20Monte%0ACarlo%20algorithm%20%28PD-LMC%29%20that%20simultaneously%20constrains%20the%20target%20distribution%0Aand%20samples%20from%20it.%20We%20analyze%20the%20convergence%20of%20PD-LMC%20under%20standard%0Aassumptions%20on%20the%20target%20distribution%20and%20constraints%2C%20namely%20%28strong%29%0Aconvexity%20and%20log-Sobolev%20inequalities.%20To%20do%20so%2C%20we%20bring%20classical%0Aoptimization%20arguments%20for%20saddle-point%20algorithms%20to%20the%20geometry%20of%0AWasserstein%20space.%20We%20illustrate%20the%20relevance%20and%20effectiveness%20of%20PD-LMC%20in%0Aseveral%20applications.%0A&entry.1838667208=http%3A//arxiv.org/abs/2411.00568v2&entry.124074799=Read"},
{"title": "From Glucose Patterns to Health Outcomes: A Generalizable Foundation\n  Model for Continuous Glucose Monitor Data Analysis", "author": "Guy Lutsker and Gal Sapir and Smadar Shilo and Jordi Merino and Anastasia Godneva and Jerry R Greenfield and Dorit Samocha-Bonet and Raja Dhir and Francisco Gude and Shie Mannor and Eli Meirom and Gal Chechik and Hagai Rossman and Eran Segal", "abstract": "  Recent advances in SSL enabled novel medical AI models, known as foundation\nmodels, offer great potential for better characterizing health from diverse\nbiomedical data. CGM provides rich, temporal data on glycemic patterns, but its\nfull potential for predicting broader health outcomes remains underutilized.\nHere, we present GluFormer, a generative foundation model for CGM data that\nlearns nuanced glycemic patterns and translates them into predictive\nrepresentations of metabolic health. Trained on over 10 million CGM\nmeasurements from 10,812 adults, primarily without diabetes, GluFormer uses\nautoregressive token prediction to capture longitudinal glucose dynamics. We\nshow that GluFormer generalizes to 19 external cohorts (n=6,044) spanning\ndifferent ethnicities and ages, 5 countries, 8 CGM devices, and diverse\npathophysiological states. GluFormers representations exceed the performance of\ncurrent CGM metrics, such as the Glucose Management Indicator (GMI), for\nforecasting clinical measures. In a longitudinal study of 580 adults with CGM\ndata and 12-year follow-up, GluFormer identifies individuals at elevated risk\nof developing diabetes more effectively than blood HbA1C%, capturing 66% of all\nnew-onset diabetes diagnoses in the top quartile versus 7% in the bottom\nquartile. Similarly, 69% of cardiovascular-death events occurred in the top\nquartile with none in the bottom quartile, demonstrating powerful risk\nstratification beyond traditional glycemic metrics. We also show that CGM\nrepresentations from pre-intervention periods in Randomized Clinical Trials\noutperform other methods in predicting primary and secondary outcomes. When\nintegrating dietary data into GluFormer, we show that the multi-modal version\nof the model can accurately generate CGM data based on dietary intake data,\nsimulate outcomes of dietary interventions, and predict individual responses to\nspecific foods.\n", "link": "http://arxiv.org/abs/2408.11876v2", "date": "2025-01-07", "relevancy": 1.9959, "topK": [{"title": "GST: Precise 3D Human Body from a Single Image with Gaussian Splatting\n  Transformers", "link": "http://arxiv.org/abs/2409.04196v1", "similarity": 0.5218}, {"title": "UGG: Unified Generative Grasping", "link": "https://arxiv.org/abs/2311.16917", "similarity": 0.4858}, {"title": "Scaling Rectified Flow Transformers for High-Resolution Image Synthesis", "link": "http://arxiv.org/abs/2403.03206v1", "similarity": 0.4748}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20From%20Glucose%20Patterns%20to%20Health%20Outcomes%3A%20A%20Generalizable%20Foundation%0A%20%20Model%20for%20Continuous%20Glucose%20Monitor%20Data%20Analysis&body=Title%3A%20From%20Glucose%20Patterns%20to%20Health%20Outcomes%3A%20A%20Generalizable%20Foundation%0A%20%20Model%20for%20Continuous%20Glucose%20Monitor%20Data%20Analysis%0AAuthor%3A%20Guy%20Lutsker%20and%20Gal%20Sapir%20and%20Smadar%20Shilo%20and%20Jordi%20Merino%20and%20Anastasia%20Godneva%20and%20Jerry%20R%20Greenfield%20and%20Dorit%20Samocha-Bonet%20and%20Raja%20Dhir%20and%20Francisco%20Gude%20and%20Shie%20Mannor%20and%20Eli%20Meirom%20and%20Gal%20Chechik%20and%20Hagai%20Rossman%20and%20Eran%20Segal%0AAbstract%3A%20%20%20Recent%20advances%20in%20SSL%20enabled%20novel%20medical%20AI%20models%2C%20known%20as%20foundation%0Amodels%2C%20offer%20great%20potential%20for%20better%20characterizing%20health%20from%20diverse%0Abiomedical%20data.%20CGM%20provides%20rich%2C%20temporal%20data%20on%20glycemic%20patterns%2C%20but%20its%0Afull%20potential%20for%20predicting%20broader%20health%20outcomes%20remains%20underutilized.%0AHere%2C%20we%20present%20GluFormer%2C%20a%20generative%20foundation%20model%20for%20CGM%20data%20that%0Alearns%20nuanced%20glycemic%20patterns%20and%20translates%20them%20into%20predictive%0Arepresentations%20of%20metabolic%20health.%20Trained%20on%20over%2010%20million%20CGM%0Ameasurements%20from%2010%2C812%20adults%2C%20primarily%20without%20diabetes%2C%20GluFormer%20uses%0Aautoregressive%20token%20prediction%20to%20capture%20longitudinal%20glucose%20dynamics.%20We%0Ashow%20that%20GluFormer%20generalizes%20to%2019%20external%20cohorts%20%28n%3D6%2C044%29%20spanning%0Adifferent%20ethnicities%20and%20ages%2C%205%20countries%2C%208%20CGM%20devices%2C%20and%20diverse%0Apathophysiological%20states.%20GluFormers%20representations%20exceed%20the%20performance%20of%0Acurrent%20CGM%20metrics%2C%20such%20as%20the%20Glucose%20Management%20Indicator%20%28GMI%29%2C%20for%0Aforecasting%20clinical%20measures.%20In%20a%20longitudinal%20study%20of%20580%20adults%20with%20CGM%0Adata%20and%2012-year%20follow-up%2C%20GluFormer%20identifies%20individuals%20at%20elevated%20risk%0Aof%20developing%20diabetes%20more%20effectively%20than%20blood%20HbA1C%25%2C%20capturing%2066%25%20of%20all%0Anew-onset%20diabetes%20diagnoses%20in%20the%20top%20quartile%20versus%207%25%20in%20the%20bottom%0Aquartile.%20Similarly%2C%2069%25%20of%20cardiovascular-death%20events%20occurred%20in%20the%20top%0Aquartile%20with%20none%20in%20the%20bottom%20quartile%2C%20demonstrating%20powerful%20risk%0Astratification%20beyond%20traditional%20glycemic%20metrics.%20We%20also%20show%20that%20CGM%0Arepresentations%20from%20pre-intervention%20periods%20in%20Randomized%20Clinical%20Trials%0Aoutperform%20other%20methods%20in%20predicting%20primary%20and%20secondary%20outcomes.%20When%0Aintegrating%20dietary%20data%20into%20GluFormer%2C%20we%20show%20that%20the%20multi-modal%20version%0Aof%20the%20model%20can%20accurately%20generate%20CGM%20data%20based%20on%20dietary%20intake%20data%2C%0Asimulate%20outcomes%20of%20dietary%20interventions%2C%20and%20predict%20individual%20responses%20to%0Aspecific%20foods.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2408.11876v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DFrom%2520Glucose%2520Patterns%2520to%2520Health%2520Outcomes%253A%2520A%2520Generalizable%2520Foundation%250A%2520%2520Model%2520for%2520Continuous%2520Glucose%2520Monitor%2520Data%2520Analysis%26entry.906535625%3DGuy%2520Lutsker%2520and%2520Gal%2520Sapir%2520and%2520Smadar%2520Shilo%2520and%2520Jordi%2520Merino%2520and%2520Anastasia%2520Godneva%2520and%2520Jerry%2520R%2520Greenfield%2520and%2520Dorit%2520Samocha-Bonet%2520and%2520Raja%2520Dhir%2520and%2520Francisco%2520Gude%2520and%2520Shie%2520Mannor%2520and%2520Eli%2520Meirom%2520and%2520Gal%2520Chechik%2520and%2520Hagai%2520Rossman%2520and%2520Eran%2520Segal%26entry.1292438233%3D%2520%2520Recent%2520advances%2520in%2520SSL%2520enabled%2520novel%2520medical%2520AI%2520models%252C%2520known%2520as%2520foundation%250Amodels%252C%2520offer%2520great%2520potential%2520for%2520better%2520characterizing%2520health%2520from%2520diverse%250Abiomedical%2520data.%2520CGM%2520provides%2520rich%252C%2520temporal%2520data%2520on%2520glycemic%2520patterns%252C%2520but%2520its%250Afull%2520potential%2520for%2520predicting%2520broader%2520health%2520outcomes%2520remains%2520underutilized.%250AHere%252C%2520we%2520present%2520GluFormer%252C%2520a%2520generative%2520foundation%2520model%2520for%2520CGM%2520data%2520that%250Alearns%2520nuanced%2520glycemic%2520patterns%2520and%2520translates%2520them%2520into%2520predictive%250Arepresentations%2520of%2520metabolic%2520health.%2520Trained%2520on%2520over%252010%2520million%2520CGM%250Ameasurements%2520from%252010%252C812%2520adults%252C%2520primarily%2520without%2520diabetes%252C%2520GluFormer%2520uses%250Aautoregressive%2520token%2520prediction%2520to%2520capture%2520longitudinal%2520glucose%2520dynamics.%2520We%250Ashow%2520that%2520GluFormer%2520generalizes%2520to%252019%2520external%2520cohorts%2520%2528n%253D6%252C044%2529%2520spanning%250Adifferent%2520ethnicities%2520and%2520ages%252C%25205%2520countries%252C%25208%2520CGM%2520devices%252C%2520and%2520diverse%250Apathophysiological%2520states.%2520GluFormers%2520representations%2520exceed%2520the%2520performance%2520of%250Acurrent%2520CGM%2520metrics%252C%2520such%2520as%2520the%2520Glucose%2520Management%2520Indicator%2520%2528GMI%2529%252C%2520for%250Aforecasting%2520clinical%2520measures.%2520In%2520a%2520longitudinal%2520study%2520of%2520580%2520adults%2520with%2520CGM%250Adata%2520and%252012-year%2520follow-up%252C%2520GluFormer%2520identifies%2520individuals%2520at%2520elevated%2520risk%250Aof%2520developing%2520diabetes%2520more%2520effectively%2520than%2520blood%2520HbA1C%2525%252C%2520capturing%252066%2525%2520of%2520all%250Anew-onset%2520diabetes%2520diagnoses%2520in%2520the%2520top%2520quartile%2520versus%25207%2525%2520in%2520the%2520bottom%250Aquartile.%2520Similarly%252C%252069%2525%2520of%2520cardiovascular-death%2520events%2520occurred%2520in%2520the%2520top%250Aquartile%2520with%2520none%2520in%2520the%2520bottom%2520quartile%252C%2520demonstrating%2520powerful%2520risk%250Astratification%2520beyond%2520traditional%2520glycemic%2520metrics.%2520We%2520also%2520show%2520that%2520CGM%250Arepresentations%2520from%2520pre-intervention%2520periods%2520in%2520Randomized%2520Clinical%2520Trials%250Aoutperform%2520other%2520methods%2520in%2520predicting%2520primary%2520and%2520secondary%2520outcomes.%2520When%250Aintegrating%2520dietary%2520data%2520into%2520GluFormer%252C%2520we%2520show%2520that%2520the%2520multi-modal%2520version%250Aof%2520the%2520model%2520can%2520accurately%2520generate%2520CGM%2520data%2520based%2520on%2520dietary%2520intake%2520data%252C%250Asimulate%2520outcomes%2520of%2520dietary%2520interventions%252C%2520and%2520predict%2520individual%2520responses%2520to%250Aspecific%2520foods.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2408.11876v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=From%20Glucose%20Patterns%20to%20Health%20Outcomes%3A%20A%20Generalizable%20Foundation%0A%20%20Model%20for%20Continuous%20Glucose%20Monitor%20Data%20Analysis&entry.906535625=Guy%20Lutsker%20and%20Gal%20Sapir%20and%20Smadar%20Shilo%20and%20Jordi%20Merino%20and%20Anastasia%20Godneva%20and%20Jerry%20R%20Greenfield%20and%20Dorit%20Samocha-Bonet%20and%20Raja%20Dhir%20and%20Francisco%20Gude%20and%20Shie%20Mannor%20and%20Eli%20Meirom%20and%20Gal%20Chechik%20and%20Hagai%20Rossman%20and%20Eran%20Segal&entry.1292438233=%20%20Recent%20advances%20in%20SSL%20enabled%20novel%20medical%20AI%20models%2C%20known%20as%20foundation%0Amodels%2C%20offer%20great%20potential%20for%20better%20characterizing%20health%20from%20diverse%0Abiomedical%20data.%20CGM%20provides%20rich%2C%20temporal%20data%20on%20glycemic%20patterns%2C%20but%20its%0Afull%20potential%20for%20predicting%20broader%20health%20outcomes%20remains%20underutilized.%0AHere%2C%20we%20present%20GluFormer%2C%20a%20generative%20foundation%20model%20for%20CGM%20data%20that%0Alearns%20nuanced%20glycemic%20patterns%20and%20translates%20them%20into%20predictive%0Arepresentations%20of%20metabolic%20health.%20Trained%20on%20over%2010%20million%20CGM%0Ameasurements%20from%2010%2C812%20adults%2C%20primarily%20without%20diabetes%2C%20GluFormer%20uses%0Aautoregressive%20token%20prediction%20to%20capture%20longitudinal%20glucose%20dynamics.%20We%0Ashow%20that%20GluFormer%20generalizes%20to%2019%20external%20cohorts%20%28n%3D6%2C044%29%20spanning%0Adifferent%20ethnicities%20and%20ages%2C%205%20countries%2C%208%20CGM%20devices%2C%20and%20diverse%0Apathophysiological%20states.%20GluFormers%20representations%20exceed%20the%20performance%20of%0Acurrent%20CGM%20metrics%2C%20such%20as%20the%20Glucose%20Management%20Indicator%20%28GMI%29%2C%20for%0Aforecasting%20clinical%20measures.%20In%20a%20longitudinal%20study%20of%20580%20adults%20with%20CGM%0Adata%20and%2012-year%20follow-up%2C%20GluFormer%20identifies%20individuals%20at%20elevated%20risk%0Aof%20developing%20diabetes%20more%20effectively%20than%20blood%20HbA1C%25%2C%20capturing%2066%25%20of%20all%0Anew-onset%20diabetes%20diagnoses%20in%20the%20top%20quartile%20versus%207%25%20in%20the%20bottom%0Aquartile.%20Similarly%2C%2069%25%20of%20cardiovascular-death%20events%20occurred%20in%20the%20top%0Aquartile%20with%20none%20in%20the%20bottom%20quartile%2C%20demonstrating%20powerful%20risk%0Astratification%20beyond%20traditional%20glycemic%20metrics.%20We%20also%20show%20that%20CGM%0Arepresentations%20from%20pre-intervention%20periods%20in%20Randomized%20Clinical%20Trials%0Aoutperform%20other%20methods%20in%20predicting%20primary%20and%20secondary%20outcomes.%20When%0Aintegrating%20dietary%20data%20into%20GluFormer%2C%20we%20show%20that%20the%20multi-modal%20version%0Aof%20the%20model%20can%20accurately%20generate%20CGM%20data%20based%20on%20dietary%20intake%20data%2C%0Asimulate%20outcomes%20of%20dietary%20interventions%2C%20and%20predict%20individual%20responses%20to%0Aspecific%20foods.%0A&entry.1838667208=http%3A//arxiv.org/abs/2408.11876v2&entry.124074799=Read"},
{"title": "PRMBench: A Fine-grained and Challenging Benchmark for Process-Level\n  Reward Models", "author": "Mingyang Song and Zhaochen Su and Xiaoye Qu and Jiawei Zhou and Yu Cheng", "abstract": "  Process-level Reward Models (PRMs) are crucial for complex reasoning and\ndecision-making tasks, where each intermediate step plays an important role in\nthe reasoning process. Since language models are prone to various types of\nerrors during the reasoning process, PRMs are required to possess nuanced\ncapabilities for detecting various implicit error types in real-world\nscenarios. However, current benchmarks primarily focus on step correctness,\nfailing to evaluate PRMs' performance systematically. To address this gap, we\nintroduce PRMBench, a process-level benchmark specifically designed to assess\nthe fine-grained error detection capabilities of PRMs. PRMBench comprises 6,216\ncarefully designed problems and 83,456 step-level labels, evaluating models\nacross multiple dimensions, including simplicity, soundness, and sensitivity.\nIn our experiments on 15 models, spanning both open-source PRMs and\nclosed-source large language models prompted as critic models, we uncover\nsignificant weaknesses in current PRMs. These findings underscore the\nchallenges inherent in process-level evaluation and highlight key directions\nfor future research. We hope PRMBench can be a robust bench for advancing\nresearch on PRM evaluation and development.\n", "link": "http://arxiv.org/abs/2501.03124v2", "date": "2025-01-07", "relevancy": 1.9941, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5033}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5033}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.4745}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20PRMBench%3A%20A%20Fine-grained%20and%20Challenging%20Benchmark%20for%20Process-Level%0A%20%20Reward%20Models&body=Title%3A%20PRMBench%3A%20A%20Fine-grained%20and%20Challenging%20Benchmark%20for%20Process-Level%0A%20%20Reward%20Models%0AAuthor%3A%20Mingyang%20Song%20and%20Zhaochen%20Su%20and%20Xiaoye%20Qu%20and%20Jiawei%20Zhou%20and%20Yu%20Cheng%0AAbstract%3A%20%20%20Process-level%20Reward%20Models%20%28PRMs%29%20are%20crucial%20for%20complex%20reasoning%20and%0Adecision-making%20tasks%2C%20where%20each%20intermediate%20step%20plays%20an%20important%20role%20in%0Athe%20reasoning%20process.%20Since%20language%20models%20are%20prone%20to%20various%20types%20of%0Aerrors%20during%20the%20reasoning%20process%2C%20PRMs%20are%20required%20to%20possess%20nuanced%0Acapabilities%20for%20detecting%20various%20implicit%20error%20types%20in%20real-world%0Ascenarios.%20However%2C%20current%20benchmarks%20primarily%20focus%20on%20step%20correctness%2C%0Afailing%20to%20evaluate%20PRMs%27%20performance%20systematically.%20To%20address%20this%20gap%2C%20we%0Aintroduce%20PRMBench%2C%20a%20process-level%20benchmark%20specifically%20designed%20to%20assess%0Athe%20fine-grained%20error%20detection%20capabilities%20of%20PRMs.%20PRMBench%20comprises%206%2C216%0Acarefully%20designed%20problems%20and%2083%2C456%20step-level%20labels%2C%20evaluating%20models%0Aacross%20multiple%20dimensions%2C%20including%20simplicity%2C%20soundness%2C%20and%20sensitivity.%0AIn%20our%20experiments%20on%2015%20models%2C%20spanning%20both%20open-source%20PRMs%20and%0Aclosed-source%20large%20language%20models%20prompted%20as%20critic%20models%2C%20we%20uncover%0Asignificant%20weaknesses%20in%20current%20PRMs.%20These%20findings%20underscore%20the%0Achallenges%20inherent%20in%20process-level%20evaluation%20and%20highlight%20key%20directions%0Afor%20future%20research.%20We%20hope%20PRMBench%20can%20be%20a%20robust%20bench%20for%20advancing%0Aresearch%20on%20PRM%20evaluation%20and%20development.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2501.03124v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DPRMBench%253A%2520A%2520Fine-grained%2520and%2520Challenging%2520Benchmark%2520for%2520Process-Level%250A%2520%2520Reward%2520Models%26entry.906535625%3DMingyang%2520Song%2520and%2520Zhaochen%2520Su%2520and%2520Xiaoye%2520Qu%2520and%2520Jiawei%2520Zhou%2520and%2520Yu%2520Cheng%26entry.1292438233%3D%2520%2520Process-level%2520Reward%2520Models%2520%2528PRMs%2529%2520are%2520crucial%2520for%2520complex%2520reasoning%2520and%250Adecision-making%2520tasks%252C%2520where%2520each%2520intermediate%2520step%2520plays%2520an%2520important%2520role%2520in%250Athe%2520reasoning%2520process.%2520Since%2520language%2520models%2520are%2520prone%2520to%2520various%2520types%2520of%250Aerrors%2520during%2520the%2520reasoning%2520process%252C%2520PRMs%2520are%2520required%2520to%2520possess%2520nuanced%250Acapabilities%2520for%2520detecting%2520various%2520implicit%2520error%2520types%2520in%2520real-world%250Ascenarios.%2520However%252C%2520current%2520benchmarks%2520primarily%2520focus%2520on%2520step%2520correctness%252C%250Afailing%2520to%2520evaluate%2520PRMs%2527%2520performance%2520systematically.%2520To%2520address%2520this%2520gap%252C%2520we%250Aintroduce%2520PRMBench%252C%2520a%2520process-level%2520benchmark%2520specifically%2520designed%2520to%2520assess%250Athe%2520fine-grained%2520error%2520detection%2520capabilities%2520of%2520PRMs.%2520PRMBench%2520comprises%25206%252C216%250Acarefully%2520designed%2520problems%2520and%252083%252C456%2520step-level%2520labels%252C%2520evaluating%2520models%250Aacross%2520multiple%2520dimensions%252C%2520including%2520simplicity%252C%2520soundness%252C%2520and%2520sensitivity.%250AIn%2520our%2520experiments%2520on%252015%2520models%252C%2520spanning%2520both%2520open-source%2520PRMs%2520and%250Aclosed-source%2520large%2520language%2520models%2520prompted%2520as%2520critic%2520models%252C%2520we%2520uncover%250Asignificant%2520weaknesses%2520in%2520current%2520PRMs.%2520These%2520findings%2520underscore%2520the%250Achallenges%2520inherent%2520in%2520process-level%2520evaluation%2520and%2520highlight%2520key%2520directions%250Afor%2520future%2520research.%2520We%2520hope%2520PRMBench%2520can%2520be%2520a%2520robust%2520bench%2520for%2520advancing%250Aresearch%2520on%2520PRM%2520evaluation%2520and%2520development.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2501.03124v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=PRMBench%3A%20A%20Fine-grained%20and%20Challenging%20Benchmark%20for%20Process-Level%0A%20%20Reward%20Models&entry.906535625=Mingyang%20Song%20and%20Zhaochen%20Su%20and%20Xiaoye%20Qu%20and%20Jiawei%20Zhou%20and%20Yu%20Cheng&entry.1292438233=%20%20Process-level%20Reward%20Models%20%28PRMs%29%20are%20crucial%20for%20complex%20reasoning%20and%0Adecision-making%20tasks%2C%20where%20each%20intermediate%20step%20plays%20an%20important%20role%20in%0Athe%20reasoning%20process.%20Since%20language%20models%20are%20prone%20to%20various%20types%20of%0Aerrors%20during%20the%20reasoning%20process%2C%20PRMs%20are%20required%20to%20possess%20nuanced%0Acapabilities%20for%20detecting%20various%20implicit%20error%20types%20in%20real-world%0Ascenarios.%20However%2C%20current%20benchmarks%20primarily%20focus%20on%20step%20correctness%2C%0Afailing%20to%20evaluate%20PRMs%27%20performance%20systematically.%20To%20address%20this%20gap%2C%20we%0Aintroduce%20PRMBench%2C%20a%20process-level%20benchmark%20specifically%20designed%20to%20assess%0Athe%20fine-grained%20error%20detection%20capabilities%20of%20PRMs.%20PRMBench%20comprises%206%2C216%0Acarefully%20designed%20problems%20and%2083%2C456%20step-level%20labels%2C%20evaluating%20models%0Aacross%20multiple%20dimensions%2C%20including%20simplicity%2C%20soundness%2C%20and%20sensitivity.%0AIn%20our%20experiments%20on%2015%20models%2C%20spanning%20both%20open-source%20PRMs%20and%0Aclosed-source%20large%20language%20models%20prompted%20as%20critic%20models%2C%20we%20uncover%0Asignificant%20weaknesses%20in%20current%20PRMs.%20These%20findings%20underscore%20the%0Achallenges%20inherent%20in%20process-level%20evaluation%20and%20highlight%20key%20directions%0Afor%20future%20research.%20We%20hope%20PRMBench%20can%20be%20a%20robust%20bench%20for%20advancing%0Aresearch%20on%20PRM%20evaluation%20and%20development.%0A&entry.1838667208=http%3A//arxiv.org/abs/2501.03124v2&entry.124074799=Read"},
{"title": "SALE-Based Offline Reinforcement Learning with Ensemble Q-Networks", "author": "Zheng Chun", "abstract": "  In this work, we build upon the offline reinforcement learning algorithm TD7,\nwhich incorporates State-Action Learned Embeddings (SALE) and LAP, and propose\na model-free actor-critic algorithm that integrates ensemble Q-networks and a\ngradient diversity penalty from EDAC. The ensemble Q-networks effectively\naddress the challenge of out-of-distribution actions by introducing penalties\nthat guide the actor network to focus on in-distribution actions. Meanwhile,\nthe gradient diversity penalty encourages diverse Q-value gradients, further\nsuppressing overestimation for out-of-distribution actions. Additionally, our\nmethod retains an adjustable behavior cloning (BC) term that directs the actor\nnetwork toward dataset actions during early training stages, while gradually\nreducing its influence as the precision of the Q-ensemble improves. These\nenhancements work synergistically to improve training stability and accuracy.\nExperimental results on the D4RL MuJoCo benchmarks demonstrate that our\nalgorithm achieves superior convergence speed, stability, and performance\ncompared to existing methods.\n", "link": "http://arxiv.org/abs/2501.03676v1", "date": "2025-01-07", "relevancy": 1.9753, "topK": [{"title": "Boosted dynamic neural networks", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/26302", "similarity": 0.5227}, {"title": "DDM-NET: End-to-end learning of keypoint feature Detection, Description and Matching for 3D localization", "link": "https://arxiv.org/abs/2212.04575", "similarity": 0.4771}, {"title": "RoboEXP: Action-Conditioned Scene Graph via Interactive Exploration for\n  Robotic Manipulation", "link": "http://arxiv.org/abs/2402.15487v1", "similarity": 0.4635}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20SALE-Based%20Offline%20Reinforcement%20Learning%20with%20Ensemble%20Q-Networks&body=Title%3A%20SALE-Based%20Offline%20Reinforcement%20Learning%20with%20Ensemble%20Q-Networks%0AAuthor%3A%20Zheng%20Chun%0AAbstract%3A%20%20%20In%20this%20work%2C%20we%20build%20upon%20the%20offline%20reinforcement%20learning%20algorithm%20TD7%2C%0Awhich%20incorporates%20State-Action%20Learned%20Embeddings%20%28SALE%29%20and%20LAP%2C%20and%20propose%0Aa%20model-free%20actor-critic%20algorithm%20that%20integrates%20ensemble%20Q-networks%20and%20a%0Agradient%20diversity%20penalty%20from%20EDAC.%20The%20ensemble%20Q-networks%20effectively%0Aaddress%20the%20challenge%20of%20out-of-distribution%20actions%20by%20introducing%20penalties%0Athat%20guide%20the%20actor%20network%20to%20focus%20on%20in-distribution%20actions.%20Meanwhile%2C%0Athe%20gradient%20diversity%20penalty%20encourages%20diverse%20Q-value%20gradients%2C%20further%0Asuppressing%20overestimation%20for%20out-of-distribution%20actions.%20Additionally%2C%20our%0Amethod%20retains%20an%20adjustable%20behavior%20cloning%20%28BC%29%20term%20that%20directs%20the%20actor%0Anetwork%20toward%20dataset%20actions%20during%20early%20training%20stages%2C%20while%20gradually%0Areducing%20its%20influence%20as%20the%20precision%20of%20the%20Q-ensemble%20improves.%20These%0Aenhancements%20work%20synergistically%20to%20improve%20training%20stability%20and%20accuracy.%0AExperimental%20results%20on%20the%20D4RL%20MuJoCo%20benchmarks%20demonstrate%20that%20our%0Aalgorithm%20achieves%20superior%20convergence%20speed%2C%20stability%2C%20and%20performance%0Acompared%20to%20existing%20methods.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2501.03676v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DSALE-Based%2520Offline%2520Reinforcement%2520Learning%2520with%2520Ensemble%2520Q-Networks%26entry.906535625%3DZheng%2520Chun%26entry.1292438233%3D%2520%2520In%2520this%2520work%252C%2520we%2520build%2520upon%2520the%2520offline%2520reinforcement%2520learning%2520algorithm%2520TD7%252C%250Awhich%2520incorporates%2520State-Action%2520Learned%2520Embeddings%2520%2528SALE%2529%2520and%2520LAP%252C%2520and%2520propose%250Aa%2520model-free%2520actor-critic%2520algorithm%2520that%2520integrates%2520ensemble%2520Q-networks%2520and%2520a%250Agradient%2520diversity%2520penalty%2520from%2520EDAC.%2520The%2520ensemble%2520Q-networks%2520effectively%250Aaddress%2520the%2520challenge%2520of%2520out-of-distribution%2520actions%2520by%2520introducing%2520penalties%250Athat%2520guide%2520the%2520actor%2520network%2520to%2520focus%2520on%2520in-distribution%2520actions.%2520Meanwhile%252C%250Athe%2520gradient%2520diversity%2520penalty%2520encourages%2520diverse%2520Q-value%2520gradients%252C%2520further%250Asuppressing%2520overestimation%2520for%2520out-of-distribution%2520actions.%2520Additionally%252C%2520our%250Amethod%2520retains%2520an%2520adjustable%2520behavior%2520cloning%2520%2528BC%2529%2520term%2520that%2520directs%2520the%2520actor%250Anetwork%2520toward%2520dataset%2520actions%2520during%2520early%2520training%2520stages%252C%2520while%2520gradually%250Areducing%2520its%2520influence%2520as%2520the%2520precision%2520of%2520the%2520Q-ensemble%2520improves.%2520These%250Aenhancements%2520work%2520synergistically%2520to%2520improve%2520training%2520stability%2520and%2520accuracy.%250AExperimental%2520results%2520on%2520the%2520D4RL%2520MuJoCo%2520benchmarks%2520demonstrate%2520that%2520our%250Aalgorithm%2520achieves%2520superior%2520convergence%2520speed%252C%2520stability%252C%2520and%2520performance%250Acompared%2520to%2520existing%2520methods.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2501.03676v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=SALE-Based%20Offline%20Reinforcement%20Learning%20with%20Ensemble%20Q-Networks&entry.906535625=Zheng%20Chun&entry.1292438233=%20%20In%20this%20work%2C%20we%20build%20upon%20the%20offline%20reinforcement%20learning%20algorithm%20TD7%2C%0Awhich%20incorporates%20State-Action%20Learned%20Embeddings%20%28SALE%29%20and%20LAP%2C%20and%20propose%0Aa%20model-free%20actor-critic%20algorithm%20that%20integrates%20ensemble%20Q-networks%20and%20a%0Agradient%20diversity%20penalty%20from%20EDAC.%20The%20ensemble%20Q-networks%20effectively%0Aaddress%20the%20challenge%20of%20out-of-distribution%20actions%20by%20introducing%20penalties%0Athat%20guide%20the%20actor%20network%20to%20focus%20on%20in-distribution%20actions.%20Meanwhile%2C%0Athe%20gradient%20diversity%20penalty%20encourages%20diverse%20Q-value%20gradients%2C%20further%0Asuppressing%20overestimation%20for%20out-of-distribution%20actions.%20Additionally%2C%20our%0Amethod%20retains%20an%20adjustable%20behavior%20cloning%20%28BC%29%20term%20that%20directs%20the%20actor%0Anetwork%20toward%20dataset%20actions%20during%20early%20training%20stages%2C%20while%20gradually%0Areducing%20its%20influence%20as%20the%20precision%20of%20the%20Q-ensemble%20improves.%20These%0Aenhancements%20work%20synergistically%20to%20improve%20training%20stability%20and%20accuracy.%0AExperimental%20results%20on%20the%20D4RL%20MuJoCo%20benchmarks%20demonstrate%20that%20our%0Aalgorithm%20achieves%20superior%20convergence%20speed%2C%20stability%2C%20and%20performance%0Acompared%20to%20existing%20methods.%0A&entry.1838667208=http%3A//arxiv.org/abs/2501.03676v1&entry.124074799=Read"},
{"title": "BoRA: Bayesian Hierarchical Low-Rank Adaption for Multi-Task Large\n  Language Models", "author": "Simen Eide and Arnoldo Frigessi", "abstract": "  This paper introduces Bayesian Hierarchical Low-Rank Adaption (BoRA), a novel\nmethod for finetuning multi-task Large Language Models (LLMs). Current\nfinetuning approaches, such as Low-Rank Adaption (LoRA), perform exeptionally\nwell in reducing training parameters and memory usage but face limitations when\napplied to multiple similar tasks. Practitioners usually have to choose between\ntraining separate models for each task or a single model for all tasks, both of\nwhich come with trade-offs in specialization and data utilization. BoRA\naddresses these trade-offs by leveraging a Bayesian hierarchical model that\nallows tasks to share information through global hierarchical priors. This\nenables tasks with limited data to benefit from the overall structure derived\nfrom related tasks while allowing tasks with more data to specialize. Our\nexperimental results show that BoRA outperforms both individual and unified\nmodel approaches, achieving lower perplexity and better generalization across\ntasks. This method provides a scalable and efficient solution for multi-task\nLLM finetuning, with significant practical implications for diverse\napplications.\n", "link": "http://arxiv.org/abs/2407.15857v2", "date": "2025-01-07", "relevancy": 1.96, "topK": [{"title": "Breadcrumbs: Adversarial class-balanced sampling for long-tailed recognition", "link": "https://link.springer.com/chapter/10.1007/978-3-031-20053-3_37", "similarity": 0.4975}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.4913}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.482}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20BoRA%3A%20Bayesian%20Hierarchical%20Low-Rank%20Adaption%20for%20Multi-Task%20Large%0A%20%20Language%20Models&body=Title%3A%20BoRA%3A%20Bayesian%20Hierarchical%20Low-Rank%20Adaption%20for%20Multi-Task%20Large%0A%20%20Language%20Models%0AAuthor%3A%20Simen%20Eide%20and%20Arnoldo%20Frigessi%0AAbstract%3A%20%20%20This%20paper%20introduces%20Bayesian%20Hierarchical%20Low-Rank%20Adaption%20%28BoRA%29%2C%20a%20novel%0Amethod%20for%20finetuning%20multi-task%20Large%20Language%20Models%20%28LLMs%29.%20Current%0Afinetuning%20approaches%2C%20such%20as%20Low-Rank%20Adaption%20%28LoRA%29%2C%20perform%20exeptionally%0Awell%20in%20reducing%20training%20parameters%20and%20memory%20usage%20but%20face%20limitations%20when%0Aapplied%20to%20multiple%20similar%20tasks.%20Practitioners%20usually%20have%20to%20choose%20between%0Atraining%20separate%20models%20for%20each%20task%20or%20a%20single%20model%20for%20all%20tasks%2C%20both%20of%0Awhich%20come%20with%20trade-offs%20in%20specialization%20and%20data%20utilization.%20BoRA%0Aaddresses%20these%20trade-offs%20by%20leveraging%20a%20Bayesian%20hierarchical%20model%20that%0Aallows%20tasks%20to%20share%20information%20through%20global%20hierarchical%20priors.%20This%0Aenables%20tasks%20with%20limited%20data%20to%20benefit%20from%20the%20overall%20structure%20derived%0Afrom%20related%20tasks%20while%20allowing%20tasks%20with%20more%20data%20to%20specialize.%20Our%0Aexperimental%20results%20show%20that%20BoRA%20outperforms%20both%20individual%20and%20unified%0Amodel%20approaches%2C%20achieving%20lower%20perplexity%20and%20better%20generalization%20across%0Atasks.%20This%20method%20provides%20a%20scalable%20and%20efficient%20solution%20for%20multi-task%0ALLM%20finetuning%2C%20with%20significant%20practical%20implications%20for%20diverse%0Aapplications.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2407.15857v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DBoRA%253A%2520Bayesian%2520Hierarchical%2520Low-Rank%2520Adaption%2520for%2520Multi-Task%2520Large%250A%2520%2520Language%2520Models%26entry.906535625%3DSimen%2520Eide%2520and%2520Arnoldo%2520Frigessi%26entry.1292438233%3D%2520%2520This%2520paper%2520introduces%2520Bayesian%2520Hierarchical%2520Low-Rank%2520Adaption%2520%2528BoRA%2529%252C%2520a%2520novel%250Amethod%2520for%2520finetuning%2520multi-task%2520Large%2520Language%2520Models%2520%2528LLMs%2529.%2520Current%250Afinetuning%2520approaches%252C%2520such%2520as%2520Low-Rank%2520Adaption%2520%2528LoRA%2529%252C%2520perform%2520exeptionally%250Awell%2520in%2520reducing%2520training%2520parameters%2520and%2520memory%2520usage%2520but%2520face%2520limitations%2520when%250Aapplied%2520to%2520multiple%2520similar%2520tasks.%2520Practitioners%2520usually%2520have%2520to%2520choose%2520between%250Atraining%2520separate%2520models%2520for%2520each%2520task%2520or%2520a%2520single%2520model%2520for%2520all%2520tasks%252C%2520both%2520of%250Awhich%2520come%2520with%2520trade-offs%2520in%2520specialization%2520and%2520data%2520utilization.%2520BoRA%250Aaddresses%2520these%2520trade-offs%2520by%2520leveraging%2520a%2520Bayesian%2520hierarchical%2520model%2520that%250Aallows%2520tasks%2520to%2520share%2520information%2520through%2520global%2520hierarchical%2520priors.%2520This%250Aenables%2520tasks%2520with%2520limited%2520data%2520to%2520benefit%2520from%2520the%2520overall%2520structure%2520derived%250Afrom%2520related%2520tasks%2520while%2520allowing%2520tasks%2520with%2520more%2520data%2520to%2520specialize.%2520Our%250Aexperimental%2520results%2520show%2520that%2520BoRA%2520outperforms%2520both%2520individual%2520and%2520unified%250Amodel%2520approaches%252C%2520achieving%2520lower%2520perplexity%2520and%2520better%2520generalization%2520across%250Atasks.%2520This%2520method%2520provides%2520a%2520scalable%2520and%2520efficient%2520solution%2520for%2520multi-task%250ALLM%2520finetuning%252C%2520with%2520significant%2520practical%2520implications%2520for%2520diverse%250Aapplications.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2407.15857v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=BoRA%3A%20Bayesian%20Hierarchical%20Low-Rank%20Adaption%20for%20Multi-Task%20Large%0A%20%20Language%20Models&entry.906535625=Simen%20Eide%20and%20Arnoldo%20Frigessi&entry.1292438233=%20%20This%20paper%20introduces%20Bayesian%20Hierarchical%20Low-Rank%20Adaption%20%28BoRA%29%2C%20a%20novel%0Amethod%20for%20finetuning%20multi-task%20Large%20Language%20Models%20%28LLMs%29.%20Current%0Afinetuning%20approaches%2C%20such%20as%20Low-Rank%20Adaption%20%28LoRA%29%2C%20perform%20exeptionally%0Awell%20in%20reducing%20training%20parameters%20and%20memory%20usage%20but%20face%20limitations%20when%0Aapplied%20to%20multiple%20similar%20tasks.%20Practitioners%20usually%20have%20to%20choose%20between%0Atraining%20separate%20models%20for%20each%20task%20or%20a%20single%20model%20for%20all%20tasks%2C%20both%20of%0Awhich%20come%20with%20trade-offs%20in%20specialization%20and%20data%20utilization.%20BoRA%0Aaddresses%20these%20trade-offs%20by%20leveraging%20a%20Bayesian%20hierarchical%20model%20that%0Aallows%20tasks%20to%20share%20information%20through%20global%20hierarchical%20priors.%20This%0Aenables%20tasks%20with%20limited%20data%20to%20benefit%20from%20the%20overall%20structure%20derived%0Afrom%20related%20tasks%20while%20allowing%20tasks%20with%20more%20data%20to%20specialize.%20Our%0Aexperimental%20results%20show%20that%20BoRA%20outperforms%20both%20individual%20and%20unified%0Amodel%20approaches%2C%20achieving%20lower%20perplexity%20and%20better%20generalization%20across%0Atasks.%20This%20method%20provides%20a%20scalable%20and%20efficient%20solution%20for%20multi-task%0ALLM%20finetuning%2C%20with%20significant%20practical%20implications%20for%20diverse%0Aapplications.%0A&entry.1838667208=http%3A//arxiv.org/abs/2407.15857v2&entry.124074799=Read"},
{"title": "Superpixel Boundary Correction for Weakly-Supervised Semantic\n  Segmentation on Histopathology Images", "author": "Hongyi Wu and Hong Zhang", "abstract": "  With the rapid advancement of deep learning, computational pathology has made\nsignificant progress in cancer diagnosis and subtyping. Tissue segmentation is\na core challenge, essential for prognosis and treatment decisions. Weakly\nsupervised semantic segmentation (WSSS) reduces the annotation requirement by\nusing image-level labels instead of pixel-level ones. However, Class Activation\nMap (CAM)-based methods still suffer from low spatial resolution and unclear\nboundaries. To address these issues, we propose a multi-level superpixel\ncorrection algorithm that refines CAM boundaries using superpixel clustering\nand floodfill. Experimental results show that our method achieves great\nperformance on breast cancer segmentation dataset with mIoU of 71.08%,\nsignificantly improving tumor microenvironment boundary delineation.\n", "link": "http://arxiv.org/abs/2501.03891v1", "date": "2025-01-07", "relevancy": 1.9465, "topK": [{"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.4922}, {"title": "Self-supervised Photographic Image Layout Representation Learning", "link": "http://arxiv.org/abs/2403.03740v1", "similarity": 0.4841}, {"title": "SDXS: Real-Time One-Step Latent Diffusion Models with Image Conditions", "link": "http://arxiv.org/abs/2403.16627v1", "similarity": 0.479}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Superpixel%20Boundary%20Correction%20for%20Weakly-Supervised%20Semantic%0A%20%20Segmentation%20on%20Histopathology%20Images&body=Title%3A%20Superpixel%20Boundary%20Correction%20for%20Weakly-Supervised%20Semantic%0A%20%20Segmentation%20on%20Histopathology%20Images%0AAuthor%3A%20Hongyi%20Wu%20and%20Hong%20Zhang%0AAbstract%3A%20%20%20With%20the%20rapid%20advancement%20of%20deep%20learning%2C%20computational%20pathology%20has%20made%0Asignificant%20progress%20in%20cancer%20diagnosis%20and%20subtyping.%20Tissue%20segmentation%20is%0Aa%20core%20challenge%2C%20essential%20for%20prognosis%20and%20treatment%20decisions.%20Weakly%0Asupervised%20semantic%20segmentation%20%28WSSS%29%20reduces%20the%20annotation%20requirement%20by%0Ausing%20image-level%20labels%20instead%20of%20pixel-level%20ones.%20However%2C%20Class%20Activation%0AMap%20%28CAM%29-based%20methods%20still%20suffer%20from%20low%20spatial%20resolution%20and%20unclear%0Aboundaries.%20To%20address%20these%20issues%2C%20we%20propose%20a%20multi-level%20superpixel%0Acorrection%20algorithm%20that%20refines%20CAM%20boundaries%20using%20superpixel%20clustering%0Aand%20floodfill.%20Experimental%20results%20show%20that%20our%20method%20achieves%20great%0Aperformance%20on%20breast%20cancer%20segmentation%20dataset%20with%20mIoU%20of%2071.08%25%2C%0Asignificantly%20improving%20tumor%20microenvironment%20boundary%20delineation.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2501.03891v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DSuperpixel%2520Boundary%2520Correction%2520for%2520Weakly-Supervised%2520Semantic%250A%2520%2520Segmentation%2520on%2520Histopathology%2520Images%26entry.906535625%3DHongyi%2520Wu%2520and%2520Hong%2520Zhang%26entry.1292438233%3D%2520%2520With%2520the%2520rapid%2520advancement%2520of%2520deep%2520learning%252C%2520computational%2520pathology%2520has%2520made%250Asignificant%2520progress%2520in%2520cancer%2520diagnosis%2520and%2520subtyping.%2520Tissue%2520segmentation%2520is%250Aa%2520core%2520challenge%252C%2520essential%2520for%2520prognosis%2520and%2520treatment%2520decisions.%2520Weakly%250Asupervised%2520semantic%2520segmentation%2520%2528WSSS%2529%2520reduces%2520the%2520annotation%2520requirement%2520by%250Ausing%2520image-level%2520labels%2520instead%2520of%2520pixel-level%2520ones.%2520However%252C%2520Class%2520Activation%250AMap%2520%2528CAM%2529-based%2520methods%2520still%2520suffer%2520from%2520low%2520spatial%2520resolution%2520and%2520unclear%250Aboundaries.%2520To%2520address%2520these%2520issues%252C%2520we%2520propose%2520a%2520multi-level%2520superpixel%250Acorrection%2520algorithm%2520that%2520refines%2520CAM%2520boundaries%2520using%2520superpixel%2520clustering%250Aand%2520floodfill.%2520Experimental%2520results%2520show%2520that%2520our%2520method%2520achieves%2520great%250Aperformance%2520on%2520breast%2520cancer%2520segmentation%2520dataset%2520with%2520mIoU%2520of%252071.08%2525%252C%250Asignificantly%2520improving%2520tumor%2520microenvironment%2520boundary%2520delineation.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2501.03891v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Superpixel%20Boundary%20Correction%20for%20Weakly-Supervised%20Semantic%0A%20%20Segmentation%20on%20Histopathology%20Images&entry.906535625=Hongyi%20Wu%20and%20Hong%20Zhang&entry.1292438233=%20%20With%20the%20rapid%20advancement%20of%20deep%20learning%2C%20computational%20pathology%20has%20made%0Asignificant%20progress%20in%20cancer%20diagnosis%20and%20subtyping.%20Tissue%20segmentation%20is%0Aa%20core%20challenge%2C%20essential%20for%20prognosis%20and%20treatment%20decisions.%20Weakly%0Asupervised%20semantic%20segmentation%20%28WSSS%29%20reduces%20the%20annotation%20requirement%20by%0Ausing%20image-level%20labels%20instead%20of%20pixel-level%20ones.%20However%2C%20Class%20Activation%0AMap%20%28CAM%29-based%20methods%20still%20suffer%20from%20low%20spatial%20resolution%20and%20unclear%0Aboundaries.%20To%20address%20these%20issues%2C%20we%20propose%20a%20multi-level%20superpixel%0Acorrection%20algorithm%20that%20refines%20CAM%20boundaries%20using%20superpixel%20clustering%0Aand%20floodfill.%20Experimental%20results%20show%20that%20our%20method%20achieves%20great%0Aperformance%20on%20breast%20cancer%20segmentation%20dataset%20with%20mIoU%20of%2071.08%25%2C%0Asignificantly%20improving%20tumor%20microenvironment%20boundary%20delineation.%0A&entry.1838667208=http%3A//arxiv.org/abs/2501.03891v1&entry.124074799=Read"},
{"title": "Imitation Learning of MPC with Neural Networks: Error Guarantees and\n  Sparsification", "author": "Hendrik Alsmeier and Lukas Theiner and Anton Savchenko and Ali Mesbah and Rolf Findeisen", "abstract": "  This paper presents a framework for bounding the approximation error in\nimitation model predictive controllers utilizing neural networks. Leveraging\nthe Lipschitz properties of these neural networks, we derive a bound that\nguides dataset design to ensure the approximation error remains at chosen\nlimits. We discuss how this method can be used to design a stable neural\nnetwork controller with performance guarantees employing existing robust model\npredictive control approaches for data generation. Additionally, we introduce a\ntraining adjustment, which is based on the sensitivities of the optimization\nproblem and reduces dataset density requirements based on the derived bounds.\nWe verify that the proposed augmentation results in improvements to the\nnetwork's predictive capabilities and a reduction of the Lipschitz constant.\nMoreover, on a simulated inverted pendulum problem, we show that the approach\nresults in a closer match of the closed-loop behavior between the imitation and\nthe original model predictive controller.\n", "link": "http://arxiv.org/abs/2501.03671v1", "date": "2025-01-07", "relevancy": 1.9415, "topK": [{"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.5128}, {"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.4933}, {"title": "Safe and efficient training of a control agent", "link": "https://patents.google.com/patent/US11709462B2/en", "similarity": 0.4665}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Imitation%20Learning%20of%20MPC%20with%20Neural%20Networks%3A%20Error%20Guarantees%20and%0A%20%20Sparsification&body=Title%3A%20Imitation%20Learning%20of%20MPC%20with%20Neural%20Networks%3A%20Error%20Guarantees%20and%0A%20%20Sparsification%0AAuthor%3A%20Hendrik%20Alsmeier%20and%20Lukas%20Theiner%20and%20Anton%20Savchenko%20and%20Ali%20Mesbah%20and%20Rolf%20Findeisen%0AAbstract%3A%20%20%20This%20paper%20presents%20a%20framework%20for%20bounding%20the%20approximation%20error%20in%0Aimitation%20model%20predictive%20controllers%20utilizing%20neural%20networks.%20Leveraging%0Athe%20Lipschitz%20properties%20of%20these%20neural%20networks%2C%20we%20derive%20a%20bound%20that%0Aguides%20dataset%20design%20to%20ensure%20the%20approximation%20error%20remains%20at%20chosen%0Alimits.%20We%20discuss%20how%20this%20method%20can%20be%20used%20to%20design%20a%20stable%20neural%0Anetwork%20controller%20with%20performance%20guarantees%20employing%20existing%20robust%20model%0Apredictive%20control%20approaches%20for%20data%20generation.%20Additionally%2C%20we%20introduce%20a%0Atraining%20adjustment%2C%20which%20is%20based%20on%20the%20sensitivities%20of%20the%20optimization%0Aproblem%20and%20reduces%20dataset%20density%20requirements%20based%20on%20the%20derived%20bounds.%0AWe%20verify%20that%20the%20proposed%20augmentation%20results%20in%20improvements%20to%20the%0Anetwork%27s%20predictive%20capabilities%20and%20a%20reduction%20of%20the%20Lipschitz%20constant.%0AMoreover%2C%20on%20a%20simulated%20inverted%20pendulum%20problem%2C%20we%20show%20that%20the%20approach%0Aresults%20in%20a%20closer%20match%20of%20the%20closed-loop%20behavior%20between%20the%20imitation%20and%0Athe%20original%20model%20predictive%20controller.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2501.03671v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DImitation%2520Learning%2520of%2520MPC%2520with%2520Neural%2520Networks%253A%2520Error%2520Guarantees%2520and%250A%2520%2520Sparsification%26entry.906535625%3DHendrik%2520Alsmeier%2520and%2520Lukas%2520Theiner%2520and%2520Anton%2520Savchenko%2520and%2520Ali%2520Mesbah%2520and%2520Rolf%2520Findeisen%26entry.1292438233%3D%2520%2520This%2520paper%2520presents%2520a%2520framework%2520for%2520bounding%2520the%2520approximation%2520error%2520in%250Aimitation%2520model%2520predictive%2520controllers%2520utilizing%2520neural%2520networks.%2520Leveraging%250Athe%2520Lipschitz%2520properties%2520of%2520these%2520neural%2520networks%252C%2520we%2520derive%2520a%2520bound%2520that%250Aguides%2520dataset%2520design%2520to%2520ensure%2520the%2520approximation%2520error%2520remains%2520at%2520chosen%250Alimits.%2520We%2520discuss%2520how%2520this%2520method%2520can%2520be%2520used%2520to%2520design%2520a%2520stable%2520neural%250Anetwork%2520controller%2520with%2520performance%2520guarantees%2520employing%2520existing%2520robust%2520model%250Apredictive%2520control%2520approaches%2520for%2520data%2520generation.%2520Additionally%252C%2520we%2520introduce%2520a%250Atraining%2520adjustment%252C%2520which%2520is%2520based%2520on%2520the%2520sensitivities%2520of%2520the%2520optimization%250Aproblem%2520and%2520reduces%2520dataset%2520density%2520requirements%2520based%2520on%2520the%2520derived%2520bounds.%250AWe%2520verify%2520that%2520the%2520proposed%2520augmentation%2520results%2520in%2520improvements%2520to%2520the%250Anetwork%2527s%2520predictive%2520capabilities%2520and%2520a%2520reduction%2520of%2520the%2520Lipschitz%2520constant.%250AMoreover%252C%2520on%2520a%2520simulated%2520inverted%2520pendulum%2520problem%252C%2520we%2520show%2520that%2520the%2520approach%250Aresults%2520in%2520a%2520closer%2520match%2520of%2520the%2520closed-loop%2520behavior%2520between%2520the%2520imitation%2520and%250Athe%2520original%2520model%2520predictive%2520controller.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2501.03671v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Imitation%20Learning%20of%20MPC%20with%20Neural%20Networks%3A%20Error%20Guarantees%20and%0A%20%20Sparsification&entry.906535625=Hendrik%20Alsmeier%20and%20Lukas%20Theiner%20and%20Anton%20Savchenko%20and%20Ali%20Mesbah%20and%20Rolf%20Findeisen&entry.1292438233=%20%20This%20paper%20presents%20a%20framework%20for%20bounding%20the%20approximation%20error%20in%0Aimitation%20model%20predictive%20controllers%20utilizing%20neural%20networks.%20Leveraging%0Athe%20Lipschitz%20properties%20of%20these%20neural%20networks%2C%20we%20derive%20a%20bound%20that%0Aguides%20dataset%20design%20to%20ensure%20the%20approximation%20error%20remains%20at%20chosen%0Alimits.%20We%20discuss%20how%20this%20method%20can%20be%20used%20to%20design%20a%20stable%20neural%0Anetwork%20controller%20with%20performance%20guarantees%20employing%20existing%20robust%20model%0Apredictive%20control%20approaches%20for%20data%20generation.%20Additionally%2C%20we%20introduce%20a%0Atraining%20adjustment%2C%20which%20is%20based%20on%20the%20sensitivities%20of%20the%20optimization%0Aproblem%20and%20reduces%20dataset%20density%20requirements%20based%20on%20the%20derived%20bounds.%0AWe%20verify%20that%20the%20proposed%20augmentation%20results%20in%20improvements%20to%20the%0Anetwork%27s%20predictive%20capabilities%20and%20a%20reduction%20of%20the%20Lipschitz%20constant.%0AMoreover%2C%20on%20a%20simulated%20inverted%20pendulum%20problem%2C%20we%20show%20that%20the%20approach%0Aresults%20in%20a%20closer%20match%20of%20the%20closed-loop%20behavior%20between%20the%20imitation%20and%0Athe%20original%20model%20predictive%20controller.%0A&entry.1838667208=http%3A//arxiv.org/abs/2501.03671v1&entry.124074799=Read"},
{"title": "Efficient Generative Modeling via Penalized Optimal Transport Network", "author": "Wenhui Sophia Lu and Chenyang Zhong and Wing Hung Wong", "abstract": "  The generation of synthetic data with distributions that faithfully emulate\nthe underlying data-generating mechanism holds paramount significance.\nWasserstein Generative Adversarial Networks (WGANs) have emerged as a prominent\ntool for this task; however, due to the delicate equilibrium of the minimax\nformulation and the instability of Wasserstein distance in high dimensions,\nWGAN often manifests the pathological phenomenon of mode collapse. This results\nin generated samples that converge to a restricted set of outputs and fail to\nadequately capture the tail behaviors of the true distribution. Such\nlimitations can lead to serious downstream consequences. To this end, we\npropose the Penalized Optimal Transport Network (POTNet), a versatile deep\ngenerative model based on the marginally-penalized Wasserstein (MPW) distance.\nThrough the MPW distance, POTNet effectively leverages low-dimensional marginal\ninformation to guide the overall alignment of joint distributions. Furthermore,\nour primal-based framework enables direct evaluation of the MPW distance, thus\neliminating the need for a critic network. This formulation circumvents\ntraining instabilities inherent in adversarial approaches and avoids the need\nfor extensive parameter tuning. We derive a non-asymptotic bound on the\ngeneralization error of the MPW loss and establish convergence rates of the\ngenerative distribution learned by POTNet. Our theoretical analysis together\nwith extensive empirical evaluations demonstrate the superior performance of\nPOTNet in accurately capturing underlying data structures, including their tail\nbehaviors and minor modalities. Moreover, our model achieves orders of\nmagnitude speedup during the sampling stage compared to state-of-the-art\nalternatives, which enables computationally efficient large-scale synthetic\ndata generation.\n", "link": "http://arxiv.org/abs/2402.10456v2", "date": "2025-01-07", "relevancy": 1.6542, "topK": [{"title": "Thinking Outside the BBox: Unconstrained Generative Object Compositing", "link": "http://arxiv.org/abs/2409.04559v2", "similarity": 0.5835}, {"title": "Scaling Rectified Flow Transformers for High-Resolution Image Synthesis", "link": "http://arxiv.org/abs/2403.03206v1", "similarity": 0.5567}, {"title": "UGG: Unified Generative Grasping", "link": "https://arxiv.org/abs/2311.16917", "similarity": 0.5365}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Efficient%20Generative%20Modeling%20via%20Penalized%20Optimal%20Transport%20Network&body=Title%3A%20Efficient%20Generative%20Modeling%20via%20Penalized%20Optimal%20Transport%20Network%0AAuthor%3A%20Wenhui%20Sophia%20Lu%20and%20Chenyang%20Zhong%20and%20Wing%20Hung%20Wong%0AAbstract%3A%20%20%20The%20generation%20of%20synthetic%20data%20with%20distributions%20that%20faithfully%20emulate%0Athe%20underlying%20data-generating%20mechanism%20holds%20paramount%20significance.%0AWasserstein%20Generative%20Adversarial%20Networks%20%28WGANs%29%20have%20emerged%20as%20a%20prominent%0Atool%20for%20this%20task%3B%20however%2C%20due%20to%20the%20delicate%20equilibrium%20of%20the%20minimax%0Aformulation%20and%20the%20instability%20of%20Wasserstein%20distance%20in%20high%20dimensions%2C%0AWGAN%20often%20manifests%20the%20pathological%20phenomenon%20of%20mode%20collapse.%20This%20results%0Ain%20generated%20samples%20that%20converge%20to%20a%20restricted%20set%20of%20outputs%20and%20fail%20to%0Aadequately%20capture%20the%20tail%20behaviors%20of%20the%20true%20distribution.%20Such%0Alimitations%20can%20lead%20to%20serious%20downstream%20consequences.%20To%20this%20end%2C%20we%0Apropose%20the%20Penalized%20Optimal%20Transport%20Network%20%28POTNet%29%2C%20a%20versatile%20deep%0Agenerative%20model%20based%20on%20the%20marginally-penalized%20Wasserstein%20%28MPW%29%20distance.%0AThrough%20the%20MPW%20distance%2C%20POTNet%20effectively%20leverages%20low-dimensional%20marginal%0Ainformation%20to%20guide%20the%20overall%20alignment%20of%20joint%20distributions.%20Furthermore%2C%0Aour%20primal-based%20framework%20enables%20direct%20evaluation%20of%20the%20MPW%20distance%2C%20thus%0Aeliminating%20the%20need%20for%20a%20critic%20network.%20This%20formulation%20circumvents%0Atraining%20instabilities%20inherent%20in%20adversarial%20approaches%20and%20avoids%20the%20need%0Afor%20extensive%20parameter%20tuning.%20We%20derive%20a%20non-asymptotic%20bound%20on%20the%0Ageneralization%20error%20of%20the%20MPW%20loss%20and%20establish%20convergence%20rates%20of%20the%0Agenerative%20distribution%20learned%20by%20POTNet.%20Our%20theoretical%20analysis%20together%0Awith%20extensive%20empirical%20evaluations%20demonstrate%20the%20superior%20performance%20of%0APOTNet%20in%20accurately%20capturing%20underlying%20data%20structures%2C%20including%20their%20tail%0Abehaviors%20and%20minor%20modalities.%20Moreover%2C%20our%20model%20achieves%20orders%20of%0Amagnitude%20speedup%20during%20the%20sampling%20stage%20compared%20to%20state-of-the-art%0Aalternatives%2C%20which%20enables%20computationally%20efficient%20large-scale%20synthetic%0Adata%20generation.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2402.10456v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DEfficient%2520Generative%2520Modeling%2520via%2520Penalized%2520Optimal%2520Transport%2520Network%26entry.906535625%3DWenhui%2520Sophia%2520Lu%2520and%2520Chenyang%2520Zhong%2520and%2520Wing%2520Hung%2520Wong%26entry.1292438233%3D%2520%2520The%2520generation%2520of%2520synthetic%2520data%2520with%2520distributions%2520that%2520faithfully%2520emulate%250Athe%2520underlying%2520data-generating%2520mechanism%2520holds%2520paramount%2520significance.%250AWasserstein%2520Generative%2520Adversarial%2520Networks%2520%2528WGANs%2529%2520have%2520emerged%2520as%2520a%2520prominent%250Atool%2520for%2520this%2520task%253B%2520however%252C%2520due%2520to%2520the%2520delicate%2520equilibrium%2520of%2520the%2520minimax%250Aformulation%2520and%2520the%2520instability%2520of%2520Wasserstein%2520distance%2520in%2520high%2520dimensions%252C%250AWGAN%2520often%2520manifests%2520the%2520pathological%2520phenomenon%2520of%2520mode%2520collapse.%2520This%2520results%250Ain%2520generated%2520samples%2520that%2520converge%2520to%2520a%2520restricted%2520set%2520of%2520outputs%2520and%2520fail%2520to%250Aadequately%2520capture%2520the%2520tail%2520behaviors%2520of%2520the%2520true%2520distribution.%2520Such%250Alimitations%2520can%2520lead%2520to%2520serious%2520downstream%2520consequences.%2520To%2520this%2520end%252C%2520we%250Apropose%2520the%2520Penalized%2520Optimal%2520Transport%2520Network%2520%2528POTNet%2529%252C%2520a%2520versatile%2520deep%250Agenerative%2520model%2520based%2520on%2520the%2520marginally-penalized%2520Wasserstein%2520%2528MPW%2529%2520distance.%250AThrough%2520the%2520MPW%2520distance%252C%2520POTNet%2520effectively%2520leverages%2520low-dimensional%2520marginal%250Ainformation%2520to%2520guide%2520the%2520overall%2520alignment%2520of%2520joint%2520distributions.%2520Furthermore%252C%250Aour%2520primal-based%2520framework%2520enables%2520direct%2520evaluation%2520of%2520the%2520MPW%2520distance%252C%2520thus%250Aeliminating%2520the%2520need%2520for%2520a%2520critic%2520network.%2520This%2520formulation%2520circumvents%250Atraining%2520instabilities%2520inherent%2520in%2520adversarial%2520approaches%2520and%2520avoids%2520the%2520need%250Afor%2520extensive%2520parameter%2520tuning.%2520We%2520derive%2520a%2520non-asymptotic%2520bound%2520on%2520the%250Ageneralization%2520error%2520of%2520the%2520MPW%2520loss%2520and%2520establish%2520convergence%2520rates%2520of%2520the%250Agenerative%2520distribution%2520learned%2520by%2520POTNet.%2520Our%2520theoretical%2520analysis%2520together%250Awith%2520extensive%2520empirical%2520evaluations%2520demonstrate%2520the%2520superior%2520performance%2520of%250APOTNet%2520in%2520accurately%2520capturing%2520underlying%2520data%2520structures%252C%2520including%2520their%2520tail%250Abehaviors%2520and%2520minor%2520modalities.%2520Moreover%252C%2520our%2520model%2520achieves%2520orders%2520of%250Amagnitude%2520speedup%2520during%2520the%2520sampling%2520stage%2520compared%2520to%2520state-of-the-art%250Aalternatives%252C%2520which%2520enables%2520computationally%2520efficient%2520large-scale%2520synthetic%250Adata%2520generation.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2402.10456v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Efficient%20Generative%20Modeling%20via%20Penalized%20Optimal%20Transport%20Network&entry.906535625=Wenhui%20Sophia%20Lu%20and%20Chenyang%20Zhong%20and%20Wing%20Hung%20Wong&entry.1292438233=%20%20The%20generation%20of%20synthetic%20data%20with%20distributions%20that%20faithfully%20emulate%0Athe%20underlying%20data-generating%20mechanism%20holds%20paramount%20significance.%0AWasserstein%20Generative%20Adversarial%20Networks%20%28WGANs%29%20have%20emerged%20as%20a%20prominent%0Atool%20for%20this%20task%3B%20however%2C%20due%20to%20the%20delicate%20equilibrium%20of%20the%20minimax%0Aformulation%20and%20the%20instability%20of%20Wasserstein%20distance%20in%20high%20dimensions%2C%0AWGAN%20often%20manifests%20the%20pathological%20phenomenon%20of%20mode%20collapse.%20This%20results%0Ain%20generated%20samples%20that%20converge%20to%20a%20restricted%20set%20of%20outputs%20and%20fail%20to%0Aadequately%20capture%20the%20tail%20behaviors%20of%20the%20true%20distribution.%20Such%0Alimitations%20can%20lead%20to%20serious%20downstream%20consequences.%20To%20this%20end%2C%20we%0Apropose%20the%20Penalized%20Optimal%20Transport%20Network%20%28POTNet%29%2C%20a%20versatile%20deep%0Agenerative%20model%20based%20on%20the%20marginally-penalized%20Wasserstein%20%28MPW%29%20distance.%0AThrough%20the%20MPW%20distance%2C%20POTNet%20effectively%20leverages%20low-dimensional%20marginal%0Ainformation%20to%20guide%20the%20overall%20alignment%20of%20joint%20distributions.%20Furthermore%2C%0Aour%20primal-based%20framework%20enables%20direct%20evaluation%20of%20the%20MPW%20distance%2C%20thus%0Aeliminating%20the%20need%20for%20a%20critic%20network.%20This%20formulation%20circumvents%0Atraining%20instabilities%20inherent%20in%20adversarial%20approaches%20and%20avoids%20the%20need%0Afor%20extensive%20parameter%20tuning.%20We%20derive%20a%20non-asymptotic%20bound%20on%20the%0Ageneralization%20error%20of%20the%20MPW%20loss%20and%20establish%20convergence%20rates%20of%20the%0Agenerative%20distribution%20learned%20by%20POTNet.%20Our%20theoretical%20analysis%20together%0Awith%20extensive%20empirical%20evaluations%20demonstrate%20the%20superior%20performance%20of%0APOTNet%20in%20accurately%20capturing%20underlying%20data%20structures%2C%20including%20their%20tail%0Abehaviors%20and%20minor%20modalities.%20Moreover%2C%20our%20model%20achieves%20orders%20of%0Amagnitude%20speedup%20during%20the%20sampling%20stage%20compared%20to%20state-of-the-art%0Aalternatives%2C%20which%20enables%20computationally%20efficient%20large-scale%20synthetic%0Adata%20generation.%0A&entry.1838667208=http%3A//arxiv.org/abs/2402.10456v2&entry.124074799=Read"},
{"title": "OmniManip: Towards General Robotic Manipulation via Object-Centric\n  Interaction Primitives as Spatial Constraints", "author": "Mingjie Pan and Jiyao Zhang and Tianshu Wu and Yinghao Zhao and Wenlong Gao and Hao Dong", "abstract": "  The development of general robotic systems capable of manipulating in\nunstructured environments is a significant challenge. While Vision-Language\nModels(VLM) excel in high-level commonsense reasoning, they lack the\nfine-grained 3D spatial understanding required for precise manipulation tasks.\nFine-tuning VLM on robotic datasets to create Vision-Language-Action\nModels(VLA) is a potential solution, but it is hindered by high data collection\ncosts and generalization issues. To address these challenges, we propose a\nnovel object-centric representation that bridges the gap between VLM's\nhigh-level reasoning and the low-level precision required for manipulation. Our\nkey insight is that an object's canonical space, defined by its functional\naffordances, provides a structured and semantically meaningful way to describe\ninteraction primitives, such as points and directions. These primitives act as\na bridge, translating VLM's commonsense reasoning into actionable 3D spatial\nconstraints. In this context, we introduce a dual closed-loop, open-vocabulary\nrobotic manipulation system: one loop for high-level planning through primitive\nresampling, interaction rendering and VLM checking, and another for low-level\nexecution via 6D pose tracking. This design ensures robust, real-time control\nwithout requiring VLM fine-tuning. Extensive experiments demonstrate strong\nzero-shot generalization across diverse robotic manipulation tasks,\nhighlighting the potential of this approach for automating large-scale\nsimulation data generation.\n", "link": "http://arxiv.org/abs/2501.03841v1", "date": "2025-01-07", "relevancy": 1.819, "topK": [{"title": "RoboEXP: Action-Conditioned Scene Graph via Interactive Exploration for\n  Robotic Manipulation", "link": "http://arxiv.org/abs/2402.15487v1", "similarity": 0.6617}, {"title": "Egocentric Computer Vision for Hands-Free Robotic Wheelchair Navigation", "link": "https://link.springer.com/article/10.1007/s10846-023-01807-4", "similarity": 0.5946}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.5802}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20OmniManip%3A%20Towards%20General%20Robotic%20Manipulation%20via%20Object-Centric%0A%20%20Interaction%20Primitives%20as%20Spatial%20Constraints&body=Title%3A%20OmniManip%3A%20Towards%20General%20Robotic%20Manipulation%20via%20Object-Centric%0A%20%20Interaction%20Primitives%20as%20Spatial%20Constraints%0AAuthor%3A%20Mingjie%20Pan%20and%20Jiyao%20Zhang%20and%20Tianshu%20Wu%20and%20Yinghao%20Zhao%20and%20Wenlong%20Gao%20and%20Hao%20Dong%0AAbstract%3A%20%20%20The%20development%20of%20general%20robotic%20systems%20capable%20of%20manipulating%20in%0Aunstructured%20environments%20is%20a%20significant%20challenge.%20While%20Vision-Language%0AModels%28VLM%29%20excel%20in%20high-level%20commonsense%20reasoning%2C%20they%20lack%20the%0Afine-grained%203D%20spatial%20understanding%20required%20for%20precise%20manipulation%20tasks.%0AFine-tuning%20VLM%20on%20robotic%20datasets%20to%20create%20Vision-Language-Action%0AModels%28VLA%29%20is%20a%20potential%20solution%2C%20but%20it%20is%20hindered%20by%20high%20data%20collection%0Acosts%20and%20generalization%20issues.%20To%20address%20these%20challenges%2C%20we%20propose%20a%0Anovel%20object-centric%20representation%20that%20bridges%20the%20gap%20between%20VLM%27s%0Ahigh-level%20reasoning%20and%20the%20low-level%20precision%20required%20for%20manipulation.%20Our%0Akey%20insight%20is%20that%20an%20object%27s%20canonical%20space%2C%20defined%20by%20its%20functional%0Aaffordances%2C%20provides%20a%20structured%20and%20semantically%20meaningful%20way%20to%20describe%0Ainteraction%20primitives%2C%20such%20as%20points%20and%20directions.%20These%20primitives%20act%20as%0Aa%20bridge%2C%20translating%20VLM%27s%20commonsense%20reasoning%20into%20actionable%203D%20spatial%0Aconstraints.%20In%20this%20context%2C%20we%20introduce%20a%20dual%20closed-loop%2C%20open-vocabulary%0Arobotic%20manipulation%20system%3A%20one%20loop%20for%20high-level%20planning%20through%20primitive%0Aresampling%2C%20interaction%20rendering%20and%20VLM%20checking%2C%20and%20another%20for%20low-level%0Aexecution%20via%206D%20pose%20tracking.%20This%20design%20ensures%20robust%2C%20real-time%20control%0Awithout%20requiring%20VLM%20fine-tuning.%20Extensive%20experiments%20demonstrate%20strong%0Azero-shot%20generalization%20across%20diverse%20robotic%20manipulation%20tasks%2C%0Ahighlighting%20the%20potential%20of%20this%20approach%20for%20automating%20large-scale%0Asimulation%20data%20generation.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2501.03841v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DOmniManip%253A%2520Towards%2520General%2520Robotic%2520Manipulation%2520via%2520Object-Centric%250A%2520%2520Interaction%2520Primitives%2520as%2520Spatial%2520Constraints%26entry.906535625%3DMingjie%2520Pan%2520and%2520Jiyao%2520Zhang%2520and%2520Tianshu%2520Wu%2520and%2520Yinghao%2520Zhao%2520and%2520Wenlong%2520Gao%2520and%2520Hao%2520Dong%26entry.1292438233%3D%2520%2520The%2520development%2520of%2520general%2520robotic%2520systems%2520capable%2520of%2520manipulating%2520in%250Aunstructured%2520environments%2520is%2520a%2520significant%2520challenge.%2520While%2520Vision-Language%250AModels%2528VLM%2529%2520excel%2520in%2520high-level%2520commonsense%2520reasoning%252C%2520they%2520lack%2520the%250Afine-grained%25203D%2520spatial%2520understanding%2520required%2520for%2520precise%2520manipulation%2520tasks.%250AFine-tuning%2520VLM%2520on%2520robotic%2520datasets%2520to%2520create%2520Vision-Language-Action%250AModels%2528VLA%2529%2520is%2520a%2520potential%2520solution%252C%2520but%2520it%2520is%2520hindered%2520by%2520high%2520data%2520collection%250Acosts%2520and%2520generalization%2520issues.%2520To%2520address%2520these%2520challenges%252C%2520we%2520propose%2520a%250Anovel%2520object-centric%2520representation%2520that%2520bridges%2520the%2520gap%2520between%2520VLM%2527s%250Ahigh-level%2520reasoning%2520and%2520the%2520low-level%2520precision%2520required%2520for%2520manipulation.%2520Our%250Akey%2520insight%2520is%2520that%2520an%2520object%2527s%2520canonical%2520space%252C%2520defined%2520by%2520its%2520functional%250Aaffordances%252C%2520provides%2520a%2520structured%2520and%2520semantically%2520meaningful%2520way%2520to%2520describe%250Ainteraction%2520primitives%252C%2520such%2520as%2520points%2520and%2520directions.%2520These%2520primitives%2520act%2520as%250Aa%2520bridge%252C%2520translating%2520VLM%2527s%2520commonsense%2520reasoning%2520into%2520actionable%25203D%2520spatial%250Aconstraints.%2520In%2520this%2520context%252C%2520we%2520introduce%2520a%2520dual%2520closed-loop%252C%2520open-vocabulary%250Arobotic%2520manipulation%2520system%253A%2520one%2520loop%2520for%2520high-level%2520planning%2520through%2520primitive%250Aresampling%252C%2520interaction%2520rendering%2520and%2520VLM%2520checking%252C%2520and%2520another%2520for%2520low-level%250Aexecution%2520via%25206D%2520pose%2520tracking.%2520This%2520design%2520ensures%2520robust%252C%2520real-time%2520control%250Awithout%2520requiring%2520VLM%2520fine-tuning.%2520Extensive%2520experiments%2520demonstrate%2520strong%250Azero-shot%2520generalization%2520across%2520diverse%2520robotic%2520manipulation%2520tasks%252C%250Ahighlighting%2520the%2520potential%2520of%2520this%2520approach%2520for%2520automating%2520large-scale%250Asimulation%2520data%2520generation.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2501.03841v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=OmniManip%3A%20Towards%20General%20Robotic%20Manipulation%20via%20Object-Centric%0A%20%20Interaction%20Primitives%20as%20Spatial%20Constraints&entry.906535625=Mingjie%20Pan%20and%20Jiyao%20Zhang%20and%20Tianshu%20Wu%20and%20Yinghao%20Zhao%20and%20Wenlong%20Gao%20and%20Hao%20Dong&entry.1292438233=%20%20The%20development%20of%20general%20robotic%20systems%20capable%20of%20manipulating%20in%0Aunstructured%20environments%20is%20a%20significant%20challenge.%20While%20Vision-Language%0AModels%28VLM%29%20excel%20in%20high-level%20commonsense%20reasoning%2C%20they%20lack%20the%0Afine-grained%203D%20spatial%20understanding%20required%20for%20precise%20manipulation%20tasks.%0AFine-tuning%20VLM%20on%20robotic%20datasets%20to%20create%20Vision-Language-Action%0AModels%28VLA%29%20is%20a%20potential%20solution%2C%20but%20it%20is%20hindered%20by%20high%20data%20collection%0Acosts%20and%20generalization%20issues.%20To%20address%20these%20challenges%2C%20we%20propose%20a%0Anovel%20object-centric%20representation%20that%20bridges%20the%20gap%20between%20VLM%27s%0Ahigh-level%20reasoning%20and%20the%20low-level%20precision%20required%20for%20manipulation.%20Our%0Akey%20insight%20is%20that%20an%20object%27s%20canonical%20space%2C%20defined%20by%20its%20functional%0Aaffordances%2C%20provides%20a%20structured%20and%20semantically%20meaningful%20way%20to%20describe%0Ainteraction%20primitives%2C%20such%20as%20points%20and%20directions.%20These%20primitives%20act%20as%0Aa%20bridge%2C%20translating%20VLM%27s%20commonsense%20reasoning%20into%20actionable%203D%20spatial%0Aconstraints.%20In%20this%20context%2C%20we%20introduce%20a%20dual%20closed-loop%2C%20open-vocabulary%0Arobotic%20manipulation%20system%3A%20one%20loop%20for%20high-level%20planning%20through%20primitive%0Aresampling%2C%20interaction%20rendering%20and%20VLM%20checking%2C%20and%20another%20for%20low-level%0Aexecution%20via%206D%20pose%20tracking.%20This%20design%20ensures%20robust%2C%20real-time%20control%0Awithout%20requiring%20VLM%20fine-tuning.%20Extensive%20experiments%20demonstrate%20strong%0Azero-shot%20generalization%20across%20diverse%20robotic%20manipulation%20tasks%2C%0Ahighlighting%20the%20potential%20of%20this%20approach%20for%20automating%20large-scale%0Asimulation%20data%20generation.%0A&entry.1838667208=http%3A//arxiv.org/abs/2501.03841v1&entry.124074799=Read"},
{"title": "Machine learning applications in archaeological practices: a review", "author": "Mathias Bellat and Jordy D. Orellana Figueroa and Jonathan S. Reeves and Ruhollah Taghizadeh-Mehrjardi and Claudio Tennie and Thomas Scholten", "abstract": "  Artificial intelligence and machine learning applications in archaeology have\nincreased significantly in recent years, and these now span all subfields,\ngeographical regions, and time periods. The prevalence and success of these\napplications have remained largely unexamined, as recent reviews on the use of\nmachine learning in archaeology have only focused only on specific subfields of\narchaeology. Our review examined an exhaustive corpus of 135 articles published\nbetween 1997 and 2022. We observed a significant increase in the number of\nrelevant publications from 2019 onwards. Automatic structure detection and\nartefact classification were the most represented tasks in the articles\nreviewed, followed by taphonomy, and archaeological predictive modelling. From\nthe review, clustering and unsupervised methods were underrepresented compared\nto supervised models. Artificial neural networks and ensemble learning account\nfor two thirds of the total number of models used. However, if machine learning\nis gaining in popularity it remains subject to misunderstanding. We observed,\nin some cases, poorly defined requirements and caveats of the machine learning\nmethods used. Furthermore, the goals and the needs of machine learning\napplications for archaeological purposes are in some cases unclear or poorly\nexpressed. To address this, we proposed a workflow guide for archaeologists to\ndevelop coherent and consistent methodologies adapted to their research\nquestions, project scale and data. As in many other areas, machine learning is\nrapidly becoming an important tool in archaeological research and practice,\nuseful for the analyses of large and multivariate data, although not without\nlimitations. This review highlights the importance of well-defined and\nwell-reported structured methodologies and collaborative practices to maximise\nthe potential of applications of machine learning methods in archaeology.\n", "link": "http://arxiv.org/abs/2501.03840v1", "date": "2025-01-07", "relevancy": 1.7412, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.4353}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.4353}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.4351}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Machine%20learning%20applications%20in%20archaeological%20practices%3A%20a%20review&body=Title%3A%20Machine%20learning%20applications%20in%20archaeological%20practices%3A%20a%20review%0AAuthor%3A%20Mathias%20Bellat%20and%20Jordy%20D.%20Orellana%20Figueroa%20and%20Jonathan%20S.%20Reeves%20and%20Ruhollah%20Taghizadeh-Mehrjardi%20and%20Claudio%20Tennie%20and%20Thomas%20Scholten%0AAbstract%3A%20%20%20Artificial%20intelligence%20and%20machine%20learning%20applications%20in%20archaeology%20have%0Aincreased%20significantly%20in%20recent%20years%2C%20and%20these%20now%20span%20all%20subfields%2C%0Ageographical%20regions%2C%20and%20time%20periods.%20The%20prevalence%20and%20success%20of%20these%0Aapplications%20have%20remained%20largely%20unexamined%2C%20as%20recent%20reviews%20on%20the%20use%20of%0Amachine%20learning%20in%20archaeology%20have%20only%20focused%20only%20on%20specific%20subfields%20of%0Aarchaeology.%20Our%20review%20examined%20an%20exhaustive%20corpus%20of%20135%20articles%20published%0Abetween%201997%20and%202022.%20We%20observed%20a%20significant%20increase%20in%20the%20number%20of%0Arelevant%20publications%20from%202019%20onwards.%20Automatic%20structure%20detection%20and%0Aartefact%20classification%20were%20the%20most%20represented%20tasks%20in%20the%20articles%0Areviewed%2C%20followed%20by%20taphonomy%2C%20and%20archaeological%20predictive%20modelling.%20From%0Athe%20review%2C%20clustering%20and%20unsupervised%20methods%20were%20underrepresented%20compared%0Ato%20supervised%20models.%20Artificial%20neural%20networks%20and%20ensemble%20learning%20account%0Afor%20two%20thirds%20of%20the%20total%20number%20of%20models%20used.%20However%2C%20if%20machine%20learning%0Ais%20gaining%20in%20popularity%20it%20remains%20subject%20to%20misunderstanding.%20We%20observed%2C%0Ain%20some%20cases%2C%20poorly%20defined%20requirements%20and%20caveats%20of%20the%20machine%20learning%0Amethods%20used.%20Furthermore%2C%20the%20goals%20and%20the%20needs%20of%20machine%20learning%0Aapplications%20for%20archaeological%20purposes%20are%20in%20some%20cases%20unclear%20or%20poorly%0Aexpressed.%20To%20address%20this%2C%20we%20proposed%20a%20workflow%20guide%20for%20archaeologists%20to%0Adevelop%20coherent%20and%20consistent%20methodologies%20adapted%20to%20their%20research%0Aquestions%2C%20project%20scale%20and%20data.%20As%20in%20many%20other%20areas%2C%20machine%20learning%20is%0Arapidly%20becoming%20an%20important%20tool%20in%20archaeological%20research%20and%20practice%2C%0Auseful%20for%20the%20analyses%20of%20large%20and%20multivariate%20data%2C%20although%20not%20without%0Alimitations.%20This%20review%20highlights%20the%20importance%20of%20well-defined%20and%0Awell-reported%20structured%20methodologies%20and%20collaborative%20practices%20to%20maximise%0Athe%20potential%20of%20applications%20of%20machine%20learning%20methods%20in%20archaeology.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2501.03840v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DMachine%2520learning%2520applications%2520in%2520archaeological%2520practices%253A%2520a%2520review%26entry.906535625%3DMathias%2520Bellat%2520and%2520Jordy%2520D.%2520Orellana%2520Figueroa%2520and%2520Jonathan%2520S.%2520Reeves%2520and%2520Ruhollah%2520Taghizadeh-Mehrjardi%2520and%2520Claudio%2520Tennie%2520and%2520Thomas%2520Scholten%26entry.1292438233%3D%2520%2520Artificial%2520intelligence%2520and%2520machine%2520learning%2520applications%2520in%2520archaeology%2520have%250Aincreased%2520significantly%2520in%2520recent%2520years%252C%2520and%2520these%2520now%2520span%2520all%2520subfields%252C%250Ageographical%2520regions%252C%2520and%2520time%2520periods.%2520The%2520prevalence%2520and%2520success%2520of%2520these%250Aapplications%2520have%2520remained%2520largely%2520unexamined%252C%2520as%2520recent%2520reviews%2520on%2520the%2520use%2520of%250Amachine%2520learning%2520in%2520archaeology%2520have%2520only%2520focused%2520only%2520on%2520specific%2520subfields%2520of%250Aarchaeology.%2520Our%2520review%2520examined%2520an%2520exhaustive%2520corpus%2520of%2520135%2520articles%2520published%250Abetween%25201997%2520and%25202022.%2520We%2520observed%2520a%2520significant%2520increase%2520in%2520the%2520number%2520of%250Arelevant%2520publications%2520from%25202019%2520onwards.%2520Automatic%2520structure%2520detection%2520and%250Aartefact%2520classification%2520were%2520the%2520most%2520represented%2520tasks%2520in%2520the%2520articles%250Areviewed%252C%2520followed%2520by%2520taphonomy%252C%2520and%2520archaeological%2520predictive%2520modelling.%2520From%250Athe%2520review%252C%2520clustering%2520and%2520unsupervised%2520methods%2520were%2520underrepresented%2520compared%250Ato%2520supervised%2520models.%2520Artificial%2520neural%2520networks%2520and%2520ensemble%2520learning%2520account%250Afor%2520two%2520thirds%2520of%2520the%2520total%2520number%2520of%2520models%2520used.%2520However%252C%2520if%2520machine%2520learning%250Ais%2520gaining%2520in%2520popularity%2520it%2520remains%2520subject%2520to%2520misunderstanding.%2520We%2520observed%252C%250Ain%2520some%2520cases%252C%2520poorly%2520defined%2520requirements%2520and%2520caveats%2520of%2520the%2520machine%2520learning%250Amethods%2520used.%2520Furthermore%252C%2520the%2520goals%2520and%2520the%2520needs%2520of%2520machine%2520learning%250Aapplications%2520for%2520archaeological%2520purposes%2520are%2520in%2520some%2520cases%2520unclear%2520or%2520poorly%250Aexpressed.%2520To%2520address%2520this%252C%2520we%2520proposed%2520a%2520workflow%2520guide%2520for%2520archaeologists%2520to%250Adevelop%2520coherent%2520and%2520consistent%2520methodologies%2520adapted%2520to%2520their%2520research%250Aquestions%252C%2520project%2520scale%2520and%2520data.%2520As%2520in%2520many%2520other%2520areas%252C%2520machine%2520learning%2520is%250Arapidly%2520becoming%2520an%2520important%2520tool%2520in%2520archaeological%2520research%2520and%2520practice%252C%250Auseful%2520for%2520the%2520analyses%2520of%2520large%2520and%2520multivariate%2520data%252C%2520although%2520not%2520without%250Alimitations.%2520This%2520review%2520highlights%2520the%2520importance%2520of%2520well-defined%2520and%250Awell-reported%2520structured%2520methodologies%2520and%2520collaborative%2520practices%2520to%2520maximise%250Athe%2520potential%2520of%2520applications%2520of%2520machine%2520learning%2520methods%2520in%2520archaeology.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2501.03840v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Machine%20learning%20applications%20in%20archaeological%20practices%3A%20a%20review&entry.906535625=Mathias%20Bellat%20and%20Jordy%20D.%20Orellana%20Figueroa%20and%20Jonathan%20S.%20Reeves%20and%20Ruhollah%20Taghizadeh-Mehrjardi%20and%20Claudio%20Tennie%20and%20Thomas%20Scholten&entry.1292438233=%20%20Artificial%20intelligence%20and%20machine%20learning%20applications%20in%20archaeology%20have%0Aincreased%20significantly%20in%20recent%20years%2C%20and%20these%20now%20span%20all%20subfields%2C%0Ageographical%20regions%2C%20and%20time%20periods.%20The%20prevalence%20and%20success%20of%20these%0Aapplications%20have%20remained%20largely%20unexamined%2C%20as%20recent%20reviews%20on%20the%20use%20of%0Amachine%20learning%20in%20archaeology%20have%20only%20focused%20only%20on%20specific%20subfields%20of%0Aarchaeology.%20Our%20review%20examined%20an%20exhaustive%20corpus%20of%20135%20articles%20published%0Abetween%201997%20and%202022.%20We%20observed%20a%20significant%20increase%20in%20the%20number%20of%0Arelevant%20publications%20from%202019%20onwards.%20Automatic%20structure%20detection%20and%0Aartefact%20classification%20were%20the%20most%20represented%20tasks%20in%20the%20articles%0Areviewed%2C%20followed%20by%20taphonomy%2C%20and%20archaeological%20predictive%20modelling.%20From%0Athe%20review%2C%20clustering%20and%20unsupervised%20methods%20were%20underrepresented%20compared%0Ato%20supervised%20models.%20Artificial%20neural%20networks%20and%20ensemble%20learning%20account%0Afor%20two%20thirds%20of%20the%20total%20number%20of%20models%20used.%20However%2C%20if%20machine%20learning%0Ais%20gaining%20in%20popularity%20it%20remains%20subject%20to%20misunderstanding.%20We%20observed%2C%0Ain%20some%20cases%2C%20poorly%20defined%20requirements%20and%20caveats%20of%20the%20machine%20learning%0Amethods%20used.%20Furthermore%2C%20the%20goals%20and%20the%20needs%20of%20machine%20learning%0Aapplications%20for%20archaeological%20purposes%20are%20in%20some%20cases%20unclear%20or%20poorly%0Aexpressed.%20To%20address%20this%2C%20we%20proposed%20a%20workflow%20guide%20for%20archaeologists%20to%0Adevelop%20coherent%20and%20consistent%20methodologies%20adapted%20to%20their%20research%0Aquestions%2C%20project%20scale%20and%20data.%20As%20in%20many%20other%20areas%2C%20machine%20learning%20is%0Arapidly%20becoming%20an%20important%20tool%20in%20archaeological%20research%20and%20practice%2C%0Auseful%20for%20the%20analyses%20of%20large%20and%20multivariate%20data%2C%20although%20not%20without%0Alimitations.%20This%20review%20highlights%20the%20importance%20of%20well-defined%20and%0Awell-reported%20structured%20methodologies%20and%20collaborative%20practices%20to%20maximise%0Athe%20potential%20of%20applications%20of%20machine%20learning%20methods%20in%20archaeology.%0A&entry.1838667208=http%3A//arxiv.org/abs/2501.03840v1&entry.124074799=Read"},
{"title": "Motion-Aware Generative Frame Interpolation", "author": "Guozhen Zhang and Yuhan Zhu and Yutao Cui and Xiaotong Zhao and Kai Ma and Limin Wang", "abstract": "  Generative frame interpolation, empowered by large-scale pre-trained video\ngeneration models, has demonstrated remarkable advantages in complex scenes.\nHowever, existing methods heavily rely on the generative model to independently\ninfer the correspondences between input frames, an ability that is inadequately\ndeveloped during pre-training. In this work, we propose a novel framework,\ntermed Motion-aware Generative frame interpolation (MoG), to significantly\nenhance the model's motion awareness by integrating explicit motion guidance.\nSpecifically we investigate two key questions: what can serve as an effective\nmotion guidance, and how we can seamlessly embed this guidance into the\ngenerative model. For the first question, we reveal that the intermediate flow\nfrom flow-based interpolation models could efficiently provide task-oriented\nmotion guidance. Regarding the second, we first obtain guidance-based\nrepresentations of intermediate frames by warping input frames' representations\nusing guidance, and then integrate them into the model at both latent and\nfeature levels. To demonstrate the versatility of our method, we train MoG on\nboth real-world and animation datasets. Comprehensive evaluations show that our\nMoG significantly outperforms the existing methods in both domains, achieving\nsuperior video quality and improved fidelity.\n", "link": "http://arxiv.org/abs/2501.03699v1", "date": "2025-01-07", "relevancy": 1.79, "topK": [{"title": "Synthesizing Moving People with 3D Control", "link": "http://arxiv.org/abs/2401.10889v2", "similarity": 0.602}, {"title": "Thinking Outside the BBox: Unconstrained Generative Object Compositing", "link": "http://arxiv.org/abs/2409.04559v2", "similarity": 0.5947}, {"title": "Customize-A-Video: One-Shot Motion Customization of Text-to-Video\n  Diffusion Models", "link": "http://arxiv.org/abs/2402.14780v1", "similarity": 0.5854}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Motion-Aware%20Generative%20Frame%20Interpolation&body=Title%3A%20Motion-Aware%20Generative%20Frame%20Interpolation%0AAuthor%3A%20Guozhen%20Zhang%20and%20Yuhan%20Zhu%20and%20Yutao%20Cui%20and%20Xiaotong%20Zhao%20and%20Kai%20Ma%20and%20Limin%20Wang%0AAbstract%3A%20%20%20Generative%20frame%20interpolation%2C%20empowered%20by%20large-scale%20pre-trained%20video%0Ageneration%20models%2C%20has%20demonstrated%20remarkable%20advantages%20in%20complex%20scenes.%0AHowever%2C%20existing%20methods%20heavily%20rely%20on%20the%20generative%20model%20to%20independently%0Ainfer%20the%20correspondences%20between%20input%20frames%2C%20an%20ability%20that%20is%20inadequately%0Adeveloped%20during%20pre-training.%20In%20this%20work%2C%20we%20propose%20a%20novel%20framework%2C%0Atermed%20Motion-aware%20Generative%20frame%20interpolation%20%28MoG%29%2C%20to%20significantly%0Aenhance%20the%20model%27s%20motion%20awareness%20by%20integrating%20explicit%20motion%20guidance.%0ASpecifically%20we%20investigate%20two%20key%20questions%3A%20what%20can%20serve%20as%20an%20effective%0Amotion%20guidance%2C%20and%20how%20we%20can%20seamlessly%20embed%20this%20guidance%20into%20the%0Agenerative%20model.%20For%20the%20first%20question%2C%20we%20reveal%20that%20the%20intermediate%20flow%0Afrom%20flow-based%20interpolation%20models%20could%20efficiently%20provide%20task-oriented%0Amotion%20guidance.%20Regarding%20the%20second%2C%20we%20first%20obtain%20guidance-based%0Arepresentations%20of%20intermediate%20frames%20by%20warping%20input%20frames%27%20representations%0Ausing%20guidance%2C%20and%20then%20integrate%20them%20into%20the%20model%20at%20both%20latent%20and%0Afeature%20levels.%20To%20demonstrate%20the%20versatility%20of%20our%20method%2C%20we%20train%20MoG%20on%0Aboth%20real-world%20and%20animation%20datasets.%20Comprehensive%20evaluations%20show%20that%20our%0AMoG%20significantly%20outperforms%20the%20existing%20methods%20in%20both%20domains%2C%20achieving%0Asuperior%20video%20quality%20and%20improved%20fidelity.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2501.03699v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DMotion-Aware%2520Generative%2520Frame%2520Interpolation%26entry.906535625%3DGuozhen%2520Zhang%2520and%2520Yuhan%2520Zhu%2520and%2520Yutao%2520Cui%2520and%2520Xiaotong%2520Zhao%2520and%2520Kai%2520Ma%2520and%2520Limin%2520Wang%26entry.1292438233%3D%2520%2520Generative%2520frame%2520interpolation%252C%2520empowered%2520by%2520large-scale%2520pre-trained%2520video%250Ageneration%2520models%252C%2520has%2520demonstrated%2520remarkable%2520advantages%2520in%2520complex%2520scenes.%250AHowever%252C%2520existing%2520methods%2520heavily%2520rely%2520on%2520the%2520generative%2520model%2520to%2520independently%250Ainfer%2520the%2520correspondences%2520between%2520input%2520frames%252C%2520an%2520ability%2520that%2520is%2520inadequately%250Adeveloped%2520during%2520pre-training.%2520In%2520this%2520work%252C%2520we%2520propose%2520a%2520novel%2520framework%252C%250Atermed%2520Motion-aware%2520Generative%2520frame%2520interpolation%2520%2528MoG%2529%252C%2520to%2520significantly%250Aenhance%2520the%2520model%2527s%2520motion%2520awareness%2520by%2520integrating%2520explicit%2520motion%2520guidance.%250ASpecifically%2520we%2520investigate%2520two%2520key%2520questions%253A%2520what%2520can%2520serve%2520as%2520an%2520effective%250Amotion%2520guidance%252C%2520and%2520how%2520we%2520can%2520seamlessly%2520embed%2520this%2520guidance%2520into%2520the%250Agenerative%2520model.%2520For%2520the%2520first%2520question%252C%2520we%2520reveal%2520that%2520the%2520intermediate%2520flow%250Afrom%2520flow-based%2520interpolation%2520models%2520could%2520efficiently%2520provide%2520task-oriented%250Amotion%2520guidance.%2520Regarding%2520the%2520second%252C%2520we%2520first%2520obtain%2520guidance-based%250Arepresentations%2520of%2520intermediate%2520frames%2520by%2520warping%2520input%2520frames%2527%2520representations%250Ausing%2520guidance%252C%2520and%2520then%2520integrate%2520them%2520into%2520the%2520model%2520at%2520both%2520latent%2520and%250Afeature%2520levels.%2520To%2520demonstrate%2520the%2520versatility%2520of%2520our%2520method%252C%2520we%2520train%2520MoG%2520on%250Aboth%2520real-world%2520and%2520animation%2520datasets.%2520Comprehensive%2520evaluations%2520show%2520that%2520our%250AMoG%2520significantly%2520outperforms%2520the%2520existing%2520methods%2520in%2520both%2520domains%252C%2520achieving%250Asuperior%2520video%2520quality%2520and%2520improved%2520fidelity.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2501.03699v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Motion-Aware%20Generative%20Frame%20Interpolation&entry.906535625=Guozhen%20Zhang%20and%20Yuhan%20Zhu%20and%20Yutao%20Cui%20and%20Xiaotong%20Zhao%20and%20Kai%20Ma%20and%20Limin%20Wang&entry.1292438233=%20%20Generative%20frame%20interpolation%2C%20empowered%20by%20large-scale%20pre-trained%20video%0Ageneration%20models%2C%20has%20demonstrated%20remarkable%20advantages%20in%20complex%20scenes.%0AHowever%2C%20existing%20methods%20heavily%20rely%20on%20the%20generative%20model%20to%20independently%0Ainfer%20the%20correspondences%20between%20input%20frames%2C%20an%20ability%20that%20is%20inadequately%0Adeveloped%20during%20pre-training.%20In%20this%20work%2C%20we%20propose%20a%20novel%20framework%2C%0Atermed%20Motion-aware%20Generative%20frame%20interpolation%20%28MoG%29%2C%20to%20significantly%0Aenhance%20the%20model%27s%20motion%20awareness%20by%20integrating%20explicit%20motion%20guidance.%0ASpecifically%20we%20investigate%20two%20key%20questions%3A%20what%20can%20serve%20as%20an%20effective%0Amotion%20guidance%2C%20and%20how%20we%20can%20seamlessly%20embed%20this%20guidance%20into%20the%0Agenerative%20model.%20For%20the%20first%20question%2C%20we%20reveal%20that%20the%20intermediate%20flow%0Afrom%20flow-based%20interpolation%20models%20could%20efficiently%20provide%20task-oriented%0Amotion%20guidance.%20Regarding%20the%20second%2C%20we%20first%20obtain%20guidance-based%0Arepresentations%20of%20intermediate%20frames%20by%20warping%20input%20frames%27%20representations%0Ausing%20guidance%2C%20and%20then%20integrate%20them%20into%20the%20model%20at%20both%20latent%20and%0Afeature%20levels.%20To%20demonstrate%20the%20versatility%20of%20our%20method%2C%20we%20train%20MoG%20on%0Aboth%20real-world%20and%20animation%20datasets.%20Comprehensive%20evaluations%20show%20that%20our%0AMoG%20significantly%20outperforms%20the%20existing%20methods%20in%20both%20domains%2C%20achieving%0Asuperior%20video%20quality%20and%20improved%20fidelity.%0A&entry.1838667208=http%3A//arxiv.org/abs/2501.03699v1&entry.124074799=Read"},
{"title": "Highway Graph to Accelerate Reinforcement Learning", "author": "Zidu Yin and Zhen Zhang and Dong Gong and Stefano V. Albrecht and Javen Q. Shi", "abstract": "  Reinforcement Learning (RL) algorithms often struggle with low training\nefficiency. A common approach to address this challenge is integrating\nmodel-based planning algorithms, such as Monte Carlo Tree Search (MCTS) or\nValue Iteration (VI), into the environmental model. However, VI requires\niterating over a large tensor which updates the value of the preceding state\nbased on the succeeding state through value propagation, resulting in\ncomputationally intensive operations. To enhance the RL training efficiency, we\npropose improving the efficiency of the value learning process. In\ndeterministic environments with discrete state and action spaces, we observe\nthat on the sampled empirical state-transition graph, a non-branching sequence\nof transitions-termed a highway-can take the agent to another state without\ndeviation through intermediate states. On these non-branching highways, the\nvalue-updating process can be streamlined into a single-step operation,\neliminating the need for step-by-step updates. Building on this observation, we\nintroduce the highway graph to model state transitions. The highway graph\ncompresses the transition model into a compact representation, where edges can\nencapsulate multiple state transitions, enabling value propagation across\nmultiple time steps in a single iteration. By integrating the highway graph\ninto RL, the training process is significantly accelerated, particularly in the\nearly stages of training. Experiments across four categories of environments\ndemonstrate that our method learns significantly faster than established and\nstate-of-the-art RL algorithms (often by a factor of 10 to 150) while\nmaintaining equal or superior expected returns. Furthermore, a deep neural\nnetwork-based agent trained using the highway graph exhibits improved\ngeneralization capabilities and reduced storage costs. Code is publicly\navailable at https://github.com/coodest/highwayRL.\n", "link": "http://arxiv.org/abs/2405.11727v2", "date": "2025-01-07", "relevancy": 1.503, "topK": [{"title": "RoboEXP: Action-Conditioned Scene Graph via Interactive Exploration for\n  Robotic Manipulation", "link": "http://arxiv.org/abs/2402.15487v1", "similarity": 0.5133}, {"title": "Safe and efficient training of a control agent", "link": "https://patents.google.com/patent/US11709462B2/en", "similarity": 0.5001}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.4908}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Highway%20Graph%20to%20Accelerate%20Reinforcement%20Learning&body=Title%3A%20Highway%20Graph%20to%20Accelerate%20Reinforcement%20Learning%0AAuthor%3A%20Zidu%20Yin%20and%20Zhen%20Zhang%20and%20Dong%20Gong%20and%20Stefano%20V.%20Albrecht%20and%20Javen%20Q.%20Shi%0AAbstract%3A%20%20%20Reinforcement%20Learning%20%28RL%29%20algorithms%20often%20struggle%20with%20low%20training%0Aefficiency.%20A%20common%20approach%20to%20address%20this%20challenge%20is%20integrating%0Amodel-based%20planning%20algorithms%2C%20such%20as%20Monte%20Carlo%20Tree%20Search%20%28MCTS%29%20or%0AValue%20Iteration%20%28VI%29%2C%20into%20the%20environmental%20model.%20However%2C%20VI%20requires%0Aiterating%20over%20a%20large%20tensor%20which%20updates%20the%20value%20of%20the%20preceding%20state%0Abased%20on%20the%20succeeding%20state%20through%20value%20propagation%2C%20resulting%20in%0Acomputationally%20intensive%20operations.%20To%20enhance%20the%20RL%20training%20efficiency%2C%20we%0Apropose%20improving%20the%20efficiency%20of%20the%20value%20learning%20process.%20In%0Adeterministic%20environments%20with%20discrete%20state%20and%20action%20spaces%2C%20we%20observe%0Athat%20on%20the%20sampled%20empirical%20state-transition%20graph%2C%20a%20non-branching%20sequence%0Aof%20transitions-termed%20a%20highway-can%20take%20the%20agent%20to%20another%20state%20without%0Adeviation%20through%20intermediate%20states.%20On%20these%20non-branching%20highways%2C%20the%0Avalue-updating%20process%20can%20be%20streamlined%20into%20a%20single-step%20operation%2C%0Aeliminating%20the%20need%20for%20step-by-step%20updates.%20Building%20on%20this%20observation%2C%20we%0Aintroduce%20the%20highway%20graph%20to%20model%20state%20transitions.%20The%20highway%20graph%0Acompresses%20the%20transition%20model%20into%20a%20compact%20representation%2C%20where%20edges%20can%0Aencapsulate%20multiple%20state%20transitions%2C%20enabling%20value%20propagation%20across%0Amultiple%20time%20steps%20in%20a%20single%20iteration.%20By%20integrating%20the%20highway%20graph%0Ainto%20RL%2C%20the%20training%20process%20is%20significantly%20accelerated%2C%20particularly%20in%20the%0Aearly%20stages%20of%20training.%20Experiments%20across%20four%20categories%20of%20environments%0Ademonstrate%20that%20our%20method%20learns%20significantly%20faster%20than%20established%20and%0Astate-of-the-art%20RL%20algorithms%20%28often%20by%20a%20factor%20of%2010%20to%20150%29%20while%0Amaintaining%20equal%20or%20superior%20expected%20returns.%20Furthermore%2C%20a%20deep%20neural%0Anetwork-based%20agent%20trained%20using%20the%20highway%20graph%20exhibits%20improved%0Ageneralization%20capabilities%20and%20reduced%20storage%20costs.%20Code%20is%20publicly%0Aavailable%20at%20https%3A//github.com/coodest/highwayRL.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2405.11727v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DHighway%2520Graph%2520to%2520Accelerate%2520Reinforcement%2520Learning%26entry.906535625%3DZidu%2520Yin%2520and%2520Zhen%2520Zhang%2520and%2520Dong%2520Gong%2520and%2520Stefano%2520V.%2520Albrecht%2520and%2520Javen%2520Q.%2520Shi%26entry.1292438233%3D%2520%2520Reinforcement%2520Learning%2520%2528RL%2529%2520algorithms%2520often%2520struggle%2520with%2520low%2520training%250Aefficiency.%2520A%2520common%2520approach%2520to%2520address%2520this%2520challenge%2520is%2520integrating%250Amodel-based%2520planning%2520algorithms%252C%2520such%2520as%2520Monte%2520Carlo%2520Tree%2520Search%2520%2528MCTS%2529%2520or%250AValue%2520Iteration%2520%2528VI%2529%252C%2520into%2520the%2520environmental%2520model.%2520However%252C%2520VI%2520requires%250Aiterating%2520over%2520a%2520large%2520tensor%2520which%2520updates%2520the%2520value%2520of%2520the%2520preceding%2520state%250Abased%2520on%2520the%2520succeeding%2520state%2520through%2520value%2520propagation%252C%2520resulting%2520in%250Acomputationally%2520intensive%2520operations.%2520To%2520enhance%2520the%2520RL%2520training%2520efficiency%252C%2520we%250Apropose%2520improving%2520the%2520efficiency%2520of%2520the%2520value%2520learning%2520process.%2520In%250Adeterministic%2520environments%2520with%2520discrete%2520state%2520and%2520action%2520spaces%252C%2520we%2520observe%250Athat%2520on%2520the%2520sampled%2520empirical%2520state-transition%2520graph%252C%2520a%2520non-branching%2520sequence%250Aof%2520transitions-termed%2520a%2520highway-can%2520take%2520the%2520agent%2520to%2520another%2520state%2520without%250Adeviation%2520through%2520intermediate%2520states.%2520On%2520these%2520non-branching%2520highways%252C%2520the%250Avalue-updating%2520process%2520can%2520be%2520streamlined%2520into%2520a%2520single-step%2520operation%252C%250Aeliminating%2520the%2520need%2520for%2520step-by-step%2520updates.%2520Building%2520on%2520this%2520observation%252C%2520we%250Aintroduce%2520the%2520highway%2520graph%2520to%2520model%2520state%2520transitions.%2520The%2520highway%2520graph%250Acompresses%2520the%2520transition%2520model%2520into%2520a%2520compact%2520representation%252C%2520where%2520edges%2520can%250Aencapsulate%2520multiple%2520state%2520transitions%252C%2520enabling%2520value%2520propagation%2520across%250Amultiple%2520time%2520steps%2520in%2520a%2520single%2520iteration.%2520By%2520integrating%2520the%2520highway%2520graph%250Ainto%2520RL%252C%2520the%2520training%2520process%2520is%2520significantly%2520accelerated%252C%2520particularly%2520in%2520the%250Aearly%2520stages%2520of%2520training.%2520Experiments%2520across%2520four%2520categories%2520of%2520environments%250Ademonstrate%2520that%2520our%2520method%2520learns%2520significantly%2520faster%2520than%2520established%2520and%250Astate-of-the-art%2520RL%2520algorithms%2520%2528often%2520by%2520a%2520factor%2520of%252010%2520to%2520150%2529%2520while%250Amaintaining%2520equal%2520or%2520superior%2520expected%2520returns.%2520Furthermore%252C%2520a%2520deep%2520neural%250Anetwork-based%2520agent%2520trained%2520using%2520the%2520highway%2520graph%2520exhibits%2520improved%250Ageneralization%2520capabilities%2520and%2520reduced%2520storage%2520costs.%2520Code%2520is%2520publicly%250Aavailable%2520at%2520https%253A//github.com/coodest/highwayRL.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2405.11727v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Highway%20Graph%20to%20Accelerate%20Reinforcement%20Learning&entry.906535625=Zidu%20Yin%20and%20Zhen%20Zhang%20and%20Dong%20Gong%20and%20Stefano%20V.%20Albrecht%20and%20Javen%20Q.%20Shi&entry.1292438233=%20%20Reinforcement%20Learning%20%28RL%29%20algorithms%20often%20struggle%20with%20low%20training%0Aefficiency.%20A%20common%20approach%20to%20address%20this%20challenge%20is%20integrating%0Amodel-based%20planning%20algorithms%2C%20such%20as%20Monte%20Carlo%20Tree%20Search%20%28MCTS%29%20or%0AValue%20Iteration%20%28VI%29%2C%20into%20the%20environmental%20model.%20However%2C%20VI%20requires%0Aiterating%20over%20a%20large%20tensor%20which%20updates%20the%20value%20of%20the%20preceding%20state%0Abased%20on%20the%20succeeding%20state%20through%20value%20propagation%2C%20resulting%20in%0Acomputationally%20intensive%20operations.%20To%20enhance%20the%20RL%20training%20efficiency%2C%20we%0Apropose%20improving%20the%20efficiency%20of%20the%20value%20learning%20process.%20In%0Adeterministic%20environments%20with%20discrete%20state%20and%20action%20spaces%2C%20we%20observe%0Athat%20on%20the%20sampled%20empirical%20state-transition%20graph%2C%20a%20non-branching%20sequence%0Aof%20transitions-termed%20a%20highway-can%20take%20the%20agent%20to%20another%20state%20without%0Adeviation%20through%20intermediate%20states.%20On%20these%20non-branching%20highways%2C%20the%0Avalue-updating%20process%20can%20be%20streamlined%20into%20a%20single-step%20operation%2C%0Aeliminating%20the%20need%20for%20step-by-step%20updates.%20Building%20on%20this%20observation%2C%20we%0Aintroduce%20the%20highway%20graph%20to%20model%20state%20transitions.%20The%20highway%20graph%0Acompresses%20the%20transition%20model%20into%20a%20compact%20representation%2C%20where%20edges%20can%0Aencapsulate%20multiple%20state%20transitions%2C%20enabling%20value%20propagation%20across%0Amultiple%20time%20steps%20in%20a%20single%20iteration.%20By%20integrating%20the%20highway%20graph%0Ainto%20RL%2C%20the%20training%20process%20is%20significantly%20accelerated%2C%20particularly%20in%20the%0Aearly%20stages%20of%20training.%20Experiments%20across%20four%20categories%20of%20environments%0Ademonstrate%20that%20our%20method%20learns%20significantly%20faster%20than%20established%20and%0Astate-of-the-art%20RL%20algorithms%20%28often%20by%20a%20factor%20of%2010%20to%20150%29%20while%0Amaintaining%20equal%20or%20superior%20expected%20returns.%20Furthermore%2C%20a%20deep%20neural%0Anetwork-based%20agent%20trained%20using%20the%20highway%20graph%20exhibits%20improved%0Ageneralization%20capabilities%20and%20reduced%20storage%20costs.%20Code%20is%20publicly%0Aavailable%20at%20https%3A//github.com/coodest/highwayRL.%0A&entry.1838667208=http%3A//arxiv.org/abs/2405.11727v2&entry.124074799=Read"},
{"title": "A Synergistic Framework for Learning Shape Estimation and Shape-Aware\n  Whole-Body Control Policy for Continuum Robots", "author": "Mohammadreza Kasaei and Farshid Alambeigi and Mohsen Khadem", "abstract": "  In this paper, we present a novel synergistic framework for learning shape\nestimation and a shape-aware whole-body control policy for tendon-driven\ncontinuum robots. Our approach leverages the interaction between two Augmented\nNeural Ordinary Differential Equations (ANODEs) -- the Shape-NODE and\nControl-NODE -- to achieve continuous shape estimation and shape-aware control.\nThe Shape-NODE integrates prior knowledge from Cosserat rod theory, allowing it\nto adapt and account for model mismatches, while the Control-NODE uses this\nshape information to optimize a whole-body control policy, trained in a Model\nPredictive Control (MPC) fashion. This unified framework effectively overcomes\nlimitations of existing data-driven methods, such as poor shape awareness and\nchallenges in capturing complex nonlinear dynamics. Extensive evaluations in\nboth simulation and real-world environments demonstrate the framework's robust\nperformance in shape estimation, trajectory tracking, and obstacle avoidance.\nThe proposed method consistently outperforms state-of-the-art end-to-end,\nNeural-ODE, and Recurrent Neural Network (RNN) models, particularly in terms of\ntracking accuracy and generalization capabilities.\n", "link": "http://arxiv.org/abs/2501.03859v1", "date": "2025-01-07", "relevancy": 1.8245, "topK": [{"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.6523}, {"title": "Synthesizing Moving People with 3D Control", "link": "http://arxiv.org/abs/2401.10889v2", "similarity": 0.5962}, {"title": "RoboEXP: Action-Conditioned Scene Graph via Interactive Exploration for\n  Robotic Manipulation", "link": "http://arxiv.org/abs/2402.15487v1", "similarity": 0.594}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20A%20Synergistic%20Framework%20for%20Learning%20Shape%20Estimation%20and%20Shape-Aware%0A%20%20Whole-Body%20Control%20Policy%20for%20Continuum%20Robots&body=Title%3A%20A%20Synergistic%20Framework%20for%20Learning%20Shape%20Estimation%20and%20Shape-Aware%0A%20%20Whole-Body%20Control%20Policy%20for%20Continuum%20Robots%0AAuthor%3A%20Mohammadreza%20Kasaei%20and%20Farshid%20Alambeigi%20and%20Mohsen%20Khadem%0AAbstract%3A%20%20%20In%20this%20paper%2C%20we%20present%20a%20novel%20synergistic%20framework%20for%20learning%20shape%0Aestimation%20and%20a%20shape-aware%20whole-body%20control%20policy%20for%20tendon-driven%0Acontinuum%20robots.%20Our%20approach%20leverages%20the%20interaction%20between%20two%20Augmented%0ANeural%20Ordinary%20Differential%20Equations%20%28ANODEs%29%20--%20the%20Shape-NODE%20and%0AControl-NODE%20--%20to%20achieve%20continuous%20shape%20estimation%20and%20shape-aware%20control.%0AThe%20Shape-NODE%20integrates%20prior%20knowledge%20from%20Cosserat%20rod%20theory%2C%20allowing%20it%0Ato%20adapt%20and%20account%20for%20model%20mismatches%2C%20while%20the%20Control-NODE%20uses%20this%0Ashape%20information%20to%20optimize%20a%20whole-body%20control%20policy%2C%20trained%20in%20a%20Model%0APredictive%20Control%20%28MPC%29%20fashion.%20This%20unified%20framework%20effectively%20overcomes%0Alimitations%20of%20existing%20data-driven%20methods%2C%20such%20as%20poor%20shape%20awareness%20and%0Achallenges%20in%20capturing%20complex%20nonlinear%20dynamics.%20Extensive%20evaluations%20in%0Aboth%20simulation%20and%20real-world%20environments%20demonstrate%20the%20framework%27s%20robust%0Aperformance%20in%20shape%20estimation%2C%20trajectory%20tracking%2C%20and%20obstacle%20avoidance.%0AThe%20proposed%20method%20consistently%20outperforms%20state-of-the-art%20end-to-end%2C%0ANeural-ODE%2C%20and%20Recurrent%20Neural%20Network%20%28RNN%29%20models%2C%20particularly%20in%20terms%20of%0Atracking%20accuracy%20and%20generalization%20capabilities.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2501.03859v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DA%2520Synergistic%2520Framework%2520for%2520Learning%2520Shape%2520Estimation%2520and%2520Shape-Aware%250A%2520%2520Whole-Body%2520Control%2520Policy%2520for%2520Continuum%2520Robots%26entry.906535625%3DMohammadreza%2520Kasaei%2520and%2520Farshid%2520Alambeigi%2520and%2520Mohsen%2520Khadem%26entry.1292438233%3D%2520%2520In%2520this%2520paper%252C%2520we%2520present%2520a%2520novel%2520synergistic%2520framework%2520for%2520learning%2520shape%250Aestimation%2520and%2520a%2520shape-aware%2520whole-body%2520control%2520policy%2520for%2520tendon-driven%250Acontinuum%2520robots.%2520Our%2520approach%2520leverages%2520the%2520interaction%2520between%2520two%2520Augmented%250ANeural%2520Ordinary%2520Differential%2520Equations%2520%2528ANODEs%2529%2520--%2520the%2520Shape-NODE%2520and%250AControl-NODE%2520--%2520to%2520achieve%2520continuous%2520shape%2520estimation%2520and%2520shape-aware%2520control.%250AThe%2520Shape-NODE%2520integrates%2520prior%2520knowledge%2520from%2520Cosserat%2520rod%2520theory%252C%2520allowing%2520it%250Ato%2520adapt%2520and%2520account%2520for%2520model%2520mismatches%252C%2520while%2520the%2520Control-NODE%2520uses%2520this%250Ashape%2520information%2520to%2520optimize%2520a%2520whole-body%2520control%2520policy%252C%2520trained%2520in%2520a%2520Model%250APredictive%2520Control%2520%2528MPC%2529%2520fashion.%2520This%2520unified%2520framework%2520effectively%2520overcomes%250Alimitations%2520of%2520existing%2520data-driven%2520methods%252C%2520such%2520as%2520poor%2520shape%2520awareness%2520and%250Achallenges%2520in%2520capturing%2520complex%2520nonlinear%2520dynamics.%2520Extensive%2520evaluations%2520in%250Aboth%2520simulation%2520and%2520real-world%2520environments%2520demonstrate%2520the%2520framework%2527s%2520robust%250Aperformance%2520in%2520shape%2520estimation%252C%2520trajectory%2520tracking%252C%2520and%2520obstacle%2520avoidance.%250AThe%2520proposed%2520method%2520consistently%2520outperforms%2520state-of-the-art%2520end-to-end%252C%250ANeural-ODE%252C%2520and%2520Recurrent%2520Neural%2520Network%2520%2528RNN%2529%2520models%252C%2520particularly%2520in%2520terms%2520of%250Atracking%2520accuracy%2520and%2520generalization%2520capabilities.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2501.03859v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=A%20Synergistic%20Framework%20for%20Learning%20Shape%20Estimation%20and%20Shape-Aware%0A%20%20Whole-Body%20Control%20Policy%20for%20Continuum%20Robots&entry.906535625=Mohammadreza%20Kasaei%20and%20Farshid%20Alambeigi%20and%20Mohsen%20Khadem&entry.1292438233=%20%20In%20this%20paper%2C%20we%20present%20a%20novel%20synergistic%20framework%20for%20learning%20shape%0Aestimation%20and%20a%20shape-aware%20whole-body%20control%20policy%20for%20tendon-driven%0Acontinuum%20robots.%20Our%20approach%20leverages%20the%20interaction%20between%20two%20Augmented%0ANeural%20Ordinary%20Differential%20Equations%20%28ANODEs%29%20--%20the%20Shape-NODE%20and%0AControl-NODE%20--%20to%20achieve%20continuous%20shape%20estimation%20and%20shape-aware%20control.%0AThe%20Shape-NODE%20integrates%20prior%20knowledge%20from%20Cosserat%20rod%20theory%2C%20allowing%20it%0Ato%20adapt%20and%20account%20for%20model%20mismatches%2C%20while%20the%20Control-NODE%20uses%20this%0Ashape%20information%20to%20optimize%20a%20whole-body%20control%20policy%2C%20trained%20in%20a%20Model%0APredictive%20Control%20%28MPC%29%20fashion.%20This%20unified%20framework%20effectively%20overcomes%0Alimitations%20of%20existing%20data-driven%20methods%2C%20such%20as%20poor%20shape%20awareness%20and%0Achallenges%20in%20capturing%20complex%20nonlinear%20dynamics.%20Extensive%20evaluations%20in%0Aboth%20simulation%20and%20real-world%20environments%20demonstrate%20the%20framework%27s%20robust%0Aperformance%20in%20shape%20estimation%2C%20trajectory%20tracking%2C%20and%20obstacle%20avoidance.%0AThe%20proposed%20method%20consistently%20outperforms%20state-of-the-art%20end-to-end%2C%0ANeural-ODE%2C%20and%20Recurrent%20Neural%20Network%20%28RNN%29%20models%2C%20particularly%20in%20terms%20of%0Atracking%20accuracy%20and%20generalization%20capabilities.%0A&entry.1838667208=http%3A//arxiv.org/abs/2501.03859v1&entry.124074799=Read"},
{"title": "Koopman Learning with Episodic Memory", "author": "William T. Redman and Dean Huang and Maria Fonoberova and Igor Mezi\u0107", "abstract": "  Koopman operator theory has found significant success in learning models of\ncomplex, real-world dynamical systems, enabling prediction and control. The\ngreater interpretability and lower computational costs of these models,\ncompared to traditional machine learning methodologies, make Koopman learning\nan especially appealing approach. Despite this, little work has been performed\non endowing Koopman learning with the ability to leverage its own failures. To\naddress this, we equip Koopman methods -- developed for predicting\nnon-autonomous time-series -- with an episodic memory mechanism, enabling\nglobal recall of (or attention to) periods in time where similar dynamics\npreviously occurred. We find that a basic implementation of Koopman learning\nwith episodic memory leads to significant improvements in prediction on\nsynthetic and real-world data. Our framework has considerable potential for\nexpansion, allowing for future advances, and opens exciting new directions for\nKoopman learning.\n", "link": "http://arxiv.org/abs/2311.12615v3", "date": "2025-01-07", "relevancy": 1.4243, "topK": [{"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.4887}, {"title": "RoboEXP: Action-Conditioned Scene Graph via Interactive Exploration for\n  Robotic Manipulation", "link": "http://arxiv.org/abs/2402.15487v1", "similarity": 0.4875}, {"title": "Self-supervised Photographic Image Layout Representation Learning", "link": "http://arxiv.org/abs/2403.03740v1", "similarity": 0.4641}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Koopman%20Learning%20with%20Episodic%20Memory&body=Title%3A%20Koopman%20Learning%20with%20Episodic%20Memory%0AAuthor%3A%20William%20T.%20Redman%20and%20Dean%20Huang%20and%20Maria%20Fonoberova%20and%20Igor%20Mezi%C4%87%0AAbstract%3A%20%20%20Koopman%20operator%20theory%20has%20found%20significant%20success%20in%20learning%20models%20of%0Acomplex%2C%20real-world%20dynamical%20systems%2C%20enabling%20prediction%20and%20control.%20The%0Agreater%20interpretability%20and%20lower%20computational%20costs%20of%20these%20models%2C%0Acompared%20to%20traditional%20machine%20learning%20methodologies%2C%20make%20Koopman%20learning%0Aan%20especially%20appealing%20approach.%20Despite%20this%2C%20little%20work%20has%20been%20performed%0Aon%20endowing%20Koopman%20learning%20with%20the%20ability%20to%20leverage%20its%20own%20failures.%20To%0Aaddress%20this%2C%20we%20equip%20Koopman%20methods%20--%20developed%20for%20predicting%0Anon-autonomous%20time-series%20--%20with%20an%20episodic%20memory%20mechanism%2C%20enabling%0Aglobal%20recall%20of%20%28or%20attention%20to%29%20periods%20in%20time%20where%20similar%20dynamics%0Apreviously%20occurred.%20We%20find%20that%20a%20basic%20implementation%20of%20Koopman%20learning%0Awith%20episodic%20memory%20leads%20to%20significant%20improvements%20in%20prediction%20on%0Asynthetic%20and%20real-world%20data.%20Our%20framework%20has%20considerable%20potential%20for%0Aexpansion%2C%20allowing%20for%20future%20advances%2C%20and%20opens%20exciting%20new%20directions%20for%0AKoopman%20learning.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2311.12615v3%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DKoopman%2520Learning%2520with%2520Episodic%2520Memory%26entry.906535625%3DWilliam%2520T.%2520Redman%2520and%2520Dean%2520Huang%2520and%2520Maria%2520Fonoberova%2520and%2520Igor%2520Mezi%25C4%2587%26entry.1292438233%3D%2520%2520Koopman%2520operator%2520theory%2520has%2520found%2520significant%2520success%2520in%2520learning%2520models%2520of%250Acomplex%252C%2520real-world%2520dynamical%2520systems%252C%2520enabling%2520prediction%2520and%2520control.%2520The%250Agreater%2520interpretability%2520and%2520lower%2520computational%2520costs%2520of%2520these%2520models%252C%250Acompared%2520to%2520traditional%2520machine%2520learning%2520methodologies%252C%2520make%2520Koopman%2520learning%250Aan%2520especially%2520appealing%2520approach.%2520Despite%2520this%252C%2520little%2520work%2520has%2520been%2520performed%250Aon%2520endowing%2520Koopman%2520learning%2520with%2520the%2520ability%2520to%2520leverage%2520its%2520own%2520failures.%2520To%250Aaddress%2520this%252C%2520we%2520equip%2520Koopman%2520methods%2520--%2520developed%2520for%2520predicting%250Anon-autonomous%2520time-series%2520--%2520with%2520an%2520episodic%2520memory%2520mechanism%252C%2520enabling%250Aglobal%2520recall%2520of%2520%2528or%2520attention%2520to%2529%2520periods%2520in%2520time%2520where%2520similar%2520dynamics%250Apreviously%2520occurred.%2520We%2520find%2520that%2520a%2520basic%2520implementation%2520of%2520Koopman%2520learning%250Awith%2520episodic%2520memory%2520leads%2520to%2520significant%2520improvements%2520in%2520prediction%2520on%250Asynthetic%2520and%2520real-world%2520data.%2520Our%2520framework%2520has%2520considerable%2520potential%2520for%250Aexpansion%252C%2520allowing%2520for%2520future%2520advances%252C%2520and%2520opens%2520exciting%2520new%2520directions%2520for%250AKoopman%2520learning.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2311.12615v3%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Koopman%20Learning%20with%20Episodic%20Memory&entry.906535625=William%20T.%20Redman%20and%20Dean%20Huang%20and%20Maria%20Fonoberova%20and%20Igor%20Mezi%C4%87&entry.1292438233=%20%20Koopman%20operator%20theory%20has%20found%20significant%20success%20in%20learning%20models%20of%0Acomplex%2C%20real-world%20dynamical%20systems%2C%20enabling%20prediction%20and%20control.%20The%0Agreater%20interpretability%20and%20lower%20computational%20costs%20of%20these%20models%2C%0Acompared%20to%20traditional%20machine%20learning%20methodologies%2C%20make%20Koopman%20learning%0Aan%20especially%20appealing%20approach.%20Despite%20this%2C%20little%20work%20has%20been%20performed%0Aon%20endowing%20Koopman%20learning%20with%20the%20ability%20to%20leverage%20its%20own%20failures.%20To%0Aaddress%20this%2C%20we%20equip%20Koopman%20methods%20--%20developed%20for%20predicting%0Anon-autonomous%20time-series%20--%20with%20an%20episodic%20memory%20mechanism%2C%20enabling%0Aglobal%20recall%20of%20%28or%20attention%20to%29%20periods%20in%20time%20where%20similar%20dynamics%0Apreviously%20occurred.%20We%20find%20that%20a%20basic%20implementation%20of%20Koopman%20learning%0Awith%20episodic%20memory%20leads%20to%20significant%20improvements%20in%20prediction%20on%0Asynthetic%20and%20real-world%20data.%20Our%20framework%20has%20considerable%20potential%20for%0Aexpansion%2C%20allowing%20for%20future%20advances%2C%20and%20opens%20exciting%20new%20directions%20for%0AKoopman%20learning.%0A&entry.1838667208=http%3A//arxiv.org/abs/2311.12615v3&entry.124074799=Read"},
{"title": "Class-Balance Bias in Regularized Regression", "author": "Johan Larsson and Jonas Wallin", "abstract": "  Regularized models are often sensitive to the scales of the features in the\ndata and it has therefore become standard practice to normalize (center and\nscale) the features before fitting the model. But there are many different ways\nto normalize the features and the choice may have dramatic effects on the\nresulting model. In spite of this, there has so far been no research on this\ntopic. In this paper, we begin to bridge this knowledge gap by studying\nnormalization in the context of lasso, ridge, and elastic net regression. We\nfocus on normal and binary features and show that the class balances of binary\nfeatures directly influences the regression coefficients and that this effect\ndepends on the combination of normalization and regularization methods used. We\ndemonstrate that this effect can be mitigated by scaling binary features with\ntheir variance in the case of the lasso and standard deviation in the case of\nridge regression, but that this comes at the cost of increased variance. For\nthe elastic net, we show that scaling the penalty weights, rather than the\nfeatures, can achieve the same effect. Finally, we also tackle mixes of binary\nand normal features as well as interactions and provide some initial results on\nhow to normalize features in these cases.\n", "link": "http://arxiv.org/abs/2501.03821v1", "date": "2025-01-07", "relevancy": 1.7873, "topK": [{"title": "Breadcrumbs: Adversarial class-balanced sampling for long-tailed recognition", "link": "https://link.springer.com/chapter/10.1007/978-3-031-20053-3_37", "similarity": 0.484}, {"title": "Boosted dynamic neural networks", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/26302", "similarity": 0.4209}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.4188}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Class-Balance%20Bias%20in%20Regularized%20Regression&body=Title%3A%20Class-Balance%20Bias%20in%20Regularized%20Regression%0AAuthor%3A%20Johan%20Larsson%20and%20Jonas%20Wallin%0AAbstract%3A%20%20%20Regularized%20models%20are%20often%20sensitive%20to%20the%20scales%20of%20the%20features%20in%20the%0Adata%20and%20it%20has%20therefore%20become%20standard%20practice%20to%20normalize%20%28center%20and%0Ascale%29%20the%20features%20before%20fitting%20the%20model.%20But%20there%20are%20many%20different%20ways%0Ato%20normalize%20the%20features%20and%20the%20choice%20may%20have%20dramatic%20effects%20on%20the%0Aresulting%20model.%20In%20spite%20of%20this%2C%20there%20has%20so%20far%20been%20no%20research%20on%20this%0Atopic.%20In%20this%20paper%2C%20we%20begin%20to%20bridge%20this%20knowledge%20gap%20by%20studying%0Anormalization%20in%20the%20context%20of%20lasso%2C%20ridge%2C%20and%20elastic%20net%20regression.%20We%0Afocus%20on%20normal%20and%20binary%20features%20and%20show%20that%20the%20class%20balances%20of%20binary%0Afeatures%20directly%20influences%20the%20regression%20coefficients%20and%20that%20this%20effect%0Adepends%20on%20the%20combination%20of%20normalization%20and%20regularization%20methods%20used.%20We%0Ademonstrate%20that%20this%20effect%20can%20be%20mitigated%20by%20scaling%20binary%20features%20with%0Atheir%20variance%20in%20the%20case%20of%20the%20lasso%20and%20standard%20deviation%20in%20the%20case%20of%0Aridge%20regression%2C%20but%20that%20this%20comes%20at%20the%20cost%20of%20increased%20variance.%20For%0Athe%20elastic%20net%2C%20we%20show%20that%20scaling%20the%20penalty%20weights%2C%20rather%20than%20the%0Afeatures%2C%20can%20achieve%20the%20same%20effect.%20Finally%2C%20we%20also%20tackle%20mixes%20of%20binary%0Aand%20normal%20features%20as%20well%20as%20interactions%20and%20provide%20some%20initial%20results%20on%0Ahow%20to%20normalize%20features%20in%20these%20cases.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2501.03821v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DClass-Balance%2520Bias%2520in%2520Regularized%2520Regression%26entry.906535625%3DJohan%2520Larsson%2520and%2520Jonas%2520Wallin%26entry.1292438233%3D%2520%2520Regularized%2520models%2520are%2520often%2520sensitive%2520to%2520the%2520scales%2520of%2520the%2520features%2520in%2520the%250Adata%2520and%2520it%2520has%2520therefore%2520become%2520standard%2520practice%2520to%2520normalize%2520%2528center%2520and%250Ascale%2529%2520the%2520features%2520before%2520fitting%2520the%2520model.%2520But%2520there%2520are%2520many%2520different%2520ways%250Ato%2520normalize%2520the%2520features%2520and%2520the%2520choice%2520may%2520have%2520dramatic%2520effects%2520on%2520the%250Aresulting%2520model.%2520In%2520spite%2520of%2520this%252C%2520there%2520has%2520so%2520far%2520been%2520no%2520research%2520on%2520this%250Atopic.%2520In%2520this%2520paper%252C%2520we%2520begin%2520to%2520bridge%2520this%2520knowledge%2520gap%2520by%2520studying%250Anormalization%2520in%2520the%2520context%2520of%2520lasso%252C%2520ridge%252C%2520and%2520elastic%2520net%2520regression.%2520We%250Afocus%2520on%2520normal%2520and%2520binary%2520features%2520and%2520show%2520that%2520the%2520class%2520balances%2520of%2520binary%250Afeatures%2520directly%2520influences%2520the%2520regression%2520coefficients%2520and%2520that%2520this%2520effect%250Adepends%2520on%2520the%2520combination%2520of%2520normalization%2520and%2520regularization%2520methods%2520used.%2520We%250Ademonstrate%2520that%2520this%2520effect%2520can%2520be%2520mitigated%2520by%2520scaling%2520binary%2520features%2520with%250Atheir%2520variance%2520in%2520the%2520case%2520of%2520the%2520lasso%2520and%2520standard%2520deviation%2520in%2520the%2520case%2520of%250Aridge%2520regression%252C%2520but%2520that%2520this%2520comes%2520at%2520the%2520cost%2520of%2520increased%2520variance.%2520For%250Athe%2520elastic%2520net%252C%2520we%2520show%2520that%2520scaling%2520the%2520penalty%2520weights%252C%2520rather%2520than%2520the%250Afeatures%252C%2520can%2520achieve%2520the%2520same%2520effect.%2520Finally%252C%2520we%2520also%2520tackle%2520mixes%2520of%2520binary%250Aand%2520normal%2520features%2520as%2520well%2520as%2520interactions%2520and%2520provide%2520some%2520initial%2520results%2520on%250Ahow%2520to%2520normalize%2520features%2520in%2520these%2520cases.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2501.03821v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Class-Balance%20Bias%20in%20Regularized%20Regression&entry.906535625=Johan%20Larsson%20and%20Jonas%20Wallin&entry.1292438233=%20%20Regularized%20models%20are%20often%20sensitive%20to%20the%20scales%20of%20the%20features%20in%20the%0Adata%20and%20it%20has%20therefore%20become%20standard%20practice%20to%20normalize%20%28center%20and%0Ascale%29%20the%20features%20before%20fitting%20the%20model.%20But%20there%20are%20many%20different%20ways%0Ato%20normalize%20the%20features%20and%20the%20choice%20may%20have%20dramatic%20effects%20on%20the%0Aresulting%20model.%20In%20spite%20of%20this%2C%20there%20has%20so%20far%20been%20no%20research%20on%20this%0Atopic.%20In%20this%20paper%2C%20we%20begin%20to%20bridge%20this%20knowledge%20gap%20by%20studying%0Anormalization%20in%20the%20context%20of%20lasso%2C%20ridge%2C%20and%20elastic%20net%20regression.%20We%0Afocus%20on%20normal%20and%20binary%20features%20and%20show%20that%20the%20class%20balances%20of%20binary%0Afeatures%20directly%20influences%20the%20regression%20coefficients%20and%20that%20this%20effect%0Adepends%20on%20the%20combination%20of%20normalization%20and%20regularization%20methods%20used.%20We%0Ademonstrate%20that%20this%20effect%20can%20be%20mitigated%20by%20scaling%20binary%20features%20with%0Atheir%20variance%20in%20the%20case%20of%20the%20lasso%20and%20standard%20deviation%20in%20the%20case%20of%0Aridge%20regression%2C%20but%20that%20this%20comes%20at%20the%20cost%20of%20increased%20variance.%20For%0Athe%20elastic%20net%2C%20we%20show%20that%20scaling%20the%20penalty%20weights%2C%20rather%20than%20the%0Afeatures%2C%20can%20achieve%20the%20same%20effect.%20Finally%2C%20we%20also%20tackle%20mixes%20of%20binary%0Aand%20normal%20features%20as%20well%20as%20interactions%20and%20provide%20some%20initial%20results%20on%0Ahow%20to%20normalize%20features%20in%20these%20cases.%0A&entry.1838667208=http%3A//arxiv.org/abs/2501.03821v1&entry.124074799=Read"},
{"title": "Implicit Coordination using Active Epistemic Inference", "author": "Lauren Bramblett and Jonathan Reasoner and Nicola Bezzo", "abstract": "  A Multi-robot system (MRS) provides significant advantages for intricate\ntasks such as environmental monitoring, underwater inspections, and space\nmissions. However, addressing potential communication failures or the lack of\ncommunication infrastructure in these fields remains a challenge. A significant\nportion of MRS research presumes that the system can maintain communication\nwith proximity constraints, but this approach does not solve situations where\ncommunication is either non-existent, unreliable, or poses a security risk.\nSome approaches tackle this issue using predictions about other robots while\nnot communicating, but these methods generally only permit agents to utilize\nfirst-order reasoning, which involves reasoning based purely on their own\nobservations. In contrast, to deal with this problem, our proposed framework\nutilizes Theory of Mind (ToM), employing higher-order reasoning by shifting a\nrobot's perspective to reason about a belief of others observations. Our\napproach has two main phases: i) an efficient runtime plan adaptation using\nactive inference to signal intentions and reason about a robot's own belief and\nthe beliefs of others in the system, and ii) a hierarchical epistemic planning\nframework to iteratively reason about the current MRS mission state. The\nproposed framework outperforms greedy and first-order reasoning approaches and\nis validated using simulations and experiments with heterogeneous robotic\nsystems.\n", "link": "http://arxiv.org/abs/2501.03907v1", "date": "2025-01-07", "relevancy": 1.5707, "topK": [{"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.5919}, {"title": "RoboEXP: Action-Conditioned Scene Graph via Interactive Exploration for\n  Robotic Manipulation", "link": "http://arxiv.org/abs/2402.15487v1", "similarity": 0.5616}, {"title": "Egocentric Computer Vision for Hands-Free Robotic Wheelchair Navigation", "link": "https://link.springer.com/article/10.1007/s10846-023-01807-4", "similarity": 0.481}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Implicit%20Coordination%20using%20Active%20Epistemic%20Inference&body=Title%3A%20Implicit%20Coordination%20using%20Active%20Epistemic%20Inference%0AAuthor%3A%20Lauren%20Bramblett%20and%20Jonathan%20Reasoner%20and%20Nicola%20Bezzo%0AAbstract%3A%20%20%20A%20Multi-robot%20system%20%28MRS%29%20provides%20significant%20advantages%20for%20intricate%0Atasks%20such%20as%20environmental%20monitoring%2C%20underwater%20inspections%2C%20and%20space%0Amissions.%20However%2C%20addressing%20potential%20communication%20failures%20or%20the%20lack%20of%0Acommunication%20infrastructure%20in%20these%20fields%20remains%20a%20challenge.%20A%20significant%0Aportion%20of%20MRS%20research%20presumes%20that%20the%20system%20can%20maintain%20communication%0Awith%20proximity%20constraints%2C%20but%20this%20approach%20does%20not%20solve%20situations%20where%0Acommunication%20is%20either%20non-existent%2C%20unreliable%2C%20or%20poses%20a%20security%20risk.%0ASome%20approaches%20tackle%20this%20issue%20using%20predictions%20about%20other%20robots%20while%0Anot%20communicating%2C%20but%20these%20methods%20generally%20only%20permit%20agents%20to%20utilize%0Afirst-order%20reasoning%2C%20which%20involves%20reasoning%20based%20purely%20on%20their%20own%0Aobservations.%20In%20contrast%2C%20to%20deal%20with%20this%20problem%2C%20our%20proposed%20framework%0Autilizes%20Theory%20of%20Mind%20%28ToM%29%2C%20employing%20higher-order%20reasoning%20by%20shifting%20a%0Arobot%27s%20perspective%20to%20reason%20about%20a%20belief%20of%20others%20observations.%20Our%0Aapproach%20has%20two%20main%20phases%3A%20i%29%20an%20efficient%20runtime%20plan%20adaptation%20using%0Aactive%20inference%20to%20signal%20intentions%20and%20reason%20about%20a%20robot%27s%20own%20belief%20and%0Athe%20beliefs%20of%20others%20in%20the%20system%2C%20and%20ii%29%20a%20hierarchical%20epistemic%20planning%0Aframework%20to%20iteratively%20reason%20about%20the%20current%20MRS%20mission%20state.%20The%0Aproposed%20framework%20outperforms%20greedy%20and%20first-order%20reasoning%20approaches%20and%0Ais%20validated%20using%20simulations%20and%20experiments%20with%20heterogeneous%20robotic%0Asystems.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2501.03907v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DImplicit%2520Coordination%2520using%2520Active%2520Epistemic%2520Inference%26entry.906535625%3DLauren%2520Bramblett%2520and%2520Jonathan%2520Reasoner%2520and%2520Nicola%2520Bezzo%26entry.1292438233%3D%2520%2520A%2520Multi-robot%2520system%2520%2528MRS%2529%2520provides%2520significant%2520advantages%2520for%2520intricate%250Atasks%2520such%2520as%2520environmental%2520monitoring%252C%2520underwater%2520inspections%252C%2520and%2520space%250Amissions.%2520However%252C%2520addressing%2520potential%2520communication%2520failures%2520or%2520the%2520lack%2520of%250Acommunication%2520infrastructure%2520in%2520these%2520fields%2520remains%2520a%2520challenge.%2520A%2520significant%250Aportion%2520of%2520MRS%2520research%2520presumes%2520that%2520the%2520system%2520can%2520maintain%2520communication%250Awith%2520proximity%2520constraints%252C%2520but%2520this%2520approach%2520does%2520not%2520solve%2520situations%2520where%250Acommunication%2520is%2520either%2520non-existent%252C%2520unreliable%252C%2520or%2520poses%2520a%2520security%2520risk.%250ASome%2520approaches%2520tackle%2520this%2520issue%2520using%2520predictions%2520about%2520other%2520robots%2520while%250Anot%2520communicating%252C%2520but%2520these%2520methods%2520generally%2520only%2520permit%2520agents%2520to%2520utilize%250Afirst-order%2520reasoning%252C%2520which%2520involves%2520reasoning%2520based%2520purely%2520on%2520their%2520own%250Aobservations.%2520In%2520contrast%252C%2520to%2520deal%2520with%2520this%2520problem%252C%2520our%2520proposed%2520framework%250Autilizes%2520Theory%2520of%2520Mind%2520%2528ToM%2529%252C%2520employing%2520higher-order%2520reasoning%2520by%2520shifting%2520a%250Arobot%2527s%2520perspective%2520to%2520reason%2520about%2520a%2520belief%2520of%2520others%2520observations.%2520Our%250Aapproach%2520has%2520two%2520main%2520phases%253A%2520i%2529%2520an%2520efficient%2520runtime%2520plan%2520adaptation%2520using%250Aactive%2520inference%2520to%2520signal%2520intentions%2520and%2520reason%2520about%2520a%2520robot%2527s%2520own%2520belief%2520and%250Athe%2520beliefs%2520of%2520others%2520in%2520the%2520system%252C%2520and%2520ii%2529%2520a%2520hierarchical%2520epistemic%2520planning%250Aframework%2520to%2520iteratively%2520reason%2520about%2520the%2520current%2520MRS%2520mission%2520state.%2520The%250Aproposed%2520framework%2520outperforms%2520greedy%2520and%2520first-order%2520reasoning%2520approaches%2520and%250Ais%2520validated%2520using%2520simulations%2520and%2520experiments%2520with%2520heterogeneous%2520robotic%250Asystems.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2501.03907v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Implicit%20Coordination%20using%20Active%20Epistemic%20Inference&entry.906535625=Lauren%20Bramblett%20and%20Jonathan%20Reasoner%20and%20Nicola%20Bezzo&entry.1292438233=%20%20A%20Multi-robot%20system%20%28MRS%29%20provides%20significant%20advantages%20for%20intricate%0Atasks%20such%20as%20environmental%20monitoring%2C%20underwater%20inspections%2C%20and%20space%0Amissions.%20However%2C%20addressing%20potential%20communication%20failures%20or%20the%20lack%20of%0Acommunication%20infrastructure%20in%20these%20fields%20remains%20a%20challenge.%20A%20significant%0Aportion%20of%20MRS%20research%20presumes%20that%20the%20system%20can%20maintain%20communication%0Awith%20proximity%20constraints%2C%20but%20this%20approach%20does%20not%20solve%20situations%20where%0Acommunication%20is%20either%20non-existent%2C%20unreliable%2C%20or%20poses%20a%20security%20risk.%0ASome%20approaches%20tackle%20this%20issue%20using%20predictions%20about%20other%20robots%20while%0Anot%20communicating%2C%20but%20these%20methods%20generally%20only%20permit%20agents%20to%20utilize%0Afirst-order%20reasoning%2C%20which%20involves%20reasoning%20based%20purely%20on%20their%20own%0Aobservations.%20In%20contrast%2C%20to%20deal%20with%20this%20problem%2C%20our%20proposed%20framework%0Autilizes%20Theory%20of%20Mind%20%28ToM%29%2C%20employing%20higher-order%20reasoning%20by%20shifting%20a%0Arobot%27s%20perspective%20to%20reason%20about%20a%20belief%20of%20others%20observations.%20Our%0Aapproach%20has%20two%20main%20phases%3A%20i%29%20an%20efficient%20runtime%20plan%20adaptation%20using%0Aactive%20inference%20to%20signal%20intentions%20and%20reason%20about%20a%20robot%27s%20own%20belief%20and%0Athe%20beliefs%20of%20others%20in%20the%20system%2C%20and%20ii%29%20a%20hierarchical%20epistemic%20planning%0Aframework%20to%20iteratively%20reason%20about%20the%20current%20MRS%20mission%20state.%20The%0Aproposed%20framework%20outperforms%20greedy%20and%20first-order%20reasoning%20approaches%20and%0Ais%20validated%20using%20simulations%20and%20experiments%20with%20heterogeneous%20robotic%0Asystems.%0A&entry.1838667208=http%3A//arxiv.org/abs/2501.03907v1&entry.124074799=Read"},
{"title": "Statistical Error Bounds for GANs with Nonlinear Objective Functionals", "author": "Jeremiah Birrell", "abstract": "  Generative adversarial networks (GANs) are unsupervised learning methods for\ntraining a generator distribution to produce samples that approximate those\ndrawn from a target distribution. Many such methods can be formulated as\nminimization of a metric or divergence between probability distributions.\nRecent works have derived statistical error bounds for GANs that are based on\nintegral probability metrics (IPMs), e.g., WGAN which is based on the\n1-Wasserstein metric. In general, IPMs are defined by optimizing a linear\nfunctional (difference of expectations) over a space of discriminators. A much\nlarger class of GANs, which we here call $(f,\\Gamma)$-GANs, can be constructed\nusing $f$-divergences (e.g., Jensen-Shannon, KL, or $\\alpha$-divergences)\ntogether with a regularizing discriminator space $\\Gamma$ (e.g., $1$-Lipschitz\nfunctions). These GANs have nonlinear objective functions, depending on the\nchoice of $f$, and have been shown to exhibit improved performance in a number\nof applications. In this work we derive statistical error bounds for\n$(f,\\Gamma)$-GANs for general classes of $f$ and $\\Gamma$ in the form of\nfinite-sample concentration inequalities. These results prove the statistical\nconsistency of $(f,\\Gamma)$-GANs and reduce to the known results for IPM-GANs\nin the appropriate limit. Finally, our results also give new insight into the\nperformance of GANs for distributions with unbounded support.\n", "link": "http://arxiv.org/abs/2406.16834v2", "date": "2025-01-07", "relevancy": 1.9049, "topK": [{"title": "Thinking Outside the BBox: Unconstrained Generative Object Compositing", "link": "http://arxiv.org/abs/2409.04559v2", "similarity": 0.5169}, {"title": "Tuning-Free Noise Rectification for High Fidelity Image-to-Video\n  Generation", "link": "http://arxiv.org/abs/2403.02827v1", "similarity": 0.4771}, {"title": "UGG: Unified Generative Grasping", "link": "https://arxiv.org/abs/2311.16917", "similarity": 0.4591}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Statistical%20Error%20Bounds%20for%20GANs%20with%20Nonlinear%20Objective%20Functionals&body=Title%3A%20Statistical%20Error%20Bounds%20for%20GANs%20with%20Nonlinear%20Objective%20Functionals%0AAuthor%3A%20Jeremiah%20Birrell%0AAbstract%3A%20%20%20Generative%20adversarial%20networks%20%28GANs%29%20are%20unsupervised%20learning%20methods%20for%0Atraining%20a%20generator%20distribution%20to%20produce%20samples%20that%20approximate%20those%0Adrawn%20from%20a%20target%20distribution.%20Many%20such%20methods%20can%20be%20formulated%20as%0Aminimization%20of%20a%20metric%20or%20divergence%20between%20probability%20distributions.%0ARecent%20works%20have%20derived%20statistical%20error%20bounds%20for%20GANs%20that%20are%20based%20on%0Aintegral%20probability%20metrics%20%28IPMs%29%2C%20e.g.%2C%20WGAN%20which%20is%20based%20on%20the%0A1-Wasserstein%20metric.%20In%20general%2C%20IPMs%20are%20defined%20by%20optimizing%20a%20linear%0Afunctional%20%28difference%20of%20expectations%29%20over%20a%20space%20of%20discriminators.%20A%20much%0Alarger%20class%20of%20GANs%2C%20which%20we%20here%20call%20%24%28f%2C%5CGamma%29%24-GANs%2C%20can%20be%20constructed%0Ausing%20%24f%24-divergences%20%28e.g.%2C%20Jensen-Shannon%2C%20KL%2C%20or%20%24%5Calpha%24-divergences%29%0Atogether%20with%20a%20regularizing%20discriminator%20space%20%24%5CGamma%24%20%28e.g.%2C%20%241%24-Lipschitz%0Afunctions%29.%20These%20GANs%20have%20nonlinear%20objective%20functions%2C%20depending%20on%20the%0Achoice%20of%20%24f%24%2C%20and%20have%20been%20shown%20to%20exhibit%20improved%20performance%20in%20a%20number%0Aof%20applications.%20In%20this%20work%20we%20derive%20statistical%20error%20bounds%20for%0A%24%28f%2C%5CGamma%29%24-GANs%20for%20general%20classes%20of%20%24f%24%20and%20%24%5CGamma%24%20in%20the%20form%20of%0Afinite-sample%20concentration%20inequalities.%20These%20results%20prove%20the%20statistical%0Aconsistency%20of%20%24%28f%2C%5CGamma%29%24-GANs%20and%20reduce%20to%20the%20known%20results%20for%20IPM-GANs%0Ain%20the%20appropriate%20limit.%20Finally%2C%20our%20results%20also%20give%20new%20insight%20into%20the%0Aperformance%20of%20GANs%20for%20distributions%20with%20unbounded%20support.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2406.16834v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DStatistical%2520Error%2520Bounds%2520for%2520GANs%2520with%2520Nonlinear%2520Objective%2520Functionals%26entry.906535625%3DJeremiah%2520Birrell%26entry.1292438233%3D%2520%2520Generative%2520adversarial%2520networks%2520%2528GANs%2529%2520are%2520unsupervised%2520learning%2520methods%2520for%250Atraining%2520a%2520generator%2520distribution%2520to%2520produce%2520samples%2520that%2520approximate%2520those%250Adrawn%2520from%2520a%2520target%2520distribution.%2520Many%2520such%2520methods%2520can%2520be%2520formulated%2520as%250Aminimization%2520of%2520a%2520metric%2520or%2520divergence%2520between%2520probability%2520distributions.%250ARecent%2520works%2520have%2520derived%2520statistical%2520error%2520bounds%2520for%2520GANs%2520that%2520are%2520based%2520on%250Aintegral%2520probability%2520metrics%2520%2528IPMs%2529%252C%2520e.g.%252C%2520WGAN%2520which%2520is%2520based%2520on%2520the%250A1-Wasserstein%2520metric.%2520In%2520general%252C%2520IPMs%2520are%2520defined%2520by%2520optimizing%2520a%2520linear%250Afunctional%2520%2528difference%2520of%2520expectations%2529%2520over%2520a%2520space%2520of%2520discriminators.%2520A%2520much%250Alarger%2520class%2520of%2520GANs%252C%2520which%2520we%2520here%2520call%2520%2524%2528f%252C%255CGamma%2529%2524-GANs%252C%2520can%2520be%2520constructed%250Ausing%2520%2524f%2524-divergences%2520%2528e.g.%252C%2520Jensen-Shannon%252C%2520KL%252C%2520or%2520%2524%255Calpha%2524-divergences%2529%250Atogether%2520with%2520a%2520regularizing%2520discriminator%2520space%2520%2524%255CGamma%2524%2520%2528e.g.%252C%2520%25241%2524-Lipschitz%250Afunctions%2529.%2520These%2520GANs%2520have%2520nonlinear%2520objective%2520functions%252C%2520depending%2520on%2520the%250Achoice%2520of%2520%2524f%2524%252C%2520and%2520have%2520been%2520shown%2520to%2520exhibit%2520improved%2520performance%2520in%2520a%2520number%250Aof%2520applications.%2520In%2520this%2520work%2520we%2520derive%2520statistical%2520error%2520bounds%2520for%250A%2524%2528f%252C%255CGamma%2529%2524-GANs%2520for%2520general%2520classes%2520of%2520%2524f%2524%2520and%2520%2524%255CGamma%2524%2520in%2520the%2520form%2520of%250Afinite-sample%2520concentration%2520inequalities.%2520These%2520results%2520prove%2520the%2520statistical%250Aconsistency%2520of%2520%2524%2528f%252C%255CGamma%2529%2524-GANs%2520and%2520reduce%2520to%2520the%2520known%2520results%2520for%2520IPM-GANs%250Ain%2520the%2520appropriate%2520limit.%2520Finally%252C%2520our%2520results%2520also%2520give%2520new%2520insight%2520into%2520the%250Aperformance%2520of%2520GANs%2520for%2520distributions%2520with%2520unbounded%2520support.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2406.16834v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Statistical%20Error%20Bounds%20for%20GANs%20with%20Nonlinear%20Objective%20Functionals&entry.906535625=Jeremiah%20Birrell&entry.1292438233=%20%20Generative%20adversarial%20networks%20%28GANs%29%20are%20unsupervised%20learning%20methods%20for%0Atraining%20a%20generator%20distribution%20to%20produce%20samples%20that%20approximate%20those%0Adrawn%20from%20a%20target%20distribution.%20Many%20such%20methods%20can%20be%20formulated%20as%0Aminimization%20of%20a%20metric%20or%20divergence%20between%20probability%20distributions.%0ARecent%20works%20have%20derived%20statistical%20error%20bounds%20for%20GANs%20that%20are%20based%20on%0Aintegral%20probability%20metrics%20%28IPMs%29%2C%20e.g.%2C%20WGAN%20which%20is%20based%20on%20the%0A1-Wasserstein%20metric.%20In%20general%2C%20IPMs%20are%20defined%20by%20optimizing%20a%20linear%0Afunctional%20%28difference%20of%20expectations%29%20over%20a%20space%20of%20discriminators.%20A%20much%0Alarger%20class%20of%20GANs%2C%20which%20we%20here%20call%20%24%28f%2C%5CGamma%29%24-GANs%2C%20can%20be%20constructed%0Ausing%20%24f%24-divergences%20%28e.g.%2C%20Jensen-Shannon%2C%20KL%2C%20or%20%24%5Calpha%24-divergences%29%0Atogether%20with%20a%20regularizing%20discriminator%20space%20%24%5CGamma%24%20%28e.g.%2C%20%241%24-Lipschitz%0Afunctions%29.%20These%20GANs%20have%20nonlinear%20objective%20functions%2C%20depending%20on%20the%0Achoice%20of%20%24f%24%2C%20and%20have%20been%20shown%20to%20exhibit%20improved%20performance%20in%20a%20number%0Aof%20applications.%20In%20this%20work%20we%20derive%20statistical%20error%20bounds%20for%0A%24%28f%2C%5CGamma%29%24-GANs%20for%20general%20classes%20of%20%24f%24%20and%20%24%5CGamma%24%20in%20the%20form%20of%0Afinite-sample%20concentration%20inequalities.%20These%20results%20prove%20the%20statistical%0Aconsistency%20of%20%24%28f%2C%5CGamma%29%24-GANs%20and%20reduce%20to%20the%20known%20results%20for%20IPM-GANs%0Ain%20the%20appropriate%20limit.%20Finally%2C%20our%20results%20also%20give%20new%20insight%20into%20the%0Aperformance%20of%20GANs%20for%20distributions%20with%20unbounded%20support.%0A&entry.1838667208=http%3A//arxiv.org/abs/2406.16834v2&entry.124074799=Read"},
      ];
      const content = document.getElementById('content');
      function createPostElement(post) {
        const postElement = document.createElement('div');
        postElement.className = 'post';
        const dateElem = document.createElement('p');
        dateElem.setAttribute("class", "date");
        dateElem.textContent = post.date;
        postElement.appendChild(dateElem);

        const textElem = document.createElement('p');
        textElem.setAttribute("class", "text");
        const titleElem = document.createElement('p');
        titleElem.setAttribute("class", "title");
        titleElem.textContent = post.title;
        textElem.appendChild(titleElem);
        const authorElem = document.createElement('p');
        authorElem.setAttribute("class", "author");
        authorElem.textContent = post.author;
        textElem.appendChild(authorElem);
        const abstractElem = document.createElement('p');
        abstractElem.setAttribute("class", "abstract");
        abstractElem.textContent = post.abstract;
        textElem.appendChild(abstractElem);

        const linkElement = document.createElement('a');
        linkElement.setAttribute("class", "link");
        linkElement.href = post.link;
        linkElement.target = "_blank";
        linkElement.textContent = post.link.length > 50 ? post.link.substring(0, 50) + '...' : post.link;
        textElem.appendChild(linkElement);
        postElement.appendChild(textElem);

        const linkElementContainer = document.createElement('div');
        linkElementContainer.setAttribute("class", "comment");
        const actionElement = document.createElement('a');
        actionElement.setAttribute("class", "comment");
        actionElement.href = post.form;
        actionElement.textContent = "Action";
        actionElement.target = "_blank";
        linkElementContainer.appendChild(actionElement);
        const emailElement = document.createElement('a');
        emailElement.setAttribute("class", "comment");
        emailElement.href = post.mailto;
        emailElement.textContent = "Email";
        emailElement.target = "_blank";
        linkElementContainer.appendChild(emailElement);
        postElement.appendChild(linkElementContainer);
        const e = document.createElement('div');
        e.setAttribute("class", "clear");
        postElement.appendChild(e);

        const relevancyContainer = document.createElement('div');
        const relevancyValElem = document.createElement('p');
        relevancyValElem.textContent = "Relevancy " + post.relevancy;
        relevancyContainer.appendChild(relevancyValElem);
        post.topK.forEach((sub) => {
          const topKElem = document.createElement('a');
          topKElem.setAttribute("class", "topK");
          topKElem.href = sub.link;
          topKElem.textContent = sub.title + " (" + sub.similarity + ")";
          topKElem.target = "_blank";
          relevancyContainer.appendChild(topKElem);
        });
        postElement.appendChild(relevancyContainer);
        return postElement;
      }
      function loadPosts() {
        // Simulate loading more posts
        posts.forEach((post) => {
          const postElement = createPostElement(post);
          content.appendChild(postElement);
        });
      }
      // Load initial posts
      loadPosts();
    </script>

  </body>
</html>


