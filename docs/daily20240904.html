<!DOCTYPE html>
<html lang="en">
  <head>
    <script async src="https://www.googletagmanager.com/gtag/js?id=G-V34CNNDP8V"></script>
    <script>
      window.dataLayer = window.dataLayer || [];
      function gtag(){dataLayer.push(arguments);}
      gtag('js', new Date());
      gtag('config', 'G-V34CNNDP8V');
    </script>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Arxiv Paper Selection</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.3/css/all.min.css" crossorigin="anonymous" referrerpolicy="no-referrer" />
    <style>
    body {
      font-family: Arial, sans-serif;
      margin: 0;
      padding: 0;
      background-color: #f4f4f4;
    }
    header {
      position: fixed;
      top: 0;
      left: 0;
      width: 100%;
      background-color: #ffffff;
      color: black;
      padding: 10px;
      text-align: center;
      z-index: 1000;
      border-bottom: 1px solid #ddd;
    }
    header div {
      display: block;
      margin: 10px auto;
    }

    #home-icon {
      display: block;
      float: left;
      margin: 5px;
      text-decoration: none;
      color: black;
    }

    main {
      margin-top: 60px; /* Adjusted margin to account for fixed header */
      padding: 20px;
    }

    .post {
      background-color: white;
      border: 1px solid #ddd;
      border-radius: 5px;
      margin-bottom: 10px;
      padding: 10px 20px;
      max-height: 2000px;
      overflow: scroll;
    }
    .post img {
      display: block;
      margin-top: 5px;
      max-width: auto;
      max-height: 100px;
    }
    .post .clear {
      clear: both;
      display: block;
    }
    .post a {
      text-decoration: none;
    }
    .post a:hover {
      color: #0056b3;
    }
    .post a:visited {
      color: #0056b3;
    }
    .post div.comment {
      text-align: right;
    }
    .post div.comment a {
      margin: 1em;
    }
    .post .text {
      margin: 1em 0em;
      padding: 0;
    }
    .post .text .title {
    }
    .post .text .author {
    }
    .post .text .abstract {
    }
    .post .topK {
      display: block;
      margin: 0.5em;
    }
    .post .date {
      margin: 0;
      padding: 0;
      text-size: small; 
      color: gray;
    }
    .post .link {
      margin: 0;
      padding: 0;
    }
    @media screen and (max-width: 600px) {
      body {
        max-width: 100%; 
      }
      #home-icon {
        float: none;
        display: block;
        text-align: center;
        margin-bottom: 10px;
      }
    }
    footer {
      width: 100%;
      background-color: #ddd;
      text-align: center;
      z-index: 1000;
      padding: 20px 0px;
      margin-bottom: 20px;
      left: 0;
    }

    #next-btn,
    #prev-btn {
      background-color: #4CAF50;
      color: white;
      padding: 8px 16px;
      margin: 0 50px;
      border: none;
      border-radius: 4px;
      cursor: pointer;
    }

    .links {
      padding: 20px;
    }
    .links a {
      text-decoration: none;
    }
    .links a:hover {
      color: #0056b3;
    }
    .links a:visited {
      color: #0056b3;
    }

    #page-index {
      font-size: small;
    }
    .ads {
      width: 100%;
    }
    #prev-btn {
      background-color: #4CAF50;
      color: white;
      padding: 8px 16px;
      margin: 0 50px;
      border: none;
      border-radius: 4px;
      cursor: pointer;
    }
    </style>
  </head>
  <body>

    <header>
      <a id="prev-btn" href="daily20240902.html"><i class="fas fa-chevron-left"></i></a>
      <a href="https://haoxiang.org/">About</a>
    </header>

    <main id="content">
      <!-- Posts will be dynamically added here using JavaScript -->
    </main>

    <script>
      // Dummy data for posts
      const posts = [
{"title": "Learning a Generalized Physical Face Model From Data", "author": "Lingchen Yang and Gaspard Zoss and Prashanth Chandran and Markus Gross and Barbara Solenthaler and Eftychios Sifakis and Derek Bradley", "abstract": "  Physically-based simulation is a powerful approach for 3D facial animation as\nthe resulting deformations are governed by physical constraints, allowing to\neasily resolve self-collisions, respond to external forces and perform\nrealistic anatomy edits. Today's methods are data-driven, where the actuations\nfor finite elements are inferred from captured skin geometry. Unfortunately,\nthese approaches have not been widely adopted due to the complexity of\ninitializing the material space and learning the deformation model for each\ncharacter separately, which often requires a skilled artist followed by lengthy\nnetwork training. In this work, we aim to make physics-based facial animation\nmore accessible by proposing a generalized physical face model that we learn\nfrom a large 3D face dataset. Once trained, our model can be quickly fit to any\nunseen identity and produce a ready-to-animate physical face model\nautomatically. Fitting is as easy as providing a single 3D face scan, or even a\nsingle face image. After fitting, we offer intuitive animation controls, as\nwell as the ability to retarget animations across characters. All the while,\nthe resulting animations allow for physical effects like collision avoidance,\ngravity, paralysis, bone reshaping and more.\n", "link": "http://arxiv.org/abs/2402.19477v2", "date": "2024-09-03", "relevancy": 3.061, "topK": [{"title": "3D Gaussian Blendshapes for Head Avatar Animation", "link": "http://arxiv.org/abs/2404.19398v2", "similarity": 0.6324}, {"title": "3D Gaussian Blendshapes for Head Avatar Animation", "link": "http://arxiv.org/abs/2404.19398v2", "similarity": 0.6324}, {"title": "PoseAnimate: Zero-shot high fidelity pose controllable character\n  animation", "link": "http://arxiv.org/abs/2404.13680v2", "similarity": 0.5719}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Learning%20a%20Generalized%20Physical%20Face%20Model%20From%20Data&body=Title%3A%20Learning%20a%20Generalized%20Physical%20Face%20Model%20From%20Data%0AAuthor%3A%20Lingchen%20Yang%20and%20Gaspard%20Zoss%20and%20Prashanth%20Chandran%20and%20Markus%20Gross%20and%20Barbara%20Solenthaler%20and%20Eftychios%20Sifakis%20and%20Derek%20Bradley%0AAbstract%3A%20%20%20Physically-based%20simulation%20is%20a%20powerful%20approach%20for%203D%20facial%20animation%20as%0Athe%20resulting%20deformations%20are%20governed%20by%20physical%20constraints%2C%20allowing%20to%0Aeasily%20resolve%20self-collisions%2C%20respond%20to%20external%20forces%20and%20perform%0Arealistic%20anatomy%20edits.%20Today%27s%20methods%20are%20data-driven%2C%20where%20the%20actuations%0Afor%20finite%20elements%20are%20inferred%20from%20captured%20skin%20geometry.%20Unfortunately%2C%0Athese%20approaches%20have%20not%20been%20widely%20adopted%20due%20to%20the%20complexity%20of%0Ainitializing%20the%20material%20space%20and%20learning%20the%20deformation%20model%20for%20each%0Acharacter%20separately%2C%20which%20often%20requires%20a%20skilled%20artist%20followed%20by%20lengthy%0Anetwork%20training.%20In%20this%20work%2C%20we%20aim%20to%20make%20physics-based%20facial%20animation%0Amore%20accessible%20by%20proposing%20a%20generalized%20physical%20face%20model%20that%20we%20learn%0Afrom%20a%20large%203D%20face%20dataset.%20Once%20trained%2C%20our%20model%20can%20be%20quickly%20fit%20to%20any%0Aunseen%20identity%20and%20produce%20a%20ready-to-animate%20physical%20face%20model%0Aautomatically.%20Fitting%20is%20as%20easy%20as%20providing%20a%20single%203D%20face%20scan%2C%20or%20even%20a%0Asingle%20face%20image.%20After%20fitting%2C%20we%20offer%20intuitive%20animation%20controls%2C%20as%0Awell%20as%20the%20ability%20to%20retarget%20animations%20across%20characters.%20All%20the%20while%2C%0Athe%20resulting%20animations%20allow%20for%20physical%20effects%20like%20collision%20avoidance%2C%0Agravity%2C%20paralysis%2C%20bone%20reshaping%20and%20more.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2402.19477v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DLearning%2520a%2520Generalized%2520Physical%2520Face%2520Model%2520From%2520Data%26entry.906535625%3DLingchen%2520Yang%2520and%2520Gaspard%2520Zoss%2520and%2520Prashanth%2520Chandran%2520and%2520Markus%2520Gross%2520and%2520Barbara%2520Solenthaler%2520and%2520Eftychios%2520Sifakis%2520and%2520Derek%2520Bradley%26entry.1292438233%3D%2520%2520Physically-based%2520simulation%2520is%2520a%2520powerful%2520approach%2520for%25203D%2520facial%2520animation%2520as%250Athe%2520resulting%2520deformations%2520are%2520governed%2520by%2520physical%2520constraints%252C%2520allowing%2520to%250Aeasily%2520resolve%2520self-collisions%252C%2520respond%2520to%2520external%2520forces%2520and%2520perform%250Arealistic%2520anatomy%2520edits.%2520Today%2527s%2520methods%2520are%2520data-driven%252C%2520where%2520the%2520actuations%250Afor%2520finite%2520elements%2520are%2520inferred%2520from%2520captured%2520skin%2520geometry.%2520Unfortunately%252C%250Athese%2520approaches%2520have%2520not%2520been%2520widely%2520adopted%2520due%2520to%2520the%2520complexity%2520of%250Ainitializing%2520the%2520material%2520space%2520and%2520learning%2520the%2520deformation%2520model%2520for%2520each%250Acharacter%2520separately%252C%2520which%2520often%2520requires%2520a%2520skilled%2520artist%2520followed%2520by%2520lengthy%250Anetwork%2520training.%2520In%2520this%2520work%252C%2520we%2520aim%2520to%2520make%2520physics-based%2520facial%2520animation%250Amore%2520accessible%2520by%2520proposing%2520a%2520generalized%2520physical%2520face%2520model%2520that%2520we%2520learn%250Afrom%2520a%2520large%25203D%2520face%2520dataset.%2520Once%2520trained%252C%2520our%2520model%2520can%2520be%2520quickly%2520fit%2520to%2520any%250Aunseen%2520identity%2520and%2520produce%2520a%2520ready-to-animate%2520physical%2520face%2520model%250Aautomatically.%2520Fitting%2520is%2520as%2520easy%2520as%2520providing%2520a%2520single%25203D%2520face%2520scan%252C%2520or%2520even%2520a%250Asingle%2520face%2520image.%2520After%2520fitting%252C%2520we%2520offer%2520intuitive%2520animation%2520controls%252C%2520as%250Awell%2520as%2520the%2520ability%2520to%2520retarget%2520animations%2520across%2520characters.%2520All%2520the%2520while%252C%250Athe%2520resulting%2520animations%2520allow%2520for%2520physical%2520effects%2520like%2520collision%2520avoidance%252C%250Agravity%252C%2520paralysis%252C%2520bone%2520reshaping%2520and%2520more.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2402.19477v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Learning%20a%20Generalized%20Physical%20Face%20Model%20From%20Data&entry.906535625=Lingchen%20Yang%20and%20Gaspard%20Zoss%20and%20Prashanth%20Chandran%20and%20Markus%20Gross%20and%20Barbara%20Solenthaler%20and%20Eftychios%20Sifakis%20and%20Derek%20Bradley&entry.1292438233=%20%20Physically-based%20simulation%20is%20a%20powerful%20approach%20for%203D%20facial%20animation%20as%0Athe%20resulting%20deformations%20are%20governed%20by%20physical%20constraints%2C%20allowing%20to%0Aeasily%20resolve%20self-collisions%2C%20respond%20to%20external%20forces%20and%20perform%0Arealistic%20anatomy%20edits.%20Today%27s%20methods%20are%20data-driven%2C%20where%20the%20actuations%0Afor%20finite%20elements%20are%20inferred%20from%20captured%20skin%20geometry.%20Unfortunately%2C%0Athese%20approaches%20have%20not%20been%20widely%20adopted%20due%20to%20the%20complexity%20of%0Ainitializing%20the%20material%20space%20and%20learning%20the%20deformation%20model%20for%20each%0Acharacter%20separately%2C%20which%20often%20requires%20a%20skilled%20artist%20followed%20by%20lengthy%0Anetwork%20training.%20In%20this%20work%2C%20we%20aim%20to%20make%20physics-based%20facial%20animation%0Amore%20accessible%20by%20proposing%20a%20generalized%20physical%20face%20model%20that%20we%20learn%0Afrom%20a%20large%203D%20face%20dataset.%20Once%20trained%2C%20our%20model%20can%20be%20quickly%20fit%20to%20any%0Aunseen%20identity%20and%20produce%20a%20ready-to-animate%20physical%20face%20model%0Aautomatically.%20Fitting%20is%20as%20easy%20as%20providing%20a%20single%203D%20face%20scan%2C%20or%20even%20a%0Asingle%20face%20image.%20After%20fitting%2C%20we%20offer%20intuitive%20animation%20controls%2C%20as%0Awell%20as%20the%20ability%20to%20retarget%20animations%20across%20characters.%20All%20the%20while%2C%0Athe%20resulting%20animations%20allow%20for%20physical%20effects%20like%20collision%20avoidance%2C%0Agravity%2C%20paralysis%2C%20bone%20reshaping%20and%20more.%0A&entry.1838667208=http%3A//arxiv.org/abs/2402.19477v2&entry.124074799=Read"},
{"title": "3DGS.zip: A survey on 3D Gaussian Splatting Compression Methods", "author": "Milena T. Bagdasarian and Paul Knoll and Florian Barthel and Anna Hilsmann and Peter Eisert and Wieland Morgenstern", "abstract": "  We present a work-in-progress survey on 3D Gaussian Splatting compression\nmethods, focusing on their statistical performance across various benchmarks.\nThis survey aims to facilitate comparability by summarizing key statistics of\ndifferent compression approaches in a tabulated format. The datasets evaluated\ninclude TanksAndTemples, MipNeRF360, DeepBlending, and SyntheticNeRF. For each\nmethod, we report the Peak Signal-to-Noise Ratio (PSNR), Structural Similarity\nIndex (SSIM), Learned Perceptual Image Patch Similarity (LPIPS), and the\nresultant size in megabytes (MB), as provided by the respective authors. This\nis an ongoing, open project, and we invite contributions from the research\ncommunity as GitHub issues or pull requests. Please visit\nhttp://w-m.github.io/3dgs-compression-survey/ for more information and a\nsortable version of the table.\n", "link": "http://arxiv.org/abs/2407.09510v3", "date": "2024-09-03", "relevancy": 2.9735, "topK": [{"title": "RTG-SLAM: Real-time 3D Reconstruction at Scale using Gaussian Splatting", "link": "http://arxiv.org/abs/2404.19706v1", "similarity": 0.6604}, {"title": "Dynamic Gaussians Mesh: Consistent Mesh Reconstruction from Monocular\n  Videos", "link": "http://arxiv.org/abs/2404.12379v2", "similarity": 0.5965}, {"title": "CAT3D: Create Anything in 3D with Multi-View Diffusion Models", "link": "http://arxiv.org/abs/2405.10314v1", "similarity": 0.5272}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%203DGS.zip%3A%20A%20survey%20on%203D%20Gaussian%20Splatting%20Compression%20Methods&body=Title%3A%203DGS.zip%3A%20A%20survey%20on%203D%20Gaussian%20Splatting%20Compression%20Methods%0AAuthor%3A%20Milena%20T.%20Bagdasarian%20and%20Paul%20Knoll%20and%20Florian%20Barthel%20and%20Anna%20Hilsmann%20and%20Peter%20Eisert%20and%20Wieland%20Morgenstern%0AAbstract%3A%20%20%20We%20present%20a%20work-in-progress%20survey%20on%203D%20Gaussian%20Splatting%20compression%0Amethods%2C%20focusing%20on%20their%20statistical%20performance%20across%20various%20benchmarks.%0AThis%20survey%20aims%20to%20facilitate%20comparability%20by%20summarizing%20key%20statistics%20of%0Adifferent%20compression%20approaches%20in%20a%20tabulated%20format.%20The%20datasets%20evaluated%0Ainclude%20TanksAndTemples%2C%20MipNeRF360%2C%20DeepBlending%2C%20and%20SyntheticNeRF.%20For%20each%0Amethod%2C%20we%20report%20the%20Peak%20Signal-to-Noise%20Ratio%20%28PSNR%29%2C%20Structural%20Similarity%0AIndex%20%28SSIM%29%2C%20Learned%20Perceptual%20Image%20Patch%20Similarity%20%28LPIPS%29%2C%20and%20the%0Aresultant%20size%20in%20megabytes%20%28MB%29%2C%20as%20provided%20by%20the%20respective%20authors.%20This%0Ais%20an%20ongoing%2C%20open%20project%2C%20and%20we%20invite%20contributions%20from%20the%20research%0Acommunity%20as%20GitHub%20issues%20or%20pull%20requests.%20Please%20visit%0Ahttp%3A//w-m.github.io/3dgs-compression-survey/%20for%20more%20information%20and%20a%0Asortable%20version%20of%20the%20table.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2407.09510v3%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3D3DGS.zip%253A%2520A%2520survey%2520on%25203D%2520Gaussian%2520Splatting%2520Compression%2520Methods%26entry.906535625%3DMilena%2520T.%2520Bagdasarian%2520and%2520Paul%2520Knoll%2520and%2520Florian%2520Barthel%2520and%2520Anna%2520Hilsmann%2520and%2520Peter%2520Eisert%2520and%2520Wieland%2520Morgenstern%26entry.1292438233%3D%2520%2520We%2520present%2520a%2520work-in-progress%2520survey%2520on%25203D%2520Gaussian%2520Splatting%2520compression%250Amethods%252C%2520focusing%2520on%2520their%2520statistical%2520performance%2520across%2520various%2520benchmarks.%250AThis%2520survey%2520aims%2520to%2520facilitate%2520comparability%2520by%2520summarizing%2520key%2520statistics%2520of%250Adifferent%2520compression%2520approaches%2520in%2520a%2520tabulated%2520format.%2520The%2520datasets%2520evaluated%250Ainclude%2520TanksAndTemples%252C%2520MipNeRF360%252C%2520DeepBlending%252C%2520and%2520SyntheticNeRF.%2520For%2520each%250Amethod%252C%2520we%2520report%2520the%2520Peak%2520Signal-to-Noise%2520Ratio%2520%2528PSNR%2529%252C%2520Structural%2520Similarity%250AIndex%2520%2528SSIM%2529%252C%2520Learned%2520Perceptual%2520Image%2520Patch%2520Similarity%2520%2528LPIPS%2529%252C%2520and%2520the%250Aresultant%2520size%2520in%2520megabytes%2520%2528MB%2529%252C%2520as%2520provided%2520by%2520the%2520respective%2520authors.%2520This%250Ais%2520an%2520ongoing%252C%2520open%2520project%252C%2520and%2520we%2520invite%2520contributions%2520from%2520the%2520research%250Acommunity%2520as%2520GitHub%2520issues%2520or%2520pull%2520requests.%2520Please%2520visit%250Ahttp%253A//w-m.github.io/3dgs-compression-survey/%2520for%2520more%2520information%2520and%2520a%250Asortable%2520version%2520of%2520the%2520table.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2407.09510v3%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=3DGS.zip%3A%20A%20survey%20on%203D%20Gaussian%20Splatting%20Compression%20Methods&entry.906535625=Milena%20T.%20Bagdasarian%20and%20Paul%20Knoll%20and%20Florian%20Barthel%20and%20Anna%20Hilsmann%20and%20Peter%20Eisert%20and%20Wieland%20Morgenstern&entry.1292438233=%20%20We%20present%20a%20work-in-progress%20survey%20on%203D%20Gaussian%20Splatting%20compression%0Amethods%2C%20focusing%20on%20their%20statistical%20performance%20across%20various%20benchmarks.%0AThis%20survey%20aims%20to%20facilitate%20comparability%20by%20summarizing%20key%20statistics%20of%0Adifferent%20compression%20approaches%20in%20a%20tabulated%20format.%20The%20datasets%20evaluated%0Ainclude%20TanksAndTemples%2C%20MipNeRF360%2C%20DeepBlending%2C%20and%20SyntheticNeRF.%20For%20each%0Amethod%2C%20we%20report%20the%20Peak%20Signal-to-Noise%20Ratio%20%28PSNR%29%2C%20Structural%20Similarity%0AIndex%20%28SSIM%29%2C%20Learned%20Perceptual%20Image%20Patch%20Similarity%20%28LPIPS%29%2C%20and%20the%0Aresultant%20size%20in%20megabytes%20%28MB%29%2C%20as%20provided%20by%20the%20respective%20authors.%20This%0Ais%20an%20ongoing%2C%20open%20project%2C%20and%20we%20invite%20contributions%20from%20the%20research%0Acommunity%20as%20GitHub%20issues%20or%20pull%20requests.%20Please%20visit%0Ahttp%3A//w-m.github.io/3dgs-compression-survey/%20for%20more%20information%20and%20a%0Asortable%20version%20of%20the%20table.%0A&entry.1838667208=http%3A//arxiv.org/abs/2407.09510v3&entry.124074799=Read"},
{"title": "Expansion-GRR: Efficient Generation of Smooth Global Redundancy\n  Resolution Roadmaps", "author": "Zhuoyun Zhong and Zhi Li and Constantinos Chamzas", "abstract": "  Global redundancy resolution (GRR) roadmaps is a novel concept in robotics\nthat facilitates the mapping from task space paths to configuration space paths\nin a legible, predictable, and repeatable way. Such roadmaps could find\nwidespread utility in applications such as safe teleoperation, consistent path\nplanning, and motion primitives generation. However, previous methods to\ncompute GRR roadmaps often necessitate a lengthy computation time and produce\nnon-smooth paths, limiting their practical efficacy. To address this challenge,\nwe introduce a novel method Expansion-GRR that leverages efficient\nconfiguration space projections and enables rapid generation of smooth roadmaps\nthat satisfy the task constraints. Additionally, we propose a simple multi-seed\nstrategy that further enhances the final quality. We conducted experiments in\nsimulation with a 5-link planar manipulator and a Kinova arm. We were able to\ngenerate the Expansion-GRR roadmaps up to 2 orders of magnitude faster while\nachieving higher smoothness. We also demonstrate the utility of the GRR\nroadmaps in teleoperation tasks where our method outperformed prior methods and\nreactive IK solvers in terms of success rate and solution quality.\n", "link": "http://arxiv.org/abs/2405.13770v2", "date": "2024-09-03", "relevancy": 2.2165, "topK": [{"title": "RoboEXP: Action-Conditioned Scene Graph via Interactive Exploration for\n  Robotic Manipulation", "link": "http://arxiv.org/abs/2402.15487v1", "similarity": 0.598}, {"title": "RTG-SLAM: Real-time 3D Reconstruction at Scale using Gaussian Splatting", "link": "http://arxiv.org/abs/2404.19706v1", "similarity": 0.5647}, {"title": "UGG: Unified Generative Grasping", "link": "https://arxiv.org/abs/2311.16917", "similarity": 0.5261}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Expansion-GRR%3A%20Efficient%20Generation%20of%20Smooth%20Global%20Redundancy%0A%20%20Resolution%20Roadmaps&body=Title%3A%20Expansion-GRR%3A%20Efficient%20Generation%20of%20Smooth%20Global%20Redundancy%0A%20%20Resolution%20Roadmaps%0AAuthor%3A%20Zhuoyun%20Zhong%20and%20Zhi%20Li%20and%20Constantinos%20Chamzas%0AAbstract%3A%20%20%20Global%20redundancy%20resolution%20%28GRR%29%20roadmaps%20is%20a%20novel%20concept%20in%20robotics%0Athat%20facilitates%20the%20mapping%20from%20task%20space%20paths%20to%20configuration%20space%20paths%0Ain%20a%20legible%2C%20predictable%2C%20and%20repeatable%20way.%20Such%20roadmaps%20could%20find%0Awidespread%20utility%20in%20applications%20such%20as%20safe%20teleoperation%2C%20consistent%20path%0Aplanning%2C%20and%20motion%20primitives%20generation.%20However%2C%20previous%20methods%20to%0Acompute%20GRR%20roadmaps%20often%20necessitate%20a%20lengthy%20computation%20time%20and%20produce%0Anon-smooth%20paths%2C%20limiting%20their%20practical%20efficacy.%20To%20address%20this%20challenge%2C%0Awe%20introduce%20a%20novel%20method%20Expansion-GRR%20that%20leverages%20efficient%0Aconfiguration%20space%20projections%20and%20enables%20rapid%20generation%20of%20smooth%20roadmaps%0Athat%20satisfy%20the%20task%20constraints.%20Additionally%2C%20we%20propose%20a%20simple%20multi-seed%0Astrategy%20that%20further%20enhances%20the%20final%20quality.%20We%20conducted%20experiments%20in%0Asimulation%20with%20a%205-link%20planar%20manipulator%20and%20a%20Kinova%20arm.%20We%20were%20able%20to%0Agenerate%20the%20Expansion-GRR%20roadmaps%20up%20to%202%20orders%20of%20magnitude%20faster%20while%0Aachieving%20higher%20smoothness.%20We%20also%20demonstrate%20the%20utility%20of%20the%20GRR%0Aroadmaps%20in%20teleoperation%20tasks%20where%20our%20method%20outperformed%20prior%20methods%20and%0Areactive%20IK%20solvers%20in%20terms%20of%20success%20rate%20and%20solution%20quality.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2405.13770v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DExpansion-GRR%253A%2520Efficient%2520Generation%2520of%2520Smooth%2520Global%2520Redundancy%250A%2520%2520Resolution%2520Roadmaps%26entry.906535625%3DZhuoyun%2520Zhong%2520and%2520Zhi%2520Li%2520and%2520Constantinos%2520Chamzas%26entry.1292438233%3D%2520%2520Global%2520redundancy%2520resolution%2520%2528GRR%2529%2520roadmaps%2520is%2520a%2520novel%2520concept%2520in%2520robotics%250Athat%2520facilitates%2520the%2520mapping%2520from%2520task%2520space%2520paths%2520to%2520configuration%2520space%2520paths%250Ain%2520a%2520legible%252C%2520predictable%252C%2520and%2520repeatable%2520way.%2520Such%2520roadmaps%2520could%2520find%250Awidespread%2520utility%2520in%2520applications%2520such%2520as%2520safe%2520teleoperation%252C%2520consistent%2520path%250Aplanning%252C%2520and%2520motion%2520primitives%2520generation.%2520However%252C%2520previous%2520methods%2520to%250Acompute%2520GRR%2520roadmaps%2520often%2520necessitate%2520a%2520lengthy%2520computation%2520time%2520and%2520produce%250Anon-smooth%2520paths%252C%2520limiting%2520their%2520practical%2520efficacy.%2520To%2520address%2520this%2520challenge%252C%250Awe%2520introduce%2520a%2520novel%2520method%2520Expansion-GRR%2520that%2520leverages%2520efficient%250Aconfiguration%2520space%2520projections%2520and%2520enables%2520rapid%2520generation%2520of%2520smooth%2520roadmaps%250Athat%2520satisfy%2520the%2520task%2520constraints.%2520Additionally%252C%2520we%2520propose%2520a%2520simple%2520multi-seed%250Astrategy%2520that%2520further%2520enhances%2520the%2520final%2520quality.%2520We%2520conducted%2520experiments%2520in%250Asimulation%2520with%2520a%25205-link%2520planar%2520manipulator%2520and%2520a%2520Kinova%2520arm.%2520We%2520were%2520able%2520to%250Agenerate%2520the%2520Expansion-GRR%2520roadmaps%2520up%2520to%25202%2520orders%2520of%2520magnitude%2520faster%2520while%250Aachieving%2520higher%2520smoothness.%2520We%2520also%2520demonstrate%2520the%2520utility%2520of%2520the%2520GRR%250Aroadmaps%2520in%2520teleoperation%2520tasks%2520where%2520our%2520method%2520outperformed%2520prior%2520methods%2520and%250Areactive%2520IK%2520solvers%2520in%2520terms%2520of%2520success%2520rate%2520and%2520solution%2520quality.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2405.13770v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Expansion-GRR%3A%20Efficient%20Generation%20of%20Smooth%20Global%20Redundancy%0A%20%20Resolution%20Roadmaps&entry.906535625=Zhuoyun%20Zhong%20and%20Zhi%20Li%20and%20Constantinos%20Chamzas&entry.1292438233=%20%20Global%20redundancy%20resolution%20%28GRR%29%20roadmaps%20is%20a%20novel%20concept%20in%20robotics%0Athat%20facilitates%20the%20mapping%20from%20task%20space%20paths%20to%20configuration%20space%20paths%0Ain%20a%20legible%2C%20predictable%2C%20and%20repeatable%20way.%20Such%20roadmaps%20could%20find%0Awidespread%20utility%20in%20applications%20such%20as%20safe%20teleoperation%2C%20consistent%20path%0Aplanning%2C%20and%20motion%20primitives%20generation.%20However%2C%20previous%20methods%20to%0Acompute%20GRR%20roadmaps%20often%20necessitate%20a%20lengthy%20computation%20time%20and%20produce%0Anon-smooth%20paths%2C%20limiting%20their%20practical%20efficacy.%20To%20address%20this%20challenge%2C%0Awe%20introduce%20a%20novel%20method%20Expansion-GRR%20that%20leverages%20efficient%0Aconfiguration%20space%20projections%20and%20enables%20rapid%20generation%20of%20smooth%20roadmaps%0Athat%20satisfy%20the%20task%20constraints.%20Additionally%2C%20we%20propose%20a%20simple%20multi-seed%0Astrategy%20that%20further%20enhances%20the%20final%20quality.%20We%20conducted%20experiments%20in%0Asimulation%20with%20a%205-link%20planar%20manipulator%20and%20a%20Kinova%20arm.%20We%20were%20able%20to%0Agenerate%20the%20Expansion-GRR%20roadmaps%20up%20to%202%20orders%20of%20magnitude%20faster%20while%0Aachieving%20higher%20smoothness.%20We%20also%20demonstrate%20the%20utility%20of%20the%20GRR%0Aroadmaps%20in%20teleoperation%20tasks%20where%20our%20method%20outperformed%20prior%20methods%20and%0Areactive%20IK%20solvers%20in%20terms%20of%20success%20rate%20and%20solution%20quality.%0A&entry.1838667208=http%3A//arxiv.org/abs/2405.13770v2&entry.124074799=Read"},
{"title": "SPIdepth: Strengthened Pose Information for Self-supervised Monocular\n  Depth Estimation", "author": "Mykola Lavreniuk", "abstract": "  Self-supervised monocular depth estimation has garnered considerable\nattention for its applications in autonomous driving and robotics. While recent\nmethods have made strides in leveraging techniques like the Self Query Layer\n(SQL) to infer depth from motion, they often overlook the potential of\nstrengthening pose information. In this paper, we introduce SPIdepth, a novel\napproach that prioritizes enhancing the pose network for improved depth\nestimation. Building upon the foundation laid by SQL, SPIdepth emphasizes the\nimportance of pose information in capturing fine-grained scene structures. By\nenhancing the pose network's capabilities, SPIdepth achieves remarkable\nadvancements in scene understanding and depth estimation. Experimental results\non benchmark datasets such as KITTI, Cityscapes, and Make3D showcase SPIdepth's\nstate-of-the-art performance, surpassing previous methods by significant\nmargins. Specifically, SPIdepth tops the self-supervised KITTI benchmark.\nAdditionally, SPIdepth achieves the lowest AbsRel (0.029), SqRel (0.069), and\nRMSE (1.394) on KITTI, establishing new state-of-the-art results. On\nCityscapes, SPIdepth shows improvements over SQLdepth of 21.7% in AbsRel, 36.8%\nin SqRel, and 16.5% in RMSE, even without using motion masks. On Make3D,\nSPIdepth in zero-shot outperforms all other models. Remarkably, SPIdepth\nachieves these results using only a single image for inference, surpassing even\nmethods that utilize video sequences for inference, thus demonstrating its\nefficacy and efficiency in real-world applications. Our approach represents a\nsignificant leap forward in self-supervised monocular depth estimation,\nunderscoring the importance of strengthening pose information for advancing\nscene understanding in real-world applications. The code and pre-trained models\nare publicly available at https://github.com/Lavreniuk/SPIdepth.\n", "link": "http://arxiv.org/abs/2404.12501v3", "date": "2024-09-03", "relevancy": 2.2076, "topK": [{"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.5797}, {"title": "DDM-NET: End-to-end learning of keypoint feature Detection, Description and Matching for 3D localization", "link": "https://arxiv.org/abs/2212.04575", "similarity": 0.5464}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5463}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20SPIdepth%3A%20Strengthened%20Pose%20Information%20for%20Self-supervised%20Monocular%0A%20%20Depth%20Estimation&body=Title%3A%20SPIdepth%3A%20Strengthened%20Pose%20Information%20for%20Self-supervised%20Monocular%0A%20%20Depth%20Estimation%0AAuthor%3A%20Mykola%20Lavreniuk%0AAbstract%3A%20%20%20Self-supervised%20monocular%20depth%20estimation%20has%20garnered%20considerable%0Aattention%20for%20its%20applications%20in%20autonomous%20driving%20and%20robotics.%20While%20recent%0Amethods%20have%20made%20strides%20in%20leveraging%20techniques%20like%20the%20Self%20Query%20Layer%0A%28SQL%29%20to%20infer%20depth%20from%20motion%2C%20they%20often%20overlook%20the%20potential%20of%0Astrengthening%20pose%20information.%20In%20this%20paper%2C%20we%20introduce%20SPIdepth%2C%20a%20novel%0Aapproach%20that%20prioritizes%20enhancing%20the%20pose%20network%20for%20improved%20depth%0Aestimation.%20Building%20upon%20the%20foundation%20laid%20by%20SQL%2C%20SPIdepth%20emphasizes%20the%0Aimportance%20of%20pose%20information%20in%20capturing%20fine-grained%20scene%20structures.%20By%0Aenhancing%20the%20pose%20network%27s%20capabilities%2C%20SPIdepth%20achieves%20remarkable%0Aadvancements%20in%20scene%20understanding%20and%20depth%20estimation.%20Experimental%20results%0Aon%20benchmark%20datasets%20such%20as%20KITTI%2C%20Cityscapes%2C%20and%20Make3D%20showcase%20SPIdepth%27s%0Astate-of-the-art%20performance%2C%20surpassing%20previous%20methods%20by%20significant%0Amargins.%20Specifically%2C%20SPIdepth%20tops%20the%20self-supervised%20KITTI%20benchmark.%0AAdditionally%2C%20SPIdepth%20achieves%20the%20lowest%20AbsRel%20%280.029%29%2C%20SqRel%20%280.069%29%2C%20and%0ARMSE%20%281.394%29%20on%20KITTI%2C%20establishing%20new%20state-of-the-art%20results.%20On%0ACityscapes%2C%20SPIdepth%20shows%20improvements%20over%20SQLdepth%20of%2021.7%25%20in%20AbsRel%2C%2036.8%25%0Ain%20SqRel%2C%20and%2016.5%25%20in%20RMSE%2C%20even%20without%20using%20motion%20masks.%20On%20Make3D%2C%0ASPIdepth%20in%20zero-shot%20outperforms%20all%20other%20models.%20Remarkably%2C%20SPIdepth%0Aachieves%20these%20results%20using%20only%20a%20single%20image%20for%20inference%2C%20surpassing%20even%0Amethods%20that%20utilize%20video%20sequences%20for%20inference%2C%20thus%20demonstrating%20its%0Aefficacy%20and%20efficiency%20in%20real-world%20applications.%20Our%20approach%20represents%20a%0Asignificant%20leap%20forward%20in%20self-supervised%20monocular%20depth%20estimation%2C%0Aunderscoring%20the%20importance%20of%20strengthening%20pose%20information%20for%20advancing%0Ascene%20understanding%20in%20real-world%20applications.%20The%20code%20and%20pre-trained%20models%0Aare%20publicly%20available%20at%20https%3A//github.com/Lavreniuk/SPIdepth.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2404.12501v3%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DSPIdepth%253A%2520Strengthened%2520Pose%2520Information%2520for%2520Self-supervised%2520Monocular%250A%2520%2520Depth%2520Estimation%26entry.906535625%3DMykola%2520Lavreniuk%26entry.1292438233%3D%2520%2520Self-supervised%2520monocular%2520depth%2520estimation%2520has%2520garnered%2520considerable%250Aattention%2520for%2520its%2520applications%2520in%2520autonomous%2520driving%2520and%2520robotics.%2520While%2520recent%250Amethods%2520have%2520made%2520strides%2520in%2520leveraging%2520techniques%2520like%2520the%2520Self%2520Query%2520Layer%250A%2528SQL%2529%2520to%2520infer%2520depth%2520from%2520motion%252C%2520they%2520often%2520overlook%2520the%2520potential%2520of%250Astrengthening%2520pose%2520information.%2520In%2520this%2520paper%252C%2520we%2520introduce%2520SPIdepth%252C%2520a%2520novel%250Aapproach%2520that%2520prioritizes%2520enhancing%2520the%2520pose%2520network%2520for%2520improved%2520depth%250Aestimation.%2520Building%2520upon%2520the%2520foundation%2520laid%2520by%2520SQL%252C%2520SPIdepth%2520emphasizes%2520the%250Aimportance%2520of%2520pose%2520information%2520in%2520capturing%2520fine-grained%2520scene%2520structures.%2520By%250Aenhancing%2520the%2520pose%2520network%2527s%2520capabilities%252C%2520SPIdepth%2520achieves%2520remarkable%250Aadvancements%2520in%2520scene%2520understanding%2520and%2520depth%2520estimation.%2520Experimental%2520results%250Aon%2520benchmark%2520datasets%2520such%2520as%2520KITTI%252C%2520Cityscapes%252C%2520and%2520Make3D%2520showcase%2520SPIdepth%2527s%250Astate-of-the-art%2520performance%252C%2520surpassing%2520previous%2520methods%2520by%2520significant%250Amargins.%2520Specifically%252C%2520SPIdepth%2520tops%2520the%2520self-supervised%2520KITTI%2520benchmark.%250AAdditionally%252C%2520SPIdepth%2520achieves%2520the%2520lowest%2520AbsRel%2520%25280.029%2529%252C%2520SqRel%2520%25280.069%2529%252C%2520and%250ARMSE%2520%25281.394%2529%2520on%2520KITTI%252C%2520establishing%2520new%2520state-of-the-art%2520results.%2520On%250ACityscapes%252C%2520SPIdepth%2520shows%2520improvements%2520over%2520SQLdepth%2520of%252021.7%2525%2520in%2520AbsRel%252C%252036.8%2525%250Ain%2520SqRel%252C%2520and%252016.5%2525%2520in%2520RMSE%252C%2520even%2520without%2520using%2520motion%2520masks.%2520On%2520Make3D%252C%250ASPIdepth%2520in%2520zero-shot%2520outperforms%2520all%2520other%2520models.%2520Remarkably%252C%2520SPIdepth%250Aachieves%2520these%2520results%2520using%2520only%2520a%2520single%2520image%2520for%2520inference%252C%2520surpassing%2520even%250Amethods%2520that%2520utilize%2520video%2520sequences%2520for%2520inference%252C%2520thus%2520demonstrating%2520its%250Aefficacy%2520and%2520efficiency%2520in%2520real-world%2520applications.%2520Our%2520approach%2520represents%2520a%250Asignificant%2520leap%2520forward%2520in%2520self-supervised%2520monocular%2520depth%2520estimation%252C%250Aunderscoring%2520the%2520importance%2520of%2520strengthening%2520pose%2520information%2520for%2520advancing%250Ascene%2520understanding%2520in%2520real-world%2520applications.%2520The%2520code%2520and%2520pre-trained%2520models%250Aare%2520publicly%2520available%2520at%2520https%253A//github.com/Lavreniuk/SPIdepth.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2404.12501v3%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=SPIdepth%3A%20Strengthened%20Pose%20Information%20for%20Self-supervised%20Monocular%0A%20%20Depth%20Estimation&entry.906535625=Mykola%20Lavreniuk&entry.1292438233=%20%20Self-supervised%20monocular%20depth%20estimation%20has%20garnered%20considerable%0Aattention%20for%20its%20applications%20in%20autonomous%20driving%20and%20robotics.%20While%20recent%0Amethods%20have%20made%20strides%20in%20leveraging%20techniques%20like%20the%20Self%20Query%20Layer%0A%28SQL%29%20to%20infer%20depth%20from%20motion%2C%20they%20often%20overlook%20the%20potential%20of%0Astrengthening%20pose%20information.%20In%20this%20paper%2C%20we%20introduce%20SPIdepth%2C%20a%20novel%0Aapproach%20that%20prioritizes%20enhancing%20the%20pose%20network%20for%20improved%20depth%0Aestimation.%20Building%20upon%20the%20foundation%20laid%20by%20SQL%2C%20SPIdepth%20emphasizes%20the%0Aimportance%20of%20pose%20information%20in%20capturing%20fine-grained%20scene%20structures.%20By%0Aenhancing%20the%20pose%20network%27s%20capabilities%2C%20SPIdepth%20achieves%20remarkable%0Aadvancements%20in%20scene%20understanding%20and%20depth%20estimation.%20Experimental%20results%0Aon%20benchmark%20datasets%20such%20as%20KITTI%2C%20Cityscapes%2C%20and%20Make3D%20showcase%20SPIdepth%27s%0Astate-of-the-art%20performance%2C%20surpassing%20previous%20methods%20by%20significant%0Amargins.%20Specifically%2C%20SPIdepth%20tops%20the%20self-supervised%20KITTI%20benchmark.%0AAdditionally%2C%20SPIdepth%20achieves%20the%20lowest%20AbsRel%20%280.029%29%2C%20SqRel%20%280.069%29%2C%20and%0ARMSE%20%281.394%29%20on%20KITTI%2C%20establishing%20new%20state-of-the-art%20results.%20On%0ACityscapes%2C%20SPIdepth%20shows%20improvements%20over%20SQLdepth%20of%2021.7%25%20in%20AbsRel%2C%2036.8%25%0Ain%20SqRel%2C%20and%2016.5%25%20in%20RMSE%2C%20even%20without%20using%20motion%20masks.%20On%20Make3D%2C%0ASPIdepth%20in%20zero-shot%20outperforms%20all%20other%20models.%20Remarkably%2C%20SPIdepth%0Aachieves%20these%20results%20using%20only%20a%20single%20image%20for%20inference%2C%20surpassing%20even%0Amethods%20that%20utilize%20video%20sequences%20for%20inference%2C%20thus%20demonstrating%20its%0Aefficacy%20and%20efficiency%20in%20real-world%20applications.%20Our%20approach%20represents%20a%0Asignificant%20leap%20forward%20in%20self-supervised%20monocular%20depth%20estimation%2C%0Aunderscoring%20the%20importance%20of%20strengthening%20pose%20information%20for%20advancing%0Ascene%20understanding%20in%20real-world%20applications.%20The%20code%20and%20pre-trained%20models%0Aare%20publicly%20available%20at%20https%3A//github.com/Lavreniuk/SPIdepth.%0A&entry.1838667208=http%3A//arxiv.org/abs/2404.12501v3&entry.124074799=Read"},
{"title": "Open-vocabulary Temporal Action Localization using VLMs", "author": "Naoki Wake and Atsushi Kanehira and Kazuhiro Sasabuchi and Jun Takamatsu and Katsushi Ikeuchi", "abstract": "  Video action localization aims to find timings of a specific action from a\nlong video. Although existing learning-based approaches have been successful,\nthose require annotating videos that come with a considerable labor cost. This\npaper proposes a learning-free, open-vocabulary approach based on emerging\noff-the-shelf vision-language models (VLM). The challenge stems from the fact\nthat VLMs are neither designed to process long videos nor tailored for finding\nactions. We overcome these problems by extending an iterative visual prompting\ntechnique. Specifically, we sample video frames into a concatenated image with\nframe index labels, making a VLM guess a frame that is considered to be closest\nto the start/end of the action. Iterating this process by narrowing a sampling\ntime window results in finding a specific frame of start and end of an action.\nWe demonstrate that this sampling technique yields reasonable results,\nillustrating a practical extension of VLMs for understanding videos. A sample\ncode is available at\nhttps://microsoft.github.io/VLM-Video-Action-Localization/.\n", "link": "http://arxiv.org/abs/2408.17422v2", "date": "2024-09-03", "relevancy": 2.1996, "topK": [{"title": "Customize-A-Video: One-Shot Motion Customization of Text-to-Video\n  Diffusion Models", "link": "http://arxiv.org/abs/2402.14780v1", "similarity": 0.5669}, {"title": "Weakly-guided self-supervised pretraining for temporal activity detection", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/25189", "similarity": 0.5631}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5299}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Open-vocabulary%20Temporal%20Action%20Localization%20using%20VLMs&body=Title%3A%20Open-vocabulary%20Temporal%20Action%20Localization%20using%20VLMs%0AAuthor%3A%20Naoki%20Wake%20and%20Atsushi%20Kanehira%20and%20Kazuhiro%20Sasabuchi%20and%20Jun%20Takamatsu%20and%20Katsushi%20Ikeuchi%0AAbstract%3A%20%20%20Video%20action%20localization%20aims%20to%20find%20timings%20of%20a%20specific%20action%20from%20a%0Along%20video.%20Although%20existing%20learning-based%20approaches%20have%20been%20successful%2C%0Athose%20require%20annotating%20videos%20that%20come%20with%20a%20considerable%20labor%20cost.%20This%0Apaper%20proposes%20a%20learning-free%2C%20open-vocabulary%20approach%20based%20on%20emerging%0Aoff-the-shelf%20vision-language%20models%20%28VLM%29.%20The%20challenge%20stems%20from%20the%20fact%0Athat%20VLMs%20are%20neither%20designed%20to%20process%20long%20videos%20nor%20tailored%20for%20finding%0Aactions.%20We%20overcome%20these%20problems%20by%20extending%20an%20iterative%20visual%20prompting%0Atechnique.%20Specifically%2C%20we%20sample%20video%20frames%20into%20a%20concatenated%20image%20with%0Aframe%20index%20labels%2C%20making%20a%20VLM%20guess%20a%20frame%20that%20is%20considered%20to%20be%20closest%0Ato%20the%20start/end%20of%20the%20action.%20Iterating%20this%20process%20by%20narrowing%20a%20sampling%0Atime%20window%20results%20in%20finding%20a%20specific%20frame%20of%20start%20and%20end%20of%20an%20action.%0AWe%20demonstrate%20that%20this%20sampling%20technique%20yields%20reasonable%20results%2C%0Aillustrating%20a%20practical%20extension%20of%20VLMs%20for%20understanding%20videos.%20A%20sample%0Acode%20is%20available%20at%0Ahttps%3A//microsoft.github.io/VLM-Video-Action-Localization/.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2408.17422v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DOpen-vocabulary%2520Temporal%2520Action%2520Localization%2520using%2520VLMs%26entry.906535625%3DNaoki%2520Wake%2520and%2520Atsushi%2520Kanehira%2520and%2520Kazuhiro%2520Sasabuchi%2520and%2520Jun%2520Takamatsu%2520and%2520Katsushi%2520Ikeuchi%26entry.1292438233%3D%2520%2520Video%2520action%2520localization%2520aims%2520to%2520find%2520timings%2520of%2520a%2520specific%2520action%2520from%2520a%250Along%2520video.%2520Although%2520existing%2520learning-based%2520approaches%2520have%2520been%2520successful%252C%250Athose%2520require%2520annotating%2520videos%2520that%2520come%2520with%2520a%2520considerable%2520labor%2520cost.%2520This%250Apaper%2520proposes%2520a%2520learning-free%252C%2520open-vocabulary%2520approach%2520based%2520on%2520emerging%250Aoff-the-shelf%2520vision-language%2520models%2520%2528VLM%2529.%2520The%2520challenge%2520stems%2520from%2520the%2520fact%250Athat%2520VLMs%2520are%2520neither%2520designed%2520to%2520process%2520long%2520videos%2520nor%2520tailored%2520for%2520finding%250Aactions.%2520We%2520overcome%2520these%2520problems%2520by%2520extending%2520an%2520iterative%2520visual%2520prompting%250Atechnique.%2520Specifically%252C%2520we%2520sample%2520video%2520frames%2520into%2520a%2520concatenated%2520image%2520with%250Aframe%2520index%2520labels%252C%2520making%2520a%2520VLM%2520guess%2520a%2520frame%2520that%2520is%2520considered%2520to%2520be%2520closest%250Ato%2520the%2520start/end%2520of%2520the%2520action.%2520Iterating%2520this%2520process%2520by%2520narrowing%2520a%2520sampling%250Atime%2520window%2520results%2520in%2520finding%2520a%2520specific%2520frame%2520of%2520start%2520and%2520end%2520of%2520an%2520action.%250AWe%2520demonstrate%2520that%2520this%2520sampling%2520technique%2520yields%2520reasonable%2520results%252C%250Aillustrating%2520a%2520practical%2520extension%2520of%2520VLMs%2520for%2520understanding%2520videos.%2520A%2520sample%250Acode%2520is%2520available%2520at%250Ahttps%253A//microsoft.github.io/VLM-Video-Action-Localization/.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2408.17422v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Open-vocabulary%20Temporal%20Action%20Localization%20using%20VLMs&entry.906535625=Naoki%20Wake%20and%20Atsushi%20Kanehira%20and%20Kazuhiro%20Sasabuchi%20and%20Jun%20Takamatsu%20and%20Katsushi%20Ikeuchi&entry.1292438233=%20%20Video%20action%20localization%20aims%20to%20find%20timings%20of%20a%20specific%20action%20from%20a%0Along%20video.%20Although%20existing%20learning-based%20approaches%20have%20been%20successful%2C%0Athose%20require%20annotating%20videos%20that%20come%20with%20a%20considerable%20labor%20cost.%20This%0Apaper%20proposes%20a%20learning-free%2C%20open-vocabulary%20approach%20based%20on%20emerging%0Aoff-the-shelf%20vision-language%20models%20%28VLM%29.%20The%20challenge%20stems%20from%20the%20fact%0Athat%20VLMs%20are%20neither%20designed%20to%20process%20long%20videos%20nor%20tailored%20for%20finding%0Aactions.%20We%20overcome%20these%20problems%20by%20extending%20an%20iterative%20visual%20prompting%0Atechnique.%20Specifically%2C%20we%20sample%20video%20frames%20into%20a%20concatenated%20image%20with%0Aframe%20index%20labels%2C%20making%20a%20VLM%20guess%20a%20frame%20that%20is%20considered%20to%20be%20closest%0Ato%20the%20start/end%20of%20the%20action.%20Iterating%20this%20process%20by%20narrowing%20a%20sampling%0Atime%20window%20results%20in%20finding%20a%20specific%20frame%20of%20start%20and%20end%20of%20an%20action.%0AWe%20demonstrate%20that%20this%20sampling%20technique%20yields%20reasonable%20results%2C%0Aillustrating%20a%20practical%20extension%20of%20VLMs%20for%20understanding%20videos.%20A%20sample%0Acode%20is%20available%20at%0Ahttps%3A//microsoft.github.io/VLM-Video-Action-Localization/.%0A&entry.1838667208=http%3A//arxiv.org/abs/2408.17422v2&entry.124074799=Read"},
{"title": "SUMix: Mixup with Semantic and Uncertain Information", "author": "Huafeng Qin and Xin Jin and Hongyu Zhu and Hongchao Liao and Moun\u00eem A. El-Yacoubi and Xinbo Gao", "abstract": "  Mixup data augmentation approaches have been applied for various tasks of\ndeep learning to improve the generalization ability of deep neural networks.\nSome existing approaches CutMix, SaliencyMix, etc. randomly replace a patch in\none image with patches from another to generate the mixed image. Similarly, the\ncorresponding labels are linearly combined by a fixed ratio $\\lambda$ by l. The\nobjects in two images may be overlapped during the mixing process, so some\nsemantic information is corrupted in the mixed samples. In this case, the mixed\nimage does not match the mixed label information. Besides, such a label may\nmislead the deep learning model training, which results in poor performance. To\nsolve this problem, we proposed a novel approach named SUMix to learn the\nmixing ratio as well as the uncertainty for the mixed samples during the\ntraining process. First, we design a learnable similarity function to compute\nan accurate mix ratio. Second, an approach is investigated as a regularized\nterm to model the uncertainty of the mixed samples. We conduct experiments on\nfive image benchmarks, and extensive experimental results imply that our method\nis capable of improving the performance of classifiers with different\ncutting-based mixup approaches. The source code is available at\nhttps://github.com/JinXins/SUMix.\n", "link": "http://arxiv.org/abs/2407.07805v3", "date": "2024-09-03", "relevancy": 2.1499, "topK": [{"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.5767}, {"title": "Flexible Visual Recognition by Evidential Modeling of Confusion and Ignorance", "link": "http://openaccess.thecvf.com/content/ICCV2023/html/Fan_Flexible_Visual_Recognition_by_Evidential_Modeling_of_Confusion_and_Ignorance_ICCV_2023_paper.html", "similarity": 0.5444}, {"title": "Breadcrumbs: Adversarial class-balanced sampling for long-tailed recognition", "link": "https://link.springer.com/chapter/10.1007/978-3-031-20053-3_37", "similarity": 0.5149}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20SUMix%3A%20Mixup%20with%20Semantic%20and%20Uncertain%20Information&body=Title%3A%20SUMix%3A%20Mixup%20with%20Semantic%20and%20Uncertain%20Information%0AAuthor%3A%20Huafeng%20Qin%20and%20Xin%20Jin%20and%20Hongyu%20Zhu%20and%20Hongchao%20Liao%20and%20Moun%C3%AEm%20A.%20El-Yacoubi%20and%20Xinbo%20Gao%0AAbstract%3A%20%20%20Mixup%20data%20augmentation%20approaches%20have%20been%20applied%20for%20various%20tasks%20of%0Adeep%20learning%20to%20improve%20the%20generalization%20ability%20of%20deep%20neural%20networks.%0ASome%20existing%20approaches%20CutMix%2C%20SaliencyMix%2C%20etc.%20randomly%20replace%20a%20patch%20in%0Aone%20image%20with%20patches%20from%20another%20to%20generate%20the%20mixed%20image.%20Similarly%2C%20the%0Acorresponding%20labels%20are%20linearly%20combined%20by%20a%20fixed%20ratio%20%24%5Clambda%24%20by%20l.%20The%0Aobjects%20in%20two%20images%20may%20be%20overlapped%20during%20the%20mixing%20process%2C%20so%20some%0Asemantic%20information%20is%20corrupted%20in%20the%20mixed%20samples.%20In%20this%20case%2C%20the%20mixed%0Aimage%20does%20not%20match%20the%20mixed%20label%20information.%20Besides%2C%20such%20a%20label%20may%0Amislead%20the%20deep%20learning%20model%20training%2C%20which%20results%20in%20poor%20performance.%20To%0Asolve%20this%20problem%2C%20we%20proposed%20a%20novel%20approach%20named%20SUMix%20to%20learn%20the%0Amixing%20ratio%20as%20well%20as%20the%20uncertainty%20for%20the%20mixed%20samples%20during%20the%0Atraining%20process.%20First%2C%20we%20design%20a%20learnable%20similarity%20function%20to%20compute%0Aan%20accurate%20mix%20ratio.%20Second%2C%20an%20approach%20is%20investigated%20as%20a%20regularized%0Aterm%20to%20model%20the%20uncertainty%20of%20the%20mixed%20samples.%20We%20conduct%20experiments%20on%0Afive%20image%20benchmarks%2C%20and%20extensive%20experimental%20results%20imply%20that%20our%20method%0Ais%20capable%20of%20improving%20the%20performance%20of%20classifiers%20with%20different%0Acutting-based%20mixup%20approaches.%20The%20source%20code%20is%20available%20at%0Ahttps%3A//github.com/JinXins/SUMix.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2407.07805v3%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DSUMix%253A%2520Mixup%2520with%2520Semantic%2520and%2520Uncertain%2520Information%26entry.906535625%3DHuafeng%2520Qin%2520and%2520Xin%2520Jin%2520and%2520Hongyu%2520Zhu%2520and%2520Hongchao%2520Liao%2520and%2520Moun%25C3%25AEm%2520A.%2520El-Yacoubi%2520and%2520Xinbo%2520Gao%26entry.1292438233%3D%2520%2520Mixup%2520data%2520augmentation%2520approaches%2520have%2520been%2520applied%2520for%2520various%2520tasks%2520of%250Adeep%2520learning%2520to%2520improve%2520the%2520generalization%2520ability%2520of%2520deep%2520neural%2520networks.%250ASome%2520existing%2520approaches%2520CutMix%252C%2520SaliencyMix%252C%2520etc.%2520randomly%2520replace%2520a%2520patch%2520in%250Aone%2520image%2520with%2520patches%2520from%2520another%2520to%2520generate%2520the%2520mixed%2520image.%2520Similarly%252C%2520the%250Acorresponding%2520labels%2520are%2520linearly%2520combined%2520by%2520a%2520fixed%2520ratio%2520%2524%255Clambda%2524%2520by%2520l.%2520The%250Aobjects%2520in%2520two%2520images%2520may%2520be%2520overlapped%2520during%2520the%2520mixing%2520process%252C%2520so%2520some%250Asemantic%2520information%2520is%2520corrupted%2520in%2520the%2520mixed%2520samples.%2520In%2520this%2520case%252C%2520the%2520mixed%250Aimage%2520does%2520not%2520match%2520the%2520mixed%2520label%2520information.%2520Besides%252C%2520such%2520a%2520label%2520may%250Amislead%2520the%2520deep%2520learning%2520model%2520training%252C%2520which%2520results%2520in%2520poor%2520performance.%2520To%250Asolve%2520this%2520problem%252C%2520we%2520proposed%2520a%2520novel%2520approach%2520named%2520SUMix%2520to%2520learn%2520the%250Amixing%2520ratio%2520as%2520well%2520as%2520the%2520uncertainty%2520for%2520the%2520mixed%2520samples%2520during%2520the%250Atraining%2520process.%2520First%252C%2520we%2520design%2520a%2520learnable%2520similarity%2520function%2520to%2520compute%250Aan%2520accurate%2520mix%2520ratio.%2520Second%252C%2520an%2520approach%2520is%2520investigated%2520as%2520a%2520regularized%250Aterm%2520to%2520model%2520the%2520uncertainty%2520of%2520the%2520mixed%2520samples.%2520We%2520conduct%2520experiments%2520on%250Afive%2520image%2520benchmarks%252C%2520and%2520extensive%2520experimental%2520results%2520imply%2520that%2520our%2520method%250Ais%2520capable%2520of%2520improving%2520the%2520performance%2520of%2520classifiers%2520with%2520different%250Acutting-based%2520mixup%2520approaches.%2520The%2520source%2520code%2520is%2520available%2520at%250Ahttps%253A//github.com/JinXins/SUMix.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2407.07805v3%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=SUMix%3A%20Mixup%20with%20Semantic%20and%20Uncertain%20Information&entry.906535625=Huafeng%20Qin%20and%20Xin%20Jin%20and%20Hongyu%20Zhu%20and%20Hongchao%20Liao%20and%20Moun%C3%AEm%20A.%20El-Yacoubi%20and%20Xinbo%20Gao&entry.1292438233=%20%20Mixup%20data%20augmentation%20approaches%20have%20been%20applied%20for%20various%20tasks%20of%0Adeep%20learning%20to%20improve%20the%20generalization%20ability%20of%20deep%20neural%20networks.%0ASome%20existing%20approaches%20CutMix%2C%20SaliencyMix%2C%20etc.%20randomly%20replace%20a%20patch%20in%0Aone%20image%20with%20patches%20from%20another%20to%20generate%20the%20mixed%20image.%20Similarly%2C%20the%0Acorresponding%20labels%20are%20linearly%20combined%20by%20a%20fixed%20ratio%20%24%5Clambda%24%20by%20l.%20The%0Aobjects%20in%20two%20images%20may%20be%20overlapped%20during%20the%20mixing%20process%2C%20so%20some%0Asemantic%20information%20is%20corrupted%20in%20the%20mixed%20samples.%20In%20this%20case%2C%20the%20mixed%0Aimage%20does%20not%20match%20the%20mixed%20label%20information.%20Besides%2C%20such%20a%20label%20may%0Amislead%20the%20deep%20learning%20model%20training%2C%20which%20results%20in%20poor%20performance.%20To%0Asolve%20this%20problem%2C%20we%20proposed%20a%20novel%20approach%20named%20SUMix%20to%20learn%20the%0Amixing%20ratio%20as%20well%20as%20the%20uncertainty%20for%20the%20mixed%20samples%20during%20the%0Atraining%20process.%20First%2C%20we%20design%20a%20learnable%20similarity%20function%20to%20compute%0Aan%20accurate%20mix%20ratio.%20Second%2C%20an%20approach%20is%20investigated%20as%20a%20regularized%0Aterm%20to%20model%20the%20uncertainty%20of%20the%20mixed%20samples.%20We%20conduct%20experiments%20on%0Afive%20image%20benchmarks%2C%20and%20extensive%20experimental%20results%20imply%20that%20our%20method%0Ais%20capable%20of%20improving%20the%20performance%20of%20classifiers%20with%20different%0Acutting-based%20mixup%20approaches.%20The%20source%20code%20is%20available%20at%0Ahttps%3A//github.com/JinXins/SUMix.%0A&entry.1838667208=http%3A//arxiv.org/abs/2407.07805v3&entry.124074799=Read"},
{"title": "A Multiscale Gradient Fusion Method for Edge Detection in Color Images\n  Utilizing the CBM3D Filter", "author": "Zhuoyue Wang and Yiyi Tao and Danqing Ma and Jiajing Chen", "abstract": "  In this paper, a color edge detection strategy based on collaborative\nfiltering combined with multiscale gradient fusion is proposed. The\nblock-matching and 3D (BM3D) filter are used to enhance the sparse\nrepresentation in the transform domain and achieve the effect of denoising,\nwhereas the multiscale gradient fusion makes up for the defect of loss of\ndetails in single-scale edge detection and improves the edge detection\nresolution and quality. First, the RGB images in the dataset are converted to\nXYZ color space images through mathematical operations. Second, the colored\nblock-matching and 3D (CBM3D) filter are used on the sparse images and to\nremove noise interference. Then, the vector gradients of the color image and\nthe anisotropic Gaussian directional derivative of the two scale parameters are\ncalculated and averaged pixel-by-pixel to obtain a new edge strength map.\nFinally, the edge features are enhanced by image normalization and non-maximum\nsuppression technology, and on that basis, the edge contour is obtained by\ndouble threshold selection and a new morphological refinement method. Through\nan experimental analysis of the edge detection dataset, the method proposed has\ngood noise robustness and high edge quality, which is better than the Color\nSobel, Color Canny, SE and Color AGDD as shown by the PR curve, AUC, PSNR, MSE,\nand FOM indicators.\n", "link": "http://arxiv.org/abs/2408.14013v2", "date": "2024-09-03", "relevancy": 2.0694, "topK": [{"title": "DDM-NET: End-to-end learning of keypoint feature Detection, Description and Matching for 3D localization", "link": "https://arxiv.org/abs/2212.04575", "similarity": 0.5411}, {"title": "Multimodal-Conditioned Latent Diffusion Models for Fashion Image Editing", "link": "http://arxiv.org/abs/2403.14828v2", "similarity": 0.5171}, {"title": "RTG-SLAM: Real-time 3D Reconstruction at Scale using Gaussian Splatting", "link": "http://arxiv.org/abs/2404.19706v1", "similarity": 0.4937}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20A%20Multiscale%20Gradient%20Fusion%20Method%20for%20Edge%20Detection%20in%20Color%20Images%0A%20%20Utilizing%20the%20CBM3D%20Filter&body=Title%3A%20A%20Multiscale%20Gradient%20Fusion%20Method%20for%20Edge%20Detection%20in%20Color%20Images%0A%20%20Utilizing%20the%20CBM3D%20Filter%0AAuthor%3A%20Zhuoyue%20Wang%20and%20Yiyi%20Tao%20and%20Danqing%20Ma%20and%20Jiajing%20Chen%0AAbstract%3A%20%20%20In%20this%20paper%2C%20a%20color%20edge%20detection%20strategy%20based%20on%20collaborative%0Afiltering%20combined%20with%20multiscale%20gradient%20fusion%20is%20proposed.%20The%0Ablock-matching%20and%203D%20%28BM3D%29%20filter%20are%20used%20to%20enhance%20the%20sparse%0Arepresentation%20in%20the%20transform%20domain%20and%20achieve%20the%20effect%20of%20denoising%2C%0Awhereas%20the%20multiscale%20gradient%20fusion%20makes%20up%20for%20the%20defect%20of%20loss%20of%0Adetails%20in%20single-scale%20edge%20detection%20and%20improves%20the%20edge%20detection%0Aresolution%20and%20quality.%20First%2C%20the%20RGB%20images%20in%20the%20dataset%20are%20converted%20to%0AXYZ%20color%20space%20images%20through%20mathematical%20operations.%20Second%2C%20the%20colored%0Ablock-matching%20and%203D%20%28CBM3D%29%20filter%20are%20used%20on%20the%20sparse%20images%20and%20to%0Aremove%20noise%20interference.%20Then%2C%20the%20vector%20gradients%20of%20the%20color%20image%20and%0Athe%20anisotropic%20Gaussian%20directional%20derivative%20of%20the%20two%20scale%20parameters%20are%0Acalculated%20and%20averaged%20pixel-by-pixel%20to%20obtain%20a%20new%20edge%20strength%20map.%0AFinally%2C%20the%20edge%20features%20are%20enhanced%20by%20image%20normalization%20and%20non-maximum%0Asuppression%20technology%2C%20and%20on%20that%20basis%2C%20the%20edge%20contour%20is%20obtained%20by%0Adouble%20threshold%20selection%20and%20a%20new%20morphological%20refinement%20method.%20Through%0Aan%20experimental%20analysis%20of%20the%20edge%20detection%20dataset%2C%20the%20method%20proposed%20has%0Agood%20noise%20robustness%20and%20high%20edge%20quality%2C%20which%20is%20better%20than%20the%20Color%0ASobel%2C%20Color%20Canny%2C%20SE%20and%20Color%20AGDD%20as%20shown%20by%20the%20PR%20curve%2C%20AUC%2C%20PSNR%2C%20MSE%2C%0Aand%20FOM%20indicators.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2408.14013v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DA%2520Multiscale%2520Gradient%2520Fusion%2520Method%2520for%2520Edge%2520Detection%2520in%2520Color%2520Images%250A%2520%2520Utilizing%2520the%2520CBM3D%2520Filter%26entry.906535625%3DZhuoyue%2520Wang%2520and%2520Yiyi%2520Tao%2520and%2520Danqing%2520Ma%2520and%2520Jiajing%2520Chen%26entry.1292438233%3D%2520%2520In%2520this%2520paper%252C%2520a%2520color%2520edge%2520detection%2520strategy%2520based%2520on%2520collaborative%250Afiltering%2520combined%2520with%2520multiscale%2520gradient%2520fusion%2520is%2520proposed.%2520The%250Ablock-matching%2520and%25203D%2520%2528BM3D%2529%2520filter%2520are%2520used%2520to%2520enhance%2520the%2520sparse%250Arepresentation%2520in%2520the%2520transform%2520domain%2520and%2520achieve%2520the%2520effect%2520of%2520denoising%252C%250Awhereas%2520the%2520multiscale%2520gradient%2520fusion%2520makes%2520up%2520for%2520the%2520defect%2520of%2520loss%2520of%250Adetails%2520in%2520single-scale%2520edge%2520detection%2520and%2520improves%2520the%2520edge%2520detection%250Aresolution%2520and%2520quality.%2520First%252C%2520the%2520RGB%2520images%2520in%2520the%2520dataset%2520are%2520converted%2520to%250AXYZ%2520color%2520space%2520images%2520through%2520mathematical%2520operations.%2520Second%252C%2520the%2520colored%250Ablock-matching%2520and%25203D%2520%2528CBM3D%2529%2520filter%2520are%2520used%2520on%2520the%2520sparse%2520images%2520and%2520to%250Aremove%2520noise%2520interference.%2520Then%252C%2520the%2520vector%2520gradients%2520of%2520the%2520color%2520image%2520and%250Athe%2520anisotropic%2520Gaussian%2520directional%2520derivative%2520of%2520the%2520two%2520scale%2520parameters%2520are%250Acalculated%2520and%2520averaged%2520pixel-by-pixel%2520to%2520obtain%2520a%2520new%2520edge%2520strength%2520map.%250AFinally%252C%2520the%2520edge%2520features%2520are%2520enhanced%2520by%2520image%2520normalization%2520and%2520non-maximum%250Asuppression%2520technology%252C%2520and%2520on%2520that%2520basis%252C%2520the%2520edge%2520contour%2520is%2520obtained%2520by%250Adouble%2520threshold%2520selection%2520and%2520a%2520new%2520morphological%2520refinement%2520method.%2520Through%250Aan%2520experimental%2520analysis%2520of%2520the%2520edge%2520detection%2520dataset%252C%2520the%2520method%2520proposed%2520has%250Agood%2520noise%2520robustness%2520and%2520high%2520edge%2520quality%252C%2520which%2520is%2520better%2520than%2520the%2520Color%250ASobel%252C%2520Color%2520Canny%252C%2520SE%2520and%2520Color%2520AGDD%2520as%2520shown%2520by%2520the%2520PR%2520curve%252C%2520AUC%252C%2520PSNR%252C%2520MSE%252C%250Aand%2520FOM%2520indicators.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2408.14013v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=A%20Multiscale%20Gradient%20Fusion%20Method%20for%20Edge%20Detection%20in%20Color%20Images%0A%20%20Utilizing%20the%20CBM3D%20Filter&entry.906535625=Zhuoyue%20Wang%20and%20Yiyi%20Tao%20and%20Danqing%20Ma%20and%20Jiajing%20Chen&entry.1292438233=%20%20In%20this%20paper%2C%20a%20color%20edge%20detection%20strategy%20based%20on%20collaborative%0Afiltering%20combined%20with%20multiscale%20gradient%20fusion%20is%20proposed.%20The%0Ablock-matching%20and%203D%20%28BM3D%29%20filter%20are%20used%20to%20enhance%20the%20sparse%0Arepresentation%20in%20the%20transform%20domain%20and%20achieve%20the%20effect%20of%20denoising%2C%0Awhereas%20the%20multiscale%20gradient%20fusion%20makes%20up%20for%20the%20defect%20of%20loss%20of%0Adetails%20in%20single-scale%20edge%20detection%20and%20improves%20the%20edge%20detection%0Aresolution%20and%20quality.%20First%2C%20the%20RGB%20images%20in%20the%20dataset%20are%20converted%20to%0AXYZ%20color%20space%20images%20through%20mathematical%20operations.%20Second%2C%20the%20colored%0Ablock-matching%20and%203D%20%28CBM3D%29%20filter%20are%20used%20on%20the%20sparse%20images%20and%20to%0Aremove%20noise%20interference.%20Then%2C%20the%20vector%20gradients%20of%20the%20color%20image%20and%0Athe%20anisotropic%20Gaussian%20directional%20derivative%20of%20the%20two%20scale%20parameters%20are%0Acalculated%20and%20averaged%20pixel-by-pixel%20to%20obtain%20a%20new%20edge%20strength%20map.%0AFinally%2C%20the%20edge%20features%20are%20enhanced%20by%20image%20normalization%20and%20non-maximum%0Asuppression%20technology%2C%20and%20on%20that%20basis%2C%20the%20edge%20contour%20is%20obtained%20by%0Adouble%20threshold%20selection%20and%20a%20new%20morphological%20refinement%20method.%20Through%0Aan%20experimental%20analysis%20of%20the%20edge%20detection%20dataset%2C%20the%20method%20proposed%20has%0Agood%20noise%20robustness%20and%20high%20edge%20quality%2C%20which%20is%20better%20than%20the%20Color%0ASobel%2C%20Color%20Canny%2C%20SE%20and%20Color%20AGDD%20as%20shown%20by%20the%20PR%20curve%2C%20AUC%2C%20PSNR%2C%20MSE%2C%0Aand%20FOM%20indicators.%0A&entry.1838667208=http%3A//arxiv.org/abs/2408.14013v2&entry.124074799=Read"},
{"title": "$OC^4-ReID$: Occluded Cloth-Changing Person Re-Identification", "author": "Zhihao Chen and Yiyuan Ge and Ziyang Wang and Jiaju Kang and Mingya Zhang", "abstract": "  The study of Cloth-Changing Person Re-identification (CC-ReID) focuses on\nretrieving specific pedestrians when their clothing has changed, typically\nunder the assumption that the entire pedestrian images are visible. Pedestrian\nimages in real-world scenarios, however, are often partially obscured by\nobstacles, presenting a significant challenge to existing CC-ReID systems. In\nthis paper, we introduce a more challenging task termed Occluded Cloth-Changing\nPerson Re-Identification ($OC^4-ReID$), which simultaneously addresses two\nchallenges of clothing changes and occlusion. Concretely, we construct two new\ndatasets, Occ-LTCC and Occ-PRCC, based on original CC-ReID datasets to include\nrandom occlusions of key pedestrians components (e.g., head, torso). Moreover,\na novel benchmark is proposed for $OC^4-ReID$ incorporating a Train-Test Micro\nGranularity Screening ($T^2MGS$) module to mitigate the influence of occlusion\nand proposing a Part-Robust Triplet (PRT) loss for partial features learning.\nComprehensive experiments on the proposed datasets, as well as on two CC-ReID\nbenchmark datasets demonstrate the superior performance of proposed method\nagainst other state-of-the-art methods. The codes and datasets are available\nat: https://github.com/1024AILab/OC4-ReID.\n", "link": "http://arxiv.org/abs/2403.08557v3", "date": "2024-09-03", "relevancy": 2.0453, "topK": [{"title": "Magic-Me: Identity-Specific Video Customized Diffusion", "link": "http://arxiv.org/abs/2402.09368v1", "similarity": 0.5209}, {"title": "VirtualModel: Generating Object-ID-retentive Human-object Interaction\n  Image by Diffusion Model for E-commerce Marketing", "link": "http://arxiv.org/abs/2405.09985v1", "similarity": 0.5056}, {"title": "Multimodal-Conditioned Latent Diffusion Models for Fashion Image Editing", "link": "http://arxiv.org/abs/2403.14828v2", "similarity": 0.5018}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20%24OC%5E4-ReID%24%3A%20Occluded%20Cloth-Changing%20Person%20Re-Identification&body=Title%3A%20%24OC%5E4-ReID%24%3A%20Occluded%20Cloth-Changing%20Person%20Re-Identification%0AAuthor%3A%20Zhihao%20Chen%20and%20Yiyuan%20Ge%20and%20Ziyang%20Wang%20and%20Jiaju%20Kang%20and%20Mingya%20Zhang%0AAbstract%3A%20%20%20The%20study%20of%20Cloth-Changing%20Person%20Re-identification%20%28CC-ReID%29%20focuses%20on%0Aretrieving%20specific%20pedestrians%20when%20their%20clothing%20has%20changed%2C%20typically%0Aunder%20the%20assumption%20that%20the%20entire%20pedestrian%20images%20are%20visible.%20Pedestrian%0Aimages%20in%20real-world%20scenarios%2C%20however%2C%20are%20often%20partially%20obscured%20by%0Aobstacles%2C%20presenting%20a%20significant%20challenge%20to%20existing%20CC-ReID%20systems.%20In%0Athis%20paper%2C%20we%20introduce%20a%20more%20challenging%20task%20termed%20Occluded%20Cloth-Changing%0APerson%20Re-Identification%20%28%24OC%5E4-ReID%24%29%2C%20which%20simultaneously%20addresses%20two%0Achallenges%20of%20clothing%20changes%20and%20occlusion.%20Concretely%2C%20we%20construct%20two%20new%0Adatasets%2C%20Occ-LTCC%20and%20Occ-PRCC%2C%20based%20on%20original%20CC-ReID%20datasets%20to%20include%0Arandom%20occlusions%20of%20key%20pedestrians%20components%20%28e.g.%2C%20head%2C%20torso%29.%20Moreover%2C%0Aa%20novel%20benchmark%20is%20proposed%20for%20%24OC%5E4-ReID%24%20incorporating%20a%20Train-Test%20Micro%0AGranularity%20Screening%20%28%24T%5E2MGS%24%29%20module%20to%20mitigate%20the%20influence%20of%20occlusion%0Aand%20proposing%20a%20Part-Robust%20Triplet%20%28PRT%29%20loss%20for%20partial%20features%20learning.%0AComprehensive%20experiments%20on%20the%20proposed%20datasets%2C%20as%20well%20as%20on%20two%20CC-ReID%0Abenchmark%20datasets%20demonstrate%20the%20superior%20performance%20of%20proposed%20method%0Aagainst%20other%20state-of-the-art%20methods.%20The%20codes%20and%20datasets%20are%20available%0Aat%3A%20https%3A//github.com/1024AILab/OC4-ReID.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2403.08557v3%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3D%2524OC%255E4-ReID%2524%253A%2520Occluded%2520Cloth-Changing%2520Person%2520Re-Identification%26entry.906535625%3DZhihao%2520Chen%2520and%2520Yiyuan%2520Ge%2520and%2520Ziyang%2520Wang%2520and%2520Jiaju%2520Kang%2520and%2520Mingya%2520Zhang%26entry.1292438233%3D%2520%2520The%2520study%2520of%2520Cloth-Changing%2520Person%2520Re-identification%2520%2528CC-ReID%2529%2520focuses%2520on%250Aretrieving%2520specific%2520pedestrians%2520when%2520their%2520clothing%2520has%2520changed%252C%2520typically%250Aunder%2520the%2520assumption%2520that%2520the%2520entire%2520pedestrian%2520images%2520are%2520visible.%2520Pedestrian%250Aimages%2520in%2520real-world%2520scenarios%252C%2520however%252C%2520are%2520often%2520partially%2520obscured%2520by%250Aobstacles%252C%2520presenting%2520a%2520significant%2520challenge%2520to%2520existing%2520CC-ReID%2520systems.%2520In%250Athis%2520paper%252C%2520we%2520introduce%2520a%2520more%2520challenging%2520task%2520termed%2520Occluded%2520Cloth-Changing%250APerson%2520Re-Identification%2520%2528%2524OC%255E4-ReID%2524%2529%252C%2520which%2520simultaneously%2520addresses%2520two%250Achallenges%2520of%2520clothing%2520changes%2520and%2520occlusion.%2520Concretely%252C%2520we%2520construct%2520two%2520new%250Adatasets%252C%2520Occ-LTCC%2520and%2520Occ-PRCC%252C%2520based%2520on%2520original%2520CC-ReID%2520datasets%2520to%2520include%250Arandom%2520occlusions%2520of%2520key%2520pedestrians%2520components%2520%2528e.g.%252C%2520head%252C%2520torso%2529.%2520Moreover%252C%250Aa%2520novel%2520benchmark%2520is%2520proposed%2520for%2520%2524OC%255E4-ReID%2524%2520incorporating%2520a%2520Train-Test%2520Micro%250AGranularity%2520Screening%2520%2528%2524T%255E2MGS%2524%2529%2520module%2520to%2520mitigate%2520the%2520influence%2520of%2520occlusion%250Aand%2520proposing%2520a%2520Part-Robust%2520Triplet%2520%2528PRT%2529%2520loss%2520for%2520partial%2520features%2520learning.%250AComprehensive%2520experiments%2520on%2520the%2520proposed%2520datasets%252C%2520as%2520well%2520as%2520on%2520two%2520CC-ReID%250Abenchmark%2520datasets%2520demonstrate%2520the%2520superior%2520performance%2520of%2520proposed%2520method%250Aagainst%2520other%2520state-of-the-art%2520methods.%2520The%2520codes%2520and%2520datasets%2520are%2520available%250Aat%253A%2520https%253A//github.com/1024AILab/OC4-ReID.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2403.08557v3%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=%24OC%5E4-ReID%24%3A%20Occluded%20Cloth-Changing%20Person%20Re-Identification&entry.906535625=Zhihao%20Chen%20and%20Yiyuan%20Ge%20and%20Ziyang%20Wang%20and%20Jiaju%20Kang%20and%20Mingya%20Zhang&entry.1292438233=%20%20The%20study%20of%20Cloth-Changing%20Person%20Re-identification%20%28CC-ReID%29%20focuses%20on%0Aretrieving%20specific%20pedestrians%20when%20their%20clothing%20has%20changed%2C%20typically%0Aunder%20the%20assumption%20that%20the%20entire%20pedestrian%20images%20are%20visible.%20Pedestrian%0Aimages%20in%20real-world%20scenarios%2C%20however%2C%20are%20often%20partially%20obscured%20by%0Aobstacles%2C%20presenting%20a%20significant%20challenge%20to%20existing%20CC-ReID%20systems.%20In%0Athis%20paper%2C%20we%20introduce%20a%20more%20challenging%20task%20termed%20Occluded%20Cloth-Changing%0APerson%20Re-Identification%20%28%24OC%5E4-ReID%24%29%2C%20which%20simultaneously%20addresses%20two%0Achallenges%20of%20clothing%20changes%20and%20occlusion.%20Concretely%2C%20we%20construct%20two%20new%0Adatasets%2C%20Occ-LTCC%20and%20Occ-PRCC%2C%20based%20on%20original%20CC-ReID%20datasets%20to%20include%0Arandom%20occlusions%20of%20key%20pedestrians%20components%20%28e.g.%2C%20head%2C%20torso%29.%20Moreover%2C%0Aa%20novel%20benchmark%20is%20proposed%20for%20%24OC%5E4-ReID%24%20incorporating%20a%20Train-Test%20Micro%0AGranularity%20Screening%20%28%24T%5E2MGS%24%29%20module%20to%20mitigate%20the%20influence%20of%20occlusion%0Aand%20proposing%20a%20Part-Robust%20Triplet%20%28PRT%29%20loss%20for%20partial%20features%20learning.%0AComprehensive%20experiments%20on%20the%20proposed%20datasets%2C%20as%20well%20as%20on%20two%20CC-ReID%0Abenchmark%20datasets%20demonstrate%20the%20superior%20performance%20of%20proposed%20method%0Aagainst%20other%20state-of-the-art%20methods.%20The%20codes%20and%20datasets%20are%20available%0Aat%3A%20https%3A//github.com/1024AILab/OC4-ReID.%0A&entry.1838667208=http%3A//arxiv.org/abs/2403.08557v3&entry.124074799=Read"},
{"title": "Low-Rank Quantization-Aware Training for LLMs", "author": "Yelysei Bondarenko and Riccardo Del Chiaro and Markus Nagel", "abstract": "  Large language models (LLMs) are omnipresent, however their practical\ndeployment is challenging due to their ever increasing computational and memory\ndemands. Quantization is one of the most effective ways to make them more\ncompute and memory efficient. Quantization-aware training (QAT) methods,\ngenerally produce the best quantized performance, however it comes at the cost\nof potentially long training time and excessive memory usage, making it\nimpractical when applying for LLMs. Inspired by parameter-efficient fine-tuning\n(PEFT) and low-rank adaptation (LoRA) literature, we propose LR-QAT -- a\nlightweight and memory-efficient QAT algorithm for LLMs. LR-QAT employs several\ncomponents to save memory without sacrificing predictive performance: (a)\nlow-rank auxiliary weights that are aware of the quantization grid; (b) a\ndowncasting operator using fixed-point or double-packed integers and (c)\ncheckpointing. Unlike most related work, our method (i) is inference-efficient,\nleading to no additional overhead compared to traditional PTQ; (ii) can be seen\nas a general extended pretraining framework, meaning that the resulting model\ncan still be utilized for any downstream task afterwards; (iii) can be applied\nacross a wide range of quantization settings, such as different choices\nquantization granularity, activation quantization, and seamlessly combined with\nmany PTQ techniques. We apply LR-QAT to LLaMA-1/2/3 and Mistral model families\nand validate its effectiveness on several downstream tasks. Our method\noutperforms common post-training quantization (PTQ) approaches and reaches the\nsame model performance as full-model QAT at the fraction of its memory usage.\nSpecifically, we can train a 7B LLM on a single consumer grade GPU with 24GB of\nmemory. Our source code is available at\nhttps://github.com/qualcomm-ai-research/LR-QAT\n", "link": "http://arxiv.org/abs/2406.06385v3", "date": "2024-09-03", "relevancy": 2.0412, "topK": [{"title": "ToDo: Token Downsampling for Efficient Generation of High-Resolution\n  Images", "link": "http://arxiv.org/abs/2402.13573v2", "similarity": 0.5197}, {"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.5075}, {"title": "Breadcrumbs: Adversarial class-balanced sampling for long-tailed recognition", "link": "https://link.springer.com/chapter/10.1007/978-3-031-20053-3_37", "similarity": 0.502}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Low-Rank%20Quantization-Aware%20Training%20for%20LLMs&body=Title%3A%20Low-Rank%20Quantization-Aware%20Training%20for%20LLMs%0AAuthor%3A%20Yelysei%20Bondarenko%20and%20Riccardo%20Del%20Chiaro%20and%20Markus%20Nagel%0AAbstract%3A%20%20%20Large%20language%20models%20%28LLMs%29%20are%20omnipresent%2C%20however%20their%20practical%0Adeployment%20is%20challenging%20due%20to%20their%20ever%20increasing%20computational%20and%20memory%0Ademands.%20Quantization%20is%20one%20of%20the%20most%20effective%20ways%20to%20make%20them%20more%0Acompute%20and%20memory%20efficient.%20Quantization-aware%20training%20%28QAT%29%20methods%2C%0Agenerally%20produce%20the%20best%20quantized%20performance%2C%20however%20it%20comes%20at%20the%20cost%0Aof%20potentially%20long%20training%20time%20and%20excessive%20memory%20usage%2C%20making%20it%0Aimpractical%20when%20applying%20for%20LLMs.%20Inspired%20by%20parameter-efficient%20fine-tuning%0A%28PEFT%29%20and%20low-rank%20adaptation%20%28LoRA%29%20literature%2C%20we%20propose%20LR-QAT%20--%20a%0Alightweight%20and%20memory-efficient%20QAT%20algorithm%20for%20LLMs.%20LR-QAT%20employs%20several%0Acomponents%20to%20save%20memory%20without%20sacrificing%20predictive%20performance%3A%20%28a%29%0Alow-rank%20auxiliary%20weights%20that%20are%20aware%20of%20the%20quantization%20grid%3B%20%28b%29%20a%0Adowncasting%20operator%20using%20fixed-point%20or%20double-packed%20integers%20and%20%28c%29%0Acheckpointing.%20Unlike%20most%20related%20work%2C%20our%20method%20%28i%29%20is%20inference-efficient%2C%0Aleading%20to%20no%20additional%20overhead%20compared%20to%20traditional%20PTQ%3B%20%28ii%29%20can%20be%20seen%0Aas%20a%20general%20extended%20pretraining%20framework%2C%20meaning%20that%20the%20resulting%20model%0Acan%20still%20be%20utilized%20for%20any%20downstream%20task%20afterwards%3B%20%28iii%29%20can%20be%20applied%0Aacross%20a%20wide%20range%20of%20quantization%20settings%2C%20such%20as%20different%20choices%0Aquantization%20granularity%2C%20activation%20quantization%2C%20and%20seamlessly%20combined%20with%0Amany%20PTQ%20techniques.%20We%20apply%20LR-QAT%20to%20LLaMA-1/2/3%20and%20Mistral%20model%20families%0Aand%20validate%20its%20effectiveness%20on%20several%20downstream%20tasks.%20Our%20method%0Aoutperforms%20common%20post-training%20quantization%20%28PTQ%29%20approaches%20and%20reaches%20the%0Asame%20model%20performance%20as%20full-model%20QAT%20at%20the%20fraction%20of%20its%20memory%20usage.%0ASpecifically%2C%20we%20can%20train%20a%207B%20LLM%20on%20a%20single%20consumer%20grade%20GPU%20with%2024GB%20of%0Amemory.%20Our%20source%20code%20is%20available%20at%0Ahttps%3A//github.com/qualcomm-ai-research/LR-QAT%0A%0ALink%3A%20http%3A//arxiv.org/abs/2406.06385v3%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DLow-Rank%2520Quantization-Aware%2520Training%2520for%2520LLMs%26entry.906535625%3DYelysei%2520Bondarenko%2520and%2520Riccardo%2520Del%2520Chiaro%2520and%2520Markus%2520Nagel%26entry.1292438233%3D%2520%2520Large%2520language%2520models%2520%2528LLMs%2529%2520are%2520omnipresent%252C%2520however%2520their%2520practical%250Adeployment%2520is%2520challenging%2520due%2520to%2520their%2520ever%2520increasing%2520computational%2520and%2520memory%250Ademands.%2520Quantization%2520is%2520one%2520of%2520the%2520most%2520effective%2520ways%2520to%2520make%2520them%2520more%250Acompute%2520and%2520memory%2520efficient.%2520Quantization-aware%2520training%2520%2528QAT%2529%2520methods%252C%250Agenerally%2520produce%2520the%2520best%2520quantized%2520performance%252C%2520however%2520it%2520comes%2520at%2520the%2520cost%250Aof%2520potentially%2520long%2520training%2520time%2520and%2520excessive%2520memory%2520usage%252C%2520making%2520it%250Aimpractical%2520when%2520applying%2520for%2520LLMs.%2520Inspired%2520by%2520parameter-efficient%2520fine-tuning%250A%2528PEFT%2529%2520and%2520low-rank%2520adaptation%2520%2528LoRA%2529%2520literature%252C%2520we%2520propose%2520LR-QAT%2520--%2520a%250Alightweight%2520and%2520memory-efficient%2520QAT%2520algorithm%2520for%2520LLMs.%2520LR-QAT%2520employs%2520several%250Acomponents%2520to%2520save%2520memory%2520without%2520sacrificing%2520predictive%2520performance%253A%2520%2528a%2529%250Alow-rank%2520auxiliary%2520weights%2520that%2520are%2520aware%2520of%2520the%2520quantization%2520grid%253B%2520%2528b%2529%2520a%250Adowncasting%2520operator%2520using%2520fixed-point%2520or%2520double-packed%2520integers%2520and%2520%2528c%2529%250Acheckpointing.%2520Unlike%2520most%2520related%2520work%252C%2520our%2520method%2520%2528i%2529%2520is%2520inference-efficient%252C%250Aleading%2520to%2520no%2520additional%2520overhead%2520compared%2520to%2520traditional%2520PTQ%253B%2520%2528ii%2529%2520can%2520be%2520seen%250Aas%2520a%2520general%2520extended%2520pretraining%2520framework%252C%2520meaning%2520that%2520the%2520resulting%2520model%250Acan%2520still%2520be%2520utilized%2520for%2520any%2520downstream%2520task%2520afterwards%253B%2520%2528iii%2529%2520can%2520be%2520applied%250Aacross%2520a%2520wide%2520range%2520of%2520quantization%2520settings%252C%2520such%2520as%2520different%2520choices%250Aquantization%2520granularity%252C%2520activation%2520quantization%252C%2520and%2520seamlessly%2520combined%2520with%250Amany%2520PTQ%2520techniques.%2520We%2520apply%2520LR-QAT%2520to%2520LLaMA-1/2/3%2520and%2520Mistral%2520model%2520families%250Aand%2520validate%2520its%2520effectiveness%2520on%2520several%2520downstream%2520tasks.%2520Our%2520method%250Aoutperforms%2520common%2520post-training%2520quantization%2520%2528PTQ%2529%2520approaches%2520and%2520reaches%2520the%250Asame%2520model%2520performance%2520as%2520full-model%2520QAT%2520at%2520the%2520fraction%2520of%2520its%2520memory%2520usage.%250ASpecifically%252C%2520we%2520can%2520train%2520a%25207B%2520LLM%2520on%2520a%2520single%2520consumer%2520grade%2520GPU%2520with%252024GB%2520of%250Amemory.%2520Our%2520source%2520code%2520is%2520available%2520at%250Ahttps%253A//github.com/qualcomm-ai-research/LR-QAT%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2406.06385v3%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Low-Rank%20Quantization-Aware%20Training%20for%20LLMs&entry.906535625=Yelysei%20Bondarenko%20and%20Riccardo%20Del%20Chiaro%20and%20Markus%20Nagel&entry.1292438233=%20%20Large%20language%20models%20%28LLMs%29%20are%20omnipresent%2C%20however%20their%20practical%0Adeployment%20is%20challenging%20due%20to%20their%20ever%20increasing%20computational%20and%20memory%0Ademands.%20Quantization%20is%20one%20of%20the%20most%20effective%20ways%20to%20make%20them%20more%0Acompute%20and%20memory%20efficient.%20Quantization-aware%20training%20%28QAT%29%20methods%2C%0Agenerally%20produce%20the%20best%20quantized%20performance%2C%20however%20it%20comes%20at%20the%20cost%0Aof%20potentially%20long%20training%20time%20and%20excessive%20memory%20usage%2C%20making%20it%0Aimpractical%20when%20applying%20for%20LLMs.%20Inspired%20by%20parameter-efficient%20fine-tuning%0A%28PEFT%29%20and%20low-rank%20adaptation%20%28LoRA%29%20literature%2C%20we%20propose%20LR-QAT%20--%20a%0Alightweight%20and%20memory-efficient%20QAT%20algorithm%20for%20LLMs.%20LR-QAT%20employs%20several%0Acomponents%20to%20save%20memory%20without%20sacrificing%20predictive%20performance%3A%20%28a%29%0Alow-rank%20auxiliary%20weights%20that%20are%20aware%20of%20the%20quantization%20grid%3B%20%28b%29%20a%0Adowncasting%20operator%20using%20fixed-point%20or%20double-packed%20integers%20and%20%28c%29%0Acheckpointing.%20Unlike%20most%20related%20work%2C%20our%20method%20%28i%29%20is%20inference-efficient%2C%0Aleading%20to%20no%20additional%20overhead%20compared%20to%20traditional%20PTQ%3B%20%28ii%29%20can%20be%20seen%0Aas%20a%20general%20extended%20pretraining%20framework%2C%20meaning%20that%20the%20resulting%20model%0Acan%20still%20be%20utilized%20for%20any%20downstream%20task%20afterwards%3B%20%28iii%29%20can%20be%20applied%0Aacross%20a%20wide%20range%20of%20quantization%20settings%2C%20such%20as%20different%20choices%0Aquantization%20granularity%2C%20activation%20quantization%2C%20and%20seamlessly%20combined%20with%0Amany%20PTQ%20techniques.%20We%20apply%20LR-QAT%20to%20LLaMA-1/2/3%20and%20Mistral%20model%20families%0Aand%20validate%20its%20effectiveness%20on%20several%20downstream%20tasks.%20Our%20method%0Aoutperforms%20common%20post-training%20quantization%20%28PTQ%29%20approaches%20and%20reaches%20the%0Asame%20model%20performance%20as%20full-model%20QAT%20at%20the%20fraction%20of%20its%20memory%20usage.%0ASpecifically%2C%20we%20can%20train%20a%207B%20LLM%20on%20a%20single%20consumer%20grade%20GPU%20with%2024GB%20of%0Amemory.%20Our%20source%20code%20is%20available%20at%0Ahttps%3A//github.com/qualcomm-ai-research/LR-QAT%0A&entry.1838667208=http%3A//arxiv.org/abs/2406.06385v3&entry.124074799=Read"},
{"title": "PID Accelerated Temporal Difference Algorithms", "author": "Mark Bedaywi and Amin Rakhsha and Amir-massoud Farahmand", "abstract": "  Long-horizon tasks, which have a large discount factor, pose a challenge for\nmost conventional reinforcement learning (RL) algorithms. Algorithms such as\nValue Iteration and Temporal Difference (TD) learning have a slow convergence\nrate and become inefficient in these tasks. When the transition distributions\nare given, PID VI was recently introduced to accelerate the convergence of\nValue Iteration using ideas from control theory. Inspired by this, we introduce\nPID TD Learning and PID Q-Learning algorithms for the RL setting, in which only\nsamples from the environment are available. We give a theoretical analysis of\nthe convergence of PID TD Learning and its acceleration compared to the\nconventional TD Learning. We also introduce a method for adapting PID gains in\nthe presence of noise and empirically verify its effectiveness.\n", "link": "http://arxiv.org/abs/2407.08803v2", "date": "2024-09-03", "relevancy": 1.9654, "topK": [{"title": "Safe and efficient training of a control agent", "link": "https://patents.google.com/patent/US11709462B2/en", "similarity": 0.5122}, {"title": "SDXS: Real-Time One-Step Latent Diffusion Models with Image Conditions", "link": "http://arxiv.org/abs/2403.16627v1", "similarity": 0.4974}, {"title": "ToDo: Token Downsampling for Efficient Generation of High-Resolution\n  Images", "link": "http://arxiv.org/abs/2402.13573v2", "similarity": 0.468}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20PID%20Accelerated%20Temporal%20Difference%20Algorithms&body=Title%3A%20PID%20Accelerated%20Temporal%20Difference%20Algorithms%0AAuthor%3A%20Mark%20Bedaywi%20and%20Amin%20Rakhsha%20and%20Amir-massoud%20Farahmand%0AAbstract%3A%20%20%20Long-horizon%20tasks%2C%20which%20have%20a%20large%20discount%20factor%2C%20pose%20a%20challenge%20for%0Amost%20conventional%20reinforcement%20learning%20%28RL%29%20algorithms.%20Algorithms%20such%20as%0AValue%20Iteration%20and%20Temporal%20Difference%20%28TD%29%20learning%20have%20a%20slow%20convergence%0Arate%20and%20become%20inefficient%20in%20these%20tasks.%20When%20the%20transition%20distributions%0Aare%20given%2C%20PID%20VI%20was%20recently%20introduced%20to%20accelerate%20the%20convergence%20of%0AValue%20Iteration%20using%20ideas%20from%20control%20theory.%20Inspired%20by%20this%2C%20we%20introduce%0APID%20TD%20Learning%20and%20PID%20Q-Learning%20algorithms%20for%20the%20RL%20setting%2C%20in%20which%20only%0Asamples%20from%20the%20environment%20are%20available.%20We%20give%20a%20theoretical%20analysis%20of%0Athe%20convergence%20of%20PID%20TD%20Learning%20and%20its%20acceleration%20compared%20to%20the%0Aconventional%20TD%20Learning.%20We%20also%20introduce%20a%20method%20for%20adapting%20PID%20gains%20in%0Athe%20presence%20of%20noise%20and%20empirically%20verify%20its%20effectiveness.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2407.08803v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DPID%2520Accelerated%2520Temporal%2520Difference%2520Algorithms%26entry.906535625%3DMark%2520Bedaywi%2520and%2520Amin%2520Rakhsha%2520and%2520Amir-massoud%2520Farahmand%26entry.1292438233%3D%2520%2520Long-horizon%2520tasks%252C%2520which%2520have%2520a%2520large%2520discount%2520factor%252C%2520pose%2520a%2520challenge%2520for%250Amost%2520conventional%2520reinforcement%2520learning%2520%2528RL%2529%2520algorithms.%2520Algorithms%2520such%2520as%250AValue%2520Iteration%2520and%2520Temporal%2520Difference%2520%2528TD%2529%2520learning%2520have%2520a%2520slow%2520convergence%250Arate%2520and%2520become%2520inefficient%2520in%2520these%2520tasks.%2520When%2520the%2520transition%2520distributions%250Aare%2520given%252C%2520PID%2520VI%2520was%2520recently%2520introduced%2520to%2520accelerate%2520the%2520convergence%2520of%250AValue%2520Iteration%2520using%2520ideas%2520from%2520control%2520theory.%2520Inspired%2520by%2520this%252C%2520we%2520introduce%250APID%2520TD%2520Learning%2520and%2520PID%2520Q-Learning%2520algorithms%2520for%2520the%2520RL%2520setting%252C%2520in%2520which%2520only%250Asamples%2520from%2520the%2520environment%2520are%2520available.%2520We%2520give%2520a%2520theoretical%2520analysis%2520of%250Athe%2520convergence%2520of%2520PID%2520TD%2520Learning%2520and%2520its%2520acceleration%2520compared%2520to%2520the%250Aconventional%2520TD%2520Learning.%2520We%2520also%2520introduce%2520a%2520method%2520for%2520adapting%2520PID%2520gains%2520in%250Athe%2520presence%2520of%2520noise%2520and%2520empirically%2520verify%2520its%2520effectiveness.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2407.08803v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=PID%20Accelerated%20Temporal%20Difference%20Algorithms&entry.906535625=Mark%20Bedaywi%20and%20Amin%20Rakhsha%20and%20Amir-massoud%20Farahmand&entry.1292438233=%20%20Long-horizon%20tasks%2C%20which%20have%20a%20large%20discount%20factor%2C%20pose%20a%20challenge%20for%0Amost%20conventional%20reinforcement%20learning%20%28RL%29%20algorithms.%20Algorithms%20such%20as%0AValue%20Iteration%20and%20Temporal%20Difference%20%28TD%29%20learning%20have%20a%20slow%20convergence%0Arate%20and%20become%20inefficient%20in%20these%20tasks.%20When%20the%20transition%20distributions%0Aare%20given%2C%20PID%20VI%20was%20recently%20introduced%20to%20accelerate%20the%20convergence%20of%0AValue%20Iteration%20using%20ideas%20from%20control%20theory.%20Inspired%20by%20this%2C%20we%20introduce%0APID%20TD%20Learning%20and%20PID%20Q-Learning%20algorithms%20for%20the%20RL%20setting%2C%20in%20which%20only%0Asamples%20from%20the%20environment%20are%20available.%20We%20give%20a%20theoretical%20analysis%20of%0Athe%20convergence%20of%20PID%20TD%20Learning%20and%20its%20acceleration%20compared%20to%20the%0Aconventional%20TD%20Learning.%20We%20also%20introduce%20a%20method%20for%20adapting%20PID%20gains%20in%0Athe%20presence%20of%20noise%20and%20empirically%20verify%20its%20effectiveness.%0A&entry.1838667208=http%3A//arxiv.org/abs/2407.08803v2&entry.124074799=Read"},
{"title": "Bayesian Learning in a Nonlinear Multiscale State-Space Model", "author": "Nayely V\u00e9lez-Cruz and Manfred D. Laubichler", "abstract": "  The ubiquity of multiscale interactions in complex systems is\nwell-recognized, with development and heredity serving as a prime example of\nhow processes at different temporal scales influence one another. This work\nintroduces a novel multiscale state-space model to explore the dynamic\ninterplay between systems interacting across different time scales, with\nfeedback between each scale. We propose a Bayesian learning framework to\nestimate unknown states by learning the unknown process noise covariances\nwithin this multiscale model. We develop a Particle Gibbs with Ancestor\nSampling (PGAS) algorithm for inference and demonstrate through simulations the\nefficacy of our approach.\n", "link": "http://arxiv.org/abs/2408.06425v6", "date": "2024-09-03", "relevancy": 1.92, "topK": [{"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.5959}, {"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.4602}, {"title": "Genie: Generative Interactive Environments", "link": "http://arxiv.org/abs/2402.15391v1", "similarity": 0.4535}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Bayesian%20Learning%20in%20a%20Nonlinear%20Multiscale%20State-Space%20Model&body=Title%3A%20Bayesian%20Learning%20in%20a%20Nonlinear%20Multiscale%20State-Space%20Model%0AAuthor%3A%20Nayely%20V%C3%A9lez-Cruz%20and%20Manfred%20D.%20Laubichler%0AAbstract%3A%20%20%20The%20ubiquity%20of%20multiscale%20interactions%20in%20complex%20systems%20is%0Awell-recognized%2C%20with%20development%20and%20heredity%20serving%20as%20a%20prime%20example%20of%0Ahow%20processes%20at%20different%20temporal%20scales%20influence%20one%20another.%20This%20work%0Aintroduces%20a%20novel%20multiscale%20state-space%20model%20to%20explore%20the%20dynamic%0Ainterplay%20between%20systems%20interacting%20across%20different%20time%20scales%2C%20with%0Afeedback%20between%20each%20scale.%20We%20propose%20a%20Bayesian%20learning%20framework%20to%0Aestimate%20unknown%20states%20by%20learning%20the%20unknown%20process%20noise%20covariances%0Awithin%20this%20multiscale%20model.%20We%20develop%20a%20Particle%20Gibbs%20with%20Ancestor%0ASampling%20%28PGAS%29%20algorithm%20for%20inference%20and%20demonstrate%20through%20simulations%20the%0Aefficacy%20of%20our%20approach.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2408.06425v6%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DBayesian%2520Learning%2520in%2520a%2520Nonlinear%2520Multiscale%2520State-Space%2520Model%26entry.906535625%3DNayely%2520V%25C3%25A9lez-Cruz%2520and%2520Manfred%2520D.%2520Laubichler%26entry.1292438233%3D%2520%2520The%2520ubiquity%2520of%2520multiscale%2520interactions%2520in%2520complex%2520systems%2520is%250Awell-recognized%252C%2520with%2520development%2520and%2520heredity%2520serving%2520as%2520a%2520prime%2520example%2520of%250Ahow%2520processes%2520at%2520different%2520temporal%2520scales%2520influence%2520one%2520another.%2520This%2520work%250Aintroduces%2520a%2520novel%2520multiscale%2520state-space%2520model%2520to%2520explore%2520the%2520dynamic%250Ainterplay%2520between%2520systems%2520interacting%2520across%2520different%2520time%2520scales%252C%2520with%250Afeedback%2520between%2520each%2520scale.%2520We%2520propose%2520a%2520Bayesian%2520learning%2520framework%2520to%250Aestimate%2520unknown%2520states%2520by%2520learning%2520the%2520unknown%2520process%2520noise%2520covariances%250Awithin%2520this%2520multiscale%2520model.%2520We%2520develop%2520a%2520Particle%2520Gibbs%2520with%2520Ancestor%250ASampling%2520%2528PGAS%2529%2520algorithm%2520for%2520inference%2520and%2520demonstrate%2520through%2520simulations%2520the%250Aefficacy%2520of%2520our%2520approach.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2408.06425v6%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Bayesian%20Learning%20in%20a%20Nonlinear%20Multiscale%20State-Space%20Model&entry.906535625=Nayely%20V%C3%A9lez-Cruz%20and%20Manfred%20D.%20Laubichler&entry.1292438233=%20%20The%20ubiquity%20of%20multiscale%20interactions%20in%20complex%20systems%20is%0Awell-recognized%2C%20with%20development%20and%20heredity%20serving%20as%20a%20prime%20example%20of%0Ahow%20processes%20at%20different%20temporal%20scales%20influence%20one%20another.%20This%20work%0Aintroduces%20a%20novel%20multiscale%20state-space%20model%20to%20explore%20the%20dynamic%0Ainterplay%20between%20systems%20interacting%20across%20different%20time%20scales%2C%20with%0Afeedback%20between%20each%20scale.%20We%20propose%20a%20Bayesian%20learning%20framework%20to%0Aestimate%20unknown%20states%20by%20learning%20the%20unknown%20process%20noise%20covariances%0Awithin%20this%20multiscale%20model.%20We%20develop%20a%20Particle%20Gibbs%20with%20Ancestor%0ASampling%20%28PGAS%29%20algorithm%20for%20inference%20and%20demonstrate%20through%20simulations%20the%0Aefficacy%20of%20our%20approach.%0A&entry.1838667208=http%3A//arxiv.org/abs/2408.06425v6&entry.124074799=Read"},
{"title": "OceanGPT: A Large Language Model for Ocean Science Tasks", "author": "Zhen Bi and Ningyu Zhang and Yida Xue and Yixin Ou and Daxiong Ji and Guozhou Zheng and Huajun Chen", "abstract": "  Ocean science, which delves into the oceans that are reservoirs of life and\nbiodiversity, is of great significance given that oceans cover over 70% of our\nplanet's surface. Recently, advances in Large Language Models (LLMs) have\ntransformed the paradigm in science. Despite the success in other domains,\ncurrent LLMs often fall short in catering to the needs of domain experts like\noceanographers, and the potential of LLMs for ocean science is under-explored.\nThe intrinsic reasons are the immense and intricate nature of ocean data as\nwell as the necessity for higher granularity and richness in knowledge. To\nalleviate these issues, we introduce OceanGPT, the first-ever large language\nmodel in the ocean domain, which is expert in various ocean science tasks. We\nalso propose OceanGPT, a novel framework to automatically obtain a large volume\nof ocean domain instruction data, which generates instructions based on\nmulti-agent collaboration. Additionally, we construct the first oceanography\nbenchmark, OceanBench, to evaluate the capabilities of LLMs in the ocean\ndomain. Though comprehensive experiments, OceanGPT not only shows a higher\nlevel of knowledge expertise for oceans science tasks but also gains\npreliminary embodied intelligence capabilities in ocean technology.\n", "link": "http://arxiv.org/abs/2310.02031v8", "date": "2024-09-03", "relevancy": 1.8791, "topK": [{"title": "Genie: Generative Interactive Environments", "link": "http://arxiv.org/abs/2402.15391v1", "similarity": 0.4733}, {"title": "RoboEXP: Action-Conditioned Scene Graph via Interactive Exploration for\n  Robotic Manipulation", "link": "http://arxiv.org/abs/2402.15487v1", "similarity": 0.4687}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.4667}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20OceanGPT%3A%20A%20Large%20Language%20Model%20for%20Ocean%20Science%20Tasks&body=Title%3A%20OceanGPT%3A%20A%20Large%20Language%20Model%20for%20Ocean%20Science%20Tasks%0AAuthor%3A%20Zhen%20Bi%20and%20Ningyu%20Zhang%20and%20Yida%20Xue%20and%20Yixin%20Ou%20and%20Daxiong%20Ji%20and%20Guozhou%20Zheng%20and%20Huajun%20Chen%0AAbstract%3A%20%20%20Ocean%20science%2C%20which%20delves%20into%20the%20oceans%20that%20are%20reservoirs%20of%20life%20and%0Abiodiversity%2C%20is%20of%20great%20significance%20given%20that%20oceans%20cover%20over%2070%25%20of%20our%0Aplanet%27s%20surface.%20Recently%2C%20advances%20in%20Large%20Language%20Models%20%28LLMs%29%20have%0Atransformed%20the%20paradigm%20in%20science.%20Despite%20the%20success%20in%20other%20domains%2C%0Acurrent%20LLMs%20often%20fall%20short%20in%20catering%20to%20the%20needs%20of%20domain%20experts%20like%0Aoceanographers%2C%20and%20the%20potential%20of%20LLMs%20for%20ocean%20science%20is%20under-explored.%0AThe%20intrinsic%20reasons%20are%20the%20immense%20and%20intricate%20nature%20of%20ocean%20data%20as%0Awell%20as%20the%20necessity%20for%20higher%20granularity%20and%20richness%20in%20knowledge.%20To%0Aalleviate%20these%20issues%2C%20we%20introduce%20OceanGPT%2C%20the%20first-ever%20large%20language%0Amodel%20in%20the%20ocean%20domain%2C%20which%20is%20expert%20in%20various%20ocean%20science%20tasks.%20We%0Aalso%20propose%20OceanGPT%2C%20a%20novel%20framework%20to%20automatically%20obtain%20a%20large%20volume%0Aof%20ocean%20domain%20instruction%20data%2C%20which%20generates%20instructions%20based%20on%0Amulti-agent%20collaboration.%20Additionally%2C%20we%20construct%20the%20first%20oceanography%0Abenchmark%2C%20OceanBench%2C%20to%20evaluate%20the%20capabilities%20of%20LLMs%20in%20the%20ocean%0Adomain.%20Though%20comprehensive%20experiments%2C%20OceanGPT%20not%20only%20shows%20a%20higher%0Alevel%20of%20knowledge%20expertise%20for%20oceans%20science%20tasks%20but%20also%20gains%0Apreliminary%20embodied%20intelligence%20capabilities%20in%20ocean%20technology.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2310.02031v8%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DOceanGPT%253A%2520A%2520Large%2520Language%2520Model%2520for%2520Ocean%2520Science%2520Tasks%26entry.906535625%3DZhen%2520Bi%2520and%2520Ningyu%2520Zhang%2520and%2520Yida%2520Xue%2520and%2520Yixin%2520Ou%2520and%2520Daxiong%2520Ji%2520and%2520Guozhou%2520Zheng%2520and%2520Huajun%2520Chen%26entry.1292438233%3D%2520%2520Ocean%2520science%252C%2520which%2520delves%2520into%2520the%2520oceans%2520that%2520are%2520reservoirs%2520of%2520life%2520and%250Abiodiversity%252C%2520is%2520of%2520great%2520significance%2520given%2520that%2520oceans%2520cover%2520over%252070%2525%2520of%2520our%250Aplanet%2527s%2520surface.%2520Recently%252C%2520advances%2520in%2520Large%2520Language%2520Models%2520%2528LLMs%2529%2520have%250Atransformed%2520the%2520paradigm%2520in%2520science.%2520Despite%2520the%2520success%2520in%2520other%2520domains%252C%250Acurrent%2520LLMs%2520often%2520fall%2520short%2520in%2520catering%2520to%2520the%2520needs%2520of%2520domain%2520experts%2520like%250Aoceanographers%252C%2520and%2520the%2520potential%2520of%2520LLMs%2520for%2520ocean%2520science%2520is%2520under-explored.%250AThe%2520intrinsic%2520reasons%2520are%2520the%2520immense%2520and%2520intricate%2520nature%2520of%2520ocean%2520data%2520as%250Awell%2520as%2520the%2520necessity%2520for%2520higher%2520granularity%2520and%2520richness%2520in%2520knowledge.%2520To%250Aalleviate%2520these%2520issues%252C%2520we%2520introduce%2520OceanGPT%252C%2520the%2520first-ever%2520large%2520language%250Amodel%2520in%2520the%2520ocean%2520domain%252C%2520which%2520is%2520expert%2520in%2520various%2520ocean%2520science%2520tasks.%2520We%250Aalso%2520propose%2520OceanGPT%252C%2520a%2520novel%2520framework%2520to%2520automatically%2520obtain%2520a%2520large%2520volume%250Aof%2520ocean%2520domain%2520instruction%2520data%252C%2520which%2520generates%2520instructions%2520based%2520on%250Amulti-agent%2520collaboration.%2520Additionally%252C%2520we%2520construct%2520the%2520first%2520oceanography%250Abenchmark%252C%2520OceanBench%252C%2520to%2520evaluate%2520the%2520capabilities%2520of%2520LLMs%2520in%2520the%2520ocean%250Adomain.%2520Though%2520comprehensive%2520experiments%252C%2520OceanGPT%2520not%2520only%2520shows%2520a%2520higher%250Alevel%2520of%2520knowledge%2520expertise%2520for%2520oceans%2520science%2520tasks%2520but%2520also%2520gains%250Apreliminary%2520embodied%2520intelligence%2520capabilities%2520in%2520ocean%2520technology.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2310.02031v8%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=OceanGPT%3A%20A%20Large%20Language%20Model%20for%20Ocean%20Science%20Tasks&entry.906535625=Zhen%20Bi%20and%20Ningyu%20Zhang%20and%20Yida%20Xue%20and%20Yixin%20Ou%20and%20Daxiong%20Ji%20and%20Guozhou%20Zheng%20and%20Huajun%20Chen&entry.1292438233=%20%20Ocean%20science%2C%20which%20delves%20into%20the%20oceans%20that%20are%20reservoirs%20of%20life%20and%0Abiodiversity%2C%20is%20of%20great%20significance%20given%20that%20oceans%20cover%20over%2070%25%20of%20our%0Aplanet%27s%20surface.%20Recently%2C%20advances%20in%20Large%20Language%20Models%20%28LLMs%29%20have%0Atransformed%20the%20paradigm%20in%20science.%20Despite%20the%20success%20in%20other%20domains%2C%0Acurrent%20LLMs%20often%20fall%20short%20in%20catering%20to%20the%20needs%20of%20domain%20experts%20like%0Aoceanographers%2C%20and%20the%20potential%20of%20LLMs%20for%20ocean%20science%20is%20under-explored.%0AThe%20intrinsic%20reasons%20are%20the%20immense%20and%20intricate%20nature%20of%20ocean%20data%20as%0Awell%20as%20the%20necessity%20for%20higher%20granularity%20and%20richness%20in%20knowledge.%20To%0Aalleviate%20these%20issues%2C%20we%20introduce%20OceanGPT%2C%20the%20first-ever%20large%20language%0Amodel%20in%20the%20ocean%20domain%2C%20which%20is%20expert%20in%20various%20ocean%20science%20tasks.%20We%0Aalso%20propose%20OceanGPT%2C%20a%20novel%20framework%20to%20automatically%20obtain%20a%20large%20volume%0Aof%20ocean%20domain%20instruction%20data%2C%20which%20generates%20instructions%20based%20on%0Amulti-agent%20collaboration.%20Additionally%2C%20we%20construct%20the%20first%20oceanography%0Abenchmark%2C%20OceanBench%2C%20to%20evaluate%20the%20capabilities%20of%20LLMs%20in%20the%20ocean%0Adomain.%20Though%20comprehensive%20experiments%2C%20OceanGPT%20not%20only%20shows%20a%20higher%0Alevel%20of%20knowledge%20expertise%20for%20oceans%20science%20tasks%20but%20also%20gains%0Apreliminary%20embodied%20intelligence%20capabilities%20in%20ocean%20technology.%0A&entry.1838667208=http%3A//arxiv.org/abs/2310.02031v8&entry.124074799=Read"},
{"title": "Heterogeneity-Informed Meta-Parameter Learning for Spatiotemporal Time\n  Series Forecasting", "author": "Zheng Dong and Renhe Jiang and Haotian Gao and Hangchen Liu and Jinliang Deng and Qingsong Wen and Xuan Song", "abstract": "  Spatiotemporal time series forecasting plays a key role in a wide range of\nreal-world applications. While significant progress has been made in this area,\nfully capturing and leveraging spatiotemporal heterogeneity remains a\nfundamental challenge. Therefore, we propose a novel Heterogeneity-Informed\nMeta-Parameter Learning scheme. Specifically, our approach implicitly captures\nspatiotemporal heterogeneity through learning spatial and temporal embeddings,\nwhich can be viewed as a clustering process. Then, a novel spatiotemporal\nmeta-parameter learning paradigm is proposed to learn spatiotemporal-specific\nparameters from meta-parameter pools, which is informed by the captured\nheterogeneity. Based on these ideas, we develop a Heterogeneity-Informed\nSpatiotemporal Meta-Network (HimNet) for spatiotemporal time series\nforecasting. Extensive experiments on five widely-used benchmarks demonstrate\nour method achieves state-of-the-art performance while exhibiting superior\ninterpretability. Our code is available at\nhttps://github.com/XDZhelheim/HimNet.\n", "link": "http://arxiv.org/abs/2405.10800v2", "date": "2024-09-03", "relevancy": 1.8697, "topK": [{"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.4997}, {"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.4751}, {"title": "DDM-NET: End-to-end learning of keypoint feature Detection, Description and Matching for 3D localization", "link": "https://arxiv.org/abs/2212.04575", "similarity": 0.4469}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Heterogeneity-Informed%20Meta-Parameter%20Learning%20for%20Spatiotemporal%20Time%0A%20%20Series%20Forecasting&body=Title%3A%20Heterogeneity-Informed%20Meta-Parameter%20Learning%20for%20Spatiotemporal%20Time%0A%20%20Series%20Forecasting%0AAuthor%3A%20Zheng%20Dong%20and%20Renhe%20Jiang%20and%20Haotian%20Gao%20and%20Hangchen%20Liu%20and%20Jinliang%20Deng%20and%20Qingsong%20Wen%20and%20Xuan%20Song%0AAbstract%3A%20%20%20Spatiotemporal%20time%20series%20forecasting%20plays%20a%20key%20role%20in%20a%20wide%20range%20of%0Areal-world%20applications.%20While%20significant%20progress%20has%20been%20made%20in%20this%20area%2C%0Afully%20capturing%20and%20leveraging%20spatiotemporal%20heterogeneity%20remains%20a%0Afundamental%20challenge.%20Therefore%2C%20we%20propose%20a%20novel%20Heterogeneity-Informed%0AMeta-Parameter%20Learning%20scheme.%20Specifically%2C%20our%20approach%20implicitly%20captures%0Aspatiotemporal%20heterogeneity%20through%20learning%20spatial%20and%20temporal%20embeddings%2C%0Awhich%20can%20be%20viewed%20as%20a%20clustering%20process.%20Then%2C%20a%20novel%20spatiotemporal%0Ameta-parameter%20learning%20paradigm%20is%20proposed%20to%20learn%20spatiotemporal-specific%0Aparameters%20from%20meta-parameter%20pools%2C%20which%20is%20informed%20by%20the%20captured%0Aheterogeneity.%20Based%20on%20these%20ideas%2C%20we%20develop%20a%20Heterogeneity-Informed%0ASpatiotemporal%20Meta-Network%20%28HimNet%29%20for%20spatiotemporal%20time%20series%0Aforecasting.%20Extensive%20experiments%20on%20five%20widely-used%20benchmarks%20demonstrate%0Aour%20method%20achieves%20state-of-the-art%20performance%20while%20exhibiting%20superior%0Ainterpretability.%20Our%20code%20is%20available%20at%0Ahttps%3A//github.com/XDZhelheim/HimNet.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2405.10800v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DHeterogeneity-Informed%2520Meta-Parameter%2520Learning%2520for%2520Spatiotemporal%2520Time%250A%2520%2520Series%2520Forecasting%26entry.906535625%3DZheng%2520Dong%2520and%2520Renhe%2520Jiang%2520and%2520Haotian%2520Gao%2520and%2520Hangchen%2520Liu%2520and%2520Jinliang%2520Deng%2520and%2520Qingsong%2520Wen%2520and%2520Xuan%2520Song%26entry.1292438233%3D%2520%2520Spatiotemporal%2520time%2520series%2520forecasting%2520plays%2520a%2520key%2520role%2520in%2520a%2520wide%2520range%2520of%250Areal-world%2520applications.%2520While%2520significant%2520progress%2520has%2520been%2520made%2520in%2520this%2520area%252C%250Afully%2520capturing%2520and%2520leveraging%2520spatiotemporal%2520heterogeneity%2520remains%2520a%250Afundamental%2520challenge.%2520Therefore%252C%2520we%2520propose%2520a%2520novel%2520Heterogeneity-Informed%250AMeta-Parameter%2520Learning%2520scheme.%2520Specifically%252C%2520our%2520approach%2520implicitly%2520captures%250Aspatiotemporal%2520heterogeneity%2520through%2520learning%2520spatial%2520and%2520temporal%2520embeddings%252C%250Awhich%2520can%2520be%2520viewed%2520as%2520a%2520clustering%2520process.%2520Then%252C%2520a%2520novel%2520spatiotemporal%250Ameta-parameter%2520learning%2520paradigm%2520is%2520proposed%2520to%2520learn%2520spatiotemporal-specific%250Aparameters%2520from%2520meta-parameter%2520pools%252C%2520which%2520is%2520informed%2520by%2520the%2520captured%250Aheterogeneity.%2520Based%2520on%2520these%2520ideas%252C%2520we%2520develop%2520a%2520Heterogeneity-Informed%250ASpatiotemporal%2520Meta-Network%2520%2528HimNet%2529%2520for%2520spatiotemporal%2520time%2520series%250Aforecasting.%2520Extensive%2520experiments%2520on%2520five%2520widely-used%2520benchmarks%2520demonstrate%250Aour%2520method%2520achieves%2520state-of-the-art%2520performance%2520while%2520exhibiting%2520superior%250Ainterpretability.%2520Our%2520code%2520is%2520available%2520at%250Ahttps%253A//github.com/XDZhelheim/HimNet.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2405.10800v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Heterogeneity-Informed%20Meta-Parameter%20Learning%20for%20Spatiotemporal%20Time%0A%20%20Series%20Forecasting&entry.906535625=Zheng%20Dong%20and%20Renhe%20Jiang%20and%20Haotian%20Gao%20and%20Hangchen%20Liu%20and%20Jinliang%20Deng%20and%20Qingsong%20Wen%20and%20Xuan%20Song&entry.1292438233=%20%20Spatiotemporal%20time%20series%20forecasting%20plays%20a%20key%20role%20in%20a%20wide%20range%20of%0Areal-world%20applications.%20While%20significant%20progress%20has%20been%20made%20in%20this%20area%2C%0Afully%20capturing%20and%20leveraging%20spatiotemporal%20heterogeneity%20remains%20a%0Afundamental%20challenge.%20Therefore%2C%20we%20propose%20a%20novel%20Heterogeneity-Informed%0AMeta-Parameter%20Learning%20scheme.%20Specifically%2C%20our%20approach%20implicitly%20captures%0Aspatiotemporal%20heterogeneity%20through%20learning%20spatial%20and%20temporal%20embeddings%2C%0Awhich%20can%20be%20viewed%20as%20a%20clustering%20process.%20Then%2C%20a%20novel%20spatiotemporal%0Ameta-parameter%20learning%20paradigm%20is%20proposed%20to%20learn%20spatiotemporal-specific%0Aparameters%20from%20meta-parameter%20pools%2C%20which%20is%20informed%20by%20the%20captured%0Aheterogeneity.%20Based%20on%20these%20ideas%2C%20we%20develop%20a%20Heterogeneity-Informed%0ASpatiotemporal%20Meta-Network%20%28HimNet%29%20for%20spatiotemporal%20time%20series%0Aforecasting.%20Extensive%20experiments%20on%20five%20widely-used%20benchmarks%20demonstrate%0Aour%20method%20achieves%20state-of-the-art%20performance%20while%20exhibiting%20superior%0Ainterpretability.%20Our%20code%20is%20available%20at%0Ahttps%3A//github.com/XDZhelheim/HimNet.%0A&entry.1838667208=http%3A//arxiv.org/abs/2405.10800v2&entry.124074799=Read"},
{"title": "On the Convergence of Gradient Descent for Large Learning Rates", "author": "Alexandru Cr\u0103ciun and Debarghya Ghoshdastidar", "abstract": "  A vast literature on convergence guarantees for gradient descent and derived\nmethods exists at the moment. However, a simple practical situation remains\nunexplored: when a fixed step size is used, can we expect gradient descent to\nconverge starting from any initialization? We provide fundamental impossibility\nresults showing that convergence becomes impossible no matter the\ninitialization if the step size gets too big. Looking at the asymptotic value\nof the gradient norm along the optimization trajectory, we see that there is a\nphase transition as the step size crosses a critical value. This has been\nobserved by practitioners, yet the true mechanisms through which this happens\nremain unclear beyond heuristics. Using results from dynamical systems theory,\nwe provide a proof of this in the case of linear neural networks with a squared\nloss. We also prove the impossibility of convergence for more general losses\nwithout requiring strong assumptions such as Lipschitz continuity for the\ngradient. We validate our findings through experiments with non-linear\nnetworks.\n", "link": "http://arxiv.org/abs/2402.13108v2", "date": "2024-09-03", "relevancy": 1.8244, "topK": [{"title": "ToDo: Token Downsampling for Efficient Generation of High-Resolution\n  Images", "link": "http://arxiv.org/abs/2402.13573v2", "similarity": 0.4581}, {"title": "Boosted dynamic neural networks", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/26302", "similarity": 0.4571}, {"title": "Scaling Rectified Flow Transformers for High-Resolution Image Synthesis", "link": "http://arxiv.org/abs/2403.03206v1", "similarity": 0.4485}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20On%20the%20Convergence%20of%20Gradient%20Descent%20for%20Large%20Learning%20Rates&body=Title%3A%20On%20the%20Convergence%20of%20Gradient%20Descent%20for%20Large%20Learning%20Rates%0AAuthor%3A%20Alexandru%20Cr%C4%83ciun%20and%20Debarghya%20Ghoshdastidar%0AAbstract%3A%20%20%20A%20vast%20literature%20on%20convergence%20guarantees%20for%20gradient%20descent%20and%20derived%0Amethods%20exists%20at%20the%20moment.%20However%2C%20a%20simple%20practical%20situation%20remains%0Aunexplored%3A%20when%20a%20fixed%20step%20size%20is%20used%2C%20can%20we%20expect%20gradient%20descent%20to%0Aconverge%20starting%20from%20any%20initialization%3F%20We%20provide%20fundamental%20impossibility%0Aresults%20showing%20that%20convergence%20becomes%20impossible%20no%20matter%20the%0Ainitialization%20if%20the%20step%20size%20gets%20too%20big.%20Looking%20at%20the%20asymptotic%20value%0Aof%20the%20gradient%20norm%20along%20the%20optimization%20trajectory%2C%20we%20see%20that%20there%20is%20a%0Aphase%20transition%20as%20the%20step%20size%20crosses%20a%20critical%20value.%20This%20has%20been%0Aobserved%20by%20practitioners%2C%20yet%20the%20true%20mechanisms%20through%20which%20this%20happens%0Aremain%20unclear%20beyond%20heuristics.%20Using%20results%20from%20dynamical%20systems%20theory%2C%0Awe%20provide%20a%20proof%20of%20this%20in%20the%20case%20of%20linear%20neural%20networks%20with%20a%20squared%0Aloss.%20We%20also%20prove%20the%20impossibility%20of%20convergence%20for%20more%20general%20losses%0Awithout%20requiring%20strong%20assumptions%20such%20as%20Lipschitz%20continuity%20for%20the%0Agradient.%20We%20validate%20our%20findings%20through%20experiments%20with%20non-linear%0Anetworks.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2402.13108v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DOn%2520the%2520Convergence%2520of%2520Gradient%2520Descent%2520for%2520Large%2520Learning%2520Rates%26entry.906535625%3DAlexandru%2520Cr%25C4%2583ciun%2520and%2520Debarghya%2520Ghoshdastidar%26entry.1292438233%3D%2520%2520A%2520vast%2520literature%2520on%2520convergence%2520guarantees%2520for%2520gradient%2520descent%2520and%2520derived%250Amethods%2520exists%2520at%2520the%2520moment.%2520However%252C%2520a%2520simple%2520practical%2520situation%2520remains%250Aunexplored%253A%2520when%2520a%2520fixed%2520step%2520size%2520is%2520used%252C%2520can%2520we%2520expect%2520gradient%2520descent%2520to%250Aconverge%2520starting%2520from%2520any%2520initialization%253F%2520We%2520provide%2520fundamental%2520impossibility%250Aresults%2520showing%2520that%2520convergence%2520becomes%2520impossible%2520no%2520matter%2520the%250Ainitialization%2520if%2520the%2520step%2520size%2520gets%2520too%2520big.%2520Looking%2520at%2520the%2520asymptotic%2520value%250Aof%2520the%2520gradient%2520norm%2520along%2520the%2520optimization%2520trajectory%252C%2520we%2520see%2520that%2520there%2520is%2520a%250Aphase%2520transition%2520as%2520the%2520step%2520size%2520crosses%2520a%2520critical%2520value.%2520This%2520has%2520been%250Aobserved%2520by%2520practitioners%252C%2520yet%2520the%2520true%2520mechanisms%2520through%2520which%2520this%2520happens%250Aremain%2520unclear%2520beyond%2520heuristics.%2520Using%2520results%2520from%2520dynamical%2520systems%2520theory%252C%250Awe%2520provide%2520a%2520proof%2520of%2520this%2520in%2520the%2520case%2520of%2520linear%2520neural%2520networks%2520with%2520a%2520squared%250Aloss.%2520We%2520also%2520prove%2520the%2520impossibility%2520of%2520convergence%2520for%2520more%2520general%2520losses%250Awithout%2520requiring%2520strong%2520assumptions%2520such%2520as%2520Lipschitz%2520continuity%2520for%2520the%250Agradient.%2520We%2520validate%2520our%2520findings%2520through%2520experiments%2520with%2520non-linear%250Anetworks.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2402.13108v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=On%20the%20Convergence%20of%20Gradient%20Descent%20for%20Large%20Learning%20Rates&entry.906535625=Alexandru%20Cr%C4%83ciun%20and%20Debarghya%20Ghoshdastidar&entry.1292438233=%20%20A%20vast%20literature%20on%20convergence%20guarantees%20for%20gradient%20descent%20and%20derived%0Amethods%20exists%20at%20the%20moment.%20However%2C%20a%20simple%20practical%20situation%20remains%0Aunexplored%3A%20when%20a%20fixed%20step%20size%20is%20used%2C%20can%20we%20expect%20gradient%20descent%20to%0Aconverge%20starting%20from%20any%20initialization%3F%20We%20provide%20fundamental%20impossibility%0Aresults%20showing%20that%20convergence%20becomes%20impossible%20no%20matter%20the%0Ainitialization%20if%20the%20step%20size%20gets%20too%20big.%20Looking%20at%20the%20asymptotic%20value%0Aof%20the%20gradient%20norm%20along%20the%20optimization%20trajectory%2C%20we%20see%20that%20there%20is%20a%0Aphase%20transition%20as%20the%20step%20size%20crosses%20a%20critical%20value.%20This%20has%20been%0Aobserved%20by%20practitioners%2C%20yet%20the%20true%20mechanisms%20through%20which%20this%20happens%0Aremain%20unclear%20beyond%20heuristics.%20Using%20results%20from%20dynamical%20systems%20theory%2C%0Awe%20provide%20a%20proof%20of%20this%20in%20the%20case%20of%20linear%20neural%20networks%20with%20a%20squared%0Aloss.%20We%20also%20prove%20the%20impossibility%20of%20convergence%20for%20more%20general%20losses%0Awithout%20requiring%20strong%20assumptions%20such%20as%20Lipschitz%20continuity%20for%20the%0Agradient.%20We%20validate%20our%20findings%20through%20experiments%20with%20non-linear%0Anetworks.%0A&entry.1838667208=http%3A//arxiv.org/abs/2402.13108v2&entry.124074799=Read"},
{"title": "Force-Guided Bridge Matching for Full-Atom Time-Coarsened Dynamics of\n  Peptides", "author": "Ziyang Yu and Wenbing Huang and Yang Liu", "abstract": "  Molecular Dynamics (MD) simulations are irreplaceable and ubiquitous in\nfields of materials science, chemistry, pharmacology just to name a few.\nConventional MD simulations are plagued by numerical stability as well as long\nequilibration time issues, which limits broader applications of MD simulations.\nRecently, a surge of deep learning approaches have been devised for\ntime-coarsened dynamics, which learns the state transition mechanism over much\nlarger time scales to overcome these limitations. However, only a few methods\ntarget the underlying Boltzmann distribution by resampling techniques, where\nproposals are rarely accepted as new states with low efficiency. In this work,\nwe propose a force-guided bridge matching model, FBM, a novel framework that\nfirst incorporates physical priors into bridge matching for full-atom\ntime-coarsened dynamics. With the guidance of our well-designed intermediate\nforce field, FBM is feasible to target the Boltzmann-like distribution by\ndirect inference without extra steps. Experiments on small peptides verify our\nsuperiority in terms of comprehensive metrics and demonstrate transferability\nto unseen peptide systems.\n", "link": "http://arxiv.org/abs/2408.15126v3", "date": "2024-09-03", "relevancy": 1.8125, "topK": [{"title": "Dynamic Gaussians Mesh: Consistent Mesh Reconstruction from Monocular\n  Videos", "link": "http://arxiv.org/abs/2404.12379v2", "similarity": 0.4752}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.4527}, {"title": "DDM-NET: End-to-end learning of keypoint feature Detection, Description and Matching for 3D localization", "link": "https://arxiv.org/abs/2212.04575", "similarity": 0.4312}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Force-Guided%20Bridge%20Matching%20for%20Full-Atom%20Time-Coarsened%20Dynamics%20of%0A%20%20Peptides&body=Title%3A%20Force-Guided%20Bridge%20Matching%20for%20Full-Atom%20Time-Coarsened%20Dynamics%20of%0A%20%20Peptides%0AAuthor%3A%20Ziyang%20Yu%20and%20Wenbing%20Huang%20and%20Yang%20Liu%0AAbstract%3A%20%20%20Molecular%20Dynamics%20%28MD%29%20simulations%20are%20irreplaceable%20and%20ubiquitous%20in%0Afields%20of%20materials%20science%2C%20chemistry%2C%20pharmacology%20just%20to%20name%20a%20few.%0AConventional%20MD%20simulations%20are%20plagued%20by%20numerical%20stability%20as%20well%20as%20long%0Aequilibration%20time%20issues%2C%20which%20limits%20broader%20applications%20of%20MD%20simulations.%0ARecently%2C%20a%20surge%20of%20deep%20learning%20approaches%20have%20been%20devised%20for%0Atime-coarsened%20dynamics%2C%20which%20learns%20the%20state%20transition%20mechanism%20over%20much%0Alarger%20time%20scales%20to%20overcome%20these%20limitations.%20However%2C%20only%20a%20few%20methods%0Atarget%20the%20underlying%20Boltzmann%20distribution%20by%20resampling%20techniques%2C%20where%0Aproposals%20are%20rarely%20accepted%20as%20new%20states%20with%20low%20efficiency.%20In%20this%20work%2C%0Awe%20propose%20a%20force-guided%20bridge%20matching%20model%2C%20FBM%2C%20a%20novel%20framework%20that%0Afirst%20incorporates%20physical%20priors%20into%20bridge%20matching%20for%20full-atom%0Atime-coarsened%20dynamics.%20With%20the%20guidance%20of%20our%20well-designed%20intermediate%0Aforce%20field%2C%20FBM%20is%20feasible%20to%20target%20the%20Boltzmann-like%20distribution%20by%0Adirect%20inference%20without%20extra%20steps.%20Experiments%20on%20small%20peptides%20verify%20our%0Asuperiority%20in%20terms%20of%20comprehensive%20metrics%20and%20demonstrate%20transferability%0Ato%20unseen%20peptide%20systems.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2408.15126v3%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DForce-Guided%2520Bridge%2520Matching%2520for%2520Full-Atom%2520Time-Coarsened%2520Dynamics%2520of%250A%2520%2520Peptides%26entry.906535625%3DZiyang%2520Yu%2520and%2520Wenbing%2520Huang%2520and%2520Yang%2520Liu%26entry.1292438233%3D%2520%2520Molecular%2520Dynamics%2520%2528MD%2529%2520simulations%2520are%2520irreplaceable%2520and%2520ubiquitous%2520in%250Afields%2520of%2520materials%2520science%252C%2520chemistry%252C%2520pharmacology%2520just%2520to%2520name%2520a%2520few.%250AConventional%2520MD%2520simulations%2520are%2520plagued%2520by%2520numerical%2520stability%2520as%2520well%2520as%2520long%250Aequilibration%2520time%2520issues%252C%2520which%2520limits%2520broader%2520applications%2520of%2520MD%2520simulations.%250ARecently%252C%2520a%2520surge%2520of%2520deep%2520learning%2520approaches%2520have%2520been%2520devised%2520for%250Atime-coarsened%2520dynamics%252C%2520which%2520learns%2520the%2520state%2520transition%2520mechanism%2520over%2520much%250Alarger%2520time%2520scales%2520to%2520overcome%2520these%2520limitations.%2520However%252C%2520only%2520a%2520few%2520methods%250Atarget%2520the%2520underlying%2520Boltzmann%2520distribution%2520by%2520resampling%2520techniques%252C%2520where%250Aproposals%2520are%2520rarely%2520accepted%2520as%2520new%2520states%2520with%2520low%2520efficiency.%2520In%2520this%2520work%252C%250Awe%2520propose%2520a%2520force-guided%2520bridge%2520matching%2520model%252C%2520FBM%252C%2520a%2520novel%2520framework%2520that%250Afirst%2520incorporates%2520physical%2520priors%2520into%2520bridge%2520matching%2520for%2520full-atom%250Atime-coarsened%2520dynamics.%2520With%2520the%2520guidance%2520of%2520our%2520well-designed%2520intermediate%250Aforce%2520field%252C%2520FBM%2520is%2520feasible%2520to%2520target%2520the%2520Boltzmann-like%2520distribution%2520by%250Adirect%2520inference%2520without%2520extra%2520steps.%2520Experiments%2520on%2520small%2520peptides%2520verify%2520our%250Asuperiority%2520in%2520terms%2520of%2520comprehensive%2520metrics%2520and%2520demonstrate%2520transferability%250Ato%2520unseen%2520peptide%2520systems.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2408.15126v3%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Force-Guided%20Bridge%20Matching%20for%20Full-Atom%20Time-Coarsened%20Dynamics%20of%0A%20%20Peptides&entry.906535625=Ziyang%20Yu%20and%20Wenbing%20Huang%20and%20Yang%20Liu&entry.1292438233=%20%20Molecular%20Dynamics%20%28MD%29%20simulations%20are%20irreplaceable%20and%20ubiquitous%20in%0Afields%20of%20materials%20science%2C%20chemistry%2C%20pharmacology%20just%20to%20name%20a%20few.%0AConventional%20MD%20simulations%20are%20plagued%20by%20numerical%20stability%20as%20well%20as%20long%0Aequilibration%20time%20issues%2C%20which%20limits%20broader%20applications%20of%20MD%20simulations.%0ARecently%2C%20a%20surge%20of%20deep%20learning%20approaches%20have%20been%20devised%20for%0Atime-coarsened%20dynamics%2C%20which%20learns%20the%20state%20transition%20mechanism%20over%20much%0Alarger%20time%20scales%20to%20overcome%20these%20limitations.%20However%2C%20only%20a%20few%20methods%0Atarget%20the%20underlying%20Boltzmann%20distribution%20by%20resampling%20techniques%2C%20where%0Aproposals%20are%20rarely%20accepted%20as%20new%20states%20with%20low%20efficiency.%20In%20this%20work%2C%0Awe%20propose%20a%20force-guided%20bridge%20matching%20model%2C%20FBM%2C%20a%20novel%20framework%20that%0Afirst%20incorporates%20physical%20priors%20into%20bridge%20matching%20for%20full-atom%0Atime-coarsened%20dynamics.%20With%20the%20guidance%20of%20our%20well-designed%20intermediate%0Aforce%20field%2C%20FBM%20is%20feasible%20to%20target%20the%20Boltzmann-like%20distribution%20by%0Adirect%20inference%20without%20extra%20steps.%20Experiments%20on%20small%20peptides%20verify%20our%0Asuperiority%20in%20terms%20of%20comprehensive%20metrics%20and%20demonstrate%20transferability%0Ato%20unseen%20peptide%20systems.%0A&entry.1838667208=http%3A//arxiv.org/abs/2408.15126v3&entry.124074799=Read"},
{"title": "Different Victims, Same Layout: Email Visual Similarity Detection for\n  Enhanced Email Protection", "author": "Sachin Shukla and Omid Mirzaei", "abstract": "  In the pursuit of an effective spam detection system, the focus has often\nbeen on identifying known spam patterns either through rule-based detection\nsystems or machine learning (ML) solutions that rely on keywords. However, both\nsystems are susceptible to evasion techniques and zero-day attacks that can be\nachieved at low cost. Therefore, an email that bypassed the defense system once\ncan do it again in the following days, even though rules are updated or the ML\nmodels are retrained. The recurrence of failures to detect emails that exhibit\nlayout similarities to previously undetected spam is concerning for customers\nand can erode their trust in a company. Our observations show that threat\nactors reuse email kits extensively and can bypass detection with little\neffort, for example, by making changes to the content of emails. In this work,\nwe propose an email visual similarity detection approach, named Pisco, to\nimprove the detection capabilities of an email threat defense system. We apply\nour proof of concept to some real-world samples received from different\nsources. Our results show that email kits are being reused extensively and\nvisually similar emails are sent to our customers at various time intervals.\nTherefore, this method could be very helpful in situations where detection\nfeatures that rely on textual features and keywords are bypassed, an occurrence\nour observations show happens frequently.\n", "link": "http://arxiv.org/abs/2408.16945v2", "date": "2024-09-03", "relevancy": 1.7764, "topK": [{"title": "Self-supervised Photographic Image Layout Representation Learning", "link": "http://arxiv.org/abs/2403.03740v1", "similarity": 0.4666}, {"title": "Breadcrumbs: Adversarial class-balanced sampling for long-tailed recognition", "link": "https://link.springer.com/chapter/10.1007/978-3-031-20053-3_37", "similarity": 0.4349}, {"title": "Multimodal-Conditioned Latent Diffusion Models for Fashion Image Editing", "link": "http://arxiv.org/abs/2403.14828v2", "similarity": 0.4108}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Different%20Victims%2C%20Same%20Layout%3A%20Email%20Visual%20Similarity%20Detection%20for%0A%20%20Enhanced%20Email%20Protection&body=Title%3A%20Different%20Victims%2C%20Same%20Layout%3A%20Email%20Visual%20Similarity%20Detection%20for%0A%20%20Enhanced%20Email%20Protection%0AAuthor%3A%20Sachin%20Shukla%20and%20Omid%20Mirzaei%0AAbstract%3A%20%20%20In%20the%20pursuit%20of%20an%20effective%20spam%20detection%20system%2C%20the%20focus%20has%20often%0Abeen%20on%20identifying%20known%20spam%20patterns%20either%20through%20rule-based%20detection%0Asystems%20or%20machine%20learning%20%28ML%29%20solutions%20that%20rely%20on%20keywords.%20However%2C%20both%0Asystems%20are%20susceptible%20to%20evasion%20techniques%20and%20zero-day%20attacks%20that%20can%20be%0Aachieved%20at%20low%20cost.%20Therefore%2C%20an%20email%20that%20bypassed%20the%20defense%20system%20once%0Acan%20do%20it%20again%20in%20the%20following%20days%2C%20even%20though%20rules%20are%20updated%20or%20the%20ML%0Amodels%20are%20retrained.%20The%20recurrence%20of%20failures%20to%20detect%20emails%20that%20exhibit%0Alayout%20similarities%20to%20previously%20undetected%20spam%20is%20concerning%20for%20customers%0Aand%20can%20erode%20their%20trust%20in%20a%20company.%20Our%20observations%20show%20that%20threat%0Aactors%20reuse%20email%20kits%20extensively%20and%20can%20bypass%20detection%20with%20little%0Aeffort%2C%20for%20example%2C%20by%20making%20changes%20to%20the%20content%20of%20emails.%20In%20this%20work%2C%0Awe%20propose%20an%20email%20visual%20similarity%20detection%20approach%2C%20named%20Pisco%2C%20to%0Aimprove%20the%20detection%20capabilities%20of%20an%20email%20threat%20defense%20system.%20We%20apply%0Aour%20proof%20of%20concept%20to%20some%20real-world%20samples%20received%20from%20different%0Asources.%20Our%20results%20show%20that%20email%20kits%20are%20being%20reused%20extensively%20and%0Avisually%20similar%20emails%20are%20sent%20to%20our%20customers%20at%20various%20time%20intervals.%0ATherefore%2C%20this%20method%20could%20be%20very%20helpful%20in%20situations%20where%20detection%0Afeatures%20that%20rely%20on%20textual%20features%20and%20keywords%20are%20bypassed%2C%20an%20occurrence%0Aour%20observations%20show%20happens%20frequently.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2408.16945v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DDifferent%2520Victims%252C%2520Same%2520Layout%253A%2520Email%2520Visual%2520Similarity%2520Detection%2520for%250A%2520%2520Enhanced%2520Email%2520Protection%26entry.906535625%3DSachin%2520Shukla%2520and%2520Omid%2520Mirzaei%26entry.1292438233%3D%2520%2520In%2520the%2520pursuit%2520of%2520an%2520effective%2520spam%2520detection%2520system%252C%2520the%2520focus%2520has%2520often%250Abeen%2520on%2520identifying%2520known%2520spam%2520patterns%2520either%2520through%2520rule-based%2520detection%250Asystems%2520or%2520machine%2520learning%2520%2528ML%2529%2520solutions%2520that%2520rely%2520on%2520keywords.%2520However%252C%2520both%250Asystems%2520are%2520susceptible%2520to%2520evasion%2520techniques%2520and%2520zero-day%2520attacks%2520that%2520can%2520be%250Aachieved%2520at%2520low%2520cost.%2520Therefore%252C%2520an%2520email%2520that%2520bypassed%2520the%2520defense%2520system%2520once%250Acan%2520do%2520it%2520again%2520in%2520the%2520following%2520days%252C%2520even%2520though%2520rules%2520are%2520updated%2520or%2520the%2520ML%250Amodels%2520are%2520retrained.%2520The%2520recurrence%2520of%2520failures%2520to%2520detect%2520emails%2520that%2520exhibit%250Alayout%2520similarities%2520to%2520previously%2520undetected%2520spam%2520is%2520concerning%2520for%2520customers%250Aand%2520can%2520erode%2520their%2520trust%2520in%2520a%2520company.%2520Our%2520observations%2520show%2520that%2520threat%250Aactors%2520reuse%2520email%2520kits%2520extensively%2520and%2520can%2520bypass%2520detection%2520with%2520little%250Aeffort%252C%2520for%2520example%252C%2520by%2520making%2520changes%2520to%2520the%2520content%2520of%2520emails.%2520In%2520this%2520work%252C%250Awe%2520propose%2520an%2520email%2520visual%2520similarity%2520detection%2520approach%252C%2520named%2520Pisco%252C%2520to%250Aimprove%2520the%2520detection%2520capabilities%2520of%2520an%2520email%2520threat%2520defense%2520system.%2520We%2520apply%250Aour%2520proof%2520of%2520concept%2520to%2520some%2520real-world%2520samples%2520received%2520from%2520different%250Asources.%2520Our%2520results%2520show%2520that%2520email%2520kits%2520are%2520being%2520reused%2520extensively%2520and%250Avisually%2520similar%2520emails%2520are%2520sent%2520to%2520our%2520customers%2520at%2520various%2520time%2520intervals.%250ATherefore%252C%2520this%2520method%2520could%2520be%2520very%2520helpful%2520in%2520situations%2520where%2520detection%250Afeatures%2520that%2520rely%2520on%2520textual%2520features%2520and%2520keywords%2520are%2520bypassed%252C%2520an%2520occurrence%250Aour%2520observations%2520show%2520happens%2520frequently.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2408.16945v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Different%20Victims%2C%20Same%20Layout%3A%20Email%20Visual%20Similarity%20Detection%20for%0A%20%20Enhanced%20Email%20Protection&entry.906535625=Sachin%20Shukla%20and%20Omid%20Mirzaei&entry.1292438233=%20%20In%20the%20pursuit%20of%20an%20effective%20spam%20detection%20system%2C%20the%20focus%20has%20often%0Abeen%20on%20identifying%20known%20spam%20patterns%20either%20through%20rule-based%20detection%0Asystems%20or%20machine%20learning%20%28ML%29%20solutions%20that%20rely%20on%20keywords.%20However%2C%20both%0Asystems%20are%20susceptible%20to%20evasion%20techniques%20and%20zero-day%20attacks%20that%20can%20be%0Aachieved%20at%20low%20cost.%20Therefore%2C%20an%20email%20that%20bypassed%20the%20defense%20system%20once%0Acan%20do%20it%20again%20in%20the%20following%20days%2C%20even%20though%20rules%20are%20updated%20or%20the%20ML%0Amodels%20are%20retrained.%20The%20recurrence%20of%20failures%20to%20detect%20emails%20that%20exhibit%0Alayout%20similarities%20to%20previously%20undetected%20spam%20is%20concerning%20for%20customers%0Aand%20can%20erode%20their%20trust%20in%20a%20company.%20Our%20observations%20show%20that%20threat%0Aactors%20reuse%20email%20kits%20extensively%20and%20can%20bypass%20detection%20with%20little%0Aeffort%2C%20for%20example%2C%20by%20making%20changes%20to%20the%20content%20of%20emails.%20In%20this%20work%2C%0Awe%20propose%20an%20email%20visual%20similarity%20detection%20approach%2C%20named%20Pisco%2C%20to%0Aimprove%20the%20detection%20capabilities%20of%20an%20email%20threat%20defense%20system.%20We%20apply%0Aour%20proof%20of%20concept%20to%20some%20real-world%20samples%20received%20from%20different%0Asources.%20Our%20results%20show%20that%20email%20kits%20are%20being%20reused%20extensively%20and%0Avisually%20similar%20emails%20are%20sent%20to%20our%20customers%20at%20various%20time%20intervals.%0ATherefore%2C%20this%20method%20could%20be%20very%20helpful%20in%20situations%20where%20detection%0Afeatures%20that%20rely%20on%20textual%20features%20and%20keywords%20are%20bypassed%2C%20an%20occurrence%0Aour%20observations%20show%20happens%20frequently.%0A&entry.1838667208=http%3A//arxiv.org/abs/2408.16945v2&entry.124074799=Read"},
{"title": "On the Federated Learning Framework for Cooperative Perception", "author": "Zhenrong Zhang and Jianan Liu and Xi Zhou and Tao Huang and Qing-Long Han and Jingxin Liu and Hongbin Liu", "abstract": "  Cooperative perception is essential to enhance the efficiency and safety of\nfuture transportation systems, requiring extensive data sharing among vehicles\non the road, which raises significant privacy concerns. Federated learning\noffers a promising solution by enabling data privacy-preserving collaborative\nenhancements in perception, decision-making, and planning among connected and\nautonomous vehicles (CAVs). However, federated learning is impeded by\nsignificant challenges arising from data heterogeneity across diverse clients,\npotentially diminishing model accuracy and prolonging convergence periods. This\nstudy introduces a specialized federated learning framework for CP, termed the\nfederated dynamic weighted aggregation (FedDWA) algorithm, facilitated by\ndynamic adjusting loss (DALoss) function. This framework employs dynamic client\nweighting to direct model convergence and integrates a novel loss function that\nutilizes Kullback-Leibler divergence (KLD) to counteract the detrimental\neffects of non-independently and identically distributed (Non-IID) and\nunbalanced data. Utilizing the BEV transformer as the primary model, our\nrigorous testing on the OpenV2V dataset, augmented with FedBEVT data,\ndemonstrates significant improvements in the average intersection over union\n(IoU). These results highlight the substantial potential of our federated\nlearning framework to address data heterogeneity challenges in CP, thereby\nenhancing the accuracy of environmental perception models and facilitating more\nrobust and efficient collaborative learning solutions in the transportation\nsector.\n", "link": "http://arxiv.org/abs/2404.17147v4", "date": "2024-09-03", "relevancy": 1.7265, "topK": [{"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.5959}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.5517}, {"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.5484}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20On%20the%20Federated%20Learning%20Framework%20for%20Cooperative%20Perception&body=Title%3A%20On%20the%20Federated%20Learning%20Framework%20for%20Cooperative%20Perception%0AAuthor%3A%20Zhenrong%20Zhang%20and%20Jianan%20Liu%20and%20Xi%20Zhou%20and%20Tao%20Huang%20and%20Qing-Long%20Han%20and%20Jingxin%20Liu%20and%20Hongbin%20Liu%0AAbstract%3A%20%20%20Cooperative%20perception%20is%20essential%20to%20enhance%20the%20efficiency%20and%20safety%20of%0Afuture%20transportation%20systems%2C%20requiring%20extensive%20data%20sharing%20among%20vehicles%0Aon%20the%20road%2C%20which%20raises%20significant%20privacy%20concerns.%20Federated%20learning%0Aoffers%20a%20promising%20solution%20by%20enabling%20data%20privacy-preserving%20collaborative%0Aenhancements%20in%20perception%2C%20decision-making%2C%20and%20planning%20among%20connected%20and%0Aautonomous%20vehicles%20%28CAVs%29.%20However%2C%20federated%20learning%20is%20impeded%20by%0Asignificant%20challenges%20arising%20from%20data%20heterogeneity%20across%20diverse%20clients%2C%0Apotentially%20diminishing%20model%20accuracy%20and%20prolonging%20convergence%20periods.%20This%0Astudy%20introduces%20a%20specialized%20federated%20learning%20framework%20for%20CP%2C%20termed%20the%0Afederated%20dynamic%20weighted%20aggregation%20%28FedDWA%29%20algorithm%2C%20facilitated%20by%0Adynamic%20adjusting%20loss%20%28DALoss%29%20function.%20This%20framework%20employs%20dynamic%20client%0Aweighting%20to%20direct%20model%20convergence%20and%20integrates%20a%20novel%20loss%20function%20that%0Autilizes%20Kullback-Leibler%20divergence%20%28KLD%29%20to%20counteract%20the%20detrimental%0Aeffects%20of%20non-independently%20and%20identically%20distributed%20%28Non-IID%29%20and%0Aunbalanced%20data.%20Utilizing%20the%20BEV%20transformer%20as%20the%20primary%20model%2C%20our%0Arigorous%20testing%20on%20the%20OpenV2V%20dataset%2C%20augmented%20with%20FedBEVT%20data%2C%0Ademonstrates%20significant%20improvements%20in%20the%20average%20intersection%20over%20union%0A%28IoU%29.%20These%20results%20highlight%20the%20substantial%20potential%20of%20our%20federated%0Alearning%20framework%20to%20address%20data%20heterogeneity%20challenges%20in%20CP%2C%20thereby%0Aenhancing%20the%20accuracy%20of%20environmental%20perception%20models%20and%20facilitating%20more%0Arobust%20and%20efficient%20collaborative%20learning%20solutions%20in%20the%20transportation%0Asector.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2404.17147v4%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DOn%2520the%2520Federated%2520Learning%2520Framework%2520for%2520Cooperative%2520Perception%26entry.906535625%3DZhenrong%2520Zhang%2520and%2520Jianan%2520Liu%2520and%2520Xi%2520Zhou%2520and%2520Tao%2520Huang%2520and%2520Qing-Long%2520Han%2520and%2520Jingxin%2520Liu%2520and%2520Hongbin%2520Liu%26entry.1292438233%3D%2520%2520Cooperative%2520perception%2520is%2520essential%2520to%2520enhance%2520the%2520efficiency%2520and%2520safety%2520of%250Afuture%2520transportation%2520systems%252C%2520requiring%2520extensive%2520data%2520sharing%2520among%2520vehicles%250Aon%2520the%2520road%252C%2520which%2520raises%2520significant%2520privacy%2520concerns.%2520Federated%2520learning%250Aoffers%2520a%2520promising%2520solution%2520by%2520enabling%2520data%2520privacy-preserving%2520collaborative%250Aenhancements%2520in%2520perception%252C%2520decision-making%252C%2520and%2520planning%2520among%2520connected%2520and%250Aautonomous%2520vehicles%2520%2528CAVs%2529.%2520However%252C%2520federated%2520learning%2520is%2520impeded%2520by%250Asignificant%2520challenges%2520arising%2520from%2520data%2520heterogeneity%2520across%2520diverse%2520clients%252C%250Apotentially%2520diminishing%2520model%2520accuracy%2520and%2520prolonging%2520convergence%2520periods.%2520This%250Astudy%2520introduces%2520a%2520specialized%2520federated%2520learning%2520framework%2520for%2520CP%252C%2520termed%2520the%250Afederated%2520dynamic%2520weighted%2520aggregation%2520%2528FedDWA%2529%2520algorithm%252C%2520facilitated%2520by%250Adynamic%2520adjusting%2520loss%2520%2528DALoss%2529%2520function.%2520This%2520framework%2520employs%2520dynamic%2520client%250Aweighting%2520to%2520direct%2520model%2520convergence%2520and%2520integrates%2520a%2520novel%2520loss%2520function%2520that%250Autilizes%2520Kullback-Leibler%2520divergence%2520%2528KLD%2529%2520to%2520counteract%2520the%2520detrimental%250Aeffects%2520of%2520non-independently%2520and%2520identically%2520distributed%2520%2528Non-IID%2529%2520and%250Aunbalanced%2520data.%2520Utilizing%2520the%2520BEV%2520transformer%2520as%2520the%2520primary%2520model%252C%2520our%250Arigorous%2520testing%2520on%2520the%2520OpenV2V%2520dataset%252C%2520augmented%2520with%2520FedBEVT%2520data%252C%250Ademonstrates%2520significant%2520improvements%2520in%2520the%2520average%2520intersection%2520over%2520union%250A%2528IoU%2529.%2520These%2520results%2520highlight%2520the%2520substantial%2520potential%2520of%2520our%2520federated%250Alearning%2520framework%2520to%2520address%2520data%2520heterogeneity%2520challenges%2520in%2520CP%252C%2520thereby%250Aenhancing%2520the%2520accuracy%2520of%2520environmental%2520perception%2520models%2520and%2520facilitating%2520more%250Arobust%2520and%2520efficient%2520collaborative%2520learning%2520solutions%2520in%2520the%2520transportation%250Asector.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2404.17147v4%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=On%20the%20Federated%20Learning%20Framework%20for%20Cooperative%20Perception&entry.906535625=Zhenrong%20Zhang%20and%20Jianan%20Liu%20and%20Xi%20Zhou%20and%20Tao%20Huang%20and%20Qing-Long%20Han%20and%20Jingxin%20Liu%20and%20Hongbin%20Liu&entry.1292438233=%20%20Cooperative%20perception%20is%20essential%20to%20enhance%20the%20efficiency%20and%20safety%20of%0Afuture%20transportation%20systems%2C%20requiring%20extensive%20data%20sharing%20among%20vehicles%0Aon%20the%20road%2C%20which%20raises%20significant%20privacy%20concerns.%20Federated%20learning%0Aoffers%20a%20promising%20solution%20by%20enabling%20data%20privacy-preserving%20collaborative%0Aenhancements%20in%20perception%2C%20decision-making%2C%20and%20planning%20among%20connected%20and%0Aautonomous%20vehicles%20%28CAVs%29.%20However%2C%20federated%20learning%20is%20impeded%20by%0Asignificant%20challenges%20arising%20from%20data%20heterogeneity%20across%20diverse%20clients%2C%0Apotentially%20diminishing%20model%20accuracy%20and%20prolonging%20convergence%20periods.%20This%0Astudy%20introduces%20a%20specialized%20federated%20learning%20framework%20for%20CP%2C%20termed%20the%0Afederated%20dynamic%20weighted%20aggregation%20%28FedDWA%29%20algorithm%2C%20facilitated%20by%0Adynamic%20adjusting%20loss%20%28DALoss%29%20function.%20This%20framework%20employs%20dynamic%20client%0Aweighting%20to%20direct%20model%20convergence%20and%20integrates%20a%20novel%20loss%20function%20that%0Autilizes%20Kullback-Leibler%20divergence%20%28KLD%29%20to%20counteract%20the%20detrimental%0Aeffects%20of%20non-independently%20and%20identically%20distributed%20%28Non-IID%29%20and%0Aunbalanced%20data.%20Utilizing%20the%20BEV%20transformer%20as%20the%20primary%20model%2C%20our%0Arigorous%20testing%20on%20the%20OpenV2V%20dataset%2C%20augmented%20with%20FedBEVT%20data%2C%0Ademonstrates%20significant%20improvements%20in%20the%20average%20intersection%20over%20union%0A%28IoU%29.%20These%20results%20highlight%20the%20substantial%20potential%20of%20our%20federated%0Alearning%20framework%20to%20address%20data%20heterogeneity%20challenges%20in%20CP%2C%20thereby%0Aenhancing%20the%20accuracy%20of%20environmental%20perception%20models%20and%20facilitating%20more%0Arobust%20and%20efficient%20collaborative%20learning%20solutions%20in%20the%20transportation%0Asector.%0A&entry.1838667208=http%3A//arxiv.org/abs/2404.17147v4&entry.124074799=Read"},
{"title": "Restorer: Removing Multi-Degradation with All-Axis Attention and Prompt\n  Guidance", "author": "Jiawei Mao and Juncheng Wu and Yuyin Zhou and Xuesong Yin and Yuanqi Chang", "abstract": "  There are many excellent solutions in image restoration.However, most methods\nrequire on training separate models to restore images with different types of\ndegradation.Although existing all-in-one models effectively address multiple\ntypes of degradation simultaneously, their performance in real-world scenarios\nis still constrained by the task confusion problem.In this work, we attempt to\naddress this issue by introducing \\textbf{Restorer}, a novel Transformer-based\nall-in-one image restoration model.To effectively address the complex\ndegradation present in real-world images, we propose All-Axis Attention (AAA),\na mechanism that simultaneously models long-range dependencies across both\nspatial and channel dimensions, capturing potential correlations along all\naxes.Additionally, we introduce textual prompts in Restorer to incorporate\nexplicit task priors, enabling the removal of specific degradation types based\non user instructions. By iterating over these prompts, Restorer can handle\ncomposite degradation in real-world scenarios without requiring additional\ntraining.Based on these designs, Restorer with one set of parameters\ndemonstrates state-of-the-art performance in multiple image restoration tasks\ncompared to existing all-in-one and even single-task models.Additionally,\nRestorer is efficient during inference, suggesting the potential in real-world\napplications.\n", "link": "http://arxiv.org/abs/2406.12587v2", "date": "2024-09-03", "relevancy": 1.6134, "topK": [{"title": "Scaling Rectified Flow Transformers for High-Resolution Image Synthesis", "link": "http://arxiv.org/abs/2403.03206v1", "similarity": 0.5535}, {"title": "ToDo: Token Downsampling for Efficient Generation of High-Resolution\n  Images", "link": "http://arxiv.org/abs/2402.13573v2", "similarity": 0.5392}, {"title": "SDXS: Real-Time One-Step Latent Diffusion Models with Image Conditions", "link": "http://arxiv.org/abs/2403.16627v1", "similarity": 0.5187}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Restorer%3A%20Removing%20Multi-Degradation%20with%20All-Axis%20Attention%20and%20Prompt%0A%20%20Guidance&body=Title%3A%20Restorer%3A%20Removing%20Multi-Degradation%20with%20All-Axis%20Attention%20and%20Prompt%0A%20%20Guidance%0AAuthor%3A%20Jiawei%20Mao%20and%20Juncheng%20Wu%20and%20Yuyin%20Zhou%20and%20Xuesong%20Yin%20and%20Yuanqi%20Chang%0AAbstract%3A%20%20%20There%20are%20many%20excellent%20solutions%20in%20image%20restoration.However%2C%20most%20methods%0Arequire%20on%20training%20separate%20models%20to%20restore%20images%20with%20different%20types%20of%0Adegradation.Although%20existing%20all-in-one%20models%20effectively%20address%20multiple%0Atypes%20of%20degradation%20simultaneously%2C%20their%20performance%20in%20real-world%20scenarios%0Ais%20still%20constrained%20by%20the%20task%20confusion%20problem.In%20this%20work%2C%20we%20attempt%20to%0Aaddress%20this%20issue%20by%20introducing%20%5Ctextbf%7BRestorer%7D%2C%20a%20novel%20Transformer-based%0Aall-in-one%20image%20restoration%20model.To%20effectively%20address%20the%20complex%0Adegradation%20present%20in%20real-world%20images%2C%20we%20propose%20All-Axis%20Attention%20%28AAA%29%2C%0Aa%20mechanism%20that%20simultaneously%20models%20long-range%20dependencies%20across%20both%0Aspatial%20and%20channel%20dimensions%2C%20capturing%20potential%20correlations%20along%20all%0Aaxes.Additionally%2C%20we%20introduce%20textual%20prompts%20in%20Restorer%20to%20incorporate%0Aexplicit%20task%20priors%2C%20enabling%20the%20removal%20of%20specific%20degradation%20types%20based%0Aon%20user%20instructions.%20By%20iterating%20over%20these%20prompts%2C%20Restorer%20can%20handle%0Acomposite%20degradation%20in%20real-world%20scenarios%20without%20requiring%20additional%0Atraining.Based%20on%20these%20designs%2C%20Restorer%20with%20one%20set%20of%20parameters%0Ademonstrates%20state-of-the-art%20performance%20in%20multiple%20image%20restoration%20tasks%0Acompared%20to%20existing%20all-in-one%20and%20even%20single-task%20models.Additionally%2C%0ARestorer%20is%20efficient%20during%20inference%2C%20suggesting%20the%20potential%20in%20real-world%0Aapplications.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2406.12587v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DRestorer%253A%2520Removing%2520Multi-Degradation%2520with%2520All-Axis%2520Attention%2520and%2520Prompt%250A%2520%2520Guidance%26entry.906535625%3DJiawei%2520Mao%2520and%2520Juncheng%2520Wu%2520and%2520Yuyin%2520Zhou%2520and%2520Xuesong%2520Yin%2520and%2520Yuanqi%2520Chang%26entry.1292438233%3D%2520%2520There%2520are%2520many%2520excellent%2520solutions%2520in%2520image%2520restoration.However%252C%2520most%2520methods%250Arequire%2520on%2520training%2520separate%2520models%2520to%2520restore%2520images%2520with%2520different%2520types%2520of%250Adegradation.Although%2520existing%2520all-in-one%2520models%2520effectively%2520address%2520multiple%250Atypes%2520of%2520degradation%2520simultaneously%252C%2520their%2520performance%2520in%2520real-world%2520scenarios%250Ais%2520still%2520constrained%2520by%2520the%2520task%2520confusion%2520problem.In%2520this%2520work%252C%2520we%2520attempt%2520to%250Aaddress%2520this%2520issue%2520by%2520introducing%2520%255Ctextbf%257BRestorer%257D%252C%2520a%2520novel%2520Transformer-based%250Aall-in-one%2520image%2520restoration%2520model.To%2520effectively%2520address%2520the%2520complex%250Adegradation%2520present%2520in%2520real-world%2520images%252C%2520we%2520propose%2520All-Axis%2520Attention%2520%2528AAA%2529%252C%250Aa%2520mechanism%2520that%2520simultaneously%2520models%2520long-range%2520dependencies%2520across%2520both%250Aspatial%2520and%2520channel%2520dimensions%252C%2520capturing%2520potential%2520correlations%2520along%2520all%250Aaxes.Additionally%252C%2520we%2520introduce%2520textual%2520prompts%2520in%2520Restorer%2520to%2520incorporate%250Aexplicit%2520task%2520priors%252C%2520enabling%2520the%2520removal%2520of%2520specific%2520degradation%2520types%2520based%250Aon%2520user%2520instructions.%2520By%2520iterating%2520over%2520these%2520prompts%252C%2520Restorer%2520can%2520handle%250Acomposite%2520degradation%2520in%2520real-world%2520scenarios%2520without%2520requiring%2520additional%250Atraining.Based%2520on%2520these%2520designs%252C%2520Restorer%2520with%2520one%2520set%2520of%2520parameters%250Ademonstrates%2520state-of-the-art%2520performance%2520in%2520multiple%2520image%2520restoration%2520tasks%250Acompared%2520to%2520existing%2520all-in-one%2520and%2520even%2520single-task%2520models.Additionally%252C%250ARestorer%2520is%2520efficient%2520during%2520inference%252C%2520suggesting%2520the%2520potential%2520in%2520real-world%250Aapplications.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2406.12587v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Restorer%3A%20Removing%20Multi-Degradation%20with%20All-Axis%20Attention%20and%20Prompt%0A%20%20Guidance&entry.906535625=Jiawei%20Mao%20and%20Juncheng%20Wu%20and%20Yuyin%20Zhou%20and%20Xuesong%20Yin%20and%20Yuanqi%20Chang&entry.1292438233=%20%20There%20are%20many%20excellent%20solutions%20in%20image%20restoration.However%2C%20most%20methods%0Arequire%20on%20training%20separate%20models%20to%20restore%20images%20with%20different%20types%20of%0Adegradation.Although%20existing%20all-in-one%20models%20effectively%20address%20multiple%0Atypes%20of%20degradation%20simultaneously%2C%20their%20performance%20in%20real-world%20scenarios%0Ais%20still%20constrained%20by%20the%20task%20confusion%20problem.In%20this%20work%2C%20we%20attempt%20to%0Aaddress%20this%20issue%20by%20introducing%20%5Ctextbf%7BRestorer%7D%2C%20a%20novel%20Transformer-based%0Aall-in-one%20image%20restoration%20model.To%20effectively%20address%20the%20complex%0Adegradation%20present%20in%20real-world%20images%2C%20we%20propose%20All-Axis%20Attention%20%28AAA%29%2C%0Aa%20mechanism%20that%20simultaneously%20models%20long-range%20dependencies%20across%20both%0Aspatial%20and%20channel%20dimensions%2C%20capturing%20potential%20correlations%20along%20all%0Aaxes.Additionally%2C%20we%20introduce%20textual%20prompts%20in%20Restorer%20to%20incorporate%0Aexplicit%20task%20priors%2C%20enabling%20the%20removal%20of%20specific%20degradation%20types%20based%0Aon%20user%20instructions.%20By%20iterating%20over%20these%20prompts%2C%20Restorer%20can%20handle%0Acomposite%20degradation%20in%20real-world%20scenarios%20without%20requiring%20additional%0Atraining.Based%20on%20these%20designs%2C%20Restorer%20with%20one%20set%20of%20parameters%0Ademonstrates%20state-of-the-art%20performance%20in%20multiple%20image%20restoration%20tasks%0Acompared%20to%20existing%20all-in-one%20and%20even%20single-task%20models.Additionally%2C%0ARestorer%20is%20efficient%20during%20inference%2C%20suggesting%20the%20potential%20in%20real-world%0Aapplications.%0A&entry.1838667208=http%3A//arxiv.org/abs/2406.12587v2&entry.124074799=Read"},
{"title": "An embedding-based distance for temporal graphs", "author": "Lorenzo Dall'Amico and Alain Barrat and Ciro Cattuto", "abstract": "  Temporal graphs are commonly used to represent time-resolved relations\nbetween entities in many natural and artificial systems. Many techniques were\ndevised to investigate the evolution of temporal graphs by comparing their\nstate at different time points. However, quantifying the similarity between\ntemporal graphs as a whole is an open problem. Here, we use embeddings based on\ntime-respecting random walks to introduce a new notion of distance between\ntemporal graphs. This distance is well-defined for pairs of temporal graphs\nwith different numbers of nodes and different time spans. We study the case of\na matched pair of graphs, when a known relation exists between their nodes, and\nthe case of unmatched graphs, when such a relation is unavailable and the\ngraphs may be of different sizes. We use empirical and synthetic temporal\nnetwork data to show that the distance we introduce discriminates graphs with\ndifferent topological and temporal properties. We provide an efficient\nimplementation of the distance computation suitable for large-scale temporal\ngraphs.\n", "link": "http://arxiv.org/abs/2401.12843v2", "date": "2024-09-03", "relevancy": 1.6111, "topK": [{"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.4103}, {"title": "Dynamic Gaussians Mesh: Consistent Mesh Reconstruction from Monocular\n  Videos", "link": "http://arxiv.org/abs/2404.12379v2", "similarity": 0.4031}, {"title": "Self-supervised Photographic Image Layout Representation Learning", "link": "http://arxiv.org/abs/2403.03740v1", "similarity": 0.3994}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20An%20embedding-based%20distance%20for%20temporal%20graphs&body=Title%3A%20An%20embedding-based%20distance%20for%20temporal%20graphs%0AAuthor%3A%20Lorenzo%20Dall%27Amico%20and%20Alain%20Barrat%20and%20Ciro%20Cattuto%0AAbstract%3A%20%20%20Temporal%20graphs%20are%20commonly%20used%20to%20represent%20time-resolved%20relations%0Abetween%20entities%20in%20many%20natural%20and%20artificial%20systems.%20Many%20techniques%20were%0Adevised%20to%20investigate%20the%20evolution%20of%20temporal%20graphs%20by%20comparing%20their%0Astate%20at%20different%20time%20points.%20However%2C%20quantifying%20the%20similarity%20between%0Atemporal%20graphs%20as%20a%20whole%20is%20an%20open%20problem.%20Here%2C%20we%20use%20embeddings%20based%20on%0Atime-respecting%20random%20walks%20to%20introduce%20a%20new%20notion%20of%20distance%20between%0Atemporal%20graphs.%20This%20distance%20is%20well-defined%20for%20pairs%20of%20temporal%20graphs%0Awith%20different%20numbers%20of%20nodes%20and%20different%20time%20spans.%20We%20study%20the%20case%20of%0Aa%20matched%20pair%20of%20graphs%2C%20when%20a%20known%20relation%20exists%20between%20their%20nodes%2C%20and%0Athe%20case%20of%20unmatched%20graphs%2C%20when%20such%20a%20relation%20is%20unavailable%20and%20the%0Agraphs%20may%20be%20of%20different%20sizes.%20We%20use%20empirical%20and%20synthetic%20temporal%0Anetwork%20data%20to%20show%20that%20the%20distance%20we%20introduce%20discriminates%20graphs%20with%0Adifferent%20topological%20and%20temporal%20properties.%20We%20provide%20an%20efficient%0Aimplementation%20of%20the%20distance%20computation%20suitable%20for%20large-scale%20temporal%0Agraphs.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2401.12843v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DAn%2520embedding-based%2520distance%2520for%2520temporal%2520graphs%26entry.906535625%3DLorenzo%2520Dall%2527Amico%2520and%2520Alain%2520Barrat%2520and%2520Ciro%2520Cattuto%26entry.1292438233%3D%2520%2520Temporal%2520graphs%2520are%2520commonly%2520used%2520to%2520represent%2520time-resolved%2520relations%250Abetween%2520entities%2520in%2520many%2520natural%2520and%2520artificial%2520systems.%2520Many%2520techniques%2520were%250Adevised%2520to%2520investigate%2520the%2520evolution%2520of%2520temporal%2520graphs%2520by%2520comparing%2520their%250Astate%2520at%2520different%2520time%2520points.%2520However%252C%2520quantifying%2520the%2520similarity%2520between%250Atemporal%2520graphs%2520as%2520a%2520whole%2520is%2520an%2520open%2520problem.%2520Here%252C%2520we%2520use%2520embeddings%2520based%2520on%250Atime-respecting%2520random%2520walks%2520to%2520introduce%2520a%2520new%2520notion%2520of%2520distance%2520between%250Atemporal%2520graphs.%2520This%2520distance%2520is%2520well-defined%2520for%2520pairs%2520of%2520temporal%2520graphs%250Awith%2520different%2520numbers%2520of%2520nodes%2520and%2520different%2520time%2520spans.%2520We%2520study%2520the%2520case%2520of%250Aa%2520matched%2520pair%2520of%2520graphs%252C%2520when%2520a%2520known%2520relation%2520exists%2520between%2520their%2520nodes%252C%2520and%250Athe%2520case%2520of%2520unmatched%2520graphs%252C%2520when%2520such%2520a%2520relation%2520is%2520unavailable%2520and%2520the%250Agraphs%2520may%2520be%2520of%2520different%2520sizes.%2520We%2520use%2520empirical%2520and%2520synthetic%2520temporal%250Anetwork%2520data%2520to%2520show%2520that%2520the%2520distance%2520we%2520introduce%2520discriminates%2520graphs%2520with%250Adifferent%2520topological%2520and%2520temporal%2520properties.%2520We%2520provide%2520an%2520efficient%250Aimplementation%2520of%2520the%2520distance%2520computation%2520suitable%2520for%2520large-scale%2520temporal%250Agraphs.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2401.12843v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=An%20embedding-based%20distance%20for%20temporal%20graphs&entry.906535625=Lorenzo%20Dall%27Amico%20and%20Alain%20Barrat%20and%20Ciro%20Cattuto&entry.1292438233=%20%20Temporal%20graphs%20are%20commonly%20used%20to%20represent%20time-resolved%20relations%0Abetween%20entities%20in%20many%20natural%20and%20artificial%20systems.%20Many%20techniques%20were%0Adevised%20to%20investigate%20the%20evolution%20of%20temporal%20graphs%20by%20comparing%20their%0Astate%20at%20different%20time%20points.%20However%2C%20quantifying%20the%20similarity%20between%0Atemporal%20graphs%20as%20a%20whole%20is%20an%20open%20problem.%20Here%2C%20we%20use%20embeddings%20based%20on%0Atime-respecting%20random%20walks%20to%20introduce%20a%20new%20notion%20of%20distance%20between%0Atemporal%20graphs.%20This%20distance%20is%20well-defined%20for%20pairs%20of%20temporal%20graphs%0Awith%20different%20numbers%20of%20nodes%20and%20different%20time%20spans.%20We%20study%20the%20case%20of%0Aa%20matched%20pair%20of%20graphs%2C%20when%20a%20known%20relation%20exists%20between%20their%20nodes%2C%20and%0Athe%20case%20of%20unmatched%20graphs%2C%20when%20such%20a%20relation%20is%20unavailable%20and%20the%0Agraphs%20may%20be%20of%20different%20sizes.%20We%20use%20empirical%20and%20synthetic%20temporal%0Anetwork%20data%20to%20show%20that%20the%20distance%20we%20introduce%20discriminates%20graphs%20with%0Adifferent%20topological%20and%20temporal%20properties.%20We%20provide%20an%20efficient%0Aimplementation%20of%20the%20distance%20computation%20suitable%20for%20large-scale%20temporal%0Agraphs.%0A&entry.1838667208=http%3A//arxiv.org/abs/2401.12843v2&entry.124074799=Read"},
{"title": "Behavioral Learning of Dish Rinsing and Scrubbing based on Interruptive\n  Direct Teaching Considering Assistance Rate", "author": "Shumpei Wakabayashi and Kento Kawaharazuka and Kei Okada and Masayuki Inaba", "abstract": "  Robots are expected to manipulate objects in a safe and dexterous way. For\nexample, washing dishes is a dexterous operation that involves scrubbing the\ndishes with a sponge and rinsing them with water. It is necessary to learn it\nsafely without splashing water and without dropping the dishes. In this study,\nwe propose a safe and dexterous manipulation system. The robot learns a\ndynamics model of the object by estimating the state of the object and the\nrobot itself, the control input, and the amount of human assistance required\n(assistance rate) after the human corrects the initial trajectory of the\nrobot's hands by interruptive direct teaching. By backpropagating the error\nbetween the estimated and the reference value using the acquired dynamics\nmodel, the robot can generate a control input that approaches the reference\nvalue, for example, so that human assistance is not required and the dish does\nnot move excessively. This allows for adaptive rinsing and scrubbing of dishes\nwith unknown shapes and properties. As a result, it is possible to generate\nsafe actions that require less human assistance.\n", "link": "http://arxiv.org/abs/2408.09360v2", "date": "2024-09-03", "relevancy": 1.5321, "topK": [{"title": "RoboEXP: Action-Conditioned Scene Graph via Interactive Exploration for\n  Robotic Manipulation", "link": "http://arxiv.org/abs/2402.15487v1", "similarity": 0.5383}, {"title": "Safe and efficient training of a control agent", "link": "https://patents.google.com/patent/US11709462B2/en", "similarity": 0.5074}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.4912}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Behavioral%20Learning%20of%20Dish%20Rinsing%20and%20Scrubbing%20based%20on%20Interruptive%0A%20%20Direct%20Teaching%20Considering%20Assistance%20Rate&body=Title%3A%20Behavioral%20Learning%20of%20Dish%20Rinsing%20and%20Scrubbing%20based%20on%20Interruptive%0A%20%20Direct%20Teaching%20Considering%20Assistance%20Rate%0AAuthor%3A%20Shumpei%20Wakabayashi%20and%20Kento%20Kawaharazuka%20and%20Kei%20Okada%20and%20Masayuki%20Inaba%0AAbstract%3A%20%20%20Robots%20are%20expected%20to%20manipulate%20objects%20in%20a%20safe%20and%20dexterous%20way.%20For%0Aexample%2C%20washing%20dishes%20is%20a%20dexterous%20operation%20that%20involves%20scrubbing%20the%0Adishes%20with%20a%20sponge%20and%20rinsing%20them%20with%20water.%20It%20is%20necessary%20to%20learn%20it%0Asafely%20without%20splashing%20water%20and%20without%20dropping%20the%20dishes.%20In%20this%20study%2C%0Awe%20propose%20a%20safe%20and%20dexterous%20manipulation%20system.%20The%20robot%20learns%20a%0Adynamics%20model%20of%20the%20object%20by%20estimating%20the%20state%20of%20the%20object%20and%20the%0Arobot%20itself%2C%20the%20control%20input%2C%20and%20the%20amount%20of%20human%20assistance%20required%0A%28assistance%20rate%29%20after%20the%20human%20corrects%20the%20initial%20trajectory%20of%20the%0Arobot%27s%20hands%20by%20interruptive%20direct%20teaching.%20By%20backpropagating%20the%20error%0Abetween%20the%20estimated%20and%20the%20reference%20value%20using%20the%20acquired%20dynamics%0Amodel%2C%20the%20robot%20can%20generate%20a%20control%20input%20that%20approaches%20the%20reference%0Avalue%2C%20for%20example%2C%20so%20that%20human%20assistance%20is%20not%20required%20and%20the%20dish%20does%0Anot%20move%20excessively.%20This%20allows%20for%20adaptive%20rinsing%20and%20scrubbing%20of%20dishes%0Awith%20unknown%20shapes%20and%20properties.%20As%20a%20result%2C%20it%20is%20possible%20to%20generate%0Asafe%20actions%20that%20require%20less%20human%20assistance.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2408.09360v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DBehavioral%2520Learning%2520of%2520Dish%2520Rinsing%2520and%2520Scrubbing%2520based%2520on%2520Interruptive%250A%2520%2520Direct%2520Teaching%2520Considering%2520Assistance%2520Rate%26entry.906535625%3DShumpei%2520Wakabayashi%2520and%2520Kento%2520Kawaharazuka%2520and%2520Kei%2520Okada%2520and%2520Masayuki%2520Inaba%26entry.1292438233%3D%2520%2520Robots%2520are%2520expected%2520to%2520manipulate%2520objects%2520in%2520a%2520safe%2520and%2520dexterous%2520way.%2520For%250Aexample%252C%2520washing%2520dishes%2520is%2520a%2520dexterous%2520operation%2520that%2520involves%2520scrubbing%2520the%250Adishes%2520with%2520a%2520sponge%2520and%2520rinsing%2520them%2520with%2520water.%2520It%2520is%2520necessary%2520to%2520learn%2520it%250Asafely%2520without%2520splashing%2520water%2520and%2520without%2520dropping%2520the%2520dishes.%2520In%2520this%2520study%252C%250Awe%2520propose%2520a%2520safe%2520and%2520dexterous%2520manipulation%2520system.%2520The%2520robot%2520learns%2520a%250Adynamics%2520model%2520of%2520the%2520object%2520by%2520estimating%2520the%2520state%2520of%2520the%2520object%2520and%2520the%250Arobot%2520itself%252C%2520the%2520control%2520input%252C%2520and%2520the%2520amount%2520of%2520human%2520assistance%2520required%250A%2528assistance%2520rate%2529%2520after%2520the%2520human%2520corrects%2520the%2520initial%2520trajectory%2520of%2520the%250Arobot%2527s%2520hands%2520by%2520interruptive%2520direct%2520teaching.%2520By%2520backpropagating%2520the%2520error%250Abetween%2520the%2520estimated%2520and%2520the%2520reference%2520value%2520using%2520the%2520acquired%2520dynamics%250Amodel%252C%2520the%2520robot%2520can%2520generate%2520a%2520control%2520input%2520that%2520approaches%2520the%2520reference%250Avalue%252C%2520for%2520example%252C%2520so%2520that%2520human%2520assistance%2520is%2520not%2520required%2520and%2520the%2520dish%2520does%250Anot%2520move%2520excessively.%2520This%2520allows%2520for%2520adaptive%2520rinsing%2520and%2520scrubbing%2520of%2520dishes%250Awith%2520unknown%2520shapes%2520and%2520properties.%2520As%2520a%2520result%252C%2520it%2520is%2520possible%2520to%2520generate%250Asafe%2520actions%2520that%2520require%2520less%2520human%2520assistance.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2408.09360v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Behavioral%20Learning%20of%20Dish%20Rinsing%20and%20Scrubbing%20based%20on%20Interruptive%0A%20%20Direct%20Teaching%20Considering%20Assistance%20Rate&entry.906535625=Shumpei%20Wakabayashi%20and%20Kento%20Kawaharazuka%20and%20Kei%20Okada%20and%20Masayuki%20Inaba&entry.1292438233=%20%20Robots%20are%20expected%20to%20manipulate%20objects%20in%20a%20safe%20and%20dexterous%20way.%20For%0Aexample%2C%20washing%20dishes%20is%20a%20dexterous%20operation%20that%20involves%20scrubbing%20the%0Adishes%20with%20a%20sponge%20and%20rinsing%20them%20with%20water.%20It%20is%20necessary%20to%20learn%20it%0Asafely%20without%20splashing%20water%20and%20without%20dropping%20the%20dishes.%20In%20this%20study%2C%0Awe%20propose%20a%20safe%20and%20dexterous%20manipulation%20system.%20The%20robot%20learns%20a%0Adynamics%20model%20of%20the%20object%20by%20estimating%20the%20state%20of%20the%20object%20and%20the%0Arobot%20itself%2C%20the%20control%20input%2C%20and%20the%20amount%20of%20human%20assistance%20required%0A%28assistance%20rate%29%20after%20the%20human%20corrects%20the%20initial%20trajectory%20of%20the%0Arobot%27s%20hands%20by%20interruptive%20direct%20teaching.%20By%20backpropagating%20the%20error%0Abetween%20the%20estimated%20and%20the%20reference%20value%20using%20the%20acquired%20dynamics%0Amodel%2C%20the%20robot%20can%20generate%20a%20control%20input%20that%20approaches%20the%20reference%0Avalue%2C%20for%20example%2C%20so%20that%20human%20assistance%20is%20not%20required%20and%20the%20dish%20does%0Anot%20move%20excessively.%20This%20allows%20for%20adaptive%20rinsing%20and%20scrubbing%20of%20dishes%0Awith%20unknown%20shapes%20and%20properties.%20As%20a%20result%2C%20it%20is%20possible%20to%20generate%0Asafe%20actions%20that%20require%20less%20human%20assistance.%0A&entry.1838667208=http%3A//arxiv.org/abs/2408.09360v2&entry.124074799=Read"},
{"title": "IBO: Inpainting-Based Occlusion to Enhance Explainable Artificial\n  Intelligence Evaluation in Histopathology", "author": "Pardis Afshar and Sajjad Hashembeiki and Pouya Khani and Emad Fatemizadeh and Mohammad Hossein Rohban", "abstract": "  Histopathological image analysis is crucial for accurate cancer diagnosis and\ntreatment planning. While deep learning models, especially convolutional neural\nnetworks, have advanced this field, their \"black-box\" nature raises concerns\nabout interpretability and trustworthiness. Explainable Artificial Intelligence\n(XAI) techniques aim to address these concerns, but evaluating their\neffectiveness remains challenging. A significant issue with current\nocclusion-based XAI methods is that they often generate Out-of-Distribution\n(OoD) samples, leading to inaccurate evaluations. In this paper, we introduce\nInpainting-Based Occlusion (IBO), a novel occlusion strategy that utilizes a\nDenoising Diffusion Probabilistic Model to inpaint occluded regions in\nhistopathological images. By replacing cancerous areas with realistic,\nnon-cancerous tissue, IBO minimizes OoD artifacts and preserves data integrity.\nWe evaluate our method on the CAMELYON16 dataset through two phases: first, by\nassessing perceptual similarity using the Learned Perceptual Image Patch\nSimilarity (LPIPS) metric, and second, by quantifying the impact on model\npredictions through Area Under the Curve (AUC) analysis. Our results\ndemonstrate that IBO significantly improves perceptual fidelity, achieving\nnearly twice the improvement in LPIPS scores compared to the best existing\nocclusion strategy. Additionally, IBO increased the precision of XAI\nperformance prediction from 42% to 71% compared to traditional methods. These\nresults demonstrate IBO's potential to provide more reliable evaluations of XAI\ntechniques, benefiting histopathology and other applications. The source code\nfor this study is available at https://github.com/a-fsh-r/IBO.\n", "link": "http://arxiv.org/abs/2408.16395v2", "date": "2024-09-03", "relevancy": 1.5218, "topK": [{"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.5145}, {"title": "Tuning-Free Noise Rectification for High Fidelity Image-to-Video\n  Generation", "link": "http://arxiv.org/abs/2403.02827v1", "similarity": 0.509}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.4956}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20IBO%3A%20Inpainting-Based%20Occlusion%20to%20Enhance%20Explainable%20Artificial%0A%20%20Intelligence%20Evaluation%20in%20Histopathology&body=Title%3A%20IBO%3A%20Inpainting-Based%20Occlusion%20to%20Enhance%20Explainable%20Artificial%0A%20%20Intelligence%20Evaluation%20in%20Histopathology%0AAuthor%3A%20Pardis%20Afshar%20and%20Sajjad%20Hashembeiki%20and%20Pouya%20Khani%20and%20Emad%20Fatemizadeh%20and%20Mohammad%20Hossein%20Rohban%0AAbstract%3A%20%20%20Histopathological%20image%20analysis%20is%20crucial%20for%20accurate%20cancer%20diagnosis%20and%0Atreatment%20planning.%20While%20deep%20learning%20models%2C%20especially%20convolutional%20neural%0Anetworks%2C%20have%20advanced%20this%20field%2C%20their%20%22black-box%22%20nature%20raises%20concerns%0Aabout%20interpretability%20and%20trustworthiness.%20Explainable%20Artificial%20Intelligence%0A%28XAI%29%20techniques%20aim%20to%20address%20these%20concerns%2C%20but%20evaluating%20their%0Aeffectiveness%20remains%20challenging.%20A%20significant%20issue%20with%20current%0Aocclusion-based%20XAI%20methods%20is%20that%20they%20often%20generate%20Out-of-Distribution%0A%28OoD%29%20samples%2C%20leading%20to%20inaccurate%20evaluations.%20In%20this%20paper%2C%20we%20introduce%0AInpainting-Based%20Occlusion%20%28IBO%29%2C%20a%20novel%20occlusion%20strategy%20that%20utilizes%20a%0ADenoising%20Diffusion%20Probabilistic%20Model%20to%20inpaint%20occluded%20regions%20in%0Ahistopathological%20images.%20By%20replacing%20cancerous%20areas%20with%20realistic%2C%0Anon-cancerous%20tissue%2C%20IBO%20minimizes%20OoD%20artifacts%20and%20preserves%20data%20integrity.%0AWe%20evaluate%20our%20method%20on%20the%20CAMELYON16%20dataset%20through%20two%20phases%3A%20first%2C%20by%0Aassessing%20perceptual%20similarity%20using%20the%20Learned%20Perceptual%20Image%20Patch%0ASimilarity%20%28LPIPS%29%20metric%2C%20and%20second%2C%20by%20quantifying%20the%20impact%20on%20model%0Apredictions%20through%20Area%20Under%20the%20Curve%20%28AUC%29%20analysis.%20Our%20results%0Ademonstrate%20that%20IBO%20significantly%20improves%20perceptual%20fidelity%2C%20achieving%0Anearly%20twice%20the%20improvement%20in%20LPIPS%20scores%20compared%20to%20the%20best%20existing%0Aocclusion%20strategy.%20Additionally%2C%20IBO%20increased%20the%20precision%20of%20XAI%0Aperformance%20prediction%20from%2042%25%20to%2071%25%20compared%20to%20traditional%20methods.%20These%0Aresults%20demonstrate%20IBO%27s%20potential%20to%20provide%20more%20reliable%20evaluations%20of%20XAI%0Atechniques%2C%20benefiting%20histopathology%20and%20other%20applications.%20The%20source%20code%0Afor%20this%20study%20is%20available%20at%20https%3A//github.com/a-fsh-r/IBO.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2408.16395v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DIBO%253A%2520Inpainting-Based%2520Occlusion%2520to%2520Enhance%2520Explainable%2520Artificial%250A%2520%2520Intelligence%2520Evaluation%2520in%2520Histopathology%26entry.906535625%3DPardis%2520Afshar%2520and%2520Sajjad%2520Hashembeiki%2520and%2520Pouya%2520Khani%2520and%2520Emad%2520Fatemizadeh%2520and%2520Mohammad%2520Hossein%2520Rohban%26entry.1292438233%3D%2520%2520Histopathological%2520image%2520analysis%2520is%2520crucial%2520for%2520accurate%2520cancer%2520diagnosis%2520and%250Atreatment%2520planning.%2520While%2520deep%2520learning%2520models%252C%2520especially%2520convolutional%2520neural%250Anetworks%252C%2520have%2520advanced%2520this%2520field%252C%2520their%2520%2522black-box%2522%2520nature%2520raises%2520concerns%250Aabout%2520interpretability%2520and%2520trustworthiness.%2520Explainable%2520Artificial%2520Intelligence%250A%2528XAI%2529%2520techniques%2520aim%2520to%2520address%2520these%2520concerns%252C%2520but%2520evaluating%2520their%250Aeffectiveness%2520remains%2520challenging.%2520A%2520significant%2520issue%2520with%2520current%250Aocclusion-based%2520XAI%2520methods%2520is%2520that%2520they%2520often%2520generate%2520Out-of-Distribution%250A%2528OoD%2529%2520samples%252C%2520leading%2520to%2520inaccurate%2520evaluations.%2520In%2520this%2520paper%252C%2520we%2520introduce%250AInpainting-Based%2520Occlusion%2520%2528IBO%2529%252C%2520a%2520novel%2520occlusion%2520strategy%2520that%2520utilizes%2520a%250ADenoising%2520Diffusion%2520Probabilistic%2520Model%2520to%2520inpaint%2520occluded%2520regions%2520in%250Ahistopathological%2520images.%2520By%2520replacing%2520cancerous%2520areas%2520with%2520realistic%252C%250Anon-cancerous%2520tissue%252C%2520IBO%2520minimizes%2520OoD%2520artifacts%2520and%2520preserves%2520data%2520integrity.%250AWe%2520evaluate%2520our%2520method%2520on%2520the%2520CAMELYON16%2520dataset%2520through%2520two%2520phases%253A%2520first%252C%2520by%250Aassessing%2520perceptual%2520similarity%2520using%2520the%2520Learned%2520Perceptual%2520Image%2520Patch%250ASimilarity%2520%2528LPIPS%2529%2520metric%252C%2520and%2520second%252C%2520by%2520quantifying%2520the%2520impact%2520on%2520model%250Apredictions%2520through%2520Area%2520Under%2520the%2520Curve%2520%2528AUC%2529%2520analysis.%2520Our%2520results%250Ademonstrate%2520that%2520IBO%2520significantly%2520improves%2520perceptual%2520fidelity%252C%2520achieving%250Anearly%2520twice%2520the%2520improvement%2520in%2520LPIPS%2520scores%2520compared%2520to%2520the%2520best%2520existing%250Aocclusion%2520strategy.%2520Additionally%252C%2520IBO%2520increased%2520the%2520precision%2520of%2520XAI%250Aperformance%2520prediction%2520from%252042%2525%2520to%252071%2525%2520compared%2520to%2520traditional%2520methods.%2520These%250Aresults%2520demonstrate%2520IBO%2527s%2520potential%2520to%2520provide%2520more%2520reliable%2520evaluations%2520of%2520XAI%250Atechniques%252C%2520benefiting%2520histopathology%2520and%2520other%2520applications.%2520The%2520source%2520code%250Afor%2520this%2520study%2520is%2520available%2520at%2520https%253A//github.com/a-fsh-r/IBO.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2408.16395v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=IBO%3A%20Inpainting-Based%20Occlusion%20to%20Enhance%20Explainable%20Artificial%0A%20%20Intelligence%20Evaluation%20in%20Histopathology&entry.906535625=Pardis%20Afshar%20and%20Sajjad%20Hashembeiki%20and%20Pouya%20Khani%20and%20Emad%20Fatemizadeh%20and%20Mohammad%20Hossein%20Rohban&entry.1292438233=%20%20Histopathological%20image%20analysis%20is%20crucial%20for%20accurate%20cancer%20diagnosis%20and%0Atreatment%20planning.%20While%20deep%20learning%20models%2C%20especially%20convolutional%20neural%0Anetworks%2C%20have%20advanced%20this%20field%2C%20their%20%22black-box%22%20nature%20raises%20concerns%0Aabout%20interpretability%20and%20trustworthiness.%20Explainable%20Artificial%20Intelligence%0A%28XAI%29%20techniques%20aim%20to%20address%20these%20concerns%2C%20but%20evaluating%20their%0Aeffectiveness%20remains%20challenging.%20A%20significant%20issue%20with%20current%0Aocclusion-based%20XAI%20methods%20is%20that%20they%20often%20generate%20Out-of-Distribution%0A%28OoD%29%20samples%2C%20leading%20to%20inaccurate%20evaluations.%20In%20this%20paper%2C%20we%20introduce%0AInpainting-Based%20Occlusion%20%28IBO%29%2C%20a%20novel%20occlusion%20strategy%20that%20utilizes%20a%0ADenoising%20Diffusion%20Probabilistic%20Model%20to%20inpaint%20occluded%20regions%20in%0Ahistopathological%20images.%20By%20replacing%20cancerous%20areas%20with%20realistic%2C%0Anon-cancerous%20tissue%2C%20IBO%20minimizes%20OoD%20artifacts%20and%20preserves%20data%20integrity.%0AWe%20evaluate%20our%20method%20on%20the%20CAMELYON16%20dataset%20through%20two%20phases%3A%20first%2C%20by%0Aassessing%20perceptual%20similarity%20using%20the%20Learned%20Perceptual%20Image%20Patch%0ASimilarity%20%28LPIPS%29%20metric%2C%20and%20second%2C%20by%20quantifying%20the%20impact%20on%20model%0Apredictions%20through%20Area%20Under%20the%20Curve%20%28AUC%29%20analysis.%20Our%20results%0Ademonstrate%20that%20IBO%20significantly%20improves%20perceptual%20fidelity%2C%20achieving%0Anearly%20twice%20the%20improvement%20in%20LPIPS%20scores%20compared%20to%20the%20best%20existing%0Aocclusion%20strategy.%20Additionally%2C%20IBO%20increased%20the%20precision%20of%20XAI%0Aperformance%20prediction%20from%2042%25%20to%2071%25%20compared%20to%20traditional%20methods.%20These%0Aresults%20demonstrate%20IBO%27s%20potential%20to%20provide%20more%20reliable%20evaluations%20of%20XAI%0Atechniques%2C%20benefiting%20histopathology%20and%20other%20applications.%20The%20source%20code%0Afor%20this%20study%20is%20available%20at%20https%3A//github.com/a-fsh-r/IBO.%0A&entry.1838667208=http%3A//arxiv.org/abs/2408.16395v2&entry.124074799=Read"},
{"title": "Towards Explainable Traffic Flow Prediction with Large Language Models", "author": "Xusen Guo and Qiming Zhang and Junyue Jiang and Mingxing Peng and Meixin Zhu and  Hao and  Yang", "abstract": "  Traffic forecasting is crucial for intelligent transportation systems. It has\nexperienced significant advancements thanks to the power of deep learning in\ncapturing latent patterns of traffic data. However, recent deep-learning\narchitectures require intricate model designs and lack an intuitive\nunderstanding of the mapping from input data to predicted results. Achieving\nboth accuracy and explainability in traffic prediction models remains a\nchallenge due to the complexity of traffic data and the inherent opacity of\ndeep learning models. To tackle these challenges, we propose a Traffic flow\nPrediction model based on Large Language Models (LLMs) to generate explainable\ntraffic predictions, named xTP-LLM. By transferring multi-modal traffic data\ninto natural language descriptions, xTP-LLM captures complex time-series\npatterns and external factors from comprehensive traffic data. The LLM\nframework is fine-tuned using language-based instructions to align with\nspatial-temporal traffic flow data. Empirically, xTP-LLM shows competitive\naccuracy compared with deep learning baselines, while providing an intuitive\nand reliable explanation for predictions. This paper contributes to advancing\nexplainable traffic prediction models and lays a foundation for future\nexploration of LLM applications in transportation. To the best of our\nknowledge, this is the first study to use LLM for explainable prediction of\ntraffic flows.\n", "link": "http://arxiv.org/abs/2404.02937v5", "date": "2024-09-03", "relevancy": 1.4597, "topK": [{"title": "Scaling Rectified Flow Transformers for High-Resolution Image Synthesis", "link": "http://arxiv.org/abs/2403.03206v1", "similarity": 0.5065}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.4847}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.4714}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Towards%20Explainable%20Traffic%20Flow%20Prediction%20with%20Large%20Language%20Models&body=Title%3A%20Towards%20Explainable%20Traffic%20Flow%20Prediction%20with%20Large%20Language%20Models%0AAuthor%3A%20Xusen%20Guo%20and%20Qiming%20Zhang%20and%20Junyue%20Jiang%20and%20Mingxing%20Peng%20and%20Meixin%20Zhu%20and%20%20Hao%20and%20%20Yang%0AAbstract%3A%20%20%20Traffic%20forecasting%20is%20crucial%20for%20intelligent%20transportation%20systems.%20It%20has%0Aexperienced%20significant%20advancements%20thanks%20to%20the%20power%20of%20deep%20learning%20in%0Acapturing%20latent%20patterns%20of%20traffic%20data.%20However%2C%20recent%20deep-learning%0Aarchitectures%20require%20intricate%20model%20designs%20and%20lack%20an%20intuitive%0Aunderstanding%20of%20the%20mapping%20from%20input%20data%20to%20predicted%20results.%20Achieving%0Aboth%20accuracy%20and%20explainability%20in%20traffic%20prediction%20models%20remains%20a%0Achallenge%20due%20to%20the%20complexity%20of%20traffic%20data%20and%20the%20inherent%20opacity%20of%0Adeep%20learning%20models.%20To%20tackle%20these%20challenges%2C%20we%20propose%20a%20Traffic%20flow%0APrediction%20model%20based%20on%20Large%20Language%20Models%20%28LLMs%29%20to%20generate%20explainable%0Atraffic%20predictions%2C%20named%20xTP-LLM.%20By%20transferring%20multi-modal%20traffic%20data%0Ainto%20natural%20language%20descriptions%2C%20xTP-LLM%20captures%20complex%20time-series%0Apatterns%20and%20external%20factors%20from%20comprehensive%20traffic%20data.%20The%20LLM%0Aframework%20is%20fine-tuned%20using%20language-based%20instructions%20to%20align%20with%0Aspatial-temporal%20traffic%20flow%20data.%20Empirically%2C%20xTP-LLM%20shows%20competitive%0Aaccuracy%20compared%20with%20deep%20learning%20baselines%2C%20while%20providing%20an%20intuitive%0Aand%20reliable%20explanation%20for%20predictions.%20This%20paper%20contributes%20to%20advancing%0Aexplainable%20traffic%20prediction%20models%20and%20lays%20a%20foundation%20for%20future%0Aexploration%20of%20LLM%20applications%20in%20transportation.%20To%20the%20best%20of%20our%0Aknowledge%2C%20this%20is%20the%20first%20study%20to%20use%20LLM%20for%20explainable%20prediction%20of%0Atraffic%20flows.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2404.02937v5%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DTowards%2520Explainable%2520Traffic%2520Flow%2520Prediction%2520with%2520Large%2520Language%2520Models%26entry.906535625%3DXusen%2520Guo%2520and%2520Qiming%2520Zhang%2520and%2520Junyue%2520Jiang%2520and%2520Mingxing%2520Peng%2520and%2520Meixin%2520Zhu%2520and%2520%2520Hao%2520and%2520%2520Yang%26entry.1292438233%3D%2520%2520Traffic%2520forecasting%2520is%2520crucial%2520for%2520intelligent%2520transportation%2520systems.%2520It%2520has%250Aexperienced%2520significant%2520advancements%2520thanks%2520to%2520the%2520power%2520of%2520deep%2520learning%2520in%250Acapturing%2520latent%2520patterns%2520of%2520traffic%2520data.%2520However%252C%2520recent%2520deep-learning%250Aarchitectures%2520require%2520intricate%2520model%2520designs%2520and%2520lack%2520an%2520intuitive%250Aunderstanding%2520of%2520the%2520mapping%2520from%2520input%2520data%2520to%2520predicted%2520results.%2520Achieving%250Aboth%2520accuracy%2520and%2520explainability%2520in%2520traffic%2520prediction%2520models%2520remains%2520a%250Achallenge%2520due%2520to%2520the%2520complexity%2520of%2520traffic%2520data%2520and%2520the%2520inherent%2520opacity%2520of%250Adeep%2520learning%2520models.%2520To%2520tackle%2520these%2520challenges%252C%2520we%2520propose%2520a%2520Traffic%2520flow%250APrediction%2520model%2520based%2520on%2520Large%2520Language%2520Models%2520%2528LLMs%2529%2520to%2520generate%2520explainable%250Atraffic%2520predictions%252C%2520named%2520xTP-LLM.%2520By%2520transferring%2520multi-modal%2520traffic%2520data%250Ainto%2520natural%2520language%2520descriptions%252C%2520xTP-LLM%2520captures%2520complex%2520time-series%250Apatterns%2520and%2520external%2520factors%2520from%2520comprehensive%2520traffic%2520data.%2520The%2520LLM%250Aframework%2520is%2520fine-tuned%2520using%2520language-based%2520instructions%2520to%2520align%2520with%250Aspatial-temporal%2520traffic%2520flow%2520data.%2520Empirically%252C%2520xTP-LLM%2520shows%2520competitive%250Aaccuracy%2520compared%2520with%2520deep%2520learning%2520baselines%252C%2520while%2520providing%2520an%2520intuitive%250Aand%2520reliable%2520explanation%2520for%2520predictions.%2520This%2520paper%2520contributes%2520to%2520advancing%250Aexplainable%2520traffic%2520prediction%2520models%2520and%2520lays%2520a%2520foundation%2520for%2520future%250Aexploration%2520of%2520LLM%2520applications%2520in%2520transportation.%2520To%2520the%2520best%2520of%2520our%250Aknowledge%252C%2520this%2520is%2520the%2520first%2520study%2520to%2520use%2520LLM%2520for%2520explainable%2520prediction%2520of%250Atraffic%2520flows.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2404.02937v5%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Towards%20Explainable%20Traffic%20Flow%20Prediction%20with%20Large%20Language%20Models&entry.906535625=Xusen%20Guo%20and%20Qiming%20Zhang%20and%20Junyue%20Jiang%20and%20Mingxing%20Peng%20and%20Meixin%20Zhu%20and%20%20Hao%20and%20%20Yang&entry.1292438233=%20%20Traffic%20forecasting%20is%20crucial%20for%20intelligent%20transportation%20systems.%20It%20has%0Aexperienced%20significant%20advancements%20thanks%20to%20the%20power%20of%20deep%20learning%20in%0Acapturing%20latent%20patterns%20of%20traffic%20data.%20However%2C%20recent%20deep-learning%0Aarchitectures%20require%20intricate%20model%20designs%20and%20lack%20an%20intuitive%0Aunderstanding%20of%20the%20mapping%20from%20input%20data%20to%20predicted%20results.%20Achieving%0Aboth%20accuracy%20and%20explainability%20in%20traffic%20prediction%20models%20remains%20a%0Achallenge%20due%20to%20the%20complexity%20of%20traffic%20data%20and%20the%20inherent%20opacity%20of%0Adeep%20learning%20models.%20To%20tackle%20these%20challenges%2C%20we%20propose%20a%20Traffic%20flow%0APrediction%20model%20based%20on%20Large%20Language%20Models%20%28LLMs%29%20to%20generate%20explainable%0Atraffic%20predictions%2C%20named%20xTP-LLM.%20By%20transferring%20multi-modal%20traffic%20data%0Ainto%20natural%20language%20descriptions%2C%20xTP-LLM%20captures%20complex%20time-series%0Apatterns%20and%20external%20factors%20from%20comprehensive%20traffic%20data.%20The%20LLM%0Aframework%20is%20fine-tuned%20using%20language-based%20instructions%20to%20align%20with%0Aspatial-temporal%20traffic%20flow%20data.%20Empirically%2C%20xTP-LLM%20shows%20competitive%0Aaccuracy%20compared%20with%20deep%20learning%20baselines%2C%20while%20providing%20an%20intuitive%0Aand%20reliable%20explanation%20for%20predictions.%20This%20paper%20contributes%20to%20advancing%0Aexplainable%20traffic%20prediction%20models%20and%20lays%20a%20foundation%20for%20future%0Aexploration%20of%20LLM%20applications%20in%20transportation.%20To%20the%20best%20of%20our%0Aknowledge%2C%20this%20is%20the%20first%20study%20to%20use%20LLM%20for%20explainable%20prediction%20of%0Atraffic%20flows.%0A&entry.1838667208=http%3A//arxiv.org/abs/2404.02937v5&entry.124074799=Read"},
{"title": "Improving Rare Word Translation With Dictionaries and Attention Masking", "author": "Kenneth J. Sible and David Chiang", "abstract": "  In machine translation, rare words continue to be a problem for the dominant\nencoder-decoder architecture, especially in low-resource and out-of-domain\ntranslation settings. Human translators solve this problem with monolingual or\nbilingual dictionaries. In this paper, we propose appending definitions from a\nbilingual dictionary to source sentences and using attention masking to link\ntogether rare words with their definitions. We find that including definitions\nfor rare words improves performance by up to 1.0 BLEU and 1.6 MacroF1.\n", "link": "http://arxiv.org/abs/2408.09075v2", "date": "2024-09-03", "relevancy": 1.4048, "topK": [{"title": "ToDo: Token Downsampling for Efficient Generation of High-Resolution\n  Images", "link": "http://arxiv.org/abs/2402.13573v2", "similarity": 0.4853}, {"title": "Scaling Rectified Flow Transformers for High-Resolution Image Synthesis", "link": "http://arxiv.org/abs/2403.03206v1", "similarity": 0.4518}, {"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.442}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Improving%20Rare%20Word%20Translation%20With%20Dictionaries%20and%20Attention%20Masking&body=Title%3A%20Improving%20Rare%20Word%20Translation%20With%20Dictionaries%20and%20Attention%20Masking%0AAuthor%3A%20Kenneth%20J.%20Sible%20and%20David%20Chiang%0AAbstract%3A%20%20%20In%20machine%20translation%2C%20rare%20words%20continue%20to%20be%20a%20problem%20for%20the%20dominant%0Aencoder-decoder%20architecture%2C%20especially%20in%20low-resource%20and%20out-of-domain%0Atranslation%20settings.%20Human%20translators%20solve%20this%20problem%20with%20monolingual%20or%0Abilingual%20dictionaries.%20In%20this%20paper%2C%20we%20propose%20appending%20definitions%20from%20a%0Abilingual%20dictionary%20to%20source%20sentences%20and%20using%20attention%20masking%20to%20link%0Atogether%20rare%20words%20with%20their%20definitions.%20We%20find%20that%20including%20definitions%0Afor%20rare%20words%20improves%20performance%20by%20up%20to%201.0%20BLEU%20and%201.6%20MacroF1.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2408.09075v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DImproving%2520Rare%2520Word%2520Translation%2520With%2520Dictionaries%2520and%2520Attention%2520Masking%26entry.906535625%3DKenneth%2520J.%2520Sible%2520and%2520David%2520Chiang%26entry.1292438233%3D%2520%2520In%2520machine%2520translation%252C%2520rare%2520words%2520continue%2520to%2520be%2520a%2520problem%2520for%2520the%2520dominant%250Aencoder-decoder%2520architecture%252C%2520especially%2520in%2520low-resource%2520and%2520out-of-domain%250Atranslation%2520settings.%2520Human%2520translators%2520solve%2520this%2520problem%2520with%2520monolingual%2520or%250Abilingual%2520dictionaries.%2520In%2520this%2520paper%252C%2520we%2520propose%2520appending%2520definitions%2520from%2520a%250Abilingual%2520dictionary%2520to%2520source%2520sentences%2520and%2520using%2520attention%2520masking%2520to%2520link%250Atogether%2520rare%2520words%2520with%2520their%2520definitions.%2520We%2520find%2520that%2520including%2520definitions%250Afor%2520rare%2520words%2520improves%2520performance%2520by%2520up%2520to%25201.0%2520BLEU%2520and%25201.6%2520MacroF1.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2408.09075v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Improving%20Rare%20Word%20Translation%20With%20Dictionaries%20and%20Attention%20Masking&entry.906535625=Kenneth%20J.%20Sible%20and%20David%20Chiang&entry.1292438233=%20%20In%20machine%20translation%2C%20rare%20words%20continue%20to%20be%20a%20problem%20for%20the%20dominant%0Aencoder-decoder%20architecture%2C%20especially%20in%20low-resource%20and%20out-of-domain%0Atranslation%20settings.%20Human%20translators%20solve%20this%20problem%20with%20monolingual%20or%0Abilingual%20dictionaries.%20In%20this%20paper%2C%20we%20propose%20appending%20definitions%20from%20a%0Abilingual%20dictionary%20to%20source%20sentences%20and%20using%20attention%20masking%20to%20link%0Atogether%20rare%20words%20with%20their%20definitions.%20We%20find%20that%20including%20definitions%0Afor%20rare%20words%20improves%20performance%20by%20up%20to%201.0%20BLEU%20and%201.6%20MacroF1.%0A&entry.1838667208=http%3A//arxiv.org/abs/2408.09075v2&entry.124074799=Read"},
{"title": "Foundation Models for Music: A Survey", "author": "Yinghao Ma and Anders \u00d8land and Anton Ragni and Bleiz MacSen Del Sette and Charalampos Saitis and Chris Donahue and Chenghua Lin and Christos Plachouras and Emmanouil Benetos and Elona Shatri and Fabio Morreale and Ge Zhang and Gy\u00f6rgy Fazekas and Gus Xia and Huan Zhang and Ilaria Manco and Jiawen Huang and Julien Guinot and Liwei Lin and Luca Marinelli and Max W. Y. Lam and Megha Sharma and Qiuqiang Kong and Roger B. Dannenberg and Ruibin Yuan and Shangda Wu and Shih-Lun Wu and Shuqi Dai and Shun Lei and Shiyin Kang and Simon Dixon and Wenhu Chen and Wenhao Huang and Xingjian Du and Xingwei Qu and Xu Tan and Yizhi Li and Zeyue Tian and Zhiyong Wu and Zhizheng Wu and Ziyang Ma and Ziyu Wang", "abstract": "  In recent years, foundation models (FMs) such as large language models (LLMs)\nand latent diffusion models (LDMs) have profoundly impacted diverse sectors,\nincluding music. This comprehensive review examines state-of-the-art (SOTA)\npre-trained models and foundation models in music, spanning from representation\nlearning, generative learning and multimodal learning. We first contextualise\nthe significance of music in various industries and trace the evolution of AI\nin music. By delineating the modalities targeted by foundation models, we\ndiscover many of the music representations are underexplored in FM development.\nThen, emphasis is placed on the lack of versatility of previous methods on\ndiverse music applications, along with the potential of FMs in music\nunderstanding, generation and medical application. By comprehensively exploring\nthe details of the model pre-training paradigm, architectural choices,\ntokenisation, finetuning methodologies and controllability, we emphasise the\nimportant topics that should have been well explored, like instruction tuning\nand in-context learning, scaling law and emergent ability, as well as\nlong-sequence modelling etc. A dedicated section presents insights into music\nagents, accompanied by a thorough analysis of datasets and evaluations\nessential for pre-training and downstream tasks. Finally, by underscoring the\nvital importance of ethical considerations, we advocate that following research\non FM for music should focus more on such issues as interpretability,\ntransparency, human responsibility, and copyright issues. The paper offers\ninsights into future challenges and trends on FMs for music, aiming to shape\nthe trajectory of human-AI collaboration in the music realm.\n", "link": "http://arxiv.org/abs/2408.14340v3", "date": "2024-09-03", "relevancy": 1.3924, "topK": [{"title": "Scaling Rectified Flow Transformers for High-Resolution Image Synthesis", "link": "http://arxiv.org/abs/2403.03206v1", "similarity": 0.4726}, {"title": "Multimodal-Conditioned Latent Diffusion Models for Fashion Image Editing", "link": "http://arxiv.org/abs/2403.14828v2", "similarity": 0.4677}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.4594}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Foundation%20Models%20for%20Music%3A%20A%20Survey&body=Title%3A%20Foundation%20Models%20for%20Music%3A%20A%20Survey%0AAuthor%3A%20Yinghao%20Ma%20and%20Anders%20%C3%98land%20and%20Anton%20Ragni%20and%20Bleiz%20MacSen%20Del%20Sette%20and%20Charalampos%20Saitis%20and%20Chris%20Donahue%20and%20Chenghua%20Lin%20and%20Christos%20Plachouras%20and%20Emmanouil%20Benetos%20and%20Elona%20Shatri%20and%20Fabio%20Morreale%20and%20Ge%20Zhang%20and%20Gy%C3%B6rgy%20Fazekas%20and%20Gus%20Xia%20and%20Huan%20Zhang%20and%20Ilaria%20Manco%20and%20Jiawen%20Huang%20and%20Julien%20Guinot%20and%20Liwei%20Lin%20and%20Luca%20Marinelli%20and%20Max%20W.%20Y.%20Lam%20and%20Megha%20Sharma%20and%20Qiuqiang%20Kong%20and%20Roger%20B.%20Dannenberg%20and%20Ruibin%20Yuan%20and%20Shangda%20Wu%20and%20Shih-Lun%20Wu%20and%20Shuqi%20Dai%20and%20Shun%20Lei%20and%20Shiyin%20Kang%20and%20Simon%20Dixon%20and%20Wenhu%20Chen%20and%20Wenhao%20Huang%20and%20Xingjian%20Du%20and%20Xingwei%20Qu%20and%20Xu%20Tan%20and%20Yizhi%20Li%20and%20Zeyue%20Tian%20and%20Zhiyong%20Wu%20and%20Zhizheng%20Wu%20and%20Ziyang%20Ma%20and%20Ziyu%20Wang%0AAbstract%3A%20%20%20In%20recent%20years%2C%20foundation%20models%20%28FMs%29%20such%20as%20large%20language%20models%20%28LLMs%29%0Aand%20latent%20diffusion%20models%20%28LDMs%29%20have%20profoundly%20impacted%20diverse%20sectors%2C%0Aincluding%20music.%20This%20comprehensive%20review%20examines%20state-of-the-art%20%28SOTA%29%0Apre-trained%20models%20and%20foundation%20models%20in%20music%2C%20spanning%20from%20representation%0Alearning%2C%20generative%20learning%20and%20multimodal%20learning.%20We%20first%20contextualise%0Athe%20significance%20of%20music%20in%20various%20industries%20and%20trace%20the%20evolution%20of%20AI%0Ain%20music.%20By%20delineating%20the%20modalities%20targeted%20by%20foundation%20models%2C%20we%0Adiscover%20many%20of%20the%20music%20representations%20are%20underexplored%20in%20FM%20development.%0AThen%2C%20emphasis%20is%20placed%20on%20the%20lack%20of%20versatility%20of%20previous%20methods%20on%0Adiverse%20music%20applications%2C%20along%20with%20the%20potential%20of%20FMs%20in%20music%0Aunderstanding%2C%20generation%20and%20medical%20application.%20By%20comprehensively%20exploring%0Athe%20details%20of%20the%20model%20pre-training%20paradigm%2C%20architectural%20choices%2C%0Atokenisation%2C%20finetuning%20methodologies%20and%20controllability%2C%20we%20emphasise%20the%0Aimportant%20topics%20that%20should%20have%20been%20well%20explored%2C%20like%20instruction%20tuning%0Aand%20in-context%20learning%2C%20scaling%20law%20and%20emergent%20ability%2C%20as%20well%20as%0Along-sequence%20modelling%20etc.%20A%20dedicated%20section%20presents%20insights%20into%20music%0Aagents%2C%20accompanied%20by%20a%20thorough%20analysis%20of%20datasets%20and%20evaluations%0Aessential%20for%20pre-training%20and%20downstream%20tasks.%20Finally%2C%20by%20underscoring%20the%0Avital%20importance%20of%20ethical%20considerations%2C%20we%20advocate%20that%20following%20research%0Aon%20FM%20for%20music%20should%20focus%20more%20on%20such%20issues%20as%20interpretability%2C%0Atransparency%2C%20human%20responsibility%2C%20and%20copyright%20issues.%20The%20paper%20offers%0Ainsights%20into%20future%20challenges%20and%20trends%20on%20FMs%20for%20music%2C%20aiming%20to%20shape%0Athe%20trajectory%20of%20human-AI%20collaboration%20in%20the%20music%20realm.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2408.14340v3%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DFoundation%2520Models%2520for%2520Music%253A%2520A%2520Survey%26entry.906535625%3DYinghao%2520Ma%2520and%2520Anders%2520%25C3%2598land%2520and%2520Anton%2520Ragni%2520and%2520Bleiz%2520MacSen%2520Del%2520Sette%2520and%2520Charalampos%2520Saitis%2520and%2520Chris%2520Donahue%2520and%2520Chenghua%2520Lin%2520and%2520Christos%2520Plachouras%2520and%2520Emmanouil%2520Benetos%2520and%2520Elona%2520Shatri%2520and%2520Fabio%2520Morreale%2520and%2520Ge%2520Zhang%2520and%2520Gy%25C3%25B6rgy%2520Fazekas%2520and%2520Gus%2520Xia%2520and%2520Huan%2520Zhang%2520and%2520Ilaria%2520Manco%2520and%2520Jiawen%2520Huang%2520and%2520Julien%2520Guinot%2520and%2520Liwei%2520Lin%2520and%2520Luca%2520Marinelli%2520and%2520Max%2520W.%2520Y.%2520Lam%2520and%2520Megha%2520Sharma%2520and%2520Qiuqiang%2520Kong%2520and%2520Roger%2520B.%2520Dannenberg%2520and%2520Ruibin%2520Yuan%2520and%2520Shangda%2520Wu%2520and%2520Shih-Lun%2520Wu%2520and%2520Shuqi%2520Dai%2520and%2520Shun%2520Lei%2520and%2520Shiyin%2520Kang%2520and%2520Simon%2520Dixon%2520and%2520Wenhu%2520Chen%2520and%2520Wenhao%2520Huang%2520and%2520Xingjian%2520Du%2520and%2520Xingwei%2520Qu%2520and%2520Xu%2520Tan%2520and%2520Yizhi%2520Li%2520and%2520Zeyue%2520Tian%2520and%2520Zhiyong%2520Wu%2520and%2520Zhizheng%2520Wu%2520and%2520Ziyang%2520Ma%2520and%2520Ziyu%2520Wang%26entry.1292438233%3D%2520%2520In%2520recent%2520years%252C%2520foundation%2520models%2520%2528FMs%2529%2520such%2520as%2520large%2520language%2520models%2520%2528LLMs%2529%250Aand%2520latent%2520diffusion%2520models%2520%2528LDMs%2529%2520have%2520profoundly%2520impacted%2520diverse%2520sectors%252C%250Aincluding%2520music.%2520This%2520comprehensive%2520review%2520examines%2520state-of-the-art%2520%2528SOTA%2529%250Apre-trained%2520models%2520and%2520foundation%2520models%2520in%2520music%252C%2520spanning%2520from%2520representation%250Alearning%252C%2520generative%2520learning%2520and%2520multimodal%2520learning.%2520We%2520first%2520contextualise%250Athe%2520significance%2520of%2520music%2520in%2520various%2520industries%2520and%2520trace%2520the%2520evolution%2520of%2520AI%250Ain%2520music.%2520By%2520delineating%2520the%2520modalities%2520targeted%2520by%2520foundation%2520models%252C%2520we%250Adiscover%2520many%2520of%2520the%2520music%2520representations%2520are%2520underexplored%2520in%2520FM%2520development.%250AThen%252C%2520emphasis%2520is%2520placed%2520on%2520the%2520lack%2520of%2520versatility%2520of%2520previous%2520methods%2520on%250Adiverse%2520music%2520applications%252C%2520along%2520with%2520the%2520potential%2520of%2520FMs%2520in%2520music%250Aunderstanding%252C%2520generation%2520and%2520medical%2520application.%2520By%2520comprehensively%2520exploring%250Athe%2520details%2520of%2520the%2520model%2520pre-training%2520paradigm%252C%2520architectural%2520choices%252C%250Atokenisation%252C%2520finetuning%2520methodologies%2520and%2520controllability%252C%2520we%2520emphasise%2520the%250Aimportant%2520topics%2520that%2520should%2520have%2520been%2520well%2520explored%252C%2520like%2520instruction%2520tuning%250Aand%2520in-context%2520learning%252C%2520scaling%2520law%2520and%2520emergent%2520ability%252C%2520as%2520well%2520as%250Along-sequence%2520modelling%2520etc.%2520A%2520dedicated%2520section%2520presents%2520insights%2520into%2520music%250Aagents%252C%2520accompanied%2520by%2520a%2520thorough%2520analysis%2520of%2520datasets%2520and%2520evaluations%250Aessential%2520for%2520pre-training%2520and%2520downstream%2520tasks.%2520Finally%252C%2520by%2520underscoring%2520the%250Avital%2520importance%2520of%2520ethical%2520considerations%252C%2520we%2520advocate%2520that%2520following%2520research%250Aon%2520FM%2520for%2520music%2520should%2520focus%2520more%2520on%2520such%2520issues%2520as%2520interpretability%252C%250Atransparency%252C%2520human%2520responsibility%252C%2520and%2520copyright%2520issues.%2520The%2520paper%2520offers%250Ainsights%2520into%2520future%2520challenges%2520and%2520trends%2520on%2520FMs%2520for%2520music%252C%2520aiming%2520to%2520shape%250Athe%2520trajectory%2520of%2520human-AI%2520collaboration%2520in%2520the%2520music%2520realm.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2408.14340v3%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Foundation%20Models%20for%20Music%3A%20A%20Survey&entry.906535625=Yinghao%20Ma%20and%20Anders%20%C3%98land%20and%20Anton%20Ragni%20and%20Bleiz%20MacSen%20Del%20Sette%20and%20Charalampos%20Saitis%20and%20Chris%20Donahue%20and%20Chenghua%20Lin%20and%20Christos%20Plachouras%20and%20Emmanouil%20Benetos%20and%20Elona%20Shatri%20and%20Fabio%20Morreale%20and%20Ge%20Zhang%20and%20Gy%C3%B6rgy%20Fazekas%20and%20Gus%20Xia%20and%20Huan%20Zhang%20and%20Ilaria%20Manco%20and%20Jiawen%20Huang%20and%20Julien%20Guinot%20and%20Liwei%20Lin%20and%20Luca%20Marinelli%20and%20Max%20W.%20Y.%20Lam%20and%20Megha%20Sharma%20and%20Qiuqiang%20Kong%20and%20Roger%20B.%20Dannenberg%20and%20Ruibin%20Yuan%20and%20Shangda%20Wu%20and%20Shih-Lun%20Wu%20and%20Shuqi%20Dai%20and%20Shun%20Lei%20and%20Shiyin%20Kang%20and%20Simon%20Dixon%20and%20Wenhu%20Chen%20and%20Wenhao%20Huang%20and%20Xingjian%20Du%20and%20Xingwei%20Qu%20and%20Xu%20Tan%20and%20Yizhi%20Li%20and%20Zeyue%20Tian%20and%20Zhiyong%20Wu%20and%20Zhizheng%20Wu%20and%20Ziyang%20Ma%20and%20Ziyu%20Wang&entry.1292438233=%20%20In%20recent%20years%2C%20foundation%20models%20%28FMs%29%20such%20as%20large%20language%20models%20%28LLMs%29%0Aand%20latent%20diffusion%20models%20%28LDMs%29%20have%20profoundly%20impacted%20diverse%20sectors%2C%0Aincluding%20music.%20This%20comprehensive%20review%20examines%20state-of-the-art%20%28SOTA%29%0Apre-trained%20models%20and%20foundation%20models%20in%20music%2C%20spanning%20from%20representation%0Alearning%2C%20generative%20learning%20and%20multimodal%20learning.%20We%20first%20contextualise%0Athe%20significance%20of%20music%20in%20various%20industries%20and%20trace%20the%20evolution%20of%20AI%0Ain%20music.%20By%20delineating%20the%20modalities%20targeted%20by%20foundation%20models%2C%20we%0Adiscover%20many%20of%20the%20music%20representations%20are%20underexplored%20in%20FM%20development.%0AThen%2C%20emphasis%20is%20placed%20on%20the%20lack%20of%20versatility%20of%20previous%20methods%20on%0Adiverse%20music%20applications%2C%20along%20with%20the%20potential%20of%20FMs%20in%20music%0Aunderstanding%2C%20generation%20and%20medical%20application.%20By%20comprehensively%20exploring%0Athe%20details%20of%20the%20model%20pre-training%20paradigm%2C%20architectural%20choices%2C%0Atokenisation%2C%20finetuning%20methodologies%20and%20controllability%2C%20we%20emphasise%20the%0Aimportant%20topics%20that%20should%20have%20been%20well%20explored%2C%20like%20instruction%20tuning%0Aand%20in-context%20learning%2C%20scaling%20law%20and%20emergent%20ability%2C%20as%20well%20as%0Along-sequence%20modelling%20etc.%20A%20dedicated%20section%20presents%20insights%20into%20music%0Aagents%2C%20accompanied%20by%20a%20thorough%20analysis%20of%20datasets%20and%20evaluations%0Aessential%20for%20pre-training%20and%20downstream%20tasks.%20Finally%2C%20by%20underscoring%20the%0Avital%20importance%20of%20ethical%20considerations%2C%20we%20advocate%20that%20following%20research%0Aon%20FM%20for%20music%20should%20focus%20more%20on%20such%20issues%20as%20interpretability%2C%0Atransparency%2C%20human%20responsibility%2C%20and%20copyright%20issues.%20The%20paper%20offers%0Ainsights%20into%20future%20challenges%20and%20trends%20on%20FMs%20for%20music%2C%20aiming%20to%20shape%0Athe%20trajectory%20of%20human-AI%20collaboration%20in%20the%20music%20realm.%0A&entry.1838667208=http%3A//arxiv.org/abs/2408.14340v3&entry.124074799=Read"},
{"title": "FairX: A comprehensive benchmarking tool for model analysis using\n  fairness, utility, and explainability", "author": "Md Fahim Sikder and Resmi Ramachandranpillai and Daniel de Leng and Fredrik Heintz", "abstract": "  We present FairX, an open-source Python-based benchmarking tool designed for\nthe comprehensive analysis of models under the umbrella of fairness, utility,\nand eXplainability (XAI). FairX enables users to train benchmarking\nbias-mitigation models and evaluate their fairness using a wide array of\nfairness metrics, data utility metrics, and generate explanations for model\npredictions, all within a unified framework. Existing benchmarking tools do not\nhave the way to evaluate synthetic data generated from fair generative models,\nalso they do not have the support for training fair generative models either.\nIn FairX, we add fair generative models in the collection of our fair-model\nlibrary (pre-processing, in-processing, post-processing) and evaluation metrics\nfor evaluating the quality of synthetic fair data. This version of FairX\nsupports both tabular and image datasets. It also allows users to provide their\nown custom datasets. The open-source FairX benchmarking package is publicly\navailable at \\url{https://github.com/fahim-sikder/FairX}.\n", "link": "http://arxiv.org/abs/2406.14281v4", "date": "2024-09-03", "relevancy": 1.3892, "topK": [{"title": "Scaling Rectified Flow Transformers for High-Resolution Image Synthesis", "link": "http://arxiv.org/abs/2403.03206v1", "similarity": 0.465}, {"title": "UGG: Unified Generative Grasping", "link": "https://arxiv.org/abs/2311.16917", "similarity": 0.4631}, {"title": "SDXS: Real-Time One-Step Latent Diffusion Models with Image Conditions", "link": "http://arxiv.org/abs/2403.16627v1", "similarity": 0.4611}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20FairX%3A%20A%20comprehensive%20benchmarking%20tool%20for%20model%20analysis%20using%0A%20%20fairness%2C%20utility%2C%20and%20explainability&body=Title%3A%20FairX%3A%20A%20comprehensive%20benchmarking%20tool%20for%20model%20analysis%20using%0A%20%20fairness%2C%20utility%2C%20and%20explainability%0AAuthor%3A%20Md%20Fahim%20Sikder%20and%20Resmi%20Ramachandranpillai%20and%20Daniel%20de%20Leng%20and%20Fredrik%20Heintz%0AAbstract%3A%20%20%20We%20present%20FairX%2C%20an%20open-source%20Python-based%20benchmarking%20tool%20designed%20for%0Athe%20comprehensive%20analysis%20of%20models%20under%20the%20umbrella%20of%20fairness%2C%20utility%2C%0Aand%20eXplainability%20%28XAI%29.%20FairX%20enables%20users%20to%20train%20benchmarking%0Abias-mitigation%20models%20and%20evaluate%20their%20fairness%20using%20a%20wide%20array%20of%0Afairness%20metrics%2C%20data%20utility%20metrics%2C%20and%20generate%20explanations%20for%20model%0Apredictions%2C%20all%20within%20a%20unified%20framework.%20Existing%20benchmarking%20tools%20do%20not%0Ahave%20the%20way%20to%20evaluate%20synthetic%20data%20generated%20from%20fair%20generative%20models%2C%0Aalso%20they%20do%20not%20have%20the%20support%20for%20training%20fair%20generative%20models%20either.%0AIn%20FairX%2C%20we%20add%20fair%20generative%20models%20in%20the%20collection%20of%20our%20fair-model%0Alibrary%20%28pre-processing%2C%20in-processing%2C%20post-processing%29%20and%20evaluation%20metrics%0Afor%20evaluating%20the%20quality%20of%20synthetic%20fair%20data.%20This%20version%20of%20FairX%0Asupports%20both%20tabular%20and%20image%20datasets.%20It%20also%20allows%20users%20to%20provide%20their%0Aown%20custom%20datasets.%20The%20open-source%20FairX%20benchmarking%20package%20is%20publicly%0Aavailable%20at%20%5Curl%7Bhttps%3A//github.com/fahim-sikder/FairX%7D.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2406.14281v4%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DFairX%253A%2520A%2520comprehensive%2520benchmarking%2520tool%2520for%2520model%2520analysis%2520using%250A%2520%2520fairness%252C%2520utility%252C%2520and%2520explainability%26entry.906535625%3DMd%2520Fahim%2520Sikder%2520and%2520Resmi%2520Ramachandranpillai%2520and%2520Daniel%2520de%2520Leng%2520and%2520Fredrik%2520Heintz%26entry.1292438233%3D%2520%2520We%2520present%2520FairX%252C%2520an%2520open-source%2520Python-based%2520benchmarking%2520tool%2520designed%2520for%250Athe%2520comprehensive%2520analysis%2520of%2520models%2520under%2520the%2520umbrella%2520of%2520fairness%252C%2520utility%252C%250Aand%2520eXplainability%2520%2528XAI%2529.%2520FairX%2520enables%2520users%2520to%2520train%2520benchmarking%250Abias-mitigation%2520models%2520and%2520evaluate%2520their%2520fairness%2520using%2520a%2520wide%2520array%2520of%250Afairness%2520metrics%252C%2520data%2520utility%2520metrics%252C%2520and%2520generate%2520explanations%2520for%2520model%250Apredictions%252C%2520all%2520within%2520a%2520unified%2520framework.%2520Existing%2520benchmarking%2520tools%2520do%2520not%250Ahave%2520the%2520way%2520to%2520evaluate%2520synthetic%2520data%2520generated%2520from%2520fair%2520generative%2520models%252C%250Aalso%2520they%2520do%2520not%2520have%2520the%2520support%2520for%2520training%2520fair%2520generative%2520models%2520either.%250AIn%2520FairX%252C%2520we%2520add%2520fair%2520generative%2520models%2520in%2520the%2520collection%2520of%2520our%2520fair-model%250Alibrary%2520%2528pre-processing%252C%2520in-processing%252C%2520post-processing%2529%2520and%2520evaluation%2520metrics%250Afor%2520evaluating%2520the%2520quality%2520of%2520synthetic%2520fair%2520data.%2520This%2520version%2520of%2520FairX%250Asupports%2520both%2520tabular%2520and%2520image%2520datasets.%2520It%2520also%2520allows%2520users%2520to%2520provide%2520their%250Aown%2520custom%2520datasets.%2520The%2520open-source%2520FairX%2520benchmarking%2520package%2520is%2520publicly%250Aavailable%2520at%2520%255Curl%257Bhttps%253A//github.com/fahim-sikder/FairX%257D.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2406.14281v4%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=FairX%3A%20A%20comprehensive%20benchmarking%20tool%20for%20model%20analysis%20using%0A%20%20fairness%2C%20utility%2C%20and%20explainability&entry.906535625=Md%20Fahim%20Sikder%20and%20Resmi%20Ramachandranpillai%20and%20Daniel%20de%20Leng%20and%20Fredrik%20Heintz&entry.1292438233=%20%20We%20present%20FairX%2C%20an%20open-source%20Python-based%20benchmarking%20tool%20designed%20for%0Athe%20comprehensive%20analysis%20of%20models%20under%20the%20umbrella%20of%20fairness%2C%20utility%2C%0Aand%20eXplainability%20%28XAI%29.%20FairX%20enables%20users%20to%20train%20benchmarking%0Abias-mitigation%20models%20and%20evaluate%20their%20fairness%20using%20a%20wide%20array%20of%0Afairness%20metrics%2C%20data%20utility%20metrics%2C%20and%20generate%20explanations%20for%20model%0Apredictions%2C%20all%20within%20a%20unified%20framework.%20Existing%20benchmarking%20tools%20do%20not%0Ahave%20the%20way%20to%20evaluate%20synthetic%20data%20generated%20from%20fair%20generative%20models%2C%0Aalso%20they%20do%20not%20have%20the%20support%20for%20training%20fair%20generative%20models%20either.%0AIn%20FairX%2C%20we%20add%20fair%20generative%20models%20in%20the%20collection%20of%20our%20fair-model%0Alibrary%20%28pre-processing%2C%20in-processing%2C%20post-processing%29%20and%20evaluation%20metrics%0Afor%20evaluating%20the%20quality%20of%20synthetic%20fair%20data.%20This%20version%20of%20FairX%0Asupports%20both%20tabular%20and%20image%20datasets.%20It%20also%20allows%20users%20to%20provide%20their%0Aown%20custom%20datasets.%20The%20open-source%20FairX%20benchmarking%20package%20is%20publicly%0Aavailable%20at%20%5Curl%7Bhttps%3A//github.com/fahim-sikder/FairX%7D.%0A&entry.1838667208=http%3A//arxiv.org/abs/2406.14281v4&entry.124074799=Read"},
{"title": "Chemical Reaction Neural Networks for Fitting Accelerating Rate\n  Calorimetry Data", "author": "Saakaar Bhatnagar and Andrew Comerford and Zelu Xu and Davide Berti Polato and Araz Banaeizadeh and Alessandro Ferraris", "abstract": "  As the demand for lithium-ion batteries rapidly increases there is a need to\ndesign these cells in a safe manner to mitigate thermal runaway. Thermal\nrunaway in batteries leads to an uncontrollable temperature rise and\npotentially fires, which is a major safety concern. Typically, when modelling\nthe chemical kinetics of thermal runaway calorimetry data ( e.g. Accelerating\nRate Calorimetry (ARC)) is needed to determine the temperature-driven\ndecomposition kinetics. Conventional methods of fitting Arrhenius Ordinary\nDifferential Equation (ODE) thermal runaway models to Accelerated Rate\nCalorimetry (ARC) data make several assumptions that reduce the fidelity and\ngeneralizability of the obtained model. In this paper, Chemical Reaction Neural\nNetworks (CRNNs) are trained to fit the kinetic parameters of N-equation\nArrhenius ODEs to ARC data obtained from a Molicel 21700 P45B. The models are\nfound to be better approximations of the experimental data. The flexibility of\nthe method is demonstrated by experimenting with two-equation and four-equation\nmodels. Thermal runaway simulations are conducted in 3D using the obtained\nkinetic parameters, showing the applicability of the obtained thermal runaway\nmodels to large-scale simulations.\n", "link": "http://arxiv.org/abs/2408.11984v2", "date": "2024-09-03", "relevancy": 1.3476, "topK": [{"title": "Boosted dynamic neural networks", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/26302", "similarity": 0.4552}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.4506}, {"title": "SDXS: Real-Time One-Step Latent Diffusion Models with Image Conditions", "link": "http://arxiv.org/abs/2403.16627v1", "similarity": 0.4327}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Chemical%20Reaction%20Neural%20Networks%20for%20Fitting%20Accelerating%20Rate%0A%20%20Calorimetry%20Data&body=Title%3A%20Chemical%20Reaction%20Neural%20Networks%20for%20Fitting%20Accelerating%20Rate%0A%20%20Calorimetry%20Data%0AAuthor%3A%20Saakaar%20Bhatnagar%20and%20Andrew%20Comerford%20and%20Zelu%20Xu%20and%20Davide%20Berti%20Polato%20and%20Araz%20Banaeizadeh%20and%20Alessandro%20Ferraris%0AAbstract%3A%20%20%20As%20the%20demand%20for%20lithium-ion%20batteries%20rapidly%20increases%20there%20is%20a%20need%20to%0Adesign%20these%20cells%20in%20a%20safe%20manner%20to%20mitigate%20thermal%20runaway.%20Thermal%0Arunaway%20in%20batteries%20leads%20to%20an%20uncontrollable%20temperature%20rise%20and%0Apotentially%20fires%2C%20which%20is%20a%20major%20safety%20concern.%20Typically%2C%20when%20modelling%0Athe%20chemical%20kinetics%20of%20thermal%20runaway%20calorimetry%20data%20%28%20e.g.%20Accelerating%0ARate%20Calorimetry%20%28ARC%29%29%20is%20needed%20to%20determine%20the%20temperature-driven%0Adecomposition%20kinetics.%20Conventional%20methods%20of%20fitting%20Arrhenius%20Ordinary%0ADifferential%20Equation%20%28ODE%29%20thermal%20runaway%20models%20to%20Accelerated%20Rate%0ACalorimetry%20%28ARC%29%20data%20make%20several%20assumptions%20that%20reduce%20the%20fidelity%20and%0Ageneralizability%20of%20the%20obtained%20model.%20In%20this%20paper%2C%20Chemical%20Reaction%20Neural%0ANetworks%20%28CRNNs%29%20are%20trained%20to%20fit%20the%20kinetic%20parameters%20of%20N-equation%0AArrhenius%20ODEs%20to%20ARC%20data%20obtained%20from%20a%20Molicel%2021700%20P45B.%20The%20models%20are%0Afound%20to%20be%20better%20approximations%20of%20the%20experimental%20data.%20The%20flexibility%20of%0Athe%20method%20is%20demonstrated%20by%20experimenting%20with%20two-equation%20and%20four-equation%0Amodels.%20Thermal%20runaway%20simulations%20are%20conducted%20in%203D%20using%20the%20obtained%0Akinetic%20parameters%2C%20showing%20the%20applicability%20of%20the%20obtained%20thermal%20runaway%0Amodels%20to%20large-scale%20simulations.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2408.11984v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DChemical%2520Reaction%2520Neural%2520Networks%2520for%2520Fitting%2520Accelerating%2520Rate%250A%2520%2520Calorimetry%2520Data%26entry.906535625%3DSaakaar%2520Bhatnagar%2520and%2520Andrew%2520Comerford%2520and%2520Zelu%2520Xu%2520and%2520Davide%2520Berti%2520Polato%2520and%2520Araz%2520Banaeizadeh%2520and%2520Alessandro%2520Ferraris%26entry.1292438233%3D%2520%2520As%2520the%2520demand%2520for%2520lithium-ion%2520batteries%2520rapidly%2520increases%2520there%2520is%2520a%2520need%2520to%250Adesign%2520these%2520cells%2520in%2520a%2520safe%2520manner%2520to%2520mitigate%2520thermal%2520runaway.%2520Thermal%250Arunaway%2520in%2520batteries%2520leads%2520to%2520an%2520uncontrollable%2520temperature%2520rise%2520and%250Apotentially%2520fires%252C%2520which%2520is%2520a%2520major%2520safety%2520concern.%2520Typically%252C%2520when%2520modelling%250Athe%2520chemical%2520kinetics%2520of%2520thermal%2520runaway%2520calorimetry%2520data%2520%2528%2520e.g.%2520Accelerating%250ARate%2520Calorimetry%2520%2528ARC%2529%2529%2520is%2520needed%2520to%2520determine%2520the%2520temperature-driven%250Adecomposition%2520kinetics.%2520Conventional%2520methods%2520of%2520fitting%2520Arrhenius%2520Ordinary%250ADifferential%2520Equation%2520%2528ODE%2529%2520thermal%2520runaway%2520models%2520to%2520Accelerated%2520Rate%250ACalorimetry%2520%2528ARC%2529%2520data%2520make%2520several%2520assumptions%2520that%2520reduce%2520the%2520fidelity%2520and%250Ageneralizability%2520of%2520the%2520obtained%2520model.%2520In%2520this%2520paper%252C%2520Chemical%2520Reaction%2520Neural%250ANetworks%2520%2528CRNNs%2529%2520are%2520trained%2520to%2520fit%2520the%2520kinetic%2520parameters%2520of%2520N-equation%250AArrhenius%2520ODEs%2520to%2520ARC%2520data%2520obtained%2520from%2520a%2520Molicel%252021700%2520P45B.%2520The%2520models%2520are%250Afound%2520to%2520be%2520better%2520approximations%2520of%2520the%2520experimental%2520data.%2520The%2520flexibility%2520of%250Athe%2520method%2520is%2520demonstrated%2520by%2520experimenting%2520with%2520two-equation%2520and%2520four-equation%250Amodels.%2520Thermal%2520runaway%2520simulations%2520are%2520conducted%2520in%25203D%2520using%2520the%2520obtained%250Akinetic%2520parameters%252C%2520showing%2520the%2520applicability%2520of%2520the%2520obtained%2520thermal%2520runaway%250Amodels%2520to%2520large-scale%2520simulations.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2408.11984v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Chemical%20Reaction%20Neural%20Networks%20for%20Fitting%20Accelerating%20Rate%0A%20%20Calorimetry%20Data&entry.906535625=Saakaar%20Bhatnagar%20and%20Andrew%20Comerford%20and%20Zelu%20Xu%20and%20Davide%20Berti%20Polato%20and%20Araz%20Banaeizadeh%20and%20Alessandro%20Ferraris&entry.1292438233=%20%20As%20the%20demand%20for%20lithium-ion%20batteries%20rapidly%20increases%20there%20is%20a%20need%20to%0Adesign%20these%20cells%20in%20a%20safe%20manner%20to%20mitigate%20thermal%20runaway.%20Thermal%0Arunaway%20in%20batteries%20leads%20to%20an%20uncontrollable%20temperature%20rise%20and%0Apotentially%20fires%2C%20which%20is%20a%20major%20safety%20concern.%20Typically%2C%20when%20modelling%0Athe%20chemical%20kinetics%20of%20thermal%20runaway%20calorimetry%20data%20%28%20e.g.%20Accelerating%0ARate%20Calorimetry%20%28ARC%29%29%20is%20needed%20to%20determine%20the%20temperature-driven%0Adecomposition%20kinetics.%20Conventional%20methods%20of%20fitting%20Arrhenius%20Ordinary%0ADifferential%20Equation%20%28ODE%29%20thermal%20runaway%20models%20to%20Accelerated%20Rate%0ACalorimetry%20%28ARC%29%20data%20make%20several%20assumptions%20that%20reduce%20the%20fidelity%20and%0Ageneralizability%20of%20the%20obtained%20model.%20In%20this%20paper%2C%20Chemical%20Reaction%20Neural%0ANetworks%20%28CRNNs%29%20are%20trained%20to%20fit%20the%20kinetic%20parameters%20of%20N-equation%0AArrhenius%20ODEs%20to%20ARC%20data%20obtained%20from%20a%20Molicel%2021700%20P45B.%20The%20models%20are%0Afound%20to%20be%20better%20approximations%20of%20the%20experimental%20data.%20The%20flexibility%20of%0Athe%20method%20is%20demonstrated%20by%20experimenting%20with%20two-equation%20and%20four-equation%0Amodels.%20Thermal%20runaway%20simulations%20are%20conducted%20in%203D%20using%20the%20obtained%0Akinetic%20parameters%2C%20showing%20the%20applicability%20of%20the%20obtained%20thermal%20runaway%0Amodels%20to%20large-scale%20simulations.%0A&entry.1838667208=http%3A//arxiv.org/abs/2408.11984v2&entry.124074799=Read"},
{"title": "rerankers: A Lightweight Python Library to Unify Ranking Methods", "author": "Benjamin Clavi\u00e9", "abstract": "  This paper presents rerankers, a Python library which provides an easy-to-use\ninterface to the most commonly used re-ranking approaches. Re-ranking is an\nintegral component of many retrieval pipelines; however, there exist numerous\napproaches to it, relying on different implementation methods. rerankers\nunifies these methods into a single user-friendly interface, allowing\npractitioners and researchers alike to explore different methods while only\nchanging a single line of Python code. Moreover ,rerankers ensures that its\nimplementations are done with the fewest dependencies possible, and re-uses the\noriginal implementation whenever possible, guaranteeing that our simplified\ninterface results in no performance degradation compared to more complex ones.\nThe full source code and list of supported models are updated regularly and\navailable at https://github.com/answerdotai/rerankers.\n", "link": "http://arxiv.org/abs/2408.17344v2", "date": "2024-09-03", "relevancy": 1.1452, "topK": [{"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.3941}, {"title": "Scaling Rectified Flow Transformers for High-Resolution Image Synthesis", "link": "http://arxiv.org/abs/2403.03206v1", "similarity": 0.3873}, {"title": "Breadcrumbs: Adversarial class-balanced sampling for long-tailed recognition", "link": "https://link.springer.com/chapter/10.1007/978-3-031-20053-3_37", "similarity": 0.3746}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20rerankers%3A%20A%20Lightweight%20Python%20Library%20to%20Unify%20Ranking%20Methods&body=Title%3A%20rerankers%3A%20A%20Lightweight%20Python%20Library%20to%20Unify%20Ranking%20Methods%0AAuthor%3A%20Benjamin%20Clavi%C3%A9%0AAbstract%3A%20%20%20This%20paper%20presents%20rerankers%2C%20a%20Python%20library%20which%20provides%20an%20easy-to-use%0Ainterface%20to%20the%20most%20commonly%20used%20re-ranking%20approaches.%20Re-ranking%20is%20an%0Aintegral%20component%20of%20many%20retrieval%20pipelines%3B%20however%2C%20there%20exist%20numerous%0Aapproaches%20to%20it%2C%20relying%20on%20different%20implementation%20methods.%20rerankers%0Aunifies%20these%20methods%20into%20a%20single%20user-friendly%20interface%2C%20allowing%0Apractitioners%20and%20researchers%20alike%20to%20explore%20different%20methods%20while%20only%0Achanging%20a%20single%20line%20of%20Python%20code.%20Moreover%20%2Crerankers%20ensures%20that%20its%0Aimplementations%20are%20done%20with%20the%20fewest%20dependencies%20possible%2C%20and%20re-uses%20the%0Aoriginal%20implementation%20whenever%20possible%2C%20guaranteeing%20that%20our%20simplified%0Ainterface%20results%20in%20no%20performance%20degradation%20compared%20to%20more%20complex%20ones.%0AThe%20full%20source%20code%20and%20list%20of%20supported%20models%20are%20updated%20regularly%20and%0Aavailable%20at%20https%3A//github.com/answerdotai/rerankers.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2408.17344v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3Drerankers%253A%2520A%2520Lightweight%2520Python%2520Library%2520to%2520Unify%2520Ranking%2520Methods%26entry.906535625%3DBenjamin%2520Clavi%25C3%25A9%26entry.1292438233%3D%2520%2520This%2520paper%2520presents%2520rerankers%252C%2520a%2520Python%2520library%2520which%2520provides%2520an%2520easy-to-use%250Ainterface%2520to%2520the%2520most%2520commonly%2520used%2520re-ranking%2520approaches.%2520Re-ranking%2520is%2520an%250Aintegral%2520component%2520of%2520many%2520retrieval%2520pipelines%253B%2520however%252C%2520there%2520exist%2520numerous%250Aapproaches%2520to%2520it%252C%2520relying%2520on%2520different%2520implementation%2520methods.%2520rerankers%250Aunifies%2520these%2520methods%2520into%2520a%2520single%2520user-friendly%2520interface%252C%2520allowing%250Apractitioners%2520and%2520researchers%2520alike%2520to%2520explore%2520different%2520methods%2520while%2520only%250Achanging%2520a%2520single%2520line%2520of%2520Python%2520code.%2520Moreover%2520%252Crerankers%2520ensures%2520that%2520its%250Aimplementations%2520are%2520done%2520with%2520the%2520fewest%2520dependencies%2520possible%252C%2520and%2520re-uses%2520the%250Aoriginal%2520implementation%2520whenever%2520possible%252C%2520guaranteeing%2520that%2520our%2520simplified%250Ainterface%2520results%2520in%2520no%2520performance%2520degradation%2520compared%2520to%2520more%2520complex%2520ones.%250AThe%2520full%2520source%2520code%2520and%2520list%2520of%2520supported%2520models%2520are%2520updated%2520regularly%2520and%250Aavailable%2520at%2520https%253A//github.com/answerdotai/rerankers.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2408.17344v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=rerankers%3A%20A%20Lightweight%20Python%20Library%20to%20Unify%20Ranking%20Methods&entry.906535625=Benjamin%20Clavi%C3%A9&entry.1292438233=%20%20This%20paper%20presents%20rerankers%2C%20a%20Python%20library%20which%20provides%20an%20easy-to-use%0Ainterface%20to%20the%20most%20commonly%20used%20re-ranking%20approaches.%20Re-ranking%20is%20an%0Aintegral%20component%20of%20many%20retrieval%20pipelines%3B%20however%2C%20there%20exist%20numerous%0Aapproaches%20to%20it%2C%20relying%20on%20different%20implementation%20methods.%20rerankers%0Aunifies%20these%20methods%20into%20a%20single%20user-friendly%20interface%2C%20allowing%0Apractitioners%20and%20researchers%20alike%20to%20explore%20different%20methods%20while%20only%0Achanging%20a%20single%20line%20of%20Python%20code.%20Moreover%20%2Crerankers%20ensures%20that%20its%0Aimplementations%20are%20done%20with%20the%20fewest%20dependencies%20possible%2C%20and%20re-uses%20the%0Aoriginal%20implementation%20whenever%20possible%2C%20guaranteeing%20that%20our%20simplified%0Ainterface%20results%20in%20no%20performance%20degradation%20compared%20to%20more%20complex%20ones.%0AThe%20full%20source%20code%20and%20list%20of%20supported%20models%20are%20updated%20regularly%20and%0Aavailable%20at%20https%3A//github.com/answerdotai/rerankers.%0A&entry.1838667208=http%3A//arxiv.org/abs/2408.17344v2&entry.124074799=Read"},
{"title": "Verifiable cloud-based variational quantum algorithms", "author": "Junhong Yang and Banghai Wang and Junyu Quan and Qin Li", "abstract": "  Variational quantum algorithms (VQAs) have shown potential for quantum\nadvantage with noisy intermediate-scale quantum (NISQ) devices for quantum\nmachine learning (QML). However, given the high cost and limited availability\nof quantum resources, delegating VQAs via cloud networks is a more practical\nsolution for clients with limited quantum capabilities. Recently, Shingu et\nal.[Physical Review A, 105, 022603 (2022)] proposed a variational secure cloud\nquantum computing protocol, utilizing ancilla-driven quantum computation (ADQC)\nfor cloud-based VQAs with minimal quantum resource consumption. However, their\nprotocol lacks verifiability, which exposes it to potential malicious behaviors\nby the server. Additionally, channel loss requires frequent re-delegation as\nthe size of the delegated variational circuit grows, complicating verification\ndue to increased circuit complexity. This paper introduces a new protocol to\naddress these challenges and enhance both verifiability and tolerance to\nchannel loss in cloud-based VQAs.\n", "link": "http://arxiv.org/abs/2408.13713v3", "date": "2024-09-03", "relevancy": 1.1359, "topK": [{"title": "Tuning-Free Noise Rectification for High Fidelity Image-to-Video\n  Generation", "link": "http://arxiv.org/abs/2403.02827v1", "similarity": 0.3824}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.3793}, {"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.3685}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Verifiable%20cloud-based%20variational%20quantum%20algorithms&body=Title%3A%20Verifiable%20cloud-based%20variational%20quantum%20algorithms%0AAuthor%3A%20Junhong%20Yang%20and%20Banghai%20Wang%20and%20Junyu%20Quan%20and%20Qin%20Li%0AAbstract%3A%20%20%20Variational%20quantum%20algorithms%20%28VQAs%29%20have%20shown%20potential%20for%20quantum%0Aadvantage%20with%20noisy%20intermediate-scale%20quantum%20%28NISQ%29%20devices%20for%20quantum%0Amachine%20learning%20%28QML%29.%20However%2C%20given%20the%20high%20cost%20and%20limited%20availability%0Aof%20quantum%20resources%2C%20delegating%20VQAs%20via%20cloud%20networks%20is%20a%20more%20practical%0Asolution%20for%20clients%20with%20limited%20quantum%20capabilities.%20Recently%2C%20Shingu%20et%0Aal.%5BPhysical%20Review%20A%2C%20105%2C%20022603%20%282022%29%5D%20proposed%20a%20variational%20secure%20cloud%0Aquantum%20computing%20protocol%2C%20utilizing%20ancilla-driven%20quantum%20computation%20%28ADQC%29%0Afor%20cloud-based%20VQAs%20with%20minimal%20quantum%20resource%20consumption.%20However%2C%20their%0Aprotocol%20lacks%20verifiability%2C%20which%20exposes%20it%20to%20potential%20malicious%20behaviors%0Aby%20the%20server.%20Additionally%2C%20channel%20loss%20requires%20frequent%20re-delegation%20as%0Athe%20size%20of%20the%20delegated%20variational%20circuit%20grows%2C%20complicating%20verification%0Adue%20to%20increased%20circuit%20complexity.%20This%20paper%20introduces%20a%20new%20protocol%20to%0Aaddress%20these%20challenges%20and%20enhance%20both%20verifiability%20and%20tolerance%20to%0Achannel%20loss%20in%20cloud-based%20VQAs.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2408.13713v3%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DVerifiable%2520cloud-based%2520variational%2520quantum%2520algorithms%26entry.906535625%3DJunhong%2520Yang%2520and%2520Banghai%2520Wang%2520and%2520Junyu%2520Quan%2520and%2520Qin%2520Li%26entry.1292438233%3D%2520%2520Variational%2520quantum%2520algorithms%2520%2528VQAs%2529%2520have%2520shown%2520potential%2520for%2520quantum%250Aadvantage%2520with%2520noisy%2520intermediate-scale%2520quantum%2520%2528NISQ%2529%2520devices%2520for%2520quantum%250Amachine%2520learning%2520%2528QML%2529.%2520However%252C%2520given%2520the%2520high%2520cost%2520and%2520limited%2520availability%250Aof%2520quantum%2520resources%252C%2520delegating%2520VQAs%2520via%2520cloud%2520networks%2520is%2520a%2520more%2520practical%250Asolution%2520for%2520clients%2520with%2520limited%2520quantum%2520capabilities.%2520Recently%252C%2520Shingu%2520et%250Aal.%255BPhysical%2520Review%2520A%252C%2520105%252C%2520022603%2520%25282022%2529%255D%2520proposed%2520a%2520variational%2520secure%2520cloud%250Aquantum%2520computing%2520protocol%252C%2520utilizing%2520ancilla-driven%2520quantum%2520computation%2520%2528ADQC%2529%250Afor%2520cloud-based%2520VQAs%2520with%2520minimal%2520quantum%2520resource%2520consumption.%2520However%252C%2520their%250Aprotocol%2520lacks%2520verifiability%252C%2520which%2520exposes%2520it%2520to%2520potential%2520malicious%2520behaviors%250Aby%2520the%2520server.%2520Additionally%252C%2520channel%2520loss%2520requires%2520frequent%2520re-delegation%2520as%250Athe%2520size%2520of%2520the%2520delegated%2520variational%2520circuit%2520grows%252C%2520complicating%2520verification%250Adue%2520to%2520increased%2520circuit%2520complexity.%2520This%2520paper%2520introduces%2520a%2520new%2520protocol%2520to%250Aaddress%2520these%2520challenges%2520and%2520enhance%2520both%2520verifiability%2520and%2520tolerance%2520to%250Achannel%2520loss%2520in%2520cloud-based%2520VQAs.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2408.13713v3%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Verifiable%20cloud-based%20variational%20quantum%20algorithms&entry.906535625=Junhong%20Yang%20and%20Banghai%20Wang%20and%20Junyu%20Quan%20and%20Qin%20Li&entry.1292438233=%20%20Variational%20quantum%20algorithms%20%28VQAs%29%20have%20shown%20potential%20for%20quantum%0Aadvantage%20with%20noisy%20intermediate-scale%20quantum%20%28NISQ%29%20devices%20for%20quantum%0Amachine%20learning%20%28QML%29.%20However%2C%20given%20the%20high%20cost%20and%20limited%20availability%0Aof%20quantum%20resources%2C%20delegating%20VQAs%20via%20cloud%20networks%20is%20a%20more%20practical%0Asolution%20for%20clients%20with%20limited%20quantum%20capabilities.%20Recently%2C%20Shingu%20et%0Aal.%5BPhysical%20Review%20A%2C%20105%2C%20022603%20%282022%29%5D%20proposed%20a%20variational%20secure%20cloud%0Aquantum%20computing%20protocol%2C%20utilizing%20ancilla-driven%20quantum%20computation%20%28ADQC%29%0Afor%20cloud-based%20VQAs%20with%20minimal%20quantum%20resource%20consumption.%20However%2C%20their%0Aprotocol%20lacks%20verifiability%2C%20which%20exposes%20it%20to%20potential%20malicious%20behaviors%0Aby%20the%20server.%20Additionally%2C%20channel%20loss%20requires%20frequent%20re-delegation%20as%0Athe%20size%20of%20the%20delegated%20variational%20circuit%20grows%2C%20complicating%20verification%0Adue%20to%20increased%20circuit%20complexity.%20This%20paper%20introduces%20a%20new%20protocol%20to%0Aaddress%20these%20challenges%20and%20enhance%20both%20verifiability%20and%20tolerance%20to%0Achannel%20loss%20in%20cloud-based%20VQAs.%0A&entry.1838667208=http%3A//arxiv.org/abs/2408.13713v3&entry.124074799=Read"},
      ];
      const content = document.getElementById('content');
      function createPostElement(post) {
        const postElement = document.createElement('div');
        postElement.className = 'post';
        const dateElem = document.createElement('p');
        dateElem.setAttribute("class", "date");
        dateElem.textContent = post.date;
        postElement.appendChild(dateElem);

        const textElem = document.createElement('p');
        textElem.setAttribute("class", "text");
        const titleElem = document.createElement('p');
        titleElem.setAttribute("class", "title");
        titleElem.textContent = post.title;
        textElem.appendChild(titleElem);
        const authorElem = document.createElement('p');
        authorElem.setAttribute("class", "author");
        authorElem.textContent = post.author;
        textElem.appendChild(authorElem);
        const abstractElem = document.createElement('p');
        abstractElem.setAttribute("class", "abstract");
        abstractElem.textContent = post.abstract;
        textElem.appendChild(abstractElem);

        const linkElement = document.createElement('a');
        linkElement.setAttribute("class", "link");
        linkElement.href = post.link;
        linkElement.target = "_blank";
        linkElement.textContent = post.link.length > 50 ? post.link.substring(0, 50) + '...' : post.link;
        textElem.appendChild(linkElement);
        postElement.appendChild(textElem);

        const linkElementContainer = document.createElement('div');
        linkElementContainer.setAttribute("class", "comment");
        const actionElement = document.createElement('a');
        actionElement.setAttribute("class", "comment");
        actionElement.href = post.form;
        actionElement.textContent = "Action";
        actionElement.target = "_blank";
        linkElementContainer.appendChild(actionElement);
        const emailElement = document.createElement('a');
        emailElement.setAttribute("class", "comment");
        emailElement.href = post.mailto;
        emailElement.textContent = "Email";
        emailElement.target = "_blank";
        linkElementContainer.appendChild(emailElement);
        postElement.appendChild(linkElementContainer);
        const e = document.createElement('div');
        e.setAttribute("class", "clear");
        postElement.appendChild(e);

        const relevancyContainer = document.createElement('div');
        const relevancyValElem = document.createElement('p');
        relevancyValElem.textContent = "Relevancy " + post.relevancy;
        relevancyContainer.appendChild(relevancyValElem);
        post.topK.forEach((sub) => {
          const topKElem = document.createElement('a');
          topKElem.setAttribute("class", "topK");
          topKElem.href = sub.link;
          topKElem.textContent = sub.title + " (" + sub.similarity + ")";
          topKElem.target = "_blank";
          relevancyContainer.appendChild(topKElem);
        });
        postElement.appendChild(relevancyContainer);
        return postElement;
      }
      function loadPosts() {
        // Simulate loading more posts
        posts.forEach((post) => {
          const postElement = createPostElement(post);
          content.appendChild(postElement);
        });
      }
      // Load initial posts
      loadPosts();
    </script>

  </body>
</html>


